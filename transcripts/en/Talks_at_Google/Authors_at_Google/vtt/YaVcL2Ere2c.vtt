WEBVTT
Kind: captions
Language: en

00:00:00.499 --> 00:00:01.768
[APPLAUSE]

00:00:01.768 --> 00:00:04.059
STEVE LOHR: Stu, thanks for
that gracious introduction.

00:00:04.059 --> 00:00:06.182
I kept looking over my
shoulder for the person who

00:00:06.182 --> 00:00:07.390
was supposed to show up here.

00:00:07.390 --> 00:00:09.950
And as Stu alluded
to, I've been writing

00:00:09.950 --> 00:00:11.700
about the evolution
of these technologies

00:00:11.700 --> 00:00:15.660
long before terms like
data science and big data

00:00:15.660 --> 00:00:18.640
became common currency.

00:00:18.640 --> 00:00:21.240
The fun bit of what
I do-- journalism--

00:00:21.240 --> 00:00:22.790
is that it's a
curiosity license.

00:00:22.790 --> 00:00:25.250
You get to go out and talk
to people, to learn things.

00:00:25.250 --> 00:00:27.350
And this book project
gave me an opportunity

00:00:27.350 --> 00:00:30.140
to go deeper than
I would otherwise

00:00:30.140 --> 00:00:35.550
into what is obviously an
undeniably rich subject.

00:00:35.550 --> 00:00:37.380
I signed up for it in 2012.

00:00:37.380 --> 00:00:41.900
But really, it got
underway in earnest

00:00:41.900 --> 00:00:44.162
when I took a seven month
leave from "The New York

00:00:44.162 --> 00:00:50.470
Times" in the fall of 2013
through the spring of 2014.

00:00:50.470 --> 00:00:52.590
And Columbia University
was kind enough

00:00:52.590 --> 00:00:54.950
to give me a
[? bolt hold ?] to stay in.

00:00:54.950 --> 00:00:59.530
And I was there and
traveled from there.

00:00:59.530 --> 00:01:04.459
A decade ago when I did this,
you could just beg space.

00:01:04.459 --> 00:01:07.320
Now you can't, so I was part
of the Brown Institute, which

00:01:07.320 --> 00:01:09.320
is a collaboration of
the Columbia Graduate

00:01:09.320 --> 00:01:13.780
School of Journalism and
Stanford Engineering School.

00:01:13.780 --> 00:01:16.830
And they were kind enough
to give me a place to work.

00:01:16.830 --> 00:01:20.670
What I'm going to do is read
the first two pages of the book.

00:01:20.670 --> 00:01:22.650
Only two, I promise.

00:01:22.650 --> 00:01:26.230
but I think it does give you
a flavor of what I was up to

00:01:26.230 --> 00:01:27.850
and how I tried to go about it.

00:01:27.850 --> 00:01:32.310
After that, I was going to make
a few observations about things

00:01:32.310 --> 00:01:36.470
I think I learned through
the course of this research,

00:01:36.470 --> 00:01:41.360
and offer my tentative answers
to some of the questions

00:01:41.360 --> 00:01:42.665
that this technology raises.

00:01:45.530 --> 00:01:48.810
The book begins with
two short vignettes.

00:01:48.810 --> 00:01:50.985
One set in Memphis.

00:01:50.985 --> 00:01:56.450
It was a large product
distribution center.

00:01:56.450 --> 00:01:59.270
The second is in Atlanta.

00:01:59.270 --> 00:02:05.670
And that is a large intensive
care unit at Emory University

00:02:05.670 --> 00:02:08.150
Medical Research Center.

00:02:08.150 --> 00:02:10.199
This will not be as
good as the audiobook,

00:02:10.199 --> 00:02:13.230
but I'll give it a try.

00:02:13.230 --> 00:02:16.620
(READING) Just outside
Memphis, an industrial symphony

00:02:16.620 --> 00:02:20.420
of machines and humans
shuttles goods to and fro.

00:02:20.420 --> 00:02:24.320
Their carefully orchestrated
movements and identifying marks

00:02:24.320 --> 00:02:29.090
tracked by bar code scanners
and chips emitting radio waves.

00:02:29.090 --> 00:02:32.180
Medical arms snatch the
plastic shrink wrapped bundles

00:02:32.180 --> 00:02:35.960
off a conveyor belt as forklifts
ferry the packages onto trucks

00:02:35.960 --> 00:02:37.880
for long distance travel.

00:02:37.880 --> 00:02:41.270
Flesh and blood humans guide
and monitor the flow of goods

00:02:41.270 --> 00:02:43.580
and drive the
forklifts and trucks.

00:02:43.580 --> 00:02:46.090
McKesson, which
distributes about a third

00:02:46.090 --> 00:02:49.450
of all the pharmaceutical
products in the United States,

00:02:49.450 --> 00:02:53.040
runs this sprawling
showcase of efficiency.

00:02:53.040 --> 00:02:54.950
Its buildings span
the equivalent

00:02:54.950 --> 00:02:57.700
of more than eight
football fields,

00:02:57.700 --> 00:03:02.810
forming the hub of McKesson's
national distribution network.

00:03:02.810 --> 00:03:05.730
A feat of logistics
that sends goods

00:03:05.730 --> 00:03:09.420
to 26,000 customer locations
from neighborhood pharmacies

00:03:09.420 --> 00:03:10.790
to Walmart.

00:03:10.790 --> 00:03:12.630
The main cargo is drugs.

00:03:12.630 --> 00:03:16.690
Roughly 240 million pills a day.

00:03:16.690 --> 00:03:19.350
The pharmaceutical business
is one of high volume

00:03:19.350 --> 00:03:21.050
and razor thin margins.

00:03:21.050 --> 00:03:23.210
So understandably,
efficiency has

00:03:23.210 --> 00:03:27.190
been all but a religion
for McKesson for decades.

00:03:27.190 --> 00:03:29.120
Yet in the last
few years, McKesson

00:03:29.120 --> 00:03:32.080
has taken a striking
step further.

00:03:32.080 --> 00:03:34.520
Cutting the inventory
flowing through its network

00:03:34.520 --> 00:03:38.080
at any given time by
a billion dollars.

00:03:38.080 --> 00:03:40.160
The payoff came from
insights gleaned

00:03:40.160 --> 00:03:44.840
from harvesting all the product
location and transport data

00:03:44.840 --> 00:03:47.760
from scanners and
sensors, and then mining

00:03:47.760 --> 00:03:50.300
that data with clever
software to identify

00:03:50.300 --> 00:03:53.950
potential time-saving and
cost-cutting opportunities.

00:03:53.950 --> 00:03:56.190
The technology enhanced
view of the business

00:03:56.190 --> 00:04:00.070
was a breakthrough that Don
Walker, a senior executive

00:04:00.070 --> 00:04:05.160
at McKesson, calls "making
the invisible visible."

00:04:05.160 --> 00:04:08.720
In Atlanta, I stand outside
one of the glassed in rooms

00:04:08.720 --> 00:04:11.400
in the fifth floor intensive
care unit at the Emory

00:04:11.400 --> 00:04:13.140
University Hospital.

00:04:13.140 --> 00:04:16.440
Inside, the dense thicket
of electronic devices,

00:04:16.440 --> 00:04:20.690
a veritable forest of medical
computing, crowds the room.

00:04:20.690 --> 00:04:23.370
A respirator, kidney
machine, infusion machines

00:04:23.370 --> 00:04:26.410
pumping antibiotics and
pain killing opiates,

00:04:26.410 --> 00:04:29.240
and gadgets monitoring
heart rate, breathing,

00:04:29.240 --> 00:04:30.975
blood pressure,
oxygen saturation,

00:04:30.975 --> 00:04:33.420
and other vital signs.

00:04:33.420 --> 00:04:36.280
Nearly every machine has
its own computer monitor,

00:04:36.280 --> 00:04:40.250
each emitting an electronic
cacophony of beats and alerts.

00:04:40.250 --> 00:04:42.020
I count a dozen screens.

00:04:42.020 --> 00:04:44.630
Larger flat panels,
and smaller ones.

00:04:44.630 --> 00:04:46.800
Smartphone sized.

00:04:46.800 --> 00:04:50.000
A typical 20 bed
intensive care unit

00:04:50.000 --> 00:04:55.800
generates an estimated
160,000 data points a second.

00:04:55.800 --> 00:04:59.240
Amid all that data, informed
and distracted by it,

00:04:59.240 --> 00:05:02.560
doctors and nurses make
decisions at a rapid clip.

00:05:02.560 --> 00:05:05.360
About 100 decisions
a day per patient,

00:05:05.360 --> 00:05:08.220
according to research at Emory.

00:05:08.220 --> 00:05:10.560
So there's ample room for error.

00:05:10.560 --> 00:05:12.610
The overwhelmed
people need help.

00:05:12.610 --> 00:05:15.710
And Emory is one of a handful
of medical research centers

00:05:15.710 --> 00:05:19.410
that is working to transform
critical care with data,

00:05:19.410 --> 00:05:23.320
both in adult and neonatal
intensive care wards.

00:05:23.320 --> 00:05:25.200
The data streams from
the medical devices

00:05:25.200 --> 00:05:27.989
monitoring patients
are parsed by software

00:05:27.989 --> 00:05:29.780
that has been trained
to spot early warning

00:05:29.780 --> 00:05:33.740
signals that a patient's
condition is worsening.

00:05:33.740 --> 00:05:38.010
Doctor Timothy Buchman heads
up one such effort at Emory.

00:05:38.010 --> 00:05:41.470
A surgeon, a scientist, and
an experienced pilot, Buchman

00:05:41.470 --> 00:05:44.780
uses a flight analogy
to explain his goal.

00:05:44.780 --> 00:05:47.480
GPS location data on
planes is translated

00:05:47.480 --> 00:05:51.000
to screen images that show
air traffic controllers when

00:05:51.000 --> 00:05:53.860
a flight is going
astray-- "off trajectory,"

00:05:53.860 --> 00:05:59.310
as he puts it-- well
before they crash.

00:05:59.310 --> 00:06:02.360
Buchman wants the same sort
of early warning system

00:06:02.360 --> 00:06:04.850
for patients, whose
pattern of vital signs

00:06:04.850 --> 00:06:09.630
is off trajectory, before
they crash, in medical terms.

00:06:09.630 --> 00:06:14.070
That's where big data
is taking us, he says.

00:06:14.070 --> 00:06:16.730
The Age of Big Data
is coming of age,

00:06:16.730 --> 00:06:21.060
moving well beyond internet
incubators in Silicon Valley,

00:06:21.060 --> 00:06:23.610
such as Google and Facebook.

00:06:23.610 --> 00:06:25.800
It began in the digital
only world of bits

00:06:25.800 --> 00:06:29.450
and is rapidly marching into
the physical world of atoms.

00:06:29.450 --> 00:06:31.200
Into the mainstream.

00:06:31.200 --> 00:06:34.350
The McKesson distribution center
and the Emory intensive care

00:06:34.350 --> 00:06:36.300
unit show the way.

00:06:36.300 --> 00:06:39.510
Big data is saving
money and saving lives.

00:06:39.510 --> 00:06:41.970
Indeed, the long view
of the technology

00:06:41.970 --> 00:06:45.810
is that it will become a layer
of data-driven artificial

00:06:45.810 --> 00:06:50.060
intelligence that resides
on top of both the digital

00:06:50.060 --> 00:06:51.460
and the physical realms.

00:06:51.460 --> 00:06:55.295
And today, we're seeing the
early steps toward that vision.

00:06:55.295 --> 00:06:59.260
That's the end of the reading.

00:06:59.260 --> 00:07:00.135
So that's what I did.

00:07:00.135 --> 00:07:01.960
I ventured out into
this wider world

00:07:01.960 --> 00:07:04.270
to get a feel of where
this technology shines

00:07:04.270 --> 00:07:05.670
and where it stumbles.

00:07:05.670 --> 00:07:09.370
And I built the
narrative in good part

00:07:09.370 --> 00:07:11.980
around one young man
in one old company.

00:07:11.980 --> 00:07:14.320
And Stu referred
to the old company.

00:07:14.320 --> 00:07:19.331
The young man is Jeff
Hammerbacher, a data scientist.

00:07:19.331 --> 00:07:20.580
Some of you may well know him.

00:07:20.580 --> 00:07:22.120
He's 32 years old.

00:07:22.120 --> 00:07:24.320
And I chose him because
the arc of his career

00:07:24.320 --> 00:07:26.770
really illustrates the
spread in evolution

00:07:26.770 --> 00:07:28.930
of these quantitative
techniques.

00:07:28.930 --> 00:07:30.650
He was an Applied
Math major at Harvard.

00:07:30.650 --> 00:07:33.280
He went to Wall Street as
a quant for about a year.

00:07:33.280 --> 00:07:35.420
He started the data
team at Facebook.

00:07:35.420 --> 00:07:37.870
Left there after about
three years with a tidy sum

00:07:37.870 --> 00:07:39.950
and a reputation.

00:07:39.950 --> 00:07:42.616
Became one of the fore
founders of Cloudera company

00:07:42.616 --> 00:07:44.490
that many of you are
familiar with, I'm sure.

00:07:44.490 --> 00:07:46.182
He's still Chief
Scientist there.

00:07:46.182 --> 00:07:47.890
But these days he
spends most of his time

00:07:47.890 --> 00:07:50.970
at Mount Sinai Medical
Center here in New York.

00:07:50.970 --> 00:07:53.450
He thinks it's the
best use of his skills.

00:07:53.450 --> 00:07:56.190
And I also chose him
partly for his life story.

00:07:56.190 --> 00:07:59.460
We had an excerpt of it,
actually, two weeks ago Sunday

00:07:59.460 --> 00:08:01.050
in the "Times."

00:08:01.050 --> 00:08:04.470
But his father was a
General Motors auto worker

00:08:04.470 --> 00:08:05.590
from Fort Wayne, Indiana.

00:08:05.590 --> 00:08:07.350
Mom's a nurse.

00:08:07.350 --> 00:08:09.350
Jeff is somebody
who's stepped well

00:08:09.350 --> 00:08:11.240
beyond his background in life.

00:08:11.240 --> 00:08:14.070
Beyond the data of his life.

00:08:14.070 --> 00:08:19.870
And the old company is IBM,
whose data handling origins

00:08:19.870 --> 00:08:22.640
go back to its founding
technology of the Hollerith

00:08:22.640 --> 00:08:23.330
punch cards.

00:08:23.330 --> 00:08:25.880
This was the cutting
edge technology

00:08:25.880 --> 00:08:30.280
that was used to tabulate the
1890 census, when the U.S.

00:08:30.280 --> 00:08:33.549
population had
surged to 63 million,

00:08:33.549 --> 00:08:36.130
which was the big
data of its day.

00:08:36.130 --> 00:08:40.440
And with IBM, I use that
mainly its research arm

00:08:40.440 --> 00:08:47.240
as a conduit into some of these
projects that are in mainstream

00:08:47.240 --> 00:08:49.060
use across the economy.

00:08:49.060 --> 00:08:55.250
And both this McKesson and
Emory examples are part of that.

00:08:55.250 --> 00:08:57.265
Initially, I was
going to avoid Google.

00:08:57.265 --> 00:08:59.230
But it turned out, I couldn't.

00:08:59.230 --> 00:09:03.524
Because I did a fair bit of
reporting on Nest for a chapter

00:09:03.524 --> 00:09:05.940
on what we're now calling the
"internet of things''-- some

00:09:05.940 --> 00:09:07.610
calls it the
"industrial internet".

00:09:07.610 --> 00:09:12.990
I talked to Tony Fadell
at length Others there.

00:09:12.990 --> 00:09:17.060
Yoky Matsuoka, who has
defected since I wrote this.

00:09:20.400 --> 00:09:24.055
So Nest became a part of it
because Google acquired them

00:09:24.055 --> 00:09:28.720
in 2014, before this book,
I had finished writing it.

00:09:28.720 --> 00:09:31.920
In some ways it's not surprising
that Google is part of this.

00:09:31.920 --> 00:09:34.360
I mean, given its
growth and evolution.

00:09:34.360 --> 00:09:37.800
I mean, with search, ads,
language translation,

00:09:37.800 --> 00:09:41.460
image recognition, learning
thermostats, self-driving cars.

00:09:41.460 --> 00:09:44.100
The underlying technology
is much the same.

00:09:44.100 --> 00:09:47.170
It's again, this layer
of data-driven artificial

00:09:47.170 --> 00:09:48.610
intelligence.

00:09:48.610 --> 00:09:51.540
And in that sense, personalized
search and targeted ads

00:09:51.540 --> 00:09:53.912
have a lot in common with
personalized medicine

00:09:53.912 --> 00:09:54.995
and precision agriculture.

00:09:57.730 --> 00:10:02.050
So why is big data a big
deal in the wider world?

00:10:02.050 --> 00:10:05.080
I think the main answer here
is it's a powerful measurement

00:10:05.080 --> 00:10:06.212
revolution.

00:10:06.212 --> 00:10:08.670
And we've seen this before with
other technologies that let

00:10:08.670 --> 00:10:11.080
you see things as never before.

00:10:11.080 --> 00:10:14.080
With the telescope,
you were able to see

00:10:14.080 --> 00:10:16.170
biology and the
mysteries of human life

00:10:16.170 --> 00:10:18.590
down to the cellular level.

00:10:18.590 --> 00:10:20.770
With the telescope,
the analogy that you

00:10:20.770 --> 00:10:25.130
can look up and into the stars
in the galaxies and see man's

00:10:25.130 --> 00:10:29.740
place in the universe in
a way you couldn't before.

00:10:29.740 --> 00:10:33.520
But I would argue that the data
science-- as more generalized

00:10:33.520 --> 00:10:35.500
as computing as a
universal machine,

00:10:35.500 --> 00:10:39.520
Turing-- is a more
generalized tool then some

00:10:39.520 --> 00:10:41.340
that we've seen in the past.

00:10:41.340 --> 00:10:44.040
And if the internet
transformed the economics

00:10:44.040 --> 00:10:47.150
of communication-- which I think
it did-- I think data science

00:10:47.150 --> 00:10:49.490
can transform the
economics of discovery.

00:10:49.490 --> 00:10:53.110
And I'm certainly not the
only person that thinks that.

00:10:53.110 --> 00:10:57.190
And it also opens the door to a
shift in the nature of decision

00:10:57.190 --> 00:10:57.690
making.

00:10:57.690 --> 00:11:01.490
And here, it's a
shift more toward,

00:11:01.490 --> 00:11:05.910
decisions will be increasingly
based on data and analysis

00:11:05.910 --> 00:11:09.280
as opposed to experience
and intuition.

00:11:09.280 --> 00:11:13.720
More science, less gut
feel and rule of thumb.

00:11:13.720 --> 00:11:15.240
But there's a real caveat here.

00:11:15.240 --> 00:11:17.850
Because at best, what we
call experience and intuition

00:11:17.850 --> 00:11:20.190
is actually a huge
amount of data

00:11:20.190 --> 00:11:22.200
synthesized by a human brain.

00:11:22.200 --> 00:11:26.500
But data that you can't
attach a crisp number to.

00:11:26.500 --> 00:11:28.550
And it's sort of what
people talk about,

00:11:28.550 --> 00:11:31.180
design thinking or
systems thinking.

00:11:31.180 --> 00:11:35.010
There's an anecdote, an episode
that occurred at "The New York

00:11:35.010 --> 00:11:36.390
Times" years back.

00:11:36.390 --> 00:11:39.950
A gathering in the board room
that makes me think of this.

00:11:39.950 --> 00:11:43.340
And it was early 2010.

00:11:43.340 --> 00:11:46.960
Apple had introduced the iPad,
but it wasn't on sale yet.

00:11:46.960 --> 00:11:48.750
And Steve Jobs was
around, showing it off

00:11:48.750 --> 00:11:49.749
and answering questions.

00:11:49.749 --> 00:11:52.620
And there were about 13
people in the boardroom

00:11:52.620 --> 00:11:54.000
from various parts of the paper.

00:11:54.000 --> 00:11:56.400
So some people knew
him, most didn't.

00:11:56.400 --> 00:11:59.220
So after he does
his demonstration,

00:11:59.220 --> 00:12:01.710
the editor asks him what
seemed a reasonable question.

00:12:01.710 --> 00:12:04.150
It's a relatively
new product category

00:12:04.150 --> 00:12:06.050
that they were
making a big bet on.

00:12:06.050 --> 00:12:08.170
How much market
research went into it?

00:12:08.170 --> 00:12:10.984
And those of us that
knew Jobs kind of

00:12:10.984 --> 00:12:13.400
knew what the answer was, but
it was still fun to hear it.

00:12:13.400 --> 00:12:14.600
His answer was, none.

00:12:14.600 --> 00:12:18.110
It's not the consumer's
job to know what they want.

00:12:18.110 --> 00:12:24.350
And Steve Jobs is
obviously not a model

00:12:24.350 --> 00:12:27.490
for a generalization in a way.

00:12:27.490 --> 00:12:31.760
But one of the things that
he always talked about

00:12:31.760 --> 00:12:34.100
was what he called taste.

00:12:34.100 --> 00:12:39.550
And taste to him was immersing
yourself and exposing yourself

00:12:39.550 --> 00:12:42.130
to the best your culture has.

00:12:42.130 --> 00:12:46.680
And again, a lot of
this was cultural stuff.

00:12:46.680 --> 00:12:53.950
And his view was that doing
that would enhance your decision

00:12:53.950 --> 00:12:56.580
making and improve your work,
whether you were a software

00:12:56.580 --> 00:12:59.260
engineer or a fashion designer.

00:12:59.260 --> 00:13:03.340
So it's not all
numbers by any means.

00:13:03.340 --> 00:13:05.600
Now, data driven
decision making.

00:13:05.600 --> 00:13:07.840
Haven't we been down
this path before?

00:13:07.840 --> 00:13:12.710
Many times, but most
notably a century ago.

00:13:12.710 --> 00:13:15.425
Frederick Winslow
Taylor was the father

00:13:15.425 --> 00:13:18.120
of what was called
scientific management.

00:13:18.120 --> 00:13:21.710
His time and motion studies
were going to revolutionize work

00:13:21.710 --> 00:13:22.320
at the time.

00:13:22.320 --> 00:13:25.500
And it's easy to look back at
it now, view it from today,

00:13:25.500 --> 00:13:29.310
and you look at this as a
simple-minded deficiency

00:13:29.310 --> 00:13:31.120
measurement run amok.

00:13:31.120 --> 00:13:33.620
And he would be satirized
by Charlie Chaplin

00:13:33.620 --> 00:13:38.570
in "Modern Times" with
machines grinding you up.

00:13:38.570 --> 00:13:40.910
But in its day, it was seen
as a liberating movement.

00:13:40.910 --> 00:13:43.540
And much of what's
being said about data

00:13:43.540 --> 00:13:47.780
science these days has some
eerie-- the language is

00:13:47.780 --> 00:13:50.460
a little bit more
Elizabethan-- but this kind of,

00:13:50.460 --> 00:13:53.130
it can change everything
mentality was very much

00:13:53.130 --> 00:13:55.610
a part of the dialogue then.

00:13:55.610 --> 00:13:57.690
And it was a force that
was going to free workers

00:13:57.690 --> 00:14:00.590
from the dictates of
authoritarian bosses.

00:14:00.590 --> 00:14:03.910
And the economy from price
fix and corporate trust.

00:14:03.910 --> 00:14:07.550
It was championed by leading
progressives of the day.

00:14:07.550 --> 00:14:10.610
Louis Brandeis, known as the
people's lawyer who later

00:14:10.610 --> 00:14:13.000
became a Supreme Court justice.

00:14:13.000 --> 00:14:15.840
And Ida Tarbell, the
muckraking journalist

00:14:15.840 --> 00:14:18.510
who took on Standard Oil.

00:14:18.510 --> 00:14:19.730
Is it different this time?

00:14:19.730 --> 00:14:21.220
Yes.

00:14:21.220 --> 00:14:22.860
It's qualitatively different.

00:14:22.860 --> 00:14:23.770
There's just so much.

00:14:23.770 --> 00:14:28.360
There's obviously not much of
a comparison between the data

00:14:28.360 --> 00:14:31.050
and artificial
intelligence tools today,

00:14:31.050 --> 00:14:33.740
and Taylor's tools, which
were a stopwatch, a clipboard,

00:14:33.740 --> 00:14:35.440
and his own brain.

00:14:35.440 --> 00:14:36.960
But that said, of
course, it doesn't

00:14:36.960 --> 00:14:40.487
mean that data technology
can't be abused.

00:14:40.487 --> 00:14:42.820
Many of you know far better
than I, there's so much data

00:14:42.820 --> 00:14:43.220
these days.

00:14:43.220 --> 00:14:44.310
There's always patterns in it.

00:14:44.310 --> 00:14:45.185
Are they significant?

00:14:45.185 --> 00:14:49.181
And it's easy to make this the
fodder for bias fact finding

00:14:49.181 --> 00:14:49.680
missions.

00:14:49.680 --> 00:14:53.580
I know the facts,
now let's find them.

00:14:53.580 --> 00:14:55.720
So what's worth worrying about?

00:14:55.720 --> 00:14:56.560
Privacy?

00:14:56.560 --> 00:14:58.990
A real concern.

00:14:58.990 --> 00:15:02.145
But my view is that this
really should be manageable.

00:15:02.145 --> 00:15:05.880
And in good part because we
have a healthy marketplace

00:15:05.880 --> 00:15:09.550
of competing interests that
will get us not to a solution,

00:15:09.550 --> 00:15:12.270
but to an accommodation
with this technology.

00:15:12.270 --> 00:15:15.430
And we've seen that in the
past innumerable times.

00:15:15.430 --> 00:15:20.610
At the turn of the century,
film cameras came on the market.

00:15:20.610 --> 00:15:24.590
The brownie eventually cost $1,
a lot of money in those days,

00:15:24.590 --> 00:15:25.590
but still affordable.

00:15:25.590 --> 00:15:29.750
So people could take snapshots
and photograph people

00:15:29.750 --> 00:15:35.470
in unguarded moments as opposed
to the old chemistry experiment

00:15:35.470 --> 00:15:39.999
cameras where they had to be
frozen like a stone statue

00:15:39.999 --> 00:15:40.790
to be photographed.

00:15:40.790 --> 00:15:44.540
But this spontaneity
of photography

00:15:44.540 --> 00:15:46.090
really scared people.

00:15:46.090 --> 00:15:49.840
These people who took
snapshots, as they were called,

00:15:49.840 --> 00:15:51.540
they were called camera fiends.

00:15:51.540 --> 00:15:54.620
They were banned from beaches
and from the Washington

00:15:54.620 --> 00:15:56.530
Monument for awhile.

00:15:56.530 --> 00:16:00.260
In that case, people adapted,
social attitudes changed,

00:16:00.260 --> 00:16:02.230
and things moved on.

00:16:02.230 --> 00:16:03.920
Sometimes laws are needed.

00:16:03.920 --> 00:16:06.230
In the 1960s,
mainframe computers

00:16:06.230 --> 00:16:08.970
arrived and amassed
large databases

00:16:08.970 --> 00:16:10.030
of personal information.

00:16:10.030 --> 00:16:12.740
The IRS had tax returns.

00:16:12.740 --> 00:16:16.020
Credit reporting bureaus had
a lot of personal information.

00:16:16.020 --> 00:16:18.570
There was a lot of talk about
a corporate or government

00:16:18.570 --> 00:16:19.710
big brother.

00:16:19.710 --> 00:16:25.560
In 1964, the same year the IBM
360 mainframe was introduced,

00:16:25.560 --> 00:16:27.490
Vance Packard--
who is better known

00:16:27.490 --> 00:16:30.040
for writing "The Hidden
Persuaders" years

00:16:30.040 --> 00:16:33.750
earlier on the evils
of advertising--

00:16:33.750 --> 00:16:36.555
wrote a book in 1964
called "The Naked Society."

00:16:36.555 --> 00:16:39.310
And it echoed much of
what we hear today,

00:16:39.310 --> 00:16:41.650
including his warning
of what he called,

00:16:41.650 --> 00:16:45.040
quote, "the hidden
eyes of business."

00:16:45.040 --> 00:16:48.060
But what emerged
were basic rules.

00:16:48.060 --> 00:16:50.460
First, the Fair Credit
Reporting Act in '64.

00:16:50.460 --> 00:16:52.740
And later, the
basic Privacy Act.

00:16:52.740 --> 00:16:55.250
You had baseline rules.

00:16:55.250 --> 00:16:58.170
Some practices were
banned and changed.

00:16:58.170 --> 00:17:00.010
There was a workable
accommodation.

00:17:00.010 --> 00:17:05.000
And industries thrived,
including consumer finance.

00:17:05.000 --> 00:17:10.480
And also, people make personal
adjustments in their behavior.

00:17:10.480 --> 00:17:12.310
I mean, I do it now.

00:17:12.310 --> 00:17:17.020
My personal concern is much
more my credit card transactions

00:17:17.020 --> 00:17:20.319
being watched than
my browsing behavior.

00:17:20.319 --> 00:17:22.300
It's what I'm actually
doing as opposed to what

00:17:22.300 --> 00:17:24.520
I might be interested in.

00:17:24.520 --> 00:17:27.510
So for me, the health
club membership

00:17:27.510 --> 00:17:30.500
goes in the credit card, the
liquor bills I get with cash.

00:17:33.770 --> 00:17:36.360
Ed Felton of Princeton, who
has a role in this book,

00:17:36.360 --> 00:17:41.260
he's a former chief technologist
with the Federal Trade

00:17:41.260 --> 00:17:41.760
Commission.

00:17:41.760 --> 00:17:43.000
He's a computer security expert.

00:17:43.000 --> 00:17:45.375
I first ran across him when
he testified in the Microsoft

00:17:45.375 --> 00:17:47.020
trial years ago.

00:17:47.020 --> 00:17:49.530
And he's a computer scientist
and public policy expert.

00:17:49.530 --> 00:17:51.060
But Ed, when I was
talking to him,

00:17:51.060 --> 00:17:54.790
said he likes "Breaking
Bad," the television series.

00:17:54.790 --> 00:17:58.860
And he's interested in how
methamphetamine is made.

00:17:58.860 --> 00:18:03.330
And so he looks it
up, but it's always

00:18:03.330 --> 00:18:07.730
with a separate browser
in anonymous mode.

00:18:07.730 --> 00:18:14.340
Whether any of this
does any good, I wonder.

00:18:14.340 --> 00:18:17.700
Julia Angwin did a really nice
book on computer security.

00:18:17.700 --> 00:18:18.980
She's now in ProPublica.

00:18:18.980 --> 00:18:21.020
And kind of where
you come out is

00:18:21.020 --> 00:18:23.430
there's no way to be
a privacy survivalist.

00:18:23.430 --> 00:18:26.050
It just doesn't work.

00:18:26.050 --> 00:18:28.200
But again, we've
been this way before.

00:18:28.200 --> 00:18:31.730
With every significant
technology there's trade off.

00:18:31.730 --> 00:18:33.210
Fire?

00:18:33.210 --> 00:18:35.460
Cook your food with it
or burn down your hut.

00:18:35.460 --> 00:18:37.600
Automobile?

00:18:37.600 --> 00:18:39.720
People die in car accidents,
it pollutes the air.

00:18:39.720 --> 00:18:43.450
But it's also a vehicle
of personal freedom,

00:18:43.450 --> 00:18:46.980
gave rise to national
and regional markets.

00:18:46.980 --> 00:18:49.310
And I think the
bet here is you're

00:18:49.310 --> 00:18:51.330
going to see the same
pattern of adoption.

00:18:51.330 --> 00:18:53.610
There'd be some rules, there'd
be checks and balances,

00:18:53.610 --> 00:18:56.100
and social norms change.

00:18:56.100 --> 00:18:59.120
This technology is just not
fundamentally different.

00:18:59.120 --> 00:19:02.970
And it's a world that we're all
going to increasingly live in

00:19:02.970 --> 00:19:05.360
in any case, so we might
as well get used to it.

00:19:05.360 --> 00:19:09.740
But I'm a qualified
optimist on that front.

00:19:09.740 --> 00:19:14.400
Again, in part, because the
market forces are there.

00:19:14.400 --> 00:19:17.220
We have a lot of people who
are concerned and watching.

00:19:17.220 --> 00:19:19.290
I think that will
sort itself out

00:19:19.290 --> 00:19:22.140
as these things
have in the past.

00:19:22.140 --> 00:19:24.760
I'm going to close with an issue
that I think is more subtle,

00:19:24.760 --> 00:19:26.620
but it is concerning, I think.

00:19:26.620 --> 00:19:30.840
And it's the black box nature
of data science in making

00:19:30.840 --> 00:19:33.630
certain kinds of decisions.

00:19:33.630 --> 00:19:36.500
Up until now, much
of the application

00:19:36.500 --> 00:19:40.150
has been in things like
marketing, ad targeting,

00:19:40.150 --> 00:19:42.762
product recommendations,
and the like.

00:19:42.762 --> 00:19:47.330
A fairly prominent data
scientist, Claudia Perlich,

00:19:47.330 --> 00:19:49.580
who used to work at
IBM research and is now

00:19:49.580 --> 00:19:51.477
working for an ad
targeting firm,

00:19:51.477 --> 00:19:53.060
takes the view that,
look, this market

00:19:53.060 --> 00:19:56.480
is a great time to
experiment with data science.

00:19:56.480 --> 00:19:58.710
She says, look, if
my algorithm's wrong,

00:19:58.710 --> 00:19:59.820
what happens?

00:19:59.820 --> 00:20:02.570
Somebody sees the wrong ad.

00:20:02.570 --> 00:20:06.285
It's not a false positive
for breast cancer.

00:20:06.285 --> 00:20:06.910
Certainly true.

00:20:06.910 --> 00:20:09.230
But I do think we're moving
into realms now where

00:20:09.230 --> 00:20:10.760
the stakes are getting higher.

00:20:10.760 --> 00:20:13.400
Data science is being used
in medical diagnosis, crime

00:20:13.400 --> 00:20:16.170
prevention, loan approvals.

00:20:16.170 --> 00:20:19.360
Is statistical
correlation good enough

00:20:19.360 --> 00:20:22.590
to make decisions that will have
a lasting effect on someone's

00:20:22.590 --> 00:20:23.580
life?

00:20:23.580 --> 00:20:25.950
And I think that's a
question we'll wrestle with.

00:20:26.570 --> 00:20:28.565
Danny Hillis-- a
computer scientist

00:20:28.565 --> 00:20:30.070
and artificial
intelligence expert--

00:20:30.070 --> 00:20:32.950
says what this technology
needs is what he calls

00:20:32.950 --> 00:20:35.250
"an explanatory assistant.

00:20:35.250 --> 00:20:38.350
An audit trail that traces
the data and inferences that

00:20:38.350 --> 00:20:42.100
went into the mix of a
software generated decision."

00:20:42.100 --> 00:20:44.110
The technology,
Hillis says, must

00:20:44.110 --> 00:20:48.820
be able to quote, "tell a story
about why it did what it did.

00:20:48.820 --> 00:20:50.530
The key thing that
will make it work

00:20:50.530 --> 00:20:54.460
and make it acceptable to
society is storytelling."

00:20:54.460 --> 00:20:55.380
I agree.

00:20:55.380 --> 00:20:58.800
And it's a good problem,
as they say in computing.

00:20:58.800 --> 00:20:59.684
Thank you.

00:20:59.684 --> 00:21:02.648
[APPLAUSE]

00:21:05.120 --> 00:21:08.980
AUDIENCE: I was struck by
your first two examples, which

00:21:08.980 --> 00:21:10.600
you've sort of lumped together.

00:21:10.600 --> 00:21:14.450
But it seemed to me one
group could actually tell

00:21:14.450 --> 00:21:17.510
whether what they were doing
with big data was effective,

00:21:17.510 --> 00:21:19.460
which were the drug
people, who can measure.

00:21:19.460 --> 00:21:23.330
And the medical
guys have more hope

00:21:23.330 --> 00:21:27.980
until you have sort of actual,
peer reviewed outcome studies.

00:21:27.980 --> 00:21:29.710
And medicine is
filled with people

00:21:29.710 --> 00:21:32.900
who have gotten the wrong
conclusion from lots of data.

00:21:32.900 --> 00:21:34.717
So how do you correct this.

00:21:34.717 --> 00:21:36.050
STEVE LOHR: I'd make two points.

00:21:36.050 --> 00:21:43.440
And one is, partly what
I wanted to do here--

00:21:43.440 --> 00:21:47.060
and I was writing to
a moving target-- was

00:21:47.060 --> 00:21:51.700
I was looking on the latter
side for research projects

00:21:51.700 --> 00:21:53.810
that are sort of in progress.

00:21:53.810 --> 00:21:57.240
That weren't going to be over
by the time this book came out.

00:21:57.240 --> 00:22:01.670
And they have real
physical world

00:22:01.670 --> 00:22:06.170
and commercial implications,
but aren't there yet.

00:22:06.170 --> 00:22:08.620
And that one, the
Emory stuff, certainly

00:22:08.620 --> 00:22:10.300
falls into that category.

00:22:10.300 --> 00:22:15.750
The McKesson one is
interesting, in part,

00:22:15.750 --> 00:22:17.740
because you actually
get a dollar number.

00:22:17.740 --> 00:22:19.060
They did something.

00:22:19.060 --> 00:22:25.230
And it's also a lot easier, to
be fair, because their data was

00:22:25.230 --> 00:22:26.585
always pretty tame.

00:22:29.740 --> 00:22:32.304
This was almost all
their internal systems.

00:22:32.304 --> 00:22:34.470
Yeah, there's some weather
report stuff put into it,

00:22:34.470 --> 00:22:37.259
but basically, this was
a cake you could bake.

00:22:37.259 --> 00:22:39.300
I mean, people would argue
that that's not really

00:22:39.300 --> 00:22:41.310
a big data problem.

00:22:41.310 --> 00:22:43.880
But again, on these
sides of things,

00:22:43.880 --> 00:22:49.350
I think that the most distorted
thing is the "big" in big data.

00:22:49.350 --> 00:22:51.739
The numerator you can
inflate easily with video.

00:22:51.739 --> 00:22:53.530
I mean, there's a lot
of water in the ocean

00:22:53.530 --> 00:22:54.640
too, you can't drink it.

00:22:54.640 --> 00:23:01.190
I think a lot of these problems
like the McKesson one--

00:23:01.190 --> 00:23:04.570
which nobody here would
think that's big data--

00:23:04.570 --> 00:23:06.690
but it's pretty interesting.

00:23:06.690 --> 00:23:08.660
It applies the same
quantitative [INAUDIBLE].

00:23:08.660 --> 00:23:10.285
AUDIENCE: That's the
one I believed in.

00:23:10.285 --> 00:23:11.780
STEVE LOHR: Yeah.

00:23:11.780 --> 00:23:13.705
Look, we'll see.

00:23:13.705 --> 00:23:17.890
FDA Approval, everything that
this kid Hammerbacher is doing.

00:23:17.890 --> 00:23:19.990
Mount Sinai is doing a
lot that's interesting.

00:23:19.990 --> 00:23:21.380
It's Carl Icahn's money.

00:23:21.380 --> 00:23:24.620
He also has a big
institute at Princeton

00:23:24.620 --> 00:23:26.420
that's doing some of this.

00:23:26.420 --> 00:23:27.730
All the genetic stuff.

00:23:27.730 --> 00:23:29.510
I mean, nothing
has done anything

00:23:29.510 --> 00:23:33.480
to cancer that is anywhere
close to quitting smoking.

00:23:33.480 --> 00:23:35.400
This is always the critique.

00:23:35.400 --> 00:23:39.540
Now, to say that it
isn't there yet-- there's

00:23:39.540 --> 00:23:42.190
a lot of things that
aren't there yet.

00:23:42.190 --> 00:23:43.930
You've got to bet on the future.

00:23:43.930 --> 00:23:46.820
STU: Well, nothing has
done more for medicine

00:23:46.820 --> 00:23:48.870
than cleaning up the water.

00:23:48.870 --> 00:23:50.730
Doctors used to
be negative value,

00:23:50.730 --> 00:23:53.652
now they're poking up
above zero historically.

00:23:53.652 --> 00:23:55.610
STEVE LOHR: There's this
whole broader argument

00:23:55.610 --> 00:23:56.984
about whether the
stuff is really

00:23:56.984 --> 00:23:59.230
going to increase productivity
and benefit and such.

00:23:59.230 --> 00:24:01.650
And Bob Gordon of Northwestern
is the skeptic that

00:24:01.650 --> 00:24:03.210
makes just your point of view.

00:24:03.210 --> 00:24:07.900
Nothing of this is close is
as close to modern sanitation

00:24:07.900 --> 00:24:09.435
in toilets.

00:24:09.435 --> 00:24:12.870
STU: And besides, fire
was a really good start.

00:24:12.870 --> 00:24:13.412
Please, next?

00:24:13.412 --> 00:24:14.869
AUDIENCE: So you
mentioned that you

00:24:14.869 --> 00:24:17.560
think a lot of the privacy
concerns can be addressed.

00:24:17.560 --> 00:24:19.520
And I'm wondering if you
think laws are really

00:24:19.520 --> 00:24:20.686
going to catch up with them?

00:24:20.686 --> 00:24:22.570
If we need to be more active?

00:24:22.570 --> 00:24:25.370
Especially as we have new
ways of gathering data.

00:24:25.370 --> 00:24:27.100
One example would be,
many retail stores

00:24:27.100 --> 00:24:29.324
now have free Wi-Fi
for the sole purpose

00:24:29.324 --> 00:24:30.740
that they can
actually track where

00:24:30.740 --> 00:24:34.020
people physically go since there
phone's automatically connect.

00:24:34.020 --> 00:24:35.500
And then other
companies aggregate

00:24:35.500 --> 00:24:37.416
that data and they say,
this person physically

00:24:37.416 --> 00:24:40.440
walked into a JCrew or walked
into this other scenario.

00:24:40.440 --> 00:24:42.690
And then they start
automatically sending things.

00:24:42.690 --> 00:24:45.130
And they have it down
to your exact device ID

00:24:45.130 --> 00:24:46.400
number, which can't be reset.

00:24:46.400 --> 00:24:48.380
So they can trace
you everywhere.

00:24:48.380 --> 00:24:50.152
Do we need more active laws?

00:24:50.152 --> 00:24:52.110
STEVE LOHR: You know,
what you're going to get,

00:24:52.110 --> 00:24:54.370
it's like this whole
debate about privacy.

00:24:54.370 --> 00:24:57.550
The Obama privacy
proposals have come up.

00:24:57.550 --> 00:25:01.060
The good thing is that
both sides are opposed.

00:25:01.060 --> 00:25:05.290
The internet association, which
Google is, they don't like it.

00:25:05.290 --> 00:25:07.015
And the privacy guys
are just up in arms.

00:25:07.015 --> 00:25:08.100
Riddled with loopholes.

00:25:11.670 --> 00:25:14.231
I think you do need some
baseline rules to set things

00:25:14.231 --> 00:25:14.730
up.

00:25:14.730 --> 00:25:16.720
But I also think-- from
a company standpoint--

00:25:16.720 --> 00:25:18.970
I think some of this, like
the retail example that you

00:25:18.970 --> 00:25:21.080
mentioned, I think
that's something

00:25:21.080 --> 00:25:27.946
that companies are going to have
to decide what they're doing.

00:25:27.946 --> 00:25:29.320
I think there are
a lot of things

00:25:29.320 --> 00:25:35.300
that are going to be
legally just fine,

00:25:35.300 --> 00:25:38.940
but are a problem they're
going to have to deal with.

00:25:38.940 --> 00:25:41.360
And this is not to
your retail example,

00:25:41.360 --> 00:25:44.420
I digress here, but remember
this Facebook thing?

00:25:44.420 --> 00:25:46.340
About when they
got in the stream

00:25:46.340 --> 00:25:47.980
and they could kind
of do something.

00:25:52.010 --> 00:25:55.560
What struck me about
it was three things.

00:25:55.560 --> 00:25:59.580
How modest was the intervention
if you looked at the numbers?

00:25:59.580 --> 00:26:01.380
How slight was the outcome?

00:26:01.380 --> 00:26:03.680
And how great was the rage?

00:26:03.680 --> 00:26:06.880
And I think there's a
signal there to everybody.

00:26:06.880 --> 00:26:09.030
Don't worry about it.

00:26:09.030 --> 00:26:11.180
And getting back to Nest.

00:26:11.180 --> 00:26:13.560
This is the human control side.

00:26:13.560 --> 00:26:15.970
I realize this isn't entirely
responsive to your laws

00:26:15.970 --> 00:26:17.180
question.

00:26:17.180 --> 00:26:22.540
They first started with Nest
well before Google bought it.

00:26:22.540 --> 00:26:24.520
And they did the
preliminary testing.

00:26:24.520 --> 00:26:27.247
And they thought, well,
we have these sensors.

00:26:27.247 --> 00:26:28.830
There's a near one,
there's a far one.

00:26:28.830 --> 00:26:31.270
And we'll see how
people operate,

00:26:31.270 --> 00:26:33.740
and then the machine
will take over.

00:26:33.740 --> 00:26:35.810
And people hated that.

00:26:39.190 --> 00:26:41.550
But they gave people logins
so you could constantly

00:26:41.550 --> 00:26:42.940
check if you want.

00:26:42.940 --> 00:26:44.670
Little alerts,
messages coming back.

00:26:44.670 --> 00:26:46.790
Do you still want this?

00:26:46.790 --> 00:26:48.260
They didn't change anything.

00:26:48.260 --> 00:26:49.980
The machine took over.

00:26:49.980 --> 00:26:52.300
But they had the
illusion of control.

00:26:52.300 --> 00:26:54.530
Or to be fair,
they had the option

00:26:54.530 --> 00:26:56.580
if they really wanted to.

00:26:56.580 --> 00:26:58.350
And I think that's
the kind of thing

00:26:58.350 --> 00:27:02.770
that is going to be crucial.

00:27:06.610 --> 00:27:09.490
The answer on technology and
law is, well, of course not.

00:27:09.490 --> 00:27:14.020
That doesn't mean you're
not going to get them.

00:27:14.020 --> 00:27:16.530
STU: Good to hear the
Betty Crocker plus break

00:27:16.530 --> 00:27:20.190
an egg into it moment repeated.

00:27:20.190 --> 00:27:22.800
For those of you who don't
know, when the first cake mixes

00:27:22.800 --> 00:27:26.810
came out, all you did
was add water and bake.

00:27:26.810 --> 00:27:29.490
And they were quite
unpopular with the housewives

00:27:29.490 --> 00:27:30.800
of the time.

00:27:30.800 --> 00:27:33.940
Yes, I know what I just
said sexlessly it's true.

00:27:33.940 --> 00:27:36.600
And they fixed it with
some market studies.

00:27:36.600 --> 00:27:39.310
And the problem was, they felt
they weren't adding enough.

00:27:39.310 --> 00:27:41.120
So they removed the
powder egg, made

00:27:41.120 --> 00:27:42.920
you break an egg
into the mixture,

00:27:42.920 --> 00:27:45.600
and suddenly the sales took off.

00:27:45.600 --> 00:27:48.930
And no, it didn't
taste any better.

00:27:48.930 --> 00:27:49.780
AUDIENCE: Hi.

00:27:49.780 --> 00:27:51.280
I was very interested
in a point you

00:27:51.280 --> 00:27:57.870
made about how human intuition
is nothing but connecting

00:27:57.870 --> 00:27:59.710
a large amount of data points.

00:27:59.710 --> 00:28:01.085
But then you also
gave an example

00:28:01.085 --> 00:28:03.910
of Steve Jobs who, with a
very little amount of data,

00:28:03.910 --> 00:28:06.380
knew that the iPad
would succeed.

00:28:06.380 --> 00:28:09.890
So I wanted to ask you, do you
think that those large amounts

00:28:09.890 --> 00:28:13.670
of data-- do you think the human
brain can connect them in ways

00:28:13.670 --> 00:28:14.360
a machine can't?

00:28:14.360 --> 00:28:16.045
What are your thoughts on that?

00:28:16.045 --> 00:28:18.400
STEVE LOHR: And in
fairness to Jobs on this,

00:28:18.400 --> 00:28:20.590
he's a great
product team leader.

00:28:20.590 --> 00:28:23.170
This is a guy who's seen a lot.

00:28:23.170 --> 00:28:25.390
There's a lot that
kind of went into it.

00:28:25.390 --> 00:28:27.550
As well as his own prejudices.

00:28:33.580 --> 00:28:36.550
The reason I hesitate
is-- people in this room

00:28:36.550 --> 00:28:42.160
know it better than I--
maybe we just aren't there.

00:28:42.160 --> 00:28:46.630
There's a lot of people, the
Ray Kurzweil's of the world,

00:28:46.630 --> 00:28:47.470
it's a done deal.

00:28:47.470 --> 00:28:49.886
We're just kind of waiting a
few years for this to happen.

00:28:53.068 --> 00:28:56.070
A few years back, of
a colleague of mine,

00:28:56.070 --> 00:28:58.340
John Markoff and
I at the "Times"

00:28:58.340 --> 00:29:01.100
did a science series, actually,
on artificial intelligence

00:29:01.100 --> 00:29:03.910
called "Smarter Than You
Think" is the rubric we used.

00:29:03.910 --> 00:29:05.760
It was probably too
early, actually.

00:29:05.760 --> 00:29:07.770
The nice thing is that
John broke the Google car

00:29:07.770 --> 00:29:10.050
story because of that.

00:29:10.050 --> 00:29:13.950
But in talking to
almost everybody who

00:29:13.950 --> 00:29:17.280
was a real expert on this, none
of them are religious people,

00:29:17.280 --> 00:29:19.850
but they had a real
awe for what we call

00:29:19.850 --> 00:29:21.205
generalized human intelligence.

00:29:24.030 --> 00:29:27.536
And the history of A.I.
Has been kind of humbling.

00:29:27.536 --> 00:29:35.950
If you go back, people thought
this would all be a lot easier.

00:29:35.950 --> 00:29:38.120
And yet, the
counterargument, of course,

00:29:38.120 --> 00:29:40.810
is that machines are going
to do it differently,

00:29:40.810 --> 00:29:43.810
but they can do it.

00:29:43.810 --> 00:29:48.650
One of the great pioneers
of voice recognition,

00:29:48.650 --> 00:29:52.440
Fred Jelinek said
to me at the time,

00:29:52.440 --> 00:29:54.950
used one of his classic
lines, that airplanes

00:29:54.950 --> 00:29:58.382
don't flap their wings.

00:29:58.382 --> 00:30:02.250
Yeah, they'll do it
differently, but they can do it.

00:30:02.250 --> 00:30:02.970
What do I know?

00:30:02.970 --> 00:30:08.650
I mean, history would
suggest this has been harder.

00:30:08.650 --> 00:30:09.780
Those kinds of things.

00:30:09.780 --> 00:30:12.440
And frankly, the
management issue

00:30:12.440 --> 00:30:17.025
with all of this is-- I didn't
mention here-- is awesome.

00:30:17.025 --> 00:30:18.650
This stuff has been
good for discovery.

00:30:21.310 --> 00:30:25.275
And will be and continue to be.

00:30:25.275 --> 00:30:28.270
Knowing more should help
you make the big decisions

00:30:28.270 --> 00:30:30.080
more accurately.

00:30:30.080 --> 00:30:34.740
But it's not clear
that it's really going

00:30:34.740 --> 00:30:38.610
to help with the big stuff.

00:30:38.610 --> 00:30:40.630
I mean, look at
technology companies.

00:30:40.630 --> 00:30:41.800
What's the message?

00:30:41.800 --> 00:30:44.430
You get the big stuff
right, everything

00:30:44.430 --> 00:30:46.330
takes care of itself.

00:30:46.330 --> 00:30:48.100
That's a platform sermon.

00:30:48.100 --> 00:30:50.374
Nobody cares about margin.

00:30:50.374 --> 00:30:52.540
I remember having this
conversation with Gates once.

00:30:52.540 --> 00:30:54.373
He said, when it gets
down to a few margins,

00:30:54.373 --> 00:30:56.160
I'm out of this business.

00:30:56.160 --> 00:30:57.050
He's gone now.

00:31:01.090 --> 00:31:04.725
For the people who
are creating this,

00:31:04.725 --> 00:31:10.380
the decision lesson is that you
make that big bet, you get it.

00:31:10.380 --> 00:31:12.944
In theory, look at
venture capital.

00:31:12.944 --> 00:31:14.360
Shouldn't a good
prediction market

00:31:14.360 --> 00:31:17.370
be able to do better
than these guys?

00:31:17.370 --> 00:31:20.340
Tell that to any
venture capitalist.

00:31:20.340 --> 00:31:21.914
Because what do they plug in?

00:31:21.914 --> 00:31:22.580
Who's your team?

00:31:22.580 --> 00:31:24.360
What's your idea?

00:31:24.360 --> 00:31:27.320
Basic business plan.

00:31:27.320 --> 00:31:28.320
But we're not there yet.

00:31:31.627 --> 00:31:33.960
AUDIENCE: You mentioned that
being a privacy survivalist

00:31:33.960 --> 00:31:35.996
today is near impossible.

00:31:35.996 --> 00:31:37.870
And you didn't seem too
concerned about that.

00:31:37.870 --> 00:31:41.300
So I'm wondering, do you
think anonymity or the ability

00:31:41.300 --> 00:31:43.340
to be anonymous is
something that is really

00:31:43.340 --> 00:31:44.790
valued in our society today?

00:31:44.790 --> 00:31:47.826
Or is it something just on
the fringes, and should it be?

00:31:47.826 --> 00:31:49.200
Is it just the
privacy advocates,

00:31:49.200 --> 00:31:52.460
or is this something that
people should care more about?

00:31:52.460 --> 00:31:54.910
And what do you think the
impact of the loss of anonymity

00:31:54.910 --> 00:31:56.710
will be in the years to come?

00:32:00.474 --> 00:32:01.890
STEVE LOHR: A few
questions there.

00:32:07.110 --> 00:32:10.649
The evidence would
suggest that people don't

00:32:10.649 --> 00:32:11.815
care that much in aggregate.

00:32:14.930 --> 00:32:19.190
The issue, though,
one of asymmetry.

00:32:19.190 --> 00:32:20.700
On one side of
that screen, you've

00:32:20.700 --> 00:32:24.520
got grandma looking for
rheumatics and remedies

00:32:24.520 --> 00:32:26.266
and gifts for her grandchild.

00:32:26.266 --> 00:32:27.640
On the other side
of that screen,

00:32:27.640 --> 00:32:29.740
you've got a PhD quant
from Google or someplace

00:32:29.740 --> 00:32:32.950
trying to maximize
his lifetime value.

00:32:32.950 --> 00:32:39.240
And that's just
fundamentally unfair.

00:32:39.240 --> 00:32:41.520
The two sides are going into
it with different agendas

00:32:41.520 --> 00:32:43.020
and a different
amount of knowledge.

00:32:43.020 --> 00:32:46.090
And I think that's concerning.

00:32:46.090 --> 00:32:56.456
But I think most people are
going to see value in this.

00:32:56.456 --> 00:33:01.380
I was just up at MIT, did a
story-- it hasn't run yet--

00:33:01.380 --> 00:33:03.540
about a young man who
essentially diagnosed himself

00:33:03.540 --> 00:33:04.600
with brain cancer.

00:33:04.600 --> 00:33:08.430
He studied his data, he's
not a liar, obviously.

00:33:08.430 --> 00:33:13.240
But the broader point is
all this sharing of data,

00:33:13.240 --> 00:33:14.630
of medical data.

00:33:14.630 --> 00:33:17.620
And people at the Harvard
Genome Project and stuff,

00:33:17.620 --> 00:33:20.120
people who really are
champions of this.

00:33:20.120 --> 00:33:23.040
And he's one of those people
who has a stake in it.

00:33:23.040 --> 00:33:26.090
He's 26 years old.

00:33:26.090 --> 00:33:27.190
But he sees the benefit.

00:33:27.190 --> 00:33:32.095
And he says, look, people of
my generation live on data

00:33:32.095 --> 00:33:34.560
and we're going to want this.

00:33:34.560 --> 00:33:37.475
And we're responding
to that in a [? lot. ?]

00:33:37.475 --> 00:33:39.320
There's a thing called
Open Notes, which

00:33:39.320 --> 00:33:43.860
allows now five million people
to see what their doctors are

00:33:43.860 --> 00:33:44.920
writing about them.

00:33:44.920 --> 00:33:47.960
So the nature of
medical information

00:33:47.960 --> 00:33:53.070
is really easy to
put back together.

00:33:53.070 --> 00:33:54.720
Moreso than a lot of things.

00:33:54.720 --> 00:33:56.950
Particularly when you
get in the genome stuff.

00:33:56.950 --> 00:34:00.310
You can take name, Social
Security, none of that stuff

00:34:00.310 --> 00:34:02.910
is going to get--
so potentially,

00:34:02.910 --> 00:34:05.810
I think it's a big problem.

00:34:20.754 --> 00:34:23.920
I was a victim of identity
theft last time I did this.

00:34:26.452 --> 00:34:27.940
This was 10 years ago.

00:34:27.940 --> 00:34:31.409
And you find out
there's a system.

00:34:31.409 --> 00:34:35.250
And first they charge
the $4,500 Cartier watch

00:34:35.250 --> 00:34:38.790
to the "Times" credit card
which I'd forgotten I had.

00:34:38.790 --> 00:34:40.460
And then you cancel
everything else,

00:34:40.460 --> 00:34:47.409
but then eight weeks later you
get 10 store credit card bills

00:34:47.409 --> 00:34:51.120
with charges to your account.

00:34:51.120 --> 00:34:53.989
And then six months later,
Morgan Phillip's office

00:34:53.989 --> 00:34:54.489
calls you.

00:34:59.530 --> 00:35:04.360
And because this would have
been before 9/11, at that time

00:35:04.360 --> 00:35:08.740
I'm not sure about
the driver's license,

00:35:08.740 --> 00:35:11.470
but the medical card had
my Social Security number.

00:35:11.470 --> 00:35:13.670
You're baked if
anybody wants you.

00:35:17.160 --> 00:35:20.310
And changing bank accounts
doesn't really help.

00:35:20.310 --> 00:35:23.692
But it was a system of crimes.

00:35:23.692 --> 00:35:24.900
There's a guy who dropped it.

00:35:24.900 --> 00:35:28.570
Then the people
dropped it off and made

00:35:28.570 --> 00:35:29.676
the false identification.

00:35:29.676 --> 00:35:31.800
And then there's the guy
who was the harvester, who

00:35:31.800 --> 00:35:34.620
would be the equivalent
of a website now.

00:35:34.620 --> 00:35:37.250
And what you realized
is this whole thing

00:35:37.250 --> 00:35:39.060
about the bear and
the track shoes.

00:35:39.060 --> 00:35:40.210
Two guys running and stuff.

00:35:40.210 --> 00:35:44.304
And one guy's like, you
can't outrun the bear.

00:35:44.304 --> 00:35:46.720
No, I don't have to outrun the
bear, I have to outrun you.

00:35:50.000 --> 00:35:52.370
I feel that way about
myself, personally.

00:35:52.370 --> 00:35:55.600
I mean, I'm nobody.

00:35:55.600 --> 00:35:58.520
Why would anybody
be interested in me?

00:36:01.970 --> 00:36:05.000
I guess it's just a long
winded answer, again,

00:36:05.000 --> 00:36:14.390
to say real anonymity
is a tough thing.

00:36:14.390 --> 00:36:21.240
And I'm not sure I want it.

00:36:21.240 --> 00:36:25.099
Except for certain people, and
then it's a premium product.

00:36:25.099 --> 00:36:26.265
There should be marketplace.

00:36:30.750 --> 00:36:33.060
STU: --side of the
room, there is actually

00:36:33.060 --> 00:36:36.460
a nice table back there
where books are being sold.

00:36:36.460 --> 00:36:38.600
You may not be able
to see them by angle,

00:36:38.600 --> 00:36:39.630
the pile is shrinking.

00:36:39.630 --> 00:36:41.610
So you may decide
to go over there.

00:36:41.610 --> 00:36:43.510
Please.

00:36:43.510 --> 00:36:46.500
This was a paid
commercial announced.

00:36:46.500 --> 00:36:48.590
AUDIENCE: Thank you.

00:36:48.590 --> 00:36:51.310
I see that there's
a lot more data,

00:36:51.310 --> 00:36:55.580
but I'm not sure that
there's an education system

00:36:55.580 --> 00:36:58.090
to support that
understanding of data.

00:36:58.090 --> 00:37:00.270
So I know that
data is expanding,

00:37:00.270 --> 00:37:02.490
but looking at the
curriculum of my brother

00:37:02.490 --> 00:37:04.490
who is in high school
and seeing the curriculum

00:37:04.490 --> 00:37:07.297
of my previous
college, it doesn't

00:37:07.297 --> 00:37:08.880
seem like there's
more people enrolled

00:37:08.880 --> 00:37:11.380
in trying to understand
this sort of data.

00:37:11.380 --> 00:37:13.970
So my observation
is that there's

00:37:13.970 --> 00:37:18.740
sort of a danger in this gap
because sometimes in my role,

00:37:18.740 --> 00:37:20.330
I'll present a data point.

00:37:20.330 --> 00:37:23.450
And there's no questions about
where that data came from.

00:37:23.450 --> 00:37:26.810
And if you talk about
a certain significance,

00:37:26.810 --> 00:37:29.610
sometimes there's blank stares
depending on which audience.

00:37:29.610 --> 00:37:31.900
So I can imagine if
you're telling a guy,

00:37:31.900 --> 00:37:34.420
you have X percentage chance
of getting a certain disease,

00:37:34.420 --> 00:37:35.920
and he doesn't know
what that means,

00:37:35.920 --> 00:37:39.060
he could think he
could die tomorrow.

00:37:39.060 --> 00:37:41.120
So I'm just wondering
if you think

00:37:41.120 --> 00:37:43.850
my observation is accurate.

00:37:43.850 --> 00:37:48.140
And two, within the
data community--

00:37:48.140 --> 00:37:50.160
and even within government
and the education

00:37:50.160 --> 00:37:52.326
community-- if this is
something that is recognized.

00:37:55.710 --> 00:37:57.450
STEVE LOHR: Data
institutes, as they say,

00:37:57.450 --> 00:37:59.120
are cropping up
all over the place.

00:37:59.120 --> 00:38:01.460
And this is how
universities experiment

00:38:01.460 --> 00:38:04.290
with new things without starting
a department, obviously.

00:38:04.290 --> 00:38:08.340
So, Columbia's got one.

00:38:08.340 --> 00:38:12.740
All sorts of places
are doing this.

00:38:17.660 --> 00:38:20.655
A bunch of foundations are
lined up to push this one.

00:38:20.655 --> 00:38:26.090
A lot of it has to do with,
it depends on what level

00:38:26.090 --> 00:38:27.700
you're talking about.

00:38:27.700 --> 00:38:31.500
General population
knowledge or experts.

00:38:31.500 --> 00:38:34.032
McKenzie did this study
two or three years ago

00:38:34.032 --> 00:38:35.990
about what we need from
a workforce standpoint.

00:38:35.990 --> 00:38:37.364
And I think, off
the top my head,

00:38:37.364 --> 00:38:44.920
it was 140,000 to 190,000
really literate data quants.

00:38:44.920 --> 00:38:49.620
But would you really need is
the 1.5 million managers who

00:38:49.620 --> 00:38:51.219
are savvy about
data to kind of let

00:38:51.219 --> 00:38:52.385
it spread through the world.

00:38:57.150 --> 00:39:04.060
I just look at what's happened
to journalism in my lifetime

00:39:04.060 --> 00:39:06.530
in terms of, it
used to be economics

00:39:06.530 --> 00:39:09.277
was something you couldn't do.

00:39:09.277 --> 00:39:11.110
I was interested in it,
so I said all right.

00:39:13.890 --> 00:39:15.690
But it was all anecdote.

00:39:19.340 --> 00:39:22.390
And you know the old cliche,
once is a coincidence,

00:39:22.390 --> 00:39:23.520
twice it's a trend.

00:39:23.520 --> 00:39:26.190
That's changed dramatically.

00:39:26.190 --> 00:39:29.660
And you can already digress,
because you can really

00:39:29.660 --> 00:39:32.430
take a shot at it from a
statistical standpoint.

00:39:32.430 --> 00:39:36.130
I mean, Upshot,
Nate Silver, Vox.

00:39:36.130 --> 00:39:38.340
I think we should have
a manifesto that says,

00:39:38.340 --> 00:39:42.800
this is stuff that boldly
goes between case study

00:39:42.800 --> 00:39:46.410
journalism and real
statistical significance.

00:39:46.410 --> 00:39:49.220
We run a lot more stuff
that, you now, this

00:39:49.220 --> 00:39:54.640
is based on a longitudinal
study of 47 men in Denmark.

00:39:54.640 --> 00:40:01.069
And we did self reported
studies of 147 people

00:40:01.069 --> 00:40:03.110
who are unemployed and
how they spent their time.

00:40:03.110 --> 00:40:05.610
It's really interesting, a huge
amount of interesting stuff.

00:40:14.862 --> 00:40:17.070
Stu would probably know more
about this than I would.

00:40:17.070 --> 00:40:19.470
But my sense is
that in universities

00:40:19.470 --> 00:40:26.050
and-- I don't know about high
schools-- but more of this

00:40:26.050 --> 00:40:31.820
is part of the language.

00:40:31.820 --> 00:40:37.240
And the difference between
correlation and causation.

00:40:37.240 --> 00:40:39.620
I mean, it's just more
of, "for example."

00:40:39.620 --> 00:40:41.860
It's just something that
people think more about.

00:40:41.860 --> 00:40:42.860
Stu, you probably know.

00:40:42.860 --> 00:40:46.360
STU: Well, I'll just ask, how
many people here know people

00:40:46.360 --> 00:40:49.260
in their families or friends
who complained bitterly

00:40:49.260 --> 00:40:52.700
when last night it said there
was a 70% chance of rain

00:40:52.700 --> 00:40:54.470
tomorrow, and it doesn't rain?

00:40:54.470 --> 00:40:57.840
So obviously, they don't know
what they're talking about.

00:40:57.840 --> 00:41:00.410
And you say, what do
you think 70% means?

00:41:04.160 --> 00:41:05.590
And everybody in
this room grew up

00:41:05.590 --> 00:41:08.845
with that custom of
data interpretation.

00:41:08.845 --> 00:41:12.530
So, I'm curious what you think
about the educability given

00:41:12.530 --> 00:41:17.070
these behavioral
economics studies

00:41:17.070 --> 00:41:21.010
that suggest people don't
really understand probability.

00:41:21.010 --> 00:41:23.620
AUDIENCE: I'm not so much
concerned about the pushers.

00:41:23.620 --> 00:41:26.590
I know these institutions
exists and the curriculum

00:41:26.590 --> 00:41:29.160
is there for people who are
very interested in data.

00:41:29.160 --> 00:41:30.760
And you can be very good at it.

00:41:30.760 --> 00:41:33.930
But that's still a very
niche part of the population.

00:41:33.930 --> 00:41:35.680
It's the receivers
that I'm worried about.

00:41:35.680 --> 00:41:41.260
Because privacy is
a concern, but also

00:41:41.260 --> 00:41:42.970
misuse of this power of data.

00:41:42.970 --> 00:41:44.470
So, you can tell
someone a number,

00:41:44.470 --> 00:41:45.820
they don't know what
that number means,

00:41:45.820 --> 00:41:48.070
and then you can really
manipulate people in that way.

00:41:48.070 --> 00:41:51.110
And there's really no
protection against that.

00:41:51.110 --> 00:41:53.590
STU: May I come into an old
classic book, just by the way,

00:41:53.590 --> 00:41:54.390
before we go on.

00:41:54.390 --> 00:41:56.640
It's entitled, "How to
Lie With Statistics."

00:41:56.640 --> 00:41:57.820
AUDIENCE: Exactly.

00:41:57.820 --> 00:41:59.430
STU: It's a wonderful book.

00:41:59.430 --> 00:42:01.140
And I think about
it every time that I

00:42:01.140 --> 00:42:04.646
see certain newspapers
do interesting graphs.

00:42:04.646 --> 00:42:06.395
The other newspapers
don't even do graphs.

00:42:08.662 --> 00:42:11.120
STEVE LOHR: None of us have
answered your question, really.

00:42:13.890 --> 00:42:16.110
It sort of depends on
what you mean by people.

00:42:16.110 --> 00:42:20.640
I mean, if you're really looking
at the population as a whole,

00:42:20.640 --> 00:42:22.160
popular tastes
and stuff, I think

00:42:22.160 --> 00:42:27.240
you're talking about people
who influence things.

00:42:27.240 --> 00:42:28.260
Who can shape things.

00:42:33.050 --> 00:42:35.970
I see that in
education generally.

00:42:35.970 --> 00:42:40.620
The analogy I use in the book is
Dartmouth in the '60s teaching

00:42:40.620 --> 00:42:41.340
basic.

00:42:41.340 --> 00:42:45.515
The whole idea was this was
C.P. Snow's two cultures.

00:42:45.515 --> 00:42:47.640
People who understood the
technology and the people

00:42:47.640 --> 00:42:48.610
ruled by it.

00:42:48.610 --> 00:42:51.450
And therefore, the whole
idea that John Kemeny and Tom

00:42:51.450 --> 00:42:56.860
Kurtz for the rationale for
starting up basic at Dartmouth

00:42:56.860 --> 00:43:03.300
was that potentially future
leaders-- flattering themselves

00:43:03.300 --> 00:43:09.930
or not-- should be
exposed to this, by God.

00:43:09.930 --> 00:43:13.470
Brian Kernighan of C++.

00:43:13.470 --> 00:43:14.904
STU: Over there, I think.

00:43:14.904 --> 00:43:16.070
You'll see Brian over there.

00:43:16.070 --> 00:43:16.986
STEVE LOHR: Oh really?

00:43:16.986 --> 00:43:19.530
He's going to make the little
environments program up

00:43:19.530 --> 00:43:21.890
in Princeton.

00:43:21.890 --> 00:43:25.620
He usually teaches
this course that

00:43:25.620 --> 00:43:31.360
shows these basics of computing
to undergraduates who are not

00:43:31.360 --> 00:43:32.360
computer science majors.

00:43:32.360 --> 00:43:39.430
And I just I think we can
get more of it over time.

00:43:39.430 --> 00:43:40.440
It's almost inescapable.

00:43:43.550 --> 00:43:49.110
The regulatory issue
is sort of another one.

00:43:49.110 --> 00:43:53.510
I think we're seeing all
kinds popular science.

00:43:53.510 --> 00:43:56.700
Journalism's part of
it, as are other things.

00:43:56.700 --> 00:43:59.160
More of this is taking hold.

00:43:59.160 --> 00:44:00.930
AUDIENCE: Thank you.

00:44:00.930 --> 00:44:03.740
STU: Last one please.

00:44:03.740 --> 00:44:06.470
AUDIENCE: So I think over the
last, maybe the 20th century,

00:44:06.470 --> 00:44:09.720
you had graphs that could
graph the way that subcultures

00:44:09.720 --> 00:44:10.760
tend to develop.

00:44:10.760 --> 00:44:13.402
They start out as a
very niche market,

00:44:13.402 --> 00:44:15.360
and then a group of people
surrounds around it.

00:44:15.360 --> 00:44:17.637
And eventually, the
successful ones become hip,

00:44:17.637 --> 00:44:19.220
and they become in
mainstream culture.

00:44:19.220 --> 00:44:22.240
And then eventually,
they become monetized.

00:44:22.240 --> 00:44:24.550
I think today that graph
is collapsing a lot

00:44:24.550 --> 00:44:27.466
and that cycle is
happening a lot faster.

00:44:27.466 --> 00:44:29.090
I was wondering if
you thought that had

00:44:29.090 --> 00:44:31.570
any dangers for
homogenizing our culture

00:44:31.570 --> 00:44:35.430
and making people more likely
to not branch out and explore

00:44:35.430 --> 00:44:37.760
smaller subcultures, and
maybe limiting their ability

00:44:37.760 --> 00:44:38.468
to be successful.

00:44:40.322 --> 00:44:41.530
STEVE LOHR: Sure potentially.

00:44:44.680 --> 00:44:45.690
I believe in free will.

00:44:52.030 --> 00:44:55.010
Nobody's telling you you
can have the machine on.

00:44:55.010 --> 00:44:58.830
A lot of this is being an adult
with individual responsibility.

00:45:09.950 --> 00:45:12.740
An FTC commissioner said to
me, never quote me on this,

00:45:12.740 --> 00:45:14.680
you can't protect people
from every dumb thing

00:45:14.680 --> 00:45:15.513
they're going to do.

00:45:15.513 --> 00:45:24.360
And the shaping power-- whether
it's media or technology--

00:45:24.360 --> 00:45:25.120
I hear you.

00:45:25.120 --> 00:45:27.280
And it's absolutely
true, in a sense,

00:45:27.280 --> 00:45:31.860
but the way we deal with
this as individuals,

00:45:31.860 --> 00:45:34.740
I think that's part of what
being an informed adults

00:45:34.740 --> 00:45:37.320
is about.

00:45:37.320 --> 00:45:38.780
Maybe I'm wrong.

00:45:38.780 --> 00:45:40.910
STU: It's a good point.

00:45:40.910 --> 00:45:42.430
Thank you very much, Steve.

00:45:42.430 --> 00:45:45.140
It's always a pleasure reading
you, and even more a pleasure

00:45:45.140 --> 00:45:45.740
seeing you.

00:45:45.740 --> 00:45:49.090
[APPLAUSE]

