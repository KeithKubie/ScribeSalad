WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.000
There was a very interesting and wide-reaching question on the forums

00:00:05.000 --> 00:00:09.000
about the uses of lexical analysis and regular expressions

00:00:09.000 --> 00:00:14.000
and this general sort of string processing outside of what we're doing in this class.

00:00:14.000 --> 00:00:19.000
The essence of the question was, what else is this good for in the real world?

00:00:19.000 --> 00:00:21.000
Is it used to do anything interesting?

00:00:21.000 --> 00:00:24.000
And the question is actually a resounding yes.

00:00:24.000 --> 00:00:30.000
There are a large number of great uses of regular expressions and lexical analysis

00:00:30.000 --> 00:00:34.000
beyond just building web browsers or writing language interpreters.

00:00:34.000 --> 00:00:37.000
One of the most basic is in electronic commerce.

00:00:37.000 --> 00:00:39.000
I hinted at this in the beginning of Unit 1,

00:00:39.000 --> 00:00:43.000
but things like phone numbers and credit card numbers are recognized

00:00:43.000 --> 00:00:46.000
and processed using regular expressions--

00:00:46.000 --> 00:00:48.000
essentially using lexical analysis.

00:00:48.000 --> 00:00:53.000
Break down a big input string from the user into something that you want to deal with.

00:00:53.000 --> 00:00:56.000
For example, users often enter their phone numbers or credit card numbers

00:00:56.000 --> 00:00:59.000
with hyphens in the middle, but maybe you want to get rid of all of the hyphens

00:00:59.000 --> 00:01:05.000
so that you can just treat it as one big number and ask about it in your database.

00:01:05.000 --> 00:01:10.000
However, quite a few other things make use of lexical analysis.

00:01:10.000 --> 00:01:14.000
Let me give you one you probably haven't thought of: virus detection.

00:01:14.000 --> 00:01:18.000
These days there are a number of adversaries out there in the world of computing--

00:01:18.000 --> 00:01:21.000
people who are interested in taking over your machine remotely,

00:01:21.000 --> 00:01:23.000
perhaps for economic reasons.

00:01:23.000 --> 00:01:26.000
Maybe they'd like to take over your machine and use it to send spam

00:01:26.000 --> 00:01:30.000
or do something else malicious--delete your files, something fun like that.

00:01:30.000 --> 00:01:35.000
It turns out that the virus software you probably have running on your computer now--

00:01:35.000 --> 00:01:41.000
that is, the antivirus software, the good stuff designed to keep away the evil attackers--

00:01:41.000 --> 00:01:44.000
is based on regular expressions and lexing.

00:01:44.000 --> 00:01:47.000
In essence, in order to be relatively interesting,

00:01:47.000 --> 00:01:51.000
a virus typically has to have some sort of important payload,

00:01:51.000 --> 00:01:55.000
a part where it replicates and takes over another program,

00:01:55.000 --> 00:01:59.000
writing out something to the disk or changing data structures in memory.

00:01:59.000 --> 00:02:04.000
A virus definition file, which you may have seen your antivirus software talk about updating,

00:02:04.000 --> 00:02:08.000
is actually just a big list of tokens.

00:02:08.000 --> 00:02:11.000
For every virus out there, we write a regular expression

00:02:11.000 --> 00:02:14.000
corresponding to the machine code,

00:02:14.000 --> 00:02:20.000
corresponding to this payload that it uses to take over or infect other files.

00:02:20.000 --> 00:02:22.000
And a virus scanner is actually just like a lexer

00:02:22.000 --> 00:02:26.000
except that if you match any of those tokens, it's a bad scene.

00:02:26.000 --> 00:02:31.000
A virus scanner runs all of those finite state machines over executable programs

00:02:31.000 --> 00:02:34.000
before you run them or attachments you download from the Internet.

00:02:34.000 --> 00:02:37.000
And if they find any of those patterns, they stop and say,

00:02:37.000 --> 00:02:42.000
"Oh, I think this file may be malicious. Let's do something special with it."

00:02:42.000 --> 00:02:47.000
So virus scanning or virus checking also uses regular expressions.

00:02:47.000 --> 00:02:49.000
Here's a third use.

00:02:49.000 --> 00:02:55.000
String matching in general is commonly used for dealing with computational biology

00:02:55.000 --> 00:02:59.000
like DNA sequencing or protein folding.

00:02:59.000 --> 00:03:02.000
When we get pieces of DNA from biological laboratories,

00:03:02.000 --> 00:03:05.000
you can view human DNA or animal DNA in the world

00:03:05.000 --> 00:03:11.000
as basically just a long string over a very simple alphabet of 4 characters--G, C, A, T--

00:03:11.000 --> 00:03:15.000
corresponding to special objects or special structures

00:03:15.000 --> 00:03:18.000
in the world of biology and biochemistry.

00:03:18.000 --> 00:03:21.000
And everything about us, we believe, in the physical world

00:03:21.000 --> 00:03:27.000
is based on the interactions or the unrolling or instructions gleaned from DNA,

00:03:27.000 --> 00:03:30.000
perhaps through DNA or RNA transcription.

00:03:30.000 --> 00:03:34.000
I'm not much of a biologist, but I know that these days a lot of biology research

00:03:34.000 --> 00:03:38.000
is done with the aid of or enabled by computers.

00:03:38.000 --> 00:03:43.000
So for example, if I want to check to see if 2 particular DNA sources

00:03:43.000 --> 00:03:47.000
have a relatively high overlap but when they come out of the biology lab

00:03:47.000 --> 00:03:51.000
these strings are a bit scrambled because they've gone through a physical process,

00:03:51.000 --> 00:03:54.000
what I essentially want to do is say, "Oh, could this blob of text over here

00:03:54.000 --> 00:03:58.000
"match up with this DNA text from some other person?"

00:03:58.000 --> 00:04:01.000
"Can I find the best matching or the best alignment?"

00:04:01.000 --> 00:04:06.000
And that task uses exactly the same sort of lexical analysis string matching

00:04:06.000 --> 00:04:08.000
that we've talked about here.

00:04:08.000 --> 00:04:10.000
But I can take it 1 step further.

00:04:10.000 --> 00:04:14.000
You might have wondered at some point, "How do we come up with new drugs?"

00:04:14.000 --> 00:04:18.000
"How do I make a better aspirin? How does that sort of thing go?"

00:04:18.000 --> 00:04:21.000
And while there are definitely in-lab scientific techniques involved--

00:04:21.000 --> 00:04:25.000
you might imagine, "Oh, maybe I take some sort of test subject,

00:04:25.000 --> 00:04:27.000
"I inject them with my candidate medicine and see if they get better"--

00:04:27.000 --> 00:04:29.000
that sort of trial is very expensive.

00:04:29.000 --> 00:04:32.000
It would be really nice if we could use mathematical models

00:04:32.000 --> 00:04:34.000
to help narrow down the search space

00:04:34.000 --> 00:04:39.000
to help get an idea for how drugs might interact in your body, how they might behave.

00:04:39.000 --> 00:04:42.000
And it turns out that a lot of chemical interactions at that level

00:04:42.000 --> 00:04:45.000
are governed by protein folding--

00:04:45.000 --> 00:04:49.000
how various proteins will sort of rub against each other and interact or not.

00:04:49.000 --> 00:04:54.000
We can use computers to simulate protein folding

00:04:54.000 --> 00:04:57.000
and thus get a better handle on candidate drugs we're designing.

00:04:57.000 --> 00:05:00.000
Maybe we can use the computers to simulate a bunch of possibilities,

00:05:00.000 --> 00:05:03.000
come up with the best 10, present those to the scientists,

00:05:03.000 --> 00:05:05.000
and then those are the ones that we try on live subjects.

00:05:05.000 --> 00:05:11.000
That sort of protein folding experiment involves a number of the same sort of techniques

00:05:11.000 --> 00:05:13.000
that we're learning about here in lexing--

00:05:13.000 --> 00:05:17.000
go over strings, do comparisons, do big simulations.

00:05:17.000 --> 00:05:21.000
And in fact, if you're curious about this sort of genome sequence alignment,

00:05:21.000 --> 00:05:23.000
string matching, protein folding sort of world,

00:05:23.000 --> 00:05:27.000
you might want to check out BLAST--B-L-A-S-T, all capital letters--

00:05:27.000 --> 00:05:31.000
which is a common software project for doing just that.

00:05:31.000 --> 00:05:35.000
It's not necessarily the fastest, but it is one of the most popular.

00:05:35.000 --> 00:05:40.000
And then the final thing I would mention is there are a lot of very high level

00:05:40.000 --> 00:05:45.000
readability metrics, both for software and also for natural language text,

00:05:45.000 --> 00:05:50.000
that are based on the sort of lexing or lexical analysis or token definition principles

00:05:50.000 --> 00:05:52.000
that we've come to know.

00:05:52.000 --> 00:05:56.000
For example, one of the least expensive ways of figuring out

00:05:56.000 --> 00:05:59.000
the appropriate grade level for a book

00:05:59.000 --> 00:06:03.000
is to measure very simple things like the number of words in a sentence

00:06:03.000 --> 00:06:06.000
and the number of syllables per word.

00:06:06.000 --> 00:06:09.000
Then through some division and multiplication you can arrive at

00:06:09.000 --> 00:06:13.000
whether that should be given to a 10-year-old child or a 14-year-old child or whatnot

00:06:13.000 --> 00:06:16.000
to read based on expected reading norms.

00:06:16.000 --> 00:06:19.000
That sort of readability metric can be easily computed

00:06:19.000 --> 00:06:23.000
using exactly the sort of break a string up into words kind of lexical analysis

00:06:23.000 --> 00:06:25.000
that we've been focusing on.

00:06:25.000 --> 00:06:28.000
And that segues into the final part of this question.

00:06:28.000 --> 00:06:32.000
Students were asking about natural language processing.

00:06:32.000 --> 00:06:36.000
With the lexing and parsing, the lexical analysis and syntactical analysis

00:06:36.000 --> 00:06:40.000
that we'll cover in this class, do they carry over to parsing real-world languages

00:06:40.000 --> 00:06:43.000
like Japanese or English or German?

00:06:43.000 --> 00:06:47.000
And the answer is, to some degree, yes.

00:06:47.000 --> 00:06:50.000
Linguists or computation linguists are often interested

00:06:50.000 --> 00:06:52.000
in modeling real-world natural languages

00:06:52.000 --> 00:06:56.000
using the same sorts of formal grammars that we're going to introduce in Unit 3.

00:06:56.000 --> 00:06:58.000
Stay tuned. Fun stuff.

00:06:58.000 --> 00:07:02.000
And this question of breaking a sentence up into words is legitimately tricky

00:07:02.000 --> 00:07:04.000
in languages other than English.

00:07:04.000 --> 00:07:08.000
I hinted at this in some of the lecture material.

00:07:08.000 --> 00:07:13.000
But in languages like Japanese or in Latin where spaces might not be written explicitly,

00:07:13.000 --> 00:07:18.000
we really have to think hard about how to break down an utterance into its substructures.

00:07:18.000 --> 00:07:23.000
However, natural language processing is actually, in some sense, significantly harder

00:07:23.000 --> 00:07:27.000
than the unnatural language processing that we do here.

00:07:27.000 --> 00:07:32.000
Real-world languages like English are not well behaved compared to JavaScript.

00:07:32.000 --> 00:07:34.000
JavaScript is very artificial, it's very regular.

00:07:34.000 --> 00:07:38.000
We know just how it will go, what the nouns are and what the verbs are.

00:07:38.000 --> 00:07:41.000
In a language like English, that's very hard to figure out.

00:07:41.000 --> 00:07:47.000
So natural language processing is currently, to some degree, still in its infancy.

00:07:47.000 --> 00:07:51.000
There are still a lot of tests or a lot of tasks that we would like to be able to carry out

00:07:51.000 --> 00:07:54.000
on natural language that are hard to do.

00:07:54.000 --> 00:07:58.000
If you're interested in natural language processing, writing computer programs

00:07:58.000 --> 00:08:01.000
that look at human written documents and do something with them,

00:08:01.000 --> 00:08:05.000
a common task to start with is document summarization.

00:08:05.000 --> 00:08:09.000
For example, you might go to a news aggregator site on the Web

00:08:09.000 --> 00:08:11.000
and there's a really long article.

00:08:11.000 --> 00:08:14.000
Wouldn't it be nice if you could say something like, "Summarize this for me."

00:08:14.000 --> 00:08:18.000
"What's the gist of this? Boil it down to just a few sentences."

00:08:18.000 --> 00:08:23.000
That's one of the common tasks that natural language processing researchers try to solve.

00:08:23.000 --> 00:08:27.000
And it turns out that that task is very difficult.

00:08:27.000 --> 00:08:30.000
Sort of a straw algorithm, a cheap way to do it

00:08:30.000 --> 00:08:34.000
might be to just take the topic sentence from each paragraph

00:08:34.000 --> 00:08:37.000
and put them together, counting on the human who wrote it

00:08:37.000 --> 00:08:39.000
to have used a special structure where each paragraph

00:08:39.000 --> 00:08:42.000
is introduced by a declarative topic sentence.

00:08:42.000 --> 00:08:47.000
It turns out that that simple approach is actually relatively hard to beat

00:08:47.000 --> 00:08:52.000
because understanding natural language requires your computer program to have,

00:08:52.000 --> 00:08:55.000
in some sense, a model of semantic meaning or the rest of the world.

00:08:55.000 --> 00:08:59.000
People will sometimes jokingly say that natural language processing

00:08:59.000 --> 00:09:03.000
is AI-complete where complete has a special mathematical meaning.

00:09:03.000 --> 00:09:08.000
We may get to deal with jokes and puns like that in a later class on theory.

00:09:08.000 --> 00:09:11.000
Suffice it to say for now, to some degree, I'm actually glad

00:09:11.000 --> 00:09:14.000
that I work in the easy world of unnatural language processing.

00:09:14.000 --> 00:09:18.000
It's significantly harder to deal with the utterances we use day to day

00:09:18.000 --> 09:59:59.000
when talking to other people.

