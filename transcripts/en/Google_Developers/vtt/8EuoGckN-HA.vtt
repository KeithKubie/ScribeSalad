WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.170
TIMOTHY JORDAN: Hi,
Timothy Jordan here, again,

00:00:02.170 --> 00:00:03.890
now with Nicole Limtiaco.

00:00:03.890 --> 00:00:06.330
She's a software engineer on
Machine Learning at Google.

00:00:06.330 --> 00:00:06.830
Hi, Nicole.

00:00:06.830 --> 00:00:07.400
NICOLE LIMTIACO: Hi.

00:00:07.400 --> 00:00:08.360
TIMOTHY JORDAN: How are you?

00:00:08.360 --> 00:00:09.020
NICOLE LIMTIACO: I'm good.

00:00:09.020 --> 00:00:09.855
Happy to be at Grace Hopper.

00:00:09.855 --> 00:00:10.750
TIMOTHY JORDAN: I am, too.

00:00:10.750 --> 00:00:11.500
It is so cool.

00:00:11.500 --> 00:00:11.830
NICOLE LIMTIACO: Yeah.

00:00:11.830 --> 00:00:13.788
TIMOTHY JORDAN: So you
work on machine learning

00:00:13.788 --> 00:00:16.985
and machine learning is huge
at Grace Hopper this year.

00:00:16.985 --> 00:00:17.860
NICOLE LIMTIACO: Yes.

00:00:17.860 --> 00:00:19.097
AI is huge in general.

00:00:19.097 --> 00:00:20.680
TIMOTHY JORDAN: AI
is huge in general.

00:00:20.680 --> 00:00:23.410
We heard Fei-Fei's keynote--

00:00:23.410 --> 00:00:24.070
day one.

00:00:24.070 --> 00:00:26.800
And that was incredibly
inspiring to hear her story

00:00:26.800 --> 00:00:29.580
and really talk about
bias in ML and how

00:00:29.580 --> 00:00:31.060
we have to pay attention to it.

00:00:31.060 --> 00:00:32.710
I found that really fascinating.

00:00:32.710 --> 00:00:34.120
What are some
themes that you see

00:00:34.120 --> 00:00:36.220
sort of repeating there
that feel kind of new.

00:00:36.220 --> 00:00:37.928
NICOLE LIMTIACO: Well,
I at least a theme

00:00:37.928 --> 00:00:39.490
that I've been
sort of tracking on

00:00:39.490 --> 00:00:42.220
is this idea of
social good in AI.

00:00:42.220 --> 00:00:45.460
So whether that's like being
socially conscious within AI

00:00:45.460 --> 00:00:48.490
technologies, or
using AI technologies

00:00:48.490 --> 00:00:50.380
to do social good in the world.

00:00:50.380 --> 00:00:51.340
It's like a huge thing.

00:00:51.340 --> 00:00:53.470
I've been to tons of
social good sessions now,

00:00:53.470 --> 00:00:55.210
and like it's just
really great to see

00:00:55.210 --> 00:01:00.317
that we're putting our skills to
something that's really useful.

00:01:00.317 --> 00:01:01.900
TIMOTHY JORDAN: What
is it that you're

00:01:01.900 --> 00:01:04.209
most excited about in
machine learning right now.

00:01:04.209 --> 00:01:06.670
NICOLE LIMTIACO: So I get
really excited about this idea

00:01:06.670 --> 00:01:08.620
of being able to
do model knowledge

00:01:08.620 --> 00:01:10.390
transfer across models.

00:01:10.390 --> 00:01:12.430
So like, can we take a model--

00:01:12.430 --> 00:01:14.480
say a language model--
that's sort of what I do,

00:01:14.480 --> 00:01:16.810
I'm a natural language
person in my work.

00:01:16.810 --> 00:01:20.230
Can we take information
that we've done in one model

00:01:20.230 --> 00:01:22.300
and try to use that
information to learn

00:01:22.300 --> 00:01:26.110
a totally different task or
a totally different domain.

00:01:26.110 --> 00:01:28.630
Or even make one
very, very large model

00:01:28.630 --> 00:01:31.660
that knows a really generic
representation of language

00:01:31.660 --> 00:01:35.255
and adapt that into a more--
into finer grained task,

00:01:35.255 --> 00:01:35.755
for example.

00:01:35.755 --> 00:01:37.171
TIMOTHY JORDAN:
That's is so cool.

00:01:37.171 --> 00:01:39.067
I think one of the
things that excites

00:01:39.067 --> 00:01:40.900
me the most about
machinery, and in general,

00:01:40.900 --> 00:01:42.400
is what we can learn
about ourselves.

00:01:42.400 --> 00:01:43.106
NICOLE LIMTIACO: Hmm.

00:01:43.106 --> 00:01:45.385
TIMOTHY JORDAN: Which is what
cognitive science is all about,

00:01:45.385 --> 00:01:45.610
right?

00:01:45.610 --> 00:01:47.990
Like building these models so
that we can learn about it.

00:01:47.990 --> 00:01:49.630
And this is one of
those things where

00:01:49.630 --> 00:01:52.630
it's like how do
we sort of visually

00:01:52.630 --> 00:01:56.654
recognize things as human, and
then something new comes in

00:01:56.654 --> 00:01:58.570
and we recognize that,
as well, without having

00:01:58.570 --> 00:01:59.200
a previous example.

00:01:59.200 --> 00:02:00.110
NICOLE LIMTIACO:
Yeah, absolutely.

00:02:00.110 --> 00:02:01.526
Like there are
tons of experiments

00:02:01.526 --> 00:02:03.370
where they show that
humans only need

00:02:03.370 --> 00:02:06.070
to be shown one example
of a word in the context,

00:02:06.070 --> 00:02:09.949
and they get this idea of what
it means, sort of, immediately.

00:02:09.949 --> 00:02:12.970
And so that would be great if we
could have our models see that,

00:02:12.970 --> 00:02:14.140
but I don't think
we're quite there yet.

00:02:14.140 --> 00:02:15.670
But we're working
really hard on it.

00:02:15.670 --> 00:02:17.350
TIMOTHY JORDAN: Thank you so
much for sharing that with us.

00:02:17.350 --> 00:02:17.950
NICOLE LIMTIACO:
Yeah, absolutely.

00:02:17.950 --> 00:02:18.490
TIMOTHY JORDAN: All right.

00:02:18.490 --> 00:02:19.650
NICOLE LIMTIACO: I
love talking about it.

00:02:19.650 --> 00:02:20.960
TIMOTHY JORDAN: Let's do that.

00:02:20.960 --> 00:02:23.170
We're going to go and geek
out about ML some more.

00:02:23.170 --> 00:02:24.770
You guys watch some more videos.

00:02:24.770 --> 00:02:26.170
All right, I'll see you later.

00:02:26.170 --> 00:02:27.961
Thanks for tuning into
one of our developer

00:02:27.961 --> 00:02:29.770
show segments from
somewhere in the world.

00:02:29.770 --> 00:02:32.150
If you'd like to catch
some of our prior episodes,

00:02:32.150 --> 00:02:33.460
check them out right over here.

00:02:33.460 --> 00:02:36.187
[MUSIC PLAYING]

