WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.060
 
in the rise of deep learning one of the

00:00:02.060 --> 00:00:02.070
in the rise of deep learning one of the
 

00:00:02.070 --> 00:00:04.010
in the rise of deep learning one of the
most important ideas has been an

00:00:04.010 --> 00:00:04.020
most important ideas has been an
 

00:00:04.020 --> 00:00:05.869
most important ideas has been an
algorithm called batch normalization

00:00:05.869 --> 00:00:05.879
algorithm called batch normalization
 

00:00:05.879 --> 00:00:08.810
algorithm called batch normalization
created by two researchers Sergey iov

00:00:08.810 --> 00:00:08.820
created by two researchers Sergey iov
 

00:00:08.820 --> 00:00:10.910
created by two researchers Sergey iov
and Christians a greedy batch

00:00:10.910 --> 00:00:10.920
and Christians a greedy batch
 

00:00:10.920 --> 00:00:12.620
and Christians a greedy batch
normalization it makes your hyper

00:00:12.620 --> 00:00:12.630
normalization it makes your hyper
 

00:00:12.630 --> 00:00:14.270
normalization it makes your hyper
parameter search probably much easier it

00:00:14.270 --> 00:00:14.280
parameter search probably much easier it
 

00:00:14.280 --> 00:00:15.740
parameter search probably much easier it
makes your neural network much more

00:00:15.740 --> 00:00:15.750
makes your neural network much more
 

00:00:15.750 --> 00:00:17.510
makes your neural network much more
robust to the choice of hyper parameters

00:00:17.510 --> 00:00:17.520
robust to the choice of hyper parameters
 

00:00:17.520 --> 00:00:19.730
robust to the choice of hyper parameters
is much bigger range of hyper parameters

00:00:19.730 --> 00:00:19.740
is much bigger range of hyper parameters
 

00:00:19.740 --> 00:00:21.769
is much bigger range of hyper parameters
that work well and they'll also enable

00:00:21.769 --> 00:00:21.779
that work well and they'll also enable
 

00:00:21.779 --> 00:00:23.900
that work well and they'll also enable
you to much more easily train even very

00:00:23.900 --> 00:00:23.910
you to much more easily train even very
 

00:00:23.910 --> 00:00:26.120
you to much more easily train even very
deep networks let's see how batch

00:00:26.120 --> 00:00:26.130
deep networks let's see how batch
 

00:00:26.130 --> 00:00:28.519
deep networks let's see how batch
normalization works when training a

00:00:28.519 --> 00:00:28.529
normalization works when training a
 

00:00:28.529 --> 00:00:31.009
normalization works when training a
model such as logistic regression you

00:00:31.009 --> 00:00:31.019
model such as logistic regression you
 

00:00:31.019 --> 00:00:34.069
model such as logistic regression you
might remember that normalizing the

00:00:34.069 --> 00:00:34.079
might remember that normalizing the
 

00:00:34.079 --> 00:00:36.470
might remember that normalizing the
input features can speed up learning in

00:00:36.470 --> 00:00:36.480
input features can speed up learning in
 

00:00:36.480 --> 00:00:39.319
input features can speed up learning in
computer means subtract off the means of

00:00:39.319 --> 00:00:39.329
computer means subtract off the means of
 

00:00:39.329 --> 00:00:43.420
computer means subtract off the means of
your training set computer variances

00:00:43.420 --> 00:00:43.430
your training set computer variances
 

00:00:43.430 --> 00:00:46.340
your training set computer variances
this sum of X I squared this is a

00:00:46.340 --> 00:00:46.350
this sum of X I squared this is a
 

00:00:46.350 --> 00:00:50.750
this sum of X I squared this is a
element wise squaring and then normalize

00:00:50.750 --> 00:00:50.760
element wise squaring and then normalize
 

00:00:50.760 --> 00:00:53.299
element wise squaring and then normalize
your data set according to the variances

00:00:53.299 --> 00:00:53.309
your data set according to the variances
 

00:00:53.309 --> 00:00:55.610
your data set according to the variances
and we form an earlier video how this

00:00:55.610 --> 00:00:55.620
and we form an earlier video how this
 

00:00:55.620 --> 00:00:57.950
and we form an earlier video how this
can turn the contours of the learning

00:00:57.950 --> 00:00:57.960
can turn the contours of the learning
 

00:00:57.960 --> 00:00:59.270
can turn the contours of the learning
problem from something that might be

00:00:59.270 --> 00:00:59.280
problem from something that might be
 

00:00:59.280 --> 00:01:01.790
problem from something that might be
very elongated you know something that

00:01:01.790 --> 00:01:01.800
very elongated you know something that
 

00:01:01.800 --> 00:01:04.399
very elongated you know something that
is more round and easier for an

00:01:04.399 --> 00:01:04.409
is more round and easier for an
 

00:01:04.409 --> 00:01:06.050
is more round and easier for an
algorithm like gradient descent to

00:01:06.050 --> 00:01:06.060
algorithm like gradient descent to
 

00:01:06.060 --> 00:01:09.410
algorithm like gradient descent to
optimize so this works in terms of

00:01:09.410 --> 00:01:09.420
optimize so this works in terms of
 

00:01:09.420 --> 00:01:12.260
optimize so this works in terms of
normalizing the input feature values to

00:01:12.260 --> 00:01:12.270
normalizing the input feature values to
 

00:01:12.270 --> 00:01:15.050
normalizing the input feature values to
a new network or to logistic regression

00:01:15.050 --> 00:01:15.060
a new network or to logistic regression
 

00:01:15.060 --> 00:01:18.050
a new network or to logistic regression
now how about a deeper model you have

00:01:18.050 --> 00:01:18.060
now how about a deeper model you have
 

00:01:18.060 --> 00:01:20.240
now how about a deeper model you have
not just input features X but in this

00:01:20.240 --> 00:01:20.250
not just input features X but in this
 

00:01:20.250 --> 00:01:23.630
not just input features X but in this
layer you have activations a one in this

00:01:23.630 --> 00:01:23.640
layer you have activations a one in this
 

00:01:23.640 --> 00:01:26.270
layer you have activations a one in this
layer you have activations a two and so

00:01:26.270 --> 00:01:26.280
layer you have activations a two and so
 

00:01:26.280 --> 00:01:28.850
layer you have activations a two and so
on so if you want to train the

00:01:28.850 --> 00:01:28.860
on so if you want to train the
 

00:01:28.860 --> 00:01:34.789
on so if you want to train the
parameters say W 3 B 3 then one would be

00:01:34.789 --> 00:01:34.799
parameters say W 3 B 3 then one would be
 

00:01:34.799 --> 00:01:36.800
parameters say W 3 B 3 then one would be
nice if you can normalize the mean and

00:01:36.800 --> 00:01:36.810
nice if you can normalize the mean and
 

00:01:36.810 --> 00:01:39.740
nice if you can normalize the mean and
variance of a two to make the training

00:01:39.740 --> 00:01:39.750
variance of a two to make the training
 

00:01:39.750 --> 00:01:43.700
variance of a two to make the training
of W 3 B 3 more efficient in the case of

00:01:43.700 --> 00:01:43.710
of W 3 B 3 more efficient in the case of
 

00:01:43.710 --> 00:01:45.760
of W 3 B 3 more efficient in the case of
logistic regression we saw how

00:01:45.760 --> 00:01:45.770
logistic regression we saw how
 

00:01:45.770 --> 00:01:48.679
logistic regression we saw how
normalizing x1 x2 x3 maybe helps you

00:01:48.679 --> 00:01:48.689
normalizing x1 x2 x3 maybe helps you
 

00:01:48.689 --> 00:01:51.980
normalizing x1 x2 x3 maybe helps you
train W and B more efficiently so here

00:01:51.980 --> 00:01:51.990
train W and B more efficiently so here
 

00:01:51.990 --> 00:01:54.889
train W and B more efficiently so here
the question is for any hidden layer can

00:01:54.889 --> 00:01:54.899
the question is for any hidden layer can
 

00:01:54.899 --> 00:02:00.709
the question is for any hidden layer can
we normalize the values of a let's say a

00:02:00.709 --> 00:02:00.719
we normalize the values of a let's say a
 

00:02:00.719 --> 00:02:02.929
we normalize the values of a let's say a
2 in this example but really anything

00:02:02.929 --> 00:02:02.939
2 in this example but really anything
 

00:02:02.939 --> 00:02:08.690
2 in this example but really anything
later so as to train W

00:02:08.690 --> 00:02:08.700
later so as to train W
 

00:02:08.700 --> 00:02:14.009
later so as to train W
bb3 faster right since a2 is the input

00:02:14.009 --> 00:02:14.019
bb3 faster right since a2 is the input
 

00:02:14.019 --> 00:02:17.190
bb3 faster right since a2 is the input
to the next layer that therefore affects

00:02:17.190 --> 00:02:17.200
to the next layer that therefore affects
 

00:02:17.200 --> 00:02:20.880
to the next layer that therefore affects
the training of w3 and b3 so this is

00:02:20.880 --> 00:02:20.890
the training of w3 and b3 so this is
 

00:02:20.890 --> 00:02:23.640
the training of w3 and b3 so this is
what passional does batch normalization

00:02:23.640 --> 00:02:23.650
what passional does batch normalization
 

00:02:23.650 --> 00:02:25.830
what passional does batch normalization
or national for short does although

00:02:25.830 --> 00:02:25.840
or national for short does although
 

00:02:25.840 --> 00:02:28.710
or national for short does although
technically will actually normalize the

00:02:28.710 --> 00:02:28.720
technically will actually normalize the
 

00:02:28.720 --> 00:02:32.910
technically will actually normalize the
values of not a two but z 2 there is

00:02:32.910 --> 00:02:32.920
values of not a two but z 2 there is
 

00:02:32.920 --> 00:02:34.500
values of not a two but z 2 there is
some debate in the deep learning

00:02:34.500 --> 00:02:34.510
some debate in the deep learning
 

00:02:34.510 --> 00:02:35.960
some debate in the deep learning
literature about whether you should

00:02:35.960 --> 00:02:35.970
literature about whether you should
 

00:02:35.970 --> 00:02:38.009
literature about whether you should
normalize the value before the

00:02:38.009 --> 00:02:38.019
normalize the value before the
 

00:02:38.019 --> 00:02:40.710
normalize the value before the
activation functions on v2 or whether

00:02:40.710 --> 00:02:40.720
activation functions on v2 or whether
 

00:02:40.720 --> 00:02:42.089
activation functions on v2 or whether
they should normalize the value after

00:02:42.089 --> 00:02:42.099
they should normalize the value after
 

00:02:42.099 --> 00:02:44.910
they should normalize the value after
applying the activation function a to in

00:02:44.910 --> 00:02:44.920
applying the activation function a to in
 

00:02:44.920 --> 00:02:48.030
applying the activation function a to in
practice normalizing z2 is done much

00:02:48.030 --> 00:02:48.040
practice normalizing z2 is done much
 

00:02:48.040 --> 00:02:50.940
practice normalizing z2 is done much
more often so that's the version I

00:02:50.940 --> 00:02:50.950
more often so that's the version I
 

00:02:50.950 --> 00:02:52.710
more often so that's the version I
present and what I would recommend you

00:02:52.710 --> 00:02:52.720
present and what I would recommend you
 

00:02:52.720 --> 00:02:56.160
present and what I would recommend you
use as a default choice so here is how

00:02:56.160 --> 00:02:56.170
use as a default choice so here is how
 

00:02:56.170 --> 00:02:58.860
use as a default choice so here is how
you would implement batch norm given

00:02:58.860 --> 00:02:58.870
you would implement batch norm given
 

00:02:58.870 --> 00:03:05.640
you would implement batch norm given
some intermediate values in your neural

00:03:05.640 --> 00:03:05.650
some intermediate values in your neural
 

00:03:05.650 --> 00:03:12.539
some intermediate values in your neural
net let's say that you have some hidden

00:03:12.539 --> 00:03:12.549
net let's say that you have some hidden
 

00:03:12.549 --> 00:03:19.289
net let's say that you have some hidden
unit values z 1 up to ZM and this is

00:03:19.289 --> 00:03:19.299
unit values z 1 up to ZM and this is
 

00:03:19.299 --> 00:03:22.770
unit values z 1 up to ZM and this is
really from some hidden layer so it be

00:03:22.770 --> 00:03:22.780
really from some hidden layer so it be
 

00:03:22.780 --> 00:03:25.409
really from some hidden layer so it be
more accurate dividers as if the some

00:03:25.409 --> 00:03:25.419
more accurate dividers as if the some
 

00:03:25.419 --> 00:03:28.080
more accurate dividers as if the some
hidden layer I for I equals 1 through m

00:03:28.080 --> 00:03:28.090
hidden layer I for I equals 1 through m
 

00:03:28.090 --> 00:03:30.629
hidden layer I for I equals 1 through m
but to videos writing I'm going to omit

00:03:30.629 --> 00:03:30.639
but to videos writing I'm going to omit
 

00:03:30.639 --> 00:03:33.659
but to videos writing I'm going to omit
this square bracket L just to simplify

00:03:33.659 --> 00:03:33.669
this square bracket L just to simplify
 

00:03:33.669 --> 00:03:35.879
this square bracket L just to simplify
the notation on this line so giving

00:03:35.879 --> 00:03:35.889
the notation on this line so giving
 

00:03:35.889 --> 00:03:37.740
the notation on this line so giving
these values what you do is compute the

00:03:37.740 --> 00:03:37.750
these values what you do is compute the
 

00:03:37.750 --> 00:03:42.150
these values what you do is compute the
mean as follows again all this is

00:03:42.150 --> 00:03:42.160
mean as follows again all this is
 

00:03:42.160 --> 00:03:44.280
mean as follows again all this is
specific to some layer l but I'm waiting

00:03:44.280 --> 00:03:44.290
specific to some layer l but I'm waiting
 

00:03:44.290 --> 00:03:47.009
specific to some layer l but I'm waiting
the square bracket L m and then you

00:03:47.009 --> 00:03:47.019
the square bracket L m and then you
 

00:03:47.019 --> 00:03:50.490
the square bracket L m and then you
compute the variance using the pretty

00:03:50.490 --> 00:03:50.500
compute the variance using the pretty
 

00:03:50.500 --> 00:03:52.199
compute the variance using the pretty
much the formula you would expect and

00:03:52.199 --> 00:03:52.209
much the formula you would expect and
 

00:03:52.209 --> 00:03:54.000
much the formula you would expect and
then you will take each of the i's and

00:03:54.000 --> 00:03:54.010
then you will take each of the i's and
 

00:03:54.010 --> 00:03:58.550
then you will take each of the i's and
normalize it to get zi normalized by

00:03:58.550 --> 00:03:58.560
normalize it to get zi normalized by
 

00:03:58.560 --> 00:04:01.229
normalize it to get zi normalized by
subtracting off the mean and dividing by

00:04:01.229 --> 00:04:01.239
subtracting off the mean and dividing by
 

00:04:01.239 --> 00:04:06.089
subtracting off the mean and dividing by
the standard deviation um for numerical

00:04:06.089 --> 00:04:06.099
the standard deviation um for numerical
 

00:04:06.099 --> 00:04:08.939
the standard deviation um for numerical
stability we usually add epsilon to

00:04:08.939 --> 00:04:08.949
stability we usually add epsilon to
 

00:04:08.949 --> 00:04:11.369
stability we usually add epsilon to
denominator like that just in case Sigma

00:04:11.369 --> 00:04:11.379
denominator like that just in case Sigma
 

00:04:11.379 --> 00:04:13.289
denominator like that just in case Sigma
squared turns out to be 0 and some

00:04:13.289 --> 00:04:13.299
squared turns out to be 0 and some
 

00:04:13.299 --> 00:04:13.880
squared turns out to be 0 and some
estimate

00:04:13.880 --> 00:04:13.890
estimate
 

00:04:13.890 --> 00:04:17.180
estimate
and so now we're taking these values E

00:04:17.180 --> 00:04:17.190
and so now we're taking these values E
 

00:04:17.190 --> 00:04:20.449
and so now we're taking these values E
and normalize them to have mean 0 and

00:04:20.449 --> 00:04:20.459
and normalize them to have mean 0 and
 

00:04:20.459 --> 00:04:23.360
and normalize them to have mean 0 and
standard unit variance so every

00:04:23.360 --> 00:04:23.370
standard unit variance so every
 

00:04:23.370 --> 00:04:26.180
standard unit variance so every
component of Z has mean 0 and variance 1

00:04:26.180 --> 00:04:26.190
component of Z has mean 0 and variance 1
 

00:04:26.190 --> 00:04:28.520
component of Z has mean 0 and variance 1
but we don't want the hidden units to

00:04:28.520 --> 00:04:28.530
but we don't want the hidden units to
 

00:04:28.530 --> 00:04:31.610
but we don't want the hidden units to
always have mean 0 and variance 1 maybe

00:04:31.610 --> 00:04:31.620
always have mean 0 and variance 1 maybe
 

00:04:31.620 --> 00:04:33.770
always have mean 0 and variance 1 maybe
it make sense but hidden units to have a

00:04:33.770 --> 00:04:33.780
it make sense but hidden units to have a
 

00:04:33.780 --> 00:04:35.720
it make sense but hidden units to have a
different distribution so what we'll do

00:04:35.720 --> 00:04:35.730
different distribution so what we'll do
 

00:04:35.730 --> 00:04:39.520
different distribution so what we'll do
instead is compute the call to Z tilde

00:04:39.520 --> 00:04:39.530
instead is compute the call to Z tilde
 

00:04:39.530 --> 00:04:48.830
instead is compute the call to Z tilde
equals gamma Z I known plus beta and

00:04:48.830 --> 00:04:48.840
equals gamma Z I known plus beta and
 

00:04:48.840 --> 00:04:53.780
equals gamma Z I known plus beta and
here gamma and beta are learn about

00:04:53.780 --> 00:04:53.790
here gamma and beta are learn about
 

00:04:53.790 --> 00:04:59.570
here gamma and beta are learn about
parameters of your model so they're

00:04:59.570 --> 00:04:59.580
parameters of your model so they're
 

00:04:59.580 --> 00:05:01.610
parameters of your model so they're
using gradient descents or some other

00:05:01.610 --> 00:05:01.620
using gradient descents or some other
 

00:05:01.620 --> 00:05:03.409
using gradient descents or some other
algorithm like the gradient descent with

00:05:03.409 --> 00:05:03.419
algorithm like the gradient descent with
 

00:05:03.419 --> 00:05:06.140
algorithm like the gradient descent with
momentum RMS proper atom you would

00:05:06.140 --> 00:05:06.150
momentum RMS proper atom you would
 

00:05:06.150 --> 00:05:08.210
momentum RMS proper atom you would
update the parameters gamma and beta

00:05:08.210 --> 00:05:08.220
update the parameters gamma and beta
 

00:05:08.220 --> 00:05:10.580
update the parameters gamma and beta
just as you would update the weights of

00:05:10.580 --> 00:05:10.590
just as you would update the weights of
 

00:05:10.590 --> 00:05:13.850
just as you would update the weights of
the neural network now notice that the

00:05:13.850 --> 00:05:13.860
the neural network now notice that the
 

00:05:13.860 --> 00:05:16.940
the neural network now notice that the
effect of gamma and beta is that it

00:05:16.940 --> 00:05:16.950
effect of gamma and beta is that it
 

00:05:16.950 --> 00:05:20.300
effect of gamma and beta is that it
allows you to set the mean of V total to

00:05:20.300 --> 00:05:20.310
allows you to set the mean of V total to
 

00:05:20.310 --> 00:05:21.740
allows you to set the mean of V total to
be whatever you want it to be

00:05:21.740 --> 00:05:21.750
be whatever you want it to be
 

00:05:21.750 --> 00:05:26.630
be whatever you want it to be
in fact if gamma equals square root

00:05:26.630 --> 00:05:26.640
in fact if gamma equals square root
 

00:05:26.640 --> 00:05:29.659
in fact if gamma equals square root
Sigma squared plus Epsilon

00:05:29.659 --> 00:05:29.669
Sigma squared plus Epsilon
 

00:05:29.669 --> 00:05:32.120
Sigma squared plus Epsilon
so if camera were equal to this

00:05:32.120 --> 00:05:32.130
so if camera were equal to this
 

00:05:32.130 --> 00:05:35.719
so if camera were equal to this
denominator term and if beta were equal

00:05:35.719 --> 00:05:35.729
denominator term and if beta were equal
 

00:05:35.729 --> 00:05:42.950
denominator term and if beta were equal
to MU so this value up here then the

00:05:42.950 --> 00:05:42.960
to MU so this value up here then the
 

00:05:42.960 --> 00:05:46.219
to MU so this value up here then the
effect of gamma xenon plus beta is that

00:05:46.219 --> 00:05:46.229
effect of gamma xenon plus beta is that
 

00:05:46.229 --> 00:05:49.610
effect of gamma xenon plus beta is that
it would exactly invert this equation so

00:05:49.610 --> 00:05:49.620
it would exactly invert this equation so
 

00:05:49.620 --> 00:05:54.320
it would exactly invert this equation so
if this is true then actually these

00:05:54.320 --> 00:05:54.330
if this is true then actually these
 

00:05:54.330 --> 00:05:58.880
if this is true then actually these
older I is equal to VI and so by an

00:05:58.880 --> 00:05:58.890
older I is equal to VI and so by an
 

00:05:58.890 --> 00:06:00.080
older I is equal to VI and so by an
appropriate setting of the parameters

00:06:00.080 --> 00:06:00.090
appropriate setting of the parameters
 

00:06:00.090 --> 00:06:04.159
appropriate setting of the parameters
gamma and beta this normalization step

00:06:04.159 --> 00:06:04.169
gamma and beta this normalization step
 

00:06:04.169 --> 00:06:08.420
gamma and beta this normalization step
that is these four equations is just

00:06:08.420 --> 00:06:08.430
that is these four equations is just
 

00:06:08.430 --> 00:06:10.490
that is these four equations is just
computing essentially the identity

00:06:10.490 --> 00:06:10.500
computing essentially the identity
 

00:06:10.500 --> 00:06:12.380
computing essentially the identity
function but by choosing other values of

00:06:12.380 --> 00:06:12.390
function but by choosing other values of
 

00:06:12.390 --> 00:06:16.130
function but by choosing other values of
gamma and beta this allows you to make

00:06:16.130 --> 00:06:16.140
gamma and beta this allows you to make
 

00:06:16.140 --> 00:06:18.200
gamma and beta this allows you to make
the hidden unit values of other means

00:06:18.200 --> 00:06:18.210
the hidden unit values of other means
 

00:06:18.210 --> 00:06:18.650
the hidden unit values of other means
and be

00:06:18.650 --> 00:06:18.660
and be
 

00:06:18.660 --> 00:06:20.960
and be
winces as well and so the way you fit

00:06:20.960 --> 00:06:20.970
winces as well and so the way you fit
 

00:06:20.970 --> 00:06:23.210
winces as well and so the way you fit
this into your neural network is whereas

00:06:23.210 --> 00:06:23.220
this into your neural network is whereas
 

00:06:23.220 --> 00:06:25.490
this into your neural network is whereas
previously you are using these values V

00:06:25.490 --> 00:06:25.500
previously you are using these values V
 

00:06:25.500 --> 00:06:30.920
previously you are using these values V
1 Z 2 and so on you will now use Z 2

00:06:30.920 --> 00:06:30.930
1 Z 2 and so on you will now use Z 2
 

00:06:30.930 --> 00:06:37.880
1 Z 2 and so on you will now use Z 2
there I instead of Z I for the later

00:06:37.880 --> 00:06:37.890
there I instead of Z I for the later
 

00:06:37.890 --> 00:06:40.220
there I instead of Z I for the later
computations on your network and we want

00:06:40.220 --> 00:06:40.230
computations on your network and we want
 

00:06:40.230 --> 00:06:42.710
computations on your network and we want
to put back in this sum square bracket L

00:06:42.710 --> 00:06:42.720
to put back in this sum square bracket L
 

00:06:42.720 --> 00:06:44.660
to put back in this sum square bracket L
you know to explicitly to know which

00:06:44.660 --> 00:06:44.670
you know to explicitly to know which
 

00:06:44.670 --> 00:06:46.820
you know to explicitly to know which
layer it is in you can put it back there

00:06:46.820 --> 00:06:46.830
layer it is in you can put it back there
 

00:06:46.830 --> 00:06:48.980
layer it is in you can put it back there
so the intuition I hope you take away

00:06:48.980 --> 00:06:48.990
so the intuition I hope you take away
 

00:06:48.990 --> 00:06:52.370
so the intuition I hope you take away
from this is that we saw how normalizing

00:06:52.370 --> 00:06:52.380
from this is that we saw how normalizing
 

00:06:52.380 --> 00:06:55.250
from this is that we saw how normalizing
the input features X can help learning

00:06:55.250 --> 00:06:55.260
the input features X can help learning
 

00:06:55.260 --> 00:06:57.380
the input features X can help learning
in a neural network and what - alone

00:06:57.380 --> 00:06:57.390
in a neural network and what - alone
 

00:06:57.390 --> 00:06:59.450
in a neural network and what - alone
does is apply that normalization process

00:06:59.450 --> 00:06:59.460
does is apply that normalization process
 

00:06:59.460 --> 00:07:01.880
does is apply that normalization process
not just to the input layer but to the

00:07:01.880 --> 00:07:01.890
not just to the input layer but to the
 

00:07:01.890 --> 00:07:04.040
not just to the input layer but to the
values even deep in some hidden there in

00:07:04.040 --> 00:07:04.050
values even deep in some hidden there in
 

00:07:04.050 --> 00:07:05.540
values even deep in some hidden there in
the neural networks we apply this type

00:07:05.540 --> 00:07:05.550
the neural networks we apply this type
 

00:07:05.550 --> 00:07:07.910
the neural networks we apply this type
of normalization to normalize the mean

00:07:07.910 --> 00:07:07.920
of normalization to normalize the mean
 

00:07:07.920 --> 00:07:10.970
of normalization to normalize the mean
and variance of some of your hidden unit

00:07:10.970 --> 00:07:10.980
and variance of some of your hidden unit
 

00:07:10.980 --> 00:07:13.940
and variance of some of your hidden unit
values V but one difference between the

00:07:13.940 --> 00:07:13.950
values V but one difference between the
 

00:07:13.950 --> 00:07:15.890
values V but one difference between the
trading inputs and these hidden unit

00:07:15.890 --> 00:07:15.900
trading inputs and these hidden unit
 

00:07:15.900 --> 00:07:18.170
trading inputs and these hidden unit
values is you might not want your hidden

00:07:18.170 --> 00:07:18.180
values is you might not want your hidden
 

00:07:18.180 --> 00:07:20.570
values is you might not want your hidden
unit values to be forced to mean 0 and

00:07:20.570 --> 00:07:20.580
unit values to be forced to mean 0 and
 

00:07:20.580 --> 00:07:23.210
unit values to be forced to mean 0 and
variance 1 for example if you have a

00:07:23.210 --> 00:07:23.220
variance 1 for example if you have a
 

00:07:23.220 --> 00:07:25.250
variance 1 for example if you have a
sigmoid activation function you don't

00:07:25.250 --> 00:07:25.260
sigmoid activation function you don't
 

00:07:25.260 --> 00:07:27.350
sigmoid activation function you don't
want your values to always be clustered

00:07:27.350 --> 00:07:27.360
want your values to always be clustered
 

00:07:27.360 --> 00:07:29.600
want your values to always be clustered
here you might want them to the larger

00:07:29.600 --> 00:07:29.610
here you might want them to the larger
 

00:07:29.610 --> 00:07:32.240
here you might want them to the larger
variance or have a mean that's different

00:07:32.240 --> 00:07:32.250
variance or have a mean that's different
 

00:07:32.250 --> 00:07:33.950
variance or have a mean that's different
than 0 in order to better take advantage

00:07:33.950 --> 00:07:33.960
than 0 in order to better take advantage
 

00:07:33.960 --> 00:07:36.260
than 0 in order to better take advantage
of the non-linearity of the sigmoid

00:07:36.260 --> 00:07:36.270
of the non-linearity of the sigmoid
 

00:07:36.270 --> 00:07:38.000
of the non-linearity of the sigmoid
function rather than have all your

00:07:38.000 --> 00:07:38.010
function rather than have all your
 

00:07:38.010 --> 00:07:40.040
function rather than have all your
values being just listed within your

00:07:40.040 --> 00:07:40.050
values being just listed within your
 

00:07:40.050 --> 00:07:42.800
values being just listed within your
vision so that's why with the parameters

00:07:42.800 --> 00:07:42.810
vision so that's why with the parameters
 

00:07:42.810 --> 00:07:46.370
vision so that's why with the parameters
gamma and beta you can now make sure

00:07:46.370 --> 00:07:46.380
gamma and beta you can now make sure
 

00:07:46.380 --> 00:07:49.700
gamma and beta you can now make sure
that your VI values have the range of

00:07:49.700 --> 00:07:49.710
that your VI values have the range of
 

00:07:49.710 --> 00:07:52.220
that your VI values have the range of
values that you want but what it does

00:07:52.220 --> 00:07:52.230
values that you want but what it does
 

00:07:52.230 --> 00:07:55.220
values that you want but what it does
really it ensures that your hidden units

00:07:55.220 --> 00:07:55.230
really it ensures that your hidden units
 

00:07:55.230 --> 00:07:57.890
really it ensures that your hidden units
have standardized mean and variance

00:07:57.890 --> 00:07:57.900
have standardized mean and variance
 

00:07:57.900 --> 00:07:59.750
have standardized mean and variance
where the mean and variance are

00:07:59.750 --> 00:07:59.760
where the mean and variance are
 

00:07:59.760 --> 00:08:03.020
where the mean and variance are
controlled by two explicit parameters

00:08:03.020 --> 00:08:03.030
controlled by two explicit parameters
 

00:08:03.030 --> 00:08:05.120
controlled by two explicit parameters
gamma and beta which the learning

00:08:05.120 --> 00:08:05.130
gamma and beta which the learning
 

00:08:05.130 --> 00:08:08.540
gamma and beta which the learning
algorithm concentr whatever one so what

00:08:08.540 --> 00:08:08.550
algorithm concentr whatever one so what
 

00:08:08.550 --> 00:08:10.490
algorithm concentr whatever one so what
it really does is it normalizes in mean

00:08:10.490 --> 00:08:10.500
it really does is it normalizes in mean
 

00:08:10.500 --> 00:08:13.280
it really does is it normalizes in mean
and variance of these hidden unit values

00:08:13.280 --> 00:08:13.290
and variance of these hidden unit values
 

00:08:13.290 --> 00:08:16.460
and variance of these hidden unit values
really the VI to have some fixed

00:08:16.460 --> 00:08:16.470
really the VI to have some fixed
 

00:08:16.470 --> 00:08:19.220
really the VI to have some fixed
mean and variance and that mean and

00:08:19.220 --> 00:08:19.230
mean and variance and that mean and
 

00:08:19.230 --> 00:08:21.470
mean and variance and that mean and
variance could be 0 and 1 or it could be

00:08:21.470 --> 00:08:21.480
variance could be 0 and 1 or it could be
 

00:08:21.480 --> 00:08:23.900
variance could be 0 and 1 or it could be
some other value and is controlled by

00:08:23.900 --> 00:08:23.910
some other value and is controlled by
 

00:08:23.910 --> 00:08:26.750
some other value and is controlled by
these parameters gamma and beta so I

00:08:26.750 --> 00:08:26.760
these parameters gamma and beta so I
 

00:08:26.760 --> 00:08:28.070
these parameters gamma and beta so I
hope that gives you a sense of the

00:08:28.070 --> 00:08:28.080
hope that gives you a sense of the
 

00:08:28.080 --> 00:08:30.350
hope that gives you a sense of the
mechanics of how to implement a tional

00:08:30.350 --> 00:08:30.360
mechanics of how to implement a tional
 

00:08:30.360 --> 00:08:32.640
mechanics of how to implement a tional
at least for a single layer in the net

00:08:32.640 --> 00:08:32.650
at least for a single layer in the net
 

00:08:32.650 --> 00:08:34.890
at least for a single layer in the net
in the next video I want to show you how

00:08:34.890 --> 00:08:34.900
in the next video I want to show you how
 

00:08:34.900 --> 00:08:36.779
in the next video I want to show you how
to fit bash them into the neural network

00:08:36.779 --> 00:08:36.789
to fit bash them into the neural network
 

00:08:36.789 --> 00:08:38.850
to fit bash them into the neural network
you can deepen into network and how to

00:08:38.850 --> 00:08:38.860
you can deepen into network and how to
 

00:08:38.860 --> 00:08:40.230
you can deepen into network and how to
make it work for the many different

00:08:40.230 --> 00:08:40.240
make it work for the many different
 

00:08:40.240 --> 00:08:41.310
make it work for the many different
layers on your network

00:08:41.310 --> 00:08:41.320
layers on your network
 

00:08:41.320 --> 00:08:43.110
layers on your network
and after that will give some more

00:08:43.110 --> 00:08:43.120
and after that will give some more
 

00:08:43.120 --> 00:08:45.360
and after that will give some more
intuition about why bash storm could

00:08:45.360 --> 00:08:45.370
intuition about why bash storm could
 

00:08:45.370 --> 00:08:47.519
intuition about why bash storm could
help you train your network so in case

00:08:47.519 --> 00:08:47.529
help you train your network so in case
 

00:08:47.529 --> 00:08:49.550
help you train your network so in case
why were filthy a little bit mysterious

00:08:49.550 --> 00:08:49.560
why were filthy a little bit mysterious
 

00:08:49.560 --> 00:08:52.170
why were filthy a little bit mysterious
stay with me and I think in the two

00:08:52.170 --> 00:08:52.180
stay with me and I think in the two
 

00:08:52.180 --> 00:08:53.880
stay with me and I think in the two
videos from now we'll really make that

00:08:53.880 --> 00:08:53.890
videos from now we'll really make that
 

00:08:53.890 --> 00:08:56.220
videos from now we'll really make that
clearer

