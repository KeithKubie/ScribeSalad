Speaker 1:          00:00          In this video, we'll see a first algorithm for adapting and learning the, uh, dictionary matrix in this par scoring model, assuming that we have found some good spots, representations, uh, for our inputs. Okay. So in our parks coding objective, we have to minimization one where we perform inference of the Layton representations and one where we're trying to find a, uh, the best dictionary, a matrix, given these representations here. So now we'll look at how we can perform this minimization.

Speaker 1:          00:38          So another way to ride the previous objective is, uh, to define the loss per example, which is the, uh, uh, which is the sparsity, sorry, the reconstruction and the sparsity term, and just say we'll minimize it, uh, with respect to age. And we've seen in the previous video that, uh, how to get this representation. We were called the minimizer here, h of XD. And so we just put it back into the reconstruction objective and that we have to minimize with respected. So let's assume that the later representation that we extract the sparse representation from, uh, the input x t actually doesn't depend on the dictionary matrix. This is of course, falls, uh, for different, uh, Matrix d we'll get different spars, schooling representations, but let's see whether we could just assume that these are fixed and try to update d so that we get, uh, uh, a better, uh, improve our objective.

Speaker 1:          01:39          I'm pretty improve it at least with respect to TD. So now what do we have to do is only minimize the reconstruction part because if, uh, h of x is fixed, then this here doesn't depend on d, so we can just scrap it and ignore it. So we only have to minimize the spread reconstruction. That's actually a fairly nice problem. With respect to d, it's actually a convex optimization problem. But again, we have a constraint in a four d and the constraint is that, uh, we want it's columns to be of unit norm. So we have to do this so that the motto cannot cheat by, uh, making h as small as possible and compensating by multiplying the, by, uh, uh, the current funding factor. Okay. So again, we could do some grading dissent, but we'll have to adapt it, uh, to respect the constraint that the columns of d must remain, uh, uh, with a uniform.

Speaker 1:          02:40          So, uh, this great in the sense, uh, adapted for this constraint is a known as a projected grade in dissent algorithm. This is what we present here. So what we'll do is that as long as our dictionary, a matrix keeps changing, uh, and, and now we'll be using later representations. We've extracted for all of our training example, we keep those fixed and we only update the d matrix. And then we will do that is with the regular green in the sense of data. We'll, so we'll take d and that will subtract alpha times the gradient of the reconstruction air. And so I'll let you do the math. But here, uh, you can see that you should be able to see fairly easily that disc responds to minus the great and the reconstruction term times alpha. And with regular grading dissent, we wouldn't have this part here.

Speaker 1:          03:32          We just keep updating the matrix like this, using this formula until it has converged. But now we have to take into account the constraint. And so what we'll do is that we will simply renormalize the columns of these so that they, after the update, uh, even though after the update they might not be of unit norm, we'll put them back into, there are constraints set of that. We're exploring. We'll renormalize the columns of d and make sure that there are normal one. So here what we're doing is, is that for each column, uh, uh, the, uh, dot j, uh, we'll replace it by the column, but divided by its norms. So now it's, we normalize it. It's a, again, in unit norm, so it's called a projected great in the sense algorithm because every time we do an update, right after we read project our solution into the space of values that we allow for optimization problem. And the way we do that is through a projection. Um, so it's more formally, you can define this as a projection into a set and a, so I won't go into the mathematical details for why this is, but in this particular algorithm, projected great in the sense, simply corresponds to doing the regular grid in the set update followed by a renormalization of the columns, and you iterate until convergence, right? So that's our first algorithm for performing dictionary update, a known as projected projected grade in dissent.