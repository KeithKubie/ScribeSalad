WEBVTT
Kind: captions
Language: en

00:00:00.075 --> 00:00:03.642
It's important to remember that CUDA isn't just a specific language

00:00:03.642 --> 00:00:06.788
or a set of extensions to the CC++ language.

00:00:06.788 --> 00:00:09.726
CUDA is the whole GPU computing architecture.

00:00:09.726 --> 00:00:12.693
So let's talk about some other platforms that support CUDA.

00:00:12.693 --> 00:00:18.073
First we'll talk about other languages that support CUDA, then we'll talk about cross-platform solutions.

00:00:18.073 --> 00:00:23.834
So the lightweight version of targeting CUDA is simply to wrap CUDA C++ into the other language,

00:00:23.834 --> 00:00:28.888
so that you can transfer data to the GPU and call a CUDA C++ kernel from within the other language.

00:00:28.888 --> 00:00:35.099
And a great example of this is PyCUDA, which allows Python programs to call CUDA C++ kernels,

00:00:35.099 --> 00:00:37.793
and even to construct them programmatically from within Python.

00:00:37.793 --> 00:00:42.719
At a deeper level, an increasing number of languages are directly targeting the CUDA architecture.

00:00:42.719 --> 00:00:46.210
So fans of Python will also want to check out Copperhead.

00:00:46.210 --> 00:00:51.648
Copperhead is a data parallel subset of Python, that uses a library of data parallel

00:00:51.648 --> 00:00:54.654
functions such as map, produce, filter, scan and sort.

00:00:54.654 --> 00:00:57.824
When a Copperhead function is called, the runtime generates thrust code

00:00:57.824 --> 00:01:00.540
and calls it from the Python interpreter.

00:01:00.540 --> 00:01:05.200
Memory is managed by the Python garbage collector and lazily transferred to and from the GPU.

00:01:05.200 --> 00:01:08.155
So, a cool thing about Copperhead programs is that they interoperate

00:01:08.155 --> 00:01:13.240
with Python programs, using packages like numpy or matplotlib.

00:01:13.240 --> 00:01:16.637
This makes it easier to write entire applicatons and not just kernels.

00:01:16.637 --> 00:01:21.817
Cuda Fortran is a product form the Portland group That does exactly what it sounds like.

00:01:21.817 --> 00:01:24.178
It integrates CUDA constructs such as thread blocks

00:01:24.178 --> 00:01:26.799
and shared memory directly into the FORTRAN language.

00:01:26.799 --> 00:01:32.263
More on the research front, Halide is a brand new language specifically designed for image processing.

00:01:32.263 --> 00:01:37.232
This makes it a DSL or Domain Specific Language. And it's a really cool example of this.

00:01:37.232 --> 00:01:43.130
Halide targets GPUs as well as multi-core CPUs, on both desktops and on mobile devices.

00:01:43.130 --> 00:01:46.180
And many of the parallel optimization patterns we've discussed,

00:01:46.180 --> 00:01:50.080
such as tiling, are particularly easy to express and optimize in Halide.

00:01:50.080 --> 00:01:53.365
And that makes it possible to get very high-performance image processing code.

00:01:53.365 --> 00:01:57.205
Finally, the widely used Matlab platform supports CUDA both

00:01:57.205 --> 00:02:02.306
for wrapping CUDA kernels as well as directly targeting CUDA with Matlab code in core functions.

00:02:02.306 --> 00:02:05.016
So let's talk about Matlab a little further.

