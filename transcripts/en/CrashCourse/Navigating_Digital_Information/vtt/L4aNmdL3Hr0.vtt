WEBVTT
Kind: captions
Language: en

00:00:00.099 --> 00:00:07.350
Hi, my name is John Green…

00:00:07.350 --> 00:00:14.860
In a few weeks we will begin a 10-episode
series on Navigating Digital Information,

00:00:14.860 --> 00:00:16.790
which probably sounds boring.

00:00:16.790 --> 00:00:20.991
We thought it about calling something else--Crash
Course Saving the Internet from Itself, Crash

00:00:20.991 --> 00:00:26.070
Course Oh My God This Virtual Place Is On
Fire, Crash Course Maybe We Should Go Back

00:00:26.070 --> 00:00:28.270
to Trusting Experts, et cetera.

00:00:28.270 --> 00:00:33.289
But we ended up calling it Crash Course Navigating
Digital Information because that is what we

00:00:33.289 --> 00:00:35.920
are actually going to try to learn how to
do together.

00:00:35.920 --> 00:00:41.870
And I say together because I am not particularly
good at navigating digital information.

00:00:41.870 --> 00:00:46.640
Like a lot of you, I’ve read entire stories
online before even looking at the name of

00:00:46.640 --> 00:00:48.269
the website I’m on.

00:00:48.269 --> 00:00:53.839
My brain has to an extent been hacked by large
corporations that are able to monetize my

00:00:53.839 --> 00:00:59.089
attention, and they hold my attention in part
by showing me information that outrages and

00:00:59.089 --> 00:01:04.780
astonishes me, that scratches some itch so
deep down in my consciousness that I can’t

00:01:04.780 --> 00:01:06.399
even quite identify it.

00:01:06.399 --> 00:01:11.549
I find myself scrolling into the infinite,
refreshing to see what is newer than the news

00:01:11.549 --> 00:01:17.000
that broke ten minutes ago, passively ingesting
all kinds of information without pausing to

00:01:17.000 --> 00:01:22.200
consider the quality of that information or
how it is shaping my understanding of the

00:01:22.200 --> 00:01:24.939
universe and my place in that universe.

00:01:24.939 --> 00:01:30.970
And as we let that happen, as we allow ourselves
to fall into the vast endlessness of passive

00:01:30.970 --> 00:01:37.210
scrolling, we allow the information we ingest,
and the algorithms feeding us that information,

00:01:37.210 --> 00:01:44.039
to shape who we are as people--to shape how
we think, what we value, whom we trust, and

00:01:44.039 --> 00:01:45.179
what we do.

00:01:45.179 --> 00:01:49.619
Much attention has rightly been paid to the
ways that misinformation and disinformation

00:01:49.619 --> 00:01:55.299
are shaping our political and social discourse,
but they are also shaping us--as individuals

00:01:55.299 --> 00:01:57.009
and as communities.

00:01:57.009 --> 00:02:02.049
Getting better at evaluating information means
becoming a better citizen of the communities

00:02:02.049 --> 00:02:06.950
where you live; it also means become a better
informed and more engaged person.

00:02:06.950 --> 00:02:08.250
I love the Internet.

00:02:08.250 --> 00:02:13.510
As a child in the early 1990s, I felt isolated
and struggled socially, and the Internet helped

00:02:13.510 --> 00:02:18.550
me feel less alone and better connected to
nerdy, weird people who were like me.

00:02:18.550 --> 00:02:23.350
It has made us a closer species and given
voice to people who otherwise would not have

00:02:23.350 --> 00:02:24.350
been heard.

00:02:24.350 --> 00:02:25.480
I am not here to attack the Internet.

00:02:25.480 --> 00:02:31.420
I’m also not here to celebrate one ideology’s
misinformation over another’s.

00:02:31.420 --> 00:02:37.300
Everyone is susceptible to being misled online,
and anyone who believes themselves to be somehow

00:02:37.300 --> 00:02:42.560
immune to misinformation is, in fact, especially
susceptible to it.

00:02:42.560 --> 00:02:47.770
Instead, I want to share with you proven methods
for evaluating the quality of information

00:02:47.770 --> 00:02:51.810
you encounter online, and for becoming a more
active consumer of information.

00:02:51.810 --> 00:02:56.350
You may be wondering, how is this different
from your crash course in Media Literacy.

00:02:56.350 --> 00:03:02.240
Well, in some ways that was more of an academic
introduction and this is more of a practical

00:03:02.240 --> 00:03:03.240
one.

00:03:03.240 --> 00:03:06.450
This is vocational school for being online

00:03:06.450 --> 00:03:11.230
Internet is different for each of us--and
never more so than in this era of endlessly

00:03:11.230 --> 00:03:14.710
personalized and customized information flow.

00:03:14.710 --> 00:03:20.440
So I don’t know if we’re going to figure
out how to fix the Internet in the next ten

00:03:20.440 --> 00:03:26.870
weeks, but each of us can improve our approach
to information on the Internet.

00:03:26.870 --> 00:03:31.730
To do this, Crash Course is working with MediaWise,
a project from the Poynter Institute designed

00:03:31.730 --> 00:03:35.190
to help students evaluate the accuracy of
digital information.

00:03:35.190 --> 00:03:41.130
Mediawise--and so indirectly this series--is
funded by Google, which owns YouTube.

00:03:41.130 --> 00:03:46.110
Google also loaned Crash Course its initial
funding way back in 2011, although we eventually

00:03:46.110 --> 00:03:47.110
paid them back.

00:03:47.110 --> 00:03:51.470
I’m saying all of this, and will say it
again during the series, because it’s important

00:03:51.470 --> 00:03:56.830
to understand where funding comes from when
evaluating the accuracy of digital information,

00:03:56.830 --> 00:04:02.190
including when you’re evaluating the accuracy
of digital information about evaluating the

00:04:02.190 --> 00:04:04.230
accuracy of digital information.

00:04:04.230 --> 00:04:08.060
It’s evaluating the accuracy of digital
information all the way down.

00:04:08.060 --> 00:04:12.170
The curriculum itself we’ll be using was
developed by the Stanford History Education

00:04:12.170 --> 00:04:17.120
Group, based around research on civic online
reasoning they began in 2015.

00:04:17.120 --> 00:04:20.340
With their help, we’re going to learn how
to interact with the Internet the way professional

00:04:20.340 --> 00:04:24.710
fact-checkers do, and along the way maybe
also come to understand some of what’s wrong

00:04:24.710 --> 00:04:30.070
with the way our information feeds are working,
and how we can tack against the prevailing

00:04:30.070 --> 00:04:31.070
winds of misinformation.

00:04:31.070 --> 00:04:33.440
Thanks for watching; I’ll see you in a few
weeks.

