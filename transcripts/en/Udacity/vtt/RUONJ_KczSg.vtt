WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:03.760
I think that just about covers it as
an introduction to generalization,

00:00:03.760 --> 00:00:06.510
so I think maybe it's a good time
to reflect on what we've learned.

00:00:06.510 --> 00:00:07.150
&gt;&gt; I'm willing to do that,

00:00:07.150 --> 00:00:09.300
especially if it means I don't
have to do another quiz.

00:00:09.300 --> 00:00:11.880
&gt;&gt; Okay, well, but I, eh, okay, no,
I'm not going to do another quiz, but

00:00:11.880 --> 00:00:14.400
I am going to ask you what we learned.

00:00:14.400 --> 00:00:18.540
&gt;&gt; Okay, I think I can remember some
of that, so I think the big thing we

00:00:18.540 --> 00:00:25.140
learned is that there is both a need for
and a way to do generalization.

00:00:25.140 --> 00:00:26.220
&gt;&gt; In reinforcement learning, yeah.

00:00:26.220 --> 00:00:28.580
&gt;&gt; And the need part comes
from the fact that for

00:00:28.580 --> 00:00:32.580
a lot of problems we care about, there
are zillions and zillions of states.

00:00:32.580 --> 00:00:34.940
&gt;&gt; You could be our
generation's Carl Sagan.

00:00:34.940 --> 00:00:37.780
&gt;&gt; Yes, zillion and zillions of states.

00:00:37.780 --> 00:00:41.170
And that's a problem because of course,
we'll visit states that we've never seen

00:00:41.170 --> 00:00:44.640
before, and the chances of seeing every
state an infinite number of times and

00:00:44.640 --> 00:00:48.340
all that other stuff that the theory
requires is just kind of unreasonable.

00:00:48.340 --> 00:00:50.318
Or at least slow, and so
it would be nice to overcome that.

00:00:50.318 --> 00:00:52.510
&gt;&gt; Good, okay, and methods for?

00:00:52.510 --> 00:00:53.590
&gt;&gt; We talked about a few methods, but

00:00:53.590 --> 00:00:58.540
I think at the end of the day, it really
boiled down to let's just take all

00:00:58.540 --> 00:01:02.020
that stuff we learned in
supervised learning and apply it.

00:01:02.020 --> 00:01:04.150
&gt;&gt; Cool, but
we did talk about some specific ones.

00:01:04.150 --> 00:01:07.030
Do you remember any particular
supervised learning methods that we

00:01:07.030 --> 00:01:08.820
looked at in the context
of generalization in

00:01:08.820 --> 00:01:09.800
reinforcement learning?

00:01:09.800 --> 00:01:12.040
&gt;&gt; Sure, we looked at linear
function approximation,

00:01:12.040 --> 00:01:15.480
which makes sense because linear
is nice and well behaved.

00:01:15.480 --> 00:01:17.390
Or at least we hoped it was.

00:01:17.390 --> 00:01:18.980
And it does seem like the simplest most

00:01:18.980 --> 00:01:20.610
straightforward thing to
do based on what we know.

00:01:20.610 --> 00:01:23.510
But I think it's worth mentioning
that we were function approximating

00:01:23.510 --> 00:01:24.630
something in particular, right?

00:01:24.630 --> 00:01:30.530
So we were deciding to do the value
functions and to try to learn those.

00:01:30.530 --> 00:01:31.960
That's how we were doing
our generalization.

00:01:31.960 --> 00:01:32.690
&gt;&gt; Right, that's a good point.

00:01:32.690 --> 00:01:35.370
We actually looked at different
functions that we might approximate,

00:01:35.370 --> 00:01:38.370
value functions,
policies and models, and

00:01:38.370 --> 00:01:40.720
we focused primarily on value functions.

00:01:40.720 --> 00:01:44.590
Models, there's interesting
things to say about that, but

00:01:44.590 --> 00:01:47.860
it's not nearly as well studied or
as well understood.

00:01:47.860 --> 00:01:51.610
The value function case, we just focused
on representing like the V function or

00:01:51.610 --> 00:01:55.190
the Q function, and
that could be done in a number of ways.

00:01:55.190 --> 00:01:57.905
And we looked at kind of
a general gradient form,

00:01:57.905 --> 00:02:02.158
where we turned the Bellman equation
itself into a kind of an error measure.

00:02:02.158 --> 00:02:06.085
And then we looked specifically at
linear value function approximation,

00:02:06.085 --> 00:02:09.758
how you could, if you have a set of
features, you can map them to values

00:02:09.758 --> 00:02:13.398
using a linear function and
how we might learn that linear function.

00:02:13.398 --> 00:02:15.850
&gt;&gt; Right, and that's pretty much what
has been almost done from the start.

00:02:15.850 --> 00:02:20.010
I don't think we did hardly anything
with the policy function approximation.

00:02:20.010 --> 00:02:20.878
&gt;&gt; Nope, and that's okay.

00:02:20.878 --> 00:02:21.696
[LAUGH]
&gt;&gt; [LAUGH] And

00:02:21.696 --> 00:02:26.407
one of the reasons it's okay is because
you're able to talk about lots of

00:02:26.407 --> 00:02:30.428
successes that people have had
doing function approximation

00:02:30.428 --> 00:02:31.990
with value functions.

00:02:31.990 --> 00:02:35.258
&gt;&gt; Yeah, and there are some successes
on doing function approximation with

00:02:35.258 --> 00:02:36.108
policies as well.

00:02:36.108 --> 00:02:40.592
In fact a lot of the coolest stuff in
robotics in the reinforcement learning

00:02:40.592 --> 00:02:45.221
setting has happened where there's
an actual gradient taken with respect to

00:02:45.221 --> 00:02:48.080
representation of the policy.

00:02:48.080 --> 00:02:52.280
It's not exactly clear why it seems
to be the more effective way to use

00:02:52.280 --> 00:02:56.820
feedback in the robotic setting,
but it's worth noting at least that

00:02:57.980 --> 00:03:02.770
policy gradient methods do seem to
work well in the robotics setting.

00:03:02.770 --> 00:03:04.700
&gt;&gt; That's a good point, but
it doesn't really matter.

00:03:04.700 --> 00:03:06.670
It's not like robotics is the future or
anything.

00:03:06.670 --> 00:03:07.866
&gt;&gt; Robotics is totally the future.

00:03:07.866 --> 00:03:09.180
&gt;&gt; Mm, we'll see.

00:03:09.180 --> 00:03:11.880
We'll have this conversation in 2050 and
see what we think.

00:03:11.880 --> 00:03:13.268
&gt;&gt; In 2015, it is 2015.

00:03:13.268 --> 00:03:14.574
&gt;&gt; No, 2050.

00:03:14.574 --> 00:03:16.008
&gt;&gt; 2050, phew.

00:03:16.008 --> 00:03:18.768
I'm like, my gosh, it's the future and
I wasn't even paying attention.

00:03:18.768 --> 00:03:21.186
&gt;&gt; [LAUGH] Well that's okay.

00:03:21.186 --> 00:03:24.726
I, for one,
welcome our new robotic overlords.

00:03:24.726 --> 00:03:25.964
&gt;&gt; Good.
&gt;&gt; In the meantime,

00:03:25.964 --> 00:03:28.833
we had all these successes,
and you named a few.

00:03:28.833 --> 00:03:31.462
Right, the one I remember
the most was TD gammon, but

00:03:31.462 --> 00:03:35.204
there were others, which if I went back
and looked at what we've recorded,

00:03:35.204 --> 00:03:37.261
I'm sure I could find
out what they were.

00:03:37.261 --> 00:03:38.840
&gt;&gt; [LAUGH]
&gt;&gt; Or maybe you'll just remember.

00:03:38.840 --> 00:03:43.780
&gt;&gt; Well, I mentioned several, but
Atari is one that actually came up

00:03:43.780 --> 00:03:47.328
relatively recently, and
hopefully will actually change

00:03:47.328 --> 00:03:51.150
the way people think about this whole
topic of generalization, but we'll see.

00:03:51.150 --> 00:03:52.670
It's still too new to know.

00:03:52.670 --> 00:03:53.500
&gt;&gt; Very good, prince.

00:03:53.500 --> 00:03:57.090
Okay, so now those successes
of course were followed by

00:03:57.090 --> 00:04:02.350
the downer of problems and
sort of problem cases,

00:04:02.350 --> 00:04:07.050
where things that seem like they would
obviously work do not obviously and

00:04:07.050 --> 00:04:10.140
easily work, even in the case
of linear function of arguments.

00:04:10.140 --> 00:04:11.480
&gt;&gt; Yes, that's exactly right.

00:04:11.480 --> 00:04:14.160
The good news is in the last few years,

00:04:14.160 --> 00:04:19.670
there's a new set of methods that
are based on TD and a gradient.

00:04:19.670 --> 00:04:21.720
But it's a different kind of
gradient that actually uses

00:04:21.720 --> 00:04:26.210
the function approximator as part of the
error metric that's being resolved and

00:04:26.210 --> 00:04:28.810
that these can actually
these provably converge.

00:04:28.810 --> 00:04:30.995
For the case of linear, in particular,

00:04:30.995 --> 00:04:35.307
the paper that talks about GTD2 actually
shows what happens when you apply this

00:04:35.307 --> 00:04:39.248
particular problem case, the Baird
counter example that we looked at.

00:04:39.248 --> 00:04:43.428
And you can actually correctly learn
it if you use this other update rule

00:04:43.428 --> 00:04:47.970
instead of kind of the classic Bellman
residual minimization kind of thing.

