WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:00.860
Okay, Michael.

00:00:00.860 --> 00:00:03.700
So, we did supervised learning and

00:00:03.700 --> 00:00:05.870
we did unsupervised learning
in randomized optimization.

00:00:05.870 --> 00:00:07.580
&gt;&gt; Yep.
&gt;&gt; And the last section of the course

00:00:07.580 --> 00:00:09.080
was on reinforcement learning.

00:00:09.080 --> 00:00:09.630
&gt;&gt; Nice.

00:00:09.630 --> 00:00:11.530
&gt;&gt; Now there's a lot of
stuff that we didn't cover

00:00:11.530 --> 00:00:12.610
in that section of the course.

00:00:12.610 --> 00:00:15.100
Which sort of makes me sad,
because that's what I do for a living.

00:00:15.100 --> 00:00:17.130
I do reinforcement learning,
a little bit of game theory.

00:00:17.130 --> 00:00:19.980
You do a lot of game theory and
a lot of reinforcement learning.

00:00:19.980 --> 00:00:21.430
And there were a bunch of
things that we didn't cover.

00:00:21.430 --> 00:00:24.285
Can you think of anything in
particular you wish you did mention?

00:00:24.285 --> 00:00:26.430
&gt;&gt; Well I think the thing that most
people want to know about when

00:00:26.430 --> 00:00:29.030
they hear about reinforcement
learning is function approximation.

00:00:29.030 --> 00:00:31.577
The idea of can you apply
this idea to something

00:00:31.577 --> 00:00:34.410
other than a three by three
grid world [LAUGH] right?

00:00:34.410 --> 00:00:36.820
So to do that you often have to use

00:00:36.820 --> 00:00:39.540
the ideas that we talked about in
the other sections of the class.

00:00:39.540 --> 00:00:44.090
So use ideas from supervised learning to
learn something about the environment

00:00:44.090 --> 00:00:45.510
that you are acting in.

00:00:45.510 --> 00:00:49.136
&gt;&gt; Or even things from the unsupervised
learning section of the class.

00:00:49.136 --> 00:00:50.059
&gt;&gt; Like feature selection.

00:00:50.059 --> 00:00:51.161
&gt;&gt; Like feature selection.

00:00:51.161 --> 00:00:55.526
And in fact, the notion of function
approximation is sort of a a special

00:00:55.526 --> 00:00:57.590
case, maybe, the right term.

00:00:57.590 --> 00:01:02.230
Attraction, a particular state of
attraction, action attraction,

00:01:03.920 --> 00:01:07.770
this sort of notion of being able to
learn about one part of the space and

00:01:07.770 --> 00:01:09.260
have it teach you about
other parts of the space.

00:01:09.260 --> 00:01:09.930
&gt;&gt; Generalization?

00:01:09.930 --> 00:01:12.468
&gt;&gt; Generalization,
in general, that's a big

00:01:12.468 --> 00:01:16.024
part of making reinforcement
learning work in the real world.

00:01:16.024 --> 00:01:17.878
Speaking of which, there's POMDPs.

00:01:17.878 --> 00:01:20.370
We didn't talk about POMDPs-
&gt;&gt; Yeah, I get asked about that a lot.

00:01:20.370 --> 00:01:23.210
So, should I say what it is?

00:01:23.210 --> 00:01:26.280
&gt;&gt; People on the street walk up to you
and say POMDPs what's that all about?

00:01:26.280 --> 00:01:27.740
&gt;&gt; It's surprising.

00:01:27.740 --> 00:01:30.710
&gt;&gt; So
why don't you explain what POMDPs are?

00:01:30.710 --> 00:01:33.190
&gt;&gt; Usually it's after class,
when I'm talking about something else.

00:01:33.190 --> 00:01:34.060
&gt;&gt; Oh, well that I believe.

00:01:34.060 --> 00:01:36.830
&gt;&gt; So partially observable Markov
decision processes, right.

00:01:36.830 --> 00:01:39.830
So when we talked about without the PO.

00:01:39.830 --> 00:01:41.420
MDP, yeah, all right.

00:01:41.420 --> 00:01:46.410
So when you talk about MDPs,
the agent always has complete

00:01:46.410 --> 00:01:48.530
information about what
the current state is.

00:01:48.530 --> 00:01:50.410
That's the state is what you're
using to decide what to do.

00:01:50.410 --> 00:01:52.760
The policy maps state to action.

00:01:52.760 --> 00:01:56.740
But in reality, you don't really
have complete state information.

00:01:56.740 --> 00:01:58.720
If you're a helicopter and
you're flying through the air,

00:01:58.720 --> 00:02:01.740
you have to decide what to do based
on what you can sense right now.

00:02:01.740 --> 00:02:04.920
And there may be even uncertainty about
what's actually going on around you.

00:02:04.920 --> 00:02:05.980
&gt;&gt; Right.
&gt;&gt; And so,

00:02:05.980 --> 00:02:09.180
you can't just use state information,
because you don't have it.

00:02:09.180 --> 00:02:14.170
In the POMDP world, there's a separation
between what the actual world has

00:02:14.170 --> 00:02:18.260
as its current state, and
what the decision maker knows to be

00:02:18.260 --> 00:02:20.810
its perception of the state,
and those can be out of whack.

00:02:20.810 --> 00:02:23.950
&gt;&gt; And that makes sense, I mean we live
in a world where we don't know what

00:02:23.950 --> 00:02:28.290
is happening for every atom in the
universe that might have actually met.

00:02:28.290 --> 00:02:30.840
So you know there's a whole other
class of problems that I wish

00:02:30.840 --> 00:02:32.070
we could have talked
about that we didn't.

00:02:32.070 --> 00:02:34.755
And those are all the ones
that involve humans.

00:02:34.755 --> 00:02:35.360
&gt;&gt; Yes.
&gt;&gt; So

00:02:35.360 --> 00:02:39.110
a lot of game theory is mathematical and
abstract.

00:02:39.110 --> 00:02:41.219
But really it's about trying
to understand human behavior.

00:02:42.230 --> 00:02:42.890
Right.
In fact there's been

00:02:42.890 --> 00:02:46.920
a bunch of work done on bringing in
behavior into the game theory and

00:02:46.920 --> 00:02:51.710
game theory into behavior and
marrying economics, game theory,

00:02:51.710 --> 00:02:54.770
sociology, marketing, and-
&gt;&gt; Psychology, right, yeah.

00:02:54.770 --> 00:02:57.798
I mean, behavioral game theory
is one of the names for that.

00:02:57.798 --> 00:03:00.890
Though neuroeconomics also comes up
as a word that just seems like it

00:03:00.890 --> 00:03:01.590
shouldn't be a word.

00:03:01.590 --> 00:03:03.450
&gt;&gt; Yeah, that feels kind of made up.

00:03:03.450 --> 00:03:05.665
Except of course-
&gt;&gt; I will pay you in brain cells.

00:03:05.665 --> 00:03:08.360
&gt;&gt; [LAUGHTER] I'll get those brains.

00:03:08.360 --> 00:03:10.615
&gt;&gt; That's the currency that zombies use.

00:03:10.615 --> 00:03:13.810
&gt;&gt; [LAUGHTER] Okay, so there's a lot
about that with people with game theory,

00:03:13.810 --> 00:03:16.620
and even in reinforcement learning,
there's been a huge move.

00:03:16.620 --> 00:03:18.610
Actually, dating back decades, but

00:03:18.610 --> 00:03:21.560
really has exploded over
the last four or five years.

00:03:21.560 --> 00:03:24.180
&gt;&gt; Well in fact, what is the name of
school that you're in at Georgia Tech?

00:03:24.180 --> 00:03:25.930
&gt;&gt; The School of Interactive Computing.

00:03:25.930 --> 00:03:28.240
&gt;&gt; Yeah. That's it. Interaction turned
out to be a really important thing to

00:03:28.240 --> 00:03:28.970
some people.

00:03:28.970 --> 00:03:31.510
&gt;&gt; Right.
So, a lot of my work and reinforcement

00:03:31.510 --> 00:03:34.990
learning, in particular, has been
about bringing people into the loop.

00:03:34.990 --> 00:03:37.932
Learning from people,
watching what people do in

00:03:37.932 --> 00:03:42.780
order to do feature selection, or state
abstraction, or just simple learning.

00:03:42.780 --> 00:03:44.420
&gt;&gt; Some of my best friends
care about people.

00:03:45.490 --> 00:03:46.794
&gt;&gt; Are you saying I'm your best friend,
Michael?

00:03:46.794 --> 00:03:48.609
[SOUND] Very good, very good.

00:03:48.609 --> 00:03:50.612
Okay, so
there's a whole bunch of stuff there.

00:03:50.612 --> 00:03:52.066
[CROSSTALK]
&gt;&gt; No, I said some of my best friends.

00:03:52.066 --> 00:03:53.990
&gt;&gt; [LAUGH]
&gt;&gt; You are some of my best friends.

00:03:53.990 --> 00:03:54.770
&gt;&gt; I am some of your best friends?

00:03:54.770 --> 00:03:57.950
&gt;&gt; I don't know.
But the point is that there's

00:03:57.950 --> 00:04:00.710
this interesting line that happens
when you start thinking about

00:04:00.710 --> 00:04:04.080
how these learning systems are
interacting with people either once they

00:04:04.080 --> 00:04:07.450
start to behave in the world or
just during the learning process itself.

00:04:07.450 --> 00:04:09.995
&gt;&gt; Right, because in fact,
in practice that's how we teach it.

00:04:09.995 --> 00:04:11.595
So obviously we can talk
about this forever.

00:04:11.595 --> 00:04:12.155
&gt;&gt; Yeah.

00:04:12.155 --> 00:04:14.515
I'll just mention one,
let me just mention one buzzword.

00:04:14.515 --> 00:04:16.204
&gt;&gt; Okay.
&gt;&gt; So, inverse reinforcement learning is

00:04:16.204 --> 00:04:17.654
one that I particularly like.

00:04:17.654 --> 00:04:21.035
We talked about reinforcement learning
in the class, where you take a reward

00:04:21.035 --> 00:04:24.318
function and an interaction with
an environment and you create behavior.

00:04:24.318 --> 00:04:29.140
An inverse reinforcement learning
goes in the other direction.

00:04:29.140 --> 00:04:32.850
No, you start from
observations of behavior,

00:04:32.850 --> 00:04:36.310
interacting in an environment that some
expert is doing, and you try to guess

00:04:36.310 --> 00:04:38.960
what the reward function was that,
that expert would have been using.

00:04:38.960 --> 00:04:40.990
&gt;&gt; Or a reward function that's
consistent with that behavior.

00:04:40.990 --> 00:04:42.100
&gt;&gt; Exactly.
because you can't know,

00:04:42.100 --> 00:04:47.000
really how rewarding is it to say,
get groaned at when you make a pun.

00:04:47.000 --> 00:04:49.560
But you can tell that it's
obviously more rewarding than not.

00:04:49.560 --> 00:04:50.460
&gt;&gt; Right.

00:04:50.460 --> 00:04:53.470
But reward function, and reward function
times seven is basically the same thing.

00:04:53.470 --> 00:04:55.680
&gt;&gt; Yeah, because you can scale them and

00:04:55.680 --> 00:04:57.130
it doesn't change what
the behavior looks like.

00:04:57.130 --> 00:04:57.720
That's right.

00:04:57.720 --> 00:05:03.690
So what we take away from this is some
representation of the motivations,

00:05:03.690 --> 00:05:07.990
desires, of the individual who
demonstrated the behavior, that we can

00:05:07.990 --> 00:05:10.890
then transfer into new environments and
get good behavior out of that.

00:05:10.890 --> 00:05:12.130
&gt;&gt; You know,
I think it's broader than that.

00:05:12.130 --> 00:05:18.100
It's even broader than that, which is
really you can think of this whole

00:05:18.100 --> 00:05:22.470
learning framework as kind
of programming framework.

00:05:22.470 --> 00:05:23.430
Software engineering.

00:05:23.430 --> 00:05:29.710
Where reinforcement signals are the
mechanism by which you program the agent

00:05:29.710 --> 00:05:34.660
in order to get some particular behavior
as opposed to simple function calls.

00:05:34.660 --> 00:05:36.640
&gt;&gt; I like that topic,

00:05:36.640 --> 00:05:38.950
in fact, I think we should write
a grant proposal about that.

00:05:38.950 --> 00:05:39.920
&gt;&gt; Let's write a grant proposal.

00:05:39.920 --> 00:05:41.843
Maybe a paper in triple x.

00:05:41.843 --> 00:05:42.820
&gt;&gt; Nice.

00:05:42.820 --> 00:05:43.720
&gt;&gt; Let's do that.

00:05:43.720 --> 00:05:46.880
So look I think it's obvious we can talk
about reinforcement learning forever

00:05:46.880 --> 00:05:49.120
even though we didn't spend a lot
of time on it in the class.

00:05:49.120 --> 00:05:51.920
Because there's all these basic things
you really have to get about supervisory

00:05:51.920 --> 00:05:52.720
and unsupervised.

00:05:52.720 --> 00:05:54.940
&gt;&gt; But how can we get all
this information out there?

00:05:54.940 --> 00:05:56.100
&gt;&gt; I can think of one way.

00:05:56.100 --> 00:05:59.140
&gt;&gt; Mm-hm.
Direct brain interface.

00:05:59.140 --> 00:06:00.560
&gt;&gt; No.
&gt;&gt; Okay.

00:06:00.560 --> 00:06:01.860
&gt;&gt; I mean yes, but no.

00:06:01.860 --> 00:06:02.650
&gt;&gt; Okay.

00:06:02.650 --> 00:06:05.740
&gt;&gt; We could do another class.

00:06:05.740 --> 00:06:07.520
&gt;&gt; You mean, like together?

00:06:07.520 --> 00:06:10.790
&gt;&gt; Yes, we could do a whole class on
reinforcement learning, game theory and

00:06:10.790 --> 00:06:11.910
explore all these ideas.

00:06:11.910 --> 00:06:14.130
&gt;&gt; But who would even
want to come to such a class?

00:06:14.130 --> 00:06:15.971
&gt;&gt; Everyone because we're going to
make it required [CROSSTALK]

00:06:15.971 --> 00:06:16.488
&gt;&gt; Oh!

00:06:16.488 --> 00:06:17.956
That's great!

00:06:17.956 --> 00:06:18.690
&gt;&gt; Mm-hm.

00:06:18.690 --> 00:06:19.420
&gt;&gt; Let's do that.

00:06:19.420 --> 00:06:19.920
&gt;&gt; Let's do that.

00:06:19.920 --> 00:06:21.950
&gt;&gt; Let's make a class on
reinforcement learning.

00:06:21.950 --> 00:06:27.120
&gt;&gt; Yes, we'll do a nice three hour
course on reinforcement learning and

00:06:27.120 --> 00:06:29.480
game theory, and we can come back and
do all the same thing.

00:06:29.480 --> 00:06:30.390
We can use the same editor.

00:06:32.010 --> 00:06:34.720
&gt;&gt; Yeah, though it's very
possible that he'll cut this out.

00:06:34.720 --> 00:06:35.885
If we even talk about it.

00:06:35.885 --> 00:06:38.290
&gt;&gt; [LAUGH] No we'll use the same edit,
push car will be there.

00:06:38.290 --> 00:06:39.890
It'll, we'll get the band back together.

00:06:39.890 --> 00:06:42.080
&gt;&gt; Get the band back together again,
that sounds really fun.

00:06:42.080 --> 00:06:43.520
And the groupies can be the same.

00:06:43.520 --> 00:06:45.390
&gt;&gt; Yes.
We can have exactly the same set.

00:06:45.390 --> 00:06:46.760
So, you want to do it?

00:06:47.800 --> 00:06:50.010
&gt;&gt; I think I would.

00:06:50.010 --> 00:06:51.310
&gt;&gt; Let's shake on it.

00:06:51.310 --> 00:06:52.170
&gt;&gt; Let's do this thing.

00:06:52.170 --> 00:06:52.790
&gt;&gt; Done.

00:06:52.790 --> 00:06:53.330
&gt;&gt; Alright.

00:06:53.330 --> 00:06:55.400
&gt;&gt; Well, then,
see you in the next class.

00:06:55.400 --> 00:06:56.260
&gt;&gt; Yeah, we'll get started next week.

