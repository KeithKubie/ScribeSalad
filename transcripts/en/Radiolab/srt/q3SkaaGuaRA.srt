1
00:00:00,730 --> 00:00:05,730
You are listening to
radiolab radio from W N y.

2
00:00:05,881 --> 00:00:10,050
S. E. N. P. R.

3
00:00:11,520 --> 00:00:15,580
Hello David. Yes. Hello. This
is pat. Oh, hi pat. All right,

4
00:00:15,660 --> 00:00:20,250
let's begin with this story from our
producer Pat Walters. Pat, go ahead. Okay.

5
00:00:20,251 --> 00:00:24,720
So I heard this one from this guy
named David, Ms. David bus to ss.

6
00:00:24,840 --> 00:00:28,530
He's a psychology professor at
the University of Texas at Austin.

7
00:00:28,650 --> 00:00:32,310
And this particular story, it comes
from a book that David wrote. Um,

8
00:00:32,610 --> 00:00:34,760
could you just, just tell me, uh,

9
00:00:34,830 --> 00:00:39,780
the little story that you begin
your book with? Okay. Yes. Um,

10
00:00:39,810 --> 00:00:42,380
this is one of the things that, uh,

11
00:00:42,510 --> 00:00:46,200
this was one of the things that sparked
my interest in the topic of murder.

12
00:00:46,560 --> 00:00:49,890
The whole thing happened several
years ago. I had a very good friend,

13
00:00:49,950 --> 00:00:51,510
another professor at the university,

14
00:00:51,580 --> 00:00:55,360
and I used to socialize
with him and his wife.

15
00:00:55,361 --> 00:00:59,740
And one evening they were throwing
a party and invited me over.

16
00:00:59,741 --> 00:01:03,040
And so when I went to the party,
a party was already in full swing.

17
00:01:03,041 --> 00:01:07,090
And I got there, uh, walked
in and asked his wife, uh,

18
00:01:07,120 --> 00:01:09,630
where this friend of mine was. And, uh,

19
00:01:09,680 --> 00:01:14,650
she got a disgusted look on her face
and said that he was up in the bedroom.

20
00:01:15,550 --> 00:01:19,790
And so I went up to the bedroom
to find them. And he was in a,

21
00:01:19,791 --> 00:01:21,730
in a rage, in a rage.

22
00:01:22,700 --> 00:01:25,840
How, like you walk into the room,
what do you find? Well, he's

23
00:01:25,910 --> 00:01:29,960
started, he started fuming that
his wife had, had dissed him.

24
00:01:30,440 --> 00:01:32,000
And what did she do? Uh,

25
00:01:32,030 --> 00:01:36,650
she expressed disapproval about his, uh,

26
00:01:36,680 --> 00:01:39,740
clothing choices. She made
fun of his shirt or something,

27
00:01:39,770 --> 00:01:43,750
but did it in publicly in front of
her friends. So it was a kind of,

28
00:01:43,790 --> 00:01:45,590
he felt publicly humiliated.

29
00:01:45,670 --> 00:01:48,880
And while David's sitting in
the bedroom with this friend,

30
00:01:49,060 --> 00:01:52,720
the guy looks up at him and he
says, I'm going to kill her.

31
00:01:54,070 --> 00:01:58,470
How did he say it? Like quietly
or like through his teeth,

32
00:01:58,590 --> 00:02:01,410
you know, I'm going to kill her.

33
00:02:01,840 --> 00:02:05,860
David had always known this
guy to be pretty mild mannered,

34
00:02:05,980 --> 00:02:06,940
but he,

35
00:02:07,080 --> 00:02:09,890
it is a, uh, a large,

36
00:02:10,070 --> 00:02:14,060
very strong man, um,

37
00:02:14,090 --> 00:02:18,110
with a black belt in karate.
I knew what he was capable of.

38
00:02:18,200 --> 00:02:23,200
And so I suggested that we go out for a
walk and I basically spent the next half

39
00:02:24,500 --> 00:02:27,710
hour walking around with
him, trying to cool him off.

40
00:02:27,740 --> 00:02:32,040
But eventually he did.
He just calmed down. Hmm.

41
00:02:33,000 --> 00:02:37,640
And did you go back to the party then
and like continue dinner partying for

42
00:02:38,010 --> 00:02:38,110
awhile?

43
00:02:38,110 --> 00:02:41,200
Yeah, I did. Yes. And he did too.

44
00:02:41,560 --> 00:02:46,030
And then he seemed fine when I said
goodbye to him, it seemed common.

45
00:02:46,540 --> 00:02:51,250
I left and went home and then it was
several hours later in the middle of the

46
00:02:51,250 --> 00:02:52,083
night that I got the call

47
00:02:53,190 --> 00:02:54,023
and it was his friend

48
00:02:54,200 --> 00:02:58,970
and he says, can I come over
and sleep on your couch?

49
00:02:59,210 --> 00:03:02,650
Uh, if I don't leave my house
right now, I'm going to kill her.

50
00:03:04,300 --> 00:03:08,560
He was in this, um, a state of fury,

51
00:03:09,190 --> 00:03:13,360
he said, and um, and
instead of hitting his wife,

52
00:03:13,510 --> 00:03:18,510
he smashed his fist into the bathroom
mirror and then realized that he had to

53
00:03:20,291 --> 00:03:24,400
leave the house or he was
gonna do damage to her.

54
00:03:25,140 --> 00:03:28,850
And so he says that in, you're
like, okay, yes, come over now.

55
00:03:28,980 --> 00:03:30,930
Like, yeah, exactly.

56
00:03:31,560 --> 00:03:33,690
Meanwhile, later that night
and the other side of town,

57
00:03:33,740 --> 00:03:36,320
his wife, um, went into hiding,

58
00:03:36,890 --> 00:03:41,270
literally disappeared for six months
and didn't tell anyone where she was

59
00:03:41,271 --> 00:03:43,730
because she was terrified
that he was going to kill her.

60
00:03:46,040 --> 00:03:49,070
This story made us
wonder, is David's friend,

61
00:03:49,550 --> 00:03:54,550
is he unusual or does everybody at some
point have something dark in them that

62
00:03:56,061 --> 00:03:59,720
just tip toes out? Just
from time to time? Yeah.

63
00:04:01,160 --> 00:04:05,420
This is Radiolab and today we're
going to get back so to speak.

64
00:04:05,690 --> 00:04:08,510
We've done a good show. This is
the bad show. So you asked like,

65
00:04:08,511 --> 00:04:12,380
why do people do bad things
actually mean to be bad anyways?

66
00:04:12,381 --> 00:04:16,130
Like how do you tell the real baddies
from the rest of us? That's time our,

67
00:04:16,370 --> 00:04:20,330
I'm Jad have them. Ron,
I'm up. [inaudible] this
is Radiolab, the bad show.

68
00:04:28,300 --> 00:04:28,510
[inaudible]

69
00:04:28,510 --> 00:04:33,100
back to bed. Okay. Uh, so what happened
to David that night with his friend,

70
00:04:33,370 --> 00:04:36,610
got him really curious about murder
and badness and all these things we're

71
00:04:36,611 --> 00:04:37,444
thinking about.

72
00:04:37,600 --> 00:04:41,560
But it wasn't until a few years later
that he learned something that really put

73
00:04:41,561 --> 00:04:44,410
what happened that night in the
context. The next, at this point,

74
00:04:44,411 --> 00:04:48,820
David moved on to a new university and
he's teaching an introductory psychology

75
00:04:48,821 --> 00:04:49,230
class

76
00:04:49,230 --> 00:04:54,230
and I devoted one class session to the
topic of homicide and why people kill.

77
00:04:55,440 --> 00:04:58,470
And I designed a little, um,

78
00:04:58,500 --> 00:05:01,770
questionnaire where I simply
asked the students, you know,

79
00:05:01,771 --> 00:05:06,270
have you ever thought about killing
someone? And they would circle yes or no.

80
00:05:06,480 --> 00:05:10,920
Then he left some space at the bottom
for them to elaborate if they said yes,

81
00:05:11,100 --> 00:05:12,930
and, you know, the class ended.

82
00:05:12,990 --> 00:05:16,680
And I went back to my office and I just
sat at my desk and I started reading

83
00:05:16,681 --> 00:05:19,620
these and I was just astonished

84
00:05:19,900 --> 00:05:24,850
to find page after page of
yeses and not just yeses,

85
00:05:24,940 --> 00:05:28,210
but these very vivid descriptions about

86
00:05:28,210 --> 00:05:32,440
who they would kill, where they
do it, when the precise method,

87
00:05:32,590 --> 00:05:35,410
how many of them went
into that kind of detail?

88
00:05:36,230 --> 00:05:40,010
Uh, I would say 75 or 80%. Wow. Um,

89
00:05:40,320 --> 00:05:42,680
and are you a little bit like
horrified? Like, oh my God,

90
00:05:42,710 --> 00:05:47,360
my students are murderer. All
right. Well, I horrified as I was,

91
00:05:47,540 --> 00:05:48,800
I was pretty stunned.

92
00:05:48,950 --> 00:05:53,950
And so I expanded the sample where we
asked about 5,000 people all over the

93
00:05:55,431 --> 00:05:58,100
world. Singapore Heru the UK.

94
00:05:58,280 --> 00:06:00,110
That same question, have
you ever thought about that

95
00:06:00,150 --> 00:06:00,990
killing someone?

96
00:06:01,530 --> 00:06:06,450
And 91% of the men said
yes and 84% of the women,

97
00:06:08,030 --> 00:06:09,790
yes. I've thought about killing someone.

98
00:06:10,280 --> 00:06:11,090
Yes.

99
00:06:11,090 --> 00:06:15,410
If any sizable fraction actually
acted on their homicidal fantasies,

100
00:06:15,411 --> 00:06:17,810
the streets would be running, running red.

101
00:06:18,680 --> 00:06:20,710
Yeah. But that's just
a, those are fantasies.

102
00:06:20,780 --> 00:06:23,780
And some of them actually
seem like, well, here's one,

103
00:06:23,781 --> 00:06:26,540
something more than just fantasies from

104
00:06:26,960 --> 00:06:31,510
a woman. Sure. Okay. This
is a 20 year old female. Uh,

105
00:06:31,630 --> 00:06:35,500
we ask, who did you think about
killing? And she said, my ex boyfriend,

106
00:06:36,860 --> 00:06:40,510
um, we lived together for a couple
of months. He was very aggressive.

107
00:06:40,810 --> 00:06:44,110
He started calling me a whore and
told me he didn't love me anymore.

108
00:06:44,350 --> 00:06:48,100
So I broke up with him. Then a few
months later he started calling me,

109
00:06:48,130 --> 00:06:50,290
trying to get back together,
but I didn't want to.

110
00:06:50,860 --> 00:06:53,830
He said that if I ever had a
relationship with another man,

111
00:06:54,070 --> 00:06:58,870
he was going to send the videos of
us having sex to all the people in my

112
00:06:58,871 --> 00:07:02,860
university. The thing is that
I do have a new boyfriend,

113
00:07:02,920 --> 00:07:07,900
but my ex boyfriend doesn't know it
that yet and I'm terrified that he'll do

114
00:07:07,901 --> 00:07:11,740
what he says. Then suddenly the
thought occurred to me that would,

115
00:07:11,741 --> 00:07:16,390
my life would be much happier without
him and existence and then she said,

116
00:07:16,391 --> 00:07:17,590
I actually did this.

117
00:07:17,860 --> 00:07:22,150
I invited him for dinner and as he
was in the kitchen looking stupid,

118
00:07:22,151 --> 00:07:26,220
peeling the carrots to make salad,
I came up to him laughingly,

119
00:07:26,260 --> 00:07:28,330
gently so that he
wouldn't suspect anything.

120
00:07:29,050 --> 00:07:32,350
I thought about grabbing a knife
quickly and stabbing him in the chest

121
00:07:32,351 --> 00:07:35,830
repeatedly until he was dead.
I actually did the first thing,

122
00:07:36,220 --> 00:07:38,410
but he saw my intentions and ran away.

123
00:07:39,490 --> 00:07:44,230
When asked how close she came to
killing him, she estimated 60%

124
00:07:45,000 --> 00:07:45,833
well

125
00:07:46,770 --> 00:07:51,770
60 I don't think I've ever had a fantasy
that that anatomically specific where I

126
00:07:51,981 --> 00:07:56,300
would see the part of the other person
that I was going to stab or plan it like

127
00:07:56,301 --> 00:07:56,990
that. Well.

128
00:07:56,990 --> 00:08:00,560
Have you ever been blackmailed the way
this woman was being blackmailed? No.

129
00:08:00,561 --> 00:08:04,070
No one has ever said about a six tape
that I've ever know. So you don't know.

130
00:08:04,100 --> 00:08:05,450
It is a fair question to ask.

131
00:08:05,610 --> 00:08:10,610
What are the conditions under which you
or me or any of us could do evil things?

132
00:08:11,300 --> 00:08:15,360
I think they'd have to be extreme in
the extreme. Well, you know how mine?

133
00:08:16,010 --> 00:08:19,220
No. And you know what? This actually
brings us to our first top of the hour.

134
00:08:19,221 --> 00:08:21,440
So let me just to set it up. Robert,

135
00:08:21,441 --> 00:08:24,650
I'm going to give you this piece
of paper here. What is this?

136
00:08:24,830 --> 00:08:28,490
So these are some word pairs. Read these
words that use these words here. Yep.

137
00:08:28,550 --> 00:08:33,260
Nice Day. Fat Neck. Yes. Sad face.

138
00:08:33,261 --> 00:08:35,970
What is it? Soft hair. Yeah. I don't
know what this is. You just wear it.

139
00:08:36,560 --> 00:08:39,530
I want you to commit them to memory
fi. Commit them to memory. You know,

140
00:08:39,531 --> 00:08:41,210
and while you're doing that,
just give me your finger.

141
00:08:41,530 --> 00:08:46,170
I'm going to ask that first little
electro test finger. Let me go hard, Dan.

142
00:08:46,180 --> 00:08:50,340
Just wait a second. Clear air. He
said, give me the paper back. W ready?

143
00:08:50,490 --> 00:08:54,900
Time's up. So I'm just gonna
go into this other room over.

144
00:08:57,900 --> 00:09:02,090
Can you hear me? All right. So I'm gonna
talk to you over this intercom. Okay,

145
00:09:02,600 --> 00:09:05,570
let me give you a test. I'm not
ready for the day. Intention, right?

146
00:09:06,100 --> 00:09:10,630
To the best of your memory.
Which word was mashed with Nice?

147
00:09:11,090 --> 00:09:15,160
Was it? Nice Day. Nice sky. Nice job.

148
00:09:15,440 --> 00:09:20,440
Nice chair. Antifreeze. I
don't want, wait a second.

149
00:09:20,620 --> 00:09:23,040
Just push the button. The
corresponds to the right way to go.

150
00:09:23,950 --> 00:09:27,700
Okay. I'm choosing job wrong.

151
00:09:28,020 --> 00:09:32,280
The answer is day hiring, man. 285
olds have to give you a little,

152
00:09:35,680 --> 00:09:39,940
what did you just, you just
of you're drunk. Obviously.

153
00:09:39,941 --> 00:09:43,630
No need to be an alarm. That was not a
real shock. We were just enacting an old,

154
00:09:43,810 --> 00:09:47,710
very famous experiment that you
might've heard about. It is May,

155
00:09:47,710 --> 00:09:49,780
1962 done by this guy.

156
00:09:50,020 --> 00:09:54,340
An experiment is being conducted in the
elegant interaction laboratory at Yale

157
00:09:54,340 --> 00:09:57,580
University that Stanley Milgram talking
about the experiment in the film in case

158
00:09:57,581 --> 00:09:59,890
you've never heard of, this probably
happened in case you haven't.

159
00:10:00,490 --> 00:10:02,350
Here's what he did. He
recruited a bunch of subjects.

160
00:10:02,351 --> 00:10:06,790
Subjects are 40 males between the ages
of 20 and 50 normal everyday dudes.

161
00:10:06,791 --> 00:10:09,910
The subjects range in occupation
from corporation president,

162
00:10:10,060 --> 00:10:13,390
the good humor man and plumbers and he
ran them through something like what you

163
00:10:13,391 --> 00:10:14,224
and I just did, right.

164
00:10:14,260 --> 00:10:18,250
He would have each subject sit down
at a table right here in front of this

165
00:10:18,251 --> 00:10:22,510
really impressive looking machine. This
machine that had lots of switches on. It

166
00:10:22,780 --> 00:10:25,840
generates electric shocks on your press,
water, the switches all the way down.

167
00:10:25,841 --> 00:10:26,740
The learner gets a shock

168
00:10:27,630 --> 00:10:29,000
and in the other room there was a guy,

169
00:10:29,200 --> 00:10:33,140
then he called the learner who is
supposed to have memorized some words and

170
00:10:33,141 --> 00:10:36,640
every time that guy got a word
wrong, I'm like, you just did. Yup.

171
00:10:36,970 --> 00:10:38,340
Tapping costs, answers. Night.

172
00:10:38,690 --> 00:10:43,690
Volunteer was instructed to shock that
guy with higher and higher voltage.

173
00:10:44,781 --> 00:10:47,120
Now the volunteer couldn't
see the guy who was shocking,

174
00:10:47,590 --> 00:10:51,830
but he could definitely hear him
Milgrom staged the whole thing,

175
00:10:51,831 --> 00:10:54,260
like it was some experiment
about memory and punishment,

176
00:10:54,261 --> 00:10:57,910
but of course it wasn't about
that. I can tell you please.

177
00:10:58,050 --> 00:10:59,810
It was about how far
would these people go,

178
00:11:00,170 --> 00:11:05,170
how many times would they shock that sad
SAP in the next room just because they

179
00:11:05,361 --> 00:11:08,570
were being told Tim guy yelling.

180
00:11:08,571 --> 00:11:10,880
Of course it was an actor
and the shocks weren't real,

181
00:11:10,940 --> 00:11:15,940
but the questions in the air at the
time were very real prosecution.

182
00:11:16,431 --> 00:11:20,510
The attorney general, this was a
moment when human cruelty was on trial.

183
00:11:21,590 --> 00:11:26,590
Quite literally when I stand before
you judges of Israel in this commode,

184
00:11:27,310 --> 00:11:31,520
I used to accuse, I don't
five them and any or maybe d,

185
00:11:31,640 --> 00:11:33,110
I do not stand alone.

186
00:11:34,000 --> 00:11:38,970
The Milgrim actually begins
these experiments the same
year that Adam Eichmann

187
00:11:38,971 --> 00:11:43,410
goes on trial for Nazi war crimes.
That's radio producer Ben Walker.

188
00:11:43,420 --> 00:11:47,040
He'll be our guide for the segment
and in the trial when the prosecutors

189
00:11:47,041 --> 00:11:51,480
essentially ask him how you
came to commit genocide,

190
00:11:52,140 --> 00:11:56,230
he would say over and over again,
it was not my personal affair.

191
00:11:56,680 --> 00:11:58,720
I was just following orders.

192
00:11:58,810 --> 00:12:02,650
I had to do what I was
ordered and it's this defense.

193
00:12:02,980 --> 00:12:07,980
This is basically what Stanley Milgrim
set out to test 185 votes in a lab.

194
00:12:09,080 --> 00:12:11,330
Yale University with a
bunch of regular Americans,

195
00:12:13,370 --> 00:12:17,340
like is that something that the universal
yeah, or just 10 Eichmann thing. Yeah.

196
00:12:17,520 --> 00:12:21,810
He figured maybe 1% of these men would
keep flicking the switches up to the

197
00:12:21,811 --> 00:12:25,410
highest voltage, but
that's not what he found.

198
00:12:26,440 --> 00:12:31,440
65% were willing to shock their fellow
citizens over and over and even past when

199
00:12:35,381 --> 00:12:36,580
they were screaming in pain.

200
00:12:38,700 --> 00:12:40,140
Sounds happened to that man in that

201
00:12:40,490 --> 00:12:43,520
even when they stopped screaming.
Yeah, when they were maybe dead.

202
00:12:43,770 --> 00:12:48,520
You better check in on him, sir. He won't
answer me or nothing. Please continue.

203
00:12:48,540 --> 00:12:49,340
Go on. Twitter.

204
00:12:49,340 --> 00:12:52,010
They continued shocking their corpses.

205
00:12:53,790 --> 00:12:54,623
Wow.

206
00:12:55,670 --> 00:13:00,670
His experiment remains one of the most
famous experiments of the 20th century in

207
00:13:01,551 --> 00:13:06,551
1962 Stanley Milgrim shocked the world
was his study on obedience is still

208
00:13:06,741 --> 00:13:11,300
trotted out to explain everything
from hazing to war crimes.

209
00:13:11,390 --> 00:13:15,800
What is there in human nature to gang
behavior allows an individual to act

210
00:13:15,870 --> 00:13:20,600
inhumanely genocide harshly. It's
like a downloadable from the Internet.

211
00:13:20,630 --> 00:13:23,870
Instant defense for doing wrong, but

212
00:13:25,680 --> 00:13:28,670
if you look at Milgram's work
closely. Yeah, yeah, yeah, yeah.

213
00:13:29,040 --> 00:13:30,850
Like this guy did, oh x,

214
00:13:31,070 --> 00:13:34,170
how's Lim professor of psychology
at the University of Exeter?

215
00:13:34,200 --> 00:13:36,780
Then a different picture will emerge.

216
00:13:36,840 --> 00:13:41,040
Really that story has been told
1,000,001 times for the last 50 years.

217
00:13:41,041 --> 00:13:42,620
We've just got to get,
oh, got to get out for it.

218
00:13:42,810 --> 00:13:47,010
Now what you need to understand about
Alex Haslam is that he hates it when

219
00:13:47,011 --> 00:13:50,520
interviewers only want to
talk about the baseline study.

220
00:13:50,550 --> 00:13:54,460
The one that everybody knows, the
so called baseline, the 65% one,

221
00:13:54,720 --> 00:13:57,820
the one we just talked about. Yeah. So
there's more, there's more too. Yeah.

222
00:13:58,050 --> 00:14:03,050
Because actually he studied between 20
and 40 different variants of this same

223
00:14:03,571 --> 00:14:07,410
paradigm. Stanley Milgram took
electric shock very seriously.

224
00:14:07,710 --> 00:14:12,710
He did this experiment a bunch of times
and a bunch of different ways how all

225
00:14:12,781 --> 00:14:16,410
sorts of different things. He would change
where the shocker and the Shaki Sat.

226
00:14:16,440 --> 00:14:20,760
Yet women participantcy had an
experiment who wasn't a scientist,

227
00:14:20,870 --> 00:14:24,510
but it was a member of the general
public and every scenario produced a

228
00:14:24,511 --> 00:14:29,220
different result. Really? Yup.
Let me, I mean, I'm just, uh,

229
00:14:29,221 --> 00:14:32,440
I've got in front of me, I've just got
the, uh, the data from the Milgrim.

230
00:14:32,610 --> 00:14:34,560
So let me just get that. I mean, so again,

231
00:14:34,590 --> 00:14:38,550
the baseline studies is the one where
65% of volunteers go all the way,

232
00:14:38,830 --> 00:14:42,730
highest dose of electricity, exciting
sex. But in experiment number three,

233
00:14:43,960 --> 00:14:47,290
if they put the Shaki in the
same room with a shocker,

234
00:14:47,350 --> 00:14:51,650
so the shocker can actually see the
person that he's shocking obedience tropes

235
00:14:51,651 --> 00:14:54,920
to about 40% in an
experiment. Number four,

236
00:14:54,950 --> 00:14:58,250
when the teacher has to hold the
learner's hand down on the plate,

237
00:14:58,310 --> 00:15:03,140
in order to him to feel the shock,
they drops to about 30%. Wow.

238
00:15:03,230 --> 00:15:07,400
Experiment 14. If the experiments
are, he is not a scientist,

239
00:15:07,430 --> 00:15:12,380
but is an ordinary man not wearing a
white coat, obedience drops to 20%.

240
00:15:13,580 --> 00:15:17,570
Well, how low can we go? Okay,
here's another one. Experiment 17.

241
00:15:17,800 --> 00:15:21,770
There's you and this to other
participants, both actors.

242
00:15:21,800 --> 00:15:25,660
If those two participants refuse
to go on, like saying like,

243
00:15:25,700 --> 00:15:29,870
I don't want to kill a guy. Boney
10% under those circumstances, go on.

244
00:15:29,990 --> 00:15:32,480
And then the final one,
experiment 15 of course,

245
00:15:32,740 --> 00:15:35,360
normally you just have one experiment
who's giving you these instructions.

246
00:15:35,361 --> 00:15:39,320
But if you put two experimenters in the
room and they start disagreeing with

247
00:15:39,321 --> 00:15:44,160
each other, and this one you get 0%
going on zero z arrive in that condition.

248
00:15:44,160 --> 00:15:48,900
You said Zero Nolan and go right to the
zero at one post. No one not assault.

249
00:15:49,080 --> 00:15:51,600
Exactly 0% well, all right,

250
00:15:51,601 --> 00:15:54,540
I'm starting to feel a little bit
better about my fellow man. One second.

251
00:15:54,541 --> 00:15:59,070
Hey Sh. Okay, where is he?

252
00:15:59,460 --> 00:16:03,210
Am I'm in a closet closet because this
room is echoey and you know there's

253
00:16:03,211 --> 00:16:06,780
nothing like a closet full of clothes
to like help balance that out.

254
00:16:06,840 --> 00:16:10,290
That's true. That's true. All
right, let's keep going. So you see,

255
00:16:10,291 --> 00:16:15,240
it's just in that one experiment that
65% of people are willing to go all the

256
00:16:15,241 --> 00:16:18,180
way, but in all of these
other scenarios, they don't.

257
00:16:18,840 --> 00:16:21,750
And even when they do say yes, even
when they go along with the experiment,

258
00:16:21,990 --> 00:16:25,170
as you can see in the film, they struggle

259
00:16:25,640 --> 00:16:28,790
using the last switch on the board.
Please. I'm not getting no answer.

260
00:16:29,090 --> 00:16:30,800
Please continue. The next word is white.

261
00:16:30,900 --> 00:16:32,550
They have debates with them.

262
00:16:33,230 --> 00:16:34,950
Do you think you should
look in on him? Please?

263
00:16:35,150 --> 00:16:37,030
Fights with the experiments, Huh?

264
00:16:37,120 --> 00:16:40,330
Not Once we've started the experiment of
something happened on mad and attacked

265
00:16:40,360 --> 00:16:43,080
or something like that. The
experiment requires that we continue,

266
00:16:43,480 --> 00:16:46,060
go on and put down that. Don't
the man's health mean anything?

267
00:16:46,480 --> 00:16:49,270
Whether the learner likes it or not.
We might might be better than that.

268
00:16:49,430 --> 00:16:52,070
What's interesting is that
how all of these struggles,

269
00:16:52,100 --> 00:16:55,280
all of them play out the same way.

270
00:16:55,310 --> 00:16:59,870
It's the experimenter
prodding the shockers along.

271
00:17:00,140 --> 00:17:02,980
You're going to keep giving them
what, 450 volts every shot now.

272
00:17:03,220 --> 00:17:06,290
Correct. For me, it's all about
the prods connect words white.

273
00:17:06,370 --> 00:17:08,890
This is what totally
pulled me into this story.

274
00:17:09,280 --> 00:17:14,280
Prods Stanley Milgrim had four scripted
prods that he wrote out for his

275
00:17:14,560 --> 00:17:17,670
experimenters for when the subjects
didn't want to continue. Yup.

276
00:17:17,740 --> 00:17:19,150
The first one was,

277
00:17:19,330 --> 00:17:23,920
please go on and if they
didn't go on, if they resisted,

278
00:17:24,860 --> 00:17:27,190
he experimented with
break out prod number two.

279
00:17:27,640 --> 00:17:29,830
The experiment requires that you continue.

280
00:17:29,900 --> 00:17:32,480
Well, the experiment requires,
I mean I know it does, sir,

281
00:17:32,481 --> 00:17:35,160
but I mean he's up to 195 [inaudible]

282
00:17:35,290 --> 00:17:39,520
and if they still were resisting or
struggling, they'd get prod number three.

283
00:17:39,850 --> 00:17:43,290
It's absolutely essential that you
continue. It's absolutely essential.

284
00:17:43,840 --> 00:17:46,690
It's a little bit more direct. It's
a bit stronger. It's not an order,

285
00:17:46,960 --> 00:17:50,160
not quite, but the fourth prod,
really the, the critical, the critical

286
00:17:50,430 --> 00:17:53,100
false prod is an absolute
order. The fourth prod is

287
00:17:54,020 --> 00:17:57,570
you got to know where the
choice. Do you have no other

288
00:17:57,690 --> 00:18:02,430
choice teacher? You must continue.
That is definitely an order.

289
00:18:02,460 --> 00:18:07,200
Exactly, but every time the experiment
are pulled out, the fourth prod,

290
00:18:07,530 --> 00:18:10,560
and this was confirmed when the
experiment was redone in 2006

291
00:18:12,660 --> 00:18:15,240
total disobedience, Hodel, disobedient.

292
00:18:15,390 --> 00:18:19,650
Anytime the experiment said you must
continue, the shocker would say,

293
00:18:19,860 --> 00:18:21,290
hell no. I've

294
00:18:21,350 --> 00:18:25,310
don't. You have no other choice
teacher. I have a choice.

295
00:18:25,311 --> 00:18:26,180
I'm not going to go ahead with it.

296
00:18:30,800 --> 00:18:33,980
Well, we'll have to discontinue
the experiment. I'm sorry.

297
00:18:34,940 --> 00:18:35,773
Here's another one.

298
00:18:35,960 --> 00:18:38,090
You have no other choice. You
must want to have a choice.

299
00:18:41,040 --> 00:18:42,420
How does it do you all continue up?

300
00:18:42,421 --> 00:18:46,590
We're going to have to discontinue the
experiment, have to, he says cut it out.

301
00:18:46,710 --> 00:18:50,610
After all, he knows what he
can stand. That's my opinion.

302
00:18:50,611 --> 00:18:51,750
That's where I'm going to stand on it.

303
00:18:53,470 --> 00:18:57,510
Well, so the subjects seem willing
to shock another human being,

304
00:18:57,511 --> 00:19:02,310
but as soon as you say it's an order,
they don't do it. Now that's important.

305
00:19:02,311 --> 00:19:05,670
It's very important because if
you ask university undergraduates,

306
00:19:05,700 --> 00:19:07,320
what does the Milgram study show,

307
00:19:07,440 --> 00:19:10,890
they will invariably say something like
they showed that people obey orders.

308
00:19:10,980 --> 00:19:11,430
Okay.

309
00:19:11,430 --> 00:19:14,640
Well actually the one thing that the
study really doesn't show is that people

310
00:19:14,641 --> 00:19:17,870
are, bay ordered is a
pretty big thing. To me.

311
00:19:18,000 --> 00:19:21,740
This is a pretty big thing to
mess, isn't it? Really? See, wait,

312
00:19:21,750 --> 00:19:26,720
if it doesn't show that people
are just obeying orders. Yeah,

313
00:19:26,910 --> 00:19:29,720
and what does it show? Okay. I think
it looks, it's like this. All right,

314
00:19:29,760 --> 00:19:32,580
let's go on to our instructions.
We will begin with this test.

315
00:19:32,610 --> 00:19:37,440
The participants are there in the, in
the study, I've got a, a very plausible,

316
00:19:37,441 --> 00:19:41,310
very credible high status scientists. And
as I say, just scientific institution,

317
00:19:41,860 --> 00:19:44,820
Yale, who is going to do this
powerful piece of science, right?

318
00:19:44,821 --> 00:19:46,410
Your voice toward that
microphone is the room.

319
00:19:46,411 --> 00:19:50,010
So they sit down in the chair thinking,
wow, this is really important.

320
00:19:50,190 --> 00:19:54,480
I'm about to help this quest for
knowledge. I really want to do a good job.

321
00:19:54,510 --> 00:19:58,040
Now as we sort of know in life,
lots of things that we do,

322
00:19:58,060 --> 00:20:00,690
they worthwhile doing and not always easy.

323
00:20:00,960 --> 00:20:04,290
And you find yourself in a situation
where you've got to do something that's

324
00:20:04,291 --> 00:20:07,560
hard, like shocking, an
innocent stranger over and over.

325
00:20:07,561 --> 00:20:09,480
But if you think that's the right thing,

326
00:20:09,510 --> 00:20:13,140
if you think that science is
worth pursuing, you say, okay,

327
00:20:13,141 --> 00:20:14,070
I'll go along with this.

328
00:20:15,690 --> 00:20:18,300
So you're saying you're shocking these
people because they thought it was

329
00:20:18,450 --> 00:20:21,390
worthwhile. Look to participants.
You know, they're not, it's not.

330
00:20:21,570 --> 00:20:24,660
It's not just blind to beaten.
So tell me, so yes sir. No Sir.

331
00:20:24,661 --> 00:20:25,530
Three bags full sir.

332
00:20:26,340 --> 00:20:30,510
They're engaged with the task that
trying to be good participants.

333
00:20:31,660 --> 00:20:34,080
They're trying to do the right thing.

334
00:20:34,500 --> 00:20:37,950
They're not doing something
because they have to.

335
00:20:38,010 --> 00:20:40,530
They're doing it because
they think they ought to.

336
00:20:41,730 --> 00:20:43,620
And that's all the
difference in the world.

337
00:20:47,470 --> 00:20:48,100
Suddenly I'm thinking

338
00:20:48,100 --> 00:20:50,560
this is actually a darker
interpretations. The original,

339
00:20:50,620 --> 00:20:53,680
absolutely darker stay are
doing. And no question about it.

340
00:20:53,681 --> 00:20:56,350
They have the agency. Yep.
And they think it's right.

341
00:20:57,150 --> 00:20:58,890
Although clearly they on some
level, they know it isn't

342
00:20:59,040 --> 00:21:00,840
does this sort of chilling comparison,

343
00:21:00,841 --> 00:21:05,841
which is a speech that Himmler gave to
the SS cmss leaders when they were about

344
00:21:07,171 --> 00:21:09,420
to commit a range of
atrocities. And he said, look,

345
00:21:09,421 --> 00:21:12,390
this is what you're going to do is
of course you don't want to do this.

346
00:21:12,391 --> 00:21:16,530
Of course nobody wants to be killing
other people and we realize this is hard

347
00:21:16,531 --> 00:21:17,070
work.

348
00:21:17,070 --> 00:21:21,390
But what you're doing is for the good of
Germany and this is necessary in order

349
00:21:21,391 --> 00:21:24,060
to advance our noble cause.

350
00:21:24,590 --> 00:21:25,423
Wow.

351
00:21:25,850 --> 00:21:28,820
So then, Hey, wait, I'm almost done
guys. Give me two more minutes.

352
00:21:28,850 --> 00:21:29,690
Two more minutes. Okay.

353
00:21:29,930 --> 00:21:34,280
So in the Milgrim case [inaudible] with
the idea is that people will do bad if

354
00:21:34,281 --> 00:21:36,950
they think it's good,
like the good noble cause.

355
00:21:36,951 --> 00:21:40,250
Well what's the noble cause in
this case? Science, science.

356
00:21:40,280 --> 00:21:44,180
You can see this in the surveys that
the men filled out after the experiments

357
00:21:44,181 --> 00:21:47,090
were over. This was exactly what
was on my mind. If the experiment,

358
00:21:47,750 --> 00:21:51,320
if the experiment had to be
successful, it had to be carried on.

359
00:21:51,470 --> 00:21:55,100
The questionnaires they filled out are
part of the Milgrim archive at Yale.

360
00:21:55,440 --> 00:22:00,440
We're willing to help in a worthwhile
experiment and it's kind of surprising.

361
00:22:01,070 --> 00:22:05,660
A lot of them are really positive even
though they've just been told that they

362
00:22:05,661 --> 00:22:09,170
were duped. Research
in any field is a must,

363
00:22:09,171 --> 00:22:10,940
particularly in this day and age.

364
00:22:10,941 --> 00:22:14,660
Do you think that more studies of this
sort should be carried out? Definitely,

365
00:22:14,840 --> 00:22:15,673
yes.

366
00:22:16,250 --> 00:22:20,760
We as, as onlookers to the study,
we have this kind of Godlike, uh,

367
00:22:20,840 --> 00:22:23,180
sort of vision of like, well of
course what they're doing is wrong,

368
00:22:23,450 --> 00:22:25,700
but if it looks at it
from another perspective,

369
00:22:26,150 --> 00:22:29,420
there's a sense in which
you could celebrate what
they're doing. You're, I mean,

370
00:22:29,450 --> 00:22:30,770
I'm not suggesting one should,

371
00:22:30,771 --> 00:22:34,160
but I'm just saying there is a sense in
which these people are prepared to do

372
00:22:34,161 --> 00:22:37,760
something that's very painful to them
and to someone else because they want to

373
00:22:37,761 --> 00:22:40,820
promote science. Well, you know, you
can see that's a good thing. I mean,

374
00:22:40,821 --> 00:22:41,654
you know

375
00:22:42,010 --> 00:22:47,010
God cause it's like we started with this
experiment that we all see as evidence

376
00:22:47,051 --> 00:22:51,160
of humans, latent capacity for
evil. You tell us actually no,

377
00:22:51,161 --> 00:22:55,180
under some circumstances we don't do
the bad thing we're told to do because

378
00:22:55,450 --> 00:22:58,660
here's another flip. We don't have to
be told. In fact, we hate being told,

379
00:22:58,661 --> 00:23:01,660
but we will do it on our own
if we think it's good. Yeah.

380
00:23:01,840 --> 00:23:04,150
Now you're saying actually
that you could read that,

381
00:23:04,330 --> 00:23:08,660
that very dark fact as being actually
evidence of something quite Nobel. Yeah.

382
00:23:08,710 --> 00:23:11,260
Well if you dressed it up and if
you just had some minor variants,

383
00:23:11,261 --> 00:23:13,900
the paradigm you could presume that
we make, you know, make these up.

384
00:23:13,901 --> 00:23:16,030
These are people who are
incredibly noble. They are,

385
00:23:16,210 --> 00:23:19,510
I mean it's the fact of course that
they're administering pain to a stranger.

386
00:23:19,511 --> 00:23:20,920
That's what's horrifying about it.

387
00:23:21,100 --> 00:23:22,990
But imagine they were
administering paint themselves.

388
00:23:22,991 --> 00:23:25,630
Imagine they were really where I had
to administer shocks themselves or

389
00:23:25,631 --> 00:23:27,370
something. But if they
were prepared to do that,

390
00:23:27,371 --> 00:23:30,070
when I suspect a lot of them
would, um, then we'd say,

391
00:23:30,071 --> 00:23:32,680
these are people who really believe in
Saas and isn't this a good thing that we

392
00:23:32,681 --> 00:23:37,681
have people in our society who are
willing to make sacrifices for the greater

393
00:23:37,991 --> 00:23:38,824
good?

394
00:23:39,130 --> 00:23:39,963
Huh.

395
00:23:40,410 --> 00:23:41,640
So in the end, where do you come down?

396
00:23:41,641 --> 00:23:46,040
Do you leave this experiment in a
light mood or in a dark mood? Uh, I th

397
00:23:46,070 --> 00:23:48,680
I, I overall, I would
say in a powerful mood,

398
00:23:48,770 --> 00:23:52,370
we're close to some really fundamental
truths about human nature. And you know,

399
00:23:52,371 --> 00:23:56,900
my views about human nature is that it
affords infinite potential for lightness

400
00:23:56,901 --> 00:23:59,690
and dark. There's lots and lots of
lessons here and, but one is, I think,

401
00:23:59,691 --> 00:24:03,170
you know, when you are enjoined to
do something for the greater good,

402
00:24:03,200 --> 00:24:06,320
maybe ask yourself the question
what is greater and what is good?

403
00:24:07,210 --> 00:24:09,570
That right there. Slap some
quotations around that.

404
00:24:09,610 --> 00:24:11,560
Yeah. Yeah.

405
00:24:19,980 --> 00:24:20,300
[inaudible]

406
00:24:20,300 --> 00:24:23,550
my thanks to Ben Walker whose podcast
he has a podcast and it's a good one.

407
00:24:23,551 --> 00:24:26,550
It's called too much information.
Yes, it's awesome. Thank you Ben,

408
00:24:26,551 --> 00:24:28,530
and also thank you to Alex Haslam,

409
00:24:28,531 --> 00:24:32,520
professor of psychology at the University
of Exeter. We'll be right back.

