WEBVTT
Kind: captions
Language: en

00:00:01.335 --> 00:00:02.710
MALE SPEAKER:
Welcome, everybody,

00:00:02.710 --> 00:00:06.260
to one more Authors
at Google Talk.

00:00:06.260 --> 00:00:12.470
Today, our guest speaker
is Pedro Domingos,

00:00:12.470 --> 00:00:17.490
whose new book is called
"The Master Algorithm."

00:00:17.490 --> 00:00:21.440
We have it here and you
can buy copies outside.

00:00:21.440 --> 00:00:23.770
So one definition
of machine learning

00:00:23.770 --> 00:00:26.450
is "the automation
of discovery."

00:00:26.450 --> 00:00:29.270
Our guest, Pedro Domingos,
is at the very forefront

00:00:29.270 --> 00:00:32.680
of the search for the master
algorithm, a universal learner

00:00:32.680 --> 00:00:35.910
capable of deriving all
knowledge, past, present

00:00:35.910 --> 00:00:37.900
and future, from data.

00:00:37.900 --> 00:00:40.620
Pedro Domingos is a
professor of Computer Science

00:00:40.620 --> 00:00:43.310
and Engineering at the
University of Washington.

00:00:43.310 --> 00:00:46.110
He's the co-founder of the
International Machine Learning

00:00:46.110 --> 00:00:47.320
Society.

00:00:47.320 --> 00:00:50.130
Pedro received his MS in
Electrical Engineering

00:00:50.130 --> 00:00:54.520
and Computer Science
from IST in Lisbon,

00:00:54.520 --> 00:00:57.500
his Master's of Science
and PhD in Information

00:00:57.500 --> 00:01:00.280
and Computer Science from
the University of California

00:01:00.280 --> 00:01:01.510
at Irvine.

00:01:01.510 --> 00:01:04.950
He spent two years as an
assistant professor at IST

00:01:04.950 --> 00:01:08.460
before joining the faculty of
the University of Washington

00:01:08.460 --> 00:01:10.270
in 1999.

00:01:10.270 --> 00:01:13.330
Pedro is the author or
co-author of over 200

00:01:13.330 --> 00:01:16.360
technical publications in
machine learning, data mining,

00:01:16.360 --> 00:01:17.670
and other areas.

00:01:17.670 --> 00:01:23.010
He is the winner of the SIGKDD
Innovation Award, the highest

00:01:23.010 --> 00:01:24.870
honor in data science.

00:01:24.870 --> 00:01:30.570
He's an AAAI Fellow and has
received the Sloan Fellowship

00:01:30.570 --> 00:01:35.700
and NSF Career Award, a
Fulbright scholarship, an IBM

00:01:35.700 --> 00:01:38.600
Faculty Award, several
best paper awards,

00:01:38.600 --> 00:01:40.100
and other distinctions.

00:01:40.100 --> 00:01:42.641
He's a member of the editorial
board of "The Machine Learning

00:01:42.641 --> 00:01:43.790
Journal."

00:01:43.790 --> 00:01:47.670
Please join me in welcoming
Pedro, today, to Google.

00:01:47.670 --> 00:01:50.861
[APPLAUSE]

00:01:51.360 --> 00:01:53.270
PEDRO DOMINGOS: Thank you.

00:01:53.270 --> 00:01:55.470
Let me start with a
very simple question--

00:01:55.470 --> 00:01:57.630
where does knowledge come from?

00:01:57.630 --> 00:02:01.550
Until very recently, it came
from just three sources, number

00:02:01.550 --> 00:02:03.490
one, evolution-- that's
the knowledge that's

00:02:03.490 --> 00:02:05.770
encoded in your
DNA-- number two,

00:02:05.770 --> 00:02:07.840
experience-- that's
the knowledge that's

00:02:07.840 --> 00:02:10.464
encoded in your neurons--
and number three, culture,

00:02:10.464 --> 00:02:11.880
which is the
knowledge you acquire

00:02:11.880 --> 00:02:15.520
by talking with other people,
reading books, and so on.

00:02:15.520 --> 00:02:17.750
And everything
that we do, right,

00:02:17.750 --> 00:02:21.050
everything that we are basically
comes from these three sources

00:02:21.050 --> 00:02:22.200
of knowledge.

00:02:22.200 --> 00:02:27.160
Now what's quite extraordinary
is just, only recently,

00:02:27.160 --> 00:02:30.990
there's a fourth source of
knowledge on the planet.

00:02:30.990 --> 00:02:33.240
And that's computers.

00:02:33.240 --> 00:02:36.040
There's more and more knowledge
now that comes from computers,

00:02:36.040 --> 00:02:38.410
is discovered by computers.

00:02:38.410 --> 00:02:42.060
And this is as big of a
change as the emergence

00:02:42.060 --> 00:02:44.200
of each of these four was.

00:02:44.200 --> 00:02:46.930
Like evolution, right,
well, that's life on earth.

00:02:46.930 --> 00:02:48.680
It's the product of evolution.

00:02:48.680 --> 00:02:53.740
Experience is what distinguishes
us mammals from insects.

00:02:53.740 --> 00:02:56.290
And culture is what
makes humans what we are

00:02:56.290 --> 00:02:58.250
and as successful as we are.

00:02:58.250 --> 00:03:02.230
Notice, also, that each of these
forms of knowledge discovery

00:03:02.230 --> 00:03:05.720
is orders of magnitude
faster than the previous one

00:03:05.720 --> 00:03:08.650
and discovers orders of
magnitude more knowledge.

00:03:08.650 --> 00:03:12.210
And indeed, the same thing
is true of computers.

00:03:12.210 --> 00:03:15.400
Computers can discover
knowledge orders of magnitude

00:03:15.400 --> 00:03:17.430
faster than any of these
things that went before

00:03:17.430 --> 00:03:20.130
and that co-exist with them
and orders of magnitude more

00:03:20.130 --> 00:03:21.980
knowledge in the
same amount of time.

00:03:21.980 --> 00:03:24.940
In fact, Yann LeCun
says that "most

00:03:24.940 --> 00:03:27.490
of the knowledge in
the world in the future

00:03:27.490 --> 00:03:29.810
is going to be
extracted by machines

00:03:29.810 --> 00:03:31.840
and will reside in machines."

00:03:31.840 --> 00:03:35.120
So this is a major change that,
I think, is not just for us

00:03:35.120 --> 00:03:37.817
computer scientists
to know about and deal

00:03:37.817 --> 00:03:39.650
with, it's actually
something that everybody

00:03:39.650 --> 00:03:42.060
needs to understand.

00:03:45.510 --> 00:03:48.820
So how do computers
discover new knowledge?

00:03:48.820 --> 00:03:51.280
This is, of course, the
province of machine learning.

00:03:51.280 --> 00:03:54.010
And in a way, what I'm going
to try to do in this talk

00:03:54.010 --> 00:03:57.870
is try to give you a sense
of what machine learning is

00:03:57.870 --> 00:03:58.719
and what it does.

00:03:58.719 --> 00:04:00.760
If you're already familiar
with machine learning,

00:04:00.760 --> 00:04:05.442
this will hopefully give you
a different perspective on it.

00:04:05.442 --> 00:04:07.650
If you're not familiar with
machine learning already,

00:04:07.650 --> 00:04:13.790
this should be quite
fascinating and interesting.

00:04:13.790 --> 00:04:17.800
So there are five main
paradigms in machine learning.

00:04:17.800 --> 00:04:20.339
And I will talk about
each one of them in turn

00:04:20.339 --> 00:04:23.950
and then try to step back and
see, what is the big picture

00:04:23.950 --> 00:04:27.600
and what is this idea
of the master algorithm.

00:04:27.600 --> 00:04:30.260
The first way computers
discover knowledge

00:04:30.260 --> 00:04:33.460
is by filling gaps in
existing knowledge.

00:04:33.460 --> 00:04:37.080
Pretty much the same way
that scientists work, right?

00:04:37.080 --> 00:04:40.000
You make observations,
you hypothesize

00:04:40.000 --> 00:04:41.940
theories to explain
them, and then

00:04:41.940 --> 00:04:43.340
you see where they fall short.

00:04:43.340 --> 00:04:46.170
And then you adapt
them, or throw them away

00:04:46.170 --> 00:04:47.700
and try new ones, and so on.

00:04:47.700 --> 00:04:48.940
So this is one.

00:04:48.940 --> 00:04:51.690
Another one is to
emulate the brain.

00:04:51.690 --> 00:04:52.190
Right?

00:04:52.190 --> 00:04:53.773
The greatest learning
machine on earth

00:04:53.773 --> 00:04:57.960
is the one inside your skull,
so let's reverse engineer it.

00:04:57.960 --> 00:05:02.160
Third one is to
simulate evolution.

00:05:02.160 --> 00:05:04.984
Evolution, by some standards,
is actually an even greater

00:05:04.984 --> 00:05:06.400
learning algorithm
than your brain

00:05:06.400 --> 00:05:08.610
is, because, first of
all, it made your brain.

00:05:08.610 --> 00:05:09.800
It also made your body.

00:05:09.800 --> 00:05:12.210
And it also made every
other life form on Earth.

00:05:12.210 --> 00:05:14.910
So maybe that's something
worth figuring out how it works

00:05:14.910 --> 00:05:17.950
and doing it with computers.

00:05:17.950 --> 00:05:19.270
Here's another one.

00:05:19.270 --> 00:05:23.800
And this is to realize that all
the knowledge that you learn

00:05:23.800 --> 00:05:26.180
is necessarily uncertain.

00:05:26.180 --> 00:05:26.840
Right?

00:05:26.840 --> 00:05:28.466
When something is
induced from data,

00:05:28.466 --> 00:05:29.840
you're never quite
sure about it.

00:05:29.840 --> 00:05:34.240
So the way to learn is to
quantify that uncertainty using

00:05:34.240 --> 00:05:35.230
probability.

00:05:35.230 --> 00:05:36.880
And then as you
see more evidence,

00:05:36.880 --> 00:05:39.290
the probability of different
hypotheses evolves.

00:05:39.290 --> 00:05:40.580
Right?

00:05:40.580 --> 00:05:43.750
And there's an optimal way to
do this using Bayes' theorem.

00:05:43.750 --> 00:05:45.892
And that's what
this approach is.

00:05:45.892 --> 00:05:47.600
Finally, the last
approach, in some ways,

00:05:47.600 --> 00:05:50.640
is actually the simplest and
maybe even the most intuitive.

00:05:50.640 --> 00:05:53.890
It's actually to just
reason by analogy.

00:05:53.890 --> 00:05:56.060
There's a lot of
evidence in psychology

00:05:56.060 --> 00:05:58.770
that humans do
this all the time.

00:05:58.770 --> 00:06:00.720
You're faced with
a new situation,

00:06:00.720 --> 00:06:03.840
you try to find a matching
situation in your experience,

00:06:03.840 --> 00:06:05.330
and then you
transfer the solution

00:06:05.330 --> 00:06:06.788
from the situation
that you already

00:06:06.788 --> 00:06:10.760
know to the new situation
that you're faced with.

00:06:10.760 --> 00:06:14.330
And connected with each of
these approaches to learning,

00:06:14.330 --> 00:06:17.780
there is a school of
thought in machine learning.

00:06:17.780 --> 00:06:21.680
So the five main ones are the
Symbolists, Connectionists,

00:06:21.680 --> 00:06:24.940
Evolutionaries, Bayesians,
and Analogizers.

00:06:24.940 --> 00:06:26.360
The Symbolists
are the people who

00:06:26.360 --> 00:06:28.590
believe in discovering
new knowledge

00:06:28.590 --> 00:06:30.300
by filling in the
gaps in the knowledge

00:06:30.300 --> 00:06:31.940
that you already have.

00:06:31.940 --> 00:06:35.160
One of the things that's
fascinating about machine

00:06:35.160 --> 00:06:37.870
learning is that the
ideas in the algorithms

00:06:37.870 --> 00:06:40.360
come from all of these
different fields.

00:06:40.360 --> 00:06:43.190
So for example, the Symbolists,
they have their origins

00:06:43.190 --> 00:06:44.630
in logic, philosophy.

00:06:44.630 --> 00:06:47.220
And they're, in some sense,
the most "computer-sciency"

00:06:47.220 --> 00:06:50.090
of the five tribes.

00:06:50.090 --> 00:06:51.541
The Connectionists,
their origins

00:06:51.541 --> 00:06:53.540
are, of course, in
neuroscience, because they're

00:06:53.540 --> 00:06:56.530
trying to take inspiration
from how the brain works.

00:06:56.530 --> 00:06:58.590
The Evolutionaries,
well, their origins

00:06:58.590 --> 00:07:00.990
are, of course, in
evolutionary biology,

00:07:00.990 --> 00:07:03.950
in the algorithm of evolution.

00:07:03.950 --> 00:07:06.520
The Bayesians come
from statistics.

00:07:06.520 --> 00:07:08.480
The Analogizers
actually have influences

00:07:08.480 --> 00:07:10.980
from a lot of different fields,
but probably the single most

00:07:10.980 --> 00:07:12.820
important one is psychology.

00:07:12.820 --> 00:07:17.430
So in addition to being very
important for our lives,

00:07:17.430 --> 00:07:19.340
machine learning is also
a fascinating thing,

00:07:19.340 --> 00:07:22.560
I think, to study, because in
the process of studying machine

00:07:22.560 --> 00:07:26.350
learning, you can actually study
all of these different things.

00:07:26.350 --> 00:07:29.910
Now each of these "tribes" of
machine learning, if you will,

00:07:29.910 --> 00:07:34.430
has its own master algorithm,
meaning its own general purpose

00:07:34.430 --> 00:07:38.400
learner that, in principle,
can be used to learn anything.

00:07:38.400 --> 00:07:40.200
In fact, each of these
master algorithms

00:07:40.200 --> 00:07:42.530
has a mathematical
proof that says,

00:07:42.530 --> 00:07:45.300
if you give it enough data,
it can learn anything.

00:07:45.300 --> 00:07:46.130
OK?

00:07:46.130 --> 00:07:48.870
For the Symbolists, the master
algorithm is inverse deduction.

00:07:48.870 --> 00:07:50.780
And we'll see, in a
second, what that is.

00:07:50.780 --> 00:07:53.390
For the Connectionists,
it's backpropagation.

00:07:53.390 --> 00:07:56.050
For the Evolutionaries,
it's genetic programming.

00:07:56.050 --> 00:07:58.520
For the Bayesians, it's
probabilistic inference

00:07:58.520 --> 00:07:59.750
using Bayes' theorem.

00:07:59.750 --> 00:08:03.080
And for the Analogizers, it's
kernel machines, also known

00:08:03.080 --> 00:08:04.350
as support vector machines.

00:08:04.350 --> 00:08:05.080
OK?

00:08:05.080 --> 00:08:11.020
So let's see what just the key
ideas in each one of these are.

00:08:11.020 --> 00:08:12.624
So the Symbolists--
here are some

00:08:12.624 --> 00:08:14.540
of the most prominent
Symbolists in the world.

00:08:14.540 --> 00:08:16.206
There's Tom Mitchell
at Carnegie Mellon,

00:08:16.206 --> 00:08:21.820
Steve Muggleton in the UK,
and Russ Quinlan in Australia.

00:08:21.820 --> 00:08:26.230
And their idea is actually
a very interesting one.

00:08:26.230 --> 00:08:28.960
It's to think of
deduction-- sorry,

00:08:28.960 --> 00:08:33.640
it's to think of learning as
being the inverse of deduction.

00:08:33.640 --> 00:08:35.330
Learning is induction
of knowledge.

00:08:35.330 --> 00:08:35.830
Right?

00:08:35.830 --> 00:08:39.150
Deduction is going from general
rules to specific facts.

00:08:39.150 --> 00:08:40.580
Induction is the opposite.

00:08:40.580 --> 00:08:42.980
It's going from specific
facts to general rules.

00:08:42.980 --> 00:08:46.194
So in some sense, one is
the inverse of the other.

00:08:46.194 --> 00:08:47.610
And so maybe we
can figure out how

00:08:47.610 --> 00:08:50.480
to do induction in the same
way that people in mathematics

00:08:50.480 --> 00:08:53.060
figure out how to do
other inverse operations.

00:08:53.060 --> 00:08:56.450
Like, for example, subtraction
is the inverse of addition,

00:08:56.450 --> 00:09:01.610
or integration is the inverse of
differentiation, and so forth.

00:09:01.610 --> 00:09:03.684
So as a very, very
simple example,

00:09:03.684 --> 00:09:05.600
addition gives you the
answer to the question,

00:09:05.600 --> 00:09:08.980
if I add 2 and 2, what do I get.

00:09:08.980 --> 00:09:11.840
Subtraction-- and the
answer, of course, is 4.

00:09:11.840 --> 00:09:15.440
And this is the deepest thing
I'll say in this whole talk.

00:09:15.440 --> 00:09:18.460
And subtraction, of course,
gives you the answer

00:09:18.460 --> 00:09:21.460
to the inverse question,
which is, what do I

00:09:21.460 --> 00:09:25.410
need to add to 2 in order to
get 4, the answer, of course,

00:09:25.410 --> 00:09:27.060
being 2.

00:09:27.060 --> 00:09:33.300
Now inverse deduction works
in a very similar way.

00:09:33.300 --> 00:09:35.760
So here's a simple
example of deduction.

00:09:35.760 --> 00:09:37.930
You know that Socrates
is human and you

00:09:37.930 --> 00:09:39.680
know that humans are mortal.

00:09:39.680 --> 00:09:41.820
And the question is, what
can you infer from that.

00:09:41.820 --> 00:09:43.778
Well, of course, the
answer is that, from that,

00:09:43.778 --> 00:09:46.900
you can infer that
Socrates, too, is mortal.

00:09:46.900 --> 00:09:48.610
Now the inverse of
this-- and that's

00:09:48.610 --> 00:09:52.630
when it becomes induction--
is, if I know that Socrates

00:09:52.630 --> 00:09:56.480
is human, what else do I
need to know in order to be

00:09:56.480 --> 00:09:58.840
able to infer that he's mortal.

00:09:58.840 --> 00:10:02.530
And of course, what I need to
know is that humans are mortal.

00:10:02.530 --> 00:10:05.900
And so in this way, I have just
introduced a new general rule

00:10:05.900 --> 00:10:06.970
that humans are mortal.

00:10:06.970 --> 00:10:08.280
Of course, in general,
I wouldn't just

00:10:08.280 --> 00:10:09.696
do it from Socrates,
I would do it

00:10:09.696 --> 00:10:12.550
from Socrates and a
bunch of other people.

00:10:12.550 --> 00:10:14.600
But that's the general
way that this works.

00:10:14.600 --> 00:10:15.470
OK?

00:10:15.470 --> 00:10:17.510
And then once I've
induced rules like this,

00:10:17.510 --> 00:10:20.040
I can now combine them in
all sorts of different ways

00:10:20.040 --> 00:10:22.980
to answer questions that I may
never even have thought of.

00:10:22.980 --> 00:10:24.970
And this kind of
flexibility and composition

00:10:24.970 --> 00:10:27.600
is actually something that,
of all the five tribes,

00:10:27.600 --> 00:10:30.660
only the Symbolists have.

00:10:30.660 --> 00:10:33.820
Now of course, these
examples are in English

00:10:33.820 --> 00:10:36.400
and computers don't understand
natural language yet.

00:10:36.400 --> 00:10:39.140
So what they use is something
like first order logic.

00:10:39.140 --> 00:10:41.080
So these things, both
the facts and the rules

00:10:41.080 --> 00:10:43.330
that are discovered,
are represented

00:10:43.330 --> 00:10:44.490
in first order logic.

00:10:44.490 --> 00:10:47.392
And then questions are answered
by chaining those rules,

00:10:47.392 --> 00:10:48.350
by reasoning with them.

00:10:48.350 --> 00:10:49.120
OK?

00:10:49.120 --> 00:10:51.050
But whether it's in logic
or natural language,

00:10:51.050 --> 00:10:52.730
the principle is the same.

00:10:52.730 --> 00:10:53.670
OK?

00:10:53.670 --> 00:10:56.360
And as I said, of all
the five paradigms,

00:10:56.360 --> 00:10:58.580
this is the one that is most
like scientists at work.

00:10:58.580 --> 00:10:59.080
Right?

00:10:59.080 --> 00:11:03.160
They figure out, where are
the gaps in my knowledge.

00:11:03.160 --> 00:11:05.230
Let me enunciate a
general principle

00:11:05.230 --> 00:11:06.660
that will fill that gap.

00:11:06.660 --> 00:11:08.080
And then let me
see what follows.

00:11:08.080 --> 00:11:09.950
Let me see if it's
correct given the data.

00:11:09.950 --> 00:11:12.920
Let me see what gaps I
identified and so on.

00:11:12.920 --> 00:11:16.000
And in fact, one of the
most amazing applications

00:11:16.000 --> 00:11:21.770
of inverse deduction to date
is actually a robot scientist.

00:11:21.770 --> 00:11:25.160
So if you look at this
picture, the biologist

00:11:25.160 --> 00:11:27.100
is not the guy in the lab coat.

00:11:27.100 --> 00:11:28.670
The guy in the lab
coat is actually

00:11:28.670 --> 00:11:31.020
a computer scientist and
machine learning researcher

00:11:31.020 --> 00:11:33.790
by the name of Ross King.

00:11:33.790 --> 00:11:38.790
The biologist in this
picture is this machine here.

00:11:38.790 --> 00:11:42.780
This machine is a complete,
automated biologist.

00:11:42.780 --> 00:11:44.880
It starts out with
basic knowledge

00:11:44.880 --> 00:11:48.400
of molecular biology,
DNA, proteins, RNA,

00:11:48.400 --> 00:11:50.086
and all of that stuff.

00:11:50.086 --> 00:11:51.460
And then what it
actually does is

00:11:51.460 --> 00:11:56.000
it formulates hypotheses
using inverse deduction.

00:11:56.000 --> 00:11:59.320
It designs experiments to test
this hypothesis using things

00:11:59.320 --> 00:12:01.670
like DNA sequences and whatnot.

00:12:01.670 --> 00:12:03.700
That's what's on there.

00:12:03.700 --> 00:12:06.490
It physically carries them out.

00:12:06.490 --> 00:12:09.000
So it does the whole
process with no human help.

00:12:09.000 --> 00:12:11.990
And then, given the results,
it refines the hypotheses,

00:12:11.990 --> 00:12:14.410
or comes up with
new ones, and so on.

00:12:14.410 --> 00:12:15.557
OK?

00:12:15.557 --> 00:12:17.890
Now there's only one of these
robots in the world today.

00:12:17.890 --> 00:12:18.890
Its name is Eve.

00:12:18.890 --> 00:12:21.680
There was actually a
previous one called Adam.

00:12:21.680 --> 00:12:26.296
And Eve, last year,
discovered a new malaria drug.

00:12:26.296 --> 00:12:27.920
And the thing that's
amazing about this

00:12:27.920 --> 00:12:31.310
is that, well, once you've made
one robot scientist like this,

00:12:31.310 --> 00:12:34.864
there's nothing stopping
you from making a million.

00:12:34.864 --> 00:12:37.030
And I have a million
scientists working on a problem

00:12:37.030 --> 00:12:40.500
where, before, maybe all
we could afford was a few.

00:12:40.500 --> 00:12:42.802
So this can really speed
up the progress of science.

00:12:42.802 --> 00:12:45.010
And I would say that in
areas like molecular biology,

00:12:45.010 --> 00:12:47.850
there is no hope of really
understanding, for example,

00:12:47.850 --> 00:12:50.780
very well, how cells work
without this kind of thing.

00:12:50.780 --> 00:12:51.280
Right?

00:12:51.280 --> 00:12:54.240
There's just too much
information for human beings

00:12:54.240 --> 00:12:56.000
to discover on their own.

00:12:56.000 --> 00:12:57.310
OK?

00:12:57.310 --> 00:13:00.689
Now this is one very,
I think, attractive way

00:13:00.689 --> 00:13:03.230
to do machine learning, but of
course, it's not the only one.

00:13:03.230 --> 00:13:04.604
In particular,
the Connectionists

00:13:04.604 --> 00:13:05.760
are very skeptical of it.

00:13:05.760 --> 00:13:07.900
They say, well, this
is too abstract.

00:13:07.900 --> 00:13:08.930
It's too clean.

00:13:08.930 --> 00:13:09.930
It's too rigid.

00:13:09.930 --> 00:13:12.660
Logic is not how the
real world works.

00:13:12.660 --> 00:13:15.540
You have to do things
more like human beings do.

00:13:15.540 --> 00:13:17.180
And of course, the
way human beings

00:13:17.180 --> 00:13:19.480
do things is with their
brains, so let's figure out

00:13:19.480 --> 00:13:23.120
how brains work, at least enough
that we can take inspiration

00:13:23.120 --> 00:13:25.480
from them and build
algorithms based on that.

00:13:25.480 --> 00:13:27.940
So this is what the
Connectionists do.

00:13:27.940 --> 00:13:31.710
The most prominent
Connectionist is Geoff Hinton.

00:13:31.710 --> 00:13:34.550
He actually started out as
a psychologist 40 years ago

00:13:34.550 --> 00:13:37.920
and now he's really more
of a computer scientist.

00:13:37.920 --> 00:13:40.930
And he's been
incredibly determined

00:13:40.930 --> 00:13:44.120
in his goal of understanding
how the brain learns.

00:13:44.120 --> 00:13:46.020
In fact, he tells the
story that one day he

00:13:46.020 --> 00:13:48.030
came home from
work very excited,

00:13:48.030 --> 00:13:50.250
saying, yea, I've done it.

00:13:50.250 --> 00:13:51.930
I've figured out
how the brain works.

00:13:51.930 --> 00:13:54.235
And his daughter replied,
oh dad, not again.

00:13:57.920 --> 00:13:59.750
And he's the first one
to say that he's had

00:13:59.750 --> 00:14:01.270
some successes and failures.

00:14:01.270 --> 00:14:03.974
But I think the bottom line is
that, these days-- for example,

00:14:03.974 --> 00:14:05.640
he's one of the
co-inventors, of course,

00:14:05.640 --> 00:14:08.400
of backpropagation--
the successes far

00:14:08.400 --> 00:14:09.470
outweigh the failures.

00:14:09.470 --> 00:14:12.270
So definitely, the
long-term research agenda

00:14:12.270 --> 00:14:13.700
has already given
a lot of fruits.

00:14:13.700 --> 00:14:15.887
And it's going to give more.

00:14:15.887 --> 00:14:17.470
And two other prominent
Connectionists

00:14:17.470 --> 00:14:19.920
are Yann LeCun and [INAUDIBLE].

00:14:19.920 --> 00:14:21.990
Of course, Connectionism
is also known

00:14:21.990 --> 00:14:24.550
as neural networks and,
these days, as deep learning.

00:14:24.550 --> 00:14:26.660
But it's really all
the same paradigm.

00:14:26.660 --> 00:14:27.690
OK?

00:14:27.690 --> 00:14:29.230
So how does it all work?

00:14:29.230 --> 00:14:31.760
Well, what we're
going to do is we're

00:14:31.760 --> 00:14:35.300
going to build a model, a
mathematical model of how

00:14:35.300 --> 00:14:37.090
a single neuron works.

00:14:37.090 --> 00:14:39.850
We're going to make it as
simple as we can provided

00:14:39.850 --> 00:14:42.640
it's enough to learn and to
do the inferences that we

00:14:42.640 --> 00:14:44.090
need it to do.

00:14:44.090 --> 00:14:44.700
OK?

00:14:44.700 --> 00:14:47.540
And then we're going to
put these models of neurons

00:14:47.540 --> 00:14:49.355
together into big networks.

00:14:49.355 --> 00:14:51.230
And then we're going to
train those networks.

00:14:51.230 --> 00:14:51.630
OK?

00:14:51.630 --> 00:14:52.840
And at the end of
the day, what we have

00:14:52.840 --> 00:14:54.390
is, in some sense,
a miniature brain,

00:14:54.390 --> 00:14:56.972
if you will-- of course, much
simpler than the real one,

00:14:56.972 --> 00:14:58.930
but hopefully with some
of the same properties.

00:14:58.930 --> 00:14:59.740
OK?

00:14:59.740 --> 00:15:03.140
So now a neuron is a very
interesting kind of cell.

00:15:03.140 --> 00:15:04.060
All right?

00:15:04.060 --> 00:15:06.570
It's a cell that actually
looks like a tree.

00:15:06.570 --> 00:15:08.480
There's the cell
body, and then there's

00:15:08.480 --> 00:15:11.090
the trunk of the tree is
what is called the axon,

00:15:11.090 --> 00:15:13.890
and then the branches
are called dendrites.

00:15:16.680 --> 00:15:19.920
But where neurons get
very different from trees

00:15:19.920 --> 00:15:22.260
is that the branches
of one neuron

00:15:22.260 --> 00:15:25.860
actually connect back with the
roots of another-- or in fact,

00:15:25.860 --> 00:15:27.110
with the roots of many others.

00:15:27.110 --> 00:15:29.970
And that's how you get a
big network of neurons.

00:15:29.970 --> 00:15:32.590
And where the
dendrites of one neuron

00:15:32.590 --> 00:15:36.850
join the dendrites of another,
that's called the synapse.

00:15:36.850 --> 00:15:39.070
And to the best
of our knowledge,

00:15:39.070 --> 00:15:41.273
the way humans
learn everything you

00:15:41.273 --> 00:15:44.210
know is encoded in the
strings of the synapses

00:15:44.210 --> 00:15:45.800
between your neurons.

00:15:45.800 --> 00:15:46.500
OK?

00:15:46.500 --> 00:15:50.314
So if the synapse
is strong, then--

00:15:50.314 --> 00:15:52.440
let me backtrack
for just a second.

00:15:52.440 --> 00:15:54.980
The way this works
is that the neurons

00:15:54.980 --> 00:15:58.436
communicate via electric
discharges down their axons.

00:15:58.436 --> 00:16:00.060
They're literally an
electric discharge

00:16:00.060 --> 00:16:01.790
called an action potential.

00:16:01.790 --> 00:16:05.040
And what happens is that if you
look at all the charge coming

00:16:05.040 --> 00:16:08.040
into a neuron through
its various synapses,

00:16:08.040 --> 00:16:10.740
if it exceeds a certain
threshold, then that neuron

00:16:10.740 --> 00:16:12.130
itself fires.

00:16:12.130 --> 00:16:14.320
And of course, then
it sends currents

00:16:14.320 --> 00:16:16.240
to the neurons downstream.

00:16:16.240 --> 00:16:16.740
OK?

00:16:16.740 --> 00:16:19.520
And the synaptic process itself
involves chemistry and whatnot,

00:16:19.520 --> 00:16:22.110
but those details are
not important for us.

00:16:22.110 --> 00:16:23.040
OK?

00:16:23.040 --> 00:16:28.660
So what's going to happen is
that the learning basically

00:16:28.660 --> 00:16:32.870
happens when a neuron helps
to make another neuron fire.

00:16:32.870 --> 00:16:34.870
And then the strength of
the connection goes up.

00:16:34.870 --> 00:16:38.060
This is called Hebb's Rule.

00:16:38.060 --> 00:16:39.480
And as far as we
know, this is how

00:16:39.480 --> 00:16:42.580
all our knowledge is encoded is
in how strong the synapses are.

00:16:42.580 --> 00:16:44.040
If the neurons fire
together a lot,

00:16:44.040 --> 00:16:46.086
then the synapses
become stronger

00:16:46.086 --> 00:16:47.835
and it becomes easier
for the first neuron

00:16:47.835 --> 00:16:49.900
to fire the second one.

00:16:49.900 --> 00:16:51.310
So this is the basic idea.

00:16:51.310 --> 00:16:54.630
Now what we have to do is, first
of all, turn it into a model.

00:16:54.630 --> 00:16:56.790
That's not that hard.

00:16:56.790 --> 00:16:58.120
So the model is just this.

00:16:58.120 --> 00:17:00.150
What my neuron's
going to do is it's

00:17:00.150 --> 00:17:02.650
going to do a weighted
combination of the inputs.

00:17:02.650 --> 00:17:03.227
OK?

00:17:03.227 --> 00:17:05.810
Let's suppose, for example, that
this was actually the retina.

00:17:05.810 --> 00:17:05.980
All right?

00:17:05.980 --> 00:17:07.150
So this is the input.

00:17:07.150 --> 00:17:09.099
Each one of these
inputs is a pixel.

00:17:09.099 --> 00:17:10.810
And then each one of
them gets multiplied

00:17:10.810 --> 00:17:13.890
by a weight that corresponds
to the strength of the synapse.

00:17:13.890 --> 00:17:17.420
And if that weighted
sum exceeds a threshold,

00:17:17.420 --> 00:17:18.930
then I get 1 as the output.

00:17:18.930 --> 00:17:20.530
Otherwise, I get 0.

00:17:20.530 --> 00:17:23.510
So for example, if this neuron
is trying to recognize a cat,

00:17:23.510 --> 00:17:25.490
if this is a cat, then
hopefully what happens

00:17:25.490 --> 00:17:28.380
is that this weighted
sum will be high enough

00:17:28.380 --> 00:17:31.970
that this will fire and
the neuron will say,

00:17:31.970 --> 00:17:34.200
yes, this is a cat.

00:17:34.200 --> 00:17:35.310
Now this is all easy.

00:17:35.310 --> 00:17:39.370
This goes back to the 1950s, and
Frank Rosenblatt, and whatnot.

00:17:39.370 --> 00:17:42.240
The really hard, and
interesting, and important

00:17:42.240 --> 00:17:47.260
question is, how do you train a
whole network of these neurons.

00:17:47.260 --> 00:17:49.590
That's when things actually
become very difficult.

00:17:49.590 --> 00:17:51.980
Because if you have a
big network of neurons--

00:17:51.980 --> 00:17:53.230
so here are my inputs.

00:17:53.230 --> 00:17:54.620
They go to one set of neurons.

00:17:54.620 --> 00:17:55.120
Right?

00:17:55.120 --> 00:17:57.510
These are the functions
that they compute.

00:17:57.510 --> 00:17:58.010
Right?

00:17:58.010 --> 00:17:59.270
They're in purple here.

00:17:59.270 --> 00:18:02.300
And then, those go to another
layer, and many, many layers,

00:18:02.300 --> 00:18:04.390
until finally, you
get the output.

00:18:04.390 --> 00:18:08.150
Another question is, well, if
the output is correct-- right?

00:18:08.150 --> 00:18:11.250
So let's say this was a cat
and the network said that, yes,

00:18:11.250 --> 00:18:13.550
it is a cat, then life is good.

00:18:13.550 --> 00:18:14.570
Nothing needs to change.

00:18:14.570 --> 00:18:15.070
Right?

00:18:15.070 --> 00:18:16.920
Why go fixing what ain't broke?

00:18:16.920 --> 00:18:19.230
But what happens if
there was an error?

00:18:19.230 --> 00:18:19.750
Right?

00:18:19.750 --> 00:18:22.800
This should have been
firing, but wasn't.

00:18:22.800 --> 00:18:25.680
The key question
is, what do I change

00:18:25.680 --> 00:18:29.090
in that whole big, messy
network to try to make it

00:18:29.090 --> 00:18:31.717
give the right answer tomorrow.

00:18:31.717 --> 00:18:33.550
There is no obvious
answer to this question.

00:18:33.550 --> 00:18:35.252
Because think of
one neuron somewhere

00:18:35.252 --> 00:18:36.460
in the middle of the network.

00:18:36.460 --> 00:18:38.700
How is it responsible for
the error at the output?

00:18:38.700 --> 00:18:39.934
All right?

00:18:39.934 --> 00:18:41.350
The error at the
output could have

00:18:41.350 --> 00:18:43.580
come from an infinitude
of different places.

00:18:43.580 --> 00:18:45.519
This is called the credit
assignment problem.

00:18:45.519 --> 00:18:47.560
Or maybe it should be
called the blame assignment

00:18:47.560 --> 00:18:50.110
problem, because it's deciding
who's to blame for an error,

00:18:50.110 --> 00:18:51.810
and therefore, needs to change.

00:18:51.810 --> 00:18:55.240
And this is the problem
that backpropagation solves.

00:18:55.240 --> 00:18:55.850
All right?

00:18:55.850 --> 00:18:58.882
And when people first came
up with neural networks

00:18:58.882 --> 00:19:00.340
in the '60s, they
didn't have this.

00:19:00.340 --> 00:19:03.560
It was when, finally,
it was invented

00:19:03.560 --> 00:19:06.830
in the '80s by David
Rumelhart and others

00:19:06.830 --> 00:19:08.810
that things really took off.

00:19:08.810 --> 00:19:13.021
And the basic idea in backprop
is actually quite intuitive,

00:19:13.021 --> 00:19:13.520
I think.

00:19:13.520 --> 00:19:14.700
It's the following.

00:19:14.700 --> 00:19:17.510
Well, let's think
of the difference

00:19:17.510 --> 00:19:20.112
between my actual output
and my desired output.

00:19:20.112 --> 00:19:21.070
Let me call that delta.

00:19:21.070 --> 00:19:21.570
Right?

00:19:21.570 --> 00:19:22.340
This is the error.

00:19:22.340 --> 00:19:25.640
The output should have
been 1, but it was 0.2.

00:19:25.640 --> 00:19:27.770
So it needs to go up.

00:19:27.770 --> 00:19:31.440
Another question is, what
can I tweak in these weights

00:19:31.440 --> 00:19:32.650
to make it go up.

00:19:32.650 --> 00:19:33.780
All right?

00:19:33.780 --> 00:19:38.980
Well, a weight that is
thinking at this last layer,

00:19:38.980 --> 00:19:41.230
well, at that last layer,
the neurons with the highest

00:19:41.230 --> 00:19:44.000
weights are the ones most
responsible for the result.

00:19:44.000 --> 00:19:46.322
And if this neuron is
saying no but the answer

00:19:46.322 --> 00:19:47.780
is yes, well, then
its weight needs

00:19:47.780 --> 00:19:50.120
to go down, because it's
preventing it from firing.

00:19:50.120 --> 00:19:55.340
And if this one was saying
yes but this is not firing,

00:19:55.340 --> 00:19:58.300
then its weight needs to go up.

00:19:58.300 --> 00:20:00.280
So what I do is I
compute the derivative

00:20:00.280 --> 00:20:04.520
of this error with respect to
the weights in the last layer.

00:20:04.520 --> 00:20:07.640
And then with those, I
now have an error signal

00:20:07.640 --> 00:20:08.860
coming from these neurons.

00:20:08.860 --> 00:20:11.040
It's like, oh, you
should have been higher.

00:20:11.040 --> 00:20:11.712
All right?

00:20:11.712 --> 00:20:14.170
You should have been higher in
order to make this guy fire,

00:20:14.170 --> 00:20:15.550
because you have
a positive weight.

00:20:15.550 --> 00:20:17.120
You should have been
lower, because you

00:20:17.120 --> 00:20:18.040
have a negative
weight and you're

00:20:18.040 --> 00:20:19.123
preventing it from firing.

00:20:19.123 --> 00:20:22.242
So now I know an error signal
at the neurons at this layer.

00:20:22.242 --> 00:20:23.700
And I can keep
doing the same thing

00:20:23.700 --> 00:20:25.777
all the way back to the input.

00:20:25.777 --> 00:20:27.610
And this is why it's
called backpropagation.

00:20:27.610 --> 00:20:30.750
Because what I'm doing is I'm
propagating back the errors

00:20:30.750 --> 00:20:33.470
and then updating the weights,
changing the weights in order

00:20:33.470 --> 00:20:36.830
to make that error
as small as possible.

00:20:36.830 --> 00:20:37.930
OK?

00:20:37.930 --> 00:20:40.030
Well, this is the
backpropagation algorithm.

00:20:40.030 --> 00:20:42.380
It's what's at the
heart of deep learning.

00:20:42.380 --> 00:20:46.080
And these days, this is used for
just about everything on Earth.

00:20:46.080 --> 00:20:46.960
Right?

00:20:46.960 --> 00:20:48.960
Very early on, people
used it to do things

00:20:48.960 --> 00:20:50.570
like predict the stock market.

00:20:50.570 --> 00:20:52.505
These days, you
use it for search,

00:20:52.505 --> 00:20:55.380
use it for ad placement, use
it for video recognition,

00:20:55.380 --> 00:20:58.480
use it for speech
recognition, use it

00:20:58.480 --> 00:21:01.930
for things like simultaneous
translation and whatnot.

00:21:01.930 --> 00:21:04.140
But I think, at
least for the public,

00:21:04.140 --> 00:21:06.460
the best known instance
of this is still

00:21:06.460 --> 00:21:10.400
Google's own famous cat network.

00:21:10.400 --> 00:21:13.402
Unfortunately, people think
it's-- it got called "the cat

00:21:13.402 --> 00:21:14.860
network" by the
journalists, right,

00:21:14.860 --> 00:21:17.290
because it happened to
recognize cats very well.

00:21:17.290 --> 00:21:19.000
Actually, it wasn't
an accident, right?

00:21:19.000 --> 00:21:20.850
This was all learned
from YouTube videos,

00:21:20.850 --> 00:21:22.270
right, as you probably know.

00:21:22.270 --> 00:21:25.650
And people really like to
upload videos of their cats.

00:21:25.650 --> 00:21:28.490
So there was more data on
cats than on anything else.

00:21:28.490 --> 00:21:31.180
But maybe it should be
called "the couch potato

00:21:31.180 --> 00:21:36.180
network," because it's based
on watching a lot of video.

00:21:36.180 --> 00:21:39.402
And at the time, this was
the biggest neural network

00:21:39.402 --> 00:21:41.110
ever built. It had,
I think, on the order

00:21:41.110 --> 00:21:42.109
of a billion parameters.

00:21:42.109 --> 00:21:44.550
But of course, these days,
there's much bigger ones

00:21:44.550 --> 00:21:47.290
and they're continuing to grow.

00:21:47.290 --> 00:21:51.950
So we'll see how far we
can take this paradigm.

00:21:51.950 --> 00:21:54.520
Now the Evolutionaries,
they say, well sure,

00:21:54.520 --> 00:21:57.500
but all you're doing is
adjusting the weights

00:21:57.500 --> 00:21:59.030
on this model.

00:21:59.030 --> 00:22:01.870
You're just tweaking the
strengths of the synapses.

00:22:01.870 --> 00:22:04.090
Where did that brain come from?

00:22:04.090 --> 00:22:06.530
That, in some sense,
is the bigger question.

00:22:06.530 --> 00:22:09.081
That brain was
produced by evolution.

00:22:09.081 --> 00:22:11.580
So maybe what we really should
do if we want to learn really

00:22:11.580 --> 00:22:14.972
powerful things is figure
out how evolution works.

00:22:14.972 --> 00:22:16.680
And we actually already
have a good sense

00:22:16.680 --> 00:22:17.638
of how evolution works.

00:22:17.638 --> 00:22:19.400
There's a lot of
details, of course, that

00:22:19.400 --> 00:22:20.525
are still to be understood.

00:22:20.525 --> 00:22:23.180
But at a high level, we
understand how evolution works.

00:22:23.180 --> 00:22:26.500
So let us do the same
thing on the computer.

00:22:26.500 --> 00:22:28.620
And the first guy who
really ran with this idea

00:22:28.620 --> 00:22:29.710
was John Holland.

00:22:29.710 --> 00:22:31.940
He started in the '50s, '60s.

00:22:31.940 --> 00:22:36.460
And for a long time, this whole
area of evolutionary computing,

00:22:36.460 --> 00:22:38.610
the joke was that it
was John, his students,

00:22:38.610 --> 00:22:40.040
and their students.

00:22:40.040 --> 00:22:42.450
But then in the '80s, things
took off and a lot of people

00:22:42.450 --> 00:22:43.720
started doing it.

00:22:43.720 --> 00:22:46.570
John Holland called what he
did "genetic algorithms."

00:22:46.570 --> 00:22:49.400
And then John Koza developed
a more powerful version

00:22:49.400 --> 00:22:52.150
called genetic programming,
which we'll see in a second.

00:22:52.150 --> 00:22:55.159
And Hod Lipson is
one of the people

00:22:55.159 --> 00:22:56.700
who are doing very
interesting things

00:22:56.700 --> 00:23:02.410
with evolutionary learning these
days, as we'll see in a bit.

00:23:02.410 --> 00:23:04.750
So what's the basic idea here?

00:23:04.750 --> 00:23:06.700
Well, how does evolution work?

00:23:06.700 --> 00:23:09.830
You have a population of
individuals, each of which

00:23:09.830 --> 00:23:12.110
is described by its genome.

00:23:12.110 --> 00:23:12.750
OK?

00:23:12.750 --> 00:23:14.430
But in our case,
in the computer,

00:23:14.430 --> 00:23:16.220
the genome, instead
of being base pairs,

00:23:16.220 --> 00:23:18.553
instead of being DNA, it's
just going to be bits, right,

00:23:18.553 --> 00:23:21.810
because 0 and 1, in some sense,
are the DNA of computers.

00:23:21.810 --> 00:23:23.420
And then each of
these individuals

00:23:23.420 --> 00:23:26.772
gets to go out in the
world and be evaluated.

00:23:26.772 --> 00:23:30.930
It gets evaluated at the task
that it's supposed to be doing.

00:23:30.930 --> 00:23:33.090
And the individuals
that do better

00:23:33.090 --> 00:23:35.320
will have a higher
fitness and will therefore

00:23:35.320 --> 00:23:37.240
have a higher chance
of being the parents

00:23:37.240 --> 00:23:40.080
of the next generation.

00:23:40.080 --> 00:23:43.250
You get two very fit parents
and you cross over their genomes

00:23:43.250 --> 00:23:45.620
just like people do.

00:23:45.620 --> 00:23:47.640
And so now you
have a child genome

00:23:47.640 --> 00:23:49.850
that is partly the
genome of one parent

00:23:49.850 --> 00:23:52.470
and partly the genome of
the other parent, the mother

00:23:52.470 --> 00:23:55.834
and father genomes, if you will.

00:23:55.834 --> 00:23:57.750
And then you also have
random mutation, right?

00:23:57.750 --> 00:24:00.320
Some bits just get
randomly mutated because

00:24:00.320 --> 00:24:02.090
of copying errors or whatever.

00:24:02.090 --> 00:24:04.491
And then you have
a new population.

00:24:04.491 --> 00:24:06.490
And what's remarkable is
that you could actually

00:24:06.490 --> 00:24:09.094
start out with a population
that is essentially random.

00:24:09.094 --> 00:24:11.260
And after you do some number
of generations of this,

00:24:11.260 --> 00:24:13.050
you actually have
things that are doing

00:24:13.050 --> 00:24:16.100
a lot of non-trivial functions.

00:24:16.100 --> 00:24:18.700
Like, for example, you
can evolve circuits

00:24:18.700 --> 00:24:22.990
that-- you can develop radio
receivers, and amplifiers,

00:24:22.990 --> 00:24:25.270
and things like that
in just this way.

00:24:25.270 --> 00:24:27.510
And they often work
better than the ones

00:24:27.510 --> 00:24:29.280
that are designed by humans.

00:24:29.280 --> 00:24:30.880
In fact, people
like John Koza, they

00:24:30.880 --> 00:24:34.070
have a whole bunch of
patents that were actually--

00:24:34.070 --> 00:24:36.490
the patent designs
were actually invented

00:24:36.490 --> 00:24:39.230
by the genetic algorithms,
not by the humans.

00:24:39.230 --> 00:24:41.320
OK?

00:24:41.320 --> 00:24:47.770
Now Koza's idea was actually
to take this one step beyond.

00:24:47.770 --> 00:24:48.950
And that's the following.

00:24:48.950 --> 00:24:52.910
Well you know, these
strings are a very low level

00:24:52.910 --> 00:24:54.450
representation.

00:24:54.450 --> 00:24:58.290
And just cutting a string
in the middle to cross over

00:24:58.290 --> 00:25:00.631
is probably going
to muck things up.

00:25:00.631 --> 00:25:01.130
Right?

00:25:01.130 --> 00:25:02.930
We are trying to
evolve programs, right?

00:25:02.930 --> 00:25:03.530
At the end of the
day, that's all

00:25:03.530 --> 00:25:04.660
we're trying to do
is evolve programs.

00:25:04.660 --> 00:25:06.530
So why don't we
actually work directly

00:25:06.530 --> 00:25:08.820
with the programs themselves?

00:25:08.820 --> 00:25:13.770
And a program is really a tree
of subroutine calls all the way

00:25:13.770 --> 00:25:16.530
down to simple operations like
additions, multiplications,

00:25:16.530 --> 00:25:18.680
and ands, and ors.

00:25:18.680 --> 00:25:21.750
So let's represent
the problems as trees.

00:25:21.750 --> 00:25:24.779
And then in order to cross
over two programs, what we're

00:25:24.779 --> 00:25:26.570
going to do is we're
going to randomly pick

00:25:26.570 --> 00:25:28.460
a node in each of the
trees and then we're

00:25:28.460 --> 00:25:31.050
going to swap the
sub-trees at those nodes.

00:25:31.050 --> 00:25:33.020
And now we have
two child programs.

00:25:33.020 --> 00:25:33.870
OK?

00:25:33.870 --> 00:25:37.552
So for example, here, if I do
the crossover at this point,

00:25:37.552 --> 00:25:39.260
then one of the
sub-trees that I will get

00:25:39.260 --> 00:25:42.110
is the one that's all in white.

00:25:42.110 --> 00:25:43.530
And this tree
actually represents

00:25:43.530 --> 00:25:45.460
one of Kepler's laws.

00:25:45.460 --> 00:25:49.200
This is the law that gives the
duration of a planet's year

00:25:49.200 --> 00:25:51.500
as a function of its average
distance from the sun.

00:25:51.500 --> 00:25:53.430
It's actually a constant
times the square root

00:25:53.430 --> 00:25:54.596
of the cube of the distance.

00:25:54.596 --> 00:25:56.924
And that's what this
is representing.

00:25:56.924 --> 00:25:58.340
So genetic algorithms
can actually

00:25:58.340 --> 00:26:03.250
discover a thing like this and
much more complex ones as well.

00:26:03.250 --> 00:26:07.820
In fact, these days,
what the genetic folks

00:26:07.820 --> 00:26:11.030
are having a lot of
fun with is something

00:26:11.030 --> 00:26:14.350
that's exciting and
scary at the same time.

00:26:14.350 --> 00:26:16.720
They're not just doing
this with software anymore.

00:26:16.720 --> 00:26:19.210
They're not doing it as a
simulation inside the computer,

00:26:19.210 --> 00:26:21.790
but actually doing it out
in the physical, real world

00:26:21.790 --> 00:26:24.770
with robots.

00:26:24.770 --> 00:26:29.280
You literally start with robots
that are random piles of parts.

00:26:29.280 --> 00:26:31.250
And then once those
robots are good enough,

00:26:31.250 --> 00:26:35.470
they actually get
printed by a 3-D printer.

00:26:35.470 --> 00:26:37.450
And then they start
crawling, and walking,

00:26:37.450 --> 00:26:39.920
and doing things in the real
world-- seeing how fast they

00:26:39.920 --> 00:26:43.260
can crawl, trying to recover
from injury, and so on and so

00:26:43.260 --> 00:26:43.810
forth.

00:26:43.810 --> 00:26:46.860
This is actually a real
robot from Hod Lipson's lab.

00:26:46.860 --> 00:26:47.670
OK?

00:26:47.670 --> 00:26:50.580
And then what happens is
that in each generation,

00:26:50.580 --> 00:26:52.860
the fittest robots
get to program

00:26:52.860 --> 00:26:57.640
the 3-D printer to produce
the next generation of robots.

00:26:57.640 --> 00:26:59.260
So if "Terminator"
comes to pass,

00:26:59.260 --> 00:27:02.842
this might be how we get there.

00:27:02.842 --> 00:27:04.300
Of course, these
robots are not yet

00:27:04.300 --> 00:27:06.341
ready to take over the
world, but they've already

00:27:06.341 --> 00:27:08.540
come remarkably far
from the soup of parts

00:27:08.540 --> 00:27:09.590
that they began with.

00:27:09.590 --> 00:27:10.140
Right?

00:27:10.140 --> 00:27:13.250
And these days, if you see a
little spider running around,

00:27:13.250 --> 00:27:13.960
take a good look.

00:27:13.960 --> 00:27:15.918
Because it could be one
of these instead of one

00:27:15.918 --> 00:27:18.840
of the biological ones.

00:27:18.840 --> 00:27:19.340
All right.

00:27:19.340 --> 00:27:23.760
So the Bayesians-- so here
are some famous Bayesians.

00:27:23.760 --> 00:27:25.417
Judea Pearl, within
computer science,

00:27:25.417 --> 00:27:26.750
is probably the most famous one.

00:27:26.750 --> 00:27:28.810
He actually won the Turing
Award, the Nobel Prize

00:27:28.810 --> 00:27:30.439
of computer science,
a few years ago

00:27:30.439 --> 00:27:32.730
for inventing something called
Bayesian networks, which

00:27:32.730 --> 00:27:35.120
is one very powerful
type of Bayesian model

00:27:35.120 --> 00:27:37.870
where it's a big graph where
each node is a variable.

00:27:37.870 --> 00:27:40.810
And then the edges
between the nodes

00:27:40.810 --> 00:27:43.590
represent dependencies
between the variables.

00:27:43.590 --> 00:27:46.010
Two other famous Bayesians
are David Heckerman

00:27:46.010 --> 00:27:48.200
and Mike Jordan.

00:27:48.200 --> 00:27:49.980
Bayesians are known,
in machine learning,

00:27:49.980 --> 00:27:53.672
as the most fanatical
of the five tribes.

00:27:53.672 --> 00:27:54.330
All right?

00:27:54.330 --> 00:27:56.764
They really have a
near-religious devotion

00:27:56.764 --> 00:27:58.430
to their paradigm,
and you know, they're

00:27:58.430 --> 00:28:00.550
the first ones to say so.

00:28:00.550 --> 00:28:04.227
And I think the reason for
this is that Bayesians had

00:28:04.227 --> 00:28:05.720
their origins in statistics.

00:28:05.720 --> 00:28:08.550
And for 200 years, Bayesians
were a persecuted minority

00:28:08.550 --> 00:28:10.200
in statistics.

00:28:10.200 --> 00:28:13.577
So they had to become very
hard-core in order to survive.

00:28:13.577 --> 00:28:15.410
And it's a good thing
that they did survive,

00:28:15.410 --> 00:28:17.300
because they have a
lot to contribute.

00:28:17.300 --> 00:28:19.880
And these days, with
computers, and algorithms

00:28:19.880 --> 00:28:23.380
like Markov chain Monte Carlo,
and large data, and whatnot,

00:28:23.380 --> 00:28:26.500
they're actually on the
ascendant in statistics.

00:28:26.500 --> 00:28:30.690
So what is the basic idea
behind Bayesian learning?

00:28:30.690 --> 00:28:33.390
Well, the basic idea
is that, as I said,

00:28:33.390 --> 00:28:35.480
everything that you
learn is uncertain.

00:28:35.480 --> 00:28:37.530
So what you have to do is
compute the probability

00:28:37.530 --> 00:28:40.070
of each one of your
hypotheses and then update it

00:28:40.070 --> 00:28:41.660
as new evidence comes in.

00:28:41.660 --> 00:28:44.400
And the way you do that
is with Bayes' theorem.

00:28:44.400 --> 00:28:45.290
OK?

00:28:45.290 --> 00:28:47.025
And Bayesians love
Bayes' theorem so much

00:28:47.025 --> 00:28:48.900
that there was this
Bayesian machine learning

00:28:48.900 --> 00:28:54.280
startup that actually had a neon
sign of Bayes' theorem made.

00:28:54.280 --> 00:28:57.410
And then they stuck it
outside their office.

00:28:57.410 --> 00:29:01.270
So there's Bayes' theorem
in big, neon letters.

00:29:01.270 --> 00:29:02.070
OK?

00:29:02.070 --> 00:29:05.420
So how does Bayes' theorem work?

00:29:05.420 --> 00:29:07.620
Bayes' theorem is actually
incredibly simple.

00:29:07.620 --> 00:29:09.400
It's so simple it's
barely worth being--

00:29:09.400 --> 00:29:11.650
it would be barely
worth calling a theorem

00:29:11.650 --> 00:29:13.330
if it wasn't so important.

00:29:13.330 --> 00:29:15.050
So the idea is this.

00:29:15.050 --> 00:29:18.970
So let's suppose that you
have all your hypotheses.

00:29:18.970 --> 00:29:21.150
You define your space of
hypotheses in some way.

00:29:21.150 --> 00:29:23.358
And it could be, for example,
the set of all Bayesian

00:29:23.358 --> 00:29:25.230
networks, or the set
of all neural networks,

00:29:25.230 --> 00:29:26.699
or all decision
trees, or whatever.

00:29:26.699 --> 00:29:28.740
And now the first thing
that you're going to have

00:29:28.740 --> 00:29:32.260
is the prior probability
for each hypothesis.

00:29:32.260 --> 00:29:34.930
This is how much you
believe in that hypothesis

00:29:34.930 --> 00:29:37.529
before you've even
seen any data.

00:29:37.529 --> 00:29:39.570
And this is actually what
makes Bayesian learning

00:29:39.570 --> 00:29:42.480
very controversial is that many
statisticians say, well, you

00:29:42.480 --> 00:29:45.830
have no grounds on which
to just make up a prior.

00:29:45.830 --> 00:29:50.320
And the Bayesians answer to that
is, you have to make that up,

00:29:50.320 --> 00:29:51.800
whether explicitly
or implicitly.

00:29:51.800 --> 00:29:54.550
So let's just be
explicit about it.

00:29:54.550 --> 00:29:56.990
So the prior is how much you
believe in each hypothesis

00:29:56.990 --> 00:29:58.900
before you see the evidence.

00:29:58.900 --> 00:30:02.360
But then what happens is that
as the evidence comes in,

00:30:02.360 --> 00:30:06.050
you update the probability
of each hypothesis.

00:30:06.050 --> 00:30:08.330
A hypothesis that is
consistent with the data

00:30:08.330 --> 00:30:10.570
will see its probability go up.

00:30:10.570 --> 00:30:12.590
A hypothesis that is
inconsistent with the data

00:30:12.590 --> 00:30:14.370
will see its
probability go down.

00:30:14.370 --> 00:30:14.910
OK?

00:30:14.910 --> 00:30:16.840
And the consistency of
the hypothesis of data

00:30:16.840 --> 00:30:19.298
is measured by what's called
the likelihood function, which

00:30:19.298 --> 00:30:23.670
is the probability of seeing the
data if the hypothesis is true.

00:30:23.670 --> 00:30:24.210
OK?

00:30:24.210 --> 00:30:27.030
And this theory is actually
no different from frequency

00:30:27.030 --> 00:30:29.190
statistics and the
maximum likelihood

00:30:29.190 --> 00:30:30.830
principle, which
is basically saying

00:30:30.830 --> 00:30:35.410
that if your hypothesis makes
what you're seeing likely,

00:30:35.410 --> 00:30:37.710
then conversely,
what you're seeing

00:30:37.710 --> 00:30:39.340
makes your hypothesis likely.

00:30:39.340 --> 00:30:39.840
OK?

00:30:39.840 --> 00:30:42.048
And the Bayesians incorporate
that in the likelihood.

00:30:42.048 --> 00:30:44.020
And the product of the
likelihood and the prior

00:30:44.020 --> 00:30:46.630
is just the posterior,
which is how much you

00:30:46.630 --> 00:30:49.960
believe the hypothesis after
you've seen the evidence.

00:30:49.960 --> 00:30:53.030
So as you see more evidence,
the probabilities evolve.

00:30:53.030 --> 00:30:54.660
And hopefully, at
the end of the day,

00:30:54.660 --> 00:30:58.090
one hypothesis will come out as
clearly better than the others.

00:30:58.090 --> 00:30:59.940
But that won't
necessarily be the case.

00:30:59.940 --> 00:31:02.500
You might still be entertaining
a lot of hypotheses

00:31:02.500 --> 00:31:04.842
even after you've
seen a lot data.

00:31:04.842 --> 00:31:06.300
There's also the
marginal, which is

00:31:06.300 --> 00:31:08.549
just something that you have
to divide by to make sure

00:31:08.549 --> 00:31:10.830
that the probabilities
add up to 1, so let's

00:31:10.830 --> 00:31:12.890
not worry about it for now.

00:31:12.890 --> 00:31:14.530
And a lot of great
things have been

00:31:14.530 --> 00:31:16.570
done with Bayesian learning.

00:31:16.570 --> 00:31:18.520
Like, for example,
self-driving cars

00:31:18.520 --> 00:31:21.720
have Bayesian learning
in their brains.

00:31:21.720 --> 00:31:26.720
So in some sense, Bayes' theorem
is helping to drive that car

00:31:26.720 --> 00:31:30.150
or helping the car to
learn how to drive.

00:31:30.150 --> 00:31:32.870
And without it, it
would be much harder.

00:31:32.870 --> 00:31:34.950
But one application
of Bayesian learning

00:31:34.950 --> 00:31:37.230
that everybody is
probably familiar with

00:31:37.230 --> 00:31:40.180
is spam filtering.

00:31:40.180 --> 00:31:43.460
The first spam
filter was actually

00:31:43.460 --> 00:31:46.662
designed by David Heckerman
and his coworkers.

00:31:46.662 --> 00:31:49.120
And they just used this very
simple Bayesian learner called

00:31:49.120 --> 00:31:51.280
the naive Bayes classifier.

00:31:51.280 --> 00:31:53.940
And the way it works
there is the following.

00:31:53.940 --> 00:31:57.220
The hypothesis, before
I've seen evidence,

00:31:57.220 --> 00:32:00.910
is that the email is a spam or
that the email is not a spam.

00:32:00.910 --> 00:32:01.410
OK?

00:32:01.410 --> 00:32:03.743
And the prior probability is
like your prior probability

00:32:03.743 --> 00:32:10.090
of an email being spam-- 90%,
99%, 99.999%-- take your pick.

00:32:10.090 --> 00:32:14.657
And then the evidence is the
actual content of the email.

00:32:14.657 --> 00:32:16.990
So for example, if the email
contains the word "Viagra,"

00:32:16.990 --> 00:32:19.700
that probably makes it
more likely to be spam.

00:32:19.700 --> 00:32:22.280
If it contains the word
"free" in capitals,

00:32:22.280 --> 00:32:24.310
that makes it even
more likely to be spam.

00:32:24.310 --> 00:32:26.880
And if that "FREE" is followed
by four exclamation marks,

00:32:26.880 --> 00:32:29.030
that makes it even
more likely to be spam.

00:32:29.030 --> 00:32:29.820
OK?

00:32:29.820 --> 00:32:33.160
On the other hand, if it
contains the name of your best

00:32:33.160 --> 00:32:35.360
friend on the signature
line, that actually

00:32:35.360 --> 00:32:37.430
makes it less likely to be spam.

00:32:37.430 --> 00:32:38.079
OK?

00:32:38.079 --> 00:32:39.870
And so what the naive
Bayes classifier does

00:32:39.870 --> 00:32:41.340
is it incorporates
that evidence.

00:32:41.340 --> 00:32:43.580
And at the end of the day,
it computes a probability

00:32:43.580 --> 00:32:46.180
that the email is spam or
not spam taking all of that

00:32:46.180 --> 00:32:46.910
into account.

00:32:46.910 --> 00:32:47.700
OK?

00:32:47.700 --> 00:32:49.158
And then based on
that probability,

00:32:49.158 --> 00:32:51.340
you can decide whether to
filter it out or actually

00:32:51.340 --> 00:32:53.510
show it to the user.

00:32:53.510 --> 00:32:56.310
And we're all grateful
that spam filters exist.

00:32:56.310 --> 00:32:59.551
Otherwise our mailboxes
would be unmanageable.

00:32:59.551 --> 00:33:01.800
These days, all sorts of
different learning algorithms

00:33:01.800 --> 00:33:02.966
get used for spam filtering.

00:33:02.966 --> 00:33:05.750
But Bayesian learning
was the first one

00:33:05.750 --> 00:33:10.440
and it's still used
in many spam filters.

00:33:10.440 --> 00:33:10.940
OK.

00:33:10.940 --> 00:33:15.330
Finally, the Analogizers--
so as I mentioned,

00:33:15.330 --> 00:33:16.910
the basic idea of
the Analogizers

00:33:16.910 --> 00:33:22.060
is that everything that we
do, everything that we learn

00:33:22.060 --> 00:33:24.420
is reasoning by analogy.

00:33:24.420 --> 00:33:27.710
It's looking at similarities
between the new situation

00:33:27.710 --> 00:33:29.650
that we need to
make a decision in

00:33:29.650 --> 00:33:32.264
and the situations that
we're already familiar with.

00:33:32.264 --> 00:33:33.100
OK?

00:33:33.100 --> 00:33:37.020
And one of the early pioneers
in this area was Peter Hart.

00:33:37.020 --> 00:33:40.690
He proved some things related to
the nearest neighbor algorithm,

00:33:40.690 --> 00:33:42.220
which we'll see
shortly, which is

00:33:42.220 --> 00:33:46.180
kind of like the first
similarity-based algorithm.

00:33:46.180 --> 00:33:50.230
Vladimir Vapnik is the inventor
of support vector machines,

00:33:50.230 --> 00:33:50.740
a.k.a.

00:33:50.740 --> 00:33:54.080
kernel machines, which is
the most widely used and most

00:33:54.080 --> 00:33:59.150
successful type of
similarity-based learner.

00:33:59.150 --> 00:34:01.050
But these are both
actually still

00:34:01.050 --> 00:34:04.490
fairly primitive forms
of analogical reasoning.

00:34:04.490 --> 00:34:06.460
There are some much
more sophisticated ones

00:34:06.460 --> 00:34:10.060
that people, like for example,
Douglas Hofstadter, work on.

00:34:10.060 --> 00:34:12.594
And Douglas Hofstadter,
of course, is also famous.

00:34:12.594 --> 00:34:14.719
He's not just a quantitative
scientist and computer

00:34:14.719 --> 00:34:17.581
scientist, he's also famous as
the author of "Godel, Escher,

00:34:17.581 --> 00:34:21.150
Bach," which,
ironically, is actually,

00:34:21.150 --> 00:34:24.409
itself, an extended
analogy between Godel's

00:34:24.409 --> 00:34:28.040
theorem and the music of
Bach and the art of Escher.

00:34:28.040 --> 00:34:31.179
And in fact, his
most recent book

00:34:31.179 --> 00:34:36.970
is 500 pages arguing that all
of intelligence is just analogy.

00:34:36.970 --> 00:34:38.469
So he really does
think that analogy

00:34:38.469 --> 00:34:39.696
is the master algorithm.

00:34:39.696 --> 00:34:41.070
And in fact, "the
terminologizer"

00:34:41.070 --> 00:34:43.090
was coined by him.

00:34:43.090 --> 00:34:46.570
So let's see how this works.

00:34:46.570 --> 00:34:48.900
And I'm going to
do it by proposing

00:34:48.900 --> 00:34:51.639
to you a very small puzzle.

00:34:51.639 --> 00:34:53.650
And the puzzle is this.

00:34:53.650 --> 00:34:56.409
Let's suppose I give you a
map of two countries, which

00:34:56.409 --> 00:34:58.820
I fancifully called "Posistan"
and "Negaland" because

00:34:58.820 --> 00:35:01.909
of positive and
negative examples.

00:35:01.909 --> 00:35:04.200
And I don't tell you where
the frontier between the two

00:35:04.200 --> 00:35:04.990
countries is.

00:35:04.990 --> 00:35:06.850
I just tell you where
the major cities are.

00:35:06.850 --> 00:35:09.500
So the major cities in
Posistan are the plus signs,

00:35:09.500 --> 00:35:12.300
Positiveville is the
capital, and similarly

00:35:12.300 --> 00:35:14.280
for the negative cities.

00:35:14.280 --> 00:35:15.200
OK?

00:35:15.200 --> 00:35:17.200
And now my question to
you is the following.

00:35:17.200 --> 00:35:19.630
If I only tell you where
the major cities are,

00:35:19.630 --> 00:35:22.200
can you tell me
where the frontier,

00:35:22.200 --> 00:35:25.700
where the border between
the two countries is?

00:35:25.700 --> 00:35:27.570
Of course, you can't
know for sure, right,

00:35:27.570 --> 00:35:29.559
because the cities don't
determine the border.

00:35:29.559 --> 00:35:31.850
But that's what the machine
learning problem is, right?

00:35:31.850 --> 00:35:33.660
You have to generalize.

00:35:33.660 --> 00:35:35.690
Now the nearest
neighbor algorithm

00:35:35.690 --> 00:35:37.850
has a very simple
answer to this question.

00:35:37.850 --> 00:35:42.310
It just says, I'm going to
assume that a point on the map

00:35:42.310 --> 00:35:47.630
is in Posistan if it's
closer to some positive city

00:35:47.630 --> 00:35:49.910
then to any negative city.

00:35:49.910 --> 00:35:51.030
OK?

00:35:51.030 --> 00:35:53.720
And the effect that this
has is to divide the map

00:35:53.720 --> 00:35:56.180
into the neighborhood
of each city.

00:35:56.180 --> 00:35:59.010
And then Posistan is just the
union of the neighborhoods

00:35:59.010 --> 00:36:00.380
of the positive cities.

00:36:00.380 --> 00:36:02.840
The neighborhood of
a city is the points

00:36:02.840 --> 00:36:05.390
that are closer to it
than to any other city.

00:36:05.390 --> 00:36:06.360
OK?

00:36:06.360 --> 00:36:09.280
And then as a result, you
get this jagged straight line

00:36:09.280 --> 00:36:10.724
frontier.

00:36:10.724 --> 00:36:13.390
And what's remarkable about this
is that even though the nearest

00:36:13.390 --> 00:36:16.630
neighbor algorithm is
extremely simple-- in fact,

00:36:16.630 --> 00:36:18.650
at learning time, it
doesn't do anything, right?

00:36:18.650 --> 00:36:20.220
It's an O of 0 algorithm.

00:36:20.220 --> 00:36:22.270
It just sits there,
doesn't have to do

00:36:22.270 --> 00:36:26.110
anything-- it can
actually form very, very

00:36:26.110 --> 00:36:27.410
complicated frontiers.

00:36:27.410 --> 00:36:29.070
In fact, if you
give it enough data,

00:36:29.070 --> 00:36:31.330
it will converge very closely.

00:36:31.330 --> 00:36:34.342
In fact, it will converge to
the best possible hypothesis

00:36:34.342 --> 00:36:36.800
that you could ever have if
you use more than one neighbor.

00:36:36.800 --> 00:36:39.750
But let's not go
into those details.

00:36:39.750 --> 00:36:43.200
Now there's a couple of things
here that are not ideal.

00:36:43.200 --> 00:36:44.730
One is that this
line is probably

00:36:44.730 --> 00:36:46.060
not quite the right one, right?

00:36:46.060 --> 00:36:48.018
Because the real frontier
is probably smoother.

00:36:48.018 --> 00:36:49.530
It's not jagged like that.

00:36:49.530 --> 00:36:53.520
And another one is that if
you look at this map closely,

00:36:53.520 --> 00:36:55.040
you could actually
throw away some

00:36:55.040 --> 00:36:57.700
of the cities, like
this one, for example,

00:36:57.700 --> 00:37:00.140
and it wouldn't change anything.

00:37:00.140 --> 00:37:02.600
If you threw away
this city, its area

00:37:02.600 --> 00:37:05.490
gets absorbed by the areas
of these two other cities.

00:37:05.490 --> 00:37:08.640
And the frontier
doesn't change at all.

00:37:08.640 --> 00:37:11.200
The only cities that
you need to keep

00:37:11.200 --> 00:37:13.940
are the ones that
actually take part

00:37:13.940 --> 00:37:18.550
in defining the frontier, the
so-called "support vectors."

00:37:18.550 --> 00:37:21.564
In general, these are
vectors in hyperspace.

00:37:21.564 --> 00:37:22.980
And they're called
support vectors

00:37:22.980 --> 00:37:26.570
because they're the ones that
keep the frontier where it is.

00:37:26.570 --> 00:37:30.420
So often, you can throw away the
great majority of your examples

00:37:30.420 --> 00:37:33.464
and that doesn't
change anything.

00:37:33.464 --> 00:37:35.630
And of course, in this
example, this doesn't matter.

00:37:35.630 --> 00:37:37.879
But when you have a data set
with millions or billions

00:37:37.879 --> 00:37:39.310
of examples, that does matter.

00:37:39.310 --> 00:37:44.020
And support vector machines,
or kernel machines, for short,

00:37:44.020 --> 00:37:46.600
solve both of these problems.

00:37:46.600 --> 00:37:49.250
So they have a
learning procedure

00:37:49.250 --> 00:37:51.590
that lets them throw away
all the examples that are not

00:37:51.590 --> 00:37:53.070
necessary to define
the frontier,

00:37:53.070 --> 00:37:54.940
so they leave only these.

00:37:54.940 --> 00:37:57.800
And they also make
the frontier smoother.

00:37:57.800 --> 00:38:00.580
And the way they draw the
frontier is by saying, well,

00:38:00.580 --> 00:38:04.830
let me try to walk from
south to north while

00:38:04.830 --> 00:38:06.900
keeping the positive
cities on my left

00:38:06.900 --> 00:38:09.240
and the negative
cities on my right.

00:38:09.240 --> 00:38:14.400
But I want to always stay as far
as possible from them as I can.

00:38:14.400 --> 00:38:14.940
OK?

00:38:14.940 --> 00:38:17.710
Think of the cities as mines and
think of this as a minefield.

00:38:17.710 --> 00:38:18.310
Right?

00:38:18.310 --> 00:38:21.630
If I told you to walk
all along a minefield,

00:38:21.630 --> 00:38:24.700
you would try to give each
mine the widest possible berth.

00:38:24.700 --> 00:38:27.580
You would try to maximize
your margin of safety.

00:38:27.580 --> 00:38:29.890
And that's exactly what
support vector machines

00:38:29.890 --> 00:38:32.610
do is that they maximize the
margin between the frontier

00:38:32.610 --> 00:38:34.314
and the nearest examples.

00:38:36.884 --> 00:38:38.800
And in the days before
deep learning took off,

00:38:38.800 --> 00:38:43.000
support vector
machines were probably

00:38:43.000 --> 00:38:44.790
the most powerful
type of learning

00:38:44.790 --> 00:38:46.280
that was commonly used.

00:38:46.280 --> 00:38:48.020
OK?

00:38:48.020 --> 00:38:52.840
All right, let's look at
one application of this.

00:38:52.840 --> 00:38:57.260
Again, this type of
analogy-based learning, people

00:38:57.260 --> 00:39:00.140
have been doing
it since the '50s,

00:39:00.140 --> 00:39:03.000
so it's used for just
about everything on Earth.

00:39:03.000 --> 00:39:05.110
There is, however,
one application

00:39:05.110 --> 00:39:07.950
that I think everybody
has experienced

00:39:07.950 --> 00:39:10.080
even if they don't know
it's an application

00:39:10.080 --> 00:39:12.450
of analogy-based learning.

00:39:12.450 --> 00:39:14.480
And that is recommender systems.

00:39:14.480 --> 00:39:16.190
OK?

00:39:16.190 --> 00:39:18.300
So for example,
let's say that I want

00:39:18.300 --> 00:39:22.830
to figure out what movies
to recommend to you.

00:39:22.830 --> 00:39:28.400
The idea that folks
had almost 20 years

00:39:28.400 --> 00:39:30.730
ago now was a very simple one.

00:39:30.730 --> 00:39:32.980
It is, let me not
try to do that based

00:39:32.980 --> 00:39:35.770
on the properties of the
movie, because that's hard.

00:39:35.770 --> 00:39:36.270
Right?

00:39:36.270 --> 00:39:39.450
You know, people's tastes
are very complicated things.

00:39:39.450 --> 00:39:41.810
What I should do
is what is called

00:39:41.810 --> 00:39:43.890
"collaborative
filtering," is find people

00:39:43.890 --> 00:39:47.710
who have similar tastes to
you, meaning they gave five

00:39:47.710 --> 00:39:49.930
stars to a movie that
you gave five stars to,

00:39:49.930 --> 00:39:52.700
they gave one star to movies
that you gave one star to.

00:39:52.700 --> 00:39:57.210
And now if they give
five stars to a movie

00:39:57.210 --> 00:39:59.190
that you haven't
seen, then I'm going

00:39:59.190 --> 00:40:03.470
to hypothesize that, by analogy,
you will also like that movie.

00:40:03.470 --> 00:40:05.140
And so I will
recommend it to you.

00:40:05.140 --> 00:40:06.120
OK?

00:40:06.120 --> 00:40:10.230
And this turns out to
work spectacularly well.

00:40:10.230 --> 00:40:16.550
It works so well, in fact,
that 3/4 of Netflix's business

00:40:16.550 --> 00:40:19.050
comes from its
recommender system.

00:40:19.050 --> 00:40:21.310
And Amazon also has
a recommender system.

00:40:21.310 --> 00:40:25.400
About 1/3 of its business comes
from its recommender system.

00:40:25.400 --> 00:40:27.670
And every e-commerce
site worth its salt

00:40:27.670 --> 00:40:28.670
has something like this.

00:40:28.670 --> 00:40:30.440
And of course,
these days, people

00:40:30.440 --> 00:40:32.900
use all sorts of different
learning algorithms for it.

00:40:32.900 --> 00:40:37.370
But the first one was this
nearest neighbor type method.

00:40:37.370 --> 00:40:40.130
And it's still one of the best.

00:40:40.130 --> 00:40:42.800
OK?

00:40:42.800 --> 00:40:47.690
So stepping back, we've
met the five tribes

00:40:47.690 --> 00:40:49.270
of machine learning.

00:40:49.270 --> 00:40:54.700
We've seen that each one of them
has a problem that it can solve

00:40:54.700 --> 00:40:56.590
better than all the others.

00:40:56.590 --> 00:41:00.120
And it has a particular
master algorithm

00:41:00.120 --> 00:41:01.920
that solves that problem.

00:41:01.920 --> 00:41:03.982
So for example, the
problem that the Symbolists

00:41:03.982 --> 00:41:05.940
solve that none of the
others know how to solve

00:41:05.940 --> 00:41:07.440
is the problem of
learning knowledge

00:41:07.440 --> 00:41:09.632
that you can compose
in many different ways.

00:41:09.632 --> 00:41:11.840
And they learn that knowledge
with inverse deduction,

00:41:11.840 --> 00:41:12.960
as we saw.

00:41:12.960 --> 00:41:15.220
Connectionists solve the
credit assignment problem

00:41:15.220 --> 00:41:17.510
using backprop.

00:41:17.510 --> 00:41:20.804
Evolutionaries solve the problem
of learning structure, right?

00:41:20.804 --> 00:41:22.970
The Connectionists just
start with a fixed structure

00:41:22.970 --> 00:41:24.136
and just adjust the weights.

00:41:24.136 --> 00:41:26.240
The Evolutionaries
know how to come up

00:41:26.240 --> 00:41:28.270
with that structure in
the first place using

00:41:28.270 --> 00:41:30.270
genetic programming.

00:41:30.270 --> 00:41:32.720
The Bayesians are all
about uncertainty.

00:41:32.720 --> 00:41:34.400
They know how to
deal with the fact

00:41:34.400 --> 00:41:36.140
that all the knowledge that
you learn is uncertain.

00:41:36.140 --> 00:41:37.940
They know how to update the
probabilities of hypotheses

00:41:37.940 --> 00:41:39.180
as they see more data.

00:41:39.180 --> 00:41:42.080
They use that using
probabilistic inference, which

00:41:42.080 --> 00:41:44.640
is essentially
computationally efficient ways

00:41:44.640 --> 00:41:48.660
to apply Bayes' theorem to
very large sets of hypotheses.

00:41:48.660 --> 00:41:52.220
And finally, the Analogizers,
they can reason by similarity.

00:41:52.220 --> 00:41:56.650
They can actually generalize
from just one or two examples.

00:41:56.650 --> 00:41:57.820
All right?

00:41:57.820 --> 00:41:58.980
Think of Niels Bohr.

00:41:58.980 --> 00:42:01.480
He came up with the first
theory of quantum mechanics

00:42:01.480 --> 00:42:04.700
by doing an analogy between
the atom and the solar system,

00:42:04.700 --> 00:42:06.480
generalizing from one example.

00:42:06.480 --> 00:42:08.360
None of the others can do this.

00:42:08.360 --> 00:42:12.150
And probably the best
known analogizer algorithm

00:42:12.150 --> 00:42:15.367
these days is kernel machines.

00:42:15.367 --> 00:42:16.950
But the thing that
I want to point out

00:42:16.950 --> 00:42:19.780
is that, precisely because
each of these problems

00:42:19.780 --> 00:42:25.510
is real and important, none
of the individual algorithms

00:42:25.510 --> 00:42:26.950
are enough.

00:42:26.950 --> 00:42:29.930
What we really need
is a single algorithm

00:42:29.930 --> 00:42:34.730
that solves all five
problems at the same time.

00:42:34.730 --> 00:42:37.850
We need a "grand unified
theory" of machine learning

00:42:37.850 --> 00:42:39.890
in the same sense that
the standard model is

00:42:39.890 --> 00:42:43.080
a grand unified theory of
physics or the central dogma

00:42:43.080 --> 00:42:45.910
is a grand unified
theory of biology.

00:42:45.910 --> 00:42:47.760
And in fact, a bunch
of us have been working

00:42:47.760 --> 00:42:48.990
on this problem for a while.

00:42:48.990 --> 00:42:51.729
And we've actually
made a lot of progress.

00:42:51.729 --> 00:42:53.270
We're still far from
the end of this,

00:42:53.270 --> 00:42:57.360
but let me just give you a sense
of where we are at this point.

00:42:57.360 --> 00:42:59.710
So if you think for a moment
about this problem of,

00:42:59.710 --> 00:43:03.810
so we have these five
algorithms or these five

00:43:03.810 --> 00:43:08.080
types of learning, how can
we unify them all into one,

00:43:08.080 --> 00:43:10.580
at first, this seems
like a very hard problem.

00:43:10.580 --> 00:43:12.538
And in fact, some people
have claimed that it's

00:43:12.538 --> 00:43:14.050
an impossible problem to solve.

00:43:14.050 --> 00:43:16.220
It seems very hard,
because the algorithms all

00:43:16.220 --> 00:43:17.747
look very different.

00:43:17.747 --> 00:43:19.830
But if you look at them
closely, actually, they're

00:43:19.830 --> 00:43:21.520
not that different.

00:43:21.520 --> 00:43:25.360
They all have the same
three parts, representation,

00:43:25.360 --> 00:43:27.700
evaluation, and optimization.

00:43:27.700 --> 00:43:29.710
So let's look at
what those parts are

00:43:29.710 --> 00:43:32.160
and then how we can
do the unification.

00:43:32.160 --> 00:43:37.130
So representation is how
the learner represents

00:43:37.130 --> 00:43:39.911
what it's learning, the model or
the program that it's learning.

00:43:39.911 --> 00:43:40.410
Right?

00:43:40.410 --> 00:43:42.520
It's, in some sense,
the programming language

00:43:42.520 --> 00:43:45.180
in which the learner is going
to write the algorithm that it

00:43:45.180 --> 00:43:46.156
discovered.

00:43:46.156 --> 00:43:48.030
Typically, it's not
going to be Java, or C++,

00:43:48.030 --> 00:43:50.300
or anything like that, it's
going to be something like

00:43:50.300 --> 00:43:51.265
first-order logic.

00:43:51.265 --> 00:43:53.150
All right?

00:43:53.150 --> 00:43:55.017
But it could be
differential equations.

00:43:55.017 --> 00:43:56.350
It could be a linear regression.

00:43:56.350 --> 00:43:58.360
It could be all sorts of things.

00:43:58.360 --> 00:44:00.370
So the first thing
that we need to do

00:44:00.370 --> 00:44:04.750
is to unify the representations.

00:44:04.750 --> 00:44:06.780
And a natural
thing to do here is

00:44:06.780 --> 00:44:09.170
to start with the
representations

00:44:09.170 --> 00:44:12.990
that the Symbolists use,
which are variations

00:44:12.990 --> 00:44:15.720
on first-order logic,
and the representation

00:44:15.720 --> 00:44:19.340
that the Bayesians use,
which are generally

00:44:19.340 --> 00:44:20.560
known as graphical models.

00:44:20.560 --> 00:44:22.920
Bayesian networks are one
type of graphical model.

00:44:22.920 --> 00:44:25.450
Another type is Markov
networks and so on.

00:44:25.450 --> 00:44:28.080
Each of these is already
extremely general.

00:44:28.080 --> 00:44:30.260
If you can combine the
two, you can pretty much

00:44:30.260 --> 00:44:32.644
represent anything you
might want to represent.

00:44:32.644 --> 00:44:34.560
Any computer program
can, for example, already

00:44:34.560 --> 00:44:36.610
be represented in
first-order logic.

00:44:36.610 --> 00:44:38.860
Any way to deal with uncertainty
and weighing evidence

00:44:38.860 --> 00:44:41.180
can be represented
in graphical models.

00:44:41.180 --> 00:44:43.190
There's bazillions
of different models

00:44:43.190 --> 00:44:45.240
that people have in
statistics that all fit

00:44:45.240 --> 00:44:46.240
into that framework.

00:44:46.240 --> 00:44:48.180
So if we can combine
the two, then we

00:44:48.180 --> 00:44:50.800
have a very good
representation to start with.

00:44:50.800 --> 00:44:52.720
And indeed, we have done that.

00:44:52.720 --> 00:44:55.720
In essence, what we've
developed is various forms

00:44:55.720 --> 00:44:57.820
of probabilistic logic.

00:44:57.820 --> 00:44:59.720
So this is a logic
that also incorporates

00:44:59.720 --> 00:45:01.500
probability and uncertainty.

00:45:01.500 --> 00:45:04.880
And the most widely used is
called Markov logic networks.

00:45:04.880 --> 00:45:09.710
It's essentially a combination
of logic and Markov networks.

00:45:09.710 --> 00:45:11.880
And it's very simple.

00:45:11.880 --> 00:45:14.130
It just starts with formulas
and first-order logic.

00:45:14.130 --> 00:45:17.160
Think of a rule in logic,
like if this, then that,

00:45:17.160 --> 00:45:18.300
for example.

00:45:18.300 --> 00:45:22.444
And then what it does is it
gives each rule a weight.

00:45:22.444 --> 00:45:23.860
So if you really
believe the rule,

00:45:23.860 --> 00:45:24.943
you give it a high weight.

00:45:24.943 --> 00:45:28.550
If you're not sure, you
give it a lower weight.

00:45:28.550 --> 00:45:31.240
And then the probability
of a state of the world

00:45:31.240 --> 00:45:34.040
goes up with the number
and the weight of the rules

00:45:34.040 --> 00:45:35.400
that are true in that world.

00:45:35.400 --> 00:45:36.010
OK?

00:45:36.010 --> 00:45:38.135
So with this, we can
represent pretty much anything

00:45:38.135 --> 00:45:39.230
that we'd like.

00:45:39.230 --> 00:45:41.930
Now the next part of
every learning algorithm

00:45:41.930 --> 00:45:45.160
is the evaluation.

00:45:45.160 --> 00:45:48.100
The evaluation is the scoring
function that tells me

00:45:48.100 --> 00:45:52.300
how good a candidate model is.

00:45:52.300 --> 00:45:54.340
How well does it fit the data?

00:45:54.340 --> 00:45:57.710
How well does it
fit my purposes?

00:45:57.710 --> 00:46:00.590
In essence, what
the learning problem

00:46:00.590 --> 00:46:03.930
is is to find-- within the space
defined by a representation--

00:46:03.930 --> 00:46:07.990
find the program that maximizes
my evaluation function.

00:46:07.990 --> 00:46:08.490
OK?

00:46:08.490 --> 00:46:10.240
So what should our
evaluation function be?

00:46:10.240 --> 00:46:13.930
Well, one obvious candidate is
just the posterior probability

00:46:13.930 --> 00:46:14.846
that Bayesians use.

00:46:14.846 --> 00:46:16.970
And that, again, has a lot
of other things already,

00:46:16.970 --> 00:46:18.150
special cases.

00:46:18.150 --> 00:46:21.670
But more generally,
the evaluation

00:46:21.670 --> 00:46:23.880
shouldn't really be
part of the algorithm.

00:46:23.880 --> 00:46:27.320
It should be
provided by the user.

00:46:27.320 --> 00:46:29.800
It's for the user to decide
what the learner should

00:46:29.800 --> 00:46:31.440
be optimizing.

00:46:31.440 --> 00:46:34.690
So if you're a company and your
purpose is to maximize profits,

00:46:34.690 --> 00:46:37.250
then that's what the
evaluation function should be.

00:46:37.250 --> 00:46:39.030
If you're a consumer
and your purpose

00:46:39.030 --> 00:46:40.599
is to maximize your
happiness, then

00:46:40.599 --> 00:46:42.140
that's what should
be being maximized

00:46:42.140 --> 00:46:44.220
is some measure
of your happiness.

00:46:44.220 --> 00:46:44.950
OK?

00:46:44.950 --> 00:46:47.340
So what the mass problem
should be able to do

00:46:47.340 --> 00:46:50.380
is take anybody's
objective function

00:46:50.380 --> 00:46:52.635
and then learn to optimize that.

00:46:52.635 --> 00:46:54.090
OK?

00:46:54.090 --> 00:46:55.000
Finally-- right?

00:46:55.000 --> 00:46:57.120
I just said the word
"optimize"-- the third part

00:46:57.120 --> 00:46:59.280
of this is optimization,
is, how do we actually

00:46:59.280 --> 00:47:03.570
find the model that
maximizes that function.

00:47:03.570 --> 00:47:04.400
OK?

00:47:04.400 --> 00:47:06.080
And here, there's a
natural combination

00:47:06.080 --> 00:47:07.750
of ideas from
genetic programming

00:47:07.750 --> 00:47:11.220
and backpropagation, namely,
to discover formulas,

00:47:11.220 --> 00:47:12.700
we can use genetic programming.

00:47:12.700 --> 00:47:15.330
Each formula in first-order
logic is a tree.

00:47:15.330 --> 00:47:15.880
Right?

00:47:15.880 --> 00:47:17.970
And now I can cross
these trees over

00:47:17.970 --> 00:47:20.510
and apply the genetic
process to come up

00:47:20.510 --> 00:47:24.620
with better formulas that
better describe my domain.

00:47:24.620 --> 00:47:27.390
And then once I have
those formulas, of course,

00:47:27.390 --> 00:47:28.810
if I'm using Markov
logic, I need

00:47:28.810 --> 00:47:30.632
to come up with weights
for those formulas.

00:47:30.632 --> 00:47:32.590
But of course, this is
where backprop comes in.

00:47:32.590 --> 00:47:33.450
All right?

00:47:33.450 --> 00:47:36.370
I have my big chain
of reasoning involving

00:47:36.370 --> 00:47:38.950
many different formulas, and
facts, and different steps.

00:47:38.950 --> 00:47:40.530
And all of those have weights.

00:47:40.530 --> 00:47:43.210
And in order to
learn those weights,

00:47:43.210 --> 00:47:45.490
I can naturally use
backpropagation.

00:47:45.490 --> 00:47:46.980
OK?

00:47:46.980 --> 00:47:49.030
So we're pretty
far along in this.

00:47:49.030 --> 00:47:50.740
We haven't succeeded yet.

00:47:50.740 --> 00:47:55.170
But some people think it's only
a matter of time before we do.

00:47:55.170 --> 00:47:59.300
I'm actually a
little less gung-ho.

00:47:59.300 --> 00:48:02.710
I think that even after we've
successfully unified these five

00:48:02.710 --> 00:48:07.810
paradigms, there are still
major ideas missing, ideas

00:48:07.810 --> 00:48:11.080
that we haven't had and
without which we will not

00:48:11.080 --> 00:48:14.380
have a truly universal learner.

00:48:14.380 --> 00:48:16.380
And in fact, part of my
goal in writing the book

00:48:16.380 --> 00:48:19.130
was to open it up to people
who are not already machine

00:48:19.130 --> 00:48:21.770
learning researchers so they
can think about the problem

00:48:21.770 --> 00:48:23.900
and maybe have ideas that
people who are already

00:48:23.900 --> 00:48:27.290
thinking along the tracks of
one of the tribes wouldn't have.

00:48:27.290 --> 00:48:29.590
So if you figure out how
to solve this problem,

00:48:29.590 --> 00:48:31.430
let me know so I can
publish your solution.

00:48:33.890 --> 00:48:34.390
OK.

00:48:34.390 --> 00:48:39.120
Let me conclude by talking a
little bit about what I think

00:48:39.120 --> 00:48:44.910
that the master algorithm will
enable that we cannot do today.

00:48:44.910 --> 00:48:47.906
I have four items here.

00:48:47.906 --> 00:48:49.030
There are, of course, more.

00:48:49.030 --> 00:48:50.910
But I think these
four give a good sense

00:48:50.910 --> 00:48:55.320
of just how momentous a
development this would be.

00:48:55.320 --> 00:48:58.540
The first one is home robots.

00:48:58.540 --> 00:49:01.260
We would all love to have
robots that cook dinner for us,

00:49:01.260 --> 00:49:03.710
and do the dishes, and
make the beds, and whatnot.

00:49:03.710 --> 00:49:05.990
But why don't we
have them today?

00:49:05.990 --> 00:49:08.600
Well, first of all, it can't be
done without machine learning,

00:49:08.600 --> 00:49:08.720
right?

00:49:08.720 --> 00:49:10.290
There's just no way
to program a robot

00:49:10.290 --> 00:49:12.210
to do all the things
that it might have to do.

00:49:12.210 --> 00:49:14.840
But second of all, the learning
algorithms that we have today

00:49:14.840 --> 00:49:17.250
are not good enough.

00:49:17.250 --> 00:49:20.700
Because a robot, a home robot,
in the course of a normal day,

00:49:20.700 --> 00:49:24.060
will run into all five of
those problems multiple times.

00:49:24.060 --> 00:49:26.240
So it needs to be able
to solve all of them.

00:49:26.240 --> 00:49:29.020
So with a master algorithm,
we're on our way.

00:49:29.020 --> 00:49:32.030
But without it, I think it'll
be much harder and much slower

00:49:32.030 --> 00:49:34.030
if we ever get there.

00:49:34.030 --> 00:49:35.650
Here's another one.

00:49:35.650 --> 00:49:37.760
Everybody, including,
of course, Google,

00:49:37.760 --> 00:49:42.780
has a project to try to turn
the web into a knowledge base.

00:49:42.780 --> 00:49:43.400
Right?

00:49:43.400 --> 00:49:46.460
Instead of issuing keyword
queries and getting back

00:49:46.460 --> 00:49:47.950
pages, what I
would like to do is

00:49:47.950 --> 00:49:50.850
ask questions and get answers.

00:49:50.850 --> 00:49:53.250
But for that, all the
knowledge that's on the web

00:49:53.250 --> 00:49:55.430
has to be represented in a
way that the computer can

00:49:55.430 --> 00:49:58.880
reason with, something like,
for example, first-order logic.

00:49:58.880 --> 00:50:01.720
On the other hand, the web
is full of contradiction,

00:50:01.720 --> 00:50:03.320
and noise, and
gaps, and whatnot,

00:50:03.320 --> 00:50:04.740
so it's going to be very noisy.

00:50:04.740 --> 00:50:06.480
So I need probability.

00:50:06.480 --> 00:50:07.170
OK?

00:50:07.170 --> 00:50:10.610
So again, I need to be
able to combine those five

00:50:10.610 --> 00:50:12.400
types of learning
in order to really

00:50:12.400 --> 00:50:14.840
be able to extract this
knowledge basis from the web.

00:50:14.840 --> 00:50:16.030
OK?

00:50:16.030 --> 00:50:20.690
Here's perhaps the most
important one, cancer.

00:50:20.690 --> 00:50:24.010
Why haven't we cured cancer yet?

00:50:24.010 --> 00:50:28.580
The problem is that
cancer is not one disease.

00:50:28.580 --> 00:50:29.260
Right?

00:50:29.260 --> 00:50:31.190
Everybody's cancer is different.

00:50:31.190 --> 00:50:34.520
And in fact, the same cancer
mutates as it goes along.

00:50:34.520 --> 00:50:36.270
So it's very unlikely
that there will ever

00:50:36.270 --> 00:50:39.650
be one drug that
cures all cancers.

00:50:39.650 --> 00:50:41.660
The real cure for
cancer-- or at least,

00:50:41.660 --> 00:50:45.080
that's what an increasing number
of cancer researchers believe--

00:50:45.080 --> 00:50:48.400
is something like
a learning program

00:50:48.400 --> 00:50:51.580
that takes in the genome of the
patient, the medical history,

00:50:51.580 --> 00:50:54.460
the mutations in
the tumor cells,

00:50:54.460 --> 00:50:58.160
and predicts, for
that tumor, what

00:50:58.160 --> 00:51:01.060
is the drug that's going
to kill it without harming

00:51:01.060 --> 00:51:04.860
the patient's normal cells--
or maybe the sequence of drugs,

00:51:04.860 --> 00:51:06.750
or a combination of
drugs, or perhaps even

00:51:06.750 --> 00:51:09.040
a drug that will be
designed from scratch

00:51:09.040 --> 00:51:10.750
for that particular patient.

00:51:10.750 --> 00:51:11.870
OK?

00:51:11.870 --> 00:51:14.020
In some ways, this is not
a very different problem

00:51:14.020 --> 00:51:17.810
from recommender systems that
recommend a book or a movie

00:51:17.810 --> 00:51:19.200
to you.

00:51:19.200 --> 00:51:22.230
However what they do here
is they recommend a drug.

00:51:22.230 --> 00:51:26.180
The problem, however,
is that this problem

00:51:26.180 --> 00:51:29.010
is orders of magnitude
harder than the problem

00:51:29.010 --> 00:51:32.150
of recommending a drug-- or
the problem of recommending

00:51:32.150 --> 00:51:34.120
a movie or a book.

00:51:34.120 --> 00:51:36.610
You have to understand
how the cell works.

00:51:36.610 --> 00:51:39.540
You have to understand
the interactions

00:51:39.540 --> 00:51:41.740
between the genes and the
proteins that they make.

00:51:41.740 --> 00:51:43.440
And then they go back
and regulate the genes.

00:51:43.440 --> 00:51:45.030
And it's when this
machinery gets out

00:51:45.030 --> 00:51:46.420
of whack that you get cancer.

00:51:46.420 --> 00:51:47.416
OK?

00:51:47.416 --> 00:51:48.790
The good news is
we actually have

00:51:48.790 --> 00:51:50.873
a lot of data to do this,
things like microarrays,

00:51:50.873 --> 00:51:52.380
and sequences, and whatnot.

00:51:52.380 --> 00:51:55.060
But again, with the learning
algorithms that we have today,

00:51:55.060 --> 00:51:56.180
we're not going
be able to do it.

00:51:56.180 --> 00:51:57.888
With something like
the master algorithm,

00:51:57.888 --> 00:51:59.380
we will be able to do it.

00:51:59.380 --> 00:51:59.880
OK.

00:51:59.880 --> 00:52:01.610
Finally, apropos of
recommender systems,

00:52:01.610 --> 00:52:02.610
let me mention this one.

00:52:05.220 --> 00:52:08.640
What I would really like
to have as a consumer

00:52:08.640 --> 00:52:10.836
is not 500 different
recommender systems

00:52:10.836 --> 00:52:12.210
recommending 500
different things

00:52:12.210 --> 00:52:14.290
to me-- Netflix
recommending movies

00:52:14.290 --> 00:52:17.560
and Amazon recommending books
and Facebook selecting updates

00:52:17.560 --> 00:52:19.310
and Twitter selecting tweets.

00:52:19.310 --> 00:52:24.210
What I want is a complete
360-degree model of me.

00:52:24.210 --> 00:52:27.949
Learn from all the
data that I generate.

00:52:27.949 --> 00:52:29.990
And then that model knows
me much better than all

00:52:29.990 --> 00:52:32.610
these tiny, little
models and, as a result,

00:52:32.610 --> 00:52:36.330
can make can make much better
recommendations-- and not

00:52:36.330 --> 00:52:37.950
just recommendations
of small things,

00:52:37.950 --> 00:52:41.650
but recommendations of
jobs, recommendations

00:52:41.650 --> 00:52:48.150
of houses, recommendations of
what to major in, or-- oops,

00:52:48.150 --> 00:52:49.990
I guess I skipped
this slide here.

00:52:49.990 --> 00:52:56.610
As the foremost city of the
federal government said,

00:52:56.610 --> 00:52:59.090
if we use these
things, we can actually

00:52:59.090 --> 00:53:01.490
have a recommender system
that is, in essence, your best

00:53:01.490 --> 00:53:04.060
friend throughout your life,
recommending the things

00:53:04.060 --> 00:53:06.190
that you need at every step.

00:53:06.190 --> 00:53:06.840
OK?

00:53:06.840 --> 00:53:10.240
And again, in order to do that,
we need not just the data,

00:53:10.240 --> 00:53:12.930
which, increasingly, we
have, we need the algorithms

00:53:12.930 --> 00:53:15.900
that are powerful enough to
learn that rich model of you.

00:53:15.900 --> 00:53:17.300
OK?

00:53:17.300 --> 00:53:19.150
More about all these
things in the book,

00:53:19.150 --> 00:53:20.680
"The Master Algorithm."

00:53:20.680 --> 00:53:24.706
And thank you for listening
and I'll take questions.

00:53:24.706 --> 00:53:27.947
[APPLAUSE]

00:53:30.190 --> 00:53:32.160
AUDIENCE: The representation
that you proposed,

00:53:32.160 --> 00:53:37.340
the Markov logic network,
covered the natural language.

00:53:37.340 --> 00:53:39.224
Like, is there a
one-to-one mapping between

00:53:39.224 --> 00:53:40.390
the representation and the--

00:53:40.390 --> 00:53:40.600
PEDRO DOMINGOS: Yeah.

00:53:40.600 --> 00:53:42.120
In fact, one of
the biggest areas

00:53:42.120 --> 00:53:44.557
where Markov logic has been
applied is natural language.

00:53:44.557 --> 00:53:46.390
It's a very good match
for natural language,

00:53:46.390 --> 00:53:49.620
because natural language is
both very compositional-- right,

00:53:49.620 --> 00:53:53.150
so you need the logic for
that-- and also very ambiguous,

00:53:53.150 --> 00:53:55.710
very uncertain, very noisy.

00:53:55.710 --> 00:53:57.290
So you need a
probability for that.

00:53:57.290 --> 00:53:59.790
So people, at this point, have
applied Markov logic networks

00:53:59.790 --> 00:54:04.360
to pretty much every major
problem in natural language,

00:54:04.360 --> 00:54:06.296
and won competitions
using it, and so forth.

00:54:06.296 --> 00:54:07.670
So in some ways,
natural language

00:54:07.670 --> 00:54:11.072
is one of the killer apps
for Markov logic networks.

00:54:11.072 --> 00:54:12.960
AUDIENCE: Is there a
way for us to learn

00:54:12.960 --> 00:54:16.520
to use it in the genetic
programming algorithms?

00:54:16.520 --> 00:54:18.270
PEDRO DOMINGOS: So the
question is, are we

00:54:18.270 --> 00:54:21.300
able to learn Markov
logic networks using

00:54:21.300 --> 00:54:23.050
genetic programming.

00:54:23.050 --> 00:54:24.890
People haven't done that yet.

00:54:24.890 --> 00:54:28.860
So we've learned-- there's
a wide range of algorithms

00:54:28.860 --> 00:54:31.410
for learning the structure
of Markov logic networks.

00:54:31.410 --> 00:54:33.480
They are similar to
genetic programming,

00:54:33.480 --> 00:54:34.710
but without the crossover.

00:54:34.710 --> 00:54:36.430
All right?

00:54:36.430 --> 00:54:38.530
The Evolutionaries really
believe in crossover.

00:54:38.530 --> 00:54:39.946
Everybody else in
machine learning

00:54:39.946 --> 00:54:42.260
thinks something like hill
climbing, greedy search,

00:54:42.260 --> 00:54:43.970
or beam search is
probably enough.

00:54:43.970 --> 00:54:45.620
And there are many
different search

00:54:45.620 --> 00:54:48.320
methods that have been used
and that work pretty well.

00:54:48.320 --> 00:54:49.620
It's actually a good question.

00:54:49.620 --> 00:54:52.520
That part has not been done
yet, whether actually doing

00:54:52.520 --> 00:54:55.240
the crossover between
the formulas will help.

00:54:55.240 --> 00:54:57.780
AUDIENCE: Related question
to genetic programming--

00:54:57.780 --> 00:54:59.720
how is the solution
space limited?

00:54:59.720 --> 00:55:02.090
Because it seems that
combining various sub-trees

00:55:02.090 --> 00:55:05.335
would lead to a rapidly fast,
exponentially growing solution

00:55:05.335 --> 00:55:05.835
space.

00:55:05.835 --> 00:55:10.680
And has there been any
success with NP-hard problems?

00:55:10.680 --> 00:55:14.610
PEDRO DOMINGOS: So something
that people have observed

00:55:14.610 --> 00:55:17.570
in genetic programming is
that the trees tend to get

00:55:17.570 --> 00:55:19.120
bigger and bigger over time.

00:55:19.120 --> 00:55:22.470
People jokingly call it the
"survival of the fattest."

00:55:22.470 --> 00:55:24.180
What you can do
to combat that is

00:55:24.180 --> 00:55:26.040
to actually have
a bias, a penalty

00:55:26.040 --> 00:55:27.640
on the size of the trees.

00:55:27.640 --> 00:55:30.360
So if the trees are big, they're
less fit just because of that.

00:55:30.360 --> 00:55:32.140
You put that into your
objective function.

00:55:34.940 --> 00:55:37.270
AUDIENCE: NP-hard
problems-- any success

00:55:37.270 --> 00:55:40.210
in that area with genetic
algorithms or programming?

00:55:40.210 --> 00:55:43.790
PEDRO DOMINGOS: Well, the
short answer to that is yes

00:55:43.790 --> 00:55:45.316
and the longer
answer is yes, but.

00:55:45.316 --> 00:55:46.690
A lot of the
problems that people

00:55:46.690 --> 00:55:48.315
are approaching with
genetic algorithms

00:55:48.315 --> 00:55:50.260
are NP hard problems.

00:55:50.260 --> 00:55:53.680
The question that the people
who aren't Evolutionaries ask

00:55:53.680 --> 00:55:56.167
is, did you really
solve-- so first of all,

00:55:56.167 --> 00:55:57.500
they're NP-hard problems, right?

00:55:57.500 --> 00:56:00.410
So you can't solve the
worst, the hardest instances

00:56:00.410 --> 00:56:01.230
efficiently.

00:56:01.230 --> 00:56:03.438
But there are many instances
that aren't the hardest.

00:56:03.438 --> 00:56:06.370
And so, can you solve
them well enough?

00:56:06.370 --> 00:56:10.835
And there are examples of
genetic algorithms solving

00:56:10.835 --> 00:56:11.460
these problems.

00:56:11.460 --> 00:56:12.330
But there are also
counterexamples

00:56:12.330 --> 00:56:13.970
of people saying,
look, I could have

00:56:13.970 --> 00:56:15.450
done that with hill
climbing and it

00:56:15.450 --> 00:56:17.440
would have been just as
fast or just as good.

00:56:17.440 --> 00:56:20.870
So the jury's still out on this.

00:56:20.870 --> 00:56:24.580
AUDIENCE: So with your
360-degree recommender systems,

00:56:24.580 --> 00:56:28.740
what's to keep that from being
self-fulfilling or guiding

00:56:28.740 --> 00:56:32.690
a person into some particular
path that's not, perhaps,

00:56:32.690 --> 00:56:33.402
[INAUDIBLE]?

00:56:33.402 --> 00:56:35.110
PEDRO DOMINGOS: What's
going to keep that

00:56:35.110 --> 00:56:37.651
from being self-fulfilling is
the way machine learning works,

00:56:37.651 --> 00:56:40.202
which is the system
recommends something to you.

00:56:40.202 --> 00:56:42.410
Or for example, it recommends
10 alternatives to you.

00:56:42.410 --> 00:56:42.982
All right?

00:56:42.982 --> 00:56:45.190
Or let's say it recommends
one, but then you say, no,

00:56:45.190 --> 00:56:46.148
this was the wrong one.

00:56:46.148 --> 00:56:47.451
Then it learns from that.

00:56:47.451 --> 00:56:49.200
So the recommender
system is not something

00:56:49.200 --> 00:56:51.972
that you learn one day
offline and then you use it.

00:56:51.972 --> 00:56:53.430
It's something that
is continuously

00:56:53.430 --> 00:56:55.300
learning from what you do.

00:56:55.300 --> 00:56:58.890
So if it starts doing the wrong
thing, you start being unhappy

00:56:58.890 --> 00:57:01.030
and you start displaying
that unhappiness

00:57:01.030 --> 00:57:02.290
in one form or another.

00:57:02.290 --> 00:57:04.750
And then it learns, from
that, to try something else.

00:57:04.750 --> 00:57:08.317
And also remember,
these systems, they

00:57:08.317 --> 00:57:09.900
can talk with each
other to the extent

00:57:09.900 --> 00:57:11.567
that you decide that they can.

00:57:11.567 --> 00:57:13.150
So it won't just be
learning from you,

00:57:13.150 --> 00:57:14.858
it will be learning
from a lot of people.

00:57:14.858 --> 00:57:15.510
All right?

00:57:15.510 --> 00:57:20.500
So it's always the
question of, how far do

00:57:20.500 --> 00:57:21.810
you have to generalize, right?

00:57:21.810 --> 00:57:24.410
The more data that you have,
the easier the problem becomes.

00:57:24.410 --> 00:57:26.920
And the more you have a
continuous loop of feedback,

00:57:26.920 --> 00:57:28.702
the more robust the learning is.

00:57:28.702 --> 00:57:30.410
AUDIENCE: The 360
recommender, how do you

00:57:30.410 --> 00:57:33.225
square it with the need
for privacy for people?

00:57:33.225 --> 00:57:35.600
PEDRO DOMINGOS: Yeah, so that's
a very important question

00:57:35.600 --> 00:57:37.090
which I didn't allude to.

00:57:37.090 --> 00:57:40.020
I want to have a
360-degree model of me,

00:57:40.020 --> 00:57:42.460
but I want it to be
under my control.

00:57:42.460 --> 00:57:44.210
Because if somebody
else has a model of me

00:57:44.210 --> 00:57:46.230
that knows me better
than my best friend,

00:57:46.230 --> 00:57:48.440
they have too much power.

00:57:48.440 --> 00:57:52.620
So I think what needs to happen
is that-- right now, what

00:57:52.620 --> 00:57:54.530
happens is that the
data that you generate

00:57:54.530 --> 00:57:56.770
is spread all over the place.

00:57:56.770 --> 00:57:59.180
It would be good to bring
it all to one place,

00:57:59.180 --> 00:58:02.000
but that place has to
be under your control.

00:58:02.000 --> 00:58:03.960
And I think one way
that this could work

00:58:03.960 --> 00:58:06.240
is that you could have a
company that is to a data

00:58:06.240 --> 00:58:08.470
like a bank is to your money.

00:58:08.470 --> 00:58:09.080
Right?

00:58:09.080 --> 00:58:10.680
You put your money in the
bank, and then the bank

00:58:10.680 --> 00:58:12.770
invests it for you,
and so on and so forth.

00:58:12.770 --> 00:58:15.530
But ultimately, the money
is under your control.

00:58:15.530 --> 00:58:18.170
So I think the same thing
should happen with your data.

00:58:18.170 --> 00:58:20.150
So this company
aggregates the data.

00:58:20.150 --> 00:58:21.240
It learns the model.

00:58:21.240 --> 00:58:22.645
It uses the model
on your behalf.

00:58:22.645 --> 00:58:24.270
But at the end of
the day, it's for you

00:58:24.270 --> 00:58:26.270
to decide whether,
maybe, for example, you

00:58:26.270 --> 00:58:27.770
want to take the
data somewhere else

00:58:27.770 --> 00:58:29.430
or you want to do
something else with it.

00:58:29.430 --> 00:58:31.470
I think if you don't do
that, people won't trust

00:58:31.470 --> 00:58:33.300
this enough for it to happen.

00:58:33.300 --> 00:58:35.407
AUDIENCE: What are your
thoughts on TensorFlow?

00:58:35.407 --> 00:58:37.740
PEDRO DOMINGOS: I haven't
played with TensorFlow myself.

00:58:37.740 --> 00:58:39.400
I think, first of
all, it's great

00:58:39.400 --> 00:58:41.690
that TensorFlow
has been released.

00:58:41.690 --> 00:58:43.410
I think releasing
open source software

00:58:43.410 --> 00:58:47.060
like this is a large part
of how we make progress.

00:58:47.060 --> 00:58:49.870
And definitely, deep learning
and doing it on a large scale

00:58:49.870 --> 00:58:51.020
are very important.

00:58:51.020 --> 00:58:53.730
There are a number of other
alternatives out there.

00:58:53.730 --> 00:58:57.040
We'll see how TensorFlow
compares with them.

00:58:57.040 --> 00:59:00.220
And we'll also see-- like just
from my own point of view--

00:59:00.220 --> 00:59:02.709
I have some of my students
working on deep learning--

00:59:02.709 --> 00:59:04.500
the question with each
one of these systems

00:59:04.500 --> 00:59:07.830
is, what does it
support well versus not.

00:59:07.830 --> 00:59:10.000
I think if the learning
that you're trying to do

00:59:10.000 --> 00:59:12.360
fits within the
paradigm of TensorFlow,

00:59:12.360 --> 00:59:14.184
then it's probably
a good thing to use.

00:59:14.184 --> 00:59:16.350
But if it doesn't, then you
may need something else.

00:59:16.350 --> 00:59:18.433
Or maybe what's going to
happen is that there will

00:59:18.433 --> 00:59:22.549
be an extension of TensorFlow to
do these things in the future.

00:59:22.549 --> 00:59:24.090
AUDIENCE: One of
its goals, actually,

00:59:24.090 --> 00:59:28.334
is to become a universal system
for expressing AI solutions.

00:59:28.334 --> 00:59:31.000
PEDRO DOMINGOS: Yeah, and I very
much sympathize with that goal.

00:59:31.000 --> 00:59:33.360
I think that in order to
be a universal system,

00:59:33.360 --> 00:59:35.760
you have to cover
these five paradigms.

00:59:35.760 --> 00:59:38.660
But one way to cover them
is to start from one--

00:59:38.660 --> 00:59:40.400
let's say deep
learning-- and then try

00:59:40.400 --> 00:59:42.474
to absorb another, and
another, and another.

00:59:42.474 --> 00:59:44.390
And there are people
doing this kind of thing.

00:59:44.390 --> 00:59:46.810
Like for example, we
have the Alchemy System,

00:59:46.810 --> 00:59:54.810
which combines the Symbolist
and the Bayesian learning.

00:59:54.810 --> 00:59:56.500
We've also developed,
for example,

00:59:56.500 --> 00:59:58.640
combinations of
symbolic learning

00:59:58.640 --> 01:00:03.830
and instance-based learning
and this with neural networks.

01:00:03.830 --> 01:00:07.970
So what I hope to see from
a platform like TensorFlow

01:00:07.970 --> 01:00:11.770
is see it absorbing more and
more of these capabilities.

01:00:11.770 --> 01:00:13.370
And absorbing them
doesn't necessarily

01:00:13.370 --> 01:00:16.230
mean going and giving
us some primitives

01:00:16.230 --> 01:00:18.220
to do what that school does.

01:00:18.220 --> 01:00:21.600
That's OK, but it also
increases the complexity.

01:00:21.600 --> 01:00:23.480
The ideal thing
would be you still

01:00:23.480 --> 01:00:27.747
have something that is simple,
but yet with that simple thing,

01:00:27.747 --> 01:00:29.580
you can do all these
combinations of things.

01:00:29.580 --> 01:00:31.280
And I do think that's possible.

01:00:31.280 --> 01:00:32.870
And I'm curious to
see how this is all

01:00:32.870 --> 01:00:36.812
going to evolve in the
next several years.

01:00:36.812 --> 01:00:38.270
MALE SPEAKER: And
with that, please

01:00:38.270 --> 01:00:40.167
join me in thanking
Pedro for this talk.

01:00:40.167 --> 01:00:41.250
PEDRO DOMINGOS: Thank you.

01:00:41.250 --> 01:00:44.600
[APPLAUSE]

