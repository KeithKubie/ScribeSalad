WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:02.730
Okay Charles so we've been talking
about temporal difference learning.

00:00:02.730 --> 00:00:06.420
And in particular various flavors
of the TD lambda algorithm.

00:00:06.420 --> 00:00:08.970
Can you remember some of the important
things that we've touched on?

00:00:08.970 --> 00:00:10.950
&gt;&gt; Well I think you
just said the big one.

00:00:10.950 --> 00:00:15.300
That all this discussion has really
been about temporal difference learning.

00:00:15.300 --> 00:00:17.870
&gt;&gt; And in a nutshell what's
a temporal difference?

00:00:17.870 --> 00:00:22.240
&gt;&gt; Well it's the difference
between things you see on

00:00:22.240 --> 00:00:24.180
different subsequent time steps.

00:00:24.180 --> 00:00:26.640
&gt;&gt; What kind of things
in different time steps?

00:00:26.640 --> 00:00:27.500
&gt;&gt; Reward.

00:00:27.500 --> 00:00:31.230
&gt;&gt; So, sort of the difference in value
estimates as we go from one step to

00:00:31.230 --> 00:00:31.960
another, right?

00:00:31.960 --> 00:00:35.135
&gt;&gt; That's exactly right, because value
is just a nice summary statistic for

00:00:35.135 --> 00:00:35.960
long-term reward.

00:00:35.960 --> 00:00:36.900
&gt;&gt; Good, all right.

00:00:36.900 --> 00:00:41.410
&gt;&gt; But actually, I think it was
important that you started off talking

00:00:41.410 --> 00:00:45.020
about this is the thing that we wanted
to do, but first you put it in context.

00:00:45.020 --> 00:00:47.680
By talking about different

00:00:47.680 --> 00:00:49.920
ways that we could solve
the reinforcement learning problem.

00:00:51.750 --> 00:00:57.600
As I recall, there were three ways to
think about it: model based learning,

00:00:57.600 --> 00:01:01.730
value based learning, and
let's say policy based learning.

00:01:01.730 --> 00:01:03.980
&gt;&gt; That's right and
TD methods kind of fall where?

00:01:04.989 --> 00:01:06.470
&gt;&gt; In the value based.

00:01:06.470 --> 00:01:09.840
&gt;&gt; All right then, we dove into
the update rules themselves.

00:01:09.840 --> 00:01:13.160
So what do we talk about there?

00:01:13.160 --> 00:01:15.711
&gt;&gt; We actually deroved?

00:01:15.711 --> 00:01:17.780
&gt;&gt; [LAUGHS] Derived?

00:01:17.780 --> 00:01:19.830
&gt;&gt; Either one of those things.

00:01:19.830 --> 00:01:21.270
It's just semantics.

00:01:21.270 --> 00:01:23.060
This kind of incremental way.

00:01:23.060 --> 00:01:28.330
Of building up estimates of the values,
and I think you called it outcome based.

00:01:28.330 --> 00:01:32.270
And I suppose all that really means,
of course, is that you actually look at

00:01:32.270 --> 00:01:36.540
the outcomes of what you experience
from different trajectories.

00:01:36.540 --> 00:01:39.930
And episodes through the space,
and you use that to build up

00:01:39.930 --> 00:01:43.620
your sort of estimate of what
the values of various states are.

00:01:44.620 --> 00:01:47.780
&gt;&gt; Yeah, and in particular,
the idea of an outcome based says, well.

00:01:47.780 --> 00:01:50.070
You can kind of treat
reinforcement learning as a kind of

00:01:50.070 --> 00:01:51.080
supervised learning.

00:01:51.080 --> 00:01:54.250
Just wait for time to stop and
see what happened, and

00:01:54.250 --> 00:01:56.050
use that as a training example.

00:01:56.050 --> 00:01:57.790
But we can't really wait for
time to end,

00:01:57.790 --> 00:02:00.330
because it's kind of too late to
use any of the information there.

00:02:00.330 --> 00:02:02.140
So what we want are incremental methods,

00:02:02.140 --> 00:02:04.060
methods that can actually
do updates along the way.

00:02:04.060 --> 00:02:06.500
That would be equivalent to
waiting until the end, but

00:02:06.500 --> 00:02:08.101
actually gives us results sooner.

00:02:08.101 --> 00:02:10.858
&gt;&gt; Right, and there was a technical
point that you made that I guess is

00:02:10.858 --> 00:02:12.295
something that we learned about.

00:02:12.295 --> 00:02:14.581
Which we had talked about in
the old machine learning course but

00:02:14.581 --> 00:02:15.928
we didn't go into any detail about.

00:02:15.928 --> 00:02:19.978
Which is this sort of incremental update
by making it look like a supervised

00:02:19.978 --> 00:02:23.662
learning rule, just look like
the normal perceptron update rule.

00:02:23.662 --> 00:02:26.088
And it had a learning rate.

00:02:26.088 --> 00:02:29.028
And in order for us to be sure that
we're going to learn something that we

00:02:29.028 --> 00:02:32.380
wanted to learn, that learning rate
had to have certain properties.

00:02:32.380 --> 00:02:33.300
&gt;&gt; Great, that's exactly right.

00:02:33.300 --> 00:02:34.260
And in a nutshell,

00:02:34.260 --> 00:02:37.260
what were these properties that
the learning rate had to have?

00:02:37.260 --> 00:02:41.430
&gt;&gt; Well, let's see, that the sum over
all the learning rate values, so

00:02:41.430 --> 00:02:46.300
let's call that Alpha T for
time, needed to be infinite.

00:02:46.300 --> 00:02:53.385
And that the sum of the learning
rate squared needed to be finite.

00:02:55.350 --> 00:02:59.290
&gt;&gt; Yeah, so intuitively we want a
learning rate sequences that would allow

00:02:59.290 --> 00:03:02.665
us to move the value to wherever
it needed to go, to converge.

00:03:02.665 --> 00:03:05.198
But would dampen overtime, decay so

00:03:05.198 --> 00:03:11.350
that the estimates wouldn't keep bumping
around as a function of the noise.

00:03:11.350 --> 00:03:12.530
&gt;&gt; Right, I think that's great.

00:03:12.530 --> 00:03:16.160
Well, anyway I think that pretty
much brought us to the end.

00:03:16.160 --> 00:03:21.262
We took a lot of slides to get there,
but it turned out the algorithm that we

00:03:21.262 --> 00:03:26.520
derived looked a lot like something we
call TD(1) give it a nice little name.

00:03:26.520 --> 00:03:30.200
And TD(1) was very nice
except it was inefficient.

00:03:30.200 --> 00:03:31.420
In the way that it used data.

00:03:31.420 --> 00:03:34.700
&gt;&gt; Yeah, I don't know that everybody
would agree exactly with that

00:03:34.700 --> 00:03:36.600
way of characterizing it.

00:03:36.600 --> 00:03:40.910
But it is true that it had this
sort of weird property that it was,

00:03:40.910 --> 00:03:42.222
there's a lot of variability in it.

00:03:42.222 --> 00:03:44.740
There was a very high variance,
because if we had one run,

00:03:44.740 --> 00:03:49.930
we had a value that we're
estimating from one place and.

00:03:49.930 --> 00:03:55.030
The estimate was based
on a very noisy example,

00:03:55.030 --> 00:03:56.750
then we're going to have
a very noisy estimate.

00:03:56.750 --> 00:03:59.530
It wasn't going to be able to use
the rest of the examples that it's seen

00:03:59.530 --> 00:04:00.430
to kind of smooth this out.

00:04:01.510 --> 00:04:04.710
&gt;&gt; Right it didn't use
information along the way.

00:04:04.710 --> 00:04:07.170
It used information sort of
at the end of every episode.

00:04:07.170 --> 00:04:09.330
And that's what made it
sort of data inefficient.

00:04:09.330 --> 00:04:11.264
And I guess in the end that
gives you high variance.

00:04:11.264 --> 00:04:12.769
But we came to the rescue and

00:04:12.769 --> 00:04:15.860
when I say we I mean you came
to the rescue with TD(0).

00:04:15.860 --> 00:04:19.952
Which has the nice property that it
gives you the maximum likelihood

00:04:19.952 --> 00:04:20.680
estimate.

00:04:20.680 --> 00:04:23.360
&gt;&gt; I prefer to think of it as
the Micheal Litman estimate.

00:04:23.360 --> 00:04:24.006
&gt;&gt; Yeah, I know you do.

00:04:24.006 --> 00:04:28.510
I've see how you snuck that in and then
it got us to the very end which is good

00:04:28.510 --> 00:04:30.330
because you're running out
of space on the slide.

00:04:30.330 --> 00:04:36.071
We generalize TD1 and
TD0 into something we called TD lambda.

00:04:36.071 --> 00:04:39.057
&gt;&gt; That has the property that when
lambda is zero we get TD(0) and

00:04:39.057 --> 00:04:40.725
when lambda is one we get TD (1).

00:04:40.725 --> 00:04:42.931
What about the intermediate values?

00:04:42.931 --> 00:04:44.949
&gt;&gt; Well,
then that just gives you dubstep.

00:04:45.980 --> 00:04:47.040
&gt;&gt; Wait what?

00:04:47.040 --> 00:04:47.790
&gt;&gt; It gives you dub step.

00:04:47.790 --> 00:04:49.224
You get the dub step look ahead.

00:04:49.224 --> 00:04:55.197
[SOUND] [SOUND] [SOUND]
&gt;&gt; [LAUGH] I see that's right,

00:04:55.197 --> 00:04:57.390
multiple steps of updates.

00:04:57.390 --> 00:05:01.090
I see what you mean by dub step,
like double the number of steps.

00:05:01.090 --> 00:05:01.720
&gt;&gt; Oh yeah I like that.

00:05:01.720 --> 00:05:02.770
That sounds like I meant to do that.

00:05:02.770 --> 00:05:03.790
Yeah let's go with that.

00:05:03.790 --> 00:05:05.260
And there you go,
I think that's just about everything.

00:05:05.260 --> 00:05:10.109
Oh, one more thing, is that you talked
a little bit about different values of

00:05:10.109 --> 00:05:13.624
lambda and why you might
choose one value over another.

00:05:13.624 --> 00:05:17.666
And it turns out that values
of lambda between 0.3 and

00:05:17.666 --> 00:05:22.000
0.7 empirically seem
to work pretty well.

00:05:22.000 --> 00:05:25.495
Yeah, because it's somehow
making a nice trade-off

00:05:25.495 --> 00:05:28.990
between estimating the right
kind of quantity and

00:05:28.990 --> 00:05:33.499
estimating it using the values
that you get from looking ahead.

00:05:33.499 --> 00:05:34.272
&gt;&gt; Right, and

00:05:34.272 --> 00:05:39.460
first off that's neat because otherwise
you wouldn't need to have TD lambda.

00:05:39.460 --> 00:05:41.730
But it's in some ways
a little surprising.

00:05:41.730 --> 00:05:42.910
&gt;&gt; Why is that?

00:05:42.910 --> 00:05:44.810
&gt;&gt; Well because it kind of
just works out the way that

00:05:44.810 --> 00:05:45.720
you would want it work out.

00:05:45.720 --> 00:05:48.820
You went through all this trouble to
kind of generalize between things.

00:05:48.820 --> 00:05:51.810
You have this kind of highly
non-linear way of combining

00:05:51.810 --> 00:05:53.890
all the different k step look aheads.

00:05:53.890 --> 00:05:58.410
And it terms out that combining them so
it's not quite 0 it's not quite one

00:05:58.410 --> 00:06:03.140
is better than either 0 or 1, and
often that's not how it works.

00:06:03.140 --> 00:06:05.987
Right, I mean often it's like
a line from 0 to 1 is bad.

00:06:05.987 --> 00:06:08.310
It's like a line, or
it gets really better,

00:06:08.310 --> 00:06:12.180
then it sort of flattens out
towards the end, like you said.

00:06:12.180 --> 00:06:14.670
But, in this case it just gets better,
and better, and better, and

00:06:14.670 --> 00:06:18.310
then it starts to get worse when
you go from one end to the other.

00:06:18.310 --> 00:06:18.990
&gt;&gt; Yeah that's right.
So

00:06:18.990 --> 00:06:22.920
often when you combine two good things,
you get the worst of both.

00:06:22.920 --> 00:06:25.760
In this case, it does somehow
manage to get the best of both.

00:06:25.760 --> 00:06:26.720
&gt;&gt; Yeah, that's pretty cool.

00:06:26.720 --> 00:06:33.528
&gt;&gt; All right, so what I'd like to dive
into next is a deeper understanding of.

00:06:33.528 --> 00:06:37.026
How we could show that these kinds
of methods actually converge, and

00:06:37.026 --> 00:06:41.450
they actually produce optimal estimates
and optimal behavior in the limit.

00:06:41.450 --> 00:06:42.960
And so
we're going to take a step back and

00:06:42.960 --> 00:06:44.580
look at the Belmont
equation again to do that.

00:06:45.580 --> 00:06:46.460
&gt;&gt; Oh I like it.

00:06:46.460 --> 00:06:47.990
I love the Belmont equation.

00:06:47.990 --> 00:06:50.680
&gt;&gt; All right see you then then.

00:06:50.680 --> 00:06:51.840
&gt;&gt; All right see you, bye.

