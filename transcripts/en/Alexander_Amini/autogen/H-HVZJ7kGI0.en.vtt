WEBVTT
Kind: captions
Language: en

00:00:02.570 --> 00:00:05.540
 
all right so let's get started so thank

00:00:05.540 --> 00:00:05.550
all right so let's get started so thank
 

00:00:05.550 --> 00:00:08.180
all right so let's get started so thank
you all for coming to day two of six

00:00:08.180 --> 00:00:08.190
you all for coming to day two of six
 

00:00:08.190 --> 00:00:11.810
you all for coming to day two of six
s-191 we're really excited to to have

00:00:11.810 --> 00:00:11.820
s-191 we're really excited to to have
 

00:00:11.820 --> 00:00:13.699
s-191 we're really excited to to have
you for these two lectures and lab today

00:00:13.699 --> 00:00:13.709
you for these two lectures and lab today
 

00:00:13.709 --> 00:00:16.160
you for these two lectures and lab today
which are largely going to be focused on

00:00:16.160 --> 00:00:16.170
which are largely going to be focused on
 

00:00:16.170 --> 00:00:21.590
which are largely going to be focused on
deep learning for computer vision so to

00:00:21.590 --> 00:00:21.600
deep learning for computer vision so to
 

00:00:21.600 --> 00:00:24.410
deep learning for computer vision so to
motivate I think we can all agree that

00:00:24.410 --> 00:00:24.420
motivate I think we can all agree that
 

00:00:24.420 --> 00:00:26.690
motivate I think we can all agree that
vision is one of the most important

00:00:26.690 --> 00:00:26.700
vision is one of the most important
 

00:00:26.700 --> 00:00:29.000
vision is one of the most important
human senses and sighted people rely on

00:00:29.000 --> 00:00:29.010
human senses and sighted people rely on
 

00:00:29.010 --> 00:00:31.880
human senses and sighted people rely on
vision quite a lot for everything from

00:00:31.880 --> 00:00:31.890
vision quite a lot for everything from
 

00:00:31.890 --> 00:00:34.369
vision quite a lot for everything from
navigating in the physical world to

00:00:34.369 --> 00:00:34.379
navigating in the physical world to
 

00:00:34.379 --> 00:00:37.550
navigating in the physical world to
recognizing and manipulating objects to

00:00:37.550 --> 00:00:37.560
recognizing and manipulating objects to
 

00:00:37.560 --> 00:00:39.590
recognizing and manipulating objects to
interpreting facial expressions and

00:00:39.590 --> 00:00:39.600
interpreting facial expressions and
 

00:00:39.600 --> 00:00:42.110
interpreting facial expressions and
understanding emotion and I think it's

00:00:42.110 --> 00:00:42.120
understanding emotion and I think it's
 

00:00:42.120 --> 00:00:44.300
understanding emotion and I think it's
safe to say that for many of for all of

00:00:44.300 --> 00:00:44.310
safe to say that for many of for all of
 

00:00:44.310 --> 00:00:46.580
safe to say that for many of for all of
us or many of us vision is a huge part

00:00:46.580 --> 00:00:46.590
us or many of us vision is a huge part
 

00:00:46.590 --> 00:00:49.460
us or many of us vision is a huge part
of our lives and that's largely thanks

00:00:49.460 --> 00:00:49.470
of our lives and that's largely thanks
 

00:00:49.470 --> 00:00:52.790
of our lives and that's largely thanks
to the power of evolution evolutionary

00:00:52.790 --> 00:00:52.800
to the power of evolution evolutionary
 

00:00:52.800 --> 00:00:55.220
to the power of evolution evolutionary
biologists traced the origins of vision

00:00:55.220 --> 00:00:55.230
biologists traced the origins of vision
 

00:00:55.230 --> 00:00:58.250
biologists traced the origins of vision
back 540 million years ago to the

00:00:58.250 --> 00:00:58.260
back 540 million years ago to the
 

00:00:58.260 --> 00:01:01.100
back 540 million years ago to the
Cambrian explosion and the reason that

00:01:01.100 --> 00:01:01.110
Cambrian explosion and the reason that
 

00:01:01.110 --> 00:01:04.429
Cambrian explosion and the reason that
vision seems so easy for us as humans is

00:01:04.429 --> 00:01:04.439
vision seems so easy for us as humans is
 

00:01:04.439 --> 00:01:07.489
vision seems so easy for us as humans is
because we have 540 million years of

00:01:07.489 --> 00:01:07.499
because we have 540 million years of
 

00:01:07.499 --> 00:01:09.709
because we have 540 million years of
data that evolution has effectively

00:01:09.709 --> 00:01:09.719
data that evolution has effectively
 

00:01:09.719 --> 00:01:12.319
data that evolution has effectively
trained on and if you compare that to

00:01:12.319 --> 00:01:12.329
trained on and if you compare that to
 

00:01:12.329 --> 00:01:15.319
trained on and if you compare that to
other capabilities like bipedal movement

00:01:15.319 --> 00:01:15.329
other capabilities like bipedal movement
 

00:01:15.329 --> 00:01:18.019
other capabilities like bipedal movement
and language the difference is is quite

00:01:18.019 --> 00:01:18.029
and language the difference is is quite
 

00:01:18.029 --> 00:01:21.230
and language the difference is is quite
significant and starting in the 1960s

00:01:21.230 --> 00:01:21.240
significant and starting in the 1960s
 

00:01:21.240 --> 00:01:23.870
significant and starting in the 1960s
there was a surge of interest in both

00:01:23.870 --> 00:01:23.880
there was a surge of interest in both
 

00:01:23.880 --> 00:01:26.859
there was a surge of interest in both
the neural basis of vision and how to

00:01:26.859 --> 00:01:26.869
the neural basis of vision and how to
 

00:01:26.869 --> 00:01:29.480
the neural basis of vision and how to
systematically characterize visual

00:01:29.480 --> 00:01:29.490
systematically characterize visual
 

00:01:29.490 --> 00:01:31.999
systematically characterize visual
processing and this led to computer

00:01:31.999 --> 00:01:32.009
processing and this led to computer
 

00:01:32.009 --> 00:01:34.789
processing and this led to computer
scientists beginning to wonder about how

00:01:34.789 --> 00:01:34.799
scientists beginning to wonder about how
 

00:01:34.799 --> 00:01:36.830
scientists beginning to wonder about how
findings in neuroscience could be

00:01:36.830 --> 00:01:36.840
findings in neuroscience could be
 

00:01:36.840 --> 00:01:39.769
findings in neuroscience could be
applied to achieve artificial computer

00:01:39.769 --> 00:01:39.779
applied to achieve artificial computer
 

00:01:39.779 --> 00:01:42.739
applied to achieve artificial computer
vision and it all started with these

00:01:42.739 --> 00:01:42.749
vision and it all started with these
 

00:01:42.749 --> 00:01:45.760
vision and it all started with these
series of seminal experiments from two

00:01:45.760 --> 00:01:45.770
series of seminal experiments from two
 

00:01:45.770 --> 00:01:48.409
series of seminal experiments from two
neuroscientists David Hubel and Torsten

00:01:48.409 --> 00:01:48.419
neuroscientists David Hubel and Torsten
 

00:01:48.419 --> 00:01:50.419
neuroscientists David Hubel and Torsten
vessel who were working at Harvard at

00:01:50.419 --> 00:01:50.429
vessel who were working at Harvard at
 

00:01:50.429 --> 00:01:52.370
vessel who were working at Harvard at
the time and they were looking at

00:01:52.370 --> 00:01:52.380
the time and they were looking at
 

00:01:52.380 --> 00:01:55.039
the time and they were looking at
processing in the visual cortex of cats

00:01:55.039 --> 00:01:55.049
processing in the visual cortex of cats
 

00:01:55.049 --> 00:01:57.319
processing in the visual cortex of cats
and what they were able to demonstrate

00:01:57.319 --> 00:01:57.329
and what they were able to demonstrate
 

00:01:57.329 --> 00:02:00.589
and what they were able to demonstrate
was that their neural mechanisms for

00:02:00.589 --> 00:02:00.599
was that their neural mechanisms for
 

00:02:00.599 --> 00:02:03.199
was that their neural mechanisms for
spatially invariant pattern recognition

00:02:03.199 --> 00:02:03.209
spatially invariant pattern recognition
 

00:02:03.209 --> 00:02:05.569
spatially invariant pattern recognition
and that certain neurons in the visual

00:02:05.569 --> 00:02:05.579
and that certain neurons in the visual
 

00:02:05.579 --> 00:02:08.559
and that certain neurons in the visual
cortex respond very specifically to

00:02:08.559 --> 00:02:08.569
cortex respond very specifically to
 

00:02:08.569 --> 00:02:11.480
cortex respond very specifically to
specific patterns and regions of visual

00:02:11.480 --> 00:02:11.490
specific patterns and regions of visual
 

00:02:11.490 --> 00:02:14.210
specific patterns and regions of visual
stimuli and furthermore that there's an

00:02:14.210 --> 00:02:14.220
stimuli and furthermore that there's an
 

00:02:14.220 --> 00:02:15.950
stimuli and furthermore that there's an
exquisite hierarchy

00:02:15.950 --> 00:02:15.960
exquisite hierarchy
 

00:02:15.960 --> 00:02:18.170
exquisite hierarchy
of neural layers that exist within the

00:02:18.170 --> 00:02:18.180
of neural layers that exist within the
 

00:02:18.180 --> 00:02:20.810
of neural layers that exist within the
visual cortex and these concepts have

00:02:20.810 --> 00:02:20.820
visual cortex and these concepts have
 

00:02:20.820 --> 00:02:23.000
visual cortex and these concepts have
transformed both neuroscience and

00:02:23.000 --> 00:02:23.010
transformed both neuroscience and
 

00:02:23.010 --> 00:02:26.090
transformed both neuroscience and
artificial intelligence alike and today

00:02:26.090 --> 00:02:26.100
artificial intelligence alike and today
 

00:02:26.100 --> 00:02:28.340
artificial intelligence alike and today
we're going to learn about how to use

00:02:28.340 --> 00:02:28.350
we're going to learn about how to use
 

00:02:28.350 --> 00:02:30.950
we're going to learn about how to use
deep learning to build powerful computer

00:02:30.950 --> 00:02:30.960
deep learning to build powerful computer
 

00:02:30.960 --> 00:02:33.890
deep learning to build powerful computer
vision systems that have been have been

00:02:33.890 --> 00:02:33.900
vision systems that have been have been
 

00:02:33.900 --> 00:02:35.710
vision systems that have been have been
shown to be capable of extraordinary

00:02:35.710 --> 00:02:35.720
shown to be capable of extraordinary
 

00:02:35.720 --> 00:02:40.940
shown to be capable of extraordinary
complex computer vision tasks so now

00:02:40.940 --> 00:02:40.950
complex computer vision tasks so now
 

00:02:40.950 --> 00:02:42.440
complex computer vision tasks so now
that we've gone in a sense at a very

00:02:42.440 --> 00:02:42.450
that we've gone in a sense at a very
 

00:02:42.450 --> 00:02:44.660
that we've gone in a sense at a very
high level of why this is important and

00:02:44.660 --> 00:02:44.670
high level of why this is important and
 

00:02:44.670 --> 00:02:46.730
high level of why this is important and
sort of how our brains may process

00:02:46.730 --> 00:02:46.740
sort of how our brains may process
 

00:02:46.740 --> 00:02:48.830
sort of how our brains may process
visual information we can turn our

00:02:48.830 --> 00:02:48.840
visual information we can turn our
 

00:02:48.840 --> 00:02:52.340
visual information we can turn our
attention to what computers see how does

00:02:52.340 --> 00:02:52.350
attention to what computers see how does
 

00:02:52.350 --> 00:02:57.530
attention to what computers see how does
a computer process an image so well to a

00:02:57.530 --> 00:02:57.540
a computer process an image so well to a
 

00:02:57.540 --> 00:03:00.350
a computer process an image so well to a
computer images are just numbers so

00:03:00.350 --> 00:03:00.360
computer images are just numbers so
 

00:03:00.360 --> 00:03:02.480
computer images are just numbers so
suppose we have this picture of Abraham

00:03:02.480 --> 00:03:02.490
suppose we have this picture of Abraham
 

00:03:02.490 --> 00:03:05.720
suppose we have this picture of Abraham
Lincoln it's made up of pixels and since

00:03:05.720 --> 00:03:05.730
Lincoln it's made up of pixels and since
 

00:03:05.730 --> 00:03:07.700
Lincoln it's made up of pixels and since
this is a grayscale image each of these

00:03:07.700 --> 00:03:07.710
this is a grayscale image each of these
 

00:03:07.710 --> 00:03:11.360
this is a grayscale image each of these
pixels is just a single number and we

00:03:11.360 --> 00:03:11.370
pixels is just a single number and we
 

00:03:11.370 --> 00:03:13.940
pixels is just a single number and we
can represent this image as a 2d matrix

00:03:13.940 --> 00:03:13.950
can represent this image as a 2d matrix
 

00:03:13.950 --> 00:03:16.880
can represent this image as a 2d matrix
of numbers one for each pixel in the

00:03:16.880 --> 00:03:16.890
of numbers one for each pixel in the
 

00:03:16.890 --> 00:03:19.460
of numbers one for each pixel in the
image and this is how a computer sees

00:03:19.460 --> 00:03:19.470
image and this is how a computer sees
 

00:03:19.470 --> 00:03:22.880
image and this is how a computer sees
this image likewise if we were to have a

00:03:22.880 --> 00:03:22.890
this image likewise if we were to have a
 

00:03:22.890 --> 00:03:26.270
this image likewise if we were to have a
RGB color image not grayscale we can

00:03:26.270 --> 00:03:26.280
RGB color image not grayscale we can
 

00:03:26.280 --> 00:03:29.300
RGB color image not grayscale we can
represent that with a 3d array where now

00:03:29.300 --> 00:03:29.310
represent that with a 3d array where now
 

00:03:29.310 --> 00:03:31.490
represent that with a 3d array where now
we have two D matrices for each of the

00:03:31.490 --> 00:03:31.500
we have two D matrices for each of the
 

00:03:31.500 --> 00:03:35.540
we have two D matrices for each of the
channels are g and b so now that we have

00:03:35.540 --> 00:03:35.550
channels are g and b so now that we have
 

00:03:35.550 --> 00:03:38.210
channels are g and b so now that we have
a way to represent images to computers

00:03:38.210 --> 00:03:38.220
a way to represent images to computers
 

00:03:38.220 --> 00:03:40.550
a way to represent images to computers
we can think about what types of

00:03:40.550 --> 00:03:40.560
we can think about what types of
 

00:03:40.560 --> 00:03:42.980
we can think about what types of
computer computer vision tasks we can

00:03:42.980 --> 00:03:42.990
computer computer vision tasks we can
 

00:03:42.990 --> 00:03:46.520
computer computer vision tasks we can
perform and two very common types of

00:03:46.520 --> 00:03:46.530
perform and two very common types of
 

00:03:46.530 --> 00:03:48.860
perform and two very common types of
tasks and machine learning broadly are

00:03:48.860 --> 00:03:48.870
tasks and machine learning broadly are
 

00:03:48.870 --> 00:03:50.660
tasks and machine learning broadly are
those of regression and those of

00:03:50.660 --> 00:03:50.670
those of regression and those of
 

00:03:50.670 --> 00:03:53.270
those of regression and those of
classification in a regression our

00:03:53.270 --> 00:03:53.280
classification in a regression our
 

00:03:53.280 --> 00:03:56.360
classification in a regression our
output takes a continuous value while in

00:03:56.360 --> 00:03:56.370
output takes a continuous value while in
 

00:03:56.370 --> 00:03:58.640
output takes a continuous value while in
classification our output takes a single

00:03:58.640 --> 00:03:58.650
classification our output takes a single
 

00:03:58.650 --> 00:04:01.730
classification our output takes a single
class label so for example let's

00:04:01.730 --> 00:04:01.740
class label so for example let's
 

00:04:01.740 --> 00:04:03.320
class label so for example let's
consider the task of image

00:04:03.320 --> 00:04:03.330
consider the task of image
 

00:04:03.330 --> 00:04:06.230
consider the task of image
classification say we want to predict a

00:04:06.230 --> 00:04:06.240
classification say we want to predict a
 

00:04:06.240 --> 00:04:09.080
classification say we want to predict a
single label for some image and let's

00:04:09.080 --> 00:04:09.090
single label for some image and let's
 

00:04:09.090 --> 00:04:10.910
single label for some image and let's
say we have a bunch of images of US

00:04:10.910 --> 00:04:10.920
say we have a bunch of images of US
 

00:04:10.920 --> 00:04:13.070
say we have a bunch of images of US
presidents and we want to build a

00:04:13.070 --> 00:04:13.080
presidents and we want to build a
 

00:04:13.080 --> 00:04:16.160
presidents and we want to build a
classification pipeline to tell us which

00:04:16.160 --> 00:04:16.170
classification pipeline to tell us which
 

00:04:16.170 --> 00:04:18.890
classification pipeline to tell us which
President is in an image outputting the

00:04:18.890 --> 00:04:18.900
President is in an image outputting the
 

00:04:18.900 --> 00:04:21.770
President is in an image outputting the
probability that that image is of a

00:04:21.770 --> 00:04:21.780
probability that that image is of a
 

00:04:21.780 --> 00:04:25.280
probability that that image is of a
particular President and so you can

00:04:25.280 --> 00:04:25.290
particular President and so you can
 

00:04:25.290 --> 00:04:27.640
particular President and so you can
imagine right that in order to cry

00:04:27.640 --> 00:04:27.650
imagine right that in order to cry
 

00:04:27.650 --> 00:04:30.070
imagine right that in order to cry
classified these images our pipeline

00:04:30.070 --> 00:04:30.080
classified these images our pipeline
 

00:04:30.080 --> 00:04:32.439
classified these images our pipeline
needs to be able to tell what is unique

00:04:32.439 --> 00:04:32.449
needs to be able to tell what is unique
 

00:04:32.449 --> 00:04:34.480
needs to be able to tell what is unique
about a picture of Lincoln versus a

00:04:34.480 --> 00:04:34.490
about a picture of Lincoln versus a
 

00:04:34.490 --> 00:04:36.760
about a picture of Lincoln versus a
picture of Washington versus a picture

00:04:36.760 --> 00:04:36.770
picture of Washington versus a picture
 

00:04:36.770 --> 00:04:40.150
picture of Washington versus a picture
of Obama and another way to think about

00:04:40.150 --> 00:04:40.160
of Obama and another way to think about
 

00:04:40.160 --> 00:04:42.670
of Obama and another way to think about
this problem at a high level is in terms

00:04:42.670 --> 00:04:42.680
this problem at a high level is in terms
 

00:04:42.680 --> 00:04:44.439
this problem at a high level is in terms
of the features that are characteristic

00:04:44.439 --> 00:04:44.449
of the features that are characteristic
 

00:04:44.449 --> 00:04:47.560
of the features that are characteristic
of a particular class and classification

00:04:47.560 --> 00:04:47.570
of a particular class and classification
 

00:04:47.570 --> 00:04:50.680
of a particular class and classification
it can then be thought of as involving

00:04:50.680 --> 00:04:50.690
it can then be thought of as involving
 

00:04:50.690 --> 00:04:52.629
it can then be thought of as involving
detection of the features in a given

00:04:52.629 --> 00:04:52.639
detection of the features in a given
 

00:04:52.639 --> 00:04:56.290
detection of the features in a given
image and sort of deciding okay well if

00:04:56.290 --> 00:04:56.300
image and sort of deciding okay well if
 

00:04:56.300 --> 00:04:58.719
image and sort of deciding okay well if
the feature is for a particular class

00:04:58.719 --> 00:04:58.729
the feature is for a particular class
 

00:04:58.729 --> 00:05:01.689
the feature is for a particular class
are present in an image we can then

00:05:01.689 --> 00:05:01.699
are present in an image we can then
 

00:05:01.699 --> 00:05:04.150
are present in an image we can then
predict that that image is of that class

00:05:04.150 --> 00:05:04.160
predict that that image is of that class
 

00:05:04.160 --> 00:05:07.060
predict that that image is of that class
with a higher probability and so if

00:05:07.060 --> 00:05:07.070
with a higher probability and so if
 

00:05:07.070 --> 00:05:08.730
with a higher probability and so if
we're building a image classification

00:05:08.730 --> 00:05:08.740
we're building a image classification
 

00:05:08.740 --> 00:05:12.250
we're building a image classification
pipeline our model needs to know what

00:05:12.250 --> 00:05:12.260
pipeline our model needs to know what
 

00:05:12.260 --> 00:05:14.500
pipeline our model needs to know what
those features are and it needs to be

00:05:14.500 --> 00:05:14.510
those features are and it needs to be
 

00:05:14.510 --> 00:05:16.840
those features are and it needs to be
able to detect those features in an

00:05:16.840 --> 00:05:16.850
able to detect those features in an
 

00:05:16.850 --> 00:05:19.270
able to detect those features in an
image in order to generate this

00:05:19.270 --> 00:05:19.280
image in order to generate this
 

00:05:19.280 --> 00:05:22.839
image in order to generate this
prediction one way to solve this problem

00:05:22.839 --> 00:05:22.849
prediction one way to solve this problem
 

00:05:22.849 --> 00:05:25.719
prediction one way to solve this problem
is to leverage our knowledge about a

00:05:25.719 --> 00:05:25.729
is to leverage our knowledge about a
 

00:05:25.729 --> 00:05:28.450
is to leverage our knowledge about a
particular field say those of human

00:05:28.450 --> 00:05:28.460
particular field say those of human
 

00:05:28.460 --> 00:05:31.540
particular field say those of human
faces and use our prior knowledge to

00:05:31.540 --> 00:05:31.550
faces and use our prior knowledge to
 

00:05:31.550 --> 00:05:35.050
faces and use our prior knowledge to
define those features ourselves and so a

00:05:35.050 --> 00:05:35.060
define those features ourselves and so a
 

00:05:35.060 --> 00:05:37.089
define those features ourselves and so a
classification pipeline would then try

00:05:37.089 --> 00:05:37.099
classification pipeline would then try
 

00:05:37.099 --> 00:05:39.370
classification pipeline would then try
to detect these manually defined

00:05:39.370 --> 00:05:39.380
to detect these manually defined
 

00:05:39.380 --> 00:05:42.159
to detect these manually defined
features and images and use the results

00:05:42.159 --> 00:05:42.169
features and images and use the results
 

00:05:42.169 --> 00:05:44.290
features and images and use the results
of some sort of detection algorithm to

00:05:44.290 --> 00:05:44.300
of some sort of detection algorithm to
 

00:05:44.300 --> 00:05:47.589
of some sort of detection algorithm to
do the classification but there's a big

00:05:47.589 --> 00:05:47.599
do the classification but there's a big
 

00:05:47.599 --> 00:05:50.379
do the classification but there's a big
problem with this approach and if you

00:05:50.379 --> 00:05:50.389
problem with this approach and if you
 

00:05:50.389 --> 00:05:53.170
problem with this approach and if you
remember images are just 3d arrays of

00:05:53.170 --> 00:05:53.180
remember images are just 3d arrays of
 

00:05:53.180 --> 00:05:55.540
remember images are just 3d arrays of
effectively brightness values and they

00:05:55.540 --> 00:05:55.550
effectively brightness values and they
 

00:05:55.550 --> 00:05:57.700
effectively brightness values and they
can have lots and lots and lots of

00:05:57.700 --> 00:05:57.710
can have lots and lots and lots of
 

00:05:57.710 --> 00:06:00.640
can have lots and lots and lots of
variation such as occlusion variations

00:06:00.640 --> 00:06:00.650
variation such as occlusion variations
 

00:06:00.650 --> 00:06:03.879
variation such as occlusion variations
in illumination and intraclass variation

00:06:03.879 --> 00:06:03.889
in illumination and intraclass variation
 

00:06:03.889 --> 00:06:07.210
in illumination and intraclass variation
and if we want to build a robust

00:06:07.210 --> 00:06:07.220
and if we want to build a robust
 

00:06:07.220 --> 00:06:10.120
and if we want to build a robust
pipeline for doing this classification

00:06:10.120 --> 00:06:10.130
pipeline for doing this classification
 

00:06:10.130 --> 00:06:13.029
pipeline for doing this classification
task our model has to be invariant to

00:06:13.029 --> 00:06:13.039
task our model has to be invariant to
 

00:06:13.039 --> 00:06:15.159
task our model has to be invariant to
these variations while still being

00:06:15.159 --> 00:06:15.169
these variations while still being
 

00:06:15.169 --> 00:06:18.310
these variations while still being
sensitive to the differences that define

00:06:18.310 --> 00:06:18.320
sensitive to the differences that define
 

00:06:18.320 --> 00:06:22.960
sensitive to the differences that define
the individual classes even though our

00:06:22.960 --> 00:06:22.970
the individual classes even though our
 

00:06:22.970 --> 00:06:25.450
the individual classes even though our
pipeline could use these features that

00:06:25.450 --> 00:06:25.460
pipeline could use these features that
 

00:06:25.460 --> 00:06:28.870
pipeline could use these features that
we the human define where this manual

00:06:28.870 --> 00:06:28.880
we the human define where this manual
 

00:06:28.880 --> 00:06:31.270
we the human define where this manual
extraction will break down is actually

00:06:31.270 --> 00:06:31.280
extraction will break down is actually
 

00:06:31.280 --> 00:06:34.390
extraction will break down is actually
in the detection task and that's again

00:06:34.390 --> 00:06:34.400
in the detection task and that's again
 

00:06:34.400 --> 00:06:36.459
in the detection task and that's again
due to the incredible variability in

00:06:36.459 --> 00:06:36.469
due to the incredible variability in
 

00:06:36.469 --> 00:06:39.790
due to the incredible variability in
visual data because of this the

00:06:39.790 --> 00:06:39.800
visual data because of this the
 

00:06:39.800 --> 00:06:41.330
visual data because of this the
detection of these features is

00:06:41.330 --> 00:06:41.340
detection of these features is
 

00:06:41.340 --> 00:06:43.060
detection of these features is
actually really difficult in practice

00:06:43.060 --> 00:06:43.070
actually really difficult in practice
 

00:06:43.070 --> 00:06:45.740
actually really difficult in practice
because your detection algorithm would

00:06:45.740 --> 00:06:45.750
because your detection algorithm would
 

00:06:45.750 --> 00:06:48.110
because your detection algorithm would
need to withstand each of these

00:06:48.110 --> 00:06:48.120
need to withstand each of these
 

00:06:48.120 --> 00:06:52.040
need to withstand each of these
different variations so how can we do

00:06:52.040 --> 00:06:52.050
different variations so how can we do
 

00:06:52.050 --> 00:06:55.490
different variations so how can we do
better we want a way to both extract

00:06:55.490 --> 00:06:55.500
better we want a way to both extract
 

00:06:55.500 --> 00:06:58.040
better we want a way to both extract
features and detect their presence in

00:06:58.040 --> 00:06:58.050
features and detect their presence in
 

00:06:58.050 --> 00:07:00.890
features and detect their presence in
images automatically in a hierarchical

00:07:00.890 --> 00:07:00.900
images automatically in a hierarchical
 

00:07:00.900 --> 00:07:03.830
images automatically in a hierarchical
fashion and again right you came to a

00:07:03.830 --> 00:07:03.840
fashion and again right you came to a
 

00:07:03.840 --> 00:07:07.219
fashion and again right you came to a
class on deep learning we we we

00:07:07.219 --> 00:07:07.229
class on deep learning we we we
 

00:07:07.229 --> 00:07:09.140
class on deep learning we we we
hypothesize right that we could use a

00:07:09.140 --> 00:07:09.150
hypothesize right that we could use a
 

00:07:09.150 --> 00:07:11.780
hypothesize right that we could use a
neural network based approach to learn

00:07:11.780 --> 00:07:11.790
neural network based approach to learn
 

00:07:11.790 --> 00:07:14.030
neural network based approach to learn
visual features directly from data

00:07:14.030 --> 00:07:14.040
visual features directly from data
 

00:07:14.040 --> 00:07:17.659
visual features directly from data
without any you know manual definition

00:07:17.659 --> 00:07:17.669
without any you know manual definition
 

00:07:17.669 --> 00:07:20.240
without any you know manual definition
and to learn a hierarchy of these

00:07:20.240 --> 00:07:20.250
and to learn a hierarchy of these
 

00:07:20.250 --> 00:07:22.460
and to learn a hierarchy of these
features to construct a representation

00:07:22.460 --> 00:07:22.470
features to construct a representation
 

00:07:22.470 --> 00:07:24.379
features to construct a representation
of the image that's internal to the

00:07:24.379 --> 00:07:24.389
of the image that's internal to the
 

00:07:24.389 --> 00:07:27.409
of the image that's internal to the
network for example if we wanted to be

00:07:27.409 --> 00:07:27.419
network for example if we wanted to be
 

00:07:27.419 --> 00:07:29.930
network for example if we wanted to be
able to classify images of faces maybe

00:07:29.930 --> 00:07:29.940
able to classify images of faces maybe
 

00:07:29.940 --> 00:07:32.629
able to classify images of faces maybe
we could learn how to detect low-level

00:07:32.629 --> 00:07:32.639
we could learn how to detect low-level
 

00:07:32.639 --> 00:07:35.330
we could learn how to detect low-level
features like edges and dark spots mid

00:07:35.330 --> 00:07:35.340
features like edges and dark spots mid
 

00:07:35.340 --> 00:07:38.060
features like edges and dark spots mid
level features like eyes ears and noses

00:07:38.060 --> 00:07:38.070
level features like eyes ears and noses
 

00:07:38.070 --> 00:07:40.189
level features like eyes ears and noses
and then high level features that

00:07:40.189 --> 00:07:40.199
and then high level features that
 

00:07:40.199 --> 00:07:43.540
and then high level features that
actually resemble facial structure and

00:07:43.540 --> 00:07:43.550
actually resemble facial structure and
 

00:07:43.550 --> 00:07:46.909
actually resemble facial structure and
we'll see how neural networks will allow

00:07:46.909 --> 00:07:46.919
we'll see how neural networks will allow
 

00:07:46.919 --> 00:07:49.279
we'll see how neural networks will allow
us to directly learn these visual

00:07:49.279 --> 00:07:49.289
us to directly learn these visual
 

00:07:49.289 --> 00:07:52.190
us to directly learn these visual
features from visual data if we

00:07:52.190 --> 00:07:52.200
features from visual data if we
 

00:07:52.200 --> 00:07:56.690
features from visual data if we
construct them cleverly going back in

00:07:56.690 --> 00:07:56.700
construct them cleverly going back in
 

00:07:56.700 --> 00:07:59.330
construct them cleverly going back in
lecture 1 right we learned about fully

00:07:59.330 --> 00:07:59.340
lecture 1 right we learned about fully
 

00:07:59.340 --> 00:08:01.909
lecture 1 right we learned about fully
connected architectures where you can

00:08:01.909 --> 00:08:01.919
connected architectures where you can
 

00:08:01.919 --> 00:08:04.129
connected architectures where you can
have multiple hidden layers and where

00:08:04.129 --> 00:08:04.139
have multiple hidden layers and where
 

00:08:04.139 --> 00:08:06.140
have multiple hidden layers and where
each neuron in a given layer is

00:08:06.140 --> 00:08:06.150
each neuron in a given layer is
 

00:08:06.150 --> 00:08:08.330
each neuron in a given layer is
connected to every single neuron in the

00:08:08.330 --> 00:08:08.340
connected to every single neuron in the
 

00:08:08.340 --> 00:08:11.480
connected to every single neuron in the
subsequent layer and let's say that we

00:08:11.480 --> 00:08:11.490
subsequent layer and let's say that we
 

00:08:11.490 --> 00:08:13.610
subsequent layer and let's say that we
wanted to use a fully connected neural

00:08:13.610 --> 00:08:13.620
wanted to use a fully connected neural
 

00:08:13.620 --> 00:08:17.000
wanted to use a fully connected neural
network for image classification in this

00:08:17.000 --> 00:08:17.010
network for image classification in this
 

00:08:17.010 --> 00:08:19.909
network for image classification in this
case our 2d input image is transformed

00:08:19.909 --> 00:08:19.919
case our 2d input image is transformed
 

00:08:19.919 --> 00:08:22.550
case our 2d input image is transformed
into a vector of pixel values and this

00:08:22.550 --> 00:08:22.560
into a vector of pixel values and this
 

00:08:22.560 --> 00:08:24.680
into a vector of pixel values and this
vector is then fed into the network

00:08:24.680 --> 00:08:24.690
vector is then fed into the network
 

00:08:24.690 --> 00:08:27.200
vector is then fed into the network
where each neuron in the hidden in the

00:08:27.200 --> 00:08:27.210
where each neuron in the hidden in the
 

00:08:27.210 --> 00:08:29.360
where each neuron in the hidden in the
first hidden layer is connected to all

00:08:29.360 --> 00:08:29.370
first hidden layer is connected to all
 

00:08:29.370 --> 00:08:32.089
first hidden layer is connected to all
neurons in the input layer and here

00:08:32.089 --> 00:08:32.099
neurons in the input layer and here
 

00:08:32.099 --> 00:08:34.519
neurons in the input layer and here
hopefully you can appreciate that by

00:08:34.519 --> 00:08:34.529
hopefully you can appreciate that by
 

00:08:34.529 --> 00:08:38.149
hopefully you can appreciate that by
squashing our 2d our 2d matrix into this

00:08:38.149 --> 00:08:38.159
squashing our 2d our 2d matrix into this
 

00:08:38.159 --> 00:08:41.060
squashing our 2d our 2d matrix into this
1d vector and defining these fully

00:08:41.060 --> 00:08:41.070
1d vector and defining these fully
 

00:08:41.070 --> 00:08:44.990
1d vector and defining these fully
connected connections all spatial

00:08:44.990 --> 00:08:45.000
connected connections all spatial
 

00:08:45.000 --> 00:08:47.470
connected connections all spatial
information is completely lost

00:08:47.470 --> 00:08:47.480
information is completely lost
 

00:08:47.480 --> 00:08:50.120
information is completely lost
furthermore in in defining the network

00:08:50.120 --> 00:08:50.130
furthermore in in defining the network
 

00:08:50.130 --> 00:08:52.240
furthermore in in defining the network
in this way we end up having men

00:08:52.240 --> 00:08:52.250
in this way we end up having men
 

00:08:52.250 --> 00:08:54.940
in this way we end up having men
many different parameters right you need

00:08:54.940 --> 00:08:54.950
many different parameters right you need
 

00:08:54.950 --> 00:08:57.040
many different parameters right you need
a different weight parameter for every

00:08:57.040 --> 00:08:57.050
a different weight parameter for every
 

00:08:57.050 --> 00:08:59.380
a different weight parameter for every
single neural connection in your network

00:08:59.380 --> 00:08:59.390
single neural connection in your network
 

00:08:59.390 --> 00:09:02.140
single neural connection in your network
because it's fully connected and this

00:09:02.140 --> 00:09:02.150
because it's fully connected and this
 

00:09:02.150 --> 00:09:04.840
because it's fully connected and this
means that training in network like this

00:09:04.840 --> 00:09:04.850
means that training in network like this
 

00:09:04.850 --> 00:09:07.770
means that training in network like this
on a task like image classification

00:09:07.770 --> 00:09:07.780
on a task like image classification
 

00:09:07.780 --> 00:09:11.250
on a task like image classification
becomes infeasible in practice

00:09:11.250 --> 00:09:11.260
becomes infeasible in practice
 

00:09:11.260 --> 00:09:13.960
becomes infeasible in practice
importantly right visual data has this

00:09:13.960 --> 00:09:13.970
importantly right visual data has this
 

00:09:13.970 --> 00:09:16.900
importantly right visual data has this
really rich spatial structure how can we

00:09:16.900 --> 00:09:16.910
really rich spatial structure how can we
 

00:09:16.910 --> 00:09:19.390
really rich spatial structure how can we
leverage this to inform the architecture

00:09:19.390 --> 00:09:19.400
leverage this to inform the architecture
 

00:09:19.400 --> 00:09:23.410
leverage this to inform the architecture
of the network that we design to do this

00:09:23.410 --> 00:09:23.420
of the network that we design to do this
 

00:09:23.420 --> 00:09:26.590
of the network that we design to do this
let's represent our 2d input image as an

00:09:26.590 --> 00:09:26.600
let's represent our 2d input image as an
 

00:09:26.600 --> 00:09:28.480
let's represent our 2d input image as an
array of pixel values like I mentioned

00:09:28.480 --> 00:09:28.490
array of pixel values like I mentioned
 

00:09:28.490 --> 00:09:30.790
array of pixel values like I mentioned
before and one way we can immediately

00:09:30.790 --> 00:09:30.800
before and one way we can immediately
 

00:09:30.800 --> 00:09:32.830
before and one way we can immediately
use the spatial structure that's

00:09:32.830 --> 00:09:32.840
use the spatial structure that's
 

00:09:32.840 --> 00:09:35.580
use the spatial structure that's
inherent to this input is to connect

00:09:35.580 --> 00:09:35.590
inherent to this input is to connect
 

00:09:35.590 --> 00:09:38.500
inherent to this input is to connect
patches of the input to neurons in the

00:09:38.500 --> 00:09:38.510
patches of the input to neurons in the
 

00:09:38.510 --> 00:09:41.350
patches of the input to neurons in the
hidden layer another way of thinking

00:09:41.350 --> 00:09:41.360
hidden layer another way of thinking
 

00:09:41.360 --> 00:09:43.450
hidden layer another way of thinking
about this is that each neuron in a

00:09:43.450 --> 00:09:43.460
about this is that each neuron in a
 

00:09:43.460 --> 00:09:45.940
about this is that each neuron in a
hidden layer only sees a particular

00:09:45.940 --> 00:09:45.950
hidden layer only sees a particular
 

00:09:45.950 --> 00:09:48.640
hidden layer only sees a particular
region of what the input to that layer

00:09:48.640 --> 00:09:48.650
region of what the input to that layer
 

00:09:48.650 --> 00:09:51.880
region of what the input to that layer
is and this not only reduces the number

00:09:51.880 --> 00:09:51.890
is and this not only reduces the number
 

00:09:51.890 --> 00:09:55.150
is and this not only reduces the number
of weights in our model but also allows

00:09:55.150 --> 00:09:55.160
of weights in our model but also allows
 

00:09:55.160 --> 00:09:57.460
of weights in our model but also allows
us to leverage the fact that in an image

00:09:57.460 --> 00:09:57.470
us to leverage the fact that in an image
 

00:09:57.470 --> 00:09:59.620
us to leverage the fact that in an image
pixels that are spatially close to each

00:09:59.620 --> 00:09:59.630
pixels that are spatially close to each
 

00:09:59.630 --> 00:10:02.380
pixels that are spatially close to each
other are probably somehow related and

00:10:02.380 --> 00:10:02.390
other are probably somehow related and
 

00:10:02.390 --> 00:10:05.710
other are probably somehow related and
so I'd like you to really notice how the

00:10:05.710 --> 00:10:05.720
so I'd like you to really notice how the
 

00:10:05.720 --> 00:10:10.630
so I'd like you to really notice how the
only region how only a region of the

00:10:10.630 --> 00:10:10.640
only region how only a region of the
 

00:10:10.640 --> 00:10:13.150
only region how only a region of the
input layer influences this particular

00:10:13.150 --> 00:10:13.160
input layer influences this particular
 

00:10:13.160 --> 00:10:16.720
input layer influences this particular
neuron and we can define connections

00:10:16.720 --> 00:10:16.730
neuron and we can define connections
 

00:10:16.730 --> 00:10:19.750
neuron and we can define connections
across the whole input by applying the

00:10:19.750 --> 00:10:19.760
across the whole input by applying the
 

00:10:19.760 --> 00:10:21.820
across the whole input by applying the
same principle of connecting patches in

00:10:21.820 --> 00:10:21.830
same principle of connecting patches in
 

00:10:21.830 --> 00:10:23.800
same principle of connecting patches in
the input layer to neurons in the

00:10:23.800 --> 00:10:23.810
the input layer to neurons in the
 

00:10:23.810 --> 00:10:27.010
the input layer to neurons in the
subsequent subsequent layer and we do

00:10:27.010 --> 00:10:27.020
subsequent subsequent layer and we do
 

00:10:27.020 --> 00:10:28.900
subsequent subsequent layer and we do
this by actually sliding the patch

00:10:28.900 --> 00:10:28.910
this by actually sliding the patch
 

00:10:28.910 --> 00:10:32.170
this by actually sliding the patch
window across the input image in this

00:10:32.170 --> 00:10:32.180
window across the input image in this
 

00:10:32.180 --> 00:10:34.560
window across the input image in this
case we're sliding it by two units and

00:10:34.560 --> 00:10:34.570
case we're sliding it by two units and
 

00:10:34.570 --> 00:10:37.570
case we're sliding it by two units and
in doing this we take into account the

00:10:37.570 --> 00:10:37.580
in doing this we take into account the
 

00:10:37.580 --> 00:10:39.460
in doing this we take into account the
spatial structure that's inherent to the

00:10:39.460 --> 00:10:39.470
spatial structure that's inherent to the
 

00:10:39.470 --> 00:10:42.340
spatial structure that's inherent to the
input but remember right that our

00:10:42.340 --> 00:10:42.350
input but remember right that our
 

00:10:42.350 --> 00:10:45.190
input but remember right that our
ultimate task is to learn visual

00:10:45.190 --> 00:10:45.200
ultimate task is to learn visual
 

00:10:45.200 --> 00:10:48.520
ultimate task is to learn visual
features and the way we achieve this is

00:10:48.520 --> 00:10:48.530
features and the way we achieve this is
 

00:10:48.530 --> 00:10:50.560
features and the way we achieve this is
by weighting these connections between

00:10:50.560 --> 00:10:50.570
by weighting these connections between
 

00:10:50.570 --> 00:10:54.130
by weighting these connections between
the patch and the neuron and the neuron

00:10:54.130 --> 00:10:54.140
the patch and the neuron and the neuron
 

00:10:54.140 --> 00:10:56.410
the patch and the neuron and the neuron
in the next layer so as to detect

00:10:56.410 --> 00:10:56.420
in the next layer so as to detect
 

00:10:56.420 --> 00:11:02.829
in the next layer so as to detect
particular features so this this

00:11:02.829 --> 00:11:02.839
particular features so this this
 

00:11:02.839 --> 00:11:05.770
particular features so this this
principle is is called

00:11:05.770 --> 00:11:05.780
principle is is called
 

00:11:05.780 --> 00:11:08.110
principle is is called
we think of what you can think of it as

00:11:08.110 --> 00:11:08.120
we think of what you can think of it as
 

00:11:08.120 --> 00:11:11.530
we think of what you can think of it as
is applying a filter essentially a set

00:11:11.530 --> 00:11:11.540
is applying a filter essentially a set
 

00:11:11.540 --> 00:11:14.680
is applying a filter essentially a set
of weights to extract some sort of local

00:11:14.680 --> 00:11:14.690
of weights to extract some sort of local
 

00:11:14.690 --> 00:11:16.960
of weights to extract some sort of local
features that are present in your input

00:11:16.960 --> 00:11:16.970
features that are present in your input
 

00:11:16.970 --> 00:11:19.840
features that are present in your input
image and we can apply multiple

00:11:19.840 --> 00:11:19.850
image and we can apply multiple
 

00:11:19.850 --> 00:11:23.020
image and we can apply multiple
different filters to extract different

00:11:23.020 --> 00:11:23.030
different filters to extract different
 

00:11:23.030 --> 00:11:26.200
different filters to extract different
types of features and furthermore we can

00:11:26.200 --> 00:11:26.210
types of features and furthermore we can
 

00:11:26.210 --> 00:11:28.450
types of features and furthermore we can
spatially share the parameters of each

00:11:28.450 --> 00:11:28.460
spatially share the parameters of each
 

00:11:28.460 --> 00:11:31.210
spatially share the parameters of each
of these filters across the input so

00:11:31.210 --> 00:11:31.220
of these filters across the input so
 

00:11:31.220 --> 00:11:33.580
of these filters across the input so
that features that matter in one part of

00:11:33.580 --> 00:11:33.590
that features that matter in one part of
 

00:11:33.590 --> 00:11:36.130
that features that matter in one part of
the image will still matter elsewhere in

00:11:36.130 --> 00:11:36.140
the image will still matter elsewhere in
 

00:11:36.140 --> 00:11:40.570
the image will still matter elsewhere in
the image in practice this amounts to

00:11:40.570 --> 00:11:40.580
the image in practice this amounts to
 

00:11:40.580 --> 00:11:42.490
the image in practice this amounts to
this patchy operation that's called

00:11:42.490 --> 00:11:42.500
this patchy operation that's called
 

00:11:42.500 --> 00:11:45.460
this patchy operation that's called
convolution and if we first think about

00:11:45.460 --> 00:11:45.470
convolution and if we first think about
 

00:11:45.470 --> 00:11:48.250
convolution and if we first think about
this at a high level suppose we have a

00:11:48.250 --> 00:11:48.260
this at a high level suppose we have a
 

00:11:48.260 --> 00:11:51.040
this at a high level suppose we have a
four by four filter which means we have

00:11:51.040 --> 00:11:51.050
four by four filter which means we have
 

00:11:51.050 --> 00:11:54.220
four by four filter which means we have
16 different weights right and we're

00:11:54.220 --> 00:11:54.230
16 different weights right and we're
 

00:11:54.230 --> 00:11:56.830
16 different weights right and we're
going to apply this same filter to four

00:11:56.830 --> 00:11:56.840
going to apply this same filter to four
 

00:11:56.840 --> 00:12:00.310
going to apply this same filter to four
by four patches in the input and use the

00:12:00.310 --> 00:12:00.320
by four patches in the input and use the
 

00:12:00.320 --> 00:12:03.040
by four patches in the input and use the
result of that filter operation to

00:12:03.040 --> 00:12:03.050
result of that filter operation to
 

00:12:03.050 --> 00:12:05.620
result of that filter operation to
define the state of the neuron that the

00:12:05.620 --> 00:12:05.630
define the state of the neuron that the
 

00:12:05.630 --> 00:12:08.800
define the state of the neuron that the
patch is connected to right then we're

00:12:08.800 --> 00:12:08.810
patch is connected to right then we're
 

00:12:08.810 --> 00:12:10.960
patch is connected to right then we're
going to shift our filter over by a

00:12:10.960 --> 00:12:10.970
going to shift our filter over by a
 

00:12:10.970 --> 00:12:13.900
going to shift our filter over by a
certain width like two pixels grab the

00:12:13.900 --> 00:12:13.910
certain width like two pixels grab the
 

00:12:13.910 --> 00:12:16.420
certain width like two pixels grab the
next patch and apply that filtering

00:12:16.420 --> 00:12:16.430
next patch and apply that filtering
 

00:12:16.430 --> 00:12:19.060
next patch and apply that filtering
operation again and this is how we can

00:12:19.060 --> 00:12:19.070
operation again and this is how we can
 

00:12:19.070 --> 00:12:21.010
operation again and this is how we can
start to think about convolution at a

00:12:21.010 --> 00:12:21.020
start to think about convolution at a
 

00:12:21.020 --> 00:12:23.410
start to think about convolution at a
really high level but you're probably

00:12:23.410 --> 00:12:23.420
really high level but you're probably
 

00:12:23.420 --> 00:12:25.210
really high level but you're probably
wondering how does this actually work

00:12:25.210 --> 00:12:25.220
wondering how does this actually work
 

00:12:25.220 --> 00:12:27.490
wondering how does this actually work
what am I talking about when I keep

00:12:27.490 --> 00:12:27.500
what am I talking about when I keep
 

00:12:27.500 --> 00:12:29.829
what am I talking about when I keep
saying oh features extract visual

00:12:29.829 --> 00:12:29.839
saying oh features extract visual
 

00:12:29.839 --> 00:12:31.900
saying oh features extract visual
features how does this convolution

00:12:31.900 --> 00:12:31.910
features how does this convolution
 

00:12:31.910 --> 00:12:34.660
features how does this convolution
operation allow us to do this so let's

00:12:34.660 --> 00:12:34.670
operation allow us to do this so let's
 

00:12:34.670 --> 00:12:36.790
operation allow us to do this so let's
make this concrete by walking through a

00:12:36.790 --> 00:12:36.800
make this concrete by walking through a
 

00:12:36.800 --> 00:12:40.750
make this concrete by walking through a
couple of examples suppose we want to

00:12:40.750 --> 00:12:40.760
couple of examples suppose we want to
 

00:12:40.760 --> 00:12:43.750
couple of examples suppose we want to
classify X's from a set of black and

00:12:43.750 --> 00:12:43.760
classify X's from a set of black and
 

00:12:43.760 --> 00:12:46.210
classify X's from a set of black and
white images of letters where black is

00:12:46.210 --> 00:12:46.220
white images of letters where black is
 

00:12:46.220 --> 00:12:48.130
white images of letters where black is
equal to minus 1 and white is

00:12:48.130 --> 00:12:48.140
equal to minus 1 and white is
 

00:12:48.140 --> 00:12:51.430
equal to minus 1 and white is
represented by a value of 1 to classify

00:12:51.430 --> 00:12:51.440
represented by a value of 1 to classify
 

00:12:51.440 --> 00:12:53.800
represented by a value of 1 to classify
it's it's really not possible to simply

00:12:53.800 --> 00:12:53.810
it's it's really not possible to simply
 

00:12:53.810 --> 00:12:56.050
it's it's really not possible to simply
compare the two matrices to see if

00:12:56.050 --> 00:12:56.060
compare the two matrices to see if
 

00:12:56.060 --> 00:12:58.180
compare the two matrices to see if
they're equal because we want to be able

00:12:58.180 --> 00:12:58.190
they're equal because we want to be able
 

00:12:58.190 --> 00:13:01.920
they're equal because we want to be able
to classify and act as an X even if it's

00:13:01.920 --> 00:13:01.930
to classify and act as an X even if it's
 

00:13:01.930 --> 00:13:05.410
to classify and act as an X even if it's
transformed rotated reflected deformed

00:13:05.410 --> 00:13:05.420
transformed rotated reflected deformed
 

00:13:05.420 --> 00:13:10.060
transformed rotated reflected deformed
etc instead we want our model to compare

00:13:10.060 --> 00:13:10.070
etc instead we want our model to compare
 

00:13:10.070 --> 00:13:13.260
etc instead we want our model to compare
the images of an X piece by piece and

00:13:13.260 --> 00:13:13.270
the images of an X piece by piece and
 

00:13:13.270 --> 00:13:16.329
the images of an X piece by piece and
those important pieces that it learns to

00:13:16.329 --> 00:13:16.339
those important pieces that it learns to
 

00:13:16.339 --> 00:13:18.870
those important pieces that it learns to
look for are the features and

00:13:18.870 --> 00:13:18.880
look for are the features and
 

00:13:18.880 --> 00:13:21.810
look for are the features and
our model can get rough feature matches

00:13:21.810 --> 00:13:21.820
our model can get rough feature matches
 

00:13:21.820 --> 00:13:24.450
our model can get rough feature matches
in roughly the same positions relatively

00:13:24.450 --> 00:13:24.460
in roughly the same positions relatively
 

00:13:24.460 --> 00:13:26.850
in roughly the same positions relatively
speaking in two different images it can

00:13:26.850 --> 00:13:26.860
speaking in two different images it can
 

00:13:26.860 --> 00:13:30.150
speaking in two different images it can
get a lot better sense at seeing the

00:13:30.150 --> 00:13:30.160
get a lot better sense at seeing the
 

00:13:30.160 --> 00:13:32.370
get a lot better sense at seeing the
similarity between different examples of

00:13:32.370 --> 00:13:32.380
similarity between different examples of
 

00:13:32.380 --> 00:13:36.660
similarity between different examples of
exes so each feature is like a mini

00:13:36.660 --> 00:13:36.670
exes so each feature is like a mini
 

00:13:36.670 --> 00:13:37.680
exes so each feature is like a mini
image right

00:13:37.680 --> 00:13:37.690
image right
 

00:13:37.690 --> 00:13:40.320
image right
a small two-dimensional array of values

00:13:40.320 --> 00:13:40.330
a small two-dimensional array of values
 

00:13:40.330 --> 00:13:43.620
a small two-dimensional array of values
and we can use these filters to pick up

00:13:43.620 --> 00:13:43.630
and we can use these filters to pick up
 

00:13:43.630 --> 00:13:46.260
and we can use these filters to pick up
on the features that are common to exes

00:13:46.260 --> 00:13:46.270
on the features that are common to exes
 

00:13:46.270 --> 00:13:49.320
on the features that are common to exes
so in the case of exes right filters

00:13:49.320 --> 00:13:49.330
so in the case of exes right filters
 

00:13:49.330 --> 00:13:52.110
so in the case of exes right filters
that can pick up on diagonal lines and a

00:13:52.110 --> 00:13:52.120
that can pick up on diagonal lines and a
 

00:13:52.120 --> 00:13:54.690
that can pick up on diagonal lines and a
crossing capture what's important about

00:13:54.690 --> 00:13:54.700
crossing capture what's important about
 

00:13:54.700 --> 00:13:58.830
crossing capture what's important about
an X so we can probably capture these

00:13:58.830 --> 00:13:58.840
an X so we can probably capture these
 

00:13:58.840 --> 00:14:01.410
an X so we can probably capture these
features in the arms and center of any

00:14:01.410 --> 00:14:01.420
features in the arms and center of any
 

00:14:01.420 --> 00:14:04.800
features in the arms and center of any
image of an X and I'd like you to notice

00:14:04.800 --> 00:14:04.810
image of an X and I'd like you to notice
 

00:14:04.810 --> 00:14:07.590
image of an X and I'd like you to notice
that these smaller matrices are the

00:14:07.590 --> 00:14:07.600
that these smaller matrices are the
 

00:14:07.600 --> 00:14:09.660
that these smaller matrices are the
filters of weights that we'll actually

00:14:09.660 --> 00:14:09.670
filters of weights that we'll actually
 

00:14:09.670 --> 00:14:12.330
filters of weights that we'll actually
use to detect the corresponding features

00:14:12.330 --> 00:14:12.340
use to detect the corresponding features
 

00:14:12.340 --> 00:14:15.120
use to detect the corresponding features
in the input image now all that's left

00:14:15.120 --> 00:14:15.130
in the input image now all that's left
 

00:14:15.130 --> 00:14:18.240
in the input image now all that's left
is to define an operation that picks up

00:14:18.240 --> 00:14:18.250
is to define an operation that picks up
 

00:14:18.250 --> 00:14:20.880
is to define an operation that picks up
where these features pop up in our image

00:14:20.880 --> 00:14:20.890
where these features pop up in our image
 

00:14:20.890 --> 00:14:25.040
where these features pop up in our image
and that operation is convolution and

00:14:25.040 --> 00:14:25.050
and that operation is convolution and
 

00:14:25.050 --> 00:14:27.900
and that operation is convolution and
convolution is able to preserve the

00:14:27.900 --> 00:14:27.910
convolution is able to preserve the
 

00:14:27.910 --> 00:14:30.630
convolution is able to preserve the
spatial relationship between pixels by

00:14:30.630 --> 00:14:30.640
spatial relationship between pixels by
 

00:14:30.640 --> 00:14:33.030
spatial relationship between pixels by
learning image features in small squares

00:14:33.030 --> 00:14:33.040
learning image features in small squares
 

00:14:33.040 --> 00:14:36.600
learning image features in small squares
of the input and to do this what we do

00:14:36.600 --> 00:14:36.610
of the input and to do this what we do
 

00:14:36.610 --> 00:14:38.880
of the input and to do this what we do
is we simply perform an element-wise

00:14:38.880 --> 00:14:38.890
is we simply perform an element-wise
 

00:14:38.890 --> 00:14:42.300
is we simply perform an element-wise
multiplication between the filter weight

00:14:42.300 --> 00:14:42.310
multiplication between the filter weight
 

00:14:42.310 --> 00:14:44.880
multiplication between the filter weight
matrix and the patch of the input image

00:14:44.880 --> 00:14:44.890
matrix and the patch of the input image
 

00:14:44.890 --> 00:14:48.630
matrix and the patch of the input image
of the same dimensions and this results

00:14:48.630 --> 00:14:48.640
of the same dimensions and this results
 

00:14:48.640 --> 00:14:51.540
of the same dimensions and this results
in this case in a three by three matrix

00:14:51.540 --> 00:14:51.550
in this case in a three by three matrix
 

00:14:51.550 --> 00:14:55.530
in this case in a three by three matrix
and here in this example all the entries

00:14:55.530 --> 00:14:55.540
and here in this example all the entries
 

00:14:55.540 --> 00:14:58.620
and here in this example all the entries
in this matrix are 1 and that's because

00:14:58.620 --> 00:14:58.630
in this matrix are 1 and that's because
 

00:14:58.630 --> 00:15:00.360
in this matrix are 1 and that's because
everything is black and white either

00:15:00.360 --> 00:15:00.370
everything is black and white either
 

00:15:00.370 --> 00:15:04.170
everything is black and white either
minus 1 and 1 and this indicates that

00:15:04.170 --> 00:15:04.180
minus 1 and 1 and this indicates that
 

00:15:04.180 --> 00:15:06.240
minus 1 and 1 and this indicates that
there is a perfect correspondence

00:15:06.240 --> 00:15:06.250
there is a perfect correspondence
 

00:15:06.250 --> 00:15:09.420
there is a perfect correspondence
between our filter matrix and the patch

00:15:09.420 --> 00:15:09.430
between our filter matrix and the patch
 

00:15:09.430 --> 00:15:11.730
between our filter matrix and the patch
of the input image where we multiplied

00:15:11.730 --> 00:15:11.740
of the input image where we multiplied
 

00:15:11.740 --> 00:15:14.130
of the input image where we multiplied
it right so this is our filter this is

00:15:14.130 --> 00:15:14.140
it right so this is our filter this is
 

00:15:14.140 --> 00:15:16.950
it right so this is our filter this is
our patch they directly correspond and

00:15:16.950 --> 00:15:16.960
our patch they directly correspond and
 

00:15:16.960 --> 00:15:22.080
our patch they directly correspond and
the result is is as follows finally if

00:15:22.080 --> 00:15:22.090
the result is is as follows finally if
 

00:15:22.090 --> 00:15:24.360
the result is is as follows finally if
we add all the elements of this of this

00:15:24.360 --> 00:15:24.370
we add all the elements of this of this
 

00:15:24.370 --> 00:15:26.970
we add all the elements of this of this
matrix this is the result of convolving

00:15:26.970 --> 00:15:26.980
matrix this is the result of convolving
 

00:15:26.980 --> 00:15:30.180
matrix this is the result of convolving
this 3x3 filter with that particular

00:15:30.180 --> 00:15:30.190
this 3x3 filter with that particular
 

00:15:30.190 --> 00:15:31.310
this 3x3 filter with that particular
region of the input

00:15:31.310 --> 00:15:31.320
region of the input
 

00:15:31.320 --> 00:15:35.330
region of the input
and we get back to number nine so let's

00:15:35.330 --> 00:15:35.340
and we get back to number nine so let's
 

00:15:35.340 --> 00:15:37.460
and we get back to number nine so let's
consider another example right right to

00:15:37.460 --> 00:15:37.470
consider another example right right to
 

00:15:37.470 --> 00:15:39.700
consider another example right right to
hopefully drive this home even further

00:15:39.700 --> 00:15:39.710
hopefully drive this home even further
 

00:15:39.710 --> 00:15:43.250
hopefully drive this home even further
suppose we want to compute the

00:15:43.250 --> 00:15:43.260
suppose we want to compute the
 

00:15:43.260 --> 00:15:46.490
suppose we want to compute the
convolution of this 5x5 representation

00:15:46.490 --> 00:15:46.500
convolution of this 5x5 representation
 

00:15:46.500 --> 00:15:50.120
convolution of this 5x5 representation
of an image and this 3x3 filter to do

00:15:50.120 --> 00:15:50.130
of an image and this 3x3 filter to do
 

00:15:50.130 --> 00:15:51.920
of an image and this 3x3 filter to do
this we need to cover the entirety of

00:15:51.920 --> 00:15:51.930
this we need to cover the entirety of
 

00:15:51.930 --> 00:15:54.590
this we need to cover the entirety of
the input image by sliding this filter

00:15:54.590 --> 00:15:54.600
the input image by sliding this filter
 

00:15:54.600 --> 00:15:58.070
the input image by sliding this filter
over over the over the image performing

00:15:58.070 --> 00:15:58.080
over over the over the image performing
 

00:15:58.080 --> 00:16:01.340
over over the over the image performing
this element wise multiplication at each

00:16:01.340 --> 00:16:01.350
this element wise multiplication at each
 

00:16:01.350 --> 00:16:04.070
this element wise multiplication at each
step and adding the outputs that result

00:16:04.070 --> 00:16:04.080
step and adding the outputs that result
 

00:16:04.080 --> 00:16:06.700
step and adding the outputs that result
after each element wise multiplication

00:16:06.700 --> 00:16:06.710
after each element wise multiplication
 

00:16:06.710 --> 00:16:10.490
after each element wise multiplication
so let's see what this looks like first

00:16:10.490 --> 00:16:10.500
so let's see what this looks like first
 

00:16:10.500 --> 00:16:12.350
so let's see what this looks like first
we start off in this upper left corner

00:16:12.350 --> 00:16:12.360
we start off in this upper left corner
 

00:16:12.360 --> 00:16:16.310
we start off in this upper left corner
we multiply this filter by the values of

00:16:16.310 --> 00:16:16.320
we multiply this filter by the values of
 

00:16:16.320 --> 00:16:20.330
we multiply this filter by the values of
our of our input image and add the

00:16:20.330 --> 00:16:20.340
our of our input image and add the
 

00:16:20.340 --> 00:16:24.860
our of our input image and add the
result and we end up with the value of 4

00:16:24.860 --> 00:16:24.870
result and we end up with the value of 4
 

00:16:24.870 --> 00:16:27.620
result and we end up with the value of 4
this results in the first entry in our

00:16:27.620 --> 00:16:27.630
this results in the first entry in our
 

00:16:27.630 --> 00:16:29.960
this results in the first entry in our
output matrix which we can call the

00:16:29.960 --> 00:16:29.970
output matrix which we can call the
 

00:16:29.970 --> 00:16:32.900
output matrix which we can call the
feature map we next slide the 3 by 3

00:16:32.900 --> 00:16:32.910
feature map we next slide the 3 by 3
 

00:16:32.910 --> 00:16:36.140
feature map we next slide the 3 by 3
filter over by 1 to grab the next patch

00:16:36.140 --> 00:16:36.150
filter over by 1 to grab the next patch
 

00:16:36.150 --> 00:16:38.090
filter over by 1 to grab the next patch
and repeat this element wise

00:16:38.090 --> 00:16:38.100
and repeat this element wise
 

00:16:38.100 --> 00:16:40.940
and repeat this element wise
multiplication in addition this gives us

00:16:40.940 --> 00:16:40.950
multiplication in addition this gives us
 

00:16:40.950 --> 00:16:44.090
multiplication in addition this gives us
our second entry 3 we continue this

00:16:44.090 --> 00:16:44.100
our second entry 3 we continue this
 

00:16:44.100 --> 00:16:47.240
our second entry 3 we continue this
process until we have covered the

00:16:47.240 --> 00:16:47.250
process until we have covered the
 

00:16:47.250 --> 00:16:49.420
process until we have covered the
entirety of this input image

00:16:49.420 --> 00:16:49.430
entirety of this input image
 

00:16:49.430 --> 00:16:52.520
entirety of this input image
progressively sliding our filter to

00:16:52.520 --> 00:16:52.530
progressively sliding our filter to
 

00:16:52.530 --> 00:16:54.440
progressively sliding our filter to
cover it doing this element wise

00:16:54.440 --> 00:16:54.450
cover it doing this element wise
 

00:16:54.450 --> 00:16:58.040
cover it doing this element wise
multiplication patch by patch adding the

00:16:58.040 --> 00:16:58.050
multiplication patch by patch adding the
 

00:16:58.050 --> 00:17:01.640
multiplication patch by patch adding the
result and filling out our feature map

00:17:01.640 --> 00:17:01.650
result and filling out our feature map
 

00:17:01.650 --> 00:17:03.200
result and filling out our feature map
and that's it

00:17:03.200 --> 00:17:03.210
and that's it
 

00:17:03.210 --> 00:17:06.319
and that's it
that's convolution and this feature map

00:17:06.319 --> 00:17:06.329
that's convolution and this feature map
 

00:17:06.329 --> 00:17:08.329
that's convolution and this feature map
right this is a toy example but in

00:17:08.329 --> 00:17:08.339
right this is a toy example but in
 

00:17:08.339 --> 00:17:09.920
right this is a toy example but in
practice you can imagine that this

00:17:09.920 --> 00:17:09.930
practice you can imagine that this
 

00:17:09.930 --> 00:17:13.340
practice you can imagine that this
feature map reflects where in the input

00:17:13.340 --> 00:17:13.350
feature map reflects where in the input
 

00:17:13.350 --> 00:17:16.189
feature map reflects where in the input
image was activated by this filter where

00:17:16.189 --> 00:17:16.199
image was activated by this filter where
 

00:17:16.199 --> 00:17:19.250
image was activated by this filter where
in the input image that filter picked up

00:17:19.250 --> 00:17:19.260
in the input image that filter picked up
 

00:17:19.260 --> 00:17:21.350
in the input image that filter picked up
on right because higher values are going

00:17:21.350 --> 00:17:21.360
on right because higher values are going
 

00:17:21.360 --> 00:17:23.329
on right because higher values are going
to represent like sort of a greater

00:17:23.329 --> 00:17:23.339
to represent like sort of a greater
 

00:17:23.339 --> 00:17:26.390
to represent like sort of a greater
activation if you will and so this is

00:17:26.390 --> 00:17:26.400
activation if you will and so this is
 

00:17:26.400 --> 00:17:28.910
activation if you will and so this is
really the the bare-bones mechanism of

00:17:28.910 --> 00:17:28.920
really the the bare-bones mechanism of
 

00:17:28.920 --> 00:17:32.900
really the the bare-bones mechanism of
this convolution operation and to

00:17:32.900 --> 00:17:32.910
this convolution operation and to
 

00:17:32.910 --> 00:17:35.440
this convolution operation and to
consider really how powerful this is

00:17:35.440 --> 00:17:35.450
consider really how powerful this is
 

00:17:35.450 --> 00:17:39.290
consider really how powerful this is
different different different weight

00:17:39.290 --> 00:17:39.300
different different different weight
 

00:17:39.300 --> 00:17:42.020
different different different weight
filters can be used to produce distinct

00:17:42.020 --> 00:17:42.030
filters can be used to produce distinct
 

00:17:42.030 --> 00:17:45.110
filters can be used to produce distinct
feature Maps so this is a very fit

00:17:45.110 --> 00:17:45.120
feature Maps so this is a very fit
 

00:17:45.120 --> 00:17:47.480
feature Maps so this is a very fit
famous picture of this woman who's

00:17:47.480 --> 00:17:47.490
famous picture of this woman who's
 

00:17:47.490 --> 00:17:51.260
famous picture of this woman who's
called Lenna and as you can see here in

00:17:51.260 --> 00:17:51.270
called Lenna and as you can see here in
 

00:17:51.270 --> 00:17:53.630
called Lenna and as you can see here in
these three examples we've taken three

00:17:53.630 --> 00:17:53.640
these three examples we've taken three
 

00:17:53.640 --> 00:17:56.900
these three examples we've taken three
different filters applied this filter to

00:17:56.900 --> 00:17:56.910
different filters applied this filter to
 

00:17:56.910 --> 00:18:00.920
different filters applied this filter to
the same input image and generated three

00:18:00.920 --> 00:18:00.930
the same input image and generated three
 

00:18:00.930 --> 00:18:04.970
the same input image and generated three
very different outputs and as you can

00:18:04.970 --> 00:18:04.980
very different outputs and as you can
 

00:18:04.980 --> 00:18:06.890
very different outputs and as you can
see by simply changing the weights of

00:18:06.890 --> 00:18:06.900
see by simply changing the weights of
 

00:18:06.900 --> 00:18:09.440
see by simply changing the weights of
the filters we can detect we can detect

00:18:09.440 --> 00:18:09.450
the filters we can detect we can detect
 

00:18:09.450 --> 00:18:12.170
the filters we can detect we can detect
and extract different features like

00:18:12.170 --> 00:18:12.180
and extract different features like
 

00:18:12.180 --> 00:18:14.510
and extract different features like
edges that are present in the input

00:18:14.510 --> 00:18:14.520
edges that are present in the input
 

00:18:14.520 --> 00:18:19.610
edges that are present in the input
image this is really really powerful so

00:18:19.610 --> 00:18:19.620
image this is really really powerful so
 

00:18:19.620 --> 00:18:21.770
image this is really really powerful so
hopefully you can now appreciate how

00:18:21.770 --> 00:18:21.780
hopefully you can now appreciate how
 

00:18:21.780 --> 00:18:24.440
hopefully you can now appreciate how
convolution allows us to capitalize on

00:18:24.440 --> 00:18:24.450
convolution allows us to capitalize on
 

00:18:24.450 --> 00:18:27.799
convolution allows us to capitalize on
the spatial structure that's inherent in

00:18:27.799 --> 00:18:27.809
the spatial structure that's inherent in
 

00:18:27.809 --> 00:18:30.080
the spatial structure that's inherent in
visual data and use these sets of

00:18:30.080 --> 00:18:30.090
visual data and use these sets of
 

00:18:30.090 --> 00:18:33.350
visual data and use these sets of
weights to extract local features and we

00:18:33.350 --> 00:18:33.360
weights to extract local features and we
 

00:18:33.360 --> 00:18:35.090
weights to extract local features and we
can very easily detect different

00:18:35.090 --> 00:18:35.100
can very easily detect different
 

00:18:35.100 --> 00:18:37.490
can very easily detect different
features simply by applying different

00:18:37.490 --> 00:18:37.500
features simply by applying different
 

00:18:37.500 --> 00:18:41.180
features simply by applying different
filters and these concepts of preserving

00:18:41.180 --> 00:18:41.190
filters and these concepts of preserving
 

00:18:41.190 --> 00:18:43.010
filters and these concepts of preserving
spatial structure and local feature

00:18:43.010 --> 00:18:43.020
spatial structure and local feature
 

00:18:43.020 --> 00:18:45.770
spatial structure and local feature
extraction using these convolutions are

00:18:45.770 --> 00:18:45.780
extraction using these convolutions are
 

00:18:45.780 --> 00:18:48.860
extraction using these convolutions are
at the core of the neural networks used

00:18:48.860 --> 00:18:48.870
at the core of the neural networks used
 

00:18:48.870 --> 00:18:52.040
at the core of the neural networks used
for computer vision tasks which are

00:18:52.040 --> 00:18:52.050
for computer vision tasks which are
 

00:18:52.050 --> 00:18:54.400
for computer vision tasks which are
called convolutional neural networks or

00:18:54.400 --> 00:18:54.410
called convolutional neural networks or
 

00:18:54.410 --> 00:18:59.260
called convolutional neural networks or
CNN's so sort of with these bare bones

00:18:59.260 --> 00:18:59.270
CNN's so sort of with these bare bones
 

00:18:59.270 --> 00:19:02.120
CNN's so sort of with these bare bones
mechanism under our belt we can think

00:19:02.120 --> 00:19:02.130
mechanism under our belt we can think
 

00:19:02.130 --> 00:19:04.400
mechanism under our belt we can think
about how we can utilize this to build

00:19:04.400 --> 00:19:04.410
about how we can utilize this to build
 

00:19:04.410 --> 00:19:06.440
about how we can utilize this to build
neural networks for computer vision

00:19:06.440 --> 00:19:06.450
neural networks for computer vision
 

00:19:06.450 --> 00:19:09.610
neural networks for computer vision
tasks so let's first consider a CNN

00:19:09.610 --> 00:19:09.620
tasks so let's first consider a CNN
 

00:19:09.620 --> 00:19:13.180
tasks so let's first consider a CNN
designed for image classification

00:19:13.180 --> 00:19:13.190
designed for image classification
 

00:19:13.190 --> 00:19:15.470
designed for image classification
remember the goal here is to learn

00:19:15.470 --> 00:19:15.480
remember the goal here is to learn
 

00:19:15.480 --> 00:19:18.020
remember the goal here is to learn
features directly from the image data

00:19:18.020 --> 00:19:18.030
features directly from the image data
 

00:19:18.030 --> 00:19:20.750
features directly from the image data
this means learning the weights of those

00:19:20.750 --> 00:19:20.760
this means learning the weights of those
 

00:19:20.760 --> 00:19:23.540
this means learning the weights of those
filters and using these learned feature

00:19:23.540 --> 00:19:23.550
filters and using these learned feature
 

00:19:23.550 --> 00:19:26.260
filters and using these learned feature
maps for classification of these images

00:19:26.260 --> 00:19:26.270
maps for classification of these images
 

00:19:26.270 --> 00:19:28.940
maps for classification of these images
now there are three main operations to a

00:19:28.940 --> 00:19:28.950
now there are three main operations to a
 

00:19:28.950 --> 00:19:32.299
now there are three main operations to a
CNN the first is convolution which we

00:19:32.299 --> 00:19:32.309
CNN the first is convolution which we
 

00:19:32.309 --> 00:19:35.510
CNN the first is convolution which we
went through right and we saw how we can

00:19:35.510 --> 00:19:35.520
went through right and we saw how we can
 

00:19:35.520 --> 00:19:37.370
went through right and we saw how we can
apply these filters to generate future

00:19:37.370 --> 00:19:37.380
apply these filters to generate future
 

00:19:37.380 --> 00:19:39.530
apply these filters to generate future
maps the second is applying

00:19:39.530 --> 00:19:39.540
maps the second is applying
 

00:19:39.540 --> 00:19:41.780
maps the second is applying
non-linearity the same concept from

00:19:41.780 --> 00:19:41.790
non-linearity the same concept from
 

00:19:41.790 --> 00:19:44.660
non-linearity the same concept from
lecture one and the third key idea is

00:19:44.660 --> 00:19:44.670
lecture one and the third key idea is
 

00:19:44.670 --> 00:19:47.000
lecture one and the third key idea is
pooling which is effectively like a down

00:19:47.000 --> 00:19:47.010
pooling which is effectively like a down
 

00:19:47.010 --> 00:19:50.120
pooling which is effectively like a down
sampling operation to reduce the size of

00:19:50.120 --> 00:19:50.130
sampling operation to reduce the size of
 

00:19:50.130 --> 00:19:55.330
sampling operation to reduce the size of
of a map and finally the computation of

00:19:55.330 --> 00:19:55.340
of a map and finally the computation of
 

00:19:55.340 --> 00:19:57.280
of a map and finally the computation of
class scores and actually outputting a

00:19:57.280 --> 00:19:57.290
class scores and actually outputting a
 

00:19:57.290 --> 00:19:59.470
class scores and actually outputting a
prediction for the class of an image is

00:19:59.470 --> 00:19:59.480
prediction for the class of an image is
 

00:19:59.480 --> 00:20:02.260
prediction for the class of an image is
achieved by a fully connected layer at

00:20:02.260 --> 00:20:02.270
achieved by a fully connected layer at
 

00:20:02.270 --> 00:20:04.720
achieved by a fully connected layer at
the end of our network and so in

00:20:04.720 --> 00:20:04.730
the end of our network and so in
 

00:20:04.730 --> 00:20:07.180
the end of our network and so in
training we train our model on a set of

00:20:07.180 --> 00:20:07.190
training we train our model on a set of
 

00:20:07.190 --> 00:20:09.880
training we train our model on a set of
images and we actually learn those

00:20:09.880 --> 00:20:09.890
images and we actually learn those
 

00:20:09.890 --> 00:20:12.040
images and we actually learn those
weights of the filters that are going to

00:20:12.040 --> 00:20:12.050
weights of the filters that are going to
 

00:20:12.050 --> 00:20:14.860
weights of the filters that are going to
be used in the network as well as the

00:20:14.860 --> 00:20:14.870
be used in the network as well as the
 

00:20:14.870 --> 00:20:17.830
be used in the network as well as the
weights in the fully connected layer and

00:20:17.830 --> 00:20:17.840
weights in the fully connected layer and
 

00:20:17.840 --> 00:20:20.140
weights in the fully connected layer and
we'll go through each of these to break

00:20:20.140 --> 00:20:20.150
we'll go through each of these to break
 

00:20:20.150 --> 00:20:23.410
we'll go through each of these to break
down this basic architecture of CNN

00:20:23.410 --> 00:20:23.420
down this basic architecture of CNN
 

00:20:23.420 --> 00:20:25.660
down this basic architecture of CNN
so first right as we've already seen

00:20:25.660 --> 00:20:25.670
so first right as we've already seen
 

00:20:25.670 --> 00:20:27.790
so first right as we've already seen
let's consider the convolution operation

00:20:27.790 --> 00:20:27.800
let's consider the convolution operation
 

00:20:27.800 --> 00:20:31.570
let's consider the convolution operation
as before each neuron in a hidden layer

00:20:31.570 --> 00:20:31.580
as before each neuron in a hidden layer
 

00:20:31.580 --> 00:20:33.610
as before each neuron in a hidden layer
will compute a weighted sum of its

00:20:33.610 --> 00:20:33.620
will compute a weighted sum of its
 

00:20:33.620 --> 00:20:36.490
will compute a weighted sum of its
inputs apply a bias and activate with a

00:20:36.490 --> 00:20:36.500
inputs apply a bias and activate with a
 

00:20:36.500 --> 00:20:38.230
inputs apply a bias and activate with a
non-linearity that's the same exact

00:20:38.230 --> 00:20:38.240
non-linearity that's the same exact
 

00:20:38.240 --> 00:20:40.690
non-linearity that's the same exact
concept from lecture one but what's

00:20:40.690 --> 00:20:40.700
concept from lecture one but what's
 

00:20:40.700 --> 00:20:42.700
concept from lecture one but what's
special here is this local connectivity

00:20:42.700 --> 00:20:42.710
special here is this local connectivity
 

00:20:42.710 --> 00:20:45.610
special here is this local connectivity
the fact that each neuron in a hidden

00:20:45.610 --> 00:20:45.620
the fact that each neuron in a hidden
 

00:20:45.620 --> 00:20:48.220
the fact that each neuron in a hidden
layer is only seeing a patch of what

00:20:48.220 --> 00:20:48.230
layer is only seeing a patch of what
 

00:20:48.230 --> 00:20:51.880
layer is only seeing a patch of what
comes before it and so this relation

00:20:51.880 --> 00:20:51.890
comes before it and so this relation
 

00:20:51.890 --> 00:20:54.610
comes before it and so this relation
defines how neurons and convolutional

00:20:54.610 --> 00:20:54.620
defines how neurons and convolutional
 

00:20:54.620 --> 00:20:59.080
defines how neurons and convolutional
layers are connected what it boils down

00:20:59.080 --> 00:20:59.090
layers are connected what it boils down
 

00:20:59.090 --> 00:21:01.690
layers are connected what it boils down
to is the same idea of applying a window

00:21:01.690 --> 00:21:01.700
to is the same idea of applying a window
 

00:21:01.700 --> 00:21:03.760
to is the same idea of applying a window
of weights computing the linear

00:21:03.760 --> 00:21:03.770
of weights computing the linear
 

00:21:03.770 --> 00:21:06.610
of weights computing the linear
combination of those weights against the

00:21:06.610 --> 00:21:06.620
combination of those weights against the
 

00:21:06.620 --> 00:21:09.670
combination of those weights against the
input and then activating with non with

00:21:09.670 --> 00:21:09.680
input and then activating with non with
 

00:21:09.680 --> 00:21:11.500
input and then activating with non with
a nonlinear activation function after

00:21:11.500 --> 00:21:11.510
a nonlinear activation function after
 

00:21:11.510 --> 00:21:15.940
a nonlinear activation function after
applying a bias another thing we can

00:21:15.940 --> 00:21:15.950
applying a bias another thing we can
 

00:21:15.950 --> 00:21:18.580
applying a bias another thing we can
think about is the fact that within a

00:21:18.580 --> 00:21:18.590
think about is the fact that within a
 

00:21:18.590 --> 00:21:20.830
think about is the fact that within a
single convolutional layer we can

00:21:20.830 --> 00:21:20.840
single convolutional layer we can
 

00:21:20.840 --> 00:21:22.930
single convolutional layer we can
actually have many different filters

00:21:22.930 --> 00:21:22.940
actually have many different filters
 

00:21:22.940 --> 00:21:24.940
actually have many different filters
that we are learning different sets of

00:21:24.940 --> 00:21:24.950
that we are learning different sets of
 

00:21:24.950 --> 00:21:27.790
that we are learning different sets of
weights to be able to extract different

00:21:27.790 --> 00:21:27.800
weights to be able to extract different
 

00:21:27.800 --> 00:21:31.150
weights to be able to extract different
features and so the output layer after a

00:21:31.150 --> 00:21:31.160
features and so the output layer after a
 

00:21:31.160 --> 00:21:33.550
features and so the output layer after a
convolution operation will have a volume

00:21:33.550 --> 00:21:33.560
convolution operation will have a volume
 

00:21:33.560 --> 00:21:38.320
convolution operation will have a volume
where the height and the width are the

00:21:38.320 --> 00:21:38.330
where the height and the width are the
 

00:21:38.330 --> 00:21:41.800
where the height and the width are the
spatial dimensions dependent on the

00:21:41.800 --> 00:21:41.810
spatial dimensions dependent on the
 

00:21:41.810 --> 00:21:45.810
spatial dimensions dependent on the
input layer so if we had say a 40 by 40

00:21:45.810 --> 00:21:45.820
input layer so if we had say a 40 by 40
 

00:21:45.820 --> 00:21:48.880
input layer so if we had say a 40 by 40
input image the width and height would

00:21:48.880 --> 00:21:48.890
input image the width and height would
 

00:21:48.890 --> 00:21:52.950
input image the width and height would
be 40 by 40 assuming that you know the

00:21:52.950 --> 00:21:52.960
be 40 by 40 assuming that you know the
 

00:21:52.960 --> 00:21:55.660
be 40 by 40 assuming that you know the
dimensionality scales after the

00:21:55.660 --> 00:21:55.670
dimensionality scales after the
 

00:21:55.670 --> 00:22:01.230
dimensionality scales after the
operation and and these dimensions are

00:22:01.230 --> 00:22:01.240
operation and and these dimensions are
 

00:22:01.240 --> 00:22:04.810
operation and and these dimensions are
dependent on the size of the filter as

00:22:04.810 --> 00:22:04.820
dependent on the size of the filter as
 

00:22:04.820 --> 00:22:06.160
dependent on the size of the filter as
well as the

00:22:06.160 --> 00:22:06.170
well as the
 

00:22:06.170 --> 00:22:08.920
well as the
degree to which we're sliding it over

00:22:08.920 --> 00:22:08.930
degree to which we're sliding it over
 

00:22:08.930 --> 00:22:12.820
degree to which we're sliding it over
the over the input layer finally the

00:22:12.820 --> 00:22:12.830
the over the input layer finally the
 

00:22:12.830 --> 00:22:15.310
the over the input layer finally the
depth is defined by the number of

00:22:15.310 --> 00:22:15.320
depth is defined by the number of
 

00:22:15.320 --> 00:22:19.320
depth is defined by the number of
different filters that we we are using

00:22:19.320 --> 00:22:19.330
different filters that we we are using
 

00:22:19.330 --> 00:22:22.420
different filters that we we are using
the last key thing that I would like to

00:22:22.420 --> 00:22:22.430
the last key thing that I would like to
 

00:22:22.430 --> 00:22:24.730
the last key thing that I would like to
like for you to keep in mind is this

00:22:24.730 --> 00:22:24.740
like for you to keep in mind is this
 

00:22:24.740 --> 00:22:27.850
like for you to keep in mind is this
notion of the receptive field which is

00:22:27.850 --> 00:22:27.860
notion of the receptive field which is
 

00:22:27.860 --> 00:22:30.190
notion of the receptive field which is
essentially a term to describe the fact

00:22:30.190 --> 00:22:30.200
essentially a term to describe the fact
 

00:22:30.200 --> 00:22:35.560
essentially a term to describe the fact
that locations and input layers and are

00:22:35.560 --> 00:22:35.570
that locations and input layers and are
 

00:22:35.570 --> 00:22:40.120
that locations and input layers and are
connected to excuse me a neuron in a

00:22:40.120 --> 00:22:40.130
connected to excuse me a neuron in a
 

00:22:40.130 --> 00:22:42.670
connected to excuse me a neuron in a
downstream layer is only connected to a

00:22:42.670 --> 00:22:42.680
downstream layer is only connected to a
 

00:22:42.680 --> 00:22:45.900
downstream layer is only connected to a
particular location in its in its

00:22:45.900 --> 00:22:45.910
particular location in its in its
 

00:22:45.910 --> 00:22:48.490
particular location in its in its
respective input layer and that is

00:22:48.490 --> 00:22:48.500
respective input layer and that is
 

00:22:48.500 --> 00:22:54.610
respective input layer and that is
termed its receptive field okay so this

00:22:54.610 --> 00:22:54.620
termed its receptive field okay so this
 

00:22:54.620 --> 00:22:59.200
termed its receptive field okay so this
kind of at a high level explains sort of

00:22:59.200 --> 00:22:59.210
kind of at a high level explains sort of
 

00:22:59.210 --> 00:23:01.360
kind of at a high level explains sort of
how these convolutional operations work

00:23:01.360 --> 00:23:01.370
how these convolutional operations work
 

00:23:01.370 --> 00:23:04.690
how these convolutional operations work
in within convolutional layers the next

00:23:04.690 --> 00:23:04.700
in within convolutional layers the next
 

00:23:04.700 --> 00:23:06.670
in within convolutional layers the next
step that I mentioned is applying a

00:23:06.670 --> 00:23:06.680
step that I mentioned is applying a
 

00:23:06.680 --> 00:23:09.460
step that I mentioned is applying a
non-linearity to the output of a

00:23:09.460 --> 00:23:09.470
non-linearity to the output of a
 

00:23:09.470 --> 00:23:13.060
non-linearity to the output of a
convolutional layer and exactly the same

00:23:13.060 --> 00:23:13.070
convolutional layer and exactly the same
 

00:23:13.070 --> 00:23:16.060
convolutional layer and exactly the same
concept as the first lecture we do this

00:23:16.060 --> 00:23:16.070
concept as the first lecture we do this
 

00:23:16.070 --> 00:23:18.700
concept as the first lecture we do this
because image data is highly nonlinear

00:23:18.700 --> 00:23:18.710
because image data is highly nonlinear
 

00:23:18.710 --> 00:23:21.220
because image data is highly nonlinear
right and in CNN's it's very common

00:23:21.220 --> 00:23:21.230
right and in CNN's it's very common
 

00:23:21.230 --> 00:23:24.160
right and in CNN's it's very common
practice to apply nonlinearities after

00:23:24.160 --> 00:23:24.170
practice to apply nonlinearities after
 

00:23:24.170 --> 00:23:26.650
practice to apply nonlinearities after
every convolution operation that is

00:23:26.650 --> 00:23:26.660
every convolution operation that is
 

00:23:26.660 --> 00:23:29.740
every convolution operation that is
after each convolutional layer and the

00:23:29.740 --> 00:23:29.750
after each convolutional layer and the
 

00:23:29.750 --> 00:23:32.290
after each convolutional layer and the
most common activation function that is

00:23:32.290 --> 00:23:32.300
most common activation function that is
 

00:23:32.300 --> 00:23:35.380
most common activation function that is
used is called the relu function which

00:23:35.380 --> 00:23:35.390
used is called the relu function which
 

00:23:35.390 --> 00:23:36.900
used is called the relu function which
is essentially a pixel by pixel

00:23:36.900 --> 00:23:36.910
is essentially a pixel by pixel
 

00:23:36.910 --> 00:23:40.060
is essentially a pixel by pixel
operation that reply replaces all

00:23:40.060 --> 00:23:40.070
operation that reply replaces all
 

00:23:40.070 --> 00:23:42.520
operation that reply replaces all
negative values that follow from a

00:23:42.520 --> 00:23:42.530
negative values that follow from a
 

00:23:42.530 --> 00:23:45.130
negative values that follow from a
convolution with zero and you can think

00:23:45.130 --> 00:23:45.140
convolution with zero and you can think
 

00:23:45.140 --> 00:23:50.190
convolution with zero and you can think
of this as sort of a threshold incurring

00:23:50.190 --> 00:23:50.200
of this as sort of a threshold incurring
 

00:23:50.200 --> 00:23:53.380
of this as sort of a threshold incurring
indicates sort of negative direction of

00:23:53.380 --> 00:23:53.390
indicates sort of negative direction of
 

00:23:53.390 --> 00:23:58.090
indicates sort of negative direction of
a negative detection of that associated

00:23:58.090 --> 00:23:58.100
a negative detection of that associated
 

00:23:58.100 --> 00:24:03.910
a negative detection of that associated
feature the final key operation in cnn's

00:24:03.910 --> 00:24:03.920
feature the final key operation in cnn's
 

00:24:03.920 --> 00:24:07.540
feature the final key operation in cnn's
is pooling and pooling is a operation

00:24:07.540 --> 00:24:07.550
is pooling and pooling is a operation
 

00:24:07.550 --> 00:24:10.330
is pooling and pooling is a operation
that's used to reduce dimensionality and

00:24:10.330 --> 00:24:10.340
that's used to reduce dimensionality and
 

00:24:10.340 --> 00:24:13.210
that's used to reduce dimensionality and
to preserve spatial invariants and a

00:24:13.210 --> 00:24:13.220
to preserve spatial invariants and a
 

00:24:13.220 --> 00:24:16.840
to preserve spatial invariants and a
common technique is called max pooling

00:24:16.840 --> 00:24:16.850
common technique is called max pooling
 

00:24:16.850 --> 00:24:18.910
common technique is called max pooling
it's shown in this example and it's

00:24:18.910 --> 00:24:18.920
it's shown in this example and it's
 

00:24:18.920 --> 00:24:21.220
it's shown in this example and it's
exactly what it sounds you simply take

00:24:21.220 --> 00:24:21.230
exactly what it sounds you simply take
 

00:24:21.230 --> 00:24:24.010
exactly what it sounds you simply take
the maximum value in a patch in this

00:24:24.010 --> 00:24:24.020
the maximum value in a patch in this
 

00:24:24.020 --> 00:24:28.360
the maximum value in a patch in this
case a 2x2 patch that is is being

00:24:28.360 --> 00:24:28.370
case a 2x2 patch that is is being
 

00:24:28.370 --> 00:24:31.410
case a 2x2 patch that is is being
applied with a stride of 2 over this

00:24:31.410 --> 00:24:31.420
applied with a stride of 2 over this
 

00:24:31.420 --> 00:24:36.220
applied with a stride of 2 over this
over this array and the key idea here is

00:24:36.220 --> 00:24:36.230
over this array and the key idea here is
 

00:24:36.230 --> 00:24:38.170
over this array and the key idea here is
that we're reducing the dimensionality

00:24:38.170 --> 00:24:38.180
that we're reducing the dimensionality
 

00:24:38.180 --> 00:24:40.480
that we're reducing the dimensionality
going from one layer to the next and I

00:24:40.480 --> 00:24:40.490
going from one layer to the next and I
 

00:24:40.490 --> 00:24:42.460
going from one layer to the next and I
encourage you to think about other ways

00:24:42.460 --> 00:24:42.470
encourage you to think about other ways
 

00:24:42.470 --> 00:24:45.100
encourage you to think about other ways
in which we can perform this sort of

00:24:45.100 --> 00:24:45.110
in which we can perform this sort of
 

00:24:45.110 --> 00:24:49.090
in which we can perform this sort of
down sampling operation so these are the

00:24:49.090 --> 00:24:49.100
down sampling operation so these are the
 

00:24:49.100 --> 00:24:51.490
down sampling operation so these are the
three key operations to cnn's

00:24:51.490 --> 00:24:51.500
three key operations to cnn's
 

00:24:51.500 --> 00:24:53.770
three key operations to cnn's
and we're now ready to put them together

00:24:53.770 --> 00:24:53.780
and we're now ready to put them together
 

00:24:53.780 --> 00:24:55.780
and we're now ready to put them together
to actually construct our network and

00:24:55.780 --> 00:24:55.790
to actually construct our network and
 

00:24:55.790 --> 00:24:58.660
to actually construct our network and
with cnn's the key the key is that we

00:24:58.660 --> 00:24:58.670
with cnn's the key the key is that we
 

00:24:58.670 --> 00:25:00.100
with cnn's the key the key is that we
can layer these operations

00:25:00.100 --> 00:25:00.110
can layer these operations
 

00:25:00.110 --> 00:25:02.740
can layer these operations
hierarchically and by layering them in

00:25:02.740 --> 00:25:02.750
hierarchically and by layering them in
 

00:25:02.750 --> 00:25:05.860
hierarchically and by layering them in
this way our network can be trained to

00:25:05.860 --> 00:25:05.870
this way our network can be trained to
 

00:25:05.870 --> 00:25:08.110
this way our network can be trained to
learn a hierarchy of features present in

00:25:08.110 --> 00:25:08.120
learn a hierarchy of features present in
 

00:25:08.120 --> 00:25:11.920
learn a hierarchy of features present in
the image data so CNN for image

00:25:11.920 --> 00:25:11.930
the image data so CNN for image
 

00:25:11.930 --> 00:25:13.960
the image data so CNN for image
classification can be broken down into

00:25:13.960 --> 00:25:13.970
classification can be broken down into
 

00:25:13.970 --> 00:25:16.810
classification can be broken down into
two parts first this feature learning

00:25:16.810 --> 00:25:16.820
two parts first this feature learning
 

00:25:16.820 --> 00:25:20.140
two parts first this feature learning
pipeline where we learn features in our

00:25:20.140 --> 00:25:20.150
pipeline where we learn features in our
 

00:25:20.150 --> 00:25:23.130
pipeline where we learn features in our
input images through convolution and

00:25:23.130 --> 00:25:23.140
input images through convolution and
 

00:25:23.140 --> 00:25:27.120
input images through convolution and
through these convolutional layers and

00:25:27.120 --> 00:25:27.130
through these convolutional layers and
 

00:25:27.130 --> 00:25:30.310
through these convolutional layers and
finally the the second half is that

00:25:30.310 --> 00:25:30.320
finally the the second half is that
 

00:25:30.320 --> 00:25:32.080
finally the the second half is that
these convolutional and pooling layers

00:25:32.080 --> 00:25:32.090
these convolutional and pooling layers
 

00:25:32.090 --> 00:25:34.900
these convolutional and pooling layers
will output high level features that are

00:25:34.900 --> 00:25:34.910
will output high level features that are
 

00:25:34.910 --> 00:25:37.450
will output high level features that are
present in the input images and we can

00:25:37.450 --> 00:25:37.460
present in the input images and we can
 

00:25:37.460 --> 00:25:39.940
present in the input images and we can
then pass these features on to fully

00:25:39.940 --> 00:25:39.950
then pass these features on to fully
 

00:25:39.950 --> 00:25:42.460
then pass these features on to fully
connected layers to actually do the

00:25:42.460 --> 00:25:42.470
connected layers to actually do the
 

00:25:42.470 --> 00:25:45.280
connected layers to actually do the
classification task and these fully

00:25:45.280 --> 00:25:45.290
classification task and these fully
 

00:25:45.290 --> 00:25:48.520
classification task and these fully
connected layers can effectively output

00:25:48.520 --> 00:25:48.530
connected layers can effectively output
 

00:25:48.530 --> 00:25:51.040
connected layers can effectively output
a probability distribution for the

00:25:51.040 --> 00:25:51.050
a probability distribution for the
 

00:25:51.050 --> 00:25:54.070
a probability distribution for the
images membership over a set of possible

00:25:54.070 --> 00:25:54.080
images membership over a set of possible
 

00:25:54.080 --> 00:25:56.980
images membership over a set of possible
classes and a common way that this is

00:25:56.980 --> 00:25:56.990
classes and a common way that this is
 

00:25:56.990 --> 00:25:59.140
classes and a common way that this is
achieved is using this function called

00:25:59.140 --> 00:25:59.150
achieved is using this function called
 

00:25:59.150 --> 00:26:03.150
achieved is using this function called
the softmax whose output represents a

00:26:03.150 --> 00:26:03.160
the softmax whose output represents a
 

00:26:03.160 --> 00:26:06.060
the softmax whose output represents a
categorical probability distribution

00:26:06.060 --> 00:26:06.070
categorical probability distribution
 

00:26:06.070 --> 00:26:08.770
categorical probability distribution
over the set of classes that you're

00:26:08.770 --> 00:26:08.780
over the set of classes that you're
 

00:26:08.780 --> 00:26:13.870
over the set of classes that you're
interested in okay so the final key

00:26:13.870 --> 00:26:13.880
interested in okay so the final key
 

00:26:13.880 --> 00:26:16.890
interested in okay so the final key
piece right is how do we train this and

00:26:16.890 --> 00:26:16.900
piece right is how do we train this and
 

00:26:16.900 --> 00:26:20.230
piece right is how do we train this and
it's the same idea as we introduced in

00:26:20.230 --> 00:26:20.240
it's the same idea as we introduced in
 

00:26:20.240 --> 00:26:23.980
it's the same idea as we introduced in
lecture 1 back propagation the important

00:26:23.980 --> 00:26:23.990
lecture 1 back propagation the important
 

00:26:23.990 --> 00:26:25.780
lecture 1 back propagation the important
thing to keep in mind is what it is

00:26:25.780 --> 00:26:25.790
thing to keep in mind is what it is
 

00:26:25.790 --> 00:26:30.330
thing to keep in mind is what it is
we're learning when we're training SIA

00:26:30.330 --> 00:26:30.340
we're learning when we're training SIA
 

00:26:30.340 --> 00:26:34.180
we're learning when we're training SIA
what we learn when we train a CNN model

00:26:34.180 --> 00:26:34.190
what we learn when we train a CNN model
 

00:26:34.190 --> 00:26:36.550
what we learn when we train a CNN model
is the weights of those convolution

00:26:36.550 --> 00:26:36.560
is the weights of those convolution
 

00:26:36.560 --> 00:26:41.050
is the weights of those convolution
filters at another degree of abstraction

00:26:41.050 --> 00:26:41.060
filters at another degree of abstraction
 

00:26:41.060 --> 00:26:43.060
filters at another degree of abstraction
right you can think of this as what

00:26:43.060 --> 00:26:43.070
right you can think of this as what
 

00:26:43.070 --> 00:26:45.430
right you can think of this as what
features the network is learning to

00:26:45.430 --> 00:26:45.440
features the network is learning to
 

00:26:45.440 --> 00:26:48.760
features the network is learning to
detect in addition we also learn the

00:26:48.760 --> 00:26:48.770
detect in addition we also learn the
 

00:26:48.770 --> 00:26:50.830
detect in addition we also learn the
weights for the fully connected layers

00:26:50.830 --> 00:26:50.840
weights for the fully connected layers
 

00:26:50.840 --> 00:26:53.290
weights for the fully connected layers
if we're performing a classification

00:26:53.290 --> 00:26:53.300
if we're performing a classification
 

00:26:53.300 --> 00:26:57.730
if we're performing a classification
task and since our output in this case

00:26:57.730 --> 00:26:57.740
task and since our output in this case
 

00:26:57.740 --> 00:26:59.740
task and since our output in this case
is going to be a probability

00:26:59.740 --> 00:26:59.750
is going to be a probability
 

00:26:59.750 --> 00:27:02.380
is going to be a probability
distribution we can use that cross

00:27:02.380 --> 00:27:02.390
distribution we can use that cross
 

00:27:02.390 --> 00:27:04.240
distribution we can use that cross
entropy loss that was introduced in

00:27:04.240 --> 00:27:04.250
entropy loss that was introduced in
 

00:27:04.250 --> 00:27:07.180
entropy loss that was introduced in
lecture 1 to optimize via back

00:27:07.180 --> 00:27:07.190
lecture 1 to optimize via back
 

00:27:07.190 --> 00:27:12.300
lecture 1 to optimize via back
propagation so arguably the most famous

00:27:12.300 --> 00:27:12.310
propagation so arguably the most famous
 

00:27:12.310 --> 00:27:16.660
propagation so arguably the most famous
example of cnn's and for cnn's for

00:27:16.660 --> 00:27:16.670
example of cnn's and for cnn's for
 

00:27:16.670 --> 00:27:18.460
example of cnn's and for cnn's for
classification and maybe cnn's in

00:27:18.460 --> 00:27:18.470
classification and maybe cnn's in
 

00:27:18.470 --> 00:27:21.760
classification and maybe cnn's in
general is those trained and tested on

00:27:21.760 --> 00:27:21.770
general is those trained and tested on
 

00:27:21.770 --> 00:27:25.150
general is those trained and tested on
the famous imagenet data set an image

00:27:25.150 --> 00:27:25.160
the famous imagenet data set an image
 

00:27:25.160 --> 00:27:28.330
the famous imagenet data set an image
net is a massive data set with over 14

00:27:28.330 --> 00:27:28.340
net is a massive data set with over 14
 

00:27:28.340 --> 00:27:31.660
net is a massive data set with over 14
million images across over 20,000

00:27:31.660 --> 00:27:31.670
million images across over 20,000
 

00:27:31.670 --> 00:27:34.150
million images across over 20,000
different categories and this was

00:27:34.150 --> 00:27:34.160
different categories and this was
 

00:27:34.160 --> 00:27:37.020
different categories and this was
created and curated by a lab at Stanford

00:27:37.020 --> 00:27:37.030
created and curated by a lab at Stanford
 

00:27:37.030 --> 00:27:42.490
created and curated by a lab at Stanford
and is really become a very widely used

00:27:42.490 --> 00:27:42.500
and is really become a very widely used
 

00:27:42.500 --> 00:27:45.910
and is really become a very widely used
data set across a machine learning so

00:27:45.910 --> 00:27:45.920
data set across a machine learning so
 

00:27:45.920 --> 00:27:48.310
data set across a machine learning so
just to take an example in image net

00:27:48.310 --> 00:27:48.320
just to take an example in image net
 

00:27:48.320 --> 00:27:51.160
just to take an example in image net
there are 1,400 nine different pictures

00:27:51.160 --> 00:27:51.170
there are 1,400 nine different pictures
 

00:27:51.170 --> 00:27:54.610
there are 1,400 nine different pictures
of bananas and even better than the size

00:27:54.610 --> 00:27:54.620
of bananas and even better than the size
 

00:27:54.620 --> 00:27:57.730
of bananas and even better than the size
of this data set I really appreciated

00:27:57.730 --> 00:27:57.740
of this data set I really appreciated
 

00:27:57.740 --> 00:28:01.380
of this data set I really appreciated
their description of what a banana is

00:28:01.380 --> 00:28:01.390
their description of what a banana is
 

00:28:01.390 --> 00:28:04.030
their description of what a banana is
succinctly described as an elongated

00:28:04.030 --> 00:28:04.040
succinctly described as an elongated
 

00:28:04.040 --> 00:28:06.280
succinctly described as an elongated
crescent-shaped yellow fruit with soft

00:28:06.280 --> 00:28:06.290
crescent-shaped yellow fruit with soft
 

00:28:06.290 --> 00:28:09.340
crescent-shaped yellow fruit with soft
sweet / which both gives a pretty good

00:28:09.340 --> 00:28:09.350
sweet / which both gives a pretty good
 

00:28:09.350 --> 00:28:11.770
sweet / which both gives a pretty good
description of what a banana is and

00:28:11.770 --> 00:28:11.780
description of what a banana is and
 

00:28:11.780 --> 00:28:15.360
description of what a banana is and
speaks to its obvious deliciousness

00:28:15.360 --> 00:28:15.370
speaks to its obvious deliciousness
 

00:28:15.370 --> 00:28:17.860
speaks to its obvious deliciousness
so the creators of image net also

00:28:17.860 --> 00:28:17.870
so the creators of image net also
 

00:28:17.870 --> 00:28:20.340
so the creators of image net also
created a set of visual recognition

00:28:20.340 --> 00:28:20.350
created a set of visual recognition
 

00:28:20.350 --> 00:28:24.070
created a set of visual recognition
challenges on their data set and most

00:28:24.070 --> 00:28:24.080
challenges on their data set and most
 

00:28:24.080 --> 00:28:26.110
challenges on their data set and most
notably the image net classification

00:28:26.110 --> 00:28:26.120
notably the image net classification
 

00:28:26.120 --> 00:28:29.140
notably the image net classification
task which is really simple it's

00:28:29.140 --> 00:28:29.150
task which is really simple it's
 

00:28:29.150 --> 00:28:32.050
task which is really simple it's
produced a list of the object different

00:28:32.050 --> 00:28:32.060
produced a list of the object different
 

00:28:32.060 --> 00:28:34.540
produced a list of the object different
object categories that are present in

00:28:34.540 --> 00:28:34.550
object categories that are present in
 

00:28:34.550 --> 00:28:38.350
object categories that are present in
images across a ground truth set of

00:28:38.350 --> 00:28:38.360
images across a ground truth set of
 

00:28:38.360 --> 00:28:40.470
images across a ground truth set of
1,000 different categories

00:28:40.470 --> 00:28:40.480
1,000 different categories
 

00:28:40.480 --> 00:28:42.810
1,000 different categories
and in this competition they measured

00:28:42.810 --> 00:28:42.820
and in this competition they measured
 

00:28:42.820 --> 00:28:46.500
and in this competition they measured
the accuracy of the models that were

00:28:46.500 --> 00:28:46.510
the accuracy of the models that were
 

00:28:46.510 --> 00:28:49.169
the accuracy of the models that were
submitted in terms of the rate at which

00:28:49.169 --> 00:28:49.179
submitted in terms of the rate at which
 

00:28:49.179 --> 00:28:52.169
submitted in terms of the rate at which
the model did not output the correct

00:28:52.169 --> 00:28:52.179
the model did not output the correct
 

00:28:52.179 --> 00:28:55.140
the model did not output the correct
label in its top five predictions for a

00:28:55.140 --> 00:28:55.150
label in its top five predictions for a
 

00:28:55.150 --> 00:28:59.880
label in its top five predictions for a
particular image and the results of this

00:28:59.880 --> 00:28:59.890
particular image and the results of this
 

00:28:59.890 --> 00:29:02.010
particular image and the results of this
image net classification challenge are

00:29:02.010 --> 00:29:02.020
image net classification challenge are
 

00:29:02.020 --> 00:29:05.610
image net classification challenge are
pretty astonishing 2012 was the first

00:29:05.610 --> 00:29:05.620
pretty astonishing 2012 was the first
 

00:29:05.620 --> 00:29:09.060
pretty astonishing 2012 was the first
time a CNN won the challenge and this

00:29:09.060 --> 00:29:09.070
time a CNN won the challenge and this
 

00:29:09.070 --> 00:29:12.210
time a CNN won the challenge and this
was the famous CNN called Alex net and

00:29:12.210 --> 00:29:12.220
was the famous CNN called Alex net and
 

00:29:12.220 --> 00:29:15.390
was the famous CNN called Alex net and
since then the neural networks that have

00:29:15.390 --> 00:29:15.400
since then the neural networks that have
 

00:29:15.400 --> 00:29:17.190
since then the neural networks that have
have neural networks have completely

00:29:17.190 --> 00:29:17.200
have neural networks have completely
 

00:29:17.200 --> 00:29:20.030
have neural networks have completely
dominated this competition and the error

00:29:20.030 --> 00:29:20.040
dominated this competition and the error
 

00:29:20.040 --> 00:29:22.770
dominated this competition and the error
that the state of the art is able to

00:29:22.770 --> 00:29:22.780
that the state of the art is able to
 

00:29:22.780 --> 00:29:24.860
that the state of the art is able to
achieve keeps decreasing and decreasing

00:29:24.860 --> 00:29:24.870
achieve keeps decreasing and decreasing
 

00:29:24.870 --> 00:29:29.340
achieve keeps decreasing and decreasing
surpassing human error in 2015 with the

00:29:29.340 --> 00:29:29.350
surpassing human error in 2015 with the
 

00:29:29.350 --> 00:29:33.440
surpassing human error in 2015 with the
famous ResNet Network which had 152

00:29:33.440 --> 00:29:33.450
famous ResNet Network which had 152
 

00:29:33.450 --> 00:29:38.240
famous ResNet Network which had 152
convolutional layers in its design but

00:29:38.240 --> 00:29:38.250
convolutional layers in its design but
 

00:29:38.250 --> 00:29:41.610
convolutional layers in its design but
with improved accuracy the number of

00:29:41.610 --> 00:29:41.620
with improved accuracy the number of
 

00:29:41.620 --> 00:29:43.650
with improved accuracy the number of
layers in these networks has steadily

00:29:43.650 --> 00:29:43.660
layers in these networks has steadily
 

00:29:43.660 --> 00:29:47.310
layers in these networks has steadily
been increasing so take it as what you

00:29:47.310 --> 00:29:47.320
been increasing so take it as what you
 

00:29:47.320 --> 00:29:50.370
been increasing so take it as what you
will you know there's something to be

00:29:50.370 --> 00:29:50.380
will you know there's something to be
 

00:29:50.380 --> 00:29:53.039
will you know there's something to be
said about building deeper and deeper

00:29:53.039 --> 00:29:53.049
said about building deeper and deeper
 

00:29:53.049 --> 00:29:54.810
said about building deeper and deeper
networks to achieve higher and higher

00:29:54.810 --> 00:29:54.820
networks to achieve higher and higher
 

00:29:54.820 --> 00:29:59.130
networks to achieve higher and higher
accuracies so so far we've only talked

00:29:59.130 --> 00:29:59.140
accuracies so so far we've only talked
 

00:29:59.140 --> 00:30:02.460
accuracies so so far we've only talked
about classification but in truth CNN's

00:30:02.460 --> 00:30:02.470
about classification but in truth CNN's
 

00:30:02.470 --> 00:30:05.150
about classification but in truth CNN's
are extremely flexible architecture and

00:30:05.150 --> 00:30:05.160
are extremely flexible architecture and
 

00:30:05.160 --> 00:30:08.340
are extremely flexible architecture and
have been shown to be really powerful

00:30:08.340 --> 00:30:08.350
have been shown to be really powerful
 

00:30:08.350 --> 00:30:09.870
have been shown to be really powerful
for a number of different applications

00:30:09.870 --> 00:30:09.880
for a number of different applications
 

00:30:09.880 --> 00:30:13.409
for a number of different applications
and when we considered a CNN for

00:30:13.409 --> 00:30:13.419
and when we considered a CNN for
 

00:30:13.419 --> 00:30:15.690
and when we considered a CNN for
classification I showed this general

00:30:15.690 --> 00:30:15.700
classification I showed this general
 

00:30:15.700 --> 00:30:18.060
classification I showed this general
pipeline schematic where we had two

00:30:18.060 --> 00:30:18.070
pipeline schematic where we had two
 

00:30:18.070 --> 00:30:20.340
pipeline schematic where we had two
parts the feature learning part and the

00:30:20.340 --> 00:30:20.350
parts the feature learning part and the
 

00:30:20.350 --> 00:30:23.580
parts the feature learning part and the
classification part and what makes a

00:30:23.580 --> 00:30:23.590
classification part and what makes a
 

00:30:23.590 --> 00:30:25.230
classification part and what makes a
convolutional neural network a

00:30:25.230 --> 00:30:25.240
convolutional neural network a
 

00:30:25.240 --> 00:30:27.510
convolutional neural network a
convolutional neural network is really

00:30:27.510 --> 00:30:27.520
convolutional neural network is really
 

00:30:27.520 --> 00:30:30.120
convolutional neural network is really
this feature learning portion and after

00:30:30.120 --> 00:30:30.130
this feature learning portion and after
 

00:30:30.130 --> 00:30:31.950
this feature learning portion and after
that we can really change the second

00:30:31.950 --> 00:30:31.960
that we can really change the second
 

00:30:31.960 --> 00:30:34.860
that we can really change the second
part to suit the application that we

00:30:34.860 --> 00:30:34.870
part to suit the application that we
 

00:30:34.870 --> 00:30:37.669
part to suit the application that we
desire so for example this portion is

00:30:37.669 --> 00:30:37.679
desire so for example this portion is
 

00:30:37.679 --> 00:30:39.960
desire so for example this portion is
going to look different for different

00:30:39.960 --> 00:30:39.970
going to look different for different
 

00:30:39.970 --> 00:30:43.110
going to look different for different
image classification domains and we can

00:30:43.110 --> 00:30:43.120
image classification domains and we can
 

00:30:43.120 --> 00:30:45.600
image classification domains and we can
also introduce new architectures for

00:30:45.600 --> 00:30:45.610
also introduce new architectures for
 

00:30:45.610 --> 00:30:48.450
also introduce new architectures for
different types of tasks such as object

00:30:48.450 --> 00:30:48.460
different types of tasks such as object
 

00:30:48.460 --> 00:30:51.060
different types of tasks such as object
recognition segmentation and the image

00:30:51.060 --> 00:30:51.070
recognition segmentation and the image
 

00:30:51.070 --> 00:30:53.769
recognition segmentation and the image
captioning so I'd like to consider

00:30:53.769 --> 00:30:53.779
captioning so I'd like to consider
 

00:30:53.779 --> 00:30:55.719
captioning so I'd like to consider
three different applications of CNN's

00:30:55.719 --> 00:30:55.729
three different applications of CNN's
 

00:30:55.729 --> 00:30:58.719
three different applications of CNN's
beyond image classification the first is

00:30:58.719 --> 00:30:58.729
beyond image classification the first is
 

00:30:58.729 --> 00:31:01.599
beyond image classification the first is
semantic segmentation where the task is

00:31:01.599 --> 00:31:01.609
semantic segmentation where the task is
 

00:31:01.609 --> 00:31:04.089
semantic segmentation where the task is
to assign each pixel in the image an

00:31:04.089 --> 00:31:04.099
to assign each pixel in the image an
 

00:31:04.099 --> 00:31:06.969
to assign each pixel in the image an
object class to produce a segmentation

00:31:06.969 --> 00:31:06.979
object class to produce a segmentation
 

00:31:06.979 --> 00:31:10.330
object class to produce a segmentation
of the image object detection where we

00:31:10.330 --> 00:31:10.340
of the image object detection where we
 

00:31:10.340 --> 00:31:13.599
of the image object detection where we
want to detect instances of specific

00:31:13.599 --> 00:31:13.609
want to detect instances of specific
 

00:31:13.609 --> 00:31:16.269
want to detect instances of specific
objects in the image and finally image

00:31:16.269 --> 00:31:16.279
objects in the image and finally image
 

00:31:16.279 --> 00:31:18.459
objects in the image and finally image
captioning where the task is to generate

00:31:18.459 --> 00:31:18.469
captioning where the task is to generate
 

00:31:18.469 --> 00:31:21.959
captioning where the task is to generate
a language description of the image that

00:31:21.959 --> 00:31:21.969
a language description of the image that
 

00:31:21.969 --> 00:31:26.940
a language description of the image that
captures its semantic meaning so first

00:31:26.940 --> 00:31:26.950
captures its semantic meaning so first
 

00:31:26.950 --> 00:31:29.680
captures its semantic meaning so first
let's talk about semantic segmentation

00:31:29.680 --> 00:31:29.690
let's talk about semantic segmentation
 

00:31:29.690 --> 00:31:32.379
let's talk about semantic segmentation
with this architecture called fully

00:31:32.379 --> 00:31:32.389
with this architecture called fully
 

00:31:32.389 --> 00:31:35.609
with this architecture called fully
convolutional networks or f c ends and

00:31:35.609 --> 00:31:35.619
convolutional networks or f c ends and
 

00:31:35.619 --> 00:31:38.769
convolutional networks or f c ends and
here the way it works is the network

00:31:38.769 --> 00:31:38.779
here the way it works is the network
 

00:31:38.779 --> 00:31:41.609
here the way it works is the network
takes in an input of arbitrary size and

00:31:41.609 --> 00:31:41.619
takes in an input of arbitrary size and
 

00:31:41.619 --> 00:31:46.119
takes in an input of arbitrary size and
produces an output of of corresponding

00:31:46.119 --> 00:31:46.129
produces an output of of corresponding
 

00:31:46.129 --> 00:31:48.489
produces an output of of corresponding
size where each pixel has been assigned

00:31:48.489 --> 00:31:48.499
size where each pixel has been assigned
 

00:31:48.499 --> 00:31:50.619
size where each pixel has been assigned
an object class which we can then

00:31:50.619 --> 00:31:50.629
an object class which we can then
 

00:31:50.629 --> 00:31:54.279
an object class which we can then
visualize as a segmentation and so as

00:31:54.279 --> 00:31:54.289
visualize as a segmentation and so as
 

00:31:54.289 --> 00:31:57.009
visualize as a segmentation and so as
before we have a series of convolutional

00:31:57.009 --> 00:31:57.019
before we have a series of convolutional
 

00:31:57.019 --> 00:32:02.079
before we have a series of convolutional
layers arranged in this hierarchical

00:32:02.079 --> 00:32:02.089
layers arranged in this hierarchical
 

00:32:02.089 --> 00:32:06.519
layers arranged in this hierarchical
fashion to create this learned hierarchy

00:32:06.519 --> 00:32:06.529
fashion to create this learned hierarchy
 

00:32:06.529 --> 00:32:09.249
fashion to create this learned hierarchy
of features but then we can supplement

00:32:09.249 --> 00:32:09.259
of features but then we can supplement
 

00:32:09.259 --> 00:32:14.109
of features but then we can supplement
this these down sampling operations with

00:32:14.109 --> 00:32:14.119
this these down sampling operations with
 

00:32:14.119 --> 00:32:17.440
this these down sampling operations with
up sampling operations that increase the

00:32:17.440 --> 00:32:17.450
up sampling operations that increase the
 

00:32:17.450 --> 00:32:19.269
up sampling operations that increase the
resolution of the output from those

00:32:19.269 --> 00:32:19.279
resolution of the output from those
 

00:32:19.279 --> 00:32:21.759
resolution of the output from those
feature learning layers and then you can

00:32:21.759 --> 00:32:21.769
feature learning layers and then you can
 

00:32:21.769 --> 00:32:24.909
feature learning layers and then you can
combine the output from these up

00:32:24.909 --> 00:32:24.919
combine the output from these up
 

00:32:24.919 --> 00:32:27.310
combine the output from these up
sampling layers with those from the down

00:32:27.310 --> 00:32:27.320
sampling layers with those from the down
 

00:32:27.320 --> 00:32:29.289
sampling layers with those from the down
sampling layers to actually produce a

00:32:29.289 --> 00:32:29.299
sampling layers to actually produce a
 

00:32:29.299 --> 00:32:32.829
sampling layers to actually produce a
segmentation one application of this

00:32:32.829 --> 00:32:32.839
segmentation one application of this
 

00:32:32.839 --> 00:32:35.009
segmentation one application of this
sort of architecture is to the real-time

00:32:35.009 --> 00:32:35.019
sort of architecture is to the real-time
 

00:32:35.019 --> 00:32:39.070
sort of architecture is to the real-time
segmentation of driving scenes so this

00:32:39.070 --> 00:32:39.080
segmentation of driving scenes so this
 

00:32:39.080 --> 00:32:41.229
segmentation of driving scenes so this
was a results from a couple years ago

00:32:41.229 --> 00:32:41.239
was a results from a couple years ago
 

00:32:41.239 --> 00:32:43.959
was a results from a couple years ago
where the authors were using this

00:32:43.959 --> 00:32:43.969
where the authors were using this
 

00:32:43.969 --> 00:32:48.459
where the authors were using this
encoder decoder like structure where you

00:32:48.459 --> 00:32:48.469
encoder decoder like structure where you
 

00:32:48.469 --> 00:32:51.070
encoder decoder like structure where you
have down sampling layers followed by up

00:32:51.070 --> 00:32:51.080
have down sampling layers followed by up
 

00:32:51.080 --> 00:32:54.879
have down sampling layers followed by up
sampling layers to to produce these

00:32:54.879 --> 00:32:54.889
sampling layers to to produce these
 

00:32:54.889 --> 00:32:58.149
sampling layers to to produce these
these segmentations and last year this

00:32:58.149 --> 00:32:58.159
these segmentations and last year this
 

00:32:58.159 --> 00:32:59.560
these segmentations and last year this
was sort of the state of the art

00:32:59.560 --> 00:32:59.570
was sort of the state of the art
 

00:32:59.570 --> 00:33:01.869
was sort of the state of the art
performance that you could achieve with

00:33:01.869 --> 00:33:01.879
performance that you could achieve with
 

00:33:01.879 --> 00:33:04.269
performance that you could achieve with
an architecture like this in in doing

00:33:04.269 --> 00:33:04.279
an architecture like this in in doing
 

00:33:04.279 --> 00:33:05.190
an architecture like this in in doing
semantic segment

00:33:05.190 --> 00:33:05.200
semantic segment
 

00:33:05.200 --> 00:33:08.190
semantic segment
pation and deep learning is moving

00:33:08.190 --> 00:33:08.200
pation and deep learning is moving
 

00:33:08.200 --> 00:33:10.560
pation and deep learning is moving
extremely fast and now the new state of

00:33:10.560 --> 00:33:10.570
extremely fast and now the new state of
 

00:33:10.570 --> 00:33:13.080
extremely fast and now the new state of
the art in semantic segmentation is what

00:33:13.080 --> 00:33:13.090
the art in semantic segmentation is what
 

00:33:13.090 --> 00:33:15.299
the art in semantic segmentation is what
you see here and it's actually the same

00:33:15.299 --> 00:33:15.309
you see here and it's actually the same
 

00:33:15.309 --> 00:33:17.700
you see here and it's actually the same
the same authors as that previous result

00:33:17.700 --> 00:33:17.710
the same authors as that previous result
 

00:33:17.710 --> 00:33:20.879
the same authors as that previous result
but with an improved architecture where

00:33:20.879 --> 00:33:20.889
but with an improved architecture where
 

00:33:20.889 --> 00:33:23.039
but with an improved architecture where
now they're using one network trained to

00:33:23.039 --> 00:33:23.049
now they're using one network trained to
 

00:33:23.049 --> 00:33:26.279
now they're using one network trained to
do three tasks simultaneously semantic

00:33:26.279 --> 00:33:26.289
do three tasks simultaneously semantic
 

00:33:26.289 --> 00:33:30.779
do three tasks simultaneously semantic
segmentation shown here depth estimation

00:33:30.779 --> 00:33:30.789
segmentation shown here depth estimation
 

00:33:30.789 --> 00:33:34.769
segmentation shown here depth estimation
and instant segmentation which means

00:33:34.769 --> 00:33:34.779
and instant segmentation which means
 

00:33:34.779 --> 00:33:37.049
and instant segmentation which means
identifying different instances of the

00:33:37.049 --> 00:33:37.059
identifying different instances of the
 

00:33:37.059 --> 00:33:39.899
identifying different instances of the
same object type and as you can see in

00:33:39.899 --> 00:33:39.909
same object type and as you can see in
 

00:33:39.909 --> 00:33:41.340
same object type and as you can see in
this upper right corner

00:33:41.340 --> 00:33:41.350
this upper right corner
 

00:33:41.350 --> 00:33:43.259
this upper right corner
these segmentation results are pretty

00:33:43.259 --> 00:33:43.269
these segmentation results are pretty
 

00:33:43.269 --> 00:33:45.180
these segmentation results are pretty
astonishing and they've they

00:33:45.180 --> 00:33:45.190
astonishing and they've they
 

00:33:45.190 --> 00:33:47.070
astonishing and they've they
significantly improved in terms of their

00:33:47.070 --> 00:33:47.080
significantly improved in terms of their
 

00:33:47.080 --> 00:33:50.340
significantly improved in terms of their
crypts crispness compared to the

00:33:50.340 --> 00:33:50.350
crypts crispness compared to the
 

00:33:50.350 --> 00:33:56.159
crypts crispness compared to the
previous result another way CNN's have

00:33:56.159 --> 00:33:56.169
previous result another way CNN's have
 

00:33:56.169 --> 00:33:58.590
previous result another way CNN's have
been extended is for object detection

00:33:58.590 --> 00:33:58.600
been extended is for object detection
 

00:33:58.600 --> 00:34:01.259
been extended is for object detection
where here the task is to learn features

00:34:01.259 --> 00:34:01.269
where here the task is to learn features
 

00:34:01.269 --> 00:34:03.690
where here the task is to learn features
that characterize particular regions of

00:34:03.690 --> 00:34:03.700
that characterize particular regions of
 

00:34:03.700 --> 00:34:06.240
that characterize particular regions of
the input image then classify those

00:34:06.240 --> 00:34:06.250
the input image then classify those
 

00:34:06.250 --> 00:34:09.060
the input image then classify those
regions as belonging to particular

00:34:09.060 --> 00:34:09.070
regions as belonging to particular
 

00:34:09.070 --> 00:34:11.819
regions as belonging to particular
object classes and the pipeline for

00:34:11.819 --> 00:34:11.829
object classes and the pipeline for
 

00:34:11.829 --> 00:34:14.220
object classes and the pipeline for
doing this is is an architecture called

00:34:14.220 --> 00:34:14.230
doing this is is an architecture called
 

00:34:14.230 --> 00:34:16.470
doing this is is an architecture called
our CNN and it's pretty straight forward

00:34:16.470 --> 00:34:16.480
our CNN and it's pretty straight forward
 

00:34:16.480 --> 00:34:19.760
our CNN and it's pretty straight forward
so given an input image this algorithm

00:34:19.760 --> 00:34:19.770
so given an input image this algorithm
 

00:34:19.770 --> 00:34:22.680
so given an input image this algorithm
extracts a set of region proposals

00:34:22.680 --> 00:34:22.690
extracts a set of region proposals
 

00:34:22.690 --> 00:34:25.470
extracts a set of region proposals
bottom-up computes features for these

00:34:25.470 --> 00:34:25.480
bottom-up computes features for these
 

00:34:25.480 --> 00:34:28.020
bottom-up computes features for these
proposals using convolutional layers and

00:34:28.020 --> 00:34:28.030
proposals using convolutional layers and
 

00:34:28.030 --> 00:34:31.940
proposals using convolutional layers and
then classifies each region proposal and

00:34:31.940 --> 00:34:31.950
then classifies each region proposal and
 

00:34:31.950 --> 00:34:34.700
then classifies each region proposal and
there have been many many different

00:34:34.700 --> 00:34:34.710
there have been many many different
 

00:34:34.710 --> 00:34:37.889
there have been many many different
approaches to computing these different

00:34:37.889 --> 00:34:37.899
approaches to computing these different
 

00:34:37.899 --> 00:34:40.050
approaches to computing these different
computing and estimating these region

00:34:40.050 --> 00:34:40.060
computing and estimating these region
 

00:34:40.060 --> 00:34:42.659
computing and estimating these region
proposals step two in this in this

00:34:42.659 --> 00:34:42.669
proposals step two in this in this
 

00:34:42.669 --> 00:34:48.059
proposals step two in this in this
pipeline and as this has resulted in a

00:34:48.059 --> 00:34:48.069
pipeline and as this has resulted in a
 

00:34:48.069 --> 00:34:49.980
pipeline and as this has resulted in a
number of different extensions of this

00:34:49.980 --> 00:34:49.990
number of different extensions of this
 

00:34:49.990 --> 00:34:53.399
number of different extensions of this
of this general principle the final

00:34:53.399 --> 00:34:53.409
of this general principle the final
 

00:34:53.409 --> 00:34:56.010
of this general principle the final
application that I'd like to consider is

00:34:56.010 --> 00:34:56.020
application that I'd like to consider is
 

00:34:56.020 --> 00:34:59.099
application that I'd like to consider is
image captioning and so suppose we're

00:34:59.099 --> 00:34:59.109
image captioning and so suppose we're
 

00:34:59.109 --> 00:35:00.960
image captioning and so suppose we're
given this image of a cat riding the

00:35:00.960 --> 00:35:00.970
given this image of a cat riding the
 

00:35:00.970 --> 00:35:04.020
given this image of a cat riding the
skateboard in classification our task

00:35:04.020 --> 00:35:04.030
skateboard in classification our task
 

00:35:04.030 --> 00:35:06.990
skateboard in classification our task
could be to output the class label for

00:35:06.990 --> 00:35:07.000
could be to output the class label for
 

00:35:07.000 --> 00:35:10.200
could be to output the class label for
this particular image cat and as we saw

00:35:10.200 --> 00:35:10.210
this particular image cat and as we saw
 

00:35:10.210 --> 00:35:12.240
this particular image cat and as we saw
this is done by feeding the image

00:35:12.240 --> 00:35:12.250
this is done by feeding the image
 

00:35:12.250 --> 00:35:14.790
this is done by feeding the image
through a set of convolutional layers to

00:35:14.790 --> 00:35:14.800
through a set of convolutional layers to
 

00:35:14.800 --> 00:35:17.460
through a set of convolutional layers to
extract features and then a set of fully

00:35:17.460 --> 00:35:17.470
extract features and then a set of fully
 

00:35:17.470 --> 00:35:18.220
extract features and then a set of fully
connected

00:35:18.220 --> 00:35:18.230
connected
 

00:35:18.230 --> 00:35:22.210
connected
to generate a prediction in image

00:35:22.210 --> 00:35:22.220
to generate a prediction in image
 

00:35:22.220 --> 00:35:23.890
to generate a prediction in image
captioning what we want to do is

00:35:23.890 --> 00:35:23.900
captioning what we want to do is
 

00:35:23.900 --> 00:35:26.620
captioning what we want to do is
generate a sentence that describes the

00:35:26.620 --> 00:35:26.630
generate a sentence that describes the
 

00:35:26.630 --> 00:35:29.530
generate a sentence that describes the
semantic content of the image so if we

00:35:29.530 --> 00:35:29.540
semantic content of the image so if we
 

00:35:29.540 --> 00:35:33.040
semantic content of the image so if we
take that same CNN Network from before

00:35:33.040 --> 00:35:33.050
take that same CNN Network from before
 

00:35:33.050 --> 00:35:35.650
take that same CNN Network from before
and instead of fully connected layers at

00:35:35.650 --> 00:35:35.660
and instead of fully connected layers at
 

00:35:35.660 --> 00:35:39.130
and instead of fully connected layers at
the end we replace it with an RNN what

00:35:39.130 --> 00:35:39.140
the end we replace it with an RNN what
 

00:35:39.140 --> 00:35:41.849
the end we replace it with an RNN what
we can do is we can use a set of

00:35:41.849 --> 00:35:41.859
we can do is we can use a set of
 

00:35:41.859 --> 00:35:44.349
we can do is we can use a set of
convolutional layers to extract visual

00:35:44.349 --> 00:35:44.359
convolutional layers to extract visual
 

00:35:44.359 --> 00:35:48.220
convolutional layers to extract visual
features encode them in put them into a

00:35:48.220 --> 00:35:48.230
features encode them in put them into a
 

00:35:48.230 --> 00:35:49.870
features encode them in put them into a
recurrent neural network which we

00:35:49.870 --> 00:35:49.880
recurrent neural network which we
 

00:35:49.880 --> 00:35:53.760
recurrent neural network which we
learned about yesterday and then

00:35:53.760 --> 00:35:53.770
learned about yesterday and then
 

00:35:53.770 --> 00:35:57.359
learned about yesterday and then
generate a sentence that describes the

00:35:57.359 --> 00:35:57.369
generate a sentence that describes the
 

00:35:57.369 --> 00:35:59.710
generate a sentence that describes the
semantic content that's present in that

00:35:59.710 --> 00:35:59.720
semantic content that's present in that
 

00:35:59.720 --> 00:36:02.079
semantic content that's present in that
image and the reason we can do this is

00:36:02.079 --> 00:36:02.089
image and the reason we can do this is
 

00:36:02.089 --> 00:36:04.089
image and the reason we can do this is
that the output of these convolutional

00:36:04.089 --> 00:36:04.099
that the output of these convolutional
 

00:36:04.099 --> 00:36:06.730
that the output of these convolutional
layers gives us a fixed length encoding

00:36:06.730 --> 00:36:06.740
layers gives us a fixed length encoding
 

00:36:06.740 --> 00:36:09.970
layers gives us a fixed length encoding
that initializes our that we can use to

00:36:09.970 --> 00:36:09.980
that initializes our that we can use to
 

00:36:09.980 --> 00:36:14.339
that initializes our that we can use to
initialize an RNN and train it on this

00:36:14.339 --> 00:36:14.349
initialize an RNN and train it on this
 

00:36:14.349 --> 00:36:19.290
initialize an RNN and train it on this
captioning task so these are three very

00:36:19.290 --> 00:36:19.300
captioning task so these are three very
 

00:36:19.300 --> 00:36:24.040
captioning task so these are three very
concrete fundamental applications of of

00:36:24.040 --> 00:36:24.050
concrete fundamental applications of of
 

00:36:24.050 --> 00:36:28.030
concrete fundamental applications of of
CNN's but to take it a step further in

00:36:28.030 --> 00:36:28.040
CNN's but to take it a step further in
 

00:36:28.040 --> 00:36:31.569
CNN's but to take it a step further in
terms of sort of the depth and breadth

00:36:31.569 --> 00:36:31.579
terms of sort of the depth and breadth
 

00:36:31.579 --> 00:36:33.069
terms of sort of the depth and breadth
of impact that these sort of

00:36:33.069 --> 00:36:33.079
of impact that these sort of
 

00:36:33.079 --> 00:36:35.410
of impact that these sort of
architectures have had across a variety

00:36:35.410 --> 00:36:35.420
architectures have had across a variety
 

00:36:35.420 --> 00:36:39.099
architectures have had across a variety
of fields I'd like to first appreciate

00:36:39.099 --> 00:36:39.109
of fields I'd like to first appreciate
 

00:36:39.109 --> 00:36:41.710
of fields I'd like to first appreciate
the fact that these advances would not

00:36:41.710 --> 00:36:41.720
the fact that these advances would not
 

00:36:41.720 --> 00:36:45.609
the fact that these advances would not
have been possible without the curation

00:36:45.609 --> 00:36:45.619
have been possible without the curation
 

00:36:45.619 --> 00:36:48.460
have been possible without the curation
and availability of large well annotated

00:36:48.460 --> 00:36:48.470
and availability of large well annotated
 

00:36:48.470 --> 00:36:52.030
and availability of large well annotated
image datasets and this is what has been

00:36:52.030 --> 00:36:52.040
image datasets and this is what has been
 

00:36:52.040 --> 00:36:54.309
image datasets and this is what has been
fundamental to really rapidly

00:36:54.309 --> 00:36:54.319
fundamental to really rapidly
 

00:36:54.319 --> 00:36:56.710
fundamental to really rapidly
accelerating the progress in the

00:36:56.710 --> 00:36:56.720
accelerating the progress in the
 

00:36:56.720 --> 00:36:58.870
accelerating the progress in the
development of convolutional neural

00:36:58.870 --> 00:36:58.880
development of convolutional neural
 

00:36:58.880 --> 00:37:01.290
development of convolutional neural
networks and so some really famous

00:37:01.290 --> 00:37:01.300
networks and so some really famous
 

00:37:01.300 --> 00:37:04.180
networks and so some really famous
examples of image data sets are shown

00:37:04.180 --> 00:37:04.190
examples of image data sets are shown
 

00:37:04.190 --> 00:37:08.920
examples of image data sets are shown
here amnesty in today's lab image net

00:37:08.920 --> 00:37:08.930
here amnesty in today's lab image net
 

00:37:08.930 --> 00:37:12.540
here amnesty in today's lab image net
which I already mentioned and the places

00:37:12.540 --> 00:37:12.550
which I already mentioned and the places
 

00:37:12.550 --> 00:37:15.010
which I already mentioned and the places
data set which is out of MIT of

00:37:15.010 --> 00:37:15.020
data set which is out of MIT of
 

00:37:15.020 --> 00:37:19.510
data set which is out of MIT of
different scenes and landscapes and as I

00:37:19.510 --> 00:37:19.520
different scenes and landscapes and as I
 

00:37:19.520 --> 00:37:21.940
different scenes and landscapes and as I
as I sort of alluded to the impact of

00:37:21.940 --> 00:37:21.950
as I sort of alluded to the impact of
 

00:37:21.950 --> 00:37:24.670
as I sort of alluded to the impact of
these sorts of approaches has been

00:37:24.670 --> 00:37:24.680
these sorts of approaches has been
 

00:37:24.680 --> 00:37:28.359
these sorts of approaches has been
extremely far-reaching and and deep no

00:37:28.359 --> 00:37:28.369
extremely far-reaching and and deep no
 

00:37:28.369 --> 00:37:31.630
extremely far-reaching and and deep no
pun intended

00:37:31.630 --> 00:37:31.640
 
 

00:37:31.640 --> 00:37:33.850
 
and one area that convolutional neural

00:37:33.850 --> 00:37:33.860
and one area that convolutional neural
 

00:37:33.860 --> 00:37:35.830
and one area that convolutional neural
networks have been have made a really

00:37:35.830 --> 00:37:35.840
networks have been have made a really
 

00:37:35.840 --> 00:37:37.900
networks have been have made a really
big impact is in face detection and

00:37:37.900 --> 00:37:37.910
big impact is in face detection and
 

00:37:37.910 --> 00:37:39.670
big impact is in face detection and
recognition software and this is you

00:37:39.670 --> 00:37:39.680
recognition software and this is you
 

00:37:39.680 --> 00:37:41.110
recognition software and this is you
know every time you pick up your phone

00:37:41.110 --> 00:37:41.120
know every time you pick up your phone
 

00:37:41.120 --> 00:37:43.540
know every time you pick up your phone
this your your phone is running these

00:37:43.540 --> 00:37:43.550
this your your phone is running these
 

00:37:43.550 --> 00:37:45.640
this your your phone is running these
sorts of algorithms to pick up you know

00:37:45.640 --> 00:37:45.650
sorts of algorithms to pick up you know
 

00:37:45.650 --> 00:37:49.780
sorts of algorithms to pick up you know
your face and your friends face and this

00:37:49.780 --> 00:37:49.790
your face and your friends face and this
 

00:37:49.790 --> 00:37:51.220
your face and your friends face and this
type of software is pretty much

00:37:51.220 --> 00:37:51.230
type of software is pretty much
 

00:37:51.230 --> 00:37:53.680
type of software is pretty much
everywhere from social media to security

00:37:53.680 --> 00:37:53.690
everywhere from social media to security
 

00:37:53.690 --> 00:37:56.620
everywhere from social media to security
and in today's lab you'll have the

00:37:56.620 --> 00:37:56.630
and in today's lab you'll have the
 

00:37:56.630 --> 00:37:59.650
and in today's lab you'll have the
chance to build a CNN based architecture

00:37:59.650 --> 00:37:59.660
chance to build a CNN based architecture
 

00:37:59.660 --> 00:38:01.750
chance to build a CNN based architecture
for facial detection and you'll actually

00:38:01.750 --> 00:38:01.760
for facial detection and you'll actually
 

00:38:01.760 --> 00:38:04.230
for facial detection and you'll actually
take this a step further by exploring

00:38:04.230 --> 00:38:04.240
take this a step further by exploring
 

00:38:04.240 --> 00:38:06.790
take this a step further by exploring
how these how these models can be

00:38:06.790 --> 00:38:06.800
how these how these models can be
 

00:38:06.800 --> 00:38:09.580
how these how these models can be
potentially biased based on the nature

00:38:09.580 --> 00:38:09.590
potentially biased based on the nature
 

00:38:09.590 --> 00:38:12.570
potentially biased based on the nature
of the training data that they use

00:38:12.570 --> 00:38:12.580
of the training data that they use
 

00:38:12.580 --> 00:38:15.280
of the training data that they use
another application area that has led to

00:38:15.280 --> 00:38:15.290
another application area that has led to
 

00:38:15.290 --> 00:38:17.470
another application area that has led to
a lot of excitement is in autonomous

00:38:17.470 --> 00:38:17.480
a lot of excitement is in autonomous
 

00:38:17.480 --> 00:38:22.060
a lot of excitement is in autonomous
vehicles and self-driving cars so the

00:38:22.060 --> 00:38:22.070
vehicles and self-driving cars so the
 

00:38:22.070 --> 00:38:24.160
vehicles and self-driving cars so the
man in this video was a guest lecturer

00:38:24.160 --> 00:38:24.170
man in this video was a guest lecturer
 

00:38:24.170 --> 00:38:26.740
man in this video was a guest lecturer
that we had last year and he was really

00:38:26.740 --> 00:38:26.750
that we had last year and he was really
 

00:38:26.750 --> 00:38:30.130
that we had last year and he was really
fun and dynamic and this is work from

00:38:30.130 --> 00:38:30.140
fun and dynamic and this is work from
 

00:38:30.140 --> 00:38:32.230
fun and dynamic and this is work from
Nvidia where they have this pipeline

00:38:32.230 --> 00:38:32.240
Nvidia where they have this pipeline
 

00:38:32.240 --> 00:38:34.780
Nvidia where they have this pipeline
where they take a single image from a

00:38:34.780 --> 00:38:34.790
where they take a single image from a
 

00:38:34.790 --> 00:38:37.840
where they take a single image from a
camera on the car feed it into a CNN

00:38:37.840 --> 00:38:37.850
camera on the car feed it into a CNN
 

00:38:37.850 --> 00:38:40.930
camera on the car feed it into a CNN
that directly outputs a single number

00:38:40.930 --> 00:38:40.940
that directly outputs a single number
 

00:38:40.940 --> 00:38:42.760
that directly outputs a single number
which is a predicted steering wheel

00:38:42.760 --> 00:38:42.770
which is a predicted steering wheel
 

00:38:42.770 --> 00:38:46.120
which is a predicted steering wheel
angle and Beyond self-driving cars

00:38:46.120 --> 00:38:46.130
angle and Beyond self-driving cars
 

00:38:46.130 --> 00:38:48.520
angle and Beyond self-driving cars
NVIDIA has a really large-scale research

00:38:48.520 --> 00:38:48.530
NVIDIA has a really large-scale research
 

00:38:48.530 --> 00:38:50.920
NVIDIA has a really large-scale research
effort that's focused on computer vision

00:38:50.920 --> 00:38:50.930
effort that's focused on computer vision
 

00:38:50.930 --> 00:38:54.280
effort that's focused on computer vision
and on Friday we'll hear from the leader

00:38:54.280 --> 00:38:54.290
and on Friday we'll hear from the leader
 

00:38:54.290 --> 00:38:56.830
and on Friday we'll hear from the leader
of Nvidia's entire computer vision team

00:38:56.830 --> 00:38:56.840
of Nvidia's entire computer vision team
 

00:38:56.840 --> 00:38:59.890
of Nvidia's entire computer vision team
and he'll talk about some of the latest

00:38:59.890 --> 00:38:59.900
and he'll talk about some of the latest
 

00:38:59.900 --> 00:39:02.350
and he'll talk about some of the latest
and greatest research that they're doing

00:39:02.350 --> 00:39:02.360
and greatest research that they're doing
 

00:39:02.360 --> 00:39:06.160
and greatest research that they're doing
there finally there's been a pretty

00:39:06.160 --> 00:39:06.170
there finally there's been a pretty
 

00:39:06.170 --> 00:39:08.590
there finally there's been a pretty
significant impact of of these types of

00:39:08.590 --> 00:39:08.600
significant impact of of these types of
 

00:39:08.600 --> 00:39:10.420
significant impact of of these types of
architectures in medicine and healthcare

00:39:10.420 --> 00:39:10.430
architectures in medicine and healthcare
 

00:39:10.430 --> 00:39:12.700
architectures in medicine and healthcare
where deep learning models are being

00:39:12.700 --> 00:39:12.710
where deep learning models are being
 

00:39:12.710 --> 00:39:14.950
where deep learning models are being
applied to the analysis of a whole host

00:39:14.950 --> 00:39:14.960
applied to the analysis of a whole host
 

00:39:14.960 --> 00:39:18.430
applied to the analysis of a whole host
of types of medical images so this is a

00:39:18.430 --> 00:39:18.440
of types of medical images so this is a
 

00:39:18.440 --> 00:39:20.440
of types of medical images so this is a
paper from Nature Medicine from just a

00:39:20.440 --> 00:39:20.450
paper from Nature Medicine from just a
 

00:39:20.450 --> 00:39:23.080
paper from Nature Medicine from just a
few weeks ago where it was a multi

00:39:23.080 --> 00:39:23.090
few weeks ago where it was a multi
 

00:39:23.090 --> 00:39:26.230
few weeks ago where it was a multi
Institute team and they presented a CNN

00:39:26.230 --> 00:39:26.240
Institute team and they presented a CNN
 

00:39:26.240 --> 00:39:29.430
Institute team and they presented a CNN
that uses a pretty standard architecture

00:39:29.430 --> 00:39:29.440
that uses a pretty standard architecture
 

00:39:29.440 --> 00:39:33.330
that uses a pretty standard architecture
to identify rare genetic conditions from

00:39:33.330 --> 00:39:33.340
to identify rare genetic conditions from
 

00:39:33.340 --> 00:39:36.340
to identify rare genetic conditions from
analysis of just a picture of a child's

00:39:36.340 --> 00:39:36.350
analysis of just a picture of a child's
 

00:39:36.350 --> 00:39:39.310
analysis of just a picture of a child's
face and in their paper they report that

00:39:39.310 --> 00:39:39.320
face and in their paper they report that
 

00:39:39.320 --> 00:39:41.290
face and in their paper they report that
their model can actually outperform

00:39:41.290 --> 00:39:41.300
their model can actually outperform
 

00:39:41.300 --> 00:39:43.190
their model can actually outperform
physicians when Tess

00:39:43.190 --> 00:39:43.200
physicians when Tess
 

00:39:43.200 --> 00:39:45.530
physicians when Tess
on a set of images that are would be

00:39:45.530 --> 00:39:45.540
on a set of images that are would be
 

00:39:45.540 --> 00:39:49.970
on a set of images that are would be
relevant to a clinical scenario and one

00:39:49.970 --> 00:39:49.980
relevant to a clinical scenario and one
 

00:39:49.980 --> 00:39:51.770
relevant to a clinical scenario and one
reason that work like this is really

00:39:51.770 --> 00:39:51.780
reason that work like this is really
 

00:39:51.780 --> 00:39:54.140
reason that work like this is really
exciting is because it presents sort of

00:39:54.140 --> 00:39:54.150
exciting is because it presents sort of
 

00:39:54.150 --> 00:39:56.329
exciting is because it presents sort of
another standard approach standardized

00:39:56.329 --> 00:39:56.339
another standard approach standardized
 

00:39:56.339 --> 00:39:58.970
another standard approach standardized
approach to identifying and diagnosing

00:39:58.970 --> 00:39:58.980
approach to identifying and diagnosing
 

00:39:58.980 --> 00:40:02.180
approach to identifying and diagnosing
in this case genetic disorders and you

00:40:02.180 --> 00:40:02.190
in this case genetic disorders and you
 

00:40:02.190 --> 00:40:03.829
in this case genetic disorders and you
can imagine that this could be combined

00:40:03.829 --> 00:40:03.839
can imagine that this could be combined
 

00:40:03.839 --> 00:40:07.010
can imagine that this could be combined
with already existing clinical tests to

00:40:07.010 --> 00:40:07.020
with already existing clinical tests to
 

00:40:07.020 --> 00:40:11.980
with already existing clinical tests to
improve classification or subtyping

00:40:11.980 --> 00:40:11.990
improve classification or subtyping
 

00:40:11.990 --> 00:40:15.109
improve classification or subtyping
alright so to summarize what we've

00:40:15.109 --> 00:40:15.119
alright so to summarize what we've
 

00:40:15.119 --> 00:40:17.780
alright so to summarize what we've
covered in today's lecture we first

00:40:17.780 --> 00:40:17.790
covered in today's lecture we first
 

00:40:17.790 --> 00:40:20.089
covered in today's lecture we first
considered sort of the origins of the

00:40:20.089 --> 00:40:20.099
considered sort of the origins of the
 

00:40:20.099 --> 00:40:22.430
considered sort of the origins of the
computer vision problem and how we can

00:40:22.430 --> 00:40:22.440
computer vision problem and how we can
 

00:40:22.440 --> 00:40:25.579
computer vision problem and how we can
represent images as arrays of brightness

00:40:25.579 --> 00:40:25.589
represent images as arrays of brightness
 

00:40:25.589 --> 00:40:28.640
represent images as arrays of brightness
values and what convolutions are and how

00:40:28.640 --> 00:40:28.650
values and what convolutions are and how
 

00:40:28.650 --> 00:40:31.250
values and what convolutions are and how
they work and we then discussed the

00:40:31.250 --> 00:40:31.260
they work and we then discussed the
 

00:40:31.260 --> 00:40:33.890
they work and we then discussed the
basic architecture of convolutional

00:40:33.890 --> 00:40:33.900
basic architecture of convolutional
 

00:40:33.900 --> 00:40:36.530
basic architecture of convolutional
neural networks and kind of went in

00:40:36.530 --> 00:40:36.540
neural networks and kind of went in
 

00:40:36.540 --> 00:40:38.599
neural networks and kind of went in
depth on how cnn's can be used for

00:40:38.599 --> 00:40:38.609
depth on how cnn's can be used for
 

00:40:38.609 --> 00:40:40.970
depth on how cnn's can be used for
classification and finally we talked a

00:40:40.970 --> 00:40:40.980
classification and finally we talked a
 

00:40:40.980 --> 00:40:42.559
classification and finally we talked a
bit about extensions and the

00:40:42.559 --> 00:40:42.569
bit about extensions and the
 

00:40:42.569 --> 00:40:44.720
bit about extensions and the
applications of the basic CNN

00:40:44.720 --> 00:40:44.730
applications of the basic CNN
 

00:40:44.730 --> 00:40:47.809
applications of the basic CNN
architecture and why they have been so

00:40:47.809 --> 00:40:47.819
architecture and why they have been so
 

00:40:47.819 --> 00:40:52.460
architecture and why they have been so
impactful over the past several years so

00:40:52.460 --> 00:40:52.470
impactful over the past several years so
 

00:40:52.470 --> 00:40:54.980
impactful over the past several years so
I'm happy to take questions at the end

00:40:54.980 --> 00:40:54.990
I'm happy to take questions at the end
 

00:40:54.990 --> 00:40:57.230
I'm happy to take questions at the end
of the at the end of the lecture portion

00:40:57.230 --> 00:40:57.240
of the at the end of the lecture portion
 

00:40:57.240 --> 00:41:00.859
of the at the end of the lecture portion
you feel free feel free to come to the

00:41:00.859 --> 00:41:00.869
you feel free feel free to come to the
 

00:41:00.869 --> 00:41:04.940
you feel free feel free to come to the
front to speak to Alexander or myself so

00:41:04.940 --> 00:41:04.950
front to speak to Alexander or myself so
 

00:41:04.950 --> 00:41:07.160
front to speak to Alexander or myself so
with that I'm going to hand it off to

00:41:07.160 --> 00:41:07.170
with that I'm going to hand it off to
 

00:41:07.170 --> 00:41:12.580
with that I'm going to hand it off to
him for the second lecture of today

00:41:12.580 --> 00:41:12.590
him for the second lecture of today
 

00:41:12.590 --> 00:41:18.050
 
[Applause]

