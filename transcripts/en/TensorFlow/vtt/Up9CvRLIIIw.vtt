WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.964
[MUSIC PLAYING]

00:00:08.347 --> 00:00:10.700
ALEXANDRE PASSOS: I'm
Alex, and I'm here

00:00:10.700 --> 00:00:13.580
to tell you about how
you're going to build graphs

00:00:13.580 --> 00:00:15.350
in TensorFlow 2.0.

00:00:15.350 --> 00:00:17.750
And this might make you
a little uncomfortable,

00:00:17.750 --> 00:00:20.390
because we already spent
quite some time earlier today

00:00:20.390 --> 00:00:22.570
telling you that
in TensorFlow 2.0,

00:00:22.570 --> 00:00:24.870
we use eager
execution by default.

00:00:24.870 --> 00:00:26.840
So why am I taking
that away from you?

00:00:26.840 --> 00:00:28.160
And I'm not.

00:00:28.160 --> 00:00:30.230
You still have your eager
execution by default,

00:00:30.230 --> 00:00:33.530
but graphs are useful
for quite a few things.

00:00:33.530 --> 00:00:35.990
The two ones that I care
the most about personally

00:00:35.990 --> 00:00:39.440
are that some hardware,
like our TPUs,

00:00:39.440 --> 00:00:42.350
really benefit from the kind
of full program optimization

00:00:42.350 --> 00:00:44.600
that we can get
if we have graphs.

00:00:44.600 --> 00:00:47.060
And if you have graphs,
you can take your model

00:00:47.060 --> 00:00:50.240
and deploy it on servers and
deploy it on mobile devices

00:00:50.240 --> 00:00:52.670
and deploy it on
whatever thing you want,

00:00:52.670 --> 00:00:56.490
make it available to as many
people as you can think of.

00:00:56.490 --> 00:00:59.540
So at this point,
you're probably, eh, I

00:00:59.540 --> 00:01:01.210
remember TensorFlow 1.0.

00:01:01.210 --> 00:01:04.370
I remember the kind of code
I had to write to use graphs,

00:01:04.370 --> 00:01:06.710
and I wasn't proud of it.

00:01:06.710 --> 00:01:09.610
Is he just going to tell me
that I have to keep doing that?

00:01:09.610 --> 00:01:11.303
And no.

00:01:11.303 --> 00:01:13.720
One of the biggest changes
we're doing with TensorFlow 2.0

00:01:13.720 --> 00:01:15.678
is we're fundamentally
changing the programming

00:01:15.678 --> 00:01:18.830
model with which you build
graphs in TensorFlow.

00:01:18.830 --> 00:01:21.200
We're removing that
model where you first add

00:01:21.200 --> 00:01:24.955
a bunch of nodes to a graph
and then rely on session.run

00:01:24.955 --> 00:01:26.330
to prune things
out of the graph,

00:01:26.330 --> 00:01:28.038
to figure out the
precise things you want

00:01:28.038 --> 00:01:30.170
to run in the correct
order and replacing it

00:01:30.170 --> 00:01:34.380
with a much simpler model based
on this notion of a function.

00:01:34.380 --> 00:01:37.710
We're calling it tf.function,
because that's the main API

00:01:37.710 --> 00:01:39.790
entry point for you to use it.

00:01:39.790 --> 00:01:42.090
And I'm here to tell you
that with tf.function,

00:01:42.090 --> 00:01:44.810
many things that you're used
to are going to go away.

00:01:44.810 --> 00:01:47.190
And I dearly hope you're
not going to miss them.

00:01:47.190 --> 00:01:49.140
The first one that
goes away that I really

00:01:49.140 --> 00:01:51.270
think no one in this
room is going to miss

00:01:51.270 --> 00:01:55.254
is that you'll never have
to use session.run anymore.

00:01:55.254 --> 00:01:56.658
[APPLAUSE]

00:01:59.470 --> 00:02:01.620
So if you've TensorFlow
with eager execution,

00:02:01.620 --> 00:02:02.585
you know how it works.

00:02:02.585 --> 00:02:04.710
You have your tensors and
you have your operations,

00:02:04.710 --> 00:02:06.970
and you pass your tensors
to your operations,

00:02:06.970 --> 00:02:08.639
and the operations execute.

00:02:08.639 --> 00:02:11.350
And this is all very
simple and straightforward.

00:02:11.350 --> 00:02:15.540
And tf.function is just
like an operation except one

00:02:15.540 --> 00:02:18.600
that you get to define
using a composition

00:02:18.600 --> 00:02:21.810
of the other operations in
TensorFlow however you wish.

00:02:21.810 --> 00:02:24.330
Once you have your
tf.function, you can call it.

00:02:24.330 --> 00:02:26.340
You can call it inside
another function.

00:02:26.340 --> 00:02:27.750
You can take its gradient.

00:02:27.750 --> 00:02:31.230
You can run it on the GPU,
on the TPU, on the CPU,

00:02:31.230 --> 00:02:33.120
on distributed
things, just like how

00:02:33.120 --> 00:02:35.922
you would do with any
other TensorFlow operation.

00:02:35.922 --> 00:02:38.130
So really, the way you should
think about tf.function

00:02:38.130 --> 00:02:40.790
is we're letting you define
your own operations in Python

00:02:40.790 --> 00:02:43.540
and making it as easy as
possible for you to do this

00:02:43.540 --> 00:02:46.980
and trying to preserve as many
of the semantics of the Python

00:02:46.980 --> 00:02:49.590
programming language that
you already know and love

00:02:49.590 --> 00:02:53.390
and when you execute these
functions in a graph.

00:02:53.390 --> 00:02:56.980
So obviously, the first
thing you would think

00:02:56.980 --> 00:02:59.150
is that is it actually faster?

00:02:59.150 --> 00:03:00.940
And if you look
at models that are

00:03:00.940 --> 00:03:03.470
large convolutions or big
matrix multiplications,

00:03:03.470 --> 00:03:05.470
large reductions, it's
not actually any faster,

00:03:05.470 --> 00:03:07.810
because you get
executions plenty fast.

00:03:07.810 --> 00:03:10.390
But as your models get small,
and as the operations in them

00:03:10.390 --> 00:03:12.070
get small, you can
actually measure

00:03:12.070 --> 00:03:13.320
the difference in performance.

00:03:13.320 --> 00:03:16.770
And here, I show that for this
tiny lstm_cell with 10 units,

00:03:16.770 --> 00:03:18.840
there is actually
a tenfold speed up

00:03:18.840 --> 00:03:21.580
if we used tf.function versus
if you don't use tf.function

00:03:21.580 --> 00:03:23.170
to execute it.

00:03:23.170 --> 00:03:27.020
And as I was saying, we
really try to preserve as much

00:03:27.020 --> 00:03:28.910
of the Python
semantics as we can

00:03:28.910 --> 00:03:30.200
to make this code easy to use.

00:03:30.200 --> 00:03:31.880
So if you've seen
TensorFlow graphs,

00:03:31.880 --> 00:03:34.490
you know that they are
very much not polymorphic.

00:03:34.490 --> 00:03:36.410
If you built a
graph for float64,

00:03:36.410 --> 00:03:39.620
you cannot use it for float32
or, God forbid, float16.

00:03:39.620 --> 00:03:42.950
But tf.function--
but Python code

00:03:42.950 --> 00:03:46.340
tends to be very free into the
types of things it accepts.

00:03:46.340 --> 00:03:48.410
With tf.function,
we do the same.

00:03:48.410 --> 00:03:52.080
So under the hood, when
you call a tf.function,

00:03:52.080 --> 00:03:54.622
we look at the tensors
you're passing as inputs

00:03:54.622 --> 00:03:56.330
and then try to see,
have we already made

00:03:56.330 --> 00:03:59.600
a function graph that is
compatible with those inputs?

00:03:59.600 --> 00:04:01.490
If not, we make a new one.

00:04:01.490 --> 00:04:03.530
And we hide this from
you so that you can just

00:04:03.530 --> 00:04:08.150
use your tf.function as you
would use normal TensorFlow

00:04:08.150 --> 00:04:08.990
operation.

00:04:08.990 --> 00:04:11.490
And eventually, you'll get all
the graphs you need built up,

00:04:11.490 --> 00:04:13.250
and your code will
run blazingly fast.

00:04:13.250 --> 00:04:15.482
And this is not
completely hidden.

00:04:15.482 --> 00:04:17.149
If you want to have
access to the graphs

00:04:17.149 --> 00:04:19.279
that we're generating,
you can get them.

00:04:19.279 --> 00:04:20.260
We expose them to you.

00:04:20.260 --> 00:04:23.000
So if you need to manipulate
these graphs somehow or do

00:04:23.000 --> 00:04:24.950
weird things to them
that I do not approve,

00:04:24.950 --> 00:04:28.190
you can still do it.

00:04:28.190 --> 00:04:31.370
But really, the main reason why
we changed this model is not

00:04:31.370 --> 00:04:34.760
to replace session.run
with tf.function,

00:04:34.760 --> 00:04:38.180
it's that by changing
the promise for what

00:04:38.180 --> 00:04:41.030
we do to your code, we can
do so much more for you

00:04:41.030 --> 00:04:42.030
than we could do before.

00:04:42.030 --> 00:04:44.363
With the model where you add
a bunch of notes to a graph

00:04:44.363 --> 00:04:47.210
and then prune them, it's very
hard for the TensorFlow runtime

00:04:47.210 --> 00:04:50.630
to know what order do you want
those operations to be executed

00:04:50.630 --> 00:04:51.170
in.

00:04:51.170 --> 00:04:53.390
Almost every TensorFlow
operation is stateless

00:04:53.390 --> 00:04:54.540
so that doesn't matter.

00:04:54.540 --> 00:04:56.410
But for the few ones
where it does matter,

00:04:56.410 --> 00:04:58.340
you probably had to use
control dependencies

00:04:58.340 --> 00:05:00.560
and other complicated
things to make it work.

00:05:00.560 --> 00:05:02.750
So again, I'm here to tell
you that you will never

00:05:02.750 --> 00:05:05.270
have to use control
dependencies again

00:05:05.270 --> 00:05:08.000
if you're using tf.function.

00:05:08.000 --> 00:05:12.100
And how can I make
this claim happen?

00:05:12.100 --> 00:05:14.680
So the premise
behind tf.function

00:05:14.680 --> 00:05:17.310
is that you write code that
you'd like to run eagerly,

00:05:17.310 --> 00:05:19.600
we take it and we make it fast.

00:05:19.600 --> 00:05:24.400
So as we trace your Python
code to generate a graph,

00:05:24.400 --> 00:05:26.800
we look at the
operations you run,

00:05:26.800 --> 00:05:28.870
and every time we see
a stateful operation,

00:05:28.870 --> 00:05:32.740
we add the minimum necessary
set of control dependencies

00:05:32.740 --> 00:05:34.840
to ensure that all
the resources accessed

00:05:34.840 --> 00:05:38.230
by those stateful operations
are accessed in the order you

00:05:38.230 --> 00:05:39.160
want them to be.

00:05:39.160 --> 00:05:41.410
So if you have two variables
and you're updating them,

00:05:41.410 --> 00:05:42.580
we'll do that in parallel.

00:05:42.580 --> 00:05:45.122
When you have one variable and
you're updating it many times,

00:05:45.122 --> 00:05:47.110
we'll order those updates
so that you're not

00:05:47.110 --> 00:05:49.780
surprised by them happening
out of order or something

00:05:49.780 --> 00:05:51.440
like that.

00:05:51.440 --> 00:05:53.890
So there's really
no crazy surprises

00:05:53.890 --> 00:05:55.990
and weird, undefined behavior.

00:05:55.990 --> 00:05:58.120
And really, you
should never need

00:05:58.120 --> 00:06:01.540
to explicitly add control
dependencies to your code.

00:06:01.540 --> 00:06:04.180
But you'll still get
the ability of knowing

00:06:04.180 --> 00:06:05.360
what order things execute.

00:06:05.360 --> 00:06:07.777
And if you want something to
execute before somebody else,

00:06:07.777 --> 00:06:10.420
just put that line of code
above that other line of code.

00:06:10.420 --> 00:06:13.787
You know, how you do
in a normal program.

00:06:13.787 --> 00:06:15.370
Another thing that
we can dramatically

00:06:15.370 --> 00:06:18.160
simplify in
tf.function is how you

00:06:18.160 --> 00:06:20.440
use variables in TensorFlow.

00:06:20.440 --> 00:06:24.130
And I'm sure you've all
used variables before.

00:06:24.130 --> 00:06:26.110
And you know that while
they're very useful--

00:06:26.110 --> 00:06:27.670
they allow you to share
state across devices,

00:06:27.670 --> 00:06:29.045
they let you
persist, checkpoint,

00:06:29.045 --> 00:06:31.510
do all those things, it
can be a little finicky.

00:06:31.510 --> 00:06:34.800
Things like initializing
them is very hard, especially

00:06:34.800 --> 00:06:36.550
if you're using your
variables of any kind

00:06:36.550 --> 00:06:38.560
of non-trivial initialization.

00:06:38.560 --> 00:06:41.170
So another thing that we're
removing from TensorFlow

00:06:41.170 --> 00:06:45.100
is the need to manually
initialize variables yourself.

00:06:45.100 --> 00:06:48.540
And the story for variables
is a little complicated,

00:06:48.540 --> 00:06:50.160
though, because
as you try to make

00:06:50.160 --> 00:06:53.790
code compatible with both eager
execution and graph semantics,

00:06:53.790 --> 00:06:57.000
you very quickly
find examples where

00:06:57.000 --> 00:06:58.890
it's unclear what we should do.

00:06:58.890 --> 00:07:00.700
My favorite one is this one--

00:07:00.700 --> 00:07:03.300
if you run this code
in TensorFlow 1.x,

00:07:03.300 --> 00:07:05.940
and you session.run
repeatedly, the result,

00:07:05.940 --> 00:07:09.085
you're going to get a series
of numbers that goes up.

00:07:09.085 --> 00:07:10.460
But if you run
this code eagerly,

00:07:10.460 --> 00:07:11.840
every time you run
it, you're going

00:07:11.840 --> 00:07:13.590
to get the same number
back, because we're

00:07:13.590 --> 00:07:15.680
creating a new variable,
updating it, and then

00:07:15.680 --> 00:07:17.220
destroying it.

00:07:17.220 --> 00:07:19.350
So if I wanted to
turn this code-- wrap

00:07:19.350 --> 00:07:21.740
this code with tf.function--
which one should it do?

00:07:21.740 --> 00:07:24.720
Should it follow the 1.x
behavior or the eager behavior?

00:07:24.720 --> 00:07:26.990
And I think if I took a
poll, I would probably

00:07:26.990 --> 00:07:28.750
find that you don't
agree with each other.

00:07:28.750 --> 00:07:32.130
I don't agree with myself,
so this is an error.

00:07:32.130 --> 00:07:33.800
Nonambiguous at
creating variables,

00:07:33.800 --> 00:07:35.360
though, is perfectly OK.

00:07:35.360 --> 00:07:37.150
So as you've seen
in an earlier slide,

00:07:37.150 --> 00:07:38.900
you can create the
variable and capture it

00:07:38.900 --> 00:07:40.580
by closure in a function.

00:07:40.580 --> 00:07:42.740
That's a way a lot of
TensorFlow code gets written.

00:07:42.740 --> 00:07:44.120
This just works.

00:07:44.120 --> 00:07:47.270
Another way you can do is
like write your function such

00:07:47.270 --> 00:07:49.850
that it only creates variables
the first time it's called.

00:07:49.850 --> 00:07:52.940
This is incidentally what
most libraries in TensorFlow

00:07:52.940 --> 00:07:54.002
do under the hood.

00:07:54.002 --> 00:07:55.710
This is how Keras
layers are implemented,

00:07:55.710 --> 00:07:58.050
how the TF 1.x layer
is implemented,

00:07:58.050 --> 00:08:00.440
Sonnet, and all sorts
of other libraries

00:08:00.440 --> 00:08:01.850
that use TensorFlow variables.

00:08:01.850 --> 00:08:03.725
They try to take care
to not create variables

00:08:03.725 --> 00:08:05.780
every time they're
called, otherwise you're

00:08:05.780 --> 00:08:07.550
creating way too many variables,
and you're not actually

00:08:07.550 --> 00:08:08.600
training anything.

00:08:08.600 --> 00:08:12.050
So code that behaves well just
gets turned into function,

00:08:12.050 --> 00:08:13.100
and it's fine.

00:08:13.100 --> 00:08:14.990
And if you've seen
this, I didn't actually

00:08:14.990 --> 00:08:17.090
need to call the initializer
for this variable

00:08:17.090 --> 00:08:19.460
that I'm creating,
and it's even better.

00:08:19.460 --> 00:08:21.680
I can make the
initializer depend

00:08:21.680 --> 00:08:24.020
on the value of the
arguments to the function

00:08:24.020 --> 00:08:26.780
or the value of other
variables in arbitrarily

00:08:26.780 --> 00:08:28.200
complicated ways.

00:08:28.200 --> 00:08:30.750
And because we control--

00:08:30.750 --> 00:08:32.570
we add the necessary
control dependencies

00:08:32.570 --> 00:08:35.549
to ensure that the state
updates happen in the way you

00:08:35.549 --> 00:08:37.039
want them to happen.

00:08:37.039 --> 00:08:38.980
There is no need for
you to worry about this.

00:08:38.980 --> 00:08:40.530
You can just create
your variables,

00:08:40.530 --> 00:08:42.929
like how you would use in a
normal programming language.

00:08:42.929 --> 00:08:46.060
And things will behave the
way you want them to behave.

00:08:46.060 --> 00:08:50.020
Another thing that I'm really
happy about tf.function

00:08:50.020 --> 00:08:52.020
is our autograph integration.

00:08:52.020 --> 00:08:54.340
And if anyone here has used
Control Flow in TensorFlow,

00:08:54.340 --> 00:08:56.962
you probably know that
it can be awkward.

00:08:56.962 --> 00:08:59.170
And I'm really happy to tell
you that with Autograph,

00:08:59.170 --> 00:09:03.750
we're finally breaking up with
tf.cond and tf.while_loop.

00:09:03.750 --> 00:09:06.860
And now, you can just write
code that looks like this--

00:09:06.860 --> 00:09:08.830
so if you see here,
I have a while loop,

00:09:08.830 --> 00:09:11.440
where the predicate depends on
the value of a tf.reduce_sum

00:09:11.440 --> 00:09:12.580
on a tensor.

00:09:12.580 --> 00:09:14.830
This is probably the
worst way to make a tensor

00:09:14.830 --> 00:09:16.490
sum to 1 that I could think of.

00:09:16.490 --> 00:09:18.940
But it fits in a slide.

00:09:18.940 --> 00:09:19.510
So yay.

00:09:22.820 --> 00:09:24.340
If you put this
in a tf.function,

00:09:24.340 --> 00:09:26.370
we'll create a graph
and we'll execute it.

00:09:26.370 --> 00:09:27.120
And this is nice.

00:09:27.120 --> 00:09:27.830
This is great.

00:09:27.830 --> 00:09:30.050
But how does it work?

00:09:30.050 --> 00:09:33.140
Under the hood, things like
tf.cond and tf with our while

00:09:33.140 --> 00:09:36.770
loop are still there, but we
wrote this Python compiler

00:09:36.770 --> 00:09:40.148
called Autograph that rewrites
Control Flow expressions

00:09:40.148 --> 00:09:41.690
into something that
looks like this--

00:09:44.920 --> 00:09:45.420
yes.

00:09:45.420 --> 00:09:47.253
Something that looks
like this, which is not

00:09:47.253 --> 00:09:51.060
like how you would
want to write code.

00:09:51.060 --> 00:09:53.510
And this then can be
taken by TensorFlow

00:09:53.510 --> 00:09:56.670
and turned into fast
dynamic graph code.

00:09:56.670 --> 00:09:59.272
So how does this work?

00:09:59.272 --> 00:10:01.730
To explain that, I like to take
a step back and think about

00:10:01.730 --> 00:10:04.160
how does anything
in TensorFlow work?

00:10:04.160 --> 00:10:05.810
So you can have
a tensor, and you

00:10:05.810 --> 00:10:08.780
can do tensor plus tensor
times other tensor, et cetera.

00:10:08.780 --> 00:10:12.050
Just use a tensor as you'd
use a normal Python integer

00:10:12.050 --> 00:10:13.470
or floating point number.

00:10:13.470 --> 00:10:15.320
And how do we do that?

00:10:15.320 --> 00:10:17.030
I'm sure you all
know this, but Python

00:10:17.030 --> 00:10:19.460
has a thing called
operator overloading that

00:10:19.460 --> 00:10:23.630
lets us change the behavior
of standard Python operators

00:10:23.630 --> 00:10:26.390
when applied on our custom
data types, like tensors.

00:10:26.390 --> 00:10:30.950
So we can override the
__add, __sub, et cetera,

00:10:30.950 --> 00:10:32.630
and change how
TensorFlow does addition,

00:10:32.630 --> 00:10:34.170
subtraction of tensors.

00:10:34.170 --> 00:10:37.890
This is all fine and dandy, but
Python does not let us override

00:10:37.890 --> 00:10:40.290
__if.

00:10:40.290 --> 00:10:42.620
Indeed, that's not an
operator in Python.

00:10:42.620 --> 00:10:46.280
It makes me very sad.

00:10:46.280 --> 00:10:48.690
But if you think about
it for a few seconds,

00:10:48.690 --> 00:10:52.110
you can probably come up with
rewrite rules that would let

00:10:52.110 --> 00:10:56.210
us, like, lower to byte
code that would have __if

00:10:56.210 --> 00:10:57.480
overwritable.

00:10:57.480 --> 00:11:00.420
So for example, if code
looks like this, if condition

00:11:00.420 --> 00:11:03.620
a else b, you could
conceptually write this

00:11:03.620 --> 00:11:06.930
as condition.if a and b.

00:11:06.930 --> 00:11:09.260
You would need to do some
fiddling with the scopes,

00:11:09.260 --> 00:11:12.330
because I'm sure you know that
Python's lexical scoping is not

00:11:12.330 --> 00:11:14.130
really as lexical
as you would think,

00:11:14.130 --> 00:11:17.370
and names can leak
out of scopes.

00:11:17.370 --> 00:11:19.620
And it's kind of a little
messy, but that's also

00:11:19.620 --> 00:11:23.340
a mechanical transformation.

00:11:23.340 --> 00:11:26.400
So if this is potentially a
mechanical transformation,

00:11:26.400 --> 00:11:28.540
let's do this mechanical
transformation.

00:11:28.540 --> 00:11:30.570
So we wrote this Python
to TensorFlow compiler

00:11:30.570 --> 00:11:32.310
called Autograph
that does this--

00:11:32.310 --> 00:11:35.460
it takes your Python code, and
it rewrites it in a form that

00:11:35.460 --> 00:11:41.280
lets us call __if, __while,
et cetera on tensors.

00:11:41.280 --> 00:11:43.500
This is all it
does, but this just

00:11:43.500 --> 00:11:47.460
unlocks a lot of the power
of native Python Control Flow

00:11:47.460 --> 00:11:49.740
into your TensorFlow graphs.

00:11:49.740 --> 00:11:51.060
And you got to choose.

00:11:51.060 --> 00:11:55.570
So for example, on this
function here, I have two loops.

00:11:55.570 --> 00:12:00.120
One, it's a static Python loop,
because I write for i in range.

00:12:00.120 --> 00:12:03.180
I is an integer, because
a range returns integers.

00:12:03.180 --> 00:12:05.860
Autograph sees this,
leaves it untouched.

00:12:05.860 --> 00:12:07.680
So you've still got
to use Python Control

00:12:07.680 --> 00:12:10.470
Flow to choose how many layers
a network's going to have

00:12:10.470 --> 00:12:12.760
and constructing
dynamically or iterate over

00:12:12.760 --> 00:12:14.280
a sequential, et cetera.

00:12:14.280 --> 00:12:16.770
But when your Control
Flow does depend

00:12:16.770 --> 00:12:20.235
on the properties of tensors,
like in the second loop for i

00:12:20.235 --> 00:12:23.880
in tf.range, then Autograph
sees it and turns it

00:12:23.880 --> 00:12:25.955
into a dynamic tf.while loop.

00:12:25.955 --> 00:12:27.330
This means that
you can implement

00:12:27.330 --> 00:12:29.247
something like a dynamic
or an n in TensorFlow

00:12:29.247 --> 00:12:31.490
in 10 lines of
code, just like how

00:12:31.490 --> 00:12:34.870
you would use in a normal
language, which is pretty nice.

00:12:34.870 --> 00:12:38.850
And anything really that you
can do in a TensorFlow graph,

00:12:38.850 --> 00:12:40.658
you can make happen dynamically.

00:12:40.658 --> 00:12:43.200
So you can make your prints and
assertions happen dynamically

00:12:43.200 --> 00:12:44.280
if you want to debug.

00:12:44.280 --> 00:12:46.840
But just use in
tf.print and tf.Assert.

00:12:46.840 --> 00:12:48.360
And notice here
that I don't need

00:12:48.360 --> 00:12:50.100
to add control dependencies
to ensure that they happen

00:12:50.100 --> 00:12:51.630
in the right order,
because of the thing

00:12:51.630 --> 00:12:52.838
that we were talking earlier.

00:12:52.838 --> 00:12:56.080
We already do this, like,
we've tried these control

00:12:56.080 --> 00:12:57.850
dependencies
automatically for you

00:12:57.850 --> 00:13:00.160
to try to really
make your code look

00:13:00.160 --> 00:13:02.860
and behave the same as
Python code would look like.

00:13:02.860 --> 00:13:06.640
But all that we're doing here
is converting Control Flow.

00:13:06.640 --> 00:13:09.040
We're not actually compiling
Python to TensorFlow graph,

00:13:09.040 --> 00:13:11.770
because the TensorFlow runtime
right now is not really

00:13:11.770 --> 00:13:15.050
powerful enough to support
everything that Python can do.

00:13:15.050 --> 00:13:17.050
So for example, if
you're manipulating

00:13:17.050 --> 00:13:19.120
lists of tensors at
runtime, you should still

00:13:19.120 --> 00:13:20.800
use a tensor array.

00:13:20.800 --> 00:13:22.820
It's a perfectly
fine data structure.

00:13:22.820 --> 00:13:23.697
It works very well.

00:13:23.697 --> 00:13:25.780
It compiles down to very
efficient TensorFlow code

00:13:25.780 --> 00:13:28.037
and CPUs, GPUs, TPUs.

00:13:28.037 --> 00:13:30.370
But you no longer need to
write a lot of the boilerplate

00:13:30.370 --> 00:13:31.730
associated with it.

00:13:31.730 --> 00:13:35.890
So this is how you stack a bunch
of tensors together in a loop.

00:13:35.890 --> 00:13:38.640
So wrapping up, I
think we've changed

00:13:38.640 --> 00:13:41.730
a lot in TF 2.0,
how we build graphs,

00:13:41.730 --> 00:13:42.730
how we use those graphs.

00:13:42.730 --> 00:13:44.855
And I think you'll all
agree that these changes are

00:13:44.855 --> 00:13:45.355
very big.

00:13:45.355 --> 00:13:47.605
But I hope you'll agree with
me that those changes are

00:13:47.605 --> 00:13:48.340
worth it.

00:13:48.340 --> 00:13:50.363
And I'll just quickly
walk you through a diff

00:13:50.363 --> 00:13:51.780
of what your code
is going to look

00:13:51.780 --> 00:13:53.550
like before and after this.

00:13:53.550 --> 00:13:57.420
So session.run goes away.

00:13:57.420 --> 00:14:01.020
Control dependencies go away.

00:14:01.020 --> 00:14:03.630
Variable initialization
goes away.

00:14:03.630 --> 00:14:07.980
Combed and while loop go away,
and you just use functions,

00:14:07.980 --> 00:14:10.690
like how you would use in a
normal programming language.

00:14:10.690 --> 00:14:14.604
So thank you, and
welcome to TF 2.0.

00:14:14.604 --> 00:14:16.098
[APPLAUSE]

00:14:18.590 --> 00:14:20.630
All the examples on
these slides, they run.

00:14:20.630 --> 00:14:22.630
If you go on
tensorflow.org/alpha and you

00:14:22.630 --> 00:14:25.130
dig it a little, you'll find a
colab notebook that has these

00:14:25.130 --> 00:14:26.990
and a lot more, which will
play around with tf.function

00:14:26.990 --> 00:14:27.980
and Autograph.

00:14:27.980 --> 00:14:30.430
[MUSIC PLAYING]

