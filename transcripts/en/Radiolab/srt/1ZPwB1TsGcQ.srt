1
00:00:00,370 --> 00:00:03,550
Hey, I'm Janet. Boom, rod. I am
Robert Krulwich. This is Radiolab.

2
00:00:03,580 --> 00:00:06,760
And this next segment began
with a simple question.

3
00:00:07,210 --> 00:00:09,490
Seeing as our topics
before has been limits.

4
00:00:09,491 --> 00:00:13,000
And sure we'd done body when we did the
brain. Oh, we're going to go really big.

5
00:00:13,060 --> 00:00:17,380
Yeah. Yeah. So we called up
Steve Strogatz, mathematician
at Cornell University,

6
00:00:17,410 --> 00:00:19,690
frequent guest on the
show. And we asked him,

7
00:00:19,870 --> 00:00:23,080
are there limits to human knowledge? Yeah.

8
00:00:23,480 --> 00:00:27,370
And his answer sent us on a
little adventure. Um, yeah.

9
00:00:27,730 --> 00:00:31,930
Is there anything that's at the limits
of our knowledge is a question that a lot

10
00:00:31,931 --> 00:00:34,480
of us scientists worry about. And, and uh,

11
00:00:35,200 --> 00:00:38,650
certainly the 20th century taught us
that there are many things that limit our

12
00:00:38,651 --> 00:00:39,700
knowledge. For instance,

13
00:00:39,701 --> 00:00:44,020
that the Heisenberg uncertainty principle
in quantum physics showed us that you

14
00:00:44,021 --> 00:00:48,550
can't know the position and momentum of
a subatomic particle at the same time.

15
00:00:48,551 --> 00:00:49,480
You just can't do it.

16
00:00:49,930 --> 00:00:53,380
It's not a matter of not having good
enough instruments or not being clever

17
00:00:53,381 --> 00:00:57,460
enough. It's just a fundamental
barrier that nature puts in your way.

18
00:00:58,420 --> 00:01:02,890
Um, in logic girdles theorem tells us you
can't prove certain things even though

19
00:01:02,891 --> 00:01:05,290
they're true. So we, there
are all kinds of limits,

20
00:01:05,291 --> 00:01:08,350
but those seem a bit remote
from everyday experience.

21
00:01:08,920 --> 00:01:12,130
And yet I think there are really important
limits on our knowledge that we're

22
00:01:12,131 --> 00:01:13,060
all familiar with.

23
00:01:13,160 --> 00:01:18,160
What I'm thinking of here is our inability
to think about big numbers because

24
00:01:18,881 --> 00:01:23,260
with your fingers you've got 10 you
know, normally. So we're good at 10,

25
00:01:23,630 --> 00:01:28,460
we're barely good at a hundred and
once you start getting to thousands,

26
00:01:28,460 --> 00:01:30,920
millions, billions and trillions,
it gets hazier and hazier.

27
00:01:30,980 --> 00:01:34,580
When you hear now about the trillions
of dollars in the deficit or whatever it

28
00:01:34,581 --> 00:01:38,060
is, the debt, you know, we
don't, that means nothing.

29
00:01:38,100 --> 00:01:42,410
How are you supposed to think
about that? Now when you ask,

30
00:01:42,411 --> 00:01:46,070
why can't we understand the common cold?
But we can put a person on the moon.

31
00:01:46,940 --> 00:01:51,290
It has to do with large numbers, not
just large numbers of numbers. So Steve,

32
00:01:51,291 --> 00:01:53,530
that large numbers of things interacting,

33
00:01:53,680 --> 00:01:57,920
that there are so many genes involved
and so many biochemical reactions

34
00:01:58,370 --> 00:02:03,150
involved, our brains are limited, our
memories are very limited. And so,

35
00:02:03,540 --> 00:02:05,870
um, I worry a little bit that the,

36
00:02:05,880 --> 00:02:09,360
we might be approaching the end of our
ability to have insight into certain

37
00:02:09,361 --> 00:02:12,750
kinds of questions. What Steve Means
by the word insight is not like a,

38
00:02:12,751 --> 00:02:15,750
you found the answer. It's like, that's
like a feeling, right? You like that.

39
00:02:15,870 --> 00:02:20,370
Oh, I get it. Feeling you get when you
really understand the answer. Yeah.

40
00:02:20,620 --> 00:02:25,040
That satisfying feeling that I can see
the reasoning and I can actually feel it

41
00:02:25,041 --> 00:02:29,260
in my bones. That's, that's a
very pleasurable feeling. But, um,

42
00:02:29,330 --> 00:02:33,230
one that we may not
always be able to enjoy.

43
00:02:34,700 --> 00:02:35,990
I mean, you can see the space.

44
00:02:36,410 --> 00:02:40,950
We weren't really quite sure how to feel
about this, but then Steve said, yeah,

45
00:02:40,970 --> 00:02:44,060
don't take my word for it. Talk to these
guys that work down the hall from me.

46
00:02:44,180 --> 00:02:47,750
You'll see we can, we can go right ahead.
Cool. Can you guys introduce yourself?

47
00:02:47,751 --> 00:02:51,020
Tell me who I'm talking to. Yeah.
So, uh, my name is Hod Lipson.

48
00:02:51,080 --> 00:02:54,440
My name is Michael Schmitz.
I'm a phd student and a,

49
00:02:54,441 --> 00:02:58,580
I'm a roboticist and hot and Mike have
developed this thing which does make you

50
00:02:58,581 --> 00:03:01,510
wonder if Steve's right.
It's a computer. Yes.

51
00:03:01,750 --> 00:03:06,280
Actually many a whole tower of computers
that are all grinding away in this

52
00:03:06,280 --> 00:03:07,180
morning calculations.

53
00:03:10,510 --> 00:03:12,400
Actually when you get down to it,
it's just a piece of software,

54
00:03:12,401 --> 00:03:15,760
but they've named it Eureka cause
that's what it was designed to do,

55
00:03:15,761 --> 00:03:20,470
to have you Rica moments. Uh,
maybe a, a kind of simpler example.

56
00:03:20,550 --> 00:03:24,520
And the story of Eureka begins
pretty simply with a pendulum. Okay?

57
00:03:24,830 --> 00:03:28,690
Depending on just one of these things
you see hanging off a grandfather clock,

58
00:03:29,050 --> 00:03:31,330
okay, I've got a regular pendulum
swinging in my mind. Okay?

59
00:03:31,331 --> 00:03:36,331
Swinging left and right now says w
instead of a string connected to ball,

60
00:03:36,670 --> 00:03:39,670
make it a string, connected to a
ball, connected to another string,

61
00:03:39,671 --> 00:03:42,640
connected to another ball, which is
basically like a double pendulum.

62
00:03:42,760 --> 00:03:46,120
The cool thing about this is you just put
it, you, you lift it up and let it go.

63
00:03:46,150 --> 00:03:49,030
And what you'll get says Mike is chaos.

64
00:03:52,370 --> 00:03:55,400
That's really crazy
behavior instead of Nice.

65
00:03:55,401 --> 00:03:58,010
And even now you got random.

66
00:03:58,100 --> 00:04:01,460
It's almost impossible actually try
to predict where the single move.

67
00:04:02,390 --> 00:04:06,860
So what they did was they got a camera
connected it to Eureka and Jessica just

68
00:04:06,861 --> 00:04:09,680
had Eureka Watch this thing.
I mean a move about crazily.

69
00:04:12,110 --> 00:04:13,910
And then they asked the computer
a really simple question,

70
00:04:13,911 --> 00:04:18,140
can you make some kind of sense
out of this erratic behavior?

71
00:04:18,590 --> 00:04:22,870
Like is there something in this
system that always stays the same?

72
00:04:23,190 --> 00:04:27,300
Tell me what about these pendulums
over time is not changing cause with

73
00:04:27,301 --> 00:04:29,880
everything there's gotta be
some kind of logic in there.

74
00:04:29,940 --> 00:04:31,530
So you're looking for
a law, basically I'm,

75
00:04:31,560 --> 00:04:35,020
you're looking for the law of the
double pendulum. Yes. That's the idea.

76
00:04:36,160 --> 00:04:39,910
So Eureka is there watching this pending
numbers about 3:00 AM in the lab is

77
00:04:39,911 --> 00:04:44,290
basically spitting out all of these
different guesses, formulating hypotheses.

78
00:04:44,410 --> 00:04:46,960
It's getting closer and closer.

79
00:04:47,200 --> 00:04:52,200
And then onto the screen pops
this simple formula f equals m.

80
00:04:55,720 --> 00:04:57,460
What is f equals? I mean, is
that actually the [inaudible]?

81
00:04:57,500 --> 00:05:00,740
The law that f equals the May
is Newton's law of motion.

82
00:05:01,600 --> 00:05:04,210
The Isaac Newton that Sir Isaac. To you.

83
00:05:04,270 --> 00:05:08,710
So Basic Law of physics in one of the
greatest discoveries in the history of

84
00:05:08,711 --> 00:05:09,580
human thinking.

85
00:05:11,520 --> 00:05:11,750
Uh,

86
00:05:11,750 --> 00:05:14,480
took it about day 24 hours.

87
00:05:16,850 --> 00:05:17,100
Okay.

88
00:05:17,100 --> 00:05:21,390
But the interesting thing is that it
came up with this thing without knowing

89
00:05:21,391 --> 00:05:24,150
anything about physics,
nothing. That's why we kind of,

90
00:05:24,380 --> 00:05:27,660
we think that this algorithm might be
able to find new laws that we don't know

91
00:05:27,661 --> 00:05:28,494
about yet.

92
00:05:30,750 --> 00:05:35,640
In fact, once word got out about Eureka,

93
00:05:37,590 --> 00:05:42,330
that's when the emails start a couple
emails a day to scientists all over the

94
00:05:42,331 --> 00:05:43,500
place who were like, Hey,

95
00:05:43,710 --> 00:05:46,020
do you mind if we borrow your
robot for what kinds of stuff?

96
00:05:46,480 --> 00:05:46,540
Um,

97
00:05:46,540 --> 00:05:51,540
anything you can think of from trying
to predict behaviors of cows in the herd

98
00:05:52,630 --> 00:05:54,730
to particle physics, to a stock market

99
00:05:54,890 --> 00:05:59,030
and fat. And this is when we get Steve's
point about the limits of insight.

100
00:05:59,390 --> 00:06:01,400
That's when they met this guy.

101
00:06:01,430 --> 00:06:06,050
My name is girl soil girl is a biologist
at the University of Texas Southwestern

102
00:06:06,051 --> 00:06:07,880
Medical Center. He got in touch with hod

103
00:06:07,930 --> 00:06:12,790
when you said, I have this amazing
data, which is single cell dynamics,

104
00:06:13,010 --> 00:06:16,640
meaning he's got this tiny little
things. So simple bacteria, really basic,

105
00:06:16,641 --> 00:06:20,150
and he's been collecting this information
on how it works on its inside,

106
00:06:20,220 --> 00:06:24,060
how things go up and down. Certain
nutrients increase certain nutrients,

107
00:06:24,540 --> 00:06:25,770
decrease over time,
just like a [inaudible].

108
00:06:26,020 --> 00:06:28,550
The thing is in a cell, it's
like a thousands of pendulums.

109
00:06:28,551 --> 00:06:31,690
There's so many part genes turning
on and off, thousands and thousands,

110
00:06:31,730 --> 00:06:35,240
tens of thousands proteins turning on
other genes and nutrients going up and

111
00:06:35,241 --> 00:06:38,720
down. It's this crazy quilt
of complicated feedback.

112
00:06:39,080 --> 00:06:42,590
And you wanted to know inside of this
cell, how are all of these things related.

113
00:06:42,820 --> 00:06:45,620
I mean, we can measure it all, we can see
things going up and down and all that.

114
00:06:45,621 --> 00:06:48,170
But what are the rules? What
are the rules here in this?

115
00:06:48,171 --> 00:06:50,390
He says is the problem for biology.

116
00:06:50,420 --> 00:06:53,270
Biology is one of the least well
understood systems compared to let's say,

117
00:06:53,271 --> 00:06:56,930
chemistry and physics. They're still
lacking the basics. So we said, look,

118
00:06:56,990 --> 00:06:57,823
Mr Robot,

119
00:06:58,370 --> 00:07:03,350
can you tell us what you think are sort
of the important principles governing

120
00:07:03,351 --> 00:07:06,770
this organism and maybe detect
things that were hidden from us.

121
00:07:06,780 --> 00:07:10,560
So he sent us the data
and uh, we analyzed it

122
00:07:11,700 --> 00:07:14,410
and uh, well, OK, let's not let, yeah,

123
00:07:14,411 --> 00:07:18,350
so what happened suddenly
equation starting, popping out

124
00:07:19,940 --> 00:07:24,230
almost immediately the robot
came back to us and said, okay,

125
00:07:24,231 --> 00:07:27,560
here's a set of two equations
that describe your data.

126
00:07:27,620 --> 00:07:31,670
Do you remember by any chance what
the, what the actual equation was? Not,

127
00:07:31,700 --> 00:07:34,730
not that we'd understand it, but just
sort of to hear it said out loud. Yeah,

128
00:07:34,731 --> 00:07:35,390
no, I don't want,

129
00:07:35,390 --> 00:07:38,960
you don't have my rain man skills
developed to that degree yet.

130
00:07:39,020 --> 00:07:42,230
The important thing is that the equation
was telling them things like when this

131
00:07:42,231 --> 00:07:45,020
protein goes up, this other
thing always goes down.

132
00:07:45,230 --> 00:07:48,710
And when that thing goes down, this gene
turns on and there's a loop de loop.

133
00:07:48,800 --> 00:07:53,300
And when he went to his cell to check
all this out, the equation was right.

134
00:07:53,450 --> 00:07:58,070
These equations match the data.
And in fact, they explain new data.

135
00:07:58,550 --> 00:08:01,130
These equations could even predict
with the cell was about to do,

136
00:08:03,760 --> 00:08:07,330
but hold the champagne. There's
just one little problem here.

137
00:08:07,930 --> 00:08:10,750
The formulas checkout, but
we don't know what they mean.

138
00:08:11,650 --> 00:08:14,540
You don't know what they mean. Right?

139
00:08:15,120 --> 00:08:18,480
Meaning they don't know why these
equations work. I why when this goes up,

140
00:08:18,510 --> 00:08:21,930
does that go down? Why when that
goes up, does this go sideways? Why?

141
00:08:21,931 --> 00:08:26,860
I had to first look at this and try
to make sense of it. We said like, oh,

142
00:08:27,340 --> 00:08:29,890
okay, I think we understand. And
we were like, oh maybe we don't.

143
00:08:30,120 --> 00:08:33,470
We think that we're close
to understanding it,

144
00:08:33,530 --> 00:08:36,830
but you know, now we're
in this bizarre situation.

145
00:08:36,860 --> 00:08:40,910
We can't even publish it right now
cause we can't just publish a equation

146
00:08:40,940 --> 00:08:41,870
without explaining it.

147
00:08:42,710 --> 00:08:47,330
So in the end, they're in this awkward
position when they've got the answer,

148
00:08:48,000 --> 00:08:49,880
but they don't have the insight.

149
00:08:49,940 --> 00:08:54,010
And I think it's a preview
of what's the common signs.

150
00:08:54,300 --> 00:08:56,100
The more we turned to with these big

151
00:08:56,100 --> 00:09:00,120
questions, the more they'll give us
answers that we just don't understand.

152
00:09:00,210 --> 00:09:05,210
We'll be faced with this challenge of
having to find ways to get a computer to

153
00:09:05,221 --> 00:09:07,260
explain what it found.

154
00:09:07,350 --> 00:09:10,440
But that will leave us if this really
happens in some weird position as

155
00:09:10,441 --> 00:09:14,190
bystanders where we're, we're
sort of listening to the oracle,

156
00:09:14,490 --> 00:09:18,240
but not really understanding the answer.
Is there gonna be a time when we,

157
00:09:19,800 --> 00:09:23,370
we can't cut it anymore. We've had this,

158
00:09:23,850 --> 00:09:27,750
this window in human history when we
could not just know things but actually

159
00:09:27,751 --> 00:09:31,470
understand them. That is, you could
know why they were true, not just know,

160
00:09:31,471 --> 00:09:35,280
but to know why. And that's a
beautiful moment in human history.

161
00:09:35,670 --> 00:09:40,650
But I feel like it may only be a moment.
Well, I don't really see it quite that,

162
00:09:41,100 --> 00:09:46,100
that sort of sad and dramatic because at
the end there will be simple principles

163
00:09:46,921 --> 00:09:49,560
to describe even the most
complicated of processes.

164
00:09:49,590 --> 00:09:54,120
So you have a bias that prevents you from
feeling the kind of despair that Steve

165
00:09:54,121 --> 00:09:57,270
feels that we were hoping you
would feel. Oh, well I'm, I,

166
00:09:57,271 --> 00:10:00,870
I have a positive outlook, but
I just wondering about the,

167
00:10:01,290 --> 00:10:03,570
we look what we have discovered.

168
00:10:03,571 --> 00:10:06,780
You'll say when you're an old man with
your robot sitting there and address next

169
00:10:06,781 --> 00:10:09,360
to you and the robot will
be holding your hand.

170
00:10:09,361 --> 00:10:12,870
But that will be a cold hand
and Jad and I will be thinking,

171
00:10:12,900 --> 00:10:17,160
I dunno who's the we here. I as well.
I would say we is sort of knowledge.

172
00:10:17,520 --> 00:10:20,730
I'm just thirsty for understanding
and thirsty for knowledge.

173
00:10:21,150 --> 00:10:23,190
Me and the cold hand holding my hand,

174
00:10:23,610 --> 00:10:27,690
we've accumulated and contributed to the
overall understanding of something that

175
00:10:27,691 --> 00:10:30,000
we thought maybe 50 years
ago wasn't possible.

176
00:10:30,060 --> 00:10:32,700
And that would be something
that would make me happy.

