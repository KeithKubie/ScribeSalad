WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:02.480
 
in this video we'll finally see how we

00:00:02.480 --> 00:00:02.490
in this video we'll finally see how we
 

00:00:02.490 --> 00:00:04.820
in this video we'll finally see how we
can train a conditional random field and

00:00:04.820 --> 00:00:04.830
can train a conditional random field and
 

00:00:04.830 --> 00:00:06.980
can train a conditional random field and
so will first discuss the last function

00:00:06.980 --> 00:00:06.990
so will first discuss the last function
 

00:00:06.990 --> 00:00:10.129
so will first discuss the last function
that we'll use to train and learn our

00:00:10.129 --> 00:00:10.139
that we'll use to train and learn our
 

00:00:10.139 --> 00:00:14.480
that we'll use to train and learn our
conditional random field so in the

00:00:14.480 --> 00:00:14.490
conditional random field so in the
 

00:00:14.490 --> 00:00:17.000
conditional random field so in the
previous videos we've defined what we

00:00:17.000 --> 00:00:17.010
previous videos we've defined what we
 

00:00:17.010 --> 00:00:18.740
previous videos we've defined what we
meant by a linear chain conditional

00:00:18.740 --> 00:00:18.750
meant by a linear chain conditional
 

00:00:18.750 --> 00:00:19.990
meant by a linear chain conditional
random field so that's the specific

00:00:19.990 --> 00:00:20.000
random field so that's the specific
 

00:00:20.000 --> 00:00:21.980
random field so that's the specific
conditional random field we'll consider

00:00:21.980 --> 00:00:21.990
conditional random field we'll consider
 

00:00:21.990 --> 00:00:25.490
conditional random field we'll consider
so P of Y given X square Y is a sequence

00:00:25.490 --> 00:00:25.500
so P of Y given X square Y is a sequence
 

00:00:25.500 --> 00:00:27.830
so P of Y given X square Y is a sequence
of label and X is sequence of inputs is

00:00:27.830 --> 00:00:27.840
of label and X is sequence of inputs is
 

00:00:27.840 --> 00:00:31.569
of label and X is sequence of inputs is
going to be the exponential of a sum of

00:00:31.569 --> 00:00:31.579
going to be the exponential of a sum of
 

00:00:31.579 --> 00:00:36.889
going to be the exponential of a sum of
unary potential log factors and pairwise

00:00:36.889 --> 00:00:36.899
unary potential log factors and pairwise
 

00:00:36.899 --> 00:00:39.560
unary potential log factors and pairwise
log factors divided by normalization

00:00:39.560 --> 00:00:39.570
log factors divided by normalization
 

00:00:39.570 --> 00:00:42.080
log factors divided by normalization
constant that we have to compute with

00:00:42.080 --> 00:00:42.090
constant that we have to compute with
 

00:00:42.090 --> 00:00:44.240
constant that we have to compute with
some dynamic programming using the

00:00:44.240 --> 00:00:44.250
some dynamic programming using the
 

00:00:44.250 --> 00:00:49.549
some dynamic programming using the
forward backward algorithm and so thus

00:00:49.549 --> 00:00:49.559
forward backward algorithm and so thus
 

00:00:49.559 --> 00:00:51.560
forward backward algorithm and so thus
far in the previous videos I always

00:00:51.560 --> 00:00:51.570
far in the previous videos I always
 

00:00:51.570 --> 00:00:53.660
far in the previous videos I always
assumed that our conditional random

00:00:53.660 --> 00:00:53.670
assumed that our conditional random
 

00:00:53.670 --> 00:00:55.459
assumed that our conditional random
field was already trained and I've

00:00:55.459 --> 00:00:55.469
field was already trained and I've
 

00:00:55.469 --> 00:00:56.900
field was already trained and I've
talked about how we can perform

00:00:56.900 --> 00:00:56.910
talked about how we can perform
 

00:00:56.910 --> 00:00:58.939
talked about how we can perform
inference compute the partition function

00:00:58.939 --> 00:00:58.949
inference compute the partition function
 

00:00:58.949 --> 00:01:01.099
inference compute the partition function
perform classification and so on

00:01:01.099 --> 00:01:01.109
perform classification and so on
 

00:01:01.109 --> 00:01:02.660
perform classification and so on
so now we'll finally look at how we can

00:01:02.660 --> 00:01:02.670
so now we'll finally look at how we can
 

00:01:02.670 --> 00:01:04.850
so now we'll finally look at how we can
train a conditional random field on some

00:01:04.850 --> 00:01:04.860
train a conditional random field on some
 

00:01:04.860 --> 00:01:08.450
train a conditional random field on some
available data so again we'll use

00:01:08.450 --> 00:01:08.460
available data so again we'll use
 

00:01:08.460 --> 00:01:11.149
available data so again we'll use
empirical risk minimization as our

00:01:11.149 --> 00:01:11.159
empirical risk minimization as our
 

00:01:11.159 --> 00:01:13.340
empirical risk minimization as our
guiding principle for deriving and

00:01:13.340 --> 00:01:13.350
guiding principle for deriving and
 

00:01:13.350 --> 00:01:15.740
guiding principle for deriving and
learning algorithm that is we are going

00:01:15.740 --> 00:01:15.750
learning algorithm that is we are going
 

00:01:15.750 --> 00:01:18.830
learning algorithm that is we are going
to choose a loss function and to train a

00:01:18.830 --> 00:01:18.840
to choose a loss function and to train a
 

00:01:18.840 --> 00:01:21.980
to choose a loss function and to train a
conditional random field on training the

00:01:21.980 --> 00:01:21.990
conditional random field on training the
 

00:01:21.990 --> 00:01:24.950
conditional random field on training the
training set that is a set of pairs of

00:01:24.950 --> 00:01:24.960
training set that is a set of pairs of
 

00:01:24.960 --> 00:01:28.100
training set that is a set of pairs of
input sequences and target sequences or

00:01:28.100 --> 00:01:28.110
input sequences and target sequences or
 

00:01:28.110 --> 00:01:30.499
input sequences and target sequences or
label sequences will minimize the

00:01:30.499 --> 00:01:30.509
label sequences will minimize the
 

00:01:30.509 --> 00:01:32.960
label sequences will minimize the
average of the sum of the loss function

00:01:32.960 --> 00:01:32.970
average of the sum of the loss function
 

00:01:32.970 --> 00:01:34.880
average of the sum of the loss function
where we compare to what extent our

00:01:34.880 --> 00:01:34.890
where we compare to what extent our
 

00:01:34.890 --> 00:01:39.260
where we compare to what extent our
model fits well with the target sequence

00:01:39.260 --> 00:01:39.270
model fits well with the target sequence
 

00:01:39.270 --> 00:01:41.870
model fits well with the target sequence
information we have so we'll optimize

00:01:41.870 --> 00:01:41.880
information we have so we'll optimize
 

00:01:41.880 --> 00:01:46.399
information we have so we'll optimize
this average loss along so plus the way

00:01:46.399 --> 00:01:46.409
this average loss along so plus the way
 

00:01:46.409 --> 00:01:49.700
this average loss along so plus the way
the regularizer to control to what

00:01:49.700 --> 00:01:49.710
the regularizer to control to what
 

00:01:49.710 --> 00:01:51.980
the regularizer to control to what
extent we won our model to fit well the

00:01:51.980 --> 00:01:51.990
extent we won our model to fit well the
 

00:01:51.990 --> 00:01:54.499
extent we won our model to fit well the
data and to be able to control for

00:01:54.499 --> 00:01:54.509
data and to be able to control for
 

00:01:54.509 --> 00:01:57.020
data and to be able to control for
overfitting so again we're casting

00:01:57.020 --> 00:01:57.030
overfitting so again we're casting
 

00:01:57.030 --> 00:01:58.700
overfitting so again we're casting
learning as optimization like we've done

00:01:58.700 --> 00:01:58.710
learning as optimization like we've done
 

00:01:58.710 --> 00:02:01.459
learning as optimization like we've done
in the regular neural network and though

00:02:01.459 --> 00:02:01.469
in the regular neural network and though
 

00:02:01.469 --> 00:02:03.469
in the regular neural network and though
ideally we'd like to optimize the

00:02:03.469 --> 00:02:03.479
ideally we'd like to optimize the
 

00:02:03.479 --> 00:02:05.389
ideally we'd like to optimize the
classification error so in this case

00:02:05.389 --> 00:02:05.399
classification error so in this case
 

00:02:05.399 --> 00:02:07.190
classification error so in this case
perhaps would like to optimize the sum

00:02:07.190 --> 00:02:07.200
perhaps would like to optimize the sum
 

00:02:07.200 --> 00:02:10.369
perhaps would like to optimize the sum
of per label classification there this

00:02:10.369 --> 00:02:10.379
of per label classification there this
 

00:02:10.379 --> 00:02:12.140
of per label classification there this
error again is non smooth so we have to

00:02:12.140 --> 00:02:12.150
error again is non smooth so we have to
 

00:02:12.150 --> 00:02:13.309
error again is non smooth so we have to
use what we use

00:02:13.309 --> 00:02:13.319
use what we use
 

00:02:13.319 --> 00:02:14.599
use what we use
we call a surrogate loss or an

00:02:14.599 --> 00:02:14.609
we call a surrogate loss or an
 

00:02:14.609 --> 00:02:16.369
we call a surrogate loss or an
alternative loss that sort of looks like

00:02:16.369 --> 00:02:16.379
alternative loss that sort of looks like
 

00:02:16.379 --> 00:02:18.559
alternative loss that sort of looks like
what we want to optimize but that is

00:02:18.559 --> 00:02:18.569
what we want to optimize but that is
 

00:02:18.569 --> 00:02:21.770
what we want to optimize but that is
better behaved and specifically in this

00:02:21.770 --> 00:02:21.780
better behaved and specifically in this
 

00:02:21.780 --> 00:02:23.539
better behaved and specifically in this
case is going to be differentiable so we

00:02:23.539 --> 00:02:23.549
case is going to be differentiable so we
 

00:02:23.549 --> 00:02:26.949
case is going to be differentiable so we
can perform gradient descent training

00:02:26.949 --> 00:02:26.959
can perform gradient descent training
 

00:02:26.959 --> 00:02:29.959
can perform gradient descent training
and again we'll use stochastic gradient

00:02:29.959 --> 00:02:29.969
and again we'll use stochastic gradient
 

00:02:29.969 --> 00:02:33.080
and again we'll use stochastic gradient
descent that is the way where training

00:02:33.080 --> 00:02:33.090
descent that is the way where training
 

00:02:33.090 --> 00:02:34.520
descent that is the way where training
is going to proceed is that we'll first

00:02:34.520 --> 00:02:34.530
is going to proceed is that we'll first
 

00:02:34.530 --> 00:02:36.229
is going to proceed is that we'll first
initialize the parameters of our model

00:02:36.229 --> 00:02:36.239
initialize the parameters of our model
 

00:02:36.239 --> 00:02:38.360
initialize the parameters of our model
for a certain number of iterations will

00:02:38.360 --> 00:02:38.370
for a certain number of iterations will
 

00:02:38.370 --> 00:02:40.729
for a certain number of iterations will
cycle through our pairs of input

00:02:40.729 --> 00:02:40.739
cycle through our pairs of input
 

00:02:40.739 --> 00:02:43.610
cycle through our pairs of input
sequences and target sequences we'll

00:02:43.610 --> 00:02:43.620
sequences and target sequences we'll
 

00:02:43.620 --> 00:02:46.429
sequences and target sequences we'll
compute the update direction which is

00:02:46.429 --> 00:02:46.439
compute the update direction which is
 

00:02:46.439 --> 00:02:48.709
compute the update direction which is
going to be minus the gradient of the

00:02:48.709 --> 00:02:48.719
going to be minus the gradient of the
 

00:02:48.719 --> 00:02:53.539
going to be minus the gradient of the
loss for my current pair of input and

00:02:53.539 --> 00:02:53.549
loss for my current pair of input and
 

00:02:53.549 --> 00:02:56.869
loss for my current pair of input and
target sequences - also the gradient of

00:02:56.869 --> 00:02:56.879
target sequences - also the gradient of
 

00:02:56.879 --> 00:02:58.789
target sequences - also the gradient of
the regularizer and then I'll take a

00:02:58.789 --> 00:02:58.799
the regularizer and then I'll take a
 

00:02:58.799 --> 00:03:00.140
the regularizer and then I'll take a
small step

00:03:00.140 --> 00:03:00.150
small step
 

00:03:00.150 --> 00:03:04.459
small step
so alpha times the update direction to

00:03:04.459 --> 00:03:04.469
so alpha times the update direction to
 

00:03:04.469 --> 00:03:07.699
so alpha times the update direction to
change my vector from is my set of

00:03:07.699 --> 00:03:07.709
change my vector from is my set of
 

00:03:07.709 --> 00:03:09.800
change my vector from is my set of
parameters and so again I have to

00:03:09.800 --> 00:03:09.810
parameters and so again I have to
 

00:03:09.810 --> 00:03:12.199
parameters and so again I have to
specify a loss function a specified

00:03:12.199 --> 00:03:12.209
specify a loss function a specified
 

00:03:12.209 --> 00:03:13.729
specify a loss function a specified
procedure for computing the gradient

00:03:13.729 --> 00:03:13.739
procedure for computing the gradient
 

00:03:13.739 --> 00:03:15.740
procedure for computing the gradient
with respect to the loss function I also

00:03:15.740 --> 00:03:15.750
with respect to the loss function I also
 

00:03:15.750 --> 00:03:17.929
with respect to the loss function I also
have to specify the regularizer and its

00:03:17.929 --> 00:03:17.939
have to specify the regularizer and its
 

00:03:17.939 --> 00:03:19.909
have to specify the regularizer and its
gradient as well as the initialization

00:03:19.909 --> 00:03:19.919
gradient as well as the initialization
 

00:03:19.919 --> 00:03:21.979
gradient as well as the initialization
method so these two parts are not really

00:03:21.979 --> 00:03:21.989
method so these two parts are not really
 

00:03:21.989 --> 00:03:23.089
method so these two parts are not really
going to change so we're not really

00:03:23.089 --> 00:03:23.099
going to change so we're not really
 

00:03:23.099 --> 00:03:25.069
going to change so we're not really
gonna guess discussed them will focus on

00:03:25.069 --> 00:03:25.079
gonna guess discussed them will focus on
 

00:03:25.079 --> 00:03:27.860
gonna guess discussed them will focus on
choosing a loss function and also

00:03:27.860 --> 00:03:27.870
choosing a loss function and also
 

00:03:27.870 --> 00:03:30.379
choosing a loss function and also
specifying how we compute the parameter

00:03:30.379 --> 00:03:30.389
specifying how we compute the parameter
 

00:03:30.389 --> 00:03:32.479
specifying how we compute the parameter
gradient and in this particular video

00:03:32.479 --> 00:03:32.489
gradient and in this particular video
 

00:03:32.489 --> 00:03:34.429
gradient and in this particular video
we'll look at the loss function would

00:03:34.429 --> 00:03:34.439
we'll look at the loss function would
 

00:03:34.439 --> 00:03:40.069
we'll look at the loss function would
optimize so again because a conditional

00:03:40.069 --> 00:03:40.079
optimize so again because a conditional
 

00:03:40.079 --> 00:03:42.140
optimize so again because a conditional
random field estimates the conditional

00:03:42.140 --> 00:03:42.150
random field estimates the conditional
 

00:03:42.150 --> 00:03:44.179
random field estimates the conditional
probability of the target given the

00:03:44.179 --> 00:03:44.189
probability of the target given the
 

00:03:44.189 --> 00:03:47.659
probability of the target given the
input what is an actual thing to do is

00:03:47.659 --> 00:03:47.669
input what is an actual thing to do is
 

00:03:47.669 --> 00:03:50.719
input what is an actual thing to do is
to maximize the probability assigned to

00:03:50.719 --> 00:03:50.729
to maximize the probability assigned to
 

00:03:50.729 --> 00:03:54.140
to maximize the probability assigned to
the true target the sequence given some

00:03:54.140 --> 00:03:54.150
the true target the sequence given some
 

00:03:54.150 --> 00:03:56.869
the true target the sequence given some
input sequence as extracted from our

00:03:56.869 --> 00:03:56.879
input sequence as extracted from our
 

00:03:56.879 --> 00:04:00.559
input sequence as extracted from our
training set and as we've seen in the

00:04:00.559 --> 00:04:00.569
training set and as we've seen in the
 

00:04:00.569 --> 00:04:02.659
training set and as we've seen in the
case of a regular neural network instead

00:04:02.659 --> 00:04:02.669
case of a regular neural network instead
 

00:04:02.669 --> 00:04:04.759
case of a regular neural network instead
of maximizing probabilities we can

00:04:04.759 --> 00:04:04.769
of maximizing probabilities we can
 

00:04:04.769 --> 00:04:07.159
of maximizing probabilities we can
minimize the negative log of the

00:04:07.159 --> 00:04:07.169
minimize the negative log of the
 

00:04:07.169 --> 00:04:09.830
minimize the negative log of the
probability which is more better behaved

00:04:09.830 --> 00:04:09.840
probability which is more better behaved
 

00:04:09.840 --> 00:04:13.610
probability which is more better behaved
and yield simpler math and so to firm it

00:04:13.610 --> 00:04:13.620
and yield simpler math and so to firm it
 

00:04:13.620 --> 00:04:16.009
and yield simpler math and so to firm it
as a minimization problem we minimize

00:04:16.009 --> 00:04:16.019
as a minimization problem we minimize
 

00:04:16.019 --> 00:04:17.870
as a minimization problem we minimize
the negative log likelihood so our last

00:04:17.870 --> 00:04:17.880
the negative log likelihood so our last
 

00:04:17.880 --> 00:04:20.569
the negative log likelihood so our last
function with the minus the log of the

00:04:20.569 --> 00:04:20.579
function with the minus the log of the
 

00:04:20.579 --> 00:04:23.180
function with the minus the log of the
probability of the true target so

00:04:23.180 --> 00:04:23.190
probability of the true target so
 

00:04:23.190 --> 00:04:24.680
probability of the true target so
normally this would be the true target

00:04:24.680 --> 00:04:24.690
normally this would be the true target
 

00:04:24.690 --> 00:04:26.370
normally this would be the true target
given the

00:04:26.370 --> 00:04:26.380
given the
 

00:04:26.380 --> 00:04:29.280
given the
input sequence so this is really the

00:04:29.280 --> 00:04:29.290
input sequence so this is really the
 

00:04:29.290 --> 00:04:31.320
input sequence so this is really the
same that we've done in a regular neural

00:04:31.320 --> 00:04:31.330
same that we've done in a regular neural
 

00:04:31.330 --> 00:04:33.330
same that we've done in a regular neural
network where we have an input vector

00:04:33.330 --> 00:04:33.340
network where we have an input vector
 

00:04:33.340 --> 00:04:35.370
network where we have an input vector
and we're trying to classify it to do a

00:04:35.370 --> 00:04:35.380
and we're trying to classify it to do a
 

00:04:35.380 --> 00:04:38.430
and we're trying to classify it to do a
set of C classes the only difference is

00:04:38.430 --> 00:04:38.440
set of C classes the only difference is
 

00:04:38.440 --> 00:04:40.440
set of C classes the only difference is
that now this is a sequence and this is

00:04:40.440 --> 00:04:40.450
that now this is a sequence and this is
 

00:04:40.450 --> 00:04:43.620
that now this is a sequence and this is
also a sequence one sort of subtle

00:04:43.620 --> 00:04:43.630
also a sequence one sort of subtle
 

00:04:43.630 --> 00:04:46.700
also a sequence one sort of subtle
difference also is that while in

00:04:46.700 --> 00:04:46.710
difference also is that while in
 

00:04:46.710 --> 00:04:48.900
difference also is that while in
non-sequential classification if you

00:04:48.900 --> 00:04:48.910
non-sequential classification if you
 

00:04:48.910 --> 00:04:50.400
non-sequential classification if you
have ten classes we might actually

00:04:50.400 --> 00:04:50.410
have ten classes we might actually
 

00:04:50.410 --> 00:04:53.280
have ten classes we might actually
explicitly compute the full conditional

00:04:53.280 --> 00:04:53.290
explicitly compute the full conditional
 

00:04:53.290 --> 00:04:55.530
explicitly compute the full conditional
distribution P of Y given X by iterating

00:04:55.530 --> 00:04:55.540
distribution P of Y given X by iterating
 

00:04:55.540 --> 00:04:58.170
distribution P of Y given X by iterating
over all classes and so obtaining the

00:04:58.170 --> 00:04:58.180
over all classes and so obtaining the
 

00:04:58.180 --> 00:05:00.960
over all classes and so obtaining the
full vector of probability now with a

00:05:00.960 --> 00:05:00.970
full vector of probability now with a
 

00:05:00.970 --> 00:05:02.550
full vector of probability now with a
sequence we can do this because the

00:05:02.550 --> 00:05:02.560
sequence we can do this because the
 

00:05:02.560 --> 00:05:05.370
sequence we can do this because the
number of sequences is exponential so

00:05:05.370 --> 00:05:05.380
number of sequences is exponential so
 

00:05:05.380 --> 00:05:08.670
number of sequences is exponential so
really when we're computing this we'll

00:05:08.670 --> 00:05:08.680
really when we're computing this we'll
 

00:05:08.680 --> 00:05:13.110
really when we're computing this we'll
use forward backward to get our alpha or

00:05:13.110 --> 00:05:13.120
use forward backward to get our alpha or
 

00:05:13.120 --> 00:05:15.480
use forward backward to get our alpha or
beta table and to compute the partition

00:05:15.480 --> 00:05:15.490
beta table and to compute the partition
 

00:05:15.490 --> 00:05:18.570
beta table and to compute the partition
function and we'll use then explicitly

00:05:18.570 --> 00:05:18.580
function and we'll use then explicitly
 

00:05:18.580 --> 00:05:20.160
function and we'll use then explicitly
the partition function into the

00:05:20.160 --> 00:05:20.170
the partition function into the
 

00:05:20.170 --> 00:05:23.220
the partition function into the
expression for P of Y given X and then

00:05:23.220 --> 00:05:23.230
expression for P of Y given X and then
 

00:05:23.230 --> 00:05:25.320
expression for P of Y given X and then
we'll take minus the log of that so we

00:05:25.320 --> 00:05:25.330
we'll take minus the log of that so we
 

00:05:25.330 --> 00:05:28.530
we'll take minus the log of that so we
won't actually compute we don't have the

00:05:28.530 --> 00:05:28.540
won't actually compute we don't have the
 

00:05:28.540 --> 00:05:30.540
won't actually compute we don't have the
full distribution over all sequences

00:05:30.540 --> 00:05:30.550
full distribution over all sequences
 

00:05:30.550 --> 00:05:33.150
full distribution over all sequences
that just because that's too big of an

00:05:33.150 --> 00:05:33.160
that just because that's too big of an
 

00:05:33.160 --> 00:05:36.150
that just because that's too big of an
object it's something that's there's an

00:05:36.150 --> 00:05:36.160
object it's something that's there's an
 

00:05:36.160 --> 00:05:37.440
object it's something that's there's an
exponential of all sequences that I

00:05:37.440 --> 00:05:37.450
exponential of all sequences that I
 

00:05:37.450 --> 00:05:38.550
exponential of all sequences that I
would be an exponential number of

00:05:38.550 --> 00:05:38.560
would be an exponential number of
 

00:05:38.560 --> 00:05:40.830
would be an exponential number of
probabilities so that's the last

00:05:40.830 --> 00:05:40.840
probabilities so that's the last
 

00:05:40.840 --> 00:05:42.300
probabilities so that's the last
function we'll use for training a

00:05:42.300 --> 00:05:42.310
function we'll use for training a
 

00:05:42.310 --> 00:05:45.360
function we'll use for training a
conditional random field

