1
00:00:00,990 --> 00:00:01,240
Okay.

2
00:00:01,240 --> 00:00:01,931
In this video,

3
00:00:01,931 --> 00:00:06,931
we'll look at a little bit
more the details of why
it's difficult to train deep

4
00:00:06,941 --> 00:00:07,774
neural network.

5
00:00:07,990 --> 00:00:08,823
Okay.

6
00:00:10,100 --> 00:00:13,790
So I mentioned in the previous video
that historically we've often found that

7
00:00:13,791 --> 00:00:18,791
training a single hidden their neural
network without say a second hidden,

8
00:00:20,131 --> 00:00:21,920
they're like in this case,
uh,

9
00:00:21,921 --> 00:00:26,180
would often work better than
having a two or more hidden layers.

10
00:00:26,810 --> 00:00:27,470
And,
uh,

11
00:00:27,470 --> 00:00:31,100
even though there are some problems where
we'd actually think that we would gain

12
00:00:31,101 --> 00:00:34,880
by having many hidden layers. So, uh,

13
00:00:34,910 --> 00:00:39,470
well I'll talk about in this video is
two types of reasons where we tend to see

14
00:00:39,471 --> 00:00:43,880
this difficulty, which will help
us then introduce ways of, of, uh,

15
00:00:43,920 --> 00:00:46,430
addressing these difficulties and, uh, uh,

16
00:00:46,610 --> 00:00:50,600
solve our problems and get
effective learning algorithms
or deep neural networks.

17
00:00:52,950 --> 00:00:53,150
Yeah,

18
00:00:53,150 --> 00:00:57,380
yeah. Actually, two types of problems
was, are, uh, or difficulty difficulties,

19
00:00:57,381 --> 00:00:59,990
which are quite different.
Uh,

20
00:01:00,020 --> 00:01:03,680
so the first is that a training,

21
00:01:03,681 --> 00:01:08,600
a deep neural network is a
harder optimization problem
than training a single

22
00:01:08,601 --> 00:01:13,400
hidden, they're known network, um,
in particular. So in this case,

23
00:01:13,401 --> 00:01:18,401
we would be in an under fitting a
situation where if we had a better

24
00:01:18,590 --> 00:01:23,230
optimization procedure, then,
uh, we should be able to, uh, if,

25
00:01:23,260 --> 00:01:26,750
if indeed we are having a harder
time to optimize on neural network,

26
00:01:26,751 --> 00:01:29,240
then it means we're probably
could fit the data better.

27
00:01:29,360 --> 00:01:34,030
So it would be then in under fitting a
situation, which would mean that, uh,

28
00:01:34,160 --> 00:01:38,450
we would get better results
in digitalization by, uh,

29
00:01:38,510 --> 00:01:43,190
optimizing better fitting better
our training data. So this,

30
00:01:43,430 --> 00:01:47,480
uh, difficulty optimization is
often a, it is trading by, uh,

31
00:01:47,600 --> 00:01:49,460
the vanishing gradient problem,

32
00:01:49,970 --> 00:01:53,600
which is a well known problem
in a neural networks, uh,

33
00:01:53,800 --> 00:01:58,340
essentially corresponds to the
following, uh, following situation.

34
00:01:59,030 --> 00:02:03,890
So, uh, we, uh, if we go back to
the backpropagation algorithm,

35
00:02:03,891 --> 00:02:08,891
we know that whenever we are taking the
gradient of the activation of a unit and

36
00:02:10,461 --> 00:02:14,300
then we passing that gradient to get
degree in, in of the pre activation,

37
00:02:14,690 --> 00:02:18,890
we have to multiply by the
partial derivative of the
activation with respect to

38
00:02:18,891 --> 00:02:23,180
their pre activation.
Now if the hidden unit is at,

39
00:02:23,290 --> 00:02:28,130
it's a core close to a saturation
point, that is, it's uh, it has a,

40
00:02:28,131 --> 00:02:31,910
it's a activation close to the
highest or lowest value it can take.

41
00:02:32,150 --> 00:02:35,480
It means that the partial dude IV
is going to be very close to zero,

42
00:02:35,930 --> 00:02:40,930
which means that the gradient with respect
to the pre activation is going to be

43
00:02:41,390 --> 00:02:44,270
close to zero because we're
multiplying by that partial derivative.

44
00:02:44,930 --> 00:02:46,700
And so it means that at this point,

45
00:02:46,701 --> 00:02:51,020
not all the great is being back
propagated towards the other layers.

46
00:02:51,020 --> 00:02:53,630
Below is going to be very close to zero.

47
00:02:53,690 --> 00:02:56,180
And if we repeat this process many times,

48
00:02:56,480 --> 00:03:00,550
and we have many saturated hidden
units in means, uh, as we get closer,

49
00:03:00,551 --> 00:03:04,300
closer to the input layer, we get great
ins that are closer and closer to zero.

50
00:03:04,420 --> 00:03:06,650
So that's why we say that
the gradient is Spanish ing.

51
00:03:07,810 --> 00:03:10,650
And this is essentially caused by the,
uh,

52
00:03:10,720 --> 00:03:15,720
saturated units because saturated in units
as essentially a partial do they have

53
00:03:16,680 --> 00:03:21,550
a of zero. And so that essentially
shrinks the gradient, uh,

54
00:03:21,640 --> 00:03:22,473
towards zero.

55
00:03:23,110 --> 00:03:28,110
And so that's a big problem because
intuitively the advantage of having hidden

56
00:03:28,211 --> 00:03:32,930
units that are non linear and saturate
is that it makes the prediction non

57
00:03:32,931 --> 00:03:34,930
linear and we want that nonlinearity.

58
00:03:35,410 --> 00:03:39,890
So we actually want the neural network
to get to a regime which is close to the

59
00:03:39,891 --> 00:03:42,340
saturation point to take advantage of,
uh,

60
00:03:42,460 --> 00:03:46,090
the nonlinear already of the hidden units.
But as we get there,

61
00:03:46,091 --> 00:03:50,440
we get great ins that become a closer
and closer to zero and yet towards the

62
00:03:50,441 --> 00:03:53,470
vanishing gradient situation.
Um,

63
00:03:53,800 --> 00:03:57,130
and so there's sort of this dilemma here,
which is very unfortunate,

64
00:03:57,340 --> 00:04:02,340
which is that when we get to a situation
where we're signing to be in the

65
00:04:02,680 --> 00:04:05,920
interesting configuration of the
neural network, which is non linear,

66
00:04:06,070 --> 00:04:09,370
while it becomes actually
harder and harder to optimize
because the green it is,

67
00:04:09,400 --> 00:04:14,080
becomes very small for the first
hidden layers, which might not train,

68
00:04:14,170 --> 00:04:15,970
uh, at all. Uh,

69
00:04:16,060 --> 00:04:19,570
with respect to how quickly the
top hit in layers with train.

70
00:04:20,020 --> 00:04:23,060
And which means that these
hidden units here are,

71
00:04:23,061 --> 00:04:27,510
these units here might start to adapt to
hidden units that are more or less, uh,

72
00:04:27,830 --> 00:04:29,330
a random because they're,

73
00:04:29,360 --> 00:04:33,280
they're not actually training as quickly
as the connections that are here.

74
00:04:33,281 --> 00:04:38,260
For instance, this is in fact a well
known problem in record neural networks.

75
00:04:38,260 --> 00:04:40,580
The fact that there's
this dilemma between, uh,

76
00:04:40,810 --> 00:04:45,580
getting gradient that pass as well
through the hidden layers and being in a

77
00:04:45,581 --> 00:04:49,210
situation where the neural network
is in a configuration that allows for

78
00:04:49,211 --> 00:04:51,350
saturation and there's more interesting,
uh,

79
00:04:51,410 --> 00:04:53,260
I'm not going to talk about this too much,

80
00:04:53,261 --> 00:04:55,660
but there's quite a bit of
a few papers about that.

81
00:04:56,380 --> 00:05:00,190
So that's the first hypothesis
that optimize optimization
is harder and this is

82
00:05:00,191 --> 00:05:03,220
made harder in particular by
the fact that hidden units,

83
00:05:03,221 --> 00:05:07,120
saturate and block gradients when we
back propagate them towards the, uh,

84
00:05:07,150 --> 00:05:08,950
all the lower hidden layers

85
00:05:11,470 --> 00:05:16,290
is second possibility is that we are
actually not in or fitting situation.

86
00:05:16,291 --> 00:05:21,291
But in overfitting situation that is
when we're considering the possibility of

87
00:05:21,911 --> 00:05:23,260
having many hidden layers.

88
00:05:23,440 --> 00:05:27,790
It means that we're exploring a
more complex space of functions.

89
00:05:28,060 --> 00:05:30,700
So functions are more and more
complicated and there are many,

90
00:05:30,701 --> 00:05:35,701
many more of them that are functions as
we add layers and a possible functions

91
00:05:36,491 --> 00:05:39,810
we can, we can model.
And also just one way of,

92
00:05:39,811 --> 00:05:44,811
of sort of informally measuring the
complexity of a space of functions is to

93
00:05:45,041 --> 00:05:48,040
consider the number of parameters
if these functions are parametrized.

94
00:05:48,080 --> 00:05:50,950
So in the deep neural net,
the parameters are all the connections.

95
00:05:51,250 --> 00:05:56,050
And of course when we add more layers we
have more parameters. So, um, you know,

96
00:05:56,051 --> 00:06:00,740
we can expect that we'd get
some overfitting problems
where we'd be in this, uh,

97
00:06:00,770 --> 00:06:02,840
hi Varun,
slow bias situation.

98
00:06:03,200 --> 00:06:08,200
Where are the family of deep networks
corresponds to a very large space of

99
00:06:08,841 --> 00:06:12,230
possible functions and
forgiven training set. Uh,

100
00:06:12,440 --> 00:06:14,000
there'd be many function that would reach,

101
00:06:14,001 --> 00:06:18,360
say zero error or close
to zero error and uh, and,

102
00:06:18,370 --> 00:06:23,060
and also if we varied the training set
because our space of functions is very,

103
00:06:23,061 --> 00:06:25,670
very complex,
it can adapt very easily.

104
00:06:25,671 --> 00:06:29,720
Then we actually get very many different,
uh,

105
00:06:29,750 --> 00:06:32,670
neural networks as we say,
varied the training center.

106
00:06:32,750 --> 00:06:34,430
So this would correspond
to a high variance.

107
00:06:34,431 --> 00:06:38,570
We have low bias because neural nets
have a lot of capacity, deep neural nets,

108
00:06:38,840 --> 00:06:42,400
perhaps even more capacity.
So they, for any training set,

109
00:06:42,670 --> 00:06:46,490
it's actually able to reach a
very low training error, uh,

110
00:06:46,610 --> 00:06:48,020
because of that high capacity.

111
00:06:48,380 --> 00:06:52,880
And so we might be in a situation where
the variance is too high. Uh, and, uh,

112
00:06:53,000 --> 00:06:56,990
even though we have low bias and thus
we were not great at generalization and

113
00:06:56,991 --> 00:06:58,430
thus were overfitting.

114
00:07:01,390 --> 00:07:04,700
So these two problems are
actually quite different,

115
00:07:04,701 --> 00:07:06,830
but depending on the situation,

116
00:07:06,860 --> 00:07:11,420
the intuition is that there's one
that dominates over the other. Um,

117
00:07:11,480 --> 00:07:16,480
so if we are under fitting and there
are ways we can try to measure this,

118
00:07:16,581 --> 00:07:19,450
if the training error is very
close to the validation set there,

119
00:07:19,480 --> 00:07:23,630
then perhaps we'll under fitting then one
solution is to use better optimization

120
00:07:23,631 --> 00:07:25,700
methods than just to
cast a great in dissent.

121
00:07:25,790 --> 00:07:30,790
And this isn't current active area of
research as in the right now in 2013.

122
00:07:31,820 --> 00:07:32,450
Uh,

123
00:07:32,450 --> 00:07:35,810
and I'm not going to talk too much about
this because I don't want to talk too

124
00:07:35,811 --> 00:07:38,450
much about optimization in this class,

125
00:07:38,451 --> 00:07:42,680
but there are references on the
website of the, uh, of discourse.

126
00:07:43,340 --> 00:07:46,850
And we've got to talk more about the
second hypothesis, uh, over fitting,

127
00:07:46,851 --> 00:07:49,490
which in my experience tends to,
uh,

128
00:07:49,520 --> 00:07:53,870
be more prevalent in problems where say
we're doing, we're doing classification,

129
00:07:53,871 --> 00:07:56,480
for instance,
which is the main topic of this class.

130
00:07:57,140 --> 00:08:01,940
And now the approach is essentially
going to be to use better reclamation

131
00:08:01,941 --> 00:08:04,970
methods, uh, for deep neural networks.

132
00:08:05,210 --> 00:08:08,120
And we'll look at two types of
virtualization approaches. One,

133
00:08:08,121 --> 00:08:11,210
which is based on the corporation of
unsupervised learning and the training

134
00:08:11,211 --> 00:08:15,940
procedure. And another one, which is
based on the use of so-called dropout, uh,

135
00:08:16,100 --> 00:08:18,680
training, which is also can be, uh,

136
00:08:18,830 --> 00:08:22,190
which we can understand as a way of
improving regularization in deep neural

137
00:08:22,191 --> 00:08:22,700
networks.

