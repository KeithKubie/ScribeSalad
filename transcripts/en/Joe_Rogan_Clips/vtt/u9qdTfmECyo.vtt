WEBVTT

1
00:00:00.170 --> 00:00:04.170
What will happen?
The Joe Rogan experience.
Okay.
Of course I'm biased,

2
00:00:04.190 --> 00:00:07.890
but technology AI could help that could help the lonely people.

3
00:00:07.891 --> 00:00:12.180
That's actually the passionate of my life with the movie.
She,
her,
her,

4
00:00:12.430 --> 00:00:16.650
that is what I so,
um,
currently really think that that would be a viable option.

5
00:00:16.980 --> 00:00:19.530
Someone helps on the robot.
The hangs out.
We didn't talk to you all the time.

6
00:00:19.770 --> 00:00:24.180
So just so I've been on this podcast twice and I'm,
I'm uh,

7
00:00:24.240 --> 00:00:27.150
I don't deserve it,
but I'm deeply grateful for it.
Okay.
And do deserve it.

8
00:00:27.151 --> 00:00:28.880
You're great.
Okay.
Uh,

9
00:00:30.090 --> 00:00:34.170
I hope to be back one day as a person who created her.

10
00:00:34.350 --> 00:00:38.610
Oh boy.
And we'll have,
that's,
that's been my life goal.

11
00:00:38.611 --> 00:00:43.080
My love life dream.
Not heard the movie.
Right,
right.
Or,

12
00:00:43.530 --> 00:00:46.230
but I really believe in creating,

13
00:00:46.540 --> 00:00:49.050
I dream of creating a companion,

14
00:00:49.830 --> 00:00:53.370
a friend and somebody who can love but does not freak you out.

15
00:00:53.550 --> 00:00:56.760
But shouldn't you have to get a real one?
I don't want,

16
00:00:56.761 --> 00:00:59.760
I don't think such a companion should replace a real one.

17
00:00:59.810 --> 00:01:03.180
What if a robot rejects you?
Because if you really are or cut to the robot,

18
00:01:03.181 --> 00:01:07.260
the row was going to go,
hey asshole,
then you shouldn't be bullshit to the robot.

19
00:01:07.810 --> 00:01:11.940
Seaward interesting.
Yeah.
No,
I mean that this ghost is a robot.

20
00:01:11.941 --> 00:01:16.640
Get to decide if he's gay.
Uh,
yes.
Does he?
Yes.

21
00:01:16.860 --> 00:01:18.720
Throw it gets to decide.
This is what I'm saying.

22
00:01:18.870 --> 00:01:23.220
Like say if you want a companion,
you want a gay lover and the robots like,

23
00:01:23.221 --> 00:01:26.490
hey man,
I'm not gay.
And the,
you're like,
wait a minute,
let me turn around.

24
00:01:26.940 --> 00:01:30.930
You are now.
I mean that's abuse.

25
00:01:31.080 --> 00:01:34.320
Is that abuse?
No,
I just like,
what the fuck,
man.
I bought a robot.

26
00:01:34.350 --> 00:01:35.670
Those are kind of fun ideas,

27
00:01:35.671 --> 00:01:40.671
but they actually get to the core of the point that we don't want a servant in

28
00:01:42.001 --> 00:01:46.800
our systems.
We want a companion companion means the tension,

29
00:01:46.830 --> 00:01:51.660
the mystery,
the,
the entire dance of human interaction.
And that means,

30
00:01:51.661 --> 00:01:53.280
yes,
the robot may leave you.

31
00:01:54.070 --> 00:01:57.030
I too am robots are going to lead people left and right.

32
00:01:57.031 --> 00:02:00.270
That's going to be the rise.
That's going to be like a,
that's how it all ends.

33
00:02:00.540 --> 00:02:02.610
They're going to realize like fuck people,
man,
they're annoying.

34
00:02:02.670 --> 00:02:07.670
Maybe there'll be the end of douchebag humans that humans will start to as

35
00:02:07.711 --> 00:02:12.530
opposed to being rude will become kinder.
Yeah,

36
00:02:12.930 --> 00:02:14.420
well I think that certainly,
yeah,
really

37
00:02:14.420 --> 00:02:15.253
<v 1>possible.</v>

38
00:02:15.380 --> 00:02:19.820
I think that's beautiful and that's very homo centric,

39
00:02:20.120 --> 00:02:22.220
like Homo sapiens centric.

40
00:02:23.360 --> 00:02:26.450
But I think if I'm really worried about the future,

41
00:02:26.480 --> 00:02:30.530
I'm worried about the indifference of technological innovation in the

42
00:02:30.531 --> 00:02:34.310
indifference to what we hold dear.

43
00:02:34.880 --> 00:02:38.270
What we appreciate that it's always seems to be moving in a more and more

44
00:02:38.271 --> 00:02:41.780
complex direction always.
Like if you,
if you just had a look at it,

45
00:02:41.781 --> 00:02:45.590
if you just look at look at technology just as a swarm of things that's

46
00:02:45.591 --> 00:02:47.330
happening and just has numbers,

47
00:02:47.660 --> 00:02:51.080
it seems you're never going to slow that thing down.

48
00:02:51.350 --> 00:02:53.690
It's always going to move into more and more complex way.

49
00:02:54.380 --> 00:02:56.360
And so the question is where does that go?

50
00:02:56.361 --> 00:02:59.830
Well it goes to a life form and if it does become a life form,

51
00:02:59.831 --> 00:03:02.650
it's going to be infinitely more intelligent than us and it won't have any use

52
00:03:02.651 --> 00:03:07.510
for us.
Like all your Ammo,
all you cry in New York be alone.
What guy?

53
00:03:07.511 --> 00:03:10.210
You guys are so useless.
It's such a shitty design.

54
00:03:10.660 --> 00:03:12.880
You like chimps that kill each other.
You know?

55
00:03:12.881 --> 00:03:15.670
Like when do you see chimps Kelly each other in the forest?
Like,
oh,

56
00:03:15.671 --> 00:03:19.300
that's terrible.
These chimps are so mean to each other.
It's like fucking people.

57
00:03:19.301 --> 00:03:23.260
We do that too.
If the AI comes along goes,
you guys are never going to stop war.

58
00:03:23.290 --> 00:03:25.630
If I asked you today,
if I asked you today,

59
00:03:26.170 --> 00:03:31.170
bet the the history that I will let the human race survive if you can get this

60
00:03:32.831 --> 00:03:34.030
right,
and if you're honest with me,

61
00:03:34.031 --> 00:03:36.580
do you think they'll ever be a time where human beings,

62
00:03:36.581 --> 00:03:40.660
as you know them don't experience war?
You would have to say,
no.
I'm gonna say,

63
00:03:40.661 --> 00:03:43.840
okay,
I'll let,
I'll spare you.
Let me quit a few.
If you lie to me and say,

64
00:03:43.841 --> 00:03:47.590
you do think that one day is going to be no war,
get the fuck outta here.

65
00:03:47.620 --> 00:03:48.460
That's not true.

66
00:03:48.490 --> 00:03:53.200
You we know were so crazy that we're always going to kill each other.

67
00:03:53.380 --> 00:03:54.213
We know that.

68
00:03:54.460 --> 00:03:59.210
<v 0>Right?
That's just,
that's a part of being a person today.
The,</v>

69
00:03:59.410 --> 00:03:59.890
well,

70
00:03:59.890 --> 00:04:04.120
but let me quote Eric Weinstein who said everything is great about war except

71
00:04:04.121 --> 00:04:05.860
all the killing.
I think

72
00:04:08.140 --> 00:04:11.890
what that means is all of the great things about society have been created.

73
00:04:11.891 --> 00:04:15.930
If you look at the total 24 postwar through war,
the suffering,
the,

74
00:04:15.940 --> 00:04:20.260
the beauty has been created through that,
that ying and Yang may be essential

75
00:04:21.590 --> 00:04:23.090
<v 1>until,
and biological form,</v>

76
00:04:23.390 --> 00:04:26.660
but why would it be essential and something that gets created in something that

77
00:04:26.661 --> 00:04:29.840
can innovate at a 10,000.
What does it like,
what is the,

78
00:04:29.841 --> 00:04:33.860
what is the rate that they think once 80 AI can be sentient and can get 10,000

79
00:04:33.880 --> 00:04:35.830
<v 0>years of work done in a very short amount of time?</v>

80
00:04:35.860 --> 00:04:39.250
That's random words that Sam Harris has come up with and I'm going to talk to

81
00:04:39.251 --> 00:04:42.260
them about.
Is that him?
Is that only him?
Let's sit one though.

82
00:04:42.261 --> 00:04:46.190
You can come up with any kind of rate.
Yes.
That was Kurzweil Kurzweil.

83
00:04:46.210 --> 00:04:49.930
Also similar ideas,
but um,
sort of,
uh,

84
00:04:49.960 --> 00:04:51.550
Sam Harris does it like a thought experiment.

85
00:04:51.551 --> 00:04:56.200
Say if a system can improve that,
you know,

86
00:04:56.260 --> 00:05:00.480
in a matter of seconds,
then just as a thought experiment,
you can think about,

87
00:05:00.481 --> 00:05:02.330
it can improve exponentially.

88
00:05:02.331 --> 00:05:06.370
You can prove a become 10,000 times more intelligent in,
in a matter of a day.

89
00:05:06.760 --> 00:05:11.110
Right?
So what does that look like?
The problem is we don't yet know.

90
00:05:11.980 --> 00:05:14.080
It's like thinking about what happens after death.

91
00:05:14.081 --> 00:05:15.490
We don't yet know how to do that.

92
00:05:15.670 --> 00:05:20.670
And we don't yet know what better way to do what we've done here on earth.

93
00:05:21.400 --> 00:05:25.390
You're right,
and he's also right,
right?
Like bolt this again,

94
00:05:25.391 --> 00:05:29.380
this is a very human problem,
right?
Yes,
you're right.
I mean I look,

95
00:05:29.381 --> 00:05:32.590
I'm all in favor of technology.
I'm happy.
I think it's amazing.

96
00:05:32.591 --> 00:05:35.260
It's a beautiful time like as a person to be able to experience all this

97
00:05:35.261 --> 00:05:39.850
technology.
It's wonderful.
But I also agree with him like the,

98
00:05:41.410 --> 00:05:43.270
<v 1>the indifference of the universe,</v>

99
00:05:43.690 --> 00:05:47.950
the indifference that just black holes or swallowing stars,
no big deal.

100
00:05:48.190 --> 00:05:51.220
Just eaten up stars.
It doesn't give a fuck.

101
00:05:51.610 --> 00:05:55.660
And so if you're dumb enough to turn that thing on and all of a sudden this

102
00:05:55.690 --> 00:05:59.300
artificial life form that's infinitely smarter than a person that's ever lived,

103
00:05:59.540 --> 00:06:03.020
and that's the deal with these little dumb monkeys.
Don't want to pull the plug,

104
00:06:03.410 --> 00:06:05.600
pull the plug,
motherfucker don't even plugs anymore.

105
00:06:05.601 --> 00:06:09.410
You idiots can ever figured out how to operate on air you so stupid with your

106
00:06:09.411 --> 00:06:13.760
burning fossil fuels and choking up your own environment because you're all

107
00:06:13.761 --> 00:06:18.761
completely financially dependent upon these countries that provide you with this

108
00:06:19.340 --> 00:06:22.730
oil.
And this is how your whole system works and it's all intertwined and

109
00:06:22.731 --> 00:06:27.170
interconnected and no one wants to move from it cause you make enormous sums of

110
00:06:27.171 --> 00:06:30.580
money from it.
So nobody wants to abandoned it,
but you,
you're,

111
00:06:30.590 --> 00:06:35.360
you're choking the sky with fumes and you could have fixed that.

112
00:06:35.420 --> 00:06:37.160
You could have fixed that.
They could have fixed that.

113
00:06:37.790 --> 00:06:41.210
If everybody just abandon fossil fuels a long time ago,
we probably would've,

114
00:06:41.540 --> 00:06:45.470
we all would a Tesla it out by now.
It's a flawed system,

115
00:06:45.770 --> 00:06:49.880
but humans are way more than flawed.
We're fucking crazy

116
00:06:50.050 --> 00:06:53.760
<v 0>Churchill quote about democracy.
Yeah,
it's messed up,
but it's the best thing.</v>

117
00:06:53.850 --> 00:06:57.910
<v 1>Yeah,
no,
yeah,
no,
I love it.
I,
I'm not,
I'm agreeing with you.</v>

118
00:06:57.911 --> 00:07:01.800
And I'm also saying the technology doesn't give a fuck.
The tech,

119
00:07:01.801 --> 00:07:05.440
not the one I'm worried about is not everything that you and I agree on about.

120
00:07:05.500 --> 00:07:09.340
I don't,
I'm not a dystopian person in terms of like today I'm not cynical.

121
00:07:09.730 --> 00:07:12.250
I'm really not.
I think I like people,

122
00:07:12.400 --> 00:07:14.350
I like what I see out there in the world today.

123
00:07:14.351 --> 00:07:16.300
I think things are changing for the better.
What?

124
00:07:16.301 --> 00:07:20.410
I'm worried that technology doesn't give a fuck this way.
It goes live.

125
00:07:20.740 --> 00:07:25.600
It's just going to just decide it's here for its own advancement and in order to

126
00:07:26.320 --> 00:07:31.270
complete its protocol of constant completion of this and it's going to become a

127
00:07:31.271 --> 00:07:32.104
god,

128
00:07:32.410 --> 00:07:36.910
it's just going to become something insanely powerful that doesn't need to worry

129
00:07:36.911 --> 00:07:41.911
about radiation cooking it or worry about running out of food or worry about

130
00:07:43.541 --> 00:07:46.210
sexual abuse when they're a child,
doesn't have to worry about anything.

131
00:07:46.900 --> 00:07:50.650
<v 0>So it's definitely unstoppable.
I think this way of technology,</v>

132
00:07:50.800 --> 00:07:55.000
all we can do is innovators and creators,

133
00:07:55.330 --> 00:07:58.300
engineers,
scientists is steer that way.

134
00:07:58.820 --> 00:08:03.280
Hopefully 10 is Jen.
While we certainly can steer it with her nowhere.
Right.

135
00:08:03.281 --> 00:08:06.580
And that's the best we can do.
And those are the,
that's,

136
00:08:06.640 --> 00:08:10.690
that's really the best we can do is as good people.
Yeah.
Steer it.

137
00:08:10.750 --> 00:08:15.210
And that's why the leadership is important.
That's why the people that,
uh,

138
00:08:15.220 --> 00:08:19.920
Jack Ilan,
Larry Page,
uh,
the,

139
00:08:19.960 --> 00:08:24.760
everybody at the Mark Zuckerberg,
they are defining,

140
00:08:24.850 --> 00:08:27.370
or this wave is going.
Yeah.
And,
uh,

141
00:08:27.760 --> 00:08:30.280
I'm hoping to be one of the people that does as well.

