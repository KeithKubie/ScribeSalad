1
00:00:00,780 --> 00:00:03,840
You are listening to radiolab radiolab

2
00:00:05,700 --> 00:00:07,680
from W N y.

3
00:00:11,460 --> 00:00:15,030
S. E. N. P. Are you ready Robert? Hey,

4
00:00:15,060 --> 00:00:19,560
I'm Jad Abumrad and I'm Robert
Krulwich. This is radiolab, the podcast,

5
00:00:19,600 --> 00:00:23,760
the podcast today on radio
lab. We are the, well,

6
00:00:23,910 --> 00:00:25,170
I'd actually don't even
know what we're doing.

7
00:00:25,410 --> 00:00:28,140
I know we're revisiting some old
questions, but you've kept me in the dark.

8
00:00:28,141 --> 00:00:29,670
So what are we doing today on the podcast?

9
00:00:29,700 --> 00:00:32,370
Well today we're going to
chicken with who? Who's here?

10
00:00:32,970 --> 00:00:36,630
Somebody who you might remember actually.
Oh, Hi Robert. How are you doing?

11
00:00:36,870 --> 00:00:38,400
Can you just tell me who you are?

12
00:00:38,730 --> 00:00:41,630
Just say I'm Josh and where you
are and what you do and style.

13
00:00:41,660 --> 00:00:42,990
I am Joshua Green.

14
00:00:43,200 --> 00:00:46,570
I am an assistant professor of
psychology at Harvard University. And,

15
00:00:46,590 --> 00:00:51,090
and you may remember second
grow and in the morality show.

16
00:00:51,570 --> 00:00:53,700
Sure. I mean Josh was the
guy with the moral puzzles.

17
00:00:53,730 --> 00:00:56,250
I study moral judgment
and decision making.

18
00:00:56,430 --> 00:01:00,000
Are you going to get into the hole baby?
Would you kill your baby question? Yes,

19
00:01:00,001 --> 00:01:00,900
exactly. Exactly.

20
00:01:01,260 --> 00:01:04,890
So for those of you who need to follow
this in that earlier radio that we

21
00:01:05,810 --> 00:01:07,950
described, the last episode
of the TV show Mash,

22
00:01:09,040 --> 00:01:11,590
it's wartime isn't any
patrol coming down the road.

23
00:01:11,910 --> 00:01:14,860
You are hiding in the basement
with some of your fellow villagers.

24
00:01:15,040 --> 00:01:19,060
Let's kill those lights. And
the enemy soldiers are outside.

25
00:01:19,270 --> 00:01:22,240
They have orders to kill
anyone that they find quiet.

26
00:01:22,990 --> 00:01:26,890
Nobody make a sound until the
[inaudible]. So there you are here

27
00:01:26,940 --> 00:01:28,440
huddled in the basement,

28
00:01:28,980 --> 00:01:33,840
all around your enemy troops and
you're holding your baby in your arms,

29
00:01:35,850 --> 00:01:38,430
your baby with a cold, a bit of a sniffle,

30
00:01:38,940 --> 00:01:43,340
and you know that your baby could
cost at any moment. [inaudible]

31
00:01:43,740 --> 00:01:47,070
they hear your baby, they're gonna find
you and the baby and everyone else,

32
00:01:47,071 --> 00:01:48,180
and they're going to kill everybody.

33
00:01:49,170 --> 00:01:52,440
And the only way you can stop this from
happening is cover the baby's mouth.

34
00:01:53,490 --> 00:01:55,590
But if you do that, the baby's
going to smother and die.

35
00:01:56,010 --> 00:01:57,450
If you don't cover the baby's mouth,

36
00:01:57,720 --> 00:02:00,090
soldiers are going to find everybody
and everybody's going to be killed,

37
00:02:00,091 --> 00:02:03,570
including you, including your
baby, and you have the choice.

38
00:02:04,380 --> 00:02:09,380
Would you smother your own baby to save
the village or would you like your baby

39
00:02:09,811 --> 00:02:14,480
cough? Knowing the consequences
and, and, and, and,

40
00:02:15,290 --> 00:02:18,870
and, and it may clear for me why, where
we're going with this, Robert, like,

41
00:02:18,871 --> 00:02:19,704
I don't know.

42
00:02:21,240 --> 00:02:26,240
You asked me a question at the time
and how many people chose to kill their

43
00:02:26,941 --> 00:02:31,860
baby? About half. Wow. That's not bad.

44
00:02:32,010 --> 00:02:35,460
What do you mean slept? You're in
favor of killing the baby. Well,

45
00:02:35,470 --> 00:02:38,790
what would you do? Me,
I would never, I would,

46
00:02:38,850 --> 00:02:41,880
I wouldn't even consider
kill the baby. You would,

47
00:02:42,090 --> 00:02:46,340
the village will go on to have a
hundred babies. Your babies. Just one.

48
00:02:46,890 --> 00:02:49,320
My baby is my world,
my babies, my universe.

49
00:02:49,480 --> 00:02:53,490
So you're going to erase all those
people based on your one child.

50
00:02:54,600 --> 00:02:54,961
First of all,

51
00:02:54,961 --> 00:02:58,470
the audience should know that [inaudible]
not does not have a child of his own.

52
00:02:58,740 --> 00:03:01,060
Yes. Okay. Now,

53
00:03:01,120 --> 00:03:05,920
now we have the benefit of time passing.
Yeah. Just out of sheer curiosity,

54
00:03:05,921 --> 00:03:09,490
now that you have a child
and you've looked into that
child's face over and over

55
00:03:09,491 --> 00:03:12,550
and over again, I'm
just curious, would you,

56
00:03:12,770 --> 00:03:16,420
is there any other reason why you're
doing no, no, no, no, no, no, no, no.

57
00:03:16,421 --> 00:03:18,220
I'm going to know you and that
young people shouldn't worry,

58
00:03:18,400 --> 00:03:21,010
but just out of curiosity,
what would you do?

59
00:03:22,300 --> 00:03:25,840
But I killed the baby when I your
baby, not a baby. You are baby

60
00:03:27,670 --> 00:03:28,503
[inaudible].

61
00:03:29,710 --> 00:03:33,260
Would you like to see a little picture
of him? No, I don't want to see a photo.

62
00:03:33,290 --> 00:03:36,530
I know what a meal looks like
for crying out loud. So just,

63
00:03:36,790 --> 00:03:38,430
no, see, here's, I,

64
00:03:38,690 --> 00:03:41,320
I have thought about this actually because
people send us emails about this for

65
00:03:41,321 --> 00:03:46,060
some reason. Um, um, I don't really know.

66
00:03:46,061 --> 00:03:49,720
I mean the thing is though, I mean now
there's not just like an abstract baby,

67
00:03:49,721 --> 00:03:53,740
but it's my baby. Well that does
change everything obviously.

68
00:03:53,741 --> 00:03:57,580
So I'm kind of in a place where I don't
really know that frankly, don't know.

69
00:03:59,030 --> 00:04:00,380
Wait, let me just think about this.

70
00:04:04,070 --> 00:04:04,903
I Dunno.

71
00:04:06,350 --> 00:04:09,770
It's kind of an impossible question cause
like in order to answer it truthfully,

72
00:04:09,771 --> 00:04:14,060
which is I would not kill my baby,
I'd have to sacrifice a principle,

73
00:04:14,061 --> 00:04:18,590
which is like not as important
to me as my baby. But almost

74
00:04:18,780 --> 00:04:20,160
that principle being,

75
00:04:20,320 --> 00:04:25,320
well it's sometimes you have to sacrifice
something very dear for the greater

76
00:04:25,961 --> 00:04:29,680
good. I just think that's a really, I
mean, not to get all communistic on you,

77
00:04:29,681 --> 00:04:30,960
but that's a really important idea to me.

78
00:04:31,030 --> 00:04:35,260
And in this case, by the way, the, the
calculus of what is about to happen,

79
00:04:35,261 --> 00:04:37,240
if the baby cost is
really not known to you.

80
00:04:37,270 --> 00:04:38,290
Well, I mean, if you, you know,

81
00:04:38,291 --> 00:04:40,390
if you're the philosopher king
and you give me two options,

82
00:04:40,391 --> 00:04:44,830
one is to kill my baby, to save the
village or to allow my baby to live,

83
00:04:44,831 --> 00:04:48,020
in which case everybody dies. If
those were the only two options,

84
00:04:48,540 --> 00:04:51,470
then I still feel like you
kind of have to kill the baby.

85
00:04:53,170 --> 00:04:53,290
[inaudible] [inaudible]

86
00:04:53,290 --> 00:04:56,590
but I don't think I could do that. I
don't think any father could do that.

87
00:04:57,190 --> 00:05:01,060
So th my sort of pathetic answer at
this point is I can't kill my baby,

88
00:05:01,510 --> 00:05:05,090
but then I can't sacrifice the
village. So I think I would just, um,

89
00:05:05,530 --> 00:05:09,680
like close my eyes and
wish I was [inaudible]

90
00:05:09,710 --> 00:05:10,543
where else?

91
00:05:11,280 --> 00:05:13,830
So the idea is that, you know,
when you think about this case,

92
00:05:13,920 --> 00:05:17,550
on the one hand you have an intuitive
emotional response that says, no,

93
00:05:17,551 --> 00:05:20,610
this is terrible killing a baby or
killing my own baby. Even worse,

94
00:05:20,940 --> 00:05:24,090
at the same time, a different system
within your brain is saying, look,

95
00:05:24,330 --> 00:05:27,000
this is horrible as this is,
this is a sensible thing to do.

96
00:05:27,001 --> 00:05:30,060
It's the only sensible thing to do because
if you do nothing, everyone will die.

97
00:05:30,270 --> 00:05:33,480
Whereas if you kill the baby, then at
least you and the other people can live.

98
00:05:33,750 --> 00:05:34,570
And uh,

99
00:05:34,570 --> 00:05:38,580
w what the evidence suggests is that
these two competing moral perspectives are

100
00:05:38,581 --> 00:05:40,750
really grounded in different
parts of the brain. The,

101
00:05:40,830 --> 00:05:42,570
and the competition is not been resolved.

102
00:05:42,740 --> 00:05:44,930
That's where we were the last time. Right?

103
00:05:44,931 --> 00:05:49,490
Now I want to step forward for a second
and think about it a little more deeply.

104
00:05:49,690 --> 00:05:50,090
All right.

105
00:05:50,090 --> 00:05:54,710
If our sense of right and wrong comes
from like these competing brain systems.

106
00:05:55,460 --> 00:05:58,010
Let me revisit the question.
Are Our brains built

107
00:05:58,010 --> 00:06:00,440
to fever? Certain outcomes.

108
00:06:00,860 --> 00:06:05,860
Let's suppose that you are
walking alongside a lake
and you see a girl drowning

109
00:06:06,321 --> 00:06:08,270
right in front of you and
she's screaming for help,

110
00:06:08,300 --> 00:06:10,400
but you're wearing a very expensive suit.

111
00:06:10,880 --> 00:06:15,740
Should you jump into the
lake and saver? No, no,

112
00:06:15,741 --> 00:06:17,120
of course. Of course you should. Yes.

113
00:06:17,720 --> 00:06:20,350
You mean like the suit is the only thing
that's preventing me from doing that?

114
00:06:20,360 --> 00:06:21,193
Yeah,

115
00:06:21,290 --> 00:06:24,320
but now suppose you're walking down past
your mailbox and there's a letter in

116
00:06:24,321 --> 00:06:24,861
the mailbox,

117
00:06:24,861 --> 00:06:29,720
which is please give us $1,000 so we
can help save girls on the other side of

118
00:06:29,721 --> 00:06:32,780
the globe. Girls, you'll never meet
girls who screams. You'll never here,

119
00:06:33,200 --> 00:06:37,010
but there are girls in trouble on the
other side of the world. Go help them.

120
00:06:37,100 --> 00:06:39,950
And so wait until the, the equivalence
is that you jump into the lake.

121
00:06:39,951 --> 00:06:43,790
You say if the girl who's
drowning one-on-one or you
send the check and you save

122
00:06:43,791 --> 00:06:48,740
the girl who is in peril,
a girl, not that girl,

123
00:06:48,770 --> 00:06:50,660
a girl somewhere on the
other side of the globe.

124
00:06:51,170 --> 00:06:55,760
So the question that go to Josh, if
you didn't give the thousand dollars,

125
00:06:55,761 --> 00:06:58,280
would that make you a bad guy? Well,

126
00:06:58,490 --> 00:07:01,720
there is something funny about
these cases, right? That,

127
00:07:01,721 --> 00:07:05,570
that most of us say that of course
you have to rescue the drowning child,

128
00:07:05,780 --> 00:07:09,550
but you know, it's, you're not a saint
if you don't give your money over to, to,

129
00:07:09,551 --> 00:07:11,510
to save the children on the
other side of the world,

130
00:07:11,511 --> 00:07:15,050
but you're certainly not a terrible
person or so it seems to us. And so, yes,

131
00:07:15,051 --> 00:07:18,440
there's this, there's this, uh, putting
aside whether it's a good or bad,

132
00:07:18,441 --> 00:07:22,250
whether you're a good or bad person,
how do you explain the difference? Well,

133
00:07:22,251 --> 00:07:25,940
I think it makes a lot of evolutionary
sense. Uh, that is, you know,

134
00:07:26,060 --> 00:07:31,060
a lot of our social emotional responses
are geared towards life in the kind of

135
00:07:31,791 --> 00:07:33,740
environment in which
our ancestors evolved.

136
00:07:34,030 --> 00:07:38,200
And it makes sense that we would
have a moral buttons so to speak.

137
00:07:38,201 --> 00:07:42,160
They get pushed by the kinds of things
that our ancestors might've encountered

138
00:07:42,340 --> 00:07:45,490
cause tens of thousands of
years of evolution have have
essentially been quietly

139
00:07:45,491 --> 00:07:48,580
tugging at your heart in both in those
kinds of seats actually. Exactly.

140
00:07:48,581 --> 00:07:49,480
Where is the idea?

141
00:07:49,481 --> 00:07:54,481
The idea of spending a minimal amount of
money to save the life of some stranger

142
00:07:54,611 --> 00:07:56,380
on the other side of the
world that you've never,

143
00:07:56,560 --> 00:07:58,360
that you're never going to meet. Um,

144
00:07:58,840 --> 00:08:01,630
that's a totally new modern phenomenon.

145
00:08:01,660 --> 00:08:04,270
It's not something that our
emotions are our prepared for.

146
00:08:05,410 --> 00:08:09,730
Well now doesn't that leave us in a
funny place because I think it does.

147
00:08:09,790 --> 00:08:14,320
What happens if the most
important questions that we
face as a species or as a

148
00:08:14,321 --> 00:08:18,640
group involve thinking
abstractly those problems,

149
00:08:18,641 --> 00:08:21,250
pollution, global warming
and things like that.

150
00:08:21,280 --> 00:08:25,480
Those aren't really local
problems. They're global problems.

151
00:08:25,540 --> 00:08:26,800
Exactly. This is,

152
00:08:26,801 --> 00:08:29,860
I think that gets right at the heart
of the matter and this is why I do this

153
00:08:29,861 --> 00:08:33,060
research. I think that the kind of
thinking that we apply to those problems,

154
00:08:33,061 --> 00:08:37,650
what we call common sense is really hunter
gatherer or common sense or at least

155
00:08:37,651 --> 00:08:39,930
a lot of it is. And uh,

156
00:08:40,200 --> 00:08:44,340
if we're going to face these big problems
that were our minds were not designed

157
00:08:44,341 --> 00:08:45,480
by evolution to handle,

158
00:08:45,660 --> 00:08:49,350
then we have to learn to turn off parts
of our brain that are getting in the way

159
00:08:49,351 --> 00:08:52,710
and turn on other parts that may seem
like the wrong parts to be using.

160
00:08:52,800 --> 00:08:56,370
So he's saying that we
should tamp down primitive

161
00:08:56,370 --> 00:09:00,690
emotional instincts that are in our
reptile brain. Let those instincts say,

162
00:09:00,930 --> 00:09:04,740
don't kill your baby like that
stuff. And then we should,

163
00:09:04,770 --> 00:09:06,870
we should amp up somehow the, uh,

164
00:09:07,710 --> 00:09:10,950
the part of us that thinks more abstractly
about the greater good and about

165
00:09:10,951 --> 00:09:12,240
people that aren't right in front of it.

166
00:09:12,350 --> 00:09:16,280
Yeah. So, so if you're sitting there with
a soda can in your hand and you think,

167
00:09:16,281 --> 00:09:18,950
oh, I guess I can just throw this on
the street and you'll get clank, clank,

168
00:09:19,100 --> 00:09:21,590
clank of the clinic, your
primitive parsing. Well,

169
00:09:21,591 --> 00:09:23,270
I can get away with that
cause no one's seeing it.

170
00:09:23,271 --> 00:09:24,770
But of course the
calculating part would say,

171
00:09:24,771 --> 00:09:28,400
well if we all do this then the
world will be full of trash.

172
00:09:28,401 --> 00:09:30,440
And it's problems like that
that in order to solve that,

173
00:09:30,441 --> 00:09:31,620
you have to think abstractly

174
00:09:32,330 --> 00:09:36,330
why that's interesting.
Because we might be, I mean,

175
00:09:36,331 --> 00:09:38,840
I think he might be wrong. I mean,
cause we didn't countered this already.

176
00:09:38,841 --> 00:09:41,810
He's asking us to rely on a part
of our brain that, you know,

177
00:09:42,290 --> 00:09:46,270
is not exactly Hercules'. Do you remember
the thing we talked about in the,

178
00:09:46,271 --> 00:09:50,360
in the show is that Soren, was
it the, the choice show, uh,

179
00:09:50,361 --> 00:09:53,540
with the Bobby Shiv. Can we get that
audio and throw that into the mix?

180
00:09:54,660 --> 00:09:57,700
I'm a Baba share. I'm a
professor here at, uh,

181
00:09:57,720 --> 00:10:00,330
the Stanford Graduate School
of Business in marketing.

182
00:10:00,420 --> 00:10:05,010
A lot of my research it has to do with
the brain and tricking people. Oh yeah,

183
00:10:05,070 --> 00:10:08,100
absolutely. So Robert,

184
00:10:08,101 --> 00:10:10,590
I want to tell you about one
particular experiment that he did.

185
00:10:10,980 --> 00:10:13,620
So the experiment is pretty
straight forward. There's like this,

186
00:10:13,980 --> 00:10:16,080
you got a bunch of subjects
together. He said, okay,

187
00:10:16,081 --> 00:10:19,020
I'm going to give you all a number,
the number on a little card.

188
00:10:19,021 --> 00:10:21,690
You're going to read the number and I
want you to commit that number to memory.

189
00:10:21,750 --> 00:10:25,140
Take as much time as you want to
memorize the number, and then he says,

190
00:10:25,170 --> 00:10:28,320
you'll know going to walk to the next
room and recalled the number and that's

191
00:10:28,321 --> 00:10:31,380
what subjects think test subjects think
that they're going to be doing so they

192
00:10:31,381 --> 00:10:32,550
know they're going to be in one place.

193
00:10:32,640 --> 00:10:34,980
Getting a number of going into another
place and reciting that number.

194
00:10:34,981 --> 00:10:36,930
That's right. That's all they
know. That's all they know,

195
00:10:37,050 --> 00:10:41,220
but they don't know is that not everybody
is getting the same kind of number.

196
00:10:41,250 --> 00:10:42,680
Some people get a seven digit numbers.

197
00:10:42,681 --> 00:10:44,880
Some people get a two digit
number that I can do. By the way,

198
00:10:45,390 --> 00:10:46,380
I think I can do two digit.

199
00:10:46,500 --> 00:10:47,333
No, I doubt it.

200
00:10:48,720 --> 00:10:52,230
All the subjects have to do is they've
got to memorize a number of walk out of

201
00:10:52,231 --> 00:10:55,260
room one down the hall of room
to then recite their number.

202
00:10:55,261 --> 00:10:58,970
Now just imagine you and me person with
a two digit number in their head and was

203
00:10:58,971 --> 00:11:01,380
walking out of room one. Two is my number.

204
00:11:01,381 --> 00:11:03,180
I can definitely remember
this down the hall,

205
00:11:03,360 --> 00:11:07,470
the same time someone with seven
digits in their head and two eight,

206
00:11:07,480 --> 00:11:09,330
nine walks down the hall.

207
00:11:10,110 --> 00:11:13,350
Now here's where the trickery comes
in as they're walking down the hall,

208
00:11:13,351 --> 00:11:15,510
mid memorizing. All of
a sudden, excuse me,

209
00:11:16,110 --> 00:11:19,980
they pass a lady in the hallway and
she's holding something up to you,

210
00:11:20,180 --> 00:11:24,240
but would you like a
snack? Um, she says here,

211
00:11:24,241 --> 00:11:26,010
have it have a snack. Just as our,

212
00:11:26,011 --> 00:11:28,350
as our way of saying thanks for
participating in this study.

213
00:11:28,560 --> 00:11:29,880
You can have one of two snacks.

214
00:11:29,881 --> 00:11:34,881
You choose between a big fat slice of
chocolate cake or be a nice bowl of fruit

215
00:11:36,751 --> 00:11:40,590
salad. Meanwhile, they've both got
these numbers still in their head.

216
00:11:40,591 --> 00:11:43,500
Now here's the weird thing. When
they finally make their choice,

217
00:11:43,520 --> 00:11:48,150
what would you like? Some yummy cake
or some healthy fruits and the people,

218
00:11:48,180 --> 00:11:51,510
this is crazy. The people with two digits
in their head. You know, I love cake,

219
00:11:51,511 --> 00:11:54,370
but I think I'll take the
fruit. Almost always choose

220
00:11:54,400 --> 00:11:55,240
the fruit healthy.

221
00:11:55,270 --> 00:11:58,750
Whereas the people with seven digits
in their head almost always choose the

222
00:11:58,751 --> 00:12:03,640
cake. You know the cake. I want the cake
and we're talking by huge margins here.

223
00:12:03,670 --> 00:12:06,670
It was significant. I mean, this
was like in some cases a 20,

224
00:12:06,671 --> 00:12:10,720
25 30 point difference. The,
the lesson we took from that,

225
00:12:10,750 --> 00:12:13,060
which is the lesson you
are not telling me now,

226
00:12:13,090 --> 00:12:16,150
is that your rational system,

227
00:12:16,180 --> 00:12:20,560
the hope of humankind,
part of your brain is very,

228
00:12:20,561 --> 00:12:25,561
very suggestible week and almost barely
struggling to manage the situation.

229
00:12:27,610 --> 00:12:32,350
Give us something too much to do. And
Oh man, it just, it each week cake.

230
00:12:32,480 --> 00:12:34,700
I would take a very different
lesson from that study.

231
00:12:34,790 --> 00:12:38,300
Imagine if you told those people who
say, look, here's how your mind works.

232
00:12:38,480 --> 00:12:40,760
When you have to remember,
remember a long number,

233
00:12:40,880 --> 00:12:44,330
it's going to clog up your memory and
it's gonna make it harder for you to

234
00:12:44,331 --> 00:12:48,020
resist the temptation to have chocolate
cake instead of fruit salad. Um,

235
00:12:48,140 --> 00:12:49,250
but I'm telling you this now,

236
00:12:49,251 --> 00:12:53,570
you're armed with the truth about how
your own mind works. Here's a long number.

237
00:12:53,750 --> 00:12:55,310
Go right now.

238
00:12:55,311 --> 00:12:58,220
How many of those people are going to
be able to resist the chocolate cake?

239
00:12:58,370 --> 00:12:59,900
I think a lot more of them are

240
00:13:00,220 --> 00:13:02,140
right. Has anyone done
that? Has anyone said, okay,

241
00:13:02,141 --> 00:13:05,380
I'm sending you down and that is
going to be this siren seductive.

242
00:13:05,381 --> 00:13:09,730
Take handling a temptress and
let's see if you can resist it.

243
00:13:09,820 --> 00:13:10,640
Has that ever been done?

244
00:13:10,640 --> 00:13:12,200
It hasn't. I don't know if it's been done,

245
00:13:12,230 --> 00:13:14,630
but I'm willing to place bets
on how that will turn out.

246
00:13:14,720 --> 00:13:18,870
That is that we can recognize the quirks
and the flaws and the inconsistencies

247
00:13:18,890 --> 00:13:22,130
in our cognitive systems and do
something better. That makes more sense.

248
00:13:23,600 --> 00:13:24,280
[inaudible]

249
00:13:24,280 --> 00:13:26,370
does that, does that a,

250
00:13:26,371 --> 00:13:30,000
is this just blind optimism hiring or
does he have evidence to support this?

251
00:13:30,530 --> 00:13:34,290
Well, one thing that gives me hope
is something called the Flynn effect,

252
00:13:34,470 --> 00:13:35,303
the Flynn effect.

253
00:13:35,580 --> 00:13:39,690
So the Flynn effect is something that
was noticed by a philosopher now,

254
00:13:39,691 --> 00:13:44,210
political scientist named uh, Jim Flynn.
I knew it was going to be Flynn. Yeah.

255
00:13:44,220 --> 00:13:47,090
Who is really surprising if his
name was [inaudible]. That's right.

256
00:13:47,340 --> 00:13:48,121
Know that they, they, they,

257
00:13:48,121 --> 00:13:51,820
they line these things up so that
they make sense of the Flynn effect,

258
00:13:51,840 --> 00:13:52,561
the Flynn effect.

259
00:13:52,561 --> 00:13:55,830
What Flint noticed is that over
the course of the 20th century,

260
00:13:56,190 --> 00:14:00,720
IQ scores kept going up and up and
up in the industrialized world.

261
00:14:00,990 --> 00:14:03,930
So much so that by his estimates,

262
00:14:04,260 --> 00:14:09,260
a person of average intelligence in 1900
would register somewhere near the line,

263
00:14:09,980 --> 00:14:13,590
uh, for mental retardation,
by present standards. Um,

264
00:14:13,740 --> 00:14:17,940
how could this be same test by
the way. So everything, I mean,

265
00:14:17,941 --> 00:14:20,940
that's why it's a bit complicated because
the tests have changed and the norms

266
00:14:21,030 --> 00:14:24,750
have changed, but doing your best to
control for all of that. By his estimates,

267
00:14:24,990 --> 00:14:29,850
we have gained about 30 IQ points as
a society in the last hundred years,

268
00:14:29,851 --> 00:14:32,700
which is enormous. Now, there are a
lot of people who would say, well,

269
00:14:32,701 --> 00:14:37,350
the IQ test really doesn't really tell
you that we're getting smarter or really

270
00:14:37,351 --> 00:14:38,440
different Bot.

271
00:14:39,160 --> 00:14:43,870
If you ask Josh, well, why would we
be getting better at the IQ test?

272
00:14:43,871 --> 00:14:46,240
He says that in the last hundred years

273
00:14:46,330 --> 00:14:48,790
people learn how to think abstractly,

274
00:14:51,590 --> 00:14:52,490
crude oil,

275
00:14:52,720 --> 00:14:55,580
palm dirty things that
we take for granted.

276
00:14:55,610 --> 00:14:59,060
Like thinking about abstract things
like a market where a market is not a

277
00:14:59,061 --> 00:15:04,040
particular place with fruit stands,
but a more abstract space so to speak,

278
00:15:04,041 --> 00:15:06,360
in which goods and services
are exchanged for money.

279
00:15:07,270 --> 00:15:11,150
He's to fit these [inaudible] toe

280
00:15:12,800 --> 00:15:15,820
to bed, five to 10 Havi. I'll bet

281
00:15:15,910 --> 00:15:20,910
things like that have become part of
the our cognitive Boucher meaning,

282
00:15:21,800 --> 00:15:26,800
and I think this is how Josh would
argue with these are deeply abstract

283
00:15:27,610 --> 00:15:32,610
occupations mean natural gas to try to
figure out patterning and numbers and

284
00:15:33,491 --> 00:15:34,480
future values,

285
00:15:34,660 --> 00:15:37,750
crude oil, natural gas, edit it.

286
00:15:37,990 --> 00:15:40,250
I think Josh [inaudible]
it can change you.

287
00:15:40,300 --> 00:15:44,860
Cultural Evolution essentially has given
us much higher iqs when it comes to

288
00:15:44,861 --> 00:15:46,450
thinking about a lot of things. Wow.

289
00:15:46,451 --> 00:15:51,340
So you're saying that we are learning
to exercise our rational systems.

290
00:15:51,580 --> 00:15:53,740
It's not that we're growing
any new brain cells,

291
00:15:53,830 --> 00:15:57,490
we're making a whole new
set of connections. It's
just that what we've got,

292
00:15:57,491 --> 00:15:59,800
we're just making more muscular. Exactly.

293
00:15:59,801 --> 00:16:01,780
It's like learning to play
an instrument. Right? I mean,

294
00:16:01,781 --> 00:16:05,140
when you first start playing
guitar, you're totally useless.

295
00:16:05,141 --> 00:16:08,290
It sounds like a dying animal,
uh, end, but you know, give it a,

296
00:16:08,291 --> 00:16:10,900
give it a couple of years and it can
sound great. And basically we're,

297
00:16:10,930 --> 00:16:13,710
that's a very specific
sort of motor skill, right?

298
00:16:13,780 --> 00:16:18,780
But being better at abstraction
and thinking about right
and wrong in a new way.

299
00:16:19,870 --> 00:16:22,870
That seems, that seems what you're
saying is kind of down. Well, I mean,

300
00:16:22,871 --> 00:16:27,070
you think that you can exercise yourself
into being a better man and a better

301
00:16:27,071 --> 00:16:29,050
woman and a better speaker.
I think that's right.

302
00:16:29,051 --> 00:16:32,740
I think that we can learn to play our
dorsolateral prefrontal cortices better.

303
00:16:32,860 --> 00:16:33,880
At the end of the day.

304
00:16:33,881 --> 00:16:38,140
You think that the pressure of dealing
with these big abstract problems will

305
00:16:38,141 --> 00:16:41,950
eventually change our minds?
Well, I hope so. I mean,

306
00:16:41,951 --> 00:16:46,570
the problem is that as a species we
tend to learn from trial and error. Um,

307
00:16:46,600 --> 00:16:51,460
the problem with issues like
nuclear proliferation and
global warming is that we

308
00:16:51,461 --> 00:16:53,640
only have one earth and it are,

309
00:16:53,641 --> 00:16:57,010
what I hope is that if we have to learn
the lesson from some kind of trial and

310
00:16:57,011 --> 00:16:59,770
error, the errors are not so big
that we don't get another chance.

311
00:16:59,830 --> 00:17:04,060
But I also think that there's reason for
optimism, or at least you hope think.

312
00:17:04,090 --> 00:17:05,920
Yeah. At least I hope
you know. But I mean,

313
00:17:05,980 --> 00:17:08,950
I may just be because I'm an
optimistic person. I mean,

314
00:17:08,951 --> 00:17:11,140
I might just sort of throw up
my hands and say, forget it.

315
00:17:11,141 --> 00:17:14,950
I'll go do something else, enjoy my
time before we kill ourselves. But, um,

316
00:17:15,280 --> 00:17:18,280
I think that, you know,
I the, it makes sense.

317
00:17:18,281 --> 00:17:22,570
It's worth a shot to see if we can teach
ourselves to live happily on a, on a,

318
00:17:22,571 --> 00:17:23,470
on a small planet.

319
00:17:26,690 --> 00:17:27,400
[inaudible]

320
00:17:27,400 --> 00:17:31,080
I heard you. The teacher. Wow. Yeah.
I'm pretty pretty. Pretty. Dantic Huh?

321
00:17:32,770 --> 00:17:36,280
Teach in the world.
No, no. I kind of mean,

322
00:17:36,340 --> 00:17:40,510
I certainly think anyone normal but be
rooting for you. Absolutely. Thanks.

323
00:17:40,570 --> 00:17:43,240
I appreciate that. There are a lot
of abnormal people who root for me,

324
00:17:43,241 --> 00:17:47,830
but I hope [inaudible] Josh Green is
an assistant professor of psychology at

325
00:17:47,831 --> 00:17:52,831
Harvard University written these ideas
in an essay in a volume called what's

326
00:17:53,131 --> 00:17:57,090
next edited by Max Brockman and hey,
did you, when you were talking with him,

327
00:17:57,091 --> 00:18:01,170
did you ask him about his babies? What
would he kill his babies? You know,

328
00:18:01,171 --> 00:18:04,230
I should, I forgot any case. We
should. We should wrap it. Yeah.

329
00:18:04,490 --> 00:18:07,260
He killed this baby cause the, the,
we have to see our funding, right?

330
00:18:07,300 --> 00:18:12,060
So Radiolab is supported in part by the
National Science Foundation Corporation

331
00:18:12,061 --> 00:18:14,790
for public broadcasting and one
other, the Sloan Foundation,

332
00:18:15,240 --> 00:18:19,590
which by the way is supporting Kepler
to Phillip Glass Opera about the great

333
00:18:19,591 --> 00:18:20,880
17th century astronomer.

334
00:18:20,881 --> 00:18:25,740
It's premiering November 18th right
down the street from me as the Brooklyn

335
00:18:25,740 --> 00:18:28,530
Academy of Music. I'm Jad Apple Rod.
I remember going into listening.

