WEBVTT
Kind: captions
Language: en

00:00:14.169 --> 00:00:17.710
think about this confluence of different

00:00:17.710 --> 00:00:17.720
think about this confluence of different
 

00:00:17.720 --> 00:00:19.870
think about this confluence of different
technologies suddenly becoming a big

00:00:19.870 --> 00:00:19.880
technologies suddenly becoming a big
 

00:00:19.880 --> 00:00:22.240
technologies suddenly becoming a big
enabler of a revolutionary new step

00:00:22.240 --> 00:00:22.250
enabler of a revolutionary new step
 

00:00:22.250 --> 00:00:24.249
enabler of a revolutionary new step
self-driving vehicles were enabled by

00:00:24.249 --> 00:00:24.259
self-driving vehicles were enabled by
 

00:00:24.259 --> 00:00:26.819
self-driving vehicles were enabled by
advances in sensing capability

00:00:26.819 --> 00:00:26.829
advances in sensing capability
 

00:00:26.829 --> 00:00:29.800
advances in sensing capability
electronics becoming smaller faster and

00:00:29.800 --> 00:00:29.810
electronics becoming smaller faster and
 

00:00:29.810 --> 00:00:31.689
electronics becoming smaller faster and
cheaper and then when you combine that

00:00:31.689 --> 00:00:31.699
cheaper and then when you combine that
 

00:00:31.699 --> 00:00:34.299
cheaper and then when you combine that
with artificial intelligence improvement

00:00:34.299 --> 00:00:34.309
with artificial intelligence improvement
 

00:00:34.309 --> 00:00:35.979
with artificial intelligence improvement
that we have seen over the past decade

00:00:35.979 --> 00:00:35.989
that we have seen over the past decade
 

00:00:35.989 --> 00:00:39.009
that we have seen over the past decade
or so that in turn is enabling the

00:00:39.009 --> 00:00:39.019
or so that in turn is enabling the
 

00:00:39.019 --> 00:00:42.100
or so that in turn is enabling the
creation and deployment of sampling

00:00:42.100 --> 00:00:42.110
creation and deployment of sampling
 

00:00:42.110 --> 00:00:44.470
creation and deployment of sampling
vehicles if we are able to sense all

00:00:44.470 --> 00:00:44.480
vehicles if we are able to sense all
 

00:00:44.480 --> 00:00:46.810
vehicles if we are able to sense all
activities happening within a

00:00:46.810 --> 00:00:46.820
activities happening within a
 

00:00:46.820 --> 00:00:49.510
activities happening within a
metropolitan region things happening in

00:00:49.510 --> 00:00:49.520
metropolitan region things happening in
 

00:00:49.520 --> 00:00:53.260
metropolitan region things happening in
transportation water sewage air quality

00:00:53.260 --> 00:00:53.270
transportation water sewage air quality
 

00:00:53.270 --> 00:00:56.410
transportation water sewage air quality
noise quality law enforcement healthcare

00:00:56.410 --> 00:00:56.420
noise quality law enforcement healthcare
 

00:00:56.420 --> 00:00:58.660
noise quality law enforcement healthcare
you name it if you can basically sense

00:00:58.660 --> 00:00:58.670
you name it if you can basically sense
 

00:00:58.670 --> 00:01:01.319
you name it if you can basically sense
those things process that with the

00:01:01.319 --> 00:01:01.329
those things process that with the
 

00:01:01.329 --> 00:01:03.160
those things process that with the
enormous amounts of computational power

00:01:03.160 --> 00:01:03.170
enormous amounts of computational power
 

00:01:03.170 --> 00:01:05.169
enormous amounts of computational power
that we have at our disposal today and

00:01:05.169 --> 00:01:05.179
that we have at our disposal today and
 

00:01:05.179 --> 00:01:08.490
that we have at our disposal today and
then run a on it we can actually extract

00:01:08.490 --> 00:01:08.500
then run a on it we can actually extract
 

00:01:08.500 --> 00:01:11.859
then run a on it we can actually extract
inferences trends and patterns which can

00:01:11.859 --> 00:01:11.869
inferences trends and patterns which can
 

00:01:11.869 --> 00:01:16.440
inferences trends and patterns which can
be exploited fed back to citizens

00:01:16.440 --> 00:01:16.450
be exploited fed back to citizens
 

00:01:16.450 --> 00:01:20.940
be exploited fed back to citizens
planners decision-makers employers and

00:01:20.940 --> 00:01:20.950
planners decision-makers employers and
 

00:01:20.950 --> 00:01:23.320
planners decision-makers employers and
long-term planners they can literally

00:01:23.320 --> 00:01:23.330
long-term planners they can literally
 

00:01:23.330 --> 00:01:25.870
long-term planners they can literally
make cities actually operate much more

00:01:25.870 --> 00:01:25.880
make cities actually operate much more
 

00:01:25.880 --> 00:01:29.020
make cities actually operate much more
efficiently and effectively making them

00:01:29.020 --> 00:01:29.030
efficiently and effectively making them
 

00:01:29.030 --> 00:01:32.469
efficiently and effectively making them
smarter and metropolitan lives like they

00:01:32.469 --> 00:01:32.479
smarter and metropolitan lives like they
 

00:01:32.479 --> 00:01:34.359
smarter and metropolitan lives like they
have a much better quality of life

00:01:34.359 --> 00:01:34.369
have a much better quality of life
 

00:01:34.369 --> 00:01:36.460
have a much better quality of life
there's not just about innovations in

00:01:36.460 --> 00:01:36.470
there's not just about innovations in
 

00:01:36.470 --> 00:01:38.710
there's not just about innovations in
technology that needs to be innovations

00:01:38.710 --> 00:01:38.720
technology that needs to be innovations
 

00:01:38.720 --> 00:01:41.260
technology that needs to be innovations
in policy as well so the the

00:01:41.260 --> 00:01:41.270
in policy as well so the the
 

00:01:41.270 --> 00:01:44.260
in policy as well so the the
improvements in policy and technology

00:01:44.260 --> 00:01:44.270
improvements in policy and technology
 

00:01:44.270 --> 00:01:45.969
improvements in policy and technology
really have to work with each other it's

00:01:45.969 --> 00:01:45.979
really have to work with each other it's
 

00:01:45.979 --> 00:01:47.650
really have to work with each other it's
not job-related technology's just doing

00:01:47.650 --> 00:01:47.660
not job-related technology's just doing
 

00:01:47.660 --> 00:01:49.839
not job-related technology's just doing
that in the air - division labs the

00:01:49.839 --> 00:01:49.849
that in the air - division labs the
 

00:01:49.849 --> 00:01:51.460
that in the air - division labs the
Dodgers about policy makers making

00:01:51.460 --> 00:01:51.470
Dodgers about policy makers making
 

00:01:51.470 --> 00:01:53.320
Dodgers about policy makers making
decisions in a vacuum mean to correctly

00:01:53.320 --> 00:01:53.330
decisions in a vacuum mean to correctly
 

00:01:53.330 --> 00:01:55.540
decisions in a vacuum mean to correctly
get this or two groups of people working

00:01:55.540 --> 00:01:55.550
get this or two groups of people working
 

00:01:55.550 --> 00:01:57.430
get this or two groups of people working
together talking to each other in the

00:01:57.430 --> 00:01:57.440
together talking to each other in the
 

00:01:57.440 --> 00:02:02.860
together talking to each other in the
same goal

00:02:02.860 --> 00:02:02.870
 

00:02:02.870 --> 00:02:05.600
welcome back from lunch I hope everybody

00:02:05.600 --> 00:02:05.610
welcome back from lunch I hope everybody
 

00:02:05.610 --> 00:02:09.469
welcome back from lunch I hope everybody
was able to get some food and it seemed

00:02:09.469 --> 00:02:09.479
was able to get some food and it seemed
 

00:02:09.479 --> 00:02:10.760
was able to get some food and it seemed
as though the conversation was perhaps

00:02:10.760 --> 00:02:10.770
as though the conversation was perhaps
 

00:02:10.770 --> 00:02:12.170
as though the conversation was perhaps
even better than the food which is

00:02:12.170 --> 00:02:12.180
even better than the food which is
 

00:02:12.180 --> 00:02:14.570
even better than the food which is
saying something it's wonderful to be

00:02:14.570 --> 00:02:14.580
saying something it's wonderful to be
 

00:02:14.580 --> 00:02:17.300
saying something it's wonderful to be
here to start the next session on policy

00:02:17.300 --> 00:02:17.310
here to start the next session on policy
 

00:02:17.310 --> 00:02:20.030
here to start the next session on policy
and governance that is how can we

00:02:20.030 --> 00:02:20.040
and governance that is how can we
 

00:02:20.040 --> 00:02:22.520
and governance that is how can we
perhaps not simply in a bottom-up

00:02:22.520 --> 00:02:22.530
perhaps not simply in a bottom-up
 

00:02:22.530 --> 00:02:25.070
perhaps not simply in a bottom-up
fashion as technology developers or as

00:02:25.070 --> 00:02:25.080
fashion as technology developers or as
 

00:02:25.080 --> 00:02:26.750
fashion as technology developers or as
people thinking about the as uses of

00:02:26.750 --> 00:02:26.760
people thinking about the as uses of
 

00:02:26.760 --> 00:02:28.850
people thinking about the as uses of
technology but perhaps in a more

00:02:28.850 --> 00:02:28.860
technology but perhaps in a more
 

00:02:28.860 --> 00:02:31.100
technology but perhaps in a more
top-down way think about the kinds of

00:02:31.100 --> 00:02:31.110
top-down way think about the kinds of
 

00:02:31.110 --> 00:02:33.890
top-down way think about the kinds of
structures and policies that we can put

00:02:33.890 --> 00:02:33.900
structures and policies that we can put
 

00:02:33.900 --> 00:02:35.600
structures and policies that we can put
into place to help ensure that these

00:02:35.600 --> 00:02:35.610
into place to help ensure that these
 

00:02:35.610 --> 00:02:37.790
into place to help ensure that these
technologies are actually serving our

00:02:37.790 --> 00:02:37.800
technologies are actually serving our
 

00:02:37.800 --> 00:02:39.710
technologies are actually serving our
human values our human interests our

00:02:39.710 --> 00:02:39.720
human values our human interests our
 

00:02:39.720 --> 00:02:40.699
human values our human interests our
human goals

00:02:40.699 --> 00:02:40.709
human goals
 

00:02:40.709 --> 00:02:43.340
human goals
how can nations work together how can

00:02:43.340 --> 00:02:43.350
how can nations work together how can
 

00:02:43.350 --> 00:02:45.650
how can nations work together how can
universities work together and so at

00:02:45.650 --> 00:02:45.660
universities work together and so at
 

00:02:45.660 --> 00:02:48.470
universities work together and so at
every level what kinds of governance

00:02:48.470 --> 00:02:48.480
every level what kinds of governance
 

00:02:48.480 --> 00:02:50.330
every level what kinds of governance
structures do we want and what can

00:02:50.330 --> 00:02:50.340
structures do we want and what can
 

00:02:50.340 --> 00:02:52.850
structures do we want and what can
advance our interests and our

00:02:52.850 --> 00:02:52.860
advance our interests and our
 

00:02:52.860 --> 00:02:55.460
advance our interests and our
technologies so we'll begin the session

00:02:55.460 --> 00:02:55.470
technologies so we'll begin the session
 

00:02:55.470 --> 00:02:57.560
technologies so we'll begin the session
as we did the last one with a talk by

00:02:57.560 --> 00:02:57.570
as we did the last one with a talk by
 

00:02:57.570 --> 00:02:59.390
as we did the last one with a talk by
one of our K&amp;L gates presidential

00:02:59.390 --> 00:02:59.400
one of our K&amp;L gates presidential
 

00:02:59.400 --> 00:03:02.060
one of our K&amp;L gates presidential
fellows Allante Whitmore received her BS

00:03:02.060 --> 00:03:02.070
fellows Allante Whitmore received her BS
 

00:03:02.070 --> 00:03:03.680
fellows Allante Whitmore received her BS
and biological engineering from North

00:03:03.680 --> 00:03:03.690
and biological engineering from North
 

00:03:03.690 --> 00:03:06.410
and biological engineering from North
Carolina and T State University as well

00:03:06.410 --> 00:03:06.420
Carolina and T State University as well
 

00:03:06.420 --> 00:03:08.060
Carolina and T State University as well
as an MS in agricultural and biological

00:03:08.060 --> 00:03:08.070
as an MS in agricultural and biological
 

00:03:08.070 --> 00:03:10.130
as an MS in agricultural and biological
engineering from the University of

00:03:10.130 --> 00:03:10.140
engineering from the University of
 

00:03:10.140 --> 00:03:12.770
engineering from the University of
Illinois at urbana-champaign she's come

00:03:12.770 --> 00:03:12.780
Illinois at urbana-champaign she's come
 

00:03:12.780 --> 00:03:14.720
Illinois at urbana-champaign she's come
to us now here at CMU and as a PhD

00:03:14.720 --> 00:03:14.730
to us now here at CMU and as a PhD
 

00:03:14.730 --> 00:03:17.420
to us now here at CMU and as a PhD
student in both civil and environmental

00:03:17.420 --> 00:03:17.430
student in both civil and environmental
 

00:03:17.430 --> 00:03:19.640
student in both civil and environmental
engineering as well as engineering and

00:03:19.640 --> 00:03:19.650
engineering as well as engineering and
 

00:03:19.650 --> 00:03:21.229
engineering as well as engineering and
public policy so as you can see she's

00:03:21.229 --> 00:03:21.239
public policy so as you can see she's
 

00:03:21.239 --> 00:03:23.930
public policy so as you can see she's
sort of ideally situated for this

00:03:23.930 --> 00:03:23.940
sort of ideally situated for this
 

00:03:23.940 --> 00:03:26.300
sort of ideally situated for this
session and has been focusing her

00:03:26.300 --> 00:03:26.310
session and has been focusing her
 

00:03:26.310 --> 00:03:28.640
session and has been focusing her
research on the societal and the

00:03:28.640 --> 00:03:28.650
research on the societal and the
 

00:03:28.650 --> 00:03:30.199
research on the societal and the
environmental implications of this

00:03:30.199 --> 00:03:30.209
environmental implications of this
 

00:03:30.209 --> 00:03:31.850
environmental implications of this
transition that we're seeing in

00:03:31.850 --> 00:03:31.860
transition that we're seeing in
 

00:03:31.860 --> 00:03:35.000
transition that we're seeing in
Pittsburgh and globally to a more

00:03:35.000 --> 00:03:35.010
Pittsburgh and globally to a more
 

00:03:35.010 --> 00:03:37.090
Pittsburgh and globally to a more
automated more autonomous transportation

00:03:37.090 --> 00:03:37.100
automated more autonomous transportation
 

00:03:37.100 --> 00:03:49.729
automated more autonomous transportation
infrastructure so Allante thank you

00:03:49.729 --> 00:03:49.739
 

00:03:49.739 --> 00:03:52.679
the trolley problem is an on has

00:03:52.679 --> 00:03:52.689
the trolley problem is an on has
 

00:03:52.689 --> 00:03:54.360
the trolley problem is an on has
garnered a lot of attention with the

00:03:54.360 --> 00:03:54.370
garnered a lot of attention with the
 

00:03:54.370 --> 00:03:55.860
garnered a lot of attention with the
oncoming introduction of autonomous

00:03:55.860 --> 00:03:55.870
oncoming introduction of autonomous
 

00:03:55.870 --> 00:03:58.979
oncoming introduction of autonomous
vehicles take a minute with me and

00:03:58.979 --> 00:03:58.989
vehicles take a minute with me and
 

00:03:58.989 --> 00:04:02.399
vehicles take a minute with me and
imagine a trolley barreling down the

00:04:02.399 --> 00:04:02.409
imagine a trolley barreling down the
 

00:04:02.409 --> 00:04:04.110
imagine a trolley barreling down the
railway tracks and in front of it five

00:04:04.110 --> 00:04:04.120
railway tracks and in front of it five
 

00:04:04.120 --> 00:04:06.210
railway tracks and in front of it five
people that would be killed you can pull

00:04:06.210 --> 00:04:06.220
people that would be killed you can pull
 

00:04:06.220 --> 00:04:09.000
people that would be killed you can pull
a lever and on the other track there

00:04:09.000 --> 00:04:09.010
a lever and on the other track there
 

00:04:09.010 --> 00:04:11.460
a lever and on the other track there
there's another person that could be

00:04:11.460 --> 00:04:11.470
there's another person that could be
 

00:04:11.470 --> 00:04:14.369
there's another person that could be
killed on the one track we've replaced

00:04:14.369 --> 00:04:14.379
killed on the one track we've replaced
 

00:04:14.379 --> 00:04:19.110
killed on the one track we've replaced
the Charlie well our eyes are being

00:04:19.110 --> 00:04:19.120
the Charlie well our eyes are being
 

00:04:19.120 --> 00:04:22.830
the Charlie well our eyes are being
replaced with a sensor and the levers

00:04:22.830 --> 00:04:22.840
replaced with a sensor and the levers
 

00:04:22.840 --> 00:04:24.750
replaced with a sensor and the levers
replaced by a line of code and the

00:04:24.750 --> 00:04:24.760
replaced by a line of code and the
 

00:04:24.760 --> 00:04:27.060
replaced by a line of code and the
question has now become how will a vs

00:04:27.060 --> 00:04:27.070
question has now become how will a vs
 

00:04:27.070 --> 00:04:29.909
question has now become how will a vs
behave when at when an accident is

00:04:29.909 --> 00:04:29.919
behave when at when an accident is
 

00:04:29.919 --> 00:04:32.250
behave when at when an accident is
unavoidable we've become obsessed

00:04:32.250 --> 00:04:32.260
unavoidable we've become obsessed
 

00:04:32.260 --> 00:04:34.610
unavoidable we've become obsessed
fixated with this issue in academia

00:04:34.610 --> 00:04:34.620
fixated with this issue in academia
 

00:04:34.620 --> 00:04:38.100
fixated with this issue in academia
media private and public sector but I

00:04:38.100 --> 00:04:38.110
media private and public sector but I
 

00:04:38.110 --> 00:04:39.900
media private and public sector but I
kind of think that the trolley problem

00:04:39.900 --> 00:04:39.910
kind of think that the trolley problem
 

00:04:39.910 --> 00:04:41.460
kind of think that the trolley problem
is sort of limiting because it's

00:04:41.460 --> 00:04:41.470
is sort of limiting because it's
 

00:04:41.470 --> 00:04:43.650
is sort of limiting because it's
focusing on order their hardware and

00:04:43.650 --> 00:04:43.660
focusing on order their hardware and
 

00:04:43.660 --> 00:04:45.600
focusing on order their hardware and
software choices that we're making and

00:04:45.600 --> 00:04:45.610
software choices that we're making and
 

00:04:45.610 --> 00:04:48.600
software choices that we're making and
if we're ok with those decisions I want

00:04:48.600 --> 00:04:48.610
if we're ok with those decisions I want
 

00:04:48.610 --> 00:04:50.850
if we're ok with those decisions I want
to take a moment to kind of take a step

00:04:50.850 --> 00:04:50.860
to take a moment to kind of take a step
 

00:04:50.860 --> 00:04:53.700
to take a moment to kind of take a step
back and look at the avian ethics more

00:04:53.700 --> 00:04:53.710
back and look at the avian ethics more
 

00:04:53.710 --> 00:04:55.920
back and look at the avian ethics more
broadly in society and that's what I've

00:04:55.920 --> 00:04:55.930
broadly in society and that's what I've
 

00:04:55.930 --> 00:04:57.750
broadly in society and that's what I've
had the opportunity to do as a KL

00:04:57.750 --> 00:04:57.760
had the opportunity to do as a KL
 

00:04:57.760 --> 00:04:59.969
had the opportunity to do as a KL
presidential Kano gates presidential

00:04:59.969 --> 00:04:59.979
presidential Kano gates presidential
 

00:04:59.979 --> 00:05:05.820
presidential Kano gates presidential
fellow here at CMU the fatal uber crash

00:05:05.820 --> 00:05:05.830
fellow here at CMU the fatal uber crash
 

00:05:05.830 --> 00:05:08.219
fellow here at CMU the fatal uber crash
in Arizona a couple weeks ago is a

00:05:08.219 --> 00:05:08.229
in Arizona a couple weeks ago is a
 

00:05:08.229 --> 00:05:10.110
in Arizona a couple weeks ago is a
microcosm of these bigger ethical

00:05:10.110 --> 00:05:10.120
microcosm of these bigger ethical
 

00:05:10.120 --> 00:05:13.290
microcosm of these bigger ethical
questions questions beyond the Charlie

00:05:13.290 --> 00:05:13.300
questions questions beyond the Charlie
 

00:05:13.300 --> 00:05:15.360
questions questions beyond the Charlie
problem look at things like how do open

00:05:15.360 --> 00:05:15.370
problem look at things like how do open
 

00:05:15.370 --> 00:05:17.100
problem look at things like how do open
design come into play or lighting or

00:05:17.100 --> 00:05:17.110
design come into play or lighting or
 

00:05:17.110 --> 00:05:19.890
design come into play or lighting or
sight like silent sidewalk placement or

00:05:19.890 --> 00:05:19.900
sight like silent sidewalk placement or
 

00:05:19.900 --> 00:05:23.040
sight like silent sidewalk placement or
even what happens when a navy speeding

00:05:23.040 --> 00:05:23.050
even what happens when a navy speeding
 

00:05:23.050 --> 00:05:26.219
even what happens when a navy speeding
who's responsible the point is the

00:05:26.219 --> 00:05:26.229
who's responsible the point is the
 

00:05:26.229 --> 00:05:28.170
who's responsible the point is the
decisions that we make permeate outside

00:05:28.170 --> 00:05:28.180
decisions that we make permeate outside
 

00:05:28.180 --> 00:05:29.760
decisions that we make permeate outside
the confines which transportation and

00:05:29.760 --> 00:05:29.770
the confines which transportation and
 

00:05:29.770 --> 00:05:32.040
the confines which transportation and
mobility and we have to think about that

00:05:32.040 --> 00:05:32.050
mobility and we have to think about that
 

00:05:32.050 --> 00:05:34.800
mobility and we have to think about that
as planners policymakers and programmers

00:05:34.800 --> 00:05:34.810
as planners policymakers and programmers
 

00:05:34.810 --> 00:05:37.500
as planners policymakers and programmers
that will affect our society our access

00:05:37.500 --> 00:05:37.510
that will affect our society our access
 

00:05:37.510 --> 00:05:41.360
that will affect our society our access
and our access to opportunity

00:05:41.360 --> 00:05:41.370
and our access to opportunity
 

00:05:41.370 --> 00:05:43.279
and our access to opportunity
we can begin with our built environment

00:05:43.279 --> 00:05:43.289
we can begin with our built environment
 

00:05:43.289 --> 00:05:45.839
we can begin with our built environment
which we have to think about how are we

00:05:45.839 --> 00:05:45.849
which we have to think about how are we
 

00:05:45.849 --> 00:05:48.210
which we have to think about how are we
going to mix human and non-human drivers

00:05:48.210 --> 00:05:48.220
going to mix human and non-human drivers
 

00:05:48.220 --> 00:05:49.710
going to mix human and non-human drivers
that will be sharing the road for the

00:05:49.710 --> 00:05:49.720
that will be sharing the road for the
 

00:05:49.720 --> 00:05:52.140
that will be sharing the road for the
next couple of years previous

00:05:52.140 --> 00:05:52.150
next couple of years previous
 

00:05:52.150 --> 00:05:55.080
next couple of years previous
transportation revolutions have provided

00:05:55.080 --> 00:05:55.090
transportation revolutions have provided
 

00:05:55.090 --> 00:05:56.850
transportation revolutions have provided
a lot of opportunity and they've shaped

00:05:56.850 --> 00:05:56.860
a lot of opportunity and they've shaped
 

00:05:56.860 --> 00:05:59.070
a lot of opportunity and they've shaped
our cities they've also contributed to

00:05:59.070 --> 00:05:59.080
our cities they've also contributed to
 

00:05:59.080 --> 00:06:01.860
our cities they've also contributed to
pre-existing racial or socio-economic to

00:06:01.860 --> 00:06:01.870
pre-existing racial or socio-economic to
 

00:06:01.870 --> 00:06:05.190
pre-existing racial or socio-economic to
verities we have an opportunity now to

00:06:05.190 --> 00:06:05.200
verities we have an opportunity now to
 

00:06:05.200 --> 00:06:07.290
verities we have an opportunity now to
think more deeply about our work and the

00:06:07.290 --> 00:06:07.300
think more deeply about our work and the
 

00:06:07.300 --> 00:06:09.720
think more deeply about our work and the
outcomes and we can work to mitigate and

00:06:09.720 --> 00:06:09.730
outcomes and we can work to mitigate and
 

00:06:09.730 --> 00:06:11.370
outcomes and we can work to mitigate and
reduce those disparities from the

00:06:11.370 --> 00:06:11.380
reduce those disparities from the
 

00:06:11.380 --> 00:06:15.120
reduce those disparities from the
decisions made in the past we also can

00:06:15.120 --> 00:06:15.130
decisions made in the past we also can
 

00:06:15.130 --> 00:06:18.000
decisions made in the past we also can
be thinking about land exchange as a

00:06:18.000 --> 00:06:18.010
be thinking about land exchange as a
 

00:06:18.010 --> 00:06:20.790
be thinking about land exchange as a
these are posed to reduce parking demand

00:06:20.790 --> 00:06:20.800
these are posed to reduce parking demand
 

00:06:20.800 --> 00:06:23.130
these are posed to reduce parking demand
and so what are we going to do with that

00:06:23.130 --> 00:06:23.140
and so what are we going to do with that
 

00:06:23.140 --> 00:06:25.410
and so what are we going to do with that
land use just and how are we going to

00:06:25.410 --> 00:06:25.420
land use just and how are we going to
 

00:06:25.420 --> 00:06:27.240
land use just and how are we going to
allocate it so that the communities

00:06:27.240 --> 00:06:27.250
allocate it so that the communities
 

00:06:27.250 --> 00:06:30.960
allocate it so that the communities
where these new spaces available will be

00:06:30.960 --> 00:06:30.970
where these new spaces available will be
 

00:06:30.970 --> 00:06:34.680
where these new spaces available will be
impacted positively lastly we have to

00:06:34.680 --> 00:06:34.690
impacted positively lastly we have to
 

00:06:34.690 --> 00:06:36.660
impacted positively lastly we have to
think about policy and regulation as it

00:06:36.660 --> 00:06:36.670
think about policy and regulation as it
 

00:06:36.670 --> 00:06:39.090
think about policy and regulation as it
was largely shaped the outcomes of Av

00:06:39.090 --> 00:06:39.100
was largely shaped the outcomes of Av
 

00:06:39.100 --> 00:06:42.780
was largely shaped the outcomes of Av
deployment we know that one highly

00:06:42.780 --> 00:06:42.790
deployment we know that one highly
 

00:06:42.790 --> 00:06:45.030
deployment we know that one highly
contested point is how safe is safe

00:06:45.030 --> 00:06:45.040
contested point is how safe is safe
 

00:06:45.040 --> 00:06:46.980
contested point is how safe is safe
enough for AVS to be released to the

00:06:46.980 --> 00:06:46.990
enough for AVS to be released to the
 

00:06:46.990 --> 00:06:53.340
enough for AVS to be released to the
public we have to think about why are we

00:06:53.340 --> 00:06:53.350
public we have to think about why are we
 

00:06:53.350 --> 00:06:56.010
public we have to think about why are we
well one highly contested point of

00:06:56.010 --> 00:06:56.020
well one highly contested point of
 

00:06:56.020 --> 00:06:58.740
well one highly contested point of
course is like I said the idea of how

00:06:58.740 --> 00:06:58.750
course is like I said the idea of how
 

00:06:58.750 --> 00:07:01.950
course is like I said the idea of how
safe is safe enough one side argues we

00:07:01.950 --> 00:07:01.960
safe is safe enough one side argues we
 

00:07:01.960 --> 00:07:04.080
safe is safe enough one side argues we
need to have a these out immediately

00:07:04.080 --> 00:07:04.090
need to have a these out immediately
 

00:07:04.090 --> 00:07:06.360
need to have a these out immediately
because we are losing more lives in

00:07:06.360 --> 00:07:06.370
because we are losing more lives in
 

00:07:06.370 --> 00:07:08.370
because we are losing more lives in
fatal car crashes every day well on

00:07:08.370 --> 00:07:08.380
fatal car crashes every day well on
 

00:07:08.380 --> 00:07:10.350
fatal car crashes every day well on
other side argues it's unethical to

00:07:10.350 --> 00:07:10.360
other side argues it's unethical to
 

00:07:10.360 --> 00:07:11.940
other side argues it's unethical to
release a technology of this magnitude

00:07:11.940 --> 00:07:11.950
release a technology of this magnitude
 

00:07:11.950 --> 00:07:14.130
release a technology of this magnitude
without making sure that it's as safe as

00:07:14.130 --> 00:07:14.140
without making sure that it's as safe as
 

00:07:14.140 --> 00:07:16.950
without making sure that it's as safe as
we claim it to be now the things I've

00:07:16.950 --> 00:07:16.960
we claim it to be now the things I've
 

00:07:16.960 --> 00:07:18.780
we claim it to be now the things I've
covered are in no way comprehensive

00:07:18.780 --> 00:07:18.790
covered are in no way comprehensive
 

00:07:18.790 --> 00:07:20.940
covered are in no way comprehensive
rather a quick peek at what I'm along

00:07:20.940 --> 00:07:20.950
rather a quick peek at what I'm along
 

00:07:20.950 --> 00:07:22.620
rather a quick peek at what I'm along
with my research advisors are working on

00:07:22.620 --> 00:07:22.630
with my research advisors are working on
 

00:07:22.630 --> 00:07:25.410
with my research advisors are working on
I'm hoping that I can inform answers to

00:07:25.410 --> 00:07:25.420
I'm hoping that I can inform answers to
 

00:07:25.420 --> 00:07:27.300
I'm hoping that I can inform answers to
questions like what is societal and

00:07:27.300 --> 00:07:27.310
questions like what is societal and
 

00:07:27.310 --> 00:07:30.030
questions like what is societal and
environment so impacts of AVS or will

00:07:30.030 --> 00:07:30.040
environment so impacts of AVS or will
 

00:07:30.040 --> 00:07:31.980
environment so impacts of AVS or will
this potential shift to share mobility

00:07:31.980 --> 00:07:31.990
this potential shift to share mobility
 

00:07:31.990 --> 00:07:35.460
this potential shift to share mobility
hurt or help lower-income communities he

00:07:35.460 --> 00:07:35.470
hurt or help lower-income communities he
 

00:07:35.470 --> 00:07:39.630
hurt or help lower-income communities he
may have been thinking how can you apply

00:07:39.630 --> 00:07:39.640
may have been thinking how can you apply
 

00:07:39.640 --> 00:07:41.580
may have been thinking how can you apply
ethics into your work or if you even

00:07:41.580 --> 00:07:41.590
ethics into your work or if you even
 

00:07:41.590 --> 00:07:43.680
ethics into your work or if you even
should be concerned about it but the

00:07:43.680 --> 00:07:43.690
should be concerned about it but the
 

00:07:43.690 --> 00:07:45.890
should be concerned about it but the
reality is we should be very concerned

00:07:45.890 --> 00:07:45.900
reality is we should be very concerned
 

00:07:45.900 --> 00:07:48.390
reality is we should be very concerned
the Association for Computing Machinery

00:07:48.390 --> 00:07:48.400
the Association for Computing Machinery
 

00:07:48.400 --> 00:07:50.970
the Association for Computing Machinery
states that professional competence

00:07:50.970 --> 00:07:50.980
states that professional competence
 

00:07:50.980 --> 00:07:53.160
states that professional competence
begins with technical knowledge but

00:07:53.160 --> 00:07:53.170
begins with technical knowledge but
 

00:07:53.170 --> 00:07:55.590
begins with technical knowledge but
includes identifying in navigating

00:07:55.590 --> 00:07:55.600
includes identifying in navigating
 

00:07:55.600 --> 00:07:57.750
includes identifying in navigating
ethical challenges and the American

00:07:57.750 --> 00:07:57.760
ethical challenges and the American
 

00:07:57.760 --> 00:08:00.110
ethical challenges and the American
Society of Society of Civil Engineers

00:08:00.110 --> 00:08:00.120
Society of Society of Civil Engineers
 

00:08:00.120 --> 00:08:02.280
Society of Society of Civil Engineers
employers that we seek to be of

00:08:02.280 --> 00:08:02.290
employers that we seek to be of
 

00:08:02.290 --> 00:08:04.290
employers that we seek to be of
constructive service to our communities

00:08:04.290 --> 00:08:04.300
constructive service to our communities
 

00:08:04.300 --> 00:08:06.450
constructive service to our communities
for the Advancement of the safety health

00:08:06.450 --> 00:08:06.460
for the Advancement of the safety health
 

00:08:06.460 --> 00:08:09.420
for the Advancement of the safety health
and well-being those are direct

00:08:09.420 --> 00:08:09.430
and well-being those are direct
 

00:08:09.430 --> 00:08:12.060
and well-being those are direct
instructions and so the Charlie problem

00:08:12.060 --> 00:08:12.070
instructions and so the Charlie problem
 

00:08:12.070 --> 00:08:14.220
instructions and so the Charlie problem
is just the tip of the iceberg and

00:08:14.220 --> 00:08:14.230
is just the tip of the iceberg and
 

00:08:14.230 --> 00:08:16.410
is just the tip of the iceberg and
now have an opportunity to think more

00:08:16.410 --> 00:08:16.420
now have an opportunity to think more
 

00:08:16.420 --> 00:08:18.000
now have an opportunity to think more
deeply about our work and how it will

00:08:18.000 --> 00:08:18.010
deeply about our work and how it will
 

00:08:18.010 --> 00:08:20.850
deeply about our work and how it will
impact us in this way we can make a

00:08:20.850 --> 00:08:20.860
impact us in this way we can make a
 

00:08:20.860 --> 00:08:22.470
impact us in this way we can make a
neighborhood around Navis that's better

00:08:22.470 --> 00:08:22.480
neighborhood around Navis that's better
 

00:08:22.480 --> 00:08:23.280
neighborhood around Navis that's better
for everyone

00:08:23.280 --> 00:08:23.290
for everyone
 

00:08:23.290 --> 00:08:36.990
for everyone
thank you thanks so much to to allanté

00:08:36.990 --> 00:08:37.000
thank you thanks so much to to allanté
 

00:08:37.000 --> 00:08:41.219
thank you thanks so much to to allanté
for that so I feel very much like Mister

00:08:41.219 --> 00:08:41.229
for that so I feel very much like Mister
 

00:08:41.229 --> 00:08:43.710
for that so I feel very much like Mister
Rogers going to get a stool to come out

00:08:43.710 --> 00:08:43.720
Rogers going to get a stool to come out
 

00:08:43.720 --> 00:08:46.830
Rogers going to get a stool to come out
to the center of the stage and just talk

00:08:46.830 --> 00:08:46.840
to the center of the stage and just talk
 

00:08:46.840 --> 00:08:49.290
to the center of the stage and just talk
with you all for a little while

00:08:49.290 --> 00:08:49.300
with you all for a little while
 

00:08:49.300 --> 00:08:52.020
with you all for a little while
no so one of the things that is a real

00:08:52.020 --> 00:08:52.030
no so one of the things that is a real
 

00:08:52.030 --> 00:08:54.720
no so one of the things that is a real
issue that's been coming up over and

00:08:54.720 --> 00:08:54.730
issue that's been coming up over and
 

00:08:54.730 --> 00:08:57.210
issue that's been coming up over and
over is the importance of thinking not

00:08:57.210 --> 00:08:57.220
over is the importance of thinking not
 

00:08:57.220 --> 00:09:00.150
over is the importance of thinking not
just at Carnegie Mellon not just even

00:09:00.150 --> 00:09:00.160
just at Carnegie Mellon not just even
 

00:09:00.160 --> 00:09:02.100
just at Carnegie Mellon not just even
locally within our cities such as the

00:09:02.100 --> 00:09:02.110
locally within our cities such as the
 

00:09:02.110 --> 00:09:04.470
locally within our cities such as the
city of Pittsburgh or Arizona or Phoenix

00:09:04.470 --> 00:09:04.480
city of Pittsburgh or Arizona or Phoenix
 

00:09:04.480 --> 00:09:06.330
city of Pittsburgh or Arizona or Phoenix
Arizona not even just thinking

00:09:06.330 --> 00:09:06.340
Arizona not even just thinking
 

00:09:06.340 --> 00:09:08.130
Arizona not even just thinking
nationally but thinking internationally

00:09:08.130 --> 00:09:08.140
nationally but thinking internationally
 

00:09:08.140 --> 00:09:09.900
nationally but thinking internationally
when it comes to AI one of the real

00:09:09.900 --> 00:09:09.910
when it comes to AI one of the real
 

00:09:09.910 --> 00:09:12.810
when it comes to AI one of the real
challenges and opportunities of AI

00:09:12.810 --> 00:09:12.820
challenges and opportunities of AI
 

00:09:12.820 --> 00:09:14.490
challenges and opportunities of AI
technologies as has come up in a few of

00:09:14.490 --> 00:09:14.500
technologies as has come up in a few of
 

00:09:14.500 --> 00:09:18.300
technologies as has come up in a few of
the panels is this ability to simply

00:09:18.300 --> 00:09:18.310
the panels is this ability to simply
 

00:09:18.310 --> 00:09:20.010
the panels is this ability to simply
port these technologies for many

00:09:20.010 --> 00:09:20.020
port these technologies for many
 

00:09:20.020 --> 00:09:23.130
port these technologies for many
purposes across national boundaries and

00:09:23.130 --> 00:09:23.140
purposes across national boundaries and
 

00:09:23.140 --> 00:09:25.400
purposes across national boundaries and
so international and multilateral

00:09:25.400 --> 00:09:25.410
so international and multilateral
 

00:09:25.410 --> 00:09:27.420
so international and multilateral
deliberations and discussions are really

00:09:27.420 --> 00:09:27.430
deliberations and discussions are really
 

00:09:27.430 --> 00:09:29.730
deliberations and discussions are really
a key part of a lot of the policy and

00:09:29.730 --> 00:09:29.740
a key part of a lot of the policy and
 

00:09:29.740 --> 00:09:31.140
a key part of a lot of the policy and
governance efforts that are taking place

00:09:31.140 --> 00:09:31.150
governance efforts that are taking place
 

00:09:31.150 --> 00:09:33.780
governance efforts that are taking place
around AI and robotic and other

00:09:33.780 --> 00:09:33.790
around AI and robotic and other
 

00:09:33.790 --> 00:09:36.540
around AI and robotic and other
computational technologies in fact this

00:09:36.540 --> 00:09:36.550
computational technologies in fact this
 

00:09:36.550 --> 00:09:39.120
computational technologies in fact this
very week there are a number of

00:09:39.120 --> 00:09:39.130
very week there are a number of
 

00:09:39.130 --> 00:09:41.160
very week there are a number of
high-level discussions and deliberations

00:09:41.160 --> 00:09:41.170
high-level discussions and deliberations
 

00:09:41.170 --> 00:09:43.730
high-level discussions and deliberations
at the UN United Nations in Geneva

00:09:43.730 --> 00:09:43.740
at the UN United Nations in Geneva
 

00:09:43.740 --> 00:09:47.310
at the UN United Nations in Geneva
around the issue of the increasing

00:09:47.310 --> 00:09:47.320
around the issue of the increasing
 

00:09:47.320 --> 00:09:49.710
around the issue of the increasing
weaponization or potential for

00:09:49.710 --> 00:09:49.720
weaponization or potential for
 

00:09:49.720 --> 00:09:51.840
weaponization or potential for
weaponization of autonomous technologies

00:09:51.840 --> 00:09:51.850
weaponization of autonomous technologies
 

00:09:51.850 --> 00:09:55.290
weaponization of autonomous technologies
and so we are in fact very happy today

00:09:55.290 --> 00:09:55.300
and so we are in fact very happy today
 

00:09:55.300 --> 00:09:56.820
and so we are in fact very happy today
that two of the people who are leading

00:09:56.820 --> 00:09:56.830
that two of the people who are leading
 

00:09:56.830 --> 00:09:59.730
that two of the people who are leading
those discussions helping to shape and

00:09:59.730 --> 00:09:59.740
those discussions helping to shape and
 

00:09:59.740 --> 00:10:01.800
those discussions helping to shape and
guide much of our deliberation on the

00:10:01.800 --> 00:10:01.810
guide much of our deliberation on the
 

00:10:01.810 --> 00:10:03.480
guide much of our deliberation on the
international stage are gonna be joining

00:10:03.480 --> 00:10:03.490
international stage are gonna be joining
 

00:10:03.490 --> 00:10:06.150
international stage are gonna be joining
us by videoconference from Geneva where

00:10:06.150 --> 00:10:06.160
us by videoconference from Geneva where
 

00:10:06.160 --> 00:10:07.860
us by videoconference from Geneva where
they've just actually finished up a a

00:10:07.860 --> 00:10:07.870
they've just actually finished up a a
 

00:10:07.870 --> 00:10:10.320
they've just actually finished up a a
I'm sure very busy day and yet we're

00:10:10.320 --> 00:10:10.330
I'm sure very busy day and yet we're
 

00:10:10.330 --> 00:10:13.590
I'm sure very busy day and yet we're
willing to take some other time to join

00:10:13.590 --> 00:10:13.600
willing to take some other time to join
 

00:10:13.600 --> 00:10:16.410
willing to take some other time to join
us by videoconference so ambassador

00:10:16.410 --> 00:10:16.420
us by videoconference so ambassador
 

00:10:16.420 --> 00:10:19.110
us by videoconference so ambassador
Amandeep Gil currently serves as the in

00:10:19.110 --> 00:10:19.120
Amandeep Gil currently serves as the in
 

00:10:19.120 --> 00:10:21.810
Amandeep Gil currently serves as the in
at the Indian mission to the conference

00:10:21.810 --> 00:10:21.820
at the Indian mission to the conference
 

00:10:21.820 --> 00:10:24.300
at the Indian mission to the conference
on Disarmament in Geneva and has served

00:10:24.300 --> 00:10:24.310
on Disarmament in Geneva and has served
 

00:10:24.310 --> 00:10:26.580
on Disarmament in Geneva and has served
at the Indian Embassy in Tehran the High

00:10:26.580 --> 00:10:26.590
at the Indian Embassy in Tehran the High
 

00:10:26.590 --> 00:10:27.530
at the Indian Embassy in Tehran the High
Commission on

00:10:27.530 --> 00:10:27.540
Commission on
 

00:10:27.540 --> 00:10:29.870
Commission on
of India in Colombo and the Indian

00:10:29.870 --> 00:10:29.880
of India in Colombo and the Indian
 

00:10:29.880 --> 00:10:32.300
of India in Colombo and the Indian
mission to the UN in Geneva he was the

00:10:32.300 --> 00:10:32.310
mission to the UN in Geneva he was the
 

00:10:32.310 --> 00:10:33.889
mission to the UN in Geneva he was the
head of disarmament and international

00:10:33.889 --> 00:10:33.899
head of disarmament and international
 

00:10:33.899 --> 00:10:35.629
head of disarmament and international
security in the Indian Ministry of

00:10:35.629 --> 00:10:35.639
security in the Indian Ministry of
 

00:10:35.639 --> 00:10:38.269
security in the Indian Ministry of
External Affairs from 2013 until just

00:10:38.269 --> 00:10:38.279
External Affairs from 2013 until just
 

00:10:38.279 --> 00:10:40.460
External Affairs from 2013 until just
recently and has also served as an

00:10:40.460 --> 00:10:40.470
recently and has also served as an
 

00:10:40.470 --> 00:10:42.590
recently and has also served as an
expert on the UN security generals

00:10:42.590 --> 00:10:42.600
expert on the UN security generals
 

00:10:42.600 --> 00:10:44.689
expert on the UN security generals
panels of experts on the fissile

00:10:44.689 --> 00:10:44.699
panels of experts on the fissile
 

00:10:44.699 --> 00:10:47.480
panels of experts on the fissile
material cutoff treaty on small arms and

00:10:47.480 --> 00:10:47.490
material cutoff treaty on small arms and
 

00:10:47.490 --> 00:10:50.689
material cutoff treaty on small arms and
light weapons and on missiles he's not

00:10:50.689 --> 00:10:50.699
light weapons and on missiles he's not
 

00:10:50.699 --> 00:10:52.939
light weapons and on missiles he's not
simply a career diplomat though he was

00:10:52.939 --> 00:10:52.949
simply a career diplomat though he was
 

00:10:52.949 --> 00:10:54.530
simply a career diplomat though he was
actually originally educated as an

00:10:54.530 --> 00:10:54.540
actually originally educated as an
 

00:10:54.540 --> 00:10:57.050
actually originally educated as an
engineer at Punjab University and has a

00:10:57.050 --> 00:10:57.060
engineer at Punjab University and has a
 

00:10:57.060 --> 00:10:58.850
engineer at Punjab University and has a
PhD from the department of war studies

00:10:58.850 --> 00:10:58.860
PhD from the department of war studies
 

00:10:58.860 --> 00:11:02.329
PhD from the department of war studies
at King's College London so ambassador

00:11:02.329 --> 00:11:02.339
at King's College London so ambassador
 

00:11:02.339 --> 00:11:04.129
at King's College London so ambassador
Gill is one of these people that really

00:11:04.129 --> 00:11:04.139
Gill is one of these people that really
 

00:11:04.139 --> 00:11:07.249
Gill is one of these people that really
is merging together in this case the

00:11:07.249 --> 00:11:07.259
is merging together in this case the
 

00:11:07.259 --> 00:11:10.100
is merging together in this case the
policy and the technology sides together

00:11:10.100 --> 00:11:10.110
policy and the technology sides together
 

00:11:10.110 --> 00:11:14.030
policy and the technology sides together
into a single sensible perspective and

00:11:14.030 --> 00:11:14.040
into a single sensible perspective and
 

00:11:14.040 --> 00:11:17.329
into a single sensible perspective and
since 2017 anja cos person has served as

00:11:17.329 --> 00:11:17.339
since 2017 anja cos person has served as
 

00:11:17.339 --> 00:11:19.420
since 2017 anja cos person has served as
director for un disarmament Affairs

00:11:19.420 --> 00:11:19.430
director for un disarmament Affairs
 

00:11:19.430 --> 00:11:21.350
director for un disarmament Affairs
she's the former head of strategic

00:11:21.350 --> 00:11:21.360
she's the former head of strategic
 

00:11:21.360 --> 00:11:23.629
she's the former head of strategic
engagement in new technologies at the

00:11:23.629 --> 00:11:23.639
engagement in new technologies at the
 

00:11:23.639 --> 00:11:25.040
engagement in new technologies at the
International Committee of the Red Cross

00:11:25.040 --> 00:11:25.050
International Committee of the Red Cross
 

00:11:25.050 --> 00:11:28.189
International Committee of the Red Cross
the ICRC and was previously a member of

00:11:28.189 --> 00:11:28.199
the ICRC and was previously a member of
 

00:11:28.199 --> 00:11:29.870
the ICRC and was previously a member of
the executive committee and senior

00:11:29.870 --> 00:11:29.880
the executive committee and senior
 

00:11:29.880 --> 00:11:32.540
the executive committee and senior
director at the World Economic Forum

00:11:32.540 --> 00:11:32.550
director at the World Economic Forum
 

00:11:32.550 --> 00:11:34.970
director at the World Economic Forum
she's had a remarkable career that's

00:11:34.970 --> 00:11:34.980
she's had a remarkable career that's
 

00:11:34.980 --> 00:11:37.990
she's had a remarkable career that's
highly interdisciplinary

00:11:37.990 --> 00:11:38.000
 

00:11:38.000 --> 00:11:41.720
spanning over technology policy business

00:11:41.720 --> 00:11:41.730
spanning over technology policy business
 

00:11:41.730 --> 00:11:46.250
spanning over technology policy business
and even academia in including working

00:11:46.250 --> 00:11:46.260
and even academia in including working
 

00:11:46.260 --> 00:11:47.900
and even academia in including working
for the Norwegian government as well as

00:11:47.900 --> 00:11:47.910
for the Norwegian government as well as
 

00:11:47.910 --> 00:11:49.790
for the Norwegian government as well as
a number of international organizations

00:11:49.790 --> 00:11:49.800
a number of international organizations
 

00:11:49.800 --> 00:11:54.350
a number of international organizations
so I'm hoping that I'm going to look

00:11:54.350 --> 00:11:54.360
so I'm hoping that I'm going to look
 

00:11:54.360 --> 00:11:55.759
so I'm hoping that I'm going to look
over at our technical people and say I'm

00:11:55.759 --> 00:11:55.769
over at our technical people and say I'm
 

00:11:55.769 --> 00:11:58.910
over at our technical people and say I'm
hoping and we're getting I'm getting a

00:11:58.910 --> 00:11:58.920
hoping and we're getting I'm getting a
 

00:11:58.920 --> 00:12:03.259
hoping and we're getting I'm getting a
signal for just one more minute so one

00:12:03.259 --> 00:12:03.269
signal for just one more minute so one
 

00:12:03.269 --> 00:12:04.900
signal for just one more minute so one
of the things that that is a real

00:12:04.900 --> 00:12:04.910
of the things that that is a real
 

00:12:04.910 --> 00:12:07.550
of the things that that is a real
challenge of course even a technological

00:12:07.550 --> 00:12:07.560
challenge of course even a technological
 

00:12:07.560 --> 00:12:09.350
challenge of course even a technological
school such as this one is this

00:12:09.350 --> 00:12:09.360
school such as this one is this
 

00:12:09.360 --> 00:12:11.259
school such as this one is this
difficulty of communicating across

00:12:11.259 --> 00:12:11.269
difficulty of communicating across
 

00:12:11.269 --> 00:12:14.540
difficulty of communicating across
physical space across cultural space and

00:12:14.540 --> 00:12:14.550
physical space across cultural space and
 

00:12:14.550 --> 00:12:16.340
physical space across cultural space and
I think we've even seen some of that in

00:12:16.340 --> 00:12:16.350
I think we've even seen some of that in
 

00:12:16.350 --> 00:12:17.480
I think we've even seen some of that in
the discussions that have had been

00:12:17.480 --> 00:12:17.490
the discussions that have had been
 

00:12:17.490 --> 00:12:18.500
the discussions that have had been
happening certainly some of the

00:12:18.500 --> 00:12:18.510
happening certainly some of the
 

00:12:18.510 --> 00:12:20.030
happening certainly some of the
discussions that I've been involved with

00:12:20.030 --> 00:12:20.040
discussions that I've been involved with
 

00:12:20.040 --> 00:12:23.329
discussions that I've been involved with
here how do we bridge disciplinary gaps

00:12:23.329 --> 00:12:23.339
here how do we bridge disciplinary gaps
 

00:12:23.339 --> 00:12:25.400
here how do we bridge disciplinary gaps
as well there's been the talk of how do

00:12:25.400 --> 00:12:25.410
as well there's been the talk of how do
 

00:12:25.410 --> 00:12:27.199
as well there's been the talk of how do
we increase the technical literacy of

00:12:27.199 --> 00:12:27.209
we increase the technical literacy of
 

00:12:27.209 --> 00:12:29.030
we increase the technical literacy of
the populace at the same time I think

00:12:29.030 --> 00:12:29.040
the populace at the same time I think
 

00:12:29.040 --> 00:12:30.139
the populace at the same time I think
there have been questions about how do

00:12:30.139 --> 00:12:30.149
there have been questions about how do
 

00:12:30.149 --> 00:12:32.420
there have been questions about how do
we increase the understanding of the

00:12:32.420 --> 00:12:32.430
we increase the understanding of the
 

00:12:32.430 --> 00:12:34.340
we increase the understanding of the
role of ethics and values perhaps within

00:12:34.340 --> 00:12:34.350
role of ethics and values perhaps within
 

00:12:34.350 --> 00:12:36.439
role of ethics and values perhaps within
the technological community and I'm now

00:12:36.439 --> 00:12:36.449
the technological community and I'm now
 

00:12:36.449 --> 00:12:38.150
the technological community and I'm now
getting a thumbs-up so I'd like to say

00:12:38.150 --> 00:12:38.160
getting a thumbs-up so I'd like to say
 

00:12:38.160 --> 00:12:40.189
getting a thumbs-up so I'd like to say
Amandeep and Anja thank you so much for

00:12:40.189 --> 00:12:40.199
Amandeep and Anja thank you so much for
 

00:12:40.199 --> 00:12:41.450
Amandeep and Anja thank you so much for
joining us

00:12:41.450 --> 00:12:41.460
joining us
 

00:12:41.460 --> 00:12:48.410
joining us
from afar it's a great thank you for the

00:12:48.410 --> 00:12:48.420
from afar it's a great thank you for the
 

00:12:48.420 --> 00:12:51.200
from afar it's a great thank you for the
kind words I'm very happy to join this

00:12:51.200 --> 00:12:51.210
kind words I'm very happy to join this
 

00:12:51.210 --> 00:12:56.150
kind words I'm very happy to join this
discussion thank you so much so I

00:12:56.150 --> 00:12:56.160
discussion thank you so much so I
 

00:12:56.160 --> 00:12:58.250
discussion thank you so much so I
thought I would just start with a very

00:12:58.250 --> 00:12:58.260
thought I would just start with a very
 

00:12:58.260 --> 00:13:00.890
thought I would just start with a very
somewhat high-level and general question

00:13:00.890 --> 00:13:00.900
somewhat high-level and general question
 

00:13:00.900 --> 00:13:03.590
somewhat high-level and general question
for the both of you which is in terms of

00:13:03.590 --> 00:13:03.600
for the both of you which is in terms of
 

00:13:03.600 --> 00:13:05.450
for the both of you which is in terms of
thinking about international policy and

00:13:05.450 --> 00:13:05.460
thinking about international policy and
 

00:13:05.460 --> 00:13:07.670
thinking about international policy and
governance I'm curious what kinds of

00:13:07.670 --> 00:13:07.680
governance I'm curious what kinds of
 

00:13:07.680 --> 00:13:10.490
governance I'm curious what kinds of
distinctive challenges that is ones that

00:13:10.490 --> 00:13:10.500
distinctive challenges that is ones that
 

00:13:10.500 --> 00:13:12.830
distinctive challenges that is ones that
are new to these kinds of technologies

00:13:12.830 --> 00:13:12.840
are new to these kinds of technologies
 

00:13:12.840 --> 00:13:15.140
are new to these kinds of technologies
do you think are presented by AI and

00:13:15.140 --> 00:13:15.150
do you think are presented by AI and
 

00:13:15.150 --> 00:13:17.120
do you think are presented by AI and
other autonomous systems whether in the

00:13:17.120 --> 00:13:17.130
other autonomous systems whether in the
 

00:13:17.130 --> 00:13:22.430
other autonomous systems whether in the
weapons space or elsewhere right I think

00:13:22.430 --> 00:13:22.440
weapons space or elsewhere right I think
 

00:13:22.440 --> 00:13:25.220
weapons space or elsewhere right I think
in the multilateral space the biggest

00:13:25.220 --> 00:13:25.230
in the multilateral space the biggest
 

00:13:25.230 --> 00:13:29.480
in the multilateral space the biggest
challenge is to harmonize the very

00:13:29.480 --> 00:13:29.490
challenge is to harmonize the very
 

00:13:29.490 --> 00:13:33.410
challenge is to harmonize the very
different perspectives on technology in

00:13:33.410 --> 00:13:33.420
different perspectives on technology in
 

00:13:33.420 --> 00:13:35.570
different perspectives on technology in
this particular context the meeting that

00:13:35.570 --> 00:13:35.580
this particular context the meeting that
 

00:13:35.580 --> 00:13:39.200
this particular context the meeting that
I'm sharing you have 125 states with

00:13:39.200 --> 00:13:39.210
I'm sharing you have 125 states with
 

00:13:39.210 --> 00:13:41.660
I'm sharing you have 125 states with
very different economic systems with

00:13:41.660 --> 00:13:41.670
very different economic systems with
 

00:13:41.670 --> 00:13:44.050
very different economic systems with
very different levels of development

00:13:44.050 --> 00:13:44.060
very different levels of development
 

00:13:44.060 --> 00:13:47.600
very different levels of development
including technology development so how

00:13:47.600 --> 00:13:47.610
including technology development so how
 

00:13:47.610 --> 00:13:49.790
including technology development so how
do you bring everyone on to the same

00:13:49.790 --> 00:13:49.800
do you bring everyone on to the same
 

00:13:49.800 --> 00:13:54.100
do you bring everyone on to the same
page the other challenge is how do you

00:13:54.100 --> 00:13:54.110
page the other challenge is how do you
 

00:13:54.110 --> 00:13:57.410
page the other challenge is how do you
bring the world of technology into the

00:13:57.410 --> 00:13:57.420
bring the world of technology into the
 

00:13:57.420 --> 00:14:00.710
bring the world of technology into the
world of diplomacy and international

00:14:00.710 --> 00:14:00.720
world of diplomacy and international
 

00:14:00.720 --> 00:14:03.800
world of diplomacy and international
relations there are expert communities

00:14:03.800 --> 00:14:03.810
relations there are expert communities
 

00:14:03.810 --> 00:14:06.740
relations there are expert communities
in this field of which do not

00:14:06.740 --> 00:14:06.750
in this field of which do not
 

00:14:06.750 --> 00:14:09.410
in this field of which do not
necessarily interact with the technology

00:14:09.410 --> 00:14:09.420
necessarily interact with the technology
 

00:14:09.420 --> 00:14:12.320
necessarily interact with the technology
world it is easier in some places for

00:14:12.320 --> 00:14:12.330
world it is easier in some places for
 

00:14:12.330 --> 00:14:14.480
world it is easier in some places for
example Vienna where the International

00:14:14.480 --> 00:14:14.490
example Vienna where the International
 

00:14:14.490 --> 00:14:17.230
example Vienna where the International
Atomic Energy agency's that culture of

00:14:17.230 --> 00:14:17.240
Atomic Energy agency's that culture of
 

00:14:17.240 --> 00:14:19.550
Atomic Energy agency's that culture of
interactivity where the tech world has

00:14:19.550 --> 00:14:19.560
interactivity where the tech world has
 

00:14:19.560 --> 00:14:21.710
interactivity where the tech world has
been built up over the years but it's

00:14:21.710 --> 00:14:21.720
been built up over the years but it's
 

00:14:21.720 --> 00:14:25.040
been built up over the years but it's
not always so in Tudor so easy in other

00:14:25.040 --> 00:14:25.050
not always so in Tudor so easy in other
 

00:14:25.050 --> 00:14:28.420
not always so in Tudor so easy in other
places so that is the the other

00:14:28.420 --> 00:14:28.430
places so that is the the other
 

00:14:28.430 --> 00:14:31.220
places so that is the the other
challenge and third a challenge I would

00:14:31.220 --> 00:14:31.230
challenge and third a challenge I would
 

00:14:31.230 --> 00:14:34.040
challenge and third a challenge I would
say is simply keeping up with the pace

00:14:34.040 --> 00:14:34.050
say is simply keeping up with the pace
 

00:14:34.050 --> 00:14:35.860
say is simply keeping up with the pace
of Technology

00:14:35.860 --> 00:14:35.870
of Technology
 

00:14:35.870 --> 00:14:39.050
of Technology
technology moves very fast it also moves

00:14:39.050 --> 00:14:39.060
technology moves very fast it also moves
 

00:14:39.060 --> 00:14:43.460
technology moves very fast it also moves
in mysterious ways and often we are

00:14:43.460 --> 00:14:43.470
in mysterious ways and often we are
 

00:14:43.470 --> 00:14:45.380
in mysterious ways and often we are
looking at technology from the

00:14:45.380 --> 00:14:45.390
looking at technology from the
 

00:14:45.390 --> 00:14:47.420
looking at technology from the
perspective of legally binding

00:14:47.420 --> 00:14:47.430
perspective of legally binding
 

00:14:47.430 --> 00:14:50.360
perspective of legally binding
convention a forum with certain rules

00:14:50.360 --> 00:14:50.370
convention a forum with certain rules
 

00:14:50.370 --> 00:14:53.930
convention a forum with certain rules
and regulations and that's a rigid times

00:14:53.930 --> 00:14:53.940
and regulations and that's a rigid times
 

00:14:53.940 --> 00:14:54.650
and regulations and that's a rigid times
best

00:14:54.650 --> 00:14:54.660
best
 

00:14:54.660 --> 00:14:57.230
best
framework from which to look at

00:14:57.230 --> 00:14:57.240
framework from which to look at
 

00:14:57.240 --> 00:15:00.939
framework from which to look at
technology that's a fast-moving mutating

00:15:00.939 --> 00:15:00.949
technology that's a fast-moving mutating
 

00:15:00.949 --> 00:15:03.680
technology that's a fast-moving mutating
target so those are some of the

00:15:03.680 --> 00:15:03.690
target so those are some of the
 

00:15:03.690 --> 00:15:07.249
target so those are some of the
challenges I can think of honor thank

00:15:07.249 --> 00:15:07.259
challenges I can think of honor thank
 

00:15:07.259 --> 00:15:09.139
challenges I can think of honor thank
you thank you so much for for inviting

00:15:09.139 --> 00:15:09.149
you thank you so much for for inviting
 

00:15:09.149 --> 00:15:11.210
you thank you so much for for inviting
us as letting us be part of this

00:15:11.210 --> 00:15:11.220
us as letting us be part of this
 

00:15:11.220 --> 00:15:14.629
us as letting us be part of this
remotely just to follow up what would a

00:15:14.629 --> 00:15:14.639
remotely just to follow up what would a
 

00:15:14.639 --> 00:15:17.809
remotely just to follow up what would a
MIDI be saying the you know it's hard

00:15:17.809 --> 00:15:17.819
MIDI be saying the you know it's hard
 

00:15:17.819 --> 00:15:19.670
MIDI be saying the you know it's hard
enough to follow the pace of Technology

00:15:19.670 --> 00:15:19.680
enough to follow the pace of Technology
 

00:15:19.680 --> 00:15:22.189
enough to follow the pace of Technology
but also building the literacy on

00:15:22.189 --> 00:15:22.199
but also building the literacy on
 

00:15:22.199 --> 00:15:25.460
but also building the literacy on
somewhat unusual areas you know for for

00:15:25.460 --> 00:15:25.470
somewhat unusual areas you know for for
 

00:15:25.470 --> 00:15:27.110
somewhat unusual areas you know for for
the crowd that usually comes together to

00:15:27.110 --> 00:15:27.120
the crowd that usually comes together to
 

00:15:27.120 --> 00:15:30.019
the crowd that usually comes together to
discuss this this issue issues and also

00:15:30.019 --> 00:15:30.029
discuss this this issue issues and also
 

00:15:30.029 --> 00:15:31.670
discuss this this issue issues and also
to make the I will say the organic

00:15:31.670 --> 00:15:31.680
to make the I will say the organic
 

00:15:31.680 --> 00:15:34.240
to make the I will say the organic
relationships between the different

00:15:34.240 --> 00:15:34.250
relationships between the different
 

00:15:34.250 --> 00:15:35.929
relationships between the different
communities and the different

00:15:35.929 --> 00:15:35.939
communities and the different
 

00:15:35.939 --> 00:15:39.460
communities and the different
stakeholders to make sure that voices of

00:15:39.460 --> 00:15:39.470
stakeholders to make sure that voices of
 

00:15:39.470 --> 00:15:41.660
stakeholders to make sure that voices of
technique you know technologists and

00:15:41.660 --> 00:15:41.670
technique you know technologists and
 

00:15:41.670 --> 00:15:43.999
technique you know technologists and
scientists you know people in industry

00:15:43.999 --> 00:15:44.009
scientists you know people in industry
 

00:15:44.009 --> 00:15:46.939
scientists you know people in industry
in various participating in various

00:15:46.939 --> 00:15:46.949
in various participating in various
 

00:15:46.949 --> 00:15:50.480
in various participating in various
levels of the value chain of Technology

00:15:50.480 --> 00:15:50.490
levels of the value chain of Technology
 

00:15:50.490 --> 00:15:52.249
levels of the value chain of Technology
development but also application and

00:15:52.249 --> 00:15:52.259
development but also application and
 

00:15:52.259 --> 00:15:54.800
development but also application and
deployment you know from our civilian

00:15:54.800 --> 00:15:54.810
deployment you know from our civilian
 

00:15:54.810 --> 00:15:57.050
deployment you know from our civilian
life and day to day life to the armed

00:15:57.050 --> 00:15:57.060
life and day to day life to the armed
 

00:15:57.060 --> 00:15:59.629
life and day to day life to the armed
space so creating those organic links

00:15:59.629 --> 00:15:59.639
space so creating those organic links
 

00:15:59.639 --> 00:16:01.850
space so creating those organic links
and platforms to have discussions where

00:16:01.850 --> 00:16:01.860
and platforms to have discussions where
 

00:16:01.860 --> 00:16:04.220
and platforms to have discussions where
we can share information and and build

00:16:04.220 --> 00:16:04.230
we can share information and and build
 

00:16:04.230 --> 00:16:06.980
we can share information and and build
that much needed literacy but also

00:16:06.980 --> 00:16:06.990
that much needed literacy but also
 

00:16:06.990 --> 00:16:10.879
that much needed literacy but also
common vernacular a common use and

00:16:10.879 --> 00:16:10.889
common vernacular a common use and
 

00:16:10.889 --> 00:16:13.280
common vernacular a common use and
understanding of the terms and and some

00:16:13.280 --> 00:16:13.290
understanding of the terms and and some
 

00:16:13.290 --> 00:16:15.949
understanding of the terms and and some
of the developments is it's challenging

00:16:15.949 --> 00:16:15.959
of the developments is it's challenging
 

00:16:15.959 --> 00:16:17.780
of the developments is it's challenging
but worthwhile and something that we are

00:16:17.780 --> 00:16:17.790
but worthwhile and something that we are
 

00:16:17.790 --> 00:16:20.480
but worthwhile and something that we are
certainly working every day to try to to

00:16:20.480 --> 00:16:20.490
certainly working every day to try to to
 

00:16:20.490 --> 00:16:24.530
certainly working every day to try to to
remedy and and to improve there was a

00:16:24.530 --> 00:16:24.540
remedy and and to improve there was a
 

00:16:24.540 --> 00:16:28.850
remedy and and to improve there was a
mention of of the IAEA in Vienna as a

00:16:28.850 --> 00:16:28.860
mention of of the IAEA in Vienna as a
 

00:16:28.860 --> 00:16:31.059
mention of of the IAEA in Vienna as a
place where the technological

00:16:31.059 --> 00:16:31.069
place where the technological
 

00:16:31.069 --> 00:16:33.079
place where the technological
communities have come together at the

00:16:33.079 --> 00:16:33.089
communities have come together at the
 

00:16:33.089 --> 00:16:34.910
communities have come together at the
diplomatic are there lessons that we can

00:16:34.910 --> 00:16:34.920
diplomatic are there lessons that we can
 

00:16:34.920 --> 00:16:38.150
diplomatic are there lessons that we can
learn from the experiences around that

00:16:38.150 --> 00:16:38.160
learn from the experiences around that
 

00:16:38.160 --> 00:16:41.210
learn from the experiences around that
space that might be might be useful over

00:16:41.210 --> 00:16:41.220
space that might be might be useful over
 

00:16:41.220 --> 00:16:43.429
space that might be might be useful over
in the AI and robotics face because I

00:16:43.429 --> 00:16:43.439
in the AI and robotics face because I
 

00:16:43.439 --> 00:16:46.069
in the AI and robotics face because I
think one of the challenges is exactly

00:16:46.069 --> 00:16:46.079
think one of the challenges is exactly
 

00:16:46.079 --> 00:16:48.650
think one of the challenges is exactly
people learning to talk to one another

00:16:48.650 --> 00:16:48.660
people learning to talk to one another
 

00:16:48.660 --> 00:16:51.049
people learning to talk to one another
as you were both saying and so are there

00:16:51.049 --> 00:16:51.059
as you were both saying and so are there
 

00:16:51.059 --> 00:16:52.730
as you were both saying and so are there
perhaps lessons learned that we should

00:16:52.730 --> 00:16:52.740
perhaps lessons learned that we should
 

00:16:52.740 --> 00:16:58.240
perhaps lessons learned that we should
in this space be trying to bring to bear

00:16:58.240 --> 00:16:58.250
 

00:16:58.250 --> 00:17:01.670
well I think in jumping to the AI and

00:17:01.670 --> 00:17:01.680
well I think in jumping to the AI and
 

00:17:01.680 --> 00:17:07.610
well I think in jumping to the AI and
robotics space I would be careful and

00:17:07.610 --> 00:17:07.620
robotics space I would be careful and
 

00:17:07.620 --> 00:17:08.050
robotics space I would be careful and
Link

00:17:08.050 --> 00:17:08.060
Link
 

00:17:08.060 --> 00:17:11.320
Link
into existing models of governance

00:17:11.320 --> 00:17:11.330
into existing models of governance
 

00:17:11.330 --> 00:17:15.130
into existing models of governance
howsoever successful they have been this

00:17:15.130 --> 00:17:15.140
howsoever successful they have been this
 

00:17:15.140 --> 00:17:20.770
howsoever successful they have been this
is a unique area in many ways and unlike

00:17:20.770 --> 00:17:20.780
is a unique area in many ways and unlike
 

00:17:20.780 --> 00:17:22.630
is a unique area in many ways and unlike
the nuclear field where you know the

00:17:22.630 --> 00:17:22.640
the nuclear field where you know the
 

00:17:22.640 --> 00:17:25.210
the nuclear field where you know the
pathways to a nuclear weapon these are

00:17:25.210 --> 00:17:25.220
pathways to a nuclear weapon these are
 

00:17:25.220 --> 00:17:27.270
pathways to a nuclear weapon these are
the ways you produce fissile material

00:17:27.270 --> 00:17:27.280
the ways you produce fissile material
 

00:17:27.280 --> 00:17:30.610
the ways you produce fissile material
and these are the ways to count them or

00:17:30.610 --> 00:17:30.620
and these are the ways to count them or
 

00:17:30.620 --> 00:17:32.860
and these are the ways to count them or
to deliver them you know this is this is

00:17:32.860 --> 00:17:32.870
to deliver them you know this is this is
 

00:17:32.870 --> 00:17:36.310
to deliver them you know this is this is
a different beast and so we have to cast

00:17:36.310 --> 00:17:36.320
a different beast and so we have to cast
 

00:17:36.320 --> 00:17:38.920
a different beast and so we have to cast
a net wide to look for good examples of

00:17:38.920 --> 00:17:38.930
a net wide to look for good examples of
 

00:17:38.930 --> 00:17:43.060
a net wide to look for good examples of
governance and what I have been focusing

00:17:43.060 --> 00:17:43.070
governance and what I have been focusing
 

00:17:43.070 --> 00:17:45.700
governance and what I have been focusing
on is is something called the

00:17:45.700 --> 00:17:45.710
on is is something called the
 

00:17:45.710 --> 00:17:47.290
on is is something called the
distributed technology governance

00:17:47.290 --> 00:17:47.300
distributed technology governance
 

00:17:47.300 --> 00:17:50.380
distributed technology governance
approach where you do what you can do at

00:17:50.380 --> 00:17:50.390
approach where you do what you can do at
 

00:17:50.390 --> 00:17:52.750
approach where you do what you can do at
the international level at the treaty

00:17:52.750 --> 00:17:52.760
the international level at the treaty
 

00:17:52.760 --> 00:17:56.020
the international level at the treaty
making level but you also do that

00:17:56.020 --> 00:17:56.030
making level but you also do that
 

00:17:56.030 --> 00:17:58.000
making level but you also do that
mindful of what's happening in the

00:17:58.000 --> 00:17:58.010
mindful of what's happening in the
 

00:17:58.010 --> 00:18:01.960
mindful of what's happening in the
national space so in harmony in sync

00:18:01.960 --> 00:18:01.970
national space so in harmony in sync
 

00:18:01.970 --> 00:18:04.720
national space so in harmony in sync
with national governance national

00:18:04.720 --> 00:18:04.730
with national governance national
 

00:18:04.730 --> 00:18:07.990
with national governance national
regulation and also in sync with this

00:18:07.990 --> 00:18:08.000
regulation and also in sync with this
 

00:18:08.000 --> 00:18:12.660
regulation and also in sync with this
third very important level of industry

00:18:12.660 --> 00:18:12.670
third very important level of industry
 

00:18:12.670 --> 00:18:15.910
third very important level of industry
industry driven governance so these

00:18:15.910 --> 00:18:15.920
industry driven governance so these
 

00:18:15.920 --> 00:18:17.800
industry driven governance so these
three levels have to work in sync and

00:18:17.800 --> 00:18:17.810
three levels have to work in sync and
 

00:18:17.810 --> 00:18:20.800
three levels have to work in sync and
have to come together perhaps from the

00:18:20.800 --> 00:18:20.810
have to come together perhaps from the
 

00:18:20.810 --> 00:18:22.900
have to come together perhaps from the
nuclear space or one relatively

00:18:22.900 --> 00:18:22.910
nuclear space or one relatively
 

00:18:22.910 --> 00:18:24.820
nuclear space or one relatively
successful example is the Nuclear

00:18:24.820 --> 00:18:24.830
successful example is the Nuclear
 

00:18:24.830 --> 00:18:28.110
successful example is the Nuclear
Security Summit which President Obama

00:18:28.110 --> 00:18:28.120
Security Summit which President Obama
 

00:18:28.120 --> 00:18:31.030
Security Summit which President Obama
initiated where different communities

00:18:31.030 --> 00:18:31.040
initiated where different communities
 

00:18:31.040 --> 00:18:33.730
initiated where different communities
came together and they was shared

00:18:33.730 --> 00:18:33.740
came together and they was shared
 

00:18:33.740 --> 00:18:36.490
came together and they was shared
learning as soon as you brought the

00:18:36.490 --> 00:18:36.500
learning as soon as you brought the
 

00:18:36.500 --> 00:18:39.930
learning as soon as you brought the
issue out of the the basements of

00:18:39.930 --> 00:18:39.940
issue out of the the basements of
 

00:18:39.940 --> 00:18:42.340
issue out of the the basements of
departments of Energy Department's of

00:18:42.340 --> 00:18:42.350
departments of Energy Department's of
 

00:18:42.350 --> 00:18:44.590
departments of Energy Department's of
state and you injected new communities

00:18:44.590 --> 00:18:44.600
state and you injected new communities
 

00:18:44.600 --> 00:18:46.810
state and you injected new communities
into the discussion whether it was the

00:18:46.810 --> 00:18:46.820
into the discussion whether it was the
 

00:18:46.820 --> 00:18:49.840
into the discussion whether it was the
intelligence community or the scientific

00:18:49.840 --> 00:18:49.850
intelligence community or the scientific
 

00:18:49.850 --> 00:18:52.780
intelligence community or the scientific
community the vernacular as Anna was

00:18:52.780 --> 00:18:52.790
community the vernacular as Anna was
 

00:18:52.790 --> 00:18:55.630
community the vernacular as Anna was
saying evolved common understandings got

00:18:55.630 --> 00:18:55.640
saying evolved common understandings got
 

00:18:55.640 --> 00:18:58.750
saying evolved common understandings got
established and you had links to the

00:18:58.750 --> 00:18:58.760
established and you had links to the
 

00:18:58.760 --> 00:19:02.500
established and you had links to the
leaders and that speeded up the pace of

00:19:02.500 --> 00:19:02.510
leaders and that speeded up the pace of
 

00:19:02.510 --> 00:19:05.320
leaders and that speeded up the pace of
learning so AI and robotics

00:19:05.320 --> 00:19:05.330
learning so AI and robotics
 

00:19:05.330 --> 00:19:07.380
learning so AI and robotics
there are currently I would say

00:19:07.380 --> 00:19:07.390
there are currently I would say
 

00:19:07.390 --> 00:19:09.490
there are currently I would say
experiments being carried out in

00:19:09.490 --> 00:19:09.500
experiments being carried out in
 

00:19:09.500 --> 00:19:11.800
experiments being carried out in
governance the meeting that I'm sharing

00:19:11.800 --> 00:19:11.810
governance the meeting that I'm sharing
 

00:19:11.810 --> 00:19:14.710
governance the meeting that I'm sharing
is one such experiment these summits

00:19:14.710 --> 00:19:14.720
is one such experiment these summits
 

00:19:14.720 --> 00:19:16.720
is one such experiment these summits
that the international telecommunication

00:19:16.720 --> 00:19:16.730
that the international telecommunication
 

00:19:16.730 --> 00:19:19.810
that the international telecommunication
union has started since last year the AI

00:19:19.810 --> 00:19:19.820
union has started since last year the AI
 

00:19:19.820 --> 00:19:21.369
union has started since last year the AI
for good

00:19:21.369 --> 00:19:21.379
for good
 

00:19:21.379 --> 00:19:25.659
for good
some another experiment what i tripoli

00:19:25.659 --> 00:19:25.669
some another experiment what i tripoli
 

00:19:25.669 --> 00:19:27.460
some another experiment what i tripoli
and other industry associations are

00:19:27.460 --> 00:19:27.470
and other industry associations are
 

00:19:27.470 --> 00:19:29.909
and other industry associations are
doing with the common glossary of terms

00:19:29.909 --> 00:19:29.919
doing with the common glossary of terms
 

00:19:29.919 --> 00:19:32.259
doing with the common glossary of terms
inviting inputs from across the globe

00:19:32.259 --> 00:19:32.269
inviting inputs from across the globe
 

00:19:32.269 --> 00:19:35.499
inviting inputs from across the globe
that's another attempt at governance so

00:19:35.499 --> 00:19:35.509
that's another attempt at governance so
 

00:19:35.509 --> 00:19:38.680
that's another attempt at governance so
we have to be open to all these

00:19:38.680 --> 00:19:38.690
we have to be open to all these
 

00:19:38.690 --> 00:19:40.749
we have to be open to all these
different experiences and be creative

00:19:40.749 --> 00:19:40.759
different experiences and be creative
 

00:19:40.759 --> 00:19:43.089
different experiences and be creative
and be sensitive about the requirements

00:19:43.089 --> 00:19:43.099
and be sensitive about the requirements
 

00:19:43.099 --> 00:19:47.440
and be sensitive about the requirements
of these new technologies I mean thank

00:19:47.440 --> 00:19:47.450
of these new technologies I mean thank
 

00:19:47.450 --> 00:19:49.930
of these new technologies I mean thank
you it's a great question I mean in the

00:19:49.930 --> 00:19:49.940
you it's a great question I mean in the
 

00:19:49.940 --> 00:19:51.549
you it's a great question I mean in the
debate it's a lot of talk about you know

00:19:51.549 --> 00:19:51.559
debate it's a lot of talk about you know
 

00:19:51.559 --> 00:19:53.649
debate it's a lot of talk about you know
adaptive governance or distributed

00:19:53.649 --> 00:19:53.659
adaptive governance or distributed
 

00:19:53.659 --> 00:19:56.680
adaptive governance or distributed
governance as Amandeep was referring to

00:19:56.680 --> 00:19:56.690
governance as Amandeep was referring to
 

00:19:56.690 --> 00:19:59.830
governance as Amandeep was referring to
I think the the issue which we certainly

00:19:59.830 --> 00:19:59.840
I think the the issue which we certainly
 

00:19:59.840 --> 00:20:02.560
I think the the issue which we certainly
face a lot working for the UN because we

00:20:02.560 --> 00:20:02.570
face a lot working for the UN because we
 

00:20:02.570 --> 00:20:04.330
face a lot working for the UN because we
we operate on so many different levels

00:20:04.330 --> 00:20:04.340
we operate on so many different levels
 

00:20:04.340 --> 00:20:06.820
we operate on so many different levels
you know from dialogues that we are

00:20:06.820 --> 00:20:06.830
you know from dialogues that we are
 

00:20:06.830 --> 00:20:09.339
you know from dialogues that we are
having this week you know on the broader

00:20:09.339 --> 00:20:09.349
having this week you know on the broader
 

00:20:09.349 --> 00:20:11.560
having this week you know on the broader
strategic level to also being an

00:20:11.560 --> 00:20:11.570
strategic level to also being an
 

00:20:11.570 --> 00:20:13.119
strategic level to also being an
operational actor and dealing with

00:20:13.119 --> 00:20:13.129
operational actor and dealing with
 

00:20:13.129 --> 00:20:14.889
operational actor and dealing with
people in the day-to-day lives you know

00:20:14.889 --> 00:20:14.899
people in the day-to-day lives you know
 

00:20:14.899 --> 00:20:17.289
people in the day-to-day lives you know
being in a conflict zone or or in an

00:20:17.289 --> 00:20:17.299
being in a conflict zone or or in an
 

00:20:17.299 --> 00:20:19.210
being in a conflict zone or or in an
area of great development or even in a

00:20:19.210 --> 00:20:19.220
area of great development or even in a
 

00:20:19.220 --> 00:20:21.009
area of great development or even in a
in a more kind of modern you know

00:20:21.009 --> 00:20:21.019
in a more kind of modern you know
 

00:20:21.019 --> 00:20:22.629
in a more kind of modern you know
Western setting but where there are

00:20:22.629 --> 00:20:22.639
Western setting but where there are
 

00:20:22.639 --> 00:20:24.940
Western setting but where there are
other issues in terms of labor laws or

00:20:24.940 --> 00:20:24.950
other issues in terms of labor laws or
 

00:20:24.950 --> 00:20:26.349
other issues in terms of labor laws or
other regulations that need to be put in

00:20:26.349 --> 00:20:26.359
other regulations that need to be put in
 

00:20:26.359 --> 00:20:30.009
other regulations that need to be put in
place and what you learn seeing the sort

00:20:30.009 --> 00:20:30.019
place and what you learn seeing the sort
 

00:20:30.019 --> 00:20:33.249
place and what you learn seeing the sort
of scale of for me is not just the the

00:20:33.249 --> 00:20:33.259
of scale of for me is not just the the
 

00:20:33.259 --> 00:20:35.680
of scale of for me is not just the the
scale of technology development but it's

00:20:35.680 --> 00:20:35.690
scale of technology development but it's
 

00:20:35.690 --> 00:20:39.009
scale of technology development but it's
also the scale of our adoption and with

00:20:39.009 --> 00:20:39.019
also the scale of our adoption and with
 

00:20:39.019 --> 00:20:41.680
also the scale of our adoption and with
our maturity not following the same

00:20:41.680 --> 00:20:41.690
our maturity not following the same
 

00:20:41.690 --> 00:20:44.440
our maturity not following the same
maturity around what the impact is you

00:20:44.440 --> 00:20:44.450
maturity around what the impact is you
 

00:20:44.450 --> 00:20:46.869
maturity around what the impact is you
know of this adoption doesn't follow the

00:20:46.869 --> 00:20:46.879
know of this adoption doesn't follow the
 

00:20:46.879 --> 00:20:49.719
know of this adoption doesn't follow the
same pace so we have to the governance

00:20:49.719 --> 00:20:49.729
same pace so we have to the governance
 

00:20:49.729 --> 00:20:51.940
same pace so we have to the governance
also need to be directed towards the

00:20:51.940 --> 00:20:51.950
also need to be directed towards the
 

00:20:51.950 --> 00:20:53.769
also need to be directed towards the
different impact this will have at the

00:20:53.769 --> 00:20:53.779
different impact this will have at the
 

00:20:53.779 --> 00:20:55.479
different impact this will have at the
different governance levels the way you

00:20:55.479 --> 00:20:55.489
different governance levels the way you
 

00:20:55.489 --> 00:20:57.639
different governance levels the way you
speak about the global level the

00:20:57.639 --> 00:20:57.649
speak about the global level the
 

00:20:57.649 --> 00:21:00.039
speak about the global level the
regional level the national level the

00:21:00.039 --> 00:21:00.049
regional level the national level the
 

00:21:00.049 --> 00:21:02.229
regional level the national level the
local level more city structures and

00:21:02.229 --> 00:21:02.239
local level more city structures and
 

00:21:02.239 --> 00:21:04.930
local level more city structures and
community structures how it impacts on

00:21:04.930 --> 00:21:04.940
community structures how it impacts on
 

00:21:04.940 --> 00:21:06.489
community structures how it impacts on
industry which kind of reflects

00:21:06.489 --> 00:21:06.499
industry which kind of reflects
 

00:21:06.499 --> 00:21:08.320
industry which kind of reflects
depending on what industry you in and

00:21:08.320 --> 00:21:08.330
depending on what industry you in and
 

00:21:08.330 --> 00:21:10.479
depending on what industry you in and
the size of your industry will kind of

00:21:10.479 --> 00:21:10.489
the size of your industry will kind of
 

00:21:10.489 --> 00:21:12.989
the size of your industry will kind of
be linked into each one of these levels

00:21:12.989 --> 00:21:12.999
be linked into each one of these levels
 

00:21:12.999 --> 00:21:15.700
be linked into each one of these levels
so and that's one big challenge the

00:21:15.700 --> 00:21:15.710
so and that's one big challenge the
 

00:21:15.710 --> 00:21:17.830
so and that's one big challenge the
other challenge that the challenge you

00:21:17.830 --> 00:21:17.840
other challenge that the challenge you
 

00:21:17.840 --> 00:21:21.460
other challenge that the challenge you
know to a if you may like a unified vers

00:21:21.460 --> 00:21:21.470
know to a if you may like a unified vers
 

00:21:21.470 --> 00:21:23.769
know to a if you may like a unified vers
unified understanding of what governance

00:21:23.769 --> 00:21:23.779
unified understanding of what governance
 

00:21:23.779 --> 00:21:26.379
unified understanding of what governance
can mean in this space is also as I was

00:21:26.379 --> 00:21:26.389
can mean in this space is also as I was
 

00:21:26.389 --> 00:21:29.560
can mean in this space is also as I was
referring to earlier there's no there is

00:21:29.560 --> 00:21:29.570
referring to earlier there's no there is
 

00:21:29.570 --> 00:21:31.479
referring to earlier there's no there is
there's a convergent technologies are

00:21:31.479 --> 00:21:31.489
there's a convergent technologies are
 

00:21:31.489 --> 00:21:34.180
there's a convergent technologies are
converging at an increasing rate but are

00:21:34.180 --> 00:21:34.190
converging at an increasing rate but are
 

00:21:34.190 --> 00:21:36.910
converging at an increasing rate but are
our framing of it our understanding of

00:21:36.910 --> 00:21:36.920
our framing of it our understanding of
 

00:21:36.920 --> 00:21:39.340
our framing of it our understanding of
it even what we call it sometimes how we

00:21:39.340 --> 00:21:39.350
it even what we call it sometimes how we
 

00:21:39.350 --> 00:21:43.960
it even what we call it sometimes how we
look at that impact is not converging in

00:21:43.960 --> 00:21:43.970
look at that impact is not converging in
 

00:21:43.970 --> 00:21:45.760
look at that impact is not converging in
in the same way and and that's a

00:21:45.760 --> 00:21:45.770
in the same way and and that's a
 

00:21:45.770 --> 00:21:47.410
in the same way and and that's a
challenge because you find that even

00:21:47.410 --> 00:21:47.420
challenge because you find that even
 

00:21:47.420 --> 00:21:50.560
challenge because you find that even
within a country that whether it's the

00:21:50.560 --> 00:21:50.570
within a country that whether it's the
 

00:21:50.570 --> 00:21:51.820
within a country that whether it's the
Justice Department of the foreign

00:21:51.820 --> 00:21:51.830
Justice Department of the foreign
 

00:21:51.830 --> 00:21:54.520
Justice Department of the foreign
ministry department or or the Defense

00:21:54.520 --> 00:21:54.530
ministry department or or the Defense
 

00:21:54.530 --> 00:21:56.890
ministry department or or the Defense
Department or the people dealing with

00:21:56.890 --> 00:21:56.900
Department or the people dealing with
 

00:21:56.900 --> 00:21:59.140
Department or the people dealing with
telecommunications issues they see these

00:21:59.140 --> 00:21:59.150
telecommunications issues they see these
 

00:21:59.150 --> 00:22:01.150
telecommunications issues they see these
things extremely different from their

00:22:01.150 --> 00:22:01.160
things extremely different from their
 

00:22:01.160 --> 00:22:05.170
things extremely different from their
own silo so governance and you know

00:22:05.170 --> 00:22:05.180
own silo so governance and you know
 

00:22:05.180 --> 00:22:07.270
own silo so governance and you know
proper adaptive forward-looking

00:22:07.270 --> 00:22:07.280
proper adaptive forward-looking
 

00:22:07.280 --> 00:22:10.570
proper adaptive forward-looking
governance need to have greater elements

00:22:10.570 --> 00:22:10.580
governance need to have greater elements
 

00:22:10.580 --> 00:22:13.930
governance need to have greater elements
of D siloing and and converging of the

00:22:13.930 --> 00:22:13.940
of D siloing and and converging of the
 

00:22:13.940 --> 00:22:16.780
of D siloing and and converging of the
various areas and and that will take

00:22:16.780 --> 00:22:16.790
various areas and and that will take
 

00:22:16.790 --> 00:22:19.810
various areas and and that will take
some work but I'm very positive that we

00:22:19.810 --> 00:22:19.820
some work but I'm very positive that we
 

00:22:19.820 --> 00:22:22.840
some work but I'm very positive that we
will get there in the end yes that

00:22:22.840 --> 00:22:22.850
will get there in the end yes that
 

00:22:22.850 --> 00:22:24.430
will get there in the end yes that
certainly raises a number of questions

00:22:24.430 --> 00:22:24.440
certainly raises a number of questions
 

00:22:24.440 --> 00:22:26.890
certainly raises a number of questions
as you said Amandeep there's I think you

00:22:26.890 --> 00:22:26.900
as you said Amandeep there's I think you
 

00:22:26.900 --> 00:22:30.070
as you said Amandeep there's I think you
said 125 groups involved in the the

00:22:30.070 --> 00:22:30.080
said 125 groups involved in the the
 

00:22:30.080 --> 00:22:32.410
said 125 groups involved in the the
meeting that you're chairing it's hard

00:22:32.410 --> 00:22:32.420
meeting that you're chairing it's hard
 

00:22:32.420 --> 00:22:33.550
meeting that you're chairing it's hard
to imagine it that's at the

00:22:33.550 --> 00:22:33.560
to imagine it that's at the
 

00:22:33.560 --> 00:22:34.780
to imagine it that's at the
international level now start

00:22:34.780 --> 00:22:34.790
international level now start
 

00:22:34.790 --> 00:22:37.150
international level now start
multiplying with all of the the local

00:22:37.150 --> 00:22:37.160
multiplying with all of the the local
 

00:22:37.160 --> 00:22:39.850
multiplying with all of the the local
governance one can quickly see where

00:22:39.850 --> 00:22:39.860
governance one can quickly see where
 

00:22:39.860 --> 00:22:41.320
governance one can quickly see where
this is going to be a very hard problem

00:22:41.320 --> 00:22:41.330
this is going to be a very hard problem
 

00:22:41.330 --> 00:22:45.550
this is going to be a very hard problem
over many years I like to follow up on

00:22:45.550 --> 00:22:45.560
over many years I like to follow up on
 

00:22:45.560 --> 00:22:47.940
over many years I like to follow up on
something that had cut that I think was

00:22:47.940 --> 00:22:47.950
something that had cut that I think was
 

00:22:47.950 --> 00:22:50.260
something that had cut that I think was
latent in some of the things that both

00:22:50.260 --> 00:22:50.270
latent in some of the things that both
 

00:22:50.270 --> 00:22:53.080
latent in some of the things that both
of you were saying which is whether in

00:22:53.080 --> 00:22:53.090
of you were saying which is whether in
 

00:22:53.090 --> 00:22:54.630
of you were saying which is whether in
the weapons space or in other spaces

00:22:54.630 --> 00:22:54.640
the weapons space or in other spaces
 

00:22:54.640 --> 00:22:57.490
the weapons space or in other spaces
policy often requires some kind of

00:22:57.490 --> 00:22:57.500
policy often requires some kind of
 

00:22:57.500 --> 00:23:00.010
policy often requires some kind of
verification some or you know some way

00:23:00.010 --> 00:23:00.020
verification some or you know some way
 

00:23:00.020 --> 00:23:02.620
verification some or you know some way
of establishing a trust that that the

00:23:02.620 --> 00:23:02.630
of establishing a trust that that the
 

00:23:02.630 --> 00:23:05.380
of establishing a trust that that the
group that is being governed is is

00:23:05.380 --> 00:23:05.390
group that is being governed is is
 

00:23:05.390 --> 00:23:07.450
group that is being governed is is
following the rules as it were to put it

00:23:07.450 --> 00:23:07.460
following the rules as it were to put it
 

00:23:07.460 --> 00:23:10.630
following the rules as it were to put it
in very blunt terms how is that changed

00:23:10.630 --> 00:23:10.640
in very blunt terms how is that changed
 

00:23:10.640 --> 00:23:13.150
in very blunt terms how is that changed
for AI technologies whether again in the

00:23:13.150 --> 00:23:13.160
for AI technologies whether again in the
 

00:23:13.160 --> 00:23:15.460
for AI technologies whether again in the
weapons space or otherwise when they are

00:23:15.460 --> 00:23:15.470
weapons space or otherwise when they are
 

00:23:15.470 --> 00:23:17.020
weapons space or otherwise when they are
computational they aren't necessarily a

00:23:17.020 --> 00:23:17.030
computational they aren't necessarily a
 

00:23:17.030 --> 00:23:19.090
computational they aren't necessarily a
physical thing that you can pick up that

00:23:19.090 --> 00:23:19.100
physical thing that you can pick up that
 

00:23:19.100 --> 00:23:21.490
physical thing that you can pick up that
that leads it seems it seems it would

00:23:21.490 --> 00:23:21.500
that leads it seems it seems it would
 

00:23:21.500 --> 00:23:22.960
that leads it seems it seems it would
lead to a number of challenges in this

00:23:22.960 --> 00:23:22.970
lead to a number of challenges in this
 

00:23:22.970 --> 00:23:24.490
lead to a number of challenges in this
area of sort of verification and

00:23:24.490 --> 00:23:24.500
area of sort of verification and
 

00:23:24.500 --> 00:23:28.240
area of sort of verification and
monitoring this is an excellent question

00:23:28.240 --> 00:23:28.250
monitoring this is an excellent question
 

00:23:28.250 --> 00:23:34.360
monitoring this is an excellent question
I think the area raises some unique

00:23:34.360 --> 00:23:34.370
I think the area raises some unique
 

00:23:34.370 --> 00:23:36.760
I think the area raises some unique
questions about verification monitoring

00:23:36.760 --> 00:23:36.770
questions about verification monitoring
 

00:23:36.770 --> 00:23:39.640
questions about verification monitoring
even exchange of national good practices

00:23:39.640 --> 00:23:39.650
even exchange of national good practices
 

00:23:39.650 --> 00:23:42.730
even exchange of national good practices
no one is going to be allowing

00:23:42.730 --> 00:23:42.740
no one is going to be allowing
 

00:23:42.740 --> 00:23:45.790
no one is going to be allowing
international inspectors to pour over

00:23:45.790 --> 00:23:45.800
international inspectors to pour over
 

00:23:45.800 --> 00:23:46.990
international inspectors to pour over
their shoulders and low

00:23:46.990 --> 00:23:47.000
their shoulders and low
 

00:23:47.000 --> 00:23:49.450
their shoulders and low
get what they are coding no one is at

00:23:49.450 --> 00:23:49.460
get what they are coding no one is at
 

00:23:49.460 --> 00:23:51.490
get what they are coding no one is at
this point looking at turning over pages

00:23:51.490 --> 00:23:51.500
this point looking at turning over pages
 

00:23:51.500 --> 00:23:54.280
this point looking at turning over pages
of code because you know most of this is

00:23:54.280 --> 00:23:54.290
of code because you know most of this is
 

00:23:54.290 --> 00:23:56.800
of code because you know most of this is
happening in the commercial space so

00:23:56.800 --> 00:23:56.810
happening in the commercial space so
 

00:23:56.810 --> 00:23:58.390
happening in the commercial space so
there are proprietary intellectual

00:23:58.390 --> 00:23:58.400
there are proprietary intellectual
 

00:23:58.400 --> 00:23:59.920
there are proprietary intellectual
property related issues there are

00:23:59.920 --> 00:23:59.930
property related issues there are
 

00:23:59.930 --> 00:24:01.810
property related issues there are
commercial issues so we're not even

00:24:01.810 --> 00:24:01.820
commercial issues so we're not even
 

00:24:01.820 --> 00:24:05.320
commercial issues so we're not even
talking about that just in terms of what

00:24:05.320 --> 00:24:05.330
talking about that just in terms of what
 

00:24:05.330 --> 00:24:08.530
talking about that just in terms of what
is being developed and fielded what kind

00:24:08.530 --> 00:24:08.540
is being developed and fielded what kind
 

00:24:08.540 --> 00:24:12.310
is being developed and fielded what kind
of characteristics it has whether it

00:24:12.310 --> 00:24:12.320
of characteristics it has whether it
 

00:24:12.320 --> 00:24:14.680
of characteristics it has whether it
interfaces properly with existing

00:24:14.680 --> 00:24:14.690
interfaces properly with existing
 

00:24:14.690 --> 00:24:16.510
interfaces properly with existing
obligations on international

00:24:16.510 --> 00:24:16.520
obligations on international
 

00:24:16.520 --> 00:24:18.970
obligations on international
humanitarian law that itself is going to

00:24:18.970 --> 00:24:18.980
humanitarian law that itself is going to
 

00:24:18.980 --> 00:24:21.160
humanitarian law that itself is going to
be a huge challenge so let's leave aside

00:24:21.160 --> 00:24:21.170
be a huge challenge so let's leave aside
 

00:24:21.170 --> 00:24:24.190
be a huge challenge so let's leave aside
the harder problem of verifying actual

00:24:24.190 --> 00:24:24.200
the harder problem of verifying actual
 

00:24:24.200 --> 00:24:26.380
the harder problem of verifying actual
systems in terms of the coding or in

00:24:26.380 --> 00:24:26.390
systems in terms of the coding or in
 

00:24:26.390 --> 00:24:28.630
systems in terms of the coding or in
terms of their potential to learn on the

00:24:28.630 --> 00:24:28.640
terms of their potential to learn on the
 

00:24:28.640 --> 00:24:32.910
terms of their potential to learn on the
fly to be true machine learning or other

00:24:32.910 --> 00:24:32.920
fly to be true machine learning or other
 

00:24:32.920 --> 00:24:35.380
fly to be true machine learning or other
techniques so let's leave that aside the

00:24:35.380 --> 00:24:35.390
techniques so let's leave that aside the
 

00:24:35.390 --> 00:24:37.120
techniques so let's leave that aside the
simpler problem relatively simpler

00:24:37.120 --> 00:24:37.130
simpler problem relatively simpler
 

00:24:37.130 --> 00:24:38.920
simpler problem relatively simpler
problem of exchange of national students

00:24:38.920 --> 00:24:38.930
problem of exchange of national students
 

00:24:38.930 --> 00:24:40.870
problem of exchange of national students
that's going to be hard what I have

00:24:40.870 --> 00:24:40.880
that's going to be hard what I have
 

00:24:40.880 --> 00:24:44.260
that's going to be hard what I have
tried since last year is to create a

00:24:44.260 --> 00:24:44.270
tried since last year is to create a
 

00:24:44.270 --> 00:24:47.410
tried since last year is to create a
safe space for delegations to come and

00:24:47.410 --> 00:24:47.420
safe space for delegations to come and
 

00:24:47.420 --> 00:24:50.470
safe space for delegations to come and
talk about these systems and using a

00:24:50.470 --> 00:24:50.480
talk about these systems and using a
 

00:24:50.480 --> 00:24:52.780
talk about these systems and using a
variety of ways looking at the

00:24:52.780 --> 00:24:52.790
variety of ways looking at the
 

00:24:52.790 --> 00:24:54.520
variety of ways looking at the
technology dimension looking at the

00:24:54.520 --> 00:24:54.530
technology dimension looking at the
 

00:24:54.530 --> 00:24:56.440
technology dimension looking at the
legal ethical dimension the military

00:24:56.440 --> 00:24:56.450
legal ethical dimension the military
 

00:24:56.450 --> 00:24:58.920
legal ethical dimension the military
affects dimension and some cross-cutting

00:24:58.920 --> 00:24:58.930
affects dimension and some cross-cutting
 

00:24:58.930 --> 00:25:02.070
affects dimension and some cross-cutting
dimensions where this issue of being

00:25:02.070 --> 00:25:02.080
dimensions where this issue of being
 

00:25:02.080 --> 00:25:04.870
dimensions where this issue of being
intelligible to other communities also

00:25:04.870 --> 00:25:04.880
intelligible to other communities also
 

00:25:04.880 --> 00:25:06.610
intelligible to other communities also
comes up what kind of language we are

00:25:06.610 --> 00:25:06.620
comes up what kind of language we are
 

00:25:06.620 --> 00:25:09.010
comes up what kind of language we are
using how we are communicating amongst

00:25:09.010 --> 00:25:09.020
using how we are communicating amongst
 

00:25:09.020 --> 00:25:13.470
using how we are communicating amongst
diplomats military personnel lawyers

00:25:13.470 --> 00:25:13.480
diplomats military personnel lawyers
 

00:25:13.480 --> 00:25:17.290
diplomats military personnel lawyers
security experts ethicists and what

00:25:17.290 --> 00:25:17.300
security experts ethicists and what
 

00:25:17.300 --> 00:25:21.520
security experts ethicists and what
assists so to these different lenses

00:25:21.520 --> 00:25:21.530
assists so to these different lenses
 

00:25:21.530 --> 00:25:25.330
assists so to these different lenses
we've tried to get more clarity on these

00:25:25.330 --> 00:25:25.340
we've tried to get more clarity on these
 

00:25:25.340 --> 00:25:27.460
we've tried to get more clarity on these
issues encourage countries to talk about

00:25:27.460 --> 00:25:27.470
issues encourage countries to talk about
 

00:25:27.470 --> 00:25:30.310
issues encourage countries to talk about
them their concerns their challenges and

00:25:30.310 --> 00:25:30.320
them their concerns their challenges and
 

00:25:30.320 --> 00:25:33.820
them their concerns their challenges and
we hope eventually to have two things

00:25:33.820 --> 00:25:33.830
we hope eventually to have two things
 

00:25:33.830 --> 00:25:38.140
we hope eventually to have two things
one a system whereby the technology and

00:25:38.140 --> 00:25:38.150
one a system whereby the technology and
 

00:25:38.150 --> 00:25:40.060
one a system whereby the technology and
its development in space is kept under

00:25:40.060 --> 00:25:40.070
its development in space is kept under
 

00:25:40.070 --> 00:25:42.280
its development in space is kept under
some kind of review because whatever

00:25:42.280 --> 00:25:42.290
some kind of review because whatever
 

00:25:42.290 --> 00:25:44.290
some kind of review because whatever
governance system you come up with that

00:25:44.290 --> 00:25:44.300
governance system you come up with that
 

00:25:44.300 --> 00:25:45.850
governance system you come up with that
could get quickly outdated

00:25:45.850 --> 00:25:45.860
could get quickly outdated
 

00:25:45.860 --> 00:25:47.830
could get quickly outdated
so you need to be keep watching what's

00:25:47.830 --> 00:25:47.840
so you need to be keep watching what's
 

00:25:47.840 --> 00:25:50.500
so you need to be keep watching what's
happening in the real world and to

00:25:50.500 --> 00:25:50.510
happening in the real world and to
 

00:25:50.510 --> 00:25:53.830
happening in the real world and to
alongside this latter house tour you

00:25:53.830 --> 00:25:53.840
alongside this latter house tour you
 

00:25:53.840 --> 00:25:56.890
alongside this latter house tour you
need to have a platform for nations to

00:25:56.890 --> 00:25:56.900
need to have a platform for nations to
 

00:25:56.900 --> 00:26:00.950
need to have a platform for nations to
bring their experience of implementing a

00:26:00.950 --> 00:26:00.960
bring their experience of implementing a
 

00:26:00.960 --> 00:26:05.029
bring their experience of implementing a
use-cases regulating AI of tackling some

00:26:05.029 --> 00:26:05.039
use-cases regulating AI of tackling some
 

00:26:05.039 --> 00:26:06.799
use-cases regulating AI of tackling some
of the concerns whether these are social

00:26:06.799 --> 00:26:06.809
of the concerns whether these are social
 

00:26:06.809 --> 00:26:10.279
of the concerns whether these are social
concerns privacy related or otherwise or

00:26:10.279 --> 00:26:10.289
concerns privacy related or otherwise or
 

00:26:10.289 --> 00:26:14.149
concerns privacy related or otherwise or
these are control issues encouraging

00:26:14.149 --> 00:26:14.159
these are control issues encouraging
 

00:26:14.159 --> 00:26:15.590
these are control issues encouraging
them to bring those to the table in

00:26:15.590 --> 00:26:15.600
them to bring those to the table in
 

00:26:15.600 --> 00:26:17.899
them to bring those to the table in
exchange with others so those are

00:26:17.899 --> 00:26:17.909
exchange with others so those are
 

00:26:17.909 --> 00:26:20.990
exchange with others so those are
preliminary steps that we are looking at

00:26:20.990 --> 00:26:21.000
preliminary steps that we are looking at
 

00:26:21.000 --> 00:26:25.399
preliminary steps that we are looking at
but initially as AI develops and as it

00:26:25.399 --> 00:26:25.409
but initially as AI develops and as it
 

00:26:25.409 --> 00:26:29.570
but initially as AI develops and as it
gets applied in the security space which

00:26:29.570 --> 00:26:29.580
gets applied in the security space which
 

00:26:29.580 --> 00:26:34.399
gets applied in the security space which
seems likely to do my math then we'll

00:26:34.399 --> 00:26:34.409
seems likely to do my math then we'll
 

00:26:34.409 --> 00:26:37.100
seems likely to do my math then we'll
have to move up the ladder to the next

00:26:37.100 --> 00:26:37.110
have to move up the ladder to the next
 

00:26:37.110 --> 00:26:37.669
have to move up the ladder to the next
step

00:26:37.669 --> 00:26:37.679
step
 

00:26:37.679 --> 00:26:40.909
step
where we should be in a position to ask

00:26:40.909 --> 00:26:40.919
where we should be in a position to ask
 

00:26:40.919 --> 00:26:44.119
where we should be in a position to ask
some questions and get some answers but

00:26:44.119 --> 00:26:44.129
some questions and get some answers but
 

00:26:44.129 --> 00:26:48.499
some questions and get some answers but
we are not there now I think you know

00:26:48.499 --> 00:26:48.509
we are not there now I think you know
 

00:26:48.509 --> 00:26:51.230
we are not there now I think you know
with verification means different things

00:26:51.230 --> 00:26:51.240
with verification means different things
 

00:26:51.240 --> 00:26:53.539
with verification means different things
depending on the context and you ask in

00:26:53.539 --> 00:26:53.549
depending on the context and you ask in
 

00:26:53.549 --> 00:26:55.789
depending on the context and you ask in
terms of weapon systems and I think it's

00:26:55.789 --> 00:26:55.799
terms of weapon systems and I think it's
 

00:26:55.799 --> 00:26:57.110
terms of weapon systems and I think it's
important that what we are dealing with

00:26:57.110 --> 00:26:57.120
important that what we are dealing with
 

00:26:57.120 --> 00:26:59.990
important that what we are dealing with
here in this various for us in in UN

00:26:59.990 --> 00:27:00.000
here in this various for us in in UN
 

00:27:00.000 --> 00:27:02.830
here in this various for us in in UN
Geneva in particular are fundamentally

00:27:02.830 --> 00:27:02.840
Geneva in particular are fundamentally
 

00:27:02.840 --> 00:27:06.169
Geneva in particular are fundamentally
aligning or trying to reach a common

00:27:06.169 --> 00:27:06.179
aligning or trying to reach a common
 

00:27:06.179 --> 00:27:07.580
aligning or trying to reach a common
understanding of each other's national

00:27:07.580 --> 00:27:07.590
understanding of each other's national
 

00:27:07.590 --> 00:27:10.639
understanding of each other's national
security positions and technological and

00:27:10.639 --> 00:27:10.649
security positions and technological and
 

00:27:10.649 --> 00:27:12.379
security positions and technological and
scientific advances always been a very

00:27:12.379 --> 00:27:12.389
scientific advances always been a very
 

00:27:12.389 --> 00:27:15.350
scientific advances always been a very
key component of any national security

00:27:15.350 --> 00:27:15.360
key component of any national security
 

00:27:15.360 --> 00:27:18.830
key component of any national security
strategy and thinking and and and policy

00:27:18.830 --> 00:27:18.840
strategy and thinking and and and policy
 

00:27:18.840 --> 00:27:22.310
strategy and thinking and and and policy
and in bringing these various agendas

00:27:22.310 --> 00:27:22.320
and in bringing these various agendas
 

00:27:22.320 --> 00:27:24.409
and in bringing these various agendas
together I mean the two key component

00:27:24.409 --> 00:27:24.419
together I mean the two key component
 

00:27:24.419 --> 00:27:26.899
together I mean the two key component
for any effective verification regime is

00:27:26.899 --> 00:27:26.909
for any effective verification regime is
 

00:27:26.909 --> 00:27:30.230
for any effective verification regime is
transparency and and Trust and so how do

00:27:30.230 --> 00:27:30.240
transparency and and Trust and so how do
 

00:27:30.240 --> 00:27:31.850
transparency and and Trust and so how do
you build that level of transparency

00:27:31.850 --> 00:27:31.860
you build that level of transparency
 

00:27:31.860 --> 00:27:35.330
you build that level of transparency
without you know giving up certain I

00:27:35.330 --> 00:27:35.340
without you know giving up certain I
 

00:27:35.340 --> 00:27:37.159
without you know giving up certain I
mean doesn't have to even to be the

00:27:37.159 --> 00:27:37.169
mean doesn't have to even to be the
 

00:27:37.169 --> 00:27:39.019
mean doesn't have to even to be the
military or defense side of it but also

00:27:39.019 --> 00:27:39.029
military or defense side of it but also
 

00:27:39.029 --> 00:27:41.570
military or defense side of it but also
as a semantics was referring to you more

00:27:41.570 --> 00:27:41.580
as a semantics was referring to you more
 

00:27:41.580 --> 00:27:43.490
as a semantics was referring to you more
the industrial economic competitiveness

00:27:43.490 --> 00:27:43.500
the industrial economic competitiveness
 

00:27:43.500 --> 00:27:47.180
the industrial economic competitiveness
angle to it as these are truly multi

00:27:47.180 --> 00:27:47.190
angle to it as these are truly multi
 

00:27:47.190 --> 00:27:48.769
angle to it as these are truly multi
purposed technologies that would have

00:27:48.769 --> 00:27:48.779
purposed technologies that would have
 

00:27:48.779 --> 00:27:51.860
purposed technologies that would have
impact I go way beyond just II that sort

00:27:51.860 --> 00:27:51.870
impact I go way beyond just II that sort
 

00:27:51.870 --> 00:27:54.320
impact I go way beyond just II that sort
of the mers national security space so

00:27:54.320 --> 00:27:54.330
of the mers national security space so
 

00:27:54.330 --> 00:27:56.029
of the mers national security space so
bringing you know building that

00:27:56.029 --> 00:27:56.039
bringing you know building that
 

00:27:56.039 --> 00:27:59.029
bringing you know building that
transparency and that trust this is in

00:27:59.029 --> 00:27:59.039
transparency and that trust this is in
 

00:27:59.039 --> 00:28:01.190
transparency and that trust this is in
the current geopolitical climate is of

00:28:01.190 --> 00:28:01.200
the current geopolitical climate is of
 

00:28:01.200 --> 00:28:02.749
the current geopolitical climate is of
course quite challenging and it

00:28:02.749 --> 00:28:02.759
course quite challenging and it
 

00:28:02.759 --> 00:28:06.529
course quite challenging and it
challenges also any future verification

00:28:06.529 --> 00:28:06.539
challenges also any future verification
 

00:28:06.539 --> 00:28:09.230
challenges also any future verification
regime or even even to talk about

00:28:09.230 --> 00:28:09.240
regime or even even to talk about
 

00:28:09.240 --> 00:28:11.060
regime or even even to talk about
verification now in sort of the milder

00:28:11.060 --> 00:28:11.070
verification now in sort of the milder
 

00:28:11.070 --> 00:28:12.390
verification now in sort of the milder
form which in

00:28:12.390 --> 00:28:12.400
form which in
 

00:28:12.400 --> 00:28:14.430
form which in
world will be more referred to as

00:28:14.430 --> 00:28:14.440
world will be more referred to as
 

00:28:14.440 --> 00:28:17.730
world will be more referred to as
validation of that the intention and

00:28:17.730 --> 00:28:17.740
validation of that the intention and
 

00:28:17.740 --> 00:28:20.130
validation of that the intention and
design of the technology actually means

00:28:20.130 --> 00:28:20.140
design of the technology actually means
 

00:28:20.140 --> 00:28:23.900
design of the technology actually means
this it's it's targeted objectives and

00:28:23.900 --> 00:28:23.910
this it's it's targeted objectives and
 

00:28:23.910 --> 00:28:26.280
this it's it's targeted objectives and
just one more thing is that one thing

00:28:26.280 --> 00:28:26.290
just one more thing is that one thing
 

00:28:26.290 --> 00:28:27.780
just one more thing is that one thing
that we are facing and you were here the

00:28:27.780 --> 00:28:27.790
that we are facing and you were here the
 

00:28:27.790 --> 00:28:29.310
that we are facing and you were here the
secretary-general of the UN speak about

00:28:29.310 --> 00:28:29.320
secretary-general of the UN speak about
 

00:28:29.320 --> 00:28:32.280
secretary-general of the UN speak about
this is lawed is that these developments

00:28:32.280 --> 00:28:32.290
this is lawed is that these developments
 

00:28:32.290 --> 00:28:35.070
this is lawed is that these developments
because of the scale and the speed and

00:28:35.070 --> 00:28:35.080
because of the scale and the speed and
 

00:28:35.080 --> 00:28:37.290
because of the scale and the speed and
like I said the the adoption rate itself

00:28:37.290 --> 00:28:37.300
like I said the the adoption rate itself
 

00:28:37.300 --> 00:28:40.230
like I said the the adoption rate itself
is challenging the systems as they are

00:28:40.230 --> 00:28:40.240
is challenging the systems as they are
 

00:28:40.240 --> 00:28:42.050
is challenging the systems as they are
certainly challenging the entire

00:28:42.050 --> 00:28:42.060
certainly challenging the entire
 

00:28:42.060 --> 00:28:43.710
certainly challenging the entire
disarmament arms control and

00:28:43.710 --> 00:28:43.720
disarmament arms control and
 

00:28:43.720 --> 00:28:47.550
disarmament arms control and
non-proliferation architecture and you

00:28:47.550 --> 00:28:47.560
non-proliferation architecture and you
 

00:28:47.560 --> 00:28:49.740
non-proliferation architecture and you
know my kind of my managers and then the

00:28:49.740 --> 00:28:49.750
know my kind of my managers and then the
 

00:28:49.750 --> 00:28:50.630
know my kind of my managers and then the
sector generally in particular

00:28:50.630 --> 00:28:50.640
sector generally in particular
 

00:28:50.640 --> 00:28:53.090
sector generally in particular
constantly refers to in saying we need

00:28:53.090 --> 00:28:53.100
constantly refers to in saying we need
 

00:28:53.100 --> 00:28:55.470
constantly refers to in saying we need
normative entrepreneurship we need to

00:28:55.470 --> 00:28:55.480
normative entrepreneurship we need to
 

00:28:55.480 --> 00:28:57.320
normative entrepreneurship we need to
stay true to our norms our values

00:28:57.320 --> 00:28:57.330
stay true to our norms our values
 

00:28:57.330 --> 00:28:59.730
stay true to our norms our values
especially from a UN context you know

00:28:59.730 --> 00:28:59.740
especially from a UN context you know
 

00:28:59.740 --> 00:29:01.890
especially from a UN context you know
the Charter that we were built on but we

00:29:01.890 --> 00:29:01.900
the Charter that we were built on but we
 

00:29:01.900 --> 00:29:03.750
the Charter that we were built on but we
also need to be normative and thinking

00:29:03.750 --> 00:29:03.760
also need to be normative and thinking
 

00:29:03.760 --> 00:29:05.670
also need to be normative and thinking
you know be entrepreneurial and really

00:29:05.670 --> 00:29:05.680
you know be entrepreneurial and really
 

00:29:05.680 --> 00:29:08.940
you know be entrepreneurial and really
go outside our comfort zone in terms of

00:29:08.940 --> 00:29:08.950
go outside our comfort zone in terms of
 

00:29:08.950 --> 00:29:10.500
go outside our comfort zone in terms of
going back to my earlier comment you

00:29:10.500 --> 00:29:10.510
going back to my earlier comment you
 

00:29:10.510 --> 00:29:12.180
going back to my earlier comment you
know who do we bring in who do we talk

00:29:12.180 --> 00:29:12.190
know who do we bring in who do we talk
 

00:29:12.190 --> 00:29:13.920
know who do we bring in who do we talk
to how do we improve our understanding

00:29:13.920 --> 00:29:13.930
to how do we improve our understanding
 

00:29:13.930 --> 00:29:17.130
to how do we improve our understanding
of each other's particular circumstances

00:29:17.130 --> 00:29:17.140
of each other's particular circumstances
 

00:29:17.140 --> 00:29:19.350
of each other's particular circumstances
I worked a lot with the tech community

00:29:19.350 --> 00:29:19.360
I worked a lot with the tech community
 

00:29:19.360 --> 00:29:21.390
I worked a lot with the tech community
and engineering community on thinking

00:29:21.390 --> 00:29:21.400
and engineering community on thinking
 

00:29:21.400 --> 00:29:22.620
and engineering community on thinking
through this issues and one thing I

00:29:22.620 --> 00:29:22.630
through this issues and one thing I
 

00:29:22.630 --> 00:29:26.760
through this issues and one thing I
constantly see is you know diplomats and

00:29:26.760 --> 00:29:26.770
constantly see is you know diplomats and
 

00:29:26.770 --> 00:29:28.560
constantly see is you know diplomats and
I'm a former diplomat and diplomats or

00:29:28.560 --> 00:29:28.570
I'm a former diplomat and diplomats or
 

00:29:28.570 --> 00:29:31.140
I'm a former diplomat and diplomats or
you know civil service bureaucrats are

00:29:31.140 --> 00:29:31.150
you know civil service bureaucrats are
 

00:29:31.150 --> 00:29:32.940
you know civil service bureaucrats are
not always very good as thinking about

00:29:32.940 --> 00:29:32.950
not always very good as thinking about
 

00:29:32.950 --> 00:29:35.940
not always very good as thinking about
the the design the research development

00:29:35.940 --> 00:29:35.950
the the design the research development
 

00:29:35.950 --> 00:29:38.520
the the design the research development
the sine side of things but I would also

00:29:38.520 --> 00:29:38.530
the sine side of things but I would also
 

00:29:38.530 --> 00:29:40.260
the sine side of things but I would also
say that the people in the developing

00:29:40.260 --> 00:29:40.270
say that the people in the developing
 

00:29:40.270 --> 00:29:41.970
say that the people in the developing
research development the design phase

00:29:41.970 --> 00:29:41.980
research development the design phase
 

00:29:41.980 --> 00:29:45.740
research development the design phase
are not always attuned enough to the

00:29:45.740 --> 00:29:45.750
are not always attuned enough to the
 

00:29:45.750 --> 00:29:49.470
are not always attuned enough to the
implications of their design and so

00:29:49.470 --> 00:29:49.480
implications of their design and so
 

00:29:49.480 --> 00:29:51.600
implications of their design and so
improving that interaction so that we

00:29:51.600 --> 00:29:51.610
improving that interaction so that we
 

00:29:51.610 --> 00:29:53.550
improving that interaction so that we
can actually truly start assigning

00:29:53.550 --> 00:29:53.560
can actually truly start assigning
 

00:29:53.560 --> 00:29:55.500
can actually truly start assigning
technology in such a way that those

00:29:55.500 --> 00:29:55.510
technology in such a way that those
 

00:29:55.510 --> 00:29:58.230
technology in such a way that those
risks are managed upfront will require

00:29:58.230 --> 00:29:58.240
risks are managed upfront will require
 

00:29:58.240 --> 00:30:00.690
risks are managed upfront will require
both some normative entrepreneurship but

00:30:00.690 --> 00:30:00.700
both some normative entrepreneurship but
 

00:30:00.700 --> 00:30:02.490
both some normative entrepreneurship but
also entrepreneurship in general and I

00:30:02.490 --> 00:30:02.500
also entrepreneurship in general and I
 

00:30:02.500 --> 00:30:04.290
also entrepreneurship in general and I
would call it social or social

00:30:04.290 --> 00:30:04.300
would call it social or social
 

00:30:04.300 --> 00:30:06.660
would call it social or social
entrepreneurship I hope that answered

00:30:06.660 --> 00:30:06.670
entrepreneurship I hope that answered
 

00:30:06.670 --> 00:30:08.960
entrepreneurship I hope that answered
your I partially answered your question

00:30:08.960 --> 00:30:08.970
your I partially answered your question
 

00:30:08.970 --> 00:30:12.600
your I partially answered your question
well as you both made very clear these

00:30:12.600 --> 00:30:12.610
well as you both made very clear these
 

00:30:12.610 --> 00:30:14.550
well as you both made very clear these
are incredibly complex issues I think

00:30:14.550 --> 00:30:14.560
are incredibly complex issues I think
 

00:30:14.560 --> 00:30:15.870
are incredibly complex issues I think
you know this the talk of

00:30:15.870 --> 00:30:15.880
you know this the talk of
 

00:30:15.880 --> 00:30:17.520
you know this the talk of
entrepreneurship the talk of companies

00:30:17.520 --> 00:30:17.530
entrepreneurship the talk of companies
 

00:30:17.530 --> 00:30:19.170
entrepreneurship the talk of companies
does of course raise an issue which

00:30:19.170 --> 00:30:19.180
does of course raise an issue which
 

00:30:19.180 --> 00:30:20.940
does of course raise an issue which
which Amandeep you raised obliquely

00:30:20.940 --> 00:30:20.950
which Amandeep you raised obliquely
 

00:30:20.950 --> 00:30:24.090
which Amandeep you raised obliquely
which is so much of what's happening now

00:30:24.090 --> 00:30:24.100
which is so much of what's happening now
 

00:30:24.100 --> 00:30:25.539
which is so much of what's happening now
in the development in the

00:30:25.539 --> 00:30:25.549
in the development in the
 

00:30:25.549 --> 00:30:28.180
in the development in the
and AI space is happening in the

00:30:28.180 --> 00:30:28.190
and AI space is happening in the
 

00:30:28.190 --> 00:30:29.830
and AI space is happening in the
corporate world and you have these

00:30:29.830 --> 00:30:29.840
corporate world and you have these
 

00:30:29.840 --> 00:30:32.950
corporate world and you have these
companies that don't are no longer

00:30:32.950 --> 00:30:32.960
companies that don't are no longer
 

00:30:32.960 --> 00:30:34.960
companies that don't are no longer
restricted to national boundaries but of

00:30:34.960 --> 00:30:34.970
restricted to national boundaries but of
 

00:30:34.970 --> 00:30:37.539
restricted to national boundaries but of
course extend far beyond that so how do

00:30:37.539 --> 00:30:37.549
course extend far beyond that so how do
 

00:30:37.549 --> 00:30:39.879
course extend far beyond that so how do
we think about the engagement between

00:30:39.879 --> 00:30:39.889
we think about the engagement between
 

00:30:39.889 --> 00:30:42.850
we think about the engagement between
the diplomatic world and the commercial

00:30:42.850 --> 00:30:42.860
the diplomatic world and the commercial
 

00:30:42.860 --> 00:30:44.499
the diplomatic world and the commercial
world when you have companies whose

00:30:44.499 --> 00:30:44.509
world when you have companies whose
 

00:30:44.509 --> 00:30:47.590
world when you have companies whose
economic impact and economic power

00:30:47.590 --> 00:30:47.600
economic impact and economic power
 

00:30:47.600 --> 00:30:50.350
economic impact and economic power
exceeds that of many small nations how

00:30:50.350 --> 00:30:50.360
exceeds that of many small nations how
 

00:30:50.360 --> 00:30:52.389
exceeds that of many small nations how
can we work together between both the

00:30:52.389 --> 00:30:52.399
can we work together between both the
 

00:30:52.399 --> 00:30:54.430
can we work together between both the
political side and the economic side in

00:30:54.430 --> 00:30:54.440
political side and the economic side in
 

00:30:54.440 --> 00:31:00.060
political side and the economic side in
this AI and technology space it's true

00:31:00.060 --> 00:31:00.070
this AI and technology space it's true
 

00:31:00.070 --> 00:31:03.729
this AI and technology space it's true
and well many companies in tech space

00:31:03.729 --> 00:31:03.739
and well many companies in tech space
 

00:31:03.739 --> 00:31:07.239
and well many companies in tech space
exceeds that of many countries but at

00:31:07.239 --> 00:31:07.249
exceeds that of many countries but at
 

00:31:07.249 --> 00:31:09.999
exceeds that of many countries but at
the same time these tech companies are

00:31:09.999 --> 00:31:10.009
the same time these tech companies are
 

00:31:10.009 --> 00:31:13.119
the same time these tech companies are
very vulnerable to shifts in the public

00:31:13.119 --> 00:31:13.129
very vulnerable to shifts in the public
 

00:31:13.129 --> 00:31:16.840
very vulnerable to shifts in the public
mode to shifts in levels of trust in

00:31:16.840 --> 00:31:16.850
mode to shifts in levels of trust in
 

00:31:16.850 --> 00:31:18.879
mode to shifts in levels of trust in
them so I think it's in everyone's

00:31:18.879 --> 00:31:18.889
them so I think it's in everyone's
 

00:31:18.889 --> 00:31:23.590
them so I think it's in everyone's
interest to think about these problems

00:31:23.590 --> 00:31:23.600
interest to think about these problems
 

00:31:23.600 --> 00:31:28.109
interest to think about these problems
what AI and other autonomous

00:31:28.109 --> 00:31:28.119
what AI and other autonomous
 

00:31:28.119 --> 00:31:32.080
what AI and other autonomous
technologies mean for our future and

00:31:32.080 --> 00:31:32.090
technologies mean for our future and
 

00:31:32.090 --> 00:31:35.019
technologies mean for our future and
then work together to ensure that the

00:31:35.019 --> 00:31:35.029
then work together to ensure that the
 

00:31:35.029 --> 00:31:37.599
then work together to ensure that the
public continues to have trust in the

00:31:37.599 --> 00:31:37.609
public continues to have trust in the
 

00:31:37.609 --> 00:31:40.060
public continues to have trust in the
deployment of these technologies now

00:31:40.060 --> 00:31:40.070
deployment of these technologies now
 

00:31:40.070 --> 00:31:42.460
deployment of these technologies now
apart from this role I've been involved

00:31:42.460 --> 00:31:42.470
apart from this role I've been involved
 

00:31:42.470 --> 00:31:44.409
apart from this role I've been involved
with the national level Task Force on

00:31:44.409 --> 00:31:44.419
with the national level Task Force on
 

00:31:44.419 --> 00:31:47.200
with the national level Task Force on
deploying AI for India's economic

00:31:47.200 --> 00:31:47.210
deploying AI for India's economic
 

00:31:47.210 --> 00:31:48.999
deploying AI for India's economic
transformation and one of the key

00:31:48.999 --> 00:31:49.009
transformation and one of the key
 

00:31:49.009 --> 00:31:53.649
transformation and one of the key
enablers that we have come up with is

00:31:53.649 --> 00:31:53.659
enablers that we have come up with is
 

00:31:53.659 --> 00:31:57.460
enablers that we have come up with is
this issue of public confidence in AI

00:31:57.460 --> 00:31:57.470
this issue of public confidence in AI
 

00:31:57.470 --> 00:32:00.369
this issue of public confidence in AI
use cases so we have come out with some

00:32:00.369 --> 00:32:00.379
use cases so we have come out with some
 

00:32:00.379 --> 00:32:03.129
use cases so we have come out with some
suggestions on how to build up trust and

00:32:03.129 --> 00:32:03.139
suggestions on how to build up trust and
 

00:32:03.139 --> 00:32:06.549
suggestions on how to build up trust and
I think in the multilateral domain as

00:32:06.549 --> 00:32:06.559
I think in the multilateral domain as
 

00:32:06.559 --> 00:32:09.099
I think in the multilateral domain as
well I think companies would find it

00:32:09.099 --> 00:32:09.109
well I think companies would find it
 

00:32:09.109 --> 00:32:11.169
well I think companies would find it
useful to connect with these

00:32:11.169 --> 00:32:11.179
useful to connect with these
 

00:32:11.179 --> 00:32:14.019
useful to connect with these
conversations so if a community of arms

00:32:14.019 --> 00:32:14.029
conversations so if a community of arms
 

00:32:14.029 --> 00:32:15.970
conversations so if a community of arms
controllers international security

00:32:15.970 --> 00:32:15.980
controllers international security
 

00:32:15.980 --> 00:32:19.330
controllers international security
experts humanitarian law experts is

00:32:19.330 --> 00:32:19.340
experts humanitarian law experts is
 

00:32:19.340 --> 00:32:21.970
experts humanitarian law experts is
concerned about possible loss of human

00:32:21.970 --> 00:32:21.980
concerned about possible loss of human
 

00:32:21.980 --> 00:32:25.090
concerned about possible loss of human
control and supervision over critical

00:32:25.090 --> 00:32:25.100
control and supervision over critical
 

00:32:25.100 --> 00:32:27.580
control and supervision over critical
functions of n systems in the future I

00:32:27.580 --> 00:32:27.590
functions of n systems in the future I
 

00:32:27.590 --> 00:32:30.729
functions of n systems in the future I
think companies would be well advised to

00:32:30.729 --> 00:32:30.739
think companies would be well advised to
 

00:32:30.739 --> 00:32:34.060
think companies would be well advised to
follow that discussion try and engage

00:32:34.060 --> 00:32:34.070
follow that discussion try and engage
 

00:32:34.070 --> 00:32:36.519
follow that discussion try and engage
and try and influence it make sure that

00:32:36.519 --> 00:32:36.529
and try and influence it make sure that
 

00:32:36.529 --> 00:32:38.380
and try and influence it make sure that
we don't end up with a we

00:32:38.380 --> 00:32:38.390
we don't end up with a we
 

00:32:38.390 --> 00:32:41.200
we don't end up with a we
regulation scheme that offends their

00:32:41.200 --> 00:32:41.210
regulation scheme that offends their
 

00:32:41.210 --> 00:32:44.590
regulation scheme that offends their
plans or they don't end up with some

00:32:44.590 --> 00:32:44.600
plans or they don't end up with some
 

00:32:44.600 --> 00:32:46.990
plans or they don't end up with some
real use cases that upend our current

00:32:46.990 --> 00:32:47.000
real use cases that upend our current
 

00:32:47.000 --> 00:32:50.650
real use cases that upend our current
understanding of international law so I

00:32:50.650 --> 00:32:50.660
understanding of international law so I
 

00:32:50.660 --> 00:32:54.460
understanding of international law so I
think we need to work together as I

00:32:54.460 --> 00:32:54.470
think we need to work together as I
 

00:32:54.470 --> 00:32:55.990
think we need to work together as I
mentioned there are these experiments

00:32:55.990 --> 00:32:56.000
mentioned there are these experiments
 

00:32:56.000 --> 00:32:59.440
mentioned there are these experiments
that are taking place and I'm very happy

00:32:59.440 --> 00:32:59.450
that are taking place and I'm very happy
 

00:32:59.450 --> 00:33:02.580
that are taking place and I'm very happy
that we see more and more entrepreneurs

00:33:02.580 --> 00:33:02.590
that we see more and more entrepreneurs
 

00:33:02.590 --> 00:33:06.400
that we see more and more entrepreneurs
ai entrepreneurs academics young

00:33:06.400 --> 00:33:06.410
ai entrepreneurs academics young
 

00:33:06.410 --> 00:33:09.040
ai entrepreneurs academics young
students at these meetings they are

00:33:09.040 --> 00:33:09.050
students at these meetings they are
 

00:33:09.050 --> 00:33:11.860
students at these meetings they are
absorbing this policy discussion and we

00:33:11.860 --> 00:33:11.870
absorbing this policy discussion and we
 

00:33:11.870 --> 00:33:15.250
absorbing this policy discussion and we
are absorbing their impulses we limit we

00:33:15.250 --> 00:33:15.260
are absorbing their impulses we limit we
 

00:33:15.260 --> 00:33:17.710
are absorbing their impulses we limit we
have expert panels we try and bring

00:33:17.710 --> 00:33:17.720
have expert panels we try and bring
 

00:33:17.720 --> 00:33:20.230
have expert panels we try and bring
people from around the globe different

00:33:20.230 --> 00:33:20.240
people from around the globe different
 

00:33:20.240 --> 00:33:21.700
people from around the globe different
perspectives on technology and

00:33:21.700 --> 00:33:21.710
perspectives on technology and
 

00:33:21.710 --> 00:33:24.280
perspectives on technology and
development because the future of AI is

00:33:24.280 --> 00:33:24.290
development because the future of AI is
 

00:33:24.290 --> 00:33:27.340
development because the future of AI is
going to be a global future the manpower

00:33:27.340 --> 00:33:27.350
going to be a global future the manpower
 

00:33:27.350 --> 00:33:29.230
going to be a global future the manpower
that powers the research and development

00:33:29.230 --> 00:33:29.240
that powers the research and development
 

00:33:29.240 --> 00:33:33.250
that powers the research and development
is globe even today so I think we'll

00:33:33.250 --> 00:33:33.260
is globe even today so I think we'll
 

00:33:33.260 --> 00:33:36.760
is globe even today so I think we'll
have to think together about global

00:33:36.760 --> 00:33:36.770
have to think together about global
 

00:33:36.770 --> 00:33:40.200
have to think together about global
governance and work on it at different

00:33:40.200 --> 00:33:40.210
governance and work on it at different
 

00:33:40.210 --> 00:33:42.640
governance and work on it at different
levels in different communities in

00:33:42.640 --> 00:33:42.650
levels in different communities in
 

00:33:42.650 --> 00:33:48.100
levels in different communities in
different forums just just to add to

00:33:48.100 --> 00:33:48.110
different forums just just to add to
 

00:33:48.110 --> 00:33:50.710
different forums just just to add to
what Amanda said you know in in the UN

00:33:50.710 --> 00:33:50.720
what Amanda said you know in in the UN
 

00:33:50.720 --> 00:33:52.900
what Amanda said you know in in the UN
context you know we have I mean one of

00:33:52.900 --> 00:33:52.910
context you know we have I mean one of
 

00:33:52.910 --> 00:33:54.700
context you know we have I mean one of
the groups that has always also been

00:33:54.700 --> 00:33:54.710
the groups that has always also been
 

00:33:54.710 --> 00:33:57.340
the groups that has always also been
here this week its investors site which

00:33:57.340 --> 00:33:57.350
here this week its investors site which
 

00:33:57.350 --> 00:33:59.740
here this week its investors site which
is incredibly important you know both

00:33:59.740 --> 00:33:59.750
is incredibly important you know both
 

00:33:59.750 --> 00:34:02.290
is incredibly important you know both
for any developments happening in the

00:34:02.290 --> 00:34:02.300
for any developments happening in the
 

00:34:02.300 --> 00:34:04.500
for any developments happening in the
government or or in the private sphere

00:34:04.500 --> 00:34:04.510
government or or in the private sphere
 

00:34:04.510 --> 00:34:06.640
government or or in the private sphere
the UN has its framework called

00:34:06.640 --> 00:34:06.650
the UN has its framework called
 

00:34:06.650 --> 00:34:08.169
the UN has its framework called
sustainable development goals which is

00:34:08.169 --> 00:34:08.179
sustainable development goals which is
 

00:34:08.179 --> 00:34:12.850
sustainable development goals which is
17 goals with a bunch of loosely you

00:34:12.850 --> 00:34:12.860
17 goals with a bunch of loosely you
 

00:34:12.860 --> 00:34:14.500
17 goals with a bunch of loosely you
know some would say there are quite

00:34:14.500 --> 00:34:14.510
know some would say there are quite
 

00:34:14.510 --> 00:34:16.690
know some would say there are quite
generic but nevertheless quite detailed

00:34:16.690 --> 00:34:16.700
generic but nevertheless quite detailed
 

00:34:16.700 --> 00:34:19.389
generic but nevertheless quite detailed
indicators now these sustainable

00:34:19.389 --> 00:34:19.399
indicators now these sustainable
 

00:34:19.399 --> 00:34:21.040
indicators now these sustainable
developments small development goals

00:34:21.040 --> 00:34:21.050
developments small development goals
 

00:34:21.050 --> 00:34:22.930
developments small development goals
that has been adopted by all the members

00:34:22.930 --> 00:34:22.940
that has been adopted by all the members
 

00:34:22.940 --> 00:34:26.980
that has been adopted by all the members
of the UN so under 94 members and also

00:34:26.980 --> 00:34:26.990
of the UN so under 94 members and also
 

00:34:26.990 --> 00:34:30.100
of the UN so under 94 members and also
at a I think many quite surprising pick

00:34:30.100 --> 00:34:30.110
at a I think many quite surprising pick
 

00:34:30.110 --> 00:34:32.740
at a I think many quite surprising pick
up multinationals and industries because

00:34:32.740 --> 00:34:32.750
up multinationals and industries because
 

00:34:32.750 --> 00:34:34.899
up multinationals and industries because
they find them quite easy to work with

00:34:34.899 --> 00:34:34.909
they find them quite easy to work with
 

00:34:34.909 --> 00:34:36.970
they find them quite easy to work with
you know they're tough they have clear

00:34:36.970 --> 00:34:36.980
you know they're tough they have clear
 

00:34:36.980 --> 00:34:39.340
you know they're tough they have clear
indicators and you can measure you know

00:34:39.340 --> 00:34:39.350
indicators and you can measure you know
 

00:34:39.350 --> 00:34:41.250
indicators and you can measure you know
what you're doing in sort of your normal

00:34:41.250 --> 00:34:41.260
what you're doing in sort of your normal
 

00:34:41.260 --> 00:34:43.930
what you're doing in sort of your normal
communal operational Genuity plans

00:34:43.930 --> 00:34:43.940
communal operational Genuity plans
 

00:34:43.940 --> 00:34:45.340
communal operational Genuity plans
business continuity plans you know

00:34:45.340 --> 00:34:45.350
business continuity plans you know
 

00:34:45.350 --> 00:34:48.190
business continuity plans you know
against these these indicators and 1/2

00:34:48.190 --> 00:34:48.200
against these these indicators and 1/2
 

00:34:48.200 --> 00:34:50.649
against these these indicators and 1/2
of these goals are are actually so it's

00:34:50.649 --> 00:34:50.659
of these goals are are actually so it's
 

00:34:50.659 --> 00:34:51.340
of these goals are are actually so it's
go

00:34:51.340 --> 00:34:51.350
go
 

00:34:51.350 --> 00:34:53.440
go
number nine which is industrial gross

00:34:53.440 --> 00:34:53.450
number nine which is industrial gross
 

00:34:53.450 --> 00:34:56.230
number nine which is industrial gross
evasion and then goal number sixteen is

00:34:56.230 --> 00:34:56.240
evasion and then goal number sixteen is
 

00:34:56.240 --> 00:34:59.410
evasion and then goal number sixteen is
peace peace justice and strong

00:34:59.410 --> 00:34:59.420
peace peace justice and strong
 

00:34:59.420 --> 00:35:00.820
peace peace justice and strong
institutions and then you can both

00:35:00.820 --> 00:35:00.830
institutions and then you can both
 

00:35:00.830 --> 00:35:02.710
institutions and then you can both
seventeen which is around partnership

00:35:02.710 --> 00:35:02.720
seventeen which is around partnership
 

00:35:02.720 --> 00:35:05.380
seventeen which is around partnership
now if we look at this tree together you

00:35:05.380 --> 00:35:05.390
now if we look at this tree together you
 

00:35:05.390 --> 00:35:07.360
now if we look at this tree together you
really come out thinking okay how do we

00:35:07.360 --> 00:35:07.370
really come out thinking okay how do we
 

00:35:07.370 --> 00:35:09.670
really come out thinking okay how do we
ensure that at industrial growth and

00:35:09.670 --> 00:35:09.680
ensure that at industrial growth and
 

00:35:09.680 --> 00:35:11.740
ensure that at industrial growth and
innovation is done in a responsible way

00:35:11.740 --> 00:35:11.750
innovation is done in a responsible way
 

00:35:11.750 --> 00:35:13.720
innovation is done in a responsible way
and that we have the institutions and

00:35:13.720 --> 00:35:13.730
and that we have the institutions and
 

00:35:13.730 --> 00:35:15.400
and that we have the institutions and
the governance frameworks in place to

00:35:15.400 --> 00:35:15.410
the governance frameworks in place to
 

00:35:15.410 --> 00:35:18.850
the governance frameworks in place to
you know provide the right impetus and

00:35:18.850 --> 00:35:18.860
you know provide the right impetus and
 

00:35:18.860 --> 00:35:20.590
you know provide the right impetus and
not necessarily regulations but the

00:35:20.590 --> 00:35:20.600
not necessarily regulations but the
 

00:35:20.600 --> 00:35:23.260
not necessarily regulations but the
Rhind impetus and framing for for that

00:35:23.260 --> 00:35:23.270
Rhind impetus and framing for for that
 

00:35:23.270 --> 00:35:27.100
Rhind impetus and framing for for that
growth to truly benefit everyone else so

00:35:27.100 --> 00:35:27.110
growth to truly benefit everyone else so
 

00:35:27.110 --> 00:35:29.650
growth to truly benefit everyone else so
the SDGs as they called the acronym for

00:35:29.650 --> 00:35:29.660
the SDGs as they called the acronym for
 

00:35:29.660 --> 00:35:31.990
the SDGs as they called the acronym for
it you know it's not I mean it's not an

00:35:31.990 --> 00:35:32.000
it you know it's not I mean it's not an
 

00:35:32.000 --> 00:35:33.970
it you know it's not I mean it's not an
Alec heart it's not like you go with one

00:35:33.970 --> 00:35:33.980
Alec heart it's not like you go with one
 

00:35:33.980 --> 00:35:35.440
Alec heart it's not like you go with one
goal or the other but it's how do you

00:35:35.440 --> 00:35:35.450
goal or the other but it's how do you
 

00:35:35.450 --> 00:35:38.740
goal or the other but it's how do you
see how these goals converge you also

00:35:38.740 --> 00:35:38.750
see how these goals converge you also
 

00:35:38.750 --> 00:35:42.070
see how these goals converge you also
have life on land you have education

00:35:42.070 --> 00:35:42.080
have life on land you have education
 

00:35:42.080 --> 00:35:43.630
have life on land you have education
quality education and learning for all

00:35:43.630 --> 00:35:43.640
quality education and learning for all
 

00:35:43.640 --> 00:35:46.270
quality education and learning for all
you have children's rights and you have

00:35:46.270 --> 00:35:46.280
you have children's rights and you have
 

00:35:46.280 --> 00:35:48.370
you have children's rights and you have
gender and you know there's been a lot

00:35:48.370 --> 00:35:48.380
gender and you know there's been a lot
 

00:35:48.380 --> 00:35:50.170
gender and you know there's been a lot
of talk about you know the gender biases

00:35:50.170 --> 00:35:50.180
of talk about you know the gender biases
 

00:35:50.180 --> 00:35:51.670
of talk about you know the gender biases
and technology and technology is not

00:35:51.670 --> 00:35:51.680
and technology and technology is not
 

00:35:51.680 --> 00:35:53.830
and technology and technology is not
neutral and the lack of women in STEM

00:35:53.830 --> 00:35:53.840
neutral and the lack of women in STEM
 

00:35:53.840 --> 00:35:57.040
neutral and the lack of women in STEM
fields over all and so one of the other

00:35:57.040 --> 00:35:57.050
fields over all and so one of the other
 

00:35:57.050 --> 00:35:59.290
fields over all and so one of the other
issues looking at is nine and and gender

00:35:59.290 --> 00:35:59.300
issues looking at is nine and and gender
 

00:35:59.300 --> 00:36:01.270
issues looking at is nine and and gender
how do we make sure that industrial

00:36:01.270 --> 00:36:01.280
how do we make sure that industrial
 

00:36:01.280 --> 00:36:03.430
how do we make sure that industrial
growth and innovation also reflects the

00:36:03.430 --> 00:36:03.440
growth and innovation also reflects the
 

00:36:03.440 --> 00:36:06.190
growth and innovation also reflects the
fact that we are 50/50 share planet how

00:36:06.190 --> 00:36:06.200
fact that we are 50/50 share planet how
 

00:36:06.200 --> 00:36:08.590
fact that we are 50/50 share planet how
does grows an innovation reflect that we

00:36:08.590 --> 00:36:08.600
does grows an innovation reflect that we
 

00:36:08.600 --> 00:36:10.150
does grows an innovation reflect that we
have to really think about the future

00:36:10.150 --> 00:36:10.160
have to really think about the future
 

00:36:10.160 --> 00:36:12.160
have to really think about the future
digital footprints of our children do we

00:36:12.160 --> 00:36:12.170
digital footprints of our children do we
 

00:36:12.170 --> 00:36:13.630
digital footprints of our children do we
have the right framing in place which

00:36:13.630 --> 00:36:13.640
have the right framing in place which
 

00:36:13.640 --> 00:36:15.040
have the right framing in place which
goes to 60 again you know do you have

00:36:15.040 --> 00:36:15.050
goes to 60 again you know do you have
 

00:36:15.050 --> 00:36:17.260
goes to 60 again you know do you have
the right institutions in place so there

00:36:17.260 --> 00:36:17.270
the right institutions in place so there
 

00:36:17.270 --> 00:36:18.700
the right institutions in place so there
are a lot of I will call this

00:36:18.700 --> 00:36:18.710
are a lot of I will call this
 

00:36:18.710 --> 00:36:21.640
are a lot of I will call this
integrative models that would help us

00:36:21.640 --> 00:36:21.650
integrative models that would help us
 

00:36:21.650 --> 00:36:24.160
integrative models that would help us
move forward on this but only if we

00:36:24.160 --> 00:36:24.170
move forward on this but only if we
 

00:36:24.170 --> 00:36:26.440
move forward on this but only if we
really pay attention to it which is of

00:36:26.440 --> 00:36:26.450
really pay attention to it which is of
 

00:36:26.450 --> 00:36:28.360
really pay attention to it which is of
course where we all have to do our part

00:36:28.360 --> 00:36:28.370
course where we all have to do our part
 

00:36:28.370 --> 00:36:31.540
course where we all have to do our part
to make sure that happens well I'm and

00:36:31.540 --> 00:36:31.550
to make sure that happens well I'm and
 

00:36:31.550 --> 00:36:33.130
to make sure that happens well I'm and
depend on yeah I want to say thank you

00:36:33.130 --> 00:36:33.140
depend on yeah I want to say thank you
 

00:36:33.140 --> 00:36:34.950
depend on yeah I want to say thank you
so much for taking time in this

00:36:34.950 --> 00:36:34.960
so much for taking time in this
 

00:36:34.960 --> 00:36:37.990
so much for taking time in this
incredibly busy week for you I think you

00:36:37.990 --> 00:36:38.000
incredibly busy week for you I think you
 

00:36:38.000 --> 00:36:40.150
incredibly busy week for you I think you
know you've really laid out an enormous

00:36:40.150 --> 00:36:40.160
know you've really laid out an enormous
 

00:36:40.160 --> 00:36:43.540
know you've really laid out an enormous
space but it's a complex one but one

00:36:43.540 --> 00:36:43.550
space but it's a complex one but one
 

00:36:43.550 --> 00:36:45.130
space but it's a complex one but one
that we might hope is tractable through

00:36:45.130 --> 00:36:45.140
that we might hope is tractable through
 

00:36:45.140 --> 00:36:46.540
that we might hope is tractable through
the kinds of diplomatic efforts that

00:36:46.540 --> 00:36:46.550
the kinds of diplomatic efforts that
 

00:36:46.550 --> 00:36:47.590
the kinds of diplomatic efforts that
you're leading and the kinds of

00:36:47.590 --> 00:36:47.600
you're leading and the kinds of
 

00:36:47.600 --> 00:36:49.450
you're leading and the kinds of
conversations that we're trying to have

00:36:49.450 --> 00:36:49.460
conversations that we're trying to have
 

00:36:49.460 --> 00:36:51.640
conversations that we're trying to have
here in Pittsburgh and we really

00:36:51.640 --> 00:36:51.650
here in Pittsburgh and we really
 

00:36:51.650 --> 00:36:53.830
here in Pittsburgh and we really
appreciate you taking this time when I'm

00:36:53.830 --> 00:36:53.840
appreciate you taking this time when I'm
 

00:36:53.840 --> 00:36:56.020
appreciate you taking this time when I'm
sure it's been a hectic day already for

00:36:56.020 --> 00:36:56.030
sure it's been a hectic day already for
 

00:36:56.030 --> 00:37:00.310
sure it's been a hectic day already for
you thank you very much great pleasure

00:37:00.310 --> 00:37:00.320
you thank you very much great pleasure
 

00:37:00.320 --> 00:37:08.060
you thank you very much great pleasure
thank you

00:37:08.060 --> 00:37:08.070
 

00:37:08.070 --> 00:37:12.660
so as we were just talking the it's a

00:37:12.660 --> 00:37:12.670
so as we were just talking the it's a
 

00:37:12.670 --> 00:37:14.610
so as we were just talking the it's a
space of incredible complexity it's a

00:37:14.610 --> 00:37:14.620
space of incredible complexity it's a
 

00:37:14.620 --> 00:37:17.310
space of incredible complexity it's a
spray space where the on the ground

00:37:17.310 --> 00:37:17.320
spray space where the on the ground
 

00:37:17.320 --> 00:37:19.830
spray space where the on the ground
kinds of details matter how these

00:37:19.830 --> 00:37:19.840
kinds of details matter how these
 

00:37:19.840 --> 00:37:21.810
kinds of details matter how these
policies get implemented who's able to

00:37:21.810 --> 00:37:21.820
policies get implemented who's able to
 

00:37:21.820 --> 00:37:23.190
policies get implemented who's able to
get in the room to have these

00:37:23.190 --> 00:37:23.200
get in the room to have these
 

00:37:23.200 --> 00:37:25.080
get in the room to have these
discussions and how do we make sure that

00:37:25.080 --> 00:37:25.090
discussions and how do we make sure that
 

00:37:25.090 --> 00:37:27.990
discussions and how do we make sure that
it's all the right issues that are

00:37:27.990 --> 00:37:28.000
it's all the right issues that are
 

00:37:28.000 --> 00:37:29.910
it's all the right issues that are
supporting the human whether it be

00:37:29.910 --> 00:37:29.920
supporting the human whether it be
 

00:37:29.920 --> 00:37:32.130
supporting the human whether it be
political or economic or technological

00:37:32.130 --> 00:37:32.140
political or economic or technological
 

00:37:32.140 --> 00:37:34.830
political or economic or technological
needs so we're gonna in the remainder of

00:37:34.830 --> 00:37:34.840
needs so we're gonna in the remainder of
 

00:37:34.840 --> 00:37:36.720
needs so we're gonna in the remainder of
the session have two more spotlights

00:37:36.720 --> 00:37:36.730
the session have two more spotlights
 

00:37:36.730 --> 00:37:38.970
the session have two more spotlights
followed by a conversation spotlight by

00:37:38.970 --> 00:37:38.980
followed by a conversation spotlight by
 

00:37:38.980 --> 00:37:41.130
followed by a conversation spotlight by
laurie faith Craner of Carnegie Mellon

00:37:41.130 --> 00:37:41.140
laurie faith Craner of Carnegie Mellon
 

00:37:41.140 --> 00:37:43.830
laurie faith Craner of Carnegie Mellon
and Kay Firth Butterfield but from the

00:37:43.830 --> 00:37:43.840
and Kay Firth Butterfield but from the
 

00:37:43.840 --> 00:37:45.780
and Kay Firth Butterfield but from the
World Economic Forum and to introduce

00:37:45.780 --> 00:37:45.790
World Economic Forum and to introduce
 

00:37:45.790 --> 00:37:47.700
World Economic Forum and to introduce
and moderate the subsequent conversation

00:37:47.700 --> 00:37:47.710
and moderate the subsequent conversation
 

00:37:47.710 --> 00:37:50.190
and moderate the subsequent conversation
I'm pleased to welcome back up Tom Simon

00:37:50.190 --> 00:37:50.200
I'm pleased to welcome back up Tom Simon
 

00:37:50.200 --> 00:37:56.640
I'm pleased to welcome back up Tom Simon
night from Wired magazine thank you so

00:37:56.640 --> 00:37:56.650
night from Wired magazine thank you so
 

00:37:56.650 --> 00:37:58.920
night from Wired magazine thank you so
our first speaker is Laurie crema she's

00:37:58.920 --> 00:37:58.930
our first speaker is Laurie crema she's
 

00:37:58.930 --> 00:38:01.320
our first speaker is Laurie crema she's
a great person to have up here being as

00:38:01.320 --> 00:38:01.330
a great person to have up here being as
 

00:38:01.330 --> 00:38:03.450
a great person to have up here being as
she is a professor of computer science

00:38:03.450 --> 00:38:03.460
she is a professor of computer science
 

00:38:03.460 --> 00:38:05.880
she is a professor of computer science
but also public policy and engineering

00:38:05.880 --> 00:38:05.890
but also public policy and engineering
 

00:38:05.890 --> 00:38:08.430
but also public policy and engineering
here at Carnegie Mellon she's been a

00:38:08.430 --> 00:38:08.440
here at Carnegie Mellon she's been a
 

00:38:08.440 --> 00:38:11.130
here at Carnegie Mellon she's been a
pioneer in researching the usability and

00:38:11.130 --> 00:38:11.140
pioneer in researching the usability and
 

00:38:11.140 --> 00:38:14.040
pioneer in researching the usability and
human aspects of security and privacy

00:38:14.040 --> 00:38:14.050
human aspects of security and privacy
 

00:38:14.050 --> 00:38:17.070
human aspects of security and privacy
and she spent 2016 as the chief

00:38:17.070 --> 00:38:17.080
and she spent 2016 as the chief
 

00:38:17.080 --> 00:38:18.450
and she spent 2016 as the chief
technologist at the Federal Trade

00:38:18.450 --> 00:38:18.460
technologist at the Federal Trade
 

00:38:18.460 --> 00:38:26.190
technologist at the Federal Trade
Commission Lori please join us thank you

00:38:26.190 --> 00:38:26.200
Commission Lori please join us thank you
 

00:38:26.200 --> 00:38:32.480
Commission Lori please join us thank you
I'm really delighted to be here today so

00:38:32.480 --> 00:38:32.490
 

00:38:32.490 --> 00:38:35.280
increasingly algorithms are being used

00:38:35.280 --> 00:38:35.290
increasingly algorithms are being used
 

00:38:35.290 --> 00:38:37.620
increasingly algorithms are being used
to make decisions that impact our lives

00:38:37.620 --> 00:38:37.630
to make decisions that impact our lives
 

00:38:37.630 --> 00:38:40.860
to make decisions that impact our lives
as we've heard about earlier today the

00:38:40.860 --> 00:38:40.870
as we've heard about earlier today the
 

00:38:40.870 --> 00:38:43.440
as we've heard about earlier today the
privacy concerns we have about the

00:38:43.440 --> 00:38:43.450
privacy concerns we have about the
 

00:38:43.450 --> 00:38:46.040
privacy concerns we have about the
collection of our social network data

00:38:46.040 --> 00:38:46.050
collection of our social network data
 

00:38:46.050 --> 00:38:49.590
collection of our social network data
transactions location traces and other

00:38:49.590 --> 00:38:49.600
transactions location traces and other
 

00:38:49.600 --> 00:38:52.290
transactions location traces and other
information are made all the worse by

00:38:52.290 --> 00:38:52.300
information are made all the worse by
 

00:38:52.300 --> 00:38:54.450
information are made all the worse by
the uncertainty about how our

00:38:54.450 --> 00:38:54.460
the uncertainty about how our
 

00:38:54.460 --> 00:38:56.550
the uncertainty about how our
information is going to be used and

00:38:56.550 --> 00:38:56.560
information is going to be used and
 

00:38:56.560 --> 00:38:59.130
information is going to be used and
analyzed by algorithms that we don't

00:38:59.130 --> 00:38:59.140
analyzed by algorithms that we don't
 

00:38:59.140 --> 00:39:02.660
analyzed by algorithms that we don't
understand and may not even know exists

00:39:02.660 --> 00:39:02.670
understand and may not even know exists
 

00:39:02.670 --> 00:39:05.460
understand and may not even know exists
much of my labs research focuses on

00:39:05.460 --> 00:39:05.470
much of my labs research focuses on
 

00:39:05.470 --> 00:39:09.180
much of my labs research focuses on
privacy when we interview people about

00:39:09.180 --> 00:39:09.190
privacy when we interview people about
 

00:39:09.190 --> 00:39:10.440
privacy when we interview people about
online privacy

00:39:10.440 --> 00:39:10.450
online privacy
 

00:39:10.450 --> 00:39:13.590
online privacy
they are often puzzled about why ads for

00:39:13.590 --> 00:39:13.600
they are often puzzled about why ads for
 

00:39:13.600 --> 00:39:15.510
they are often puzzled about why ads for
online merchants that they visited

00:39:15.510 --> 00:39:15.520
online merchants that they visited
 

00:39:15.520 --> 00:39:16.230
online merchants that they visited
yesterday

00:39:16.230 --> 00:39:16.240
yesterday
 

00:39:16.240 --> 00:39:18.839
yesterday
are following them around the internet

00:39:18.839 --> 00:39:18.849
are following them around the internet
 

00:39:18.849 --> 00:39:22.170
are following them around the internet
today they don't understand where their

00:39:22.170 --> 00:39:22.180
today they don't understand where their
 

00:39:22.180 --> 00:39:24.950
today they don't understand where their
data goes or how it is used by

00:39:24.950 --> 00:39:24.960
data goes or how it is used by
 

00:39:24.960 --> 00:39:30.780
data goes or how it is used by
algorithms to target ads to them besides

00:39:30.780 --> 00:39:30.790
algorithms to target ads to them besides
 

00:39:30.790 --> 00:39:34.170
algorithms to target ads to them besides
being used to target ads algorithms may

00:39:34.170 --> 00:39:34.180
being used to target ads algorithms may
 

00:39:34.180 --> 00:39:36.480
being used to target ads algorithms may
be used for decisions that may have a

00:39:36.480 --> 00:39:36.490
be used for decisions that may have a
 

00:39:36.490 --> 00:39:40.020
be used for decisions that may have a
big impact on our lives for example to

00:39:40.020 --> 00:39:40.030
big impact on our lives for example to
 

00:39:40.030 --> 00:39:42.150
big impact on our lives for example to
decide whether we qualify for a mortgage

00:39:42.150 --> 00:39:42.160
decide whether we qualify for a mortgage
 

00:39:42.160 --> 00:39:47.430
decide whether we qualify for a mortgage
or other loan they may also be used to

00:39:47.430 --> 00:39:47.440
or other loan they may also be used to
 

00:39:47.440 --> 00:39:49.829
or other loan they may also be used to
decide whether we're interviewed for a

00:39:49.829 --> 00:39:49.839
decide whether we're interviewed for a
 

00:39:49.839 --> 00:39:52.859
decide whether we're interviewed for a
job or have the opportunity to enroll in

00:39:52.859 --> 00:39:52.869
job or have the opportunity to enroll in
 

00:39:52.869 --> 00:39:57.800
job or have the opportunity to enroll in
college or as we recently learned

00:39:57.800 --> 00:39:57.810
college or as we recently learned
 

00:39:57.810 --> 00:40:00.599
college or as we recently learned
algorithms are used to decide what

00:40:00.599 --> 00:40:00.609
algorithms are used to decide what
 

00:40:00.609 --> 00:40:03.800
algorithms are used to decide what
political ads to show us on social media

00:40:03.800 --> 00:40:03.810
political ads to show us on social media
 

00:40:03.810 --> 00:40:07.650
political ads to show us on social media
thus algorithms may be used not only to

00:40:07.650 --> 00:40:07.660
thus algorithms may be used not only to
 

00:40:07.660 --> 00:40:10.790
thus algorithms may be used not only to
market to us but to manipulate us and

00:40:10.790 --> 00:40:10.800
market to us but to manipulate us and
 

00:40:10.800 --> 00:40:13.530
market to us but to manipulate us and
potentially to impact our political

00:40:13.530 --> 00:40:13.540
potentially to impact our political
 

00:40:13.540 --> 00:40:19.410
potentially to impact our political
process algorithms make decisions about

00:40:19.410 --> 00:40:19.420
process algorithms make decisions about
 

00:40:19.420 --> 00:40:22.320
process algorithms make decisions about
us all the time and as we are hearing

00:40:22.320 --> 00:40:22.330
us all the time and as we are hearing
 

00:40:22.330 --> 00:40:24.630
us all the time and as we are hearing
throughout this conference algorithms

00:40:24.630 --> 00:40:24.640
throughout this conference algorithms
 

00:40:24.640 --> 00:40:28.290
throughout this conference algorithms
often encode bias where does this bias

00:40:28.290 --> 00:40:28.300
often encode bias where does this bias
 

00:40:28.300 --> 00:40:31.980
often encode bias where does this bias
come from machine learning algorithms

00:40:31.980 --> 00:40:31.990
come from machine learning algorithms
 

00:40:31.990 --> 00:40:34.200
come from machine learning algorithms
are trained with a set of training data

00:40:34.200 --> 00:40:34.210
are trained with a set of training data
 

00:40:34.210 --> 00:40:36.420
are trained with a set of training data
that teaches the algorithm through

00:40:36.420 --> 00:40:36.430
that teaches the algorithm through
 

00:40:36.430 --> 00:40:39.210
that teaches the algorithm through
examples of matching and non matching

00:40:39.210 --> 00:40:39.220
examples of matching and non matching
 

00:40:39.220 --> 00:40:42.570
examples of matching and non matching
patterns the choice of training data set

00:40:42.570 --> 00:40:42.580
patterns the choice of training data set
 

00:40:42.580 --> 00:40:45.300
patterns the choice of training data set
can impact the decisions the algorithm

00:40:45.300 --> 00:40:45.310
can impact the decisions the algorithm
 

00:40:45.310 --> 00:40:49.140
can impact the decisions the algorithm
makes indeed any biases inherent in the

00:40:49.140 --> 00:40:49.150
makes indeed any biases inherent in the
 

00:40:49.150 --> 00:40:51.390
makes indeed any biases inherent in the
training set whether introduced

00:40:51.390 --> 00:40:51.400
training set whether introduced
 

00:40:51.400 --> 00:40:54.690
training set whether introduced
intentionally or unintentionally will

00:40:54.690 --> 00:40:54.700
intentionally or unintentionally will
 

00:40:54.700 --> 00:40:57.000
intentionally or unintentionally will
likely show up in the algorithms

00:40:57.000 --> 00:40:57.010
likely show up in the algorithms
 

00:40:57.010 --> 00:41:01.349
likely show up in the algorithms
decisions for example a hiring algorithm

00:41:01.349 --> 00:41:01.359
decisions for example a hiring algorithm
 

00:41:01.359 --> 00:41:04.200
decisions for example a hiring algorithm
trained with data from past hires will

00:41:04.200 --> 00:41:04.210
trained with data from past hires will
 

00:41:04.210 --> 00:41:06.990
trained with data from past hires will
likely reproduce past prejudice and

00:41:06.990 --> 00:41:07.000
likely reproduce past prejudice and
 

00:41:07.000 --> 00:41:09.810
likely reproduce past prejudice and
continue to hire people with the same

00:41:09.810 --> 00:41:09.820
continue to hire people with the same
 

00:41:09.820 --> 00:41:12.510
continue to hire people with the same
characteristics thus it may be biased

00:41:12.510 --> 00:41:12.520
characteristics thus it may be biased
 

00:41:12.520 --> 00:41:14.250
characteristics thus it may be biased
towards hiring people of a particular

00:41:14.250 --> 00:41:14.260
towards hiring people of a particular
 

00:41:14.260 --> 00:41:16.980
towards hiring people of a particular
gender or people of a particular race or

00:41:16.980 --> 00:41:16.990
gender or people of a particular race or
 

00:41:16.990 --> 00:41:23.790
gender or people of a particular race or
people who attended certain schools

00:41:23.790 --> 00:41:23.800
 

00:41:23.800 --> 00:41:25.950
as awareness of algorithmic

00:41:25.950 --> 00:41:25.960
as awareness of algorithmic
 

00:41:25.960 --> 00:41:28.770
as awareness of algorithmic
decision-making increases with it comes

00:41:28.770 --> 00:41:28.780
decision-making increases with it comes
 

00:41:28.780 --> 00:41:32.820
decision-making increases with it comes
calls for algorithmic transparency what

00:41:32.820 --> 00:41:32.830
calls for algorithmic transparency what
 

00:41:32.830 --> 00:41:36.240
calls for algorithmic transparency what
is algorithmic transparency this is the

00:41:36.240 --> 00:41:36.250
is algorithmic transparency this is the
 

00:41:36.250 --> 00:41:38.450
is algorithmic transparency this is the
idea that we should be able to provide

00:41:38.450 --> 00:41:38.460
idea that we should be able to provide
 

00:41:38.460 --> 00:41:41.400
idea that we should be able to provide
explanations about how an algorithm

00:41:41.400 --> 00:41:41.410
explanations about how an algorithm
 

00:41:41.410 --> 00:41:44.580
explanations about how an algorithm
makes decisions we want to know when

00:41:44.580 --> 00:41:44.590
makes decisions we want to know when
 

00:41:44.590 --> 00:41:47.280
makes decisions we want to know when
algorithms are making decisions and how

00:41:47.280 --> 00:41:47.290
algorithms are making decisions and how
 

00:41:47.290 --> 00:41:50.670
algorithms are making decisions and how
those decisions are being made but this

00:41:50.670 --> 00:41:50.680
those decisions are being made but this
 

00:41:50.680 --> 00:41:53.100
those decisions are being made but this
can be difficult to determine and even

00:41:53.100 --> 00:41:53.110
can be difficult to determine and even
 

00:41:53.110 --> 00:41:58.140
can be difficult to determine and even
more difficult to explain when the

00:41:58.140 --> 00:41:58.150
more difficult to explain when the
 

00:41:58.150 --> 00:42:00.570
more difficult to explain when the
algorithms are based on simple rules it

00:42:00.570 --> 00:42:00.580
algorithms are based on simple rules it
 

00:42:00.580 --> 00:42:03.300
algorithms are based on simple rules it
may be relatively straightforward for

00:42:03.300 --> 00:42:03.310
may be relatively straightforward for
 

00:42:03.310 --> 00:42:06.450
may be relatively straightforward for
example here it is easy to tell someone

00:42:06.450 --> 00:42:06.460
example here it is easy to tell someone
 

00:42:06.460 --> 00:42:08.760
example here it is easy to tell someone
why they are seeing an ad for sweaters

00:42:08.760 --> 00:42:08.770
why they are seeing an ad for sweaters
 

00:42:08.770 --> 00:42:11.220
why they are seeing an ad for sweaters
and dresses there's a little blue

00:42:11.220 --> 00:42:11.230
and dresses there's a little blue
 

00:42:11.230 --> 00:42:13.980
and dresses there's a little blue
triangle icon I can click on that gives

00:42:13.980 --> 00:42:13.990
triangle icon I can click on that gives
 

00:42:13.990 --> 00:42:17.670
triangle icon I can click on that gives
me more information as you can see it

00:42:17.670 --> 00:42:17.680
me more information as you can see it
 

00:42:17.680 --> 00:42:19.770
me more information as you can see it
seems that these were the last products

00:42:19.770 --> 00:42:19.780
seems that these were the last products
 

00:42:19.780 --> 00:42:22.590
seems that these were the last products
that I viewed on this website and people

00:42:22.590 --> 00:42:22.600
that I viewed on this website and people
 

00:42:22.600 --> 00:42:24.900
that I viewed on this website and people
who looked at these products often ended

00:42:24.900 --> 00:42:24.910
who looked at these products often ended
 

00:42:24.910 --> 00:42:26.220
who looked at these products often ended
up choosing the ones that they are

00:42:26.220 --> 00:42:26.230
up choosing the ones that they are
 

00:42:26.230 --> 00:42:29.880
up choosing the ones that they are
advertising to me but when decisions are

00:42:29.880 --> 00:42:29.890
advertising to me but when decisions are
 

00:42:29.890 --> 00:42:32.670
advertising to me but when decisions are
based on artificial intelligence it may

00:42:32.670 --> 00:42:32.680
based on artificial intelligence it may
 

00:42:32.680 --> 00:42:35.430
based on artificial intelligence it may
be difficult to determine exactly why a

00:42:35.430 --> 00:42:35.440
be difficult to determine exactly why a
 

00:42:35.440 --> 00:42:38.400
be difficult to determine exactly why a
particular decision was made and the

00:42:38.400 --> 00:42:38.410
particular decision was made and the
 

00:42:38.410 --> 00:42:40.710
particular decision was made and the
explanations may not all be all that

00:42:40.710 --> 00:42:40.720
explanations may not all be all that
 

00:42:40.720 --> 00:42:47.220
explanations may not all be all that
insightful here is an example from my

00:42:47.220 --> 00:42:47.230
insightful here is an example from my
 

00:42:47.230 --> 00:42:50.640
insightful here is an example from my
own research a couple of years ago we

00:42:50.640 --> 00:42:50.650
own research a couple of years ago we
 

00:42:50.650 --> 00:42:53.280
own research a couple of years ago we
developed a password meter that analyzed

00:42:53.280 --> 00:42:53.290
developed a password meter that analyzed
 

00:42:53.290 --> 00:42:55.470
developed a password meter that analyzed
the password as a user created it and

00:42:55.470 --> 00:42:55.480
the password as a user created it and
 

00:42:55.480 --> 00:42:57.930
the password as a user created it and
used neural networks to come up with a

00:42:57.930 --> 00:42:57.940
used neural networks to come up with a
 

00:42:57.940 --> 00:43:01.140
used neural networks to come up with a
guess ability score the guess ability

00:43:01.140 --> 00:43:01.150
guess ability score the guess ability
 

00:43:01.150 --> 00:43:03.120
guess ability score the guess ability
score indicates the number of guesses

00:43:03.120 --> 00:43:03.130
score indicates the number of guesses
 

00:43:03.130 --> 00:43:05.640
score indicates the number of guesses
that the algorithm predicts it will take

00:43:05.640 --> 00:43:05.650
that the algorithm predicts it will take
 

00:43:05.650 --> 00:43:09.210
that the algorithm predicts it will take
an attacker to guess that password so

00:43:09.210 --> 00:43:09.220
an attacker to guess that password so
 

00:43:09.220 --> 00:43:11.250
an attacker to guess that password so
the higher the guess ability score the

00:43:11.250 --> 00:43:11.260
the higher the guess ability score the
 

00:43:11.260 --> 00:43:13.370
the higher the guess ability score the
stronger the password and that's good

00:43:13.370 --> 00:43:13.380
stronger the password and that's good
 

00:43:13.380 --> 00:43:16.470
stronger the password and that's good
the problem is that if the algorithm

00:43:16.470 --> 00:43:16.480
the problem is that if the algorithm
 

00:43:16.480 --> 00:43:19.520
the problem is that if the algorithm
just tells you your password is rated

00:43:19.520 --> 00:43:19.530
just tells you your password is rated
 

00:43:19.530 --> 00:43:23.970
just tells you your password is rated
575 or even your password is weak create

00:43:23.970 --> 00:43:23.980
575 or even your password is weak create
 

00:43:23.980 --> 00:43:26.700
575 or even your password is weak create
a strong password you still don't have

00:43:26.700 --> 00:43:26.710
a strong password you still don't have
 

00:43:26.710 --> 00:43:29.310
a strong password you still don't have
any information that will help you make

00:43:29.310 --> 00:43:29.320
any information that will help you make
 

00:43:29.320 --> 00:43:33.630
any information that will help you make
the password better my students came up

00:43:33.630 --> 00:43:33.640
the password better my students came up
 

00:43:33.640 --> 00:43:35.870
the password better my students came up
with a clever way to solve this problem

00:43:35.870 --> 00:43:35.880
with a clever way to solve this problem
 

00:43:35.880 --> 00:43:37.690
with a clever way to solve this problem
they generated

00:43:37.690 --> 00:43:37.700
they generated
 

00:43:37.700 --> 00:43:41.020
they generated
list of 21 heuristics for things users

00:43:41.020 --> 00:43:41.030
list of 21 heuristics for things users
 

00:43:41.030 --> 00:43:44.170
list of 21 heuristics for things users
do that make their passwords bad for

00:43:44.170 --> 00:43:44.180
do that make their passwords bad for
 

00:43:44.180 --> 00:43:46.720
do that make their passwords bad for
example they may include dictionary

00:43:46.720 --> 00:43:46.730
example they may include dictionary
 

00:43:46.730 --> 00:43:49.600
example they may include dictionary
words they may include common phrases

00:43:49.600 --> 00:43:49.610
words they may include common phrases
 

00:43:49.610 --> 00:43:52.300
words they may include common phrases
like I love you they may capitalize

00:43:52.300 --> 00:43:52.310
like I love you they may capitalize
 

00:43:52.310 --> 00:43:55.510
like I love you they may capitalize
letters only at the beginning or mean or

00:43:55.510 --> 00:43:55.520
letters only at the beginning or mean or
 

00:43:55.520 --> 00:43:57.370
letters only at the beginning or mean or
they may put all their digits at the end

00:43:57.370 --> 00:43:57.380
they may put all their digits at the end
 

00:43:57.380 --> 00:43:59.680
they may put all their digits at the end
which is a very common thing people use

00:43:59.680 --> 00:43:59.690
which is a very common thing people use
 

00:43:59.690 --> 00:44:02.890
which is a very common thing people use
to create passwords they make make

00:44:02.890 --> 00:44:02.900
to create passwords they make make
 

00:44:02.900 --> 00:44:05.380
to create passwords they make make
passwords that aren't long enough or

00:44:05.380 --> 00:44:05.390
passwords that aren't long enough or
 

00:44:05.390 --> 00:44:10.270
passwords that aren't long enough or
they might not use any symbols my

00:44:10.270 --> 00:44:10.280
they might not use any symbols my
 

00:44:10.280 --> 00:44:12.850
they might not use any symbols my
students analyzed 30,000 stolen

00:44:12.850 --> 00:44:12.860
students analyzed 30,000 stolen
 

00:44:12.860 --> 00:44:15.280
students analyzed 30,000 stolen
passwords using the neural network and

00:44:15.280 --> 00:44:15.290
passwords using the neural network and
 

00:44:15.290 --> 00:44:17.980
passwords using the neural network and
determined statistically how much each

00:44:17.980 --> 00:44:17.990
determined statistically how much each
 

00:44:17.990 --> 00:44:21.010
determined statistically how much each
of these 21 heuristics accounted for the

00:44:21.010 --> 00:44:21.020
of these 21 heuristics accounted for the
 

00:44:21.020 --> 00:44:23.980
of these 21 heuristics accounted for the
guess ability of each password this

00:44:23.980 --> 00:44:23.990
guess ability of each password this
 

00:44:23.990 --> 00:44:26.410
guess ability of each password this
allowed them to weight the heuristics to

00:44:26.410 --> 00:44:26.420
allowed them to weight the heuristics to
 

00:44:26.420 --> 00:44:28.960
allowed them to weight the heuristics to
determine which ones mattered the most

00:44:28.960 --> 00:44:28.970
determine which ones mattered the most
 

00:44:28.970 --> 00:44:32.320
determine which ones mattered the most
then after analyzing a password using

00:44:32.320 --> 00:44:32.330
then after analyzing a password using
 

00:44:32.330 --> 00:44:34.150
then after analyzing a password using
both the neural network and the

00:44:34.150 --> 00:44:34.160
both the neural network and the
 

00:44:34.160 --> 00:44:36.490
both the neural network and the
heuristics the password meter could

00:44:36.490 --> 00:44:36.500
heuristics the password meter could
 

00:44:36.500 --> 00:44:38.770
heuristics the password meter could
determine the top three reasons a

00:44:38.770 --> 00:44:38.780
determine the top three reasons a
 

00:44:38.780 --> 00:44:41.440
determine the top three reasons a
password was likely rated as bad by the

00:44:41.440 --> 00:44:41.450
password was likely rated as bad by the
 

00:44:41.450 --> 00:44:43.600
password was likely rated as bad by the
neural network and present those in

00:44:43.600 --> 00:44:43.610
neural network and present those in
 

00:44:43.610 --> 00:44:47.260
neural network and present those in
plain English to the users this approach

00:44:47.260 --> 00:44:47.270
plain English to the users this approach
 

00:44:47.270 --> 00:44:49.780
plain English to the users this approach
does not tell us precisely why the

00:44:49.780 --> 00:44:49.790
does not tell us precisely why the
 

00:44:49.790 --> 00:44:51.940
does not tell us precisely why the
neural network made the decision it did

00:44:51.940 --> 00:44:51.950
neural network made the decision it did
 

00:44:51.950 --> 00:44:54.280
neural network made the decision it did
and if the neural network figures out

00:44:54.280 --> 00:44:54.290
and if the neural network figures out
 

00:44:54.290 --> 00:44:56.710
and if the neural network figures out
something that is not captured by the

00:44:56.710 --> 00:44:56.720
something that is not captured by the
 

00:44:56.720 --> 00:44:58.960
something that is not captured by the
heuristics we won't be able to recognize

00:44:58.960 --> 00:44:58.970
heuristics we won't be able to recognize
 

00:44:58.970 --> 00:45:01.770
heuristics we won't be able to recognize
it let alone explain it to the user

00:45:01.770 --> 00:45:01.780
it let alone explain it to the user
 

00:45:01.780 --> 00:45:04.960
it let alone explain it to the user
but our user studies demonstrated that

00:45:04.960 --> 00:45:04.970
but our user studies demonstrated that
 

00:45:04.970 --> 00:45:07.180
but our user studies demonstrated that
overall this approach gives us

00:45:07.180 --> 00:45:07.190
overall this approach gives us
 

00:45:07.190 --> 00:45:09.730
overall this approach gives us
meaningful information that aids user

00:45:09.730 --> 00:45:09.740
meaningful information that aids user
 

00:45:09.740 --> 00:45:12.040
meaningful information that aids user
understanding and helps people improve

00:45:12.040 --> 00:45:12.050
understanding and helps people improve
 

00:45:12.050 --> 00:45:15.010
understanding and helps people improve
their passwords now this approach won't

00:45:15.010 --> 00:45:15.020
their passwords now this approach won't
 

00:45:15.020 --> 00:45:17.410
their passwords now this approach won't
work to explain all algorithmic

00:45:17.410 --> 00:45:17.420
work to explain all algorithmic
 

00:45:17.420 --> 00:45:19.930
work to explain all algorithmic
decision-making but it is an approach of

00:45:19.930 --> 00:45:19.940
decision-making but it is an approach of
 

00:45:19.940 --> 00:45:22.390
decision-making but it is an approach of
one - it is what an example of one type

00:45:22.390 --> 00:45:22.400
one - it is what an example of one type
 

00:45:22.400 --> 00:45:24.400
one - it is what an example of one type
of an approach towards achieving

00:45:24.400 --> 00:45:24.410
of an approach towards achieving
 

00:45:24.410 --> 00:45:28.840
of an approach towards achieving
algorithmic transparency but this is

00:45:28.840 --> 00:45:28.850
algorithmic transparency but this is
 

00:45:28.850 --> 00:45:30.910
algorithmic transparency but this is
just one approach and it's a fairly

00:45:30.910 --> 00:45:30.920
just one approach and it's a fairly
 

00:45:30.920 --> 00:45:33.730
just one approach and it's a fairly
simple one more research is needed to

00:45:33.730 --> 00:45:33.740
simple one more research is needed to
 

00:45:33.740 --> 00:45:36.010
simple one more research is needed to
come up with other approaches that will

00:45:36.010 --> 00:45:36.020
come up with other approaches that will
 

00:45:36.020 --> 00:45:38.590
come up with other approaches that will
help us provide algorithm algorithmic

00:45:38.590 --> 00:45:38.600
help us provide algorithm algorithmic
 

00:45:38.600 --> 00:45:41.020
help us provide algorithm algorithmic
transparency without resorting to

00:45:41.020 --> 00:45:41.030
transparency without resorting to
 

00:45:41.030 --> 00:45:43.390
transparency without resorting to
spitting out internal algorithmic

00:45:43.390 --> 00:45:43.400
spitting out internal algorithmic
 

00:45:43.400 --> 00:45:46.030
spitting out internal algorithmic
details that make sense only to experts

00:45:46.030 --> 00:45:46.040
details that make sense only to experts
 

00:45:46.040 --> 00:45:49.300
details that make sense only to experts
or maybe to no one at all there are many

00:45:49.300 --> 00:45:49.310
or maybe to no one at all there are many
 

00:45:49.310 --> 00:45:51.190
or maybe to no one at all there are many
researchers hard at work on this

00:45:51.190 --> 00:45:51.200
researchers hard at work on this
 

00:45:51.200 --> 00:45:53.320
researchers hard at work on this
including some of my colleagues here at

00:45:53.320 --> 00:45:53.330
including some of my colleagues here at
 

00:45:53.330 --> 00:45:56.050
including some of my colleagues here at
Carnegie Mellon but we have a ways to go

00:45:56.050 --> 00:45:56.060
Carnegie Mellon but we have a ways to go
 

00:45:56.060 --> 00:45:58.630
Carnegie Mellon but we have a ways to go
and indeed we may not ever be able to

00:45:58.630 --> 00:45:58.640
and indeed we may not ever be able to
 

00:45:58.640 --> 00:46:02.170
and indeed we may not ever be able to
fully explain how AI algorithms work in

00:46:02.170 --> 00:46:02.180
fully explain how AI algorithms work in
 

00:46:02.180 --> 00:46:03.850
fully explain how AI algorithms work in
a way that makes sense

00:46:03.850 --> 00:46:03.860
a way that makes sense
 

00:46:03.860 --> 00:46:11.170
a way that makes sense
to non experts in 2017 the Association

00:46:11.170 --> 00:46:11.180
to non experts in 2017 the Association
 

00:46:11.180 --> 00:46:13.360
to non experts in 2017 the Association
for Computing Machinery released seven

00:46:13.360 --> 00:46:13.370
for Computing Machinery released seven
 

00:46:13.370 --> 00:46:16.000
for Computing Machinery released seven
principles for algorithmic transparency

00:46:16.000 --> 00:46:16.010
principles for algorithmic transparency
 

00:46:16.010 --> 00:46:18.460
principles for algorithmic transparency
and accountability I will paraphrase

00:46:18.460 --> 00:46:18.470
and accountability I will paraphrase
 

00:46:18.470 --> 00:46:22.420
and accountability I will paraphrase
them here briefly the first principle is

00:46:22.420 --> 00:46:22.430
them here briefly the first principle is
 

00:46:22.430 --> 00:46:25.660
them here briefly the first principle is
awareness be aware of potential biases

00:46:25.660 --> 00:46:25.670
awareness be aware of potential biases
 

00:46:25.670 --> 00:46:28.420
awareness be aware of potential biases
and their potential harm there is a

00:46:28.420 --> 00:46:28.430
and their potential harm there is a
 

00:46:28.430 --> 00:46:31.210
and their potential harm there is a
temptation to assume that the algorithm

00:46:31.210 --> 00:46:31.220
temptation to assume that the algorithm
 

00:46:31.220 --> 00:46:33.840
temptation to assume that the algorithm
is pure and unbiased and can't be wrong

00:46:33.840 --> 00:46:33.850
is pure and unbiased and can't be wrong
 

00:46:33.850 --> 00:46:36.610
is pure and unbiased and can't be wrong
people blindly report that they're just

00:46:36.610 --> 00:46:36.620
people blindly report that they're just
 

00:46:36.620 --> 00:46:38.940
people blindly report that they're just
doing what the computer told them to do

00:46:38.940 --> 00:46:38.950
doing what the computer told them to do
 

00:46:38.950 --> 00:46:42.310
doing what the computer told them to do
the awareness principle encourages us to

00:46:42.310 --> 00:46:42.320
the awareness principle encourages us to
 

00:46:42.320 --> 00:46:44.650
the awareness principle encourages us to
think through sources of potential bias

00:46:44.650 --> 00:46:44.660
think through sources of potential bias
 

00:46:44.660 --> 00:46:47.170
think through sources of potential bias
and ways that people may be harmed by

00:46:47.170 --> 00:46:47.180
and ways that people may be harmed by
 

00:46:47.180 --> 00:46:49.450
and ways that people may be harmed by
algorithms and make sure that all

00:46:49.450 --> 00:46:49.460
algorithms and make sure that all
 

00:46:49.460 --> 00:46:54.100
algorithms and make sure that all
stakeholders are aware of this access

00:46:54.100 --> 00:46:54.110
stakeholders are aware of this access
 

00:46:54.110 --> 00:46:57.100
stakeholders are aware of this access
and redress adopt mechanisms for

00:46:57.100 --> 00:46:57.110
and redress adopt mechanisms for
 

00:46:57.110 --> 00:46:59.890
and redress adopt mechanisms for
questioning and redress by those who may

00:46:59.890 --> 00:46:59.900
questioning and redress by those who may
 

00:46:59.900 --> 00:47:03.340
questioning and redress by those who may
be adversely affected it is important

00:47:03.340 --> 00:47:03.350
be adversely affected it is important
 

00:47:03.350 --> 00:47:05.380
be adversely affected it is important
that we not delegate decisions to

00:47:05.380 --> 00:47:05.390
that we not delegate decisions to
 

00:47:05.390 --> 00:47:07.660
that we not delegate decisions to
algorithms without any ability to ask

00:47:07.660 --> 00:47:07.670
algorithms without any ability to ask
 

00:47:07.670 --> 00:47:10.320
algorithms without any ability to ask
questions or appeal their decisions if

00:47:10.320 --> 00:47:10.330
questions or appeal their decisions if
 

00:47:10.330 --> 00:47:12.910
questions or appeal their decisions if
someone wants to challenge a decision

00:47:12.910 --> 00:47:12.920
someone wants to challenge a decision
 

00:47:12.920 --> 00:47:15.070
someone wants to challenge a decision
there should be a clear process through

00:47:15.070 --> 00:47:15.080
there should be a clear process through
 

00:47:15.080 --> 00:47:17.110
there should be a clear process through
which they can request information about

00:47:17.110 --> 00:47:17.120
which they can request information about
 

00:47:17.120 --> 00:47:19.900
which they can request information about
how a decision was made and challenge it

00:47:19.900 --> 00:47:19.910
how a decision was made and challenge it
 

00:47:19.910 --> 00:47:22.600
how a decision was made and challenge it
it is not enough to know how a decision

00:47:22.600 --> 00:47:22.610
it is not enough to know how a decision
 

00:47:22.610 --> 00:47:24.970
it is not enough to know how a decision
was made but we need a mechanism to

00:47:24.970 --> 00:47:24.980
was made but we need a mechanism to
 

00:47:24.980 --> 00:47:27.010
was made but we need a mechanism to
investigate and correct faulty

00:47:27.010 --> 00:47:27.020
investigate and correct faulty
 

00:47:27.020 --> 00:47:31.890
investigate and correct faulty
decision-making accountability

00:47:31.890 --> 00:47:31.900
decision-making accountability
 

00:47:31.900 --> 00:47:33.640
decision-making accountability
organizations should be held accountable

00:47:33.640 --> 00:47:33.650
organizations should be held accountable
 

00:47:33.650 --> 00:47:36.340
organizations should be held accountable
for decisions made by algorithms they

00:47:36.340 --> 00:47:36.350
for decisions made by algorithms they
 

00:47:36.350 --> 00:47:39.010
for decisions made by algorithms they
use even if they can't explain how the

00:47:39.010 --> 00:47:39.020
use even if they can't explain how the
 

00:47:39.020 --> 00:47:42.790
use even if they can't explain how the
algorithm produced those results it is

00:47:42.790 --> 00:47:42.800
algorithm produced those results it is
 

00:47:42.800 --> 00:47:44.020
algorithm produced those results it is
important than a non-transparent

00:47:44.020 --> 00:47:44.030
important than a non-transparent
 

00:47:44.030 --> 00:47:47.170
important than a non-transparent
algorithm not be used as an excuse for

00:47:47.170 --> 00:47:47.180
algorithm not be used as an excuse for
 

00:47:47.180 --> 00:47:49.240
algorithm not be used as an excuse for
decisions and a way to dodge

00:47:49.240 --> 00:47:49.250
decisions and a way to dodge
 

00:47:49.250 --> 00:47:52.180
decisions and a way to dodge
accountability it is not acceptable for

00:47:52.180 --> 00:47:52.190
accountability it is not acceptable for
 

00:47:52.190 --> 00:47:54.460
accountability it is not acceptable for
an organization to throw up their hands

00:47:54.460 --> 00:47:54.470
an organization to throw up their hands
 

00:47:54.470 --> 00:47:57.340
an organization to throw up their hands
and say we didn't program the algorithm

00:47:57.340 --> 00:47:57.350
and say we didn't program the algorithm
 

00:47:57.350 --> 00:47:59.650
and say we didn't program the algorithm
to discriminate we don't know what it

00:47:59.650 --> 00:47:59.660
to discriminate we don't know what it
 

00:47:59.660 --> 00:48:01.330
to discriminate we don't know what it
learned from the training data that

00:48:01.330 --> 00:48:01.340
learned from the training data that
 

00:48:01.340 --> 00:48:03.820
learned from the training data that
caused it to be biased it's not our

00:48:03.820 --> 00:48:03.830
caused it to be biased it's not our
 

00:48:03.830 --> 00:48:04.430
caused it to be biased it's not our
response

00:48:04.430 --> 00:48:04.440
response
 

00:48:04.440 --> 00:48:07.640
response
ability if organizations are going to

00:48:07.640 --> 00:48:07.650
ability if organizations are going to
 

00:48:07.650 --> 00:48:09.410
ability if organizations are going to
delegate their decision-making to

00:48:09.410 --> 00:48:09.420
delegate their decision-making to
 

00:48:09.420 --> 00:48:11.480
delegate their decision-making to
algorithms they must be held accountable

00:48:11.480 --> 00:48:11.490
algorithms they must be held accountable
 

00:48:11.490 --> 00:48:14.059
algorithms they must be held accountable
for the decisions those algorithms make

00:48:14.059 --> 00:48:14.069
for the decisions those algorithms make
 

00:48:14.069 --> 00:48:16.940
for the decisions those algorithms make
even if they can't explain why the

00:48:16.940 --> 00:48:16.950
even if they can't explain why the
 

00:48:16.950 --> 00:48:19.490
even if they can't explain why the
algorithms made those decisions you

00:48:19.490 --> 00:48:19.500
algorithms made those decisions you
 

00:48:19.500 --> 00:48:22.400
algorithms made those decisions you
can't sue an algorithm or at least not

00:48:22.400 --> 00:48:22.410
can't sue an algorithm or at least not
 

00:48:22.410 --> 00:48:24.680
can't sue an algorithm or at least not
yet I think but you should be able to

00:48:24.680 --> 00:48:24.690
yet I think but you should be able to
 

00:48:24.690 --> 00:48:26.780
yet I think but you should be able to
sue the organization has decided to

00:48:26.780 --> 00:48:26.790
sue the organization has decided to
 

00:48:26.790 --> 00:48:31.990
sue the organization has decided to
blindly rely on the algorithm

00:48:31.990 --> 00:48:32.000
 

00:48:32.000 --> 00:48:35.240
explanation explain how the algorithm

00:48:35.240 --> 00:48:35.250
explanation explain how the algorithm
 

00:48:35.250 --> 00:48:36.920
explanation explain how the algorithm
works and how it makes specific

00:48:36.920 --> 00:48:36.930
works and how it makes specific
 

00:48:36.930 --> 00:48:39.829
works and how it makes specific
decisions this is particularly important

00:48:39.829 --> 00:48:39.839
decisions this is particularly important
 

00:48:39.839 --> 00:48:43.010
decisions this is particularly important
in public policy context as I discussed

00:48:43.010 --> 00:48:43.020
in public policy context as I discussed
 

00:48:43.020 --> 00:48:45.290
in public policy context as I discussed
it is not always possible to explain

00:48:45.290 --> 00:48:45.300
it is not always possible to explain
 

00:48:45.300 --> 00:48:47.960
it is not always possible to explain
specific algorithmic decisions in a

00:48:47.960 --> 00:48:47.970
specific algorithmic decisions in a
 

00:48:47.970 --> 00:48:50.240
specific algorithmic decisions in a
meaningful way however there is usually

00:48:50.240 --> 00:48:50.250
meaningful way however there is usually
 

00:48:50.250 --> 00:48:52.970
meaningful way however there is usually
something that can be explained that

00:48:52.970 --> 00:48:52.980
something that can be explained that
 

00:48:52.980 --> 00:48:55.190
something that can be explained that
will provide insights into how the

00:48:55.190 --> 00:48:55.200
will provide insights into how the
 

00:48:55.200 --> 00:48:58.790
will provide insights into how the
algorithm works generally at the very

00:48:58.790 --> 00:48:58.800
algorithm works generally at the very
 

00:48:58.800 --> 00:49:01.609
algorithm works generally at the very
least we can illustrate the algorithms

00:49:01.609 --> 00:49:01.619
least we can illustrate the algorithms
 

00:49:01.619 --> 00:49:05.359
least we can illustrate the algorithms
functionality by example we can show a

00:49:05.359 --> 00:49:05.369
functionality by example we can show a
 

00:49:05.369 --> 00:49:08.210
functionality by example we can show a
large set of inputs and what decision

00:49:08.210 --> 00:49:08.220
large set of inputs and what decision
 

00:49:08.220 --> 00:49:13.210
large set of inputs and what decision
the algorithm makes in response to each

00:49:13.210 --> 00:49:13.220
 

00:49:13.220 --> 00:49:17.660
data provenance be aware of how training

00:49:17.660 --> 00:49:17.670
data provenance be aware of how training
 

00:49:17.670 --> 00:49:20.000
data provenance be aware of how training
data was collected and explore potential

00:49:20.000 --> 00:49:20.010
data was collected and explore potential
 

00:49:20.010 --> 00:49:23.109
data was collected and explore potential
biases induced by that process if

00:49:23.109 --> 00:49:23.119
biases induced by that process if
 

00:49:23.119 --> 00:49:25.760
biases induced by that process if
possible allow public scrutiny of

00:49:25.760 --> 00:49:25.770
possible allow public scrutiny of
 

00:49:25.770 --> 00:49:29.059
possible allow public scrutiny of
training data the data used to train an

00:49:29.059 --> 00:49:29.069
training data the data used to train an
 

00:49:29.069 --> 00:49:31.250
training data the data used to train an
algorithm can make a big difference in

00:49:31.250 --> 00:49:31.260
algorithm can make a big difference in
 

00:49:31.260 --> 00:49:33.530
algorithm can make a big difference in
how an algorithm makes decisions for

00:49:33.530 --> 00:49:33.540
how an algorithm makes decisions for
 

00:49:33.540 --> 00:49:36.920
how an algorithm makes decisions for
example a healthcare algorithm that is

00:49:36.920 --> 00:49:36.930
example a healthcare algorithm that is
 

00:49:36.930 --> 00:49:39.589
example a healthcare algorithm that is
trained using data mostly from men may

00:49:39.589 --> 00:49:39.599
trained using data mostly from men may
 

00:49:39.599 --> 00:49:41.450
trained using data mostly from men may
not be very good at making healthcare

00:49:41.450 --> 00:49:41.460
not be very good at making healthcare
 

00:49:41.460 --> 00:49:44.750
not be very good at making healthcare
decisions for women in my passwords

00:49:44.750 --> 00:49:44.760
decisions for women in my passwords
 

00:49:44.760 --> 00:49:47.120
decisions for women in my passwords
research we have found that a passwords

00:49:47.120 --> 00:49:47.130
research we have found that a passwords
 

00:49:47.130 --> 00:49:49.819
research we have found that a passwords
cracking algorithm trained on passwords

00:49:49.819 --> 00:49:49.829
cracking algorithm trained on passwords
 

00:49:49.829 --> 00:49:52.190
cracking algorithm trained on passwords
that are mostly 8 characters or shorter

00:49:52.190 --> 00:49:52.200
that are mostly 8 characters or shorter
 

00:49:52.200 --> 00:49:54.680
that are mostly 8 characters or shorter
is not very good at cracking longer

00:49:54.680 --> 00:49:54.690
is not very good at cracking longer
 

00:49:54.690 --> 00:49:57.950
is not very good at cracking longer
passwords this is also where biases can

00:49:57.950 --> 00:49:57.960
passwords this is also where biases can
 

00:49:57.960 --> 00:50:01.190
passwords this is also where biases can
get baked in in computer science we have

00:50:01.190 --> 00:50:01.200
get baked in in computer science we have
 

00:50:01.200 --> 00:50:03.770
get baked in in computer science we have
been struggling to recruit more women so

00:50:03.770 --> 00:50:03.780
been struggling to recruit more women so
 

00:50:03.780 --> 00:50:05.569
been struggling to recruit more women so
data sets about computer science

00:50:05.569 --> 00:50:05.579
data sets about computer science
 

00:50:05.579 --> 00:50:08.300
data sets about computer science
students or tech company employees are

00:50:08.300 --> 00:50:08.310
students or tech company employees are
 

00:50:08.310 --> 00:50:11.270
students or tech company employees are
likely to have few women and even fewer

00:50:11.270 --> 00:50:11.280
likely to have few women and even fewer
 

00:50:11.280 --> 00:50:14.359
likely to have few women and even fewer
racial minorities and these data sets

00:50:14.359 --> 00:50:14.369
racial minorities and these data sets
 

00:50:14.369 --> 00:50:16.849
racial minorities and these data sets
are used to train algorithms that will

00:50:16.849 --> 00:50:16.859
are used to train algorithms that will
 

00:50:16.859 --> 00:50:17.630
are used to train algorithms that will
be used to make

00:50:17.630 --> 00:50:17.640
be used to make
 

00:50:17.640 --> 00:50:19.940
be used to make
hiring or college admissions decissions

00:50:19.940 --> 00:50:19.950
hiring or college admissions decissions
 

00:50:19.950 --> 00:50:22.460
hiring or college admissions decissions
they may make decisions biased against

00:50:22.460 --> 00:50:22.470
they may make decisions biased against
 

00:50:22.470 --> 00:50:25.700
they may make decisions biased against
women and minorities if we allow people

00:50:25.700 --> 00:50:25.710
women and minorities if we allow people
 

00:50:25.710 --> 00:50:27.950
women and minorities if we allow people
to examine the data on which an

00:50:27.950 --> 00:50:27.960
to examine the data on which an
 

00:50:27.960 --> 00:50:30.529
to examine the data on which an
algorithm is trained they may be able to

00:50:30.529 --> 00:50:30.539
algorithm is trained they may be able to
 

00:50:30.539 --> 00:50:33.349
algorithm is trained they may be able to
discover biases or deficiencies in the

00:50:33.349 --> 00:50:33.359
discover biases or deficiencies in the
 

00:50:33.359 --> 00:50:35.000
discover biases or deficiencies in the
training data that may reduce the

00:50:35.000 --> 00:50:35.010
training data that may reduce the
 

00:50:35.010 --> 00:50:37.839
training data that may reduce the
quality of the decisions it makes

00:50:37.839 --> 00:50:37.849
quality of the decisions it makes
 

00:50:37.849 --> 00:50:40.220
quality of the decisions it makes
sometimes training data is proprietary

00:50:40.220 --> 00:50:40.230
sometimes training data is proprietary
 

00:50:40.230 --> 00:50:43.640
sometimes training data is proprietary
or contains confidential information so

00:50:43.640 --> 00:50:43.650
or contains confidential information so
 

00:50:43.650 --> 00:50:46.190
or contains confidential information so
it may not be possible to publish it or

00:50:46.190 --> 00:50:46.200
it may not be possible to publish it or
 

00:50:46.200 --> 00:50:48.950
it may not be possible to publish it or
expose it in its entirety but we may

00:50:48.950 --> 00:50:48.960
expose it in its entirety but we may
 

00:50:48.960 --> 00:50:50.509
expose it in its entirety but we may
still be able to provide information

00:50:50.509 --> 00:50:50.519
still be able to provide information
 

00:50:50.519 --> 00:50:53.240
still be able to provide information
about how it was collected and allow

00:50:53.240 --> 00:50:53.250
about how it was collected and allow
 

00:50:53.250 --> 00:50:58.480
about how it was collected and allow
experts to examine the full data set

00:50:58.480 --> 00:50:58.490
 

00:50:58.490 --> 00:51:02.569
auditability record models algorithms

00:51:02.569 --> 00:51:02.579
auditability record models algorithms
 

00:51:02.579 --> 00:51:04.519
auditability record models algorithms
data and decisions so they can be

00:51:04.519 --> 00:51:04.529
data and decisions so they can be
 

00:51:04.529 --> 00:51:07.370
data and decisions so they can be
audited when harm is suspected it is

00:51:07.370 --> 00:51:07.380
audited when harm is suspected it is
 

00:51:07.380 --> 00:51:09.470
audited when harm is suspected it is
important to keep a record of how an

00:51:09.470 --> 00:51:09.480
important to keep a record of how an
 

00:51:09.480 --> 00:51:12.289
important to keep a record of how an
algorithm is created trained and used so

00:51:12.289 --> 00:51:12.299
algorithm is created trained and used so
 

00:51:12.299 --> 00:51:14.029
algorithm is created trained and used so
we have the ability to trace what

00:51:14.029 --> 00:51:14.039
we have the ability to trace what
 

00:51:14.039 --> 00:51:15.950
we have the ability to trace what
happened should we suspect that it

00:51:15.950 --> 00:51:15.960
happened should we suspect that it
 

00:51:15.960 --> 00:51:18.620
happened should we suspect that it
behaved in a harmful way we may not be

00:51:18.620 --> 00:51:18.630
behaved in a harmful way we may not be
 

00:51:18.630 --> 00:51:21.079
behaved in a harmful way we may not be
able to trace exactly what happened in

00:51:21.079 --> 00:51:21.089
able to trace exactly what happened in
 

00:51:21.089 --> 00:51:23.329
able to trace exactly what happened in
an understandable way but maintaining

00:51:23.329 --> 00:51:23.339
an understandable way but maintaining
 

00:51:23.339 --> 00:51:25.460
an understandable way but maintaining
these records can provide important

00:51:25.460 --> 00:51:25.470
these records can provide important
 

00:51:25.470 --> 00:51:30.400
these records can provide important
insights finally validation and testing

00:51:30.400 --> 00:51:30.410
insights finally validation and testing
 

00:51:30.410 --> 00:51:32.720
insights finally validation and testing
routinely test models to determine

00:51:32.720 --> 00:51:32.730
routinely test models to determine
 

00:51:32.730 --> 00:51:35.150
routinely test models to determine
whether they generate discriminatory

00:51:35.150 --> 00:51:35.160
whether they generate discriminatory
 

00:51:35.160 --> 00:51:38.990
whether they generate discriminatory
harm document results and make them

00:51:38.990 --> 00:51:39.000
harm document results and make them
 

00:51:39.000 --> 00:51:42.019
harm document results and make them
public let's not just assume our

00:51:42.019 --> 00:51:42.029
public let's not just assume our
 

00:51:42.029 --> 00:51:44.599
public let's not just assume our
algorithms are unbiased and hope for the

00:51:44.599 --> 00:51:44.609
algorithms are unbiased and hope for the
 

00:51:44.609 --> 00:51:47.450
algorithms are unbiased and hope for the
best we need to test them in on a

00:51:47.450 --> 00:51:47.460
best we need to test them in on a
 

00:51:47.460 --> 00:51:49.789
best we need to test them in on a
regular basis with a diverse set of

00:51:49.789 --> 00:51:49.799
regular basis with a diverse set of
 

00:51:49.799 --> 00:51:51.859
regular basis with a diverse set of
inputs to determine the types of

00:51:51.859 --> 00:51:51.869
inputs to determine the types of
 

00:51:51.869 --> 00:51:54.470
inputs to determine the types of
decisions they make if the decisions are

00:51:54.470 --> 00:51:54.480
decisions they make if the decisions are
 

00:51:54.480 --> 00:51:57.049
decisions they make if the decisions are
discriminatory or harmful we need to

00:51:57.049 --> 00:51:57.059
discriminatory or harmful we need to
 

00:51:57.059 --> 00:52:00.259
discriminatory or harmful we need to
adjust the algorithms to reduce bias by

00:52:00.259 --> 00:52:00.269
adjust the algorithms to reduce bias by
 

00:52:00.269 --> 00:52:03.049
adjust the algorithms to reduce bias by
documenting and publicizing this process

00:52:03.049 --> 00:52:03.059
documenting and publicizing this process
 

00:52:03.059 --> 00:52:05.420
documenting and publicizing this process
we can allow for public input and

00:52:05.420 --> 00:52:05.430
we can allow for public input and
 

00:52:05.430 --> 00:52:09.009
we can allow for public input and
increase the overall trust in the system

00:52:09.009 --> 00:52:09.019
increase the overall trust in the system
 

00:52:09.019 --> 00:52:11.900
increase the overall trust in the system
so now getting back to the topic of this

00:52:11.900 --> 00:52:11.910
so now getting back to the topic of this
 

00:52:11.910 --> 00:52:14.509
so now getting back to the topic of this
session policy and governance should we

00:52:14.509 --> 00:52:14.519
session policy and governance should we
 

00:52:14.519 --> 00:52:17.569
session policy and governance should we
regulate algorithmic transparency can we

00:52:17.569 --> 00:52:17.579
regulate algorithmic transparency can we
 

00:52:17.579 --> 00:52:20.539
regulate algorithmic transparency can we
write this into law the seven ACM

00:52:20.539 --> 00:52:20.549
write this into law the seven ACM
 

00:52:20.549 --> 00:52:22.309
write this into law the seven ACM
principle seemed like a good starting

00:52:22.309 --> 00:52:22.319
principle seemed like a good starting
 

00:52:22.319 --> 00:52:24.440
principle seemed like a good starting
point they've been well thought out by a

00:52:24.440 --> 00:52:24.450
point they've been well thought out by a
 

00:52:24.450 --> 00:52:26.329
point they've been well thought out by a
group of experts considering both

00:52:26.329 --> 00:52:26.339
group of experts considering both
 

00:52:26.339 --> 00:52:28.910
group of experts considering both
technical and ethical issues but we

00:52:28.910 --> 00:52:28.920
technical and ethical issues but we
 

00:52:28.920 --> 00:52:30.890
technical and ethical issues but we
haven't had too much experience putting

00:52:30.890 --> 00:52:30.900
haven't had too much experience putting
 

00:52:30.900 --> 00:52:31.140
haven't had too much experience putting
the

00:52:31.140 --> 00:52:31.150
the
 

00:52:31.150 --> 00:52:33.030
the
to practice yet they were just published

00:52:33.030 --> 00:52:33.040
to practice yet they were just published
 

00:52:33.040 --> 00:52:35.700
to practice yet they were just published
last year it seems to me that we should

00:52:35.700 --> 00:52:35.710
last year it seems to me that we should
 

00:52:35.710 --> 00:52:37.980
last year it seems to me that we should
start by encouraging voluntary adoption

00:52:37.980 --> 00:52:37.990
start by encouraging voluntary adoption
 

00:52:37.990 --> 00:52:40.200
start by encouraging voluntary adoption
of these principles and in gain

00:52:40.200 --> 00:52:40.210
of these principles and in gain
 

00:52:40.210 --> 00:52:41.720
of these principles and in gain
experience with them

00:52:41.720 --> 00:52:41.730
experience with them
 

00:52:41.730 --> 00:52:44.490
experience with them
however voluntary principles may not be

00:52:44.490 --> 00:52:44.500
however voluntary principles may not be
 

00:52:44.500 --> 00:52:46.530
however voluntary principles may not be
sufficient in some domains going forward

00:52:46.530 --> 00:52:46.540
sufficient in some domains going forward
 

00:52:46.540 --> 00:52:48.900
sufficient in some domains going forward
there may not be sufficient incentives

00:52:48.900 --> 00:52:48.910
there may not be sufficient incentives
 

00:52:48.910 --> 00:52:51.990
there may not be sufficient incentives
for organizations to adopt them thus at

00:52:51.990 --> 00:52:52.000
for organizations to adopt them thus at
 

00:52:52.000 --> 00:52:53.640
for organizations to adopt them thus at
some point it may be useful to

00:52:53.640 --> 00:52:53.650
some point it may be useful to
 

00:52:53.650 --> 00:52:55.350
some point it may be useful to
incorporate some or all of these

00:52:55.350 --> 00:52:55.360
incorporate some or all of these
 

00:52:55.360 --> 00:53:01.010
incorporate some or all of these
principles into regulation thank you

00:53:01.010 --> 00:53:01.020
 

00:53:01.020 --> 00:53:03.270
thank you very much Laurie please please

00:53:03.270 --> 00:53:03.280
thank you very much Laurie please please
 

00:53:03.280 --> 00:53:04.680
thank you very much Laurie please please
take a seat we'll have a chance to talk

00:53:04.680 --> 00:53:04.690
take a seat we'll have a chance to talk
 

00:53:04.690 --> 00:53:06.270
take a seat we'll have a chance to talk
about some of those fascinating points

00:53:06.270 --> 00:53:06.280
about some of those fascinating points
 

00:53:06.280 --> 00:53:08.400
about some of those fascinating points
after our next speaker and our next

00:53:08.400 --> 00:53:08.410
after our next speaker and our next
 

00:53:08.410 --> 00:53:10.860
after our next speaker and our next
speaker is Kate Firth Butterfield she

00:53:10.860 --> 00:53:10.870
speaker is Kate Firth Butterfield she
 

00:53:10.870 --> 00:53:12.780
speaker is Kate Firth Butterfield she
has a background in law and has worked

00:53:12.780 --> 00:53:12.790
has a background in law and has worked
 

00:53:12.790 --> 00:53:15.690
has a background in law and has worked
as a barrister and judge in the UK but

00:53:15.690 --> 00:53:15.700
as a barrister and judge in the UK but
 

00:53:15.700 --> 00:53:17.760
as a barrister and judge in the UK but
since September of last year she has

00:53:17.760 --> 00:53:17.770
since September of last year she has
 

00:53:17.770 --> 00:53:19.980
since September of last year she has
been with the World Economic Forum where

00:53:19.980 --> 00:53:19.990
been with the World Economic Forum where
 

00:53:19.990 --> 00:53:23.160
been with the World Economic Forum where
she leads a new project on AI and

00:53:23.160 --> 00:53:23.170
she leads a new project on AI and
 

00:53:23.170 --> 00:53:24.980
she leads a new project on AI and
machine learning

00:53:24.980 --> 00:53:24.990
machine learning
 

00:53:24.990 --> 00:53:32.040
machine learning
please welcome Kate good afternoon

00:53:32.040 --> 00:53:32.050
please welcome Kate good afternoon
 

00:53:32.050 --> 00:53:35.010
please welcome Kate good afternoon
ladies and gentlemen it's a great honor

00:53:35.010 --> 00:53:35.020
ladies and gentlemen it's a great honor
 

00:53:35.020 --> 00:53:39.690
ladies and gentlemen it's a great honor
to be speaking at an AI and ethics

00:53:39.690 --> 00:53:39.700
to be speaking at an AI and ethics
 

00:53:39.700 --> 00:53:41.670
to be speaking at an AI and ethics
conference that's actually sponsored by

00:53:41.670 --> 00:53:41.680
conference that's actually sponsored by
 

00:53:41.680 --> 00:53:47.730
conference that's actually sponsored by
lawyers and in honor of that I'm sort of

00:53:47.730 --> 00:53:47.740
lawyers and in honor of that I'm sort of
 

00:53:47.740 --> 00:53:51.690
lawyers and in honor of that I'm sort of
channeling the former trial lawyer in me

00:53:51.690 --> 00:53:51.700
channeling the former trial lawyer in me
 

00:53:51.700 --> 00:53:56.280
channeling the former trial lawyer in me
by wearing all black and also dispensing

00:53:56.280 --> 00:53:56.290
by wearing all black and also dispensing
 

00:53:56.290 --> 00:53:58.950
by wearing all black and also dispensing
with PowerPoint actually dispensing with

00:53:58.950 --> 00:53:58.960
with PowerPoint actually dispensing with
 

00:53:58.960 --> 00:54:00.720
with PowerPoint actually dispensing with
PowerPoint is in the hope that you'll

00:54:00.720 --> 00:54:00.730
PowerPoint is in the hope that you'll
 

00:54:00.730 --> 00:54:03.900
PowerPoint is in the hope that you'll
listen to me rather than trying to make

00:54:03.900 --> 00:54:03.910
listen to me rather than trying to make
 

00:54:03.910 --> 00:54:08.250
listen to me rather than trying to make
sense or read my slides so I have been

00:54:08.250 --> 00:54:08.260
sense or read my slides so I have been
 

00:54:08.260 --> 00:54:10.530
sense or read my slides so I have been
asked to speak about the challenges of

00:54:10.530 --> 00:54:10.540
asked to speak about the challenges of
 

00:54:10.540 --> 00:54:14.790
asked to speak about the challenges of
good governance in AI in other words how

00:54:14.790 --> 00:54:14.800
good governance in AI in other words how
 

00:54:14.800 --> 00:54:16.890
good governance in AI in other words how
we think about shaping this technology

00:54:16.890 --> 00:54:16.900
we think about shaping this technology
 

00:54:16.900 --> 00:54:20.780
we think about shaping this technology
for good across multiple stakeholders

00:54:20.780 --> 00:54:20.790
for good across multiple stakeholders
 

00:54:20.790 --> 00:54:25.320
for good across multiple stakeholders
government business ideas NGOs civil

00:54:25.320 --> 00:54:25.330
government business ideas NGOs civil
 

00:54:25.330 --> 00:54:28.770
government business ideas NGOs civil
society and academia because this isn't

00:54:28.770 --> 00:54:28.780
society and academia because this isn't
 

00:54:28.780 --> 00:54:32.430
society and academia because this isn't
as simple as just trust this is more

00:54:32.430 --> 00:54:32.440
as simple as just trust this is more
 

00:54:32.440 --> 00:54:35.840
as simple as just trust this is more
complex because the technology itself is

00:54:35.840 --> 00:54:35.850
complex because the technology itself is
 

00:54:35.850 --> 00:54:40.170
complex because the technology itself is
very fast changing all the time and is

00:54:40.170 --> 00:54:40.180
very fast changing all the time and is
 

00:54:40.180 --> 00:54:43.890
very fast changing all the time and is
complex as well so we need to be much

00:54:43.890 --> 00:54:43.900
complex as well so we need to be much
 

00:54:43.900 --> 00:54:44.520
complex as well so we need to be much
more

00:54:44.520 --> 00:54:44.530
more
 

00:54:44.530 --> 00:54:46.350
more
jaelyn the way that we think about these

00:54:46.350 --> 00:54:46.360
jaelyn the way that we think about these
 

00:54:46.360 --> 00:54:50.160
jaelyn the way that we think about these
governance mechanisms and I guess that

00:54:50.160 --> 00:54:50.170
governance mechanisms and I guess that
 

00:54:50.170 --> 00:54:54.080
governance mechanisms and I guess that
like like me you've been thinking that

00:54:54.080 --> 00:54:54.090
like like me you've been thinking that
 

00:54:54.090 --> 00:54:56.760
like like me you've been thinking that
actually the future is not shaped by

00:54:56.760 --> 00:54:56.770
actually the future is not shaped by
 

00:54:56.770 --> 00:54:59.720
actually the future is not shaped by
technology it's shaped by us but

00:54:59.720 --> 00:54:59.730
technology it's shaped by us but
 

00:54:59.730 --> 00:55:02.340
technology it's shaped by us but
increasingly we're sort of in this

00:55:02.340 --> 00:55:02.350
increasingly we're sort of in this
 

00:55:02.350 --> 00:55:04.440
increasingly we're sort of in this
feeling that actually the technology is

00:55:04.440 --> 00:55:04.450
feeling that actually the technology is
 

00:55:04.450 --> 00:55:07.430
feeling that actually the technology is
out there and we're reacting to things

00:55:07.430 --> 00:55:07.440
out there and we're reacting to things
 

00:55:07.440 --> 00:55:12.330
out there and we're reacting to things
rather than actually directing them so

00:55:12.330 --> 00:55:12.340
rather than actually directing them so
 

00:55:12.340 --> 00:55:14.730
rather than actually directing them so
my purpose in this very short speech is

00:55:14.730 --> 00:55:14.740
my purpose in this very short speech is
 

00:55:14.740 --> 00:55:17.150
my purpose in this very short speech is
really to talk about a couple of things

00:55:17.150 --> 00:55:17.160
really to talk about a couple of things
 

00:55:17.160 --> 00:55:20.430
really to talk about a couple of things
around traditional governance mechanisms

00:55:20.430 --> 00:55:20.440
around traditional governance mechanisms
 

00:55:20.440 --> 00:55:23.700
around traditional governance mechanisms
and then to propose some what I call

00:55:23.700 --> 00:55:23.710
and then to propose some what I call
 

00:55:23.710 --> 00:55:27.540
and then to propose some what I call
additive agile forms of governance at

00:55:27.540 --> 00:55:27.550
additive agile forms of governance at
 

00:55:27.550 --> 00:55:29.520
additive agile forms of governance at
least and I'm going to illustrate that

00:55:29.520 --> 00:55:29.530
least and I'm going to illustrate that
 

00:55:29.530 --> 00:55:30.990
least and I'm going to illustrate that
by some of the things that we're

00:55:30.990 --> 00:55:31.000
by some of the things that we're
 

00:55:31.000 --> 00:55:32.730
by some of the things that we're
actually doing at the World Economic

00:55:32.730 --> 00:55:32.740
actually doing at the World Economic
 

00:55:32.740 --> 00:55:37.200
actually doing at the World Economic
Forum out in San Francisco say first of

00:55:37.200 --> 00:55:37.210
Forum out in San Francisco say first of
 

00:55:37.210 --> 00:55:39.780
Forum out in San Francisco say first of
all thinking about there's two big

00:55:39.780 --> 00:55:39.790
all thinking about there's two big
 

00:55:39.790 --> 00:55:42.440
all thinking about there's two big
things of traditional governance

00:55:42.440 --> 00:55:42.450
things of traditional governance
 

00:55:42.450 --> 00:55:47.130
things of traditional governance
legislation or regulation well with this

00:55:47.130 --> 00:55:47.140
legislation or regulation well with this
 

00:55:47.140 --> 00:55:50.400
legislation or regulation well with this
technology moving so fast we're very

00:55:50.400 --> 00:55:50.410
technology moving so fast we're very
 

00:55:50.410 --> 00:55:52.830
technology moving so fast we're very
often in what could be called the too

00:55:52.830 --> 00:55:52.840
often in what could be called the too
 

00:55:52.840 --> 00:55:55.380
often in what could be called the too
late zone saved by the time we've

00:55:55.380 --> 00:55:55.390
late zone saved by the time we've
 

00:55:55.390 --> 00:55:59.220
late zone saved by the time we've
legislated the horses not is four or

00:55:59.220 --> 00:55:59.230
legislated the horses not is four or
 

00:55:59.230 --> 00:56:02.070
legislated the horses not is four or
five fields down there down the road and

00:56:02.070 --> 00:56:02.080
five fields down there down the road and
 

00:56:02.080 --> 00:56:05.970
five fields down there down the road and
lane and so once we undoubtedly will

00:56:05.970 --> 00:56:05.980
lane and so once we undoubtedly will
 

00:56:05.980 --> 00:56:08.820
lane and so once we undoubtedly will
need some regulations on legislation in

00:56:08.820 --> 00:56:08.830
need some regulations on legislation in
 

00:56:08.830 --> 00:56:13.020
need some regulations on legislation in
this area we need to be thoughtful about

00:56:13.020 --> 00:56:13.030
this area we need to be thoughtful about
 

00:56:13.030 --> 00:56:17.250
this area we need to be thoughtful about
what we tackle additionally we can't

00:56:17.250 --> 00:56:17.260
what we tackle additionally we can't
 

00:56:17.260 --> 00:56:21.180
what we tackle additionally we can't
legislate what we can't define and as

00:56:21.180 --> 00:56:21.190
legislate what we can't define and as
 

00:56:21.190 --> 00:56:23.430
legislate what we can't define and as
both lawyers and scientists we all know

00:56:23.430 --> 00:56:23.440
both lawyers and scientists we all know
 

00:56:23.440 --> 00:56:26.790
both lawyers and scientists we all know
that the definitions in this space are

00:56:26.790 --> 00:56:26.800
that the definitions in this space are
 

00:56:26.800 --> 00:56:29.550
that the definitions in this space are
really difficult and so it looks as if

00:56:29.550 --> 00:56:29.560
really difficult and so it looks as if
 

00:56:29.560 --> 00:56:32.490
really difficult and so it looks as if
maybe we'll be legendary legislating a

00:56:32.490 --> 00:56:32.500
maybe we'll be legendary legislating a
 

00:56:32.500 --> 00:56:35.190
maybe we'll be legendary legislating a
in terms of product rather than the

00:56:35.190 --> 00:56:35.200
in terms of product rather than the
 

00:56:35.200 --> 00:56:36.840
in terms of product rather than the
artificial intelligence or machine

00:56:36.840 --> 00:56:36.850
artificial intelligence or machine
 

00:56:36.850 --> 00:56:39.510
artificial intelligence or machine
learning itself but that insert in

00:56:39.510 --> 00:56:39.520
learning itself but that insert in
 

00:56:39.520 --> 00:56:42.270
learning itself but that insert in
itself leaves problems and if you have

00:56:42.270 --> 00:56:42.280
itself leaves problems and if you have
 

00:56:42.280 --> 00:56:44.880
itself leaves problems and if you have
bad legislation what does that lead to

00:56:44.880 --> 00:56:44.890
bad legislation what does that lead to
 

00:56:44.890 --> 00:56:49.280
bad legislation what does that lead to
well that leads in our jurisdiction to

00:56:49.280 --> 00:56:49.290
well that leads in our jurisdiction to
 

00:56:49.290 --> 00:56:54.300
well that leads in our jurisdiction to
lawyers debating in court what was meant

00:56:54.300 --> 00:56:54.310
lawyers debating in court what was meant
 

00:56:54.310 --> 00:56:56.320
lawyers debating in court what was meant
by the legislation

00:56:56.320 --> 00:56:56.330
by the legislation
 

00:56:56.330 --> 00:56:59.050
by the legislation
say let's think about the scenario a

00:56:59.050 --> 00:56:59.060
say let's think about the scenario a
 

00:56:59.060 --> 00:57:03.670
say let's think about the scenario a
moment we have perhaps in a fairly lowly

00:57:03.670 --> 00:57:03.680
moment we have perhaps in a fairly lowly
 

00:57:03.680 --> 00:57:06.520
moment we have perhaps in a fairly lowly
Court two lawyers who don't really

00:57:06.520 --> 00:57:06.530
Court two lawyers who don't really
 

00:57:06.530 --> 00:57:08.370
Court two lawyers who don't really
understand the legislative the

00:57:08.370 --> 00:57:08.380
understand the legislative the
 

00:57:08.380 --> 00:57:12.750
understand the legislative the
legislation or the technology very well

00:57:12.750 --> 00:57:12.760
legislation or the technology very well
 

00:57:12.760 --> 00:57:16.180
legislation or the technology very well
trying to convince a judge to make a

00:57:16.180 --> 00:57:16.190
trying to convince a judge to make a
 

00:57:16.190 --> 00:57:18.520
trying to convince a judge to make a
decision and that judge doesn't

00:57:18.520 --> 00:57:18.530
decision and that judge doesn't
 

00:57:18.530 --> 00:57:21.160
decision and that judge doesn't
necessarily understand the technology

00:57:21.160 --> 00:57:21.170
necessarily understand the technology
 

00:57:21.170 --> 00:57:24.400
necessarily understand the technology
very well and then you have experts who

00:57:24.400 --> 00:57:24.410
very well and then you have experts who
 

00:57:24.410 --> 00:57:28.810
very well and then you have experts who
may or may not be good experts this is

00:57:28.810 --> 00:57:28.820
may or may not be good experts this is
 

00:57:28.820 --> 00:57:30.850
may or may not be good experts this is
not necessarily the way I would submit

00:57:30.850 --> 00:57:30.860
not necessarily the way I would submit
 

00:57:30.860 --> 00:57:33.820
not necessarily the way I would submit
that we want to construct our legal

00:57:33.820 --> 00:57:33.830
that we want to construct our legal
 

00:57:33.830 --> 00:57:37.240
that we want to construct our legal
system and our laws in this area moving

00:57:37.240 --> 00:57:37.250
system and our laws in this area moving
 

00:57:37.250 --> 00:57:42.070
system and our laws in this area moving
forwards so what are the additives that

00:57:42.070 --> 00:57:42.080
forwards so what are the additives that
 

00:57:42.080 --> 00:57:43.570
forwards so what are the additives that
we can bring to this traditional

00:57:43.570 --> 00:57:43.580
we can bring to this traditional
 

00:57:43.580 --> 00:57:46.390
we can bring to this traditional
government system well I would say that

00:57:46.390 --> 00:57:46.400
government system well I would say that
 

00:57:46.400 --> 00:57:48.010
government system well I would say that
leadership in this area needs to come

00:57:48.010 --> 00:57:48.020
leadership in this area needs to come
 

00:57:48.020 --> 00:57:51.280
leadership in this area needs to come
from two different perspectives first of

00:57:51.280 --> 00:57:51.290
from two different perspectives first of
 

00:57:51.290 --> 00:57:53.890
from two different perspectives first of
all we need to ask ourselves not only

00:57:53.890 --> 00:57:53.900
all we need to ask ourselves not only
 

00:57:53.900 --> 00:57:57.100
all we need to ask ourselves not only
what we govern that definitions piece

00:57:57.100 --> 00:57:57.110
what we govern that definitions piece
 

00:57:57.110 --> 00:58:01.990
what we govern that definitions piece
but I will say the question why and how

00:58:01.990 --> 00:58:02.000
but I will say the question why and how
 

00:58:02.000 --> 00:58:04.090
but I will say the question why and how
we're going to do this which may not be

00:58:04.090 --> 00:58:04.100
we're going to do this which may not be
 

00:58:04.100 --> 00:58:06.760
we're going to do this which may not be
that traditional mechanism but rather

00:58:06.760 --> 00:58:06.770
that traditional mechanism but rather
 

00:58:06.770 --> 00:58:11.080
that traditional mechanism but rather
agile governance roots and as we at the

00:58:11.080 --> 00:58:11.090
agile governance roots and as we at the
 

00:58:11.090 --> 00:58:14.830
agile governance roots and as we at the
forum would say and I will read you the

00:58:14.830 --> 00:58:14.840
forum would say and I will read you the
 

00:58:14.840 --> 00:58:20.760
forum would say and I will read you the
definition agile governance is adaptive

00:58:20.760 --> 00:58:20.770
definition agile governance is adaptive
 

00:58:20.770 --> 00:58:25.740
definition agile governance is adaptive
human centered inclusive sustainable

00:58:25.740 --> 00:58:25.750
human centered inclusive sustainable
 

00:58:25.750 --> 00:58:30.120
human centered inclusive sustainable
policy policy development which is not

00:58:30.120 --> 00:58:30.130
policy policy development which is not
 

00:58:30.130 --> 00:58:34.290
policy policy development which is not
limited to government's but rather is an

00:58:34.290 --> 00:58:34.300
limited to government's but rather is an
 

00:58:34.300 --> 00:58:39.540
limited to government's but rather is an
increasingly multi-stakeholder effort

00:58:39.540 --> 00:58:39.550
increasingly multi-stakeholder effort
 

00:58:39.550 --> 00:58:43.510
increasingly multi-stakeholder effort
it's a continued readiness to rapidly

00:58:43.510 --> 00:58:43.520
it's a continued readiness to rapidly
 

00:58:43.520 --> 00:58:47.440
it's a continued readiness to rapidly
navigate change and to learn from change

00:58:47.440 --> 00:58:47.450
navigate change and to learn from change
 

00:58:47.450 --> 00:58:53.230
navigate change and to learn from change
and in reactively embrace change whilst

00:58:53.230 --> 00:58:53.240
and in reactively embrace change whilst
 

00:58:53.240 --> 00:58:56.550
and in reactively embrace change whilst
also contributing to actual or perceived

00:58:56.550 --> 00:58:56.560
also contributing to actual or perceived
 

00:58:56.560 --> 00:59:02.680
also contributing to actual or perceived
user value so if I can adjust unpick

00:59:02.680 --> 00:59:02.690
user value so if I can adjust unpick
 

00:59:02.690 --> 00:59:05.740
user value so if I can adjust unpick
that for a little bit some examples of

00:59:05.740 --> 00:59:05.750
that for a little bit some examples of
 

00:59:05.750 --> 00:59:08.410
that for a little bit some examples of
what I'm talking about first of all the

00:59:08.410 --> 00:59:08.420
what I'm talking about first of all the
 

00:59:08.420 --> 00:59:10.030
what I'm talking about first of all the
development of and you

00:59:10.030 --> 00:59:10.040
development of and you
 

00:59:10.040 --> 00:59:12.250
development of and you
of standards safe standards from the I

00:59:12.250 --> 00:59:12.260
of standards safe standards from the I
 

00:59:12.260 --> 00:59:14.950
of standards safe standards from the I
Triple E the forums aim protocols

00:59:14.950 --> 00:59:14.960
Triple E the forums aim protocols
 

00:59:14.960 --> 00:59:18.100
Triple E the forums aim protocols
recently on the use of the Internet of

00:59:18.100 --> 00:59:18.110
recently on the use of the Internet of
 

00:59:18.110 --> 00:59:20.860
recently on the use of the Internet of
Things and something Laurie talked about

00:59:20.860 --> 00:59:20.870
Things and something Laurie talked about
 

00:59:20.870 --> 00:59:24.520
Things and something Laurie talked about
earlier the ACM rules the emergence of

00:59:24.520 --> 00:59:24.530
earlier the ACM rules the emergence of
 

00:59:24.530 --> 00:59:27.820
earlier the ACM rules the emergence of
social norms which constrain or endorse

00:59:27.820 --> 00:59:27.830
social norms which constrain or endorse
 

00:59:27.830 --> 00:59:32.520
social norms which constrain or endorse
the technology private incentive schemes

00:59:32.520 --> 00:59:32.530
the technology private incentive schemes
 

00:59:32.530 --> 00:59:34.510
the technology private incentive schemes
Eric talked yesterday about

00:59:34.510 --> 00:59:34.520
Eric talked yesterday about
 

00:59:34.520 --> 00:59:38.950
Eric talked yesterday about
certification oversight by professional

00:59:38.950 --> 00:59:38.960
certification oversight by professional
 

00:59:38.960 --> 00:59:43.120
certification oversight by professional
bodies industria greement sand policies

00:59:43.120 --> 00:59:43.130
bodies industria greement sand policies
 

00:59:43.130 --> 00:59:45.930
bodies industria greement sand policies
which organizations apply voluntarily

00:59:45.930 --> 00:59:45.940
which organizations apply voluntarily
 

00:59:45.940 --> 00:59:49.420
which organizations apply voluntarily
for example like ether also something

00:59:49.420 --> 00:59:49.430
for example like ether also something
 

00:59:49.430 --> 00:59:52.840
for example like ether also something
Eric talked about yes Jake or by

00:59:52.840 --> 00:59:52.850
Eric talked about yes Jake or by
 

00:59:52.850 --> 00:59:55.830
Eric talked about yes Jake or by
contract within their relationships with

00:59:55.830 --> 00:59:55.840
contract within their relationships with
 

00:59:55.840 --> 00:59:58.840
contract within their relationships with
competitors suppliers partners and

00:59:58.840 --> 00:59:58.850
competitors suppliers partners and
 

00:59:58.850 --> 01:00:02.500
competitors suppliers partners and
customers also we should think about

01:00:02.500 --> 01:00:02.510
customers also we should think about
 

01:00:02.510 --> 01:00:04.630
customers also we should think about
whether there is actually existing law

01:00:04.630 --> 01:00:04.640
whether there is actually existing law
 

01:00:04.640 --> 01:00:08.020
whether there is actually existing law
that deals with some of these things so

01:00:08.020 --> 01:00:08.030
that deals with some of these things so
 

01:00:08.030 --> 01:00:10.320
that deals with some of these things so
for example company law in Europe

01:00:10.320 --> 01:00:10.330
for example company law in Europe
 

01:00:10.330 --> 01:00:13.060
for example company law in Europe
requires companies not just to look at

01:00:13.060 --> 01:00:13.070
requires companies not just to look at
 

01:00:13.070 --> 01:00:16.120
requires companies not just to look at
the their profit and loss sheets but it

01:00:16.120 --> 01:00:16.130
the their profit and loss sheets but it
 

01:00:16.130 --> 01:00:19.090
the their profit and loss sheets but it
will say at the impact their company is

01:00:19.090 --> 01:00:19.100
will say at the impact their company is
 

01:00:19.100 --> 01:00:22.150
will say at the impact their company is
having on stake holders why in the wider

01:00:22.150 --> 01:00:22.160
having on stake holders why in the wider
 

01:00:22.160 --> 01:00:26.940
having on stake holders why in the wider
world also Germany recently used an old

01:00:26.940 --> 01:00:26.950
world also Germany recently used an old
 

01:00:26.950 --> 01:00:30.840
world also Germany recently used an old
telecommunications act to ban AI toys

01:00:30.840 --> 01:00:30.850
telecommunications act to ban AI toys
 

01:00:30.850 --> 01:00:33.490
telecommunications act to ban AI toys
because of the concerns are around

01:00:33.490 --> 01:00:33.500
because of the concerns are around
 

01:00:33.500 --> 01:00:38.170
because of the concerns are around
hacking and values-based legislation

01:00:38.170 --> 01:00:38.180
hacking and values-based legislation
 

01:00:38.180 --> 01:00:40.360
hacking and values-based legislation
that's already on the books we know what

01:00:40.360 --> 01:00:40.370
that's already on the books we know what
 

01:00:40.370 --> 01:00:43.560
that's already on the books we know what
Europe thinks for good or ill about

01:00:43.560 --> 01:00:43.570
Europe thinks for good or ill about
 

01:00:43.570 --> 01:00:47.490
Europe thinks for good or ill about
privacy because it passed the GD P R and

01:00:47.490 --> 01:00:47.500
privacy because it passed the GD P R and
 

01:00:47.500 --> 01:00:49.660
privacy because it passed the GD P R and
therefore it said privacy is so

01:00:49.660 --> 01:00:49.670
therefore it said privacy is so
 

01:00:49.670 --> 01:00:52.540
therefore it said privacy is so
important that we want it designed in at

01:00:52.540 --> 01:00:52.550
important that we want it designed in at
 

01:00:52.550 --> 01:00:57.040
important that we want it designed in at
the start so agile governance it

01:00:57.040 --> 01:00:57.050
the start so agile governance it
 

01:00:57.050 --> 01:00:58.740
the start so agile governance it
includes not only the traditional

01:00:58.740 --> 01:00:58.750
includes not only the traditional
 

01:00:58.750 --> 01:01:02.410
includes not only the traditional
lawmakers international national state

01:01:02.410 --> 01:01:02.420
lawmakers international national state
 

01:01:02.420 --> 01:01:05.800
lawmakers international national state
and local governments and then the

01:01:05.800 --> 01:01:05.810
and local governments and then the
 

01:01:05.810 --> 01:01:10.020
and local governments and then the
courts interpreting that but also

01:01:10.020 --> 01:01:10.030
courts interpreting that but also
 

01:01:10.030 --> 01:01:15.730
courts interpreting that but also
industry civil society and academia say

01:01:15.730 --> 01:01:15.740
industry civil society and academia say
 

01:01:15.740 --> 01:01:18.460
industry civil society and academia say
picking up on all of those pieces of

01:01:18.460 --> 01:01:18.470
picking up on all of those pieces of
 

01:01:18.470 --> 01:01:20.380
picking up on all of those pieces of
agile governance what the former has

01:01:20.380 --> 01:01:20.390
agile governance what the former has
 

01:01:20.390 --> 01:01:21.530
agile governance what the former has
done

01:01:21.530 --> 01:01:21.540
done
 

01:01:21.540 --> 01:01:25.010
done
create what we call a do tank to work in

01:01:25.010 --> 01:01:25.020
create what we call a do tank to work in
 

01:01:25.020 --> 01:01:27.410
create what we call a do tank to work in
this multi-stakeholder environment on

01:01:27.410 --> 01:01:27.420
this multi-stakeholder environment on
 

01:01:27.420 --> 01:01:30.760
this multi-stakeholder environment on
the governance not only of AI but also

01:01:30.760 --> 01:01:30.770
the governance not only of AI but also
 

01:01:30.770 --> 01:01:34.310
the governance not only of AI but also
blockchain IOT autonomous vehicles

01:01:34.310 --> 01:01:34.320
blockchain IOT autonomous vehicles
 

01:01:34.320 --> 01:01:37.880
blockchain IOT autonomous vehicles
drones precision medicine ecommerce and

01:01:37.880 --> 01:01:37.890
drones precision medicine ecommerce and
 

01:01:37.890 --> 01:01:41.450
drones precision medicine ecommerce and
cross-border data phase and what we call

01:01:41.450 --> 01:01:41.460
cross-border data phase and what we call
 

01:01:41.460 --> 01:01:45.380
cross-border data phase and what we call
in for IR technologies around the

01:01:45.380 --> 01:01:45.390
in for IR technologies around the
 

01:01:45.390 --> 01:01:49.550
in for IR technologies around the
environment we work on projects each one

01:01:49.550 --> 01:01:49.560
environment we work on projects each one
 

01:01:49.560 --> 01:01:52.550
environment we work on projects each one
is 18 months long and we have a

01:01:52.550 --> 01:01:52.560
is 18 months long and we have a
 

01:01:52.560 --> 01:01:54.590
is 18 months long and we have a
commitment from the partners that were

01:01:54.590 --> 01:01:54.600
commitment from the partners that were
 

01:01:54.600 --> 01:01:57.650
commitment from the partners that were
working with to actually pilot these

01:01:57.650 --> 01:01:57.660
working with to actually pilot these
 

01:01:57.660 --> 01:02:01.490
working with to actually pilot these
projects in the last six months of that

01:02:01.490 --> 01:02:01.500
projects in the last six months of that
 

01:02:01.500 --> 01:02:06.550
projects in the last six months of that
18-month period and all projects have to

01:02:06.550 --> 01:02:06.560
18-month period and all projects have to
 

01:02:06.560 --> 01:02:10.520
18-month period and all projects have to
have our cross-cutting themes of ethics

01:02:10.520 --> 01:02:10.530
have our cross-cutting themes of ethics
 

01:02:10.530 --> 01:02:12.890
have our cross-cutting themes of ethics
and values social inclusion and putting

01:02:12.890 --> 01:02:12.900
and values social inclusion and putting
 

01:02:12.900 --> 01:02:15.760
and values social inclusion and putting
the human in the middle of this thinking

01:02:15.760 --> 01:02:15.770
the human in the middle of this thinking
 

01:02:15.770 --> 01:02:18.860
the human in the middle of this thinking
as we designed for the benefit of

01:02:18.860 --> 01:02:18.870
as we designed for the benefit of
 

01:02:18.870 --> 01:02:22.790
as we designed for the benefit of
humanity as a whole say this multi

01:02:22.790 --> 01:02:22.800
humanity as a whole say this multi
 

01:02:22.800 --> 01:02:26.120
humanity as a whole say this multi
stakeholder group also includes fellows

01:02:26.120 --> 01:02:26.130
stakeholder group also includes fellows
 

01:02:26.130 --> 01:02:29.180
stakeholder group also includes fellows
working on our projects from all of

01:02:29.180 --> 01:02:29.190
working on our projects from all of
 

01:02:29.190 --> 01:02:31.040
working on our projects from all of
these different multi-stakeholders i've

01:02:31.040 --> 01:02:31.050
these different multi-stakeholders i've
 

01:02:31.050 --> 01:02:33.560
these different multi-stakeholders i've
spoken about and so we're actually

01:02:33.560 --> 01:02:33.570
spoken about and so we're actually
 

01:02:33.570 --> 01:02:37.880
spoken about and so we're actually
delighted that CMU will be sending us

01:02:37.880 --> 01:02:37.890
delighted that CMU will be sending us
 

01:02:37.890 --> 01:02:42.110
delighted that CMU will be sending us
fellows to work on these projects with

01:02:42.110 --> 01:02:42.120
fellows to work on these projects with
 

01:02:42.120 --> 01:02:44.000
fellows to work on these projects with
us and so we're really looking forward

01:02:44.000 --> 01:02:44.010
us and so we're really looking forward
 

01:02:44.010 --> 01:02:48.620
us and so we're really looking forward
to welcoming them in San Francisco and I

01:02:48.620 --> 01:02:48.630
to welcoming them in San Francisco and I
 

01:02:48.630 --> 01:02:51.250
to welcoming them in San Francisco and I
think so that you can really understand

01:02:51.250 --> 01:02:51.260
think so that you can really understand
 

01:02:51.260 --> 01:02:54.080
think so that you can really understand
the background of what I'm talking about

01:02:54.080 --> 01:02:54.090
the background of what I'm talking about
 

01:02:54.090 --> 01:02:56.900
the background of what I'm talking about
in terms of agile governments I just

01:02:56.900 --> 01:02:56.910
in terms of agile governments I just
 

01:02:56.910 --> 01:02:59.900
in terms of agile governments I just
want to give you four exemplars from the

01:02:59.900 --> 01:02:59.910
want to give you four exemplars from the
 

01:02:59.910 --> 01:03:02.690
want to give you four exemplars from the
projects that we are actually doing in

01:03:02.690 --> 01:03:02.700
projects that we are actually doing in
 

01:03:02.700 --> 01:03:07.550
projects that we are actually doing in
San Francisco so the first one is a

01:03:07.550 --> 01:03:07.560
San Francisco so the first one is a
 

01:03:07.560 --> 01:03:09.290
San Francisco so the first one is a
number of speakers have said well we

01:03:09.290 --> 01:03:09.300
number of speakers have said well we
 

01:03:09.300 --> 01:03:11.690
number of speakers have said well we
need to stop interesting science

01:03:11.690 --> 01:03:11.700
need to stop interesting science
 

01:03:11.700 --> 01:03:14.240
need to stop interesting science
projects without thinking about what the

01:03:14.240 --> 01:03:14.250
projects without thinking about what the
 

01:03:14.250 --> 01:03:17.750
projects without thinking about what the
future looks like and those of you who

01:03:17.750 --> 01:03:17.760
future looks like and those of you who
 

01:03:17.760 --> 01:03:20.810
future looks like and those of you who
read The Guardian article an interview

01:03:20.810 --> 01:03:20.820
read The Guardian article an interview
 

01:03:20.820 --> 01:03:22.820
read The Guardian article an interview
with Christopher Wiley will have noted

01:03:22.820 --> 01:03:22.830
with Christopher Wiley will have noted
 

01:03:22.830 --> 01:03:26.480
with Christopher Wiley will have noted
that it said he thrilled to the

01:03:26.480 --> 01:03:26.490
that it said he thrilled to the
 

01:03:26.490 --> 01:03:30.110
that it said he thrilled to the
intellectual possibilities of it he

01:03:30.110 --> 01:03:30.120
intellectual possibilities of it he
 

01:03:30.120 --> 01:03:32.599
intellectual possibilities of it he
didn't think of the consequences

01:03:32.599 --> 01:03:32.609
didn't think of the consequences
 

01:03:32.609 --> 01:03:37.880
didn't think of the consequences
and so with Ella and Jennifer Keating we

01:03:37.880 --> 01:03:37.890
and so with Ella and Jennifer Keating we
 

01:03:37.890 --> 01:03:43.279
and so with Ella and Jennifer Keating we
have a project co-creating a way of

01:03:43.279 --> 01:03:43.289
have a project co-creating a way of
 

01:03:43.289 --> 01:03:46.069
have a project co-creating a way of
helping to increase the number of a ice

01:03:46.069 --> 01:03:46.079
helping to increase the number of a ice
 

01:03:46.079 --> 01:03:49.370
helping to increase the number of a ice
young AI scientists so undergraduates

01:03:49.370 --> 01:03:49.380
young AI scientists so undergraduates
 

01:03:49.380 --> 01:03:52.819
young AI scientists so undergraduates
graduates and post graduates to really

01:03:52.819 --> 01:03:52.829
graduates and post graduates to really
 

01:03:52.829 --> 01:03:55.969
graduates and post graduates to really
think through these consequences and the

01:03:55.969 --> 01:03:55.979
think through these consequences and the
 

01:03:55.979 --> 01:03:58.449
think through these consequences and the
way the project works is very simple

01:03:58.449 --> 01:03:58.459
way the project works is very simple
 

01:03:58.459 --> 01:04:00.769
way the project works is very simple
people like Ella who have already

01:04:00.769 --> 01:04:00.779
people like Ella who have already
 

01:04:00.779 --> 01:04:02.749
people like Ella who have already
thought through these things and created

01:04:02.749 --> 01:04:02.759
thought through these things and created
 

01:04:02.759 --> 01:04:05.630
thought through these things and created
curricula will deposit their curricula

01:04:05.630 --> 01:04:05.640
curricula will deposit their curricula
 

01:04:05.640 --> 01:04:08.209
curricula will deposit their curricula
into a repository on the World Economic

01:04:08.209 --> 01:04:08.219
into a repository on the World Economic
 

01:04:08.219 --> 01:04:12.650
into a repository on the World Economic
Forum's website and we will ask

01:04:12.650 --> 01:04:12.660
Forum's website and we will ask
 

01:04:12.660 --> 01:04:15.079
Forum's website and we will ask
professors from around the world who are

01:04:15.079 --> 01:04:15.089
professors from around the world who are
 

01:04:15.089 --> 01:04:19.189
professors from around the world who are
yet to teach this to you take vote that

01:04:19.189 --> 01:04:19.199
yet to teach this to you take vote that
 

01:04:19.199 --> 01:04:20.120
yet to teach this to you take vote that
curricula

01:04:20.120 --> 01:04:20.130
curricula
 

01:04:20.130 --> 01:04:24.219
curricula
use it customize it put their own

01:04:24.219 --> 01:04:24.229
use it customize it put their own
 

01:04:24.229 --> 01:04:27.259
use it customize it put their own
cultural values into it and then all

01:04:27.259 --> 01:04:27.269
cultural values into it and then all
 

01:04:27.269 --> 01:04:29.150
cultural values into it and then all
that we ask is that you put it back into

01:04:29.150 --> 01:04:29.160
that we ask is that you put it back into
 

01:04:29.160 --> 01:04:32.620
that we ask is that you put it back into
the repository for other people to use

01:04:32.620 --> 01:04:32.630
the repository for other people to use
 

01:04:32.630 --> 01:04:34.880
the repository for other people to use
so you might say well how is that

01:04:34.880 --> 01:04:34.890
so you might say well how is that
 

01:04:34.890 --> 01:04:37.759
so you might say well how is that
governance well actually you know if we

01:04:37.759 --> 01:04:37.769
governance well actually you know if we
 

01:04:37.769 --> 01:04:43.329
governance well actually you know if we
can influence ethical curricula in

01:04:43.329 --> 01:04:43.339
can influence ethical curricula in
 

01:04:43.339 --> 01:04:46.009
can influence ethical curricula in
universities all around the world then

01:04:46.009 --> 01:04:46.019
universities all around the world then
 

01:04:46.019 --> 01:04:48.079
universities all around the world then
there'll be many young a AI scientists

01:04:48.079 --> 01:04:48.089
there'll be many young a AI scientists
 

01:04:48.089 --> 01:04:50.719
there'll be many young a AI scientists
globally who understand the importance

01:04:50.719 --> 01:04:50.729
globally who understand the importance
 

01:04:50.729 --> 01:04:56.359
globally who understand the importance
of ethical thinking a second one is

01:04:56.359 --> 01:04:56.369
of ethical thinking a second one is
 

01:04:56.369 --> 01:05:00.199
of ethical thinking a second one is
obviously as robotics and AI systems

01:05:00.199 --> 01:05:00.209
obviously as robotics and AI systems
 

01:05:00.209 --> 01:05:03.199
obviously as robotics and AI systems
become more powerful and more capable

01:05:03.199 --> 01:05:03.209
become more powerful and more capable
 

01:05:03.209 --> 01:05:05.569
become more powerful and more capable
than the decision making process for

01:05:05.569 --> 01:05:05.579
than the decision making process for
 

01:05:05.579 --> 01:05:08.539
than the decision making process for
boards and managers of companies really

01:05:08.539 --> 01:05:08.549
boards and managers of companies really
 

01:05:08.549 --> 01:05:12.919
boards and managers of companies really
rise in importance thinking about AI so

01:05:12.919 --> 01:05:12.929
rise in importance thinking about AI so
 

01:05:12.929 --> 01:05:15.919
rise in importance thinking about AI so
we're with our partners creating a board

01:05:15.919 --> 01:05:15.929
we're with our partners creating a board
 

01:05:15.929 --> 01:05:18.799
we're with our partners creating a board
toolkit that will help them to ask the

01:05:18.799 --> 01:05:18.809
toolkit that will help them to ask the
 

01:05:18.809 --> 01:05:20.929
toolkit that will help them to ask the
right questions it'll help them to think

01:05:20.929 --> 01:05:20.939
right questions it'll help them to think
 

01:05:20.939 --> 01:05:23.959
right questions it'll help them to think
about the ethics of AI that they're

01:05:23.959 --> 01:05:23.969
about the ethics of AI that they're
 

01:05:23.969 --> 01:05:26.479
about the ethics of AI that they're
deploying in their company how they're

01:05:26.479 --> 01:05:26.489
deploying in their company how they're
 

01:05:26.489 --> 01:05:28.579
deploying in their company how they're
deploying it how they're thinking about

01:05:28.579 --> 01:05:28.589
deploying it how they're thinking about
 

01:05:28.589 --> 01:05:32.689
deploying it how they're thinking about
their work force creating best practices

01:05:32.689 --> 01:05:32.699
their work force creating best practices
 

01:05:32.699 --> 01:05:36.109
their work force creating best practices
around deployment and perhaps even

01:05:36.109 --> 01:05:36.119
around deployment and perhaps even
 

01:05:36.119 --> 01:05:39.199
around deployment and perhaps even
creating something like Microsoft has or

01:05:39.199 --> 01:05:39.209
creating something like Microsoft has or
 

01:05:39.209 --> 01:05:42.589
creating something like Microsoft has or
an ethics advisory board of some sort so

01:05:42.589 --> 01:05:42.599
an ethics advisory board of some sort so
 

01:05:42.599 --> 01:05:44.929
an ethics advisory board of some sort so
again how does that fit into the agile

01:05:44.929 --> 01:05:44.939
again how does that fit into the agile
 

01:05:44.939 --> 01:05:45.930
again how does that fit into the agile
governance

01:05:45.930 --> 01:05:45.940
governance
 

01:05:45.940 --> 01:05:47.940
governance
well it's agile governments through

01:05:47.940 --> 01:05:47.950
well it's agile governments through
 

01:05:47.950 --> 01:05:51.480
well it's agile governments through
informed boards or as I said companies

01:05:51.480 --> 01:05:51.490
informed boards or as I said companies
 

01:05:51.490 --> 01:05:57.260
informed boards or as I said companies
voluntarily applying governance methods

01:05:57.260 --> 01:05:57.270
 

01:05:57.270 --> 01:06:02.250
then we have two projects which work

01:06:02.250 --> 01:06:02.260
then we have two projects which work
 

01:06:02.260 --> 01:06:04.170
then we have two projects which work
actually with governments

01:06:04.170 --> 01:06:04.180
actually with governments
 

01:06:04.180 --> 01:06:07.940
actually with governments
so the first will be working with

01:06:07.940 --> 01:06:07.950
so the first will be working with
 

01:06:07.950 --> 01:06:11.880
so the first will be working with
government partners to cope to co-create

01:06:11.880 --> 01:06:11.890
government partners to cope to co-create
 

01:06:11.890 --> 01:06:14.970
government partners to cope to co-create
soft regulation drawing a line in the

01:06:14.970 --> 01:06:14.980
soft regulation drawing a line in the
 

01:06:14.980 --> 01:06:15.930
soft regulation drawing a line in the
sand

01:06:15.930 --> 01:06:15.940
sand
 

01:06:15.940 --> 01:06:18.690
sand
the project will co-create best

01:06:18.690 --> 01:06:18.700
the project will co-create best
 

01:06:18.700 --> 01:06:22.079
the project will co-create best
practices for those governments and then

01:06:22.079 --> 01:06:22.089
practices for those governments and then
 

01:06:22.089 --> 01:06:25.349
practices for those governments and then
those governments will say if we are

01:06:25.349 --> 01:06:25.359
those governments will say if we are
 

01:06:25.359 --> 01:06:28.349
those governments will say if we are
going to buy artificial intelligence the

01:06:28.349 --> 01:06:28.359
going to buy artificial intelligence the
 

01:06:28.359 --> 01:06:30.660
going to buy artificial intelligence the
artificial intelligence product that we

01:06:30.660 --> 01:06:30.670
artificial intelligence product that we
 

01:06:30.670 --> 01:06:32.970
artificial intelligence product that we
buy for our government and use in our

01:06:32.970 --> 01:06:32.980
buy for our government and use in our
 

01:06:32.980 --> 01:06:35.609
buy for our government and use in our
government must first meet these best

01:06:35.609 --> 01:06:35.619
government must first meet these best
 

01:06:35.619 --> 01:06:39.120
government must first meet these best
practices I'm sure that it's very simple

01:06:39.120 --> 01:06:39.130
practices I'm sure that it's very simple
 

01:06:39.130 --> 01:06:42.089
practices I'm sure that it's very simple
for you to see that what that does is

01:06:42.089 --> 01:06:42.099
for you to see that what that does is
 

01:06:42.099 --> 01:06:44.309
for you to see that what that does is
it's agile governance it's the

01:06:44.309 --> 01:06:44.319
it's agile governance it's the
 

01:06:44.319 --> 01:06:47.099
it's agile governance it's the
government government instead of passing

01:06:47.099 --> 01:06:47.109
government government instead of passing
 

01:06:47.109 --> 01:06:50.730
government government instead of passing
law saying well you need to meet these

01:06:50.730 --> 01:06:50.740
law saying well you need to meet these
 

01:06:50.740 --> 01:06:53.579
law saying well you need to meet these
criteria and this is what is acceptable

01:06:53.579 --> 01:06:53.589
criteria and this is what is acceptable
 

01:06:53.589 --> 01:06:56.460
criteria and this is what is acceptable
to us regarding AI within our

01:06:56.460 --> 01:06:56.470
to us regarding AI within our
 

01:06:56.470 --> 01:07:02.400
to us regarding AI within our
jurisdiction and then I want you to just

01:07:02.400 --> 01:07:02.410
jurisdiction and then I want you to just
 

01:07:02.410 --> 01:07:05.520
jurisdiction and then I want you to just
with me imagine what a regulator of the

01:07:05.520 --> 01:07:05.530
with me imagine what a regulator of the
 

01:07:05.530 --> 01:07:09.059
with me imagine what a regulator of the
future might look like so again working

01:07:09.059 --> 01:07:09.069
future might look like so again working
 

01:07:09.069 --> 01:07:14.910
future might look like so again working
with countries to imagine perhaps Eric

01:07:14.910 --> 01:07:14.920
with countries to imagine perhaps Eric
 

01:07:14.920 --> 01:07:18.300
with countries to imagine perhaps Eric
certifying body as you know the current

01:07:18.300 --> 01:07:18.310
certifying body as you know the current
 

01:07:18.310 --> 01:07:20.160
certifying body as you know the current
regulator waits for something to go

01:07:20.160 --> 01:07:20.170
regulator waits for something to go
 

01:07:20.170 --> 01:07:21.660
regulator waits for something to go
wrong and then moves in and probably

01:07:21.660 --> 01:07:21.670
wrong and then moves in and probably
 

01:07:21.670 --> 01:07:25.109
wrong and then moves in and probably
finds people well what we want to think

01:07:25.109 --> 01:07:25.119
finds people well what we want to think
 

01:07:25.119 --> 01:07:27.359
finds people well what we want to think
about is a regulator that works with

01:07:27.359 --> 01:07:27.369
about is a regulator that works with
 

01:07:27.369 --> 01:07:33.930
about is a regulator that works with
businesses to create to help create

01:07:33.930 --> 01:07:33.940
businesses to create to help create
 

01:07:33.940 --> 01:07:36.270
businesses to create to help create
technology which then the regulator can

01:07:36.270 --> 01:07:36.280
technology which then the regulator can
 

01:07:36.280 --> 01:07:41.609
technology which then the regulator can
certify it can say this AI is fit for

01:07:41.609 --> 01:07:41.619
certify it can say this AI is fit for
 

01:07:41.619 --> 01:07:44.849
certify it can say this AI is fit for
the purpose said that government can be

01:07:44.849 --> 01:07:44.859
the purpose said that government can be
 

01:07:44.859 --> 01:07:47.339
the purpose said that government can be
say that the government can be assured

01:07:47.339 --> 01:07:47.349
say that the government can be assured
 

01:07:47.349 --> 01:07:50.870
say that the government can be assured
the citizens can be assured and the

01:07:50.870 --> 01:07:50.880
the citizens can be assured and the
 

01:07:50.880 --> 01:07:53.550
the citizens can be assured and the
company can know that it's put its tech

01:07:53.550 --> 01:07:53.560
company can know that it's put its tech
 

01:07:53.560 --> 01:07:56.849
company can know that it's put its tech
dollar it's its R&amp;D dollars into doing

01:07:56.849 --> 01:07:56.859
dollar it's its R&amp;D dollars into doing
 

01:07:56.859 --> 01:07:59.960
dollar it's its R&amp;D dollars into doing
something that it can think

01:07:59.960 --> 01:07:59.970
something that it can think
 

01:07:59.970 --> 01:08:04.290
something that it can think
now because this is a global issue we

01:08:04.290 --> 01:08:04.300
now because this is a global issue we
 

01:08:04.300 --> 01:08:06.660
now because this is a global issue we
won't just be having projects based here

01:08:06.660 --> 01:08:06.670
won't just be having projects based here
 

01:08:06.670 --> 01:08:10.650
won't just be having projects based here
in San Francisco but in we will also be

01:08:10.650 --> 01:08:10.660
in San Francisco but in we will also be
 

01:08:10.660 --> 01:08:12.450
in San Francisco but in we will also be
having sister centers around the world

01:08:12.450 --> 01:08:12.460
having sister centers around the world
 

01:08:12.460 --> 01:08:16.499
having sister centers around the world
doing projects that that are applicable

01:08:16.499 --> 01:08:16.509
doing projects that that are applicable
 

01:08:16.509 --> 01:08:18.900
doing projects that that are applicable
globally so at the moment

01:08:18.900 --> 01:08:18.910
globally so at the moment
 

01:08:18.910 --> 01:08:23.150
globally so at the moment
starting in India China and Japan and

01:08:23.150 --> 01:08:23.160
starting in India China and Japan and
 

01:08:23.160 --> 01:08:25.950
starting in India China and Japan and
obviously wanting to move into the

01:08:25.950 --> 01:08:25.960
obviously wanting to move into the
 

01:08:25.960 --> 01:08:28.829
obviously wanting to move into the
developing world because I'm sure like

01:08:28.829 --> 01:08:28.839
developing world because I'm sure like
 

01:08:28.839 --> 01:08:31.050
developing world because I'm sure like
you like me don't want to see the

01:08:31.050 --> 01:08:31.060
you like me don't want to see the
 

01:08:31.060 --> 01:08:32.610
you like me don't want to see the
developing world falling any further

01:08:32.610 --> 01:08:32.620
developing world falling any further
 

01:08:32.620 --> 01:08:35.220
developing world falling any further
behind as a result of these technologies

01:08:35.220 --> 01:08:35.230
behind as a result of these technologies
 

01:08:35.230 --> 01:08:39.420
behind as a result of these technologies
so each project that we do is actually

01:08:39.420 --> 01:08:39.430
so each project that we do is actually
 

01:08:39.430 --> 01:08:42.539
so each project that we do is actually
an agile governance model which although

01:08:42.539 --> 01:08:42.549
an agile governance model which although
 

01:08:42.549 --> 01:08:44.039
an agile governance model which although
we've done with one or two countries

01:08:44.039 --> 01:08:44.049
we've done with one or two countries
 

01:08:44.049 --> 01:08:47.249
we've done with one or two countries
over here will be available to the rest

01:08:47.249 --> 01:08:47.259
over here will be available to the rest
 

01:08:47.259 --> 01:08:51.019
over here will be available to the rest
of the world to actually use as a model

01:08:51.019 --> 01:08:51.029
of the world to actually use as a model
 

01:08:51.029 --> 01:08:54.749
of the world to actually use as a model
so you might say to me who governs the

01:08:54.749 --> 01:08:54.759
so you might say to me who governs the
 

01:08:54.759 --> 01:08:59.910
so you might say to me who governs the
governance and and I have an answer for

01:08:59.910 --> 01:08:59.920
governance and and I have an answer for
 

01:08:59.920 --> 01:09:04.070
governance and and I have an answer for
that we will be creating a global AI

01:09:04.070 --> 01:09:04.080
that we will be creating a global AI
 

01:09:04.080 --> 01:09:06.360
that we will be creating a global AI
Council or we are in the process of

01:09:06.360 --> 01:09:06.370
Council or we are in the process of
 

01:09:06.370 --> 01:09:09.510
Council or we are in the process of
creating the global AI Council which

01:09:09.510 --> 01:09:09.520
creating the global AI Council which
 

01:09:09.520 --> 01:09:11.430
creating the global AI Council which
will be empowered to look across the

01:09:11.430 --> 01:09:11.440
will be empowered to look across the
 

01:09:11.440 --> 01:09:14.390
will be empowered to look across the
world think about AI and make

01:09:14.390 --> 01:09:14.400
world think about AI and make
 

01:09:14.400 --> 01:09:16.620
world think about AI and make
recommendations for those projects that

01:09:16.620 --> 01:09:16.630
recommendations for those projects that
 

01:09:16.630 --> 01:09:18.360
recommendations for those projects that
we should be thinking about doing

01:09:18.360 --> 01:09:18.370
we should be thinking about doing
 

01:09:18.370 --> 01:09:21.059
we should be thinking about doing
they're comprised of world leaders from

01:09:21.059 --> 01:09:21.069
they're comprised of world leaders from
 

01:09:21.069 --> 01:09:22.920
they're comprised of world leaders from
government say for example Prime

01:09:22.920 --> 01:09:22.930
government say for example Prime
 

01:09:22.930 --> 01:09:25.820
government say for example Prime
Minister Mei has agreed to serve

01:09:25.820 --> 01:09:25.830
Minister Mei has agreed to serve
 

01:09:25.830 --> 01:09:28.709
Minister Mei has agreed to serve
business civil society international

01:09:28.709 --> 01:09:28.719
business civil society international
 

01:09:28.719 --> 01:09:33.420
business civil society international
global organizations and academia so

01:09:33.420 --> 01:09:33.430
global organizations and academia so
 

01:09:33.430 --> 01:09:36.090
global organizations and academia so
this has been a really concrete talk but

01:09:36.090 --> 01:09:36.100
this has been a really concrete talk but
 

01:09:36.100 --> 01:09:38.729
this has been a really concrete talk but
I hope it's been helpful to actually

01:09:38.729 --> 01:09:38.739
I hope it's been helpful to actually
 

01:09:38.739 --> 01:09:40.950
I hope it's been helpful to actually
sort of get down to some of the bones of

01:09:40.950 --> 01:09:40.960
sort of get down to some of the bones of
 

01:09:40.960 --> 01:09:45.539
sort of get down to some of the bones of
these ideas and think deeply about the

01:09:45.539 --> 01:09:45.549
these ideas and think deeply about the
 

01:09:45.549 --> 01:09:47.610
these ideas and think deeply about the
complex nature of what good governance

01:09:47.610 --> 01:09:47.620
complex nature of what good governance
 

01:09:47.620 --> 01:09:50.220
complex nature of what good governance
looks like in this area and I hope that

01:09:50.220 --> 01:09:50.230
looks like in this area and I hope that
 

01:09:50.230 --> 01:09:52.680
looks like in this area and I hope that
we can continue that conversation on our

01:09:52.680 --> 01:09:52.690
we can continue that conversation on our
 

01:09:52.690 --> 01:09:58.450
we can continue that conversation on our
panel thank you

01:09:58.450 --> 01:09:58.460
 

01:09:58.460 --> 01:10:00.770
thank you very much Kay and thank you

01:10:00.770 --> 01:10:00.780
thank you very much Kay and thank you
 

01:10:00.780 --> 01:10:03.020
thank you very much Kay and thank you
for joining us for the panel as well so

01:10:03.020 --> 01:10:03.030
for joining us for the panel as well so
 

01:10:03.030 --> 01:10:04.729
for joining us for the panel as well so
you you both alluded to the fact that

01:10:04.729 --> 01:10:04.739
you you both alluded to the fact that
 

01:10:04.739 --> 01:10:06.229
you you both alluded to the fact that
there are many different forms of

01:10:06.229 --> 01:10:06.239
there are many different forms of
 

01:10:06.239 --> 01:10:07.939
there are many different forms of
governance and some of them involve

01:10:07.939 --> 01:10:07.949
governance and some of them involve
 

01:10:07.949 --> 01:10:09.740
governance and some of them involve
governments so maybe we could start with

01:10:09.740 --> 01:10:09.750
governments so maybe we could start with
 

01:10:09.750 --> 01:10:12.200
governments so maybe we could start with
that I think it's interesting that both

01:10:12.200 --> 01:10:12.210
that I think it's interesting that both
 

01:10:12.210 --> 01:10:14.870
that I think it's interesting that both
of you have experience trying to engage

01:10:14.870 --> 01:10:14.880
of you have experience trying to engage
 

01:10:14.880 --> 01:10:18.109
of you have experience trying to engage
governments on complex technical

01:10:18.109 --> 01:10:18.119
governments on complex technical
 

01:10:18.119 --> 01:10:20.959
governments on complex technical
subjects that are at the you know

01:10:20.959 --> 01:10:20.969
subjects that are at the you know
 

01:10:20.969 --> 01:10:21.890
subjects that are at the you know
leading edge of technological

01:10:21.890 --> 01:10:21.900
leading edge of technological
 

01:10:21.900 --> 01:10:23.930
leading edge of technological
development and I wondered if you could

01:10:23.930 --> 01:10:23.940
development and I wondered if you could
 

01:10:23.940 --> 01:10:25.820
development and I wondered if you could
reflect a little bit on what what that

01:10:25.820 --> 01:10:25.830
reflect a little bit on what what that
 

01:10:25.830 --> 01:10:27.709
reflect a little bit on what what that
is like you know how the governments

01:10:27.709 --> 01:10:27.719
is like you know how the governments
 

01:10:27.719 --> 01:10:30.109
is like you know how the governments
come at subjects like AI or privacy

01:10:30.109 --> 01:10:30.119
come at subjects like AI or privacy
 

01:10:30.119 --> 01:10:32.149
come at subjects like AI or privacy
perhaps in a way that's different from

01:10:32.149 --> 01:10:32.159
perhaps in a way that's different from
 

01:10:32.159 --> 01:10:34.939
perhaps in a way that's different from
technologists and are they equipped to

01:10:34.939 --> 01:10:34.949
technologists and are they equipped to
 

01:10:34.949 --> 01:10:36.830
technologists and are they equipped to
deal with these things how do you help

01:10:36.830 --> 01:10:36.840
deal with these things how do you help
 

01:10:36.840 --> 01:10:38.750
deal with these things how do you help
that process so Laurie maybe you could

01:10:38.750 --> 01:10:38.760
that process so Laurie maybe you could
 

01:10:38.760 --> 01:10:39.729
that process so Laurie maybe you could
start

01:10:39.729 --> 01:10:39.739
start
 

01:10:39.739 --> 01:10:44.510
start
so I spent 2016 as chief technologists

01:10:44.510 --> 01:10:44.520
so I spent 2016 as chief technologists
 

01:10:44.520 --> 01:10:47.300
so I spent 2016 as chief technologists
at the Federal Trade Commission and it

01:10:47.300 --> 01:10:47.310
at the Federal Trade Commission and it
 

01:10:47.310 --> 01:10:49.760
at the Federal Trade Commission and it
was in some ways kind of a surreal

01:10:49.760 --> 01:10:49.770
was in some ways kind of a surreal
 

01:10:49.770 --> 01:10:52.939
was in some ways kind of a surreal
experience because this was an agency of

01:10:52.939 --> 01:10:52.949
experience because this was an agency of
 

01:10:52.949 --> 01:10:56.770
experience because this was an agency of
a thousand people and I was one of two

01:10:56.770 --> 01:10:56.780
a thousand people and I was one of two
 

01:10:56.780 --> 01:11:00.919
a thousand people and I was one of two
PhD technologists in the agency most of

01:11:00.919 --> 01:11:00.929
PhD technologists in the agency most of
 

01:11:00.929 --> 01:11:03.919
PhD technologists in the agency most of
that time and there were a handful of

01:11:03.919 --> 01:11:03.929
that time and there were a handful of
 

01:11:03.929 --> 01:11:06.740
that time and there were a handful of
other technical people there were a lot

01:11:06.740 --> 01:11:06.750
other technical people there were a lot
 

01:11:06.750 --> 01:11:09.200
other technical people there were a lot
of lawyers and I think about a hundred

01:11:09.200 --> 01:11:09.210
of lawyers and I think about a hundred
 

01:11:09.210 --> 01:11:13.280
of lawyers and I think about a hundred
economists so the thing is that the

01:11:13.280 --> 01:11:13.290
economists so the thing is that the
 

01:11:13.290 --> 01:11:15.169
economists so the thing is that the
Federal Trade Commission is actually an

01:11:15.169 --> 01:11:15.179
Federal Trade Commission is actually an
 

01:11:15.179 --> 01:11:17.479
Federal Trade Commission is actually an
agency that deals a lot with technology

01:11:17.479 --> 01:11:17.489
agency that deals a lot with technology
 

01:11:17.489 --> 01:11:21.530
agency that deals a lot with technology
a lot of a lot of what they regulate and

01:11:21.530 --> 01:11:21.540
a lot of a lot of what they regulate and
 

01:11:21.540 --> 01:11:23.780
a lot of a lot of what they regulate and
a lot of the issues increasingly are

01:11:23.780 --> 01:11:23.790
a lot of the issues increasingly are
 

01:11:23.790 --> 01:11:26.899
a lot of the issues increasingly are
based on technology and I think there

01:11:26.899 --> 01:11:26.909
based on technology and I think there
 

01:11:26.909 --> 01:11:28.760
based on technology and I think there
are a lot of attorneys and economists

01:11:28.760 --> 01:11:28.770
are a lot of attorneys and economists
 

01:11:28.770 --> 01:11:31.339
are a lot of attorneys and economists
there who are pretty proficient and

01:11:31.339 --> 01:11:31.349
there who are pretty proficient and
 

01:11:31.349 --> 01:11:34.189
there who are pretty proficient and
talking about technology but they are

01:11:34.189 --> 01:11:34.199
talking about technology but they are
 

01:11:34.199 --> 01:11:37.510
talking about technology but they are
also aware that they need more expertise

01:11:37.510 --> 01:11:37.520
also aware that they need more expertise
 

01:11:37.520 --> 01:11:41.450
also aware that they need more expertise
so I obviously couldn't provide all of

01:11:41.450 --> 01:11:41.460
so I obviously couldn't provide all of
 

01:11:41.460 --> 01:11:43.310
so I obviously couldn't provide all of
the expertise that was needed I didn't

01:11:43.310 --> 01:11:43.320
the expertise that was needed I didn't
 

01:11:43.320 --> 01:11:46.010
the expertise that was needed I didn't
have the time nor you know my expertise

01:11:46.010 --> 01:11:46.020
have the time nor you know my expertise
 

01:11:46.020 --> 01:11:49.399
have the time nor you know my expertise
is relatively narrow so I spent a lot of

01:11:49.399 --> 01:11:49.409
is relatively narrow so I spent a lot of
 

01:11:49.409 --> 01:11:51.200
is relatively narrow so I spent a lot of
time helping them reach out to experts

01:11:51.200 --> 01:11:51.210
time helping them reach out to experts
 

01:11:51.210 --> 01:11:53.959
time helping them reach out to experts
who might be able to bring in the

01:11:53.959 --> 01:11:53.969
who might be able to bring in the
 

01:11:53.969 --> 01:11:56.839
who might be able to bring in the
expertise that they needed and also

01:11:56.839 --> 01:11:56.849
expertise that they needed and also
 

01:11:56.849 --> 01:11:58.520
expertise that they needed and also
encouraging them to hire more

01:11:58.520 --> 01:11:58.530
encouraging them to hire more
 

01:11:58.530 --> 01:12:02.120
encouraging them to hire more
technologists but but I think this is

01:12:02.120 --> 01:12:02.130
technologists but but I think this is
 

01:12:02.130 --> 01:12:04.580
technologists but but I think this is
not an atypical experience in US

01:12:04.580 --> 01:12:04.590
not an atypical experience in US
 

01:12:04.590 --> 01:12:06.080
not an atypical experience in US
government agents

01:12:06.080 --> 01:12:06.090
government agents
 

01:12:06.090 --> 01:12:07.580
government agents
you know they're starting to bring in

01:12:07.580 --> 01:12:07.590
you know they're starting to bring in
 

01:12:07.590 --> 01:12:09.830
you know they're starting to bring in
technologists but they don't necessarily

01:12:09.830 --> 01:12:09.840
technologists but they don't necessarily
 

01:12:09.840 --> 01:12:13.489
technologists but they don't necessarily
have enough for the kind of policymaking

01:12:13.489 --> 01:12:13.499
have enough for the kind of policymaking
 

01:12:13.499 --> 01:12:15.229
have enough for the kind of policymaking
that they're increasingly getting called

01:12:15.229 --> 01:12:15.239
that they're increasingly getting called
 

01:12:15.239 --> 01:12:18.020
that they're increasingly getting called
on to do right okay what's your

01:12:18.020 --> 01:12:18.030
on to do right okay what's your
 

01:12:18.030 --> 01:12:19.699
on to do right okay what's your
experience there's some governments it

01:12:19.699 --> 01:12:19.709
experience there's some governments it
 

01:12:19.709 --> 01:12:22.640
experience there's some governments it
sounds like are actively engaging with

01:12:22.640 --> 01:12:22.650
sounds like are actively engaging with
 

01:12:22.650 --> 01:12:25.100
sounds like are actively engaging with
this topic do they come at it in a

01:12:25.100 --> 01:12:25.110
this topic do they come at it in a
 

01:12:25.110 --> 01:12:27.169
this topic do they come at it in a
different way from other people in the

01:12:27.169 --> 01:12:27.179
different way from other people in the
 

01:12:27.179 --> 01:12:28.790
different way from other people in the
room or that we've heard on stage talk

01:12:28.790 --> 01:12:28.800
room or that we've heard on stage talk
 

01:12:28.800 --> 01:12:31.819
room or that we've heard on stage talk
about it I I I think that what we're

01:12:31.819 --> 01:12:31.829
about it I I I think that what we're
 

01:12:31.829 --> 01:12:33.979
about it I I I think that what we're
finding at the Forum is that actually

01:12:33.979 --> 01:12:33.989
finding at the Forum is that actually
 

01:12:33.989 --> 01:12:36.169
finding at the Forum is that actually
governments are really a number of

01:12:36.169 --> 01:12:36.179
governments are really a number of
 

01:12:36.179 --> 01:12:37.699
governments are really a number of
governments are really interested in

01:12:37.699 --> 01:12:37.709
governments are really interested in
 

01:12:37.709 --> 01:12:42.290
governments are really interested in
this topic and are doubtful as to what

01:12:42.290 --> 01:12:42.300
this topic and are doubtful as to what
 

01:12:42.300 --> 01:12:44.649
this topic and are doubtful as to what
they should be doing and what

01:12:44.649 --> 01:12:44.659
they should be doing and what
 

01:12:44.659 --> 01:12:48.319
they should be doing and what
governments mechanisms are necessary and

01:12:48.319 --> 01:12:48.329
governments mechanisms are necessary and
 

01:12:48.329 --> 01:12:52.819
governments mechanisms are necessary and
so we have partners at the Forum a

01:12:52.819 --> 01:12:52.829
so we have partners at the Forum a
 

01:12:52.829 --> 01:12:55.310
so we have partners at the Forum a
number of governments who are actually

01:12:55.310 --> 01:12:55.320
number of governments who are actually
 

01:12:55.320 --> 01:12:57.199
number of governments who are actually
working with us in San Francisco and

01:12:57.199 --> 01:12:57.209
working with us in San Francisco and
 

01:12:57.209 --> 01:13:01.969
working with us in San Francisco and
give you an idea of diversity Japan to

01:13:01.969 --> 01:13:01.979
give you an idea of diversity Japan to
 

01:13:01.979 --> 01:13:06.529
give you an idea of diversity Japan to
Rwanda so lots of governments interested

01:13:06.529 --> 01:13:06.539
Rwanda so lots of governments interested
 

01:13:06.539 --> 01:13:12.049
Rwanda so lots of governments interested
and wanting to to do better okay and are

01:13:12.049 --> 01:13:12.059
and wanting to to do better okay and are
 

01:13:12.059 --> 01:13:13.699
and wanting to to do better okay and are
there some emerging models of how

01:13:13.699 --> 01:13:13.709
there some emerging models of how
 

01:13:13.709 --> 01:13:15.199
there some emerging models of how
different governments are doing that I

01:13:15.199 --> 01:13:15.209
different governments are doing that I
 

01:13:15.209 --> 01:13:17.899
different governments are doing that I
mean one idea I've heard discussed here

01:13:17.899 --> 01:13:17.909
mean one idea I've heard discussed here
 

01:13:17.909 --> 01:13:19.459
mean one idea I've heard discussed here
in the u.s. is maybe the government

01:13:19.459 --> 01:13:19.469
in the u.s. is maybe the government
 

01:13:19.469 --> 01:13:21.799
in the u.s. is maybe the government
should have a federal agency for AI or

01:13:21.799 --> 01:13:21.809
should have a federal agency for AI or
 

01:13:21.809 --> 01:13:25.219
should have a federal agency for AI or
robotics and then I like its way to

01:13:25.219 --> 01:13:25.229
robotics and then I like its way to
 

01:13:25.229 --> 01:13:27.529
robotics and then I like its way to
having a federal Food and Drug

01:13:27.529 --> 01:13:27.539
having a federal Food and Drug
 

01:13:27.539 --> 01:13:29.660
having a federal Food and Drug
Administration I mean is anyone

01:13:29.660 --> 01:13:29.670
Administration I mean is anyone
 

01:13:29.670 --> 01:13:32.629
Administration I mean is anyone
exploring that possibility yes I think

01:13:32.629 --> 01:13:32.639
exploring that possibility yes I think
 

01:13:32.639 --> 01:13:34.219
exploring that possibility yes I think
the obviously there's going to be a

01:13:34.219 --> 01:13:34.229
the obviously there's going to be a
 

01:13:34.229 --> 01:13:37.069
the obviously there's going to be a
Center for data ethics in the UK which

01:13:37.069 --> 01:13:37.079
Center for data ethics in the UK which
 

01:13:37.079 --> 01:13:40.189
Center for data ethics in the UK which
was budgeted for in November and there

01:13:40.189 --> 01:13:40.199
was budgeted for in November and there
 

01:13:40.199 --> 01:13:42.859
was budgeted for in November and there
is a segment of one of the ministries

01:13:42.859 --> 01:13:42.869
is a segment of one of the ministries
 

01:13:42.869 --> 01:13:47.750
is a segment of one of the ministries
that has now become an AI and AI sickest

01:13:47.750 --> 01:13:47.760
that has now become an AI and AI sickest
 

01:13:47.760 --> 01:13:50.989
that has now become an AI and AI sickest
piece of that ministry obviously you've

01:13:50.989 --> 01:13:50.999
piece of that ministry obviously you've
 

01:13:50.999 --> 01:13:52.939
piece of that ministry obviously you've
got the Minister of Juba a minister of

01:13:52.939 --> 01:13:52.949
got the Minister of Juba a minister of
 

01:13:52.949 --> 01:13:53.899
got the Minister of Juba a minister of
AI

01:13:53.899 --> 01:13:53.909
AI
 

01:13:53.909 --> 01:13:58.700
AI
the UAE what is his remit what does a

01:13:58.700 --> 01:13:58.710
the UAE what is his remit what does a
 

01:13:58.710 --> 01:14:03.649
the UAE what is his remit what does a
Minister for very I do you probably

01:14:03.649 --> 01:14:03.659
Minister for very I do you probably
 

01:14:03.659 --> 01:14:08.810
Minister for very I do you probably
better ask him bad but my understanding

01:14:08.810 --> 01:14:08.820
better ask him bad but my understanding
 

01:14:08.820 --> 01:14:12.860
better ask him bad but my understanding
is to think about all the possible ways

01:14:12.860 --> 01:14:12.870
is to think about all the possible ways
 

01:14:12.870 --> 01:14:15.110
is to think about all the possible ways
in which AI would be used within the

01:14:15.110 --> 01:14:15.120
in which AI would be used within the
 

01:14:15.120 --> 01:14:18.890
in which AI would be used within the
country and that really gels with some

01:14:18.890 --> 01:14:18.900
country and that really gels with some
 

01:14:18.900 --> 01:14:20.600
country and that really gels with some
of some other countries that are

01:14:20.600 --> 01:14:20.610
of some other countries that are
 

01:14:20.610 --> 01:14:24.530
of some other countries that are
producing AI strategies say like we we

01:14:24.530 --> 01:14:24.540
producing AI strategies say like we we
 

01:14:24.540 --> 01:14:27.500
producing AI strategies say like we we
famously know of the China AI strategies

01:14:27.500 --> 01:14:27.510
famously know of the China AI strategies
 

01:14:27.510 --> 01:14:31.729
famously know of the China AI strategies
so lots and and president macron spoke

01:14:31.729 --> 01:14:31.739
so lots and and president macron spoke
 

01:14:31.739 --> 01:14:33.919
so lots and and president macron spoke
recently about the way that France would

01:14:33.919 --> 01:14:33.929
recently about the way that France would
 

01:14:33.929 --> 01:14:36.860
recently about the way that France would
be so I think there is lots of interest

01:14:36.860 --> 01:14:36.870
be so I think there is lots of interest
 

01:14:36.870 --> 01:14:38.959
be so I think there is lots of interest
in developing strategies and people

01:14:38.959 --> 01:14:38.969
in developing strategies and people
 

01:14:38.969 --> 01:14:42.979
in developing strategies and people
thinking about a specific okay you must

01:14:42.979 --> 01:14:42.989
thinking about a specific okay you must
 

01:14:42.989 --> 01:14:44.990
thinking about a specific okay you must
have mentioned president macrons plan

01:14:44.990 --> 01:14:45.000
have mentioned president macrons plan
 

01:14:45.000 --> 01:14:46.490
have mentioned president macrons plan
and I think that was interesting because

01:14:46.490 --> 01:14:46.500
and I think that was interesting because
 

01:14:46.500 --> 01:14:49.850
and I think that was interesting because
he well it's a little bit unusual to

01:14:49.850 --> 01:14:49.860
he well it's a little bit unusual to
 

01:14:49.860 --> 01:14:52.280
he well it's a little bit unusual to
have a world leader say you know now is

01:14:52.280 --> 01:14:52.290
have a world leader say you know now is
 

01:14:52.290 --> 01:14:54.050
have a world leader say you know now is
the time to be thinking about rules for

01:14:54.050 --> 01:14:54.060
the time to be thinking about rules for
 

01:14:54.060 --> 01:14:56.000
the time to be thinking about rules for
AI but he also went a bit further and

01:14:56.000 --> 01:14:56.010
AI but he also went a bit further and
 

01:14:56.010 --> 01:14:58.189
AI but he also went a bit further and
said these rules are going to impose

01:14:58.189 --> 01:14:58.199
said these rules are going to impose
 

01:14:58.199 --> 01:15:02.600
said these rules are going to impose
certain cultural characteristics that we

01:15:02.600 --> 01:15:02.610
certain cultural characteristics that we
 

01:15:02.610 --> 01:15:04.820
certain cultural characteristics that we
value in France on to the technology and

01:15:04.820 --> 01:15:04.830
value in France on to the technology and
 

01:15:04.830 --> 01:15:06.740
value in France on to the technology and
he sort of painted this picture of you

01:15:06.740 --> 01:15:06.750
he sort of painted this picture of you
 

01:15:06.750 --> 01:15:08.660
he sort of painted this picture of you
know a certain Frenchness about the

01:15:08.660 --> 01:15:08.670
know a certain Frenchness about the
 

01:15:08.670 --> 01:15:11.330
know a certain Frenchness about the
systems that will be created under this

01:15:11.330 --> 01:15:11.340
systems that will be created under this
 

01:15:11.340 --> 01:15:15.140
systems that will be created under this
this framework is that a model you think

01:15:15.140 --> 01:15:15.150
this framework is that a model you think
 

01:15:15.150 --> 01:15:18.560
this framework is that a model you think
other countries might adopt I don't know

01:15:18.560 --> 01:15:18.570
other countries might adopt I don't know
 

01:15:18.570 --> 01:15:20.180
other countries might adopt I don't know
whether its model that other countries

01:15:20.180 --> 01:15:20.190
whether its model that other countries
 

01:15:20.190 --> 01:15:22.280
whether its model that other countries
might adopt but it's certainly something

01:15:22.280 --> 01:15:22.290
might adopt but it's certainly something
 

01:15:22.290 --> 01:15:24.290
might adopt but it's certainly something
that I think that those of us working in

01:15:24.290 --> 01:15:24.300
that I think that those of us working in
 

01:15:24.300 --> 01:15:28.340
that I think that those of us working in
AI and ethics come up with a lot you

01:15:28.340 --> 01:15:28.350
AI and ethics come up with a lot you
 

01:15:28.350 --> 01:15:31.160
AI and ethics come up with a lot you
know whose ethics anyway

01:15:31.160 --> 01:15:31.170
know whose ethics anyway
 

01:15:31.170 --> 01:15:36.020
know whose ethics anyway
and so I about the project that illa and

01:15:36.020 --> 01:15:36.030
and so I about the project that illa and
 

01:15:36.030 --> 01:15:38.680
and so I about the project that illa and
Jennifer and the iron co-creating and

01:15:38.680 --> 01:15:38.690
Jennifer and the iron co-creating and
 

01:15:38.690 --> 01:15:41.570
Jennifer and the iron co-creating and
really sort of helping people to think

01:15:41.570 --> 01:15:41.580
really sort of helping people to think
 

01:15:41.580 --> 01:15:44.570
really sort of helping people to think
about their own cultural frames of

01:15:44.570 --> 01:15:44.580
about their own cultural frames of
 

01:15:44.580 --> 01:15:47.360
about their own cultural frames of
reference for using their technology and

01:15:47.360 --> 01:15:47.370
reference for using their technology and
 

01:15:47.370 --> 01:15:51.020
reference for using their technology and
if we think about use of AI globally and

01:15:51.020 --> 01:15:51.030
if we think about use of AI globally and
 

01:15:51.030 --> 01:15:54.200
if we think about use of AI globally and
perhaps extrapolator and bring in the

01:15:54.200 --> 01:15:54.210
perhaps extrapolator and bring in the
 

01:15:54.210 --> 01:15:56.479
perhaps extrapolator and bring in the
developing world they're going to have

01:15:56.479 --> 01:15:56.489
developing world they're going to have
 

01:15:56.489 --> 01:15:58.410
developing world they're going to have
very different needs

01:15:58.410 --> 01:15:58.420
very different needs
 

01:15:58.420 --> 01:16:02.220
very different needs
for AI technology than we have and so it

01:16:02.220 --> 01:16:02.230
for AI technology than we have and so it
 

01:16:02.230 --> 01:16:05.130
for AI technology than we have and so it
would it be understandable that they

01:16:05.130 --> 01:16:05.140
would it be understandable that they
 

01:16:05.140 --> 01:16:06.660
would it be understandable that they
would want to have their own cultural

01:16:06.660 --> 01:16:06.670
would want to have their own cultural
 

01:16:06.670 --> 01:16:09.900
would want to have their own cultural
reference I see okay and right at the

01:16:09.900 --> 01:16:09.910
reference I see okay and right at the
 

01:16:09.910 --> 01:16:11.820
reference I see okay and right at the
end of your talk you mentioned the I

01:16:11.820 --> 01:16:11.830
end of your talk you mentioned the I
 

01:16:11.830 --> 01:16:16.260
end of your talk you mentioned the I
think it was the Global Council what

01:16:16.260 --> 01:16:16.270
think it was the Global Council what
 

01:16:16.270 --> 01:16:18.780
think it was the Global Council what
will function will that serve I mean do

01:16:18.780 --> 01:16:18.790
will function will that serve I mean do
 

01:16:18.790 --> 01:16:20.010
will function will that serve I mean do
you imagine it helping with the issue

01:16:20.010 --> 01:16:20.020
you imagine it helping with the issue
 

01:16:20.020 --> 01:16:21.840
you imagine it helping with the issue
you just mentioned there of ensuring

01:16:21.840 --> 01:16:21.850
you just mentioned there of ensuring
 

01:16:21.850 --> 01:16:24.030
you just mentioned there of ensuring
that all countries get some benefits I

01:16:24.030 --> 01:16:24.040
that all countries get some benefits I
 

01:16:24.040 --> 01:16:27.270
that all countries get some benefits I
mean will it set international norms

01:16:27.270 --> 01:16:27.280
mean will it set international norms
 

01:16:27.280 --> 01:16:29.010
mean will it set international norms
about the technology what is the goal

01:16:29.010 --> 01:16:29.020
about the technology what is the goal
 

01:16:29.020 --> 01:16:32.070
about the technology what is the goal
the idea isn't to set international

01:16:32.070 --> 01:16:32.080
the idea isn't to set international
 

01:16:32.080 --> 01:16:36.120
the idea isn't to set international
norms the idea is to help us to think

01:16:36.120 --> 01:16:36.130
norms the idea is to help us to think
 

01:16:36.130 --> 01:16:39.330
norms the idea is to help us to think
about which projects we should be

01:16:39.330 --> 01:16:39.340
about which projects we should be
 

01:16:39.340 --> 01:16:42.810
about which projects we should be
working on across the various centers of

01:16:42.810 --> 01:16:42.820
working on across the various centers of
 

01:16:42.820 --> 01:16:44.669
working on across the various centers of
which will be located around the world

01:16:44.669 --> 01:16:44.679
which will be located around the world
 

01:16:44.679 --> 01:16:50.370
which will be located around the world
and so aspects of the projects that I've

01:16:50.370 --> 01:16:50.380
and so aspects of the projects that I've
 

01:16:50.380 --> 01:16:54.479
and so aspects of the projects that I've
already discussed today okay Laurie you

01:16:54.479 --> 01:16:54.489
already discussed today okay Laurie you
 

01:16:54.489 --> 01:16:56.729
already discussed today okay Laurie you
spoke about these ACM principles and you

01:16:56.729 --> 01:16:56.739
spoke about these ACM principles and you
 

01:16:56.739 --> 01:16:57.870
spoke about these ACM principles and you
spoke a little bit at the end there

01:16:57.870 --> 01:16:57.880
spoke a little bit at the end there
 

01:16:57.880 --> 01:17:00.360
spoke a little bit at the end there
about the path forward for those so

01:17:00.360 --> 01:17:00.370
about the path forward for those so
 

01:17:00.370 --> 01:17:04.800
about the path forward for those so
maybe you could expand on that are

01:17:04.800 --> 01:17:04.810
maybe you could expand on that are
 

01:17:04.810 --> 01:17:07.080
maybe you could expand on that are
people you know signing allegiance to

01:17:07.080 --> 01:17:07.090
people you know signing allegiance to
 

01:17:07.090 --> 01:17:09.150
people you know signing allegiance to
that document yeah is it does it operate

01:17:09.150 --> 01:17:09.160
that document yeah is it does it operate
 

01:17:09.160 --> 01:17:12.810
that document yeah is it does it operate
just that the level of organizations or

01:17:12.810 --> 01:17:12.820
just that the level of organizations or
 

01:17:12.820 --> 01:17:15.900
just that the level of organizations or
individuals how is how is this going to

01:17:15.900 --> 01:17:15.910
individuals how is how is this going to
 

01:17:15.910 --> 01:17:19.380
individuals how is how is this going to
work so so as far as I know people

01:17:19.380 --> 01:17:19.390
work so so as far as I know people
 

01:17:19.390 --> 01:17:21.750
work so so as far as I know people
aren't specifically signing allegiance

01:17:21.750 --> 01:17:21.760
aren't specifically signing allegiance
 

01:17:21.760 --> 01:17:25.430
aren't specifically signing allegiance
to it I think it was put out there as I

01:17:25.430 --> 01:17:25.440
to it I think it was put out there as I
 

01:17:25.440 --> 01:17:28.250
to it I think it was put out there as I
guess kind of an aspirational document

01:17:28.250 --> 01:17:28.260
guess kind of an aspirational document
 

01:17:28.260 --> 01:17:32.220
guess kind of an aspirational document
okay okay and I wonder if your

01:17:32.220 --> 01:17:32.230
okay okay and I wonder if your
 

01:17:32.230 --> 01:17:34.979
okay okay and I wonder if your
experience in privacy could help us look

01:17:34.979 --> 01:17:34.989
experience in privacy could help us look
 

01:17:34.989 --> 01:17:37.290
experience in privacy could help us look
ahead a little bit as to what might

01:17:37.290 --> 01:17:37.300
ahead a little bit as to what might
 

01:17:37.300 --> 01:17:39.470
ahead a little bit as to what might
happen with a iron governance so one

01:17:39.470 --> 01:17:39.480
happen with a iron governance so one
 

01:17:39.480 --> 01:17:42.360
happen with a iron governance so one
pattern in technology policy and I think

01:17:42.360 --> 01:17:42.370
pattern in technology policy and I think
 

01:17:42.370 --> 01:17:45.570
pattern in technology policy and I think
privacy is that people like when

01:17:45.570 --> 01:17:45.580
privacy is that people like when
 

01:17:45.580 --> 01:17:46.740
privacy is that people like when
informed people like yourself will do

01:17:46.740 --> 01:17:46.750
informed people like yourself will do
 

01:17:46.750 --> 01:17:48.270
informed people like yourself will do
some research and put your hands up and

01:17:48.270 --> 01:17:48.280
some research and put your hands up and
 

01:17:48.280 --> 01:17:49.950
some research and put your hands up and
said that there could be a problem with

01:17:49.950 --> 01:17:49.960
said that there could be a problem with
 

01:17:49.960 --> 01:17:50.880
said that there could be a problem with
this we need to be thinking about this

01:17:50.880 --> 01:17:50.890
this we need to be thinking about this
 

01:17:50.890 --> 01:17:53.700
this we need to be thinking about this
issue and nothing much happens until

01:17:53.700 --> 01:17:53.710
issue and nothing much happens until
 

01:17:53.710 --> 01:17:56.729
issue and nothing much happens until
there's a terrible incident and then you

01:17:56.729 --> 01:17:56.739
there's a terrible incident and then you
 

01:17:56.739 --> 01:17:58.320
there's a terrible incident and then you
know maybe the courts are involved at

01:17:58.320 --> 01:17:58.330
know maybe the courts are involved at
 

01:17:58.330 --> 01:18:00.240
know maybe the courts are involved at
all there's certainly interest from

01:18:00.240 --> 01:18:00.250
all there's certainly interest from
 

01:18:00.250 --> 01:18:03.419
all there's certainly interest from
policy makers I could see that happening

01:18:03.419 --> 01:18:03.429
policy makers I could see that happening
 

01:18:03.429 --> 01:18:05.340
policy makers I could see that happening
in AI as well I don't know if you agree

01:18:05.340 --> 01:18:05.350
in AI as well I don't know if you agree
 

01:18:05.350 --> 01:18:07.800
in AI as well I don't know if you agree
I mean is there an example from privacy

01:18:07.800 --> 01:18:07.810
I mean is there an example from privacy
 

01:18:07.810 --> 01:18:09.810
I mean is there an example from privacy
that might help us think about those

01:18:09.810 --> 01:18:09.820
that might help us think about those
 

01:18:09.820 --> 01:18:10.820
that might help us think about those
types of scenarios

01:18:10.820 --> 01:18:10.830
types of scenarios
 

01:18:10.830 --> 01:18:14.120
types of scenarios
yeah I think we've seen that repeatedly

01:18:14.120 --> 01:18:14.130
yeah I think we've seen that repeatedly
 

01:18:14.130 --> 01:18:17.030
yeah I think we've seen that repeatedly
in privacy so in the United States we

01:18:17.030 --> 01:18:17.040
in privacy so in the United States we
 

01:18:17.040 --> 01:18:18.710
in privacy so in the United States we
don't actually have much in the way of

01:18:18.710 --> 01:18:18.720
don't actually have much in the way of
 

01:18:18.720 --> 01:18:25.580
don't actually have much in the way of
privacy laws and back in the 1980s we we

01:18:25.580 --> 01:18:25.590
privacy laws and back in the 1980s we we
 

01:18:25.590 --> 01:18:27.410
privacy laws and back in the 1980s we we
didn't even really have health care data

01:18:27.410 --> 01:18:27.420
didn't even really have health care data
 

01:18:27.420 --> 01:18:32.540
didn't even really have health care data
privacy laws and there was somebody a

01:18:32.540 --> 01:18:32.550
privacy laws and there was somebody a
 

01:18:32.550 --> 01:18:35.180
privacy laws and there was somebody a
judge who was being considered for the

01:18:35.180 --> 01:18:35.190
judge who was being considered for the
 

01:18:35.190 --> 01:18:40.190
judge who was being considered for the
supreme court judge Bork and in the in

01:18:40.190 --> 01:18:40.200
supreme court judge Bork and in the in
 

01:18:40.200 --> 01:18:43.760
supreme court judge Bork and in the in
that process there was a lot of people

01:18:43.760 --> 01:18:43.770
that process there was a lot of people
 

01:18:43.770 --> 01:18:45.590
that process there was a lot of people
who are trying to dig up dirt on him and

01:18:45.590 --> 01:18:45.600
who are trying to dig up dirt on him and
 

01:18:45.600 --> 01:18:48.620
who are trying to dig up dirt on him and
a journalist went to the video rental

01:18:48.620 --> 01:18:48.630
a journalist went to the video rental
 

01:18:48.630 --> 01:18:51.710
a journalist went to the video rental
store that he rented videos from and was

01:18:51.710 --> 01:18:51.720
store that he rented videos from and was
 

01:18:51.720 --> 01:18:54.110
store that he rented videos from and was
able to get all his video rental records

01:18:54.110 --> 01:18:54.120
able to get all his video rental records
 

01:18:54.120 --> 01:18:56.780
able to get all his video rental records
and and then this was published in the

01:18:56.780 --> 01:18:56.790
and and then this was published in the
 

01:18:56.790 --> 01:18:59.390
and and then this was published in the
major newspapers and you know there

01:18:59.390 --> 01:18:59.400
major newspapers and you know there
 

01:18:59.400 --> 01:19:00.980
major newspapers and you know there
wasn't really much in it other than he

01:19:00.980 --> 01:19:00.990
wasn't really much in it other than he
 

01:19:00.990 --> 01:19:04.010
wasn't really much in it other than he
had bad taste in movies but you know the

01:19:04.010 --> 01:19:04.020
had bad taste in movies but you know the
 

01:19:04.020 --> 01:19:06.140
had bad taste in movies but you know the
expectation had been that there would be

01:19:06.140 --> 01:19:06.150
expectation had been that there would be
 

01:19:06.150 --> 01:19:08.960
expectation had been that there would be
you know some adult entertainment or

01:19:08.960 --> 01:19:08.970
you know some adult entertainment or
 

01:19:08.970 --> 01:19:12.530
you know some adult entertainment or
whatnot and immediately we saw members

01:19:12.530 --> 01:19:12.540
whatnot and immediately we saw members
 

01:19:12.540 --> 01:19:15.080
whatnot and immediately we saw members
of Congress falling over themselves to

01:19:15.080 --> 01:19:15.090
of Congress falling over themselves to
 

01:19:15.090 --> 01:19:17.480
of Congress falling over themselves to
try to outlaw this and try to make it so

01:19:17.480 --> 01:19:17.490
try to outlaw this and try to make it so
 

01:19:17.490 --> 01:19:19.820
try to outlaw this and try to make it so
that you could not go to a video rental

01:19:19.820 --> 01:19:19.830
that you could not go to a video rental
 

01:19:19.830 --> 01:19:21.710
that you could not go to a video rental
store and get somebody's records and

01:19:21.710 --> 01:19:21.720
store and get somebody's records and
 

01:19:21.720 --> 01:19:24.500
store and get somebody's records and
they very quickly passed the video

01:19:24.500 --> 01:19:24.510
they very quickly passed the video
 

01:19:24.510 --> 01:19:27.830
they very quickly passed the video
rental Privacy Protection Act in record

01:19:27.830 --> 01:19:27.840
rental Privacy Protection Act in record
 

01:19:27.840 --> 01:19:30.260
rental Privacy Protection Act in record
time and our video rental records to

01:19:30.260 --> 01:19:30.270
time and our video rental records to
 

01:19:30.270 --> 01:19:33.490
time and our video rental records to
this day are among the best protected

01:19:33.490 --> 01:19:33.500
this day are among the best protected
 

01:19:33.500 --> 01:19:35.840
this day are among the best protected
personal data records in the United

01:19:35.840 --> 01:19:35.850
personal data records in the United
 

01:19:35.850 --> 01:19:38.090
personal data records in the United
States okay

01:19:38.090 --> 01:19:38.100
States okay
 

01:19:38.100 --> 01:19:44.650
States okay
that's a I mean can we think of a

01:19:44.650 --> 01:19:44.660
that's a I mean can we think of a
 

01:19:44.660 --> 01:19:48.140
that's a I mean can we think of a
possible scenario in in the case of the

01:19:48.140 --> 01:19:48.150
possible scenario in in the case of the
 

01:19:48.150 --> 01:19:49.820
possible scenario in in the case of the
application of AI where there's a risk

01:19:49.820 --> 01:19:49.830
application of AI where there's a risk
 

01:19:49.830 --> 01:19:53.750
application of AI where there's a risk
of a sudden incident that causes big

01:19:53.750 --> 01:19:53.760
of a sudden incident that causes big
 

01:19:53.760 --> 01:19:56.420
of a sudden incident that causes big
change yeah well I think in AI the most

01:19:56.420 --> 01:19:56.430
change yeah well I think in AI the most
 

01:19:56.430 --> 01:19:58.880
change yeah well I think in AI the most
likely scenario is that somebody dies

01:19:58.880 --> 01:19:58.890
likely scenario is that somebody dies
 

01:19:58.890 --> 01:20:01.970
likely scenario is that somebody dies
and and I think we're already starting

01:20:01.970 --> 01:20:01.980
and and I think we're already starting
 

01:20:01.980 --> 01:20:04.340
and and I think we're already starting
to see that with self-driving cars where

01:20:04.340 --> 01:20:04.350
to see that with self-driving cars where
 

01:20:04.350 --> 01:20:07.070
to see that with self-driving cars where
there's been some unfortunate accidents

01:20:07.070 --> 01:20:07.080
there's been some unfortunate accidents
 

01:20:07.080 --> 01:20:10.640
there's been some unfortunate accidents
recently that I think are causing people

01:20:10.640 --> 01:20:10.650
recently that I think are causing people
 

01:20:10.650 --> 01:20:12.830
recently that I think are causing people
to ask questions and they say you know

01:20:12.830 --> 01:20:12.840
to ask questions and they say you know
 

01:20:12.840 --> 01:20:14.690
to ask questions and they say you know
how can we let these car companies do

01:20:14.690 --> 01:20:14.700
how can we let these car companies do
 

01:20:14.700 --> 01:20:16.610
how can we let these car companies do
this do we need to regulate them do we

01:20:16.610 --> 01:20:16.620
this do we need to regulate them do we
 

01:20:16.620 --> 01:20:17.930
this do we need to regulate them do we
need to stop them maybe they're moving

01:20:17.930 --> 01:20:17.940
need to stop them maybe they're moving
 

01:20:17.940 --> 01:20:18.560
need to stop them maybe they're moving
too fast

01:20:18.560 --> 01:20:18.570
too fast
 

01:20:18.570 --> 01:20:19.479
too fast
I see

01:20:19.479 --> 01:20:19.489
I see
 

01:20:19.489 --> 01:20:23.260
I see
and okay you you spoke about a concern

01:20:23.260 --> 01:20:23.270
and okay you you spoke about a concern
 

01:20:23.270 --> 01:20:25.000
and okay you you spoke about a concern
you have that there may be you know

01:20:25.000 --> 01:20:25.010
you have that there may be you know
 

01:20:25.010 --> 01:20:27.610
you have that there may be you know
cases that end up before a court that's

01:20:27.610 --> 01:20:27.620
cases that end up before a court that's
 

01:20:27.620 --> 01:20:30.040
cases that end up before a court that's
not particularly well prepared you do

01:20:30.040 --> 01:20:30.050
not particularly well prepared you do
 

01:20:30.050 --> 01:20:32.350
not particularly well prepared you do
you see a particular way to prepare for

01:20:32.350 --> 01:20:32.360
you see a particular way to prepare for
 

01:20:32.360 --> 01:20:34.540
you see a particular way to prepare for
that materiality or stop it is that one

01:20:34.540 --> 01:20:34.550
that materiality or stop it is that one
 

01:20:34.550 --> 01:20:37.780
that materiality or stop it is that one
of the goal or preventive I suppose it's

01:20:37.780 --> 01:20:37.790
of the goal or preventive I suppose it's
 

01:20:37.790 --> 01:20:39.640
of the goal or preventive I suppose it's
not currently a project that we're

01:20:39.640 --> 01:20:39.650
not currently a project that we're
 

01:20:39.650 --> 01:20:41.560
not currently a project that we're
running at the forum but it is something

01:20:41.560 --> 01:20:41.570
running at the forum but it is something
 

01:20:41.570 --> 01:20:44.080
running at the forum but it is something
that I worked with on one of the Supreme

01:20:44.080 --> 01:20:44.090
that I worked with on one of the Supreme
 

01:20:44.090 --> 01:20:46.860
that I worked with on one of the Supreme
Court justices in the United Kingdom to

01:20:46.860 --> 01:20:46.870
Court justices in the United Kingdom to
 

01:20:46.870 --> 01:20:52.540
Court justices in the United Kingdom to
begin to educate lawyers and judges and

01:20:52.540 --> 01:20:52.550
begin to educate lawyers and judges and
 

01:20:52.550 --> 01:20:56.370
begin to educate lawyers and judges and
the Royal Society actually has a

01:20:56.370 --> 01:20:56.380
the Royal Society actually has a
 

01:20:56.380 --> 01:21:00.970
the Royal Society actually has a
training group that teaches judges about

01:21:00.970 --> 01:21:00.980
training group that teaches judges about
 

01:21:00.980 --> 01:21:04.540
training group that teaches judges about
technical issues and so I think there is

01:21:04.540 --> 01:21:04.550
technical issues and so I think there is
 

01:21:04.550 --> 01:21:08.080
technical issues and so I think there is
some level of preparation that I know of

01:21:08.080 --> 01:21:08.090
some level of preparation that I know of
 

01:21:08.090 --> 01:21:14.400
some level of preparation that I know of
in the UK but I don't have anything and

01:21:14.400 --> 01:21:14.410
in the UK but I don't have anything and
 

01:21:14.410 --> 01:21:17.320
in the UK but I don't have anything and
the forum is working with governments

01:21:17.320 --> 01:21:17.330
the forum is working with governments
 

01:21:17.330 --> 01:21:19.150
the forum is working with governments
but there are also companies involved is

01:21:19.150 --> 01:21:19.160
but there are also companies involved is
 

01:21:19.160 --> 01:21:21.310
but there are also companies involved is
that is that right what what is driving

01:21:21.310 --> 01:21:21.320
that is that right what what is driving
 

01:21:21.320 --> 01:21:25.200
that is that right what what is driving
companies to come come forward is is it

01:21:25.200 --> 01:21:25.210
companies to come come forward is is it
 

01:21:25.210 --> 01:21:27.729
companies to come come forward is is it
you know sometimes people talk about a

01:21:27.729 --> 01:21:27.739
you know sometimes people talk about a
 

01:21:27.739 --> 01:21:30.790
you know sometimes people talk about a
classic race to hear to self-regulate

01:21:30.790 --> 01:21:30.800
classic race to hear to self-regulate
 

01:21:30.800 --> 01:21:33.640
classic race to hear to self-regulate
before you know state regulation comes

01:21:33.640 --> 01:21:33.650
before you know state regulation comes
 

01:21:33.650 --> 01:21:36.190
before you know state regulation comes
into play is that that driver or is it

01:21:36.190 --> 01:21:36.200
into play is that that driver or is it
 

01:21:36.200 --> 01:21:38.800
into play is that that driver or is it
people adding concern about these topics

01:21:38.800 --> 01:21:38.810
people adding concern about these topics
 

01:21:38.810 --> 01:21:42.640
people adding concern about these topics
to the corporate responsibility you know

01:21:42.640 --> 01:21:42.650
to the corporate responsibility you know
 

01:21:42.650 --> 01:21:45.700
to the corporate responsibility you know
manifesto well the companies come from

01:21:45.700 --> 01:21:45.710
manifesto well the companies come from
 

01:21:45.710 --> 01:21:48.900
manifesto well the companies come from
very diverse countries there we have

01:21:48.900 --> 01:21:48.910
very diverse countries there we have
 

01:21:48.910 --> 01:21:52.120
very diverse countries there we have
companies from countries around the

01:21:52.120 --> 01:21:52.130
companies from countries around the
 

01:21:52.130 --> 01:21:55.270
companies from countries around the
world from South America from Africa

01:21:55.270 --> 01:21:55.280
world from South America from Africa
 

01:21:55.280 --> 01:22:00.420
world from South America from Africa
from China from Japan Europe America and

01:22:00.420 --> 01:22:00.430
from China from Japan Europe America and
 

01:22:00.430 --> 01:22:03.720
from China from Japan Europe America and
I will actually have 15 different

01:22:03.720 --> 01:22:03.730
I will actually have 15 different
 

01:22:03.730 --> 01:22:08.190
I will actually have 15 different
businesses working on my projects and

01:22:08.190 --> 01:22:08.200
businesses working on my projects and
 

01:22:08.200 --> 01:22:14.010
businesses working on my projects and
they are coming because the AI is

01:22:14.010 --> 01:22:14.020
they are coming because the AI is
 

01:22:14.020 --> 01:22:17.200
they are coming because the AI is
something that and emerging technologies

01:22:17.200 --> 01:22:17.210
something that and emerging technologies
 

01:22:17.210 --> 01:22:19.420
something that and emerging technologies
governments of emerging technologies is

01:22:19.420 --> 01:22:19.430
governments of emerging technologies is
 

01:22:19.430 --> 01:22:21.160
governments of emerging technologies is
something that they are now thinking

01:22:21.160 --> 01:22:21.170
something that they are now thinking
 

01:22:21.170 --> 01:22:25.630
something that they are now thinking
very deeply about and as I say if you're

01:22:25.630 --> 01:22:25.640
very deeply about and as I say if you're
 

01:22:25.640 --> 01:22:28.900
very deeply about and as I say if you're
thinking if you think of companies in

01:22:28.900 --> 01:22:28.910
thinking if you think of companies in
 

01:22:28.910 --> 01:22:31.380
thinking if you think of companies in
Japan for example they're thinking about

01:22:31.380 --> 01:22:31.390
Japan for example they're thinking about
 

01:22:31.390 --> 01:22:46.590
Japan for example they're thinking about
how they can ramp up to address okay and

01:22:46.590 --> 01:22:46.600
how they can ramp up to address okay and
 

01:22:46.600 --> 01:22:48.420
how they can ramp up to address okay and
one of the governance mechanisms that's

01:22:48.420 --> 01:22:48.430
one of the governance mechanisms that's
 

01:22:48.430 --> 01:22:50.100
one of the governance mechanisms that's
forming is it is a kind of

01:22:50.100 --> 01:22:50.110
forming is it is a kind of
 

01:22:50.110 --> 01:22:52.620
forming is it is a kind of
self-regulation and the partnership on

01:22:52.620 --> 01:22:52.630
self-regulation and the partnership on
 

01:22:52.630 --> 01:22:57.750
self-regulation and the partnership on
AI I has been discussed on this stage we

01:22:57.750 --> 01:22:57.760
AI I has been discussed on this stage we
 

01:22:57.760 --> 01:22:59.490
AI I has been discussed on this stage we
have tried that before on technical

01:22:59.490 --> 01:22:59.500
have tried that before on technical
 

01:22:59.500 --> 01:23:02.220
have tried that before on technical
topics haven't we I wonder if you know

01:23:02.220 --> 01:23:02.230
topics haven't we I wonder if you know
 

01:23:02.230 --> 01:23:03.720
topics haven't we I wonder if you know
it might reflect a little bit on on that

01:23:03.720 --> 01:23:03.730
it might reflect a little bit on on that
 

01:23:03.730 --> 01:23:04.860
it might reflect a little bit on on that
you know there has been a degree of

01:23:04.860 --> 01:23:04.870
you know there has been a degree of
 

01:23:04.870 --> 01:23:07.080
you know there has been a degree of
self-regulation around privacy in the US

01:23:07.080 --> 01:23:07.090
self-regulation around privacy in the US
 

01:23:07.090 --> 01:23:10.590
self-regulation around privacy in the US
for example yes as I said we don't have

01:23:10.590 --> 01:23:10.600
for example yes as I said we don't have
 

01:23:10.600 --> 01:23:13.170
for example yes as I said we don't have
a lot of privacy laws in the u.s. and we

01:23:13.170 --> 01:23:13.180
a lot of privacy laws in the u.s. and we
 

01:23:13.180 --> 01:23:15.080
a lot of privacy laws in the u.s. and we
have relied very heavily on

01:23:15.080 --> 01:23:15.090
have relied very heavily on
 

01:23:15.090 --> 01:23:19.340
have relied very heavily on
self-regulation in the privacy space and

01:23:19.340 --> 01:23:19.350
self-regulation in the privacy space and
 

01:23:19.350 --> 01:23:22.020
self-regulation in the privacy space and
I've been involved in it since the late

01:23:22.020 --> 01:23:22.030
I've been involved in it since the late
 

01:23:22.030 --> 01:23:26.000
I've been involved in it since the late
1990s it's been rather frustrating that

01:23:26.000 --> 01:23:26.010
1990s it's been rather frustrating that
 

01:23:26.010 --> 01:23:29.760
1990s it's been rather frustrating that
we have industry and advocates come

01:23:29.760 --> 01:23:29.770
we have industry and advocates come
 

01:23:29.770 --> 01:23:33.330
we have industry and advocates come
together and come up with privacy self

01:23:33.330 --> 01:23:33.340
together and come up with privacy self
 

01:23:33.340 --> 01:23:36.840
together and come up with privacy self
regulatory programs and then the

01:23:36.840 --> 01:23:36.850
regulatory programs and then the
 

01:23:36.850 --> 01:23:39.300
regulatory programs and then the
incentives don't actually exist for them

01:23:39.300 --> 01:23:39.310
incentives don't actually exist for them
 

01:23:39.310 --> 01:23:41.580
incentives don't actually exist for them
to become widely adopted and we keep

01:23:41.580 --> 01:23:41.590
to become widely adopted and we keep
 

01:23:41.590 --> 01:23:43.320
to become widely adopted and we keep
seeing this cycle of people saying we

01:23:43.320 --> 01:23:43.330
seeing this cycle of people saying we
 

01:23:43.330 --> 01:23:44.790
seeing this cycle of people saying we
have a problem let's come up with a

01:23:44.790 --> 01:23:44.800
have a problem let's come up with a
 

01:23:44.800 --> 01:23:46.500
have a problem let's come up with a
solution we come up with a solution we

01:23:46.500 --> 01:23:46.510
solution we come up with a solution we
 

01:23:46.510 --> 01:23:48.660
solution we come up with a solution we
issue a press release we celebrate and

01:23:48.660 --> 01:23:48.670
issue a press release we celebrate and
 

01:23:48.670 --> 01:23:50.220
issue a press release we celebrate and
nobody does anything and then a few

01:23:50.220 --> 01:23:50.230
nobody does anything and then a few
 

01:23:50.230 --> 01:23:52.740
nobody does anything and then a few
years later we do it all again so I

01:23:52.740 --> 01:23:52.750
years later we do it all again so I
 

01:23:52.750 --> 01:23:56.090
years later we do it all again so I
think there's great flexibility in

01:23:56.090 --> 01:23:56.100
think there's great flexibility in
 

01:23:56.100 --> 01:23:59.880
think there's great flexibility in
self-regulation and I think it's a

01:23:59.880 --> 01:23:59.890
self-regulation and I think it's a
 

01:23:59.890 --> 01:24:02.220
self-regulation and I think it's a
really good idea to be able to instead

01:24:02.220 --> 01:24:02.230
really good idea to be able to instead
 

01:24:02.230 --> 01:24:05.100
really good idea to be able to instead
of having people in Congress who don't

01:24:05.100 --> 01:24:05.110
of having people in Congress who don't
 

01:24:05.110 --> 01:24:06.960
of having people in Congress who don't
necessarily know a lot about the

01:24:06.960 --> 01:24:06.970
necessarily know a lot about the
 

01:24:06.970 --> 01:24:10.140
necessarily know a lot about the
technology decide let the experts come

01:24:10.140 --> 01:24:10.150
technology decide let the experts come
 

01:24:10.150 --> 01:24:12.660
technology decide let the experts come
together and develop self regulatory

01:24:12.660 --> 01:24:12.670
together and develop self regulatory
 

01:24:12.670 --> 01:24:15.330
together and develop self regulatory
programs but at the same time I think we

01:24:15.330 --> 01:24:15.340
programs but at the same time I think we
 

01:24:15.340 --> 01:24:18.240
programs but at the same time I think we
need some incentives so that they'll

01:24:18.240 --> 01:24:18.250
need some incentives so that they'll
 

01:24:18.250 --> 01:24:20.850
need some incentives so that they'll
actually be adopted okay

01:24:20.850 --> 01:24:20.860
actually be adopted okay
 

01:24:20.860 --> 01:24:24.830
actually be adopted okay
miss president our alternative model is

01:24:24.830 --> 01:24:24.840
miss president our alternative model is
 

01:24:24.840 --> 01:24:28.530
miss president our alternative model is
presented by gdpr which in the European

01:24:28.530 --> 01:24:28.540
presented by gdpr which in the European
 

01:24:28.540 --> 01:24:32.270
presented by gdpr which in the European
Union sets down some very broad rules

01:24:32.270 --> 01:24:32.280
Union sets down some very broad rules
 

01:24:32.280 --> 01:24:35.820
Union sets down some very broad rules
and it also kind of applies to AI or

01:24:35.820 --> 01:24:35.830
and it also kind of applies to AI or
 

01:24:35.830 --> 01:24:37.260
and it also kind of applies to AI or
some application machining as well

01:24:37.260 --> 01:24:37.270
some application machining as well
 

01:24:37.270 --> 01:24:41.400
some application machining as well
doesn't it yeah yeah so the European

01:24:41.400 --> 01:24:41.410
doesn't it yeah yeah so the European
 

01:24:41.410 --> 01:24:45.210
doesn't it yeah yeah so the European
privacy laws are very broad and they

01:24:45.210 --> 01:24:45.220
privacy laws are very broad and they
 

01:24:45.220 --> 01:24:49.100
privacy laws are very broad and they
have lots of very detailed provisions

01:24:49.100 --> 01:24:49.110
have lots of very detailed provisions
 

01:24:49.110 --> 01:24:51.690
have lots of very detailed provisions
it's not entirely clear what they mean

01:24:51.690 --> 01:24:51.700
it's not entirely clear what they mean
 

01:24:51.700 --> 01:24:54.030
it's not entirely clear what they mean
yet we'll find out when they start

01:24:54.030 --> 01:24:54.040
yet we'll find out when they start
 

01:24:54.040 --> 01:24:56.820
yet we'll find out when they start
enforcing it but I think there there are

01:24:56.820 --> 01:24:56.830
enforcing it but I think there there are
 

01:24:56.830 --> 01:24:58.050
enforcing it but I think there there are
parts of it that are very well

01:24:58.050 --> 01:24:58.060
parts of it that are very well
 

01:24:58.060 --> 01:25:01.140
parts of it that are very well
intentioned and I agree with their goal

01:25:01.140 --> 01:25:01.150
intentioned and I agree with their goal
 

01:25:01.150 --> 01:25:03.630
intentioned and I agree with their goal
to protect privacy but I disagree with

01:25:03.630 --> 01:25:03.640
to protect privacy but I disagree with
 

01:25:03.640 --> 01:25:05.940
to protect privacy but I disagree with
some of the mechanisms that are written

01:25:05.940 --> 01:25:05.950
some of the mechanisms that are written
 

01:25:05.950 --> 01:25:08.550
some of the mechanisms that are written
into the law that I think practically

01:25:08.550 --> 01:25:08.560
into the law that I think practically
 

01:25:08.560 --> 01:25:10.970
into the law that I think practically
don't make a lot of sense for really

01:25:10.970 --> 01:25:10.980
don't make a lot of sense for really
 

01:25:10.980 --> 01:25:13.920
don't make a lot of sense for really
protecting privacy and so I think that

01:25:13.920 --> 01:25:13.930
protecting privacy and so I think that
 

01:25:13.930 --> 01:25:17.880
protecting privacy and so I think that
that's a concern okay okay presumably

01:25:17.880 --> 01:25:17.890
that's a concern okay okay presumably
 

01:25:17.890 --> 01:25:22.200
that's a concern okay okay presumably
you're quite familiar with gdpr and so

01:25:22.200 --> 01:25:22.210
you're quite familiar with gdpr and so
 

01:25:22.210 --> 01:25:25.860
you're quite familiar with gdpr and so
far as anyone can be here this I wonder

01:25:25.860 --> 01:25:25.870
far as anyone can be here this I wonder
 

01:25:25.870 --> 01:25:27.830
far as anyone can be here this I wonder
are there things in there that might be

01:25:27.830 --> 01:25:27.840
are there things in there that might be
 

01:25:27.840 --> 01:25:30.690
are there things in there that might be
useful inspiration to people who are

01:25:30.690 --> 01:25:30.700
useful inspiration to people who are
 

01:25:30.700 --> 01:25:34.740
useful inspiration to people who are
thinking about governance of AI well as

01:25:34.740 --> 01:25:34.750
thinking about governance of AI well as
 

01:25:34.750 --> 01:25:36.870
thinking about governance of AI well as
I said I think one of the things that is

01:25:36.870 --> 01:25:36.880
I said I think one of the things that is
 

01:25:36.880 --> 01:25:39.510
I said I think one of the things that is
interesting about the G GPR is is that

01:25:39.510 --> 01:25:39.520
interesting about the G GPR is is that
 

01:25:39.520 --> 01:25:43.860
interesting about the G GPR is is that
it does show the values the value set

01:25:43.860 --> 01:25:43.870
it does show the values the value set
 

01:25:43.870 --> 01:25:46.230
it does show the values the value set
and say you're looking at sort of value

01:25:46.230 --> 01:25:46.240
and say you're looking at sort of value
 

01:25:46.240 --> 01:25:50.880
and say you're looking at sort of value
alignment governance there I am NOT an

01:25:50.880 --> 01:25:50.890
alignment governance there I am NOT an
 

01:25:50.890 --> 01:25:56.460
alignment governance there I am NOT an
expert on GDP that's actually somebody

01:25:56.460 --> 01:25:56.470
expert on GDP that's actually somebody
 

01:25:56.470 --> 01:25:59.640
expert on GDP that's actually somebody
who's head of cross-border data players

01:25:59.640 --> 01:25:59.650
who's head of cross-border data players
 

01:25:59.650 --> 01:26:05.160
who's head of cross-border data players
at the at the Forum but it's it's games

01:26:05.160 --> 01:26:05.170
at the at the Forum but it's it's games
 

01:26:05.170 --> 01:26:07.290
at the at the Forum but it's it's games
I think it's interesting because as

01:26:07.290 --> 01:26:07.300
I think it's interesting because as
 

01:26:07.300 --> 01:26:09.660
I think it's interesting because as
companies around the world gear up to

01:26:09.660 --> 01:26:09.670
companies around the world gear up to
 

01:26:09.670 --> 01:26:12.200
companies around the world gear up to
deal with it they're making to perhaps

01:26:12.200 --> 01:26:12.210
deal with it they're making to perhaps
 

01:26:12.210 --> 01:26:14.850
deal with it they're making to perhaps
find that they're incorporating that in

01:26:14.850 --> 01:26:14.860
find that they're incorporating that in
 

01:26:14.860 --> 01:26:17.220
find that they're incorporating that in
their own self regulatory mechanisms so

01:26:17.220 --> 01:26:17.230
their own self regulatory mechanisms so
 

01:26:17.230 --> 01:26:18.420
their own self regulatory mechanisms so
I think it's interesting from that

01:26:18.420 --> 01:26:18.430
I think it's interesting from that
 

01:26:18.430 --> 01:26:20.370
I think it's interesting from that
perspective and I think there's been a

01:26:20.370 --> 01:26:20.380
perspective and I think there's been a
 

01:26:20.380 --> 01:26:22.640
perspective and I think there's been a
question actually as to whether gdpr

01:26:22.640 --> 01:26:22.650
question actually as to whether gdpr
 

01:26:22.650 --> 01:26:24.900
question actually as to whether gdpr
requires things like algorithmic

01:26:24.900 --> 01:26:24.910
requires things like algorithmic
 

01:26:24.910 --> 01:26:27.210
requires things like algorithmic
transparency and I've seen debates about

01:26:27.210 --> 01:26:27.220
transparency and I've seen debates about
 

01:26:27.220 --> 01:26:28.950
transparency and I've seen debates about
that and I'm not enough of an expert in

01:26:28.950 --> 01:26:28.960
that and I'm not enough of an expert in
 

01:26:28.960 --> 01:26:34.830
that and I'm not enough of an expert in
gdpr to really weigh it's probably good

01:26:34.830 --> 01:26:34.840
gdpr to really weigh it's probably good
 

01:26:34.840 --> 01:26:35.880
gdpr to really weigh it's probably good
for the audience there were know about

01:26:35.880 --> 01:26:35.890
for the audience there were know about
 

01:26:35.890 --> 01:26:40.890
for the audience there were know about
to get into that but the point you made

01:26:40.890 --> 01:26:40.900
to get into that but the point you made
 

01:26:40.900 --> 01:26:42.390
to get into that but the point you made
then about transparency and also in your

01:26:42.390 --> 01:26:42.400
then about transparency and also in your
 

01:26:42.400 --> 01:26:44.600
then about transparency and also in your
your talk and I think you touched on it

01:26:44.600 --> 01:26:44.610
your talk and I think you touched on it
 

01:26:44.610 --> 01:26:48.590
your talk and I think you touched on it
okay is that some of the things we have

01:26:48.590 --> 01:26:48.600
okay is that some of the things we have
 

01:26:48.600 --> 01:26:49.790
okay is that some of the things we have
done in the past where governance will

01:26:49.790 --> 01:26:49.800
done in the past where governance will
 

01:26:49.800 --> 01:26:51.620
done in the past where governance will
apply in this area but it seems like

01:26:51.620 --> 01:26:51.630
apply in this area but it seems like
 

01:26:51.630 --> 01:26:54.649
apply in this area but it seems like
there are some characteristics of

01:26:54.649 --> 01:26:54.659
there are some characteristics of
 

01:26:54.659 --> 01:26:56.149
there are some characteristics of
machine learning and AI based

01:26:56.149 --> 01:26:56.159
machine learning and AI based
 

01:26:56.159 --> 01:26:57.649
machine learning and AI based
technologies that are very different and

01:26:57.649 --> 01:26:57.659
technologies that are very different and
 

01:26:57.659 --> 01:26:59.810
technologies that are very different and
they're challenging and so you mentioned

01:26:59.810 --> 01:26:59.820
they're challenging and so you mentioned
 

01:26:59.820 --> 01:27:02.810
they're challenging and so you mentioned
Laurie that you know you want to ask

01:27:02.810 --> 01:27:02.820
Laurie that you know you want to ask
 

01:27:02.820 --> 01:27:04.189
Laurie that you know you want to ask
people to be transparent about how

01:27:04.189 --> 01:27:04.199
people to be transparent about how
 

01:27:04.199 --> 01:27:06.350
people to be transparent about how
things work but you also had a carve out

01:27:06.350 --> 01:27:06.360
things work but you also had a carve out
 

01:27:06.360 --> 01:27:08.810
things work but you also had a carve out
saying you know we accept that sometimes

01:27:08.810 --> 01:27:08.820
saying you know we accept that sometimes
 

01:27:08.820 --> 01:27:10.640
saying you know we accept that sometimes
you won't know exactly or you won't be

01:27:10.640 --> 01:27:10.650
you won't know exactly or you won't be
 

01:27:10.650 --> 01:27:12.530
you won't know exactly or you won't be
able to exactly explain it right so

01:27:12.530 --> 01:27:12.540
able to exactly explain it right so
 

01:27:12.540 --> 01:27:14.240
able to exactly explain it right so
that's kind of that's the challenge

01:27:14.240 --> 01:27:14.250
that's kind of that's the challenge
 

01:27:14.250 --> 01:27:16.390
that's kind of that's the challenge
isn't it yeah I mean I think currently

01:27:16.390 --> 01:27:16.400
isn't it yeah I mean I think currently
 

01:27:16.400 --> 01:27:20.390
isn't it yeah I mean I think currently
we we often can't fully explain the

01:27:20.390 --> 01:27:20.400
we we often can't fully explain the
 

01:27:20.400 --> 01:27:23.209
we we often can't fully explain the
decisions that AI is making and they're

01:27:23.209 --> 01:27:23.219
decisions that AI is making and they're
 

01:27:23.219 --> 01:27:25.070
decisions that AI is making and they're
people who are working on trying to do

01:27:25.070 --> 01:27:25.080
people who are working on trying to do
 

01:27:25.080 --> 01:27:27.379
people who are working on trying to do
it and I think they'll get us closer

01:27:27.379 --> 01:27:27.389
it and I think they'll get us closer
 

01:27:27.389 --> 01:27:30.500
it and I think they'll get us closer
there but I I think ultimately we're not

01:27:30.500 --> 01:27:30.510
there but I I think ultimately we're not
 

01:27:30.510 --> 01:27:32.660
there but I I think ultimately we're not
gonna be able to explain everything as

01:27:32.660 --> 01:27:32.670
gonna be able to explain everything as
 

01:27:32.670 --> 01:27:35.149
gonna be able to explain everything as
much as we would like okay is that is

01:27:35.149 --> 01:27:35.159
much as we would like okay is that is
 

01:27:35.159 --> 01:27:37.490
much as we would like okay is that is
that a challenge or a problem I mean you

01:27:37.490 --> 01:27:37.500
that a challenge or a problem I mean you
 

01:27:37.500 --> 01:27:38.479
that a challenge or a problem I mean you
know people talk about the black box

01:27:38.479 --> 01:27:38.489
know people talk about the black box
 

01:27:38.489 --> 01:27:40.550
know people talk about the black box
problem and and suggest that there are

01:27:40.550 --> 01:27:40.560
problem and and suggest that there are
 

01:27:40.560 --> 01:27:41.990
problem and and suggest that there are
certain things we can't use these

01:27:41.990 --> 01:27:42.000
certain things we can't use these
 

01:27:42.000 --> 01:27:44.300
certain things we can't use these
systems for until we are able to explain

01:27:44.300 --> 01:27:44.310
systems for until we are able to explain
 

01:27:44.310 --> 01:27:47.899
systems for until we are able to explain
fully well so I think this the ACM

01:27:47.899 --> 01:27:47.909
fully well so I think this the ACM
 

01:27:47.909 --> 01:27:49.790
fully well so I think this the ACM
principle suggests there are ways of

01:27:49.790 --> 01:27:49.800
principle suggests there are ways of
 

01:27:49.800 --> 01:27:52.129
principle suggests there are ways of
having accountability even if you can't

01:27:52.129 --> 01:27:52.139
having accountability even if you can't
 

01:27:52.139 --> 01:27:54.439
having accountability even if you can't
fully explain everything that's going on

01:27:54.439 --> 01:27:54.449
fully explain everything that's going on
 

01:27:54.449 --> 01:27:57.919
fully explain everything that's going on
inside the black box and so you know the

01:27:57.919 --> 01:27:57.929
inside the black box and so you know the
 

01:27:57.929 --> 01:28:00.169
inside the black box and so you know the
approach of throwing a lot of inputs at

01:28:00.169 --> 01:28:00.179
approach of throwing a lot of inputs at
 

01:28:00.179 --> 01:28:02.780
approach of throwing a lot of inputs at
it and seeing what comes out and being

01:28:02.780 --> 01:28:02.790
it and seeing what comes out and being
 

01:28:02.790 --> 01:28:04.490
it and seeing what comes out and being
able to look statistically these kind of

01:28:04.490 --> 01:28:04.500
able to look statistically these kind of
 

01:28:04.500 --> 01:28:07.550
able to look statistically these kind of
inputs drive these kind of outputs may

01:28:07.550 --> 01:28:07.560
inputs drive these kind of outputs may
 

01:28:07.560 --> 01:28:09.109
inputs drive these kind of outputs may
be satisfactory

01:28:09.109 --> 01:28:09.119
be satisfactory
 

01:28:09.119 --> 01:28:11.689
be satisfactory
in some cases for providing enough of an

01:28:11.689 --> 01:28:11.699
in some cases for providing enough of an
 

01:28:11.699 --> 01:28:14.030
in some cases for providing enough of an
explanation as to what's going on I see

01:28:14.030 --> 01:28:14.040
explanation as to what's going on I see
 

01:28:14.040 --> 01:28:16.490
explanation as to what's going on I see
I think it's probably interesting the

01:28:16.490 --> 01:28:16.500
I think it's probably interesting the
 

01:28:16.500 --> 01:28:19.220
I think it's probably interesting the
structure to just go back to one of the

01:28:19.220 --> 01:28:19.230
structure to just go back to one of the
 

01:28:19.230 --> 01:28:20.450
structure to just go back to one of the
things I mentioned about agile

01:28:20.450 --> 01:28:20.460
things I mentioned about agile
 

01:28:20.460 --> 01:28:22.939
things I mentioned about agile
governance and that's a standard setting

01:28:22.939 --> 01:28:22.949
governance and that's a standard setting
 

01:28:22.949 --> 01:28:27.830
governance and that's a standard setting
and it may be useful to know that the I

01:28:27.830 --> 01:28:27.840
and it may be useful to know that the I
 

01:28:27.840 --> 01:28:30.229
and it may be useful to know that the I
Triple E is actually working on the

01:28:30.229 --> 01:28:30.239
Triple E is actually working on the
 

01:28:30.239 --> 01:28:34.310
Triple E is actually working on the
standard to require transparency and so

01:28:34.310 --> 01:28:34.320
standard to require transparency and so
 

01:28:34.320 --> 01:28:38.419
standard to require transparency and so
that follows up on some of the ACM

01:28:38.419 --> 01:28:38.429
that follows up on some of the ACM
 

01:28:38.429 --> 01:28:42.080
that follows up on some of the ACM
things so standard setting in this space

01:28:42.080 --> 01:28:42.090
things so standard setting in this space
 

01:28:42.090 --> 01:28:43.660
things so standard setting in this space
is something that we should be watching

01:28:43.660 --> 01:28:43.670
is something that we should be watching
 

01:28:43.670 --> 01:28:46.820
is something that we should be watching
okay and I think you alluded to this Kay

01:28:46.820 --> 01:28:46.830
okay and I think you alluded to this Kay
 

01:28:46.830 --> 01:28:49.100
okay and I think you alluded to this Kay
that there's also this question of these

01:28:49.100 --> 01:28:49.110
that there's also this question of these
 

01:28:49.110 --> 01:28:51.890
that there's also this question of these
systems you know you may develop and

01:28:51.890 --> 01:28:51.900
systems you know you may develop and
 

01:28:51.900 --> 01:28:53.600
systems you know you may develop and
test it and then deploy it and it may

01:28:53.600 --> 01:28:53.610
test it and then deploy it and it may
 

01:28:53.610 --> 01:28:55.000
test it and then deploy it and it may
continue to change

01:28:55.000 --> 01:28:55.010
continue to change
 

01:28:55.010 --> 01:28:57.850
continue to change
behavior it may be may have a very

01:28:57.850 --> 01:28:57.860
behavior it may be may have a very
 

01:28:57.860 --> 01:28:59.109
behavior it may be may have a very
different behavior for different people

01:28:59.109 --> 01:28:59.119
different behavior for different people
 

01:28:59.119 --> 01:29:01.149
different behavior for different people
for example over time and I guess that's

01:29:01.149 --> 01:29:01.159
for example over time and I guess that's
 

01:29:01.159 --> 01:29:03.100
for example over time and I guess that's
a challenge but for government

01:29:03.100 --> 01:29:03.110
a challenge but for government
 

01:29:03.110 --> 01:29:07.060
a challenge but for government
structures as well yes absolutely it's

01:29:07.060 --> 01:29:07.070
structures as well yes absolutely it's
 

01:29:07.070 --> 01:29:09.189
structures as well yes absolutely it's
such a it's a challenge for all

01:29:09.189 --> 01:29:09.199
such a it's a challenge for all
 

01:29:09.199 --> 01:29:13.029
such a it's a challenge for all
government structures so just as much as

01:29:13.029 --> 01:29:13.039
government structures so just as much as
 

01:29:13.039 --> 01:29:15.879
government structures so just as much as
it's a challenge for how do we define it

01:29:15.879 --> 01:29:15.889
it's a challenge for how do we define it
 

01:29:15.889 --> 01:29:18.790
it's a challenge for how do we define it
and legislate against it it's also a

01:29:18.790 --> 01:29:18.800
and legislate against it it's also a
 

01:29:18.800 --> 01:29:20.979
and legislate against it it's also a
problem for other forms of agile

01:29:20.979 --> 01:29:20.989
problem for other forms of agile
 

01:29:20.989 --> 01:29:23.049
problem for other forms of agile
governance and I it's going to be

01:29:23.049 --> 01:29:23.059
governance and I it's going to be
 

01:29:23.059 --> 01:29:25.990
governance and I it's going to be
something that we find as we move

01:29:25.990 --> 01:29:26.000
something that we find as we move
 

01:29:26.000 --> 01:29:27.939
something that we find as we move
forward say for example one of my

01:29:27.939 --> 01:29:27.949
forward say for example one of my
 

01:29:27.949 --> 01:29:30.459
forward say for example one of my
students when I was teaching this as a

01:29:30.459 --> 01:29:30.469
students when I was teaching this as a
 

01:29:30.469 --> 01:29:34.060
students when I was teaching this as a
law class came up with the idea that

01:29:34.060 --> 01:29:34.070
law class came up with the idea that
 

01:29:34.070 --> 01:29:37.359
law class came up with the idea that
perhaps one would have a robot who would

01:29:37.359 --> 01:29:37.369
perhaps one would have a robot who would
 

01:29:37.369 --> 01:29:41.830
perhaps one would have a robot who would
be taught to be sexist and by its

01:29:41.830 --> 01:29:41.840
be taught to be sexist and by its
 

01:29:41.840 --> 01:29:45.370
be taught to be sexist and by its
ultimate owner and then it goes ahead

01:29:45.370 --> 01:29:45.380
ultimate owner and then it goes ahead
 

01:29:45.380 --> 01:29:48.819
ultimate owner and then it goes ahead
and kills somebody because of that who

01:29:48.819 --> 01:29:48.829
and kills somebody because of that who
 

01:29:48.829 --> 01:29:50.890
and kills somebody because of that who
you know where do you where do you

01:29:50.890 --> 01:29:50.900
you know where do you where do you
 

01:29:50.900 --> 01:29:52.750
you know where do you where do you
create where do you put the blame or on

01:29:52.750 --> 01:29:52.760
create where do you put the blame or on
 

01:29:52.760 --> 01:29:54.310
create where do you put the blame or on
the safe software manufacturer the

01:29:54.310 --> 01:29:54.320
the safe software manufacturer the
 

01:29:54.320 --> 01:29:57.430
the safe software manufacturer the
hardware manufacturer and we weave those

01:29:57.430 --> 01:29:57.440
hardware manufacturer and we weave those
 

01:29:57.440 --> 01:29:59.709
hardware manufacturer and we weave those
things we're still grappling with ok

01:29:59.709 --> 01:29:59.719
things we're still grappling with ok
 

01:29:59.719 --> 01:30:01.299
things we're still grappling with ok
well I'm afraid that's we have time for

01:30:01.299 --> 01:30:01.309
well I'm afraid that's we have time for
 

01:30:01.309 --> 01:30:08.290
well I'm afraid that's we have time for
please thank Lori and Kay

01:30:08.290 --> 01:30:08.300
 

01:30:08.300 --> 01:30:11.070
and now we'll hand back over to David

01:30:11.070 --> 01:30:11.080
and now we'll hand back over to David
 

01:30:11.080 --> 01:30:13.780
and now we'll hand back over to David
thank you Tom Laurie and Kato is really

01:30:13.780 --> 01:30:13.790
thank you Tom Laurie and Kato is really
 

01:30:13.790 --> 01:30:15.400
thank you Tom Laurie and Kato is really
great and it was a it was wonderful to

01:30:15.400 --> 01:30:15.410
great and it was a it was wonderful to
 

01:30:15.410 --> 01:30:18.550
great and it was a it was wonderful to
see sort of the complexities at work

01:30:18.550 --> 01:30:18.560
see sort of the complexities at work
 

01:30:18.560 --> 01:30:20.380
see sort of the complexities at work
right the sorts of things that onion

01:30:20.380 --> 01:30:20.390
right the sorts of things that onion
 

01:30:20.390 --> 01:30:21.580
right the sorts of things that onion
Amandeep laid out is these real

01:30:21.580 --> 01:30:21.590
Amandeep laid out is these real
 

01:30:21.590 --> 01:30:22.870
Amandeep laid out is these real
challenges that we're gonna have to

01:30:22.870 --> 01:30:22.880
challenges that we're gonna have to
 

01:30:22.880 --> 01:30:23.980
challenges that we're gonna have to
wrestle with on the governance

01:30:23.980 --> 01:30:23.990
wrestle with on the governance
 

01:30:23.990 --> 01:30:26.500
wrestle with on the governance
perspective to see what it looks like as

01:30:26.500 --> 01:30:26.510
perspective to see what it looks like as
 

01:30:26.510 --> 01:30:29.260
perspective to see what it looks like as
it were on the ground all the challenges

01:30:29.260 --> 01:30:29.270
it were on the ground all the challenges
 

01:30:29.270 --> 01:30:31.150
it were on the ground all the challenges
that we have to try and struggle with so

01:30:31.150 --> 01:30:31.160
that we have to try and struggle with so
 

01:30:31.160 --> 01:30:34.150
that we have to try and struggle with so
this unfortunately we were supposed to

01:30:34.150 --> 01:30:34.160
this unfortunately we were supposed to
 

01:30:34.160 --> 01:30:35.350
this unfortunately we were supposed to
have one other person I did want to

01:30:35.350 --> 01:30:35.360
have one other person I did want to
 

01:30:35.360 --> 01:30:37.780
have one other person I did want to
mention Sola and Baracus from Cornell

01:30:37.780 --> 01:30:37.790
mention Sola and Baracus from Cornell
 

01:30:37.790 --> 01:30:40.000
mention Sola and Baracus from Cornell
University was unfortunately unable to

01:30:40.000 --> 01:30:40.010
University was unfortunately unable to
 

01:30:40.010 --> 01:30:43.630
University was unfortunately unable to
join us at the last minute and so we

01:30:43.630 --> 01:30:43.640
join us at the last minute and so we
 

01:30:43.640 --> 01:30:45.790
join us at the last minute and so we
were supposed to have another voice but

01:30:45.790 --> 01:30:45.800
were supposed to have another voice but
 

01:30:45.800 --> 01:30:47.590
were supposed to have another voice but
we'll have to wait until the next

01:30:47.590 --> 01:30:47.600
we'll have to wait until the next
 

01:30:47.600 --> 01:30:50.170
we'll have to wait until the next
conference I supposed to bring Solan so

01:30:50.170 --> 01:30:50.180
conference I supposed to bring Solan so
 

01:30:50.180 --> 01:30:51.760
conference I supposed to bring Solan so
this concludes this session on policy

01:30:51.760 --> 01:30:51.770
this concludes this session on policy
 

01:30:51.770 --> 01:30:54.370
this concludes this session on policy
and governance we have a coffee break

01:30:54.370 --> 01:30:54.380
and governance we have a coffee break
 

01:30:54.380 --> 01:30:57.580
and governance we have a coffee break
and a few refreshments in the back room

01:30:57.580 --> 01:30:57.590
and a few refreshments in the back room
 

01:30:57.590 --> 01:30:59.320
and a few refreshments in the back room
and we'll be starting the session on

01:30:59.320 --> 01:30:59.330
and we'll be starting the session on
 

01:30:59.330 --> 01:31:01.330
and we'll be starting the session on
agency and empowerment at two o'clock

01:31:01.330 --> 01:31:01.340
agency and empowerment at two o'clock
 

01:31:01.340 --> 01:31:03.250
agency and empowerment at two o'clock
thank you so much

01:31:03.250 --> 01:31:03.260
thank you so much
 

01:31:03.260 --> 01:31:09.180
thank you so much
[Music]

