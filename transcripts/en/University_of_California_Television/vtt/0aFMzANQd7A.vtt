WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.500
- [automated message] This UCSD TV program
is presented by University of California

00:00:05.680 --> 00:00:10.670
Television. Like what you learn? Visit our
website, or follow us on Facebook and

00:00:10.850 --> 00:00:14.053
Twitter to keep up with the latest
programs.

00:00:18.822 --> 00:00:21.700
♪ [music] ♪

00:00:23.090 --> 00:00:31.260
- [male] We are the paradoxical ape,
bi-pedal, naked, large brain, long the

00:00:31.440 --> 00:00:37.050
master of fire, tools, and language, but
still trying to understand ourselves.

00:00:37.230 --> 00:00:42.530
Aware that death is inevitable, yet filled
with optimism. We grow up slowly. We hand

00:00:42.710 --> 00:00:59.600
down knowledge. We empathize and deceive.
We shape the future from our shared

00:00:59.780 --> 00:01:03.040
understanding of the past. CARTA brings
together experts from diverse disciplines

00:01:03.220 --> 00:01:11.250
to exchange insights on who we are and how
we got here. An exploration made possible

00:01:11.430 --> 00:01:15.265
by the generosity of humans like you.

00:01:48.011 --> 00:01:50.472
♪ [music] ♪

00:02:33.400 --> 00:02:37.110
- [Evelina] Thanks very much for having me
here. So today, I will tell you about the

00:02:37.290 --> 00:02:40.880
language system and some of its properties
that may bear on the question of how this

00:02:41.060 --> 00:02:44.840
system came about. I will begin by
defining the scope of my inquiry, because

00:02:45.020 --> 00:02:48.650
people mean many different things by
language. I will then present you some

00:02:48.830 --> 00:02:53.220
evidence, suggesting that our language
system is highly specialized for language.

00:02:53.400 --> 00:02:57.190
Finally I will speculate a bit on the
evolutionary origins of language.

00:02:57.370 --> 00:03:02.240
So linguistic input comes in through our
ears or our eyes. Once it's been analyzed

00:03:02.420 --> 00:03:06.120
perceptually, we interpret it by linking
the incoming representations toward our

00:03:06.300 --> 00:03:10.080
stored language knowledge. This knowledge,
of course, is also used for generating

00:03:10.260 --> 00:03:15.230
utterances during language production.
Once we've generated an utterance in our

00:03:15.410 --> 00:03:20.380
heads, we can send it down to our
articulation system. The part that I focus

00:03:20.560 --> 00:03:23.930
on is this kind of high-level component of
language processing.

00:03:24.110 --> 00:03:27.170
This cartoon shows you the approximate
locations of the brain regions that

00:03:27.350 --> 00:03:31.690
support speech perception, shown in
yellow, visual letter and word recognition

00:03:31.870 --> 00:03:37.020
in green. This region is known as the
visual word-form area. Articulation in

00:03:37.200 --> 00:03:39.860
pink and high-level language processing in
red.

00:03:40.040 --> 00:03:42.770
What differentiates the high-level
language processing regions from the

00:03:42.950 --> 00:03:46.030
perceptual and articulation regions is
that they're sensitive to the

00:03:46.210 --> 00:03:49.810
meaningfulness of the signal. So for
example, the speech regions will respond

00:03:49.990 --> 00:03:53.470
just as strongly to foreign speech as they
respond to meaningful speech in our own

00:03:53.650 --> 00:03:56.850
language. The visual word-form area
responds just as much to a string of

00:03:57.030 --> 00:04:00.870
consonants as it does to real words. The
articulation regions can be driven to a

00:04:01.050 --> 00:04:05.330
full extent by people producing
meaningless sequences of syllables.

00:04:05.510 --> 00:04:08.710
But in contrast, the high-level
language-processing system or network, or

00:04:08.890 --> 00:04:13.370
sometimes I refer to it just as the
language network for short, cares deeply

00:04:13.550 --> 00:04:16.630
about whether the linguistics signal is
meaningful or not.

00:04:16.810 --> 00:04:22.550
In fact, the easiest way to find this
system in the brain is to contrast

00:04:22.730 --> 00:04:27.000
responses to meaningful language stimuli,
like words, phrases, or sentences. Some

00:04:27.180 --> 00:04:30.470
control conditions like linguistically
degraded stimuli.

00:04:30.650 --> 00:04:34.860
The contrast I use most frequently is
between sentences and sequences of

00:04:35.040 --> 00:04:38.820
non-words. A key methodological innovation
that laid the foundation for much of my

00:04:39.000 --> 00:04:42.440
work was the development of tools that
enable us to define the regions of the

00:04:42.620 --> 00:04:46.250
language network, functionally at the
individual subject level, using contrasts

00:04:46.430 --> 00:04:50.810
like these. Here, I'm showing you sample
language regions in three individual

00:04:50.990 --> 00:04:56.170
brains. This so-called functional
localization approach has two advantages.

00:04:56.350 --> 00:05:00.640
One, it circumvents the need to average
brains together, which is what's done in

00:05:00.820 --> 00:05:03.990
the common approach, and it's a very
difficult thing to do because brains are

00:05:04.170 --> 00:05:08.170
quite different across people. Instead, in
this approach, we can just average the

00:05:08.350 --> 00:05:11.210
signals that we extract from these key
regions of interest.

00:05:11.390 --> 00:05:15.000
The second advantage is that it allows us
to establish a cumulative research

00:05:15.180 --> 00:05:19.700
enterprise, which I think we all agree is
important in science, because comparing

00:05:19.880 --> 00:05:23.230
results across studies and labs is quite
straightforward if we're confident that

00:05:23.410 --> 00:05:27.950
we're referring to the same brain regions
across different studies. This is just

00:05:28.130 --> 00:05:31.870
hard or impossible to do in the
traditional approach, which relies on very

00:05:32.050 --> 00:05:35.570
coarse anatomical landmarks, like the
inferior frontal gyrus or the superior

00:05:35.750 --> 00:05:39.710
temporal sulcus, which span many
centimeters of cortex and are just not at

00:05:39.890 --> 00:05:43.580
the right level of analysis.
So what drives me and my work is the

00:05:43.760 --> 00:05:47.170
desire to understand the nature of our
language knowledge and the computations

00:05:47.350 --> 00:05:51.240
that mediate language comprehension and
production. However, these questions are

00:05:51.420 --> 00:05:55.780
hard, especially given the lack of animal
models for language. So for now, I settle

00:05:55.960 --> 00:06:00.780
on more tractable questions. For example,
one, what is the relationship between the

00:06:00.960 --> 00:06:04.930
language system and the rest of human
cognition? Language didn't evolve, and it

00:06:05.110 --> 00:06:09.490
doesn't exist in isolation from other
evolutionarily older systems, which

00:06:09.670 --> 00:06:12.070
include the memory and attention
mechanisms, the visual and the motor

00:06:12.250 --> 00:06:16.570
systems, the system that supports social
cognition, and so on. That means that we

00:06:16.750 --> 00:06:21.280
just can't study language as an isolated
system. A lot of my research effort is

00:06:21.460 --> 00:06:24.070
aimed at trying to figure out how language
fits with the rest of our mind and brain.

00:06:24.250 --> 00:06:28.300
The second question delves inside the
language system, asking, "What does its

00:06:28.480 --> 00:06:32.870
internal architecture look like?" It
encompasses questions like, "What are the

00:06:33.050 --> 00:06:36.270
component parts of the language system?
And what is the division of labor among

00:06:36.450 --> 00:06:39.890
them, in space and time?"
Of course, both of those questions

00:06:40.070 --> 00:06:43.340
ultimately should constrain the space of
possibilities for how language actually

00:06:43.520 --> 00:06:48.010
works. So they place constraints on both
the data structures that underlie language

00:06:48.190 --> 00:06:51.540
and the computations that are likely
performed by the regions of the system.

00:06:51.720 --> 00:06:54.880
Today, I focus on the first of these
questions. Okay. So now onto some

00:06:55.060 --> 00:07:01.470
evidence. So the relationship between
language and the rest of the mind and

00:07:01.650 --> 00:07:05.870
brain has been long debated, and the
literature actually is quite abundant with

00:07:06.050 --> 00:07:09.790
claims that language makes use of the same
machinery that we use for performing other

00:07:09.970 --> 00:07:14.200
cognitive tasks, including arithmetic
processing, various kinds of executive

00:07:14.380 --> 00:07:19.550
function tasks, perceiving music,
perceiving actions, abstract conceptual

00:07:19.730 --> 00:07:23.330
processing, and so on.
I will argue that these claims are not

00:07:23.510 --> 00:07:27.100
supported by the evidence. Two kinds of
evidence speak to the relationship between

00:07:27.280 --> 00:07:31.180
language and other cognitive systems.
There is brain-imaging studies,

00:07:31.360 --> 00:07:36.030
brain-imaging evidence, and investigations
of patients with brain damage. In fMRI

00:07:36.210 --> 00:07:40.410
studies, we do something very simple. We
find our regions that seem to respond a

00:07:40.590 --> 00:07:44.240
lot to language, and then we ask how do
they respond to other various

00:07:44.420 --> 00:07:48.610
non-linguistic tasks. If they don't show
much of a response, then we can conclude

00:07:48.790 --> 00:07:52.750
that these regions are not engaged during
those tasks. In the patient studies, we

00:07:52.930 --> 00:07:57.880
can evaluate non-linguistic abilities in
individuals who don't have a functioning

00:07:58.060 --> 00:08:01.510
language system. If they perform well, we
can conclude that the language system is

00:08:01.690 --> 00:08:05.580
not necessary for performing those various
non-linguistic tasks. So starting with the

00:08:05.760 --> 00:08:09.210
fMRI evidence, I will show you responses
in the language regions to arithmetic,

00:08:09.390 --> 00:08:15.110
executive function tasks, and music
perception today. So here are two sample

00:08:15.290 --> 00:08:19.130
regions, the regions of the inferior
frontal cortex around Broca's area and the

00:08:19.310 --> 00:08:23.080
region in the posterior middle temporal
gyrus, but the rest of the regions of this

00:08:23.260 --> 00:08:28.740
network look similar in their profiles.
The region on the top is kind of in and

00:08:28.920 --> 00:08:33.020
around this region known as Broca's area,
except I don't use that term because I

00:08:33.200 --> 00:08:38.250
don't think it's a very useful term. In
black and gray, I show you the responses

00:08:38.430 --> 00:08:42.750
to the two localizer conditions, sentences
and non-words. These are estimated in data

00:08:42.930 --> 00:08:47.710
that's not used for defining these
regions. So we divide the data in half,

00:08:47.890 --> 00:08:50.930
use half of the data to define the regions
and the other half to quantify their

00:08:51.110 --> 00:08:56.280
responses. I will now show you how these
regions respond when people are asked to

00:08:56.460 --> 00:09:01.260
do some simple arithmetic additions,
perform a series of executive function

00:09:01.440 --> 00:09:06.470
tasks, like for example, hold a set of
locations in spacial memory, spacial

00:09:06.650 --> 00:09:13.000
locations in working memory, or perform
this classic flanker task, or listen to

00:09:13.180 --> 00:09:18.100
various musical stimuli. For arithmetic
and various executive tasks, we included a

00:09:18.280 --> 00:09:23.480
harder and an easier condition, because we
wanted to make sure that we can identify

00:09:23.660 --> 00:09:26.930
regions that are classically associated
with performing these tasks, which is

00:09:27.110 --> 00:09:31.320
typically done by contrasting a harder and
an easier condition of a task.

00:09:31.500 --> 00:09:36.110
So I'll show you now, in different colors,
responses to these various tasks, starting

00:09:36.290 --> 00:09:40.750
with the region on the lower part of the
screen. So we find that this region

00:09:40.930 --> 00:09:44.790
doesn't respond during arithmetic
processing, doesn't respond during working

00:09:44.970 --> 00:09:48.560
memory, doesn't respond during cognitive
control tasks, and doesn't respond during

00:09:48.740 --> 00:09:54.320
music perception. Quite strikingly to me
at the time, a very similar profile is

00:09:54.500 --> 00:09:57.890
observed around this region, which is
smack in the middle of so-called Broca's

00:09:58.070 --> 00:10:03.230
area, which appears to be incredibly
selective in its response for language.

00:10:03.410 --> 00:10:06.840
Know that it's not just the case that
these, for example, demanding tasks fail

00:10:07.020 --> 00:10:12.230
to show a hard versus an easy difference.
They respond pretty much at or below

00:10:12.410 --> 00:10:16.310
fixation baseline when people are engaged
in these tasks. So that basically tells

00:10:16.490 --> 00:10:20.770
you that these language regions work as
much when you're doing a bunch of math in

00:10:20.950 --> 00:10:24.180
your head or hold information in working
memory, as what you're doing when you're

00:10:24.360 --> 00:10:28.270
looking at a blank screen. So they really
do not care. So of course, to interpret

00:10:28.450 --> 00:10:31.440
the lack of the response in these language
regions, you want to make sure that these

00:10:31.620 --> 00:10:36.380
tasks activate the brain somewhere else.
Otherwise, you may have really bad tasks

00:10:36.560 --> 00:10:38.990
that you don't want to use. Indeed, they
do.

00:10:39.170 --> 00:10:44.200
So here, I'll show you activations for the
executive function tasks, but music also

00:10:44.380 --> 00:10:47.720
robustly activates the brain outside of
the language system. So here are two

00:10:47.900 --> 00:10:52.300
sample regions, one in the right frontal
cortex, one in the left parietal cortex.

00:10:52.480 --> 00:10:57.020
You see the profiles of response are quite
different from the language regions. For

00:10:57.200 --> 00:11:01.590
each task, we see robust responses, but
also a stronger response to the harder

00:11:01.770 --> 00:11:06.150
than the easier condition across these
various domains. These regions turn out to

00:11:06.330 --> 00:11:11.430
be part of this bilateral frontal parietal
network, which is known in the literature

00:11:11.610 --> 00:11:15.180
by many names, including the cognitive
control network or the multiple demand

00:11:15.360 --> 00:11:20.440
system, the latter term advanced by John
Duncan, who wanted to highlight the notion

00:11:20.620 --> 00:11:24.100
that these regions are driven by many
different kinds of cognitive demands. So

00:11:24.280 --> 00:11:27.900
these regions appeared to be sensitive to
effort across tasks, and their activity

00:11:28.080 --> 00:11:32.230
has been linked to a variety of
goal-directed behaviors. Interestingly if

00:11:32.410 --> 00:11:37.050
you look at the responses of these regions
to our language localizer conditions, we

00:11:37.230 --> 00:11:40.930
find exactly the opposite of what we find
in the language regions. They respond less

00:11:41.110 --> 00:11:45.130
to sentences than sequences of non-words,
presumably because processing sentences

00:11:45.310 --> 00:11:49.500
requires less effort, but clearly this
highlights, again, the language and the

00:11:49.680 --> 00:11:52.490
cognitive control system are clearly
functionally distinct.

00:11:52.670 --> 00:11:55.870
Moreover, damage to the regions of the
multiple demand network has been shown to

00:11:56.050 --> 00:12:00.670
lead to decreases in fluid intelligence.
So Alex Woolgar reported a strong

00:12:00.850 --> 00:12:04.990
relationship between the amount of tissue
loss in frontal and parietal cortices and

00:12:05.170 --> 00:12:10.910
a measure of IQ. This is not true for
tissue loss in the temporal lobes. It's

00:12:11.090 --> 00:12:15.130
quite striking. You can actually calculate
for this many cubic centimeters of loss in

00:12:15.310 --> 00:12:20.990
the MD system, you lose so many IQ points.
It's a strong, clear relationship. So this

00:12:21.170 --> 00:12:25.000
system is clearly an important part of the
cognitive arsenal of humans because the

00:12:25.180 --> 00:12:29.020
ability to think flexibly and abstractly
and to solve new problems are exactly . .

00:12:29.200 --> 00:12:34.530
. These are the kinds of abilities that IQ
tests aim to measure, are considered kind

00:12:34.710 --> 00:12:38.520
of one of the hallmarks of human
cognition. Okay. So as I mentioned, the

00:12:38.700 --> 00:12:43.030
complementary approach for addressing
questions about language specificity and

00:12:43.210 --> 00:12:46.590
relationship to other mental functions is
to examine cognitive abilities in

00:12:46.770 --> 00:12:50.400
individuals who lack a properly
functioning language system. Most telling

00:12:50.580 --> 00:12:54.740
are cases of global aphasia. So this is a
severe disorder which affects pretty much

00:12:54.920 --> 00:12:58.760
the entire front temporal language system,
typically due to a large stroke in the

00:12:58.940 --> 00:13:03.040
middle cerebral artery and lead to
profound deficits in comprehension and

00:13:03.220 --> 00:13:07.820
production. Rosemary Varley at UCL has
been studying this population for a few

00:13:08.000 --> 00:13:10.790
years now.
With her colleagues, she has shown that

00:13:10.970 --> 00:13:17.180
actually these patients seem to have
preserved abilities across many, many

00:13:17.360 --> 00:13:21.030
domains. So she showed that they have
in-tact arithmetic abilities. They can

00:13:21.210 --> 00:13:25.440
reason causally. They have good nonverbal
social skills. They can navigate in the

00:13:25.620 --> 00:13:29.710
world. They can perceive music and so on
and so forth. Of course, these findings

00:13:29.890 --> 00:13:34.600
are then consistent with the kind of
picture that emerges in our work in fMRI.

00:13:34.780 --> 00:13:38.870
Let's consider another important
non-linguistic capacity, which a lot of

00:13:39.050 --> 00:13:43.290
people often bring up when I tell them
about this work. How about the ability to

00:13:43.470 --> 00:13:47.800
extract meaning from non-linguistic
stimuli? Right? So given that our language

00:13:47.980 --> 00:13:52.110
regions are so sensitive to meaning, we
can ask how much of that response is due

00:13:52.290 --> 00:13:55.270
to the activation of some kind of
abstract, conceptual representation that

00:13:55.450 --> 00:14:00.000
language may elicit, rather than something
more language-specific, a semantic

00:14:00.180 --> 00:14:05.920
representation type. So to ask these
questions, we can look at how language

00:14:06.100 --> 00:14:10.130
regions respond to nonverbal, meaningful
representations. In one study, we had

00:14:10.310 --> 00:14:16.250
people look at events like this or the
sentence-level descriptions of them, and

00:14:16.430 --> 00:14:21.170
either we had them do kind of a high-level
semantic judgment test, like decide

00:14:21.350 --> 00:14:25.700
whether the event is plausible, or do a
very demanding perceptual control task.

00:14:25.880 --> 00:14:30.320
Basically what you find here is, again,
the black and gray are responses to the

00:14:30.500 --> 00:14:35.240
localizer conditions. So in red, as you
would expect, you find strong responses to

00:14:35.420 --> 00:14:39.780
this and to the condition where people see
sentences and make semantic judgments on

00:14:39.960 --> 00:14:44.990
them. So what happens when people make
semantic judgments on pictures? We find

00:14:45.170 --> 00:14:50.190
that some regions don't care at all about
those conditions, and other regions show

00:14:50.370 --> 00:14:53.860
reliable responses, but they're much
weaker than those elicited by the

00:14:54.040 --> 00:14:59.130
meaningful sentence condition. So could it
be that some of our language regions are

00:14:59.310 --> 00:15:02.880
actually abstract semantic regions?
Perhaps. But for now, keep in mind that

00:15:03.060 --> 00:15:08.910
the response to the sentence-meaning
condition is twice stronger, and it is

00:15:09.090 --> 00:15:12.710
also possible that participants may be
activating linguistic representations to

00:15:12.890 --> 00:15:16.850
some extent when they encounter meaningful
visual stimuli. So to answer this question

00:15:17.030 --> 00:15:20.510
more definitively, we're turning to the
patient evidence again. If parts of the

00:15:20.690 --> 00:15:23.610
language system are critical for
processing meaning in non-linguistic

00:15:23.790 --> 00:15:27.680
representations, then aphasic individuals
should have some difficulties with

00:15:27.860 --> 00:15:32.920
nonverbal semantics. First, I want to
share a quote with you from Tom Lubbock, a

00:15:33.100 --> 00:15:38.420
former art critic at The Independent, who
developed a tumor in the left temporal

00:15:38.600 --> 00:15:42.570
lobe which eventually killed him. As the
tumor progressed, and he was losing his

00:15:42.750 --> 00:15:47.150
linguistic abilities, he was documenting
his impressions of what it feels like to

00:15:47.330 --> 00:15:52.720
lose the capacity to express yourself
using verbal means.

00:15:52.900 --> 00:15:56.510
So he wrote, "My language to describe
things in the world is very small,

00:15:56.690 --> 00:16:00.320
limited. My thoughts, when I look at the
world, are vast, limitless, and normal,

00:16:00.500 --> 00:16:04.560
same as they ever were. My experience of
the world is not made less by lack of

00:16:04.740 --> 00:16:08.480
language, but is essentially unchanged."
I think this quote quite powerfully

00:16:08.660 --> 00:16:14.560
highlights the separability of language
and thought. So in work that I'm currently

00:16:14.740 --> 00:16:18.800
collaborating on with Rosemary Varley and
Nancy Kanwisher, we are evaluating the

00:16:18.980 --> 00:16:24.260
global aphasics performance on a wide
range of tasks, requiring you to process

00:16:24.440 --> 00:16:28.670
meaning in nonverbal stimuli. So for
example, can they distinguish between real

00:16:28.850 --> 00:16:32.530
objects and novel objects that are matched
for low-level visual properties? Can they

00:16:32.710 --> 00:16:37.520
make plausibility judgments for visual
events? What about events where

00:16:37.700 --> 00:16:43.760
plausibility is conveyed simply by the
prototypicality of the roles? So you can't

00:16:43.940 --> 00:16:48.900
do this task by simply inferring that a
watering can doesn't appear next to an egg

00:16:49.080 --> 00:16:53.550
very frequently. Right? It seems like the
data so far is suggesting that they indeed

00:16:53.730 --> 00:16:59.220
seem fine on all of these tasks, and they
laugh just like we do when they see these

00:16:59.400 --> 00:17:03.500
pictures because they're sometimes a
little funny. So they seem to process

00:17:03.680 --> 00:17:08.960
these just fine. So this suggests, to me,
that these kinds of tasks can be performed

00:17:09.140 --> 00:17:14.480
without a functioning language system. So
even if our language system stores some

00:17:14.660 --> 00:17:17.950
abstract conceptual knowledge in some
parts of it, it tells me at least that

00:17:18.130 --> 00:17:22.090
that code must live somewhere else as
well. So even if we lose our linguistic

00:17:22.270 --> 00:17:25.710
way to encode this information, we can
have access to it elsewhere.

00:17:25.890 --> 00:17:30.660
So to conclude this part, fMRI in patients
sudies converge suggesting that the front

00:17:30.840 --> 00:17:35.120
temporal language system is not engaged in
and is not needed for non-linguistic

00:17:35.300 --> 00:17:39.000
cognition. Instead, it appears that these
regions are highly specialized for

00:17:39.180 --> 00:17:44.390
interpreting and generating linguistics
signals. So just a couple minutes on what

00:17:44.570 --> 00:17:48.100
this means. So given this highly selective
response to language stimuli that we

00:17:48.280 --> 00:17:52.860
observe, can we make some guesses already
about what these regions actually do? I

00:17:53.040 --> 00:17:57.160
think so. I think a plausible hypothesis
is that this network houses our linguistic

00:17:57.340 --> 00:18:01.380
knowledge, including our knowledge of the
sounds of the language, the words, the

00:18:01.560 --> 00:18:05.770
constraints on how sounds and words can
combine with one another. Then essentially

00:18:05.950 --> 00:18:09.450
the process of language interpretation is
finding matches between the pieces of the

00:18:09.630 --> 00:18:12.820
input that are getting into our language
system and our previously stored

00:18:13.000 --> 00:18:17.810
representations. Language production is
just selecting the relevant subset of the

00:18:17.990 --> 00:18:24.370
representations to then convey to our
communication partner. This way . . . The

00:18:24.550 --> 00:18:29.150
form that this knowledge takes is a huge
question in linguistic psychology and

00:18:29.330 --> 00:18:32.590
neuroscience. So one result I don't have
time to discuss is that contra some

00:18:32.770 --> 00:18:37.000
claims, it doesn't seem to be the case
that syntactic processing is localized to

00:18:37.180 --> 00:18:41.390
a particular part of this language system.
It seems it's widely distributed across.

00:18:41.570 --> 00:18:45.500
Anywhere throughout the system, you find
sensitivity to both word-level meanings

00:18:45.680 --> 00:18:49.280
and compositional aspects of language,
which is much in line with all current

00:18:49.460 --> 00:18:52.950
linguistic theorizing, which doesn't draw
a short boundary between the lexicon and

00:18:53.130 --> 00:18:57.370
grammar. So this way of thinking about the
language system as a store of our language

00:18:57.550 --> 00:19:02.020
knowledge makes it pretty clear that the
system is probably not innate. In fact, it

00:19:02.200 --> 00:19:07.110
must arise via experience with language as
we accumulate this language store. It's

00:19:07.290 --> 00:19:10.830
also presumably dynamic, changing all the
time as we get more and more linguistic

00:19:11.010 --> 00:19:14.920
input through our lifetimes. I assume that
our language knowledge is plausibly

00:19:15.100 --> 00:19:19.600
acquired with domain general statistical
learning mechanisms, just like much other

00:19:19.780 --> 00:19:26.400
knowledge. So what changed in our brains
that allowed for the emergence of this

00:19:26.580 --> 00:19:32.940
system? So one thing that changed is that
our association cortices expanded. So

00:19:33.120 --> 00:19:36.630
these are regions that are sensory and
motor regions and include frontal,

00:19:36.810 --> 00:19:42.000
temporal, and parietal regions. Okay. So
these people have noted for a long time. I

00:19:42.180 --> 00:19:45.820
think I'm kind of in the camp of people
who think that our brains are not

00:19:46.000 --> 00:19:49.540
categorically different in any way.
They're just scaled-up versions of other

00:19:49.720 --> 00:19:53.640
primate brains. I think there's quite good
evidence for that. So how does the system

00:19:53.820 --> 00:19:56.890
emerge?
So I think one thing that was different

00:19:57.070 --> 00:20:03.140
between us and chimps is that there is a
protracted course to the brain development

00:20:03.320 --> 00:20:08.580
in humans. So between birth and adulthood,
our brains increase threefold, compared to

00:20:08.760 --> 00:20:12.680
just twofold in chimps. It's a big
difference. Basically this just makes us

00:20:12.860 --> 00:20:17.290
exceptionally susceptible to environmental
influences, and we can soak stuff up from

00:20:17.470 --> 00:20:22.120
the environment very, very easily. So as
our brains grow, we make more glial cells.

00:20:22.300 --> 00:20:26.820
We make more synapses. Our axions continue
to grow and become myelinated, and it's

00:20:27.000 --> 00:20:31.250
basically tissue that's ready to soak up
the regularity that we see in the world.

00:20:31.430 --> 00:20:34.400
Of course, it comes at a cost. That's why
we have totally useless babies that can't

00:20:34.580 --> 00:20:40.760
do anything, but apparently somehow it was
worth. . . The tradeoffs were worth it.

00:20:40.940 --> 00:20:44.060
Okay. So the conclusions. We have this
system. It's highly selective in its

00:20:44.240 --> 00:20:48.130
responses. It presumably emerges over the
course of our development and would enable

00:20:48.310 --> 00:20:51.570
probably some combination of the expansion
of these association cortices, where we

00:20:51.750 --> 00:20:55.170
can store vast amounts of symbolic
information and this protracted brain

00:20:55.350 --> 00:20:59.820
development, which makes us great learners
early on. Thank you.

00:21:00.000 --> 00:21:03.120
[applause]

00:21:03.510 --> 00:21:05.900
♪ [music] ♪

00:21:08.090 --> 00:21:13.820
- [Rachel] So I want to start this story
actually in the 1800s. So in 1800, a young

00:21:14.000 --> 00:21:22.960
physician named Itard decided to take on
the task of teaching a young boy French.

00:21:23.140 --> 00:21:27.760
Turns out that this young boy, who they
think was between 10 and 12 years old, he

00:21:27.940 --> 00:21:33.680
had no language. He'd been discovered
running around in the woods, naked and

00:21:33.860 --> 00:21:38.460
unable to communicate. Itard thought that
this was a very important task because he

00:21:38.640 --> 00:21:45.830
thought that civilization was based on the
ability to empathize and also on language.

00:21:46.010 --> 00:21:50.510
So he tried valiantly, for two years, to
teach this young boy named Victor . . . He

00:21:50.690 --> 00:21:56.900
named him Victor because over this
two-year time span, the only language or

00:21:57.080 --> 00:22:02.130
sounds that this boy could make was the
French 'er', which sounds like Victor.

00:22:02.310 --> 00:22:07.440
Hence, he had his name. But after two
years, Victor was unable to speak any

00:22:07.620 --> 00:22:12.820
French and comprehended very little French
and primarily communicated with objects.

00:22:13.000 --> 00:22:17.720
Then Itard wrote this up after two years.
He wrote up his findings, and he said that

00:22:17.900 --> 00:22:22.650
he thought that a major reason why Victor
didn't learn French was because he was

00:22:22.830 --> 00:22:26.140
simply too old.
But of course, this was the 1800s, and he

00:22:26.320 --> 00:22:32.080
didn't speculate as to what it was about
being 10 years old with no language that

00:22:32.260 --> 00:22:36.660
would prevent you from learning language
if you had a daily tutor trying to teach

00:22:36.840 --> 00:22:42.590
you French. There's an enormous amount of
irony in this particular story, because

00:22:42.770 --> 00:22:47.430
Itard was the house physician for the
first school for the deaf in the entire

00:22:47.610 --> 00:22:51.780
world. Here, we have a picture of it. The
first school for the deaf was begun in

00:22:51.960 --> 00:23:00.370
1760 in Paris. At the time that Itard was
teaching Victor, he was at the school with

00:23:00.550 --> 00:23:03.920
all of these children who used sign
language and all of these teachers who

00:23:04.100 --> 00:23:09.850
used sign language. Nonetheless, it never
occurred to him to try to teach Victor

00:23:10.030 --> 00:23:13.870
sign language, but we can't blame him
because in the 1800s, sign language was

00:23:14.050 --> 00:23:19.350
not considered to be a language. In fact,
that particular discovery and realization

00:23:19.530 --> 00:23:25.180
wouldn't happen for another century and a
half. So we can imagine why he didn't

00:23:25.360 --> 00:23:30.620
teach Victor sign language. The question
is: could Victor have actually learned

00:23:30.800 --> 00:23:36.180
sign language if Itard had tried to teach
it to him? So I'm going to try to answer

00:23:36.360 --> 00:23:41.040
that question today through a series of
studies. But before I talk about our

00:23:41.220 --> 00:23:45.090
studies, I want to talk about something
that all of the speakers who have preceded

00:23:45.270 --> 00:23:49.390
me have talked about, which is that one
thing that's . . . The defining

00:23:49.570 --> 00:23:53.800
characteristic of language is that it's
highly structured, and it's highly

00:23:53.980 --> 00:23:58.390
structured at all of these multiple
levels, so that speech sounds make up

00:23:58.570 --> 00:24:04.870
words. Words make up words and phonemes.
These words are strung together in

00:24:05.050 --> 00:24:10.600
sentences with syntax, and the specific
syntax helps us understand and produce

00:24:10.780 --> 00:24:18.560
very specific kinds of meanings.
Now one aspect of this language structure

00:24:18.740 --> 00:24:23.980
is that humans have evolved to the point
where children learn this structure

00:24:24.160 --> 00:24:28.340
naturally. Nobody has to teach them.
Nobody has to have a tutor to sit with

00:24:28.520 --> 00:24:31.930
them for two years to teach them the
structure of French or the structure of

00:24:32.110 --> 00:24:37.380
English or the structure of ASL. Simply by
being around people who use the language,

00:24:37.560 --> 00:24:42.540
young children naturally acquire all of
this multilevel and complicated structure.

00:24:42.720 --> 00:24:47.960
That is to say all children do this if, in
fact, they can access the language around

00:24:48.140 --> 00:24:51.710
them, if they have normal hearing. They're
born with normal hearing. They hear people

00:24:51.890 --> 00:24:57.010
talk. Before you know it, they're talking
themselves. But if children are born

00:24:57.190 --> 00:25:01.950
profoundly deaf, they cannot hear the
speech around them. We know that

00:25:02.130 --> 00:25:05.970
lip-reading is insufficient to learn
language because most of the speech sounds

00:25:06.150 --> 00:25:10.390
are invisible. What happens to these
children? If there's no sign language in

00:25:10.570 --> 00:25:15.010
the environment, they can't learn a visual
form of language either. So it happens

00:25:15.190 --> 00:25:20.240
that there are large numbers of children
who actually are like Victor, in the sense

00:25:20.420 --> 00:25:23.900
that they grow up without language,
without learning a language, but they are

00:25:24.080 --> 00:25:27.360
like Victor-- they are unlike Victor, in
that they weren't running around in the

00:25:27.540 --> 00:25:34.400
woods, nude and having a very harsh life.
So how does the lack of language in

00:25:34.580 --> 00:25:39.100
childhood affect the ability to learn
language? Or does it affect the ability to

00:25:39.280 --> 00:25:44.480
learn language? This has been the focus of
studies that we have been doing for many

00:25:44.660 --> 00:25:48.790
years in our laboratory. Because we're
using deaf children and sign language as a

00:25:48.970 --> 00:25:52.830
means to model language acquisition and
its effect on the brain, I think it's

00:25:53.010 --> 00:25:57.530
appropriate that I talk a little bit about
the kinds of stimuli we do and about

00:25:57.710 --> 00:26:01.910
American Sign Language. So first of all,
you should know that American Sign

00:26:02.090 --> 00:26:05.100
Language, unlike many of the sign
languages that have been discussed up to

00:26:05.280 --> 00:26:10.130
this point, is a very sophisticated
language. It's evolved clearly over 200

00:26:10.310 --> 00:26:15.040
years. We might even say that American
Sign Language evolved with the development

00:26:15.220 --> 00:26:20.250
of the United States of America and spread
as civilization went across, as white men

00:26:20.430 --> 00:26:26.210
went across the continent. So American
Sign Language has a phonological system, a

00:26:26.390 --> 00:26:31.440
morphological system, syntax, and so
forth. So for those of you who don't know

00:26:31.620 --> 00:26:34.830
sign . . . I know there are many people
here who do know sign. I want to show you

00:26:35.010 --> 00:26:40.590
what some of this structure looks like. So
I'm going to play you two video tapes. For

00:26:40.770 --> 00:26:44.490
those of you who don't know sign, I would
like you to guess which one is

00:26:44.670 --> 00:26:50.560
syntactically structured. For those of you
who do know sign, maybe you could keep the

00:26:50.740 --> 00:26:52.786
answer to yourselves.

00:26:58.900 --> 00:27:02.691
That's one. Here's two.

00:27:09.160 --> 00:27:16.780
So how many think two? How many think one?
Okay. For those of you who think two, I

00:27:16.960 --> 00:27:29.250
captioned this. So this is really 'kon,
dird, lun, blid, mackers, gancakes.'

00:27:29.430 --> 00:27:41.070
Number one is a fully formed sentence with
a subordinate clause.

00:27:41.250 --> 00:27:44.430
The reason I'm showing you these two
sentences is, for those of you who don't

00:27:44.610 --> 00:27:49.190
know the language, you can't perceive or
parse this particular structure. This is

00:27:49.370 --> 00:27:54.180
what knowing a language is about. For
those of you who know ASL and who know

00:27:54.360 --> 00:27:59.120
this language, you know that the second
example had all of these signs which were

00:27:59.300 --> 00:28:04.860
non-signs, possible signs, but really just
non-signs. This is part of what knowing

00:28:05.040 --> 00:28:09.240
language is about. How do people learn
this particular structure? The question

00:28:09.420 --> 00:28:13.370
that we're interested in is: how does
being a young child help people learn this

00:28:13.550 --> 00:28:20.190
particular structure? So we did a series
of experiments. When we started this work,

00:28:20.370 --> 00:28:23.840
it wasn't even clear that age would make a
difference in sign language acquisition.

00:28:24.020 --> 00:28:28.360
Sign language is gestural. Sign language
is mimetic. Maybe anybody can learn sign

00:28:28.540 --> 00:28:33.150
language at any time in their lives.
So in one experiment, what we did is we

00:28:33.330 --> 00:28:37.710
recruited a number of people. This is in
Canada, who were born deaf and who used

00:28:37.890 --> 00:28:43.720
ASL. We asked them. We created an
experiment where we had a set of sentences

00:28:43.900 --> 00:28:47.880
that varied in complexity, and we showed
them these ASL sentences, and we asked

00:28:48.060 --> 00:28:52.110
them simply to point to a picture that
reflected the meaning of the sentence that

00:28:52.290 --> 00:28:59.480
they saw. We were quite struck by our
findings. What you see here is that deaf

00:28:59.660 --> 00:29:03.310
people who learned ASL from birth, from
their parents, performed very, very well

00:29:03.490 --> 00:29:08.470
on this task, in contrast to deaf people
who were adults, who've been signing for

00:29:08.650 --> 00:29:14.860
over 20 years performed at chance. So they
had great difficulty understanding some of

00:29:15.040 --> 00:29:19.420
these basic sentences in ASL. So this
suggests that there are age of acquisition

00:29:19.600 --> 00:29:23.710
effects for sign, as there are for spoken
language. Everybody sort of . . . The word

00:29:23.890 --> 00:29:27.010
on the street is it's much harder to learn
a language if you're an adult than you're

00:29:27.190 --> 00:29:31.870
a child. But what if it's something deeper
than this? What if there's something about

00:29:32.050 --> 00:29:36.030
learning a language in childhood that sets
up the ability to learn language, that

00:29:36.210 --> 00:29:42.370
creates the ability to learn language? So
we did another experiment, also in Canada,

00:29:42.550 --> 00:29:46.970
but we decided in order to test this
particular hypothesis, we should switch

00:29:47.150 --> 00:29:52.380
languages. So we're no longer testing ASL
here. What we're testing here is English.

00:29:52.560 --> 00:29:57.690
We devised an experiment in English, where
we had a set of sentences, and some of

00:29:57.870 --> 00:30:02.230
them were ungrammatical, and some of them
were grammatical. This is a common kind of

00:30:02.410 --> 00:30:08.040
task that psycholinguists use.
Notice here that the people who were born

00:30:08.220 --> 00:30:14.640
profoundly deaf and for whom ASL was a
first language are near-native in English.

00:30:14.820 --> 00:30:18.430
So this is a second language. So learning
a language early, even though it's in

00:30:18.610 --> 00:30:23.220
sign, helps people learn a second
language. Notice also that they performed

00:30:23.400 --> 00:30:27.080
. . . Their performance was
indistinguishable from normally-hearing

00:30:27.260 --> 00:30:32.980
people who had learned other languages at
birth, German, Urdu, Spanish, and French.

00:30:33.160 --> 00:30:36.300
So there seems to be that there's
something about learning a language early

00:30:36.480 --> 00:30:40.760
in life, regardless of whether it's sign
language or spoken language, that actually

00:30:40.940 --> 00:30:44.950
helps people learn more language. It's not
simply learning language when one is

00:30:45.130 --> 00:30:49.160
little. But as also part of this
experiment, we tested a group of

00:30:49.340 --> 00:30:53.290
individuals who had been signing for 20
years and had gone through the educational

00:30:53.470 --> 00:30:59.140
system in Canada, who were born deaf. On
this task, they performed at chance. On a

00:30:59.320 --> 00:31:03.770
grammaticality judgment task, it's either
yes or no. So it's at chance. So we see

00:31:03.950 --> 00:31:07.330
that individuals who are deprived of
language, who aren't able to learn

00:31:07.510 --> 00:31:13.020
language at a young age, perform poorly on
their primary language, sign language, and

00:31:13.200 --> 00:31:17.520
they perform very poorly on a second
language, which is ASL, and we see the

00:31:17.700 --> 00:31:21.470
reverse. So there's something really
special going on here about learning

00:31:21.650 --> 00:31:26.980
language at an early age. What might this
be? And might it be in the brain?

00:31:27.160 --> 00:31:34.450
So in another set of studies, what we did
is take this population that we had been

00:31:34.630 --> 00:31:38.720
looking at, and we decided to neuro-image
their language processing to see whether

00:31:38.900 --> 00:31:42.400
this might give us some clues as to
differences between first and

00:31:42.580 --> 00:31:45.760
second-language learners and people who
had language and people who did not have

00:31:45.940 --> 00:31:50.490
language. So in this study, also done in
Canada, with colleagues at the Montreal

00:31:50.670 --> 00:31:59.050
Neurological Institute, we did fMRI. Maybe
many of you have had MRIs. We showed the

00:31:59.230 --> 00:32:03.130
subjects sentences, like the sentences
that you saw, and we asked them to make

00:32:03.310 --> 00:32:10.290
grammatical judgments on these sentences.
We tested 22 people. They were all born

00:32:10.470 --> 00:32:14.870
profoundly deaf. They all used American
Sign Language as a primary language. They

00:32:15.050 --> 00:32:18.880
had all gone through the educational
system, but they ranged in the age at

00:32:19.060 --> 00:32:23.280
which they were first able to acquire
language. This is all the way from birth

00:32:23.460 --> 00:32:29.280
up to age 14. So if the age at which you
learn your first language doesn't make a

00:32:29.460 --> 00:32:33.060
difference, then we should expect the
neuro-processing patterns of all of these

00:32:33.240 --> 00:32:37.350
individuals to be similar. If age of
acquisition makes a difference, we should

00:32:37.530 --> 00:32:41.500
see different patterns in the brain. In
fact, this is what we have. This is what

00:32:41.680 --> 00:32:46.280
we found. When we did the analysis, we
found that there were seven regions in the

00:32:46.460 --> 00:32:52.590
brain, primarily in the left hemisphere.
As you know now, the left hemisphere has

00:32:52.770 --> 00:32:57.510
areas that are responsible for language.
One effect that we found was that in the

00:32:57.690 --> 00:33:03.090
language regions of the left hemisphere,
the earlier the person learned their first

00:33:03.270 --> 00:33:08.110
language, the more activation we saw in
the language hemisphere. However, the

00:33:08.290 --> 00:33:12.860
older the person was when they learned
their first language, the less activation

00:33:13.040 --> 00:33:18.410
we got in the language areas of the brain.
So if there's less activation, is there

00:33:18.590 --> 00:33:23.440
something else going on here? We actually
found a second effect, which we were not

00:33:23.620 --> 00:33:28.240
expecting at all, which is in the back
part of the brain, the posterior part of

00:33:28.420 --> 00:33:35.570
the brain, in visual processing. This
particular effect was that the longer the

00:33:35.750 --> 00:33:39.820
person matured, the older the . . .
Without language, the older they were when

00:33:40.000 --> 00:33:45.980
they learned language, we found greater
activation, more neural resources being

00:33:46.160 --> 00:33:51.020
devoted to visual processing. So we see
that here in this group of deaf signers,

00:33:51.200 --> 00:33:57.570
we have two complementary reciprocal
effects of when a child learns his or her

00:33:57.750 --> 00:34:01.350
. . .when an individual learns their first
language and what the brain seems to be

00:34:01.530 --> 00:34:04.970
doing in terms of processing that
language. So that for people who learned

00:34:05.150 --> 00:34:09.120
language early in life, almost all of
their neural resources are devoted to

00:34:09.300 --> 00:34:12.460
processing the meaning and structure of
that language. For people who learned

00:34:12.640 --> 00:34:16.620
their first language later in life, more
neural resources are devoted to just

00:34:16.800 --> 00:34:21.810
trying, perhaps, to figure out what the
signal was. Was this a word? Was it glum?

00:34:21.990 --> 00:34:27.130
Or was it gleam? So we have this
reciprocal relationship between perceptual

00:34:27.310 --> 00:34:32.350
processing and language processing. This
particular pattern is not unique to deaf

00:34:32.530 --> 00:34:38.050
signers. There's work by Tim Brown and
Shleger that showed that younger children

00:34:38.230 --> 00:34:42.090
often have more posterior activation than
older children. There are also some

00:34:42.270 --> 00:34:46.200
clinical populations, such as autistic
individuals, particularly those who have

00:34:46.380 --> 00:34:50.920
low . . . whose language skills are not
well-developed, will often show more

00:34:51.100 --> 00:34:55.240
processing in the occipital lobe. So this
is not a pattern that is unique to

00:34:55.420 --> 00:34:59.060
deafness.
So then the next question we had is

00:34:59.240 --> 00:35:04.660
whether, in fact . . . How does language
develop when an individual first starts to

00:35:04.840 --> 00:35:10.610
learn it when they're much older, for
example, when they're a teen? We have been

00:35:10.790 --> 00:35:17.580
very fortunate to have followed five or
six children in our laboratory who had no

00:35:17.760 --> 00:35:23.230
language until they were 13 to 14 years of
age, for a variety of reasons. Two of

00:35:23.410 --> 00:35:28.200
these children are from the United States.
These other children are from other

00:35:28.380 --> 00:35:31.190
countries. Actually this particular
circumstance, while we might think of it

00:35:31.370 --> 00:35:36.610
as being very rare, is actually very
common, particularly in underdeveloped

00:35:36.790 --> 00:35:41.670
countries. So the way in which we have
observed or analyzed language acquisition

00:35:41.850 --> 00:35:45.560
is to use normal procedures that people
use to study children's language

00:35:45.740 --> 00:35:50.410
acquisition. We get a lot of spontaneous
data from them, and we analyze it. So one

00:35:50.590 --> 00:35:55.730
question we had is: if you're 13 years
old, and you don't have a language system,

00:35:55.910 --> 00:35:59.090
will you develop language like a baby? Or
will you do something else? Because you

00:35:59.270 --> 00:36:03.360
have a developed cognitive system. Will
you jump in the middle of the task? How

00:36:03.540 --> 00:36:08.120
will this progress? To answer this
question, we need to look a little bit at

00:36:08.300 --> 00:36:14.470
how normally-hearing children or deaf or
hearing children develop language when

00:36:14.650 --> 00:36:20.370
they are exposed to it as a young age. The
major hallmark of children's language

00:36:20.550 --> 00:36:25.810
acquisition is that they very quickly, as
they're acquiring the grammar of their

00:36:25.990 --> 00:36:28.460
language, their sentences get longer and
longer.

00:36:28.640 --> 00:36:32.870
The reasons their sentences get longer and
longer is because they're learning all of

00:36:33.050 --> 00:36:37.860
these . . .the morphology, the syntax. As
they say ideas, as they're expressing

00:36:38.040 --> 00:36:44.050
their ideas, they're better able to use
grammar to express them. So these data

00:36:44.230 --> 00:36:49.240
show the average length of children's
expressions. Two of these children are

00:36:49.420 --> 00:36:54.200
normally hearing and acquiring English,
and two of these children are acquiring

00:36:54.380 --> 00:37:00.710
ASL. So we see that, in fact, the teens
that we have been following show no

00:37:00.890 --> 00:37:04.510
increase in their language. They're able
to learn language and put words together,

00:37:04.690 --> 00:37:09.240
but, in fact, we don't see an increase in
their grammar. In the last study, we

00:37:09.420 --> 00:37:13.520
wanted to neuro-image these children. We
wanted to see what are their brains doing

00:37:13.700 --> 00:37:18.300
with the language that they have. So we
used magnetoencephalography, which is a

00:37:18.480 --> 00:37:25.710
different technique, which is
complementary to the fMRI. What we did for

00:37:25.890 --> 00:37:30.140
this is we studied their vocabulary, and
we made stimuli that we knew that they

00:37:30.320 --> 00:37:41.780
knew, words that were in their vocabulary,
and that looked something like this. In

00:37:41.960 --> 00:37:45.870
the first instance, the picture matched
the sign. In the second instance, it

00:37:46.050 --> 00:37:50.640
didn't. When that happens, the brain goes,
"Uh-oh," and you get this N-400 response.

00:37:50.820 --> 00:37:55.480
That's what we were localizing in the
brain for these children. Because we're

00:37:55.660 --> 00:38:01.470
using vocabulary that they have, we know
that they knew these words. We asked them

00:38:01.650 --> 00:38:04.710
to press buttons while they were doing
this task, and we knew that they were

00:38:04.890 --> 00:38:09.360
accurate. We didn't only test these
children. We also tested control groups.

00:38:09.540 --> 00:38:13.570
So some of these control groups are deaf.
Some are hearing. Some are first language

00:38:13.750 --> 00:38:15.910
learners, and some are second language
learners.

00:38:16.090 --> 00:38:20.890
The first panel shows the response of a
group of normally hearing adults doing

00:38:21.070 --> 00:38:27.040
this task while looking at pictures and
listening to words. This is data that

00:38:27.220 --> 00:38:32.620
Katie Travis used also to look at
children's development, neural

00:38:32.800 --> 00:38:37.910
development. The second panel, these are
deaf adults who learned ASL from birth.

00:38:38.090 --> 00:38:42.310
You can see that their processing is very
much like the hearing adults who are

00:38:42.490 --> 00:38:45.710
speaking English, primarily left
hemisphere in the language areas, with

00:38:45.890 --> 00:38:49.620
some support or help from the right
hemisphere. Actually these patterns are

00:38:49.800 --> 00:38:54.350
indistinguishable. Both the hearing adults
and the deaf adults learned language from

00:38:54.530 --> 00:38:59.860
birth, even though it was in a different
form. What's this last panel? These are

00:39:00.040 --> 00:39:04.060
college students who are normally hearing.
They have been learning sign for about

00:39:04.240 --> 00:39:07.580
three years, which is about the same
amount of time that our cases were

00:39:07.760 --> 00:39:15.470
learning language. So we see that
responding on this task in speech in ASL,

00:39:15.650 --> 00:39:19.490
whether it's a first language or a second
language, so long as the subject had

00:39:19.670 --> 00:39:24.480
language from birth, looks fairly similar.
What about the cases? We were able to

00:39:24.660 --> 00:39:29.390
neuro-image two cases, and you can see
that their neural processing patterns are

00:39:29.570 --> 00:39:35.420
quite different, and they look neither
like second language learners, nor do they

00:39:35.600 --> 00:39:42.030
look like deaf adults. These children have
been signing for three years, and they had

00:39:42.210 --> 00:39:46.990
no language before they started to learn
how to sign. You can see primarily that

00:39:47.170 --> 00:39:52.320
there's a huge response in the right
hemisphere, in the occipital and parietal

00:39:52.500 --> 00:39:58.280
areas. One of the subjects also shows some
response in the language areas.

00:39:58.460 --> 00:40:01.400
We can see that even though they're
acquiring language, they're doing it in a

00:40:01.580 --> 00:40:05.650
very different way, and their brains are
responding very differently. So we see

00:40:05.830 --> 00:40:12.400
that, in fact, there are huge effects of
language environment on both the

00:40:12.580 --> 00:40:17.750
development of language, but also how the
brain processes language. So we see that

00:40:17.930 --> 00:40:23.290
it seems to be that the human language
capacity, both understanding language and

00:40:23.470 --> 00:40:28.120
expressing language, but also the brain's
ability to process language is very

00:40:28.300 --> 00:40:34.000
dependent upon the baby's brain being
exposed to or immersed in language from

00:40:34.180 --> 00:40:39.490
birth. It's through this analyzing
language and working on the data that it's

00:40:39.670 --> 00:40:43.590
being fed that, in fact, I think the
neural networks of language are being

00:40:43.770 --> 00:40:48.940
created. So language is a skill that is
not innate but emerges from the

00:40:49.120 --> 00:40:52.280
interaction of the child with the
environment, through linguistic

00:40:52.460 --> 00:40:56.550
communication. That was probably the
answer that Itard was looking for and

00:40:56.730 --> 00:41:01.830
might be the reason why Victor did not
acquire language. Thank you.

00:41:02.010 --> 00:41:04.800
[applause]

00:41:07.360 --> 00:41:09.900
♪ [music] ♪

00:41:11.090 --> 00:41:16.830
- [Edward] Okay. So I wanted to first
thank the organizers really for this kind

00:41:17.010 --> 00:41:23.080
invitation to join this cast of stellar
thinkers about the biology and behavior of

00:41:23.260 --> 00:41:26.550
language. I actually want to switch the
title a little bit to something more

00:41:26.730 --> 00:41:31.890
specific. I want to talk to you about
organization, in particular a kind of

00:41:32.070 --> 00:41:37.910
organization that I refer to as a
taxonomy. The organization that I'm

00:41:38.090 --> 00:41:42.370
actually really referring to is the
organization of sound, in particular

00:41:42.550 --> 00:41:47.150
speech sounds and how those are actually
processed in a very important part of the

00:41:47.330 --> 00:41:53.020
brain called the superior temporal gyrus,
also known as Wernicke's area. The main

00:41:53.200 --> 00:42:00.000
focus of my lab is actually to understand
the basic transformation that occurs when

00:42:00.180 --> 00:42:06.130
you have an acoustic stimulus and how it
becomes transformed into phonetic units.

00:42:06.310 --> 00:42:10.340
In other words, basically, how do we go
from the physical stimulus that enters our

00:42:10.520 --> 00:42:16.500
ears into one that's essentially a mental
construct, one that's a linguistic one? To

00:42:16.680 --> 00:42:21.000
basically ask the simple question, what is
the structure of that kind of information

00:42:21.180 --> 00:42:28.370
as it's processed in the brain?
Now this actually turns out to be a very

00:42:28.550 --> 00:42:33.810
complex problem because it's one that
actually arises from many levels of

00:42:33.990 --> 00:42:38.950
computation that occur in the ascending
auditory system. As sounds actually come

00:42:39.130 --> 00:42:44.210
through the ears, they go up through at
least seven different synaptic connections

00:42:44.390 --> 00:42:47.940
across many different parts of the brain,
even bilaterally, to where they're

00:42:48.120 --> 00:42:51.750
actually processed in the non-primary
auditory cortex in the superior temporal

00:42:51.930 --> 00:42:56.070
gyrus. What we know about this area from
animal studies and non-human primates, for

00:42:56.250 --> 00:43:02.750
example, is this is an area that no longer
is tuned to basic low-level sound

00:43:02.930 --> 00:43:07.880
features, like pure tones, pure
frequencies, but in one that is actually

00:43:08.060 --> 00:43:13.580
tuned to very broad, complex sounds. There
have been very nice work in fMRI that's

00:43:13.760 --> 00:43:18.820
actually demonstrated that this area is
far more selective to complex sounds, like

00:43:19.000 --> 00:43:26.220
speech, over non-speech sounds. So the
basic question is not really about where

00:43:26.400 --> 00:43:31.740
is this processing going on. The question
is how. Okay. What is the structure of

00:43:31.920 --> 00:43:37.670
information in this transformation that's
going on? In particular, what kind of

00:43:37.850 --> 00:43:42.690
linkages can we make between that physical
stimulus and the internal one, which is a

00:43:42.870 --> 00:43:47.570
phonetic one? For me, I think it's really
important to acknowledge some really

00:43:47.750 --> 00:43:53.610
important fundamental contributions that
occur, that give us some insight and put

00:43:53.790 --> 00:43:58.930
them in a very important perspective. For
me, I think one of the most important

00:43:59.110 --> 00:44:02.900
pieces of work that led to this work that
I'm about to describe from our lab was

00:44:03.080 --> 00:44:07.840
actually 25 years ago, using an approach
that's actually far more complicated and

00:44:08.020 --> 00:44:10.910
difficult to achieve than what we do in
our own work.

00:44:11.090 --> 00:44:16.030
This is using single unit and single
neuron recordings that were recorded from

00:44:16.210 --> 00:44:19.720
patients that were undergoing
neurosurgical procedures for their

00:44:19.900 --> 00:44:26.160
clinical routine care. This very extremely
rare but precious opportunity to actually

00:44:26.340 --> 00:44:31.720
record from certain brain areas while
someone is actually recording-- listening

00:44:31.900 --> 00:44:36.170
to speech. These are from my close
colleague and mentor, Dr. Ojamin, who's in

00:44:36.350 --> 00:44:40.700
the audience today. But why I think it's
so important to acknowledge this work is a

00:44:40.880 --> 00:44:46.820
lot of the clues about what I'm about to
describe were actually seen 25 years ago.

00:44:47.000 --> 00:44:50.320
This figure that's extracted from that
paper, where they actually showed and

00:44:50.500 --> 00:44:56.170
could record from single brain cells,
called neurons, in the superior temporal

00:44:56.350 --> 00:45:01.640
gyrus, that they were active and
corresponding to very specific sounds. But

00:45:01.820 --> 00:45:06.410
if you actually look at where those sounds
are, they're not exactly corresponding to

00:45:06.590 --> 00:45:12.270
the same exact, let's say, phonemes or the
same exact sounds, but, in fact, they are

00:45:12.450 --> 00:45:17.120
corresponding to a class of sounds. This
was an observation that was made in this

00:45:17.300 --> 00:45:22.380
paper. They thought perhaps this is some
mention of phonetic category

00:45:22.560 --> 00:45:26.580
representation there. But it wasn't that
clear, actually, and there were a lot of

00:45:26.760 --> 00:45:32.740
other really important observations that
were made in that paper. Now from a

00:45:32.920 --> 00:45:39.110
linguistic perspective, in thinking about
how, behaviorally, we organize this

00:45:39.290 --> 00:45:43.450
information in the brain, there's actually
a wonderful way to approach it. It's not

00:45:43.630 --> 00:45:49.660
perfect, but a very wonderful way to think
about how languages across the world

00:45:49.840 --> 00:45:55.870
actually share a similar and shared
inventory of speech sounds, not all

00:45:56.050 --> 00:46:00.050
completely the same. Each language has a
different number, but they highly overlap.

00:46:00.230 --> 00:46:05.920
The reason why they overlap is because
they are produced by the same vocal tract.

00:46:06.100 --> 00:46:12.690
This is essentially like a periodic table
of sound elements for human language and

00:46:12.870 --> 00:46:18.200
speech. So this table actually has two
really important dimensions. The

00:46:18.380 --> 00:46:22.190
horizontal dimension is actually one that
we call the place of articulation. It's

00:46:22.370 --> 00:46:26.980
referencing where in the vocal track these
sounds are made. For example, bilabial

00:46:27.160 --> 00:46:32.790
sounds, the 'P' and the 'B' require you to
actually have a transient occlusion at the

00:46:32.970 --> 00:46:38.010
lips, 'ba. ' You cannot make those sounds
without that particular articulatory

00:46:38.190 --> 00:46:44.040
movement. Whereas some of the other
sounds, like a 'D' or a 'T', a 'da' a 'ta'

00:46:44.220 --> 00:46:48.260
a 'da' we call alveolar because the front
of the tongue tip is actually placed

00:46:48.440 --> 00:46:53.080
against the teeth. So these are actually
referencing where occlusions are occurring

00:46:53.260 --> 00:46:57.380
in the vocal tract when we speak, and
those actually correlate necessarily to

00:46:57.560 --> 00:47:03.890
very specific acoustic signatures. The
other dimension is what we call the manner

00:47:04.070 --> 00:47:07.530
of articulation. So the manner of
articulation is actually telling you a

00:47:07.710 --> 00:47:12.180
little bit more about not so much where,
but how in the vocal tract the

00:47:12.360 --> 00:47:16.310
constrictions are made in order to produce
those sounds. We have certain ones, like

00:47:16.490 --> 00:47:21.420
plosives, where you have complete closure
of the vocal tract and then a transient

00:47:21.600 --> 00:47:25.600
release, other sounds where you have
near-complete, like a fricative, like

00:47:25.780 --> 00:47:29.000
'sha,' 'za,' those sounds that we call
fricative.

00:47:29.180 --> 00:47:34.950
If you actually look at vowels, they
actually have a similar structural

00:47:35.130 --> 00:47:38.740
organization. There actually is something
that actually references where in the

00:47:38.920 --> 00:47:43.420
vocal tract, either the front, middle, or
back, or the degree of open and closure.

00:47:43.600 --> 00:47:48.250
So for both consonants and vowels, there
actually is a structure that we know

00:47:48.430 --> 00:47:54.500
about, linguistically and phonologically,
about how these things are organized. I

00:47:54.680 --> 00:48:00.340
think the thing that interests me is that,
like I referenced before, that this is

00:48:00.520 --> 00:48:05.900
something like a periodic table. There is
something fundamental about these units to

00:48:06.080 --> 00:48:11.260
our ability to perceive speech. These
phonological representations are not

00:48:11.440 --> 00:48:15.210
necessarily the ones that we think of as
these letters that we call phonemes, but

00:48:15.390 --> 00:48:21.040
actually groups of phonemes that share
something in common, what we call

00:48:21.220 --> 00:48:25.340
features. These are the members of small
categories which combine to form the

00:48:25.520 --> 00:48:30.800
speech sounds of human language. This
became very attractive to me as a model of

00:48:30.980 --> 00:48:35.560
something to look for in the brain because
of . . . Essentially why it could be so

00:48:35.740 --> 00:48:39.470
important is that languages actually do
not vary without limit, but they actually

00:48:39.650 --> 00:48:44.400
reflect some single or limited general
pattern, which is actually rooted in both

00:48:44.580 --> 00:48:49.400
the physical and cognitive capacities of
the human brain, and I would add the vocal

00:48:49.580 --> 00:48:53.750
tract. This is not a new kind of thinking,
but it's one that has not been clearly

00:48:53.930 --> 00:49:00.030
elucidated in terms of its biological
mechanisms. So in order for us to get this

00:49:00.210 --> 00:49:05.150
information, it requires a very special
opportunity, the one where we can't

00:49:05.330 --> 00:49:09.510
actually record directly from the brain.
In many ways, this is actually a lot more

00:49:09.690 --> 00:49:14.170
coarse than the kind of recordings that
were done almost 25 years ago. These are

00:49:14.350 --> 00:49:19.640
ones from electrode sensors that are
placed on the brain in order to localize

00:49:19.820 --> 00:49:23.510
seizures in patients that have epilepsy.
In the seven to 10 days that they are

00:49:23.690 --> 00:49:28.700
usually waiting to be localized, we have a
very, again, precious opportunity to

00:49:28.880 --> 00:49:33.370
actually have some of the participant, the
patient volunteers, listen to natural,

00:49:33.550 --> 00:49:39.000
continuous speech and look at those neural
responses on these electrode recordings to

00:49:39.180 --> 00:49:43.920
see how information is distributed in the
superior temporal gyrus when they're

00:49:44.100 --> 00:49:49.920
listening to these sounds. This gives you
a sense of actually what that neural

00:49:50.100 --> 00:49:52.910
activity pattern looks like.
[audio sample]

00:49:53.090 --> 00:49:56.910
- We're going to slow down that sentence a
lot here.

00:49:57.090 --> 00:50:01.910
- [audio sample] Ready tiger go to green
five now.

00:50:02.090 --> 00:50:08.010
- So you can see that the information is
being processed in a very precise, both

00:50:08.190 --> 00:50:13.530
spacial and temporal, manner in the brain.
This is exactly the reason why this kind

00:50:13.710 --> 00:50:17.020
of information has been elusive, because
we do not currently have a method that

00:50:17.200 --> 00:50:21.320
actually has both spacial and temporal
resolution and, at the same time, covers

00:50:21.500 --> 00:50:28.070
all of these areas simultaneously. So
it's, again, in the context of these rare

00:50:28.250 --> 00:50:32.230
opportunities with human patient
volunteers that we can conduct this kind

00:50:32.410 --> 00:50:36.390
of research. So the natural question is .
. . Of course, now that I've shown you

00:50:36.570 --> 00:50:41.080
that we can actually see a pattern in the
brain, both that's temporally and

00:50:41.260 --> 00:50:47.810
spatially specific, what actually happens
when we try to deconstruct some of those

00:50:47.990 --> 00:50:52.000
sound patterns from the brain? This just
gives you an example, again, in the

00:50:52.180 --> 00:50:57.510
superior temporal gyrus, where those
sounds are activating the brain. An

00:50:57.690 --> 00:51:02.180
example of the spectrogram for a given
sentence, in this case, it's, "In what

00:51:02.360 --> 00:51:07.060
eyes there were." The last part of that
figure basically shows you that pattern

00:51:07.240 --> 00:51:10.710
across different electrodes. It's not all
happening in the same particular way. You

00:51:10.890 --> 00:51:16.160
have very specific evoked responses that
actually occur at different parts of the

00:51:16.340 --> 00:51:19.490
superior temporal gyrus.
I want to show you what happens when you

00:51:19.670 --> 00:51:23.800
look at just one of those electrodes. If
you look at the neural response of that

00:51:23.980 --> 00:51:29.010
one particular electrode that's labeled
e1, and you organize the neural response

00:51:29.190 --> 00:51:34.820
by different phonemes, okay, you can
actually see, again, on the vertical

00:51:35.000 --> 00:51:40.220
access, starting with 'da,' 'ba,' 'ga,'
'ta,' 'ka.' You can see that this

00:51:40.400 --> 00:51:44.540
electrode . . . Those hundreds or
thousands of neurons that are under this

00:51:44.720 --> 00:51:52.300
electrode are very selectively responsive
to this set of sounds that we call

00:51:52.480 --> 00:51:59.030
plosives. It's not one phoneme, but a
category, and they share this feature that

00:51:59.210 --> 00:52:03.480
we actually know, linguistically, to be
called plosive. I can show you a series of

00:52:03.660 --> 00:52:09.080
other electrodes. Electrode two has a very
different kind of sensitivity. It's

00:52:09.260 --> 00:52:14.450
showing you that it really likes those
sounds 'sha,' 'za,' 'sa,' 'fa.' This

00:52:14.630 --> 00:52:19.430
is an electrode that is, again, not tuned
to one phoneme, but actually tuned to the

00:52:19.610 --> 00:52:24.700
category of sibilant fricatives in
linguistic jargon. We have another

00:52:24.880 --> 00:52:31.900
electrode, e3, that is selective to
low-back vowels, these "ah" based ones.

00:52:32.080 --> 00:52:37.740
Another one that is a little bit more
selective to high-fronted vowels, 'E.'

00:52:37.920 --> 00:52:44.150
Even another electrode, e5, that is
corresponding to nasal sounds.

00:52:44.330 --> 00:52:49.210
So this is a very low-level description,
but it's actually the first time we've

00:52:49.390 --> 00:52:55.650
ever seen in this kind of principled way,
obtained through very precise spatial and

00:52:55.830 --> 00:53:01.450
temporal recordings, the ability to
resolve phonetic feature selectivity at

00:53:01.630 --> 00:53:07.770
single electrodes in the human brain. Now
this is not enough. We need to really

00:53:07.950 --> 00:53:14.340
address this issue of structure. That's
one of the themes here. Are all of these

00:53:14.520 --> 00:53:18.820
things just equally distributed as
features? In the original thinking about

00:53:19.000 --> 00:53:23.880
these things, you could have a binary list
of features. It turns out that features,

00:53:24.060 --> 00:53:27.970
in and of themselves, actually have
structure and have relationships with one

00:53:28.150 --> 00:53:34.300
another. So what we did, in order to look
at that structure in the brain, we looked

00:53:34.480 --> 00:53:40.550
at hundreds of electrodes that were
recorded over a dozen patients. Each one

00:53:40.730 --> 00:53:45.940
of those columns actually corresponds to
one electrode and one particular superior

00:53:46.120 --> 00:53:54.150
temporal gyrus in someone's brain. Like I
just showed you before, the vertical axis

00:53:54.330 --> 00:53:59.260
is actually how they're organized by
different phonemes. What we did here was

00:53:59.440 --> 00:54:03.600
we used a statistical method called
hierarchical clustering. What hierarchical

00:54:03.780 --> 00:54:10.610
clustering is used for is finding the
patterns in this data. What the

00:54:10.790 --> 00:54:14.560
hierarchical clustering showed us and
sorted this data was that, in fact, there

00:54:14.740 --> 00:54:21.000
is, indeed, structure in the brain's
responses to human speech sounds, and it

00:54:21.180 --> 00:54:24.720
looks like this.
So we've organized the hierarchical

00:54:24.900 --> 00:54:30.990
clustering as a function of a single
electrode's, again, a single column's

00:54:31.170 --> 00:54:37.740
selectivity to different phonemes, but
we've also organized this clustering as a

00:54:37.920 --> 00:54:43.710
population response across all of the
electrodes and looking at that selectivity

00:54:43.890 --> 00:54:47.530
for different phonemes. So we have two
different axes that we're actually looking

00:54:47.710 --> 00:54:54.080
at the brains large distributed response
to speech sounds. We're using this method

00:54:54.260 --> 00:54:57.840
which is what we call unsupervised,
meaning we're not telling it any

00:54:58.020 --> 00:55:01.770
linguistic information, or we're not
organizing the data. We're just saying,

00:55:01.950 --> 00:55:06.800
"Tell us how the brain is organizing this
information." What we see from this is

00:55:06.980 --> 00:55:12.000
that when we actually look at where this
information is being organized, one of the

00:55:12.180 --> 00:55:16.560
biggest divisions between different parts
of different kinds of selectivity in the

00:55:16.740 --> 00:55:19.310
brain are what we would call the
difference between consonants and vowels

00:55:19.490 --> 00:55:27.080
or really, actually, between obstruents
and continuants, in linguistic jargon. But

00:55:27.260 --> 00:55:31.740
within those different categories, you
actually have sub-classification. So

00:55:31.920 --> 00:55:36.270
within the consonants, you actually have a
subdivision between plosives and

00:55:36.450 --> 00:55:40.240
fricatives. Between the sonorants you
actually have referencing for different

00:55:40.420 --> 00:55:44.560
positions of the tongue, low back, low
front, high front, different classes of

00:55:44.740 --> 00:55:49.160
vowels and, in fact, nasal. So basically
this is telling you that feature

00:55:49.340 --> 00:55:55.230
selectivity in the brain is actually
hierarchically structured. The second

00:55:55.410 --> 00:56:00.680
thing is that instead of using phonemes in
order to organize the responses, we

00:56:00.860 --> 00:56:08.710
actually use features. So as an example,
that term dorsal actually refers to the

00:56:08.890 --> 00:56:14.270
tongue position when it's fairly back,
like for 'G' 'K' sounds.

00:56:14.450 --> 00:56:18.720
You can see that when we organize things
by features, you have a much cleaner

00:56:18.900 --> 00:56:26.710
delineation. The electrode responses seem
to be much more tuned to phonetic features

00:56:26.890 --> 00:56:33.030
shown below, as they are, compared to when
you plot them as phonemes. Okay. So this

00:56:33.210 --> 00:56:39.700
essentially disproves any idea that there
is individual phoneme representation in

00:56:39.880 --> 00:56:44.630
the brain, at least not one that's locally
encoded, but tells you that the brain is

00:56:44.810 --> 00:56:50.830
organized by its sensitivity to phonetic
features. Now relating it to a phonetic

00:56:51.010 --> 00:56:54.470
feature is the first step, and it's one
that's really important because it's

00:56:54.650 --> 00:56:58.720
referencing the one we know about from
linguistics and the one that we know

00:56:58.900 --> 00:57:03.960
behaviorally. But how do we connect this
to the physical stimulus that's actually

00:57:04.140 --> 00:57:08.600
coming through our ears? That's where we
have to make a linkage to actually

00:57:08.780 --> 00:57:13.890
something about the sound properties. Are
these things truly abstract features that

00:57:14.070 --> 00:57:18.040
are being picked up by the brain? Or
actually, are they referencing specific

00:57:18.220 --> 00:57:24.530
sound properties? Basically the answer is
the latter. It's that what we're actually

00:57:24.710 --> 00:57:30.880
seeing is sensitivity to particular
spectral temporal features. In the top

00:57:31.060 --> 00:57:35.820
row, I am showing you basically . . . When
we look at the average tuning curves, the

00:57:36.000 --> 00:57:39.220
frequency versus time, tuning curves for
each one of those different

00:57:39.400 --> 00:57:44.730
classifications for plosive fricatives,
they're very similar to the acoustic

00:57:44.910 --> 00:57:48.910
structure when you average those
particular phonemes in the brain.

00:57:49.090 --> 00:57:52.180
So what this means is the tuning that
we're seeing that's corresponding to

00:57:52.360 --> 00:57:58.370
phonetic features is, in fact, one that is
tuned to high order acoustic spectral

00:57:58.550 --> 00:58:04.850
temporal ones. The brain is selecting
specific kind of acoustic information and

00:58:05.030 --> 00:58:10.120
converting it into what we perceive as
phonetic. In the interest of time, I'm

00:58:10.300 --> 00:58:16.500
going to sort of skip more in-depth
information about vowels and plosives and

00:58:16.680 --> 00:58:20.530
how those are specifically encoded. But in
summary, what we've found is that there's

00:58:20.710 --> 00:58:24.580
actually a multidimensional feature space,
actually, for speech sounds in the human

00:58:24.760 --> 00:58:31.240
superior temporal gyrus. This feature
space is organized in a way that actually

00:58:31.420 --> 00:58:37.450
shows hierarchical structure. The
hierarchical structure is fairly strongly

00:58:37.630 --> 00:58:43.370
driven by the brain, in particular, this
auditory cortex sensitivity to acoustic

00:58:43.550 --> 00:58:48.270
differences, which are most signified
actually in the manner of articulation

00:58:48.450 --> 00:58:52.440
distinctions, linguistically. What's
interesting about this is it actually does

00:58:52.620 --> 00:58:58.720
correlate quite well with some known
perceptual behavior. So I would like to

00:58:58.900 --> 00:59:03.380
conclude there and acknowledge some of the
really important people from my lab,

00:59:03.560 --> 00:59:07.140
postdoctoral fellow Nima Mesgarani who did
most of this work with one of our graduate

00:59:07.320 --> 00:59:11.000
students, Connie Cheung. Thank you.

00:59:11.100 --> 00:59:13.000
[applause]

00:59:15.867 --> 00:59:18.800
♪ [music] ♪

