WEBVTT
Kind: captions
Language: en

00:00:03.080 --> 00:00:06.320
Hi, I’m Adriene Hill and welcome back to
Crash Course Statistics.

00:00:06.320 --> 00:00:08.830
There’s something to be said for flexibility.

00:00:08.830 --> 00:00:11.160
It allows you to adapt to new circumstances.

00:00:11.169 --> 00:00:15.059
Like a Transformer is a truck, but it can
also be an awesome fighting robot.

00:00:15.059 --> 00:00:18.830
Today we’ll introduce you to one of the
most flexible statistical tools--the General

00:00:18.830 --> 00:00:21.089
Linear Model, or GLM.

00:00:21.089 --> 00:00:25.439
The GLM will allow us to create many different models to help describe the world.

00:00:25.440 --> 00:00:27.580
The first we’ll talk about is The Regression
Model.

00:00:27.580 --> 00:00:36.960
INTRO

00:00:36.960 --> 00:00:41.500
General Linear Models say that your data can be explained by two things: your model, and

00:00:41.500 --> 00:00:42.500
some error:

00:00:42.510 --> 00:00:43.510
First, the model.

00:00:43.510 --> 00:00:49.789
It usually takes the form Y = mx + b, or rather, Y = b + mx in most cases.

00:00:49.789 --> 00:00:54.780
Say I want to predict the number of trick-or-treaters I’ll get this Halloween by using enrollment

00:00:54.780 --> 00:00:56.219
numbers from the local middle school.

00:00:56.219 --> 00:00:58.430
I have to make sure I have enough candy on hand.

00:00:58.430 --> 00:01:01.160
I expect a baseline of 25 trick-or-treaters.

00:01:01.160 --> 00:01:04.910
And then for every middle school student,
I’ll increase the number of trick-or-treaters

00:01:04.910 --> 00:01:07.800
I expect by 0.01.

00:01:07.800 --> 00:01:09.290
So this would be my model:

00:01:09.290 --> 00:01:13.130
There were about 1,000 middle school students nearby last year, so based on my model, I

00:01:13.130 --> 00:01:16.130
predicted that I’d get 35 trick-or-treaters.

00:01:16.130 --> 00:01:19.180
But reality doesn’t always match predictions.

00:01:19.180 --> 00:01:24.170
When Halloween came around, I got 42, which means that the error in this case was 7.

00:01:24.170 --> 00:01:27.100
Now, error doesn’t mean that something’s
WRONG, per se.

00:01:27.100 --> 00:01:30.240
We call it error because it’s a deviation
from our model.

00:01:30.240 --> 00:01:32.970
So the data isn’t wrong, the model is.

00:01:32.970 --> 00:01:36.761
And these errors can come from many sources: like variables we didn’t account for in

00:01:36.761 --> 00:01:42.960
our model-- including the candy-crazed kindergartners from the elementary school--or just random variation

00:01:42.960 --> 00:01:46.940
Models allow us to make inferences --whether it’s the number of kids on my doorstep at

00:01:46.940 --> 00:01:50.740
Halloween, or the number of credit card frauds
committed in a year.

00:01:50.740 --> 00:01:54.640
General Linear Models take the information
that data give us and portion it out into

00:01:54.640 --> 00:02:00.760
two major parts: information that can be accounted for by our model, and information that can’t be.

00:02:00.760 --> 00:02:04.980
There’s many types of GLMS, one is Linear
Regression.

00:02:04.980 --> 00:02:07.880
Which can also provide a prediction for our
data.

00:02:07.880 --> 00:02:12.540
But instead of predicting our data using a
categorical variable like we do in a t-test,

00:02:12.540 --> 00:02:14.319
we use a continuous one.

00:02:14.319 --> 00:02:18.909
For example, we can predict the number of
likes a trending YouTube video gets based

00:02:18.909 --> 00:02:21.090
on the number of comments that it has.

00:02:21.090 --> 00:02:25.930
Here, the number of comments would be our
input variable and the number of likes our

00:02:25.930 --> 00:02:27.290
output variable.

00:02:27.290 --> 00:02:29.159
Our model will look something like this:

00:02:29.159 --> 00:02:33.459
The first thing we want to do is plot our
datafrom 100 videos:

00:02:33.459 --> 00:02:38.300
This allows us to check whether we think that
the data is best fit by a straight line, and

00:02:38.300 --> 00:02:43.660
look for outliers--those are points that are
really extreme compared to the rest of our data.

00:02:43.660 --> 00:02:46.680
These two points look pretty far away from
our data.

00:02:46.680 --> 00:02:48.540
So we need to decide how to handle them.

00:02:48.549 --> 00:02:52.030
We covered outliers in a previous episode,
and the same rules apply here.

00:02:52.030 --> 00:02:54.860
We’re trying to catch data that doesn’t
belong.

00:02:54.860 --> 00:03:00.430
Since we can’t always tell when that happened,
we set a criteria for what an outlier is,

00:03:00.430 --> 00:03:01.430
and stick to it.

00:03:01.430 --> 00:03:05.389
One reason that we’re concerned with outliers
in regression is that values that are really

00:03:05.389 --> 00:03:10.379
far away from the rest of our data can have
an undue influence on the regression line.

00:03:10.379 --> 00:03:13.170
Without this extreme point, our line would
look like this.

00:03:13.170 --> 00:03:14.909
But with it, like this.

00:03:14.909 --> 00:03:17.579
That’s a lot of difference for one little
point!

00:03:17.580 --> 00:03:21.800
There’s a lot of different ways to decide,
but in this case we’re gonna leave them in.

00:03:21.800 --> 00:03:26.340
One of the assumptions that we make when using
linear regression, is that the relationship

00:03:26.340 --> 00:03:27.599
is linear.

00:03:27.600 --> 00:03:32.220
So if there’s some other shape our data
takes, we may want to look into some other models.

00:03:32.220 --> 00:03:35.600
This plot looks linear, so we’ll go ahead
and fit our regression model.

00:03:35.600 --> 00:03:39.600
Usually a computer is going to do this part
for us, but we want to show you how this line fits.

00:03:39.600 --> 00:03:44.480
A regression line is the straight line that’s
as close as possible to all the data points

00:03:44.500 --> 00:03:45.540
at once.

00:03:45.540 --> 00:03:51.349
That means that it’s the one straight line
that minimizes the sum of the squared distance

00:03:51.349 --> 00:03:53.430
of each point to the line.

00:03:53.430 --> 00:03:55.420
The blue line is our regression line.

00:03:55.420 --> 00:03:57.150
Its equation looks like this:

00:03:57.150 --> 00:04:01.970
This number--the y-intercept--tells us how
many likes we’d expect a trending video

00:04:01.970 --> 00:04:04.590
with zero comments to have.

00:04:04.590 --> 00:04:07.279
Often, the intercept might not make much sense.

00:04:07.279 --> 00:04:11.980
In this model, it’s possible that you could
have a video with 0 comments, but a video

00:04:11.980 --> 00:04:18.620
with 0 comments and 9104 likes does seem to
conflict with our experience on youtube.

00:04:18.620 --> 00:04:23.070
The slope, aka, the coefficient--tells us
how much our likes are determined by the number

00:04:23.070 --> 00:04:24.370
of comments.

00:04:24.370 --> 00:04:29.290
Our coefficient here is about 6.5, which means
that on average, an increase in 1 comment

00:04:29.290 --> 00:04:33.430
is associated with an increase of about 6.5
likes.

00:04:33.430 --> 00:04:36.880
But There’s another part of the General
Linear Model: the error.

00:04:36.880 --> 00:04:40.870
Before we go any further, let’s take a look
at these errors--also called residuals.

00:04:40.870 --> 00:04:43.030
The residual plot looks like this:

00:04:43.030 --> 00:04:45.160
And we can tell a lot by looking at its shape.

00:04:45.160 --> 00:04:48.850
We want a pretty evenly spaced cloud of residuals.

00:04:48.850 --> 00:04:54.060
Ideally, we don’t want them to be extreme
in some areas and close to 0 in others.

00:04:54.060 --> 00:04:58.200
It’s especially concerning if you can see
a weird pattern in your residuals like this:

00:04:58.200 --> 00:05:03.120
Which would indicate that the error of your
predictions is dependent on how big your predictor

00:05:03.120 --> 00:05:05.560
variable value is.

00:05:05.560 --> 00:05:08.840
That would be like if our YouTube model was
pretty accurate at predicting the number of

00:05:08.840 --> 00:05:13.960
likes for videos with very few comments, but
was wildly inaccurate on videos with a lot

00:05:13.960 --> 00:05:14.680
of comments.

00:05:14.680 --> 00:05:18.840
So, now that we’ve looked at this error,
This is where Statistical tests come in.

00:05:18.850 --> 00:05:24.620
There are actually two common ways to do a Null Hypothesis Significance test on a regression coefficient.

00:05:24.620 --> 00:05:26.480
Today we’ll cover the F-test.

00:05:26.480 --> 00:05:31.360
The F-test, like the t-test, helps us quantify
how well we think our data fit a distribution,

00:05:31.360 --> 00:05:32.500
like the null distribution.

00:05:32.500 --> 00:05:35.890
Remember, the general form of many test statistics
is this:

00:05:35.890 --> 00:05:39.660
But I’m going to make one small tweak to
the wording of our general formula to help

00:05:39.660 --> 00:05:41.710
us understand F-tests a little better.

00:05:41.710 --> 00:05:45.770
The null hypothesis here is that there’s
NO relationship between the number of comments

00:05:45.770 --> 00:05:48.880
on a trending YouTube video and the number
of likes.

00:05:48.880 --> 00:05:53.330
IF that were true, we’d expect a kind of
blob-y, amorphous-cloud-looking scatter plot

00:05:53.330 --> 00:05:56.010
and a regression line with a slope of 0.

00:05:56.010 --> 00:05:59.670
It would mean that the number of comments
wouldn’t help us predict the number of likes.

00:05:59.670 --> 00:06:04.100
We’d just predict the mean number of likes
no matter how many comments there were.

00:06:04.100 --> 00:06:05.700
Back to our actual data.

00:06:05.700 --> 00:06:08.270
This blue line is our observed model.

00:06:08.270 --> 00:06:12.000
And the red is the model we’d expect if
the null hypothesis were true.

00:06:12.000 --> 00:06:15.600
Let’s add some notation so it’s easier
to read our formulas.

00:06:15.600 --> 00:06:20.330
Y-hat looks like this, and it represents the
predicted value for our outcome variable--here

00:06:20.330 --> 00:06:22.860
it’s the predicted number of likes.

00:06:22.860 --> 00:06:27.890
Y-bar looks like this, and it represents the
mean value of likes in this sample.

00:06:27.890 --> 00:06:32.580
Taking the squared difference between each
data point and the mean line tells us the

00:06:32.580 --> 00:06:34.830
total variation in our data set.

00:06:34.830 --> 00:06:39.150
This might look similar to how we calculated
variance, because it is.

00:06:39.150 --> 00:06:44.120
Variance is just this sum of squared deviations--called
the Sum of Squares Total--divided by N.

00:06:44.120 --> 00:06:47.490
And we want to know how much of that total
Variation is accounted for by our regression

00:06:47.490 --> 00:06:49.980
model, and how much is just error.

00:06:49.980 --> 00:06:53.781
That would allow us to follow the General
Linear Model framework and explain our data

00:06:53.781 --> 00:06:56.580
with two things: the model’s prediction,
and error.

00:06:56.580 --> 00:07:01.490
We can look at the difference between our
observed slope coefficient--6.468--and the

00:07:01.490 --> 00:07:05.140
one we’d expect if there were no relationship--0,
for each point.

00:07:05.140 --> 00:07:06.860
And we’ll start here with this point:

00:07:06.860 --> 00:07:11.770
The green line represents the difference between
our observed model--which is the blue line--and

00:07:11.770 --> 00:07:15.670
the model that would occur if the null were
true--which is the red line.

00:07:15.670 --> 00:07:17.990
And we can do this for EVERY point in the
data set.

00:07:17.990 --> 00:07:21.990
We want negative differences and positive
differences to count equally, so we square

00:07:21.990 --> 00:07:24.970
each difference so that they’re all positive.

00:07:24.970 --> 00:07:28.620
Then we add them all up to get part of the
numerator of our F-statistic:

00:07:28.620 --> 00:07:31.090
The numerator has a special name in statistics.

00:07:31.090 --> 00:07:35.370
It’s called the Sums of Squares for Regression,
or SSR for short.

00:07:35.370 --> 00:07:39.500
Like the name suggests, this is the sum of
the squared distances between our regression

00:07:39.500 --> 00:07:40.960
model and the null model.

00:07:40.960 --> 00:07:43.080
Now we just need a measure of average variation.

00:07:43.080 --> 00:07:48.560
We already found a measure of the total variation
in our sample data, the Total Sums of Squares.

00:07:48.560 --> 00:07:52.000
And we calculated the variation that’s explained
by our model.

00:07:52.000 --> 00:07:55.560
The other portion of the variation should
then represent the error, the variation of

00:07:55.560 --> 00:07:57.470
data points around our model.

00:07:57.470 --> 00:07:58.740
Shown here in Orange.

00:07:58.740 --> 00:08:03.820
The sum of these squared distances are called
the Sums of Squares for Error (SSE).

00:08:03.820 --> 00:08:07.770
If data points are close to the regression
line, then our model is pretty good at predicting

00:08:07.770 --> 00:08:11.170
outcome values like likes on trending YouTube
Videos.

00:08:11.170 --> 00:08:12.840
And so our SSE will be small.

00:08:12.840 --> 00:08:16.650
If the data are far from the regression line,
then our model isn’t too good at predicting

00:08:16.650 --> 00:08:17.650
outcome values.

00:08:17.650 --> 00:08:18.950
And our SSE is going to be big.

00:08:18.950 --> 00:08:21.670
Alright, so now we have all the pieces of
our puzzle.

00:08:21.670 --> 00:08:26.190
Total Sums of Squares, Sums of Squares for
Regression, and Sums of Squares for Error:

00:08:26.190 --> 00:08:31.390
Total Sums of Squares represents ALL the information
that we have from our Data on YouTube likes.

00:08:31.390 --> 00:08:35.060
Sums of Squares for Regression represents
the proportion of that information that we

00:08:35.060 --> 00:08:37.840
can explain using the model we created.

00:08:37.840 --> 00:08:42.729
And Sums of Squares for Error represents the
leftover information--the portion of Total

00:08:42.729 --> 00:08:45.110
Sums of Squares that the model can’t explain.

00:08:45.110 --> 00:08:48.670
So the Total Sums of Squares is the Sum of
SSR and SSE.

00:08:48.670 --> 00:08:52.370
Now we’ve followed the General Linear Model
framework and taken our data and portioned

00:08:52.370 --> 00:08:56.110
it into two categories: Regression Model,
and Error.

00:08:56.110 --> 00:08:59.749
And now that we have the SSE, our measurement
of error, we can finally start to fill in

00:08:59.749 --> 00:09:01.560
the Bottom of our F-statistic.

00:09:01.560 --> 00:09:02.610
But we’re not quite done yet.

00:09:02.610 --> 00:09:07.870
The last and final step to getting our F-statistic
is to divide each Sums of Squares by their

00:09:07.870 --> 00:09:09.820
respective Degrees of freedom.

00:09:09.840 --> 00:09:14.240
Remember degrees of freedom represent the amount of independent information that we have.

00:09:14.240 --> 00:09:18.920
The sums of square error has n--the sample
size--minus 2 degrees of freedom.

00:09:18.920 --> 00:09:23.200
We had 100 pieces of independent information
from our data, and we used 1 to calculate

00:09:23.209 --> 00:09:27.160
the y-intercept and 1 to calculate the regression
coefficient.

00:09:27.160 --> 00:09:30.999
So the Sums of Squares for Error has 98 degrees
of freedom.

00:09:30.999 --> 00:09:35.260
The Sums of Squares for Regression has one
degree of freedom, because we’re using one

00:09:35.260 --> 00:09:39.470
piece of independent information to estimate
our coefficient our slope.

00:09:39.470 --> 00:09:43.740
We have to divide each sums of squares by
its degrees of freedom because we want to

00:09:43.740 --> 00:09:45.910
weight each one appropriately.

00:09:45.910 --> 00:09:47.920
More degrees of freedom mean more information.

00:09:47.920 --> 00:09:51.620
It’s like how you wouldn’t be surprised
that Katie Mack who has a PhD in AstroPhysics

00:09:51.620 --> 00:09:55.779
can explain more about the planets than someone
taking a high school Physics class.

00:09:55.779 --> 00:09:58.870
Of course she can she has way more information.

00:09:58.870 --> 00:10:04.069
Similarly, we want to make sure to scale the
Sums of Squares based on the amount of independent

00:10:04.069 --> 00:10:05.680
information each have.

00:10:05.680 --> 00:10:07.399
So we’re finally left with this:

00:10:07.399 --> 00:10:12.149
And using an F-distribution, we can find our
p-value: the probability that we’d get a

00:10:12.149 --> 00:10:16.059
F statistic as big or bigger than 59.613.

00:10:16.059 --> 00:10:18.559
Our p-value is super tiny.

00:10:18.559 --> 00:10:24.120
It’s about 0.000-000-000-000-99.

00:10:24.120 --> 00:10:28.660
With an alpha level of 0.05, we reject the
null that there is NO relationship between

00:10:28.660 --> 00:10:31.769
likes and YouTube comments on trending videos.

00:10:31.769 --> 00:10:36.540
So we reject that true coefficient for the
relationship between likes and comments on

00:10:36.540 --> 00:10:37.980
YouTube is 0.

00:10:37.980 --> 00:10:43.290
The F-statistic allows us to directly compare
the amount of variation that our model can

00:10:43.290 --> 00:10:45.040
and cannot explain.

00:10:45.040 --> 00:10:49.839
When our model explains a lot of variation,
we consider it statistically significant.

00:10:49.839 --> 00:10:54.490
And it turns out, if we did a t-test on this
coefficient, we’d get the exact same p-value.

00:10:54.490 --> 00:10:58.640
That’s because these two methods of hypothesis
testing are equivalent, in fact if you square

00:10:58.640 --> 00:11:01.420
our t-statistic, you’ll get our F-statistic!

00:11:01.420 --> 00:11:04.459
And we’re going to talk more about why F-tests
are important later.

00:11:04.459 --> 00:11:07.480
Regression is a really useful tool to understand.

00:11:07.480 --> 00:11:11.059
Scientists, economists, and political scientists
use it to make discoveries and communicate

00:11:11.059 --> 00:11:13.380
those discoveries to the public.

00:11:13.380 --> 00:11:17.779
Regression can be used to model the relationship
between increased taxes on cigarettes and

00:11:17.779 --> 00:11:20.079
the average number of cigarettes people buy.

00:11:20.079 --> 00:11:24.660
Or to show the relationship between peak-heart-rate-during-exercise
and blood pressure.

00:11:24.660 --> 00:11:29.050
Not that we’re able to use regression alone
to determine if it causes changes.

00:11:29.050 --> 00:11:33.180
But more abstractly, we learned today about
the General Linear Model framework.

00:11:33.180 --> 00:11:37.360
What happens in life can be explained by two
things: what we know about how the world works,

00:11:37.360 --> 00:11:40.059
and error--or deviations--from that model.

00:11:40.059 --> 00:11:45.100
Like say you budgeted $30 for gas and only
ended up needing $28 last week.

00:11:45.100 --> 00:11:49.079
The reality deviated from your guess and now
you get to to go to The Blend Den again!

00:11:49.079 --> 00:11:53.430
Or just how angry your roommate is that you
left dishes in the sink can be explained by

00:11:53.430 --> 00:11:58.019
how many days you left them out with a little
wiggle room for error depending on how your

00:11:58.019 --> 00:11:59.139
roommate's day was.

00:11:59.140 --> 00:12:01.780
Alright, thanks for watching, I’ll see you
next time.

