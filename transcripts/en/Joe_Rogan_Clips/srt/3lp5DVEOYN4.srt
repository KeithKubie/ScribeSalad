1
00:00:00,060 --> 00:00:01,440
Danger. Other people. So so,

2
00:00:01,470 --> 00:00:05,130
so my question then is when I was
physically threatened on Twitter,

3
00:00:05,550 --> 00:00:08,670
you guys refuse to take down the tweet
and I showed up in Berkeley and someone

4
00:00:08,671 --> 00:00:10,680
physically threatened me
because they were encouraged to,

5
00:00:10,920 --> 00:00:14,310
when I was in Venezuela I was physically
threatened by high profile individual,

6
00:00:14,340 --> 00:00:16,890
10,000 people tweeting at me.
You guys do nothing. Right?

7
00:00:16,891 --> 00:00:20,730
So I guess there's the obvious question
of why does it always feel like your

8
00:00:20,731 --> 00:00:22,770
policies are going one
direction politically?

9
00:00:23,010 --> 00:00:25,800
You say it's about behavior and you said
several times already, but I've already,

10
00:00:25,801 --> 00:00:28,290
I've got a ton of examples
of that not being the case

11
00:00:28,380 --> 00:00:30,300
and you will always be
able to find those exams.

12
00:00:30,560 --> 00:00:34,790
Yeah. Examples where you guys were
alerted multiple times and did nothing.

13
00:00:35,060 --> 00:00:38,000
Like when Antifa docs,
a bunch of law enforcement agents,

14
00:00:38,240 --> 00:00:39,590
some of the tweets are removed,

15
00:00:39,591 --> 00:00:43,700
but since September this tweet is still
alive with a list of private phone

16
00:00:43,701 --> 00:00:47,150
numbers addresses. Yet
Kathy Griffin, she's fine.

17
00:00:47,600 --> 00:00:50,630
The guy who threatened the lives of
these kids in Covington and said,

18
00:00:50,631 --> 00:00:53,750
lock him in school and burn it down. You
did nothing. I mean he got to spend it.

19
00:00:53,751 --> 00:00:56,330
I take his tweets down. Was he banned
for threatening the lives of kids?

20
00:00:56,331 --> 00:00:57,164
Absolutely not.

21
00:00:57,520 --> 00:01:01,100
So again, we have an, um, I'm happy
to talk about all these details.

22
00:01:01,430 --> 00:01:05,930
We have our policies that are meant to
protect people and are meant to enable

23
00:01:05,931 --> 00:01:08,300
for expression as long as you're
not trying to silence somebody else.

24
00:01:08,750 --> 00:01:12,200
Now we take a variety of different
enforcement mechanisms around that.

25
00:01:12,410 --> 00:01:15,830
Sometimes you get warned, sometimes we
were, your tweet is forced to be deleted.

26
00:01:16,190 --> 00:01:20,000
It's a very rare occasion where we will
outright suspend someone without any

27
00:01:20,001 --> 00:01:24,290
sort of warning or any sort of, um,
ability to understand what happened.

28
00:01:24,340 --> 00:01:27,250
What did you guys do with Kathy Griffin
when she was saying she wanted the names

29
00:01:27,251 --> 00:01:31,090
of those young kids were in the mag
of hats at the Covington high school.

30
00:01:31,350 --> 00:01:34,720
That's all right. That's a great example,
Joe. So in that particular case, you know,

31
00:01:34,721 --> 00:01:38,920
our doxing policy really focuses
on, uh, posting private information,

32
00:01:38,921 --> 00:01:42,610
which we don't consider names to be
private. We consider your home address,

33
00:01:42,700 --> 00:01:45,160
your home phone, your home phone
number, or your mobile phone number,

34
00:01:45,400 --> 00:01:46,660
those types of things to be private.

35
00:01:47,050 --> 00:01:51,100
So in that particular case we took what
I think now is probably a very literal

36
00:01:51,101 --> 00:01:55,030
interpretation of our policy. And so
that, that was not a doxing incident.

37
00:01:55,060 --> 00:01:56,050
Do you think that was an error?

38
00:01:56,230 --> 00:01:59,860
I think that it was shortsighted and
given the context of what was going on

39
00:01:59,861 --> 00:02:03,040
there, that um, if I was
doing this all over again,

40
00:02:03,070 --> 00:02:06,250
I would probably ask my team to look at
that through the lens of what was the

41
00:02:06,251 --> 00:02:07,390
purpose behind that tweet.

42
00:02:07,840 --> 00:02:11,440
And if the purpose was in fact to identify
these kids to either docs in them or

43
00:02:11,460 --> 00:02:13,390
abusing, harassing him,
which it probably was,

44
00:02:13,780 --> 00:02:16,180
then we should be taking
a more expansive view of,

45
00:02:16,240 --> 00:02:18,430
of that policy and including
that type of content,

46
00:02:18,490 --> 00:02:20,550
especially considering the
fact they're minors. I mean,

47
00:02:20,560 --> 00:02:23,410
I would think that right away
that would be like the approach.

48
00:02:23,680 --> 00:02:28,210
So this is a trial and error sort of
learn and grow and move on with new

49
00:02:28,211 --> 00:02:29,050
information sort of.
Yeah,

50
00:02:29,350 --> 00:02:31,900
absolutely. We're gonna learn,
we're gonna make a ton of mistakes.

51
00:02:31,901 --> 00:02:33,190
We're trying to do this,
uh,

52
00:02:33,220 --> 00:02:37,150
with hundreds of millions of accounts
all around the world, numerous languages.

53
00:02:37,510 --> 00:02:39,340
We're going to make mistakes.
Even if we get better,

54
00:02:39,341 --> 00:02:40,660
there will always be mistakes,

55
00:02:40,661 --> 00:02:44,800
but we're hoping to learn from those and
to make ourselves better and to catch

56
00:02:44,801 --> 00:02:48,280
cases like Tim's or others where
we clearly may have made an error.

57
00:02:48,520 --> 00:02:50,830
And I'm open to having those discussions.
I'm not,

58
00:02:51,010 --> 00:02:53,320
I'm sorry Tim familiar
with your specific cases,

59
00:02:53,590 --> 00:02:55,780
but I'd love to follow up with you and

60
00:02:57,010 --> 00:03:01,240
I just want to see the tweet we don't
need to pull up. So it's a, B I t. Dot.

61
00:03:01,241 --> 00:03:04,300
L Y slash Antifa tweet,
all lower case.

62
00:03:04,620 --> 00:03:08,530
This is also an evolution
in prioritization as well.

63
00:03:08,720 --> 00:03:12,640
Wondering one of the things we've
come to recently is we do, we do need,

64
00:03:12,970 --> 00:03:17,300
we do need to prioritize these efforts
both in terms of policy enforcement, um,

65
00:03:17,350 --> 00:03:19,090
how we're thinking about evolving them.
Um,

66
00:03:19,180 --> 00:03:22,570
one of the things that we want to focus
on as number one is physical safety.

67
00:03:22,990 --> 00:03:25,690
And this leads you immediately
to something like doxing.

68
00:03:25,780 --> 00:03:30,780
And right now the only way we take action
on a docksin case is if it's reported

69
00:03:31,961 --> 00:03:36,280
or not. Where we want to move to is to
be able to recognize those in real time,

70
00:03:36,281 --> 00:03:37,600
at least in the English language,

71
00:03:37,990 --> 00:03:41,860
recognize those in real time through
our machine learning algorithms and take

72
00:03:41,861 --> 00:03:43,630
the action before it has to report it.

73
00:03:43,660 --> 00:03:48,660
So we're focused purely right now on
going after a Dachsund cases with our

74
00:03:49,601 --> 00:03:51,460
algorithms so that we can be proactive.

75
00:03:51,880 --> 00:03:56,880
That also requires a much more rigorous
appeals process to correct us when we're

76
00:03:57,491 --> 00:04:00,340
wrong,
but we think it's tightly scoped enough.

77
00:04:00,341 --> 00:04:03,370
It impacts the most important thing,
which is someone's physical safety.

78
00:04:03,910 --> 00:04:06,850
Once we learned from that,
we can really look at the,

79
00:04:06,880 --> 00:04:10,870
the biggest issue with our system right
now is all the burden is placed upon the

80
00:04:10,871 --> 00:04:15,520
victim. So we only act based on reports.
We don't have a lot of enforcement.

81
00:04:16,060 --> 00:04:19,170
Um, especially with,
with more of the more,

82
00:04:19,171 --> 00:04:24,040
the more of the take downs that are
run through machine learning and deep

83
00:04:24,041 --> 00:04:24,610
learning algorithm.

84
00:04:24,610 --> 00:04:25,890
But if something is reported,

85
00:04:25,900 --> 00:04:28,880
a human does review it eventually or
are there a series of reports that you

86
00:04:28,881 --> 00:04:29,714
never get to?

87
00:04:30,240 --> 00:04:32,550
There's, there's probably
reports we don't, I mean we,

88
00:04:32,551 --> 00:04:35,550
we prioritize the queue
based on severity and the,

89
00:04:35,720 --> 00:04:38,820
the thing that Walmart severity is
something like physical safety or private

90
00:04:38,821 --> 00:04:39,654
information or whatnot.

91
00:04:39,660 --> 00:04:44,280
So generally we try to get through
everything but we have to prioritize that

92
00:04:44,281 --> 00:04:45,150
cute even coming in.

93
00:04:45,340 --> 00:04:48,490
So if, if someone threatened the
lives of someone else, you would,

94
00:04:48,520 --> 00:04:50,700
would you band that account?
Would you tell them like,

95
00:04:51,050 --> 00:04:54,490
like let's say someone tweeted
three times, kill these people.

96
00:04:54,491 --> 00:04:58,330
I want them dead three times.
Is that yes. Violin. You didn't,

97
00:04:58,331 --> 00:05:02,440
you didn't ban him though. Let's pull
that up Jamie. That's a, I don't know.

98
00:05:02,450 --> 00:05:02,610
I don't,

99
00:05:02,610 --> 00:05:06,550
I don't necessarily want to give out
specific user names because then people

100
00:05:06,560 --> 00:05:08,800
just point the finger at me and
saying I'm getting these people band.

101
00:05:08,801 --> 00:05:12,520
But you know, during Covington
this guy said multiple times too.

102
00:05:12,521 --> 00:05:14,500
He wanted his followers
to go and kill these kids.

103
00:05:14,930 --> 00:05:16,190
Yeah.
And we have to look at that,

104
00:05:16,191 --> 00:05:18,710
but we also have to look in the
context because we also have,

105
00:05:18,740 --> 00:05:21,200
I think we talked about this a little
bit in the last podcast, but we,

106
00:05:21,580 --> 00:05:26,390
we have gamers on the platform who are
saying exactly that to their friends that

107
00:05:26,391 --> 00:05:28,340
they're going to meet at the
game in the game tonight.

108
00:05:28,820 --> 00:05:31,240
And without the context of
their relationship with,

109
00:05:31,241 --> 00:05:32,990
at the context of the
conversation that we're having,

110
00:05:33,310 --> 00:05:36,590
we would take the exact same action on,
on them incorrectly.

111
00:05:36,880 --> 00:05:39,550
Yeah, absolutely. That
I, that I understand.

112
00:05:39,790 --> 00:05:43,390
I think in the case of Covington though,
this user was so high profile,

113
00:05:43,900 --> 00:05:44,830
he's a verified user.

114
00:05:44,831 --> 00:05:48,220
He's got something like 20,000 followers
and it was highlighted by numerous

115
00:05:48,221 --> 00:05:51,150
conservative media outlets saying,
wow, this guys, it screenshot it.

116
00:05:51,160 --> 00:05:51,993
It's being shared.

117
00:05:52,120 --> 00:05:56,050
I mean you had a Disney producer in like
saying a picture of a woodchipper with

118
00:05:56,051 --> 00:05:58,970
a body being thrown in saying that's
what he wanted to happen, you know?

119
00:05:58,980 --> 00:06:01,040
So I do know that some of
these accounts got locked.

120
00:06:01,060 --> 00:06:04,910
Disney producer was doing that. Well,
I'll, I'll clarify fact check me on that.

121
00:06:04,911 --> 00:06:07,310
But that's the, basically the
conversation that was had it,

122
00:06:07,311 --> 00:06:08,630
there's a guy at Disney was,

123
00:06:08,631 --> 00:06:11,540
he posted a picture of him Fargo of
someone being tossed into woodchip are and

124
00:06:11,541 --> 00:06:14,930
he says, I want all these Maga
kids, you know, done like this.

125
00:06:14,960 --> 00:06:19,250
You had another guy who specifically said
lock them in the school, Barnett down,

126
00:06:19,310 --> 00:06:22,310
set a bunch of disparaging things and
then said if you see them fire on that

127
00:06:22,311 --> 00:06:23,510
many tweeted that more than once

128
00:06:23,630 --> 00:06:25,940
and that those accounts where
those streets were taken down,

129
00:06:25,970 --> 00:06:27,980
those were violations of our rules.
That's,

130
00:06:27,981 --> 00:06:31,270
I'm pretty sure it's actually illegal
to do that. Right. It's to, to to tell,

131
00:06:31,340 --> 00:06:36,340
to tell you or any individual to
commit a felony is a crime like right.

132
00:06:36,890 --> 00:06:39,470
Incitement of violence or yeah.
Yeah. So I have in many places,

133
00:06:39,471 --> 00:06:42,500
I just have to wonder how, how,
like I understand the context issue,

134
00:06:42,950 --> 00:06:44,720
but this is what,
this is what I talk.

135
00:06:44,750 --> 00:06:47,600
What were those context and
scale too though, right? The Tim,

136
00:06:47,601 --> 00:06:49,010
those accounts were actioned.

137
00:06:49,070 --> 00:06:51,680
They may not have been action the
way you wanted to too, but the,

138
00:06:51,681 --> 00:06:53,150
the tweets were forced to be deleted.

139
00:06:53,240 --> 00:06:57,910
And the account I can penalty for that.
So I understand that.

140
00:06:57,920 --> 00:07:01,960
But kind of a penalty. Well, again,
as I said earlier, Joe, we don't, uh,

141
00:07:02,000 --> 00:07:03,210
usually uh,

142
00:07:03,230 --> 00:07:06,080
automatically suspend accounts with
one violation because we want people to

143
00:07:06,081 --> 00:07:06,500
learn,

144
00:07:06,500 --> 00:07:09,410
we want people to understand what they
did wrong and give them an opportunity

145
00:07:09,470 --> 00:07:10,560
not to do it again.
Right?

146
00:07:10,610 --> 00:07:14,270
And it's a big thing to kick someone
off the platform and I take that very,

147
00:07:14,271 --> 00:07:15,170
very seriously.
So,

148
00:07:15,171 --> 00:07:19,340
so I want to make sure that
when someone violates our rules,

149
00:07:19,341 --> 00:07:22,660
they understand what happened and they're
given an opportunity to, you know,

150
00:07:22,700 --> 00:07:24,830
get back on the platform
and change their behavior.

151
00:07:25,190 --> 00:07:26,840
And so in many of these cases,

152
00:07:26,841 --> 00:07:30,530
what happens is we will force someone
to acknowledge that their tweet violated

153
00:07:30,531 --> 00:07:31,100
our rules,

154
00:07:31,100 --> 00:07:34,340
forced them to delete that tweet before
they can get back on the platform.

155
00:07:34,790 --> 00:07:38,180
And in, in many cases, if they do
it again, we give them a timeout,

156
00:07:38,181 --> 00:07:40,850
which is like seven days. When we
say, look, you've done it again,

157
00:07:41,240 --> 00:07:45,480
the temporary suspension. If you do it
again, you're a mom and the totally a mom.

158
00:07:45,500 --> 00:07:49,970
Exactly. And your mom too. And if you
do it again, then you're done. Right?

159
00:07:49,971 --> 00:07:53,150
So it's kind of like, you know, three
strikes. So it's Sorta like baseball.

160
00:07:53,480 --> 00:07:56,090
And so in some of these
cases at Tim's referencing,

161
00:07:56,091 --> 00:07:57,860
I have to imagine because
these tweets were deleted,

162
00:07:57,861 --> 00:07:59,240
they are violations of our rules.

163
00:07:59,570 --> 00:08:02,690
People are upset that the account came
back again and was allowed to say other

164
00:08:02,691 --> 00:08:06,240
things. But we did take action on those
streets. They were violations of our.

