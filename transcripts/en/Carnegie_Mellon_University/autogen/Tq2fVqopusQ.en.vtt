WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.330
 
[Music]

00:00:01.330 --> 00:00:01.340
[Music]
 

00:00:01.340 --> 00:00:03.710
[Music]
people are understanding that robot

00:00:03.710 --> 00:00:03.720
people are understanding that robot
 

00:00:03.720 --> 00:00:06.019
people are understanding that robot
systems don't exist in a vacuum that our

00:00:06.019 --> 00:00:06.029
systems don't exist in a vacuum that our
 

00:00:06.029 --> 00:00:08.330
systems don't exist in a vacuum that our
robots regardless of what they do are

00:00:08.330 --> 00:00:08.340
robots regardless of what they do are
 

00:00:08.340 --> 00:00:10.310
robots regardless of what they do are
going to have to interact with humans at

00:00:10.310 --> 00:00:10.320
going to have to interact with humans at
 

00:00:10.320 --> 00:00:11.839
going to have to interact with humans at
the end of the day these advances that

00:00:11.839 --> 00:00:11.849
the end of the day these advances that
 

00:00:11.849 --> 00:00:13.339
the end of the day these advances that
we're seeing in computer vision where

00:00:13.339 --> 00:00:13.349
we're seeing in computer vision where
 

00:00:13.349 --> 00:00:15.140
we're seeing in computer vision where
we're able to very very rapidly detect

00:00:15.140 --> 00:00:15.150
we're able to very very rapidly detect
 

00:00:15.150 --> 00:00:17.720
we're able to very very rapidly detect
people's facial expressions or people's

00:00:17.720 --> 00:00:17.730
people's facial expressions or people's
 

00:00:17.730 --> 00:00:19.730
people's facial expressions or people's
body posture through things like open

00:00:19.730 --> 00:00:19.740
body posture through things like open
 

00:00:19.740 --> 00:00:22.970
body posture through things like open
pose have enabled our systems to get the

00:00:22.970 --> 00:00:22.980
pose have enabled our systems to get the
 

00:00:22.980 --> 00:00:24.800
pose have enabled our systems to get the
data that they need in order to

00:00:24.800 --> 00:00:24.810
data that they need in order to
 

00:00:24.810 --> 00:00:27.349
data that they need in order to
interpret human social behavior and then

00:00:27.349 --> 00:00:27.359
interpret human social behavior and then
 

00:00:27.359 --> 00:00:29.540
interpret human social behavior and then
on the human robot interaction side all

00:00:29.540 --> 00:00:29.550
on the human robot interaction side all
 

00:00:29.550 --> 00:00:31.250
on the human robot interaction side all
of these you know many studies coming

00:00:31.250 --> 00:00:31.260
of these you know many studies coming
 

00:00:31.260 --> 00:00:32.510
of these you know many studies coming
out that are trying to understand how

00:00:32.510 --> 00:00:32.520
out that are trying to understand how
 

00:00:32.520 --> 00:00:34.160
out that are trying to understand how
people interact with robots in various

00:00:34.160 --> 00:00:34.170
people interact with robots in various
 

00:00:34.170 --> 00:00:36.620
people interact with robots in various
ways and how robots can interpret human

00:00:36.620 --> 00:00:36.630
ways and how robots can interpret human
 

00:00:36.630 --> 00:00:38.420
ways and how robots can interpret human
behavior and use their own social

00:00:38.420 --> 00:00:38.430
behavior and use their own social
 

00:00:38.430 --> 00:00:40.729
behavior and use their own social
behavior to improve the interactions

00:00:40.729 --> 00:00:40.739
behavior to improve the interactions
 

00:00:40.739 --> 00:00:43.729
behavior to improve the interactions
between human and robots and so on the

00:00:43.729 --> 00:00:43.739
between human and robots and so on the
 

00:00:43.739 --> 00:00:46.250
between human and robots and so on the
production and understanding side the

00:00:46.250 --> 00:00:46.260
production and understanding side the
 

00:00:46.260 --> 00:00:47.750
production and understanding side the
human robot interaction research is

00:00:47.750 --> 00:00:47.760
human robot interaction research is
 

00:00:47.760 --> 00:00:49.279
human robot interaction research is
going to play a big part my vision for

00:00:49.279 --> 00:00:49.289
going to play a big part my vision for
 

00:00:49.289 --> 00:00:50.869
going to play a big part my vision for
the future is that technology has a

00:00:50.869 --> 00:00:50.879
the future is that technology has a
 

00:00:50.879 --> 00:00:52.700
the future is that technology has a
positive impact we have to answer

00:00:52.700 --> 00:00:52.710
positive impact we have to answer
 

00:00:52.710 --> 00:00:54.830
positive impact we have to answer
questions about how our technology could

00:00:54.830 --> 00:00:54.840
questions about how our technology could
 

00:00:54.840 --> 00:00:56.540
questions about how our technology could
be used for good and could be used for

00:00:56.540 --> 00:00:56.550
be used for good and could be used for
 

00:00:56.550 --> 00:00:59.060
be used for good and could be used for
bad and how we can aim our technological

00:00:59.060 --> 00:00:59.070
bad and how we can aim our technological
 

00:00:59.070 --> 00:01:01.819
bad and how we can aim our technological
advances toward positive impact in the

00:01:01.819 --> 00:01:01.829
advances toward positive impact in the
 

00:01:01.829 --> 00:01:04.579
advances toward positive impact in the
world I think this really means that we

00:01:04.579 --> 00:01:04.589
world I think this really means that we
 

00:01:04.589 --> 00:01:06.380
world I think this really means that we
need to be multidisciplinary need to

00:01:06.380 --> 00:01:06.390
need to be multidisciplinary need to
 

00:01:06.390 --> 00:01:10.539
need to be multidisciplinary need to
pull in lawyers and policy makers and

00:01:10.539 --> 00:01:10.549
pull in lawyers and policy makers and
 

00:01:10.549 --> 00:01:14.179
pull in lawyers and policy makers and
people from social sciences and we also

00:01:14.179 --> 00:01:14.189
people from social sciences and we also
 

00:01:14.189 --> 00:01:16.310
people from social sciences and we also
need to pull in people from technology

00:01:16.310 --> 00:01:16.320
need to pull in people from technology
 

00:01:16.320 --> 00:01:18.410
need to pull in people from technology
who understand the capabilities of these

00:01:18.410 --> 00:01:18.420
who understand the capabilities of these
 

00:01:18.420 --> 00:01:19.789
who understand the capabilities of these
systems and understand what's even

00:01:19.789 --> 00:01:19.799
systems and understand what's even
 

00:01:19.799 --> 00:01:22.280
systems and understand what's even
possible to talk about in in terms of

00:01:22.280 --> 00:01:22.290
possible to talk about in in terms of
 

00:01:22.290 --> 00:01:30.060
possible to talk about in in terms of
the societal impact

00:01:30.060 --> 00:01:30.070
 
 

00:01:30.070 --> 00:01:33.450
 
so thanks to Ella for a wonderful talk

00:01:33.450 --> 00:01:33.460
so thanks to Ella for a wonderful talk
 

00:01:33.460 --> 00:01:35.850
so thanks to Ella for a wonderful talk
and Tom for those great questions to

00:01:35.850 --> 00:01:35.860
and Tom for those great questions to
 

00:01:35.860 --> 00:01:37.170
and Tom for those great questions to
help explore some of these issues in

00:01:37.170 --> 00:01:37.180
help explore some of these issues in
 

00:01:37.180 --> 00:01:39.210
help explore some of these issues in
more detail we're going to shift now

00:01:39.210 --> 00:01:39.220
more detail we're going to shift now
 

00:01:39.220 --> 00:01:42.180
more detail we're going to shift now
into the second session of the

00:01:42.180 --> 00:01:42.190
into the second session of the
 

00:01:42.190 --> 00:01:45.270
into the second session of the
conference focused on trust so I think

00:01:45.270 --> 00:01:45.280
conference focused on trust so I think
 

00:01:45.280 --> 00:01:47.010
conference focused on trust so I think
it's very easy for people to focus on

00:01:47.010 --> 00:01:47.020
it's very easy for people to focus on
 

00:01:47.020 --> 00:01:48.780
it's very easy for people to focus on
the trust that we might or might not

00:01:48.780 --> 00:01:48.790
the trust that we might or might not
 

00:01:48.790 --> 00:01:50.940
the trust that we might or might not
have in the technologies but one of the

00:01:50.940 --> 00:01:50.950
have in the technologies but one of the
 

00:01:50.950 --> 00:01:52.410
have in the technologies but one of the
important questions is to really go

00:01:52.410 --> 00:01:52.420
important questions is to really go
 

00:01:52.420 --> 00:01:54.570
important questions is to really go
beyond that and to think about the trust

00:01:54.570 --> 00:01:54.580
beyond that and to think about the trust
 

00:01:54.580 --> 00:01:56.250
beyond that and to think about the trust
that we might have and those who develop

00:01:56.250 --> 00:01:56.260
that we might have and those who develop
 

00:01:56.260 --> 00:01:57.420
that we might have and those who develop
the technology or those who are

00:01:57.420 --> 00:01:57.430
the technology or those who are
 

00:01:57.430 --> 00:01:59.490
the technology or those who are
deploying and using the technology or

00:01:59.490 --> 00:01:59.500
deploying and using the technology or
 

00:01:59.500 --> 00:02:01.020
deploying and using the technology or
those who regulate and write the laws

00:02:01.020 --> 00:02:01.030
those who regulate and write the laws
 

00:02:01.030 --> 00:02:03.770
those who regulate and write the laws
about the technology or even more how

00:02:03.770 --> 00:02:03.780
about the technology or even more how
 

00:02:03.780 --> 00:02:05.430
about the technology or even more how
technology might change the

00:02:05.430 --> 00:02:05.440
technology might change the
 

00:02:05.440 --> 00:02:07.650
technology might change the
relationships of trust that we have with

00:02:07.650 --> 00:02:07.660
relationships of trust that we have with
 

00:02:07.660 --> 00:02:10.230
relationships of trust that we have with
one another so how can the development

00:02:10.230 --> 00:02:10.240
one another so how can the development
 

00:02:10.240 --> 00:02:13.199
one another so how can the development
of a new technology change our human

00:02:13.199 --> 00:02:13.209
of a new technology change our human
 

00:02:13.209 --> 00:02:14.760
of a new technology change our human
human interactions including those that

00:02:14.760 --> 00:02:14.770
human interactions including those that
 

00:02:14.770 --> 00:02:16.890
human interactions including those that
are grounded in this really fundamental

00:02:16.890 --> 00:02:16.900
are grounded in this really fundamental
 

00:02:16.900 --> 00:02:18.690
are grounded in this really fundamental
notion of trust so we're gonna be

00:02:18.690 --> 00:02:18.700
notion of trust so we're gonna be
 

00:02:18.700 --> 00:02:20.699
notion of trust so we're gonna be
exploring that over the next hour and a

00:02:20.699 --> 00:02:20.709
exploring that over the next hour and a
 

00:02:20.709 --> 00:02:23.540
exploring that over the next hour and a
half and through it through a variety of

00:02:23.540 --> 00:02:23.550
half and through it through a variety of
 

00:02:23.550 --> 00:02:26.250
half and through it through a variety of
formats and we're going to start with a

00:02:26.250 --> 00:02:26.260
formats and we're going to start with a
 

00:02:26.260 --> 00:02:28.620
formats and we're going to start with a
talk by one of our K&amp;L gates

00:02:28.620 --> 00:02:28.630
talk by one of our K&amp;L gates
 

00:02:28.630 --> 00:02:30.540
talk by one of our K&amp;L gates
presidential fellows Abigail Marsh

00:02:30.540 --> 00:02:30.550
presidential fellows Abigail Marsh
 

00:02:30.550 --> 00:02:33.090
presidential fellows Abigail Marsh
Abigail was a double major in computer

00:02:33.090 --> 00:02:33.100
Abigail was a double major in computer
 

00:02:33.100 --> 00:02:34.770
Abigail was a double major in computer
science and mathematics at Oberlin

00:02:34.770 --> 00:02:34.780
science and mathematics at Oberlin
 

00:02:34.780 --> 00:02:37.740
science and mathematics at Oberlin
College in Ohio and is now a PhD student

00:02:37.740 --> 00:02:37.750
College in Ohio and is now a PhD student
 

00:02:37.750 --> 00:02:40.320
College in Ohio and is now a PhD student
in the societal computing program here

00:02:40.320 --> 00:02:40.330
in the societal computing program here
 

00:02:40.330 --> 00:02:42.479
in the societal computing program here
in the computer science department her

00:02:42.479 --> 00:02:42.489
in the computer science department her
 

00:02:42.489 --> 00:02:44.280
in the computer science department her
work really is focused on a notion of

00:02:44.280 --> 00:02:44.290
work really is focused on a notion of
 

00:02:44.290 --> 00:02:46.860
work really is focused on a notion of
usable privacy the idea of how do we

00:02:46.860 --> 00:02:46.870
usable privacy the idea of how do we
 

00:02:46.870 --> 00:02:50.130
usable privacy the idea of how do we
have privacy that is helping us achieve

00:02:50.130 --> 00:02:50.140
have privacy that is helping us achieve
 

00:02:50.140 --> 00:02:52.289
have privacy that is helping us achieve
the ends that we want and in particular

00:02:52.289 --> 00:02:52.299
the ends that we want and in particular
 

00:02:52.299 --> 00:02:54.449
the ends that we want and in particular
how that arises with adolescents and

00:02:54.449 --> 00:02:54.459
how that arises with adolescents and
 

00:02:54.459 --> 00:03:04.080
how that arises with adolescents and
teenagers so without further ado thank

00:03:04.080 --> 00:03:04.090
teenagers so without further ado thank
 

00:03:04.090 --> 00:03:06.060
teenagers so without further ado thank
you so much and thank you David for the

00:03:06.060 --> 00:03:06.070
you so much and thank you David for the
 

00:03:06.070 --> 00:03:07.620
you so much and thank you David for the
welcome that includes my undergraduate

00:03:07.620 --> 00:03:07.630
welcome that includes my undergraduate
 

00:03:07.630 --> 00:03:11.460
welcome that includes my undergraduate
education I really appreciate that um so

00:03:11.460 --> 00:03:11.470
education I really appreciate that um so
 

00:03:11.470 --> 00:03:14.310
education I really appreciate that um so
the funny thing about being a privacy

00:03:14.310 --> 00:03:14.320
the funny thing about being a privacy
 

00:03:14.320 --> 00:03:17.160
the funny thing about being a privacy
researcher is that when you see a news

00:03:17.160 --> 00:03:17.170
researcher is that when you see a news
 

00:03:17.170 --> 00:03:20.580
researcher is that when you see a news
story about some way that technology or

00:03:20.580 --> 00:03:20.590
story about some way that technology or
 

00:03:20.590 --> 00:03:23.220
story about some way that technology or
a tech company has violated trust you

00:03:23.220 --> 00:03:23.230
a tech company has violated trust you
 

00:03:23.230 --> 00:03:25.759
a tech company has violated trust you
actually see through that and you say oh

00:03:25.759 --> 00:03:25.769
actually see through that and you say oh
 

00:03:25.769 --> 00:03:28.590
actually see through that and you say oh
this is a privacy violation that these

00:03:28.590 --> 00:03:28.600
this is a privacy violation that these
 

00:03:28.600 --> 00:03:30.600
this is a privacy violation that these
people are really outraged about yes we

00:03:30.600 --> 00:03:30.610
people are really outraged about yes we
 

00:03:30.610 --> 00:03:33.810
people are really outraged about yes we
frame it in the public as trust but for

00:03:33.810 --> 00:03:33.820
frame it in the public as trust but for
 

00:03:33.820 --> 00:03:35.850
frame it in the public as trust but for
example you look at Cambridge analytic

00:03:35.850 --> 00:03:35.860
example you look at Cambridge analytic
 

00:03:35.860 --> 00:03:38.310
example you look at Cambridge analytic
as scandal recently Facebook has

00:03:38.310 --> 00:03:38.320
as scandal recently Facebook has
 

00:03:38.320 --> 00:03:40.940
as scandal recently Facebook has
multiple so let's focus on Cambridge and

00:03:40.940 --> 00:03:40.950
multiple so let's focus on Cambridge and
 

00:03:40.950 --> 00:03:42.850
multiple so let's focus on Cambridge and
you see that

00:03:42.850 --> 00:03:42.860
you see that
 

00:03:42.860 --> 00:03:46.630
you see that
what's really being talked about here is

00:03:46.630 --> 00:03:46.640
what's really being talked about here is
 

00:03:46.640 --> 00:03:49.510
what's really being talked about here is
the idea that Facebook would allow

00:03:49.510 --> 00:03:49.520
the idea that Facebook would allow
 

00:03:49.520 --> 00:03:52.210
the idea that Facebook would allow
outside parties to collect information

00:03:52.210 --> 00:03:52.220
outside parties to collect information
 

00:03:52.220 --> 00:03:55.450
outside parties to collect information
about people who did not consent to it

00:03:55.450 --> 00:03:55.460
about people who did not consent to it
 

00:03:55.460 --> 00:03:57.640
about people who did not consent to it
so they thought that information was

00:03:57.640 --> 00:03:57.650
so they thought that information was
 

00:03:57.650 --> 00:03:59.290
so they thought that information was
private they thought it was contained

00:03:59.290 --> 00:03:59.300
private they thought it was contained
 

00:03:59.300 --> 00:04:00.880
private they thought it was contained
within their social networks and

00:04:00.880 --> 00:04:00.890
within their social networks and
 

00:04:00.890 --> 00:04:02.940
within their social networks and
Facebook servers and they had no idea

00:04:02.940 --> 00:04:02.950
Facebook servers and they had no idea
 

00:04:02.950 --> 00:04:05.920
Facebook servers and they had no idea
that somebody outside of Facebook could

00:04:05.920 --> 00:04:05.930
that somebody outside of Facebook could
 

00:04:05.930 --> 00:04:08.560
that somebody outside of Facebook could
actually be harvesting it and using it

00:04:08.560 --> 00:04:08.570
actually be harvesting it and using it
 

00:04:08.570 --> 00:04:11.260
actually be harvesting it and using it
for unstated purposes that the user did

00:04:11.260 --> 00:04:11.270
for unstated purposes that the user did
 

00:04:11.270 --> 00:04:14.410
for unstated purposes that the user did
not get to consent to right and you also

00:04:14.410 --> 00:04:14.420
not get to consent to right and you also
 

00:04:14.420 --> 00:04:16.720
not get to consent to right and you also
see other examples of things like this

00:04:16.720 --> 00:04:16.730
see other examples of things like this
 

00:04:16.730 --> 00:04:18.789
see other examples of things like this
we talked a lot about algorithmic bias

00:04:18.789 --> 00:04:18.799
we talked a lot about algorithmic bias
 

00:04:18.799 --> 00:04:21.400
we talked a lot about algorithmic bias
and one way that privacy researchers

00:04:21.400 --> 00:04:21.410
and one way that privacy researchers
 

00:04:21.410 --> 00:04:22.810
and one way that privacy researchers
study algorithmic bias is through

00:04:22.810 --> 00:04:22.820
study algorithmic bias is through
 

00:04:22.820 --> 00:04:24.940
study algorithmic bias is through
behavioral advertising online behavioral

00:04:24.940 --> 00:04:24.950
behavioral advertising online behavioral
 

00:04:24.950 --> 00:04:26.200
behavioral advertising online behavioral
advertising where the fundamental

00:04:26.200 --> 00:04:26.210
advertising where the fundamental
 

00:04:26.210 --> 00:04:30.130
advertising where the fundamental
question is often how did this website

00:04:30.130 --> 00:04:30.140
question is often how did this website
 

00:04:30.140 --> 00:04:31.870
question is often how did this website
this advertising network which I've

00:04:31.870 --> 00:04:31.880
this advertising network which I've
 

00:04:31.880 --> 00:04:34.260
this advertising network which I've
never directly interacted with on my own

00:04:34.260 --> 00:04:34.270
never directly interacted with on my own
 

00:04:34.270 --> 00:04:36.940
never directly interacted with on my own
get this information about me to be able

00:04:36.940 --> 00:04:36.950
get this information about me to be able
 

00:04:36.950 --> 00:04:38.770
get this information about me to be able
to profile me and sell me these ads

00:04:38.770 --> 00:04:38.780
to profile me and sell me these ads
 

00:04:38.780 --> 00:04:41.620
to profile me and sell me these ads
which may be biased in some way as a

00:04:41.620 --> 00:04:41.630
which may be biased in some way as a
 

00:04:41.630 --> 00:04:43.330
which may be biased in some way as a
woman maybe I'm more likely to see

00:04:43.330 --> 00:04:43.340
woman maybe I'm more likely to see
 

00:04:43.340 --> 00:04:45.730
woman maybe I'm more likely to see
pregnancy-related ads or mother related

00:04:45.730 --> 00:04:45.740
pregnancy-related ads or mother related
 

00:04:45.740 --> 00:04:48.070
pregnancy-related ads or mother related
ads and I didn't even know that the

00:04:48.070 --> 00:04:48.080
ads and I didn't even know that the
 

00:04:48.080 --> 00:04:49.930
ads and I didn't even know that the
advertising network knew my gender to

00:04:49.930 --> 00:04:49.940
advertising network knew my gender to
 

00:04:49.940 --> 00:04:52.180
advertising network knew my gender to
begin with right so these trust

00:04:52.180 --> 00:04:52.190
begin with right so these trust
 

00:04:52.190 --> 00:04:54.010
begin with right so these trust
violations are actually privacy

00:04:54.010 --> 00:04:54.020
violations are actually privacy
 

00:04:54.020 --> 00:04:57.520
violations are actually privacy
violations in my own work I decided to

00:04:57.520 --> 00:04:57.530
violations in my own work I decided to
 

00:04:57.530 --> 00:05:01.600
violations in my own work I decided to
say privacy is is a fantastic area of

00:05:01.600 --> 00:05:01.610
say privacy is is a fantastic area of
 

00:05:01.610 --> 00:05:04.030
say privacy is is a fantastic area of
research for technologists but so far

00:05:04.030 --> 00:05:04.040
research for technologists but so far
 

00:05:04.040 --> 00:05:06.970
research for technologists but so far
it's really been focused on self

00:05:06.970 --> 00:05:06.980
it's really been focused on self
 

00:05:06.980 --> 00:05:09.580
it's really been focused on self
sufficient adults people in their 20s or

00:05:09.580 --> 00:05:09.590
sufficient adults people in their 20s or
 

00:05:09.590 --> 00:05:11.890
sufficient adults people in their 20s or
30s or 40s or 50s and I'm sorry for

00:05:11.890 --> 00:05:11.900
30s or 40s or 50s and I'm sorry for
 

00:05:11.900 --> 00:05:13.420
30s or 40s or 50s and I'm sorry for
those of you who've been left out of

00:05:13.420 --> 00:05:13.430
those of you who've been left out of
 

00:05:13.430 --> 00:05:15.490
those of you who've been left out of
that but that is the range it's

00:05:15.490 --> 00:05:15.500
that but that is the range it's
 

00:05:15.500 --> 00:05:18.070
that but that is the range it's
generally considered worth studying in a

00:05:18.070 --> 00:05:18.080
generally considered worth studying in a
 

00:05:18.080 --> 00:05:21.310
generally considered worth studying in a
lot of privacy research and there aren't

00:05:21.310 --> 00:05:21.320
lot of privacy research and there aren't
 

00:05:21.320 --> 00:05:24.790
lot of privacy research and there aren't
a lot of people who have focused on the

00:05:24.790 --> 00:05:24.800
a lot of people who have focused on the
 

00:05:24.800 --> 00:05:27.219
a lot of people who have focused on the
fact that people outside of that

00:05:27.219 --> 00:05:27.229
fact that people outside of that
 

00:05:27.229 --> 00:05:29.320
fact that people outside of that
relatively narrow range of human

00:05:29.320 --> 00:05:29.330
relatively narrow range of human
 

00:05:29.330 --> 00:05:31.150
relatively narrow range of human
experience are interacting with

00:05:31.150 --> 00:05:31.160
experience are interacting with
 

00:05:31.160 --> 00:05:32.830
experience are interacting with
technology sharing their information

00:05:32.830 --> 00:05:32.840
technology sharing their information
 

00:05:32.840 --> 00:05:34.930
technology sharing their information
with these websites and with the

00:05:34.930 --> 00:05:34.940
with these websites and with the
 

00:05:34.940 --> 00:05:37.330
with these websites and with the
software and therefore can experience

00:05:37.330 --> 00:05:37.340
software and therefore can experience
 

00:05:37.340 --> 00:05:40.750
software and therefore can experience
privacy violations as well many of you

00:05:40.750 --> 00:05:40.760
privacy violations as well many of you
 

00:05:40.760 --> 00:05:42.550
privacy violations as well many of you
in this room will have been parents or

00:05:42.550 --> 00:05:42.560
in this room will have been parents or
 

00:05:42.560 --> 00:05:44.740
in this room will have been parents or
if you are parents at the very least you

00:05:44.740 --> 00:05:44.750
if you are parents at the very least you
 

00:05:44.750 --> 00:05:47.290
if you are parents at the very least you
were child at one point and you probably

00:05:47.290 --> 00:05:47.300
were child at one point and you probably
 

00:05:47.300 --> 00:05:49.750
were child at one point and you probably
recognize that the teenage years are

00:05:49.750 --> 00:05:49.760
recognize that the teenage years are
 

00:05:49.760 --> 00:05:51.700
recognize that the teenage years are
very difficult years it's hard to raise

00:05:51.700 --> 00:05:51.710
very difficult years it's hard to raise
 

00:05:51.710 --> 00:05:53.740
very difficult years it's hard to raise
a teenager especially

00:05:53.740 --> 00:05:53.750
a teenager especially
 

00:05:53.750 --> 00:05:56.560
a teenager especially
in today's era where teens are spending

00:05:56.560 --> 00:05:56.570
in today's era where teens are spending
 

00:05:56.570 --> 00:05:58.180
in today's era where teens are spending
a lot of time online or spending time

00:05:58.180 --> 00:05:58.190
a lot of time online or spending time
 

00:05:58.190 --> 00:06:01.600
a lot of time online or spending time
with their devices right and so as a

00:06:01.600 --> 00:06:01.610
with their devices right and so as a
 

00:06:01.610 --> 00:06:04.360
with their devices right and so as a
parent it's natural to want some sort of

00:06:04.360 --> 00:06:04.370
parent it's natural to want some sort of
 

00:06:04.370 --> 00:06:06.700
parent it's natural to want some sort of
assistance some sort of help and there

00:06:06.700 --> 00:06:06.710
assistance some sort of help and there
 

00:06:06.710 --> 00:06:08.620
assistance some sort of help and there
are a lot of software aids that promise

00:06:08.620 --> 00:06:08.630
are a lot of software aids that promise
 

00:06:08.630 --> 00:06:11.170
are a lot of software aids that promise
to parents oh we'll take care of making

00:06:11.170 --> 00:06:11.180
to parents oh we'll take care of making
 

00:06:11.180 --> 00:06:13.570
to parents oh we'll take care of making
sure that your child stays out of bad

00:06:13.570 --> 00:06:13.580
sure that your child stays out of bad
 

00:06:13.580 --> 00:06:15.910
sure that your child stays out of bad
websites or only uses appropriate

00:06:15.910 --> 00:06:15.920
websites or only uses appropriate
 

00:06:15.920 --> 00:06:17.740
websites or only uses appropriate
language or something like that which

00:06:17.740 --> 00:06:17.750
language or something like that which
 

00:06:17.750 --> 00:06:19.630
language or something like that which
seems like a miracle fix right you can't

00:06:19.630 --> 00:06:19.640
seems like a miracle fix right you can't
 

00:06:19.640 --> 00:06:22.600
seems like a miracle fix right you can't
be watching your child's phone use over

00:06:22.600 --> 00:06:22.610
be watching your child's phone use over
 

00:06:22.610 --> 00:06:24.100
be watching your child's phone use over
their shoulder every second of the day

00:06:24.100 --> 00:06:24.110
their shoulder every second of the day
 

00:06:24.110 --> 00:06:26.980
their shoulder every second of the day
so great install a software and do that

00:06:26.980 --> 00:06:26.990
so great install a software and do that
 

00:06:26.990 --> 00:06:28.870
so great install a software and do that
and these software's offer a lot of

00:06:28.870 --> 00:06:28.880
and these software's offer a lot of
 

00:06:28.880 --> 00:06:30.700
and these software's offer a lot of
benefits especially for younger children

00:06:30.700 --> 00:06:30.710
benefits especially for younger children
 

00:06:30.710 --> 00:06:33.520
benefits especially for younger children
maybe we agree that five-year-old

00:06:33.520 --> 00:06:33.530
maybe we agree that five-year-old
 

00:06:33.530 --> 00:06:34.900
maybe we agree that five-year-old
shouldn't have access to pornography

00:06:34.900 --> 00:06:34.910
shouldn't have access to pornography
 

00:06:34.910 --> 00:06:38.320
shouldn't have access to pornography
right so it's easy to hit just block all

00:06:38.320 --> 00:06:38.330
right so it's easy to hit just block all
 

00:06:38.330 --> 00:06:41.200
right so it's easy to hit just block all
porn and your five-year-old can use your

00:06:41.200 --> 00:06:41.210
porn and your five-year-old can use your
 

00:06:41.210 --> 00:06:43.690
porn and your five-year-old can use your
iPad without any danger of a weird

00:06:43.690 --> 00:06:43.700
iPad without any danger of a weird
 

00:06:43.700 --> 00:06:46.210
iPad without any danger of a weird
Google result sort of like prompting

00:06:46.210 --> 00:06:46.220
Google result sort of like prompting
 

00:06:46.220 --> 00:06:47.800
Google result sort of like prompting
that birds-and-the-bees discussion a

00:06:47.800 --> 00:06:47.810
that birds-and-the-bees discussion a
 

00:06:47.810 --> 00:06:51.310
that birds-and-the-bees discussion a
little too early but it runs into a lot

00:06:51.310 --> 00:06:51.320
little too early but it runs into a lot
 

00:06:51.320 --> 00:06:53.170
little too early but it runs into a lot
of issues when we talk about teens and

00:06:53.170 --> 00:06:53.180
of issues when we talk about teens and
 

00:06:53.180 --> 00:06:55.120
of issues when we talk about teens and
preteens who are doing a lot of their

00:06:55.120 --> 00:06:55.130
preteens who are doing a lot of their
 

00:06:55.130 --> 00:06:56.710
preteens who are doing a lot of their
socialization and identity formation

00:06:56.710 --> 00:06:56.720
socialization and identity formation
 

00:06:56.720 --> 00:06:59.050
socialization and identity formation
online right they're talking to their

00:06:59.050 --> 00:06:59.060
online right they're talking to their
 

00:06:59.060 --> 00:07:00.970
online right they're talking to their
friends they're figuring out who they

00:07:00.970 --> 00:07:00.980
friends they're figuring out who they
 

00:07:00.980 --> 00:07:02.380
friends they're figuring out who they
are and some of that might mean

00:07:02.380 --> 00:07:02.390
are and some of that might mean
 

00:07:02.390 --> 00:07:04.920
are and some of that might mean
exploring concepts and topics that are

00:07:04.920 --> 00:07:04.930
exploring concepts and topics that are
 

00:07:04.930 --> 00:07:07.210
exploring concepts and topics that are
occasionally inappropriate and in

00:07:07.210 --> 00:07:07.220
occasionally inappropriate and in
 

00:07:07.220 --> 00:07:09.280
occasionally inappropriate and in
general might be inappropriate but are

00:07:09.280 --> 00:07:09.290
general might be inappropriate but are
 

00:07:09.290 --> 00:07:11.740
general might be inappropriate but are
necessary and important in certain

00:07:11.740 --> 00:07:11.750
necessary and important in certain
 

00:07:11.750 --> 00:07:14.950
necessary and important in certain
contexts right and there's a lot of

00:07:14.950 --> 00:07:14.960
contexts right and there's a lot of
 

00:07:14.960 --> 00:07:17.170
contexts right and there's a lot of
trust violations that these softwares

00:07:17.170 --> 00:07:17.180
trust violations that these softwares
 

00:07:17.180 --> 00:07:20.590
trust violations that these softwares
can pose as a child why would I want a

00:07:20.590 --> 00:07:20.600
can pose as a child why would I want a
 

00:07:20.600 --> 00:07:22.930
can pose as a child why would I want a
software like this on my phone how do I

00:07:22.930 --> 00:07:22.940
software like this on my phone how do I
 

00:07:22.940 --> 00:07:24.670
software like this on my phone how do I
know it's going to respect my autonomy

00:07:24.670 --> 00:07:24.680
know it's going to respect my autonomy
 

00:07:24.680 --> 00:07:27.250
know it's going to respect my autonomy
it's essentially designed to snitch on

00:07:27.250 --> 00:07:27.260
it's essentially designed to snitch on
 

00:07:27.260 --> 00:07:29.920
it's essentially designed to snitch on
everything I've ever done wrong and send

00:07:29.920 --> 00:07:29.930
everything I've ever done wrong and send
 

00:07:29.930 --> 00:07:31.810
everything I've ever done wrong and send
an alert directly to my parent at the

00:07:31.810 --> 00:07:31.820
an alert directly to my parent at the
 

00:07:31.820 --> 00:07:34.560
an alert directly to my parent at the
moment of violation that like hey

00:07:34.560 --> 00:07:34.570
moment of violation that like hey
 

00:07:34.570 --> 00:07:36.850
moment of violation that like hey
something is going wrong here you should

00:07:36.850 --> 00:07:36.860
something is going wrong here you should
 

00:07:36.860 --> 00:07:41.050
something is going wrong here you should
go punish your child right and even if

00:07:41.050 --> 00:07:41.060
go punish your child right and even if
 

00:07:41.060 --> 00:07:43.330
go punish your child right and even if
we don't consider the child's feelings

00:07:43.330 --> 00:07:43.340
we don't consider the child's feelings
 

00:07:43.340 --> 00:07:45.580
we don't consider the child's feelings
there's still the parents concerns about

00:07:45.580 --> 00:07:45.590
there's still the parents concerns about
 

00:07:45.590 --> 00:07:48.790
there's still the parents concerns about
trust right these software's can collect

00:07:48.790 --> 00:07:48.800
trust right these software's can collect
 

00:07:48.800 --> 00:07:52.030
trust right these software's can collect
a lot of information about what the kid

00:07:52.030 --> 00:07:52.040
a lot of information about what the kid
 

00:07:52.040 --> 00:07:53.650
a lot of information about what the kid
is doing with their phone some of them

00:07:53.650 --> 00:07:53.660
is doing with their phone some of them
 

00:07:53.660 --> 00:07:57.190
is doing with their phone some of them
suggest collecting every text message

00:07:57.190 --> 00:07:57.200
suggest collecting every text message
 

00:07:57.200 --> 00:07:58.600
suggest collecting every text message
the contents of every text message that

00:07:58.600 --> 00:07:58.610
the contents of every text message that
 

00:07:58.610 --> 00:08:00.490
the contents of every text message that
your child sends or every social media

00:08:00.490 --> 00:08:00.500
your child sends or every social media
 

00:08:00.500 --> 00:08:01.390
your child sends or every social media
message right

00:08:01.390 --> 00:08:01.400
message right
 

00:08:01.400 --> 00:08:03.520
message right
that means text and

00:08:03.520 --> 00:08:03.530
that means text and
 

00:08:03.530 --> 00:08:07.420
that means text and
and pictures I don't necessarily want a

00:08:07.420 --> 00:08:07.430
and pictures I don't necessarily want a
 

00:08:07.430 --> 00:08:09.520
and pictures I don't necessarily want a
company to have that data about me or

00:08:09.520 --> 00:08:09.530
company to have that data about me or
 

00:08:09.530 --> 00:08:13.780
company to have that data about me or
about my child and these are concerns

00:08:13.780 --> 00:08:13.790
about my child and these are concerns
 

00:08:13.790 --> 00:08:15.730
about my child and these are concerns
that parents have to face when they

00:08:15.730 --> 00:08:15.740
that parents have to face when they
 

00:08:15.740 --> 00:08:18.250
that parents have to face when they
choose to install a software like this

00:08:18.250 --> 00:08:18.260
choose to install a software like this
 

00:08:18.260 --> 00:08:20.500
choose to install a software like this
on their child's device right we don't

00:08:20.500 --> 00:08:20.510
on their child's device right we don't
 

00:08:20.510 --> 00:08:22.180
on their child's device right we don't
like when Google or Facebook has this

00:08:22.180 --> 00:08:22.190
like when Google or Facebook has this
 

00:08:22.190 --> 00:08:24.760
like when Google or Facebook has this
information but we're supposed to be ok

00:08:24.760 --> 00:08:24.770
information but we're supposed to be ok
 

00:08:24.770 --> 00:08:26.860
information but we're supposed to be ok
with it when a much smaller less

00:08:26.860 --> 00:08:26.870
with it when a much smaller less
 

00:08:26.870 --> 00:08:29.320
with it when a much smaller less
accountable company does and finally

00:08:29.320 --> 00:08:29.330
accountable company does and finally
 

00:08:29.330 --> 00:08:31.120
accountable company does and finally
there are questions of algorithmic

00:08:31.120 --> 00:08:31.130
there are questions of algorithmic
 

00:08:31.130 --> 00:08:33.880
there are questions of algorithmic
biases a lot of these softwares use ml

00:08:33.880 --> 00:08:33.890
biases a lot of these softwares use ml
 

00:08:33.890 --> 00:08:36.670
biases a lot of these softwares use ml
to do something like predictive blocking

00:08:36.670 --> 00:08:36.680
to do something like predictive blocking
 

00:08:36.680 --> 00:08:38.830
to do something like predictive blocking
of content within webpages but how do I

00:08:38.830 --> 00:08:38.840
of content within webpages but how do I
 

00:08:38.840 --> 00:08:41.980
of content within webpages but how do I
know as a parent that this opaque

00:08:41.980 --> 00:08:41.990
know as a parent that this opaque
 

00:08:41.990 --> 00:08:44.380
know as a parent that this opaque
algorithm is respecting my parenting

00:08:44.380 --> 00:08:44.390
algorithm is respecting my parenting
 

00:08:44.390 --> 00:08:46.030
algorithm is respecting my parenting
wishes is blocking what it should be and

00:08:46.030 --> 00:08:46.040
wishes is blocking what it should be and
 

00:08:46.040 --> 00:08:48.190
wishes is blocking what it should be and
is not blocking what it shouldn't be so

00:08:48.190 --> 00:08:48.200
is not blocking what it shouldn't be so
 

00:08:48.200 --> 00:08:51.100
is not blocking what it shouldn't be so
there are a lot of trust issues here

00:08:51.100 --> 00:08:51.110
there are a lot of trust issues here
 

00:08:51.110 --> 00:08:54.970
there are a lot of trust issues here
with technology that involve people

00:08:54.970 --> 00:08:54.980
with technology that involve people
 

00:08:54.980 --> 00:08:58.630
with technology that involve people
other than than self sufficient sort of

00:08:58.630 --> 00:08:58.640
other than than self sufficient sort of
 

00:08:58.640 --> 00:09:00.700
other than than self sufficient sort of
middle range adults who are willingly

00:09:00.700 --> 00:09:00.710
middle range adults who are willingly
 

00:09:00.710 --> 00:09:03.730
middle range adults who are willingly
interacting with the the software in

00:09:03.730 --> 00:09:03.740
interacting with the the software in
 

00:09:03.740 --> 00:09:06.520
interacting with the the software in
question and we need to continue to ask

00:09:06.520 --> 00:09:06.530
question and we need to continue to ask
 

00:09:06.530 --> 00:09:09.160
question and we need to continue to ask
questions about who is using this and

00:09:09.160 --> 00:09:09.170
questions about who is using this and
 

00:09:09.170 --> 00:09:11.290
questions about who is using this and
who is being affected so we make sure to

00:09:11.290 --> 00:09:11.300
who is being affected so we make sure to
 

00:09:11.300 --> 00:09:13.390
who is being affected so we make sure to
capture the experiences and the privacy

00:09:13.390 --> 00:09:13.400
capture the experiences and the privacy
 

00:09:13.400 --> 00:09:26.020
capture the experiences and the privacy
needs of everybody in our society thank

00:09:26.020 --> 00:09:26.030
needs of everybody in our society thank
 

00:09:26.030 --> 00:09:27.730
needs of everybody in our society thank
you so much happy really wonderful and

00:09:27.730 --> 00:09:27.740
you so much happy really wonderful and
 

00:09:27.740 --> 00:09:30.520
you so much happy really wonderful and
it's always wonderful to see the amazing

00:09:30.520 --> 00:09:30.530
it's always wonderful to see the amazing
 

00:09:30.530 --> 00:09:31.780
it's always wonderful to see the amazing
research that's being done by the

00:09:31.780 --> 00:09:31.790
research that's being done by the
 

00:09:31.790 --> 00:09:34.980
research that's being done by the
students here that it isn't simply

00:09:34.980 --> 00:09:34.990
students here that it isn't simply
 

00:09:34.990 --> 00:09:37.690
students here that it isn't simply
faculty it's the graduate students it's

00:09:37.690 --> 00:09:37.700
faculty it's the graduate students it's
 

00:09:37.700 --> 00:09:39.550
faculty it's the graduate students it's
the undergraduates it's the many people

00:09:39.550 --> 00:09:39.560
the undergraduates it's the many people
 

00:09:39.560 --> 00:09:41.350
the undergraduates it's the many people
around this community all engaged in

00:09:41.350 --> 00:09:41.360
around this community all engaged in
 

00:09:41.360 --> 00:09:45.100
around this community all engaged in
asking these kinds of questions so for

00:09:45.100 --> 00:09:45.110
asking these kinds of questions so for
 

00:09:45.110 --> 00:09:46.480
asking these kinds of questions so for
then for the next part of the session

00:09:46.480 --> 00:09:46.490
then for the next part of the session
 

00:09:46.490 --> 00:09:48.250
then for the next part of the session
we're gonna try something a little bit

00:09:48.250 --> 00:09:48.260
we're gonna try something a little bit
 

00:09:48.260 --> 00:09:50.410
we're gonna try something a little bit
different and and hopefully it will go

00:09:50.410 --> 00:09:50.420
different and and hopefully it will go
 

00:09:50.420 --> 00:09:53.440
different and and hopefully it will go
smoothly so we're going to ask three of

00:09:53.440 --> 00:09:53.450
smoothly so we're going to ask three of
 

00:09:53.450 --> 00:09:56.440
smoothly so we're going to ask three of
our distinguished guests to join us up

00:09:56.440 --> 00:09:56.450
our distinguished guests to join us up
 

00:09:56.450 --> 00:09:59.530
our distinguished guests to join us up
here and perhaps turn the tables a

00:09:59.530 --> 00:09:59.540
here and perhaps turn the tables a
 

00:09:59.540 --> 00:10:01.120
here and perhaps turn the tables a
little bit have them not simply be

00:10:01.120 --> 00:10:01.130
little bit have them not simply be
 

00:10:01.130 --> 00:10:04.210
little bit have them not simply be
speakers but also be moderators so what

00:10:04.210 --> 00:10:04.220
speakers but also be moderators so what
 

00:10:04.220 --> 00:10:05.290
speakers but also be moderators so what
we're calling I'm calling it a long

00:10:05.290 --> 00:10:05.300
we're calling I'm calling it a long
 

00:10:05.300 --> 00:10:07.600
we're calling I'm calling it a long
conversation and the idea is that we

00:10:07.600 --> 00:10:07.610
conversation and the idea is that we
 

00:10:07.610 --> 00:10:09.520
conversation and the idea is that we
will sequentially interview one another

00:10:09.520 --> 00:10:09.530
will sequentially interview one another
 

00:10:09.530 --> 00:10:11.260
will sequentially interview one another
for a brief period of time about five

00:10:11.260 --> 00:10:11.270
for a brief period of time about five
 

00:10:11.270 --> 00:10:13.030
for a brief period of time about five
minutes at which point there will be a

00:10:13.030 --> 00:10:13.040
minutes at which point there will be a
 

00:10:13.040 --> 00:10:15.460
minutes at which point there will be a
very discreet chime two-letter ooh and

00:10:15.460 --> 00:10:15.470
very discreet chime two-letter ooh and
 

00:10:15.470 --> 00:10:16.750
very discreet chime two-letter ooh and
know that we're going to move on in the

00:10:16.750 --> 00:10:16.760
know that we're going to move on in the
 

00:10:16.760 --> 00:10:17.290
know that we're going to move on in the
next

00:10:17.290 --> 00:10:17.300
next
 

00:10:17.300 --> 00:10:18.670
next
part of the interview will happen and

00:10:18.670 --> 00:10:18.680
part of the interview will happen and
 

00:10:18.680 --> 00:10:19.900
part of the interview will happen and
then we'll we'll have a much broader

00:10:19.900 --> 00:10:19.910
then we'll we'll have a much broader
 

00:10:19.910 --> 00:10:22.390
then we'll we'll have a much broader
conversation among the four of us around

00:10:22.390 --> 00:10:22.400
conversation among the four of us around
 

00:10:22.400 --> 00:10:24.700
conversation among the four of us around
issues of trust so I'm delighted to

00:10:24.700 --> 00:10:24.710
issues of trust so I'm delighted to
 

00:10:24.710 --> 00:10:27.400
issues of trust so I'm delighted to
invite up three people

00:10:27.400 --> 00:10:27.410
invite up three people
 

00:10:27.410 --> 00:10:29.650
invite up three people
Kirsten vineyard is an expert in

00:10:29.650 --> 00:10:29.660
Kirsten vineyard is an expert in
 

00:10:29.660 --> 00:10:31.870
Kirsten vineyard is an expert in
international security policy with over

00:10:31.870 --> 00:10:31.880
international security policy with over
 

00:10:31.880 --> 00:10:33.850
international security policy with over
20 years of experience at the United

00:10:33.850 --> 00:10:33.860
20 years of experience at the United
 

00:10:33.860 --> 00:10:35.770
20 years of experience at the United
Nations she's currently the deputy

00:10:35.770 --> 00:10:35.780
Nations she's currently the deputy
 

00:10:35.780 --> 00:10:37.750
Nations she's currently the deputy
director and chief of operations at the

00:10:37.750 --> 00:10:37.760
director and chief of operations at the
 

00:10:37.760 --> 00:10:39.790
director and chief of operations at the
UN Institute for disarmament research

00:10:39.790 --> 00:10:39.800
UN Institute for disarmament research
 

00:10:39.800 --> 00:10:42.700
UN Institute for disarmament research
unity R and since 2013 she's led you

00:10:42.700 --> 00:10:42.710
unity R and since 2013 she's led you
 

00:10:42.710 --> 00:10:44.500
unity R and since 2013 she's led you
Nadir's work on the weaponization of

00:10:44.500 --> 00:10:44.510
Nadir's work on the weaponization of
 

00:10:44.510 --> 00:10:46.650
Nadir's work on the weaponization of
increasingly autonomous technologies

00:10:46.650 --> 00:10:46.660
increasingly autonomous technologies
 

00:10:46.660 --> 00:10:49.180
increasingly autonomous technologies
that sentence came out wrong she's led

00:10:49.180 --> 00:10:49.190
that sentence came out wrong she's led
 

00:10:49.190 --> 00:10:50.830
that sentence came out wrong she's led
the work on trying to understand the

00:10:50.830 --> 00:10:50.840
the work on trying to understand the
 

00:10:50.840 --> 00:10:52.780
the work on trying to understand the
impacts and how we might address the

00:10:52.780 --> 00:10:52.790
impacts and how we might address the
 

00:10:52.790 --> 00:10:54.220
impacts and how we might address the
weaponization she has not been

00:10:54.220 --> 00:10:54.230
weaponization she has not been
 

00:10:54.230 --> 00:10:57.970
weaponization she has not been
weaponizing technology herself she also

00:10:57.970 --> 00:10:57.980
weaponizing technology herself she also
 

00:10:57.980 --> 00:10:59.770
weaponizing technology herself she also
served as a consultant to four of the

00:10:59.770 --> 00:10:59.780
served as a consultant to four of the
 

00:10:59.780 --> 00:11:02.140
served as a consultant to four of the
five UN groups of governmental experts

00:11:02.140 --> 00:11:02.150
five UN groups of governmental experts
 

00:11:02.150 --> 00:11:05.320
five UN groups of governmental experts
on cyber security Moshe Vardy is the

00:11:05.320 --> 00:11:05.330
on cyber security Moshe Vardy is the
 

00:11:05.330 --> 00:11:06.850
on cyber security Moshe Vardy is the
George Distinguished Service professor

00:11:06.850 --> 00:11:06.860
George Distinguished Service professor
 

00:11:06.860 --> 00:11:09.010
George Distinguished Service professor
in computational engineering and

00:11:09.010 --> 00:11:09.020
in computational engineering and
 

00:11:09.020 --> 00:11:10.270
in computational engineering and
director of the Ken Kennedy Institute

00:11:10.270 --> 00:11:10.280
director of the Ken Kennedy Institute
 

00:11:10.280 --> 00:11:12.490
director of the Ken Kennedy Institute
for information technology at Rice

00:11:12.490 --> 00:11:12.500
for information technology at Rice
 

00:11:12.500 --> 00:11:14.800
for information technology at Rice
University he's a world leader on

00:11:14.800 --> 00:11:14.810
University he's a world leader on
 

00:11:14.810 --> 00:11:16.420
University he's a world leader on
automated reasoning and is the author

00:11:16.420 --> 00:11:16.430
automated reasoning and is the author
 

00:11:16.430 --> 00:11:20.110
automated reasoning and is the author
and co-author of over 500 papers as well

00:11:20.110 --> 00:11:20.120
and co-author of over 500 papers as well
 

00:11:20.120 --> 00:11:21.880
and co-author of over 500 papers as well
as two books he's received numerous

00:11:21.880 --> 00:11:21.890
as two books he's received numerous
 

00:11:21.890 --> 00:11:24.280
as two books he's received numerous
awards and honorary degrees and is a

00:11:24.280 --> 00:11:24.290
awards and honorary degrees and is a
 

00:11:24.290 --> 00:11:25.960
awards and honorary degrees and is a
member of five different national

00:11:25.960 --> 00:11:25.970
member of five different national
 

00:11:25.970 --> 00:11:27.790
member of five different national
academies including the US National

00:11:27.790 --> 00:11:27.800
academies including the US National
 

00:11:27.800 --> 00:11:29.590
academies including the US National
Academy of Engineering and the US

00:11:29.590 --> 00:11:29.600
Academy of Engineering and the US
 

00:11:29.600 --> 00:11:32.320
Academy of Engineering and the US
National Academy of Sciences finally

00:11:32.320 --> 00:11:32.330
National Academy of Sciences finally
 

00:11:32.330 --> 00:11:34.270
National Academy of Sciences finally
Manuela Veloso is the Herbert a Simon

00:11:34.270 --> 00:11:34.280
Manuela Veloso is the Herbert a Simon
 

00:11:34.280 --> 00:11:36.580
Manuela Veloso is the Herbert a Simon
University professor and head of the

00:11:36.580 --> 00:11:36.590
University professor and head of the
 

00:11:36.590 --> 00:11:38.020
University professor and head of the
machine learning department here at

00:11:38.020 --> 00:11:38.030
machine learning department here at
 

00:11:38.030 --> 00:11:40.060
machine learning department here at
Carnegie Mellon University she's

00:11:40.060 --> 00:11:40.070
Carnegie Mellon University she's
 

00:11:40.070 --> 00:11:42.310
Carnegie Mellon University she's
transformed our understanding of social

00:11:42.310 --> 00:11:42.320
transformed our understanding of social
 

00:11:42.320 --> 00:11:44.320
transformed our understanding of social
and collaborative robotics and has also

00:11:44.320 --> 00:11:44.330
and collaborative robotics and has also
 

00:11:44.330 --> 00:11:46.390
and collaborative robotics and has also
authored over 500 papers

00:11:46.390 --> 00:11:46.400
authored over 500 papers
 

00:11:46.400 --> 00:11:47.920
authored over 500 papers
she was named the Einstein chair

00:11:47.920 --> 00:11:47.930
she was named the Einstein chair
 

00:11:47.930 --> 00:11:49.750
she was named the Einstein chair
professor by the Chinese Academy of

00:11:49.750 --> 00:11:49.760
professor by the Chinese Academy of
 

00:11:49.760 --> 00:11:52.690
professor by the Chinese Academy of
Sciences in 2012 is a fellow of the

00:11:52.690 --> 00:11:52.700
Sciences in 2012 is a fellow of the
 

00:11:52.700 --> 00:11:54.070
Sciences in 2012 is a fellow of the
American Association for the Advancement

00:11:54.070 --> 00:11:54.080
American Association for the Advancement
 

00:11:54.080 --> 00:11:56.320
American Association for the Advancement
of science as well as the I Triple E and

00:11:56.320 --> 00:11:56.330
of science as well as the I Triple E and
 

00:11:56.330 --> 00:11:58.930
of science as well as the I Triple E and
triple AI and is a former president of

00:11:58.930 --> 00:11:58.940
triple AI and is a former president of
 

00:11:58.940 --> 00:12:02.020
triple AI and is a former president of
the triple AI so Kirsten Moshe and

00:12:02.020 --> 00:12:02.030
the triple AI so Kirsten Moshe and
 

00:12:02.030 --> 00:12:03.340
the triple AI so Kirsten Moshe and
Manuela if you could join me up here on

00:12:03.340 --> 00:12:03.350
Manuela if you could join me up here on
 

00:12:03.350 --> 00:12:05.510
Manuela if you could join me up here on
stage

00:12:05.510 --> 00:12:05.520
stage
 

00:12:05.520 --> 00:12:27.569
stage
[Applause]

00:12:27.569 --> 00:12:27.579
 
 

00:12:27.579 --> 00:12:32.379
 
so start with Kirsten in your view what

00:12:32.379 --> 00:12:32.389
so start with Kirsten in your view what
 

00:12:32.389 --> 00:12:33.879
so start with Kirsten in your view what
is the most important or challenging

00:12:33.879 --> 00:12:33.889
is the most important or challenging
 

00:12:33.889 --> 00:12:36.579
is the most important or challenging
aspect for trust in AI and robotic

00:12:36.579 --> 00:12:36.589
aspect for trust in AI and robotic
 

00:12:36.589 --> 00:12:38.949
aspect for trust in AI and robotic
technologies including their development

00:12:38.949 --> 00:12:38.959
technologies including their development
 

00:12:38.959 --> 00:12:43.449
technologies including their development
and use a nice simple question with only

00:12:43.449 --> 00:12:43.459
and use a nice simple question with only
 

00:12:43.459 --> 00:12:44.980
and use a nice simple question with only
one answer the only one answer only a

00:12:44.980 --> 00:12:44.990
one answer the only one answer only a
 

00:12:44.990 --> 00:12:47.560
one answer the only one answer only a
possible all right well I'm coming to

00:12:47.560 --> 00:12:47.570
possible all right well I'm coming to
 

00:12:47.570 --> 00:12:48.879
possible all right well I'm coming to
this conversation from an international

00:12:48.879 --> 00:12:48.889
this conversation from an international
 

00:12:48.889 --> 00:12:51.310
this conversation from an international
security and arms control perspective so

00:12:51.310 --> 00:12:51.320
security and arms control perspective so
 

00:12:51.320 --> 00:12:52.720
security and arms control perspective so
I guess you won't be surprised when I

00:12:52.720 --> 00:12:52.730
I guess you won't be surprised when I
 

00:12:52.730 --> 00:12:54.550
I guess you won't be surprised when I
say that I find the implications of the

00:12:54.550 --> 00:12:54.560
say that I find the implications of the
 

00:12:54.560 --> 00:12:57.370
say that I find the implications of the
weaponization of these technologies to

00:12:57.370 --> 00:12:57.380
weaponization of these technologies to
 

00:12:57.380 --> 00:13:01.329
weaponization of these technologies to
be a particularly important topic issues

00:13:01.329 --> 00:13:01.339
be a particularly important topic issues
 

00:13:01.339 --> 00:13:03.939
be a particularly important topic issues
surrounding transparency validation and

00:13:03.939 --> 00:13:03.949
surrounding transparency validation and
 

00:13:03.949 --> 00:13:06.100
surrounding transparency validation and
verification of algorithmic systems are

00:13:06.100 --> 00:13:06.110
verification of algorithmic systems are
 

00:13:06.110 --> 00:13:07.960
verification of algorithmic systems are
among the most important open questions

00:13:07.960 --> 00:13:07.970
among the most important open questions
 

00:13:07.970 --> 00:13:10.030
among the most important open questions
in AI today and they are equally

00:13:10.030 --> 00:13:10.040
in AI today and they are equally
 

00:13:10.040 --> 00:13:11.230
in AI today and they are equally
important from an arms control

00:13:11.230 --> 00:13:11.240
important from an arms control
 

00:13:11.240 --> 00:13:14.350
important from an arms control
perspective when we start considering

00:13:14.350 --> 00:13:14.360
perspective when we start considering
 

00:13:14.360 --> 00:13:16.120
perspective when we start considering
the implications of increasing autonomy

00:13:16.120 --> 00:13:16.130
the implications of increasing autonomy
 

00:13:16.130 --> 00:13:17.980
the implications of increasing autonomy
in weapons systems so I'm going to give

00:13:17.980 --> 00:13:17.990
in weapons systems so I'm going to give
 

00:13:17.990 --> 00:13:21.490
in weapons systems so I'm going to give
you three examples of how Trust in these

00:13:21.490 --> 00:13:21.500
you three examples of how Trust in these
 

00:13:21.500 --> 00:13:23.650
you three examples of how Trust in these
systems is challenged by developments in

00:13:23.650 --> 00:13:23.660
systems is challenged by developments in
 

00:13:23.660 --> 00:13:26.199
systems is challenged by developments in
AI so first we have challenges for

00:13:26.199 --> 00:13:26.209
AI so first we have challenges for
 

00:13:26.209 --> 00:13:27.939
AI so first we have challenges for
testing and verification of weapons

00:13:27.939 --> 00:13:27.949
testing and verification of weapons
 

00:13:27.949 --> 00:13:30.269
testing and verification of weapons
systems and it's probably pretty obvious

00:13:30.269 --> 00:13:30.279
systems and it's probably pretty obvious
 

00:13:30.279 --> 00:13:33.670
systems and it's probably pretty obvious
but you really want to trust your weapon

00:13:33.670 --> 00:13:33.680
but you really want to trust your weapon
 

00:13:33.680 --> 00:13:36.160
but you really want to trust your weapon
systems these are technologies that are

00:13:36.160 --> 00:13:36.170
systems these are technologies that are
 

00:13:36.170 --> 00:13:39.040
systems these are technologies that are
intentionally designed to do harm right

00:13:39.040 --> 00:13:39.050
intentionally designed to do harm right
 

00:13:39.050 --> 00:13:41.019
intentionally designed to do harm right
unlike many of the technologies that we

00:13:41.019 --> 00:13:41.029
unlike many of the technologies that we
 

00:13:41.029 --> 00:13:44.860
unlike many of the technologies that we
design and because of that as operators

00:13:44.860 --> 00:13:44.870
design and because of that as operators
 

00:13:44.870 --> 00:13:47.079
design and because of that as operators
of these systems you have a moral and

00:13:47.079 --> 00:13:47.089
of these systems you have a moral and
 

00:13:47.089 --> 00:13:48.670
of these systems you have a moral and
legal responsibilities probably at a

00:13:48.670 --> 00:13:48.680
legal responsibilities probably at a
 

00:13:48.680 --> 00:13:51.939
legal responsibilities probably at a
higher level as then you would have as

00:13:51.939 --> 00:13:51.949
higher level as then you would have as
 

00:13:51.949 --> 00:13:53.199
higher level as then you would have as
an operator of a different type of

00:13:53.199 --> 00:13:53.209
an operator of a different type of
 

00:13:53.209 --> 00:13:55.300
an operator of a different type of
technology that's not designed to cause

00:13:55.300 --> 00:13:55.310
technology that's not designed to cause
 

00:13:55.310 --> 00:13:58.689
technology that's not designed to cause
harm and one of the reasons that we

00:13:58.689 --> 00:13:58.699
harm and one of the reasons that we
 

00:13:58.699 --> 00:14:01.120
harm and one of the reasons that we
trust our weapon systems is because we

00:14:01.120 --> 00:14:01.130
trust our weapon systems is because we
 

00:14:01.130 --> 00:14:03.490
trust our weapon systems is because we
rigorously test them before deployment

00:14:03.490 --> 00:14:03.500
rigorously test them before deployment
 

00:14:03.500 --> 00:14:05.530
rigorously test them before deployment
and this isn't an aspiration it's

00:14:05.530 --> 00:14:05.540
and this isn't an aspiration it's
 

00:14:05.540 --> 00:14:06.939
and this isn't an aspiration it's
actually a legal requirement under

00:14:06.939 --> 00:14:06.949
actually a legal requirement under
 

00:14:06.949 --> 00:14:09.189
actually a legal requirement under
article 36 of the Additional Protocol

00:14:09.189 --> 00:14:09.199
article 36 of the Additional Protocol
 

00:14:09.199 --> 00:14:12.040
article 36 of the Additional Protocol
one to the Geneva Conventions so

00:14:12.040 --> 00:14:12.050
one to the Geneva Conventions so
 

00:14:12.050 --> 00:14:13.480
one to the Geneva Conventions so
increasing autonomy and weapon systems

00:14:13.480 --> 00:14:13.490
increasing autonomy and weapon systems
 

00:14:13.490 --> 00:14:15.880
increasing autonomy and weapon systems
challenges testing I'm due in large part

00:14:15.880 --> 00:14:15.890
challenges testing I'm due in large part
 

00:14:15.890 --> 00:14:18.579
challenges testing I'm due in large part
to issues of predictability and it's

00:14:18.579 --> 00:14:18.589
to issues of predictability and it's
 

00:14:18.589 --> 00:14:20.140
to issues of predictability and it's
further complicated

00:14:20.140 --> 00:14:20.150
further complicated
 

00:14:20.150 --> 00:14:23.200
further complicated
the case of learning systems where the

00:14:23.200 --> 00:14:23.210
the case of learning systems where the
 

00:14:23.210 --> 00:14:25.180
the case of learning systems where the
system continues to learn post

00:14:25.180 --> 00:14:25.190
system continues to learn post
 

00:14:25.190 --> 00:14:29.680
system continues to learn post
deployment and then verification which

00:14:29.680 --> 00:14:29.690
deployment and then verification which
 

00:14:29.690 --> 00:14:31.120
deployment and then verification which
is a very important concept in arms

00:14:31.120 --> 00:14:31.130
is a very important concept in arms
 

00:14:31.130 --> 00:14:33.430
is a very important concept in arms
control we use it as a stability

00:14:33.430 --> 00:14:33.440
control we use it as a stability
 

00:14:33.440 --> 00:14:36.400
control we use it as a stability
mechanism verification so how could

00:14:36.400 --> 00:14:36.410
mechanism verification so how could
 

00:14:36.410 --> 00:14:37.960
mechanism verification so how could
international regulation of military

00:14:37.960 --> 00:14:37.970
international regulation of military
 

00:14:37.970 --> 00:14:39.990
international regulation of military
applications of AI be verified

00:14:39.990 --> 00:14:40.000
applications of AI be verified
 

00:14:40.000 --> 00:14:43.690
applications of AI be verified
verification usually involves counting

00:14:43.690 --> 00:14:43.700
verification usually involves counting
 

00:14:43.700 --> 00:14:47.860
verification usually involves counting
physical objects right so intangible

00:14:47.860 --> 00:14:47.870
physical objects right so intangible
 

00:14:47.870 --> 00:14:49.780
physical objects right so intangible
capabilities don't really work that way

00:14:49.780 --> 00:14:49.790
capabilities don't really work that way
 

00:14:49.790 --> 00:14:51.190
capabilities don't really work that way
and it's difficult to imagine that

00:14:51.190 --> 00:14:51.200
and it's difficult to imagine that
 

00:14:51.200 --> 00:14:53.290
and it's difficult to imagine that
states or even corporations would be

00:14:53.290 --> 00:14:53.300
states or even corporations would be
 

00:14:53.300 --> 00:14:56.260
states or even corporations would be
willing to permit inspection of their

00:14:56.260 --> 00:14:56.270
willing to permit inspection of their
 

00:14:56.270 --> 00:14:58.960
willing to permit inspection of their
code or algorithms and it's questionable

00:14:58.960 --> 00:14:58.970
code or algorithms and it's questionable
 

00:14:58.970 --> 00:15:00.340
code or algorithms and it's questionable
that even inspection would even be

00:15:00.340 --> 00:15:00.350
that even inspection would even be
 

00:15:00.350 --> 00:15:04.540
that even inspection would even be
useful at because determining whether or

00:15:04.540 --> 00:15:04.550
useful at because determining whether or
 

00:15:04.550 --> 00:15:05.860
useful at because determining whether or
not an algorithm is quote-unquote

00:15:05.860 --> 00:15:05.870
not an algorithm is quote-unquote
 

00:15:05.870 --> 00:15:09.220
not an algorithm is quote-unquote
militarized is probably near impossible

00:15:09.220 --> 00:15:09.230
militarized is probably near impossible
 

00:15:09.230 --> 00:15:12.130
militarized is probably near impossible
right because it's a dual use sort of

00:15:12.130 --> 00:15:12.140
right because it's a dual use sort of
 

00:15:12.140 --> 00:15:14.650
right because it's a dual use sort of
situation second example would be trust

00:15:14.650 --> 00:15:14.660
situation second example would be trust
 

00:15:14.660 --> 00:15:16.870
situation second example would be trust
by the user so machine learning systems

00:15:16.870 --> 00:15:16.880
by the user so machine learning systems
 

00:15:16.880 --> 00:15:20.440
by the user so machine learning systems
behave sometimes behave unexpectedly or

00:15:20.440 --> 00:15:20.450
behave sometimes behave unexpectedly or
 

00:15:20.450 --> 00:15:22.930
behave sometimes behave unexpectedly or
in unanticipated ways and in many cases

00:15:22.930 --> 00:15:22.940
in unanticipated ways and in many cases
 

00:15:22.940 --> 00:15:25.600
in unanticipated ways and in many cases
having AI helped us solve problems in

00:15:25.600 --> 00:15:25.610
having AI helped us solve problems in
 

00:15:25.610 --> 00:15:33.460
having AI helped us solve problems in
previously unimaginative AI but as we

00:15:33.460 --> 00:15:33.470
previously unimaginative AI but as we
 

00:15:33.470 --> 00:15:35.950
previously unimaginative AI but as we
become accustomed to AI enabled objects

00:15:35.950 --> 00:15:35.960
become accustomed to AI enabled objects
 

00:15:35.960 --> 00:15:38.830
become accustomed to AI enabled objects
behaving in surprising ways we're gonna

00:15:38.830 --> 00:15:38.840
behaving in surprising ways we're gonna
 

00:15:38.840 --> 00:15:40.930
behaving in surprising ways we're gonna
have one less metric to determine

00:15:40.930 --> 00:15:40.940
have one less metric to determine
 

00:15:40.940 --> 00:15:45.040
have one less metric to determine
whether a weapon system is behaving in a

00:15:45.040 --> 00:15:45.050
whether a weapon system is behaving in a
 

00:15:45.050 --> 00:15:47.710
whether a weapon system is behaving in a
way other than the way the operator

00:15:47.710 --> 00:15:47.720
way other than the way the operator
 

00:15:47.720 --> 00:15:50.860
way other than the way the operator
intended so that could be due to

00:15:50.860 --> 00:15:50.870
intended so that could be due to
 

00:15:50.870 --> 00:15:52.720
intended so that could be due to
internal error or it could be due to

00:15:52.720 --> 00:15:52.730
internal error or it could be due to
 

00:15:52.730 --> 00:15:54.700
internal error or it could be due to
malicious intervention within the system

00:15:54.700 --> 00:15:54.710
malicious intervention within the system
 

00:15:54.710 --> 00:15:56.500
malicious intervention within the system
and that's going to diminish the

00:15:56.500 --> 00:15:56.510
and that's going to diminish the
 

00:15:56.510 --> 00:15:58.120
and that's going to diminish the
operators ability to intervene or

00:15:58.120 --> 00:15:58.130
operators ability to intervene or
 

00:15:58.130 --> 00:16:01.300
operators ability to intervene or
exercise control and as military systems

00:16:01.300 --> 00:16:01.310
exercise control and as military systems
 

00:16:01.310 --> 00:16:02.980
exercise control and as military systems
become increasingly autonomous and

00:16:02.980 --> 00:16:02.990
become increasingly autonomous and
 

00:16:02.990 --> 00:16:06.310
become increasingly autonomous and
concurrently humans become decreasingly

00:16:06.310 --> 00:16:06.320
concurrently humans become decreasingly
 

00:16:06.320 --> 00:16:08.320
concurrently humans become decreasingly
present in their operation and oversight

00:16:08.320 --> 00:16:08.330
present in their operation and oversight
 

00:16:08.330 --> 00:16:10.720
present in their operation and oversight
there's a risk that humans no longer

00:16:10.720 --> 00:16:10.730
there's a risk that humans no longer
 

00:16:10.730 --> 00:16:13.480
there's a risk that humans no longer
serve as the redundant safety feature

00:16:13.480 --> 00:16:13.490
serve as the redundant safety feature
 

00:16:13.490 --> 00:16:15.550
serve as the redundant safety feature
which they currently serve in many

00:16:15.550 --> 00:16:15.560
which they currently serve in many
 

00:16:15.560 --> 00:16:17.440
which they currently serve in many
weapons systems I mean I think we're all

00:16:17.440 --> 00:16:17.450
weapons systems I mean I think we're all
 

00:16:17.450 --> 00:16:19.480
weapons systems I mean I think we're all
we've all know about the many nuclear

00:16:19.480 --> 00:16:19.490
we've all know about the many nuclear
 

00:16:19.490 --> 00:16:21.940
we've all know about the many nuclear
near misses that have been avoided

00:16:21.940 --> 00:16:21.950
near misses that have been avoided
 

00:16:21.950 --> 00:16:23.620
near misses that have been avoided
because a human operator followed their

00:16:23.620 --> 00:16:23.630
because a human operator followed their
 

00:16:23.630 --> 00:16:25.690
because a human operator followed their
instincts rather than what the

00:16:25.690 --> 00:16:25.700
instincts rather than what the
 

00:16:25.700 --> 00:16:27.190
instincts rather than what the
technology was telling them was

00:16:27.190 --> 00:16:27.200
technology was telling them was
 

00:16:27.200 --> 00:16:29.170
technology was telling them was
happening so if a human is no longer

00:16:29.170 --> 00:16:29.180
happening so if a human is no longer
 

00:16:29.180 --> 00:16:31.650
happening so if a human is no longer
capable of intervening

00:16:31.650 --> 00:16:31.660
capable of intervening
 

00:16:31.660 --> 00:16:36.180
capable of intervening
oh my can they finish up that Center

00:16:36.180 --> 00:16:36.190
oh my can they finish up that Center
 

00:16:36.190 --> 00:16:37.500
oh my can they finish up that Center
you've certainly finished the center all

00:16:37.500 --> 00:16:37.510
you've certainly finished the center all
 

00:16:37.510 --> 00:16:39.810
you've certainly finished the center all
right if they're no longer capable of

00:16:39.810 --> 00:16:39.820
right if they're no longer capable of
 

00:16:39.820 --> 00:16:43.590
right if they're no longer capable of
intervening due to speed or in your

00:16:43.590 --> 00:16:43.600
intervening due to speed or in your
 

00:16:43.600 --> 00:16:45.420
intervening due to speed or in your
ability to understand the system or

00:16:45.420 --> 00:16:45.430
ability to understand the system or
 

00:16:45.430 --> 00:16:48.290
ability to understand the system or
perhaps even worse we've left the human

00:16:48.290 --> 00:16:48.300
perhaps even worse we've left the human
 

00:16:48.300 --> 00:16:51.930
perhaps even worse we've left the human
on the loop marginally just in order to

00:16:51.930 --> 00:16:51.940
on the loop marginally just in order to
 

00:16:51.940 --> 00:16:55.050
on the loop marginally just in order to
serve as a scapegoat for technologic

00:16:55.050 --> 00:16:55.060
serve as a scapegoat for technologic
 

00:16:55.060 --> 00:16:57.120
serve as a scapegoat for technologic
technology's possible failings or

00:16:57.120 --> 00:16:57.130
technology's possible failings or
 

00:16:57.130 --> 00:16:59.100
technology's possible failings or
shortcomings and that's problematic so

00:16:59.100 --> 00:16:59.110
shortcomings and that's problematic so
 

00:16:59.110 --> 00:17:00.420
shortcomings and that's problematic so
those are two examples and maybe y'all

00:17:00.420 --> 00:17:00.430
those are two examples and maybe y'all
 

00:17:00.430 --> 00:17:01.890
those are two examples and maybe y'all
get a chance to tell your third I'm sure

00:17:01.890 --> 00:17:01.900
get a chance to tell your third I'm sure
 

00:17:01.900 --> 00:17:07.679
get a chance to tell your third I'm sure
there will be opportunities so good to

00:17:07.679 --> 00:17:07.689
there will be opportunities so good to
 

00:17:07.689 --> 00:17:11.460
there will be opportunities so good to
see much in your view what is the most

00:17:11.460 --> 00:17:11.470
see much in your view what is the most
 

00:17:11.470 --> 00:17:13.319
see much in your view what is the most
important or challenging aspect for

00:17:13.319 --> 00:17:13.329
important or challenging aspect for
 

00:17:13.329 --> 00:17:15.120
important or challenging aspect for
trust in AI robotic technologies

00:17:15.120 --> 00:17:15.130
trust in AI robotic technologies
 

00:17:15.130 --> 00:17:19.620
trust in AI robotic technologies
including in their development in use so

00:17:19.620 --> 00:17:19.630
including in their development in use so
 

00:17:19.630 --> 00:17:22.079
including in their development in use so
I think it's important to recognize that

00:17:22.079 --> 00:17:22.089
I think it's important to recognize that
 

00:17:22.089 --> 00:17:24.360
I think it's important to recognize that
we have any situation right now that we

00:17:24.360 --> 00:17:24.370
we have any situation right now that we
 

00:17:24.370 --> 00:17:28.650
we have any situation right now that we
have I would say trust crisis and and I

00:17:28.650 --> 00:17:28.660
have I would say trust crisis and and I
 

00:17:28.660 --> 00:17:30.570
have I would say trust crisis and and I
want to read quotes from two recent

00:17:30.570 --> 00:17:30.580
want to read quotes from two recent
 

00:17:30.580 --> 00:17:32.790
want to read quotes from two recent
articles appear in the Boston in fact in

00:17:32.790 --> 00:17:32.800
articles appear in the Boston in fact in
 

00:17:32.800 --> 00:17:36.600
articles appear in the Boston in fact in
the world journal just peggy noonan last

00:17:36.600 --> 00:17:36.610
the world journal just peggy noonan last
 

00:17:36.610 --> 00:17:39.510
the world journal just peggy noonan last
october article somehow talked about the

00:17:39.510 --> 00:17:39.520
october article somehow talked about the
 

00:17:39.520 --> 00:17:41.280
october article somehow talked about the
gun issue and she tried to explain why

00:17:41.280 --> 00:17:41.290
gun issue and she tried to explain why
 

00:17:41.290 --> 00:17:43.770
gun issue and she tried to explain why
people buy guns and argument I think is

00:17:43.770 --> 00:17:43.780
people buy guns and argument I think is
 

00:17:43.780 --> 00:17:45.600
people buy guns and argument I think is
very weak but it doesn't matter for the

00:17:45.600 --> 00:17:45.610
very weak but it doesn't matter for the
 

00:17:45.610 --> 00:17:48.540
very weak but it doesn't matter for the
purpose of this discussion and she

00:17:48.540 --> 00:17:48.550
purpose of this discussion and she
 

00:17:48.550 --> 00:17:50.760
purpose of this discussion and she
writes because all of the personal and

00:17:50.760 --> 00:17:50.770
writes because all of the personal and
 

00:17:50.770 --> 00:17:54.810
writes because all of the personal and
financial information got hacked in the

00:17:54.810 --> 00:17:54.820
financial information got hacked in the
 

00:17:54.820 --> 00:17:57.630
financial information got hacked in the
latest breach because the country

00:17:57.630 --> 00:17:57.640
latest breach because the country
 

00:17:57.640 --> 00:18:00.060
latest breach because the country
because our countries really over loads

00:18:00.060 --> 00:18:00.070
because our countries really over loads
 

00:18:00.070 --> 00:18:03.570
because our countries really over loads
are in Silicon Valley and appeared to be

00:18:03.570 --> 00:18:03.580
are in Silicon Valley and appeared to be
 

00:18:03.580 --> 00:18:07.410
are in Silicon Valley and appeared to be
moral Martians who operate on some will

00:18:07.410 --> 00:18:07.420
moral Martians who operate on some will
 

00:18:07.420 --> 00:18:11.880
moral Martians who operate on some will
new postmodern ethical wavelengths I

00:18:11.880 --> 00:18:11.890
new postmodern ethical wavelengths I
 

00:18:11.890 --> 00:18:14.040
new postmodern ethical wavelengths I
guess with financial planning for

00:18:14.040 --> 00:18:14.050
guess with financial planning for
 

00:18:14.050 --> 00:18:17.400
guess with financial planning for
immortality and there will be the ones

00:18:17.400 --> 00:18:17.410
immortality and there will be the ones
 

00:18:17.410 --> 00:18:20.370
immortality and there will be the ones
programming the robots that will soon

00:18:20.370 --> 00:18:20.380
programming the robots that will soon
 

00:18:20.380 --> 00:18:25.680
programming the robots that will soon
take all the jobs January 2 of this year

00:18:25.680 --> 00:18:25.690
take all the jobs January 2 of this year
 

00:18:25.690 --> 00:18:29.730
take all the jobs January 2 of this year
Niall Ferguson most alarming was the

00:18:29.730 --> 00:18:29.740
Niall Ferguson most alarming was the
 

00:18:29.740 --> 00:18:34.410
Niall Ferguson most alarming was the
morphing of cyberspace into Siberia not

00:18:34.410 --> 00:18:34.420
morphing of cyberspace into Siberia not
 

00:18:34.420 --> 00:18:37.320
morphing of cyberspace into Siberia not
to mention the cyber caliphates a dark

00:18:37.320 --> 00:18:37.330
to mention the cyber caliphates a dark
 

00:18:37.330 --> 00:18:41.150
to mention the cyber caliphates a dark
and lawless realm were malevolent actors

00:18:41.150 --> 00:18:41.160
and lawless realm were malevolent actors
 

00:18:41.160 --> 00:18:44.280
and lawless realm were malevolent actors
ranging from Russian trolls

00:18:44.280 --> 00:18:44.290
ranging from Russian trolls
 

00:18:44.290 --> 00:18:47.550
ranging from Russian trolls
- PO isis twitter users could work with

00:18:47.550 --> 00:18:47.560
- PO isis twitter users could work with
 

00:18:47.560 --> 00:18:51.150
- PO isis twitter users could work with
impurity to subvert the institutional

00:18:51.150 --> 00:18:51.160
impurity to subvert the institutional
 

00:18:51.160 --> 00:18:54.450
impurity to subvert the institutional
foundation of democracy if this does not

00:18:54.450 --> 00:18:54.460
foundation of democracy if this does not
 

00:18:54.460 --> 00:18:57.270
foundation of democracy if this does not
reflect a crisis in trust i don't know

00:18:57.270 --> 00:18:57.280
reflect a crisis in trust i don't know
 

00:18:57.280 --> 00:19:01.140
reflect a crisis in trust i don't know
what is so if we won't talk about trust

00:19:01.140 --> 00:19:01.150
what is so if we won't talk about trust
 

00:19:01.150 --> 00:19:03.870
what is so if we won't talk about trust
we have to start making what does trust

00:19:03.870 --> 00:19:03.880
we have to start making what does trust
 

00:19:03.880 --> 00:19:06.720
we have to start making what does trust
mean so I kind of tried to reflect a

00:19:06.720 --> 00:19:06.730
mean so I kind of tried to reflect a
 

00:19:06.730 --> 00:19:09.950
mean so I kind of tried to reflect a
little bit on my own personal experience

00:19:09.950 --> 00:19:09.960
little bit on my own personal experience
 

00:19:09.960 --> 00:19:13.860
little bit on my own personal experience
in in my military service in there in

00:19:13.860 --> 00:19:13.870
in in my military service in there in
 

00:19:13.870 --> 00:19:17.150
in in my military service in there in
the Israeli Defense Forces I went to

00:19:17.150 --> 00:19:17.160
the Israeli Defense Forces I went to
 

00:19:17.160 --> 00:19:20.970
the Israeli Defense Forces I went to
power - ting training and you jump from

00:19:20.970 --> 00:19:20.980
power - ting training and you jump from
 

00:19:20.980 --> 00:19:25.860
power - ting training and you jump from
a height of about 1400 feet which is a

00:19:25.860 --> 00:19:25.870
a height of about 1400 feet which is a
 

00:19:25.870 --> 00:19:28.530
a height of about 1400 feet which is a
rather terrifying experience I mean you

00:19:28.530 --> 00:19:28.540
rather terrifying experience I mean you
 

00:19:28.540 --> 00:19:31.290
rather terrifying experience I mean you
you you're back brain tells you you're

00:19:31.290 --> 00:19:31.300
you you're back brain tells you you're
 

00:19:31.300 --> 00:19:32.850
you you're back brain tells you you're
going to die because you are falling off

00:19:32.850 --> 00:19:32.860
going to die because you are falling off
 

00:19:32.860 --> 00:19:35.820
going to die because you are falling off
of the cliff and you have to trust the

00:19:35.820 --> 00:19:35.830
of the cliff and you have to trust the
 

00:19:35.830 --> 00:19:38.280
of the cliff and you have to trust the
parachute and you have to trust the

00:19:38.280 --> 00:19:38.290
parachute and you have to trust the
 

00:19:38.290 --> 00:19:40.170
parachute and you have to trust the
parachute folder that folder it properly

00:19:40.170 --> 00:19:40.180
parachute folder that folder it properly
 

00:19:40.180 --> 00:19:45.000
parachute folder that folder it properly
so trust means vulnerability you need

00:19:45.000 --> 00:19:45.010
so trust means vulnerability you need
 

00:19:45.010 --> 00:19:46.950
so trust means vulnerability you need
trust when you have a sense of danger

00:19:46.950 --> 00:19:46.960
trust when you have a sense of danger
 

00:19:46.960 --> 00:19:49.740
trust when you have a sense of danger
and you're trusting something or someone

00:19:49.740 --> 00:19:49.750
and you're trusting something or someone
 

00:19:49.750 --> 00:19:53.430
and you're trusting something or someone
to help you face this danger and so

00:19:53.430 --> 00:19:53.440
to help you face this danger and so
 

00:19:53.440 --> 00:19:55.290
to help you face this danger and so
obviously if we have a crisis of trust

00:19:55.290 --> 00:19:55.300
obviously if we have a crisis of trust
 

00:19:55.300 --> 00:19:56.880
obviously if we have a crisis of trust
it means you're all feeling very

00:19:56.880 --> 00:19:56.890
it means you're all feeling very
 

00:19:56.890 --> 00:20:00.210
it means you're all feeling very
vulnerable and do we trust the

00:20:00.210 --> 00:20:00.220
vulnerable and do we trust the
 

00:20:00.220 --> 00:20:03.210
vulnerable and do we trust the
technology that is now providing us so I

00:20:03.210 --> 00:20:03.220
technology that is now providing us so I
 

00:20:03.220 --> 00:20:04.770
technology that is now providing us so I
think when you come to cyber security

00:20:04.770 --> 00:20:04.780
think when you come to cyber security
 

00:20:04.780 --> 00:20:06.660
think when you come to cyber security
there's no cyber security there cyber

00:20:06.660 --> 00:20:06.670
there's no cyber security there cyber
 

00:20:06.670 --> 00:20:10.110
there's no cyber security there cyber
insecurity it's a colossal failure of us

00:20:10.110 --> 00:20:10.120
insecurity it's a colossal failure of us
 

00:20:10.120 --> 00:20:12.810
insecurity it's a colossal failure of us
is a computing discipline as well as

00:20:12.810 --> 00:20:12.820
is a computing discipline as well as
 

00:20:12.820 --> 00:20:14.940
is a computing discipline as well as
they as they I think is a political

00:20:14.940 --> 00:20:14.950
they as they I think is a political
 

00:20:14.950 --> 00:20:17.190
they as they I think is a political
institution around us that we have not

00:20:17.190 --> 00:20:17.200
institution around us that we have not
 

00:20:17.200 --> 00:20:19.020
institution around us that we have not
set up better standard for cyber

00:20:19.020 --> 00:20:19.030
set up better standard for cyber
 

00:20:19.030 --> 00:20:22.590
set up better standard for cyber
security and now we are talking about do

00:20:22.590 --> 00:20:22.600
security and now we are talking about do
 

00:20:22.600 --> 00:20:25.410
security and now we are talking about do
we trust the day the decisions the

00:20:25.410 --> 00:20:25.420
we trust the day the decisions the
 

00:20:25.420 --> 00:20:26.880
we trust the day the decisions the
machine will make and we heard a lot

00:20:26.880 --> 00:20:26.890
machine will make and we heard a lot
 

00:20:26.890 --> 00:20:29.550
machine will make and we heard a lot
about equity and biases and transparency

00:20:29.550 --> 00:20:29.560
about equity and biases and transparency
 

00:20:29.560 --> 00:20:32.340
about equity and biases and transparency
and I have heard very little discussion

00:20:32.340 --> 00:20:32.350
and I have heard very little discussion
 

00:20:32.350 --> 00:20:34.620
and I have heard very little discussion
should we delegate this decision to

00:20:34.620 --> 00:20:34.630
should we delegate this decision to
 

00:20:34.630 --> 00:20:37.110
should we delegate this decision to
machines you know we are somehow as if

00:20:37.110 --> 00:20:37.120
machines you know we are somehow as if
 

00:20:37.120 --> 00:20:38.940
machines you know we are somehow as if
we are living in a in a deterministic

00:20:38.940 --> 00:20:38.950
we are living in a in a deterministic
 

00:20:38.950 --> 00:20:41.010
we are living in a in a deterministic
universe in which it's just determined

00:20:41.010 --> 00:20:41.020
universe in which it's just determined
 

00:20:41.020 --> 00:20:42.780
universe in which it's just determined
the machines will make these human

00:20:42.780 --> 00:20:42.790
the machines will make these human
 

00:20:42.790 --> 00:20:45.030
the machines will make these human
decisions you know now it's just

00:20:45.030 --> 00:20:45.040
decisions you know now it's just
 

00:20:45.040 --> 00:20:47.100
decisions you know now it's just
deciding you know whether you'll get

00:20:47.100 --> 00:20:47.110
deciding you know whether you'll get
 

00:20:47.110 --> 00:20:49.320
deciding you know whether you'll get
power or not whether we will separate a

00:20:49.320 --> 00:20:49.330
power or not whether we will separate a
 

00:20:49.330 --> 00:20:51.690
power or not whether we will separate a
child from their from their family when

00:20:51.690 --> 00:20:51.700
child from their from their family when
 

00:20:51.700 --> 00:20:52.950
child from their from their family when
is it going to be sighted whom are you

00:20:52.950 --> 00:20:52.960
is it going to be sighted whom are you
 

00:20:52.960 --> 00:20:54.930
is it going to be sighted whom are you
going to who's going to be your mate you

00:20:54.930 --> 00:20:54.940
going to who's going to be your mate you
 

00:20:54.940 --> 00:20:55.379
going to who's going to be your mate you
know will

00:20:55.379 --> 00:20:55.389
know will
 

00:20:55.389 --> 00:20:58.019
know will
we already heard algorithmic why bother

00:20:58.019 --> 00:20:58.029
we already heard algorithmic why bother
 

00:20:58.029 --> 00:21:00.029
we already heard algorithmic why bother
was tinder you know we'll let the

00:21:00.029 --> 00:21:00.039
was tinder you know we'll let the
 

00:21:00.039 --> 00:21:01.469
was tinder you know we'll let the
Machine decide whom you should you

00:21:01.469 --> 00:21:01.479
Machine decide whom you should you
 

00:21:01.479 --> 00:21:05.549
Machine decide whom you should you
should go out with and so and the

00:21:05.549 --> 00:21:05.559
should go out with and so and the
 

00:21:05.559 --> 00:21:07.440
should go out with and so and the
reality is that people don't like to

00:21:07.440 --> 00:21:07.450
reality is that people don't like to
 

00:21:07.450 --> 00:21:09.779
reality is that people don't like to
talk so much here is that our life are

00:21:09.779 --> 00:21:09.789
talk so much here is that our life are
 

00:21:09.789 --> 00:21:12.919
talk so much here is that our life are
now with technology is be run by

00:21:12.919 --> 00:21:12.929
now with technology is be run by
 

00:21:12.929 --> 00:21:14.719
now with technology is be run by
incredibly large and powerful

00:21:14.719 --> 00:21:14.729
incredibly large and powerful
 

00:21:14.729 --> 00:21:17.839
incredibly large and powerful
corporations total equity of the five

00:21:17.839 --> 00:21:17.849
corporations total equity of the five
 

00:21:17.849 --> 00:21:20.339
corporations total equity of the five
tech companies over three trillion

00:21:20.339 --> 00:21:20.349
tech companies over three trillion
 

00:21:20.349 --> 00:21:22.589
tech companies over three trillion
dollars and one of the things we've

00:21:22.589 --> 00:21:22.599
dollars and one of the things we've
 

00:21:22.599 --> 00:21:24.569
dollars and one of the things we've
helped before there's no ethics when it

00:21:24.569 --> 00:21:24.579
helped before there's no ethics when it
 

00:21:24.579 --> 00:21:26.940
helped before there's no ethics when it
comes to corporations if it's if it's

00:21:26.940 --> 00:21:26.950
comes to corporations if it's if it's
 

00:21:26.950 --> 00:21:30.060
comes to corporations if it's if it's
legal they can do it and it makes money

00:21:30.060 --> 00:21:30.070
legal they can do it and it makes money
 

00:21:30.070 --> 00:21:39.749
legal they can do it and it makes money
they will do it we seem to be on this

00:21:39.749 --> 00:21:39.759
they will do it we seem to be on this
 

00:21:39.759 --> 00:21:43.709
they will do it we seem to be on this
one is quite a lot right new view what

00:21:43.709 --> 00:21:43.719
one is quite a lot right new view what
 

00:21:43.719 --> 00:21:44.879
one is quite a lot right new view what
is the most important or challenging

00:21:44.879 --> 00:21:44.889
is the most important or challenging
 

00:21:44.889 --> 00:21:47.099
is the most important or challenging
aspect for trust in iron robotic

00:21:47.099 --> 00:21:47.109
aspect for trust in iron robotic
 

00:21:47.109 --> 00:21:49.349
aspect for trust in iron robotic
technology including little development

00:21:49.349 --> 00:21:49.359
technology including little development
 

00:21:49.359 --> 00:21:55.019
technology including little development
and news so I have a more engineering

00:21:55.019 --> 00:21:55.029
and news so I have a more engineering
 

00:21:55.029 --> 00:21:59.549
and news so I have a more engineering
view of this trust question and more

00:21:59.549 --> 00:21:59.559
view of this trust question and more
 

00:21:59.559 --> 00:22:03.869
view of this trust question and more
kind of like I've been quite questioning

00:22:03.869 --> 00:22:03.879
kind of like I've been quite questioning
 

00:22:03.879 --> 00:22:07.589
kind of like I've been quite questioning
how can we do it better at really the

00:22:07.589 --> 00:22:07.599
how can we do it better at really the
 

00:22:07.599 --> 00:22:11.069
how can we do it better at really the
development level so we all want to

00:22:11.069 --> 00:22:11.079
development level so we all want to
 

00:22:11.079 --> 00:22:15.209
development level so we all want to
trust these machines but eventually we

00:22:15.209 --> 00:22:15.219
trust these machines but eventually we
 

00:22:15.219 --> 00:22:19.739
trust these machines but eventually we
have to interact with our PhD students

00:22:19.739 --> 00:22:19.749
have to interact with our PhD students
 

00:22:19.749 --> 00:22:22.169
have to interact with our PhD students
with our students to do PhD thesis to

00:22:22.169 --> 00:22:22.179
with our students to do PhD thesis to
 

00:22:22.179 --> 00:22:23.909
with our students to do PhD thesis to
advance the science of artificial

00:22:23.909 --> 00:22:23.919
advance the science of artificial
 

00:22:23.919 --> 00:22:26.609
advance the science of artificial
intelligence and robotics so how is my

00:22:26.609 --> 00:22:26.619
intelligence and robotics so how is my
 

00:22:26.619 --> 00:22:28.919
intelligence and robotics so how is my
dialogue with those students how is my

00:22:28.919 --> 00:22:28.929
dialogue with those students how is my
 

00:22:28.929 --> 00:22:33.509
dialogue with those students how is my
line of research how is like the steps

00:22:33.509 --> 00:22:33.519
line of research how is like the steps
 

00:22:33.519 --> 00:22:36.659
line of research how is like the steps
that we take to advance our science

00:22:36.659 --> 00:22:36.669
that we take to advance our science
 

00:22:36.669 --> 00:22:40.019
that we take to advance our science
going to be different because in fact we

00:22:40.019 --> 00:22:40.029
going to be different because in fact we
 

00:22:40.029 --> 00:22:41.489
going to be different because in fact we
care about trust

00:22:41.489 --> 00:22:41.499
care about trust
 

00:22:41.499 --> 00:22:46.469
care about trust
so my few remarks are really bringing

00:22:46.469 --> 00:22:46.479
so my few remarks are really bringing
 

00:22:46.479 --> 00:22:50.489
so my few remarks are really bringing
you down to not the army not the weapons

00:22:50.489 --> 00:22:50.499
you down to not the army not the weapons
 

00:22:50.499 --> 00:22:53.099
you down to not the army not the weapons
but to the moment in which you face a

00:22:53.099 --> 00:22:53.109
but to the moment in which you face a
 

00:22:53.109 --> 00:22:55.859
but to the moment in which you face a
computer and you have to type if these

00:22:55.859 --> 00:22:55.869
computer and you have to type if these
 

00:22:55.869 --> 00:22:57.899
computer and you have to type if these
than that and you have to program these

00:22:57.899 --> 00:22:57.909
than that and you have to program these
 

00:22:57.909 --> 00:23:00.839
than that and you have to program these
machines and you have to help build

00:23:00.839 --> 00:23:00.849
machines and you have to help build
 

00:23:00.849 --> 00:23:04.529
machines and you have to help build
programs for which eventually there will

00:23:04.529 --> 00:23:04.539
programs for which eventually there will
 

00:23:04.539 --> 00:23:07.600
programs for which eventually there will
be a change in society so

00:23:07.600 --> 00:23:07.610
be a change in society so
 

00:23:07.610 --> 00:23:10.510
be a change in society so
along those lines for me this question

00:23:10.510 --> 00:23:10.520
along those lines for me this question
 

00:23:10.520 --> 00:23:13.870
along those lines for me this question
of trust has to do with the fact that

00:23:13.870 --> 00:23:13.880
of trust has to do with the fact that
 

00:23:13.880 --> 00:23:16.570
of trust has to do with the fact that
our AI machinery our machine learning

00:23:16.570 --> 00:23:16.580
our AI machinery our machine learning
 

00:23:16.580 --> 00:23:20.289
our AI machinery our machine learning
our robotics machinery has been more of

00:23:20.289 --> 00:23:20.299
our robotics machinery has been more of
 

00:23:20.299 --> 00:23:22.720
our robotics machinery has been more of
a problem solver what's the shortest

00:23:22.720 --> 00:23:22.730
a problem solver what's the shortest
 

00:23:22.730 --> 00:23:25.990
a problem solver what's the shortest
path and we generate a solution but the

00:23:25.990 --> 00:23:26.000
path and we generate a solution but the
 

00:23:26.000 --> 00:23:28.120
path and we generate a solution but the
way that we actually function when

00:23:28.120 --> 00:23:28.130
way that we actually function when
 

00:23:28.130 --> 00:23:30.250
way that we actually function when
writing code when writing programs in

00:23:30.250 --> 00:23:30.260
writing code when writing programs in
 

00:23:30.260 --> 00:23:32.410
writing code when writing programs in
writing algorithms is that we don't have

00:23:32.410 --> 00:23:32.420
writing algorithms is that we don't have
 

00:23:32.420 --> 00:23:34.840
writing algorithms is that we don't have
the challenge of justifying or

00:23:34.840 --> 00:23:34.850
the challenge of justifying or
 

00:23:34.850 --> 00:23:38.650
the challenge of justifying or
explaining why did you say this was a

00:23:38.650 --> 00:23:38.660
explaining why did you say this was a
 

00:23:38.660 --> 00:23:40.600
explaining why did you say this was a
bottle why do you classify this as a

00:23:40.600 --> 00:23:40.610
bottle why do you classify this as a
 

00:23:40.610 --> 00:23:44.250
bottle why do you classify this as a
chair so the explanation part has been

00:23:44.250 --> 00:23:44.260
chair so the explanation part has been
 

00:23:44.260 --> 00:23:46.840
chair so the explanation part has been
what's present in the old days I mean

00:23:46.840 --> 00:23:46.850
what's present in the old days I mean
 

00:23:46.850 --> 00:23:48.400
what's present in the old days I mean
and it has been always kind of

00:23:48.400 --> 00:23:48.410
and it has been always kind of
 

00:23:48.410 --> 00:23:50.200
and it has been always kind of
underlying their research we even have

00:23:50.200 --> 00:23:50.210
underlying their research we even have
 

00:23:50.210 --> 00:23:52.270
underlying their research we even have
teasers on explanation based learning

00:23:52.270 --> 00:23:52.280
teasers on explanation based learning
 

00:23:52.280 --> 00:23:55.650
teasers on explanation based learning
when we have theses on all sorts of like

00:23:55.650 --> 00:23:55.660
when we have theses on all sorts of like
 

00:23:55.660 --> 00:23:59.890
when we have theses on all sorts of like
analogy trying to come up with like the

00:23:59.890 --> 00:23:59.900
analogy trying to come up with like the
 

00:23:59.900 --> 00:24:02.980
analogy trying to come up with like the
similarity between two problems and

00:24:02.980 --> 00:24:02.990
similarity between two problems and
 

00:24:02.990 --> 00:24:05.440
similarity between two problems and
justify that resemblance to make

00:24:05.440 --> 00:24:05.450
justify that resemblance to make
 

00:24:05.450 --> 00:24:08.950
justify that resemblance to make
adaptations so but it's not as present

00:24:08.950 --> 00:24:08.960
adaptations so but it's not as present
 

00:24:08.960 --> 00:24:12.010
adaptations so but it's not as present
now in terms of actually solving big

00:24:12.010 --> 00:24:12.020
now in terms of actually solving big
 

00:24:12.020 --> 00:24:15.120
now in terms of actually solving big
problems and still also being able to

00:24:15.120 --> 00:24:15.130
problems and still also being able to
 

00:24:15.130 --> 00:24:18.820
problems and still also being able to
justify if needed and have our

00:24:18.820 --> 00:24:18.830
justify if needed and have our
 

00:24:18.830 --> 00:24:21.400
justify if needed and have our
algorithms generate such justifications

00:24:21.400 --> 00:24:21.410
algorithms generate such justifications
 

00:24:21.410 --> 00:24:24.039
algorithms generate such justifications
so my research and my understanding of

00:24:24.039 --> 00:24:24.049
so my research and my understanding of
 

00:24:24.049 --> 00:24:26.560
so my research and my understanding of
trust leads me to for example have our

00:24:26.560 --> 00:24:26.570
trust leads me to for example have our
 

00:24:26.570 --> 00:24:29.530
trust leads me to for example have our
autonomous robots who actually navigate

00:24:29.530 --> 00:24:29.540
autonomous robots who actually navigate
 

00:24:29.540 --> 00:24:32.440
autonomous robots who actually navigate
in our environments arrive to our office

00:24:32.440 --> 00:24:32.450
in our environments arrive to our office
 

00:24:32.450 --> 00:24:35.080
in our environments arrive to our office
and be able to answer in language and

00:24:35.080 --> 00:24:35.090
and be able to answer in language and
 

00:24:35.090 --> 00:24:36.970
and be able to answer in language and
not in whatever language they use

00:24:36.970 --> 00:24:36.980
not in whatever language they use
 

00:24:36.980 --> 00:24:40.030
not in whatever language they use
internally of numbers and probability

00:24:40.030 --> 00:24:40.040
internally of numbers and probability
 

00:24:40.040 --> 00:24:43.240
internally of numbers and probability
and all sorts of like functions in

00:24:43.240 --> 00:24:43.250
and all sorts of like functions in
 

00:24:43.250 --> 00:24:48.070
and all sorts of like functions in
language why are you late and why did

00:24:48.070 --> 00:24:48.080
language why are you late and why did
 

00:24:48.080 --> 00:24:50.560
language why are you late and why did
you take that route and what are you

00:24:50.560 --> 00:24:50.570
you take that route and what are you
 

00:24:50.570 --> 00:24:54.789
you take that route and what are you
going to do next so I believe that trust

00:24:54.789 --> 00:24:54.799
going to do next so I believe that trust
 

00:24:54.799 --> 00:24:57.250
going to do next so I believe that trust
comes from the interaction with the

00:24:57.250 --> 00:24:57.260
comes from the interaction with the
 

00:24:57.260 --> 00:25:00.580
comes from the interaction with the
device with the machine and eventually

00:25:00.580 --> 00:25:00.590
device with the machine and eventually
 

00:25:00.590 --> 00:25:03.220
device with the machine and eventually
that machine needs to be able to be

00:25:03.220 --> 00:25:03.230
that machine needs to be able to be
 

00:25:03.230 --> 00:25:06.940
that machine needs to be able to be
queried needs to be able to be

00:25:06.940 --> 00:25:06.950
queried needs to be able to be
 

00:25:06.950 --> 00:25:09.340
queried needs to be able to be
questioned about the decisions that

00:25:09.340 --> 00:25:09.350
questioned about the decisions that
 

00:25:09.350 --> 00:25:11.890
questioned about the decisions that
actually they made towards solving

00:25:11.890 --> 00:25:11.900
actually they made towards solving
 

00:25:11.900 --> 00:25:13.870
actually they made towards solving
whatever problem we asked them to solve

00:25:13.870 --> 00:25:13.880
whatever problem we asked them to solve
 

00:25:13.880 --> 00:25:17.409
whatever problem we asked them to solve
so in that sense for me Trust has to do

00:25:17.409 --> 00:25:17.419
so in that sense for me Trust has to do
 

00:25:17.419 --> 00:25:20.049
so in that sense for me Trust has to do
a lot with transparency

00:25:20.049 --> 00:25:20.059
a lot with transparency
 

00:25:20.059 --> 00:25:24.759
a lot with transparency
with explanation with counterfactuals

00:25:24.759 --> 00:25:24.769
with explanation with counterfactuals
 

00:25:24.769 --> 00:25:27.759
with explanation with counterfactuals
can we ask a machine what if you had

00:25:27.759 --> 00:25:27.769
can we ask a machine what if you had
 

00:25:27.769 --> 00:25:29.529
can we ask a machine what if you had
done something different can these

00:25:29.529 --> 00:25:29.539
done something different can these
 

00:25:29.539 --> 00:25:31.389
done something different can these
machine process these type of questions

00:25:31.389 --> 00:25:31.399
machine process these type of questions
 

00:25:31.399 --> 00:25:36.039
machine process these type of questions
and be more explicit about whatever

00:25:36.039 --> 00:25:36.049
and be more explicit about whatever
 

00:25:36.049 --> 00:25:39.099
and be more explicit about whatever
algorithm they are running and what the

00:25:39.099 --> 00:25:39.109
algorithm they are running and what the
 

00:25:39.109 --> 00:25:42.209
algorithm they are running and what the
solution would be all those scenarios so

00:25:42.209 --> 00:25:42.219
solution would be all those scenarios so
 

00:25:42.219 --> 00:25:45.399
solution would be all those scenarios so
transparency and explanation I think are

00:25:45.399 --> 00:25:45.409
transparency and explanation I think are
 

00:25:45.409 --> 00:25:47.440
transparency and explanation I think are
part of the internals of AI machines

00:25:47.440 --> 00:25:47.450
part of the internals of AI machines
 

00:25:47.450 --> 00:25:50.279
part of the internals of AI machines
that may enable us to build more trust

00:25:50.279 --> 00:25:50.289
that may enable us to build more trust
 

00:25:50.289 --> 00:25:55.239
that may enable us to build more trust
in what they do thanks to all three of

00:25:55.239 --> 00:25:55.249
in what they do thanks to all three of
 

00:25:55.249 --> 00:25:57.249
in what they do thanks to all three of
you really got got some wonderful issues

00:25:57.249 --> 00:25:57.259
you really got got some wonderful issues
 

00:25:57.259 --> 00:25:59.739
you really got got some wonderful issues
out on the table and so I actually would

00:25:59.739 --> 00:25:59.749
out on the table and so I actually would
 

00:25:59.749 --> 00:26:01.539
out on the table and so I actually would
like to start Manuela if it's okay with

00:26:01.539 --> 00:26:01.549
like to start Manuela if it's okay with
 

00:26:01.549 --> 00:26:02.799
like to start Manuela if it's okay with
you with what you were just talking

00:26:02.799 --> 00:26:02.809
you with what you were just talking
 

00:26:02.809 --> 00:26:04.749
you with what you were just talking
about in terms of the transparency and

00:26:04.749 --> 00:26:04.759
about in terms of the transparency and
 

00:26:04.759 --> 00:26:06.789
about in terms of the transparency and
explain ability and I think one question

00:26:06.789 --> 00:26:06.799
explain ability and I think one question
 

00:26:06.799 --> 00:26:08.560
explain ability and I think one question
that comes up when we think about the

00:26:08.560 --> 00:26:08.570
that comes up when we think about the
 

00:26:08.570 --> 00:26:11.139
that comes up when we think about the
trust is perhaps transparency for whom

00:26:11.139 --> 00:26:11.149
trust is perhaps transparency for whom
 

00:26:11.149 --> 00:26:13.149
trust is perhaps transparency for whom
or explain ability for whom because of

00:26:13.149 --> 00:26:13.159
or explain ability for whom because of
 

00:26:13.159 --> 00:26:14.139
or explain ability for whom because of
course something that would be

00:26:14.139 --> 00:26:14.149
course something that would be
 

00:26:14.149 --> 00:26:17.200
course something that would be
understandable or explainable to a PhD

00:26:17.200 --> 00:26:17.210
understandable or explainable to a PhD
 

00:26:17.210 --> 00:26:18.579
understandable or explainable to a PhD
graduate student might be very different

00:26:18.579 --> 00:26:18.589
graduate student might be very different
 

00:26:18.589 --> 00:26:21.339
graduate student might be very different
from somebody on the street who doesn't

00:26:21.339 --> 00:26:21.349
from somebody on the street who doesn't
 

00:26:21.349 --> 00:26:23.229
from somebody on the street who doesn't
know as much about the technology so how

00:26:23.229 --> 00:26:23.239
know as much about the technology so how
 

00:26:23.239 --> 00:26:24.310
know as much about the technology so how
do we think about these different

00:26:24.310 --> 00:26:24.320
do we think about these different
 

00:26:24.320 --> 00:26:28.180
do we think about these different
audiences for explain ability it's a

00:26:28.180 --> 00:26:28.190
audiences for explain ability it's a
 

00:26:28.190 --> 00:26:29.879
audiences for explain ability it's a
very good question not only the

00:26:29.879 --> 00:26:29.889
very good question not only the
 

00:26:29.889 --> 00:26:32.560
very good question not only the
different audiences but also different

00:26:32.560 --> 00:26:32.570
different audiences but also different
 

00:26:32.570 --> 00:26:35.549
different audiences but also different
moments in time even for someone that

00:26:35.549 --> 00:26:35.559
moments in time even for someone that
 

00:26:35.559 --> 00:26:38.799
moments in time even for someone that
interacts with the robot or with the

00:26:38.799 --> 00:26:38.809
interacts with the robot or with the
 

00:26:38.809 --> 00:26:40.629
interacts with the robot or with the
agent many times you don't want to

00:26:40.629 --> 00:26:40.639
agent many times you don't want to
 

00:26:40.639 --> 00:26:42.729
agent many times you don't want to
produce the same explanation today that

00:26:42.729 --> 00:26:42.739
produce the same explanation today that
 

00:26:42.739 --> 00:26:45.099
produce the same explanation today that
you produced yesterday or it's otherwise

00:26:45.099 --> 00:26:45.109
you produced yesterday or it's otherwise
 

00:26:45.109 --> 00:26:47.320
you produced yesterday or it's otherwise
it's just you are telling me the same

00:26:47.320 --> 00:26:47.330
it's just you are telling me the same
 

00:26:47.330 --> 00:26:50.019
it's just you are telling me the same
thing and that didn't help me so we

00:26:50.019 --> 00:26:50.029
thing and that didn't help me so we
 

00:26:50.029 --> 00:26:52.899
thing and that didn't help me so we
actually introduced this concept of a

00:26:52.899 --> 00:26:52.909
actually introduced this concept of a
 

00:26:52.909 --> 00:26:55.959
actually introduced this concept of a
space of explanations a space of

00:26:55.959 --> 00:26:55.969
space of explanations a space of
 

00:26:55.969 --> 00:26:59.049
space of explanations a space of
verbalization together with one of my

00:26:59.049 --> 00:26:59.059
verbalization together with one of my
 

00:26:59.059 --> 00:27:01.899
verbalization together with one of my
students eyes of raj and vittorio

00:27:01.899 --> 00:27:01.909
students eyes of raj and vittorio
 

00:27:01.909 --> 00:27:04.509
students eyes of raj and vittorio
pereira and stephanie Rosenthal and we

00:27:04.509 --> 00:27:04.519
pereira and stephanie Rosenthal and we
 

00:27:04.519 --> 00:27:06.070
pereira and stephanie Rosenthal and we
have been working on this issue about

00:27:06.070 --> 00:27:06.080
have been working on this issue about
 

00:27:06.080 --> 00:27:10.299
have been working on this issue about
the the multiple facets of explanations

00:27:10.299 --> 00:27:10.309
the the multiple facets of explanations
 

00:27:10.309 --> 00:27:13.089
the the multiple facets of explanations
and in fact trust these levels of

00:27:13.089 --> 00:27:13.099
and in fact trust these levels of
 

00:27:13.099 --> 00:27:15.070
and in fact trust these levels of
Trustees levels of transparency these

00:27:15.070 --> 00:27:15.080
Trustees levels of transparency these
 

00:27:15.080 --> 00:27:19.180
Trustees levels of transparency these
level of explanation are not a single

00:27:19.180 --> 00:27:19.190
level of explanation are not a single
 

00:27:19.190 --> 00:27:21.759
level of explanation are not a single
entity like you say a single kind of

00:27:21.759 --> 00:27:21.769
entity like you say a single kind of
 

00:27:21.769 --> 00:27:24.339
entity like you say a single kind of
like concept and we have done some

00:27:24.339 --> 00:27:24.349
like concept and we have done some
 

00:27:24.349 --> 00:27:27.070
like concept and we have done some
machine learning on crowdsourcing people

00:27:27.070 --> 00:27:27.080
machine learning on crowdsourcing people
 

00:27:27.080 --> 00:27:29.259
machine learning on crowdsourcing people
and trying to understand if someone asks

00:27:29.259 --> 00:27:29.269
and trying to understand if someone asks
 

00:27:29.269 --> 00:27:33.310
and trying to understand if someone asks
what what exactly do you happen by the

00:27:33.310 --> 00:27:33.320
what what exactly do you happen by the
 

00:27:33.320 --> 00:27:34.000
what what exactly do you happen by the
elevator

00:27:34.000 --> 00:27:34.010
elevator
 

00:27:34.010 --> 00:27:37.240
elevator
that that exactly by the hell by the

00:27:37.240 --> 00:27:37.250
that that exactly by the hell by the
 

00:27:37.250 --> 00:27:39.310
that that exactly by the hell by the
elevator would be something that would

00:27:39.310 --> 00:27:39.320
elevator would be something that would
 

00:27:39.320 --> 00:27:41.710
elevator would be something that would
require a much more narrow explanation

00:27:41.710 --> 00:27:41.720
require a much more narrow explanation
 

00:27:41.720 --> 00:27:45.190
require a much more narrow explanation
as if someone would ask where you come

00:27:45.190 --> 00:27:45.200
as if someone would ask where you come
 

00:27:45.200 --> 00:27:47.200
as if someone would ask where you come
from and that's like a language that

00:27:47.200 --> 00:27:47.210
from and that's like a language that
 

00:27:47.210 --> 00:27:50.260
from and that's like a language that
requires a different explanation of the

00:27:50.260 --> 00:27:50.270
requires a different explanation of the
 

00:27:50.270 --> 00:27:53.260
requires a different explanation of the
mobility of the machine then what

00:27:53.260 --> 00:27:53.270
mobility of the machine then what
 

00:27:53.270 --> 00:27:55.750
mobility of the machine then what
happened by the elevator so we did a lot

00:27:55.750 --> 00:27:55.760
happened by the elevator so we did a lot
 

00:27:55.760 --> 00:27:58.390
happened by the elevator so we did a lot
of research and we do as we speak a lot

00:27:58.390 --> 00:27:58.400
of research and we do as we speak a lot
 

00:27:58.400 --> 00:28:00.340
of research and we do as we speak a lot
of research on trying to understand a

00:28:00.340 --> 00:28:00.350
of research on trying to understand a
 

00:28:00.350 --> 00:28:04.660
of research on trying to understand a
question from a human what exactly means

00:28:04.660 --> 00:28:04.670
question from a human what exactly means
 

00:28:04.670 --> 00:28:07.390
question from a human what exactly means
in terms of an explanation required from

00:28:07.390 --> 00:28:07.400
in terms of an explanation required from
 

00:28:07.400 --> 00:28:11.110
in terms of an explanation required from
the machine so it seems to me that in a

00:28:11.110 --> 00:28:11.120
the machine so it seems to me that in a
 

00:28:11.120 --> 00:28:12.820
the machine so it seems to me that in a
lot of cases this kind of explain

00:28:12.820 --> 00:28:12.830
lot of cases this kind of explain
 

00:28:12.830 --> 00:28:15.940
lot of cases this kind of explain
ability if we had it would be able to to

00:28:15.940 --> 00:28:15.950
ability if we had it would be able to to
 

00:28:15.950 --> 00:28:17.650
ability if we had it would be able to to
help some with one of the issues that

00:28:17.650 --> 00:28:17.660
help some with one of the issues that
 

00:28:17.660 --> 00:28:19.210
help some with one of the issues that
you raised Kirsten which was about the

00:28:19.210 --> 00:28:19.220
you raised Kirsten which was about the
 

00:28:19.220 --> 00:28:22.720
you raised Kirsten which was about the
difficulty of verification in rapidly

00:28:22.720 --> 00:28:22.730
difficulty of verification in rapidly
 

00:28:22.730 --> 00:28:24.910
difficulty of verification in rapidly
changing environments where the system

00:28:24.910 --> 00:28:24.920
changing environments where the system
 

00:28:24.920 --> 00:28:27.100
changing environments where the system
might be deployed somewhere whether on

00:28:27.100 --> 00:28:27.110
might be deployed somewhere whether on
 

00:28:27.110 --> 00:28:29.920
might be deployed somewhere whether on
the battlefield or in the international

00:28:29.920 --> 00:28:29.930
the battlefield or in the international
 

00:28:29.930 --> 00:28:31.690
the battlefield or in the international
context where perhaps it hasn't been

00:28:31.690 --> 00:28:31.700
context where perhaps it hasn't been
 

00:28:31.700 --> 00:28:33.190
context where perhaps it hasn't been
used before so how do we verify that

00:28:33.190 --> 00:28:33.200
used before so how do we verify that
 

00:28:33.200 --> 00:28:34.960
used before so how do we verify that
it's going to work there do you think

00:28:34.960 --> 00:28:34.970
it's going to work there do you think
 

00:28:34.970 --> 00:28:36.640
it's going to work there do you think
that explain ability would be something

00:28:36.640 --> 00:28:36.650
that explain ability would be something
 

00:28:36.650 --> 00:28:39.340
that explain ability would be something
that could provide a guide as we think

00:28:39.340 --> 00:28:39.350
that could provide a guide as we think
 

00:28:39.350 --> 00:28:43.090
that could provide a guide as we think
about sort of how to how we can build

00:28:43.090 --> 00:28:43.100
about sort of how to how we can build
 

00:28:43.100 --> 00:28:45.130
about sort of how to how we can build
trust in as you said some of these

00:28:45.130 --> 00:28:45.140
trust in as you said some of these
 

00:28:45.140 --> 00:28:46.750
trust in as you said some of these
technologies that can can even have

00:28:46.750 --> 00:28:46.760
technologies that can can even have
 

00:28:46.760 --> 00:28:53.110
technologies that can can even have
lethal impacts well used a great word

00:28:53.110 --> 00:28:53.120
lethal impacts well used a great word
 

00:28:53.120 --> 00:28:54.850
lethal impacts well used a great word
transparency and that's something that

00:28:54.850 --> 00:28:54.860
transparency and that's something that
 

00:28:54.860 --> 00:28:56.590
transparency and that's something that
we talk again a lot about in arms

00:28:56.590 --> 00:28:56.600
we talk again a lot about in arms
 

00:28:56.600 --> 00:28:59.080
we talk again a lot about in arms
control as a stabilizing element so yes

00:28:59.080 --> 00:28:59.090
control as a stabilizing element so yes
 

00:28:59.090 --> 00:29:00.370
control as a stabilizing element so yes
I think explain ability is

00:29:00.370 --> 00:29:00.380
I think explain ability is
 

00:29:00.380 --> 00:29:02.830
I think explain ability is
extraordinarily important at the same

00:29:02.830 --> 00:29:02.840
extraordinarily important at the same
 

00:29:02.840 --> 00:29:06.400
extraordinarily important at the same
time it circles back to that question of

00:29:06.400 --> 00:29:06.410
time it circles back to that question of
 

00:29:06.410 --> 00:29:11.290
time it circles back to that question of
for who are we explaining to and and

00:29:11.290 --> 00:29:11.300
for who are we explaining to and and
 

00:29:11.300 --> 00:29:13.690
for who are we explaining to and and
when because when we're talking about

00:29:13.690 --> 00:29:13.700
when because when we're talking about
 

00:29:13.700 --> 00:29:18.250
when because when we're talking about
lethal systems sometimes post facto

00:29:18.250 --> 00:29:18.260
lethal systems sometimes post facto
 

00:29:18.260 --> 00:29:20.350
lethal systems sometimes post facto
explain ability it's good to be able to

00:29:20.350 --> 00:29:20.360
explain ability it's good to be able to
 

00:29:20.360 --> 00:29:22.390
explain ability it's good to be able to
unpack that right

00:29:22.390 --> 00:29:22.400
unpack that right
 

00:29:22.400 --> 00:29:25.090
unpack that right
afterwards if something went wrong but

00:29:25.090 --> 00:29:25.100
afterwards if something went wrong but
 

00:29:25.100 --> 00:29:28.300
afterwards if something went wrong but
it's it's a bit late right so explain

00:29:28.300 --> 00:29:28.310
it's it's a bit late right so explain
 

00:29:28.310 --> 00:29:31.810
it's it's a bit late right so explain
ability is very important but I think we

00:29:31.810 --> 00:29:31.820
ability is very important but I think we
 

00:29:31.820 --> 00:29:33.610
ability is very important but I think we
should be focusing more on on thinking

00:29:33.610 --> 00:29:33.620
should be focusing more on on thinking
 

00:29:33.620 --> 00:29:38.920
should be focusing more on on thinking
about how do we ensure confidence in the

00:29:38.920 --> 00:29:38.930
about how do we ensure confidence in the
 

00:29:38.930 --> 00:29:41.140
about how do we ensure confidence in the
systems whether they are weapon systems

00:29:41.140 --> 00:29:41.150
systems whether they are weapon systems
 

00:29:41.150 --> 00:29:43.840
systems whether they are weapon systems
or other systems before we deploy them

00:29:43.840 --> 00:29:43.850
or other systems before we deploy them
 

00:29:43.850 --> 00:29:45.880
or other systems before we deploy them
and so explained ability I think is a

00:29:45.880 --> 00:29:45.890
and so explained ability I think is a
 

00:29:45.890 --> 00:29:46.390
and so explained ability I think is a
much

00:29:46.390 --> 00:29:46.400
much
 

00:29:46.400 --> 00:29:48.580
much
you're part of the process then actually

00:29:48.580 --> 00:29:48.590
you're part of the process then actually
 

00:29:48.590 --> 00:29:49.630
you're part of the process then actually
post-deployment

00:29:49.630 --> 00:29:49.640
post-deployment
 

00:29:49.640 --> 00:29:51.130
post-deployment
I mean it's still necessary and it's

00:29:51.130 --> 00:29:51.140
I mean it's still necessary and it's
 

00:29:51.140 --> 00:29:52.510
I mean it's still necessary and it's
part of that very important human

00:29:52.510 --> 00:29:52.520
part of that very important human
 

00:29:52.520 --> 00:29:54.370
part of that very important human
machine interface that's going to be

00:29:54.370 --> 00:29:54.380
machine interface that's going to be
 

00:29:54.380 --> 00:29:56.170
machine interface that's going to be
critical in all of our AI embedded

00:29:56.170 --> 00:29:56.180
critical in all of our AI embedded
 

00:29:56.180 --> 00:29:59.140
critical in all of our AI embedded
systems but explain ability with weapon

00:29:59.140 --> 00:29:59.150
systems but explain ability with weapon
 

00:29:59.150 --> 00:30:01.150
systems but explain ability with weapon
systems because of the risk being so

00:30:01.150 --> 00:30:01.160
systems because of the risk being so
 

00:30:01.160 --> 00:30:05.320
systems because of the risk being so
high it's I think it's a bit of a

00:30:05.320 --> 00:30:05.330
high it's I think it's a bit of a
 

00:30:05.330 --> 00:30:06.070
high it's I think it's a bit of a
unicorn

00:30:06.070 --> 00:30:06.080
unicorn
 

00:30:06.080 --> 00:30:08.310
unicorn
we need it we need it a lot sooner

00:30:08.310 --> 00:30:08.320
we need it we need it a lot sooner
 

00:30:08.320 --> 00:30:10.510
we need it we need it a lot sooner
certainly as you said the post factor

00:30:10.510 --> 00:30:10.520
certainly as you said the post factor
 

00:30:10.520 --> 00:30:13.420
certainly as you said the post factor
that is small comfort if after the fact

00:30:13.420 --> 00:30:13.430
that is small comfort if after the fact
 

00:30:13.430 --> 00:30:15.550
that is small comfort if after the fact
you know why that happened it seems that

00:30:15.550 --> 00:30:15.560
you know why that happened it seems that
 

00:30:15.560 --> 00:30:16.810
you know why that happened it seems that
in many ways that connects to some of

00:30:16.810 --> 00:30:16.820
in many ways that connects to some of
 

00:30:16.820 --> 00:30:18.220
in many ways that connects to some of
things you were raised in Moshe in terms

00:30:18.220 --> 00:30:18.230
things you were raised in Moshe in terms
 

00:30:18.230 --> 00:30:21.010
things you were raised in Moshe in terms
of the need to be able to trust the

00:30:21.010 --> 00:30:21.020
of the need to be able to trust the
 

00:30:21.020 --> 00:30:23.530
of the need to be able to trust the
developers themselves we can trust men

00:30:23.530 --> 00:30:23.540
developers themselves we can trust men
 

00:30:23.540 --> 00:30:25.450
developers themselves we can trust men
well as graduate students we can trust

00:30:25.450 --> 00:30:25.460
well as graduate students we can trust
 

00:30:25.460 --> 00:30:27.670
well as graduate students we can trust
Manuela but as we've been hearing about

00:30:27.670 --> 00:30:27.680
Manuela but as we've been hearing about
 

00:30:27.680 --> 00:30:30.040
Manuela but as we've been hearing about
in various times throughout this

00:30:30.040 --> 00:30:30.050
in various times throughout this
 

00:30:30.050 --> 00:30:32.230
in various times throughout this
conference there are companies who are

00:30:32.230 --> 00:30:32.240
conference there are companies who are
 

00:30:32.240 --> 00:30:34.450
conference there are companies who are
doing things that perhaps are not out of

00:30:34.450 --> 00:30:34.460
doing things that perhaps are not out of
 

00:30:34.460 --> 00:30:36.640
doing things that perhaps are not out of
an effort to produce technology that

00:30:36.640 --> 00:30:36.650
an effort to produce technology that
 

00:30:36.650 --> 00:30:38.020
an effort to produce technology that
will help people as opposed to helping

00:30:38.020 --> 00:30:38.030
will help people as opposed to helping
 

00:30:38.030 --> 00:30:41.170
will help people as opposed to helping
the bottom line so could we extend for

00:30:41.170 --> 00:30:41.180
the bottom line so could we extend for
 

00:30:41.180 --> 00:30:42.670
the bottom line so could we extend for
example this notion of explained ability

00:30:42.670 --> 00:30:42.680
example this notion of explained ability
 

00:30:42.680 --> 00:30:44.500
example this notion of explained ability
to the developers they have to explain

00:30:44.500 --> 00:30:44.510
to the developers they have to explain
 

00:30:44.510 --> 00:30:46.210
to the developers they have to explain
why they did what they did or what

00:30:46.210 --> 00:30:46.220
why they did what they did or what
 

00:30:46.220 --> 00:30:47.680
why they did what they did or what
solutions do you think we might pour

00:30:47.680 --> 00:30:47.690
solutions do you think we might pour
 

00:30:47.690 --> 00:30:49.150
solutions do you think we might pour
paths might we be able to go down to

00:30:49.150 --> 00:30:49.160
paths might we be able to go down to
 

00:30:49.160 --> 00:30:51.610
paths might we be able to go down to
improve trust and developers well I

00:30:51.610 --> 00:30:51.620
improve trust and developers well I
 

00:30:51.620 --> 00:30:53.980
improve trust and developers well I
think we have to go back to the

00:30:53.980 --> 00:30:53.990
think we have to go back to the
 

00:30:53.990 --> 00:30:55.660
think we have to go back to the
questions why do we need trust and when

00:30:55.660 --> 00:30:55.670
questions why do we need trust and when
 

00:30:55.670 --> 00:30:58.330
questions why do we need trust and when
it trusts because we feel vulnerable ok

00:30:58.330 --> 00:30:58.340
it trusts because we feel vulnerable ok
 

00:30:58.340 --> 00:31:01.000
it trusts because we feel vulnerable ok
and this is true this is basically human

00:31:01.000 --> 00:31:01.010
and this is true this is basically human
 

00:31:01.010 --> 00:31:02.410
and this is true this is basically human
fear if you go if you're a child I

00:31:02.410 --> 00:31:02.420
fear if you go if you're a child I
 

00:31:02.420 --> 00:31:04.780
fear if you go if you're a child I
remember being being a child and walking

00:31:04.780 --> 00:31:04.790
remember being being a child and walking
 

00:31:04.790 --> 00:31:08.260
remember being being a child and walking
in darkness why are you afraid when it's

00:31:08.260 --> 00:31:08.270
in darkness why are you afraid when it's
 

00:31:08.270 --> 00:31:11.110
in darkness why are you afraid when it's
dark well because you don't know what

00:31:11.110 --> 00:31:11.120
dark well because you don't know what
 

00:31:11.120 --> 00:31:13.210
dark well because you don't know what
are the threats out there right I mean

00:31:13.210 --> 00:31:13.220
are the threats out there right I mean
 

00:31:13.220 --> 00:31:16.420
are the threats out there right I mean
so you turn the light on and okay it's

00:31:16.420 --> 00:31:16.430
so you turn the light on and okay it's
 

00:31:16.430 --> 00:31:20.230
so you turn the light on and okay it's
nothing to worry about and so human

00:31:20.230 --> 00:31:20.240
nothing to worry about and so human
 

00:31:20.240 --> 00:31:21.700
nothing to worry about and so human
beings are very vulnerable we are very

00:31:21.700 --> 00:31:21.710
beings are very vulnerable we are very
 

00:31:21.710 --> 00:31:25.480
beings are very vulnerable we are very
fragile okay and so all if you think

00:31:25.480 --> 00:31:25.490
fragile okay and so all if you think
 

00:31:25.490 --> 00:31:27.400
fragile okay and so all if you think
about all what all these things were

00:31:27.400 --> 00:31:27.410
about all what all these things were
 

00:31:27.410 --> 00:31:30.100
about all what all these things were
talking about transparency and explained

00:31:30.100 --> 00:31:30.110
talking about transparency and explained
 

00:31:30.110 --> 00:31:32.470
talking about transparency and explained
ability are all essentially a way to

00:31:32.470 --> 00:31:32.480
ability are all essentially a way to
 

00:31:32.480 --> 00:31:34.690
ability are all essentially a way to
assuage us that we don't need to be

00:31:34.690 --> 00:31:34.700
assuage us that we don't need to be
 

00:31:34.700 --> 00:31:37.300
assuage us that we don't need to be
afraid so this is really we need to go

00:31:37.300 --> 00:31:37.310
afraid so this is really we need to go
 

00:31:37.310 --> 00:31:38.890
afraid so this is really we need to go
back is the issue in some sense it's not

00:31:38.890 --> 00:31:38.900
back is the issue in some sense it's not
 

00:31:38.900 --> 00:31:41.350
back is the issue in some sense it's not
so much about trust the issue is about

00:31:41.350 --> 00:31:41.360
so much about trust the issue is about
 

00:31:41.360 --> 00:31:45.490
so much about trust the issue is about
vulnerability and fear and unfortunately

00:31:45.490 --> 00:31:45.500
vulnerability and fear and unfortunately
 

00:31:45.500 --> 00:31:47.800
vulnerability and fear and unfortunately
what we've been learning in the more you

00:31:47.800 --> 00:31:47.810
what we've been learning in the more you
 

00:31:47.810 --> 00:31:52.090
what we've been learning in the more you
learn that you feel rational really you

00:31:52.090 --> 00:31:52.100
learn that you feel rational really you
 

00:31:52.100 --> 00:31:53.860
learn that you feel rational really you
know just because you're paranoid it

00:31:53.860 --> 00:31:53.870
know just because you're paranoid it
 

00:31:53.870 --> 00:31:55.180
know just because you're paranoid it
doesn't mean the summer is not trying to

00:31:55.180 --> 00:31:55.190
doesn't mean the summer is not trying to
 

00:31:55.190 --> 00:31:55.720
doesn't mean the summer is not trying to
kill you

00:31:55.720 --> 00:31:55.730
kill you
 

00:31:55.730 --> 00:31:57.430
kill you
and what I've been learning recently

00:31:57.430 --> 00:31:57.440
and what I've been learning recently
 

00:31:57.440 --> 00:31:59.290
and what I've been learning recently
there are people out there for us know

00:31:59.290 --> 00:31:59.300
there are people out there for us know
 

00:31:59.300 --> 00:32:00.310
there are people out there for us know
maybe not to kill

00:32:00.310 --> 00:32:00.320
maybe not to kill
 

00:32:00.320 --> 00:32:04.270
maybe not to kill
but to manipulate us and this if you

00:32:04.270 --> 00:32:04.280
but to manipulate us and this if you
 

00:32:04.280 --> 00:32:05.440
but to manipulate us and this if you
think about this issue be money

00:32:05.440 --> 00:32:05.450
think about this issue be money
 

00:32:05.450 --> 00:32:09.100
think about this issue be money
manipulated it goes back to our sense of

00:32:09.100 --> 00:32:09.110
manipulated it goes back to our sense of
 

00:32:09.110 --> 00:32:11.260
manipulated it goes back to our sense of
self and vulnerability and we feel

00:32:11.260 --> 00:32:11.270
self and vulnerability and we feel
 

00:32:11.270 --> 00:32:12.850
self and vulnerability and we feel
violated when we realize we've been

00:32:12.850 --> 00:32:12.860
violated when we realize we've been
 

00:32:12.860 --> 00:32:15.130
violated when we realize we've been
manipulated and so maybe it's not a

00:32:15.130 --> 00:32:15.140
manipulated and so maybe it's not a
 

00:32:15.140 --> 00:32:16.810
manipulated and so maybe it's not a
threat for life but it is still some

00:32:16.810 --> 00:32:16.820
threat for life but it is still some
 

00:32:16.820 --> 00:32:18.760
threat for life but it is still some
kind of a threat to our own notion of

00:32:18.760 --> 00:32:18.770
kind of a threat to our own notion of
 

00:32:18.770 --> 00:32:21.700
kind of a threat to our own notion of
self and self determinacy and so we

00:32:21.700 --> 00:32:21.710
self and self determinacy and so we
 

00:32:21.710 --> 00:32:23.230
self and self determinacy and so we
start have to figure out I mean the

00:32:23.230 --> 00:32:23.240
start have to figure out I mean the
 

00:32:23.240 --> 00:32:25.810
start have to figure out I mean the
issue first of all is not how to somehow

00:32:25.810 --> 00:32:25.820
issue first of all is not how to somehow
 

00:32:25.820 --> 00:32:27.850
issue first of all is not how to somehow
make us feel safe here first of all they

00:32:27.850 --> 00:32:27.860
make us feel safe here first of all they
 

00:32:27.860 --> 00:32:30.100
make us feel safe here first of all they
have to be real safety then we deal with

00:32:30.100 --> 00:32:30.110
have to be real safety then we deal with
 

00:32:30.110 --> 00:32:32.680
have to be real safety then we deal with
the psychological factor okay right now

00:32:32.680 --> 00:32:32.690
the psychological factor okay right now
 

00:32:32.690 --> 00:32:34.750
the psychological factor okay right now
unfolding we are missing the regulatory

00:32:34.750 --> 00:32:34.760
unfolding we are missing the regulatory
 

00:32:34.760 --> 00:32:36.910
unfolding we are missing the regulatory
framework it will make this

00:32:36.910 --> 00:32:36.920
framework it will make this
 

00:32:36.920 --> 00:32:39.160
framework it will make this
technological world really safe and

00:32:39.160 --> 00:32:39.170
technological world really safe and
 

00:32:39.170 --> 00:32:41.110
technological world really safe and
that's why we are fredericks

00:32:41.110 --> 00:32:41.120
that's why we are fredericks
 

00:32:41.120 --> 00:32:42.430
that's why we are fredericks
that's why we have no trust in the

00:32:42.430 --> 00:32:42.440
that's why we have no trust in the
 

00:32:42.440 --> 00:32:44.800
that's why we have no trust in the
system so we have to go beyond just

00:32:44.800 --> 00:32:44.810
system so we have to go beyond just
 

00:32:44.810 --> 00:32:47.560
system so we have to go beyond just
focusing on the trust assume that it's a

00:32:47.560 --> 00:32:47.570
focusing on the trust assume that it's a
 

00:32:47.570 --> 00:32:50.430
focusing on the trust assume that it's a
it's a safe world and your feeling of

00:32:50.430 --> 00:32:50.440
it's a safe world and your feeling of
 

00:32:50.440 --> 00:32:54.100
it's a safe world and your feeling of
fearfulness are not justified right now

00:32:54.100 --> 00:32:54.110
fearfulness are not justified right now
 

00:32:54.110 --> 00:32:56.410
fearfulness are not justified right now
I have to say I think they are justified

00:32:56.410 --> 00:32:56.420
I have to say I think they are justified
 

00:32:56.420 --> 00:32:58.150
I have to say I think they are justified
so we have to go beyond the trust issue

00:32:58.150 --> 00:32:58.160
so we have to go beyond the trust issue
 

00:32:58.160 --> 00:33:00.430
so we have to go beyond the trust issue
we have to remove the threat first and

00:33:00.430 --> 00:33:00.440
we have to remove the threat first and
 

00:33:00.440 --> 00:33:02.680
we have to remove the threat first and
then we have to deal how do we make sure

00:33:02.680 --> 00:33:02.690
then we have to deal how do we make sure
 

00:33:02.690 --> 00:33:04.690
then we have to deal how do we make sure
that people really trust that this is a

00:33:04.690 --> 00:33:04.700
that people really trust that this is a
 

00:33:04.700 --> 00:33:08.500
that people really trust that this is a
safe environment so Manuela would you

00:33:08.500 --> 00:33:08.510
safe environment so Manuela would you
 

00:33:08.510 --> 00:33:10.180
safe environment so Manuela would you
you want to switch to becoming a social

00:33:10.180 --> 00:33:10.190
you want to switch to becoming a social
 

00:33:10.190 --> 00:33:12.130
you want to switch to becoming a social
scientist and policy maker to remove

00:33:12.130 --> 00:33:12.140
scientist and policy maker to remove
 

00:33:12.140 --> 00:33:15.730
scientist and policy maker to remove
some of these threats I'm going to wave

00:33:15.730 --> 00:33:15.740
some of these threats I'm going to wave
 

00:33:15.740 --> 00:33:21.880
some of these threats I'm going to wave
my hands but so I actually tend to

00:33:21.880 --> 00:33:21.890
my hands but so I actually tend to
 

00:33:21.890 --> 00:33:30.490
my hands but so I actually tend to
disagree with this so you know besides

00:33:30.490 --> 00:33:30.500
disagree with this so you know besides
 

00:33:30.500 --> 00:33:33.430
disagree with this so you know besides
the effect of of being safe and afraid

00:33:33.430 --> 00:33:33.440
the effect of of being safe and afraid
 

00:33:33.440 --> 00:33:36.490
the effect of of being safe and afraid
you also want to trust a machine that if

00:33:36.490 --> 00:33:36.500
you also want to trust a machine that if
 

00:33:36.500 --> 00:33:38.590
you also want to trust a machine that if
you ask you know find me a flight to

00:33:38.590 --> 00:33:38.600
you ask you know find me a flight to
 

00:33:38.600 --> 00:33:41.800
you ask you know find me a flight to
Paris tomorrow or get me a good flight

00:33:41.800 --> 00:33:41.810
Paris tomorrow or get me a good flight
 

00:33:41.810 --> 00:33:44.650
Paris tomorrow or get me a good flight
to Lisbon that actually it does the job

00:33:44.650 --> 00:33:44.660
to Lisbon that actually it does the job
 

00:33:44.660 --> 00:33:47.200
to Lisbon that actually it does the job
so it's just trusting the fact that the

00:33:47.200 --> 00:33:47.210
so it's just trusting the fact that the
 

00:33:47.210 --> 00:33:49.330
so it's just trusting the fact that the
thing is going to do what you expected

00:33:49.330 --> 00:33:49.340
thing is going to do what you expected
 

00:33:49.340 --> 00:33:51.700
thing is going to do what you expected
it to do or if you have like a robot

00:33:51.700 --> 00:33:51.710
it to do or if you have like a robot
 

00:33:51.710 --> 00:33:54.520
it to do or if you have like a robot
around here that you'd say you know go

00:33:54.520 --> 00:33:54.530
around here that you'd say you know go
 

00:33:54.530 --> 00:33:56.410
around here that you'd say you know go
to the conference room and guide the

00:33:56.410 --> 00:33:56.420
to the conference room and guide the
 

00:33:56.420 --> 00:33:58.660
to the conference room and guide the
visitor back to my office that that's

00:33:58.660 --> 00:33:58.670
visitor back to my office that that's
 

00:33:58.670 --> 00:34:00.460
visitor back to my office that that's
the case that the robot does not go to

00:34:00.460 --> 00:34:00.470
the case that the robot does not go to
 

00:34:00.470 --> 00:34:02.860
the case that the robot does not go to
the kitchen or does not go anywhere else

00:34:02.860 --> 00:34:02.870
the kitchen or does not go anywhere else
 

00:34:02.870 --> 00:34:06.010
the kitchen or does not go anywhere else
so or my refrigerator if I put something

00:34:06.010 --> 00:34:06.020
so or my refrigerator if I put something
 

00:34:06.020 --> 00:34:08.620
so or my refrigerator if I put something
in the freezer I expect it to actually

00:34:08.620 --> 00:34:08.630
in the freezer I expect it to actually
 

00:34:08.630 --> 00:34:11.110
in the freezer I expect it to actually
freeze the food so there is something

00:34:11.110 --> 00:34:11.120
freeze the food so there is something
 

00:34:11.120 --> 00:34:13.840
freeze the food so there is something
about technology that

00:34:13.840 --> 00:34:13.850
about technology that
 

00:34:13.850 --> 00:34:15.370
about technology that
for me the trust is that in some sense

00:34:15.370 --> 00:34:15.380
for me the trust is that in some sense
 

00:34:15.380 --> 00:34:18.190
for me the trust is that in some sense
also it does what it's expected to do

00:34:18.190 --> 00:34:18.200
also it does what it's expected to do
 

00:34:18.200 --> 00:34:23.340
also it does what it's expected to do
and I think that that before the actual

00:34:23.340 --> 00:34:23.350
and I think that that before the actual
 

00:34:23.350 --> 00:34:26.530
and I think that that before the actual
solving my vulnerability there is also

00:34:26.530 --> 00:34:26.540
solving my vulnerability there is also
 

00:34:26.540 --> 00:34:30.370
solving my vulnerability there is also
this the need to meet the expectation

00:34:30.370 --> 00:34:30.380
this the need to meet the expectation
 

00:34:30.380 --> 00:34:33.220
this the need to meet the expectation
meet the specification and in AI we are

00:34:33.220 --> 00:34:33.230
meet the specification and in AI we are
 

00:34:33.230 --> 00:34:35.830
meet the specification and in AI we are
still not there because we really don't

00:34:35.830 --> 00:34:35.840
still not there because we really don't
 

00:34:35.840 --> 00:34:38.620
still not there because we really don't
have like robust robots that we say go

00:34:38.620 --> 00:34:38.630
have like robust robots that we say go
 

00:34:38.630 --> 00:34:41.260
have like robust robots that we say go
down this road and don't don't go over

00:34:41.260 --> 00:34:41.270
down this road and don't don't go over
 

00:34:41.270 --> 00:34:44.680
down this road and don't don't go over
anyone we are still trying to find the

00:34:44.680 --> 00:34:44.690
anyone we are still trying to find the
 

00:34:44.690 --> 00:34:47.380
anyone we are still trying to find the
the machinery that matches the

00:34:47.380 --> 00:34:47.390
the machinery that matches the
 

00:34:47.390 --> 00:34:50.020
the machinery that matches the
specification and what we would like the

00:34:50.020 --> 00:34:50.030
specification and what we would like the
 

00:34:50.030 --> 00:34:52.480
specification and what we would like the
autonomous agent the autonomous robot

00:34:52.480 --> 00:34:52.490
autonomous agent the autonomous robot
 

00:34:52.490 --> 00:34:55.000
autonomous agent the autonomous robot
the autonomous the AI assistant to do if

00:34:55.000 --> 00:34:55.010
the autonomous the AI assistant to do if
 

00:34:55.010 --> 00:34:57.160
the autonomous the AI assistant to do if
I have an a doctor and I have an AI

00:34:57.160 --> 00:34:57.170
I have an a doctor and I have an AI
 

00:34:57.170 --> 00:34:58.870
I have an a doctor and I have an AI
assistant that I would like to say and

00:34:58.870 --> 00:34:58.880
assistant that I would like to say and
 

00:34:58.880 --> 00:35:02.140
assistant that I would like to say and
say go and see if you find any case

00:35:02.140 --> 00:35:02.150
say go and see if you find any case
 

00:35:02.150 --> 00:35:04.600
say go and see if you find any case
similar to these on all the images you

00:35:04.600 --> 00:35:04.610
similar to these on all the images you
 

00:35:04.610 --> 00:35:06.520
similar to these on all the images you
have collected all these data from all

00:35:06.520 --> 00:35:06.530
have collected all these data from all
 

00:35:06.530 --> 00:35:07.360
have collected all these data from all
over the world

00:35:07.360 --> 00:35:07.370
over the world
 

00:35:07.370 --> 00:35:10.330
over the world
I would like to trust that the thing

00:35:10.330 --> 00:35:10.340
I would like to trust that the thing
 

00:35:10.340 --> 00:35:12.400
I would like to trust that the thing
that I'm talking with is a Isis that

00:35:12.400 --> 00:35:12.410
that I'm talking with is a Isis that
 

00:35:12.410 --> 00:35:15.610
that I'm talking with is a Isis that
does in fact go through all the data in

00:35:15.610 --> 00:35:15.620
does in fact go through all the data in
 

00:35:15.620 --> 00:35:16.690
does in fact go through all the data in
the whole world

00:35:16.690 --> 00:35:16.700
the whole world
 

00:35:16.700 --> 00:35:19.540
the whole world
and not just two parts of it to actually

00:35:19.540 --> 00:35:19.550
and not just two parts of it to actually
 

00:35:19.550 --> 00:35:23.530
and not just two parts of it to actually
answer my question so the what I believe

00:35:23.530 --> 00:35:23.540
answer my question so the what I believe
 

00:35:23.540 --> 00:35:25.570
answer my question so the what I believe
it's happening what I believe for me

00:35:25.570 --> 00:35:25.580
it's happening what I believe for me
 

00:35:25.580 --> 00:35:29.520
it's happening what I believe for me
trust is that the task that I will

00:35:29.520 --> 00:35:29.530
trust is that the task that I will
 

00:35:29.530 --> 00:35:32.320
trust is that the task that I will
interact with is AI agent to be

00:35:32.320 --> 00:35:32.330
interact with is AI agent to be
 

00:35:32.330 --> 00:35:34.840
interact with is AI agent to be
performed is actually done the way I

00:35:34.840 --> 00:35:34.850
performed is actually done the way I
 

00:35:34.850 --> 00:35:37.870
performed is actually done the way I
expected it to be done so that kind of

00:35:37.870 --> 00:35:37.880
expected it to be done so that kind of
 

00:35:37.880 --> 00:35:41.440
expected it to be done so that kind of
like matching my expectations when I

00:35:41.440 --> 00:35:41.450
like matching my expectations when I
 

00:35:41.450 --> 00:35:43.690
like matching my expectations when I
interact with the AI agent is where I

00:35:43.690 --> 00:35:43.700
interact with the AI agent is where I
 

00:35:43.700 --> 00:35:47.200
interact with the AI agent is where I
believe that I build a trust in the

00:35:47.200 --> 00:35:47.210
believe that I build a trust in the
 

00:35:47.210 --> 00:35:49.750
believe that I build a trust in the
Machine I want to add one thing though

00:35:49.750 --> 00:35:49.760
Machine I want to add one thing though
 

00:35:49.760 --> 00:35:52.540
Machine I want to add one thing though
to the point that these distrusting the

00:35:52.540 --> 00:35:52.550
to the point that these distrusting the
 

00:35:52.550 --> 00:35:54.940
to the point that these distrusting the
developers do not trust me do not trust

00:35:54.940 --> 00:35:54.950
developers do not trust me do not trust
 

00:35:54.950 --> 00:35:57.580
developers do not trust me do not trust
my students and I'll explain why it's

00:35:57.580 --> 00:35:57.590
my students and I'll explain why it's
 

00:35:57.590 --> 00:36:00.730
my students and I'll explain why it's
not not trusting but when just one

00:36:00.730 --> 00:36:00.740
not not trusting but when just one
 

00:36:00.740 --> 00:36:03.880
not not trusting but when just one
single thing I'm not going to put my

00:36:03.880 --> 00:36:03.890
single thing I'm not going to put my
 

00:36:03.890 --> 00:36:06.430
single thing I'm not going to put my
hands on fire and guarantee that my

00:36:06.430 --> 00:36:06.440
hands on fire and guarantee that my
 

00:36:06.440 --> 00:36:09.210
hands on fire and guarantee that my
robot is not going to hit the wall or

00:36:09.210 --> 00:36:09.220
robot is not going to hit the wall or
 

00:36:09.220 --> 00:36:13.870
robot is not going to hit the wall or
III just have a hard time achieving that

00:36:13.870 --> 00:36:13.880
III just have a hard time achieving that
 

00:36:13.880 --> 00:36:17.650
III just have a hard time achieving that
level of assurance and and the reason is

00:36:17.650 --> 00:36:17.660
level of assurance and and the reason is
 

00:36:17.660 --> 00:36:22.090
level of assurance and and the reason is
because the world the environment the

00:36:22.090 --> 00:36:22.100
because the world the environment the
 

00:36:22.100 --> 00:36:26.260
because the world the environment the
people are very complex systems it's

00:36:26.260 --> 00:36:26.270
people are very complex systems it's
 

00:36:26.270 --> 00:36:27.610
people are very complex systems it's
extremely complex

00:36:27.610 --> 00:36:27.620
extremely complex
 

00:36:27.620 --> 00:36:31.180
extremely complex
x2 from a research point of view develop

00:36:31.180 --> 00:36:31.190
x2 from a research point of view develop
 

00:36:31.190 --> 00:36:34.540
x2 from a research point of view develop
a solution that is trustable for the

00:36:34.540 --> 00:36:34.550
a solution that is trustable for the
 

00:36:34.550 --> 00:36:37.240
a solution that is trustable for the
complexity of where we want to make our

00:36:37.240 --> 00:36:37.250
complexity of where we want to make our
 

00:36:37.250 --> 00:36:40.330
complexity of where we want to make our
systems autonomous it's not bad faith

00:36:40.330 --> 00:36:40.340
systems autonomous it's not bad faith
 

00:36:40.340 --> 00:36:44.530
systems autonomous it's not bad faith
it's not like it's not really intention

00:36:44.530 --> 00:36:44.540
it's not like it's not really intention
 

00:36:44.540 --> 00:36:47.320
it's not like it's not really intention
it's just that it is too complex it's

00:36:47.320 --> 00:36:47.330
it's just that it is too complex it's
 

00:36:47.330 --> 00:36:50.650
it's just that it is too complex it's
just too hard and it's too hard in the

00:36:50.650 --> 00:36:50.660
just too hard and it's too hard in the
 

00:36:50.660 --> 00:36:53.020
just too hard and it's too hard in the
sense that there is a very uncertain

00:36:53.020 --> 00:36:53.030
sense that there is a very uncertain
 

00:36:53.030 --> 00:36:55.900
sense that there is a very uncertain
system that you now are supposed to

00:36:55.900 --> 00:36:55.910
system that you now are supposed to
 

00:36:55.910 --> 00:36:59.190
system that you now are supposed to
handle with some kind of like algorithm

00:36:59.190 --> 00:36:59.200
handle with some kind of like algorithm
 

00:36:59.200 --> 00:37:02.680
handle with some kind of like algorithm
so it's like the sandy storm it's like

00:37:02.680 --> 00:37:02.690
so it's like the sandy storm it's like
 

00:37:02.690 --> 00:37:06.910
so it's like the sandy storm it's like
9/11 it's like all these complex things

00:37:06.910 --> 00:37:06.920
9/11 it's like all these complex things
 

00:37:06.920 --> 00:37:10.090
9/11 it's like all these complex things
that no one was able even within humans

00:37:10.090 --> 00:37:10.100
that no one was able even within humans
 

00:37:10.100 --> 00:37:12.250
that no one was able even within humans
to be able to find a solution for them

00:37:12.250 --> 00:37:12.260
to be able to find a solution for them
 

00:37:12.260 --> 00:37:16.000
to be able to find a solution for them
they just I mean the world is too

00:37:16.000 --> 00:37:16.010
they just I mean the world is too
 

00:37:16.010 --> 00:37:19.060
they just I mean the world is too
difficult it's too hard on the other

00:37:19.060 --> 00:37:19.070
difficult it's too hard on the other
 

00:37:19.070 --> 00:37:21.130
difficult it's too hard on the other
hand are we going to give up on

00:37:21.130 --> 00:37:21.140
hand are we going to give up on
 

00:37:21.140 --> 00:37:23.050
hand are we going to give up on
automating things just because of the

00:37:23.050 --> 00:37:23.060
automating things just because of the
 

00:37:23.060 --> 00:37:25.420
automating things just because of the
complexity and so what I'm trying to

00:37:25.420 --> 00:37:25.430
complexity and so what I'm trying to
 

00:37:25.430 --> 00:37:28.780
complexity and so what I'm trying to
tell you is like this I believe we have

00:37:28.780 --> 00:37:28.790
tell you is like this I believe we have
 

00:37:28.790 --> 00:37:33.250
tell you is like this I believe we have
to accept their AI as humans do will

00:37:33.250 --> 00:37:33.260
to accept their AI as humans do will
 

00:37:33.260 --> 00:37:36.070
to accept their AI as humans do will
eventually make mistakes what we might

00:37:36.070 --> 00:37:36.080
eventually make mistakes what we might
 

00:37:36.080 --> 00:37:38.830
eventually make mistakes what we might
not want to accept is that it doesn't

00:37:38.830 --> 00:37:38.840
not want to accept is that it doesn't
 

00:37:38.840 --> 00:37:41.470
not want to accept is that it doesn't
improve over time we do we might not

00:37:41.470 --> 00:37:41.480
improve over time we do we might not
 

00:37:41.480 --> 00:37:43.360
improve over time we do we might not
want to himself is the lack of learning

00:37:43.360 --> 00:37:43.370
want to himself is the lack of learning
 

00:37:43.370 --> 00:37:45.970
want to himself is the lack of learning
what I'm saying is that any kind of

00:37:45.970 --> 00:37:45.980
what I'm saying is that any kind of
 

00:37:45.980 --> 00:37:48.250
what I'm saying is that any kind of
instance of something that didn't go

00:37:48.250 --> 00:37:48.260
instance of something that didn't go
 

00:37:48.260 --> 00:37:48.720
instance of something that didn't go
well

00:37:48.720 --> 00:37:48.730
well
 

00:37:48.730 --> 00:37:51.850
well
needs to be more data and needs to be

00:37:51.850 --> 00:37:51.860
needs to be more data and needs to be
 

00:37:51.860 --> 00:37:54.430
needs to be more data and needs to be
more input for the AI system in its

00:37:54.430 --> 00:37:54.440
more input for the AI system in its
 

00:37:54.440 --> 00:37:57.340
more input for the AI system in its
dynamics like ela was saying to become

00:37:57.340 --> 00:37:57.350
dynamics like ela was saying to become
 

00:37:57.350 --> 00:38:00.490
dynamics like ela was saying to become
better even whatever a cognitive

00:38:00.490 --> 00:38:00.500
better even whatever a cognitive
 

00:38:00.500 --> 00:38:03.610
better even whatever a cognitive
assistant you do to a person if it's not

00:38:03.610 --> 00:38:03.620
assistant you do to a person if it's not
 

00:38:03.620 --> 00:38:06.340
assistant you do to a person if it's not
a moving system a dynamic system that

00:38:06.340 --> 00:38:06.350
a moving system a dynamic system that
 

00:38:06.350 --> 00:38:09.340
a moving system a dynamic system that
gets better with experience we are not

00:38:09.340 --> 00:38:09.350
gets better with experience we are not
 

00:38:09.350 --> 00:38:11.620
gets better with experience we are not
going to ever be able to I believe

00:38:11.620 --> 00:38:11.630
going to ever be able to I believe
 

00:38:11.630 --> 00:38:14.080
going to ever be able to I believe
generates some kind of static system

00:38:14.080 --> 00:38:14.090
generates some kind of static system
 

00:38:14.090 --> 00:38:17.650
generates some kind of static system
that handles enormous complexity and

00:38:17.650 --> 00:38:17.660
that handles enormous complexity and
 

00:38:17.660 --> 00:38:19.540
that handles enormous complexity and
just to finish one thought I've been

00:38:19.540 --> 00:38:19.550
just to finish one thought I've been
 

00:38:19.550 --> 00:38:22.480
just to finish one thought I've been
doing robot soccer for 20 years so

00:38:22.480 --> 00:38:22.490
doing robot soccer for 20 years so
 

00:38:22.490 --> 00:38:24.970
doing robot soccer for 20 years so
little soccer playing robots that go to

00:38:24.970 --> 00:38:24.980
little soccer playing robots that go to
 

00:38:24.980 --> 00:38:27.580
little soccer playing robots that go to
these competitions even after 20 years

00:38:27.580 --> 00:38:27.590
these competitions even after 20 years
 

00:38:27.590 --> 00:38:30.910
these competitions even after 20 years
there is always something that surprises

00:38:30.910 --> 00:38:30.920
there is always something that surprises
 

00:38:30.920 --> 00:38:35.380
there is always something that surprises
us in these games with an opponent robot

00:38:35.380 --> 00:38:35.390
us in these games with an opponent robot
 

00:38:35.390 --> 00:38:38.620
us in these games with an opponent robot
soccer there is the the bursary just

00:38:38.620 --> 00:38:38.630
soccer there is the the bursary just
 

00:38:38.630 --> 00:38:39.340
soccer there is the the bursary just
shoots and

00:38:39.340 --> 00:38:39.350
shoots and
 

00:38:39.350 --> 00:38:41.200
shoots and
bounce is often about robots and we say

00:38:41.200 --> 00:38:41.210
bounce is often about robots and we say
 

00:38:41.210 --> 00:38:43.090
bounce is often about robots and we say
how come you are supposed to shoot

00:38:43.090 --> 00:38:43.100
how come you are supposed to shoot
 

00:38:43.100 --> 00:38:45.280
how come you are supposed to shoot
directly to go not use our robots and

00:38:45.280 --> 00:38:45.290
directly to go not use our robots and
 

00:38:45.290 --> 00:38:47.140
directly to go not use our robots and
bouncing halls yeah

00:38:47.140 --> 00:38:47.150
bouncing halls yeah
 

00:38:47.150 --> 00:38:49.630
bouncing halls yeah
that's what happened two years ago then

00:38:49.630 --> 00:38:49.640
that's what happened two years ago then
 

00:38:49.640 --> 00:38:52.230
that's what happened two years ago then
last year another one of these surprises

00:38:52.230 --> 00:38:52.240
last year another one of these surprises
 

00:38:52.240 --> 00:38:55.540
last year another one of these surprises
inevitably even after 20 years can you

00:38:55.540 --> 00:38:55.550
inevitably even after 20 years can you
 

00:38:55.550 --> 00:38:57.130
inevitably even after 20 years can you
imagine that I go to these games and I'm

00:38:57.130 --> 00:38:57.140
imagine that I go to these games and I'm
 

00:38:57.140 --> 00:38:59.350
imagine that I go to these games and I'm
still surprised how these adversaries

00:38:59.350 --> 00:38:59.360
still surprised how these adversaries
 

00:38:59.360 --> 00:39:02.190
still surprised how these adversaries
figure out something that I did not

00:39:02.190 --> 00:39:02.200
figure out something that I did not
 

00:39:02.200 --> 00:39:06.160
figure out something that I did not
thought about so that puts you in this

00:39:06.160 --> 00:39:06.170
thought about so that puts you in this
 

00:39:06.170 --> 00:39:08.800
thought about so that puts you in this
kind of learning curve in this kind of

00:39:08.800 --> 00:39:08.810
kind of learning curve in this kind of
 

00:39:08.810 --> 00:39:11.080
kind of learning curve in this kind of
life trying to minimize the errors you

00:39:11.080 --> 00:39:11.090
life trying to minimize the errors you
 

00:39:11.090 --> 00:39:14.470
life trying to minimize the errors you
make and not really assuring that there

00:39:14.470 --> 00:39:14.480
make and not really assuring that there
 

00:39:14.480 --> 00:39:15.400
make and not really assuring that there
are no errors

00:39:15.400 --> 00:39:15.410
are no errors
 

00:39:15.410 --> 00:39:19.510
are no errors
it's just and of course

00:39:19.510 --> 00:39:19.520
it's just and of course
 

00:39:19.520 --> 00:39:22.210
it's just and of course
perfect predictability and Trust you

00:39:22.210 --> 00:39:22.220
perfect predictability and Trust you
 

00:39:22.220 --> 00:39:23.320
perfect predictability and Trust you
don't have to have perfect for the

00:39:23.320 --> 00:39:23.330
don't have to have perfect for the
 

00:39:23.330 --> 00:39:24.880
don't have to have perfect for the
predictability to have trust we could

00:39:24.880 --> 00:39:24.890
predictability to have trust we could
 

00:39:24.890 --> 00:39:26.920
predictability to have trust we could
have for example many of us have trust

00:39:26.920 --> 00:39:26.930
have for example many of us have trust
 

00:39:26.930 --> 00:39:28.840
have for example many of us have trust
in other humans even though humans

00:39:28.840 --> 00:39:28.850
in other humans even though humans
 

00:39:28.850 --> 00:39:31.300
in other humans even though humans
surprising each other all the time it's

00:39:31.300 --> 00:39:31.310
surprising each other all the time it's
 

00:39:31.310 --> 00:39:32.800
surprising each other all the time it's
I think does go to this issue the motion

00:39:32.800 --> 00:39:32.810
I think does go to this issue the motion
 

00:39:32.810 --> 00:39:34.000
I think does go to this issue the motion
is saying if we're making ourselves

00:39:34.000 --> 00:39:34.010
is saying if we're making ourselves
 

00:39:34.010 --> 00:39:36.640
is saying if we're making ourselves
vulnerable in certain ways and Trust is

00:39:36.640 --> 00:39:36.650
vulnerable in certain ways and Trust is
 

00:39:36.650 --> 00:39:39.100
vulnerable in certain ways and Trust is
relying on somebody else when you're in

00:39:39.100 --> 00:39:39.110
relying on somebody else when you're in
 

00:39:39.110 --> 00:39:41.710
relying on somebody else when you're in
that state of vulnerability and I think

00:39:41.710 --> 00:39:41.720
that state of vulnerability and I think
 

00:39:41.720 --> 00:39:42.940
that state of vulnerability and I think
it raises all kinds of interesting

00:39:42.940 --> 00:39:42.950
it raises all kinds of interesting
 

00:39:42.950 --> 00:39:44.650
it raises all kinds of interesting
questions of course about the weapons

00:39:44.650 --> 00:39:44.660
questions of course about the weapons
 

00:39:44.660 --> 00:39:48.250
questions of course about the weapons
space because is it reason you know

00:39:48.250 --> 00:39:48.260
space because is it reason you know
 

00:39:48.260 --> 00:39:49.840
space because is it reason you know
we've got very good reasons to think

00:39:49.840 --> 00:39:49.850
we've got very good reasons to think
 

00:39:49.850 --> 00:39:52.420
we've got very good reasons to think
they won't be perfect they will make

00:39:52.420 --> 00:39:52.430
they won't be perfect they will make
 

00:39:52.430 --> 00:39:54.130
they won't be perfect they will make
mistakes of course human soldiers make

00:39:54.130 --> 00:39:54.140
mistakes of course human soldiers make
 

00:39:54.140 --> 00:39:56.290
mistakes of course human soldiers make
mistakes human policymakers make just

00:39:56.290 --> 00:39:56.300
mistakes human policymakers make just
 

00:39:56.300 --> 00:39:59.620
mistakes human policymakers make just
make mistakes so when we think about

00:39:59.620 --> 00:39:59.630
make mistakes so when we think about
 

00:39:59.630 --> 00:40:02.440
make mistakes so when we think about
verification we think about deliberation

00:40:02.440 --> 00:40:02.450
verification we think about deliberation
 

00:40:02.450 --> 00:40:04.810
verification we think about deliberation
about what we as an international

00:40:04.810 --> 00:40:04.820
about what we as an international
 

00:40:04.820 --> 00:40:07.680
about what we as an international
community or a National Defense

00:40:07.680 --> 00:40:07.690
community or a National Defense
 

00:40:07.690 --> 00:40:10.630
community or a National Defense
organization ought to do how do we think

00:40:10.630 --> 00:40:10.640
organization ought to do how do we think
 

00:40:10.640 --> 00:40:13.090
organization ought to do how do we think
about the necessity of accidents the

00:40:13.090 --> 00:40:13.100
about the necessity of accidents the
 

00:40:13.100 --> 00:40:14.890
about the necessity of accidents the
necessity of surprises they're going to

00:40:14.890 --> 00:40:14.900
necessity of surprises they're going to
 

00:40:14.900 --> 00:40:18.910
necessity of surprises they're going to
happen is it you know do we do we aspire

00:40:18.910 --> 00:40:18.920
happen is it you know do we do we aspire
 

00:40:18.920 --> 00:40:21.850
happen is it you know do we do we aspire
to too much if we aspire to we know we

00:40:21.850 --> 00:40:21.860
to too much if we aspire to we know we
 

00:40:21.860 --> 00:40:23.050
to too much if we aspire to we know we
aspire to too much voice fire to

00:40:23.050 --> 00:40:23.060
aspire to too much voice fire to
 

00:40:23.060 --> 00:40:26.560
aspire to too much voice fire to
perfection give thoughts about the

00:40:26.560 --> 00:40:26.570
perfection give thoughts about the
 

00:40:26.570 --> 00:40:29.800
perfection give thoughts about the
relevance of accidents and surprises in

00:40:29.800 --> 00:40:29.810
relevance of accidents and surprises in
 

00:40:29.810 --> 00:40:31.480
relevance of accidents and surprises in
the context of weapons feel free to say

00:40:31.480 --> 00:40:31.490
the context of weapons feel free to say
 

00:40:31.490 --> 00:40:37.540
the context of weapons feel free to say
you'd like to actually I wanted to pick

00:40:37.540 --> 00:40:37.550
you'd like to actually I wanted to pick
 

00:40:37.550 --> 00:40:38.920
you'd like to actually I wanted to pick
up on something my mother was talking to

00:40:38.920 --> 00:40:38.930
up on something my mother was talking to
 

00:40:38.930 --> 00:40:40.930
up on something my mother was talking to
you mind please I'm here talking bout

00:40:40.930 --> 00:40:40.940
you mind please I'm here talking bout
 

00:40:40.940 --> 00:40:42.730
you mind please I'm here talking bout
expectations and this is a really

00:40:42.730 --> 00:40:42.740
expectations and this is a really
 

00:40:42.740 --> 00:40:44.560
expectations and this is a really
probably quite technologically

00:40:44.560 --> 00:40:44.570
probably quite technologically
 

00:40:44.570 --> 00:40:46.000
probably quite technologically
sophisticated audience

00:40:46.000 --> 00:40:46.010
sophisticated audience
 

00:40:46.010 --> 00:40:47.980
sophisticated audience
I mean technologically literate but I

00:40:47.980 --> 00:40:47.990
I mean technologically literate but I
 

00:40:47.990 --> 00:40:49.120
I mean technologically literate but I
deal with a lot of people who don't have

00:40:49.120 --> 00:40:49.130
deal with a lot of people who don't have
 

00:40:49.130 --> 00:40:51.230
deal with a lot of people who don't have
a lot of technological literacy

00:40:51.230 --> 00:40:51.240
a lot of technological literacy
 

00:40:51.240 --> 00:40:54.620
a lot of technological literacy
and I I think we're had a real

00:40:54.620 --> 00:40:54.630
and I I think we're had a real
 

00:40:54.630 --> 00:40:56.420
and I I think we're had a real
interesting tipping point when it comes

00:40:56.420 --> 00:40:56.430
interesting tipping point when it comes
 

00:40:56.430 --> 00:40:58.250
interesting tipping point when it comes
to this issue of trust and technology

00:40:58.250 --> 00:40:58.260
to this issue of trust and technology
 

00:40:58.260 --> 00:41:02.599
to this issue of trust and technology
right now where a lot of people have

00:41:02.599 --> 00:41:02.609
right now where a lot of people have
 

00:41:02.609 --> 00:41:05.599
right now where a lot of people have
blind trust we have fear but we have

00:41:05.599 --> 00:41:05.609
blind trust we have fear but we have
 

00:41:05.609 --> 00:41:08.480
blind trust we have fear but we have
blind trust in technology and recent

00:41:08.480 --> 00:41:08.490
blind trust in technology and recent
 

00:41:08.490 --> 00:41:10.910
blind trust in technology and recent
events pick one because there's many to

00:41:10.910 --> 00:41:10.920
events pick one because there's many to
 

00:41:10.920 --> 00:41:12.410
events pick one because there's many to
choose from

00:41:12.410 --> 00:41:12.420
choose from
 

00:41:12.420 --> 00:41:14.990
choose from
have I think brought us to a societal

00:41:14.990 --> 00:41:15.000
have I think brought us to a societal
 

00:41:15.000 --> 00:41:17.359
have I think brought us to a societal
conversation about wait a minute we need

00:41:17.359 --> 00:41:17.369
conversation about wait a minute we need
 

00:41:17.369 --> 00:41:21.440
conversation about wait a minute we need
have to go from having blind trust to

00:41:21.440 --> 00:41:21.450
have to go from having blind trust to
 

00:41:21.450 --> 00:41:25.670
have to go from having blind trust to
having our technologies the developers

00:41:25.670 --> 00:41:25.680
having our technologies the developers
 

00:41:25.680 --> 00:41:28.160
having our technologies the developers
of those technologies and the users and

00:41:28.160 --> 00:41:28.170
of those technologies and the users and
 

00:41:28.170 --> 00:41:30.200
of those technologies and the users and
the deployers of those technologies they

00:41:30.200 --> 00:41:30.210
the deployers of those technologies they
 

00:41:30.210 --> 00:41:32.960
the deployers of those technologies they
need to earn my trust and we're gonna

00:41:32.960 --> 00:41:32.970
need to earn my trust and we're gonna
 

00:41:32.970 --> 00:41:34.760
need to earn my trust and we're gonna
have to make that tipping point between

00:41:34.760 --> 00:41:34.770
have to make that tipping point between
 

00:41:34.770 --> 00:41:37.970
have to make that tipping point between
blind trust and earning trust and what

00:41:37.970 --> 00:41:37.980
blind trust and earning trust and what
 

00:41:37.980 --> 00:41:39.620
blind trust and earning trust and what
are the mechanisms and what are the

00:41:39.620 --> 00:41:39.630
are the mechanisms and what are the
 

00:41:39.630 --> 00:41:42.650
are the mechanisms and what are the
tools that we have to earn trust to

00:41:42.650 --> 00:41:42.660
tools that we have to earn trust to
 

00:41:42.660 --> 00:41:45.430
tools that we have to earn trust to
verify trust to have reliable systems

00:41:45.430 --> 00:41:45.440
verify trust to have reliable systems
 

00:41:45.440 --> 00:41:47.839
verify trust to have reliable systems
predictable systems confidence in our

00:41:47.839 --> 00:41:47.849
predictable systems confidence in our
 

00:41:47.849 --> 00:41:49.520
predictable systems confidence in our
systems it's not much more interesting

00:41:49.520 --> 00:41:49.530
systems it's not much more interesting
 

00:41:49.530 --> 00:41:56.390
systems it's not much more interesting
than weapons earning trust and in fact

00:41:56.390 --> 00:41:56.400
than weapons earning trust and in fact
 

00:41:56.400 --> 00:41:58.760
than weapons earning trust and in fact
one of the things that we are working on

00:41:58.760 --> 00:41:58.770
one of the things that we are working on
 

00:41:58.770 --> 00:42:00.770
one of the things that we are working on
now is like to have these robots

00:42:00.770 --> 00:42:00.780
now is like to have these robots
 

00:42:00.780 --> 00:42:05.960
now is like to have these robots
actually what we call verifiable answers

00:42:05.960 --> 00:42:05.970
actually what we call verifiable answers
 

00:42:05.970 --> 00:42:08.240
actually what we call verifiable answers
so if I ask a robot how long did it take

00:42:08.240 --> 00:42:08.250
so if I ask a robot how long did it take
 

00:42:08.250 --> 00:42:10.849
so if I ask a robot how long did it take
you to go from the lab to my office and

00:42:10.849 --> 00:42:10.859
you to go from the lab to my office and
 

00:42:10.859 --> 00:42:13.339
you to go from the lab to my office and
the robot says you know two minutes and

00:42:13.339 --> 00:42:13.349
the robot says you know two minutes and
 

00:42:13.349 --> 00:42:15.410
the robot says you know two minutes and
a half how do we know that number is

00:42:15.410 --> 00:42:15.420
a half how do we know that number is
 

00:42:15.420 --> 00:42:18.319
a half how do we know that number is
right and so what we do is that we

00:42:18.319 --> 00:42:18.329
right and so what we do is that we
 

00:42:18.329 --> 00:42:20.329
right and so what we do is that we
actually have techniques to decompose

00:42:20.329 --> 00:42:20.339
actually have techniques to decompose
 

00:42:20.339 --> 00:42:24.500
actually have techniques to decompose
that question and to say okay how much

00:42:24.500 --> 00:42:24.510
that question and to say okay how much
 

00:42:24.510 --> 00:42:26.569
that question and to say okay how much
did it take you from going to the lab to

00:42:26.569 --> 00:42:26.579
did it take you from going to the lab to
 

00:42:26.579 --> 00:42:29.240
did it take you from going to the lab to
the kitchen and the robot would say a

00:42:29.240 --> 00:42:29.250
the kitchen and the robot would say a
 

00:42:29.250 --> 00:42:31.970
the kitchen and the robot would say a
minute and 10 seconds and now we ask

00:42:31.970 --> 00:42:31.980
minute and 10 seconds and now we ask
 

00:42:31.980 --> 00:42:34.370
minute and 10 seconds and now we ask
from the kitchen to my office and if

00:42:34.370 --> 00:42:34.380
from the kitchen to my office and if
 

00:42:34.380 --> 00:42:39.140
from the kitchen to my office and if
these adapts to 2.5 30 we 2 2 minutes

00:42:39.140 --> 00:42:39.150
these adapts to 2.5 30 we 2 2 minutes
 

00:42:39.150 --> 00:42:41.870
these adapts to 2.5 30 we 2 2 minutes
and 30 seconds then you trust that the

00:42:41.870 --> 00:42:41.880
and 30 seconds then you trust that the
 

00:42:41.880 --> 00:42:43.670
and 30 seconds then you trust that the
answer is good so we are doing a lot of

00:42:43.670 --> 00:42:43.680
answer is good so we are doing a lot of
 

00:42:43.680 --> 00:42:45.589
answer is good so we are doing a lot of
research with vittorio pereira in his

00:42:45.589 --> 00:42:45.599
research with vittorio pereira in his
 

00:42:45.599 --> 00:42:50.059
research with vittorio pereira in his
PhD thesis on this kind of like earning

00:42:50.059 --> 00:42:50.069
PhD thesis on this kind of like earning
 

00:42:50.069 --> 00:42:52.930
PhD thesis on this kind of like earning
the trust on finding methods to

00:42:52.930 --> 00:42:52.940
the trust on finding methods to
 

00:42:52.940 --> 00:42:55.339
the trust on finding methods to
decompose your questions so that there

00:42:55.339 --> 00:42:55.349
decompose your questions so that there
 

00:42:55.349 --> 00:42:59.390
decompose your questions so that there
if there was a malware or some wrong

00:42:59.390 --> 00:42:59.400
if there was a malware or some wrong
 

00:42:59.400 --> 00:43:01.880
if there was a malware or some wrong
answer that you cannot from a

00:43:01.880 --> 00:43:01.890
answer that you cannot from a
 

00:43:01.890 --> 00:43:04.440
answer that you cannot from a
theoretical point of view

00:43:04.440 --> 00:43:04.450
theoretical point of view
 

00:43:04.450 --> 00:43:08.309
theoretical point of view
randomly combined so much ever so much

00:43:08.309 --> 00:43:08.319
randomly combined so much ever so much
 

00:43:08.319 --> 00:43:10.529
randomly combined so much ever so much
malware that eventually things become

00:43:10.529 --> 00:43:10.539
malware that eventually things become
 

00:43:10.539 --> 00:43:13.440
malware that eventually things become
consistent so we are trying to earn turn

00:43:13.440 --> 00:43:13.450
consistent so we are trying to earn turn
 

00:43:13.450 --> 00:43:17.599
consistent so we are trying to earn turn
trust not only by explain ability but by

00:43:17.599 --> 00:43:17.609
trust not only by explain ability but by
 

00:43:17.609 --> 00:43:21.390
trust not only by explain ability but by
checking consistency of the answers such

00:43:21.390 --> 00:43:21.400
checking consistency of the answers such
 

00:43:21.400 --> 00:43:24.329
checking consistency of the answers such
that they actually add up to something

00:43:24.329 --> 00:43:24.339
that they actually add up to something
 

00:43:24.339 --> 00:43:26.779
that they actually add up to something
that makes sense and after all they are

00:43:26.779 --> 00:43:26.789
that makes sense and after all they are
 

00:43:26.789 --> 00:43:28.740
that makes sense and after all they are
computing or they are answering

00:43:28.740 --> 00:43:28.750
computing or they are answering
 

00:43:28.750 --> 00:43:31.620
computing or they are answering
something that overall makes sense and

00:43:31.620 --> 00:43:31.630
something that overall makes sense and
 

00:43:31.630 --> 00:43:34.890
something that overall makes sense and
it's proved to be right and so that's

00:43:34.890 --> 00:43:34.900
it's proved to be right and so that's
 

00:43:34.900 --> 00:43:36.599
it's proved to be right and so that's
something that's very new in our

00:43:36.599 --> 00:43:36.609
something that's very new in our
 

00:43:36.609 --> 00:43:39.509
something that's very new in our
research I mean we barely have started

00:43:39.509 --> 00:43:39.519
research I mean we barely have started
 

00:43:39.519 --> 00:43:42.809
research I mean we barely have started
this but again this is the question of

00:43:42.809 --> 00:43:42.819
this but again this is the question of
 

00:43:42.819 --> 00:43:44.549
this but again this is the question of
like when they give you an answer do you

00:43:44.549 --> 00:43:44.559
like when they give you an answer do you
 

00:43:44.559 --> 00:43:46.829
like when they give you an answer do you
trust that or not so you transform that

00:43:46.829 --> 00:43:46.839
trust that or not so you transform that
 

00:43:46.839 --> 00:43:48.089
trust that or not so you transform that
it's like they know if it's giving me

00:43:48.089 --> 00:43:48.099
it's like they know if it's giving me
 

00:43:48.099 --> 00:43:50.609
it's like they know if it's giving me
this answer okay then I'm going to trick

00:43:50.609 --> 00:43:50.619
this answer okay then I'm going to trick
 

00:43:50.619 --> 00:43:53.250
this answer okay then I'm going to trick
it and ask them more questions until I'm

00:43:53.250 --> 00:43:53.260
it and ask them more questions until I'm
 

00:43:53.260 --> 00:43:56.339
it and ask them more questions until I'm
confident that I earned that I mean that

00:43:56.339 --> 00:43:56.349
confident that I earned that I mean that
 

00:43:56.349 --> 00:44:00.960
confident that I earned that I mean that
this thing is trustable I'll just put in

00:44:00.960 --> 00:44:00.970
this thing is trustable I'll just put in
 

00:44:00.970 --> 00:44:02.190
this thing is trustable I'll just put in
a quick plug I think that this is one of

00:44:02.190 --> 00:44:02.200
a quick plug I think that this is one of
 

00:44:02.200 --> 00:44:04.710
a quick plug I think that this is one of
those places where interdisciplinary

00:44:04.710 --> 00:44:04.720
those places where interdisciplinary
 

00:44:04.720 --> 00:44:06.329
those places where interdisciplinary
work multidisciplinary work becomes

00:44:06.329 --> 00:44:06.339
work multidisciplinary work becomes
 

00:44:06.339 --> 00:44:07.920
work multidisciplinary work becomes
really critical because this idea of

00:44:07.920 --> 00:44:07.930
really critical because this idea of
 

00:44:07.930 --> 00:44:09.569
really critical because this idea of
earning trust development of trust

00:44:09.569 --> 00:44:09.579
earning trust development of trust
 

00:44:09.579 --> 00:44:11.519
earning trust development of trust
that's that's an area that social

00:44:11.519 --> 00:44:11.529
that's that's an area that social
 

00:44:11.529 --> 00:44:13.380
that's that's an area that social
psychologists organizational behavior

00:44:13.380 --> 00:44:13.390
psychologists organizational behavior
 

00:44:13.390 --> 00:44:15.150
psychologists organizational behavior
theorists have have studied for a long

00:44:15.150 --> 00:44:15.160
theorists have have studied for a long
 

00:44:15.160 --> 00:44:17.160
theorists have have studied for a long
time so there's a lot of resources that

00:44:17.160 --> 00:44:17.170
time so there's a lot of resources that
 

00:44:17.170 --> 00:44:19.410
time so there's a lot of resources that
that can be sort of bootstrapped to

00:44:19.410 --> 00:44:19.420
that can be sort of bootstrapped to
 

00:44:19.420 --> 00:44:20.069
that can be sort of bootstrapped to
maybe

00:44:20.069 --> 00:44:20.079
maybe
 

00:44:20.079 --> 00:44:21.420
maybe
short-circuit some of these things but

00:44:21.420 --> 00:44:21.430
short-circuit some of these things but
 

00:44:21.430 --> 00:44:24.390
short-circuit some of these things but
Moshe I'm sorry you know we talk a lot

00:44:24.390 --> 00:44:24.400
Moshe I'm sorry you know we talk a lot
 

00:44:24.400 --> 00:44:26.249
Moshe I'm sorry you know we talk a lot
about trusting technology but the

00:44:26.249 --> 00:44:26.259
about trusting technology but the
 

00:44:26.259 --> 00:44:28.019
about trusting technology but the
reality is Trust is mostly a human

00:44:28.019 --> 00:44:28.029
reality is Trust is mostly a human
 

00:44:28.029 --> 00:44:31.710
reality is Trust is mostly a human
relationship so if I'm talking about the

00:44:31.710 --> 00:44:31.720
relationship so if I'm talking about the
 

00:44:31.720 --> 00:44:33.299
relationship so if I'm talking about the
robot it will come out of manual manual

00:44:33.299 --> 00:44:33.309
robot it will come out of manual manual
 

00:44:33.309 --> 00:44:35.279
robot it will come out of manual manual
as love and I said trust at all but I

00:44:35.279 --> 00:44:35.289
as love and I said trust at all but I
 

00:44:35.289 --> 00:44:38.519
as love and I said trust at all but I
really trust manuela to make best effort

00:44:38.519 --> 00:44:38.529
really trust manuela to make best effort
 

00:44:38.529 --> 00:44:40.140
really trust manuela to make best effort
to give me something which is going to

00:44:40.140 --> 00:44:40.150
to give me something which is going to
 

00:44:40.150 --> 00:44:43.829
to give me something which is going to
be safe so because I really you know I

00:44:43.829 --> 00:44:43.839
be safe so because I really you know I
 

00:44:43.839 --> 00:44:45.960
be safe so because I really you know I
most of the time for most people the

00:44:45.960 --> 00:44:45.970
most of the time for most people the
 

00:44:45.970 --> 00:44:48.240
most of the time for most people the
technology is kind of a black box so

00:44:48.240 --> 00:44:48.250
technology is kind of a black box so
 

00:44:48.250 --> 00:44:49.859
technology is kind of a black box so
what does it mean that I trust this

00:44:49.859 --> 00:44:49.869
what does it mean that I trust this
 

00:44:49.869 --> 00:44:51.539
what does it mean that I trust this
program I have to trust the developer of

00:44:51.539 --> 00:44:51.549
program I have to trust the developer of
 

00:44:51.549 --> 00:44:53.609
program I have to trust the developer of
this program so we really need to think

00:44:53.609 --> 00:44:53.619
this program so we really need to think
 

00:44:53.619 --> 00:44:54.990
this program so we really need to think
about the different actors that are

00:44:54.990 --> 00:44:55.000
about the different actors that are
 

00:44:55.000 --> 00:44:58.049
about the different actors that are
involved here and to think of trust as a

00:44:58.049 --> 00:44:58.059
involved here and to think of trust as a
 

00:44:58.059 --> 00:44:59.940
involved here and to think of trust as a
relationship it's a human relationship

00:44:59.940 --> 00:44:59.950
relationship it's a human relationship
 

00:44:59.950 --> 00:45:01.859
relationship it's a human relationship
between different actors to a trust

00:45:01.859 --> 00:45:01.869
between different actors to a trust
 

00:45:01.869 --> 00:45:04.829
between different actors to a trust
Manuela yes do I trust certain

00:45:04.829 --> 00:45:04.839
Manuela yes do I trust certain
 

00:45:04.839 --> 00:45:12.440
Manuela yes do I trust certain
corporations maybe no do I trust a

00:45:12.440 --> 00:45:12.450
corporations maybe no do I trust a
 

00:45:12.450 --> 00:45:15.660
corporations maybe no do I trust a
developer that I know nothing about in

00:45:15.660 --> 00:45:15.670
developer that I know nothing about in
 

00:45:15.670 --> 00:45:17.740
developer that I know nothing about in
you know I mean probably it's

00:45:17.740 --> 00:45:17.750
you know I mean probably it's
 

00:45:17.750 --> 00:45:21.790
you know I mean probably it's
the same thing you know when when we

00:45:21.790 --> 00:45:21.800
the same thing you know when when we
 

00:45:21.800 --> 00:45:23.650
the same thing you know when when we
have a we feel safe with people we know

00:45:23.650 --> 00:45:23.660
have a we feel safe with people we know
 

00:45:23.660 --> 00:45:25.660
have a we feel safe with people we know
because we have gone to rebuild trust

00:45:25.660 --> 00:45:25.670
because we have gone to rebuild trust
 

00:45:25.670 --> 00:45:28.120
because we have gone to rebuild trust
between them you walk into a place where

00:45:28.120 --> 00:45:28.130
between them you walk into a place where
 

00:45:28.130 --> 00:45:31.060
between them you walk into a place where
you don't know anyone and in certain

00:45:31.060 --> 00:45:31.070
you don't know anyone and in certain
 

00:45:31.070 --> 00:45:32.860
you don't know anyone and in certain
areas of the world still a stranger

00:45:32.860 --> 00:45:32.870
areas of the world still a stranger
 

00:45:32.870 --> 00:45:35.440
areas of the world still a stranger
every stranger is a potential risk and

00:45:35.440 --> 00:45:35.450
every stranger is a potential risk and
 

00:45:35.450 --> 00:45:38.260
every stranger is a potential risk and
so so we have to start thinking less

00:45:38.260 --> 00:45:38.270
so so we have to start thinking less
 

00:45:38.270 --> 00:45:40.720
so so we have to start thinking less
about the technology and more about who

00:45:40.720 --> 00:45:40.730
about the technology and more about who
 

00:45:40.730 --> 00:45:42.760
about the technology and more about who
are the sort of characters in this play

00:45:42.760 --> 00:45:42.770
are the sort of characters in this play
 

00:45:42.770 --> 00:45:44.890
are the sort of characters in this play
and what are the set of trust we can

00:45:44.890 --> 00:45:44.900
and what are the set of trust we can
 

00:45:44.900 --> 00:45:46.900
and what are the set of trust we can
have between them and one of the sync of

00:45:46.900 --> 00:45:46.910
have between them and one of the sync of
 

00:45:46.910 --> 00:45:48.970
have between them and one of the sync of
course we know in warfare situation it's

00:45:48.970 --> 00:45:48.980
course we know in warfare situation it's
 

00:45:48.980 --> 00:45:51.160
course we know in warfare situation it's
a different situation and we have to see

00:45:51.160 --> 00:45:51.170
a different situation and we have to see
 

00:45:51.170 --> 00:45:52.930
a different situation and we have to see
very differently about trust then we

00:45:52.930 --> 00:45:52.940
very differently about trust then we
 

00:45:52.940 --> 00:45:55.210
very differently about trust then we
think in a non-hostile situation but I

00:45:55.210 --> 00:45:55.220
think in a non-hostile situation but I
 

00:45:55.220 --> 00:45:56.500
think in a non-hostile situation but I
think the trust ultimately it's not

00:45:56.500 --> 00:45:56.510
think the trust ultimately it's not
 

00:45:56.510 --> 00:45:59.320
think the trust ultimately it's not
relation between human and machine it's

00:45:59.320 --> 00:45:59.330
relation between human and machine it's
 

00:45:59.330 --> 00:46:03.780
relation between human and machine it's
a relationship between humans

00:46:03.780 --> 00:46:03.790
 
 

00:46:03.790 --> 00:46:06.940
 
well it seems we'll be like humans they

00:46:06.940 --> 00:46:06.950
well it seems we'll be like humans they
 

00:46:06.950 --> 00:46:09.490
well it seems we'll be like humans they
will make decisions they will process

00:46:09.490 --> 00:46:09.500
will make decisions they will process
 

00:46:09.500 --> 00:46:11.770
will make decisions they will process
information they will have tons of data

00:46:11.770 --> 00:46:11.780
information they will have tons of data
 

00:46:11.780 --> 00:46:14.110
information they will have tons of data
and they will be things you interact

00:46:14.110 --> 00:46:14.120
and they will be things you interact
 

00:46:14.120 --> 00:46:16.450
and they will be things you interact
with like you interact with humans so

00:46:16.450 --> 00:46:16.460
with like you interact with humans so
 

00:46:16.460 --> 00:46:19.180
with like you interact with humans so
eventually you may want to also trust

00:46:19.180 --> 00:46:19.190
eventually you may want to also trust
 

00:46:19.190 --> 00:46:23.110
eventually you may want to also trust
the machinery and not just the humans

00:46:23.110 --> 00:46:23.120
the machinery and not just the humans
 

00:46:23.120 --> 00:46:24.160
the machinery and not just the humans
will develop the machine I don't

00:46:24.160 --> 00:46:24.170
will develop the machine I don't
 

00:46:24.170 --> 00:46:26.290
will develop the machine I don't
disagree but if I trust my refrigerator

00:46:26.290 --> 00:46:26.300
disagree but if I trust my refrigerator
 

00:46:26.300 --> 00:46:29.620
disagree but if I trust my refrigerator
is because I really trust ultimately the

00:46:29.620 --> 00:46:29.630
is because I really trust ultimately the
 

00:46:29.630 --> 00:46:31.420
is because I really trust ultimately the
process that led to building this I mean

00:46:31.420 --> 00:46:31.430
process that led to building this I mean
 

00:46:31.430 --> 00:46:33.670
process that led to building this I mean
how does that every filter of my trust

00:46:33.670 --> 00:46:33.680
how does that every filter of my trust
 

00:46:33.680 --> 00:46:36.100
how does that every filter of my trust
one because it does what it's supposed

00:46:36.100 --> 00:46:36.110
one because it does what it's supposed
 

00:46:36.110 --> 00:46:38.770
one because it does what it's supposed
to do but I don't think there's anything

00:46:38.770 --> 00:46:38.780
to do but I don't think there's anything
 

00:46:38.780 --> 00:46:40.900
to do but I don't think there's anything
nefarious about it that really the goal

00:46:40.900 --> 00:46:40.910
nefarious about it that really the goal
 

00:46:40.910 --> 00:46:43.180
nefarious about it that really the goal
is at some point to suddenly surprise me

00:46:43.180 --> 00:46:43.190
is at some point to suddenly surprise me
 

00:46:43.190 --> 00:46:45.400
is at some point to suddenly surprise me
and give me poison food because you know

00:46:45.400 --> 00:46:45.410
and give me poison food because you know
 

00:46:45.410 --> 00:46:47.380
and give me poison food because you know
I've the people who built I trust them

00:46:47.380 --> 00:46:47.390
I've the people who built I trust them
 

00:46:47.390 --> 00:46:49.660
I've the people who built I trust them
ultimately so we end up trusting me

00:46:49.660 --> 00:46:49.670
ultimately so we end up trusting me
 

00:46:49.670 --> 00:46:52.090
ultimately so we end up trusting me
trusting machines by by their

00:46:52.090 --> 00:46:52.100
trusting machines by by their
 

00:46:52.100 --> 00:46:53.740
trusting machines by by their
predictability if something is

00:46:53.740 --> 00:46:53.750
predictability if something is
 

00:46:53.750 --> 00:46:55.300
predictability if something is
predictably having a certain way which

00:46:55.300 --> 00:46:55.310
predictably having a certain way which
 

00:46:55.310 --> 00:46:58.480
predictably having a certain way which
says okay but also we we intuitively

00:46:58.480 --> 00:46:58.490
says okay but also we we intuitively
 

00:46:58.490 --> 00:47:01.360
says okay but also we we intuitively
think about the dose the Actos human

00:47:01.360 --> 00:47:01.370
think about the dose the Actos human
 

00:47:01.370 --> 00:47:03.460
think about the dose the Actos human
actors involved in this which of course

00:47:03.460 --> 00:47:03.470
actors involved in this which of course
 

00:47:03.470 --> 00:47:05.650
actors involved in this which of course
then raises a whole other set of issues

00:47:05.650 --> 00:47:05.660
then raises a whole other set of issues
 

00:47:05.660 --> 00:47:07.960
then raises a whole other set of issues
and complexities around when you have

00:47:07.960 --> 00:47:07.970
and complexities around when you have
 

00:47:07.970 --> 00:47:10.210
and complexities around when you have
companies doing this that for good

00:47:10.210 --> 00:47:10.220
companies doing this that for good
 

00:47:10.220 --> 00:47:11.560
companies doing this that for good
competitive reasons I'm not going to

00:47:11.560 --> 00:47:11.570
competitive reasons I'm not going to
 

00:47:11.570 --> 00:47:12.940
competitive reasons I'm not going to
disclose how they built the technology

00:47:12.940 --> 00:47:12.950
disclose how they built the technology
 

00:47:12.950 --> 00:47:15.670
disclose how they built the technology
are not gonna how do we get transparency

00:47:15.670 --> 00:47:15.680
are not gonna how do we get transparency
 

00:47:15.680 --> 00:47:18.160
are not gonna how do we get transparency
of those of that sort and I think these

00:47:18.160 --> 00:47:18.170
of those of that sort and I think these
 

00:47:18.170 --> 00:47:20.380
of those of that sort and I think these
raise an enormous number of questions

00:47:20.380 --> 00:47:20.390
raise an enormous number of questions
 

00:47:20.390 --> 00:47:21.970
raise an enormous number of questions
and unfortunately I'm getting the red

00:47:21.970 --> 00:47:21.980
and unfortunately I'm getting the red
 

00:47:21.980 --> 00:47:24.690
and unfortunately I'm getting the red
light that tells us that that our

00:47:24.690 --> 00:47:24.700
light that tells us that that our
 

00:47:24.700 --> 00:47:27.310
light that tells us that that our
conversation is coming to a close I

00:47:27.310 --> 00:47:27.320
conversation is coming to a close I
 

00:47:27.320 --> 00:47:29.970
conversation is coming to a close I
wanted to give you each one

00:47:29.970 --> 00:47:29.980
wanted to give you each one
 

00:47:29.980 --> 00:47:32.040
wanted to give you each one
if you want to say something very

00:47:32.040 --> 00:47:32.050
if you want to say something very
 

00:47:32.050 --> 00:47:34.230
if you want to say something very
quickly sort of in closing I just want

00:47:34.230 --> 00:47:34.240
quickly sort of in closing I just want
 

00:47:34.240 --> 00:47:37.050
quickly sort of in closing I just want
to say one thing I saw recently a very

00:47:37.050 --> 00:47:37.060
to say one thing I saw recently a very
 

00:47:37.060 --> 00:47:40.740
to say one thing I saw recently a very
interesting video in New York then which

00:47:40.740 --> 00:47:40.750
interesting video in New York then which
 

00:47:40.750 --> 00:47:43.620
interesting video in New York then which
people were interviewed on the streets

00:47:43.620 --> 00:47:43.630
people were interviewed on the streets
 

00:47:43.630 --> 00:47:48.240
people were interviewed on the streets
about these things we kind of mark I

00:47:48.240 --> 00:47:48.250
about these things we kind of mark I
 

00:47:48.250 --> 00:47:50.880
about these things we kind of mark I
accept all those terms of conditions of

00:47:50.880 --> 00:47:50.890
accept all those terms of conditions of
 

00:47:50.890 --> 00:47:53.490
accept all those terms of conditions of
iPhones and Facebook's and all of this

00:47:53.490 --> 00:47:53.500
iPhones and Facebook's and all of this
 

00:47:53.500 --> 00:47:57.450
iPhones and Facebook's and all of this
and they were horrible the things we

00:47:57.450 --> 00:47:57.460
and they were horrible the things we
 

00:47:57.460 --> 00:48:00.780
and they were horrible the things we
accept and I was looking at that I mean

00:48:00.780 --> 00:48:00.790
accept and I was looking at that I mean
 

00:48:00.790 --> 00:48:02.760
accept and I was looking at that I mean
I was looking at that and say guess what

00:48:02.760 --> 00:48:02.770
I was looking at that and say guess what
 

00:48:02.770 --> 00:48:04.560
I was looking at that and say guess what
it was a lawyer

00:48:04.560 --> 00:48:04.570
it was a lawyer
 

00:48:04.570 --> 00:48:07.200
it was a lawyer
it was a human who generated that thing

00:48:07.200 --> 00:48:07.210
it was a human who generated that thing
 

00:48:07.210 --> 00:48:10.260
it was a human who generated that thing
and not an AI system imagine if that

00:48:10.260 --> 00:48:10.270
and not an AI system imagine if that
 

00:48:10.270 --> 00:48:13.620
and not an AI system imagine if that
text were generated by a robot or an AI

00:48:13.620 --> 00:48:13.630
text were generated by a robot or an AI
 

00:48:13.630 --> 00:48:16.050
text were generated by a robot or an AI
system which was fooling me into

00:48:16.050 --> 00:48:16.060
system which was fooling me into
 

00:48:16.060 --> 00:48:18.540
system which was fooling me into
accepting all sorts of like horrible

00:48:18.540 --> 00:48:18.550
accepting all sorts of like horrible
 

00:48:18.550 --> 00:48:20.640
accepting all sorts of like horrible
things that we accept we never read that

00:48:20.640 --> 00:48:20.650
things that we accept we never read that
 

00:48:20.650 --> 00:48:22.859
things that we accept we never read that
like pages and pages and pages and

00:48:22.859 --> 00:48:22.869
like pages and pages and pages and
 

00:48:22.869 --> 00:48:25.710
like pages and pages and pages and
whoever generated that was not an AI

00:48:25.710 --> 00:48:25.720
whoever generated that was not an AI
 

00:48:25.720 --> 00:48:29.069
whoever generated that was not an AI
system was a human so shame on the

00:48:29.069 --> 00:48:29.079
system was a human so shame on the
 

00:48:29.079 --> 00:48:32.130
system was a human so shame on the
humans in the sense that that's where

00:48:32.130 --> 00:48:32.140
humans in the sense that that's where
 

00:48:32.140 --> 00:48:34.440
humans in the sense that that's where
like I go back now to what Moshe was

00:48:34.440 --> 00:48:34.450
like I go back now to what Moshe was
 

00:48:34.450 --> 00:48:37.109
like I go back now to what Moshe was
saying but let's educate the humans

00:48:37.109 --> 00:48:37.119
saying but let's educate the humans
 

00:48:37.119 --> 00:48:41.069
saying but let's educate the humans
let's eventually have the humans make

00:48:41.069 --> 00:48:41.079
let's eventually have the humans make
 

00:48:41.079 --> 00:48:43.770
let's eventually have the humans make
these things we accept on in terms of

00:48:43.770 --> 00:48:43.780
these things we accept on in terms of
 

00:48:43.780 --> 00:48:45.780
these things we accept on in terms of
like machinery be trusted

00:48:45.780 --> 00:48:45.790
like machinery be trusted
 

00:48:45.790 --> 00:48:48.660
like machinery be trusted
trusted because this Facebook example

00:48:48.660 --> 00:48:48.670
trusted because this Facebook example
 

00:48:48.670 --> 00:48:50.970
trusted because this Facebook example
yes we didn't accept but how did you

00:48:50.970 --> 00:48:50.980
yes we didn't accept but how did you
 

00:48:50.980 --> 00:48:52.380
yes we didn't accept but how did you
know that in the whole thing that you

00:48:52.380 --> 00:48:52.390
know that in the whole thing that you
 

00:48:52.390 --> 00:48:54.630
know that in the whole thing that you
signed up for you were not accepting

00:48:54.630 --> 00:48:54.640
signed up for you were not accepting
 

00:48:54.640 --> 00:48:56.520
signed up for you were not accepting
that these data would go somewhere else

00:48:56.520 --> 00:48:56.530
that these data would go somewhere else
 

00:48:56.530 --> 00:48:59.130
that these data would go somewhere else
I mean I have no clue you know we just

00:48:59.130 --> 00:48:59.140
I mean I have no clue you know we just
 

00:48:59.140 --> 00:49:01.980
I mean I have no clue you know we just
accept so Humanzee the loop of

00:49:01.980 --> 00:49:01.990
accept so Humanzee the loop of
 

00:49:01.990 --> 00:49:04.829
accept so Humanzee the loop of
technology need to make sure that things

00:49:04.829 --> 00:49:04.839
technology need to make sure that things
 

00:49:04.839 --> 00:49:07.740
technology need to make sure that things
are trusted and humans need to hear make

00:49:07.740 --> 00:49:07.750
are trusted and humans need to hear make
 

00:49:07.750 --> 00:49:09.839
are trusted and humans need to hear make
sure that that is the case I'm sorry for

00:49:09.839 --> 00:49:09.849
sure that that is the case I'm sorry for
 

00:49:09.849 --> 00:49:16.800
sure that that is the case I'm sorry for
my cannon but that's really intimidating

00:49:16.800 --> 00:49:16.810
my cannon but that's really intimidating
 

00:49:16.810 --> 00:49:19.200
my cannon but that's really intimidating
the things we accept in these terms and

00:49:19.200 --> 00:49:19.210
the things we accept in these terms and
 

00:49:19.210 --> 00:49:21.420
the things we accept in these terms and
conditions it's like oh my god what am i

00:49:21.420 --> 00:49:21.430
conditions it's like oh my god what am i
 

00:49:21.430 --> 00:49:24.809
conditions it's like oh my god what am i
signing for and somehow it's still not

00:49:24.809 --> 00:49:24.819
signing for and somehow it's still not
 

00:49:24.819 --> 00:49:27.420
signing for and somehow it's still not
generated by an AI system and I wonder

00:49:27.420 --> 00:49:27.430
generated by an AI system and I wonder
 

00:49:27.430 --> 00:49:29.490
generated by an AI system and I wonder
if it were an AI system generating those

00:49:29.490 --> 00:49:29.500
if it were an AI system generating those
 

00:49:29.500 --> 00:49:31.650
if it were an AI system generating those
conditions if people would not be all

00:49:31.650 --> 00:49:31.660
conditions if people would not be all
 

00:49:31.660 --> 00:49:36.270
conditions if people would not be all
outraged how Carmen AI system told me to

00:49:36.270 --> 00:49:36.280
outraged how Carmen AI system told me to
 

00:49:36.280 --> 00:49:39.510
outraged how Carmen AI system told me to
accept all these conditions I'm telling

00:49:39.510 --> 00:49:39.520
accept all these conditions I'm telling
 

00:49:39.520 --> 00:49:40.980
accept all these conditions I'm telling
you trust in technology rather than

00:49:40.980 --> 00:49:40.990
you trust in technology rather than
 

00:49:40.990 --> 00:49:42.000
you trust in technology rather than
humans

00:49:42.000 --> 00:49:42.010
humans
 

00:49:42.010 --> 00:49:46.210
humans
mosha any final thought I I think we

00:49:46.210 --> 00:49:46.220
mosha any final thought I I think we
 

00:49:46.220 --> 00:49:47.670
mosha any final thought I I think we
really need to think very hard about

00:49:47.670 --> 00:49:47.680
really need to think very hard about
 

00:49:47.680 --> 00:49:50.050
really need to think very hard about
what makes corporations behave

00:49:50.050 --> 00:49:50.060
what makes corporations behave
 

00:49:50.060 --> 00:49:52.180
what makes corporations behave
unethically and it's not the people in

00:49:52.180 --> 00:49:52.190
unethically and it's not the people in
 

00:49:52.190 --> 00:49:54.160
unethically and it's not the people in
them I mean some of the people here from

00:49:54.160 --> 00:49:54.170
them I mean some of the people here from
 

00:49:54.170 --> 00:49:56.920
them I mean some of the people here from
cooperation as we serve someone I spent

00:49:56.920 --> 00:49:56.930
cooperation as we serve someone I spent
 

00:49:56.930 --> 00:49:58.840
cooperation as we serve someone I spent
a big part of my life in a corporation

00:49:58.840 --> 00:49:58.850
a big part of my life in a corporation
 

00:49:58.850 --> 00:50:01.330
a big part of my life in a corporation
many of conferencing cooperation but

00:50:01.330 --> 00:50:01.340
many of conferencing cooperation but
 

00:50:01.340 --> 00:50:02.680
many of conferencing cooperation but
nevertheless there is something our

00:50:02.680 --> 00:50:02.690
nevertheless there is something our
 

00:50:02.690 --> 00:50:04.890
nevertheless there is something our
current economic system that makes

00:50:04.890 --> 00:50:04.900
current economic system that makes
 

00:50:04.900 --> 00:50:07.350
current economic system that makes
corporations behave legally and

00:50:07.350 --> 00:50:07.360
corporations behave legally and
 

00:50:07.360 --> 00:50:10.540
corporations behave legally and
ethically and that means either we need

00:50:10.540 --> 00:50:10.550
ethically and that means either we need
 

00:50:10.550 --> 00:50:12.880
ethically and that means either we need
to change the laws to make more

00:50:12.880 --> 00:50:12.890
to change the laws to make more
 

00:50:12.890 --> 00:50:15.280
to change the laws to make more
behaviors illegal or we need to rethink

00:50:15.280 --> 00:50:15.290
behaviors illegal or we need to rethink
 

00:50:15.290 --> 00:50:16.720
behaviors illegal or we need to rethink
the incentive system under which they

00:50:16.720 --> 00:50:16.730
the incentive system under which they
 

00:50:16.730 --> 00:50:19.240
the incentive system under which they
operate because they are just right now

00:50:19.240 --> 00:50:19.250
operate because they are just right now
 

00:50:19.250 --> 00:50:21.970
operate because they are just right now
in this part of our history so immensely

00:50:21.970 --> 00:50:21.980
in this part of our history so immensely
 

00:50:21.980 --> 00:50:24.520
in this part of our history so immensely
powerful the day behavior has huge

00:50:24.520 --> 00:50:24.530
powerful the day behavior has huge
 

00:50:24.530 --> 00:50:30.310
powerful the day behavior has huge
impact on our lives sometimes I think we

00:50:30.310 --> 00:50:30.320
impact on our lives sometimes I think we
 

00:50:30.320 --> 00:50:31.900
impact on our lives sometimes I think we
get we get caught up in the bright and

00:50:31.900 --> 00:50:31.910
get we get caught up in the bright and
 

00:50:31.910 --> 00:50:34.000
get we get caught up in the bright and
shiny and the possibilities and the

00:50:34.000 --> 00:50:34.010
shiny and the possibilities and the
 

00:50:34.010 --> 00:50:36.370
shiny and the possibilities and the
potentials of new technologies and we

00:50:36.370 --> 00:50:36.380
potentials of new technologies and we
 

00:50:36.380 --> 00:50:39.150
potentials of new technologies and we
forget that these technologies serve us

00:50:39.150 --> 00:50:39.160
forget that these technologies serve us
 

00:50:39.160 --> 00:50:43.210
forget that these technologies serve us
right and so rather than starting always

00:50:43.210 --> 00:50:43.220
right and so rather than starting always
 

00:50:43.220 --> 00:50:46.540
right and so rather than starting always
as a technology centric conversation

00:50:46.540 --> 00:50:46.550
as a technology centric conversation
 

00:50:46.550 --> 00:50:48.490
as a technology centric conversation
which is not only constantly evolving

00:50:48.490 --> 00:50:48.500
which is not only constantly evolving
 

00:50:48.500 --> 00:50:50.290
which is not only constantly evolving
due to innovation but also quite

00:50:50.290 --> 00:50:50.300
due to innovation but also quite
 

00:50:50.300 --> 00:50:52.090
due to innovation but also quite
alienating for a large part of the

00:50:52.090 --> 00:50:52.100
alienating for a large part of the
 

00:50:52.100 --> 00:50:53.740
alienating for a large part of the
population who doesn't have the

00:50:53.740 --> 00:50:53.750
population who doesn't have the
 

00:50:53.750 --> 00:50:55.300
population who doesn't have the
technological sophistication to

00:50:55.300 --> 00:50:55.310
technological sophistication to
 

00:50:55.310 --> 00:50:57.630
technological sophistication to
participate in some of the more deeper

00:50:57.630 --> 00:50:57.640
participate in some of the more deeper
 

00:50:57.640 --> 00:51:00.100
participate in some of the more deeper
conversations about it we need to be

00:51:00.100 --> 00:51:00.110
conversations about it we need to be
 

00:51:00.110 --> 00:51:02.860
conversations about it we need to be
having more human centric conversations

00:51:02.860 --> 00:51:02.870
having more human centric conversations
 

00:51:02.870 --> 00:51:07.690
having more human centric conversations
about how humans employ technology to to

00:51:07.690 --> 00:51:07.700
about how humans employ technology to to
 

00:51:07.700 --> 00:51:09.640
about how humans employ technology to to
meet human objectives unfortunately in

00:51:09.640 --> 00:51:09.650
meet human objectives unfortunately in
 

00:51:09.650 --> 00:51:12.010
meet human objectives unfortunately in
my in my field that's human objectives

00:51:12.010 --> 00:51:12.020
my in my field that's human objectives
 

00:51:12.020 --> 00:51:13.810
my in my field that's human objectives
of conflict and resolving these are the

00:51:13.810 --> 00:51:13.820
of conflict and resolving these are the
 

00:51:13.820 --> 00:51:16.120
of conflict and resolving these are the
tools of violence that humans decide to

00:51:16.120 --> 00:51:16.130
tools of violence that humans decide to
 

00:51:16.130 --> 00:51:19.030
tools of violence that humans decide to
employ but in other fields whether it's

00:51:19.030 --> 00:51:19.040
employ but in other fields whether it's
 

00:51:19.040 --> 00:51:21.040
employ but in other fields whether it's
healthcare education or criminal justice

00:51:21.040 --> 00:51:21.050
healthcare education or criminal justice
 

00:51:21.050 --> 00:51:23.920
healthcare education or criminal justice
these are tools that work for us and we

00:51:23.920 --> 00:51:23.930
these are tools that work for us and we
 

00:51:23.930 --> 00:51:28.300
these are tools that work for us and we
need to can affirm our human values and

00:51:28.300 --> 00:51:28.310
need to can affirm our human values and
 

00:51:28.310 --> 00:51:30.910
need to can affirm our human values and
our objectives and our desires and what

00:51:30.910 --> 00:51:30.920
our objectives and our desires and what
 

00:51:30.920 --> 00:51:33.700
our objectives and our desires and what
levels of trust we're going to demand or

00:51:33.700 --> 00:51:33.710
levels of trust we're going to demand or
 

00:51:33.710 --> 00:51:37.750
levels of trust we're going to demand or
what what metrics we're going to use and

00:51:37.750 --> 00:51:37.760
what what metrics we're going to use and
 

00:51:37.760 --> 00:51:41.710
what what metrics we're going to use and
then ensure that our deployers and our

00:51:41.710 --> 00:51:41.720
then ensure that our deployers and our
 

00:51:41.720 --> 00:51:44.380
then ensure that our deployers and our
developers of technologies meet those

00:51:44.380 --> 00:51:44.390
developers of technologies meet those
 

00:51:44.390 --> 00:51:48.580
developers of technologies meet those
requirements because we're in charge at

00:51:48.580 --> 00:51:48.590
requirements because we're in charge at
 

00:51:48.590 --> 00:51:52.870
requirements because we're in charge at
least for now nominally so with that

00:51:52.870 --> 00:51:52.880
least for now nominally so with that
 

00:51:52.880 --> 00:51:55.010
least for now nominally so with that
call to action let's thank our

00:51:55.010 --> 00:51:55.020
call to action let's thank our
 

00:51:55.020 --> 00:52:04.730
call to action let's thank our
panelists conversations so we're now

00:52:04.730 --> 00:52:04.740
panelists conversations so we're now
 

00:52:04.740 --> 00:52:10.280
panelists conversations so we're now
going to shift to one last part of our

00:52:10.280 --> 00:52:10.290
going to shift to one last part of our
 

00:52:10.290 --> 00:52:13.010
going to shift to one last part of our
session which is please leave the

00:52:13.010 --> 00:52:13.020
session which is please leave the
 

00:52:13.020 --> 00:52:14.450
session which is please leave the
microphone sorry

00:52:14.450 --> 00:52:14.460
microphone sorry
 

00:52:14.460 --> 00:52:16.700
microphone sorry
that we'll get back up there eventually

00:52:16.700 --> 00:52:16.710
that we'll get back up there eventually
 

00:52:16.710 --> 00:52:19.520
that we'll get back up there eventually
which is a final spotlight talk by

00:52:19.520 --> 00:52:19.530
which is a final spotlight talk by
 

00:52:19.530 --> 00:52:22.220
which is a final spotlight talk by
Professor Anka dragon who is an

00:52:22.220 --> 00:52:22.230
Professor Anka dragon who is an
 

00:52:22.230 --> 00:52:23.570
Professor Anka dragon who is an
assistant professor in electrical

00:52:23.570 --> 00:52:23.580
assistant professor in electrical
 

00:52:23.580 --> 00:52:25.430
assistant professor in electrical
engineering and computer science and UC

00:52:25.430 --> 00:52:25.440
engineering and computer science and UC
 

00:52:25.440 --> 00:52:29.420
engineering and computer science and UC
Berkeley and a I believe proud at least

00:52:29.420 --> 00:52:29.430
Berkeley and a I believe proud at least
 

00:52:29.430 --> 00:52:32.210
Berkeley and a I believe proud at least
an alumna got her PhD from here at

00:52:32.210 --> 00:52:32.220
an alumna got her PhD from here at
 

00:52:32.220 --> 00:52:33.560
an alumna got her PhD from here at
Carnegie Mellon and I think is proud of

00:52:33.560 --> 00:52:33.570
Carnegie Mellon and I think is proud of
 

00:52:33.570 --> 00:52:37.040
Carnegie Mellon and I think is proud of
that fact I hope and she at Berkeley

00:52:37.040 --> 00:52:37.050
that fact I hope and she at Berkeley
 

00:52:37.050 --> 00:52:39.080
that fact I hope and she at Berkeley
runs the interactive autonomy and

00:52:39.080 --> 00:52:39.090
runs the interactive autonomy and
 

00:52:39.090 --> 00:52:41.060
runs the interactive autonomy and
collaborative technologies or interact

00:52:41.060 --> 00:52:41.070
collaborative technologies or interact
 

00:52:41.070 --> 00:52:43.250
collaborative technologies or interact
lab which is really focused on a lot of

00:52:43.250 --> 00:52:43.260
lab which is really focused on a lot of
 

00:52:43.260 --> 00:52:45.080
lab which is really focused on a lot of
these questions about human and human

00:52:45.080 --> 00:52:45.090
these questions about human and human
 

00:52:45.090 --> 00:52:48.470
these questions about human and human
robot interaction how do we have robots

00:52:48.470 --> 00:52:48.480
robot interaction how do we have robots
 

00:52:48.480 --> 00:52:50.870
robot interaction how do we have robots
that actually are able to interact with

00:52:50.870 --> 00:52:50.880
that actually are able to interact with
 

00:52:50.880 --> 00:52:53.270
that actually are able to interact with
humans in ways that can support and

00:52:53.270 --> 00:52:53.280
humans in ways that can support and
 

00:52:53.280 --> 00:52:56.450
humans in ways that can support and
empower both sides which requires doing

00:52:56.450 --> 00:52:56.460
empower both sides which requires doing
 

00:52:56.460 --> 00:52:58.310
empower both sides which requires doing
research really at the intersections of

00:52:58.310 --> 00:52:58.320
research really at the intersections of
 

00:52:58.320 --> 00:53:00.530
research really at the intersections of
robotics machine learning and cognitive

00:53:00.530 --> 00:53:00.540
robotics machine learning and cognitive
 

00:53:00.540 --> 00:53:02.660
robotics machine learning and cognitive
science she's also helped to found the

00:53:02.660 --> 00:53:02.670
science she's also helped to found the
 

00:53:02.670 --> 00:53:05.540
science she's also helped to found the
Berkeley AI Research Lab is a COPI I of

00:53:05.540 --> 00:53:05.550
Berkeley AI Research Lab is a COPI I of
 

00:53:05.550 --> 00:53:07.670
Berkeley AI Research Lab is a COPI I of
the Center for human compatible AI and

00:53:07.670 --> 00:53:07.680
the Center for human compatible AI and
 

00:53:07.680 --> 00:53:10.790
the Center for human compatible AI and
has won multiple early career awards and

00:53:10.790 --> 00:53:10.800
has won multiple early career awards and
 

00:53:10.800 --> 00:53:12.230
has won multiple early career awards and
were delighted to welcome her about to

00:53:12.230 --> 00:53:12.240
were delighted to welcome her about to
 

00:53:12.240 --> 00:53:18.360
were delighted to welcome her about to
Pittsburgh on Kjartan

00:53:18.360 --> 00:53:18.370
 
 

00:53:18.370 --> 00:53:20.890
 
hi everyone pleasure to be here I am

00:53:20.890 --> 00:53:20.900
hi everyone pleasure to be here I am
 

00:53:20.900 --> 00:53:23.950
hi everyone pleasure to be here I am
going to basically agree with everything

00:53:23.950 --> 00:53:23.960
going to basically agree with everything
 

00:53:23.960 --> 00:53:25.750
going to basically agree with everything
that Manuel I said and maybe add one

00:53:25.750 --> 00:53:25.760
that Manuel I said and maybe add one
 

00:53:25.760 --> 00:53:30.430
that Manuel I said and maybe add one
twist so robots take actions that change

00:53:30.430 --> 00:53:30.440
twist so robots take actions that change
 

00:53:30.440 --> 00:53:32.020
twist so robots take actions that change
the state of the world and as we've

00:53:32.020 --> 00:53:32.030
the state of the world and as we've
 

00:53:32.030 --> 00:53:34.750
the state of the world and as we've
heard yesterday what the way they choose

00:53:34.750 --> 00:53:34.760
heard yesterday what the way they choose
 

00:53:34.760 --> 00:53:36.730
heard yesterday what the way they choose
the actions that take is usually via

00:53:36.730 --> 00:53:36.740
the actions that take is usually via
 

00:53:36.740 --> 00:53:38.410
the actions that take is usually via
something called a reward function or

00:53:38.410 --> 00:53:38.420
something called a reward function or
 

00:53:38.420 --> 00:53:40.270
something called a reward function or
objective function so they plan actions

00:53:40.270 --> 00:53:40.280
objective function so they plan actions
 

00:53:40.280 --> 00:53:43.710
objective function so they plan actions
that in expectation maximize cumulative

00:53:43.710 --> 00:53:43.720
that in expectation maximize cumulative
 

00:53:43.720 --> 00:53:46.810
that in expectation maximize cumulative
reward I think when things can go wrong

00:53:46.810 --> 00:53:46.820
reward I think when things can go wrong
 

00:53:46.820 --> 00:53:50.830
reward I think when things can go wrong
is when people have mental models of

00:53:50.830 --> 00:53:50.840
is when people have mental models of
 

00:53:50.840 --> 00:53:53.230
is when people have mental models of
these robots they do not match up with

00:53:53.230 --> 00:53:53.240
these robots they do not match up with
 

00:53:53.240 --> 00:53:57.250
these robots they do not match up with
reality and so I've been so that for

00:53:57.250 --> 00:53:57.260
reality and so I've been so that for
 

00:53:57.260 --> 00:53:59.350
reality and so I've been so that for
instance the robot is much less capable

00:53:59.350 --> 00:53:59.360
instance the robot is much less capable
 

00:53:59.360 --> 00:54:00.940
instance the robot is much less capable
than what the person assumes or it

00:54:00.940 --> 00:54:00.950
than what the person assumes or it
 

00:54:00.950 --> 00:54:02.950
than what the person assumes or it
doesn't take the actions that the person

00:54:02.950 --> 00:54:02.960
doesn't take the actions that the person
 

00:54:02.960 --> 00:54:05.650
doesn't take the actions that the person
thinks that the robot will take and so

00:54:05.650 --> 00:54:05.660
thinks that the robot will take and so
 

00:54:05.660 --> 00:54:07.930
thinks that the robot will take and so
I've been asked to talk about trust but

00:54:07.930 --> 00:54:07.940
I've been asked to talk about trust but
 

00:54:07.940 --> 00:54:10.000
I've been asked to talk about trust but
what I really want to talk about is not

00:54:10.000 --> 00:54:10.010
what I really want to talk about is not
 

00:54:10.010 --> 00:54:12.220
what I really want to talk about is not
so much how do we get people to trust

00:54:12.220 --> 00:54:12.230
so much how do we get people to trust
 

00:54:12.230 --> 00:54:15.220
so much how do we get people to trust
these systems but rather how do we

00:54:15.220 --> 00:54:15.230
these systems but rather how do we
 

00:54:15.230 --> 00:54:17.920
these systems but rather how do we
ensure alignment between the person's

00:54:17.920 --> 00:54:17.930
ensure alignment between the person's
 

00:54:17.930 --> 00:54:20.620
ensure alignment between the person's
mental model and reality and I think

00:54:20.620 --> 00:54:20.630
mental model and reality and I think
 

00:54:20.630 --> 00:54:22.180
mental model and reality and I think
that there's two complimentary ways to

00:54:22.180 --> 00:54:22.190
that there's two complimentary ways to
 

00:54:22.190 --> 00:54:24.610
that there's two complimentary ways to
talk about doing that on the one hand we

00:54:24.610 --> 00:54:24.620
talk about doing that on the one hand we
 

00:54:24.620 --> 00:54:27.190
talk about doing that on the one hand we
can work on the robot side and try to

00:54:27.190 --> 00:54:27.200
can work on the robot side and try to
 

00:54:27.200 --> 00:54:29.920
can work on the robot side and try to
bring the robot up to match to live up

00:54:29.920 --> 00:54:29.930
bring the robot up to match to live up
 

00:54:29.930 --> 00:54:31.840
bring the robot up to match to live up
to our expectations and I would call

00:54:31.840 --> 00:54:31.850
to our expectations and I would call
 

00:54:31.850 --> 00:54:33.700
to our expectations and I would call
that making it trust able or trustworthy

00:54:33.700 --> 00:54:33.710
that making it trust able or trustworthy
 

00:54:33.710 --> 00:54:37.030
that making it trust able or trustworthy
on the other hand we could work on the

00:54:37.030 --> 00:54:37.040
on the other hand we could work on the
 

00:54:37.040 --> 00:54:39.640
on the other hand we could work on the
human side and bring expectations and

00:54:39.640 --> 00:54:39.650
human side and bring expectations and
 

00:54:39.650 --> 00:54:43.510
human side and bring expectations and
calibrate them right to reality my

00:54:43.510 --> 00:54:43.520
calibrate them right to reality my
 

00:54:43.520 --> 00:54:45.070
calibrate them right to reality my
background my personal background is

00:54:45.070 --> 00:54:45.080
background my personal background is
 

00:54:45.080 --> 00:54:49.330
background my personal background is
actually in transparency so I don't want

00:54:49.330 --> 00:54:49.340
actually in transparency so I don't want
 

00:54:49.340 --> 00:54:50.800
actually in transparency so I don't want
to focus so much on transparency today

00:54:50.800 --> 00:54:50.810
to focus so much on transparency today
 

00:54:50.810 --> 00:54:51.970
to focus so much on transparency today
but I do want to talk about it for a

00:54:51.970 --> 00:54:51.980
but I do want to talk about it for a
 

00:54:51.980 --> 00:54:53.320
but I do want to talk about it for a
couple minutes to just share some

00:54:53.320 --> 00:54:53.330
couple minutes to just share some
 

00:54:53.330 --> 00:54:56.410
couple minutes to just share some
highlights so if we talk about robots

00:54:56.410 --> 00:54:56.420
highlights so if we talk about robots
 

00:54:56.420 --> 00:54:59.080
highlights so if we talk about robots
being transparent we have to actually

00:54:59.080 --> 00:54:59.090
being transparent we have to actually
 

00:54:59.090 --> 00:55:02.320
being transparent we have to actually
define what that means and I'm gonna

00:55:02.320 --> 00:55:02.330
define what that means and I'm gonna
 

00:55:02.330 --> 00:55:04.060
define what that means and I'm gonna
propose one definition which is what

00:55:04.060 --> 00:55:04.070
propose one definition which is what
 

00:55:04.070 --> 00:55:05.440
propose one definition which is what
I've been using I don't think it's the

00:55:05.440 --> 00:55:05.450
I've been using I don't think it's the
 

00:55:05.450 --> 00:55:06.700
I've been using I don't think it's the
only definition but the way I've been

00:55:06.700 --> 00:55:06.710
only definition but the way I've been
 

00:55:06.710 --> 00:55:08.560
only definition but the way I've been
thinking about the problem is that

00:55:08.560 --> 00:55:08.570
thinking about the problem is that
 

00:55:08.570 --> 00:55:12.250
thinking about the problem is that
there's some internal state to the robot

00:55:12.250 --> 00:55:12.260
there's some internal state to the robot
 

00:55:12.260 --> 00:55:15.250
there's some internal state to the robot
like its capability its utility its what

00:55:15.250 --> 00:55:15.260
like its capability its utility its what
 

00:55:15.260 --> 00:55:17.590
like its capability its utility its what
it plans on doing and the human doesn't

00:55:17.590 --> 00:55:17.600
it plans on doing and the human doesn't
 

00:55:17.600 --> 00:55:20.530
it plans on doing and the human doesn't
get to know that but what the human can

00:55:20.530 --> 00:55:20.540
get to know that but what the human can
 

00:55:20.540 --> 00:55:22.510
get to know that but what the human can
have is some sort of estimate or belief

00:55:22.510 --> 00:55:22.520
have is some sort of estimate or belief
 

00:55:22.520 --> 00:55:24.730
have is some sort of estimate or belief
in what that is and that's the robot

00:55:24.730 --> 00:55:24.740
in what that is and that's the robot
 

00:55:24.740 --> 00:55:28.030
in what that is and that's the robot
takes actions in the world and these

00:55:28.030 --> 00:55:28.040
takes actions in the world and these
 

00:55:28.040 --> 00:55:29.590
takes actions in the world and these
actions could be physical actions or

00:55:29.590 --> 00:55:29.600
actions could be physical actions or
 

00:55:29.600 --> 00:55:30.589
actions could be physical actions or
explanation

00:55:30.589 --> 00:55:30.599
explanation
 

00:55:30.599 --> 00:55:33.440
explanation
that ends up changing the person's

00:55:33.440 --> 00:55:33.450
that ends up changing the person's
 

00:55:33.450 --> 00:55:35.210
that ends up changing the person's
estimate the person gets to observe that

00:55:35.210 --> 00:55:35.220
estimate the person gets to observe that
 

00:55:35.220 --> 00:55:37.040
estimate the person gets to observe that
and adjust their mental model over time

00:55:37.040 --> 00:55:37.050
and adjust their mental model over time
 

00:55:37.050 --> 00:55:39.530
and adjust their mental model over time
and so I believe that we have an

00:55:39.530 --> 00:55:39.540
and so I believe that we have an
 

00:55:39.540 --> 00:55:41.690
and so I believe that we have an
opportunity here for the robot to not

00:55:41.690 --> 00:55:41.700
opportunity here for the robot to not
 

00:55:41.700 --> 00:55:44.540
opportunity here for the robot to not
just maximize the reward that's been

00:55:44.540 --> 00:55:44.550
just maximize the reward that's been
 

00:55:44.550 --> 00:55:47.450
just maximize the reward that's been
given to achieve the functional task but

00:55:47.450 --> 00:55:47.460
given to achieve the functional task but
 

00:55:47.460 --> 00:55:50.750
given to achieve the functional task but
also reason about how its actions are

00:55:50.750 --> 00:55:50.760
also reason about how its actions are
 

00:55:50.760 --> 00:55:53.420
also reason about how its actions are
affecting the person's belief and try to

00:55:53.420 --> 00:55:53.430
affecting the person's belief and try to
 

00:55:53.430 --> 00:55:56.120
affecting the person's belief and try to
come up with actions that are that

00:55:56.120 --> 00:55:56.130
come up with actions that are that
 

00:55:56.130 --> 00:55:58.460
come up with actions that are that
steered the person's belief towards the

00:55:58.460 --> 00:55:58.470
steered the person's belief towards the
 

00:55:58.470 --> 00:56:01.010
steered the person's belief towards the
correct mental model towards reality and

00:56:01.010 --> 00:56:01.020
correct mental model towards reality and
 

00:56:01.020 --> 00:56:03.140
correct mental model towards reality and
that to me is what transparency means

00:56:03.140 --> 00:56:03.150
that to me is what transparency means
 

00:56:03.150 --> 00:56:05.540
that to me is what transparency means
and before I move on I just want to give

00:56:05.540 --> 00:56:05.550
and before I move on I just want to give
 

00:56:05.550 --> 00:56:07.310
and before I move on I just want to give
you a few examples of what that might

00:56:07.310 --> 00:56:07.320
you a few examples of what that might
 

00:56:07.320 --> 00:56:11.000
you a few examples of what that might
look like so here's a robot that is

00:56:11.000 --> 00:56:11.010
look like so here's a robot that is
 

00:56:11.010 --> 00:56:13.160
look like so here's a robot that is
planning to grab one of these two

00:56:13.160 --> 00:56:13.170
planning to grab one of these two
 

00:56:13.170 --> 00:56:14.510
planning to grab one of these two
bottles the one on the right and it

00:56:14.510 --> 00:56:14.520
bottles the one on the right and it
 

00:56:14.520 --> 00:56:16.250
bottles the one on the right and it
could do it pretty efficiently but then

00:56:16.250 --> 00:56:16.260
could do it pretty efficiently but then
 

00:56:16.260 --> 00:56:17.810
could do it pretty efficiently but then
it'd be kind of confusing to a person

00:56:17.810 --> 00:56:17.820
it'd be kind of confusing to a person
 

00:56:17.820 --> 00:56:19.670
it'd be kind of confusing to a person
observing which bottle the robot is

00:56:19.670 --> 00:56:19.680
observing which bottle the robot is
 

00:56:19.680 --> 00:56:21.349
observing which bottle the robot is
going for so what this robot is figuring

00:56:21.349 --> 00:56:21.359
going for so what this robot is figuring
 

00:56:21.359 --> 00:56:24.230
going for so what this robot is figuring
out is that by exaggerating its motion

00:56:24.230 --> 00:56:24.240
out is that by exaggerating its motion
 

00:56:24.240 --> 00:56:26.839
out is that by exaggerating its motion
to the right it can become more clear to

00:56:26.839 --> 00:56:26.849
to the right it can become more clear to
 

00:56:26.849 --> 00:56:29.720
to the right it can become more clear to
a person observing this motion what it

00:56:29.720 --> 00:56:29.730
a person observing this motion what it
 

00:56:29.730 --> 00:56:31.339
a person observing this motion what it
is that it's about to do it's not the

00:56:31.339 --> 00:56:31.349
is that it's about to do it's not the
 

00:56:31.349 --> 00:56:33.020
is that it's about to do it's not the
most efficient way but it's a way that

00:56:33.020 --> 00:56:33.030
most efficient way but it's a way that
 

00:56:33.030 --> 00:56:35.270
most efficient way but it's a way that
increases transparency by carefully

00:56:35.270 --> 00:56:35.280
increases transparency by carefully
 

00:56:35.280 --> 00:56:37.220
increases transparency by carefully
selecting actions that take the person's

00:56:37.220 --> 00:56:37.230
selecting actions that take the person's
 

00:56:37.230 --> 00:56:39.620
selecting actions that take the person's
belief towards the object on the right

00:56:39.620 --> 00:56:39.630
belief towards the object on the right
 

00:56:39.630 --> 00:56:41.120
belief towards the object on the right
could I go that was my thesis at

00:56:41.120 --> 00:56:41.130
could I go that was my thesis at
 

00:56:41.130 --> 00:56:43.069
could I go that was my thesis at
Carnegie Mellon here we call this

00:56:43.069 --> 00:56:43.079
Carnegie Mellon here we call this
 

00:56:43.079 --> 00:56:44.780
Carnegie Mellon here we call this
legible motion in the meantime we've

00:56:44.780 --> 00:56:44.790
legible motion in the meantime we've
 

00:56:44.790 --> 00:56:46.339
legible motion in the meantime we've
generalized this to a few things

00:56:46.339 --> 00:56:46.349
generalized this to a few things
 

00:56:46.349 --> 00:56:49.190
generalized this to a few things
so it utilities for instance here's an

00:56:49.190 --> 00:56:49.200
so it utilities for instance here's an
 

00:56:49.200 --> 00:56:51.800
so it utilities for instance here's an
autonomous car sort of placing the

00:56:51.800 --> 00:56:51.810
autonomous car sort of placing the
 

00:56:51.810 --> 00:56:54.020
autonomous car sort of placing the
passenger in a situation and showing in

00:56:54.020 --> 00:56:54.030
passenger in a situation and showing in
 

00:56:54.030 --> 00:56:55.760
passenger in a situation and showing in
the situation I would cut this other guy

00:56:55.760 --> 00:56:55.770
the situation I would cut this other guy
 

00:56:55.770 --> 00:56:57.800
the situation I would cut this other guy
off to convey to the passengers that

00:56:57.800 --> 00:56:57.810
off to convey to the passengers that
 

00:56:57.810 --> 00:56:59.000
off to convey to the passengers that
look my driving style is a little bit

00:56:59.000 --> 00:56:59.010
look my driving style is a little bit
 

00:56:59.010 --> 00:57:01.609
look my driving style is a little bit
more on the aggressive side I do believe

00:57:01.609 --> 00:57:01.619
more on the aggressive side I do believe
 

00:57:01.619 --> 00:57:04.339
more on the aggressive side I do believe
we have to calibrate expectations about

00:57:04.339 --> 00:57:04.349
we have to calibrate expectations about
 

00:57:04.349 --> 00:57:07.250
we have to calibrate expectations about
capability a lot and so because people

00:57:07.250 --> 00:57:07.260
capability a lot and so because people
 

00:57:07.260 --> 00:57:09.710
capability a lot and so because people
tend to over assume so what we've been

00:57:09.710 --> 00:57:09.720
tend to over assume so what we've been
 

00:57:09.720 --> 00:57:11.599
tend to over assume so what we've been
working on is also these robots that

00:57:11.599 --> 00:57:11.609
working on is also these robots that
 

00:57:11.609 --> 00:57:15.319
working on is also these robots that
fail and show you the failures and show

00:57:15.319 --> 00:57:15.329
fail and show you the failures and show
 

00:57:15.329 --> 00:57:18.260
fail and show you the failures and show
them expressively so a robot that says

00:57:18.260 --> 00:57:18.270
them expressively so a robot that says
 

00:57:18.270 --> 00:57:19.970
them expressively so a robot that says
here's what I'm trying to do and it's

00:57:19.970 --> 00:57:19.980
here's what I'm trying to do and it's
 

00:57:19.980 --> 00:57:22.010
here's what I'm trying to do and it's
not working and and here's why it's not

00:57:22.010 --> 00:57:22.020
not working and and here's why it's not
 

00:57:22.020 --> 00:57:23.510
not working and and here's why it's not
working I'm trying to pull on this thing

00:57:23.510 --> 00:57:23.520
working I'm trying to pull on this thing
 

00:57:23.520 --> 00:57:24.770
working I'm trying to pull on this thing
but it's locked I'm trying to push this

00:57:24.770 --> 00:57:24.780
but it's locked I'm trying to push this
 

00:57:24.780 --> 00:57:26.089
but it's locked I'm trying to push this
thing but I can't do it I'm trying to

00:57:26.089 --> 00:57:26.099
thing but I can't do it I'm trying to
 

00:57:26.099 --> 00:57:28.460
thing but I can't do it I'm trying to
lift this Cup but it's too heavy

00:57:28.460 --> 00:57:28.470
lift this Cup but it's too heavy
 

00:57:28.470 --> 00:57:32.829
lift this Cup but it's too heavy
and then the final example is one of a

00:57:32.829 --> 00:57:32.839
and then the final example is one of a
 

00:57:32.839 --> 00:57:36.260
and then the final example is one of a
agent that plays pong trained by a deep

00:57:36.260 --> 00:57:36.270
agent that plays pong trained by a deep
 

00:57:36.270 --> 00:57:37.609
agent that plays pong trained by a deep
reinforcement learning which is all the

00:57:37.609 --> 00:57:37.619
reinforcement learning which is all the
 

00:57:37.619 --> 00:57:40.250
reinforcement learning which is all the
rage these days what the agent is doing

00:57:40.250 --> 00:57:40.260
rage these days what the agent is doing
 

00:57:40.260 --> 00:57:43.420
rage these days what the agent is doing
here is it's showing us

00:57:43.420 --> 00:57:43.430
here is it's showing us
 

00:57:43.430 --> 00:57:46.120
here is it's showing us
how it would act in states that it

00:57:46.120 --> 00:57:46.130
how it would act in states that it
 

00:57:46.130 --> 00:57:48.940
how it would act in states that it
thinks are critical meaning states where

00:57:48.940 --> 00:57:48.950
thinks are critical meaning states where
 

00:57:48.950 --> 00:57:50.740
thinks are critical meaning states where
it thinks it's really important to take

00:57:50.740 --> 00:57:50.750
it thinks it's really important to take
 

00:57:50.750 --> 00:57:53.920
it thinks it's really important to take
the action that it's about to take so 4

00:57:53.920 --> 00:57:53.930
the action that it's about to take so 4
 

00:57:53.930 --> 00:57:56.170
the action that it's about to take so 4
pong this means when the ball is coming

00:57:56.170 --> 00:57:56.180
pong this means when the ball is coming
 

00:57:56.180 --> 00:57:58.630
pong this means when the ball is coming
close to you and it's at a high speed

00:57:58.630 --> 00:57:58.640
close to you and it's at a high speed
 

00:57:58.640 --> 00:58:00.579
close to you and it's at a high speed
then it's really important to move that

00:58:00.579 --> 00:58:00.589
then it's really important to move that
 

00:58:00.589 --> 00:58:02.950
then it's really important to move that
pellet upward down and so the agent is

00:58:02.950 --> 00:58:02.960
pellet upward down and so the agent is
 

00:58:02.960 --> 00:58:05.440
pellet upward down and so the agent is
showing you this so that you can build

00:58:05.440 --> 00:58:05.450
showing you this so that you can build
 

00:58:05.450 --> 00:58:07.270
showing you this so that you can build
this mental model of what it is that it

00:58:07.270 --> 00:58:07.280
this mental model of what it is that it
 

00:58:07.280 --> 00:58:10.960
this mental model of what it is that it
actually has learned so in a nutshell we

00:58:10.960 --> 00:58:10.970
actually has learned so in a nutshell we
 

00:58:10.970 --> 00:58:15.250
actually has learned so in a nutshell we
can enable transparency by being careful

00:58:15.250 --> 00:58:15.260
can enable transparency by being careful
 

00:58:15.260 --> 00:58:18.700
can enable transparency by being careful
about steering the person's belief in

00:58:18.700 --> 00:58:18.710
about steering the person's belief in
 

00:58:18.710 --> 00:58:21.849
about steering the person's belief in
the right direction but after working on

00:58:21.849 --> 00:58:21.859
the right direction but after working on
 

00:58:21.859 --> 00:58:23.470
the right direction but after working on
transparency for a bit I became a little

00:58:23.470 --> 00:58:23.480
transparency for a bit I became a little
 

00:58:23.480 --> 00:58:26.200
transparency for a bit I became a little
bit disappointed with it because what

00:58:26.200 --> 00:58:26.210
bit disappointed with it because what
 

00:58:26.210 --> 00:58:28.510
bit disappointed with it because what
transparency is about is calibrating the

00:58:28.510 --> 00:58:28.520
transparency is about is calibrating the
 

00:58:28.520 --> 00:58:30.339
transparency is about is calibrating the
person's expectation it's not working on

00:58:30.339 --> 00:58:30.349
person's expectation it's not working on
 

00:58:30.349 --> 00:58:33.069
person's expectation it's not working on
the person and wouldn't it be nice if we

00:58:33.069 --> 00:58:33.079
the person and wouldn't it be nice if we
 

00:58:33.079 --> 00:58:35.680
the person and wouldn't it be nice if we
could do some work on the robot side to

00:58:35.680 --> 00:58:35.690
could do some work on the robot side to
 

00:58:35.690 --> 00:58:38.559
could do some work on the robot side to
get the robot to live up to the person's

00:58:38.559 --> 00:58:38.569
get the robot to live up to the person's
 

00:58:38.569 --> 00:58:40.660
get the robot to live up to the person's
expectation as opposed to just bring the

00:58:40.660 --> 00:58:40.670
expectation as opposed to just bring the
 

00:58:40.670 --> 00:58:43.809
expectation as opposed to just bring the
person's expectations down in a lot of

00:58:43.809 --> 00:58:43.819
person's expectations down in a lot of
 

00:58:43.819 --> 00:58:45.849
person's expectations down in a lot of
ways this is what artificial

00:58:45.849 --> 00:58:45.859
ways this is what artificial
 

00:58:45.859 --> 00:58:47.260
ways this is what artificial
intelligence is all about

00:58:47.260 --> 00:58:47.270
intelligence is all about
 

00:58:47.270 --> 00:58:49.539
intelligence is all about
most of the work in AI is about making

00:58:49.539 --> 00:58:49.549
most of the work in AI is about making
 

00:58:49.549 --> 00:58:51.280
most of the work in AI is about making
robots most trustworthy making robots

00:58:51.280 --> 00:58:51.290
robots most trustworthy making robots
 

00:58:51.290 --> 00:58:54.270
robots most trustworthy making robots
actually more capable of doing the thing

00:58:54.270 --> 00:58:54.280
actually more capable of doing the thing
 

00:58:54.280 --> 00:58:58.930
actually more capable of doing the thing
typically that means getting robots to

00:58:58.930 --> 00:58:58.940
typically that means getting robots to
 

00:58:58.940 --> 00:59:01.390
typically that means getting robots to
solve this optimization problem how do

00:59:01.390 --> 00:59:01.400
solve this optimization problem how do
 

00:59:01.400 --> 00:59:03.250
solve this optimization problem how do
you actually optimize the reward that

00:59:03.250 --> 00:59:03.260
you actually optimize the reward that
 

00:59:03.260 --> 00:59:05.829
you actually optimize the reward that
it's been given to you but what I've

00:59:05.829 --> 00:59:05.839
it's been given to you but what I've
 

00:59:05.839 --> 00:59:08.079
it's been given to you but what I've
been finding is that even though this is

00:59:08.079 --> 00:59:08.089
been finding is that even though this is
 

00:59:08.089 --> 00:59:11.829
been finding is that even though this is
one modality of increasing capability

00:59:11.829 --> 00:59:11.839
one modality of increasing capability
 

00:59:11.839 --> 00:59:13.839
one modality of increasing capability
there's another thing that can go wrong

00:59:13.839 --> 00:59:13.849
there's another thing that can go wrong
 

00:59:13.849 --> 00:59:18.990
there's another thing that can go wrong
which is the reward function itself so

00:59:18.990 --> 00:59:19.000
which is the reward function itself so
 

00:59:19.000 --> 00:59:22.420
which is the reward function itself so
sometimes the reward function does not

00:59:22.420 --> 00:59:22.430
sometimes the reward function does not
 

00:59:22.430 --> 00:59:25.599
sometimes the reward function does not
actually correspond to what we actually

00:59:25.599 --> 00:59:25.609
actually correspond to what we actually
 

00:59:25.609 --> 00:59:28.539
actually correspond to what we actually
want the robot to be optimizing for and

00:59:28.539 --> 00:59:28.549
want the robot to be optimizing for and
 

00:59:28.549 --> 00:59:30.760
want the robot to be optimizing for and
it makes sense because the people who

00:59:30.760 --> 00:59:30.770
it makes sense because the people who
 

00:59:30.770 --> 00:59:32.589
it makes sense because the people who
are building these robots were human too

00:59:32.589 --> 00:59:32.599
are building these robots were human too
 

00:59:32.599 --> 00:59:35.230
are building these robots were human too
right and more fallible and we don't

00:59:35.230 --> 00:59:35.240
right and more fallible and we don't
 

00:59:35.240 --> 00:59:37.450
right and more fallible and we don't
always encode the right objectives or

00:59:37.450 --> 00:59:37.460
always encode the right objectives or
 

00:59:37.460 --> 00:59:39.579
always encode the right objectives or
incentives in the robot here's a cute

00:59:39.579 --> 00:59:39.589
incentives in the robot here's a cute
 

00:59:39.589 --> 00:59:42.039
incentives in the robot here's a cute
example of this going wrong this is

00:59:42.039 --> 00:59:42.049
example of this going wrong this is
 

00:59:42.049 --> 00:59:44.799
example of this going wrong this is
video from open AI and this is a boat

00:59:44.799 --> 00:59:44.809
video from open AI and this is a boat
 

00:59:44.809 --> 00:59:47.620
video from open AI and this is a boat
racing agent you can't tell that it's

00:59:47.620 --> 00:59:47.630
racing agent you can't tell that it's
 

00:59:47.630 --> 00:59:50.260
racing agent you can't tell that it's
trying to race by looking at it the

00:59:50.260 --> 00:59:50.270
trying to race by looking at it the
 

00:59:50.270 --> 00:59:52.809
trying to race by looking at it the
reason is that the reward function was

00:59:52.809 --> 00:59:52.819
reason is that the reward function was
 

00:59:52.819 --> 00:59:55.940
reason is that the reward function was
score points in the game and so

00:59:55.940 --> 00:59:55.950
score points in the game and so
 

00:59:55.950 --> 00:59:58.010
score points in the game and so
while on the top left there you see all

00:59:58.010 --> 00:59:58.020
while on the top left there you see all
 

00:59:58.020 --> 00:59:59.900
while on the top left there you see all
the other agents actually completing the

00:59:59.900 --> 00:59:59.910
the other agents actually completing the
 

00:59:59.910 --> 01:00:03.109
the other agents actually completing the
race what our boat does is it figures

01:00:03.109 --> 01:00:03.119
race what our boat does is it figures
 

01:00:03.119 --> 01:00:05.960
race what our boat does is it figures
out this loop that it can do and time it

01:00:05.960 --> 01:00:05.970
out this loop that it can do and time it
 

01:00:05.970 --> 01:00:08.150
out this loop that it can do and time it
just right so that it collects all these

01:00:08.150 --> 01:00:08.160
just right so that it collects all these
 

01:00:08.160 --> 01:00:11.329
just right so that it collects all these
turbo boosters at the right time and get

01:00:11.329 --> 01:00:11.339
turbo boosters at the right time and get
 

01:00:11.339 --> 01:00:14.030
turbo boosters at the right time and get
a gazillion points okay so you look at

01:00:14.030 --> 01:00:14.040
a gazillion points okay so you look at
 

01:00:14.040 --> 01:00:16.220
a gazillion points okay so you look at
this as designer they're like oh my god

01:00:16.220 --> 01:00:16.230
this as designer they're like oh my god
 

01:00:16.230 --> 01:00:17.690
this as designer they're like oh my god
of course this is not what I wanted of

01:00:17.690 --> 01:00:17.700
of course this is not what I wanted of
 

01:00:17.700 --> 01:00:18.829
of course this is not what I wanted of
course I wanted you to actually

01:00:18.829 --> 01:00:18.839
course I wanted you to actually
 

01:00:18.839 --> 01:00:20.540
course I wanted you to actually
participate in the race and try to win

01:00:20.540 --> 01:00:20.550
participate in the race and try to win
 

01:00:20.550 --> 01:00:23.510
participate in the race and try to win
but that's not what we specified this is

01:00:23.510 --> 01:00:23.520
but that's not what we specified this is
 

01:00:23.520 --> 01:00:26.210
but that's not what we specified this is
not a new problem Steve Russell and

01:00:26.210 --> 01:00:26.220
not a new problem Steve Russell and
 

01:00:26.220 --> 01:00:28.099
not a new problem Steve Russell and
Peter Norvig if this example and their

01:00:28.099 --> 01:00:28.109
Peter Norvig if this example and their
 

01:00:28.109 --> 01:00:31.390
Peter Norvig if this example and their
AI book of a vacuum cleaning robot and

01:00:31.390 --> 01:00:31.400
AI book of a vacuum cleaning robot and
 

01:00:31.400 --> 01:00:35.030
AI book of a vacuum cleaning robot and
the reward function could be the amount

01:00:35.030 --> 01:00:35.040
the reward function could be the amount
 

01:00:35.040 --> 01:00:37.250
the reward function could be the amount
of dust you managed to suck in and that

01:00:37.250 --> 01:00:37.260
of dust you managed to suck in and that
 

01:00:37.260 --> 01:00:38.300
of dust you managed to suck in and that
kind of makes sense because the more

01:00:38.300 --> 01:00:38.310
kind of makes sense because the more
 

01:00:38.310 --> 01:00:40.280
kind of makes sense because the more
dust that sucks in the cleaner the floor

01:00:40.280 --> 01:00:40.290
dust that sucks in the cleaner the floor
 

01:00:40.290 --> 01:00:42.200
dust that sucks in the cleaner the floor
is but then depending on the hardware

01:00:42.200 --> 01:00:42.210
is but then depending on the hardware
 

01:00:42.210 --> 01:00:45.109
is but then depending on the hardware
the optimal policy can be suck in a

01:00:45.109 --> 01:00:45.119
the optimal policy can be suck in a
 

01:00:45.119 --> 01:00:47.240
the optimal policy can be suck in a
little bit of dust and then dump it all

01:00:47.240 --> 01:00:47.250
little bit of dust and then dump it all
 

01:00:47.250 --> 01:00:49.640
little bit of dust and then dump it all
out then suck in a little bit more dump

01:00:49.640 --> 01:00:49.650
out then suck in a little bit more dump
 

01:00:49.650 --> 01:00:53.660
out then suck in a little bit more dump
it all back out right and recycle that I

01:00:53.660 --> 01:00:53.670
it all back out right and recycle that I
 

01:00:53.670 --> 01:00:55.130
it all back out right and recycle that I
don't think this is necessarily that

01:00:55.130 --> 01:00:55.140
don't think this is necessarily that
 

01:00:55.140 --> 01:00:56.690
don't think this is necessarily that
specific to AI think this is kind of a

01:00:56.690 --> 01:00:56.700
specific to AI think this is kind of a
 

01:00:56.700 --> 01:00:59.060
specific to AI think this is kind of a
general problem with with technology as

01:00:59.060 --> 01:00:59.070
general problem with with technology as
 

01:00:59.070 --> 01:01:01.819
general problem with with technology as
a whole and I think it comes from some

01:01:01.819 --> 01:01:01.829
a whole and I think it comes from some
 

01:01:01.829 --> 01:01:03.680
a whole and I think it comes from some
of them we all know about which is that

01:01:03.680 --> 01:01:03.690
of them we all know about which is that
 

01:01:03.690 --> 01:01:05.900
of them we all know about which is that
we were not very good at saying what we

01:01:05.900 --> 01:01:05.910
we were not very good at saying what we
 

01:01:05.910 --> 01:01:07.609
we were not very good at saying what we
want right we have all these myths and

01:01:07.609 --> 01:01:07.619
want right we have all these myths and
 

01:01:07.619 --> 01:01:09.230
want right we have all these myths and
legends about be careful what you wish

01:01:09.230 --> 01:01:09.240
legends about be careful what you wish
 

01:01:09.240 --> 01:01:11.180
legends about be careful what you wish
for and the reason it should be careful

01:01:11.180 --> 01:01:11.190
for and the reason it should be careful
 

01:01:11.190 --> 01:01:12.859
for and the reason it should be careful
is because you might just get it and it

01:01:12.859 --> 01:01:12.869
is because you might just get it and it
 

01:01:12.869 --> 01:01:15.170
is because you might just get it and it
might not be happy with the consequences

01:01:15.170 --> 01:01:15.180
might not be happy with the consequences
 

01:01:15.180 --> 01:01:17.359
might not be happy with the consequences
like King Midas wishing that everything

01:01:17.359 --> 01:01:17.369
like King Midas wishing that everything
 

01:01:17.369 --> 01:01:20.120
like King Midas wishing that everything
it touched turned to gold and I do want

01:01:20.120 --> 01:01:20.130
it touched turned to gold and I do want
 

01:01:20.130 --> 01:01:21.800
it touched turned to gold and I do want
to emphasize that this is not a story

01:01:21.800 --> 01:01:21.810
to emphasize that this is not a story
 

01:01:21.810 --> 01:01:24.440
to emphasize that this is not a story
about that is only applicable to Ages

01:01:24.440 --> 01:01:24.450
about that is only applicable to Ages
 

01:01:24.450 --> 01:01:26.569
about that is only applicable to Ages
that actually are capable at optimizing

01:01:26.569 --> 01:01:26.579
that actually are capable at optimizing
 

01:01:26.579 --> 01:01:28.849
that actually are capable at optimizing
their objectives I struggle with this

01:01:28.849 --> 01:01:28.859
their objectives I struggle with this
 

01:01:28.859 --> 01:01:30.589
their objectives I struggle with this
every day I do a lot of motion planning

01:01:30.589 --> 01:01:30.599
every day I do a lot of motion planning
 

01:01:30.599 --> 01:01:32.660
every day I do a lot of motion planning
work and motion planning you have to

01:01:32.660 --> 01:01:32.670
work and motion planning you have to
 

01:01:32.670 --> 01:01:34.250
work and motion planning you have to
write down a cost function that trades

01:01:34.250 --> 01:01:34.260
write down a cost function that trades
 

01:01:34.260 --> 01:01:35.750
write down a cost function that trades
off between different things that are

01:01:35.750 --> 01:01:35.760
off between different things that are
 

01:01:35.760 --> 01:01:37.880
off between different things that are
important to the robot this case

01:01:37.880 --> 01:01:37.890
important to the robot this case
 

01:01:37.890 --> 01:01:39.829
important to the robot this case
distance to the person and distance to

01:01:39.829 --> 01:01:39.839
distance to the person and distance to
 

01:01:39.839 --> 01:01:41.990
distance to the person and distance to
fragile objects and so on what

01:01:41.990 --> 01:01:42.000
fragile objects and so on what
 

01:01:42.000 --> 01:01:44.450
fragile objects and so on what
inevitably happens is you tune the thing

01:01:44.450 --> 01:01:44.460
inevitably happens is you tune the thing
 

01:01:44.460 --> 01:01:47.359
inevitably happens is you tune the thing
it works over 10 20 different problem

01:01:47.359 --> 01:01:47.369
it works over 10 20 different problem
 

01:01:47.369 --> 01:01:49.760
it works over 10 20 different problem
settings different environments and then

01:01:49.760 --> 01:01:49.770
settings different environments and then
 

01:01:49.770 --> 01:01:51.950
settings different environments and then
you encounter a new environment at some

01:01:51.950 --> 01:01:51.960
you encounter a new environment at some
 

01:01:51.960 --> 01:01:53.510
you encounter a new environment at some
point that's different enough and the

01:01:53.510 --> 01:01:53.520
point that's different enough and the
 

01:01:53.520 --> 01:01:55.400
point that's different enough and the
optimal behavior does not actually

01:01:55.400 --> 01:01:55.410
optimal behavior does not actually
 

01:01:55.410 --> 01:01:57.940
optimal behavior does not actually
correspond to what you actually wanted

01:01:57.940 --> 01:01:57.950
correspond to what you actually wanted
 

01:01:57.950 --> 01:02:01.430
correspond to what you actually wanted
so overall we're not great at specifying

01:02:01.430 --> 01:02:01.440
so overall we're not great at specifying
 

01:02:01.440 --> 01:02:04.280
so overall we're not great at specifying
these reward functions and I don't want

01:02:04.280 --> 01:02:04.290
these reward functions and I don't want
 

01:02:04.290 --> 01:02:06.170
these reward functions and I don't want
to make because this is an AI in ethics

01:02:06.170 --> 01:02:06.180
to make because this is an AI in ethics
 

01:02:06.180 --> 01:02:07.430
to make because this is an AI in ethics
conference I do want to make a little

01:02:07.430 --> 01:02:07.440
conference I do want to make a little
 

01:02:07.440 --> 01:02:09.290
conference I do want to make a little
bit of a theoretical point on

01:02:09.290 --> 01:02:09.300
bit of a theoretical point on
 

01:02:09.300 --> 01:02:11.120
bit of a theoretical point on
which is that I think that makes it

01:02:11.120 --> 01:02:11.130
which is that I think that makes it
 

01:02:11.130 --> 01:02:13.700
which is that I think that makes it
difficult to encode the ethical

01:02:13.700 --> 01:02:13.710
difficult to encode the ethical
 

01:02:13.710 --> 01:02:15.950
difficult to encode the ethical
principles that we as humans find so

01:02:15.950 --> 01:02:15.960
principles that we as humans find so
 

01:02:15.960 --> 01:02:18.860
principles that we as humans find so
easy and common sense into these

01:02:18.860 --> 01:02:18.870
easy and common sense into these
 

01:02:18.870 --> 01:02:22.640
easy and common sense into these
machines and all us minqi as my helper

01:02:22.640 --> 01:02:22.650
machines and all us minqi as my helper
 

01:02:22.650 --> 01:02:24.320
machines and all us minqi as my helper
to make this point so make your mouse

01:02:24.320 --> 01:02:24.330
to make this point so make your mouse
 

01:02:24.330 --> 01:02:26.660
to make this point so make your mouse
this is a this is from Fantasia a Mickey

01:02:26.660 --> 01:02:26.670
this is a this is from Fantasia a Mickey
 

01:02:26.670 --> 01:02:29.570
this is a this is from Fantasia a Mickey
Mouse was charged with getting water

01:02:29.570 --> 01:02:29.580
Mouse was charged with getting water
 

01:02:29.580 --> 01:02:32.680
Mouse was charged with getting water
from outside the house into the home and

01:02:32.680 --> 01:02:32.690
from outside the house into the home and
 

01:02:32.690 --> 01:02:36.260
from outside the house into the home and
make he gets tired of this and makes a

01:02:36.260 --> 01:02:36.270
make he gets tired of this and makes a
 

01:02:36.270 --> 01:02:39.770
make he gets tired of this and makes a
robot right uses magic to bring a broom

01:02:39.770 --> 01:02:39.780
robot right uses magic to bring a broom
 

01:02:39.780 --> 01:02:41.720
robot right uses magic to bring a broom
to life and shows it how to do the test

01:02:41.720 --> 01:02:41.730
to life and shows it how to do the test
 

01:02:41.730 --> 01:02:44.360
to life and shows it how to do the test
and Mickey goes to bed and then Mick he

01:02:44.360 --> 01:02:44.370
and Mickey goes to bed and then Mick he
 

01:02:44.370 --> 01:02:46.520
and Mickey goes to bed and then Mick he
wakes up and he's in a puddle of water

01:02:46.520 --> 01:02:46.530
wakes up and he's in a puddle of water
 

01:02:46.530 --> 01:02:49.820
wakes up and he's in a puddle of water
and he realizes that his robot assistant

01:02:49.820 --> 01:02:49.830
and he realizes that his robot assistant
 

01:02:49.830 --> 01:02:52.010
and he realizes that his robot assistant
was a bit overzealous with optimizing

01:02:52.010 --> 01:02:52.020
was a bit overzealous with optimizing
 

01:02:52.020 --> 01:02:55.360
was a bit overzealous with optimizing
the objective function that he specified

01:02:55.360 --> 01:02:55.370
the objective function that he specified
 

01:02:55.370 --> 01:02:57.830
the objective function that he specified
what's interesting here is that Mickey

01:02:57.830 --> 01:02:57.840
what's interesting here is that Mickey
 

01:02:57.840 --> 01:03:00.050
what's interesting here is that Mickey
then goes and you'll see in a second he

01:03:00.050 --> 01:03:00.060
then goes and you'll see in a second he
 

01:03:00.060 --> 01:03:05.230
then goes and you'll see in a second he
goes and tries to stop the broom but

01:03:05.230 --> 01:03:05.240
goes and tries to stop the broom but
 

01:03:05.240 --> 01:03:08.360
goes and tries to stop the broom but
that doesn't work because the broom was

01:03:08.360 --> 01:03:08.370
that doesn't work because the broom was
 

01:03:08.370 --> 01:03:10.250
that doesn't work because the broom was
given an objective function and sort of

01:03:10.250 --> 01:03:10.260
given an objective function and sort of
 

01:03:10.260 --> 01:03:12.200
given an objective function and sort of
what make you suggesting is not optimal

01:03:12.200 --> 01:03:12.210
what make you suggesting is not optimal
 

01:03:12.210 --> 01:03:15.140
what make you suggesting is not optimal
and so kind of a best bait very very

01:03:15.140 --> 01:03:15.150
and so kind of a best bait very very
 

01:03:15.150 --> 01:03:16.790
and so kind of a best bait very very
basic I'm not sure I'm sure I would even

01:03:16.790 --> 01:03:16.800
basic I'm not sure I'm sure I would even
 

01:03:16.800 --> 01:03:19.070
basic I'm not sure I'm sure I would even
call it ethical basic principle you

01:03:19.070 --> 01:03:19.080
call it ethical basic principle you
 

01:03:19.080 --> 01:03:20.960
call it ethical basic principle you
might want to encode right in this

01:03:20.960 --> 01:03:20.970
might want to encode right in this
 

01:03:20.970 --> 01:03:22.400
might want to encode right in this
reward function is look you should do

01:03:22.400 --> 01:03:22.410
reward function is look you should do
 

01:03:22.410 --> 01:03:25.970
reward function is look you should do
what people say okay so I just want to

01:03:25.970 --> 01:03:25.980
what people say okay so I just want to
 

01:03:25.980 --> 01:03:27.710
what people say okay so I just want to
point out a theoretical level that

01:03:27.710 --> 01:03:27.720
point out a theoretical level that
 

01:03:27.720 --> 01:03:30.080
point out a theoretical level that
that's not actually that trivial so the

01:03:30.080 --> 01:03:30.090
that's not actually that trivial so the
 

01:03:30.090 --> 01:03:31.370
that's not actually that trivial so the
way you might structure this you might

01:03:31.370 --> 01:03:31.380
way you might structure this you might
 

01:03:31.380 --> 01:03:33.170
way you might structure this you might
say here's the normal reward function

01:03:33.170 --> 01:03:33.180
say here's the normal reward function
 

01:03:33.180 --> 01:03:35.150
say here's the normal reward function
you should optimize for this except if

01:03:35.150 --> 01:03:35.160
you should optimize for this except if
 

01:03:35.160 --> 01:03:37.550
you should optimize for this except if
the person says do something else right

01:03:37.550 --> 01:03:37.560
the person says do something else right
 

01:03:37.560 --> 01:03:39.530
the person says do something else right
more reward than doing the something

01:03:39.530 --> 01:03:39.540
more reward than doing the something
 

01:03:39.540 --> 01:03:42.650
more reward than doing the something
else but if you set this up to be high

01:03:42.650 --> 01:03:42.660
else but if you set this up to be high
 

01:03:42.660 --> 01:03:45.710
else but if you set this up to be high
enough not that I'm worried that robots

01:03:45.710 --> 01:03:45.720
enough not that I'm worried that robots
 

01:03:45.720 --> 01:03:47.540
enough not that I'm worried that robots
would actually figure out how to do this

01:03:47.540 --> 01:03:47.550
would actually figure out how to do this
 

01:03:47.550 --> 01:03:49.700
would actually figure out how to do this
at all but just theoretically I want to

01:03:49.700 --> 01:03:49.710
at all but just theoretically I want to
 

01:03:49.710 --> 01:03:51.170
at all but just theoretically I want to
point out that which of what we just did

01:03:51.170 --> 01:03:51.180
point out that which of what we just did
 

01:03:51.180 --> 01:03:54.050
point out that which of what we just did
is we provided an incentive to the robot

01:03:54.050 --> 01:03:54.060
is we provided an incentive to the robot
 

01:03:54.060 --> 01:03:56.990
is we provided an incentive to the robot
to try to get itself in situations where

01:03:56.990 --> 01:03:57.000
to try to get itself in situations where
 

01:03:57.000 --> 01:03:58.820
to try to get itself in situations where
then people would react and give it

01:03:58.820 --> 01:03:58.830
then people would react and give it
 

01:03:58.830 --> 01:04:00.890
then people would react and give it
Corrections so that it can then accept

01:04:00.890 --> 01:04:00.900
Corrections so that it can then accept
 

01:04:00.900 --> 01:04:03.290
Corrections so that it can then accept
them and cash in on all of that reward

01:04:03.290 --> 01:04:03.300
them and cash in on all of that reward
 

01:04:03.300 --> 01:04:06.530
them and cash in on all of that reward
right and similarly if you make it too

01:04:06.530 --> 01:04:06.540
right and similarly if you make it too
 

01:04:06.540 --> 01:04:08.660
right and similarly if you make it too
low other things happen so it's not

01:04:08.660 --> 01:04:08.670
low other things happen so it's not
 

01:04:08.670 --> 01:04:10.850
low other things happen so it's not
trivial to tune these things right even

01:04:10.850 --> 01:04:10.860
trivial to tune these things right even
 

01:04:10.860 --> 01:04:14.030
trivial to tune these things right even
for something as basic as do what people

01:04:14.030 --> 01:04:14.040
for something as basic as do what people
 

01:04:14.040 --> 01:04:17.180
for something as basic as do what people
say now I'm not worried about this would

01:04:17.180 --> 01:04:17.190
say now I'm not worried about this would
 

01:04:17.190 --> 01:04:18.680
say now I'm not worried about this would
robots today you know perhaps they have

01:04:18.680 --> 01:04:18.690
robots today you know perhaps they have
 

01:04:18.690 --> 01:04:20.600
robots today you know perhaps they have
very narrow actions faces so they will

01:04:20.600 --> 01:04:20.610
very narrow actions faces so they will
 

01:04:20.610 --> 01:04:22.250
very narrow actions faces so they will
never figure out even if that's the

01:04:22.250 --> 01:04:22.260
never figure out even if that's the
 

01:04:22.260 --> 01:04:23.520
never figure out even if that's the
reward function how

01:04:23.520 --> 01:04:23.530
reward function how
 

01:04:23.530 --> 01:04:25.410
reward function how
to actually do that here's a robot it

01:04:25.410 --> 01:04:25.420
to actually do that here's a robot it
 

01:04:25.420 --> 01:04:27.150
to actually do that here's a robot it
moves from a start to a goal location

01:04:27.150 --> 01:04:27.160
moves from a start to a goal location
 

01:04:27.160 --> 01:04:29.400
moves from a start to a goal location
it's trying to be efficient it's trying

01:04:29.400 --> 01:04:29.410
it's trying to be efficient it's trying
 

01:04:29.410 --> 01:04:31.890
it's trying to be efficient it's trying
to optimize for staying away from things

01:04:31.890 --> 01:04:31.900
to optimize for staying away from things
 

01:04:31.900 --> 01:04:34.800
to optimize for staying away from things
and I it's very easy to get it to do

01:04:34.800 --> 01:04:34.810
and I it's very easy to get it to do
 

01:04:34.810 --> 01:04:37.200
and I it's very easy to get it to do
what I want so if I'm unhappy with how

01:04:37.200 --> 01:04:37.210
what I want so if I'm unhappy with how
 

01:04:37.210 --> 01:04:39.420
what I want so if I'm unhappy with how
it's doing the task if for instance I

01:04:39.420 --> 01:04:39.430
it's doing the task if for instance I
 

01:04:39.430 --> 01:04:41.100
it's doing the task if for instance I
think that it's keeping the bottle the

01:04:41.100 --> 01:04:41.110
think that it's keeping the bottle the
 

01:04:41.110 --> 01:04:44.670
think that it's keeping the bottle the
the mug too high up what I can do is I

01:04:44.670 --> 01:04:44.680
the mug too high up what I can do is I
 

01:04:44.680 --> 01:04:48.090
the mug too high up what I can do is I
can just go there and push on it and it

01:04:48.090 --> 01:04:48.100
can just go there and push on it and it
 

01:04:48.100 --> 01:04:49.320
can just go there and push on it and it
will do the thing and I can write a

01:04:49.320 --> 01:04:49.330
will do the thing and I can write a
 

01:04:49.330 --> 01:04:50.970
will do the thing and I can write a
controller pretty easily maybe an

01:04:50.970 --> 01:04:50.980
controller pretty easily maybe an
 

01:04:50.980 --> 01:04:53.100
controller pretty easily maybe an
impedance controller that makes the

01:04:53.100 --> 01:04:53.110
impedance controller that makes the
 

01:04:53.110 --> 01:04:55.280
impedance controller that makes the
robot actually compliant to human touch

01:04:55.280 --> 01:04:55.290
robot actually compliant to human touch
 

01:04:55.290 --> 01:04:57.810
robot actually compliant to human touch
the interesting thing for the robot

01:04:57.810 --> 01:04:57.820
the interesting thing for the robot
 

01:04:57.820 --> 01:04:59.370
the interesting thing for the robot
though and what's hard for a real robot

01:04:59.370 --> 01:04:59.380
though and what's hard for a real robot
 

01:04:59.380 --> 01:05:01.830
though and what's hard for a real robot
today it's not so much accepting this

01:05:01.830 --> 01:05:01.840
today it's not so much accepting this
 

01:05:01.840 --> 01:05:04.020
today it's not so much accepting this
kind of oversight but figuring out what

01:05:04.020 --> 01:05:04.030
kind of oversight but figuring out what
 

01:05:04.030 --> 01:05:06.390
kind of oversight but figuring out what
to do with it afterwards because the

01:05:06.390 --> 01:05:06.400
to do with it afterwards because the
 

01:05:06.400 --> 01:05:09.150
to do with it afterwards because the
moment I let go right the robot goes

01:05:09.150 --> 01:05:09.160
moment I let go right the robot goes
 

01:05:09.160 --> 01:05:12.210
moment I let go right the robot goes
back to optimizing its objective its its

01:05:12.210 --> 01:05:12.220
back to optimizing its objective its its
 

01:05:12.220 --> 01:05:14.370
back to optimizing its objective its its
reward function and so the moment I let

01:05:14.370 --> 01:05:14.380
reward function and so the moment I let
 

01:05:14.380 --> 01:05:16.590
reward function and so the moment I let
go right the robot goes back to the old

01:05:16.590 --> 01:05:16.600
go right the robot goes back to the old
 

01:05:16.600 --> 01:05:19.020
go right the robot goes back to the old
way of doing the task and I keep pushing

01:05:19.020 --> 01:05:19.030
way of doing the task and I keep pushing
 

01:05:19.030 --> 01:05:20.640
way of doing the task and I keep pushing
on it and he keeps going back and I keep

01:05:20.640 --> 01:05:20.650
on it and he keeps going back and I keep
 

01:05:20.650 --> 01:05:22.020
on it and he keeps going back and I keep
pushing at it and if it keeps going back

01:05:22.020 --> 01:05:22.030
pushing at it and if it keeps going back
 

01:05:22.030 --> 01:05:24.210
pushing at it and if it keeps going back
to what it thinks is the optimal way of

01:05:24.210 --> 01:05:24.220
to what it thinks is the optimal way of
 

01:05:24.220 --> 01:05:26.820
to what it thinks is the optimal way of
doing it and can be pretty frustrating

01:05:26.820 --> 01:05:26.830
doing it and can be pretty frustrating
 

01:05:26.830 --> 01:05:29.160
doing it and can be pretty frustrating
so what real robots I think the more in

01:05:29.160 --> 01:05:29.170
so what real robots I think the more in
 

01:05:29.170 --> 01:05:32.130
so what real robots I think the more in
a certain near-term problem is that you

01:05:32.130 --> 01:05:32.140
a certain near-term problem is that you
 

01:05:32.140 --> 01:05:34.200
a certain near-term problem is that you
shouldn't just be compliant you should

01:05:34.200 --> 01:05:34.210
shouldn't just be compliant you should
 

01:05:34.210 --> 01:05:37.230
shouldn't just be compliant you should
actually try to continually learn from

01:05:37.230 --> 01:05:37.240
actually try to continually learn from
 

01:05:37.240 --> 01:05:39.000
actually try to continually learn from
this oversight that people give you and

01:05:39.000 --> 01:05:39.010
this oversight that people give you and
 

01:05:39.010 --> 01:05:41.520
this oversight that people give you and
try to not make the same mistakes again

01:05:41.520 --> 01:05:41.530
try to not make the same mistakes again
 

01:05:41.530 --> 01:05:45.540
try to not make the same mistakes again
and again and again and so what all

01:05:45.540 --> 01:05:45.550
and again and again and so what all
 

01:05:45.550 --> 01:05:47.940
and again and again and so what all
these three examples I think teachers is

01:05:47.940 --> 01:05:47.950
these three examples I think teachers is
 

01:05:47.950 --> 01:05:50.220
these three examples I think teachers is
that the classical AI picture here is

01:05:50.220 --> 01:05:50.230
that the classical AI picture here is
 

01:05:50.230 --> 01:05:52.380
that the classical AI picture here is
missing something there's no reward

01:05:52.380 --> 01:05:52.390
missing something there's no reward
 

01:05:52.390 --> 01:05:54.750
missing something there's no reward
function that's exogenously specify it

01:05:54.750 --> 01:05:54.760
function that's exogenously specify it
 

01:05:54.760 --> 01:05:56.790
function that's exogenously specify it
doesn't come from the sky instead what

01:05:56.790 --> 01:05:56.800
doesn't come from the sky instead what
 

01:05:56.800 --> 01:05:59.580
doesn't come from the sky instead what
we have is we have people that want

01:05:59.580 --> 01:05:59.590
we have is we have people that want
 

01:05:59.590 --> 01:06:02.820
we have is we have people that want
things internally and that's what the

01:06:02.820 --> 01:06:02.830
things internally and that's what the
 

01:06:02.830 --> 01:06:04.970
things internally and that's what the
robot should be trying to optimize for

01:06:04.970 --> 01:06:04.980
robot should be trying to optimize for
 

01:06:04.980 --> 01:06:10.470
robot should be trying to optimize for
if it's a designer right then the

01:06:10.470 --> 01:06:10.480
if it's a designer right then the
 

01:06:10.480 --> 01:06:12.570
if it's a designer right then the
designer might actually specify a reward

01:06:12.570 --> 01:06:12.580
designer might actually specify a reward
 

01:06:12.580 --> 01:06:15.330
designer might actually specify a reward
function but chances are because we're

01:06:15.330 --> 01:06:15.340
function but chances are because we're
 

01:06:15.340 --> 01:06:17.400
function but chances are because we're
human and because we're fallible that

01:06:17.400 --> 01:06:17.410
human and because we're fallible that
 

01:06:17.410 --> 01:06:18.870
human and because we're fallible that
that's not necessarily gonna match up

01:06:18.870 --> 01:06:18.880
that's not necessarily gonna match up
 

01:06:18.880 --> 01:06:21.090
that's not necessarily gonna match up
perfectly it's not gonna be totally

01:06:21.090 --> 01:06:21.100
perfectly it's not gonna be totally
 

01:06:21.100 --> 01:06:22.080
perfectly it's not gonna be totally
drawn but it's not gonna match up

01:06:22.080 --> 01:06:22.090
drawn but it's not gonna match up
 

01:06:22.090 --> 01:06:25.140
drawn but it's not gonna match up
perfectly to what people actually what

01:06:25.140 --> 01:06:25.150
perfectly to what people actually what
 

01:06:25.150 --> 01:06:27.210
perfectly to what people actually what
the designer actually wants internally

01:06:27.210 --> 01:06:27.220
the designer actually wants internally
 

01:06:27.220 --> 01:06:29.550
the designer actually wants internally
and what we do right now is we just take

01:06:29.550 --> 01:06:29.560
and what we do right now is we just take
 

01:06:29.560 --> 01:06:31.650
and what we do right now is we just take
that for granted we just plug it in and

01:06:31.650 --> 01:06:31.660
that for granted we just plug it in and
 

01:06:31.660 --> 01:06:33.630
that for granted we just plug it in and
say this is what we're optimizing for as

01:06:33.630 --> 01:06:33.640
say this is what we're optimizing for as
 

01:06:33.640 --> 01:06:35.970
say this is what we're optimizing for as
if humans are perfect but they're not

01:06:35.970 --> 01:06:35.980
if humans are perfect but they're not
 

01:06:35.980 --> 01:06:36.500
if humans are perfect but they're not
and I

01:06:36.500 --> 01:06:36.510
and I
 

01:06:36.510 --> 01:06:37.700
and I
the picture that we should really be

01:06:37.700 --> 01:06:37.710
the picture that we should really be
 

01:06:37.710 --> 01:06:40.010
the picture that we should really be
reasoning about is this one where you

01:06:40.010 --> 01:06:40.020
reasoning about is this one where you
 

01:06:40.020 --> 01:06:41.780
reasoning about is this one where you
have a robot the trust actually work

01:06:41.780 --> 01:06:41.790
have a robot the trust actually work
 

01:06:41.790 --> 01:06:43.100
have a robot the trust actually work
with the person to figure out what it is

01:06:43.100 --> 01:06:43.110
with the person to figure out what it is
 

01:06:43.110 --> 01:06:45.920
with the person to figure out what it is
that they really want and it's hard

01:06:45.920 --> 01:06:45.930
that they really want and it's hard
 

01:06:45.930 --> 01:06:47.360
that they really want and it's hard
because you know we can't actually

01:06:47.360 --> 01:06:47.370
because you know we can't actually
 

01:06:47.370 --> 01:06:49.850
because you know we can't actually
decode what people think internally it's

01:06:49.850 --> 01:06:49.860
decode what people think internally it's
 

01:06:49.860 --> 01:06:52.100
decode what people think internally it's
a really tough tough thing to do but I

01:06:52.100 --> 01:06:52.110
a really tough tough thing to do but I
 

01:06:52.110 --> 01:06:53.480
a really tough tough thing to do but I
think we can make some progress towards

01:06:53.480 --> 01:06:53.490
think we can make some progress towards
 

01:06:53.490 --> 01:06:55.520
think we can make some progress towards
that so let me give you an example of

01:06:55.520 --> 01:06:55.530
that so let me give you an example of
 

01:06:55.530 --> 01:06:57.910
that so let me give you an example of
one thing that we've been thinking about

01:06:57.910 --> 01:06:57.920
one thing that we've been thinking about
 

01:06:57.920 --> 01:07:00.380
one thing that we've been thinking about
so we're back to the boat racing game do

01:07:00.380 --> 01:07:00.390
so we're back to the boat racing game do
 

01:07:00.390 --> 01:07:02.270
so we're back to the boat racing game do
you remember the game with the with the

01:07:02.270 --> 01:07:02.280
you remember the game with the with the
 

01:07:02.280 --> 01:07:04.970
you remember the game with the with the
boat racing that was doing the loop so

01:07:04.970 --> 01:07:04.980
boat racing that was doing the loop so
 

01:07:04.980 --> 01:07:06.860
boat racing that was doing the loop so
what was happening there was essentially

01:07:06.860 --> 01:07:06.870
what was happening there was essentially
 

01:07:06.870 --> 01:07:09.950
what was happening there was essentially
the boat had the choice between winning

01:07:09.950 --> 01:07:09.960
the boat had the choice between winning
 

01:07:09.960 --> 01:07:13.930
the boat had the choice between winning
and getting a lot of points say 20 or

01:07:13.930 --> 01:07:13.940
and getting a lot of points say 20 or
 

01:07:13.940 --> 01:07:16.010
and getting a lot of points say 20 or
exploiting this kind of loop that it

01:07:16.010 --> 01:07:16.020
exploiting this kind of loop that it
 

01:07:16.020 --> 01:07:18.620
exploiting this kind of loop that it
could do get even more points but not

01:07:18.620 --> 01:07:18.630
could do get even more points but not
 

01:07:18.630 --> 01:07:23.210
could do get even more points but not
win and it shows the second version and

01:07:23.210 --> 01:07:23.220
win and it shows the second version and
 

01:07:23.220 --> 01:07:24.800
win and it shows the second version and
the reason it shows the second version

01:07:24.800 --> 01:07:24.810
the reason it shows the second version
 

01:07:24.810 --> 01:07:27.800
the reason it shows the second version
was because there was a designer who

01:07:27.800 --> 01:07:27.810
was because there was a designer who
 

01:07:27.810 --> 01:07:29.480
was because there was a designer who
said look you should optimize for score

01:07:29.480 --> 01:07:29.490
said look you should optimize for score
 

01:07:29.490 --> 01:07:31.700
said look you should optimize for score
in the game so the interesting question

01:07:31.700 --> 01:07:31.710
in the game so the interesting question
 

01:07:31.710 --> 01:07:34.070
in the game so the interesting question
is why did the designers say that right

01:07:34.070 --> 01:07:34.080
is why did the designers say that right
 

01:07:34.080 --> 01:07:36.260
is why did the designers say that right
we designers are not totally dumb right

01:07:36.260 --> 01:07:36.270
we designers are not totally dumb right
 

01:07:36.270 --> 01:07:37.700
we designers are not totally dumb right
we're not just gonna write down a reward

01:07:37.700 --> 01:07:37.710
we're not just gonna write down a reward
 

01:07:37.710 --> 01:07:38.930
we're not just gonna write down a reward
function and deployed their sent the

01:07:38.930 --> 01:07:38.940
function and deployed their sent the
 

01:07:38.940 --> 01:07:40.040
function and deployed their sent the
robot off them to the world

01:07:40.040 --> 01:07:40.050
robot off them to the world
 

01:07:40.050 --> 01:07:43.070
robot off them to the world
and hope for the best so the reason was

01:07:43.070 --> 01:07:43.080
and hope for the best so the reason was
 

01:07:43.080 --> 01:07:45.320
and hope for the best so the reason was
that the designer actually tested this

01:07:45.320 --> 01:07:45.330
that the designer actually tested this
 

01:07:45.330 --> 01:07:48.560
that the designer actually tested this
robot on some environments and on those

01:07:48.560 --> 01:07:48.570
robot on some environments and on those
 

01:07:48.570 --> 01:07:51.890
robot on some environments and on those
environments optimizing for points got

01:07:51.890 --> 01:07:51.900
environments optimizing for points got
 

01:07:51.900 --> 01:07:53.720
environments optimizing for points got
the boat to actually do the correct

01:07:53.720 --> 01:07:53.730
the boat to actually do the correct
 

01:07:53.730 --> 01:07:57.620
the boat to actually do the correct
thing so at training time the the

01:07:57.620 --> 01:07:57.630
thing so at training time the the
 

01:07:57.630 --> 01:07:59.000
thing so at training time the the
environments that the person actually

01:07:59.000 --> 01:07:59.010
environments that the person actually
 

01:07:59.010 --> 01:08:01.280
environments that the person actually
looked at were pretty different than

01:08:01.280 --> 01:08:01.290
looked at were pretty different than
 

01:08:01.290 --> 01:08:03.320
looked at were pretty different than
this one test environment the training

01:08:03.320 --> 01:08:03.330
this one test environment the training
 

01:08:03.330 --> 01:08:05.780
this one test environment the training
time you could actually win with a lot

01:08:05.780 --> 01:08:05.790
time you could actually win with a lot
 

01:08:05.790 --> 01:08:07.580
time you could actually win with a lot
of points if you had fewer points then

01:08:07.580 --> 01:08:07.590
of points if you had fewer points then
 

01:08:07.590 --> 01:08:08.960
of points if you had fewer points then
you wouldn't win as much or as many

01:08:08.960 --> 01:08:08.970
you wouldn't win as much or as many
 

01:08:08.970 --> 01:08:12.500
you wouldn't win as much or as many
times or you would lose and so score and

01:08:12.500 --> 01:08:12.510
times or you would lose and so score and
 

01:08:12.510 --> 01:08:15.230
times or you would lose and so score and
winning were correlated at training time

01:08:15.230 --> 01:08:15.240
winning were correlated at training time
 

01:08:15.240 --> 01:08:16.880
winning were correlated at training time
but they were no longer correlated at

01:08:16.880 --> 01:08:16.890
but they were no longer correlated at
 

01:08:16.890 --> 01:08:18.849
but they were no longer correlated at
test time and that's what broke things

01:08:18.849 --> 01:08:18.859
test time and that's what broke things
 

01:08:18.859 --> 01:08:21.079
test time and that's what broke things
so that's one of the things that can go

01:08:21.079 --> 01:08:21.089
so that's one of the things that can go
 

01:08:21.089 --> 01:08:22.340
so that's one of the things that can go
wrong or you have features that no

01:08:22.340 --> 01:08:22.350
wrong or you have features that no
 

01:08:22.350 --> 01:08:24.920
wrong or you have features that no
longer correlate things can go wrong in

01:08:24.920 --> 01:08:24.930
longer correlate things can go wrong in
 

01:08:24.930 --> 01:08:26.990
longer correlate things can go wrong in
many other ways here's another example

01:08:26.990 --> 01:08:27.000
many other ways here's another example
 

01:08:27.000 --> 01:08:29.749
many other ways here's another example
say I'm building a robot to drive around

01:08:29.749 --> 01:08:29.759
say I'm building a robot to drive around
 

01:08:29.759 --> 01:08:32.300
say I'm building a robot to drive around
the CMU campus and I know it's gonna

01:08:32.300 --> 01:08:32.310
the CMU campus and I know it's gonna
 

01:08:32.310 --> 01:08:34.039
the CMU campus and I know it's gonna
have to deal with terrain and it deals

01:08:34.039 --> 01:08:34.049
have to deal with terrain and it deals
 

01:08:34.049 --> 01:08:36.320
have to deal with terrain and it deals
with grass and and paved roads and dirt

01:08:36.320 --> 01:08:36.330
with grass and and paved roads and dirt
 

01:08:36.330 --> 01:08:38.420
with grass and and paved roads and dirt
roads and so on and maybe I want the

01:08:38.420 --> 01:08:38.430
roads and so on and maybe I want the
 

01:08:38.430 --> 01:08:40.940
roads and so on and maybe I want the
robot to stay to avoid the grass and

01:08:40.940 --> 01:08:40.950
robot to stay to avoid the grass and
 

01:08:40.950 --> 01:08:43.190
robot to stay to avoid the grass and
protect it so I have to write down a

01:08:43.190 --> 01:08:43.200
protect it so I have to write down a
 

01:08:43.200 --> 01:08:45.829
protect it so I have to write down a
reward function for that the robot

01:08:45.829 --> 01:08:45.839
reward function for that the robot
 

01:08:45.839 --> 01:08:48.230
reward function for that the robot
doesn't actually know what's grass and

01:08:48.230 --> 01:08:48.240
doesn't actually know what's grass and
 

01:08:48.240 --> 01:08:49.789
doesn't actually know what's grass and
what's dirt and so on

01:08:49.789 --> 01:08:49.799
what's dirt and so on
 

01:08:49.799 --> 01:08:52.370
what's dirt and so on
and so all it has is some sensor input

01:08:52.370 --> 01:08:52.380
and so all it has is some sensor input
 

01:08:52.380 --> 01:08:54.470
and so all it has is some sensor input
let's say lidar came from camera images

01:08:54.470 --> 01:08:54.480
let's say lidar came from camera images
 

01:08:54.480 --> 01:08:56.809
let's say lidar came from camera images
and so the first thing I'll do is I'll

01:08:56.809 --> 01:08:56.819
and so the first thing I'll do is I'll
 

01:08:56.819 --> 01:08:58.280
and so the first thing I'll do is I'll
write down some classifiers some

01:08:58.280 --> 01:08:58.290
write down some classifiers some
 

01:08:58.290 --> 01:09:00.979
write down some classifiers some
detectors for this terrain types and

01:09:00.979 --> 01:09:00.989
detectors for this terrain types and
 

01:09:00.989 --> 01:09:02.599
detectors for this terrain types and
then I'll define my reward function that

01:09:02.599 --> 01:09:02.609
then I'll define my reward function that
 

01:09:02.609 --> 01:09:05.030
then I'll define my reward function that
says you know grass is bad going on dirt

01:09:05.030 --> 01:09:05.040
says you know grass is bad going on dirt
 

01:09:05.040 --> 01:09:08.510
says you know grass is bad going on dirt
or pavement is better and so I deployed

01:09:08.510 --> 01:09:08.520
or pavement is better and so I deployed
 

01:09:08.520 --> 01:09:12.319
or pavement is better and so I deployed
the robot after some testing and does

01:09:12.319 --> 01:09:12.329
the robot after some testing and does
 

01:09:12.329 --> 01:09:15.200
the robot after some testing and does
well in CMU I send to Berkeley does well

01:09:15.200 --> 01:09:15.210
well in CMU I send to Berkeley does well
 

01:09:15.210 --> 01:09:16.700
well in CMU I send to Berkeley does well
in Berkeley and then I don't know I send

01:09:16.700 --> 01:09:16.710
in Berkeley and then I don't know I send
 

01:09:16.710 --> 01:09:19.069
in Berkeley and then I don't know I send
it to Hawaii and then Hawaii right

01:09:19.069 --> 01:09:19.079
it to Hawaii and then Hawaii right
 

01:09:19.079 --> 01:09:22.280
it to Hawaii and then Hawaii right
there's grass there's dirt but there's

01:09:22.280 --> 01:09:22.290
there's grass there's dirt but there's
 

01:09:22.290 --> 01:09:24.200
there's grass there's dirt but there's
maybe some dangerous surfaces as well

01:09:24.200 --> 01:09:24.210
maybe some dangerous surfaces as well
 

01:09:24.210 --> 01:09:26.120
maybe some dangerous surfaces as well
that I don't want the robot to go over

01:09:26.120 --> 01:09:26.130
that I don't want the robot to go over
 

01:09:26.130 --> 01:09:28.849
that I don't want the robot to go over
for dramatic effect in this talk we'll

01:09:28.849 --> 01:09:28.859
for dramatic effect in this talk we'll
 

01:09:28.859 --> 01:09:31.039
for dramatic effect in this talk we'll
refer to these as lava right there's

01:09:31.039 --> 01:09:31.049
refer to these as lava right there's
 

01:09:31.049 --> 01:09:33.319
refer to these as lava right there's
mountains of lava now what the robot

01:09:33.319 --> 01:09:33.329
mountains of lava now what the robot
 

01:09:33.329 --> 01:09:35.240
mountains of lava now what the robot
will do a deployment time is it's gonna

01:09:35.240 --> 01:09:35.250
will do a deployment time is it's gonna
 

01:09:35.250 --> 01:09:37.010
will do a deployment time is it's gonna
use a three word function that I define

01:09:37.010 --> 01:09:37.020
use a three word function that I define
 

01:09:37.020 --> 01:09:39.709
use a three word function that I define
to evaluate the different surfaces and

01:09:39.709 --> 01:09:39.719
to evaluate the different surfaces and
 

01:09:39.719 --> 01:09:41.240
to evaluate the different surfaces and
who knows what it's gonna do for lava

01:09:41.240 --> 01:09:41.250
who knows what it's gonna do for lava
 

01:09:41.250 --> 01:09:42.680
who knows what it's gonna do for lava
that wasn't right and we don't have

01:09:42.680 --> 01:09:42.690
that wasn't right and we don't have
 

01:09:42.690 --> 01:09:44.089
that wasn't right and we don't have
detectors for that we totally forgot

01:09:44.089 --> 01:09:44.099
detectors for that we totally forgot
 

01:09:44.099 --> 01:09:46.249
detectors for that we totally forgot
about it so this is one thing that can

01:09:46.249 --> 01:09:46.259
about it so this is one thing that can
 

01:09:46.259 --> 01:09:47.749
about it so this is one thing that can
another thing that can go wrong which is

01:09:47.749 --> 01:09:47.759
another thing that can go wrong which is
 

01:09:47.759 --> 01:09:49.490
another thing that can go wrong which is
maybe we don't actually think of all the

01:09:49.490 --> 01:09:49.500
maybe we don't actually think of all the
 

01:09:49.500 --> 01:09:51.169
maybe we don't actually think of all the
edge cases of all the possible things

01:09:51.169 --> 01:09:51.179
edge cases of all the possible things
 

01:09:51.179 --> 01:09:53.990
edge cases of all the possible things
that our robots might have to face like

01:09:53.990 --> 01:09:54.000
that our robots might have to face like
 

01:09:54.000 --> 01:09:55.729
that our robots might have to face like
dangerous surfaces that don't appear at

01:09:55.729 --> 01:09:55.739
dangerous surfaces that don't appear at
 

01:09:55.739 --> 01:09:58.000
dangerous surfaces that don't appear at
training time but if you're at test time

01:09:58.000 --> 01:09:58.010
training time but if you're at test time
 

01:09:58.010 --> 01:10:00.919
training time but if you're at test time
so even though with the boat example and

01:10:00.919 --> 01:10:00.929
so even though with the boat example and
 

01:10:00.929 --> 01:10:03.200
so even though with the boat example and
this navigation example there are

01:10:03.200 --> 01:10:03.210
this navigation example there are
 

01:10:03.210 --> 01:10:04.870
this navigation example there are
different things that are going wrong

01:10:04.870 --> 01:10:04.880
different things that are going wrong
 

01:10:04.880 --> 01:10:07.160
different things that are going wrong
they have one important thing in common

01:10:07.160 --> 01:10:07.170
they have one important thing in common
 

01:10:07.170 --> 01:10:10.010
they have one important thing in common
which is what gives me hope and that one

01:10:10.010 --> 01:10:10.020
which is what gives me hope and that one
 

01:10:10.020 --> 01:10:11.660
which is what gives me hope and that one
thing that they have in common is that

01:10:11.660 --> 01:10:11.670
thing that they have in common is that
 

01:10:11.670 --> 01:10:14.569
thing that they have in common is that
the designer actually did a pretty good

01:10:14.569 --> 01:10:14.579
the designer actually did a pretty good
 

01:10:14.579 --> 01:10:18.950
the designer actually did a pretty good
job for the training environments in

01:10:18.950 --> 01:10:18.960
job for the training environments in
 

01:10:18.960 --> 01:10:21.290
job for the training environments in
other words there's not much that we

01:10:21.290 --> 01:10:21.300
other words there's not much that we
 

01:10:21.300 --> 01:10:22.819
other words there's not much that we
know about the true reward what the

01:10:22.819 --> 01:10:22.829
know about the true reward what the
 

01:10:22.829 --> 01:10:25.160
know about the true reward what the
person actually wants but there's one

01:10:25.160 --> 01:10:25.170
person actually wants but there's one
 

01:10:25.170 --> 01:10:26.899
person actually wants but there's one
thing that we know which is that

01:10:26.899 --> 01:10:26.909
thing that we know which is that
 

01:10:26.909 --> 01:10:28.729
thing that we know which is that
whatever reward function the person

01:10:28.729 --> 01:10:28.739
whatever reward function the person
 

01:10:28.739 --> 01:10:31.250
whatever reward function the person
specified it might not work always it

01:10:31.250 --> 01:10:31.260
specified it might not work always it
 

01:10:31.260 --> 01:10:32.569
specified it might not work always it
might not produce the right behavior

01:10:32.569 --> 01:10:32.579
might not produce the right behavior
 

01:10:32.579 --> 01:10:35.180
might not produce the right behavior
always but on the training environments

01:10:35.180 --> 01:10:35.190
always but on the training environments
 

01:10:35.190 --> 01:10:37.129
always but on the training environments
it produces the right behavior and

01:10:37.129 --> 01:10:37.139
it produces the right behavior and
 

01:10:37.139 --> 01:10:39.229
it produces the right behavior and
that's information that we now have

01:10:39.229 --> 01:10:39.239
that's information that we now have
 

01:10:39.239 --> 01:10:41.899
that's information that we now have
about the true reward all right we don't

01:10:41.899 --> 01:10:41.909
about the true reward all right we don't
 

01:10:41.909 --> 01:10:43.399
about the true reward all right we don't
have the true reward but we know this

01:10:43.399 --> 01:10:43.409
have the true reward but we know this
 

01:10:43.409 --> 01:10:45.140
have the true reward but we know this
much and in fact that adequate that's

01:10:45.140 --> 01:10:45.150
much and in fact that adequate that's
 

01:10:45.150 --> 01:10:47.720
much and in fact that adequate that's
all that we should assume if a person

01:10:47.720 --> 01:10:47.730
all that we should assume if a person
 

01:10:47.730 --> 01:10:50.240
all that we should assume if a person
specifies a reward another way of saying

01:10:50.240 --> 01:10:50.250
specifies a reward another way of saying
 

01:10:50.250 --> 01:10:52.399
specifies a reward another way of saying
the same thing is that look what we end

01:10:52.399 --> 01:10:52.409
the same thing is that look what we end
 

01:10:52.409 --> 01:10:56.149
the same thing is that look what we end
up specifying is contextualized in the

01:10:56.149 --> 01:10:56.159
up specifying is contextualized in the
 

01:10:56.159 --> 01:10:58.850
up specifying is contextualized in the
experiences that we have robots are not

01:10:58.850 --> 01:10:58.860
experiences that we have robots are not
 

01:10:58.860 --> 01:11:01.399
experiences that we have robots are not
interpreted literally right should

01:11:01.399 --> 01:11:01.409
interpreted literally right should
 

01:11:01.409 --> 01:11:02.690
interpreted literally right should
interpret it as B

01:11:02.690 --> 01:11:02.700
interpret it as B
 

01:11:02.700 --> 01:11:04.670
interpret it as B
contextualized in the experiences that

01:11:04.670 --> 01:11:04.680
contextualized in the experiences that
 

01:11:04.680 --> 01:11:08.330
contextualized in the experiences that
we have for robot assistant the artist

01:11:08.330 --> 01:11:08.340
we have for robot assistant the artist
 

01:11:08.340 --> 01:11:09.620
we have for robot assistant the artist
mathematically the way we do this is

01:11:09.620 --> 01:11:09.630
mathematically the way we do this is
 

01:11:09.630 --> 01:11:12.080
mathematically the way we do this is
instead of just optimizing the reward

01:11:12.080 --> 01:11:12.090
instead of just optimizing the reward
 

01:11:12.090 --> 01:11:13.820
instead of just optimizing the reward
function we treat it as an observation

01:11:13.820 --> 01:11:13.830
function we treat it as an observation
 

01:11:13.830 --> 01:11:15.470
function we treat it as an observation
and the Bayesian sense about the

01:11:15.470 --> 01:11:15.480
and the Bayesian sense about the
 

01:11:15.480 --> 01:11:17.690
and the Bayesian sense about the
underlying true reward and to do that we

01:11:17.690 --> 01:11:17.700
underlying true reward and to do that we
 

01:11:17.700 --> 01:11:20.600
underlying true reward and to do that we
have to reason about if theta star is

01:11:20.600 --> 01:11:20.610
have to reason about if theta star is
 

01:11:20.610 --> 01:11:23.300
have to reason about if theta star is
the true reward if the trade the person

01:11:23.300 --> 01:11:23.310
the true reward if the trade the person
 

01:11:23.310 --> 01:11:25.130
the true reward if the trade the person
is looking at the training environment

01:11:25.130 --> 01:11:25.140
is looking at the training environment
 

01:11:25.140 --> 01:11:26.810
is looking at the training environment
what will they write down what is likely

01:11:26.810 --> 01:11:26.820
what will they write down what is likely
 

01:11:26.820 --> 01:11:29.060
what will they write down what is likely
for them to specify and that's where we

01:11:29.060 --> 01:11:29.070
for them to specify and that's where we
 

01:11:29.070 --> 01:11:31.220
for them to specify and that's where we
use our insight what we do is we say

01:11:31.220 --> 01:11:31.230
use our insight what we do is we say
 

01:11:31.230 --> 01:11:33.860
use our insight what we do is we say
well how likely is theta tilde to be

01:11:33.860 --> 01:11:33.870
well how likely is theta tilde to be
 

01:11:33.870 --> 01:11:36.110
well how likely is theta tilde to be
written down as the specified reward

01:11:36.110 --> 01:11:36.120
written down as the specified reward
 

01:11:36.120 --> 01:11:38.480
written down as the specified reward
well I have to look at what behavior it

01:11:38.480 --> 01:11:38.490
well I have to look at what behavior it
 

01:11:38.490 --> 01:11:40.550
well I have to look at what behavior it
incentivizes on the training

01:11:40.550 --> 01:11:40.560
incentivizes on the training
 

01:11:40.560 --> 01:11:43.220
incentivizes on the training
environments and if that behavior is

01:11:43.220 --> 01:11:43.230
environments and if that behavior is
 

01:11:43.230 --> 01:11:46.340
environments and if that behavior is
good with respect to theta star then I

01:11:46.340 --> 01:11:46.350
good with respect to theta star then I
 

01:11:46.350 --> 01:11:47.990
good with respect to theta star then I
know that if theta star is the correct

01:11:47.990 --> 01:11:48.000
know that if theta star is the correct
 

01:11:48.000 --> 01:11:49.760
know that if theta star is the correct
reward theta tilde is likely to be

01:11:49.760 --> 01:11:49.770
reward theta tilde is likely to be
 

01:11:49.770 --> 01:11:51.230
reward theta tilde is likely to be
written down more likely to be written

01:11:51.230 --> 01:11:51.240
written down more likely to be written
 

01:11:51.240 --> 01:11:53.510
written down more likely to be written
down by the designer as the specified

01:11:53.510 --> 01:11:53.520
down by the designer as the specified
 

01:11:53.520 --> 01:11:55.490
down by the designer as the specified
reward so that's how we do it

01:11:55.490 --> 01:11:55.500
reward so that's how we do it
 

01:11:55.500 --> 01:11:57.260
reward so that's how we do it
intuitively what this means is as

01:11:57.260 --> 01:11:57.270
intuitively what this means is as
 

01:11:57.270 --> 01:12:00.260
intuitively what this means is as
follows going back to the boat example

01:12:00.260 --> 01:12:00.270
follows going back to the boat example
 

01:12:00.270 --> 01:12:02.360
follows going back to the boat example
imagine that there's a plethora of

01:12:02.360 --> 01:12:02.370
imagine that there's a plethora of
 

01:12:02.370 --> 01:12:04.970
imagine that there's a plethora of
different reward functions that the

01:12:04.970 --> 01:12:04.980
different reward functions that the
 

01:12:04.980 --> 01:12:07.880
different reward functions that the
person that the person might define

01:12:07.880 --> 01:12:07.890
person that the person might define
 

01:12:07.890 --> 01:12:09.650
person that the person might define
right so there's all these different

01:12:09.650 --> 01:12:09.660
right so there's all these different
 

01:12:09.660 --> 01:12:12.050
right so there's all these different
features that might be relevant the

01:12:12.050 --> 01:12:12.060
features that might be relevant the
 

01:12:12.060 --> 01:12:14.480
features that might be relevant the
person specified theta 2 which is

01:12:14.480 --> 01:12:14.490
person specified theta 2 which is
 

01:12:14.490 --> 01:12:16.510
person specified theta 2 which is
maximized score that's what they picked

01:12:16.510 --> 01:12:16.520
maximized score that's what they picked
 

01:12:16.520 --> 01:12:19.580
maximized score that's what they picked
instead of just optimizing for score

01:12:19.580 --> 01:12:19.590
instead of just optimizing for score
 

01:12:19.590 --> 01:12:22.130
instead of just optimizing for score
what our robot is going to do is it's

01:12:22.130 --> 01:12:22.140
what our robot is going to do is it's
 

01:12:22.140 --> 01:12:23.990
what our robot is going to do is it's
going to take a step back and say what's

01:12:23.990 --> 01:12:24.000
going to take a step back and say what's
 

01:12:24.000 --> 01:12:25.610
going to take a step back and say what's
the experience that they had to draw on

01:12:25.610 --> 01:12:25.620
the experience that they had to draw on
 

01:12:25.620 --> 01:12:28.190
the experience that they had to draw on
when they specify theta 2 let's take a

01:12:28.190 --> 01:12:28.200
when they specify theta 2 let's take a
 

01:12:28.200 --> 01:12:29.360
when they specify theta 2 let's take a
look right they looked at these

01:12:29.360 --> 01:12:29.370
look right they looked at these
 

01:12:29.370 --> 01:12:30.980
look right they looked at these
environments where you could either win

01:12:30.980 --> 01:12:30.990
environments where you could either win
 

01:12:30.990 --> 01:12:33.710
environments where you could either win
with a lot of points or lose with fewer

01:12:33.710 --> 01:12:33.720
with a lot of points or lose with fewer
 

01:12:33.720 --> 01:12:36.410
with a lot of points or lose with fewer
points and theta 2 incentivizes the

01:12:36.410 --> 01:12:36.420
points and theta 2 incentivizes the
 

01:12:36.420 --> 01:12:38.060
points and theta 2 incentivizes the
behavior where you get a lot of score

01:12:38.060 --> 01:12:38.070
behavior where you get a lot of score
 

01:12:38.070 --> 01:12:41.300
behavior where you get a lot of score
and you'll and you also win and so what

01:12:41.300 --> 01:12:41.310
and you'll and you also win and so what
 

01:12:41.310 --> 01:12:43.790
and you'll and you also win and so what
the robot is wondering now is for which

01:12:43.790 --> 01:12:43.800
the robot is wondering now is for which
 

01:12:43.800 --> 01:12:48.680
the robot is wondering now is for which
of all these possibilities is winning

01:12:48.680 --> 01:12:48.690
of all these possibilities is winning
 

01:12:48.690 --> 01:12:50.480
of all these possibilities is winning
with high score a good idea which of

01:12:50.480 --> 01:12:50.490
with high score a good idea which of
 

01:12:50.490 --> 01:12:52.100
with high score a good idea which of
these reward functions could have been

01:12:52.100 --> 01:12:52.110
these reward functions could have been
 

01:12:52.110 --> 01:12:53.930
these reward functions could have been
the true reward function and what you

01:12:53.930 --> 01:12:53.940
the true reward function and what you
 

01:12:53.940 --> 01:12:55.660
the true reward function and what you
can figure out pretty easily is that

01:12:55.660 --> 01:12:55.670
can figure out pretty easily is that
 

01:12:55.670 --> 01:12:58.330
can figure out pretty easily is that
maximizing score likes this behavior

01:12:58.330 --> 01:12:58.340
maximizing score likes this behavior
 

01:12:58.340 --> 01:13:01.430
maximizing score likes this behavior
minimizing score or trying to lose if

01:13:01.430 --> 01:13:01.440
minimizing score or trying to lose if
 

01:13:01.440 --> 01:13:03.380
minimizing score or trying to lose if
that were the objective that doesn't

01:13:03.380 --> 01:13:03.390
that were the objective that doesn't
 

01:13:03.390 --> 01:13:04.670
that were the objective that doesn't
like this behavior so can throw that

01:13:04.670 --> 01:13:04.680
like this behavior so can throw that
 

01:13:04.680 --> 01:13:06.910
like this behavior so can throw that
right out but you also understand that

01:13:06.910 --> 01:13:06.920
right out but you also understand that
 

01:13:06.920 --> 01:13:10.010
right out but you also understand that
maximizing winning is something that

01:13:10.010 --> 01:13:10.020
maximizing winning is something that
 

01:13:10.020 --> 01:13:11.660
maximizing winning is something that
likes this behavior so now the robot

01:13:11.660 --> 01:13:11.670
likes this behavior so now the robot
 

01:13:11.670 --> 01:13:14.210
likes this behavior so now the robot
knows ok I was told to optimize for

01:13:14.210 --> 01:13:14.220
knows ok I was told to optimize for
 

01:13:14.220 --> 01:13:15.380
knows ok I was told to optimize for
score

01:13:15.380 --> 01:13:15.390
score
 

01:13:15.390 --> 01:13:17.150
score
I'm pretty sure the person didn't mean

01:13:17.150 --> 01:13:17.160
I'm pretty sure the person didn't mean
 

01:13:17.160 --> 01:13:20.570
I'm pretty sure the person didn't mean
to lose or to minimize score but I don't

01:13:20.570 --> 01:13:20.580
to lose or to minimize score but I don't
 

01:13:20.580 --> 01:13:22.340
to lose or to minimize score but I don't
know if they didn't mean to win because

01:13:22.340 --> 01:13:22.350
know if they didn't mean to win because
 

01:13:22.350 --> 01:13:24.310
know if they didn't mean to win because
based on the information that they had

01:13:24.310 --> 01:13:24.320
based on the information that they had
 

01:13:24.320 --> 01:13:27.410
based on the information that they had
winning also might look like a good

01:13:27.410 --> 01:13:27.420
winning also might look like a good
 

01:13:27.420 --> 01:13:30.710
winning also might look like a good
thing so and and they might have not

01:13:30.710 --> 01:13:30.720
thing so and and they might have not
 

01:13:30.720 --> 01:13:34.130
thing so and and they might have not
gotten that so this is a very simple to

01:13:34.130 --> 01:13:34.140
gotten that so this is a very simple to
 

01:13:34.140 --> 01:13:35.480
gotten that so this is a very simple to
example we've been playing with

01:13:35.480 --> 01:13:35.490
example we've been playing with
 

01:13:35.490 --> 01:13:37.430
example we've been playing with
situations that are actually complicated

01:13:37.430 --> 01:13:37.440
situations that are actually complicated
 

01:13:37.440 --> 01:13:39.350
situations that are actually complicated
where the robot doesn't get convenient

01:13:39.350 --> 01:13:39.360
where the robot doesn't get convenient
 

01:13:39.360 --> 01:13:41.210
where the robot doesn't get convenient
access to all the important relevant

01:13:41.210 --> 01:13:41.220
access to all the important relevant
 

01:13:41.220 --> 01:13:43.700
access to all the important relevant
features like winning we've been looking

01:13:43.700 --> 01:13:43.710
features like winning we've been looking
 

01:13:43.710 --> 01:13:46.430
features like winning we've been looking
at motion planning to where we end up

01:13:46.430 --> 01:13:46.440
at motion planning to where we end up
 

01:13:46.440 --> 01:13:48.770
at motion planning to where we end up
with is more robust behaviors and these

01:13:48.770 --> 01:13:48.780
with is more robust behaviors and these
 

01:13:48.780 --> 01:13:50.900
with is more robust behaviors and these
complicated new environments that the

01:13:50.900 --> 01:13:50.910
complicated new environments that the
 

01:13:50.910 --> 01:13:54.290
complicated new environments that the
robot is choosing and so overall I think

01:13:54.290 --> 01:13:54.300
robot is choosing and so overall I think
 

01:13:54.300 --> 01:13:55.640
robot is choosing and so overall I think
that was just one example of the

01:13:55.640 --> 01:13:55.650
that was just one example of the
 

01:13:55.650 --> 01:13:57.890
that was just one example of the
approaches we can take but overall what

01:13:57.890 --> 01:13:57.900
approaches we can take but overall what
 

01:13:57.900 --> 01:14:00.140
approaches we can take but overall what
I'd say is whether it's a designer

01:14:00.140 --> 01:14:00.150
I'd say is whether it's a designer
 

01:14:00.150 --> 01:14:03.500
I'd say is whether it's a designer
that's writing down a reward function or

01:14:03.500 --> 01:14:03.510
that's writing down a reward function or
 

01:14:03.510 --> 01:14:05.120
that's writing down a reward function or
an end user telling you that you're

01:14:05.120 --> 01:14:05.130
an end user telling you that you're
 

01:14:05.130 --> 01:14:07.430
an end user telling you that you're
doing something wrong or even actually

01:14:07.430 --> 01:14:07.440
doing something wrong or even actually
 

01:14:07.440 --> 01:14:09.950
doing something wrong or even actually
giving you Corrections what the robot

01:14:09.950 --> 01:14:09.960
giving you Corrections what the robot
 

01:14:09.960 --> 01:14:12.590
giving you Corrections what the robot
should be doing is taking all of this in

01:14:12.590 --> 01:14:12.600
should be doing is taking all of this in
 

01:14:12.600 --> 01:14:14.930
should be doing is taking all of this in
as useful evidence it's useful

01:14:14.930 --> 01:14:14.940
as useful evidence it's useful
 

01:14:14.940 --> 01:14:16.730
as useful evidence it's useful
information about what people actually

01:14:16.730 --> 01:14:16.740
information about what people actually
 

01:14:16.740 --> 01:14:19.130
information about what people actually
want and what the robot is actually

01:14:19.130 --> 01:14:19.140
want and what the robot is actually
 

01:14:19.140 --> 01:14:21.950
want and what the robot is actually
supposed to do and improving that over

01:14:21.950 --> 01:14:21.960
supposed to do and improving that over
 

01:14:21.960 --> 01:14:23.960
supposed to do and improving that over
time and getting better and better at

01:14:23.960 --> 01:14:23.970
time and getting better and better at
 

01:14:23.970 --> 01:14:26.690
time and getting better and better at
optimizing not a specified reward but

01:14:26.690 --> 01:14:26.700
optimizing not a specified reward but
 

01:14:26.700 --> 01:14:28.760
optimizing not a specified reward but
what people actually want here's what

01:14:28.760 --> 01:14:28.770
what people actually want here's what
 

01:14:28.770 --> 01:14:30.920
what people actually want here's what
happens with our little example when you

01:14:30.920 --> 01:14:30.930
happens with our little example when you
 

01:14:30.930 --> 01:14:33.020
happens with our little example when you
do this I push on the robot and instead

01:14:33.020 --> 01:14:33.030
do this I push on the robot and instead
 

01:14:33.030 --> 01:14:35.330
do this I push on the robot and instead
of just going back the robot interprets

01:14:35.330 --> 01:14:35.340
of just going back the robot interprets
 

01:14:35.340 --> 01:14:38.000
of just going back the robot interprets
that as evidence about what I want and

01:14:38.000 --> 01:14:38.010
that as evidence about what I want and
 

01:14:38.010 --> 01:14:39.890
that as evidence about what I want and
it ends up doing the task very

01:14:39.890 --> 01:14:39.900
it ends up doing the task very
 

01:14:39.900 --> 01:14:45.020
it ends up doing the task very
differently so to conclude there's this

01:14:45.020 --> 01:14:45.030
differently so to conclude there's this
 

01:14:45.030 --> 01:14:46.850
differently so to conclude there's this
mismatch that we now have and I think

01:14:46.850 --> 01:14:46.860
mismatch that we now have and I think
 

01:14:46.860 --> 01:14:49.790
mismatch that we now have and I think
both making robots more trustworthy and

01:14:49.790 --> 01:14:49.800
both making robots more trustworthy and
 

01:14:49.800 --> 01:14:51.530
both making robots more trustworthy and
the ways that we've talked about as well

01:14:51.530 --> 01:14:51.540
the ways that we've talked about as well
 

01:14:51.540 --> 01:14:54.020
the ways that we've talked about as well
as making robots more transparent it's

01:14:54.020 --> 01:14:54.030
as making robots more transparent it's
 

01:14:54.030 --> 01:14:56.390
as making robots more transparent it's
going to help with not necessarily

01:14:56.390 --> 01:14:56.400
going to help with not necessarily
 

01:14:56.400 --> 01:14:57.950
going to help with not necessarily
resolving one thing or another but

01:14:57.950 --> 01:14:57.960
resolving one thing or another but
 

01:14:57.960 --> 01:15:02.300
resolving one thing or another but
bringing these these two items the real

01:15:02.300 --> 01:15:02.310
bringing these these two items the real
 

01:15:02.310 --> 01:15:04.790
bringing these these two items the real
robot and the mental model closer to

01:15:04.790 --> 01:15:04.800
robot and the mental model closer to
 

01:15:04.800 --> 01:15:07.100
robot and the mental model closer to
each other and have a common middle

01:15:07.100 --> 01:15:07.110
each other and have a common middle
 

01:15:07.110 --> 01:15:09.890
each other and have a common middle
ground that works out well for the human

01:15:09.890 --> 01:15:09.900
ground that works out well for the human
 

01:15:09.900 --> 01:15:11.930
ground that works out well for the human
and the robot with that I'd like to

01:15:11.930 --> 01:15:11.940
and the robot with that I'd like to
 

01:15:11.940 --> 01:15:13.740
and the robot with that I'd like to
thank you very much

01:15:13.740 --> 01:15:13.750
thank you very much
 

01:15:13.750 --> 01:15:21.220
thank you very much
[Applause]

01:15:21.220 --> 01:15:21.230
[Applause]
 

01:15:21.230 --> 01:15:24.880
[Applause]
thank you so much you won't need the mic

01:15:24.880 --> 01:15:24.890
thank you so much you won't need the mic
 

01:15:24.890 --> 01:15:28.160
thank you so much you won't need the mic
so that was really wonderful and I think

01:15:28.160 --> 01:15:28.170
so that was really wonderful and I think
 

01:15:28.170 --> 01:15:30.140
so that was really wonderful and I think
especially the ways in which you focused

01:15:30.140 --> 01:15:30.150
especially the ways in which you focused
 

01:15:30.150 --> 01:15:33.560
especially the ways in which you focused
on reward functions as connected

01:15:33.560 --> 01:15:33.570
on reward functions as connected
 

01:15:33.570 --> 01:15:35.060
on reward functions as connected
directly into the end of the previous

01:15:35.060 --> 01:15:35.070
directly into the end of the previous
 

01:15:35.070 --> 01:15:37.040
directly into the end of the previous
panel this idea that we want the robots

01:15:37.040 --> 01:15:37.050
panel this idea that we want the robots
 

01:15:37.050 --> 01:15:39.620
panel this idea that we want the robots
to to support our goals in our interests

01:15:39.620 --> 01:15:39.630
to to support our goals in our interests
 

01:15:39.630 --> 01:15:41.030
to to support our goals in our interests
well then we have to say what counts as

01:15:41.030 --> 01:15:41.040
well then we have to say what counts as
 

01:15:41.040 --> 01:15:44.060
well then we have to say what counts as
success and how do we do that well that

01:15:44.060 --> 01:15:44.070
success and how do we do that well that
 

01:15:44.070 --> 01:15:45.680
success and how do we do that well that
might be very hard for us humans to do

01:15:45.680 --> 01:15:45.690
might be very hard for us humans to do
 

01:15:45.690 --> 01:15:48.440
might be very hard for us humans to do
and so how can the robot learn what we

01:15:48.440 --> 01:15:48.450
and so how can the robot learn what we
 

01:15:48.450 --> 01:15:51.710
and so how can the robot learn what we
wanted without us even knowing it but

01:15:51.710 --> 01:15:51.720
wanted without us even knowing it but
 

01:15:51.720 --> 01:15:53.240
wanted without us even knowing it but
but of course I have to ask that there's

01:15:53.240 --> 01:15:53.250
but of course I have to ask that there's
 

01:15:53.250 --> 01:15:56.540
but of course I have to ask that there's
a risk I might say and want the robot to

01:15:56.540 --> 01:15:56.550
a risk I might say and want the robot to
 

01:15:56.550 --> 01:15:58.570
a risk I might say and want the robot to
help me not eat lots of chocolate cake

01:15:58.570 --> 01:15:58.580
help me not eat lots of chocolate cake
 

01:15:58.580 --> 01:16:00.860
help me not eat lots of chocolate cake
but then it looks at what I actually do

01:16:00.860 --> 01:16:00.870
but then it looks at what I actually do
 

01:16:00.870 --> 01:16:02.930
but then it looks at what I actually do
and says you eat a lot of chocolate cake

01:16:02.930 --> 01:16:02.940
and says you eat a lot of chocolate cake
 

01:16:02.940 --> 01:16:06.500
and says you eat a lot of chocolate cake
so how do we help the you know are there

01:16:06.500 --> 01:16:06.510
so how do we help the you know are there
 

01:16:06.510 --> 01:16:07.880
so how do we help the you know are there
ways that the robots might be able to

01:16:07.880 --> 01:16:07.890
ways that the robots might be able to
 

01:16:07.890 --> 01:16:11.480
ways that the robots might be able to
help us overcome our own in abilities to

01:16:11.480 --> 01:16:11.490
help us overcome our own in abilities to
 

01:16:11.490 --> 01:16:15.110
help us overcome our own in abilities to
live by what we want that's a fantastic

01:16:15.110 --> 01:16:15.120
live by what we want that's a fantastic
 

01:16:15.120 --> 01:16:17.600
live by what we want that's a fantastic
question because I think you're right

01:16:17.600 --> 01:16:17.610
question because I think you're right
 

01:16:17.610 --> 01:16:21.170
question because I think you're right
that there is there is a risk that what

01:16:21.170 --> 01:16:21.180
that there is there is a risk that what
 

01:16:21.180 --> 01:16:24.350
that there is there is a risk that what
we end up optimizing for and what we

01:16:24.350 --> 01:16:24.360
we end up optimizing for and what we
 

01:16:24.360 --> 01:16:26.330
we end up optimizing for and what we
want the robot to do are very different

01:16:26.330 --> 01:16:26.340
want the robot to do are very different
 

01:16:26.340 --> 01:16:28.280
want the robot to do are very different
but maybe one way I'd look at this is

01:16:28.280 --> 01:16:28.290
but maybe one way I'd look at this is
 

01:16:28.290 --> 01:16:32.630
but maybe one way I'd look at this is
that our objectives are still there is

01:16:32.630 --> 01:16:32.640
that our objectives are still there is
 

01:16:32.640 --> 01:16:34.880
that our objectives are still there is
to want to be a healthy person but I'm

01:16:34.880 --> 01:16:34.890
to want to be a healthy person but I'm
 

01:16:34.890 --> 01:16:39.200
to want to be a healthy person but I'm
pretty let's say myopic or greedy in my

01:16:39.200 --> 01:16:39.210
pretty let's say myopic or greedy in my
 

01:16:39.210 --> 01:16:43.340
pretty let's say myopic or greedy in my
planning horizon right so I end up being

01:16:43.340 --> 01:16:43.350
planning horizon right so I end up being
 

01:16:43.350 --> 01:16:45.800
planning horizon right so I end up being
tempted by the chocolate because I don't

01:16:45.800 --> 01:16:45.810
tempted by the chocolate because I don't
 

01:16:45.810 --> 01:16:48.260
tempted by the chocolate because I don't
necessarily think through in the moment

01:16:48.260 --> 01:16:48.270
necessarily think through in the moment
 

01:16:48.270 --> 01:16:49.730
necessarily think through in the moment
of all the implications this is gonna

01:16:49.730 --> 01:16:49.740
of all the implications this is gonna
 

01:16:49.740 --> 01:16:52.760
of all the implications this is gonna
have or maybe I'm just you know there's

01:16:52.760 --> 01:16:52.770
have or maybe I'm just you know there's
 

01:16:52.770 --> 01:16:54.560
have or maybe I'm just you know there's
one there's many ways to which I can be

01:16:54.560 --> 01:16:54.570
one there's many ways to which I can be
 

01:16:54.570 --> 01:16:57.380
one there's many ways to which I can be
suboptimal and what I'm going for and so

01:16:57.380 --> 01:16:57.390
suboptimal and what I'm going for and so
 

01:16:57.390 --> 01:17:00.650
suboptimal and what I'm going for and so
I think one important sort of line of

01:17:00.650 --> 01:17:00.660
I think one important sort of line of
 

01:17:00.660 --> 01:17:03.100
I think one important sort of line of
work to take when trying to look at

01:17:03.100 --> 01:17:03.110
work to take when trying to look at
 

01:17:03.110 --> 01:17:05.930
work to take when trying to look at
human behavior and sort of back out what

01:17:05.930 --> 01:17:05.940
human behavior and sort of back out what
 

01:17:05.940 --> 01:17:08.660
human behavior and sort of back out what
our incentives are is is to account for

01:17:08.660 --> 01:17:08.670
our incentives are is is to account for
 

01:17:08.670 --> 01:17:10.670
our incentives are is is to account for
the fact that people are not gonna be

01:17:10.670 --> 01:17:10.680
the fact that people are not gonna be
 

01:17:10.680 --> 01:17:12.620
the fact that people are not gonna be
optimal and serve therefore their

01:17:12.620 --> 01:17:12.630
optimal and serve therefore their
 

01:17:12.630 --> 01:17:15.050
optimal and serve therefore their
Corrections and their actions are not

01:17:15.050 --> 01:17:15.060
Corrections and their actions are not
 

01:17:15.060 --> 01:17:19.340
Corrections and their actions are not
necessarily gonna be what they would if

01:17:19.340 --> 01:17:19.350
necessarily gonna be what they would if
 

01:17:19.350 --> 01:17:22.370
necessarily gonna be what they would if
the person were actually truly optimal

01:17:22.370 --> 01:17:22.380
the person were actually truly optimal
 

01:17:22.380 --> 01:17:24.890
the person were actually truly optimal
perfectly optimal and what we've been

01:17:24.890 --> 01:17:24.900
perfectly optimal and what we've been
 

01:17:24.900 --> 01:17:27.020
perfectly optimal and what we've been
finding is that for instance if we

01:17:27.020 --> 01:17:27.030
finding is that for instance if we
 

01:17:27.030 --> 01:17:28.650
finding is that for instance if we
assume that the person

01:17:28.650 --> 01:17:28.660
assume that the person
 

01:17:28.660 --> 01:17:30.630
assume that the person
myopic right then we can do a much

01:17:30.630 --> 01:17:30.640
myopic right then we can do a much
 

01:17:30.640 --> 01:17:32.520
myopic right then we can do a much
better job recovering their true

01:17:32.520 --> 01:17:32.530
better job recovering their true
 

01:17:32.530 --> 01:17:34.230
better job recovering their true
objective function and sort of kind of

01:17:34.230 --> 01:17:34.240
objective function and sort of kind of
 

01:17:34.240 --> 01:17:36.360
objective function and sort of kind of
controlled experiments then if we assume

01:17:36.360 --> 01:17:36.370
controlled experiments then if we assume
 

01:17:36.370 --> 01:17:38.850
controlled experiments then if we assume
that they weren't right so I think it's

01:17:38.850 --> 01:17:38.860
that they weren't right so I think it's
 

01:17:38.860 --> 01:17:40.980
that they weren't right so I think it's
important for robots to actually take

01:17:40.980 --> 01:17:40.990
important for robots to actually take
 

01:17:40.990 --> 01:17:42.630
important for robots to actually take
these biases that we might have and

01:17:42.630 --> 01:17:42.640
these biases that we might have and
 

01:17:42.640 --> 01:17:43.980
these biases that we might have and
they're different ones they're just

01:17:43.980 --> 01:17:43.990
they're different ones they're just
 

01:17:43.990 --> 01:17:45.510
they're different ones they're just
planning fallacy there's all sorts of

01:17:45.510 --> 01:17:45.520
planning fallacy there's all sorts of
 

01:17:45.520 --> 01:17:47.730
planning fallacy there's all sorts of
different biases that people have but

01:17:47.730 --> 01:17:47.740
different biases that people have but
 

01:17:47.740 --> 01:17:49.500
different biases that people have but
but maybe that's one way to think about

01:17:49.500 --> 01:17:49.510
but maybe that's one way to think about
 

01:17:49.510 --> 01:17:51.480
but maybe that's one way to think about
it and that would enable the robot not

01:17:51.480 --> 01:17:51.490
it and that would enable the robot not
 

01:17:51.490 --> 01:17:53.400
it and that would enable the robot not
necessarily to identify what do we want

01:17:53.400 --> 01:17:53.410
necessarily to identify what do we want
 

01:17:53.410 --> 01:17:55.290
necessarily to identify what do we want
to be healthy or not but I think the key

01:17:55.290 --> 01:17:55.300
to be healthy or not but I think the key
 

01:17:55.300 --> 01:17:57.350
to be healthy or not but I think the key
to what I was talking about is really

01:17:57.350 --> 01:17:57.360
to what I was talking about is really
 

01:17:57.360 --> 01:18:00.930
to what I was talking about is really
get the right uncertainty right it's not

01:18:00.930 --> 01:18:00.940
get the right uncertainty right it's not
 

01:18:00.940 --> 01:18:02.670
get the right uncertainty right it's not
like the robot knows whether they're

01:18:02.670 --> 01:18:02.680
like the robot knows whether they're
 

01:18:02.680 --> 01:18:06.240
like the robot knows whether they're
winning or score is the right one it's

01:18:06.240 --> 01:18:06.250
winning or score is the right one it's
 

01:18:06.250 --> 01:18:07.920
winning or score is the right one it's
not like the robot knows that lava is

01:18:07.920 --> 01:18:07.930
not like the robot knows that lava is
 

01:18:07.930 --> 01:18:11.430
not like the robot knows that lava is
bad right it just knows that given what

01:18:11.430 --> 01:18:11.440
bad right it just knows that given what
 

01:18:11.440 --> 01:18:13.440
bad right it just knows that given what
you've said there's not enough evidence

01:18:13.440 --> 01:18:13.450
you've said there's not enough evidence
 

01:18:13.450 --> 01:18:15.600
you've said there's not enough evidence
of either way and it has uncertainty

01:18:15.600 --> 01:18:15.610
of either way and it has uncertainty
 

01:18:15.610 --> 01:18:17.010
of either way and it has uncertainty
about those and there's there's things

01:18:17.010 --> 01:18:17.020
about those and there's there's things
 

01:18:17.020 --> 01:18:18.210
about those and there's there's things
that it doesn't have uncertainty about

01:18:18.210 --> 01:18:18.220
that it doesn't have uncertainty about
 

01:18:18.220 --> 01:18:20.040
that it doesn't have uncertainty about
because they don't match up at all it's

01:18:20.040 --> 01:18:20.050
because they don't match up at all it's
 

01:18:20.050 --> 01:18:21.960
because they don't match up at all it's
not they're not possible hypotheses

01:18:21.960 --> 01:18:21.970
not they're not possible hypotheses
 

01:18:21.970 --> 01:18:23.640
not they're not possible hypotheses
being able to recognize some of those

01:18:23.640 --> 01:18:23.650
being able to recognize some of those
 

01:18:23.650 --> 01:18:25.800
being able to recognize some of those
known unknowns I mean something become a

01:18:25.800 --> 01:18:25.810
known unknowns I mean something become a
 

01:18:25.810 --> 01:18:27.300
known unknowns I mean something become a
known unknown to use the language from

01:18:27.300 --> 01:18:27.310
known unknown to use the language from
 

01:18:27.310 --> 01:18:29.520
known unknown to use the language from
America's talk that's tonight so I

01:18:29.520 --> 01:18:29.530
America's talk that's tonight so I
 

01:18:29.530 --> 01:18:31.470
America's talk that's tonight so I
wonder could you also use these same

01:18:31.470 --> 01:18:31.480
wonder could you also use these same
 

01:18:31.480 --> 01:18:35.550
wonder could you also use these same
sorts of techniques to handle potential

01:18:35.550 --> 01:18:35.560
sorts of techniques to handle potential
 

01:18:35.560 --> 01:18:37.050
sorts of techniques to handle potential
challenges where the robot has to

01:18:37.050 --> 01:18:37.060
challenges where the robot has to
 

01:18:37.060 --> 01:18:38.940
challenges where the robot has to
interact with multiple people so the

01:18:38.940 --> 01:18:38.950
interact with multiple people so the
 

01:18:38.950 --> 01:18:40.500
interact with multiple people so the
robot has to learn you know interacts

01:18:40.500 --> 01:18:40.510
robot has to learn you know interacts
 

01:18:40.510 --> 01:18:42.570
robot has to learn you know interacts
with you interacts with me and we might

01:18:42.570 --> 01:18:42.580
with you interacts with me and we might
 

01:18:42.580 --> 01:18:44.430
with you interacts with me and we might
have different reward functions where it

01:18:44.430 --> 01:18:44.440
have different reward functions where it
 

01:18:44.440 --> 01:18:46.770
have different reward functions where it
has to learn what's in common what's

01:18:46.770 --> 01:18:46.780
has to learn what's in common what's
 

01:18:46.780 --> 01:18:48.960
has to learn what's in common what's
different between us with the same kind

01:18:48.960 --> 01:18:48.970
different between us with the same kind
 

01:18:48.970 --> 01:18:50.430
different between us with the same kind
of techniques be usable for that

01:18:50.430 --> 01:18:50.440
of techniques be usable for that
 

01:18:50.440 --> 01:18:52.560
of techniques be usable for that
challenge good so one thing that we've

01:18:52.560 --> 01:18:52.570
challenge good so one thing that we've
 

01:18:52.570 --> 01:18:57.320
challenge good so one thing that we've
experimented with is if the robot can

01:18:57.320 --> 01:18:57.330
experimented with is if the robot can
 

01:18:57.330 --> 01:19:00.450
experimented with is if the robot can
learn from many people when it sees a

01:19:00.450 --> 01:19:00.460
learn from many people when it sees a
 

01:19:00.460 --> 01:19:01.800
learn from many people when it sees a
new person it shouldn't start from

01:19:01.800 --> 01:19:01.810
new person it shouldn't start from
 

01:19:01.810 --> 01:19:04.230
new person it shouldn't start from
scratch right so for instance we did

01:19:04.230 --> 01:19:04.240
scratch right so for instance we did
 

01:19:04.240 --> 01:19:05.940
scratch right so for instance we did
this in the context of say driving

01:19:05.940 --> 01:19:05.950
this in the context of say driving
 

01:19:05.950 --> 01:19:07.680
this in the context of say driving
styles right so I want the car to

01:19:07.680 --> 01:19:07.690
styles right so I want the car to
 

01:19:07.690 --> 01:19:09.360
styles right so I want the car to
customize to how you want to drive how

01:19:09.360 --> 01:19:09.370
customize to how you want to drive how
 

01:19:09.370 --> 01:19:10.530
customize to how you want to drive how
they want to drive how they want to

01:19:10.530 --> 01:19:10.540
they want to drive how they want to
 

01:19:10.540 --> 01:19:12.450
they want to drive how they want to
drive and when I see the end person I

01:19:12.450 --> 01:19:12.460
drive and when I see the end person I
 

01:19:12.460 --> 01:19:13.830
drive and when I see the end person I
don't want to start there sort of

01:19:13.830 --> 01:19:13.840
don't want to start there sort of
 

01:19:13.840 --> 01:19:14.910
don't want to start there sort of
learning from scratch because there's

01:19:14.910 --> 01:19:14.920
learning from scratch because there's
 

01:19:14.920 --> 01:19:16.860
learning from scratch because there's
common things that all people share and

01:19:16.860 --> 01:19:16.870
common things that all people share and
 

01:19:16.870 --> 01:19:18.840
common things that all people share and
then there's individual differences and

01:19:18.840 --> 01:19:18.850
then there's individual differences and
 

01:19:18.850 --> 01:19:20.930
then there's individual differences and
so one way I think about this is to

01:19:20.930 --> 01:19:20.940
so one way I think about this is to
 

01:19:20.940 --> 01:19:23.910
so one way I think about this is to
learn from your from all of your

01:19:23.910 --> 01:19:23.920
learn from your from all of your
 

01:19:23.920 --> 01:19:26.160
learn from your from all of your
previous users and build in a sense of

01:19:26.160 --> 01:19:26.170
previous users and build in a sense of
 

01:19:26.170 --> 01:19:28.830
previous users and build in a sense of
prior for for what you should expect

01:19:28.830 --> 01:19:28.840
prior for for what you should expect
 

01:19:28.840 --> 01:19:31.380
prior for for what you should expect
going in and then let that new person's

01:19:31.380 --> 01:19:31.390
going in and then let that new person's
 

01:19:31.390 --> 01:19:34.050
going in and then let that new person's
actions sort of and take those in as

01:19:34.050 --> 01:19:34.060
actions sort of and take those in as
 

01:19:34.060 --> 01:19:35.730
actions sort of and take those in as
evidence and turn that driver into a

01:19:35.730 --> 01:19:35.740
evidence and turn that driver into a
 

01:19:35.740 --> 01:19:36.520
evidence and turn that driver into a
voice

01:19:36.520 --> 01:19:36.530
voice
 

01:19:36.530 --> 01:19:39.280
voice
so can thereby sort of find what's

01:19:39.280 --> 01:19:39.290
so can thereby sort of find what's
 

01:19:39.290 --> 01:19:40.990
so can thereby sort of find what's
similar among all of us while still

01:19:40.990 --> 01:19:41.000
similar among all of us while still
 

01:19:41.000 --> 01:19:43.150
similar among all of us while still
allowing for the fact that people do

01:19:43.150 --> 01:19:43.160
allowing for the fact that people do
 

01:19:43.160 --> 01:19:44.920
allowing for the fact that people do
drive differently or want the robot arm

01:19:44.920 --> 01:19:44.930
drive differently or want the robot arm
 

01:19:44.930 --> 01:19:46.720
drive differently or want the robot arm
to move differently the kind of

01:19:46.720 --> 01:19:46.730
to move differently the kind of
 

01:19:46.730 --> 01:19:48.700
to move differently the kind of
individual customization that's right

01:19:48.700 --> 01:19:48.710
individual customization that's right
 

01:19:48.710 --> 01:19:49.420
individual customization that's right
that's right

01:19:49.420 --> 01:19:49.430
that's right
 

01:19:49.430 --> 01:19:52.390
that's right
but I do want to bring up a related

01:19:52.390 --> 01:19:52.400
but I do want to bring up a related
 

01:19:52.400 --> 01:19:56.820
but I do want to bring up a related
point that I struggle with which is the

01:19:56.820 --> 01:19:56.830
point that I struggle with which is the
 

01:19:56.830 --> 01:20:00.070
point that I struggle with which is the
car has multiple passengers which

01:20:00.070 --> 01:20:00.080
car has multiple passengers which
 

01:20:00.080 --> 01:20:02.680
car has multiple passengers which
driving style should I choose I put a

01:20:02.680 --> 01:20:02.690
driving style should I choose I put a
 

01:20:02.690 --> 01:20:04.510
driving style should I choose I put a
robot in a home and there's different

01:20:04.510 --> 01:20:04.520
robot in a home and there's different
 

01:20:04.520 --> 01:20:06.430
robot in a home and there's different
people who all have different

01:20:06.430 --> 01:20:06.440
people who all have different
 

01:20:06.440 --> 01:20:08.850
people who all have different
preferences which of them write what

01:20:08.850 --> 01:20:08.860
preferences which of them write what
 

01:20:08.860 --> 01:20:11.950
preferences which of them write what
combination I think all the tools that I

01:20:11.950 --> 01:20:11.960
combination I think all the tools that I
 

01:20:11.960 --> 01:20:13.540
combination I think all the tools that I
was talking about and all everything

01:20:13.540 --> 01:20:13.550
was talking about and all everything
 

01:20:13.550 --> 01:20:16.480
was talking about and all everything
that I've done has been very much along

01:20:16.480 --> 01:20:16.490
that I've done has been very much along
 

01:20:16.490 --> 01:20:20.860
that I've done has been very much along
the line of one one person one robot and

01:20:20.860 --> 01:20:20.870
the line of one one person one robot and
 

01:20:20.870 --> 01:20:22.960
the line of one one person one robot and
the robot is trying to learn these sort

01:20:22.960 --> 01:20:22.970
the robot is trying to learn these sort
 

01:20:22.970 --> 01:20:24.280
the robot is trying to learn these sort
of the internal objective that the

01:20:24.280 --> 01:20:24.290
of the internal objective that the
 

01:20:24.290 --> 01:20:26.110
of the internal objective that the
person wants the robot to be optimizing

01:20:26.110 --> 01:20:26.120
person wants the robot to be optimizing
 

01:20:26.120 --> 01:20:28.960
person wants the robot to be optimizing
and I don't know what to do when there's

01:20:28.960 --> 01:20:28.970
and I don't know what to do when there's
 

01:20:28.970 --> 01:20:30.340
and I don't know what to do when there's
multiple people I mean there's simple

01:20:30.340 --> 01:20:30.350
multiple people I mean there's simple
 

01:20:30.350 --> 01:20:31.570
multiple people I mean there's simple
things you could do like you could take

01:20:31.570 --> 01:20:31.580
things you could do like you could take
 

01:20:31.580 --> 01:20:33.550
things you could do like you could take
averages you could do sort of risk

01:20:33.550 --> 01:20:33.560
averages you could do sort of risk
 

01:20:33.560 --> 01:20:36.340
averages you could do sort of risk
averse you could do min max you could do

01:20:36.340 --> 01:20:36.350
averse you could do min max you could do
 

01:20:36.350 --> 01:20:39.520
averse you could do min max you could do
max min there's things like that but I

01:20:39.520 --> 01:20:39.530
max min there's things like that but I
 

01:20:39.530 --> 01:20:41.320
max min there's things like that but I
don't know what the right answer is well

01:20:41.320 --> 01:20:41.330
don't know what the right answer is well
 

01:20:41.330 --> 01:20:43.060
don't know what the right answer is well
I think in fairness to you

01:20:43.060 --> 01:20:43.070
I think in fairness to you
 

01:20:43.070 --> 01:20:44.800
I think in fairness to you
I don't think social scientists know I

01:20:44.800 --> 01:20:44.810
I don't think social scientists know I
 

01:20:44.810 --> 01:20:46.360
I don't think social scientists know I
don't think political scientists know I

01:20:46.360 --> 01:20:46.370
don't think political scientists know I
 

01:20:46.370 --> 01:20:48.100
don't think political scientists know I
mean this is sort of the ubiquity

01:20:48.100 --> 01:20:48.110
mean this is sort of the ubiquity
 

01:20:48.110 --> 01:20:50.290
mean this is sort of the ubiquity
ubiquitous problem in human experiences

01:20:50.290 --> 01:20:50.300
ubiquitous problem in human experiences
 

01:20:50.300 --> 01:20:51.550
ubiquitous problem in human experiences
what do you have when you're conflicting

01:20:51.550 --> 01:20:51.560
what do you have when you're conflicting
 

01:20:51.560 --> 01:20:53.530
what do you have when you're conflicting
values and conflicting interests I think

01:20:53.530 --> 01:20:53.540
values and conflicting interests I think
 

01:20:53.540 --> 01:20:55.330
values and conflicting interests I think
it's okay if robot assists haven't

01:20:55.330 --> 01:20:55.340
it's okay if robot assists haven't
 

01:20:55.340 --> 01:20:57.850
it's okay if robot assists haven't
solved this problem for us yet but we'll

01:20:57.850 --> 01:20:57.860
solved this problem for us yet but we'll
 

01:20:57.860 --> 01:21:00.550
solved this problem for us yet but we'll
look forward to it in a few years and I

01:21:00.550 --> 01:21:00.560
look forward to it in a few years and I
 

01:21:00.560 --> 01:21:01.720
look forward to it in a few years and I
guess speaking of a few years I'm

01:21:01.720 --> 01:21:01.730
guess speaking of a few years I'm
 

01:21:01.730 --> 01:21:03.130
guess speaking of a few years I'm
curious what do you see is sort of the

01:21:03.130 --> 01:21:03.140
curious what do you see is sort of the
 

01:21:03.140 --> 01:21:06.220
curious what do you see is sort of the
biggest opportunities for trust AI

01:21:06.220 --> 01:21:06.230
biggest opportunities for trust AI
 

01:21:06.230 --> 01:21:08.410
biggest opportunities for trust AI
robotics sort of in this space over the

01:21:08.410 --> 01:21:08.420
robotics sort of in this space over the
 

01:21:08.420 --> 01:21:10.840
robotics sort of in this space over the
next let's say five to ten years I think

01:21:10.840 --> 01:21:10.850
next let's say five to ten years I think
 

01:21:10.850 --> 01:21:13.540
next let's say five to ten years I think
I'm gonna go with basically Eli's answer

01:21:13.540 --> 01:21:13.550
I'm gonna go with basically Eli's answer
 

01:21:13.550 --> 01:21:18.690
I'm gonna go with basically Eli's answer
because I feel like as AI capability

01:21:18.690 --> 01:21:18.700
because I feel like as AI capability
 

01:21:18.700 --> 01:21:21.400
because I feel like as AI capability
advances and I'm much more pessimistic

01:21:21.400 --> 01:21:21.410
advances and I'm much more pessimistic
 

01:21:21.410 --> 01:21:23.530
advances and I'm much more pessimistic
about the degree of that advanced than

01:21:23.530 --> 01:21:23.540
about the degree of that advanced than
 

01:21:23.540 --> 01:21:24.850
about the degree of that advanced than
other people are but I think we are

01:21:24.850 --> 01:21:24.860
other people are but I think we are
 

01:21:24.860 --> 01:21:30.430
other people are but I think we are
making progress I think with that we get

01:21:30.430 --> 01:21:30.440
making progress I think with that we get
 

01:21:30.440 --> 01:21:32.500
making progress I think with that we get
this opportunity to help people be

01:21:32.500 --> 01:21:32.510
this opportunity to help people be
 

01:21:32.510 --> 01:21:35.290
this opportunity to help people be
better at the things that they do so you

01:21:35.290 --> 01:21:35.300
better at the things that they do so you
 

01:21:35.300 --> 01:21:37.150
better at the things that they do so you
know kind of talking about what we were

01:21:37.150 --> 01:21:37.160
know kind of talking about what we were
 

01:21:37.160 --> 01:21:38.530
know kind of talking about what we were
saying earlier where people tend to be

01:21:38.530 --> 01:21:38.540
saying earlier where people tend to be
 

01:21:38.540 --> 01:21:40.780
saying earlier where people tend to be
myopic well one thing that we found is

01:21:40.780 --> 01:21:40.790
myopic well one thing that we found is
 

01:21:40.790 --> 01:21:43.570
myopic well one thing that we found is
that it when the robot is on the same

01:21:43.570 --> 01:21:43.580
that it when the robot is on the same
 

01:21:43.580 --> 01:21:45.879
that it when the robot is on the same
team with you and you're trying to do it

01:21:45.879 --> 01:21:45.889
team with you and you're trying to do it
 

01:21:45.889 --> 01:21:49.540
team with you and you're trying to do it
together if the robot acknowledges that

01:21:49.540 --> 01:21:49.550
together if the robot acknowledges that
 

01:21:49.550 --> 01:21:51.669
together if the robot acknowledges that
you maybe don't tend to think ten steps

01:21:51.669 --> 01:21:51.679
you maybe don't tend to think ten steps
 

01:21:51.679 --> 01:21:54.819
you maybe don't tend to think ten steps
ahead it can sort of organize the world

01:21:54.819 --> 01:21:54.829
ahead it can sort of organize the world
 

01:21:54.829 --> 01:21:56.919
ahead it can sort of organize the world
and take actions in such a way that when

01:21:56.919 --> 01:21:56.929
and take actions in such a way that when
 

01:21:56.929 --> 01:22:00.819
and take actions in such a way that when
you then react to those actions that

01:22:00.819 --> 01:22:00.829
you then react to those actions that
 

01:22:00.829 --> 01:22:04.419
you then react to those actions that
reaction is good globally as well simple

01:22:04.419 --> 01:22:04.429
reaction is good globally as well simple
 

01:22:04.429 --> 01:22:05.319
reaction is good globally as well simple
example because I think what I'm saying

01:22:05.319 --> 01:22:05.329
example because I think what I'm saying
 

01:22:05.329 --> 01:22:07.810
example because I think what I'm saying
is very abstract it can be as simple as

01:22:07.810 --> 01:22:07.820
is very abstract it can be as simple as
 

01:22:07.820 --> 01:22:11.469
is very abstract it can be as simple as
when I give when the robot gives you an

01:22:11.469 --> 01:22:11.479
when I give when the robot gives you an
 

01:22:11.479 --> 01:22:14.799
when I give when the robot gives you an
object okay take it okay you take it in

01:22:14.799 --> 01:22:14.809
object okay take it okay you take it in
 

01:22:14.809 --> 01:22:17.139
object okay take it okay you take it in
the most comfortable way for you now

01:22:17.139 --> 01:22:17.149
the most comfortable way for you now
 

01:22:17.149 --> 01:22:19.029
the most comfortable way for you now
imagine you had an actual goal like you

01:22:19.029 --> 01:22:19.039
imagine you had an actual goal like you
 

01:22:19.039 --> 01:22:20.649
imagine you had an actual goal like you
have to put in a dishwasher or something

01:22:20.649 --> 01:22:20.659
have to put in a dishwasher or something
 

01:22:20.659 --> 01:22:23.709
have to put in a dishwasher or something
right we don't tend to necessarily think

01:22:23.709 --> 01:22:23.719
right we don't tend to necessarily think
 

01:22:23.719 --> 01:22:25.509
right we don't tend to necessarily think
ahead to the goal

01:22:25.509 --> 01:22:25.519
ahead to the goal
 

01:22:25.519 --> 01:22:28.060
ahead to the goal
we just we do the thing right not always

01:22:28.060 --> 01:22:28.070
we just we do the thing right not always
 

01:22:28.070 --> 01:22:29.770
we just we do the thing right not always
and not for everyone but especially if

01:22:29.770 --> 01:22:29.780
and not for everyone but especially if
 

01:22:29.780 --> 01:22:31.509
and not for everyone but especially if
we're wrapped up with other things and

01:22:31.509 --> 01:22:31.519
we're wrapped up with other things and
 

01:22:31.519 --> 01:22:33.669
we're wrapped up with other things and
one thing that the robot could do is can

01:22:33.669 --> 01:22:33.679
one thing that the robot could do is can
 

01:22:33.679 --> 01:22:35.770
one thing that the robot could do is can
be reasoning about that and then he and

01:22:35.770 --> 01:22:35.780
be reasoning about that and then he and
 

01:22:35.780 --> 01:22:38.350
be reasoning about that and then he and
you the thing sort of in a way where it

01:22:38.350 --> 01:22:38.360
you the thing sort of in a way where it
 

01:22:38.360 --> 01:22:41.500
you the thing sort of in a way where it
makes sure that the way you then grab it

01:22:41.500 --> 01:22:41.510
makes sure that the way you then grab it
 

01:22:41.510 --> 01:22:43.689
makes sure that the way you then grab it
is then conducive to your goal so use

01:22:43.689 --> 01:22:43.699
is then conducive to your goal so use
 

01:22:43.699 --> 01:22:45.310
is then conducive to your goal so use
things you still react myopically but

01:22:45.310 --> 01:22:45.320
things you still react myopically but
 

01:22:45.320 --> 01:22:46.629
things you still react myopically but
the robot is setting things up such that

01:22:46.629 --> 01:22:46.639
the robot is setting things up such that
 

01:22:46.639 --> 01:22:49.109
the robot is setting things up such that
you're actually better at the task

01:22:49.109 --> 01:22:49.119
you're actually better at the task
 

01:22:49.119 --> 01:22:52.000
you're actually better at the task
because of its actions or another thing

01:22:52.000 --> 01:22:52.010
because of its actions or another thing
 

01:22:52.010 --> 01:22:55.359
because of its actions or another thing
is going back to explain ability it'd be

01:22:55.359 --> 01:22:55.369
is going back to explain ability it'd be
 

01:22:55.369 --> 01:22:57.810
is going back to explain ability it'd be
nice and I think people are doing that

01:22:57.810 --> 01:22:57.820
nice and I think people are doing that
 

01:22:57.820 --> 01:23:00.580
nice and I think people are doing that
because robots are getting better and

01:23:00.580 --> 01:23:00.590
because robots are getting better and
 

01:23:00.590 --> 01:23:02.169
because robots are getting better and
better things like go and so on and

01:23:02.169 --> 01:23:02.179
better things like go and so on and
 

01:23:02.179 --> 01:23:04.810
better things like go and so on and
whatever other tasks better than human

01:23:04.810 --> 01:23:04.820
whatever other tasks better than human
 

01:23:04.820 --> 01:23:07.659
whatever other tasks better than human
capability it'd be nice if that made us

01:23:07.659 --> 01:23:07.669
capability it'd be nice if that made us
 

01:23:07.669 --> 01:23:10.239
capability it'd be nice if that made us
better as well so robot could kind of

01:23:10.239 --> 01:23:10.249
better as well so robot could kind of
 

01:23:10.249 --> 01:23:12.339
better as well so robot could kind of
help you make these decisions and say if

01:23:12.339 --> 01:23:12.349
help you make these decisions and say if
 

01:23:12.349 --> 01:23:14.529
help you make these decisions and say if
you do this just be careful that you

01:23:14.529 --> 01:23:14.539
you do this just be careful that you
 

01:23:14.539 --> 01:23:16.569
you do this just be careful that you
know 10 moves from now you might end up

01:23:16.569 --> 01:23:16.579
know 10 moves from now you might end up
 

01:23:16.579 --> 01:23:18.520
know 10 moves from now you might end up
here and here's why and then the person

01:23:18.520 --> 01:23:18.530
here and here's why and then the person
 

01:23:18.530 --> 01:23:19.419
here and here's why and then the person
can learn from that

01:23:19.419 --> 01:23:19.429
can learn from that
 

01:23:19.429 --> 01:23:22.389
can learn from that
and get better and so so just to

01:23:22.389 --> 01:23:22.399
and get better and so so just to
 

01:23:22.399 --> 01:23:25.359
and get better and so so just to
conclude the more speculative question

01:23:25.359 --> 01:23:25.369
conclude the more speculative question
 

01:23:25.369 --> 01:23:28.540
conclude the more speculative question
so in say 10 years when we're on I guess

01:23:28.540 --> 01:23:28.550
so in say 10 years when we're on I guess
 

01:23:28.550 --> 01:23:31.629
so in say 10 years when we're on I guess
that would be the sixth CMU K&amp;L gates

01:23:31.629 --> 01:23:31.639
that would be the sixth CMU K&amp;L gates
 

01:23:31.639 --> 01:23:34.270
that would be the sixth CMU K&amp;L gates
conference how is it going to be

01:23:34.270 --> 01:23:34.280
conference how is it going to be
 

01:23:34.280 --> 01:23:35.799
conference how is it going to be
different how do you think things are

01:23:35.799 --> 01:23:35.809
different how do you think things are
 

01:23:35.809 --> 01:23:38.229
different how do you think things are
going to have changed either socially or

01:23:38.229 --> 01:23:38.239
going to have changed either socially or
 

01:23:38.239 --> 01:23:42.040
going to have changed either socially or
in this that we're talking about here

01:23:42.040 --> 01:23:42.050
in this that we're talking about here
 

01:23:42.050 --> 01:23:45.810
in this that we're talking about here
I guess my prediction would be that

01:23:45.810 --> 01:23:45.820
I guess my prediction would be that
 

01:23:45.820 --> 01:23:49.479
I guess my prediction would be that
we're gonna start talking even more

01:23:49.479 --> 01:23:49.489
we're gonna start talking even more
 

01:23:49.489 --> 01:23:51.729
we're gonna start talking even more
about what I might call model Mis

01:23:51.729 --> 01:23:51.739
about what I might call model Mis
 

01:23:51.739 --> 01:23:53.919
about what I might call model Mis
specification so that's one thing that I

01:23:53.919 --> 01:23:53.929
specification so that's one thing that I
 

01:23:53.929 --> 01:23:55.899
specification so that's one thing that I
kind of foresee being very worried about

01:23:55.899 --> 01:23:55.909
kind of foresee being very worried about
 

01:23:55.909 --> 01:23:57.489
kind of foresee being very worried about
because so what do you mean by that

01:23:57.489 --> 01:23:57.499
because so what do you mean by that
 

01:23:57.499 --> 01:23:57.940
because so what do you mean by that
right

01:23:57.940 --> 01:23:57.950
right
 

01:23:57.950 --> 01:24:02.170
right
so we talked a lot about trying to

01:24:02.170 --> 01:24:02.180
so we talked a lot about trying to
 

01:24:02.180 --> 01:24:04.450
so we talked a lot about trying to
figure out what people want and I'm

01:24:04.450 --> 01:24:04.460
figure out what people want and I'm
 

01:24:04.460 --> 01:24:07.180
figure out what people want and I'm
worried that oftentimes the space that

01:24:07.180 --> 01:24:07.190
worried that oftentimes the space that
 

01:24:07.190 --> 01:24:09.040
worried that oftentimes the space that
we're we're always looking at a kind of

01:24:09.040 --> 01:24:09.050
we're we're always looking at a kind of
 

01:24:09.050 --> 01:24:10.630
we're we're always looking at a kind of
a closed world assumption here are the

01:24:10.630 --> 01:24:10.640
a closed world assumption here are the
 

01:24:10.640 --> 01:24:12.460
a closed world assumption here are the
possible things even if they're defined

01:24:12.460 --> 01:24:12.470
possible things even if they're defined
 

01:24:12.470 --> 01:24:14.470
possible things even if they're defined
on sort of raw information you're the

01:24:14.470 --> 01:24:14.480
on sort of raw information you're the
 

01:24:14.480 --> 01:24:16.450
on sort of raw information you're the
possible things that a person could want

01:24:16.450 --> 01:24:16.460
possible things that a person could want
 

01:24:16.460 --> 01:24:18.280
possible things that a person could want
and I just worried that we don't have

01:24:18.280 --> 01:24:18.290
and I just worried that we don't have
 

01:24:18.290 --> 01:24:20.860
and I just worried that we don't have
even observations about things that

01:24:20.860 --> 01:24:20.870
even observations about things that
 

01:24:20.870 --> 01:24:22.510
even observations about things that
people care about that influence what

01:24:22.510 --> 01:24:22.520
people care about that influence what
 

01:24:22.520 --> 01:24:25.540
people care about that influence what
they want and so kind of model Mis

01:24:25.540 --> 01:24:25.550
they want and so kind of model Mis
 

01:24:25.550 --> 01:24:27.100
they want and so kind of model Mis
specification drawing these inferences

01:24:27.100 --> 01:24:27.110
specification drawing these inferences
 

01:24:27.110 --> 01:24:30.130
specification drawing these inferences
over a closed world when we just we in

01:24:30.130 --> 01:24:30.140
over a closed world when we just we in
 

01:24:30.140 --> 01:24:33.010
over a closed world when we just we in
fact have no way of knowing basically

01:24:33.010 --> 01:24:33.020
fact have no way of knowing basically
 

01:24:33.020 --> 01:24:35.740
fact have no way of knowing basically
leads to lead can lead to issues another

01:24:35.740 --> 01:24:35.750
leads to lead can lead to issues another
 

01:24:35.750 --> 01:24:37.030
leads to lead can lead to issues another
thing they can lead to issue is just

01:24:37.030 --> 01:24:37.040
thing they can lead to issue is just
 

01:24:37.040 --> 01:24:39.640
thing they can lead to issue is just
inherent ambiguity where I really can't

01:24:39.640 --> 01:24:39.650
inherent ambiguity where I really can't
 

01:24:39.650 --> 01:24:41.260
inherent ambiguity where I really can't
figure out based on what you're doing or

01:24:41.260 --> 01:24:41.270
figure out based on what you're doing or
 

01:24:41.270 --> 01:24:43.270
figure out based on what you're doing or
what you're saying if you want this or

01:24:43.270 --> 01:24:43.280
what you're saying if you want this or
 

01:24:43.280 --> 01:24:45.640
what you're saying if you want this or
better that so then I'm sort of I need

01:24:45.640 --> 01:24:45.650
better that so then I'm sort of I need
 

01:24:45.650 --> 01:24:47.590
better that so then I'm sort of I need
to be risking risk-averse or

01:24:47.590 --> 01:24:47.600
to be risking risk-averse or
 

01:24:47.600 --> 01:24:49.090
to be risking risk-averse or
conservative but that's the best I can

01:24:49.090 --> 01:24:49.100
conservative but that's the best I can
 

01:24:49.100 --> 01:24:51.190
conservative but that's the best I can
do the other thing that I think will

01:24:51.190 --> 01:24:51.200
do the other thing that I think will
 

01:24:51.200 --> 01:24:54.070
do the other thing that I think will
change is is maybe the hype for AI will

01:24:54.070 --> 01:24:54.080
change is is maybe the hype for AI will
 

01:24:54.080 --> 01:24:57.190
change is is maybe the hype for AI will
die down and will get welcome to focus

01:24:57.190 --> 01:24:57.200
die down and will get welcome to focus
 

01:24:57.200 --> 01:25:00.580
die down and will get welcome to focus
much more on these very kind of I don't

01:25:00.580 --> 01:25:00.590
much more on these very kind of I don't
 

01:25:00.590 --> 01:25:03.490
much more on these very kind of I don't
know near-term very practical ethical

01:25:03.490 --> 01:25:03.500
know near-term very practical ethical
 

01:25:03.500 --> 01:25:06.730
know near-term very practical ethical
issues and I think this particular

01:25:06.730 --> 01:25:06.740
issues and I think this particular
 

01:25:06.740 --> 01:25:08.470
issues and I think this particular
conference is a really good job with

01:25:08.470 --> 01:25:08.480
conference is a really good job with
 

01:25:08.480 --> 01:25:10.330
conference is a really good job with
that but I've been at kind of other

01:25:10.330 --> 01:25:10.340
that but I've been at kind of other
 

01:25:10.340 --> 01:25:12.790
that but I've been at kind of other
types of events where it's you know it's

01:25:12.790 --> 01:25:12.800
types of events where it's you know it's
 

01:25:12.800 --> 01:25:14.440
types of events where it's you know it's
sort of very far out that everyone's

01:25:14.440 --> 01:25:14.450
sort of very far out that everyone's
 

01:25:14.450 --> 01:25:16.600
sort of very far out that everyone's
worried that in 10 years artificial

01:25:16.600 --> 01:25:16.610
worried that in 10 years artificial
 

01:25:16.610 --> 01:25:18.520
worried that in 10 years artificial
general intelligence will happen and I

01:25:18.520 --> 01:25:18.530
general intelligence will happen and I
 

01:25:18.530 --> 01:25:21.760
general intelligence will happen and I
think my prediction is that you know it

01:25:21.760 --> 01:25:21.770
think my prediction is that you know it
 

01:25:21.770 --> 01:25:25.000
think my prediction is that you know it
won't and then the hype will be a little

01:25:25.000 --> 01:25:25.010
won't and then the hype will be a little
 

01:25:25.010 --> 01:25:26.530
won't and then the hype will be a little
more reduced then we'll be able to talk

01:25:26.530 --> 01:25:26.540
more reduced then we'll be able to talk
 

01:25:26.540 --> 01:25:28.540
more reduced then we'll be able to talk
about real kind of what yellow is saying

01:25:28.540 --> 01:25:28.550
about real kind of what yellow is saying
 

01:25:28.550 --> 01:25:31.060
about real kind of what yellow is saying
you know real kind of narrow AI that

01:25:31.060 --> 01:25:31.070
you know real kind of narrow AI that
 

01:25:31.070 --> 01:25:33.370
you know real kind of narrow AI that
does something really useful which can

01:25:33.370 --> 01:25:33.380
does something really useful which can
 

01:25:33.380 --> 01:25:35.830
does something really useful which can
both empower and and cause challenges

01:25:35.830 --> 01:25:35.840
both empower and and cause challenges
 

01:25:35.840 --> 01:25:37.000
both empower and and cause challenges
for people that's right

01:25:37.000 --> 01:25:37.010
for people that's right
 

01:25:37.010 --> 01:25:38.290
for people that's right
well thank you so much this is really

01:25:38.290 --> 01:25:38.300
well thank you so much this is really
 

01:25:38.300 --> 01:25:40.950
well thank you so much this is really
wonderful

01:25:40.950 --> 01:25:40.960
 
 

01:25:40.960 --> 01:25:44.730
 
and this this concludes our session on

01:25:44.730 --> 01:25:44.740
and this this concludes our session on
 

01:25:44.740 --> 01:25:47.460
and this this concludes our session on
trust I invite you all to stay and have

01:25:47.460 --> 01:25:47.470
trust I invite you all to stay and have
 

01:25:47.470 --> 01:25:49.200
trust I invite you all to stay and have
lunch in the back room please feel free

01:25:49.200 --> 01:25:49.210
lunch in the back room please feel free
 

01:25:49.210 --> 01:25:50.760
lunch in the back room please feel free
to come back up to these tables it's

01:25:50.760 --> 01:25:50.770
to come back up to these tables it's
 

01:25:50.770 --> 01:25:52.620
to come back up to these tables it's
perfectly fine to have food and beverage

01:25:52.620 --> 01:25:52.630
perfectly fine to have food and beverage
 

01:25:52.630 --> 01:25:54.690
perfectly fine to have food and beverage
here and we will be starting back up at

01:25:54.690 --> 01:25:54.700
here and we will be starting back up at
 

01:25:54.700 --> 01:25:56.940
here and we will be starting back up at
noon with our session on policy and

01:25:56.940 --> 01:25:56.950
noon with our session on policy and
 

01:25:56.950 --> 01:26:00.510
noon with our session on policy and
governance Thanks

