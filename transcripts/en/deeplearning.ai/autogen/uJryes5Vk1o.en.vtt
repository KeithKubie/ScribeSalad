WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:02.149
you've seen the logistic regression

00:00:02.149 --> 00:00:02.159
you've seen the logistic regression
 

00:00:02.159 --> 00:00:04.190
you've seen the logistic regression
model you're seeing the loss function

00:00:04.190 --> 00:00:04.200
model you're seeing the loss function
 

00:00:04.200 --> 00:00:06.380
model you're seeing the loss function
that measures how well you're doing on a

00:00:06.380 --> 00:00:06.390
that measures how well you're doing on a
 

00:00:06.390 --> 00:00:09.290
that measures how well you're doing on a
single training example you've also seen

00:00:09.290 --> 00:00:09.300
single training example you've also seen
 

00:00:09.300 --> 00:00:11.540
single training example you've also seen
the cost function that measures how well

00:00:11.540 --> 00:00:11.550
the cost function that measures how well
 

00:00:11.550 --> 00:00:14.629
the cost function that measures how well
your parameters W and B are doing on

00:00:14.629 --> 00:00:14.639
your parameters W and B are doing on
 

00:00:14.639 --> 00:00:17.390
your parameters W and B are doing on
your entire training set now let's talk

00:00:17.390 --> 00:00:17.400
your entire training set now let's talk
 

00:00:17.400 --> 00:00:19.460
your entire training set now let's talk
about how you can use the gradient

00:00:19.460 --> 00:00:19.470
about how you can use the gradient
 

00:00:19.470 --> 00:00:21.980
about how you can use the gradient
descent algorithm to train or to learn

00:00:21.980 --> 00:00:21.990
descent algorithm to train or to learn
 

00:00:21.990 --> 00:00:25.730
descent algorithm to train or to learn
the parameters W on your training set to

00:00:25.730 --> 00:00:25.740
the parameters W on your training set to
 

00:00:25.740 --> 00:00:29.029
the parameters W on your training set to
recap here is the familiar logistic

00:00:29.029 --> 00:00:29.039
recap here is the familiar logistic
 

00:00:29.039 --> 00:00:32.120
recap here is the familiar logistic
regression algorithm and we have on the

00:00:32.120 --> 00:00:32.130
regression algorithm and we have on the
 

00:00:32.130 --> 00:00:35.240
regression algorithm and we have on the
second line the cost function J which is

00:00:35.240 --> 00:00:35.250
second line the cost function J which is
 

00:00:35.250 --> 00:00:37.190
second line the cost function J which is
a function of your parameters W and B

00:00:37.190 --> 00:00:37.200
a function of your parameters W and B
 

00:00:37.200 --> 00:00:40.040
a function of your parameters W and B
and that's define as the average so it's

00:00:40.040 --> 00:00:40.050
and that's define as the average so it's
 

00:00:40.050 --> 00:00:42.560
and that's define as the average so it's
1 over m times of some of this loss

00:00:42.560 --> 00:00:42.570
1 over m times of some of this loss
 

00:00:42.570 --> 00:00:44.950
1 over m times of some of this loss
function and so the loss function

00:00:44.950 --> 00:00:44.960
function and so the loss function
 

00:00:44.960 --> 00:00:47.869
function and so the loss function
measures how well your algorithms

00:00:47.869 --> 00:00:47.879
measures how well your algorithms
 

00:00:47.879 --> 00:00:50.660
measures how well your algorithms
outputs y hat I on each of the training

00:00:50.660 --> 00:00:50.670
outputs y hat I on each of the training
 

00:00:50.670 --> 00:00:54.049
outputs y hat I on each of the training
examples stacks up compares to the

00:00:54.049 --> 00:00:54.059
examples stacks up compares to the
 

00:00:54.059 --> 00:00:56.450
examples stacks up compares to the
ground truth label Y I on each of the

00:00:56.450 --> 00:00:56.460
ground truth label Y I on each of the
 

00:00:56.460 --> 00:00:58.610
ground truth label Y I on each of the
training examples and the full formula

00:00:58.610 --> 00:00:58.620
training examples and the full formula
 

00:00:58.620 --> 00:01:01.279
training examples and the full formula
is expanded out on the right so the cost

00:01:01.279 --> 00:01:01.289
is expanded out on the right so the cost
 

00:01:01.289 --> 00:01:03.170
is expanded out on the right so the cost
function measures how well your

00:01:03.170 --> 00:01:03.180
function measures how well your
 

00:01:03.180 --> 00:01:05.479
function measures how well your
parameters W and B are doing on the

00:01:05.479 --> 00:01:05.489
parameters W and B are doing on the
 

00:01:05.489 --> 00:01:08.000
parameters W and B are doing on the
training set so in order to learn the

00:01:08.000 --> 00:01:08.010
training set so in order to learn the
 

00:01:08.010 --> 00:01:10.340
training set so in order to learn the
set of parameters W and V it seems

00:01:10.340 --> 00:01:10.350
set of parameters W and V it seems
 

00:01:10.350 --> 00:01:13.460
set of parameters W and V it seems
natural that we want to find W B that

00:01:13.460 --> 00:01:13.470
natural that we want to find W B that
 

00:01:13.470 --> 00:01:16.160
natural that we want to find W B that
make the cost function J of W comma be

00:01:16.160 --> 00:01:16.170
make the cost function J of W comma be
 

00:01:16.170 --> 00:01:18.499
make the cost function J of W comma be
as small as possible so here's an

00:01:18.499 --> 00:01:18.509
as small as possible so here's an
 

00:01:18.509 --> 00:01:21.560
as small as possible so here's an
illustration of gradient descent in this

00:01:21.560 --> 00:01:21.570
illustration of gradient descent in this
 

00:01:21.570 --> 00:01:24.340
illustration of gradient descent in this
diagram the horizontal axes represent

00:01:24.340 --> 00:01:24.350
diagram the horizontal axes represent
 

00:01:24.350 --> 00:01:28.370
diagram the horizontal axes represent
your space of parameters W and B in

00:01:28.370 --> 00:01:28.380
your space of parameters W and B in
 

00:01:28.380 --> 00:01:30.560
your space of parameters W and B in
practice W can be much higher

00:01:30.560 --> 00:01:30.570
practice W can be much higher
 

00:01:30.570 --> 00:01:32.210
practice W can be much higher
dimensional but for the purposes of

00:01:32.210 --> 00:01:32.220
dimensional but for the purposes of
 

00:01:32.220 --> 00:01:35.030
dimensional but for the purposes of
plotting let's illustrate W as a single

00:01:35.030 --> 00:01:35.040
plotting let's illustrate W as a single
 

00:01:35.040 --> 00:01:37.249
plotting let's illustrate W as a single
real number and B as a single real

00:01:37.249 --> 00:01:37.259
real number and B as a single real
 

00:01:37.259 --> 00:01:40.520
real number and B as a single real
number the cost function J of W B is

00:01:40.520 --> 00:01:40.530
number the cost function J of W B is
 

00:01:40.530 --> 00:01:43.730
number the cost function J of W B is
then some surface above these horizontal

00:01:43.730 --> 00:01:43.740
then some surface above these horizontal
 

00:01:43.740 --> 00:01:46.179
then some surface above these horizontal
axis WM B so the height of the surface

00:01:46.179 --> 00:01:46.189
axis WM B so the height of the surface
 

00:01:46.189 --> 00:01:49.370
axis WM B so the height of the surface
represents the value of J comma V at a

00:01:49.370 --> 00:01:49.380
represents the value of J comma V at a
 

00:01:49.380 --> 00:01:51.889
represents the value of J comma V at a
certain point and what we want to do is

00:01:51.889 --> 00:01:51.899
certain point and what we want to do is
 

00:01:51.899 --> 00:01:55.780
certain point and what we want to do is
we need to find a value of W and B that

00:01:55.780 --> 00:01:55.790
we need to find a value of W and B that
 

00:01:55.790 --> 00:01:58.940
we need to find a value of W and B that
corresponds to the minimum of the cost

00:01:58.940 --> 00:01:58.950
corresponds to the minimum of the cost
 

00:01:58.950 --> 00:02:03.130
corresponds to the minimum of the cost
function J it turns out that this took

00:02:03.130 --> 00:02:03.140
function J it turns out that this took
 

00:02:03.140 --> 00:02:06.219
function J it turns out that this took
function J is a convex function so it's

00:02:06.219 --> 00:02:06.229
function J is a convex function so it's
 

00:02:06.229 --> 00:02:08.499
function J is a convex function so it's
just a single big bowl you know so this

00:02:08.499 --> 00:02:08.509
just a single big bowl you know so this
 

00:02:08.509 --> 00:02:11.680
just a single big bowl you know so this
is a convex function and this as opposed

00:02:11.680 --> 00:02:11.690
is a convex function and this as opposed
 

00:02:11.690 --> 00:02:13.480
is a convex function and this as opposed
to functions that look like this which

00:02:13.480 --> 00:02:13.490
to functions that look like this which
 

00:02:13.490 --> 00:02:16.030
to functions that look like this which
are non convex and has lots of different

00:02:16.030 --> 00:02:16.040
are non convex and has lots of different
 

00:02:16.040 --> 00:02:19.390
are non convex and has lots of different
local Optima so the fact that our cost

00:02:19.390 --> 00:02:19.400
local Optima so the fact that our cost
 

00:02:19.400 --> 00:02:22.510
local Optima so the fact that our cost
function J of WB as defined here is

00:02:22.510 --> 00:02:22.520
function J of WB as defined here is
 

00:02:22.520 --> 00:02:25.420
function J of WB as defined here is
convex is one of the huge reasons why we

00:02:25.420 --> 00:02:25.430
convex is one of the huge reasons why we
 

00:02:25.430 --> 00:02:27.670
convex is one of the huge reasons why we
use this particular cost function J for

00:02:27.670 --> 00:02:27.680
use this particular cost function J for
 

00:02:27.680 --> 00:02:31.420
use this particular cost function J for
logistic regression so to find a good

00:02:31.420 --> 00:02:31.430
logistic regression so to find a good
 

00:02:31.430 --> 00:02:34.210
logistic regression so to find a good
value for the parameters what we'll do

00:02:34.210 --> 00:02:34.220
value for the parameters what we'll do
 

00:02:34.220 --> 00:02:38.440
value for the parameters what we'll do
is initialize W and B to some initial

00:02:38.440 --> 00:02:38.450
is initialize W and B to some initial
 

00:02:38.450 --> 00:02:41.410
is initialize W and B to some initial
value we've denoted by that little bit

00:02:41.410 --> 00:02:41.420
value we've denoted by that little bit
 

00:02:41.420 --> 00:02:44.560
value we've denoted by that little bit
dot and so the district regression

00:02:44.560 --> 00:02:44.570
dot and so the district regression
 

00:02:44.570 --> 00:02:47.410
dot and so the district regression
almost any initialization method works

00:02:47.410 --> 00:02:47.420
almost any initialization method works
 

00:02:47.420 --> 00:02:50.009
almost any initialization method works
usually you initialize the values to 0

00:02:50.009 --> 00:02:50.019
usually you initialize the values to 0
 

00:02:50.019 --> 00:02:52.780
usually you initialize the values to 0
random initialization also works but

00:02:52.780 --> 00:02:52.790
random initialization also works but
 

00:02:52.790 --> 00:02:54.310
random initialization also works but
people don't usually do that fill into

00:02:54.310 --> 00:02:54.320
people don't usually do that fill into
 

00:02:54.320 --> 00:02:56.229
people don't usually do that fill into
the regression but because this function

00:02:56.229 --> 00:02:56.239
the regression but because this function
 

00:02:56.239 --> 00:02:59.020
the regression but because this function
is convex no matter where you initialize

00:02:59.020 --> 00:02:59.030
is convex no matter where you initialize
 

00:02:59.030 --> 00:03:00.759
is convex no matter where you initialize
you should get to the same point or

00:03:00.759 --> 00:03:00.769
you should get to the same point or
 

00:03:00.769 --> 00:03:02.680
you should get to the same point or
roughly the same point and what gradient

00:03:02.680 --> 00:03:02.690
roughly the same point and what gradient
 

00:03:02.690 --> 00:03:04.780
roughly the same point and what gradient
descent does is it starts at that

00:03:04.780 --> 00:03:04.790
descent does is it starts at that
 

00:03:04.790 --> 00:03:08.319
descent does is it starts at that
initial point and then takes a step in

00:03:08.319 --> 00:03:08.329
initial point and then takes a step in
 

00:03:08.329 --> 00:03:10.810
initial point and then takes a step in
the steepest downhill direction so after

00:03:10.810 --> 00:03:10.820
the steepest downhill direction so after
 

00:03:10.820 --> 00:03:13.569
the steepest downhill direction so after
one step of gradient descent you might

00:03:13.569 --> 00:03:13.579
one step of gradient descent you might
 

00:03:13.579 --> 00:03:15.850
one step of gradient descent you might
end up there because it's trying to take

00:03:15.850 --> 00:03:15.860
end up there because it's trying to take
 

00:03:15.860 --> 00:03:18.190
end up there because it's trying to take
a step downhill in the direction of

00:03:18.190 --> 00:03:18.200
a step downhill in the direction of
 

00:03:18.200 --> 00:03:20.259
a step downhill in the direction of
steepest descent or as quickly down low

00:03:20.259 --> 00:03:20.269
steepest descent or as quickly down low
 

00:03:20.269 --> 00:03:21.039
steepest descent or as quickly down low
as possible

00:03:21.039 --> 00:03:21.049
as possible
 

00:03:21.049 --> 00:03:22.539
as possible
so that's one iteration of gradient

00:03:22.539 --> 00:03:22.549
so that's one iteration of gradient
 

00:03:22.549 --> 00:03:24.970
so that's one iteration of gradient
descent and after two iterations of your

00:03:24.970 --> 00:03:24.980
descent and after two iterations of your
 

00:03:24.980 --> 00:03:27.130
descent and after two iterations of your
in the sense you might step there three

00:03:27.130 --> 00:03:27.140
in the sense you might step there three
 

00:03:27.140 --> 00:03:29.410
in the sense you might step there three
iterations and so on against is now

00:03:29.410 --> 00:03:29.420
iterations and so on against is now
 

00:03:29.420 --> 00:03:31.030
iterations and so on against is now
hidden by the back of the plot until

00:03:31.030 --> 00:03:31.040
hidden by the back of the plot until
 

00:03:31.040 --> 00:03:33.580
hidden by the back of the plot until
eventually hopefully you converge to

00:03:33.580 --> 00:03:33.590
eventually hopefully you converge to
 

00:03:33.590 --> 00:03:36.340
eventually hopefully you converge to
this global optimum or get to something

00:03:36.340 --> 00:03:36.350
this global optimum or get to something
 

00:03:36.350 --> 00:03:39.190
this global optimum or get to something
close to the global optimum so this

00:03:39.190 --> 00:03:39.200
close to the global optimum so this
 

00:03:39.200 --> 00:03:41.050
close to the global optimum so this
picture illustrates the gradient descent

00:03:41.050 --> 00:03:41.060
picture illustrates the gradient descent
 

00:03:41.060 --> 00:03:43.270
picture illustrates the gradient descent
algorithm let's write out a bit more of

00:03:43.270 --> 00:03:43.280
algorithm let's write out a bit more of
 

00:03:43.280 --> 00:03:45.009
algorithm let's write out a bit more of
the details for the purpose of

00:03:45.009 --> 00:03:45.019
the details for the purpose of
 

00:03:45.019 --> 00:03:46.900
the details for the purpose of
illustration let's say that there's some

00:03:46.900 --> 00:03:46.910
illustration let's say that there's some
 

00:03:46.910 --> 00:03:48.970
illustration let's say that there's some
function J of W that you want to

00:03:48.970 --> 00:03:48.980
function J of W that you want to
 

00:03:48.980 --> 00:03:50.860
function J of W that you want to
minimize and maybe that function look

00:03:50.860 --> 00:03:50.870
minimize and maybe that function look
 

00:03:50.870 --> 00:03:52.870
minimize and maybe that function look
like this so make this easier to draw

00:03:52.870 --> 00:03:52.880
like this so make this easier to draw
 

00:03:52.880 --> 00:03:55.330
like this so make this easier to draw
I'm going to ignore B for now just to

00:03:55.330 --> 00:03:55.340
I'm going to ignore B for now just to
 

00:03:55.340 --> 00:03:56.979
I'm going to ignore B for now just to
make there's a one dimensional plot

00:03:56.979 --> 00:03:56.989
make there's a one dimensional plot
 

00:03:56.989 --> 00:03:59.259
make there's a one dimensional plot
instead of a higher dimensional plot so

00:03:59.259 --> 00:03:59.269
instead of a higher dimensional plot so
 

00:03:59.269 --> 00:04:01.449
instead of a higher dimensional plot so
gradient descent does this we're going

00:04:01.449 --> 00:04:01.459
gradient descent does this we're going
 

00:04:01.459 --> 00:04:05.650
gradient descent does this we're going
to repeatedly carry out the following

00:04:05.650 --> 00:04:05.660
to repeatedly carry out the following
 

00:04:05.660 --> 00:04:07.949
to repeatedly carry out the following
update and take the value of W and

00:04:07.949 --> 00:04:07.959
update and take the value of W and
 

00:04:07.959 --> 00:04:10.930
update and take the value of W and
update it going to use colon equals to

00:04:10.930 --> 00:04:10.940
update it going to use colon equals to
 

00:04:10.940 --> 00:04:14.520
update it going to use colon equals to
represent updating W so set W to W

00:04:14.520 --> 00:04:14.530
represent updating W so set W to W
 

00:04:14.530 --> 00:04:16.160
represent updating W so set W to W
liners

00:04:16.160 --> 00:04:16.170
liners
 

00:04:16.170 --> 00:04:19.910
liners
alpha times and this is a derivative D

00:04:19.910 --> 00:04:19.920
alpha times and this is a derivative D
 

00:04:19.920 --> 00:04:23.960
alpha times and this is a derivative D
of JW DW I will repeatedly do that until

00:04:23.960 --> 00:04:23.970
of JW DW I will repeatedly do that until
 

00:04:23.970 --> 00:04:26.630
of JW DW I will repeatedly do that until
the algorithm converges so a couple

00:04:26.630 --> 00:04:26.640
the algorithm converges so a couple
 

00:04:26.640 --> 00:04:29.300
the algorithm converges so a couple
points in the notation alpha here is the

00:04:29.300 --> 00:04:29.310
points in the notation alpha here is the
 

00:04:29.310 --> 00:04:33.110
points in the notation alpha here is the
learning rate and controls how big a

00:04:33.110 --> 00:04:33.120
learning rate and controls how big a
 

00:04:33.120 --> 00:04:35.480
learning rate and controls how big a
step we take on each iteration of

00:04:35.480 --> 00:04:35.490
step we take on each iteration of
 

00:04:35.490 --> 00:04:37.760
step we take on each iteration of
gradient descent we'll talk later about

00:04:37.760 --> 00:04:37.770
gradient descent we'll talk later about
 

00:04:37.770 --> 00:04:39.920
gradient descent we'll talk later about
some ways for choosing the learning rate

00:04:39.920 --> 00:04:39.930
some ways for choosing the learning rate
 

00:04:39.930 --> 00:04:43.160
some ways for choosing the learning rate
alpha and second this quantity here this

00:04:43.160 --> 00:04:43.170
alpha and second this quantity here this
 

00:04:43.170 --> 00:04:45.200
alpha and second this quantity here this
is a derivative this is basically the

00:04:45.200 --> 00:04:45.210
is a derivative this is basically the
 

00:04:45.210 --> 00:04:47.030
is a derivative this is basically the
updates or the change you want to name

00:04:47.030 --> 00:04:47.040
updates or the change you want to name
 

00:04:47.040 --> 00:04:49.430
updates or the change you want to name
to the parameters W when we start to

00:04:49.430 --> 00:04:49.440
to the parameters W when we start to
 

00:04:49.440 --> 00:04:52.250
to the parameters W when we start to
write code to implement gradient descent

00:04:52.250 --> 00:04:52.260
write code to implement gradient descent
 

00:04:52.260 --> 00:04:54.380
write code to implement gradient descent
we're going to use the convention that

00:04:54.380 --> 00:04:54.390
we're going to use the convention that
 

00:04:54.390 --> 00:04:59.570
we're going to use the convention that
the variable name in our code DW will be

00:04:59.570 --> 00:04:59.580
the variable name in our code DW will be
 

00:04:59.580 --> 00:05:01.790
the variable name in our code DW will be
used to represent this derivative term

00:05:01.790 --> 00:05:01.800
used to represent this derivative term
 

00:05:01.800 --> 00:05:03.650
used to represent this derivative term
so when you write code you write

00:05:03.650 --> 00:05:03.660
so when you write code you write
 

00:05:03.660 --> 00:05:06.530
so when you write code you write
something like W equals of colon equals

00:05:06.530 --> 00:05:06.540
something like W equals of colon equals
 

00:05:06.540 --> 00:05:11.600
something like W equals of colon equals
W minus alpha times DW so we use DW to

00:05:11.600 --> 00:05:11.610
W minus alpha times DW so we use DW to
 

00:05:11.610 --> 00:05:13.310
W minus alpha times DW so we use DW to
be the variable name to represent this

00:05:13.310 --> 00:05:13.320
be the variable name to represent this
 

00:05:13.320 --> 00:05:15.530
be the variable name to represent this
derivative term now let's just make sure

00:05:15.530 --> 00:05:15.540
derivative term now let's just make sure
 

00:05:15.540 --> 00:05:18.200
derivative term now let's just make sure
that this gradient descent update makes

00:05:18.200 --> 00:05:18.210
that this gradient descent update makes
 

00:05:18.210 --> 00:05:21.950
that this gradient descent update makes
sense let's say that W was over here so

00:05:21.950 --> 00:05:21.960
sense let's say that W was over here so
 

00:05:21.960 --> 00:05:24.530
sense let's say that W was over here so
you're at this point on the cost

00:05:24.530 --> 00:05:24.540
you're at this point on the cost
 

00:05:24.540 --> 00:05:26.810
you're at this point on the cost
function J of W remember that the

00:05:26.810 --> 00:05:26.820
function J of W remember that the
 

00:05:26.820 --> 00:05:29.750
function J of W remember that the
definition of a derivative is the slope

00:05:29.750 --> 00:05:29.760
definition of a derivative is the slope
 

00:05:29.760 --> 00:05:32.030
definition of a derivative is the slope
of a function at the point so the slope

00:05:32.030 --> 00:05:32.040
of a function at the point so the slope
 

00:05:32.040 --> 00:05:33.920
of a function at the point so the slope
of a function is really you know the

00:05:33.920 --> 00:05:33.930
of a function is really you know the
 

00:05:33.930 --> 00:05:35.810
of a function is really you know the
height divided by the width right

00:05:35.810 --> 00:05:35.820
height divided by the width right
 

00:05:35.820 --> 00:05:37.460
height divided by the width right
there's a little triangle here in this

00:05:37.460 --> 00:05:37.470
there's a little triangle here in this
 

00:05:37.470 --> 00:05:40.520
there's a little triangle here in this
tangent to jr. W at that point and so

00:05:40.520 --> 00:05:40.530
tangent to jr. W at that point and so
 

00:05:40.530 --> 00:05:44.000
tangent to jr. W at that point and so
here the derivative is positive value

00:05:44.000 --> 00:05:44.010
here the derivative is positive value
 

00:05:44.010 --> 00:05:47.150
here the derivative is positive value
gets updated at W minus a learning rate

00:05:47.150 --> 00:05:47.160
gets updated at W minus a learning rate
 

00:05:47.160 --> 00:05:49.520
gets updated at W minus a learning rate
times the derivative the derivative is

00:05:49.520 --> 00:05:49.530
times the derivative the derivative is
 

00:05:49.530 --> 00:05:51.890
times the derivative the derivative is
positive and so you end up subtracting

00:05:51.890 --> 00:05:51.900
positive and so you end up subtracting
 

00:05:51.900 --> 00:05:54.470
positive and so you end up subtracting
from W so you end up taking a step to

00:05:54.470 --> 00:05:54.480
from W so you end up taking a step to
 

00:05:54.480 --> 00:05:56.510
from W so you end up taking a step to
the left and so gradient descents would

00:05:56.510 --> 00:05:56.520
the left and so gradient descents would
 

00:05:56.520 --> 00:05:58.480
the left and so gradient descents would
you know make your algorithm slowly

00:05:58.480 --> 00:05:58.490
you know make your algorithm slowly
 

00:05:58.490 --> 00:06:01.040
you know make your algorithm slowly
decrease the parameter if you have

00:06:01.040 --> 00:06:01.050
decrease the parameter if you have
 

00:06:01.050 --> 00:06:03.800
decrease the parameter if you have
started off with this large value of W

00:06:03.800 --> 00:06:03.810
started off with this large value of W
 

00:06:03.810 --> 00:06:07.390
started off with this large value of W
as another example if W was over here

00:06:07.390 --> 00:06:07.400
as another example if W was over here
 

00:06:07.400 --> 00:06:12.730
as another example if W was over here
then at this point the slope here or DJ

00:06:12.730 --> 00:06:12.740
then at this point the slope here or DJ
 

00:06:12.740 --> 00:06:17.210
then at this point the slope here or DJ
BW will be negative and so the gradient

00:06:17.210 --> 00:06:17.220
BW will be negative and so the gradient
 

00:06:17.220 --> 00:06:20.320
BW will be negative and so the gradient
descent update would subtract alpha

00:06:20.320 --> 00:06:20.330
descent update would subtract alpha
 

00:06:20.330 --> 00:06:23.690
descent update would subtract alpha
times a negative number and so in up

00:06:23.690 --> 00:06:23.700
times a negative number and so in up
 

00:06:23.700 --> 00:06:26.540
times a negative number and so in up
slowly increasing W so you end up you're

00:06:26.540 --> 00:06:26.550
slowly increasing W so you end up you're
 

00:06:26.550 --> 00:06:29.360
slowly increasing W so you end up you're
making W for grant bigger which

00:06:29.360 --> 00:06:29.370
making W for grant bigger which
 

00:06:29.370 --> 00:06:31.520
making W for grant bigger which
excessive vibrations of green descent so

00:06:31.520 --> 00:06:31.530
excessive vibrations of green descent so
 

00:06:31.530 --> 00:06:33.500
excessive vibrations of green descent so
that hopefully whether you initialize on

00:06:33.500 --> 00:06:33.510
that hopefully whether you initialize on
 

00:06:33.510 --> 00:06:35.689
that hopefully whether you initialize on
the left or the right gradient descent

00:06:35.689 --> 00:06:35.699
the left or the right gradient descent
 

00:06:35.699 --> 00:06:38.030
the left or the right gradient descent
will move you toward this global minimum

00:06:38.030 --> 00:06:38.040
will move you toward this global minimum
 

00:06:38.040 --> 00:06:40.580
will move you toward this global minimum
here if you're not familiar with

00:06:40.580 --> 00:06:40.590
here if you're not familiar with
 

00:06:40.590 --> 00:06:43.310
here if you're not familiar with
derivatives of of calculus and what this

00:06:43.310 --> 00:06:43.320
derivatives of of calculus and what this
 

00:06:43.320 --> 00:06:48.740
derivatives of of calculus and what this
term D J of wdw means don't worry too

00:06:48.740 --> 00:06:48.750
term D J of wdw means don't worry too
 

00:06:48.750 --> 00:06:51.200
term D J of wdw means don't worry too
much about it we'll talk some more about

00:06:51.200 --> 00:06:51.210
much about it we'll talk some more about
 

00:06:51.210 --> 00:06:53.870
much about it we'll talk some more about
derivatives in the next video if you

00:06:53.870 --> 00:06:53.880
derivatives in the next video if you
 

00:06:53.880 --> 00:06:56.150
derivatives in the next video if you
have a deep knowledge of calculus you

00:06:56.150 --> 00:06:56.160
have a deep knowledge of calculus you
 

00:06:56.160 --> 00:06:58.840
have a deep knowledge of calculus you
might be able to have a deeper

00:06:58.840 --> 00:06:58.850
might be able to have a deeper
 

00:06:58.850 --> 00:07:01.280
might be able to have a deeper
intuitions about how neural networks

00:07:01.280 --> 00:07:01.290
intuitions about how neural networks
 

00:07:01.290 --> 00:07:03.409
intuitions about how neural networks
work but even if you're not that

00:07:03.409 --> 00:07:03.419
work but even if you're not that
 

00:07:03.419 --> 00:07:05.810
work but even if you're not that
familiar with calculus in the next few

00:07:05.810 --> 00:07:05.820
familiar with calculus in the next few
 

00:07:05.820 --> 00:07:07.879
familiar with calculus in the next few
videos we'll give you enough intuitions

00:07:07.879 --> 00:07:07.889
videos we'll give you enough intuitions
 

00:07:07.889 --> 00:07:10.580
videos we'll give you enough intuitions
about derivatives and about calculus

00:07:10.580 --> 00:07:10.590
about derivatives and about calculus
 

00:07:10.590 --> 00:07:13.400
about derivatives and about calculus
that you'll be able to effectively use

00:07:13.400 --> 00:07:13.410
that you'll be able to effectively use
 

00:07:13.410 --> 00:07:15.469
that you'll be able to effectively use
neural networks but the overall

00:07:15.469 --> 00:07:15.479
neural networks but the overall
 

00:07:15.479 --> 00:07:17.840
neural networks but the overall
intuition for now is that this term

00:07:17.840 --> 00:07:17.850
intuition for now is that this term
 

00:07:17.850 --> 00:07:20.810
intuition for now is that this term
represents the slope of the function and

00:07:20.810 --> 00:07:20.820
represents the slope of the function and
 

00:07:20.820 --> 00:07:23.390
represents the slope of the function and
we want to know the slope of the

00:07:23.390 --> 00:07:23.400
we want to know the slope of the
 

00:07:23.400 --> 00:07:25.520
we want to know the slope of the
function at the current setting of the

00:07:25.520 --> 00:07:25.530
function at the current setting of the
 

00:07:25.530 --> 00:07:27.980
function at the current setting of the
parameters so that we can take these

00:07:27.980 --> 00:07:27.990
parameters so that we can take these
 

00:07:27.990 --> 00:07:30.469
parameters so that we can take these
steps of steepest descent so that we

00:07:30.469 --> 00:07:30.479
steps of steepest descent so that we
 

00:07:30.479 --> 00:07:32.779
steps of steepest descent so that we
know what direction to step in in order

00:07:32.779 --> 00:07:32.789
know what direction to step in in order
 

00:07:32.789 --> 00:07:36.770
know what direction to step in in order
to go downhill on the cost function J so

00:07:36.770 --> 00:07:36.780
to go downhill on the cost function J so
 

00:07:36.780 --> 00:07:39.529
to go downhill on the cost function J so
we wrote our gradient descent for J of W

00:07:39.529 --> 00:07:39.539
we wrote our gradient descent for J of W
 

00:07:39.539 --> 00:07:43.010
we wrote our gradient descent for J of W
if only W was your parameter in logistic

00:07:43.010 --> 00:07:43.020
if only W was your parameter in logistic
 

00:07:43.020 --> 00:07:44.900
if only W was your parameter in logistic
regression your cost function is a

00:07:44.900 --> 00:07:44.910
regression your cost function is a
 

00:07:44.910 --> 00:07:47.719
regression your cost function is a
function of both W and B so in that case

00:07:47.719 --> 00:07:47.729
function of both W and B so in that case
 

00:07:47.729 --> 00:07:50.029
function of both W and B so in that case
the inner loop our gradient descent that

00:07:50.029 --> 00:07:50.039
the inner loop our gradient descent that
 

00:07:50.039 --> 00:07:51.560
the inner loop our gradient descent that
is this thing here the thing you have to

00:07:51.560 --> 00:07:51.570
is this thing here the thing you have to
 

00:07:51.570 --> 00:07:53.779
is this thing here the thing you have to
repeat becomes as follows you end up

00:07:53.779 --> 00:07:53.789
repeat becomes as follows you end up
 

00:07:53.789 --> 00:07:56.990
repeat becomes as follows you end up
updating W as W minus the learning rate

00:07:56.990 --> 00:07:57.000
updating W as W minus the learning rate
 

00:07:57.000 --> 00:08:00.409
updating W as W minus the learning rate
times the derivative of J of W be

00:08:00.409 --> 00:08:00.419
times the derivative of J of W be
 

00:08:00.419 --> 00:08:04.580
times the derivative of J of W be
respect to W and you update B as B minus

00:08:04.580 --> 00:08:04.590
respect to W and you update B as B minus
 

00:08:04.590 --> 00:08:07.730
respect to W and you update B as B minus
the learning rate times the derivative

00:08:07.730 --> 00:08:07.740
the learning rate times the derivative
 

00:08:07.740 --> 00:08:12.290
the learning rate times the derivative
of the cost function respect to B so

00:08:12.290 --> 00:08:12.300
of the cost function respect to B so
 

00:08:12.300 --> 00:08:14.930
of the cost function respect to B so
these two equations at the bottom are

00:08:14.930 --> 00:08:14.940
these two equations at the bottom are
 

00:08:14.940 --> 00:08:17.480
these two equations at the bottom are
the actual update you implement as an

00:08:17.480 --> 00:08:17.490
the actual update you implement as an
 

00:08:17.490 --> 00:08:19.089
the actual update you implement as an
aside I just wanna mention one

00:08:19.089 --> 00:08:19.099
aside I just wanna mention one
 

00:08:19.099 --> 00:08:21.770
aside I just wanna mention one
notational convention in calculus that

00:08:21.770 --> 00:08:21.780
notational convention in calculus that
 

00:08:21.780 --> 00:08:24.080
notational convention in calculus that
is a bit confusing to some people I

00:08:24.080 --> 00:08:24.090
is a bit confusing to some people I
 

00:08:24.090 --> 00:08:26.360
is a bit confusing to some people I
don't think is super important that you

00:08:26.360 --> 00:08:26.370
don't think is super important that you
 

00:08:26.370 --> 00:08:28.400
don't think is super important that you
understand calculus but in case you see

00:08:28.400 --> 00:08:28.410
understand calculus but in case you see
 

00:08:28.410 --> 00:08:30.830
understand calculus but in case you see
this I want to make sure that you don't

00:08:30.830 --> 00:08:30.840
this I want to make sure that you don't
 

00:08:30.840 --> 00:08:33.230
this I want to make sure that you don't
think too much of this which is that in

00:08:33.230 --> 00:08:33.240
think too much of this which is that in
 

00:08:33.240 --> 00:08:36.290
think too much of this which is that in
calculus this term here we actually

00:08:36.290 --> 00:08:36.300
calculus this term here we actually
 

00:08:36.300 --> 00:08:38.690
calculus this term here we actually
write as follows with that funny

00:08:38.690 --> 00:08:38.700
write as follows with that funny
 

00:08:38.700 --> 00:08:40.879
write as follows with that funny
squiggle symbol so

00:08:40.879 --> 00:08:40.889
squiggle symbol so
 

00:08:40.889 --> 00:08:44.449
squiggle symbol so
this symbol this is actually just a

00:08:44.449 --> 00:08:44.459
this symbol this is actually just a
 

00:08:44.459 --> 00:08:47.539
this symbol this is actually just a
locate deep in a fancy font in a

00:08:47.539 --> 00:08:47.549
locate deep in a fancy font in a
 

00:08:47.549 --> 00:08:49.849
locate deep in a fancy font in a
stylized font but when you see this

00:08:49.849 --> 00:08:49.859
stylized font but when you see this
 

00:08:49.859 --> 00:08:53.090
stylized font but when you see this
expression all this means is this is J

00:08:53.090 --> 00:08:53.100
expression all this means is this is J
 

00:08:53.100 --> 00:08:55.639
expression all this means is this is J
of W comma B or really the slope of the

00:08:55.639 --> 00:08:55.649
of W comma B or really the slope of the
 

00:08:55.649 --> 00:08:58.369
of W comma B or really the slope of the
function J of W comma B how much that

00:08:58.369 --> 00:08:58.379
function J of W comma B how much that
 

00:08:58.379 --> 00:09:01.099
function J of W comma B how much that
function slopes in the W direction and

00:09:01.099 --> 00:09:01.109
function slopes in the W direction and
 

00:09:01.109 --> 00:09:03.949
function slopes in the W direction and
the rule of the notation in calculus

00:09:03.949 --> 00:09:03.959
the rule of the notation in calculus
 

00:09:03.959 --> 00:09:06.650
the rule of the notation in calculus
which I think is in total article but

00:09:06.650 --> 00:09:06.660
which I think is in total article but
 

00:09:06.660 --> 00:09:09.259
which I think is in total article but
the rule in the notation for calculus

00:09:09.259 --> 00:09:09.269
the rule in the notation for calculus
 

00:09:09.269 --> 00:09:11.599
the rule in the notation for calculus
which I think just makes things much

00:09:11.599 --> 00:09:11.609
which I think just makes things much
 

00:09:11.609 --> 00:09:13.280
which I think just makes things much
more complicated than you need to be is

00:09:13.280 --> 00:09:13.290
more complicated than you need to be is
 

00:09:13.290 --> 00:09:16.549
more complicated than you need to be is
that if J is a function of two or more

00:09:16.549 --> 00:09:16.559
that if J is a function of two or more
 

00:09:16.559 --> 00:09:18.769
that if J is a function of two or more
variables then instead of using

00:09:18.769 --> 00:09:18.779
variables then instead of using
 

00:09:18.779 --> 00:09:21.139
variables then instead of using
lowercase D you use this funny symbol

00:09:21.139 --> 00:09:21.149
lowercase D you use this funny symbol
 

00:09:21.149 --> 00:09:23.179
lowercase D you use this funny symbol
this is called a partial derivative

00:09:23.179 --> 00:09:23.189
this is called a partial derivative
 

00:09:23.189 --> 00:09:26.629
this is called a partial derivative
symbol but don't worry about this and if

00:09:26.629 --> 00:09:26.639
symbol but don't worry about this and if
 

00:09:26.639 --> 00:09:28.309
symbol but don't worry about this and if
J is a function of only one variable

00:09:28.309 --> 00:09:28.319
J is a function of only one variable
 

00:09:28.319 --> 00:09:31.579
J is a function of only one variable
then you use lowercase D so the only

00:09:31.579 --> 00:09:31.589
then you use lowercase D so the only
 

00:09:31.589 --> 00:09:33.109
then you use lowercase D so the only
difference but you want to use this

00:09:33.109 --> 00:09:33.119
difference but you want to use this
 

00:09:33.119 --> 00:09:35.289
difference but you want to use this
funny partial derivative symbol or

00:09:35.289 --> 00:09:35.299
funny partial derivative symbol or
 

00:09:35.299 --> 00:09:38.269
funny partial derivative symbol or
lowercase D as we did on top is whether

00:09:38.269 --> 00:09:38.279
lowercase D as we did on top is whether
 

00:09:38.279 --> 00:09:40.970
lowercase D as we did on top is whether
J is a function of two or more variables

00:09:40.970 --> 00:09:40.980
J is a function of two or more variables
 

00:09:40.980 --> 00:09:44.059
J is a function of two or more variables
in which case you use this symbol the

00:09:44.059 --> 00:09:44.069
in which case you use this symbol the
 

00:09:44.069 --> 00:09:46.669
in which case you use this symbol the
partial derivative symbol or V J is only

00:09:46.669 --> 00:09:46.679
partial derivative symbol or V J is only
 

00:09:46.679 --> 00:09:49.280
partial derivative symbol or V J is only
a function of one variable then you use

00:09:49.280 --> 00:09:49.290
a function of one variable then you use
 

00:09:49.290 --> 00:09:52.489
a function of one variable then you use
lowercase P this is one of those funny

00:09:52.489 --> 00:09:52.499
lowercase P this is one of those funny
 

00:09:52.499 --> 00:09:54.859
lowercase P this is one of those funny
rules of notation and calculus that I

00:09:54.859 --> 00:09:54.869
rules of notation and calculus that I
 

00:09:54.869 --> 00:09:56.539
rules of notation and calculus that I
think just make things more complicated

00:09:56.539 --> 00:09:56.549
think just make things more complicated
 

00:09:56.549 --> 00:09:59.150
think just make things more complicated
than they need to be but if you see this

00:09:59.150 --> 00:09:59.160
than they need to be but if you see this
 

00:09:59.160 --> 00:10:02.119
than they need to be but if you see this
partial derivative symbol now all it

00:10:02.119 --> 00:10:02.129
partial derivative symbol now all it
 

00:10:02.129 --> 00:10:03.829
partial derivative symbol now all it
means is you're measuring the slope of

00:10:03.829 --> 00:10:03.839
means is you're measuring the slope of
 

00:10:03.839 --> 00:10:06.350
means is you're measuring the slope of
the function with respect to one of the

00:10:06.350 --> 00:10:06.360
the function with respect to one of the
 

00:10:06.360 --> 00:10:09.379
the function with respect to one of the
variables and similarly to adhere to the

00:10:09.379 --> 00:10:09.389
variables and similarly to adhere to the
 

00:10:09.389 --> 00:10:11.989
variables and similarly to adhere to the
you know formally correct mathematical

00:10:11.989 --> 00:10:11.999
you know formally correct mathematical
 

00:10:11.999 --> 00:10:16.220
you know formally correct mathematical
notation calculus because here J has two

00:10:16.220 --> 00:10:16.230
notation calculus because here J has two
 

00:10:16.230 --> 00:10:18.530
notation calculus because here J has two
inputs not just one this thing at the

00:10:18.530 --> 00:10:18.540
inputs not just one this thing at the
 

00:10:18.540 --> 00:10:20.059
inputs not just one this thing at the
bottom should be written with this

00:10:20.059 --> 00:10:20.069
bottom should be written with this
 

00:10:20.069 --> 00:10:22.850
bottom should be written with this
partial derivative simple but it really

00:10:22.850 --> 00:10:22.860
partial derivative simple but it really
 

00:10:22.860 --> 00:10:25.039
partial derivative simple but it really
means the same thing as you know almost

00:10:25.039 --> 00:10:25.049
means the same thing as you know almost
 

00:10:25.049 --> 00:10:28.639
means the same thing as you know almost
the same thing as lowercase D finally

00:10:28.639 --> 00:10:28.649
the same thing as lowercase D finally
 

00:10:28.649 --> 00:10:31.400
the same thing as lowercase D finally
when you implement this in code we're

00:10:31.400 --> 00:10:31.410
when you implement this in code we're
 

00:10:31.410 --> 00:10:33.529
when you implement this in code we're
going to use the convention that this

00:10:33.529 --> 00:10:33.539
going to use the convention that this
 

00:10:33.539 --> 00:10:36.169
going to use the convention that this
quantity really the amount by which you

00:10:36.169 --> 00:10:36.179
quantity really the amount by which you
 

00:10:36.179 --> 00:10:40.519
quantity really the amount by which you
update W will denote as the variable DW

00:10:40.519 --> 00:10:40.529
update W will denote as the variable DW
 

00:10:40.529 --> 00:10:44.239
update W will denote as the variable DW
in your code and this quantity right the

00:10:44.239 --> 00:10:44.249
in your code and this quantity right the
 

00:10:44.249 --> 00:10:46.369
in your code and this quantity right the
amount by which you want to update B

00:10:46.369 --> 00:10:46.379
amount by which you want to update B
 

00:10:46.379 --> 00:10:50.449
amount by which you want to update B
will denote by the variable DB in your

00:10:50.449 --> 00:10:50.459
will denote by the variable DB in your
 

00:10:50.459 --> 00:10:53.449
will denote by the variable DB in your
code all right so that's how you can

00:10:53.449 --> 00:10:53.459
code all right so that's how you can
 

00:10:53.459 --> 00:10:53.990
code all right so that's how you can
implement

00:10:53.990 --> 00:10:54.000
implement
 

00:10:54.000 --> 00:10:56.390
implement
straight into send now if you haven't

00:10:56.390 --> 00:10:56.400
straight into send now if you haven't
 

00:10:56.400 --> 00:10:58.910
straight into send now if you haven't
seen calculus refuge's I know that that

00:10:58.910 --> 00:10:58.920
seen calculus refuge's I know that that
 

00:10:58.920 --> 00:11:00.740
seen calculus refuge's I know that that
might seem like a lot more derivatives

00:11:00.740 --> 00:11:00.750
might seem like a lot more derivatives
 

00:11:00.750 --> 00:11:02.030
might seem like a lot more derivatives
in calculus than you might be

00:11:02.030 --> 00:11:02.040
in calculus than you might be
 

00:11:02.040 --> 00:11:04.130
in calculus than you might be
comfortable with so far but if you're

00:11:04.130 --> 00:11:04.140
comfortable with so far but if you're
 

00:11:04.140 --> 00:11:06.080
comfortable with so far but if you're
feeling that way don't worry about it in

00:11:06.080 --> 00:11:06.090
feeling that way don't worry about it in
 

00:11:06.090 --> 00:11:07.880
feeling that way don't worry about it in
the next video we'll give you better

00:11:07.880 --> 00:11:07.890
the next video we'll give you better
 

00:11:07.890 --> 00:11:10.820
the next video we'll give you better
intuition about derivatives and even

00:11:10.820 --> 00:11:10.830
intuition about derivatives and even
 

00:11:10.830 --> 00:11:12.020
intuition about derivatives and even
without a deep mathematical

00:11:12.020 --> 00:11:12.030
without a deep mathematical
 

00:11:12.030 --> 00:11:13.820
without a deep mathematical
understanding of calculus sort of just

00:11:13.820 --> 00:11:13.830
understanding of calculus sort of just
 

00:11:13.830 --> 00:11:15.830
understanding of calculus sort of just
an intuitive understanding of calculus

00:11:15.830 --> 00:11:15.840
an intuitive understanding of calculus
 

00:11:15.840 --> 00:11:17.630
an intuitive understanding of calculus
you will be able to make new networks

00:11:17.630 --> 00:11:17.640
you will be able to make new networks
 

00:11:17.640 --> 00:11:20.150
you will be able to make new networks
work effectively so that let's go on to

00:11:20.150 --> 00:11:20.160
work effectively so that let's go on to
 

00:11:20.160 --> 00:11:21.710
work effectively so that let's go on to
the next video where we talk a little

00:11:21.710 --> 00:11:21.720
the next video where we talk a little
 

00:11:21.720 --> 00:11:25.220
the next video where we talk a little
bit more about derivatives

