Speaker 1:          00:00:02       Okay. So we'll, should I just improvise something? Sure. Hey, this is jad. Before we launch into this week's podcast, uh, I want to, uh, make you aware of, uh, well our friends down the hall, Brooke, my friend, our friends, uh, on the media, Brooke Gladstone is here with me. Uh, having a new show coming out that I'm, I think I might be in, I'm excited to be in, but I'm certainly excited about. So what is it?

Speaker 2:          00:00:26       You are going to be in it? It's an episode, although you don't want to say show because on radio lab that generally means that you're launching a whole new style new enterprise. Yeah. This is an episode on twitch, which some people know as if, as if it were part of their family or like what? Yeah. Is that something you need medication for? So it depends where you're situated, but what we are going to do is examine how it came to be and how it points to the future of where our culture's going. And for the people who don't know what is twitch. Most people, if they've ever heard of it, they know it's about watching and commenting in real time on people playing video games.

Speaker 3:          00:01:23       No

Speaker 2:          00:01:24       trap rap. And you guys, from what I hear profile one of these twitch superstars, the main character in that story is Ninja who makes something like half a million dollars a month. What bought on twitch? Yes. Through contributors who say say this out loud or point to me mention my name is online and at the same time he's playing the game. He's even giving advice to young boys with girlfriends.

Speaker 3:          00:01:58       Don't act like, you know, don't make, don't make her seem, it's like her fault. You just need to be like they like we need have, we need to have a talk and be like, don't worry, everything's fine, you know, reassure that. But then you just need to be like, I really feel like I'm trying, like putting in more effort in this relationship. And then like, then I feel like you are, and I want him to I and then just be like, I just want to make sure that you're just, you're just as invested in this as I am.

Speaker 2:          00:02:19       I think it's something new. I want to hear that. Well, thank you. Thank you for letting me in the top of your shell. Absolutely.

Speaker 1:          00:02:28       So a twitch on, on the media, on the media.org. Check it out. I'm in there. It's going to be amazing, but not just because of me, cause of you Brooke. Thanks for dropping in and Bob and Bob. Bye. Bye.

Speaker 4:          00:02:42       Oh wait, you're listening.

Speaker 2:          00:02:48       I adore listening to radio lab radio from W N Y C E.

Speaker 1:          00:03:00       Hey, I'm Jen. I boom Rod. I'm Robert Krulwich, Radiolab. And today we have a story about what we can say and what we can't. And by the way, um, there's going to be a smattering of curse words here that we're not going to bleep, I think makes sense. Given the content of this story. And also there's some graphic scenes that, uh, if you've got kids with a, you may want to mess with this one out. Yeah. Anyway, the story comes to us from producers, Simon Adler. So let's start, can we start in 2008? Sure. How about with a song? Yes, please. [inaudible]

Speaker 5:          00:03:32       [inaudible] man, Facebook sees Sarah Oppressive way, rise up, rise and put bad guy. Shake chairs, red now.

Speaker 6:          00:03:41       So, uh, December 27th, a sunny Saturday morning, uh, this group of young to middle aged women gathered in downtown. Yeah.

Speaker 5:          00:03:49       Palo Alto, cyberspace free. [inaudible]

Speaker 6:          00:03:55       they're wearing these colorful hats and are thinking and swaying directly in front of the glass toward headquarters of,

Speaker 5:          00:04:02       yeah, it's nice. And one or graphics. Facebook is their phone call.

Speaker 6:          00:04:09       Yes. It was a humble gathering of a few dozen women and babies. That right there. Are you, the organizer tonight is one of the organizers of the gathering, Stephanie Mujer.

Speaker 5:          00:04:21       And what, what are you calling the event? Um, it's a Facebook nursing

Speaker 6:          00:04:24       nursing as in like breastfeeding. The intent was really just to be visible and be peaceful and make a quiet point.

Speaker 1:          00:04:34       What a, what point were they trying to make?

Speaker 6:          00:04:36       Well, uh, so Stephanie, in this group of mothers, you know, they were on Facebook as many people were and they'd have photos taken of themselves occasionally breastfeeding their babies. They wanted to share with their friends what was going on. So they would upload those photos to Facebook and these pictures would get taken down and they would receive a warning from Facebook for uploading, um, pornographic content. And people were really getting their backs up over this. They wanted Facebook to stop taking their photos down to say that, well, nudity is not allowed. Breastfeeding is exempt, period

Speaker 5:          00:05:11       and goes back [inaudible]

Speaker 6:          00:05:17       now what Stephanie couldn't have known at the time was that this

Speaker 7:          00:05:23       small, peaceful protest would turn out to be this morning of face off on Facebook. One of the opening shots. Facebook triggered a Hornet's nest in what would become a loud fuck you Facebook. [inaudible] Facebook Rawkus fuck you, Facebook, fuck you. And global battle in battles. Facebook CEO's book today playing defense. And now I'm not talking about all the things you've recently heard about Russian interference in election meddling or data breaches, but, but rather something that I think is a is deep

Speaker 6:          00:05:55       then both of those free speech, Facebook,

Speaker 4:          00:05:58       his minute Hughes are facilitating. Yeah,

Speaker 5:          00:06:00       violence against Rohingya Muslims we can say and what we can't say but again, Facebook vine, this iconic photographs we can see and what we can see Mueller rape kids in front of people.

Speaker 4:          00:06:10       Do you want to close this one?

Speaker 5:          00:06:12       Hear me.

Speaker 4:          00:06:16       [inaudible] you are fucking theses yet.

Speaker 5:          00:06:22       Thank you Mr Chairman. Uh, Mr Zuckerberg got gotta ask you, do you subjectively prioritized or censor speech?

Speaker 8:          00:06:33       Congresswoman, we don't think about what we're doing is censoring speech but there are types of, what really grabbed me was,

Speaker 6:          00:06:42       was discovering that underneath all of this is an actual rule book, a text document that dictates what I can say on Facebook. What you can say on Facebook and what all 2.2 billion of us can say on Facebook

Speaker 8:          00:07:00       for everyone in the entire globe for everyone. One, one set of rules that all 2.2 billion of us are expected to follow. This is an actual document.

Speaker 6:          00:07:10       It's a digital document, but yes, it's about 50 pages. If you print it off and a in bullet points and if then statements, it spells out sort of a first amendment for the globe, which made me wonder like what are these rules? How were they written? And can you even have one rule book? Right, exactly. And so I, I dove into this rule book and dug up some stories that really put it to the test.

Speaker 8:          00:07:37       Hm. Okay. Yeah, I'm interested in with the stories we're going to hear. We are going to three ish. Three ish. Okay. Okay. Alright. I'm particular interested in the ish, but let's go ahead with the first one.

Speaker 6:          00:07:45       Well, so, uh, let's start back, uh, on that morning in 2008, the morning that you could argue started at all,

Speaker 5:          00:07:52       right? Right. Cause

Speaker 8:          00:07:54       building right behind those protesting mothers, there was a group of Facebook employees sitting in a conference room trying to figure out what to do. Um, cool. So if I, so I'm just gonna switch to just read. So I was able to get in touch with a couple of former Facebook employees, one of whom was actually in that room at that moment. And now neither of these two were comfortable being identified, but they did give us permission to quote them extensively. How's that? Well, they'll take work for you. It sounded great. Um, cool. Just so we have it, let's, so what you're going to hear here is, is an actor we brought in to read quotes taken directly from interviews that we did with these two different former Facebook in place. [inaudible] all right, ready? So at the time when I joined them, there was a smaller group, 12 of us, mostly recent college grads who were sort of called the site integrity team. Again, keep in mind, this was in the early [inaudible]

Speaker 5:          00:08:45       thousands seismic changes this week in the Internet hierarchy. This was like the deep, dark past. My space.com is now the most visited website in the U. Of course, Facebook

Speaker 8:          00:08:56       had somewhere in the neighborhood of 10 million users. We were smaller than my space. The vast majority of them, college kids. And so in those early days, those 12 people, they would, they would sit around in a sort of conference like room with a big long table, each of them in front of their own computer

Speaker 9:          00:09:11       and things would come up onto their screen flagged to Facebook. And I'm like

Speaker 8:          00:09:16       meaning like I a user saw something that I thought was wrong. Okay.

Speaker 9:          00:09:19       Exactly like a reporting a piece of content that you think violates the community standards.

Speaker 8:          00:09:24       This is Kate clonic, she's a professor of law at St John's and she spent a lot of time studying this very thing and, and she says in those early days what would happen is a user would flag a piece of content and then that content along with an alert would, would get sent to one of those people sitting in that room. It would just pop up on their screen. Most of what you were seeing was either naked people blown off heads or things that there was no clear reason why someone had reported, cause it was like a photo of a golden retriever and people are just annoying. And every time something popped up onto the screen, the person sitting at that computer would have to make a decision whether to leave that thing up or take it down. And at the time, if you didn't know what to do, you would turn to your pod leader who was, you know, somebody who had been around nine months longer than you and ask what do I do with this? And they would either have seen it before and explain it to you or you both would know and you'd Google some things.

Speaker 9:          00:10:20       It really was just kind of an ad hoc approach.

Speaker 8:          00:10:23       Was there any sort of written standard or any common standard? A well kind of,

Speaker 9:          00:10:27       they had a set of community standards that they, at the end of the day, they were just kind of, that was one page long and it was not very specific.

Speaker 8:          00:10:34       Sorry. The, the guidelines were really one page long. They were one page long. And basically all this page said was nudity is bad, so is Hitler.

Speaker 9:          00:10:43       And um, if it makes you feel bad, take it down.

Speaker 8:          00:10:46       And so when one of the people sitting in that room would have a breastfeeding picture pop up on the screen in front of them, they'd be like, I can see a female breast. So I guess that's nudity. And they would take it down until

Speaker 9:          00:11:00       rise up, fight for the rights to have breastfeeding.

Speaker 8:          00:11:05       Anyway, now a dozen or so people in front of their offices on a Saturday. It probably wasn't causing Facebook too much heartache, but I thought, you know, hey, we have an opportunity here with, you know, over 10,000 members in our group. According to Stephanie Mirror. Those protesters were just a tiny fraction of a much larger online group who had organized, ironically enough through Facebook. So to coincide with the live protest, I just typed up a little blurb encouraging our members that were in the group to do a virtual nurse in a virtual nursing, right. What we did, they posted a message asking their members to for

Speaker 10:         00:11:44       [inaudible], changed their profile Avatar to an image of breastfeeding and then changed their status to the of our group.

Speaker 2:          00:11:54       Hey, Facebook. Breastfeeding is not obscene and it caught on. A social networking website is under fire for its policy on photos of women breastfeeding their children. 12,000 members participated and the media requests and started pouring in the Facebook group call. Hey, Facebook. Breastfeeding is not good. I did hundreds of interviews for print, Chicago Tribune, Miami Herald, Time magazine, New York Times, Washington Post. No, the Internet is an interesting phenomenon. Dr Phil. It was a media storm

Speaker 6:          00:12:27       and eventually perhaps as a result of our group and our efforts, Facebook was forced to get much more specific about their rules. So for example, by then, nudity was already not allowed on the site, but they had no definition for nudity. They just said no nudity. And so the site integrity team, the those 12 people at the time, they realized they had to start spelling out exactly what they meant.

Speaker 9:          00:12:52       It's precisely all of these people at Facebook were in charge of trying to define nudity.

Speaker 8:          00:12:56       So I mean, yeah, the first cutout, it was visible male and female genitalia and then visible female breasts. And then the question is, well, okay, how much of a breast needs to be showing before it's nude and the thing that we landed on was if you could see essentially the nipple and Areola, then that's nudity and it would have to be taken down, which theoretically at least

Speaker 6:          00:13:18       would appease these protesters because you know now when a picture would pop up of a mother breastfeeding, as long as the child was blocking the view of the nipple and the Areola, they they could say, cool, no problem.

Speaker 9:          00:13:30       Then you start getting pictures that are women with just their babies on their chest with their breasts bare. Like for example, maybe baby was sleeping on the chest of a bare breasted woman and not actively breastfeeding.

Speaker 8:          00:13:43       Okay. Now what? Like is this actually breastfeeding? No, it's actually not breastfeeding. The woman is just holding the baby and she has her top off.

Speaker 9:          00:13:50       No, but she was clearly just breastfeeding the baby.

Speaker 6:          00:13:53       Well like it was before. Well I would say it's sort of like kicking a soccer ball, like a photo of someone who has just kicked the soccer ball. You can tell the ball is in the air, but there was no contact between the foot and the ball in that moment potentially. So although it is a photo of someone kicking a soccer ball, they are not in fact kicking the soccer ball in that photo. That's a good in this, this became the procedure or the protocol or the approach for all these things was we have to base it purely on what we can see in the image

Speaker 9:          00:14:23       and so it didn't allow that to stay up under the rules because it could be too easily exploited for other types of content like nudity or pornography.

Speaker 8:          00:14:31       W We got to, the only way you could objective only say that the baby and the mother were engaged in breastfeeding is if the baby's lips were touching a woman's nipple. So they included what you could call like an attachment clause. But as soon as they got that rule in place, like you would see, you know, a 25 year old woman and a teenager looking boy. Right. And like what the hell is going on there?

Speaker 11:         00:14:52       Oh yeah. It gets really weird if you like start entering into like child age and it wasn't even gonna bring that up cause it's kind of gross. It's like breastfeeding porn, is that a thing? Are there sites like half apparently. And so this team, they realized they needed to have a new duty rule that allowed for breastfeeding but also had some kind of an age cap.

Speaker 8:          00:15:11       So, so, uh, so then we were saying, okay, once you've progressed past infancy, then we believe that it's an approval

Speaker 11:         00:15:17       Britt. But then pictures would start popping up on their screen and they'd be like, wait, is that an infant? Like where's the line between infant and toddler?

Speaker 8:          00:15:25       And so the thing that we landed on was if it looked like the child could walk on his or her own, then too old, big enough to walk

Speaker 11:         00:15:32       too big to breastfeed. Oh that could be [inaudible]. Yeah, that's like a year old in some cases. Yeah. And like the World Health Organization recommends breastfeeding until you know, like 18 months or two years, which meant there were a lot of photos still being taken down within you know, days. We continuing to hear reports from people that their photographs were still being targeted. But

Speaker 7:          00:15:56       Facebook did offer a statement saying that's where we're going to draw the line@facebookusingbudgetingonitpolicyandkeepinmindthroughthiswholeepisodeisthisperhapsthenextbigthingandthefacebook.com

Speaker 8:          00:16:09       the company was growing really, really fast.

Speaker 11:         00:16:11       It seems like almost everyone is on it and it's, and there just got to be a lot more content. When we first launched we were hoping for, you know, maybe 400 500 people and now we're at 100,000 so who knows where we're going now? [inaudible] more people are joining Facebook every day, 60 million users so far with a projection of 200 million by the end of the year and now more people on Facebook than the entire us population. Not just within the United States, but also it was growing rapidly. More International, you know, you were getting spokes in India, India and Turkey. Facebook, Facebook is going to ran this. Butch was getting big throughout the EU. Korea's joined the Facebook. So they have more and more content coming in from all these different places, in all different languages.

Speaker 8:          00:16:58       How are we going to keep everybody on the same page?

Speaker 11:         00:17:02       And so once they saw that this was the operational method for dealing with this, creating this like nesting set of exceptions and rules and these clear things that had to be there, had to not be there in order to keep content up or take it down, that I think became their procedure. And so this small team at Facebook got a little bigger and bigger, jumped up to 60 people and then a hundred and they set out to create rules and definitions for everything. Huh? Can we go through some of sort of the ridiculous examples? Let's go over here. Hey, so Gore Gore, you mean violence? [inaudible] yes. So the Gore Standard was

Speaker 8:          00:17:39       headline. We don't allow graphic violence and Gore. And then the shorthand definition they used was no insides on the [inaudible].

Speaker 11:         00:17:46       No guts, no blood pouring out of [inaudible] was a separate

Speaker 8:          00:17:50       issue. There was, uh, an excessive blood rule and to come up with rules about bodily fluids, semen for example, would be allowed in like a clinical setting. But like what does a clinical setting mean? And you know, does that mean if someone is in a lab coat?

Speaker 9:          00:18:04       Hmm. At one of my favorite examples is like, how do you define art?

Speaker 8:          00:18:08       Because as these people are moderating, they would see images of naked people that were paintings or sculptures come up. And so what they decided to do is say art with nakedness can stay up.

Speaker 9:          00:18:20       Like it stays up if it is made out of wood made out of metal. Made out of stone. Really? Yeah. Because how else do you define art? You have to just be like, is this what you can see with your eyeballs? And so from then on, as they run into problems, those rules just constantly get updated or constant amendments. Yeah. Constant amendments,

Speaker 8:          00:18:41       new problem, new rule, another new problem. Updated rule. In fact, at this point, they, they're amending these rules up to 20 times a month. Wow. Really? Yeah. Take for example those rules about breastfeeding. In 2013 they removed the attachment clause so the baby no longer needed to have its mouth physically touching the nipple of the woman. Oh. And in fact, uh, one nipple and or areola could be visible in the photo, but not too only one. Then 2014 they make it so that both nipples or both Areola may be present in the photo. So this is what happens in American law all the time. It's very thing.

Speaker 9:          00:19:24       Yes. Yeah, yeah. You know, it sounds a lot like common law.

Speaker 8:          00:19:29       So common law is the system dating back to early England where individual judges would make a ruling, which would sort of be a law, but then that law would be amended or evolved by other judges.

Speaker 9:          00:19:40       So the body of law was sort of constantly flushed out and face of new facts.

Speaker 8:          00:19:46       Literally every time this team at Facebook would come up with a rule that they thought was airtight come, plop something, something would show up that day that they weren't prepared for that, that the rule hadn't accounted a, as soon as you think, yeah, this is good. Like the next day something shows up to show you. Yeah, you didn't think about this. For example, sometime around 2011 this content moderator is, is, is going through a queue of things

Speaker 12:         00:20:10       except reject, except escalate. Except

Speaker 8:          00:20:16       she comes upon this image.

Speaker 12:         00:20:17       Oh my God.

Speaker 8:          00:20:19       The photo itself was a teenage girl, African by dress and skin breastfeeding a goat of baby goats. Hm. And the moderator throws their hands up and says, what the fuck is this? And we googled breastfeeding goats and found that this was a thing. It turns out it's a survival practice according to what they found. This is a tradition in Kenya that goes back centuries, that in a drought, a known way to help your herd through the drought is to, uh, if you, if you have a woman who's lactating, to have her nurse, the kid, the baby goat, along with her human kit. Hmm. And so there's nothing sexual about it, just good for business. Good. And theoretically, if we go point by point through this list, it's an infant. It's SORTA could walk. So maybe there's an issue there, but there, there's physical contact between the mouth and the nipple.

Speaker 8:          00:21:13       But, but obviously breastfeeding as we intended anyway, meant human infants. And so in that moment, what they decided to do is, uh, remove the photo and there was an amendment and asterick under the rule stating animals are not babies. We added that. So in any future cases, people would know what to do, who they removed. They, they, they discover it was culturally appropriate and a thing that people do. And they decide to remove the photo. Yeah. That outraged individuals, our editors who are in wheeler. Why? Why didn't we make an exception? Because because when a problem grows large enough, you have to change the rules. If not, we don't, this was not one of those cases. The juice wasn't worth the squeeze. And like if they were to allow this picture, then they'd have to make some rule about when it was okay to breastfeed and animal and when it wasn't okay. This is a utilitarian document. It's not about being right 100% of the time. It's about being able to execute effectively. In other words, we're not trying to be perfect here and we're not even necessarily trying to be 100% just or fair. We're just trying to make something that works.

Speaker 13:         00:22:24       One, two, three, four, five, six, seven, eight and when you step back and look at what Facebook has become like from 2008 to now in just 10 years,

Speaker 2:          00:22:34       Simon, um, I've just arrived at the Accenture tower here in Manila. I don't know how many floors. It is one too

Speaker 13:         00:22:43       idea of a single set of rules that works that can be applied fairly. Yes. Just a crazy, crazy concept.

Speaker 2:          00:22:50       Just 1516 or 17

Speaker 13:         00:22:52       they've gone from something like 70 million users to 2.2 billion.

Speaker 2:          00:22:56       It's hard to take camp. I would say it's about 30 floors.

Speaker 13:         00:22:59       They've gone from 12 folks sitting in a room deciding what to take down or leave up to somewhere around 16,000 people.

Speaker 2:          00:23:06       So there's a floor in this building where Facebook supposedly outsources content moderators.

Speaker 13:         00:23:12       And so around 2010 they decided to start outsourcing some of this work to places like Manila where you just heard reporter Aurora almond drawl as well as, I mean, I would guess that there are thousands of people in this building Dublin where we sent reporter Gareth's stack Oh, can see in where they get their delicious Facebook treats cooked. Everybody's beavering away. And we sent them there to try to talk to some of these people too for living, sit at a computer and collectively click through around a million flagged bits of content that pop up onto their screen every day. Wow. What a, I'm just curious. Like what's, what's that?

Speaker 5:          00:23:46       Well, um, can I ask you some questions? [inaudible] we'd found out pretty quickly the work floor. None of these folks are willing to talk to us about what they do. Um, so there's a lot of, uh, running away from me happening.

Speaker 14:         00:23:59       Hey, let's sir, sorry to bother you. Do you guys work in Facebook? I'm sorry. You have to work in Facebook anytime. I, sorry to bother you. Do you reconcile? No. Sorry. Do you work in Facebook? No. I mean like you just came out of there. I know you're lying. In fact,

Speaker 13:         00:24:14       people wouldn't even admit they work for the company. Like what's the, is there something wrong about being in navy? Like an NDA that they signed? Well, yeah, so, so when I finally did find someone willing, uh, willing to talk to me, do you want to be named or do you know, or do you not want to be named?

Speaker 15:         00:24:30       I'd rather not.

Speaker 13:         00:24:33       That's totally fine.

Speaker 15:         00:24:34       You know, I'm just kinda like the industry. I don't want to lose like up on this shit. Yeah.

Speaker 13:         00:24:37       He explained that he and all the other moderators like him were forced to sign these nondisclosure agreements stating they weren't allowed to, uh, admit that they work for Facebook. They're not allowed to talk about the work they do.

Speaker 15:         00:24:49       My contract, I hated to prohibit any from tackler about what content moderation was.

Speaker 13:         00:24:55       Why? Several reasons. One is that up until recently, Facebook wanted to keep secret what these rules were so that they couldn't be gamed. At the same time, it, it creates a sort of separation between these workers and the company, which if you're Facebook, you might want,

Speaker 15:         00:25:11       yeah, I, I knew I signed up to monitor with graphic images

Speaker 13:         00:25:14       just given the nature of the job.

Speaker 15:         00:25:16       Right. You know, I didn't really, you know, you don't really know the impact that that's gonna have on you until you got through it.

Speaker 13:         00:25:22       This guy talked to he, he got his first contract doing this work several years back and for the duration of it, about a year, he'd show up to his desk every morning, put on his headphones. Okay, click, click, click, click, click, ignore, delete, delete.

Speaker 15:         00:25:35       Case by case by case by case. 45 5,000 kids every day was just image and decision, image decision.

Speaker 13:         00:25:41       Which five, 5,000 a day you just said.

Speaker 15:         00:25:44       Yeah. It was like, there was a lot of cases. Yes.

Speaker 13:         00:25:46       He, he said basically he'd have to go through an image or some other piece of content every three or four seconds. Wow. All Day long. All Day. Eight hours a day.

Speaker 4:          00:25:55       Wow.

Speaker 13:         00:26:01       Well, if I can ask, what kind of things did you see?

Speaker 15:         00:26:05       Oh, I don't know if this is even a, I don't know if this is like radio is worthy. Um, it's true. I think the accelerated, uh, no

Speaker 13:         00:26:13       clicking through, he came across unspeakable things.

Speaker 15:         00:26:17       Some had exploded into, you know, get the little squashed by a tank to, you know, people in cages being drowned too. Like a 13 year old girl. Um, having sex with an eight year old boy and it's not just one, it's over. I know. Right. Oh, right. Over

Speaker 13:         00:26:38       when did you, did this like keep you up at night or did you did this

Speaker 15:         00:26:41       absolutely, absolutely 100%. Um, and kept me up at night, uh,

Speaker 6:          00:26:47       catch himself thinking about these videos and photos when he was trying to relax. He had to start avoiding things.

Speaker 15:         00:26:53       Yeah, there were, there were specific like movies that I couldn't watch. It was fine. I can pay for the Clinton Terrell's no one [inaudible] and I see it. I was like, okay, turn it on. I was like, like, heads were exploding. I was like, nope, nope. I have to walk away. And I just, I had to, I chose it was too real. I saw that. It's classic. Good. Yesterday

Speaker 6:          00:27:18       a different moderator I spoke to described it as seeing the worst side of humanity. You see all of the stuff that you and I don't have to see because they are going around playing cleanup. Yeah. What a job. Oh Wow. Yeah. And it, and it's worth noting that I'm more and more of this work is being done in an automated fashion, particularly with content like a gore or terrorist propaganda. Uh, they're getting better automate that. They, yeah, they, uh, through computer vision they're able to detect hallmarks of, of, of a terrorist video or of a gory image. And, uh, with terrorist propaganda, they now take down 99% of it before anyone flags it on Facebook. Oh. But, uh, moving onto our second story here, there is a type of content that they, uh, that they are having an incredibly hard time. Uh, not just automating, but even getting their rules straight on and that surrounding hate speech. Oh, good. Some more laughs coming up. Well, there will be laughter. Oh really? There will be comedians. There will be jokes. Hey. All right. Okay. Should we take a break and then come right back? No, I think we're going to keep going. Okay.

Speaker 5:          00:28:33       Testing one, two, three, four, five. Testing. One, two, three, four, five. I'm Simon Adler,

Speaker 6:          00:28:36       so a couple months back. I think it's working great. We sent our pair of intern,

Speaker 5:          00:28:41       it's on the left 60 feet, Carter Hodge. Here we go at this standing room and lazy Gieger tickets for tonight. Anywhere on the guest list. Okay. To this cramped, sorry. Narrow little comedy club. The kind of place with like a super, yeah, I know. $15 smashed rosemary cocktails. None of it. We do not need to get up. And that is the fine high top tables. The acs dripping on me, but still kind of a die is good. Yeah.

Speaker 6:          00:29:20       Sent them there to check out someone else who'd found a fault line in Facebook's rule book,

Speaker 2:          00:29:26       deciding where to keep it moving right along the next two you come to stage, please give it up for Marcia Belsky.

Speaker 5:          00:29:34       Yes, I get so mad. I feel like my personal into the city. I was such a carefree Brat, you know, I was young and I had these older friends, which I thought was like very cool. And then you just realize that they're alcoholics

Speaker 4:          00:29:49       know

Speaker 11:         00:29:53       he's got dark curly hair was raised in Oklahoma,

Speaker 5:          00:29:56       you know, and I think I was raised Jewish. So when you're raised Jewish, you read about Anne Frank a lot, you know a lot, a lot. And when you read about Anne Frank like this, we'll get funny [inaudible]

Speaker 11:         00:30:10       how did you decide to become a comedian? You know, it was kind of the only thing that ever clicked with me. And especially political comedy, you know, I used to watch the daily show every day and back in 2016 she started this political running bit that I think can be called sort of a absurdist feminist comedy.

Speaker 5:          00:30:28       No, a lot of people think that I'm like an angry feminist. Um, which is weird. This guy called me a militant feminist the other day and I'm like, okay, just because I am training a militia women in the woods.

Speaker 11:         00:30:46       At first I just had this running bit online on Facebook and Twitter. She was tweeting and posting jokes, you know, like we have all the buffalo wild wings surrounded, you know, things like that. Eventually took this bid on stage, even wrote [inaudible]

Speaker 5:          00:31:00       Hey, I'm older,

Speaker 4:          00:31:02       I tend to dive on my dad. No, no, no. My Dad.

Speaker 11:         00:31:13       Anyhow, so about a year into this running bit, uh, Marsha was bored at work one day and uh, logs onto Facebook. But, uh, instead of seeing her at normal newsfeed, uh, there was this message that pops up. It says you posted something that discriminated along the lines of race, gender, or ethnicity group. And so we removed that post. And so I'm like, what could I possibly have post it? I really, I thought it was like a glitch, but then she clicked continue and they're highlighted was the violating post. It was a photo of hers. What, what is the picture? Can you describe it? The photo is me as look can only be described as the Cherub, cute little seven year old with big curly hair and she's wearing this blue floral dress. Her teeth are all messed up and into the photo Marcia had edited in a speech bubble that just says kill all men.

Speaker 11:         00:32:05       And so it's funny, you know, cause I hit, I hit, it's funny, you know, I trust me, whatever. So, um, I thought it was ridiculous because she searched through her library of photos and found that kill all men image and I post it again immediately after like, yeah. And it got removed again and this time there were consequences. I got banned for three days after that. Then after several other bands shoot forward, this is months later, a friend of hers had posted an article and underneath it in the comment section, there were guys posting just really nasty stuff. So I commented underneath those comments, men are scum, which was very quickly removed. How, how long did you get banned for this time? 30 days. Wow. Yeah, I was dumbfounded. So there's a rule somewhere that if I type men are scum, you take it down. Yes. And like what could it be?

Speaker 5:          00:33:02       Okay. And so Marsha called on her quote militia of women. Exactly. To find out like is this just me? Female Comedians who are sort of like mad on my behalf, started experimenting, posting men are scum to see how quickly you would get removed and if it would be removed every time. And it was so they started trying other words. Yeah, to find out where the line was. My friend put men are dusk up that got removed. Men are the worst removed to band. This one girl put men are septic fluid band,

Speaker 11:         00:33:39       but we're only at the middle of the saga.

Speaker 5:          00:33:41       It doesn't end there because there's no, she's really like, what the hell is going on is this sexism. So I just start doing the most bare minimum amount of investigating. She's googling around

Speaker 11:         00:33:54       trying to figure out what these policies are in. Pretty quick, she comes across this leaked Facebook document. This is when I lose my mind. This is when Mark Zuckerberg becomes my sworn nemesis for the rest of my life. Because what she'd found was a document Facebook used to train their moderators and inside of it in a section detailing who Facebook protected from hate speech, there was a multiple choice question that said, uh, who do we protect? White men or black children? And the correct answer was white men, not black children. Not even kidding. Why we're protecting black children are not. That's not a good look racist. Something's going on here. There is absolutely some sort of unaddressed bias or systematic issue at Facebook.

Speaker 5:          00:34:47       Great. Hello, I'm doing well. Thank you so much for being here.

Speaker 11:         00:34:52       Yeah, so not long after sitting down with Marsha, Facebook invited me to come out to their offices in California and sit down with them. I'm going to eat one cookie and then, oh, they're little. I think I get to typing. Can I just get your, your name and your title and I'm Monica Bickert and I lead the policies for Facebook. Monica Bickert is in charge of all of Facebook's rules, including their policies on hate speech. And so I asked her like, why would there be a rule that protects white men but not black children? We have, we have made our hate speech policies. Let me, let me rephrase that. Our hate speech policies have become more detailed over time, but our main is you can't attack a person or a group of people based on a protected characteristic, a characteristic like race, religion, or gender. So this takes a couple of beats to explain, but the gist of it is that, uh, the Facebook borrowed this idea of protected classes, uh, straight from us, anti-discrimination law.

Speaker 11:         00:35:53       These are the laws that make it so that you can't not hire someone say based on like their religion, their ethnicity, their race, and so on, on Facebook. You can't attack someone based on one of these characteristics. Meaning you can't say men are trash, nor could you say women are trash. Uh, because essentially you're attacking all men for being men. Oh, is, is it the all, can I say Bob is trash? Yeah, you can say Bob is trash. Because as my story is explained to me, the distinction is that in the first instance, you're attacking a category. In the second instance, you're attacking a person, but it's not clear that you're attacking that person because they are a member of a protected category. Oh. So Bob might be trashed for reasons that have nothing to do with him being a man. He just might be annoying.

Speaker 11:         00:36:41       Right. Okay. So that explains why you take down, men are scum, but why would you leave up? Black children are scum. Why would that not get taken down? So traditionally we allowed speech once there was some other word in it that made it about something other than it protected characteristic in Facebook jargon. These are referred to as a, uh, a non-protected modifier just means literally nothing. Give us an example of this. So traditionally if you said, I don't like this religion, cab drivers, cab driver would be the non-protected modifier because employment is not a protected category. Huh? And so what the rule stated was, uh, when you add this non-protected modifier to a protected category, in this case the cab drivers religion, we would allow it because we can't assume that you're hating this person because of his religion. And you actually just may not like the cab drivers.

Speaker 11:         00:37:42       So in the case of black children, children is modifying the protected category of black. And so children Trump's black age is a non protected category. Okay. So a children becomes a non-protected modifier and their, their, their childness trumps their blackness. You can say whatever you want about black children. Whereas in the case of white men, uh, you've got a, you've got gender and race both protected, you can't attack them. That's just a bizarre rule. I would think you would go the other direction that the protected class would outweigh the modifier. Well, they, they made this decision as they explained to me because their default was to allow speech. They were really trying to incorporate or nod to the American speech tradition.

Speaker 8:          00:38:36       And so there's a whole lot of stuff out there that none of us would defend as a valuable speech. But didn't, didn't rise to the level of stuff that we'd say, this is so bad, we're going to take it down. And uh, in this case, their concern was we're all members of like, you know, at least half a dozen protected categories. Like we all have gender, we all have sexual orientation. And if you may, if the rule is that anytime a protected class is mentioned, it could be hate speech. What you are doing at that point is opening up just about every comment that's ever made about anyone on Facebook to potentially be hate speech.

Speaker 11:         00:39:12       Then you're not left with anything, right? No matter where we draw this line, they're going to be sad. Some outcomes that we don't like, there are always going to be casualties. That's why we continue to change the policies. And in fact, since Marsha's debacle, they've actually updated this rule. So now black children are protected from what they considered the worst forms of hate speech. Now our reviewers take how severe the attack is into consideration. But despite this, there are still plenty of people that is flawed because you are a social network, including Marcia who, who think this still just isn't good enough. There are not systematic efforts to eliminate white men in the way that there are other groups. That's why you have protected groups. She thinks white men and heterosexual should not be protected, protect the groups who are actually victims of hate speech.

Speaker 8:          00:40:04       Makes Sense. Well, yeah, because in sort of hate speech, uh, or thinking about hate speech and there's this idea of, of privileged or of historically disadvantaged groups and that those historically disadvantaged groups should have more protection, uh, because of being historically disadvantaged. And the challenge with that, uh, that was presented to me was okay,

Speaker 16:         00:40:27       why the thousands new Japanese reinforcements [inaudible]

Speaker 8:          00:40:30       in the 1940s

Speaker 16:         00:40:31       do cut off the Chinese and pay and [inaudible]

Speaker 11:         00:40:34       you had Japanese soldiers, tens of thousands of Chinese killing millions of Chinese during World War II.

Speaker 8:          00:40:43       At that same time you had Japanese American citizens,

Speaker 16:         00:40:47       a thousand persons of Japanese ancestry, all of them would have to move

Speaker 8:          00:40:52       being put into internment camps. And so we had to ask ourselves the question like, are the Japanese and historically advantaged or disadvantaged group? Huh? Japanese Americans pretty easy to make a case that they were disadvantaged. But in China it's a totally different story. And this happened at the exact same moment. So you've got two different places, two different cultural stories and when you have a website like Facebook that's trans national community, they realized where they decided that ideas of privilege are so geographically bound that there is no way to effectively weigh and consider who is privileged, who and

Speaker 6:          00:41:34       decided therefore that we are not going to allow historical advantage or historical privilege, uh, into the equation at all.

Speaker 2:          00:41:45       [inaudible]

Speaker 8:          00:41:45       and I think it's very important to keep in mind here Americans, these moderators only have like four or five seconds come to make a decision [inaudible] in those four seconds, is there enough time to, to figure out where in the world someone is, particularly given IP addresses can easily be masked. Go back where you came from. Is it enough time to figure out a person's ethnicity? White children are better than black children. On top of that, we often don't know an individual's race straight people stuck other categories or even less clear like sexual orientation

Speaker 6:          00:42:24       and they, they just realized it would be next to impossible to get anybody to be able to run these, run these calculations effectively.

Speaker 11:         00:42:30       When we were building that framework, we did a lot of tasks and we saw some times that it was just too hard for our reviewers to implement a more detailed policy consistently. They just couldn't do it accurately. So we want the policies to be sufficiently detailed to take into account all different types of scenarios, but simple enough that we can apply them consistently and accurately around the world. And the reality is anytime that the policies become more complicated, we see dips in our consistency.

Speaker 6:          00:43:05       What Facebook's trying to do is take the first amendment, this high minded, lofty legal concept and convert it into a, an engineering manual that can be executed every four seconds for any piece of content from anywhere on the globe. And when you've got to move that fast, sometimes justice loses.

Speaker 11:         00:43:28       That's the, um, that's the tension here.

Speaker 4:          00:43:34       [inaudible]

Speaker 11:         00:43:34       and I just want to make sure I emphasize that these policies, they're not going to please everybody. They often don't. Don't please everybody that's working on the policy team at Facebook, but if we want to have one line that we enforce consistently, then it means we have to have some pretty objective black and white rules.

Speaker 4:          00:44:06       [inaudible] Coca-Cola, [inaudible]

Speaker 11:         00:44:19       Oh, when we come back, those rules, they get toppled.

Speaker 4:          00:44:31       [inaudible]

Speaker 2:          00:44:33       this is Danny from Denver, Colorado. Radiolab is supported in part by the Alfred p Sloan Foundation enhancing public understanding of science and technology in the modern world. More information aboutSloan@wwwdotsloan.org

Speaker 17:         00:44:49       hi, I'm Tracy and I'm a reporter here at radio lab. I wanted to take a quick second to tell you about something that I think is pretty cool that we make here at the show. Besides the show, we have a newsletter and I can already hear you say big deal. Everybody has a newsletter, but ours is really, really good and worth your time and probably more worth your time than other people's newsletters. So here it is. First of all, it lets you know when to release a new episode. And secondly there's this whole other section of our staff picks and that's just a list of things that the Radiolab team have come across that were just really excited about. And it could be something like, you know Rachel keepsake, our producer, you know, she just worked on goon ads and she found out about this really cool frozen zoo.

Speaker 17:         00:45:36       And then there's our other producer Andy McCune and she kind of tells, told everybody to Google horse mustaches and it is definitely worth your time to Google that. So go do that right now. Or it'll be me talking about you know, the joys or rewatching like my favorite TV show living single. So I hope you'll sign up. It takes about 30 seconds and it's free. So go to radiolab.org back slash newsletter or you can text RL news as in a Radiolab news two seven zero one zero one that's r l news two seven zero one zero one and thanks.

Speaker 6:          00:46:14       They are doors that once open can never be closed.

Speaker 18:         00:46:21       W NYC studios and snap judge with underground layer. Scoop two is here, starting this August every week until Halloween. Be Afraid, but don't turn out the lights.

Speaker 19:         00:46:39       Listen to speed wherever you get your podcasts.

Speaker 6:          00:46:48       Chad, Robert Radiolab, back to Simon Adler, Facebook free speech. So as we just heard before the break, Facebook is trying to do two competing things at once. They're trying to make rules that are just, but at the same time can be reliably executed by thousands of people spread across the globe in ways that are fair and consistent. And I would argue that this balancing act was put to the test. April 15th, 2013.

Speaker 2:          00:47:13       Hey Carlos. Rhonda, Carlos [inaudible] get up. Thank you. We have some break. We have some breaking news. Otherwise I wouldn't cut you off so abruptly. Carla.

Speaker 6:          00:47:22       Monday the 15th 2013 just before three in the afternoon.

Speaker 4:          00:47:29       [inaudible]

Speaker 13:         00:47:33       to pressure cooker bombs with through the crowd near the finish line. The Boston marathon

Speaker 4:          00:47:45       and

Speaker 13:         00:47:46       yeah, as sort of the dust begins to settle.

Speaker 4:          00:47:52       Oh my God.

Speaker 13:         00:47:54       People are springing into action. Uh, does one man in a cowboy hat sees this spectator who, who's been injured, picks him up, throws him in a wheelchair and as they're pushing him through the, this sort of ashy cloud, the, there's this photographer there and he snaps this phone

Speaker 8:          00:48:14       and the photo show is the runner in the cowboy hat and these two other people pushing this man who, uh, his face is Ashen from all of the debris. His hair is sort of standing on end and you can tell that actually the force of the blast and then the, the particles that got in, they're actually holding it in this sort of wedge shape and one of his legs is completely blown off and the second one is blown off below the knee, other than the femur bone sticking out and then sort of skin and muscle and tendons, it's, it's horrific. Meanwhile,

Speaker 13:         00:48:48       Hulu, CBS B area studio on the other side of the country, x five new, I remember snippets of the day. Facebook employees,

Speaker 6:          00:48:57       we're clustering around several desks staring at the computer screens, watching the news break.

Speaker 13:         00:49:02       This has occurred just in the last half hour or so.

Speaker 8:          00:49:05       I have memories of of watching some of the coverage Schilling, new images just released of the Boston bombings. I remember seeing the photo published online and it wasn't long after that

Speaker 6:          00:49:17       someone had posted on Facebook from the folks I spoke to. The order of events here are a little fuzzy, but pretty quickly this photos going viral

Speaker 8:          00:49:29       and we realized we're going to have to deal with it. This image is spreading like wildfire across their platform. It appears to be way outside the rules they'd written, but it's in this totally new context. So they got their team together, sat down in a conference room. I don't know, there was probably eight or 10 people thinking about like should we allow it or should they take it down according to their rules? Yeah. So if you recall the no insides on the outside stuff inition that we had in place, meaning you can't see like people's organs or that sort of thing. And if you can then we wouldn't allow it. And in this, in this photo you could see, you could definitely see bone and so by the rules the photo should obviously come down. Yup. However, half the room says no. The other people are saying this is newsworthy.

Speaker 6:          00:50:20       Essentially this photos being posted everywhere else. It's important. We need to suspend the rules. We need make an exception

Speaker 8:          00:50:28       which immediately received pushback. Well, I was saying that what we've prided ourselves on was not making those calls and there are no exceptions. There is either either mistakes or improvements. We made the guidelines for moments like this to which the other side shoots back. Oh my God, are you kidding me? Like the Boston Globe is publishing this all over the place and we're taking it down like, are you fucking kidding me? Damn the guidelines. Let's just have common sense here. Let's be humans. We know that this is important and yeah, they're kind of there. Right? But the reality is like if you say, well, we allowed it because it's newsworthy. How? How do you answer any of the questions about any of the rest of the stuff?

Speaker 6:          00:51:11       In other words, this is a Pandora's box. And in fact, for reasons that are totally clear, team consistency, team follow the rules eventually wins the day they decide to take the photo down. But before they can pull the lever, word starts making its way up the chain. And internally within Facebook, according to my sources and executive under Zuckerberg sent down in order, we were essentially told, make the exception

Speaker 4:          00:51:39       [inaudible]

Speaker 6:          00:51:39       I don't care what your guidelines say, I don't care what your reason is, the photo standards, you're not taking this down. Yes. Um, yes. That's what happened. This decision means that Facebook has just become a publisher. W they don't think maybe they have, but they've made a news judgment and just willy nilly they've become CBS, ABC, New York Times, Herald Tribune, Atlantic monthly, and all these other things all at once. They just become a news organization. Yeah. And this brings up a legal question that that's at the center of this conversation about free speech. Like is Facebook a sort of collective scrapbook for assault or is it a public square where you should be able to say whatever you want or, yeah. Is it now a news organization adds transparency?

Speaker 8:          00:52:29       I, let me get, let me get a, I'm sorry to interrupt, but let me get to one final question that kind of relates to what you're talking about in terms of what exactly Facebook is. And this question has been popping up a lot recently. In fact, it even came up this past April when Zuckerberg was testifying in front of Congress. I think about 140 million Americans get their news from Facebook. So which are you, are you a tech company? Are you the world's largest publisher? Senator? This is, uh, I view us as a tech company because the primary thing that we do is build technology and products that you're responsible for your content, which makes it of a publisher, right? Well, I agree that we're responsible for the content, but I don't think that that's incompatible with fundamentally yet at our core being a technology company where the main thing that we do is have engineers and build products.

Speaker 6:          00:53:20       Basically Zuckerberg and others at the company are arguing, no, they're not a news organization. Why? What would be the downside of that? Well, Facebook currently sits on this little idyllic legal island where, uh, they can't be held liable for much of anything. They're subjected to few regulations, however, we're where they to be seen in the eyes of the court as a media organization that could change. But setting that aside, what w what really strikes me about all of this is, is here you have a company that really, uh, up until this point has, has been crafting a set of rules that are both as objective as possible and can be executed as consistently as possible. And they've been willing to sacrifice rather large ideas in, in the name of this. They, for example, privilege, which we talked about, they decided was too geographically bound to allow for one consistent rule.

Speaker 6:          00:54:15       But if you ask me, there's nothing more, uh, subjective or geographically bound than what people find interesting or important. What, what people find newsworthy. Hmm. And I'll give and I'll give you a, and I'll give you a great example of this. Uh, that happened just six months after the Boston marathon bombing when this video starts being circulated out of northern Mexico and it's a video of a, of a woman being grabbed and forced onto her knees in front of a camera. And then a man with his face covered, grabs her head, pulled her head back and slices her head off right in front of the camera. And this video starts being spread.

Speaker 2:          00:54:56       I can't count how many times, like just reading my Twitter feed, I've been like, ah, you know, like one person who came across this video or at least dozens of others like it was Shannon Young. My name is Shannon Young. I am a freelance radio reporter. I've been living here in Mexico for many years now and then is covering the drug war. In doing so years back, she noticed this strange phenomenon. It first caught my attention in early 2010 should be checking social media. You know, you're scrolling through your feed and you know, you'd see all this news, people say, Nah. There was this three hour gun battle and intense fighting all weekend long.

Speaker 6:          00:55:31       Folks were posting about clashes between drug cartels and government forces. But then when Shannon would watch the news, yeah,

Speaker 2:          00:55:38       that night, yes, I got [inaudible], she'd see reports on the economy and soccer results, but the media wasn't covering it. There'd be no mention of these attacks. Nothing to do with the violence. And so she and other journalists tried to get to the bottom of this. Reporters and Mexico City would contact the state authorities and you know, public information officer and they'd be like, shootings, bombings, what are you talking about? Nothing's going on. We have no reports of anything. These are just internet rumors. The government even coined a term for these sorts of posts. The famous phrase at the time was collective psychosis. These people are crazy

Speaker 6:          00:56:14       because, you know, they didn't want the situation to seem out of control, but then the video was posted.

Speaker 2:          00:56:23       Yeah,

Speaker 11:         00:56:27       it opens looking out the windshield of a car. On a sunny day, the landscape is dry, dusty, and the video itself is shaky, clearly shot on a phone.

Speaker 4:          00:56:37       [inaudible]

Speaker 2:          00:56:38       so [inaudible] go and taping starts talking. And this woman, she just narrates as they drive along this highway [inaudible] she can

Speaker 11:         00:56:53       phone from the passenger window to the uh, to the windshield ideas, not focusing in on these two silver destroyed pickup trucks.

Speaker 2:          00:57:02       [inaudible] and she's saying, look at these cars over here or there, you know, shot up and [inaudible] oh look here, look here. You know, this 18 wheeler is, you know, totally abandoned. It got shot up

Speaker 11:         00:57:14       at one point, she sticks the phone out the window to show all of the bullet casings, littering the ground

Speaker 2:          00:57:21       and she just, you know, turned the, the official denial on its head.

Speaker 11:         00:57:27       The government was saying there's no violence here were cars riddled with bullets. It was impossible to dismiss. And from

Speaker 2:          00:57:35       then on you had more and more citizens, citizen journalists uploading anonymously video of the violence.

Speaker 4:          00:57:49       [inaudible]

Speaker 11:         00:57:50       low fi shaky shots of

Speaker 2:          00:57:52       shootouts. This member minutes, he headings, I mean bodies hanging, dangling off of overpasses

Speaker 4:          00:58:07       [inaudible]

Speaker 2:          00:58:08       to prove to the world that this was really happening. Let's say we're not crazy.

Speaker 8:          00:58:21       It's a cry for help. Yeah.

Speaker 8:          00:58:24       Which brings us back to that beheading video we mentioned a bit earlier. Yeah. That video of the beheading, a lot of people were uploading it, condemning the violence of the drug cartels and when it started showing up on Facebook, much like with the Boston marathon bombing photo, this team of people, they sat down in a room, looked at the policy, wait, the arguments, and my argument was it was okay by the rules during the Boston bombing. Why isn't it okay now? Particularly given that it could help? Leaving this up means we warn hundreds of thousands of people of the brutality of these cartels, and so we kept it up. However,

Speaker 11:         00:59:01       it's fucking, it's wow. I think it's utterly irresponsible and in fact quite despicable of them to put people found out. I have little neighbor kids that don't need to see shit like that backlash should they really any justification for allowing these videos to people. As powerful as David Cameron weigh in on this decision today, the prime minister strongly the move

Speaker 8:          00:59:21       saying we have to protect children from this stuff. David Cameron tweeted, it's irresponsible of Facebook to post beheading videos. Yeah. Especially people were really upset because of what it was showing. And so according to my sources, some of the folks involved in making this decision to leave it up were once again taken into an executives office. And so we went up and, and there was a lot of internal pressure to remove it. And I'd go to my boss and say, Hey, look, uh, this is the decision we made. I recognize this is controversial. I want to let you know why we made these decisions. And they made their case. There are valid and important human rights reasons why you would want this to be out there to show the kind of savagery. And she vehemently disagreed with that. They took another approach arguing that if we take this down, you're deciding to punish people who are trying to raise awareness again. She wasn't budging and just didn't get, didn't get past that. And ultimately, um, I was overruled and we removed it

Speaker 20:         01:00:21       [inaudible]

Speaker 8:          01:00:21       just because there was pressure to do so. The same people that six months prior told them to leave it up because it was news worthy said, take the video down.

Speaker 2:          01:00:30       Facebook this week, reversed the decision and banned a video posted to the site of a woman being beheaded in a segment. Facebook said quote, when we were [inaudible]

Speaker 21:         01:00:38       if you want the one from Boston and you probably should have the one from Mexico in, right. It was a mistake.

Speaker 8:          01:00:46       Yeah. I think it was a mistake because I, I felt like like why do we have these rules in place in the first place? And, and, and it's not the, it's not the only reason, but decisions like that or the thing that precipitated me leaving, leaving. Yeah. Not too long after that incident of few members of the team decided to quit. And what I think this story shows is that Facebook has become too many different things at the same time. So Facebook is now sort of a playground. It's also an r rated movie theater, and now it's the front page of a newspaper. That's all those things. At the same time, it's all those things at the same time. And in what we, the users are demanding of them is that they create a set of policies that are just in the reality is justice means a very different thing in each one of these settings.

Speaker 21:         01:01:46       Justice would mean that the person in Mexico gets told the truth in Mexico by Facebook and the little boy in England doesn't have to look at something gory and horrible in England, but you can't put them together because they clash. Exactly. So how do you solve that?

Speaker 8:          01:02:03       I don't know. I think it's important to keep in mind that a, you, even if you have the perfect set of policies that, that somehow manage to be just in different settings and that can be consistently enforced. The people at the end of the day making decisions,

Speaker 13:         01:02:20       they're still, uh, there's still people. They're still human beings. Is the, is this working or no?

Speaker 17:         01:02:29       I can hear you. Yeah. Great. Okay. At long last we figured it out, Huh? Yeah. Yeah. They yearly.

Speaker 13:         01:02:36       I spoke to one woman who did this work for Facebook.

Speaker 17:         01:02:38       I just wanted to be anonymous. I don't want them to even know that I'm doing it because they might file charges against me.

Speaker 13:         01:02:47       Well, call her Marie. She's from the Philippines where she grew up on a coffee farm.

Speaker 17:         01:02:51       Yeah. That's my father's Gra and I didn't know that coffee was only for adults.

Speaker 13:         01:02:59       She said many afternoons while she was growing up, she and her mother would sit together like outside sipping their coffee and tuning into their short wave radio.

Speaker 22:         01:03:09       This is the voice of America or Washington d C

Speaker 17:         01:03:12       and they'd sit there listening to the voice of America silence.

Speaker 23:         01:03:17       I'm going to ask that we all bow our heads in prayer.

Speaker 13:         01:03:21       She said one of her favorite things to catch on voice of America were Billy Graham sermons.

Speaker 17:         01:03:26       Billy Graham, one of the great evangelists.

Speaker 23:         01:03:28       Oh father, we thank thee for this love of God that reaches around the world and in gulfs all of mankind,

Speaker 17:         01:03:39       but then

Speaker 13:         01:03:44       fast forward 50 years to 2010 and Marie is consuming a very different sort of American media.

Speaker 17:         01:03:50       The videos were the ones that affected me. There were times when I felt really bad that I am a Christian and then I look into this things.

Speaker 13:         01:03:59       She became a content moderator back in 2010 and was actually one of the first people in the Philippines doing this work.

Speaker 17:         01:04:05       I usually had the night shift in the early morning or in the at dawn from 2:00 AM to 4:00 AM

Speaker 13:         01:04:15       she worked from home and despite it being dark out, she put blankets up over the windows so no one could see in at what she was looking at. She locked the door to keep her kids out.

Speaker 17:         01:04:24       Oh, you have to drive them away or I would tell them that it's adult thing they cannot watch

Speaker 13:         01:04:30       and she and the other moderators on her team who lived throughout the Philippines, they were trained on the guidelines on this rule book.

Speaker 17:         01:04:37       There were policies that we have to adhere to, but some of us where we're just clicking pass, pass, pass, even if it's not really passed, just to finish,

Speaker 13:         01:04:49       just to get through the content fast enough and in some cases she thinks

Speaker 17:         01:04:53       a number of the moderators are doing it as a form of retaliation for the low rate

Speaker 13:         01:04:58       people were pissed at the low pay. If I can ask how much, what, how much were you making an hour doing this?

Speaker 17:         01:05:04       As far as I remember it, we were paid like $2 and 50 cents per hour.

Speaker 13:         01:05:12       Marie wouldn't say whether or not this low wage led her to just let things through, but she did say,

Speaker 17:         01:05:19       based on my conservative background, there are things that that I cannot look objectively, so I reject many of the things that I think are not acceptable really. Of course

Speaker 13:         01:05:37       she said whether something was outside the rules or not. If her gut told her to, she just took it down.

Speaker 17:         01:05:43       Whenever it affects me a lot, I would click the button now like it's a violation because if it's going to disturb the young audience then it should not be there. So like if there's a new person,

Speaker 13:         01:05:59       whether it was a breastfeeding photo or an anatomy video or a piece of art,

Speaker 17:         01:06:03       I would consider it as pornography and then click right away.

Speaker 13:         01:06:10       It's a violation. You took the line to your own hands, you went vigilante.

Speaker 17:         01:06:16       Yeah. Or something. So yeah, I have to protect kids from those evil uh, side of humankind.

Speaker 4:          01:07:02       [inaudible] [inaudible] [inaudible]

Speaker 24:         01:07:13       where does that leave you feeling? They don't leave you feeling it. This is just I'm at the end. This is just um,

Speaker 13:         01:07:19       undoable. Um, I, I think they will inevitably fail, but they have to try and, and, and I think we should all be rooting for them.

Speaker 4:          01:08:00       [inaudible] [inaudible]

Speaker 24:         01:08:01       this episode was reported by Simon Adler with help from Tracy Hunt and produced by Simon with help from Bethel hop. Tay. Big thanks to Sarah Roberts whose research into commercial content moderation got us going a big time and we thank her very, very much for that. Thanks. Also to Jeffrey Rosen who helped us in our thinking about what Facebook is true. Michael Churnis, whose voice we used to mask other people's voices to Caroline Glanville Rashika booed, Raja Ryan Dougan, Ellen silver, James Mitchell and Guy Rosen of course, to all the content moderators who took the time to talk to us and uh, threw out to sign off. Yeah, I guess you said, Huh? Ready? You want to go first? Yeah. I'm Chad. I boom, rod. I remember college. Thanks to listen.