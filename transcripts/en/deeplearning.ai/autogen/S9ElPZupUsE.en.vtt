WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.149
welcome to the last video for this week

00:00:02.149 --> 00:00:02.159
welcome to the last video for this week
 

00:00:02.159 --> 00:00:04.579
welcome to the last video for this week
there are many great deep learning

00:00:04.579 --> 00:00:04.589
there are many great deep learning
 

00:00:04.589 --> 00:00:06.680
there are many great deep learning
programming frameworks one of them is

00:00:06.680 --> 00:00:06.690
programming frameworks one of them is
 

00:00:06.690 --> 00:00:08.900
programming frameworks one of them is
tensorflow I'm excited to the help you

00:00:08.900 --> 00:00:08.910
tensorflow I'm excited to the help you
 

00:00:08.910 --> 00:00:11.509
tensorflow I'm excited to the help you
start to learn to use tender flow what I

00:00:11.509 --> 00:00:11.519
start to learn to use tender flow what I
 

00:00:11.519 --> 00:00:13.220
start to learn to use tender flow what I
want to do in this video is show you the

00:00:13.220 --> 00:00:13.230
want to do in this video is show you the
 

00:00:13.230 --> 00:00:15.589
want to do in this video is show you the
basic structure of a tensor flow program

00:00:15.589 --> 00:00:15.599
basic structure of a tensor flow program
 

00:00:15.599 --> 00:00:18.019
basic structure of a tensor flow program
and then leave you to practice and learn

00:00:18.019 --> 00:00:18.029
and then leave you to practice and learn
 

00:00:18.029 --> 00:00:20.090
and then leave you to practice and learn
more details and practice on yourself in

00:00:20.090 --> 00:00:20.100
more details and practice on yourself in
 

00:00:20.100 --> 00:00:22.429
more details and practice on yourself in
this week's projects designs this week's

00:00:22.429 --> 00:00:22.439
this week's projects designs this week's
 

00:00:22.439 --> 00:00:24.200
this week's projects designs this week's
Pro exercise will take some time to do

00:00:24.200 --> 00:00:24.210
Pro exercise will take some time to do
 

00:00:24.210 --> 00:00:26.060
Pro exercise will take some time to do
so please be sure to leave some extra

00:00:26.060 --> 00:00:26.070
so please be sure to leave some extra
 

00:00:26.070 --> 00:00:28.880
so please be sure to leave some extra
time to do it as the motivating problem

00:00:28.880 --> 00:00:28.890
time to do it as the motivating problem
 

00:00:28.890 --> 00:00:31.429
time to do it as the motivating problem
let's say that you have some cost

00:00:31.429 --> 00:00:31.439
let's say that you have some cost
 

00:00:31.439 --> 00:00:34.100
let's say that you have some cost
function J that you want to minimize and

00:00:34.100 --> 00:00:34.110
function J that you want to minimize and
 

00:00:34.110 --> 00:00:35.810
function J that you want to minimize and
for this example I'm going to use this

00:00:35.810 --> 00:00:35.820
for this example I'm going to use this
 

00:00:35.820 --> 00:00:38.990
for this example I'm going to use this
highly simple cost function J of W

00:00:38.990 --> 00:00:39.000
highly simple cost function J of W
 

00:00:39.000 --> 00:00:45.020
highly simple cost function J of W
equals W squared minus 10 W plus 25 so

00:00:45.020 --> 00:00:45.030
equals W squared minus 10 W plus 25 so
 

00:00:45.030 --> 00:00:47.029
equals W squared minus 10 W plus 25 so
that's the cost function you might

00:00:47.029 --> 00:00:47.039
that's the cost function you might
 

00:00:47.039 --> 00:00:50.319
that's the cost function you might
notice that this function is actually W

00:00:50.319 --> 00:00:50.329
notice that this function is actually W
 

00:00:50.329 --> 00:00:54.529
notice that this function is actually W
minus 5 squared if you expand out this

00:00:54.529 --> 00:00:54.539
minus 5 squared if you expand out this
 

00:00:54.539 --> 00:00:56.420
minus 5 squared if you expand out this
quadratic together expression above and

00:00:56.420 --> 00:00:56.430
quadratic together expression above and
 

00:00:56.430 --> 00:00:59.360
quadratic together expression above and
so the value of W that minimizes this is

00:00:59.360 --> 00:00:59.370
so the value of W that minimizes this is
 

00:00:59.370 --> 00:01:02.420
so the value of W that minimizes this is
w equals 5 but let's say we didn't know

00:01:02.420 --> 00:01:02.430
w equals 5 but let's say we didn't know
 

00:01:02.430 --> 00:01:05.119
w equals 5 but let's say we didn't know
that and you just have this function

00:01:05.119 --> 00:01:05.129
that and you just have this function
 

00:01:05.129 --> 00:01:06.800
that and you just have this function
let's see how you can implement

00:01:06.800 --> 00:01:06.810
let's see how you can implement
 

00:01:06.810 --> 00:01:09.560
let's see how you can implement
something intensive low to minimize this

00:01:09.560 --> 00:01:09.570
something intensive low to minimize this
 

00:01:09.570 --> 00:01:11.810
something intensive low to minimize this
because a very similar structure a

00:01:11.810 --> 00:01:11.820
because a very similar structure a
 

00:01:11.820 --> 00:01:13.760
because a very similar structure a
program can be used to train your

00:01:13.760 --> 00:01:13.770
program can be used to train your
 

00:01:13.770 --> 00:01:15.950
program can be used to train your
network where you can have some

00:01:15.950 --> 00:01:15.960
network where you can have some
 

00:01:15.960 --> 00:01:19.039
network where you can have some
complicated cost function J of W B

00:01:19.039 --> 00:01:19.049
complicated cost function J of W B
 

00:01:19.049 --> 00:01:21.980
complicated cost function J of W B
depending on all the parameters of your

00:01:21.980 --> 00:01:21.990
depending on all the parameters of your
 

00:01:21.990 --> 00:01:24.920
depending on all the parameters of your
neural network and then similarly you be

00:01:24.920 --> 00:01:24.930
neural network and then similarly you be
 

00:01:24.930 --> 00:01:27.499
neural network and then similarly you be
able to use tensorflow to automatically

00:01:27.499 --> 00:01:27.509
able to use tensorflow to automatically
 

00:01:27.509 --> 00:01:31.039
able to use tensorflow to automatically
try to find values of W and B then

00:01:31.039 --> 00:01:31.049
try to find values of W and B then
 

00:01:31.049 --> 00:01:33.230
try to find values of W and B then
minimize this cost function but let's

00:01:33.230 --> 00:01:33.240
minimize this cost function but let's
 

00:01:33.240 --> 00:01:34.999
minimize this cost function but let's
start with the simpler example on the

00:01:34.999 --> 00:01:35.009
start with the simpler example on the
 

00:01:35.009 --> 00:01:38.450
start with the simpler example on the
left so I'm running Python in my jupiter

00:01:38.450 --> 00:01:38.460
left so I'm running Python in my jupiter
 

00:01:38.460 --> 00:01:41.749
left so I'm running Python in my jupiter
notebook and so start-up tender so you

00:01:41.749 --> 00:01:41.759
notebook and so start-up tender so you
 

00:01:41.759 --> 00:01:44.960
notebook and so start-up tender so you
import numpy as empty and is idiomatic

00:01:44.960 --> 00:01:44.970
import numpy as empty and is idiomatic
 

00:01:44.970 --> 00:01:48.800
import numpy as empty and is idiomatic
to use import into flow as TF next let

00:01:48.800 --> 00:01:48.810
to use import into flow as TF next let
 

00:01:48.810 --> 00:01:54.469
to use import into flow as TF next let
me define the parameter W so intent of

00:01:54.469 --> 00:01:54.479
me define the parameter W so intent of
 

00:01:54.479 --> 00:01:56.870
me define the parameter W so intent of
flow you're going to use TF dot variable

00:01:56.870 --> 00:01:56.880
flow you're going to use TF dot variable
 

00:01:56.880 --> 00:02:03.380
flow you're going to use TF dot variable
to define a parameter title equals T F

00:02:03.380 --> 00:02:03.390
to define a parameter title equals T F
 

00:02:03.390 --> 00:02:05.240
to define a parameter title equals T F
dot float

00:02:05.240 --> 00:02:05.250
dot float
 

00:02:05.250 --> 00:02:09.800
dot float
32 and then let's define the cost

00:02:09.800 --> 00:02:09.810
32 and then let's define the cost
 

00:02:09.810 --> 00:02:11.600
32 and then let's define the cost
function so remember the cost function

00:02:11.600 --> 00:02:11.610
function so remember the cost function
 

00:02:11.610 --> 00:02:15.530
function so remember the cost function
was w squared minus 10 W plus 25

00:02:15.530 --> 00:02:15.540
was w squared minus 10 W plus 25
 

00:02:15.540 --> 00:02:20.510
was w squared minus 10 W plus 25
so you just PF dot add some would have W

00:02:20.510 --> 00:02:20.520
so you just PF dot add some would have W
 

00:02:20.520 --> 00:02:27.350
so you just PF dot add some would have W
squared plus TF dot multiply so the

00:02:27.350 --> 00:02:27.360
squared plus TF dot multiply so the
 

00:02:27.360 --> 00:02:31.040
squared plus TF dot multiply so the
second term was minus 10 times W and

00:02:31.040 --> 00:02:31.050
second term was minus 10 times W and
 

00:02:31.050 --> 00:02:35.750
second term was minus 10 times W and
then I'm going to add that to 25 so let

00:02:35.750 --> 00:02:35.760
then I'm going to add that to 25 so let
 

00:02:35.760 --> 00:02:40.520
then I'm going to add that to 25 so let
me put another GF dot ad over there so

00:02:40.520 --> 00:02:40.530
me put another GF dot ad over there so
 

00:02:40.530 --> 00:02:42.770
me put another GF dot ad over there so
that defines the cost J that we had and

00:02:42.770 --> 00:02:42.780
that defines the cost J that we had and
 

00:02:42.780 --> 00:02:46.340
that defines the cost J that we had and
then I'm going to write train equals T f

00:02:46.340 --> 00:02:46.350
then I'm going to write train equals T f
 

00:02:46.350 --> 00:02:53.080
then I'm going to write train equals T f
dot train dot gradient descent optimizer

00:02:53.080 --> 00:02:53.090
dot train dot gradient descent optimizer
 

00:02:53.090 --> 00:02:56.030
dot train dot gradient descent optimizer
let's use a learning rate of 0.01 and

00:02:56.030 --> 00:02:56.040
let's use a learning rate of 0.01 and
 

00:02:56.040 --> 00:03:01.780
let's use a learning rate of 0.01 and
the goal is to minimize the cost and

00:03:01.780 --> 00:03:01.790
the goal is to minimize the cost and
 

00:03:01.790 --> 00:03:04.670
the goal is to minimize the cost and
finally the following few lines are

00:03:04.670 --> 00:03:04.680
finally the following few lines are
 

00:03:04.680 --> 00:03:07.990
finally the following few lines are
quite idiomatic in it equals P f dot

00:03:07.990 --> 00:03:08.000
quite idiomatic in it equals P f dot
 

00:03:08.000 --> 00:03:14.680
quite idiomatic in it equals P f dot
global variables initializer and then on

00:03:14.680 --> 00:03:14.690
global variables initializer and then on
 

00:03:14.690 --> 00:03:18.110
global variables initializer and then on
session equals T F dot the profession

00:03:18.110 --> 00:03:18.120
session equals T F dot the profession
 

00:03:18.120 --> 00:03:21.229
session equals T F dot the profession
from starter sensical session session

00:03:21.229 --> 00:03:21.239
from starter sensical session session
 

00:03:21.239 --> 00:03:24.229
from starter sensical session session
you got to run a lit to initialize a

00:03:24.229 --> 00:03:24.239
you got to run a lit to initialize a
 

00:03:24.239 --> 00:03:27.560
you got to run a lit to initialize a
global variables and then for tend to

00:03:27.560 --> 00:03:27.570
global variables and then for tend to
 

00:03:27.570 --> 00:03:29.449
global variables and then for tend to
filter you value the variable we're

00:03:29.449 --> 00:03:29.459
filter you value the variable we're
 

00:03:29.459 --> 00:03:33.530
filter you value the variable we're
going to use set start run W we haven't

00:03:33.530 --> 00:03:33.540
going to use set start run W we haven't
 

00:03:33.540 --> 00:03:35.870
going to use set start run W we haven't
done anything yet so with this line

00:03:35.870 --> 00:03:35.880
done anything yet so with this line
 

00:03:35.880 --> 00:03:38.180
done anything yet so with this line
above initialize W to zero and define a

00:03:38.180 --> 00:03:38.190
above initialize W to zero and define a
 

00:03:38.190 --> 00:03:40.820
above initialize W to zero and define a
cost function will define train to be

00:03:40.820 --> 00:03:40.830
cost function will define train to be
 

00:03:40.830 --> 00:03:42.680
cost function will define train to be
our learning algorithm which uses a

00:03:42.680 --> 00:03:42.690
our learning algorithm which uses a
 

00:03:42.690 --> 00:03:45.020
our learning algorithm which uses a
gradient descent optimizer to minimize

00:03:45.020 --> 00:03:45.030
gradient descent optimizer to minimize
 

00:03:45.030 --> 00:03:46.820
gradient descent optimizer to minimize
the cost function but we haven't

00:03:46.820 --> 00:03:46.830
the cost function but we haven't
 

00:03:46.830 --> 00:03:48.890
the cost function but we haven't
actually run the learning algorithm yet

00:03:48.890 --> 00:03:48.900
actually run the learning algorithm yet
 

00:03:48.900 --> 00:03:52.190
actually run the learning algorithm yet
so sessions are run we evaluate W and

00:03:52.190 --> 00:03:52.200
so sessions are run we evaluate W and
 

00:03:52.200 --> 00:03:56.060
so sessions are run we evaluate W and
then we print session run so if you run

00:03:56.060 --> 00:03:56.070
then we print session run so if you run
 

00:03:56.070 --> 00:03:58.699
then we print session run so if you run
that evaluate W to be equal to zero

00:03:58.699 --> 00:03:58.709
that evaluate W to be equal to zero
 

00:03:58.709 --> 00:04:00.199
that evaluate W to be equal to zero
because you haven't done anything yet

00:04:00.199 --> 00:04:00.209
because you haven't done anything yet
 

00:04:00.209 --> 00:04:05.960
because you haven't done anything yet
now let's do sessions are run on train

00:04:05.960 --> 00:04:05.970
now let's do sessions are run on train
 

00:04:05.970 --> 00:04:09.440
now let's do sessions are run on train
so what this will do is run one step of

00:04:09.440 --> 00:04:09.450
so what this will do is run one step of
 

00:04:09.450 --> 00:04:13.000
so what this will do is run one step of
gradient descent and then let's

00:04:13.000 --> 00:04:13.010
gradient descent and then let's
 

00:04:13.010 --> 00:04:16.670
gradient descent and then let's
evaluate the value of W after one step

00:04:16.670 --> 00:04:16.680
evaluate the value of W after one step
 

00:04:16.680 --> 00:04:20.030
evaluate the value of W after one step
of gradient descent and print that so we

00:04:20.030 --> 00:04:20.040
of gradient descent and print that so we
 

00:04:20.040 --> 00:04:22.039
of gradient descent and print that so we
do that after one step agreeing to send

00:04:22.039 --> 00:04:22.049
do that after one step agreeing to send
 

00:04:22.049 --> 00:04:26.660
do that after one step agreeing to send
W is now zero point one let's now run a

00:04:26.660 --> 00:04:26.670
W is now zero point one let's now run a
 

00:04:26.670 --> 00:04:28.310
W is now zero point one let's now run a
thousand iterations of gradient descent

00:04:28.310 --> 00:04:28.320
thousand iterations of gradient descent
 

00:04:28.320 --> 00:04:37.310
thousand iterations of gradient descent
so run train and let's then print

00:04:37.310 --> 00:04:37.320
so run train and let's then print
 

00:04:37.320 --> 00:04:42.410
so run train and let's then print
session that and run W so this is run a

00:04:42.410 --> 00:04:42.420
session that and run W so this is run a
 

00:04:42.420 --> 00:04:44.030
session that and run W so this is run a
thousand iterations of grande descent

00:04:44.030 --> 00:04:44.040
thousand iterations of grande descent
 

00:04:44.040 --> 00:04:46.850
thousand iterations of grande descent
and at the end W ends up being four

00:04:46.850 --> 00:04:46.860
and at the end W ends up being four
 

00:04:46.860 --> 00:04:48.590
and at the end W ends up being four
point nine nine nine nine remember we

00:04:48.590 --> 00:04:48.600
point nine nine nine nine remember we
 

00:04:48.600 --> 00:04:51.409
point nine nine nine nine remember we
said that we're minimizing W minus five

00:04:51.409 --> 00:04:51.419
said that we're minimizing W minus five
 

00:04:51.419 --> 00:04:53.750
said that we're minimizing W minus five
squared so the optimal value of W is

00:04:53.750 --> 00:04:53.760
squared so the optimal value of W is
 

00:04:53.760 --> 00:04:57.110
squared so the optimal value of W is
five and got very close to this so hope

00:04:57.110 --> 00:04:57.120
five and got very close to this so hope
 

00:04:57.120 --> 00:04:59.090
five and got very close to this so hope
this gives you a sense of the broad

00:04:59.090 --> 00:04:59.100
this gives you a sense of the broad
 

00:04:59.100 --> 00:05:02.090
this gives you a sense of the broad
structure of a tensor flow program and

00:05:02.090 --> 00:05:02.100
structure of a tensor flow program and
 

00:05:02.100 --> 00:05:04.790
structure of a tensor flow program and
as you do therefore we exercise and play

00:05:04.790 --> 00:05:04.800
as you do therefore we exercise and play
 

00:05:04.800 --> 00:05:08.900
as you do therefore we exercise and play
with more tensorflow close yourself some

00:05:08.900 --> 00:05:08.910
with more tensorflow close yourself some
 

00:05:08.910 --> 00:05:10.580
with more tensorflow close yourself some
of these functions that I'm using here

00:05:10.580 --> 00:05:10.590
of these functions that I'm using here
 

00:05:10.590 --> 00:05:13.280
of these functions that I'm using here
will become more familiar some things to

00:05:13.280 --> 00:05:13.290
will become more familiar some things to
 

00:05:13.290 --> 00:05:16.100
will become more familiar some things to
notice about this w is the parameter

00:05:16.100 --> 00:05:16.110
notice about this w is the parameter
 

00:05:16.110 --> 00:05:17.510
notice about this w is the parameter
we're trying to optimize so we're going

00:05:17.510 --> 00:05:17.520
we're trying to optimize so we're going
 

00:05:17.520 --> 00:05:20.120
we're trying to optimize so we're going
to declare that as a variable and notice

00:05:20.120 --> 00:05:20.130
to declare that as a variable and notice
 

00:05:20.130 --> 00:05:22.190
to declare that as a variable and notice
that all we had to do was define a cost

00:05:22.190 --> 00:05:22.200
that all we had to do was define a cost
 

00:05:22.200 --> 00:05:24.469
that all we had to do was define a cost
function using these add and multiply

00:05:24.469 --> 00:05:24.479
function using these add and multiply
 

00:05:24.479 --> 00:05:28.270
function using these add and multiply
and so on functions and tend to throws

00:05:28.270 --> 00:05:28.280
and so on functions and tend to throws
 

00:05:28.280 --> 00:05:30.500
and so on functions and tend to throws
automatically how to take derivatives

00:05:30.500 --> 00:05:30.510
automatically how to take derivatives
 

00:05:30.510 --> 00:05:32.990
automatically how to take derivatives
respect to the add and multiply as well

00:05:32.990 --> 00:05:33.000
respect to the add and multiply as well
 

00:05:33.000 --> 00:05:35.420
respect to the add and multiply as well
as other functions which is why you only

00:05:35.420 --> 00:05:35.430
as other functions which is why you only
 

00:05:35.430 --> 00:05:37.640
as other functions which is why you only
have to implement basically forward prop

00:05:37.640 --> 00:05:37.650
have to implement basically forward prop
 

00:05:37.650 --> 00:05:40.850
have to implement basically forward prop
and it can figure out how to do the back

00:05:40.850 --> 00:05:40.860
and it can figure out how to do the back
 

00:05:40.860 --> 00:05:42.760
and it can figure out how to do the back
problem of the gradient computation

00:05:42.760 --> 00:05:42.770
problem of the gradient computation
 

00:05:42.770 --> 00:05:46.510
problem of the gradient computation
because that's already built in to the

00:05:46.510 --> 00:05:46.520
because that's already built in to the
 

00:05:46.520 --> 00:05:49.490
because that's already built in to the
add and multiply as well as the squaring

00:05:49.490 --> 00:05:49.500
add and multiply as well as the squaring
 

00:05:49.500 --> 00:05:51.800
add and multiply as well as the squaring
functions by the way in case this

00:05:51.800 --> 00:05:51.810
functions by the way in case this
 

00:05:51.810 --> 00:05:54.590
functions by the way in case this
notation seems really ugly since the

00:05:54.590 --> 00:05:54.600
notation seems really ugly since the
 

00:05:54.600 --> 00:05:56.480
notation seems really ugly since the
flow actually has overloaded the

00:05:56.480 --> 00:05:56.490
flow actually has overloaded the
 

00:05:56.490 --> 00:06:02.300
flow actually has overloaded the
computation for the usual plus minus and

00:06:02.300 --> 00:06:02.310
computation for the usual plus minus and
 

00:06:02.310 --> 00:06:04.730
computation for the usual plus minus and
so on so you can also just write this

00:06:04.730 --> 00:06:04.740
so on so you can also just write this
 

00:06:04.740 --> 00:06:07.909
so on so you can also just write this
nicer format so it cost to comment that

00:06:07.909 --> 00:06:07.919
nicer format so it cost to comment that
 

00:06:07.919 --> 00:06:10.370
nicer format so it cost to comment that
out and if you run this and get the same

00:06:10.370 --> 00:06:10.380
out and if you run this and get the same
 

00:06:10.380 --> 00:06:11.180
out and if you run this and get the same
result

00:06:11.180 --> 00:06:11.190
result
 

00:06:11.190 --> 00:06:13.730
result
so once W is declared to be attentive so

00:06:13.730 --> 00:06:13.740
so once W is declared to be attentive so
 

00:06:13.740 --> 00:06:16.250
so once W is declared to be attentive so
variable these squaring multiplication

00:06:16.250 --> 00:06:16.260
variable these squaring multiplication
 

00:06:16.260 --> 00:06:17.990
variable these squaring multiplication
adding and subtraction operations are

00:06:17.990 --> 00:06:18.000
adding and subtraction operations are
 

00:06:18.000 --> 00:06:19.969
adding and subtraction operations are
over though this you don't need to use

00:06:19.969 --> 00:06:19.979
over though this you don't need to use
 

00:06:19.979 --> 00:06:21.350
over though this you don't need to use
this a griffon check

00:06:21.350 --> 00:06:21.360
this a griffon check
 

00:06:21.360 --> 00:06:23.749
this a griffon check
had above now there's just one more

00:06:23.749 --> 00:06:23.759
had above now there's just one more
 

00:06:23.759 --> 00:06:25.279
had above now there's just one more
feature of ten to fill that I want to

00:06:25.279 --> 00:06:25.289
feature of ten to fill that I want to
 

00:06:25.289 --> 00:06:28.429
feature of ten to fill that I want to
show you which is this example minimize

00:06:28.429 --> 00:06:28.439
show you which is this example minimize
 

00:06:28.439 --> 00:06:31.670
show you which is this example minimize
a fixed function of W one of the

00:06:31.670 --> 00:06:31.680
a fixed function of W one of the
 

00:06:31.680 --> 00:06:33.140
a fixed function of W one of the
function you want to minimize is a

00:06:33.140 --> 00:06:33.150
function you want to minimize is a
 

00:06:33.150 --> 00:06:35.059
function you want to minimize is a
function of your training set so

00:06:35.059 --> 00:06:35.069
function of your training set so
 

00:06:35.069 --> 00:06:37.629
function of your training set so
whatever you have some training data X

00:06:37.629 --> 00:06:37.639
whatever you have some training data X
 

00:06:37.639 --> 00:06:40.040
whatever you have some training data X
and when you train your neural network

00:06:40.040 --> 00:06:40.050
and when you train your neural network
 

00:06:40.050 --> 00:06:43.550
and when you train your neural network
the training data X can change so how do

00:06:43.550 --> 00:06:43.560
the training data X can change so how do
 

00:06:43.560 --> 00:06:47.170
the training data X can change so how do
you get training data into a 10-2 phone

00:06:47.170 --> 00:06:47.180
you get training data into a 10-2 phone
 

00:06:47.180 --> 00:06:51.230
you get training data into a 10-2 phone
program so I'm going to find key X which

00:06:51.230 --> 00:06:51.240
program so I'm going to find key X which
 

00:06:51.240 --> 00:06:52.850
program so I'm going to find key X which
is think of this as playing a role of a

00:06:52.850 --> 00:06:52.860
is think of this as playing a role of a
 

00:06:52.860 --> 00:06:55.309
is think of this as playing a role of a
training data or really the training

00:06:55.309 --> 00:06:55.319
training data or really the training
 

00:06:55.319 --> 00:06:58.369
training data or really the training
data with both x and y but we only get X

00:06:58.369 --> 00:06:58.379
data with both x and y but we only get X
 

00:06:58.379 --> 00:07:01.309
data with both x and y but we only get X
in this example so there's going to

00:07:01.309 --> 00:07:01.319
in this example so there's going to
 

00:07:01.319 --> 00:07:03.589
in this example so there's going to
define exterior placeholder and it's

00:07:03.589 --> 00:07:03.599
define exterior placeholder and it's
 

00:07:03.599 --> 00:07:07.249
define exterior placeholder and it's
going to be of type float 32 and let's

00:07:07.249 --> 00:07:07.259
going to be of type float 32 and let's
 

00:07:07.259 --> 00:07:10.399
going to be of type float 32 and let's
make those a three by one array and what

00:07:10.399 --> 00:07:10.409
make those a three by one array and what
 

00:07:10.409 --> 00:07:13.279
make those a three by one array and what
I'm going to do is whereas the cost here

00:07:13.279 --> 00:07:13.289
I'm going to do is whereas the cost here
 

00:07:13.289 --> 00:07:15.830
I'm going to do is whereas the cost here
has fixed coefficients in front of the V

00:07:15.830 --> 00:07:15.840
has fixed coefficients in front of the V
 

00:07:15.840 --> 00:07:18.619
has fixed coefficients in front of the V
terms in this quadratic use one times W

00:07:18.619 --> 00:07:18.629
terms in this quadratic use one times W
 

00:07:18.629 --> 00:07:21.409
terms in this quadratic use one times W
squared minus ten times W plus 25 we

00:07:21.409 --> 00:07:21.419
squared minus ten times W plus 25 we
 

00:07:21.419 --> 00:07:24.260
squared minus ten times W plus 25 we
could turn these numbers 1 minus 10 and

00:07:24.260 --> 00:07:24.270
could turn these numbers 1 minus 10 and
 

00:07:24.270 --> 00:07:27.890
could turn these numbers 1 minus 10 and
25 into data so what I'm going to do is

00:07:27.890 --> 00:07:27.900
25 into data so what I'm going to do is
 

00:07:27.900 --> 00:07:33.290
25 into data so what I'm going to do is
replace the cost with cost equals x 0 0

00:07:33.290 --> 00:07:33.300
replace the cost with cost equals x 0 0
 

00:07:33.300 --> 00:07:42.649
replace the cost with cost equals x 0 0
times W squared plus x10 times W plus x2

00:07:42.649 --> 00:07:42.659
times W squared plus x10 times W plus x2
 

00:07:42.659 --> 00:07:54.230
times W squared plus x10 times W plus x2
0 o times 1 so now X becomes sort of

00:07:54.230 --> 00:07:54.240
0 o times 1 so now X becomes sort of
 

00:07:54.240 --> 00:07:56.629
0 o times 1 so now X becomes sort of
like data that controls the coefficients

00:07:56.629 --> 00:07:56.639
like data that controls the coefficients
 

00:07:56.639 --> 00:07:59.600
like data that controls the coefficients
of this quadratic function and this

00:07:59.600 --> 00:07:59.610
of this quadratic function and this
 

00:07:59.610 --> 00:08:03.860
of this quadratic function and this
placeholder function tells tensorflow

00:08:03.860 --> 00:08:03.870
placeholder function tells tensorflow
 

00:08:03.870 --> 00:08:07.159
placeholder function tells tensorflow
that X is something that you provide the

00:08:07.159 --> 00:08:07.169
that X is something that you provide the
 

00:08:07.169 --> 00:08:10.429
that X is something that you provide the
values for the retailer so let's define

00:08:10.429 --> 00:08:10.439
values for the retailer so let's define
 

00:08:10.439 --> 00:08:15.170
values for the retailer so let's define
another arrays coefficient equals NP

00:08:15.170 --> 00:08:15.180
another arrays coefficient equals NP
 

00:08:15.180 --> 00:08:21.520
another arrays coefficient equals NP
dot array 1

00:08:21.520 --> 00:08:21.530
 

00:08:21.530 --> 00:08:27.740
- 10 and yes the last value is 25 so

00:08:27.740 --> 00:08:27.750
- 10 and yes the last value is 25 so
 

00:08:27.750 --> 00:08:29.570
- 10 and yes the last value is 25 so
that's going to be the data that we're

00:08:29.570 --> 00:08:29.580
that's going to be the data that we're
 

00:08:29.580 --> 00:08:33.110
that's going to be the data that we're
going to plug into X so finally we need

00:08:33.110 --> 00:08:33.120
going to plug into X so finally we need
 

00:08:33.120 --> 00:08:36.980
going to plug into X so finally we need
a way to get this a very coefficients

00:08:36.980 --> 00:08:36.990
a way to get this a very coefficients
 

00:08:36.990 --> 00:08:39.650
a way to get this a very coefficients
into the variable X and the syntax to do

00:08:39.650 --> 00:08:39.660
into the variable X and the syntax to do
 

00:08:39.660 --> 00:08:43.820
into the variable X and the syntax to do
that is doing the training step that the

00:08:43.820 --> 00:08:43.830
that is doing the training step that the
 

00:08:43.830 --> 00:08:46.880
that is doing the training step that the
value 4 will need to be provided for X

00:08:46.880 --> 00:08:46.890
value 4 will need to be provided for X
 

00:08:46.890 --> 00:08:53.110
value 4 will need to be provided for X
so I'm going to set here 6 equals x

00:08:53.110 --> 00:08:53.120
so I'm going to set here 6 equals x
 

00:08:53.120 --> 00:08:58.280
so I'm going to set here 6 equals x
that's through coefficients and I'm

00:08:58.280 --> 00:08:58.290
that's through coefficients and I'm
 

00:08:58.290 --> 00:09:02.450
that's through coefficients and I'm
going to change this in a copy and paste

00:09:02.450 --> 00:09:02.460
going to change this in a copy and paste
 

00:09:02.460 --> 00:09:04.520
going to change this in a copy and paste
and put that here as well all right

00:09:04.520 --> 00:09:04.530
and put that here as well all right
 

00:09:04.530 --> 00:09:05.630
and put that here as well all right
hopefully I didn't have any syntax

00:09:05.630 --> 00:09:05.640
hopefully I didn't have any syntax
 

00:09:05.640 --> 00:09:09.160
hopefully I didn't have any syntax
errors let's try to be running this and

00:09:09.160 --> 00:09:09.170
errors let's try to be running this and
 

00:09:09.170 --> 00:09:12.190
errors let's try to be running this and
we get the same results hopefully as

00:09:12.190 --> 00:09:12.200
we get the same results hopefully as
 

00:09:12.200 --> 00:09:15.890
we get the same results hopefully as
before and now if you want to change the

00:09:15.890 --> 00:09:15.900
before and now if you want to change the
 

00:09:15.900 --> 00:09:17.360
before and now if you want to change the
coefficients of this quadratic function

00:09:17.360 --> 00:09:17.370
coefficients of this quadratic function
 

00:09:17.370 --> 00:09:20.090
coefficients of this quadratic function
let's say you take this 10 and change it

00:09:20.090 --> 00:09:20.100
let's say you take this 10 and change it
 

00:09:20.100 --> 00:09:25.750
let's say you take this 10 and change it
to 20 minus 20 and let's change this to

00:09:25.750 --> 00:09:25.760
to 20 minus 20 and let's change this to
 

00:09:25.760 --> 00:09:29.390
to 20 minus 20 and let's change this to
100 so this is now the function X minus

00:09:29.390 --> 00:09:29.400
100 so this is now the function X minus
 

00:09:29.400 --> 00:09:34.430
100 so this is now the function X minus
10 squared and if I rerun this hopefully

00:09:34.430 --> 00:09:34.440
10 squared and if I rerun this hopefully
 

00:09:34.440 --> 00:09:36.920
10 squared and if I rerun this hopefully
I find that the value that minimizes X

00:09:36.920 --> 00:09:36.930
I find that the value that minimizes X
 

00:09:36.930 --> 00:09:39.830
I find that the value that minimizes X
minus 10 squared this w equals 10 let's

00:09:39.830 --> 00:09:39.840
minus 10 squared this w equals 10 let's
 

00:09:39.840 --> 00:09:42.710
minus 10 squared this w equals 10 let's
see cool great we got W very close to 10

00:09:42.710 --> 00:09:42.720
see cool great we got W very close to 10
 

00:09:42.720 --> 00:09:44.900
see cool great we got W very close to 10
after running a thousand iterations of

00:09:44.900 --> 00:09:44.910
after running a thousand iterations of
 

00:09:44.910 --> 00:09:47.780
after running a thousand iterations of
gradient descent so what you see more of

00:09:47.780 --> 00:09:47.790
gradient descent so what you see more of
 

00:09:47.790 --> 00:09:50.170
gradient descent so what you see more of
when you do the exercise is that a

00:09:50.170 --> 00:09:50.180
when you do the exercise is that a
 

00:09:50.180 --> 00:09:52.640
when you do the exercise is that a
placeholder in terms of flow is a

00:09:52.640 --> 00:09:52.650
placeholder in terms of flow is a
 

00:09:52.650 --> 00:09:54.710
placeholder in terms of flow is a
variable whose value you assign later

00:09:54.710 --> 00:09:54.720
variable whose value you assign later
 

00:09:54.720 --> 00:09:59.180
variable whose value you assign later
and this is a convenient way to get your

00:09:59.180 --> 00:09:59.190
and this is a convenient way to get your
 

00:09:59.190 --> 00:10:02.270
and this is a convenient way to get your
training data into the cost function and

00:10:02.270 --> 00:10:02.280
training data into the cost function and
 

00:10:02.280 --> 00:10:05.420
training data into the cost function and
the way you get your data into the cost

00:10:05.420 --> 00:10:05.430
the way you get your data into the cost
 

00:10:05.430 --> 00:10:07.850
the way you get your data into the cost
function is with this syntax on when

00:10:07.850 --> 00:10:07.860
function is with this syntax on when
 

00:10:07.860 --> 00:10:10.310
function is with this syntax on when
you're running a training iteration to

00:10:10.310 --> 00:10:10.320
you're running a training iteration to
 

00:10:10.320 --> 00:10:13.100
you're running a training iteration to
use the feet dip to set X to be equal to

00:10:13.100 --> 00:10:13.110
use the feet dip to set X to be equal to
 

00:10:13.110 --> 00:10:15.470
use the feet dip to set X to be equal to
the coefficients here and if you're

00:10:15.470 --> 00:10:15.480
the coefficients here and if you're
 

00:10:15.480 --> 00:10:17.390
the coefficients here and if you're
doing mini-batch gradient descent where

00:10:17.390 --> 00:10:17.400
doing mini-batch gradient descent where
 

00:10:17.400 --> 00:10:19.430
doing mini-batch gradient descent where
on each iteration you need to plug in a

00:10:19.430 --> 00:10:19.440
on each iteration you need to plug in a
 

00:10:19.440 --> 00:10:21.590
on each iteration you need to plug in a
different mini batch then on different

00:10:21.590 --> 00:10:21.600
different mini batch then on different
 

00:10:21.600 --> 00:10:23.690
different mini batch then on different
iterations you use the feet thick to

00:10:23.690 --> 00:10:23.700
iterations you use the feet thick to
 

00:10:23.700 --> 00:10:25.790
iterations you use the feet thick to
feed in different subsets of your

00:10:25.790 --> 00:10:25.800
feed in different subsets of your
 

00:10:25.800 --> 00:10:27.480
feed in different subsets of your
training set different meaning

00:10:27.480 --> 00:10:27.490
training set different meaning
 

00:10:27.490 --> 00:10:29.620
training set different meaning
into where your cost function is

00:10:29.620 --> 00:10:29.630
into where your cost function is
 

00:10:29.630 --> 00:10:32.740
into where your cost function is
expecting to see data so hopefully this

00:10:32.740 --> 00:10:32.750
expecting to see data so hopefully this
 

00:10:32.750 --> 00:10:34.900
expecting to see data so hopefully this
gives you a sense of what tens of so can

00:10:34.900 --> 00:10:34.910
gives you a sense of what tens of so can
 

00:10:34.910 --> 00:10:37.180
gives you a sense of what tens of so can
do and the thing that makes it so

00:10:37.180 --> 00:10:37.190
do and the thing that makes it so
 

00:10:37.190 --> 00:10:39.310
do and the thing that makes it so
powerful is all you need to do is

00:10:39.310 --> 00:10:39.320
powerful is all you need to do is
 

00:10:39.320 --> 00:10:41.410
powerful is all you need to do is
specify how to compute the cost function

00:10:41.410 --> 00:10:41.420
specify how to compute the cost function
 

00:10:41.420 --> 00:10:44.530
specify how to compute the cost function
and then it takes derivatives and it can

00:10:44.530 --> 00:10:44.540
and then it takes derivatives and it can
 

00:10:44.540 --> 00:10:47.980
and then it takes derivatives and it can
apply a gradient optimizer or an atom

00:10:47.980 --> 00:10:47.990
apply a gradient optimizer or an atom
 

00:10:47.990 --> 00:10:50.050
apply a gradient optimizer or an atom
optimizer or some other optimizer with

00:10:50.050 --> 00:10:50.060
optimizer or some other optimizer with
 

00:10:50.060 --> 00:10:52.000
optimizer or some other optimizer with
just you know pretty much one or two

00:10:52.000 --> 00:10:52.010
just you know pretty much one or two
 

00:10:52.010 --> 00:10:55.630
just you know pretty much one or two
lines of code so here's the code again

00:10:55.630 --> 00:10:55.640
lines of code so here's the code again
 

00:10:55.640 --> 00:10:57.340
lines of code so here's the code again
I've cleaned this up just a little bit

00:10:57.340 --> 00:10:57.350
I've cleaned this up just a little bit
 

00:10:57.350 --> 00:10:59.740
I've cleaned this up just a little bit
and in case some of these functions or

00:10:59.740 --> 00:10:59.750
and in case some of these functions or
 

00:10:59.750 --> 00:11:01.120
and in case some of these functions or
variables seem a little bit mysterious

00:11:01.120 --> 00:11:01.130
variables seem a little bit mysterious
 

00:11:01.130 --> 00:11:03.400
variables seem a little bit mysterious
to you still they will become more

00:11:03.400 --> 00:11:03.410
to you still they will become more
 

00:11:03.410 --> 00:11:05.320
to you still they will become more
familiar after you practice with it a

00:11:05.320 --> 00:11:05.330
familiar after you practice with it a
 

00:11:05.330 --> 00:11:07.240
familiar after you practice with it a
couple times by working through their

00:11:07.240 --> 00:11:07.250
couple times by working through their
 

00:11:07.250 --> 00:11:10.480
couple times by working through their
programming exercise just one last thing

00:11:10.480 --> 00:11:10.490
programming exercise just one last thing
 

00:11:10.490 --> 00:11:12.640
programming exercise just one last thing
I want to mention these three lines of

00:11:12.640 --> 00:11:12.650
I want to mention these three lines of
 

00:11:12.650 --> 00:11:15.340
I want to mention these three lines of
code are quite idiomatic in terms of

00:11:15.340 --> 00:11:15.350
code are quite idiomatic in terms of
 

00:11:15.350 --> 00:11:18.520
code are quite idiomatic in terms of
flow and what some program is will do is

00:11:18.520 --> 00:11:18.530
flow and what some program is will do is
 

00:11:18.530 --> 00:11:21.010
flow and what some program is will do is
use this alternative format which

00:11:21.010 --> 00:11:21.020
use this alternative format which
 

00:11:21.020 --> 00:11:22.960
use this alternative format which
basically does the same thing set

00:11:22.960 --> 00:11:22.970
basically does the same thing set
 

00:11:22.970 --> 00:11:25.000
basically does the same thing set
session to TF dot session to start the

00:11:25.000 --> 00:11:25.010
session to TF dot session to start the
 

00:11:25.010 --> 00:11:28.150
session to TF dot session to start the
session and use the session to run in it

00:11:28.150 --> 00:11:28.160
session and use the session to run in it
 

00:11:28.160 --> 00:11:31.120
session and use the session to run in it
and then use the session to evaluate CW

00:11:31.120 --> 00:11:31.130
and then use the session to evaluate CW
 

00:11:31.130 --> 00:11:33.370
and then use the session to evaluate CW
and in print of result but this with

00:11:33.370 --> 00:11:33.380
and in print of result but this with
 

00:11:33.380 --> 00:11:36.490
and in print of result but this with
construction is used in a number of tens

00:11:36.490 --> 00:11:36.500
construction is used in a number of tens
 

00:11:36.500 --> 00:11:38.500
construction is used in a number of tens
of flow programs as well it more or less

00:11:38.500 --> 00:11:38.510
of flow programs as well it more or less
 

00:11:38.510 --> 00:11:40.810
of flow programs as well it more or less
means the same thing as the thing on the

00:11:40.810 --> 00:11:40.820
means the same thing as the thing on the
 

00:11:40.820 --> 00:11:45.070
means the same thing as the thing on the
left but the words command in Python is

00:11:45.070 --> 00:11:45.080
left but the words command in Python is
 

00:11:45.080 --> 00:11:47.740
left but the words command in Python is
a little bit better and cleaning up in

00:11:47.740 --> 00:11:47.750
a little bit better and cleaning up in
 

00:11:47.750 --> 00:11:50.320
a little bit better and cleaning up in
cases an error on exception what we're

00:11:50.320 --> 00:11:50.330
cases an error on exception what we're
 

00:11:50.330 --> 00:11:53.230
cases an error on exception what we're
accusing this in a loop so you see this

00:11:53.230 --> 00:11:53.240
accusing this in a loop so you see this
 

00:11:53.240 --> 00:11:55.990
accusing this in a loop so you see this
in there from an exercise as well so

00:11:55.990 --> 00:11:56.000
in there from an exercise as well so
 

00:11:56.000 --> 00:11:58.930
in there from an exercise as well so
what is this code really doing let's

00:11:58.930 --> 00:11:58.940
what is this code really doing let's
 

00:11:58.940 --> 00:12:02.830
what is this code really doing let's
focus on this equation the heart of a

00:12:02.830 --> 00:12:02.840
focus on this equation the heart of a
 

00:12:02.840 --> 00:12:05.890
focus on this equation the heart of a
tentacle program is something to compute

00:12:05.890 --> 00:12:05.900
tentacle program is something to compute
 

00:12:05.900 --> 00:12:08.620
tentacle program is something to compute
a cost and then ten to flow

00:12:08.620 --> 00:12:08.630
a cost and then ten to flow
 

00:12:08.630 --> 00:12:10.090
a cost and then ten to flow
automatically figures out the

00:12:10.090 --> 00:12:10.100
automatically figures out the
 

00:12:10.100 --> 00:12:11.950
automatically figures out the
derivatives and how to minimize that

00:12:11.950 --> 00:12:11.960
derivatives and how to minimize that
 

00:12:11.960 --> 00:12:16.270
derivatives and how to minimize that
cost so what this equation or what does

00:12:16.270 --> 00:12:16.280
cost so what this equation or what does
 

00:12:16.280 --> 00:12:19.360
cost so what this equation or what does
some line of code is doing is its

00:12:19.360 --> 00:12:19.370
some line of code is doing is its
 

00:12:19.370 --> 00:12:21.940
some line of code is doing is its
allowing tender flow to construct a

00:12:21.940 --> 00:12:21.950
allowing tender flow to construct a
 

00:12:21.950 --> 00:12:24.250
allowing tender flow to construct a
computation graph and a computation

00:12:24.250 --> 00:12:24.260
computation graph and a computation
 

00:12:24.260 --> 00:12:25.540
computation graph and a computation
drought does the following

00:12:25.540 --> 00:12:25.550
drought does the following
 

00:12:25.550 --> 00:12:30.130
drought does the following
it takes X 0 0 it takes W and then I

00:12:30.130 --> 00:12:30.140
it takes X 0 0 it takes W and then I
 

00:12:30.140 --> 00:12:36.130
it takes X 0 0 it takes W and then I
guess W gets squared and then X 0 0 gets

00:12:36.130 --> 00:12:36.140
guess W gets squared and then X 0 0 gets
 

00:12:36.140 --> 00:12:38.530
guess W gets squared and then X 0 0 gets
multiplied with W squared

00:12:38.530 --> 00:12:38.540
multiplied with W squared
 

00:12:38.540 --> 00:12:43.290
multiplied with W squared
you have X zero zero times W squared and

00:12:43.290 --> 00:12:43.300
you have X zero zero times W squared and
 

00:12:43.300 --> 00:12:46.750
you have X zero zero times W squared and
so on right and eventually you know this

00:12:46.750 --> 00:12:46.760
so on right and eventually you know this
 

00:12:46.760 --> 00:12:50.560
so on right and eventually you know this
gets built up to compute this xw x zero

00:12:50.560 --> 00:12:50.570
gets built up to compute this xw x zero
 

00:12:50.570 --> 00:12:53.230
gets built up to compute this xw x zero
zero times W squared plus x10 times W

00:12:53.230 --> 00:12:53.240
zero times W squared plus x10 times W
 

00:12:53.240 --> 00:12:56.260
zero times W squared plus x10 times W
plus and so on and so eventually you get

00:12:56.260 --> 00:12:56.270
plus and so on and so eventually you get
 

00:12:56.270 --> 00:12:58.810
plus and so on and so eventually you get
your the cost function right now against

00:12:58.810 --> 00:12:58.820
your the cost function right now against
 

00:12:58.820 --> 00:13:02.050
your the cost function right now against
the last term to be added would be a x2

00:13:02.050 --> 00:13:02.060
the last term to be added would be a x2
 

00:13:02.060 --> 00:13:05.620
the last term to be added would be a x2
0 where it gets added to be the cost I

00:13:05.620 --> 00:13:05.630
0 where it gets added to be the cost I
 

00:13:05.630 --> 00:13:06.730
0 where it gets added to be the cost I
won't write the other form under the

00:13:06.730 --> 00:13:06.740
won't write the other form under the
 

00:13:06.740 --> 00:13:10.900
won't write the other form under the
cost and and the nice thing about center

00:13:10.900 --> 00:13:10.910
cost and and the nice thing about center
 

00:13:10.910 --> 00:13:13.420
cost and and the nice thing about center
flow is that by implementing maybe

00:13:13.420 --> 00:13:13.430
flow is that by implementing maybe
 

00:13:13.430 --> 00:13:15.100
flow is that by implementing maybe
forward propagation through this

00:13:15.100 --> 00:13:15.110
forward propagation through this
 

00:13:15.110 --> 00:13:18.370
forward propagation through this
computation graph the computed cost tens

00:13:18.370 --> 00:13:18.380
computation graph the computed cost tens
 

00:13:18.380 --> 00:13:21.040
computation graph the computed cost tens
of flow has already back built in all

00:13:21.040 --> 00:13:21.050
of flow has already back built in all
 

00:13:21.050 --> 00:13:24.310
of flow has already back built in all
the necessary backward functions so

00:13:24.310 --> 00:13:24.320
the necessary backward functions so
 

00:13:24.320 --> 00:13:26.740
the necessary backward functions so
remember how training a thief new

00:13:26.740 --> 00:13:26.750
remember how training a thief new
 

00:13:26.750 --> 00:13:28.600
remember how training a thief new
network has a set of forward functions

00:13:28.600 --> 00:13:28.610
network has a set of forward functions
 

00:13:28.610 --> 00:13:30.750
network has a set of forward functions
instead of backward functions and

00:13:30.750 --> 00:13:30.760
instead of backward functions and
 

00:13:30.760 --> 00:13:32.950
instead of backward functions and
programming frameworks Blake tensorflow

00:13:32.950 --> 00:13:32.960
programming frameworks Blake tensorflow
 

00:13:32.960 --> 00:13:35.110
programming frameworks Blake tensorflow
have already built in the necessary

00:13:35.110 --> 00:13:35.120
have already built in the necessary
 

00:13:35.120 --> 00:13:38.380
have already built in the necessary
backward functions which is why by using

00:13:38.380 --> 00:13:38.390
backward functions which is why by using
 

00:13:38.390 --> 00:13:40.720
backward functions which is why by using
the built-in functions to compute the

00:13:40.720 --> 00:13:40.730
the built-in functions to compute the
 

00:13:40.730 --> 00:13:43.300
the built-in functions to compute the
forward function it can automatically do

00:13:43.300 --> 00:13:43.310
forward function it can automatically do
 

00:13:43.310 --> 00:13:45.430
forward function it can automatically do
the backward functions as well to

00:13:45.430 --> 00:13:45.440
the backward functions as well to
 

00:13:45.440 --> 00:13:47.890
the backward functions as well to
implement back propagation through even

00:13:47.890 --> 00:13:47.900
implement back propagation through even
 

00:13:47.900 --> 00:13:49.900
implement back propagation through even
very complicated functions and compute

00:13:49.900 --> 00:13:49.910
very complicated functions and compute
 

00:13:49.910 --> 00:13:52.120
very complicated functions and compute
derivatives for you so that's why you

00:13:52.120 --> 00:13:52.130
derivatives for you so that's why you
 

00:13:52.130 --> 00:13:54.130
derivatives for you so that's why you
don't need to explicitly implement back

00:13:54.130 --> 00:13:54.140
don't need to explicitly implement back
 

00:13:54.140 --> 00:13:56.260
don't need to explicitly implement back
prop and this is one of the things that

00:13:56.260 --> 00:13:56.270
prop and this is one of the things that
 

00:13:56.270 --> 00:13:57.910
prop and this is one of the things that
make the proving framework

00:13:57.910 --> 00:13:57.920
make the proving framework
 

00:13:57.920 --> 00:14:00.520
make the proving framework
help you become really efficient if you

00:14:00.520 --> 00:14:00.530
help you become really efficient if you
 

00:14:00.530 --> 00:14:03.280
help you become really efficient if you
look at the terms of so documentation

00:14:03.280 --> 00:14:03.290
look at the terms of so documentation
 

00:14:03.290 --> 00:14:04.840
look at the terms of so documentation
I just don't point out that the

00:14:04.840 --> 00:14:04.850
I just don't point out that the
 

00:14:04.850 --> 00:14:06.850
I just don't point out that the
tentacled documentation uses a slightly

00:14:06.850 --> 00:14:06.860
tentacled documentation uses a slightly
 

00:14:06.860 --> 00:14:09.220
tentacled documentation uses a slightly
different notation then I did for

00:14:09.220 --> 00:14:09.230
different notation then I did for
 

00:14:09.230 --> 00:14:12.130
different notation then I did for
drawing the computation graph that uses

00:14:12.130 --> 00:14:12.140
drawing the computation graph that uses
 

00:14:12.140 --> 00:14:15.550
drawing the computation graph that uses
X 0 0 W and then rather than writing the

00:14:15.550 --> 00:14:15.560
X 0 0 W and then rather than writing the
 

00:14:15.560 --> 00:14:18.220
X 0 0 W and then rather than writing the
value like W squared the tension-filled

00:14:18.220 --> 00:14:18.230
value like W squared the tension-filled
 

00:14:18.230 --> 00:14:20.200
value like W squared the tension-filled
documentation tends to just write the

00:14:20.200 --> 00:14:20.210
documentation tends to just write the
 

00:14:20.210 --> 00:14:22.630
documentation tends to just write the
operations so this would be a square

00:14:22.630 --> 00:14:22.640
operations so this would be a square
 

00:14:22.640 --> 00:14:25.540
operations so this would be a square
operation and these two get combined in

00:14:25.540 --> 00:14:25.550
operation and these two get combined in
 

00:14:25.550 --> 00:14:28.660
operation and these two get combined in
a multiplication operation and so on and

00:14:28.660 --> 00:14:28.670
a multiplication operation and so on and
 

00:14:28.670 --> 00:14:30.430
a multiplication operation and so on and
then the final note I get there'd be a

00:14:30.430 --> 00:14:30.440
then the final note I get there'd be a
 

00:14:30.440 --> 00:14:33.910
then the final note I get there'd be a
addition operation when you add X to 0

00:14:33.910 --> 00:14:33.920
addition operation when you add X to 0
 

00:14:33.920 --> 00:14:36.790
addition operation when you add X to 0
to find a final value so for the

00:14:36.790 --> 00:14:36.800
to find a final value so for the
 

00:14:36.800 --> 00:14:38.560
to find a final value so for the
purposes of this clause I thought that

00:14:38.560 --> 00:14:38.570
purposes of this clause I thought that
 

00:14:38.570 --> 00:14:41.020
purposes of this clause I thought that
this notation for the compensation drop

00:14:41.020 --> 00:14:41.030
this notation for the compensation drop
 

00:14:41.030 --> 00:14:42.940
this notation for the compensation drop
would be easier for you to understand

00:14:42.940 --> 00:14:42.950
would be easier for you to understand
 

00:14:42.950 --> 00:14:45.840
would be easier for you to understand
but if you look at the tensorflow

00:14:45.840 --> 00:14:45.850
but if you look at the tensorflow
 

00:14:45.850 --> 00:14:48.190
but if you look at the tensorflow
documentation as we look at the

00:14:48.190 --> 00:14:48.200
documentation as we look at the
 

00:14:48.200 --> 00:14:51.160
documentation as we look at the
computation graphs in the documentation

00:14:51.160 --> 00:14:51.170
computation graphs in the documentation
 

00:14:51.170 --> 00:14:53.860
computation graphs in the documentation
you see this alternative convention

00:14:53.860 --> 00:14:53.870
you see this alternative convention
 

00:14:53.870 --> 00:14:55.780
you see this alternative convention
where the nodes are labeled with the

00:14:55.780 --> 00:14:55.790
where the nodes are labeled with the
 

00:14:55.790 --> 00:14:58.569
where the nodes are labeled with the
operations rather than with the value

00:14:58.569 --> 00:14:58.579
operations rather than with the value
 

00:14:58.579 --> 00:15:01.389
operations rather than with the value
but both of these representations you

00:15:01.389 --> 00:15:01.399
but both of these representations you
 

00:15:01.399 --> 00:15:02.980
but both of these representations you
represent basically the same computation

00:15:02.980 --> 00:15:02.990
represent basically the same computation
 

00:15:02.990 --> 00:15:05.710
represent basically the same computation
graph and a lot of things they can do

00:15:05.710 --> 00:15:05.720
graph and a lot of things they can do
 

00:15:05.720 --> 00:15:07.300
graph and a lot of things they can do
with just one line of code in

00:15:07.300 --> 00:15:07.310
with just one line of code in
 

00:15:07.310 --> 00:15:09.220
with just one line of code in
programming frameworks for example if

00:15:09.220 --> 00:15:09.230
programming frameworks for example if
 

00:15:09.230 --> 00:15:10.420
programming frameworks for example if
you don't want to use gradient descent

00:15:10.420 --> 00:15:10.430
you don't want to use gradient descent
 

00:15:10.430 --> 00:15:13.210
you don't want to use gradient descent
but instead you want to use the atom

00:15:13.210 --> 00:15:13.220
but instead you want to use the atom
 

00:15:13.220 --> 00:15:15.699
but instead you want to use the atom
optimizer by changing this line of code

00:15:15.699 --> 00:15:15.709
optimizer by changing this line of code
 

00:15:15.709 --> 00:15:18.759
optimizer by changing this line of code
you can very quickly swap it swap in a

00:15:18.759 --> 00:15:18.769
you can very quickly swap it swap in a
 

00:15:18.769 --> 00:15:22.269
you can very quickly swap it swap in a
better optimization algorithm so all the

00:15:22.269 --> 00:15:22.279
better optimization algorithm so all the
 

00:15:22.279 --> 00:15:24.550
better optimization algorithm so all the
modern deep learning program frameworks

00:15:24.550 --> 00:15:24.560
modern deep learning program frameworks
 

00:15:24.560 --> 00:15:27.310
modern deep learning program frameworks
support things like this and makes it

00:15:27.310 --> 00:15:27.320
support things like this and makes it
 

00:15:27.320 --> 00:15:29.740
support things like this and makes it
really easy for you to code up even

00:15:29.740 --> 00:15:29.750
really easy for you to code up even
 

00:15:29.750 --> 00:15:32.620
really easy for you to code up even
pretty complex neural networks so I hope

00:15:32.620 --> 00:15:32.630
pretty complex neural networks so I hope
 

00:15:32.630 --> 00:15:34.480
pretty complex neural networks so I hope
this was helpful for giving you a sense

00:15:34.480 --> 00:15:34.490
this was helpful for giving you a sense
 

00:15:34.490 --> 00:15:36.730
this was helpful for giving you a sense
of the typical structure of a tensor

00:15:36.730 --> 00:15:36.740
of the typical structure of a tensor
 

00:15:36.740 --> 00:15:39.400
of the typical structure of a tensor
field program to recap the material from

00:15:39.400 --> 00:15:39.410
field program to recap the material from
 

00:15:39.410 --> 00:15:41.380
field program to recap the material from
this week you saw how to systematically

00:15:41.380 --> 00:15:41.390
this week you saw how to systematically
 

00:15:41.390 --> 00:15:43.720
this week you saw how to systematically
organize the hyper parameter search

00:15:43.720 --> 00:15:43.730
organize the hyper parameter search
 

00:15:43.730 --> 00:15:45.759
organize the hyper parameter search
process we also talked about batch

00:15:45.759 --> 00:15:45.769
process we also talked about batch
 

00:15:45.769 --> 00:15:47.500
process we also talked about batch
normalization and how you can use that

00:15:47.500 --> 00:15:47.510
normalization and how you can use that
 

00:15:47.510 --> 00:15:49.269
normalization and how you can use that
to speed up training of your networks

00:15:49.269 --> 00:15:49.279
to speed up training of your networks
 

00:15:49.279 --> 00:15:51.940
to speed up training of your networks
and finally we talked about programming

00:15:51.940 --> 00:15:51.950
and finally we talked about programming
 

00:15:51.950 --> 00:15:53.680
and finally we talked about programming
frameworks so deep learning there are

00:15:53.680 --> 00:15:53.690
frameworks so deep learning there are
 

00:15:53.690 --> 00:15:56.410
frameworks so deep learning there are
many great program frameworks and we had

00:15:56.410 --> 00:15:56.420
many great program frameworks and we had
 

00:15:56.420 --> 00:15:58.810
many great program frameworks and we had
this last video focusing on tens of so

00:15:58.810 --> 00:15:58.820
this last video focusing on tens of so
 

00:15:58.820 --> 00:16:01.930
this last video focusing on tens of so
with that I hope you enjoyed this week's

00:16:01.930 --> 00:16:01.940
with that I hope you enjoyed this week's
 

00:16:01.940 --> 00:16:04.329
with that I hope you enjoyed this week's
pro exercise and that helps you gain

00:16:04.329 --> 00:16:04.339
pro exercise and that helps you gain
 

00:16:04.339 --> 00:16:08.290
pro exercise and that helps you gain
even more familiarity with these ideas

