WEBVTT
Kind: captions
Language: en

00:00:00.319 --> 00:00:02.570
if you're building a computer vision

00:00:02.570 --> 00:00:02.580
if you're building a computer vision
 

00:00:02.580 --> 00:00:04.849
if you're building a computer vision
application rather than training the

00:00:04.849 --> 00:00:04.859
application rather than training the
 

00:00:04.859 --> 00:00:06.619
application rather than training the
waste from scratch from random

00:00:06.619 --> 00:00:06.629
waste from scratch from random
 

00:00:06.629 --> 00:00:08.660
waste from scratch from random
initialization you often make much

00:00:08.660 --> 00:00:08.670
initialization you often make much
 

00:00:08.670 --> 00:00:10.520
initialization you often make much
faster progress if you download weights

00:00:10.520 --> 00:00:10.530
faster progress if you download weights
 

00:00:10.530 --> 00:00:12.499
faster progress if you download weights
that someone else has already trained on

00:00:12.499 --> 00:00:12.509
that someone else has already trained on
 

00:00:12.509 --> 00:00:14.660
that someone else has already trained on
the network architecture and use that as

00:00:14.660 --> 00:00:14.670
the network architecture and use that as
 

00:00:14.670 --> 00:00:17.390
the network architecture and use that as
pre-training and transfer that to a new

00:00:17.390 --> 00:00:17.400
pre-training and transfer that to a new
 

00:00:17.400 --> 00:00:19.849
pre-training and transfer that to a new
task that you might be interested in do

00:00:19.849 --> 00:00:19.859
task that you might be interested in do
 

00:00:19.859 --> 00:00:21.650
task that you might be interested in do
you computer vision research community

00:00:21.650 --> 00:00:21.660
you computer vision research community
 

00:00:21.660 --> 00:00:24.230
you computer vision research community
has been pretty good at posting lots of

00:00:24.230 --> 00:00:24.240
has been pretty good at posting lots of
 

00:00:24.240 --> 00:00:26.029
has been pretty good at posting lots of
data sense on the internet so if you

00:00:26.029 --> 00:00:26.039
data sense on the internet so if you
 

00:00:26.039 --> 00:00:28.310
data sense on the internet so if you
hear of things like image net or and as

00:00:28.310 --> 00:00:28.320
hear of things like image net or and as
 

00:00:28.320 --> 00:00:30.800
hear of things like image net or and as
Coco or as gal all types of data says

00:00:30.800 --> 00:00:30.810
Coco or as gal all types of data says
 

00:00:30.810 --> 00:00:32.720
Coco or as gal all types of data says
these are the names of different data

00:00:32.720 --> 00:00:32.730
these are the names of different data
 

00:00:32.730 --> 00:00:34.670
these are the names of different data
sets that people have posted online and

00:00:34.670 --> 00:00:34.680
sets that people have posted online and
 

00:00:34.680 --> 00:00:36.200
sets that people have posted online and
that a lot of computer vision

00:00:36.200 --> 00:00:36.210
that a lot of computer vision
 

00:00:36.210 --> 00:00:38.209
that a lot of computer vision
researchers have trained their

00:00:38.209 --> 00:00:38.219
researchers have trained their
 

00:00:38.219 --> 00:00:40.729
researchers have trained their
algorithms on sometimes this training

00:00:40.729 --> 00:00:40.739
algorithms on sometimes this training
 

00:00:40.739 --> 00:00:43.639
algorithms on sometimes this training
takes several weeks and might take many

00:00:43.639 --> 00:00:43.649
takes several weeks and might take many
 

00:00:43.649 --> 00:00:46.520
takes several weeks and might take many
many GPUs and the fact that someone else

00:00:46.520 --> 00:00:46.530
many GPUs and the fact that someone else
 

00:00:46.530 --> 00:00:48.049
many GPUs and the fact that someone else
has done this and gone through the

00:00:48.049 --> 00:00:48.059
has done this and gone through the
 

00:00:48.059 --> 00:00:49.279
has done this and gone through the
painful high-performance research

00:00:49.279 --> 00:00:49.289
painful high-performance research
 

00:00:49.289 --> 00:00:50.990
painful high-performance research
process means that you can often

00:00:50.990 --> 00:00:51.000
process means that you can often
 

00:00:51.000 --> 00:00:53.779
process means that you can often
download open source weights that took

00:00:53.779 --> 00:00:53.789
download open source weights that took
 

00:00:53.789 --> 00:00:55.760
download open source weights that took
someone else many weeks or months to

00:00:55.760 --> 00:00:55.770
someone else many weeks or months to
 

00:00:55.770 --> 00:00:58.420
someone else many weeks or months to
figure out and use that it's a very good

00:00:58.420 --> 00:00:58.430
figure out and use that it's a very good
 

00:00:58.430 --> 00:01:00.590
figure out and use that it's a very good
initialization for your own neural

00:01:00.590 --> 00:01:00.600
initialization for your own neural
 

00:01:00.600 --> 00:01:03.349
initialization for your own neural
network and use transfer learning to

00:01:03.349 --> 00:01:03.359
network and use transfer learning to
 

00:01:03.359 --> 00:01:05.149
network and use transfer learning to
sort of transfer knowledge was on these

00:01:05.149 --> 00:01:05.159
sort of transfer knowledge was on these
 

00:01:05.159 --> 00:01:07.850
sort of transfer knowledge was on these
very large public datasets to your own

00:01:07.850 --> 00:01:07.860
very large public datasets to your own
 

00:01:07.860 --> 00:01:10.520
very large public datasets to your own
problem let's take a deeper look at how

00:01:10.520 --> 00:01:10.530
problem let's take a deeper look at how
 

00:01:10.530 --> 00:01:14.030
problem let's take a deeper look at how
to do this let's start an example let's

00:01:14.030 --> 00:01:14.040
to do this let's start an example let's
 

00:01:14.040 --> 00:01:16.280
to do this let's start an example let's
say you're building a CAD detector to

00:01:16.280 --> 00:01:16.290
say you're building a CAD detector to
 

00:01:16.290 --> 00:01:20.539
say you're building a CAD detector to
recognize your own pet cat so according

00:01:20.539 --> 00:01:20.549
recognize your own pet cat so according
 

00:01:20.549 --> 00:01:23.840
recognize your own pet cat so according
to the internet on Tigger is a common

00:01:23.840 --> 00:01:23.850
to the internet on Tigger is a common
 

00:01:23.850 --> 00:01:30.800
to the internet on Tigger is a common
cat name and Mistie is another common

00:01:30.800 --> 00:01:30.810
cat name and Mistie is another common
 

00:01:30.810 --> 00:01:35.569
cat name and Mistie is another common
cat's name and let's say your cats are

00:01:35.569 --> 00:01:35.579
cat's name and let's say your cats are
 

00:01:35.579 --> 00:01:35.929
cat's name and let's say your cats are
called

00:01:35.929 --> 00:01:35.939
called
 

00:01:35.939 --> 00:01:39.920
called
Tigger and Misty and there's also you

00:01:39.920 --> 00:01:39.930
Tigger and Misty and there's also you
 

00:01:39.930 --> 00:01:41.899
Tigger and Misty and there's also you
know neither so you have a

00:01:41.899 --> 00:01:41.909
know neither so you have a
 

00:01:41.909 --> 00:01:43.429
know neither so you have a
classification problem with three

00:01:43.429 --> 00:01:43.439
classification problem with three
 

00:01:43.439 --> 00:01:46.039
classification problem with three
classes as this picture Tigger

00:01:46.039 --> 00:01:46.049
classes as this picture Tigger
 

00:01:46.049 --> 00:01:49.550
classes as this picture Tigger
or is it misty or is it neither and what

00:01:49.550 --> 00:01:49.560
or is it misty or is it neither and what
 

00:01:49.560 --> 00:01:51.170
or is it misty or is it neither and what
you know the case of both of you has

00:01:51.170 --> 00:01:51.180
you know the case of both of you has
 

00:01:51.180 --> 00:01:54.830
you know the case of both of you has
appearing in a picture now you probably

00:01:54.830 --> 00:01:54.840
appearing in a picture now you probably
 

00:01:54.840 --> 00:01:56.719
appearing in a picture now you probably
don't have a lot of pictures of Tigger

00:01:56.719 --> 00:01:56.729
don't have a lot of pictures of Tigger
 

00:01:56.729 --> 00:01:59.810
don't have a lot of pictures of Tigger
or misty so your training set will be

00:01:59.810 --> 00:01:59.820
or misty so your training set will be
 

00:01:59.820 --> 00:02:02.990
or misty so your training set will be
small so what can you do I recommend you

00:02:02.990 --> 00:02:03.000
small so what can you do I recommend you
 

00:02:03.000 --> 00:02:05.719
small so what can you do I recommend you
go online and download some open source

00:02:05.719 --> 00:02:05.729
go online and download some open source
 

00:02:05.729 --> 00:02:08.109
go online and download some open source
implementation of a new network and

00:02:08.109 --> 00:02:08.119
implementation of a new network and
 

00:02:08.119 --> 00:02:11.540
implementation of a new network and
download not just the code but also the

00:02:11.540 --> 00:02:11.550
download not just the code but also the
 

00:02:11.550 --> 00:02:12.830
download not just the code but also the
weight

00:02:12.830 --> 00:02:12.840
weight
 

00:02:12.840 --> 00:02:15.920
weight
and there are a lot of networks and

00:02:15.920 --> 00:02:15.930
and there are a lot of networks and
 

00:02:15.930 --> 00:02:17.870
and there are a lot of networks and
there lot of networks can download that

00:02:17.870 --> 00:02:17.880
there lot of networks can download that
 

00:02:17.880 --> 00:02:19.790
there lot of networks can download that
have been trained on for example the

00:02:19.790 --> 00:02:19.800
have been trained on for example the
 

00:02:19.800 --> 00:02:23.449
have been trained on for example the
image net data set which has a thousand

00:02:23.449 --> 00:02:23.459
image net data set which has a thousand
 

00:02:23.459 --> 00:02:25.280
image net data set which has a thousand
different classes so the network might

00:02:25.280 --> 00:02:25.290
different classes so the network might
 

00:02:25.290 --> 00:02:28.940
different classes so the network might
have a soft max unit that outputs one of

00:02:28.940 --> 00:02:28.950
have a soft max unit that outputs one of
 

00:02:28.950 --> 00:02:31.699
have a soft max unit that outputs one of
a thousand possible classes what you can

00:02:31.699 --> 00:02:31.709
a thousand possible classes what you can
 

00:02:31.709 --> 00:02:34.490
a thousand possible classes what you can
do is then get rid of the soft max layer

00:02:34.490 --> 00:02:34.500
do is then get rid of the soft max layer
 

00:02:34.500 --> 00:02:40.100
do is then get rid of the soft max layer
and create your own soft max unit that

00:02:40.100 --> 00:02:40.110
and create your own soft max unit that
 

00:02:40.110 --> 00:02:41.630
and create your own soft max unit that
outputs tigger

00:02:41.630 --> 00:02:41.640
outputs tigger
 

00:02:41.640 --> 00:02:47.750
outputs tigger
or misty or neither and in terms of the

00:02:47.750 --> 00:02:47.760
or misty or neither and in terms of the
 

00:02:47.760 --> 00:02:50.090
or misty or neither and in terms of the
network i'd encourage you to think of

00:02:50.090 --> 00:02:50.100
network i'd encourage you to think of
 

00:02:50.100 --> 00:02:53.780
network i'd encourage you to think of
all of these layers as frozen so you

00:02:53.780 --> 00:02:53.790
all of these layers as frozen so you
 

00:02:53.790 --> 00:02:57.350
all of these layers as frozen so you
freeze the parameters in all of these

00:02:57.350 --> 00:02:57.360
freeze the parameters in all of these
 

00:02:57.360 --> 00:02:59.960
freeze the parameters in all of these
layers of the network and you would then

00:02:59.960 --> 00:02:59.970
layers of the network and you would then
 

00:02:59.970 --> 00:03:02.930
layers of the network and you would then
just train the parameters associated

00:03:02.930 --> 00:03:02.940
just train the parameters associated
 

00:03:02.940 --> 00:03:06.500
just train the parameters associated
with your soft bands layer which is a

00:03:06.500 --> 00:03:06.510
with your soft bands layer which is a
 

00:03:06.510 --> 00:03:08.030
with your soft bands layer which is a
soft matte layer with three possible

00:03:08.030 --> 00:03:08.040
soft matte layer with three possible
 

00:03:08.040 --> 00:03:11.059
soft matte layer with three possible
outputs you know take a misty or neither

00:03:11.059 --> 00:03:11.069
outputs you know take a misty or neither
 

00:03:11.069 --> 00:03:15.979
outputs you know take a misty or neither
and by using someone else's free train

00:03:15.979 --> 00:03:15.989
and by using someone else's free train
 

00:03:15.989 --> 00:03:17.900
and by using someone else's free train
weights you might prefer get pretty good

00:03:17.900 --> 00:03:17.910
weights you might prefer get pretty good
 

00:03:17.910 --> 00:03:20.810
weights you might prefer get pretty good
performance on this even with a small

00:03:20.810 --> 00:03:20.820
performance on this even with a small
 

00:03:20.820 --> 00:03:24.050
performance on this even with a small
data set fortunately a lot of deep

00:03:24.050 --> 00:03:24.060
data set fortunately a lot of deep
 

00:03:24.060 --> 00:03:26.240
data set fortunately a lot of deep
learning frameworks support this mode of

00:03:26.240 --> 00:03:26.250
learning frameworks support this mode of
 

00:03:26.250 --> 00:03:29.120
learning frameworks support this mode of
operation and in fact depending on the

00:03:29.120 --> 00:03:29.130
operation and in fact depending on the
 

00:03:29.130 --> 00:03:31.449
operation and in fact depending on the
framework it might have things like

00:03:31.449 --> 00:03:31.459
framework it might have things like
 

00:03:31.459 --> 00:03:35.210
framework it might have things like
trainable parameter equals zero you

00:03:35.210 --> 00:03:35.220
trainable parameter equals zero you
 

00:03:35.220 --> 00:03:36.979
trainable parameter equals zero you
might set that for some of these earlier

00:03:36.979 --> 00:03:36.989
might set that for some of these earlier
 

00:03:36.989 --> 00:03:39.440
might set that for some of these earlier
layers in order to just say you know

00:03:39.440 --> 00:03:39.450
layers in order to just say you know
 

00:03:39.450 --> 00:03:41.960
layers in order to just say you know
don't train those weights or sometimes

00:03:41.960 --> 00:03:41.970
don't train those weights or sometimes
 

00:03:41.970 --> 00:03:44.870
don't train those weights or sometimes
you have a parameter like freeze equals

00:03:44.870 --> 00:03:44.880
you have a parameter like freeze equals
 

00:03:44.880 --> 00:03:48.080
you have a parameter like freeze equals
one and these are different ways and

00:03:48.080 --> 00:03:48.090
one and these are different ways and
 

00:03:48.090 --> 00:03:49.670
one and these are different ways and
different deep learning premium

00:03:49.670 --> 00:03:49.680
different deep learning premium
 

00:03:49.680 --> 00:03:51.920
different deep learning premium
frameworks that let you specify whether

00:03:51.920 --> 00:03:51.930
frameworks that let you specify whether
 

00:03:51.930 --> 00:03:53.990
frameworks that let you specify whether
or not to train the weights associated

00:03:53.990 --> 00:03:54.000
or not to train the weights associated
 

00:03:54.000 --> 00:03:56.960
or not to train the weights associated
or a particular layer and so in this

00:03:56.960 --> 00:03:56.970
or a particular layer and so in this
 

00:03:56.970 --> 00:03:59.420
or a particular layer and so in this
case you will train only the softmax

00:03:59.420 --> 00:03:59.430
case you will train only the softmax
 

00:03:59.430 --> 00:04:02.000
case you will train only the softmax
layers ways but freeze all of the

00:04:02.000 --> 00:04:02.010
layers ways but freeze all of the
 

00:04:02.010 --> 00:04:05.900
layers ways but freeze all of the
earlier layers weights one other neat

00:04:05.900 --> 00:04:05.910
earlier layers weights one other neat
 

00:04:05.910 --> 00:04:07.819
earlier layers weights one other neat
trick that may help for some

00:04:07.819 --> 00:04:07.829
trick that may help for some
 

00:04:07.829 --> 00:04:10.490
trick that may help for some
implementations is that because all of

00:04:10.490 --> 00:04:10.500
implementations is that because all of
 

00:04:10.500 --> 00:04:13.129
implementations is that because all of
these early layers are frozen there is

00:04:13.129 --> 00:04:13.139
these early layers are frozen there is
 

00:04:13.139 --> 00:04:14.960
these early layers are frozen there is
some fixed function that doesn't change

00:04:14.960 --> 00:04:14.970
some fixed function that doesn't change
 

00:04:14.970 --> 00:04:16.580
some fixed function that doesn't change
because you're not changing it you're

00:04:16.580 --> 00:04:16.590
because you're not changing it you're
 

00:04:16.590 --> 00:04:18.409
because you're not changing it you're
not training it that takes this input

00:04:18.409 --> 00:04:18.419
not training it that takes this input
 

00:04:18.419 --> 00:04:21.680
not training it that takes this input
image X and maps it to some set of

00:04:21.680 --> 00:04:21.690
image X and maps it to some set of
 

00:04:21.690 --> 00:04:23.180
image X and maps it to some set of
activations

00:04:23.180 --> 00:04:23.190
activations
 

00:04:23.190 --> 00:04:25.610
activations
than there so one of the treatment that

00:04:25.610 --> 00:04:25.620
than there so one of the treatment that
 

00:04:25.620 --> 00:04:27.530
than there so one of the treatment that
could speed up training is you just

00:04:27.530 --> 00:04:27.540
could speed up training is you just
 

00:04:27.540 --> 00:04:31.690
could speed up training is you just
precompute that layer the features

00:04:31.690 --> 00:04:31.700
precompute that layer the features
 

00:04:31.700 --> 00:04:32.840
precompute that layer the features
reactivations

00:04:32.840 --> 00:04:32.850
reactivations
 

00:04:32.850 --> 00:04:35.390
reactivations
from that layer and just saved them to

00:04:35.390 --> 00:04:35.400
from that layer and just saved them to
 

00:04:35.400 --> 00:04:38.390
from that layer and just saved them to
disk and what you're doing is it using

00:04:38.390 --> 00:04:38.400
disk and what you're doing is it using
 

00:04:38.400 --> 00:04:41.840
disk and what you're doing is it using
this fixed function in this first part

00:04:41.840 --> 00:04:41.850
this fixed function in this first part
 

00:04:41.850 --> 00:04:43.790
this fixed function in this first part
of the neural network to take as input

00:04:43.790 --> 00:04:43.800
of the neural network to take as input
 

00:04:43.800 --> 00:04:47.600
of the neural network to take as input
any input any image X and compute some

00:04:47.600 --> 00:04:47.610
any input any image X and compute some
 

00:04:47.610 --> 00:04:49.700
any input any image X and compute some
feature vector for it and then your

00:04:49.700 --> 00:04:49.710
feature vector for it and then your
 

00:04:49.710 --> 00:04:52.940
feature vector for it and then your
training a shallow softmax model from

00:04:52.940 --> 00:04:52.950
training a shallow softmax model from
 

00:04:52.950 --> 00:04:55.970
training a shallow softmax model from
this feature vector to make a prediction

00:04:55.970 --> 00:04:55.980
this feature vector to make a prediction
 

00:04:55.980 --> 00:04:59.150
this feature vector to make a prediction
and so one step that could help your

00:04:59.150 --> 00:04:59.160
and so one step that could help your
 

00:04:59.160 --> 00:05:02.180
and so one step that could help your
computation is you just pre compute that

00:05:02.180 --> 00:05:02.190
computation is you just pre compute that
 

00:05:02.190 --> 00:05:04.610
computation is you just pre compute that
layers activation for all the examples

00:05:04.610 --> 00:05:04.620
layers activation for all the examples
 

00:05:04.620 --> 00:05:06.560
layers activation for all the examples
in training set and save them to disk

00:05:06.560 --> 00:05:06.570
in training set and save them to disk
 

00:05:06.570 --> 00:05:08.390
in training set and save them to disk
and then just train a soft mass

00:05:08.390 --> 00:05:08.400
and then just train a soft mass
 

00:05:08.400 --> 00:05:10.610
and then just train a soft mass
classifier on top of that all right so

00:05:10.610 --> 00:05:10.620
classifier on top of that all right so
 

00:05:10.620 --> 00:05:12.860
classifier on top of that all right so
the advantage of safety disks or the pre

00:05:12.860 --> 00:05:12.870
the advantage of safety disks or the pre
 

00:05:12.870 --> 00:05:14.660
the advantage of safety disks or the pre
compute method to save to disk method is

00:05:14.660 --> 00:05:14.670
compute method to save to disk method is
 

00:05:14.670 --> 00:05:16.970
compute method to save to disk method is
that you don't need to recompute those

00:05:16.970 --> 00:05:16.980
that you don't need to recompute those
 

00:05:16.980 --> 00:05:20.030
that you don't need to recompute those
activations every time you take a leap

00:05:20.030 --> 00:05:20.040
activations every time you take a leap
 

00:05:20.040 --> 00:05:21.920
activations every time you take a leap
off or take a pass through your training

00:05:21.920 --> 00:05:21.930
off or take a pass through your training
 

00:05:21.930 --> 00:05:25.130
off or take a pass through your training
set so this is what you do if you have a

00:05:25.130 --> 00:05:25.140
set so this is what you do if you have a
 

00:05:25.140 --> 00:05:28.100
set so this is what you do if you have a
pretty small training set for your task

00:05:28.100 --> 00:05:28.110
pretty small training set for your task
 

00:05:28.110 --> 00:05:32.150
pretty small training set for your task
whatever larger training set so one rule

00:05:32.150 --> 00:05:32.160
whatever larger training set so one rule
 

00:05:32.160 --> 00:05:34.700
whatever larger training set so one rule
of thumb is if you have a larger label

00:05:34.700 --> 00:05:34.710
of thumb is if you have a larger label
 

00:05:34.710 --> 00:05:36.500
of thumb is if you have a larger label
data set so maybe you just have a ton of

00:05:36.500 --> 00:05:36.510
data set so maybe you just have a ton of
 

00:05:36.510 --> 00:05:40.190
data set so maybe you just have a ton of
pictures of Tigger misty as was against

00:05:40.190 --> 00:05:40.200
pictures of Tigger misty as was against
 

00:05:40.200 --> 00:05:42.350
pictures of Tigger misty as was against
pictures of neither of them one thing

00:05:42.350 --> 00:05:42.360
pictures of neither of them one thing
 

00:05:42.360 --> 00:05:45.440
pictures of neither of them one thing
you could do is then freeze fewer layers

00:05:45.440 --> 00:05:45.450
you could do is then freeze fewer layers
 

00:05:45.450 --> 00:05:48.710
you could do is then freeze fewer layers
so maybe your freeze just these layers

00:05:48.710 --> 00:05:48.720
so maybe your freeze just these layers
 

00:05:48.720 --> 00:05:52.570
so maybe your freeze just these layers
and then train these later layers

00:05:52.570 --> 00:05:52.580
and then train these later layers
 

00:05:52.580 --> 00:05:55.040
and then train these later layers
although if the output layer has

00:05:55.040 --> 00:05:55.050
although if the output layer has
 

00:05:55.050 --> 00:05:57.170
although if the output layer has
different causes then you need to you

00:05:57.170 --> 00:05:57.180
different causes then you need to you
 

00:05:57.180 --> 00:05:59.360
different causes then you need to you
know have your own output unit

00:05:59.360 --> 00:05:59.370
know have your own output unit
 

00:05:59.370 --> 00:06:05.810
know have your own output unit
anyway take a misty or neither and there

00:06:05.810 --> 00:06:05.820
anyway take a misty or neither and there
 

00:06:05.820 --> 00:06:08.120
anyway take a misty or neither and there
are a couple ways to do this you could

00:06:08.120 --> 00:06:08.130
are a couple ways to do this you could
 

00:06:08.130 --> 00:06:12.110
are a couple ways to do this you could
take the last few layers and there a

00:06:12.110 --> 00:06:12.120
take the last few layers and there a
 

00:06:12.120 --> 00:06:14.030
take the last few layers and there a
couple ways to do this you could take

00:06:14.030 --> 00:06:14.040
couple ways to do this you could take
 

00:06:14.040 --> 00:06:15.740
couple ways to do this you could take
the loss view as weights and just use

00:06:15.740 --> 00:06:15.750
the loss view as weights and just use
 

00:06:15.750 --> 00:06:17.270
the loss view as weights and just use
that as initialization and do great

00:06:17.270 --> 00:06:17.280
that as initialization and do great
 

00:06:17.280 --> 00:06:20.030
that as initialization and do great
descent from there or you can also blow

00:06:20.030 --> 00:06:20.040
descent from there or you can also blow
 

00:06:20.040 --> 00:06:22.640
descent from there or you can also blow
away these lost few layers and just you

00:06:22.640 --> 00:06:22.650
away these lost few layers and just you
 

00:06:22.650 --> 00:06:25.670
away these lost few layers and just you
know use your own new hidden units and

00:06:25.670 --> 00:06:25.680
know use your own new hidden units and
 

00:06:25.680 --> 00:06:28.570
know use your own new hidden units and
in your own final softmax output so

00:06:28.570 --> 00:06:28.580
in your own final softmax output so
 

00:06:28.580 --> 00:06:30.980
in your own final softmax output so
either of these methods could be worth

00:06:30.980 --> 00:06:30.990
either of these methods could be worth
 

00:06:30.990 --> 00:06:31.840
either of these methods could be worth
trying

00:06:31.840 --> 00:06:31.850
trying
 

00:06:31.850 --> 00:06:34.300
trying
but maybe one pattern is if you have

00:06:34.300 --> 00:06:34.310
but maybe one pattern is if you have
 

00:06:34.310 --> 00:06:36.370
but maybe one pattern is if you have
more data the number of layers you

00:06:36.370 --> 00:06:36.380
more data the number of layers you
 

00:06:36.380 --> 00:06:39.220
more data the number of layers you
freeze could be smaller and then the

00:06:39.220 --> 00:06:39.230
freeze could be smaller and then the
 

00:06:39.230 --> 00:06:41.980
freeze could be smaller and then the
number of layers you train on top could

00:06:41.980 --> 00:06:41.990
number of layers you train on top could
 

00:06:41.990 --> 00:06:45.070
number of layers you train on top could
be greater and the idea is that if

00:06:45.070 --> 00:06:45.080
be greater and the idea is that if
 

00:06:45.080 --> 00:06:46.690
be greater and the idea is that if
you're bigger data set then maybe of

00:06:46.690 --> 00:06:46.700
you're bigger data set then maybe of
 

00:06:46.700 --> 00:06:49.030
you're bigger data set then maybe of
enough data not just to Train a single

00:06:49.030 --> 00:06:49.040
enough data not just to Train a single
 

00:06:49.040 --> 00:06:51.850
enough data not just to Train a single
softmax unit but to Train some you know

00:06:51.850 --> 00:06:51.860
softmax unit but to Train some you know
 

00:06:51.860 --> 00:06:54.640
softmax unit but to Train some you know
mother size new network that comprises

00:06:54.640 --> 00:06:54.650
mother size new network that comprises
 

00:06:54.650 --> 00:06:58.240
mother size new network that comprises
the last few layers of this final

00:06:58.240 --> 00:06:58.250
the last few layers of this final
 

00:06:58.250 --> 00:07:01.750
the last few layers of this final
network the end up using and then

00:07:01.750 --> 00:07:01.760
network the end up using and then
 

00:07:01.760 --> 00:07:04.390
network the end up using and then
finally if you have a lot of data one

00:07:04.390 --> 00:07:04.400
finally if you have a lot of data one
 

00:07:04.400 --> 00:07:06.790
finally if you have a lot of data one
thing you might do is take this open

00:07:06.790 --> 00:07:06.800
thing you might do is take this open
 

00:07:06.800 --> 00:07:09.850
thing you might do is take this open
source network and wait and use the

00:07:09.850 --> 00:07:09.860
source network and wait and use the
 

00:07:09.860 --> 00:07:13.180
source network and wait and use the
whole thing just as initialization and

00:07:13.180 --> 00:07:13.190
whole thing just as initialization and
 

00:07:13.190 --> 00:07:15.970
whole thing just as initialization and
train the whole network although again

00:07:15.970 --> 00:07:15.980
train the whole network although again
 

00:07:15.980 --> 00:07:19.750
train the whole network although again
if this was a thousand node softmax and

00:07:19.750 --> 00:07:19.760
if this was a thousand node softmax and
 

00:07:19.760 --> 00:07:21.310
if this was a thousand node softmax and
you have just three outputs you need

00:07:21.310 --> 00:07:21.320
you have just three outputs you need
 

00:07:21.320 --> 00:07:24.280
you have just three outputs you need
your own softmax output to output the

00:07:24.280 --> 00:07:24.290
your own softmax output to output the
 

00:07:24.290 --> 00:07:28.660
your own softmax output to output the
labels you care about but the more label

00:07:28.660 --> 00:07:28.670
labels you care about but the more label
 

00:07:28.670 --> 00:07:30.880
labels you care about but the more label
data you have for your tasks so the more

00:07:30.880 --> 00:07:30.890
data you have for your tasks so the more
 

00:07:30.890 --> 00:07:32.770
data you have for your tasks so the more
pictures you have of take a misty and

00:07:32.770 --> 00:07:32.780
pictures you have of take a misty and
 

00:07:32.780 --> 00:07:35.410
pictures you have of take a misty and
neither the more layers you could train

00:07:35.410 --> 00:07:35.420
neither the more layers you could train
 

00:07:35.420 --> 00:07:38.680
neither the more layers you could train
and in the extreme case you could use

00:07:38.680 --> 00:07:38.690
and in the extreme case you could use
 

00:07:38.690 --> 00:07:40.210
and in the extreme case you could use
the way to download just as

00:07:40.210 --> 00:07:40.220
the way to download just as
 

00:07:40.220 --> 00:07:42.130
the way to download just as
initialization so they would replace

00:07:42.130 --> 00:07:42.140
initialization so they would replace
 

00:07:42.140 --> 00:07:44.890
initialization so they would replace
random initialization and you could do

00:07:44.890 --> 00:07:44.900
random initialization and you could do
 

00:07:44.900 --> 00:07:47.650
random initialization and you could do
gradient descent training updating all

00:07:47.650 --> 00:07:47.660
gradient descent training updating all
 

00:07:47.660 --> 00:07:49.060
gradient descent training updating all
the ways in all the layers of the

00:07:49.060 --> 00:07:49.070
the ways in all the layers of the
 

00:07:49.070 --> 00:07:52.840
the ways in all the layers of the
network so that's transfer learning for

00:07:52.840 --> 00:07:52.850
network so that's transfer learning for
 

00:07:52.850 --> 00:07:55.450
network so that's transfer learning for
the training of confidence in practice

00:07:55.450 --> 00:07:55.460
the training of confidence in practice
 

00:07:55.460 --> 00:07:57.730
the training of confidence in practice
because the open datasets on the

00:07:57.730 --> 00:07:57.740
because the open datasets on the
 

00:07:57.740 --> 00:08:00.250
because the open datasets on the
Internet are so big and the ways you can

00:08:00.250 --> 00:08:00.260
Internet are so big and the ways you can
 

00:08:00.260 --> 00:08:02.230
Internet are so big and the ways you can
download that someone else has spent

00:08:02.230 --> 00:08:02.240
download that someone else has spent
 

00:08:02.240 --> 00:08:04.750
download that someone else has spent
weeks training has learned from so much

00:08:04.750 --> 00:08:04.760
weeks training has learned from so much
 

00:08:04.760 --> 00:08:07.360
weeks training has learned from so much
data you find that for a lot of computer

00:08:07.360 --> 00:08:07.370
data you find that for a lot of computer
 

00:08:07.370 --> 00:08:09.520
data you find that for a lot of computer
vision applications you just do much

00:08:09.520 --> 00:08:09.530
vision applications you just do much
 

00:08:09.530 --> 00:08:11.680
vision applications you just do much
better if you download someone else's

00:08:11.680 --> 00:08:11.690
better if you download someone else's
 

00:08:11.690 --> 00:08:13.600
better if you download someone else's
open source ways and use that as

00:08:13.600 --> 00:08:13.610
open source ways and use that as
 

00:08:13.610 --> 00:08:16.660
open source ways and use that as
initialization for your problem so in

00:08:16.660 --> 00:08:16.670
initialization for your problem so in
 

00:08:16.670 --> 00:08:18.610
initialization for your problem so in
all the different disciplines and all

00:08:18.610 --> 00:08:18.620
all the different disciplines and all
 

00:08:18.620 --> 00:08:21.040
all the different disciplines and all
the different applications of deep

00:08:21.040 --> 00:08:21.050
the different applications of deep
 

00:08:21.050 --> 00:08:23.380
the different applications of deep
learning I think that computer vision is

00:08:23.380 --> 00:08:23.390
learning I think that computer vision is
 

00:08:23.390 --> 00:08:25.450
learning I think that computer vision is
one where transfer learning is something

00:08:25.450 --> 00:08:25.460
one where transfer learning is something
 

00:08:25.460 --> 00:08:28.540
one where transfer learning is something
that you should almost always do unless

00:08:28.540 --> 00:08:28.550
that you should almost always do unless
 

00:08:28.550 --> 00:08:30.460
that you should almost always do unless
you actually have a very very large

00:08:30.460 --> 00:08:30.470
you actually have a very very large
 

00:08:30.470 --> 00:08:32.230
you actually have a very very large
unless you have an exceptionally large

00:08:32.230 --> 00:08:32.240
unless you have an exceptionally large
 

00:08:32.240 --> 00:08:34.540
unless you have an exceptionally large
dataset they train everything else from

00:08:34.540 --> 00:08:34.550
dataset they train everything else from
 

00:08:34.550 --> 00:08:37.029
dataset they train everything else from
scratch himself but transfer learning is

00:08:37.029 --> 00:08:37.039
scratch himself but transfer learning is
 

00:08:37.039 --> 00:08:40.020
scratch himself but transfer learning is
just very worth seriously considering on

00:08:40.020 --> 00:08:40.030
just very worth seriously considering on
 

00:08:40.030 --> 00:08:41.910
just very worth seriously considering on
you have an exceptionally large dataset

00:08:41.910 --> 00:08:41.920
you have an exceptionally large dataset
 

00:08:41.920 --> 00:08:44.070
you have an exceptionally large dataset
and a very large computational budget

00:08:44.070 --> 00:08:44.080
and a very large computational budget
 

00:08:44.080 --> 00:08:46.620
and a very large computational budget
they trade everything from scratch by

00:08:46.620 --> 00:08:46.630
they trade everything from scratch by
 

00:08:46.630 --> 00:08:49.050
they trade everything from scratch by
yourself

