WEBVTT
Kind: captions
Language: en

00:00:03.000 --> 00:00:05.280
ALONSO VERA: Thank you.

00:00:05.280 --> 00:00:06.380
OK so to start.

00:00:09.070 --> 00:00:12.370
When Terry asked
me to present this,

00:00:12.370 --> 00:00:14.120
he asked me almost
right away for a title.

00:00:14.120 --> 00:00:16.399
So I made one up in
about two seconds.

00:00:16.399 --> 00:00:18.190
And then later, I
realized I didn't like it

00:00:18.190 --> 00:00:21.330
because as I was actually
preparing slides for this,

00:00:21.330 --> 00:00:23.250
I realized that it's
a false dichotomy.

00:00:23.250 --> 00:00:25.560
I mean, replacing humans
or working with them--

00:00:25.560 --> 00:00:27.560
because sometimes you do
want to replace humans.

00:00:27.560 --> 00:00:29.393
So I realize that's not
really the question.

00:00:29.393 --> 00:00:36.050
So I made a second slide
which I like better.

00:00:36.050 --> 00:00:36.550
OK.

00:00:36.550 --> 00:00:39.150
So really, the talk is
about human-autonomy

00:00:39.150 --> 00:00:42.140
teaming for complex,
non-deterministic activities.

00:00:42.140 --> 00:00:46.560
So, you know, there
will be many instances

00:00:46.560 --> 00:00:51.555
where autonomy will likely
work independent of humans.

00:00:51.555 --> 00:00:54.870
And really this whole
talk is about how do we

00:00:54.870 --> 00:01:00.650
know and begin to understand the
context in which you actually

00:01:00.650 --> 00:01:03.110
want to keep humans
as part of a system?

00:01:03.110 --> 00:01:04.690
And then how do you do that?

00:01:04.690 --> 00:01:07.974
And I'm going to talk
a little bit about some

00:01:07.974 --> 00:01:09.140
of the things we do at NASA.

00:01:09.140 --> 00:01:14.920
But in some ways it's
largely a set of thoughts

00:01:14.920 --> 00:01:18.950
that I put together from work
that NASA is doing broadly

00:01:18.950 --> 00:01:21.250
in aeronautics and space.

00:01:21.250 --> 00:01:23.200
And especially on
the aeronautics side,

00:01:23.200 --> 00:01:26.030
NASA's gone through a fairly
significant transformation

00:01:26.030 --> 00:01:27.940
to focus more on autonomy.

00:01:27.940 --> 00:01:29.740
And this has become
a big debate.

00:01:29.740 --> 00:01:32.130
So do you look at
autonomy in the way

00:01:32.130 --> 00:01:35.330
that we had for really
a couple of decades.

00:01:35.330 --> 00:01:38.590
I don't know if you're familiar
with-- you probably aren't.

00:01:38.590 --> 00:01:40.470
The Department of
Defense for a long time

00:01:40.470 --> 00:01:42.920
thought of this as
levels of autonomy.

00:01:42.920 --> 00:01:44.740
And it was a
professor at MIT, Tom

00:01:44.740 --> 00:01:48.160
Sheridan, who came up with this
concept of levels of autonomy.

00:01:48.160 --> 00:01:50.660
And the idea was
that our technology

00:01:50.660 --> 00:01:52.250
would develop from
one where you've

00:01:52.250 --> 00:01:54.710
got humans doing
everything to now

00:01:54.710 --> 00:01:56.680
human is assisted a
little bit or informed

00:01:56.680 --> 00:01:58.210
a little bit by autonomy.

00:01:58.210 --> 00:02:00.630
An increasing role of autonomy
until you have autonomy

00:02:00.630 --> 00:02:02.342
doing everything, right?

00:02:02.342 --> 00:02:03.291
And no human anymore.

00:02:03.291 --> 00:02:05.040
And that was this idea
levels of autonomy.

00:02:05.040 --> 00:02:07.880
And you were trying to
understand areas of application

00:02:07.880 --> 00:02:10.380
in terms of where are
we, what level are we at,

00:02:10.380 --> 00:02:12.770
and where were we going to?

00:02:12.770 --> 00:02:21.839
And Department of Defense did
a study about 10 years ago--

00:02:21.839 --> 00:02:23.380
or funded a study
about 10 years ago,

00:02:23.380 --> 00:02:26.730
which said that's probably
not the way things are going.

00:02:26.730 --> 00:02:29.640
And so the new idea
is cognitive echelons.

00:02:29.640 --> 00:02:32.412
And cognitive echelons-- and
I'll talk about this more.

00:02:32.412 --> 00:02:33.870
And in fact, I
think the whole talk

00:02:33.870 --> 00:02:37.920
is sort of about this-- is the
idea that really what you're

00:02:37.920 --> 00:02:41.380
looking at is for any
given type of activity,

00:02:41.380 --> 00:02:44.830
what is the cognitive
demand of that activity?

00:02:44.830 --> 00:02:48.350
And what type of cognitive
demand does it have?

00:02:48.350 --> 00:02:51.980
And that tells you for that type
of activity, how much autonomy

00:02:51.980 --> 00:02:54.220
and how much human you need.

00:02:54.220 --> 00:02:55.990
And with the idea
that some of that

00:02:55.990 --> 00:02:58.710
is going to stay going
forward, that balance

00:02:58.710 --> 00:03:00.960
is going to stay about the
same going forward, even

00:03:00.960 --> 00:03:02.580
as autonomy increases, OK?

00:03:02.580 --> 00:03:04.580
And so I see it as my
goal to try and convince

00:03:04.580 --> 00:03:06.840
you a little bit of that idea.

00:03:06.840 --> 00:03:11.920
The core idea being that
although what we're seeing

00:03:11.920 --> 00:03:15.070
is that robotic and human
interfaces are progressing

00:03:15.070 --> 00:03:17.570
quickly, AI not so much.

00:03:17.570 --> 00:03:19.350
So that's one theme

00:03:19.350 --> 00:03:22.390
But even if that weren't true.

00:03:22.390 --> 00:03:26.096
So let's say AI was
progressing a lot faster.

00:03:26.096 --> 00:03:28.490
The second point--
or maybe probably

00:03:28.490 --> 00:03:33.120
the core point of the talk
is that human intelligence

00:03:33.120 --> 00:03:35.490
and the way that humans
understand things and process

00:03:35.490 --> 00:03:38.260
things is, at the
moment, fundamentally

00:03:38.260 --> 00:03:40.510
different than the way we're
doing it computationally.

00:03:40.510 --> 00:03:42.343
And that shouldn't be
a surprise to anybody,

00:03:42.343 --> 00:03:44.080
but I'll go through
that a little bit.

00:03:44.080 --> 00:03:48.850
And unless things take a big
turn sometime in the future,

00:03:48.850 --> 00:03:54.440
where either our machine
intelligence becomes

00:03:54.440 --> 00:03:57.730
effectively omniscient
and can think in every way

00:03:57.730 --> 00:03:59.730
or we really start
figuring out how

00:03:59.730 --> 00:04:03.480
to simulate human intelligence,
human intelligence by itself

00:04:03.480 --> 00:04:06.142
will remain unique and have
certain unique values that

00:04:06.142 --> 00:04:08.600
are probably going to be true
for the next 50 to 100 years,

00:04:08.600 --> 00:04:09.600
possibly longer.

00:04:09.600 --> 00:04:11.183
So that's the argument
I want to make.

00:04:13.410 --> 00:04:15.750
And then, finally, the
practical outcome of this

00:04:15.750 --> 00:04:19.470
is that if you don't
design intelligent systems,

00:04:19.470 --> 00:04:23.545
increasingly autonomous
systems, to interact with humans

00:04:23.545 --> 00:04:25.920
because your humans are going
to stay part of the system,

00:04:25.920 --> 00:04:28.880
because some of the intelligence
is going to stay in your human.

00:04:28.880 --> 00:04:30.620
If you don't design
those systems

00:04:30.620 --> 00:04:35.390
to interact with humans, your
costs go up rather than down.

00:04:35.390 --> 00:04:39.510
So let me start with this.

00:04:39.510 --> 00:04:42.980
Basically I think everybody
here is well familiar with this.

00:04:42.980 --> 00:04:47.730
But you look the last
four years in the news.

00:04:47.730 --> 00:04:51.770
Like almost weekly, there's
something about AI in the news.

00:04:51.770 --> 00:04:56.410
And so from things
like-- just jumping

00:04:56.410 --> 00:04:58.508
to the second big bullet
here, Stephen Hawking

00:04:58.508 --> 00:05:01.000
and Bill Gates and
Elon Musk all warning

00:05:01.000 --> 00:05:02.990
about the dangers of AI.

00:05:02.990 --> 00:05:07.775
So you talk to the public
and they say, oh, what's

00:05:07.775 --> 00:05:08.900
really happening out there?

00:05:08.900 --> 00:05:09.650
Is this dangerous?

00:05:09.650 --> 00:05:13.180
And so part of my argument
is, probably not really.

00:05:13.180 --> 00:05:15.290
So some of my view
is that that's not

00:05:15.290 --> 00:05:16.240
really quite correct.

00:05:16.240 --> 00:05:18.610
Although there are
certain dangers, but--

00:05:18.610 --> 00:05:21.580
Now, what we're seeing
really at its core

00:05:21.580 --> 00:05:25.550
is four things that are driving
machine intelligence forward.

00:05:25.550 --> 00:05:27.042
So, big data.

00:05:27.042 --> 00:05:31.630
And big data's been
a fundamental change

00:05:31.630 --> 00:05:34.210
in terms of getting
behavior that

00:05:34.210 --> 00:05:37.040
seems more intelligent to us.

00:05:37.040 --> 00:05:40.930
Deep learning, which is
big data plus neural nets.

00:05:40.930 --> 00:05:43.050
Networked operations and
cyber-physical systems--

00:05:43.050 --> 00:05:45.220
that begins to
really change-- it

00:05:45.220 --> 00:05:47.470
moves intelligence out
to the periphery in a way

00:05:47.470 --> 00:05:52.270
that that decentralization
creates a lot more adaptivity.

00:05:52.270 --> 00:05:57.410
And then Moore's law, which
continues reasonably unabated.

00:05:57.410 --> 00:05:59.060
And so you take
those in combination.

00:05:59.060 --> 00:06:01.980
And really, what we've seen
over the last 5 to 10 years

00:06:01.980 --> 00:06:04.790
is a significant surge in
our machine intelligence

00:06:04.790 --> 00:06:07.090
capabilities.

00:06:07.090 --> 00:06:09.241
And it's introduced to
us in our daily life.

00:06:09.241 --> 00:06:11.646
So you search for
images in Google.

00:06:11.646 --> 00:06:14.513
And it'll say, you want more
images that look like this.

00:06:14.513 --> 00:06:16.310
And it kinda comes up
with a lot of images

00:06:16.310 --> 00:06:19.101
that look like-- in a
sensible way to us humans--

00:06:19.101 --> 00:06:21.065
it'll look like the
ones you've searched.

00:06:21.065 --> 00:06:23.300
I could go through
a lot of examples

00:06:23.300 --> 00:06:29.947
So also self-driving
cars, robotic operations,

00:06:29.947 --> 00:06:35.238
like in the factories like
Amazon or Tesla-Toyota.

00:06:35.238 --> 00:06:37.650
Then Big Blue, Watson,
Poker Bot-- those

00:06:37.650 --> 00:06:39.120
have all been fairly recent.

00:06:39.120 --> 00:06:44.050
Just earlier this year, Google
DeepMind beats a human at Go.

00:06:44.050 --> 00:06:47.680
And February, there
was a piece in the news

00:06:47.680 --> 00:06:50.580
about the first artificial
intelligence investment

00:06:50.580 --> 00:06:53.000
software in Wallstreet.

00:06:53.000 --> 00:06:56.875
I haven't heard if it's
done well or poorly.

00:06:56.875 --> 00:06:59.000
Actually, I should have
done that piece of research

00:06:59.000 --> 00:07:00.250
before coming, to
see whether it made

00:07:00.250 --> 00:07:01.458
any money on its investments.

00:07:01.458 --> 00:07:04.030
Because I know my
16-year-old daughter

00:07:04.030 --> 00:07:05.310
is in the investment club.

00:07:05.310 --> 00:07:09.270
And that by itself does
not mean you're doing well.

00:07:09.270 --> 00:07:13.020
So let me give you
an example, here.

00:07:13.020 --> 00:07:19.022
And this example is really sort
of at the heart of, I think,

00:07:19.022 --> 00:07:20.100
what I'm going to argue.

00:07:27.700 --> 00:07:29.592
When the Navy needs
a new ship or just

00:07:29.592 --> 00:07:31.300
like when the Air
Force needs a new ship,

00:07:31.300 --> 00:07:32.050
there's a whole process.

00:07:32.050 --> 00:07:33.216
And you take it to Congress.

00:07:33.216 --> 00:07:34.820
And you say, we need
a new ship to do.

00:07:34.820 --> 00:07:36.420
X and its requirements based.

00:07:36.420 --> 00:07:38.470
So they took this
to Congress and said

00:07:38.470 --> 00:07:41.240
we need to reduce manpower.

00:07:41.240 --> 00:07:45.280
So we need a ship that can be
sailed by a lot fewer sailors.

00:07:45.280 --> 00:07:48.375
And so this is
many years of work.

00:07:48.375 --> 00:07:50.250
It goes and finally gets
approved by Congress

00:07:50.250 --> 00:07:50.964
and goes through.

00:07:50.964 --> 00:07:52.380
And the goal was
to build a ship--

00:07:52.380 --> 00:07:54.713
and this is called the littoral
combat ship-- that would

00:07:54.713 --> 00:07:57.930
be operated by 45 sailors.

00:07:57.930 --> 00:07:59.840
And in the end,
what happened was

00:07:59.840 --> 00:08:02.840
this was delivered around 2012.

00:08:02.840 --> 00:08:07.480
And it wasn't operable
by 45 sailors.

00:08:07.480 --> 00:08:09.960
It was operable by 60 sailors.

00:08:09.960 --> 00:08:14.740
And that- if you are in
the field of human systems

00:08:14.740 --> 00:08:17.380
integration-- that's not so bad.

00:08:17.380 --> 00:08:19.600
To be off by 25% is
actually a pretty good.

00:08:19.600 --> 00:08:22.985
Often these things are off
by an order of magnitude.

00:08:22.985 --> 00:08:24.360
But that wasn't
the real problem.

00:08:24.360 --> 00:08:27.780
So the real problem is
that on any given ship--

00:08:27.780 --> 00:08:31.870
let's say you have 400
people-- most of the sailors

00:08:31.870 --> 00:08:35.700
are actually doing
low skilled tasks.

00:08:35.700 --> 00:08:41.130
So sailors are-- it's
a pyramid, right?

00:08:41.130 --> 00:08:43.500
They go from E1 to E10.

00:08:43.500 --> 00:08:46.520
And you get to the
E5s-- E5s are very rare.

00:08:46.520 --> 00:08:51.020
Any given ship has just a
handful of E5s and above.

00:08:51.020 --> 00:08:53.200
So the bottom of the
pyramid, the E1s, the E2s,

00:08:53.200 --> 00:08:56.270
there's are the bulk of
the people on a boat.

00:08:56.270 --> 00:08:58.770
Turns out this thing,
the littoral combat ship,

00:08:58.770 --> 00:09:03.860
was saleable by 60
E5s and above, only.

00:09:06.390 --> 00:09:08.540
So it caused all
sorts of problems.

00:09:08.540 --> 00:09:10.630
And the reason it
required E5s was it

00:09:10.630 --> 00:09:15.350
was really hard to
manage the systems.

00:09:15.350 --> 00:09:17.600
You needed to have a lot of
training, a lot of skills.

00:09:17.600 --> 00:09:19.630
You needed to be fairly
intelligent to be

00:09:19.630 --> 00:09:20.750
able to do it.

00:09:20.750 --> 00:09:23.660
And so this was a
bit of a disaster.

00:09:23.660 --> 00:09:26.670
Turns out that on top of it,
the conditions on the ship

00:09:26.670 --> 00:09:28.934
were really miserable,
because it was built for 45

00:09:28.934 --> 00:09:29.850
and you're putting 60.

00:09:29.850 --> 00:09:31.730
So they actually
literally put 45 beds,

00:09:31.730 --> 00:09:33.227
the kitchen was for 45 people.

00:09:33.227 --> 00:09:35.310
Everything was for 45 and
you're putting 60 on it.

00:09:35.310 --> 00:09:39.280
So they've had a lot of
their upper grade sailors

00:09:39.280 --> 00:09:43.072
leave after one tour in the
Navy because this is very bad.

00:09:43.072 --> 00:09:45.530
Anyway, there was this guy,
Naval, postgraduate school here

00:09:45.530 --> 00:09:47.410
in Monterey who came
and told us this story.

00:09:47.410 --> 00:09:50.013
And I spent some time with him
just trying to understand--

00:09:50.013 --> 00:09:51.387
and I won't go
into too much more

00:09:51.387 --> 00:09:52.660
detail about what went wrong.

00:09:52.660 --> 00:09:56.860
But the most interesting part
of this is that if you ask,

00:09:56.860 --> 00:10:00.605
did the companies that
delivered the littoral combat

00:10:00.605 --> 00:10:03.270
ship fail to do what they
promised they would do?

00:10:03.270 --> 00:10:04.350
The answer was, no.

00:10:04.350 --> 00:10:07.500
They delivered a
largely autonomous ship.

00:10:07.500 --> 00:10:10.560
It's just that they
didn't worry about how

00:10:10.560 --> 00:10:13.870
the 45 sailors, those
remaining roles,

00:10:13.870 --> 00:10:15.970
would interact
with that autonomy.

00:10:15.970 --> 00:10:18.400
And so the cost went up.

00:10:18.400 --> 00:10:21.160
And you needed 60
people instead of 45.

00:10:21.160 --> 00:10:22.940
But on top of it,
like the real cost

00:10:22.940 --> 00:10:25.766
was, again, the fact that all
the sailors had to be E5s.

00:10:25.766 --> 00:10:27.140
And there's a
little white paper,

00:10:27.140 --> 00:10:29.121
and you can look this up.

00:10:29.121 --> 00:10:29.620
It's great.

00:10:29.620 --> 00:10:32.565
It's like a three or four page
white paper by Blackhurst.

00:10:32.565 --> 00:10:35.946
It's Jack Blackhurst, Sharon
Gresham, and Morley Stone.

00:10:35.946 --> 00:10:38.468
They're at Air Force
Research Labs in Dayton.

00:10:38.468 --> 00:10:41.990
And It's called "The
Autonomy Paradox."

00:10:41.990 --> 00:10:43.680
And they basically
say, look, you

00:10:43.680 --> 00:10:46.150
take the sum lessons of
the Department of Defense--

00:10:46.150 --> 00:10:51.330
so very broad-- and every
time you invest in autonomy,

00:10:51.330 --> 00:10:53.100
your costs increase.

00:10:53.100 --> 00:10:56.124
And it's not because the economy
didn't do what it promised,

00:10:56.124 --> 00:10:58.290
it is because you didn't
understand how it was going

00:10:58.290 --> 00:10:59.370
to change the human roles.

00:10:59.370 --> 00:11:01.369
And in the end, you had
to put more humans on it

00:11:01.369 --> 00:11:03.927
than was originally planned.

00:11:03.927 --> 00:11:05.260
So your costs actually increase.

00:11:05.260 --> 00:11:09.896
And so for example, at NASA,
we're very aware of this,

00:11:09.896 --> 00:11:16.390
So it takes more people to fly
an unmanned aerial system--

00:11:16.390 --> 00:11:21.124
like say a drone-- than
it does to pilot a plane.

00:11:21.124 --> 00:11:24.100
It's like nine people to fly a
drone versus a couple of people

00:11:24.100 --> 00:11:24.980
to fly a plane.

00:11:24.980 --> 00:11:26.496
So this is the case today.

00:11:26.496 --> 00:11:28.370
Now you know, we might
say this might change.

00:11:28.370 --> 00:11:29.619
But this is the understanding.

00:11:29.619 --> 00:11:33.190
So there's one lesson
from this, at least,

00:11:33.190 --> 00:11:34.690
that I want to start
out with, which

00:11:34.690 --> 00:11:43.210
is that as machine intelligence
advances in a system,

00:11:43.210 --> 00:11:46.180
you need to worry more about the
interfaces with humans rather

00:11:46.180 --> 00:11:47.410
than less.

00:11:47.410 --> 00:11:49.540
So the simpler
system, the less you

00:11:49.540 --> 00:11:51.110
have to worry about
the interfaces.

00:11:51.110 --> 00:11:53.380
So systems that do fairly
straightforward things,

00:11:53.380 --> 00:11:56.570
like your microwave, it
doesn't have to be that hard.

00:11:56.570 --> 00:11:59.714
Things that have increasingly
complex relationships and tasks

00:11:59.714 --> 00:12:01.380
that we do together,
like other humans--

00:12:01.380 --> 00:12:03.712
We humans have very
complex systems

00:12:03.712 --> 00:12:04.920
to interact with one another.

00:12:08.300 --> 00:12:12.000
Anyway so that's going
to be a recurring theme.

00:12:12.000 --> 00:12:18.450
I wanna give one other-- given
that this is a NASA talk,

00:12:18.450 --> 00:12:20.380
this is from our own
experience at NASA.

00:12:20.380 --> 00:12:26.080
So you can't see
the arrow very well,

00:12:26.080 --> 00:12:30.450
but there is an arrow here that
shows a temporal progression.

00:12:30.450 --> 00:12:34.270
So in 2003, we started working
on the Mars Exploration Rovers

00:12:34.270 --> 00:12:35.360
Mission.

00:12:35.360 --> 00:12:37.890
And it was a small
human-computer interaction

00:12:37.890 --> 00:12:38.390
group.

00:12:38.390 --> 00:12:40.955
And we were asked to contribute
to a number of tools that

00:12:40.955 --> 00:12:43.080
were being developed to
the Mars Exploration Rovers

00:12:43.080 --> 00:12:47.980
Mission, which was being
run out of JPL down in LA.

00:12:47.980 --> 00:12:49.350
And it was great.

00:12:49.350 --> 00:12:50.480
It was a great experience.

00:12:50.480 --> 00:12:51.930
A lot of us has
just come to NASA.

00:12:51.930 --> 00:12:52.930
It was very interesting.

00:12:52.930 --> 00:12:55.450
The whole mission
was run on Mars time.

00:12:55.450 --> 00:13:00.900
So they arranged apartments
for us that were blacked out.

00:13:00.900 --> 00:13:03.870
Food trucks would
show up on Mars time.

00:13:03.870 --> 00:13:06.970
So it was 40 minutes off a day.

00:13:06.970 --> 00:13:09.380
So every day, it'd
be another-- so you'd

00:13:09.380 --> 00:13:12.480
get to work at
9:40, then at 10:20

00:13:12.480 --> 00:13:14.080
then at 11 o'clock every day.

00:13:14.080 --> 00:13:17.350
It was very strange to do
it for a number of weeks.

00:13:17.350 --> 00:13:20.370
Because it's worse
than being on ships,

00:13:20.370 --> 00:13:23.070
because you can't even
develop a regular schedule.

00:13:23.070 --> 00:13:24.920
But that's what we were doing.

00:13:24.920 --> 00:13:28.260
I mean there were things like we
were issued Mars time watches.

00:13:28.260 --> 00:13:30.140
And at one point,
I was in a store.

00:13:30.140 --> 00:13:32.250
And I was wearing
my Mars time watch

00:13:32.250 --> 00:13:34.850
on one and my regular
watch on the other.

00:13:34.850 --> 00:13:36.980
And someone said, why are
you wearing two watches?

00:13:36.980 --> 00:13:39.230
And I said, I'm working
on a Mars mission.

00:13:39.230 --> 00:13:42.540
And, she was like, security
to the front desk, please.

00:13:42.540 --> 00:13:44.040
So there was a lot
of just oddness

00:13:44.040 --> 00:13:45.789
of trying to have your
daily life while we

00:13:45.789 --> 00:13:47.400
were working on that mission.

00:13:47.400 --> 00:13:49.930
But one of the
things that happened

00:13:49.930 --> 00:13:53.070
was that we got brought in
to work on a planning system.

00:13:53.070 --> 00:13:56.150
So it turned out that
that mission was really

00:13:56.150 --> 00:13:58.140
the first mission
where NASA decided

00:13:58.140 --> 00:14:02.100
to do real time planning,
or near real time planning.

00:14:02.100 --> 00:14:03.896
So in general,
what NASA does is,

00:14:03.896 --> 00:14:05.770
if you're going to send
a mission to Mercury,

00:14:05.770 --> 00:14:07.410
you plan out that mission.

00:14:07.410 --> 00:14:09.949
Because you pretty much
understand the orbital dynamics

00:14:09.949 --> 00:14:10.490
and all that.

00:14:10.490 --> 00:14:12.990
So you're going to send it,
it's going to go around Mercury.

00:14:12.990 --> 00:14:15.640
So you can plan that
mission down to the minute.

00:14:15.640 --> 00:14:17.730
And that's what we do.

00:14:17.730 --> 00:14:19.480
And then we're going
to take this picture,

00:14:19.480 --> 00:14:20.438
we're going to do that.

00:14:20.438 --> 00:14:23.070
And that's planned to a
high level of resolution

00:14:23.070 --> 00:14:26.440
a year before the
mission even launches.

00:14:26.440 --> 00:14:28.180
This mission was
the first one where

00:14:28.180 --> 00:14:31.200
the hope was that if both
rovers landed safely,

00:14:31.200 --> 00:14:33.750
that you would actually be
getting science data back

00:14:33.750 --> 00:14:34.530
every day.

00:14:34.530 --> 00:14:36.780
So you had these science
teams, about 100 scientists--

00:14:36.780 --> 00:14:41.120
geologists, atmospheric
scientists, dust scientists.

00:14:41.120 --> 00:14:43.770
And they would get together
and look at all the data

00:14:43.770 --> 00:14:45.270
that was coming
back and every day

00:14:45.270 --> 00:14:47.740
generate a new set of
activities to be radiated up

00:14:47.740 --> 00:14:49.390
to those rovers.

00:14:49.390 --> 00:14:52.640
And the number of activities
that were sent up to the rover

00:14:52.640 --> 00:14:54.490
was about 500 to 600 a day.

00:14:54.490 --> 00:14:56.540
And those were highly
inter-constrained.

00:14:56.540 --> 00:14:58.080
So they were all
in interdependent.

00:14:58.080 --> 00:14:59.650
Take this picture
before this one.

00:14:59.650 --> 00:15:01.230
Take this one as close
to noon as possible.

00:15:01.230 --> 00:15:03.563
Don't take this one if you
haven't taken the other ones.

00:15:03.563 --> 00:15:07.540
So there were about
say 500, 600 things

00:15:07.540 --> 00:15:10.240
to do sending up to the
rover and close to a thousand

00:15:10.240 --> 00:15:11.641
constraints every day.

00:15:11.641 --> 00:15:12.640
And that was a big plan.

00:15:12.640 --> 00:15:14.020
And then a little
engineering team

00:15:14.020 --> 00:15:15.240
had to come up with
a plan like that

00:15:15.240 --> 00:15:17.531
that wasn't going to break
the rover, that wasn't going

00:15:17.531 --> 00:15:20.380
to breach like all the
power rules for the day, all

00:15:20.380 --> 00:15:24.072
these things-- and do it in a
matter of a couple of hours.

00:15:24.072 --> 00:15:25.280
And so that hadn't been done.

00:15:25.280 --> 00:15:28.530
So the first thing
that Ames did was

00:15:28.530 --> 00:15:31.090
suggest, look, what we need
here is an intelligent planning

00:15:31.090 --> 00:15:32.210
system.

00:15:32.210 --> 00:15:36.180
And so they took the existing
Jet Propulsion Lab planning

00:15:36.180 --> 00:15:41.140
system-- which was called
APGEN, Activity Plan Generator--

00:15:41.140 --> 00:15:46.430
and put a backend that basically
took all the inputs, all

00:15:46.430 --> 00:15:49.390
the activities, all the
constraints, and optimized it

00:15:49.390 --> 00:15:52.910
and did some intelligent
things in terms of some cases

00:15:52.910 --> 00:15:54.690
with some flexibility
of the constraints,

00:15:54.690 --> 00:15:57.770
and then gave you a plan.

00:15:57.770 --> 00:16:01.540
And at that time, it was
bit of a break through.

00:16:01.540 --> 00:16:04.730
It was a real mission
using machine intelligence.

00:16:04.730 --> 00:16:06.610
Within a very short
amount of time,

00:16:06.610 --> 00:16:08.540
the engineers at
Jet Propulsion Lab

00:16:08.540 --> 00:16:10.360
said, no, this is
completely unacceptable.

00:16:10.360 --> 00:16:12.620
They called the
system "the blender."

00:16:12.620 --> 00:16:16.420
Because it would put your
activities here and there.

00:16:16.420 --> 00:16:18.754
And then it would optimize it.

00:16:18.754 --> 00:16:20.170
And you had no way
to validate it.

00:16:20.170 --> 00:16:23.150
You had no way to find
where it had put things.

00:16:23.150 --> 00:16:28.724
And so they came back to
us and they said, OK, we

00:16:28.724 --> 00:16:31.140
need to, for example, I need
to be able to pin activities.

00:16:31.140 --> 00:16:34.210
And so that's when they came to
the Human-Computer Interaction

00:16:34.210 --> 00:16:34.710
Group.

00:16:34.710 --> 00:16:37.290
And they said, could
you design an interface

00:16:37.290 --> 00:16:38.563
so that we can pin activities?

00:16:38.563 --> 00:16:41.990
And we said, yeah--
little drop down menu,

00:16:41.990 --> 00:16:44.620
you select something,
you pin it.

00:16:44.620 --> 00:16:49.300
And then I remember asking one
of the people on the Ames side

00:16:49.300 --> 00:16:52.430
who was responsible
and I said, look,

00:16:52.430 --> 00:16:54.595
do you want it so
that I, as the user,

00:16:54.595 --> 00:16:57.036
can move things around
after I've pinned them?

00:16:57.036 --> 00:16:58.370
And the person said, well, no.

00:16:58.370 --> 00:16:59.472
You just pinned it.

00:16:59.472 --> 00:17:01.430
So why would you be able
to move things around?

00:17:01.430 --> 00:17:03.638
And I said, well, I didn't
pin it so I can't move it.

00:17:03.638 --> 00:17:05.740
I pinned so the stupid
system can't move it.

00:17:05.740 --> 00:17:06.930
That's their point, right?

00:17:06.930 --> 00:17:10.099
So you had all these, who's
in charge, kind of questions.

00:17:10.099 --> 00:17:17.470
Anyway, long story short-- or
long story longer, really-- we

00:17:17.470 --> 00:17:20.450
did some adaptation to that
one, developed a couple

00:17:20.450 --> 00:17:22.140
of other tools around planning.

00:17:22.140 --> 00:17:26.389
That ended up getting expanded
into the Phoenix Science

00:17:26.389 --> 00:17:29.720
Interface, which was the 2007
Polar Lander on Mars, which

00:17:29.720 --> 00:17:32.060
was another successful mission.

00:17:32.060 --> 00:17:34.830
We were asked, what if we
tried to make the engineers do

00:17:34.830 --> 00:17:37.420
the plan-- sorry, the scientists
do the plan rather than

00:17:37.420 --> 00:17:38.370
the engineers?

00:17:38.370 --> 00:17:40.690
Before that, only the
engineers, the Rover engineers,

00:17:40.690 --> 00:17:41.660
did the planning.

00:17:41.660 --> 00:17:43.511
We said, well, maybe
you can abstract away

00:17:43.511 --> 00:17:44.510
some of the Rover stuff.

00:17:44.510 --> 00:17:46.310
And that turned out to work.

00:17:46.310 --> 00:17:48.880
And that was all Human-Computer
Interaction work.

00:17:48.880 --> 00:17:52.890
As we went through from
the Mars Exploration Rovers

00:17:52.890 --> 00:17:58.940
to the Phoenix Lander to
Mars Science Laboratory,

00:17:58.940 --> 00:18:03.550
which is the big Rover
that's on Mars now, and then

00:18:03.550 --> 00:18:06.760
Space Station, LADEE,
a different mission.

00:18:06.760 --> 00:18:08.970
And now, we're doing
these analog missions.

00:18:08.970 --> 00:18:10.720
NEEMO is the
underwater lab that's

00:18:10.720 --> 00:18:13.380
off of Key Largo where
astronauts go and [INAUDIBLE].

00:18:13.380 --> 00:18:17.920
So this is like the-- we've
moved to doing it on pads

00:18:17.920 --> 00:18:20.540
so that-- And we're looking
now at crew self scheduling.

00:18:20.540 --> 00:18:23.410
And that's our big thing, now.

00:18:23.410 --> 00:18:27.360
So the tool is now being
used on Space Station

00:18:27.360 --> 00:18:33.914
since last summer for astronauts
to schedule their own time.

00:18:33.914 --> 00:18:35.580
The point of this is
that the transition

00:18:35.580 --> 00:18:39.000
was from an early design where
we said an intelligent system

00:18:39.000 --> 00:18:41.500
that just does planning for
you-- which was basically

00:18:41.500 --> 00:18:43.870
rejected by the
mission-- to eventually

00:18:43.870 --> 00:18:48.900
an evolution to over a decade
of a lot of work to making it

00:18:48.900 --> 00:18:52.730
so that the users-- and
increasingly non-expert users--

00:18:52.730 --> 00:18:54.450
can interact with
the complex planning

00:18:54.450 --> 00:18:59.030
systems and complex plans
to really enable the human

00:18:59.030 --> 00:19:01.990
to do a much more complex task
very quickly than they would

00:19:01.990 --> 00:19:04.390
have before, but without,
in a sense, kind of doing it

00:19:04.390 --> 00:19:07.690
for them in a way that humans
tend to find frustrating.

00:19:07.690 --> 00:19:10.590
So this has been one
of our big experiences

00:19:10.590 --> 00:19:12.840
over the last
decade in this area.

00:19:12.840 --> 00:19:20.160
And this could be a talk
by itself, but it's not.

00:19:20.160 --> 00:19:24.120
So that's the
argument, that we're

00:19:24.120 --> 00:19:30.020
going in a direction
of supporting humans.

00:19:30.020 --> 00:19:32.230
And I want to make
a distinction here.

00:19:32.230 --> 00:19:35.490
We're making progress, I
think, in supporting humans

00:19:35.490 --> 00:19:37.690
interacting with
systems, but more so

00:19:37.690 --> 00:19:39.010
in certain areas than others.

00:19:39.010 --> 00:19:42.090
So here, you look at robots.

00:19:42.090 --> 00:19:46.500
So this is a picture
from the Tesla facility.

00:19:46.500 --> 00:19:48.410
And these are,
really, caged robots.

00:19:48.410 --> 00:19:51.050
So last year at the
Honda facility--

00:19:51.050 --> 00:19:53.360
at a Honda facility
in Japan, a worker

00:19:53.360 --> 00:19:56.755
was killed by one of
the robots, got crushed.

00:19:56.755 --> 00:19:58.380
It wasn't the robot's
fault. That robot

00:19:58.380 --> 00:20:01.400
had no sensors built into it
to detect a human in its space.

00:20:01.400 --> 00:20:06.830
I mean a human went
in, got crushed.

00:20:06.830 --> 00:20:11.227
So they're caged because they're
blind to the world around them.

00:20:11.227 --> 00:20:12.310
They're designed that way.

00:20:12.310 --> 00:20:15.050
And they're designed that way
because it costs more money,

00:20:15.050 --> 00:20:17.300
it's more complex to make
them so that they interact.

00:20:17.300 --> 00:20:19.000
And then you have like
these domesticated robots,

00:20:19.000 --> 00:20:19.666
like the Roomba.

00:20:19.666 --> 00:20:23.030
And this is from El
Camino hospital, here.

00:20:23.030 --> 00:20:24.530
If you've been there
recently, there

00:20:24.530 --> 00:20:26.360
are these little
boxes that go around

00:20:26.360 --> 00:20:29.784
and they're aware of you,
they'll follow you-- well

00:20:29.784 --> 00:20:30.700
they won't follow you.

00:20:30.700 --> 00:20:33.760
But if you're walking and
it walking there behind you,

00:20:33.760 --> 00:20:35.070
they won't run you over.

00:20:35.070 --> 00:20:36.750
They'll wait.

00:20:36.750 --> 00:20:38.260
They're a little
slow and clumsy.

00:20:38.260 --> 00:20:39.770
But they're are
little robots that

00:20:39.770 --> 00:20:41.020
are going around the hospital.

00:20:43.280 --> 00:20:46.212
And the point here is
that we have caged robots,

00:20:46.212 --> 00:20:47.920
and we have domesticated
robots when they

00:20:47.920 --> 00:20:50.499
have to interact with humans.

00:20:50.499 --> 00:20:52.040
And I'll talk about
this in a second.

00:20:52.040 --> 00:20:56.710
But for autonomy, in some ways--
I think one of my key points

00:20:56.710 --> 00:20:59.600
is that we're still really
talking about caged autonomy.

00:20:59.600 --> 00:21:01.840
We tend to think of
autonomy in the sense

00:21:01.840 --> 00:21:05.530
of replacing humans rather
than working with them,

00:21:05.530 --> 00:21:09.087
assisting them, and really
working on those interfaces.

00:21:09.087 --> 00:21:10.920
And that's how we end
up with these problems

00:21:10.920 --> 00:21:14.844
like the littoral combat ship.

00:21:14.844 --> 00:21:18.090
This is a robot that you're
probably familiar with.

00:21:18.090 --> 00:21:20.260
There's a version of
it on Space Station.

00:21:20.260 --> 00:21:22.160
It's called Robonaut.

00:21:22.160 --> 00:21:24.800
And what's interesting
about it is

00:21:24.800 --> 00:21:27.220
that it is designed to
physically interact with humans

00:21:27.220 --> 00:21:28.890
in a very constrained
environment.

00:21:28.890 --> 00:21:32.400
So if it's moving its
arm and you bump into it,

00:21:32.400 --> 00:21:33.723
it will withdraw its arm.

00:21:33.723 --> 00:21:37.800
So it's very
sensitive to-- So it

00:21:37.800 --> 00:21:40.410
is the opposite of a machine
that would crush somebody

00:21:40.410 --> 00:21:41.600
because it has no sensors.

00:21:41.600 --> 00:21:43.010
The thing has
sensors all over it

00:21:43.010 --> 00:21:45.355
to make sure that if
somebody nudges it,

00:21:45.355 --> 00:21:47.490
it pulls back-- all that stuff.

00:21:47.490 --> 00:21:50.615
But again, the
important thing is

00:21:50.615 --> 00:21:55.360
that that's where we have to
go with our autonomy work.

00:21:55.360 --> 00:22:01.210
How do we make it so that for
real, complex, more cognitive

00:22:01.210 --> 00:22:04.760
kind of systems, rather
than robotic systems,

00:22:04.760 --> 00:22:08.374
we're making progress in
terms of teaming with humans.

00:22:08.374 --> 00:22:10.040
I want to give just
a couple of examples

00:22:10.040 --> 00:22:15.370
of the kinds of problems
that I think characterize

00:22:15.370 --> 00:22:20.670
the areas where humans will
remain a key part of things.

00:22:20.670 --> 00:22:24.640
So there was a story in
Bloomberg about a year or two

00:22:24.640 --> 00:22:26.960
ago when I actually contacted
Toyota and talked to them

00:22:26.960 --> 00:22:27.960
a little bit about this.

00:22:27.960 --> 00:22:31.520
But they basically reported that
they were replacing humans--

00:22:31.520 --> 00:22:36.740
sorry, replacing robots
on the manufacturing

00:22:36.740 --> 00:22:39.340
floor with humans.

00:22:39.340 --> 00:22:41.700
So taking robots off
the production line

00:22:41.700 --> 00:22:42.920
and putting humans in.

00:22:42.920 --> 00:22:47.070
And when asked why, they
said, well, basically, there's

00:22:47.070 --> 00:22:49.330
a long tradition in
Japan of mastery,

00:22:49.330 --> 00:22:50.840
sort of going back
to the samurai,

00:22:50.840 --> 00:22:52.180
you how to make your own sword.

00:22:52.180 --> 00:22:54.470
But then in the end, they
said that the bottom line

00:22:54.470 --> 00:22:58.630
is that when you put a robot on
something, it stops improving.

00:22:58.630 --> 00:23:00.480
And not only does
it stop improving,

00:23:00.480 --> 00:23:03.090
nobody knows how to improve it.

00:23:03.090 --> 00:23:07.860
So in fact, Terry and I,
just a couple weeks ago,

00:23:07.860 --> 00:23:09.930
were at a car
manufacturing facility.

00:23:09.930 --> 00:23:12.570
And we talked to
the people there.

00:23:12.570 --> 00:23:17.020
And they said no, the processes
are improving all the time,

00:23:17.020 --> 00:23:17.820
like all the time.

00:23:17.820 --> 00:23:18.960
I was actually surprised.

00:23:18.960 --> 00:23:20.460
They said at any
given point in time

00:23:20.460 --> 00:23:22.420
they're working 30 or
40 process improvements

00:23:22.420 --> 00:23:23.770
at any given station.

00:23:23.770 --> 00:23:26.695
Because the humans are like,
yeah if we put this here,

00:23:26.695 --> 00:23:27.820
it will be a little better.

00:23:27.820 --> 00:23:29.730
And it's just a
constant improvement.

00:23:29.730 --> 00:23:31.887
And so Toyota is finding
that for everywhere

00:23:31.887 --> 00:23:34.220
where you've got a machine,
there's no more improvement.

00:23:34.220 --> 00:23:35.500
So that's one area.

00:23:35.500 --> 00:23:38.120
Rio Tinto is one of the big
mining companies in the world.

00:23:38.120 --> 00:23:39.500
It's Australia based.

00:23:39.500 --> 00:23:46.420
And they have a mine that is
almost entirely autonomous.

00:23:46.420 --> 00:23:48.780
So the trucks drive themselves,
the drills, everything.

00:23:48.780 --> 00:23:50.905
There are little drones
that go around and monitor.

00:23:50.905 --> 00:23:52.279
And so they have
a control center

00:23:52.279 --> 00:23:54.980
from which they watch things.

00:23:54.980 --> 00:23:57.960
But similarly, they came, at
some point, to us and said,

00:23:57.960 --> 00:23:59.696
like look-- Because
I said, well, look.

00:23:59.696 --> 00:24:01.320
Sounds like you've
solved your problem.

00:24:01.320 --> 00:24:03.080
Why are you coming
to talk to us?

00:24:03.080 --> 00:24:05.524
You've got your mine
running without people,

00:24:05.524 --> 00:24:07.940
so you presumably saved the
money that you wanted to save.

00:24:07.940 --> 00:24:09.315
And they said,
no, the problem is

00:24:09.315 --> 00:24:11.442
we need to put some
people back in.

00:24:11.442 --> 00:24:12.525
Then that was interesting.

00:24:12.525 --> 00:24:13.483
We said, like for what?

00:24:13.483 --> 00:24:15.608
Well, sometimes you need
expert geologists in there

00:24:15.608 --> 00:24:18.024
to see whether you're going
to drill this way or that way.

00:24:18.024 --> 00:24:20.540
Because you still are nowhere
near replacing, say, an expert

00:24:20.540 --> 00:24:22.490
geologist in terms
of actually being

00:24:22.490 --> 00:24:24.550
able to physically look
at a space and say,

00:24:24.550 --> 00:24:26.040
this is the way to go.

00:24:26.040 --> 00:24:29.410
Or mines are constantly changing
and where the trucks drive.

00:24:29.410 --> 00:24:32.420
So all of the
optimization, the using

00:24:32.420 --> 00:24:36.017
of the space, all of that,
they needed humans in there.

00:24:36.017 --> 00:24:38.100
But they're having trouble
putting humans back in,

00:24:38.100 --> 00:24:40.391
because the whole thing was
designed to have no humans.

00:24:42.826 --> 00:24:45.270
So, again, similar example.

00:24:48.770 --> 00:24:51.232
Self-driving cars.

00:24:51.232 --> 00:24:52.065
I said this earlier.

00:24:52.065 --> 00:24:54.770
So one thing about
driving is driving

00:24:54.770 --> 00:24:59.640
is a largely
low-cognitive demand task.

00:24:59.640 --> 00:25:03.680
So the reason that
humans-- that it's

00:25:03.680 --> 00:25:08.130
hard for us in a lot of
instances to drive, especially

00:25:08.130 --> 00:25:10.380
say on highways when
it's long and boring,

00:25:10.380 --> 00:25:13.420
is because it demands
attention, but is not

00:25:13.420 --> 00:25:15.020
at all cognitively engaging.

00:25:15.020 --> 00:25:17.020
So you have to stay
paying attention.

00:25:17.020 --> 00:25:18.520
It's like life guarding.

00:25:18.520 --> 00:25:21.478
I remember I life
guarded in the summer.

00:25:21.478 --> 00:25:23.619
It was like
mindnumbingly boring.

00:25:23.619 --> 00:25:25.160
So you have to
develop little things,

00:25:25.160 --> 00:25:28.490
like you count the number
of little kids or whatever.

00:25:28.490 --> 00:25:30.450
So there are these
tasks where you

00:25:30.450 --> 00:25:32.110
have to stay paying attention.

00:25:32.110 --> 00:25:35.248
But they're not engaging.

00:25:35.248 --> 00:25:40.110
So the fact that
we're making progress,

00:25:40.110 --> 00:25:42.210
as Google is and
other companies are

00:25:42.210 --> 00:25:45.110
in the area of
self-driving cars,

00:25:45.110 --> 00:25:49.600
is indicative of remarkable
progress in robotics,

00:25:49.600 --> 00:25:54.374
but not necessarily yet in the
high cognitive demand tasks

00:25:54.374 --> 00:25:57.110
that I'm trying to capture,
here, with for example talking

00:25:57.110 --> 00:26:00.810
about the optimization and in
factories and things like that,

00:26:00.810 --> 00:26:03.200
where if you put a robot
in, you'll lose something.

00:26:03.200 --> 00:26:05.560
And I mean, one thing
that remains to be seen

00:26:05.560 --> 00:26:06.927
and-- I don't know.

00:26:06.927 --> 00:26:09.010
We don't know the way it's
going to turn out, yet.

00:26:09.010 --> 00:26:15.670
But as we try and get
closer to human performance.

00:26:15.670 --> 00:26:17.420
And human performance
in self-driving cars

00:26:17.420 --> 00:26:18.128
isn't that great.

00:26:18.128 --> 00:26:20.510
I mean we all know in the
order of 40,000 people

00:26:20.510 --> 00:26:23.230
die every year on the roads.

00:26:23.230 --> 00:26:26.270
But when you look at the
number of hours driven-- which

00:26:26.270 --> 00:26:27.780
a remarkably small number.

00:26:27.780 --> 00:26:31.730
So it's in the order of 1
every 200,000 to 300,000 miles

00:26:31.730 --> 00:26:34.390
for regular people that we
have a serious accident--

00:26:34.390 --> 00:26:42.170
every 10, 15, 20 years-- and
most of us, knock on wood,

00:26:42.170 --> 00:26:43.540
never.

00:26:43.540 --> 00:26:47.930
And so the last 5% challenge--
which you always know,

00:26:47.930 --> 00:26:49.250
like the beginning is easy.

00:26:49.250 --> 00:26:50.624
And as you get to
the end, you're

00:26:50.624 --> 00:26:53.105
squeezing out all the little
edge cases-- that's what hard.

00:26:53.105 --> 00:26:54.570
And that's what we're
going to bump into here.

00:26:54.570 --> 00:26:55.903
And there's a bit of a question.

00:26:59.980 --> 00:27:03.600
I tend to think that it
may be a little hard as we

00:27:03.600 --> 00:27:07.310
get to the last
5% to squeeze out

00:27:07.310 --> 00:27:10.121
that performance without
actually understanding

00:27:10.121 --> 00:27:10.620
the world.

00:27:10.620 --> 00:27:11.995
And that's the
thing with humans.

00:27:11.995 --> 00:27:14.300
We actually understand
the world in certain ways

00:27:14.300 --> 00:27:17.080
rather than, kind of just
have pattern recognition

00:27:17.080 --> 00:27:18.870
and association between things.

00:27:18.870 --> 00:27:21.920
And part of the problem is we
don't necessarily understand

00:27:21.920 --> 00:27:23.530
that well how humans do it.

00:27:23.530 --> 00:27:25.832
That's the whole problem
of consciousness,

00:27:25.832 --> 00:27:28.040
and what does it mean to be
self-aware, and all that.

00:27:28.040 --> 00:27:30.030
And what does it
mean for me to be,

00:27:30.030 --> 00:27:31.738
here I am, standing
in front of you guys.

00:27:31.738 --> 00:27:33.339
I'm aware of all this stuff.

00:27:33.339 --> 00:27:34.880
We don't understand
it all that well.

00:27:34.880 --> 00:27:36.850
But we know that it
functions in a way that

00:27:36.850 --> 00:27:42.470
allows us to understand
the world in certain ways,

00:27:42.470 --> 00:27:44.520
causally, be able
to predict things,

00:27:44.520 --> 00:27:48.140
anticipate things, to learn
things faster than others.

00:27:48.140 --> 00:27:51.880
And that's what I'll
go into a little bit.

00:27:51.880 --> 00:27:55.232
And so I'll leave it at that.

00:27:55.232 --> 00:27:56.690
I'm not going to
come back to this.

00:27:56.690 --> 00:28:00.550
But the idea that at some
point, to really kind of

00:28:00.550 --> 00:28:02.367
squeeze out the last
5% of performance,

00:28:02.367 --> 00:28:04.950
to get up to where humans are--
even though humans aren't that

00:28:04.950 --> 00:28:07.540
good at driving--
it may take more

00:28:07.540 --> 00:28:09.710
of an understanding of
the world than we're

00:28:09.710 --> 00:28:12.560
moving toward right now.

00:28:12.560 --> 00:28:17.030
So here's a view of
where we are right now.

00:28:17.030 --> 00:28:18.070
And this isn't my slide.

00:28:18.070 --> 00:28:22.700
This is from Missy
Cummings at Georgia Tech.

00:28:22.700 --> 00:28:28.560
She says, look, right now,
computers can do skills

00:28:28.560 --> 00:28:34.552
and think of skills
as routine activities.

00:28:34.552 --> 00:28:36.135
Like in driving, you
might think of it

00:28:36.135 --> 00:28:40.310
as being able to effect
smooth turns and the braking.

00:28:40.310 --> 00:28:42.922
And then there are rules like,
I know what I do at stop signs.

00:28:42.922 --> 00:28:44.380
And I know what I
do at red lights.

00:28:44.380 --> 00:28:47.350
And I know what I do at four way
stop signs versus two way stop

00:28:47.350 --> 00:28:48.580
signs, and all that.

00:28:48.580 --> 00:28:51.050
And then you have
knowledge, which is really

00:28:51.050 --> 00:28:53.330
a higher level of integration.

00:28:53.330 --> 00:28:59.040
So for example, I have
knowledge of this area

00:28:59.040 --> 00:29:03.160
of Mountain View and Los Altos
and this part of the Bay Area.

00:29:03.160 --> 00:29:05.600
Sometimes you go for work to
a new city, you rent a car,

00:29:05.600 --> 00:29:07.120
you have no knowledge.

00:29:07.120 --> 00:29:08.852
It's a little bit harder.

00:29:08.852 --> 00:29:10.310
You still have your
driving skills,

00:29:10.310 --> 00:29:11.435
your still have your rules.

00:29:11.435 --> 00:29:13.210
But you don't know the road.

00:29:13.210 --> 00:29:15.340
Google Maps has
really transformed.

00:29:15.340 --> 00:29:17.430
You know that pretty
well, so you don't even

00:29:17.430 --> 00:29:20.375
have to have that knowledge
that much anymore.

00:29:20.375 --> 00:29:22.000
And then there's kind
of the expertise.

00:29:22.000 --> 00:29:25.080
What if I run into real
issues, into problems?

00:29:25.080 --> 00:29:28.054
So this is where we are in 2015.

00:29:28.054 --> 00:29:33.760
And here's a view of where,
I think, a lot of people

00:29:33.760 --> 00:29:36.402
think we're going
to be in 2035-- 2035

00:29:36.402 --> 00:29:37.610
being a somewhat random date.

00:29:37.610 --> 00:29:38.880
But 20 years away.

00:29:42.008 --> 00:29:43.640
Humans are very good
at overestimating

00:29:43.640 --> 00:29:47.900
what we can do in five years
and very good at underestimating

00:29:47.900 --> 00:29:51.000
what we can do in 50.

00:29:51.000 --> 00:29:52.160
But that's why we chose 20.

00:29:52.160 --> 00:29:54.040
It's not 50.

00:29:54.040 --> 00:29:55.860
But I think it's
generally agreed

00:29:55.860 --> 00:30:00.260
that you're going to make
progress toward getting

00:30:00.260 --> 00:30:01.224
more knowledge in here.

00:30:01.224 --> 00:30:02.890
But then humans are
still probably going

00:30:02.890 --> 00:30:04.410
to be doing your expertise part.

00:30:04.410 --> 00:30:06.720
And so things like
driving, you're

00:30:06.720 --> 00:30:09.890
going to be able to make
a lot of progress on.

00:30:09.890 --> 00:30:12.970
And I've got up here it's
air traffic management.

00:30:12.970 --> 00:30:16.020
So managing air
traffic in the US

00:30:16.020 --> 00:30:19.970
is-- this is obviously
from our domain--

00:30:19.970 --> 00:30:22.460
it's a complex,
non-deterministic problem

00:30:22.460 --> 00:30:29.540
with a lot of parts and
where the rules do not

00:30:29.540 --> 00:30:32.050
cover the full space
of possible outcomes.

00:30:32.050 --> 00:30:34.570
And so that might be
maybe the kind of problem

00:30:34.570 --> 00:30:36.570
that, in the end, you
still need some humans in.

00:30:36.570 --> 00:30:39.330
And then it all becomes
about worrying about how

00:30:39.330 --> 00:30:40.720
does the human stay engaged?

00:30:40.720 --> 00:30:42.594
How does the human get
expertise in that area

00:30:42.594 --> 00:30:45.465
if the machines are doing a
lot of the low level stuff.

00:30:45.465 --> 00:30:47.460
I'm gonna switch gears
for a second here.

00:30:47.460 --> 00:30:50.000
I just want to talk a
little bit about humans.

00:30:52.619 --> 00:30:54.410
And you're probably
all familiar with this.

00:30:54.410 --> 00:30:58.080
But there's an
interesting aspect

00:30:58.080 --> 00:31:02.290
of humans and other animals.

00:31:02.290 --> 00:31:05.552
17th century philosophers
were concerned with

00:31:05.552 --> 00:31:07.750
are humans tabula
rasa, like blank slate.

00:31:07.750 --> 00:31:09.330
Are we born blank slate?

00:31:09.330 --> 00:31:10.610
You can learn anything.

00:31:10.610 --> 00:31:12.730
Or are we born, in a
sense, preprogramed?

00:31:12.730 --> 00:31:14.710
So now there's like
very, very clear data

00:31:14.710 --> 00:31:17.670
that we're very preprogramed.

00:31:17.670 --> 00:31:22.470
So like mountain goats
are born on cliffs.

00:31:22.470 --> 00:31:25.060
And the fact that they survive
as a species suggests they're

00:31:25.060 --> 00:31:27.330
pretty much programmed
in a very different way

00:31:27.330 --> 00:31:29.680
than other animals to
not fall off that cliff.

00:31:29.680 --> 00:31:31.430
So this is like a
day-old mountain

00:31:31.430 --> 00:31:35.130
goat standing on its mother's
back at the edge of a cliff.

00:31:35.130 --> 00:31:40.536
And so Cornell there was work
done in the 50s and 60s-- Oh,

00:31:40.536 --> 00:31:41.410
and things like this.

00:31:41.410 --> 00:31:43.620
This is something
called the visual cliff.

00:31:43.620 --> 00:31:44.480
It's a box.

00:31:44.480 --> 00:31:49.140
And then you basically have
a 90 degree angle and then

00:31:49.140 --> 00:31:50.800
a piece of Plexiglas.

00:31:50.800 --> 00:31:53.110
And they did it with
little mountain goats.

00:31:53.110 --> 00:31:55.500
And a mountain goat will
not go near that edge.

00:31:55.500 --> 00:31:59.226
Whereas babies, human babies,
if the mother says, come,

00:31:59.226 --> 00:32:01.190
baby is like, OK.

00:32:01.190 --> 00:32:04.470
So different animals were
programmed in different ways.

00:32:04.470 --> 00:32:09.250
And there's certainly a lot
of qualitative understanding

00:32:09.250 --> 00:32:12.070
of this and some beginning
of like real scientific

00:32:12.070 --> 00:32:14.800
understanding of it,
although still rough.

00:32:14.800 --> 00:32:17.750
Another good example that
I like is here on the left.

00:32:17.750 --> 00:32:26.020
So think about a hawk
hunting and flying at 200,

00:32:26.020 --> 00:32:29.830
250 feet, sees a mouse,
comes down at about 40,

00:32:29.830 --> 00:32:31.490
50 miles an hour.

00:32:31.490 --> 00:32:32.880
There's wind.

00:32:32.880 --> 00:32:36.120
The mouse is running around
trying not to get caught.

00:32:36.120 --> 00:32:38.560
And this thing comes
an inch from the ground

00:32:38.560 --> 00:32:40.490
and catches the
mouse often enough

00:32:40.490 --> 00:32:42.410
that they also
survive as a species--

00:32:42.410 --> 00:32:45.050
all with a brain this big.

00:32:45.050 --> 00:32:46.870
So the insight that
the Gibsons had--

00:32:46.870 --> 00:32:51.120
and it was a married couple at
Cornell, JJ and Jackie GIbson--

00:32:51.120 --> 00:32:54.880
was that none of this
is computational.

00:32:54.880 --> 00:32:58.735
The hawk isn't coming down and
thinking hard about, turn left.

00:32:58.735 --> 00:33:01.000
Two degrees starboard.

00:33:01.000 --> 00:33:01.750
Nothing like that.

00:33:01.750 --> 00:33:04.370
It was all like flow lines
that pretty much go directly

00:33:04.370 --> 00:33:08.070
out to the muscles in the wings.

00:33:08.070 --> 00:33:10.770
And so the whole nervous system
is kind of wired in a way

00:33:10.770 --> 00:33:14.704
to respond physically
to the environment.

00:33:14.704 --> 00:33:15.745
And I won't go into this.

00:33:15.745 --> 00:33:19.155
It was this moving room
experiment that was very--

00:33:19.155 --> 00:33:20.530
So they did all
these experiments

00:33:20.530 --> 00:33:23.630
on humans and animals at
Cornell to just understand

00:33:23.630 --> 00:33:26.240
how it is that we are
physically tied to our world,

00:33:26.240 --> 00:33:29.190
and I mean that basically the
same thing is true of humans.

00:33:29.190 --> 00:33:32.170
You take humans and you
the lines of a room.

00:33:32.170 --> 00:33:36.060
if that room moves,
the baby falls over.

00:33:36.060 --> 00:33:38.970
The thing with humans is--
and it's so easy for us

00:33:38.970 --> 00:33:40.680
to not pay attention to this.

00:33:40.680 --> 00:33:43.140
Because in some ways
the fact that we

00:33:43.140 --> 00:33:45.250
have a computational engine
as powerful as we do,

00:33:45.250 --> 00:33:46.780
each one of them
in our own heads,

00:33:46.780 --> 00:33:48.380
makes us just kind
of trivialize it.

00:33:48.380 --> 00:33:51.220
And especially as you interact
with other people and you go,

00:33:51.220 --> 00:33:52.430
you don't seem that smart.

00:33:52.430 --> 00:33:57.170
But each brain has
about 80 billion neurons

00:33:57.170 --> 00:33:59.820
with about a trillion
connections, each firing

00:33:59.820 --> 00:34:01.030
about five times a second.

00:34:01.030 --> 00:34:02.650
And it's a completely
dynamic system.

00:34:05.480 --> 00:34:07.780
So literally, there's
been about 40 years

00:34:07.780 --> 00:34:11.679
of work to try and understand
even the simplest neural

00:34:11.679 --> 00:34:12.310
systems.

00:34:12.310 --> 00:34:14.620
So there's been a lot
of work on the lobster.

00:34:14.620 --> 00:34:16.989
And the reason people
work on lobsters

00:34:16.989 --> 00:34:20.330
is because the axons that
connect the neurons are

00:34:20.330 --> 00:34:22.200
almost a millimeter thick.

00:34:22.200 --> 00:34:23.699
So it's just these
big, thick axons.

00:34:23.699 --> 00:34:28.260
So you can stick electrodes in
them and say, OK, it's firing.

00:34:28.260 --> 00:34:30.570
It's not firing.

00:34:30.570 --> 00:34:34.520
The whole swimming of a lobster
tail is about 18 neurons.

00:34:34.520 --> 00:34:36.600
40 years of work
still has not actually

00:34:36.600 --> 00:34:39.159
managed to untangle how
those 18 neurons do it.

00:34:39.159 --> 00:34:42.760
18 neurons, not 80 billion.

00:34:42.760 --> 00:34:46.940
And then an ant has about
100,000-- like a lot less.

00:34:46.940 --> 00:34:49.659
But we still don't even have
artificial ants, nothing

00:34:49.659 --> 00:34:53.020
even close that is that
autonomous, independent.

00:34:53.020 --> 00:34:59.280
So part of my point here
is although-- again,

00:34:59.280 --> 00:35:01.030
because we all have
our brains, we kind of

00:35:01.030 --> 00:35:02.180
take them for granted.

00:35:02.180 --> 00:35:04.540
But it is a remarkable
computational machine.

00:35:04.540 --> 00:35:06.770
And-- going back to
the slide before-- it

00:35:06.770 --> 00:35:09.190
is a computational
machine that is very

00:35:09.190 --> 00:35:12.870
tied to our human-- sorry to
our physical world environment

00:35:12.870 --> 00:35:13.520
as it is.

00:35:17.090 --> 00:35:18.710
Now we know certain
things about it.

00:35:18.710 --> 00:35:22.790
So the way humans tend
to think is in a--

00:35:22.790 --> 00:35:24.640
And, again, a lot of
this stuff is still

00:35:24.640 --> 00:35:27.690
understood more qualitatively
than quantitatively,

00:35:27.690 --> 00:35:30.990
in part because of the example
I just gave of the lobster tail.

00:35:30.990 --> 00:35:34.300
The problem with understanding
how the brain works is not

00:35:34.300 --> 00:35:37.430
so much the morphology of the
brain or what happens where.

00:35:37.430 --> 00:35:38.910
It's really math.

00:35:38.910 --> 00:35:42.510
The math does not exist today to
even understand the 18 neurons

00:35:42.510 --> 00:35:44.740
and how they're
working dynamically

00:35:44.740 --> 00:35:46.470
to produce the swimming.

00:35:46.470 --> 00:35:49.290
So it's going to be a long
time until the brain is fully

00:35:49.290 --> 00:35:49.790
understood.

00:35:49.790 --> 00:35:53.720
And a lot of money goes into it.

00:35:53.720 --> 00:35:56.140
But we know that humans
tend to work by induction

00:35:56.140 --> 00:35:57.720
rather than deduction.

00:35:57.720 --> 00:35:59.380
So you tend to see
one or two examples

00:35:59.380 --> 00:36:02.730
and then expand to, OK, I've
got myself a little theory.

00:36:02.730 --> 00:36:04.260
And then maybe you
test it a little.

00:36:04.260 --> 00:36:05.590
Our concepts are like that.

00:36:05.590 --> 00:36:07.755
Our concepts are, if I
have a concept of a dog,

00:36:07.755 --> 00:36:10.130
it's not because I've seen
every dog in the world or even

00:36:10.130 --> 00:36:11.150
a million dogs.

00:36:11.150 --> 00:36:13.630
I've seen a few dogs
and I expand from that.

00:36:13.630 --> 00:36:15.630
And think about how
opposite that is of the way

00:36:15.630 --> 00:36:18.040
that our machine learning
is doing it right now,

00:36:18.040 --> 00:36:20.580
where you basically are
doing pattern recognition

00:36:20.580 --> 00:36:21.810
over a very large set.

00:36:21.810 --> 00:36:25.480
And eventually, you
induce something.

00:36:25.480 --> 00:36:29.560
As children, by the time you
get to be five, six years old,

00:36:29.560 --> 00:36:31.910
you know about 25,000 words.

00:36:31.910 --> 00:36:34.970
And you're learning in the
order of 10 to 20 words a day,

00:36:34.970 --> 00:36:38.200
all day typically with
one exposure-- bicycle.

00:36:38.200 --> 00:36:38.700
Dog.

00:36:38.700 --> 00:36:39.130
Hot.

00:36:39.130 --> 00:36:39.572
Love.

00:36:39.572 --> 00:36:40.071
Right?

00:36:40.071 --> 00:36:40.898
You know-- Hot.

00:36:40.898 --> 00:36:42.230
Ooh, hot.

00:36:42.230 --> 00:36:43.790
Kids get it right away.

00:36:43.790 --> 00:36:46.180
And kids is us--
we were all kids.

00:36:46.180 --> 00:36:48.250
That's the way
this brain is made.

00:36:48.250 --> 00:36:52.640
With all of that come
a set of problems.

00:36:52.640 --> 00:36:54.530
And we all understand
human error.

00:36:57.110 --> 00:37:00.010
There's been a lot of work on
kind of the human fallacies.

00:37:00.010 --> 00:37:03.077
The fact that we pay
too much attention

00:37:03.077 --> 00:37:04.410
to certain kinds of information.

00:37:04.410 --> 00:37:09.270
That's just the flip side of
the coin of the kind of learning

00:37:09.270 --> 00:37:10.180
machines that we are.

00:37:10.180 --> 00:37:11.929
We are a certain kind
of learning machine.

00:37:11.929 --> 00:37:13.180
And that's my main point.

00:37:13.180 --> 00:37:16.410
We're not a general
information processing machine.

00:37:16.410 --> 00:37:18.640
We're an information
processing machine that

00:37:18.640 --> 00:37:21.480
works in a very causal way.

00:37:21.480 --> 00:37:23.325
We build up expertise
in certain areas

00:37:23.325 --> 00:37:25.240
and can problem
solve in ways that

00:37:25.240 --> 00:37:28.260
are very different than you
get out of, effectively, data

00:37:28.260 --> 00:37:32.170
association and
pattern matching.

00:37:32.170 --> 00:37:37.610
To give you an example, last
year we had a talk on this.

00:37:37.610 --> 00:37:38.770
Mercedes-benz was there.

00:37:38.770 --> 00:37:41.854
And nobody had to do any NDAs.

00:37:41.854 --> 00:37:44.107
Actually, I've already
seen this on the web.

00:37:44.107 --> 00:37:45.190
They did something clever.

00:37:45.190 --> 00:37:48.010
They said, look, humans
also use flowlines.

00:37:48.010 --> 00:37:51.945
So as I walk here, I'm
not thinking about OK,

00:37:51.945 --> 00:37:53.950
am I doing one foot
in front of the other?

00:37:53.950 --> 00:37:56.015
My brain is using this
line and that line

00:37:56.015 --> 00:37:57.390
to tell me that
I'm still upright

00:37:57.390 --> 00:37:58.900
and walking in the
same direction.

00:37:58.900 --> 00:38:01.200
They said what if
we do that and here,

00:38:01.200 --> 00:38:04.660
we put little lines,
dots that are moving.

00:38:04.660 --> 00:38:06.470
And instead of using
it to tell the person

00:38:06.470 --> 00:38:07.940
live, yes, you're still upright.

00:38:07.940 --> 00:38:09.540
And you'll walk in the
direction you think.

00:38:09.540 --> 00:38:10.998
It will anticipate
things a little.

00:38:10.998 --> 00:38:11.900
So I'm driving.

00:38:11.900 --> 00:38:14.720
And these flowlines
will three, four seconds

00:38:14.720 --> 00:38:18.530
before my car turns will tell me
that the car is going to turn.

00:38:18.530 --> 00:38:21.173
The beauty of it is it's
not using central attention.

00:38:21.173 --> 00:38:26.937
So central attention is one
of the most constrained things

00:38:26.937 --> 00:38:27.770
that we humans have.

00:38:27.770 --> 00:38:29.660
We have one attention channel.

00:38:32.600 --> 00:38:36.185
There's been a lot
of research on this.

00:38:36.185 --> 00:38:37.880
And most of the data
points out the fact

00:38:37.880 --> 00:38:39.730
that really you
can't pay attention

00:38:39.730 --> 00:38:40.860
to two things at once.

00:38:40.860 --> 00:38:43.640
You can switch back and
forth pretty quickly,

00:38:43.640 --> 00:38:44.340
one, two things.

00:38:44.340 --> 00:38:45.798
Three things if
you get good at it.

00:38:45.798 --> 00:38:47.926
But mostly you've got
one central channel.

00:38:50.660 --> 00:38:52.620
And they've been testing
this, and it works.

00:38:52.620 --> 00:38:55.510
So this is at a low level,
what we're talking about,

00:38:55.510 --> 00:38:58.180
about intelligent systems
communicating with humans.

00:38:58.180 --> 00:39:00.680
How do you think about building
it in so you're not even

00:39:00.680 --> 00:39:01.952
taxing humans?

00:39:01.952 --> 00:39:04.160
And then in this way then
you start thinking of a car

00:39:04.160 --> 00:39:05.440
almost more like a horse.

00:39:05.440 --> 00:39:07.440
I mean the horse is doing
some of its own things

00:39:07.440 --> 00:39:12.553
and it's communicating
it to-- And this, again,

00:39:12.553 --> 00:39:13.240
came from them.

00:39:13.240 --> 00:39:15.406
They want cars to be more
like domesticated animals.

00:39:18.773 --> 00:39:24.310
On the NASA side, in a lot
of the areas that matter,

00:39:24.310 --> 00:39:25.720
we don't have big data.

00:39:25.720 --> 00:39:28.460
We have-- it's a little
joke that we had.

00:39:30.864 --> 00:39:33.030
NASA has a big problem with
data, but not a big data

00:39:33.030 --> 00:39:34.570
problem.

00:39:34.570 --> 00:39:36.670
For the whole Space
Shuttle program,

00:39:36.670 --> 00:39:40.092
we had about 980,000
problem reports.

00:39:40.092 --> 00:39:41.800
And problem reports
are really important.

00:39:41.800 --> 00:39:42.560
It's the history.

00:39:42.560 --> 00:39:44.667
It's what tells you
what risks you have.

00:39:44.667 --> 00:39:46.500
It helps you understand
the fact that you've

00:39:46.500 --> 00:39:48.060
got risk coming forward.

00:39:48.060 --> 00:39:49.305
And 980,000 is very little.

00:39:49.305 --> 00:39:51.180
It's too much for any
person to have in mind.

00:39:51.180 --> 00:39:53.150
But it's not enough
to really give you

00:39:53.150 --> 00:39:57.420
any kind of power in the way
that, for example, Google

00:39:57.420 --> 00:40:00.430
search yields, answers
the way it does because

00:40:00.430 --> 00:40:03.470
of the combination of big data
plus understanding what humans

00:40:03.470 --> 00:40:06.290
have looked at.

00:40:06.290 --> 00:40:12.224
But big data, at the same
time, is remarkably powerful.

00:40:12.224 --> 00:40:14.640
And this has been one of the
big source of transformation,

00:40:14.640 --> 00:40:20.280
it yields just associations
and your data falls out.

00:40:20.280 --> 00:40:23.460
So this was one of the
early Google deep learning

00:40:23.460 --> 00:40:28.810
experiments, where they looked
at still images from YouTube

00:40:28.810 --> 00:40:30.900
and these three things fell out.

00:40:30.900 --> 00:40:33.280
So there was like a face.

00:40:33.280 --> 00:40:36.932
These were like when you
looked at the kind of averages

00:40:36.932 --> 00:40:39.160
that the algorithm
came up with, there

00:40:39.160 --> 00:40:42.130
was a face, a blobby
thing that was maybe

00:40:42.130 --> 00:40:44.376
a human detector,
and then a cat,

00:40:44.376 --> 00:40:46.292
because I guess there
are a lot of cat videos.

00:40:50.190 --> 00:40:51.840
And the important
thing is, again, I

00:40:51.840 --> 00:40:54.830
want to contrast this with
what I was saying before.

00:40:54.830 --> 00:40:58.380
So this is what you get when you
look at millions and millions

00:40:58.380 --> 00:40:59.590
and millions of things.

00:40:59.590 --> 00:41:01.200
And then you get these averages.

00:41:01.200 --> 00:41:07.340
And babies detect
faces at birth.

00:41:07.340 --> 00:41:09.650
Babies will focus
on a face at birth.

00:41:09.650 --> 00:41:12.540
And there are studies
that show that pigeons

00:41:12.540 --> 00:41:15.100
will recognize human faces.

00:41:15.100 --> 00:41:17.430
And not just human faces,
specific human faces.

00:41:17.430 --> 00:41:22.410
OK so not just humans with
our big brains, but even birds

00:41:22.410 --> 00:41:24.620
with their little brain.

00:41:24.620 --> 00:41:27.940
And again, we're born that way.

00:41:27.940 --> 00:41:31.250
And so as you kind
of raise this up,

00:41:31.250 --> 00:41:34.280
and you say, well,
what are the--

00:41:34.280 --> 00:41:37.600
So a little more here on
kind of the deep learning

00:41:37.600 --> 00:41:38.600
for pattern recognition.

00:41:43.090 --> 00:41:45.452
So if I look up a
picture like this.

00:41:45.452 --> 00:41:47.160
And I say show me
other pictures like it,

00:41:47.160 --> 00:41:49.701
and it shows me other pictures
like it, you sort of say, boy,

00:41:49.701 --> 00:41:52.380
that seems intelligent.

00:41:52.380 --> 00:41:53.870
But the key point
is that this is

00:41:53.870 --> 00:41:56.380
being arrived at in a very
different way than humans.

00:41:56.380 --> 00:42:00.800
And the consequence of that
is that if you think about it,

00:42:00.800 --> 00:42:03.630
for certain kinds of problems,
you still want that kind

00:42:03.630 --> 00:42:11.630
of adaptive, inductive,
humans problem solving .

00:42:11.630 --> 00:42:14.940
So here, I go a little
bit more into this point.

00:42:14.940 --> 00:42:17.305
So humans are very
much about causation.

00:42:17.305 --> 00:42:18.680
Intelligence
systems are really--

00:42:18.680 --> 00:42:21.760
and have continued to
be-- about correlation.

00:42:21.760 --> 00:42:25.100
And by the way, I'm not
making any of this up.

00:42:25.100 --> 00:42:27.320
I mean this is like
when I was at Carnegie

00:42:27.320 --> 00:42:33.580
Mellon in the '90s, working
AI, what we were working on

00:42:33.580 --> 00:42:36.050
was trying to get
causal reasoning out

00:42:36.050 --> 00:42:39.440
of intelligent systems, It
just hasn't really progressed.

00:42:39.440 --> 00:42:40.510
It's been hard.

00:42:40.510 --> 00:42:42.110
The real progress,
as I said, has come

00:42:42.110 --> 00:42:47.500
from big data and Moore's law.

00:42:47.500 --> 00:42:49.420
And again, I think
that internet of things

00:42:49.420 --> 00:42:53.240
is going to be the
other big changer.

00:42:53.240 --> 00:42:58.320
The reason this is interesting
is because the minute something

00:42:58.320 --> 00:43:00.180
knows itself what it is.

00:43:00.180 --> 00:43:04.800
So this thing, in some way,
it knows it's Alonzo's phone,

00:43:04.800 --> 00:43:06.240
and can even tell
you where it is.

00:43:06.240 --> 00:43:09.710
And something can ping it and
say, are you Alonzo's phone?

00:43:09.710 --> 00:43:11.320
Yes, I am.

00:43:11.320 --> 00:43:13.530
My pen, at some point,
might know that it's my pen.

00:43:13.530 --> 00:43:14.380
And it'll know where it is.

00:43:14.380 --> 00:43:15.410
It might even know
it's being held.

00:43:15.410 --> 00:43:17.260
It might even know
it's being held by me.

00:43:17.260 --> 00:43:19.040
That is all going to
come very quickly.

00:43:19.040 --> 00:43:23.130
And that begins to
change our ability

00:43:23.130 --> 00:43:24.620
to interact with the world.

00:43:24.620 --> 00:43:26.730
Because suddenly like
the things in the world--

00:43:26.730 --> 00:43:29.100
you begin to solve some of
that kind of old AI grounding

00:43:29.100 --> 00:43:31.760
problem, this way.

00:43:31.760 --> 00:43:34.230
So humans, you know, what
we're good at, again,

00:43:34.230 --> 00:43:37.985
is solving in a slow
way, like hours today

00:43:37.985 --> 00:43:41.438
is complex problems
under uncertaintly.

00:43:41.438 --> 00:43:44.890
And machines tend to
be just the opposite--

00:43:44.890 --> 00:43:49.450
very fast, but problems
that are well determined.

00:43:49.450 --> 00:43:52.290
And so you get a real synergy.

00:43:52.290 --> 00:43:57.370
And if done right, machine
intelligence actually

00:43:57.370 --> 00:43:59.460
fills exactly the gaps
that humans are bad at

00:43:59.460 --> 00:44:02.200
and vice versa.

00:44:02.200 --> 00:44:05.254
This is from the
Department of Defense.

00:44:05.254 --> 00:44:06.920
It was just interesting
to me to go look

00:44:06.920 --> 00:44:08.940
at what their
investment profile is.

00:44:08.940 --> 00:44:12.900
This little blue side is human
and autonomous interaction

00:44:12.900 --> 00:44:14.550
and collaboration.

00:44:14.550 --> 00:44:18.520
So 50% of the investment the
Department of Defense has

00:44:18.520 --> 00:44:20.820
is on the human autonomy team.

00:44:20.820 --> 00:44:23.270
And I actually think that
what's going to happen over

00:44:23.270 --> 00:44:27.720
the next 15 to 20 years is that
those companies that are aware,

00:44:27.720 --> 00:44:30.520
really, that you don't
want to end up with things

00:44:30.520 --> 00:44:33.410
like the littoral
combat ship and that

00:44:33.410 --> 00:44:36.925
invest in understanding,
how do we pair humans

00:44:36.925 --> 00:44:39.610
with intelligent systems,
especially when you've

00:44:39.610 --> 00:44:43.420
got a need for those
humans because you

00:44:43.420 --> 00:44:47.237
need the problem solving, you
need the adaptive reasoning.

00:44:47.237 --> 00:44:48.820
I think that that's
where you're going

00:44:48.820 --> 00:44:53.550
to see the next generation of
companies that push forward.

00:44:53.550 --> 00:44:57.870
And in some ways, I think
it's one view of maybe what

00:44:57.870 --> 00:44:59.350
Steve Jobs was doing at Apple.

00:44:59.350 --> 00:45:02.280
I mean he kept that company
entirely focused on, what

00:45:02.280 --> 00:45:05.590
does the end user need to
see, what is that interaction,

00:45:05.590 --> 00:45:09.124
and developed a unique
set of products.

00:45:09.124 --> 00:45:10.090
It's a broad statement.

00:45:10.090 --> 00:45:13.169
But I think it was
part of what he really

00:45:13.169 --> 00:45:15.460
brought to bear in a way that
other companies struggled

00:45:15.460 --> 00:45:18.934
to do-- and I'm talking here
about like say mobile phone

00:45:18.934 --> 00:45:23.570
companies or whatever--
How do I integrate this

00:45:23.570 --> 00:45:26.570
with human experience so
you're not even really

00:45:26.570 --> 00:45:29.080
aware that this
thing is actually

00:45:29.080 --> 00:45:30.930
sort of working with you.

00:45:30.930 --> 00:45:33.246
In fact, I have an
example from-- so remember

00:45:33.246 --> 00:45:36.770
when there was a fallout
between Apple and Google?

00:45:36.770 --> 00:45:39.960
And Google Maps wasn't
on the latest version?

00:45:39.960 --> 00:45:41.460
And then it reappeared.

00:45:41.460 --> 00:45:43.790
So my wife said, I'm
trying to find this place.

00:45:43.790 --> 00:45:44.450
And I said, No.

00:45:44.450 --> 00:45:45.283
Install Google Maps.

00:45:45.283 --> 00:45:46.200
It's much better.

00:45:46.200 --> 00:45:47.148
And so she did.

00:45:47.148 --> 00:45:48.860
And then she entered her thing.

00:45:48.860 --> 00:45:50.970
And she said, how
do I tell it to go?

00:45:50.970 --> 00:45:53.680
And I said, well, you don't.

00:45:53.680 --> 00:45:55.540
It knows when you're starting.

00:45:55.540 --> 00:45:57.790
It's not playing a
little routine for you.

00:45:57.790 --> 00:46:01.350
It's interacting and knows--
And so she was like, oh yeah.

00:46:01.350 --> 00:46:03.695
And so it was a bit
of a conceptual shift.

00:46:03.695 --> 00:46:05.736
The funny thing is, she
walked out of the kitchen

00:46:05.736 --> 00:46:07.152
and started walking
down the hall.

00:46:07.152 --> 00:46:09.500
And it said, turn left
at North Springer Road.

00:46:09.500 --> 00:46:10.740
So it's good.

00:46:10.740 --> 00:46:13.050
But they're not great, yet.

00:46:13.050 --> 00:46:16.490
There's a certain larger
context which they lack.

00:46:16.490 --> 00:46:19.640
Similarly is a break
down that I had.

00:46:19.640 --> 00:46:21.890
A little company in the
Valley called Venture Scanner.

00:46:21.890 --> 00:46:25.410
And I had them look
at AI investments.

00:46:25.410 --> 00:46:26.860
This is an industry.

00:46:26.860 --> 00:46:29.210
And this is sort of
like current investment

00:46:29.210 --> 00:46:32.690
in artificial intelligence
of venture capital.

00:46:32.690 --> 00:46:36.310
And you look at the
blue and the red

00:46:36.310 --> 00:46:39.440
are sort of the
collaborative versus green

00:46:39.440 --> 00:46:41.210
is what you call
narrow autonomy.

00:46:41.210 --> 00:46:43.660
So, for example, here,
you get deep learning,

00:46:43.660 --> 00:46:46.679
machine learning-- tends to
be not worried about how it's

00:46:46.679 --> 00:46:49.122
going to interact with humans.

00:46:49.122 --> 00:46:50.580
Other areas like
natural language--

00:46:50.580 --> 00:46:51.490
obviously a lot of blue.

00:46:51.490 --> 00:46:53.830
But you look overall, and
it's about the same pattern.

00:46:53.830 --> 00:46:56.061
About half the
investment right now

00:46:56.061 --> 00:47:00.120
is going into human
autonomy teaming.

00:47:00.120 --> 00:47:01.690
So final thoughts.

00:47:01.690 --> 00:47:05.850
I think robotics will continue
to progress faster than AI.

00:47:05.850 --> 00:47:08.139
Humans will very likely
remain important components

00:47:08.139 --> 00:47:08.930
of complex systems.

00:47:08.930 --> 00:47:11.013
And I hope I've convinced
you a little bit, again,

00:47:11.013 --> 00:47:18.310
that in this hour that the
way our brains are shaped

00:47:18.310 --> 00:47:20.750
by evolution, and the
way that we think,

00:47:20.750 --> 00:47:23.000
and the way we learn
about the world

00:47:23.000 --> 00:47:27.000
is very much determined,
again, by evolution,

00:47:27.000 --> 00:47:30.860
by natural selection, by
what allowed us to survive.

00:47:30.860 --> 00:47:35.640
And so we are a certain kind of
learning, thinking, reasoning

00:47:35.640 --> 00:47:36.140
machine.

00:47:36.140 --> 00:47:38.120
We're not a general
kind of-- And with

00:47:38.120 --> 00:47:39.460
that comes certain benefits.

00:47:39.460 --> 00:47:42.510
Machines are a very different
kind of learning system,

00:47:42.510 --> 00:47:43.340
right now.

00:47:43.340 --> 00:47:45.060
We haven't actually
really made progress

00:47:45.060 --> 00:47:46.559
in trying to simulate
the human one.

00:47:46.559 --> 00:47:48.360
It's unclear that we want to.

00:47:48.360 --> 00:47:51.730
But what I think we do want
to is increasingly pair those.

00:47:51.730 --> 00:47:54.430
And that's something that at the
moment, we don't do very well.

00:47:54.430 --> 00:47:56.520
And it costs us.

00:47:56.520 --> 00:47:59.550
So use human adaptive
expertise as much as possible.

00:47:59.550 --> 00:48:03.920
Use the human perceptual system
as much as possible-- that idea

00:48:03.920 --> 00:48:09.557
like in the Mercedes-Benz
cars and with the flowlines.

00:48:09.557 --> 00:48:12.140
And then be aware in some areas,
when you don't have big data.

00:48:12.140 --> 00:48:14.200
Because not all problems
are associated in nature.

00:48:14.200 --> 00:48:15.510
And that's probably,
more often than not,

00:48:15.510 --> 00:48:17.009
when you actually
still need humans.

00:48:17.009 --> 00:48:20.890
So those are my last thoughts.

00:48:20.890 --> 00:48:22.118
Thank you.

00:48:22.118 --> 00:48:24.508
[APPLAUSE]

00:48:28.340 --> 00:48:30.650
AUDIENCE: Where are
we in terms of using

00:48:30.650 --> 00:48:34.076
the brain as a model for AI?

00:48:34.076 --> 00:48:37.645
Like whereabouts are
we in that research?

00:48:43.207 --> 00:48:44.790
ALONSO VERA: Because
our understanding

00:48:44.790 --> 00:48:46.980
of the brain at this point
is really so rudimentary,

00:48:46.980 --> 00:48:49.510
it's much less advanced
than you'd think.

00:48:49.510 --> 00:48:53.190
So a few years ago,
there was a project--

00:48:53.190 --> 00:48:56.720
and this is maybe five
years ago now, funded

00:48:56.720 --> 00:48:59.845
by DARPA called--
what was it called--

00:48:59.845 --> 00:49:01.675
I think it was called
the Brain Initiative.

00:49:05.220 --> 00:49:07.000
We knew people who were on that.

00:49:07.000 --> 00:49:11.620
They really went and took
the top computational

00:49:11.620 --> 00:49:14.220
cognitive modeling
people, neuroscientists

00:49:14.220 --> 00:49:16.751
from every university.

00:49:16.751 --> 00:49:18.180
The top ones.

00:49:18.180 --> 00:49:19.990
That first meeting
had about 60 people.

00:49:19.990 --> 00:49:23.490
And they were really the
top people in the country.

00:49:23.490 --> 00:49:25.734
And everybody sat in that room.

00:49:25.734 --> 00:49:27.650
And this is sort of the
way DARPA does things.

00:49:27.650 --> 00:49:29.720
But the goal was,
in two years, we

00:49:29.720 --> 00:49:33.090
want to have a functional
model brain kinda thing.

00:49:33.090 --> 00:49:38.360
Everybody in that room said
like, we're 100 years off that,

00:49:38.360 --> 00:49:39.280
as far as I know.

00:49:39.280 --> 00:49:42.710
So now there could be some
transformational, some quantum

00:49:42.710 --> 00:49:45.710
leap that happens--
something that changes

00:49:45.710 --> 00:49:46.840
the way we look at this.

00:49:46.840 --> 00:49:48.544
But it hasn't happened yet.

00:49:48.544 --> 00:49:50.210
And again, as I said
before, most people

00:49:50.210 --> 00:49:51.570
think that the real
challenge at this point

00:49:51.570 --> 00:49:52.790
is we don't have the math.

00:49:52.790 --> 00:49:56.150
So people say what we need
is the Newton for brain.

00:49:56.150 --> 00:49:57.920
Somebody comes up
and says, you're

00:49:57.920 --> 00:50:00.389
not gonna solve these problems
until you have calculus.

00:50:00.389 --> 00:50:02.430
because for this kind of
complex dynamic systems,

00:50:02.430 --> 00:50:04.660
that's the problem.

00:50:04.660 --> 00:50:09.220
Computationally, with even
our current computer power,

00:50:09.220 --> 00:50:11.536
you can't simulate
anything close to it.

00:50:11.536 --> 00:50:14.710
And really the most fundamental
thing is we barely understand.

00:50:14.710 --> 00:50:20.570
So we know, for example,
that the visual cortex,

00:50:20.570 --> 00:50:22.700
area 17 of the brain,
which is back here,

00:50:22.700 --> 00:50:30.090
actually has maps-- like
actual representations of what

00:50:30.090 --> 00:50:33.240
I'm seeing and that there
are neurons that detect lines

00:50:33.240 --> 00:50:37.460
and that detect crossing
of lines and movement.

00:50:37.460 --> 00:50:39.590
So we understand
some of those things,

00:50:39.590 --> 00:50:41.840
and to some extent,
areas of the brain, etc.

00:50:41.840 --> 00:50:44.020
But the brain as
a dynamic system

00:50:44.020 --> 00:50:48.370
that's processing information
and using that it's really

00:50:48.370 --> 00:50:51.120
not well understood yet.

00:50:51.120 --> 00:50:53.360
AUDIENCE: So you talked
quite a bit about evolution

00:50:53.360 --> 00:50:56.430
and how the human
brain has evolved

00:50:56.430 --> 00:50:57.690
to solve certain problems.

00:50:57.690 --> 00:50:59.439
And then you made a
claim late in the talk

00:50:59.439 --> 00:51:03.050
that basically people are
bad at quickly processing

00:51:03.050 --> 00:51:04.510
large datasets reliably.

00:51:04.510 --> 00:51:07.560
So vision is sort of the
counterargument to that, right?

00:51:07.560 --> 00:51:10.190
So the visual system
is specialized,

00:51:10.190 --> 00:51:12.930
as you pointed out, to
process a large volume of data

00:51:12.930 --> 00:51:14.330
very quickly and very reliably.

00:51:14.330 --> 00:51:17.360
So is that the case, perhaps,
that because of the way

00:51:17.360 --> 00:51:20.220
we've evolved, we just
have certain big data

00:51:20.220 --> 00:51:23.040
problems that the human brain
is capable of effectively

00:51:23.040 --> 00:51:24.150
and quickly processing.

00:51:24.150 --> 00:51:28.110
Whereas a simulated system
could be a generalized learning

00:51:28.110 --> 00:51:31.061
machine, could bring that, sort
of-- if we could understand

00:51:31.061 --> 00:51:33.060
a compositional apparatus
to bear on any problem

00:51:33.060 --> 00:51:36.810
rather than just those that were
evolutionarily advantageous?

00:51:36.810 --> 00:51:38.310
So do you have any
comments on that?

00:51:38.310 --> 00:51:38.650
ALONSO VERA: Yeah.

00:51:38.650 --> 00:51:40.030
No, that's a really good point.

00:51:40.030 --> 00:51:41.530
I actually hadn't
thought of that,

00:51:41.530 --> 00:51:45.260
that a lot of what we do-- like
what the visual system does,

00:51:45.260 --> 00:51:48.260
what our mobility systems do.

00:51:48.260 --> 00:51:51.540
So I mean take another example.

00:51:51.540 --> 00:51:55.760
So walking robots, biped robots
has been a very difficult

00:51:55.760 --> 00:51:56.860
thing, right?

00:51:56.860 --> 00:52:00.440
And yet babies are walking
in less than a year.

00:52:00.440 --> 00:52:03.130
And so a lot of the--
And you're right.

00:52:03.130 --> 00:52:05.420
Those are, in a sense,
they are big data problems.

00:52:05.420 --> 00:52:07.620
It's just we're
pre-wired to do those.

00:52:07.620 --> 00:52:10.840
So the things that are the
big data problems like fusion

00:52:10.840 --> 00:52:15.690
of visual information, we seem
to have a lot of preparedness

00:52:15.690 --> 00:52:18.281
for.

00:52:18.281 --> 00:52:18.780
Yeah.

00:52:18.780 --> 00:52:23.447
So maybe then the
argument becomes

00:52:23.447 --> 00:52:25.030
that the part of the
human brain which

00:52:25.030 --> 00:52:30.400
is sort of the higher level
information processing,

00:52:30.400 --> 00:52:34.750
problem solving, that it's
that part that doesn't function

00:52:34.750 --> 00:52:35.800
on big data.

00:52:35.800 --> 00:52:38.110
But yeah, the basic
perceptual system does.

00:52:38.110 --> 00:52:40.760
Yeah Thank you.

00:52:40.760 --> 00:52:42.610
[APPLAUSE]

