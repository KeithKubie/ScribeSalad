WEBVTT
Kind: captions
Language: en

00:00:00.190 --> 00:00:02.460
This is a virtual human

00:00:02.980 --> 00:00:05.219
She's currently not really paying attention to me

00:00:05.220 --> 00:00:08.399
but but later on we will turn her on and then she will be able to

00:00:08.470 --> 00:00:16.139
respond to me when I said virtually even I really what I mean is an embodied conversational agent, but that's quite a mouthful so

00:00:17.020 --> 00:00:20.699
we just call it's shorthand virtually human the point is that

00:00:21.369 --> 00:00:25.799
it looks like a human but it's on a screen so it's not a robot and

00:00:26.980 --> 00:00:31.049
It behaves and talks and listens the way humans do

00:00:33.430 --> 00:00:39.720
We use two sensors really so we use a microphone and a camera so she can see me

00:00:40.059 --> 00:00:42.959
basically, it's her vision and she can hear me and

00:00:43.780 --> 00:00:45.840
We can begin give the trials. Hello

00:00:50.290 --> 00:00:52.290
What are we doing today?

00:01:01.030 --> 00:01:03.030
Yeah, sure

00:01:07.210 --> 00:01:12.539
For each question, please answer in terms of options to show them the screen

00:01:15.429 --> 00:01:23.399
More than four days every day. Okay. Here's the first question over the last two weeks

00:01:28.810 --> 00:01:30.810
Not at all

00:01:32.619 --> 00:01:34.619
Yes

00:01:37.179 --> 00:01:39.269
Okay, here's the second question

00:01:47.259 --> 00:01:49.259
Tower Bridge

00:01:51.099 --> 00:01:54.959
You answered no

00:01:57.550 --> 00:02:00.959
I'm sorry. Can you please repeat your answer?

00:02:04.300 --> 00:02:06.300
Brown sauce

00:02:08.979 --> 00:02:10.979
No

00:02:11.110 --> 00:02:18.599
It's a study to see if we can replace a written questionnaire with a with a virtual human questionnaire, so that's it's a bit different

00:02:19.209 --> 00:02:22.229
the mock normally do we also made a

00:02:22.900 --> 00:02:28.440
Version of this called Alice where you can ask things about the book Alice in Wonderland's and in general

00:02:28.440 --> 00:02:33.329
There's many different ways in which you can use cultural units. I suppose all you're seeing is a unity

00:02:34.360 --> 00:02:42.360
skin a unity character and you can replace it with any unity character that you want but in it all starts with

00:02:43.450 --> 00:02:47.339
The input basically she you can listen to what I'm saying

00:02:47.340 --> 00:02:52.830
And you can see my face. So we're basically creating characters that are

00:02:53.830 --> 00:02:55.600
linguistically skilled

00:02:55.600 --> 00:02:57.600
emotionally skilled socially skilled

00:02:58.090 --> 00:03:02.070
So they there's a speech recognition module in there

00:03:02.740 --> 00:03:06.659
That basically captures all the words that I'm saying and based on that

00:03:08.050 --> 00:03:13.020
She can determine what how to respond, but she also takes into account

00:03:13.930 --> 00:03:21.090
My face and my facial expressions, so that means she knows when I'm looking at her when I'm looking at her it means that

00:03:21.940 --> 00:03:26.190
She knows when I'm smiling not smiling and then you can start doing interesting things

00:03:26.290 --> 00:03:31.139
Because then the agent can start back channeling so it can also start smiling back when I smile

00:03:32.050 --> 00:03:35.880
Nodding when I not or if you want to create a particular

00:03:36.670 --> 00:03:43.619
Persona, a particular character that is absolutely abrasive or even aggressive you can start doing the opposite

00:03:43.620 --> 00:03:45.620
So we made a character called spike

00:03:52.600 --> 00:03:59.429
Always an answer for everything and if you not he will shake if you say yes, you'll say no

00:04:00.370 --> 00:04:07.649
And people find it really engaging but they get really worked up talking to spike. So there's the

00:04:08.350 --> 00:04:15.899
Let's say the sensing component that recognizes your face your facial expressions your voice your speech. Also how old you are and

00:04:16.419 --> 00:04:17.500
that

00:04:17.500 --> 00:04:22.919
Information gets sent into what we call the dialogue manager and the dialogue manager maintains

00:04:23.560 --> 00:04:28.619
What to say next and also maintains what's you know what we call dialogue state?

00:04:28.620 --> 00:04:30.250
So what has it already said?

00:04:30.250 --> 00:04:37.169
what I said topic that we're talking about at the moment and then based on the input and the current state it decides what to

00:04:37.169 --> 00:04:44.309
say next that is sent in a very high-level commands using something called behavior markup language to the

00:04:44.860 --> 00:04:50.940
Realizing and they realized it and turns that all into the actions that need to be made and the speech that needs to be

00:04:51.490 --> 00:04:58.380
Spoken quite a lot of effort went into this particular virtual agent into making sure that there was synchronized lips

00:04:58.710 --> 00:04:59.890
the speech and

00:04:59.890 --> 00:05:07.590
That you can interrupt the agent because in the past quite often people made these virtually humans were in a sense. They played a little

00:05:08.140 --> 00:05:11.490
Little pre-programmed movie. It was a bit of code

00:05:12.970 --> 00:05:16.410
of what to say that was turned into a set of behaviors and

00:05:16.720 --> 00:05:20.519
Once that was executed they were played and there's no way of stopping it

00:05:20.550 --> 00:05:27.060
But of course in real life you quite often want to interrupt the agent, but then you need to be able to stop that

00:05:27.940 --> 00:05:29.940
behavior realization and

00:05:31.360 --> 00:05:34.410
Gracefully going to a neutral mode into a listening mode, etc

00:05:34.420 --> 00:05:39.090
So that's the kind of developments that we're working on with these virtual humans

00:05:39.090 --> 00:05:42.389
Is this some kind of machine learning or is it what's going on behind?

00:05:42.550 --> 00:05:43.050
No

00:05:43.050 --> 00:05:48.509
It's it's not a machine learning but it's an algorithm that can go from any

00:05:48.670 --> 00:05:55.770
State that it's in back to a neutral state to the next day. So it's it's an algorithm that allows you to break

00:05:57.490 --> 00:06:01.799
Where you currently are and go to a next phase while it's running

00:06:02.890 --> 00:06:07.169
Rather than having to wait until the end of the segment to play and then going back

00:06:07.510 --> 00:06:13.590
what sorts of challenges do you get from having to recognize speech and you know getting a device or

00:06:13.870 --> 00:06:15.870
a virtual human to respond

00:06:16.420 --> 00:06:24.000
yeah, so there's actually a number of components that they can do if you might have realized that having before the speech recognition happens, so

00:06:24.550 --> 00:06:28.560
The first one is is somebody speaking or not. So this voice Activia detection

00:06:29.080 --> 00:06:31.740
and then another very important one is

00:06:32.350 --> 00:06:37.079
Turn-taking if the agent starts interrupting you while you're speaking that's really annoying

00:06:37.450 --> 00:06:42.360
So you need to know when somebody is done speaking and you can't just work by

00:06:42.610 --> 00:06:49.979
Voice activity detection because sometimes there's little pauses in my speech and you're not supposed to just button

00:06:51.640 --> 00:06:52.810
So

00:06:52.810 --> 00:06:59.939
So that that's actually done with machine learning where we learn exactly when it's okay for an agent to interrupt

00:07:00.790 --> 00:07:02.790
Not interrupt to take that turn

00:07:03.910 --> 00:07:05.910
although for some agents

00:07:06.430 --> 00:07:08.430
Sometimes it's it is

00:07:08.770 --> 00:07:13.949
Appropriate to interrupt but but at least when you know when when somebody is speaking their turn

00:07:14.230 --> 00:07:19.740
Then the agent knows that if they would speak now, they would be interrupting and that would of course have a particular

00:07:20.350 --> 00:07:23.670
Effect only conversation the person might be unhappy about that

00:07:23.670 --> 00:07:31.140
But maybe that's the the point of the interruption as part of this project and other projects we have recorded many databases

00:07:31.360 --> 00:07:33.360
quite often of people talking

00:07:34.000 --> 00:07:40.500
Human to human but through screens. So it's as realistic as possible to the setting as we would have with a virtual human

00:07:41.020 --> 00:07:48.000
And then we annotate all the data for when I was speaking. What are they saying? Are they smiling or not?

00:07:48.000 --> 00:07:53.309
And and of course then from that also deterrence is whose turn is it and then who's you know?

00:07:53.650 --> 00:07:59.609
When those go into in somebody else's turn and then you can also when was there an interruption and how was it dealt with?

00:07:59.920 --> 00:08:04.500
And yeah, so there's a lot of there's a lot of manual work going in to that

00:08:04.630 --> 00:08:10.560
We can show you some of the these are our dashboard in a sense arousal is an emotion. That's basically how

00:08:11.170 --> 00:08:13.650
Excited I am and that was relatively high

00:08:13.650 --> 00:08:19.410
I don't think it's currently doing valence because this is from video and my face is not currently in front of the camera

00:08:19.750 --> 00:08:26.340
But it would recognize whether I'm happy or not happy and this is where I was a child youth and adult or senior

00:08:27.070 --> 00:08:30.900
Unfortunately, I'm slowly moving towards the senior a bit

00:08:30.900 --> 00:08:35.609
And this is some things about what kind of vocalizations do I have MS island. Is it a filler?

00:08:35.610 --> 00:08:41.880
Is it a brief or anything else if I swell the valence should go up but I can't

00:08:43.330 --> 00:08:48.840
So that's the kind of facial expressions that we're that we're making and then we can work with that

00:08:48.880 --> 00:08:53.280
Is there a big database behind here is an ontology. How's it work? There are?

00:08:54.280 --> 00:08:57.420
okay, so all the sensing is done on a

00:08:58.930 --> 00:09:01.020
On a learning basis, so that's all machine learning

00:09:01.090 --> 00:09:08.009
So the facial expressions are learned using large databases the speech recognition. I learned age estimation

00:09:08.650 --> 00:09:12.689
emotional technicians all done by by machine learning the

00:09:13.240 --> 00:09:15.240
dialogue manager actually is

00:09:15.280 --> 00:09:17.280
It's a rule based. So it's handcrafted

00:09:18.400 --> 00:09:23.879
So that actually gives you the ability to author a scenario quite quickly

00:09:24.779 --> 00:09:26.640
the downside of

00:09:26.640 --> 00:09:27.779
doing

00:09:27.779 --> 00:09:29.640
natural language processing

00:09:29.640 --> 00:09:34.759
With a machine learning approach is that if you wanted to create any new scenario

00:09:34.890 --> 00:09:38.899
You would basically need to first collect lots and lots of data to then ultimately

00:09:39.149 --> 00:09:41.958
Create what it is that you want to dialogue to be about

00:09:42.510 --> 00:09:45.859
and because we are mostly working on these virtual humans as

00:09:46.860 --> 00:09:52.700
Tools to do, you know a particular study with or ask particular questions?

00:09:53.490 --> 00:10:00.289
or provide particularly information you want to be able to craft to author those interactions really well, and then actually a

00:10:00.959 --> 00:10:05.779
Rule-based system is much better with a particular ontology of topics

00:10:06.690 --> 00:10:07.770
turns

00:10:07.770 --> 00:10:14.509
etcetera sentences and that you can you can all create that of course where we want to go to is sort of a

00:10:14.940 --> 00:10:19.010
halfway house between a machine learn and a

00:10:19.320 --> 00:10:24.559
crafted version so that we can still craft the outline of the of the

00:10:25.589 --> 00:10:30.769
Discourse so the topics the questions to be asked etc. But we want to

00:10:31.410 --> 00:10:39.050
Learn or all the possible variations in which you can say that and all the possible variations in which people can answer questions

00:10:39.390 --> 00:10:42.049
because at the moment we have to hard code that and that is

00:10:42.360 --> 00:10:48.650
Makes it fragile when you were making kind of really crazy random answers to what she said instead of reacting to those

00:10:48.650 --> 00:10:55.129
She just realized that wasn't what she was looking. She has no idea what HP brown sauce is so she can't talk about that

00:10:56.490 --> 00:11:00.589
Yes, so you you'd want to be able to use a machine learning system?

00:11:00.589 --> 00:11:06.769
so she's so she knows it's off topic and that we're not talking about that and then she can go back to what we

00:11:06.930 --> 00:11:09.469
Programmed her to to talk about we're basically

00:11:10.200 --> 00:11:12.150
handcrafting

00:11:12.150 --> 00:11:14.359
everything that it can say and

00:11:16.230 --> 00:11:18.769
Yeah, we want to keep that control quite quite tight

00:11:18.770 --> 00:11:24.380
Does that give you a reduced flexibility though or so you you putting this in very specific?

00:11:25.050 --> 00:11:27.289
Sort of silos. Yeah, so your your

00:11:28.950 --> 00:11:33.950
We're not creating virtual humans that can talk about anything and I think to be honest

00:11:34.350 --> 00:11:41.389
We're still quite a long way off from that and when I say we I'm talking about the big companies, etc

00:11:41.700 --> 00:11:43.700
so the problem with

00:11:45.450 --> 00:11:47.160
You know

00:11:47.160 --> 00:11:49.550
commercial products like Siri, Cortana

00:11:50.280 --> 00:11:55.759
Google assistant is that they don't they're not conversational agents. They are more like voice

00:11:56.310 --> 00:11:57.420
commands

00:11:57.420 --> 00:11:59.659
Interfaces and that works quite well

00:11:59.760 --> 00:12:03.380
But they can't you can't hold a general conversation with them either

00:12:03.380 --> 00:12:09.649
Right and I think doing that is still very much science fiction

00:12:09.990 --> 00:12:13.399
That's quite a long way away in the artificial intelligence

00:12:14.610 --> 00:12:16.230
roadmap

00:12:16.230 --> 00:12:20.990
The other thing that's quite different from what we're doing. Is that those voice?

00:12:21.540 --> 00:12:23.750
Assistants, they're all voice only

00:12:24.270 --> 00:12:26.270
their voice user interfaces

00:12:26.790 --> 00:12:30.740
What we're adding to that is your gestures your facial expressions

00:12:31.470 --> 00:12:35.329
Not just from the user because it's the first human can sense me

00:12:35.330 --> 00:12:40.190
But also she will use her own facial expressions in her own gestures

00:12:41.490 --> 00:12:46.099
To communicate with me and that also creates a sort of a point of contact

00:12:46.100 --> 00:12:49.909
so, you know, you will be facing her usually which

00:12:50.430 --> 00:12:52.430
also allows her to

00:12:52.950 --> 00:12:58.969
Distinguish whether I'm talking addressing her or my addressing someone else in the room, so that's a whole

00:12:59.610 --> 00:13:00.930
new

00:13:00.930 --> 00:13:08.299
area, I think that will seem much more about somewhere in this space when you put my face in I'll appear and when you put

00:13:08.300 --> 00:13:13.729
Steve's face in it'll appear somewhere else and this actually solves a really nice problem, right? It's called the one-shot learning problem

00:13:13.730 --> 00:13:17.779
How do we convince a phone to let me in having only seen one ever picture of my face?

00:13:17.780 --> 00:13:19.780
Which is when I first, you know

00:13:19.980 --> 00:13:23.209
Calibrated it the first time and the answer is we don't train a neural network

