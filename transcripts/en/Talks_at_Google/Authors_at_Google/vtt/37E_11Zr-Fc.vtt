WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.490
[MUSIC PLAYING]

00:00:05.490 --> 00:00:07.240
WOODROW HARTZOG: Thank
you all for coming.

00:00:07.240 --> 00:00:08.198
I really appreciate it.

00:00:08.198 --> 00:00:10.440
Thanks for the opportunity
to come speak here.

00:00:10.440 --> 00:00:14.910
Today, I'm going to be speaking
with you about my book,

00:00:14.910 --> 00:00:16.440
"Privacy's Blueprint--

00:00:16.440 --> 00:00:20.680
The Battle to Control the
Design of New Technologies."

00:00:20.680 --> 00:00:26.940
And I really want to start
off with a series of stories

00:00:26.940 --> 00:00:29.950
to tell you a little bit
about what the book is about.

00:00:29.950 --> 00:00:32.310
So many of you, if
not most of you,

00:00:32.310 --> 00:00:38.040
are familiar with the Cambridge
Analytica-Facebook dust

00:00:38.040 --> 00:00:39.740
up, as it were.

00:00:39.740 --> 00:00:42.060
And there was a
lot going on there

00:00:42.060 --> 00:00:46.371
where lots of information that
was taken from Facebook that

00:00:46.371 --> 00:00:48.870
eventually made its way towards
Cambridge Analytica and then

00:00:48.870 --> 00:00:53.670
outward, and it caused
a huge public discussion

00:00:53.670 --> 00:00:56.581
about privacy and platforms.

00:00:56.581 --> 00:00:58.580
And there was a lot going
on built in with that,

00:00:58.580 --> 00:01:00.360
but one of the most
interesting things

00:01:00.360 --> 00:01:04.585
was the fact that there was a
big debate about whether this

00:01:04.585 --> 00:01:06.210
was originally what
they referred to as

00:01:06.210 --> 00:01:08.670
a breach or something else.

00:01:08.670 --> 00:01:10.890
And one of the things
that sort of came up,

00:01:10.890 --> 00:01:13.470
and one of the things that
indeed the platform actually

00:01:13.470 --> 00:01:16.080
said is that
permission was actually

00:01:16.080 --> 00:01:19.290
granted through the
privacy settings

00:01:19.290 --> 00:01:22.860
to have this sort of information
given to third-party apps.

00:01:22.860 --> 00:01:25.784
And in fact it wasn't
just the one app

00:01:25.784 --> 00:01:27.450
that was collecting
lots of information.

00:01:27.450 --> 00:01:29.850
This was actually a
relatively common practice

00:01:29.850 --> 00:01:31.140
that people were agreeing to.

00:01:31.140 --> 00:01:34.350
But if you actually start to
dig in and think about, well,

00:01:34.350 --> 00:01:35.880
what was actually
agreed to and what

00:01:35.880 --> 00:01:37.860
did it look like, you
realized it was sort

00:01:37.860 --> 00:01:40.620
of one small little
option nested within lots

00:01:40.620 --> 00:01:43.050
of other small little
options that existed,

00:01:43.050 --> 00:01:46.120
and while you may technically
have agreed to it,

00:01:46.120 --> 00:01:49.640
it was sort of binary
and relatively hidden.

00:01:49.640 --> 00:01:51.870
And so it resulted
in a lot of confusion

00:01:51.870 --> 00:01:54.177
because the people that
seemed to be angry about it

00:01:54.177 --> 00:01:56.260
said, well, wait a second,
I didn't agree to that.

00:01:56.260 --> 00:01:58.343
And then, of course, they
were shown exactly where

00:01:58.343 --> 00:01:59.340
they did, and said, OK.

00:01:59.340 --> 00:02:01.747
Well, maybe that
wasn't the best thing.

00:02:01.747 --> 00:02:03.330
The second story I
want to talk about,

00:02:03.330 --> 00:02:09.030
this is a still taken from
an internet-connected baby

00:02:09.030 --> 00:02:10.080
monitor.

00:02:10.080 --> 00:02:12.060
And the most disturbing
thing about this--

00:02:12.060 --> 00:02:14.250
and this was disturbing
in and of itself--

00:02:14.250 --> 00:02:16.650
isn't that there's
this one still

00:02:16.650 --> 00:02:19.980
but rather there's an entire
search engine dedicated

00:02:19.980 --> 00:02:23.670
to finding these sorts of stills
because these sorts of devices

00:02:23.670 --> 00:02:29.610
are so routinely compromised
and so easy to take control

00:02:29.610 --> 00:02:32.440
of that there's actually an
entire collection of them

00:02:32.440 --> 00:02:36.510
online, which I thought was
also relatively disturbing.

00:02:36.510 --> 00:02:39.150
And then the third story
that I wanted to talk about

00:02:39.150 --> 00:02:42.360
was what many of you may have
been familiar with, which

00:02:42.360 --> 00:02:45.720
is the dispute between the
Federal Bureau of Investigation

00:02:45.720 --> 00:02:50.820
and Apple over whether to
build in some sort of bypass

00:02:50.820 --> 00:02:54.900
to its encryption scheme
and authentication protocol

00:02:54.900 --> 00:03:01.290
to allow it to access the phone
of the San Bernardino shooter.

00:03:01.290 --> 00:03:05.470
Now what do these three
things have in common?

00:03:05.470 --> 00:03:08.820
They all are stories
about the design

00:03:08.820 --> 00:03:12.300
of information technologies.

00:03:12.300 --> 00:03:17.900
And in the book, I focus
on how things are built

00:03:17.900 --> 00:03:20.097
and the way in which
they affect our privacy.

00:03:20.097 --> 00:03:21.680
And I want to make
three points today,

00:03:21.680 --> 00:03:24.140
and I try to make three
larger points in the book.

00:03:24.140 --> 00:03:26.210
One, design matters for privacy.

00:03:26.210 --> 00:03:27.530
And more than that--

00:03:27.530 --> 00:03:29.180
which may seem
relatively evident--

00:03:29.180 --> 00:03:31.310
we need to have a
better conversation

00:03:31.310 --> 00:03:34.996
to explore why design matters
and how design matters

00:03:34.996 --> 00:03:37.370
for our privacy because I
think it matters more than what

00:03:37.370 --> 00:03:40.057
most people regularly consider.

00:03:40.057 --> 00:03:42.140
Two, and this is the main
thesis of the book which

00:03:42.140 --> 00:03:44.990
is why it's in bold, which is
that privacy laws should take

00:03:44.990 --> 00:03:47.780
design more seriously,
and not just the law

00:03:47.780 --> 00:03:50.930
but also industry policy,
industry standards,

00:03:50.930 --> 00:03:52.250
industry norms.

00:03:52.250 --> 00:03:57.560
We focus a lot on personal data.

00:03:57.560 --> 00:03:58.920
What data's being collected?

00:03:58.920 --> 00:03:59.930
How is it being used?

00:03:59.930 --> 00:04:01.970
Who is it being shared with?

00:04:01.970 --> 00:04:04.100
We focus a lot
about transparency,

00:04:04.100 --> 00:04:07.280
but I argue that
actually there's a gap.

00:04:07.280 --> 00:04:11.600
There's a huge area within
which design affects

00:04:11.600 --> 00:04:14.750
a lot of what people consider
their personal privacy

00:04:14.750 --> 00:04:17.390
interests that we don't
regularly talk about

00:04:17.390 --> 00:04:20.610
in law and in industry policy.

00:04:20.610 --> 00:04:25.100
And three-- and this is perhaps
one of the more controversial

00:04:25.100 --> 00:04:26.150
aspects of the book--

00:04:26.150 --> 00:04:28.220
a design agenda
for law and policy

00:04:28.220 --> 00:04:31.220
should have roots in consumer
protection and surveillance

00:04:31.220 --> 00:04:36.350
law, not the standard
data-protection framework that

00:04:36.350 --> 00:04:39.920
is becoming incredibly
popular through frameworks

00:04:39.920 --> 00:04:44.000
like the GDPR, which everyone
is probably at least mildly

00:04:44.000 --> 00:04:45.980
aware of given the
thousands of emails

00:04:45.980 --> 00:04:48.770
that you received
somewhere around late April

00:04:48.770 --> 00:04:51.650
that says we've
updated our terms.

00:04:51.650 --> 00:04:53.780
And it's built around
this model that

00:04:53.780 --> 00:04:57.230
tends to prioritize certain
kinds of data collection

00:04:57.230 --> 00:05:00.770
and processing rules I think
at the expense of ignoring

00:05:00.770 --> 00:05:03.260
some nuances about the ways
in which design actually

00:05:03.260 --> 00:05:06.740
controls our privacy.

00:05:06.740 --> 00:05:09.430
So, let me expand upon
these three points.

00:05:09.430 --> 00:05:11.300
One, design matters for privacy.

00:05:11.300 --> 00:05:12.840
What do I mean by that?

00:05:12.840 --> 00:05:14.750
I mean that design
matters for privacy

00:05:14.750 --> 00:05:18.590
because it is everywhere,
because it is power,

00:05:18.590 --> 00:05:20.750
and because it is political.

00:05:20.750 --> 00:05:24.920
First, design is everywhere.

00:05:24.920 --> 00:05:26.990
Many people might
recognize what this is.

00:05:26.990 --> 00:05:29.912
Does anyone know what this is?

00:05:29.912 --> 00:05:32.990
Has anyone seen this before?

00:05:32.990 --> 00:05:36.640
It is a promo user interface,
early user interface

00:05:36.640 --> 00:05:38.770
for the app Snapchat.

00:05:38.770 --> 00:05:41.050
Now if you were to look
at this knowing nothing--

00:05:41.050 --> 00:05:43.660
sometimes I give
this talk and I speak

00:05:43.660 --> 00:05:46.120
with people that haven't
used technology that much

00:05:46.120 --> 00:05:49.120
or are only sort of casual
users of a lot of apps,

00:05:49.120 --> 00:05:51.050
and so they've never
seen this at all.

00:05:51.050 --> 00:05:53.380
And so I like to
sort of pause and I

00:05:53.380 --> 00:05:56.650
say if you had never seen
this before, what would

00:05:56.650 --> 00:06:03.660
you think this user interface
is designed to accomplish?

00:06:03.660 --> 00:06:07.730
And some of the usual
answers I get are, well, it's

00:06:07.730 --> 00:06:12.080
probably something that
relates to taking and sending

00:06:12.080 --> 00:06:14.090
of a picture.

00:06:14.090 --> 00:06:18.500
We can tell that exists
for-- we saw the timer here.

00:06:18.500 --> 00:06:22.370
Where does the timer probably
do, if we had to guess?

00:06:22.370 --> 00:06:26.189
Yeah, it makes the picture
disappear after maybe

00:06:26.189 --> 00:06:27.230
a certain amount of time.

00:06:27.230 --> 00:06:29.180
We see the scroll wheel
that we can select,

00:06:29.180 --> 00:06:31.250
so we clearly have options.

00:06:31.250 --> 00:06:33.890
It's clearly meant not just
for taking photos and storing

00:06:33.890 --> 00:06:36.410
them but for sharing them.

00:06:36.410 --> 00:06:40.175
We can tell because
we see a send button

00:06:40.175 --> 00:06:42.840
and it's got the little arrow.

00:06:42.840 --> 00:06:45.260
And then one of my favorite
things about this user

00:06:45.260 --> 00:06:49.825
interface and one of the most
demonstrative sort of signals

00:06:49.825 --> 00:06:51.950
that we see about the way
in which this is supposed

00:06:51.950 --> 00:06:56.210
to be used, what do you
think this particular app,

00:06:56.210 --> 00:06:59.540
just looking at this,
what kinds of pictures

00:06:59.540 --> 00:07:03.850
do you think the app is
inviting you to send?

00:07:03.850 --> 00:07:06.100
Naughty pictures, right?

00:07:06.100 --> 00:07:10.025
Originally this is how Snapchat
was sort of perceived publicly.

00:07:10.025 --> 00:07:11.650
And you can't blame
them because, look,

00:07:11.650 --> 00:07:15.797
we've got an incredibly
carefully cropped little square

00:07:15.797 --> 00:07:18.380
here where we can't tell whether
these young women are clothed

00:07:18.380 --> 00:07:18.879
or not.

00:07:18.879 --> 00:07:19.880
They're having fun.

00:07:19.880 --> 00:07:24.200
And the implicit message
built in through every aspect

00:07:24.200 --> 00:07:26.030
of design of this
user interfaces

00:07:26.030 --> 00:07:30.620
is that you can trust this app
with things that you might not

00:07:30.620 --> 00:07:33.930
trust in more-permanent
sorts of social media

00:07:33.930 --> 00:07:35.210
because it goes away.

00:07:35.210 --> 00:07:39.410
It is safer than if
you were to use, say,

00:07:39.410 --> 00:07:42.770
some other service where the
pictures are stored forever.

00:07:42.770 --> 00:07:47.210
And all of this is conveyed
with less than three words.

00:07:47.210 --> 00:07:48.170
It's all signals.

00:07:48.170 --> 00:07:50.330
It's all things
that we intuitively

00:07:50.330 --> 00:07:54.470
seem to understand through
the design of information

00:07:54.470 --> 00:07:55.320
technologies.

00:07:55.320 --> 00:07:57.740
And I started looking for
examples for this book,

00:07:57.740 --> 00:07:59.540
and once you start
to look for examples

00:07:59.540 --> 00:08:01.670
of the ways in which design
affects your privacy,

00:08:01.670 --> 00:08:05.840
you see them all over the place,
and not just in obvious ways

00:08:05.840 --> 00:08:07.040
but in subtle ways.

00:08:07.040 --> 00:08:09.080
So I don't have to
tell everybody here

00:08:09.080 --> 00:08:12.200
what this technology is,
and there were some design

00:08:12.200 --> 00:08:15.800
decisions that went
into Google Glass

00:08:15.800 --> 00:08:18.140
that I thought were
incredibly interesting.

00:08:18.140 --> 00:08:20.660
The most obvious
privacy-relevant feature

00:08:20.660 --> 00:08:23.944
for Google Glass is what?

00:08:23.944 --> 00:08:25.360
AUDIENCE: The
camera on the front.

00:08:25.360 --> 00:08:27.359
WOODROW HARTZOG: The
camera on the front, right?

00:08:27.359 --> 00:08:30.370
It's literally staring
straight at us.

00:08:30.370 --> 00:08:32.304
But that's not the
design feature--

00:08:32.304 --> 00:08:34.179
I mean, I guess you
could have designed Glass

00:08:34.179 --> 00:08:34.970
without the camera.

00:08:34.970 --> 00:08:38.470
That was a design decision that
implicated people's privacy

00:08:38.470 --> 00:08:40.270
because it's able
to be surveilled.

00:08:40.270 --> 00:08:42.880
But of course the
counterargument

00:08:42.880 --> 00:08:46.150
to that is this
camera doesn't really

00:08:46.150 --> 00:08:49.240
do a lot that
isn't already being

00:08:49.240 --> 00:08:52.780
done by another camera,
the one that everyone

00:08:52.780 --> 00:08:54.550
has in their phones.

00:08:54.550 --> 00:08:55.930
And so the design
decision that's

00:08:55.930 --> 00:09:00.460
relevant for Google Glass is
not just that there's a camera

00:09:00.460 --> 00:09:06.340
but that the camera is
always potentially visible.

00:09:06.340 --> 00:09:08.020
Now there's a small
transaction cost,

00:09:08.020 --> 00:09:10.000
but it's significant
at scale, which

00:09:10.000 --> 00:09:12.700
is in order to take a picture
with our normal phones

00:09:12.700 --> 00:09:16.360
we have to reach into our
phone, pull it out, open it up.

00:09:16.360 --> 00:09:17.710
We fiddle with it.

00:09:17.710 --> 00:09:20.020
We aim it, and then
we take the photo.

00:09:20.020 --> 00:09:23.080
Now that doesn't seem like
a lot on a per-time basis,

00:09:23.080 --> 00:09:27.370
but that small transaction
cost actually, I think,

00:09:27.370 --> 00:09:29.945
has a tendency to make
people feel more comfortable

00:09:29.945 --> 00:09:31.570
with the fact that
everyone is carrying

00:09:31.570 --> 00:09:35.710
around a sort of
persistent surveillance

00:09:35.710 --> 00:09:38.200
device in their
pockets all the time,

00:09:38.200 --> 00:09:41.380
whereas Glass was met with
much more resistance, right?

00:09:41.380 --> 00:09:44.890
People tended to be more
uncomfortable with it

00:09:44.890 --> 00:09:46.739
than they did with their phones.

00:09:46.739 --> 00:09:49.030
I think the reason why is
because the camera was always

00:09:49.030 --> 00:09:49.630
there.

00:09:49.630 --> 00:09:52.180
The transaction cost of
having to dig into your pocket

00:09:52.180 --> 00:09:54.490
was suddenly gone
and all you had to do

00:09:54.490 --> 00:09:59.626
was utter a command or sort
of readily activate it.

00:09:59.626 --> 00:10:01.000
There was another
design decision

00:10:01.000 --> 00:10:03.550
that I thought was really
relevant to Google Glass that

00:10:03.550 --> 00:10:07.120
didn't get a lot of attention,
but it's my understanding

00:10:07.120 --> 00:10:08.980
that Glass did not
provide support

00:10:08.980 --> 00:10:11.680
for facial-recognition
technologies, which

00:10:11.680 --> 00:10:15.670
was an interesting
design decision and one

00:10:15.670 --> 00:10:18.640
that I think also had privacy
implications, positive privacy

00:10:18.640 --> 00:10:19.480
implications.

00:10:19.480 --> 00:10:22.660
I don't have to tell anyone here
what this symbol is as well,

00:10:22.660 --> 00:10:25.360
but for those that
aren't familiar,

00:10:25.360 --> 00:10:29.440
this is the symbol
of Incognito mode.

00:10:29.440 --> 00:10:32.980
When you open up Google
Chrome's browser and you go

00:10:32.980 --> 00:10:34.990
into incognito mode you see--

00:10:34.990 --> 00:10:35.710
or at one point.

00:10:35.710 --> 00:10:37.820
I believe the icon
has now changed--

00:10:37.820 --> 00:10:40.950
but at one point
you saw this icon.

00:10:40.950 --> 00:10:42.640
And I always like
to ask people--

00:10:42.640 --> 00:10:44.320
this is it this is
a design, right?

00:10:44.320 --> 00:10:46.477
It's a signal to people.

00:10:46.477 --> 00:10:48.310
And if you were to just
see this, what would

00:10:48.310 --> 00:10:52.000
you think the signal was?

00:10:52.000 --> 00:10:54.220
What is this meant to convey?

00:10:54.220 --> 00:10:55.156
AUDIENCE: Concealment.

00:10:55.156 --> 00:10:56.280
WOODROW HARTZOG: I'm sorry?

00:10:56.280 --> 00:10:57.280
AUDIENCE: Concealment.

00:10:57.280 --> 00:10:58.889
WOODROW HARTZOG:
Concealment, right,

00:10:58.889 --> 00:11:00.180
which of course is the purpose.

00:11:00.180 --> 00:11:02.150
I mean, it's called
Incognito mode.

00:11:02.150 --> 00:11:05.620
And we've got, it seems, like a
sort of classic 1940s gumshoe,

00:11:05.620 --> 00:11:08.442
someone that doesn't
want to be seen.

00:11:08.442 --> 00:11:09.650
They can put their collar up.

00:11:09.650 --> 00:11:14.300
They've got a hat that they can
pull down low, maybe glasses.

00:11:14.300 --> 00:11:18.100
Again, the implicit message
just from looking at this

00:11:18.100 --> 00:11:21.370
is that you're safer
if you want to hide.

00:11:21.370 --> 00:11:24.400
Now of course that's not exactly
how Incognito mode works,

00:11:24.400 --> 00:11:25.150
right?

00:11:25.150 --> 00:11:29.110
And indeed when you
open up Incognito mode

00:11:29.110 --> 00:11:30.490
you've got a list of things.

00:11:30.490 --> 00:11:33.070
They say, by the way, here are
all the people that can still

00:11:33.070 --> 00:11:35.380
see you when using
Incognito mode, which

00:11:35.380 --> 00:11:36.880
is a very important disclosure.

00:11:36.880 --> 00:11:39.550
But absent that you
might think, oh, well,

00:11:39.550 --> 00:11:43.030
I'm using the safe browser,
the browser that doesn't record

00:11:43.030 --> 00:11:44.490
any history of me at all.

00:11:44.490 --> 00:11:45.866
It's completely incognito.

00:11:45.866 --> 00:11:47.990
And so without that, I
think, important disclosure,

00:11:47.990 --> 00:11:51.850
this design would tend to
shape people's expectations.

00:11:51.850 --> 00:11:53.800
And so when you
look around you tend

00:11:53.800 --> 00:11:55.810
to see these sort
of design decisions

00:11:55.810 --> 00:11:59.830
everywhere that tend to be
relevant for people's decisions

00:11:59.830 --> 00:12:02.510
about what to disclose,
how much to disclose,

00:12:02.510 --> 00:12:05.350
and how safe they
are in disclosing.

00:12:05.350 --> 00:12:07.780
Now the second point that
I want to make about design

00:12:07.780 --> 00:12:11.450
is that design is power.

00:12:11.450 --> 00:12:15.170
One of my favorite experiments
that I talk a little bit

00:12:15.170 --> 00:12:18.530
about in the book was done
by Leslie John, Alesandro

00:12:18.530 --> 00:12:22.550
Acquisti, and George Loewenstein
at Carnegie Mellon University.

00:12:22.550 --> 00:12:26.030
And they got several
people in to answer

00:12:26.030 --> 00:12:28.852
this series of questions
in a survey on a screen.

00:12:28.852 --> 00:12:30.560
And they sat down and
they were presented

00:12:30.560 --> 00:12:32.180
with a screen like
this, and they

00:12:32.180 --> 00:12:33.971
asked them a number of
different questions,

00:12:33.971 --> 00:12:35.330
and this is just a snapshot.

00:12:35.330 --> 00:12:38.300
And the questions ranged
from sort of banal

00:12:38.300 --> 00:12:41.300
to relatively intimate
to incredibly intimate.

00:12:41.300 --> 00:12:43.790
And this is just a short
little snapshot here.

00:12:43.790 --> 00:12:47.930
They said, have you
ever smoked marijuana?

00:12:47.930 --> 00:12:51.290
Have you ever cheated
while in a relationship?

00:12:51.290 --> 00:12:52.940
Have you ever
driven when you were

00:12:52.940 --> 00:12:56.630
pretty sure you were over the
legal blood alcohol level,

00:12:56.630 --> 00:12:59.730
asking you to admit
to committing a crime.

00:12:59.730 --> 00:13:02.600
Now what's the first
thing that you notice

00:13:02.600 --> 00:13:05.140
when I put this screen up?

00:13:05.140 --> 00:13:08.502
How BAD are U???

00:13:08.502 --> 00:13:10.960
And so there were a series of
design decisions that I think

00:13:10.960 --> 00:13:13.010
were particularly relevant here.

00:13:13.010 --> 00:13:17.140
One is the fact that BAD
is sort of capitalized.

00:13:17.140 --> 00:13:20.800
And then they use the sort of
cutesy U sort of text speak.

00:13:20.800 --> 00:13:22.600
How BAD are U???

00:13:22.600 --> 00:13:26.890
Multiple question marks,
which is indicative of what--

00:13:26.890 --> 00:13:29.800
youth, right, exuberance.

00:13:29.800 --> 00:13:33.485
What else do we have
relevant to design here?

00:13:33.485 --> 00:13:34.276
AUDIENCE: The logo.

00:13:34.276 --> 00:13:36.250
WOODROW HARTZOG:
The logo, right?

00:13:36.250 --> 00:13:39.180
It's a devil, but
it's not a bad devil.

00:13:39.180 --> 00:13:42.800
It's like an emoji
devil, like a cute devil.

00:13:42.800 --> 00:13:44.330
There's one other
design decision

00:13:44.330 --> 00:13:46.160
that I thought was
really interesting.

00:13:46.160 --> 00:13:46.880
AUDIENCE: Font.

00:13:46.880 --> 00:13:48.140
WOODROW HARTZOG: The font.

00:13:48.140 --> 00:13:50.919
It is written in
Comic Sans, and no one

00:13:50.919 --> 00:13:52.460
in the history of
the world has taken

00:13:52.460 --> 00:13:56.990
anything seriously written
in Comic Sans font, right?

00:13:56.990 --> 00:13:59.520
And the overall
implication, of course,

00:13:59.520 --> 00:14:01.130
of all of these
design decisions,

00:14:01.130 --> 00:14:05.300
despite asking incredibly
intimate questions, questions

00:14:05.300 --> 00:14:10.370
that can implicate you in a
crime, is that how bad are you?

00:14:10.370 --> 00:14:11.210
You're bad.

00:14:11.210 --> 00:14:12.124
You know you're bad.

00:14:12.124 --> 00:14:13.790
Everyone's just a
little bit bad, right?

00:14:13.790 --> 00:14:16.820
That's sort of the implication
of every single design

00:14:16.820 --> 00:14:18.867
feature built into this.

00:14:18.867 --> 00:14:20.450
And then, of course,
the control group

00:14:20.450 --> 00:14:23.900
they had looked like this.

00:14:23.900 --> 00:14:30.050
Gone is the Comic Sans font
replaced with this very formal

00:14:30.050 --> 00:14:33.370
sans serif font.

00:14:33.370 --> 00:14:36.430
Gone is our cute little
question about how BAD are U,

00:14:36.430 --> 00:14:38.470
and now it's just replaced
with the imprimatur

00:14:38.470 --> 00:14:43.150
of Carnegie Mellon University,
a very prestigious university.

00:14:43.150 --> 00:14:48.710
But the questions
remained exactly the same.

00:14:48.710 --> 00:14:50.800
And I want to read
directly because I

00:14:50.800 --> 00:14:54.440
don't want to misquote the
results of their study,

00:14:54.440 --> 00:14:59.770
but they call this the
frivolous-looking interface

00:14:59.770 --> 00:15:03.280
in this the
non-frivolous-looking

00:15:03.280 --> 00:15:04.000
interface.

00:15:04.000 --> 00:15:08.890
And they found that relative
to the non-frivolous interface,

00:15:08.890 --> 00:15:11.170
participants in the
frivolous-looking survey

00:15:11.170 --> 00:15:13.540
that asked identical
questions were,

00:15:13.540 --> 00:15:17.590
on average, 1.7 times more
likely to admit having

00:15:17.590 --> 00:15:19.910
engaged in risky behaviors.

00:15:19.910 --> 00:15:23.230
For example, a participant in
the frivolous-looking survey

00:15:23.230 --> 00:15:26.680
was, on average, 2.03
times more likely

00:15:26.680 --> 00:15:29.980
to admit having ever taken
nude pictures of himself

00:15:29.980 --> 00:15:30.877
or a partner.

00:15:30.877 --> 00:15:32.710
And the authors conclude,
"People, it seems,

00:15:32.710 --> 00:15:35.620
feel more comfortable
providing personal information

00:15:35.620 --> 00:15:39.940
on unprofessional sites that
are arguably particularly

00:15:39.940 --> 00:15:41.960
likely to misuse it."

00:15:41.960 --> 00:15:44.300
Design is power.

00:15:44.300 --> 00:15:48.000
Design doesn't necessarily
dictate behavior

00:15:48.000 --> 00:15:49.310
but it channels it.

00:15:49.310 --> 00:15:53.300
Every single design decision
makes a certain reality

00:15:53.300 --> 00:15:56.420
more or less likely.

00:15:56.420 --> 00:15:59.600
And so it provides an
incredible amount of power.

00:15:59.600 --> 00:16:02.570
And that leads me to
my third point, which

00:16:02.570 --> 00:16:05.270
is that design is political.

00:16:05.270 --> 00:16:06.970
Design is always political.

00:16:06.970 --> 00:16:09.230
And when I say political
I don't mean political

00:16:09.230 --> 00:16:10.730
in terms of Capitol Hill.

00:16:10.730 --> 00:16:14.600
I mean political in terms of
the distribution of power.

00:16:14.600 --> 00:16:18.050
So I give this talk a
little, and sometimes

00:16:18.050 --> 00:16:20.660
in response people will come
up afterwards and they'll say,

00:16:20.660 --> 00:16:23.750
why are you targeting
design as something

00:16:23.750 --> 00:16:26.510
that we should be
focused about in law?

00:16:26.510 --> 00:16:31.430
Instead of targeting the tools
that we use to create problems,

00:16:31.430 --> 00:16:34.260
why don't we target the
harmful problems themselves?

00:16:34.260 --> 00:16:36.380
One of the examples that's
been put forth recently

00:16:36.380 --> 00:16:38.960
is a knife can be
used for good or bad,

00:16:38.960 --> 00:16:40.962
but we don't ban knives.

00:16:40.962 --> 00:16:43.170
Instead we say you can't
use a knife to stab someone,

00:16:43.170 --> 00:16:46.730
but you can use it to cut food.

00:16:46.730 --> 00:16:49.810
And so it's always some
version of the argument

00:16:49.810 --> 00:16:53.030
that there are no bad
technologies, only bad uses

00:16:53.030 --> 00:16:54.650
of those technologies,
and that should

00:16:54.650 --> 00:16:58.920
be the focus of our
policy and law efforts.

00:16:58.920 --> 00:17:04.849
But I think that that actually
tends to cut against the fact

00:17:04.849 --> 00:17:07.040
that I don't think that
there's such a thing

00:17:07.040 --> 00:17:09.240
as a neutral technology.

00:17:09.240 --> 00:17:12.859
Given the fact that every
single design decision

00:17:12.859 --> 00:17:15.200
is meant to affect the world--

00:17:15.200 --> 00:17:16.430
that's what design is.

00:17:16.430 --> 00:17:20.089
It's a series of decisions
that get reflected

00:17:20.089 --> 00:17:21.920
in some sort of
substance that affects

00:17:21.920 --> 00:17:24.359
the world in some
particular way.

00:17:24.359 --> 00:17:26.720
Then there's no such thing
as neutral technologies.

00:17:26.720 --> 00:17:29.240
Even ignoring
certain realities is

00:17:29.240 --> 00:17:32.690
an acceptance of those
realities in a certain sense.

00:17:32.690 --> 00:17:36.020
And so we cannot ignore
technology and the role

00:17:36.020 --> 00:17:38.610
of design in law and policy.

00:17:38.610 --> 00:17:40.650
We have to confront it head on.

00:17:40.650 --> 00:17:44.720
And there may be reasons
why we explicitly

00:17:44.720 --> 00:17:48.140
decide not to have rules
against legislating technology.

00:17:48.140 --> 00:17:51.170
We don't want regulators
for example dictating

00:17:51.170 --> 00:17:53.960
from top to bottom every
single design decision

00:17:53.960 --> 00:17:59.300
in a ham-fisted way because it's
difficult to think that they

00:17:59.300 --> 00:18:02.150
would know better how to build
a lot of particular technologies

00:18:02.150 --> 00:18:04.430
than people that live it.

00:18:04.430 --> 00:18:06.980
But that too has its
own costs, and we

00:18:06.980 --> 00:18:10.400
need to be specific about
which costs we want to embrace

00:18:10.400 --> 00:18:14.059
and how to better balance them
with the full range of concerns

00:18:14.059 --> 00:18:14.600
that we have.

00:18:14.600 --> 00:18:16.940
And the only way that
we're going to do that

00:18:16.940 --> 00:18:21.710
is if we explicitly embrace
the design of technology

00:18:21.710 --> 00:18:26.570
and how that is powerful
in our laws and policies.

00:18:26.570 --> 00:18:29.580
So that leads me
to my next point,

00:18:29.580 --> 00:18:34.070
which is that privacy law should
take design more seriously

00:18:34.070 --> 00:18:36.410
because right now it doesn't.

00:18:36.410 --> 00:18:39.590
It may seem like it
relatively does and there

00:18:39.590 --> 00:18:42.740
may be some in this room
or listening in online

00:18:42.740 --> 00:18:45.950
that have had experience
dealing with regulators

00:18:45.950 --> 00:18:50.420
and with the European Union
and the General Data Protection

00:18:50.420 --> 00:18:53.690
Regulation that deal with
privacy by design and privacy

00:18:53.690 --> 00:18:58.640
by default, but actually I
will argue that, largely,

00:18:58.640 --> 00:19:01.820
privacy law and privacy policy
and the rules that we have even

00:19:01.820 --> 00:19:07.490
internally about privacy
have a major design gap.

00:19:07.490 --> 00:19:11.030
So I teach privacy law at
Northeastern University.

00:19:11.030 --> 00:19:12.980
And it's a whole
semester-long course,

00:19:12.980 --> 00:19:16.070
but it turns out there are
basically only three privacy

00:19:16.070 --> 00:19:18.574
rules that you have to follow.

00:19:18.574 --> 00:19:20.240
And so I can save you
all a lot of money

00:19:20.240 --> 00:19:21.980
if you're interested in
taking a privacy-law course.

00:19:21.980 --> 00:19:23.690
I'm just going to
summarize it now.

00:19:23.690 --> 00:19:29.010
One of the rules is follow the
Fair Information Practices.

00:19:29.010 --> 00:19:34.070
This is the underlying ethos
of the entire General Data

00:19:34.070 --> 00:19:36.890
Protection Regulation.

00:19:36.890 --> 00:19:41.450
It's the idea behind statutes
that you are probably

00:19:41.450 --> 00:19:45.370
very familiar with like the
Fair Credit Reporting Act,

00:19:45.370 --> 00:19:49.580
or even HIPAA has a sense of
follow the Fair Information

00:19:49.580 --> 00:19:50.210
Practices.

00:19:50.210 --> 00:19:52.280
And the Fair
Information Practices

00:19:52.280 --> 00:19:54.320
are things that
are so fundamental

00:19:54.320 --> 00:19:57.200
to our understanding of
fairness with data processing

00:19:57.200 --> 00:19:59.634
that you probably have
already internalized them

00:19:59.634 --> 00:20:00.800
without thinking about them.

00:20:00.800 --> 00:20:06.380
For example, people should have
control over their information.

00:20:06.380 --> 00:20:09.710
Databases should be transparent
about what they're collecting.

00:20:09.710 --> 00:20:12.290
They should give people notice
about what they're collecting

00:20:12.290 --> 00:20:14.720
and the ability to delete
information or correct

00:20:14.720 --> 00:20:15.700
information.

00:20:15.700 --> 00:20:17.541
You should only
collect what you need,

00:20:17.541 --> 00:20:19.040
and you should
delete it when you're

00:20:19.040 --> 00:20:21.810
done with it, the
data-minimization principle.

00:20:21.810 --> 00:20:23.510
There's the principle
of accountability,

00:20:23.510 --> 00:20:25.426
that we should be
accountable for these rules.

00:20:25.426 --> 00:20:28.790
There's the
principle that people

00:20:28.790 --> 00:20:31.880
should have certain sort of
access rights or portability

00:20:31.880 --> 00:20:32.777
rights to their data.

00:20:32.777 --> 00:20:34.610
These are all things
that we've been talking

00:20:34.610 --> 00:20:36.530
about for a long
time, and they all

00:20:36.530 --> 00:20:39.830
started with the Fair
Information Practices, which

00:20:39.830 --> 00:20:43.280
actually originated in
the mid and late 1970s

00:20:43.280 --> 00:20:46.330
and then made their
way through and now are

00:20:46.330 --> 00:20:51.730
as close as the world has
come to a common language

00:20:51.730 --> 00:20:55.270
of privacy, which is pretty
remarkable when you consider

00:20:55.270 --> 00:20:58.840
all the varying different
cultural connotations that play

00:20:58.840 --> 00:21:00.970
into conceptions of privacy.

00:21:00.970 --> 00:21:03.520
The fact that we
have a basic set

00:21:03.520 --> 00:21:06.490
of rules on how to fairly
collect and process data

00:21:06.490 --> 00:21:07.550
is remarkable.

00:21:07.550 --> 00:21:09.670
And the FIPs are important.

00:21:09.670 --> 00:21:11.030
They're necessary.

00:21:11.030 --> 00:21:15.770
But here's the
problem with the FIPs.

00:21:15.770 --> 00:21:17.510
They look like this.

00:21:17.510 --> 00:21:19.400
This is the way
in which they get

00:21:19.400 --> 00:21:21.470
reflected because if
you look at the FIPs,

00:21:21.470 --> 00:21:27.410
it almost always boils down
to this idea of control.

00:21:27.410 --> 00:21:30.560
If you ask the CEOs
of every major tech

00:21:30.560 --> 00:21:33.140
company in the United
States and the world

00:21:33.140 --> 00:21:36.050
how do you respect the
privacy of your users,

00:21:36.050 --> 00:21:40.400
I'd bet money that they
would almost all respond

00:21:40.400 --> 00:21:44.060
with some version of we
put people in control

00:21:44.060 --> 00:21:46.880
of their own information.

00:21:46.880 --> 00:21:49.970
This is one of the most dominant
conceptualization of privacy

00:21:49.970 --> 00:21:52.370
in the United States
and around the world,

00:21:52.370 --> 00:21:54.344
the idea being that
if we give users

00:21:54.344 --> 00:21:55.760
control of their
information, then

00:21:55.760 --> 00:21:57.980
they get to decide
for themselves

00:21:57.980 --> 00:22:00.210
what they want to do
with their information

00:22:00.210 --> 00:22:01.550
and what they're OK with.

00:22:01.550 --> 00:22:03.590
They do a risk calculation.

00:22:03.590 --> 00:22:06.230
And if they are given
that control, then

00:22:06.230 --> 00:22:08.480
if they exercise
it then presumably

00:22:08.480 --> 00:22:12.500
the giving of that control
is enough to justify

00:22:12.500 --> 00:22:15.050
certain sorts of data practices
because people approve it.

00:22:15.050 --> 00:22:16.520
They have control over it.

00:22:16.520 --> 00:22:21.440
Data subjects' autonomy
in theory is respected.

00:22:21.440 --> 00:22:25.760
The platform
collecting information

00:22:25.760 --> 00:22:28.820
has done its moral duty
under this conceptualization

00:22:28.820 --> 00:22:32.606
by offering the control, and
we can all proceed together.

00:22:32.606 --> 00:22:33.980
And here's the
problem with that,

00:22:33.980 --> 00:22:36.470
and here's the problem
of thinking privacy

00:22:36.470 --> 00:22:39.320
in terms of control generally,
which is really popular

00:22:39.320 --> 00:22:41.150
not just in industry.

00:22:41.150 --> 00:22:44.060
Governments conceptualize
privacy as control.

00:22:44.060 --> 00:22:46.310
Advocates conceptualize
privacy as control.

00:22:46.310 --> 00:22:53.030
It is by far, probably, the most
popular way to define privacy.

00:22:53.030 --> 00:22:55.340
I teach privacy law,
and on the first day

00:22:55.340 --> 00:22:59.570
I ask my students to
write out what privacy

00:22:59.570 --> 00:23:00.979
is and say what is privacy?

00:23:00.979 --> 00:23:02.270
And then we go around the room.

00:23:02.270 --> 00:23:06.740
I have 20 to 30 students,
and I usually get around

00:23:06.740 --> 00:23:10.220
5 to 10 different
answers, but almost always

00:23:10.220 --> 00:23:12.500
the most popular
response is control

00:23:12.500 --> 00:23:14.810
over personal information.

00:23:14.810 --> 00:23:19.740
But again, the way that control
is reflected in practice

00:23:19.740 --> 00:23:21.270
is through design.

00:23:21.270 --> 00:23:25.410
It is through buttons,
toggle switches,

00:23:25.410 --> 00:23:27.990
which in theory is still fine.

00:23:27.990 --> 00:23:29.700
Everyone knows what this is.

00:23:29.700 --> 00:23:31.350
Green means yes.

00:23:31.350 --> 00:23:32.910
Gray means no.

00:23:32.910 --> 00:23:34.230
It's a toggle switch.

00:23:34.230 --> 00:23:39.060
Can Google Maps collect your
geolocation information?

00:23:39.060 --> 00:23:40.020
Green means yes.

00:23:40.020 --> 00:23:40.770
Gray means no.

00:23:40.770 --> 00:23:41.520
Yes.

00:23:41.520 --> 00:23:43.230
I have exercised control.

00:23:43.230 --> 00:23:44.430
I have made a decision.

00:23:44.430 --> 00:23:45.780
Privacy is respected.

00:23:45.780 --> 00:23:47.460
Onward we go.

00:23:47.460 --> 00:23:49.590
Except, of course,
it doesn't always

00:23:49.590 --> 00:23:52.740
work out like that because it
never looks just like that,

00:23:52.740 --> 00:23:55.260
does it?

00:23:55.260 --> 00:23:59.700
we have a series of decisions
to make because we don't want

00:23:59.700 --> 00:24:02.340
the decision to be just binary.

00:24:02.340 --> 00:24:04.980
If it's too binary,
it's too abstracted.

00:24:04.980 --> 00:24:06.240
Privacy is complicated.

00:24:06.240 --> 00:24:08.070
There are nuances.

00:24:08.070 --> 00:24:10.390
Yes/no doesn't give
enough choices.

00:24:10.390 --> 00:24:12.300
So we say, OK, I
need lots of choices.

00:24:12.300 --> 00:24:14.010
I want nuance.

00:24:14.010 --> 00:24:17.160
This is the inherent
tension in defining privacy

00:24:17.160 --> 00:24:19.650
as control is give me nuance.

00:24:19.650 --> 00:24:22.830
So tech companies respond and
they say, OK, we'll give you

00:24:22.830 --> 00:24:24.200
lots of different choices.

00:24:24.200 --> 00:24:26.399
Rather than just sort
of one on/off binary

00:24:26.399 --> 00:24:27.690
we'll give you lots of choices.

00:24:27.690 --> 00:24:31.410
Except, of course, that
means that users are now

00:24:31.410 --> 00:24:33.780
responsible for
analyzing those choices.

00:24:33.780 --> 00:24:37.540
And you say, OK, I've got
the Find My iPad button

00:24:37.540 --> 00:24:38.887
and location-based alerts.

00:24:38.887 --> 00:24:39.720
I guess that's good.

00:24:39.720 --> 00:24:42.240
Location-based iAds, I
don't know what that means.

00:24:42.240 --> 00:24:44.520
Share my location,
green, OK, I guess.

00:24:44.520 --> 00:24:46.290
I guess I'll try to
figure that one out.

00:24:46.290 --> 00:24:48.810
Wi-Fi network, I don't
know what that button does,

00:24:48.810 --> 00:24:50.700
but I guess it's OK.

00:24:50.700 --> 00:24:54.390
Diagnostics and usage, popular
near me, I guess that's good.

00:24:54.390 --> 00:24:56.640
And we have to make all of
these sort of implicit risk

00:24:56.640 --> 00:24:58.840
calculations over
and over again,

00:24:58.840 --> 00:25:02.400
which is costly in terms
of personal resources

00:25:02.400 --> 00:25:05.190
because people have
limited amounts of time.

00:25:05.190 --> 00:25:06.760
And so we go through--

00:25:06.760 --> 00:25:09.180
OK, so let's say
we look at this.

00:25:09.180 --> 00:25:10.890
Design is set up
in this way for us

00:25:10.890 --> 00:25:14.080
to effectuate the
control that we're given.

00:25:14.080 --> 00:25:17.999
We do it and we say,
phew, I think I'm good.

00:25:17.999 --> 00:25:20.040
I think I've set my settings
the way I want them.

00:25:20.040 --> 00:25:23.280
And then we take a deep
breath and we back up

00:25:23.280 --> 00:25:25.560
and we say, oh dear.

00:25:25.560 --> 00:25:29.292
I've got lots of buttons
to press over and over.

00:25:29.292 --> 00:25:32.520
And when the settings change,
I've got to update them again.

00:25:32.520 --> 00:25:36.960
The problem with thinking about
privacy in terms of control

00:25:36.960 --> 00:25:39.300
is that people are gifted
with so much control

00:25:39.300 --> 00:25:40.900
that they choke on it.

00:25:40.900 --> 00:25:44.820
And it becomes, in practice,
a way for risk of loss

00:25:44.820 --> 00:25:49.200
to be shifted on to users
because if you fail to exercise

00:25:49.200 --> 00:25:53.450
that control, it's not the
problem of the company that

00:25:53.450 --> 00:25:54.779
gave you the control, right?

00:25:54.779 --> 00:25:56.070
They're giving you the options.

00:25:56.070 --> 00:25:57.320
That's what you wanted.

00:25:57.320 --> 00:26:01.100
And so there's an inherent
tension that almost is never

00:26:01.100 --> 00:26:05.390
going to be resolved which
is if you abstract it away

00:26:05.390 --> 00:26:07.220
and make control
easy, if you provide

00:26:07.220 --> 00:26:10.730
one button for
all of our issues,

00:26:10.730 --> 00:26:13.490
then it becomes too generalized.

00:26:13.490 --> 00:26:15.210
There is too much
stuff washed in,

00:26:15.210 --> 00:26:18.980
and so our ability to do
a meaningful risk calculus

00:26:18.980 --> 00:26:20.670
is washed away.

00:26:20.670 --> 00:26:23.420
But if you provide every
option under the sun,

00:26:23.420 --> 00:26:25.450
then it becomes over burdensome.

00:26:25.450 --> 00:26:29.270
It's a DDoS attack on our
brain, and there's no way

00:26:29.270 --> 00:26:31.350
that we can respond at scale.

00:26:31.350 --> 00:26:36.320
And so control, I think,
is actually fundamentally

00:26:36.320 --> 00:26:39.710
one of the misguided,
actually, ways

00:26:39.710 --> 00:26:42.035
to think about privacy in
the modern data ecosystem

00:26:42.035 --> 00:26:43.410
because it's never
going to work.

00:26:43.410 --> 00:26:47.060
It just doesn't scale unless
we start having a conversation

00:26:47.060 --> 00:26:50.120
about prioritizing control which
gets really interesting, which

00:26:50.120 --> 00:26:52.560
we can talk about later.

00:26:52.560 --> 00:26:56.120
So there's this massive
design gap in follow the FIPs,

00:26:56.120 --> 00:26:58.760
this ethic that we
follow in all privacy law

00:26:58.760 --> 00:27:00.470
and in all industry
sort of rules, which

00:27:00.470 --> 00:27:03.380
is you should give people
control, because design is

00:27:03.380 --> 00:27:07.700
leveraged to, instead of
being autonomy enhancing,

00:27:07.700 --> 00:27:10.331
it actually is autonomy
corrosive because we've

00:27:10.331 --> 00:27:11.330
given all these choices.

00:27:11.330 --> 00:27:13.010
We're sort of burdened
in them, or it's

00:27:13.010 --> 00:27:16.310
abstracted away to the point
where it's meaningless.

00:27:16.310 --> 00:27:17.630
That's rule one.

00:27:17.630 --> 00:27:20.120
Rule two is do not lie.

00:27:20.120 --> 00:27:22.352
Relatively simple rule, right?

00:27:22.352 --> 00:27:23.810
The Federal Trade
Commission, which

00:27:23.810 --> 00:27:27.500
is the nation's top
regulator of privacy,

00:27:27.500 --> 00:27:32.420
has authority to regulate unfair
and deceptive trade practices.

00:27:32.420 --> 00:27:35.360
And as part of its regulation
it has one major rule

00:27:35.360 --> 00:27:37.310
which is don't lie to people.

00:27:37.310 --> 00:27:38.900
That sounds relatively easy.

00:27:38.900 --> 00:27:42.240
The problem with the
do-not-lie ethos, of course,

00:27:42.240 --> 00:27:46.070
is one of the similar problems
to transparency and control

00:27:46.070 --> 00:27:49.100
generally which is you can put
technical truths in a place

00:27:49.100 --> 00:27:51.084
that nobody's
going to read them.

00:27:51.084 --> 00:27:53.000
And we all know where
you put the things where

00:27:53.000 --> 00:27:55.130
you want nobody to read it.

00:27:55.130 --> 00:27:57.430
Where do we put it?

00:27:57.430 --> 00:28:00.640
If I want to be sure to
put something in a place

00:28:00.640 --> 00:28:02.680
where I know no user is
ever going to see it,

00:28:02.680 --> 00:28:03.670
where am I going to stick it?

00:28:03.670 --> 00:28:04.878
AUDIENCE: The privacy policy.

00:28:04.878 --> 00:28:07.660
WOODROW HARTZOG: The privacy
policy or the terms of use.

00:28:07.660 --> 00:28:09.160
I do this for a living.

00:28:09.160 --> 00:28:15.800
I am a privacy law professor,
and I cannot scroll through

00:28:15.800 --> 00:28:20.080
the terms of use quick
enough to get to I Agree

00:28:20.080 --> 00:28:22.690
because of course there's
no way that, at scale,

00:28:22.690 --> 00:28:25.090
anyone could
collectively read them.

00:28:25.090 --> 00:28:27.670
We'd have to take two days
off of vacation a year just

00:28:27.670 --> 00:28:30.190
to read all the privacy
policies we come into.

00:28:30.190 --> 00:28:34.180
And so do not lie can also get
circumvented through design

00:28:34.180 --> 00:28:36.190
through putting it
in a place where

00:28:36.190 --> 00:28:38.870
technical truth is ignored.

00:28:38.870 --> 00:28:41.290
And then there's the
do not harm ethic.

00:28:41.290 --> 00:28:42.650
That's the last one.

00:28:42.650 --> 00:28:44.170
So the Federal Trade
Commission also

00:28:44.170 --> 00:28:49.330
says don't engage in
unfair trade practices that

00:28:49.330 --> 00:28:52.894
harm users in a way that's
not avoidable by the users

00:28:52.894 --> 00:28:54.310
themselves and in
a way that's not

00:28:54.310 --> 00:28:57.110
outbalanced by countervailing
benefits to the consumer.

00:28:57.110 --> 00:28:59.140
That's the sort
of test they use.

00:28:59.140 --> 00:29:00.970
But of course the
do-not-harm ethic

00:29:00.970 --> 00:29:04.120
is also easily
subverted through design

00:29:04.120 --> 00:29:07.540
because in the modern
age the threshold

00:29:07.540 --> 00:29:12.610
for harm to rise to a legal
violation is sort of high.

00:29:12.610 --> 00:29:16.550
If you're going to bring a
lawsuit against a company,

00:29:16.550 --> 00:29:19.270
then you don't want
the harm to be this

00:29:19.270 --> 00:29:22.270
is creepy, which is some
of the things that happen.

00:29:22.270 --> 00:29:25.990
And creepy is not a real word
that has boundaries around it.

00:29:25.990 --> 00:29:26.740
What is creepy?

00:29:26.740 --> 00:29:29.860
Lots of things get
possibly creepy,

00:29:29.860 --> 00:29:33.260
but that varies wildly
across the spectrum.

00:29:33.260 --> 00:29:35.950
And so the law, and
in many ways rightly,

00:29:35.950 --> 00:29:38.530
actually says you've got to
have a real tangible harm here.

00:29:38.530 --> 00:29:40.680
You've got to have some
sort of financial harm.

00:29:40.680 --> 00:29:42.340
Someone stole your identity.

00:29:42.340 --> 00:29:43.870
Someone stole your money.

00:29:43.870 --> 00:29:46.930
Or you have to have some
sort of clear emotional harm,

00:29:46.930 --> 00:29:50.740
and often it is your naked
body was exposed publicly

00:29:50.740 --> 00:29:53.320
or something very visceral
and something that

00:29:53.320 --> 00:29:55.280
has a clear boundary to it.

00:29:55.280 --> 00:29:57.520
But, of course,
many of the sorts

00:29:57.520 --> 00:29:59.710
of things that people
quantify as privacy

00:29:59.710 --> 00:30:03.490
harms in the modern
age is not actually

00:30:03.490 --> 00:30:06.580
those sorts of visceral harms,
which can be relatively rare,

00:30:06.580 --> 00:30:08.050
but rather it's
a little bit more

00:30:08.050 --> 00:30:10.300
of a death by a thousand cuts.

00:30:10.300 --> 00:30:12.190
We reveal a little bit
of information here

00:30:12.190 --> 00:30:15.730
and we trickle a little bit of
information there, none of it

00:30:15.730 --> 00:30:17.650
which rises to the
level of what we would

00:30:17.650 --> 00:30:19.600
consider to be a privacy harm.

00:30:19.600 --> 00:30:22.300
But collectively
we look up one day

00:30:22.300 --> 00:30:24.310
and we all become vulnerable.

00:30:24.310 --> 00:30:27.730
And we all see that we've gone
down this road without actually

00:30:27.730 --> 00:30:29.800
having any one
particular violation,

00:30:29.800 --> 00:30:32.320
yet here we are with
lots of our information

00:30:32.320 --> 00:30:34.600
sort of exposed that
could ultimately

00:30:34.600 --> 00:30:36.220
be leveraged against us.

00:30:36.220 --> 00:30:38.020
And so design also
sort of does that

00:30:38.020 --> 00:30:41.020
through the way in
which we encourage

00:30:41.020 --> 00:30:43.419
short little amounts
of information

00:30:43.419 --> 00:30:44.710
to be disclosed here and there.

00:30:44.710 --> 00:30:45.940
Oh, it's just a little
information here.

00:30:45.940 --> 00:30:47.900
It's just a little
information there.

00:30:47.900 --> 00:30:51.370
But collectively it
becomes a big issue.

00:30:51.370 --> 00:30:54.089
So at this point you may be
saying, all right smart guy.

00:30:54.089 --> 00:30:55.630
Well, if we've got
the problems, what

00:30:55.630 --> 00:30:56.540
are we going to do about it?

00:30:56.540 --> 00:30:57.956
And that actually
is the next part

00:30:57.956 --> 00:31:00.850
of the book which is where I
propose a theory of privacy law

00:31:00.850 --> 00:31:01.930
and design.

00:31:01.930 --> 00:31:06.040
And the theory is actually
built around values, boundaries,

00:31:06.040 --> 00:31:07.330
and tools.

00:31:07.330 --> 00:31:12.640
And the values that I advocate
for are actually not control.

00:31:12.640 --> 00:31:14.390
Even though that's the
dominant framework,

00:31:14.390 --> 00:31:17.660
I think there are better
values that we can embrace

00:31:17.660 --> 00:31:22.190
not only that will give
US industry and US law

00:31:22.190 --> 00:31:24.950
a much clearer
identity with respect

00:31:24.950 --> 00:31:27.320
to protecting
personal information

00:31:27.320 --> 00:31:29.390
but also one that doesn't
sort of inherently

00:31:29.390 --> 00:31:31.490
have this tension where
you have to choose

00:31:31.490 --> 00:31:33.920
between a meaningless
abstraction

00:31:33.920 --> 00:31:36.590
or overwhelming
nuance, and that's

00:31:36.590 --> 00:31:38.580
trust, obscurity, and autonomy.

00:31:38.580 --> 00:31:43.730
In other words, I argue that our
law and policy should encourage

00:31:43.730 --> 00:31:48.110
design that is trustworthy,
that promotes or at least values

00:31:48.110 --> 00:31:50.540
obscurity, and
enhances autonomy.

00:31:50.540 --> 00:31:52.690
Now what do I mean by that?

00:31:52.690 --> 00:31:56.300
Trust, I think, is one of
the most important values

00:31:56.300 --> 00:31:59.030
in the modern age with
respect to the disclosure

00:31:59.030 --> 00:32:00.860
of personal information
in platforms.

00:32:00.860 --> 00:32:03.020
Trust is key for commerce.

00:32:03.020 --> 00:32:06.590
Trust is key for personal
relationships and intimacy.

00:32:06.590 --> 00:32:09.050
Trust is key for
self-exploration.

00:32:09.050 --> 00:32:12.046
We tend not to disclose
to other people

00:32:12.046 --> 00:32:13.670
in an attempt to sort
of figure out who

00:32:13.670 --> 00:32:15.590
we are unless we trust those.

00:32:15.590 --> 00:32:17.780
If everything we say
can be used against us,

00:32:17.780 --> 00:32:21.530
then we tend not to sort of
engage in self-exploration.

00:32:21.530 --> 00:32:23.390
And so our design
should reflect that.

00:32:23.390 --> 00:32:28.100
Our design should reflect
a sense of discretion--

00:32:28.100 --> 00:32:30.650
not necessarily confidentiality,
but just discretion.

00:32:30.650 --> 00:32:32.870
We don't disclose
everything all the time.

00:32:32.870 --> 00:32:34.700
This can be deidentification.

00:32:34.700 --> 00:32:37.720
This can be disclosure
within a limited community.

00:32:37.720 --> 00:32:40.820
It should encourage a
sense of protection.

00:32:40.820 --> 00:32:42.260
Now this is a
little more obvious,

00:32:42.260 --> 00:32:46.520
right, which is don't store
people's passwords in cleartext

00:32:46.520 --> 00:32:49.426
and salt and hash and
let's encrypt traffic.

00:32:49.426 --> 00:32:50.550
We should protect the data.

00:32:50.550 --> 00:32:53.330
This is what we will refer
to maybe as data security.

00:32:53.330 --> 00:32:55.040
A sense of honesty--

00:32:55.040 --> 00:32:58.370
and honesty is different
from transparency.

00:32:58.370 --> 00:33:01.290
Transparency often
gets touted as, look,

00:33:01.290 --> 00:33:03.980
we've opened up
the books to you.

00:33:03.980 --> 00:33:07.227
You can look inside and you
can see whatever is available,

00:33:07.227 --> 00:33:09.560
but that's actually different
than what I would consider

00:33:09.560 --> 00:33:11.780
to be acting honestly.

00:33:11.780 --> 00:33:15.230
Honestly is affirmatively
disclosing the things

00:33:15.230 --> 00:33:17.420
that users want to
know that maybe you

00:33:17.420 --> 00:33:21.327
would prefer not to tell them
but they should probably know.

00:33:21.327 --> 00:33:22.910
In other words, it's
a little bit more

00:33:22.910 --> 00:33:26.450
of affirmative
obligation as a warning

00:33:26.450 --> 00:33:27.980
rather than full transparency.

00:33:27.980 --> 00:33:31.790
And then finally-- and this
is the hard one with respect

00:33:31.790 --> 00:33:33.770
to trust--

00:33:33.770 --> 00:33:36.136
design should be
loyal to the user.

00:33:36.136 --> 00:33:37.760
And what I mean by
that is it shouldn't

00:33:37.760 --> 00:33:42.500
elevate the interests of
the platform unreasonably

00:33:42.500 --> 00:33:46.015
or in unreasonable ways over the
interests of the data subject,

00:33:46.015 --> 00:33:49.400
and we can talk about what
I mean by that in a minute.

00:33:49.400 --> 00:33:51.230
And that would be
trustworthy design,

00:33:51.230 --> 00:33:56.880
design that is discrete,
protective, honest, and loyal.

00:33:56.880 --> 00:34:01.530
Now another sort
of design choice

00:34:01.530 --> 00:34:06.150
which isn't quite as
established in law and policy

00:34:06.150 --> 00:34:08.909
is the value of obscurity.

00:34:08.909 --> 00:34:11.340
Obscurity is the idea
that when information

00:34:11.340 --> 00:34:15.150
is hard or unlikely to be
found or understood then

00:34:15.150 --> 00:34:18.480
it is, to a relative
degree, safe.

00:34:18.480 --> 00:34:22.260
We rely upon zones of
obscurity all the time

00:34:22.260 --> 00:34:24.810
in our everyday lives, and
we don't even realize it.

00:34:24.810 --> 00:34:27.989
How many people
here have eaten out

00:34:27.989 --> 00:34:32.780
at a restaurant within
the last two weeks?

00:34:32.780 --> 00:34:33.969
A number of people.

00:34:33.969 --> 00:34:38.020
Do any of you remember who
was sitting two tables away

00:34:38.020 --> 00:34:39.679
from you?

00:34:39.679 --> 00:34:40.760
Probably not.

00:34:40.760 --> 00:34:41.699
You were in public.

00:34:41.699 --> 00:34:44.929
Everyone could see you, but
of course we've long since

00:34:44.929 --> 00:34:46.370
deleted that information.

00:34:46.370 --> 00:34:48.506
I flew here yesterday
on an airplane.

00:34:48.506 --> 00:34:50.630
I don't remember what the
person sitting next to me

00:34:50.630 --> 00:34:53.000
looked like even though
it was in public,

00:34:53.000 --> 00:34:56.659
and we do this to help
sort of prevent cognitive

00:34:56.659 --> 00:34:57.910
overburdening.

00:34:57.910 --> 00:35:00.140
And we rely upon
this risk calculus

00:35:00.140 --> 00:35:02.630
all the time to make decisions.

00:35:02.630 --> 00:35:08.480
When I'm walking out in
public, if you quickly

00:35:08.480 --> 00:35:10.730
want to sort of scratch
in a delicate place

00:35:10.730 --> 00:35:12.500
and you look around
and no one's looking,

00:35:12.500 --> 00:35:18.080
you do it even though maybe if
that were posted on the Times

00:35:18.080 --> 00:35:20.970
Square jumbotron then you would
have second thoughts about it,

00:35:20.970 --> 00:35:23.480
but the odds of that
are incredibly small.

00:35:23.480 --> 00:35:29.660
When you go shopping in a
grocery store or a drug store,

00:35:29.660 --> 00:35:30.560
you're in public.

00:35:30.560 --> 00:35:31.940
You're picking out
things that are delicate,

00:35:31.940 --> 00:35:33.731
but the odds that
someone is standing right

00:35:33.731 --> 00:35:37.010
behind you writing down every
single thing that you purchase

00:35:37.010 --> 00:35:39.800
is incredibly low, and the
odds of that ever coming back

00:35:39.800 --> 00:35:41.730
to hurt you is also
incredibly low.

00:35:41.730 --> 00:35:44.030
So we value obscurity,
and the harm

00:35:44.030 --> 00:35:48.020
comes when our obscurity
is taken away from us

00:35:48.020 --> 00:35:48.980
in dramatic ways.

00:35:48.980 --> 00:35:52.850
In other words, there are
lurches, obscurity lurches.

00:35:52.850 --> 00:35:56.090
I've written very critically
about facial-recognition

00:35:56.090 --> 00:35:58.250
technologies, and one
of the reasons why is I

00:35:58.250 --> 00:36:01.580
view it as an incredible
obscurity lurch.

00:36:01.580 --> 00:36:06.650
Our faces, while public,
basically allow us still

00:36:06.650 --> 00:36:09.230
to be relatively private
as we walk in a crowd,

00:36:09.230 --> 00:36:10.810
but facial-recognition
technologies

00:36:10.810 --> 00:36:12.140
threaten that ability.

00:36:12.140 --> 00:36:15.100
It represents a dramatic
obscurity lurch.

00:36:15.100 --> 00:36:17.390
And online, some
of my research is

00:36:17.390 --> 00:36:20.810
focused on there are things
that can obscure you or make you

00:36:20.810 --> 00:36:22.970
more obvious like
search visibility,

00:36:22.970 --> 00:36:26.300
unprotected access, whether
we protect something

00:36:26.300 --> 00:36:29.000
with a password,
whether someone uses

00:36:29.000 --> 00:36:33.470
their real name or a pseudonym
or no identifier at all,

00:36:33.470 --> 00:36:35.270
and whether things are clear.

00:36:35.270 --> 00:36:38.270
Sometimes things are obscure
because people lack the context

00:36:38.270 --> 00:36:39.530
to make sense of them.

00:36:39.530 --> 00:36:43.760
If I were to tell you all
right now, hey, by the way

00:36:43.760 --> 00:36:46.730
I just got the test results
back, it's positive,

00:36:46.730 --> 00:36:49.580
you would have no
idea what that meant.

00:36:49.580 --> 00:36:51.830
It's obscure to you
and it's safe to me

00:36:51.830 --> 00:36:54.660
because you have no
idea what that means.

00:36:54.660 --> 00:36:57.467
But if you had the back
story which is maybe

00:36:57.467 --> 00:37:00.050
that my wife and I were trying
to conceive then you would say,

00:37:00.050 --> 00:37:02.500
oh, I now understand.

00:37:02.500 --> 00:37:04.625
This is sort of hiding in
plain sight with content.

00:37:04.625 --> 00:37:07.250
And so there are ways in which
we can obscure information

00:37:07.250 --> 00:37:09.320
and preserve that obscurity.

00:37:09.320 --> 00:37:12.890
And then finally the value that
I advocate for is autonomy.

00:37:12.890 --> 00:37:16.500
Now autonomy is
different than control.

00:37:16.500 --> 00:37:18.950
Control can serve autonomy--

00:37:18.950 --> 00:37:20.990
the right to be free from
external interference,

00:37:20.990 --> 00:37:24.140
the right to
self-determination--

00:37:24.140 --> 00:37:27.120
but too much control actually
threatens that autonomy.

00:37:27.120 --> 00:37:30.410
So I argue that a
better value to embrace

00:37:30.410 --> 00:37:33.960
is autonomy rather than control.

00:37:33.960 --> 00:37:37.074
All right, and so that
leads me to the final part

00:37:37.074 --> 00:37:39.740
of the book where I argue that a
design agenda should have roots

00:37:39.740 --> 00:37:42.290
in consumer protection
and surveillance law

00:37:42.290 --> 00:37:45.830
rather than saying, well, the
Fair Information Practices

00:37:45.830 --> 00:37:50.919
which everybody accepts are the
thing that we should adhere to.

00:37:50.919 --> 00:37:52.460
If you look, I say
there are actually

00:37:52.460 --> 00:37:54.950
other areas of the law that
have thought this out, that

00:37:54.950 --> 00:37:59.900
have taken design seriously, and
I propose sorts of boundaries

00:37:59.900 --> 00:38:02.630
to avoid deceptive
design, abusive design,

00:38:02.630 --> 00:38:04.340
and dangerous design.

00:38:04.340 --> 00:38:10.760
Deceptive design is
something that we relatively

00:38:10.760 --> 00:38:12.244
are doing a good
job with, both, I

00:38:12.244 --> 00:38:13.910
think, within industry
and within policy

00:38:13.910 --> 00:38:16.880
in terms of not outright
deceiving people.

00:38:16.880 --> 00:38:21.230
It says this is Path complaints
filed by the Federal Trade

00:38:21.230 --> 00:38:23.420
Commission where add
friends actually didn't just

00:38:23.420 --> 00:38:25.878
allow you to add friends but
actually automatically sort of

00:38:25.878 --> 00:38:30.410
went in and scooped up
the entire contact list.

00:38:30.410 --> 00:38:34.370
And then there's also
the idea that maybe we

00:38:34.370 --> 00:38:36.320
should avoid
abusive design which

00:38:36.320 --> 00:38:40.210
is deleveraging of people's
own limitations against them.

00:38:40.210 --> 00:38:43.540
And this happens sometimes
in everyday life.

00:38:43.540 --> 00:38:45.920
We tend to sort of leverage
people's own limitations

00:38:45.920 --> 00:38:47.450
against them, but
it's another thing

00:38:47.450 --> 00:38:49.970
entirely to build an
entire machine built

00:38:49.970 --> 00:38:52.950
to leverage people's own
limitations against them.

00:38:52.950 --> 00:38:58.460
This is an example of different
differential pricing based

00:38:58.460 --> 00:39:00.110
on people's personal
characteristics,

00:39:00.110 --> 00:39:03.650
or the thing that made my
privacy-law students really mad

00:39:03.650 --> 00:39:07.560
one year is the fact
that a certain travel

00:39:07.560 --> 00:39:10.580
site could tell whether
you're using a PC or a Mac.

00:39:10.580 --> 00:39:17.000
It would show the
cheaper hotel rooms

00:39:17.000 --> 00:39:19.880
to the people using Windows PCs
and the more expensive hotel

00:39:19.880 --> 00:39:23.180
rooms to the people using
Macs based on the assumption

00:39:23.180 --> 00:39:27.320
that people that had
Macintoshes actually

00:39:27.320 --> 00:39:30.150
had more had more money
and were more affluent.

00:39:30.150 --> 00:39:31.470
But there are tons of examples.

00:39:31.470 --> 00:39:35.882
So one of the
examples that pops up

00:39:35.882 --> 00:39:38.090
when leveraging people's
own limitations against them

00:39:38.090 --> 00:39:40.920
is our desire for conformity.

00:39:40.920 --> 00:39:44.495
People have a strong desire
sort of to be with the group

00:39:44.495 --> 00:39:45.870
or to do what
everyone else does,

00:39:45.870 --> 00:39:47.790
and sometimes that can
be worked against us.

00:39:47.790 --> 00:39:49.950
So this shows up
in the literature

00:39:49.950 --> 00:39:53.580
around dark patterns, which is
a really interesting website.

00:39:53.580 --> 00:39:55.737
And this is one where
it says get 10% off,

00:39:55.737 --> 00:39:57.570
and then at the bottom
it says, "No, thanks.

00:39:57.570 --> 00:40:00.240
I'd rather pay full price for
delicious tea," which they're

00:40:00.240 --> 00:40:01.620
sort of shaming people, right?

00:40:01.620 --> 00:40:02.700
Be like, oh, OK.

00:40:02.700 --> 00:40:04.500
Well, I don't want to
be the sucker here.

00:40:04.500 --> 00:40:07.020
And there are several other
examples where you see--

00:40:07.020 --> 00:40:11.070
because people's voting records
tend to be public in most

00:40:11.070 --> 00:40:13.680
states, some people have
actually leveraged that

00:40:13.680 --> 00:40:17.430
and they said all your
neighbors voted and you didn't.

00:40:17.430 --> 00:40:20.460
Here are the people that
voted and you didn't vote.

00:40:20.460 --> 00:40:24.900
Look how irresponsible you are,
which is sort of a shaming.

00:40:24.900 --> 00:40:30.300
Or one here which is at the
bottom it says, "No thanks.

00:40:30.300 --> 00:40:31.765
I'm fine with losing customers."

00:40:31.765 --> 00:40:33.390
No one here wants to
click that, right?

00:40:33.390 --> 00:40:37.560
Now again, this is a very subtle
sort of manipulative technique,

00:40:37.560 --> 00:40:39.930
but it's one that,
at scale, could tend

00:40:39.930 --> 00:40:43.169
to be harmful in the right way.

00:40:43.169 --> 00:40:45.210
Another example of using
people's own limitations

00:40:45.210 --> 00:40:49.980
against them is the use of
double and triple negatives.

00:40:49.980 --> 00:40:53.520
So here it's decline release
of directory information.

00:40:53.520 --> 00:40:56.610
Note, most parents do
not choose this option.

00:40:56.610 --> 00:40:58.000
So we've got lots of nots here.

00:40:58.000 --> 00:41:00.769
I do not want the
release information.

00:41:00.769 --> 00:41:03.060
So when you start layering
double and triple negatives,

00:41:03.060 --> 00:41:05.940
then it's not lying.

00:41:05.940 --> 00:41:10.110
It's not deceitful, but it's
leveraging people's inability

00:41:10.110 --> 00:41:12.210
to sort of process the
double and triple negative.

00:41:12.210 --> 00:41:16.710
I don't not never want
this information released.

00:41:16.710 --> 00:41:19.110
And then finally there's
dangerous design.

00:41:19.110 --> 00:41:21.660
Some designs are just
inherently dangerous.

00:41:21.660 --> 00:41:22.950
This is a spy camera.

00:41:22.950 --> 00:41:26.490
It's designed for more
or less one thing, which

00:41:26.490 --> 00:41:30.120
is to catch people in
various states of undress

00:41:30.120 --> 00:41:31.840
because it's a clothes hanger.

00:41:31.840 --> 00:41:34.710
It's meant to be put in rooms
where people are taking clothes

00:41:34.710 --> 00:41:35.770
on and off.

00:41:35.770 --> 00:41:37.800
And I argue that that's
dangerous design.

00:41:37.800 --> 00:41:41.312
As dangerous as surveillance
that you know about is,

00:41:41.312 --> 00:41:43.020
I view surveillances
you don't know about

00:41:43.020 --> 00:41:45.334
to be even more
potentially dangerous.

00:41:45.334 --> 00:41:47.250
There are lots of potential
responses to this,

00:41:47.250 --> 00:41:49.440
and I go into this in the
second part of the book--

00:41:49.440 --> 00:41:51.600
soft responses,
moderate responses,

00:41:51.600 --> 00:41:53.310
and robust responses.

00:41:53.310 --> 00:41:54.840
So a soft response
is we don't have

00:41:54.840 --> 00:41:58.500
to come in as regulators heavy
handed and say we're regulating

00:41:58.500 --> 00:42:02.760
this to the hilt. Sometimes what
privacy needs is some funding.

00:42:02.760 --> 00:42:04.767
It needs some innovation.

00:42:04.767 --> 00:42:05.850
It needs some opportunity.

00:42:05.850 --> 00:42:07.980
It needs standardization.

00:42:07.980 --> 00:42:10.340
It needs educational
opportunities.

00:42:10.340 --> 00:42:12.300
And so I argue for a
lot of different ways

00:42:12.300 --> 00:42:14.970
to improve people's privacy
without passing some really

00:42:14.970 --> 00:42:16.700
heavy-handed regulations.

00:42:16.700 --> 00:42:18.450
Sometimes we need some
moderate responses.

00:42:18.450 --> 00:42:24.240
Maybe judges just need to
be a little more sensitive

00:42:24.240 --> 00:42:26.670
to the role the design
plays in shaping

00:42:26.670 --> 00:42:28.710
people's expectations
and their choices,

00:42:28.710 --> 00:42:29.970
and maybe regulators do too.

00:42:29.970 --> 00:42:32.580
This is an example
of the padlock icon

00:42:32.580 --> 00:42:34.320
which is ubiquitous online.

00:42:34.320 --> 00:42:35.720
We use it all the time.

00:42:35.720 --> 00:42:39.091
It is the physical
symbol of security,

00:42:39.091 --> 00:42:40.590
but we don't really
question what it

00:42:40.590 --> 00:42:42.480
means to people as much, right?

00:42:42.480 --> 00:42:44.430
When people see the
padlock icon they usually

00:42:44.430 --> 00:42:47.630
think they're safe or secure.

00:42:47.630 --> 00:42:49.500
That's what a lock means.

00:42:49.500 --> 00:42:51.630
But what does it really
mean on the practice?

00:42:51.630 --> 00:42:53.730
And maybe there
are ways in which

00:42:53.730 --> 00:42:55.800
these sort of padlock
icons might act

00:42:55.800 --> 00:42:58.110
as sort of implicit promises.

00:42:58.110 --> 00:42:59.790
And then finally
there may be a need

00:42:59.790 --> 00:43:02.830
for heavy-handed regulations
in certain instances.

00:43:02.830 --> 00:43:06.300
One of the things I've
argued for is in support

00:43:06.300 --> 00:43:11.910
of the ban on spyware that we
have right now, which I think

00:43:11.910 --> 00:43:13.500
can be used in really
malicious ways.

00:43:13.500 --> 00:43:15.890
And then another thing that's
a little more controversial

00:43:15.890 --> 00:43:18.750
is I've argued in favor
of moratoriums and bans

00:43:18.750 --> 00:43:20.460
on facial-recognition
technology, which

00:43:20.460 --> 00:43:24.690
I view as one of the most
uniquely dangerous surveillance

00:43:24.690 --> 00:43:27.221
technologies ever invented.

00:43:27.221 --> 00:43:28.720
And then the final
part of the book,

00:43:28.720 --> 00:43:32.760
I lay out what this blueprint
of this design agenda

00:43:32.760 --> 00:43:34.740
might look in three
different kinds

00:43:34.740 --> 00:43:38.250
of contexts-- social media,
hide-and-seek technologies,

00:43:38.250 --> 00:43:41.640
and the internet of
things, which is basically

00:43:41.640 --> 00:43:46.224
just a plea to tech
companies to stop

00:43:46.224 --> 00:43:48.390
connecting people's underwear
to the internet, which

00:43:48.390 --> 00:43:50.310
is one of the things.

00:43:50.310 --> 00:43:52.260
And with that, I would
love to go ahead and go

00:43:52.260 --> 00:43:53.250
to the question-and-answer
period.

00:43:53.250 --> 00:43:54.100
Thank you very much.

00:43:54.100 --> 00:43:54.766
I appreciate it.

00:43:54.766 --> 00:43:57.380
[APPLAUSE]

00:43:57.380 --> 00:44:00.110
AUDIENCE: You've given some
examples of bad designs.

00:44:00.110 --> 00:44:01.910
Can you give an example
of a good design,

00:44:01.910 --> 00:44:03.590
particularly in the
area of providing

00:44:03.590 --> 00:44:05.550
user autonomy as you described?

00:44:05.550 --> 00:44:09.360
WOODROW HARTZOG: Oh,
OK, actually there

00:44:09.360 --> 00:44:12.530
are several, I think,
very good designs.

00:44:12.530 --> 00:44:16.790
One might be there are areas
in which-- so after I sort

00:44:16.790 --> 00:44:19.700
of rail on control
a lot, there are

00:44:19.700 --> 00:44:22.710
areas in which control might
actually be really useful.

00:44:22.710 --> 00:44:24.320
I like the data-subject right--

00:44:24.320 --> 00:44:26.520
I also sort of critiqued
the GDPR, but now

00:44:26.520 --> 00:44:28.280
I'll backtrack on
that a little too--

00:44:28.280 --> 00:44:30.260
data-subject rights
that allow you

00:44:30.260 --> 00:44:33.292
to log in and review profiles.

00:44:33.292 --> 00:44:35.000
Information that's
kept about you I think

00:44:35.000 --> 00:44:37.220
are very useful because
they can be done

00:44:37.220 --> 00:44:39.740
at the election of the user.

00:44:39.740 --> 00:44:41.810
Dashboards, for
example, that allow

00:44:41.810 --> 00:44:44.960
you to review
histories, locations,

00:44:44.960 --> 00:44:48.560
I think those are very good.

00:44:48.560 --> 00:44:51.800
My general principle is that
people should be protected

00:44:51.800 --> 00:44:53.490
regardless of what they choose.

00:44:53.490 --> 00:44:57.170
But if there are ways in which
they can exercise autonomy

00:44:57.170 --> 00:44:59.870
on top of that baseline
level of protection,

00:44:59.870 --> 00:45:02.480
then that should be encouraged.

00:45:02.480 --> 00:45:04.010
There are also
small little things

00:45:04.010 --> 00:45:05.870
that I like that a
lot of people actually

00:45:05.870 --> 00:45:08.000
critique as not going
far enough but I

00:45:08.000 --> 00:45:10.280
like because they're
obscurity protective.

00:45:10.280 --> 00:45:14.180
For example, YouTube
released a tool

00:45:14.180 --> 00:45:17.420
that allowed you to blur
people's faces I believe.

00:45:17.420 --> 00:45:19.140
I love that tool.

00:45:19.140 --> 00:45:21.140
And one of the reasons I
love it is because it's

00:45:21.140 --> 00:45:23.534
obscurity protecting.

00:45:23.534 --> 00:45:25.700
It's one that if you happen
to know the person who's

00:45:25.700 --> 00:45:27.741
in the video then maybe
you would recognize them,

00:45:27.741 --> 00:45:31.190
but to the world that's actually
a pretty good design, right?

00:45:31.190 --> 00:45:33.830
So in other words, people
are largely protected.

00:45:33.830 --> 00:45:36.230
We often don't want to be
protected from everybody, just

00:45:36.230 --> 00:45:37.640
from certain sorts of risks.

00:45:41.759 --> 00:45:44.300
The right to be forgotten is
one of these things where people

00:45:44.300 --> 00:45:48.110
often want their name
disassociated with being

00:45:48.110 --> 00:45:50.180
found for certain results.

00:45:50.180 --> 00:45:51.860
They don't need the
information gone.

00:45:51.860 --> 00:45:53.900
They just don't
want the HR person

00:45:53.900 --> 00:45:56.060
who is reviewing
their application

00:45:56.060 --> 00:45:57.814
for a job they want to find it.

00:45:57.814 --> 00:45:59.480
And so there are
certain sorts of things

00:45:59.480 --> 00:46:01.790
that can protect obscurity
in design like that,

00:46:01.790 --> 00:46:04.220
and I think the YouTube
face-blurring video

00:46:04.220 --> 00:46:06.550
is a great example of that.

00:46:06.550 --> 00:46:08.930
AUDIENCE: So how do
you consider metadata

00:46:08.930 --> 00:46:10.770
like derived data
for [INAUDIBLE] data?

00:46:10.770 --> 00:46:12.720
How does that kind
of play into this?

00:46:12.720 --> 00:46:16.356
And other people's data of me--

00:46:16.356 --> 00:46:17.730
so there was the
Facebook thing I

00:46:17.730 --> 00:46:21.720
think where other
people had data on me

00:46:21.720 --> 00:46:24.200
and I won't be able to
see their data on me.

00:46:24.200 --> 00:46:26.809
So, for instance, if they
had a phone number of me

00:46:26.809 --> 00:46:29.100
but I never authorized Facebook
to have my phone number

00:46:29.100 --> 00:46:30.780
but they had it, Facebook
now has my phone number

00:46:30.780 --> 00:46:32.800
and I don't know that
Facebook has my phone number.

00:46:32.800 --> 00:46:33.390
WOODROW HARTZOG:
The question is sort

00:46:33.390 --> 00:46:36.030
of what is the role of
metadata in all of this,

00:46:36.030 --> 00:46:37.950
and this is what I
think really illustrates

00:46:37.950 --> 00:46:41.430
the limits of the control
conception of privacy

00:46:41.430 --> 00:46:43.920
because so much is actually
out of our control.

00:46:43.920 --> 00:46:47.010
A really good example
is genetic information.

00:46:47.010 --> 00:46:53.490
So my sister got one of
the popular sort of tests

00:46:53.490 --> 00:46:56.220
and gave away her
genetic information,

00:46:56.220 --> 00:46:58.500
and I was really a
little uneasy about it

00:46:58.500 --> 00:47:00.510
because I was like,
that's my information too,

00:47:00.510 --> 00:47:02.040
and I've got no
control over that

00:47:02.040 --> 00:47:05.070
at all, which is why I argue
that we should have less

00:47:05.070 --> 00:47:07.950
reliance on control
because control ultimately

00:47:07.950 --> 00:47:10.640
sort of becomes unraveled
at this stage right.

00:47:10.640 --> 00:47:12.630
It's our inability
to control it.

00:47:12.630 --> 00:47:15.300
This is where I argue for
the importance of trust.

00:47:15.300 --> 00:47:19.620
This is where relationships
of trust matter.

00:47:19.620 --> 00:47:22.470
Now there are costs to that.

00:47:22.470 --> 00:47:25.800
So if we think of privacy
in terms of trust,

00:47:25.800 --> 00:47:28.605
then inherently we've just
made this conceptualization

00:47:28.605 --> 00:47:32.010
of privacy relational.

00:47:32.010 --> 00:47:34.950
So then there are
advantages to that

00:47:34.950 --> 00:47:38.220
in that now I can ask you
if you have metadata on me

00:47:38.220 --> 00:47:40.860
or if a platform has
metadata on multiple people

00:47:40.860 --> 00:47:43.920
to act in a certain way-- loyal,
for example, to not leverage

00:47:43.920 --> 00:47:47.090
that information against me.

00:47:47.090 --> 00:47:50.670
I don't think that just because
information is disclosed

00:47:50.670 --> 00:47:52.890
your privacy is
necessarily gone.

00:47:52.890 --> 00:47:56.010
I think that we
disclose information

00:47:56.010 --> 00:47:57.916
within relationships
of trust all the time.

00:47:57.916 --> 00:47:59.790
But what it does mean
is that sometimes there

00:47:59.790 --> 00:48:03.570
are parties that have
no relationship that

00:48:03.570 --> 00:48:07.140
have that information that
can then violate your privacy.

00:48:07.140 --> 00:48:09.240
So data brokers would
be an obvious example

00:48:09.240 --> 00:48:12.330
where they don't have a direct
relationship with the data

00:48:12.330 --> 00:48:16.290
subject in ways that platforms
that collect information

00:48:16.290 --> 00:48:18.210
directly do, but
they get information

00:48:18.210 --> 00:48:20.460
from various streams.

00:48:20.460 --> 00:48:22.890
And so my general
sense is that we

00:48:22.890 --> 00:48:25.590
should have a baseline set of
rules that even apply to data

00:48:25.590 --> 00:48:29.760
brokers but that a lot of our
issues with respect to metadata

00:48:29.760 --> 00:48:31.500
and a lot of these
unraveling problems

00:48:31.500 --> 00:48:35.516
can be solved through thinking
with privacy in terms of trust.

00:48:35.516 --> 00:48:37.140
So just the fact that
you have it right

00:48:37.140 --> 00:48:40.050
or that someone else has it
isn't the end of the story.

00:48:40.050 --> 00:48:41.880
Rather, there are
now things that you

00:48:41.880 --> 00:48:45.560
shouldn't be able to do
with that information.

00:48:45.560 --> 00:48:48.680
AUDIENCE: We have a question
from the Dory from Cain.

00:48:48.680 --> 00:48:50.360
I found that I quickly
learned to click

00:48:50.360 --> 00:48:53.120
on those "I'm fine with
losing customers" buttons

00:48:53.120 --> 00:48:55.340
because the pop up is
blocking the task I

00:48:55.340 --> 00:48:56.750
was trying to complete.

00:48:56.750 --> 00:48:58.790
What are some tips for
making sure people know

00:48:58.790 --> 00:49:01.670
how to go back and change their
mind when it's no longer time

00:49:01.670 --> 00:49:03.054
critical?

00:49:03.054 --> 00:49:04.970
WOODROW HARTZOG: Again,
not to make everything

00:49:04.970 --> 00:49:07.730
an illustration of the limits
of the control conceptualization

00:49:07.730 --> 00:49:12.440
of privacy, but often when
we think of consent, consent

00:49:12.440 --> 00:49:14.870
is implemented in
sort of binary terms.

00:49:14.870 --> 00:49:17.750
Like I agree to this, and then
once that it's gotten then

00:49:17.750 --> 00:49:19.880
it sort of in perpetuity
exists and you

00:49:19.880 --> 00:49:21.320
don't have to
re-ask for it again

00:49:21.320 --> 00:49:24.440
even though people often change
their minds or circumstances

00:49:24.440 --> 00:49:25.640
change.

00:49:25.640 --> 00:49:28.700
This is where I do
think that there

00:49:28.700 --> 00:49:31.910
is some responsibility on the
part of users to take advantage

00:49:31.910 --> 00:49:35.630
of the tools that are given to
them if those tools are given

00:49:35.630 --> 00:49:38.390
to them in an easy-to-use way.

00:49:38.390 --> 00:49:41.090
I encourage everyone
to at least take

00:49:41.090 --> 00:49:43.820
a few minutes as
part of your maybe

00:49:43.820 --> 00:49:45.920
not daily but maybe
weekly or monthly routine

00:49:45.920 --> 00:49:49.250
to do a little digital hygiene.

00:49:49.250 --> 00:49:51.830
Search your own name on
various search engines

00:49:51.830 --> 00:49:54.520
to see what's coming up.

00:49:54.520 --> 00:49:56.920
If you have accounts--

00:49:56.920 --> 00:49:58.430
and sometimes you
have accounts even

00:49:58.430 --> 00:49:59.990
for only defensive purposes.

00:49:59.990 --> 00:50:02.320
So sometimes I get
resistance where

00:50:02.320 --> 00:50:04.904
they say, well, I don't want to
sign up for an account for XYZ

00:50:04.904 --> 00:50:07.278
because then they'll have even
more information about me,

00:50:07.278 --> 00:50:08.390
which I'm sympathetic to.

00:50:08.390 --> 00:50:12.170
But if it allows you to then
sort of log in and regularly

00:50:12.170 --> 00:50:14.000
delete information or
cleanse information,

00:50:14.000 --> 00:50:16.010
then that might be positive.

00:50:16.010 --> 00:50:22.460
And then take advantage
of popular add-ons.

00:50:22.460 --> 00:50:25.790
I use ad blockers, which
I know is controversial,

00:50:25.790 --> 00:50:28.865
but I think that's
another possible tool.

00:50:28.865 --> 00:50:30.740
And then finally-- and
this is just the thing

00:50:30.740 --> 00:50:33.830
that I throw out generally-- is
if two-factor authentication is

00:50:33.830 --> 00:50:40.190
ever offered anywhere,
use it, and use it

00:50:40.190 --> 00:50:43.890
sort of early and
often is my example.

00:50:43.890 --> 00:50:47.840
But I do think that
vanity searches, learning

00:50:47.840 --> 00:50:51.750
how to work within dashboards
is at least a key part.

00:50:51.750 --> 00:50:53.000
It can't be everything, right?

00:50:53.000 --> 00:50:57.500
My argument is that often users
are being asked to do too much,

00:50:57.500 --> 00:50:59.780
but there is some
sort of expectation

00:50:59.780 --> 00:51:02.480
that I think we can have of
users to at least sort of try

00:51:02.480 --> 00:51:07.300
to exercise some diligence
with the tools that are given.

00:51:07.300 --> 00:51:09.770
AUDIENCE: What are your
thoughts on premium options?

00:51:09.770 --> 00:51:11.671
So we're starting to
see a lot of companies

00:51:11.671 --> 00:51:12.670
moving towards that end.

00:51:12.670 --> 00:51:15.600
We have a YouTube premium
option, et cetera,

00:51:15.600 --> 00:51:18.919
as a method for coming back
to your concept of control.

00:51:18.919 --> 00:51:19.960
Do you see that as valid?

00:51:22.104 --> 00:51:24.520
WOODROW HARTZOG: I'm of two
minds for the premium options.

00:51:24.520 --> 00:51:29.410
I mean, on one hand I
like the premium options

00:51:29.410 --> 00:51:33.130
because the business model
seems a little clearer.

00:51:33.130 --> 00:51:36.280
So one of the things that
I've always been drawn to

00:51:36.280 --> 00:51:40.192
is when everybody's incentives
are sort of out in the open

00:51:40.192 --> 00:51:42.150
and we know sort of what
we're dealing with it,

00:51:42.150 --> 00:51:45.460
it becomes a lot easier
and easier to trust.

00:51:45.460 --> 00:51:48.860
I think for the
premium models to work,

00:51:48.860 --> 00:51:53.540
there has to be not just
some sort of soft promises

00:51:53.540 --> 00:51:55.750
but I think really
robust promises.

00:51:55.750 --> 00:51:59.650
I would love the
idea of platforms

00:51:59.650 --> 00:52:02.720
being able to opt into sort of
the gold standard of privacy,

00:52:02.720 --> 00:52:05.980
which is the true level
of trustworthiness where

00:52:05.980 --> 00:52:08.120
they say we will be loyal.

00:52:08.120 --> 00:52:09.070
We will be discreet.

00:52:09.070 --> 00:52:11.153
We will be honest to the
sort of highest standard.

00:52:13.930 --> 00:52:17.110
And then that can drive people
to sign up for companies.

00:52:17.110 --> 00:52:18.760
It could be a
competitive advantage.

00:52:18.760 --> 00:52:20.770
And I've always
longed for privacy

00:52:20.770 --> 00:52:22.990
to be a competitive
advantage, and I

00:52:22.990 --> 00:52:25.814
think that we're seeing a
little traction on that model.

00:52:25.814 --> 00:52:27.730
But then the other side
of me worries a little

00:52:27.730 --> 00:52:30.940
about premium models because the
other thing that I worry about

00:52:30.940 --> 00:52:33.760
is that privacy
becomes something

00:52:33.760 --> 00:52:38.020
only that people that
have affordances can have,

00:52:38.020 --> 00:52:41.710
and I really worry about
the sort of equities

00:52:41.710 --> 00:52:46.570
where we say if you pay the
gold-standard option you

00:52:46.570 --> 00:52:47.590
get privacy.

00:52:47.590 --> 00:52:52.990
For everyone that can't afford
it, they don't have that.

00:52:52.990 --> 00:52:56.680
And so for that reason, I'm
still actually struggling

00:52:56.680 --> 00:53:02.380
with a lot of the way that that
cashes out in terms of policy

00:53:02.380 --> 00:53:04.170
because I do worry about that.

00:53:06.406 --> 00:53:08.030
AUDIENCE: And we have
one more question

00:53:08.030 --> 00:53:11.990
on the Dory from Keith.

00:53:11.990 --> 00:53:14.810
What are we all missing
about Dragonfly?

00:53:14.810 --> 00:53:17.040
And the context
is a "Wired" story

00:53:17.040 --> 00:53:19.350
of Sundar at the "Wired" summit.

00:53:19.350 --> 00:53:22.880
WOODROW HARTZOG: So
Dragonfly, as I understand it,

00:53:22.880 --> 00:53:30.920
is the initiative for Google
to enter China's market,

00:53:30.920 --> 00:53:34.340
right, if I'm understanding
the stories correctly.

00:53:34.340 --> 00:53:36.120
And I haven't read
a lot about it,

00:53:36.120 --> 00:53:40.640
so I actually can't provide
a nuanced discussion,

00:53:40.640 --> 00:53:44.570
but I have thought a lot
about the values they

00:53:44.570 --> 00:53:46.940
get reflected in technology.

00:53:46.940 --> 00:53:49.970
And so to that extent,
I think it's worth

00:53:49.970 --> 00:53:56.520
having an explicit
conversation about what

00:53:56.520 --> 00:54:03.690
the affordances of Dragonfly
would be because it doesn't

00:54:03.690 --> 00:54:08.050
exist in a vacuum.

00:54:08.050 --> 00:54:12.420
This is a browser that will get
deployed within a system that

00:54:12.420 --> 00:54:17.750
is actively leveraging things
like social credit scores.

00:54:17.750 --> 00:54:20.600
And the social credit
score actually I will say

00:54:20.600 --> 00:54:23.390
scares me a lot,
the idea that we

00:54:23.390 --> 00:54:26.390
would be ranked based on any
number of different factors

00:54:26.390 --> 00:54:28.839
and then have benefits
sort of denied to us based

00:54:28.839 --> 00:54:30.380
upon those factors--
that we wouldn't

00:54:30.380 --> 00:54:32.510
be able to board
a plane or that we

00:54:32.510 --> 00:54:34.310
would have sort of
demerits taken away

00:54:34.310 --> 00:54:38.300
from us because we used up too
many rolls of toilet paper.

00:54:38.300 --> 00:54:41.810
The fact that we would be
sort of datafying everything,

00:54:41.810 --> 00:54:44.690
does this contribute
to that effort,

00:54:44.690 --> 00:54:47.540
because that's a value
that's implicated.

00:54:47.540 --> 00:54:49.370
Will it help
exacerbate the spread

00:54:49.370 --> 00:54:51.230
of facial-recognition
technology, which

00:54:51.230 --> 00:54:54.530
is another thing that
I've worried about a lot.

00:54:54.530 --> 00:54:57.500
Because in order to gain the
benefits of facial-recognition

00:54:57.500 --> 00:55:02.120
technology, of which there
admittedly are many--

00:55:02.120 --> 00:55:03.340
we can find the bad guy.

00:55:03.340 --> 00:55:05.210
We can find missing children.

00:55:05.210 --> 00:55:12.250
We can help those that
don't have the ability

00:55:12.250 --> 00:55:16.070
to sense in ways of others.

00:55:16.070 --> 00:55:19.990
But in order to get a lot of the
real uses of facial-recognition

00:55:19.990 --> 00:55:23.230
technology, not just the sort of
minor conveniences of unlocking

00:55:23.230 --> 00:55:25.640
your phone with your face
rather than your fingerprint--

00:55:25.640 --> 00:55:28.540
which is fine, but I view
as just a very incremental

00:55:28.540 --> 00:55:29.950
benefit--

00:55:29.950 --> 00:55:32.420
but in order to get
the real benefits,

00:55:32.420 --> 00:55:36.970
we will have to give up
a lot, almost everything.

00:55:36.970 --> 00:55:38.890
We have to have
cameras everywhere.

00:55:38.890 --> 00:55:41.290
We have to have databases
that are shared.

00:55:41.290 --> 00:55:42.880
Facial-recognition
technology depends

00:55:42.880 --> 00:55:45.880
upon the existence of a
name-face database that

00:55:45.880 --> 00:55:48.700
can recognize people.

00:55:48.700 --> 00:55:50.980
And I think we have to think
about that value as well,

00:55:50.980 --> 00:55:53.880
and will this aid
that in the aggregate,

00:55:53.880 --> 00:56:01.090
and is it worth then defining
the values of the company

00:56:01.090 --> 00:56:04.420
based on that because your
values are reflected not just

00:56:04.420 --> 00:56:07.430
in what you do but
in what you build.

00:56:07.430 --> 00:56:11.170
And I think that it merits
a serious conversation

00:56:11.170 --> 00:56:16.300
about the direction of a company
and the ethos of a company

00:56:16.300 --> 00:56:18.540
with that creation.

00:56:18.540 --> 00:56:21.190
But that being said, I'd be
scared to comment any more

00:56:21.190 --> 00:56:25.546
because I actually don't know
the specifics of the project.

00:56:25.546 --> 00:56:26.920
AUDIENCE: So terms
and conditions

00:56:26.920 --> 00:56:30.700
seems like a necessary evil,
but clearly the way they're done

00:56:30.700 --> 00:56:34.090
today is pretty terrible
because nobody reads them,

00:56:34.090 --> 00:56:36.820
and now people are I
guess legally bound.

00:56:36.820 --> 00:56:39.592
I'm not even sure if they're
legally bind in because nobody

00:56:39.592 --> 00:56:40.300
understands them.

00:56:40.300 --> 00:56:41.430
Nobody reads them.

00:56:41.430 --> 00:56:43.420
Can you agree to
something you don't read?

00:56:43.420 --> 00:56:44.940
At least that's the point.

00:56:44.940 --> 00:56:47.410
And there's no expectation
to read them either,

00:56:47.410 --> 00:56:48.680
which is another issue.

00:56:48.680 --> 00:56:51.740
But how do you think that
could be done better?

00:56:51.740 --> 00:56:52.750
WOODROW HARTZOG: Great.

00:56:52.750 --> 00:56:53.450
Thank you very much.

00:56:53.450 --> 00:56:54.070
I love that question.

00:56:54.070 --> 00:56:56.020
So I just wrote an essay
a little while back

00:56:56.020 --> 00:56:58.900
called "User agreements
are betraying you"

00:56:58.900 --> 00:57:03.850
with the idea that it's this
abstraction, nuanced tension,

00:57:03.850 --> 00:57:05.560
that's unresolvable.

00:57:05.560 --> 00:57:07.480
We could either have--
someone proposed

00:57:07.480 --> 00:57:11.020
at some point a 100-word
user agreement, which

00:57:11.020 --> 00:57:14.540
that's not even an
advertising slogan almost.

00:57:14.540 --> 00:57:17.440
I mean, it's so sort of
abstracted to not really tell

00:57:17.440 --> 00:57:18.910
you anything.

00:57:18.910 --> 00:57:21.520
I think that user
agreements could

00:57:21.520 --> 00:57:27.910
be improved upon if they
are written in a way

00:57:27.910 --> 00:57:32.140
that they are written for
basically transparency purposes

00:57:32.140 --> 00:57:33.820
because some people
do read them.

00:57:33.820 --> 00:57:39.670
Actually there's a really good
article by Mike Hintze called

00:57:39.670 --> 00:57:41.710
"In Defense of the Long
Privacy Statement,"

00:57:41.710 --> 00:57:43.570
and he says that
some people actually

00:57:43.570 --> 00:57:44.920
do read privacy policies.

00:57:44.920 --> 00:57:46.060
Regulators do.

00:57:46.060 --> 00:57:47.620
Advocates do.

00:57:47.620 --> 00:57:48.707
They serve an-- huh?

00:57:48.707 --> 00:57:49.540
AUDIENCE: Attorneys.

00:57:49.540 --> 00:57:50.956
WOODROW HARTZOG:
Attorneys, right.

00:57:50.956 --> 00:57:52.060
Attorneys do.

00:57:52.060 --> 00:57:54.910
They serve an important
hygienic function for companies.

00:57:54.910 --> 00:57:58.001
So they let you know
like internally like, OK,

00:57:58.001 --> 00:57:59.500
here's where all
our information is.

00:57:59.500 --> 00:58:01.160
We've got stock of it.

00:58:01.160 --> 00:58:04.660
It's an incredibly good
force functioning tool.

00:58:04.660 --> 00:58:06.760
It's just not for users.

00:58:06.760 --> 00:58:14.950
And so my preference would be
to have a regulatory regime--

00:58:14.950 --> 00:58:16.450
and this is a little
controversial--

00:58:16.450 --> 00:58:19.780
where there's certain things
that you cannot trade away

00:58:19.780 --> 00:58:20.930
in a user agreement.

00:58:20.930 --> 00:58:23.263
So in other words, there's a
baseline set of protections

00:58:23.263 --> 00:58:26.020
that cannot be compromised
through terms and conditions.

00:58:26.020 --> 00:58:29.142
We have this in other
areas of the law.

00:58:29.142 --> 00:58:30.850
And then that way
users can agree to them

00:58:30.850 --> 00:58:32.016
without worrying about them.

00:58:32.016 --> 00:58:35.050
Because I always sort of had
this like small sense of dread

00:58:35.050 --> 00:58:37.457
when I click I agree like
I didn't read any of this.

00:58:37.457 --> 00:58:39.790
I don't know what's in there,
but I agreed to it anyway.

00:58:39.790 --> 00:58:42.430
And they actually are
enforceable, largely.

00:58:42.430 --> 00:58:45.670
So the law-- and I teach
contracts sometimes too--

00:58:45.670 --> 00:58:48.210
and the law of contracts is
that if you click I Agree,

00:58:48.210 --> 00:58:51.650
then courts are going
to say you agreed.

00:58:51.650 --> 00:58:54.490
And I would like to
have a talk about things

00:58:54.490 --> 00:58:57.400
that are unwaveable so that it
basically didn't matter what

00:58:57.400 --> 00:58:59.440
was in the terms of
agreements because they are

00:58:59.440 --> 00:59:02.650
important risk-mitigation
tools for companies

00:59:02.650 --> 00:59:06.730
and they could have nice
transparency and accountability

00:59:06.730 --> 00:59:07.660
aspects to them.

00:59:07.660 --> 00:59:09.493
We just have to stop
pretending that they're

00:59:09.493 --> 00:59:12.760
for users because the things
that shape users' expectations

00:59:12.760 --> 00:59:15.160
are the designs, the website.

00:59:15.160 --> 00:59:16.450
It's the padlock icon.

00:59:16.450 --> 00:59:18.560
It's the button.

00:59:18.560 --> 00:59:19.310
Thank you so much.

00:59:19.310 --> 00:59:20.726
I really appreciate
it, everybody.

00:59:20.726 --> 00:59:24.960
[APPLAUSE]

