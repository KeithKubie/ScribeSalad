WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:04.010
Q learning is named
after the Q function.

00:00:05.120 --> 00:00:06.430
What is that?

00:00:06.430 --> 00:00:08.972
Well, let's dig in and find out.

00:00:08.972 --> 00:00:13.968
Q can be written as a function, so
we might have parentheses around s and

00:00:13.968 --> 00:00:16.950
a, or you can think of it as a table.

00:00:16.950 --> 00:00:20.710
So in this class we're
going to view Q as a table.

00:00:20.710 --> 00:00:24.190
And it's got two dimensions, s and a.

00:00:24.190 --> 00:00:31.580
So s is the state that we're looking at,
and a is the action we might take.

00:00:31.580 --> 00:00:35.510
Q represents the value of taking action

00:00:35.510 --> 00:00:41.080
a in state s, and
there's two components to that.

00:00:41.080 --> 00:00:45.730
The two components are the immediate
reward that you get for

00:00:45.730 --> 00:00:50.201
taking action a in state s,
plus the discounted reward.

00:00:50.201 --> 00:00:53.945
And what that is about,
what the discounted reward is about,

00:00:53.945 --> 00:00:57.130
is the reward you get for
future actions.

00:00:57.130 --> 00:01:02.150
So an important thing to know
is that Q is not greedy,

00:01:02.150 --> 00:01:06.620
in the sense that it just represents
the reward you get for acting now.

00:01:06.620 --> 00:01:11.180
It also represents the reward you
get for acting in the future.

00:01:11.180 --> 00:01:14.970
Let's suppose we have Q
already created for us.

00:01:14.970 --> 00:01:16.520
We have this table.

00:01:16.520 --> 00:01:18.940
How can we use it to
figure out what to do?

00:01:20.010 --> 00:01:24.190
So what we do in any particular
state is the policy.

00:01:24.190 --> 00:01:27.900
And we represent the policy with pi.

00:01:27.900 --> 00:01:34.000
So pi of s means, what is the action
we take when we are in state s,

00:01:34.000 --> 00:01:37.810
or what is the policy for state s?

00:01:37.810 --> 00:01:41.480
And we take advantage of our
Q table to figure that out.

00:01:41.480 --> 00:01:42.880
Here's how it works.

00:01:42.880 --> 00:01:47.950
We're in state s and we want to
find out which action is the best.

00:01:47.950 --> 00:01:52.340
Well, all we need to do is look
across all the potential actions and

00:01:52.340 --> 00:01:58.840
find out which value of
(Q [s,a]) is maximized.

00:01:58.840 --> 00:02:02.820
So we don't change s, we just
step through each value of a, and

00:02:02.820 --> 00:02:06.710
the one that is the largest
is the action we should take.

00:02:06.710 --> 00:02:12.386
And the mathematical way to represent
this is to use the function argmax,

00:02:12.386 --> 00:02:17.210
so argmax of a of this function.

00:02:17.210 --> 00:02:21.190
So what that does is it finds
the a that maximizes this, and

00:02:21.190 --> 00:02:22.430
then the answer is a.

00:02:23.580 --> 00:02:25.930
After we run Q learning for long enough,

00:02:25.930 --> 00:02:29.430
we will eventually converge
to the optimal policy.

00:02:29.430 --> 00:02:32.870
And we represent that
with a little star.

00:02:32.870 --> 00:02:35.863
So the optimal policy is pi star of s.

00:02:35.863 --> 00:02:44.370
[COUGH] And similarly the optimal
Q table is Q star [s, a].

00:02:44.370 --> 00:02:47.249
Now this is how to use
a Q table if you have it.

00:02:48.380 --> 00:02:52.090
We need now to consider how do we
build that Q table in the first place.

