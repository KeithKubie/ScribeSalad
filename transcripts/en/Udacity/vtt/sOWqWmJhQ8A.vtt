WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:02.090
&gt;&gt; Okay, so we agreed, Michael, that

00:00:02.090 --> 00:00:04.430
the problem's pretty hard. It's exponential in

00:00:04.430 --> 00:00:08.640
the number of features in this case N. And there are sort of two general

00:00:08.640 --> 00:00:11.280
approaches that people have used to try

00:00:11.280 --> 00:00:14.978
to attack this problem, broadly speaking. The first

00:00:14.978 --> 00:00:19.290
is called filtering, and the second is called wrapping, with a W, not an R.

00:00:19.290 --> 00:00:19.495
&gt;&gt; Hm.

00:00:19.495 --> 00:00:22.419
&gt;&gt; Well, it has an R too, but it starts with

00:00:22.419 --> 00:00:25.400
a W. And they actually look pretty similar. So I drew these

00:00:25.400 --> 00:00:27.690
little cartoons, which I hope would, would make it easier for

00:00:27.690 --> 00:00:30.720
you to see. So filtering basically works like this. You have a

00:00:30.720 --> 00:00:34.590
set of features as input. You run them through some algorithm

00:00:34.590 --> 00:00:38.670
that maximizes some kind of criteria. And then it outputs fewer features.

00:00:38.670 --> 00:00:39.450
&gt;&gt; Mm-hm.

00:00:39.450 --> 00:00:41.050
&gt;&gt; And passes those to some learning

00:00:41.050 --> 00:00:44.110
algorithm, that willl then use it for classification,

00:00:45.210 --> 00:00:48.480
say. It could be a regression, more generally,

00:00:48.480 --> 00:00:50.480
but you know, without loss of generality let's

00:00:50.480 --> 00:00:54.070
assume that we're worried about classification problems here. So

00:00:54.070 --> 00:00:56.260
you'll notice then that the criterion of how you know

00:00:56.260 --> 00:00:59.120
whether you're doing well or not is buried inside the

00:00:59.120 --> 00:01:02.360
search algorithm itself. By contrast, you could do what's called

00:01:02.360 --> 00:01:06.440
wrapping. And in the wrapping approach, you've taken your set

00:01:06.440 --> 00:01:08.830
of features, your N features, and you run them into

00:01:08.830 --> 00:01:11.700
this little box that basically searches over a subset of

00:01:11.700 --> 00:01:15.390
features, asks the learning algorithm to do something with them.

00:01:15.390 --> 00:01:17.340
The learning algorithm basically reports how well it

00:01:17.340 --> 00:01:20.240
does. And it uses that to update this new

00:01:20.240 --> 00:01:22.110
subset of features that it might look for, and

00:01:22.110 --> 00:01:24.620
passes to the algorithm. So, this is called filtering

00:01:24.620 --> 00:01:26.320
because this is a process that filters the

00:01:26.320 --> 00:01:28.210
features before handing it to a learning algorithm. And

00:01:28.210 --> 00:01:31.160
this is called wrapping because the search for the

00:01:31.160 --> 00:01:34.490
features is wrapped around whatever your learning algorithm is.

00:01:34.490 --> 00:01:35.395
&gt;&gt; Hm.

00:01:35.395 --> 00:01:35.830
&gt;&gt; You see that?

00:01:35.830 --> 00:01:37.980
&gt;&gt; I think so.

00:01:37.980 --> 00:01:40.810
&gt;&gt; Okay, so you might ask yourself,

00:01:40.810 --> 00:01:44.150
okay great, so you got two different approaches to this. Which one might

00:01:44.150 --> 00:01:48.180
you pick? And, unsurprisingly, there are tradeoffs. So, let's talk a little bit

00:01:48.180 --> 00:01:50.750
about what those tradeoffs are. Actually, before we do that, Michael do you

00:01:50.750 --> 00:01:53.370
have any suggestions for what would be

00:01:53.370 --> 00:01:55.660
good or bad about filtering versus wrapping?

00:01:55.660 --> 00:01:57.160
&gt;&gt; So filtering, it feels like is how we

00:01:57.160 --> 00:02:00.110
were talking about it before. It's this idea of whittling

00:02:00.110 --> 00:02:01.990
down to a set of fewer features and then

00:02:01.990 --> 00:02:05.720
handing it over, kind of one thing after the other.

00:02:05.720 --> 00:02:08.669
I, an advantage of that seems like it's sort of very flow

00:02:08.669 --> 00:02:11.750
forward, right? So it, it, the features come in, the search does its

00:02:11.750 --> 00:02:15.330
thing, it hands it over, everybody's got its job. The disadvantage of

00:02:15.330 --> 00:02:18.880
that, though, is that there isn't any feedback. So the learning algorithm can't

00:02:18.880 --> 00:02:21.810
inform the search algorithm as to hey, you know, you gave me

00:02:21.810 --> 00:02:24.830
this feature, but without that, you took away that feature, but without it

00:02:24.830 --> 00:02:27.030
I'm having a lot of trouble doing the learning. Maybe you should put

00:02:27.030 --> 00:02:30.960
that one back in. The second, the wrapping approach, I guess, would allow

00:02:30.960 --> 00:02:31.240
for that.

00:02:31.240 --> 00:02:32.990
&gt;&gt; In fact, that's exactly right. So as I

00:02:32.990 --> 00:02:36.550
said before, in the filtering case, the criterion or

00:02:36.550 --> 00:02:40.430
criteria is built inside the search algorithm itself without

00:02:40.430 --> 00:02:43.440
reference to the learner. By contrast, in wrapping the

00:02:43.440 --> 00:02:46.680
criteria, or criterion if it's a single one, is

00:02:46.680 --> 00:02:49.440
actually built inside what the learner wants. It's built

00:02:49.440 --> 00:02:53.130
inside something like classification error, or sum of squared

00:02:53.130 --> 00:02:56.050
errors, or whatever it is you measure the learner by.

00:02:56.050 --> 00:02:58.380
So we can write those down and then stare at them for a little while.

