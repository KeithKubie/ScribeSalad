WEBVTT
Kind: captions
Language: en

00:00:00.350 --> 00:00:02.390
Let's talk about
Dimensionality Reduction.

00:00:02.390 --> 00:00:04.770
So, here,
I've got a bunch of points, okay?

00:00:04.770 --> 00:00:07.300
And these points are in 2D, all right?

00:00:07.300 --> 00:00:09.860
It's called kind of green and
red, but that's sort of a joke.

00:00:09.860 --> 00:00:12.320
We're not going to worry too
much about why it was done that.

00:00:12.320 --> 00:00:14.960
What you should notice is
that in the middle here

00:00:14.960 --> 00:00:17.260
are these orangeish yellowish points.

00:00:17.260 --> 00:00:19.160
Do you see why it's R and G?

00:00:19.160 --> 00:00:21.890
Red plus green makes sort of yellowish,
or so, so

00:00:21.890 --> 00:00:24.580
along that diagonal
it'll be kind of yellow.

00:00:24.580 --> 00:00:27.190
I don't know who did this originally,
but it's kind of cool.

00:00:27.190 --> 00:00:29.370
All right,
if you think about these orange points,

00:00:29.370 --> 00:00:32.600
let's think about how
they're distributed, okay?

00:00:32.600 --> 00:00:37.140
So, they have a have a mean sit
somewhere, and they've got a.

00:00:37.140 --> 00:00:38.930
Set of eigenvectors, right?

00:00:38.930 --> 00:00:41.710
They've got a co-variants
matrix that describes them.

00:00:41.710 --> 00:00:45.570
They have a axis of least inertia and
then the next axis.

00:00:45.570 --> 00:00:47.120
So here you're seeing the points,

00:00:47.120 --> 00:00:51.790
the orange points are x, x bar here
is supposed to be their, their mean.

00:00:51.790 --> 00:00:56.570
And then we've got the big eigenvector
v1 and then the smaller one v2.

00:00:56.570 --> 00:00:59.900
And the idea is.

00:00:59.900 --> 00:01:05.170
We can represent those orange points

00:01:05.170 --> 00:01:09.140
by only their v1 coordinates,
plus the mean.

00:01:09.140 --> 00:01:11.640
And what that would mean is haha.

00:01:11.640 --> 00:01:14.010
Essentially that we're going to

00:01:14.010 --> 00:01:17.620
think of all the orange points
as just being on that line.

00:01:17.620 --> 00:01:21.180
And all I'm going to tell you
is where on the line they are.

00:01:21.180 --> 00:01:26.060
And we're going to essentially ignore
the amount that they're off that line.

00:01:27.080 --> 00:01:30.440
So we've just reduced
the dimensions from how many?

00:01:30.440 --> 00:01:31.170
Two.

00:01:31.170 --> 00:01:32.420
To how many?

00:01:32.420 --> 00:01:33.110
One.

00:01:33.110 --> 00:01:36.040
Okay, that doesn't sound like
that big a deal, nor is it.

00:01:36.040 --> 00:01:38.270
But in higher dimensions,
this could be a huge deal.

00:01:38.270 --> 00:01:42.220
Imagine you've got something in
10,000-dimensional space, and yes,

00:01:42.220 --> 00:01:45.130
in just a minute, we're going to
do a 10,000-dimensional space.

00:01:45.130 --> 00:01:48.960
If you said, well, I'm going to
represent them by one number, or

00:01:48.960 --> 00:01:51.890
even 30, that would be a huge reduction.

00:01:51.890 --> 00:01:54.150
So if I say, well,
what direction does it vary the most in?

00:01:54.150 --> 00:01:56.630
And I just give you that value.

00:01:56.630 --> 00:01:59.350
If that's good enough for
what we want to do, you've reduced

00:01:59.350 --> 00:02:02.660
the description from being a lot of
numbers to being a much smaller number.

00:02:03.780 --> 00:02:08.000
In fact, we can sort of express that
here algebraically in terms of just,

00:02:08.000 --> 00:02:10.810
thinking about it,
whatever dimension x happens to be in.

00:02:10.810 --> 00:02:14.850
So if I've got a whole bunch of data
points in some N-dimensional space,

00:02:14.850 --> 00:02:17.590
what I want to know is
the direction of projection.

00:02:17.590 --> 00:02:19.300
And we'll just say it's v.

00:02:19.300 --> 00:02:21.210
All right,
that if I projected those points,

00:02:21.210 --> 00:02:24.660
after subtracting out the mean,
that I'd have the greatest amount.

00:02:24.660 --> 00:02:26.370
Right, the greatest variation.

00:02:26.370 --> 00:02:28.210
And that's what that says here, right?

00:02:28.210 --> 00:02:30.975
So take x, subtract out the mean,
dotted with the v,

00:02:30.975 --> 00:02:33.190
summed over all the x's, take the norm.

00:02:33.190 --> 00:02:34.280
Take the square.

00:02:34.280 --> 00:02:37.170
Well that can be written as
an expression like this of just v

00:02:37.170 --> 00:02:41.580
transpose Av,
where A is just this outer product.

00:02:41.580 --> 00:02:45.760
Okay, that's the co-variants matrix
that we're, we're familiar with before.

00:02:45.760 --> 00:02:48.000
And as we said before.

00:02:48.000 --> 00:02:50.840
The eigenvector with
the largest eigenvalue lambda

00:02:50.840 --> 00:02:53.610
is going to be the one that
captures that greatest variation.

00:02:53.610 --> 00:02:55.990
In a minute I'll give you a little
argument about why it's the largest

00:02:55.990 --> 00:02:58.250
eigenvector with the largest eigenvalue.

00:02:58.250 --> 00:02:59.760
Or you can just take my word for it.

00:02:59.760 --> 00:03:03.650
And, in fact, the smallest eigenvalue
would be the least amount of dimension,

00:03:03.650 --> 00:03:06.110
so basically, what we're
going to have to do at some point

00:03:06.110 --> 00:03:09.760
is take the eigenvectors
of this co-variance matrix.

