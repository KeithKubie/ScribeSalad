WEBVTT
Kind: captions
Language: en

00:00:04.572 --> 00:00:06.030
WILLIAM VANBENEPE:
In this session,

00:00:06.030 --> 00:00:09.570
I'm going to describe
how Cloud is especially

00:00:09.570 --> 00:00:12.790
useful and convenient
for Big Data processing.

00:00:12.790 --> 00:00:17.130
I'm going to show how to
use Big Data the Cloud way.

00:00:17.130 --> 00:00:19.760
Some of you may think
Big Data is out of reach.

00:00:19.760 --> 00:00:21.920
I'll show you why
it's not the case.

00:00:21.920 --> 00:00:23.950
Some of you are probably
already using it.

00:00:23.950 --> 00:00:26.590
I'm going to show you how
you can get a lot more done

00:00:26.590 --> 00:00:29.144
for a lot less work.

00:00:29.144 --> 00:00:30.060
I'm William Vanbenepe.

00:00:30.060 --> 00:00:34.260
I'm a product manager in
Google Cloud Platform.

00:00:34.260 --> 00:00:36.302
When you think about
Big Data, these

00:00:36.302 --> 00:00:38.010
are some of the things
that come to mind.

00:00:38.010 --> 00:00:39.635
Obviously there's a
lot more than that.

00:00:39.635 --> 00:00:43.260
But these are the concepts
and tools that come to mind.

00:00:43.260 --> 00:00:46.920
But if you think about it, none
of those are about Big Data.

00:00:46.920 --> 00:00:50.600
None of those are about what
you're trying to get done.

00:00:50.600 --> 00:00:53.060
Those are constraints
of the tools.

00:00:53.060 --> 00:00:54.062
Those are limitations.

00:00:54.062 --> 00:00:55.520
Those are things
you have to learn,

00:00:55.520 --> 00:00:57.650
things you have to deal with.

00:00:57.650 --> 00:01:01.200
So today, when thinking about
how to run Big Data the Cloud

00:01:01.200 --> 00:01:05.000
way, I'm going to ask
you to forget about this.

00:01:05.000 --> 00:01:08.130
If you don't know about then,
don't assume you have to know

00:01:08.130 --> 00:01:11.220
and master them to use
Big Data efficiently.

00:01:11.220 --> 00:01:14.651
If you already know about
them, take a mental holiday.

00:01:14.651 --> 00:01:16.192
Or take a real
holiday-- even better.

00:01:16.192 --> 00:01:18.280
But at least a mental holiday.

00:01:18.280 --> 00:01:19.450
Clear your mind.

00:01:19.450 --> 00:01:22.560
You don't have to
actually forget all of it,

00:01:22.560 --> 00:01:25.250
but don't assume that these are
the starting points for any Big

00:01:25.250 --> 00:01:27.220
Data project.

00:01:27.220 --> 00:01:29.590
Instead, focus on
your application.

00:01:29.590 --> 00:01:32.510
Focus on what you're
trying to get done.

00:01:32.510 --> 00:01:34.300
And focusing on your
application really

00:01:34.300 --> 00:01:36.670
means focusing on the people.

00:01:36.670 --> 00:01:38.330
So let's pick an example.

00:01:38.330 --> 00:01:41.030
Let's say that you're
building a multi-player mobile

00:01:41.030 --> 00:01:45.630
application, a game, where
player interact in real life,

00:01:45.630 --> 00:01:47.840
where they also
interact in the game.

00:01:47.840 --> 00:01:48.979
And that's the application.

00:01:48.979 --> 00:01:50.520
That's what we're
trying to get done.

00:01:50.520 --> 00:01:52.610
Who are the people who matter?

00:01:52.610 --> 00:01:55.215
Obviously, first and
foremost, your users,

00:01:55.215 --> 00:01:58.376
on the upper left, people trying
to just want to use the game

00:01:58.376 --> 00:02:00.500
and have a good experience,
have interesting things

00:02:00.500 --> 00:02:02.040
happen to them.

00:02:02.040 --> 00:02:04.870
And then you have developers,
in charge of building the game.

00:02:04.870 --> 00:02:08.300
And because this is a game with
a lot of player interaction,

00:02:08.300 --> 00:02:10.009
part of what they
have to do is make

00:02:10.009 --> 00:02:12.740
sure those interactions happen
in a way that's meaningful

00:02:12.740 --> 00:02:14.130
and engaging.

00:02:14.130 --> 00:02:15.917
Of course, on the
flip side you don't

00:02:15.917 --> 00:02:17.000
want negative interaction.

00:02:17.000 --> 00:02:19.350
So you also have
to watch for abuse

00:02:19.350 --> 00:02:23.080
and make sure you
police the game.

00:02:23.080 --> 00:02:25.840
You also have people who
have more of a data science

00:02:25.840 --> 00:02:29.970
angle, who want to analyze
user engagement models,

00:02:29.970 --> 00:02:33.230
try to understand what motivates
users to come back to the game,

00:02:33.230 --> 00:02:36.220
and what happens if they lose
too many challenges, if they

00:02:36.220 --> 00:02:37.970
win too easily, if
they meet friends,

00:02:37.970 --> 00:02:39.330
if they don't meet friends.

00:02:39.330 --> 00:02:42.810
How does that impact
their tendency

00:02:42.810 --> 00:02:44.540
to come back to the game?

00:02:44.540 --> 00:02:47.310
And obviously, you have
people with business concerns,

00:02:47.310 --> 00:02:49.040
trying to understand,
how am I doing?

00:02:49.040 --> 00:02:50.440
How am I growing?

00:02:50.440 --> 00:02:52.840
Am I investing in the
right marketing channel

00:02:52.840 --> 00:02:54.440
to bring the right users?

00:02:54.440 --> 00:02:55.850
Are my users staying with me?

00:02:55.850 --> 00:02:58.120
And just understanding
what the revenue streams

00:02:58.120 --> 00:02:59.800
are and things like that.

00:02:59.800 --> 00:03:01.290
So those are the
people who matter.

00:03:01.290 --> 00:03:03.206
And these are the things
they're trying to do.

00:03:03.206 --> 00:03:05.560
None of those people say, I
want to work with Big Data.

00:03:05.560 --> 00:03:08.910
What they say is, I
want to do those things.

00:03:08.910 --> 00:03:12.205
So let's show how using
Big Data the Cloud

00:03:12.205 --> 00:03:14.750
way we can make it really
easy for those things

00:03:14.750 --> 00:03:16.850
to happen for these people.

00:03:16.850 --> 00:03:20.640
Starting with this events--
users playing on the game.

00:03:20.640 --> 00:03:21.530
Things happen.

00:03:21.530 --> 00:03:24.470
Events, either
[INAUDIBLE] are emitted.

00:03:24.470 --> 00:03:26.880
First thing to do is
capture the events.

00:03:26.880 --> 00:03:29.330
And for that we're
introducing today

00:03:29.330 --> 00:03:34.140
a new Cloud service
called Cloud Pub/Sub.

00:03:34.140 --> 00:03:38.630
Cloud Pub/Sub is a reliable
many-to-many asynchronous

00:03:38.630 --> 00:03:40.900
message delivery system.

00:03:40.900 --> 00:03:42.770
So just by the
name, Cloud Pub/Sub,

00:03:42.770 --> 00:03:45.540
I assume some of you in the
room already know what it does.

00:03:45.540 --> 00:03:48.830
It's a well-known
architectural pattern.

00:03:48.830 --> 00:03:51.380
It's widely used
in some domains.

00:03:51.380 --> 00:03:53.980
It's very convenient from an
architectural perspective.

00:03:53.980 --> 00:03:57.860
It makes your architecture
easy, simple, and also easy

00:03:57.860 --> 00:03:59.030
to evolve.

00:03:59.030 --> 00:04:03.000
And yet it's not used as
broadly as you would think,

00:04:03.000 --> 00:04:05.030
because it also comes at a cost.

00:04:05.030 --> 00:04:09.260
It comes at a cost of
relying on an infrastructure

00:04:09.260 --> 00:04:11.870
to manage that
stream of messages.

00:04:11.870 --> 00:04:14.050
And that can become a
single point of failure.

00:04:14.050 --> 00:04:16.700
And that can become a
bottleneck in your application.

00:04:16.700 --> 00:04:19.410
So for that reason, that
very convenient pattern

00:04:19.410 --> 00:04:21.470
is not as widely
used as it could.

00:04:21.470 --> 00:04:24.880
Today with Cloud Pub/Sub
we are taking the burden

00:04:24.880 --> 00:04:27.350
of running and managing
the system for you.

00:04:27.350 --> 00:04:30.130
So you can benefit from the
simplicity of the pattern

00:04:30.130 --> 00:04:32.860
without paying the cost
and the risk of having

00:04:32.860 --> 00:04:34.820
to manage the infrastructure.

00:04:34.820 --> 00:04:38.180
Cloud Pub/Sub is a global
service delivered and managed

00:04:38.180 --> 00:04:39.060
by Google.

00:04:39.060 --> 00:04:41.235
When you start using
it we don't ask you,

00:04:41.235 --> 00:04:44.590
do want to be located on the
Easy Coast, on the West Coast?

00:04:44.590 --> 00:04:47.500
It's one single
Global Entry point,

00:04:47.500 --> 00:04:51.650
supported by Google's
global infrastructure.

00:04:51.650 --> 00:04:53.690
It scales to just what you need.

00:04:53.690 --> 00:04:58.460
So if you saw the keynote this
morning about Cloud data flow,

00:04:58.460 --> 00:05:00.340
because we had so little
time in the keynote,

00:05:00.340 --> 00:05:01.798
we didn't describe
the [INAUDIBLE].

00:05:01.798 --> 00:05:04.720
But actually that demo had
Cloud Sub/Pub in the front,

00:05:04.720 --> 00:05:08.150
ingesting the tweets
that were being analyzed,

00:05:08.150 --> 00:05:10.465
that's the demo
about the World Cup.

00:05:10.465 --> 00:05:11.840
Something like
the World Cup, you

00:05:11.840 --> 00:05:13.256
can imagine when
the goal happened

00:05:13.256 --> 00:05:15.810
you had a spike in
traffic that goes down.

00:05:15.810 --> 00:05:17.690
We're not talking
a two-month spike.

00:05:17.690 --> 00:05:19.620
We're talking like
a two-minute spike.

00:05:19.620 --> 00:05:22.200
So these are the kind of
things that normally you

00:05:22.200 --> 00:05:23.830
would have to provision for.

00:05:23.830 --> 00:05:25.750
Or you would just
be running behind.

00:05:25.750 --> 00:05:28.050
And when you don't need it
you would be overpaying.

00:05:28.050 --> 00:05:31.160
Cloud Pub/Sub scales
to just what you need.

00:05:31.160 --> 00:05:32.450
You don't have to manage it.

00:05:32.450 --> 00:05:33.810
And you pay just what you need.

00:05:33.810 --> 00:05:36.510
And that's Big
Data the Cloud way.

00:05:36.510 --> 00:05:39.340
So looking through a bit
more detail about how you use

00:05:39.340 --> 00:05:42.710
Cloud Pub/Sub, at the
heart of if are topics.

00:05:42.710 --> 00:05:45.550
Topics are what you define,
what you care about.

00:05:45.550 --> 00:05:48.410
You say, I'm going to
create a new topic, topic

00:05:48.410 --> 00:05:50.060
of first-class resources.

00:05:50.060 --> 00:05:52.480
So they have access
control lists.

00:05:52.480 --> 00:05:55.250
So you can give permission
to users in an application

00:05:55.250 --> 00:05:58.520
to post to a topic, and
give permission to others

00:05:58.520 --> 00:06:01.320
to subscribe to the
topic and read from it.

00:06:01.320 --> 00:06:03.510
Obviously I'm not
breaking some trade secret

00:06:03.510 --> 00:06:06.150
by saying that there are
computers under the cover, many

00:06:06.150 --> 00:06:07.660
of them powering that.

00:06:07.660 --> 00:06:09.400
But they're not your concerns.

00:06:09.400 --> 00:06:13.340
What you worry about
are topic and messages.

00:06:13.340 --> 00:06:14.530
Cloud Pub/Sub is durable.

00:06:14.530 --> 00:06:18.770
So if your subscribers are
available at the time when

00:06:18.770 --> 00:06:21.350
the message comes through,
it gets immediately delivered

00:06:21.350 --> 00:06:22.090
to them.

00:06:22.090 --> 00:06:24.530
If they're not, the
messages are safely

00:06:24.530 --> 00:06:28.570
persisted until they come up
and all the subscribers have

00:06:28.570 --> 00:06:30.930
consumed and acknowledged
the messages.

00:06:30.930 --> 00:06:33.600
Scalable, reliable
with no management.

00:06:33.600 --> 00:06:36.950
That's the Cloud way.

00:06:36.950 --> 00:06:39.705
It's not just that the
system itself is resilient,

00:06:39.705 --> 00:06:41.360
it's that it makes
the architecture

00:06:41.360 --> 00:06:43.550
resilient and
resilient to change.

00:06:43.550 --> 00:06:46.829
So in our example, in the
early days of the company

00:06:46.829 --> 00:06:48.370
the only thing that
matters is having

00:06:48.370 --> 00:06:49.453
a game that can be played.

00:06:49.453 --> 00:06:53.750
There is no time for analytics
or any other niceties.

00:06:53.750 --> 00:06:56.940
And yet you know that there is
value in those early events,

00:06:56.940 --> 00:06:59.130
in those early logs
that are coming out.

00:06:59.130 --> 00:07:02.040
So what you can do is just
create a Pub/Sub topic.

00:07:02.040 --> 00:07:05.720
Have all your application
clients just post events there.

00:07:05.720 --> 00:07:09.090
And have a very simple consumer
just to write to storage,

00:07:09.090 --> 00:07:10.420
and leave it there.

00:07:10.420 --> 00:07:14.350
And you can forget about it, and
you're safely storing the data.

00:07:14.350 --> 00:07:16.870
And then as you mature,
as you need evolve,

00:07:16.870 --> 00:07:19.050
as you have a bit
more breathing room,

00:07:19.050 --> 00:07:20.530
time comes for you
to do something

00:07:20.530 --> 00:07:21.880
useful with those events.

00:07:21.880 --> 00:07:24.320
And at that point, you
can create an application

00:07:24.320 --> 00:07:28.260
which is going to register to
the topic and consume events.

00:07:28.260 --> 00:07:31.060
And by doing that you're
not changing anything else.

00:07:31.060 --> 00:07:32.520
It's a very safe move.

00:07:32.520 --> 00:07:33.900
You're not breaking anything.

00:07:33.900 --> 00:07:35.540
You're not modifying anything.

00:07:35.540 --> 00:07:37.890
On the left side you don't
have to change anything

00:07:37.890 --> 00:07:38.810
to your publishers.

00:07:38.810 --> 00:07:42.230
You don't have to update the
application and deal with half

00:07:42.230 --> 00:07:44.300
your users having the
new version, half of them

00:07:44.300 --> 00:07:45.640
having the old version.

00:07:45.640 --> 00:07:47.490
Or even worse, in
some cases you're

00:07:47.490 --> 00:07:49.850
publishing from sensors
or devices embedded

00:07:49.850 --> 00:07:52.560
in the field, which are
almost impossible to update.

00:07:52.560 --> 00:07:54.159
No update on the left side.

00:07:54.159 --> 00:07:55.450
You're not disrupting anything.

00:07:55.450 --> 00:07:58.740
You're just adding one consumer.

00:07:58.740 --> 00:08:03.120
And as time goes, as you evolve,
it's a very flexible pattern.

00:08:03.120 --> 00:08:06.560
So you can add more consumers,
many consumers to one topic.

00:08:06.560 --> 00:08:08.440
You can even chain consumers.

00:08:08.440 --> 00:08:11.060
So for example, you
could have one consumer,

00:08:11.060 --> 00:08:15.230
which is trusted to see the raw
data coming from the players.

00:08:15.230 --> 00:08:17.690
And it removes
confidential information.

00:08:17.690 --> 00:08:21.030
It enriches, cleans,
filters, any preprocessing.

00:08:21.030 --> 00:08:23.760
And it can republish
to a second topic.

00:08:23.760 --> 00:08:25.930
And then that second
topic can be the one

00:08:25.930 --> 00:08:29.450
that you open up to your
larger set of users,

00:08:29.450 --> 00:08:34.120
for them to run analytics
or other development tasks.

00:08:34.120 --> 00:08:36.679
As you evolve over time,
as your needs evolve,

00:08:36.679 --> 00:08:40.260
it's a very powerful
architectural pattern.

00:08:40.260 --> 00:08:45.110
And now we give you the tool
that lets you use it safely.

00:08:45.110 --> 00:08:46.890
So we've been talking
about applications

00:08:46.890 --> 00:08:48.556
without saying much
about what they are.

00:08:48.556 --> 00:08:51.060
What are those applications
that consume events

00:08:51.060 --> 00:08:52.950
from Cloud Pub/Sub?

00:08:52.950 --> 00:08:54.580
Well, they can be
anything, really.

00:08:54.580 --> 00:08:57.380
It can be an application
on Google Compute Engine.

00:08:57.380 --> 00:08:59.460
Could be an application
on Google App Engine.

00:08:59.460 --> 00:09:01.210
It could be an application
on this laptop.

00:09:01.210 --> 00:09:02.710
It could be in your data center.

00:09:02.710 --> 00:09:05.510
It's a very open and
versatile service

00:09:05.510 --> 00:09:07.750
you can consume from anywhere.

00:09:07.750 --> 00:09:10.970
Today, for the purpose
of this session,

00:09:10.970 --> 00:09:14.040
we're going to focus on one
special case, one special kind

00:09:14.040 --> 00:09:18.900
of consumers, which is
data processing pipeline.

00:09:18.900 --> 00:09:21.380
Because remember,
what we really want

00:09:21.380 --> 00:09:25.760
is to allow developers to
focus on the application,

00:09:25.760 --> 00:09:28.330
not on the infrastructure.

00:09:28.330 --> 00:09:31.690
And in our example, many of the
use cases, what they call for

00:09:31.690 --> 00:09:33.150
is a data processing pipeline.

00:09:33.150 --> 00:09:37.860
So looking on the upper right,
managing user interactions,

00:09:37.860 --> 00:09:40.600
so making sure that users
who are in the same team,

00:09:40.600 --> 00:09:43.660
in the same location, who
have similar objectives,

00:09:43.660 --> 00:09:47.090
can find one another
and play together.

00:09:47.090 --> 00:09:49.470
Detecting abuse, making
sure people aren't harassing

00:09:49.470 --> 00:09:52.320
one another, analyzing
user engagement,

00:09:52.320 --> 00:09:54.710
all those things
are best expressed

00:09:54.710 --> 00:09:56.680
as data processing pipeline.

00:09:56.680 --> 00:09:59.600
And that's why we're introducing
today another service, which

00:09:59.600 --> 00:10:03.700
you saw in the keynote this
morning, Google Cloud Dataflow.

00:10:03.700 --> 00:10:06.870
Google Cloud Dataflow
is the right tool

00:10:06.870 --> 00:10:08.200
for data processing pipeline.

00:10:08.200 --> 00:10:11.090
It lets you write your
processing pipeline

00:10:11.090 --> 00:10:13.450
as one simple Java program.

00:10:13.450 --> 00:10:14.699
The initial version is Java.

00:10:14.699 --> 00:10:16.740
The service itself is
actually language agnostic.

00:10:16.740 --> 00:10:19.460
But the first SDK is a Java one.

00:10:19.460 --> 00:10:22.377
And then you let us,
you let Google deal

00:10:22.377 --> 00:10:24.960
with everything that's needed
to run that processing pipeline,

00:10:24.960 --> 00:10:29.502
with all deployment, monitoring,
tuning, configuration tasks.

00:10:29.502 --> 00:10:32.540
As [INAUDIBLE]
mentioned this morning,

00:10:32.540 --> 00:10:35.140
this has replaced
[? My Produce ?] at Google.

00:10:35.140 --> 00:10:38.270
It's actually third generation.

00:10:38.270 --> 00:10:40.720
There's a complex family tree
of data processing products

00:10:40.720 --> 00:10:41.540
at Google.

00:10:41.540 --> 00:10:44.070
This is replacing
the service which

00:10:44.070 --> 00:10:45.940
replaced [? My Produce ?]
at Google.

00:10:45.940 --> 00:10:48.580
So this is something that,
even though it's a new Cloud

00:10:48.580 --> 00:10:51.930
service, it's a very
mature data processing

00:10:51.930 --> 00:10:55.200
tool that we've been
using internally.

00:10:55.200 --> 00:10:58.410
So let's find out a bit
more about what it does.

00:10:58.410 --> 00:11:01.680
This is your application in
many cases, like in our example,

00:11:01.680 --> 00:11:07.880
a constant stream of events
happening to the system,

00:11:07.880 --> 00:11:09.840
in our case, going
through Cloud [INAUDIBLE].

00:11:09.840 --> 00:11:11.940
In many cases, this
is the shape in which

00:11:11.940 --> 00:11:13.110
you want to consume them.

00:11:13.110 --> 00:11:16.370
You want to handle them in
real time as they come through.

00:11:16.370 --> 00:11:19.460
Because remember, we're
talking about user interaction.

00:11:19.460 --> 00:11:24.380
It's not very useful if I get a
notification at 4:00 AM telling

00:11:24.380 --> 00:11:27.480
me, last afternoon you
and two team members

00:11:27.480 --> 00:11:28.576
were in the same location.

00:11:28.576 --> 00:11:30.450
You could have mounted
an operation together,

00:11:30.450 --> 00:11:31.408
or something like that.

00:11:31.408 --> 00:11:32.720
It's too late.

00:11:32.720 --> 00:11:34.230
Same thing with abuse.

00:11:34.230 --> 00:11:37.340
If you find people who are
harrassing others, or people--

00:11:37.340 --> 00:11:39.690
somebody reverse
engineered your client,

00:11:39.690 --> 00:11:41.940
and they're playing
programmatically in the game

00:11:41.940 --> 00:11:43.270
and creating havoc.

00:11:43.270 --> 00:11:45.580
If you find them 24
hours later, they've

00:11:45.580 --> 00:11:47.590
had hours to make
other miserable.

00:11:47.590 --> 00:11:49.870
And those users are
not coming back.

00:11:49.870 --> 00:11:54.140
So in those cases, you want to
analyze on the fly, right away,

00:11:54.140 --> 00:11:55.960
and take action right away.

00:11:55.960 --> 00:11:58.570
And that's why Cloud
data flow supports

00:11:58.570 --> 00:12:00.790
running in streaming mode.

00:12:00.790 --> 00:12:04.240
In that mode we
let you tell us how

00:12:04.240 --> 00:12:06.810
you want events
grouped and aggregated.

00:12:06.810 --> 00:12:08.400
We make that happen for you.

00:12:08.400 --> 00:12:09.690
We call them buckets.

00:12:09.690 --> 00:12:13.160
We let you tell us if you want
things bucketized by session ID

00:12:13.160 --> 00:12:15.710
for web traffic, by time
window, either fixed

00:12:15.710 --> 00:12:18.490
window or sliding window,
in the case of the game,

00:12:18.490 --> 00:12:20.760
probably by team, by location.

00:12:20.760 --> 00:12:23.670
You tell us how you
want events aggregated.

00:12:23.670 --> 00:12:26.700
We make sure that those
buckets are created for you,

00:12:26.700 --> 00:12:30.930
that your application logic
is processed on the content.

00:12:30.930 --> 00:12:34.720
And as new events come in and
as all the events age out,

00:12:34.720 --> 00:12:37.405
we take care of keeping
the processing always fresh

00:12:37.405 --> 00:12:40.290
and up to date.

00:12:40.290 --> 00:12:42.080
There are other
cases where you want

00:12:42.080 --> 00:12:44.220
to run against
historical data, so

00:12:44.220 --> 00:12:47.300
taking the example of
user engagement models.

00:12:47.300 --> 00:12:50.340
When trying to understand
how user behave and react

00:12:50.340 --> 00:12:52.350
to things happening
in the game, you

00:12:52.350 --> 00:12:54.240
want to benefit
from weeks, months,

00:12:54.240 --> 00:12:58.240
years of historical information,
where you've seen the impact.

00:12:58.240 --> 00:13:00.760
And so for that,
Cloud Dataflow lets

00:13:00.760 --> 00:13:02.670
you run your pipeline
in batch mode

00:13:02.670 --> 00:13:05.972
and go against all
this historical data.

00:13:05.972 --> 00:13:07.930
And the interesting thing
is that it's actually

00:13:07.930 --> 00:13:10.640
the same program which
is running in batch mode

00:13:10.640 --> 00:13:11.770
and in streaming mode.

00:13:11.770 --> 00:13:13.090
The changes are very minute.

00:13:13.090 --> 00:13:15.510
There are some changes
to how some of the IU

00:13:15.510 --> 00:13:20.180
operates, to switch from reading
files or tables to reading

00:13:20.180 --> 00:13:22.920
streams, voice input and output.

00:13:22.920 --> 00:13:24.960
And then you've
configuring your bucket.

00:13:24.960 --> 00:13:29.030
Other than that, the actual
algorithm that you're running

00:13:29.030 --> 00:13:31.930
is the exact same in both cases.

00:13:31.930 --> 00:13:33.400
And that's very powerful.

00:13:33.400 --> 00:13:36.920
That means that, in the case
of the user engagement model,

00:13:36.920 --> 00:13:41.520
once you have created your
model about how users react,

00:13:41.520 --> 00:13:44.580
you can take the same mobile
and run it in stream mode

00:13:44.580 --> 00:13:47.780
so you can see in real time
as you adjust parameters

00:13:47.780 --> 00:13:50.266
of the game, or
as new users come.

00:13:50.266 --> 00:13:51.640
You may have a
marketing campaign

00:13:51.640 --> 00:13:53.530
that brings a new kind of users.

00:13:53.530 --> 00:13:55.820
As those things
change, you can see

00:13:55.820 --> 00:13:59.150
how your user behavior
compares to what you expect.

00:13:59.150 --> 00:14:02.240
And you can make
adjustments on the fly.

00:14:02.240 --> 00:14:03.750
Another use case
for which we'd want

00:14:03.750 --> 00:14:06.650
to run the same pipeline
in by batch and stream

00:14:06.650 --> 00:14:09.500
is the case where
you make changes

00:14:09.500 --> 00:14:11.010
to your pricing algorithm.

00:14:11.010 --> 00:14:14.160
So you've been pricing events
as they occur, and then

00:14:14.160 --> 00:14:15.830
you realize you
have a bug or you

00:14:15.830 --> 00:14:19.020
have an idea for a new feature,
just additional insight

00:14:19.020 --> 00:14:20.750
that you're trying to extract.

00:14:20.750 --> 00:14:22.620
At that point, you can
change your pipeline,

00:14:22.620 --> 00:14:25.040
and all the new events
will get processed

00:14:25.040 --> 00:14:26.650
with the new algorithm.

00:14:26.650 --> 00:14:29.270
But the old ones aren't
comparable anymore.

00:14:29.270 --> 00:14:31.300
What you'd like to
do is reprocess them.

00:14:31.300 --> 00:14:34.430
And that's where running the
same pipeline in stream mode

00:14:34.430 --> 00:14:36.240
and in batch mode
is very useful,

00:14:36.240 --> 00:14:39.015
because you get to reprocess
as much of your historical data

00:14:39.015 --> 00:14:40.190
as you want.

00:14:40.190 --> 00:14:42.220
And of course, because
you're in the Cloud,

00:14:42.220 --> 00:14:44.040
you have capacity
available to do that.

00:14:44.040 --> 00:14:47.080
You don't have to worry
about taking capacity away

00:14:47.080 --> 00:14:48.810
from supporting
new events in order

00:14:48.810 --> 00:14:50.090
to manage the reprocessing.

00:14:50.090 --> 00:14:51.990
You just get what you need.

00:14:51.990 --> 00:14:53.662
That is Big Data the Cloud way.

00:14:56.489 --> 00:14:58.280
So I mentioned, and
[? Ors ?] said earlier,

00:14:58.280 --> 00:15:02.260
this product is based on
10 years of parallel data

00:15:02.260 --> 00:15:04.290
processing research at Google.

00:15:04.290 --> 00:15:07.770
And when I say research, I
mean in a very applied way.

00:15:07.770 --> 00:15:10.190
Pretty much all the Google
product you use every day

00:15:10.190 --> 00:15:12.090
are based on these technologies.

00:15:12.090 --> 00:15:14.020
So even though it's
a new Cloud product,

00:15:14.020 --> 00:15:16.690
there's a lot of
maturity internally.

00:15:16.690 --> 00:15:18.710
And that maturity
doesn't just show up

00:15:18.710 --> 00:15:21.980
in the capabilities in the
APIs that we expose to you.

00:15:21.980 --> 00:15:24.080
A lot of it is under the cover.

00:15:24.080 --> 00:15:27.820
It's in our ability to take
your pricing pipeline, which

00:15:27.820 --> 00:15:30.760
is expressed in a very
simple, intuitive way,

00:15:30.760 --> 00:15:33.750
and turn that into a
processing pipeline, which

00:15:33.750 --> 00:15:35.920
is very efficient to execute.

00:15:35.920 --> 00:15:39.690
And then, as we execute the
pipeline on your behalf,

00:15:39.690 --> 00:15:42.200
we take care of all the
work that you shouldn't

00:15:42.200 --> 00:15:45.590
have to worry about--
starting machines, starting

00:15:45.590 --> 00:15:50.120
the right number of machines,
monitoring, tuning, babysitting

00:15:50.120 --> 00:15:53.630
in cases where some workers
have a problem restarting work.

00:15:53.630 --> 00:15:58.810
All that is done for you,
buyers, in an optimized way.

00:15:58.810 --> 00:16:02.260
There's a session in this same
room tomorrow at 10:00 AM which

00:16:02.260 --> 00:16:04.850
is dedicated entirely
to Cloud Dataflow.

00:16:04.850 --> 00:16:06.060
Shows you a lot more detail.

00:16:06.060 --> 00:16:07.440
We'll show you
some code as well.

00:16:07.440 --> 00:16:09.170
So please come
back here at 10:00

00:16:09.170 --> 00:16:13.910
AM and you'll find out a
lot more about how it works.

00:16:13.910 --> 00:16:16.820
One of the things with those
highly optimized systems

00:16:16.820 --> 00:16:19.510
is that it's really hard
to know what's going on.

00:16:19.510 --> 00:16:22.230
So if everything goes
well, you're happy.

00:16:22.230 --> 00:16:24.630
As soon as you're trying to
understand what's happening,

00:16:24.630 --> 00:16:26.730
either because
there's a problem,

00:16:26.730 --> 00:16:29.000
there's a bug in your code
or there's a bottleneck

00:16:29.000 --> 00:16:32.480
that you're trying to
eliminate by writing

00:16:32.480 --> 00:16:34.620
your code differently,
then you're

00:16:34.620 --> 00:16:39.065
left with the task of assembling
all the logs, to finding them

00:16:39.065 --> 00:16:40.440
and trying to make
sense of them.

00:16:40.440 --> 00:16:42.990
Even once you've found all
the logs and brought them,

00:16:42.990 --> 00:16:44.730
they're expressed,
in many cases,

00:16:44.730 --> 00:16:47.340
in terms of the
optimized execution

00:16:47.340 --> 00:16:48.950
plan, which makes
no sense to you.

00:16:48.950 --> 00:16:50.630
That's not a code you wrote.

00:16:50.630 --> 00:16:54.150
So for Cloud Dataflow we've
made sure not only to collect

00:16:54.150 --> 00:16:57.100
all the relevant logs and matrix
for you when we present them,

00:16:57.100 --> 00:17:00.760
but to do that in a way that
matches directly to the way

00:17:00.760 --> 00:17:02.700
you have defined your pipeline.

00:17:02.700 --> 00:17:05.250
So every step,
every task described

00:17:05.250 --> 00:17:09.089
here corresponds to some
line of code in your code

00:17:09.089 --> 00:17:10.190
that you recognize.

00:17:10.190 --> 00:17:11.874
It's understandable
and it's actionable.

00:17:15.030 --> 00:17:16.640
So these are the
two new products

00:17:16.640 --> 00:17:19.690
that we are introducing
today, Cloud Pub/Sub

00:17:19.690 --> 00:17:22.839
in limited preview, and
Cloud Dataflow, currently

00:17:22.839 --> 00:17:24.450
in private beta.

00:17:24.450 --> 00:17:27.650
What about this green cylinder?

00:17:27.650 --> 00:17:30.710
It represents all the
Cloud storage services

00:17:30.710 --> 00:17:33.640
at Google, of which
there are many.

00:17:33.640 --> 00:17:36.350
Today I'm going to focus
specifically on one of them,

00:17:36.350 --> 00:17:36.850
BigQuery.

00:17:36.850 --> 00:17:39.710
That's the one that's the most
relevant for large scale data

00:17:39.710 --> 00:17:41.360
analytics.

00:17:41.360 --> 00:17:45.150
So unlike Cloud
Dataflow and Pub/Sub,

00:17:45.150 --> 00:17:46.580
BigQuery is not a new service.

00:17:46.580 --> 00:17:48.120
It's been out for
a couple of years.

00:17:48.120 --> 00:17:50.010
Some of you I'm
sure know about it.

00:17:50.010 --> 00:17:51.420
Some of you use it.

00:17:51.420 --> 00:17:54.260
You may not be aware of the
latest improvement, though.

00:17:54.260 --> 00:17:56.510
For those who are not
familiar with BigQuery,

00:17:56.510 --> 00:17:59.990
it's a large-scale SQL
based analytics engine.

00:17:59.990 --> 00:18:01.940
You bring your data.

00:18:01.940 --> 00:18:04.450
And then you send
queries that we

00:18:04.450 --> 00:18:07.510
execute on the data
you've brought.

00:18:07.510 --> 00:18:10.040
So once again, it's a
fully managed service.

00:18:10.040 --> 00:18:12.760
You're not responsible for
deploying, tuning, managing

00:18:12.760 --> 00:18:14.670
a cluster.

00:18:14.670 --> 00:18:18.830
The storage is handled
separately from the processing.

00:18:18.830 --> 00:18:21.220
So if you store data
and do not query it,

00:18:21.220 --> 00:18:22.170
that's perfectly fine.

00:18:22.170 --> 00:18:25.070
You don't have to pay for an
unused cluster which is just

00:18:25.070 --> 00:18:27.150
sitting around to store data.

00:18:27.150 --> 00:18:29.140
You only pay for
storage at the same rate

00:18:29.140 --> 00:18:31.440
as Google Cloud Storage.

00:18:31.440 --> 00:18:32.870
When you want to
process, you have

00:18:32.870 --> 00:18:34.480
an engine at your
disposal, which

00:18:34.480 --> 00:18:38.560
is capable of processing
terrabytes of data in seconds

00:18:38.560 --> 00:18:41.070
and gives you an
interactive experience

00:18:41.070 --> 00:18:42.105
for real time analytics.

00:18:44.790 --> 00:18:48.470
That flexibility in the model
between storage and consumption

00:18:48.470 --> 00:18:51.800
becomes very clear in cases
such as public data sets.

00:18:51.800 --> 00:18:55.150
So some of you may have
heard, just last week,

00:18:55.150 --> 00:18:58.220
that some data was released
from New York City around all

00:18:58.220 --> 00:19:02.340
the taxi cab rides in 2013,
I think a bit under 200

00:19:02.340 --> 00:19:06.060
million taxi cab rides,
and pretty detailed data

00:19:06.060 --> 00:19:09.230
including starting point,
ending point, price, tip,

00:19:09.230 --> 00:19:10.430
time of the day.

00:19:10.430 --> 00:19:12.950
That was released, and somebody
just put it in BigQuery

00:19:12.950 --> 00:19:14.540
as a public data set.

00:19:14.540 --> 00:19:16.570
If that happened
anywhere else and you

00:19:16.570 --> 00:19:18.652
wanted to use that,
you would have to say,

00:19:18.652 --> 00:19:19.860
well, thank you for the data.

00:19:19.860 --> 00:19:21.640
Let me set up a cluster.

00:19:21.640 --> 00:19:23.290
Let me import your data.

00:19:23.290 --> 00:19:24.530
And then I can work.

00:19:24.530 --> 00:19:26.940
And by that point I've
paid already a couple hours

00:19:26.940 --> 00:19:28.980
of cluster [? pricing ?] time.

00:19:28.980 --> 00:19:31.030
With BigQuery all
you have to do is

00:19:31.030 --> 00:19:33.720
go to the web console,
enter a SQL query,

00:19:33.720 --> 00:19:35.700
and in seconds you
get your results.

00:19:35.700 --> 00:19:38.340
And you can find
out that December

00:19:38.340 --> 00:19:41.060
is the month where tips
are the highest in average.

00:19:41.060 --> 00:19:42.680
Maybe not very surprising.

00:19:42.680 --> 00:19:44.070
In seconds you're in business.

00:19:44.070 --> 00:19:46.540
And you're only paying for
that query or the query

00:19:46.540 --> 00:19:47.760
that you decide to run.

00:19:47.760 --> 00:19:49.710
And by the way, the
first terabyte of queries

00:19:49.710 --> 00:19:53.042
is free every month.

00:19:53.042 --> 00:19:54.500
Going to more detail
into BigQuery,

00:19:54.500 --> 00:19:57.340
especially for those of you
who have used the product

00:19:57.340 --> 00:20:01.010
but may not have seen all
the recent announcement,

00:20:01.010 --> 00:20:04.660
it supports a very rich and
flexible ingestion model.

00:20:04.660 --> 00:20:06.540
So there is trimming ingestion.

00:20:06.540 --> 00:20:07.870
That's what Dataflow uses.

00:20:07.870 --> 00:20:10.940
When Dataflow processes events
and puts them into BigQuery,

00:20:10.940 --> 00:20:12.900
it happens at a streaming API.

00:20:12.900 --> 00:20:17.060
Obviously you can load
files in batch mode.

00:20:17.060 --> 00:20:19.180
If you use Googly
Analytics premium,

00:20:19.180 --> 00:20:25.130
we have an integration where
we can take your unsampled user

00:20:25.130 --> 00:20:28.020
clicks data from Google
Analytics premium

00:20:28.020 --> 00:20:30.220
and put that for you
directly in BigQuery.

00:20:30.220 --> 00:20:32.980
So that data is available
for you to analyze any way

00:20:32.980 --> 00:20:33.900
you want.

00:20:33.900 --> 00:20:36.450
You can bring additional
data and join it

00:20:36.450 --> 00:20:40.551
with CRM data, marketing data,
third party demographic data,

00:20:40.551 --> 00:20:41.300
whatever you want.

00:20:41.300 --> 00:20:44.660
The data is yours, easily
accessible in BigQuery.

00:20:44.660 --> 00:20:47.580
We've also partnered with
many other companies who

00:20:47.580 --> 00:20:50.820
provide additional
sources of information,

00:20:50.820 --> 00:20:55.020
like social network information,
and also companies who provide

00:20:55.020 --> 00:20:59.400
tools like ETL tools to simplify
bringing data into BigQuery.

00:20:59.400 --> 00:21:01.260
So that's on the ingestion side.

00:21:01.260 --> 00:21:04.810
On the consumption side, it's
also a very flexible service

00:21:04.810 --> 00:21:07.680
with many ways to consume data.

00:21:07.680 --> 00:21:09.900
For example, you
can consume data

00:21:09.900 --> 00:21:11.980
from Cloud Dataflow,
as we saw earlier

00:21:11.980 --> 00:21:14.530
or from Hadoop, which
we'll discuss later.

00:21:14.530 --> 00:21:16.640
And to support that,
BigQuery allows

00:21:16.640 --> 00:21:18.610
data to be made
available in parallel

00:21:18.610 --> 00:21:21.500
to many workers
at the same time.

00:21:21.500 --> 00:21:23.830
There's also a simple
table read API.

00:21:23.830 --> 00:21:26.730
If you use Python, you can
use the pandas library,

00:21:26.730 --> 00:21:27.900
for example.

00:21:27.900 --> 00:21:30.560
There's ODBC support.

00:21:30.560 --> 00:21:33.850
But obviously out of all of
those, the most popular way

00:21:33.850 --> 00:21:36.720
to use data in BigQuery
is via the BigQuery engine

00:21:36.720 --> 00:21:40.610
itself, that powerful SQL
engine I described earlier.

00:21:40.610 --> 00:21:42.030
And it's not just SQL.

00:21:42.030 --> 00:21:43.620
In addition to
that, the engine has

00:21:43.620 --> 00:21:48.100
a lot of additional support for
more modern data pricing tasks,

00:21:48.100 --> 00:21:49.590
or more Big Data kind of tasks.

00:21:49.590 --> 00:21:52.820
So for example, it
supports nested records.

00:21:52.820 --> 00:21:55.440
It lets you query
inside JSONObject.

00:21:55.440 --> 00:21:58.060
If you have JSONObject
stored in the database

00:21:58.060 --> 00:22:00.640
you can query inside of
them during the query.

00:22:00.640 --> 00:22:01.520
It supports view.

00:22:01.520 --> 00:22:03.650
It supports regular expressions.

00:22:03.650 --> 00:22:06.240
And coming soon we will
have support for user

00:22:06.240 --> 00:22:07.690
defined functions in JavaScript.

00:22:13.560 --> 00:22:16.420
So here's our business user.

00:22:16.420 --> 00:22:18.900
In addition to
BigQuery, which gives

00:22:18.900 --> 00:22:21.800
him or her interactive
access using

00:22:21.800 --> 00:22:26.080
the dashboard to a large amount
of historical information,

00:22:26.080 --> 00:22:28.190
either a dashboard that
they did themselves

00:22:28.190 --> 00:22:30.250
or a dashboard from
our many partners,

00:22:30.250 --> 00:22:33.070
like Tableau and
Click and [INAUDIBLE]

00:22:33.070 --> 00:22:35.440
and others who've created
commercial dashboard

00:22:35.440 --> 00:22:37.780
with great visualization
[INAUDIBLE] Big Query.

00:22:37.780 --> 00:22:42.380
In addition to that interactive
view of historical information,

00:22:42.380 --> 00:22:44.910
thanks to Dataflow
working in streaming mode,

00:22:44.910 --> 00:22:47.640
the business user can
also get a real time view

00:22:47.640 --> 00:22:49.280
of how things are
going right now.

00:22:49.280 --> 00:22:53.240
So which are my top five
most popular games right now?

00:22:53.240 --> 00:22:55.540
What's my rate of
new users joining

00:22:55.540 --> 00:22:57.640
the game of what's
my attrition rate?

00:22:57.640 --> 00:23:00.000
All those can be
calculated in real time

00:23:00.000 --> 00:23:03.740
and also presented
for a full real time

00:23:03.740 --> 00:23:06.024
and historical business view.

00:23:06.024 --> 00:23:08.440
And obviously BigQuery is not
just for business analytics.

00:23:08.440 --> 00:23:10.070
That's just one example.

00:23:10.070 --> 00:23:12.590
It can also be
used by developers.

00:23:12.590 --> 00:23:14.630
Today a lot of developers
use [? hive, ?]

00:23:14.630 --> 00:23:18.560
for example for data processing
of structure data in SQL.

00:23:18.560 --> 00:23:21.210
BigQuery can be used
as a development tool

00:23:21.210 --> 00:23:25.290
to do just that, except much
faster and without any overhead

00:23:25.290 --> 00:23:27.080
or management complexity.

00:23:27.080 --> 00:23:30.710
And that's Big
Data the Cloud way.

00:23:30.710 --> 00:23:35.160
So I mentioned other storage
services on Cloud platform.

00:23:35.160 --> 00:23:37.850
I'm not going to go into
details of any of them.

00:23:37.850 --> 00:23:42.620
But they're Google Cloud Storage
for object store, Cloud data

00:23:42.620 --> 00:23:47.070
store, our managed NoSQL, and
Cloud SQL, a managed MySQL

00:23:47.070 --> 00:23:48.060
solution.

00:23:48.060 --> 00:23:50.930
They're not specific to
analytics use cases, obviously.

00:23:50.930 --> 00:23:52.990
But they are well
integrated in the platform

00:23:52.990 --> 00:23:54.640
and can play a role there too.

00:23:58.000 --> 00:23:59.640
Let's say your
developer has decided

00:23:59.640 --> 00:24:02.450
she wants to use
Pig to ingest data.

00:24:02.450 --> 00:24:05.930
I have some ETL pipeline for
which that's the right tool.

00:24:05.930 --> 00:24:07.940
Or maybe your data
scientist wants

00:24:07.940 --> 00:24:10.290
to run mission learning
algorithm using

00:24:10.290 --> 00:24:12.050
a library for Spark.

00:24:12.050 --> 00:24:15.820
Or he or she wants to
do real time scalar

00:24:15.820 --> 00:24:17.790
analytics using Spark.

00:24:17.790 --> 00:24:19.860
Those are the right
tools for the job.

00:24:19.860 --> 00:24:22.990
Those are available on
Google Cloud Platform.

00:24:22.990 --> 00:24:27.090
And those can be made to
run the Cloud way as well.

00:24:27.090 --> 00:24:29.830
So what does it mean to
run opensource software

00:24:29.830 --> 00:24:32.610
like Hadoop and
Spark the Cloud way?

00:24:32.610 --> 00:24:37.050
Well first thing, Hadoop runs
great on Google Cloud Platform.

00:24:37.050 --> 00:24:40.870
Because of the quality and
the consistency of the VMs,

00:24:40.870 --> 00:24:43.810
of the network,
and of the storage

00:24:43.810 --> 00:24:45.620
it runs really efficiently
on the platform.

00:24:45.620 --> 00:24:48.100
And you can have a
traditional deployment.

00:24:48.100 --> 00:24:50.260
And when it comes to
HDFS you have a choice.

00:24:50.260 --> 00:24:52.860
There are three different
[? blog ?] storage services

00:24:52.860 --> 00:24:54.780
appropriate for HDFS.

00:24:54.780 --> 00:24:57.350
You can use the
traditional persistent disk

00:24:57.350 --> 00:24:59.880
backed by spinning disk.

00:24:59.880 --> 00:25:03.080
You can use the SSD-backed
persistent disk,

00:25:03.080 --> 00:25:06.210
which just went generally
available today.

00:25:06.210 --> 00:25:11.000
Or you can use the newly
announced local SSD, which

00:25:11.000 --> 00:25:13.990
was announced today and is now
available in Trusted Tester.

00:25:13.990 --> 00:25:15.740
So you have three
different services,

00:25:15.740 --> 00:25:17.850
three different performance
characteristics,

00:25:17.850 --> 00:25:20.440
and three different price
points available to you

00:25:20.440 --> 00:25:22.230
depending on your needs.

00:25:22.230 --> 00:25:25.880
But you don't actually
even need to set up HDFS.

00:25:25.880 --> 00:25:30.520
We've created three connectors
to Google storage services,

00:25:30.520 --> 00:25:34.950
one for Google Cloud
Storage, one for BigQuery,

00:25:34.950 --> 00:25:36.950
one for the tester.

00:25:36.950 --> 00:25:39.910
These connectors
implement Hadoop native

00:25:39.910 --> 00:25:44.210
API such that your code in
Hadoop, your [? map produce, ?]

00:25:44.210 --> 00:25:47.560
your Pig, our Hive, if
you use Hadoop streaming

00:25:47.560 --> 00:25:51.610
in other languages, or Spark,
because Spark uses the same IU

00:25:51.610 --> 00:25:55.510
interfaces as Hadoop, your
user code in Spark or Hadoop

00:25:55.510 --> 00:25:59.420
can run entirely unchanged
and access data directly

00:25:59.420 --> 00:26:02.780
in Cloud storage in
BigQuery, in Datastore.

00:26:02.780 --> 00:26:04.740
That means that the
data is not logged

00:26:04.740 --> 00:26:06.370
in silos into one cluster.

00:26:06.370 --> 00:26:08.700
You can have several
Hadoop cluster

00:26:08.700 --> 00:26:10.150
working as the same data.

00:26:10.150 --> 00:26:12.457
And you can also have
BigQuery working [INAUDIBLE].

00:26:12.457 --> 00:26:14.790
You can have Cloud Dataflow
working [? against state. ?]

00:26:14.790 --> 00:26:18.200
The data is not locked into
a specific pricing engine.

00:26:18.200 --> 00:26:19.660
It's available for all.

00:26:19.660 --> 00:26:22.360
And so those are available,
all three of them.

00:26:22.360 --> 00:26:25.720
The GCS connector's actually
opensource on GitHub.

00:26:25.720 --> 00:26:27.646
The other two, we
haven't done that yet.

00:26:27.646 --> 00:26:29.520
But the plan is to make
them widely available

00:26:29.520 --> 00:26:33.240
in opensource for everybody,
to make it very easy,

00:26:33.240 --> 00:26:34.980
architecturally makes
it very easy now.

00:26:34.980 --> 00:26:36.910
Because you don't have
to worry about when

00:26:36.910 --> 00:26:39.130
you spin your cluster,
coping data into it,

00:26:39.130 --> 00:26:41.590
and then making sure that
you set the data back

00:26:41.590 --> 00:26:43.590
before you turn it off.

00:26:43.590 --> 00:26:46.630
You can use those connectors
in conjunction HDFS.

00:26:46.630 --> 00:26:48.120
Or you can use without HDFS.

00:26:48.120 --> 00:26:52.540
This is really as you
see fit, as you prefer.

00:26:52.540 --> 00:26:56.420
In addition to the
connectors we have also

00:26:56.420 --> 00:27:01.560
created a tool called BDUTL,
which stands for Big Data UTL.

00:27:01.560 --> 00:27:03.420
That tool automates
the deployment

00:27:03.420 --> 00:27:06.535
and the configuration
of Hadoop and Spark

00:27:06.535 --> 00:27:07.820
on Google Cloud Platform.

00:27:07.820 --> 00:27:08.840
So we'll deploy them.

00:27:08.840 --> 00:27:10.800
We'll spin the VM,
as many as you want.

00:27:10.800 --> 00:27:11.960
We'll deploy.

00:27:11.960 --> 00:27:14.310
We'll configure including
installing and configuring

00:27:14.310 --> 00:27:15.430
new connectors.

00:27:15.430 --> 00:27:18.590
And the cluster is
available for you to use.

00:27:18.590 --> 00:27:21.200
So you should take that
level of automation.

00:27:21.200 --> 00:27:23.090
If you add to that
the connectors that

00:27:23.090 --> 00:27:25.500
give immediate
access to the data

00:27:25.500 --> 00:27:27.760
as soon as the
cluster is up, if you

00:27:27.760 --> 00:27:31.800
add the fast boot-up
time of the VMs on Google

00:27:31.800 --> 00:27:34.630
[INAUDIBLE] engine and the fact
that you pay by the minute,

00:27:34.630 --> 00:27:36.750
all those things
together really give you

00:27:36.750 --> 00:27:41.030
a tool set that allows a much
more flexible and dynamic way

00:27:41.030 --> 00:27:44.870
to run Hadoop clusters
in Google Cloud Platform.

00:27:44.870 --> 00:27:46.910
And that's how you can
run Hadoop and Spark

00:27:46.910 --> 00:27:49.037
and other opensource
tools the Cloud way

00:27:49.037 --> 00:27:52.620
on Google Cloud Platform.

00:27:52.620 --> 00:27:56.440
So as we describe the
services of the platform

00:27:56.440 --> 00:28:01.140
and how together they can
help users focus on the task

00:28:01.140 --> 00:28:03.530
they're trying to achieve,
not focus on setting up

00:28:03.530 --> 00:28:06.130
emerging infrastructure,
as we describe

00:28:06.130 --> 00:28:08.790
that we [INAUDIBLE] with
that someone complex picture.

00:28:08.790 --> 00:28:10.920
So let's simplify
it a little bit.

00:28:10.920 --> 00:28:16.530
At the end, the heart of it
is three fully managed cloud

00:28:16.530 --> 00:28:20.420
services which do one thing,
do it really well, and make

00:28:20.420 --> 00:28:24.690
you really productive-- Cloud
Pub/Sub for event handling.

00:28:24.690 --> 00:28:27.490
Cloud Dataflow for data
processing pipeline,

00:28:27.490 --> 00:28:30.260
either in stream mode
or in batch mode.

00:28:30.260 --> 00:28:33.100
BigQuery for
large-scale analytics.

00:28:33.100 --> 00:28:36.670
And then the ability
to integrate and bring

00:28:36.670 --> 00:28:39.270
the best of the
opensource ecosystem

00:28:39.270 --> 00:28:42.940
and to have those opensource
tools work in the manner that's

00:28:42.940 --> 00:28:45.410
well integrated with
the platform, such

00:28:45.410 --> 00:28:48.390
that you don't have to choose
to be in one world or the other.

00:28:48.390 --> 00:28:50.430
You can mix and
match as you see fit

00:28:50.430 --> 00:28:53.704
and have all those
services work together.

00:28:53.704 --> 00:28:54.870
And we see that in practice.

00:28:54.870 --> 00:28:58.130
And we have customers who
already have set up Big Data

00:28:58.130 --> 00:29:01.800
clusters on premise
or on some IES cloud.

00:29:01.800 --> 00:29:05.400
And they can bring that as
is to Google Cloud platform,

00:29:05.400 --> 00:29:08.110
to benefit from the
Cloud consumption

00:29:08.110 --> 00:29:10.550
model, the quality of
the infrastructure,

00:29:10.550 --> 00:29:12.060
the cost, the scalability.

00:29:12.060 --> 00:29:14.360
They can bring it
with no modification.

00:29:14.360 --> 00:29:16.860
And then as they see
fit, if they want to,

00:29:16.860 --> 00:29:18.380
opportunistically
they can decide

00:29:18.380 --> 00:29:22.240
to use BigQuery or
Datflow, Pub/Sub

00:29:22.240 --> 00:29:26.320
to lowered their emission
burden for the task

00:29:26.320 --> 00:29:28.650
where they think
that's the right tool.

00:29:28.650 --> 00:29:32.360
And vice versa, going back to
our example of a new company,

00:29:32.360 --> 00:29:34.210
at that station in
the life cycle, what

00:29:34.210 --> 00:29:36.600
companies value
most is velocity.

00:29:36.600 --> 00:29:38.100
The last thing they
want is to spend

00:29:38.100 --> 00:29:39.730
time worrying about
infrastructure

00:29:39.730 --> 00:29:41.110
and configuration.

00:29:41.110 --> 00:29:44.300
And so the fully managed
services are by far

00:29:44.300 --> 00:29:46.750
the most productive
way to get started

00:29:46.750 --> 00:29:48.690
and to build an entire platform.

00:29:48.690 --> 00:29:50.930
And it's not just the
easy tool to get started,

00:29:50.930 --> 00:29:52.100
because the scale is there.

00:29:52.100 --> 00:29:54.170
These are the tools
that can carry you up

00:29:54.170 --> 00:29:56.081
to the larger scale
that you will need.

00:29:56.081 --> 00:29:57.830
That being said, they
don't do everything.

00:29:57.830 --> 00:29:59.380
They don't try to do everything.

00:29:59.380 --> 00:30:02.260
There are tasks for which
Pub/Sub, Dataflow, and BigQuery

00:30:02.260 --> 00:30:03.330
are not the right tool.

00:30:03.330 --> 00:30:07.080
And for that there is a rich
ecosystem of opensource tools

00:30:07.080 --> 00:30:09.900
which we've made sure
can run well integrated

00:30:09.900 --> 00:30:11.179
into Google Cloud Platform.

00:30:11.179 --> 00:30:13.470
And that's the overall choice
that we want to give you.

00:30:13.470 --> 00:30:15.303
And that's how we want
to put you in control

00:30:15.303 --> 00:30:21.140
and let you decide how you want
to run Big Data the Cloud way.

00:30:21.140 --> 00:30:23.670
More information
coming up in this room.

00:30:23.670 --> 00:30:25.080
This is the right place to be.

00:30:25.080 --> 00:30:27.700
10:00 AM tomorrow,
the [INAUDIBLE] Data.

00:30:27.700 --> 00:30:31.310
This is a session
specifically on Dataflow.

00:30:31.310 --> 00:30:33.450
4:00 PM also here tomorrow.

00:30:33.450 --> 00:30:35.580
Predicting the Future with
Google Cloud Platform,

00:30:35.580 --> 00:30:38.580
which will show you how to
use BigQuery and Dataflow

00:30:38.580 --> 00:30:41.450
for a concrete
practical example.

00:30:41.450 --> 00:30:44.460
And then we'll have
many developers and PMs

00:30:44.460 --> 00:30:47.230
at the sandbox, talking about
those [INAUDIBLE] topics,

00:30:47.230 --> 00:30:50.400
as well as on the Cloud
booths on the floor.

00:30:50.400 --> 00:30:52.400
So please come and see us there.

00:30:52.400 --> 00:30:54.640
And we'd like to
discuss your use cases

00:30:54.640 --> 00:30:57.640
and how these tools
would apply to you.

00:30:57.640 --> 00:31:00.420
And finally, thank you for
coming here in San Francisco.

00:31:00.420 --> 00:31:01.980
Some of you have traveled a lot.

00:31:01.980 --> 00:31:03.540
We're coming to you as well.

00:31:03.540 --> 00:31:06.030
We will have Cloud
roadshows, hopefully

00:31:06.030 --> 00:31:07.620
to a city close to yours.

00:31:07.620 --> 00:31:12.380
So please check out this URL and
see when they'll be available

00:31:12.380 --> 00:31:14.190
and when we're coming to you.

00:31:14.190 --> 00:31:17.302
If you want more information
about Cloud Dataflow

00:31:17.302 --> 00:31:18.760
we have set up a
Google group where

00:31:18.760 --> 00:31:22.040
you can register to be
informed of new announcement

00:31:22.040 --> 00:31:23.290
for the product.

00:31:23.290 --> 00:31:28.340
And Cloud Pub/Sub is now
available on the website.

00:31:28.340 --> 00:31:29.550
The documentation is there.

00:31:29.550 --> 00:31:30.560
It's a limited preview.

00:31:30.560 --> 00:31:32.320
So if you're
interested in joining,

00:31:32.320 --> 00:31:34.880
there is a form where you
can put your information

00:31:34.880 --> 00:31:37.620
and request an invitation
to the limited preview.

00:31:37.620 --> 00:31:38.950
Thank you very much.

00:31:38.950 --> 00:31:40.500
[APPLAUSE]

