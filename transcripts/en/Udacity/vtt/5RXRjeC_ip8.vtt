WEBVTT
Kind: captions
Language: en

00:00:00.600 --> 00:00:02.788
This lesson is about Q-learning.

00:00:02.788 --> 00:00:08.460
Recall that Q-learning is a model-free
approach, meaning that it does not know

00:00:08.460 --> 00:00:14.580
about or use models of
the transitions T or the rewards R.

00:00:14.580 --> 00:00:18.630
Instead, Q-learning builds
a table of utility values

00:00:18.630 --> 00:00:21.250
as the agent interacts with the world.

00:00:21.250 --> 00:00:26.300
These Q-values can be used at each
step to select the best action

00:00:26.300 --> 00:00:29.200
based on what it has learned so far.

00:00:29.200 --> 00:00:31.930
The fantastic thing about Q-learning

00:00:31.930 --> 00:00:35.440
is that it is guaranteed to
provide an optimal policy.

00:00:35.440 --> 00:00:37.900
There is a hitch, however,
that we'll cover later.

