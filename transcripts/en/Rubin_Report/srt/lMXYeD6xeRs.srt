1
00:00:00,260 --> 00:00:04,370
[inaudible]

2
00:00:04,510 --> 00:00:08,530
joining me today is the founding editor
of Colette clearly made welcome to the

3
00:00:08,531 --> 00:00:09,360
Rubin report.

4
00:00:09,360 --> 00:00:10,860
Thanks Dave.
Thanks for having me.

5
00:00:10,970 --> 00:00:13,880
I am thrilled to finally have you here.
This has been like what,

6
00:00:13,881 --> 00:00:16,020
87 years in the making, right? Yeah.

7
00:00:16,470 --> 00:00:20,640
Well I live a very long way away,
so it's difficult for me to get here.

8
00:00:20,740 --> 00:00:23,860
Yeah, well you made it, you
made it thousands of miles.

9
00:00:23,861 --> 00:00:27,830
You are part of what I would
say is IDW Australia, I suppose.

10
00:00:28,200 --> 00:00:32,920
Spora yeah, exactly. Um, okay, so there's
a ton I want to talk to you about.

11
00:00:32,921 --> 00:00:35,800
Obviously I want to focus on Colette
and the work that you're doing and we

12
00:00:35,801 --> 00:00:39,250
obviously have a lot in common on the
issues that we care about and things of

13
00:00:39,251 --> 00:00:41,350
that nature. But first off,
you are from Australia.

14
00:00:41,351 --> 00:00:46,090
I've only had I think a two other guests
that are Australian natives on the

15
00:00:46,091 --> 00:00:49,050
show. Yeah. What's going on
in Australia these days? What,

16
00:00:49,051 --> 00:00:51,550
what does the rest of the world
need to know about Australia?

17
00:00:51,680 --> 00:00:55,790
I would say that in Australia were
pretty chilled out at the moment,

18
00:00:55,791 --> 00:01:00,260
at least politically and
culturally compared to America.

19
00:01:00,350 --> 00:01:05,030
And even Europe to a large
extent, we have a, we've had, um,

20
00:01:05,060 --> 00:01:09,920
25 years or more of economic growth.
So people are pretty relaxed.

21
00:01:09,921 --> 00:01:13,610
People are doing well, the
middle class is doing well. Um,

22
00:01:13,640 --> 00:01:17,900
there's not so much of a feeling that
is a zero sum competition going on.

23
00:01:17,901 --> 00:01:21,860
Like people are fighting over
scarce resources and you know,

24
00:01:21,861 --> 00:01:26,240
we obviously have political
correctness and identity politics. Um,

25
00:01:26,360 --> 00:01:28,850
a lot of that's being
imported from America.

26
00:01:28,970 --> 00:01:31,430
Yeah.
And how does that get imported?

27
00:01:31,431 --> 00:01:34,400
Because I was mentioning to you right
before we started that a disproportionate

28
00:01:34,401 --> 00:01:38,280
amount of our patrons, the people who
support the show or Australia. Yeah.

29
00:01:38,281 --> 00:01:39,680
So even if things are going pretty well,

30
00:01:39,681 --> 00:01:41,870
there's definitely a segment
of people that are going,

31
00:01:42,200 --> 00:01:44,750
something's either not right or
where we're seeing the science.

32
00:01:45,030 --> 00:01:45,863
That's something coming.

33
00:01:45,880 --> 00:01:47,530
Yeah, it's true. I mean,

34
00:01:47,620 --> 00:01:52,240
it's difficult to identify exactly how
these cultural trends get imported. But,

35
00:01:52,870 --> 00:01:57,550
um, for example, through
universities, through social media,

36
00:01:57,580 --> 00:01:59,650
through traditional medium,

37
00:01:59,740 --> 00:02:04,740
so similar narratives get picked up
by journalists like the oppression,

38
00:02:05,351 --> 00:02:09,880
narratives and, um, and
we're seeing with the,

39
00:02:09,940 --> 00:02:12,730
with the young,
younger generations how they're,

40
00:02:13,090 --> 00:02:17,200
how they pick up ideology through social
media and through the echo chambers and

41
00:02:17,201 --> 00:02:22,180
the filter bubbles that are so
easily accessible. Yeah. Yeah.

42
00:02:22,670 --> 00:02:26,960
Do you think that there's a geographic
reason that some of this stuff hasn't

43
00:02:26,961 --> 00:02:28,580
fully hit Australia yet?
I mean,

44
00:02:28,581 --> 00:02:32,390
being surrounded by water and
a place from the rest of us,

45
00:02:32,391 --> 00:02:35,270
there's probably a little bit of
safety in that I would imagine.

46
00:02:35,920 --> 00:02:38,390
Although borders are changing
just because of technology.

47
00:02:38,490 --> 00:02:42,810
Yeah. It's um, yeah, so
we are more isolated,

48
00:02:42,811 --> 00:02:47,460
that's for sure. Where smaller.
Um, so we don't have the same,

49
00:02:47,670 --> 00:02:52,670
we're not as diverse as other places
like Europe and the United States.

50
00:02:53,400 --> 00:02:58,230
Um, we have less income
inequality in Australia,

51
00:02:58,290 --> 00:03:01,030
which makes a difference. And, um,

52
00:03:01,060 --> 00:03:04,330
well we're actually kind of
parochial as well. I mean,

53
00:03:04,331 --> 00:03:06,850
we do pick up cultural
trends from America,

54
00:03:06,851 --> 00:03:11,851
but at the same time we just focus on
cooking shows and reality TV and football

55
00:03:12,671 --> 00:03:13,440
games.

56
00:03:13,440 --> 00:03:14,280
Yeah.
Yeah.

57
00:03:14,370 --> 00:03:19,370
So when the article came out back in
was it may or so the renegades of the

58
00:03:20,101 --> 00:03:21,220
intellectual dark web,

59
00:03:21,260 --> 00:03:25,140
you were included in there and many
of us had not even met you. Yeah.

60
00:03:25,380 --> 00:03:27,660
This is actually our first
time meeting in person. Yeah.

61
00:03:27,950 --> 00:03:31,110
Even though we've been
chatting online for awhile. Um,

62
00:03:31,740 --> 00:03:36,630
were you shocked about the reaction to
that article because it really did sort

63
00:03:36,631 --> 00:03:40,150
of level up all of the issues
that we've been all talking. Yeah,

64
00:03:40,220 --> 00:03:42,840
no, no, I wasn't shocked at all.

65
00:03:42,841 --> 00:03:46,830
I think that the general movement,

66
00:03:46,831 --> 00:03:50,090
this ideas movement and
this push back of the,

67
00:03:50,100 --> 00:03:55,100
the sort of moral orthodoxy that the
politically correct left have at the

68
00:03:56,041 --> 00:03:56,670
moment.

69
00:03:56,670 --> 00:04:00,600
It's been brewing for some time and
someone just had to put a label on it.

70
00:04:00,601 --> 00:04:03,570
Someone just had to name it.
And um,

71
00:04:04,410 --> 00:04:09,410
I wasn't shocked by the groundswell of
reaction to it because I've been seeing

72
00:04:11,251 --> 00:04:13,200
it through my work with Colette.

73
00:04:13,201 --> 00:04:16,890
There are just so many people who
write to me every day who say,

74
00:04:17,280 --> 00:04:21,870
I love what you're doing. I can't,
um, write a blog myself account,

75
00:04:21,960 --> 00:04:25,470
put my name on it because I would get
in trouble at work or in my university

76
00:04:25,770 --> 00:04:28,850
where there are so many people who,
um,

77
00:04:29,070 --> 00:04:33,840
want to participate in these discussions
but can't for whatever reason. And um,

78
00:04:34,950 --> 00:04:35,161
you know,

79
00:04:35,161 --> 00:04:39,730
they liked it to be now be able to
put a label on this general movement.

80
00:04:40,040 --> 00:04:43,040
Yeah. So tell me a little bit
about your personal background.

81
00:04:43,041 --> 00:04:45,200
What led you to be one of
the people that is okay.

82
00:04:45,201 --> 00:04:48,590
Putting your name on this stuff because
I do think that's one of the huge issues

83
00:04:48,591 --> 00:04:51,970
here. Yeah. We get all this
support from academics. Yeah.

84
00:04:52,010 --> 00:04:56,780
Even people in politics and, and regular
folks who were like, yeah, I'm with Ya.

85
00:04:57,120 --> 00:05:00,170
And I see the dangers, but I don't
want to put my name money then.

86
00:05:00,171 --> 00:05:02,840
So how did Claire Lehman ended up here?
Uh,

87
00:05:02,841 --> 00:05:05,090
willing to put your name
and faced all this stuff?

88
00:05:05,490 --> 00:05:06,151
You know,
I,

89
00:05:06,151 --> 00:05:11,151
I think I have bit more of a risk taking
streak in my nature and I don't feel

90
00:05:12,631 --> 00:05:17,631
the same fear around social
disapproval as a lot of other people.

91
00:05:18,360 --> 00:05:19,193
Um,

92
00:05:19,980 --> 00:05:24,980
so I was a psychology graduate student
and I left my Grad program and before I

93
00:05:27,241 --> 00:05:31,710
left I sort of, um, uh,

94
00:05:31,770 --> 00:05:34,780
took a few risks during,
during my program and,

95
00:05:34,781 --> 00:05:38,880
and we're sort of hung out to dry
by the university, so to speak.

96
00:05:38,920 --> 00:05:40,910
Yeah. Do you want to give me
an example of one of those,

97
00:05:41,400 --> 00:05:42,850
you sort of lit up when you said it?

98
00:05:43,270 --> 00:05:44,510
Well,
I,

99
00:05:44,590 --> 00:05:49,590
I complained about the amount of unpaid
work that psychology students had to do

100
00:05:49,901 --> 00:05:54,640
in a clinic that was associated with the
university and they weren't happy with

101
00:05:54,641 --> 00:05:57,040
that.
And I sort of received a bit of,

102
00:05:58,010 --> 00:06:02,810
for sort of almost whistle blowing on the
amount of exploitation this clinic was

103
00:06:02,990 --> 00:06:03,823
involved in.

104
00:06:04,640 --> 00:06:09,260
So I've always said on the
first time I took a risk, um,

105
00:06:09,320 --> 00:06:14,320
I've always taken risks when I've had
felt that my conscience demands that I do

106
00:06:15,651 --> 00:06:20,570
so. And what led me to
create Colette was, um,

107
00:06:21,590 --> 00:06:23,330
when I was a graduate student,

108
00:06:23,360 --> 00:06:27,800
I was involved in a lot of online
discussions with academics in psychology.

109
00:06:28,250 --> 00:06:30,020
And we had such fascinating,

110
00:06:30,021 --> 00:06:34,610
interesting discussions
that were completely unlike
anything that you would see

111
00:06:34,611 --> 00:06:37,280
in mainstream media.
And I thought, firstly,

112
00:06:37,281 --> 00:06:41,090
there's a business opportunity here if
I can bring some of these conversations

113
00:06:41,091 --> 00:06:44,000
to a market. And secondly, you know,

114
00:06:44,001 --> 00:06:46,550
people need to know that there are,

115
00:06:46,580 --> 00:06:51,580
there is scientific evidence and some of
these topics and mainstream journalists

116
00:06:52,490 --> 00:06:55,940
on and neglecting to,
um,

117
00:06:56,240 --> 00:07:00,310
inform readers about some of these
issues or some of some of this evidence.

118
00:07:00,690 --> 00:07:04,060
Yeah. So I want to read the mission
statement from Colette Cause it's,

119
00:07:04,061 --> 00:07:08,400
it's pretty freaking perfect.
Colette is a platform for free thought.

120
00:07:08,401 --> 00:07:11,310
We respect ideas,
even dangerous ones.

121
00:07:11,311 --> 00:07:15,720
We also believe that free expression and
the free exchange of ideas help human

122
00:07:15,721 --> 00:07:17,610
societies flourish and progress.

123
00:07:17,880 --> 00:07:21,690
Colette aims to provide a
platform for this exchange.

124
00:07:22,920 --> 00:07:26,670
Everybody seems to be afraid of
dangerous ideas right now. Right?

125
00:07:26,671 --> 00:07:30,930
I mean that's why this little crew of
people are together for whatever our

126
00:07:30,931 --> 00:07:31,950
differences are.

127
00:07:32,110 --> 00:07:35,170
Yeah.
I think that there's a,

128
00:07:35,290 --> 00:07:37,660
there's been a conflation between

129
00:07:39,250 --> 00:07:44,250
giving a platform or exploring ideas
and endorsing ideas and they're not the

130
00:07:44,381 --> 00:07:45,250
same thing.

131
00:07:45,580 --> 00:07:50,320
So I will publish stuff on Colette
that I don't necessarily agree with

132
00:07:50,321 --> 00:07:52,450
politically.
Um,

133
00:07:52,960 --> 00:07:57,200
I don't see the fact of publishing and,
uh,

134
00:07:57,460 --> 00:08:01,310
necessarily as an endorsement. I also, um,

135
00:08:01,390 --> 00:08:05,680
am very clear that I'm going
to make mistakes as an editor.

136
00:08:06,160 --> 00:08:10,270
I'm Colette, we don't want to
make mistakes about accuracy.

137
00:08:10,300 --> 00:08:13,780
And if we're ever called out for
publishing something that's not factually

138
00:08:13,781 --> 00:08:18,100
correct, we will own up to
that mistake. But you know,

139
00:08:18,101 --> 00:08:23,101
we're living in a time of
intense ideological flux
and to work out where we're

140
00:08:24,851 --> 00:08:25,571
all going,

141
00:08:25,571 --> 00:08:29,590
we need to be able to talk about things
freely and we need to be able to make

142
00:08:29,591 --> 00:08:31,780
mistakes.
And I think make,

143
00:08:31,781 --> 00:08:36,310
being able to make mistakes is a sign
of a healthy environment and it's a sign

144
00:08:36,311 --> 00:08:40,690
of, you know, it's part of the
creative process. Yeah. Um, so the,

145
00:08:40,840 --> 00:08:44,920
the, the, the warriors who
want to shut down debate,

146
00:08:44,921 --> 00:08:48,250
who want to shut down free speech and um,

147
00:08:48,640 --> 00:08:53,350
argue that giving a platform for
exploring dangerous ideas is equivalent to

148
00:08:53,351 --> 00:08:57,660
endorsing them. They're basically
that you can't make a mistake.

149
00:08:58,140 --> 00:08:59,430
And that's,

150
00:08:59,520 --> 00:09:03,390
that's really scary because the only
way we move forward is to experiment,

151
00:09:03,730 --> 00:09:08,560
test new ideas, see, see what works
and pick the best ones out of the,

152
00:09:08,970 --> 00:09:11,820
the experimentation process
and move forward that way.

153
00:09:11,930 --> 00:09:14,450
Yeah. So I want to discuss some
of the dangerous ideas. Yeah.

154
00:09:14,770 --> 00:09:16,120
Or that your platform,

155
00:09:17,060 --> 00:09:19,370
but it's interesting that you said
this thing about you don't mind making

156
00:09:19,371 --> 00:09:21,650
mistakes because I feel
very much the same.

157
00:09:21,651 --> 00:09:24,440
I will sit down with all sorts of people.
I've done it before,

158
00:09:24,441 --> 00:09:27,830
people I agree with and disagree with
some people who I really don't like what

159
00:09:27,831 --> 00:09:30,620
they're talking about.
But I try to give everybody a fair shake.

160
00:09:30,621 --> 00:09:35,210
And what I'm noticing is
there's a tremendous amount
of pushback just on that.

161
00:09:35,211 --> 00:09:37,130
That's what you're getting to.
Like the idea that a,

162
00:09:37,131 --> 00:09:41,960
you can sit down with someone in separate,
a human being, mm. From their ideas,

163
00:09:41,961 --> 00:09:45,020
but also that there's this sort of
secondary thing that I'm noticing.

164
00:09:45,021 --> 00:09:48,020
And I think you, you get it too, just
from the articles you guys put out,

165
00:09:48,410 --> 00:09:53,330
which is that if I don't ask the exact
question that someone wants me to ask or

166
00:09:53,331 --> 00:09:57,320
I miss this or I word this this way,
but just this,

167
00:09:57,350 --> 00:10:01,850
this army that doesn't feel that
organic to me is ready to pounce.

168
00:10:02,180 --> 00:10:03,920
And I'm really trying
to work through that.

169
00:10:03,921 --> 00:10:05,840
Like I don't spend too much
time thinking about it,

170
00:10:05,841 --> 00:10:07,910
but it is a conscious thing like,
oh,

171
00:10:08,450 --> 00:10:11,840
there's going to be this group of people
that no matter what I do and really

172
00:10:11,841 --> 00:10:15,140
what they're trying to do
is chill everybody out so
that you'll only talk about

173
00:10:15,141 --> 00:10:15,980
what they're comfortable with it.

174
00:10:16,750 --> 00:10:18,010
Yeah. Well, there are,

175
00:10:18,310 --> 00:10:23,310
there are people at the moment who feel
that all of the important questions have

176
00:10:23,681 --> 00:10:24,520
been answered.

177
00:10:25,000 --> 00:10:30,000
All of the important moral and
ethical dilemmas have an answer.

178
00:10:30,430 --> 00:10:34,990
And that at this point in history,
everything's been worked out.

179
00:10:34,991 --> 00:10:37,210
And anyone who's,
um,

180
00:10:37,630 --> 00:10:42,630
exploring different ideas or is doubtful
about these moral issues is evil.

181
00:10:45,730 --> 00:10:47,800
You're part of the Archi.
You're racist. Yeah,

182
00:10:47,900 --> 00:10:51,150
yeah, yeah, yeah. Um, so I come,

183
00:10:51,420 --> 00:10:56,100
I come from a different approach where
like I don't know what's going on.

184
00:10:56,101 --> 00:10:59,300
Like I, I'm trying to work it
out with other people and, um,

185
00:10:59,340 --> 00:11:01,500
I want the freedom to be able to explore

186
00:11:02,010 --> 00:11:03,930
in a weird way.
Does that get you the most hate?

187
00:11:04,020 --> 00:11:06,660
Cause I think I really believe
that's where most people are.

188
00:11:06,661 --> 00:11:08,340
I think that's why people
appreciate what I do here.

189
00:11:08,341 --> 00:11:11,860
I'm just being open about my journey
and listening to people and learning and

190
00:11:12,120 --> 00:11:15,480
all of that. Yeah, yeah. And that,
but that opens you up from hate,

191
00:11:15,481 --> 00:11:16,590
from the both sides.

192
00:11:16,591 --> 00:11:20,190
It's a lot easier to just stake out a
position and then you'll only get heat one

193
00:11:20,191 --> 00:11:23,160
way. But when you're like, I am
going to hear both sides, you know,

194
00:11:23,161 --> 00:11:27,030
it's why I love listening to Sam Harris
and Jordan Peterson debate. Yeah.

195
00:11:27,540 --> 00:11:30,570
You know, the biggest existential
questions that there are. It's like, yeah,

196
00:11:30,810 --> 00:11:34,950
it's like, if I've completely settled
my mind on this, I don't know.

197
00:11:34,951 --> 00:11:38,250
I wouldn't be that impressed with
my thought process or something.

198
00:11:38,350 --> 00:11:42,430
Yeah. Well, you know, I
don't get a lot of Hay. Um,

199
00:11:42,550 --> 00:11:44,380
I certainly have Twitter trolls,
but,

200
00:11:44,700 --> 00:11:47,170
well, wait til we post. Yeah.

201
00:11:47,640 --> 00:11:51,220
Um, I sort of don't really notice
it. Yeah. Okay. Yeah. Yeah.

202
00:11:51,520 --> 00:11:54,490
That's probably the most simple to
deal with this. All right. So let's,

203
00:11:54,550 --> 00:11:56,320
let's talk about some of
those dangerous ideas.

204
00:11:56,321 --> 00:12:00,370
What are some dangerous ideas that have
bubbled up from your writers that you've

205
00:12:00,371 --> 00:12:04,090
had to think about or that you're,
you're focused on these days?

206
00:12:04,320 --> 00:12:08,670
Yeah, so because my background
is in psychology, um,

207
00:12:09,480 --> 00:12:14,250
we tackle issues that come
from the behavioral sciences.

208
00:12:14,280 --> 00:12:17,610
So we've looked at sex differences,
um,

209
00:12:18,090 --> 00:12:21,510
psychological sex differences
between men and women, um,

210
00:12:21,511 --> 00:12:25,260
differences in interests in occupations.
Um,

211
00:12:25,261 --> 00:12:30,261
so we've published some articles on
why there are fewer women in computer

212
00:12:31,411 --> 00:12:34,380
science, for example. And, um,

213
00:12:34,410 --> 00:12:39,410
we published a analysis of Jameson
DeMaurice Google memo written by four

214
00:12:40,121 --> 00:12:44,520
different scientists who have
some expertise in that area. Uh,

215
00:12:44,550 --> 00:12:47,330
we've published on intelligence Reese.

216
00:12:47,530 --> 00:12:49,330
Well,
what were some of the results of,

217
00:12:50,600 --> 00:12:52,650
yeah, well they, you know,

218
00:12:53,040 --> 00:12:58,040
they basically said that something pretty
similar that he was in the ballpark in

219
00:12:58,471 --> 00:13:03,471
terms of getting the science right and
his position is valid in terms of regular

220
00:13:05,461 --> 00:13:10,380
scientific debate. There are observable
sex differences between men and women.

221
00:13:10,860 --> 00:13:15,860
The differences in interests in
occupations is very robust and reliable.

222
00:13:17,520 --> 00:13:22,520
So women are overwhelmingly interested
in occupations which have something to do

223
00:13:23,941 --> 00:13:28,650
with people. So caring
professions, medicine, even law.

224
00:13:29,760 --> 00:13:34,740
Whereas men have more
interest in occupations that
are to do with systems and

225
00:13:34,741 --> 00:13:37,260
things.
So anything mechanical,

226
00:13:37,560 --> 00:13:39,930
computer science and there's no,

227
00:13:40,470 --> 00:13:44,430
there's no difference between the average
intelligence scores between men and

228
00:13:44,431 --> 00:13:44,850
women.

229
00:13:44,850 --> 00:13:49,850
And the evidence around differences
in verbal and mathematical ability is

230
00:13:50,641 --> 00:13:53,340
somewhat mixed.
I mean you can,

231
00:13:53,341 --> 00:13:56,940
you can have debates backwards and
forwards over the strengths of those

232
00:13:56,941 --> 00:14:00,840
differences, but the difference
in interest is very robust.

233
00:14:00,940 --> 00:14:03,640
Why are people so afraid of that?

234
00:14:03,670 --> 00:14:08,670
Why is it that there is a set of people
out there right now that want us to act

235
00:14:09,221 --> 00:14:11,560
to be all equal in their minds,

236
00:14:11,561 --> 00:14:15,760
but it would actually force us to go
against nature? I mean that's what,

237
00:14:15,790 --> 00:14:17,530
that's what this really is.

238
00:14:17,950 --> 00:14:22,950
Well there is a fear that if
you acknowledge that there
are differences between

239
00:14:23,711 --> 00:14:28,711
individuals or groups that you're
saying that one group is inferior.

240
00:14:30,940 --> 00:14:35,940
So there's been a co like the idea that
we have to be the same to be equal has

241
00:14:39,581 --> 00:14:42,490
been a very prevalent and strong idea.

242
00:14:42,491 --> 00:14:47,260
I don't know who came where the
idea comes from. Right. But it's,

243
00:14:47,770 --> 00:14:52,760
it's, um, it's dangerous
because if you, um,

244
00:14:53,300 --> 00:14:57,350
if you argue that no true
or quality can be achieved,

245
00:14:57,410 --> 00:15:01,730
unless we're all the same, then no, then
we can't have a quality. Yeah. However,

246
00:15:01,731 --> 00:15:02,960
if you argue that no,

247
00:15:02,961 --> 00:15:07,961
we're all morally equal and that
we deserve equal opportunity and,

248
00:15:09,310 --> 00:15:12,680
uh, the equal, um, uh,

249
00:15:12,740 --> 00:15:17,720
w we deserve equal opportunities to
live a happy, flourishing life. However,

250
00:15:17,721 --> 00:15:22,590
there are differences between us.
Yeah. Then you can preserve that, um,

251
00:15:22,730 --> 00:15:24,380
ethical principle.

252
00:15:25,100 --> 00:15:30,100
So it's very problematic for people
who think that there can be no equality

253
00:15:30,291 --> 00:15:35,000
between men and women unless we are
proven to be the same identical.

254
00:15:35,480 --> 00:15:40,430
Um, because the evidence doesn't support
the idea that we're the same. Yeah.

255
00:15:40,910 --> 00:15:44,460
What are, what are some of the other
dangerous ideas? Okay. So we've,

256
00:15:44,990 --> 00:15:46,190
we're doing the dangerous role.
Yeah,

257
00:15:46,220 --> 00:15:50,570
yeah, yeah. Um, we've published
work on intelligence research,

258
00:15:50,571 --> 00:15:55,430
which is controversial. Uh,
we've published articles about

259
00:15:55,820 --> 00:15:58,430
Harris Intelligence. Are you
talking about intelligence and race?

260
00:15:58,750 --> 00:16:03,190
Um, cause this seems to be one that
people seem to be all that's, yeah,

261
00:16:03,191 --> 00:16:08,050
that's a hugely controversial
issue. And uh, we've published, um,

262
00:16:08,680 --> 00:16:13,680
we published a defense of the Charles
Murray's the bell curve because after he

263
00:16:14,951 --> 00:16:17,350
had his conversation with Sam Harris,

264
00:16:17,770 --> 00:16:21,730
vox came out and wrote a hit piece.
Yeah.

265
00:16:21,820 --> 00:16:22,840
Or right at some,

266
00:16:22,870 --> 00:16:27,010
some kind of article that misrepresented
what they talked about and the bell

267
00:16:27,011 --> 00:16:27,844
curve.

268
00:16:27,940 --> 00:16:32,940
And so we published a response
from Professor Richard Hire,

269
00:16:33,890 --> 00:16:37,420
who's the editor of the
intelligence journal.

270
00:16:37,421 --> 00:16:40,210
So he's a preeminent expert in this area.

271
00:16:40,720 --> 00:16:45,010
And he defended Charles Murray's book
and defended the conversation that they

272
00:16:45,011 --> 00:16:47,890
had and said,
this is a important issue.

273
00:16:48,340 --> 00:16:53,340
It's scientifically valid and has
merit to talk about these issues.

274
00:16:54,521 --> 00:16:58,750
And they got, they were, they
got the science right. Um,

275
00:16:59,620 --> 00:17:00,520
so we,

276
00:17:00,940 --> 00:17:05,940
we don't aim to be provocative for
the sake of just being inflammatory,

277
00:17:07,631 --> 00:17:08,464
right?
We,

278
00:17:09,010 --> 00:17:13,780
we have an objective where we want
to protect the scientists who have,

279
00:17:13,840 --> 00:17:17,740
who are doing this work or who are
having these competent conversations.

280
00:17:18,010 --> 00:17:18,730
We want us,

281
00:17:18,730 --> 00:17:23,730
we want to protect the individuals who
are doing the knowledge production or the

282
00:17:27,431 --> 00:17:28,450
research.

283
00:17:28,540 --> 00:17:33,540
We want to protect them from the boxes
and the journalists who just want to

284
00:17:34,090 --> 00:17:37,530
write hate pieces just to get the clicks.
So we have a,

285
00:17:37,540 --> 00:17:41,530
a sort of a commitment to those people
who are doing risky work and need that

286
00:17:41,531 --> 00:17:42,460
kind of protection.

287
00:17:42,630 --> 00:17:44,640
You're going to my soft
spot today because as,

288
00:17:44,700 --> 00:17:48,000
as we're taking this only as we're
taping this, only like an hour ago,

289
00:17:48,001 --> 00:17:52,230
vox did a piece me about how I'm
part of the reactionary, right? Yeah.

290
00:17:52,320 --> 00:17:56,520
Something and it's just like, you guys
are just awful. So in a weird way, your,

291
00:17:56,521 --> 00:17:57,354
your,

292
00:17:57,720 --> 00:18:02,580
your job in some respect has become
sort of like a force field to just allow

293
00:18:02,581 --> 00:18:05,660
people to do the work that
they're supposed to be doing.

294
00:18:05,930 --> 00:18:07,040
Yeah,
that's right.

295
00:18:07,100 --> 00:18:12,100
Because universities for all sorts of
different reasons are neglecting their

296
00:18:12,891 --> 00:18:16,370
duty to academic freedom.
Right.

297
00:18:16,460 --> 00:18:21,460
And academic freedom is meant to exist
to protect scientists in particular who

298
00:18:23,031 --> 00:18:23,930
are doing work,

299
00:18:23,931 --> 00:18:28,931
which contradicts the religious orthodoxy
of the time or the moral orthodoxy in.

300
00:18:30,261 --> 00:18:34,970
It's quite clear to me that there are
scientists doing work which challenges

301
00:18:35,000 --> 00:18:40,000
politically correct dogmas and I don't
see universities doing a very good job at

302
00:18:41,180 --> 00:18:42,920
protecting those scientists.

303
00:18:43,370 --> 00:18:46,730
And so we feel like we can fill the gap.

304
00:18:47,380 --> 00:18:51,050
Where, where do you think the gender
studies stuff fits into all this?

305
00:18:51,051 --> 00:18:56,051
Because in a weird way it seems like it's
all being widdled down to that somehow

306
00:18:56,421 --> 00:18:58,910
that now,
especially related to trans issues,

307
00:18:58,911 --> 00:19:03,680
I was just talking to a fairly well
known public professor yesterday off the

308
00:19:03,681 --> 00:19:07,040
record. Yeah. About how
he's going into his old,

309
00:19:07,340 --> 00:19:11,310
even though he's factually
correct and it's calm, you know,

310
00:19:11,360 --> 00:19:12,620
confident about his research.

311
00:19:12,740 --> 00:19:16,670
He's going into old lesson plans to change
wording just because he doesn't want

312
00:19:16,671 --> 00:19:20,840
to deal with the mob. So things that
were completely acceptable three,

313
00:19:20,841 --> 00:19:25,100
four years ago is now having to edit out.
I mean that's, that's truly dangerous.

314
00:19:25,101 --> 00:19:28,190
So what is going on with the gender
studies department and the rest of this?

315
00:19:28,840 --> 00:19:29,540
It's all about

316
00:19:29,540 --> 00:19:33,050
moving so fast. It's hard to really, um,

317
00:19:33,890 --> 00:19:37,550
to analyze what's going on
when it's shifting so fast.

318
00:19:37,551 --> 00:19:40,430
But certainly,
um,

319
00:19:40,580 --> 00:19:45,580
many of the humanities departments took
a turn back in the 1970s and they took

320
00:19:46,160 --> 00:19:47,120
the,
um,

321
00:19:47,390 --> 00:19:52,390
took on these fashionable theories
such as poststructuralism and uh,

322
00:19:53,680 --> 00:19:57,150
they, they rejected empirical methods. Um,

323
00:19:58,200 --> 00:20:01,450
and so when the argument, if we were
trying to give them the credit yeah.

324
00:20:01,750 --> 00:20:04,070
That they're due, at least in
the argument, if you're, yeah,

325
00:20:04,140 --> 00:20:07,470
if you're rejecting empirical
science or verbal back,

326
00:20:07,830 --> 00:20:11,360
what is your argument that leads you
there? Cause I looked at their watches.

327
00:20:11,361 --> 00:20:14,700
Go next judge just sounds crazy,
but they obviously believe it.

328
00:20:14,701 --> 00:20:16,600
So let's try to give the devil
his deal. Right? Yeah. So

329
00:20:16,600 --> 00:20:21,600
postmodernists have a good point and
that is that we are all biased and a

330
00:20:22,511 --> 00:20:26,170
scientist looking at an issue such as,
um,

331
00:20:26,230 --> 00:20:30,940
sexuality has his or her own biases.

332
00:20:30,970 --> 00:20:35,820
Right? And so the questions that
the scientists are asks when he, oh,

333
00:20:35,890 --> 00:20:39,280
he or she is designing a survey
or designing a lab experiment,

334
00:20:39,490 --> 00:20:44,350
I go in and have some bias embedded
within them that, so that's true.

335
00:20:44,650 --> 00:20:47,770
And the postmodern has had that insight.
However,

336
00:20:47,860 --> 00:20:52,860
the mistake they make is getting rid
of a PR empirical and objective methods

337
00:20:53,360 --> 00:20:58,270
altogether and just saying, because
we're biased, what's the point? Right?

338
00:20:58,720 --> 00:21:00,980
So they're doing like a
massive over correction.

339
00:21:01,090 --> 00:21:03,250
Yes, yes. Yeah. When,

340
00:21:03,670 --> 00:21:07,990
when what should be done is
acknowledging, yes, we all have our bias.

341
00:21:08,350 --> 00:21:11,410
Um,
and if I'm a scientist designing a study,

342
00:21:11,411 --> 00:21:15,250
I'm going to have my bias and it might
get embedded into the work that I'm doing.

343
00:21:15,460 --> 00:21:16,300
However,

344
00:21:16,600 --> 00:21:21,070
there are ways to correct that and
we want to improve our quantitative

345
00:21:21,160 --> 00:21:25,210
methodologies in order to correct for
our own biases to the problem that the

346
00:21:25,211 --> 00:21:30,040
postmodern is have is that they
say, well, the bias is there,

347
00:21:30,041 --> 00:21:33,490
so let's put it all in the Bin and we're
just going to double down on the bias

348
00:21:33,491 --> 00:21:35,350
and just talk about lived experience.

349
00:21:36,100 --> 00:21:39,460
I mean that it's just taking the
completely the wrong direction.

350
00:21:39,670 --> 00:21:42,490
Right? I mean it's quite literally the
reverse of the scientific method. Yeah,

351
00:21:42,520 --> 00:21:46,900
so basically they're saying that
that bias is truth, right? I mean,

352
00:21:46,901 --> 00:21:50,410
if you're saying bias that the
lived experience is what truth is,

353
00:21:50,411 --> 00:21:53,230
then bias is truth. You know, I don't
know if you saw, but I did this,

354
00:21:53,470 --> 00:21:57,100
this talk at University of New Hampshire
and this woman was yelling at me,

355
00:21:57,101 --> 00:21:59,500
this trans woman calling me all right,

356
00:21:59,501 --> 00:22:01,540
and all of this other
nuts and shouting me down.

357
00:22:01,690 --> 00:22:03,880
I didn't realize this until
two months later or something,

358
00:22:03,881 --> 00:22:07,150
but it turned out she was a gender
studies professor at the school. Yeah.

359
00:22:07,170 --> 00:22:11,230
So you've got a gender studies professor
trying to shout down an invited speaker

360
00:22:11,290 --> 00:22:14,590
while live tweeting that I'm all
right or whatever. And it's like,

361
00:22:14,890 --> 00:22:17,320
I'm pretty sure maybe I'm
not right about everything.

362
00:22:17,321 --> 00:22:19,080
I'll concede on that
even though I want it.

363
00:22:19,081 --> 00:22:22,930
I said she should be treated with
respect and equality and all that.

364
00:22:23,200 --> 00:22:27,790
But your biased that you're acting on
right now definitely is not closer to

365
00:22:27,791 --> 00:22:29,740
truth than anything I was saying.

366
00:22:29,840 --> 00:22:34,280
Yeah. So I, the way I think
of it ease that you know,

367
00:22:34,281 --> 00:22:39,281
there is an objective reality and the way
we get to objective reality is through

368
00:22:39,621 --> 00:22:41,450
the scientific method.
Um,

369
00:22:41,960 --> 00:22:46,960
whereas postmodernists academics believe
a lot of them believe that reality's

370
00:22:48,050 --> 00:22:51,710
created through language and discourse.

371
00:22:51,920 --> 00:22:55,250
And I think that's one of the reasons
why they believe that words can be

372
00:22:55,251 --> 00:22:56,084
violence.

373
00:22:56,270 --> 00:23:00,890
Because if reality is actually created
through language and discourse and

374
00:23:00,891 --> 00:23:02,900
there's, um, some kind of,

375
00:23:03,440 --> 00:23:08,440
there's some toxic discourse than they
truly believe that reality can be changed

376
00:23:08,780 --> 00:23:09,710
through words.

377
00:23:10,160 --> 00:23:15,160
And so I think that's why they get so
upset about freedom of speech and just

378
00:23:15,231 --> 00:23:20,000
having discussions because they truly
believe that words can change the way the

379
00:23:20,001 --> 00:23:21,310
world is constructed.

380
00:23:21,810 --> 00:23:25,320
Right? So for now they say
it's okay to punch a Nazi,

381
00:23:25,380 --> 00:23:26,880
will they call everybody Nazis,

382
00:23:26,881 --> 00:23:31,881
then it's okay to punch a Nazi and then
we can go however far down that road we

383
00:23:31,951 --> 00:23:34,500
want to go. And then, yeah, then
you're completely condoning minds.

384
00:23:34,950 --> 00:23:39,600
How concerned are you that these ideas
are going to trickle up higher into

385
00:23:39,601 --> 00:23:43,020
places of power and that once they look,
we know they're in the media,

386
00:23:43,021 --> 00:23:46,370
you've already mentioned VOCs
and a couple of those other, um,

387
00:23:46,490 --> 00:23:49,640
but that once they're really in the
places of power in Australia or in the

388
00:23:49,641 --> 00:23:50,480
states or wherever else,

389
00:23:50,481 --> 00:23:55,060
that the screws are really gonna
be turned against us because yeah,

390
00:23:55,700 --> 00:23:57,140
we're fighting for free speech.

391
00:23:57,170 --> 00:23:57,690
Yeah.

392
00:23:57,690 --> 00:24:02,570
I'm mostly be concerned about
these ideas getting into the law.

393
00:24:03,530 --> 00:24:08,530
Traditionally the lore is a conservative
field and they are pretty good at

394
00:24:10,340 --> 00:24:12,200
resisting political fads.

395
00:24:12,950 --> 00:24:17,570
But you can see with the way that law
is taught in law schools at the moment

396
00:24:17,571 --> 00:24:22,571
that there is critical legal studies
and often critical legal studies is a

397
00:24:23,690 --> 00:24:25,050
mandatory component.

398
00:24:25,051 --> 00:24:30,051
So they teach like the
feminist interpretation of
law and these approaches,

399
00:24:33,320 --> 00:24:37,370
um, can undermine some of the basic, uh,

400
00:24:38,240 --> 00:24:42,680
fundamentals and foundations of how
the law is meant to work. So I, yeah,

401
00:24:42,681 --> 00:24:47,681
I am worried that there are
a lot of attacks on droop
due process at the moment,

402
00:24:48,500 --> 00:24:51,200
freedom of speech and
the scientific method.

403
00:24:51,440 --> 00:24:56,440
Those three things are foundations of
a civilized society and all of them are

404
00:24:57,651 --> 00:24:59,090
being attacked at the moment.

405
00:24:59,260 --> 00:25:02,500
Yeah. How does me to fit into
this because you've been,

406
00:25:02,501 --> 00:25:03,790
you've been somewhat critical of that.

407
00:25:03,791 --> 00:25:06,940
I think it gets all three of those
criteria that you just lay it out. Yeah.

408
00:25:07,520 --> 00:25:08,353
Look,
you know,

409
00:25:08,570 --> 00:25:13,390
obviously me to started as
a really legitimate and um,

410
00:25:13,440 --> 00:25:17,180
power for movement in identifying the,

411
00:25:17,390 --> 00:25:21,080
the predations that Harvey
Weinstein was engaged in. And,

412
00:25:21,620 --> 00:25:25,430
and surely there are,
there have been many powerful men,

413
00:25:25,940 --> 00:25:26,773
um,

414
00:25:27,170 --> 00:25:31,010
in the last couple of decades working in
industries where there's a lot of young

415
00:25:31,011 --> 00:25:33,710
women around and they have
positions of power and they,

416
00:25:33,720 --> 00:25:38,720
they're able to exploit those positions.
But,

417
00:25:38,721 --> 00:25:41,480
you know, I'm a young
woman myself, right? And I,

418
00:25:41,810 --> 00:25:45,740
I don't see sexual
harassment, um, as being,

419
00:25:45,741 --> 00:25:50,590
as pervasive as a lot of
people claim. And, uh,

420
00:25:51,010 --> 00:25:55,850
I also think it's very dangerous to
make accusations against people publicly

421
00:25:56,210 --> 00:26:00,740
because our reputations are so
valuable and so important that,

422
00:26:01,310 --> 00:26:03,120
um, you know, we,

423
00:26:03,121 --> 00:26:07,370
we need to respect other people's
ability to have, have their,

424
00:26:07,470 --> 00:26:09,590
their reputation and um,

425
00:26:09,740 --> 00:26:14,450
making accusations in public
sort of goes against my, um,

426
00:26:15,290 --> 00:26:16,370
my feeling that

427
00:26:18,390 --> 00:26:21,710
it feels vindictive to me
and it feels dangerous,

428
00:26:21,820 --> 00:26:26,820
dangerous path to go down because for
every true accusation there is of sexual

429
00:26:27,561 --> 00:26:30,560
harassment, there's going
to be someone who is, um,

430
00:26:32,540 --> 00:26:33,291
you know,
there's,

431
00:26:33,291 --> 00:26:37,130
there are going to be people who
have conflicts and then the conflict,

432
00:26:37,190 --> 00:26:40,940
someone feels like they can resolve a
conflict through making an accusation of

433
00:26:40,941 --> 00:26:42,710
sexual harassment.
And that's really scary.

434
00:26:42,711 --> 00:26:46,140
We don't don't want people to resolve
their conflicts by making public

435
00:26:46,141 --> 00:26:46,974
accusation.

436
00:26:47,030 --> 00:26:50,300
Right. But I definitely think there's
a certain set of people that don't mind

437
00:26:50,301 --> 00:26:53,360
collateral damage. Right? I mean that
seems to be what's happening right now.

438
00:26:53,361 --> 00:26:56,270
And people you see this public, people
that are tweeting these kinds of things,

439
00:26:56,271 --> 00:26:59,810
like it doesn't matter if you're in a
safe down a couple of ladies and people.

440
00:26:59,811 --> 00:27:01,490
That's how dangerous this thing is.

441
00:27:01,940 --> 00:27:06,350
That strikes me is far more dangerous
and I'm not belittling any of the actual

442
00:27:06,370 --> 00:27:08,770
yeah. Things that some of
these women have gone through.

443
00:27:08,970 --> 00:27:11,600
Yeah, it's scary. It's the, uh,

444
00:27:11,850 --> 00:27:16,850
desire for collective punishment
and that collective punishment is,

445
00:27:20,340 --> 00:27:22,320
is, is scary. It,

446
00:27:22,470 --> 00:27:27,470
if you look at the Geneva Convention
that was established after World War II,

447
00:27:28,530 --> 00:27:33,530
the UN declared collective
punishment a war crime.

448
00:27:34,260 --> 00:27:39,260
So the idea is that when there is a
conflict between ethnic groups in certain

449
00:27:39,691 --> 00:27:42,960
places, if, if one,

450
00:27:42,961 --> 00:27:46,920
if some people are killed in that,
in a conflict, you can't then go on.

451
00:27:46,921 --> 00:27:49,560
Wife had a village as a
collective collective,

452
00:27:49,561 --> 00:27:53,250
retro retribution because that's
how these war crimes happen.

453
00:27:53,790 --> 00:27:56,340
So now where this,
um,

454
00:27:57,030 --> 00:27:59,190
lucky said this collateral damage,

455
00:27:59,220 --> 00:28:01,860
this desire for collateral
damage is being normalized.

456
00:28:01,861 --> 00:28:06,861
Like people don't seem to understand how
dangerous this is and how unjust it is.

457
00:28:08,490 --> 00:28:11,700
What is, is part of the issue that
perhaps they do understand it,

458
00:28:11,701 --> 00:28:15,600
but they so want to destroy the
system. This is, you know, I a,

459
00:28:15,620 --> 00:28:16,920
I've been on tour with Jordan Peterson.

460
00:28:16,921 --> 00:28:20,250
One of the things he's been
talking about a lot lately is yes,

461
00:28:20,251 --> 00:28:23,730
you can acknowledge that we don't have
perfect systems and then there's perfect

462
00:28:23,731 --> 00:28:25,590
systems are probably
impossible to ever get to.

463
00:28:26,190 --> 00:28:28,830
But now we have this
new thing in the system,

464
00:28:28,831 --> 00:28:32,940
which is let's just freaking destroy the
whole thing to build our perfect system.

465
00:28:33,100 --> 00:28:37,450
Yeah. I think people don't
understand because, um,

466
00:28:37,540 --> 00:28:41,560
we're not taught anymore. We're not
taught history properly. We're not,

467
00:28:41,561 --> 00:28:46,561
we don't learn about human nature in
school and how it's almost an instinct to

468
00:28:47,681 --> 00:28:52,150
want to desire retribution,
retribution and vindictive justice.

469
00:28:52,151 --> 00:28:52,984
I think it's,

470
00:28:53,200 --> 00:28:57,910
it's part of our nature is to want
to punish people and punish groups,

471
00:28:58,450 --> 00:28:59,283
um,

472
00:28:59,530 --> 00:29:04,530
and our institutions like a due process
and the presumption of innocence,

473
00:29:04,900 --> 00:29:08,890
they sort of,
they go against our instincts,

474
00:29:09,220 --> 00:29:11,590
but that's why they're
thankfully. Yeah, yeah. Yeah.

475
00:29:11,950 --> 00:29:15,700
And I think what we're seeing is just
sort of like a regression back to our more

476
00:29:15,701 --> 00:29:17,680
tribal instinctive nature.

477
00:29:18,220 --> 00:29:23,200
And because our education system is so
atrophy [inaudible] young people aren't

478
00:29:23,201 --> 00:29:27,790
being taught the value and the
fragility of these institutions,

479
00:29:27,910 --> 00:29:31,480
how they came about, why they're so
important and why we must protect them.

480
00:29:31,810 --> 00:29:36,610
So then I think there's just that basic
ignorance about why these institutions,

481
00:29:36,940 --> 00:29:41,800
institutions exist in the first place
and then we're regressing to our more

482
00:29:41,801 --> 00:29:43,190
primitive natures.

483
00:29:43,340 --> 00:29:46,760
Yeah. How do you think social media fits
into this? Because it's, I did a video,

484
00:29:46,761 --> 00:29:48,500
I think it's almost three years ago now,

485
00:29:48,770 --> 00:29:51,800
that online culture is
becoming mainstream culture.

486
00:29:51,801 --> 00:29:53,870
And I think that that's
what we're seeing now,

487
00:29:53,871 --> 00:29:58,250
what used to be just relegated to the
meme makers and the pet bay people has now

488
00:29:58,251 --> 00:30:00,830
just leaked everywhere.
And just everyone in,

489
00:30:00,840 --> 00:30:05,330
in public is acting sort of as
their worst selves all the time.

490
00:30:05,920 --> 00:30:07,400
And then that feeds all of this.

491
00:30:07,401 --> 00:30:10,560
Let's either destroy the system or
let's ignore legitimate problems.

492
00:30:11,200 --> 00:30:13,480
Well,
when you don't have proper law and order,

493
00:30:13,481 --> 00:30:17,050
you do get mob justice in
vigilante justice in real life.

494
00:30:17,080 --> 00:30:21,130
And you can see that throughout
history you've got, you get lynchings.

495
00:30:21,490 --> 00:30:24,580
Um, you get pogroms, um,

496
00:30:24,610 --> 00:30:29,610
you get groups like mobs of people going
and attacking an individual who they

497
00:30:30,161 --> 00:30:35,050
think is guilty of like a sex crime or
some other crime and killing the person.

498
00:30:35,470 --> 00:30:37,810
So,
and then what happened was,

499
00:30:37,811 --> 00:30:42,811
is we develop legal and social norms that
stopped that kind of vigilante justice

500
00:30:43,781 --> 00:30:47,060
from taking place. And if you
are a vigilante, you can just,

501
00:30:47,080 --> 00:30:48,640
you can get in trouble with the law.

502
00:30:49,660 --> 00:30:54,160
And then we've developed social norms
that prevent people from just forming mobs

503
00:30:54,161 --> 00:30:56,020
and going attacking people in the street.

504
00:30:56,650 --> 00:31:00,250
But social media is this
new technology and we don't,

505
00:31:00,251 --> 00:31:03,220
haven't developed any of the
legal or social norms yet.

506
00:31:03,400 --> 00:31:06,670
And so it's brought back
this mobbing behavior,

507
00:31:06,671 --> 00:31:09,700
which is part of our nature.
It's brought it back.

508
00:31:09,701 --> 00:31:14,590
But we haven't yet developed any of the
norms to sort of suppress it or mitigate

509
00:31:14,591 --> 00:31:18,670
against it. Yeah. And I think we
eventually will develop these norms,

510
00:31:18,671 --> 00:31:22,060
but it's gonna take awhile and there's
going to be a fee innocent people sort of

511
00:31:22,570 --> 00:31:24,670
metaphorically lynched in the process.

512
00:31:24,880 --> 00:31:27,790
Yeah. So that's actually an interesting
segue to something that's been on my mind

513
00:31:27,791 --> 00:31:28,930
lately. So, you know,

514
00:31:28,931 --> 00:31:33,010
Alex Jones got booted from Twitter
and Youtube and Facebook or whatever.

515
00:31:33,820 --> 00:31:37,300
I don't even want to talk about any
of the things that he talks about.

516
00:31:37,301 --> 00:31:39,110
It does not matter,
but,

517
00:31:39,280 --> 00:31:43,030
but these tech companies basically decided
that this one guy is too dangerous.

518
00:31:43,031 --> 00:31:45,910
You know, Farah can can be on
here and Hamas, whoever you,

519
00:31:45,950 --> 00:31:48,850
whoever else you want to pick that you
don't like can be on here. But this guy,

520
00:31:48,851 --> 00:31:52,240
you know, even paypal took
them out last week. Um,

521
00:31:52,570 --> 00:31:55,930
do you think that the free speech
crew for getting his ideas,

522
00:31:56,260 --> 00:32:01,210
do you think we should have offered
a better defense of why this person,

523
00:32:01,740 --> 00:32:04,060
boat person should be allowed
to use these things? I mean,

524
00:32:04,061 --> 00:32:06,890
should the guy be allowed to make
a phone call? Should he allow it?

525
00:32:06,940 --> 00:32:09,180
Should he be allowed to have
running water at his house? Yeah.

526
00:32:09,250 --> 00:32:10,690
Where do we draw the lines for this?

527
00:32:11,030 --> 00:32:14,450
Yeah. Well I think these tools, uh,

528
00:32:14,660 --> 00:32:18,770
that we use such as payment processing
and social media such as Twitter,

529
00:32:18,771 --> 00:32:21,680
the instant messaging, the, the tools,

530
00:32:21,740 --> 00:32:24,770
and they should be treated as,
as such,

531
00:32:24,771 --> 00:32:28,430
like having electricity or water.
And um,

532
00:32:28,590 --> 00:32:31,380
so you believe there are public
good, basically the same. Yeah.

533
00:32:31,760 --> 00:32:34,710
Getting water to your house, getting
the phone line connected to your house.

534
00:32:34,760 --> 00:32:38,750
Yeah. And, and they
shouldn't be politicized.

535
00:32:39,020 --> 00:32:41,660
And as soon as you politicize these tools,
then it's,

536
00:32:41,661 --> 00:32:44,450
there's a slippery slope and you know,

537
00:32:44,451 --> 00:32:47,900
then there's going to be arguments
about who do we band next,

538
00:32:48,350 --> 00:32:51,410
what standards do we want to
sit and that kind of thing.

539
00:32:52,580 --> 00:32:54,940
And in the context of Alex Jones, I, I,

540
00:32:55,040 --> 00:32:58,340
I think there's probably more to the
story that the public is aware of,

541
00:32:58,341 --> 00:33:01,340
so I don't want to comment
on him specifically.

542
00:33:01,460 --> 00:33:02,930
Yeah. And I don't want
to make it a bit, yeah,

543
00:33:03,000 --> 00:33:07,620
just the general idea that a digital
assassination can occur because we're all

544
00:33:07,621 --> 00:33:11,250
agreeing to terms of services that I'm
not sitting around with a lawyer every

545
00:33:11,251 --> 00:33:14,190
time I click except to the new service.
Yeah.

546
00:33:14,430 --> 00:33:18,140
And that eventually you could move
on anybody, you know what I mean?

547
00:33:18,330 --> 00:33:19,710
We know the way these ideas work.

548
00:33:19,711 --> 00:33:23,340
You don't get one person and then suddenly
be like, oh well we're good to go.

549
00:33:23,670 --> 00:33:24,710
It's like who do you get neck?

550
00:33:24,820 --> 00:33:29,080
Yeah. And the PA, the payment
processing is, is scary. I mean,

551
00:33:29,560 --> 00:33:33,220
the idea that that should be
politicized is just, that is crazy.

552
00:33:33,380 --> 00:33:36,140
Yeah. Hmm. So what do you
think the solution is? I mean,

553
00:33:36,170 --> 00:33:40,280
I think you probably know my, my solution
is that we need more competition,

554
00:33:40,470 --> 00:33:40,880
but I'm,

555
00:33:40,880 --> 00:33:45,670
I'm really understanding of big counter
to that because it's like where is the

556
00:33:45,671 --> 00:33:46,520
county?
Where is it?

557
00:33:46,560 --> 00:33:49,650
Yeah. No, look, I don't have the answer.

558
00:33:49,651 --> 00:33:53,760
And I tend towards being, um, uh,

559
00:33:54,130 --> 00:33:58,920
economically liberal. However,
some of these tools, as you said,

560
00:33:58,921 --> 00:34:03,570
are public goods and perhaps need to be
thought of as public widget rather than

561
00:34:04,080 --> 00:34:07,710
rather than private. And
um, you know, I'm not,

562
00:34:08,160 --> 00:34:10,170
I'm open to ideas and regulation,

563
00:34:10,680 --> 00:34:13,440
government intervention and um,

564
00:34:13,520 --> 00:34:18,520
and I'm also open to ideas about breaking
up some of the monopolies because we,

565
00:34:21,570 --> 00:34:22,890
competition would be lovely,

566
00:34:22,891 --> 00:34:27,891
but it's a bit difficult at the moment
because some of these companies have

567
00:34:28,410 --> 00:34:32,970
gotten monopolies in lots of different
verticals. So Google for example,

568
00:34:32,971 --> 00:34:37,560
has the search engine monopoly they've
got that almost got the advertising

569
00:34:37,561 --> 00:34:41,940
monopoly, so they're leveraging their
monopoly in one area into others.

570
00:34:41,970 --> 00:34:44,700
And that that just shuts down competition.

571
00:34:44,760 --> 00:34:47,460
Yeah. Do you think it's even something
a little more perverse than that,

572
00:34:47,461 --> 00:34:48,960
which is that, you know, look,

573
00:34:48,961 --> 00:34:51,600
there's obviously I'm not a
blow the lid on anything here.

574
00:34:51,601 --> 00:34:54,840
There's obviously tons of discussions
with all sorts of people about how do we

575
00:34:54,841 --> 00:34:57,500
compete with youtube or how do we
compete with any of these things? Yeah.

576
00:34:57,570 --> 00:34:59,850
But I think partly what's happening,
it's not just people who are going there,

577
00:34:59,851 --> 00:35:01,560
this,
this big monolith.

578
00:35:01,980 --> 00:35:05,730
I think people are actually genuinely
afraid of what it means to go up against

579
00:35:05,731 --> 00:35:09,180
them. It's not like it's going to just
be a hard work situation. It's like,

580
00:35:09,390 --> 00:35:14,390
what does it actually mean
to go up against the world's
information leader, come

581
00:35:14,440 --> 00:35:17,760
compete against the monopoly.
That's the definition of a monopoly.

582
00:35:17,761 --> 00:35:19,680
There's no competition.
And these,

583
00:35:19,681 --> 00:35:23,820
some of these companies are monopolies
and they have captured the market so

584
00:35:23,821 --> 00:35:26,790
effectively that you
cannot compete with them.

585
00:35:27,270 --> 00:35:32,270
And so we either have to think about
regulating them so that they're free and

586
00:35:32,611 --> 00:35:36,030
open to everybody or,
um,

587
00:35:36,060 --> 00:35:37,830
breaking up their

588
00:35:38,310 --> 00:35:42,090
ability to control the
market. I mean, yeah. Yeah.

589
00:35:42,120 --> 00:35:45,690
This is the tough part for the Libertarian
Board. He was like, no, don't do it yet.

590
00:35:45,760 --> 00:35:49,560
I do send some being pinned into,
yeah, that, that kind of answer

591
00:35:50,250 --> 00:35:53,080
[inaudible].

