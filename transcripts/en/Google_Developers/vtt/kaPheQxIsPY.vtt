WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.952
[MUSIC PLAYING]

00:00:10.340 --> 00:00:12.120
MAYA GUPTA: TensorFlow
Lattice is useful

00:00:12.120 --> 00:00:14.150
when you have global
trends you want your model

00:00:14.150 --> 00:00:17.790
to respect even if your
training data is very noisy.

00:00:17.790 --> 00:00:20.130
For example, imagine
you're training a model

00:00:20.130 --> 00:00:22.890
to predict the time it will
take to get across town.

00:00:22.890 --> 00:00:25.920
You know that as traffic gets
worse it should take longer.

00:00:25.920 --> 00:00:28.920
But with noisy training examples
and a very flexible model,

00:00:28.920 --> 00:00:32.020
the machine learning might
not pick up this basic trend.

00:00:32.020 --> 00:00:33.840
TensorFlow Lattice
can ensure your model

00:00:33.840 --> 00:00:36.330
learns the trends you want.

00:00:36.330 --> 00:00:37.670
So what is a lattice?

00:00:37.670 --> 00:00:40.020
A lattice is an
interpolated lookup table

00:00:40.020 --> 00:00:42.480
that can approximate any
input/output relations

00:00:42.480 --> 00:00:44.100
from your data.

00:00:44.100 --> 00:00:45.960
You might remember
seeing a simple lattice

00:00:45.960 --> 00:00:47.820
in the back of your
geometry textbook

00:00:47.820 --> 00:00:49.860
to approximate
the sine function.

00:00:49.860 --> 00:00:53.560
That was an interpolated
lookup table on just one input.

00:00:53.560 --> 00:00:56.010
And if the exact value you're
looking for is in the table,

00:00:56.010 --> 00:00:56.924
you can use it.

00:00:56.924 --> 00:00:59.340
Otherwise, you find the nearest
values in the lookup table

00:00:59.340 --> 00:01:01.590
to the one you want, and
you interpolate between them

00:01:01.590 --> 00:01:03.980
to get the output you want.

00:01:03.980 --> 00:01:06.030
Lattices are interpolated
lookup tables

00:01:06.030 --> 00:01:08.380
that could be keyed
by multiple inputs

00:01:08.380 --> 00:01:11.200
so we can approximate
multi-dimensional functions.

00:01:11.200 --> 00:01:13.890
In fact, if you use a lattice
with enough lookup table

00:01:13.890 --> 00:01:18.050
values, you can represent any
input/output relationship.

00:01:18.050 --> 00:01:20.510
How does it figure out
the lookup table values?

00:01:20.510 --> 00:01:22.370
The machine learning
trains the lookup table

00:01:22.370 --> 00:01:25.130
values on your training
examples to minimize the loss.

00:01:29.590 --> 00:01:32.500
SEUNGIL YOU: Lattices can help
us represent any function,

00:01:32.500 --> 00:01:35.980
but what's great about lattices
is you can restrict them

00:01:35.980 --> 00:01:38.560
so that they behave
sensibly rather than feeding

00:01:38.560 --> 00:01:40.570
the noise in the data.

00:01:40.570 --> 00:01:42.820
Suppose you're designing
a model to recommend

00:01:42.820 --> 00:01:45.110
coffee shops to users.

00:01:45.110 --> 00:01:47.890
You may want your system to
recommend the closest coffee

00:01:47.890 --> 00:01:50.880
shop if all other
characteristics of each coffee

00:01:50.880 --> 00:01:53.330
shop are exactly the same.

00:01:53.330 --> 00:01:57.310
This type of prior knowledge
is called monotonicity.

00:01:57.310 --> 00:01:59.440
If we have noisy
data, machine learning

00:01:59.440 --> 00:02:02.450
doesn't always pick up
these global trends.

00:02:02.450 --> 00:02:06.620
For example, say you train your
model on examples from Tokyo

00:02:06.620 --> 00:02:09.289
where you can walk
to many coffee shops.

00:02:09.289 --> 00:02:11.740
So you don't have much
training data for coffee shops

00:02:11.740 --> 00:02:13.630
a few kilometers out.

00:02:13.630 --> 00:02:16.300
And the data is noisy,
so the model doesn't

00:02:16.300 --> 00:02:18.070
learn the global trend--

00:02:18.070 --> 00:02:21.010
users prefer nearer
coffee shops.

00:02:21.010 --> 00:02:23.410
If we try to use the
model in Texas where

00:02:23.410 --> 00:02:26.110
you have to drive
to get coffee, we

00:02:26.110 --> 00:02:29.110
may be surprised by the
model's unexpected behavior

00:02:29.110 --> 00:02:31.510
for faraway coffee shops.

00:02:31.510 --> 00:02:33.400
With lattice models,
you can easily

00:02:33.400 --> 00:02:36.640
train a model that captures
your prior knowledge about which

00:02:36.640 --> 00:02:39.970
subset of inputs should be
monotonically increasing

00:02:39.970 --> 00:02:42.220
or decreasing inputs.

00:02:42.220 --> 00:02:44.710
Under the hood, we
express monotonicity

00:02:44.710 --> 00:02:47.980
as constraints on the
lookup table parameters.

00:02:47.980 --> 00:02:49.900
We formulate the
machine learning task

00:02:49.900 --> 00:02:52.270
as a constraint
optimization problem

00:02:52.270 --> 00:02:54.820
and use TensorFlow to train it.

00:02:54.820 --> 00:02:57.370
After the training is
done, the trained model

00:02:57.370 --> 00:03:01.120
satisfies any monotonic
relationship you choose.

00:03:01.120 --> 00:03:04.030
Going back to the coffee
shop recommendation example,

00:03:04.030 --> 00:03:06.970
you can specify that
the recommendation score

00:03:06.970 --> 00:03:10.090
decreases with the distance
but increases with the coffee

00:03:10.090 --> 00:03:11.440
quality.

00:03:11.440 --> 00:03:15.310
The monotonic model can behave
much more sensibly on the Texas

00:03:15.310 --> 00:03:18.250
examples compared to a
non-monotonic flexible

00:03:18.250 --> 00:03:20.150
function.

00:03:20.150 --> 00:03:23.310
Capturing your prior knowledge
about monotonic relationships

00:03:23.310 --> 00:03:25.670
really helps such
analyzation, especially when

00:03:25.670 --> 00:03:27.710
the testing data
distribution is very

00:03:27.710 --> 00:03:30.609
different from the
training data distribution.

00:03:35.010 --> 00:03:36.660
JAN PFEIFER: In
TensorFlow Lattice,

00:03:36.660 --> 00:03:39.120
we provide four
TensorFlow estimators

00:03:39.120 --> 00:03:41.580
that are easy to use
and fit different types

00:03:41.580 --> 00:03:43.590
and sizes of problems.

00:03:43.590 --> 00:03:46.200
We also provide a
hyperparameters class

00:03:46.200 --> 00:03:48.780
that allows to specify
for each feature

00:03:48.780 --> 00:03:51.810
its monotonicity, its smoothness
regularization, and how

00:03:51.810 --> 00:03:54.090
to handle its missing value.

00:03:54.090 --> 00:03:57.220
Besides the usual L1
and L2 regularizers,

00:03:57.220 --> 00:04:00.480
Lattice models also support
graph Laplacian and torsion

00:04:00.480 --> 00:04:02.040
regularizers.

00:04:02.040 --> 00:04:04.470
Both can help your
model generalize better

00:04:04.470 --> 00:04:06.960
to unexplored parts
of your feature space

00:04:06.960 --> 00:04:08.580
and can be configured
per feature

00:04:08.580 --> 00:04:10.980
with the hyperparameters class.

00:04:10.980 --> 00:04:15.240
TensorFlow Lattice can also
help you build customized models

00:04:15.240 --> 00:04:17.360
by providing
calibration and lattices

00:04:17.360 --> 00:04:20.880
as layers or as individual
TensorFlow operators.

00:04:20.880 --> 00:04:24.230
For instance, you could extend
your already-existing model

00:04:24.230 --> 00:04:25.980
with a lattice
layer that connects

00:04:25.980 --> 00:04:29.640
your model and monotonic inputs
and have a mixed monotonic

00:04:29.640 --> 00:04:30.564
model.

00:04:30.564 --> 00:04:31.980
MAYA GUPTA: We're
excited to share

00:04:31.980 --> 00:04:34.080
TensorFlow Lattice with
the developer community

00:04:34.080 --> 00:04:35.914
and invite you to experiment.

00:04:35.914 --> 00:04:37.830
You can learn more about
the technical details

00:04:37.830 --> 00:04:40.410
from our paper, "Deep
Lattice Networks."

00:04:40.410 --> 00:04:42.690
And we wish everyone,
happy machine learning.

00:04:42.690 --> 00:04:46.040
[MUSIC PLAYING]

