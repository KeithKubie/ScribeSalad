WEBVTT
Kind: captions
Language: en

00:00:01.980 --> 00:00:04.640
STACIE CHAN: Welcome, everyone,
to another Media Talks

00:00:04.640 --> 00:00:05.550
at Google.

00:00:05.550 --> 00:00:07.030
My name is Stacie Chan.

00:00:07.030 --> 00:00:08.940
And I'm with the
Google news team.

00:00:08.940 --> 00:00:11.300
And the team and I
are so incredibly

00:00:11.300 --> 00:00:14.020
excited to have Richard Engel
here in the Mountain View

00:00:14.020 --> 00:00:14.540
office.

00:00:14.540 --> 00:00:14.690
I know.

00:00:14.690 --> 00:00:16.810
I see, like, silent claps
already in the audience.

00:00:16.810 --> 00:00:17.570
That's so great.

00:00:17.570 --> 00:00:18.180
[APPLAUSE]

00:00:18.180 --> 00:00:22.045
Well, we'll save-- OK, we'll do
the big round of applause now.

00:00:22.045 --> 00:00:23.420
I'm so excited to
have him here--

00:00:23.420 --> 00:00:24.270
RICHARD ENGEL: It's
great to be here.

00:00:24.270 --> 00:00:26.340
STACIE CHAN: --as the next
speaker in our Media Talks

00:00:26.340 --> 00:00:27.050
at Google series.

00:00:27.050 --> 00:00:29.980
And we're really looking forward
to hearing Richard's insights

00:00:29.980 --> 00:00:33.410
into the ever-changing
media industry,

00:00:33.410 --> 00:00:35.370
especially as newsrooms
are figuring out

00:00:35.370 --> 00:00:40.170
their own digital revolutions
at their own pace, we'll say.

00:00:40.170 --> 00:00:42.580
And it was really fortuitous
that we got Richard here.

00:00:42.580 --> 00:00:45.160
I don't know if any of you were
able to see his commencement

00:00:45.160 --> 00:00:47.520
speech at Stanford
University yesterday.

00:00:47.520 --> 00:00:51.000
But he was just
there, across the way.

00:00:51.000 --> 00:00:53.100
And we reached out
to him and said,

00:00:53.100 --> 00:00:55.330
could you actually
extend your trip

00:00:55.330 --> 00:00:58.380
to the Bay Area for just a
day and come speak at Google?

00:00:58.380 --> 00:01:00.160
And he graciously said yes.

00:01:00.160 --> 00:01:02.870
So we were able to get him here
at the Mountain View campus.

00:01:02.870 --> 00:01:03.980
RICHARD ENGEL: Oh,
it's not a bad place

00:01:03.980 --> 00:01:05.020
to spend an extra day.

00:01:05.020 --> 00:01:05.820
STACIE CHAN: Yeah.

00:01:05.820 --> 00:01:07.610
RICHARD ENGEL: You know
where I normally live.

00:01:07.610 --> 00:01:08.350
STACIE CHAN: It's not as hectic.

00:01:08.350 --> 00:01:09.140
RICHARD ENGEL: Certainly not.

00:01:09.140 --> 00:01:09.925
STACIE CHAN: People
biking around campus.

00:01:09.925 --> 00:01:10.210
RICHARD ENGEL:
Yeah, that's great.

00:01:10.210 --> 00:01:12.460
STACIE CHAN: And you might
have to watch out for them.

00:01:12.460 --> 00:01:14.700
But otherwise, pretty
safe here, I guess.

00:01:14.700 --> 00:01:17.790
As always, we're going to
have plenty of time for Q&amp;A

00:01:17.790 --> 00:01:19.330
at the end.

00:01:19.330 --> 00:01:21.690
So we're passing around a mic.

00:01:21.690 --> 00:01:25.190
Everyone watching over the
live stream, we have a Dory.

00:01:25.190 --> 00:01:31.265
Go/richardengel, spelling
E-N-G-E-L. Richard,

00:01:31.265 --> 00:01:32.140
I sort of explained--

00:01:32.140 --> 00:01:32.430
RICHARD ENGEL: These are the--

00:01:32.430 --> 00:01:34.335
STACIE CHAN: These are the
internal questions asked by--

00:01:34.335 --> 00:01:35.220
RICHARD ENGEL: --people
who are online now,

00:01:35.220 --> 00:01:35.840
watching internally.

00:01:35.840 --> 00:01:36.826
STACIE CHAN: Yes, they're
watching at their desks

00:01:36.826 --> 00:01:38.116
in different time zones.

00:01:38.116 --> 00:01:39.990
Richard is looking
forward to your questions.

00:01:39.990 --> 00:01:41.650
So ask some good ones.

00:01:41.650 --> 00:01:44.540
He's a journalist, so he's used
asking the questions to you.

00:01:44.540 --> 00:01:46.534
Now we get to turn
the tables on him.

00:01:46.534 --> 00:01:47.450
So that should be fun.

00:01:47.450 --> 00:01:48.617
RICHARD ENGEL: Thanks a lot.

00:01:48.617 --> 00:01:50.199
STACIE CHAN: Nothing
you can't handle.

00:01:50.199 --> 00:01:52.860
I think you've been in other
precarious situations far worse

00:01:52.860 --> 00:01:53.942
than this.

00:01:53.942 --> 00:01:55.400
So I'll start off
with a brief bio,

00:01:55.400 --> 00:01:59.100
and then we'll just get
started into questions.

00:01:59.100 --> 00:02:02.920
So Richard Engel is NBC News'
chief foreign correspondent.

00:02:02.920 --> 00:02:06.290
He's held that
position since 2008.

00:02:06.290 --> 00:02:08.240
But Richard is a
lifelong journalist.

00:02:08.240 --> 00:02:11.590
Since graduating
Stanford in 1996,

00:02:11.590 --> 00:02:14.820
he intrepidly decided
to leave to Cairo

00:02:14.820 --> 00:02:17.280
and become a journalist
in the Middle East.

00:02:17.280 --> 00:02:22.680
Since then, covered wars,
conflicts, disasters,

00:02:22.680 --> 00:02:27.480
revolutions for nearly 20
years all over the world

00:02:27.480 --> 00:02:30.340
in some of the most remote
countries, probably some

00:02:30.340 --> 00:02:32.600
that I have never even heard of.

00:02:32.600 --> 00:02:36.560
In 2003, he went to
Iraq to cover the war.

00:02:36.560 --> 00:02:38.990
And he was the only
American television

00:02:38.990 --> 00:02:41.280
corresponded to be
stationed in Iraq

00:02:41.280 --> 00:02:42.860
for the entirety of the war.

00:02:42.860 --> 00:02:45.740
So pretty much the leading
expert on all things

00:02:45.740 --> 00:02:47.900
Iraq and the Middle East.

00:02:47.900 --> 00:02:51.700
Then joined NBC news in 2003.

00:02:51.700 --> 00:02:54.720
Became the Middle East
correspondent, then the Beirut

00:02:54.720 --> 00:02:57.070
bureau chief, and
then now into his role

00:02:57.070 --> 00:02:59.680
as the chief foreign
correspondent.

00:02:59.680 --> 00:03:01.600
RICHARD ENGEL: She's
memorized all of this.

00:03:01.600 --> 00:03:02.540
STACIE CHAN: He's a
fascinating person.

00:03:02.540 --> 00:03:04.435
RICHARD ENGEL: She hasn't
looked down once, by the way.

00:03:04.435 --> 00:03:05.570
I was just saying.

00:03:05.570 --> 00:03:07.240
STACIE CHAN: This
kind of experience--

00:03:07.240 --> 00:03:07.940
RICHARD ENGEL: I don't
remember my resume that well.

00:03:07.940 --> 00:03:09.990
STACIE CHAN: --is
really easy to recap.

00:03:09.990 --> 00:03:12.491
I could always be your PR person
if Google doesn't work out.

00:03:12.491 --> 00:03:14.781
RICHARD ENGEL: If this little
company doesn't work out,

00:03:14.781 --> 00:03:17.167
if it folds tomorrow, I
think we can talk about it.

00:03:17.167 --> 00:03:18.250
But I don't think it will.

00:03:18.250 --> 00:03:20.208
STACIE CHAN: And I know
someone in the audience

00:03:20.208 --> 00:03:22.110
wanted to talk
about Richard's time

00:03:22.110 --> 00:03:24.860
covering the Syrian
civil war when many of us

00:03:24.860 --> 00:03:28.470
remember in December, 2012,
anxiously watching the news

00:03:28.470 --> 00:03:34.506
reports of his kidnapping when
he and his five crew members?

00:03:34.506 --> 00:03:36.380
RICHARD ENGEL: There
were six of us in total.

00:03:36.380 --> 00:03:38.240
But that's right.

00:03:38.240 --> 00:03:40.945
STACIE CHAN: So I'll definitely
ask some questions about that.

00:03:40.945 --> 00:03:41.592
RICHARD ENGEL:
We'll get into it.

00:03:41.592 --> 00:03:42.380
We'll get into it.

00:03:42.380 --> 00:03:44.270
STACIE CHAN: But thankfully,
as you can see, he and his crew

00:03:44.270 --> 00:03:45.600
escaped and made it out alive.

00:03:45.600 --> 00:03:48.600
So very glad he's
here today with us.

00:03:48.600 --> 00:03:52.140
And then most recently, you've
got a documentary coming out

00:03:52.140 --> 00:03:55.180
about Nepal, the
devastating earthquake that

00:03:55.180 --> 00:03:58.480
happened just last month--
two months ago now--

00:03:58.480 --> 00:04:05.670
April 25, killing 9,000 people,
injuring countless others.

00:04:05.670 --> 00:04:08.234
That documentary is airing--
remind me of the date?

00:04:08.234 --> 00:04:09.150
MALE SPEAKER: June 28.

00:04:09.150 --> 00:04:09.820
RICHARD ENGEL: June 28.

00:04:09.820 --> 00:04:10.695
STACIE CHAN: June 28.

00:04:11.194 --> 00:04:13.705
RICHARD ENGEL: No, actually
since she brought it up, it's--

00:04:13.705 --> 00:04:15.330
STACIE CHAN: Sure,
we can start there.f

00:04:15.330 --> 00:04:17.670
RICHARD ENGEL:
When we went there,

00:04:17.670 --> 00:04:21.620
I think we arrived about 15 or
16 hours after the earthquake

00:04:21.620 --> 00:04:23.010
had struck.

00:04:23.010 --> 00:04:26.520
And we happened to be
in Istanbul, Turkey.

00:04:26.520 --> 00:04:29.260
And not many places
were flying to Nepal.

00:04:29.260 --> 00:04:31.910
And there was a flight
leaving from Istanbul

00:04:31.910 --> 00:04:34.340
that was originally
canceled, and then the flight

00:04:34.340 --> 00:04:37.560
got taken over by
Turkish aid workers.

00:04:37.560 --> 00:04:39.110
And we managed to
get on that flight.

00:04:39.110 --> 00:04:41.110
So it was just us and
Turkish aid workers

00:04:41.110 --> 00:04:43.960
and a few other journalists
and the rescue dogs.

00:04:43.960 --> 00:04:45.325
And we got on the flight.

00:04:45.325 --> 00:04:47.860
And I wasn't sure if it was
going to take off at all.

00:04:47.860 --> 00:04:48.970
And we got there.

00:04:48.970 --> 00:04:50.260
And it did.

00:04:50.260 --> 00:04:52.230
And it landed in Kathmandu.

00:04:52.230 --> 00:04:55.310
And the earth was still
rolling with aftershocks.

00:04:55.310 --> 00:05:00.100
When we got into the landing
arrival hall and an aftershock

00:05:00.100 --> 00:05:00.600
hit.

00:05:00.600 --> 00:05:03.050
And everybody clears
out because we

00:05:03.050 --> 00:05:05.300
thought the building was
going to come down, including

00:05:05.300 --> 00:05:06.980
the customs officials--
including the customs

00:05:06.980 --> 00:05:08.188
official who had my passport.

00:05:08.188 --> 00:05:10.080
I was like, wait!

00:05:10.080 --> 00:05:11.380
And then I'm in a dilemma.

00:05:11.380 --> 00:05:13.350
I was like, do I
enter the country?

00:05:13.350 --> 00:05:17.519
It's like, imagine you arrived
at JFK and everyone leaves.

00:05:17.519 --> 00:05:19.810
And you're standing there,
and they have your passport.

00:05:19.810 --> 00:05:21.900
Do you go in?

00:05:21.900 --> 00:05:23.812
So I waited 'til
the guy came back.

00:05:23.812 --> 00:05:26.020
I was like, (WHISPERING)
can I have my passport back?

00:05:26.020 --> 00:05:27.590
So he stamped me in
because I figured

00:05:27.590 --> 00:05:30.545
I'd have trouble leaving,
which I would have had trouble.

00:05:30.545 --> 00:05:34.180
And we stayed in Kathmandu
and the surrounding

00:05:34.180 --> 00:05:36.900
area for the first
week or so, reporting

00:05:36.900 --> 00:05:39.580
on the earthquake and the
buildings that went down

00:05:39.580 --> 00:05:41.965
and the temples
that were destroyed

00:05:41.965 --> 00:05:44.290
and the reactions
people were having

00:05:44.290 --> 00:05:46.880
to that, how everyone
kind of moved outside.

00:05:46.880 --> 00:05:49.170
It was a really
strange phenomenon

00:05:49.170 --> 00:05:52.130
that I hadn't expected,
that everyone just

00:05:52.130 --> 00:05:56.980
suddenly went to live in the
parks and was calm about it.

00:05:56.980 --> 00:05:57.880
There was no looting.

00:05:57.880 --> 00:05:59.800
There was no
violence that I saw.

00:05:59.800 --> 00:06:02.760
People were not hostile.

00:06:02.760 --> 00:06:05.980
They were incredibly
kind and receptive to us.

00:06:05.980 --> 00:06:08.640
And they just were
taking it as it came.

00:06:08.640 --> 00:06:11.940
Again, like New York City,
imagine everyone moves

00:06:11.940 --> 00:06:15.550
into Central Park all at
once and is living there

00:06:15.550 --> 00:06:16.370
for a while.

00:06:16.370 --> 00:06:17.802
And there's no fighting.

00:06:17.802 --> 00:06:18.710
STACIE CHAN: That
would be incredible.

00:06:18.710 --> 00:06:20.060
RICHARD ENGEL: And all
the buildings are empty.

00:06:20.060 --> 00:06:22.510
And everyone's just sort of
chilling out in Central Park

00:06:22.510 --> 00:06:23.956
without anyone
telling them to do

00:06:23.956 --> 00:06:26.080
this-- no public announcements,
go to Central Park.

00:06:26.080 --> 00:06:27.277
They just did it.

00:06:27.277 --> 00:06:29.235
STACIE CHAN: So you don't
have to worry about--

00:06:29.235 --> 00:06:30.660
RICHARD ENGEL: I'm almost done.

00:06:30.660 --> 00:06:35.870
And then we went up to
Mount Everest base camp

00:06:35.870 --> 00:06:40.950
because the earthquake triggered
an avalanche that devastated

00:06:40.950 --> 00:06:43.310
the central part of base camp.

00:06:43.310 --> 00:06:49.710
There's a base camp at about,
almost 18,000 feet-- 17,598

00:06:49.710 --> 00:06:50.650
feet.

00:06:50.650 --> 00:06:50.950
STACIE CHAN: High.

00:06:50.950 --> 00:06:51.250
Very high.

00:06:51.250 --> 00:06:52.083
RICHARD ENGEL: High.

00:06:52.083 --> 00:06:56.360
And this base camp is where
the climbers go and congregate,

00:06:56.360 --> 00:06:58.850
and then attempt to make
the ascent on Everest.

00:06:58.850 --> 00:07:02.330
Or sometimes trekkers will
just go there and visit it.

00:07:02.330 --> 00:07:04.770
And I know Google had a
terrible experience up there.

00:07:04.770 --> 00:07:06.020
And I'm very sorry.

00:07:06.020 --> 00:07:07.392
STACIE CHAN: Thank you.

00:07:07.392 --> 00:07:09.100
RICHARD ENGEL: So we
wanted to go and see

00:07:09.100 --> 00:07:11.630
what was going on there.

00:07:11.630 --> 00:07:15.450
And we got to the
Everest base camp,

00:07:15.450 --> 00:07:19.101
saw the devastation there,
and we're doing a documentary

00:07:19.101 --> 00:07:21.100
that I think because of
your terrible experience

00:07:21.100 --> 00:07:22.933
might be very interesting
to a lot of people

00:07:22.933 --> 00:07:26.680
at this company about what
happened at Everest base

00:07:26.680 --> 00:07:28.830
camp told through a
group of survivors.

00:07:28.830 --> 00:07:29.830
STACIE CHAN: Absolutely.

00:07:29.830 --> 00:07:30.497
Well, thank you.

00:07:30.497 --> 00:07:32.079
Yeah, we'll definitely
keep an eye out

00:07:32.079 --> 00:07:34.180
for that documentary
airing sometime this month.

00:07:34.180 --> 00:07:34.530
RICHARD ENGEL: 28th.

00:07:34.530 --> 00:07:35.446
STACIE CHAN: The 28th.

00:07:35.446 --> 00:07:36.960
OK, we've got it on schedule.

00:07:36.960 --> 00:07:39.960
So you said that when you land
in a place like Kathmandu,

00:07:39.960 --> 00:07:42.470
the ground is still rolling.

00:07:42.470 --> 00:07:45.210
With such unpredictable
factors like Mother Nature,

00:07:45.210 --> 00:07:48.720
how do you ensure yours
and your crew's safety

00:07:48.720 --> 00:07:51.719
when you go into some of these
very dangerous situations?

00:07:51.719 --> 00:07:53.260
RICHARD ENGEL: I
actually don't cover

00:07:53.260 --> 00:07:54.690
that many natural disasters.

00:07:54.690 --> 00:07:55.950
This was an unusual thing.

00:07:55.950 --> 00:07:57.830
Mostly I cover
man-made disasters.

00:07:57.830 --> 00:08:00.080
STACIE CHAN: Which are,
arguably, even more dangerous.

00:08:00.080 --> 00:08:02.538
RICHARD ENGEL: Yeah, which I
think are much more dangerous.

00:08:02.538 --> 00:08:05.314
I mean, in a natural
disaster, OK,

00:08:05.314 --> 00:08:07.564
you're in a building and it
either falls on top of you

00:08:07.564 --> 00:08:08.920
or it doesn't.

00:08:08.920 --> 00:08:10.112
That's pretty easy.

00:08:10.112 --> 00:08:11.570
And then in a
natural disaster, you

00:08:11.570 --> 00:08:13.720
have to deal with the
hostility of the people.

00:08:13.720 --> 00:08:14.370
Are they angry?

00:08:14.370 --> 00:08:15.578
Are they going to be looting?

00:08:15.578 --> 00:08:17.970
Is there going to be theft?

00:08:17.970 --> 00:08:21.180
That's relatively
straightforward.

00:08:21.180 --> 00:08:23.300
Most of the time,
however, in a war zone,

00:08:23.300 --> 00:08:25.008
it's much more
complicated because you're

00:08:25.008 --> 00:08:26.990
dealing with rebel groups.

00:08:26.990 --> 00:08:28.880
You're dealing with
government troops.

00:08:28.880 --> 00:08:30.000
You're dealing with guns.

00:08:30.000 --> 00:08:33.400
You're dealing with
possible kidnappers.

00:08:33.400 --> 00:08:36.850
You're dealing with
possibly soldiers who

00:08:36.850 --> 00:08:40.600
are rebels, who are drunk, who
don't know what they're doing,

00:08:40.600 --> 00:08:44.657
who have a grievance against
you or have a perceived

00:08:44.657 --> 00:08:46.490
grievance because you
don't know necessarily

00:08:46.490 --> 00:08:48.920
what's in their mind.

00:08:48.920 --> 00:08:50.480
A friend of mine,
a colleague, said

00:08:50.480 --> 00:08:52.800
the most frightening thing
he ever saw in his life

00:08:52.800 --> 00:08:55.610
was pulling up to a checkpoint--
I think it was in Somalia--

00:08:55.610 --> 00:08:59.200
and there was a 14-year-old
boy with a blond wig

00:08:59.200 --> 00:09:02.119
on holding an AK 47.

00:09:02.119 --> 00:09:04.160
Because that's person who
controls whether you're

00:09:04.160 --> 00:09:05.230
going to live or die.

00:09:05.230 --> 00:09:08.660
So it's not the same
as an earthquake.

00:09:08.660 --> 00:09:11.880
You're dealing with someone
who's-- god only knows what

00:09:11.880 --> 00:09:15.430
brought them to
that moment in time.

00:09:15.430 --> 00:09:18.170
STACIE CHAN: And you're dealing
with some of these people,

00:09:18.170 --> 00:09:21.950
like you say, who are
in control of your fate.

00:09:21.950 --> 00:09:24.230
And you cover these incredibly
dangerous situations,

00:09:24.230 --> 00:09:25.590
all these conflicts.

00:09:25.590 --> 00:09:28.200
What draws you to
places like these?

00:09:28.200 --> 00:09:29.850
Is there something
inherent about war

00:09:29.850 --> 00:09:35.645
conflict that you say, that's
just so inherently interesting?

00:09:35.645 --> 00:09:37.020
RICHARD ENGEL: I
spoke about this

00:09:37.020 --> 00:09:39.367
at the commencement
yesterday, which

00:09:39.367 --> 00:09:40.575
was such an incredible honor.

00:09:40.575 --> 00:09:42.831
I'm still tingling.

00:09:42.831 --> 00:09:44.580
STACIE CHAN: That's
my follow-up question.

00:09:44.580 --> 00:09:47.500
RICHARD ENGEL: I was
amazed and humbled

00:09:47.500 --> 00:09:48.705
to be asked, and shocked.

00:09:48.705 --> 00:09:51.370
I just got an email one day and
said, would you [INAUDIBLE]?

00:09:51.370 --> 00:09:52.060
I was like, are you serious?

00:09:52.060 --> 00:09:52.780
Yes.

00:09:52.780 --> 00:09:55.325
Yes.

00:09:55.325 --> 00:09:57.075
I didn't actually ask
if you were serious.

00:09:57.075 --> 00:09:58.310
I just said, yes.

00:09:58.310 --> 00:10:01.470
And then I asked, I was like,
(WHISPERING) are they serious?

00:10:01.470 --> 00:10:06.670
But as I said yesterday,
the reason I go to war zones

00:10:06.670 --> 00:10:09.055
is not because I like them.

00:10:09.055 --> 00:10:10.760
It's not because I like war.

00:10:10.760 --> 00:10:11.990
I don't like the violence.

00:10:11.990 --> 00:10:15.645
I don't like the spectacle
of groups getting together

00:10:15.645 --> 00:10:19.110
and trying to kill each other.

00:10:19.110 --> 00:10:20.070
I hate it.

00:10:20.070 --> 00:10:23.319
And I hate it more and
more as time goes on

00:10:23.319 --> 00:10:24.610
and the more conflicts I cover.

00:10:24.610 --> 00:10:27.910
But I go there because
I think it's revealing.

00:10:27.910 --> 00:10:32.410
And think of an atom smasher.

00:10:32.410 --> 00:10:36.490
You smash two things together
because when they break apart

00:10:36.490 --> 00:10:39.240
you could understand something
about their components, maybe

00:10:39.240 --> 00:10:42.240
even something universal.

00:10:42.240 --> 00:10:47.920
And I like to think of
war as like a collision.

00:10:47.920 --> 00:10:49.910
You think of a car crash.

00:10:49.910 --> 00:10:53.790
Two cars smash into each
other on an intersection.

00:10:53.790 --> 00:10:55.600
And in that tiny
split second-- it's

00:10:55.600 --> 00:10:58.860
a horrible moment-- you see
everything, the whole range

00:10:58.860 --> 00:11:00.630
of the human experience.

00:11:00.630 --> 00:11:02.670
You see maybe someone is dead.

00:11:02.670 --> 00:11:06.150
Somebody else is alive, injured.

00:11:06.150 --> 00:11:08.820
Is the society still working?

00:11:08.820 --> 00:11:10.330
Is an ambulance coming?

00:11:10.330 --> 00:11:11.360
Maybe it's not coming.

00:11:11.360 --> 00:11:12.890
Who was that person?

00:11:12.890 --> 00:11:14.940
Did he have a fight,
the dead person?

00:11:14.940 --> 00:11:16.150
Was he was a man?

00:11:16.150 --> 00:11:18.860
Did he have a fight with
his wife that morning?

00:11:18.860 --> 00:11:21.320
Everything is
encapsulated right there.

00:11:21.320 --> 00:11:22.680
Is someone rushing in to help?

00:11:22.680 --> 00:11:24.590
A policeman or a bystander?

00:11:24.590 --> 00:11:28.180
Is somebody else pushing,
rushing away to escape?

00:11:28.180 --> 00:11:31.050
In that tiny split second,
you see the whole range

00:11:31.050 --> 00:11:32.690
of the human experience.

00:11:32.690 --> 00:11:36.860
Now imagine a war where you have
two countries or two religions

00:11:36.860 --> 00:11:40.440
or two ethnic groups
smashing into each other.

00:11:40.440 --> 00:11:44.690
It's that car accident magnified
by a thousand or by a million.

00:11:44.690 --> 00:11:45.960
And it's very revealing.

00:11:45.960 --> 00:11:48.070
You understand a lot
about how societies

00:11:48.070 --> 00:11:51.032
work, how they don't work,
what happens when people are

00:11:51.032 --> 00:11:57.540
pressed, the
cowardice, the courage,

00:11:57.540 --> 00:11:59.940
the degree of
sophistication of a society

00:11:59.940 --> 00:12:02.050
or the degree of its barbarity.

00:12:02.050 --> 00:12:03.360
It's all right there.

00:12:03.360 --> 00:12:04.700
So that's why I go to war.

00:12:04.700 --> 00:12:06.330
Because it's revealing.

00:12:06.330 --> 00:12:08.220
But it's revealing
in a tragic way.

00:12:08.220 --> 00:12:09.928
STACIE CHAN: What are
some of the biggest

00:12:09.928 --> 00:12:11.960
revelations you've
seen in covering

00:12:11.960 --> 00:12:13.930
all these different conflicts?

00:12:13.930 --> 00:12:19.360
RICHARD ENGEL: I would say that
it takes courage to be good.

00:12:19.360 --> 00:12:21.780
It takes courage to be kind.

00:12:21.780 --> 00:12:23.390
It's easy to be mean.

00:12:23.390 --> 00:12:26.340
It's easy to be cynical and
to take advantage of people

00:12:26.340 --> 00:12:27.490
when they're down.

00:12:27.490 --> 00:12:35.002
It's easy to exploit through
misogyny or through ageism

00:12:35.002 --> 00:12:36.210
or whatever it happens to be.

00:12:36.210 --> 00:12:40.130
It's easy to exploit those
that a particular culture--

00:12:40.130 --> 00:12:44.550
or racist-- that a particular
culture has marginalized.

00:12:44.550 --> 00:12:48.010
And it takes courage
and confidence

00:12:48.010 --> 00:12:53.000
to do something brave,
to help someone out.

00:12:53.000 --> 00:12:57.770
And when I see it-- and I see
it a lot-- I'm always uplifted.

00:12:57.770 --> 00:13:02.790
But unfortunately, it is those
who take a stand to be good.

00:13:02.790 --> 00:13:08.140
And I, unfortunately,
think that the opposite is

00:13:08.140 --> 00:13:10.087
the path of least resistance.

00:13:10.087 --> 00:13:12.420
So that's something that's
been a little bit depressing.

00:13:12.420 --> 00:13:15.120
But I'm always encouraged when I
see people who are brave enough

00:13:15.120 --> 00:13:21.250
to put themselves at risk
to take in a refugee family

00:13:21.250 --> 00:13:29.180
or to hide people, like in
World War II hiding Jews

00:13:29.180 --> 00:13:30.980
in their homes, or
taking in someone

00:13:30.980 --> 00:13:32.730
from a different ethnic
or religious group

00:13:32.730 --> 00:13:35.060
now in the current conflict.

00:13:35.060 --> 00:13:36.599
So that's always
encouraging to see.

00:13:36.599 --> 00:13:38.515
STACIE CHAN: And in your
role as a journalist,

00:13:38.515 --> 00:13:42.740
do you feel it's your job to
be magnifying those uplifting

00:13:42.740 --> 00:13:46.730
stories or to give the
actual reality of what's

00:13:46.730 --> 00:13:48.329
going on in these places?

00:13:48.329 --> 00:13:49.870
RICHARD ENGEL: Well,
when I see them,

00:13:49.870 --> 00:13:52.610
I'll cover them because
I'm excited by them.

00:13:52.610 --> 00:13:54.950
But like I was saying,
unfortunately, they're

00:13:54.950 --> 00:13:56.650
not the norm.

00:13:56.650 --> 00:13:59.764
Unfortunately, the
norm is usually

00:13:59.764 --> 00:14:01.180
the path of least
resistance where

00:14:01.180 --> 00:14:08.840
people are cruel or
negligent or just selfish.

00:14:08.840 --> 00:14:09.890
That's unfortunate.

00:14:09.890 --> 00:14:12.990
So, yes, when I see them, I
love doing stories like that.

00:14:12.990 --> 00:14:16.740
But I just kind of take
stories as they come.

00:14:16.740 --> 00:14:18.810
When you arrive in
a place-- again,

00:14:18.810 --> 00:14:20.320
to go back to that
car accident--

00:14:20.320 --> 00:14:22.319
and everything is smashed
together and thrown up

00:14:22.319 --> 00:14:27.250
in the air, you just
see what you see.

00:14:27.250 --> 00:14:29.444
You find someone who's
got an amazing story.

00:14:29.444 --> 00:14:31.610
And maybe they're doing
something wonderful-- great.

00:14:31.610 --> 00:14:34.109
Or you find somebody else and
he's doing something horrible.

00:14:34.109 --> 00:14:35.310
OK, you talk about that.

00:14:35.310 --> 00:14:38.670
And you see a situation--
We were Nepal.

00:14:38.670 --> 00:14:43.190
And there was an
orphanage that we went to.

00:14:43.190 --> 00:14:45.510
And the orphanage
didn't collapse,

00:14:45.510 --> 00:14:47.400
but the foundation cracked.

00:14:47.400 --> 00:14:50.150
So everyone had to move outside.

00:14:50.150 --> 00:14:52.440
And they were all living
under a tarp in back,

00:14:52.440 --> 00:14:55.245
about 100 children
from-- I don't

00:14:55.245 --> 00:15:00.210
think they were all, like, 12
and younger, 8 to 12 range.

00:15:00.210 --> 00:15:02.340
And they were living
under this tarp.

00:15:02.340 --> 00:15:04.180
And they were playing.

00:15:04.180 --> 00:15:05.760
And they were making
the best of it.

00:15:05.760 --> 00:15:09.290
But they were orphans who
were now homeless orphans.

00:15:09.290 --> 00:15:12.180
And some of them were living
in a Buddhist monastery

00:15:12.180 --> 00:15:13.330
temporarily next door.

00:15:13.330 --> 00:15:16.700
So was an interesting microcosm
of-- actually, in this case,

00:15:16.700 --> 00:15:19.440
it was a very uplifting story
because the whole community was

00:15:19.440 --> 00:15:22.130
helping this-- The weakest
members of the society

00:15:22.130 --> 00:15:25.300
were now homeless
orphans in Nepal.

00:15:25.300 --> 00:15:27.800
If you can imagine a more
vulnerable-- kind of hard

00:15:27.800 --> 00:15:30.420
to imagine a more vulnerable
community than that.

00:15:30.420 --> 00:15:32.040
STACIE CHAN: And
so I'm going to tie

00:15:32.040 --> 00:15:34.540
what you've been talking about
to the theme of your Stanford

00:15:34.540 --> 00:15:38.050
commencement speech, which was
the intersection of technology

00:15:38.050 --> 00:15:39.160
and geopolitics.

00:15:39.160 --> 00:15:41.640
So can you elaborate a
little bit more about that

00:15:41.640 --> 00:15:44.020
since you're speaking to
a room full of people who

00:15:44.020 --> 00:15:45.849
work for a technology company?

00:15:45.849 --> 00:15:47.390
RICHARD ENGEL: Yeah,
absolutely, This

00:15:47.390 --> 00:15:49.930
is something I've been
thinking about a lot recently.

00:15:49.930 --> 00:15:54.792
And I watched the Arab Spring
movements, the revolutions--

00:15:54.792 --> 00:15:56.500
which I also include
the Green Revolution

00:15:56.500 --> 00:15:58.970
in that kind of spectrum.

00:15:58.970 --> 00:16:03.000
So the Green Revolution
in Iran, Tunisia, Egypt,

00:16:03.000 --> 00:16:06.880
where one revolution
was inspiring the next.

00:16:06.880 --> 00:16:11.310
And if you go back
to 1989, there

00:16:11.310 --> 00:16:17.540
was a series-- an explosion,
really-- of political reform

00:16:17.540 --> 00:16:20.370
when the Iron Curtain
came crumbling down.

00:16:20.370 --> 00:16:26.850
And those, in many ways, were
facilitated by television.

00:16:26.850 --> 00:16:28.610
They were the
television revolutions.

00:16:28.610 --> 00:16:31.397
You probably read
about them in poly-sci.

00:16:31.397 --> 00:16:32.980
Clearly, it wasn't
television that was

00:16:32.980 --> 00:16:34.146
the cause of the revolution.

00:16:34.146 --> 00:16:36.620
The cause of the revolution
was the oppression,

00:16:36.620 --> 00:16:38.877
was the economic suffering.

00:16:38.877 --> 00:16:41.210
People in Eastern Europe who
were living behind the Iron

00:16:41.210 --> 00:16:43.270
Curtain simply
weren't living as well

00:16:43.270 --> 00:16:44.824
as people in Western Europe.

00:16:44.824 --> 00:16:46.240
And then on
television, they could

00:16:46.240 --> 00:16:48.260
see how badly they were living.

00:16:48.260 --> 00:16:50.250
And they could see how
oppressed they were.

00:16:50.250 --> 00:16:53.470
And then they could see
the revolutions start.

00:16:53.470 --> 00:16:55.110
And once you were
in Poland and you

00:16:55.110 --> 00:17:00.135
watched people going to the
streets in Czechoslovakia,

00:17:00.135 --> 00:17:02.730
you said, well, why not us?

00:17:02.730 --> 00:17:04.520
And the revolutions
inspired each other.

00:17:04.520 --> 00:17:07.970
And then you had a
series of revolutions.

00:17:07.970 --> 00:17:11.089
I think the Arab Spring
movements were probably

00:17:11.089 --> 00:17:14.200
the world's first
internet-based revolutions.

00:17:14.200 --> 00:17:16.170
Where the same reason--
it wasn't the internet

00:17:16.170 --> 00:17:17.770
that caused the revolutions.

00:17:17.770 --> 00:17:20.530
The revolutions were because
of mismanagement in Egypt,

00:17:20.530 --> 00:17:22.230
because of police
brutality, because

00:17:22.230 --> 00:17:26.260
of economic wild disparities
between rich and poor,

00:17:26.260 --> 00:17:28.020
the tension that was there.

00:17:28.020 --> 00:17:30.710
But the internet, especially
when it became pocket-sized,

00:17:30.710 --> 00:17:34.080
became cellphone, which is much
more subversive than television

00:17:34.080 --> 00:17:37.720
ever was, it allowed
people to communicate,

00:17:37.720 --> 00:17:41.350
commiserate, plot,
plan, and then

00:17:41.350 --> 00:17:43.900
to experience the
revolution and share it

00:17:43.900 --> 00:17:45.970
and share lessons
learned and pass it on.

00:17:45.970 --> 00:17:49.740
It sort of was a
lubricant for revolutions.

00:17:49.740 --> 00:17:52.142
And it made them faster.

00:17:52.142 --> 00:17:55.240
But the question
that I think one

00:17:55.240 --> 00:18:00.290
needs to consider and
ponder in this experience

00:18:00.290 --> 00:18:03.140
was, is it a good
thing or a bad thing?

00:18:03.140 --> 00:18:07.160
Because all those revolutions
happened very quickly

00:18:07.160 --> 00:18:13.640
and almost in every case led
to chaos and not a good result.

00:18:13.640 --> 00:18:17.440
In Egypt, you've seen a return
of an authoritarian system.

00:18:17.440 --> 00:18:19.860
In Syria, there's
chaos and civil war.

00:18:19.860 --> 00:18:22.770
Libya, there's
civil war and chaos.

00:18:22.770 --> 00:18:25.980
Tunisia, probably the
best example, but it's

00:18:25.980 --> 00:18:29.000
a very small country and not
necessarily representative

00:18:29.000 --> 00:18:30.820
of the wider region.

00:18:30.820 --> 00:18:34.760
So was it too fast?

00:18:34.760 --> 00:18:37.560
Were the societies ready yet?

00:18:37.560 --> 00:18:42.280
Or was this lubricant too
slippery and the revolution

00:18:42.280 --> 00:18:46.480
happened not on its own pace,
but at a digitally enhanced

00:18:46.480 --> 00:18:47.886
pace?

00:18:47.886 --> 00:18:49.760
So I think one of the
most interesting things

00:18:49.760 --> 00:18:52.460
to think about going
forward is this intersection

00:18:52.460 --> 00:18:56.790
between geopolitics
and technology.

00:18:56.790 --> 00:18:59.830
And is it necessarily
a good thing?

00:18:59.830 --> 00:19:02.309
I think, going
forward, it could be.

00:19:02.309 --> 00:19:03.850
But I think, going
forward, what it's

00:19:03.850 --> 00:19:06.370
going to lead to is
much more volatility.

00:19:06.370 --> 00:19:09.690
And if you take a
city like Cairo,

00:19:09.690 --> 00:19:12.140
there's 18 million people
in Cairo right now.

00:19:12.140 --> 00:19:13.805
It's big, hard to manage.

00:19:13.805 --> 00:19:14.900
It's poor.

00:19:14.900 --> 00:19:16.740
Infrastructure's terrible.

00:19:16.740 --> 00:19:18.970
Education is poor.

00:19:18.970 --> 00:19:21.170
It's naturally
explosive because you

00:19:21.170 --> 00:19:23.760
have so many people who are
often unhappy, living right

00:19:23.760 --> 00:19:25.060
on top of each other.

00:19:25.060 --> 00:19:28.300
And now they can all talk
to each other and exchange

00:19:28.300 --> 00:19:31.280
lessons learned and
commiserate and, in exchange,

00:19:31.280 --> 00:19:34.100
good things and bad things.

00:19:34.100 --> 00:19:37.390
So it makes it a little bit
more volatile because of that.

00:19:37.390 --> 00:19:41.760
Now what happens when there's
25 million people in Cairo

00:19:41.760 --> 00:19:45.610
and the air is hard to breathe,
that is, it already is now.

00:19:45.610 --> 00:19:49.610
And communications technology
are even more advanced.

00:19:49.610 --> 00:19:52.020
I think it could be
even more volatile.

00:19:52.020 --> 00:19:57.570
And there will be,
in my postulating

00:19:57.570 --> 00:20:01.380
ahead, I think you'll see
sort of rapid revolutions

00:20:01.380 --> 00:20:03.540
or a tendency toward
rapid revolutions,

00:20:03.540 --> 00:20:05.790
and a countervailing
tendency for strong men

00:20:05.790 --> 00:20:10.090
to come in and use the same
kind of technologies to hunt out

00:20:10.090 --> 00:20:13.540
and hunt down-- I should say,
ferret out and hunt down--

00:20:13.540 --> 00:20:15.100
the revolutionaries.

00:20:15.100 --> 00:20:17.800
So you'll have a tendency
for quick uprisings

00:20:17.800 --> 00:20:22.370
and a tendency for strong men
to come in and squash them.

00:20:22.370 --> 00:20:23.770
That's where I think.

00:20:23.770 --> 00:20:26.230
I could be totally wrong.

00:20:26.230 --> 00:20:29.989
And I think people in
this room and beyond

00:20:29.989 --> 00:20:31.530
will have different
opinions on this.

00:20:31.530 --> 00:20:32.920
And I'd love to hear them.

00:20:32.920 --> 00:20:35.810
But the idea of where
does technology--

00:20:35.810 --> 00:20:39.580
as it becomes part of
the-- more and more

00:20:39.580 --> 00:20:43.340
of a factor in big
geopolitical issues, issues

00:20:43.340 --> 00:20:45.880
of war and peace,
where does it lead us?

00:20:45.880 --> 00:20:47.460
STACIE CHAN: Well, I was going
to follow up on your question--

00:20:47.460 --> 00:20:49.220
RICHARD ENGEL: I think it
leads us in that place,

00:20:49.220 --> 00:20:49.915
but we'll see.

00:20:49.915 --> 00:20:51.840
STACIE CHAN: Is
technology a good thing

00:20:51.840 --> 00:20:55.430
for spurring revolutions,
inciting them

00:20:55.430 --> 00:20:57.910
from a technology perspective?

00:20:57.910 --> 00:20:59.840
RICHARD ENGEL: If you're
a dictatorship, no.

00:20:59.840 --> 00:21:01.895
But maybe it's good.

00:21:01.895 --> 00:21:03.270
And I'm not saying
that-- I'm not

00:21:03.270 --> 00:21:05.340
trying to sound like someone
who says, oh, it's bad.

00:21:05.340 --> 00:21:06.798
You've got to keep
the people down.

00:21:06.798 --> 00:21:07.780
No.

00:21:07.780 --> 00:21:09.110
Absolutely not.

00:21:09.110 --> 00:21:13.980
Maybe it will ultimately lead
to a better place, the exchange

00:21:13.980 --> 00:21:15.940
of ideas, the exchange
of information,

00:21:15.940 --> 00:21:18.180
the exchange of knowledge.

00:21:18.180 --> 00:21:22.190
Maybe this volatility ultimately
leads to a better place.

00:21:22.190 --> 00:21:24.200
Or maybe it doesn't.

00:21:24.200 --> 00:21:27.180
You know people say, oh,
there's a rough road ahead,

00:21:27.180 --> 00:21:29.550
and you have to go
through this rough road

00:21:29.550 --> 00:21:31.260
to get to a better place.

00:21:31.260 --> 00:21:35.680
Well, sometimes the journey
kills the patient, too.

00:21:35.680 --> 00:21:38.282
And it's really,
I think, unclear.

00:21:38.282 --> 00:21:40.240
And that's kind of the
excitement of what I do.

00:21:40.240 --> 00:21:40.781
I don't know.

00:21:40.781 --> 00:21:44.160
I suspect that going
forward-- I know

00:21:44.160 --> 00:21:47.740
Google was involved in Egypt
and you had some Googlers,

00:21:47.740 --> 00:21:49.310
I should say, who were involved.

00:21:49.310 --> 00:21:52.730
And the allowing this
sort of exchange of ideas

00:21:52.730 --> 00:21:55.270
was seen at the time is
an enormously important

00:21:55.270 --> 00:21:57.432
political objective.

00:21:57.432 --> 00:21:58.890
And I can understand
that tendency,

00:21:58.890 --> 00:22:01.620
but look at what
happened in Egypt.

00:22:01.620 --> 00:22:02.160
We'll see.

00:22:02.160 --> 00:22:02.720
We'll see.

00:22:02.720 --> 00:22:03.928
Is it a good thing long term?

00:22:03.928 --> 00:22:05.240
Yes, I think it probably is.

00:22:05.240 --> 00:22:07.750
How could exchange of
ideas not be a good thing?

00:22:07.750 --> 00:22:08.620
Of course, it is.

00:22:08.620 --> 00:22:13.690
But it's going to lead to
a very rough road ahead

00:22:13.690 --> 00:22:15.270
of an extreme period
of volatility.

00:22:15.270 --> 00:22:17.103
And I'm not sure all
the countries out there

00:22:17.103 --> 00:22:20.169
are going to get through
that road safely.

00:22:20.169 --> 00:22:22.460
STACIE CHAN: In addition to
the exchange of information

00:22:22.460 --> 00:22:27.920
and ideas, do you have
specific ideas of how different

00:22:27.920 --> 00:22:30.520
social media platforms,
different technology products

00:22:30.520 --> 00:22:34.817
can help facilitate
these good revolutions?

00:22:34.817 --> 00:22:36.400
RICHARD ENGEL: Well,
it's not up to me

00:22:36.400 --> 00:22:39.060
to say it's a good revolution.

00:22:39.060 --> 00:22:42.660
I think it makes them faster.

00:22:42.660 --> 00:22:48.420
I've described war as a car
crash between two big things--

00:22:48.420 --> 00:22:51.435
nation states, religions,
ethnic groups-- banged together.

00:22:51.435 --> 00:22:52.310
Everything's exposed.

00:22:55.160 --> 00:22:57.725
I think you can think of
geopolitics a little bit

00:22:57.725 --> 00:22:59.210
like the plates of the Earth.

00:22:59.210 --> 00:23:03.780
Tension builds up because
of poor management,

00:23:03.780 --> 00:23:08.190
police brutality, a
closed political system.

00:23:08.190 --> 00:23:11.497
You have these tensions building
up, building up in the system.

00:23:11.497 --> 00:23:13.080
And the more oppressive
the system is,

00:23:13.080 --> 00:23:14.480
the more the tension builds.

00:23:14.480 --> 00:23:16.810
And the longer the system
has been oppressing,

00:23:16.810 --> 00:23:19.840
the higher the tenion
is, like in the Earth.

00:23:19.840 --> 00:23:23.480
And then a revolution
happens, and it snaps.

00:23:23.480 --> 00:23:27.820
And I think technology makes
the snap happen faster.

00:23:27.820 --> 00:23:30.550
So the question is, is
that a good-- Does it snap

00:23:30.550 --> 00:23:33.860
in a way that heels a society?

00:23:33.860 --> 00:23:38.040
Or does the snap cause the whole
thing to come crashing down.

00:23:38.040 --> 00:23:39.182
The snap's going to happen.

00:23:39.182 --> 00:23:41.015
So maybe you shouldn't
blame the technology.

00:23:41.015 --> 00:23:45.640
You should blame the poor
management of the state.

00:23:45.640 --> 00:23:50.690
But sometimes the slip happens
so fast, that it's not ready.

00:23:50.690 --> 00:23:52.609
And the thing comes
crashing down.

00:23:52.609 --> 00:23:55.150
STACIE CHAN: So before I open
it up to questions in the room,

00:23:55.150 --> 00:23:59.110
this was actually a
question before we started.

00:23:59.110 --> 00:24:02.080
Is Richard going to discuss
his time in Syria when

00:24:02.080 --> 00:24:03.932
he and his crew were kidnapped?

00:24:03.932 --> 00:24:05.890
So whatever you feel
comfortable with, Richard.

00:24:05.890 --> 00:24:05.940
RICHARD ENGEL: Sure.

00:24:05.940 --> 00:24:06.240
No I'd like to.

00:24:06.240 --> 00:24:07.470
STACIE CHAN: I know he
has a great, actually,

00:24:07.470 --> 00:24:09.350
first-person editorial
in "Vanity Fair."

00:24:09.350 --> 00:24:11.580
So I'm pretty sure he's
open about talking about it.

00:24:11.580 --> 00:24:11.980
RICHARD ENGEL: Yeah, sure.

00:24:11.980 --> 00:24:13.396
STACIE CHAN: But
for the audience,

00:24:13.396 --> 00:24:15.160
can you share this
experience with us,

00:24:15.160 --> 00:24:18.390
and how you go back to covering
the war after experiencing

00:24:18.390 --> 00:24:19.422
such a traumatic event?

00:24:19.422 --> 00:24:20.755
RICHARD ENGEL: Yeah, absolutely.

00:24:23.300 --> 00:24:26.970
It was complicated because
we originally thought

00:24:26.970 --> 00:24:28.570
it was one group that took us.

00:24:28.570 --> 00:24:31.330
And then we later learned
that it was another group.

00:24:31.330 --> 00:24:34.730
But it doesn't
change the sequence.

00:24:34.730 --> 00:24:38.720
Or it doesn't radically
change the sequence of events.

00:24:38.720 --> 00:24:42.840
So we were driving
along in Syria

00:24:42.840 --> 00:24:44.990
about two and a half years ago.

00:24:44.990 --> 00:24:47.220
And I was with a team.

00:24:47.220 --> 00:24:48.860
Some of them are
my closest friends

00:24:48.860 --> 00:24:50.790
and still are my
closest friends.

00:24:50.790 --> 00:24:55.710
And we get surrounded by
a group of gunmen-- masks,

00:24:55.710 --> 00:24:59.240
black, hooded, ski masks.

00:24:59.240 --> 00:25:00.640
And they were screaming at us.

00:25:00.640 --> 00:25:07.985
And they were all dressed in the
same black unofficial uniforms.

00:25:07.985 --> 00:25:13.060
And they take us
out of our cars,

00:25:13.060 --> 00:25:17.150
and they load us into a
container truck that's parked

00:25:17.150 --> 00:25:19.052
backwards under the trees.

00:25:19.052 --> 00:25:21.010
And I see this container
truck under the trees.

00:25:21.010 --> 00:25:23.880
And I thought, oh, no.

00:25:23.880 --> 00:25:25.300
We're going in there.

00:25:25.300 --> 00:25:28.059
And they'd clearly done
this before they load us

00:25:28.059 --> 00:25:29.100
into the container truck.

00:25:29.100 --> 00:25:30.150
They slam the door shut.

00:25:30.150 --> 00:25:34.910
They have flashlights that
they turn on, duct tape us up.

00:25:34.910 --> 00:25:36.350
Duct tape over my
mouth and nose,

00:25:36.350 --> 00:25:38.720
I thought I was going to
suffocate right there.

00:25:38.720 --> 00:25:46.250
And they hold us for
the next five days.

00:25:46.250 --> 00:25:49.270
And we thought we were
going to die sometimes

00:25:49.270 --> 00:25:51.040
right then and there.

00:25:51.040 --> 00:25:52.610
They told us we
were going to die.

00:25:52.610 --> 00:25:58.660
And then after the
end of the five days,

00:25:58.660 --> 00:26:02.240
on the morning of the
sixth, we got out.

00:26:02.240 --> 00:26:05.830
And we went back into Turkey.

00:26:05.830 --> 00:26:09.550
And a rebel group escorted
us back across the border

00:26:09.550 --> 00:26:11.400
into Turkey.

00:26:11.400 --> 00:26:13.530
And we're still reporting.

00:26:13.530 --> 00:26:16.190
And we're still
very close friends.

00:26:16.190 --> 00:26:21.960
And we have had--
it's a wake-up call.

00:26:21.960 --> 00:26:24.380
I think it's almost
like surviving

00:26:24.380 --> 00:26:25.880
some sort of terminal illness.

00:26:25.880 --> 00:26:28.560
The flowers smell
better the next day.

00:26:28.560 --> 00:26:30.610
And the food tastes
better the next day.

00:26:30.610 --> 00:26:32.385
And you're like, wow, that one
really could have gone badly.

00:26:32.385 --> 00:26:33.926
I mean, it did go
badly, but it could

00:26:33.926 --> 00:26:36.110
have gone much, much worse.

00:26:36.110 --> 00:26:38.820
And unfortunately,
the people who

00:26:38.820 --> 00:26:44.040
were kidnapped around the same
time or a little bit later,

00:26:44.040 --> 00:26:46.270
we all saw what
happened to them.

00:26:46.270 --> 00:26:48.800
You know, James Foley
other journalists

00:26:48.800 --> 00:26:52.680
who were taken by
ISIS and were really

00:26:52.680 --> 00:26:55.230
brutalized for several
years before they

00:26:55.230 --> 00:26:57.140
were savagely butchered.

00:26:57.140 --> 00:26:59.760
So I consider myself
incredibly lucky,

00:26:59.760 --> 00:27:01.760
and everyone on my team
to be incredibly lucky.

00:27:01.760 --> 00:27:04.080
And, yeah, the
food tastes better.

00:27:04.080 --> 00:27:05.289
And the flowers smell better.

00:27:05.289 --> 00:27:06.080
STACIE CHAN: I bet.

00:27:06.080 --> 00:27:08.705
Well, we're all so grateful that
you made it out alive as well.

00:27:08.705 --> 00:27:10.730
I have to ask one more
journalistic question.

00:27:10.730 --> 00:27:13.240
You mentioned that the
kidnappers were not

00:27:13.240 --> 00:27:16.010
the group, the pro-government
Shiite group that you thought.

00:27:16.010 --> 00:27:18.010
RICHARD ENGEL: That's
what they were telling us.

00:27:18.010 --> 00:27:19.200
And that's what we thought.

00:27:19.200 --> 00:27:21.607
So what was the--

00:27:21.607 --> 00:27:23.440
STACIE CHAN: Well, the
journalistic question

00:27:23.440 --> 00:27:27.720
was "The New York Times" was
planning to do this big expose

00:27:27.720 --> 00:27:30.595
and expose who the true
identity of the kidnappers were.

00:27:30.595 --> 00:27:32.210
They were planning
this big scoop.

00:27:32.210 --> 00:27:36.100
And then Richard's team and
the investigation team at NBC

00:27:36.100 --> 00:27:38.490
actually scooped "The
New York Times"'s scoop.

00:27:38.490 --> 00:27:42.670
So how important was
it for you to get

00:27:42.670 --> 00:27:47.200
the story correct and out there
before any other organization?

00:27:47.200 --> 00:27:50.190
RICHARD ENGEL: We
thought-- I thought that we

00:27:50.190 --> 00:27:51.880
knew who this group was.

00:27:51.880 --> 00:27:57.320
We were captured, we thought,
by pro-government militia men.

00:27:57.320 --> 00:27:58.950
That's how they were dressing.

00:27:58.950 --> 00:28:01.000
That's how they were speaking.

00:28:01.000 --> 00:28:02.480
That's what they
were telling us.

00:28:02.480 --> 00:28:06.184
It seemed very credible to
me and the other people who

00:28:06.184 --> 00:28:06.850
were on my team.

00:28:06.850 --> 00:28:08.120
I speak Arabic.

00:28:08.120 --> 00:28:10.090
And there were three
Arabic speakers there.

00:28:10.090 --> 00:28:11.800
We were all of the
same impression

00:28:11.800 --> 00:28:13.970
that that's who they were.

00:28:13.970 --> 00:28:16.400
They did an incredible
job convincing us

00:28:16.400 --> 00:28:19.750
that they were
government militia men.

00:28:19.750 --> 00:28:20.460
So we got out.

00:28:20.460 --> 00:28:21.640
And that's what we thought.

00:28:21.640 --> 00:28:22.530
There were five people there.

00:28:22.530 --> 00:28:23.780
We all thought the same thing.

00:28:23.780 --> 00:28:25.310
We said, good.

00:28:25.310 --> 00:28:28.040
Not good, but that's
what we thought.

00:28:28.040 --> 00:28:29.210
That's what we said.

00:28:29.210 --> 00:28:32.440
And then about just
a few months ago,

00:28:32.440 --> 00:28:35.750
we got this information
from "The New York Times"

00:28:35.750 --> 00:28:41.954
that the people who we thought
had taken us were actually

00:28:41.954 --> 00:28:42.870
hiding their identity.

00:28:42.870 --> 00:28:44.328
It may have been
a different group.

00:28:44.328 --> 00:28:46.350
So of course, we
started looking into it.

00:28:46.350 --> 00:28:48.850
I have the most
interest out of anyone

00:28:48.850 --> 00:28:52.190
in the world to find out
who these people are.

00:28:52.190 --> 00:28:53.400
So we started reexamining it.

00:28:53.400 --> 00:28:54.910
And we did what
journalists always do.

00:28:54.910 --> 00:28:55.826
You look at the facts.

00:28:55.826 --> 00:29:00.470
You look it over the best way
you could put it together.

00:29:00.470 --> 00:29:05.270
And it's much harder to figure
out 2 and 1/2 years later

00:29:05.270 --> 00:29:07.201
what had happened because--

00:29:07.201 --> 00:29:08.700
STACIE CHAN: This
is December, 2012.

00:29:08.700 --> 00:29:10.324
RICHARD ENGEL: Yeah,
so looking into it

00:29:10.324 --> 00:29:12.340
2 and 1/2 years later
is obviously much harder

00:29:12.340 --> 00:29:14.839
because some of the
people are dead.

00:29:14.839 --> 00:29:16.380
Some of the other
people are missing.

00:29:19.550 --> 00:29:21.030
People's-- you
have to figure out,

00:29:21.030 --> 00:29:22.660
have they changed
their story much?

00:29:22.660 --> 00:29:23.440
Who are they?

00:29:23.440 --> 00:29:25.260
What are they competing agendas?

00:29:25.260 --> 00:29:28.900
So it took us a long
time to put it together.

00:29:28.900 --> 00:29:33.645
And after re-researching it,
we updated the information

00:29:33.645 --> 00:29:36.480
and said, ah, we actually
learned some new information,

00:29:36.480 --> 00:29:39.170
that the story was even more
complicated than we thought.

00:29:39.170 --> 00:29:44.330
And it reveals how in Syria
these days, it's a jungle.

00:29:44.330 --> 00:29:46.340
You don't necessarily
know who's who,

00:29:46.340 --> 00:29:52.150
and what the competing agendas
are, and who's on whose side,

00:29:52.150 --> 00:29:59.140
and how this conflict has really
deteriorated into something

00:29:59.140 --> 00:30:02.724
where it's very hard to
tell who the good guys are

00:30:02.724 --> 00:30:03.765
and who the bad guys are.

00:30:03.765 --> 00:30:05.310
STACIE CHAN: Well, I think
that was an incredible win

00:30:05.310 --> 00:30:06.824
for journalistic integrity.

00:30:06.824 --> 00:30:08.990
So we're glad that you and
your team did that story.

00:30:08.990 --> 00:30:10.740
RICHARD ENGEL: Look,
we would do it again.

00:30:10.740 --> 00:30:12.380
If somebody comes to
me and says, look,

00:30:12.380 --> 00:30:19.330
we have information
that pertains to you,

00:30:19.330 --> 00:30:22.656
I'm the most interested party
in trying to figure it out.

00:30:22.656 --> 00:30:23.940
STACIE CHAN: Definitely.

00:30:23.940 --> 00:30:26.530
I know we had a couple
questions from the audience.

00:30:26.530 --> 00:30:28.100
Is the mic?

00:30:28.100 --> 00:30:31.450
AUDIENCE: You somehow got
away from these captors.

00:30:31.450 --> 00:30:32.440
Did they release you?

00:30:32.440 --> 00:30:34.200
Did you negotiate?

00:30:34.200 --> 00:30:36.650
Or was it some type
of they just realized

00:30:36.650 --> 00:30:38.990
that there wasn't, I
guess, something positive

00:30:38.990 --> 00:30:41.900
they could do out
of the situation?

00:30:41.900 --> 00:30:45.260
RICHARD ENGEL: Again, that
was complicated initially.

00:30:48.590 --> 00:30:51.450
I think in the end--
let me put it this way.

00:30:51.450 --> 00:30:55.760
I think in the end, we became
too complicated for them.

00:30:55.760 --> 00:31:00.650
I think in the end,
they wanted a ransom.

00:31:00.650 --> 00:31:04.620
But it was not going well.

00:31:04.620 --> 00:31:08.490
It wasn't going as
well as they'd planned.

00:31:08.490 --> 00:31:12.930
So I think they realized
it wasn't worth it anymore.

00:31:12.930 --> 00:31:15.950
They could have either
killed us or let us go.

00:31:15.950 --> 00:31:19.550
And instead, I think
we were-- they arranged

00:31:19.550 --> 00:31:24.600
for us to get out in a way
that they could walk away

00:31:24.600 --> 00:31:29.550
from it without getting
themselves caught,

00:31:29.550 --> 00:31:33.710
and to back off
from the situation.

00:31:33.710 --> 00:31:39.630
So it was for them--
for us, it was

00:31:39.630 --> 00:31:41.539
a kidnapping that went badly.

00:31:41.539 --> 00:31:42.955
And I think for
the kidnappers, it

00:31:42.955 --> 00:31:44.580
was a kidnapping that
didn't go exactly

00:31:44.580 --> 00:31:45.770
as they planned, either.

00:31:45.770 --> 00:31:47.145
AUDIENCE: For a
couple years now,

00:31:47.145 --> 00:31:49.620
I've been trying to
wrap my head around ISIS

00:31:49.620 --> 00:31:50.630
and how it came to be.

00:31:50.630 --> 00:31:52.546
Because it seemed like
it was such a surprise.

00:31:52.546 --> 00:31:54.450
Obviously, Obama made
the famous JV comment.

00:31:54.450 --> 00:31:58.380
So I was wondering, you knowing
Syria and Iraq as well as you

00:31:58.380 --> 00:32:01.860
do, was it a surprise to you?

00:32:01.860 --> 00:32:02.700
Did you expect it?

00:32:02.700 --> 00:32:04.449
Or was it just-- were
you caught off guard

00:32:04.449 --> 00:32:06.570
like everybody else
seemed to have been?

00:32:06.570 --> 00:32:08.270
RICHARD ENGEL: We
were reporting on it,

00:32:08.270 --> 00:32:12.490
expanding, pretty consistently.

00:32:12.490 --> 00:32:14.890
Because it was al-Qaeda in Iraq.

00:32:14.890 --> 00:32:17.520
ISIS is al-Qaeda in Iraq.

00:32:17.520 --> 00:32:21.330
It is the same group
that was the Zarqai.

00:32:21.330 --> 00:32:22.900
And if you listen
to ISIS statements,

00:32:22.900 --> 00:32:24.483
they talk about their
founding members

00:32:24.483 --> 00:32:28.422
being the insurgency in Iraq
that was fighting in Ramadi,

00:32:28.422 --> 00:32:30.630
Fallujah, and Anbar province,
where they, by the way,

00:32:30.630 --> 00:32:32.170
are still fighting.

00:32:32.170 --> 00:32:35.220
So this happened
quite gradually.

00:32:35.220 --> 00:32:40.390
US forces go into Iraq in 2003.

00:32:40.390 --> 00:32:42.130
That's really the start of this.

00:32:42.130 --> 00:32:44.700
It's not Syria war.

00:32:44.700 --> 00:32:47.500
It's going into Iraq in 2003.

00:32:47.500 --> 00:32:52.350
The Sunni movement there
is-- The Sunni population,

00:32:52.350 --> 00:32:56.250
some of which were Saddam
Hussein regime loyalists,

00:32:56.250 --> 00:32:57.840
are cornered.

00:32:57.840 --> 00:32:59.200
They see they have no future.

00:32:59.200 --> 00:33:00.250
They're antagonized.

00:33:00.250 --> 00:33:01.170
They're angry.

00:33:01.170 --> 00:33:02.210
They start to organize.

00:33:02.210 --> 00:33:03.590
They become an insurgency.

00:33:03.590 --> 00:33:07.180
That insurgency becomes
more and more radical,

00:33:07.180 --> 00:33:11.080
becomes more of an
al-Qaeda insurgency.

00:33:11.080 --> 00:33:14.770
Then the US troop
surge comes in,

00:33:14.770 --> 00:33:20.360
an enormous force with a
practically limitless budget.

00:33:20.360 --> 00:33:23.500
More weaponry than you
could really imagine

00:33:23.500 --> 00:33:28.130
is poured on to tamp
down this insurgency.

00:33:28.130 --> 00:33:29.360
And it didn't crush it.

00:33:29.360 --> 00:33:30.170
It didn't kill it.

00:33:30.170 --> 00:33:36.990
But it tamps it down to a point
where it's almost meaningless.

00:33:36.990 --> 00:33:39.350
It's quieted down.

00:33:39.350 --> 00:33:42.380
Then the US troops leave,
gives that insurgency

00:33:42.380 --> 00:33:45.300
a little bit more breathing
room, but not much.

00:33:45.300 --> 00:33:48.760
Then Syria, just across the
border, totally collapses.

00:33:48.760 --> 00:33:52.940
So that seed of insurgency,
which was always there,

00:33:52.940 --> 00:33:56.130
goes over and finds incredibly
fertile ground in Syria

00:33:56.130 --> 00:33:57.892
and blossoms into ISIS.

00:33:57.892 --> 00:33:58.850
And then what happened?

00:33:58.850 --> 00:34:00.183
What is the first thing they do?

00:34:00.183 --> 00:34:03.102
They move back across
the border to Iraq,

00:34:03.102 --> 00:34:05.310
to where that which was
their homeland to begin with.

00:34:05.310 --> 00:34:09.604
And now they have
one group or one ISIS

00:34:09.604 --> 00:34:12.440
caliphate, as they call it, that
spans the border between Iraq

00:34:12.440 --> 00:34:13.639
and Syria.

00:34:13.639 --> 00:34:16.329
So I think when you talk
about, could we see it coming?

00:34:16.329 --> 00:34:17.620
Yeah, we could see this coming.

00:34:17.620 --> 00:34:20.330
We could see it coming
right from 2003.

00:34:20.330 --> 00:34:22.159
And there's a line
that goes right back

00:34:22.159 --> 00:34:25.141
to the insurgency in Iraq.

00:34:25.141 --> 00:34:28.139
It's not a strictly
Syria phenomenon.

00:34:28.139 --> 00:34:30.730
STACIE CHAN: So you're
dealing with extremely

00:34:30.730 --> 00:34:35.179
dangerous, unpredictable
ISIS insurgents.

00:34:35.179 --> 00:34:36.810
How do you go about
endearing yourself

00:34:36.810 --> 00:34:39.179
to these people/groups
that are hostile

00:34:39.179 --> 00:34:42.050
and often brutal towards
people other than their own,

00:34:42.050 --> 00:34:43.790
including other
Arabs and Americans,

00:34:43.790 --> 00:34:46.530
and convincing them that they
should trust you to write

00:34:46.530 --> 00:34:48.280
their story responsibly?

00:34:48.280 --> 00:34:51.159
RICHARD ENGEL: I don't
think you can anymore.

00:34:51.159 --> 00:34:53.580
I don't think I
could go to ISIS.

00:34:53.580 --> 00:34:54.900
I don't try.

00:34:54.900 --> 00:34:57.730
And I think those
days that-- You

00:34:57.730 --> 00:35:01.080
have to know your limitations.

00:35:01.080 --> 00:35:02.810
Certain groups will
not be convinced

00:35:02.810 --> 00:35:05.810
and don't want you to be
there to tell their story,

00:35:05.810 --> 00:35:07.430
and don't think they need to.

00:35:07.430 --> 00:35:10.000
They don't think they need us.

00:35:10.000 --> 00:35:13.920
So it's not just an
example of being charming.

00:35:13.920 --> 00:35:15.545
You can't be charming
enough with ISIS.

00:35:15.545 --> 00:35:18.140
It's not going to work.

00:35:18.140 --> 00:35:19.440
It used to be.

00:35:19.440 --> 00:35:21.110
And I watched this change.

00:35:21.110 --> 00:35:22.840
And it has-- since
this theme, it

00:35:22.840 --> 00:35:25.357
seems appropriate talk
about, media and technology,

00:35:25.357 --> 00:35:28.310
and geopolitics and
technology-- which

00:35:28.310 --> 00:35:31.000
is what I sort of thought--
It was what I've been thinking

00:35:31.000 --> 00:35:38.020
about in coming here--
was in 2006, again

00:35:38.020 --> 00:35:40.990
the Iraq war is raging.

00:35:40.990 --> 00:35:46.460
And that's when things like
YouTube and internet videos

00:35:46.460 --> 00:35:50.640
really start becoming
easy and popular.

00:35:50.640 --> 00:35:52.140
It was really simple
for some people

00:35:52.140 --> 00:35:55.890
to start making their own videos
and uploading them to YouTube.

00:35:55.890 --> 00:36:01.250
And it coincided just
with the rising peak

00:36:01.250 --> 00:36:03.940
of the Iraq insurgency.

00:36:03.940 --> 00:36:07.730
So previously, before
that, in the late '90s,

00:36:07.730 --> 00:36:09.520
when I was reporting,
it was much more

00:36:09.520 --> 00:36:11.410
like along this example.

00:36:11.410 --> 00:36:12.790
You would go to a group.

00:36:12.790 --> 00:36:13.840
You would meet.

00:36:13.840 --> 00:36:15.390
You would chit chat.

00:36:15.390 --> 00:36:16.960
They didn't like me.

00:36:16.960 --> 00:36:18.580
I did necessarily like them.

00:36:18.580 --> 00:36:21.340
But we had to
respect each other.

00:36:21.340 --> 00:36:24.355
There was a modus operandi.

00:36:24.355 --> 00:36:26.180
You couldn't be hostile
to me and beat me

00:36:26.180 --> 00:36:30.110
up or worse because then I would
say nasty things about you.

00:36:30.110 --> 00:36:31.990
And you didn't want that.

00:36:31.990 --> 00:36:33.520
And I wanted access to you.

00:36:33.520 --> 00:36:34.560
So we would talk.

00:36:34.560 --> 00:36:37.260
And we would come to an
agreement for my safety,

00:36:37.260 --> 00:36:39.290
and I would hear your story.

00:36:39.290 --> 00:36:41.640
And then I would leave and
I would write the story.

00:36:41.640 --> 00:36:44.180
And inevitably, you're not
going to happy with it.

00:36:44.180 --> 00:36:45.580
You're the insurgent, by
the way, in this example.

00:36:45.580 --> 00:36:45.840
STACIE CHAN: Oh, OK.

00:36:45.840 --> 00:36:47.155
RICHARD ENGEL: You're
the terrorist group.

00:36:47.155 --> 00:36:48.300
STACIE CHAN: I could put
on a ski mask or something.

00:36:48.300 --> 00:36:48.400
RICHARD ENGEL: Yes.

00:36:48.400 --> 00:36:50.430
Inevitably, you're not
going to be happy with it

00:36:50.430 --> 00:36:53.530
because we didn't
tell your full story.

00:36:53.530 --> 00:36:55.482
Or we took you out of
context in your opinion.

00:36:55.482 --> 00:36:57.190
Inevitably, you're
not going to be happy.

00:36:57.190 --> 00:37:00.560
But you figure, OK, at least
I got the message out there.

00:37:00.560 --> 00:37:04.020
Jump ahead to 2006,
these groups decide,

00:37:04.020 --> 00:37:06.720
we don't need the journalists.

00:37:06.720 --> 00:37:09.600
We'll just go direct
to our audience.

00:37:09.600 --> 00:37:11.460
We'll cut them out entirely.

00:37:11.460 --> 00:37:15.680
Well just post our
message on YouTube.

00:37:15.680 --> 00:37:20.570
And we can say exactly what
we want to say for as long

00:37:20.570 --> 00:37:22.560
as we want to say it.

00:37:22.560 --> 00:37:23.390
And you know what?

00:37:23.390 --> 00:37:24.300
Journalists?

00:37:24.300 --> 00:37:28.260
It's better to kill them
and put them in the video

00:37:28.260 --> 00:37:30.100
than it is, necessarily,
to have to filter

00:37:30.100 --> 00:37:32.020
the message through them.

00:37:32.020 --> 00:37:37.500
So I think technology
profoundly changed the way

00:37:37.500 --> 00:37:40.381
that relationship happens.

00:37:40.381 --> 00:37:42.880
STACIE CHAN: And earlier, you
talked about exchange of info.

00:37:42.880 --> 00:37:45.590
RICHARD ENGEL: It's not like
you sit down with ISIS and say,

00:37:45.590 --> 00:37:49.540
so let's, you know-- Nah, you're
not going to win that one.

00:37:49.540 --> 00:37:51.040
STACIE CHAN: And
that's interesting.

00:37:51.040 --> 00:37:53.300
You talked about technology
being a force for good

00:37:53.300 --> 00:37:55.110
and speeding up some
of these revolutions.

00:37:55.110 --> 00:37:57.010
But it sounds like you
just gave an example

00:37:57.010 --> 00:37:58.680
where it could be
very deadly when

00:37:58.680 --> 00:38:02.097
you have the bad guys having
direct access to the people

00:38:02.097 --> 00:38:03.180
they're trying to recruit.

00:38:03.180 --> 00:38:03.680
RICHARD ENGEL: Look
at the internet.

00:38:03.680 --> 00:38:05.937
STACIE CHAN: People trying
to get to their side.

00:38:05.937 --> 00:38:08.020
RICHARD ENGEL: Some of
these videos are appalling.

00:38:08.020 --> 00:38:08.880
STACIE CHAN: Right.

00:38:08.880 --> 00:38:09.560
RICHARD ENGEL: Appalling.

00:38:09.560 --> 00:38:10.060
Brutalized.

00:38:10.060 --> 00:38:12.490
It brutalizes the spectator.

00:38:12.490 --> 00:38:14.800
You look on these
videos-- which I

00:38:14.800 --> 00:38:17.550
look at a lot--
they're absolutely

00:38:17.550 --> 00:38:19.670
pornographic in their violence.

00:38:19.670 --> 00:38:22.860
And yeah, that's not a
good side of technology.

00:38:22.860 --> 00:38:24.760
You don't want the
internet to just become

00:38:24.760 --> 00:38:29.650
some highway for
bandits and criminals

00:38:29.650 --> 00:38:32.300
to use to speed up and
down their messages

00:38:32.300 --> 00:38:37.430
into our bedrooms and into
our houses, into our minds.

00:38:37.430 --> 00:38:39.281
No, that's not wonderful.

00:38:39.281 --> 00:38:40.780
STACIE CHAN: And
with all technology

00:38:40.780 --> 00:38:42.310
comes the good and the bad.

00:38:42.310 --> 00:38:42.650
RICHARD ENGEL: Absolutely.

00:38:42.650 --> 00:38:44.270
STACIE CHAN: So we'll
see how that plays out.

00:38:44.270 --> 00:38:45.079
Another question?

00:38:45.079 --> 00:38:45.620
AUDIENCE: Hi.

00:38:45.620 --> 00:38:49.320
Yeah, I was wondering what you
think news organizations can

00:38:49.320 --> 00:38:53.130
do to better maintain the
public's attention to wars

00:38:53.130 --> 00:38:55.700
and natural disasters.

00:38:55.700 --> 00:38:57.560
RICHARD ENGEL: How to
keep the story going.

00:38:57.560 --> 00:39:00.820
Why aren't we
covering Nepal now?

00:39:00.820 --> 00:39:04.230
And why does it move on?

00:39:04.230 --> 00:39:06.677
I wish, to be
honest, that we were.

00:39:06.677 --> 00:39:07.510
And we are actually.

00:39:07.510 --> 00:39:09.426
We have a documentary
coming up in a few days.

00:39:09.426 --> 00:39:11.810
But I do know your point.

00:39:11.810 --> 00:39:13.587
Generally there's a new cycle.

00:39:13.587 --> 00:39:14.920
And it gets shorter and shorter.

00:39:14.920 --> 00:39:16.160
Sometimes it's 24 hours.

00:39:16.160 --> 00:39:17.490
Sometimes it's a week.

00:39:17.490 --> 00:39:19.990
And there seems like there's
the movie of the week.

00:39:19.990 --> 00:39:21.240
And then everyone moves on.

00:39:24.240 --> 00:39:26.200
I don't know how.

00:39:26.200 --> 00:39:28.030
I'm a provider of news.

00:39:28.030 --> 00:39:29.600
I don't decide programming.

00:39:29.600 --> 00:39:30.300
I gather.

00:39:30.300 --> 00:39:32.620
I'm still in the
hunter-gatherer mode.

00:39:32.620 --> 00:39:36.250
But yes, I think
it's really important

00:39:36.250 --> 00:39:37.580
to stay on top of stories.

00:39:37.580 --> 00:39:41.610
But how to convince news
organizations, my bosses,

00:39:41.610 --> 00:39:44.320
other people's bosses
to stick on this story

00:39:44.320 --> 00:39:47.190
when there's another
story happening that week?

00:39:47.190 --> 00:39:50.470
I think it's a challenge.

00:39:50.470 --> 00:39:54.280
Maybe you have better
ideas for that.

00:39:54.280 --> 00:39:55.780
I still gather the stories.

00:39:55.780 --> 00:40:02.510
But how to break out of this
cyclical story of the week

00:40:02.510 --> 00:40:04.350
phenomenon that
we're in right now?

00:40:04.350 --> 00:40:07.370
Sometimes story of the
hour, story of the day?

00:40:07.370 --> 00:40:09.580
I think, actually,
technology sometimes really

00:40:09.580 --> 00:40:12.440
is working against
us on that one.

00:40:12.440 --> 00:40:15.480
Because people go for
the trending topics.

00:40:15.480 --> 00:40:17.360
And oh, this thing is trending.

00:40:17.360 --> 00:40:20.970
And they follow what's trending
on YouTube or on Twitter

00:40:20.970 --> 00:40:22.890
or whatever for a minute.

00:40:22.890 --> 00:40:26.890
And media organizations
are chasing that.

00:40:26.890 --> 00:40:33.740
So I think, if anything, this--
you're not helping us in this.

00:40:33.740 --> 00:40:36.030
You're making it harder to
get where you want to go

00:40:36.030 --> 00:40:38.455
and where, I think,
I'd like to go as well.

00:40:38.455 --> 00:40:40.880
STACIE CHAN: Well, could you
argue that the media could

00:40:40.880 --> 00:40:43.670
be the trendsetters, and
it sounds like it actually

00:40:43.670 --> 00:40:46.210
might be a battle within
the newsroom to convince

00:40:46.210 --> 00:40:47.969
the programming people--

00:40:47.969 --> 00:40:48.760
RICHARD ENGEL: Yes.

00:40:48.760 --> 00:40:51.580
STACIE CHAN: --that, no, we want
to do a follow-up Nepal story.

00:40:51.580 --> 00:40:51.780
This is going to--

00:40:51.780 --> 00:40:53.779
RICHARD ENGEL: I fight
that battle all the time.

00:40:53.779 --> 00:40:55.260
I fight that battle
all the time.

00:40:55.260 --> 00:40:55.926
Sometimes I win.

00:40:55.926 --> 00:40:56.780
Sometimes I lose.

00:40:56.780 --> 00:40:59.134
But yes, there are
battles in the newsroom.

00:40:59.134 --> 00:41:01.450
BEN PLESSER: [INAUDIBLE].

00:41:01.450 --> 00:41:04.610
RICHARD ENGEL: This
is Ben Plesser.

00:41:04.610 --> 00:41:07.751
Ben Plesser is a
fabulous producer.

00:41:07.751 --> 00:41:09.750
He and I have worked
together for some time now.

00:41:09.750 --> 00:41:13.300
We go all around the world.

00:41:13.300 --> 00:41:14.850
This is a very
serious individual.

00:41:14.850 --> 00:41:17.749
BEN PLESSER: I like it on this
side of the camera, generally--

00:41:17.749 --> 00:41:18.290
or that side.

00:41:18.290 --> 00:41:22.760
But just a shameless plug to
answer the three last questions

00:41:22.760 --> 00:41:26.860
in another way-- we did
an hour-long documentary

00:41:26.860 --> 00:41:29.145
on ISIS, which aired on MSNBC.

00:41:29.145 --> 00:41:35.560
We would never manage to get
that on our larger platforms.

00:41:35.560 --> 00:41:39.405
But A, that's our answer
to where ISIS came from.

00:41:39.405 --> 00:41:40.780
There's a whole
big section there

00:41:40.780 --> 00:41:44.090
where Richard walks through
that in a much longer way

00:41:44.090 --> 00:41:45.700
than he was able to do here.

00:41:45.700 --> 00:41:48.310
Two, that's our answer
on some level, too,

00:41:48.310 --> 00:41:52.010
is to find other platforms.

00:41:52.010 --> 00:41:54.550
This is the last group
we need to tell--

00:41:54.550 --> 00:41:56.390
different platforms
have different audiences

00:41:56.390 --> 00:41:58.140
with different
kind of tolerances.

00:41:58.140 --> 00:42:01.420
And we try to find
them as best we can.

00:42:01.420 --> 00:42:03.820
And three, I think
Richard could discuss

00:42:03.820 --> 00:42:05.820
in the intersection
between YouTube

00:42:05.820 --> 00:42:08.280
and what we do, the
beginning of that hour

00:42:08.280 --> 00:42:14.330
with the challenge on the
press not coming to Kobani.

00:42:14.330 --> 00:42:21.000
RICHARD ENGEL: So as Ben was
saying, maybe a way that you

00:42:21.000 --> 00:42:23.390
can do it is-- and the way
in which we try and do--

00:42:23.390 --> 00:42:24.980
is you find different platforms.

00:42:24.980 --> 00:42:29.360
So if NBC News, which is a
big platform, won't run it,

00:42:29.360 --> 00:42:33.580
MSNBC will give us an
hour and put it on air

00:42:33.580 --> 00:42:34.580
and put it together.

00:42:34.580 --> 00:42:39.410
But there was a challenge.

00:42:39.410 --> 00:42:45.220
There's a lot of
user-driven video out there.

00:42:45.220 --> 00:42:47.260
And there's a tendency,
I think, among people

00:42:47.260 --> 00:42:49.830
who troll the internet
to believe it.

00:42:49.830 --> 00:42:51.220
I don't know why.

00:42:51.220 --> 00:42:53.190
But there is a huge tendency.

00:42:53.190 --> 00:42:55.560
The more obscure
the source, it seems

00:42:55.560 --> 00:42:58.140
to be that the people on the
street think the more credible

00:42:58.140 --> 00:42:58.640
it is.

00:42:58.640 --> 00:43:00.620
I don't know why, but they do.

00:43:00.620 --> 00:43:02.447
Oh, that's a source
I've never heard of,

00:43:02.447 --> 00:43:04.280
so therefore they must
be telling the truth.

00:43:04.280 --> 00:43:05.930
Not the big media
organizations who are--

00:43:05.930 --> 00:43:07.680
STACIE CHAN: Not the
legacy organizations.

00:43:07.680 --> 00:43:08.700
RICHARD ENGEL: Yes.

00:43:08.700 --> 00:43:11.370
But there was a
town-- there still

00:43:11.370 --> 00:43:13.550
is a town in Syria--
called Kobani.

00:43:13.550 --> 00:43:15.310
You may have heard of it.

00:43:15.310 --> 00:43:16.830
It's right on the
Turkish border.

00:43:16.830 --> 00:43:19.000
It's a Kurdish town.

00:43:19.000 --> 00:43:24.200
And ISIS, through its own media
channels, was putting out,

00:43:24.200 --> 00:43:25.700
we've controlled Kobani.

00:43:25.700 --> 00:43:26.760
We've taken it over.

00:43:26.760 --> 00:43:28.820
We control 90% of the town.

00:43:28.820 --> 00:43:30.200
We're victorious.

00:43:30.200 --> 00:43:31.390
They were raising flags.

00:43:31.390 --> 00:43:34.820
They were releasing videos
of flags being released.

00:43:34.820 --> 00:43:39.150
And we said, OK, maybe.

00:43:39.150 --> 00:43:41.240
So we went in.

00:43:41.240 --> 00:43:44.190
And we went in and we
found it was not the case.

00:43:44.190 --> 00:43:49.840
And it was a small group of
rebels holding out, fighting.

00:43:49.840 --> 00:43:53.599
And they eventually got
some more American support.

00:43:53.599 --> 00:43:54.640
And they turned the tide.

00:43:54.640 --> 00:43:56.780
And this sort of
nucleus of rebels

00:43:56.780 --> 00:44:00.580
that was in the town
with incredible stamina

00:44:00.580 --> 00:44:03.950
and determination held out--
then with later American air

00:44:03.950 --> 00:44:07.530
support-- and turned the tides
and pushed all these people

00:44:07.530 --> 00:44:08.290
out.

00:44:08.290 --> 00:44:12.150
But ISIS was saying
the opposite.

00:44:12.150 --> 00:44:13.430
We've take it over.

00:44:13.430 --> 00:44:16.510
And then by saying it, they
were making a fait accompli

00:44:16.510 --> 00:44:19.980
because they were terrifying
even the townspeople who

00:44:19.980 --> 00:44:21.480
thought, OK, everything's lost.

00:44:21.480 --> 00:44:23.560
We'd better leave.

00:44:23.560 --> 00:44:27.332
It was a way to say,
oh, it's all done.

00:44:27.332 --> 00:44:29.290
And a lot of these refugees
and a lot of people

00:44:29.290 --> 00:44:30.040
who were fleeing--

00:44:30.040 --> 00:44:32.190
One of ISIS's most
powerful weapons

00:44:32.190 --> 00:44:34.550
is it scare tactics
through the internet.

00:44:34.550 --> 00:44:36.140
Sometimes ISIS
will come to a town

00:44:36.140 --> 00:44:37.982
and the town will
already be empty.

00:44:37.982 --> 00:44:40.190
Sometimes they'll announce
that they've taken a town,

00:44:40.190 --> 00:44:41.810
they haven't even taken it yet.

00:44:41.810 --> 00:44:43.830
But the people in the
town will say, what?

00:44:43.830 --> 00:44:45.205
This town has been
taken by ISIS?

00:44:45.205 --> 00:44:47.900
And they'll run away.

00:44:47.900 --> 00:44:51.400
STACIE CHAN: So with all that
misinformation just rampant

00:44:51.400 --> 00:44:53.780
on the internet,
people plagiarizing,

00:44:53.780 --> 00:44:58.540
this misinformation, ripping it
off, shaping into another form,

00:44:58.540 --> 00:45:02.240
the follow-up question online
is, how do you and the NBC

00:45:02.240 --> 00:45:05.400
digital team then make sure
your online audience is

00:45:05.400 --> 00:45:09.900
reading and/or watching your
originally reported story

00:45:09.900 --> 00:45:11.000
with all the facts?

00:45:11.000 --> 00:45:11.769
Or can you?

00:45:11.769 --> 00:45:12.810
RICHARD ENGEL: It's hard.

00:45:12.810 --> 00:45:15.260
I mean, you have to
put it out there.

00:45:15.260 --> 00:45:22.490
There's so many different ways
to get into people's devices.

00:45:22.490 --> 00:45:26.275
How can I make sure that
you get the story that

00:45:26.275 --> 00:45:28.400
is the right story, the
story that I worked hardest

00:45:28.400 --> 00:45:29.900
to prioritize that one?

00:45:29.900 --> 00:45:30.890
STACIE CHAN: That I'm
not going to click

00:45:30.890 --> 00:45:31.848
on that obscure source?

00:45:31.848 --> 00:45:33.109
RICHARD ENGEL: I don't know.

00:45:33.109 --> 00:45:33.650
I don't know.

00:45:33.650 --> 00:45:36.034
I try and-- I mean, I think
NBC has a vested interest

00:45:36.034 --> 00:45:37.700
to try and promote
the material and make

00:45:37.700 --> 00:45:38.866
sure it's easily accessible.

00:45:38.866 --> 00:45:44.400
But I do not own an
algorithm that sorts data.

00:45:44.400 --> 00:45:46.922
I know another company,
however, that does,

00:45:46.922 --> 00:45:48.880
that is good at finding
things on the internet.

00:45:48.880 --> 00:45:49.320
STACIE CHAN: Hint,
hint, nudge, nudge.

00:45:49.320 --> 00:45:49.820
Point taken.

00:45:49.820 --> 00:45:51.559
RICHARD ENGEL: But
that's not my job.

00:45:51.559 --> 00:45:52.850
Although I hear people do that.

00:45:52.850 --> 00:45:53.990
They do organize data.

00:45:53.990 --> 00:45:54.700
And they find it.

00:45:54.700 --> 00:45:56.460
And they sort of
select groups and they

00:45:56.460 --> 00:45:58.244
try and stream data to them.

00:45:58.244 --> 00:45:59.410
AUDIENCE: You can find news.

00:45:59.410 --> 00:46:00.310
RICHARD ENGEL: Yes.

00:46:00.310 --> 00:46:01.486
STACIE CHAN: Fair point.

00:46:01.486 --> 00:46:02.160
All right.

00:46:02.160 --> 00:46:04.000
A question from the audience.

00:46:04.000 --> 00:46:06.330
AUDIENCE: Thank you for coming
to speak to us, Richard.

00:46:06.330 --> 00:46:07.740
RICHARD ENGEL: Oh, absolutely.

00:46:07.740 --> 00:46:11.000
AUDIENCE: So recently
on a cable news channel,

00:46:11.000 --> 00:46:13.400
you characterized the
president's foreign policy

00:46:13.400 --> 00:46:15.800
as convoluted and
self-contradictory.

00:46:15.800 --> 00:46:20.680
And I think you give examples
of the administration supporting

00:46:20.680 --> 00:46:23.430
Iran and some of its
activities in the Middle East,

00:46:23.430 --> 00:46:26.580
and then Saudi Arabia and
some of its activities.

00:46:26.580 --> 00:46:30.260
So I'd be interested
to hear more about why

00:46:30.260 --> 00:46:34.590
you think that that
isn't the best way to go.

00:46:34.590 --> 00:46:37.636
RICHARD ENGEL: Well, talking
about being self-contradictory,

00:46:37.636 --> 00:46:39.260
you speak to a lot
of analysts and this

00:46:39.260 --> 00:46:41.340
is what a lot of analysts
have been telling me.

00:46:41.340 --> 00:46:43.715
And a lot of people in the
region, some political leaders

00:46:43.715 --> 00:46:44.320
in the region.

00:46:44.320 --> 00:46:48.940
And they say if you look
at the policy, objectively,

00:46:48.940 --> 00:46:51.190
is it consistent?

00:46:51.190 --> 00:46:52.640
And their answer is no.

00:46:52.640 --> 00:46:54.180
It's not consistent.

00:46:54.180 --> 00:46:56.920
So if you take Syria
and Iraq, for example,

00:46:56.920 --> 00:47:00.130
where they're both really
proxy wars in many ways.

00:47:00.130 --> 00:47:02.030
They're civil wars,
but also proxy wars.

00:47:02.030 --> 00:47:08.480
So in Iraq, you have the Iraqi
army and the Iraqi government,

00:47:08.480 --> 00:47:11.090
which are being backed by Iran.

00:47:11.090 --> 00:47:15.860
And when those Shiite militias,
which are also backed by Iran,

00:47:15.860 --> 00:47:20.750
fight in a city like Tikrit,
the US is giving them support.

00:47:20.750 --> 00:47:24.540
So the US, in this case,
in this piece of the war,

00:47:24.540 --> 00:47:27.340
is supporting Iran.

00:47:27.340 --> 00:47:31.590
Up in the north of
Iraq, there's Kurdistan.

00:47:31.590 --> 00:47:34.770
In that case, the
US is more or less

00:47:34.770 --> 00:47:36.770
supporting Kurdish autonomy.

00:47:36.770 --> 00:47:40.440
We are helping by helping
the Kurds to establish

00:47:40.440 --> 00:47:42.540
their own state.

00:47:42.540 --> 00:47:46.270
Like it or not-- a lot of people
like it, plenty of countries

00:47:46.270 --> 00:47:49.760
do not like it-- but that's what
the effective result of the US

00:47:49.760 --> 00:47:51.400
policy is.

00:47:51.400 --> 00:47:55.660
So it's supporting an
independent Kurdistan

00:47:55.660 --> 00:47:57.970
while telling Baghdad,
at the same time,

00:47:57.970 --> 00:48:00.730
that the government needs to
reconstitute itself and seize

00:48:00.730 --> 00:48:03.510
control of all of its territory.

00:48:03.510 --> 00:48:06.905
So if you're a Baghdad, that
seems very contradictory

00:48:06.905 --> 00:48:08.780
because the Americans
will come and tell you,

00:48:08.780 --> 00:48:12.137
Baghdad, you need to do more to
take control of your country.

00:48:12.137 --> 00:48:14.470
And then Baghdad says, but
you're establishing Kurdistan

00:48:14.470 --> 00:48:15.880
in the north of the country.

00:48:15.880 --> 00:48:17.990
That's a contradiction.

00:48:17.990 --> 00:48:22.970
The Sunnis in Anbar province,
which is western Iraq, the US

00:48:22.970 --> 00:48:26.000
just recently-- just
few days ago, in fact--

00:48:26.000 --> 00:48:29.740
decided to send 400 more,
450 more advisers to Anbar.

00:48:29.740 --> 00:48:32.390
So we're effectively,
more or less becoming

00:48:32.390 --> 00:48:36.720
a sponsor of the Sunnis there.

00:48:36.720 --> 00:48:38.960
They are radically anti-Iran.

00:48:41.850 --> 00:48:44.920
So they are confused
by the policy,

00:48:44.920 --> 00:48:47.990
which, again, when the
Iranian forces move upward

00:48:47.990 --> 00:48:50.940
toward Tikrit,
we're helping them.

00:48:50.940 --> 00:48:53.490
So the people in Anbar say,
well, this doesn't make sense.

00:48:53.490 --> 00:48:55.320
This is totally convoluted.

00:48:55.320 --> 00:48:59.620
Cross the border
into Syria, the US

00:48:59.620 --> 00:49:03.170
is fighting ISIS and bombing
ISIS almost on a regular basis.

00:49:03.170 --> 00:49:07.640
Bashar al-Assad is also bombing
ISIS on a regular basis.

00:49:07.640 --> 00:49:11.950
So when we bomb ISIS, are
we helping Bashar al-Assad?

00:49:11.950 --> 00:49:13.380
In effect, yes.

00:49:13.380 --> 00:49:16.410
But we're also
still backing rebels

00:49:16.410 --> 00:49:21.970
who are against Bashar al-Assad,
and paying and training them.

00:49:21.970 --> 00:49:27.940
Iran backs the government of
Bashar al-Assad and attacks

00:49:27.940 --> 00:49:28.440
ISIS.

00:49:31.020 --> 00:49:34.070
We're negotiating with
Iran at the same time

00:49:34.070 --> 00:49:36.150
for a nuclear deal.

00:49:36.150 --> 00:49:38.570
So a lot of people say,
what is all of this?

00:49:38.570 --> 00:49:43.640
It's so self-contradictory
in many ways.

00:49:43.640 --> 00:49:45.080
And it's inconsistent.

00:49:45.080 --> 00:49:48.410
And is it working?

00:49:48.410 --> 00:49:50.130
If you look at the
region right now,

00:49:50.130 --> 00:49:53.780
it doesn't necessarily
appear to be working.

00:49:53.780 --> 00:49:56.470
So that's why I would say--
I know it's a long answer,

00:49:56.470 --> 00:49:58.220
but that's why I would
say a lot of people

00:49:58.220 --> 00:50:03.449
I speak to say that it is
internally inconsistent.

00:50:03.449 --> 00:50:03.990
AUDIENCE: Hi.

00:50:03.990 --> 00:50:10.050
So my question is more about
your path to getting here.

00:50:10.050 --> 00:50:11.890
It seems like when
you graduated,

00:50:11.890 --> 00:50:16.785
you just decided to pursue this
calling for adventure and stuff

00:50:16.785 --> 00:50:20.380
like that, and this region
that seems very interesting.

00:50:20.380 --> 00:50:23.440
And I'm just
wondering how you were

00:50:23.440 --> 00:50:27.150
able to-- I guess, in a way,
there's a lot of pressure

00:50:27.150 --> 00:50:29.700
from I'm sure your
parents or other people

00:50:29.700 --> 00:50:32.540
or just seeing your
classmates doing other stuff.

00:50:32.540 --> 00:50:36.680
How are you able to
stick to your plan

00:50:36.680 --> 00:50:39.050
through I'm sure many years
of just, what am I doing?

00:50:39.050 --> 00:50:40.135
RICHARD ENGEL: Almost 20.

00:50:40.135 --> 00:50:42.525
I've been doing the same
thing for almost 20 years.

00:50:42.525 --> 00:50:43.150
AUDIENCE: Yeah.

00:50:43.150 --> 00:50:46.090
It was a sense of, I
don't know what I'm doing,

00:50:46.090 --> 00:50:52.050
there's no clear, but
there is this ambition.

00:50:52.050 --> 00:50:55.760
RICHARD ENGEL: Well, I
graduated from Stanford just

00:50:55.760 --> 00:50:57.510
down the street from here.

00:50:57.510 --> 00:51:02.650
And I left immediately for
Cairo, or almost immediately,

00:51:02.650 --> 00:51:04.540
because I wanted
to be a journalist.

00:51:04.540 --> 00:51:06.020
And I thought the
Middle East was

00:51:06.020 --> 00:51:07.870
going to allow
opportunities and was

00:51:07.870 --> 00:51:10.812
going to be-- I like to use this
analogy-- the train of history.

00:51:10.812 --> 00:51:12.770
And I thought that's
where the train of history

00:51:12.770 --> 00:51:14.232
was going to go through next.

00:51:14.232 --> 00:51:16.315
And I wanted to get on
board, sit in the front car

00:51:16.315 --> 00:51:18.190
and watch it happen.

00:51:18.190 --> 00:51:21.730
And I wanted to do
that passionately.

00:51:21.730 --> 00:51:23.060
And I still want to do that.

00:51:23.060 --> 00:51:26.980
So I haven't had a lot of
second guessing about it.

00:51:26.980 --> 00:51:28.140
There's been tough days.

00:51:28.140 --> 00:51:32.300
And there've been bumps
along the road, certainly.

00:51:32.300 --> 00:51:34.080
We've talked about
some of them today.

00:51:34.080 --> 00:51:39.670
But I still very
much enjoy-- I would

00:51:39.670 --> 00:51:42.550
say love-- sitting
in that front car

00:51:42.550 --> 00:51:45.320
and watching what's
happening outside

00:51:45.320 --> 00:51:47.445
and seeing history unfold.

00:51:47.445 --> 00:51:50.070
And I don't know if it's always
going to be in the Middle East.

00:51:50.070 --> 00:51:53.550
I've done almost 20
years now in that region.

00:51:53.550 --> 00:51:56.080
But I'm spending more
time in Russia these days,

00:51:56.080 --> 00:51:57.780
just got back in Nepal.

00:51:57.780 --> 00:52:00.810
I think what's happening
in China is fascinating.

00:52:00.810 --> 00:52:03.000
The train keeps moving.

00:52:03.000 --> 00:52:07.290
And we've talked a little
bit about the intersection

00:52:07.290 --> 00:52:12.480
between technology and
political change today.

00:52:12.480 --> 00:52:14.140
And I gave some
thoughts about where

00:52:14.140 --> 00:52:16.770
I think maybe the way
of the world is heading.

00:52:16.770 --> 00:52:17.950
But I don't know.

00:52:17.950 --> 00:52:19.140
We'll see.

00:52:19.140 --> 00:52:21.160
And that's the exciting part.

00:52:21.160 --> 00:52:26.494
That's why I haven't had a lot
of second guesses from watching

00:52:26.494 --> 00:52:27.910
my friends and
what they're doing.

00:52:27.910 --> 00:52:33.250
And I've been really
enjoying the ride.

00:52:33.250 --> 00:52:34.625
STACIE CHAN: And
on a final note,

00:52:34.625 --> 00:52:36.375
when you're giving the
commencement speech

00:52:36.375 --> 00:52:37.810
at Stanford, it
was to an audience

00:52:37.810 --> 00:52:41.450
of relatively young,
recent college graduates.

00:52:41.450 --> 00:52:45.210
But I believe you're talking
to a room and a company

00:52:45.210 --> 00:52:49.204
full of people who have that
similar wide-eyed approach

00:52:49.204 --> 00:52:50.870
to the world, where
the sky's the limit.

00:52:50.870 --> 00:52:53.374
We're here to
innovate and to change

00:52:53.374 --> 00:52:54.415
the world for the better.

00:52:54.415 --> 00:52:54.690
RICHARD ENGEL: Good.

00:52:54.690 --> 00:52:56.231
STACIE CHAN: So do
you have a message

00:52:56.231 --> 00:52:59.325
for us Googlers going forward
as we go back to our desks

00:52:59.325 --> 00:53:00.200
and get back to work?

00:53:00.200 --> 00:53:00.850
RICHARD ENGEL: Well, yeah.

00:53:00.850 --> 00:53:02.840
Then please do change
the world for the better.

00:53:02.840 --> 00:53:04.760
I mean, I'm not saying
that you're not now.

00:53:04.760 --> 00:53:07.810
But I'm saying that I think
that's an important goal

00:53:07.810 --> 00:53:12.350
to stick to, that the
internet doesn't have

00:53:12.350 --> 00:53:21.560
to be just a vehicle for
crazy propaganda or porn

00:53:21.560 --> 00:53:26.230
or a way to share
photos of your lunch.

00:53:26.230 --> 00:53:29.185
Is there a way to actually
make it better, make

00:53:29.185 --> 00:53:33.470
our society better, not just
more interconnected and more

00:53:33.470 --> 00:53:36.950
volatile, but actually better?

00:53:36.950 --> 00:53:39.040
That would be, I mean,
an incredible machine.

00:53:39.040 --> 00:53:43.116
If you could do that,
I'll come work here.

00:53:43.116 --> 00:53:44.990
STACIE CHAN: We Just
have a great recruitment

00:53:44.990 --> 00:53:46.425
tool for Richard Engel.

00:53:46.425 --> 00:53:48.300
Well, thank you so much,
Richard, for coming.

00:53:48.300 --> 00:53:48.500
RICHARD ENGEL: If
you'll have me.

00:53:48.500 --> 00:53:49.630
STACIE CHAN: We
really appreciate it.

00:53:49.630 --> 00:53:49.880
Yes.

00:53:49.880 --> 00:53:51.490
Thank you so much
for coming today.

00:53:51.490 --> 00:53:52.390
It's very kind.

00:53:52.390 --> 00:53:54.840
[APPLAUSE]

