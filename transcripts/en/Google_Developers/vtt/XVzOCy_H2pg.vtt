WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.218
[MUSIC PLAYING]

00:00:05.218 --> 00:00:07.260
JACQUELLINE FULLER: Well,
good morning, everyone.

00:00:07.260 --> 00:00:11.090
I'm Jacquelline Fuller, and I'm
the president of Google.org,

00:00:11.090 --> 00:00:13.020
which is Google's philanthropy.

00:00:13.020 --> 00:00:15.320
And I am so pleased,
so, so pleased,

00:00:15.320 --> 00:00:17.420
to be here today
with our friends.

00:00:17.420 --> 00:00:21.630
This is Gavin McCormick,
from WattTime, Nada Malou,

00:00:21.630 --> 00:00:24.620
from MÃ©decins Sans Frontiers,
also known as Doctors Without

00:00:24.620 --> 00:00:29.820
Borders, and Anandan,
from Wadhwani AI.

00:00:29.820 --> 00:00:33.770
So I'm going to provide
a little bit of context

00:00:33.770 --> 00:00:37.340
on Google.org itself and
the AI impact challenge,

00:00:37.340 --> 00:00:39.710
and then we're going
to hear from some

00:00:39.710 --> 00:00:42.050
of our winners about
the important work

00:00:42.050 --> 00:00:44.030
that they are doing globally.

00:00:44.030 --> 00:00:46.640
So Google.org is
Google's philanthropy,

00:00:46.640 --> 00:00:50.390
and our mission is to help
bring together technologists,

00:00:50.390 --> 00:00:54.260
such as all of you, and
experts on the front lines

00:00:54.260 --> 00:00:58.050
who are dedicated to improving
the environment, education,

00:00:58.050 --> 00:00:59.180
poverty.

00:00:59.180 --> 00:01:01.640
To really solve
complex challenges,

00:01:01.640 --> 00:01:06.950
and apply resources,
expertise, communications,

00:01:06.950 --> 00:01:10.370
and reach, together,
to help us really

00:01:10.370 --> 00:01:12.800
take a step change forward.

00:01:12.800 --> 00:01:15.410
For anyone who was in
Moustapha's talk right

00:01:15.410 --> 00:01:20.600
before this one, he talked about
how applying machine learning

00:01:20.600 --> 00:01:24.170
can help us take a step change,
even when all we're doing

00:01:24.170 --> 00:01:28.700
is changing the ML models,
and not even any changing

00:01:28.700 --> 00:01:31.050
to the underlying data
and infrastructure.

00:01:31.050 --> 00:01:33.860
So as Google.org, we
like to think about,

00:01:33.860 --> 00:01:37.910
how can we help Google itself
with our own technologies

00:01:37.910 --> 00:01:40.040
that we're applying
such as the flood

00:01:40.040 --> 00:01:43.010
forecasting you might have
heard about in the keynote?

00:01:43.010 --> 00:01:46.880
Well, we have a winner from
the AI Impact Challenge

00:01:46.880 --> 00:01:52.560
who's applying those kinds
of techniques for landslides.

00:01:52.560 --> 00:01:57.980
So the AI Impact Challenge
itself was global in nature.

00:01:57.980 --> 00:02:04.190
We provided $25 million in a
pool and opened it to the world

00:02:04.190 --> 00:02:07.710
and said, how would you use
artificial intelligence,

00:02:07.710 --> 00:02:11.960
machine learning, to make a
huge difference in improving

00:02:11.960 --> 00:02:14.060
the world in your specific area?

00:02:14.060 --> 00:02:17.190
And then the winners receive
the funding, the resources,

00:02:17.190 --> 00:02:19.160
but also expertise.

00:02:19.160 --> 00:02:25.010
They all get assigned help
from Googlers from our research

00:02:25.010 --> 00:02:25.760
group.

00:02:25.760 --> 00:02:28.560
They have access to an
accelerator called Credits.

00:02:28.560 --> 00:02:31.520
So basically it's a partnership,
an extended partnership,

00:02:31.520 --> 00:02:34.910
to see how Google can help
them carry out their vision.

00:02:34.910 --> 00:02:38.523
And what was so exciting is when
we first did this competition,

00:02:38.523 --> 00:02:40.190
it came out of a
conversation, actually,

00:02:40.190 --> 00:02:43.640
with Sundar Pichai,
our CEO, who said

00:02:43.640 --> 00:02:48.500
he is so excited to see
how AI can be applied

00:02:48.500 --> 00:02:51.620
not only at companies
like Google,

00:02:51.620 --> 00:02:56.390
but for the front line heroes
who are really trying to change

00:02:56.390 --> 00:02:59.030
the world when it
comes to global health

00:02:59.030 --> 00:03:01.730
and environmental concerns.

00:03:01.730 --> 00:03:05.480
How can we apply AI in
those settings as well?

00:03:05.480 --> 00:03:09.200
So out of that conversation we
launched the impact challenge,

00:03:09.200 --> 00:03:13.580
not really sure exactly
how it would turn out.

00:03:13.580 --> 00:03:17.270
We partnered with Jeff Dean,
the Google Research team,

00:03:17.270 --> 00:03:20.690
and the efforts being led by
Bridget and Molly, who are here

00:03:20.690 --> 00:03:23.540
and can talk to you afterwards
if you're interested.

00:03:23.540 --> 00:03:25.290
And so we put it out
there in the world.

00:03:25.290 --> 00:03:29.870
And we had more than 2,600
high-quality applications

00:03:29.870 --> 00:03:32.510
from more than 119 countries.

00:03:32.510 --> 00:03:36.000
And from those, we distilled
it down to 20 winners.

00:03:36.000 --> 00:03:39.420
And we have three
of them here today.

00:03:39.420 --> 00:03:40.970
So the conversation
is really going

00:03:40.970 --> 00:03:44.480
to be to understand
what you all are doing,

00:03:44.480 --> 00:03:47.490
to hear about your
inspiring stories.

00:03:47.490 --> 00:03:50.300
And I think we probably
have some technologists

00:03:50.300 --> 00:03:52.550
in the room who are really
interested in thinking

00:03:52.550 --> 00:03:55.490
about, how can they
apply technology

00:03:55.490 --> 00:03:56.930
to their issue's areas?

00:03:56.930 --> 00:03:58.620
The causes that they
care about most.

00:03:58.620 --> 00:04:01.700
So maybe you'll have some
advice and inspiring words

00:04:01.700 --> 00:04:03.080
for the audience as well.

00:04:03.080 --> 00:04:04.880
But why don't we start
by each of you just

00:04:04.880 --> 00:04:09.710
sharing your name, a
little bit of your bio,

00:04:09.710 --> 00:04:12.160
and then tell us a little
bit about the problem

00:04:12.160 --> 00:04:13.910
that your organization
is trying to solve.

00:04:13.910 --> 00:04:15.600
And Gavin, should
we start with you?

00:04:15.600 --> 00:04:15.800
GAVIN MCCORMICK: Sure.

00:04:15.800 --> 00:04:17.462
So I'm Gavin
McCormick, co-founder

00:04:17.462 --> 00:04:18.920
and executive
director at WattTime.

00:04:18.920 --> 00:04:20.690
I am not a developer
by training.

00:04:20.690 --> 00:04:23.790
I'm an economist, so
what am I doing here?

00:04:23.790 --> 00:04:26.510
I had worked formerly at
the US Department of Energy,

00:04:26.510 --> 00:04:29.265
and then was a PhD student
at UC Berkeley, when

00:04:29.265 --> 00:04:30.890
a new form of statistics
was coming out

00:04:30.890 --> 00:04:33.750
in economics, the so-called
credibility revolution.

00:04:33.750 --> 00:04:35.750
And it's being applied
to environmental problems

00:04:35.750 --> 00:04:37.333
to solve problems
that were previously

00:04:37.333 --> 00:04:38.300
considered impossible.

00:04:38.300 --> 00:04:41.450
So one of those is, where
is this energy coming from?

00:04:41.450 --> 00:04:43.100
We used to think
you couldn't know.

00:04:43.100 --> 00:04:46.100
Turns out with new algorithms,
we can flip a light switch

00:04:46.100 --> 00:04:48.920
and say what power
plant precisely

00:04:48.920 --> 00:04:51.110
will provide a little bit
more electricity if you

00:04:51.110 --> 00:04:53.630
plug in a phone, or use
an electric vehicle,

00:04:53.630 --> 00:04:54.435
or use some energy.

00:04:54.435 --> 00:04:56.060
And the only reason
that's interesting,

00:04:56.060 --> 00:04:58.230
is it turns out it
changes incredibly fast.

00:04:58.230 --> 00:04:59.733
So if you wait
five minutes, that

00:04:59.733 --> 00:05:01.650
could be the difference
between getting energy

00:05:01.650 --> 00:05:04.610
from a coal plant or getting
energy from a solar panel.

00:05:04.610 --> 00:05:06.735
And so there's actually
this entirely new dimension

00:05:06.735 --> 00:05:09.360
in environmentalism that
data just opened up.

00:05:09.360 --> 00:05:11.070
Instead of just
using less energy

00:05:11.070 --> 00:05:12.930
and helping the
environment, let's use it

00:05:12.930 --> 00:05:14.760
at very specific times.

00:05:14.760 --> 00:05:16.470
So my organization
WattTime was founded

00:05:16.470 --> 00:05:18.480
at a hackathon
where we figured out

00:05:18.480 --> 00:05:20.910
there's about $20 billion
IoT devices worldwide

00:05:20.910 --> 00:05:22.620
that can actually
have some code that

00:05:22.620 --> 00:05:24.960
is very flexible and exact
to when they use energy.

00:05:24.960 --> 00:05:28.463
So we produce an API
that says, what time

00:05:28.463 --> 00:05:30.630
is the best moment to use
energy if you want to have

00:05:30.630 --> 00:05:31.970
less environmental impact?

00:05:31.970 --> 00:05:34.470
Doesn't make any money, but
it's a really significant source

00:05:34.470 --> 00:05:36.840
of emissions reduction
for climate change and air

00:05:36.840 --> 00:05:39.090
pollution worldwide.

00:05:39.090 --> 00:05:41.370
And one of the interesting
things about it

00:05:41.370 --> 00:05:44.100
is, it was only possible because
the US federal government has

00:05:44.100 --> 00:05:47.730
this amazing open training
data set on all the pollution

00:05:47.730 --> 00:05:49.770
from all the power
plants in America.

00:05:49.770 --> 00:05:51.852
Basically no other
country has that.

00:05:51.852 --> 00:05:53.310
And so WattTime
realized that if we

00:05:53.310 --> 00:05:55.830
wanted to provide emissions
reductions in other countries,

00:05:55.830 --> 00:05:58.320
if we wanted to make
this a global technology,

00:05:58.320 --> 00:06:01.260
we needed some way to know
what's going on at every power

00:06:01.260 --> 00:06:02.817
plant in the world.

00:06:02.817 --> 00:06:04.650
So for this project,
what we are going to do

00:06:04.650 --> 00:06:08.940
is use visual recognition
software and machine learning

00:06:08.940 --> 00:06:12.660
on imagery taken by
satellites of all the power

00:06:12.660 --> 00:06:13.800
plants in the world.

00:06:13.800 --> 00:06:16.008
And it turns out, if you
use the right types of data,

00:06:16.008 --> 00:06:17.850
such as infrared
imagery, you can actually

00:06:17.850 --> 00:06:19.800
detect how much
pollution is coming out

00:06:19.800 --> 00:06:22.320
of every single power
plant worldwide,

00:06:22.320 --> 00:06:24.767
based on a satellite
infrastructure.

00:06:24.767 --> 00:06:26.350
So first of all,
that was kind of fun.

00:06:26.350 --> 00:06:27.933
But then the other
piece of the puzzle

00:06:27.933 --> 00:06:30.810
is we quickly discovered that a
lot of other environmental NGOs

00:06:30.810 --> 00:06:31.823
want the same data.

00:06:31.823 --> 00:06:33.240
So I'm here
representing WattTime,

00:06:33.240 --> 00:06:34.860
but we're actually
partnering with a lot

00:06:34.860 --> 00:06:36.818
of the biggest environmental
NGOs in the world.

00:06:36.818 --> 00:06:39.172
We decided to collectively
pool our resources

00:06:39.172 --> 00:06:40.630
and go on in this
project together.

00:06:40.630 --> 00:06:42.810
So I think it's fun
that Carbon Tracker will

00:06:42.810 --> 00:06:45.420
be using this to figure out,
what are all the cases where

00:06:45.420 --> 00:06:47.100
actually it would be cheaper
to use renewable energy

00:06:47.100 --> 00:06:47.850
than fossil fuels?

00:06:47.850 --> 00:06:50.250
Can we just shut down a
lot of world's coal plants?

00:06:50.250 --> 00:06:52.620
World Resources Institute
will be using this to see,

00:06:52.620 --> 00:06:54.870
are any countries polluting
more than they legally

00:06:54.870 --> 00:06:56.760
are bound to under
the Paris treaty?

00:06:56.760 --> 00:06:58.760
There's a whole bunch of
different environmental

00:06:58.760 --> 00:07:00.177
applications for
the same project.

00:07:00.177 --> 00:07:01.843
JACQUELLINE FULLER:
Great, thanks Gavin.

00:07:01.843 --> 00:07:02.370
Nada?

00:07:02.370 --> 00:07:03.460
NADA MALOU: Thank you.

00:07:03.460 --> 00:07:05.580
So I'm Doctor Nada Malou.

00:07:05.580 --> 00:07:07.860
I'm a PhD in microbiology.

00:07:07.860 --> 00:07:11.790
And so after my PhD, I decided
to work with the Doctors

00:07:11.790 --> 00:07:14.880
Without Borders, or MÃ©decins
Sans Frontieres in French.

00:07:14.880 --> 00:07:17.910
So we are an international,
medical, humanitarian

00:07:17.910 --> 00:07:22.510
organization providing care
for populations in need,

00:07:22.510 --> 00:07:26.550
because of war context, because
of outbreaks like Ebola,

00:07:26.550 --> 00:07:30.450
cholera, or for populations who
are excluded from their health

00:07:30.450 --> 00:07:31.690
care system.

00:07:31.690 --> 00:07:35.460
So we work in more
than 70 countries.

00:07:35.460 --> 00:07:38.950
And my work for MÃ©decins
Sans Frontieres,

00:07:38.950 --> 00:07:40.950
or Doctors Without Borders,
as a microbiologist,

00:07:40.950 --> 00:07:46.710
is to identify diagnostic
tools that can help to diagnose

00:07:46.710 --> 00:07:51.240
infection in the patients that
we receive in our different

00:07:51.240 --> 00:07:52.210
projects.

00:07:52.210 --> 00:07:56.340
So I worked in Ghana
and Mali in Africa,

00:07:56.340 --> 00:07:58.500
where I was mainly
specialized and worked

00:07:58.500 --> 00:08:01.770
in pediatric
population, so children

00:08:01.770 --> 00:08:05.820
arriving with malnutrition
and having also an infection

00:08:05.820 --> 00:08:07.140
with the bacteria.

00:08:07.140 --> 00:08:11.850
And then I worked in the Middle
East with war wounded patients.

00:08:11.850 --> 00:08:15.000
So those patients were also
arriving in our facility

00:08:15.000 --> 00:08:16.050
with infection.

00:08:16.050 --> 00:08:18.780
And it was very important
to diagnose this infection

00:08:18.780 --> 00:08:24.180
in order to avoid amputation
or a medical complication.

00:08:24.180 --> 00:08:29.970
So the challenge in
identifying diagnostic tools

00:08:29.970 --> 00:08:32.940
for low and middle
income countries

00:08:32.940 --> 00:08:34.530
is that most of the
diagnostic tools

00:08:34.530 --> 00:08:38.340
that we have are not adapted to
the constraints of the field.

00:08:38.340 --> 00:08:41.150
Here in the US, or even in
Europe, if you are sick,

00:08:41.150 --> 00:08:43.549
you have fever, you
can go to the hospital.

00:08:43.549 --> 00:08:44.730
They take your blood.

00:08:44.730 --> 00:08:46.620
They do a test, and
they will tell you

00:08:46.620 --> 00:08:48.960
which bacteria is
causing the infection

00:08:48.960 --> 00:08:50.940
and what is the
most effective drug

00:08:50.940 --> 00:08:53.310
to use to treat this infection.

00:08:53.310 --> 00:08:56.740
Those tests are not available
in the context where we work.

00:08:56.740 --> 00:08:59.550
So the question was, why
they are not available?

00:08:59.550 --> 00:09:02.460
So there are many reasons
to the problem of access,

00:09:02.460 --> 00:09:05.190
but also the fact that
they are not adapted.

00:09:05.190 --> 00:09:07.740
So one of the reasons
is the limitation

00:09:07.740 --> 00:09:11.220
in term of human resources
and trained human resources.

00:09:11.220 --> 00:09:15.030
So the idea for us was to try
to find a solution in order

00:09:15.030 --> 00:09:19.770
to make the interpretation and
the use of this test easier

00:09:19.770 --> 00:09:22.740
and available for the
majority of those patients

00:09:22.740 --> 00:09:23.940
that we are seeing.

00:09:23.940 --> 00:09:27.480
And I think you all
heard about this problem

00:09:27.480 --> 00:09:29.250
of antimicrobial resistance.

00:09:29.250 --> 00:09:31.740
We know that for
the coming decades,

00:09:31.740 --> 00:09:36.000
it is going to be one of the
major public health threats.

00:09:36.000 --> 00:09:45.720
We estimate that in 2050, the
death of 10 million patients

00:09:45.720 --> 00:09:48.720
will be attributable to
antimicrobial resistance.

00:09:48.720 --> 00:09:52.500
And it's going to be mainly
in the Asian and African

00:09:52.500 --> 00:09:53.190
continent.

00:09:53.190 --> 00:09:56.160
So it's very urgent
for us to give access

00:09:56.160 --> 00:10:00.235
to those diagnostic tools in
order to target the bacteria

00:10:00.235 --> 00:10:04.310
and to target the effective
treatment for the patients

00:10:04.310 --> 00:10:06.246
in a timely manner.

00:10:06.246 --> 00:10:07.663
JACQUELLINE FULLER:
Thanks, great.

00:10:07.663 --> 00:10:08.228
Anandan?

00:10:08.228 --> 00:10:09.020
ANANDAN: Thank you.

00:10:09.020 --> 00:10:10.705
My name is Anandan.

00:10:10.705 --> 00:10:12.830
Let me start with a little
bit of a personal story,

00:10:12.830 --> 00:10:14.370
because it's very relevant.

00:10:14.370 --> 00:10:17.900
I tell my friends and colleagues
I've been doing AI as long

00:10:17.900 --> 00:10:20.540
as most of them have been
alive, and in many cases more.

00:10:20.540 --> 00:10:24.140
I started my PhD in 1981,
went to UMass Amherst

00:10:24.140 --> 00:10:25.430
in computer vision.

00:10:25.430 --> 00:10:28.670
Had a good, driving research
and publishing career.

00:10:28.670 --> 00:10:30.350
I was the head of
computer vision

00:10:30.350 --> 00:10:33.020
in Microsoft Research in
Redmond for a number of years.

00:10:33.020 --> 00:10:37.830
But in 2004, I felt like I
wanted to do something more

00:10:37.830 --> 00:10:41.540
and saw the opportunity
to build technology

00:10:41.540 --> 00:10:44.120
for the development
of social good

00:10:44.120 --> 00:10:46.720
as a potential agenda
from a research side.

00:10:46.720 --> 00:10:50.660
And so, convinced Microsoft to
set up a research lab in India.

00:10:50.660 --> 00:10:52.550
And I went and set up
that lab and ran it.

00:10:52.550 --> 00:10:55.910
Although it does much more than
technology for social good,

00:10:55.910 --> 00:10:58.550
it did become known
for that work.

00:10:58.550 --> 00:11:00.490
In fact we spun out
a couple of NGOs.

00:11:00.490 --> 00:11:02.570
Nine years of that, came back.

00:11:02.570 --> 00:11:07.250
Then in 2017, I met two
philanthropic donors.

00:11:07.250 --> 00:11:09.990
Doctors Romesh Wadhwani,
who lives in California,

00:11:09.990 --> 00:11:13.100
and his brother, Sunil
Wadhwani, both of Indian origin,

00:11:13.100 --> 00:11:15.950
but done well in
the tech industry.

00:11:15.950 --> 00:11:17.420
Well-known philanthropists.

00:11:17.420 --> 00:11:20.720
And we're looking to see how
they can apply their ideas

00:11:20.720 --> 00:11:23.370
and money towards social good.

00:11:23.370 --> 00:11:25.520
So they got the idea
of actually setting up

00:11:25.520 --> 00:11:27.980
a research institute
that would build

00:11:27.980 --> 00:11:31.730
AI solutions for a variety
of social domain problems,

00:11:31.730 --> 00:11:33.220
and then get it implemented.

00:11:33.220 --> 00:11:35.180
And they tapped me to be CEO.

00:11:35.180 --> 00:11:36.980
So here I thought,
here's my second chance

00:11:36.980 --> 00:11:38.130
to change the world.

00:11:38.130 --> 00:11:40.130
So I quit my job, what I
was doing at that time,

00:11:40.130 --> 00:11:42.050
and went back to
India to set it up.

00:11:42.050 --> 00:11:44.120
We started, and it took
us a while to launch.

00:11:44.120 --> 00:11:47.030
We started last year in
February the Wadhwani Institute

00:11:47.030 --> 00:11:49.790
for Artificial Intelligence
in Bombay, Mumbai.

00:11:49.790 --> 00:11:54.530
And because of the importance
of the agenda and how hot AI is,

00:11:54.530 --> 00:11:58.070
the prime minister of India came
and inaugurated our institute.

00:11:58.070 --> 00:12:00.350
Our charter is really simple.

00:12:00.350 --> 00:12:04.400
We want to identify societal
domain problems specifically

00:12:04.400 --> 00:12:06.020
to help the under-served
communities.

00:12:06.020 --> 00:12:08.120
There are roughly 3
or 4 billion people

00:12:08.120 --> 00:12:11.210
in the world who don't
benefit from the kind of AI

00:12:11.210 --> 00:12:14.130
that we all do on
an everyday basis.

00:12:14.130 --> 00:12:15.050
They don't take Uber.

00:12:15.050 --> 00:12:16.450
They don't do search.

00:12:16.450 --> 00:12:17.540
All that kind of stuff.

00:12:17.540 --> 00:12:19.040
And it doesn't
affect their lives.

00:12:19.040 --> 00:12:21.020
And many of them, in
fact, most of them,

00:12:21.020 --> 00:12:22.800
are typically wards
of the government.

00:12:22.800 --> 00:12:25.650
In fact, their lives depend
a lot on government programs.

00:12:25.650 --> 00:12:27.950
These are poor farmers
trying to do agriculture

00:12:27.950 --> 00:12:29.390
at a very basic level.

00:12:29.390 --> 00:12:33.260
Small hold farmers, like one
acre of land, or something.

00:12:33.260 --> 00:12:34.360
Basic health care.

00:12:34.360 --> 00:12:36.320
There are 900 million
people in India

00:12:36.320 --> 00:12:39.020
who depend on the public
health care system, which

00:12:39.020 --> 00:12:40.760
has about 30,000 doctors.

00:12:40.760 --> 00:12:43.070
And this is true of a lot
of the developing world.

00:12:43.070 --> 00:12:44.790
Basic level of education.

00:12:44.790 --> 00:12:46.290
Literacy is still a problem.

00:12:46.290 --> 00:12:48.740
But beyond that,
financial inclusion.

00:12:48.740 --> 00:12:52.020
Getting credit if you are a poor
person is next to impossible.

00:12:52.020 --> 00:12:55.850
And basic infrastructures, clean
water, utilities, and stuff

00:12:55.850 --> 00:12:58.190
like that, and done in
an efficient manner.

00:12:58.190 --> 00:13:01.760
So we saw the opportunity
that AI, in fact,

00:13:01.760 --> 00:13:05.000
is this technology that can
learn the particular conditions

00:13:05.000 --> 00:13:08.660
locally and be able to develop
solutions to address them.

00:13:08.660 --> 00:13:11.360
Because these problems vary
a lot from place to place,

00:13:11.360 --> 00:13:14.100
and you can't do a one shoe
fits all type of approach.

00:13:14.100 --> 00:13:15.950
So we took that up.

00:13:15.950 --> 00:13:19.040
We are a bit more than a year
old now, having a great time.

00:13:19.040 --> 00:13:21.320
A couple of projects in
health and one agriculture.

00:13:21.320 --> 00:13:25.535
The agriculture one is the one
that's funded by the Google AI

00:13:25.535 --> 00:13:26.480
Impact Challenge.

00:13:26.480 --> 00:13:27.360
Really exciting.

00:13:27.360 --> 00:13:30.110
I have a team of about 25,
30 people now, growing.

00:13:30.110 --> 00:13:32.160
My vice president for
product and programs,

00:13:32.160 --> 00:13:33.860
Raghu, is in the front row.

00:13:33.860 --> 00:13:35.540
You can talk to
him later as well.

00:13:35.540 --> 00:13:37.520
Thank you.

00:13:37.520 --> 00:13:40.742
JACQUELLINE FULLER: So when we
started this program, the AI

00:13:40.742 --> 00:13:42.200
Impact Challenge,
it was because we

00:13:42.200 --> 00:13:47.840
thought AI can help bring new
technology to old challenges.

00:13:47.840 --> 00:13:49.940
And that it's really
important to bring

00:13:49.940 --> 00:13:52.290
the experts who
know the issue best,

00:13:52.290 --> 00:13:55.130
the people who are closest
to the problem, together

00:13:55.130 --> 00:13:58.790
with not only the tools,
but also expertise

00:13:58.790 --> 00:14:00.270
from Google and others.

00:14:00.270 --> 00:14:02.270
So what was the
moment, or was there

00:14:02.270 --> 00:14:05.180
a specific moment, where
the light kind of went on

00:14:05.180 --> 00:14:06.500
and you thought, you know what?

00:14:06.500 --> 00:14:09.083
AI might be able to solve this
in a way that it hasn't before.

00:14:11.570 --> 00:14:13.210
NADA MALOU: I
remember that moment.

00:14:13.210 --> 00:14:16.790
And as a medical person, I
was not having a solution.

00:14:16.790 --> 00:14:21.530
I was having an idea, but
no solution to this need.

00:14:21.530 --> 00:14:23.930
So I spent a few
years to implement

00:14:23.930 --> 00:14:27.200
in those microbiology
laboratories and training

00:14:27.200 --> 00:14:28.850
a lot of lab technicians.

00:14:28.850 --> 00:14:31.940
And what I noticed is that
training them to do the test

00:14:31.940 --> 00:14:34.820
was very easy, and
they were very good.

00:14:34.820 --> 00:14:38.480
But then how to interpret
the test and how to use it

00:14:38.480 --> 00:14:42.080
was much more difficult
and takes a lot of time.

00:14:42.080 --> 00:14:44.990
That's why in
Europe, or in the US,

00:14:44.990 --> 00:14:47.990
we have clinical microbiologists
who study for years

00:14:47.990 --> 00:14:49.550
to have this expertise.

00:14:49.550 --> 00:14:52.670
And the moment where I thought,
OK, this is not sustainable,

00:14:52.670 --> 00:14:57.450
we need to find a
solution, was in Yemen

00:14:57.450 --> 00:14:59.660
where we opened a
microbiology lab because there

00:14:59.660 --> 00:15:00.590
was a medical need.

00:15:00.590 --> 00:15:02.660
The patients were
arriving infected,

00:15:02.660 --> 00:15:04.970
and it was important
for us to treat them.

00:15:04.970 --> 00:15:09.200
So we opened that
lab, and I was not

00:15:09.200 --> 00:15:12.350
able to find expertise locally.

00:15:12.350 --> 00:15:16.560
And I was neither able to
send expert microbiologists

00:15:16.560 --> 00:15:19.200
because it was a difficult
security context.

00:15:19.200 --> 00:15:21.980
And so they were sending
me the results by email.

00:15:21.980 --> 00:15:24.752
And I was checking each results
and validating each result,

00:15:24.752 --> 00:15:26.210
and I thought this
is not possible.

00:15:26.210 --> 00:15:27.290
JACQUELLINE FULLER:
That doesn't scale.

00:15:27.290 --> 00:15:29.370
NADA MALOU: No, this
is not sustainable.

00:15:29.370 --> 00:15:31.790
And it was a crazy idea.

00:15:31.790 --> 00:15:34.640
And by coincidence,
one day, I spoke

00:15:34.640 --> 00:15:37.260
with one of my PhD friends.

00:15:37.260 --> 00:15:39.140
And so he tried to
convince me to use

00:15:39.140 --> 00:15:40.770
very complicated technology.

00:15:40.770 --> 00:15:43.340
And I said, no, the
solution is easy.

00:15:43.340 --> 00:15:45.560
I want something very simple.

00:15:45.560 --> 00:15:49.550
I want to take my phone to
take a picture of the test

00:15:49.550 --> 00:15:52.430
and to have the results
and the interpretation.

00:15:52.430 --> 00:15:54.590
And he called me
three months later,

00:15:54.590 --> 00:15:56.293
and he's a bioinformatician.

00:15:56.293 --> 00:15:57.710
So he called me
three months later

00:15:57.710 --> 00:15:59.400
and he said, we can do it.

00:15:59.400 --> 00:16:01.010
We can do it.

00:16:01.010 --> 00:16:05.860
And for me, it was like,
OK, maybe, if you want.

00:16:05.860 --> 00:16:10.910
And one year and a half after,
here we are with a prototype,

00:16:10.910 --> 00:16:16.918
and with something that can
be really a game changer.

00:16:16.918 --> 00:16:19.460
JACQUELLINE FULLER: Anandan,
did you have a moment like that?

00:16:19.460 --> 00:16:21.770
ANANDAN: Last summer,
after we started,

00:16:21.770 --> 00:16:25.220
we started looking out for what
problems we want to work on.

00:16:25.220 --> 00:16:26.690
Because of the
initial visibility,

00:16:26.690 --> 00:16:29.790
we got a lot of people who were
interested in talking to us.

00:16:29.790 --> 00:16:31.910
However, when we
started learning

00:16:31.910 --> 00:16:35.090
about how to go about it, what
we realized was fundamentally,

00:16:35.090 --> 00:16:38.210
there are three questions
that we have to answer.

00:16:38.210 --> 00:16:39.710
What should we work on?

00:16:39.710 --> 00:16:41.720
How do we go about
building it in order

00:16:41.720 --> 00:16:43.070
so that it will be effective?

00:16:43.070 --> 00:16:46.400
And how do we get it deployed
and eventually scale?

00:16:46.400 --> 00:16:48.600
And to this, what
we realized was--

00:16:48.600 --> 00:16:50.870
see, in commercial
domain, there is a market.

00:16:50.870 --> 00:16:52.790
Companies like Google
or other companies

00:16:52.790 --> 00:16:54.890
know how to develop the market.

00:16:54.890 --> 00:16:57.230
In social domain, well,
there is a problem of course.

00:16:57.230 --> 00:16:59.900
The capacity to pay is
not there among the people

00:16:59.900 --> 00:17:01.070
that you are trying to help.

00:17:01.070 --> 00:17:03.590
But even if we find
a model to do that,

00:17:03.590 --> 00:17:05.660
the market is
extremely fragmented.

00:17:05.660 --> 00:17:09.530
The approach to getting to
your audience in one state, one

00:17:09.530 --> 00:17:12.410
city, one village, one region,
of one part of the world,

00:17:12.410 --> 00:17:13.910
is very different from another.

00:17:13.910 --> 00:17:15.859
Because every place
you go, there's

00:17:15.859 --> 00:17:18.079
some kind of a program that
is a government program,

00:17:18.079 --> 00:17:20.180
or other NGO
program, that exists.

00:17:20.180 --> 00:17:21.780
And you have to
work through them.

00:17:21.780 --> 00:17:23.750
In fact, they exist
simply because no one else

00:17:23.750 --> 00:17:24.990
is taking care of them.

00:17:24.990 --> 00:17:26.450
So what we learned
is that in order

00:17:26.450 --> 00:17:27.950
to answer those
questions, we have

00:17:27.950 --> 00:17:31.380
to partner with organizations
that are capable of execution

00:17:31.380 --> 00:17:32.370
and delivery.

00:17:32.370 --> 00:17:34.310
So we consulted with them a lot.

00:17:34.310 --> 00:17:36.830
So they became the source
of problems for us,

00:17:36.830 --> 00:17:38.450
identifying what to work on.

00:17:38.450 --> 00:17:41.390
But equally then, they are also
the people who can co-create.

00:17:41.390 --> 00:17:43.910
Because if they are willing
to put in their energy

00:17:43.910 --> 00:17:47.210
and interest in working with
us, the solution we produce

00:17:47.210 --> 00:17:49.290
will be the right
one for them to use.

00:17:49.290 --> 00:17:50.930
And one of the other
things we learned

00:17:50.930 --> 00:17:53.990
in this process is the primary
users of our technology

00:17:53.990 --> 00:17:55.160
are not the end users.

00:17:55.160 --> 00:17:57.290
Not the farmer,
not the poor mother

00:17:57.290 --> 00:17:58.820
in a village who is pregnant.

00:17:58.820 --> 00:18:01.160
But in fact, front-line
workers, health workers,

00:18:01.160 --> 00:18:03.167
front-line agriculture
and extension workers,

00:18:03.167 --> 00:18:04.250
who are the advice givers.

00:18:04.250 --> 00:18:06.530
Because they are capable
of using that technology,

00:18:06.530 --> 00:18:08.610
and they are part
of a system of care.

00:18:08.610 --> 00:18:11.120
So we needed to learn
from the partners

00:18:11.120 --> 00:18:14.060
to build solutions for that
kind of a use scenario.

00:18:14.060 --> 00:18:16.740
And finally, we have to work
with government in order

00:18:16.740 --> 00:18:17.240
to scale.

00:18:17.240 --> 00:18:19.910
Because all these solutions,
only the governments

00:18:19.910 --> 00:18:22.370
have the capacity to scale at
the national or international

00:18:22.370 --> 00:18:23.300
level.

00:18:23.300 --> 00:18:25.040
And you know how
challenging that can

00:18:25.040 --> 00:18:26.992
be, as governments come
and go, bureaucrats

00:18:26.992 --> 00:18:27.950
change, and everything.

00:18:27.950 --> 00:18:31.250
But what we learned is
that there is actually

00:18:31.250 --> 00:18:35.013
experienced groups of NGOs
and other partners and donor

00:18:35.013 --> 00:18:36.680
organizations, like
the Gates Foundation

00:18:36.680 --> 00:18:38.870
and so on, who know
how to do this.

00:18:38.870 --> 00:18:43.290
So they became our teachers
on how we go about doing this.

00:18:43.290 --> 00:18:46.970
JACQUELLINE FULLER: So it's
early days with AI, especially

00:18:46.970 --> 00:18:50.310
applied AI, on some of
these kinds of challenges.

00:18:50.310 --> 00:18:52.340
So as you think about
your road ahead,

00:18:52.340 --> 00:18:56.140
what are some of the
biggest risks or challenges

00:18:56.140 --> 00:18:58.550
that you see popping
up in front of you?

00:19:01.580 --> 00:19:02.080
Sure.

00:19:02.080 --> 00:19:04.288
GAVIN MCCORMICK: I think
for us, privacy and security

00:19:04.288 --> 00:19:05.720
is a surprisingly big deal.

00:19:05.720 --> 00:19:07.880
So this is supposed to be
a massively collaborative

00:19:07.880 --> 00:19:09.740
open-data project.

00:19:09.740 --> 00:19:11.890
But we're talking about a
literal global panopticon

00:19:11.890 --> 00:19:13.720
that can see critical
energy infrastructure

00:19:13.720 --> 00:19:14.740
in every country.

00:19:14.740 --> 00:19:16.870
And that would be really,
really easy to abuse.

00:19:16.870 --> 00:19:18.453
So for us the challenge
of, how do you

00:19:18.453 --> 00:19:21.310
make data as open as
possible to collaborators,

00:19:21.310 --> 00:19:23.530
while simultaneously
making sure that it can't

00:19:23.530 --> 00:19:25.720
be used for security risks?

00:19:25.720 --> 00:19:28.390
Or there are fossil fuel
companies that would love,

00:19:28.390 --> 00:19:32.410
instead of decreasing emissions,
to increase oil consumption.

00:19:32.410 --> 00:19:34.510
How do we make a system
that's as open as possible

00:19:34.510 --> 00:19:36.730
without enabling abuse?

00:19:36.730 --> 00:19:39.610
And so we're currently working
on unusual contract structures

00:19:39.610 --> 00:19:41.680
and unusual data
structures that we would

00:19:41.680 --> 00:19:45.903
consider semi open-source.

00:19:45.903 --> 00:19:47.820
JACQUELLINE FULLER:
Risks, challenges you see?

00:19:47.820 --> 00:19:50.245
ANANDAN: So I mean,
there are actually

00:19:50.245 --> 00:19:51.370
three levels of challenges.

00:19:51.370 --> 00:19:53.560
The simplest one
is the AI itself.

00:19:53.560 --> 00:19:55.810
I think that's actually the
one that we know how to do

00:19:55.810 --> 00:19:56.740
and can be done.

00:19:56.740 --> 00:19:58.450
The medium one is data.

00:19:58.450 --> 00:20:00.770
In the social domain,
data is not around,

00:20:00.770 --> 00:20:02.530
and data collection
is not very easy.

00:20:02.530 --> 00:20:05.830
In fact, whatever data
that exists is bad,

00:20:05.830 --> 00:20:07.440
meaning that it's
not good quality.

00:20:07.440 --> 00:20:10.210
It's often false, meaning
people make up stories.

00:20:10.210 --> 00:20:12.640
And it's sparse.

00:20:12.640 --> 00:20:15.460
All the information is
not available everywhere.

00:20:15.460 --> 00:20:17.630
And historic data is
not usually available.

00:20:17.630 --> 00:20:20.440
It's in silos and not
one centralized place.

00:20:20.440 --> 00:20:22.730
So every aspect of
data challenge exists.

00:20:22.730 --> 00:20:24.610
But, that said, it
can actually be met.

00:20:24.610 --> 00:20:27.790
You can actually work, create
a system of data creation,

00:20:27.790 --> 00:20:28.780
and work through that.

00:20:28.780 --> 00:20:31.540
The biggest challenge is
actually the execution.

00:20:31.540 --> 00:20:33.880
Because the kind
of organizations

00:20:33.880 --> 00:20:36.520
that you have to work with are
actually limited in funding

00:20:36.520 --> 00:20:39.830
and limited in capacity, and
they're working short-handed,

00:20:39.830 --> 00:20:40.480
really.

00:20:40.480 --> 00:20:42.520
And a diverse range of them.

00:20:42.520 --> 00:20:44.290
And potentially for
us, the risk factor

00:20:44.290 --> 00:20:45.970
always is the program risk.

00:20:45.970 --> 00:20:49.330
So unless we believe that
there exists current programs,

00:20:49.330 --> 00:20:52.240
like for instance, in
our carbon farming work.

00:20:52.240 --> 00:20:53.980
There is an agriculture
extension system

00:20:53.980 --> 00:20:56.980
for pest counting and
pest management, which

00:20:56.980 --> 00:20:59.575
we can improve or
replace through AI

00:20:59.575 --> 00:21:01.870
that will actually
solve the problem.

00:21:01.870 --> 00:21:03.848
Then we'll feed into the system.

00:21:03.848 --> 00:21:05.890
And the only way to do
that is to look for places

00:21:05.890 --> 00:21:07.690
where there is a system
approach that you

00:21:07.690 --> 00:21:08.840
can leverage and so on.

00:21:08.840 --> 00:21:11.475
So that's the biggest challenge.

00:21:11.475 --> 00:21:12.850
NADA MALOU: I
think we're sharing

00:21:12.850 --> 00:21:17.070
all the same challenges, at
least for the two first ones.

00:21:17.070 --> 00:21:23.380
So for us, yes, the AI part
is one of the challenges.

00:21:23.380 --> 00:21:27.010
The quality and the data is
also one of the challenges

00:21:27.010 --> 00:21:31.390
because we want to identify
resistance in bacteria.

00:21:31.390 --> 00:21:35.830
So we need to cover the maximum
patterns of resistance, which

00:21:35.830 --> 00:21:37.750
means that we need
to have access

00:21:37.750 --> 00:21:43.180
to a huge library of
resistance pictures.

00:21:43.180 --> 00:21:45.440
And we need to be
always updated.

00:21:45.440 --> 00:21:48.700
Because if bacteria
decide to mutate tomorrow

00:21:48.700 --> 00:21:51.640
and to appear
resistant in a new way,

00:21:51.640 --> 00:21:54.820
we need to have the
possibility to cover this.

00:21:54.820 --> 00:22:00.180
And finally, for diagnostic
and medical care in general,

00:22:00.180 --> 00:22:02.440
I think there is this
debate about, be careful,

00:22:02.440 --> 00:22:08.560
AI is going to replace doctors,
nurses, and lab technicians.

00:22:08.560 --> 00:22:11.530
I think it's very important
to say that it facilitates,

00:22:11.530 --> 00:22:15.190
but it will never replace a
clinician, or lab technician,

00:22:15.190 --> 00:22:16.570
or a microbiologist.

00:22:16.570 --> 00:22:19.840
This is just going to help
us to gain also some time

00:22:19.840 --> 00:22:22.420
in order to train and
to build the capacity

00:22:22.420 --> 00:22:24.490
of our human resources
in the field.

00:22:24.490 --> 00:22:26.000
Because the problem is today.

00:22:26.000 --> 00:22:28.060
And we need to find
a solution for today.

00:22:28.060 --> 00:22:30.010
So in terms of, I
think, communication

00:22:30.010 --> 00:22:32.560
and the way that we
present the tool,

00:22:32.560 --> 00:22:37.090
we should be very careful
that the medical professionals

00:22:37.090 --> 00:22:40.265
understand that they are
not going to be replaced.

00:22:40.265 --> 00:22:41.890
JACQUELLINE FULLER:
Let's talk a moment

00:22:41.890 --> 00:22:43.730
about the power of partnership.

00:22:43.730 --> 00:22:46.390
So I think we're all familiar
with the African proverb,

00:22:46.390 --> 00:22:49.700
if you want to go fast, go
alone, if you want to go far,

00:22:49.700 --> 00:22:50.920
go together.

00:22:50.920 --> 00:22:55.430
But going together
can be complicated.

00:22:55.430 --> 00:22:57.070
But we know that
none of these kind

00:22:57.070 --> 00:23:01.390
of global, audacious,
bold, innovative solutions

00:23:01.390 --> 00:23:04.030
are going to be able to
be accomplished by one

00:23:04.030 --> 00:23:05.710
small organization alone.

00:23:05.710 --> 00:23:08.500
So maybe some of your
experiences or thoughts

00:23:08.500 --> 00:23:11.650
that you've had as you
sought out the right partners

00:23:11.650 --> 00:23:13.570
and built that collaboration?

00:23:18.140 --> 00:23:18.930
ANANDAN: OK.

00:23:18.930 --> 00:23:21.170
So I'll give you a couple
of different examples.

00:23:21.170 --> 00:23:23.570
In the front-line
health case, we

00:23:23.570 --> 00:23:26.270
talked to a bunch of front-line
health organizations,

00:23:26.270 --> 00:23:30.050
and felt like triaging by
front-line workers at home care

00:23:30.050 --> 00:23:31.078
might be a good problem.

00:23:31.078 --> 00:23:32.870
And then we talked to
the Gates Foundation.

00:23:32.870 --> 00:23:34.910
They organized a
couple of workshops,

00:23:34.910 --> 00:23:39.050
inviting about 30 or 40 of
their grantees to day workshops.

00:23:39.050 --> 00:23:41.280
We learned a lot, and
two things happened.

00:23:41.280 --> 00:23:44.000
One is that these
organizations helped

00:23:44.000 --> 00:23:47.060
us realize that certain problems
in maternal and child care,

00:23:47.060 --> 00:23:51.420
like infant mortality, actually
were more urgent and doable.

00:23:51.420 --> 00:23:52.640
So we changed our mind.

00:23:52.640 --> 00:23:55.820
But moreover in that process,
they signed up to work with us.

00:23:55.820 --> 00:23:57.320
They said, you know,
if you do this,

00:23:57.320 --> 00:23:59.000
we'll actually be interested.

00:23:59.000 --> 00:24:01.460
On the other problem
that we're working on,

00:24:01.460 --> 00:24:03.800
tuberculosis
eradication, in India

00:24:03.800 --> 00:24:05.990
there is a tuberculosis
care system

00:24:05.990 --> 00:24:08.570
managed by something called
the Central TB Division.

00:24:08.570 --> 00:24:10.670
Anything you do, it's
better you work with them,

00:24:10.670 --> 00:24:13.480
because the only real TB
data available is theirs.

00:24:13.480 --> 00:24:14.570
I mean, they own it.

00:24:14.570 --> 00:24:17.270
So we have become an AI partner
for the Central TB Division,

00:24:17.270 --> 00:24:20.810
but that also means being
part of their discussions

00:24:20.810 --> 00:24:22.290
and planning and so on.

00:24:22.290 --> 00:24:24.500
But it's very valuable
because it tells us where

00:24:24.500 --> 00:24:25.940
we can actually make progress.

00:24:25.940 --> 00:24:28.850
In cotton farming, the project
for which we are funded,

00:24:28.850 --> 00:24:31.180
there is actually an
agriculture extension program

00:24:31.180 --> 00:24:32.570
that the state government has.

00:24:32.570 --> 00:24:35.820
So we start with them, and they
introduced us to that system.

00:24:35.820 --> 00:24:38.253
But then we also found that
there is a private sector,

00:24:38.253 --> 00:24:39.920
not-for-profit, kind
of an organization,

00:24:39.920 --> 00:24:43.160
called Better Cotton
Initiative, that can actually

00:24:43.160 --> 00:24:45.620
be an early adopter
of the solution

00:24:45.620 --> 00:24:47.990
and allow us to file
it and try ourself.

00:24:47.990 --> 00:24:51.110
So we simultaneously partnered
with these organizations.

00:24:51.110 --> 00:24:53.840
And in each case,
once they agree,

00:24:53.840 --> 00:24:56.420
they take you to the problem
domain, take you to the people.

00:24:56.420 --> 00:24:58.340
Data collection becomes
easier because they

00:24:58.340 --> 00:24:59.390
will do it for you.

00:24:59.390 --> 00:25:02.450
They'll give you the
opening to do it.

00:25:02.450 --> 00:25:05.487
And then execution, of course.

00:25:05.487 --> 00:25:06.570
JACQUELLINE FULLER: Gavin?

00:25:06.570 --> 00:25:07.400
GAVIN MCCORMICK: Thank you.

00:25:07.400 --> 00:25:09.620
So partnerships can be hard for
a slightly awkward introvert,

00:25:09.620 --> 00:25:10.400
I have learned.

00:25:10.400 --> 00:25:12.428
But I think that it's
really interesting

00:25:12.428 --> 00:25:13.970
that in the business
sector, I think,

00:25:13.970 --> 00:25:15.000
partnerships are important.

00:25:15.000 --> 00:25:17.583
And I think for impact they're
just absolutely not negotiable.

00:25:17.583 --> 00:25:19.220
You absolutely have to do it.

00:25:19.220 --> 00:25:21.950
It has been stunning
to see the impact that

00:25:21.950 --> 00:25:24.530
has happened in the climate
movement in the last year,

00:25:24.530 --> 00:25:27.470
as a lot of NGOs have started
to say, why would we stand up

00:25:27.470 --> 00:25:29.150
separate technology stacks?

00:25:29.150 --> 00:25:31.040
Why don't we just
all collaborate?

00:25:31.040 --> 00:25:32.090
And it's been amazing.

00:25:32.090 --> 00:25:35.630
95% of companies worldwide
have a sustainability team

00:25:35.630 --> 00:25:39.623
that uses the World
Resources Institute protocol.

00:25:39.623 --> 00:25:41.540
We just called them up
and said, why don't you

00:25:41.540 --> 00:25:42.560
have all our data for free?

00:25:42.560 --> 00:25:44.030
We won't charge you,
and you'll distribute it

00:25:44.030 --> 00:25:45.655
to every company in
the world for free.

00:25:45.655 --> 00:25:46.490
You won't charge us.

00:25:46.490 --> 00:25:49.160
And then a similar deal with
RMI, 90% of renewable energy

00:25:49.160 --> 00:25:50.610
developers worldwide.

00:25:50.610 --> 00:25:52.563
So the scale and
speed of progress

00:25:52.563 --> 00:25:54.980
has just been stunning for
what a little team of 10 people

00:25:54.980 --> 00:25:57.022
can accomplish because
everyone's willing to work

00:25:57.022 --> 00:25:58.370
together in our field suddenly.

00:25:58.370 --> 00:25:59.995
JACQUELLINE FULLER:
But to do that, you

00:25:59.995 --> 00:26:02.490
have to overcome what's known
as database hugging syndrome,

00:26:02.490 --> 00:26:02.990
right?

00:26:02.990 --> 00:26:05.150
So every NGO, every
company, every entity

00:26:05.150 --> 00:26:06.740
wants to hold on to their data.

00:26:06.740 --> 00:26:09.830
But sounds like, in this case,
you are willing to share.

00:26:09.830 --> 00:26:10.430
GAVIN MCCORMICK:
And we've done it

00:26:10.430 --> 00:26:12.110
through a really interesting
legal mechanism, which

00:26:12.110 --> 00:26:13.760
I think has potential
for impact, which

00:26:13.760 --> 00:26:16.052
is that it's shared under
what we call a mission source

00:26:16.052 --> 00:26:16.760
license.

00:26:16.760 --> 00:26:18.500
So we can get access
to their IP and vice

00:26:18.500 --> 00:26:21.390
versa as long as we have the
same environmental mission.

00:26:21.390 --> 00:26:23.750
And I think that that is a
really interesting innovation

00:26:23.750 --> 00:26:26.190
that will apply to
other projects as well.

00:26:26.190 --> 00:26:28.730
JACQUELLINE FULLER: Well,
this room contains a group.

00:26:28.730 --> 00:26:31.460
This is a very self-selected
group of folks.

00:26:31.460 --> 00:26:33.300
I think technologists,
developers,

00:26:33.300 --> 00:26:36.680
people who are really interested
in thinking about applying

00:26:36.680 --> 00:26:42.770
AI to improving environment,
social issues, poverty,

00:26:42.770 --> 00:26:44.250
women's rights.

00:26:44.250 --> 00:26:46.880
So to this room,
that's filled with,

00:26:46.880 --> 00:26:53.240
I think, aspiring and inspired
pragmatic solutions folks, what

00:26:53.240 --> 00:26:56.420
would be your words
of advice or maybe

00:26:56.420 --> 00:26:58.730
some lessons learned
that you have

00:26:58.730 --> 00:27:01.340
for people who are
thinking about how

00:27:01.340 --> 00:27:02.570
they can help contribute?

00:27:02.570 --> 00:27:06.050
How they can be part
of applying technology

00:27:06.050 --> 00:27:08.240
to these huge global challenges?

00:27:10.723 --> 00:27:12.890
GAVIN MCCORMICK: I would
certainly say, collaborate,

00:27:12.890 --> 00:27:14.390
collaborate, collaborate.

00:27:14.390 --> 00:27:15.553
For two reasons.

00:27:15.553 --> 00:27:17.720
One, you might be surprised
at the low state of tech

00:27:17.720 --> 00:27:18.680
in the NGO sector.

00:27:18.680 --> 00:27:21.110
If you're not already familiar,
it's often just stunning

00:27:21.110 --> 00:27:22.670
what a really good
developer can do

00:27:22.670 --> 00:27:24.890
in four hours of volunteering.

00:27:24.890 --> 00:27:26.750
We've been really struck.

00:27:26.750 --> 00:27:29.270
And the other is, it's really
easy for well-meaning code

00:27:29.270 --> 00:27:30.838
to actually cause
harm in sectors

00:27:30.838 --> 00:27:32.630
where the issues can
be really complicated.

00:27:32.630 --> 00:27:34.088
It's so valuable
to talk to subject

00:27:34.088 --> 00:27:37.390
matter experts first about
what would really help.

00:27:37.390 --> 00:27:39.775
JACQUELLINE FULLER: That
was very clear and concise.

00:27:44.720 --> 00:27:47.220
NADA MALOU: Yeah,
collaboration is a key.

00:27:47.220 --> 00:27:51.060
And for us, for
example, if I did not

00:27:51.060 --> 00:27:55.590
have my friend, who has this
bioinformatic background

00:27:55.590 --> 00:28:02.630
to translate from a medical
need to a technical solution,

00:28:02.630 --> 00:28:04.240
we would not be here.

00:28:04.240 --> 00:28:08.550
So I think it's very important
to find a bridge between us.

00:28:08.550 --> 00:28:12.760
Because as a medical person,
I know what would be the need,

00:28:12.760 --> 00:28:15.270
but you are the one who
can make it possible

00:28:15.270 --> 00:28:18.330
and who can bring the solution.

00:28:18.330 --> 00:28:22.750
And we are doing this
every day now, a project.

00:28:22.750 --> 00:28:26.640
The developer who is working
on this application, Marco

00:28:26.640 --> 00:28:29.580
Bastucci, who is in the
room, he speaks a language

00:28:29.580 --> 00:28:31.140
that I don't speak.

00:28:31.140 --> 00:28:34.150
And to be honest, most of
the time I don't understand.

00:28:34.150 --> 00:28:35.340
And it's the same for him.

00:28:35.340 --> 00:28:37.920
He does not understand when
I speak about bacteria,

00:28:37.920 --> 00:28:39.840
infection, resistance.

00:28:39.840 --> 00:28:42.330
So we need to find
away from both sides

00:28:42.330 --> 00:28:46.440
to translate and to make the
understanding of both sides

00:28:46.440 --> 00:28:47.490
easier.

00:28:47.490 --> 00:28:51.030
So people like bioinformatic
people can be a way,

00:28:51.030 --> 00:28:55.650
but simplifying the
medical development

00:28:55.650 --> 00:29:01.648
language can be a key element
to ensure this collaboration.

00:29:01.648 --> 00:29:03.190
ANANDAN: So I used
to be a professor,

00:29:03.190 --> 00:29:06.625
so I can pontificate.

00:29:06.625 --> 00:29:08.250
I think I'll break
it into three parts.

00:29:08.250 --> 00:29:10.980
Why you should get into
AI for social good,

00:29:10.980 --> 00:29:14.980
how you should do it, and
what you should not do.

00:29:14.980 --> 00:29:18.060
See, I have an academic
background, published enough,

00:29:18.060 --> 00:29:19.710
and I worked in the industry.

00:29:19.710 --> 00:29:21.570
I know what tech
transfer takes and how

00:29:21.570 --> 00:29:23.550
it is to work in a
very disciplined,

00:29:23.550 --> 00:29:25.480
structured environment.

00:29:25.480 --> 00:29:29.850
However, if you want to
completely create and work

00:29:29.850 --> 00:29:32.490
on something that is
kind of completely new,

00:29:32.490 --> 00:29:33.592
you need that space.

00:29:33.592 --> 00:29:35.550
So you go to the place
where there is the least

00:29:35.550 --> 00:29:37.120
amount of people working on it.

00:29:37.120 --> 00:29:39.510
And actually, building
technology solutions

00:29:39.510 --> 00:29:42.330
for the social domain,
it's completely empty.

00:29:42.330 --> 00:29:46.030
There is not enough people
like you out there doing it.

00:29:46.030 --> 00:29:48.587
And in fact, one of the
things that we want to do

00:29:48.587 --> 00:29:50.670
is to attract more people
to come and collaborate.

00:29:50.670 --> 00:29:52.350
You don't have to
leave your job,

00:29:52.350 --> 00:29:53.950
but you can find a
way to contribute.

00:29:53.950 --> 00:29:57.980
But the opportunity to shape
the direction is very big,

00:29:57.980 --> 00:29:59.760
and you don't have
to follow what

00:29:59.760 --> 00:30:01.980
is the next greatest
breakthrough in deep learning

00:30:01.980 --> 00:30:02.730
and do that.

00:30:02.730 --> 00:30:04.272
You can just solve
some other problem

00:30:04.272 --> 00:30:05.320
that you define yourself.

00:30:05.320 --> 00:30:07.445
So to me, it's the most
rewarding thing you can do.

00:30:07.445 --> 00:30:09.220
And, you know, if
you do it right,

00:30:09.220 --> 00:30:13.050
you can begin to see it having
tangible impact on people

00:30:13.050 --> 00:30:15.030
that you care about,
lives around you.

00:30:15.030 --> 00:30:15.990
How to do it?

00:30:15.990 --> 00:30:19.020
You have to do it fundamentally,
with a collaborative mind

00:30:19.020 --> 00:30:22.807
like Nada was saying,
which is like, you

00:30:22.807 --> 00:30:24.390
can't think of the
end-to-end problem,

00:30:24.390 --> 00:30:25.870
you don't know
how to execute it.

00:30:25.870 --> 00:30:28.980
So you need to be very open,
and to listen to and take

00:30:28.980 --> 00:30:31.260
some risks on
collaborating with people.

00:30:31.260 --> 00:30:34.500
And change your course if
you need to, dynamically.

00:30:34.500 --> 00:30:37.050
It has to be very
agile, but it also

00:30:37.050 --> 00:30:39.480
has to be something where
you know what you can do

00:30:39.480 --> 00:30:41.200
but work with other people.

00:30:41.200 --> 00:30:43.960
And what you should not do--
and this is kind of important.

00:30:43.960 --> 00:30:47.250
It's not about advancing the
state of AI or science, which

00:30:47.250 --> 00:30:49.110
is a fantastic thing to do.

00:30:49.110 --> 00:30:52.430
If your solution cannot
be implemented and scaled,

00:30:52.430 --> 00:30:53.430
it's not worth anything.

00:30:53.430 --> 00:30:55.300
It can be the greatest
piece of technology.

00:30:55.300 --> 00:30:58.200
So you have to look at
programs and systems that

00:30:58.200 --> 00:30:59.100
can help you scale.

00:30:59.100 --> 00:31:01.927
I mean, some day we hope we
will change the system itself.

00:31:01.927 --> 00:31:03.510
But you are going
to do that only when

00:31:03.510 --> 00:31:04.940
you learn how the system works.

00:31:04.940 --> 00:31:07.530
And you can't sort of
start doing something,

00:31:07.530 --> 00:31:10.360
and throw it out and see
that it will actually work.

00:31:10.360 --> 00:31:11.100
JACQUELLINE FULLER: So
I'm going to open it up

00:31:11.100 --> 00:31:13.270
to questions in the
audience in just a minute.

00:31:13.270 --> 00:31:15.420
So you can go to the
mics here if you have

00:31:15.420 --> 00:31:17.160
questions for the panelists.

00:31:17.160 --> 00:31:20.860
But this has been
incredibly inspiring.

00:31:20.860 --> 00:31:23.392
I think reading over
these applications

00:31:23.392 --> 00:31:25.350
is one of the hardest
things I've actually ever

00:31:25.350 --> 00:31:27.930
had to do in my
12 years at Google

00:31:27.930 --> 00:31:31.800
because there were so many
amazing ideas and teams.

00:31:31.800 --> 00:31:34.020
But you all really stood out.

00:31:34.020 --> 00:31:37.020
Just wondering if you have
any final words that you

00:31:37.020 --> 00:31:42.148
would like to say to this
audience, these technologists.

00:31:42.148 --> 00:31:44.190
GAVIN MCCORMICK: I think
it's much easier to jump

00:31:44.190 --> 00:31:45.720
in to impact than I realized.

00:31:45.720 --> 00:31:48.383
My organization was only
created because I showed up

00:31:48.383 --> 00:31:49.800
at a hackathon and
some developers

00:31:49.800 --> 00:31:50.820
told me what was possible.

00:31:50.820 --> 00:31:52.380
And we built something
on that day of.

00:31:52.380 --> 00:31:53.280
JACQUELLINE FULLER:
See, good things

00:31:53.280 --> 00:31:53.960
can come out of hackathons.

00:31:53.960 --> 00:31:54.480
GAVIN MCCORMICK: One day.

00:31:54.480 --> 00:31:56.105
JACQUELLINE FULLER:
We have our doubts.

00:31:58.690 --> 00:32:03.080
NADA MALOU: So for
me, it's in the title.

00:32:03.080 --> 00:32:04.340
It's the impact.

00:32:04.340 --> 00:32:08.110
I think it's very important
to visualize the impact

00:32:08.110 --> 00:32:10.930
and how it's going to be used.

00:32:10.930 --> 00:32:14.110
And at least for
medical questions,

00:32:14.110 --> 00:32:17.110
I think it's a new
era with the AI.

00:32:17.110 --> 00:32:20.780
But the solution,
we should in advance

00:32:20.780 --> 00:32:26.800
think about once it is ready,
how this is going to be used

00:32:26.800 --> 00:32:29.180
and what is going
to be the impact.

00:32:29.180 --> 00:32:33.070
And for me, it's an advice, but
also the biggest challenge when

00:32:33.070 --> 00:32:35.830
you think about the project.

00:32:35.830 --> 00:32:37.420
ANANDAN: I once
had the privilege

00:32:37.420 --> 00:32:40.840
of having Jim Kajiya, who's
a very well-known graphics

00:32:40.840 --> 00:32:43.180
researcher, a pioneer,
as my direct manager.

00:32:43.180 --> 00:32:46.080
And we would have these
one-on-one's periodically.

00:32:46.080 --> 00:32:49.240
And at the end of it, he would
say, OK, go change the world.

00:32:49.240 --> 00:32:51.430
That's basically the best thing.

00:32:51.430 --> 00:32:53.740
It doesn't matter how you
do it, where you do it.

00:32:53.740 --> 00:32:57.580
Once you get that as your
goal, you'll have fun.

00:32:57.580 --> 00:33:01.830
[MUSIC PLAYING]

