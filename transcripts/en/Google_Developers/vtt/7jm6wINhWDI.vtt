WEBVTT
Kind: captions
Language: en

00:00:03.182 --> 00:00:04.140
WESTON HUTCHINS: Hello?

00:00:04.140 --> 00:00:05.990
Hey, everyone.

00:00:05.990 --> 00:00:07.490
I was really hoping
we were going

00:00:07.490 --> 00:00:09.280
to run into Justin
Bieber soundtrack.

00:00:09.280 --> 00:00:11.180
They had some great
music playing.

00:00:11.180 --> 00:00:12.270
I was hoping we were going
to break through a wall

00:00:12.270 --> 00:00:13.394
or something, but you know.

00:00:13.394 --> 00:00:15.400
MANDY WAITE: Yeah,
keep the music playing.

00:00:15.400 --> 00:00:16.932
I was enjoying that.

00:00:16.932 --> 00:00:18.390
WESTON HUTCHINS:
Welcome, everyone.

00:00:18.390 --> 00:00:22.234
We have the dubious 1:00 PM
after lunch hour session.

00:00:22.234 --> 00:00:23.650
We hope that you're
going to be as

00:00:23.650 --> 00:00:25.740
excited for this
presentation as we are.

00:00:25.740 --> 00:00:28.770
The title of this is, "Making
Your Cloud Service Google

00:00:28.770 --> 00:00:29.750
Fast."

00:00:29.750 --> 00:00:30.880
Some quick intros.

00:00:30.880 --> 00:00:32.030
My name is Weston Hutchins.

00:00:32.030 --> 00:00:35.190
I am a product manager on the
Google Cloud Platform Team.

00:00:35.190 --> 00:00:36.690
MANDY WAITE: My
name is Mandy Waite.

00:00:36.690 --> 00:00:38.981
I'm a developer advocate for
the Google Cloud platform.

00:00:38.981 --> 00:00:41.560
WESTON HUTCHINS: She's my
Dev rock star for today.

00:00:41.560 --> 00:00:45.570
And today's talk, it's really
hard to talk about a topic

00:00:45.570 --> 00:00:48.030
as deep as performance
in 30 minutes,

00:00:48.030 --> 00:00:50.577
so Mandy and I have cherry
picked a few tips and tricks.

00:00:50.577 --> 00:00:52.910
We're going to show you guys
some demos on top of Google

00:00:52.910 --> 00:00:56.360
Cloud Platform and how
we can make your app as

00:00:56.360 --> 00:00:58.470
fast as possible leveraging
the infrastructure we

00:00:58.470 --> 00:00:59.810
use at Google.

00:00:59.810 --> 00:01:02.054
So we've broken down
this into four topics.

00:01:02.054 --> 00:01:03.470
The first is I'm
going to tell you

00:01:03.470 --> 00:01:05.970
a little bit about
why fast matters.

00:01:05.970 --> 00:01:08.770
Then Mandy's going to take over
and tell you how we think about

00:01:08.770 --> 00:01:11.740
fast inside Google, how we
leverage the infrastructure

00:01:11.740 --> 00:01:13.954
to build our world
class apps today.

00:01:13.954 --> 00:01:15.370
Then the meat of
the discussion is

00:01:15.370 --> 00:01:17.780
going to be around two topics.

00:01:17.780 --> 00:01:19.667
One of them is measuring
fast, and how you

00:01:19.667 --> 00:01:21.250
should think about
correctly measuring

00:01:21.250 --> 00:01:22.934
the performance of
your application.

00:01:22.934 --> 00:01:25.350
And then Mandy is going to
take over and tell you guys how

00:01:25.350 --> 00:01:29.770
to scale out and scale fast so
you guys can be the next secret

00:01:29.770 --> 00:01:33.640
app that has 1,000x growth in
your first month of use once

00:01:33.640 --> 00:01:37.080
you reach the front
page of Hacker News.

00:01:37.080 --> 00:01:38.550
So why fast matters.

00:01:38.550 --> 00:01:41.070
You might be thinking,
duh, of course it matters,

00:01:41.070 --> 00:01:43.560
but I always like to throw
out some fun facts just

00:01:43.560 --> 00:01:46.980
to talk a little bit about
what the impact of performance

00:01:46.980 --> 00:01:49.237
can have on your users.

00:01:49.237 --> 00:01:51.820
There was a study done a little
while ago that Google actually

00:01:51.820 --> 00:01:56.060
went out and intentionally
induced a server side lag delay

00:01:56.060 --> 00:01:58.820
for every result that was coming
into the Google search page.

00:01:58.820 --> 00:02:00.630
And they wanted to
measure the behavior

00:02:00.630 --> 00:02:03.980
over a period of three weeks of
how often users were searching

00:02:03.980 --> 00:02:06.109
with various amounts
of lag built in.

00:02:06.109 --> 00:02:07.900
And what they found
was pretty interesting.

00:02:11.030 --> 00:02:14.820
If you just increase the amount
of lag by 400 milliseconds,

00:02:14.820 --> 00:02:18.320
it would reduce the amount
of searches by 0.6%.

00:02:18.320 --> 00:02:20.390
And when you're talking
at Google scale,

00:02:20.390 --> 00:02:21.705
that's actually a huge number.

00:02:21.705 --> 00:02:24.080
It ended up being about
eight billion searches a day

00:02:24.080 --> 00:02:26.920
at the time this study was run.

00:02:26.920 --> 00:02:28.960
Even more so, this
study also figured out

00:02:28.960 --> 00:02:31.200
that users are impatient,
and you guys probably

00:02:31.200 --> 00:02:34.090
don't need me to
remind you about that.

00:02:34.090 --> 00:02:36.150
But they found that
one in every four users

00:02:36.150 --> 00:02:37.970
will abandon a web
page if it takes

00:02:37.970 --> 00:02:39.920
more than four seconds to load.

00:02:39.920 --> 00:02:42.322
And given the competitive
nature of performance nowadays,

00:02:42.322 --> 00:02:43.780
I can only imagine
that this number

00:02:43.780 --> 00:02:46.500
is likely to decrease over time.

00:02:46.500 --> 00:02:48.511
Mobile apps are also
important as well.

00:02:48.511 --> 00:02:51.010
It's not just websites that we
have to start thinking about.

00:02:51.010 --> 00:02:53.301
There was a study that was
released just this last week

00:02:53.301 --> 00:02:56.240
by App Dynamics, which is a
leading performance monitoring

00:02:56.240 --> 00:02:57.670
solution out there.

00:02:57.670 --> 00:02:59.870
What they found when they
surveyed a bunch of users

00:02:59.870 --> 00:03:03.910
was that performance was the
number one top frustration when

00:03:03.910 --> 00:03:06.450
using mobile apps and websites.

00:03:06.450 --> 00:03:09.660
86% of users deleted
or uninstalled

00:03:09.660 --> 00:03:11.710
at least one mobile
app because of problems

00:03:11.710 --> 00:03:14.220
with application performance.

00:03:14.220 --> 00:03:15.740
Now, there is a
silver lining there.

00:03:15.740 --> 00:03:18.060
They also found
out that when users

00:03:18.060 --> 00:03:20.120
are happy with your
performance, they're

00:03:20.120 --> 00:03:21.650
going to spend more money.

00:03:21.650 --> 00:03:23.550
30% of users said they
would spend more money

00:03:23.550 --> 00:03:25.050
with an application
that had greater

00:03:25.050 --> 00:03:26.425
performance over
its competitors.

00:03:26.425 --> 00:03:28.620
So there is real incentive
here for you guys

00:03:28.620 --> 00:03:30.090
to spend the money,
spend the time

00:03:30.090 --> 00:03:32.370
speeding up your application
and making it fast

00:03:32.370 --> 00:03:33.407
for your end users.

00:03:33.407 --> 00:03:35.490
And with that, I'm going
to hand it over to Mandy,

00:03:35.490 --> 00:03:37.656
who's going to talk about
building a culture of fast

00:03:37.656 --> 00:03:38.590
at Google.

00:03:38.590 --> 00:03:40.230
MANDY WAITE: Thank you, Wes.

00:03:40.230 --> 00:03:42.880
So fast is at the core
of pretty much everything

00:03:42.880 --> 00:03:44.740
that we do at Google.

00:03:44.740 --> 00:03:46.787
Well, nearly everything.

00:03:46.787 --> 00:03:48.370
And users of our
products and services

00:03:48.370 --> 00:03:52.577
are used to experiencing a
consistently fast performance

00:03:52.577 --> 00:03:54.160
from all of our
products and services.

00:03:57.150 --> 00:03:59.320
You can offer that same
experience to your users

00:03:59.320 --> 00:04:02.060
by using Google Cloud
Platform because Google Cloud

00:04:02.060 --> 00:04:06.140
Platform runs on the very same
infrastructure as services

00:04:06.140 --> 00:04:08.680
such as Gmail, Google
search, YouTube,

00:04:08.680 --> 00:04:11.760
and all of our other
products and services.

00:04:11.760 --> 00:04:14.600
So let's talk about some of the
philosophies behind the fast

00:04:14.600 --> 00:04:16.600
at Google.

00:04:16.600 --> 00:04:20.579
In order to meet the
traffic demand of modern day

00:04:20.579 --> 00:04:23.217
applications, you need
the ability to scale fast.

00:04:23.217 --> 00:04:24.800
And scale fast is
something that we're

00:04:24.800 --> 00:04:26.466
going to talk about
in the last section,

00:04:26.466 --> 00:04:29.142
as Wes has already mentioned.

00:04:29.142 --> 00:04:32.480
And App Engine, our platform
as a service offering,

00:04:32.480 --> 00:04:34.160
which we'll also
talk about shortly,

00:04:34.160 --> 00:04:38.740
that epitomizes the notion and
the principle of scale fast.

00:04:38.740 --> 00:04:41.580
So we have an example
of that coming up soon.

00:04:41.580 --> 00:04:43.390
The ability to
measure fast allows

00:04:43.390 --> 00:04:45.560
you to gain insight
and understanding

00:04:45.560 --> 00:04:48.770
of the very complex nature of
modern day applications running

00:04:48.770 --> 00:04:50.020
in the cloud.

00:04:50.020 --> 00:04:52.490
And Wes is going to
talk to you very shortly

00:04:52.490 --> 00:04:57.030
about some specific
examples around latency,

00:04:57.030 --> 00:04:58.770
and also introduce
you to some tools

00:04:58.770 --> 00:05:01.195
that we're going to be
making available to users

00:05:01.195 --> 00:05:03.716
of the Google Cloud Platform.

00:05:03.716 --> 00:05:06.150
Then there's the ability
to innovate fast.

00:05:06.150 --> 00:05:09.570
This has really been
core to our ability

00:05:09.570 --> 00:05:11.350
to actually keep
ahead of the game when

00:05:11.350 --> 00:05:18.010
it comes to handling, as you can
pretty much imagine, handling

00:05:18.010 --> 00:05:20.780
the massive amounts
of traffic demand

00:05:20.780 --> 00:05:22.822
that we have on all of
our products and services,

00:05:22.822 --> 00:05:25.029
and not just on the products
and services themselves,

00:05:25.029 --> 00:05:27.100
but also on the
infrastructure that runs them.

00:05:27.100 --> 00:05:29.860
So we've innovated rapidly
over the last 15 years,

00:05:29.860 --> 00:05:34.080
and this is a timeline of some
of the things we've introduced,

00:05:34.080 --> 00:05:36.000
white papers and such.

00:05:36.000 --> 00:05:40.370
BigTable is our massively
scalable, no SWL data store,

00:05:40.370 --> 00:05:45.240
and many of our services
use BigTable directly.

00:05:45.240 --> 00:05:49.009
And in fact, App Engine itself
was built around BigTable.

00:05:49.009 --> 00:05:50.550
It was actually
built around BigTable

00:05:50.550 --> 00:05:52.100
to use it specifically.

00:05:52.100 --> 00:05:57.330
So users of App Engine had
direct access to BigTable

00:05:57.330 --> 00:06:01.596
as the App Engine data store.

00:06:01.596 --> 00:06:02.970
That data store
has now been made

00:06:02.970 --> 00:06:04.870
available to users
of the cloud platform

00:06:04.870 --> 00:06:07.280
directly, from anywhere,
any user from anywhere

00:06:07.280 --> 00:06:10.447
from any application, as
the Google Cloud Data Store.

00:06:10.447 --> 00:06:11.280
Then there's Dremel.

00:06:11.280 --> 00:06:13.500
Dremel is an analysis
tool that allows

00:06:13.500 --> 00:06:17.300
you to run interactive queries
across massive data sets.

00:06:17.300 --> 00:06:19.870
We're talking about terabytes
here, billions and billions

00:06:19.870 --> 00:06:20.900
of rows of data.

00:06:20.900 --> 00:06:23.520
And to get results back in
seconds, not minutes or hours,

00:06:23.520 --> 00:06:26.390
as you would do
with a Map Reduce.

00:06:26.390 --> 00:06:30.210
Dremel is available externally
by the Google Cloud Platform

00:06:30.210 --> 00:06:32.584
as a BigQuery service.

00:06:32.584 --> 00:06:33.500
Then there's Colossus.

00:06:33.500 --> 00:06:36.380
Colossus is our second
generation file system,

00:06:36.380 --> 00:06:39.490
and many of our data services
are built on top of Colossus.

00:06:39.490 --> 00:06:42.480
Colossus has been made
available to Cloud Platform

00:06:42.480 --> 00:06:46.200
users for many years now
as Google Cloud Storage.

00:06:46.200 --> 00:06:49.170
Then there's innovation at
the infrastructure layer,

00:06:49.170 --> 00:06:52.090
and we're going to talk
about Andromeda shortly.

00:06:52.090 --> 00:06:53.930
So we have scale fast,
we have measure fast,

00:06:53.930 --> 00:06:55.950
and we have the key
of innovate fast.

00:06:55.950 --> 00:06:59.180
All of these things make to a
wonderful Google Cloud Platform

00:06:59.180 --> 00:07:01.380
experience when you use it.

00:07:01.380 --> 00:07:04.250
Not only do you get all of
these when you use Google Cloud

00:07:04.250 --> 00:07:07.720
Platform, but you also
get fast connectivity

00:07:07.720 --> 00:07:10.680
to the rest of Google
at no extra cost.

00:07:10.680 --> 00:07:15.370
So if you want to consume Google
search results, YouTube videos,

00:07:15.370 --> 00:07:20.140
maps data, finance data, you
can do so at no additional cost.

00:07:20.140 --> 00:07:21.875
If you were using
another cloud provider,

00:07:21.875 --> 00:07:24.125
you would probably have to
pay some additional charges

00:07:24.125 --> 00:07:25.980
in order to get that
data into the cloud.

00:07:28.842 --> 00:07:33.870
So I want to talk about how
some of the changes that we've

00:07:33.870 --> 00:07:36.450
made to our infrastructure will
benefit Cloud Platform users,

00:07:36.450 --> 00:07:40.200
and I'm going to do this
in the context of a feature

00:07:40.200 --> 00:07:42.570
of our network infrastructure,
a network virtualization

00:07:42.570 --> 00:07:44.175
that I called Andromeda.

00:07:44.175 --> 00:07:48.930
And Andromeda provides all
of our virtualization needs.

00:07:48.930 --> 00:07:53.720
So Andromeda is our
SDN-based, Software Defined

00:07:53.720 --> 00:07:57.060
Networking-based network
backbone infrastructure.

00:07:57.060 --> 00:07:59.050
It provides many
benefits to us in terms

00:07:59.050 --> 00:08:02.650
of quality of service, fault
tolerance, and lower latency.

00:08:02.650 --> 00:08:05.590
But it also provides all of
our virtualization services,

00:08:05.590 --> 00:08:08.370
and that shows up in the form
of virtual networks for Google

00:08:08.370 --> 00:08:10.220
Cloud Platform users.

00:08:10.220 --> 00:08:13.130
Now, Andromeda was not built
as a Google Cloud Platform

00:08:13.130 --> 00:08:14.160
product.

00:08:14.160 --> 00:08:15.730
It was built
specifically to address

00:08:15.730 --> 00:08:18.440
the needs of our own
internal infrastructure.

00:08:18.440 --> 00:08:20.210
But because Google
Cloud Platform

00:08:20.210 --> 00:08:21.676
runs on that same
infrastructure,

00:08:21.676 --> 00:08:23.050
it gets the same
benefits when it

00:08:23.050 --> 00:08:26.260
comes to high performance,
security, isolation,

00:08:26.260 --> 00:08:28.590
and availability.

00:08:28.590 --> 00:08:31.650
So we built Andromeda
really to address the ever

00:08:31.650 --> 00:08:34.309
increasing demands of moving
traffic between storage

00:08:34.309 --> 00:08:36.909
and compute, something
that traditional network

00:08:36.909 --> 00:08:41.334
infrastructure really
wasn't able to keep up with.

00:08:41.334 --> 00:08:43.000
So as you can imagine,
Andromeda is also

00:08:43.000 --> 00:08:45.180
going to give us
better throughput.

00:08:45.180 --> 00:08:48.010
So we have this graph here,
probably a bit of an eye chart

00:08:48.010 --> 00:08:50.240
for some of you guys.

00:08:50.240 --> 00:08:52.115
On the y-axis, we have
relative performance,

00:08:52.115 --> 00:08:53.925
where higher is better.

00:08:53.925 --> 00:08:57.840
And on the bottom, we
have compute engine

00:08:57.840 --> 00:08:59.770
virtual machine instance times.

00:08:59.770 --> 00:09:02.520
Standard is the standard
memory configuration,

00:09:02.520 --> 00:09:04.830
and we have one, two,
four, and eight cores.

00:09:04.830 --> 00:09:07.230
So these are bigger
machines as we go along

00:09:07.230 --> 00:09:10.090
to the right of the x-axis.

00:09:10.090 --> 00:09:12.860
The blue bars represent
the average performance,

00:09:12.860 --> 00:09:15.580
the average
throughput, for what we

00:09:15.580 --> 00:09:18.510
had before Andromeda, the
non-Andromeda average.

00:09:18.510 --> 00:09:20.510
As you can see, this is
fairly static across all

00:09:20.510 --> 00:09:22.250
those machines.

00:09:22.250 --> 00:09:24.080
The red bar represents
Andromeda average,

00:09:24.080 --> 00:09:25.830
the average we see
with Andromeda.

00:09:25.830 --> 00:09:28.750
As you can see, with the
smaller instance, the one core

00:09:28.750 --> 00:09:33.230
instance, we get almost double,
2x throughput performance

00:09:33.230 --> 00:09:35.282
on that particular time.

00:09:35.282 --> 00:09:36.990
When it comes to the
eight core instance,

00:09:36.990 --> 00:09:43.100
we get roughly 4x improvement in
throughput on single stream TCP

00:09:43.100 --> 00:09:44.810
connections.

00:09:44.810 --> 00:09:48.690
So this makes these larger
instances perfectly suited

00:09:48.690 --> 00:09:50.920
for network IO
intensive workloads,

00:09:50.920 --> 00:09:55.050
such as maybe Hadoop, HPL,
any kind of application

00:09:55.050 --> 00:09:57.105
that is extremely
sensitive to latency.

00:10:01.140 --> 00:10:03.100
Just to mention
as well, Andromeda

00:10:03.100 --> 00:10:04.790
is our network infrastructure.

00:10:04.790 --> 00:10:06.470
It's available in
all of our zones

00:10:06.470 --> 00:10:08.475
on Compute Engine is
all of our regions.

00:10:11.930 --> 00:10:14.097
And on the subject
of latency-- it

00:10:14.097 --> 00:10:15.763
was going to be the
subject of latency--

00:10:15.763 --> 00:10:17.340
I'm going to hand
you over to Wes,

00:10:17.340 --> 00:10:19.575
who's going to talk to
you about measuring fast.

00:10:19.575 --> 00:10:21.459
WESTON HUTCHINS: Thanks, Mandy.

00:10:21.459 --> 00:10:24.000
So I wanted to spend a little
bit of time talking around what

00:10:24.000 --> 00:10:25.458
it means to measure
an application,

00:10:25.458 --> 00:10:27.410
because you guys can
make all the performance

00:10:27.410 --> 00:10:28.570
improvements in the
world, but if you

00:10:28.570 --> 00:10:30.760
don't know how they're
impacting your application,

00:10:30.760 --> 00:10:33.240
you may be doing something
bad under the covers

00:10:33.240 --> 00:10:35.650
that you need to know about.

00:10:35.650 --> 00:10:41.114
So normally, when I ask a web
developer, I say, hey John,

00:10:41.114 --> 00:10:42.530
can you go tell
me the performance

00:10:42.530 --> 00:10:45.250
of this particular web
page or application home

00:10:45.250 --> 00:10:46.995
page loading screen
that's coming up?

00:10:46.995 --> 00:10:49.120
The first number they almost
always come back with,

00:10:49.120 --> 00:10:50.661
which is the easiest
thing to report,

00:10:50.661 --> 00:10:52.250
is average response time.

00:10:52.250 --> 00:10:55.250
A lot of monitoring
solutions report this to you.

00:10:55.250 --> 00:10:58.060
But the thing I want to point
out is that this may look good.

00:10:58.060 --> 00:11:00.920
200 milliseconds may
not seem that bad,

00:11:00.920 --> 00:11:02.470
but that's only average.

00:11:02.470 --> 00:11:05.300
How do I know if 10%,
20% of my customers

00:11:05.300 --> 00:11:08.010
are seeing loading times of
greater than four seconds,

00:11:08.010 --> 00:11:11.310
and then there's a lot of people
seeing really fast requests?

00:11:11.310 --> 00:11:14.920
And average response
time is misleading.

00:11:14.920 --> 00:11:16.730
The real point I wanted
to try to make here

00:11:16.730 --> 00:11:19.960
is that anytime you
report an average,

00:11:19.960 --> 00:11:22.380
it works well when
you have a model that

00:11:22.380 --> 00:11:24.069
follows a normal
distribution curve.

00:11:24.069 --> 00:11:26.360
When you see things where
it's like a few requests that

00:11:26.360 --> 00:11:28.740
are very fast, most of them
in the middle, and then a few

00:11:28.740 --> 00:11:30.700
tailing off at the
end, but the problem

00:11:30.700 --> 00:11:33.440
is that most applications
don't behave like this.

00:11:33.440 --> 00:11:37.550
In fact, this is actually a more
typical agency analysis curve

00:11:37.550 --> 00:11:39.630
that you would see as
part of your web app

00:11:39.630 --> 00:11:41.640
or mobile application.

00:11:41.640 --> 00:11:43.830
And the reason is this
behaves a little bit more

00:11:43.830 --> 00:11:46.329
like a step function, and it
actually makes a lot more sense

00:11:46.329 --> 00:11:49.760
when you think about how an
app is architected and built.

00:11:49.760 --> 00:11:51.380
At the very beginning
of the curve,

00:11:51.380 --> 00:11:52.970
you see most of the requests.

00:11:52.970 --> 00:11:55.780
This is a good thing, and this
is because most modern web

00:11:55.780 --> 00:11:58.510
applications and apps
have some form of caching.

00:11:58.510 --> 00:12:00.150
So users go and hit
your web server,

00:12:00.150 --> 00:12:02.470
the cache serves the request,
gets the data locally,

00:12:02.470 --> 00:12:05.220
and is able to send it back to
the customer really quickly.

00:12:05.220 --> 00:12:08.000
But then what happens when
you have a cache miss?

00:12:08.000 --> 00:12:10.510
Well, that's when things
take a little bit longer.

00:12:10.510 --> 00:12:13.372
And so if most of the users are
hitting the cache, only a few

00:12:13.372 --> 00:12:14.830
of them are missing
it, and this is

00:12:14.830 --> 00:12:16.871
where you start to see
the decrease in the number

00:12:16.871 --> 00:12:19.140
of people that experience
longer latency.

00:12:19.140 --> 00:12:20.890
So as I said, it behaves
a little bit more

00:12:20.890 --> 00:12:22.950
like a step function.

00:12:22.950 --> 00:12:25.510
Finally, at the end,
you start seeing things

00:12:25.510 --> 00:12:27.810
that we call the long tail.

00:12:27.810 --> 00:12:30.400
This is where you need to
start really paying attention

00:12:30.400 --> 00:12:32.120
to some of the stuff
that's going on.

00:12:32.120 --> 00:12:34.660
This is the problematic
areas that you may see,

00:12:34.660 --> 00:12:36.430
specifically this guy.

00:12:36.430 --> 00:12:38.950
If this bump right
here starts to spike,

00:12:38.950 --> 00:12:41.010
it's usually indicative
of a problem.

00:12:41.010 --> 00:12:43.760
It could be your servers
are overloaded with traffic

00:12:43.760 --> 00:12:45.160
and they're starting to fail.

00:12:45.160 --> 00:12:47.711
It could be a third party
API that's timing out.

00:12:47.711 --> 00:12:49.460
It really could just
be a programming bug.

00:12:49.460 --> 00:12:53.970
We see a lot of examples where
some user comments end up

00:12:53.970 --> 00:12:56.420
scaling out and causing
really slow response

00:12:56.420 --> 00:12:58.810
times for a number of users.

00:12:58.810 --> 00:13:01.270
Oftentimes, it could be a
variety of these things,

00:13:01.270 --> 00:13:04.710
but you really want to start
paying attention to the tailer.

00:13:04.710 --> 00:13:06.710
Googler Jeff Dean
says this best.

00:13:06.710 --> 00:13:08.490
He goes, "Even if your
application appears

00:13:08.490 --> 00:13:11.446
to be performing fast,
always be aware of the end

00:13:11.446 --> 00:13:12.560
to end spectrum.

00:13:12.560 --> 00:13:13.780
Don't just look at average.

00:13:13.780 --> 00:13:15.570
Don't even look at
50th percentile.

00:13:15.570 --> 00:13:19.750
Make sure that your long latency
tails are performing well."

00:13:19.750 --> 00:13:22.710
So if averages aren't the
answer, what is the answer?

00:13:22.710 --> 00:13:24.720
Typically, when we
talk about performance,

00:13:24.720 --> 00:13:28.200
I love it when people
tell me percentiles

00:13:28.200 --> 00:13:29.640
rather than averages.

00:13:29.640 --> 00:13:32.040
And to give a really
quick, simple example,

00:13:32.040 --> 00:13:34.850
imagine my application got
10 requests that were served.

00:13:34.850 --> 00:13:37.600
Notice that the average of
this is 102 milliseconds,

00:13:37.600 --> 00:13:39.500
but it actually
doesn't say anything

00:13:39.500 --> 00:13:42.190
about how many users are
seeing average response

00:13:42.190 --> 00:13:44.810
times of 102 milliseconds.

00:13:44.810 --> 00:13:47.210
And if I go and take the
50th percentile of this,

00:13:47.210 --> 00:13:50.100
I can then say, oh,
half of my customers

00:13:50.100 --> 00:13:52.880
are experiencing latency
50 milliseconds or greater.

00:13:52.880 --> 00:13:54.610
And this gives me
much more confidence

00:13:54.610 --> 00:13:58.250
in the range of users that are
seeing good and bad performance

00:13:58.250 --> 00:14:00.670
on my application.

00:14:00.670 --> 00:14:02.090
So quiz time.

00:14:02.090 --> 00:14:04.730
If I was to show you guys
a quick little graph of two

00:14:04.730 --> 00:14:08.640
applications that have a latency
curve that looks like this,

00:14:08.640 --> 00:14:11.590
which do you think is better?

00:14:11.590 --> 00:14:14.059
Here two, here one, here
a little bit of both.

00:14:14.059 --> 00:14:15.600
You're probably
sitting there saying,

00:14:15.600 --> 00:14:17.850
you just told us that averages
aren't the right thing,

00:14:17.850 --> 00:14:21.330
so of course, the answer is
it depends, and that's true.

00:14:21.330 --> 00:14:23.880
In this particular example,
this is actually real world data

00:14:23.880 --> 00:14:26.296
that we took from two apps
that we were running internally

00:14:26.296 --> 00:14:26.800
at Google.

00:14:26.800 --> 00:14:29.550
And we found that even
though the average of app two

00:14:29.550 --> 00:14:33.050
was a little slower, the 95th
percentile was further in.

00:14:33.050 --> 00:14:35.880
So we got really reliable,
predictable, consistent

00:14:35.880 --> 00:14:40.810
performance, and it was under
that four second magic time.

00:14:40.810 --> 00:14:42.750
App one had some
attrition rate from people

00:14:42.750 --> 00:14:44.850
that didn't want to wait
for these long requests.

00:14:44.850 --> 00:14:47.120
So really, the whole
point of this slide

00:14:47.120 --> 00:14:50.210
is to start thinking about
the range of performance

00:14:50.210 --> 00:14:52.240
across your
application, and also

00:14:52.240 --> 00:14:56.180
realize that good
performance engineers to try

00:14:56.180 --> 00:14:57.480
to bring forward the bars.

00:14:57.480 --> 00:14:59.490
They try to move these
bars a little bit left

00:14:59.490 --> 00:15:00.990
in terms of the
percentile graphs.

00:15:00.990 --> 00:15:04.480
But any change that you make
can have an adverse affect

00:15:04.480 --> 00:15:07.280
on the other percentiles,
so always keep that in mind.

00:15:07.280 --> 00:15:10.040
You might make something better
for the 50th percentile which

00:15:10.040 --> 00:15:11.792
will impact the 95th percentile.

00:15:11.792 --> 00:15:13.500
And those are just
performance trade offs

00:15:13.500 --> 00:15:15.150
that you're going to
have to figure out

00:15:15.150 --> 00:15:18.850
what the right solution
is for your application.

00:15:18.850 --> 00:15:21.830
So I'm going to come over
here and give you guys

00:15:21.830 --> 00:15:23.050
a quick little demo.

00:15:23.050 --> 00:15:25.690
You saw some of this
in the keynote today,

00:15:25.690 --> 00:15:28.800
and I just wanted to show you
in a little bit more depth what

00:15:28.800 --> 00:15:30.990
was going on.

00:15:30.990 --> 00:15:34.037
We recently announced a
product called Cloud Trace,

00:15:34.037 --> 00:15:35.620
and I want to show
you how that works.

00:15:35.620 --> 00:15:38.490
This is a tool designed to help
you debug long latency tail

00:15:38.490 --> 00:15:39.600
apps.

00:15:39.600 --> 00:15:42.460
So I've loaded up
the Walk Share app,

00:15:42.460 --> 00:15:44.930
and let me exit out
of the comment here.

00:15:44.930 --> 00:15:47.367
It's a beautiful app that
says, this is where I walk.

00:15:47.367 --> 00:15:48.575
Look how peaceful that looks.

00:15:56.530 --> 00:15:58.595
Thank you, Mandy.

00:15:58.595 --> 00:16:00.720
And it allows you to go
and comment and figure out,

00:16:00.720 --> 00:16:03.330
OK, this is a nice walk
or it's not a nice walk,

00:16:03.330 --> 00:16:04.580
or I've done better than that.

00:16:04.580 --> 00:16:07.121
So I'm just going to go ahead
and add a test comment in here.

00:16:10.120 --> 00:16:15.004
When I hit Post, this is
exhibiting some pretty long

00:16:15.004 --> 00:16:15.670
characteristics.

00:16:15.670 --> 00:16:16.780
There was a delay there.

00:16:16.780 --> 00:16:17.660
It wasn't immediate.

00:16:17.660 --> 00:16:19.118
And I noticed when
I was using this

00:16:19.118 --> 00:16:22.760
that the more times I
had comments on the page,

00:16:22.760 --> 00:16:24.560
the more it was lagging.

00:16:24.560 --> 00:16:27.820
And so Cloud Trace is a tool
designed to help you figure out

00:16:27.820 --> 00:16:29.800
what's going on there.

00:16:29.800 --> 00:16:31.550
Under the Monitoring
tab, you can

00:16:31.550 --> 00:16:34.630
go into the Developers
console and click on Traces,

00:16:34.630 --> 00:16:37.190
and this is a tool that allows
you to see the requests that

00:16:37.190 --> 00:16:39.840
are coming into your
application in real time.

00:16:39.840 --> 00:16:42.420
So if I was to go
and filter to say,

00:16:42.420 --> 00:16:45.080
let's take a look at
things in the last hour,

00:16:45.080 --> 00:16:48.190
and I can even do some searches.

00:16:50.800 --> 00:16:52.550
And then I'll sort
by latency to see

00:16:52.550 --> 00:16:53.970
the slowest latency request.

00:16:53.970 --> 00:16:56.350
This was actually
the recent request

00:16:56.350 --> 00:16:58.284
that I made in an
earlier session.

00:16:58.284 --> 00:17:00.200
I'll go ahead and pull
it up because the graph

00:17:00.200 --> 00:17:01.033
should look similar.

00:17:03.690 --> 00:17:06.119
But for every single
one of these comments,

00:17:06.119 --> 00:17:09.189
the Cloud Trace tool
tells me how long

00:17:09.189 --> 00:17:10.730
this particular
request took and then

00:17:10.730 --> 00:17:13.690
gives me a list of the RPCs,
or Remote Procedure Calls,

00:17:13.690 --> 00:17:15.869
that were made
during this request.

00:17:15.869 --> 00:17:17.839
So it's really easy
for me to pull out

00:17:17.839 --> 00:17:20.960
and say, well, I can
obviously see the problem.

00:17:20.960 --> 00:17:23.490
I'm doing a bunch of
synchronous data store calls

00:17:23.490 --> 00:17:26.910
in a loop, which is causing
it to basically block

00:17:26.910 --> 00:17:28.494
on the number of
comments that I have.

00:17:28.494 --> 00:17:30.993
In fact, if I even go and click
on some of these other tabs,

00:17:30.993 --> 00:17:32.830
I can see that the
majority of my calls

00:17:32.830 --> 00:17:36.740
are all in this data
store.get method.

00:17:36.740 --> 00:17:39.060
And the fix for this
is pretty simple.

00:17:39.060 --> 00:17:44.180
I ended up pulling
up some code here.

00:17:44.180 --> 00:17:46.680
I'm not going to make the fix,
but what I wanted to call out

00:17:46.680 --> 00:17:48.250
was I noticed that everything
that I was doing here

00:17:48.250 --> 00:17:50.170
in data store was via
synchronous method.

00:17:50.170 --> 00:17:52.820
We actually offer Async
counterparts to most of these.

00:17:52.820 --> 00:17:54.450
So I fixed the
version of the app.

00:17:54.450 --> 00:17:57.300
I ended up pushing that out
to production using the Async

00:17:57.300 --> 00:18:00.472
APIs, and when I got was
a very different picture.

00:18:00.472 --> 00:18:02.430
You can notice that most
of my data store calls

00:18:02.430 --> 00:18:03.560
are now parallelized.

00:18:03.560 --> 00:18:06.340
I'm doing some puts in
addition to some gets here.

00:18:06.340 --> 00:18:08.165
This greatly sped
up the overall end

00:18:08.165 --> 00:18:10.510
to end performance
of my application.

00:18:10.510 --> 00:18:12.820
And a few more notes
on Cloud Trace.

00:18:12.820 --> 00:18:15.977
Right now, this is a product
that's in limited preview.

00:18:15.977 --> 00:18:18.060
We're going to support it
for App Engine right out

00:18:18.060 --> 00:18:18.850
of the box.

00:18:18.850 --> 00:18:20.160
Zero configuration.

00:18:20.160 --> 00:18:23.330
You turn this on, you get these
traces for you automatically.

00:18:23.330 --> 00:18:25.780
And as we start to roll
out the life cycle of this,

00:18:25.780 --> 00:18:27.400
we really want to
extend this across

00:18:27.400 --> 00:18:30.030
not just App Engine but
Compute Engine as well,

00:18:30.030 --> 00:18:32.640
and give you guys detail
analysis on what these RPC

00:18:32.640 --> 00:18:35.340
calls are doing, what code
line was calling them,

00:18:35.340 --> 00:18:38.690
and really take a deep look
into what your service is doing

00:18:38.690 --> 00:18:40.600
for every single request
that's coming in.

00:18:43.960 --> 00:18:46.260
Now, before I hand
it back to Mandy,

00:18:46.260 --> 00:18:49.750
I wanted to impart with you guys
a few little tips and tricks

00:18:49.750 --> 00:18:53.330
for how to manage
long tail latency.

00:18:53.330 --> 00:18:55.630
Tip number one.

00:18:55.630 --> 00:18:59.050
Before you do anything, look
to reduce the number of RPCs

00:18:59.050 --> 00:19:00.800
before optimizing
your algorithms.

00:19:00.800 --> 00:19:02.300
So many times, I've
talked to people

00:19:02.300 --> 00:19:04.092
where the first thing
they want to do is go

00:19:04.092 --> 00:19:05.550
and hunt through
your code, through

00:19:05.550 --> 00:19:07.000
every single in
squared algorithm,

00:19:07.000 --> 00:19:08.706
and try to clean that
up into some n log

00:19:08.706 --> 00:19:11.990
n or even fancier set of code.

00:19:11.990 --> 00:19:14.130
And that's usually
not what speeds up

00:19:14.130 --> 00:19:15.270
your application the most.

00:19:15.270 --> 00:19:17.530
The number of RPC calls
every single request

00:19:17.530 --> 00:19:19.390
makes is usually the
number one contributor

00:19:19.390 --> 00:19:20.710
to slow performance.

00:19:20.710 --> 00:19:23.755
And for this particular example,
this is a really common thing

00:19:23.755 --> 00:19:26.380
I see where people are making a
bunch of Memcache or data store

00:19:26.380 --> 00:19:29.760
calls, and they don't
realize that you can actually

00:19:29.760 --> 00:19:30.960
batch these together.

00:19:30.960 --> 00:19:33.130
Data store provides
really nice mechanisms

00:19:33.130 --> 00:19:36.030
for retrieving data
via batch operation

00:19:36.030 --> 00:19:38.440
where you're able to pass
in a set of keys as opposed

00:19:38.440 --> 00:19:39.481
to each one individually.

00:19:41.770 --> 00:19:42.500
Tip number two.

00:19:42.500 --> 00:19:46.575
Cache read only third
party API calls.

00:19:46.575 --> 00:19:49.200
The thing to remember is that if
you're making a bunch of calls

00:19:49.200 --> 00:19:53.700
off to Facebook, Twitter,
Google Maps, whatever, they have

00:19:53.700 --> 00:19:55.350
the same performance
characteristics

00:19:55.350 --> 00:19:57.550
that your application
has as well.

00:19:57.550 --> 00:20:01.310
So realize that for every 100
calls you make, 99% of them

00:20:01.310 --> 00:20:03.850
are going to be very fast,
but there is a chance

00:20:03.850 --> 00:20:06.330
that a few of them are
going to turn very slowly.

00:20:06.330 --> 00:20:07.520
They could either time
out or they could just

00:20:07.520 --> 00:20:09.436
take a long time to
return to you because they

00:20:09.436 --> 00:20:12.410
have the same latency
curves just like you do.

00:20:12.410 --> 00:20:14.940
Even worse, if you start to
load a page that chains a bunch

00:20:14.940 --> 00:20:18.190
of these together, say, like
an account details page that

00:20:18.190 --> 00:20:20.730
wants to show name, address,
billing information from all

00:20:20.730 --> 00:20:23.720
these different services,
you're increasing the percentage

00:20:23.720 --> 00:20:25.140
chance that one
of these services

00:20:25.140 --> 00:20:26.636
is going to return slow.

00:20:26.636 --> 00:20:28.010
And if your page
happens to block

00:20:28.010 --> 00:20:30.620
on showing that
information, well, then you

00:20:30.620 --> 00:20:32.360
end up in a little
bit of a problem.

00:20:32.360 --> 00:20:37.780
So the tip here, when you can,
if you have some data that

00:20:37.780 --> 00:20:41.170
is not likely to change
very often, is read only,

00:20:41.170 --> 00:20:42.270
cache that locally.

00:20:42.270 --> 00:20:44.000
Name and address are
really good examples

00:20:44.000 --> 00:20:45.680
where you don't
necessarily need to make

00:20:45.680 --> 00:20:47.360
API calls for all of that stuff.

00:20:47.360 --> 00:20:49.320
Store it locally, make
the retrieval faster,

00:20:49.320 --> 00:20:51.528
and try to reduce the number
of external dependencies

00:20:51.528 --> 00:20:54.010
your application has.

00:20:54.010 --> 00:20:55.280
And then the last one.

00:20:55.280 --> 00:20:56.820
This is sort of a
no brainer but I

00:20:56.820 --> 00:20:59.160
like to remind people
because not everyone realizes

00:20:59.160 --> 00:21:01.210
that we have
asynchronous methods

00:21:01.210 --> 00:21:05.710
for almost every single
one of our outgoing calls

00:21:05.710 --> 00:21:07.910
in App Engine.

00:21:07.910 --> 00:21:10.500
The synchronous methods
have an Async counterpart.

00:21:10.500 --> 00:21:13.520
So really, learn
the documentation.

00:21:13.520 --> 00:21:16.260
Be aggressive about
using the Async.

00:21:16.260 --> 00:21:18.370
Put in a pretty
aggressive deadline limit.

00:21:18.370 --> 00:21:20.340
You can actually
specify how long

00:21:20.340 --> 00:21:23.187
to wait before a particular
RPC call is returned.

00:21:23.187 --> 00:21:25.770
You can be a lot more aggressive
in sending out a few of those

00:21:25.770 --> 00:21:28.760
just in case one of them
happens to return slowly.

00:21:28.760 --> 00:21:31.260
And with that, Mandy is
going to talk to you guys

00:21:31.260 --> 00:21:34.330
about, once you have everything
up and running, scaling

00:21:34.330 --> 00:21:35.920
out fast so that
your application can

00:21:35.920 --> 00:21:37.345
handle a lot of load.

00:21:37.345 --> 00:21:38.905
MANDY WAITE: Thanks, Wes.

00:21:38.905 --> 00:21:41.700
So what do you do when you
have to scale for a one

00:21:41.700 --> 00:21:44.720
off global event involving one
of the biggest social media

00:21:44.720 --> 00:21:47.952
entities on the planet?

00:21:47.952 --> 00:21:49.785
How many of you have
heard of One Direction?

00:21:52.290 --> 00:21:56.380
How many of you have
heard of One Direction?

00:21:56.380 --> 00:21:57.540
There wasn't many hands up.

00:21:57.540 --> 00:21:58.620
I think most of you are liars.

00:21:58.620 --> 00:22:00.036
You've all heard
of One Direction.

00:22:00.036 --> 00:22:01.640
You just won't admit it.

00:22:01.640 --> 00:22:04.290
Back in October of last
year, we received a request

00:22:04.290 --> 00:22:07.610
from our marketing team
to assess an agency

00:22:07.610 --> 00:22:11.000
with the development of a second
screen application for what

00:22:11.000 --> 00:22:13.260
was known as One Direction Day.

00:22:13.260 --> 00:22:17.250
And One Direction Day was a
seven hour continuous YouTube

00:22:17.250 --> 00:22:20.550
live stream by the boy
band One Direction.

00:22:20.550 --> 00:22:23.200
And we were told in October.

00:22:23.200 --> 00:22:25.120
The event was on the
23rd of November.

00:22:25.120 --> 00:22:27.670
It wasn't a huge amount
of time to get this done.

00:22:27.670 --> 00:22:29.450
Also, scale was a
particular concern

00:22:29.450 --> 00:22:32.690
because based on previous
live streams that they

00:22:32.690 --> 00:22:35.260
had done with the
band, they estimated

00:22:35.260 --> 00:22:41.140
a potential concurrent usage
of 650,000 concurrent users.

00:22:41.140 --> 00:22:44.660
So because of the two factors,
the short amount of time

00:22:44.660 --> 00:22:47.470
involved, and also because
of the scale involved,

00:22:47.470 --> 00:22:50.320
the agency chose to
use Google App Engine.

00:22:50.320 --> 00:22:52.895
This is what they
built, very simply.

00:22:52.895 --> 00:22:56.870
The user would connect to
the App Engine application

00:22:56.870 --> 00:23:00.690
and would be asked to log
in using Google+ sign in.

00:23:00.690 --> 00:23:02.140
Once they'd logged
in, they would

00:23:02.140 --> 00:23:03.830
be presented with questions.

00:23:03.830 --> 00:23:07.280
New questions would be
available every 10 minutes,

00:23:07.280 --> 00:23:09.937
six questions every hour,
42 questions in total,

00:23:09.937 --> 00:23:11.520
and there's absolutely
no significance

00:23:11.520 --> 00:23:14.270
in that number, 42.

00:23:14.270 --> 00:23:17.380
So they were presented with
questions every 10 minutes

00:23:17.380 --> 00:23:19.235
and they would
answer the question.

00:23:19.235 --> 00:23:22.000
The client side code
would return the answer

00:23:22.000 --> 00:23:24.550
to an endpoint, run it
in the same application.

00:23:24.550 --> 00:23:26.400
An endpoint was
designed to handle

00:23:26.400 --> 00:23:27.890
all of our data services.

00:23:27.890 --> 00:23:32.200
And that data about any correct
answers, only correct answers,

00:23:32.200 --> 00:23:35.990
were stored into mem cache,
along with any demographic

00:23:35.990 --> 00:23:38.210
information that we have
that the user had allowed us

00:23:38.210 --> 00:23:41.599
to actually gain from
the Google+ sign in.

00:23:41.599 --> 00:23:43.640
Now, we stored it into
Memcache, but because this

00:23:43.640 --> 00:23:45.060
was a one off event,
we didn't actually

00:23:45.060 --> 00:23:46.684
store it through to
our back end store.

00:23:46.684 --> 00:23:48.130
This wasn't a write
through cache.

00:23:48.130 --> 00:23:50.820
This was just stored into cache.

00:23:50.820 --> 00:23:54.230
Now, App Engine has a
shared mem cache service.

00:23:54.230 --> 00:23:56.140
The service isn't
completely free.

00:23:56.140 --> 00:23:57.890
They're shared across
all applications

00:23:57.890 --> 00:23:59.964
running on App Engine.

00:23:59.964 --> 00:24:02.130
Obviously, that would make
it a little bit volatile,

00:24:02.130 --> 00:24:04.040
so we have this feature
as well in App Engine

00:24:04.040 --> 00:24:07.960
called Dedicated Memcache, which
is what we used for this event.

00:24:07.960 --> 00:24:11.680
We reserved about 20
gigabytes of space

00:24:11.680 --> 00:24:15.050
in Memcache specifically
for our application.

00:24:15.050 --> 00:24:16.989
So all of the data was
stored into Memcache,

00:24:16.989 --> 00:24:18.530
and there was another
endpoint that's

00:24:18.530 --> 00:24:20.870
not shown on the
diagram that was used

00:24:20.870 --> 00:24:23.530
by the organizers of the
event to query that data that

00:24:23.530 --> 00:24:25.110
was stored in Memcache.

00:24:25.110 --> 00:24:28.390
And that allowed them to come
up with interesting statistics,

00:24:28.390 --> 00:24:33.920
such as 95% of people in
the age range 20 to 30

00:24:33.920 --> 00:24:36.647
in Brazil answered the
last question correctly.

00:24:36.647 --> 00:24:38.480
They could then put
that data on the tickers

00:24:38.480 --> 00:24:43.282
at the bottom of the live
stream and broadcast it live.

00:24:43.282 --> 00:24:46.690
After the 42 questions
had been delivered,

00:24:46.690 --> 00:24:48.260
we would drain the data.

00:24:48.260 --> 00:24:49.930
After the event is
finished, the data

00:24:49.930 --> 00:24:53.060
would be drained out Of
memcache into Cloud SQL, which

00:24:53.060 --> 00:24:55.712
is our MySQL in
the cloud offering.

00:24:55.712 --> 00:24:56.670
That's the application.

00:24:56.670 --> 00:24:58.925
That's how complicated it
was, or not complicated.

00:25:01.870 --> 00:25:04.950
So obviously, scale
was a big concern,

00:25:04.950 --> 00:25:08.080
and we, in partnership
with the agency,

00:25:08.080 --> 00:25:11.850
did lots of load testing
before the event.

00:25:11.850 --> 00:25:14.260
After 10,000 queries
per second-- and queries

00:25:14.260 --> 00:25:16.550
per second that we use
internally in Google

00:25:16.550 --> 00:25:19.020
generally equates to
requests per second.

00:25:19.020 --> 00:25:23.850
And that was great,
10k queries per second.

00:25:23.850 --> 00:25:27.040
The URL of the application
was leaked before the event,

00:25:27.040 --> 00:25:29.940
so we were able to see a
steady ramp up in traffic.

00:25:29.940 --> 00:25:31.780
It was leaked
through social media,

00:25:31.780 --> 00:25:35.290
so that ramp up was
relatively exponential,

00:25:35.290 --> 00:25:37.250
but it did allow us to
test some assumptions,

00:25:37.250 --> 00:25:40.170
and also to fix a couple of
issues that we saw with our API

00:25:40.170 --> 00:25:43.240
quota around Google+.

00:25:43.240 --> 00:25:45.951
We didn't leak the
URL, but if I was ever

00:25:45.951 --> 00:25:47.700
going to do this again,
I would definitely

00:25:47.700 --> 00:25:52.360
leak the URL myself just to make
that happen, but it was great.

00:25:52.360 --> 00:25:55.040
Then we settled into this
steady traffic pattern

00:25:55.040 --> 00:25:58.870
where we saw every five
minutes a peak of about 600,

00:25:58.870 --> 00:26:02.370
a peak of about 700
queries per second.

00:26:02.370 --> 00:26:05.110
That's what this
graph shows here.

00:26:05.110 --> 00:26:08.690
Now, we had to try and avoid
the issue where potentially,

00:26:08.690 --> 00:26:12.950
650,000 users would all answer
a question simultaneously.

00:26:12.950 --> 00:26:15.640
So the delivery of the
questions or the availability

00:26:15.640 --> 00:26:18.140
of questions was staggered
over a period of time.

00:26:18.140 --> 00:26:19.755
That's why we have
two spikes and why

00:26:19.755 --> 00:26:23.250
the spikes are reasonably broad.

00:26:23.250 --> 00:26:26.720
So that went on for about four
hours, and about four hours

00:26:26.720 --> 00:26:36.480
into the event, the
organizers of the live stream

00:26:36.480 --> 00:26:40.740
decided to announce the
application live on the event.

00:26:40.740 --> 00:26:42.440
And that gave us
this spike here,

00:26:42.440 --> 00:26:46.640
a slightly different
x-axis here.

00:26:46.640 --> 00:26:49.560
We saw a spike of 9,000 QPS.

00:26:49.560 --> 00:26:53.890
And later on, we saw a spike of
around 5,000 or 6,000 as well.

00:26:53.890 --> 00:26:54.960
So that was fine.

00:26:54.960 --> 00:26:57.750
We'd tested up to 10,000
queries per second.

00:26:57.750 --> 00:27:00.802
We had capacity to go
way beyond that as well.

00:27:00.802 --> 00:27:01.510
So that was fine.

00:27:01.510 --> 00:27:03.260
We had no problems
with that at all.

00:27:03.260 --> 00:27:05.740
We had no problems with
this because App Engine just

00:27:05.740 --> 00:27:06.942
scales inherently.

00:27:06.942 --> 00:27:08.650
There's not a huge
amount a developer has

00:27:08.650 --> 00:27:10.729
to do to actually
scale on App Engine.

00:27:10.729 --> 00:27:12.520
Obviously, with an
event of this size where

00:27:12.520 --> 00:27:14.409
we have to scale massively
and rapidly, there

00:27:14.409 --> 00:27:16.700
are certain things we have
to do within the application

00:27:16.700 --> 00:27:18.260
to make sure it doesn't
bottleneck or anything

00:27:18.260 --> 00:27:18.970
like that.

00:27:18.970 --> 00:27:20.640
But generally, if you're
building on App Engine,

00:27:20.640 --> 00:27:21.400
you'll just scale.

00:27:21.400 --> 00:27:24.500
The instances, it's very
small micro virtual machines,

00:27:24.500 --> 00:27:27.610
are spun up very quickly,
torn down very quickly.

00:27:27.610 --> 00:27:29.980
So you always have what
you need when you need it

00:27:29.980 --> 00:27:32.360
but you only pay
for what you use.

00:27:32.360 --> 00:27:35.050
But how do you scale Compute
Engine virtual machines?

00:27:35.050 --> 00:27:36.990
So Compute Engine is
our infrastructure

00:27:36.990 --> 00:27:39.150
as a service offering
where App Engine was

00:27:39.150 --> 00:27:41.080
our platform as a
service offering,

00:27:41.080 --> 00:27:43.750
and we're trying to blur the
lines between infrastructure

00:27:43.750 --> 00:27:44.800
and platform.

00:27:44.800 --> 00:27:46.860
So how would you go about
scaling Compute engine

00:27:46.860 --> 00:27:48.200
virtual machines?

00:27:48.200 --> 00:27:49.275
So you do it this way.

00:27:49.275 --> 00:27:50.941
This is something
that we announced back

00:27:50.941 --> 00:27:53.160
in March of this year.

00:27:53.160 --> 00:27:56.195
We announced Deployment
Manager replica pools,

00:27:56.195 --> 00:27:58.130
another thing called
resource views,

00:27:58.130 --> 00:28:00.254
which we're not going to
talk about too much today.

00:28:00.254 --> 00:28:01.670
They're a little bit obscure.

00:28:01.670 --> 00:28:04.610
But basically, we like doing
things in Google declaratively.

00:28:04.610 --> 00:28:07.780
We like creating templates or
config files and building stuff

00:28:07.780 --> 00:28:09.829
from those templates
and config files.

00:28:09.829 --> 00:28:10.995
This is an example template.

00:28:10.995 --> 00:28:12.578
It's not really there
for you to read,

00:28:12.578 --> 00:28:14.880
but this is an
example of a template.

00:28:14.880 --> 00:28:17.425
That will be fed into a
service called Replica Pool,

00:28:17.425 --> 00:28:19.730
and Replica Pool would
do all of the work

00:28:19.730 --> 00:28:22.340
in converting that translate
into a deployment, which

00:28:22.340 --> 00:28:24.180
would look something like that.

00:28:24.180 --> 00:28:26.310
So this is a group
of virtual machines.

00:28:26.310 --> 00:28:28.020
These are what Chris
[INAUDIBLE] would

00:28:28.020 --> 00:28:31.699
call a fleet of homogeneous
virtual machines,

00:28:31.699 --> 00:28:32.740
and they're all the same.

00:28:32.740 --> 00:28:33.650
They're all identical.

00:28:33.650 --> 00:28:35.191
They're all built
from this template.

00:28:35.191 --> 00:28:36.845
There's no snowflake
service here.

00:28:36.845 --> 00:28:38.480
And we have a load
balancer as well

00:28:38.480 --> 00:28:39.882
configured to
distribute the load

00:28:39.882 --> 00:28:41.340
across the pool of
virtual machines

00:28:41.340 --> 00:28:44.459
with a single endpoint, a single
IP address you can connect to.

00:28:44.459 --> 00:28:46.000
So for all intents
and purposes, they

00:28:46.000 --> 00:28:48.580
look like one virtual
machine, but there's many.

00:28:51.200 --> 00:28:53.679
Replica Pool also offers a
monitoring service as well.

00:28:53.679 --> 00:28:55.970
So it can monitor the health
of these virtual machines,

00:28:55.970 --> 00:28:59.570
and if they break, they can
bring them back up again.

00:28:59.570 --> 00:29:03.560
This is all done from an agent
running on the virtual machine.

00:29:03.560 --> 00:29:04.610
So that's Replica Pool.

00:29:04.610 --> 00:29:05.980
This is a static resource.

00:29:05.980 --> 00:29:07.920
We can scale it manually.

00:29:07.920 --> 00:29:09.100
We can add replicas.

00:29:09.100 --> 00:29:11.720
We can remove replicas and
size the pool manually.

00:29:11.720 --> 00:29:16.200
But how would we manage auto
scale in this situation?

00:29:16.200 --> 00:29:19.570
So now we're going to talk about
something called autoscaler.

00:29:19.570 --> 00:29:22.610
Autoscaler is already available
as part of Deployment Manager.

00:29:22.610 --> 00:29:25.950
Ultimately, it will be made
available as a separate API.

00:29:25.950 --> 00:29:28.540
But autoscaler allows
you to create a config

00:29:28.540 --> 00:29:31.940
within your template that
sets up auto scale parameters.

00:29:31.940 --> 00:29:34.830
At the moment, the only
thing we can auto scale on--

00:29:34.830 --> 00:29:39.130
and this will change-- is
average CPU utilization

00:29:39.130 --> 00:29:40.435
across the entire pool.

00:29:40.435 --> 00:29:41.810
And that's where
you'll see there

00:29:41.810 --> 00:29:44.530
where it says Target
Utilization 0.5.

00:29:44.530 --> 00:29:49.480
That's effectively
50% CPU utilization

00:29:49.480 --> 00:29:51.660
averaged across the entire pool.

00:29:51.660 --> 00:29:54.950
Whenever we go above 50%
utilization on average,

00:29:54.950 --> 00:29:56.170
we'll spin up new instances.

00:29:56.170 --> 00:29:59.360
Whenever we go below 50%, if
we have too many instances,

00:29:59.360 --> 00:30:00.740
we'll turn them down.

00:30:00.740 --> 00:30:02.415
That's how auto scale works.

00:30:02.415 --> 00:30:05.360
And now, we're going to
give you a demo of that

00:30:05.360 --> 00:30:07.160
actually working.

00:30:07.160 --> 00:30:09.692
This is the kind of demo
you can spend 30 minutes on.

00:30:09.692 --> 00:30:11.850
I'm going to try
and do it in three.

00:30:17.075 --> 00:30:20.404
Can everybody read that?

00:30:20.404 --> 00:30:22.430
Can everybody read it?

00:30:22.430 --> 00:30:23.500
Yeah?

00:30:23.500 --> 00:30:24.530
No no's?

00:30:24.530 --> 00:30:25.280
OK, good.

00:30:25.280 --> 00:30:27.756
I should have asked if
anybody couldn't read it.

00:30:38.280 --> 00:30:40.215
So back to my
slides just briefly,

00:30:40.215 --> 00:30:41.978
I just wanted to set that up.

00:30:48.559 --> 00:30:50.100
I'm going to go
through the left hand

00:30:50.100 --> 00:30:51.550
side of what this
demo looks like,

00:30:51.550 --> 00:30:52.700
and then I'll go through
the right hand side

00:30:52.700 --> 00:30:53.730
once I've kicked it off.

00:30:53.730 --> 00:30:56.270
It takes a little
bit of time to run.

00:30:56.270 --> 00:30:59.220
So we're going to be making
use of Google Cloud Storage.

00:30:59.220 --> 00:31:00.975
Google Cloud Storage
is an object store.

00:31:00.975 --> 00:31:04.770
It's a bucket-based store
for unstructured data.

00:31:04.770 --> 00:31:06.860
It has a lot of really
nifty-- sorry, that's

00:31:06.860 --> 00:31:11.050
a British term-- a lot
of really cool features

00:31:11.050 --> 00:31:12.240
that you can make use of.

00:31:12.240 --> 00:31:14.240
One of them, the one we're
going to make use of,

00:31:14.240 --> 00:31:16.030
is called Object Notifications.

00:31:16.030 --> 00:31:17.080
So we have these buckets.

00:31:17.080 --> 00:31:19.990
We have an input bucket
and an output bucket.

00:31:19.990 --> 00:31:23.110
So what we're going to do here
is solve a relatively complex

00:31:23.110 --> 00:31:27.380
problem where a user has a
large number of images coming

00:31:27.380 --> 00:31:30.590
from some source that she wants
to convert into thumbnails,

00:31:30.590 --> 00:31:32.325
and she wants to do
this very quickly.

00:31:32.325 --> 00:31:34.408
We don't know why she wants
to do it very quickly,

00:31:34.408 --> 00:31:35.940
but she needs to
do it very quickly.

00:31:35.940 --> 00:31:37.670
That's the problem we're
going to solve here.

00:31:37.670 --> 00:31:39.044
So we have an
input bucket, which

00:31:39.044 --> 00:31:41.320
will receive incoming
images, and an output bucket,

00:31:41.320 --> 00:31:43.530
which will receive thumbnails.

00:31:43.530 --> 00:31:47.690
So having said that, I'm going
to kick the actual copying

00:31:47.690 --> 00:31:53.170
over of images off
now while I talk.

00:31:53.170 --> 00:31:58.120
So Cloud Storage has
this tool called GS Util,

00:31:58.120 --> 00:32:01.585
and it allows us to copy files
from one bucket to another.

00:32:01.585 --> 00:32:03.210
So in this case, I
have a bucket that's

00:32:03.210 --> 00:32:09.750
pre-configured with
exactly 1,111 images, which

00:32:09.750 --> 00:32:12.424
I'm going to copy over from
that bucket to another bucket.

00:32:12.424 --> 00:32:14.340
So the first thing we
need to do here, though,

00:32:14.340 --> 00:32:17.454
is look at what we
have to begin with.

00:32:17.454 --> 00:32:19.870
So if I go to the developer's
console, which we showed you

00:32:19.870 --> 00:32:23.657
earlier, this is my replica
pool which I created before.

00:32:23.657 --> 00:32:25.240
I've actually set
this replica pool up

00:32:25.240 --> 00:32:27.920
with a load balancer
with one virtual machine,

00:32:27.920 --> 00:32:31.630
and this is the virtual
machine, I/O 14 Demo Y598.

00:32:31.630 --> 00:32:32.584
That's what we have.

00:32:32.584 --> 00:32:34.250
You can see from the
spikes in the graph

00:32:34.250 --> 00:32:37.520
that I have been practicing
this demo many, many times

00:32:37.520 --> 00:32:40.150
over the last few days,
which is quite fun.

00:32:40.150 --> 00:32:42.860
And it's always worked,
so hopefully today, it's

00:32:42.860 --> 00:32:45.660
going to work as well.

00:32:45.660 --> 00:32:47.615
So what else do we have?

00:32:47.615 --> 00:32:48.980
We have Cloud Storage.

00:32:48.980 --> 00:32:51.870
If I look at Cloud Storage,
we have three buckets.

00:32:51.870 --> 00:32:55.600
We have our Images bucket, which
is where the input images are

00:32:55.600 --> 00:33:01.020
going to come into, and we
have the Thumbnails bucket,

00:33:01.020 --> 00:33:03.210
which is where the thumbnails
will be written to.

00:33:03.210 --> 00:33:07.040
Then we have the
Payload bucket, and this

00:33:07.040 --> 00:33:08.619
is where all of the images are.

00:33:08.619 --> 00:33:09.660
This could be any source.

00:33:09.660 --> 00:33:12.330
This could be some source of
images, a stream of images,

00:33:12.330 --> 00:33:14.220
a batch of images,
but we're basically

00:33:14.220 --> 00:33:17.677
going to copy them over
from one bucket to another.

00:33:17.677 --> 00:33:19.010
So GS Util allows us to do that.

00:33:19.010 --> 00:33:21.190
It allows us to copy from
one bucket to another.

00:33:21.190 --> 00:33:24.865
So we're going to copy from
Autoscale-IO-Payload all

00:33:24.865 --> 00:33:29.210
of the files into
Autoscale-IO-Images.

00:33:29.210 --> 00:33:30.972
And we're going to
do this in parallel.

00:33:30.972 --> 00:33:32.930
We're not going to pull
the files down and then

00:33:32.930 --> 00:33:33.690
push them back up.

00:33:33.690 --> 00:33:35.890
This is going to copy them
between the cloud storage

00:33:35.890 --> 00:33:39.550
buckets directly, internal
Google infrastructure,

00:33:39.550 --> 00:33:42.360
and we're going to
do it in parallel.

00:33:42.360 --> 00:33:46.545
Because for some reason
this always seems to fail,

00:33:46.545 --> 00:33:48.972
I'm going to do this first.

00:33:48.972 --> 00:33:51.140
Ulimit minus n 10,000.

00:33:51.140 --> 00:33:54.177
That allows me to have
10,000 file handles.

00:33:54.177 --> 00:33:56.510
And for some reason, that
seem to reset itself every now

00:33:56.510 --> 00:34:00.610
and again, and I
have no idea why.

00:34:00.610 --> 00:34:02.190
Now we kick off
the copy, and that

00:34:02.190 --> 00:34:06.151
will happen in the background
while I continue talking.

00:34:06.151 --> 00:34:07.650
Is it more interesting
to watch this

00:34:07.650 --> 00:34:10.440
or to listen to me talking?

00:34:10.440 --> 00:34:13.080
I'm not sure.

00:34:13.080 --> 00:34:17.960
So going back to the
slides, what we have here is

00:34:17.960 --> 00:34:19.460
object notifications.

00:34:19.460 --> 00:34:22.159
Object notifications are
really called whenever anything

00:34:22.159 --> 00:34:24.590
is added to the bucket
or anything is updated

00:34:24.590 --> 00:34:28.532
within the bucket, any object,
a notification will be fired,

00:34:28.532 --> 00:34:30.865
and that notification will
be in the form of a URL which

00:34:30.865 --> 00:34:33.790
is being called by
the notification.

00:34:33.790 --> 00:34:36.409
So with every image
we add to the bucket,

00:34:36.409 --> 00:34:38.030
we're going to have
one of these calls

00:34:38.030 --> 00:34:40.319
specifically for
that image to a URL,

00:34:40.319 --> 00:34:42.310
which we've registered
ahead of time.

00:34:42.310 --> 00:34:45.024
And that URL could be a load
balanced pool of replicas,

00:34:45.024 --> 00:34:47.324
a load balanced of
virtual machines.

00:34:47.324 --> 00:34:49.739
But that gives us no
control over the way

00:34:49.739 --> 00:34:52.498
in which data flows
into the system.

00:34:52.498 --> 00:34:54.414
So we're going to use
something in App Engine.

00:34:54.414 --> 00:34:56.250
We're going to use an
App Engine application

00:34:56.250 --> 00:34:57.720
and something
called Task Queues.

00:34:57.720 --> 00:34:59.919
How many of you have
heard of Task Queues?

00:34:59.919 --> 00:35:02.210
Task Queues are one of the
best features of App Engine.

00:35:02.210 --> 00:35:04.800
They allow us to actually
do work outside of the user

00:35:04.800 --> 00:35:06.260
request.

00:35:06.260 --> 00:35:09.290
So the request deadline for
a user request is 60 seconds.

00:35:09.290 --> 00:35:11.340
Often, you want to do
work outside of that,

00:35:11.340 --> 00:35:13.140
so you'll use a Task Queue

00:35:13.140 --> 00:35:15.730
What you do is you create a
task, you put it onto a queue,

00:35:15.730 --> 00:35:18.230
and then you have something
in App Engine consuming

00:35:18.230 --> 00:35:22.350
those tasks from the queue at
a rate of, by default, five

00:35:22.350 --> 00:35:24.578
every second.

00:35:24.578 --> 00:35:28.430
The applications consuming those
tasks will process the request

00:35:28.430 --> 00:35:32.280
and then create another HTTP
request to our pool of load

00:35:32.280 --> 00:35:33.390
balanced virtual machines.

00:35:33.390 --> 00:35:36.210
And this will be distributed
across our virtual machines,

00:35:36.210 --> 00:35:38.940
hopefully in a round robin
style, across all of them.

00:35:38.940 --> 00:35:41.060
We only have one to begin
with, so there's not

00:35:41.060 --> 00:35:41.851
much to distribute.

00:35:41.851 --> 00:35:44.972
But hopefully, we'll have more
before the end of the demo.

00:35:44.972 --> 00:35:47.430
Then we have a process running
on the virtual machines that

00:35:47.430 --> 00:35:49.322
will process the
images with Image Magic

00:35:49.322 --> 00:35:50.280
and create a thumbnail.

00:35:53.430 --> 00:35:56.080
So we go back to the Developer's
Console, look at that.

00:35:56.080 --> 00:35:59.022
First thing we want to look
at is the Thumbnails bucket.

00:35:59.022 --> 00:36:00.480
And hopefully when
I click on this,

00:36:00.480 --> 00:36:01.850
we'll see a whole
bunch of thumbnails

00:36:01.850 --> 00:36:02.975
which weren't there before.

00:36:06.030 --> 00:36:08.290
So these are the thumbnails.

00:36:08.290 --> 00:36:12.020
The thumbnails are
a picture of me

00:36:12.020 --> 00:36:15.364
because I'm an egotist
or something like that.

00:36:15.364 --> 00:36:16.530
So we're getting thumbnails.

00:36:16.530 --> 00:36:17.480
That's good.

00:36:17.480 --> 00:36:19.140
We want to get thumbnails.

00:36:19.140 --> 00:36:21.060
What about these virtual
machines, though?

00:36:21.060 --> 00:36:24.114
How many do we have?

00:36:24.114 --> 00:36:25.650
We have two currently.

00:36:25.650 --> 00:36:27.690
So we've doubled the
size of our pool.

00:36:27.690 --> 00:36:29.930
We've added another virtual
machine to do processing.

00:36:29.930 --> 00:36:34.930
So this basically is set up to
a threshold of 0.3, which 30%.

00:36:34.930 --> 00:36:38.142
So we're looking for
30% CPU utilization.

00:36:38.142 --> 00:36:40.100
The application running
on the virtual machines

00:36:40.100 --> 00:36:42.730
is a Go application and
is using Go routines,

00:36:42.730 --> 00:36:45.746
so it can do the request
processing in parallel.

00:36:45.746 --> 00:36:47.370
Normally, we get four
virtual machines.

00:36:47.370 --> 00:36:48.691
Today, we've only got two.

00:36:48.691 --> 00:36:51.190
It really depends on how quickly
we can pull that data down,

00:36:51.190 --> 00:36:54.210
suck it down from one
bucket to another.

00:36:54.210 --> 00:36:56.595
So let's see what happens
and see if we get any more.

00:37:03.720 --> 00:37:05.880
So now we've got four
virtual machines.

00:37:05.880 --> 00:37:08.510
So now, what we're going to
do is go back to the slides

00:37:08.510 --> 00:37:12.490
and look at one I created
earlier, in real just Julia

00:37:12.490 --> 00:37:15.730
Child style, as
Julia would tell me.

00:37:15.730 --> 00:37:22.660
And we have this
graph, which shows

00:37:22.660 --> 00:37:27.170
you CPU utilization,
the blue bar,

00:37:27.170 --> 00:37:28.800
against number of
virtual machines.

00:37:28.800 --> 00:37:30.466
And obviously, this
wasn't for this one.

00:37:30.466 --> 00:37:31.850
This was a run I
did on Saturday.

00:37:31.850 --> 00:37:34.310
And you can see that as
we hit 30% threshold,

00:37:34.310 --> 00:37:36.110
we spin up a new
virtual machine.

00:37:36.110 --> 00:37:38.180
We spin up more
virtual machines.

00:37:38.180 --> 00:37:40.610
The CPU utilizations come down.

00:37:40.610 --> 00:37:43.000
At some point, we say, we have
too many virtual machines,

00:37:43.000 --> 00:37:44.051
so we removed them.

00:37:44.051 --> 00:37:46.550
Obviously, you don't want that
to keep happening bang, bang,

00:37:46.550 --> 00:37:48.008
bang, bang, bang,
bang, so you have

00:37:48.008 --> 00:37:51.040
this cool down period
where you can say,

00:37:51.040 --> 00:37:55.970
there has to be a minimum of
this time between Auto Scale

00:37:55.970 --> 00:37:58.170
activations, and
that's what happens.

00:37:58.170 --> 00:37:59.220
That's the demo.

00:37:59.220 --> 00:38:01.641
We'll come back to it
later if we have time.

00:38:01.641 --> 00:38:03.390
So I just wanted to
share a couple of tips

00:38:03.390 --> 00:38:08.460
with you, as Wes did,
with measuring fast,

00:38:08.460 --> 00:38:10.440
a few tips on scaling fast.

00:38:10.440 --> 00:38:13.180
The first one is always to use
Google App Engine as the entry

00:38:13.180 --> 00:38:16.845
point for your web applications
and as the API service

00:38:16.845 --> 00:38:20.870
for your mobile and
gaming back ends.

00:38:20.870 --> 00:38:25.070
As we've seen with example
of One Direction Day,

00:38:25.070 --> 00:38:27.490
App Engine scales extremely
well and extremely quickly,

00:38:27.490 --> 00:38:29.727
and there's very, very
little work on your part

00:38:29.727 --> 00:38:30.560
that you have to do.

00:38:30.560 --> 00:38:33.860
Now, some applications are
best suited for App Engine,

00:38:33.860 --> 00:38:37.360
and in that case, you can
build a modular application

00:38:37.360 --> 00:38:40.080
where all of your front
side stuff is on App Engine

00:38:40.080 --> 00:38:44.530
but virtual machines being
managed by App Engine

00:38:44.530 --> 00:38:46.620
are running your back end code.

00:38:46.620 --> 00:38:50.330
That's really easy
and simple to set up.

00:38:50.330 --> 00:38:51.950
Also, make use of edge caching.

00:38:51.950 --> 00:38:54.540
So we've seen that
with Andromeda, we

00:38:54.540 --> 00:38:56.620
have much higher
throughput, particularly

00:38:56.620 --> 00:38:58.810
on larger instances,
but throughput always

00:38:58.810 --> 00:38:59.780
involves latency.

00:38:59.780 --> 00:39:03.250
So any kind of movement of
stuff through the network

00:39:03.250 --> 00:39:04.720
will involve latency.

00:39:04.720 --> 00:39:06.845
The best thing to do is
cache your content as close

00:39:06.845 --> 00:39:08.786
to the user as possible.

00:39:08.786 --> 00:39:11.160
And you can make use of edge
caching, which is caching it

00:39:11.160 --> 00:39:13.890
in our data centers,
or with ISPs,

00:39:13.890 --> 00:39:16.880
because we have a global peering
relationship with many ISPs.

00:39:16.880 --> 00:39:22.070
And you can do this with App
Engine using Static Content,

00:39:22.070 --> 00:39:24.080
with Cloud Storage by
setting cache control

00:39:24.080 --> 00:39:25.460
headers on any data you store.

00:39:25.460 --> 00:39:27.320
And with App Engine,
again, you can do it

00:39:27.320 --> 00:39:31.100
on dynamically generated
content by setting cache control

00:39:31.100 --> 00:39:34.289
headers on the response stream.

00:39:34.289 --> 00:39:36.455
And very quickly, because
we're running out of time,

00:39:36.455 --> 00:39:38.204
I'm going to run through
these very quick.

00:39:38.204 --> 00:39:40.050
For network I/O
intensive workloads,

00:39:40.050 --> 00:39:42.970
make use of those big, virtual
machines for more network

00:39:42.970 --> 00:39:45.430
throughput.

00:39:45.430 --> 00:39:47.580
For scaling, App
Engine can sometimes

00:39:47.580 --> 00:39:49.380
do with some help
for scaling, and you

00:39:49.380 --> 00:39:51.260
can use idle instances
on App Engine

00:39:51.260 --> 00:39:53.300
to make it scale that
little bit better.

00:39:53.300 --> 00:39:55.730
Idle Instances effectively
answer requests

00:39:55.730 --> 00:39:58.120
when there's no instance
available to actually answer

00:39:58.120 --> 00:39:59.580
that request immediately.

00:39:59.580 --> 00:40:03.360
They're [INAUDIBLE] as a
backup to answer requests.

00:40:03.360 --> 00:40:05.400
And the final one
is to take advantage

00:40:05.400 --> 00:40:08.060
of features such as
Page Speed and Memcache.

00:40:08.060 --> 00:40:11.565
Page Speed effectively is an
App Engine application or App

00:40:11.565 --> 00:40:15.290
Engine service that's
free to anybody

00:40:15.290 --> 00:40:17.450
who has a paid
application, a set billing

00:40:17.450 --> 00:40:20.480
app on your application, and
it will optimize your HTTP

00:40:20.480 --> 00:40:23.030
request, it will
compress and cache data,

00:40:23.030 --> 00:40:26.130
and it provides services such
as in lining and critical path

00:40:26.130 --> 00:40:26.830
rendering.

00:40:26.830 --> 00:40:27.800
Memcache.

00:40:27.800 --> 00:40:29.440
You guys all know
how to use Memcache,

00:40:29.440 --> 00:40:32.060
but you can also take advantage
of dedicated Memcache,

00:40:32.060 --> 00:40:34.697
as we did with
One Direction Day.

00:40:39.290 --> 00:40:42.500
So we've seen that Google is
built on a culture of fast,

00:40:42.500 --> 00:40:44.470
and that we're constantly
driving innovation

00:40:44.470 --> 00:40:48.750
within our infrastructure in
order to maintain and improve

00:40:48.750 --> 00:40:53.080
performance across all
of our APIs and services.

00:40:53.080 --> 00:40:55.410
We've also seen the Google
Cloud Platform benefits

00:40:55.410 --> 00:41:03.410
from this innovation through
network stuff like Andromeda.

00:41:03.410 --> 00:41:07.910
We've also seen that long
tail latency is an issue

00:41:07.910 --> 00:41:09.740
and it can be addressed
with the ability

00:41:09.740 --> 00:41:11.750
to measure fast,
as Wes showed you.

00:41:11.750 --> 00:41:13.880
We looked at tools such
as Cloud Trace that

00:41:13.880 --> 00:41:16.630
allow you to actually gain
rapid insight into issues

00:41:16.630 --> 00:41:20.800
like [INAUDIBLE] running in
applications in the cloud.

00:41:20.800 --> 00:41:23.450
And finally, fast matters.

00:41:23.450 --> 00:41:26.879
We come back to the message
that Wes had initially.

00:41:26.879 --> 00:41:27.420
Fast matters.

00:41:27.420 --> 00:41:30.750
Fast will always matter,
and Google Cloud Platform

00:41:30.750 --> 00:41:35.170
can help you build applications
that run fast in the cloud.

00:41:35.170 --> 00:41:37.580
So what are you going to build?

00:41:37.580 --> 00:41:40.040
With that, we're
going to sign off.

00:41:40.040 --> 00:41:43.060
We've only got a couple
of minutes for questions.

00:41:43.060 --> 00:41:45.589
The call to action, go to
the Cloud Platform blog

00:41:45.589 --> 00:41:47.630
and find out about features
that Wes talked about

00:41:47.630 --> 00:41:49.389
and that I talked about.

00:41:49.389 --> 00:41:51.180
Preview features such
as Deployment Manager

00:41:51.180 --> 00:41:52.280
and Replica Pools.

00:41:52.280 --> 00:41:53.830
You can sign up there.

00:41:53.830 --> 00:41:56.615
The next session is
in Room Seven, Zero

00:41:56.615 --> 00:41:59.800
to Hero with Google Cloud
Platform with Chris Ramsdale.

00:41:59.800 --> 00:42:02.204
You can provide feedback
via the QR code.

00:42:02.204 --> 00:42:03.620
There's one up
there on the board.

00:42:06.340 --> 00:42:08.770
And you probably haven't
received anything about this

00:42:08.770 --> 00:42:11.720
yet, but you can get $500 in
credits for the Google Cloud

00:42:11.720 --> 00:42:13.260
Platform.

00:42:13.260 --> 00:42:19.380
Go to g.co/CloudStarterPack and
enter the promo code GoogleIO.

00:42:19.380 --> 00:42:23.470
And there's also a road show
in the US happening very soon,

00:42:23.470 --> 00:42:25.630
and you can go to
googlecloudroadshow.com

00:42:25.630 --> 00:42:26.772
to sign up for that.

00:42:26.772 --> 00:42:27.730
WESTON HUTCHINS: Great.

00:42:27.730 --> 00:42:29.521
If you guys have any
questions, Mandy and I

00:42:29.521 --> 00:42:32.250
are going to be sitting outside
in the cloud booth over there,

00:42:32.250 --> 00:42:34.250
so come find us.

00:42:34.250 --> 00:42:36.030
Thank you very much.

