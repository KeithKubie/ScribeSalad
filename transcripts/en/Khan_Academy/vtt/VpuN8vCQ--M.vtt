WEBVTT
Kind: captions
Language: en

00:00:01.880 --> 00:00:08.300
Let's learn a little bit about
the law of large numbers, which

00:00:08.300 --> 00:00:11.730
is on many levels, one of the
most intuitive laws in

00:00:11.730 --> 00:00:14.190
mathematics and in
probability theory.

00:00:14.190 --> 00:00:18.570
But because it's so applicable
to so many things, it's often a

00:00:18.570 --> 00:00:22.190
misused law or sometimes,
slightly misunderstood.

00:00:22.190 --> 00:00:26.250
So just to be a little bit
formal in our mathematics, let

00:00:26.250 --> 00:00:28.570
me just define it for you first
and then we'll talk a little

00:00:28.570 --> 00:00:29.435
bit about the intuition.

00:00:29.435 --> 00:00:33.990
So let's say I have a
random variable, X.

00:00:33.990 --> 00:00:38.750
And we know its expected value
or its population mean.

00:00:38.750 --> 00:00:41.790
The law of large numbers just
says that if we take a sample

00:00:41.790 --> 00:00:46.480
of n observations of our random
variable, and if we were

00:00:46.480 --> 00:00:49.080
to average all of those
observations-- and let me

00:00:49.080 --> 00:00:50.680
define another variable.

00:00:50.680 --> 00:00:54.150
Let's call that x sub n
with a line on top of it.

00:00:54.150 --> 00:00:56.970
This is the mean of n
observations of our

00:00:56.970 --> 00:00:57.760
random variable.

00:00:57.760 --> 00:01:00.530
So it's literally this is
my first observation.

00:01:00.530 --> 00:01:03.230
So you can kind of say I run
the experiment once and I get

00:01:03.230 --> 00:01:07.120
this observation and I run it
again, I get that observation.

00:01:07.120 --> 00:01:11.640
And I keep running it n times
and then I divide by my

00:01:11.640 --> 00:01:12.630
number of observations.

00:01:12.630 --> 00:01:14.150
So this is my sample mean.

00:01:14.150 --> 00:01:17.340
This is the mean of all the
observations I've made.

00:01:17.340 --> 00:01:23.210
The law of large numbers just
tells us that my sample mean

00:01:23.210 --> 00:01:27.580
will approach my expected
value of the random variable.

00:01:27.580 --> 00:01:32.760
Or I could also write it as my
sample mean will approach my

00:01:32.760 --> 00:01:39.910
population mean for n
approaching infinity.

00:01:39.910 --> 00:01:43.290
And I'll be a little informal
with what does approach or

00:01:43.290 --> 00:01:44.270
what does convergence mean?

00:01:44.270 --> 00:01:46.410
But I think you have the
general intuitive sense that if

00:01:46.410 --> 00:01:50.510
I take a large enough sample
here that I'm going to end up

00:01:50.510 --> 00:01:54.210
getting the expected value of
the population as a whole.

00:01:54.210 --> 00:01:56.800
And I think to a lot of us
that's kind of intuitive.

00:01:56.800 --> 00:02:01.610
That if I do enough trials that
over large samples, the trials

00:02:01.610 --> 00:02:04.070
would kind of give me the
numbers that I would expect

00:02:04.070 --> 00:02:06.720
given the expected value and
the probability and all that.

00:02:06.720 --> 00:02:09.330
But I think it's often a little
bit misunderstood in terms

00:02:09.330 --> 00:02:10.620
of why that happens.

00:02:10.620 --> 00:02:13.280
And before I go into
that let me give you

00:02:13.280 --> 00:02:15.090
a particular example.

00:02:15.090 --> 00:02:17.400
The law of large numbers will
just tell us that-- let's say I

00:02:17.400 --> 00:02:24.540
have a random variable-- X is
equal to the number of heads

00:02:24.540 --> 00:02:30.780
after 100 tosses of a fair
coin-- tosses or flips

00:02:30.780 --> 00:02:33.426
of a fair coin.

00:02:36.090 --> 00:02:37.890
First of all, we know what
the expected value of

00:02:37.890 --> 00:02:39.840
this random variable is.

00:02:39.840 --> 00:02:43.080
It's the number of tosses,
the number of trials times

00:02:43.080 --> 00:02:46.500
the probabilities of
success of any trial.

00:02:46.500 --> 00:02:49.175
So that's equal to 50.

00:02:49.175 --> 00:02:53.480
So the law of large numbers
just says if I were to take a

00:02:53.480 --> 00:02:57.530
sample or if I were to average
the sample of a bunch of these

00:02:57.530 --> 00:03:03.350
trials, so you know, I get-- my
first time I run this trial I

00:03:03.350 --> 00:03:06.280
flip 100 coins or have 100
coins in a shoe box and I shake

00:03:06.280 --> 00:03:10.280
the shoe box and I count the
number of heads, and I get 55.

00:03:10.280 --> 00:03:11.880
So that Would be X1.

00:03:11.880 --> 00:03:15.240
Then I shake the box
again and I get 65.

00:03:15.240 --> 00:03:18.110
Then I shake the box
again and I get 45.

00:03:18.110 --> 00:03:22.760
And I do this n times and then
I divide it by the number

00:03:22.760 --> 00:03:24.000
of times I did it.

00:03:24.000 --> 00:03:27.060
The law of large numbers just
tells us that this the

00:03:27.060 --> 00:03:31.050
average-- the average of all
of my observations, is going

00:03:31.050 --> 00:03:38.700
to converge to 50 as n
approaches infinity.

00:03:38.700 --> 00:03:40.890
Or for n approaching 50.

00:03:40.890 --> 00:03:42.770
I'm sorry, n
approaching infinity.

00:03:42.770 --> 00:03:45.150
And I want to talk a little
bit about why this happens

00:03:45.150 --> 00:03:47.000
or intuitively why this is.

00:03:47.000 --> 00:03:50.520
A lot of people kind of feel
that oh, this means that if

00:03:50.520 --> 00:03:54.950
after 100 trials that if I'm
above the average that somehow

00:03:54.950 --> 00:03:57.650
the laws of probability are
going to give me more heads

00:03:57.650 --> 00:04:00.150
or fewer heads to kind of
make up the difference.

00:04:00.150 --> 00:04:01.940
That's not quite what's
going to happen.

00:04:01.940 --> 00:04:04.440
That's often called the
gambler's fallacy.

00:04:04.440 --> 00:04:05.480
Let me differentiate.

00:04:05.480 --> 00:04:06.510
And I'll use this example.

00:04:06.510 --> 00:04:08.420
So let's say-- let
me make a graph.

00:04:08.420 --> 00:04:09.285
And I'll switch colors.

00:04:23.010 --> 00:04:25.200
This is n, my x-axis is n.

00:04:25.200 --> 00:04:27.650
This is the number
of trials I take.

00:04:27.650 --> 00:04:32.520
And my y-axis, let me make
that the sample mean.

00:04:32.520 --> 00:04:36.190
And we know what the expected
value is, we know the expected

00:04:36.190 --> 00:04:39.090
value of this random
variable is 50.

00:04:39.090 --> 00:04:40.055
Let me draw that here.

00:04:42.670 --> 00:04:43.345
This is 50.

00:04:47.410 --> 00:04:49.710
So just going to
the example I did.

00:04:49.710 --> 00:04:53.760
So when n is equal to--
let me just [INAUDIBLE]

00:04:53.760 --> 00:04:54.890
here.

00:04:54.890 --> 00:04:59.250
So my first trial I got 55
and so that was my average.

00:04:59.250 --> 00:05:00.750
I only had one data point.

00:05:00.750 --> 00:05:04.720
Then after two trials,
let's see, then I have 65.

00:05:04.720 --> 00:05:09.150
And so my average is going to
be 65 plus 55 divided by 2.

00:05:09.150 --> 00:05:10.490
which is 60.

00:05:10.490 --> 00:05:13.410
So then my average
went up a little bit.

00:05:13.410 --> 00:05:15.380
Then I had a 45, which
will bring my average

00:05:15.380 --> 00:05:16.880
down a little bit.

00:05:16.880 --> 00:05:18.210
I won't plot a 45 here.

00:05:18.210 --> 00:05:19.500
Now I have to average
all of these out.

00:05:19.500 --> 00:05:22.410
What's 45 plus 65?

00:05:22.410 --> 00:05:23.700
Let me actually just
get the number just

00:05:23.700 --> 00:05:24.580
so you get the point.

00:05:24.580 --> 00:05:28.670
So it's 55 plus 65.

00:05:28.670 --> 00:05:32.930
It's 120 plus 45 is 165.

00:05:32.930 --> 00:05:36.370
Divided by 3.

00:05:36.370 --> 00:05:39.890
3 goes into 165 5--
5 times 3 is 15.

00:05:39.890 --> 00:05:42.280
It's 53.

00:05:42.280 --> 00:05:43.520
No, no, no.

00:05:43.520 --> 00:05:45.310
55.

00:05:45.310 --> 00:05:46.990
So the average goes
down back down to 55.

00:05:46.990 --> 00:05:49.420
And we could keep
doing these trials.

00:05:49.420 --> 00:05:51.630
So you might say that the law
of large numbers tell this,

00:05:51.630 --> 00:05:56.640
OK, after we've done 3 trials
and our average is there.

00:05:56.640 --> 00:06:00.110
So a lot of people think that
somehow the gods of probability

00:06:00.110 --> 00:06:02.310
are going to make it more
likely that we get fewer

00:06:02.310 --> 00:06:03.170
heads in the future.

00:06:03.170 --> 00:06:06.150
That somehow the next couple of
trials are going to have to

00:06:06.150 --> 00:06:09.030
be down here in order to
bring our average down.

00:06:09.030 --> 00:06:10.720
And that's not
necessarily the case.

00:06:10.720 --> 00:06:13.240
Going forward the probabilities
are always the same.

00:06:13.240 --> 00:06:15.200
The probabilities are
always 50% that I'm

00:06:15.200 --> 00:06:16.160
going to get heads.

00:06:16.160 --> 00:06:19.660
It's not like if I had a bunch
of heads to start off with or

00:06:19.660 --> 00:06:22.100
more than I would have expected
to start off with, that all of

00:06:22.100 --> 00:06:25.360
a sudden things would be made
up and I would get more tails.

00:06:25.360 --> 00:06:27.530
That would the
gambler's fallacy.

00:06:27.530 --> 00:06:29.910
That if you have a long streak
of heads or you have a

00:06:29.910 --> 00:06:32.020
disproportionate number of
heads, that at some point

00:06:32.020 --> 00:06:35.070
you're going to have-- you have
a higher likelihood of having a

00:06:35.070 --> 00:06:37.010
disproportionate
number of tails.

00:06:37.010 --> 00:06:38.440
And that's not quite true.

00:06:38.440 --> 00:06:41.230
What the law of large numbers
tells us is that it doesn't

00:06:41.230 --> 00:06:45.700
care-- let's say after some
finite number of trials your

00:06:45.700 --> 00:06:48.130
average actually-- it's a low
probability of this happening,

00:06:48.130 --> 00:06:50.300
but let's say your average
is actually up here.

00:06:50.300 --> 00:06:52.270
Is actually at 70.

00:06:52.270 --> 00:06:56.000
You're like, wow, we really
diverged a good bit from

00:06:56.000 --> 00:06:57.020
the expected value.

00:06:57.020 --> 00:06:58.450
But what the law of large
numbers says, well, I don't

00:06:58.450 --> 00:07:00.270
care how many trials this is.

00:07:00.270 --> 00:07:03.870
We have an infinite
number of trials left.

00:07:03.870 --> 00:07:06.590
And the expected value for that
infinite number of trials,

00:07:06.590 --> 00:07:11.570
especially in this type of
situation is going to be this.

00:07:11.570 --> 00:07:15.680
So when you average a finite
number that averages out to

00:07:15.680 --> 00:07:18.390
some high number, and then an
infinite number that's going to

00:07:18.390 --> 00:07:22.890
converge to this, you're going
to over time, converge back

00:07:22.890 --> 00:07:24.030
to the expected value.

00:07:24.030 --> 00:07:27.310
And that was a very informal
way of describing it, but

00:07:27.310 --> 00:07:29.580
that's what the law or
large numbers tells you.

00:07:29.580 --> 00:07:30.940
And it's an important thing.

00:07:30.940 --> 00:07:33.560
It's not telling you that if
you get a bunch of heads that

00:07:33.560 --> 00:07:36.190
somehow the probability of
getting tails is going

00:07:36.190 --> 00:07:38.260
to increase to kind of
make up for the heads.

00:07:38.260 --> 00:07:41.660
What it's telling you is, is
that no matter what happened

00:07:41.660 --> 00:07:44.590
over a finite number of trials,
no matter what the average is

00:07:44.590 --> 00:07:46.560
over a finite number of
trials, you have an infinite

00:07:46.560 --> 00:07:47.960
number of trials left.

00:07:47.960 --> 00:07:51.850
And if you do enough of them
it's going to converge back

00:07:51.850 --> 00:07:52.800
to your expected value.

00:07:52.800 --> 00:07:54.430
And this is an important
thing to think about.

00:07:54.430 --> 00:07:57.770
But this isn't used in practice
every day with the lottery and

00:07:57.770 --> 00:08:02.220
with casinos because they know
that if you do large enough

00:08:02.220 --> 00:08:04.960
samples-- and we could even
calculate-- if you do large

00:08:04.960 --> 00:08:07.760
enough samples, what's the
probability that things

00:08:07.760 --> 00:08:09.510
deviate significantly?

00:08:09.510 --> 00:08:12.960
But casinos and the lottery
every day operate on this

00:08:12.960 --> 00:08:15.640
principle that if you take
enough people-- sure, in the

00:08:15.640 --> 00:08:18.240
short-term or with a few
samples, a couple people

00:08:18.240 --> 00:08:19.570
might beat the house.

00:08:19.570 --> 00:08:22.210
But over the long-term the
house is always going to win

00:08:22.210 --> 00:08:24.300
because of the parameters of
the games that they're

00:08:24.300 --> 00:08:25.300
making you play.

00:08:25.300 --> 00:08:28.180
Anyway, this is an important
thing in probability and I

00:08:28.180 --> 00:08:29.760
think it's fairly intuitive.

00:08:29.760 --> 00:08:32.590
Although, sometimes when you
see it formally explained like

00:08:32.590 --> 00:08:34.290
this with the random variables
and that it's a little

00:08:34.290 --> 00:08:34.940
bit confusing.

00:08:34.940 --> 00:08:39.550
All it's saying is that as you
take more and more samples, the

00:08:39.550 --> 00:08:44.590
average of that sample is going
to approximate the

00:08:44.590 --> 00:08:45.670
true average.

00:08:45.670 --> 00:08:47.410
Or I should be a little
bit more particular.

00:08:47.410 --> 00:08:51.690
The mean of your sample is
going to converge to the true

00:08:51.690 --> 00:08:54.970
mean of the population or to
the expected value of

00:08:54.970 --> 00:08:56.130
the random variable.

00:08:56.130 --> 00:08:58.850
Anyway, see you in
the next video.

