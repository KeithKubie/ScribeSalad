WEBVTT
Kind: captions
Language: en

00:00:05.070 --> 00:00:07.390
FELIPE HOFFA: And
hello, everyone.

00:00:07.390 --> 00:00:10.580
Thank you for staying-- last
talk of Google I/O 2014.

00:00:10.580 --> 00:00:15.700
It's been a long road,
but we are almost done.

00:00:15.700 --> 00:00:18.900
Predicting the future with
the Google Cloud Platform,

00:00:18.900 --> 00:00:22.460
that's what we want to do
today, what we want to show you.

00:00:22.460 --> 00:00:24.825
The first question is, why?

00:00:24.825 --> 00:00:26.380
Why now?

00:00:26.380 --> 00:00:30.820
And it turns out that we never
had so much data in the world.

00:00:30.820 --> 00:00:33.820
It's really impressive how
much data we can collect.

00:00:33.820 --> 00:00:37.595
Just going to I/O last year,
we had this presentation,

00:00:37.595 --> 00:00:40.070
"All the Ships in the
World," where we showed you

00:00:40.070 --> 00:00:44.150
how you were able to just
map and visualize and collect

00:00:44.150 --> 00:00:44.890
all this data.

00:00:44.890 --> 00:00:48.340
All the teams that's how they
are moving around the world.

00:00:48.340 --> 00:00:51.060
And a month ago, we
released this data

00:00:51.060 --> 00:00:55.790
set with Professor
[INAUDIBLE], over a quarter

00:00:55.790 --> 00:00:59.970
billion events all around the
world for the last 30 years

00:00:59.970 --> 00:01:01.790
detailing everything
important that

00:01:01.790 --> 00:01:03.910
is happening around the world.

00:01:03.910 --> 00:01:07.330
But we cannot only look at
things at the world level.

00:01:07.330 --> 00:01:10.440
We can zoom into
particular regions,

00:01:10.440 --> 00:01:13.880
let's say South America
or particular countries

00:01:13.880 --> 00:01:15.300
like Brazil.

00:01:15.300 --> 00:01:18.200
Something important
might be happening there.

00:01:18.200 --> 00:01:19.150
Let's zoom in further.

00:01:19.150 --> 00:01:23.890
Let's go through any city--
Fortaleza, for example,

00:01:23.890 --> 00:01:26.600
just to show you
examples of cities.

00:01:26.600 --> 00:01:29.170
Recently, our friend
Eric demoed how

00:01:29.170 --> 00:01:32.980
to stream all the powered
meters inside a city

00:01:32.980 --> 00:01:37.420
up to be queried with
50,000 rows per second.

00:01:37.420 --> 00:01:39.960
But let's zoom in to a
particular place today.

00:01:39.960 --> 00:01:43.770
Let's go to this stadium.

00:01:43.770 --> 00:01:47.600
And we can travel to the
past with Street View.

00:01:47.600 --> 00:01:50.770
We can see how this stadium
was being constructed.

00:01:50.770 --> 00:01:51.690
We have all this data.

00:01:51.690 --> 00:01:54.100
We are collecting everything
that is happening.

00:01:54.100 --> 00:01:55.540
But the question
today is, can we

00:01:55.540 --> 00:01:57.800
use this data to
predict what is going

00:01:57.800 --> 00:02:00.680
to happen inside this
place during the 90

00:02:00.680 --> 00:02:02.850
minutes of a match?

00:02:02.850 --> 00:02:05.625
Well, your presenters--
and today, here we

00:02:05.625 --> 00:02:06.540
have Jordan Tigani.

00:02:06.540 --> 00:02:11.680
He is one of the founding
members of the BigQuery team.

00:02:11.680 --> 00:02:15.540
He also wrote the
book on BigQuery.

00:02:15.540 --> 00:02:19.990
If you stay and go to him,
he will give you a copy

00:02:19.990 --> 00:02:21.825
and sign it too.

00:02:21.825 --> 00:02:23.280
And I'm Felipe Hoffa.

00:02:23.280 --> 00:02:26.340
I work with Google
developer relations.

00:02:26.340 --> 00:02:29.900
My job here is to do exactly
this-- to bring you the Google

00:02:29.900 --> 00:02:32.720
technologies that you
can use and show you

00:02:32.720 --> 00:02:37.990
what you can do so you can
put them for your own use.

00:02:37.990 --> 00:02:40.890
So let's go to our topic
today-- predictions.

00:02:40.890 --> 00:02:45.410
We have been doing this for a
long time-- thousands of years.

00:02:45.410 --> 00:02:49.360
And people have
used crystal balls.

00:02:49.360 --> 00:02:51.280
More recently, for
the last World Cup,

00:02:51.280 --> 00:02:54.360
this octopus did pretty well.

00:02:54.360 --> 00:02:58.090
He got like 11 of
13 matches right.

00:02:58.090 --> 00:03:00.060
We're not sure why or how.

00:03:00.060 --> 00:03:02.800
It seems he really
liked the Germany flag.

00:03:02.800 --> 00:03:04.740
But we don't know.

00:03:04.740 --> 00:03:06.450
We can't ask him, anymore.

00:03:06.450 --> 00:03:08.800
But we're going to use data.

00:03:08.800 --> 00:03:12.230
And that's what we have.

00:03:12.230 --> 00:03:15.440
When you want data,
the first place

00:03:15.440 --> 00:03:19.060
you go to search for
it-- for me it's Google.

00:03:19.060 --> 00:03:20.630
And some of the
interesting data sets

00:03:20.630 --> 00:03:25.060
I got were, for
example, the Wikipedia.

00:03:25.060 --> 00:03:27.360
Wikipedia has a
lot of knowledge.

00:03:27.360 --> 00:03:32.970
But then, it's hard to put this
data into learning algorithms.

00:03:32.970 --> 00:03:35.740
But fortunately,
we have Freebase.

00:03:35.740 --> 00:03:39.170
Freebase is a
Google project that

00:03:39.170 --> 00:03:44.290
engulfs a lot of this knowledge
as fact in SQL, in a table,

00:03:44.290 --> 00:03:45.720
in an RDF table.

00:03:45.720 --> 00:03:47.046
And we can use these facts.

00:03:47.046 --> 00:03:48.730
There are like a
million facts there

00:03:48.730 --> 00:03:52.490
of the World Cup,
players, teams.

00:03:52.490 --> 00:03:54.410
I also found this
other open data

00:03:54.410 --> 00:03:57.890
set that people are
building on this on GitHub

00:03:57.890 --> 00:03:59.930
with all the
information from soccer

00:03:59.930 --> 00:04:02.530
matches, from history,
from around the world,

00:04:02.530 --> 00:04:04.030
from the World Cup.

00:04:04.030 --> 00:04:05.280
So I got back.

00:04:05.280 --> 00:04:06.530
I got Freebase.

00:04:06.530 --> 00:04:09.050
I was able to use
this data for example

00:04:09.050 --> 00:04:13.100
to find correlations like,
do countries with a bigger

00:04:13.100 --> 00:04:17.742
population do better in
the World Cup or not?

00:04:17.742 --> 00:04:19.450
Ask me later about
that, because I really

00:04:19.450 --> 00:04:20.990
want to go to the next topic.

00:04:20.990 --> 00:04:21.950
That's Opta.

00:04:21.950 --> 00:04:23.660
This is a pretty cool data set.

00:04:23.660 --> 00:04:25.210
It's not open.

00:04:25.210 --> 00:04:27.100
But they are really
changing the way

00:04:27.100 --> 00:04:31.390
that soccer can be analyzed.

00:04:31.390 --> 00:04:34.050
For any given match,
they are recording event

00:04:34.050 --> 00:04:36.130
by event, touch by
touch, what is happening.

00:04:36.130 --> 00:04:38.180
Who is holding the ball?

00:04:38.180 --> 00:04:40.900
Where is he standing?

00:04:40.900 --> 00:04:43.440
People are able to
analyze the full history

00:04:43.440 --> 00:04:47.230
of a player, the full
history of a soccer match,

00:04:47.230 --> 00:04:48.230
in a very cool way.

00:04:48.230 --> 00:04:52.970
Just to give you an example,
with this query, BigQuery,

00:04:52.970 --> 00:04:56.460
we know there's
around 2,000 matches,

00:04:56.460 --> 00:05:01.300
1,500 rows of information
for each match.

00:05:01.300 --> 00:05:04.060
And here, I'm looking at
that particular match-- Spain

00:05:04.060 --> 00:05:04.940
versus Netherlands.

00:05:04.940 --> 00:05:09.280
I knew that there was a goal
at the particular minute-- 43.

00:05:09.280 --> 00:05:12.250
And I wanted to know more
information about it.

00:05:12.250 --> 00:05:15.230
And what did I get?

00:05:15.230 --> 00:05:21.840
I got information like, when did
it happen, who was the player,

00:05:21.840 --> 00:05:26.330
where he was located at the
moment that goal happened.

00:05:26.330 --> 00:05:30.290
And I got a lot of details
too, like yes, this

00:05:30.290 --> 00:05:32.330
was a header goal.

00:05:32.330 --> 00:05:34.520
Where was the goalie standing?

00:05:34.520 --> 00:05:37.400
Where did the ball
exactly go in?

00:05:37.400 --> 00:05:41.160
And now I'm able
to use this data.

00:05:41.160 --> 00:05:44.080
The question is, when
I have this data,

00:05:44.080 --> 00:05:45.760
how can I use it to predict?

00:05:45.760 --> 00:05:49.410
And for that, we have Jordan
our machine learning expert.

00:05:49.410 --> 00:05:51.274
So please, Jordan.

00:05:51.274 --> 00:05:52.940
JORDAN TIGANI: Thanks
very much, Felipe.

00:05:52.940 --> 00:05:54.314
So I'm going to
talk a little bit

00:05:54.314 --> 00:05:59.220
about using machine learning
to predict soccer games.

00:05:59.220 --> 00:06:01.550
But first, I want to
talk about-- soccer is

00:06:01.550 --> 00:06:03.964
really hard to predict.

00:06:03.964 --> 00:06:05.130
There's a lot of randomness.

00:06:05.130 --> 00:06:08.357
The team that's better
doesn't always win.

00:06:08.357 --> 00:06:10.440
A lot of times you see a
team, they have the ball,

00:06:10.440 --> 00:06:12.110
they're shooting the whole game.

00:06:12.110 --> 00:06:13.780
The ball just ends
up not going in.

00:06:13.780 --> 00:06:17.830
And the other team,
they get the ball once,

00:06:17.830 --> 00:06:20.127
and they go and score and win.

00:06:20.127 --> 00:06:21.085
AUDIENCE: Like Germany!

00:06:23.750 --> 00:06:27.030
JORDAN TIGANI: Yes,
exactly like Germany.

00:06:27.030 --> 00:06:31.070
This picture here is a
reenactment of a famous goal.

00:06:31.070 --> 00:06:33.220
But just recently,
just yesterday,

00:06:33.220 --> 00:06:36.540
the Argentina versus Nigeria
game, there was a shot.

00:06:36.540 --> 00:06:37.430
The keeper saved it.

00:06:37.430 --> 00:06:39.215
It went off his glove.

00:06:39.215 --> 00:06:42.030
It hit the post, came
back, hit him in the head,

00:06:42.030 --> 00:06:44.890
went off the post again, bounced
out in front of the goal,

00:06:44.890 --> 00:06:48.530
and Leo Messi came and
just blasted it in.

00:06:48.530 --> 00:06:51.460
But that's the kind of thing
that no learning algorithm

00:06:51.460 --> 00:06:54.170
is going to be able to predict.

00:06:54.170 --> 00:06:57.570
Just to give an idea
of how the experts do--

00:06:57.570 --> 00:07:01.270
these are the guys whose job
it is to predict the future.

00:07:01.270 --> 00:07:03.870
And they turn their
eye to soccer.

00:07:03.870 --> 00:07:07.690
Nate Silver, the guy who
predicted Obama's victory as

00:07:07.690 --> 00:07:11.880
well as lots of other
things, of the 16 teams

00:07:11.880 --> 00:07:14.360
he predicted would make it to
the second round, only eight

00:07:14.360 --> 00:07:16.800
are still around.

00:07:16.800 --> 00:07:18.920
Goldman Sachs had
another prediction

00:07:18.920 --> 00:07:21.570
using some of the same
techniques that we're using.

00:07:21.570 --> 00:07:24.000
And they're not doing
so great-- only 36%

00:07:24.000 --> 00:07:26.217
of their first matches.

00:07:26.217 --> 00:07:27.300
This is not a competition.

00:07:27.300 --> 00:07:28.770
We're not trying
to beat anybody.

00:07:28.770 --> 00:07:32.460
We're just trying to show how
machine learning can be used

00:07:32.460 --> 00:07:36.600
and to level set expectations.

00:07:36.600 --> 00:07:39.930
So "O Jogo Bonito" is Portuguese
for "The Beautiful Game."

00:07:39.930 --> 00:07:44.260
It was first attributed to
Pele talking about soccer.

00:07:44.260 --> 00:07:46.440
Soccer is a really,
really beautiful game.

00:07:46.440 --> 00:07:49.000
And magical things
happen on the field.

00:07:49.000 --> 00:07:52.390
And players like Leo Messi
can do incredible things

00:07:52.390 --> 00:07:53.870
with the ball.

00:07:53.870 --> 00:07:56.100
Machine learning can
be magical as well.

00:07:56.100 --> 00:08:00.100
For example, Gmail, when it
detects that all of your spam

00:08:00.100 --> 00:08:04.500
is spam and you never
see spam anymore,

00:08:04.500 --> 00:08:05.850
that can be pretty magical.

00:08:05.850 --> 00:08:09.640
Or when you visit a page
in Portuguese on Chrome,

00:08:09.640 --> 00:08:12.660
and it recognizes that and
offers to translate it,

00:08:12.660 --> 00:08:16.370
that's another application
of machine learning.

00:08:16.370 --> 00:08:18.030
But what I want to
get across today

00:08:18.030 --> 00:08:20.940
is that machine
learning is not magic.

00:08:20.940 --> 00:08:24.940
And anybody can do it.

00:08:24.940 --> 00:08:28.890
And it's acutally sort of
rather than playing soccer

00:08:28.890 --> 00:08:31.630
at Messi's level, it's more like
managing a team-- that somebody

00:08:31.630 --> 00:08:38.539
can learn to do machine
learning fairly easily.

00:08:38.539 --> 00:08:42.280
So I'm going to use an extended
metaphor about managing a team,

00:08:42.280 --> 00:08:44.017
so just bear with
me a little bit.

00:08:44.017 --> 00:08:46.350
So the first thing you do
whether you're managing a team

00:08:46.350 --> 00:08:50.670
or you're doing machine learning
is you want to set a goal.

00:08:50.670 --> 00:08:52.990
If you're the manager of
the Brazilian national team

00:08:52.990 --> 00:08:57.050
with 200 million soccer-mad
fans and players,

00:08:57.050 --> 00:08:59.960
you're obviously going to
want to win the World Cup.

00:08:59.960 --> 00:09:02.500
However, if you're
the manager of Tuvalu,

00:09:02.500 --> 00:09:07.050
a tiny island in the Pacific,
even though in theory you

00:09:07.050 --> 00:09:11.330
could qualify and win the World
Cup, that may not be your goal.

00:09:11.330 --> 00:09:13.220
Obviously, you still
want to win every game.

00:09:13.220 --> 00:09:17.669
But when you're
playing Australia,

00:09:17.669 --> 00:09:19.960
you may not assume that you're
going to win every game.

00:09:19.960 --> 00:09:24.116
Your goal may be just to beat
Vanuatu, a neighboring island.

00:09:24.116 --> 00:09:25.490
Similarly, with
machine learning,

00:09:25.490 --> 00:09:28.530
you use the resources that you
have to decide the prediction

00:09:28.530 --> 00:09:30.490
task that you're going to do.

00:09:30.490 --> 00:09:35.180
So we have this really huge
data set-- this really deep data

00:09:35.180 --> 00:09:37.370
set-- on club level soccer.

00:09:37.370 --> 00:09:39.960
So we have all the
touches for three

00:09:39.960 --> 00:09:42.410
different professional
leagues-- the English Premier

00:09:42.410 --> 00:09:46.010
League, Spanish La Liga,
and the US Major League

00:09:46.010 --> 00:09:48.670
Soccer over a couple years.

00:09:48.670 --> 00:09:51.570
And we can mine
this data to build

00:09:51.570 --> 00:09:54.285
a model of how
professional soccer works.

00:09:54.285 --> 00:09:55.660
However, that
doesn't necessarily

00:09:55.660 --> 00:09:57.030
help us with the World Cup.

00:09:57.030 --> 00:09:58.490
And so we're going
to have to try

00:09:58.490 --> 00:10:00.210
to stretch that a
little bit in order

00:10:00.210 --> 00:10:02.160
to make our models
work for the World Cup.

00:10:02.160 --> 00:10:03.870
Because there just
isn't the same depth

00:10:03.870 --> 00:10:06.800
of data available there.

00:10:06.800 --> 00:10:12.037
So once you have your data,
once you have your goals,

00:10:12.037 --> 00:10:13.370
then you want to pick your team.

00:10:13.370 --> 00:10:15.036
And a manager of a
team is going to want

00:10:15.036 --> 00:10:16.710
to pick based on
certain attributes

00:10:16.710 --> 00:10:19.020
that they believe
correlates with victory.

00:10:19.020 --> 00:10:20.500
They want fast people up front.

00:10:20.500 --> 00:10:23.255
They want people who can pass
well in midfield and people who

00:10:23.255 --> 00:10:26.880
like to throw themselves on
the ground as goalkeeper.

00:10:26.880 --> 00:10:30.480
In machine learning, the
statistics that you use-- you

00:10:30.480 --> 00:10:33.270
want to build statistics
from your data that

00:10:33.270 --> 00:10:37.560
correlate well with whatever
you're trying to predict.

00:10:37.560 --> 00:10:39.520
So for the soccer
example, that may

00:10:39.520 --> 00:10:43.295
be whether the team
takes a lot of shots,

00:10:43.295 --> 00:10:50.040
whether the team gets the
ball in the other team's

00:10:50.040 --> 00:10:52.670
third of the field a lot,
whether they pass well,

00:10:52.670 --> 00:10:54.390
whether they foul a lot.

00:10:57.080 --> 00:11:03.775
So you want to-- sorry, just
lost my train of thought.

00:11:03.775 --> 00:11:05.885
But after you've
built your features,

00:11:05.885 --> 00:11:07.510
after you've computed
those statistics,

00:11:07.510 --> 00:11:10.510
the next thing you want
to do is train your team.

00:11:10.510 --> 00:11:13.780
And the nice thing is
in machine learning,

00:11:13.780 --> 00:11:16.730
you use the same
term as in soccer.

00:11:16.730 --> 00:11:18.760
You train a machine
learning model.

00:11:18.760 --> 00:11:20.412
And you train it
to find patterns.

00:11:20.412 --> 00:11:22.370
And though that sounds
a little bit mysterious,

00:11:22.370 --> 00:11:25.380
it's actually relatively
straightforward.

00:11:25.380 --> 00:11:28.880
So we can show how this works
in a somewhat simple example.

00:11:28.880 --> 00:11:31.180
So this is data that we've
gotten from our data sets

00:11:31.180 --> 00:11:34.890
about the probability
of scoring a goal based

00:11:34.890 --> 00:11:37.967
on how far away from the
goal the shot was taken.

00:11:37.967 --> 00:11:39.800
And to a human eye, you
can see that there's

00:11:39.800 --> 00:11:40.840
a clear pattern here.

00:11:40.840 --> 00:11:43.090
But it's a little bit more
difficult to get a computer

00:11:43.090 --> 00:11:44.610
to recognize this pattern.

00:11:44.610 --> 00:11:47.830
So one way to do that is to
make the pattern simpler.

00:11:47.830 --> 00:11:49.670
And if we take the
log of the distance

00:11:49.670 --> 00:11:51.710
rather than using the
distance by itself,

00:11:51.710 --> 00:11:54.390
we have a much more
simple pattern.

00:11:54.390 --> 00:11:56.460
And we can use a linear
regression, which

00:11:56.460 --> 00:12:00.940
is a statistical tool that's
present in spreadsheet packages

00:12:00.940 --> 00:12:05.420
and people may be familiar
with, to essentially draw

00:12:05.420 --> 00:12:07.274
a line through that data.

00:12:07.274 --> 00:12:08.690
And people don't
necessarily think

00:12:08.690 --> 00:12:10.680
of this as a prediction tool.

00:12:10.680 --> 00:12:14.880
But actually, this
lets us predict,

00:12:14.880 --> 00:12:18.190
given the distance
the shot was taken,

00:12:18.190 --> 00:12:19.650
the probability
that that's going

00:12:19.650 --> 00:12:22.490
to go in for any distance.

00:12:22.490 --> 00:12:24.580
So the type of machine
learning algorithm

00:12:24.580 --> 00:12:26.850
we are going to use here
is not linear regression,

00:12:26.850 --> 00:12:28.360
but it's another
type of regression.

00:12:28.360 --> 00:12:29.840
And we're just going to fit
into a different type of curve.

00:12:29.840 --> 00:12:31.440
This is a logistic curve.

00:12:31.440 --> 00:12:33.190
And the nice things
about it is it's

00:12:33.190 --> 00:12:38.090
bound between 0 and 1, which
fits well for probabilities.

00:12:38.090 --> 00:12:42.300
The other thing is that
it more easily captures

00:12:42.300 --> 00:12:45.570
nonlinear relationships
between the data.

00:12:45.570 --> 00:12:50.140
So for the same example of a
distance from shot on target,

00:12:50.140 --> 00:12:55.400
we can fit a relatively
good line to that data

00:12:55.400 --> 00:12:58.670
without having to
translate it first.

00:12:58.670 --> 00:13:01.587
So I mentioned fitting
the line to the data.

00:13:01.587 --> 00:13:03.420
So we're going to keep
track of a statistic.

00:13:03.420 --> 00:13:04.860
And it's called r squared.

00:13:04.860 --> 00:13:08.350
And it's a measure of how
close your line is to your data

00:13:08.350 --> 00:13:08.990
points.

00:13:08.990 --> 00:13:10.370
There's not necessarily
a good rule of thumb

00:13:10.370 --> 00:13:12.390
for what constitutes a
good one or a bad one.

00:13:12.390 --> 00:13:19.890
But it's a good way of comparing
two subsequent models as you're

00:13:19.890 --> 00:13:22.170
iterating through
different ideas.

00:13:22.170 --> 00:13:25.017
So when you're fitting
data to a model,

00:13:25.017 --> 00:13:27.100
one thing you have to worry
about is over-fitting.

00:13:27.100 --> 00:13:29.530
So here's three
soccer players who

00:13:29.530 --> 00:13:32.030
have won World Footballer of
the Year in the last 15 years--

00:13:32.030 --> 00:13:35.730
Ronaldo, Ronaldinho,
and Cristiano Ronaldo.

00:13:35.730 --> 00:13:38.620
You may be able to detect a
pattern here in their names.

00:13:38.620 --> 00:13:41.090
And I can come up with
a regular expression

00:13:41.090 --> 00:13:43.220
that can capture this.

00:13:43.220 --> 00:13:44.640
But you may not
want to turn this

00:13:44.640 --> 00:13:47.550
into a predictive algorithm.

00:13:47.550 --> 00:13:48.740
Or maybe you do.

00:13:48.740 --> 00:13:50.720
So I think I can
predict who's going

00:13:50.720 --> 00:13:53.420
to be the next World
Footballer of the Year.

00:13:53.420 --> 00:13:57.220
He's been under everyone's
noses for the last 40 years--

00:13:57.220 --> 00:14:02.410
Ronald McDonald,
McDonald's mascot.

00:14:02.410 --> 00:14:05.250
Premier League Scouts, you don't
have to give me a finder's fee

00:14:05.250 --> 00:14:07.410
but maybe some cheap
tickets would be nice.

00:14:07.410 --> 00:14:09.100
But clearly, this is absurd.

00:14:09.100 --> 00:14:13.130
It's a coincidence, and
machine learning algorithms

00:14:13.130 --> 00:14:15.080
can be good at
finding coincidences.

00:14:15.080 --> 00:14:17.320
And so the way you prevent
that from happening

00:14:17.320 --> 00:14:20.539
is a technique called and
parsimony or Occam's razor.

00:14:20.539 --> 00:14:22.830
And you want to find the
simplest possible explanation.

00:14:22.830 --> 00:14:24.970
And the way you do
that in an algorithm

00:14:24.970 --> 00:14:27.920
is you penalize complexity.

00:14:27.920 --> 00:14:30.920
So if you try to come up
with some really bizarre,

00:14:30.920 --> 00:14:33.580
complex model,
that model will not

00:14:33.580 --> 00:14:38.120
score as highly as another one.

00:14:38.120 --> 00:14:40.530
Whether you're building a
team, or your building models,

00:14:40.530 --> 00:14:41.840
you want to keep score.

00:14:41.840 --> 00:14:43.340
So usually when you
train your data,

00:14:43.340 --> 00:14:44.771
you break it into two pieces.

00:14:44.771 --> 00:14:46.895
One piece is your training
set, and the other piece

00:14:46.895 --> 00:14:47.900
is your test set.

00:14:47.900 --> 00:14:50.690
The training set is the
data that you actually

00:14:50.690 --> 00:14:53.150
want to use to build the model.

00:14:53.150 --> 00:14:55.380
The test set, on the other
hand, you keep to the side

00:14:55.380 --> 00:14:58.640
so that you can validate
that that model is not just

00:14:58.640 --> 00:15:01.190
regurgitating facts that it
already knows, but can actually

00:15:01.190 --> 00:15:03.260
be used on things it
hasn't seen before.

00:15:06.220 --> 00:15:10.660
So a metric that we use to
see how well we're doing

00:15:10.660 --> 00:15:12.410
is called lift.

00:15:12.410 --> 00:15:14.170
And that just tells,
how much better

00:15:14.170 --> 00:15:15.710
are we doing than random chance?

00:15:15.710 --> 00:15:18.520
Accuracy would be the
obvious thing to use.

00:15:18.520 --> 00:15:20.970
But the problem
with accuracy is it

00:15:20.970 --> 00:15:23.860
doesn't reflect
underlying probabilities.

00:15:23.860 --> 00:15:26.690
For example, soccer has
three possible outcomes.

00:15:26.690 --> 00:15:30.790
So if you can guess 50%
right 50% of the time,

00:15:30.790 --> 00:15:32.620
then you're actually
doing pretty well.

00:15:32.620 --> 00:15:34.244
But if you're just
looking at accuracy,

00:15:34.244 --> 00:15:37.180
50% sounds like
it's a coin flip.

00:15:37.180 --> 00:15:41.880
So lift actually lets us
measure that more accurately.

00:15:41.880 --> 00:15:44.230
So here's another,
more complicated way

00:15:44.230 --> 00:15:45.550
of looking at accuracy.

00:15:45.550 --> 00:15:47.910
And this is the
receiver operator curve,

00:15:47.910 --> 00:15:49.740
which is kind of a strange term.

00:15:49.740 --> 00:15:53.900
But it comes from
finding U-boats

00:15:53.900 --> 00:15:57.530
by radio detection
in World War II.

00:15:57.530 --> 00:15:59.470
And the name stuck.

00:15:59.470 --> 00:16:01.546
But we have this
dotted line going

00:16:01.546 --> 00:16:03.420
from the bottom left
corner to the top right.

00:16:03.420 --> 00:16:04.930
And that's pure luck.

00:16:04.930 --> 00:16:07.100
So if you're right
on that line, that

00:16:07.100 --> 00:16:09.160
means your model is terrible.

00:16:09.160 --> 00:16:11.410
So the further you get away
from that line the better.

00:16:11.410 --> 00:16:14.610
So I won't go into details
about what this necessarily

00:16:14.610 --> 00:16:17.480
means, but we'll see
it a little later when

00:16:17.480 --> 00:16:19.337
we're comparing models.

00:16:19.337 --> 00:16:20.920
The other thing about
machine learning

00:16:20.920 --> 00:16:23.210
is it's an iterative process.

00:16:23.210 --> 00:16:27.400
You try it once, you keep
trying, and it may not work.

00:16:27.400 --> 00:16:33.340
You need to sort of
put more creativity

00:16:33.340 --> 00:16:35.410
into coming up with
good features that

00:16:35.410 --> 00:16:37.980
are going to work for you.

00:16:37.980 --> 00:16:40.990
So then once you have your
model, you've built the model,

00:16:40.990 --> 00:16:44.080
you've tested it,
you've iterated on it,

00:16:44.080 --> 00:16:48.150
you want to do the thing
that we're all here to do,

00:16:48.150 --> 00:16:51.920
which is we want to
actually use it to predict.

00:16:51.920 --> 00:16:56.020
And so I'm going to switch
to an iPython notebook.

00:16:56.020 --> 00:17:02.800
And so iPython is sort
of the visualization

00:17:02.800 --> 00:17:07.690
of a suite of scientific
tools for Python

00:17:07.690 --> 00:17:11.859
that lets you do lots of
mathematical and scientific

00:17:11.859 --> 00:17:14.560
programming and has some
built-in machine learning

00:17:14.560 --> 00:17:15.690
algorithms as well.

00:17:24.829 --> 00:17:28.280
OK, hey, it worked.

00:17:28.280 --> 00:17:31.560
So you don't necessarily
need to understand

00:17:31.560 --> 00:17:33.000
every line of this code.

00:17:33.000 --> 00:17:37.470
I'm just going to go through--
this is how it would work

00:17:37.470 --> 00:17:39.340
and this is how you can do it.

00:17:39.340 --> 00:17:42.433
So this iPython notebook is
connected on the back end

00:17:42.433 --> 00:17:45.720
to a Google Compute
Engine instance.

00:17:45.720 --> 00:17:48.805
You could also connect it
to a cluster of instances.

00:17:48.805 --> 00:17:53.030
And we did that for some of
the more hard core computation

00:17:53.030 --> 00:17:54.670
that we have done.

00:17:54.670 --> 00:17:56.690
It also is using a
pandas, which has

00:17:56.690 --> 00:17:59.110
a convenient BigQuery
connector here.

00:17:59.110 --> 00:18:02.730
You can see that with
essentially one line of code,

00:18:02.730 --> 00:18:05.590
you can run a BigQuery and turn
it into a pandas data frame

00:18:05.590 --> 00:18:07.390
and operate on it that way.

00:18:07.390 --> 00:18:09.560
And generally, the
first thing you

00:18:09.560 --> 00:18:10.959
want to do in
machine learning is

00:18:10.959 --> 00:18:12.500
you want to get a
feel for your data.

00:18:12.500 --> 00:18:14.660
And so we'll just see
how big our data is.

00:18:14.660 --> 00:18:18.950
We've got over 2,000 matches,
1,700 touches per match.

00:18:18.950 --> 00:18:23.100
And then, as Felipe showed, we
have multiple facts per touch.

00:18:23.100 --> 00:18:26.230
And there's about 13 million
of those in our data set.

00:18:31.210 --> 00:18:32.700
Then we want to
build our features.

00:18:32.700 --> 00:18:37.500
And our futures are built by
this actually very complicated

00:18:37.500 --> 00:18:38.530
BigQuery query.

00:18:38.530 --> 00:18:43.960
It's 11,000 characters long,
15 JOINs, and eight GROUP BYs.

00:18:43.960 --> 00:18:46.920
So I won't show that here,
because it wouldn't necessarily

00:18:46.920 --> 00:18:47.690
be interesting.

00:18:47.690 --> 00:18:51.300
But with BigQuery, we can
just send one giant query

00:18:51.300 --> 00:18:55.420
to BigQuery and let it worry
about it rather than having

00:18:55.420 --> 00:18:58.590
to sort of build a pipeline
or re-pull data down

00:18:58.590 --> 00:19:00.845
and operate on it iteratively.

00:19:04.850 --> 00:19:10.049
So the next step is
just run the query.

00:19:10.049 --> 00:19:11.590
We're going to split
up the World Cup

00:19:11.590 --> 00:19:12.970
data from the club data.

00:19:12.970 --> 00:19:16.310
Because we want to build our
model on just the club data.

00:19:16.310 --> 00:19:18.660
And so here's an example
of what our features look

00:19:18.660 --> 00:19:23.190
like for-- actually, this is
the Chile-Netherlands game.

00:19:23.190 --> 00:19:25.900
Sorry about that one, Felipe.

00:19:25.900 --> 00:19:31.160
So the Netherlands won 2-0.

00:19:31.160 --> 00:19:35.390
But we have the pass rates in
various parts of the field.

00:19:35.390 --> 00:19:40.767
We have how many goals were
scored in previous games.

00:19:40.767 --> 00:19:42.350
And actually, the
way we're doing this

00:19:42.350 --> 00:19:45.120
is we're taking--
in order to predict

00:19:45.120 --> 00:19:48.010
what's going to happen
in one particular game,

00:19:48.010 --> 00:19:50.670
we're taking an average of
statistics that happened

00:19:50.670 --> 00:19:54.790
in the last three games,
assuming that those will sort

00:19:54.790 --> 00:19:56.440
of hold in the next game.

00:19:56.440 --> 00:19:58.480
And if you're following
the World Cup,

00:19:58.480 --> 00:20:01.480
you may notice that there's
a reason for that three

00:20:01.480 --> 00:20:03.779
number, which is
right now, every team

00:20:03.779 --> 00:20:05.070
has played exactly three games.

00:20:05.070 --> 00:20:07.700
So we have three
games that we can

00:20:07.700 --> 00:20:14.710
use to inform our models when
we're looking at the World Cup.

00:20:14.710 --> 00:20:17.210
As I mentioned, again, you want
to get a feel for your data.

00:20:17.210 --> 00:20:21.840
This is building crosstabs
of goals to whether you win,

00:20:21.840 --> 00:20:23.210
tie, or lose.

00:20:23.210 --> 00:20:26.370
And so we can see here
in the top left corner,

00:20:26.370 --> 00:20:29.800
if you don't score any goals,
you're going to lose about 2/3

00:20:29.800 --> 00:20:32.990
of the time, whereas kind
of towards the bottom right,

00:20:32.990 --> 00:20:36.400
we see that if you
score six goals or more,

00:20:36.400 --> 00:20:39.190
you're more or less
guaranteed to win.

00:20:39.190 --> 00:20:42.070
The nice thing about--
this particular crosstab

00:20:42.070 --> 00:20:46.690
helped us find a bug when
we were debugging BigQuery.

00:20:46.690 --> 00:20:51.690
Because in the World
Cup, or in tournaments,

00:20:51.690 --> 00:20:55.100
in the knockout stages,
if the game ends in a tie,

00:20:55.100 --> 00:20:56.600
then it goes into penalty kicks.

00:20:56.600 --> 00:20:59.640
And so we end up
seeing teams that

00:20:59.640 --> 00:21:01.677
scored seven goals
who lost, which

00:21:01.677 --> 00:21:02.760
sounds a little bit weird.

00:21:02.760 --> 00:21:05.780
But the reason was
because in our data,

00:21:05.780 --> 00:21:07.860
those all showed
as regular goals.

00:21:07.860 --> 00:21:10.610
And clearly, we want to
strip those out of our data

00:21:10.610 --> 00:21:11.690
when building the model.

00:21:11.690 --> 00:21:15.110
So kind of playing with
the data a little bit

00:21:15.110 --> 00:21:19.570
can help you prevent
bugs in your system.

00:21:19.570 --> 00:21:21.650
So next, we want
to train our model.

00:21:21.650 --> 00:21:24.095
And we're going to split
out the test set here

00:21:24.095 --> 00:21:25.720
that we're going to
use for validation.

00:21:25.720 --> 00:21:30.360
You can see the r squared
is about 12% on this one.

00:21:30.360 --> 00:21:33.240
The model training also lets
us do feature selection.

00:21:33.240 --> 00:21:35.940
And it tells us which features
turned out to be important

00:21:35.940 --> 00:21:37.890
for that model,
which ones didn't.

00:21:37.890 --> 00:21:41.160
So we can see the features
that correlated positively

00:21:41.160 --> 00:21:46.270
with wins, whether you're
playing at home-- actually,

00:21:46.270 --> 00:21:48.420
home-field advantage
is a huge deal

00:21:48.420 --> 00:21:51.630
in soccer-- things like
how well you passed

00:21:51.630 --> 00:21:54.122
in the opposing
third of the field.

00:21:54.122 --> 00:21:56.580
There were some features that
were dropped that just turned

00:21:56.580 --> 00:22:01.150
out to be not interesting all
by the Occam's razor principle--

00:22:01.150 --> 00:22:02.800
how many fouls
your opponents had

00:22:02.800 --> 00:22:06.982
or how many shots your opponents
had had in previous games.

00:22:06.982 --> 00:22:08.440
And there's also
negative features,

00:22:08.440 --> 00:22:12.280
which are features that mean
you're less likely to win.

00:22:12.280 --> 00:22:15.400
And so opp_op_pass is a
little bit of a weird name,

00:22:15.400 --> 00:22:22.320
but it's how well people
passed against your opponent.

00:22:22.320 --> 00:22:28.420
Opp_expected_goals is your
opponents-- how many shots

00:22:28.420 --> 00:22:31.930
they had and how many goals you
would've expected them to have,

00:22:31.930 --> 00:22:33.620
et cetera.

00:22:33.620 --> 00:22:39.050
So then we want to predict
wins in club level data.

00:22:41.620 --> 00:22:44.500
So we run some
quick predictions.

00:22:44.500 --> 00:22:46.010
And so here's some examples.

00:22:46.010 --> 00:22:49.390
So we predicted Manchester
United would beat Wigan.

00:22:49.390 --> 00:22:51.340
That's probably not a
surprise to anybody.

00:22:51.340 --> 00:22:53.220
And we got that one right.

00:22:53.220 --> 00:22:57.030
We predicted Manchester United
would beat Tottenham Hotspur,

00:22:57.030 --> 00:23:00.040
which people would
expect would happen.

00:23:00.040 --> 00:23:05.160
But actually, Tottenham
Hotspur upset Man United.

00:23:05.160 --> 00:23:07.620
But these are sort of anecdotes.

00:23:07.620 --> 00:23:12.000
And you want to be able to
collect these into actual data.

00:23:12.000 --> 00:23:13.270
So we can validate this.

00:23:13.270 --> 00:23:15.650
We can compute the
lift statistic.

00:23:15.650 --> 00:23:17.255
So we have the lift is 41%.

00:23:17.255 --> 00:23:21.020
So we're doing 41%
better than random.

00:23:21.020 --> 00:23:22.630
And here's the ROC curve.

00:23:22.630 --> 00:23:25.130
And this one isn't
necessarily that interesting

00:23:25.130 --> 00:23:28.310
until we show the next model.

00:23:28.310 --> 00:23:31.540
So one thing, if you're looking
at the last three games,

00:23:31.540 --> 00:23:34.522
one problem is you may have
had three really easy games.

00:23:34.522 --> 00:23:36.230
You may have had three
really hard games.

00:23:36.230 --> 00:23:39.349
And so we want to be able to
coordinate those together.

00:23:39.349 --> 00:23:41.640
And so what we do is we run
another logistic regression

00:23:41.640 --> 00:23:45.800
that ends up stack
ranking all of the games

00:23:45.800 --> 00:23:50.220
by sort of building a lattice
among who's better than whom

00:23:50.220 --> 00:23:52.239
in terms of all the
teams that have played.

00:23:52.239 --> 00:23:54.155
So we build that, do
that logistic regression,

00:23:54.155 --> 00:23:55.850
and we compute a
power statistic.

00:23:55.850 --> 00:23:59.290
And if you've seen the
FIFA Coca-Cola power

00:23:59.290 --> 00:24:01.850
statistic for soccer,
or college football

00:24:01.850 --> 00:24:05.394
has power rankings, that's
essentially how that's created.

00:24:05.394 --> 00:24:07.310
And we're going to compute
that power ranking.

00:24:07.310 --> 00:24:09.018
We're going to add
that back to our model

00:24:09.018 --> 00:24:10.090
as one of our features.

00:24:10.090 --> 00:24:13.691
So when we compute
the power statistic

00:24:13.691 --> 00:24:15.190
and add it back to
the model, we can

00:24:15.190 --> 00:24:18.450
see that our lift
goes from 41% to 55%.

00:24:18.450 --> 00:24:21.970
And our ROC curve, we can
see it moves further away

00:24:21.970 --> 00:24:26.280
from that luck
line in the middle.

00:24:26.280 --> 00:24:30.240
The new one is the blue line.

00:24:30.240 --> 00:24:33.460
OK, so we've
predicted club soccer,

00:24:33.460 --> 00:24:35.460
but we want to be able
to predict the World Cup.

00:24:35.460 --> 00:24:38.270
That's what we're here for.

00:24:38.270 --> 00:24:42.200
So we basically have to apply
some of the same techniques

00:24:42.200 --> 00:24:42.861
here.

00:24:42.861 --> 00:24:45.110
There's a lot of code here,
but I wouldn't necessarily

00:24:45.110 --> 00:24:45.890
worry about it.

00:24:45.890 --> 00:24:47.970
And we can run the predictions.

00:24:47.970 --> 00:24:49.880
And here's our predictions.

00:24:49.880 --> 00:24:51.570
And before we go
too far into that,

00:24:51.570 --> 00:24:53.070
I'm going to give
it back to Felipe.

00:24:53.070 --> 00:24:54.903
And we'll talk more
about those in a minute.

00:24:54.903 --> 00:24:56.960
FELIPE HOFFA: Thank
you, you're done.

00:24:56.960 --> 00:24:58.330
Let me take this.

00:24:58.330 --> 00:25:03.034
So let's go to
half time if I can.

00:25:03.034 --> 00:25:03.910
Where's the button?

00:25:03.910 --> 00:25:05.020
Here.

00:25:05.020 --> 00:25:05.600
Present.

00:25:05.600 --> 00:25:07.370
Awesome.

00:25:07.370 --> 00:25:09.010
So what do we have here?

00:25:09.010 --> 00:25:10.250
What did we learn?

00:25:10.250 --> 00:25:13.510
We know how we can
predict, use this data.

00:25:13.510 --> 00:25:17.650
We can use iPython, pandas
to do a linear regression,

00:25:17.650 --> 00:25:20.190
a logarithmic regression.

00:25:20.190 --> 00:25:24.100
But do you want to know what
are our actual predictions?

00:25:24.100 --> 00:25:29.410
For example, this Saturday, 9:00
AM, Brazil is playing Chile.

00:25:29.410 --> 00:25:33.240
Who thinks Brazil
is going to win?

00:25:33.240 --> 00:25:34.815
Chile?

00:25:34.815 --> 00:25:35.315
Come on.

00:25:39.220 --> 00:25:41.470
Machine learning
said that Brazil

00:25:41.470 --> 00:25:45.320
will win with a 72% chance.

00:25:45.320 --> 00:25:46.340
You were close.

00:25:46.340 --> 00:25:47.340
Columbia versus Uruguay?

00:25:50.720 --> 00:25:52.810
JORDAN TIGANI: I think
they're both good teams.

00:25:52.810 --> 00:25:57.360
But now that Luis Suarez
is out for biting,

00:25:57.360 --> 00:26:01.200
I think Columbia is
probably going to do it.

00:26:01.200 --> 00:26:03.540
FELIPE HOFFA: Let's see what
our machine learning said.

00:26:03.540 --> 00:26:05.730
69%, OK.

00:26:05.730 --> 00:26:07.478
Netherlands versus Mexico.

00:26:07.478 --> 00:26:09.190
AUDIENCE: Mexico.

00:26:09.190 --> 00:26:11.320
FELIPE HOFFA: Mexico?

00:26:11.320 --> 00:26:13.160
JORDAN TIGANI: My guess
would be Netherlands

00:26:13.160 --> 00:26:15.260
having seen them play Spain.

00:26:15.260 --> 00:26:17.720
But Mexico is a good team, too.

00:26:17.720 --> 00:26:21.580
FELIPE HOFFA: Computer
says 55% Netherlands.

00:26:21.580 --> 00:26:24.610
We can call this
too close to call,

00:26:24.610 --> 00:26:27.340
so we have to wait for this.

00:26:27.340 --> 00:26:28.330
Well done, Mexico.

00:26:28.330 --> 00:26:30.124
Costa Rica versus Greece?

00:26:30.124 --> 00:26:31.540
JORDAN TIGANI:
That's a tough one.

00:26:31.540 --> 00:26:35.301
I'm going to have
to go with Greece.

00:26:35.301 --> 00:26:36.550
FELIPE HOFFA: Costa Rica, 60%.

00:26:36.550 --> 00:26:38.240
France versus Nigeria?

00:26:40.510 --> 00:26:42.260
JORDAN TIGANI: That
should be an easy one.

00:26:42.260 --> 00:26:43.540
France.

00:26:43.540 --> 00:26:47.150
FELIPE HOFFA: 92%-- that's
a lot of confidence there.

00:26:47.150 --> 00:26:48.990
Argentina versus Switzerland?

00:26:48.990 --> 00:26:51.530
JORDAN TIGANI: Argentina looks
hard to beat this World Cup.

00:26:51.530 --> 00:26:53.000
FELIPE HOFFA: 68%.

00:26:53.000 --> 00:26:55.150
Germany versus Algeria?

00:26:55.150 --> 00:26:57.160
JORDAN TIGANI: Germany
looks even better.

00:26:57.160 --> 00:26:59.040
FELIPE HOFFA: 93%.

00:26:59.040 --> 00:27:00.884
Belgium versus-- Belgium?

00:27:00.884 --> 00:27:01.710
AUDIENCE: USA.

00:27:01.710 --> 00:27:02.460
FELIPE HOFFA: USA?

00:27:04.910 --> 00:27:07.090
68%.

00:27:07.090 --> 00:27:09.220
We will see.

00:27:09.220 --> 00:27:10.970
Who will be the finalists?

00:27:10.970 --> 00:27:12.190
I don't know.

00:27:12.190 --> 00:27:14.320
We might try to guess.

00:27:14.320 --> 00:27:16.930
We think it's Brazil
versus Argentina.

00:27:16.930 --> 00:27:24.730
And the prediction is 55%, so we
can call it too close to call.

00:27:24.730 --> 00:27:27.410
Now, whoever is watching
this video in the future

00:27:27.410 --> 00:27:29.350
might know what all happened.

00:27:29.350 --> 00:27:30.680
So please let me know.

00:27:30.680 --> 00:27:31.540
You're this video.

00:27:31.540 --> 00:27:33.456
JORDAN TIGANI: We can
always change the video.

00:27:33.456 --> 00:27:36.660
FELIPE HOFFA: Yes, we
can change the video.

00:27:36.660 --> 00:27:38.130
So yes.

00:27:38.130 --> 00:27:42.130
So yes, not only can
we change the view--

00:27:42.130 --> 00:27:43.150
we're not doing that.

00:27:43.150 --> 00:27:45.950
But things change
minute by minute.

00:27:45.950 --> 00:27:48.430
We ran our static
predictions here.

00:27:48.430 --> 00:27:52.307
But sometimes you might have
the best team, the best players,

00:27:52.307 --> 00:27:54.140
and things change in
the first five minutes,

00:27:54.140 --> 00:27:56.860
and you lose your best players.

00:27:56.860 --> 00:28:00.546
Or one of your players
might bite too hard.

00:28:00.546 --> 00:28:03.800
That changed the whole
outcome of the game.

00:28:03.800 --> 00:28:08.360
And as you saw yesterday with
Eric Schmidt at the keynote--

00:28:08.360 --> 00:28:12.050
the cool Eric Schmidt--
we are able to stream

00:28:12.050 --> 00:28:13.970
all the data using Dataflow.

00:28:13.970 --> 00:28:17.360
We can get second by
second what is happening.

00:28:17.360 --> 00:28:21.615
And the question here
is, can we use that data?

00:28:21.615 --> 00:28:25.080
The facts are changing during
every second around the world.

00:28:25.080 --> 00:28:25.820
Can we use it?

00:28:25.820 --> 00:28:29.080
Can we put them inside
of our prediction models?

00:28:29.080 --> 00:28:31.100
Jordan?

00:28:31.100 --> 00:28:33.070
JORDAN TIGANI: OK,
let's see we can do.

00:28:33.070 --> 00:28:36.270
So you guys may have
seen in the keynote--

00:28:36.270 --> 00:28:42.980
this is a screenshot from
the demo op that we built.

00:28:42.980 --> 00:28:44.930
It's also been available
in the sandbox,

00:28:44.930 --> 00:28:49.026
but this shows minute by minute
what we sort of expect to have.

00:28:49.026 --> 00:28:50.900
So we want to build a
real-time model that'll

00:28:50.900 --> 00:28:57.990
let us know from inside the game
who we think is going to win.

00:28:57.990 --> 00:29:00.700
The way we're going to do
that is a Markov Monte Carlo

00:29:00.700 --> 00:29:01.700
simulation.

00:29:01.700 --> 00:29:05.282
And that may sound
rather confusing.

00:29:05.282 --> 00:29:07.490
But essentially what it is
is it's just a video game.

00:29:10.292 --> 00:29:11.000
There's a player.

00:29:11.000 --> 00:29:12.980
He's got the ball
somewhere on the field.

00:29:12.980 --> 00:29:14.850
And he has some options.

00:29:14.850 --> 00:29:16.230
One option might be to pass.

00:29:16.230 --> 00:29:17.510
One option might be to shoot.

00:29:17.510 --> 00:29:20.510
One option might
be to get fouled.

00:29:20.510 --> 00:29:22.920
And we compute
conditional probabilities

00:29:22.920 --> 00:29:25.210
for each of these
possible events.

00:29:25.210 --> 00:29:27.010
And then, we traverse
a state machine

00:29:27.010 --> 00:29:30.090
based on the conditional
probabilities.

00:29:30.090 --> 00:29:33.680
Then we can iterate
through that state machine

00:29:33.680 --> 00:29:34.780
to run an entire game.

00:29:34.780 --> 00:29:37.330
So we can see-- OK,
we run through that,

00:29:37.330 --> 00:29:41.851
and Team A won, and
they scored three goals.

00:29:41.851 --> 00:29:43.350
So that doesn't
necessarily tell you

00:29:43.350 --> 00:29:46.340
too much about what
the probabilities are.

00:29:46.340 --> 00:29:48.170
But we can get
probabilities by running

00:29:48.170 --> 00:29:50.170
this 10,000 times in parallel.

00:29:50.170 --> 00:29:53.220
And then each time Team A
wins, we give Team A a point.

00:29:53.220 --> 00:29:55.370
Each time Team B wins,
we give Team B a point.

00:29:55.370 --> 00:29:58.230
And then the we can
compute the probability

00:29:58.230 --> 00:30:01.730
of the outcome that way.

00:30:01.730 --> 00:30:03.620
And so this is sort
of what the Dataflow

00:30:03.620 --> 00:30:05.190
pipeline looks like it.

00:30:05.190 --> 00:30:07.790
Don't necessarily worry
about what those boxes mean.

00:30:07.790 --> 00:30:11.250
So we have this data
that's coming in from Opta.

00:30:11.250 --> 00:30:14.930
That touch by touch data, we
get that record by record.

00:30:14.930 --> 00:30:17.080
And we can sort of
add that to our model.

00:30:17.080 --> 00:30:20.500
So what can we do to
incorporate that data

00:30:20.500 --> 00:30:22.680
into a machine learning model?

00:30:22.680 --> 00:30:25.490
So what we can do is we
can use the data that's

00:30:25.490 --> 00:30:29.100
coming in to know exactly where
we are in the game, exactly

00:30:29.100 --> 00:30:34.730
where we are in our Monte
Carlo Markov chain model.

00:30:34.730 --> 00:30:40.720
Sorry, I'm-- anyway, we can run
the game forward until the end.

00:30:40.720 --> 00:30:43.720
And then we can
compute probabilities

00:30:43.720 --> 00:30:45.810
of Team A versus Team
B winning from where

00:30:45.810 --> 00:30:48.200
we are in the current game.

00:30:48.200 --> 00:30:51.680
And so I can show a
quick demo of this.

00:30:51.680 --> 00:30:54.862
This has been up at the sandbox
for the last couple days.

00:30:54.862 --> 00:30:56.570
So you may have seen
Eric Schmidt run it.

00:30:56.570 --> 00:30:58.480
And can you just run that?

00:30:58.480 --> 00:31:04.560
So we're seeing basically
this Monte Carlo

00:31:04.560 --> 00:31:06.760
running of an entire game.

00:31:06.760 --> 00:31:09.346
And then, you can see
histograms of who's going to win

00:31:09.346 --> 00:31:10.470
and who's not going to win.

00:31:10.470 --> 00:31:14.130
And I think this is
Brazil versus Croatia?

00:31:14.130 --> 00:31:15.520
I don't know.

00:31:15.520 --> 00:31:22.320
In the sandbox, we were
showing Brazil versus Croatia.

00:31:22.320 --> 00:31:23.480
Can we switch back?

00:31:23.480 --> 00:31:25.688
We don't necessarily need
to wait until this is done.

00:31:29.104 --> 00:31:30.020
FELIPE HOFFA: Go back.

00:31:30.020 --> 00:31:31.340
JORDAN TIGANI: And that's it.

00:31:31.340 --> 00:31:33.020
FELIPE HOFFA: So
what did we learn?

00:31:33.020 --> 00:31:35.650
It's almost the end.

00:31:35.650 --> 00:31:39.340
Machine learning is like
training your own team--

00:31:39.340 --> 00:31:42.030
set a goal, pick your
features, do the training,

00:31:42.030 --> 00:31:45.710
measure the lift,
practice, predict,

00:31:45.710 --> 00:31:49.970
and after all that's
happening, try to understand.

00:31:49.970 --> 00:31:53.210
Look where you can
find correlations.

00:31:53.210 --> 00:31:57.360
And the next step is
finding [? conversation ?].

00:31:57.360 --> 00:32:00.390
We might say that making
predictions is hard.

00:32:00.390 --> 00:32:02.840
But it turns out
it is very easy.

00:32:02.840 --> 00:32:04.830
We just made it.

00:32:04.830 --> 00:32:08.340
It's harder to be right.

00:32:08.340 --> 00:32:10.810
But it's even harder-- or
more interesting-- to make

00:32:10.810 --> 00:32:11.310
them happen.

00:32:11.310 --> 00:32:14.910
Because as you
understand the world,

00:32:14.910 --> 00:32:17.460
you can use this information,
not only in soccer,

00:32:17.460 --> 00:32:20.620
not only to win the next match.

00:32:20.620 --> 00:32:22.140
You can use all
the data you have

00:32:22.140 --> 00:32:26.440
to find patterns inside your
city, how your city's moving,

00:32:26.440 --> 00:32:28.620
how the world is moving,
where things are going,

00:32:28.620 --> 00:32:30.760
and how you can change it.

00:32:30.760 --> 00:32:32.640
And with this data,
with these tools,

00:32:32.640 --> 00:32:35.390
you can go from
divination to really

00:32:35.390 --> 00:32:39.240
having the world in your hands.

00:32:39.240 --> 00:32:40.780
Thank you very much.

00:32:40.780 --> 00:32:41.730
Use this.

00:32:41.730 --> 00:32:42.987
Go change the world.

00:32:42.987 --> 00:32:51.369
[APPLAUSE]

00:32:51.369 --> 00:32:52.535
FELIPE HOFFA: Any questions?

00:32:55.080 --> 00:32:56.500
Thank you for staying.

00:32:56.500 --> 00:33:01.330
I know it's over, but we have
10 minutes for questions?

00:33:01.330 --> 00:33:02.190
Or less.

00:33:02.190 --> 00:33:03.090
AUDIENCE: Yeah.

00:33:03.090 --> 00:33:06.565
So the obvious question
is, why didn't you

00:33:06.565 --> 00:33:08.315
use Google Prediction
API, and why did you

00:33:08.315 --> 00:33:10.369
decide to build
your own algorithm?

00:33:10.369 --> 00:33:11.785
JORDAN TIGANI: So
one thing I want

00:33:11.785 --> 00:33:13.368
to say about the
Google Prediction API

00:33:13.368 --> 00:33:17.770
is that the Google Prediction--
the underlying algorithms

00:33:17.770 --> 00:33:18.730
are awesome.

00:33:18.730 --> 00:33:20.530
It's actually the
Google Prediction API

00:33:20.530 --> 00:33:26.860
uses the same technology as most
of Google's machine learning.

00:33:26.860 --> 00:33:29.700
And it's used throughout ads,
throughout spam detection,

00:33:29.700 --> 00:33:33.940
throughout language detection.

00:33:33.940 --> 00:33:38.210
So it's not that-- the
prediction API is not a toy.

00:33:38.210 --> 00:33:40.550
We didn't use it
because we wanted

00:33:40.550 --> 00:33:42.520
to show how all the bells
and whistles worked.

00:33:42.520 --> 00:33:45.380
So the prediction API
kind of abstracts you away

00:33:45.380 --> 00:33:48.490
from some of the more funky
or perhaps interesting

00:33:48.490 --> 00:33:50.555
parts of machine learning.

00:33:50.555 --> 00:33:51.930
So we wanted to
just sort of give

00:33:51.930 --> 00:33:54.570
a little bit of machine learning
primer as we were going.

00:33:54.570 --> 00:33:54.990
AUDIENCE: So that
you could understand

00:33:54.990 --> 00:33:57.047
the coefficient influence
of each feature?

00:33:57.047 --> 00:33:59.380
JORDAN TIGANI: Yeah, and
that's one thing that you don't

00:33:59.380 --> 00:34:01.390
get from the
prediction API is sort

00:34:01.390 --> 00:34:03.494
of the feedback about
feature selection.

00:34:03.494 --> 00:34:04.160
AUDIENCE: Right.

00:34:04.160 --> 00:34:09.110
Did you compare your coefficient
versus what Google Prediction

00:34:09.110 --> 00:34:12.670
API did, and did you do a
benchmark against these two

00:34:12.670 --> 00:34:13.530
things?

00:34:13.530 --> 00:34:16.070
JORDAN TIGANI: We haven't
done that, actually.

00:34:16.070 --> 00:34:18.239
AUDIENCE: Thank you.

00:34:18.239 --> 00:34:22.730
AUDIENCE: My question is, what
tools, or maybe languages,

00:34:22.730 --> 00:34:25.550
you use for past prototyping?

00:34:25.550 --> 00:34:27.469
As I understand, it is Python.

00:34:27.469 --> 00:34:29.090
But maybe something else?

00:34:29.090 --> 00:34:32.100
I mean, you don't use
Google Prediction API

00:34:32.100 --> 00:34:35.280
for your own prototyping here.

00:34:35.280 --> 00:34:38.179
JORDAN TIGANI: So Python
is used heavily at Google.

00:34:38.179 --> 00:34:43.100
So iPython is really nice
to be able to predict.

00:34:43.100 --> 00:34:46.750
There's some things that are
missing in Python, pandas, et

00:34:46.750 --> 00:34:48.520
cetera.

00:34:48.520 --> 00:34:52.449
But the nice thing about that
is you can call into R as well.

00:34:52.449 --> 00:35:00.490
So R has vast libraries of
scientific applications.

00:35:00.490 --> 00:35:02.990
AUDIENCE: And a second
personal question,

00:35:02.990 --> 00:35:07.800
I think-- what kind of data
sets do you like to learn?

00:35:10.900 --> 00:35:12.330
Each of them.

00:35:12.330 --> 00:35:17.190
FELIPE HOFFA: Yeah, so I don't
exactly hear the question.

00:35:17.190 --> 00:35:19.000
What data sets we like to--

00:35:19.000 --> 00:35:20.625
AUDIENCE: What kind
of large data sets.

00:35:20.625 --> 00:35:22.500
I see football
data sets that you

00:35:22.500 --> 00:35:24.560
try to learn, or
maybe something else.

00:35:24.560 --> 00:35:26.430
FELIPE HOFFA: Yeah,
look at the [? delt ?].

00:35:26.430 --> 00:35:29.013
With the [? delt ?] you can see
what is happening in the world

00:35:29.013 --> 00:35:30.450
right now.

00:35:30.450 --> 00:35:33.960
Last week someone uploaded
all the taxi rides in New York

00:35:33.960 --> 00:35:35.240
from 2013.

00:35:35.240 --> 00:35:38.950
And just to be able to see every
pattern, how people are moving,

00:35:38.950 --> 00:35:43.770
there are vast areas
to discover there.

00:35:43.770 --> 00:35:48.060
I also love looking
at the Wikipedia logs.

00:35:48.060 --> 00:35:49.520
DNA.

00:35:49.520 --> 00:35:54.550
You can go into genetics
also with the query.

00:35:54.550 --> 00:35:56.970
There are a lot of
impressive things you can do.

00:35:56.970 --> 00:35:59.770
AUDIENCE: And you?

00:35:59.770 --> 00:36:02.960
JORDAN TIGANI: Anything
I can get my hands on.

00:36:02.960 --> 00:36:06.060
Soccer is a passion
of mine, and so I

00:36:06.060 --> 00:36:10.969
was really excited to get the
data to be able to do this.

00:36:10.969 --> 00:36:11.510
AUDIENCE: OK.

00:36:11.510 --> 00:36:12.950
Thank you.

00:36:12.950 --> 00:36:14.870
JORDAN TIGANI: Sure.

00:36:14.870 --> 00:36:16.450
FELIPE HOFFA: You get a book.

00:36:16.450 --> 00:36:17.000
JORDAN TIGANI: You get a book.

00:36:17.000 --> 00:36:17.860
You get a book.

00:36:17.860 --> 00:36:22.130
AUDIENCE: My question is,
what visualization library

00:36:22.130 --> 00:36:28.330
did you use for the animation
after the Dataflow example?

00:36:28.330 --> 00:36:30.510
So how did you do
the visualization

00:36:30.510 --> 00:36:36.169
from Python and pandas to the
time series thing you did.

00:36:36.169 --> 00:36:37.960
JORDAN TIGANI: So that
was just JavaScript.

00:36:37.960 --> 00:36:41.960
That's just raw JavaScript
that we put together

00:36:41.960 --> 00:36:43.659
to demonstrate that.

00:36:43.659 --> 00:36:44.200
AUDIENCE: OK.

00:36:44.200 --> 00:36:48.664
And for the prediction, did use
a library like scikit-learn?

00:36:48.664 --> 00:36:49.830
JORDAN TIGANI: Yes, exactly.

00:36:49.830 --> 00:36:50.622
That's what we did.

00:36:50.622 --> 00:36:51.163
AUDIENCE: OK.

00:36:51.163 --> 00:36:51.830
Thank you.

00:36:51.830 --> 00:36:53.640
FELIPE HOFFA: Sure, thanks.

00:36:53.640 --> 00:36:55.540
You get a book.

00:36:55.540 --> 00:36:56.130
Hello.

00:36:56.130 --> 00:36:58.430
AUDIENCE: I'm actually
a novice to BigQuery

00:36:58.430 --> 00:37:01.130
and machine learning
algorithms for predictions.

00:37:01.130 --> 00:37:06.860
So my question is, once I get my
data set migrated into big data

00:37:06.860 --> 00:37:10.050
and some sort of
database there, and I

00:37:10.050 --> 00:37:13.170
want to start making predictions
and figuring out patterns

00:37:13.170 --> 00:37:18.330
in my data, how do I go about--
what machine learning algorithm

00:37:18.330 --> 00:37:20.240
options are there
for me in order

00:37:20.240 --> 00:37:22.220
to start putting
together those patterns

00:37:22.220 --> 00:37:24.654
and start making my predictions?

00:37:24.654 --> 00:37:26.320
JORDAN TIGANI: I would
say start simple.

00:37:26.320 --> 00:37:29.270
Logistic regression
is pretty simple.

00:37:29.270 --> 00:37:31.440
Linear regression
is pretty simple.

00:37:31.440 --> 00:37:35.976
RandomForest, the decision
trees are pretty simple.

00:37:35.976 --> 00:37:37.600
AUDIENCE: Are there
APIs that integrate

00:37:37.600 --> 00:37:38.975
those kinds of
regression models?

00:37:38.975 --> 00:37:39.766
JORDAN TIGANI: Yes.

00:37:39.766 --> 00:37:41.500
So actually one of
the other questioners

00:37:41.500 --> 00:37:43.210
just asked about scikit-learn.

00:37:43.210 --> 00:37:46.680
Scikit-learn has packages.

00:37:46.680 --> 00:37:50.950
You basically just feed your
data in and out comes a model.

00:37:50.950 --> 00:37:54.540
And you feed your new data
in, and out comes predictions.

00:37:54.540 --> 00:37:56.921
So they make it pretty easy.

00:37:56.921 --> 00:37:57.920
AUDIENCE: OK, thank you.

00:37:57.920 --> 00:37:59.670
FELIPE HOFFA: There's
a lot of exploration

00:37:59.670 --> 00:38:01.510
you can do with just SQL.

00:38:01.510 --> 00:38:03.830
I loaded these data
sets in BigQuery.

00:38:03.830 --> 00:38:08.310
So I ran, OK, do as
I was saying earlier.

00:38:08.310 --> 00:38:09.780
Countries with a
bigger population

00:38:09.780 --> 00:38:12.960
are correlated to
making more goals.

00:38:12.960 --> 00:38:17.190
The correlation was zero
until I removed China

00:38:17.190 --> 00:38:18.080
from the data set.

00:38:18.080 --> 00:38:20.770
And then I got 0.3 correlations.

00:38:20.770 --> 00:38:26.200
Then I removed the USA, and
the correlation went up to 0.4.

00:38:26.200 --> 00:38:30.030
And that kind of exploration
that you can get data back

00:38:30.030 --> 00:38:34.580
in seconds allows you to
see where you should look,

00:38:34.580 --> 00:38:37.010
how you should
segment your data.

00:38:37.010 --> 00:38:41.520
So for example, there is a big
correlation there if we only

00:38:41.520 --> 00:38:43.910
take countries
that call football

00:38:43.910 --> 00:38:46.814
'football', for example.

00:38:46.814 --> 00:38:48.230
JORDAN TIGANI: But
I think it also

00:38:48.230 --> 00:38:52.700
shows that if you're going to
spend time worrying about sort

00:38:52.700 --> 00:38:54.275
of building machine
learning models,

00:38:54.275 --> 00:38:56.400
the thing you really should
focus most of your time

00:38:56.400 --> 00:39:01.380
on, at least at first, is
really features in your data,

00:39:01.380 --> 00:39:05.780
being able to clean your
data, like Felipe was saying.

00:39:05.780 --> 00:39:08.430
And the actual algorithms
almost don't matter.

00:39:08.430 --> 00:39:11.520
At least, they matter in the
noise and a couple percentage

00:39:11.520 --> 00:39:12.540
points here and there.

00:39:12.540 --> 00:39:14.600
And you can tweak
those and tune those

00:39:14.600 --> 00:39:16.992
once you're happy with the
data that they're putting in.

00:39:16.992 --> 00:39:18.950
But if you don't have
good data, if there's not

00:39:18.950 --> 00:39:22.110
signal in that data, then the
best algorithm in the world

00:39:22.110 --> 00:39:26.030
is not going to be able
to predict accurately.

00:39:26.030 --> 00:39:27.050
AUDIENCE: Thank you.

00:39:27.050 --> 00:39:28.720
FELIPE HOFFA: You get a book.

00:39:28.720 --> 00:39:31.320
AUDIENCE: I want
to ask you about

00:39:31.320 --> 00:39:33.830
if there are any books that
you recommend about machine

00:39:33.830 --> 00:39:39.960
learning-- besides that one--
and also tools besides the two

00:39:39.960 --> 00:39:43.087
that were said here at the talk.

00:39:43.087 --> 00:39:45.170
JORDAN TIGANI: So for books
for machine learning--

00:39:45.170 --> 00:39:47.050
there's Russell
and Norvig is sort

00:39:47.050 --> 00:39:51.590
of the standard one
that is awesome.

00:39:51.590 --> 00:39:56.040
Peter Norvig is the chief
of research or something

00:39:56.040 --> 00:39:56.540
at Google.

00:39:56.540 --> 00:39:58.460
And he's pretty brilliant.

00:39:58.460 --> 00:40:02.810
And so I highly
recommend that one.

00:40:02.810 --> 00:40:08.910
Other tools-- there's
R. R might be easier

00:40:08.910 --> 00:40:11.820
to get started with than pandas,
because it has some better

00:40:11.820 --> 00:40:13.210
documentation.

00:40:13.210 --> 00:40:17.670
But I would take-- there
are some learning data sets.

00:40:17.670 --> 00:40:20.020
One's called like Iris
Setosa or something.

00:40:20.020 --> 00:40:23.140
It's about predicting
flower genetics.

00:40:23.140 --> 00:40:27.030
It could be fun to just
run through an example

00:40:27.030 --> 00:40:30.560
and give it a try and find
that it's not all that hard.

00:40:30.560 --> 00:40:31.450
AUDIENCE: Thank you.

00:40:31.450 --> 00:40:33.350
JORDAN TIGANI: Sure.

00:40:33.350 --> 00:40:34.360
AUDIENCE: Thank you.

00:40:34.360 --> 00:40:36.950
So I have a question
about discoveries.

00:40:36.950 --> 00:40:38.430
The cool feature
of BigQuery-- this

00:40:38.430 --> 00:40:40.013
is a generic question
about BigQuery--

00:40:40.013 --> 00:40:42.190
is you can put
data sets in there

00:40:42.190 --> 00:40:45.050
and make it open to
everybody, right?

00:40:45.050 --> 00:40:47.830
And I know my friend did it,
and he shared it with me.

00:40:47.830 --> 00:40:50.420
I mean, he shared the link
with me, so I found it.

00:40:50.420 --> 00:40:53.840
But how do you find other--
because we shouldn't all

00:40:53.840 --> 00:40:55.880
replicate New York City data.

00:40:55.880 --> 00:40:58.140
I have it, he has it, right?

00:40:58.140 --> 00:41:01.210
Is there a way to
discover other--

00:41:01.210 --> 00:41:04.640
FELIPE HOFFA: So I can
tell you what I do here.

00:41:04.640 --> 00:41:06.820
Whenever I find an
interesting data set,

00:41:06.820 --> 00:41:08.860
I can upload it to BigQuery.

00:41:08.860 --> 00:41:11.560
Once it's in BigQuery, you
can set the public, data

00:41:11.560 --> 00:41:14.840
and you can share
with other people.

00:41:14.840 --> 00:41:18.390
I have a couple of websites
where I like sharing things.

00:41:18.390 --> 00:41:22.650
So if you go to
reddit.com/r/bigquery,

00:41:22.650 --> 00:41:26.350
every data set, every news I
learn about these data sets

00:41:26.350 --> 00:41:28.170
sharing BigQuery,
I place it there.

00:41:28.170 --> 00:41:30.850
So subscribe to that one.

00:41:30.850 --> 00:41:34.610
Also my friend, Ilya
Grigorik, also Google,

00:41:34.610 --> 00:41:41.370
he has the site bigqueri.es
like big queries.

00:41:41.370 --> 00:41:43.950
There we share a lot
of interesting queries

00:41:43.950 --> 00:41:46.470
that people write.

00:41:46.470 --> 00:41:49.550
Go there, there are really a
lot of interesting data sets.

00:41:49.550 --> 00:41:53.215
And my call to action here is,
if you have an interesting data

00:41:53.215 --> 00:41:56.170
set, put it on BigQuery,
set it as Public,

00:41:56.170 --> 00:42:00.100
and let anyone else
instantly analyze it.

00:42:00.100 --> 00:42:02.160
AUDIENCE: But is
there any plan to have

00:42:02.160 --> 00:42:03.740
a Google way of doing it?

00:42:03.740 --> 00:42:07.540
Like in Fusion Table, you
can just look for public,

00:42:07.540 --> 00:42:10.790
and you just give a word, and
it finds all the public ones,

00:42:10.790 --> 00:42:13.520
you know what I mean?

00:42:13.520 --> 00:42:17.210
JORDAN TIGANI: The BigQuery
team, we take feature requests,

00:42:17.210 --> 00:42:20.940
and things the users are
asking for we will build.

00:42:20.940 --> 00:42:23.770
We haven't had--
that was something

00:42:23.770 --> 00:42:26.549
we had thought users would
want a lot of is discovery.

00:42:26.549 --> 00:42:28.590
And then, we didn't get
a lot of requests for it,

00:42:28.590 --> 00:42:29.714
so we haven't built it yet.

00:42:29.714 --> 00:42:32.250
So we have a public
issue tracker

00:42:32.250 --> 00:42:34.320
where you can set
feature requests.

00:42:34.320 --> 00:42:36.620
I would encourage you
to request that feature.

00:42:36.620 --> 00:42:40.279
And then hopefully, we
can get started on that.

00:42:40.279 --> 00:42:41.154
FELIPE HOFFA: Thanks.

00:42:44.640 --> 00:42:46.354
AUDIENCE: So I'm
really new to BigQuery.

00:42:46.354 --> 00:42:48.270
And I'm also a little
bit late to the section,

00:42:48.270 --> 00:42:51.170
so I apologize in advance if
you guys already covered this.

00:42:51.170 --> 00:42:53.610
But you used the
example of World Cup.

00:42:53.610 --> 00:42:57.260
And I've seen that in some
countries where gambling

00:42:57.260 --> 00:43:00.990
is legal, there's lots of
websites that will provide tips

00:43:00.990 --> 00:43:03.150
on which teams to bet on.

00:43:03.150 --> 00:43:06.940
Have you seen any website using
BigQuery and the prediction

00:43:06.940 --> 00:43:10.130
API to give out tips
to people and monetize

00:43:10.130 --> 00:43:16.120
that type of service, and if so,
does it match with your ethic?

00:43:16.120 --> 00:43:18.362
Or how do you guys
feel about it?

00:43:18.362 --> 00:43:19.820
JORDAN TIGANI: We
don't really know

00:43:19.820 --> 00:43:22.891
what users are using BigQuery
for unless they tell us

00:43:22.891 --> 00:43:23.390
about it.

00:43:23.390 --> 00:43:27.310
And as far as I know, there's
nobody that's doing that.

00:43:27.310 --> 00:43:31.630
We have people use it for
financial analysis or even

00:43:31.630 --> 00:43:33.840
Bitcoin arbitrage.

00:43:33.840 --> 00:43:38.842
But no, I haven't heard about
anybody using it for gambling.

00:43:38.842 --> 00:43:41.050
FELIPE HOFFA: Then again,
this is not company advice.

00:43:41.050 --> 00:43:43.556
Please don't use our
predictions to gamble.

00:43:43.556 --> 00:43:45.556
AUDIENCE: Disclaimer--
don't promote it, either.

00:43:45.556 --> 00:43:47.430
I'm just wondering.

00:43:47.430 --> 00:43:50.730
FELIPE HOFFA: Every time you
use data for decision making,

00:43:50.730 --> 00:43:52.100
you're making a prediction.

00:43:52.100 --> 00:43:53.980
Every time you make
a decision, you're

00:43:53.980 --> 00:43:56.140
gambling on that happening.

00:43:56.140 --> 00:43:58.870
So it's not pure gambling.

00:43:58.870 --> 00:44:00.900
But yes, we are always
making decisions

00:44:00.900 --> 00:44:02.990
and somehow predicting--

00:44:02.990 --> 00:44:05.900
AUDIENCE: How accurate would you
say are all those predictions?

00:44:05.900 --> 00:44:09.564
You just showed us predictions
of which team is going to win.

00:44:09.564 --> 00:44:11.230
Have you guys done
some sort of research

00:44:11.230 --> 00:44:15.090
on how many times actually it
was matched to the team that--

00:44:15.090 --> 00:44:16.840
JORDAN TIGANI: They're
extremely accurate.

00:44:16.840 --> 00:44:22.489
AUDIENCE: Whoa, thank you.

00:44:22.489 --> 00:44:24.030
JORDAN TIGANI: I
don't actually know.

00:44:24.030 --> 00:44:26.560
I hope they're very good.

00:44:26.560 --> 00:44:29.320
They did pretty well
on the training sets,

00:44:29.320 --> 00:44:32.710
or on the test set,
the Premiere League.

00:44:32.710 --> 00:44:34.590
But time will tell.

00:44:34.590 --> 00:44:35.347
We'll see.

00:44:35.347 --> 00:44:36.430
AUDIENCE: OK, cool Thanks.

00:44:36.430 --> 00:44:36.730
JORDAN TIGANI: Sure.

00:44:36.730 --> 00:44:37.890
FELIPE HOFFA: Thank you.

00:44:37.890 --> 00:44:40.416
We have 30 seconds left?

00:44:40.416 --> 00:44:43.040
AUDIENCE: Yeah, my question was
basically already covered, so--

00:44:43.040 --> 00:44:43.860
JORDAN TIGANI: OK.

00:44:43.860 --> 00:44:44.776
You just want to book?

00:44:47.870 --> 00:44:49.870
AUDIENCE: Quick question, then.

00:44:49.870 --> 00:44:53.970
I see you use
Dataflow and Python.

00:44:53.970 --> 00:44:56.810
Is it in the pipeline to
support Python and Dataflow?

00:44:56.810 --> 00:44:59.372
Because all the examples
were only showing Java.

00:44:59.372 --> 00:45:01.070
JORDAN TIGANI: That's
a good question.

00:45:01.070 --> 00:45:04.650
I don't know of any plans, but I
don't know of any plans not to.

00:45:04.650 --> 00:45:05.470
FELIPE HOFFA: Yeah.

00:45:05.470 --> 00:45:08.320
The plan with Dataflow is
to be language agnostic.

00:45:08.320 --> 00:45:09.830
But we had to start somewhere.

00:45:09.830 --> 00:45:11.767
We started with Java.

00:45:11.767 --> 00:45:12.600
AUDIENCE: Thank you.

00:45:12.600 --> 00:45:14.600
FELIPE HOFFA: But that's
a good feature request.

00:45:14.600 --> 00:45:16.474
JORDAN TIGANI: Sounds
like we're out of time,

00:45:16.474 --> 00:45:17.730
but I'm happy to stick around.

00:45:17.730 --> 00:45:19.481
Actually, since
Google I/O is over,

00:45:19.481 --> 00:45:20.980
I'm happy to ask
any more questions.

00:45:20.980 --> 00:45:23.130
Or if people want
me to sign a book,

00:45:23.130 --> 00:45:24.542
I'll be happy to sign a book.

00:45:24.542 --> 00:45:25.500
FELIPE HOFFA: Its over.

00:45:25.500 --> 00:45:27.040
Thank you, very much.

00:45:27.040 --> 00:45:29.270
I'll see you next
year, or around.

00:45:29.270 --> 00:45:29.770
Thanks.

00:45:29.770 --> 00:45:31.320
[APPLAUSE]

