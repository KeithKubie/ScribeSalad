WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.629
 
during the history of deep learning many

00:00:02.629 --> 00:00:02.639
during the history of deep learning many
 

00:00:02.639 --> 00:00:04.460
during the history of deep learning many
researchers including some very

00:00:04.460 --> 00:00:04.470
researchers including some very
 

00:00:04.470 --> 00:00:06.110
researchers including some very
well-known researchers sometimes

00:00:06.110 --> 00:00:06.120
well-known researchers sometimes
 

00:00:06.120 --> 00:00:07.880
well-known researchers sometimes
proposed optimization algorithms and

00:00:07.880 --> 00:00:07.890
proposed optimization algorithms and
 

00:00:07.890 --> 00:00:09.230
proposed optimization algorithms and
show their work well in a few problems

00:00:09.230 --> 00:00:09.240
show their work well in a few problems
 

00:00:09.240 --> 00:00:11.270
show their work well in a few problems
but those optimization algorithms

00:00:11.270 --> 00:00:11.280
but those optimization algorithms
 

00:00:11.280 --> 00:00:13.370
but those optimization algorithms
subsequently will show not to really

00:00:13.370 --> 00:00:13.380
subsequently will show not to really
 

00:00:13.380 --> 00:00:15.860
subsequently will show not to really
generalize that well to the wide range

00:00:15.860 --> 00:00:15.870
generalize that well to the wide range
 

00:00:15.870 --> 00:00:17.390
generalize that well to the wide range
of neural networks you might want to

00:00:17.390 --> 00:00:17.400
of neural networks you might want to
 

00:00:17.400 --> 00:00:19.580
of neural networks you might want to
train so over time I think the deep

00:00:19.580 --> 00:00:19.590
train so over time I think the deep
 

00:00:19.590 --> 00:00:21.320
train so over time I think the deep
learning community actually develops

00:00:21.320 --> 00:00:21.330
learning community actually develops
 

00:00:21.330 --> 00:00:23.840
learning community actually develops
some amount of skepticism about new

00:00:23.840 --> 00:00:23.850
some amount of skepticism about new
 

00:00:23.850 --> 00:00:26.060
some amount of skepticism about new
optimization algorithms and a lot of

00:00:26.060 --> 00:00:26.070
optimization algorithms and a lot of
 

00:00:26.070 --> 00:00:27.740
optimization algorithms and a lot of
people felt that you know gradient

00:00:27.740 --> 00:00:27.750
people felt that you know gradient
 

00:00:27.750 --> 00:00:29.660
people felt that you know gradient
descent with momentum really works well

00:00:29.660 --> 00:00:29.670
descent with momentum really works well
 

00:00:29.670 --> 00:00:31.669
descent with momentum really works well
is difficult to propose things that work

00:00:31.669 --> 00:00:31.679
is difficult to propose things that work
 

00:00:31.679 --> 00:00:32.510
is difficult to propose things that work
much better

00:00:32.510 --> 00:00:32.520
much better
 

00:00:32.520 --> 00:00:35.990
much better
so rmsprop and the atom optimization

00:00:35.990 --> 00:00:36.000
so rmsprop and the atom optimization
 

00:00:36.000 --> 00:00:37.160
so rmsprop and the atom optimization
algorithm which to talk about in this

00:00:37.160 --> 00:00:37.170
algorithm which to talk about in this
 

00:00:37.170 --> 00:00:39.530
algorithm which to talk about in this
video is one of those rare algorithms

00:00:39.530 --> 00:00:39.540
video is one of those rare algorithms
 

00:00:39.540 --> 00:00:42.200
video is one of those rare algorithms
that has really stood up and has been

00:00:42.200 --> 00:00:42.210
that has really stood up and has been
 

00:00:42.210 --> 00:00:44.840
that has really stood up and has been
shown to work well across a wide range

00:00:44.840 --> 00:00:44.850
shown to work well across a wide range
 

00:00:44.850 --> 00:00:47.119
shown to work well across a wide range
of deep learning architectures so

00:00:47.119 --> 00:00:47.129
of deep learning architectures so
 

00:00:47.129 --> 00:00:48.529
of deep learning architectures so
there's only algorithms that what it

00:00:48.529 --> 00:00:48.539
there's only algorithms that what it
 

00:00:48.539 --> 00:00:50.990
there's only algorithms that what it
hesitate to recommend you try because

00:00:50.990 --> 00:00:51.000
hesitate to recommend you try because
 

00:00:51.000 --> 00:00:52.639
hesitate to recommend you try because
many people have tried it and seen it

00:00:52.639 --> 00:00:52.649
many people have tried it and seen it
 

00:00:52.649 --> 00:00:55.430
many people have tried it and seen it
work well on many problems and the atom

00:00:55.430 --> 00:00:55.440
work well on many problems and the atom
 

00:00:55.440 --> 00:00:57.080
work well on many problems and the atom
optimization algorithm is basically

00:00:57.080 --> 00:00:57.090
optimization algorithm is basically
 

00:00:57.090 --> 00:01:00.260
optimization algorithm is basically
taking momentum and rmsprop and putting

00:01:00.260 --> 00:01:00.270
taking momentum and rmsprop and putting
 

00:01:00.270 --> 00:01:02.209
taking momentum and rmsprop and putting
them together so let's see how that

00:01:02.209 --> 00:01:02.219
them together so let's see how that
 

00:01:02.219 --> 00:01:04.700
them together so let's see how that
works to implement atom you would

00:01:04.700 --> 00:01:04.710
works to implement atom you would
 

00:01:04.710 --> 00:01:10.010
works to implement atom you would
initialize vgw equals 0 s DW equals 0

00:01:10.010 --> 00:01:10.020
initialize vgw equals 0 s DW equals 0
 

00:01:10.020 --> 00:01:15.499
initialize vgw equals 0 s DW equals 0
and similarly V DB s DB equals 0 and

00:01:15.499 --> 00:01:15.509
and similarly V DB s DB equals 0 and
 

00:01:15.509 --> 00:01:21.440
and similarly V DB s DB equals 0 and
then on iteration T you would compute

00:01:21.440 --> 00:01:21.450
then on iteration T you would compute
 

00:01:21.450 --> 00:01:26.570
then on iteration T you would compute
the reserve it is compute V WB be using

00:01:26.570 --> 00:01:26.580
the reserve it is compute V WB be using
 

00:01:26.580 --> 00:01:30.890
the reserve it is compute V WB be using
current mini-batch so usually you do

00:01:30.890 --> 00:01:30.900
current mini-batch so usually you do
 

00:01:30.900 --> 00:01:32.929
current mini-batch so usually you do
this with mini-batch gradient descent

00:01:32.929 --> 00:01:32.939
this with mini-batch gradient descent
 

00:01:32.939 --> 00:01:36.490
this with mini-batch gradient descent
and then you do the momentum

00:01:36.490 --> 00:01:36.500
and then you do the momentum
 

00:01:36.500 --> 00:01:39.560
and then you do the momentum
exponentially weighted average so VT w

00:01:39.560 --> 00:01:39.570
exponentially weighted average so VT w
 

00:01:39.570 --> 00:01:42.260
exponentially weighted average so VT w
equals beta but now I'm going to call

00:01:42.260 --> 00:01:42.270
equals beta but now I'm going to call
 

00:01:42.270 --> 00:01:44.870
equals beta but now I'm going to call
this beta 1 to distinguish it from the

00:01:44.870 --> 00:01:44.880
this beta 1 to distinguish it from the
 

00:01:44.880 --> 00:01:49.249
this beta 1 to distinguish it from the
hyper parameter beta 2 we'll use for the

00:01:49.249 --> 00:01:49.259
hyper parameter beta 2 we'll use for the
 

00:01:49.259 --> 00:01:55.039
hyper parameter beta 2 we'll use for the
RMS portion of this so this is exactly

00:01:55.039 --> 00:01:55.049
RMS portion of this so this is exactly
 

00:01:55.049 --> 00:01:57.980
RMS portion of this so this is exactly
what we had when we're implementing

00:01:57.980 --> 00:01:57.990
what we had when we're implementing
 

00:01:57.990 --> 00:02:00.319
what we had when we're implementing
momentum except that I've now called the

00:02:00.319 --> 00:02:00.329
momentum except that I've now called the
 

00:02:00.329 --> 00:02:03.350
momentum except that I've now called the
hyper parameter beta 1 instead of beta

00:02:03.350 --> 00:02:03.360
hyper parameter beta 1 instead of beta
 

00:02:03.360 --> 00:02:10.059
hyper parameter beta 1 instead of beta
and similarly you have V DB as follows

00:02:10.059 --> 00:02:10.069
and similarly you have V DB as follows
 

00:02:10.069 --> 00:02:15.880
and similarly you have V DB as follows
1 minus beta 1 x DB and then you do the

00:02:15.880 --> 00:02:15.890
1 minus beta 1 x DB and then you do the
 

00:02:15.890 --> 00:02:18.610
1 minus beta 1 x DB and then you do the
rmsprop like updates as well

00:02:18.610 --> 00:02:18.620
rmsprop like updates as well
 

00:02:18.620 --> 00:02:19.959
rmsprop like updates as well
so now you have a different hyper

00:02:19.959 --> 00:02:19.969
so now you have a different hyper
 

00:02:19.969 --> 00:02:25.449
so now you have a different hyper
parameter beta 2 plus 1 minus beta 2 DW

00:02:25.449 --> 00:02:25.459
parameter beta 2 plus 1 minus beta 2 DW
 

00:02:25.459 --> 00:02:28.259
parameter beta 2 plus 1 minus beta 2 DW
squared again the squaring there is

00:02:28.259 --> 00:02:28.269
squared again the squaring there is
 

00:02:28.269 --> 00:02:30.750
squared again the squaring there is
element wise squaring of your

00:02:30.750 --> 00:02:30.760
element wise squaring of your
 

00:02:30.760 --> 00:02:35.920
element wise squaring of your
derivatives BW and then s DB is equal to

00:02:35.920 --> 00:02:35.930
derivatives BW and then s DB is equal to
 

00:02:35.930 --> 00:02:44.110
derivatives BW and then s DB is equal to
this plus 1 minus beta 2 times DB so

00:02:44.110 --> 00:02:44.120
this plus 1 minus beta 2 times DB so
 

00:02:44.120 --> 00:02:48.369
this plus 1 minus beta 2 times DB so
this is the momentum like update with

00:02:48.369 --> 00:02:48.379
this is the momentum like update with
 

00:02:48.379 --> 00:02:50.789
this is the momentum like update with
hyper param 2 beta 1 and this is the

00:02:50.789 --> 00:02:50.799
hyper param 2 beta 1 and this is the
 

00:02:50.799 --> 00:02:53.860
hyper param 2 beta 1 and this is the
rmsprop like updating with hyper

00:02:53.860 --> 00:02:53.870
rmsprop like updating with hyper
 

00:02:53.870 --> 00:02:56.379
rmsprop like updating with hyper
parameter beta 2 in the typical

00:02:56.379 --> 00:02:56.389
parameter beta 2 in the typical
 

00:02:56.389 --> 00:02:59.979
parameter beta 2 in the typical
implementation of atom you do implement

00:02:59.979 --> 00:02:59.989
implementation of atom you do implement
 

00:02:59.989 --> 00:03:02.699
implementation of atom you do implement
bias correction so you can f be

00:03:02.699 --> 00:03:02.709
bias correction so you can f be
 

00:03:02.709 --> 00:03:05.679
bias correction so you can f be
perfected corrected means after bias

00:03:05.679 --> 00:03:05.689
perfected corrected means after bias
 

00:03:05.689 --> 00:03:11.470
perfected corrected means after bias
correction DW equals v DW divided by 1

00:03:11.470 --> 00:03:11.480
correction DW equals v DW divided by 1
 

00:03:11.480 --> 00:03:14.649
correction DW equals v DW divided by 1
minus beta 1 so power of T if you've

00:03:14.649 --> 00:03:14.659
minus beta 1 so power of T if you've
 

00:03:14.659 --> 00:03:18.180
minus beta 1 so power of T if you've
done T iterations and similarly B DB

00:03:18.180 --> 00:03:18.190
done T iterations and similarly B DB
 

00:03:18.190 --> 00:03:22.750
done T iterations and similarly B DB
corrected equals V DV divided by 1 minus

00:03:22.750 --> 00:03:22.760
corrected equals V DV divided by 1 minus
 

00:03:22.760 --> 00:03:25.300
corrected equals V DV divided by 1 minus
beta 1 to the power of T and then

00:03:25.300 --> 00:03:25.310
beta 1 to the power of T and then
 

00:03:25.310 --> 00:03:27.939
beta 1 to the power of T and then
similarly you implement this on bias

00:03:27.939 --> 00:03:27.949
similarly you implement this on bias
 

00:03:27.949 --> 00:03:32.229
similarly you implement this on bias
correction on s as well so that's s DW

00:03:32.229 --> 00:03:32.239
correction on s as well so that's s DW
 

00:03:32.239 --> 00:03:36.939
correction on s as well so that's s DW
divided by 1 minus beta 2 to the T and s

00:03:36.939 --> 00:03:36.949
divided by 1 minus beta 2 to the T and s
 

00:03:36.949 --> 00:03:43.589
divided by 1 minus beta 2 to the T and s
DB corrected equals s DB

00:03:43.589 --> 00:03:43.599
DB corrected equals s DB
 

00:03:43.599 --> 00:03:48.240
DB corrected equals s DB
divided by 1 1 is beta 2 to the T

00:03:48.240 --> 00:03:48.250
divided by 1 1 is beta 2 to the T
 

00:03:48.250 --> 00:03:51.580
divided by 1 1 is beta 2 to the T
finally you perform the update so W gets

00:03:51.580 --> 00:03:51.590
finally you perform the update so W gets
 

00:03:51.590 --> 00:03:55.420
finally you perform the update so W gets
updated as W minus alpha at times so if

00:03:55.420 --> 00:03:55.430
updated as W minus alpha at times so if
 

00:03:55.430 --> 00:03:57.309
updated as W minus alpha at times so if
you're just implementing momentum you'd

00:03:57.309 --> 00:03:57.319
you're just implementing momentum you'd
 

00:03:57.319 --> 00:04:03.939
you're just implementing momentum you'd
use v DW or maybe VG w corrected but now

00:04:03.939 --> 00:04:03.949
use v DW or maybe VG w corrected but now
 

00:04:03.949 --> 00:04:06.759
use v DW or maybe VG w corrected but now
we add in the rmsprop portion of this so

00:04:06.759 --> 00:04:06.769
we add in the rmsprop portion of this so
 

00:04:06.769 --> 00:04:08.050
we add in the rmsprop portion of this so
we're also going to divide by square

00:04:08.050 --> 00:04:08.060
we're also going to divide by square
 

00:04:08.060 --> 00:04:12.999
we're also going to divide by square
root of s DW corrected plus Epsilon and

00:04:12.999 --> 00:04:13.009
root of s DW corrected plus Epsilon and
 

00:04:13.009 --> 00:04:17.020
root of s DW corrected plus Epsilon and
similarly B gets updated as similar

00:04:17.020 --> 00:04:17.030
similarly B gets updated as similar
 

00:04:17.030 --> 00:04:20.159
similarly B gets updated as similar
formula the DP

00:04:20.159 --> 00:04:20.169
formula the DP
 

00:04:20.169 --> 00:04:23.640
formula the DP
directed divided by square root s

00:04:23.640 --> 00:04:23.650
directed divided by square root s
 

00:04:23.650 --> 00:04:29.309
directed divided by square root s
corrected DB plus Epsilon and so this

00:04:29.309 --> 00:04:29.319
corrected DB plus Epsilon and so this
 

00:04:29.319 --> 00:04:32.159
corrected DB plus Epsilon and so this
algorithm combines the effect of

00:04:32.159 --> 00:04:32.169
algorithm combines the effect of
 

00:04:32.169 --> 00:04:34.260
algorithm combines the effect of
gradient descent with momentum together

00:04:34.260 --> 00:04:34.270
gradient descent with momentum together
 

00:04:34.270 --> 00:04:37.080
gradient descent with momentum together
with gradient descent of rmsprop and

00:04:37.080 --> 00:04:37.090
with gradient descent of rmsprop and
 

00:04:37.090 --> 00:04:39.659
with gradient descent of rmsprop and
this is a commonly used learning

00:04:39.659 --> 00:04:39.669
this is a commonly used learning
 

00:04:39.669 --> 00:04:41.550
this is a commonly used learning
algorithm that's proven to be very

00:04:41.550 --> 00:04:41.560
algorithm that's proven to be very
 

00:04:41.560 --> 00:04:43.649
algorithm that's proven to be very
effective for many different neural

00:04:43.649 --> 00:04:43.659
effective for many different neural
 

00:04:43.659 --> 00:04:45.390
effective for many different neural
networks of a very wide variety of

00:04:45.390 --> 00:04:45.400
networks of a very wide variety of
 

00:04:45.400 --> 00:04:47.850
networks of a very wide variety of
architectures so this algorithm has a

00:04:47.850 --> 00:04:47.860
architectures so this algorithm has a
 

00:04:47.860 --> 00:04:50.730
architectures so this algorithm has a
number of hyper parameters the learning

00:04:50.730 --> 00:04:50.740
number of hyper parameters the learning
 

00:04:50.740 --> 00:04:53.850
number of hyper parameters the learning
rate hyper parameter alpha is still

00:04:53.850 --> 00:04:53.860
rate hyper parameter alpha is still
 

00:04:53.860 --> 00:04:56.459
rate hyper parameter alpha is still
important and usually needs to be tuned

00:04:56.459 --> 00:04:56.469
important and usually needs to be tuned
 

00:04:56.469 --> 00:04:59.820
important and usually needs to be tuned
so you just have to try range of values

00:04:59.820 --> 00:04:59.830
so you just have to try range of values
 

00:04:59.830 --> 00:05:02.939
so you just have to try range of values
and see what works a comment or is

00:05:02.939 --> 00:05:02.949
and see what works a comment or is
 

00:05:02.949 --> 00:05:05.189
and see what works a comment or is
really the default choice for beta 1 is

00:05:05.189 --> 00:05:05.199
really the default choice for beta 1 is
 

00:05:05.199 --> 00:05:08.189
really the default choice for beta 1 is
0.9 so this is the moving average wrote

00:05:08.189 --> 00:05:08.199
0.9 so this is the moving average wrote
 

00:05:08.199 --> 00:05:11.519
0.9 so this is the moving average wrote
an average of DWI this is the momentum

00:05:11.519 --> 00:05:11.529
an average of DWI this is the momentum
 

00:05:11.529 --> 00:05:15.209
an average of DWI this is the momentum
light term the high parameter for beta 2

00:05:15.209 --> 00:05:15.219
light term the high parameter for beta 2
 

00:05:15.219 --> 00:05:17.640
light term the high parameter for beta 2
the authors of the Adam paper inventors

00:05:17.640 --> 00:05:17.650
the authors of the Adam paper inventors
 

00:05:17.650 --> 00:05:21.450
the authors of the Adam paper inventors
the Adams album recommend 0.99 induces

00:05:21.450 --> 00:05:21.460
the Adams album recommend 0.99 induces
 

00:05:21.460 --> 00:05:23.640
the Adams album recommend 0.99 induces
computing the moving weighted average of

00:05:23.640 --> 00:05:23.650
computing the moving weighted average of
 

00:05:23.650 --> 00:05:26.399
computing the moving weighted average of
DW squared as well as DP squared and

00:05:26.399 --> 00:05:26.409
DW squared as well as DP squared and
 

00:05:26.409 --> 00:05:28.879
DW squared as well as DP squared and
then epsilon the choice of epsilon

00:05:28.879 --> 00:05:28.889
then epsilon the choice of epsilon
 

00:05:28.889 --> 00:05:31.800
then epsilon the choice of epsilon
doesn't matter very much but the authors

00:05:31.800 --> 00:05:31.810
doesn't matter very much but the authors
 

00:05:31.810 --> 00:05:33.959
doesn't matter very much but the authors
of the advent paper recommended 10 to

00:05:33.959 --> 00:05:33.969
of the advent paper recommended 10 to
 

00:05:33.969 --> 00:05:37.649
of the advent paper recommended 10 to
the minus 8 but this parameter you

00:05:37.649 --> 00:05:37.659
the minus 8 but this parameter you
 

00:05:37.659 --> 00:05:39.779
the minus 8 but this parameter you
really don't need to set it and it

00:05:39.779 --> 00:05:39.789
really don't need to set it and it
 

00:05:39.789 --> 00:05:41.899
really don't need to set it and it
doesn't affect performance much at all

00:05:41.899 --> 00:05:41.909
doesn't affect performance much at all
 

00:05:41.909 --> 00:05:44.730
doesn't affect performance much at all
but when implementing atom what people

00:05:44.730 --> 00:05:44.740
but when implementing atom what people
 

00:05:44.740 --> 00:05:46.529
but when implementing atom what people
usually do is just use the default

00:05:46.529 --> 00:05:46.539
usually do is just use the default
 

00:05:46.539 --> 00:05:49.290
usually do is just use the default
values of beta 1 and beta 2 as well as

00:05:49.290 --> 00:05:49.300
values of beta 1 and beta 2 as well as
 

00:05:49.300 --> 00:05:51.269
values of beta 1 and beta 2 as well as
Epsilon I don't think anyone ever really

00:05:51.269 --> 00:05:51.279
Epsilon I don't think anyone ever really
 

00:05:51.279 --> 00:05:54.089
Epsilon I don't think anyone ever really
choose epsilon and then try a range of

00:05:54.089 --> 00:05:54.099
choose epsilon and then try a range of
 

00:05:54.099 --> 00:05:56.339
choose epsilon and then try a range of
values of alpha to see what works best

00:05:56.339 --> 00:05:56.349
values of alpha to see what works best
 

00:05:56.349 --> 00:05:58.709
values of alpha to see what works best
you can also tune beta 1 and beta 2 but

00:05:58.709 --> 00:05:58.719
you can also tune beta 1 and beta 2 but
 

00:05:58.719 --> 00:06:00.990
you can also tune beta 1 and beta 2 but
it's not done that often among the

00:06:00.990 --> 00:06:01.000
it's not done that often among the
 

00:06:01.000 --> 00:06:03.839
it's not done that often among the
practitioners I know so where does the

00:06:03.839 --> 00:06:03.849
practitioners I know so where does the
 

00:06:03.849 --> 00:06:07.100
practitioners I know so where does the
term atom come from atom stands for

00:06:07.100 --> 00:06:07.110
term atom come from atom stands for
 

00:06:07.110 --> 00:06:16.320
term atom come from atom stands for
adaptive moment estimation so beta 1 is

00:06:16.320 --> 00:06:16.330
adaptive moment estimation so beta 1 is
 

00:06:16.330 --> 00:06:17.969
adaptive moment estimation so beta 1 is
computing the mean of the derivatives

00:06:17.969 --> 00:06:17.979
computing the mean of the derivatives
 

00:06:17.979 --> 00:06:20.279
computing the mean of the derivatives
this is called the first moment and beta

00:06:20.279 --> 00:06:20.289
this is called the first moment and beta
 

00:06:20.289 --> 00:06:22.860
this is called the first moment and beta
2 is used compute exponentially weighted

00:06:22.860 --> 00:06:22.870
2 is used compute exponentially weighted
 

00:06:22.870 --> 00:06:24.779
2 is used compute exponentially weighted
average of the squares and that's called

00:06:24.779 --> 00:06:24.789
average of the squares and that's called
 

00:06:24.789 --> 00:06:26.639
average of the squares and that's called
the second moment so that gives rise to

00:06:26.639 --> 00:06:26.649
the second moment so that gives rise to
 

00:06:26.649 --> 00:06:29.490
the second moment so that gives rise to
named adaptive moment estimation but

00:06:29.490 --> 00:06:29.500
named adaptive moment estimation but
 

00:06:29.500 --> 00:06:31.350
named adaptive moment estimation but
everyone just calls it the atom also

00:06:31.350 --> 00:06:31.360
everyone just calls it the atom also
 

00:06:31.360 --> 00:06:32.310
everyone just calls it the atom also
invention

00:06:32.310 --> 00:06:32.320
invention
 

00:06:32.320 --> 00:06:35.100
invention
and by the way one of my long-term

00:06:35.100 --> 00:06:35.110
and by the way one of my long-term
 

00:06:35.110 --> 00:06:37.140
and by the way one of my long-term
friends and collaborators is called atom

00:06:37.140 --> 00:06:37.150
friends and collaborators is called atom
 

00:06:37.150 --> 00:06:39.060
friends and collaborators is called atom
codes far as I know this algorithm

00:06:39.060 --> 00:06:39.070
codes far as I know this algorithm
 

00:06:39.070 --> 00:06:40.320
codes far as I know this algorithm
doesn't have anything to do with him

00:06:40.320 --> 00:06:40.330
doesn't have anything to do with him
 

00:06:40.330 --> 00:06:42.660
doesn't have anything to do with him
except for the fact that I think he uses

00:06:42.660 --> 00:06:42.670
except for the fact that I think he uses
 

00:06:42.670 --> 00:06:44.340
except for the fact that I think he uses
it sometimes but sometimes I get off

00:06:44.340 --> 00:06:44.350
it sometimes but sometimes I get off
 

00:06:44.350 --> 00:06:46.800
it sometimes but sometimes I get off
that question so just in case you're

00:06:46.800 --> 00:06:46.810
that question so just in case you're
 

00:06:46.810 --> 00:06:49.290
that question so just in case you're
wondering so that's it for the atom

00:06:49.290 --> 00:06:49.300
wondering so that's it for the atom
 

00:06:49.300 --> 00:06:51.300
wondering so that's it for the atom
optimization algorithm with it I think

00:06:51.300 --> 00:06:51.310
optimization algorithm with it I think
 

00:06:51.310 --> 00:06:52.830
optimization algorithm with it I think
you really train your neural networks

00:06:52.830 --> 00:06:52.840
you really train your neural networks
 

00:06:52.840 --> 00:06:55.260
you really train your neural networks
much more quickly but before we wrap up

00:06:55.260 --> 00:06:55.270
much more quickly but before we wrap up
 

00:06:55.270 --> 00:06:57.180
much more quickly but before we wrap up
for this week let's keep talking about

00:06:57.180 --> 00:06:57.190
for this week let's keep talking about
 

00:06:57.190 --> 00:07:00.000
for this week let's keep talking about
hyper parameter tuning as was getting

00:07:00.000 --> 00:07:00.010
hyper parameter tuning as was getting
 

00:07:00.010 --> 00:07:01.530
hyper parameter tuning as was getting
some more intuitions about what the

00:07:01.530 --> 00:07:01.540
some more intuitions about what the
 

00:07:01.540 --> 00:07:03.270
some more intuitions about what the
optimization problem from your networks

00:07:03.270 --> 00:07:03.280
optimization problem from your networks
 

00:07:03.280 --> 00:07:05.640
optimization problem from your networks
look like in the next video we'll talk

00:07:05.640 --> 00:07:05.650
look like in the next video we'll talk
 

00:07:05.650 --> 00:07:09.060
look like in the next video we'll talk
about learning rate decay

