WEBVTT
Kind: captions
Language: en

00:00:00.430 --> 00:00:01.180
Hi, Michael.

00:00:01.180 --> 00:00:03.300
&gt;&gt; Hey Charles, well that was fun.

00:00:03.300 --> 00:00:05.700
&gt;&gt; That was fun, we should do it again.

00:00:05.700 --> 00:00:06.910
&gt;&gt; I don't think we need to do it again.

00:00:06.910 --> 00:00:09.640
I think we've now covered
everything we wanted to say about

00:00:09.640 --> 00:00:10.790
reinforcement learning.

00:00:10.790 --> 00:00:12.330
&gt;&gt; And
I'm completely sure that we haven't.

00:00:12.330 --> 00:00:13.640
&gt;&gt; Well, what haven't we covered?

00:00:13.640 --> 00:00:15.480
&gt;&gt; Well,
what haven't we haven't we covered?

00:00:15.480 --> 00:00:18.560
&gt;&gt; Well, we haven't,
we haven't we covered TD-Lambda.

00:00:18.560 --> 00:00:20.190
&gt;&gt; Sure.
So, we talked about TD lambda.

00:00:20.190 --> 00:00:21.580
We talked about a few algorithms here or

00:00:21.580 --> 00:00:23.740
there, but we haven't really
touched on the big questions.

00:00:23.740 --> 00:00:25.380
We haven't covered
everything that we should.

00:00:25.380 --> 00:00:26.500
&gt;&gt; Like?

00:00:26.500 --> 00:00:27.080
&gt;&gt; BURLAP!

00:00:27.080 --> 00:00:28.540
&gt;&gt; We have definitely covered BURLAP.

00:00:28.540 --> 00:00:32.680
&gt;&gt; Okay, sure, there was a tutorial, and
there were assignments about it, but

00:00:32.680 --> 00:00:35.920
you haven't answered the big
question about BURLAP.

00:00:35.920 --> 00:00:36.790
Which is what?

00:00:36.790 --> 00:00:37.590
&gt;&gt; Why BURLAP?

00:00:37.590 --> 00:00:39.630
Why do we need anything like BURLAP?

00:00:39.630 --> 00:00:44.000
Why don't you just use the UCI
database and Weka we're done.

00:00:44.000 --> 00:00:48.450
&gt;&gt; Right so the UCI data set their
depository and Weka are really great for

00:00:48.450 --> 00:00:52.100
supervised learning but
they're not actually well situated for

00:00:52.100 --> 00:00:53.080
reinforcement learning.

00:00:53.080 --> 00:00:56.070
Weka doesn't include reinforcement
learning algorithms it

00:00:56.070 --> 00:01:00.130
the UCI repository doesn't include
reinforcement learning problems in it.

00:01:00.130 --> 00:01:01.960
You really need a different set up for

00:01:01.960 --> 00:01:03.370
dealing with reinforcement
learning problems.

00:01:03.370 --> 00:01:05.010
&gt;&gt; You need a different, okay,
I don't understand that.

00:01:05.010 --> 00:01:08.680
So we have algorithms, we've talked
about many of them in class.

00:01:08.680 --> 00:01:09.210
&gt;&gt; That's right.

00:01:09.210 --> 00:01:12.770
&gt;&gt; So algorithms, are algorithms,
are algorithms, are algorithms, and

00:01:12.770 --> 00:01:15.360
this is all machine learning and
so all we need is data.

00:01:15.360 --> 00:01:18.040
And that's what repositories are for,
data.

00:01:18.040 --> 00:01:21.180
So, well yes, BURLAP is trying
to answer both of those issues.

00:01:21.180 --> 00:01:22.670
One of the things that
BURLAP is trying to be is

00:01:22.670 --> 00:01:24.730
a collection of those algorithms or
algorithms or

00:01:24.730 --> 00:01:29.710
algorithms or algorithms and it's also
providing a replacement for data.

00:01:29.710 --> 00:01:33.260
So data in the reinforcement learning
setting is really hard to work with.

00:01:33.260 --> 00:01:36.430
Instead, what BURLAP has
is a set of simulators,

00:01:36.430 --> 00:01:39.830
the ability to Act
like the environment so

00:01:39.830 --> 00:01:43.525
that the learning algorithm can
interact with it and learn from them.

00:01:43.525 --> 00:01:47.369
[CROSSTALK] I see so the keyword there
is in Iraq, so the notion of supervised

00:01:47.369 --> 00:01:50.739
learning as you just got data you got
him put it up with pairs input and

00:01:50.739 --> 00:01:54.527
output pairs, but as we pointed out
multiple times reinforcement learning

00:01:54.527 --> 00:01:57.601
is not just about input output you and
back Get to see states and

00:01:57.601 --> 00:01:59.001
rewards you take actions.

00:01:59.001 --> 00:02:02.280
Those are the kinds of thing that you
do and that requires interaction.

00:02:02.280 --> 00:02:05.760
&gt;&gt; Right and the data that you actually
see depends on how you're exploring

00:02:05.760 --> 00:02:09.800
the environment so just capturing a big
static set of data and presenting that

00:02:09.800 --> 00:02:12.082
to the learner isn't really
the reinforcement learning process.

00:02:12.082 --> 00:02:12.686
&gt;&gt; Okay, so I like that.

00:02:12.686 --> 00:02:14.010
So I heard two things there.

00:02:14.010 --> 00:02:15.660
One thing I like, and
one thing I don't like.

00:02:15.660 --> 00:02:18.180
The thing that I like is that you
distinguished reinforcement learning

00:02:18.180 --> 00:02:19.530
from supervised learning.

00:02:19.530 --> 00:02:21.530
You talked about the need for
interaction and

00:02:21.530 --> 00:02:25.950
how that's sort of crucial, and you said
your solution was having simulators.

00:02:25.950 --> 00:02:28.540
&gt;&gt; Right, so,
I agree that that's not a great thing.

00:02:28.540 --> 00:02:33.610
So simulators are good in the sense that
you can actually interact with them,

00:02:33.610 --> 00:02:36.500
and so it's a way of transferring and
an environment or

00:02:36.500 --> 00:02:39.366
a test problem from one research
group to another research Group.

00:02:39.366 --> 00:02:40.038
[CROSSTALK] Sounds good.

00:02:40.038 --> 00:02:42.395
But on the bad side
they're not really data,

00:02:42.395 --> 00:02:44.830
it's not actual data
coming from a problem.

00:02:44.830 --> 00:02:45.775
It's just fit.

00:02:45.775 --> 00:02:49.167
[CROSSTALK] So then you're suggesting
this is the second thing that I didn't

00:02:49.167 --> 00:02:52.771
thing I like you're suggesting that we
don't have any way to make reinforcement

00:02:52.771 --> 00:02:54.260
learning work in the real world.

00:02:54.260 --> 00:02:57.672
It is more challenging, but it is not
the case that reinforcement learning

00:02:57.672 --> 00:03:00.330
hasn't been used to solve
some real problems.

00:03:00.330 --> 00:03:01.260
&gt;&gt; Like what?

00:03:01.260 --> 00:03:03.200
&gt;&gt; So, it's worth probably
mentioning a few of them.

00:03:03.200 --> 00:03:05.410
We talked about backgammon
as part of the class,

00:03:05.410 --> 00:03:09.140
the TD-Gammon algorithm that actually
learn to play backgammon at a level

00:03:09.140 --> 00:03:11.700
comparable to,
if not better than, human beings.

00:03:11.700 --> 00:03:14.690
There's other sorts of problems that are
probably worth talking about as well.

00:03:14.690 --> 00:03:17.870
Elevator control is an interesting
problem to think about.

00:03:17.870 --> 00:03:20.013
&gt;From a reinforcement
learning perspective.

00:03:20.013 --> 00:03:21.630
&gt;&gt; How so?
How is an elevator controller

00:03:21.630 --> 00:03:22.620
reinforcement learning about?

00:03:22.620 --> 00:03:25.030
&gt;&gt; Elevator control if you think about
the elevators in a building they're

00:03:25.030 --> 00:03:28.950
trying to work together to move
people to where they want to go.

00:03:28.950 --> 00:03:32.350
And so we have actions,
we have states and we have rewards.

00:03:32.350 --> 00:03:35.840
The actions are where the elevators
are trying to go, whether or

00:03:35.840 --> 00:03:38.320
not they're going to open
their doors at a given floor.

00:03:38.320 --> 00:03:41.380
The states are what
is the arrangement of

00:03:41.380 --> 00:03:42.820
all the different
elevators at the moment.

00:03:42.820 --> 00:03:45.040
Which floors have elevators on them,
which don't.

00:03:45.040 --> 00:03:46.010
&gt;&gt; Which buttons have been pressed.

00:03:46.010 --> 00:03:47.270
&gt;&gt; Which buttons have been pressed.

00:03:47.270 --> 00:03:49.540
So who's waiting on which floors.

00:03:49.540 --> 00:03:51.380
And from the reward perspective

00:03:52.560 --> 00:03:56.520
you can try to get the system to try to
minimize wait time for people, right?

00:03:56.520 --> 00:03:57.720
So that's something
that you can measure,

00:03:57.720 --> 00:04:00.600
how long is somebody waiting on the
third floor for the elevator to come,

00:04:00.600 --> 00:04:03.360
and then eventually getting
to the destination floor and

00:04:03.360 --> 00:04:04.720
that's something you can measure and

00:04:04.720 --> 00:04:08.355
something that you can try to
find ways of behaving to improve.

00:04:08.355 --> 00:04:10.360
Okay cool, so
that's a reinforcement learning problem.

00:04:10.360 --> 00:04:11.530
I like that, anything else?

00:04:11.530 --> 00:04:13.874
&gt;&gt; Yeah, so that one actually
was done in simulation.

00:04:13.874 --> 00:04:14.380
&gt;&gt; Okay.

00:04:14.380 --> 00:04:20.329
&gt;&gt; But there has been some good work
in reinforcement learning in robotics.

00:04:20.329 --> 00:04:24.020
So one that's maybe worth mentioning
is the the helicopter work.

