WEBVTT
Kind: captions
Language: en

00:00:00.634 --> 00:00:02.300
Barrett: Hi, everyone.

00:00:02.300 --> 00:00:03.467
Thanks for coming.

00:00:03.467 --> 00:00:05.901
I know it's getting late
in the day.

00:00:05.901 --> 00:00:07.634
After all the hallway
conversations

00:00:07.634 --> 00:00:09.133
and the other talks
you've been too,

00:00:09.133 --> 00:00:11.634
you start running out
of steam, so I appreciate it.

00:00:11.634 --> 00:00:12.968
My name's Ryan Barrett.

00:00:12.968 --> 00:00:14.834
I'm one of the co-founders
of Google App Engine

00:00:14.834 --> 00:00:17.133
and I'm the lead
on the datastore right now.

00:00:17.133 --> 00:00:19.267
I'm here to talk
about multihoming.

00:00:19.267 --> 00:00:22.100
So running a service
from multiple datacenters.

00:00:22.100 --> 00:00:23.667
This is the kind of thing
we do over the weekend

00:00:23.667 --> 00:00:25.033
when we get bored and,
you know,

00:00:25.033 --> 00:00:26.367
we don't have
any parties to go to.

00:00:26.367 --> 00:00:28.467
So figured, you know,
we'd hacked this together

00:00:28.467 --> 00:00:32.434
last weekend and, you know,
I'd give you a quick demo.

00:00:32.434 --> 00:00:34.267
So we're gonna
be taking questions

00:00:34.267 --> 00:00:36.000
using Google Moderator.

00:00:36.000 --> 00:00:38.067
And so if you go to this URL,

00:00:38.067 --> 00:00:40.634
code.google.
com/events/io/questions,

00:00:40.634 --> 00:00:42.501
go to the App Engine track

00:00:42.501 --> 00:00:46.567
and the transactions
across datacenters series

00:00:46.567 --> 00:00:48.968
and feel free to type in
questions, vote on questions,

00:00:48.968 --> 00:00:52.534
all that good stuff.

00:00:52.534 --> 00:00:54.567
So let's get started.

00:00:54.567 --> 00:00:57.033
Let's start
with a couple quotes.

00:00:57.033 --> 00:00:59.934
One is from a guy named
Eric Brewer,

00:00:59.934 --> 00:01:02.367
who is a professor,
a computer science professor.

00:01:02.367 --> 00:01:05.367
He's also done architecture
at Berkeley.

00:01:05.367 --> 00:01:07.300
He's also a co-founder
of Inktomi,

00:01:07.300 --> 00:01:08.901
which you might have heard of.

00:01:08.901 --> 00:01:10.667
He's well-known
for this theorem,

00:01:10.667 --> 00:01:13.601
the consistency, availability,
partition tolerance theorem.

00:01:13.601 --> 00:01:15.567
And so the idea is
if you have

00:01:15.567 --> 00:01:19.501
a distributed storage system,
this is just

00:01:19.501 --> 00:01:23.033
an inherent trade-off that
you can't get away from

00:01:23.033 --> 00:01:26.968
and so you better acknowledge
it early and pay attention.

00:01:26.968 --> 00:01:28.567
It's not black and white.

00:01:28.567 --> 00:01:30.167
You don't just
get two out of three.

00:01:30.167 --> 00:01:32.334
Often you can get a little on
one and a little better

00:01:32.334 --> 00:01:33.734
in another,
but this is something

00:01:33.734 --> 00:01:35.067
we pay attention to.

00:01:35.067 --> 00:01:37.601
He's also a wicked-smart guy.

00:01:37.601 --> 00:01:41.734
The other one is not as
interesting, but, you know,

00:01:41.734 --> 00:01:43.567
there is no free lunch.

00:01:43.567 --> 00:01:47.501
Making some scalable
is just plain hard.

00:01:47.501 --> 00:01:49.868
You can use a bunch of different
technologies.

00:01:49.868 --> 00:01:51.934
You can use a bunch
of different techniques

00:01:51.934 --> 00:01:55.000
and they get you a long way,
but you can't

00:01:55.000 --> 00:01:56.968
just stick your head in the sand
and ignore it altogether.

00:01:56.968 --> 00:01:58.267
You got to pay
a little attention

00:01:58.267 --> 00:02:02.067
and so this
is what we've done.

00:02:02.067 --> 00:02:06.767
So before I get started,
one of the best pieces of advice

00:02:06.767 --> 00:02:08.567
I ever heard about presenting

00:02:08.567 --> 00:02:10.234
is that whenever
you switch slides,

00:02:10.234 --> 00:02:12.234
what everyone does
is stops listening,

00:02:12.234 --> 00:02:14.934
reads through the slide,
then turns back to you

00:02:14.934 --> 00:02:16.334
and starts listening again.

00:02:16.334 --> 00:02:18.567
So what I want you all to do
is whenever I switch slides,

00:02:18.567 --> 00:02:20.501
turn away from me,
look at a screen,

00:02:20.501 --> 00:02:22.434
then when you're about done,
turn back to me

00:02:22.434 --> 00:02:24.634
and I'll actually know
that I can start talking.

00:02:24.634 --> 00:02:26.033
I know this is what I do.

00:02:26.033 --> 00:02:28.767
If I'm not on my laptop
at a talk, this is what I do.

00:02:28.767 --> 00:02:31.367
So anyway.

00:02:31.367 --> 00:02:33.167
Multihoming,
like I talked about,

00:02:33.167 --> 00:02:36.267
serving something
from multiple datacenters.

00:02:36.267 --> 00:02:38.667
Hard problem, but we need
to figure out how to do it.

00:02:38.667 --> 00:02:40.434
There are a lot
of parts of this

00:02:40.434 --> 00:02:42.767
and I'm not gonna
get into most of them.

00:02:42.767 --> 00:02:44.734
Actually running
a serving stack.

00:02:44.734 --> 00:02:46.334
Most of the stack,
for example.

00:02:46.334 --> 00:02:49.300
Offline data processing,
batch processing,

00:02:49.300 --> 00:02:53.234
map reduce, long-running,
you know, data mining analysis.

00:02:53.234 --> 00:02:54.534
That kind of thing.

00:02:54.534 --> 00:02:56.067
Read-only data.

00:02:56.067 --> 00:02:57.667
Content distribution networks.

00:02:57.667 --> 00:02:59.234
CDNs like Akamai.

00:02:59.234 --> 00:03:01.000
All these things
are really interesting

00:03:01.000 --> 00:03:03.834
problems in their own right
and I only have an hour

00:03:03.834 --> 00:03:07.067
and, you know, I even have more
than I can fit in an hour,

00:03:07.067 --> 00:03:08.834
so I'm gonna stick with
what I've been working on

00:03:08.834 --> 00:03:11.234
the last few years.

00:03:11.234 --> 00:03:12.901
Well, for a while now.

00:03:12.901 --> 00:03:14.601
It's what we call
structured data.

00:03:14.601 --> 00:03:15.767
User data.

00:03:15.767 --> 00:03:17.834
Data that is both written
and read,

00:03:17.834 --> 00:03:21.968
often by your users,
and when they take actions

00:03:21.968 --> 00:03:24.767
that write data,
they often expect to see

00:03:24.767 --> 00:03:28.400
the results of those writes
and expect some guarantees

00:03:28.400 --> 00:03:30.801
on consistency and availability
and that kind of thing.

00:03:30.801 --> 00:03:36.634
So in my humble opinion
or the biased view,

00:03:36.634 --> 00:03:38.601
this is one of the hardest
kinds of data

00:03:38.601 --> 00:03:43.400
to run a data storage system
across datacenters.

00:03:43.400 --> 00:03:45.501
So we're gonna talk about
a little ways--

00:03:45.501 --> 00:03:47.834
a few ways of doing this.

00:03:47.834 --> 00:03:50.868
How App Engine currently does
it in some places.

00:03:50.868 --> 00:03:53.801
And kind of throughout,
I'm gonna talk about

00:03:53.801 --> 00:03:55.634
the App Engine datastore and,
you know,

00:03:55.634 --> 00:03:57.667
what the choices were we made.

00:03:57.667 --> 00:03:59.567
Things we went with,
things we didn't go with

00:03:59.567 --> 00:04:02.834
to run across multiple
datacenters.

00:04:02.834 --> 00:04:05.467
Again, as a disclaimer,
there are a lot

00:04:05.467 --> 00:04:08.634
of great talks at this
conference on best practices

00:04:08.634 --> 00:04:10.267
for how to write
an App Engine app,

00:04:10.267 --> 00:04:13.567
new features, you know,
that kind of thing.

00:04:13.567 --> 00:04:15.000
And this is not one of those.

00:04:15.000 --> 00:04:16.801
I will be talking a little bit
about App Engine,

00:04:16.801 --> 00:04:20.501
but mostly just
how we did things.

00:04:20.501 --> 00:04:23.067
Mostly it's just gonna be
general lessons learned,

00:04:23.067 --> 00:04:25.167
both good and bad.

00:04:25.167 --> 00:04:26.634
So I would encourage you
to go to some

00:04:26.634 --> 00:04:27.934
of those other talks too.

00:04:27.934 --> 00:04:33.634
This is more just
kind of stories.

00:04:33.634 --> 00:04:37.567
So specifically what
are we gonna talk about?

00:04:37.567 --> 00:04:39.901
First thing is consistency.

00:04:39.901 --> 00:04:42.400
What kinds of guarantees
do you have

00:04:42.400 --> 00:04:44.634
from a cloud data system,
you know?

00:04:44.634 --> 00:04:46.200
How are they similar
and different from

00:04:46.200 --> 00:04:48.167
databases and other
data storage systems

00:04:48.167 --> 00:04:49.667
you might be familiar with.

00:04:49.667 --> 00:04:51.934
Transactions, specifically.

00:04:51.934 --> 00:04:53.167
What are they apart from
consistency?

00:04:53.167 --> 00:04:54.434
Why do you need them?

00:04:54.434 --> 00:04:56.467
How do they interact
with multihoming?

00:04:56.467 --> 00:04:58.167
Why do we need to do
multihoming at all?

00:04:58.167 --> 00:05:01.534
I mean, can't we just serve out
of one datacenter?

00:05:01.534 --> 00:05:03.501
Then we're gonna actually
get into the details.

00:05:03.501 --> 00:05:05.200
Talk about specific kinds
of multihoming,

00:05:05.200 --> 00:05:07.033
specific techniques
that you can use,

00:05:07.033 --> 00:05:11.567
and finally,
some conclusions.

00:05:11.567 --> 00:05:14.567
So there are three kinds
of consistency

00:05:14.567 --> 00:05:16.834
I'm gonna talk about.

00:05:16.834 --> 00:05:18.300
There are more.

00:05:18.300 --> 00:05:19.801
If you were
in Dan Wilkerson's talk

00:05:19.801 --> 00:05:22.434
on distributed transactions,
you heard about causal--

00:05:22.434 --> 00:05:23.634
Sorry.

00:05:23.634 --> 00:05:25.200
Cause of consistency
and a few others,

00:05:25.200 --> 00:05:26.968
but here we're mainly
gonna focus

00:05:26.968 --> 00:05:29.701
on what we call weak, eventual,
and strong consistency.

00:05:29.701 --> 00:05:32.200
And as a concrete example
for each of these,

00:05:32.200 --> 00:05:34.767
we're gonna look at what happens
when you do a read.

00:05:34.767 --> 00:05:39.734
When you read an object
after you wrote it.

00:05:39.734 --> 00:05:42.234
So the first one we're gonna
look at is weak consistency

00:05:42.234 --> 00:05:44.934
and this is the most text
I've had in a slide so far,

00:05:44.934 --> 00:05:47.133
so please look
at the slides and read

00:05:47.133 --> 00:05:51.234
and when you're done,
let me know.

00:05:51.234 --> 00:05:53.767
Weak consistency
is what you get from caches.

00:05:53.767 --> 00:05:55.934
That's the best way
to think of it.

00:05:55.934 --> 00:05:58.133
You don't really have
any guarantees.

00:05:58.133 --> 00:06:01.133
You put something in.
Later, you ask for it.

00:06:01.133 --> 00:06:03.334
It might be there.
It might not.

00:06:03.334 --> 00:06:05.300
You might never get it after
you ask for it.

00:06:05.300 --> 00:06:07.501
You know, the cache just might
be churning so much

00:06:07.501 --> 00:06:09.234
that things
are constantly evicted.

00:06:09.234 --> 00:06:11.033
This is what
we call best effort.

00:06:11.033 --> 00:06:13.334
I think of it as
"message in a bottle."

00:06:13.334 --> 00:06:16.033
The memcache API
in App Engine,

00:06:16.033 --> 00:06:18.634
it's just a simple
kind of unified

00:06:18.634 --> 00:06:21.834
large-scale in-memory
cache system

00:06:21.834 --> 00:06:24.934
is a perfect example of this.

00:06:24.934 --> 00:06:27.934
VoIP and kind of online
streaming media

00:06:27.934 --> 00:06:29.434
are great examples,
because often,

00:06:29.434 --> 00:06:32.934
if you lose connectivity
and you miss some messages

00:06:32.934 --> 00:06:35.767
or they come in delayed,
often, you don't care.

00:06:35.767 --> 00:06:37.968
If you're talking on the phone
and there was a hiccup,

00:06:37.968 --> 00:06:40.501
you don't want it to try
to catch up and get behind.

00:06:40.501 --> 00:06:42.801
You still just want that
real-time communication

00:06:42.801 --> 00:06:45.067
and so you're okay with it
dropping some packets

00:06:45.067 --> 00:06:47.701
and so some writes
or some messages

00:06:47.701 --> 00:06:50.267
that came from the other side
never get to you.

00:06:50.267 --> 00:06:52.434
But that's okay 'cause
that's the use case.

00:06:52.434 --> 00:06:56.133
That's what you expect.

00:06:56.133 --> 00:06:57.868
Low-latency multiplayer games.

00:06:57.868 --> 00:07:00.267
So first-person shooters,
that kind of thing.

00:07:00.267 --> 00:07:02.834
Stuff that you play online
often are the same.

00:07:02.834 --> 00:07:04.100
Sometimes you interpolate.

00:07:04.100 --> 00:07:06.234
Sometimes if you just
have old state,

00:07:06.234 --> 00:07:08.467
you don't really care about
where the other guy was,

00:07:08.467 --> 00:07:10.934
you know, 30 seconds ago.

00:07:10.934 --> 00:07:12.400
You care about
where they are now.

00:07:12.400 --> 00:07:15.300
So there are many cases where
weak consistency is good.

00:07:15.300 --> 00:07:16.567
It's the right trade-off.

00:07:16.567 --> 00:07:19.234
But for data storage systems,

00:07:19.234 --> 00:07:23.834
stuff that's persistent,
it's weak.

00:07:23.834 --> 00:07:26.868
Eventual consistency
is better.

00:07:26.868 --> 00:07:29.300
So there are great examples
of this.

00:07:29.300 --> 00:07:33.834
The idea is you'll eventually
see the stuff you wrote.

00:07:33.834 --> 00:07:36.567
You just might not
see it right away.

00:07:36.567 --> 00:07:39.634
The email sending API
in App Engine

00:07:39.634 --> 00:07:41.000
and email in general--

00:07:41.000 --> 00:07:43.000
SMTP for mail delivery is
a great example of this

00:07:43.000 --> 00:07:45.834
where you send an email,
you accept that

00:07:45.834 --> 00:07:48.267
it's not guaranteed
to immediately show up

00:07:48.267 --> 00:07:50.801
in the other person's inbox
as soon as it returns

00:07:50.801 --> 00:07:53.133
from that send operation,
but you know it will

00:07:53.133 --> 00:07:54.868
get there eventually.

00:07:54.868 --> 00:07:59.601
The Postal Service, despite
their budget woes right now,

00:07:59.601 --> 00:08:02.367
same kind of thing.

00:08:02.367 --> 00:08:06.934
DNS, domain name propagation,
changes to that, similar.

00:08:06.934 --> 00:08:09.934
There's some notable
cloud platforms.

00:08:09.934 --> 00:08:13.734
Amazon's S3 and SimpleDB,
also SimpleQ service

00:08:13.734 --> 00:08:15.734
are eventually
consistent systems.

00:08:15.734 --> 00:08:20.567
So if you write a file into S3
or you write a record

00:08:20.567 --> 00:08:24.000
into SimpleDB,
you won't immediately see it,

00:08:24.000 --> 00:08:26.300
but you will see it
soon after.

00:08:26.300 --> 00:08:29.367
So in SimpleDB,
I think in practice,

00:08:29.367 --> 00:08:32.968
you see the results of writes
in a few hundred milliseconds.

00:08:32.968 --> 00:08:35.033
I think they average around
400 or 500 milliseconds,

00:08:35.033 --> 00:08:36.601
sometimes up to a few seconds.

00:08:36.601 --> 00:08:38.601
S3 I'm not sure about.

00:08:38.601 --> 00:08:40.734
But this is much more useful.

00:08:40.734 --> 00:08:42.634
It's not ideal though.

00:08:42.634 --> 00:08:45.367
The ideal kind of consistency
for a read-write

00:08:45.367 --> 00:08:48.234
or structured data system
is strong consistency.

00:08:48.234 --> 00:08:50.868
You get what you put in.

00:08:50.868 --> 00:08:52.901
This is the simplest one
to think about

00:08:52.901 --> 00:08:54.934
and to program against,
and again,

00:08:54.934 --> 00:08:57.534
in my humble,
biased opinion.

00:08:57.534 --> 00:08:59.868
So you're guaranteed when,
you know,

00:08:59.868 --> 00:09:01.734
after you do a write and it
returns successfully--

00:09:01.734 --> 00:09:04.534
it doesn't throw an exception--
any read after that

00:09:04.534 --> 00:09:06.000
will see it.

00:09:06.000 --> 00:09:08.133
The App Engine datastore
works this way.

00:09:08.133 --> 00:09:10.200
File systems almost always work
this way.

00:09:10.200 --> 00:09:12.234
Relational databases.

00:09:12.234 --> 00:09:13.667
Microsoft Azure.

00:09:13.667 --> 00:09:17.267
Not enough people talk
about Microsoft Azure.

00:09:17.267 --> 00:09:18.834
I think it's actually
fascinating.

00:09:18.834 --> 00:09:21.634
They've made a number of really,
really interesting

00:09:21.634 --> 00:09:24.200
and compelling design decisions.

00:09:24.200 --> 00:09:26.200
They're a competitor
to SimpleDB

00:09:26.200 --> 00:09:28.467
and the App Engine datastore
is called Tables.

00:09:28.467 --> 00:09:31.167
They are also
strongly consistent.

00:09:31.167 --> 00:09:33.033
So these are three forms
of consistency,

00:09:33.033 --> 00:09:37.567
weak, eventual, and strong,
that depending on how we build

00:09:37.567 --> 00:09:42.133
a storage system, we're gonna
see one or more of these.

00:09:42.133 --> 00:09:45.033
And we want to pay attention
once we try to move

00:09:45.033 --> 00:09:47.100
a system like this across
datacenters.

00:09:47.100 --> 00:09:49.334
You know, do we get
the same guarantees?

00:09:49.334 --> 00:09:51.434
Have we lost some guarantee?

00:09:51.434 --> 00:09:54.033
Sometimes it's okay to lose some
of those guarantees,

00:09:54.033 --> 00:09:57.100
but you want to know.

00:09:57.100 --> 00:09:59.400
So that's consistency.

00:09:59.400 --> 00:10:00.901
Next is transactions.

00:10:00.901 --> 00:10:04.467
And for those of you who were
in Dan Wilkerson's talk,

00:10:04.467 --> 00:10:06.067
feel free to go to sleep
for five minutes

00:10:06.067 --> 00:10:08.300
and I'll wake you up
when we're done.

00:10:08.300 --> 00:10:11.167
Transactions are kind of an
extended form of consistency

00:10:11.167 --> 00:10:14.300
across multiple operations.

00:10:14.300 --> 00:10:16.234
The cliched example is a bank.

00:10:16.234 --> 00:10:18.300
You need to transfer money
from "A" to "B,"

00:10:18.300 --> 00:10:21.234
or as in Dan's talk,
from "Alice" to "Bob."

00:10:21.234 --> 00:10:23.367
There are two separate
operations here.

00:10:23.367 --> 00:10:24.734
Subtracting money from "A."

00:10:24.734 --> 00:10:26.801
Adding money to "B."

00:10:26.801 --> 00:10:29.767
Since we work with computers
and within a single thread

00:10:29.767 --> 00:10:33.100
on a single machine,
operations are serialized.

00:10:33.100 --> 00:10:35.100
You can't do those
at the same time

00:10:35.100 --> 00:10:37.133
and something bad
could happen in between.

00:10:37.133 --> 00:10:39.801
Something else could come in
and read from "A" or "B"

00:10:39.801 --> 00:10:42.334
or write to "A" or "B."

00:10:42.334 --> 00:10:44.200
The thread in the machine
that you're running

00:10:44.200 --> 00:10:47.968
this transaction in could die
for any number of reasons.

00:10:47.968 --> 00:10:49.534
On App Engine, maybe you hit
the deadline

00:10:49.534 --> 00:10:51.434
or request deadline.

00:10:51.434 --> 00:10:54.133
There are a number
of other possibilities.

00:10:54.133 --> 00:10:56.234
You don't want to be
in that position.

00:10:56.234 --> 00:11:03.200
You want some guarantees that
if you die in the middle,

00:11:03.200 --> 00:11:05.634
the invariants
will be maintained.

00:11:05.634 --> 00:11:09.501
Either that money subtracted
from "A" will go back to "A,"

00:11:09.501 --> 00:11:11.734
or the money added to "B" will
eventually still

00:11:11.734 --> 00:11:13.801
get added to "B,"
because as Dan said,

00:11:13.801 --> 00:11:16.267
this case where, you know,
you've created money

00:11:16.267 --> 00:11:19.000
or destroyed money,
you're not really allowed

00:11:19.000 --> 00:11:20.267
to do that.

00:11:20.267 --> 00:11:22.100
Only the Federal Reserve
is actually allowed

00:11:22.100 --> 00:11:24.467
to do that kind of thing
and we're not trying

00:11:24.467 --> 00:11:26.000
to get in the business
of banking.

00:11:26.000 --> 00:11:28.234
We just write software
and we like it there

00:11:28.234 --> 00:11:30.567
and we want to stay that way,
so we like our transactions

00:11:30.567 --> 00:11:33.667
to be transactional.

00:11:33.667 --> 00:11:36.667
So this is just a more abstract
of saying all of that.

00:11:36.667 --> 00:11:39.801
Transactions give you
broader consistency,

00:11:39.801 --> 00:11:41.801
correctness,
enforcing invariants,

00:11:41.801 --> 00:11:46.534
and there are four
traditional properties

00:11:46.534 --> 00:11:47.968
that people describe.

00:11:47.968 --> 00:11:49.567
Transactions are atomic,
consistent,

00:11:49.567 --> 00:11:51.334
isolated, and durable.

00:11:51.334 --> 00:11:54.167
And any database textbook will
go into details here.

00:11:54.167 --> 00:11:56.367
But these are nice properties
of transactions

00:11:56.367 --> 00:12:00.133
and just like consistency,
they're hard enough

00:12:00.133 --> 00:12:02.000
when you're operating
on one machine

00:12:02.000 --> 00:12:04.033
or on a few machines inside
a datacenter.

00:12:04.033 --> 00:12:06.000
When you start operating
across datacenters,

00:12:06.000 --> 00:12:07.701
again, you want
to pay attention

00:12:07.701 --> 00:12:10.167
to what could happen
to transactional guarantees

00:12:10.167 --> 00:12:12.400
that you had before you started
spreading out

00:12:12.400 --> 00:12:15.701
all of your work.

00:12:15.701 --> 00:12:18.367
So...I've talked
about consistency.

00:12:18.367 --> 00:12:20.601
I've talked about transactions.

00:12:20.601 --> 00:12:23.968
Now let's get into the more
interesting background.

00:12:23.968 --> 00:12:26.834
Running one datacenter
is hard enough usually.

00:12:26.834 --> 00:12:31.100
Why would you want
to run more than one?

00:12:31.100 --> 00:12:33.467
So there are some
good reasons.

00:12:33.467 --> 00:12:35.701
Catastrophic failures are,
you know, the thing

00:12:35.701 --> 00:12:37.400
no one really wants
to think about.

00:12:37.400 --> 00:12:40.167
But what if your datacenter
fell into the ocean?

00:12:40.167 --> 00:12:42.868
More realistically,
what if there was a fire,

00:12:42.868 --> 00:12:44.567
which does happen?

00:12:44.567 --> 00:12:46.400
What if the power went out
for a couple hours?

00:12:46.400 --> 00:12:48.434
Which is not quite
as violent--

00:12:48.434 --> 00:12:50.734
you probably won't lose any
data, for example--

00:12:50.734 --> 00:12:53.234
but you're down.

00:12:53.234 --> 00:12:55.467
You know, it's no fun.

00:12:55.467 --> 00:12:57.434
There are also a number
of expected failures.

00:12:57.434 --> 00:13:01.100
So there's hardware
that kind of operates

00:13:01.100 --> 00:13:04.267
at the entire datacenter
level in the NOC.

00:13:04.267 --> 00:13:05.968
The network operations center,
for example,

00:13:05.968 --> 00:13:09.934
there are lots
of big iron routers.

00:13:09.934 --> 00:13:12.067
There's the power,
as we talked about,

00:13:12.067 --> 00:13:13.601
there's cooling,

00:13:13.601 --> 00:13:17.801
there's backbone connectivity
in and out.

00:13:17.801 --> 00:13:20.734
Any of these things,
they're usually up for--

00:13:20.734 --> 00:13:23.300
they have good uptime,
but nothing lasts forever.

00:13:23.300 --> 00:13:25.767
And so the more hardware
you're operating with

00:13:25.767 --> 00:13:27.367
in a datacenter,
the more likely it is

00:13:27.367 --> 00:13:30.167
that at any one time something's
going to fail.

00:13:30.167 --> 00:13:33.167
So you don't like it
but you expect it.

00:13:33.167 --> 00:13:34.534
One of the things you do
to prevent that

00:13:34.534 --> 00:13:36.100
is routine maintenance.

00:13:36.100 --> 00:13:37.367
But the problem with
maintenance, again,

00:13:37.367 --> 00:13:38.901
is it takes some piece of
software off

00:13:38.901 --> 00:13:41.400
or some piece
of hardware offline.

00:13:41.400 --> 00:13:44.667
Finally, we call Geolocality.

00:13:44.667 --> 00:13:47.801
It's a big word for putting
stuff near your users

00:13:47.801 --> 00:13:51.601
so their requests get
to it and back fast.

00:13:51.601 --> 00:13:52.634
We like this.

00:13:52.634 --> 00:13:56.334
In the U.S.,
we are somewhat spoiled

00:13:56.334 --> 00:14:00.334
and it's easy to ignore
the benefits of geolocality,

00:14:00.334 --> 00:14:02.634
because lots of stuff
is here.

00:14:02.634 --> 00:14:06.534
And we have good backbone
connectivity to everywhere else.

00:14:06.534 --> 00:14:10.067
If you've ever spent time
in China or India,

00:14:10.067 --> 00:14:13.000
there is kind of
a mixed bag in terms

00:14:13.000 --> 00:14:14.801
of network infrastructure
at best.

00:14:14.801 --> 00:14:16.501
If you've ever spent time
in Australia,

00:14:16.501 --> 00:14:19.200
good network infrastructure,
but it's just far away.

00:14:19.200 --> 00:14:22.300
You really start hitting
speed-of-light constraints.

00:14:22.300 --> 00:14:24.901
How long does it take even
without routers?

00:14:24.901 --> 00:14:27.100
Routing delays
or queuing delays.

00:14:27.100 --> 00:14:29.267
Just that speed of light
to get a packet

00:14:29.267 --> 00:14:32.200
from Australia to the U.S.
and back is a while.

00:14:32.200 --> 00:14:33.567
I don't know that number off
the top of my head,

00:14:33.567 --> 00:14:35.868
but I know a roundtrip from
the west coast of the U.S.

00:14:35.868 --> 00:14:39.167
to the east coast of the U.S.
is 30 milliseconds

00:14:39.167 --> 00:14:41.367
and that's purely
speed of light.

00:14:41.367 --> 00:14:42.868
No routing and queuing.

00:14:42.868 --> 00:14:45.734
And if you go through some
of the big peering points,

00:14:45.734 --> 00:14:50.267
PAIX-East, PAIX-West,
MAE-West, God help you.

00:14:50.267 --> 00:14:52.434
They're always overloaded.

00:14:52.434 --> 00:14:54.701
And they're gonna add another
30, 50 milliseconds

00:14:54.701 --> 00:14:56.767
without breaking a sweat.

00:14:56.767 --> 00:15:00.033
So when you can operate
multiple datacenters,

00:15:00.033 --> 00:15:03.567
some close to your users,
it helps you.

00:15:03.567 --> 00:15:06.567
CDNs like Akamai
are masters of this

00:15:06.567 --> 00:15:09.534
and Amazon's FileFront,
a number of others.

00:15:09.534 --> 00:15:11.501
What you often do
is what we call edge caching.

00:15:11.501 --> 00:15:14.534
Data that doesn't change often,

00:15:14.534 --> 00:15:16.367
you throw it all over
the place.

00:15:16.367 --> 00:15:18.033
You try to get it
as many places

00:15:18.033 --> 00:15:20.667
and as close to as many
people as you can.

00:15:20.667 --> 00:15:25.300
So these are some of the reasons
you do want multihoming.

00:15:25.300 --> 00:15:28.968
But there are plenty of reasons
you don't want it.

00:15:28.968 --> 00:15:33.300
Within a datacenter,
everything's nice, you know?

00:15:33.300 --> 00:15:34.634
Every now and then
we talk about it.

00:15:34.634 --> 00:15:35.767
It's like being in the womb.

00:15:35.767 --> 00:15:37.167
It's warm and comfortable

00:15:37.167 --> 00:15:38.200
and you don't have
to go anywhere

00:15:38.200 --> 00:15:41.133
and everything's
there for you.

00:15:41.133 --> 00:15:43.133
Between machines inside
a datacenter,

00:15:43.133 --> 00:15:44.701
you have really high bandwidth.

00:15:44.701 --> 00:15:48.567
You have really low latency and
usually that networking cost--

00:15:48.567 --> 00:15:50.667
You know, no one charges you
for bandwidth

00:15:50.667 --> 00:15:51.767
within a datacenter.

00:15:51.767 --> 00:15:54.133
It's generally free.

00:15:54.133 --> 00:15:56.367
This is a nice place to be.

00:15:56.367 --> 00:16:00.567
All the stuff is more or less
the opposite across datacenters.

00:16:00.567 --> 00:16:04.968
Much smaller pipes depending
on which company

00:16:04.968 --> 00:16:06.968
you're going with
for backbone connectivity.

00:16:06.968 --> 00:16:08.834
Often they're very small.

00:16:08.834 --> 00:16:10.467
And you can find bigger ones,
but they cost money.

00:16:10.467 --> 00:16:12.033
A lot of money.

00:16:12.033 --> 00:16:14.334
Higher latency,
like I was talking about.

00:16:14.334 --> 00:16:16.501
A roundtrip between two
separate points,

00:16:16.501 --> 00:16:18.534
especially if you're going
across one of those big oceans.

00:16:18.534 --> 00:16:21.000
I mean, the Atlantic
and Pacific are big places.

00:16:21.000 --> 00:16:22.968
That speed of light getting
across them and back

00:16:22.968 --> 00:16:24.667
takes a while.

00:16:24.667 --> 00:16:28.200
And finally, like I mentioned,
that network connectivity costs

00:16:28.200 --> 00:16:31.000
and it's not cheap.

00:16:31.000 --> 00:16:33.734
So if you can help it,
you know,

00:16:33.734 --> 00:16:35.200
you don't want a multihome.

00:16:35.200 --> 00:16:38.701
Not unless you
absolutely have to.

00:16:38.701 --> 00:16:40.300
So--

00:16:40.300 --> 00:16:42.634
Sorry, I should
do the wrap-up.

00:16:42.634 --> 00:16:44.200
Consistency, transactions,

00:16:44.200 --> 00:16:47.734
and now why multihoming?

00:16:47.734 --> 00:16:49.701
So, yeah.

00:16:49.701 --> 00:16:51.434
This is more of the same.

00:16:51.434 --> 00:16:53.100
Like I said,
this a hard problem.

00:16:53.100 --> 00:16:55.000
Particularly hard
if you are running,

00:16:55.000 --> 00:16:58.801
like I talked about,
a read-write data system

00:16:58.801 --> 00:17:02.701
with user data where as soon
as you start accepting writes,

00:17:02.701 --> 00:17:05.634
for example,
in more than one location,

00:17:05.634 --> 00:17:08.200
you're gonna have inconsistency.

00:17:08.200 --> 00:17:11.901
If I try to transfer money
from Alice to Bob over here

00:17:11.901 --> 00:17:15.601
and I try to transfer money
from Alice to Eve over here

00:17:15.601 --> 00:17:19.000
and Alice doesn't have enough
money for both,

00:17:19.000 --> 00:17:22.601
if you let them do that and then
they try to reconcile later,

00:17:22.601 --> 00:17:25.200
again, you've created money
or you've given Alice credit

00:17:25.200 --> 00:17:26.934
where you probably
didn't mean to.

00:17:26.934 --> 00:17:30.067
And so...

00:17:30.067 --> 00:17:33.667
maintaining consistency
in the face of that distance

00:17:33.667 --> 00:17:38.968
and managing those connections
is not trivial.

00:17:38.968 --> 00:17:40.834
So what do we do?

00:17:40.834 --> 00:17:43.968
Like I said, often you just want
to avoid this entirely.

00:17:43.968 --> 00:17:46.200
And lots of people do.

00:17:46.200 --> 00:17:47.701
And this is--

00:17:47.701 --> 00:17:50.000
there's a lot
to be said for this.

00:17:50.000 --> 00:17:52.167
Solving hard problems
is by definition hard

00:17:52.167 --> 00:17:54.367
and the more hard problems
you try to solve,

00:17:54.367 --> 00:17:56.901
the less likely you are
to be good at any one of them.

00:17:56.901 --> 00:18:02.434
So lots of people just don't
and more power to 'em.

00:18:02.434 --> 00:18:06.501
Often the way you do this is
we use the word bunkerizing.

00:18:06.501 --> 00:18:09.234
You bring in five sources
of redundant power

00:18:09.234 --> 00:18:11.234
and you have four
separate companies

00:18:11.234 --> 00:18:12.534
that give you backbone
connectivity

00:18:12.534 --> 00:18:14.834
and you have cooling that,
you know,

00:18:14.834 --> 00:18:16.801
ranges from massive
water-cooled systems

00:18:16.801 --> 00:18:19.767
to ice that you picked up
at the 7-11.

00:18:19.767 --> 00:18:21.234
But you pick up
that ice every day

00:18:21.234 --> 00:18:23.000
and you pick it up from maybe
two different 7-11s

00:18:23.000 --> 00:18:24.367
just so that it's redundant.

00:18:24.367 --> 00:18:27.534
So, you know, regardless of what
you're using to cool,

00:18:27.534 --> 00:18:30.901
you got something and you got
three others in the back.

00:18:30.901 --> 00:18:32.667
You know, bags of ice
in the trunk

00:18:32.667 --> 00:18:35.767
just in case your big
water-cooled system goes down.

00:18:35.767 --> 00:18:37.934
Lots of people do this.

00:18:37.934 --> 00:18:41.334
Microsoft Azure right now
serves out of--

00:18:41.334 --> 00:18:42.901
forgotten exactly
what they said.

00:18:42.901 --> 00:18:45.067
I believe they only say
North America and not U.S.,

00:18:45.067 --> 00:18:47.901
but they right now offer
one U.S. location.

00:18:47.901 --> 00:18:49.667
In the future
they will give you the--

00:18:49.667 --> 00:18:51.300
Sorry,
one North America location.

00:18:51.300 --> 00:18:53.300
In the future,
they will give you the choice

00:18:53.300 --> 00:18:56.067
of two North America
locations.

00:18:56.067 --> 00:18:59.701
But no help in running
something across them.

00:18:59.701 --> 00:19:02.601
Amazon's SimpleDB from what
we know is similar.

00:19:02.601 --> 00:19:04.968
I believe there are, you know,
real-time backups.

00:19:04.968 --> 00:19:08.734
But unlike most of the other
Amazon web services,

00:19:08.734 --> 00:19:11.400
you can't choose
a physical region

00:19:11.400 --> 00:19:13.167
or availability zones
for SimpleDB.

00:19:13.167 --> 00:19:17.133
I'll get more
into this in a bit.

00:19:17.133 --> 00:19:20.167
All the reasons we talked
about for multihoming

00:19:20.167 --> 00:19:22.300
are reasons
why you don't want to--

00:19:22.300 --> 00:19:26.033
why not multihoming are bad.

00:19:26.033 --> 00:19:28.934
One is, if it is a destructive
or somewhat destructive

00:19:28.934 --> 00:19:31.634
problem that happens--
fire.

00:19:31.634 --> 00:19:34.200
Falling into the ocean
is the example we all use.

00:19:34.200 --> 00:19:36.000
Not gonna happen,
but it's very colorful,

00:19:36.000 --> 00:19:37.901
so that's always fun.

00:19:37.901 --> 00:19:40.801
You could lose data.

00:19:40.801 --> 00:19:42.567
You know, we always talk
about backing up

00:19:42.567 --> 00:19:45.200
and replicating across disks,
because we know

00:19:45.200 --> 00:19:46.834
disks and machines fail.

00:19:46.834 --> 00:19:49.400
But sometimes physical
locations fail too.

00:19:49.400 --> 00:19:51.133
Fire, hurricane, whatever.

00:19:51.133 --> 00:19:54.334
And ideally, you don't want
to lose data,

00:19:54.334 --> 00:19:58.467
because the data loss
is pretty much always bad.

00:19:58.467 --> 00:20:01.100
I don't know how many
of you noticed

00:20:01.100 --> 00:20:02.968
the recent SVColo outage.

00:20:02.968 --> 00:20:06.467
SVColo is a datacenter that
is located in Silicon Valley.

00:20:06.467 --> 00:20:10.734
Recently had a power outage
for 2, 2 1/2 hours.

00:20:10.734 --> 00:20:13.167
People noticed only because
Twitter and FriendFeed

00:20:13.167 --> 00:20:15.234
were singly homed in SVColo

00:20:15.234 --> 00:20:17.701
and so they're both pretty
popular.

00:20:17.701 --> 00:20:19.968
Hands up if you use Twitter.

00:20:19.968 --> 00:20:22.000
And, okay, so just out
of curiosity,

00:20:22.000 --> 00:20:24.100
hands up if you
use FriendFeed.

00:20:24.100 --> 00:20:26.067
All right, okay, good stuff.

00:20:26.067 --> 00:20:27.767
The FriendFeed--

00:20:27.767 --> 00:20:29.567
One of their co-founders
was the original

00:20:29.567 --> 00:20:31.834
product manager on App Engine.

00:20:31.834 --> 00:20:33.634
Bret Taylor.
Good guy.

00:20:33.634 --> 00:20:36.167
But, you know, poor companies.

00:20:36.167 --> 00:20:37.434
They were singly homed.

00:20:37.434 --> 00:20:38.734
They were down
for two hours straight.

00:20:38.734 --> 00:20:40.801
2 1/2 hours.

00:20:40.801 --> 00:20:42.701
They came back up.
They didn't have any data loss.

00:20:42.701 --> 00:20:44.100
Thank God.

00:20:44.100 --> 00:20:45.334
But they were still down,
you know,

00:20:45.334 --> 00:20:46.801
in the middle
of the day for two hours.

00:20:46.801 --> 00:20:48.367
No one wants that, you know?

00:20:48.367 --> 00:20:49.901
And not their fault.

00:20:49.901 --> 00:20:52.601
SVColo should have had some more
redundant power,

00:20:52.601 --> 00:20:54.334
but it happens.

00:20:54.334 --> 00:20:56.100
Often what happens
is human error

00:20:56.100 --> 00:20:57.567
where no matter how much
redundant power

00:20:57.567 --> 00:20:58.934
or connectivity you have,

00:20:58.934 --> 00:21:00.601
someone presses
the wrong button.

00:21:00.601 --> 00:21:03.934
That's the cause
of many outages.

00:21:03.934 --> 00:21:07.667
So these kinds of things,
you know,

00:21:07.667 --> 00:21:09.400
if you don't multihome,
this is what

00:21:09.400 --> 00:21:11.367
you might be in for.

00:21:11.367 --> 00:21:13.901
Finally, that geolocation thing
we talked about,

00:21:13.901 --> 00:21:16.834
being close to users,
now more than ever,

00:21:16.834 --> 00:21:19.501
your users are everywhere
physically.

00:21:19.501 --> 00:21:23.734
You know, they're connected,
but connectivity is not flat.

00:21:23.734 --> 00:21:27.801
There's a great quote from--
Ah, who is it?

00:21:27.801 --> 00:21:30.501
Um.
I forgot.

00:21:30.501 --> 00:21:32.801
I think it's one of Tim
Morelli's favorite quotes.

00:21:32.801 --> 00:21:34.033
The future is here.

00:21:34.033 --> 00:21:38.033
It's just unevenly
distributed.

00:21:38.033 --> 00:21:39.567
Connectivity is the same way.

00:21:39.567 --> 00:21:40.734
Very unevenly distributed.

00:21:40.734 --> 00:21:42.901
So, yeah, if you're
on one datacenter,

00:21:42.901 --> 00:21:44.567
everyone's got to get there

00:21:44.567 --> 00:21:45.901
and so the people
who are far away,

00:21:45.901 --> 00:21:48.968
it's gonna be slow.

00:21:48.968 --> 00:21:51.968
The next option is,
you're on multiple datacenters,

00:21:51.968 --> 00:21:54.534
but it's similar to MySQL
master-slave replication.

00:21:54.534 --> 00:21:57.801
Only one of them
is your master.

00:21:57.801 --> 00:21:59.901
The rest you could
sort of read from

00:21:59.901 --> 00:22:02.067
for this kind of read-write
database-style data,

00:22:02.067 --> 00:22:04.167
but you don't serve
writes from them.

00:22:04.167 --> 00:22:07.968
You serve writes from
the one master or datacenter.

00:22:07.968 --> 00:22:11.067
This is better,
obviously.

00:22:11.067 --> 00:22:12.801
It handles catastrophic
failure better.

00:22:12.801 --> 00:22:15.634
You still have your data
in other places.

00:22:15.634 --> 00:22:17.834
Since your replicating
asynchronously

00:22:17.834 --> 00:22:20.400
in the background...

00:22:20.400 --> 00:22:22.000
there will be some window
of data

00:22:22.000 --> 00:22:24.767
that was kind of on the wire or
hadn't quite made it

00:22:24.767 --> 00:22:26.400
out of the datacenter.

00:22:26.400 --> 00:22:28.934
You know, if a datacenter
falls in the ocean,

00:22:28.934 --> 00:22:31.868
that small window that hasn't
made it over

00:22:31.868 --> 00:22:35.801
to another datacenter
is probably lost.

00:22:35.801 --> 00:22:38.067
In reality, doesn't fall
in the ocean.

00:22:38.067 --> 00:22:39.501
It just goes offline
for a while.

00:22:39.501 --> 00:22:42.868
Comes back up and you get that
window of data back usually.

00:22:42.868 --> 00:22:45.934
But it's not available while
the datacenter's down

00:22:45.934 --> 00:22:47.400
and if there was something
destructive,

00:22:47.400 --> 00:22:50.300
often you've
lost it entirely.

00:22:50.300 --> 00:22:53.300
Finally, depending on what kind
of technique you use--

00:22:53.300 --> 00:22:55.734
and we'll get into details
in a bit--

00:22:55.734 --> 00:22:57.567
the data in your other
datacenters may or may not

00:22:57.567 --> 00:22:58.834
be consistent.

00:22:58.834 --> 00:23:00.267
They may not be consistent
at all.

00:23:00.267 --> 00:23:01.834
It may not be consistent

00:23:01.834 --> 00:23:03.200
with regard to
your transactions.

00:23:03.200 --> 00:23:05.701
So this is something
to pay attention to

00:23:05.701 --> 00:23:07.601
as we get into techniques.

00:23:07.601 --> 00:23:09.200
Examples of this.

00:23:09.200 --> 00:23:12.133
Amazon has done a good job
of making

00:23:12.133 --> 00:23:15.467
physical location
and availability zones,

00:23:15.467 --> 00:23:18.601
like, different day centers,
more transparent.

00:23:18.601 --> 00:23:21.634
So if you use EC2 or S3,
SimpleQ service,

00:23:21.634 --> 00:23:25.634
you can choose where your VMs
and EC2 or your files

00:23:25.634 --> 00:23:28.400
in S3 are located
in the U.S. or the E.U.

00:23:28.400 --> 00:23:30.968
So you can choose
your master, at least.

00:23:30.968 --> 00:23:33.868
EC2 also gives you a choice
of what they call

00:23:33.868 --> 00:23:36.434
availability zones,
which are more or less

00:23:36.434 --> 00:23:39.701
separate or physically
separated datacenters

00:23:39.701 --> 00:23:42.701
within that region.

00:23:42.701 --> 00:23:44.601
I actually had to change
this slide recently.

00:23:44.601 --> 00:23:47.501
They announced scaling,
load balancing, monitoring.

00:23:47.501 --> 00:23:50.834
Stuff that will help you
effectively multihome

00:23:50.834 --> 00:23:54.534
across their actually zones,
which is very exciting.

00:23:54.534 --> 00:23:57.133
We've been inspired by a lot
of stuff they've done

00:23:57.133 --> 00:23:58.434
and so we continue
to get excited

00:23:58.434 --> 00:24:02.033
about features like this.

00:24:02.033 --> 00:24:05.434
Many financial institutions use
this kind of multihoming.

00:24:05.434 --> 00:24:09.334
You hear about credit card
companies with data vaults,

00:24:09.334 --> 00:24:11.601
you know,
under six feet of lead.

00:24:11.601 --> 00:24:13.767
Underneath--I believe it's
Iron Mountain in Virginia

00:24:13.767 --> 00:24:15.734
or a few other locations.

00:24:15.734 --> 00:24:18.667
Makes you think of the old days
in the '50s

00:24:18.667 --> 00:24:21.400
with bomb shelters.

00:24:21.400 --> 00:24:23.901
Kind of a little scary story,
but it's, you know--

00:24:23.901 --> 00:24:28.200
it's always fun to go actually
tour one of those sites.

00:24:28.200 --> 00:24:30.167
Finally, you get some
geolocation here.

00:24:30.167 --> 00:24:32.734
All of your slaves can
serve reads,

00:24:32.734 --> 00:24:35.434
depending, again on--
we'll look at the techniques.

00:24:35.434 --> 00:24:37.400
Depending on what kind
of consistency 

00:24:37.400 --> 00:24:39.000
and transaction--

00:24:39.000 --> 00:24:41.467
respecting transactions that
you expect.

00:24:41.467 --> 00:24:44.534
If you're careful,
your slaves can serve reads

00:24:44.534 --> 00:24:46.534
just like, again,
MySQL replication,

00:24:46.534 --> 00:24:50.033
which a lot of you who've used
LAMP stacks might have used.

00:24:50.033 --> 00:24:52.100
But you still only have
the capacity

00:24:52.100 --> 00:24:53.767
of one datacenter
and the geolocation 

00:24:53.767 --> 00:24:57.100
of one datacenter
for writes.

00:24:57.100 --> 00:24:59.767
The last option
is the holy grail.

00:24:59.767 --> 00:25:01.234
True multihoming.

00:25:01.234 --> 00:25:02.701
You have lots of
different datacenters.

00:25:02.701 --> 00:25:04.200
They're all serving reads
and writes.

00:25:04.200 --> 00:25:05.701
They're all magically
consistent.

00:25:05.701 --> 00:25:08.601
They, you know--
transactions just work.

00:25:08.601 --> 00:25:10.234
That's really hard.

00:25:10.234 --> 00:25:12.868
Really hard.

00:25:12.868 --> 00:25:14.834
It's hard--so there are
a number of examples

00:25:14.834 --> 00:25:17.100
that do it with just
two datacenters.

00:25:17.100 --> 00:25:19.734
One of my favorite stories
here is--

00:25:19.734 --> 00:25:21.467
I believe this might
be an urban myth,

00:25:21.467 --> 00:25:22.701
but NASDAQ.

00:25:22.701 --> 00:25:24.334
There's a story
about how NASDAQ

00:25:24.334 --> 00:25:28.534
runs two datacenters
near each other.

00:25:28.534 --> 00:25:31.801
Near each other for that whole
speed of light thing.

00:25:31.801 --> 00:25:35.300
And they do two-phase commit
across the two datacenters.

00:25:35.300 --> 00:25:37.501
So every single trade or every
single transaction

00:25:37.501 --> 00:25:40.968
on NASDAQ is guaranteed
to atomically be persistent

00:25:40.968 --> 00:25:43.367
to either both or neither.

00:25:43.367 --> 00:25:45.734
So the story is
they had to go buy

00:25:45.734 --> 00:25:47.734
some networking equipment
at some point.

00:25:47.734 --> 00:25:49.167
And they walk in and,
you know,

00:25:49.167 --> 00:25:50.934
got their
purchasing manager

00:25:50.934 --> 00:25:53.467
and a bunch of engineers come
to do testing

00:25:53.467 --> 00:25:55.267
and they're carrying
these suitcases.

00:25:55.267 --> 00:25:57.634
They got, like,
five suitcases.

00:25:57.634 --> 00:26:01.334
You know, they look heavy and
the networking vendor

00:26:01.334 --> 00:26:04.067
looks at 'em and says, "Hey,
you know, what's that for?"

00:26:04.067 --> 00:26:05.334
And they open up the suitcases.

00:26:05.334 --> 00:26:07.801
They're full of fiber.

00:26:07.801 --> 00:26:09.701
And they say, "You know,
our datacenters

00:26:09.701 --> 00:26:12.467
"are 27 miles apart.

00:26:12.467 --> 00:26:16.100
"This is exactly 27 miles
of fiber.

00:26:16.100 --> 00:26:18.267
"Our algorithm, our two-phase
commit thing, works

00:26:18.267 --> 00:26:21.267
"if you give us, like, a two
or three-millisecond window.

00:26:21.267 --> 00:26:23.033
"So if you can guarantee--
We're gonna hook it up.

00:26:23.033 --> 00:26:24.667
"We got this 27 miles
of fiber.

00:26:24.667 --> 00:26:26.501
"If you put one of
your routers over here,

00:26:26.501 --> 00:26:28.968
"one of 'em over here,
we'll do some measurements.

00:26:28.968 --> 00:26:31.033
"If you hit, like,
that two-millisecond window,

00:26:31.033 --> 00:26:32.467
"we can do business.

00:26:32.467 --> 00:26:36.334
Otherwise, we got to take our
fiber over to the next guy."

00:26:36.334 --> 00:26:38.834
So, um...

00:26:38.834 --> 00:26:41.534
they do it across two
datacenters and that's hard.

00:26:41.534 --> 00:26:43.934
Doing it across multiple
datacenters, more than two,

00:26:43.934 --> 00:26:47.133
is fundamentally harder.

00:26:47.133 --> 00:26:49.667
On the plus side,
you get all of the benefits.

00:26:49.667 --> 00:26:51.601
This does the magic for you.

00:26:51.601 --> 00:26:53.467
You handle
catastrophic failure,

00:26:53.467 --> 00:26:55.300
expected failure,
you can serve people

00:26:55.300 --> 00:26:58.667
from whatever datacenter
was closest to them.

00:26:58.667 --> 00:27:02.000
Naturally, if you can
do this at all,

00:27:02.000 --> 00:27:04.200
you're gonna pay for it.

00:27:04.200 --> 00:27:07.133
Queuing delays, routing delays,
speed of light, all of that.

00:27:07.133 --> 00:27:08.701
You got to talk
between datacenters

00:27:08.701 --> 00:27:12.000
and it's just fundamentally
slower and a smaller pipe.

00:27:12.000 --> 00:27:14.234
So sometimes you might
also pay for it

00:27:14.234 --> 00:27:16.501
in capacity or throughput,
but you'll definitely

00:27:16.501 --> 00:27:19.434
pay for it in latency.

00:27:19.434 --> 00:27:21.367
All right,
so we've got through

00:27:21.367 --> 00:27:24.567
the bulk of the background.

00:27:24.567 --> 00:27:27.133
Again, talked about consistency
and transactions,

00:27:27.133 --> 00:27:29.767
why do you want to multihome,
and what are some of

00:27:29.767 --> 00:27:32.734
the kind of broad categories
of multihoming

00:27:32.734 --> 00:27:33.901
that you might see.

00:27:33.901 --> 00:27:35.300
Let's get into some details.

00:27:35.300 --> 00:27:36.634
Like, how do you actually
do this

00:27:36.634 --> 00:27:40.133
if you're building
one of these systems.

00:27:40.133 --> 00:27:43.234
So this is kind of
the broad outline

00:27:43.234 --> 00:27:46.467
for where we're gonna go
in the next 15 minutes.

00:27:46.467 --> 00:27:49.067
Down the left side
are things we care about.

00:27:49.067 --> 00:27:52.100
This is how we want to evaluate
different techniques

00:27:52.100 --> 00:27:53.501
that we're gonna look at.

00:27:53.501 --> 00:27:56.367
What kind of consistency
in transactions do we get?

00:27:56.367 --> 00:27:59.734
What kind of latency,
throughput?

00:27:59.734 --> 00:28:03.000
If something happens,
will we lose any data?

00:28:03.000 --> 00:28:05.067
How much data will we lose?

00:28:05.067 --> 00:28:07.467
And when we have to failover,
if there is

00:28:07.467 --> 00:28:10.234
expected maintenance
or if we need to move things--

00:28:10.234 --> 00:28:12.334
Say we're decommissioning
some datacenter,

00:28:12.334 --> 00:28:14.167
we need to turn up
another one.

00:28:14.167 --> 00:28:15.534
What we call failover,

00:28:15.534 --> 00:28:17.567
basically moving some
serving capacity

00:28:17.567 --> 00:28:19.167
from one datacenter
to another.

00:28:19.167 --> 00:28:20.334
How do we do that?

00:28:20.334 --> 00:28:23.100
How well do
the techniques support it?

00:28:23.100 --> 00:28:24.367
This is a kind of preview.

00:28:24.367 --> 00:28:25.934
The actual techniques
that we're gonna discuss

00:28:25.934 --> 00:28:27.234
are across the top.

00:28:27.234 --> 00:28:31.100
And I'll get into detail
right now.

00:28:31.100 --> 00:28:33.367
Backups,
we all know and love.

00:28:33.367 --> 00:28:37.901
We all say you should backup
and, you know,

00:28:37.901 --> 00:28:40.133
to varying degrees,
we actually do backup.

00:28:40.133 --> 00:28:43.000
You know, our laptops,
our phones, that kind of thing.

00:28:43.000 --> 00:28:48.667
Your data as a company or
organization is no different.

00:28:48.667 --> 00:28:51.701
You got to backup at minimum.

00:28:51.701 --> 00:28:52.968
So this is a sledgehammer.

00:28:52.968 --> 00:28:54.601
You know, you make a copy
of all your data.

00:28:54.601 --> 00:28:55.868
You got it.

00:28:55.868 --> 00:28:57.167
Now you got
to keep it somewhere,

00:28:57.167 --> 00:29:00.767
and, you know,
make sure it's stored

00:29:00.767 --> 00:29:03.334
and archived for a while
so that it lasts.

00:29:03.334 --> 00:29:06.567
But the nice thing is you got
this entire copy.

00:29:06.567 --> 00:29:09.000
I said weak consistency here.

00:29:09.000 --> 00:29:10.868
It depends.
So some--

00:29:10.868 --> 00:29:12.968
depending on how you designed
your system,

00:29:12.968 --> 00:29:15.934
if you can backup at
a consistent snapshot

00:29:15.934 --> 00:29:20.767
whether you use timestamps
or versioning or whatever,

00:29:20.767 --> 00:29:23.767
then you can maintain some
consistency and often

00:29:23.767 --> 00:29:25.767
you can maintain
transactions too.

00:29:25.767 --> 00:29:29.067
The majority of time, backups
don't work like this.

00:29:29.067 --> 00:29:32.167
Mostly you start reading
and copying, you know,

00:29:32.167 --> 00:29:33.501
at the beginning
of your data

00:29:33.501 --> 00:29:34.801
and you go through
to the end.

00:29:34.801 --> 00:29:35.968
It might take a few hours.

00:29:35.968 --> 00:29:38.133
It might take a day.
It might take a week.

00:29:38.133 --> 00:29:42.000
And so you see an earlier
state at the beginning

00:29:42.000 --> 00:29:43.868
than you do at the end.

00:29:43.868 --> 00:29:45.734
So if I start reading--

00:29:45.734 --> 00:29:47.467
So if I'm transferring money
from "A" to "B"

00:29:47.467 --> 00:29:49.767
and I read "A"
and I've backed up "A"

00:29:49.767 --> 00:29:52.367
and then someone transfers
that money to "B"

00:29:52.367 --> 00:29:54.801
and then I read "B,"

00:29:54.801 --> 00:29:56.801
I've lost some of that
transactional consistency

00:29:56.801 --> 00:29:59.200
in my backup
and so it takes some thought

00:29:59.200 --> 00:30:02.133
to avoid
that kind of problem.

00:30:02.133 --> 00:30:04.434
So the other thing
I should have mentioned

00:30:04.434 --> 00:30:06.968
is I'm gonna be using the
App Engine datastore again

00:30:06.968 --> 00:30:09.200
as a case study here.

00:30:09.200 --> 00:30:12.000
Because we evaluated all
of these techniques

00:30:12.000 --> 00:30:14.467
for multihoming the datastore
and multihoming App Engine

00:30:14.467 --> 00:30:15.767
in general.

00:30:15.767 --> 00:30:17.167
We did this with
the datastore.

00:30:17.167 --> 00:30:21.067
You know, when we first
dogfooded the App Engine, we--

00:30:21.067 --> 00:30:24.167
So we do a lot of what we call
"dogfooding" at Google.

00:30:24.167 --> 00:30:27.901
We iterate and make people use
our stuff internally,

00:30:27.901 --> 00:30:29.133
often when it's not so good.

00:30:29.133 --> 00:30:31.200
And so people get sick
of this at Google.

00:30:31.200 --> 00:30:33.801
You're using a ton of stuff
that's very early

00:30:33.801 --> 00:30:36.000
and, you know, sometimes it
works, sometimes it doesn't,

00:30:36.000 --> 00:30:37.834
but the benefit
is you get to iterate

00:30:37.834 --> 00:30:39.634
and you have users constantly
using your stuff

00:30:39.634 --> 00:30:42.033
and you can iterate 20,
30, 50 times on something

00:30:42.033 --> 00:30:43.801
before you're ready
to launch it.

00:30:43.801 --> 00:30:46.701
There's huge value.

00:30:46.701 --> 00:30:49.767
Again, it's early and so--

00:30:49.767 --> 00:30:52.033
when we first started dogfooding
App Engine internally

00:30:52.033 --> 00:30:53.901
at Google,
backups were good enough.

00:30:53.901 --> 00:30:55.934
You know, we were okay
with that.

00:30:55.934 --> 00:30:58.567
But clearly, it's not good
enough for a production system,

00:30:58.567 --> 00:31:00.968
so we need to keep looking.

00:31:00.968 --> 00:31:05.000
But as we evaluate, you see
there's a lot of red there.

00:31:05.000 --> 00:31:06.734
There's some green too.

00:31:06.734 --> 00:31:08.434
Backups are totally offline.

00:31:08.434 --> 00:31:11.000
They don't affect how
your storage system works,

00:31:11.000 --> 00:31:13.501
so your latency and
your throughput don't change.

00:31:13.501 --> 00:31:15.501
Assuming you have good latency
and good throughput,

00:31:15.501 --> 00:31:16.934
stays that way.

00:31:16.934 --> 00:31:19.334
The rest of it is not so good.

00:31:19.334 --> 00:31:20.701
You're gonna lose
all your data

00:31:20.701 --> 00:31:25.033
if something happens
since the last backup.

00:31:25.033 --> 00:31:28.334
Moving to another datacenter,
you got to backup and restore.

00:31:28.334 --> 00:31:30.701
You know, while you do that
move, you're down.

00:31:30.701 --> 00:31:33.234
As we talked about,
usually consistency

00:31:33.234 --> 00:31:36.434
and transaction support
is weak.

00:31:36.434 --> 00:31:38.901
Let's see if we can
do better.

00:31:38.901 --> 00:31:40.234
Master-slave replication.

00:31:40.234 --> 00:31:42.367
This is basically the same
as what we talked about

00:31:42.367 --> 00:31:45.200
primary and hot fail over
a datastore.

00:31:45.200 --> 00:31:48.968
Or--sorry.
Datacenters for multihoming.

00:31:48.968 --> 00:31:50.434
Again, like we talked about,

00:31:50.434 --> 00:31:53.934
MySQL is very commonly used,
but most relational databases

00:31:53.934 --> 00:31:56.834
support something like this.

00:31:56.834 --> 00:31:58.567
The replication
is asynchronous.

00:31:58.567 --> 00:32:00.300
It's in the background,
so it doesn't

00:32:00.300 --> 00:32:03.667
affect your online rights.

00:32:03.667 --> 00:32:06.801
Which means, like backups,
you generally keep

00:32:06.801 --> 00:32:08.767
the throughput and
the latency that you had

00:32:08.767 --> 00:32:10.300
before you turned this on.

00:32:10.300 --> 00:32:13.901
Generally.

00:32:13.901 --> 00:32:17.100
Similar to backups,
the consistency

00:32:17.100 --> 00:32:20.000
and transaction support
that you get from this--

00:32:20.000 --> 00:32:22.634
If you work hard,
you can get decent support.

00:32:22.634 --> 00:32:27.634
but often you're just kind of
sending data as it comes in.

00:32:27.634 --> 00:32:30.100
You're just kind of blindly
moving everything over.

00:32:30.100 --> 00:32:32.968
And so if you get cut off
halfway through

00:32:32.968 --> 00:32:37.033
a transaction
or something like that,

00:32:37.033 --> 00:32:42.167
the state of the data that is in
your replica is uncertain.

00:32:42.167 --> 00:32:46.934
And so like backups,
you can design it to do better.

00:32:46.934 --> 00:32:49.167
But it's non-trivial.

00:32:49.167 --> 00:32:52.501
You got to think about
that ahead of time.

00:32:52.501 --> 00:32:54.200
So this is actually
what we currently do

00:32:54.200 --> 00:32:56.801
with the datastore.

00:32:56.801 --> 00:32:58.400
The consistency
and transaction stuff,

00:32:58.400 --> 00:33:02.334
we put the time in,
we figured that out.

00:33:02.334 --> 00:33:06.033
The catch here is that
with end-user applications,

00:33:06.033 --> 00:33:07.968
you can do a lot better.

00:33:07.968 --> 00:33:09.501
You can do that whole two-phase
commit thing

00:33:09.501 --> 00:33:13.734
or even better and hide it
behind Ajax or whatever,

00:33:13.734 --> 00:33:16.767
so that that latency hit
that you're paying for,

00:33:16.767 --> 00:33:19.801
your users don't see.

00:33:19.801 --> 00:33:25.634
We're offering a much
lower level abstraction.

00:33:25.634 --> 00:33:27.968
Honestly, our competition is
as much relational databases

00:33:27.968 --> 00:33:29.567
living in the same
datacenter.

00:33:29.567 --> 00:33:31.834
Like, one machine over,
right next to you

00:33:31.834 --> 00:33:33.634
are your App servers.

00:33:33.634 --> 00:33:36.534
As it is, Amazon SimpleDB
or Azure tables

00:33:36.534 --> 00:33:38.267
or anything like that.

00:33:38.267 --> 00:33:41.834
You know, updates or inserts
into a relational database?

00:33:41.834 --> 00:33:43.767
Low milliseconds.

00:33:43.767 --> 00:33:45.801
You know, our average write
on App Engine

00:33:45.801 --> 00:33:47.000
is already much higher.

00:33:47.000 --> 00:33:50.067
You're talking 30, 40
milliseconds.

00:33:50.067 --> 00:33:51.767
It's a different model.

00:33:51.767 --> 00:33:54.534
Reads in exchange
are often much faster

00:33:54.534 --> 00:33:57.133
and we like that trade off,
because on the web

00:33:57.133 --> 00:34:00.100
reads outnumber writes vastly.

00:34:00.100 --> 00:34:03.868
But the problem
is multihoming--

00:34:03.868 --> 00:34:06.801
Truly multihoming
the datastore

00:34:06.801 --> 00:34:08.968
as we're gonna see
later down the line,

00:34:08.968 --> 00:34:10.400
would add latency.

00:34:10.400 --> 00:34:13.801
You would see that latency
doubled if not more,

00:34:13.801 --> 00:34:15.801
because you'd have to be paying
for that roundtrip

00:34:15.801 --> 00:34:17.901
between datacenters.

00:34:17.901 --> 00:34:21.467
So most startups,
most LAMP stacks,

00:34:21.467 --> 00:34:26.267
anything that's
singly homed, you know,

00:34:26.267 --> 00:34:27.834
is not even quite that good.

00:34:27.834 --> 00:34:30.634
So we already feel like there's
a win here for multihoming.

00:34:30.634 --> 00:34:32.767
And so the trade off,
we said "Okay."

00:34:32.767 --> 00:34:34.934
You know, we're gonna try
to pick the sweet spot

00:34:34.934 --> 00:34:37.200
where we give you
almost all the benefits

00:34:37.200 --> 00:34:40.267
of multihoming and still
avoid some

00:34:40.267 --> 00:34:43.567
of the most egregious costs.

00:34:43.567 --> 00:34:45.901
There's a better conclusion
that I'll get to in a bit.

00:34:45.901 --> 00:34:47.267
But this is one
of the takeaways.

00:34:47.267 --> 00:34:50.100
This is how we multihome
the datastore

00:34:50.100 --> 00:34:53.601
across multiple datacenters.

00:34:53.601 --> 00:34:57.934
So like we said,
depending on how you--

00:34:57.934 --> 00:35:00.968
So...this is actually
not quite fair.

00:35:00.968 --> 00:35:02.734
I've given backups
a bad rap compared

00:35:02.734 --> 00:35:05.634
to master-slave on consistency
in transactions,

00:35:05.634 --> 00:35:08.534
but generally it's harder to do
those with backups.

00:35:08.534 --> 00:35:11.767
So I guess that's fair.

00:35:11.767 --> 00:35:15.100
Yeah, so if something happens,
you have an almost real-time

00:35:15.100 --> 00:35:17.000
copy of data in
your other datacenters.

00:35:17.000 --> 00:35:19.467
So you're not gonna lose
nearly as much data,

00:35:19.467 --> 00:35:21.000
but you'll a little
in that small window.

00:35:21.000 --> 00:35:24.734
And failover,
often you can go read-only

00:35:24.734 --> 00:35:27.734
and serve reads from one
of your slave--

00:35:27.734 --> 00:35:29.033
your replica datacenters.

00:35:29.033 --> 00:35:30.567
But you're not gonna
serve writes

00:35:30.567 --> 00:35:32.834
until you have moved
your master

00:35:32.834 --> 00:35:35.067
and you know that,
okay, it's in a new datacenter

00:35:35.067 --> 00:35:37.033
and no writes
are only going here.

00:35:37.033 --> 00:35:40.534
Now I can turn them on.

00:35:40.534 --> 00:35:43.267
Multi-master replication
is fascinating.

00:35:43.267 --> 00:35:45.400
This has been one of
the most interesting areas

00:35:45.400 --> 00:35:47.467
of computer science
for a while now.

00:35:47.467 --> 00:35:51.067
It's a lot of fun.

00:35:51.067 --> 00:35:53.167
The idea here is that
you support writes

00:35:53.167 --> 00:35:55.100
in multiple places
and you find some way

00:35:55.100 --> 00:35:57.634
to merge them
at some point later.

00:35:57.634 --> 00:36:00.100
So this is similar
asynchronous replication,

00:36:00.100 --> 00:36:03.033
but you're serving writes
in multiple locations

00:36:03.033 --> 00:36:04.534
and you got to figure out
what to do with them

00:36:04.534 --> 00:36:07.667
when there is a collision.

00:36:07.667 --> 00:36:10.200
The best you can do here
is eventual consistency,

00:36:10.200 --> 00:36:13.767
because writes don't
immediately go everywhere.

00:36:13.767 --> 00:36:15.334
We've--

00:36:15.334 --> 00:36:17.067
There's a paradigm shift here.

00:36:17.067 --> 00:36:18.734
We've assumed
up until this point

00:36:18.734 --> 00:36:21.767
that if you have a strong
and consistent system--

00:36:21.767 --> 00:36:23.801
master-slave and backup,
we're not affecting

00:36:23.801 --> 00:36:25.033
how that system runs.

00:36:25.033 --> 00:36:26.367
We're just doing stuff
in the background

00:36:26.367 --> 00:36:28.067
to help multihome.

00:36:28.067 --> 00:36:30.901
Here we're literally changing
the way the system runs,

00:36:30.901 --> 00:36:32.834
because we've got kind
of multiple installations

00:36:32.834 --> 00:36:35.300
and they're each serving data
and in the background

00:36:35.300 --> 00:36:38.367
they have to reconcile.

00:36:38.367 --> 00:36:40.567
The way you do this
in the background--

00:36:40.567 --> 00:36:44.667
or the way you do the merging
is you find a way to serialize.

00:36:44.667 --> 00:36:47.467
You find a way to impose
an ordering

00:36:47.467 --> 00:36:50.267
across all of your writes.

00:36:50.267 --> 00:36:53.200
One of the tenets
in distributed systems

00:36:53.200 --> 00:36:55.033
is there is no global clock.

00:36:55.033 --> 00:36:58.167
At least there
is no global clock you can use.

00:36:58.167 --> 00:37:00.734
Things happen in parallel
and you can't really

00:37:00.734 --> 00:37:04.601
ever know what happens when
or at least

00:37:04.601 --> 00:37:07.033
with regards to each other.

00:37:07.033 --> 00:37:09.767
So here we make it up.

00:37:09.767 --> 00:37:12.934
You can even have something
that hands out

00:37:12.934 --> 00:37:15.634
what we call monotonically
increasing timestamps.

00:37:15.634 --> 00:37:18.968
Time stamps that are guaranteed
not to revert.

00:37:18.968 --> 00:37:22.167
You can use local timestamps

00:37:22.167 --> 00:37:24.968
with skew detection
and other things.

00:37:24.968 --> 00:37:26.767
You can use local versions.

00:37:26.767 --> 00:37:29.968
So for a given entity, I don't
need to know a global clock.

00:37:29.968 --> 00:37:31.267
All I need to know is
for this entity,

00:37:31.267 --> 00:37:33.601
what's it's version number.

00:37:33.601 --> 00:37:35.501
You can use a distributed
consensus protocol,

00:37:35.501 --> 00:37:37.067
which we'll get to
in a bit.

00:37:37.067 --> 00:37:38.767
It's kind of putting the cart
before our horse,

00:37:38.767 --> 00:37:40.701
but this is the magic
in multi-master.

00:37:40.701 --> 00:37:43.467
Merging writes and reconciling
and there are

00:37:43.467 --> 00:37:45.067
a number of ways to do it.

00:37:45.067 --> 00:37:48.033
Like I said,
since you're serving writes

00:37:48.033 --> 00:37:51.901
kind of independently,
there's no way to do

00:37:51.901 --> 00:37:54.167
a global transaction.

00:37:54.167 --> 00:37:55.801
I mean,
there's no easy way.

00:37:55.801 --> 00:37:59.934
You can't offer
a read and a write

00:37:59.934 --> 00:38:02.267
in one datacenter
and guarantee that nothing

00:38:02.267 --> 00:38:03.868
has happened to it in other
datacenters

00:38:03.868 --> 00:38:05.267
that would affect you.

00:38:05.267 --> 00:38:06.767
For that, you need
something stronger

00:38:06.767 --> 00:38:08.000
that we'll get to
in a bit.

00:38:08.000 --> 00:38:10.667
But you have to kind
of accept that this is

00:38:10.667 --> 00:38:12.234
kind of like multithreading
where you

00:38:12.234 --> 00:38:13.767
don't even have locks.

00:38:13.767 --> 00:38:16.200
Your locks are all in
the background after the fact.

00:38:16.200 --> 00:38:17.534
You do what you're gonna do.

00:38:17.534 --> 00:38:19.267
Someone else does
what they're gonna do.

00:38:19.267 --> 00:38:22.000
You figure it out afterward.

00:38:22.000 --> 00:38:24.200
The result of that is
one of things

00:38:24.200 --> 00:38:26.000
we care strongly in
the App Engine datastore

00:38:26.000 --> 00:38:28.100
is strong consistency.

00:38:28.100 --> 00:38:31.267
We feel like this is a key
differentiating factor

00:38:31.267 --> 00:38:34.767
from systems like SimpleDB,
which there are

00:38:34.767 --> 00:38:36.367
a lot of nice properties

00:38:36.367 --> 00:38:39.467
when you go eventually
consistent.

00:38:39.467 --> 00:38:41.801
But using it to build
an application,

00:38:41.801 --> 00:38:44.033
using it as infrastructure,
it's just a lot harder.

00:38:44.033 --> 00:38:45.868
That mental model of
I do something

00:38:45.868 --> 00:38:47.767
and I don't know
when it's reflected,

00:38:47.767 --> 00:38:49.267
when it's visible.

00:38:49.267 --> 00:38:51.434
It's a lot harder to use.

00:38:51.434 --> 00:38:54.901
So we didn't really
consider this much

00:38:54.901 --> 00:38:57.267
for the App Engine datastore
because it would have

00:38:57.267 --> 00:38:59.267
been harder to provide
strong consistency.

00:38:59.267 --> 00:39:00.467
We started with that.

00:39:00.467 --> 00:39:01.901
The last thing we want
to do is take it away

00:39:01.901 --> 00:39:04.367
from you, you know?

00:39:04.367 --> 00:39:06.334
So what do the trade-offs
look like

00:39:06.334 --> 00:39:08.734
for multi-master replication?

00:39:08.734 --> 00:39:12.100
We lost some
transaction support.

00:39:12.100 --> 00:39:17.701
But in exchange,
we don't have to go read-only

00:39:17.701 --> 00:39:19.000
for failover.

00:39:19.000 --> 00:39:20.901
Every datacenter is doing
writes on its own,

00:39:20.901 --> 00:39:22.067
serving both reads and writes.

00:39:22.067 --> 00:39:23.567
And so if you have
to move one

00:39:23.567 --> 00:39:25.434
or take one down,
the others can still

00:39:25.434 --> 00:39:26.901
handle the writes.

00:39:26.901 --> 00:39:31.067
So it's a trade-off.

00:39:31.067 --> 00:39:32.300
two-phase commit.

00:39:32.300 --> 00:39:34.167
This what I started
to talk about with NASDAQ

00:39:34.167 --> 00:39:35.501
and many of you might be
familiar with this

00:39:35.501 --> 00:39:37.100
if you have
database backgrounds.

00:39:37.100 --> 00:39:40.367
So this is a protocol
for doing transactions

00:39:40.367 --> 00:39:43.868
across separate systems
or at least

00:39:43.868 --> 00:39:46.300
separate shards
of the database--

00:39:46.300 --> 00:39:51.400
that kind of thing--
where's there's no single

00:39:51.400 --> 00:39:54.601
master that everything
goes through.

00:39:54.601 --> 00:39:56.801
I call this semi-distributed

00:39:56.801 --> 00:40:00.334
because there is always
a master or coordinator

00:40:00.334 --> 00:40:03.300
for a given two phase
commit transaction.

00:40:03.300 --> 00:40:06.067
You have to pick one of them.

00:40:06.067 --> 00:40:07.801
Usually you impose
a total ordering

00:40:07.801 --> 00:40:09.467
at the beginning
of all your nodes

00:40:09.467 --> 00:40:12.033
and so that you know
for any set of nodes

00:40:12.033 --> 00:40:13.167
who is the master.

00:40:13.167 --> 00:40:15.167
Oh, it's the one
with the lowest I.D.

00:40:15.167 --> 00:40:16.701
Or, you know--

00:40:16.701 --> 00:40:18.067
or you just ask someone.

00:40:18.067 --> 00:40:22.934
hey say, "Oh, this is
the master out of this set."

00:40:22.934 --> 00:40:28.601
That works when you have very
broadly sharded data

00:40:28.601 --> 00:40:30.534
or highly sharded data.

00:40:30.534 --> 00:40:33.167
So in App Engine we have
this concept of entity groups

00:40:33.167 --> 00:40:35.033
where every entity is in
an entity group

00:40:35.033 --> 00:40:37.000
and we say each entity group
should be roughly

00:40:37.000 --> 00:40:40.400
the size of a user's data,
more or less.

00:40:40.400 --> 00:40:42.501
You know, one of our large
applications,

00:40:42.501 --> 00:40:44.467
BuddyPoke,
has 40 million users.

00:40:44.467 --> 00:40:46.467
Or last they reported,
it had 40 million users.

00:40:46.467 --> 00:40:49.467
And so that's 40 million
different shards

00:40:49.467 --> 00:40:51.868
and different possible nodes
to participate

00:40:51.868 --> 00:40:53.901
in two-phase commit.

00:40:53.901 --> 00:40:57.467
If you did that
in that setting, saying,

00:40:57.467 --> 00:41:01.200
"Okay, there's one master per
two-phase commit transaction,"

00:41:01.200 --> 00:41:02.501
that's not such a big deal,

00:41:02.501 --> 00:41:03.934
'cause you got 40 million
to choose from

00:41:03.934 --> 00:41:06.534
and any transaction's only gonna
use a few of them.

00:41:06.534 --> 00:41:08.834
With datacenters, you don't
have 40 million datacenters.

00:41:08.834 --> 00:41:11.033
You usually have, like,
three.

00:41:11.033 --> 00:41:13.801
Maybe five, maybe ten if
you're really big,

00:41:13.801 --> 00:41:17.767
but it's--what?

00:41:17.767 --> 00:41:18.834
Do some math, you know?

00:41:18.834 --> 00:41:21.901
10, 12 orders
of magnitude difference.

00:41:21.901 --> 00:41:23.234
What you're gonna find
is that

00:41:23.234 --> 00:41:27.467
if you two-phase commit often,
one or a small subset

00:41:27.467 --> 00:41:29.400
of your datacenters tends
to be the master

00:41:29.400 --> 00:41:33.067
depending on how many
participate in each transaction.

00:41:33.067 --> 00:41:35.167
So it's only semi-distributed
in that sense.

00:41:35.167 --> 00:41:40.100
You're often going through
the same masters.

00:41:40.100 --> 00:41:42.033
What else do I want
to say about two-phase commit?

00:41:42.033 --> 00:41:44.734
Because of that it's so--
oh, yeah.

00:41:44.734 --> 00:41:46.300
So the next interesting thing
about two-phase commit

00:41:46.300 --> 00:41:47.701
is it's synchronous.

00:41:47.701 --> 00:41:50.934
For a given master, all two
phase commit transactions

00:41:50.934 --> 00:41:53.200
are serialized through
that master,

00:41:53.200 --> 00:41:57.834
which kind of kills
your throughput.

00:41:57.834 --> 00:41:59.868
Also hurts your latency.

00:41:59.868 --> 00:42:01.634
There's a variant called
three phase commit,

00:42:01.634 --> 00:42:03.767
which lets you do a sync,
but it doesn't

00:42:03.767 --> 00:42:07.334
get rid of that
per-transaction master.

00:42:07.334 --> 00:42:11.701
Yeah, so we care about data--
write throughput

00:42:11.701 --> 00:42:15.501
in the datastore and so
like multi-master,

00:42:15.501 --> 00:42:18.367
we never seriously
considered this.

00:42:18.367 --> 00:42:21.801
Yeah, we want--
scaling is important to us.

00:42:21.801 --> 00:42:25.267
And any single or semi-single
point of failure

00:42:25.267 --> 00:42:29.200
or serialization point is just
not gonna fly with us, so.

00:42:29.200 --> 00:42:32.834
But on the plus side,
it works.

00:42:32.834 --> 00:42:34.868
This is the first kind
of truly--

00:42:34.868 --> 00:42:37.801
true multihoming technique
that we have seen

00:42:37.801 --> 00:42:40.501
where you can transactionally
atomically

00:42:40.501 --> 00:42:43.400
write something to either two
datacenters or none

00:42:43.400 --> 00:42:45.567
and that's really powerful.

00:42:45.567 --> 00:42:47.901
NASDAQ clearly thought so.

00:42:47.901 --> 00:42:51.467
So again, this is kind of--

00:42:51.467 --> 00:42:53.367
this is the biggest paradigm
shift we're gonna see.

00:42:53.367 --> 00:42:56.434
Up until here we have seen
kind of offline background

00:42:56.434 --> 00:42:59.634
asynchronous replication
and reconciling.

00:42:59.634 --> 00:43:01.300
This is the first technique
we've seen

00:43:01.300 --> 00:43:03.567
that does synchronous,
like, online.

00:43:03.567 --> 00:43:05.601
It'll report back
to the user on a put

00:43:05.601 --> 00:43:08.367
or insert or update "yes"
in both datacenters

00:43:08.367 --> 00:43:11.100
or "No, it hit
neither of them."

00:43:11.100 --> 00:43:15.501
As you can see that in the
colors it's the opposite, right?

00:43:15.501 --> 00:43:17.801
Out of the box,
you get strong consistency.

00:43:17.801 --> 00:43:20.234
You get full
transaction support.

00:43:20.234 --> 00:43:24.167
If any one datacenter falls
in the ocean, doesn't matter.

00:43:24.167 --> 00:43:25.434
You know everything
that mattered.

00:43:25.434 --> 00:43:26.701
Everything that you reported
success on

00:43:26.701 --> 00:43:28.234
is in the other one.

00:43:28.234 --> 00:43:31.200
Same with failover.

00:43:31.200 --> 00:43:32.634
Generally you need
what we call

00:43:32.634 --> 00:43:33.834
"at least 'n' + one."

00:43:33.834 --> 00:43:36.000
So if you're serving
out of "n" datacenters,

00:43:36.000 --> 00:43:39.300
you need at least one more...

00:43:39.300 --> 00:43:42.634
so that whenever you need
to take one down temporarily

00:43:42.634 --> 00:43:44.467
to do maintenance
or to move things,

00:43:44.467 --> 00:43:47.968
you've still got that "n" for
the duration.

00:43:47.968 --> 00:43:52.634
You pay for it with latency
and throughput, as I mentioned.

00:43:52.634 --> 00:43:54.033
The last thing
we're gonna look at

00:43:54.033 --> 00:43:55.567
is what's called Paxos.

00:43:55.567 --> 00:43:58.501
And I'm sure people in
the audience can correct me.

00:43:58.501 --> 00:43:59.601
I believe it was--

00:43:59.601 --> 00:44:02.000
was it Leslie Lamport
or Bruce--

00:44:02.000 --> 00:44:04.067
I think it was Leslie Lamport.

00:44:04.067 --> 00:44:06.133
If anyone knows different,
yell out.

00:44:06.133 --> 00:44:09.601
But he wrote this
great paper about--

00:44:09.601 --> 00:44:11.834
I believe it was philosophers
on a Greek island,

00:44:11.834 --> 00:44:13.167
the island of Paxos.

00:44:13.167 --> 00:44:15.234
And it was totally esoteric
and obscure

00:44:15.234 --> 00:44:17.534
and you read the paper
and it was a fun story

00:44:17.534 --> 00:44:19.467
and at the end you had no idea
what he was proposing,

00:44:19.467 --> 00:44:23.267
but you knew he was on
to something big.

00:44:23.267 --> 00:44:24.601
I've read the paper.

00:44:24.601 --> 00:44:26.300
I don't pretend
to understand it.

00:44:26.300 --> 00:44:28.868
I learned Paxos from other
people at Google

00:44:28.868 --> 00:44:30.601
who are much smarter than me.

00:44:30.601 --> 00:44:34.801
And even then it's non-trivial,
but it works.

00:44:34.801 --> 00:44:38.234
So like two-phase commit,
Paxos is a, what we call,

00:44:38.234 --> 00:44:39.434
consensus protocol.

00:44:39.434 --> 00:44:42.434
You have a group
of independent nodes

00:44:42.434 --> 00:44:44.734
and you want
to reach consensus

00:44:44.734 --> 00:44:46.100
on at least a majority
of them

00:44:46.100 --> 00:44:48.133
if not all of them
on something.

00:44:48.133 --> 00:44:51.434
Unlike two-phase commit,
Paxos is fully distributed.

00:44:51.434 --> 00:44:55.734
There is no single master
for any given transaction.

00:44:55.734 --> 00:44:59.267
Again, as I put the slide up,
everyone reads first

00:44:59.267 --> 00:45:00.667
and then turns back
and listens

00:45:00.667 --> 00:45:02.200
and so you've
already read the joke.

00:45:02.200 --> 00:45:04.767
But the joke here is that
a really smart guy at Google

00:45:04.767 --> 00:45:06.567
named Mike Burrows

00:45:06.567 --> 00:45:08.434
says every consensus
protocol out there

00:45:08.434 --> 00:45:10.300
or every fully distributed
consensus protocol

00:45:10.300 --> 00:45:14.100
is either Paxos or Paxos
with cruft or broken.

00:45:14.100 --> 00:45:16.601
And I don't think he's been
proven wrong yet.

00:45:16.601 --> 00:45:18.267
And so, you know,

00:45:18.267 --> 00:45:21.000
I'm not gonna be the one
to bet against him.

00:45:21.000 --> 00:45:26.501
Like I said, the actual
protocol--the Paxos protocol--

00:45:26.501 --> 00:45:28.200
is similar
to two-phase commit,

00:45:28.200 --> 00:45:30.667
which I didn't even really
talk through.

00:45:30.667 --> 00:45:33.033
There is a propose--

00:45:33.033 --> 00:45:36.501
there is a propose step
and then, basically,

00:45:36.501 --> 00:45:39.968
a grieve step
or a notify step.

00:45:39.968 --> 00:45:42.767
Similar to two-phase commit
with the difference

00:45:42.767 --> 00:45:44.300
that you only need a majority.

00:45:44.300 --> 00:45:46.367
You don't need the master
running things.

00:45:46.367 --> 00:45:48.501
So as long as a majority
of your nodes

00:45:48.501 --> 00:45:50.000
that are participating
in Paxos

00:45:50.000 --> 00:45:52.634
or a majority of your
datacenters come back and say,

00:45:52.634 --> 00:45:54.734
"Yes, I agree.
We are here now.

00:45:54.734 --> 00:45:56.067
We're at this position
in the log."

00:45:56.067 --> 00:45:57.968
Or, "Yes, I agree.
This has been persistent,"

00:45:57.968 --> 00:46:00.400
then you're good.

00:46:00.400 --> 00:46:03.334
Like I said, underneath,
similar to phase commit.

00:46:03.334 --> 00:46:07.300
The nice thing is there is
less of a--

00:46:07.300 --> 00:46:10.767
less serialization.

00:46:10.767 --> 00:46:12.267
It's distributed,
but it can also--

00:46:12.267 --> 00:46:14.300
you can run multiple
Paxos around

00:46:14.300 --> 00:46:17.567
multiple Paxos transactions
in parallel.

00:46:17.567 --> 00:46:21.934
It's not serialized on whatever,
you know--per datacenter.

00:46:21.934 --> 00:46:23.701
You're not gonna have this
serial pipe

00:46:23.701 --> 00:46:26.400
that every two phase transaction
is going to go through

00:46:26.400 --> 00:46:30.367
one at a time, which gives
you better throughput.

00:46:30.367 --> 00:46:32.367
The problem here--

00:46:32.367 --> 00:46:34.534
and I didn't quite get into
enough with two-phase commit,

00:46:34.534 --> 00:46:37.567
so if we look at it we saw this
huge drop in latency

00:46:37.567 --> 00:46:39.434
in two-phase commit.

00:46:39.434 --> 00:46:43.300
The reason there is, again,
you're doing everything online.

00:46:43.300 --> 00:46:45.200
You're waiting
for those roundtrips.

00:46:45.200 --> 00:46:48.000
It's actually two roundtrips
between datacenters

00:46:48.000 --> 00:46:49.534
before you can go back
to the user and say,

00:46:49.534 --> 00:46:50.901
"Yes, your write went in."

00:46:50.901 --> 00:46:53.067
This is where you see
writes, you know--

00:46:53.067 --> 00:46:55.868
100, 150 depending on how far
your datacenters are away

00:46:55.868 --> 00:46:57.501
and how many are participating.

00:46:57.501 --> 00:47:02.734
200, 250 milliseconds,
which is a long time.

00:47:02.734 --> 00:47:05.167
And so you see that
with Paxos too.

00:47:05.167 --> 00:47:07.601
Still highly--
you can't avoid that.

00:47:07.601 --> 00:47:11.567
If you want true multihoming,
you're gonna pay for it.

00:47:11.567 --> 00:47:15.501
We really wanted to do this
for the App Engine datastore.

00:47:15.501 --> 00:47:18.667
It's just the right thing
for so many reasons

00:47:18.667 --> 00:47:21.734
and we use it in so many other
parts of Google.

00:47:21.734 --> 00:47:24.234
We really wanted to.

00:47:24.234 --> 00:47:25.634
We couldn't quite get
it to work

00:47:25.634 --> 00:47:28.334
with the latency guarantees
that we needed to pass on.

00:47:28.334 --> 00:47:29.701
Or not guarantees.

00:47:29.701 --> 00:47:32.667
But we couldn't in all
honesty tell you all,

00:47:32.667 --> 00:47:35.868
"Writes right now are
30, 40, 50 milliseconds.

00:47:35.868 --> 00:47:37.501
"They're gonna be 150.

00:47:37.501 --> 00:47:40.467
Just trust us that it's better.
Just trust us."

00:47:40.467 --> 00:47:42.400
It wouldn't quite fly.

00:47:42.400 --> 00:47:46.567
Now we can describe exactly why
for reasons like this.

00:47:46.567 --> 00:47:48.334
But confiding against,
you know, 

00:47:48.334 --> 00:47:50.834
five millisecond updates
and inserts in databases?

00:47:50.834 --> 00:47:52.934
That's a tough battle to win.

00:47:52.934 --> 00:47:54.334
We tried some variations.

00:47:54.334 --> 00:47:55.934
We tried physically
close together

00:47:55.934 --> 00:47:58.734
just like NASDAQ
with those 27 miles.

00:47:58.734 --> 00:48:00.501
It helped a little,
but you can't avoid

00:48:00.501 --> 00:48:03.133
the fact that
you're doing two roundtrips

00:48:03.133 --> 00:48:07.767
across lots of routers
and queues in between--

00:48:07.767 --> 00:48:10.767
routing and queuing delays
in between.

00:48:10.767 --> 00:48:13.267
We even tried putting two nodes
in the same datacenter,

00:48:13.267 --> 00:48:16.367
which would give you a weaker,
you know--

00:48:16.367 --> 00:48:18.100
a weaker set of guarantees,

00:48:18.100 --> 00:48:19.801
but at least those roundtrips
are within a datacenter.

00:48:19.801 --> 00:48:23.133
And even then,
it's not great.

00:48:23.133 --> 00:48:26.934
You still have to hit at least
some of the other nodes.

00:48:26.934 --> 00:48:29.234
What we can do is let
you opt in

00:48:29.234 --> 00:48:30.901
to something like Paxos
and I'll talk more

00:48:30.901 --> 00:48:33.567
about this later.

00:48:33.567 --> 00:48:36.901
Having said that, we use Paxos
a ton in App Engine.

00:48:36.901 --> 00:48:41.601
It's used in a lock server
that Google uses very heavily.

00:48:41.601 --> 00:48:45.334
And we use that in App Engine
usually via the lock server,

00:48:45.334 --> 00:48:47.601
but some on our own
for coordinating

00:48:47.601 --> 00:48:50.534
anything that we do
across datacenters.

00:48:50.534 --> 00:48:53.167
Particularly anything where
we move state.

00:48:53.167 --> 00:48:55.634
If your app is serving
in one place

00:48:55.634 --> 00:48:58.734
and we need to move it
to serve in another place,

00:48:58.734 --> 00:49:01.300
we coordinate that with Paxos.

00:49:01.300 --> 00:49:02.567
Memcache.

00:49:02.567 --> 00:49:05.634
Memcache has interesting
multihoming of its own.

00:49:05.634 --> 00:49:08.601
In coordinating and managing
that, we use Paxos.

00:49:08.601 --> 00:49:10.334
Offline processing
is the same thing.

00:49:10.334 --> 00:49:12.234
You'll hear more about
offline processing

00:49:12.234 --> 00:49:13.701
tomorrow at 10:45.

00:49:13.701 --> 00:49:15.567
I encourage you,
if there's one App Engine talk

00:49:15.567 --> 00:49:17.834
you go to,
don't make it this one,

00:49:17.834 --> 00:49:20.400
but too late.

00:49:20.400 --> 00:49:22.167
The one this morning
from Alon Levi

00:49:22.167 --> 00:49:24.267
which describes
how App Engine works was great,

00:49:24.267 --> 00:49:27.067
but the offline processing one
tomorrow at 10:45,

00:49:27.067 --> 00:49:28.667
we're excited about that.

00:49:28.667 --> 00:49:31.934
We're looking for you all to try
it out, so check that out.

00:49:31.934 --> 00:49:34.467
That uses Paxos
under the hood too.

00:49:34.467 --> 00:49:37.567
So this is the final diagram.

00:49:37.567 --> 00:49:39.934
The kind of final evaluation.

00:49:39.934 --> 00:49:41.467
You'll see that
the only change like

00:49:41.467 --> 00:49:44.000
I talked about between Paxos
and two-phase commit

00:49:44.000 --> 00:49:46.367
is that you get
better throughput.

00:49:46.367 --> 00:49:49.067
The really sad thing here,
the part that makes me,

00:49:49.067 --> 00:49:50.567
you know, cry into my pillow
at night--

00:49:50.567 --> 00:49:55.033
and I'm only half-joking--
there's no fully green bar.

00:49:55.033 --> 00:49:59.467
You can't find anything on that
diagram that makes you happy.

00:49:59.467 --> 00:50:03.400
That gets everything even--
That gets everything right.

00:50:03.400 --> 00:50:04.667
It's disappointing.

00:50:04.667 --> 00:50:06.567
It makes me sad.

00:50:06.567 --> 00:50:10.400
And so we picked one that
had green and yellow.

00:50:10.400 --> 00:50:12.267
Life is trade-offs.

00:50:12.267 --> 00:50:16.100
It's no fun,
but it's a fact of life.

00:50:16.100 --> 00:50:19.133
Conclusion, again,
no silver bullet.

00:50:19.133 --> 00:50:20.667
There is no--

00:50:20.667 --> 00:50:23.667
There is no vertical
green slice.

00:50:23.667 --> 00:50:25.300
You still got
to do it though.

00:50:25.300 --> 00:50:27.634
That two-hour outage
in SVColo?

00:50:27.634 --> 00:50:29.067
That's just unacceptable.

00:50:29.067 --> 00:50:30.801
Much less if there was
something destructive

00:50:30.801 --> 00:50:32.634
and you actually lost data.

00:50:32.634 --> 00:50:34.701
It's fundamentally unacceptable.

00:50:34.701 --> 00:50:37.534
It's just not okay.

00:50:37.534 --> 00:50:39.701
The other trade-off that
I'm looking forward

00:50:39.701 --> 00:50:43.300
to talking more about
eventually is, again,

00:50:43.300 --> 00:50:45.834
that this is inherently
a trade-off.

00:50:45.834 --> 00:50:49.033
Some apps need one thing.
Some apps need another.

00:50:49.033 --> 00:50:51.767
We're a platform and we're
a pretty high-level platform,

00:50:51.767 --> 00:50:54.267
but honestly, we don't know
what's right for you.

00:50:54.267 --> 00:50:55.601
You know what's right for you.

00:50:55.601 --> 00:50:57.601
If you're a bank,
you want Paxos.

00:50:57.601 --> 00:50:59.000
You want two-phase commit
across maybe

00:50:59.000 --> 00:51:01.400
two or more datacenters.

00:51:01.400 --> 00:51:03.367
If you are some, you know--

00:51:03.367 --> 00:51:05.434
if you're a game,
you don't really care.

00:51:05.434 --> 00:51:08.033
You just want fast responses.

00:51:08.033 --> 00:51:09.934
And so what we'd like
to do in the future

00:51:09.934 --> 00:51:13.501
is offer--
give you the option.

00:51:13.501 --> 00:51:15.567
So, okay, if you want what
we call strong writes,

00:51:15.567 --> 00:51:18.434
if you want two-phase commit
or Paxos across datacenters,

00:51:18.434 --> 00:51:19.701
you can do it.

00:51:19.701 --> 00:51:21.334
Just tell us on
a per-write basis

00:51:21.334 --> 00:51:23.834
or maybe on
per-entity group basis.

00:51:23.834 --> 00:51:25.968
On the other hand,
if you want fast writes

00:51:25.968 --> 00:51:27.434
and highly available writes,

00:51:27.434 --> 00:51:29.934
if you want to read from one
of those replica datacenters

00:51:29.934 --> 00:51:32.234
and you are okay with
giving up the guarantee

00:51:32.234 --> 00:51:34.300
that it's always
strongly consistent,

00:51:34.300 --> 00:51:36.868
maybe 1 in 100 times
you'll get stale data

00:51:36.868 --> 00:51:39.434
because the update
that you just did

00:51:39.434 --> 00:51:41.734
hadn't made it to
that datacenter yet.

00:51:41.734 --> 00:51:45.334
Tell us on a per-write basis,
you know, and we can do that.

00:51:45.334 --> 00:51:48.367
Or hopefully in the future.

00:51:48.367 --> 00:51:50.234
We talk about this a lot
internally at Google

00:51:50.234 --> 00:51:53.334
where there's
no one size fits all.

00:51:53.334 --> 00:51:57.400
And so we want
to embrace that.

00:51:57.400 --> 00:51:58.934
We, you know,
do that internally.

00:51:58.934 --> 00:52:00.934
And, you know, App Engine
is infrastructure

00:52:00.934 --> 00:52:02.033
like anything else.

00:52:02.033 --> 00:52:04.734
We want to let you all choose.

00:52:04.734 --> 00:52:05.901
That's it.

00:52:05.901 --> 00:52:07.968
I've taken much more time
than I expected.

00:52:07.968 --> 00:52:10.434
So feel free to come up
to the mics and ask questions

00:52:10.434 --> 00:52:13.467
and I'm gonna go to Moderator
and answer

00:52:13.467 --> 00:52:15.200
a few questions there too.

00:52:15.200 --> 00:52:16.400
Let's see if there's
anything there.

00:52:16.400 --> 00:52:19.334
Ah, perfect.

00:52:24.133 --> 00:52:25.934
So I'll trade off.

00:52:25.934 --> 00:52:27.901
"What is Google going to do
about providing storage

00:52:27.901 --> 00:52:29.567
"as discussed a bit
in the keynote this morning?

00:52:29.567 --> 00:52:33.467
Is Google going to attempt
to compete with S3, et cetera?"

00:52:33.467 --> 00:52:34.868
Good question.

00:52:34.868 --> 00:52:39.400
So we do plan to offer more
options for storing

00:52:39.400 --> 00:52:41.734
blob data in App Engine.

00:52:41.734 --> 00:52:43.667
Particularly blob data larger
than one megabyte,

00:52:43.667 --> 00:52:46.000
because that's the limit
in the datastore right now.

00:52:46.000 --> 00:52:47.734
I don't have any particular
announcements,

00:52:47.734 --> 00:52:50.601
but we are working on that.

00:52:50.601 --> 00:52:52.968
And we're excited about that.

00:52:52.968 --> 00:52:55.567
I tend to kind of live
in the App Engine bubble

00:52:55.567 --> 00:52:57.968
and so, I mean, I pay attention
to the other stuff,

00:52:57.968 --> 00:53:02.234
but I'm only authoritative
on App Engine.

00:53:02.234 --> 00:53:04.033
man: Could you talk a little bit
about entity groups

00:53:04.033 --> 00:53:05.667
megastore and how
that would apply

00:53:05.667 --> 00:53:07.934
in your cross-datacenter
replication models?

00:53:07.934 --> 00:53:09.300
Barrett: Sure.

00:53:09.300 --> 00:53:12.467
So entity groups
are how we shard

00:53:12.467 --> 00:53:15.234
or partition data
in App Engine.

00:53:15.234 --> 00:53:19.033
All writes and transactions
go through journals

00:53:19.033 --> 00:53:21.667
or logs at
the entity group level.

00:53:21.667 --> 00:53:23.901
And so within an entity group,

00:53:23.901 --> 00:53:25.934
all writes and transactions
are serialized.

00:53:25.934 --> 00:53:27.767
That is our unit
of consistency.

00:53:27.767 --> 00:53:30.467
And so under the hood,
a lot of the replication

00:53:30.467 --> 00:53:35.868
and data management we do is via
those logs per entity group.

00:53:35.868 --> 00:53:39.868
So if we were to use
multi-master replication

00:53:39.868 --> 00:53:42.267
or right now we can do
master-slave replication--

00:53:42.267 --> 00:53:44.100
When we replicate,
it's at the level

00:53:44.100 --> 00:53:46.801
of basically a log commit.

00:53:46.801 --> 00:53:50.000
So whether that was
a single entity write

00:53:50.000 --> 00:53:52.267
or a batch write that wrote
multiple entities

00:53:52.267 --> 00:53:55.000
to an entity group
or a transaction,

00:53:55.000 --> 00:53:57.167
that log contains
basically the DIF.

00:53:57.167 --> 00:53:58.467
all of the changes.

00:53:58.467 --> 00:54:00.100
And so what we're
gonna be replicating is,

00:54:00.100 --> 00:54:05.400
as a unit, that log for each
commit to an entity group.

00:54:05.400 --> 00:54:08.000
And so that's how we maintain
both strong consis--

00:54:08.000 --> 00:54:10.801
or we maintain consistency
across replication

00:54:10.801 --> 00:54:14.300
and also how we maintain
transactions so that

00:54:14.300 --> 00:54:16.467
at any time we can
go look at the state

00:54:16.467 --> 00:54:19.400
of one of those
replica datacenters

00:54:19.400 --> 00:54:25.167
and it would be consistent
with regards to its--

00:54:25.167 --> 00:54:29.234
well, consistent
with regard to transactions.

00:54:29.234 --> 00:54:32.033
"If I put a lot of data
in the datastore,

00:54:32.033 --> 00:54:36.067
what options do I have
for backing it up?"

00:54:36.067 --> 00:54:39.300
We would--
it's a good question.

00:54:39.300 --> 00:54:41.901
So...

00:54:41.901 --> 00:54:45.667
If you want to ensure against
failures of datacenters,

00:54:45.667 --> 00:54:47.133
you know, catastrophic
failures like the ones

00:54:47.133 --> 00:54:48.434
we were talking about,
we would hope

00:54:48.434 --> 00:54:49.634
that we would
do the right thing

00:54:49.634 --> 00:54:51.367
such that you don't need to.

00:54:51.367 --> 00:54:53.334
In reality, like I said,
much error--

00:54:53.334 --> 00:54:55.334
You know, much of error
is human error.

00:54:55.334 --> 00:54:57.834
And so, you know,
sometimes you might upload

00:54:57.834 --> 00:54:59.334
something that
has a bug in your code

00:54:59.334 --> 00:55:01.267
or anything else.

00:55:01.267 --> 00:55:05.000
And so you definitely
want a backup for that.

00:55:05.000 --> 00:55:06.367
This is all background.

00:55:06.367 --> 00:55:08.834
The answer is we have tools
for doing

00:55:08.834 --> 00:55:13.167
high throughput,
bulk upload, and download.

00:55:13.167 --> 00:55:14.667
And those are shipped
with the STK

00:55:14.667 --> 00:55:16.501
and the STK is open source,
so I would--

00:55:16.501 --> 00:55:18.367
Right now I would
say bulk upload

00:55:18.367 --> 00:55:20.968
and download
are the way to go.

00:55:20.968 --> 00:55:22.834
man: Hi.
So a couple questions.

00:55:22.834 --> 00:55:26.334
The first one
is around use cases.

00:55:26.334 --> 00:55:28.968
Do you imagine a use case where,
you know,

00:55:28.968 --> 00:55:32.000
I'm an enterprise and my data
is critical to me

00:55:32.000 --> 00:55:34.501
and that my application
is running on App Engine

00:55:34.501 --> 00:55:39.467
and I sync up with you
and you cache the data

00:55:39.467 --> 00:55:42.667
and the application
can actually run and scale,

00:55:42.667 --> 00:55:45.267
but my data is still
kept locally.

00:55:45.267 --> 00:55:47.467
What kind of use cases?

00:55:47.467 --> 00:55:49.167
And the second question is--

00:55:49.167 --> 00:55:51.133
Barrett: Sorry, do you mind if
I take 'em one at a time?

00:55:51.133 --> 00:55:53.067
So you're saying
you would write an app,

00:55:53.067 --> 00:55:54.934
like, an App Engine app,
that accesses

00:55:54.934 --> 00:55:57.534
your on-premises data
or you're saying

00:55:57.534 --> 00:55:59.234
you would synchronize
your on-premises data--

00:55:59.234 --> 00:56:00.801
man: So I'm worried
about having my data

00:56:00.801 --> 00:56:02.067
at Google completely,
all right?

00:56:02.067 --> 00:56:03.434
Barrett: Okay.

00:56:03.434 --> 00:56:04.801
man: I want the data
in my enterprise

00:56:04.801 --> 00:56:06.300
but running my app on yours.

00:56:06.300 --> 00:56:08.234
But are you thinking
of other use cases,

00:56:08.234 --> 00:56:10.334
because you didn't start
with that, you know?

00:56:10.334 --> 00:56:12.000
Sort of thinking multihoming--

00:56:12.000 --> 00:56:14.067
Since you're shielding
a lot with the platform,

00:56:14.067 --> 00:56:17.467
I don't have control over
where the data goes.

00:56:17.467 --> 00:56:18.534
Barrett: Absolutely.

00:56:18.534 --> 00:56:20.300
So there are a couple
questions here.

00:56:20.300 --> 00:56:21.934
man: That was my second
question.

00:56:21.934 --> 00:56:23.734
Barrett: Okay, sure.
Got it.

00:56:23.734 --> 00:56:25.567
One is if you
have data right now

00:56:25.567 --> 00:56:27.601
and you want to use App Engine
but you don't

00:56:27.601 --> 00:56:29.033
want to give up all your control
over that data,

00:56:29.033 --> 00:56:31.033
you want to keep
it on-premises

00:56:31.033 --> 00:56:33.501
or at least keep a copy
of it on-premise.

00:56:33.501 --> 00:56:35.434
You can use these bulk upload
and download tools

00:56:35.434 --> 00:56:37.334
to basically synchronize
your data,

00:56:37.334 --> 00:56:39.300
and there are actually people
who have written 

00:56:39.300 --> 00:56:42.868
better tools to let you
basically upload

00:56:42.868 --> 00:56:44.434
and download DIFs.

00:56:44.434 --> 00:56:46.434
So just the delta
of what happened

00:56:46.434 --> 00:56:48.901
since you last synchronized.

00:56:48.901 --> 00:56:51.167
You could even write something
that synchronizes online

00:56:51.167 --> 00:56:55.234
using HTTP APIs that you access
via URLFetch.

00:56:55.234 --> 00:56:57.567
Something like that.

00:56:57.567 --> 00:56:58.801
I wouldn't recommend it.

00:56:58.801 --> 00:57:01.267
I've worked on a number
of systems where you try

00:57:01.267 --> 00:57:02.968
to synchronize data
in multiple places

00:57:02.968 --> 00:57:05.400
and it's just inevitably
difficult and error prone

00:57:05.400 --> 00:57:10.834
and it bites you in the ass
every time and no fun.

00:57:10.834 --> 00:57:12.667
You can do it.

00:57:12.667 --> 00:57:15.167
Honestly, I would
recommend against it.

00:57:15.167 --> 00:57:17.200
The second question
is more interesting.

00:57:17.200 --> 00:57:20.534
Can you choose physically where
your app serves

00:57:20.534 --> 00:57:21.734
or where your data lives?

00:57:21.734 --> 00:57:23.200
Which is a great question.

00:57:23.200 --> 00:57:26.434
This came up actually this
morning at Alon Levi's talk.

00:57:26.434 --> 00:57:29.667
So there are regulations
like the E.U.'s safe harbor

00:57:29.667 --> 00:57:33.167
data regulations,
which require that data

00:57:33.167 --> 00:57:35.367
is stored inside the E.U.

00:57:35.367 --> 00:57:37.567
Which is honestly one of--
I believe one of

00:57:37.567 --> 00:57:40.000
the main reasons that Amazon
started offering a choice.

00:57:40.000 --> 00:57:41.467
You know, you can have
your S3 data.

00:57:41.467 --> 00:57:44.100
Live in the U.S.
or live in E.U.

00:57:44.100 --> 00:57:45.534
We want to do that too.

00:57:45.534 --> 00:57:48.200
I don't have any specific
announcements

00:57:48.200 --> 00:57:50.934
or a timeline,
but we hear you.

00:57:50.934 --> 00:57:53.300
We totally understand this is
just a fundamental requirement

00:57:53.300 --> 00:57:54.601
and we need to do it.

00:57:54.601 --> 00:57:56.634
man: Wouldn't that imply
that now I have

00:57:56.634 --> 00:57:59.801
fine-grained control over where
my data goes?

00:57:59.801 --> 00:58:04.000
So in other words, it breaks
your shielding from the platform

00:58:04.000 --> 00:58:05.467
and scaling for me.

00:58:05.467 --> 00:58:08.300
Because now I have to tell you
I have to make some decisions.

00:58:08.300 --> 00:58:10.801
Barrett: That would give you
some control,

00:58:10.801 --> 00:58:12.701
but the techniques
we're talking about here

00:58:12.701 --> 00:58:15.033
don't care about physically
where your data is,

00:58:15.033 --> 00:58:17.534
so we would--
all of the things that we--

00:58:17.534 --> 00:58:18.701
the invariance we want
to maintain

00:58:18.701 --> 00:58:19.701
to make sure your data
is safe,

00:58:19.701 --> 00:58:21.133
like a number of replicas,

00:58:21.133 --> 00:58:22.767
a number of physical
locations.

00:58:22.767 --> 00:58:24.167
We would expect
to maintain those

00:58:24.167 --> 00:58:26.601
regardless of physical
location.

00:58:26.601 --> 00:58:30.000
What we'll be giving you
is a choice some--

00:58:30.000 --> 00:58:33.734
a choice and a way
to ensure some guarantees

00:58:33.734 --> 00:58:36.767
such that make sure all copies
of my data are in the E.U.,

00:58:36.767 --> 00:58:38.267
for example.

00:58:38.267 --> 00:58:40.067
We want to do that.

00:58:40.067 --> 00:58:42.000
I don't have details
on how it would work.

00:58:42.000 --> 00:58:44.868
Honestly, we don't entirely
know yet, but we want to.

00:58:44.868 --> 00:58:45.901
man: Okay, thanks.

00:58:45.901 --> 00:58:47.267
Barrett: Yeah.

00:58:47.267 --> 00:58:50.300
Let me take one more and then
I'll close it down.

00:58:50.300 --> 00:58:52.033
"Can you explain
what an entity group is

00:58:52.033 --> 00:58:54.334
with respect to how datastore
transactions work?"

00:58:54.334 --> 00:58:56.534
I think I did that before,
so I'm actually

00:58:56.534 --> 00:58:58.801
gonna call that.

00:58:58.801 --> 00:59:00.734
"Which technique is
the Google datastore using?"

00:59:00.734 --> 00:59:02.267
We are using
master-slave replication

00:59:02.267 --> 00:59:04.467
across datacenters.

00:59:04.467 --> 00:59:06.934
The main reason is latency.

00:59:06.934 --> 00:59:09.601
It gives us the best guarantees
that we can get

00:59:09.601 --> 00:59:11.934
without making every
datastore write take

00:59:11.934 --> 00:59:14.267
under 200 milliseconds.

00:59:14.267 --> 00:59:15.400
And this is a key takeaway.

00:59:15.400 --> 00:59:17.701
I should have been clearer
about this.

00:59:17.701 --> 00:59:20.868
It's definitely one thing
I'm telling you right now.

00:59:20.868 --> 00:59:22.901
I'm gonna let you all go
if there are no more questions.

00:59:22.901 --> 00:59:24.167
I can answer the rest
of these offline.

00:59:24.167 --> 00:59:25.367
But thank you for coming.
I appreciate it.

00:59:25.367 --> 00:59:26.734
Have a good conference.

00:59:26.734 --> 00:59:29.667
[applause]

