1
00:00:00,580 --> 00:00:03,730
In this video, we'll discuss
the capacity of a single neuron.

2
00:00:03,760 --> 00:00:07,450
That is the complexity of the
computations that it can perform.

3
00:00:08,720 --> 00:00:09,553
Okay.

4
00:00:09,660 --> 00:00:13,860
So we've seen before that,
uh, in two dimensions,

5
00:00:13,920 --> 00:00:16,070
if we draw the,
uh,

6
00:00:16,170 --> 00:00:19,080
output or the activation
of a given neuron,

7
00:00:19,230 --> 00:00:22,110
there will look something
like this where we'll get, uh,

8
00:00:22,111 --> 00:00:27,111
some rich between two parts of the space
where the Ridge is essentially defined

9
00:00:27,600 --> 00:00:32,220
by the orientation of the is defined
by the vector w and the bias will also

10
00:00:32,550 --> 00:00:34,830
determine the position of this rich.

11
00:00:38,570 --> 00:00:42,220
So with a a neuron that performs a comp,

12
00:00:42,290 --> 00:00:44,000
this type of computation,

13
00:00:44,030 --> 00:00:47,780
we could perform binary
classification that is a,

14
00:00:47,781 --> 00:00:51,920
if we use this sigmoidal,
uh, activation function,

15
00:00:52,250 --> 00:00:57,250
we could interpret the activation of a
neuron as estimating the probability that

16
00:00:58,131 --> 00:01:01,730
some input x belongs to the class one.

17
00:01:02,240 --> 00:01:05,000
And so let's assume in
binary classification,

18
00:01:05,120 --> 00:01:10,120
why can be either a zero or one and
then we could think of the output or the

19
00:01:11,241 --> 00:01:14,150
activation of a neuron as giving us,

20
00:01:14,151 --> 00:01:17,270
what's the estimate of the neuron that,
uh,

21
00:01:17,300 --> 00:01:21,500
the input actually belongs to the category
one and then the problem that would

22
00:01:21,501 --> 00:01:24,230
belong to the category. To, uh,

23
00:01:24,290 --> 00:01:28,550
based on the Neuron Testament would just
be one minus this because probabilities

24
00:01:28,670 --> 00:01:29,510
must sum to one.

25
00:01:30,760 --> 00:01:35,350
And so we could do this if we use
a sigmoid because the sigmoid is a,

26
00:01:35,351 --> 00:01:37,150
a bounded between zero and one.

27
00:01:37,240 --> 00:01:41,320
And so we're always guaranteed that we
get at the output or the activation of a

28
00:01:41,321 --> 00:01:45,940
neuron, a number which can be
interpreted as a probability.

29
00:01:46,750 --> 00:01:51,340
So this in fact this exact form for
a classifier is known as a logistic

30
00:01:51,341 --> 00:01:52,750
regression classifier.

31
00:01:53,320 --> 00:01:57,280
And the way it's performing classification
is that if the output of the neuron,

32
00:01:57,281 --> 00:01:58,790
so in other words,
uh,

33
00:01:59,260 --> 00:02:04,260
if it's estimate of the probability
that x belongs to the class,

34
00:02:04,571 --> 00:02:07,030
one is greater than 0.5,

35
00:02:07,060 --> 00:02:12,060
then we will categorize the input into
the class or category one and otherwise

36
00:02:13,960 --> 00:02:18,960
our classifier with output
prediction that the input belongs to,

37
00:02:19,280 --> 00:02:24,130
um, the class zero. And so
if we were to draw this,

38
00:02:24,500 --> 00:02:27,120
uh,
in two d than uh,

39
00:02:27,150 --> 00:02:31,240
and if we looked at what is known
as the dishes decision boundary,

40
00:02:31,540 --> 00:02:36,280
where the decision boundary is
essentially the surface where, uh,

41
00:02:36,310 --> 00:02:37,420
the,
uh,

42
00:02:37,450 --> 00:02:42,100
some input can equally belong to either
class zero or one could equally belong

43
00:02:42,101 --> 00:02:47,100
to any two class, then what we
would get actually is that, uh,

44
00:02:47,250 --> 00:02:51,280
the classifier is performing a
linear classification. That is,

45
00:02:51,281 --> 00:02:54,490
it's drawing a line between two regions,

46
00:02:54,580 --> 00:02:58,720
the regions that it's associates with
one class and the other region that

47
00:02:58,780 --> 00:03:00,190
associates with the other class.

48
00:03:00,191 --> 00:03:04,090
And just drawing a boundary
which is actually linear. So, uh,

49
00:03:04,120 --> 00:03:08,410
in today's corresponds to align and in
more dimensions would correspond to a

50
00:03:08,411 --> 00:03:09,244
hyper plane.

51
00:03:11,080 --> 00:03:15,730
So if we have a problem where
we want to classify objects,

52
00:03:15,790 --> 00:03:19,540
uh, described by input vectors
into two different classes,

53
00:03:19,720 --> 00:03:24,720
if we can draw a hyperplane or align
into the between these two types of

54
00:03:25,661 --> 00:03:29,320
objects, then a single artificial
neuron could do that for us.

55
00:03:29,321 --> 00:03:31,690
It could model that type
of decision process.

56
00:03:34,030 --> 00:03:39,030
So here's a few example of a simple
functions which can be modeled by a linear

57
00:03:39,611 --> 00:03:40,444
classifier.

58
00:03:40,600 --> 00:03:44,890
So if we have a binary inputs
that are either zero or one,

59
00:03:44,920 --> 00:03:49,920
so we have [inaudible] that can be zero
one x two it can be zero one and then we

60
00:03:50,201 --> 00:03:55,201
want to model the or function which
takes the aura of x one and x two so four

61
00:03:56,621 --> 00:04:01,621
zero zero it would output a zero so that
will be class zero and for a one zero

62
00:04:02,990 --> 00:04:07,830
one one or one or a zero
one it would output once.

63
00:04:07,830 --> 00:04:10,120
So the triangles are
correspond to class one.

64
00:04:10,740 --> 00:04:12,940
Well we can see that if we draw this,

65
00:04:12,941 --> 00:04:17,260
we get easily pass align between all
the circles and all the triangles,

66
00:04:17,261 --> 00:04:21,760
all the Zeros and all the
ones is another function,

67
00:04:21,790 --> 00:04:22,810
a bit more complicated.

68
00:04:22,811 --> 00:04:27,400
The end function over the
negation of x one and x two.

69
00:04:27,790 --> 00:04:28,181
Uh,

70
00:04:28,181 --> 00:04:33,030
then we get that all of these guys will
be of course zero and this will be a

71
00:04:33,160 --> 00:04:35,800
class a one. And indeed, again,

72
00:04:35,801 --> 00:04:39,940
we can pass a line between the two
classes and we have another example,

73
00:04:39,941 --> 00:04:43,930
but instead of negation of x two where
we get a class one here and the others

74
00:04:43,931 --> 00:04:46,450
are zero, again, we can
pass a straight line.

75
00:04:46,930 --> 00:04:51,930
So these simple functions can easily be
modeled by a single artificial neuron.

76
00:04:54,700 --> 00:04:56,740
However,
there are many problems in practice,

77
00:04:56,750 --> 00:05:01,750
are not well separated
linearly and actually the are
very simple functions that

78
00:05:02,561 --> 00:05:06,700
are not even uh, uh, linearly separable.

79
00:05:07,090 --> 00:05:11,290
So there's an example of another
simple function, the x or function.

80
00:05:11,880 --> 00:05:15,520
So this is a function
that outputs zero. If, uh,

81
00:05:15,580 --> 00:05:20,580
either both inputs x one and x two are
zero or they're both one and it all the

82
00:05:21,041 --> 00:05:24,220
outputs one.
So for these two guys,

83
00:05:24,370 --> 00:05:28,120
when either one of the inputs are a one.

84
00:05:28,660 --> 00:05:30,940
So in this case here,

85
00:05:30,941 --> 00:05:35,170
we have one here we have zero four
at this point. And here at this one,

86
00:05:35,171 --> 00:05:39,070
the x one is one and x two
is zero for this point.

87
00:05:40,690 --> 00:05:44,980
And so in this case we see that we
cannot draw just a single line that would

88
00:05:44,981 --> 00:05:49,270
actually separate, uh, on one side
all of the Zeros and on one side,

89
00:05:49,300 --> 00:05:54,100
all the ones. So it's not linearly
separable. And yet it's a actually a very,

90
00:05:54,101 --> 00:05:55,210
very simple problem.

91
00:05:55,211 --> 00:05:59,660
So this suggests that a single artificial
neuron will not be sufficient for many

92
00:05:59,661 --> 00:06:02,990
problems where we want to perform
this kind of binary classification.

93
00:06:04,340 --> 00:06:04,761
However,

94
00:06:04,761 --> 00:06:09,761
we noticed that it's such a simple
function that if we had instead plotted on

95
00:06:10,521 --> 00:06:15,521
this axis deer result of applying the n
function over the negotiation of x one

96
00:06:16,040 --> 00:06:18,560
and x two like we've seen
in the previous slide.

97
00:06:18,890 --> 00:06:22,490
And on this axis we actually drew the uh,

98
00:06:22,491 --> 00:06:24,280
we used as the value in this axis,

99
00:06:24,350 --> 00:06:28,520
the output of the EMF function over
x one and the negation of x two.

100
00:06:29,240 --> 00:06:33,920
Then this point and this point we
be collapsed over a single point.

101
00:06:34,730 --> 00:06:38,060
And then this point would curse bond.

102
00:06:38,061 --> 00:06:42,830
Now to that point and this point
to that point. And so in this case,

103
00:06:42,831 --> 00:06:47,831
we could actually draw a single line
between the circles and the triangles

104
00:06:48,920 --> 00:06:52,640
between the points associated with class
zero and the points associated with

105
00:06:52,641 --> 00:06:53,474
class one.

106
00:06:54,080 --> 00:06:58,130
So what this is saying is that if we get,

107
00:06:58,450 --> 00:07:02,930
if we compute an alternate representation
or better representation of our input

108
00:07:02,931 --> 00:07:06,290
vector, the problem might
become linearly separable.

109
00:07:07,010 --> 00:07:11,870
And this also suggests that if we
have these, this other representation,

110
00:07:11,871 --> 00:07:15,200
which is actually representable
by a single neuron,

111
00:07:15,440 --> 00:07:19,400
and that's the case for this and function
in this and function we've seen before,

112
00:07:19,401 --> 00:07:22,730
that we can actually model these
two functions by a single neuron.

113
00:07:23,330 --> 00:07:27,680
Then this suggests that by having
other neurons connected by,

114
00:07:28,020 --> 00:07:31,220
uh, other neurons, which would, uh,

115
00:07:31,221 --> 00:07:35,000
so where we have a neuron that computes
this and around the compute stack,

116
00:07:35,210 --> 00:07:39,980
if we connected these two neurons to
another neuron connected directly to these

117
00:07:40,100 --> 00:07:44,120
artificial neurons here,
then we might be able to, uh,

118
00:07:44,150 --> 00:07:46,250
compute a, uh, function,

119
00:07:46,251 --> 00:07:50,440
a certain functions that
require more complicated, uh,

120
00:07:50,450 --> 00:07:53,180
computations and a more
complicated decision surface.

121
00:07:53,660 --> 00:07:58,660
And this will be the main intuition
behind developing more complicated a

122
00:07:58,940 --> 00:08:02,930
multilayer neural networks,
which we'll see in the next video.

