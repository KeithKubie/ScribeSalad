WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.848
[MUSIC PLAYING]

00:00:08.475 --> 00:00:10.600
SERGIO GUADARRAMA: So my
name is Sergio Guadarrama.

00:00:10.600 --> 00:00:13.180
I'm a senior software
engineer at Google Brain,

00:00:13.180 --> 00:00:15.920
and I'm leading
the TF agents team.

00:00:15.920 --> 00:00:18.390
EUGENE BREVDO: And I'm Eugene
Brevdo, a software engineer

00:00:18.390 --> 00:00:19.620
on Google Brain team.

00:00:19.620 --> 00:00:21.390
I work in
reinforcement learning.

00:00:21.390 --> 00:00:22.765
SERGIO GUADARRAMA:
So today we're

00:00:22.765 --> 00:00:25.690
going to talk about
reinforcement learning.

00:00:25.690 --> 00:00:31.380
So how many of you remember
how you learned how to walk?

00:00:31.380 --> 00:00:32.810
You stumble a little bit.

00:00:32.810 --> 00:00:35.053
You try one step,
it doesn't work.

00:00:35.053 --> 00:00:35.970
You lose your balance.

00:00:35.970 --> 00:00:37.092
You try again.

00:00:37.092 --> 00:00:39.300
So when you're trying to
learn something that is hard

00:00:39.300 --> 00:00:42.580
and you need a lot of practice,
you need to try many times.

00:00:42.580 --> 00:00:45.330
So this cute little robot is
basically trying to do that--

00:00:45.330 --> 00:00:46.230
just moving the legs.

00:00:46.230 --> 00:00:47.938
It doesn't coordinate
very well, but it's

00:00:47.938 --> 00:00:50.140
trying to learn how to walk.

00:00:50.140 --> 00:00:52.210
After learning to
try multiple times--

00:00:52.210 --> 00:00:53.533
in this case 1,000 times--

00:00:53.533 --> 00:00:54.950
it learns a little
bit how to move

00:00:54.950 --> 00:00:57.910
the first steps, moving
forward a little bit

00:00:57.910 --> 00:00:59.830
before falling off.

00:00:59.830 --> 00:01:01.900
If we let it train
a little longer,

00:01:01.900 --> 00:01:04.900
then it's able to actually
walk around, go from one place

00:01:04.900 --> 00:01:09.467
to another, and find
their way around the room.

00:01:09.467 --> 00:01:11.550
Probably you have heard
about all the applications

00:01:11.550 --> 00:01:13.930
of reinforcement learning
over the last couple of years,

00:01:13.930 --> 00:01:16.570
you know, including
recommended systems,

00:01:16.570 --> 00:01:21.400
data, certain [INAUDIBLE],,
real robots like that,

00:01:21.400 --> 00:01:24.490
chemistry, math, this
little [? killer ?] robot,

00:01:24.490 --> 00:01:27.640
but also like AlphaGo,
that play Go, like, better

00:01:27.640 --> 00:01:28.300
than any human.

00:01:31.120 --> 00:01:32.370
Now I have a question for you.

00:01:32.370 --> 00:01:34.660
How many of you have tried
to actually implement

00:01:34.660 --> 00:01:36.060
an RL algorithm?

00:01:36.060 --> 00:01:36.560
OK.

00:01:36.560 --> 00:01:37.960
I see quite a bit the hands.

00:01:37.960 --> 00:01:39.850
Very good.

00:01:39.850 --> 00:01:40.805
It's hard.

00:01:40.805 --> 00:01:42.020
[LAUGHTER]

00:01:42.020 --> 00:01:42.650
Yeah.

00:01:42.650 --> 00:01:44.870
We went through that pain too.

00:01:44.870 --> 00:01:48.440
Many people who try, get the
first prototype right away.

00:01:48.440 --> 00:01:50.000
It seems to be
working, but then you

00:01:50.000 --> 00:01:51.740
miss a lot of different pieces.

00:01:51.740 --> 00:01:53.360
All the details
have to be right.

00:01:53.360 --> 00:01:55.365
All the things, all
the bugs, everything,

00:01:55.365 --> 00:01:56.490
because it's very unstable.

00:01:56.490 --> 00:01:58.860
[INAUDIBLE] is a feature.

00:01:58.860 --> 00:02:02.630
So there's a lot of
pieces, a replay buffer--

00:02:02.630 --> 00:02:05.060
there's a lot of
things you need to do.

00:02:05.060 --> 00:02:07.490
So we suffer through the
same problem at Google,

00:02:07.490 --> 00:02:10.100
so we decided to implement a
library that many people can

00:02:10.100 --> 00:02:10.889
use.

00:02:10.889 --> 00:02:13.250
And today the TF-Agents
team is very happy

00:02:13.250 --> 00:02:15.150
to announce that it's
available online.

00:02:15.150 --> 00:02:16.490
You can go to GitHub.

00:02:16.490 --> 00:02:19.180
You can pip install it and
start using it right way.

00:02:19.180 --> 00:02:22.030
And hopefully you will
provide feedback contributions

00:02:22.030 --> 00:02:25.580
so we can make this
better over time.

00:02:25.580 --> 00:02:28.630
So now, what is TF-Agents
and what it provides?

00:02:28.630 --> 00:02:31.480
So we tried to make it
very robust, very scalable,

00:02:31.480 --> 00:02:33.980
and easy to use reinforced
learning for TensorFlow.

00:02:33.980 --> 00:02:37.210
So it's going to be easy
to debug, easy to try,

00:02:37.210 --> 00:02:39.040
and easy to get
good things going.

00:02:39.040 --> 00:02:41.390
For people who are new
to reinforced learning,

00:02:41.390 --> 00:02:44.560
we have colabs and things,
documentation and samples

00:02:44.560 --> 00:02:45.850
so you can learn about it.

00:02:45.850 --> 00:02:48.340
For people who want to
really solve a real problem,

00:02:48.340 --> 00:02:50.590
a complex problem,
we have already ways

00:02:50.590 --> 00:02:54.125
to state-of-the-art algorithms
and apply [? very quickly. ?]

00:02:54.125 --> 00:02:55.750
For people who are
researchers and want

00:02:55.750 --> 00:02:57.355
to develop new RL
algorithms, they

00:02:57.355 --> 00:02:59.230
don't need to build all
of the single pieces.

00:02:59.230 --> 00:03:00.670
They can build on top of it.

00:03:00.670 --> 00:03:03.130
We make it well-tested
and easy to configure,

00:03:03.130 --> 00:03:06.880
so you kind of start doing
your experiments right away.

00:03:06.880 --> 00:03:10.060
We build on top of all the
goodies of TensorFlow 2.0

00:03:10.060 --> 00:03:12.310
that you just saw
today, like TF-Eagers

00:03:12.310 --> 00:03:15.250
to make the development
and the debugging

00:03:15.250 --> 00:03:18.220
a lot easier, tf.keras
to build the networks on

00:03:18.220 --> 00:03:20.350
and models, tf.function,
when you want

00:03:20.350 --> 00:03:22.160
to make things to go faster.

00:03:22.160 --> 00:03:24.160
And then we make it very
modular and extensible,

00:03:24.160 --> 00:03:25.155
so you can cherry pick.

00:03:25.155 --> 00:03:25.780
It's elaborate.

00:03:25.780 --> 00:03:28.120
You can cherry pick with
the pieces that you need

00:03:28.120 --> 00:03:30.405
and extend them as you need it.

00:03:30.405 --> 00:03:32.530
And for those who are not
ready for the change yet,

00:03:32.530 --> 00:03:37.540
we make it compatible
with TensorFlow 1.14.

00:03:37.540 --> 00:03:40.580
So if we go back to the little
sample of the little robot

00:03:40.580 --> 00:03:42.850
trying to walk, this
is in a nutshell,

00:03:42.850 --> 00:03:44.715
how the code looks like.

00:03:44.715 --> 00:03:46.090
You have to define
some networks,

00:03:46.090 --> 00:03:47.882
in this case an active
distribution network

00:03:47.882 --> 00:03:50.720
and a critic network, and
then an actor, and an agent,

00:03:50.720 --> 00:03:53.200
[INAUDIBLE] agent in this case.

00:03:53.200 --> 00:03:55.810
And then assuming we have some
experience already collected,

00:03:55.810 --> 00:03:57.090
we can just train through it.

00:03:59.950 --> 00:04:03.150
TF-Agents provide a
lot of RL algorithms,

00:04:03.150 --> 00:04:06.300
RL environment already, like
[INAUDIBLE],, Atari, Mujoco,

00:04:06.300 --> 00:04:10.600
PyBullet, DM-Control,
and maybe your soon.

00:04:10.600 --> 00:04:14.500
We also provide state-of-the-art
algorithms, including DQN, TD3,

00:04:14.500 --> 00:04:17.138
PPO, [INAUDIBLE]
and many others.

00:04:17.138 --> 00:04:19.180
And they are more coming
soon, and hopefully more

00:04:19.180 --> 00:04:20.470
from the community.

00:04:20.470 --> 00:04:22.900
They are fully tested
with quality regression

00:04:22.900 --> 00:04:27.630
tests and a speed test to
make things keep working.

00:04:27.630 --> 00:04:30.180
As an overview of the
system, it looks like that.

00:04:30.180 --> 00:04:32.910
On the left side you have all
the collection aspects of it.

00:04:32.910 --> 00:04:34.410
And we're going to
have some policy.

00:04:34.410 --> 00:04:36.202
It's going to interact
with the environment

00:04:36.202 --> 00:04:38.070
and collect some experience.

00:04:38.070 --> 00:04:41.490
Probably we put in some
replay buffers to work later.

00:04:41.490 --> 00:04:44.240
And on the right side we have
all the training pipeline,

00:04:44.240 --> 00:04:46.260
where we're going to write
from this experience,

00:04:46.260 --> 00:04:48.630
and our agent is going to
learn to improve the policy

00:04:48.630 --> 00:04:51.470
by training a neural network.

00:04:51.470 --> 00:04:53.640
Let's focus for a little
bit in this environment.

00:04:53.640 --> 00:04:54.800
How do we define a problem?

00:04:54.800 --> 00:04:57.260
How do we define a new task?

00:04:57.260 --> 00:04:58.530
Let's take another example.

00:04:58.530 --> 00:04:59.805
In this case it's Breakout.

00:04:59.805 --> 00:05:01.680
The idea is like, you
have to play this game,

00:05:01.680 --> 00:05:03.590
move the paddle left
and right, and try

00:05:03.590 --> 00:05:06.200
to break the bricks on the top.

00:05:06.200 --> 00:05:08.990
You break the bricks, you get
rewards, so the points go up.

00:05:08.990 --> 00:05:12.007
You let the ball drop,
then the points go down.

00:05:12.007 --> 00:05:14.090
So the agent is going to
receive some observation,

00:05:14.090 --> 00:05:16.060
in this case, multiple frames.

00:05:16.060 --> 00:05:17.810
From the environment,
it's going to decide

00:05:17.810 --> 00:05:20.370
which action to take, and
then based on that it's

00:05:20.370 --> 00:05:21.450
going to get some reward.

00:05:21.450 --> 00:05:24.290
And then just loop again.

00:05:24.290 --> 00:05:26.600
How this looks into the
code is something like this.

00:05:26.600 --> 00:05:28.610
You define the observation
and specification.

00:05:28.610 --> 00:05:29.635
It's like, what
kind of observation

00:05:29.635 --> 00:05:31.220
does this environments provide?

00:05:31.220 --> 00:05:33.845
In this case it could be frames,
but it could be any tensor, so

00:05:33.845 --> 00:05:36.478
any other information-- multiple
cameras, multiple things.

00:05:36.478 --> 00:05:38.770
And then the action specs--
it's like, what actions can

00:05:38.770 --> 00:05:39.950
I make in this environment?

00:05:39.950 --> 00:05:41.650
In this case only
left and right.

00:05:41.650 --> 00:05:44.420
But in many other environments
we have multiple options.

00:05:44.420 --> 00:05:45.890
Then a reset method,
because we're

00:05:45.890 --> 00:05:47.250
going to play this
game a lot of times,

00:05:47.250 --> 00:05:48.830
so we have to reset
the environment,

00:05:48.830 --> 00:05:50.330
and then a step method.

00:05:50.330 --> 00:05:53.000
Taking an action is going
to produce a new observation

00:05:53.000 --> 00:05:55.230
and give us a reward.

00:05:55.230 --> 00:05:58.770
Given that, we could define a
policy by hand, for example,

00:05:58.770 --> 00:06:00.040
and start playing this game.

00:06:00.040 --> 00:06:02.728
You just create an environment,
define your policy,

00:06:02.728 --> 00:06:04.770
reset the environment,
and start looping over it,

00:06:04.770 --> 00:06:05.895
and start playing the game.

00:06:05.895 --> 00:06:09.620
If your policy is very good,
you will get a good score.

00:06:09.620 --> 00:06:11.250
To make the learning
even faster,

00:06:11.250 --> 00:06:12.930
we make these
parallel environments,

00:06:12.930 --> 00:06:15.780
so you can run these games
in parallel, multiple times

00:06:15.780 --> 00:06:18.520
and wrapped in TensorFlow
so it will go even faster,

00:06:18.520 --> 00:06:20.700
and then do the same loop again.

00:06:20.700 --> 00:06:22.800
What happened in
general is, like, we

00:06:22.800 --> 00:06:25.750
don't want to define
these policies by hand,

00:06:25.750 --> 00:06:27.510
so let me hand it
over to Eugene,

00:06:27.510 --> 00:06:29.830
who's going to explain how
to land those policies.

00:06:29.830 --> 00:06:32.540
EUGENE BREVDO:
Thank you, Sergio.

00:06:32.540 --> 00:06:35.370
So, yeah, as Sergio
said, we've given

00:06:35.370 --> 00:06:40.840
an example of how you would
interact with an environment

00:06:40.840 --> 00:06:42.150
via a policy.

00:06:42.150 --> 00:06:44.370
And we're going to go into
a little bit more detail

00:06:44.370 --> 00:06:46.450
and talk about how
to make policies,

00:06:46.450 --> 00:06:51.690
how to train policies
to maximize the rewards.

00:06:51.690 --> 00:06:55.110
So kind of going over
it again, policies

00:06:55.110 --> 00:07:00.890
take observations and emit a
distribution over the actions.

00:07:00.890 --> 00:07:03.480
And in this case,
the observations

00:07:03.480 --> 00:07:06.450
are an image, or
a stack of images.

00:07:06.450 --> 00:07:08.550
There is an underlying
neural network

00:07:08.550 --> 00:07:11.850
that converts those
images to the parameters

00:07:11.850 --> 00:07:13.900
of the distribution.

00:07:13.900 --> 00:07:17.130
And then the policy
emits that distribution,

00:07:17.130 --> 00:07:21.470
or you might sample from it
to actually take actions.

00:07:21.470 --> 00:07:24.640
So let's talk about networks.

00:07:24.640 --> 00:07:27.040
I think you've seen some
variation of this slide

00:07:27.040 --> 00:07:29.350
over and over again today.

00:07:29.350 --> 00:07:32.130
A network, in this
case, a network

00:07:32.130 --> 00:07:35.440
used for deep Q learning
is essentially a container

00:07:35.440 --> 00:07:37.360
for a bunch of keras.layers.

00:07:37.360 --> 00:07:42.080
In this case your inputs go
through a convolution layer

00:07:42.080 --> 00:07:43.040
and so on and so forth.

00:07:43.040 --> 00:07:47.680
And then the final
layer emits logits

00:07:47.680 --> 00:07:52.200
over the number of actions
that you might take.

00:07:52.200 --> 00:07:56.550
The core method of the
network is the call.

00:07:56.550 --> 00:08:01.830
So it takes observations in a
state, possibly an RNN state

00:08:01.830 --> 00:08:06.650
and emits the logits in
the new updated state.

00:08:06.650 --> 00:08:07.150
OK.

00:08:07.150 --> 00:08:09.470
So let's talk about policies.

00:08:09.470 --> 00:08:12.220
First of all, we
provide a large number

00:08:12.220 --> 00:08:14.530
of policies, some of them
specifically tailored

00:08:14.530 --> 00:08:18.160
to particular algorithms
and particular agents.

00:08:18.160 --> 00:08:21.025
But you can also
implement your own.

00:08:21.025 --> 00:08:24.290
So it's useful to
go through that.

00:08:24.290 --> 00:08:27.880
So a policy takes
one of more networks.

00:08:27.880 --> 00:08:31.240
And the fundamental
method on a policy

00:08:31.240 --> 00:08:32.840
is the distribution method.

00:08:32.840 --> 00:08:35.140
So this takes the time step.

00:08:35.140 --> 00:08:39.460
It essentially contains
the observation,

00:08:39.460 --> 00:08:43.690
passes that through
one or more networks

00:08:43.690 --> 00:08:46.540
and emits the parameters
of the output distribution,

00:08:46.540 --> 00:08:49.270
in this case, logits.

00:08:49.270 --> 00:08:52.010
It then returns a
tuple of three things.

00:08:52.010 --> 00:08:54.460
So the first thing is a
actual distribution object.

00:08:54.460 --> 00:08:56.740
So Josh Dylan just spoke
about this with probability,

00:08:56.740 --> 00:08:58.540
and here's this
little probability

00:08:58.540 --> 00:09:02.210
category called distribution
built from those logits.

00:09:02.210 --> 00:09:05.260
It emits the next state,
again, possibly containing

00:09:05.260 --> 00:09:10.420
some RNN state information, and
it also emits site information.

00:09:10.420 --> 00:09:13.540
So site information is useful.

00:09:13.540 --> 00:09:15.550
Perhaps you want to
emit some information

00:09:15.550 --> 00:09:19.720
that you want to log in your
metrics that's not the action,

00:09:19.720 --> 00:09:22.810
or you maybe want to log
some information that

00:09:22.810 --> 00:09:24.715
is necessary for
training later on.

00:09:24.715 --> 00:09:26.590
So the agent is going
to use that information

00:09:26.590 --> 00:09:27.340
to actually train.

00:09:30.310 --> 00:09:30.990
OK.

00:09:30.990 --> 00:09:33.990
So now let's actually
talk about training.

00:09:33.990 --> 00:09:38.970
The agent class encompasses
the main RL algorithm,

00:09:38.970 --> 00:09:44.100
and that includes the
training and reading

00:09:44.100 --> 00:09:50.660
batches of data and trajectories
to update the neural network.

00:09:50.660 --> 00:09:53.470
So here's a simple example.

00:09:53.470 --> 00:09:56.560
First you create a
deep Q learning agent.

00:09:56.560 --> 00:09:58.690
You give it a network.

00:09:58.690 --> 00:10:02.830
You can access a policy,
specifically a collection

00:10:02.830 --> 00:10:04.360
policy from that agent.

00:10:04.360 --> 00:10:06.790
That policy uses the
underlying network

00:10:06.790 --> 00:10:11.080
that you passed in, and maybe
performs some additional work.

00:10:11.080 --> 00:10:13.030
Like maybe it performs
some exploration,

00:10:13.030 --> 00:10:15.910
like epsilon greedy
exploration, and also logs

00:10:15.910 --> 00:10:18.700
site information that is
going to be necessary to be

00:10:18.700 --> 00:10:21.890
able to train the agent.

00:10:21.890 --> 00:10:25.010
The main method on the
agent is called train.

00:10:25.010 --> 00:10:28.430
It takes experience in the
form of batch trajectories.

00:10:28.430 --> 00:10:32.020
These come, for example,
from a replay buffer.

00:10:32.020 --> 00:10:36.180
Now assuming you have
trained your networks

00:10:36.180 --> 00:10:40.000
and you're performing well
during data collection,

00:10:40.000 --> 00:10:42.270
you also might want
to take a policy that

00:10:42.270 --> 00:10:45.450
performs more greedy action
and doesn't explore it all.

00:10:45.450 --> 00:10:46.680
It just exploits.

00:10:46.680 --> 00:10:50.250
It takes the best actions
that it thinks are the best

00:10:50.250 --> 00:10:52.380
and doesn't log any
site information,

00:10:52.380 --> 00:10:53.950
doesn't admit any
site information.

00:10:53.950 --> 00:10:56.040
So that's the deployment policy.

00:10:56.040 --> 00:10:57.990
You can save this
SaveModel, for example,

00:10:57.990 --> 00:10:59.247
and put it into the frame.

00:11:02.050 --> 00:11:05.380
So a more complete example--

00:11:05.380 --> 00:11:08.210
again, here we have a
deep Q learning network.

00:11:08.210 --> 00:11:11.350
It accepts the observation
and action specs

00:11:11.350 --> 00:11:15.700
from the environment,
and some other arguments

00:11:15.700 --> 00:11:20.220
describing what kind of
keras layers to combine.

00:11:20.220 --> 00:11:23.600
You build the agent
with that network.

00:11:23.600 --> 00:11:27.120
And then you get a
tf.data data set.

00:11:27.120 --> 00:11:31.180
In this case you get it
from a replay buffer object.

00:11:31.180 --> 00:11:33.420
But you can get it
from any other data

00:11:33.420 --> 00:11:37.200
set that emits the correct
form and trajectory, batch

00:11:37.200 --> 00:11:38.790
trajectory information.

00:11:38.790 --> 00:11:42.960
And then you iterate over that
data set, calling agent.train

00:11:42.960 --> 00:11:45.608
to update the underlying
neural networks, which are then

00:11:45.608 --> 00:11:47.025
reflected in the
updated policies.

00:11:49.550 --> 00:11:51.560
So let's talk a little
bit about collection.

00:11:51.560 --> 00:11:53.810
Now given a collection policy--

00:11:53.810 --> 00:11:55.390
and it doesn't
have to be trained.

00:11:55.390 --> 00:11:59.020
It can have just
random parameters.

00:11:59.020 --> 00:12:01.420
You want to be able
to collect data.

00:12:01.420 --> 00:12:04.060
And we provide a number
of tools for that.

00:12:04.060 --> 00:12:06.850
Again, if your environment is
something that is in Python,

00:12:06.850 --> 00:12:08.620
you can wrap it.

00:12:08.620 --> 00:12:14.500
So the core tool for
this is the driver.

00:12:14.500 --> 00:12:16.240
And going through
that, first you

00:12:16.240 --> 00:12:19.347
create your batched
environments at the top.

00:12:19.347 --> 00:12:20.680
Then you create a replay buffer.

00:12:20.680 --> 00:12:23.720
In this case we have a
TF uniform replay buffer.

00:12:23.720 --> 00:12:27.108
So this is a replay buffer
backed by TensorFlow variables.

00:12:27.108 --> 00:12:28.400
And then you create the driver.

00:12:28.400 --> 00:12:30.580
So the driver accepts
the environment,

00:12:30.580 --> 00:12:33.200
the collect policy
from the agent,

00:12:33.200 --> 00:12:35.780
and a number of callbacks.

00:12:35.780 --> 00:12:41.240
And when you call
driver.run, what it will do

00:12:41.240 --> 00:12:43.340
is it will iterate,
in this case,

00:12:43.340 --> 00:12:46.340
it will take 100
steps of interaction

00:12:46.340 --> 00:12:48.590
between the policy
and the environment,

00:12:48.590 --> 00:12:52.910
create trajectories, and
pass them to the observers.

00:12:52.910 --> 00:12:56.590
So after driver.run
has finished,

00:12:56.590 --> 00:13:00.620
your replay buffer has been
populated with a hundred more

00:13:00.620 --> 00:13:02.140
frames of data.

00:13:04.960 --> 00:13:08.190
So here's kind of
the complete picture.

00:13:08.190 --> 00:13:11.970
Again, you create
your environment.

00:13:11.970 --> 00:13:15.720
You interact with that
environment through the driver,

00:13:15.720 --> 00:13:18.570
given a policy.

00:13:18.570 --> 00:13:22.850
Those interactions get
stored in the replay buffer.

00:13:22.850 --> 00:13:26.730
The replay buffer, you read
from with the tf.data data set.

00:13:26.730 --> 00:13:29.130
And then the agent
trains with batches

00:13:29.130 --> 00:13:33.070
from that data set, and
updates the network underlying

00:13:33.070 --> 00:13:34.050
the policy.

00:13:36.960 --> 00:13:40.110
Here's kind of a set
of commands to do that.

00:13:40.110 --> 00:13:43.620
If you look at the
bottom, here's that loop.

00:13:43.620 --> 00:13:46.740
You call it driver.run
to collect data.

00:13:46.740 --> 00:13:49.070
It stores that in
the replay buffer.

00:13:49.070 --> 00:13:52.850
And then you read from the data
set generated from that replay

00:13:52.850 --> 00:13:54.120
buffer and train the agent.

00:13:54.120 --> 00:13:57.560
You can iterate this
over and over again.

00:13:57.560 --> 00:13:58.080
OK.

00:13:58.080 --> 00:14:02.920
So we have a lot of
exciting things coming up.

00:14:02.920 --> 00:14:05.400
For example, we have
a number of new agents

00:14:05.400 --> 00:14:06.640
that we're going to release--

00:14:06.640 --> 00:14:08.850
C51, D4PG and so on.

00:14:08.850 --> 00:14:11.070
We're adding complete support
for contextual bandits

00:14:11.070 --> 00:14:14.280
that are backed by neural
networks to the API.

00:14:14.280 --> 00:14:17.850
We're going to release
a number of baselines,

00:14:17.850 --> 00:14:21.200
as well as a number
of new replay buffers.

00:14:21.200 --> 00:14:25.230
So in particular we're going to
be releasing some distributed

00:14:25.230 --> 00:14:29.520
replay buffers in the
next couple of quarters,

00:14:29.520 --> 00:14:32.380
and those will be used for
distributed collection.

00:14:32.380 --> 00:14:34.290
So distributed
collection allows you

00:14:34.290 --> 00:14:36.090
to parallelize your
data collection

00:14:36.090 --> 00:14:40.530
across many machines,
and be able to maximize

00:14:40.530 --> 00:14:44.280
the throughput of your training
URL algorithm that way.

00:14:44.280 --> 00:14:47.700
We're also working on
distributed training using

00:14:47.700 --> 00:14:50.570
TensorFlow's new
distribution strategy API,

00:14:50.570 --> 00:14:55.500
allowing you to train at a
massive scale on many GPUs

00:14:55.500 --> 00:14:56.430
and TPUs.

00:14:56.430 --> 00:14:58.500
And we're adding support
for more environments.

00:14:58.500 --> 00:15:04.110
So please check out
TF-Agents on GitHub.

00:15:04.110 --> 00:15:06.948
And we have a
number of colabs, I

00:15:06.948 --> 00:15:08.490
think eight or nine
as of this count,

00:15:08.490 --> 00:15:11.582
exploring different
parts of the system.

00:15:11.582 --> 00:15:17.700
And as Sergio said, TF-Agents is
built to solve many real world

00:15:17.700 --> 00:15:19.060
problems.

00:15:19.060 --> 00:15:21.480
And in particular, we're
interested in seeing

00:15:21.480 --> 00:15:24.990
what your problems are, for
example, where we welcome

00:15:24.990 --> 00:15:28.620
contributions for new
environments, new RL

00:15:28.620 --> 00:15:33.310
algorithms, for those of you
out there who are RL experts.

00:15:33.310 --> 00:15:38.550
Please come chat with me
or Sergio after the talks

00:15:38.550 --> 00:15:43.080
or file an issue on the
GitHub issue tracker.

00:15:43.080 --> 00:15:44.970
And let us know.

00:15:44.970 --> 00:15:47.200
Let us know what you think.

00:15:47.200 --> 00:15:48.220
Thank you very much.

00:15:48.220 --> 00:15:51.570
[MUSIC PLAYING]

