WEBVTT
Kind: captions
Language: en

00:00:03.060 --> 00:00:05.620
Hi, I’m Carrie Anne, and welcome to CrashCourse
Computer Science!

00:00:05.620 --> 00:00:10.010
Over the past two episodes, we got our first
taste of programming in a high-level language,

00:00:10.010 --> 00:00:11.390
like Python or Java.

00:00:11.390 --> 00:00:14.269
We talked about different types of programming
language statements – like assignments,

00:00:14.269 --> 00:00:18.670
ifs, and loops – as well as putting statements
into functions that perform a computation,

00:00:18.670 --> 00:00:20.330
like calculating an exponent.

00:00:20.330 --> 00:00:24.599
Importantly, the function we wrote to calculate
exponents is only one possible solution.

00:00:24.599 --> 00:00:27.589
There are other ways to write this function
– using different statements in different

00:00:27.589 --> 00:00:30.339
orders – that achieve exactly the same numerical
result.

00:00:30.340 --> 00:00:34.420
The difference between them is the algorithm,
that is the specific steps used to complete

00:00:34.440 --> 00:00:35.160
the computation.

00:00:35.160 --> 00:00:38.500
Some algorithms are better than others even
if they produce equal results.

00:00:38.520 --> 00:00:42.870
Generally, the fewer steps it takes to compute,
the better it is, though sometimes we care

00:00:42.870 --> 00:00:45.420
about other factors, like how much memory
it uses.

00:00:45.420 --> 00:00:49.640
The term algorithm comes from Persian polymath
Muḥammad ibn Mūsā al-Khwārizmī who was

00:00:49.640 --> 00:00:52.329
one of the fathers of algebra more than a
millennium ago.

00:00:52.329 --> 00:00:56.050
The crafting of efficient algorithms – a
problem that existed long before modern computers

00:00:56.050 --> 00:01:00.840
– led to a whole science surrounding computation,
which evolved into the modern discipline of…

00:01:00.840 --> 00:01:01.840
you guessed it!

00:01:01.840 --> 00:01:02.760
Computer Science!

00:01:02.760 --> 00:01:11.960
INTRO

00:01:11.970 --> 00:01:15.781
One of the most storied algorithmic problems
in all of computer science is sorting… as

00:01:15.781 --> 00:01:18.290
in sorting names or sorting numbers.

00:01:18.290 --> 00:01:19.869
Computers sort all the time.

00:01:19.869 --> 00:01:23.840
Looking for the cheapest airfare, arranging
your email by most recently sent, or scrolling

00:01:23.840 --> 00:01:27.149
your contacts by last name -- those all require
sorting.

00:01:27.149 --> 00:01:30.920
You might think “sorting isn’t so tough…
how many algorithms can there possibly be?”

00:01:30.920 --> 00:01:32.500
The answer is: a lot.

00:01:32.500 --> 00:01:36.180
Computer Scientists have spent decades inventing
algorithms for sorting, with cool names like

00:01:36.180 --> 00:01:38.310
Bubble Sort and Spaghetti Sort.

00:01:38.310 --> 00:01:39.780
Let’s try sorting!

00:01:39.780 --> 00:01:42.450
Imagine we have a set of airfare prices to
Indianapolis.

00:01:42.450 --> 00:01:47.439
We’ll talk about how data like this is represented
in memory next week, but for now, a series

00:01:47.439 --> 00:01:49.560
of items like this is called an array.

00:01:49.560 --> 00:01:52.710
Let’s take a look at these numbers to help
see how we might sort this programmatically.

00:01:52.710 --> 00:01:55.140
We’ll start with a simple algorithm.

00:01:55.140 --> 00:01:58.210
First, let’s scan down the array to find
the smallest number.

00:01:58.210 --> 00:01:59.520
Starting at the top with 307.

00:01:59.520 --> 00:02:03.070
It’s the only number we’ve seen, so it’s
also the smallest.

00:02:03.070 --> 00:02:09.250
The next is 239, that’s smaller than 307,
so it becomes our new smallest number.

00:02:09.250 --> 00:02:11.950
Next is 214, our new smallest number.

00:02:11.950 --> 00:02:19.300
250 is not, neither is 384, 299, 223 or 312.

00:02:19.300 --> 00:02:22.940
So we’ve finished scanning all numbers,
and 214 is the smallest.

00:02:22.940 --> 00:02:27.520
To put this into ascending order, we swap
214 with the number in the top location.

00:02:27.530 --> 00:02:28.530
Great.

00:02:28.530 --> 00:02:29.580
We sorted one number!

00:02:29.580 --> 00:02:33.310
Now we repeat the same procedure, but instead
of starting at the top, we can start one spot

00:02:33.310 --> 00:02:34.040
below.

00:02:34.040 --> 00:02:38.040
First we see 239, which we save as our new
smallest number.

00:02:38.040 --> 00:02:43.280
Scanning the rest of the array, we find 223
is the next smallest, so we swap this with

00:02:43.280 --> 00:02:44.810
the number in the second spot.

00:02:44.810 --> 00:02:47.880
Now we repeat again, starting from the third
number down.

00:02:47.880 --> 00:02:51.200
This time, we swap 239 with 307.

00:02:51.200 --> 00:02:55.831
This process continues until we get to the
very last number, and voila, the array is

00:02:55.831 --> 00:02:58.230
sorted and you’re ready to book that flight
to Indianapolis!

00:02:58.230 --> 00:03:02.320
The process we just walked through is one
way – or one algorithm – for sorting an

00:03:02.320 --> 00:03:03.320
array.

00:03:03.320 --> 00:03:05.250
It’s called Selection Sort -- and it’s
pretty basic.

00:03:05.250 --> 00:03:06.990
Here’s the pseudo-code.

00:03:06.990 --> 00:03:11.550
This function can be used to sort 8, 80, or
80 million numbers - and once you've written

00:03:11.550 --> 00:03:14.150
the function, you can use it over and over
again.

00:03:14.150 --> 00:03:17.900
With this sort algorithm, we loop through
each position in the array, from top to bottom,

00:03:17.900 --> 00:03:21.300
and then for each of those positions, we have
to loop through the array to find the smallest

00:03:21.300 --> 00:03:22.400
number to swap.

00:03:22.400 --> 00:03:26.540
You can see this in the code, where one FOR
loop is nested inside of another FOR loop.

00:03:26.540 --> 00:03:30.950
This means, very roughly, that if we want
to sort N items, we have to loop N times,

00:03:30.950 --> 00:03:35.610
inside of which, we loop N times, for a grand
total of roughly N times N loops...

00:03:35.610 --> 00:03:36.610
Or N squared.

00:03:36.610 --> 00:03:41.090
This relationship of input size to the number
of steps the algorithm takes to run characterizes

00:03:41.090 --> 00:03:44.130
the complexity of the Selection Sort algorithm.

00:03:44.130 --> 00:03:48.400
It gives you an approximation of how fast,
or slow, an algorithm is going to be.

00:03:48.400 --> 00:03:52.900
Computer Scientists write this order of growth
in something known as – no joke – “big

00:03:52.910 --> 00:03:53.900
O notation”.

00:03:53.910 --> 00:03:56.010
N squared is not particularly efficient.

00:03:56.010 --> 00:04:00.440
Our example array had n = 8 items, and 8 squared
is 64.

00:04:00.440 --> 00:04:05.520
If we increase the size of our array from
8 items to 80, the running time is now 80

00:04:05.520 --> 00:04:07.870
squared, which is 6,400.

00:04:07.870 --> 00:04:11.860
So although our array only grew by 10 times
- from 8 to 80 – the running time increased

00:04:11.860 --> 00:04:16.110
by 100 times – from 64 to 6,400!

00:04:16.110 --> 00:04:18.489
This effect magnifies as the array gets larger.

00:04:18.489 --> 00:04:22.610
That’s a big problem for a company like
Google, which has to sort arrays with millions

00:04:22.610 --> 00:04:23.960
or billions of entries.

00:04:23.960 --> 00:04:28.760
So, you might ask, as a burgeoning computer scientist, is there a more efficient sorting algorithm?

00:04:28.780 --> 00:04:33.400
Let’s go back to our old, unsorted array
and try a different algorithm, merge sort.

00:04:33.400 --> 00:04:36.860
The first thing merge sort does is check if
the size of the array is greater than 1.

00:04:36.870 --> 00:04:39.539
If it is, it splits the array into two halves.

00:04:39.540 --> 00:04:43.080
Since our array is size 8, it gets split into
two arrays of size 4.

00:04:43.080 --> 00:04:47.560
These are still bigger than size 1, so they
get split again, into arrays of size 2, and

00:04:47.569 --> 00:04:50.540
finally they split into 8 arrays with 1 item
in each.

00:04:50.540 --> 00:04:53.560
Now we are ready to merge, which is how “merge
sort” gets its name.

00:04:53.560 --> 00:04:57.020
Starting with the first two arrays, we read
the first – and only – value in them,

00:04:57.020 --> 00:05:00.100
in this case, 307 and 239.

00:05:00.100 --> 00:05:03.500
239 is smaller, so we take that value first.

00:05:03.510 --> 00:05:07.120
The only number left is 307, so we put that
value second.

00:05:07.130 --> 00:05:09.759
We’ve successfully merged two arrays.

00:05:09.759 --> 00:05:13.610
We now repeat this process for the remaining
pairs, putting them each in sorted order.

00:05:13.610 --> 00:05:15.380
Then the merge process repeats.

00:05:15.380 --> 00:05:19.229
Again, we take the first two arrays, and we
compare the first numbers in them.

00:05:19.229 --> 00:05:22.340
This time its 239 and 214.

00:05:22.340 --> 00:05:25.759
214 is lowest, so we take that number first.

00:05:25.760 --> 00:05:31.180
Now we look again at the first two numbers
in both arrays: 239 and 250.

00:05:31.180 --> 00:05:34.360
239 is lower, so we take that number next.

00:05:34.370 --> 00:05:37.780
Now we look at the next two numbers: 307 and
250.

00:05:37.780 --> 00:05:40.280
250 is lower, so we take that.

00:05:40.280 --> 00:05:44.460
Finally, we’re left with just 307, so that
gets added last.

00:05:44.460 --> 00:05:48.909
In every case, we start with two arrays, each
individually sorted, and merge them into a

00:05:48.909 --> 00:05:50.310
larger sorted array.

00:05:50.310 --> 00:05:54.470
We repeat the exact same merging process for
the two remaining arrays of size two.

00:05:54.470 --> 00:05:56.949
Now we have two sorted arrays of size 4.

00:05:56.949 --> 00:06:00.490
Just as before, we merge, comparing the first
two numbers in each array, and taking the

00:06:00.490 --> 00:06:01.490
lowest.

00:06:01.490 --> 00:06:05.220
We repeat this until all the numbers are merged,
and then our array is fully sorted again!

00:06:05.220 --> 00:06:08.960
The bad news is: no matter how many times
we sort these, you’re still going to have

00:06:08.960 --> 00:06:11.939
to pay $214 to get to Indianapolis.

00:06:11.939 --> 00:06:16.439
Anyway, the “Big O” computational complexity
of merge sort is N times the Log of N.

00:06:16.439 --> 00:06:20.340
The N comes from the number of times we need
to compare and merge items, which is directly

00:06:20.340 --> 00:06:22.819
proportional to the number of items in the
array.

00:06:22.819 --> 00:06:25.250
The Log N comes from the number of merge steps.

00:06:25.250 --> 00:06:29.810
In our example, we broke our array of 8 items
into 4, then 2, and finally 1.

00:06:29.810 --> 00:06:31.120
That’s 3 splits.

00:06:31.120 --> 00:06:34.420
Splitting in half repeatedly like this has
a logarithmic relationship with the number

00:06:34.420 --> 00:06:36.270
of items - trust me!

00:06:36.270 --> 00:06:38.340
Log base 2 of 8 equals 3 splits.

00:06:38.340 --> 00:06:42.969
If we double the size of our array to 16 – that's
twice as many items to sort – it only increases

00:06:42.969 --> 00:06:46.960
the number of split steps by 1 since log base
2 of 16 equals 4.

00:06:46.960 --> 00:06:50.379
Even if we increase the size of the array
more than a thousand times, from 8 items to

00:06:50.379 --> 00:06:54.360
8000 items, the number of split steps stays
pretty low.

00:06:54.360 --> 00:06:56.660
Log base 2 of 8000 is roughly 13.

00:06:56.660 --> 00:07:01.340
That's more, but not much more than 3 -- about
four times larger – and yet we’re sorting

00:07:01.340 --> 00:07:02.529
a lot more numbers.

00:07:02.529 --> 00:07:06.430
For this reason, merge sort is much more efficient
than selection sort.

00:07:06.430 --> 00:07:10.129
And now I can put my ceramic cat collection
in name order MUCH faster!

00:07:10.129 --> 00:07:14.190
There are literally dozens of sorting algorithms
we could review, but instead, I want to move

00:07:14.190 --> 00:07:18.169
on to my other favorite category of classic
algorithmic problems: graph search!

00:07:18.169 --> 00:07:20.840
A graph is a network of nodes connected by
lines.

00:07:20.840 --> 00:07:23.680
You can think of it like a map, with cities
and roads connecting them.

00:07:23.680 --> 00:07:26.360
Routes between these cities take different
amounts of time.

00:07:26.360 --> 00:07:29.620
We can label each line with what is called
a cost or weight.

00:07:29.620 --> 00:07:31.509
In this case, it’s weeks of travel.

00:07:31.509 --> 00:07:35.150
Now let’s say we want to find the fastest
route for an army at Highgarden to reach the

00:07:35.150 --> 00:07:36.419
castle at Winterfell.

00:07:36.420 --> 00:07:41.009
The simplest approach would just be to try
every single path exhaustively and calculate

00:07:41.009 --> 00:07:42.340
the total cost of each.

00:07:42.460 --> 00:07:43.860
That’s a brute force approach.

00:07:43.870 --> 00:07:47.999
We could have used a brute force approach
in sorting, by systematically trying every

00:07:47.999 --> 00:07:50.979
permutation of the array to check if it’s
sorted.

00:07:50.979 --> 00:07:56.120
This would have an N factorial complexity
- that is the number of nodes, times one less,

00:07:56.120 --> 00:07:58.759
times one less than that, and so on until
1.

00:07:58.759 --> 00:08:00.990
Which is way worse than even N squared.

00:08:00.990 --> 00:08:03.280
But, we can be way more clever!

00:08:03.280 --> 00:08:06.930
The classic algorithmic solution to this graph
problem was invented by one of the greatest

00:08:06.930 --> 00:08:11.569
minds in computer science practice and theory,
Edsger Dijkstra, so it’s appropriately named

00:08:11.569 --> 00:08:12.710
Dijkstra's algorithm.

00:08:12.710 --> 00:08:16.210
We start in Highgarden with a cost of 0, which
we mark inside the node.

00:08:16.210 --> 00:08:20.240
For now, we mark all other cities with question
marks - we don’t know the cost of getting

00:08:20.240 --> 00:08:21.240
to them yet.

00:08:21.240 --> 00:08:24.349
Dijkstra's algorithm always starts with the
node with lowest cost.

00:08:24.349 --> 00:08:28.439
In this case, it only knows about one node,
Highgarden, so it starts there.

00:08:28.439 --> 00:08:32.660
It follows all paths from that node to all
connecting nodes that are one step away, and

00:08:32.660 --> 00:08:34.680
records the cost to get to each of them.

00:08:34.680 --> 00:08:36.770
That completes one round of the algorithm.

00:08:36.770 --> 00:08:40.450
We haven’t encountered Winterfell yet, so
we loop and run Dijkstra's algorithm again.

00:08:40.450 --> 00:08:44.260
With Highgarden already checked, the next
lowest cost node is King's Landing.

00:08:44.260 --> 00:08:47.920
Just as before, we follow every unvisited
line to any connecting cities.

00:08:47.920 --> 00:08:50.240
The line to The Trident has a cost of 5.

00:08:50.240 --> 00:08:54.490
However, we want to keep a running cost from
Highgarden, so the total cost of getting to

00:08:54.490 --> 00:08:57.360
The Trident is 8 plus 5, which is 13 weeks.

00:08:57.360 --> 00:09:02.520
Now we follow the offroad path to Riverrun,
which has a high cost of 25, for a total of 33.

00:09:02.600 --> 00:09:06.260
But we can see inside of Riverrun that we’ve
already found a path with a lower cost of

00:09:06.270 --> 00:09:07.270
just 10.

00:09:07.270 --> 00:09:10.520
So we disregard our new path, and stick with
the previous, better path.

00:09:10.520 --> 00:09:14.960
We’ve now explored every line from King's
Landing and didn’t find Winterfell, so we

00:09:14.960 --> 00:09:15.960
move on.

00:09:15.960 --> 00:09:18.680
The next lowest cost node is Riverrun, at
10 weeks.

00:09:18.680 --> 00:09:23.000
First we check the path to The Trident, which
has a total cost of 10 plus 2, or 12.

00:09:23.000 --> 00:09:27.011
That’s slightly better than the previous
path we found, which had a cost of 13, so

00:09:27.011 --> 00:09:29.090
we update the path and cost to The Trident.

00:09:29.090 --> 00:09:31.800
There is also a line from Riverrun to Pyke
with a cost of 3.

00:09:31.800 --> 00:09:36.960
10 plus 3 is 13, which beats the previous
cost of 14, and so we update Pyke's path and

00:09:36.960 --> 00:09:37.960
cost as well.

00:09:37.960 --> 00:09:41.751
That’s all paths from Riverrun checked...
so... you guessed it, Dijkstra's algorithm

00:09:41.751 --> 00:09:42.751
loops again.

00:09:42.751 --> 00:09:46.360
The node with the next lowest cost is The
Trident and the only line from The Trident

00:09:46.360 --> 00:09:49.190
that we haven’t checked is a path to Winterfell!

00:09:49.190 --> 00:09:53.550
It has a cost of 10, plus we need to add in
the cost of 12 it takes to get to The Trident,

00:09:53.550 --> 00:09:55.680
for a grand total cost of 22.

00:09:55.680 --> 00:09:59.510
We check our last path, from Pyke to Winterfell,
which sums to 31.

00:09:59.510 --> 00:10:03.900
Now we know the lowest total cost, and also
the fastest route for the army to get there,

00:10:03.900 --> 00:10:05.340
which avoids King’s Landing!

00:10:05.340 --> 00:10:09.860
Dijkstra's original algorithm, conceived in
1956, had a complexity of the number of nodes

00:10:09.860 --> 00:10:10.860
in the graph squared.

00:10:10.860 --> 00:10:14.550
And squared, as we already discussed, is never
great, because it means the algorithm can’t

00:10:14.550 --> 00:10:18.360
scale to big problems - like the entire road
map of the United States.

00:10:18.360 --> 00:10:22.500
Fortunately, Dijkstra's algorithm was improved
a few years later to take the number of nodes

00:10:22.500 --> 00:10:26.700
in the graph, times the log of the number
of nodes, PLUS the number of lines.

00:10:26.700 --> 00:10:30.140
Although this looks more complicated, it’s
actually quite a bit faster.

00:10:30.140 --> 00:10:33.520
Plugging in our example graph, with 6 cities
and 9 lines, proves it.

00:10:33.520 --> 00:10:36.560
Our algorithm drops from 36 loops to around 14.

00:10:36.560 --> 00:10:41.680
As with sorting, there are innumerable graph search algorithms, with different pros and cons.

00:10:41.680 --> 00:10:45.520
Every time you use a service like Google Maps
to find directions, an algorithm much like

00:10:45.520 --> 00:10:48.840
Dijkstra's is running on servers to figure
out the best route for you.

00:10:48.840 --> 00:10:52.760
Algorithms are everywhere and the modern world
would not be possible without them.

00:10:52.760 --> 00:10:57.150
We touched only the very tip of the algorithmic
iceberg in this episode, but a central part

00:10:57.150 --> 00:11:01.380
of being a computer scientist is leveraging
existing algorithms and writing new ones when

00:11:01.380 --> 00:11:05.420
needed, and I hope this little taste has intrigued
you to SEARCH further.

00:11:05.420 --> 00:11:06.660
I’ll see you next week.

