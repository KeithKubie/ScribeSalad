WEBVTT
Kind: captions
Language: en

00:00:01.512 --> 00:00:02.720
CLEMENS MEWALD: Hi, everyone.

00:00:02.720 --> 00:00:03.520
My name is Clemens.

00:00:03.520 --> 00:00:06.520
I'm a product manager
in Google Research.

00:00:06.520 --> 00:00:08.830
And today I'm going to talk
about TensorFlow Extended,

00:00:08.830 --> 00:00:10.913
which is a machine learning
platform that we built

00:00:10.913 --> 00:00:14.620
around TensorFlow at Google.

00:00:14.620 --> 00:00:17.050
And I'd like to start
this talk with a block

00:00:17.050 --> 00:00:22.150
diagram and the small
yellow box, or orange box.

00:00:22.150 --> 00:00:23.680
And that box
basically represents

00:00:23.680 --> 00:00:25.990
what most people care about
and talk about when they

00:00:25.990 --> 00:00:27.410
talk about machine learning.

00:00:27.410 --> 00:00:28.570
It's the machine
learning algorithm.

00:00:28.570 --> 00:00:29.945
It's the structure
of the network

00:00:29.945 --> 00:00:33.040
that you're training,
how you choose

00:00:33.040 --> 00:00:35.370
what type of machine learning
problem you're solving.

00:00:35.370 --> 00:00:37.911
And that's what you talk about
when you talk about TensorFlow

00:00:37.911 --> 00:00:38.890
and using TensorFlow.

00:00:38.890 --> 00:00:42.910
However, in addition to the
actual machine learning,

00:00:42.910 --> 00:00:44.470
and to TensorFlow
itself, you have

00:00:44.470 --> 00:00:46.625
to care about so much more.

00:00:46.625 --> 00:00:48.250
And these are all of
these other things

00:00:48.250 --> 00:00:50.590
around the actual machine
learning algorithm

00:00:50.590 --> 00:00:53.080
that you have to have in
place, and that you actually

00:00:53.080 --> 00:00:56.500
have to nail and
get right in order

00:00:56.500 --> 00:00:59.009
to actually do machine learning
in a production setting.

00:00:59.009 --> 00:01:01.300
So you have to care about
where you get your data from,

00:01:01.300 --> 00:01:03.832
that your data are clean,
how you transform them,

00:01:03.832 --> 00:01:06.040
how you train your model,
how to validate your model,

00:01:06.040 --> 00:01:07.914
how to push it out into
a production setting,

00:01:07.914 --> 00:01:10.760
and deploy it at scale.

00:01:10.760 --> 00:01:12.980
Now, some of you may
be thinking, well,

00:01:12.980 --> 00:01:14.714
I don't really need all of this.

00:01:14.714 --> 00:01:16.630
I only have my small
machine learning problem.

00:01:16.630 --> 00:01:19.750
I can live within
that small orange box.

00:01:19.750 --> 00:01:22.660
And I don't really have these
production worries as of today.

00:01:22.660 --> 00:01:25.210
But I'm going to
propose that all of you

00:01:25.210 --> 00:01:27.670
will have that problem
at some point in time.

00:01:27.670 --> 00:01:29.170
Because what I've
seen time and time

00:01:29.170 --> 00:01:32.320
again is that research
and experimentation

00:01:32.320 --> 00:01:34.987
today is production tomorrow.

00:01:34.987 --> 00:01:36.820
It's like research and
experimentation never

00:01:36.820 --> 00:01:38.680
ends just there.

00:01:38.680 --> 00:01:40.570
Eventually it will become
a production model.

00:01:40.570 --> 00:01:42.820
And at that point, you
actually have to care about all

00:01:42.820 --> 00:01:44.490
of these things.

00:01:44.490 --> 00:01:47.060
Another side of
this coin is scale.

00:01:47.060 --> 00:01:48.649
So some of you may
say, well, I do

00:01:48.649 --> 00:01:51.190
all of my machine learning on
a local machine, in a notebook.

00:01:51.190 --> 00:01:53.500
Everything fits into memory.

00:01:53.500 --> 00:01:57.730
I don't need all of these
heavy tools to get started.

00:01:57.730 --> 00:02:03.190
But similarly, small scale
today is large scale tomorrow.

00:02:03.190 --> 00:02:05.210
At Google we have this
problem all the time.

00:02:05.210 --> 00:02:08.014
That's why we always design
for scale from day one,

00:02:08.014 --> 00:02:10.180
because we always have
product teams that say, well,

00:02:10.180 --> 00:02:11.730
we have only a small
amount of data.

00:02:11.730 --> 00:02:12.610
It's fine.

00:02:12.610 --> 00:02:14.770
But then a week later
the product picks up.

00:02:14.770 --> 00:02:16.811
And suddenly they need to
distribute the workload

00:02:16.811 --> 00:02:18.070
to hundreds of machines.

00:02:18.070 --> 00:02:20.290
And then they have
all of these concerns.

00:02:22.900 --> 00:02:26.940
Now, the good news is that
we built something for this.

00:02:26.940 --> 00:02:29.380
And TFX is the solution
to this problem.

00:02:29.380 --> 00:02:31.860
So this is a block
diagram that we published

00:02:31.860 --> 00:02:35.010
in one of our papers that
is a very simplistic view

00:02:35.010 --> 00:02:36.720
of the platform.

00:02:36.720 --> 00:02:39.180
But it gives you a
broad sense of what

00:02:39.180 --> 00:02:41.400
the different components are.

00:02:41.400 --> 00:02:43.020
Now, TFX is a very
large platform.

00:02:43.020 --> 00:02:44.910
And it contains a
lot of components

00:02:44.910 --> 00:02:46.570
and a lot of services.

00:02:46.570 --> 00:02:48.780
So the paper that we
published, and also

00:02:48.780 --> 00:02:53.010
what I'm going to discuss today,
is only a small subset of this.

00:02:53.010 --> 00:02:55.980
But building TFX and
deploying it at Google

00:02:55.980 --> 00:02:59.880
has had a profound impact of
how fast product teams at Google

00:02:59.880 --> 00:03:01.740
can train machine
learning models

00:03:01.740 --> 00:03:05.400
and deploy them in production,
and how ubiquitous machine

00:03:05.400 --> 00:03:07.620
learning has become at Google.

00:03:07.620 --> 00:03:10.470
You'll see later I have a slide
to give you some sense of how

00:03:10.470 --> 00:03:13.110
widely TFX is being used.

00:03:13.110 --> 00:03:17.130
And it really has accelerated
all of our efforts

00:03:17.130 --> 00:03:19.740
to being an AI first company
and using machine learning

00:03:19.740 --> 00:03:20.700
in all of our products.

00:03:25.590 --> 00:03:29.630
Now, we use TFX
broadly at Google.

00:03:29.630 --> 00:03:31.360
And we are very
committed to make

00:03:31.360 --> 00:03:34.450
all of this available to you
through open sourcing it.

00:03:34.450 --> 00:03:36.565
So the boxes that are
just highlighted in blue

00:03:36.565 --> 00:03:40.860
are the components that
we've already open sourced.

00:03:40.860 --> 00:03:43.340
Now, I want to highlight
an important thing.

00:03:43.340 --> 00:03:46.600
TFX is a real solution
for real problems.

00:03:46.600 --> 00:03:50.800
Sometimes people ask me, well,
is this the same code that you

00:03:50.800 --> 00:03:52.570
use at Google for production?

00:03:52.570 --> 00:03:56.470
Or did you just build something
on the side and open source it?

00:03:56.470 --> 00:03:59.860
And all of these components
are the same code base

00:03:59.860 --> 00:04:02.902
that we use internally for
our production pipelines.

00:04:02.902 --> 00:04:04.360
Of course, there's
some things that

00:04:04.360 --> 00:04:06.850
are Google specific
for our deployments.

00:04:06.850 --> 00:04:08.896
But all of the code
that we open source

00:04:08.896 --> 00:04:10.270
is the same code
that we actually

00:04:10.270 --> 00:04:12.440
run in our production systems.

00:04:12.440 --> 00:04:16.730
So it's really code that solves
real problems for Google.

00:04:16.730 --> 00:04:19.600
The second part to
highlight is so far

00:04:19.600 --> 00:04:21.250
we've only open
sourced libraries, so

00:04:21.250 --> 00:04:24.430
each one of these
libraries that you can use.

00:04:24.430 --> 00:04:26.956
But you still have to
glue them together.

00:04:26.956 --> 00:04:28.330
You still have to
write some code

00:04:28.330 --> 00:04:30.940
to make them work
in a joint manner.

00:04:30.940 --> 00:04:32.440
That's just because
we haven't open

00:04:32.440 --> 00:04:34.230
sourced the full platform yet.

00:04:34.230 --> 00:04:35.830
We're actively working on this.

00:04:35.830 --> 00:04:40.679
But I would say so far
we're about 50% there.

00:04:40.679 --> 00:04:42.220
So these blue
components are the ones

00:04:42.220 --> 00:04:44.565
that I'm going to
talk about today.

00:04:44.565 --> 00:04:46.690
But first, let me talk
about some of the principles

00:04:46.690 --> 00:04:48.520
that we followed when
we developed TFX.

00:04:48.520 --> 00:04:51.610
Because I think it's
very informative

00:04:51.610 --> 00:04:53.650
to see how we think
about these platforms,

00:04:53.650 --> 00:04:57.950
and how we think about
having impact at Google.

00:04:57.950 --> 00:05:00.359
The first principle
is flexibility.

00:05:00.359 --> 00:05:01.900
And there's some
history behind this.

00:05:01.900 --> 00:05:03.441
And the short version
of that history

00:05:03.441 --> 00:05:06.280
is that I'm sure at other
companies as well there used

00:05:06.280 --> 00:05:10.540
to be problem specific
machine learning platforms.

00:05:10.540 --> 00:05:14.290
And just to be concrete,
so we had a platform

00:05:14.290 --> 00:05:17.039
that was specifically built
for large scale linear models.

00:05:17.039 --> 00:05:18.580
So if you had a
linear model that you

00:05:18.580 --> 00:05:20.290
wanted to train at
large scale, you

00:05:20.290 --> 00:05:21.930
used this piece
of infrastructure.

00:05:21.930 --> 00:05:23.680
We had a different
piece of infrastructure

00:05:23.680 --> 00:05:26.770
for large scale neural networks.

00:05:26.770 --> 00:05:29.350
But product teams usually don't
have one kind of a problem.

00:05:29.350 --> 00:05:32.060
And they usually want to train
multiple types of models.

00:05:32.060 --> 00:05:34.268
So if they wanted to train
linear [INAUDIBLE] models,

00:05:34.268 --> 00:05:37.450
they had to use two entirely
different technology stacks.

00:05:37.450 --> 00:05:39.940
Now, with TensorFlow,
as I'm sure you know,

00:05:39.940 --> 00:05:43.216
we can actually express any kind
of machine learning algorithm.

00:05:43.216 --> 00:05:44.590
So we can train
TensorFlow models

00:05:44.590 --> 00:05:48.100
that are linear, that are deep,
unsupervised and supervised.

00:05:48.100 --> 00:05:49.690
We can train tree models.

00:05:49.690 --> 00:05:52.270
And any single algorithm
that you can think of either

00:05:52.270 --> 00:05:54.340
has already been
implemented in TensorFlow,

00:05:54.340 --> 00:05:56.886
or is possible to be
implemented in TensorFlow.

00:05:56.886 --> 00:05:58.510
So building on top
of that flexibility,

00:05:58.510 --> 00:06:00.400
we have one platform
that supports

00:06:00.400 --> 00:06:02.860
all of these different use
cases from all of our users.

00:06:02.860 --> 00:06:05.410
And they don't have to
switch between platforms just

00:06:05.410 --> 00:06:09.550
because they want to implement
different types of algorithms.

00:06:09.550 --> 00:06:12.760
Another aspect of this
is the input data.

00:06:12.760 --> 00:06:17.110
Of course, also product teams
don't only have image data,

00:06:17.110 --> 00:06:19.030
or only have text data.

00:06:19.030 --> 00:06:20.800
In some cases, they
may even have both.

00:06:20.800 --> 00:06:21.300
Right.

00:06:21.300 --> 00:06:24.340
So they have models that
take in both images and text,

00:06:24.340 --> 00:06:25.760
and make a prediction.

00:06:25.760 --> 00:06:28.150
So we needed to make sure that
the platform that we built

00:06:28.150 --> 00:06:30.250
supports all of these
input modalities,

00:06:30.250 --> 00:06:33.490
and can deal with
images, text, sparse data

00:06:33.490 --> 00:06:37.000
that you will find
in logs, videos even.

00:06:37.000 --> 00:06:39.640
And with a platform
as flexible as this,

00:06:39.640 --> 00:06:41.140
you can ensure that
all of the users

00:06:41.140 --> 00:06:44.320
can represent all of their use
cases on the same platform,

00:06:44.320 --> 00:06:47.854
and don't have to adopt
different technologies.

00:06:47.854 --> 00:06:49.270
The next aspects
of flexibility is

00:06:49.270 --> 00:06:50.770
how you actually
run these pipelines

00:06:50.770 --> 00:06:52.469
and how you train models.

00:06:52.469 --> 00:06:54.760
So one very basic use case
is you have all of your data

00:06:54.760 --> 00:06:55.480
available.

00:06:55.480 --> 00:06:58.210
You train your model
once, and you're done.

00:06:58.210 --> 00:07:01.400
This works really well
for stationary problems.

00:07:01.400 --> 00:07:02.860
A good example is
always, you want

00:07:02.860 --> 00:07:07.540
to train a model that classifies
an image whether there's

00:07:07.540 --> 00:07:09.682
a cat or a dog in that image.

00:07:09.682 --> 00:07:11.890
Cats and dogs have looked
the same for quite a while.

00:07:11.890 --> 00:07:13.810
And they will look
the same in 10 years,

00:07:13.810 --> 00:07:15.650
or very much the same as today.

00:07:15.650 --> 00:07:20.030
So that same model will probably
work well in a couple of years.

00:07:20.030 --> 00:07:22.090
So you don't need to
keep that model fresh.

00:07:22.090 --> 00:07:25.180
However, if you have a non
stationary problem where

00:07:25.180 --> 00:07:27.381
data changes over time,
recommendation systems

00:07:27.381 --> 00:07:29.630
have new types of products
that you want to recommend,

00:07:29.630 --> 00:07:32.140
new types of videos that get
uploaded all the time, you

00:07:32.140 --> 00:07:35.080
actually have to retrain these
models, or keep them fresh.

00:07:35.080 --> 00:07:37.090
So one way of doing
this is to train a model

00:07:37.090 --> 00:07:38.929
on a subset of your data.

00:07:38.929 --> 00:07:40.720
Once you get new data,
you throw that away.

00:07:40.720 --> 00:07:44.050
You train a new model either
on the superset, so on the old

00:07:44.050 --> 00:07:49.190
and on the new data, or only
on the fresh data, and so on.

00:07:49.190 --> 00:07:51.830
Now, that has a couple
of disadvantages.

00:07:51.830 --> 00:07:53.410
One of them being
that you throw away

00:07:53.410 --> 00:07:55.510
learning from previous models.

00:07:55.510 --> 00:07:57.630
In some cases, you're
wasting resources,

00:07:57.630 --> 00:08:00.130
because you actually have to
retrain over the same data over

00:08:00.130 --> 00:08:01.426
and over again.

00:08:01.426 --> 00:08:02.800
And because a lot
of these models

00:08:02.800 --> 00:08:04.480
are actually not
deterministic, you

00:08:04.480 --> 00:08:06.902
may end up with vastly
different models every time.

00:08:06.902 --> 00:08:08.860
Because the way that
they're being initialized,

00:08:08.860 --> 00:08:10.970
you may end up in
different optimum

00:08:10.970 --> 00:08:12.700
every time you
train these models.

00:08:12.700 --> 00:08:14.200
So a more advanced
way of doing this

00:08:14.200 --> 00:08:16.960
is to start training
with your data.

00:08:16.960 --> 00:08:20.770
And then initialize your model
from the previous weights

00:08:20.770 --> 00:08:22.840
from these models and
continue training.

00:08:22.840 --> 00:08:27.672
So we call that warm starting
of models that may seem trivial

00:08:27.672 --> 00:08:29.130
if you just say,
well, this is just

00:08:29.130 --> 00:08:30.630
a continuation of
your training run.

00:08:30.630 --> 00:08:32.822
You just added more
data and you continue.

00:08:32.822 --> 00:08:34.530
But depending on your
model architecture,

00:08:34.530 --> 00:08:35.879
it's actually non-trivial.

00:08:35.879 --> 00:08:37.289
Which in some
cases, you may only

00:08:37.289 --> 00:08:39.267
want to warm start embeddings.

00:08:39.267 --> 00:08:41.850
So you may only want to transfer
the weights of the embeddings

00:08:41.850 --> 00:08:45.660
to a new model and initialize
the rest of your network

00:08:45.660 --> 00:08:46.411
randomly.

00:08:46.411 --> 00:08:47.910
So there's a lot
of different setups

00:08:47.910 --> 00:08:49.830
that you can achieve with this.

00:08:49.830 --> 00:08:51.510
But with this you
can continuously

00:08:51.510 --> 00:08:52.720
update your models.

00:08:52.720 --> 00:08:55.140
You retain the learning
from previous versions.

00:08:55.140 --> 00:08:57.070
You can even, depending
on how you set it up,

00:08:57.070 --> 00:08:59.100
bias your model more on
the more recent data.

00:08:59.100 --> 00:09:02.970
But you're still not
throwing away the old data.

00:09:02.970 --> 00:09:07.470
And always have a fresh model
that's updated for production.

00:09:07.470 --> 00:09:10.044
The second principle
is portability.

00:09:10.044 --> 00:09:11.460
And there's a few
aspects to this.

00:09:11.460 --> 00:09:12.501
The first one is obvious.

00:09:12.501 --> 00:09:14.430
So because we rely
on TensorFlow,

00:09:14.430 --> 00:09:16.589
we inherit the
properties of TensorFlow,

00:09:16.589 --> 00:09:18.630
which means you can already
train your TensorFlow

00:09:18.630 --> 00:09:21.400
models in different environments
and on different machines.

00:09:21.400 --> 00:09:23.730
So you can train a
TensorFlow model locally.

00:09:23.730 --> 00:09:26.480
You can distribute it
in a cloud environment.

00:09:26.480 --> 00:09:29.370
And by cloud, I mean any
setup of multiple clusters.

00:09:29.370 --> 00:09:32.040
It doesn't have to
be a managed cloud.

00:09:32.040 --> 00:09:34.560
You can train or perform
inferences with your TensorFlow

00:09:34.560 --> 00:09:37.990
models on the devices
that you care about today.

00:09:37.990 --> 00:09:40.917
And you can also train
and deploy them on devices

00:09:40.917 --> 00:09:42.500
that you may care
about in the future.

00:09:45.130 --> 00:09:47.880
Next is Apache Beam.

00:09:47.880 --> 00:09:50.460
So when we open sourced
a lot of our components

00:09:50.460 --> 00:09:52.590
we faced the challenge
that internally we

00:09:52.590 --> 00:09:54.564
use a data processing
engine that

00:09:54.564 --> 00:09:55.980
allows us to run
these large scale

00:09:55.980 --> 00:09:58.060
data processing pipelines.

00:09:58.060 --> 00:10:00.720
But in the open source world
and in all of your companies,

00:10:00.720 --> 00:10:03.789
you may use different
data processing systems.

00:10:03.789 --> 00:10:05.580
So we were looking for
a portability layer.

00:10:05.580 --> 00:10:09.380
And Apache beam provides us
with that portability layer.

00:10:09.380 --> 00:10:13.627
It allows us to express a data
graph once with the Python SDK.

00:10:13.627 --> 00:10:15.210
And then you can use
different runners

00:10:15.210 --> 00:10:18.905
to run those same data graphs
in different environments.

00:10:18.905 --> 00:10:20.280
The first one is
a direct runner.

00:10:20.280 --> 00:10:22.350
So that allows you to
run these data graphs

00:10:22.350 --> 00:10:23.950
on a single machine.

00:10:23.950 --> 00:10:26.580
There's also the one that's
being used in notebooks.

00:10:26.580 --> 00:10:28.615
So I'll come back to
that later, but we

00:10:28.615 --> 00:10:30.240
want to make sure
that all of our tools

00:10:30.240 --> 00:10:32.820
work in notebook environments,
because we know that that's

00:10:32.820 --> 00:10:35.490
where data scientists start.

00:10:35.490 --> 00:10:37.290
Then there's a data
flow runner, with which

00:10:37.290 --> 00:10:41.010
you can run these same
pipelines at scale

00:10:41.010 --> 00:10:43.395
on the cloud's
dataflow in this case.

00:10:43.395 --> 00:10:45.270
There's a Flink runner
that's being developed

00:10:45.270 --> 00:10:46.920
right now by the community.

00:10:46.920 --> 00:10:49.259
There's a [INAUDIBLE]
ticket that you can follow

00:10:49.259 --> 00:10:50.550
for the status updates on this.

00:10:50.550 --> 00:10:53.130
I'm being told it's going
to be ready at some point

00:10:53.130 --> 00:10:54.330
later this year.

00:10:54.330 --> 00:10:57.137
And the community is also
working on more runners

00:10:57.137 --> 00:10:59.220
so that these pipelines
are becoming more portable

00:10:59.220 --> 00:11:03.450
and can be run in more
different environments.

00:11:03.450 --> 00:11:08.252
In terms of cluster management
and managing your resources,

00:11:08.252 --> 00:11:10.710
we work very well together with
Kubernetes and the KubeFlow

00:11:10.710 --> 00:11:13.805
project, which actually is the
next talk right after mine.

00:11:13.805 --> 00:11:15.430
And if you're familiar
with Kubernetes,

00:11:15.430 --> 00:11:17.592
there's something
called Minikube,

00:11:17.592 --> 00:11:19.300
with which you can
deploy your Kubernetes

00:11:19.300 --> 00:11:22.240
setup on a single machine.

00:11:22.240 --> 00:11:24.450
Of course, there's managed
Kubernetes solutions

00:11:24.450 --> 00:11:26.610
such as GKE.

00:11:26.610 --> 00:11:28.240
You can run your own
Kubernetes cluster

00:11:28.240 --> 00:11:30.420
if you want to, on prem.

00:11:30.420 --> 00:11:34.140
And, again, we inherit
the portability aspects

00:11:34.140 --> 00:11:36.680
of Kubernetes.

00:11:36.680 --> 00:11:38.810
Another extremely important
aspect is scalability.

00:11:38.810 --> 00:11:40.520
And I've alluded to it before.

00:11:42.857 --> 00:11:44.440
I'm sure many of you
know the problem.

00:11:44.440 --> 00:11:46.520
There's different
roles in companies.

00:11:46.520 --> 00:11:50.810
And some very commonly, data
scientists work on-- sometimes

00:11:50.810 --> 00:11:53.600
it's down sampled set of
data on their local machines,

00:11:53.600 --> 00:11:57.020
maybe on their laptop, in
a notebook environment.

00:11:57.020 --> 00:12:00.250
And then there's data engineers
or product software engineers

00:12:00.250 --> 00:12:02.630
who actually either
take the models that

00:12:02.630 --> 00:12:04.010
were developed by
data scientists

00:12:04.010 --> 00:12:06.242
and deploy them in production.

00:12:06.242 --> 00:12:07.700
Or they're trying
to replicate what

00:12:07.700 --> 00:12:11.360
data scientists did with
different frameworks,

00:12:11.360 --> 00:12:14.620
because they work with
a different toolkit.

00:12:14.620 --> 00:12:16.760
And there's this almost
impenetrable wall

00:12:16.760 --> 00:12:17.510
between those two.

00:12:17.510 --> 00:12:20.236
Because they use
different toolsets.

00:12:20.236 --> 00:12:21.860
And there is a lot
of friction in terms

00:12:21.860 --> 00:12:24.110
of translating from one
toolset to the other,

00:12:24.110 --> 00:12:26.630
or actually deploying these
things from the data science

00:12:26.630 --> 00:12:28.520
process to the
production process.

00:12:28.520 --> 00:12:30.320
And if you've heard
the term, throw

00:12:30.320 --> 00:12:34.220
over the wall, that usually
does not have good connotations.

00:12:34.220 --> 00:12:37.190
But that's exactly
what's happening.

00:12:37.190 --> 00:12:40.220
So when we built TFX we
paid particular attention

00:12:40.220 --> 00:12:42.440
to make sure that all
of the toolsets we build

00:12:42.440 --> 00:12:44.480
are usable at a small scale.

00:12:44.480 --> 00:12:48.410
So you will see from my
demos, all of our tools

00:12:48.410 --> 00:12:50.200
work in a notebook environment.

00:12:50.200 --> 00:12:53.080
And they work on a single
machine with small datasets.

00:12:53.080 --> 00:12:56.360
And in many cases, or
actually in all cases,

00:12:56.360 --> 00:12:59.090
the same code that you
run on a single machine

00:12:59.090 --> 00:13:02.574
scales up to large workloads
in a distributed cluster.

00:13:02.574 --> 00:13:04.490
And the reason why this
is extremely important

00:13:04.490 --> 00:13:06.675
is there's no friction to
go from experimentation

00:13:06.675 --> 00:13:08.957
on a small machine
to a large cluster.

00:13:08.957 --> 00:13:11.540
And you can actually bring those
different functions together,

00:13:11.540 --> 00:13:14.690
and have data scientists and
data engineers work together

00:13:14.690 --> 00:13:16.497
with the same tools
on the same problems,

00:13:16.497 --> 00:13:18.080
and not have to dwell
in between them.

00:13:21.080 --> 00:13:22.970
The next principle
is interactivity.

00:13:22.970 --> 00:13:26.630
So the machine learning
process is not a straight line.

00:13:26.630 --> 00:13:28.430
At many points in this
process you actually

00:13:28.430 --> 00:13:31.130
have to interact with your
data, understand your data,

00:13:31.130 --> 00:13:32.790
and make changes.

00:13:32.790 --> 00:13:36.260
So this visualization
is called Facets.

00:13:36.260 --> 00:13:40.430
And it allows you to investigate
your data, and understand it.

00:13:40.430 --> 00:13:43.230
And, again, this works at scale.

00:13:43.230 --> 00:13:46.521
So sometimes when I
show these screenshots,

00:13:46.521 --> 00:13:48.020
they may seem trivial
when you think

00:13:48.020 --> 00:13:50.504
about small amounts of data
that fit into a single machine.

00:13:50.504 --> 00:13:51.920
But if you have
terabytes of data,

00:13:51.920 --> 00:13:55.970
and you want to understand
them, it's less trivial.

00:13:55.970 --> 00:13:56.935
And on the other side--

00:13:56.935 --> 00:13:59.060
I'm going to talk about
this in more detail later--

00:13:59.060 --> 00:14:01.476
this is a visualization we
have to actually understand how

00:14:01.476 --> 00:14:03.350
your models perform at scale.

00:14:03.350 --> 00:14:07.984
This is a screen capture from
TensorFlow Model Analysis.

00:14:07.984 --> 00:14:09.400
And by following
these principles,

00:14:09.400 --> 00:14:13.900
we've built a platform that has
had a profound impact on Google

00:14:13.900 --> 00:14:16.540
and the products that we build.

00:14:16.540 --> 00:14:20.415
And it's really being used
across many of our Alphabet

00:14:20.415 --> 00:14:20.915
companies.

00:14:20.915 --> 00:14:22.780
So Google, of course,
is only one company

00:14:22.780 --> 00:14:24.420
under the Alphabet umbrella.

00:14:24.420 --> 00:14:26.380
And within Google, all
of our major products

00:14:26.380 --> 00:14:30.970
are using TensorFlow Extended
to actually deploy machine

00:14:30.970 --> 00:14:33.830
learning in their products.

00:14:33.830 --> 00:14:36.620
So with this, let's look
at a quick overview.

00:14:36.620 --> 00:14:39.290
I'm going to take questions
later, if it's possible.

00:14:39.290 --> 00:14:41.230
Let's look at a quick
overview of the things

00:14:41.230 --> 00:14:42.700
that we've open sourced yet.

00:14:42.700 --> 00:14:46.510
So this is the familiar graph
that you've seen before.

00:14:46.510 --> 00:14:48.730
And I'm just going to turn
all of these boxes blue

00:14:48.730 --> 00:14:50.299
and talk about
each one of those.

00:14:50.299 --> 00:14:52.090
So data transformation
we have open sourced

00:14:52.090 --> 00:14:54.460
as TensorFlow Transform.

00:14:54.460 --> 00:14:58.150
TensorFlow Transform allows
you to express your data

00:14:58.150 --> 00:15:00.964
transformation as
a TensorFlow graph,

00:15:00.964 --> 00:15:03.130
and actually apply these
transformations at training

00:15:03.130 --> 00:15:05.020
and at serving time.

00:15:05.020 --> 00:15:07.000
Now, again, this
may sound trivial,

00:15:07.000 --> 00:15:10.270
because you can already
express your transformations

00:15:10.270 --> 00:15:11.720
with a TensorFlow graph.

00:15:11.720 --> 00:15:14.110
However, if your
transformations require

00:15:14.110 --> 00:15:17.950
an analyze phase of your
data, it's less trivial.

00:15:17.950 --> 00:15:21.010
And the easiest example for
this is mean normalization.

00:15:21.010 --> 00:15:23.219
So if you want to mean
normalize a feature,

00:15:23.219 --> 00:15:25.510
you have to compute the mean
and the standard deviation

00:15:25.510 --> 00:15:26.136
over your data.

00:15:26.136 --> 00:15:28.176
And then you need to
subtract the mean and divide

00:15:28.176 --> 00:15:29.190
by standard deviation.

00:15:29.190 --> 00:15:30.490
Right.

00:15:30.490 --> 00:15:34.720
If you work on a laptop with a
dataset that's a few gigabytes,

00:15:34.720 --> 00:15:37.970
you can do that with NumPy
and everything is great.

00:15:37.970 --> 00:15:40.014
However, if you have
terabytes of data,

00:15:40.014 --> 00:15:41.680
and you actually want
to replicate these

00:15:41.680 --> 00:15:44.810
transformations in serving
time, it's less trivial.

00:15:44.810 --> 00:15:48.964
So Transform provides you
with utility functions.

00:15:48.964 --> 00:15:50.380
And for mean
normalization there's

00:15:50.380 --> 00:15:54.430
one that's called Scale to
Z-score that is a one liner.

00:15:54.430 --> 00:15:57.580
So you can say, I want to
scale this feature such

00:15:57.580 --> 00:16:01.840
that it has a mean of zero and
a standard deviation of one.

00:16:01.840 --> 00:16:04.480
And then Transform actually
creates a Beam graph for you

00:16:04.480 --> 00:16:07.660
that computes these
metrics over your data.

00:16:07.660 --> 00:16:09.940
And then Beam handles
computing those metrics

00:16:09.940 --> 00:16:12.250
over your entire dataset.

00:16:12.250 --> 00:16:15.520
And then Transform injects the
results of this analyze phase

00:16:15.520 --> 00:16:17.722
as a constant in your
TensorFlow graph,

00:16:17.722 --> 00:16:19.180
and creates a
TensorFlow graph that

00:16:19.180 --> 00:16:21.490
does the computation needed.

00:16:21.490 --> 00:16:24.580
And the benefit of this is
that this TensorFlow graph that

00:16:24.580 --> 00:16:26.600
expresses this
transformation can now

00:16:26.600 --> 00:16:28.210
be carried forward to training.

00:16:28.210 --> 00:16:30.700
So training time, you
applied those transformations

00:16:30.700 --> 00:16:32.110
to your training data.

00:16:32.110 --> 00:16:35.680
And the exact same graph is also
applied to the inference graph,

00:16:35.680 --> 00:16:38.980
such that at inference time
the exact same transformations

00:16:38.980 --> 00:16:40.270
are being done.

00:16:40.270 --> 00:16:43.000
Now, that basically eliminates
training serving skew,

00:16:43.000 --> 00:16:44.500
because now you can
be entirely sure

00:16:44.500 --> 00:16:46.083
that the exact same
transformations is

00:16:46.083 --> 00:16:46.720
being applied.

00:16:46.720 --> 00:16:48.850
It eliminates the need
for you to have code

00:16:48.850 --> 00:16:51.250
in your serving system that
tries to replicate this

00:16:51.250 --> 00:16:54.190
transformation, because
usually the code paths that you

00:16:54.190 --> 00:16:56.860
use in your training pipelines
are different from the ones

00:16:56.860 --> 00:16:58.360
that you use in
your serving system,

00:16:58.360 --> 00:17:02.410
because that's very low latency.

00:17:02.410 --> 00:17:05.230
Here's just a code snippet
of how such a pre processing

00:17:05.230 --> 00:17:06.730
function can look like.

00:17:06.730 --> 00:17:09.170
I just spoke about
scaling to the Z-score.

00:17:09.170 --> 00:17:10.780
So that's mean normalization.

00:17:10.780 --> 00:17:13.329
String_to_int is another
very common transformation

00:17:13.329 --> 00:17:16.880
that does string to integer
mapping by creating a vocab.

00:17:16.880 --> 00:17:19.180
And bucketizing
a feature, again,

00:17:19.180 --> 00:17:20.710
is also a very
common transformation

00:17:20.710 --> 00:17:24.849
that requires an analyze
phase over your data.

00:17:24.849 --> 00:17:28.210
And all of these examples
are relatively simple.

00:17:28.210 --> 00:17:32.499
But just think about one of
the more advanced use cases

00:17:32.499 --> 00:17:34.540
where you can actually
chain together transforms.

00:17:34.540 --> 00:17:37.810
You can do a transform of your
already transformed feature.

00:17:37.810 --> 00:17:41.470
And Transform actually
handles all of these for you.

00:17:41.470 --> 00:17:43.460
So there's a few
common use cases.

00:17:43.460 --> 00:17:46.080
I've talked about scaling
and bucketization.

00:17:46.080 --> 00:17:47.990
Text transformations
are very common.

00:17:47.990 --> 00:17:50.230
So if you want to
compute ngrams,

00:17:50.230 --> 00:17:51.941
you can do that as well.

00:17:51.941 --> 00:17:53.440
And the particularly
interesting one

00:17:53.440 --> 00:17:56.030
is actually applying
a safe model.

00:17:56.030 --> 00:17:57.970
And applying a safe
model in Transform

00:17:57.970 --> 00:18:01.030
takes an already trained
or created TensorFlow model

00:18:01.030 --> 00:18:02.770
and applies it as
a transformation.

00:18:02.770 --> 00:18:05.980
So you can imagine if one
of your inputs is an image,

00:18:05.980 --> 00:18:09.310
and you want to apply an
inception model to that image

00:18:09.310 --> 00:18:12.040
to create an input
for your model,

00:18:12.040 --> 00:18:13.595
you can do that
with that function.

00:18:13.595 --> 00:18:15.730
So you can actually embed
other TensorFlow models

00:18:15.730 --> 00:18:20.000
as transformations in
your TensorFlow model.

00:18:20.000 --> 00:18:26.360
And all of this is available on
TensorFlow/Transform on GitHub.

00:18:26.360 --> 00:18:28.340
Next, we talk about the trainer.

00:18:28.340 --> 00:18:30.130
And the trainer is
really just TensorFlow.

00:18:30.130 --> 00:18:34.944
We're going to talk about the
Estimate API and the Keras API.

00:18:34.944 --> 00:18:36.860
This is just a code
snippet that shows you how

00:18:36.860 --> 00:18:39.405
to train a wide and deep model.

00:18:39.405 --> 00:18:44.150
A wide and deep model
combines a deep [INAUDIBLE],,

00:18:44.150 --> 00:18:48.794
just a [INAUDIBLE] of a network,
and the linear part together.

00:18:48.794 --> 00:18:50.210
And in the case
of this estimator,

00:18:50.210 --> 00:18:52.640
it's a matter of
instantiating this estimator.

00:18:52.640 --> 00:18:55.020
And then the Estimate API is
relatively straightforward.

00:18:55.020 --> 00:19:00.170
There's a train method that you
can call to train the model.

00:19:00.170 --> 00:19:03.530
And the estimators
that are up here

00:19:03.530 --> 00:19:05.564
are the ones that are
in core TensorFlow.

00:19:05.564 --> 00:19:06.980
So if you just
install TensorFlow,

00:19:06.980 --> 00:19:09.472
you get DNNs, Linear, DNN and
Linear combined, and boosted

00:19:09.472 --> 00:19:10.930
trees, which is a
great [INAUDIBLE]

00:19:10.930 --> 00:19:13.140
tree implementation.

00:19:13.140 --> 00:19:15.830
But if you want to
do some searching

00:19:15.830 --> 00:19:19.319
in TensorFlow Contrib,
or in other repositories

00:19:19.319 --> 00:19:21.110
under the TensorFlow
[INAUDIBLE] on GitHub,

00:19:21.110 --> 00:19:23.420
you will find many, many
more implementations

00:19:23.420 --> 00:19:28.930
of very common architectures
with the estimator framework.

00:19:28.930 --> 00:19:31.184
Now, the estimator,
there's a method

00:19:31.184 --> 00:19:32.350
that's currently in Contrib.

00:19:32.350 --> 00:19:36.625
But it will move to the
Estimate API with 2.0.

00:19:36.625 --> 00:19:39.320
It has a method called
Export Safe Models.

00:19:39.320 --> 00:19:41.986
And that actually exports
a TensorFlow graph

00:19:41.986 --> 00:19:43.360
as a safe model,
such that it can

00:19:43.360 --> 00:19:47.941
be used by a TensorFlow model
analysis in TensorFlow Survey.

00:19:47.941 --> 00:19:49.440
This is just a code
snippet from one

00:19:49.440 --> 00:19:51.870
of our examples
of how this looks.

00:19:51.870 --> 00:19:54.040
For an actual
example, in this case,

00:19:54.040 --> 00:19:56.160
it's the Chicago taxi dataset.

00:19:56.160 --> 00:19:59.970
We just instantiated the
non-linear combined classifier,

00:19:59.970 --> 00:20:02.940
called train, and
exported it for use

00:20:02.940 --> 00:20:05.490
by downstream components.

00:20:05.490 --> 00:20:07.650
Using tf.Keras, it
looks very similar.

00:20:07.650 --> 00:20:10.300
So in this case, we used
the Keras sequential API,

00:20:10.300 --> 00:20:13.920
where you can configure
the layers of your network.

00:20:13.920 --> 00:20:15.540
And the Keras API
is also getting

00:20:15.540 --> 00:20:17.610
a method called Save
Keras model that

00:20:17.610 --> 00:20:20.390
exports the same format,
which is the safe model, such

00:20:20.390 --> 00:20:23.365
that it can be used again
by downstream components.

00:20:25.980 --> 00:20:28.900
Model evaluation validation
is open sourced as TensorFlow

00:20:28.900 --> 00:20:32.500
model analysis.

00:20:32.500 --> 00:20:35.134
And that takes that
graph as an input.

00:20:35.134 --> 00:20:36.550
So the graph that
we just exported

00:20:36.550 --> 00:20:40.780
from our estimator or Keras
model flows as an input

00:20:40.780 --> 00:20:42.360
into TFMA.

00:20:42.360 --> 00:20:44.740
And TFMA computes
evaluation statistics

00:20:44.740 --> 00:20:47.530
at scale in a sliced manner.

00:20:47.530 --> 00:20:50.020
So now, this is another
one of those examples where

00:20:50.020 --> 00:20:54.281
you may say, well, I already
get my metrics from TensorBoard.

00:20:54.281 --> 00:20:56.530
TensorBoard metrics are
computed in a streaming manner

00:20:56.530 --> 00:20:59.080
during training
on minute batches.

00:20:59.080 --> 00:21:02.380
TFMA uses Beam
pipelines to compute

00:21:02.380 --> 00:21:07.940
metrics in an exact manner with
one pass over all of your data.

00:21:07.940 --> 00:21:10.820
So if you want to compute your
metrics or a terabyte of data

00:21:10.820 --> 00:21:14.440
within exactly one
pass, you can use TFMA.

00:21:14.440 --> 00:21:17.680
Now, in this case, you
run TFMA for that model

00:21:17.680 --> 00:21:20.290
and some dataset.

00:21:20.290 --> 00:21:23.710
And if you just call this method
called random slicing metrics

00:21:23.710 --> 00:21:28.190
with the result by itself, the
visualization looks like this.

00:21:28.190 --> 00:21:29.690
And I pulled this
up for one reason.

00:21:29.690 --> 00:21:32.530
And that reason is
just to highlight

00:21:32.530 --> 00:21:35.000
what we mean by sliced metrics.

00:21:35.000 --> 00:21:36.707
This is the metric
that you may be

00:21:36.707 --> 00:21:39.040
used to when someone trains
a model and tells you, well,

00:21:39.040 --> 00:21:44.920
my model has a 0.94
accuracy, or a 0.92 AUC.

00:21:44.920 --> 00:21:47.620
That's an overall metric.

00:21:47.620 --> 00:21:51.490
Over all of your data,
it's the aggregate

00:21:51.490 --> 00:21:54.670
of those metrics for
your entire model.

00:21:54.670 --> 00:21:57.460
That may tell you that the
model is doing well on average,

00:21:57.460 --> 00:21:59.860
but it will not tell
you how the model is

00:21:59.860 --> 00:22:03.050
doing on specific
slices of your data.

00:22:03.050 --> 00:22:07.780
So if you, instead, render those
slices for a specific feature--

00:22:07.780 --> 00:22:10.540
in this case we actually
sliced these metrics

00:22:10.540 --> 00:22:12.900
by trip start hour--

00:22:12.900 --> 00:22:18.490
so, again, this is from the
Chicago taxicab dataset.

00:22:18.490 --> 00:22:21.790
You actually get a visualization
in which you can now--

00:22:21.790 --> 00:22:24.900
in this case, we look at a
histogram and [INAUDIBLE]

00:22:24.900 --> 00:22:25.720
metric.

00:22:25.720 --> 00:22:28.420
We filter for buckets that
only have 100 examples so

00:22:28.420 --> 00:22:30.550
that we don't get low buckets.

00:22:30.550 --> 00:22:32.560
And then you can
actually see here

00:22:32.560 --> 00:22:35.350
how the model performs on
different slices of feature

00:22:35.350 --> 00:22:39.940
values for a specific
trip start hour.

00:22:39.940 --> 00:22:42.190
So this particular model
is trained to predict

00:22:42.190 --> 00:22:45.550
whether a tip is more
or less than 20%.

00:22:45.550 --> 00:22:48.670
And you've seen overall it has
a very high accuracy, and very

00:22:48.670 --> 00:22:50.084
high AUC.

00:22:50.084 --> 00:22:52.000
But it turns out that
on some of these slices,

00:22:52.000 --> 00:22:54.040
it actually performs poorly.

00:22:54.040 --> 00:22:56.950
So if the trip start hour
is seven, for some reason

00:22:56.950 --> 00:22:59.480
the model doesn't really have
a lot of predictive power

00:22:59.480 --> 00:23:01.587
whether the tip is
going to be good or bad.

00:23:01.587 --> 00:23:02.920
Now, that's informative to know.

00:23:02.920 --> 00:23:05.020
Because maybe that's
just because there's

00:23:05.020 --> 00:23:06.730
more variability at that time.

00:23:06.730 --> 00:23:10.040
Maybe we don't have enough
data during that time.

00:23:10.040 --> 00:23:11.590
So that's really a
very powerful tool

00:23:11.590 --> 00:23:15.350
to help you understand
how your model performs.

00:23:15.350 --> 00:23:19.480
Some other visualizations
that are available in TFMA

00:23:19.480 --> 00:23:20.250
are shown here.

00:23:20.250 --> 00:23:21.990
We haven't shown
that in the past.

00:23:21.990 --> 00:23:25.280
So the calibration plot,
which is the first one,

00:23:25.280 --> 00:23:27.380
shows you how your
model predictions

00:23:27.380 --> 00:23:28.730
behave against the label.

00:23:28.730 --> 00:23:30.910
And you would want your
model to be well calibrated,

00:23:30.910 --> 00:23:34.760
and not to be over or under
predicting in a specific area.

00:23:34.760 --> 00:23:36.530
The prediction
distribution just shows you

00:23:36.530 --> 00:23:39.980
that this distribution,
precision recall, and our C

00:23:39.980 --> 00:23:42.620
curves are commonly known.

00:23:42.620 --> 00:23:44.970
And, again, this is
the plot for overall.

00:23:44.970 --> 00:23:49.310
So this is the entire model
and the entire eval dataset.

00:23:49.310 --> 00:23:52.700
And, again, if you
specify a slice here,

00:23:52.700 --> 00:23:55.100
you can actually get the
same visualization only

00:23:55.100 --> 00:23:59.090
for a specific slice
of your features.

00:23:59.090 --> 00:24:00.720
And another really
nice feature is

00:24:00.720 --> 00:24:03.420
that if you have multiple
models or multiple eval sets

00:24:03.420 --> 00:24:06.880
over time, you can visualize
them in a time series.

00:24:06.880 --> 00:24:09.330
So in this case, we
have three models.

00:24:09.330 --> 00:24:10.800
And for all of
these three models,

00:24:10.800 --> 00:24:13.800
we show accuracy and AUC.

00:24:13.800 --> 00:24:17.280
And you can imagine if you have
long running training jobs,

00:24:17.280 --> 00:24:20.580
and as I mentioned earlier, in
some cases you want to refresh

00:24:20.580 --> 00:24:22.020
your model regularly.

00:24:22.020 --> 00:24:25.140
And you train a new model
every day for a year,

00:24:25.140 --> 00:24:26.960
you end up with 365
models, and you can

00:24:26.960 --> 00:24:28.210
see how it performs over time.

00:24:30.614 --> 00:24:32.780
So this product is called
TensorFlow Model analysis.

00:24:32.780 --> 00:24:34.456
And it's also
available on GitHub.

00:24:34.456 --> 00:24:36.080
And everything that
I've just shown you

00:24:36.080 --> 00:24:39.010
is already open sourced.

00:24:39.010 --> 00:24:43.300
So next serving, which is
called TensorFlow Serving.

00:24:43.300 --> 00:24:45.724
So serving is one of
those other areas where

00:24:45.724 --> 00:24:47.390
it's relatively easy
to set something up

00:24:47.390 --> 00:24:50.360
that performs inference with
your machine learning models.

00:24:50.360 --> 00:24:53.670
But it's harder to
do this at scale.

00:24:53.670 --> 00:24:56.170
So some of the most important
features of TensorFlow Serving

00:24:56.170 --> 00:24:59.930
is that it's able to deal
with multiple models.

00:24:59.930 --> 00:25:02.930
And this is mostly used for
actually upgrading a model

00:25:02.930 --> 00:25:03.660
version.

00:25:03.660 --> 00:25:06.140
So if you are serving
a model, and you

00:25:06.140 --> 00:25:08.150
want to update that
model to a new version,

00:25:08.150 --> 00:25:10.850
that server needs to load a
new version at the same time,

00:25:10.850 --> 00:25:14.120
and then switch over to
request to that new version.

00:25:14.120 --> 00:25:16.500
That's also where
isolation comes in.

00:25:16.500 --> 00:25:20.030
You don't want that process of
loading a new model to actually

00:25:20.030 --> 00:25:23.450
impact the current model
serving requests, because that

00:25:23.450 --> 00:25:25.730
would hurt performance.

00:25:25.730 --> 00:25:28.040
There's batching implementations
in TensorFlow Serving

00:25:28.040 --> 00:25:31.400
that make sure that
throughput is optimized.

00:25:31.400 --> 00:25:36.020
In most cases when you
have a high requests

00:25:36.020 --> 00:25:40.130
per second service, you actually
don't want to perform inference

00:25:40.130 --> 00:25:42.020
on a batch of size one.

00:25:42.020 --> 00:25:44.960
You can actually do
dynamic batching.

00:25:44.960 --> 00:25:47.790
And TensorFlow Serving
is adopted, of course,

00:25:47.790 --> 00:25:49.850
widely within Google, and
also outside of Google.

00:25:49.850 --> 00:25:51.683
There's a lot of companies
that have started

00:25:51.683 --> 00:25:55.709
using TensorFlow Serving.

00:25:55.709 --> 00:25:56.750
What does this look like?

00:25:56.750 --> 00:25:58.580
Again, the same graph
that we've exported

00:25:58.580 --> 00:26:02.420
from either our estimator
or our Keras model

00:26:02.420 --> 00:26:04.460
goes into the
TensorFlow model server.

00:26:04.460 --> 00:26:06.560
TensorFlow Serving
comes as your library.

00:26:06.560 --> 00:26:08.540
So you can build your
own server if you want,

00:26:08.540 --> 00:26:10.640
or you can use the libraries
to perform inference.

00:26:10.640 --> 00:26:12.366
We also ship a binary.

00:26:12.366 --> 00:26:14.240
And this is the command
of how you would just

00:26:14.240 --> 00:26:17.270
run the binary, tell it
what port to listen to,

00:26:17.270 --> 00:26:19.790
and what model to load.

00:26:19.790 --> 00:26:23.810
And in this case, it
will load that model

00:26:23.810 --> 00:26:26.271
and bring up that server.

00:26:26.271 --> 00:26:28.520
And this is a code snippet
again from our Chicago text

00:26:28.520 --> 00:26:32.270
example of how you
put together a request

00:26:32.270 --> 00:26:37.320
and make, in this case, a
GRPC call to that server.

00:26:37.320 --> 00:26:42.330
Now, not everyone is using
GRPC, for whatever reason.

00:26:42.330 --> 00:26:44.360
So we built a REST API.

00:26:44.360 --> 00:26:48.240
That was the top request
on GitHub for a while.

00:26:48.240 --> 00:26:51.200
And we built it such
that the TensorFlow model

00:26:51.200 --> 00:26:56.260
server binary ships with both
the GRPC and the REST API.

00:26:56.260 --> 00:27:03.352
And it supports the same
APIs as the GRPC one.

00:27:03.352 --> 00:27:04.810
So this is what
the API looks like.

00:27:04.810 --> 00:27:06.600
So you specify the model name.

00:27:06.600 --> 00:27:09.750
And, as I just mentioned,
it also supports classify,

00:27:09.750 --> 00:27:11.760
regress, and predict.

00:27:11.760 --> 00:27:13.860
And here's just two
examples of an [? iris ?]

00:27:13.860 --> 00:27:16.214
model with the classify
API, or an [INAUDIBLE] model

00:27:16.214 --> 00:27:17.130
with a particular API.

00:27:20.790 --> 00:27:22.620
Now, one of the things
that this enables

00:27:22.620 --> 00:27:26.070
is that instead of
Proto3 JSON, which

00:27:26.070 --> 00:27:29.100
is a little more verbose
than most people would like,

00:27:29.100 --> 00:27:32.280
you can actually now
use Idiomatic JSON.

00:27:32.280 --> 00:27:34.770
That seems more intuitive
to a lot of developers

00:27:34.770 --> 00:27:37.170
that are more used to this.

00:27:37.170 --> 00:27:39.480
And as I just mentioned,
the model server ships

00:27:39.480 --> 00:27:41.850
with this by default.
So when you bring up

00:27:41.850 --> 00:27:46.380
the TensorFlow model server, you
just specify the REST API port.

00:27:46.380 --> 00:27:48.090
And then, in this
case, this is just

00:27:48.090 --> 00:27:50.760
an example of how you can
make a request to this model

00:27:50.760 --> 00:27:53.740
from the command line.

00:27:53.740 --> 00:27:56.355
Last time I spoke about
this was earlier this year.

00:27:56.355 --> 00:27:57.730
And I had to make
an announcement

00:27:57.730 --> 00:27:58.813
that it will be available.

00:27:58.813 --> 00:28:01.970
But now we've made that
available earlier this year.

00:28:01.970 --> 00:28:05.500
So all of this is now
in our GitHub repository

00:28:05.500 --> 00:28:06.130
for you to use.

00:28:09.200 --> 00:28:12.440
Now, what does that look like
if we put all of this together?

00:28:12.440 --> 00:28:15.120
It's relatively straightforward.

00:28:15.120 --> 00:28:17.570
So in this case, you start
with the training data.

00:28:17.570 --> 00:28:21.470
You use TensorFlow Transform
to express your transform graph

00:28:21.470 --> 00:28:24.140
that will actually deal
with the analyze phase

00:28:24.140 --> 00:28:25.550
to compute the metrics.

00:28:25.550 --> 00:28:28.530
It will output the
transform graph itself.

00:28:28.530 --> 00:28:32.960
And, in some cases, you can also
materialize the transform data.

00:28:32.960 --> 00:28:34.670
Now, why would you
want to do that?

00:28:34.670 --> 00:28:38.060
You pay the cost of
materializing your data again.

00:28:38.060 --> 00:28:40.760
In some cases, where throughput
for the model at training time

00:28:40.760 --> 00:28:42.770
is extremely
important, namely when

00:28:42.770 --> 00:28:45.020
you use hardware
accelerators, you

00:28:45.020 --> 00:28:48.300
may actually want to materialize
expensive transformations.

00:28:48.300 --> 00:28:51.710
So if you use GPUs
or TPUs, you may

00:28:51.710 --> 00:28:53.510
want to materialize
all of you transforms

00:28:53.510 --> 00:28:56.120
such that at training time,
you can feed the model

00:28:56.120 --> 00:28:59.320
as fast as you can.

00:28:59.320 --> 00:29:01.990
Now, from there you can use
an estimator or Keras model,

00:29:01.990 --> 00:29:04.630
as I just showed you, to
export your eval graph

00:29:04.630 --> 00:29:06.430
and your inference graph.

00:29:06.430 --> 00:29:10.370
And that's the API that connects
the trainer with TensorFlow

00:29:10.370 --> 00:29:12.120
Model Analysis and
TensorFlow Serving.

00:29:17.190 --> 00:29:18.342
So all of this works today.

00:29:18.342 --> 00:29:20.050
I'll have a link for
you in a minute that

00:29:20.050 --> 00:29:22.633
has an end to end example of how
you use all of these products

00:29:22.633 --> 00:29:24.970
together.

00:29:24.970 --> 00:29:27.530
As I just mentioned
earlier, for us

00:29:27.530 --> 00:29:29.620
it's extremely important
that these products

00:29:29.620 --> 00:29:31.780
work in a notebook
environment, because we really

00:29:31.780 --> 00:29:34.510
think that that barrier between
data scientists and product

00:29:34.510 --> 00:29:37.190
engineers, or data engineers,
should not be there.

00:29:37.190 --> 00:29:38.916
So you can use all of
this in a notebook,

00:29:38.916 --> 00:29:40.540
and then use the same
code to go deploy

00:29:40.540 --> 00:29:44.140
it in a distributed
manner on a cluster.

00:29:44.140 --> 00:29:47.440
For the Beam runner,
as I mentioned,

00:29:47.440 --> 00:29:49.990
you can run it on a local
machine in a notebook

00:29:49.990 --> 00:29:51.730
and on the Cloud Dataflow.

00:29:51.730 --> 00:29:53.680
The Flink runner is in progress.

00:29:53.680 --> 00:29:55.760
And there's also plans
to develop a Spark

00:29:55.760 --> 00:29:59.829
runner so that you can deploy
these pipelines on Spark as

00:29:59.829 --> 00:30:01.800
well.

00:30:01.800 --> 00:30:04.690
This is the link to
the end to end example.

00:30:04.690 --> 00:30:07.650
You will find it currently
lives in the TensorFlow Model

00:30:07.650 --> 00:30:08.950
Analysis repo.

00:30:08.950 --> 00:30:11.190
So you will find
it on GitHub there,

00:30:11.190 --> 00:30:14.060
or you can use that short link
that takes you directly to it.

00:30:16.612 --> 00:30:18.320
But then I hear some
people saying, wait.

00:30:18.320 --> 00:30:19.960
Actually, we want more.

00:30:19.960 --> 00:30:24.110
And I totally understand why you
would want more, because maybe

00:30:24.110 --> 00:30:25.130
you've read that paper.

00:30:25.130 --> 00:30:26.671
And you've certainly
seen that graph,

00:30:26.671 --> 00:30:30.320
because it was in a lot of the
slides that I just showed you.

00:30:30.320 --> 00:30:32.660
And we just talked about
four of these things.

00:30:32.660 --> 00:30:33.380
Right.

00:30:33.380 --> 00:30:35.120
But what about the rest.

00:30:35.120 --> 00:30:38.570
And as I mentioned earlier,
it's extremely important

00:30:38.570 --> 00:30:40.820
to highlight that these are
just some of the libraries

00:30:40.820 --> 00:30:42.050
that we use.

00:30:42.050 --> 00:30:45.180
This is far from actually
being an integrated platform.

00:30:45.180 --> 00:30:48.830
And as a result, if you
actually use these together,

00:30:48.830 --> 00:30:52.860
you will see in the end to end
example it works really well.

00:30:52.860 --> 00:30:56.630
But it can be much, much
easier once they're integrated.

00:30:56.630 --> 00:30:58.556
And actually there
is a layer that

00:30:58.556 --> 00:31:00.680
pulls all of these components
together and makes it

00:31:00.680 --> 00:31:04.130
a good end to end experience.

00:31:04.130 --> 00:31:05.690
So I've announced
before that we will

00:31:05.690 --> 00:31:10.850
release next the components for
data analysis and validation.

00:31:10.850 --> 00:31:13.100
There's not much more I can
say about this today other

00:31:13.100 --> 00:31:16.630
than these will be available
really, really soon.

00:31:16.630 --> 00:31:18.302
And I'll leave it at that.

00:31:18.302 --> 00:31:19.760
And then after
that, the next phase

00:31:19.760 --> 00:31:22.340
is actually the framework that
pulls all of these components

00:31:22.340 --> 00:31:22.997
together.

00:31:22.997 --> 00:31:24.830
That actually will make
it much, much easier

00:31:24.830 --> 00:31:26.900
to configure these pipelines,
because then there's

00:31:26.900 --> 00:31:28.316
going to be a
shared configuration

00:31:28.316 --> 00:31:30.800
layer to configure all
of these components

00:31:30.800 --> 00:31:32.739
and actually pull all
of them together, such

00:31:32.739 --> 00:31:34.280
that they work as
a pipeline, and not

00:31:34.280 --> 00:31:38.090
as individual components.

00:31:38.090 --> 00:31:39.630
And I think you get the idea.

00:31:39.630 --> 00:31:41.180
So we are really
committed to making

00:31:41.180 --> 00:31:44.150
all of this available to
the community, because we've

00:31:44.150 --> 00:31:46.520
seen the profound
impact that it has had

00:31:46.520 --> 00:31:48.382
at Google and for our products.

00:31:48.382 --> 00:31:50.090
And we are really
excited to see what you

00:31:50.090 --> 00:31:52.370
can do with them in your space.

00:31:54.900 --> 00:31:57.470
So these are just the
GitHub links of the products

00:31:57.470 --> 00:31:58.952
that I just discussed.

00:31:58.952 --> 00:32:01.160
And, again, all of the things
that I showed you today

00:32:01.160 --> 00:32:04.510
are already available.

00:32:04.510 --> 00:32:06.390
Now, because we
have some time, I

00:32:06.390 --> 00:32:09.510
can also talk about
TensorFlow Hub.

00:32:09.510 --> 00:32:11.760
And TensorFlow Hub
is a library that

00:32:11.760 --> 00:32:16.890
enables you to publish,
consume, and discover

00:32:16.890 --> 00:32:18.870
what we call modules.

00:32:18.870 --> 00:32:21.390
And I'm going to come to
what we mean by modules,

00:32:21.390 --> 00:32:23.880
but it's really reusable parts
of machine learning models.

00:32:26.212 --> 00:32:27.920
And I'm going to start
with some history.

00:32:27.920 --> 00:32:31.190
And I think a lot of
you can relate to this.

00:32:31.190 --> 00:32:33.410
I've actually heard the
talk today that mentioned

00:32:33.410 --> 00:32:36.380
some of these aspects.

00:32:36.380 --> 00:32:38.780
In some ways, machine learning
and machine learning tools

00:32:38.780 --> 00:32:40.910
are 10, 15 years
behind the tools

00:32:40.910 --> 00:32:42.740
that we use for
software engineering.

00:32:42.740 --> 00:32:45.260
Software engineering
has seen rapid growth

00:32:45.260 --> 00:32:46.520
in the last decade.

00:32:46.520 --> 00:32:48.020
And as there was
a lot of growth,

00:32:48.020 --> 00:32:52.160
and as more and more developers
started working together,

00:32:52.160 --> 00:32:55.670
we built tools and systems that
made collaboration much more

00:32:55.670 --> 00:32:56.480
efficient.

00:32:56.480 --> 00:32:57.590
We built version control.

00:32:57.590 --> 00:33:00.710
We built continuous integration.

00:33:00.710 --> 00:33:02.030
We built code repositories.

00:33:02.030 --> 00:33:03.770
Right.

00:33:03.770 --> 00:33:06.380
And machine learning is now
going through that same growth.

00:33:06.380 --> 00:33:09.000
And more and more people want
to deploy machine learning.

00:33:09.000 --> 00:33:11.900
But we are now rediscovering
some of these challenges

00:33:11.900 --> 00:33:14.540
that we've seen with
software engineering.

00:33:14.540 --> 00:33:18.290
What is the version control
equivalent for these machine

00:33:18.290 --> 00:33:19.760
learning pipelines?

00:33:19.760 --> 00:33:22.101
And what is the code
repository equivalent?

00:33:22.101 --> 00:33:23.600
Well, the code
repository is the one

00:33:23.600 --> 00:33:27.770
that I'm going to talk to you
about right now for TensorFlow

00:33:27.770 --> 00:33:29.330
Hub.

00:33:29.330 --> 00:33:32.270
So code repositories
are an amazing thing,

00:33:32.270 --> 00:33:35.280
because they enable a few
really good practices.

00:33:35.280 --> 00:33:38.940
The first one is, if, as an
engineer, I want to write code,

00:33:38.940 --> 00:33:41.300
and I know that there's
a shared repository,

00:33:41.300 --> 00:33:44.840
usually I would look first if
it has already been implemented.

00:33:44.840 --> 00:33:47.260
So I would search on
GitHub or somewhere else

00:33:47.260 --> 00:33:49.760
to actually see if someone has
already implemented the thing

00:33:49.760 --> 00:33:51.770
that I'm going to build.

00:33:51.770 --> 00:33:53.540
Secondly, if I
know that I'm going

00:33:53.540 --> 00:33:55.947
to publish my code
on a code repository,

00:33:55.947 --> 00:33:57.530
I may make different
design decisions.

00:33:57.530 --> 00:34:00.290
I may build it in such a
way that it's more reusable

00:34:00.290 --> 00:34:01.750
and that's more modular.

00:34:01.750 --> 00:34:03.020
Right.

00:34:03.020 --> 00:34:05.840
And that usually leads to
better software in general.

00:34:05.840 --> 00:34:11.000
And in general, it
also increases velocity

00:34:11.000 --> 00:34:12.219
of the entire community.

00:34:12.219 --> 00:34:12.719
Right.

00:34:12.719 --> 00:34:15.380
Even if it's a private
repository within a company,

00:34:15.380 --> 00:34:18.500
if it's a public repository and
open source, such as GitHub,

00:34:18.500 --> 00:34:20.769
code sharing is
usually a good thing.

00:34:20.769 --> 00:34:22.310
Now, TensorFlow Hub
is the equivalent

00:34:22.310 --> 00:34:24.206
for machine learning.

00:34:24.206 --> 00:34:27.997
In machine learning,
you also have code.

00:34:27.997 --> 00:34:28.580
You have data.

00:34:28.580 --> 00:34:29.929
You have models.

00:34:29.929 --> 00:34:32.380
And you would want
a central repository

00:34:32.380 --> 00:34:35.330
that allows you to share these
reusable parts of machine

00:34:35.330 --> 00:34:39.170
learning between developers,
and between teams.

00:34:39.170 --> 00:34:41.389
And if you think about
it, in machine learning

00:34:41.389 --> 00:34:44.330
it's even more important
than in software engineering.

00:34:44.330 --> 00:34:47.370
Because machine learning
models are much,

00:34:47.370 --> 00:34:48.909
much more than just code.

00:34:48.909 --> 00:34:49.502
Right.

00:34:49.502 --> 00:34:51.710
So there's the algorithm
that goes into these models.

00:34:51.710 --> 00:34:52.694
There's the data.

00:34:52.694 --> 00:34:56.330
There's the compute power that
was used to train these models.

00:34:56.330 --> 00:34:58.250
And then there's the
expertise of people

00:34:58.250 --> 00:35:02.870
that built these models
that is scarce today.

00:35:02.870 --> 00:35:06.180
And I just want to
reiterate this point.

00:35:06.180 --> 00:35:08.390
If you share a machine
learning model,

00:35:08.390 --> 00:35:11.780
what you're really sharing is
a combination of all of these.

00:35:11.780 --> 00:35:15.920
If I spent 50,000 GPU hours
to train an embedding,

00:35:15.920 --> 00:35:18.050
and share it with
TensorFlow Hub,

00:35:18.050 --> 00:35:20.090
everyone who uses that
embedding can benefit

00:35:20.090 --> 00:35:21.830
from that compute power.

00:35:21.830 --> 00:35:25.940
They don't have to go
recompute that same model

00:35:25.940 --> 00:35:27.190
and those same data.

00:35:27.190 --> 00:35:29.120
Right.

00:35:29.120 --> 00:35:31.580
So all of these four
ingredients come together

00:35:31.580 --> 00:35:33.120
in what we call a module.

00:35:33.120 --> 00:35:35.900
And module is the
unit that we care

00:35:35.900 --> 00:35:38.780
about that can be published
in TensorFlow Hub,

00:35:38.780 --> 00:35:41.457
and that can now be
reused by different people

00:35:41.457 --> 00:35:42.290
in different models.

00:35:45.040 --> 00:35:48.040
And those modules are
TensorFlow graphs.

00:35:48.040 --> 00:35:50.380
And they can also
contain weights.

00:35:50.380 --> 00:35:52.270
So what that means
is they give you

00:35:52.270 --> 00:35:55.120
a reusable piece
of TensorFlow graph

00:35:55.120 --> 00:35:57.850
that has the trained
knowledge of the data

00:35:57.850 --> 00:36:01.190
and the algorithm
embedded in it.

00:36:01.190 --> 00:36:05.560
And those modules are designed
to be composable so they have

00:36:05.560 --> 00:36:07.510
common signatures
such that they can be

00:36:07.510 --> 00:36:10.410
attached to different models.

00:36:10.410 --> 00:36:11.320
They're reusable.

00:36:11.320 --> 00:36:14.050
So they come with the
graph and the weights.

00:36:14.050 --> 00:36:16.624
And importantly, they're
also retrainable.

00:36:16.624 --> 00:36:18.040
So you can actually
back propagate

00:36:18.040 --> 00:36:19.176
through these modules.

00:36:19.176 --> 00:36:20.800
And once you attach
them to your model,

00:36:20.800 --> 00:36:23.365
you can customize them
to your own data an

00:36:23.365 --> 00:36:26.362
to your own use case.

00:36:26.362 --> 00:36:27.820
So let's go through
a quick example

00:36:27.820 --> 00:36:29.830
for text classification.

00:36:29.830 --> 00:36:31.570
Let's say I'm a
startup and I want

00:36:31.570 --> 00:36:37.420
to build a new model that
takes restaurant reviews

00:36:37.420 --> 00:36:40.590
and tries to predict whether
they are positive or negative.

00:36:40.590 --> 00:36:42.090
So in this case,
we have a sentence.

00:36:42.090 --> 00:36:43.900
And if you've ever tried
to train some of these text

00:36:43.900 --> 00:36:45.850
models, you know that
you need a lot of data

00:36:45.850 --> 00:36:49.910
to actually learn a good
representation of text.

00:36:49.910 --> 00:36:53.000
So in this case we would just
want to put in a sentence.

00:36:53.000 --> 00:36:55.880
And we want to see if
it's positive or negative.

00:36:55.880 --> 00:36:58.920
And we want to reuse
the code in the graph.

00:36:58.920 --> 00:37:00.950
We want to reuse
the trained weights

00:37:00.950 --> 00:37:03.830
from someone else who's
done the work before us.

00:37:03.830 --> 00:37:06.710
And we also don't want to
do this with fewer data

00:37:06.710 --> 00:37:08.540
than is usually needed.

00:37:11.242 --> 00:37:13.700
An example of these text modules
that are already published

00:37:13.700 --> 00:37:16.700
are TensorFlow Hub are the
Universal Sentence Encoder.

00:37:16.700 --> 00:37:18.020
There's language models.

00:37:18.020 --> 00:37:20.990
And we've actually added
more languages to these.

00:37:20.990 --> 00:37:26.330
Word2vec is a very popular
type of model as well.

00:37:26.330 --> 00:37:28.220
And the key idea
behind TensorFlow Hub,

00:37:28.220 --> 00:37:31.700
similarly to code repositories,
is that the latest research

00:37:31.700 --> 00:37:33.980
can be shared with you as
fast as possible, and as

00:37:33.980 --> 00:37:35.490
easy as possible.

00:37:35.490 --> 00:37:38.480
So the use of our Universal
Sentence Encoder paper

00:37:38.480 --> 00:37:42.680
was published by some
researchers at Google.

00:37:42.680 --> 00:37:45.050
And in that paper,
the authors actually

00:37:45.050 --> 00:37:48.110
included a link
to TensorFlow Hub

00:37:48.110 --> 00:37:52.310
with the embedding for that
Universal Sentence Encoder.

00:37:52.310 --> 00:37:56.670
That link is like a
handle that you can use.

00:37:56.670 --> 00:37:58.760
So in your code
now, you actually

00:37:58.760 --> 00:38:01.520
want to train a model
that uses this embedding.

00:38:01.520 --> 00:38:04.640
In this case, we train
a DNN classifier.

00:38:04.640 --> 00:38:08.130
It's one line to say, I want
to pull from TensorFlow Hub

00:38:08.130 --> 00:38:11.730
a text embedding column
with this module.

00:38:11.730 --> 00:38:14.880
And let's take a quick look of
what that handle looks like.

00:38:14.880 --> 00:38:16.910
So the first part is
just a TF Hub domain.

00:38:16.910 --> 00:38:19.550
All of the modules
that we publish,

00:38:19.550 --> 00:38:21.290
Google and some of
our partners publish,

00:38:21.290 --> 00:38:23.520
will show up on TFHub.dev.

00:38:23.520 --> 00:38:25.400
The second part is the author.

00:38:25.400 --> 00:38:27.920
So in this case, Google
published this embedding.

00:38:27.920 --> 00:38:31.170
Universal Sentence Encoder is
the name of this embedding.

00:38:31.170 --> 00:38:33.500
And then the last
piece is the version.

00:38:33.500 --> 00:38:36.257
Because TensorFlow Hub
modules are immutable.

00:38:36.257 --> 00:38:38.090
So once they're uploaded,
they can't change,

00:38:38.090 --> 00:38:40.250
because you wouldn't
want a module

00:38:40.250 --> 00:38:41.337
to change underneath you.

00:38:41.337 --> 00:38:42.920
If you want to retrain
a model, that's

00:38:42.920 --> 00:38:45.350
not really good for
reproducibility.

00:38:45.350 --> 00:38:48.350
So if and when we upload a
new version of the Universal

00:38:48.350 --> 00:38:51.290
Sentence Encoder, this
version will increment.

00:38:51.290 --> 00:38:54.570
And then you can change
to the new code as well.

00:38:54.570 --> 00:38:56.030
But just to
reiterate this point,

00:38:56.030 --> 00:38:59.570
this is one line to pull
this embedding column

00:38:59.570 --> 00:39:02.600
from TensorFlow Hub, and uses
it as an input to your DNN

00:39:02.600 --> 00:39:04.970
classifier.

00:39:04.970 --> 00:39:08.090
And now you've just
basically benefited

00:39:08.090 --> 00:39:09.770
from the expertise
and the research that

00:39:09.770 --> 00:39:11.270
was published by
the Google Research

00:39:11.270 --> 00:39:14.770
team for text embeddings.

00:39:14.770 --> 00:39:18.700
I just mentioned earlier that
these modules are retrainable.

00:39:18.700 --> 00:39:21.255
So if you set retrainable
true, now the model

00:39:21.255 --> 00:39:23.380
will actually back propagate
through this embedding

00:39:23.380 --> 00:39:27.045
and update it as you
train with your own data.

00:39:27.045 --> 00:39:28.420
Because in many
cases, of course,

00:39:28.420 --> 00:39:30.086
you still have some
small amount of data

00:39:30.086 --> 00:39:32.350
that you want to train on,
such that the model adapts

00:39:32.350 --> 00:39:33.880
to your specific use case.

00:39:36.550 --> 00:39:39.420
And if you take the same URL,
the same handle, and type it

00:39:39.420 --> 00:39:42.600
in your browser, you end up
on the TensorFlow website,

00:39:42.600 --> 00:39:45.540
and see that documentation
for that same module.

00:39:45.540 --> 00:39:48.716
So that same handle that
you saw in the paper,

00:39:48.716 --> 00:39:50.340
you can use in your
code as a one liner

00:39:50.340 --> 00:39:51.756
to use this
embedding, and you can

00:39:51.756 --> 00:39:53.760
put in your browser
to see documentation

00:39:53.760 --> 00:39:54.870
for this embedding.

00:39:58.470 --> 00:40:01.040
So the short
version of the story

00:40:01.040 --> 00:40:03.770
is that TensorFlow Hub
really is the repository

00:40:03.770 --> 00:40:09.260
for reusable machine
learning models and modules.

00:40:09.260 --> 00:40:12.240
We have already published a
large number of these modules.

00:40:12.240 --> 00:40:15.780
So the text modules are just one
example that I just showed you.

00:40:15.780 --> 00:40:19.010
We have a large number
of image embeddings

00:40:19.010 --> 00:40:20.534
that are both cutting edge.

00:40:20.534 --> 00:40:22.700
So there's a [? neural ?]
architecture search module

00:40:22.700 --> 00:40:24.050
that's available.

00:40:24.050 --> 00:40:27.170
There's also some modules
available for image

00:40:27.170 --> 00:40:30.140
classification that are
optimized for devices so

00:40:30.140 --> 00:40:33.740
that you can use them
on a small device.

00:40:33.740 --> 00:40:36.260
And we are also working
hard to keep publishing

00:40:36.260 --> 00:40:38.060
more and more of these modules.

00:40:38.060 --> 00:40:39.830
So in addition to
Google, we now have

00:40:39.830 --> 00:40:42.421
some modules that have
been published by DeepMind.

00:40:42.421 --> 00:40:44.170
And we are also working
with the community

00:40:44.170 --> 00:40:47.370
to get more and more
modules up there.

00:40:47.370 --> 00:40:50.240
And, again, this is
available on GitHub.

00:40:50.240 --> 00:40:52.160
You can use this today.

00:40:52.160 --> 00:40:54.620
And a particularly
interesting aspect

00:40:54.620 --> 00:40:56.390
that we haven't
highlighted so far,

00:40:56.390 --> 00:40:59.390
but it's extremely important, is
that you can use the TensorFlow

00:40:59.390 --> 00:41:05.290
Hub libraries also to store
and consume your own modules.

00:41:05.290 --> 00:41:08.720
So you don't have to rely on
the TensorFlow Hub platform

00:41:08.720 --> 00:41:10.790
and use the modules
that we have published.

00:41:10.790 --> 00:41:13.910
You can internally
enable your developers

00:41:13.910 --> 00:41:18.290
to write out modules to disc
with some shared storage.

00:41:18.290 --> 00:41:20.770
And other developers can
consume those modules.

00:41:20.770 --> 00:41:22.184
And in that case,
instead of that

00:41:22.184 --> 00:41:23.600
handle that I just
showed you, you

00:41:23.600 --> 00:41:25.490
would just use the
path to those modules.

00:41:28.140 --> 00:41:30.430
And that concludes my talk.

00:41:30.430 --> 00:41:33.730
I will go up to the
TensorFlow booth

00:41:33.730 --> 00:41:36.340
to answer any of your questions.

00:41:36.340 --> 00:41:37.240
Thanks.

00:41:37.240 --> 00:41:39.990
[CLAPPING]

