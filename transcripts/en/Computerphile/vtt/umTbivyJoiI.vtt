WEBVTT
Kind: captions
Language: en

00:00:00.130 --> 00:00:07.980
There's no better way than to check something really simple and show you how you [would] derive it using Huffman tree techniques

00:00:10.300 --> 00:00:12.000
I'm going to

00:00:12.000 --> 00:00:16.380
Construct a tree from the probabilities, and I'm going to go up

00:00:16.380 --> 00:00:20.820
And I'll hope I don't run out of space all these probabilities add up to one

00:00:21.279 --> 00:00:28.139
Order them starting with the highest at the left and they lowest at the right you can do the way around if you like

00:00:28.140 --> 00:00:35.189
So long as you're consistent about it [now]. There's no problem here, because they're all a quarter they're ordered already the huffman algorithm

00:00:35.739 --> 00:00:37.979
Then says group in

00:00:38.770 --> 00:00:40.770
twos from the right

00:00:41.620 --> 00:00:46.200
Successively by going through this, I hope it'll be clear what I mean

00:00:46.899 --> 00:00:54.419
starting at the right take the lowest two probabilities that you can see so I group these together as the

00:00:55.180 --> 00:00:57.010
branches of a tree

00:00:57.010 --> 00:01:03.719
Computer scientists always draw their trees nearly always upside down the leaves if you like the base of the trees down here

00:01:03.719 --> 00:01:09.419
But the actual root where it springs from is up at the top if you group a quarter and a quarter together

00:01:09.610 --> 00:01:15.089
That gives a combined probability of a half that node these two probabilities

00:01:15.490 --> 00:01:19.890
Have been coped with in some sense you're now left with 2/3 probability

00:01:21.009 --> 00:01:23.669
situation take the lowest two

00:01:24.220 --> 00:01:26.699
probabilities, you can see if there's any

00:01:27.220 --> 00:01:34.079
[ambiguity] about which you should choose then choose the rightmost set of probabilities. Well here. There's no problem

00:01:34.079 --> 00:01:40.499
So the next stage for the two lowest probabilities is to group those together and make a 1/2

00:01:41.520 --> 00:01:42.729
combined probability

00:01:42.729 --> 00:01:49.380
They're done the next stage, so this is almost going like recursively back up the tree is to say [R]

00:01:49.380 --> 00:01:56.280
But we now group these together [the] half and the half we've now reassured ourselves that all the probabilities

00:01:56.500 --> 00:01:59.939
Here when grouped together and added up come to 1.0

00:01:59.939 --> 00:02:06.508
And we'll be in deep [trouble] if they didn't that is your huffman tree with those probabilities

00:02:07.149 --> 00:02:13.109
The next thing you do this is great fun is to decorate the tree. I now want to from this

00:02:13.629 --> 00:02:21.059
Workout what the [correct] code the minimal code should be for a given state start to get the root of the tree?

00:02:21.269 --> 00:02:22.599
I always

00:02:22.599 --> 00:02:29.909
Annotate the left going Branch with [zero] and the right going bra with a 1 you can do it the other way around

00:02:30.670 --> 00:02:35.220
Uniformly throughout if you want to it will come to a side different looking huffman code

00:02:35.220 --> 00:02:40.109
But it will have just the same properties as the one I'm going to derive when you come to this node

00:02:40.110 --> 00:02:45.989
And you say I'm now going to descend to the left again the rule is you add a zero of what you've already got

00:02:46.450 --> 00:02:48.450
seen it with zero zero

00:02:48.700 --> 00:02:50.700
How about this state?

00:02:50.890 --> 00:02:53.849
You come down on a zero, but then you go to the right?

00:02:54.280 --> 00:02:58.229
Every time you go to the right you add one to what you've already got

00:02:58.620 --> 00:03:02.730
We've already got a 0 out of 1 and here. We are look it works for the rest

00:03:02.739 --> 00:03:05.878
Go down to the right with a 1 then go down to the left

00:03:06.160 --> 00:03:10.169
Every time you go down to the left add a zero to what you've got there. We are

00:03:11.379 --> 00:03:12.819
1

00:03:12.819 --> 00:03:15.328
Down to the right had a 1 that's that one

00:03:15.329 --> 00:03:21.029
So there we are what huffman basically said in 1952 when he did this?

00:03:21.880 --> 00:03:23.880
was [that] this was the

00:03:24.310 --> 00:03:26.310
[method] of getting a

00:03:26.680 --> 00:03:33.510
compact code in other words the best you could [possibly] do for those probabilities and

00:03:35.079 --> 00:03:39.388
I think David Hoffman's. Thesis is famous for only being about 12 pages long

00:03:39.389 --> 00:03:42.959
I think many of us would love to write a thesis saying

00:03:43.720 --> 00:03:50.669
Here is an unsolved problem. I've done it. Please give me a phd. Yes. It's only 12 pages long but to be fair to

00:03:51.700 --> 00:03:57.720
To those who are examining him. It's easy enough to prove that it works for something like this

00:03:57.720 --> 00:04:01.889
which [is] so easy because they are all exact negative powers of [2]

00:04:02.019 --> 00:04:09.149
The harder thing is to prove that it still gives the best possible compromise even when the probabilities don't work in your favor

00:04:09.150 --> 00:04:12.329
I have to sort of sound a word of warning here

00:04:13.359 --> 00:04:19.978
Let me do the San Francisco [entropy] on the San Francisco page if I can find space to squeeze it in

00:04:20.620 --> 00:04:26.750
If you remember the entropy is the sum over all the [weather] states

00:04:26.750 --> 00:04:30.049
They typically see in textbooks things looking like this

00:04:30.720 --> 00:04:32.720
from the first weather to the fourth

00:04:33.090 --> 00:04:40.040
Weather situation take the sum over p. I log to the Base 2 1 over p

00:04:40.040 --> 00:04:43.730
I that of course come down to being the sum over

00:04:44.430 --> 00:04:48.140
Minus p. I log p I is

00:04:50.700 --> 00:04:56.179
equally valid so what we do for the entropy is we take the weighted sum [of]

00:04:57.060 --> 00:05:01.489
The probability of the state happening times the log of the probability of it happening

00:05:01.560 --> 00:05:06.500
And we've discovered in the entropy in compression video the entropy

00:05:07.200 --> 00:05:09.200
Overall is two bits

00:05:09.960 --> 00:05:14.900
Of course it is [it's] exact power of two all the weather states are the same

00:05:15.240 --> 00:05:22.820
But then we worked out [to] the average length of the code and the average length of the code is two bits

00:05:22.970 --> 00:05:27.320
Yeah, they all occur with probability or quarter, but they're all two bits long so the average

00:05:27.840 --> 00:05:33.859
Length of [the] message [you] set is also two bits. This is so simple and so straightforward

00:05:34.440 --> 00:05:36.440
That's the efficiency

00:05:36.720 --> 00:05:38.720
Which is measured as h?

00:05:39.540 --> 00:05:45.379
Over L in this particular case is 2 over 2

00:05:46.890 --> 00:05:51.890
2 bits for Entry [2] bits there is length which is equal to 100%

00:05:54.390 --> 00:05:57.619
You can hit the entropy limit with this weather system

00:05:58.230 --> 00:06:05.209
Encoded in this way because these probabilities are all negative powers of 2 for those of you. Who've encountered

00:06:05.850 --> 00:06:08.899
Entropy in Physics or chemistry take great care?

00:06:10.140 --> 00:06:12.349
information theorists call Entropy h

00:06:13.110 --> 00:06:20.180
don't do that in your chemistry exam in chemistry h is reserved for the amount of heat in a system, and

00:06:20.940 --> 00:06:27.169
Certainly [remembering] back to my chemistry days. I think they use the symbol s for entropy so don't get confused

00:06:27.900 --> 00:06:32.630
Shannon used h for antifreeze, so I will what about if we have?

00:06:34.029 --> 00:06:38.159
Whether states or some system where these probabilities are [not]?

00:06:38.740 --> 00:06:43.019
Exact powers of two we'll look at a fishing trip. Let's say

00:06:44.589 --> 00:06:48.958
That we want a system with probabilities of one third

00:06:50.409 --> 00:06:52.409
one third

00:06:52.419 --> 00:06:54.219
One nine

00:06:54.219 --> 00:06:56.219
One Nine one

00:06:56.349 --> 00:07:03.088
Ninth this person with a pretty good set of fishing kit and a good selection of fish to catch

00:07:03.819 --> 00:07:08.458
Actually at the end of the day ends up that one third of his

00:07:09.309 --> 00:07:13.829
[or] her catch is called another one third is sea bass one ninth

00:07:14.529 --> 00:07:18.569
proportion of the catch is tuna one ninth is a baby shark and

00:07:19.389 --> 00:07:25.978
We got a bit stuck for inspiration here Barracuda, whether you could get them all from the same boat. I don't know but let's say

00:07:26.830 --> 00:07:28.830
this person wants to

00:07:29.949 --> 00:07:33.208
Transmit back in the shortest possible Toad

00:07:33.759 --> 00:07:39.688
What the predominant catches that he's got so he wants to signal whether he's you know?

00:07:39.689 --> 00:07:41.430
He's got a good lot of Cod today

00:07:41.430 --> 00:07:48.629
Or is it bass or is it whatever first thing always is check your probabilities add up that except to three thirds

00:07:48.789 --> 00:07:50.789
Which is one here we go

00:07:51.459 --> 00:07:58.589
Huffman tree. We're still going to use log to the base 2 from the right take the pair of lowest probabilities

00:07:58.589 --> 00:08:00.719
You can see and those are these [1/9]

00:08:01.389 --> 00:08:05.758
So that makes two ninths cross out the ones you've coped with

00:08:06.550 --> 00:08:13.229
You've now got one third one third one ninth to ninth the two lowest probabilities. You can still see are

00:08:14.229 --> 00:08:19.139
[two] [ninths] and [1/9] which is going to combine to make a 1/3?

00:08:19.719 --> 00:08:21.039
next stage

00:08:21.039 --> 00:08:26.818
The two lowest probabilities you can see well you've got a third a third a third we have coped with [that]

00:08:26.819 --> 00:08:31.949
We've coped with that if you get a choice of either that pair or that pair of thirds

00:08:31.959 --> 00:08:35.758
[it] doesn't matter which actually you'll get slightly different codes

00:08:35.759 --> 00:08:38.759
But with similar properties one third and this [one] [third]

00:08:38.979 --> 00:08:43.888
Gets combined into two thirds and the final stage you can all see is dead easy

00:08:44.680 --> 00:08:46.680
We triumphantly

00:08:46.820 --> 00:08:50.029
We do add up to a total probability of [1.0]

00:08:50.040 --> 00:08:57.320
Now comes the fun bit of writing down the huffman codes for this in binary. You're descending left. So that's a [0]

00:08:57.990 --> 00:09:01.849
You're going right? So that's a 1 you. Come down on this one

00:09:01.850 --> 00:09:07.130
So that's a 1 and then a 0 this arm gets 1 1 but then you come [down] here

00:09:07.130 --> 00:09:13.369
That's going to be 1 1 0 therefore this arm coming down here is 1 1 1 this one is 1 1 [0] and that

00:09:13.370 --> 00:09:18.620
One is 1 1 so those are your huffman codes when you take?

00:09:19.350 --> 00:09:21.980
h as the sum of your

00:09:23.130 --> 00:09:25.130
this is now from

00:09:25.500 --> 00:09:31.759
State number 1 1 2 3 4 5 state number [1] is I've caught card [stay] number 2 is of Court [bath]

00:09:31.980 --> 00:09:38.329
Up to five of your minus p. I log to the base 2 of p I

00:09:39.030 --> 00:09:42.380
You are going to find to your dismay that

00:09:43.020 --> 00:09:48.980
The Entropy comes out to being [2] point 1 1 3 bits

00:09:50.100 --> 00:09:55.370
I've looked this up on my calculator if any of you aren't sure how to take

00:09:56.250 --> 00:09:58.459
Log the base to other third then

00:09:59.610 --> 00:10:03.110
We must keep ganging up on brady to do a number file on

00:10:04.080 --> 00:10:06.259
powers of number bases and logarithms

00:10:07.020 --> 00:10:10.220
The average length when you work it out is

00:10:11.400 --> 00:10:15.139
2 point 2 to 2 bits

00:10:16.530 --> 00:10:18.530
so the efficiency

00:10:19.440 --> 00:10:21.150
is

00:10:21.150 --> 00:10:22.650
equal to

00:10:22.650 --> 00:10:30.560
1 over the other two point 1 1 3 over 2 point 2 2 2 is equal to

00:10:31.620 --> 00:10:33.620
95 Point one percent

00:10:34.230 --> 00:10:35.460
efficient

00:10:35.460 --> 00:10:37.260
Not bad, not bad

00:10:37.260 --> 00:10:45.230
but not nearly as good as the weather stays why why is because these things are not exact negative powers of 2

00:10:48.490 --> 00:10:53.640
With these things being exact negative powers of three I can confidently

00:10:54.010 --> 00:10:57.869
Predict that if only we could calculate entropy in trace

00:10:58.570 --> 00:11:00.570
so so long as your

00:11:01.330 --> 00:11:03.330
properties of your line

00:11:03.550 --> 00:11:06.750
Were such that errors were relatively infrequent?

