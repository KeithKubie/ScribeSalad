WEBVTT
Kind: captions
Language: en

00:00:00.315 --> 00:00:01.940
Alright, you know the issue that we want to make sure

00:00:01.940 --> 00:00:04.610
that we think about each time we introduce a new kind

00:00:04.610 --> 00:00:08.900
of supervised learning representation is to ask what its preference bias

00:00:08.900 --> 00:00:11.330
is. So Charles, can you remind us what preference bias is?

00:00:11.330 --> 00:00:14.410
&gt;&gt; Mike researcher bias tells you what it is you

00:00:14.410 --> 00:00:17.430
are able to represent. Preference bias tells you something about the

00:00:17.430 --> 00:00:20.750
algorithm that you are using to learn. That tells you,

00:00:20.750 --> 00:00:26.270
given two representations, why I would prefer one over the other.

00:00:26.270 --> 00:00:28.290
So, perhaps you think back what we talked

00:00:28.290 --> 00:00:34.560
about with decision trees, we preferred trees where nodes

00:00:34.560 --> 00:00:37.360
near the top had high information gain We preferred

00:00:37.360 --> 00:00:40.520
correct trees. We preferred trees that were shorter to

00:00:40.520 --> 00:00:43.240
ones that were longer unnecessarily and so on and

00:00:43.240 --> 00:00:45.610
so forth. So that actually brings up a point

00:00:45.610 --> 00:00:47.890
here which is, we haven't actually chosen an algorithm.

00:00:47.890 --> 00:00:51.570
We talked about how derivatives work, how back propagation

00:00:51.570 --> 00:00:56.470
works, but you missed telling me one very important thing, which is how do

00:00:56.470 --> 00:00:59.420
we start? You tell me how to update the weights but, how do I

00:00:59.420 --> 00:01:01.450
start out with the weights? Do they all start at zero? Do they all

00:01:01.450 --> 00:01:04.260
start out at one? How do you usually set the weights in the beginning?

00:01:04.260 --> 00:01:07.530
&gt;&gt; Yes indeed. We did not talk about that, that's, it's

00:01:07.530 --> 00:01:10.420
really important. You can't run this algorithm without initializing the weights

00:01:10.420 --> 00:01:13.090
to something. Right? We did talk about how you update the

00:01:13.090 --> 00:01:16.970
weights but they don't just you know, just start undefined and you,

00:01:16.970 --> 00:01:18.850
you can't just update something that's undefined. So we

00:01:18.850 --> 00:01:21.660
have to set the initial weights to something. So pretty

00:01:21.660 --> 00:01:27.300
typical thing for people to do, is small, random,

00:01:27.300 --> 00:01:31.410
values. So why do you suppose we want random values?

00:01:31.410 --> 00:01:34.650
&gt;&gt; Because we have no particular reason to pick one set of values over

00:01:34.650 --> 00:01:36.040
another. So you start somewhere in the

00:01:36.040 --> 00:01:38.830
space. Probably helps us to avoid local minimum.

00:01:38.830 --> 00:01:42.820
&gt;&gt; Yea kind of. I mean there's also the issue of Well

00:01:42.820 --> 00:01:46.700
if we run the algorithm multiple times if we get stuck, we like

00:01:46.700 --> 00:01:49.900
it not to get stuck exactly there again, if do, if you run it

00:01:49.900 --> 00:01:53.690
again. So it gives some variability, which is a helpful thing in avoiding

00:01:53.690 --> 00:01:55.490
local minimal. And what do you suppose,

00:01:55.490 --> 00:01:57.250
it's important to start with small values.

00:01:57.250 --> 00:02:02.840
&gt;&gt; Well you just said. In our discussion before that if the weights get

00:02:02.840 --> 00:02:04.990
really big that can sometimes lead to

00:02:04.990 --> 00:02:08.758
over fitting, because it let's you represent arbitrarily

00:02:08.758 --> 00:02:09.470
complex functions.

00:02:09.470 --> 00:02:11.290
&gt;&gt; Good. And so, and what is that

00:02:11.290 --> 00:02:13.160
tell us about what the preference bias is then?

00:02:13.160 --> 00:02:17.580
&gt;&gt; Well if we start out with small random values. That means

00:02:17.580 --> 00:02:22.540
we are starting out with low complexity. So that means we prefer Simpler

00:02:22.540 --> 00:02:27.180
explanations to more complex explanations. And of course the usual stuff like

00:02:27.180 --> 00:02:29.980
we prefer, correct answers to incorrect answers, and so on and so forth.

00:02:29.980 --> 00:02:33.920
&gt;&gt; &gt; So, you'd say that neural-nets implement, or maybe we should

00:02:33.920 --> 00:02:40.330
say, that neural networks implement a kind of bias that says Prefer correct over

00:02:40.330 --> 00:02:44.080
incorrect but all things being equal, the simpler explanation, is preferred.

00:02:44.080 --> 00:02:46.360
&gt;&gt; Well, if you have the right algorithm. If the algorithm starts

00:02:46.360 --> 00:02:49.940
with small, random values and tries to stop, you know, when you

00:02:49.940 --> 00:02:54.160
start over-fitting Then you, cause you're going to start out with the simpler

00:02:54.160 --> 00:02:57.620
explanations first before you allow your weights to grow. so you, about that.

00:02:57.620 --> 00:02:59.280
&gt;&gt; So this

00:02:59.280 --> 00:03:02.240
reminiscent of the principal that is known as Occan's

00:03:02.240 --> 00:03:05.240
razor which is often stated as entities should not be

00:03:05.240 --> 00:03:08.940
multiplied unnecessarily. And given that we're working with neural

00:03:08.940 --> 00:03:14.230
networks, there's a lot of unnecessary multiplication that happens. [LAUGH]

00:03:14.230 --> 00:03:18.490
But, in fact, this actually is referring to exactly what we've been

00:03:18.490 --> 00:03:21.420
talking about. So this unnecessarily is,

00:03:21.420 --> 00:03:24.230
one interpretation of this is that, "Well,

00:03:24.230 --> 00:03:26.840
when is it necessary?" It's necessary if you're getting better

00:03:26.840 --> 00:03:30.870
explanatory power, you're fitting your data better. So Unnecessarily would

00:03:30.870 --> 00:03:32.680
mean, well we're not doing any better at fitting the

00:03:32.680 --> 00:03:35.610
data. If we're not doing any better at fitting the data,

00:03:35.610 --> 00:03:41.010
then we should not multiply entities. And multiply here means

00:03:41.010 --> 00:03:44.340
make more complex. So don't make something more complex unless

00:03:44.340 --> 00:03:46.700
you're getting better error, or if two things have similar

00:03:46.700 --> 00:03:50.070
error Choose the simpler one, use the one that's less complex.

00:03:50.070 --> 00:03:54.480
That has been shown to, if you mathematize this and you use it in

00:03:54.480 --> 00:03:57.240
the context of supervised learning, that we're

00:03:57.240 --> 00:04:01.640
going to get better generalization error with simpler hypotheses.

