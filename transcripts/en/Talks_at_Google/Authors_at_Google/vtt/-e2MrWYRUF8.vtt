WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:02.090
ALAN WINFIELD: Thank
you very much indeed.

00:00:04.490 --> 00:00:05.740
It's really great to be here.

00:00:05.740 --> 00:00:08.460
And thank you so much
for the invitation.

00:00:08.460 --> 00:00:11.270
So yes, robot intelligence.

00:00:11.270 --> 00:00:14.710
So I've titled the lecture
"The Thinking Robot."

00:00:14.710 --> 00:00:18.330
But of course, that
immediately begs the question,

00:00:18.330 --> 00:00:21.610
what on earth do we
mean by thinking?

00:00:21.610 --> 00:00:23.590
Well we could, of
course, spend the whole

00:00:23.590 --> 00:00:28.730
of the next hour debating
what we mean by thinking.

00:00:28.730 --> 00:00:32.460
But I should say that I'm
particularly interested in

00:00:32.460 --> 00:00:36.330
and will focus on
embodied intelligence.

00:00:36.330 --> 00:00:39.990
So in other words, the kind
of intelligence that we have,

00:00:39.990 --> 00:00:43.980
that animals including humans
have, and that robots have.

00:00:43.980 --> 00:00:48.310
So of course that
slightly differentiates

00:00:48.310 --> 00:00:50.150
what I'm talking about from AI.

00:00:50.150 --> 00:00:53.270
But I regard robotics as
a kind of subset of AI.

00:00:55.890 --> 00:00:58.660
And of course one of the
things that we discovered

00:00:58.660 --> 00:01:01.280
in the last 60 odd years
of artificial intelligence

00:01:01.280 --> 00:01:05.410
is that the things that we
thought were really difficult

00:01:05.410 --> 00:01:07.480
actually are relatively easy.

00:01:07.480 --> 00:01:12.050
Like playing chess, or
go, for that matter.

00:01:12.050 --> 00:01:16.150
Whereas the things that
we originally thought

00:01:16.150 --> 00:01:21.070
were really easy, like making
a cup of tea, are really hard.

00:01:21.070 --> 00:01:24.620
So it's kind of the opposite
of what was expected.

00:01:24.620 --> 00:01:27.820
So embodied intelligence
in the real world

00:01:27.820 --> 00:01:32.030
is really very difficult indeed.

00:01:32.030 --> 00:01:34.450
And that's what
I'm interested in.

00:01:34.450 --> 00:01:40.190
So this is the
outline of the talk.

00:01:40.190 --> 00:01:42.640
I'm going to talk initially
about intelligence

00:01:42.640 --> 00:01:45.590
and offer some
ideas, if you like,

00:01:45.590 --> 00:01:48.690
for a way of thinking
about intelligence

00:01:48.690 --> 00:01:50.690
and breaking it
down into categories

00:01:50.690 --> 00:01:54.080
or types of intelligence.

00:01:54.080 --> 00:01:57.160
And then I'm going to choose
a particular one which

00:01:57.160 --> 00:02:01.700
I've been really working on
the last three or four years.

00:02:01.700 --> 00:02:04.860
And it's what I call
a generic architecture

00:02:04.860 --> 00:02:08.910
for a functional imagination.

00:02:08.910 --> 00:02:11.720
Or in short, robots
with internal models.

00:02:11.720 --> 00:02:14.002
So that's really what
I want to focus on.

00:02:14.002 --> 00:02:15.460
Because I really
wanted to show you

00:02:15.460 --> 00:02:16.950
some experimental
work that we've

00:02:16.950 --> 00:02:21.190
done the last couple
of years in the lab.

00:02:21.190 --> 00:02:23.300
I mean, I'm an
electronics engineer.

00:02:23.300 --> 00:02:24.560
I'm an experimentalist.

00:02:24.560 --> 00:02:29.480
And so doing experiments
is really important for me.

00:02:29.480 --> 00:02:33.750
So the first thing that
we ought to realize--

00:02:33.750 --> 00:02:39.070
I'm sure we do realize-- is that
intelligence is not one thing

00:02:39.070 --> 00:02:42.070
that we all, animals,
humans, and robots

00:02:42.070 --> 00:02:44.210
have more or less of.

00:02:44.210 --> 00:02:45.600
Absolutely not.

00:02:45.600 --> 00:02:48.600
And you know, there are several
ways of breaking intelligence

00:02:48.600 --> 00:02:50.690
down into different
kind of categories,

00:02:50.690 --> 00:02:52.780
if you like, of
intelligence, different types

00:02:52.780 --> 00:02:54.230
of intelligence.

00:02:54.230 --> 00:02:57.290
And here's one that I came
up with in the last couple

00:02:57.290 --> 00:02:59.880
of years.

00:02:59.880 --> 00:03:02.740
It's certainly not the only way
of thinking about intelligence.

00:03:02.740 --> 00:03:05.860
But this really breaks
intelligence into four,

00:03:05.860 --> 00:03:10.360
if you like, types, four
kinds of intelligence.

00:03:10.360 --> 00:03:15.280
You could say kinds
of minds, I guess.

00:03:15.280 --> 00:03:17.990
The most fundamental
is what we call

00:03:17.990 --> 00:03:19.960
morphological intelligence.

00:03:19.960 --> 00:03:22.000
And that's the intelligence
that you get just

00:03:22.000 --> 00:03:25.190
from having a physical body.

00:03:25.190 --> 00:03:27.790
And there are some
interesting questions

00:03:27.790 --> 00:03:30.830
about how you design
morphological intelligence.

00:03:30.830 --> 00:03:35.810
You've probably all seen
pictures of or movies of robots

00:03:35.810 --> 00:03:38.610
that can walk, but in
fact don't actually

00:03:38.610 --> 00:03:42.050
have any computing, any
computation whatsoever.

00:03:42.050 --> 00:03:46.990
In other words, the
behavior of walking

00:03:46.990 --> 00:03:49.500
is an emergent property
of the mechanics,

00:03:49.500 --> 00:03:54.100
if you like, the springs and
levers and so on in the robot.

00:03:54.100 --> 00:03:57.450
So that's an example of
morphological intelligence.

00:03:57.450 --> 00:04:00.670
Individual intelligence is
the kind of intelligence

00:04:00.670 --> 00:04:05.070
that you get from
learning individually.

00:04:05.070 --> 00:04:06.840
Social intelligence,
I think, is really

00:04:06.840 --> 00:04:08.110
interesting and important.

00:04:08.110 --> 00:04:09.485
And that's the
one that I'm going

00:04:09.485 --> 00:04:11.120
to focus on most in this talk.

00:04:11.120 --> 00:04:12.930
Social intelligence
is the intelligence

00:04:12.930 --> 00:04:18.709
that you get from learning
socially, from each other.

00:04:18.709 --> 00:04:22.400
And of course, we
are a social species.

00:04:22.400 --> 00:04:23.980
And the other one
which I've been

00:04:23.980 --> 00:04:26.720
working on a lot in
the last 20 odd years

00:04:26.720 --> 00:04:27.980
is swarm intelligence.

00:04:27.980 --> 00:04:30.450
So this is the kind
of intelligence

00:04:30.450 --> 00:04:35.875
that we see most particularly
in social animals, insects.

00:04:39.170 --> 00:04:42.960
The most interesting properties
of swarm intelligence

00:04:42.960 --> 00:04:45.880
tend to be emergent
or self-organizing.

00:04:45.880 --> 00:04:48.350
So in other words,
the intelligence

00:04:48.350 --> 00:04:52.990
is typically manifest as
a collective behavior that

00:04:52.990 --> 00:04:56.230
emerges from the, if you
like, the micro interactions

00:04:56.230 --> 00:04:59.220
between the individuals
in that population.

00:04:59.220 --> 00:05:00.892
So emergence and
self-organization

00:05:00.892 --> 00:05:03.380
are particularly
interesting to me.

00:05:03.380 --> 00:05:08.320
But I said this is
absolutely not the only way

00:05:08.320 --> 00:05:10.300
to think about intelligence.

00:05:10.300 --> 00:05:13.450
And I'm going to
show you another way

00:05:13.450 --> 00:05:17.040
of thinking about intelligence
which I particularly like.

00:05:17.040 --> 00:05:22.250
And this is Dan Dennett's
tower of generate and test.

00:05:22.250 --> 00:05:26.340
So in Darwin's Dangerous
Idea, and several other books,

00:05:26.340 --> 00:05:31.730
I think, Dan Dennett
suggests that a good way

00:05:31.730 --> 00:05:35.150
of thinking about intelligence
is to think about the fact

00:05:35.150 --> 00:05:39.460
that all animals,
including ourselves, need

00:05:39.460 --> 00:05:41.910
to decide what actions to take.

00:05:41.910 --> 00:05:47.160
So choosing the next action is
really critically important.

00:05:47.160 --> 00:05:51.340
I mean it's critically important
for all of us, including

00:05:51.340 --> 00:05:52.150
humans.

00:05:52.150 --> 00:05:54.900
Even though the wrong
action may not kill us,

00:05:54.900 --> 00:05:56.487
as it were, for humans.

00:05:56.487 --> 00:05:58.070
But for many animals,
the wrong action

00:05:58.070 --> 00:06:00.370
may well kill that animal.

00:06:00.370 --> 00:06:05.170
And Dennett talks
about what he calls

00:06:05.170 --> 00:06:08.510
the tower of generate and test
which I want to show you here.

00:06:08.510 --> 00:06:11.860
It's a really cool
breakdown, if you like, way

00:06:11.860 --> 00:06:13.990
of thinking about intelligence.

00:06:13.990 --> 00:06:18.420
So at the bottom of his tower
are Darwinian creatures.

00:06:18.420 --> 00:06:20.730
And the thing about
Darwinian creatures

00:06:20.730 --> 00:06:23.940
is that they have
only one way of,

00:06:23.940 --> 00:06:29.640
as it were, learning
from, if you like,

00:06:29.640 --> 00:06:31.960
generating and testing
next possible actions.

00:06:31.960 --> 00:06:35.490
And that is natural selection.

00:06:35.490 --> 00:06:40.280
So Darwinian creatures in
his schema cannot learn.

00:06:40.280 --> 00:06:42.760
They can only try out an action.

00:06:42.760 --> 00:06:45.190
If it kills them, well
that's the end of that.

00:06:45.190 --> 00:06:48.550
So by the laws of
natural selection,

00:06:48.550 --> 00:06:50.670
that particular action
is unlikely to be

00:06:50.670 --> 00:06:55.210
passed on to descendants.

00:06:55.210 --> 00:06:58.500
Now, of course, all
animals on the planet

00:06:58.500 --> 00:07:01.100
are Darwinian creatures,
including ourselves.

00:07:01.100 --> 00:07:05.010
But a subset of what Dennett
calls Skinnerian creatures.

00:07:05.010 --> 00:07:10.330
So Skinnerian creatures
are able to generate

00:07:10.330 --> 00:07:13.150
a next possible candidate
action, if you like,

00:07:13.150 --> 00:07:16.810
a next possible
action and try it out.

00:07:16.810 --> 00:07:20.210
And here's the thing,
if it doesn't kill them

00:07:20.210 --> 00:07:23.540
but it's actually a bad action,
then they'll learn from that.

00:07:23.540 --> 00:07:26.420
Or even if it's a good
action, a Skinnerian creature

00:07:26.420 --> 00:07:29.310
will learn from
trying out an action.

00:07:29.310 --> 00:07:35.100
So really, Skinnerian creatures
are a subset of Darwinians,

00:07:35.100 --> 00:07:38.930
actually a small subset
that are able to learn

00:07:38.930 --> 00:07:43.880
by trial and error, individually
learn by trial and error.

00:07:43.880 --> 00:07:48.230
Now, the third layer,
or story, if you

00:07:48.230 --> 00:07:51.370
like, in Dennett's tower, he
calls Popperian creatures,

00:07:51.370 --> 00:07:55.120
after, obviously, the
philosopher, Karl Popper.

00:07:55.120 --> 00:07:58.610
And Popperian creatures
have a big advantage

00:07:58.610 --> 00:08:01.640
over Darwinians and
Skinnerians in that they

00:08:01.640 --> 00:08:05.340
have an internal model of
themselves in the world.

00:08:05.340 --> 00:08:07.070
And with an internal
model, it means

00:08:07.070 --> 00:08:10.130
that you can try out
an action, a candidate

00:08:10.130 --> 00:08:15.370
next possible action, if
you like, by imagining it.

00:08:15.370 --> 00:08:17.470
And it means that you
don't have to actually have

00:08:17.470 --> 00:08:21.200
to put yourself to the
risk of trying it out

00:08:21.200 --> 00:08:23.280
for real physically
in the world,

00:08:23.280 --> 00:08:28.830
and possibly it killing you,
or at least harming you.

00:08:28.830 --> 00:08:33.520
So Popperian creatures have
this amazing invention,

00:08:33.520 --> 00:08:35.179
which is internal modeling.

00:08:35.179 --> 00:08:39.340
And of course, we are examples
of Popperian creatures.

00:08:39.340 --> 00:08:41.950
But there are plenty of
other animals-- again,

00:08:41.950 --> 00:08:44.800
it's not a huge proportion.

00:08:44.800 --> 00:08:47.170
It's rather a small proportion,
in fact, of all animals.

00:08:47.170 --> 00:08:48.990
But certainly there
are plenty of animals

00:08:48.990 --> 00:08:54.020
that are capable in some
form of modeling their world

00:08:54.020 --> 00:08:58.360
and, as it were, imagining
actions before trying them out.

00:08:58.360 --> 00:09:01.020
And just to complete
Dennett's tower,

00:09:01.020 --> 00:09:06.360
he adds another layer that
he calls Gregorian creatures.

00:09:06.360 --> 00:09:09.740
Here's he's naming this
layer after Richard Gregory,

00:09:09.740 --> 00:09:14.800
the British psychologist.

00:09:14.800 --> 00:09:18.080
And the thing that
Gregorian creatures have

00:09:18.080 --> 00:09:21.400
is that in addition
to internal models,

00:09:21.400 --> 00:09:25.820
they have mind tools like
language and mathematics.

00:09:25.820 --> 00:09:31.640
Especially language because it
means that Gregorian creatures

00:09:31.640 --> 00:09:34.410
can share their experiences.

00:09:34.410 --> 00:09:37.710
In fact, a Gregorian
creature could, for instance,

00:09:37.710 --> 00:09:45.820
model in its brain, in its
mind, the possible consequences

00:09:45.820 --> 00:09:48.970
of doing a particular thing,
and then actually pass

00:09:48.970 --> 00:09:49.910
that knowledge to you.

00:09:49.910 --> 00:09:52.330
So you don't even have
to model it yourself.

00:09:52.330 --> 00:09:56.200
So the Gregorian
creatures really

00:09:56.200 --> 00:09:58.340
have the kind of
social intelligence

00:09:58.340 --> 00:10:02.680
that we probably--
perhaps not uniquely,

00:10:02.680 --> 00:10:04.650
but there are obviously
only a handful

00:10:04.650 --> 00:10:10.890
of species that are able to
communicate, if you like,

00:10:10.890 --> 00:10:13.930
traditions with each other.

00:10:13.930 --> 00:10:19.920
So I think internal models are
really, really interesting.

00:10:19.920 --> 00:10:22.090
And as I say, I've been
spending the last couple

00:10:22.090 --> 00:10:27.350
of years thinking about
robots with internal models.

00:10:27.350 --> 00:10:29.730
And actually doing
experiments with

00:10:29.730 --> 00:10:31.140
robots with internal models.

00:10:31.140 --> 00:10:35.880
So are robots with
internal models self-aware?

00:10:35.880 --> 00:10:39.690
Well probably not in the
sense that-- the everyday

00:10:39.690 --> 00:10:42.700
sense that we mean by
self-aware, sentient.

00:10:42.700 --> 00:10:44.680
But certainly internal
models, I think,

00:10:44.680 --> 00:10:46.400
can provide a
minimal level of kind

00:10:46.400 --> 00:10:48.620
of functional self-awareness.

00:10:48.620 --> 00:10:53.950
And absolutely enough to allow
us to ask what if questions.

00:10:53.950 --> 00:10:58.190
So with internal models, we have
potentially a really powerful

00:10:58.190 --> 00:11:00.200
technique for robots.

00:11:00.200 --> 00:11:02.860
Because it means that
they can actually ask

00:11:02.860 --> 00:11:06.240
themselves questions
about what if I take this

00:11:06.240 --> 00:11:08.710
or that next possible action.

00:11:08.710 --> 00:11:10.950
So there's the action
selection, if you like.

00:11:13.950 --> 00:11:18.740
So really, I'm kind of
following Dennett's model.

00:11:18.740 --> 00:11:21.850
I'm really interested in
building Popperian creatures.

00:11:21.850 --> 00:11:24.410
Actually, I'm interested in
building Gregorian creatures.

00:11:24.410 --> 00:11:29.630
But that's another, if you
like, another step in the story.

00:11:29.630 --> 00:11:31.860
So really, here I'm
focusing primarily

00:11:31.860 --> 00:11:33.770
on Popperian creatures.

00:11:33.770 --> 00:11:35.310
So robots with internal models.

00:11:38.520 --> 00:11:41.580
And what I'm talking
about in particular

00:11:41.580 --> 00:11:46.030
is a robot with a
simulation of itself

00:11:46.030 --> 00:11:49.730
and it's currently perceived
environment and of the actors

00:11:49.730 --> 00:11:51.620
inside itself.

00:11:51.620 --> 00:11:53.690
So it takes a bit of
getting your head around.

00:11:53.690 --> 00:11:56.430
The idea of a robot with
a simulation of itself

00:11:56.430 --> 00:11:57.400
inside itself.

00:11:57.400 --> 00:12:00.880
But that's really what
I'm talking about.

00:12:00.880 --> 00:12:04.060
And the famous, the late
John Holland, for instance,

00:12:04.060 --> 00:12:09.550
rather perceptively
wrote an internal model

00:12:09.550 --> 00:12:11.180
that allows a
system to look ahead

00:12:11.180 --> 00:12:13.840
to the future
consequences of actions

00:12:13.840 --> 00:12:16.300
without committing
itself to those actions.

00:12:16.300 --> 00:12:18.160
I don't know
whether John Holland

00:12:18.160 --> 00:12:20.880
was aware of Dennett's tower.

00:12:20.880 --> 00:12:21.800
Possibly not.

00:12:21.800 --> 00:12:26.540
But really saying the same
kind of thing as Dan Dennett.

00:12:26.540 --> 00:12:31.110
Now before I come on to the
work that I've been doing,

00:12:31.110 --> 00:12:34.920
I want to show you some
examples of-- a few examples,

00:12:34.920 --> 00:12:38.950
there aren't many, in fact--
of robots with self-simulation.

00:12:41.480 --> 00:12:45.330
The first one, as
far as I'm aware,

00:12:45.330 --> 00:12:47.990
was by Richard
Vaughan and his team.

00:12:47.990 --> 00:12:50.930
And he used a simulation
inside a robot

00:12:50.930 --> 00:12:57.460
to allow it to plan a safe
route with incomplete knowledge.

00:12:57.460 --> 00:13:00.710
So as far as I'm aware, this
is the world's first example

00:13:00.710 --> 00:13:03.090
of robots with self-simulation.

00:13:06.420 --> 00:13:10.020
Perhaps an example that you
might already be familiar with,

00:13:10.020 --> 00:13:14.340
this is Josh Bongard
and Hod Lipson's work.

00:13:14.340 --> 00:13:16.720
Very notable, very
interesting work.

00:13:16.720 --> 00:13:21.170
Here, self-simulation, but
for a different purpose.

00:13:21.170 --> 00:13:23.790
So this is not
self-simulation to choose,

00:13:23.790 --> 00:13:26.050
as it were, gross
actions in the world.

00:13:26.050 --> 00:13:28.420
But instead,
self-simulation to learn

00:13:28.420 --> 00:13:30.530
how to control your own body.

00:13:30.530 --> 00:13:35.630
So that the idea here is that
if you have a complex body, then

00:13:35.630 --> 00:13:38.310
a self-simulation is a really
good way of figuring out

00:13:38.310 --> 00:13:40.300
how to control
yourself, including

00:13:40.300 --> 00:13:42.800
how to repair yourself
if parts of you

00:13:42.800 --> 00:13:48.800
should break or fail or
be damaged, for instance.

00:13:48.800 --> 00:13:52.360
So that's a really
interesting example

00:13:52.360 --> 00:13:54.720
of what you can do
with self-simulation.

00:13:54.720 --> 00:13:59.810
And a similar idea,
really, was tested

00:13:59.810 --> 00:14:03.140
by my old friend, Owen Holland.

00:14:03.140 --> 00:14:06.170
He built this kind of
scary looking robot.

00:14:06.170 --> 00:14:08.510
Initially it was called
Chronos, but but then it

00:14:08.510 --> 00:14:11.500
became known as ECCE-robot.

00:14:11.500 --> 00:14:18.190
And this robot is deliberately
designed to be hard to control.

00:14:18.190 --> 00:14:21.045
In fact, Owen refers to
it as anthropomimetic.

00:14:24.030 --> 00:14:27.110
Which means anthropic
from the inside out.

00:14:27.110 --> 00:14:31.940
So most humanoid robots are
only humanoid on the outside.

00:14:31.940 --> 00:14:35.790
But here, we have a robot
that has a skeletal structure,

00:14:35.790 --> 00:14:39.120
it has tendons,
it's very-- and you

00:14:39.120 --> 00:14:40.580
can see from the
little movie clip

00:14:40.580 --> 00:14:44.050
there, if any part
of the robot moves,

00:14:44.050 --> 00:14:47.760
then the whole of
the rest of the robot

00:14:47.760 --> 00:14:54.130
tends to flex, rather like
human bodies or animal bodies.

00:14:54.130 --> 00:14:59.250
So Owen was particularly
interested in a robot

00:14:59.250 --> 00:15:01.600
that is difficult to control.

00:15:01.600 --> 00:15:06.260
And the idea then of using an
internal simulation of yourself

00:15:06.260 --> 00:15:09.130
in order to be able to
control yourself or learn

00:15:09.130 --> 00:15:10.570
to control yourself.

00:15:10.570 --> 00:15:15.410
And he was the first to
come up with this phrase,

00:15:15.410 --> 00:15:18.230
functional imagination.

00:15:18.230 --> 00:15:21.104
Really interesting work,
so do check that out.

00:15:23.710 --> 00:15:25.590
And the final example
I want to give

00:15:25.590 --> 00:15:31.200
you is from my own lab,
where-- this is swarm robotics

00:15:31.200 --> 00:15:36.560
work-- where in fact we're doing
evolutionary swarm robotics

00:15:36.560 --> 00:15:37.910
here.

00:15:37.910 --> 00:15:44.740
And we've put a simulation
of each robot and the swarm

00:15:44.740 --> 00:15:47.260
inside each robot.

00:15:47.260 --> 00:15:50.300
And in fact, we're using
those internal simulations

00:15:50.300 --> 00:15:51.830
as part of a genetic algorithm.

00:15:51.830 --> 00:15:56.170
So each robot, in fact, is
evolving its own controller.

00:15:56.170 --> 00:15:59.830
And in fact, it actually
updates its own controller

00:15:59.830 --> 00:16:01.300
about once a second.

00:16:01.300 --> 00:16:05.390
So again, it's a bit an odd
thing to get your head around.

00:16:05.390 --> 00:16:08.980
So about once a
second, each robot

00:16:08.980 --> 00:16:12.310
becomes its own great, great,
great, great grandchild.

00:16:12.310 --> 00:16:15.405
In other words, its
controller is a descendant.

00:16:17.980 --> 00:16:22.930
But the problem with this is
that the internal simulation

00:16:22.930 --> 00:16:24.560
tends to be wrong.

00:16:24.560 --> 00:16:26.760
And we have what we
call the reality gap.

00:16:26.760 --> 00:16:30.140
So the gap between the
simulation and the real world.

00:16:30.140 --> 00:16:33.250
And so we got round that-- my
student Paul O'Dowd came up

00:16:33.250 --> 00:16:35.180
with the idea that
we could co-evolve

00:16:35.180 --> 00:16:39.910
the simulators, as well as
the controllers in the robots.

00:16:39.910 --> 00:16:42.770
So we have a
population of robots

00:16:42.770 --> 00:16:45.770
inside each individual
physical robot, as it were,

00:16:45.770 --> 00:16:47.200
simulated robots.

00:16:47.200 --> 00:16:50.900
But then you also have
a swarm of 10 robots.

00:16:50.900 --> 00:16:55.550
And therefore, we have a
population of 10 simulators.

00:16:55.550 --> 00:16:58.060
So we actually co-evolve
here, the simulators

00:16:58.060 --> 00:17:01.810
and the robot controllers.

00:17:01.810 --> 00:17:05.400
So I want to now
show you the newer

00:17:05.400 --> 00:17:09.770
work I've been doing on
robots with internal models.

00:17:09.770 --> 00:17:15.339
And primarily-- I was
telling [? Yan ?] earlier

00:17:15.339 --> 00:17:18.920
that, you know, I'm kind of
an old fashioned electronics

00:17:18.920 --> 00:17:19.420
engineer.

00:17:19.420 --> 00:17:22.500
Spent much of my career
building safety systems,

00:17:22.500 --> 00:17:23.650
safety critical systems.

00:17:23.650 --> 00:17:26.790
So safety is something
that's very important to me

00:17:26.790 --> 00:17:28.380
and to robotics.

00:17:28.380 --> 00:17:31.240
So here's a kind of
generic internal modeling

00:17:31.240 --> 00:17:33.690
architecture for safety.

00:17:33.690 --> 00:17:39.780
So this is, in fact, Dennett's
loop of generate and test.

00:17:39.780 --> 00:17:42.090
So the idea is that we have
an internal model, which

00:17:42.090 --> 00:17:46.200
is a self-simulation, that
is initialized to match

00:17:46.200 --> 00:17:48.750
the current real world.

00:17:48.750 --> 00:17:53.170
And then you try out,
you run the simulator

00:17:53.170 --> 00:17:56.210
for each of your next
possible actions.

00:17:56.210 --> 00:18:00.470
To put it very simply,
imagine that you're a robot,

00:18:00.470 --> 00:18:03.930
and you could either turn left,
turn right, go straight ahead,

00:18:03.930 --> 00:18:04.950
or stand still.

00:18:04.950 --> 00:18:07.720
So you have four
possible next actions.

00:18:07.720 --> 00:18:11.110
And therefore, you'd loop
through this internal model

00:18:11.110 --> 00:18:13.950
for each of those
next possible actions.

00:18:13.950 --> 00:18:17.910
And then moderate the
action selection mechanism

00:18:17.910 --> 00:18:18.960
in your controller.

00:18:18.960 --> 00:18:21.230
So this is not part
of the controller.

00:18:21.230 --> 00:18:23.700
It's a kind of
moderator, if you like.

00:18:23.700 --> 00:18:28.580
So you could imagine
that the regular robot

00:18:28.580 --> 00:18:32.340
controller, the thing in
red, has a set of four

00:18:32.340 --> 00:18:34.670
next possible actions.

00:18:34.670 --> 00:18:38.590
But your internal
model determines

00:18:38.590 --> 00:18:41.770
that only two of them are safe.

00:18:41.770 --> 00:18:44.350
So it would effectively,
if you like,

00:18:44.350 --> 00:18:48.260
moderate or govern the
action selection mechanism

00:18:48.260 --> 00:18:50.660
of the robot's controller,
so that the robot

00:18:50.660 --> 00:18:55.591
controller, in fact, will not
choose the unsafe actions.

00:18:58.750 --> 00:19:03.950
Interestingly, if you have
a learning controller,

00:19:03.950 --> 00:19:09.110
then that's fine because we
can effectively extend or copy

00:19:09.110 --> 00:19:13.370
the learned behaviors
into the internal model.

00:19:13.370 --> 00:19:14.520
That's fine.

00:19:14.520 --> 00:19:16.670
So in principle-- we
haven't done this.

00:19:16.670 --> 00:19:19.190
But we're starting to do
it now-- in principle,

00:19:19.190 --> 00:19:24.430
we can extend this architecture
to, as it were, to adaptive

00:19:24.430 --> 00:19:25.475
or learning robots.

00:19:28.580 --> 00:19:30.770
Here's a simple
thought experiment.

00:19:30.770 --> 00:19:36.300
Imagine a robot with several
safety hazards facing it.

00:19:36.300 --> 00:19:38.780
It has four next
possible actions.

00:19:38.780 --> 00:19:45.830
Well, your internal
model can figure out

00:19:45.830 --> 00:19:50.420
what the consequence of each
of those actions might be.

00:19:50.420 --> 00:19:58.030
So two of them-- so either
turn right or stay still

00:19:58.030 --> 00:19:59.190
are safe actions.

00:19:59.190 --> 00:20:01.154
So that's a very simple
thought experiment.

00:20:03.950 --> 00:20:08.430
And here's a slightly more
complicated thought experiment.

00:20:08.430 --> 00:20:10.110
So imagine that
the robot, there's

00:20:10.110 --> 00:20:11.670
another actor in
the environment.

00:20:11.670 --> 00:20:13.000
It's a human.

00:20:13.000 --> 00:20:15.150
The human is not looking
where they're going.

00:20:15.150 --> 00:20:17.980
Perhaps walking down the
street peering at a smartphone.

00:20:17.980 --> 00:20:20.010
That never happens,
does it, of course.

00:20:20.010 --> 00:20:24.730
And about to walk into
a hole in the pavement.

00:20:24.730 --> 00:20:30.300
Well, of course, if
it were you noticing

00:20:30.300 --> 00:20:32.750
that human about to walk
into a hole in the pavement,

00:20:32.750 --> 00:20:34.870
you would almost certainly
intervene, of course.

00:20:34.870 --> 00:20:37.530
And it's not just because
you're a good person.

00:20:37.530 --> 00:20:40.090
It's because you have
the cognitive machinery

00:20:40.090 --> 00:20:44.520
to predict the consequences of
both your and their actions.

00:20:44.520 --> 00:20:46.110
And you can figure
out that if you

00:20:46.110 --> 00:20:48.565
were to rush over
towards them, you

00:20:48.565 --> 00:20:50.940
might be able to prevent them
from falling into the hole.

00:20:50.940 --> 00:20:54.130
So here's the same kind of idea.

00:20:54.130 --> 00:20:54.940
But with the robot.

00:20:54.940 --> 00:20:57.410
Imagine it's not
you, but a robot.

00:20:57.410 --> 00:21:01.240
And imagine now that
you are modeling

00:21:01.240 --> 00:21:04.410
the consequences of
yours and the human's

00:21:04.410 --> 00:21:08.450
actions for each one of
your next possible actions.

00:21:08.450 --> 00:21:11.390
And you can see
that now this time,

00:21:11.390 --> 00:21:13.840
we've given a kind
of numerical scale.

00:21:13.840 --> 00:21:19.370
So 0 is perfectly safe, whereas
10 is seriously dangerous,

00:21:19.370 --> 00:21:23.310
kind of danger of
death, if you like.

00:21:23.310 --> 00:21:27.660
And you can see that
the safest outcome

00:21:27.660 --> 00:21:29.930
is if the robot turns right.

00:21:29.930 --> 00:21:32.620
In other words, the
safest for the human.

00:21:32.620 --> 00:21:34.740
I mean, clearly the
safest for the robot

00:21:34.740 --> 00:21:37.180
is either turn
left or stay still.

00:21:37.180 --> 00:21:41.840
But in both cases, the human
would fall into the hole.

00:21:41.840 --> 00:21:43.680
So you can see that
we could actually

00:21:43.680 --> 00:21:48.350
invent a rule which
would represent

00:21:48.350 --> 00:21:51.660
the best outcome for the human.

00:21:51.660 --> 00:21:53.660
And this is what it looks like.

00:21:53.660 --> 00:21:57.450
So if all robot actions,
the human is equally safe,

00:21:57.450 --> 00:22:00.550
then that means that we don't
need to worry about the human,

00:22:00.550 --> 00:22:07.620
so the internal model will
output the safest actions

00:22:07.620 --> 00:22:09.670
for the robot.

00:22:09.670 --> 00:22:13.640
Else, then output
the robot actions

00:22:13.640 --> 00:22:17.220
for the least unsafe
human outcomes.

00:22:17.220 --> 00:22:21.115
Now remarkably-- and
we didn't intend this,

00:22:21.115 --> 00:22:23.430
this actually is
an implementation

00:22:23.430 --> 00:22:26.390
of Asimov's first
law of robotics.

00:22:26.390 --> 00:22:29.610
So a robot may not
injure a human being,

00:22:29.610 --> 00:22:31.605
or through inaction--
that's important,

00:22:31.605 --> 00:22:36.100
the or through inaction-- allow
a human being to come to harm.

00:22:36.100 --> 00:22:41.430
So we kind of ended up
building Asimovian robot,

00:22:41.430 --> 00:22:46.050
simple Asimovian ethical robot.

00:22:46.050 --> 00:22:48.560
So what does it look like?

00:22:48.560 --> 00:22:52.410
Well, we've now extended
to humanoid robots.

00:22:52.410 --> 00:22:54.350
But we started with
the e-puck robots,

00:22:54.350 --> 00:22:59.560
these little-- they're about
the size of a salt shaker,

00:22:59.560 --> 00:23:04.770
I guess, about seven
centimeters tall.

00:23:04.770 --> 00:23:09.950
And this is the little
arena in the lab.

00:23:09.950 --> 00:23:16.390
And what we actually have
inside the ethical robot is--

00:23:16.390 --> 00:23:18.520
this is the internal
architecture.

00:23:18.520 --> 00:23:23.230
So so you can see that we have
the robot controller, which

00:23:23.230 --> 00:23:27.310
is, in fact, a mirror of
the real robot controller,

00:23:27.310 --> 00:23:30.000
a model of the robot, and
a model of the world, which

00:23:30.000 --> 00:23:32.450
includes others in the world.

00:23:32.450 --> 00:23:33.690
So this is the simulator.

00:23:33.690 --> 00:23:38.530
This is a more or less a
regular robot simulator.

00:23:38.530 --> 00:23:40.920
So you probably know
that robot simulators

00:23:40.920 --> 00:23:42.480
are quite commonplace.

00:23:42.480 --> 00:23:48.016
We roboticists use them all
the time to test robots in,

00:23:48.016 --> 00:23:49.390
as it were, in
the virtual world,

00:23:49.390 --> 00:23:51.501
before then trying
out the code for real.

00:23:51.501 --> 00:23:53.250
But what we've done
here is we've actually

00:23:53.250 --> 00:23:59.320
put an off the shelf
simulator inside the robot

00:23:59.320 --> 00:24:01.350
and made it work in real time.

00:24:01.350 --> 00:24:04.580
So the output of the
simulator for each

00:24:04.580 --> 00:24:09.190
of those next possible actions
is evaluated and then goes

00:24:09.190 --> 00:24:11.130
through a logic layer.

00:24:11.130 --> 00:24:14.780
Which is essentially the
rule, the if then else rule

00:24:14.780 --> 00:24:18.520
that I showed you a
couple of slides ago.

00:24:18.520 --> 00:24:22.150
And that effectively
determines or moderates

00:24:22.150 --> 00:24:26.768
the action selection
mechanism of the real robot.

00:24:29.780 --> 00:24:31.540
So this is the
simulation budget.

00:24:31.540 --> 00:24:35.100
So we're actually using
the open source simulator

00:24:35.100 --> 00:24:39.100
Stage, a well-known simulator.

00:24:39.100 --> 00:24:42.280
And in fact, we managed to get
Stage to run about 600 times

00:24:42.280 --> 00:24:44.960
real time.

00:24:44.960 --> 00:24:47.240
Which means that
we're actually cycling

00:24:47.240 --> 00:24:50.010
through our internal
model twice a second.

00:24:50.010 --> 00:24:53.430
And for each one
of those cycles,

00:24:53.430 --> 00:24:58.480
we're actually modeling not four
but 30 next possible actions.

00:24:58.480 --> 00:25:02.120
And we're modeling about
10 seconds into the future.

00:25:02.120 --> 00:25:07.270
So every half a second, our
robot with an internal model

00:25:07.270 --> 00:25:14.930
is looking ahead 10 seconds for
about 30 next possible actions,

00:25:14.930 --> 00:25:17.330
30 of its own next
possible actions.

00:25:17.330 --> 00:25:20.380
But of course, it's also
modeling the consequences

00:25:20.380 --> 00:25:23.480
of each of the other
actors, dynamic actors

00:25:23.480 --> 00:25:25.470
in its environment.

00:25:25.470 --> 00:25:30.787
So this is quite nice to
actually do this in real time.

00:25:30.787 --> 00:25:33.370
And let me show you some of the
results that we got from that.

00:25:33.370 --> 00:25:37.320
So ignore the kind
of football pitch.

00:25:37.320 --> 00:25:40.590
So what we have here is
the ethical robot, which we

00:25:40.590 --> 00:25:44.240
call the A-robot, after Asimov.

00:25:44.240 --> 00:25:46.020
And we have a hole
in the ground.

00:25:46.020 --> 00:25:48.590
It's not a real hole, it's a
virtual hole in the ground.

00:25:48.590 --> 00:25:51.850
We don't need to be digging
holes into the lab floor.

00:25:51.850 --> 00:25:56.110
And we're using another
e-perk as a proxy human

00:25:56.110 --> 00:25:57.270
we call this the H-robot.

00:26:00.120 --> 00:26:02.290
So let me show
you what happened.

00:26:02.290 --> 00:26:08.710
Well we ran it, first of
all, with no H-robot at all,

00:26:08.710 --> 00:26:10.520
as a kind of baseline.

00:26:10.520 --> 00:26:13.990
And you can see on
the left, in 26 runs,

00:26:13.990 --> 00:26:16.710
those are the traces
of the A-robot.

00:26:16.710 --> 00:26:18.350
So you can see the
A-robot, in fact,

00:26:18.350 --> 00:26:20.400
is maintaining its own safety.

00:26:20.400 --> 00:26:23.740
Its avoiding, its
skirting around the edge

00:26:23.740 --> 00:26:28.460
almost optimally skirting the
edge of the hole in the ground.

00:26:28.460 --> 00:26:30.380
But then when we
introduce the H-robot,

00:26:30.380 --> 00:26:33.570
you get this wonderful
behavior here.

00:26:33.570 --> 00:26:37.430
Where as soon as the A-robot
notices that the H-robot is

00:26:37.430 --> 00:26:40.170
heading towards the hole,
which is about here, then

00:26:40.170 --> 00:26:44.760
it deflects, it diverts
from its original course.

00:26:44.760 --> 00:26:49.090
And in fact, more
or less collides.

00:26:49.090 --> 00:26:50.740
They don't actually
physically collide

00:26:50.740 --> 00:26:53.790
because they have low
level collision avoidance.

00:26:53.790 --> 00:26:55.230
So they don't actually collide.

00:26:55.230 --> 00:26:57.710
But nevertheless, the
A-robot effectively

00:26:57.710 --> 00:27:02.760
heads off the H-robot, but
then bounces off safely,

00:27:02.760 --> 00:27:04.830
goes off in another direction.

00:27:04.830 --> 00:27:10.610
And the A-robot then resumes its
course to its target position.

00:27:10.610 --> 00:27:12.620
Which is really nice.

00:27:12.620 --> 00:27:17.580
And interestingly, even though
our simulator is rather low

00:27:17.580 --> 00:27:20.170
fidelity, it doesn't matter.

00:27:20.170 --> 00:27:21.610
Surprisingly, it doesn't matter.

00:27:21.610 --> 00:27:24.720
Because the closer the
A-robot to the H-robot

00:27:24.720 --> 00:27:29.300
gets, then the better its
predictions about colliding.

00:27:29.300 --> 00:27:33.050
So this is why, even with a
rather low fidelity simulator,

00:27:33.050 --> 00:27:39.980
we can collide with really good
precision with the H-robot.

00:27:39.980 --> 00:27:47.930
So let me show you the movies of
this trial with a single proxy

00:27:47.930 --> 00:27:49.680
human.

00:27:49.680 --> 00:27:55.470
And I think the movie starts
in-- so this is real time.

00:27:55.470 --> 00:27:59.820
And you can see the
A-robot nicely heading off

00:27:59.820 --> 00:28:03.500
the H-robot which
then disappears off

00:28:03.500 --> 00:28:04.500
towards the left.

00:28:07.940 --> 00:28:11.600
I think then we've
speeded it four times.

00:28:11.600 --> 00:28:14.580
And this is a
whole load of runs.

00:28:14.580 --> 00:28:17.760
So you can see that
it really does work.

00:28:17.760 --> 00:28:20.910
And also notice that every
experiment is a bit different.

00:28:20.910 --> 00:28:23.010
And of course, that's what
typically happens when

00:28:23.010 --> 00:28:25.770
you have real physical robots.

00:28:25.770 --> 00:28:28.420
Simply because of the
noise in the system,

00:28:28.420 --> 00:28:31.470
the fact that these are real
robots with imperfect motors

00:28:31.470 --> 00:28:33.007
and sensors and what have you.

00:28:35.880 --> 00:28:41.580
So we wrote the paper
and were about to submit

00:28:41.580 --> 00:28:43.670
the paper, when we
kind of thought, well,

00:28:43.670 --> 00:28:45.090
this is a bit boring, isn't it?

00:28:45.090 --> 00:28:49.480
We built this
robot and it works.

00:28:49.480 --> 00:28:57.130
So we had the idea to put a
second human in the-- oh sorry.

00:28:57.130 --> 00:28:58.450
I've forgotten one slide.

00:28:58.450 --> 00:29:00.490
So before I get to
that, I just wanted

00:29:00.490 --> 00:29:08.570
to show you a little animation
of-- these little filaments

00:29:08.570 --> 00:29:14.700
here are the traces of the
A-robot and its prediction

00:29:14.700 --> 00:29:16.480
of what might happen.

00:29:16.480 --> 00:29:19.320
So at the point
where this turns red,

00:29:19.320 --> 00:29:21.820
the A-robot then
starts to intersect.

00:29:21.820 --> 00:29:26.500
And each one of those
traces is its prediction

00:29:26.500 --> 00:29:31.200
of the consequences of both
itself and the H-robot.

00:29:31.200 --> 00:29:37.570
This is really nice because you
can kind of look into the mind,

00:29:37.570 --> 00:29:40.770
to put it that
way, of the robot,

00:29:40.770 --> 00:29:42.770
and actually see
what it's doing.

00:29:42.770 --> 00:29:45.220
Which is very nice, very cool.

00:29:45.220 --> 00:29:51.820
But I was about to say we tried
the same experiment, in fact,

00:29:51.820 --> 00:29:55.600
identical code,
with two H-robots.

00:29:55.600 --> 00:29:59.130
And this is the robot's dilemma.

00:29:59.130 --> 00:30:02.080
This may be the first time
that a real physical robot

00:30:02.080 --> 00:30:05.240
has faced an ethical dilemma.

00:30:05.240 --> 00:30:08.240
So you can see the two
H-robots are more or less

00:30:08.240 --> 00:30:10.780
equidistant from the hole.

00:30:10.780 --> 00:30:14.020
And there is the
A-robot which, in fact,

00:30:14.020 --> 00:30:18.660
fails to save either of them.

00:30:18.660 --> 00:30:22.420
So what's going on there?

00:30:22.420 --> 00:30:26.430
We know that it can save
one of them every time.

00:30:26.430 --> 00:30:28.570
But in fact, it's just
failed to save either.

00:30:31.240 --> 00:30:34.310
And oh, yeah, it does
actually save one of them.

00:30:34.310 --> 00:30:38.220
And has a look at the other
one, but it's too late.

00:30:38.220 --> 00:30:41.650
So this is really
very interesting.

00:30:41.650 --> 00:30:43.465
And not at all what we expected.

00:30:51.810 --> 00:30:54.640
In fact, let me show
you the statistics.

00:30:54.640 --> 00:31:05.780
So in 33 runs, the
ethical robot failed

00:31:05.780 --> 00:31:11.130
to save either of the H-robots
just under half the time.

00:31:11.130 --> 00:31:14.930
So about 14 times, it
failed to save either.

00:31:14.930 --> 00:31:19.570
It saved one of them just
over 15, perhaps 16 times.

00:31:19.570 --> 00:31:22.170
And amazingly, saved
both of them twice,

00:31:22.170 --> 00:31:25.180
which is quite surprising.

00:31:25.180 --> 00:31:27.190
It really should perform
better than that.

00:31:30.540 --> 00:31:33.680
And in fact, when we started
to really look at this,

00:31:33.680 --> 00:31:36.420
we discovered that
the-- so here's

00:31:36.420 --> 00:31:39.530
a particularly good
example of dithering.

00:31:39.530 --> 00:31:44.250
So we realized
that we made a sort

00:31:44.250 --> 00:31:47.172
of pathologically
indecisive ethical robot.

00:31:47.172 --> 00:31:49.130
So I'm going to save this
one-- oh no, no, that

00:31:49.130 --> 00:31:51.100
one-- oh no, no,
this one-- that one.

00:31:51.100 --> 00:31:54.460
And of course, by the
time our ethical robot

00:31:54.460 --> 00:31:56.940
has changed its mind
three or four times,

00:31:56.940 --> 00:31:58.350
well, it's too late.

00:31:58.350 --> 00:31:59.420
So this is the problem.

00:31:59.420 --> 00:32:05.100
The problem, fundamentally,
is that our ethical robot

00:32:05.100 --> 00:32:08.310
doesn't make a decision
and stick to it.

00:32:08.310 --> 00:32:10.700
In fact, it's a
consequence of the fact

00:32:10.700 --> 00:32:14.960
that we are running
our consequence engine,

00:32:14.960 --> 00:32:16.900
as I mentioned, twice a second.

00:32:16.900 --> 00:32:19.250
So every half a second,
our ethical robot

00:32:19.250 --> 00:32:21.850
has the opportunity
to change its mind.

00:32:21.850 --> 00:32:23.530
That's clearly a bad strategy.

00:32:23.530 --> 00:32:26.800
But nevertheless, it
was an interesting kind

00:32:26.800 --> 00:32:32.380
of unexpected consequence
of the experiment.

00:32:32.380 --> 00:32:36.140
We've now transferred the
work to these humanoid robots.

00:32:36.140 --> 00:32:38.170
And we get the same thing.

00:32:38.170 --> 00:32:41.070
So here, there
are two red robots

00:32:41.070 --> 00:32:43.010
both heading toward danger.

00:32:43.010 --> 00:32:46.690
The blue one, the ethical
robot, changes its mind,

00:32:46.690 --> 00:32:48.670
and goes and saves
the one on the left,

00:32:48.670 --> 00:32:51.210
even though it could have
saved the one on the right.

00:32:51.210 --> 00:32:59.790
So another example of our
dithering ethical robot.

00:32:59.790 --> 00:33:03.960
And as I've just
hinted at, the reason

00:33:03.960 --> 00:33:06.340
that there our ethical
robot is so indecisive

00:33:06.340 --> 00:33:09.850
is because it's essentially
a memory-less architecture.

00:33:09.850 --> 00:33:15.470
So you could say that the
robot has a-- again, borrowing

00:33:15.470 --> 00:33:19.560
Owen Holland's description, it
has a functional imagination.

00:33:19.560 --> 00:33:21.610
But it has no
autobiographical memory.

00:33:21.610 --> 00:33:23.530
So it doesn't
remember the decision

00:33:23.530 --> 00:33:25.570
it made half a second ago.

00:33:25.570 --> 00:33:29.350
Which is clearly
not a good strategy.

00:33:29.350 --> 00:33:33.120
Really, an ethical
robot, just like you

00:33:33.120 --> 00:33:36.410
if you are acting in
a similar situation,

00:33:36.410 --> 00:33:38.410
it's probably a
good idea for you

00:33:38.410 --> 00:33:40.940
to stick to the first
decision that you made.

00:33:40.940 --> 00:33:43.360
But probably not forever.

00:33:43.360 --> 00:33:45.700
So you know, I think
the decisions probably

00:33:45.700 --> 00:33:47.690
need to be sticky somehow.

00:33:47.690 --> 00:33:51.270
So decisions like this
may need a half life.

00:33:51.270 --> 00:33:54.650
You know, sticky but not
but not absolutely rigid.

00:33:54.650 --> 00:33:58.614
So actually, at this
point, we decided

00:33:58.614 --> 00:34:01.030
that we're not going to worry
too much about this problem.

00:34:01.030 --> 00:34:04.500
Because in a sense, this is
more of a problem for ethicists

00:34:04.500 --> 00:34:06.429
than engineers, perhaps.

00:34:06.429 --> 00:34:06.970
I don't know.

00:34:06.970 --> 00:34:08.589
But maybe we could
talk about that.

00:34:11.679 --> 00:34:14.400
Before finishing,
I want to show you

00:34:14.400 --> 00:34:17.239
another experiment that we did
with the same architecture,

00:34:17.239 --> 00:34:19.230
exactly the same architecture.

00:34:19.230 --> 00:34:22.320
And this is what we call
the corridor experiment.

00:34:22.320 --> 00:34:27.900
So here we have a robot
with this internal model.

00:34:27.900 --> 00:34:32.070
And it has to get from the
left hand to the right hand

00:34:32.070 --> 00:34:35.250
of a crowded corridor
without bumping

00:34:35.250 --> 00:34:38.570
into any of the other robots
that are in the same corridor.

00:34:38.570 --> 00:34:42.630
So imagine you're walking
down a corridor in an airport

00:34:42.630 --> 00:34:45.219
and everybody else is coming
in the opposite direction.

00:34:45.219 --> 00:34:47.989
And you want to try and get to
the other end of the corridor

00:34:47.989 --> 00:34:49.750
without crashing
into any of them.

00:34:49.750 --> 00:34:53.610
But in fact, you have a
rather large body space.

00:34:53.610 --> 00:34:56.310
You don't want to get
even close to any of them.

00:34:56.310 --> 00:35:01.700
So you want to maintain
your private body space.

00:35:01.700 --> 00:35:06.470
And what the blue
robot here is doing

00:35:06.470 --> 00:35:11.330
is, in fact, modeling the
consequences of its actions

00:35:11.330 --> 00:35:14.100
and the other ones within
this radius of attention.

00:35:14.100 --> 00:35:16.860
So this blue circle is
a radius of attention.

00:35:16.860 --> 00:35:21.150
So here, we're looking at a
simple attention mechanism.

00:35:21.150 --> 00:35:24.720
Which is only worry about
the other dynamic actors

00:35:24.720 --> 00:35:27.910
within your radius of attention.

00:35:27.910 --> 00:35:30.550
In fact, we don't even worry
about ones that are behind us.

00:35:30.550 --> 00:35:33.370
It's only the ones that are
more or less in front of us.

00:35:33.370 --> 00:35:38.960
And you can see that the
robot does eventually make it

00:35:38.960 --> 00:35:40.150
to the end of the corridor.

00:35:40.150 --> 00:35:44.990
But with lots of kind
of stops and back tracks

00:35:44.990 --> 00:35:47.040
in order to prevent it
from-- because it's really

00:35:47.040 --> 00:35:50.625
frightened of any kind of
contact with the other robots.

00:35:53.590 --> 00:35:59.420
And here, we're not
showing all of the sort

00:35:59.420 --> 00:36:01.120
of filaments of prediction.

00:36:01.120 --> 00:36:03.045
Only the ones that are chosen.

00:36:08.960 --> 00:36:13.360
And here are some results
which interestingly show us--

00:36:13.360 --> 00:36:19.550
so perhaps the best one to
look at is this danger ratio.

00:36:19.550 --> 00:36:26.810
And dumb simply means robots
with no internal model at all.

00:36:26.810 --> 00:36:31.790
And intelligent means
robots with internal models.

00:36:31.790 --> 00:36:35.830
So here, the danger ratio
is the number of times

00:36:35.830 --> 00:36:38.850
that you actually come
close to another robot.

00:36:38.850 --> 00:36:41.200
And of course it's very high.

00:36:41.200 --> 00:36:44.170
This is simulated
in real robots.

00:36:44.170 --> 00:36:47.296
Very good correlation between
the real and simulated.

00:36:47.296 --> 00:36:50.260
And with the intelligent
robot, the robot

00:36:50.260 --> 00:36:53.620
with the internal model, we
get a really very much safer

00:36:53.620 --> 00:36:55.330
performance.

00:36:55.330 --> 00:36:59.110
Clearly there is some
cost in the sense

00:36:59.110 --> 00:37:03.940
that, for instance,
the intelligent robot

00:37:03.940 --> 00:37:06.990
runs with internal models
tend to cover more ground.

00:37:06.990 --> 00:37:10.730
But surprisingly, not that
much further distance.

00:37:10.730 --> 00:37:12.370
It's less than you'd expect.

00:37:12.370 --> 00:37:14.930
And clearly, there's
a computational cost.

00:37:14.930 --> 00:37:17.750
Because the computational
cost of simulating clearly

00:37:17.750 --> 00:37:22.200
is zero for the dumb robots,
whereas it's quite high for

00:37:22.200 --> 00:37:25.940
the intelligent robot, the
robot with internal models.

00:37:25.940 --> 00:37:29.150
But again, computation is
relatively free these days.

00:37:29.150 --> 00:37:31.200
So actually, we're
trading safety

00:37:31.200 --> 00:37:35.960
for computation, which I
think is a good trade off.

00:37:35.960 --> 00:37:40.960
So really, I want
to conclude there.

00:37:40.960 --> 00:37:43.850
I've not, of course,
talked about all aspects

00:37:43.850 --> 00:37:45.380
of robot intelligence.

00:37:45.380 --> 00:37:47.230
That would be a
three hour seminar.

00:37:47.230 --> 00:37:50.550
And even then, I wouldn't
be able to cover it all.

00:37:50.550 --> 00:37:53.880
But what I hope I've shown
you in the last few minutes

00:37:53.880 --> 00:37:58.160
is that with internal
models, we have

00:37:58.160 --> 00:38:01.020
a very powerful generic
architecture which we could

00:38:01.020 --> 00:38:04.920
call a functional imagination.

00:38:04.920 --> 00:38:08.200
And this is where I'm being
a little bit speculative.

00:38:08.200 --> 00:38:10.330
Perhaps this moves
us in the direction

00:38:10.330 --> 00:38:14.430
of artificial theory of mind,
perhaps even self-awareness.

00:38:14.430 --> 00:38:16.800
I'm not going to use the
word machine consciousness.

00:38:16.800 --> 00:38:17.550
Well, I just have.

00:38:17.550 --> 00:38:23.620
But that's a very much more
difficult goal, I think.

00:38:23.620 --> 00:38:26.180
And I think there
is practical value,

00:38:26.180 --> 00:38:31.080
I think there's real practical
value in robotics of robots

00:38:31.080 --> 00:38:34.770
with self and other simulation.

00:38:34.770 --> 00:38:37.340
Because as I hope
I've demonstrated,

00:38:37.340 --> 00:38:41.100
at least in a kind of prototype
sense, proof of concept,

00:38:41.100 --> 00:38:45.710
that such simulation moves
us towards safer and possibly

00:38:45.710 --> 00:38:50.530
ethical systems in
unpredictable environments

00:38:50.530 --> 00:38:52.630
with other dynamic actors.

00:38:52.630 --> 00:38:55.350
So thank you very much
indeed for listening.

00:38:55.350 --> 00:38:58.650
I'd obviously be delighted
to take any questions.

00:38:58.650 --> 00:38:59.740
Thank you.

00:38:59.740 --> 00:39:03.380
[APPLAUSE]

00:39:05.770 --> 00:39:08.160
HOST: Thank you very much for
this very fascinating view

00:39:08.160 --> 00:39:09.960
on robotics today.

00:39:09.960 --> 00:39:11.550
We have time for questions.

00:39:11.550 --> 00:39:14.110
Please wait until you've
got a microphone so we have

00:39:14.110 --> 00:39:15.740
the answer also on the video.

00:39:20.550 --> 00:39:25.790
AUDIENCE: The game playing
computers-- or perhaps more

00:39:25.790 --> 00:39:29.430
accurately would be saying
game playing algorithms,

00:39:29.430 --> 00:39:33.470
predated the examples
you listed as computers

00:39:33.470 --> 00:39:35.060
with internal models.

00:39:35.060 --> 00:39:37.220
Still, you didn't mention those.

00:39:37.220 --> 00:39:41.690
Is there a particular
reason why you didn't?

00:39:41.690 --> 00:39:44.110
ALAN WINFIELD: I guess I
should have mentioned them.

00:39:44.110 --> 00:39:45.010
You're quite right.

00:39:45.010 --> 00:39:47.700
I mean, the-- what
I'm thinking of here

00:39:47.700 --> 00:39:52.040
is particularly robots
with explicit simulations

00:39:52.040 --> 00:39:54.470
of themselves and the world.

00:39:54.470 --> 00:39:57.930
So I was limiting my examples
to simulations of themselves

00:39:57.930 --> 00:39:58.640
in the world.

00:39:58.640 --> 00:40:00.560
I mean, you're quite
right that of course

00:40:00.560 --> 00:40:05.000
game-playing algorithms need to
have a simulation of the game.

00:40:05.000 --> 00:40:07.680
And quite likely,
of the-- in fact,

00:40:07.680 --> 00:40:13.230
certainly, of the possible
moves of the opponent,

00:40:13.230 --> 00:40:17.834
as well as the game-playing
AI's own moves.

00:40:17.834 --> 00:40:18.750
So you're quite right.

00:40:18.750 --> 00:40:21.010
I mean, it's a different
kind of simulation.

00:40:21.010 --> 00:40:23.009
But I should include that.

00:40:23.009 --> 00:40:23.550
You're right.

00:40:26.070 --> 00:40:27.900
AUDIENCE: Hi there.

00:40:27.900 --> 00:40:34.050
In your simulation, you had
the H-robot with one goal,

00:40:34.050 --> 00:40:36.780
and the A-robot with
a different goal.

00:40:36.780 --> 00:40:39.540
And they interacted
with each other halfway

00:40:39.540 --> 00:40:41.470
through the goals.

00:40:41.470 --> 00:40:45.290
What happens when they
have the same goal?

00:40:45.290 --> 00:40:46.755
ALAN WINFIELD: The same goal.

00:40:46.755 --> 00:40:50.220
AUDIENCE: Reaching the
same spot, for example.

00:40:50.220 --> 00:40:52.365
ALAN WINFIELD: I don't
know is the short answer.

00:40:55.060 --> 00:40:59.590
It depends on whether that
spot is a safe spot or not.

00:40:59.590 --> 00:41:04.520
I mean, if it's a safe spot,
then they'll both go toward it.

00:41:04.520 --> 00:41:07.660
They'll both reach it,
but without crashing

00:41:07.660 --> 00:41:08.760
into each other.

00:41:08.760 --> 00:41:11.400
Because the A-robot
will make sure

00:41:11.400 --> 00:41:14.022
that it avoids the H-robot.

00:41:14.022 --> 00:41:15.480
In fact, that's
more or less what's

00:41:15.480 --> 00:41:18.060
happening in the
corridor experiment.

00:41:18.060 --> 00:41:18.870
That's right.

00:41:18.870 --> 00:41:19.430
Yeah.

00:41:19.430 --> 00:41:21.680
But it's a good question,
we should try that.

00:41:33.785 --> 00:41:35.160
AUDIENCE: The
simulation that you

00:41:35.160 --> 00:41:38.080
did for the corridor experiment,
the actual real world

00:41:38.080 --> 00:41:42.310
experiment, the simulation
track the other robots

00:41:42.310 --> 00:41:43.060
movements as well?

00:41:43.060 --> 00:41:44.851
Meaning what information
did the simulation

00:41:44.851 --> 00:41:48.051
have that it began with,
versus what did it perceive?

00:41:48.051 --> 00:41:49.925
Because, I mean, the
other robots are moving.

00:41:49.925 --> 00:41:51.700
And in the real
world, they might not

00:41:51.700 --> 00:41:53.460
move as you predict them to be.

00:41:53.460 --> 00:41:57.500
How did the blue robot
actually know for each step

00:41:57.500 --> 00:41:58.680
where the robots were?

00:41:58.680 --> 00:42:00.430
ALAN WINFIELD: Sure.

00:42:00.430 --> 00:42:02.350
That's a very good question.

00:42:02.350 --> 00:42:05.170
In fact we cheated,
in the sense that we

00:42:05.170 --> 00:42:07.300
used-- for the real
robot experiments,

00:42:07.300 --> 00:42:10.370
we used a tracking system.

00:42:10.370 --> 00:42:16.550
Which means that essentially
the robot with an internal model

00:42:16.550 --> 00:42:19.480
has access to the position.

00:42:19.480 --> 00:42:24.060
It's like a GPS,
internal GPS system.

00:42:24.060 --> 00:42:26.812
But in a way, that's
really just a kind

00:42:26.812 --> 00:42:33.730
of-- it's kind of cheating,
but even a robot with a vision

00:42:33.730 --> 00:42:37.010
system would be able
to track all the robots

00:42:37.010 --> 00:42:39.390
in its field of vision.

00:42:39.390 --> 00:42:43.900
And as for the second
part of your question,

00:42:43.900 --> 00:42:47.180
our kind of model of what
the other robot that would do

00:42:47.180 --> 00:42:47.940
is very simple.

00:42:47.940 --> 00:42:50.230
Which is it's kind
of a ballistic model.

00:42:50.230 --> 00:42:53.490
Which is if a robot is
moving at a particular speed

00:42:53.490 --> 00:42:55.270
in a particular
direction, then we

00:42:55.270 --> 00:42:59.240
assume it will continue to
do so until it encounters

00:42:59.240 --> 00:42:59.840
an obstacle.

00:43:02.820 --> 00:43:06.180
So very simple kind
of ballistic model.

00:43:06.180 --> 00:43:11.430
Which even for humans is useful
for very simple behaviors,

00:43:11.430 --> 00:43:13.220
like moving in a crowded space.

00:43:20.380 --> 00:43:22.720
Oh hi.

00:43:22.720 --> 00:43:25.180
AUDIENCE: In the
same experiment--

00:43:25.180 --> 00:43:27.610
it's a continuation of
the previous question.

00:43:27.610 --> 00:43:30.350
So in between some of
the red robots, how

00:43:30.350 --> 00:43:35.080
changed their direction
randomly-- I guess so.

00:43:35.080 --> 00:43:39.390
Does the internal model of
the blue robot consider that?

00:43:39.390 --> 00:43:40.640
ALAN WINFIELD: Not explicitly.

00:43:40.640 --> 00:43:45.660
But it does in the
sense that because it's

00:43:45.660 --> 00:43:49.480
pre- or re-initializing it's
internal model every half

00:43:49.480 --> 00:43:54.400
a second, then if the positions
and directions of the actors

00:43:54.400 --> 00:43:56.810
in its environment
are changed, then they

00:43:56.810 --> 00:44:00.370
will reflect the new positions.

00:44:00.370 --> 00:44:00.940
So--

00:44:00.940 --> 00:44:02.990
AUDIENCE: Not exactly
the positions.

00:44:02.990 --> 00:44:06.260
But as you said, you have
considered the ballistic motion

00:44:06.260 --> 00:44:07.550
of the objects.

00:44:07.550 --> 00:44:11.820
So if there is any randomness
in the environment-- so

00:44:11.820 --> 00:44:14.480
does the internal
model of the blue robot

00:44:14.480 --> 00:44:16.980
consider the
randomness, and change

00:44:16.980 --> 00:44:19.430
the view of the red robots?

00:44:19.430 --> 00:44:22.630
It's like it views the red
robot as a ballistic motion.

00:44:22.630 --> 00:44:26.410
So does it change its
view of the red robot

00:44:26.410 --> 00:44:30.540
that red robots more in
the ballistic motion?

00:44:30.540 --> 00:44:33.290
ALAN WINFIELD: Well, it's
a very good question.

00:44:33.290 --> 00:44:34.870
I think the answer is no.

00:44:34.870 --> 00:44:40.110
I think we're probably assuming
a more or less deterministic

00:44:40.110 --> 00:44:43.110
model of the world.

00:44:43.110 --> 00:44:46.190
Deterministic, yes, I think
pretty much deterministic.

00:44:46.190 --> 00:44:48.450
But we're relying
on the fact that we

00:44:48.450 --> 00:44:52.370
are updating and
rerunning the model,

00:44:52.370 --> 00:44:55.690
reinitializing and rerunning
the model every half a second,

00:44:55.690 --> 00:44:59.350
to, if you like, track
the stochasticity which is

00:44:59.350 --> 00:45:02.650
inevitable in the real world.

00:45:02.650 --> 00:45:07.050
We probably do need to
introduce some stochasticity

00:45:07.050 --> 00:45:09.790
into the internal model, yes.

00:45:09.790 --> 00:45:11.757
But not yet.

00:45:11.757 --> 00:45:12.590
AUDIENCE: Thank you.

00:45:12.590 --> 00:45:14.183
ALAN WINFIELD: But
very good question.

00:45:17.290 --> 00:45:18.352
AUDIENCE: Hello.

00:45:18.352 --> 00:45:20.690
With real life applications
with this technology,

00:45:20.690 --> 00:45:24.050
like driverless
cars, for example,

00:45:24.050 --> 00:45:27.180
I think it becomes a lot more
important how you program

00:45:27.180 --> 00:45:29.410
the robots in terms of ethics.

00:45:29.410 --> 00:45:33.650
So I mean, there could be
dilemma like if the robot has

00:45:33.650 --> 00:45:36.050
a choice between saving
a school bus full of kids

00:45:36.050 --> 00:45:41.420
versus one driver, that logic
needs to be programmed, right?

00:45:41.420 --> 00:45:44.870
And you made a distinction
between being an engineer

00:45:44.870 --> 00:45:47.740
yourself and then had
been an ethicist earlier.

00:45:47.740 --> 00:45:51.360
So to what extent
is the engineer

00:45:51.360 --> 00:45:53.210
responsible in that case?

00:45:53.210 --> 00:45:57.500
And also does a project
like this in real life

00:45:57.500 --> 00:45:59.252
always require the ethicist?

00:45:59.252 --> 00:46:02.170
How do you see this field
in real life applications

00:46:02.170 --> 00:46:03.240
evolving?

00:46:03.240 --> 00:46:04.073
ALAN WINFIELD: Sure.

00:46:04.073 --> 00:46:06.670
That's a really great question.

00:46:06.670 --> 00:46:11.300
I mean, you're right that
driverless cars will-- well,

00:46:11.300 --> 00:46:15.530
it's debatable whether they will
have to make such decisions.

00:46:15.530 --> 00:46:19.620
But many people think they will
have to make such decisions.

00:46:19.620 --> 00:46:23.030
Which are kind of the driverless
car equivalent of the trolley

00:46:23.030 --> 00:46:27.600
problem, which is a well-known
kind of ethical dilemma thought

00:46:27.600 --> 00:46:29.310
experiment.

00:46:29.310 --> 00:46:35.180
Now my view is
that the rules will

00:46:35.180 --> 00:46:39.160
need to be decided not by the
engineers, but if you like,

00:46:39.160 --> 00:46:41.430
by the whole of society.

00:46:41.430 --> 00:46:45.160
So ultimately, the
rules that decide

00:46:45.160 --> 00:46:49.120
how the driverless
car should behave

00:46:49.120 --> 00:46:53.000
under these difficult
circumstances, impossible,

00:46:53.000 --> 00:46:56.260
in fact,
circumstances-- and even

00:46:56.260 --> 00:46:59.410
if we should, in fact, program
those rules into the car.

00:46:59.410 --> 00:47:04.150
So some people argue that the
driverless cars should not

00:47:04.150 --> 00:47:10.410
attempt to, as it were,
make a rule driven decision.

00:47:10.410 --> 00:47:13.040
But just leave it to chance.

00:47:13.040 --> 00:47:15.140
And again, I think
that's an open question.

00:47:15.140 --> 00:47:20.340
But this is really why I
think this dialogue and debate

00:47:20.340 --> 00:47:27.810
and conversations with
regulators, lawyers, ethicists,

00:47:27.810 --> 00:47:31.690
and the general public,
users of driverless cars,

00:47:31.690 --> 00:47:33.960
I think is why we need
to have this debate.

00:47:33.960 --> 00:47:36.480
Because whatever those
rules are, and even

00:47:36.480 --> 00:47:38.870
whether we have them
or not, is something

00:47:38.870 --> 00:47:44.250
that should be decided,
as it were, collectively.

00:47:44.250 --> 00:47:47.750
I mean, someone
asked me last week,

00:47:47.750 --> 00:47:51.230
should you be able to alter the
ethics of your own driverless

00:47:51.230 --> 00:47:52.180
car?

00:47:52.180 --> 00:47:54.260
My answer is absolutely not.

00:47:54.260 --> 00:47:56.130
I mean, that should be illegal.

00:47:56.130 --> 00:47:59.170
So I think that if
driverless cars were

00:47:59.170 --> 00:48:01.040
to have a set of
rules, and especially

00:48:01.040 --> 00:48:04.790
if those rules had numbers
associated with them.

00:48:04.790 --> 00:48:08.400
I mean, let's think of
a less emotive example.

00:48:08.400 --> 00:48:14.600
Imagine a driverless car and
an animal runs into the road.

00:48:14.600 --> 00:48:21.150
Well, , the driverless car
can either ignore the animal

00:48:21.150 --> 00:48:26.790
and definitely kill the animal,
or it could try and brake,

00:48:26.790 --> 00:48:30.900
possibly causing harm to the
driver or the passengers.

00:48:30.900 --> 00:48:34.270
But effectively
reducing the probability

00:48:34.270 --> 00:48:35.710
of killing the animal.

00:48:35.710 --> 00:48:38.800
So there's an example
where you have some numbers

00:48:38.800 --> 00:48:42.600
to tweak if you
like, parameters.

00:48:42.600 --> 00:48:46.460
So if these rules are
built into driverless cars,

00:48:46.460 --> 00:48:48.130
they'll be parameterized.

00:48:48.130 --> 00:48:50.270
And I think it
should be absolutely

00:48:50.270 --> 00:48:56.880
illegal to hack those
parameters, to change them.

00:48:56.880 --> 00:48:59.630
In the same way that it's
probably illegal right now

00:48:59.630 --> 00:49:03.980
to hack an aircraft autopilot.

00:49:03.980 --> 00:49:06.930
I suspect that
probably is illegal.

00:49:06.930 --> 00:49:08.630
If it isn't, it should be.

00:49:08.630 --> 00:49:11.280
So I think that you
don't need to go

00:49:11.280 --> 00:49:14.730
far down this line of
argument before realizing

00:49:14.730 --> 00:49:19.640
that the regulation
and legislation has

00:49:19.640 --> 00:49:20.960
to come into play.

00:49:20.960 --> 00:49:27.370
In fact, I saw a piece in
just this morning in Wired

00:49:27.370 --> 00:49:32.530
that, I think, in the US,
regulation for driverless cars

00:49:32.530 --> 00:49:34.650
is now on the table.

00:49:34.650 --> 00:49:36.260
Which is absolutely right.

00:49:36.260 --> 00:49:40.180
I mean, we need to have
regulatory framework,

00:49:40.180 --> 00:49:44.350
or what I call governance
frameworks for driverless cars.

00:49:44.350 --> 00:49:46.820
And in fact, lots of
other autonomous systems.

00:49:46.820 --> 00:49:47.960
Not just driverless cars.

00:49:47.960 --> 00:49:51.940
But great question, thank you.

00:49:51.940 --> 00:49:54.160
AUDIENCE: In the experiment
with the corridor,

00:49:54.160 --> 00:49:57.080
you always assume-- even in the
other experiments-- you always

00:49:57.080 --> 00:49:59.310
assume that the main actor
is the most intelligent

00:49:59.310 --> 00:50:00.354
and the others are not.

00:50:00.354 --> 00:50:01.770
Like they're dumb,
or like they're

00:50:01.770 --> 00:50:03.780
ballistic models
or linear models.

00:50:03.780 --> 00:50:06.160
Have you tried doing
a similar experiment

00:50:06.160 --> 00:50:12.054
in which still each actor
is intelligent but assumes

00:50:12.054 --> 00:50:13.970
that the others are not,
but actually everyone

00:50:13.970 --> 00:50:14.780
is intelligent?

00:50:14.780 --> 00:50:17.610
So like everyone is a
blue dot in the experiment

00:50:17.610 --> 00:50:19.170
with the model that you have.

00:50:19.170 --> 00:50:21.628
And also, have you considered
changing the model so that he

00:50:21.628 --> 00:50:23.970
assumes that the others
have the same model

00:50:23.970 --> 00:50:26.460
that that particular
actor has, as well.

00:50:26.460 --> 00:50:27.650
[INTERPOSING VOICES]

00:50:27.650 --> 00:50:29.950
ALAN WINFIELD: No, we're
doing it right now.

00:50:29.950 --> 00:50:32.650
So we're doing that
experiment right now.

00:50:32.650 --> 00:50:35.100
And if you ask me
back in a year,

00:50:35.100 --> 00:50:39.640
perhaps I can tell you what
happ-- I mean, it's really mad.

00:50:39.640 --> 00:50:41.890
But it does take us
down this direction

00:50:41.890 --> 00:50:46.270
of artificial theory of mind.

00:50:46.270 --> 00:50:49.680
So if you have several
robots or actors,

00:50:49.680 --> 00:50:53.160
each of which is modeling
the behavior of the other,

00:50:53.160 --> 00:50:59.020
then you get-- I
mean, some of the--

00:50:59.020 --> 00:51:01.640
I don't even have a
movie to show you.

00:51:01.640 --> 00:51:05.120
But in simulation
we've tried this

00:51:05.120 --> 00:51:10.844
where we have two robots which
are kind of like-- imagine,

00:51:10.844 --> 00:51:12.760
this happens to all of
us, you're walking down

00:51:12.760 --> 00:51:17.710
the pavement and you do
the sort of sidestep dance

00:51:17.710 --> 00:51:19.956
with someone who's
coming towards you.

00:51:19.956 --> 00:51:21.580
And so the research
question that we're

00:51:21.580 --> 00:51:24.290
asking ourselves is do
we get the same thing.

00:51:24.290 --> 00:51:26.440
And it seems to be that we do.

00:51:26.440 --> 00:51:30.290
So if the robots are
symmetrical, in other words,

00:51:30.290 --> 00:51:32.410
they're each modeling
the other, then

00:51:32.410 --> 00:51:38.554
we can get these kind of little
interesting dances, where each

00:51:38.554 --> 00:51:40.970
is trying to get out of the
way of the other, but in fact,

00:51:40.970 --> 00:51:43.390
choosing in a
sense the opposite.

00:51:43.390 --> 00:51:46.150
So one chooses to step right,
the other chooses to step left,

00:51:46.150 --> 00:51:49.230
and they still can't
go past each other.

00:51:49.230 --> 00:51:51.150
But it's hugely interesting.

00:51:51.150 --> 00:51:52.170
Yes, hugely interesting.

00:51:57.740 --> 00:51:59.780
AUDIENCE: Hi.

00:51:59.780 --> 00:52:01.400
I think it's really
interesting how

00:52:01.400 --> 00:52:03.910
you point out the
importance of simulations

00:52:03.910 --> 00:52:05.640
and internal models.

00:52:05.640 --> 00:52:08.190
But I feel that one thing
that is slightly left out

00:52:08.190 --> 00:52:12.820
there is a huge gap from going
from simulation to real world

00:52:12.820 --> 00:52:13.880
robots, for example.

00:52:13.880 --> 00:52:16.900
And I assume that
in these simulations

00:52:16.900 --> 00:52:21.380
you kind of assume that the
sensors are 100% reliable.

00:52:21.380 --> 00:52:23.890
And that's obviously
not the case in reality.

00:52:23.890 --> 00:52:27.780
And especially if we're talking
about autonomous cars or robots

00:52:27.780 --> 00:52:29.570
and safety.

00:52:29.570 --> 00:52:32.540
How do you calculate
the uncertainty

00:52:32.540 --> 00:52:34.487
that comes with the
sensors in the equation?

00:52:34.487 --> 00:52:35.320
ALAN WINFIELD: Sure.

00:52:35.320 --> 00:52:37.740
I mean, this is a deeply
interesting question.

00:52:37.740 --> 00:52:39.950
And the short answer
is I don't know.

00:52:39.950 --> 00:52:41.906
I mean, this is all future work.

00:52:45.240 --> 00:52:48.480
I mean, my instinct
is that a robot

00:52:48.480 --> 00:52:51.740
with a simulation,
internal simulation,

00:52:51.740 --> 00:52:58.100
even if that simulation
in a sense is idealized,

00:52:58.100 --> 00:53:00.830
is still probably going to be
safer than a robot that has

00:53:00.830 --> 00:53:03.280
no internal simulation at all.

00:53:03.280 --> 00:53:09.730
And you know, I think we humans
have multiple simulations

00:53:09.730 --> 00:53:11.140
running all the time.

00:53:11.140 --> 00:53:14.680
So I think we have sort of quick
and dirty, kind of low fidelity

00:53:14.680 --> 00:53:19.160
simulations when we
need to move fast.

00:53:19.160 --> 00:53:23.430
But clearly when you need
to plan something, plan

00:53:23.430 --> 00:53:27.532
some complicated
action, like where

00:53:27.532 --> 00:53:29.240
you are going to go
on holiday next year,

00:53:29.240 --> 00:53:33.830
you clearly don't use the same
internal model, same simulation

00:53:33.830 --> 00:53:36.900
as for when you try
and stop someone

00:53:36.900 --> 00:53:39.370
from running into the road.

00:53:39.370 --> 00:53:43.300
So I think that future
intelligent robots

00:53:43.300 --> 00:53:46.740
will need also to have
multiple simulators.

00:53:46.740 --> 00:53:50.190
And also strategies
for choosing which

00:53:50.190 --> 00:53:54.760
fidelity simulator to
use at a particular time.

00:53:54.760 --> 00:53:58.740
And if a particular
situation requires

00:53:58.740 --> 00:54:02.560
that you need high
fidelity, then for instance,

00:54:02.560 --> 00:54:04.060
one of the things
that you could do,

00:54:04.060 --> 00:54:06.080
which actually I think
humans probably do,

00:54:06.080 --> 00:54:09.980
is that you simply move more
slowly to give your self time.

00:54:09.980 --> 00:54:12.870
Or even stop to
give yourself time

00:54:12.870 --> 00:54:14.750
to figure out what's going on.

00:54:14.750 --> 00:54:17.190
And in a sense,
plan your strategy.

00:54:17.190 --> 00:54:26.020
So I think even with the
computational power we have,

00:54:26.020 --> 00:54:29.280
there will still be a
limited simulation budget.

00:54:29.280 --> 00:54:31.480
And I suspect that
that simulation budget

00:54:31.480 --> 00:54:33.330
will mean that in
real time, when you're

00:54:33.330 --> 00:54:38.590
doing this in real time, you
probably can't run your highest

00:54:38.590 --> 00:54:40.250
fidelity simulator.

00:54:40.250 --> 00:54:47.560
And taking into account all
of those probabilistic, noisy

00:54:47.560 --> 00:54:50.870
sensors and actuators
and so on, you probably

00:54:50.870 --> 00:54:53.090
can't run that
simulator all the time.

00:54:53.090 --> 00:54:55.320
So I think we're
going to have to have

00:54:55.320 --> 00:54:59.230
a nuanced approach where we
have perhaps multiple simulators

00:54:59.230 --> 00:55:00.720
with multiple fidelities.

00:55:00.720 --> 00:55:03.300
Or maybe a sort of
tuning, where you can tune

00:55:03.300 --> 00:55:06.700
the fidelity of your simulator.

00:55:06.700 --> 00:55:08.746
So this is kind of a
new area of research.

00:55:08.746 --> 00:55:11.120
I don't know anybody who's
thinking about this yet, apart

00:55:11.120 --> 00:55:12.610
from ourselves.

00:55:12.610 --> 00:55:14.693
So great.

00:55:14.693 --> 00:55:17.000
AUDIENCE: [INAUDIBLE].

00:55:17.000 --> 00:55:20.690
ALAN WINFIELD: It
is pretty hard, yes.

00:55:20.690 --> 00:55:21.576
Please.

00:55:21.576 --> 00:55:23.910
AUDIENCE: [INAUDIBLE].

00:55:23.910 --> 00:55:25.680
ALAN WINFIELD: Do you
want the microphone?

00:55:25.680 --> 00:55:26.179
Sorry.

00:55:28.609 --> 00:55:30.900
AUDIENCE: Have you considered
this particular situation

00:55:30.900 --> 00:55:32.540
where there are
two Asimov robots--

00:55:32.540 --> 00:55:36.860
and that would be an extension
of the question that he asked.

00:55:36.860 --> 00:55:39.790
So for example, if there
are two guys walking

00:55:39.790 --> 00:55:42.360
on a pavement and there
could be a possibility

00:55:42.360 --> 00:55:44.130
of mutual cooperation.

00:55:44.130 --> 00:55:47.119
As in one might communicate
whether that I might step out

00:55:47.119 --> 00:55:48.410
of this place and you might go.

00:55:48.410 --> 00:55:50.300
And then I'll go after that.

00:55:50.300 --> 00:55:52.319
So if there are
two Asimov robots,

00:55:52.319 --> 00:55:53.860
will there be a
possibility, and have

00:55:53.860 --> 00:55:56.380
you considered this fact
that both will communicate

00:55:56.380 --> 00:55:59.100
with each other, and they will
eventually come to a conclusion

00:55:59.100 --> 00:56:03.150
that I will probably walk,
and the other will get out

00:56:03.150 --> 00:56:04.160
of the way.

00:56:04.160 --> 00:56:05.890
And the second part
of this question

00:56:05.890 --> 00:56:08.420
would be what if
one of the robots

00:56:08.420 --> 00:56:12.350
actually does not
agree to cooperate?

00:56:12.350 --> 00:56:15.269
I mean, since they both would
have different simulators.

00:56:15.269 --> 00:56:16.810
They could have
different simulators.

00:56:16.810 --> 00:56:19.268
And one might actually try to
communicate that you step out

00:56:19.268 --> 00:56:22.340
of the way so that I might go.

00:56:22.340 --> 00:56:24.880
And the other one
doesn't agree with that.

00:56:24.880 --> 00:56:26.960
I mean, what would
the [INAUDIBLE].

00:56:26.960 --> 00:56:28.710
ALAN WINFIELD: Yeah,
it's a good question.

00:56:28.710 --> 00:56:31.890
In fact, we've actually
gotten a new paper which

00:56:31.890 --> 00:56:35.290
we're just writing right now.

00:56:35.290 --> 00:56:40.720
And the sort of working
title is "The Dark Side

00:56:40.720 --> 00:56:42.930
of Ethical Robots."

00:56:42.930 --> 00:56:47.280
And one of the things that we
discovered-- it's actually not

00:56:47.280 --> 00:56:52.190
surprising-- is that you only
need to change one line of code

00:56:52.190 --> 00:56:57.410
for a co-operative robot to
become a competitive robot,

00:56:57.410 --> 00:57:00.380
or even an aggressive robot.

00:57:00.380 --> 00:57:05.750
So it's fairly obvious, when
you start to think about it,

00:57:05.750 --> 00:57:09.990
if your ethical rules
are very simply written,

00:57:09.990 --> 00:57:11.720
and are a kind of
layer, if you like,

00:57:11.720 --> 00:57:14.170
on top of the rest
of the architecture,

00:57:14.170 --> 00:57:16.500
then it's not that difficult
to change those rules.

00:57:19.330 --> 00:57:20.940
And yes, we've done
some experiments.

00:57:20.940 --> 00:57:23.260
And again, I don't have
any videos to show you.

00:57:23.260 --> 00:57:28.260
But they're pretty
interesting, showing

00:57:28.260 --> 00:57:32.750
how easy it is to make a
competitive robot, or even

00:57:32.750 --> 00:57:36.710
an aggressive robot
using this approach.

00:57:36.710 --> 00:57:40.680
In fact, on the BBC
six months ago or so,

00:57:40.680 --> 00:57:43.770
I was asked surely if you
can make an ethical robot,

00:57:43.770 --> 00:57:46.780
doesn't that mean you can
make an unethical robot?

00:57:46.780 --> 00:57:49.270
And the answer,
I'm afraid, is yes.

00:57:49.270 --> 00:57:52.760
It does mean that.

00:57:52.760 --> 00:57:55.850
But this really goes back
to your question earlier,

00:57:55.850 --> 00:57:58.340
which is that it should
be-- we should make

00:57:58.340 --> 00:58:01.710
sure it's illegal
to convert, to turn,

00:58:01.710 --> 00:58:04.910
if you like, to recode
an ethical robot

00:58:04.910 --> 00:58:06.240
as an unethical robot.

00:58:06.240 --> 00:58:09.970
Or even it should be illegal
to make unethical robots.

00:58:09.970 --> 00:58:10.860
Something like that.

00:58:10.860 --> 00:58:12.210
But it's a great question.

00:58:12.210 --> 00:58:16.320
And short answer, yes.

00:58:16.320 --> 00:58:18.950
And yes, we have some
interesting new results,

00:58:18.950 --> 00:58:24.410
new paper on, as it
were, unethical robots.

00:58:24.410 --> 00:58:25.950
Yeah.

00:58:25.950 --> 00:58:28.030
HOST: All right, we are
running out of time now.

00:58:28.030 --> 00:58:29.897
Thanks everyone
for coming today.

00:58:29.897 --> 00:58:31.230
Thanks, Professor Alan Winfield.

00:58:31.230 --> 00:58:32.271
ALAN WINFIELD: Thank you.

00:58:32.271 --> 00:58:33.700
[APPLAUSE]

