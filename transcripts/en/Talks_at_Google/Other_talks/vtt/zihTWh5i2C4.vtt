WEBVTT
Kind: captions
Language: en

00:00:04.970 --> 00:00:07.980
BORIS DEBIC: Welcome, everyone,
to yet another

00:00:07.980 --> 00:00:09.695
Authors@Google talk.

00:00:09.695 --> 00:00:15.550
It is my distinct privilege
today to host Dr. Ray

00:00:15.550 --> 00:00:20.500
Kurzweil, and my good friend
here, Peter Norvig, from our

00:00:20.500 --> 00:00:24.330
Artificial Intelligence
group, former director

00:00:24.330 --> 00:00:26.250
of research at Google.

00:00:26.250 --> 00:00:29.890
So just to give you a little
bit of context why I am

00:00:29.890 --> 00:00:30.860
hosting this talk.

00:00:30.860 --> 00:00:36.120
When I was a kid and wrote
my first lines of code in

00:00:36.120 --> 00:00:41.360
elementary school, I saw a
tremendous potential in that

00:00:41.360 --> 00:00:43.540
toy that I was playing with.

00:00:43.540 --> 00:00:46.560
And I said to all my friends,
you know what?

00:00:46.560 --> 00:00:48.550
One of these days, these
are going to

00:00:48.550 --> 00:00:50.410
be as smart as humans.

00:00:50.410 --> 00:00:52.760
We just have to work
a lot at it.

00:00:52.760 --> 00:00:54.640
And they would say, oh,
no, that's impossible.

00:00:54.640 --> 00:00:56.910
How can you say something
like that?

00:00:56.910 --> 00:00:59.930
I really didn't have a good
answer in those times.

00:00:59.930 --> 00:01:01.250
I was just a kid.

00:01:01.250 --> 00:01:03.030
But I told them, look,
I mean it's all

00:01:03.030 --> 00:01:04.310
built of atoms, right?

00:01:04.310 --> 00:01:06.610
The CPUs in this thing,
that's atoms, and our

00:01:06.610 --> 00:01:08.450
brains, that's atoms.

00:01:08.450 --> 00:01:10.150
So there's no theoretical

00:01:10.150 --> 00:01:13.250
impossibility for this to happen.

00:01:13.250 --> 00:01:17.180
Well, today, I'm very happy to
host two guys who can explain

00:01:17.180 --> 00:01:20.550
why this will happen in
much more detail.

00:01:20.550 --> 00:01:23.420
Please welcome Dr. Ray
Kurzweil to Google.

00:01:23.420 --> 00:01:33.490
[APPLAUSE]

00:01:33.490 --> 00:01:36.020
PETER NORVIG: I think it's
redundant to introduce Ray.

00:01:36.020 --> 00:01:40.190
You all know him as an inventor,
author, a futurist.

00:01:40.190 --> 00:01:44.650
And you know, there was a book
a few years back that accused

00:01:44.650 --> 00:01:47.440
Xerox PARC of fumbling
the future.

00:01:47.440 --> 00:01:50.580
And I would say, to continue
that metaphor, Ray has

00:01:50.580 --> 00:01:55.120
intercepted the future and
returned it for a touchdown,

00:01:55.120 --> 00:01:57.340
multiple times.

00:01:57.340 --> 00:02:00.210
He's done it with the flatbed
scanner, with OCR, with

00:02:00.210 --> 00:02:01.940
print-to-speech, text-to-speech,
speech

00:02:01.940 --> 00:02:07.090
recognition, music synthesis,
and so on and so on.

00:02:07.090 --> 00:02:11.060
I won't list all the honors,
but he's been recognized by

00:02:11.060 --> 00:02:13.600
Presidents Johnson,
Clinton, and

00:02:13.600 --> 00:02:16.330
Reagan, and by Bill Cullen.

00:02:16.330 --> 00:02:20.270
Those of you who are younger,
you'll have to Google that.

00:02:20.270 --> 00:02:22.712
But let me put it this way.

00:02:22.712 --> 00:02:27.190
Have you heard of Plato,
Aristotle, Socrates?

00:02:27.190 --> 00:02:29.150
Philosophers.

00:02:29.150 --> 00:02:30.800
And Ray is a philosopher, too.

00:02:30.800 --> 00:02:33.610
But more importantly, foremost,
he's an engineer.

00:02:33.610 --> 00:02:36.620
And when it comes to these tough
questions of creating

00:02:36.620 --> 00:02:39.780
the mind, philosophers are
useful, but I'm putting my

00:02:39.780 --> 00:02:40.890
money on the engineers.

00:02:40.890 --> 00:02:42.372
[LAUGHTER]

00:02:42.372 --> 00:02:50.770
[APPLAUSE]

00:02:50.770 --> 00:02:52.290
RAY KURZWEIL: Well, thanks
for that, Peter.

00:02:52.290 --> 00:02:54.140
I-- can you hear
me back there?

00:02:54.140 --> 00:02:55.230
Yeah?

00:02:55.230 --> 00:02:56.960
I agree with that.

00:02:56.960 --> 00:02:59.470
In fact, I decided I wanted to
be-- well, I called it an

00:02:59.470 --> 00:03:01.820
inventor when I was five.

00:03:01.820 --> 00:03:04.730
And I had this conceit.

00:03:04.730 --> 00:03:07.820
I know what I'm going to be,
and it kind of reflected my

00:03:07.820 --> 00:03:11.450
family philosophy that if you
have the right ideas, you can

00:03:11.450 --> 00:03:14.370
overcome any problem.

00:03:14.370 --> 00:03:16.560
And I particularly
like coming here.

00:03:16.560 --> 00:03:21.210
This is actually my third
time at Authors@Google.

00:03:21.210 --> 00:03:23.365
I was here in 2005.

00:03:23.365 --> 00:03:25.630
I wouldn't exactly say
Google was a young

00:03:25.630 --> 00:03:27.480
upstart at that time.

00:03:27.480 --> 00:03:30.796
It was, I think, about
4,000 people.

00:03:30.796 --> 00:03:37.310
I did it in the lunchroom
near here.

00:03:37.310 --> 00:03:39.040
The spirit hasn't changed.

00:03:39.040 --> 00:03:41.050
I think you're about
10 times the size.

00:03:41.050 --> 00:03:45.170
40,000 is like the size
of a small city.

00:03:45.170 --> 00:03:48.480
But you're still actually a
start-up compared to the

00:03:48.480 --> 00:03:52.080
opportunity, because the world
is increasingly based on

00:03:52.080 --> 00:03:53.230
knowledge and information.

00:03:53.230 --> 00:03:57.680
In fact, 65% of American workers
are knowledge workers.

00:03:57.680 --> 00:04:01.810
So the mission of organizing
and providing intelligent

00:04:01.810 --> 00:04:04.860
access to all the world's
knowledge is the most

00:04:04.860 --> 00:04:07.490
important task in the world,
and Google is clearly the

00:04:07.490 --> 00:04:09.740
leader in that.

00:04:09.740 --> 00:04:12.490
And there's tremendous
potential, because knowledge

00:04:12.490 --> 00:04:14.300
is growing exponentially.

00:04:14.300 --> 00:04:18.000
So I want to say a few words
about exponential growth and

00:04:18.000 --> 00:04:21.470
my law of accelerating returns,
which was the primary

00:04:21.470 --> 00:04:26.340
message of "The Singularity is
Near." But I think Google is

00:04:26.340 --> 00:04:29.650
actually a very good example
of that exponential growth.

00:04:29.650 --> 00:04:33.020
I happened to be on Moira
Gunn's "Tech Nation" NPR

00:04:33.020 --> 00:04:38.190
program yesterday, and she was
reminiscing about her 2001

00:04:38.190 --> 00:04:43.020
interview with Larry and Sergey,
who came in with dark

00:04:43.020 --> 00:04:44.750
suits and ties.

00:04:44.750 --> 00:04:47.530
And they were trying to explain
this cool computer

00:04:47.530 --> 00:04:48.630
they were going to create.

00:04:48.630 --> 00:04:51.190
And she didn't quite understand
what it was.

00:04:51.190 --> 00:04:53.360
And Larry said, well, it's
going to be like HAL.

00:04:53.360 --> 00:04:57.235
And then Sergey said, but
it won't kill you, so.

00:04:57.235 --> 00:05:01.040
[LAUGHTER]

00:05:01.040 --> 00:05:03.740
RAY KURZWEIL: So I think we got
the second part of that.

00:05:03.740 --> 00:05:07.710
The first part of that we have,
in the sense that Google

00:05:07.710 --> 00:05:09.995
is pretty amazing in terms
of finding information.

00:05:09.995 --> 00:05:12.300
I'm amazed by it every hour.

00:05:12.300 --> 00:05:17.290
But I think we can go further in
that direction, and that's

00:05:17.290 --> 00:05:20.036
what I'd like to talk about.

00:05:20.036 --> 00:05:24.440
You all have these billions of
pages of millions of books,

00:05:24.440 --> 00:05:29.000
and very good access to it,
but there's a lot of

00:05:29.000 --> 00:05:31.280
information there that's
reflected in the natural

00:05:31.280 --> 00:05:32.920
language ideas.

00:05:32.920 --> 00:05:36.310
And computers, now, I think can
begin to understand those.

00:05:36.310 --> 00:05:38.590
And that's something
I'm working on.

00:05:38.590 --> 00:05:41.900
That's something I talk
about in this book.

00:05:41.900 --> 00:05:45.540
And I'd like to share
that idea with you.

00:05:45.540 --> 00:05:47.650
First, I'll say a few words
about the law of

00:05:47.650 --> 00:05:49.320
accelerating returns.

00:05:49.320 --> 00:05:53.070
I mentioned I decided to be an
inventor when I was five.

00:05:53.070 --> 00:05:55.310
I realized 30 years ago
that the key to being

00:05:55.310 --> 00:05:58.480
successful is timing.

00:05:58.480 --> 00:06:01.330
Those inventors whose names you
know are the ones who got

00:06:01.330 --> 00:06:02.240
the timing right.

00:06:02.240 --> 00:06:04.850
So Larry and Sergey had
this great idea about

00:06:04.850 --> 00:06:07.330
reverse-engineering the links
on the internet to provide a

00:06:07.330 --> 00:06:08.790
better search engine,
but they did it at

00:06:08.790 --> 00:06:11.360
exactly the right time.

00:06:11.360 --> 00:06:15.710
And so in 1981, I was thinking,
my project has to

00:06:15.710 --> 00:06:17.920
make sense when I finish the
project, and the world will be

00:06:17.920 --> 00:06:20.500
a different place two, three,
four years from now.

00:06:20.500 --> 00:06:21.880
That was even true in '81.

00:06:21.880 --> 00:06:24.180
It's even more true today.

00:06:24.180 --> 00:06:27.220
Acceleration is another
feature of the law of

00:06:27.220 --> 00:06:28.430
accelerating returns.

00:06:28.430 --> 00:06:31.680
Our first communication
technology, spoken language,

00:06:31.680 --> 00:06:35.030
took hundreds of thousands
of years to develop.

00:06:35.030 --> 00:06:37.380
Then people saw that stories
were drifting.

00:06:37.380 --> 00:06:40.140
People didn't always retell the
story in the same way, so

00:06:40.140 --> 00:06:41.780
we needed some record of it.

00:06:41.780 --> 00:06:43.610
So we invented written
language.

00:06:43.610 --> 00:06:45.760
That took tens of thousands
of years.

00:06:45.760 --> 00:06:47.110
Then we needed more
efficient ways of

00:06:47.110 --> 00:06:48.250
producing written language.

00:06:48.250 --> 00:06:51.380
The printing press actually
took 400 years to

00:06:51.380 --> 00:06:52.510
reach a mass audience.

00:06:52.510 --> 00:06:56.100
I gave a speech at the
University of Basel recently

00:06:56.100 --> 00:06:59.390
on the occasion of it's
550th anniversary.

00:06:59.390 --> 00:07:02.280
It was founded 20 years after
Gutenberg's invention, right

00:07:02.280 --> 00:07:04.340
near the spot where
he invented it.

00:07:04.340 --> 00:07:07.040
And I said, well, you must have
had some of his books

00:07:07.040 --> 00:07:08.790
when you opened your doors.

00:07:08.790 --> 00:07:10.470
And they said, yes, we got
them very quickly.

00:07:10.470 --> 00:07:12.370
It was only a century later.

00:07:12.370 --> 00:07:17.700
I mean, that was the Google
of that time.

00:07:17.700 --> 00:07:21.150
It took maybe a century to find
the right information.

00:07:21.150 --> 00:07:23.130
So you didn't really find
it in your lifetime.

00:07:23.130 --> 00:07:26.700
It took 400 years for that
really to reach an appreciable

00:07:26.700 --> 00:07:27.370
number of people.

00:07:27.370 --> 00:07:32.220
The telephone reached 25% of the
US population in 50 years.

00:07:32.220 --> 00:07:34.390
The cell phone did that
in seven years.

00:07:34.390 --> 00:07:35.410
Social networks--

00:07:35.410 --> 00:07:37.910
wikis, blogs-- took
about three years.

00:07:37.910 --> 00:07:39.960
Go back three or four years ago,
most people didn't use

00:07:39.960 --> 00:07:42.680
social networks, wikis
and blogs.

00:07:42.680 --> 00:07:45.470
Ten years ago, most people
didn't use search engines.

00:07:45.470 --> 00:07:47.840
That sounds like ancient
history, but it

00:07:47.840 --> 00:07:50.050
wasn't so long ago.

00:07:50.050 --> 00:07:52.920
And then we very quickly become
dependent on these

00:07:52.920 --> 00:07:53.605
brain extenders.

00:07:53.605 --> 00:07:56.050
I mean, during that one-day
SOPA strike, I felt like a

00:07:56.050 --> 00:07:59.010
part of my brain had
gone on strike.

00:07:59.010 --> 00:08:03.320
Because there was a way around
it, but I didn't know that

00:08:03.320 --> 00:08:06.200
until the day came.

00:08:06.200 --> 00:08:11.060
So I really felt like I'm going
to lose part of my mind.

00:08:11.060 --> 00:08:13.710
Yet this was not technology
that I had

00:08:13.710 --> 00:08:17.010
even a few years earlier.

00:08:17.010 --> 00:08:19.140
What's driving this is the
exponential growth of

00:08:19.140 --> 00:08:20.450
information technology.

00:08:20.450 --> 00:08:27.430
In 1981, I began to look at
data, being an engineer.

00:08:27.430 --> 00:08:30.280
But I started out with the
common wisdom that you cannot

00:08:30.280 --> 00:08:31.740
predict the future.

00:08:31.740 --> 00:08:35.169
And that remains true as to
which company, which standard

00:08:35.169 --> 00:08:36.270
will succeed.

00:08:36.270 --> 00:08:38.570
But if you measure the
underlying properties of

00:08:38.570 --> 00:08:40.450
information technology--

00:08:40.450 --> 00:08:43.320
the first one I looked at, in a
classical one, the power of

00:08:43.320 --> 00:08:45.470
computation per constant
dollar.

00:08:45.470 --> 00:08:50.040
So the calculations per second
per constant dollar.

00:08:50.040 --> 00:08:52.890
Or the number of bits we're
moving around wirelessly, or

00:08:52.890 --> 00:08:54.610
the number of bits on the
internet, or the cost of

00:08:54.610 --> 00:08:57.350
transmitting a bit, or the
spatial resolution of

00:08:57.350 --> 00:09:00.350
brain-scanning, or the amount
of data we're downloading

00:09:00.350 --> 00:09:03.200
about the brain, or the cost of
sequencing a base pair of

00:09:03.200 --> 00:09:06.480
DNA or a genome, or the amount
of genetic data we're

00:09:06.480 --> 00:09:07.020
sequencing--

00:09:07.020 --> 00:09:10.020
I mean, these fundamental
measures follow amazingly

00:09:10.020 --> 00:09:14.370
predictable trajectories, really
belying the common

00:09:14.370 --> 00:09:16.890
wisdom that you cannot
predict the future.

00:09:16.890 --> 00:09:19.850
And what's predictable is that
they grow exponentially.

00:09:19.850 --> 00:09:21.850
And that is not intuitive.

00:09:21.850 --> 00:09:25.340
Our intuition about the future
is that it's linear, not

00:09:25.340 --> 00:09:26.620
exponential.

00:09:26.620 --> 00:09:29.390
If you ever wondered, why
do I have a brain?

00:09:29.390 --> 00:09:32.210
It's really to predict the
future, so we could predict

00:09:32.210 --> 00:09:34.860
the consequences of our
actions and inactions.

00:09:34.860 --> 00:09:38.720
So I'm walking along, and, OK,
that animal's going that way

00:09:38.720 --> 00:09:40.340
towards a rock, and I'm
going this way.

00:09:40.340 --> 00:09:44.660
We're going to meet in about
20 seconds up at that rock.

00:09:44.660 --> 00:09:46.620
I think I'll go a
different way.

00:09:46.620 --> 00:09:48.880
That proved to be useful
for survival.

00:09:48.880 --> 00:09:51.250
That became hardwired
in our brains.

00:09:51.250 --> 00:09:54.380
Those predictors of the future
are linear, and they work very

00:09:54.380 --> 00:09:57.310
well for the kinds of situations
we encountered when

00:09:57.310 --> 00:09:59.910
our brains evolved
1,000 years ago.

00:09:59.910 --> 00:10:02.650
It's not appropriate for the
progression of information

00:10:02.650 --> 00:10:03.900
technology.

00:10:03.900 --> 00:10:06.210
And I'd say the principal
difference between myself and

00:10:06.210 --> 00:10:08.560
my critics is they look at the
current situation and they

00:10:08.560 --> 00:10:11.200
make linear extrapolations.

00:10:11.200 --> 00:10:15.560
So halfway through the Genome
Project, seven years, 1% had

00:10:15.560 --> 00:10:19.180
been completed, and mainstream
scientists who were still

00:10:19.180 --> 00:10:22.230
skeptical said, I told you this
wasn't going to work.

00:10:22.230 --> 00:10:23.590
Seven years, 1%?

00:10:23.590 --> 00:10:26.130
It's going to take 700
years, like we said.

00:10:26.130 --> 00:10:28.450
My reaction was, no,
we're almost done.

00:10:28.450 --> 00:10:30.673
[LAUGHTER]

00:10:30.673 --> 00:10:33.190
RAY KURZWEIL: I mean, 1%, you're
pretty much finished.

00:10:33.190 --> 00:10:34.440
I mean that's--

00:10:36.190 --> 00:10:40.525
you can try that with your
product submission schedules.

00:10:40.525 --> 00:10:42.420
[LAUGHTER]

00:10:42.420 --> 00:10:44.150
RAY KURZWEIL: But over the
next-- it had been doubling

00:10:44.150 --> 00:10:44.800
every year.

00:10:44.800 --> 00:10:46.915
There was reason to believe
that would continue.

00:10:46.915 --> 00:10:50.210
It was only seven doublings
from 100%.

00:10:50.210 --> 00:10:51.500
And that's exactly
what happened.

00:10:51.500 --> 00:10:53.360
It kept doubling and was
finished seven years later.

00:10:53.360 --> 00:10:55.200
That has continued.

00:10:55.200 --> 00:10:57.040
Up to the present day,
the first genome

00:10:57.040 --> 00:10:58.130
cost a billion dollars.

00:10:58.130 --> 00:11:01.980
We're now down to under
$10,000, and so on.

00:11:01.980 --> 00:11:05.780
And it's true in every area
of information technology.

00:11:05.780 --> 00:11:06.850
Not everything--

00:11:06.850 --> 00:11:09.720
I mean, transportation's not yet
an information technology.

00:11:09.720 --> 00:11:11.770
But industries are converting.

00:11:11.770 --> 00:11:13.960
It's not just the gadgets
we carry around.

00:11:13.960 --> 00:11:16.185
Health and medicine has become
an information technology.

00:11:16.185 --> 00:11:17.950
I'll talk about that.

00:11:17.950 --> 00:11:19.880
The world of physical things
is going to become an

00:11:19.880 --> 00:11:22.820
information technology as
three-dimensional printing

00:11:22.820 --> 00:11:26.370
gets going, and I'll
touch on that.

00:11:26.370 --> 00:11:29.160
It's worth just examining for
a moment the difference

00:11:29.160 --> 00:11:33.320
between linear progressions,
which is our intuition, and

00:11:33.320 --> 00:11:35.740
the reality of information
technology, which is

00:11:35.740 --> 00:11:37.230
exponential.

00:11:37.230 --> 00:11:40.290
So linear goes one,
two, three, four.

00:11:40.290 --> 00:11:42.800
Exponential, which is
information technology, goes

00:11:42.800 --> 00:11:43.800
to two, four, eight, siexteen.

00:11:43.800 --> 00:11:45.900
Is that really so different?

00:11:45.900 --> 00:11:48.110
Actually, it's not
that different.

00:11:48.110 --> 00:11:51.050
A linear progression is a
good approximation of an

00:11:51.050 --> 00:11:53.930
exponential one for a short
period of time.

00:11:53.930 --> 00:11:55.510
I mean, look at an
exponential.

00:11:55.510 --> 00:11:56.630
Take a little piece of it.

00:11:56.630 --> 00:11:58.320
It looks like a straight line.

00:11:58.320 --> 00:12:01.700
It's a very bad estimate over
a long period of time.

00:12:01.700 --> 00:12:05.170
At step 30, the linear
progression's at 30.

00:12:05.170 --> 00:12:09.370
At step 30, the exponential
progression's at a billion.

00:12:09.370 --> 00:12:12.820
And that's not an idle
speculation about the future.

00:12:12.820 --> 00:12:16.450
This Android phone is several
billion times more powerful

00:12:16.450 --> 00:12:18.760
per constant dollar than the
computer I used as an

00:12:18.760 --> 00:12:20.190
undergraduate.

00:12:20.190 --> 00:12:22.580
It's a million times cheaper,
it's several thousand times

00:12:22.580 --> 00:12:25.270
more powerful, in terms of
computation, communication,

00:12:25.270 --> 00:12:27.490
memory, and so on.

00:12:27.490 --> 00:12:29.500
And it's also 100,000
times smaller.

00:12:29.500 --> 00:12:31.060
That's another exponential
progression.

00:12:31.060 --> 00:12:32.490
And we'll do both of
those things again

00:12:32.490 --> 00:12:34.660
in the next 25 years.

00:12:34.660 --> 00:12:38.470
So that gives you some idea
of what will be feasible.

00:12:38.470 --> 00:12:40.560
So this is what I
wanted to cover.

00:12:40.560 --> 00:12:42.482
Any questions on any of this?

00:12:42.482 --> 00:12:44.370
[LAUGHTER]

00:12:44.370 --> 00:12:51.745
Well, this was the first
graph I had, in 1981.

00:12:51.745 --> 00:12:54.460
So I don't know if you
can see that, but I

00:12:54.460 --> 00:12:56.600
had it through 1980.

00:12:56.600 --> 00:12:59.430
And this calculations per second
per constant dollar.

00:12:59.430 --> 00:13:03.380
It's a logarithmic scale, which
I have to take some

00:13:03.380 --> 00:13:05.600
pains to explain to
many audiences.

00:13:05.600 --> 00:13:11.290
But every labeled point on this
y-axis is 100,000 times

00:13:11.290 --> 00:13:12.930
greater than the
level below it.

00:13:12.930 --> 00:13:17.890
So this modest little uptick
represents trillions-fold

00:13:17.890 --> 00:13:20.900
increase in the amount of
computer you can get per

00:13:20.900 --> 00:13:25.580
constant dollar over the last
century, going back to the

00:13:25.580 --> 00:13:26.690
1890 census.

00:13:26.690 --> 00:13:29.540
Several billion-fold, just
since I was a student.

00:13:29.540 --> 00:13:30.790
People go, oh, Moore's law.

00:13:30.790 --> 00:13:33.260
But Moore's law is actually just
the part on the right.

00:13:33.260 --> 00:13:36.110
That had actually only been
underway for a little over a

00:13:36.110 --> 00:13:40.650
decade when I did
this estimate.

00:13:40.650 --> 00:13:44.020
This started decades before
Gordon Moore was even born.

00:13:44.020 --> 00:13:46.890
1950s, they were shrinking
vacuum tubes, making them

00:13:46.890 --> 00:13:50.170
smaller and smaller to keep this
exponential growth going.

00:13:50.170 --> 00:13:53.810
CBS predicted the election of
Eisenhower with a vacuum-tube

00:13:53.810 --> 00:13:56.860
based computer in 1952.

00:13:56.860 --> 00:13:58.075
Remember that?

00:13:58.075 --> 00:13:59.560
[LAUGHTER]

00:13:59.560 --> 00:14:03.120
A few people here might
remember it.

00:14:03.120 --> 00:14:06.790
When I first talked to Google in
2005, I don't think anybody

00:14:06.790 --> 00:14:08.430
remembered it.

00:14:08.430 --> 00:14:10.120
But finally, that hit a wall.

00:14:10.120 --> 00:14:11.950
Couldn't shrink the vacuum tubes
anymore and keep the

00:14:11.950 --> 00:14:15.360
vacuum, and that was the
end of that paradigm.

00:14:15.360 --> 00:14:18.420
But it was not the end of the
ongoing exponential, it just

00:14:18.420 --> 00:14:19.860
went to the fourth paradigm.

00:14:19.860 --> 00:14:22.160
And people have been talking
about the end of Moore's law,

00:14:22.160 --> 00:14:25.230
but the sixth paradigm will be
three-dimensional computing.

00:14:25.230 --> 00:14:28.940
We've taken baby steps
in that direction.

00:14:28.940 --> 00:14:32.700
If you talk to Justin Ratner,
the CTO of Intel, he'll show

00:14:32.700 --> 00:14:36.130
you this experimental circuits
they have that are

00:14:36.130 --> 00:14:37.430
three-dimensional

00:14:37.430 --> 00:14:39.970
self-organizing molecular circuits.

00:14:39.970 --> 00:14:42.670
Those will become practical in
the teen years, before we run

00:14:42.670 --> 00:14:44.990
out of steam with flat
integrated circuits, which is

00:14:44.990 --> 00:14:47.390
what Moore's law is all about.

00:14:47.390 --> 00:14:49.960
But the most interesting thing
about this is, just look at

00:14:49.960 --> 00:14:52.670
how smooth and predictable
a trajectory that is.

00:14:52.670 --> 00:14:54.540
People say, well, it must have
slowed down during the Great

00:14:54.540 --> 00:14:56.690
Depression, or the recent
recession--

00:14:56.690 --> 00:14:58.810
neither of which is the case.

00:14:58.810 --> 00:15:03.030
Did Google slow down during
the recent recession?

00:15:03.030 --> 00:15:05.350
I mean, these technologies
continue because we're

00:15:05.350 --> 00:15:09.060
creating the computers and the
systems and the search engines

00:15:09.060 --> 00:15:13.110
of 2013 and 2014 with the
computers of 2012.

00:15:13.110 --> 00:15:14.720
We couldn't do that in 2002.

00:15:14.720 --> 00:15:17.230
We had computers of 2002,
so we created

00:15:17.230 --> 00:15:19.450
the systems of 2003.

00:15:19.450 --> 00:15:23.680
That's why the technology
builds on itself.

00:15:23.680 --> 00:15:26.500
But it goes through thick and
thin, through war and peace,

00:15:26.500 --> 00:15:28.560
through boom times
and recessions--

00:15:28.560 --> 00:15:31.180
nothing seems to affect it.

00:15:31.180 --> 00:15:34.360
And we could talk about natural
limits, but I examine

00:15:34.360 --> 00:15:37.630
that in "The Singularity is
Near," and if you look at what

00:15:37.630 --> 00:15:41.350
we know about the physics of
computing, we do need a

00:15:41.350 --> 00:15:43.530
certain amount of matter and
energy to compute, to

00:15:43.530 --> 00:15:47.600
remember, to transmit a bit,
but it's very, very small.

00:15:47.600 --> 00:15:50.320
And based on the limits that we
understand that have been

00:15:50.320 --> 00:15:53.850
demonstrated, we can go well
into the century and develop

00:15:53.850 --> 00:15:56.530
systems that are many trillions
of times more

00:15:56.530 --> 00:15:59.090
powerful than we have today.

00:15:59.090 --> 00:16:02.290
So I won't dwell on these
examples of electronics, but

00:16:02.290 --> 00:16:05.090
you could buy one transistor
for $1 in 1968.

00:16:05.090 --> 00:16:07.230
I thought that was actually
pretty cool, at the time,

00:16:07.230 --> 00:16:10.920
because in the early '60s, I
would hang out at the surplus

00:16:10.920 --> 00:16:13.730
electronic shops on Canal
Street in New York--

00:16:13.730 --> 00:16:15.120
they're still there--

00:16:15.120 --> 00:16:17.560
and buy something this big, a
telephone relay that could

00:16:17.560 --> 00:16:19.570
switch one bit, for $50.

00:16:19.570 --> 00:16:25.190
And it was big and slow,
30-millisecond reset time.

00:16:25.190 --> 00:16:26.850
I can actually get something
much faster

00:16:26.850 --> 00:16:28.170
and smaller for $1.

00:16:28.170 --> 00:16:30.010
Today, you can get
billions for $1.

00:16:30.010 --> 00:16:32.330
And they're better, again,
because they're smaller, so

00:16:32.330 --> 00:16:34.790
the electrons have less
distance to travel.

00:16:34.790 --> 00:16:36.920
Cost for a transistor
cycle is coming down

00:16:36.920 --> 00:16:38.710
by half every year.

00:16:38.710 --> 00:16:41.400
That's a measure of
price/performance.

00:16:41.400 --> 00:16:45.090
So the fact that you can buy an
Android phone that's twice

00:16:45.090 --> 00:16:48.880
as good as the one two years ago
for half the price partly

00:16:48.880 --> 00:16:52.400
is because Google is clever,
but partly it's because of

00:16:52.400 --> 00:16:54.080
this law of accelerating
returns.

00:16:54.080 --> 00:16:55.900
It's a 50% deflation rate.

00:16:55.900 --> 00:16:58.780
We put some of that
price/performance improvement

00:16:58.780 --> 00:17:01.680
into better performance and some
of it into lower prices.

00:17:01.680 --> 00:17:05.190
So you get better products
for lower costs.

00:17:05.190 --> 00:17:09.030
And that's going to continue
for a very long time.

00:17:09.030 --> 00:17:11.540
The economists actually
worry about deflation.

00:17:11.540 --> 00:17:14.770
We had massive deflation
during the Depression.

00:17:14.770 --> 00:17:15.865
That was a different source.

00:17:15.865 --> 00:17:18.020
It was not price/performance
improvement.

00:17:18.020 --> 00:17:20.660
It was the collapse of
consumer confidence.

00:17:20.660 --> 00:17:22.940
But they're still concerned as
more and more of the economy

00:17:22.940 --> 00:17:25.579
becomes information technology,
like all of health

00:17:25.579 --> 00:17:27.920
and medicine.

00:17:27.920 --> 00:17:31.610
Peter's working on education
becoming information

00:17:31.610 --> 00:17:32.980
technology.

00:17:32.980 --> 00:17:35.570
And if you can get
the same stuff--

00:17:35.570 --> 00:17:39.070
computes, bits of communication,
base pairs of

00:17:39.070 --> 00:17:41.690
DNA, physical things
printed out on

00:17:41.690 --> 00:17:43.200
three-dimensional printers--

00:17:43.200 --> 00:17:47.530
for half the cost of a year ago,
Economics 101 will say

00:17:47.530 --> 00:17:49.030
that you will buy more.

00:17:49.030 --> 00:17:51.820
But you're not going to double
your consumption year after

00:17:51.820 --> 00:17:54.510
year, because after all,
how much do you need?

00:17:54.510 --> 00:17:56.030
You'll reach a saturation
point.

00:17:56.030 --> 00:17:59.760
So maybe you'll increase
your consumption 50%.

00:17:59.760 --> 00:18:02.480
And so the size of the economy
of these information

00:18:02.480 --> 00:18:05.750
technologies will shrink, not
as measured in bits, bytes,

00:18:05.750 --> 00:18:09.670
and base pairs, but as measured
in constant currency.

00:18:09.670 --> 00:18:12.450
And for a variety of good
reasons, that would not be a

00:18:12.450 --> 00:18:13.730
good thing.

00:18:13.730 --> 00:18:16.530
And that is not what
is happening.

00:18:16.530 --> 00:18:22.370
In fact, we more than double
our consumption each year.

00:18:22.370 --> 00:18:25.040
This is bits shipped, but I
have 50 other consumption

00:18:25.040 --> 00:18:27.920
graphs like this.

00:18:27.920 --> 00:18:31.710
Every form of information
technology has had an average

00:18:31.710 --> 00:18:36.220
growth rate of 18% per year
for the last 50 years in

00:18:36.220 --> 00:18:39.280
currency, despite the fact that
you can get twice as much

00:18:39.280 --> 00:18:41.340
of it each year for
the same price.

00:18:41.340 --> 00:18:44.575
And the reason for that is, as
we reach certain points of

00:18:44.575 --> 00:18:48.460
price/performance, whole new
applications explode.

00:18:48.460 --> 00:18:52.100
I mean search engines like we
have now, or even like we had

00:18:52.100 --> 00:18:54.920
10 years ago, weren't feasible
20 years ago.

00:18:54.920 --> 00:18:56.130
Search engines--

00:18:56.130 --> 00:18:58.340
there were search engines before
three or four years

00:18:58.340 --> 00:19:02.130
ago, but they didn't take off
because they weren't even able

00:19:02.130 --> 00:19:03.950
to upload one picture.

00:19:03.950 --> 00:19:07.750
And when the price/performance
reached a certain point, these

00:19:07.750 --> 00:19:10.310
applications exploded.

00:19:10.310 --> 00:19:14.760
And we have an insatiable
appetite for information, for

00:19:14.760 --> 00:19:15.400
knowledge--

00:19:15.400 --> 00:19:16.990
which is really information
that has

00:19:16.990 --> 00:19:19.700
been shaped by meaning.

00:19:19.700 --> 00:19:22.550
That's the mission of Google,
is to turn information into

00:19:22.550 --> 00:19:26.000
knowledge that people can
access and benefit from.

00:19:26.000 --> 00:19:28.980
So "Time Magazine" had a cover
story on my law of

00:19:28.980 --> 00:19:29.960
accelerating returns.

00:19:29.960 --> 00:19:32.710
They wanted to put a particular
computer they had

00:19:32.710 --> 00:19:35.280
covered and were fond
of on the chart.

00:19:35.280 --> 00:19:36.450
I said, well, I don't know.

00:19:36.450 --> 00:19:38.770
It might be below the chart,
because sometimes people come

00:19:38.770 --> 00:19:40.960
out with things that are not
cost-effective, and then they

00:19:40.960 --> 00:19:42.300
don't last in the marketplace.

00:19:42.300 --> 00:19:43.940
This has just come out.

00:19:43.940 --> 00:19:45.520
But it actually was
on the curve.

00:19:45.520 --> 00:19:47.300
It's the last point there.

00:19:47.300 --> 00:19:50.410
This is a curve I laid
out 30 years ago.

00:19:50.410 --> 00:19:52.460
I laid it out through 2050.

00:19:52.460 --> 00:19:56.060
But we're right where
we should be.

00:19:56.060 --> 00:19:59.990
This has been an amazingly
predictable phenomenon.

00:19:59.990 --> 00:20:03.000
Communication technology--

00:20:03.000 --> 00:20:06.850
Martin Cooper is one of the
faculty at Singularity

00:20:06.850 --> 00:20:08.025
University.

00:20:08.025 --> 00:20:15.300
He invented a product that you
sell, the mobile phone.

00:20:15.300 --> 00:20:20.200
And that's the number of bits
of data we send around

00:20:20.200 --> 00:20:21.390
wirelessly in the world.

00:20:21.390 --> 00:20:23.690
So it's over the last century.

00:20:23.690 --> 00:20:27.540
A century ago, this was Morse
code over AM radio.

00:20:27.540 --> 00:20:29.430
Today, it's 4G networks.

00:20:29.430 --> 00:20:31.660
And again this is trillions-fold
increase.

00:20:31.660 --> 00:20:34.140
That's a logarithmic scale.

00:20:34.140 --> 00:20:38.230
But look at how smooth a
progression that is.

00:20:38.230 --> 00:20:40.480
Internet data traffic.

00:20:40.480 --> 00:20:43.440
This is a graph I had just the
first few points of in the

00:20:43.440 --> 00:20:44.080
early '80s.

00:20:44.080 --> 00:20:45.530
It was the ARPAnet.

00:20:45.530 --> 00:20:48.800
And I said, wow, this is going
to be a world wide web

00:20:48.800 --> 00:20:50.900
connecting hundreds of millions
of people to each

00:20:50.900 --> 00:20:53.550
other and to vast knowledge
resources by the late '90s.

00:20:53.550 --> 00:20:54.850
I wrote that in the '80s.

00:20:54.850 --> 00:20:57.410
And people thought that was
ridiculous, when the entire

00:20:57.410 --> 00:20:59.650
defense budget could only tie
together a few thousand

00:20:59.650 --> 00:21:01.270
scientists.

00:21:01.270 --> 00:21:03.160
But that's the power of
exponential growth.

00:21:03.160 --> 00:21:04.890
That is what happened.

00:21:04.890 --> 00:21:08.150
That's the same data on the
right, seen on a linear scale.

00:21:08.150 --> 00:21:10.080
That's how we experience
the world.

00:21:10.080 --> 00:21:12.840
So to the casual observer, it
looked like, whoa, the World

00:21:12.840 --> 00:21:15.700
Wide Web is a new thing,
came out of nowhere.

00:21:15.700 --> 00:21:17.410
But you could see it coming.

00:21:17.410 --> 00:21:21.050
And you can see revolutions
coming if you look at these

00:21:21.050 --> 00:21:21.760
progressions.

00:21:21.760 --> 00:21:26.060
And that is what I advise
young companies to do.

00:21:26.060 --> 00:21:28.910
Because I get some business
plans and do some entering,

00:21:28.910 --> 00:21:32.010
and very often, these plans talk
about the world three,

00:21:32.010 --> 00:21:34.450
four years from now, like
nothing is going to change.

00:21:34.450 --> 00:21:36.730
And you only have to look at the
last three or four years

00:21:36.730 --> 00:21:38.230
to see that that's
not correct.

00:21:41.140 --> 00:21:43.445
I could talk for a long time
about this phenomenon.

00:21:46.380 --> 00:21:49.710
But we are turning health and
medicine into an information

00:21:49.710 --> 00:21:52.280
technology.

00:21:52.280 --> 00:21:53.690
I mentioned the Genome
Project.

00:21:53.690 --> 00:21:56.790
But we can actually reprogram
this outdated

00:21:56.790 --> 00:21:57.510
software in our bodies.

00:21:57.510 --> 00:21:59.800
How long do you go without
updating your

00:21:59.800 --> 00:22:00.920
Android phone software?

00:22:00.920 --> 00:22:03.065
This is probably updating
itself right now.

00:22:05.570 --> 00:22:08.780
But I'm still walking around
with software in my body that

00:22:08.780 --> 00:22:11.060
evolved thousands of years ago--
like, for example, the

00:22:11.060 --> 00:22:14.500
fat insulin receptor gene, which
says, hold onto every

00:22:14.500 --> 00:22:16.780
calorie 'cause the next hunting
season may not work

00:22:16.780 --> 00:22:17.940
out so well.

00:22:17.940 --> 00:22:20.770
That was a good idea
1,000 years ago.

00:22:20.770 --> 00:22:22.970
You worked all day to
get a few calories.

00:22:22.970 --> 00:22:24.640
There were no refrigerators,
so you stored

00:22:24.640 --> 00:22:26.630
them in your fat cells.

00:22:26.630 --> 00:22:29.640
I'd like to tell my fat insulin
receptor gene, you

00:22:29.640 --> 00:22:33.240
don't need to do that anymore.

00:22:33.240 --> 00:22:38.180
I'm confident the next hunting
season will be good at the

00:22:38.180 --> 00:22:38.630
supermarket.

00:22:38.630 --> 00:22:41.650
[LAUGHTER]

00:22:41.650 --> 00:22:43.390
RAY KURZWEIL: So that was
actually tried in animal

00:22:43.390 --> 00:22:43.910
experiments.

00:22:43.910 --> 00:22:46.200
We have a number of ways of
turning genes off, like RNA

00:22:46.200 --> 00:22:48.520
interference.

00:22:48.520 --> 00:22:51.110
And these animals ate ravenously
and remained slim

00:22:51.110 --> 00:22:53.260
and got the health benefits of
caloric restriction while

00:22:53.260 --> 00:22:54.180
doing the opposite.

00:22:54.180 --> 00:22:57.090
They lived 20% longer.

00:22:57.090 --> 00:22:59.190
They're working with a drug
company to bring that to the

00:22:59.190 --> 00:23:01.260
human market.

00:23:01.260 --> 00:23:04.790
I'm on the board of a company
that takes lung cells out of

00:23:04.790 --> 00:23:07.100
the body of patients who
have a disease caused

00:23:07.100 --> 00:23:08.740
by a missing gene.

00:23:08.740 --> 00:23:10.840
So if you're missing this gene,
you probably will get

00:23:10.840 --> 00:23:15.170
this terminal disease, pulmonary
hypertension.

00:23:15.170 --> 00:23:19.720
So they scrape out lung cells
from the throat, add a gene in

00:23:19.720 --> 00:23:23.810
vitro, and then inspect that
it got done correctly,

00:23:23.810 --> 00:23:25.670
replicate the cell several
million-fold--

00:23:25.670 --> 00:23:26.910
that's another new
technology--

00:23:26.910 --> 00:23:29.610
inject it back in the body, it
goes through the bloodstream.

00:23:29.610 --> 00:23:31.980
The body recognizes them
as lung cells.

00:23:31.980 --> 00:23:35.740
You've now added millions of
cells with that patient's DNA,

00:23:35.740 --> 00:23:37.780
but with the gene they're
missing, and this has actually

00:23:37.780 --> 00:23:41.750
cured this disease in successful
human trials, and

00:23:41.750 --> 00:23:47.570
it's doing its Phase III trial
now before it gets approved.

00:23:47.570 --> 00:23:51.820
There are hundreds of examples
of reprogramming biology.

00:23:51.820 --> 00:23:55.860
My father had a heart attack
in 1961, damaged his heart,

00:23:55.860 --> 00:23:59.430
which is the case of 50% of all
heart attack survivors,

00:23:59.430 --> 00:24:00.400
have a damaged heart.

00:24:00.400 --> 00:24:01.940
He could hardly walk.

00:24:01.940 --> 00:24:04.960
He died of that in 1970.

00:24:04.960 --> 00:24:07.340
Up until very recently, there's
nothing you could do

00:24:07.340 --> 00:24:08.710
about it, because the
heart does not

00:24:08.710 --> 00:24:10.220
rejuvenate itself naturally.

00:24:10.220 --> 00:24:14.480
You can now reprogram stem cells
to rejuvenate the heart.

00:24:14.480 --> 00:24:16.540
Now, I've talked to people who
could hardly walk, and now

00:24:16.540 --> 00:24:19.260
they're normal.

00:24:19.260 --> 00:24:22.090
We are growing organs already.

00:24:22.090 --> 00:24:26.210
Some of these simpler organs
are being used in humans.

00:24:26.210 --> 00:24:30.980
Other ones are now being
implanted in animals, where we

00:24:30.980 --> 00:24:34.210
lay down the scaffold with
three-dimensional printers and

00:24:34.210 --> 00:24:36.500
then use the three-dimensional
printer to populate it with

00:24:36.500 --> 00:24:40.280
stem cells and regrow, for
example, a kidney.

00:24:40.280 --> 00:24:41.710
So all of this is coming.

00:24:41.710 --> 00:24:42.840
It's a complex area.

00:24:42.840 --> 00:24:46.070
But the point is that health
and medicine has become an

00:24:46.070 --> 00:24:49.590
information technology, and
therefore it's subject to this

00:24:49.590 --> 00:24:51.390
law of accelerating returns.

00:24:51.390 --> 00:24:55.070
So these technologies, which are
already beginning to enter

00:24:55.070 --> 00:24:57.220
clinical practice, they're going
to be 1,000 times more

00:24:57.220 --> 00:24:59.760
powerful in 10 years and
a million times more

00:24:59.760 --> 00:25:01.530
powerful in 20 years.

00:25:01.530 --> 00:25:05.390
It gives you some idea
of what's coming.

00:25:05.390 --> 00:25:11.300
If I want to send you a music
album or a movie or a book,

00:25:11.300 --> 00:25:13.760
just a few years ago, I'd send
you a FedEx package.

00:25:13.760 --> 00:25:18.590
I can now send you a Gmail
message with those products as

00:25:18.590 --> 00:25:19.700
an attachment.

00:25:19.700 --> 00:25:21.990
I can also send you these
musical instruments, if you

00:25:21.990 --> 00:25:25.560
have the three-dimensional
printer.

00:25:25.560 --> 00:25:30.230
And this is a revolution
right before the storm.

00:25:30.230 --> 00:25:31.290
They've been expensive.

00:25:31.290 --> 00:25:33.400
They were hundreds of thousands
of dollars and tens,

00:25:33.400 --> 00:25:34.910
now thousands.

00:25:34.910 --> 00:25:37.920
They will, in a number of
years, go sub-$1,000.

00:25:37.920 --> 00:25:41.000
The resolution is improving at
a rate of about 100 in 3-D

00:25:41.000 --> 00:25:42.560
volume per decade.

00:25:42.560 --> 00:25:45.340
It's still over several
microns.

00:25:45.340 --> 00:25:47.270
Needs to be sub-micron.

00:25:47.270 --> 00:25:50.100
The range of materials
is increasing.

00:25:50.100 --> 00:25:54.790
Ultimately, a substantial
fraction of manufacturing will

00:25:54.790 --> 00:25:58.850
be done this way, turning
information files into

00:25:58.850 --> 00:26:00.680
physical products.

00:26:00.680 --> 00:26:03.990
Today, you can print out 70%
of the parts you need with

00:26:03.990 --> 00:26:06.410
your three-dimensional printer
to create another

00:26:06.410 --> 00:26:07.630
three-dimensional printer.

00:26:07.630 --> 00:26:10.250
[LAUGHTER]

00:26:10.250 --> 00:26:13.025
RAY KURZWEIL: That will be 100%
in five to eight years.

00:26:15.770 --> 00:26:18.170
So that brings me
to the brain.

00:26:18.170 --> 00:26:22.640
And I want to spend
some time on that.

00:26:22.640 --> 00:26:26.590
I've been thinking about this
topic for 50 years, actually,

00:26:26.590 --> 00:26:28.390
thinking about thinking.

00:26:28.390 --> 00:26:32.090
I wrote a paper when
I was 14--

00:26:32.090 --> 00:26:36.110
that's 50 years ago--

00:26:36.110 --> 00:26:40.255
that basically described the
human brain as a large number

00:26:40.255 --> 00:26:42.110
of pattern recognizers.

00:26:42.110 --> 00:26:44.330
That was my Westinghouse
Science Talent Search

00:26:44.330 --> 00:26:46.500
submission, and I got to
meet President Johnson.

00:26:46.500 --> 00:26:50.360
And I did a program that did
pattern recognition on musical

00:26:50.360 --> 00:26:52.860
melodies and then wrote original
music with the

00:26:52.860 --> 00:26:54.390
patterns it had discovered.

00:26:54.390 --> 00:26:56.920
So you could feed in Chopin,
and it would write, then,

00:26:56.920 --> 00:27:00.630
music like it was a student of
Chopin or Mozart, and you

00:27:00.630 --> 00:27:05.130
could recognize which composer
had been analyzed with the

00:27:05.130 --> 00:27:07.740
original music that
it was composing.

00:27:07.740 --> 00:27:11.530
And this book actually
articulates a

00:27:11.530 --> 00:27:14.950
very consistent thesis.

00:27:14.950 --> 00:27:17.550
Pattern recognition is
what we do well.

00:27:17.550 --> 00:27:19.940
We're not very good at
logical thinking.

00:27:19.940 --> 00:27:22.800
Computers do a far better
job of that.

00:27:22.800 --> 00:27:24.170
One of the predictions
I made in the early

00:27:24.170 --> 00:27:26.700
'80s was that by '97--

00:27:26.700 --> 00:27:28.320
actually, I said '98--

00:27:28.320 --> 00:27:30.950
a computer would take the World
Chess Championship.

00:27:30.950 --> 00:27:33.100
I also predicted that when
that happened, we would

00:27:33.100 --> 00:27:35.850
immediately dismiss chess as
being of any significance.

00:27:38.380 --> 00:27:40.620
Both of those things happened
in '97 when Deep

00:27:40.620 --> 00:27:42.325
Blue defeated Kasparov.

00:27:42.325 --> 00:27:44.930
And people said, well, of
course that's true.

00:27:44.930 --> 00:27:48.210
Chess is a logic game, and
computers are logic machines,

00:27:48.210 --> 00:27:50.250
so we would expect them
to do a better job

00:27:50.250 --> 00:27:52.110
than humans on chess.

00:27:52.110 --> 00:27:55.360
But what they will never do is
be able to understand the

00:27:55.360 --> 00:27:59.370
vagaries and subtleties and
ambiguities of human language.

00:27:59.370 --> 00:28:03.110
So already we're seeing
that being overturned.

00:28:03.110 --> 00:28:06.180
And there's actually a pretty
impressive range--

00:28:06.180 --> 00:28:08.190
it's just a first step--

00:28:08.190 --> 00:28:11.130
but an impressive range of
language that you can say to

00:28:11.130 --> 00:28:14.780
systems like Google now, and it
will understand you pretty

00:28:14.780 --> 00:28:18.290
well, and actually begin to
develop a model of who you

00:28:18.290 --> 00:28:23.420
are, something that
Siri doesn't do.

00:28:23.420 --> 00:28:27.110
How many of you can answer
this "Jeopardy" question?

00:28:27.110 --> 00:28:30.300
"A long tiresome speech
delivered by a frothy pie

00:28:30.300 --> 00:28:36.136
topping." What is a
meringue harangue?

00:28:36.136 --> 00:28:38.710
[LAUGHTER]

00:28:38.710 --> 00:28:40.710
So Watson got that correct.

00:28:40.710 --> 00:28:43.700
The two humans who were the best
human "Jeopardy" players

00:28:43.700 --> 00:28:48.090
ever did not get it.

00:28:48.090 --> 00:28:50.475
And Watson got a higher
score than the best

00:28:50.475 --> 00:28:54.590
two humans put together.

00:28:54.590 --> 00:28:56.000
And there's a lot of

00:28:56.000 --> 00:28:57.390
misunderstandings about Watson.

00:28:57.390 --> 00:28:59.480
People say, well, it's not
really doing any true

00:28:59.480 --> 00:29:01.790
understanding of language,
because it's just doing

00:29:01.790 --> 00:29:05.750
statistical analysis of words.

00:29:05.750 --> 00:29:06.950
Actually, what it does--

00:29:06.950 --> 00:29:10.270
I mean, it actually has many
different modules.

00:29:10.270 --> 00:29:14.340
What the IBM engineers did is
create a framework called

00:29:14.340 --> 00:29:18.400
UIMA, which runs these different
systems and is able

00:29:18.400 --> 00:29:21.790
to analyze their strengths and
weaknesses and combine them.

00:29:21.790 --> 00:29:25.010
So actually, the engineers
in charge of Watson don't

00:29:25.010 --> 00:29:27.750
necessarily understand
all of those modules.

00:29:27.750 --> 00:29:31.060
The ones I think that are most
effective are ones that are

00:29:31.060 --> 00:29:32.760
statistical, but they're
not just doing

00:29:32.760 --> 00:29:34.550
statistics on word sequences.

00:29:34.550 --> 00:29:38.120
They're building a hierarchical
model with a

00:29:38.120 --> 00:29:40.270
whole field of probabilities
at different

00:29:40.270 --> 00:29:43.080
levels of the hierarchy.

00:29:43.080 --> 00:29:46.790
And if that does not represent
a true understanding of the

00:29:46.790 --> 00:29:49.140
material, then humans have no
true understanding, either,

00:29:49.140 --> 00:29:52.660
because that is how the
neocortex works.

00:29:52.660 --> 00:29:56.160
And another misconception is
that every fact was sort of

00:29:56.160 --> 00:29:58.890
programmed in some language
like Lisp.

00:29:58.890 --> 00:30:02.230
In fact, Watson got its
knowledge by reading Wikipedia

00:30:02.230 --> 00:30:04.320
and several other encyclopedias,
200 million

00:30:04.320 --> 00:30:07.350
pages of natural language
documents.

00:30:07.350 --> 00:30:09.850
And it is true that it actually
doesn't do as good a

00:30:09.850 --> 00:30:12.810
job on any page as a human.

00:30:12.810 --> 00:30:16.650
So you could read a page, and if
you knew nothing about the

00:30:16.650 --> 00:30:21.310
presidency, you'd conclude,
wow, there's a 95% chance

00:30:21.310 --> 00:30:24.800
Barack Obama's president, having
read that one page.

00:30:24.800 --> 00:30:26.935
And Watson will read it and come
out with a conclusion,

00:30:26.935 --> 00:30:31.240
oh, there's a 58% chance that
Barack Obama is president.

00:30:31.240 --> 00:30:34.660
So it didn't do as good a job
of understanding that page.

00:30:34.660 --> 00:30:38.690
But it has read 200 million
pages, and maybe 100,000 of

00:30:38.690 --> 00:30:41.680
those have to do with Barack
Obama being president.

00:30:41.680 --> 00:30:45.140
And it can then combine all
those probabilities using

00:30:45.140 --> 00:30:47.490
sound probability theory--

00:30:47.490 --> 00:30:49.530
Bayes' theorem and so on--

00:30:49.530 --> 00:30:53.080
and conclude that there's a
99.99% chance that Barack

00:30:53.080 --> 00:30:55.680
Obama is president.

00:30:55.680 --> 00:31:00.090
It has total recall of those
200 million pages and can

00:31:00.090 --> 00:31:04.140
analyze the cross-implications
in three seconds.

00:31:04.140 --> 00:31:09.370
It's just a first step, but that
is the kind of capability

00:31:09.370 --> 00:31:10.890
that we're leading to.

00:31:10.890 --> 00:31:15.250
My vision of search engines in
the not-too-distant future is

00:31:15.250 --> 00:31:17.120
that they won't wait to
be asked questions.

00:31:17.120 --> 00:31:21.170
They'll be listening in
on our conversations--

00:31:21.170 --> 00:31:25.880
what we say, what we write, what
we read, what we hear, if

00:31:25.880 --> 00:31:28.410
you let them, and I believe
people will, because it'll be

00:31:28.410 --> 00:31:31.780
useful to have an intelligent
assistant like this--

00:31:31.780 --> 00:31:33.470
and it will anticipate
your needs.

00:31:33.470 --> 00:31:35.900
So suddenly, it might pop up and
say, oh, just yesterday,

00:31:35.900 --> 00:31:40.270
you were talking about, if
only we could have better

00:31:40.270 --> 00:31:43.280
bioavailable means of
phosphatidylcholine.

00:31:43.280 --> 00:31:46.180
Well, here's a study that came
out 36 minutes ago on just

00:31:46.180 --> 00:31:47.680
that topic.

00:31:47.680 --> 00:31:51.690
If it sees you struggling in a
conversation to come up with

00:31:51.690 --> 00:31:54.990
the name of that actress, right
in your field of view on

00:31:54.990 --> 00:31:59.230
your Google Glass, you'll get
information about that

00:31:59.230 --> 00:32:01.010
actress, not even having
asked for it.

00:32:01.010 --> 00:32:03.500
It can just see you
needed that.

00:32:03.500 --> 00:32:05.860
Obviously, that could be
annoying if it's really

00:32:05.860 --> 00:32:07.850
information you don't want.

00:32:07.850 --> 00:32:11.320
That'll be the key.

00:32:11.320 --> 00:32:14.320
But actually, we very much
want this information.

00:32:14.320 --> 00:32:19.320
I mean, people are constantly
Googling something at dinner.

00:32:19.320 --> 00:32:22.190
But we don't even want to have
to put that information in.

00:32:22.190 --> 00:32:24.020
An intelligent assistant
should be

00:32:24.020 --> 00:32:26.560
listening to what we say.

00:32:26.560 --> 00:32:32.300
So some of the best evidence
for the thesis I've come up

00:32:32.300 --> 00:32:36.910
with on how the neocortex works
has emerged just as I

00:32:36.910 --> 00:32:37.960
was sending off the book.

00:32:37.960 --> 00:32:40.840
Actually, four times I was
about to send it to the

00:32:40.840 --> 00:32:42.940
publisher and said, no,
wait, this great

00:32:42.940 --> 00:32:43.930
research just came out.

00:32:43.930 --> 00:32:46.430
I've got to include this.

00:32:46.430 --> 00:32:48.460
And we actually delayed
the book as a result.

00:32:48.460 --> 00:32:51.780
The publisher wasn't happy
with that, but these were

00:32:51.780 --> 00:32:56.810
great pieces of research
to support the thesis.

00:32:56.810 --> 00:33:02.400
The thesis is that there are
modules in the brain that are

00:33:02.400 --> 00:33:07.490
comprised of about 100 neurons,
and that each one of

00:33:07.490 --> 00:33:11.030
these recognizes a pattern and
is capable of wiring itself,

00:33:11.030 --> 00:33:14.945
literally with a wire,
biological wire, an axon and a

00:33:14.945 --> 00:33:18.970
dendrite, to other modules to
create this hierarchy that the

00:33:18.970 --> 00:33:21.320
neocortex represents.

00:33:21.320 --> 00:33:24.400
And that hierarchy doesn't
exist when

00:33:24.400 --> 00:33:26.360
the brain is created.

00:33:26.360 --> 00:33:30.330
Even before we're born, we
start building this, one

00:33:30.330 --> 00:33:33.160
conceptual layer at a time.

00:33:33.160 --> 00:33:35.620
And that's actually the secret
of human thought, the ability

00:33:35.620 --> 00:33:37.840
to build these modules.

00:33:37.840 --> 00:33:42.560
One piece of research that came
out just as I was sending

00:33:42.560 --> 00:33:46.720
off the book is that the
neocortex is comprised of

00:33:46.720 --> 00:33:49.240
these modules of about
100 neurons.

00:33:49.240 --> 00:33:51.440
The wiring and structure
of those 100

00:33:51.440 --> 00:33:53.100
neurons is not plastic.

00:33:53.100 --> 00:33:54.950
It's stable throughout life.

00:33:54.950 --> 00:33:59.030
It is the connections between
these modules which are

00:33:59.030 --> 00:34:01.900
dynamic and plastic
and are created.

00:34:01.900 --> 00:34:05.490
And our neocortex creates our
thoughts, but our thoughts

00:34:05.490 --> 00:34:07.960
create our brain, in terms of
these connections and the

00:34:07.960 --> 00:34:10.780
patterns that each
module learns.

00:34:10.780 --> 00:34:13.139
This is different from neural
nets, and I've never been a

00:34:13.139 --> 00:34:14.770
fan of neural nets.

00:34:14.770 --> 00:34:19.659
I was one of the pioneers of
hierarchical hidden Markov

00:34:19.659 --> 00:34:22.179
models in the '80s and '90s
and used that for speech

00:34:22.179 --> 00:34:26.250
recognition, and today, that is
the dominant technique in

00:34:26.250 --> 00:34:29.360
speech recognition, speech
synthesis, character

00:34:29.360 --> 00:34:30.230
recognition.

00:34:30.230 --> 00:34:33.050
It's one of the popular
techniques in natural language

00:34:33.050 --> 00:34:34.409
understanding.

00:34:34.409 --> 00:34:37.150
And it's really the closest
mathematical equivalent to

00:34:37.150 --> 00:34:38.719
what I'm talking about.

00:34:38.719 --> 00:34:43.250
This 100-neuron module is more
complex than one neuron in a

00:34:43.250 --> 00:34:44.409
neural net.

00:34:44.409 --> 00:34:48.199
It's capable of dynamically
learning a pattern,

00:34:48.199 --> 00:34:50.270
recognizing the pattern
even if parts of it

00:34:50.270 --> 00:34:53.139
are occluded or missing.

00:34:53.139 --> 00:34:56.179
It can actually tell other
pattern recognizers to expect

00:34:56.179 --> 00:35:01.310
a pattern because it's almost
recognized a pattern and

00:35:01.310 --> 00:35:03.740
another part's coming, and
so lower-level pattern

00:35:03.740 --> 00:35:07.160
recognizers should be
alert for that.

00:35:07.160 --> 00:35:10.010
It's capable of creating
these connections up

00:35:10.010 --> 00:35:12.250
and down the hierarchy.

00:35:12.250 --> 00:35:15.170
And that's much more
complex than one

00:35:15.170 --> 00:35:17.010
neuron in a neural net.

00:35:17.010 --> 00:35:21.490
So the neural net is based on
one neuron, either a model of

00:35:21.490 --> 00:35:26.920
it that we have in synthetic
neural nets or, in theory, the

00:35:26.920 --> 00:35:28.950
neural net that the
brain represents.

00:35:28.950 --> 00:35:32.860
And that's not the right
building block, either for AI

00:35:32.860 --> 00:35:35.770
or for the brain.

00:35:35.770 --> 00:35:40.290
There was this recent research
at Google that showed an

00:35:40.290 --> 00:35:46.160
ability to do image recognition
with a neural net

00:35:46.160 --> 00:35:50.910
without any labeling
of the data.

00:35:50.910 --> 00:35:57.930
It was impressive, but it only
recognized 15% accuracy.

00:35:57.930 --> 00:36:04.050
I think a much better model is
based on not having the neuron

00:36:04.050 --> 00:36:05.160
as the building blocks.

00:36:05.160 --> 00:36:07.460
The building block are
these modules.

00:36:07.460 --> 00:36:10.290
And we have about 30 billion
neurons in the neocortex.

00:36:10.290 --> 00:36:13.380
There's about 300 million of
these pattern recognizers.

00:36:13.380 --> 00:36:15.720
Now a word about
the neocortex.

00:36:15.720 --> 00:36:18.110
It is this part of the
brain where we do

00:36:18.110 --> 00:36:19.210
hierarchical thinking.

00:36:19.210 --> 00:36:21.940
It can think in hierarchies, and
it can solve problems in

00:36:21.940 --> 00:36:23.070
hierarchies.

00:36:23.070 --> 00:36:26.960
And it can see a solution to a
problem and then reapply it in

00:36:26.960 --> 00:36:29.760
situations that might be
a little different.

00:36:29.760 --> 00:36:32.220
And only mammals have
a neocortex.

00:36:32.220 --> 00:36:35.820
So 100 million years ago,
these mammals emerged,

00:36:35.820 --> 00:36:39.660
rodent-like creatures with a
neocortex that was the size of

00:36:39.660 --> 00:36:43.980
a postage stamp, about as thin
as a postage stamp, flat and

00:36:43.980 --> 00:36:46.610
smooth, and it covered
the brain.

00:36:46.610 --> 00:36:49.130
But it was capable of a
certain amount of this

00:36:49.130 --> 00:36:51.060
hierarchical thinking.

00:36:51.060 --> 00:36:58.110
So these mammals could solve
problems quickly, or could see

00:36:58.110 --> 00:37:01.570
another member of its species
solve a problem and learn it

00:37:01.570 --> 00:37:03.690
in a matter of hours.

00:37:03.690 --> 00:37:06.340
Animal species without a
neocortex could learn, too,

00:37:06.340 --> 00:37:08.830
but not in the course
of one lifetime.

00:37:08.830 --> 00:37:10.610
They had pre-programmed
behaviors.

00:37:10.610 --> 00:37:13.270
Those behaviors could evolve in
biological evolution, but

00:37:13.270 --> 00:37:15.090
that would take thousands
of lifetimes.

00:37:15.090 --> 00:37:18.560
So over thousands or tens of
thousands of years, they could

00:37:18.560 --> 00:37:21.780
gradually change
their behavior.

00:37:21.780 --> 00:37:24.000
And that was OK, because
the environment

00:37:24.000 --> 00:37:25.430
changed that slowly.

00:37:25.430 --> 00:37:28.150
So there would be environmental
changes that

00:37:28.150 --> 00:37:30.520
required an accommodation
in behavior over

00:37:30.520 --> 00:37:32.310
thousands of years.

00:37:32.310 --> 00:37:36.210
But then 65 million years ago,
there was a cataclysmic event

00:37:36.210 --> 00:37:38.830
that happened very quickly
called the Cretaceous

00:37:38.830 --> 00:37:40.320
extinction event.

00:37:40.320 --> 00:37:42.330
And we see archaeological
evidence of

00:37:42.330 --> 00:37:44.390
that around the globe.

00:37:44.390 --> 00:37:47.350
There's a layer that represents
this catastrophic

00:37:47.350 --> 00:37:50.210
change in the environment that
happened very quickly.

00:37:50.210 --> 00:37:52.260
And there are theories
about that having

00:37:52.260 --> 00:37:53.070
to do with a meteor.

00:37:53.070 --> 00:37:56.390
But it's very clear that there
was a sudden change in the

00:37:56.390 --> 00:37:58.380
environment at that time.

00:37:58.380 --> 00:38:00.810
And the animals that didn't
have a neocortex and that

00:38:00.810 --> 00:38:04.040
couldn't adjust quickly,
thousands of those

00:38:04.040 --> 00:38:04.980
species died out.

00:38:04.980 --> 00:38:11.270
That's when the mammals took
over their ecological niche of

00:38:11.270 --> 00:38:13.010
small- and medium-sized
animals.

00:38:19.780 --> 00:38:23.010
So to anthropomorphize,
biological evolution said,

00:38:23.010 --> 00:38:27.230
wow, this neocortex is a pretty
good design, and it

00:38:27.230 --> 00:38:34.010
kept growing it in size through
increasingly complex

00:38:34.010 --> 00:38:36.140
mammal species.

00:38:36.140 --> 00:38:39.300
By the time it got to primates,
it's no longer a

00:38:39.300 --> 00:38:40.050
smooth sheet.

00:38:40.050 --> 00:38:43.050
It's got all these convolutions
and ridges to

00:38:43.050 --> 00:38:45.050
increase its surface area.

00:38:45.050 --> 00:38:46.650
It's still a flat structure.

00:38:46.650 --> 00:38:49.650
If you take the human neocortex,
you can stretch it

00:38:49.650 --> 00:38:53.120
out into a flat structure the
size of a large table napkin.

00:38:53.120 --> 00:38:54.480
It's about the same thickness.

00:38:54.480 --> 00:38:55.930
It's still thin.

00:38:55.930 --> 00:38:58.130
But it has so many convolutions
and ridges, it

00:38:58.130 --> 00:39:01.360
actually comprises
80% of the brain.

00:39:01.360 --> 00:39:05.350
And that's where we do our
hierarchical thinking.

00:39:05.350 --> 00:39:08.830
So if you take a primate, it
also has one with convolutions

00:39:08.830 --> 00:39:11.720
and ridges, but the innovation
in homo sapiens is we have

00:39:11.720 --> 00:39:15.960
this large forehead to squeeze
in more of this neocortex.

00:39:15.960 --> 00:39:20.290
And that greater quantity was
the enabling factor for the

00:39:20.290 --> 00:39:23.530
qualitative leap we had of being
able to make inventions

00:39:23.530 --> 00:39:28.070
like language and art and
science and Nexus phones.

00:39:28.070 --> 00:39:30.420
[LAUGHTER]

00:39:30.420 --> 00:39:33.240
RAY KURZWEIL: So how
does this work?

00:39:33.240 --> 00:39:35.490
Well, for one thing, our ability
to actually see inside

00:39:35.490 --> 00:39:38.980
the brain and confirm these
types of insights is growing

00:39:38.980 --> 00:39:39.730
exponentially.

00:39:39.730 --> 00:39:43.370
Different types of brain
scanning are growing at an

00:39:43.370 --> 00:39:44.230
exponential rate.

00:39:44.230 --> 00:39:45.940
We can now see your brain
create your thoughts.

00:39:45.940 --> 00:39:48.650
We can see your thoughts
create your brain.

00:39:48.650 --> 00:39:51.840
We can see individual links and
neural connections forming

00:39:51.840 --> 00:39:53.090
in real time.

00:39:55.480 --> 00:39:59.220
And another piece of research
that came out just as I was

00:39:59.220 --> 00:40:02.490
sending off the book is that
at the beginning of life,

00:40:02.490 --> 00:40:07.080
there is this very uniform
wiring of the neocortex,

00:40:07.080 --> 00:40:09.600
basically connections
in waiting.

00:40:09.600 --> 00:40:12.640
So you have one pattern
recognizer, and it wants to

00:40:12.640 --> 00:40:14.130
connect itself, let's
say, to one at a

00:40:14.130 --> 00:40:15.830
higher conceptual level.

00:40:15.830 --> 00:40:19.440
It has actually connect
a wire.

00:40:19.440 --> 00:40:23.130
There's actually a grid there,
like avenues and streets of

00:40:23.130 --> 00:40:25.460
Manhattan, and it finds the
right avenue and the right

00:40:25.460 --> 00:40:27.860
street and makes the
final connections.

00:40:27.860 --> 00:40:32.460
And we actually see that process
in real time now,

00:40:32.460 --> 00:40:34.210
inside a living brain.

00:40:34.210 --> 00:40:38.000
And then it actually finalizes
that connection.

00:40:38.000 --> 00:40:40.510
And then the connections that
are never used die away.

00:40:40.510 --> 00:40:43.820
About half of the connections
that exist in a newborn

00:40:43.820 --> 00:40:46.410
actually go away by the time
you're two years old.

00:40:49.170 --> 00:40:55.980
So to take a simplified example
of how this works,

00:40:55.980 --> 00:40:59.020
these pattern recognizers learn
patterns, and there are

00:40:59.020 --> 00:41:01.870
different levels of the
conceptual hierarchy.

00:41:01.870 --> 00:41:04.720
And there's a lot of redundancy,
which is one way

00:41:04.720 --> 00:41:08.220
it deals with uncertainty and
one way it can deal with

00:41:08.220 --> 00:41:10.460
variations in patterns.

00:41:10.460 --> 00:41:14.660
So I have a bunch of pattern
recognizers that have learned

00:41:14.660 --> 00:41:17.630
to recognize a cross-bar
in a capital A.

00:41:17.630 --> 00:41:19.780
And that's all they
care about.

00:41:23.010 --> 00:41:25.680
Some exciting new technology
or a pretty girl could walk

00:41:25.680 --> 00:41:26.930
by, it doesn't care.

00:41:29.350 --> 00:41:31.420
But when it sees a cross-bar
in a capital A,

00:41:31.420 --> 00:41:32.930
it goes, whoa, crossbar!

00:41:32.930 --> 00:41:36.210
[LAUGHTER]

00:41:36.210 --> 00:41:38.170
And it sends up a signal--

00:41:38.170 --> 00:41:41.330
I believe this is
not on or off.

00:41:41.330 --> 00:41:44.020
The whole system is a network
of probabilities.

00:41:44.020 --> 00:41:46.140
But it says there's a
high probability we

00:41:46.140 --> 00:41:47.550
have a crossbar here.

00:41:47.550 --> 00:41:50.080
At that next higher level, it's
getting different inputs,

00:41:50.080 --> 00:41:53.610
and it might then fire with
a high probability--

00:41:53.610 --> 00:41:57.410
ah, capital A. And at a higher
level, a pattern recognizer

00:41:57.410 --> 00:42:00.450
might think, hm, there's a very
good probability that the

00:42:00.450 --> 00:42:03.610
word "apple" is printed here.

00:42:03.610 --> 00:42:07.430
And in another part of the
visual cortex, a pattern

00:42:07.430 --> 00:42:09.960
recognizer might go, oh, an
actual physical apple.

00:42:09.960 --> 00:42:15.030
And in another region, a pattern
recognizer might go,

00:42:15.030 --> 00:42:18.070
oh, someone just said the word
"apple." Go up a number of

00:42:18.070 --> 00:42:21.990
levels further, where you're not
getting input at a higher

00:42:21.990 --> 00:42:25.550
level of conceptual hierarchy,
so it's connected to multiple

00:42:25.550 --> 00:42:28.860
senses, it may see a certain
fabric, smell a certain

00:42:28.860 --> 00:42:32.530
perfume, here a certain voice,
and say, oh, my wife has

00:42:32.530 --> 00:42:34.000
entered the room.

00:42:34.000 --> 00:42:37.110
At a much higher level, there
are pattern recognizers that

00:42:37.110 --> 00:42:39.340
go, oh, that was funny.

00:42:39.340 --> 00:42:41.080
That was ironic.

00:42:41.080 --> 00:42:42.870
She's pretty.

00:42:42.870 --> 00:42:46.710
Those are actually no more
complicated, except for the

00:42:46.710 --> 00:42:48.910
fact that they exist at this
very high level of the

00:42:48.910 --> 00:42:49.940
conceptual hierarchy.

00:42:49.940 --> 00:42:53.370
I talk about the book this brain
surgery of a young girl.

00:42:53.370 --> 00:42:55.730
She was conscious, which you
can be in brain surgery,

00:42:55.730 --> 00:42:58.010
because there's no pain
receptors in the brain.

00:42:58.010 --> 00:42:59.970
Whenever they stimulated a
particular point in her

00:42:59.970 --> 00:43:01.805
neocortex, she would laugh.

00:43:01.805 --> 00:43:04.240
And they thought they were
triggering a laugh reflex, but

00:43:04.240 --> 00:43:06.980
they quickly discovered, no,
they're triggering the

00:43:06.980 --> 00:43:08.850
perception of humor.

00:43:08.850 --> 00:43:10.910
She just found everything
hilarious when they

00:43:10.910 --> 00:43:12.590
triggered that spot.

00:43:12.590 --> 00:43:14.690
You guys are so funny, standing
there, was her

00:43:14.690 --> 00:43:15.990
typical comment.

00:43:15.990 --> 00:43:18.070
But only when they were
triggering that spot.

00:43:18.070 --> 00:43:19.940
And these guys weren't funny.

00:43:19.940 --> 00:43:24.650
[LAUGHTER]

00:43:24.650 --> 00:43:27.300
They had one spot--

00:43:27.300 --> 00:43:29.090
and we obviously have
many of them--

00:43:29.090 --> 00:43:31.480
but they had found one that
would represent the

00:43:31.480 --> 00:43:33.360
perception of humor.

00:43:33.360 --> 00:43:35.650
Where does this hierarchy
come from?

00:43:35.650 --> 00:43:37.440
Well, we're not born
with it, obviously.

00:43:37.440 --> 00:43:40.666
That's what we're creating from
the moment we're born, or

00:43:40.666 --> 00:43:41.600
even before that.

00:43:41.600 --> 00:43:45.390
I have a one-year-old grandson
now, and he's laid down

00:43:45.390 --> 00:43:46.260
several layers.

00:43:46.260 --> 00:43:51.910
We can lay down, really, one
conceptual layer at a time.

00:43:51.910 --> 00:43:54.040
And we run through
the 300 million.

00:43:54.040 --> 00:43:57.190
One of the reasons children can
learn, say, a new language

00:43:57.190 --> 00:44:00.900
so easily is that they have
all this virgin neocortex.

00:44:00.900 --> 00:44:03.840
By the time we're 20, it's
really filled up.

00:44:03.840 --> 00:44:05.710
But that doesn't mean we
can't learn new things.

00:44:05.710 --> 00:44:08.540
We have to forget something
to learn something new.

00:44:08.540 --> 00:44:10.930
We don't necessarily have to
completely forget it, because

00:44:10.930 --> 00:44:13.040
there's a lot of redundancy, and
when we're first starting

00:44:13.040 --> 00:44:16.030
to learn something, there's lots
of redundancy and a lot

00:44:16.030 --> 00:44:17.860
of the patterns are imperfect.

00:44:17.860 --> 00:44:21.290
And over time, we can actually
perfect that model and have

00:44:21.290 --> 00:44:24.360
less redundancy and still
have a good recognition.

00:44:24.360 --> 00:44:30.810
So we can free up neocortical
recognizers for a new subject.

00:44:30.810 --> 00:44:33.310
But some people are better
at that than others.

00:44:33.310 --> 00:44:36.390
I mean, the rigidity that some
people have in learning a new

00:44:36.390 --> 00:44:41.800
idea is reflected in this
ability or inability to learn

00:44:41.800 --> 00:44:44.760
new material.

00:44:44.760 --> 00:44:48.830
Now is 300 million a
lot or a little?

00:44:48.830 --> 00:44:51.980
It was a lot compared to
other primates, who

00:44:51.980 --> 00:44:53.820
have somewhat less.

00:44:53.820 --> 00:44:57.510
And that was the enabling factor
for science and art and

00:44:57.510 --> 00:45:01.310
music and language and so on.

00:45:01.310 --> 00:45:08.140
But it's also a big limitation,
if you recognize

00:45:08.140 --> 00:45:11.070
the limitations we have in
learning new knowledge.

00:45:11.070 --> 00:45:14.990
We ultimately will be able
to expand the neocortex.

00:45:14.990 --> 00:45:19.630
So I'm working now on synthetic
neocortexes, not in

00:45:19.630 --> 00:45:22.980
the near future to be directly
connected to the brain, but I

00:45:22.980 --> 00:45:25.690
think if you go out to
the 2030s, we will

00:45:25.690 --> 00:45:27.200
be able to do that.

00:45:27.200 --> 00:45:29.380
And we actually don't have to
put them inside the brain.

00:45:29.380 --> 00:45:34.180
We just have to put the gateways
to it in the brain.

00:45:34.180 --> 00:45:37.600
If I do something interesting
on this-- do a search, do a

00:45:37.600 --> 00:45:41.830
language translation, ask
Google Now a question--

00:45:41.830 --> 00:45:44.000
it doesn't take place in
this rectangular box.

00:45:44.000 --> 00:45:45.750
It goes out to the cloud.

00:45:45.750 --> 00:45:49.800
And if I suddenly need 1,000
processors or 10,000 for a

00:45:49.800 --> 00:45:53.560
tenth of a second, the cloud
provides that, to the limits

00:45:53.560 --> 00:45:57.040
of the law of accelerated
returns at that point in time.

00:45:57.040 --> 00:45:59.770
Ultimately, we'll be able to
do that with the brain and

00:45:59.770 --> 00:46:03.150
have more than 300 million
pattern recognizers, that run

00:46:03.150 --> 00:46:06.590
faster, that can be backed up.

00:46:06.590 --> 00:46:08.570
And that's where we're headed.

00:46:08.570 --> 00:46:10.070
We'll have a greater quantity.

00:46:10.070 --> 00:46:13.630
The last time we added a greater
quantity, we got this

00:46:13.630 --> 00:46:16.110
qualitative leap of creating
art, science, and language.

00:46:16.110 --> 00:46:20.400
And we'll be able to make
another qualitative leap with

00:46:20.400 --> 00:46:22.820
that expansion.

00:46:22.820 --> 00:46:27.250
Already, these devices represent
brain-expanders, but

00:46:27.250 --> 00:46:31.940
we'll have much more powerful
means of doing that.

00:46:31.940 --> 00:46:37.550
So just a few comments.

00:46:37.550 --> 00:46:38.690
Peter will appreciate this.

00:46:38.690 --> 00:46:42.075
But we are destroying jobs at
the bottom of the skill

00:46:42.075 --> 00:46:43.790
ladder, creating new
jobs at the top.

00:46:43.790 --> 00:46:46.430
So we're investing more
in education.

00:46:46.430 --> 00:46:51.240
We spend 10 times as much on
K-12 per capita in constant

00:46:51.240 --> 00:46:53.640
dollars, compared to
a century ago.

00:46:53.640 --> 00:46:55.595
We had 50,000 college
students in 1870.

00:46:55.595 --> 00:46:58.190
We have 12 million today.

00:46:58.190 --> 00:47:01.020
There's a big revolution coming,
which Peter can tell

00:47:01.020 --> 00:47:03.230
you about, in higher
education.

00:47:03.230 --> 00:47:07.500
It's fostered by this tremendous
boon in both

00:47:07.500 --> 00:47:09.496
intelligent computation
and communication.

00:47:12.770 --> 00:47:15.080
We've tripled the amount of
education a child gets in the

00:47:15.080 --> 00:47:17.800
developing world, doubled in the
developed world, over the

00:47:17.800 --> 00:47:19.690
last half-century.

00:47:19.690 --> 00:47:22.670
Larry Page and I actually worked
on a major energy study

00:47:22.670 --> 00:47:25.730
for the National Academy
of Engineering.

00:47:25.730 --> 00:47:28.810
And the cost of solar energy--

00:47:28.810 --> 00:47:33.230
both PPV and total installed
costs-- are coming down.

00:47:33.230 --> 00:47:36.070
As a result, the total amount
of solar energy is on

00:47:36.070 --> 00:47:37.730
exponential climb.

00:47:37.730 --> 00:47:40.510
It's doubling every two years.

00:47:40.510 --> 00:47:41.200
Right now it's 1%.

00:47:41.200 --> 00:47:44.430
So people go, oh, 1%, that's
a fringe player.

00:47:44.430 --> 00:47:48.040
It's kind of a nice thing to
do, but it's not really

00:47:48.040 --> 00:47:49.600
significant.

00:47:49.600 --> 00:47:52.050
Just the way that they dismissed
the internet or the

00:47:52.050 --> 00:47:55.360
Genome Project when
they were 1% of a

00:47:55.360 --> 00:47:58.310
usable corpus of users.

00:47:58.310 --> 00:48:02.420
It's only seven doublings at
two years each from 100%.

00:48:02.420 --> 00:48:04.670
This was adopted by the
National Academy of

00:48:04.670 --> 00:48:05.330
Engineering.

00:48:05.330 --> 00:48:07.480
I presented it recently to the
prime minister of Israel.

00:48:07.480 --> 00:48:11.370
And he was in my class at the
Sloan School in the '70s, and

00:48:11.370 --> 00:48:12.820
he said, Ray, do we have enough

00:48:12.820 --> 00:48:14.770
sunlight to do this with?

00:48:14.770 --> 00:48:17.730
And I said yes, we have 10,000
times more than we need.

00:48:17.730 --> 00:48:19.460
After we double seven
more times, we'll be

00:48:19.460 --> 00:48:21.670
using one part in 10,000.

00:48:21.670 --> 00:48:23.620
So there's a whole other
discussion about

00:48:23.620 --> 00:48:24.625
resources in general.

00:48:24.625 --> 00:48:27.360
We're running out of resources
if we limit ourselves to

00:48:27.360 --> 00:48:31.120
19th-century First Industrial
Revolution technologies like

00:48:31.120 --> 00:48:33.120
fossil fuels.

00:48:33.120 --> 00:48:36.730
But in terms of water,
energy, food--

00:48:36.730 --> 00:48:41.070
with vertical agriculture,
another looming revolution

00:48:41.070 --> 00:48:43.730
coming over the next decade,
we actually will

00:48:43.730 --> 00:48:45.990
have a lot of resources.

00:48:45.990 --> 00:48:50.430
So this is the progress we've
made in longevity over the

00:48:50.430 --> 00:48:52.660
last 1,000 years.

00:48:52.660 --> 00:48:54.806
We've quadrupled life
expectancy.

00:48:54.806 --> 00:48:57.950
It's doubled in the
last 200 years.

00:48:57.950 --> 00:48:59.665
And this is from the
linear progression

00:48:59.665 --> 00:49:00.915
of health and medicine.

00:49:03.200 --> 00:49:06.060
It's now become an information
technology.

00:49:06.060 --> 00:49:11.680
This'll go into high gear once
we really master these

00:49:11.680 --> 00:49:13.920
techniques of biotechnology.

00:49:13.920 --> 00:49:15.480
There's many revolutions
coming.

00:49:15.480 --> 00:49:18.050
But the most important one is
that what's unique about the

00:49:18.050 --> 00:49:20.260
human species is that
we have knowledge.

00:49:20.260 --> 00:49:23.770
And there's many different ways
to measure knowledge, but

00:49:23.770 --> 00:49:26.670
no matter how you look at it,
it's growing exponentially.

00:49:26.670 --> 00:49:29.020
So we're doubling the amount
of knowledge, by some

00:49:29.020 --> 00:49:31.950
measures, at say every
13 months.

00:49:31.950 --> 00:49:34.520
And that's actually
what's hard to do.

00:49:34.520 --> 00:49:37.950
We have a much better means
already of finding knowledge

00:49:37.950 --> 00:49:40.560
with Google and other tools.

00:49:40.560 --> 00:49:43.770
That's going to get more
and more powerful.

00:49:43.770 --> 00:49:46.100
But we need that added
intelligence in order to

00:49:46.100 --> 00:49:48.940
actually continue this
exponential growth of

00:49:48.940 --> 00:49:50.830
information technology.

00:49:50.830 --> 00:49:55.650
So Google is still very
well-positioned for fantastic

00:49:55.650 --> 00:49:58.670
growth in importance and
success over the

00:49:58.670 --> 00:50:01.000
next several decades.

00:50:01.000 --> 00:50:02.844
Thank you very much.

00:50:02.844 --> 00:50:16.150
[APPLAUSE]

00:50:16.150 --> 00:50:16.460
BORIS DEBIC: Thank you, all.

00:50:16.460 --> 00:50:20.980
We'll do a Q&amp;A, and please use
the audience microphone.

00:50:25.510 --> 00:50:27.120
AUDIENCE: Hi, my
name is Jason.

00:50:27.120 --> 00:50:30.350
I actually work in PR, so I
think a lot about perceptions

00:50:30.350 --> 00:50:33.680
of this kind of progress.

00:50:33.680 --> 00:50:36.240
And I'm thinking about how
people have a tremendous

00:50:36.240 --> 00:50:38.910
tendency to sort of take for
granted whatever the next

00:50:38.910 --> 00:50:43.220
progression is, or to sort of
underestimate to correct for

00:50:43.220 --> 00:50:46.950
whatever improvements
there are.

00:50:46.950 --> 00:50:51.050
What do you think about that,
the fact that you see, if you

00:50:51.050 --> 00:50:52.510
measure all these things--

00:50:52.510 --> 00:50:54.990
and I'm thinking of Steven
Pinker's work on violence

00:50:54.990 --> 00:50:57.200
dropping over time as well.

00:50:57.200 --> 00:50:59.125
People tend to sort of correct
for that and take it for

00:50:59.125 --> 00:51:02.200
granted, and say, well, dismiss
it at each stage.

00:51:02.200 --> 00:51:04.120
Do you think that is just
sort of built in to us?

00:51:04.120 --> 00:51:06.990
RAY KURZWEIL: People have an
amazing ability to accept new

00:51:06.990 --> 00:51:09.680
changes and then assume
that the world's

00:51:09.680 --> 00:51:11.860
always been that way.

00:51:11.860 --> 00:51:15.040
If you described self-driving
cars a decade ago, people

00:51:15.040 --> 00:51:18.110
would dismiss that as
science fiction.

00:51:18.110 --> 00:51:19.880
Now that we have it,
people shrug.

00:51:19.880 --> 00:51:22.350
Well, it's not in everybody's
hands, but actually, I've

00:51:22.350 --> 00:51:24.810
talked to people who've driven
in the Google cars that

00:51:24.810 --> 00:51:27.810
quickly actually gain more
confidence in the AI driver

00:51:27.810 --> 00:51:28.880
than a human driver.

00:51:28.880 --> 00:51:30.130
Maybe that's not saying much.

00:51:33.070 --> 00:51:35.505
People very quickly then
take it for granted.

00:51:38.960 --> 00:51:40.630
I travel around the world.

00:51:40.630 --> 00:51:44.180
I don't get that here in Silicon
Valley, but as I go to

00:51:44.180 --> 00:51:46.300
other parts of the world, there
is a common perception

00:51:46.300 --> 00:51:48.070
that the world's
getting worse.

00:51:48.070 --> 00:51:51.670
And a big subset of that school
of thought is that

00:51:51.670 --> 00:51:54.160
technology's responsible
for it.

00:51:54.160 --> 00:51:55.425
I'd like to show them
this graph.

00:52:01.270 --> 00:52:14.200
So this is the world in 1800.

00:52:14.200 --> 00:52:15.826
And these are countries.

00:52:19.102 --> 00:52:24.940
The x-axis is the wealth of
nations, income per person.

00:52:24.940 --> 00:52:30.160
On the y-axis is life
expectancy.

00:52:30.160 --> 00:52:33.710
And over the last 200 years,
there's been dramatic

00:52:33.710 --> 00:52:35.450
improvement in both.

00:52:35.450 --> 00:52:37.400
A little bit of movement
in the First Industrial

00:52:37.400 --> 00:52:39.820
Revolution, but as you get to
the 20th century, there's a

00:52:39.820 --> 00:52:42.070
wind that carries all these
nations towards the upper

00:52:42.070 --> 00:52:45.630
right-hand corner
of the graph.

00:52:45.630 --> 00:52:47.530
And there's still a
have/have-not divide, but the

00:52:47.530 --> 00:52:49.660
countries that are worst off at
the end of the process are

00:52:49.660 --> 00:52:51.750
still far better off than the
countries that were best-off

00:52:51.750 --> 00:52:52.480
at the beginning.

00:52:52.480 --> 00:52:54.660
And I shouldn't say "end of
the process," because the

00:52:54.660 --> 00:52:58.110
process actually is going to go
into high gear as we get to

00:52:58.110 --> 00:53:01.350
the more mature phases of AI and
three-dimensional printing

00:53:01.350 --> 00:53:04.200
and biotechnology and so on.

00:53:04.200 --> 00:53:06.680
But people forget what the world
was like three or four

00:53:06.680 --> 00:53:08.600
years ago, before we
had social networks

00:53:08.600 --> 00:53:09.780
and wikis and blogs.

00:53:09.780 --> 00:53:14.060
And during that SOPA strike,
people were shocked that they

00:53:14.060 --> 00:53:18.460
could have to do without these
brain extenders which we

00:53:18.460 --> 00:53:20.720
didn't have just a
few years ago.

00:53:20.720 --> 00:53:25.090
So yes, people take changes
for granted.

00:53:25.090 --> 00:53:27.550
But also, they very readily
adopt them.

00:53:27.550 --> 00:53:29.780
You describe the world 20, 30
years from now, and people

00:53:29.780 --> 00:53:32.180
say, well, I don't know if I
want to opt in for that.

00:53:32.180 --> 00:53:33.130
It doesn't happen that way.

00:53:33.130 --> 00:53:36.590
It happens through thousands of
product announcements and

00:53:36.590 --> 00:53:38.530
research advances.

00:53:38.530 --> 00:53:42.160
But when there's a somewhat
better treatment for cancer,

00:53:42.160 --> 00:53:45.230
there's no philosophical
discussion.

00:53:45.230 --> 00:53:47.680
Is it really a good thing
to extend longevity?

00:53:47.680 --> 00:53:51.870
People adopt and celebrate
it if it works.

00:53:51.870 --> 00:53:54.650
So we will continue to make
this kind of progress.

00:53:54.650 --> 00:53:57.640
I think it's a moral imperative
that we do.

00:53:57.640 --> 00:53:59.280
There are downsides.

00:53:59.280 --> 00:54:00.520
That's a whole other
discussion.

00:54:00.520 --> 00:54:04.470
But overall, as you can see,
life is continuing to get

00:54:04.470 --> 00:54:08.470
better in all the ways that we
can measure-- health, wealth,

00:54:08.470 --> 00:54:09.790
education, so on.

00:54:15.760 --> 00:54:18.890
AUDIENCE: You mentioned one of
the great innovations of the

00:54:18.890 --> 00:54:23.990
humans is having a lot more
space up there for neocortex.

00:54:23.990 --> 00:54:27.260
What about some of our
Earth-mates, like whales?

00:54:27.260 --> 00:54:31.000
They've got a lot more
space up there.

00:54:31.000 --> 00:54:32.460
RAY KURZWEIL: Right.

00:54:32.460 --> 00:54:34.750
There are some other animals--

00:54:34.750 --> 00:54:37.110
actually, the whale
brain is bigger.

00:54:37.110 --> 00:54:39.940
We have one other enabling
factor, which is this

00:54:39.940 --> 00:54:44.590
opposable appendage, which
enabled us to take our ideas

00:54:44.590 --> 00:54:47.660
and our visions and say wow, I
could take that branch and I

00:54:47.660 --> 00:54:50.160
could strip it of the leaves,
and I could put a point on it,

00:54:50.160 --> 00:54:51.480
and I could create this tool.

00:54:51.480 --> 00:54:54.120
And then we had the opposable
appendage to do that.

00:54:54.120 --> 00:54:57.575
And then we had the tool
to create other tools.

00:54:57.575 --> 00:55:01.810
And these other species don't
have that opposable appendage.

00:55:01.810 --> 00:55:05.760
I mean, we see some clumsy
ability to move things around,

00:55:05.760 --> 00:55:10.060
say, by an elephant, which
also has a big brain.

00:55:10.060 --> 00:55:15.050
But it's actually not clear
that the neocortex,

00:55:15.050 --> 00:55:17.620
specifically, is bigger
in a whale.

00:55:17.620 --> 00:55:19.900
But it's pretty comparable.

00:55:19.900 --> 00:55:24.680
They don't have this opposable
appendage that enabled us.

00:55:24.680 --> 00:55:27.910
So those two things enable
us to create technology.

00:55:27.910 --> 00:55:30.300
And technology has reshaped
the world.

00:55:30.300 --> 00:55:32.600
AUDIENCE: But then what about
sort of deep thought, as

00:55:32.600 --> 00:55:34.550
opposed to just being able
to shape the world?

00:55:34.550 --> 00:55:34.850
Right?

00:55:34.850 --> 00:55:38.860
So taking is on a slightly
different vector.

00:55:38.860 --> 00:55:40.500
RAY KURZWEIL: It depends what
you mean by deep thought.

00:55:40.500 --> 00:55:44.650
I mean, the fact that we can
develop these greater number

00:55:44.650 --> 00:55:48.370
of levels of abstraction--

00:55:48.370 --> 00:55:53.490
the neocortex, in most other
mammals, is really devoted to

00:55:53.490 --> 00:55:57.450
the challenges of being
a raccoon or whatever.

00:55:57.450 --> 00:56:01.300
And we've been able to actually
then create these

00:56:01.300 --> 00:56:02.260
abstract levels.

00:56:02.260 --> 00:56:06.420
So we still have the old brain,
and so the neocortex is

00:56:06.420 --> 00:56:07.260
a great sublimator.

00:56:07.260 --> 00:56:11.390
And it can take the sex and
aggression of the old brain

00:56:11.390 --> 00:56:15.260
and convert it into
poetry and music.

00:56:15.260 --> 00:56:19.280
And that then becomes
an end in itself.

00:56:19.280 --> 00:56:22.370
And we've really been the only
species to master these

00:56:22.370 --> 00:56:25.790
additional levels, which you
would consider deep thought.

00:56:25.790 --> 00:56:31.210
But it's in an extension of
the neocortical hierarchy.

00:56:31.210 --> 00:56:33.500
AUDIENCE: It seems pretty clear
that the size of the pie

00:56:33.500 --> 00:56:36.320
for 3-D printing is growing
significantly, such that,

00:56:36.320 --> 00:56:37.410
like, I've already
made a couple

00:56:37.410 --> 00:56:38.600
investments in that market.

00:56:38.600 --> 00:56:40.660
And I'm wondering if based
on your research, you've

00:56:40.660 --> 00:56:44.145
identified any other markets
where you see the size of the

00:56:44.145 --> 00:56:47.770
pie growing so much, where if
you make a broad play across

00:56:47.770 --> 00:56:52.760
the industry, that it's nearly
guaranteed to grow.

00:56:52.760 --> 00:56:56.190
[LAUGHTER]

00:56:56.190 --> 00:56:58.685
RAY KURZWEIL: I think search
is very well-positioned.

00:56:58.685 --> 00:57:01.270
[LAUGHTER]

00:57:01.270 --> 00:57:05.840
RAY KURZWEIL: Even though it may
seem to be saturated, its

00:57:05.840 --> 00:57:08.270
role in our lives is not.

00:57:08.270 --> 00:57:12.960
'Cause search is going to become
much more intelligent.

00:57:12.960 --> 00:57:16.420
Our knowledge bases continue to
expand, and we can really

00:57:16.420 --> 00:57:20.280
use this as an intelligent
assistant to help guide us, to

00:57:20.280 --> 00:57:23.020
actually help us solve problems
and be more of an

00:57:23.020 --> 00:57:28.890
assistant as we make search
more intelligent.

00:57:28.890 --> 00:57:32.460
And it's not just the way we
traditionally think of search.

00:57:32.460 --> 00:57:34.090
It's this whole world
of knowledge.

00:57:34.090 --> 00:57:38.540
And Google is very much
committed to knowledge in all

00:57:38.540 --> 00:57:41.800
of its different forms and in
finding intelligent ways to

00:57:41.800 --> 00:57:44.250
find that information
and use it.

00:57:44.250 --> 00:57:46.850
So that's very well-positioned.

00:57:46.850 --> 00:57:49.760
Virtual reality is going
to become a big deal.

00:57:49.760 --> 00:57:51.070
Google has an interest
in that.

00:57:51.070 --> 00:57:57.730
The project Glass, Google Glass,
will be a first step.

00:57:57.730 --> 00:58:01.670
But ultimately, I
mean, this is--

00:58:01.670 --> 00:58:03.700
actually, I like the big screen,
but it's actually

00:58:03.700 --> 00:58:04.660
still pretty little.

00:58:04.660 --> 00:58:07.880
It's still like looking at the
world through a keyhole.

00:58:07.880 --> 00:58:11.066
I've got this big screen--

00:58:11.066 --> 00:58:13.030
AUDIENCE: Check out Ingress,
if you haven't yet.

00:58:13.030 --> 00:58:16.020
RAY KURZWEIL: Of real reality.

00:58:16.020 --> 00:58:19.400
And we will be online all the
time, with augmented reality,

00:58:19.400 --> 00:58:22.400
and just used to looking at
people and having pop-ups tell

00:58:22.400 --> 00:58:23.580
us who they are.

00:58:23.580 --> 00:58:26.840
And just telling us their name
will be very useful.

00:58:26.840 --> 00:58:28.477
That'll be a killer app.

00:58:28.477 --> 00:58:33.250
[LAUGHTER]

00:58:33.250 --> 00:58:33.480
AUDIENCE: Hi.

00:58:33.480 --> 00:58:34.670
So I had a question.

00:58:34.670 --> 00:58:39.480
Once we have these pattern
recognizers that we can access

00:58:39.480 --> 00:58:43.550
remotely, obviously, a best
of breeds will emerge and

00:58:43.550 --> 00:58:46.340
everyone will want to copy the
best, most accurate, most

00:58:46.340 --> 00:58:47.450
efficient one.

00:58:47.450 --> 00:58:51.940
At that point, if I did that,
would I still be me?

00:58:51.940 --> 00:58:54.540
RAY KURZWEIL: I talk about
that in the book.

00:58:54.540 --> 00:58:57.240
There are three great
philosophical questions--

00:58:57.240 --> 00:59:00.400
consciousness, free will,
and identity.

00:59:00.400 --> 00:59:02.260
And you're asking about
the identity issue.

00:59:02.260 --> 00:59:06.830
And I think, in my view,
identity comes from a

00:59:06.830 --> 00:59:10.200
continuity of pattern.

00:59:10.200 --> 00:59:11.110
People say, well, no, Ray.

00:59:11.110 --> 00:59:12.420
You're this physical stuff.

00:59:12.420 --> 00:59:14.210
You're flesh and blood.

00:59:14.210 --> 00:59:15.500
That's actually not true.

00:59:15.500 --> 00:59:18.800
I'm completely different
physical stuff than I was six

00:59:18.800 --> 00:59:20.690
months ago.

00:59:20.690 --> 00:59:21.980
And I go through that
in the book.

00:59:21.980 --> 00:59:26.400
All these different cells
die and are recreated.

00:59:26.400 --> 00:59:26.620
OK.

00:59:26.620 --> 00:59:29.470
The neurons persist, but the
parts of the neuron, like the

00:59:29.470 --> 00:59:32.840
tubules and the actin filaments
and all of these,

00:59:32.840 --> 00:59:35.950
turn over-- some in five hours,
some in five days.

00:59:35.950 --> 00:59:40.660
And we're completely different
stuff a few months later.

00:59:40.660 --> 00:59:44.070
So we're like a river.

00:59:44.070 --> 00:59:45.730
Charles River goes
by my office.

00:59:45.730 --> 00:59:48.160
Is that still the same river
it was yesterday?

00:59:48.160 --> 00:59:50.940
It's completely different water,
but the pattern has a

00:59:50.940 --> 00:59:54.650
continuity, so we call
it the same river.

00:59:54.650 --> 00:59:56.340
We're the same thing.

00:59:56.340 --> 00:59:59.360
Now we can augment that pattern
by, say, introducing

00:59:59.360 --> 01:00:02.750
non-biological parts to it.

01:00:02.750 --> 01:00:05.230
And I think it's very clear if
that's done in a continuous

01:00:05.230 --> 01:00:08.420
manner, it's very analogous to
what's happening naturally,

01:00:08.420 --> 01:00:11.180
which is that we're constantly
changing the stuff and

01:00:11.180 --> 01:00:13.610
gradually changing the pattern,
but there's a

01:00:13.610 --> 01:00:16.640
continuity of pattern,
and that's the

01:00:16.640 --> 01:00:18.270
nature of our identity.

01:00:18.270 --> 01:00:20.995
So I to talk about that
in that chapter.

01:00:24.520 --> 01:00:24.640
AUDIENCE: Hi.

01:00:24.640 --> 01:00:27.960
Could you comment on the
progress in the field of

01:00:27.960 --> 01:00:30.400
nanotechnology since you
wrote "Singularity?"

01:00:30.400 --> 01:00:31.550
RAY KURZWEIL: What
was the last?

01:00:31.550 --> 01:00:33.940
AUDIENCE: Could you just comment
on the progress in the

01:00:33.940 --> 01:00:36.340
field of nanotechnology since
you wrote "The Singularity is

01:00:36.340 --> 01:00:39.840
Near?"

01:00:39.840 --> 01:00:41.230
RAY KURZWEIL: Well,
there's been--

01:00:41.230 --> 01:00:44.740
nanotechnology is a further-off
revolution than

01:00:44.740 --> 01:00:47.480
biotechnology.

01:00:47.480 --> 01:00:53.440
But there have been advances in
our ability to create small

01:00:53.440 --> 01:00:56.830
structures which being
applied, actually, to

01:00:56.830 --> 01:00:57.980
electronic devices.

01:00:57.980 --> 01:01:01.010
And electronics is clearly
nanotechnology.

01:01:01.010 --> 01:01:05.470
The feature sizes are
approaching 20 nanometers,

01:01:05.470 --> 01:01:07.880
which is like 100
carbon atoms.

01:01:07.880 --> 01:01:11.530
We're starting to build
three-dimensional structures.

01:01:11.530 --> 01:01:14.600
So there's definitely been a
lot of technology there.

01:01:14.600 --> 01:01:20.050
MEMS, there are MEMS devices
now that are under 100

01:01:20.050 --> 01:01:22.460
nanometers, 'cause it's using
the same technology as

01:01:22.460 --> 01:01:23.710
semiconductors.

01:01:25.930 --> 01:01:30.860
There are experiments with
devices in the human body.

01:01:30.860 --> 01:01:32.870
There are dozens of
experiments of

01:01:32.870 --> 01:01:36.790
blood-cell-sized devices that
are nanoengineered doing

01:01:36.790 --> 01:01:39.660
therapeutic interventions
in animals.

01:01:39.660 --> 01:01:42.050
I think that's a further
evolution than the biotech.

01:01:42.050 --> 01:01:44.990
Biotech is really here.

01:01:44.990 --> 01:01:47.230
It's kind of on the experimental
cutting edge.

01:01:47.230 --> 01:01:49.990
Like if you want to fix your
heart if you've had a heart

01:01:49.990 --> 01:01:51.700
attack, you actually can't.

01:01:51.700 --> 01:01:52.800
It's not FDA-approved.

01:01:52.800 --> 01:01:54.850
It will be soon, but right
now you have to go

01:01:54.850 --> 01:01:59.100
to Israel or Thailand.

01:01:59.100 --> 01:02:04.030
So it's kind of on the edge, but
it's very close at hand.

01:02:04.030 --> 01:02:08.620
Nanotechnology is still, I
think, late 2020s for those

01:02:08.620 --> 01:02:09.870
types of applications.

01:02:12.490 --> 01:02:14.420
AUDIENCE: I hope this doesn't
come across as a flaky

01:02:14.420 --> 01:02:16.220
question, but--

01:02:16.220 --> 01:02:17.070
RAY KURZWEIL: No question
is flaky.

01:02:17.070 --> 01:02:21.630
AUDIENCE: In your research, have
you found the same law of

01:02:21.630 --> 01:02:27.290
accelerating returns in
happiness, fulfillment,

01:02:27.290 --> 01:02:28.770
satisfaction?

01:02:28.770 --> 01:02:30.430
RAY KURZWEIL: Well, this is
actually a similar question to

01:02:30.430 --> 01:02:35.430
the first one, in that
our expectations

01:02:35.430 --> 01:02:37.760
are constantly changing.

01:02:37.760 --> 01:02:42.280
If you talk to a caveman or
woman thousands of years ago,

01:02:42.280 --> 01:02:44.750
they would say, gee, if I
could just have a bigger

01:02:44.750 --> 01:02:49.320
boulder to keep the animals
out of my cave and prevent

01:02:49.320 --> 01:02:52.090
this fire from going out,
I would be happy.

01:02:52.090 --> 01:02:53.750
Well, don't you want
a better website?

01:02:53.750 --> 01:02:58.170
[LAUGHTER]

01:02:58.170 --> 01:03:01.500
RAY KURZWEIL: So we don't even
know what we want until

01:03:01.500 --> 01:03:05.606
somebody invents these ideas.

01:03:05.606 --> 01:03:10.950
And our expectations
of what should be

01:03:10.950 --> 01:03:12.100
are constantly changing.

01:03:12.100 --> 01:03:16.060
People who are poor today still
generally have access to

01:03:16.060 --> 01:03:22.930
refrigerators and to
communications and clothing.

01:03:22.930 --> 01:03:27.110
You go back several hundred
years ago, even a middle-class

01:03:27.110 --> 01:03:31.370
person only had one shirt before
there was automation in

01:03:31.370 --> 01:03:33.040
the textile industry.

01:03:33.040 --> 01:03:36.750
So our expectations of what it
takes to be happy change.

01:03:36.750 --> 01:03:39.120
I think people are happier,
because a much higher

01:03:39.120 --> 01:03:41.140
percentage of the population
gets part of their

01:03:41.140 --> 01:03:46.190
satisfaction and definition
in life from their work.

01:03:46.190 --> 01:03:47.390
Not everybody, apparently.

01:03:47.390 --> 01:03:52.880
I was interested by this French
strike where they were

01:03:52.880 --> 01:03:56.590
very upset at extending the
retirement age from 60 to 62.

01:03:56.590 --> 01:03:59.080
And I thought, gee, these people
really must not like

01:03:59.080 --> 01:04:01.420
their work.

01:04:01.420 --> 01:04:04.220
But then I realized that I had
retired when I was five,

01:04:04.220 --> 01:04:07.220
because I'm really doing
what I love to do.

01:04:07.220 --> 01:04:10.690
And I think that should be
the objective of work.

01:04:10.690 --> 01:04:15.770
And many more people have the
opportunity to do that.

01:04:15.770 --> 01:04:19.980
Work done in the information
sector, people really have a

01:04:19.980 --> 01:04:23.730
passion for it, whereas 100
years ago, they were just glad

01:04:23.730 --> 01:04:26.830
if they could earn a living.

01:04:26.830 --> 01:04:28.420
But it's a moving frontier.

01:04:28.420 --> 01:04:30.870
And I think that's a good thing,
and that's part of what

01:04:30.870 --> 01:04:34.080
propels humanity forward,
is we're constantly

01:04:34.080 --> 01:04:37.100
questing for more.

01:04:39.700 --> 01:04:43.350
And more doesn't necessarily
mean greater quantity of

01:04:43.350 --> 01:04:44.180
physical things.

01:04:44.180 --> 01:04:50.300
It could be just more music and
more opportunity to have

01:04:50.300 --> 01:04:52.820
relationships, which social
networks gives us the

01:04:52.820 --> 01:04:54.310
opportunity to do, and so on.

01:04:58.900 --> 01:05:03.840
AUDIENCE: So with the increase
in knowledge work, it requires

01:05:03.840 --> 01:05:06.010
a lot of knowledge transfer
between humans.

01:05:06.010 --> 01:05:09.040
Do you envision any efficient
methods of knowledge transfer

01:05:09.040 --> 01:05:10.660
within humans beyond?

01:05:10.660 --> 01:05:11.890
RAY KURZWEIL: Could you
speak a little louder?

01:05:11.890 --> 01:05:13.590
I'm missing some words.

01:05:13.590 --> 01:05:16.450
AUDIENCE: Do you envision any
efficient methods of knowledge

01:05:16.450 --> 01:05:18.170
transfer between humans?

01:05:18.170 --> 01:05:22.870
Not like reading books or
anything, just beaming.

01:05:22.870 --> 01:05:26.950
RAY KURZWEIL: Yeah, well, when
we can have massively

01:05:26.950 --> 01:05:30.690
distributed communication points
in a neocortex, it

01:05:30.690 --> 01:05:32.390
could provide a higher-bandwidth
way of

01:05:32.390 --> 01:05:34.040
communicating.

01:05:34.040 --> 01:05:37.540
But we have to appreciate that
there's actually a very kind

01:05:37.540 --> 01:05:41.820
of challenging translation job
for one neocortex to another.

01:05:41.820 --> 01:05:43.120
I talk about this in the book.

01:05:43.120 --> 01:05:47.000
If you could actually get this
information at any bandwidth,

01:05:47.000 --> 01:05:51.260
and even process it quickly, of
someone else's neocortex,

01:05:51.260 --> 01:05:54.060
you'd have no idea what it
means, because that pattern

01:05:54.060 --> 01:05:57.250
recognizer, say, fires with
a higher probability.

01:05:57.250 --> 01:05:59.520
But you can only interpret that
based on the ones that

01:05:59.520 --> 01:06:00.790
are connected to it.

01:06:00.790 --> 01:06:02.720
And each of those, you go only
understand by the ones

01:06:02.720 --> 01:06:05.420
connected to it, all the
way down the hierarchy.

01:06:05.420 --> 01:06:10.050
You'd have to actually have a
complete dump of most of their

01:06:10.050 --> 01:06:14.080
neocortex to understand it.

01:06:14.080 --> 01:06:16.360
And so just--

01:06:16.360 --> 01:06:19.150
it's not like we would readily
understand someone else's

01:06:19.150 --> 01:06:22.400
neocortex, even if you could
transfer that information

01:06:22.400 --> 01:06:24.060
without translating it.

01:06:24.060 --> 01:06:27.070
We have a translation mechanism,
which is language.

01:06:27.070 --> 01:06:29.730
So we could take thoughts from
one neocortex, even though

01:06:29.730 --> 01:06:32.190
it's very different from someone
else's, because we've

01:06:32.190 --> 01:06:35.750
each built this hierarchy, and
actually communicate a thought

01:06:35.750 --> 01:06:37.990
that the other person
can understand.

01:06:37.990 --> 01:06:41.700
That's what language
enables us to do.

01:06:41.700 --> 01:06:45.510
We could perhaps do some
automatic translation, just

01:06:45.510 --> 01:06:48.120
like we translate languages
now, from one neocortex to

01:06:48.120 --> 01:06:50.715
another and provide
higher-bandwidth connection.

01:06:50.715 --> 01:06:52.850
I mean, it's something we could
speculate once we're

01:06:52.850 --> 01:06:57.400
able to do that in the 2040s.

01:06:57.400 --> 01:06:58.580
AUDIENCE: Excuse me, if you've
already covered this.

01:06:58.580 --> 01:07:00.470
I was way in the back, and it
was a little hard to hear you,

01:07:00.470 --> 01:07:04.530
but do we have software
engineering stuff to model

01:07:04.530 --> 01:07:10.720
these clusters of neurons and
create these models already?

01:07:10.720 --> 01:07:14.800
RAY KURZWEIL: Well, the closest
that we've had is

01:07:14.800 --> 01:07:17.290
these hierarchical hidden Markov
models, which as I

01:07:17.290 --> 01:07:21.670
mentioned, have become a
common technique in AI.

01:07:26.730 --> 01:07:29.380
They're missing certain things,
in that generally the

01:07:29.380 --> 01:07:31.150
hierarchy is fixed.

01:07:31.150 --> 01:07:34.070
So I mean, I began pioneering
this in the '80s, and we did

01:07:34.070 --> 01:07:36.860
it for speech recognition, and
then we added simple natural

01:07:36.860 --> 01:07:40.330
language understanding and we
had some fixed levels of

01:07:40.330 --> 01:07:45.330
spectral features, phonemes,
words, and then simple

01:07:45.330 --> 01:07:48.130
syntactic structures.

01:07:48.130 --> 01:07:49.520
But it was relatively fixed.

01:07:49.520 --> 01:07:54.820
It could prune some elements,
some of these recognizers, if

01:07:54.820 --> 01:07:56.290
they weren't used.

01:07:56.290 --> 01:07:58.840
But it didn't actually
self-organize, in terms of

01:07:58.840 --> 01:08:01.160
creating the connections, which
is really the essence of

01:08:01.160 --> 01:08:03.530
what the neocortex does.

01:08:03.530 --> 01:08:06.840
If you want to get into a
better level of natural

01:08:06.840 --> 01:08:10.210
language understanding, you need
to be able to do that,

01:08:10.210 --> 01:08:13.500
because one of the features of
language is that it doesn't

01:08:13.500 --> 01:08:16.580
just have two or three fixed
levels of hierarchy.

01:08:16.580 --> 01:08:19.850
Language reflects the hierarchy
of the neocortex.

01:08:19.850 --> 01:08:22.550
It can have many different
levels.

01:08:22.550 --> 01:08:27.000
And you really need to model
quite a few levels in order to

01:08:27.000 --> 01:08:30.819
make semantic sense
of language.

01:08:30.819 --> 01:08:31.810
And we need to be able to

01:08:31.810 --> 01:08:34.660
dynamically build that hierarchy.

01:08:34.660 --> 01:08:36.939
But it's interesting, actually,
that I think there's

01:08:36.939 --> 01:08:41.060
a mathematical similarity
between this hierarchical

01:08:41.060 --> 01:08:44.620
hidden Markov model technique
and what happens in the brain.

01:08:44.620 --> 01:08:47.569
And it's not because we were
trying to emulate the brain in

01:08:47.569 --> 01:08:50.680
the '80s and '90s, because we
didn't really understand--

01:08:50.680 --> 01:08:52.840
we didn't have enough
information to confirm that

01:08:52.840 --> 01:08:54.950
that's how the brain works.

01:08:54.950 --> 01:08:58.540
It's just that technique
worked, and biological

01:08:58.540 --> 01:09:02.729
evolution evolved neocortexes
that way for the same reason.

01:09:05.810 --> 01:09:08.779
AUDIENCE: Speaking of assuming
that the world will not change

01:09:08.779 --> 01:09:14.359
a lot, I'd like you to comment
on the non-technical aspect of

01:09:14.359 --> 01:09:16.310
this change.

01:09:16.310 --> 01:09:19.330
We all assume that 20 years from
now we'll be living in a

01:09:19.330 --> 01:09:22.850
stable democracy, with
free market and

01:09:22.850 --> 01:09:25.160
a capitalist economy.

01:09:25.160 --> 01:09:28.840
Those changes that you predict,
how much of that are

01:09:28.840 --> 01:09:32.020
they going to change,
politically and economically?

01:09:32.020 --> 01:09:34.979
RAY KURZWEIL: Well, I do think
the distributed communication

01:09:34.979 --> 01:09:38.029
technologies we have
is democratizing.

01:09:38.029 --> 01:09:41.819
I wrote that in the 1980s, and
then it was discussed in my

01:09:41.819 --> 01:09:44.930
first book, which I
wrote in the '80s.

01:09:44.930 --> 01:09:47.490
I said the Soviet Union would
be swept away by the

01:09:47.490 --> 01:09:52.040
then-emerging social network,
which was communication over

01:09:52.040 --> 01:09:57.550
Teletype machines and fax
machines, by this clandestine

01:09:57.550 --> 01:09:59.490
network of hackers.

01:09:59.490 --> 01:10:02.590
And so people heavily
criticized that.

01:10:02.590 --> 01:10:05.170
At that time, the Soviet Union
was a mighty nuclear

01:10:05.170 --> 01:10:05.560
superpower.

01:10:05.560 --> 01:10:07.950
It's not going to get
swept away by a

01:10:07.950 --> 01:10:10.380
few Teletype machines.

01:10:10.380 --> 01:10:12.980
But that's exactly what happened
in the 1991 coup

01:10:12.980 --> 01:10:14.650
against Gorbachev.

01:10:14.650 --> 01:10:17.300
The authorities grabbed the
central TV and radio station,

01:10:17.300 --> 01:10:19.560
which had always worked in the
past, 'cause it kept everybody

01:10:19.560 --> 01:10:20.680
in the dark.

01:10:20.680 --> 01:10:25.180
But now this clandestine
network, this sort of first

01:10:25.180 --> 01:10:27.740
social network, kept everybody
in the know.

01:10:27.740 --> 01:10:32.040
And it just swept away the
totalitarian government.

01:10:32.040 --> 01:10:34.650
And with the rise of the web,
there was a great wave of

01:10:34.650 --> 01:10:37.410
democratization in
the late '90s.

01:10:37.410 --> 01:10:40.120
We see the effect of social
networks today.

01:10:40.120 --> 01:10:43.950
It is democratizing for people
to share knowledge at that

01:10:43.950 --> 01:10:49.480
grassroots level, see how other
people live and think.

01:10:49.480 --> 01:10:53.890
It really is able to harness
the wisdom of crowds rather

01:10:53.890 --> 01:10:57.130
than the wisdom of
a lynch mob.

01:10:57.130 --> 01:11:00.260
And we've also democratized
the tools of creativity.

01:11:00.260 --> 01:11:03.460
So a kid with a notebook
computer could start Facebook.

01:11:03.460 --> 01:11:06.560
And a couple of kids in a
late-night dorm room challenge

01:11:06.560 --> 01:11:07.530
started Google.

01:11:07.530 --> 01:11:13.800
And we see now, younger kids
doing quite dramatic things,

01:11:13.800 --> 01:11:17.910
teenagers with tools that
everybody has, a kid in Africa

01:11:17.910 --> 01:11:20.180
with a smartphone has access
to more knowledge than the

01:11:20.180 --> 01:11:23.520
president of the United States
did 15 years ago.

01:11:23.520 --> 01:11:29.900
So these are having an impact
on our economy, on society.

01:11:29.900 --> 01:11:32.740
Here's a very dramatic
demonstration of the political

01:11:32.740 --> 01:11:36.820
power of this organized group
of people who are able to

01:11:36.820 --> 01:11:38.320
communicate.

01:11:38.320 --> 01:11:45.810
The SOPA legislation was headed
for bilateral passage.

01:11:45.810 --> 01:11:48.420
Both Democrats and Republicans
were for it.

01:11:48.420 --> 01:11:51.670
It was going to be passed, one
of the few examples where

01:11:51.670 --> 01:11:55.100
there was agreement on a
piece of legislation.

01:11:55.100 --> 01:12:00.370
Well, users saw that as a threat
to the freedom on the

01:12:00.370 --> 01:12:03.260
web and organized this
demonstration.

01:12:03.260 --> 01:12:04.820
Within hours, it was dead.

01:12:04.820 --> 01:12:07.440
So I mean, just think of the
tremendous political power

01:12:07.440 --> 01:12:10.880
that was demonstrated there.

01:12:10.880 --> 01:12:13.610
Google participated in that, but
suddenly Wikipedia becomes

01:12:13.610 --> 01:12:14.970
a great political power.

01:12:14.970 --> 01:12:18.170
It just snaps its
fingers, and.

01:12:18.170 --> 01:12:23.960
So I think that these are
very positive phenomena.

01:12:23.960 --> 01:12:26.100
And it's affecting society.

01:12:26.100 --> 01:12:28.830
It's affecting communication.

01:12:28.830 --> 01:12:31.205
People criticize online
education now because it's

01:12:31.205 --> 01:12:34.830
missing a social component that
you have with a campus.

01:12:34.830 --> 01:12:39.210
But we can actually do a
better job with social

01:12:39.210 --> 01:12:44.450
networks and social
communication online, because

01:12:44.450 --> 01:12:46.560
we overcome the geographic
barrier.

01:12:50.540 --> 01:12:52.690
AUDIENCE: I'm struggling to find
the exact words-- sorry.

01:12:52.690 --> 01:12:56.500
But I wanted to ask you whether
you see power--

01:12:56.500 --> 01:12:59.380
not as in electronic power, but
power as in control over

01:12:59.380 --> 01:13:00.030
individuals--

01:13:00.030 --> 01:13:04.330
as something that's
exponentially accelerating, in

01:13:04.330 --> 01:13:07.510
terms of the state or security
apparatus versus freedom.

01:13:07.510 --> 01:13:11.330
It seems like both are
accelerating quite quickly,

01:13:11.330 --> 01:13:16.700
and there's this tension between
the power that's being

01:13:16.700 --> 01:13:20.870
centralized versus of
the individual.

01:13:20.870 --> 01:13:22.540
RAY KURZWEIL: Well you
can I imagine--

01:13:22.540 --> 01:13:29.390
these tools can be used
to spy and wreck

01:13:29.390 --> 01:13:33.930
privacy, invade privacy.

01:13:33.930 --> 01:13:38.730
The recent scandal going on in
Washington raises issues of

01:13:38.730 --> 01:13:41.257
the privacy of emails
and so on.

01:13:44.460 --> 01:13:46.850
On the other hand, I think
it's also been very

01:13:46.850 --> 01:13:49.980
democratizing, as I mentioned.

01:13:49.980 --> 01:13:51.770
I think it's led to
greater freedom.

01:13:51.770 --> 01:13:57.300
I think that trend has been more
pronounced, the ability

01:13:57.300 --> 01:14:01.480
of individuals to organize
around a set of ideas that

01:14:01.480 --> 01:14:04.635
they quickly support in
terms of freedom.

01:14:07.630 --> 01:14:10.360
And we've seen the democratizing
effect of

01:14:10.360 --> 01:14:14.330
decentralized electronic
communication.

01:14:14.330 --> 01:14:16.065
Privacy is a very
important issue.

01:14:16.065 --> 01:14:18.580
There's certainly an important
issue here.

01:14:18.580 --> 01:14:21.210
I think Google does a good job
of it, but it's something that

01:14:21.210 --> 01:14:24.140
has to be a high priority.

01:14:24.140 --> 01:14:28.880
If any service like Facebook
or something did not keep

01:14:28.880 --> 01:14:34.050
faith with its users, in terms
of these social issues, there

01:14:34.050 --> 01:14:35.300
would be a reaction.

01:14:37.600 --> 01:14:40.360
And it raises complicated
issues.

01:14:40.360 --> 01:14:43.540
Like privacy, it used to be
enough to just close the

01:14:43.540 --> 01:14:45.930
curtains in your bedroom, and
now we have 1,000 virtual

01:14:45.930 --> 01:14:48.480
windows on our lives.

01:14:48.480 --> 01:14:50.340
Nonetheless, I think we're
doing pretty well.

01:14:50.340 --> 01:14:54.940
I almost never encounter someone
who says, oh, my life

01:14:54.940 --> 01:14:58.980
was ruined by the loss of
privacy because of all these

01:14:58.980 --> 01:14:59.940
new technologies.

01:14:59.940 --> 01:15:01.660
Now, maybe those people
don't talk to me.

01:15:04.180 --> 01:15:06.990
But I think we're doing OK.

01:15:06.990 --> 01:15:12.570
But it is making these
once-routine issues much more

01:15:12.570 --> 01:15:13.820
complicated.

01:15:16.000 --> 01:15:18.672
AUDIENCE: So when you were
talking about the digitization

01:15:18.672 --> 01:15:24.560
of or the information age of
manufacturing with printers,

01:15:24.560 --> 01:15:27.640
3-D printers, I had a question
about resources.

01:15:27.640 --> 01:15:30.710
Like if you print with, like,
hydrocarbons, for example,

01:15:30.710 --> 01:15:33.610
then you might need an oil rig
and a ship and a truck to get

01:15:33.610 --> 01:15:35.790
the resources from the Earth
into the printer, and that

01:15:35.790 --> 01:15:37.660
takes a lot of time
and a lot of fuel.

01:15:37.660 --> 01:15:39.530
Whereas if you build with
plants, then you need to farm

01:15:39.530 --> 01:15:42.070
somewhere, and again, you need
transport to where the

01:15:42.070 --> 01:15:42.780
printers are.

01:15:42.780 --> 01:15:44.910
So how do you see
things changing?

01:15:44.910 --> 01:15:47.500
RAY KURZWEIL: There's not that
many resources you need to

01:15:47.500 --> 01:15:49.310
create these physical things.

01:15:49.310 --> 01:15:53.710
By far the most hydrocarbons are
used in burning them for

01:15:53.710 --> 01:15:56.260
fossil fuels.

01:15:56.260 --> 01:15:57.930
Yes, some of those products
are used now

01:15:57.930 --> 01:15:58.830
in chips, for example.

01:15:58.830 --> 01:16:02.350
But it's a very small
part of the output.

01:16:02.350 --> 01:16:05.530
And if we can actually create
the right products at the

01:16:05.530 --> 01:16:09.210
destination in a distributed
manner, and then also recycle

01:16:09.210 --> 01:16:14.340
those these materials, that's
a pretty efficient use of

01:16:14.340 --> 01:16:16.760
these materials.

01:16:16.760 --> 01:16:19.370
Peter Diamandis has a book
called "Abundance" that deals

01:16:19.370 --> 01:16:23.550
with, in detail, this issue
of energy, these kinds of

01:16:23.550 --> 01:16:25.590
resources for three-dimensional
printing--

01:16:25.590 --> 01:16:29.750
water, food, building
materials.

01:16:29.750 --> 01:16:32.910
And as we adopt new
technologies, we actually find

01:16:32.910 --> 01:16:38.260
that there's a tremendous
abundance of resources, like

01:16:38.260 --> 01:16:41.580
10,000 times more sunlight than
we need to meet all our

01:16:41.580 --> 01:16:42.770
energy needs.

01:16:42.770 --> 01:16:46.930
Larry Page was fond of going
a mile or two that way, and

01:16:46.930 --> 01:16:50.750
there's a lot of heat in the
Earth, geothermal energy,

01:16:50.750 --> 01:16:52.980
which is also thousands of
times more than we need.

01:16:52.980 --> 01:16:55.410
And there are a lot of
other scenarios.

01:16:55.410 --> 01:17:00.160
So as we find new 21st-century
technologies, we

01:17:00.160 --> 01:17:02.420
can tap these resources.

01:17:02.420 --> 01:17:06.130
There's new water technologies,
like Kamen's

01:17:06.130 --> 01:17:10.430
Slingshot machine, which are
decentralized and can create

01:17:10.430 --> 01:17:14.140
clean water very inexpensively,
vertical

01:17:14.140 --> 01:17:17.840
agriculture to grow food in
AI-controlled buildings,

01:17:17.840 --> 01:17:21.390
recycling all the nutrients so
in fact it would not be the

01:17:21.390 --> 01:17:24.980
wasteful and
ecologically-damaging

01:17:24.980 --> 01:17:28.750
food-production techniques we
use now, but we can create

01:17:28.750 --> 01:17:30.660
food very inexpensively.

01:17:30.660 --> 01:17:32.550
Including in vitro-cloned
meat--

01:17:32.550 --> 01:17:35.710
I mean, why grow meat from
animals when we only need a

01:17:35.710 --> 01:17:37.320
small part of the animal?

01:17:37.320 --> 01:17:40.300
We know how to, in fact, grow
the muscle tissue, which is

01:17:40.300 --> 01:17:40.970
what we want.

01:17:40.970 --> 01:17:42.850
It's been demonstrated.

01:17:42.850 --> 01:17:45.850
This can be done in
AI-controlled buildings at

01:17:45.850 --> 01:17:47.440
very low cost, ultimately.

01:17:47.440 --> 01:17:49.400
AUDIENCE: But do you think that,
say, a computer will be

01:17:49.400 --> 01:17:51.590
able to be printed with
resources that

01:17:51.590 --> 01:17:52.840
were sourced locally?

01:17:54.960 --> 01:17:57.510
RAY KURZWEIL: There's actually
some experimental

01:17:57.510 --> 01:18:00.110
three-dimensional printing
systems that can print

01:18:00.110 --> 01:18:01.360
electronics.

01:18:03.170 --> 01:18:07.390
Being able to actually print
electronics in a distributed

01:18:07.390 --> 01:18:11.630
matter, there are pros
and cons to it.

01:18:11.630 --> 01:18:13.740
An argument can made--

01:18:13.740 --> 01:18:16.910
computation and communication
is very universal, so let's

01:18:16.910 --> 01:18:20.020
have plants that really do
that efficiently and then

01:18:20.020 --> 01:18:23.490
customize it for people
with software.

01:18:23.490 --> 01:18:25.090
That's the model we're
using now.

01:18:25.090 --> 01:18:28.850
I mean, it's remarkable how
powerful a computational

01:18:28.850 --> 01:18:32.190
communication device you can
get for very little money.

01:18:32.190 --> 01:18:34.132
And that's continuing
to improve.

01:18:38.290 --> 01:18:41.550
BORIS DEBIC: I hope you all
made some new neocortical

01:18:41.550 --> 01:18:45.200
connections today which will be
useful in your work and in

01:18:45.200 --> 01:18:46.270
your lives.

01:18:46.270 --> 01:18:48.890
And please join me in thanking
Dr. Kurzweil.

01:18:48.890 --> 01:19:03.743
[APPLAUSE]

