WEBVTT
Kind: captions
Language: en

00:00:00.090 --> 00:00:03.698
♪ (music) ♪

00:00:05.857 --> 00:00:07.334
So hi, everyone!

00:00:08.311 --> 00:00:10.301
Hope everybody had a good lunch.

00:00:10.684 --> 00:00:14.499
I'm Sarah Sirajuddin.
I'm an engineer on TensorFlow Lite team.

00:00:14.499 --> 00:00:17.920
And I have my colleague,
Andrew Selle, also on the same team.

00:00:18.441 --> 00:00:20.070
We're really excited today

00:00:20.070 --> 00:00:22.877
to talk to you about all the work
that we've been doing

00:00:22.877 --> 00:00:25.066
to bring machine learning
to mobile devices.

00:00:26.651 --> 00:00:29.678
So in today's talk,
we'll cover three areas.

00:00:29.678 --> 00:00:33.149
First, we'll talk about
how machine learning on-device

00:00:33.149 --> 00:00:35.845
is different and important.

00:00:35.845 --> 00:00:39.291
Then we'll talk about all the work
that we've been doing on TensorFlow Lite.

00:00:39.291 --> 00:00:42.981
And lastly, we'll talk about
how you can use it in your own apps.

00:00:44.850 --> 00:00:47.782
So before that, let's talk
about devices for a little bit.

00:00:47.782 --> 00:00:51.489
Usually when we say "device,"
we mean a mobile device,

00:00:51.489 --> 00:00:53.360
basically a phone,

00:00:53.360 --> 00:00:56.208
and our phones are with us all the time.

00:00:56.208 --> 00:00:58.559
These days they have lots of sensors

00:00:58.559 --> 00:01:02.410
which are giving rich data
about the physical world around us.

00:01:02.410 --> 00:01:04.985
And lastly, we use
our phones all the time.

00:01:06.302 --> 00:01:10.138
Another category of devices
is edge devices,

00:01:10.138 --> 00:01:14.270
and this industry has seen
a huge growth in the last few years.

00:01:14.270 --> 00:01:18.560
So by some estimates there are
right now 23 billion connected devices--

00:01:18.560 --> 00:01:22.882
so smart speakers, smart watches,
smart sensors, what have you.

00:01:23.271 --> 00:01:25.580
And an interesting trend to notice here

00:01:25.580 --> 00:01:28.281
is that technology
which only used to be available

00:01:28.281 --> 00:01:30.889
on the more expensive devices

00:01:30.889 --> 00:01:33.158
is now available on the cheaper ones.

00:01:34.310 --> 00:01:40.247
So this rapid increase in the availability
of these more and more capable devices

00:01:40.247 --> 00:01:44.534
has now opened up many opportunities
for doing machine learning on-device.

00:01:45.960 --> 00:01:48.709
In addition to that, though,
there are several other reasons

00:01:48.709 --> 00:01:51.518
why you may consider doing
on-device machine learning,

00:01:51.518 --> 00:01:54.826
and probably
the most important one is latency.

00:01:54.826 --> 00:01:59.380
So if you are processing streaming data
such as audio or video,

00:01:59.380 --> 00:02:02.975
then you don't want to be making calls
back and forth to the server.

00:02:03.617 --> 00:02:06.266
Other reasons are
that your processing can happen

00:02:06.266 --> 00:02:08.358
even when your device is offline.

00:02:09.840 --> 00:02:12.075
Sensitive data can stay on-device.

00:02:12.862 --> 00:02:14.342
It's more power-efficient,

00:02:14.342 --> 00:02:17.917
because the device
is not sending data back and forth.

00:02:17.917 --> 00:02:22.519
And lastly, we're in a position
to take advantage of all the sensor data

00:02:22.519 --> 00:02:25.004
that is already present on the device.

00:02:25.771 --> 00:02:27.283
So all that is great,

00:02:27.990 --> 00:02:29.565
but there's also a catch.

00:02:29.859 --> 00:02:32.908
And the catch is that
on-device machine learning is hard.

00:02:33.143 --> 00:02:34.772
And the reason it is hard

00:02:34.772 --> 00:02:39.003
is that many of these devices
have some pretty tight constraints--

00:02:39.003 --> 00:02:43.578
small batteries, low compute power,
tight memory--

00:02:44.423 --> 00:02:47.222
and TensorFlow
wasn't a great fit for this.

00:02:47.961 --> 00:02:51.421
And that is the reason
we built TensorFlow Lite,

00:02:51.421 --> 00:02:55.084
which is a lightweight library and tools
for doing machine learning

00:02:55.084 --> 00:02:57.463
on embedded and small platforms.

00:02:58.488 --> 00:03:02.503
So we launched TensorFlow Lite
late last year in developer preview,

00:03:02.503 --> 00:03:05.970
and since then we've been working
on adding more features and support to it.

00:03:07.166 --> 00:03:10.909
So I'll just walk you through
the high-level design of the system.

00:03:11.294 --> 00:03:13.352
So we have the TensorFlow Lite format,

00:03:13.352 --> 00:03:16.095
and this is different
than what TensorFlow uses,

00:03:16.095 --> 00:03:19.072
and we had to do so
for reasons of efficiency.

00:03:19.072 --> 00:03:22.203
Then there is the interpreter,
which runs on-device.

00:03:22.203 --> 00:03:24.713
Then there are a set of optimized kernels,

00:03:24.713 --> 00:03:27.001
and then there are interfaces
which you can use

00:03:27.001 --> 00:03:30.782
to take advantage of hardware acceleration
when it is available.

00:03:32.051 --> 00:03:36.174
It's cross-platform,
so it supports Android and iOS,

00:03:36.174 --> 00:03:41.224
and I'm really happy to say today
that we also have support for Raspberry Pi

00:03:41.224 --> 00:03:43.990
and pretty much most other devices
which are running Linux.

00:03:44.773 --> 00:03:49.826
So the developer workflow roughly is
that you take a trained TensorFlow model,

00:03:49.826 --> 00:03:52.733
and then you convert it
to the TensorFlow Lite format

00:03:52.733 --> 00:03:54.434
using a converter,

00:03:54.434 --> 00:03:57.413
and then you update your apps
to invoke the interpreter

00:03:57.413 --> 00:03:59.743
using the Java or C++ APIs.

00:04:00.854 --> 00:04:02.913
One other thing
that I want to call out here

00:04:02.913 --> 00:04:06.612
is that iOS developers
have another option.

00:04:06.973 --> 00:04:08.993
They can convert
the trained TensorFlow graph

00:04:08.993 --> 00:04:11.084
into the CoreML format,

00:04:11.084 --> 00:04:14.178
and use the CoreML runtime directly.

00:04:14.178 --> 00:04:17.873
And this TensorFlow to CoreML converter
is something that we worked on

00:04:17.873 --> 00:04:20.683
together with folks who build CoreML.

00:04:22.281 --> 00:04:25.246
So there are some questions
that are top of mind for people

00:04:25.246 --> 00:04:27.610
every time we talk about TensorFlow Lite.

00:04:27.610 --> 00:04:31.963
And the two most common ones are
is it small in size, and is it fast?

00:04:32.447 --> 00:04:34.146
So let's talk about the first one.

00:04:35.106 --> 00:04:39.083
Keeping TensorFlow Lite small
was a key goal for us

00:04:39.083 --> 00:04:41.134
when we started building this,

00:04:41.134 --> 00:04:44.570
so the size of our interpreter
is only 75 KB.

00:04:45.055 --> 00:04:49.022
And when you include
all the supported ops, this is 400 KB.

00:04:49.556 --> 00:04:51.843
Another thing that is worth noting here

00:04:51.843 --> 00:04:55.033
is a feature called
<i>selective registration.</i>

00:04:55.033 --> 00:04:58.264
Developers have the option
to only include the ops

00:04:58.264 --> 00:05:00.969
that their models need,
and link those,

00:05:00.969 --> 00:05:02.979
and thereby keep the footprint small.

00:05:04.037 --> 00:05:05.674
So how did we do this?

00:05:05.674 --> 00:05:11.072
First of all, we've been pretty careful
in terms of which dependencies we include.

00:05:11.072 --> 00:05:14.944
And the second thing is
that TensorFlow Lite uses FlatBuffers,

00:05:14.944 --> 00:05:17.737
which are more memory-efficient
than protocol buffers are.

00:05:19.541 --> 00:05:22.570
And then moving on to the next question,
which is performance.

00:05:22.983 --> 00:05:25.793
So performance...
super important goal for us,

00:05:25.793 --> 00:05:29.798
and we made design choices
throughout the system to make it so.

00:05:31.677 --> 00:05:36.134
So let's look at the first thing,
which is the TensorFlow Lite format.

00:05:36.694 --> 00:05:39.262
We use FlatBuffers to represent models.

00:05:39.626 --> 00:05:42.973
And FlatBuffers is this cross-platform
serialization library

00:05:42.973 --> 00:05:46.783
that was originally developed
for game development use cases.

00:05:46.783 --> 00:05:50.783
And since then it's been used in other
performance-sensitive applications.

00:05:51.431 --> 00:05:54.033
The advantage of using FlatBuffers

00:05:54.033 --> 00:05:56.255
is that we are able
to directly access data

00:05:56.255 --> 00:06:00.324
without going through heavyweight
parsing or unpacking steps

00:06:00.324 --> 00:06:02.558
of the large files which contain weights.

00:06:03.333 --> 00:06:05.474
Another thing that we do
at conversion time

00:06:05.474 --> 00:06:08.404
is that we pre-fuse
the activations and biases,

00:06:08.404 --> 00:06:10.948
and that allows us
to execute faster later on.

00:06:11.944 --> 00:06:16.814
The TensorFlow Lite interpreter
uses static memory and execution plans,

00:06:16.814 --> 00:06:19.583
which allows us to load up faster.

00:06:20.895 --> 00:06:23.304
There are a set of optimized kernels

00:06:23.304 --> 00:06:28.072
which have been optimized to run fast
on the NEON on ARM platforms.

00:06:30.776 --> 00:06:34.673
We wanted to build TensorFlow Lite
so that we can take advantage

00:06:34.673 --> 00:06:40.233
of all the innovations that are happening
in silicon for these devices.

00:06:40.584 --> 00:06:43.274
So the first thing here
is that TensorFlow Lite supports

00:06:43.274 --> 00:06:46.013
the Android Neural Network API.

00:06:46.787 --> 00:06:50.704
The Qualcomm Hexagon DSP driver
is going to be coming out soon,

00:06:50.704 --> 00:06:53.733
as part of the Android P
developer release.

00:06:53.733 --> 00:06:56.844
And Huawei and MediaTek are
a couple of hardware renderers

00:06:56.844 --> 00:07:01.314
who have also announced their integration
with Android Neural Network APIs,

00:07:01.314 --> 00:07:03.692
so we should be seeing those
in the coming months as well.

00:07:04.683 --> 00:07:08.064
Second thing here is
that we have also been working

00:07:08.064 --> 00:07:10.943
on adding direct GPU acceleration,

00:07:10.943 --> 00:07:14.710
and we are using OpenGL on Android
and Metal on iOS.

00:07:17.617 --> 00:07:20.840
So quantization is the last bit
that I want to talk about

00:07:20.840 --> 00:07:22.755
in the context of performance.

00:07:23.264 --> 00:07:27.814
So roughly speaking, quantization
is this technique to store numbers

00:07:27.814 --> 00:07:31.403
and perform calculations on them
in representations

00:07:31.403 --> 00:07:35.164
which are more compact
than 32-bit floating point numbers.

00:07:35.683 --> 00:07:37.732
And this is important for two reasons.

00:07:37.732 --> 00:07:42.702
One, the smaller the model,
the better it is for these small devices.

00:07:43.324 --> 00:07:47.972
Second, many processors
have specialized synthe instruction sets

00:07:47.972 --> 00:07:49.734
which process fixed-point operands

00:07:49.734 --> 00:07:52.721
much faster than they process
floating point numbers.

00:07:53.459 --> 00:07:55.902
So a very naive way to do quantization

00:07:55.902 --> 00:07:58.124
would be simply shrink
the weights and activations

00:07:58.124 --> 00:08:00.000
after you're done training.

00:08:00.000 --> 00:08:02.748
But that leads to suboptimal accuracies.

00:08:03.446 --> 00:08:08.184
So we have been working
on doing quantization at training time,

00:08:08.184 --> 00:08:11.304
and we have recently released
a script which does this.

00:08:12.164 --> 00:08:13.373
And what we see is that,

00:08:13.373 --> 00:08:16.864
for architectures
like Mobilenet and Inception,

00:08:16.864 --> 00:08:19.604
we are able to get accuracies
which are fairly similar

00:08:19.604 --> 00:08:21.775
to their floating-point counterparts

00:08:21.775 --> 00:08:25.140
while seeing pretty impressive
gains in the latencies.

00:08:26.528 --> 00:08:30.950
So I've talked about a bunch
of different performance optimizations.

00:08:30.950 --> 00:08:33.913
Now let's see what do all
of these translate to together

00:08:33.913 --> 00:08:35.534
in terms of numbers.

00:08:37.382 --> 00:08:41.532
So these are two models
that we benchmarked,

00:08:41.532 --> 00:08:44.735
and we ran them
on the Android Pixel 2 phone.

00:08:45.186 --> 00:08:46.924
We were running these with four threads

00:08:46.924 --> 00:08:50.313
and using all the four
large cores of the Pixel 2.

00:08:50.971 --> 00:08:54.423
And what you see is that we are seeing
that these quantized models

00:08:54.423 --> 00:08:58.043
run more than three times faster
on TensorFlow Lite

00:08:58.043 --> 00:09:01.211
than their floating-point counterparts
run on TensorFlow.

00:09:03.557 --> 00:09:07.551
So I will move on now and talk about
what is supported on TensorFlow Lite.

00:09:08.029 --> 00:09:10.714
So currently it is limited
to inference only,

00:09:10.714 --> 00:09:14.663
although we are going to be working
on supporting training in the future.

00:09:14.663 --> 00:09:17.111
We support 50 commonly used operations,

00:09:17.111 --> 00:09:19.952
which developers can use
in their own models.

00:09:19.952 --> 00:09:21.170
In addition, they can use

00:09:21.170 --> 00:09:24.571
any of these popular
open source models that we support.

00:09:25.672 --> 00:09:29.442
One thing to note here is
that we have an extensible design,

00:09:29.442 --> 00:09:31.932
so if a developer is trying to use a model

00:09:31.932 --> 00:09:34.933
which has an op
which is not currently supported,

00:09:34.933 --> 00:09:39.539
they do have the option to use
what we call a "custom op," and use that.

00:09:40.151 --> 00:09:42.780
And later in this talk
we will show you some code snippets

00:09:42.780 --> 00:09:45.179
and how you can do that yourself.

00:09:46.421 --> 00:09:50.502
So this is all theory
about TensorFlow Lite.

00:09:50.963 --> 00:09:54.314
Let me show you a quick video
of TensorFlow Lite in practice.

00:09:57.026 --> 00:09:59.771
So we took the simple mobile-like model

00:09:59.771 --> 00:10:04.083
and we retrain on some common objects
that we could find around our office.

00:10:04.083 --> 00:10:08.468
And this is our demo classification app
which is already open sourced.

00:10:08.468 --> 00:10:12.137
As you can see,
it is able to classify these objects.

00:10:15.251 --> 00:10:16.906
(laughter)

00:10:18.573 --> 00:10:19.998
So that was demo.

00:10:19.998 --> 00:10:22.327
Now let's talk about production stuff.

00:10:22.798 --> 00:10:24.058
So I'm very excited to say

00:10:24.058 --> 00:10:26.498
that we have been working
with other teams in Google

00:10:26.498 --> 00:10:29.313
to bring TensorFlow Lite to Google Apps.

00:10:29.953 --> 00:10:32.955
So the Portrait Mode on Android camera,

00:10:32.955 --> 00:10:35.025
"Hey Google" on Google Assistant,

00:10:35.677 --> 00:10:37.374
SmartReply on WearOS--

00:10:37.374 --> 00:10:39.994
these are some examples of apps
which are going to be powered

00:10:39.994 --> 00:10:43.038
by TensorFlow Lite in the coming future.

00:10:44.885 --> 00:10:47.444
And with that,
I'm going to hand off to Andrew,

00:10:47.444 --> 00:10:50.043
who will talk to you about
how you can use TensorFlow Lite.

00:10:51.063 --> 00:10:52.740
Thanks for the introduction.

00:10:53.741 --> 00:10:58.955
So, now that we see what TensorFlow is,
or TensorFlow Lite is, in particular,

00:10:58.955 --> 00:11:01.697
let's find out how to use it,
let's jump into the code.

00:11:01.697 --> 00:11:05.677
So the first step of the four-step process
is to get a model.

00:11:05.677 --> 00:11:08.753
You can download that off the internet
or you can train it yourself.

00:11:08.753 --> 00:11:09.956
Once you have a model,

00:11:09.956 --> 00:11:12.266
you need to convert it
into TensorFlow Lite format,

00:11:12.266 --> 00:11:13.743
using our converter.

00:11:13.743 --> 00:11:14.709
Once that's done,

00:11:14.709 --> 00:11:17.406
there might be some ops
that you want to spot optimize,

00:11:17.406 --> 00:11:19.958
using special intrinsics
or special hardware

00:11:19.958 --> 00:11:22.328
that is specific to your application,

00:11:22.328 --> 00:11:24.298
or there might be ops
that we don't yet support.

00:11:24.298 --> 00:11:26.099
You need to write custom ops for that.

00:11:26.099 --> 00:11:28.309
Once all that's done,
you can go to your app

00:11:28.309 --> 00:11:31.253
and you can write it
using the client API of your choice.

00:11:32.268 --> 00:11:34.703
So let's look at the conversion process.

00:11:34.703 --> 00:11:38.601
So we support input of SavedModel
or frozen GraphDef.

00:11:38.601 --> 00:11:41.766
Once you have that, you can use
our Python API or a command line tool.

00:11:41.766 --> 00:11:44.243
Here we're showing the Python interface,

00:11:44.243 --> 00:11:46.424
so we give it a directory of a SavedModel

00:11:46.424 --> 00:11:49.547
and it gives us a TF Lite FlatBuffer out.

00:11:50.185 --> 00:11:54.453
Once that's done...
Well, before that's done,

00:11:54.453 --> 00:11:56.323
there might be some things
that you need to use

00:11:56.323 --> 00:11:58.110
to make this work better.

00:11:58.110 --> 00:12:00.722
So the first thing is you need
to use a frozen GraphDef--

00:12:00.722 --> 00:12:03.689
Or SavedModel is actually easier,
because it doesn't need to be frozen.

00:12:03.689 --> 00:12:06.715
Second, we need to avoid
unsupported operators.

00:12:06.715 --> 00:12:09.391
A lot of times a training graph
will have a lot of conditional logic

00:12:09.391 --> 00:12:12.425
or checks that are really
not necessary for inference.

00:12:12.425 --> 00:12:16.712
Sometimes it's useful to create
a special inference script.

00:12:16.712 --> 00:12:19.533
Lastly, if you need to look
at what your model is doing,

00:12:19.533 --> 00:12:21.771
the TensorBoard Visualizer
is useful for the GraphDef,

00:12:21.771 --> 00:12:24.051
but we also have
a TensorFlow Lite Visualizer

00:12:24.051 --> 00:12:26.102
to look at the FlatBuffers.

00:12:26.102 --> 00:12:29.003
And looking at these compared
to each other can help you.

00:12:29.003 --> 00:12:31.743
If you find any issues,
please file them with GitHub,

00:12:31.743 --> 00:12:35.165
and we will respond to them
as we get needs.

00:12:35.658 --> 00:12:38.210
So lastly, write custom operators.

00:12:38.210 --> 00:12:39.753
So let's see how to do that.

00:12:39.753 --> 00:12:43.426
To write a custom operator
in TensorFlow Lite is relatively simple,

00:12:43.426 --> 00:12:46.156
and it involves defining four functions.

00:12:46.156 --> 00:12:47.672
The main one is <i>invoke</i>.

00:12:47.672 --> 00:12:50.670
So here I've defined an operator
that returns <i>pi</i>,

00:12:50.670 --> 00:12:52.190
just a one scalar <i>pi</i>.

00:12:52.190 --> 00:12:53.472
This is really useful.

00:12:53.472 --> 00:12:57.569
So once you've done that
you need to register these new operations.

00:12:57.569 --> 00:13:00.231
Now there's a number of ways
that you can register operations.

00:13:00.231 --> 00:13:03.231
If you don't have any custom ops
and you don't need to do any overriding,

00:13:03.231 --> 00:13:05.020
you can use the built-in Op Resolver.

00:13:05.020 --> 00:13:08.770
But sometimes you might want to ship
a TensorFlow Lite binary

00:13:08.770 --> 00:13:10.020
that's much smaller,

00:13:10.020 --> 00:13:11.970
so you might want to do
selective registration.

00:13:11.970 --> 00:13:15.782
In that case, you should ship
a needed ops resolver

00:13:15.782 --> 00:13:18.327
or include your custom ops
in that same thing.

00:13:18.870 --> 00:13:22.163
Once you have that op set,
you just plug it into the interpreter.

00:13:23.014 --> 00:13:25.245
Okay, so now we've talked
about custom operations.

00:13:25.245 --> 00:13:28.443
Let's see how we actually
put this into practice in the Java API.

00:13:28.902 --> 00:13:33.154
So in Java you give it
your TensorFlow Lite FlatBuffer schema

00:13:33.154 --> 00:13:35.075
to the constructor of Interpreter.

00:13:35.075 --> 00:13:37.534
Once you've done that,
you fill in your inputs and outputs.

00:13:37.534 --> 00:13:38.972
And lastly, you run <i>run</i>,

00:13:38.972 --> 00:13:41.766
which will populate the outputs
with the results of the inference.

00:13:41.766 --> 00:13:43.068
Really simple.

00:13:43.554 --> 00:13:46.328
Next, how do I actually include this?

00:13:46.328 --> 00:13:47.846
Do I need to compile a bunch of code?

00:13:47.846 --> 00:13:49.287
We're working really hard to make it

00:13:49.287 --> 00:13:52.020
so that you can use TensorFlow
from the pip,

00:13:52.020 --> 00:13:53.128
and do all your training,

00:13:53.128 --> 00:13:55.648
and then you don't need
to compile TensorFlow.

00:13:55.648 --> 00:13:57.107
So one of the things we provide

00:13:57.107 --> 00:14:00.517
is an Android Gradle file
that you can include,

00:14:00.517 --> 00:14:03.887
and then you don't need to compile
your own TF Lite for an Android app.

00:14:03.887 --> 00:14:07.147
We have a similar thing for CocoaPods
to run TensorFlow Lite.

00:14:07.147 --> 00:14:09.567
So this will make it a lot easier to do.

00:14:09.567 --> 00:14:12.486
So, now that we know
how to use TensorFlow,

00:14:12.486 --> 00:14:13.738
let's look at our roadmap.

00:14:13.738 --> 00:14:16.228
As we move forward,
we're going to add more ops

00:14:16.228 --> 00:14:19.359
so we can support more and more
TensorFlow models out of the box.

00:14:19.359 --> 00:14:22.328
Second, we're going to add
on-device training,

00:14:22.328 --> 00:14:24.737
and we're going to look at
so you can do hybrid training--

00:14:24.737 --> 00:14:28.448
some of it on the server,
some of it on your device,

00:14:28.448 --> 00:14:29.677
wherever it makes sense.

00:14:29.677 --> 00:14:31.118
That should be an option for you.

00:14:31.118 --> 00:14:33.866
And third, we're going to continue
to improve tooling

00:14:33.866 --> 00:14:36.787
to analyze graphs better,
to do more optimizations.

00:14:36.787 --> 00:14:39.438
We have more than we can talk about
that we're working on,

00:14:39.438 --> 00:14:41.797
but we hope that this small peek
of what we're doing

00:14:41.797 --> 00:14:45.497
and what our basis is will make you
interested and excited to try it.

00:14:46.185 --> 00:14:48.606
So, there's one remaining question left,

00:14:48.606 --> 00:14:51.616
which is should I use
TensorFlow Mobile or TensorFlow Lite?

00:14:51.616 --> 00:14:54.015
So TensorFlow Mobile is
a stripped down version of TensorFlow

00:14:54.015 --> 00:14:56.044
that includes a subset of the ops.

00:14:56.706 --> 00:14:58.942
Moving forward,
we're going to put all of our effort

00:14:58.942 --> 00:15:02.162
into improving TensorFlow Lite
and improving its ability

00:15:02.162 --> 00:15:04.264
to map to custom hardware.

00:15:04.264 --> 00:15:08.424
So we recommend that you target
TensorFlow Lite as soon as possible,

00:15:08.424 --> 00:15:09.801
if it's possible.

00:15:09.801 --> 00:15:12.642
If there's some functionality
that you still need in TensorFlow Mobile,

00:15:12.642 --> 00:15:15.263
let us know, and we'll work
to improve TensorFlow Lite

00:15:15.263 --> 00:15:16.808
in a commensurate way.

00:15:17.821 --> 00:15:19.812
Okay, demo time!

00:15:19.812 --> 00:15:21.972
So nothing like a live demo, right?

00:15:22.322 --> 00:15:25.308
So let's switch over to the demo feed,
and we'll talk about it.

00:15:27.671 --> 00:15:29.612
So, we saw some mobile phones.

00:15:29.612 --> 00:15:32.432
Mobile phones are really exciting
because everybody has them.

00:15:32.432 --> 00:15:35.074
But another thing that's happening
is these edge computing devices.

00:15:35.074 --> 00:15:37.513
So one of the most popular ones
that has emerged for hobbyists

00:15:37.513 --> 00:15:39.003
is the Raspberry Pi.

00:15:39.003 --> 00:15:41.692
So what I've done is built some hardware
around the Raspberry Pi.

00:15:41.692 --> 00:15:43.072
So if we zoom in we can see

00:15:43.072 --> 00:15:47.333
that we have the Raspberry Pi
mobile board.

00:15:47.333 --> 00:15:51.153
And this is just a system on chips,
similar to a cellphone chip.

00:15:51.153 --> 00:15:53.143
And one of the great things
about the Raspberry Pi

00:15:53.143 --> 00:15:54.484
is that they're really cheap.

00:15:54.484 --> 00:15:56.873
Another great thing is that
they can interface the hardware.

00:15:56.873 --> 00:15:59.645
So here we're interfaced
to a microcontroller

00:15:59.645 --> 00:16:03.324
that allows us to basically
control these motors.

00:16:03.324 --> 00:16:06.583
These are server motors,
common in RC cars,

00:16:06.583 --> 00:16:09.634
and they allow us to move the camera
left and right and up and down.

00:16:09.634 --> 00:16:11.364
It's essentially a camera gimbal.

00:16:11.364 --> 00:16:15.142
And then we have it connected
to a Raspberry Pi-compatible camera.

00:16:16.029 --> 00:16:17.668
So, what are we going to do with this?

00:16:17.668 --> 00:16:20.748
Well, we showed
the classification demo before.

00:16:20.748 --> 00:16:22.648
Let's look at an SSD example.

00:16:22.648 --> 00:16:24.207
So this is single-shot detection.

00:16:24.207 --> 00:16:29.302
So what this can do is it can basically
identify bounding boxes in an image.

00:16:29.302 --> 00:16:32.147
So given an image
I get multiple bounding boxes.

00:16:32.147 --> 00:16:35.123
So, for example, we have an apple here.

00:16:35.820 --> 00:16:37.888
And it identifies an apple.

00:16:38.738 --> 00:16:40.718
Now the really cool thing
we can do with this is,

00:16:40.718 --> 00:16:41.887
now that we have the motor,

00:16:41.887 --> 00:16:44.127
we can tell it to center the apple.

00:16:44.127 --> 00:16:46.992
So we turned on the motors now,
so now the motors are active.

00:16:46.992 --> 00:16:49.862
And as I move it around
it's going to keep the apple

00:16:49.862 --> 00:16:51.184
as centered as possible.

00:16:51.184 --> 00:16:53.204
If I go up, it'll go up.

00:16:53.204 --> 00:16:55.024
If I go down, it will go down.

00:16:55.024 --> 00:16:57.203
So this is really fun.
What could you use this for?

00:16:57.203 --> 00:17:00.750
Well, if you're a person,
it can also identify you.

00:17:00.750 --> 00:17:02.652
So currently I have that filtered out.

00:17:02.652 --> 00:17:05.943
So if I stand back
it's going to center on me.

00:17:05.943 --> 00:17:08.254
So I could use this
as sort of a virtual videographer.

00:17:08.254 --> 00:17:10.944
Imagine a professor
wants to tape their lecture,

00:17:10.944 --> 00:17:13.473
but they don't have a camera person.

00:17:13.473 --> 00:17:16.024
So this would be a great way to do that.

00:17:16.024 --> 00:17:18.643
I'm sure that all the hobbyists around

00:17:18.643 --> 00:17:21.041
can now use TensorFlow 
in a really simple way,

00:17:21.041 --> 00:17:24.344
can come up with many better applications
than what I'm showing here.

00:17:25.012 --> 00:17:28.238
But I find it fun, and I hope that you do.

00:17:28.238 --> 00:17:30.606
And I'm not an electrical engineer
or a mechanical engineer,

00:17:30.606 --> 00:17:32.258
so you can do this too.

00:17:33.368 --> 00:17:34.593
Alright.

00:17:34.593 --> 00:17:36.328
So thanks for showing the demo.

00:17:36.328 --> 00:17:38.215
Let's go back to the slides please.

00:17:40.773 --> 00:17:42.874
So I had a backup video,
just in case it didn't work.

00:17:42.874 --> 00:17:44.036
(laughter)

00:17:44.036 --> 00:17:45.910
It's always a good plan.

00:17:46.429 --> 00:17:48.011
But we didn't need it, so that's great.

00:17:48.011 --> 00:17:49.476
Let's summarize.

00:17:49.476 --> 00:17:51.448
So, what should you do?

00:17:51.448 --> 00:17:53.236
I'm sure you all want to use
TensorFlow Lite.

00:17:53.236 --> 00:17:54.235
Where do you get it?

00:17:54.235 --> 00:17:59.082
Well, you can get it on GitHub,
right around the TensorFlow repository.

00:17:59.082 --> 00:18:00.735
How you can find out about it

00:18:00.735 --> 00:18:02.706
Is looking at
the TensorFlow Lite documentation

00:18:02.706 --> 00:18:05.505
on the <i>tensorflow.org</i> website.

00:18:05.505 --> 00:18:09.997
In addition, we're creating
a mailing list, <i>tflite@tensorflow.org</i>,

00:18:09.997 --> 00:18:12.976
that you can email about some
of your issues and your ideas.

00:18:12.976 --> 00:18:17.537
And we're also excited to hear
about what you use TensorFlow Lite for.

00:18:18.363 --> 00:18:21.546
So, with that I hope
to hear from all of you.

00:18:22.057 --> 00:18:24.047
One at a time, please, but...
(chuckles)

00:18:24.772 --> 00:18:27.995
So with that I want to thank everybody,
thank Sarah for her presentation.

00:18:27.995 --> 00:18:29.615
I thank your attendance,

00:18:29.615 --> 00:18:32.306
everybody who's around the world
listening to this.

00:18:32.306 --> 00:18:34.966
And, in addition, this was work
that we worked very hard

00:18:34.966 --> 00:18:39.025
with other members of the Google team,
lots of different teams,

00:18:39.025 --> 00:18:41.356
so there's a lot of work
that went into this.

00:18:41.356 --> 00:18:42.569
So thanks a lot.

00:18:42.569 --> 00:18:44.282
(applause)

00:18:44.282 --> 00:18:47.416
♪ (music) ♪

