WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.823
[MUSIC PLAYING]

00:00:04.823 --> 00:00:05.990
JULIE HANNA: Hello everyone.

00:00:05.990 --> 00:00:07.510
Welcome.

00:00:07.510 --> 00:00:08.920
My name is Julie Hanna.

00:00:08.920 --> 00:00:11.920
I'm executive chair
of the board at Kiva.

00:00:11.920 --> 00:00:14.410
I'm a venture partner
at Obvious Ventures.

00:00:14.410 --> 00:00:18.370
And I'm also an advisor
to X, formerly known

00:00:18.370 --> 00:00:21.400
as Google X, Alphabet's
Moonshot Factory.

00:00:21.400 --> 00:00:23.980
And as you all
know, with me here

00:00:23.980 --> 00:00:27.910
is Astro Teller, the
captain of Moonshots.

00:00:27.910 --> 00:00:31.630
We're here to talk about
responsible innovation--

00:00:31.630 --> 00:00:35.860
or Astro, as you prefer to
say lasting innovation--

00:00:35.860 --> 00:00:39.370
and how you've
engineered that at X.

00:00:39.370 --> 00:00:42.550
Having had the
opportunity to spend time

00:00:42.550 --> 00:00:46.090
with you over the
past year, I have

00:00:46.090 --> 00:00:48.820
to say it's a fascinating
and remarkable place.

00:00:48.820 --> 00:00:52.240
Not just because of your
mission to repeatedly produce

00:00:52.240 --> 00:00:57.250
moonshots, like the driverless
cars or stratospheric internet

00:00:57.250 --> 00:01:01.660
balloons, but also because
the scale and scope

00:01:01.660 --> 00:01:03.350
of what you're trying to do.

00:01:03.350 --> 00:01:07.300
So tell us about X. It's
a little bit of a secrecy

00:01:07.300 --> 00:01:09.040
about it and mystique.

00:01:09.040 --> 00:01:10.280
Why does it exist?

00:01:10.280 --> 00:01:12.470
Why does the world need
a Moonshot Factory?

00:01:12.470 --> 00:01:15.940
What's the impact you're
seeking to have in the world?

00:01:15.940 --> 00:01:19.600
ASTRO TELLER: So X was
created before there

00:01:19.600 --> 00:01:21.610
was something called Alphabet.

00:01:21.610 --> 00:01:24.730
But the founders of
Google, Larry and Sergey,

00:01:24.730 --> 00:01:28.000
had a vision that
over time, we were

00:01:28.000 --> 00:01:29.680
going to want this
thing which is now

00:01:29.680 --> 00:01:32.770
called Alphabet to
take on meaningfully

00:01:32.770 --> 00:01:34.620
new problems in the world.

00:01:34.620 --> 00:01:36.370
That doesn't mean that
Google itself can't

00:01:36.370 --> 00:01:37.800
take on new problems
in the world,

00:01:37.800 --> 00:01:40.300
but they're probably problems
that Google would either never

00:01:40.300 --> 00:01:42.880
get to or would
get too much later.

00:01:42.880 --> 00:01:45.880
And so setting up a separate
thing, which was originally

00:01:45.880 --> 00:01:49.510
called Google X and
now called X, to go

00:01:49.510 --> 00:01:53.090
find some of those problems
to care about in the world

00:01:53.090 --> 00:01:55.690
that Google at least wasn't
currently working on,

00:01:55.690 --> 00:02:00.910
and then to try to find unusual,
unexpected ways of trying

00:02:00.910 --> 00:02:03.940
to help solve those problems,
and then the technology

00:02:03.940 --> 00:02:06.880
that would make that science
fiction sounding product

00:02:06.880 --> 00:02:08.470
or service actually happen--

00:02:08.470 --> 00:02:11.798
maybe hopefully-- to
help solve that problem.

00:02:11.798 --> 00:02:13.090
That's what X was set up to do.

00:02:13.090 --> 00:02:19.580
So X's product is not so
much a thing as businesses.

00:02:19.580 --> 00:02:22.240
So alphabet gives us money.

00:02:22.240 --> 00:02:24.440
We spend a lot of time
going through this process.

00:02:24.440 --> 00:02:27.250
And then very occasionally,
out the other side of X

00:02:27.250 --> 00:02:31.510
comes something-- either like
Google Brain that goes back

00:02:31.510 --> 00:02:34.720
to Google, or something like
Verily, the life science

00:02:34.720 --> 00:02:38.350
business is for Alphabet, or
Waymo, the self-driving car

00:02:38.350 --> 00:02:41.890
business for Alphabet, Loon, the
stratospheric balloon project

00:02:41.890 --> 00:02:44.560
that you mentioned for
Alphabet, those kinds of things.

00:02:44.560 --> 00:02:47.680
That's our product, the
thing that we produce

00:02:47.680 --> 00:02:50.710
after we go through a
very long process of sort

00:02:50.710 --> 00:02:53.920
of exploring and trying
to find these things.

00:02:53.920 --> 00:02:55.980
JULIE HANNA: So as
I eluded to earlier,

00:02:55.980 --> 00:02:58.750
you prefer this term
lasting animation

00:02:58.750 --> 00:03:00.370
to responsible innovation.

00:03:00.370 --> 00:03:04.300
What about that distinction
is important to you?

00:03:04.300 --> 00:03:06.727
ASTRO TELLER: Well,
I wouldn't want to--

00:03:06.727 --> 00:03:07.810
I appreciate the question.

00:03:07.810 --> 00:03:09.560
I wouldn't want it to
sound like we're not

00:03:09.560 --> 00:03:12.700
interested in being
responsible in our innovation,

00:03:12.700 --> 00:03:14.950
but I prefer the term
lasting innovation.

00:03:14.950 --> 00:03:19.540
My concern is if
you believe today

00:03:19.540 --> 00:03:22.990
that you're going to
do some innovation,

00:03:22.990 --> 00:03:26.680
and that all you need to
check the responsible box

00:03:26.680 --> 00:03:29.300
is to sort of look
inside yourself and say,

00:03:29.300 --> 00:03:30.670
do I have good intentions?

00:03:30.670 --> 00:03:32.520
Yep, I'm responsible.

00:03:32.520 --> 00:03:34.462
Check, done.

00:03:34.462 --> 00:03:35.920
I don't think that
that's the alpha

00:03:35.920 --> 00:03:39.370
and the omega of the actual
obligation of a group doing

00:03:39.370 --> 00:03:40.580
innovation.

00:03:40.580 --> 00:03:43.930
I think the real issue is
whether in the long term

00:03:43.930 --> 00:03:47.560
society is happy with the thing
that you put into society.

00:03:47.560 --> 00:03:50.600
If they are, even if it was a
little painful on the way in,

00:03:50.600 --> 00:03:52.840
you have some obligation
to help reduce that pain,

00:03:52.840 --> 00:03:54.238
but it's probably worth it.

00:03:54.238 --> 00:03:56.530
And in the long run, no matter
how good your intentions

00:03:56.530 --> 00:03:58.600
were, if it doesn't
work out very well, who

00:03:58.600 --> 00:04:01.828
cares that you felt responsible
on the day you were making it?

00:04:01.828 --> 00:04:03.370
JULIE HANNA: So a
lot of people would

00:04:03.370 --> 00:04:08.050
argue that it's hard enough
to build a product that people

00:04:08.050 --> 00:04:12.070
will use in the here and
now, much less something

00:04:12.070 --> 00:04:14.290
that's going to be relevant
over the long term,

00:04:14.290 --> 00:04:18.220
and that thinking proactively
about the impact of products

00:04:18.220 --> 00:04:22.850
that we build is a tax on
innovation in the wild,

00:04:22.850 --> 00:04:26.620
as we like to do in Silicon
Valley and Velocity.

00:04:26.620 --> 00:04:27.640
What do you say to that?

00:04:27.640 --> 00:04:32.110
Why do you think it's important
for us technologists to think

00:04:32.110 --> 00:04:34.400
about enduring innovation?

00:04:34.400 --> 00:04:36.640
When's the right time
to think about it?

00:04:36.640 --> 00:04:39.400
Is it from the onset
after your products

00:04:39.400 --> 00:04:42.220
have been successful
in the market?

00:04:42.220 --> 00:04:42.950
Constantly?

00:04:42.950 --> 00:04:44.410
Whose job is it?

00:04:44.410 --> 00:04:46.380
Should companies be
hiring ethicists?

00:04:46.380 --> 00:04:48.000
Is that everybody's job?

00:04:48.000 --> 00:04:49.130
Both?

00:04:49.130 --> 00:04:49.540
ASTRO TELLER: You're
probably going

00:04:49.540 --> 00:04:50.860
to ask a whole bunch
of those things

00:04:50.860 --> 00:04:52.318
over again because
there were about

00:04:52.318 --> 00:04:54.230
six different interesting
questions in there.

00:04:54.230 --> 00:04:57.430
But the first one
is it really depends

00:04:57.430 --> 00:04:59.290
what your time horizon is.

00:04:59.290 --> 00:05:03.640
If you're going to declare
victory or defeat over six

00:05:03.640 --> 00:05:05.650
months or a year, whatever.

00:05:05.650 --> 00:05:08.080
Go make furbies, whatever it is.

00:05:08.080 --> 00:05:09.970
By the time people
are actually angry,

00:05:09.970 --> 00:05:12.010
you'll be out of the business.

00:05:12.010 --> 00:05:16.190
At X, we happened to have picked
a very long time horizons.

00:05:16.190 --> 00:05:19.330
So when we talk
about speed at X,

00:05:19.330 --> 00:05:22.120
we're very serious
about going quickly,

00:05:22.120 --> 00:05:26.780
but we're measuring that speed
over a very long time horizons.

00:05:26.780 --> 00:05:29.920
So if we very quickly
make something

00:05:29.920 --> 00:05:32.620
that ultimately society is
going to reject like a bad organ

00:05:32.620 --> 00:05:35.450
transplant, that's
not really going fast.

00:05:35.450 --> 00:05:38.990
That's a lot of motion, but it's
not actually a lot of progress.

00:05:38.990 --> 00:05:41.740
Once you come to terms
with the implications

00:05:41.740 --> 00:05:43.870
of these super
long time horizons

00:05:43.870 --> 00:05:47.710
and commit to the
concept of lasting, then

00:05:47.710 --> 00:05:49.090
what should we do first?

00:05:49.090 --> 00:05:52.960
What counts is going quickly
right now is usually what

00:05:52.960 --> 00:05:55.480
are the things that we
can do that will cause

00:05:55.480 --> 00:05:59.740
us to learn today and
head off at the past

00:05:59.740 --> 00:06:04.180
as many long distance
failure modes as we can,

00:06:04.180 --> 00:06:07.780
and seeing through that
lens, thinking about the way

00:06:07.780 --> 00:06:12.070
that society is going to
experience our innovations

00:06:12.070 --> 00:06:15.190
as we birth them into
the world, is actually

00:06:15.190 --> 00:06:20.620
critical to 10 or 15 year
thinkings on velocity.

00:06:20.620 --> 00:06:25.330
Not in opposition to it.

00:06:25.330 --> 00:06:29.050
You were just asking should
we be hiring some ethicists?

00:06:29.050 --> 00:06:33.640
I have a slightly odd sounding
maybe response to that.

00:06:33.640 --> 00:06:35.590
We have no ethicists
on staff, and that's

00:06:35.590 --> 00:06:37.870
very much on purpose.

00:06:37.870 --> 00:06:40.580
But that's not because we
don't care about the problem.

00:06:40.580 --> 00:06:43.150
I really don't like
the idea though.

00:06:43.150 --> 00:06:46.570
You all, you're in
charge of making stuff.

00:06:46.570 --> 00:06:49.390
You guys, I'm going
to hire you to gauze

00:06:49.390 --> 00:06:52.100
them to be good human beings.

00:06:52.100 --> 00:06:54.670
That just doesn't seem like
it's going to work out.

00:06:54.670 --> 00:06:57.610
What if we were to
make some stuff,

00:06:57.610 --> 00:07:01.940
and we were to agree ahead
of time that it wasn't going

00:07:01.940 --> 00:07:05.650
to count as us doing a good job
unless the things that we made

00:07:05.650 --> 00:07:08.230
were ultimately really
positive for society?

00:07:08.230 --> 00:07:12.970
Then none of us has
ethicist as our title,

00:07:12.970 --> 00:07:16.150
but we are all practicing
a lot of the thinking that

00:07:16.150 --> 00:07:19.660
goes into doing the moral
math and not just finance

00:07:19.660 --> 00:07:23.343
math around the products or
services that we're making.

00:07:23.343 --> 00:07:25.510
JULIE HANNA: Yeah, I heard
a little clap over there.

00:07:25.510 --> 00:07:27.380
And I have to say I echo that.

00:07:27.380 --> 00:07:29.200
I think it's a
refreshing evolution

00:07:29.200 --> 00:07:32.500
of the cultural mindset
we've had in the valley.

00:07:32.500 --> 00:07:35.710
And it stands in stark
contrast with the culture

00:07:35.710 --> 00:07:38.510
of experimentation, where
our attitude has been hey,

00:07:38.510 --> 00:07:41.140
if you don't get it right,
you can only fix it later.

00:07:41.140 --> 00:07:43.510
After all, it's just bits.

00:07:43.510 --> 00:07:47.030
And we can just iterate and
kind of fail forward fast

00:07:47.030 --> 00:07:47.780
as we like to say.

00:07:47.780 --> 00:07:51.310
But what we're finding
now is that as bits go so

00:07:51.310 --> 00:07:55.820
do atoms, that our digital lives
shape our physical reality.

00:07:55.820 --> 00:07:58.630
And as a result,
everything's being refactor--

00:07:58.630 --> 00:08:00.820
the way we communicate,
collaborate,

00:08:00.820 --> 00:08:05.680
the way we make love and
war, Tinder anyone, drones,

00:08:05.680 --> 00:08:08.830
whether we work.

00:08:08.830 --> 00:08:12.220
And if the future of work
is that work is finished,

00:08:12.220 --> 00:08:15.340
perhaps our identities
are being refactor to.

00:08:15.340 --> 00:08:18.100
And it's raising really
some fundamental existential

00:08:18.100 --> 00:08:21.640
questions like who am I,
and where do I belong,

00:08:21.640 --> 00:08:26.770
and who is my tribe, and who has
the right to be in that tribe?

00:08:26.770 --> 00:08:28.780
Not to mention the fragile
state of the planet.

00:08:28.780 --> 00:08:31.840
So I think we, in
some ways, are still

00:08:31.840 --> 00:08:35.380
grasping the far reaching
effect of the products

00:08:35.380 --> 00:08:36.970
that we've unleashed
on the world.

00:08:36.970 --> 00:08:40.299
And in many cases,
iterating after the fact

00:08:40.299 --> 00:08:45.010
really essentially means dealing
with the damaging effects

00:08:45.010 --> 00:08:50.290
of the products that
we have released.

00:08:50.290 --> 00:08:54.190
As creators, as an industry,
we are really grappling,

00:08:54.190 --> 00:08:58.510
I think, to how to become more
proactive rather than kind

00:08:58.510 --> 00:09:03.580
of entirely reactive and
experimental and our discovery.

00:09:03.580 --> 00:09:05.620
What I find fascinating
when we talk about this

00:09:05.620 --> 00:09:09.770
is that this is far from an
intellectual exercise for you.

00:09:09.770 --> 00:09:14.350
In fact, I dare say it's
the longest running thread

00:09:14.350 --> 00:09:15.280
in your life.

00:09:15.280 --> 00:09:17.920
Your journey is a
fascinating study

00:09:17.920 --> 00:09:20.800
in intergenerational innovation.

00:09:20.800 --> 00:09:22.630
You have quite a family legacy.

00:09:22.630 --> 00:09:24.760
And I don't think it's
an underrated statement

00:09:24.760 --> 00:09:27.850
to say that it
profoundly changed

00:09:27.850 --> 00:09:31.125
the arc of human history.

00:09:31.125 --> 00:09:32.500
I can't help but
think it figures

00:09:32.500 --> 00:09:34.670
prominently in the person
that you've become.

00:09:34.670 --> 00:09:38.110
So tell us a little bit
about your familial lineage

00:09:38.110 --> 00:09:40.960
and how it shaped your
philosophy and views.

00:09:44.700 --> 00:09:48.330
ASTRO TELLER: There's a lot
in that too, but you're right.

00:09:48.330 --> 00:09:50.640
It has had a big impact on me.

00:09:50.640 --> 00:09:54.870
So my father's father
was Edward Teller--

00:09:54.870 --> 00:09:58.050
colloquially referred to as the
Father of the Hydrogen Bomb.

00:09:58.050 --> 00:10:00.990
He helped start Lawrence
Livermore Labs up the road

00:10:00.990 --> 00:10:05.430
here, The Star Wars
Defense Initiative.

00:10:05.430 --> 00:10:11.332
He was a world-class
scientist, but he was also

00:10:11.332 --> 00:10:14.010
excommunicated from the
scientific community

00:10:14.010 --> 00:10:16.830
for his work on
some of the weapons.

00:10:16.830 --> 00:10:19.770
And so he was held up by
some people in society

00:10:19.770 --> 00:10:22.590
as one of the worst human
beings on the planet.

00:10:22.590 --> 00:10:27.810
He also grew up in World War
I and then in World War II.

00:10:27.810 --> 00:10:30.180
And he saw the horrible
things that humans

00:10:30.180 --> 00:10:32.230
are willing to each other.

00:10:32.230 --> 00:10:35.640
And this country adopted
him as an immigrant.

00:10:35.640 --> 00:10:38.740
And he believed that for
all this country's flaws,

00:10:38.740 --> 00:10:40.620
it was the best
country in the world,

00:10:40.620 --> 00:10:43.980
and that finding a way to
protect democracy, and protect

00:10:43.980 --> 00:10:47.310
freedom, and to put strength
into the hands of people

00:10:47.310 --> 00:10:51.750
who could be trusted with it
was a humanity saving exercise.

00:10:51.750 --> 00:10:54.150
And there are roughly
as many people

00:10:54.150 --> 00:10:56.588
who see him as one
of the greatest

00:10:56.588 --> 00:10:58.380
human beings in history
as one of the worst

00:10:58.380 --> 00:10:59.880
human beings in history.

00:10:59.880 --> 00:11:01.710
And I'm not so much
taking a position on it

00:11:01.710 --> 00:11:06.480
as I watched him wrestle
with these things about what

00:11:06.480 --> 00:11:08.922
his obligation to the world was.

00:11:08.922 --> 00:11:10.630
He didn't want to
spend his time on this.

00:11:10.630 --> 00:11:12.180
He wanted to just
be a scientist.

00:11:12.180 --> 00:11:14.610
And he was incredibly
sad for much of his life

00:11:14.610 --> 00:11:17.370
for having been excommunicated
from the scientific community.

00:11:17.370 --> 00:11:20.670
But he felt like he owed it to
the world to help in these ways

00:11:20.670 --> 00:11:22.350
that he could.

00:11:22.350 --> 00:11:26.640
My mother's father
was the modern founder

00:11:26.640 --> 00:11:27.780
of microeconomics.

00:11:27.780 --> 00:11:30.780
He won a Nobel Prize in 1983.

00:11:30.780 --> 00:11:34.240
And he was manic depressive.

00:11:34.240 --> 00:11:39.510
And I watched him profoundly
influence his field,

00:11:39.510 --> 00:11:43.860
but not really enjoy it,
and set as many fires

00:11:43.860 --> 00:11:46.590
I think as he put out honestly.

00:11:46.590 --> 00:11:49.290
I watched these two people
go through their lives

00:11:49.290 --> 00:11:53.700
and have an impact on the world,
and thought it's definitely not

00:11:53.700 --> 00:11:54.900
going to be easy.

00:11:54.900 --> 00:11:57.030
Some of the lessons
they've learned,

00:11:57.030 --> 00:11:59.190
I can sort of take
on and maybe take

00:11:59.190 --> 00:12:02.310
a path that will hurt a little
less, or maybe net a little bit

00:12:02.310 --> 00:12:05.170
more to the positive
in the world.

00:12:05.170 --> 00:12:08.250
I really became a
deep believer partly

00:12:08.250 --> 00:12:11.670
because of them, and because I
was the dumb one in the family.

00:12:11.670 --> 00:12:14.950
Not just the two of them, it was
other people too unfortunately.

00:12:14.950 --> 00:12:19.080
So I ended up becoming a deep
believer early in my life

00:12:19.080 --> 00:12:22.590
in the power of teams rather
than the power of individuals,

00:12:22.590 --> 00:12:26.910
because my skills were more
in helping to organize people

00:12:26.910 --> 00:12:29.790
so that we could do something
exciting together rather than

00:12:29.790 --> 00:12:33.650
believing that my ideas are just
better than everyone else's.

00:12:33.650 --> 00:12:35.740
JULIE HANNA: Thank
you for sharing that.

00:12:35.740 --> 00:12:38.600
I think that there's
a subtext in what

00:12:38.600 --> 00:12:42.230
you're kind of saying here
and in the conversations

00:12:42.230 --> 00:12:47.120
we've had over the
years, that part

00:12:47.120 --> 00:12:49.430
of sustainable or
enduring innovation

00:12:49.430 --> 00:12:51.860
is that it holds
itself accountable,

00:12:51.860 --> 00:12:55.280
not just its shareholders,
but to all stakeholders,

00:12:55.280 --> 00:13:00.500
to employees, to customers, to
the communities in which you

00:13:00.500 --> 00:13:03.715
operate, and to
society at large.

00:13:03.715 --> 00:13:05.090
And I think this
ultimately comes

00:13:05.090 --> 00:13:09.830
down to what the positive
and negative impacts are

00:13:09.830 --> 00:13:12.770
on each stakeholder
being thoughtful,

00:13:12.770 --> 00:13:14.900
proactive, and
reactive about that.

00:13:14.900 --> 00:13:21.410
And I do believe that enduring
innovation and ethical practice

00:13:21.410 --> 00:13:24.560
involves that kind
of courage to be

00:13:24.560 --> 00:13:26.870
hostile to our own mindsets,
the kind of courage

00:13:26.870 --> 00:13:35.030
that I think you've sort of
marshalled in response to some

00:13:35.030 --> 00:13:37.280
of the things you're just
talking about and witnessing

00:13:37.280 --> 00:13:38.860
your family members.

00:13:38.860 --> 00:13:43.610
Catching ourselves in the
act of making assumptions,

00:13:43.610 --> 00:13:46.790
becoming aware of our
blind spots, a willingness

00:13:46.790 --> 00:13:48.650
not just to ask the
question we love

00:13:48.650 --> 00:13:51.680
to ask in Silicon Valley, which
is just how big can this get?

00:13:51.680 --> 00:13:53.930
What's your world
domination strategy?

00:13:53.930 --> 00:13:58.630
But also kind of how
could this go very wrong?

00:13:58.630 --> 00:14:00.680
And what would we do about it?

00:14:00.680 --> 00:14:03.110
What do we believe is our
role and responsibility

00:14:03.110 --> 00:14:06.380
when things don't go as
we had imagined them to?

00:14:06.380 --> 00:14:10.640
I think it requires both radical
honesty and radical empathy,

00:14:10.640 --> 00:14:13.190
because at its essence,
this question comes down

00:14:13.190 --> 00:14:17.090
to human suffering--

00:14:17.090 --> 00:14:20.090
the willingness to say, are
the products and businesses

00:14:20.090 --> 00:14:23.210
that we're creating
contributing to human suffering

00:14:23.210 --> 00:14:25.660
in the short, medium,
and long term?

00:14:25.660 --> 00:14:28.730
I mean, it's really
fascinating because we

00:14:28.730 --> 00:14:32.240
talk a lot about surprising
and delighting users.

00:14:32.240 --> 00:14:35.462
That's become a widespread
design goal in the products

00:14:35.462 --> 00:14:36.420
and apps that we build.

00:14:36.420 --> 00:14:39.920
But why wouldn't we make
alleviating human suffering

00:14:39.920 --> 00:14:42.147
a goal too?

00:14:42.147 --> 00:14:44.480
ASTRO TELLER: If I may, I
agree with what you're saying,

00:14:44.480 --> 00:14:47.150
but it's further
complicated by the fact

00:14:47.150 --> 00:14:51.260
that most technologies
produce diffuse benefits

00:14:51.260 --> 00:14:53.450
and concentrated harm.

00:14:53.450 --> 00:14:56.240
And the harm tends
to be front-loaded,

00:14:56.240 --> 00:14:59.780
and the benefits of technology
tend to be back and weighted.

00:14:59.780 --> 00:15:01.700
So in the early
days of technology,

00:15:01.700 --> 00:15:04.280
the thing that is most
evident is that harm.

00:15:06.860 --> 00:15:09.470
It's like everyone in the
world is going to get $1,

00:15:09.470 --> 00:15:12.950
and these 20 people are
going to lose their job.

00:15:12.950 --> 00:15:15.440
Society has netted
to the positive,

00:15:15.440 --> 00:15:18.680
but some real compassion is
owed to those 20 people who

00:15:18.680 --> 00:15:20.270
are going to lose their job.

00:15:20.270 --> 00:15:23.210
And no one is going to
picket about how happy they

00:15:23.210 --> 00:15:25.610
are for getting $1,
but those 20 people

00:15:25.610 --> 00:15:28.350
will picket about the fact
that they're losing their job.

00:15:28.350 --> 00:15:31.190
And if we as society
don't empathize with them,

00:15:31.190 --> 00:15:32.900
understand where
they're coming from,

00:15:32.900 --> 00:15:36.080
and find some way to support
them through this process,

00:15:36.080 --> 00:15:39.800
they're rational to
picket about this process.

00:15:39.800 --> 00:15:41.480
But then they're
accidentally impeding

00:15:41.480 --> 00:15:46.130
this huge net positive society
overall stands to gain.

00:15:46.130 --> 00:15:51.530
So I think that we have some
more sophistication and nuance

00:15:51.530 --> 00:15:54.440
that we have to get
to as a society about

00:15:54.440 --> 00:15:56.990
appreciating where
the benefits go,

00:15:56.990 --> 00:16:00.740
where's the harm lands, looking
at these long time horizons,

00:16:00.740 --> 00:16:06.200
because it's not the same answer
on week one, or on year one,

00:16:06.200 --> 00:16:07.696
or year 50.

00:16:07.696 --> 00:16:08.450
JULIE HANNA: No.

00:16:08.450 --> 00:16:10.783
ASTRO TELLER: And being able
to tease these things apart

00:16:10.783 --> 00:16:13.550
is critical for us to do
thoughtful assessments

00:16:13.550 --> 00:16:15.380
of whether a
technology is worth it

00:16:15.380 --> 00:16:16.970
and ultimately
good for the world,

00:16:16.970 --> 00:16:19.940
and also how to take care of
the people who are losing out

00:16:19.940 --> 00:16:22.580
in this process,
so that no one is

00:16:22.580 --> 00:16:26.660
enough of a loser in
the trade that society

00:16:26.660 --> 00:16:29.400
is making, that they're
willing to fight the process.

00:16:29.400 --> 00:16:30.992
They feel lifted up by it.

00:16:30.992 --> 00:16:31.700
JULIE HANNA: Yep.

00:16:31.700 --> 00:16:32.950
Thank you for that refinement.

00:16:32.950 --> 00:16:35.780
That frame of sort of
concentrated benefit diffuse

00:16:35.780 --> 00:16:37.620
harm I think is
really excellent.

00:16:37.620 --> 00:16:40.460
I would add to that one of
the more latent discoveries

00:16:40.460 --> 00:16:45.290
we have is sort of diffuse
harm that's very subtle.

00:16:45.290 --> 00:16:47.540
For example, waking up
and discovering that we're

00:16:47.540 --> 00:16:50.190
addicted to our phones.

00:16:50.190 --> 00:16:51.770
That actually has
long roots going

00:16:51.770 --> 00:16:55.790
back to when
BlackBerry became was

00:16:55.790 --> 00:16:57.860
sort of the popular
device of the moment.

00:16:57.860 --> 00:17:01.522
I remember having a board member
who came to me meeting one day

00:17:01.522 --> 00:17:04.910
and declared, I've thrown
away my BlackBerry.

00:17:04.910 --> 00:17:05.730
Why?

00:17:05.730 --> 00:17:08.900
He said, I was
putting my son to bed.

00:17:08.900 --> 00:17:11.000
And I had my arm around
him reading to him.

00:17:11.000 --> 00:17:15.950
And I caught myself with
my BlackBerry reading it

00:17:15.950 --> 00:17:17.810
while I was in bed with my son.

00:17:17.810 --> 00:17:20.450
And I remember
thinking it's either

00:17:20.450 --> 00:17:22.040
my son or the BlackBerry.

00:17:22.040 --> 00:17:25.010
And yet that's
been 15 years ago.

00:17:25.010 --> 00:17:27.500
And we've been on this
kind of slow, steady march.

00:17:27.500 --> 00:17:31.030
So that diffuse
concentrated benefit harm,

00:17:31.030 --> 00:17:36.500
diffuse you benefit harm is
actually really useful frame.

00:17:36.500 --> 00:17:38.930
Back to actually kind
of some of the ways

00:17:38.930 --> 00:17:42.290
that you've instrumented
X to address

00:17:42.290 --> 00:17:45.297
some of these challenges.

00:17:45.297 --> 00:17:46.880
One of the most
striking things that I

00:17:46.880 --> 00:17:49.130
find about the culture
at X is how hard you all

00:17:49.130 --> 00:17:53.000
work to proactively
understand precisely

00:17:53.000 --> 00:17:54.590
these kinds of impacts.

00:17:54.590 --> 00:17:58.340
You talk about creating
tight feedback loops

00:17:58.340 --> 00:18:00.530
to look at choices,
and consequences,

00:18:00.530 --> 00:18:03.530
and unanticipated
consequences in the real world

00:18:03.530 --> 00:18:04.810
as early as possible.

00:18:04.810 --> 00:18:06.268
And that's kind of
hard when you're

00:18:06.268 --> 00:18:08.870
doing Moonshot kind of stuff.

00:18:08.870 --> 00:18:10.460
You call it sandboxing.

00:18:10.460 --> 00:18:12.470
So how do you
sandboxing at X. Tell us

00:18:12.470 --> 00:18:13.508
a little bit about that.

00:18:13.508 --> 00:18:14.300
ASTRO TELLER: Sure.

00:18:14.300 --> 00:18:17.540
Let me take a step back, so that
it'll seem more motivated why

00:18:17.540 --> 00:18:19.160
we do this.

00:18:19.160 --> 00:18:22.580
So one of the things I just
mentioned a few minutes ago,

00:18:22.580 --> 00:18:26.300
I just don't happen to
believe that any of our ideas

00:18:26.300 --> 00:18:28.440
are particularly
better than random.

00:18:28.440 --> 00:18:29.978
It's not an attack
on any one of us.

00:18:29.978 --> 00:18:32.270
I just don't think my ideas
are much better than random

00:18:32.270 --> 00:18:34.610
or your ideas or your ideas.

00:18:34.610 --> 00:18:36.950
What I think that we
as a group could do--

00:18:36.950 --> 00:18:39.800
what any group could do
if they practiced at it--

00:18:39.800 --> 00:18:43.280
would be to get somewhat
more efficient at weeding

00:18:43.280 --> 00:18:48.470
through ideas and discovering
of this big mountain of ideas

00:18:48.470 --> 00:18:50.010
that we can put in front of us.

00:18:50.010 --> 00:18:51.500
If you get a bunch
of smart people,

00:18:51.500 --> 00:18:54.320
and you don't make them feel
stupid for putting ideas

00:18:54.320 --> 00:18:56.960
on the table, most of
us are pretty creative.

00:18:56.960 --> 00:18:58.100
We were when we were six.

00:18:58.100 --> 00:19:00.260
You've just forgotten
how, most of us.

00:19:00.260 --> 00:19:02.780
But we can find it
again in ourselves.

00:19:02.780 --> 00:19:06.830
Let's get this huge pile of
creative ideas on the table.

00:19:06.830 --> 00:19:08.990
And then let's pick
them back off the table

00:19:08.990 --> 00:19:13.100
and say, how fast, how
thoughtfully, how cheaply

00:19:13.100 --> 00:19:16.340
could we discard this
for the right reasons?

00:19:16.340 --> 00:19:18.050
Because almost
everything that's on that

00:19:18.050 --> 00:19:20.270
table is not going to make it.

00:19:20.270 --> 00:19:23.420
So you have to
start by saying, why

00:19:23.420 --> 00:19:28.740
don't people do that weeding
process really efficiently?

00:19:28.740 --> 00:19:32.050
And there are a set of
reasons why they don't.

00:19:32.050 --> 00:19:34.590
We can get back to why
they don't, but if you just

00:19:34.590 --> 00:19:38.220
buy into the idea that the
real process of innovation

00:19:38.220 --> 00:19:40.990
isn't even really filling
out the table with ideas--

00:19:40.990 --> 00:19:42.610
that's actually
relatively easy to do.

00:19:42.610 --> 00:19:45.990
It's the efficiency with which
you're discarding the ideas--

00:19:45.990 --> 00:19:50.880
then you want to try stuff
because that's how you learn.

00:19:50.880 --> 00:19:54.030
But especially if you're serious
about lasting innovation,

00:19:54.030 --> 00:19:56.130
you don't just want to
throw it out in the world

00:19:56.130 --> 00:19:57.810
and let the world
essentially pay

00:19:57.810 --> 00:20:00.510
the consequences for the fact
that you didn't think first.

00:20:00.510 --> 00:20:02.640
That doesn't work very well.

00:20:02.640 --> 00:20:05.640
There needs to be
something in between where

00:20:05.640 --> 00:20:09.780
you expose your idea to
reality step by step.

00:20:09.780 --> 00:20:12.730
If you can prove that it
breaks the Law of Physics,

00:20:12.730 --> 00:20:15.500
on the whiteboard, you don't
need to build a prototype

00:20:15.500 --> 00:20:16.500
and put it in the world.

00:20:16.500 --> 00:20:18.053
You just like satisfy yourself.

00:20:18.053 --> 00:20:20.220
The Second Law of Thermodynamics
is probably smarter

00:20:20.220 --> 00:20:21.000
than you are.

00:20:21.000 --> 00:20:23.010
And you throw out the
idea, and you move on.

00:20:23.010 --> 00:20:27.030
If it doesn't sort of die on
one of those straightforward

00:20:27.030 --> 00:20:30.930
kind of white paper fronts, then
maybe you do build a prototype.

00:20:30.930 --> 00:20:32.560
But you could test
it on your roof.

00:20:32.560 --> 00:20:34.810
You don't actually have to
go and tested in the middle

00:20:34.810 --> 00:20:36.510
of some public square first.

00:20:36.510 --> 00:20:42.750
Let it suffer from physics,
or show it to one customer

00:20:42.750 --> 00:20:44.460
and say, what do you think?

00:20:44.460 --> 00:20:46.500
The first customer's
probably going to tell you

00:20:46.500 --> 00:20:47.310
it's not right.

00:20:47.310 --> 00:20:49.620
You don't need to give
it to 100,000 to find out

00:20:49.620 --> 00:20:51.750
that it's not quite right yet.

00:20:51.750 --> 00:20:54.330
And each of these
steps along the way,

00:20:54.330 --> 00:20:57.090
you want to get more in
touch with the real world.

00:20:57.090 --> 00:21:00.000
But to use the
self-driving car metaphor,

00:21:00.000 --> 00:21:02.440
you want to keep your
hands close to the wheel,

00:21:02.440 --> 00:21:05.220
so you can get the learning
without actually endangering

00:21:05.220 --> 00:21:06.580
the people around you.

00:21:06.580 --> 00:21:08.670
That's what I mean
by sandboxing.

00:21:08.670 --> 00:21:10.860
And that's both learning
about the product

00:21:10.860 --> 00:21:13.030
in terms of its
financial possibilities,

00:21:13.030 --> 00:21:15.540
but also in terms of the
purpose that you hope it has

00:21:15.540 --> 00:21:16.410
in the world.

00:21:16.410 --> 00:21:18.960
Because you may learn
along the way oh

00:21:18.960 --> 00:21:20.880
this isn't solving
the problem I thought

00:21:20.880 --> 00:21:22.500
even if people will pay for it.

00:21:22.500 --> 00:21:23.950
Or it's solving the problem.

00:21:23.950 --> 00:21:26.533
I thought it was going to solve,
but it has these really nasty

00:21:26.533 --> 00:21:27.940
unintended consequences.

00:21:27.940 --> 00:21:29.970
I guess we need to change
our product if we're

00:21:29.970 --> 00:21:34.830
serious about actually having
this ultimately be held up

00:21:34.830 --> 00:21:36.780
by society as a good thing.

00:21:36.780 --> 00:21:38.430
So that by the
time we're serious

00:21:38.430 --> 00:21:40.170
and it's out there
in the world, it's

00:21:40.170 --> 00:21:43.950
had most of its sort of
failure modes weeded out of it.

00:21:43.950 --> 00:21:44.700
JULIE HANNA: Yeah.

00:21:44.700 --> 00:21:45.680
It's fascinating.

00:21:45.680 --> 00:21:48.440
It's like a Back to
the Future moment.

00:21:48.440 --> 00:21:51.330
Back in the day when I first
began building products

00:21:51.330 --> 00:21:55.530
in the prehistoric
pre-internet days,

00:21:55.530 --> 00:21:57.557
we used to do limited betas.

00:21:57.557 --> 00:21:59.140
And then when the
internet came along,

00:21:59.140 --> 00:22:00.807
we said, hey, the
world can be our beta.

00:22:00.807 --> 00:22:02.580
I Remember when
Gmail first released.

00:22:02.580 --> 00:22:05.767
It was like it was in
beta for five years.

00:22:05.767 --> 00:22:07.350
And now, what we're
realizing is we're

00:22:07.350 --> 00:22:10.900
running a social experiment
on two billion people, right?

00:22:10.900 --> 00:22:14.520
And we're realizing
how that can unleash

00:22:14.520 --> 00:22:17.130
a devastating social impact.

00:22:17.130 --> 00:22:18.050
That's really hard.

00:22:18.050 --> 00:22:20.070
And I think some
of us are wondering

00:22:20.070 --> 00:22:21.840
if it's even possible
to recover from it.

00:22:26.040 --> 00:22:27.960
You talk about sandboxing,
but in contrast,

00:22:27.960 --> 00:22:29.970
you also say that
Silicon Valley is not

00:22:29.970 --> 00:22:33.700
the place to learn about the
impact of your products--

00:22:33.700 --> 00:22:34.980
especially at scale.

00:22:34.980 --> 00:22:38.025
That it's important to get
out of the bubble and really

00:22:38.025 --> 00:22:39.150
understand our full impact.

00:22:39.150 --> 00:22:41.550
Can you talk about
some examples of how

00:22:41.550 --> 00:22:44.220
X has done that and some
of what you've learned?

00:22:44.220 --> 00:22:45.780
ASTRO TELLER: Sure.

00:22:45.780 --> 00:22:50.880
So for many of our projects, we
do some testing in California

00:22:50.880 --> 00:22:52.230
and the United States.

00:22:52.230 --> 00:22:56.887
But we have more balloons
flying for our Loon project,

00:22:56.887 --> 00:22:59.220
more balloons over Africa
right now than over the United

00:22:59.220 --> 00:23:01.500
States, more balloons
over South America

00:23:01.500 --> 00:23:03.262
right now than over
the United States.

00:23:03.262 --> 00:23:05.220
And that's partly because
there's a lot that we

00:23:05.220 --> 00:23:07.540
can learn by talking to people.

00:23:07.540 --> 00:23:10.500
If our goal is to
connect the unconnected,

00:23:10.500 --> 00:23:13.992
there are a lot more unconnected
people in those places.

00:23:13.992 --> 00:23:15.450
And there are things
I didn't know.

00:23:15.450 --> 00:23:19.230
I think most of the people
on the Loon team didn't know.

00:23:19.230 --> 00:23:21.810
Like for example,
it turns out there

00:23:21.810 --> 00:23:26.700
are people who have connectivity
according to the statistics.

00:23:26.700 --> 00:23:29.310
But it turns out there are
people across the world--

00:23:29.310 --> 00:23:32.910
particularly in South
America, this is a thing.

00:23:32.910 --> 00:23:36.630
They build tree houses
with ladders up to them.

00:23:36.630 --> 00:23:40.560
And then entrepreneurs
rent this platform out

00:23:40.560 --> 00:23:43.020
by the minute to other
people who go up there,

00:23:43.020 --> 00:23:44.940
because it's a
place from which you

00:23:44.940 --> 00:23:49.290
can get a cell signal, which is
unbelievable entrepreneurship.

00:23:49.290 --> 00:23:50.740
I absolutely love it.

00:23:50.740 --> 00:23:54.480
But to understand that counts
as having connectivity.

00:23:54.480 --> 00:23:56.910
Not the person who owns the
platform but every person who

00:23:56.910 --> 00:24:01.440
is paying to leave their
town, go to the special tree,

00:24:01.440 --> 00:24:05.800
climb up the ladder, and
get their connectivity.

00:24:05.800 --> 00:24:08.040
I didn't know this
was a service.

00:24:08.040 --> 00:24:10.620
It turns out there are
trucks and motorbikes that

00:24:10.620 --> 00:24:13.000
come through the fields.

00:24:13.000 --> 00:24:14.490
It's probably in Africa too.

00:24:14.490 --> 00:24:18.250
I happen to know this is
prevalent in South America.

00:24:18.250 --> 00:24:20.070
And they collect
everyone's cell phone.

00:24:20.070 --> 00:24:21.720
And then they go
to the city center,

00:24:21.720 --> 00:24:23.408
where there's a Wi-Fi hotspot.

00:24:23.408 --> 00:24:25.200
And they just sit there
for a little while.

00:24:25.200 --> 00:24:27.320
And then all the text
messages download.

00:24:27.320 --> 00:24:30.300
All the sent messages upload.

00:24:30.300 --> 00:24:32.070
And then they drive
back out to the field.

00:24:32.070 --> 00:24:33.362
And they distribute the phones.

00:24:33.362 --> 00:24:36.390
This is a service that
people get paid for,

00:24:36.390 --> 00:24:38.520
and these people
have connectivity.

00:24:38.520 --> 00:24:41.042
But if you don't
actually go to the places

00:24:41.042 --> 00:24:42.750
where the people are
that you're helping,

00:24:42.750 --> 00:24:45.930
you don't really understand
their current solution set

00:24:45.930 --> 00:24:49.560
or what they would count as
enough better in the right ways

00:24:49.560 --> 00:24:51.630
that it would make a
difference to them.

00:24:51.630 --> 00:24:52.890
It's not obvious.

00:24:52.890 --> 00:24:55.950
If balloons are just sometimes
there and sometimes not,

00:24:55.950 --> 00:24:57.540
would that change
their lives or not?

00:24:57.540 --> 00:24:58.470
You shouldn't make that up.

00:24:58.470 --> 00:24:59.887
You should go talk
to these people

00:24:59.887 --> 00:25:02.850
and probably eventually
try it with them.

00:25:02.850 --> 00:25:06.060
And we do the same thing.

00:25:06.060 --> 00:25:08.610
Our self-flying vehicles
for package delivery,

00:25:08.610 --> 00:25:15.180
Wing, is ramping up service
in Finland and in Australia,

00:25:15.180 --> 00:25:18.755
a little bit here in
the United States.

00:25:18.755 --> 00:25:20.880
One of the things that we
discovered in Australia--

00:25:20.880 --> 00:25:21.520
I don't know.

00:25:21.520 --> 00:25:23.730
Probably not particular
to Australia--

00:25:23.730 --> 00:25:26.790
is we were confident
that the sound

00:25:26.790 --> 00:25:29.110
of it going up or
coming down was going

00:25:29.110 --> 00:25:30.740
to be what annoyed people.

00:25:30.740 --> 00:25:34.230
And then when it went over,
that wouldn't annoy people.

00:25:34.230 --> 00:25:38.230
It turns out that people,
for whatever reason,

00:25:38.230 --> 00:25:40.350
even though it's
technically quieter,

00:25:40.350 --> 00:25:44.480
notice the buzz when
it goes by more.

00:25:44.480 --> 00:25:45.540
OK.

00:25:45.540 --> 00:25:47.580
That can surprise
us, but we can't

00:25:47.580 --> 00:25:50.578
like argue to them with
a sound meter, no no, no.

00:25:50.578 --> 00:25:51.120
You're wrong.

00:25:51.120 --> 00:25:51.912
You're not annoyed.

00:25:51.912 --> 00:25:53.040
Look at the number here.

00:25:53.040 --> 00:25:54.390
You can't be annoyed.

00:25:54.390 --> 00:25:55.800
That's not how the world works.

00:25:55.800 --> 00:25:58.680
The way the world works is
we have to say, oh, thank you

00:25:58.680 --> 00:25:59.400
for the feedback.

00:25:59.400 --> 00:26:02.610
And then we make our
self-flying vehicles quieter

00:26:02.610 --> 00:26:04.800
because that's what
we owe society,

00:26:04.800 --> 00:26:07.695
so that they will be excited and
accept the delivery vehicles.

00:26:10.283 --> 00:26:11.950
JULIE HANNA: This is
near and dear to me

00:26:11.950 --> 00:26:17.290
because as an immigrant and as
a refugee, my work with Kiva

00:26:17.290 --> 00:26:19.630
takes me to parts
of the world where

00:26:19.630 --> 00:26:23.920
people live on very little,
as little as $1 a day.

00:26:23.920 --> 00:26:26.350
I'm always reminded how easy
it is to forget being here

00:26:26.350 --> 00:26:29.110
in Silicon Valley,
the digital divide,

00:26:29.110 --> 00:26:33.940
and how that really shapes our
lives in very different ways.

00:26:33.940 --> 00:26:36.820
And that really solving
humanity's greatest challenges

00:26:36.820 --> 00:26:40.570
and building technology for
the many, not just the few,

00:26:40.570 --> 00:26:44.170
means understanding that
divide fully in the ways

00:26:44.170 --> 00:26:46.340
that you're talking about.

00:26:46.340 --> 00:26:50.290
And at the same time,
being here in the valley,

00:26:50.290 --> 00:26:54.460
here at Google I/O, a place
where the sky is literally

00:26:54.460 --> 00:26:58.450
the limit, it's an
incredibly hopeful feeling.

00:26:58.450 --> 00:27:01.330
We can actually say
and do things here

00:27:01.330 --> 00:27:06.160
that have an impact
around the world.

00:27:06.160 --> 00:27:10.870
Those of us who've made our
way into this literal room,

00:27:10.870 --> 00:27:13.390
I mean, we can say
and do things that

00:27:13.390 --> 00:27:16.030
will impact the lives of
millions and even billions--

00:27:16.030 --> 00:27:18.980
either directly or indirectly.

00:27:18.980 --> 00:27:25.070
So really, it it's just
a reminder of that adage

00:27:25.070 --> 00:27:28.910
as Silicon Valley goes,
so does the world.

00:27:28.910 --> 00:27:34.700
With that, I guess I find great
meaning and purpose in that--

00:27:34.700 --> 00:27:37.910
not just the burden
of responsibility--

00:27:37.910 --> 00:27:40.250
to have that kind of leverage.

00:27:40.250 --> 00:27:42.260
There's a great pride
of identity in that.

00:27:42.260 --> 00:27:47.450
And I think that if we all
embrace that more fully,

00:27:47.450 --> 00:27:50.480
it's really exciting to think
of what can be unleashed.

00:27:50.480 --> 00:27:52.550
To be in that kind
of position, there's

00:27:52.550 --> 00:27:57.090
a great privilege that is
afforded to a very few people.

00:27:57.090 --> 00:28:00.650
And it's all the more
reason why having diversity

00:28:00.650 --> 00:28:02.660
of thought and
experience is just

00:28:02.660 --> 00:28:05.995
so essential in the
process of innovation.

00:28:05.995 --> 00:28:08.120
One of the things I've
drawn great inspiration from

00:28:08.120 --> 00:28:11.480
is the explicit
commitment that X

00:28:11.480 --> 00:28:14.090
has to diversity and inclusion.

00:28:14.090 --> 00:28:18.080
You have a fascinating range
of disciplines and backgrounds,

00:28:18.080 --> 00:28:22.240
and ethnicities and genders,
and experiences at the table,

00:28:22.240 --> 00:28:26.180
and more than any tech culture
I've actually ever encountered.

00:28:26.180 --> 00:28:28.350
It's clearly a
values-driven choice,

00:28:28.350 --> 00:28:32.060
but also you view it
essential to X's ability

00:28:32.060 --> 00:28:33.650
to be successful.

00:28:33.650 --> 00:28:36.980
why do you see diversity
and inclusion so crucial

00:28:36.980 --> 00:28:38.095
to Moonshot success?

00:28:41.793 --> 00:28:43.460
ASTRO TELLER: I'll
give you two answers.

00:28:43.460 --> 00:28:47.210
But before I give
you the two answers,

00:28:47.210 --> 00:28:52.730
let me just flag that working
towards diversity inclusion

00:28:52.730 --> 00:28:54.750
is just the right thing to do.

00:28:54.750 --> 00:28:57.120
And even if that doesn't
tend to change the world--

00:28:57.120 --> 00:28:59.017
and it makes me sort of
sad that it doesn't--

00:28:59.017 --> 00:29:01.100
I just want to flag that
is the right thing to do,

00:29:01.100 --> 00:29:04.010
and that secretly makes
me feel really good.

00:29:04.010 --> 00:29:06.170
[APPLAUSE]

00:29:06.170 --> 00:29:07.700
Thank you.

00:29:07.700 --> 00:29:10.920
Let's put that on the
side for a second.

00:29:10.920 --> 00:29:13.470
The second reason
is, let's say you're

00:29:13.470 --> 00:29:16.190
a creativity factory as much
as you are an engineering

00:29:16.190 --> 00:29:19.580
factory, which is what we are.

00:29:19.580 --> 00:29:21.280
So we want a perspective shift.

00:29:21.280 --> 00:29:24.130
We literally want to
see things differently

00:29:24.130 --> 00:29:26.440
from how other people see them.

00:29:26.440 --> 00:29:30.740
If we fill up the building
with any particular ethnicity,

00:29:30.740 --> 00:29:33.790
but let's say for
argument's sake white men,

00:29:33.790 --> 00:29:36.430
we're going to have
fewer perspectives.

00:29:36.430 --> 00:29:39.640
We're just going to see things
less differently than if we

00:29:39.640 --> 00:29:43.780
have people who are
different ages, who

00:29:43.780 --> 00:29:45.850
come from different
countries, who have

00:29:45.850 --> 00:29:48.210
different ethnic backgrounds.

00:29:48.210 --> 00:29:51.940
People who've been in the
military have an experience

00:29:51.940 --> 00:29:54.550
that people haven't been
in the military don't have.

00:29:54.550 --> 00:29:58.360
We want every kind of that
sort of cognitive diversity

00:29:58.360 --> 00:29:59.410
that we can get.

00:29:59.410 --> 00:30:01.060
I just see this as an arbitrage.

00:30:01.060 --> 00:30:03.310
If the world is
biased and bigoted,

00:30:03.310 --> 00:30:05.440
and we're somewhat less
by ended and bigoted

00:30:05.440 --> 00:30:07.090
than the rest of the
world, then we're

00:30:07.090 --> 00:30:09.730
just going to blap
them, because we're

00:30:09.730 --> 00:30:12.920
getting the benefit that they
just aren't realizing is there.

00:30:12.920 --> 00:30:15.070
So that's one of the
practical reasons,

00:30:15.070 --> 00:30:17.080
leaving the actual
social justice

00:30:17.080 --> 00:30:18.910
thing on the side for a second.

00:30:18.910 --> 00:30:22.660
Separately, in terms
of lasting innovation,

00:30:22.660 --> 00:30:23.740
I was making this joke.

00:30:23.740 --> 00:30:25.490
You guys are going to
work on the product,

00:30:25.490 --> 00:30:27.970
and then you're going to
get them to be good people.

00:30:27.970 --> 00:30:33.250
Part of the process of doing
moral math on whether what

00:30:33.250 --> 00:30:35.650
you're doing is
right is you need

00:30:35.650 --> 00:30:37.510
to get more voices in the room.

00:30:37.510 --> 00:30:40.840
So either we're
going to have a bunch

00:30:40.840 --> 00:30:43.945
of limited perspectives
in our building,

00:30:43.945 --> 00:30:45.070
and then we'll build stuff.

00:30:45.070 --> 00:30:48.040
And then we're going to have
to very after the fact, very

00:30:48.040 --> 00:30:51.790
laboriously, go get those
other voices to tell us

00:30:51.790 --> 00:30:53.050
what we were missing.

00:30:53.050 --> 00:30:57.190
Or crazy idea-- since it turns
out it's good business anyway--

00:30:57.190 --> 00:31:00.640
if we had those diverse
voices in our building

00:31:00.640 --> 00:31:02.470
from the beginning,
we don't need

00:31:02.470 --> 00:31:07.180
to outsource groups to tell us
what we're missing nearly as

00:31:07.180 --> 00:31:11.230
much, because they're literally
part of the process of making

00:31:11.230 --> 00:31:12.170
these things.

00:31:12.170 --> 00:31:15.430
And so we catch a lot
of the sort of oddities

00:31:15.430 --> 00:31:17.640
of how we think about
things and the points

00:31:17.640 --> 00:31:19.390
that we're missing
that way the real world

00:31:19.390 --> 00:31:23.110
works much faster and
much more naturally

00:31:23.110 --> 00:31:28.240
than if we had to outsource
discovery of our blind spots

00:31:28.240 --> 00:31:31.250
after the fact.

00:31:31.250 --> 00:31:32.740
JULIE HANNA: Yeah.

00:31:32.740 --> 00:31:34.820
What it brings to mind is this.

00:31:34.820 --> 00:31:36.530
We've had this trope
for a long time

00:31:36.530 --> 00:31:39.590
that technology
is neutral, which

00:31:39.590 --> 00:31:42.922
I think has kind of been a
blind spot for us frankly.

00:31:42.922 --> 00:31:45.380
Tech may be neutral, but the
products that we build with it

00:31:45.380 --> 00:31:47.110
or not.

00:31:47.110 --> 00:31:49.580
They very much shape our
intentions and our biases

00:31:49.580 --> 00:31:50.382
as you point out.

00:31:50.382 --> 00:31:52.590
And we see that playing it
out in all kinds of ways--

00:31:52.590 --> 00:31:56.120
whether it's facial recognition
software that doesn't recognize

00:31:56.120 --> 00:31:58.520
dark-skinned people,
or the fact that it

00:31:58.520 --> 00:32:02.810
took 30 years to get the
car companies to test

00:32:02.810 --> 00:32:04.640
with female crash test dummies.

00:32:04.640 --> 00:32:08.000
And women were being injured and
killed far more often than men

00:32:08.000 --> 00:32:08.510
in cars.

00:32:11.120 --> 00:32:13.910
When you think about the way
that this actually played out,

00:32:13.910 --> 00:32:15.890
what are some ways that
diversity is actually

00:32:15.890 --> 00:32:18.080
shaped the course of
a Moonshot project?

00:32:21.048 --> 00:32:23.590
ASTRO TELLER: I'm just going to
give you the first thing that

00:32:23.590 --> 00:32:25.690
came to my mind.

00:32:25.690 --> 00:32:27.760
This happens to be
more practical one

00:32:27.760 --> 00:32:30.430
than an ethical one.

00:32:30.430 --> 00:32:33.970
So when Loon is
building these balloons,

00:32:33.970 --> 00:32:35.830
they're not as big as
this room, but they're

00:32:35.830 --> 00:32:37.480
about the size of a house.

00:32:37.480 --> 00:32:39.460
But they're thinner
than a trash bag,

00:32:39.460 --> 00:32:43.000
and they have to cost
less than the cost

00:32:43.000 --> 00:32:46.302
of a trash bag per square
meter, because we're

00:32:46.302 --> 00:32:47.510
going to put these things up.

00:32:47.510 --> 00:32:49.660
They have to stay up for
many hundreds of days

00:32:49.660 --> 00:32:52.310
in order for this to work
and be an actual replacement

00:32:52.310 --> 00:32:54.380
to satellites.

00:32:54.380 --> 00:32:55.450
How do you do that?

00:32:55.450 --> 00:32:57.950
There's literally no one in
the world knows how to do that.

00:32:57.950 --> 00:33:00.550
And one of the people
who's been most profoundly

00:33:00.550 --> 00:33:02.560
influential in the design--

00:33:02.560 --> 00:33:04.840
And these balloons are
very complicated because we

00:33:04.840 --> 00:33:06.070
stick helium in them.

00:33:06.070 --> 00:33:08.050
But we actually are
also sticking air

00:33:08.050 --> 00:33:11.140
in a separate bag
inside the balloon--

00:33:11.140 --> 00:33:12.580
call it a ballonet--

00:33:12.580 --> 00:33:14.360
to make it heavier when we want.

00:33:14.360 --> 00:33:15.910
And then we get the
air back out when

00:33:15.910 --> 00:33:17.440
wanted make it light again.

00:33:17.440 --> 00:33:19.780
So these have multiple
different compartments.

00:33:19.780 --> 00:33:23.200
They have to undergo huge
temperature ranges between day

00:33:23.200 --> 00:33:25.460
and night of about 150 degrees.

00:33:25.460 --> 00:33:28.690
So when they're the hottest,
they were super, super tight.

00:33:28.690 --> 00:33:31.430
So there's an
architectural aspect to it,

00:33:31.430 --> 00:33:34.390
but there's also because
they're like super thin fabric.

00:33:34.390 --> 00:33:37.300
And we have to make them
when they're deflated.

00:33:37.300 --> 00:33:40.810
It turns out a
fashion designer--

00:33:40.810 --> 00:33:44.350
she has been with the
project maybe six years now--

00:33:44.350 --> 00:33:47.220
has been one of the most
profoundly influential

00:33:47.220 --> 00:33:51.640
in the evolution of what this
thing is, of how we make it,

00:33:51.640 --> 00:33:54.070
of what we make
it out of, of how

00:33:54.070 --> 00:33:57.430
we study it before we launch
it, so that we can find flaws

00:33:57.430 --> 00:33:57.970
in it.

00:33:57.970 --> 00:34:02.230
And it actually now has sort of
elements and aspects of design

00:34:02.230 --> 00:34:05.770
from the fabric trade,
which I seriously

00:34:05.770 --> 00:34:08.949
doubt that we otherwise
would have gotten to.

00:34:08.949 --> 00:34:12.010
And there are just hundreds
of those kinds of examples

00:34:12.010 --> 00:34:15.429
that naturally happen for
free when you get people

00:34:15.429 --> 00:34:17.450
with various
experiences in the room.

00:34:17.450 --> 00:34:18.670
JULIE HANNA: That's great.

00:34:18.670 --> 00:34:24.699
So the team at X sometimes
calls you a cultural engineer.

00:34:24.699 --> 00:34:30.610
And to my eye, you do these
really interesting things

00:34:30.610 --> 00:34:33.270
that I call them teaching
principles, sort of hack

00:34:33.270 --> 00:34:36.219
the organizational mindset.

00:34:36.219 --> 00:34:38.830
A lot of it's
around getting them

00:34:38.830 --> 00:34:43.850
to hold these tensions of
what are seeming opposite,

00:34:43.850 --> 00:34:47.650
like being passionately
dispassionate, responsibly

00:34:47.650 --> 00:34:55.659
irresponsible, and perhaps most
of all, audacity with humility.

00:34:55.659 --> 00:34:57.490
Talk a little bit
about these principles

00:34:57.490 --> 00:35:01.720
and how you see them as
being critical to the process

00:35:01.720 --> 00:35:03.110
of innovation.

00:35:03.110 --> 00:35:05.950
ASTRO TELLER: I think that
it's sort of a natural state

00:35:05.950 --> 00:35:07.730
for most human beings.

00:35:07.730 --> 00:35:10.450
We want simplicity
and we want clarity.

00:35:10.450 --> 00:35:15.430
And so we pick on any particular
spectrum, one of the polls.

00:35:15.430 --> 00:35:17.860
Because we pick a poll,
you can't get any farther.

00:35:17.860 --> 00:35:20.590
It's easy to
describe, and it tends

00:35:20.590 --> 00:35:25.340
to be the case that is not the
optimal point on that spectrum.

00:35:25.340 --> 00:35:28.360
So the last one that you
just described for example,

00:35:28.360 --> 00:35:32.290
I deeply believe that
for ethical reasons

00:35:32.290 --> 00:35:34.390
and also for
practical reasons, we

00:35:34.390 --> 00:35:37.480
should have huge
amounts of audacity at X

00:35:37.480 --> 00:35:40.510
and huge amounts of humility.

00:35:40.510 --> 00:35:43.930
If you only have humility, we'd
probably never try anything,

00:35:43.930 --> 00:35:45.640
so we'd never
accomplish anything.

00:35:45.640 --> 00:35:47.770
If you only have
audacity, you're

00:35:47.770 --> 00:35:51.670
going to make a large mess,
and you're not going to care.

00:35:51.670 --> 00:35:54.580
If you have a lot of audacity
and a lot of humility

00:35:54.580 --> 00:35:57.790
though, while that seems
like it's a tension

00:35:57.790 --> 00:36:00.670
to hold, hopefully
if we do it right,

00:36:00.670 --> 00:36:02.440
it's going to feel like this.

00:36:02.440 --> 00:36:05.440
I believe we can try anything.

00:36:05.440 --> 00:36:09.430
Let's try this and despite
how crazy it sounds.

00:36:09.430 --> 00:36:13.090
Then as soon as we try it,
let's have the humility,

00:36:13.090 --> 00:36:15.130
because almost all of
the time the answer

00:36:15.130 --> 00:36:17.660
is that didn't work very well.

00:36:17.660 --> 00:36:21.220
We have to be able
super easily to say

00:36:21.220 --> 00:36:22.760
that didn't work very well.

00:36:22.760 --> 00:36:23.960
What did we learn?

00:36:23.960 --> 00:36:25.940
What could we do
better next time?

00:36:25.940 --> 00:36:28.660
And it's a positive
feedback cycle

00:36:28.660 --> 00:36:30.850
that allows us both to
try things and learn

00:36:30.850 --> 00:36:33.520
from our mistakes
rather than trying

00:36:33.520 --> 00:36:35.350
to pick one of those polls.

00:36:35.350 --> 00:36:37.352
You mentioned responsibly
irresponsible,

00:36:37.352 --> 00:36:38.935
which I think is
another good example.

00:36:41.920 --> 00:36:45.490
I have the bumper
sticker on my laptop,

00:36:45.490 --> 00:36:47.380
and a number of other
people do at X. It

00:36:47.380 --> 00:36:49.750
says Save the Oompa Loompas.

00:36:49.750 --> 00:36:51.910
And this is my way
of reminding people

00:36:51.910 --> 00:36:54.370
about being responsibly
irresponsible.

00:36:54.370 --> 00:36:57.970
And what I mean by that is if
you let the oompa loompas just

00:36:57.970 --> 00:37:00.910
do whatever they wanted,
they just go crazy.

00:37:00.910 --> 00:37:02.140
They'd rip the place apart.

00:37:02.140 --> 00:37:05.330
There would be no
chocolate factory.

00:37:05.330 --> 00:37:10.190
Creativity unharnessed and
undirected is not very useful.

00:37:10.190 --> 00:37:12.940
On the other hand,
extreme amounts

00:37:12.940 --> 00:37:18.100
of rigor and responsibility is
the death of real creativity.

00:37:18.100 --> 00:37:20.290
You need to find this
place in the middle

00:37:20.290 --> 00:37:23.410
where you encourage
people to be unnatural,

00:37:23.410 --> 00:37:27.280
to be weird, to try
things, to be silly,

00:37:27.280 --> 00:37:29.320
and to be wrong
most of the time,

00:37:29.320 --> 00:37:33.400
but to create some shaping and
guardrails of responsibility

00:37:33.400 --> 00:37:35.530
that tends to cause
that creativity

00:37:35.530 --> 00:37:38.128
to head in positive directions.

00:37:38.128 --> 00:37:39.670
That's another one
of those tensions,

00:37:39.670 --> 00:37:41.800
that if you were to
pick either extreme,

00:37:41.800 --> 00:37:44.200
you would get a pale
shadow of the benefit

00:37:44.200 --> 00:37:47.530
that you can get if you hang out
in that uncomfortable tension

00:37:47.530 --> 00:37:49.300
point in the middle.

00:37:49.300 --> 00:37:50.260
JULIE HANNA: Yeah.

00:37:50.260 --> 00:37:52.270
One of the tensions, I
think, we as an industry

00:37:52.270 --> 00:37:54.603
are reckoning with the sort
of the tension between speed

00:37:54.603 --> 00:37:55.900
and ethics.

00:37:55.900 --> 00:37:57.970
The era of move fast
and break things

00:37:57.970 --> 00:38:02.250
has come to a screeching
halt or an abrupt damned.

00:38:02.250 --> 00:38:04.390
You hear talk of
minimum viable product

00:38:04.390 --> 00:38:07.825
now being moving to
minimum virtuous products.

00:38:10.750 --> 00:38:14.940
So this notion of harmonizing
seeming opposites I

00:38:14.940 --> 00:38:19.540
think in many ways,
it helps us break down

00:38:19.540 --> 00:38:21.670
the dualistic extremes
that are much easier

00:38:21.670 --> 00:38:24.310
to gravitate towards,
but ultimately

00:38:24.310 --> 00:38:29.440
yield a much greater impact.

00:38:29.440 --> 00:38:31.027
Speaking of dualistic
mindsets, one

00:38:31.027 --> 00:38:33.610
of the things that we've had a
long running conversation about

00:38:33.610 --> 00:38:35.170
is purpose and profit.

00:38:35.170 --> 00:38:38.650
They tend to be sort of
pitted against each other.

00:38:38.650 --> 00:38:41.050
And we share the view that
technology and business

00:38:41.050 --> 00:38:44.980
are the most powerful tools
for solving humanity's biggest

00:38:44.980 --> 00:38:47.013
challenges.

00:38:47.013 --> 00:38:48.430
As sometimes you
say, the world is

00:38:48.430 --> 00:38:51.610
on fire and tech can put
out some of those fires.

00:38:51.610 --> 00:38:54.100
I would also add that
despite our good intentions

00:38:54.100 --> 00:38:56.980
that we're making the
world a better place, we

00:38:56.980 --> 00:39:00.340
as technologists have started
some of the most profound fires

00:39:00.340 --> 00:39:00.940
right now.

00:39:00.940 --> 00:39:06.340
So it's a good reminder
that intentions are

00:39:06.340 --> 00:39:10.180
necessary, but not sufficient.

00:39:10.180 --> 00:39:15.550
In a time when people feel that
our pursuit of profit has come

00:39:15.550 --> 00:39:20.380
at a great cost to humanity--
much less what we see with

00:39:20.380 --> 00:39:21.970
climate change--

00:39:21.970 --> 00:39:25.340
it's a conversation that we
need to have more than ever.

00:39:25.340 --> 00:39:29.230
How do you think about the
relationship between purpose,

00:39:29.230 --> 00:39:33.240
profit, and lasting innovation?

00:39:33.240 --> 00:39:35.860
ASTRO TELLER: I
don't buy the story

00:39:35.860 --> 00:39:39.470
that they're in fundamental
conflict with each other.

00:39:39.470 --> 00:39:43.700
I believe that things that make
money get bigger over time.

00:39:43.700 --> 00:39:46.760
And things that lose money
tend to get smaller over time.

00:39:46.760 --> 00:39:49.030
So if you have a
magic box that's

00:39:49.030 --> 00:39:52.330
going to cure cancer, or create
clean water for the developing

00:39:52.330 --> 00:39:54.770
world or whatever that
is, and it loses money,

00:39:54.770 --> 00:39:56.180
I have news for you.

00:39:56.180 --> 00:39:57.430
It's not going to help.

00:39:57.430 --> 00:39:59.530
No one's going to
have one of your boxes

00:39:59.530 --> 00:40:03.130
because you can't get those
boxes out in the world enough

00:40:03.130 --> 00:40:06.290
if you're losing
money on every box.

00:40:06.290 --> 00:40:10.120
So profit is, at an
absolute minimum,

00:40:10.120 --> 00:40:14.620
the wind beneath the wings
of the purpose motive.

00:40:14.620 --> 00:40:16.990
On the other hand, who
wants to spend their lives

00:40:16.990 --> 00:40:19.270
making money and then
feeling empty about it

00:40:19.270 --> 00:40:21.880
because you didn't
actually help the world?

00:40:21.880 --> 00:40:24.460
You just moved some money
from somebody else's pocket

00:40:24.460 --> 00:40:26.590
to your pocket.

00:40:26.590 --> 00:40:28.240
couldn't live that way.

00:40:28.240 --> 00:40:30.040
I presume most people couldn't.

00:40:30.040 --> 00:40:30.860
Crazy idea.

00:40:30.860 --> 00:40:34.060
What if we found the
intersection of those two

00:40:34.060 --> 00:40:34.790
things?

00:40:34.790 --> 00:40:37.480
I think there are plenty of
problems in the world where

00:40:37.480 --> 00:40:41.110
if you just focus on
solving, or helping to solve,

00:40:41.110 --> 00:40:43.150
that particular
problem in the world,

00:40:43.150 --> 00:40:45.130
and trust that
the business model

00:40:45.130 --> 00:40:48.550
is going to come into focus
over time, if you were really

00:40:48.550 --> 00:40:49.418
solving--

00:40:49.418 --> 00:40:50.710
This happens at X all the time.

00:40:50.710 --> 00:40:52.553
Someone comes and
says, this is going

00:40:52.553 --> 00:40:54.220
to solve this huge
problem in the world,

00:40:54.220 --> 00:40:55.720
but no one's going
to pay us for it.

00:40:55.720 --> 00:40:57.060
And I say why?

00:40:57.060 --> 00:40:59.167
That doesn't make
any sense to me.

00:40:59.167 --> 00:41:01.000
If you really can solve
a problem the world,

00:41:01.000 --> 00:41:03.750
let's go for it,
because I just believe

00:41:03.750 --> 00:41:05.500
that if we're solving
a problem the world,

00:41:05.500 --> 00:41:07.570
people are going to
pay us fairly for it.

00:41:07.570 --> 00:41:11.500
Now if you have
to give someone $1

00:41:11.500 --> 00:41:13.480
and it's going to cost
us $1, tend to give them

00:41:13.480 --> 00:41:15.340
the $1, that doesn't scale.

00:41:15.340 --> 00:41:16.870
But I think that
there are plenty

00:41:16.870 --> 00:41:18.850
of things at that
intersection where

00:41:18.850 --> 00:41:21.670
you can be driven by
the desire to solve

00:41:21.670 --> 00:41:23.680
an actual problem in the world.

00:41:23.680 --> 00:41:26.260
And you may have to
pivot along the way.

00:41:26.260 --> 00:41:27.490
You might be wrong.

00:41:27.490 --> 00:41:29.770
And you were determined
to do it using

00:41:29.770 --> 00:41:31.930
and lifted up by
the profit motive.

00:41:31.930 --> 00:41:35.020
I think that's the
best case scenario.

00:41:35.020 --> 00:41:37.630
JULIE HANNA: One of my favorite
things about being at X

00:41:37.630 --> 00:41:40.510
are these conversations
that we have.

00:41:40.510 --> 00:41:44.890
And it always brings to mind
the William Gibson quote,

00:41:44.890 --> 00:41:46.690
"The future is already
here, but it's just

00:41:46.690 --> 00:41:49.300
not evenly distributed."

00:41:49.300 --> 00:41:51.790
I always feel like I'm visiting
the future when we talk,

00:41:51.790 --> 00:41:55.200
and it's the future
that I want to live in.

00:41:55.200 --> 00:41:57.790
It's a future also
that understands

00:41:57.790 --> 00:42:00.580
the importance of bringing
the massive advantages

00:42:00.580 --> 00:42:04.120
of technology and business
to all of humanity,

00:42:04.120 --> 00:42:07.540
and a conversity bringing more
of our humanity to the way

00:42:07.540 --> 00:42:09.520
that we do business
and innovate.

00:42:09.520 --> 00:42:14.320
So thank you for provoking,
inspiring, and educate us

00:42:14.320 --> 00:42:15.338
as you always do.

00:42:15.338 --> 00:42:16.630
ASTRO TELLER: Thank you, Julie.

00:42:16.630 --> 00:42:17.920
Thank you to everyone here.

00:42:17.920 --> 00:42:19.395
Thank you for listening.

00:42:19.395 --> 00:42:20.020
Thank you Judy.

00:42:20.020 --> 00:42:21.220
[APPLAUSE]

00:42:21.220 --> 00:42:24.270
[MUSIC PLAYING]

