WEBVTT
Kind: captions
Language: en

00:00:00.270 --> 00:00:03.250
So that actually brings us to
the end of the PomDP topic.

00:00:03.250 --> 00:00:05.230
&gt;&gt; How do you know at
the end of the PomDP topic?

00:00:05.230 --> 00:00:08.880
&gt;&gt; We have high probability of believing
that we're at the end of the pond E.P.

00:00:08.880 --> 00:00:11.990
topic because the observation that
we're making is the what have we

00:00:11.990 --> 00:00:12.525
learned slide.

00:00:12.525 --> 00:00:13.852
&gt;&gt; [LAUGH]
&gt;&gt; So what have we learned?

00:00:13.852 --> 00:00:16.391
&gt;&gt; Well, we learned about PomDPs.

00:00:16.391 --> 00:00:19.320
&gt;&gt; Partially Observable
Markov Decision Processes?

00:00:19.320 --> 00:00:22.170
&gt;&gt; Well we had been exposed to these
things before but we really kind of dove

00:00:22.170 --> 00:00:25.930
into sort of some detail about what
they are and how to solve them.

00:00:25.930 --> 00:00:26.815
And in particular,

00:00:26.815 --> 00:00:32.090
we've talked about belief states as kind
of a representation, which represents

00:00:32.090 --> 00:00:35.780
the states that we might be in and
how much we believe that were in them.

00:00:35.780 --> 00:00:39.286
We talked about PomDPs as
a strict generalization

00:00:39.286 --> 00:00:41.793
of MDPs which is sort of important.

00:00:41.793 --> 00:00:45.112
And that if we can solve a PomDP
that happens to be an MDP,

00:00:45.112 --> 00:00:47.710
we will solve the underlying MDP.

00:00:47.710 --> 00:00:52.810
One of the unfortunate consequences of
all that is that dealing with PomDPs

00:00:52.810 --> 00:00:57.500
is hard they're hard to solve they're
to sort of fundamentally difficult.

00:00:57.500 --> 00:00:59.160
I can't remember
the exact result now but

00:00:59.160 --> 00:01:02.210
there was something in
there about undecidability.

00:01:02.210 --> 00:01:04.830
&gt;&gt; Yeah.
&gt;&gt; And that just that was difficult.

00:01:04.830 --> 00:01:05.792
That was hard for me deal with.

00:01:05.792 --> 00:01:08.098
&gt;&gt; [LAUGH] It was hard for
you to accept hard to accept.

00:01:08.098 --> 00:01:09.841
[LAUGH]
&gt;&gt; Right,

00:01:09.841 --> 00:01:15.036
despite the fact that it's hard,
we did come up with some algorithms for

00:01:15.036 --> 00:01:17.780
solving MDPs like a value iteration.

00:01:17.780 --> 00:01:22.160
I remember something about
piecewise linear and convex.

00:01:22.160 --> 00:01:25.570
&gt;&gt; Right, and representing value
functions as piecewise linear and

00:01:25.570 --> 00:01:26.740
convex functions.

00:01:26.740 --> 00:01:31.052
&gt;&gt; Or pulc and one of the things that
was nice about that is that you show

00:01:31.052 --> 00:01:35.151
that you could build up these
sets of kind of linear functions.

00:01:35.151 --> 00:01:38.295
And because you only cared
about the max in a given point,

00:01:38.295 --> 00:01:41.834
you could throw away a bunch of
them that might be unnecessary and

00:01:41.834 --> 00:01:44.735
sort of keep the hard
problem possibly manageable.

00:01:44.735 --> 00:01:45.931
&gt;&gt; Good.
&gt;&gt; That was really nice.

00:01:45.931 --> 00:01:51.592
Then we went from the solving of PomDPs
to reinforcement learning with PomDPs,

00:01:51.592 --> 00:01:56.829
and if solving PomDPs is hard then RL
for PomDPs definitely has to be hard,

00:01:56.829 --> 00:02:01.380
and that turns out to be true but,
we tried to do it anyway.

00:02:01.380 --> 00:02:03.240
Let's see, we did memoryless.

00:02:03.240 --> 00:02:05.570
&gt;&gt; First we talked about
a model-based method.

00:02:05.570 --> 00:02:06.410
&gt;&gt; Right, model-based method.

00:02:06.410 --> 00:02:09.080
&gt;&gt; Where you try to actually
learn the PomDP from

00:02:09.080 --> 00:02:11.460
experience wandering around in it.

00:02:11.460 --> 00:02:14.290
And do you remember how we
how we did that learning?

00:02:14.290 --> 00:02:15.401
&gt;&gt; With great difficulty?

00:02:15.401 --> 00:02:19.170
&gt;&gt; Well sure, we talked about explicit
expectation maximization as a way of

00:02:19.170 --> 00:02:21.350
trying to organize that computation.

00:02:21.350 --> 00:02:24.782
&gt;&gt; Sure,
that's a favorite of my expectations.

00:02:24.782 --> 00:02:25.692
&gt;&gt; Your maximally favorite?

00:02:25.692 --> 00:02:27.359
&gt;&gt; [LAUGH] Yeah,
my maximally favorite one.

00:02:27.359 --> 00:02:30.052
We've got to go all the back and
try to remind ourselves how Worked.

00:02:30.052 --> 00:02:31.168
&gt;&gt; Yes.
&gt;&gt; That was fun.

00:02:31.168 --> 00:02:35.011
[LAUGH] And then we did memoryless?

00:02:35.011 --> 00:02:37.885
&gt;&gt; Yeah, and that we needed in
the memoryless case we might need to

00:02:37.885 --> 00:02:38.435
be random.

00:02:38.435 --> 00:02:40.540
&gt;&gt; Again, just like grad school.

00:02:40.540 --> 00:02:42.260
&gt;&gt; [LAUGH] I don't think that's true.

00:02:42.260 --> 00:02:43.990
You're saying that if
you're in grad school but

00:02:43.990 --> 00:02:48.350
you don't remember where you are in grad
school, then it helps to act randomly.

00:02:48.350 --> 00:02:50.449
&gt;&gt; Well it certainly explains
the behaviors on my grad.

00:02:51.480 --> 00:02:57.080
Okay, then we had, we probably did more
stuff there but I can't remember it.

00:02:57.080 --> 00:02:59.271
&gt;&gt; [LAUGH]
&gt;&gt; And so I do remember though that we

00:02:59.271 --> 00:03:01.650
started talking about Bayesian RL.

00:03:01.650 --> 00:03:02.252
&gt;&gt; Yes.

00:03:02.252 --> 00:03:06.402
And the cool thing about Bayesian RL
being that it blurs the line between

00:03:06.402 --> 00:03:08.850
planning and learning so.

00:03:08.850 --> 00:03:12.020
And Bayesian stuff does that in general,
this sort of notion that,

00:03:12.020 --> 00:03:16.670
well all learning really is is
estimating probabilities of things,

00:03:16.670 --> 00:03:22.036
it's not actually learning, so
just changing your posterior.

00:03:22.036 --> 00:03:26.780
And so, decision making in the Bayesian
RL setting ends up becoming just a kind

00:03:26.780 --> 00:03:30.820
of version of planning in
a continuous ugly kind of PomDP.

00:03:30.820 --> 00:03:34.490
&gt;&gt; Speaking of which, if we have
Bayesian RL, do we have frequentist RL?

00:03:34.490 --> 00:03:36.650
&gt;&gt; Well sure, I mean Bayesian and

00:03:36.650 --> 00:03:40.890
frequentist are kind
of rivals in a sense.

00:03:40.890 --> 00:03:44.239
So anything that's not
Bayesian maybe is frequentist.

00:03:44.239 --> 00:03:45.290
&gt;&gt; Okay.

00:03:45.290 --> 00:03:47.820
&gt;&gt; So all the other reinforcement
learning that we did like hue

00:03:47.820 --> 00:03:48.810
learning and whatnot.

00:03:48.810 --> 00:03:53.610
As it certainly has a frequentist flavor
in that what is frequentist statistics

00:03:53.610 --> 00:03:56.850
is about, it's about you know counting
up how many times something happens and

00:03:56.850 --> 00:03:59.010
divided by the number
of times you asked.

00:03:59.010 --> 00:04:03.420
And that is definitely, queue learning
has that flavor model based RL where

00:04:03.420 --> 00:04:06.070
you're trying to estimate
the parameters of the model by

00:04:06.070 --> 00:04:08.720
from experience they
tend to have that flavor.

00:04:08.720 --> 00:04:09.280
&gt;&gt; Okay, so good.

00:04:09.280 --> 00:04:11.580
So then we have been learning
about frequentist RL.

00:04:11.580 --> 00:04:12.860
And now that we have Bayesian RL,

00:04:12.860 --> 00:04:16.290
it reveals the frequentist things
that we'd been doing in the past.

00:04:16.290 --> 00:04:17.272
So I'm okay with that.

00:04:17.272 --> 00:04:20.485
&gt;&gt; Yeah and the Bayesian RL is very nice
because it doesn't make an exploration

00:04:20.485 --> 00:04:22.100
exploitation distinction anymore.

00:04:22.100 --> 00:04:26.650
You're always acting given your
knowledge of what the truth is and

00:04:26.650 --> 00:04:30.230
you're acting in a way that maximizes
your award given that knowledge.

00:04:30.230 --> 00:04:32.838
&gt;&gt; Right and so
exploration is exploitation.

00:04:32.838 --> 00:04:35.623
[SOUND]
&gt;&gt; Planning is learning.

00:04:35.623 --> 00:04:37.949
Exploration is exploitation.

00:04:37.949 --> 00:04:39.463
War is peace.

00:04:39.463 --> 00:04:45.730
&gt;&gt; [LAUGH] And we have always been
at war with supervised learning.

