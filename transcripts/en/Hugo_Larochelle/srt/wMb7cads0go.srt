1
00:00:00,880 --> 00:00:01,550
In this video,

2
00:00:01,550 --> 00:00:06,550
we'll complete our description of the
contrasted divergence algorithm as applied

3
00:00:06,971 --> 00:00:07,810
to every sector,

4
00:00:07,811 --> 00:00:12,260
both the machine and get an actual
concrete parameter update rule.

5
00:00:14,100 --> 00:00:14,933
Yeah.

6
00:00:15,090 --> 00:00:19,200
And the previous video we've talked
about how to estimate the negative log

7
00:00:19,201 --> 00:00:20,490
probability gradient.

8
00:00:20,730 --> 00:00:25,730
We needed to estimate this
a double expectation over
both x and h in the model.

9
00:00:27,990 --> 00:00:29,000
And then,
uh,

10
00:00:29,080 --> 00:00:33,210
this justified because this is actually
attracted book with them to Compute is

11
00:00:33,211 --> 00:00:36,120
justified in approximation
and you contrasted divergence.

12
00:00:36,121 --> 00:00:40,890
The idea is to replace this expectation
by a point estimate where we'll actually

13
00:00:40,891 --> 00:00:45,510
just simple a single value for x
and a to obtain that value of x,

14
00:00:45,630 --> 00:00:46,820
which we'll call [inaudible].

15
00:00:47,340 --> 00:00:51,030
We actually perform a few steps of gib
sampling initialized at the current

16
00:00:51,031 --> 00:00:52,020
training observation.

17
00:00:52,860 --> 00:00:57,510
So now let's look at how we can in the
context of a restricted Boltzmann machine

18
00:00:57,630 --> 00:01:01,170
compute this fall, uh, update, uh,

19
00:01:01,171 --> 00:01:03,510
assuming that we've performed a,

20
00:01:04,030 --> 00:01:08,970
a few steps of gib sampling at starting
at the current trainee example.

21
00:01:08,971 --> 00:01:09,804
X.
T

22
00:01:10,890 --> 00:01:11,723
yeah.

23
00:01:12,370 --> 00:01:13,890
Okay. Um, we need, uh,

24
00:01:13,920 --> 00:01:16,840
to perform a few more their
vacations to get the full algorithm.

25
00:01:16,960 --> 00:01:21,640
So I remember that we have to perform
an expectation over age of the partial

26
00:01:21,641 --> 00:01:23,740
derivative of the energy with
respect to any parameter.

27
00:01:23,770 --> 00:01:28,330
So let's do first the partial derivative
of the energy with respect to uh,

28
00:01:28,420 --> 00:01:29,260
any prevnar.

29
00:01:29,261 --> 00:01:34,261
So actually we'll just do the case for
the connection between a Djf hidden unit

30
00:01:34,630 --> 00:01:35,620
and the kief input.

31
00:01:37,060 --> 00:01:41,700
So the partial do they have of the energy
with respect to a connection is does

32
00:01:41,701 --> 00:01:45,520
the partial there, they've of this,
that's just the energy function.

33
00:01:45,880 --> 00:01:49,120
And we notice that there's only one
term which involves this particular

34
00:01:49,121 --> 00:01:51,880
connection between h j and x.
K.

35
00:01:52,270 --> 00:01:54,520
So really to be a strictly correct,

36
00:01:54,521 --> 00:01:58,810
I should have a j prime and k prime.
But just to simplify,

37
00:01:58,880 --> 00:02:02,150
I'm using the same indices. Um, um,

38
00:02:02,450 --> 00:02:04,420
but you should understand
what's going on here.

39
00:02:04,420 --> 00:02:08,440
So really this whole
some here in particular,

40
00:02:08,441 --> 00:02:11,470
this sum here is the only one
that involves connections.

41
00:02:11,471 --> 00:02:14,890
So the partial [inaudible]
affected this or that is zero.

42
00:02:15,520 --> 00:02:20,220
And then the only term here that involves
exactly the same connection between HD

43
00:02:20,290 --> 00:02:23,590
and ex gay is the one that
involves HJ index gay.

44
00:02:24,670 --> 00:02:29,620
So the partial derivative of that here
is really the partial [inaudible] of the

45
00:02:29,621 --> 00:02:34,540
term Wj k times h j Aix, Aix, k
for the same value of j and k.

46
00:02:34,990 --> 00:02:39,130
And uh, this is a term that's linear
in my parameter, so it's value,

47
00:02:39,140 --> 00:02:43,780
it's just whatever it was multiplying
that weight, which is h j o n x game.

48
00:02:44,140 --> 00:02:45,820
And then I get them minus which was here.

49
00:02:46,950 --> 00:02:51,150
And so if I want to put that into matrix
form so they get the grade and that the

50
00:02:51,160 --> 00:02:55,590
energy with respect to the full matrix
w then you should be able to see that

51
00:02:55,650 --> 00:03:00,160
this is just mine as the outer
product of the age vector with the,

52
00:03:00,250 --> 00:03:03,790
uh, transport. So with the, the
x factors, or in other words,

53
00:03:03,791 --> 00:03:07,480
the Matrix product between the column
vector minus the column vector,

54
00:03:07,510 --> 00:03:10,210
h times their row vector x.

55
00:03:14,050 --> 00:03:14,261
Now,

56
00:03:14,261 --> 00:03:19,261
if we perform the expectation with respect
to age condition on any value of the

57
00:03:19,631 --> 00:03:23,500
factor x, so, uh, we have this,

58
00:03:23,560 --> 00:03:27,940
we're trying to do this for a
theater equals to our connection. Uh,

59
00:03:27,941 --> 00:03:32,941
so we've shown that this is equal
to that here minus h j times x.

60
00:03:33,271 --> 00:03:35,270
K. Um, so this is, uh,

61
00:03:35,410 --> 00:03:40,410
an expectation over all neurons in the
hidden there h but this involves on the

62
00:03:40,661 --> 00:03:44,890
one had in neuron a DJ at hidden neuron.

63
00:03:45,100 --> 00:03:48,680
So really this is just
a, some uh, over, uh,

64
00:03:48,730 --> 00:03:50,800
that neuron being called two zero,

65
00:03:50,801 --> 00:03:54,430
one of the term that we have here,

66
00:03:54,520 --> 00:03:59,520
but waited by the probability that HJ
is equal equal to it's given value.

67
00:04:00,400 --> 00:04:03,950
And I notice that in this,
um, there's two cases, uh,

68
00:04:03,980 --> 00:04:07,330
HD equals to zero. HP, h j sorry,

69
00:04:07,390 --> 00:04:12,040
equals zero h j equals two one
if Hja zero, then this is zero.

70
00:04:12,070 --> 00:04:16,360
And otherwise this term is
just that equals to one.

71
00:04:16,690 --> 00:04:18,910
And so that's the only non zero terms.

72
00:04:18,911 --> 00:04:23,911
So really this is just
equivalent to minus X.

73
00:04:23,980 --> 00:04:26,770
Gay Times are probably that
Aisha is equal to one given x,

74
00:04:27,370 --> 00:04:30,640
which is given right here.
And again,

75
00:04:30,641 --> 00:04:35,440
if we want to write this into matrix
form, um, then we know this again,

76
00:04:35,441 --> 00:04:40,240
that we could write this as the
other product of minus a vector,

77
00:04:40,540 --> 00:04:42,040
which I'm going to call h of x,

78
00:04:42,340 --> 00:04:46,310
which is the vector that contains the
probability of each hidden unit being

79
00:04:46,330 --> 00:04:47,620
equal to one given X.

80
00:04:47,860 --> 00:04:52,570
I put that into a vector and a notice
that this is just the sigmoid of,

81
00:04:53,100 --> 00:04:53,500
uh,

82
00:04:53,500 --> 00:04:58,450
the bias vector plus the
Matrix of weights w times x.

83
00:04:59,380 --> 00:05:02,680
So I take that vector
Hvacs, uh, and I, uh,

84
00:05:02,710 --> 00:05:07,540
do the other product with
the vector x. And so the, uh,

85
00:05:07,570 --> 00:05:11,020
entry in that matrix at ro,
uh,

86
00:05:11,170 --> 00:05:15,250
Jay and columnK is going
to be exactly this.

87
00:05:16,790 --> 00:05:19,870
So none of this also that conveniently,
uh,

88
00:05:20,040 --> 00:05:23,950
h of x is actually just the value of
hidden layer in a feed for neural network

89
00:05:24,190 --> 00:05:27,610
with sigmoidal activation function.
Uh,

90
00:05:27,730 --> 00:05:32,140
this is somewhat of a
coincidence, but, uh, it start
seeing that, you know, this is,

91
00:05:32,200 --> 00:05:33,310
this is really a neural network.

92
00:05:33,311 --> 00:05:37,110
There are relationships between the
restrictive Bolsa machine and, uh, uh, uh,

93
00:05:37,150 --> 00:05:37,983
neural networks.

94
00:05:41,740 --> 00:05:45,230
And so if we put everything
together, it means that, eh,

95
00:05:45,340 --> 00:05:50,340
for a given a training example
and a negative sample that I've,

96
00:05:50,850 --> 00:05:51,070
uh,

97
00:05:51,070 --> 00:05:56,070
simple with case steps of gib sampling
initialize that XD some calling that x

98
00:05:56,311 --> 00:05:57,740
stilled.
Uh,

99
00:05:57,770 --> 00:06:01,460
then if I want to update w
by Matrix of connections,

100
00:06:01,770 --> 00:06:06,770
then want to take w minus some learning
rate of an estimator of the gradient of

101
00:06:07,761 --> 00:06:12,080
the negative log probability of
the observed training sample.

102
00:06:12,410 --> 00:06:15,950
We've seen that this expression
was the difference between a,

103
00:06:16,430 --> 00:06:20,180
a positive phase and a negative phase,

104
00:06:20,181 --> 00:06:23,420
the positive phase and expectation
condition on the observation.

105
00:06:23,421 --> 00:06:27,170
And the other one's an expectation where
I'm also taking the expectation over

106
00:06:27,500 --> 00:06:30,380
the x factor.
Uh,

107
00:06:30,381 --> 00:06:33,140
then I've argued that I
can compute this exactly.

108
00:06:33,200 --> 00:06:36,950
So what I'll do is that I'll
replace this double expectation, uh,

109
00:06:36,980 --> 00:06:41,810
with a point estimate where I'm
going to a sample, a value for x,

110
00:06:41,900 --> 00:06:44,390
which I'm calling instilled
and I'm given x still.

111
00:06:44,390 --> 00:06:48,860
I can actually do the remaining part
of the expectation over H and. N.

112
00:06:48,861 --> 00:06:52,910
I've shown that, uh, in a
four and the value of x,

113
00:06:53,510 --> 00:06:58,510
this expectation with correspond to
my h of x vector or four XD cause I'm

114
00:06:59,091 --> 00:07:01,970
conditioning our XD here, uh, uh,

115
00:07:02,000 --> 00:07:05,870
multiply by the transpose of XD and
then this which has the same form,

116
00:07:05,871 --> 00:07:10,390
but instead of conditioning on x
IX stilled, that's going to be uh,

117
00:07:10,520 --> 00:07:13,780
age stilled times the
transpose of external.

118
00:07:14,280 --> 00:07:18,860
So I combine that together and
multiply this by uh, Alpha.

119
00:07:19,130 --> 00:07:22,460
And then I can just increment w by that.
Okay.

120
00:07:22,461 --> 00:07:25,490
So notice we had a minus
here and the minus there.

121
00:07:25,491 --> 00:07:28,270
So that's why there's no minus here
anymore. We have a plus here but,

122
00:07:28,630 --> 00:07:29,810
and then we have a minus here.

123
00:07:32,210 --> 00:07:36,770
So to sum up the sort of go for a
contrasted divergence corresponds to,

124
00:07:37,140 --> 00:07:37,490
uh,

125
00:07:37,490 --> 00:07:42,490
going over to training examples for the
current training example performed case

126
00:07:43,011 --> 00:07:47,570
steps of Gib sampling to obtain
a negative sample x stilled and,

127
00:07:47,571 --> 00:07:48,920
and I update my parameters.

128
00:07:48,921 --> 00:07:53,570
So I have the update for the Matrix W
I'll leave it as an exercise to show that

129
00:07:53,870 --> 00:07:57,620
we get a similar update for B and c.
Uh,

130
00:07:57,621 --> 00:08:01,880
so it's going to be what is
observed minus what I've sampled.

131
00:08:02,030 --> 00:08:04,760
So for the sea vector,
which is the bias for x,

132
00:08:05,090 --> 00:08:09,380
I get a term that involves x.
And for the B vector, which, uh,

133
00:08:09,470 --> 00:08:11,210
is a bias for the hidden units,

134
00:08:11,360 --> 00:08:16,270
then I get the vector of
probabilities as, uh, given, uh,

135
00:08:16,340 --> 00:08:21,260
condition on either x or x tilt.
And so,

136
00:08:21,261 --> 00:08:25,940
and then I keep doing this
until some stopping criteria.
So for instance, if I, uh,

137
00:08:26,030 --> 00:08:30,110
until my parameters don't change too
much or for a fixed number of a parameter

138
00:08:30,111 --> 00:08:33,790
which I could cross
validate somehow, uh, uh,

139
00:08:33,830 --> 00:08:36,050
using some given criteria
of generalization.

140
00:08:39,260 --> 00:08:41,870
So, uh, this is, uh, uh,

141
00:08:41,930 --> 00:08:45,650
so the acronym we use code for contrast
advisor divergence is CD and sometimes

142
00:08:45,651 --> 00:08:49,670
you'll see CD dash gave,
uh, where I explicitly, uh,

143
00:08:49,700 --> 00:08:54,110
illustrate what's the value
of, of, of gay. Um, so,

144
00:08:54,230 --> 00:08:57,210
uh, so the number of
gifts as assembling steps,

145
00:08:57,780 --> 00:09:02,070
the bigger key is the less biased our
estimate of the gradient is going to be.

146
00:09:02,100 --> 00:09:03,660
So if k was very,

147
00:09:03,661 --> 00:09:07,410
very large and it means that my gifts
chain would have essentially converged to

148
00:09:07,411 --> 00:09:08,880
the true model of distribution.

149
00:09:08,910 --> 00:09:13,710
And so I just get a Monte Carlo estimate
of the expectation. So in other words,

150
00:09:13,711 --> 00:09:14,670
in expectation,

151
00:09:14,730 --> 00:09:18,720
this estimate would be with
corresponds to the true expectation.

152
00:09:19,410 --> 00:09:24,300
Um, and now the, the, the,
the less k is big, however,

153
00:09:24,540 --> 00:09:28,950
uh, the less it's going
to be, uh, the more bias.

154
00:09:28,951 --> 00:09:33,030
There's going to be my estimate of the,
uh, uh, negative log likelihood gradient.

155
00:09:33,600 --> 00:09:37,020
However, it turns out that to
extract some features or uh,

156
00:09:37,021 --> 00:09:39,990
to initialize neural networks,
which we call pre training,

157
00:09:39,991 --> 00:09:42,930
which is something we'll discuss
later in this course. Uh,

158
00:09:42,960 --> 00:09:46,260
k equals one works well
in practice. And, um,

159
00:09:46,410 --> 00:09:49,350
so there's been some research for
trying to understand why that is.

160
00:09:49,351 --> 00:09:52,810
The initial paper and contrasted
divergence was trying to argue that, uh,

161
00:09:52,880 --> 00:09:55,620
you know, this, this
actually makes sense. Um,

162
00:09:56,250 --> 00:09:58,350
and so we'll just live with the fact that,

163
00:09:58,410 --> 00:10:01,530
and we are actually quite happy with this,
that k equals one.

164
00:10:01,650 --> 00:10:06,450
So performing just one sample of age and
then immediately resembling IX stilled

165
00:10:06,780 --> 00:10:08,740
actually provides a,

166
00:10:08,810 --> 00:10:13,810
a is a good procedure for getting a
meaningful value for x still in order to

167
00:10:14,041 --> 00:10:18,030
learn a, uh, learn a good value of,

168
00:10:18,031 --> 00:10:21,960
of hidden units, uh, at least
in terms of extracting features.

169
00:10:22,410 --> 00:10:23,520
Um,
however,

170
00:10:23,521 --> 00:10:28,170
if we were to look at how
good the model is and,

171
00:10:28,171 --> 00:10:29,070
uh,
uh,

172
00:10:29,100 --> 00:10:33,420
it turns out that using larger
values of k is actually beneficial.

173
00:10:33,421 --> 00:10:37,170
That is if we look at, um, uh,

174
00:10:37,200 --> 00:10:39,990
whether the value of the,
uh,

175
00:10:40,020 --> 00:10:44,150
average negative log probability of,
uh,

176
00:10:44,400 --> 00:10:46,650
a test set, for instance, um,

177
00:10:46,680 --> 00:10:51,360
if k becomes bigger than typically,
this will be much better.

178
00:10:51,570 --> 00:10:56,370
But in terms of just extracting features
that is getting a good value of the

179
00:10:56,371 --> 00:10:58,570
vector h of x,
uh,

180
00:10:58,690 --> 00:11:01,920
which we could then use as a feature
representation for another learning

181
00:11:01,921 --> 00:11:06,180
algorithm. It turns out that key equals
one works well in practice. All right,

182
00:11:06,181 --> 00:11:08,810
so that summarizes the
contrasted divergence algorithms.

