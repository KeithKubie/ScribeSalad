Speaker 1:          00:00          [inaudible]

Speaker 2:          00:04          so what would you say to the person listening that's going, well, wait a minute, 1000 bucks, I can't do anything with 1000 bucks. There's basically nowhere I can get rent. You know, how am I going to do anything close to a living?

Speaker 3:          00:14          Right? So for most people now, $1,000 is not going to be enough, but it will provide a cushion. Right? So that's sort of the whole idea. We're starting off with $1,000 and that's not going to be enough. So you're still have to work.

Speaker 2:          00:25          So is that the thing? Is that the biggest confusion related to all this? That I think people hear ubi and they think that it's just enough. Just enough. So you can get by. But you're saying that's actually not really what's going on here.

Speaker 3:          00:37          It may not be enough initially. Um, I mean there may be some places in the country, not Los Angeles, but some places you could live on $1,000. So if you're really in a bad situation and you're living in La, you could pack up and move somewhere where maybe a thousand dollars we'll allow you to survive. Right? That's a possibility. But I think but most people will do is they will take that thousand dollars and they will use that to sort of cushion, um, the difficult times. But they will still be motivated to find a job, to start a business, to do something there. We just have more options, more freedom in terms of the choices that they make. Um, and that's why you mentioned you're having Andrew Yang and he actually calls this program the freedom dividends. That's what he's named it. And that's exactly what it is. It gives you more options, options, especially for someone that's living month to month and really has no income. You know, the, the number of choices that you have at in that scenario, which is very limited.

Speaker 2:          01:30          Yeah. I guess so much of this has to do with just the strange way we deal with politics. So, for example, you'll have, you know, politicians saying we have to have $15 minimum wage and then you can walk into Burger King or Mcdonald's now in order on an iPad because they're basically saying, all right, we're not going to pay our people this much. So everything just becomes sort of Uber politicized, right?

Speaker 3:          01:51          That's right. And, and you know that, that's why a basic income is maybe a better approach because we just give it to everyone. And the problem with raising the minimum wage is that it may be, you know, good for many workers, but it can actually also increase the incentive to automate or, or to do other things, right? So it could actually result in fewer jobs. So giving people a basic income and preserving the incentive for them to still work or to do other things, I think is, is a very attractive proposition.

Speaker 2:          02:21          W when you sort of play out, you know, five, 10, 20 years from now, do you think things are going to just be beyond drastically different in a way we really can't think about because of the speed of all of this because of everything we've talked about here that we really can't even envision the way is,

Speaker 3:          02:38          I think if you go out maybe a little further than that 2030 years, that really, really gets hard to imagine what the future looks like. Um, my latest book, architects of intelligence, one of the people, you know, it's a series of interviews with the top people in AI. One of the people I talk to is Ray Kurzweil. Chris is the big futurist. He thinks that it will be alive then we won't be long guy. Yeah, he absolutely, he thinks he's going to live forever. He expects what he calls the singularity is something that is going to completely change the whole paradigm. Um, he thinks that within just 10 years we're going to have human level, artificial intelligence and so forth. So that's possible. Um, the problem is that is very unpredictable. We don't know how fast all of this is coming. What I've really focused on is sort of the practical impacts of AI and robotics and the impact on the job market.

Speaker 3:          03:27          So what I would say is that within five to 10 years, we're going to definitely see a fairly dramatic and unambiguous impact on the job market and on the economy. And I think ray is basically gonna live long enough to see the robots take over one way or another. That's what he believes. You know, ray is, he's already 70. But uh, if you've seen his photos recently, he looks a lot younger than then he did a while ago. So He's, he's at least he's had some work done. So whether the, you know, the stuff under the hood is, is, is better or not, but is he the only person doing that sort of thing? There must be some other people. Oh, no, there are a lot, you know, silicon valley, there are many people very interested in this idea of living forever and, and advanced it, you know, the Google People Larry Page and Sergei and, uh, Peter Teal is very into this as well, is even I think played around with supposedly blood transfusions and stuff like that.

Speaker 3:          04:23          So yes, the, the silicon valley elite is, you know, there are true believers in terms of this idea that technology is going to completely transform things in the future, is going to be dramatically different from the past and then we're going to potentially even have the possibility of living forever. Can you explain the singularity? The singularity basically means a point at which technology takes off and begins accelerating at a rate that becomes an comprehensible to us. So they, it comes from basically a black hole, right? The center of a black hole was what's called a singularity where the laws of physics breakdown and you can't see beyond that point. Um, so this, the term singularity was coined, um, as a way to express the idea that technol technology reaches a point where it's just completely unpredictable beyond that point because things are moving so fast.

Speaker 3:          05:16          And most people that think about this associate that with the advance of what's called superintelligence are machines that are smarter than us, not only human level intelligence, but a machine that, that you know, is smarter than any human being, maybe dramatically. So maybe so smart that it makes us look like a mouse or an insect. And that's where my Psi Phi brain and every movie that I've ever seen and every dystopian future says, well, why would the robots need us at that point? Exactly. If anything, wouldn't they just see us in it as an annoying hindrance or a vestige of the past? And why wouldn't they want to get rid of us? Right. And that's a real concern, that concern, which is what you see in the terminator movie. Um, and, and even more than that, the related concern of what's called the control problem, which is that if we created a superintelligence, something that's far beyond us, maybe it won't actively want to destroy us, but it might act in ways that are not, you know,

Speaker 2:          06:13          what our by a robot,

Speaker 3:          06:15          right. Um, and there are very, very serious thinkers that are focused on this. The money, the most prominent ones is Nick Bostrom, who I also interviewed, um, in my latest book, architects of intelligence. So he, you know, believes that this is a real issue and he's working on finding ways to build systems that will be controllable even if they become super intelligent. And so this is an issue that he's focused on.

Speaker 2:          06:41          Oh, but the inherent problem being that if you create the super intelligence, it probably at some point can get around that. I mean, I know that's not mind blowing to him, but like, right, like that's the whole idea.

Speaker 3:          06:50          Yeah. Once we have a super intelligence, then, you know, it's so far beyond us that we can't control it anymore. So what, what people are working on, um, is basically a principles of computer science that will hopefully allow them to build these systems in a way that, that will remain aligned with what we want them to do even when they're super intelligent.

Speaker 2:          07:11          Right. But it's the problem with that, what we hit on earlier, which is maybe we here in America hopefully figure out some systems that are going to make some sense, but if the Chinese figure out a system that's a little bit different or just some random guy in his garage in Mexico figures out some other thing that we still have that basically there's just no way to manage. It seems like a big problems. One of

Speaker 3:          07:32          the scariest aspects of this is that competition and the fact that there would be an incredible first mover advantage to whoever gets there first. Whoever builds the first superintelligent system, you know, is, is going to be way ahead of everyone else. And the reason is, is that most people believe in what's called an intelligence explosion or kind of, um, iterative improvement where basically once the machine reaches human level intelligence or it gets beyond that, it's going to turn its attention to its own code, right, to building better versions of itself. So it's going to continuously engineer a smarter version of itself and that's something that could explode rapidly. So whoever gets there first, they're essentially uncatchable. So that is going to set up a competitive environment between the US and China and Russia and so forth. So that's something to worry about. Um, but there are a lot of people working on doing this in a safe way.

Speaker 3:          08:27          Open Ai is another good example. That's the, the organization it was set up by Elon Musk, right? And some other people and they're actively working on building systems that they basically, they're trying to get there first to be the first one to, to create a generally intelligent system and to do it in a way that is safe. So, um, I think that's a good thing. There's, there's some real focus on that and investment in that area. But at the same time there's also a lot of hype people like Ilan saying some pretty over the top things. I do think that to some extent that's a bit dangerous because it, again, this is something that is probably pretty far in the future. I would say probably at least 50 years of away. Um, there is a big debate over that. Again, in my latest book I interviewed all these people. I asked them this question, how soon are we going to have a computer that is at the level of a human being in terms of intelligence and the predictions I got ranged from 10 years from Ray Kurzweil to nearly 200 years. Wow. So there's a wide variation that the average guest was about 80 years. So at the end of this century, so pretty cool.

Speaker 2:          09:32          What are the markers that cause people to have a different response to that question? So why would someone like Kurzweil say 10 and then someone else says 200

Speaker 3:          09:39          well, you know, there are a number of breakthroughs that you have to have. You have to have machines that can learn the way people learn. Um, right now as I said, we've got machine learning, deep learning, which is highly dependent on lots and lots of data in particular labeled data. So you can train one of these algorithms to recognize pictures of a dog and you would give it maybe a million photographs that had, that was, you know, these photos. Would he labeled either there's a dog there or there's not a dog there. And based on this he could learn and eventually recognize dogs that a superhuman level, but that's not the way a human child learns, right? Um, a human child, you can, you can point to a dog and maybe you only need to do that once before the kid needs to learn. And so one of the biggest initiatives is teaching machines to learn from less data in, in an unsupervised way, in the way that that people can.

Speaker 3:          10:29          Um, and then you've got to have the ability to think generally to conceive new ideas, to be creative, to understand that one thing causes another thing is as opposed to just two things being correlated to develop counterfactuals to imagine I've got this plan for the future, but if I tweak this one thing, then this is what's going to be different. These are all uniquely human ways of thinking and it's going to take a lot of breakthroughs, um, before we have a machine that can do all of those things. And there's just a lot of disagreement even between the very smartest people working in this field, um, about how long it's going to take for those kinds of breakthroughs to happen.

Speaker 2:          11:08          How concerned are you about, uh, the unbiased thing that seems to be happening when it comes to the algorithm? So for example, you know, the famous case that everyone talks about is that if you Google American scientists, that there is, that it happens to be, it's just a function of things that most American famous American scientists who have done most of the breakthroughs, most, not all happened to be white men. But that Google is an biasing the searches. So it includes more black people are more women or things like that, which nobody has a problem with. No one in their right mind has a real problem acknowledging that there are scientists of every color and gender and all of those things. Um, but they're unbiased and things that are, that are not. So it's not really factual sort of what we're putting in the algorithm and that where that could lead us see,

Speaker 3:          11:52          right. It's pretty, that's

Speaker 2:          11:54          ultimately a decision for society I suppose how we want to address those issues. I remember the whole issue of bias in algorithms is a huge issue in, in artificial intelligence. People are working on risking that and that that operates in both ways. I mean there are definitely an absolutely have been legitimate cases of algorithms that are biased against people of color and so forth. For example. Um, and gender too. I mean, I know that um, one company for example, stopped using, um, an AI system that we choose to screen resumes because it was biased against women and so forth. And there've been other things. Where does that bias come from in a situation with bias? You know, what happens is that, again, these are systems that are learning from data, right? But where does that data come from originally? It comes from people. So if people are biased in some way and they're generating this data, then an algorithm, a machine learning, I was driven, comes along and is trained on that data, it will pick up that bias.

Speaker 2:          12:51          So basically we're, we're the flaw in the system. Absolutely. More than anything else really. Right. Um, but there is a hopeful note there, which is that fixing bias in the human being is very hard, right? I mean, we don't really know how to do that and we know that it does exist to some extent, but fixing it in an algorithm could be a lot easier. Right. It's basically tweaking some bits. So as we bought, I guess it depends who's doing it though, right? Exactly. As long as, as long as it's done in, in, in a careful proper way. But we can't imagine a future where algorithms as they were employed more maybe as, as kind of a check on decisions or maybe in some cases actually making decisions. Um, it can actually be a less biased world and not a more biased world. But you're right, there are huge numbers of issues running in both directions there. Um, well it's funny that that's sort of the theme of all of this because I'm even trying to sort of figure out as you're talking, are you optimistic about this or, or, or pessimistic about sort of where this could all lead? And I, I definitely sense both sides.

Speaker 3:          13:52          Yeah. I mean, I tend to be speaking more holistically including, you know, there are many, many issues with Ai, um, things that we should be concerned about biases. One security, the ability, the ability of people, bad people to hack into a system and, and, and do evil things with it. Um, the potential for weaponization is another thing that many people are really, really worried about. The idea that you can have autonomous weapons, um, and not just one autonomous weapon that might independently kill people without a human in the loop, but do, you could have thousands of them swarming, right, which be this guy. And that truly terrifying. And this is something that you know, is, is not really science fiction. I mean, we were talking earlier about super intelligence and the terminator where the machine's actively or making a choice to kill us, that science fiction that lies far in the future, but the idea of having thousands of swarming autonomous drones that were not intelligent independently, but we're programmed by somebody else to do something to attack someone or so. So this is something that could happen.

Speaker 2:          14:57          So the idea being that, okay, Amazon moves to drone delivery and then someone hacks into the system and instead of these drones dropping packages at our door, they're flying through our windows and

Speaker 3:          15:07          the attack on people on the streets. Or it could be hacking or whatever. Yeah. Or it could be someone, you know, manufacturing a huge numbers of these drones and then installing autonomous software. Um, because you know, the barrier to entry here is pretty low. I mean, these are, these are weapons that some people could be like weapons of mass disruption. If you had enough autonomous drones, that would be incredibly dangerous. Right now if you look at something like nuclear weapons, um, in order for our country to have nuclear weapons that, you know, you've got to be a nation state, you've got to have left, you know, resources on that level in order to develop nuclear weapons. But these kinds of technologies where you're talking, you know, and there's a lot of um, overlap between the commercial sector and things that could be done on the security or military side.

Speaker 3:          15:53          You could go on Amazon, you could purchase a thousand drones and then maybe you could, um, you know, engineered them to be, to be weapons or something. These are something that, you know, there's a much lower threshold there. People in the basement somewhere could be doing this kind of stuff, right? So it is quite, well I think they know already, but it is quite scary. And many people in the Ai community, they were very passionate about this in particular. There there's an initiative in the United Nations to actually been fully autonomous weapons for example. And, and the real worry is not just the military is would use these kinds of weapons, but it would go beyond that and you would have, you know, there's kind of shady arms dealers that now sell machine guns are selling autonomous drones. Um, and so that they then become available to terrorists and all kinds of people. And this is, this is a really scary scenario. One of the people I interviewed, uh, uh, Stuart Russell, who's a professor at UC Berkeley, created a youtube video code slaughter bots. You can go on and watch and it's really quite terrifying and it shows you exactly what could be done with huge numbers of swarming autonomous drones. And it's not, again, it's not science fiction. It's something that could happen in the next five, 10 years. Do you,

Speaker 2:          17:07          uh, are you familiar at all with just sort of the anti technology movement? The more that you pay attention to the technology movement and the amount of people that are trying to either get off the grid or limit the amount of time online and, and that whole thing,

Speaker 3:          17:20          right? I mean, I, you know, that, that's I think a natural response to a lot of this. I mean, uh, the, the, the worst example of that is, is Ted Kaczynski, right? The Unabomber, right? Who actually wrote a manifesto that he, he was published I think in the New York Times. But if you go and read that manifesto, this is guided. Okay. He's crazy, right? He's a murderer, all of this. But if you read that manifesto and not know that it was written by him, he's raising a lot of the issues that we are now talking about. You know, the issues that technology could be a threat. Um, the issue that we might become so dependent on this technology that we lose our agency, right? Or ability to think for ourselves. Um, and so far, so, you know, even back then these people, like we're, we're thinking about this.

Speaker 3:          18:07          And so this is a natural response in one of the things I fear the most is that if we don't find a way to adapt to these technologies and find a way to leverage all this progress on behalf of everyone so that everyone is better off, there's going to be a bigger and bigger backlash. People are going to turn against the technology. Um, and that might mean not just going off the grid and living as a hermit, but actually be, you know, becoming much more adversarial to the system. And that might happen politically. It might happen in some places even in the form of social unrest and so forth as things get, get bad enough if we really have unemployment. So I just think it's critically important that we begin to really address these issues and have a honest discussion about them so that we can avoid that scenario. I assume you're a fan of black mirror. I haven't really watched that, but I've heard a lot about it. But um, but yeah, those kinds of scenarios or you know, science fiction now, but they are every day becoming reality.

Speaker 2:          19:06          Yeah. Is there a Scifi movie that you think handles some of this in the best way? So not going all the way to terminator tomorrow, but like there's some movies that you think of sort of teased out some of the more realistic features or closer,

Speaker 3:          19:18          yeah, there, there are several. There's one movie, um, a few years ago called Elysium, which really got at the issue of inequality because what happen,

Speaker 2:          19:27          oh, that's the one with the, they built this love artificial sugar and all the rich people migrated. Aaron Earth became, became basically, um,

Speaker 3:          19:38          you know, I dystopian nightmare. All the, all the regular people were stuck on earth. And that's, you're seeing that already of course with wealthy people moving to gated communities and so forth and elite cds like San Francisco where things are becoming so unequal. And I really worry that that's the kind of future we could have if we don't adapt to this where you literally have got a small number of incredibly wealthy people that are benefiting from this technology and are maybe using the technology to protect themselves from the masses. Right. And everyone else is literally left behind.

Speaker 2:          20:09          So that that's sort of the ultimate irony of what's happening in Silicon Valley, right? I mean, you just said it. It's like San Francisco, they've got all these great minds are up there creating all of this incredible technology, absurd amounts of wealth. And then if you go out on the streets of San Francisco, the amount of homelessness is, is through the roof. I mean crime and everything else. San Francisco

Speaker 3:          20:26          and the bay area is ground zero for this technological revolution. And then right in their backyard, you see, you see the inequality, right? You see what's happening and um, this is something that's going to scale out, right. Um, to everywhere basically. Um, so we really need to get control of that. Um, and if we don't, it's, I think it's got a tear off society apart, right? It's going to ultimately lead to some real problems in the United States and in other countries that are less stable, that have less, you know, solid institutions that we have. It, it could be even worse. You're going to see governments fall and things like that in some countries as a result of this. I think so it's really something to be concerned about. We need, we need to have some sort of a plan.

Speaker 2:          21:12          Yeah. Well that's what everyone's trying to figure out right now is, is what is that plan

Speaker 3:          21:17          exactly. And I think this is one of the biggest challenges we're gonna face in the future. You know, this is, AI is going to be one of the primary forces that shapes the future is going to be incredibly disruptive. And of course it's going to happen in parallel with other things. Climate Change, um, geo politically, the rise of China, um, migration, right? These are all huge things that are happening already. All these things are coming at us in parallel. They're going to hit all together and I really worry about kind of a perfect storm. So we really need to begin to get a handle on all that.

Speaker 2:          21:49          How does the information war factor into all of this? You know, one of the things that people that watch this show are always taught, you know, everyone's talking about fake news all the time or that were just being handed things. You know, the algorithm pushes us stories that are favorable maybe to one side of the political aisle or something like that. And then we're all going to also siphon off into our own informational realities basically, and sort of will live in the same physical world, but digitally we're

Speaker 3:          22:14          going to just accept different truths. Exactly. And that, that, that's what makes it even more scary as to all of this disruption is coming at us at a time when we are just incredibly polarized, where to some extent we're living in different universes. Our ability to even talk to each other, it seems to be limited. How are we going to, you know, address these issues? How are we going to respond to these incredibly disruptive forces when we can't even sit down and have a conversation and agree on the same facts? Right. Um, that's a real problem. And, and there is evidence that it's getting worse and worse. That's largely the, the, and that's why I'm doing this. This is, so this is my little firewall right here. Exactly. I hope that there can be more of this because we really need everyone and that includes people on the left and people on the right to be able to talk to each other about this because otherwise we're going to have what we have now we've got to, we've got a congress that literally can do nothing. And I'm concerned no matter who wins in 2020 the presidency, what, you know, it's likely that the congress will still be divided, right? The same political polarization and social polarization and social media polarization will be there. So how are we going to address these kinds of,

Speaker 2:          23:25          so that gets to what you were talking about earlier, that you'd need something sort of separate from the government in a way to sort of be, if, if something could oversee some of our ability to deal with this technology, it almost, in a way it can't be politicized because of the way our system is,

Speaker 3:          23:41          right? I mean I, it's stagnant in terms of having a basic, I think there are good reasons to put that in the hands of a separate institution. And the Federal Reserve would be a good example of that. You've seen the Federal Reserve, which controls interest rates, right? Is relatively independent right now. Um, although, you know, Trump has tried to, to, to play around with that. But you know, if it weren't for the fact that we had an independent Federal Reserve, I think we would be in much bigger trouble now than we are. Um, and so there, there is an argument for maybe taking some essential functions of government away from the political process and having a kind of a technocratic approach to that.

Speaker 2:          24:20          All right, so my last question will be a two parter. Paint me a future if we get some of this under control and we deal with this technology maturely and properly give me sort of where, what do we look like in about 10 years? And then if we lose control and we don't do the proper things and don't have the proper firewalls, what does the future look like? In 10 years?

Speaker 3:          24:40          Well, I think that, you know, maybe looking even beyond 10 years, we know one that, how far do you want to go that say 15, 20 years ago. But I think that within that kind of a timeframe, there's going to be an unambiguous impact on, on the job market. So if we don't get control of this, we will see unemployment, at least among some workers. We will see greatly increased inequality even beyond what we see now. We will see more and more anger, um, more people left behind, possibly even social unrest. As a result of that, we will see the rise of people like Trump, which you might characterize as kind of a demagogue, someone that preys on the fear that people have, right? Um, maybe, you know, points to two immigrants as opposed to technology as being the primary cause of this and so forth. Right?

Speaker 2:          25:29          Those other people are stealing your,

Speaker 3:          25:31          it's a lot easier to always point to another human being than it is to point to an intangible force like technology. So we could have a future where, you know, everything is just very, very ugly. On a lot of people are, are really struggling, uh, me on that future. And, and the other part of that though is that there's also an economic aspect to that, that, that as people are unemployed or have lower incomes, they have less money to spend right now they're not driving the economy, so the whole economy suffers. So we could have also a financial crisis, the recession, um, perhaps people can't pay their debts and we get into a situation like we had in 2008. Right? So that's, that's sort of the worst case scenario. The best case scenario is that we find a way to adapt to this. Maybe it was something like a basic income.

Speaker 3:          26:16          So we addressed this issue of people not having jobs or not having an adequate incomes. So they still have money to spend. They go out and they spend their money in the economy. There are all kinds of new products and services and exciting things for people to spend money on. There is incredible opportunity for entrepreneurs, for people like Elon Musk's or the next Steve and Steve jobs to create things. Um, based on this new technology and people have income that they need to, to buy these products. Um, things do, um, in large measure get less expensive. So people you know, have, have greater purchasing power. Um, we have enormous breakthroughs in science and medicine. Um, we all live longer and healthier lives. So there are enormous benefits to artificial intelligence. It's going to become the primary tool that's used in scientific research and solving problems like climate change, developing new forms of clean energy, you know, medical breakthroughs, all of that.

Speaker 3:          27:17          The key thing is that we want to make sure that we get that stuff and then we get it for everyone. In other words, we want to be able to leverage it on behalf of everyone rather than just a few people. And I think that if we can find a way to navigate through this, that it's incredibly, um, optimistic. And where it kind of ends up in the far future is maybe something like Star Trek, right? Where, where you've got what has been called kind of the post scarcity economy, an economy of abundance where you know people don't have to worry about the basics of life anymore than people you know, focused on other things, right? I mean, and star track, you've got to materialize or right you, everything is basically free. People don't have to work a nine to five job to survive. They are out traveling the universe or whatever. I'm doing things that are genuinely meaningful to them and I think that's sort of the vision that we should have as we anticipate the development of these technologies. But in order to get there, we've got

Speaker 2:          28:14          to be honest, I'm serious lifter

Speaker 3:          28:17          what the implications of this is. We need to have an honest discussion and come up, I think ultimately with some real policies to address the risks and the downsides that are going to come with this progress. So we shall see.

Speaker 2:          28:30          I hope these are the three words that'll start to take us out of this one. We should probably do this every year. You want to do one of these every year and just pick up on all the incremental progress. And then I suppose if everything you're saying is right, one year we're going to do it and everything will look so absolutely different. We won't even be able to look back and make any sense of what we're talking about. Once we get past that singularity, you're going to be different. I look forward to it and for more on Martin, you can follow him on Twitter at m forward future.