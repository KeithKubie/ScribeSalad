WEBVTT
Kind: captions
Language: en

00:00:00.326 --> 00:00:02.160
Alright, Charles [LAUGH], let's, let's get back to

00:00:02.160 --> 00:00:04.540
the task at hand, and you thought maybe you

00:00:04.540 --> 00:00:08.300
had some ideas. So, tell, tell me, how can we define U and pi in terms of Q?

00:00:08.300 --> 00:00:11.283
&gt;&gt; Okay. Well, so I guess the first thing

00:00:11.283 --> 00:00:14.000
to observe is that U is a value. Right?

00:00:14.000 --> 00:00:18.510
&gt;&gt; U are a value. Okay. Yes. U is a value. That's correct.

00:00:18.510 --> 00:00:20.030
&gt;&gt; [LAUGH] U returns a number. A

00:00:20.030 --> 00:00:23.610
scalar, in particular, where pi returns an action.

00:00:23.610 --> 00:00:25.260
&gt;&gt; Yeah, U of s returns a scalar,

00:00:25.260 --> 00:00:27.060
that's right, and pi of s returns an action, very good.

00:00:27.060 --> 00:00:29.400
&gt;&gt; Right, okay. And, you know, we have the

00:00:29.400 --> 00:00:31.790
definition for U up there near the top, and it's

00:00:31.790 --> 00:00:34.100
just the reward and then sort of behaving optimally

00:00:34.100 --> 00:00:37.820
thereafter, where Q is a reward and then taking a

00:00:37.820 --> 00:00:42.350
specific action, and and behaving optimally there after. So,

00:00:42.350 --> 00:00:44.550
we can sort of turn Q into U, if we

00:00:44.550 --> 00:00:47.250
always pick the best action. And we know the best

00:00:47.250 --> 00:00:51.140
action is, it's the one that, you know maximizes the

00:00:51.140 --> 00:00:54.300
value that you're going to get from that point on. So, I would say that

00:00:54.300 --> 00:01:00.350
U of s could be defined as Max over a of Q, s comma a.

00:01:00.350 --> 00:01:01.900
&gt;&gt; Simplicity itself.

00:01:01.900 --> 00:01:02.772
&gt;&gt; Mm-hm.

00:01:02.772 --> 00:01:04.780
&gt;&gt; Nicely done. I have no idea how we're going to grade

00:01:04.780 --> 00:01:08.240
that automatically, because it's hard to type a's under other a's. But,

00:01:08.240 --> 00:01:11.340
yeah, that's exactly right, that we have the maximization over all possible

00:01:11.340 --> 00:01:14.930
actions of the Q value in that state. Like, that's just great!

00:01:14.930 --> 00:01:15.483
&gt;&gt; Mm-hm

00:01:15.483 --> 00:01:16.370
&gt;&gt; How about

00:01:16.370 --> 00:01:16.960
the policy?

00:01:16.960 --> 00:01:20.051
&gt;&gt; So the policy will look exactly the same, the, the,

00:01:20.051 --> 00:01:25.150
the policy that you want to follow anyway is the one that...

00:01:25.150 --> 00:01:29.450
Maximizes your value going forward except the differences. It's returning an

00:01:29.450 --> 00:01:32.320
A and not an actual value, so it should be argmax.

00:01:32.320 --> 00:01:34.260
&gt;&gt; Oh, So it's not exactly the same.

00:01:34.260 --> 00:01:34.710
&gt;&gt; Right.

00:01:34.710 --> 00:01:36.090
&gt;&gt; It's almost exactly the same.

00:01:36.090 --> 00:01:37.838
&gt;&gt; Rrâ€¦ max

00:01:37.838 --> 00:01:39.238
&gt;&gt; Right, so this is, it's just so

00:01:39.238 --> 00:01:41.664
trivial, so, so Q gives us everything we need.

00:01:41.664 --> 00:01:44.460
If only we had a good way of finding Q.

00:01:44.460 --> 00:01:47.870
&gt;&gt; Yeah, if only, if only we could find a Q, a Q for you.

00:01:47.870 --> 00:01:52.040
&gt;&gt; [LAUGH] and that's going to be the essense of Q LEARNING.

00:01:52.040 --> 00:01:52.980
&gt;&gt; Oh, well done.

