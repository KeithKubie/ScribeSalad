WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.444
[MUSIC PLAYING]

00:00:08.453 --> 00:00:09.370
RYAN SEPASSI: Hi, all.

00:00:09.370 --> 00:00:11.810
I'm here to tell you about
TensorFlow data sets.

00:00:11.810 --> 00:00:16.070
It's a new way to have a
collection of public research

00:00:16.070 --> 00:00:16.900
data sets.

00:00:16.900 --> 00:00:18.560
It's on GitHub
and PyPI, and it's

00:00:18.560 --> 00:00:20.330
ready to use with TensorFlow.

00:00:20.330 --> 00:00:21.620
First, a little bit of setup.

00:00:21.620 --> 00:00:23.120
Of course, this
is the partnership

00:00:23.120 --> 00:00:25.310
that we all care
about-- data and models.

00:00:25.310 --> 00:00:28.670
And TensorFlow 2 makes
it look really nice.

00:00:28.670 --> 00:00:30.710
Set up your model,
your optimizer,

00:00:30.710 --> 00:00:32.900
iterate through
your training data.

00:00:32.900 --> 00:00:36.230
You get predictions, your
loss, gradients, update.

00:00:36.230 --> 00:00:38.780
It all looks fantastic.

00:00:38.780 --> 00:00:42.950
And they're good
together-- data and models.

00:00:42.950 --> 00:00:46.770
And really, I just want to
focus in on this one piece.

00:00:46.770 --> 00:00:51.240
This one line-- iterating
through your training data set.

00:00:51.240 --> 00:00:54.540
In TF 2, with the
TF data API, you

00:00:54.540 --> 00:00:58.410
have this really
expressive, simple way

00:00:58.410 --> 00:01:02.530
of expressing high performance
but flexible input pipelines.

00:01:02.530 --> 00:01:05.580
So we might start
with some TF data set,

00:01:05.580 --> 00:01:09.150
and we can map, and shuffle,
and batch, and repeat--

00:01:09.150 --> 00:01:11.370
build really complex pipelines.

00:01:11.370 --> 00:01:14.320
And we know that this will
be really performance.

00:01:14.320 --> 00:01:16.260
So this map call,
for example, might

00:01:16.260 --> 00:01:19.140
be doing some complex
image pre-processing

00:01:19.140 --> 00:01:22.740
on each record in the data set,
and it's happening in parallel.

00:01:22.740 --> 00:01:25.053
You can see here,
num_parallel_calls=10.

00:01:25.053 --> 00:01:27.220
So TensorFlow will be
spinning up a bunch of threads

00:01:27.220 --> 00:01:30.330
so that this mapping is
really high performance.

00:01:30.330 --> 00:01:32.850
And at the end, you also have
this ability to prefetch.

00:01:32.850 --> 00:01:36.150
So as your tight model
training loop is going,

00:01:36.150 --> 00:01:38.100
you can actually be
preparing batches,

00:01:38.100 --> 00:01:41.530
ready to feed into
the next iteration.

00:01:41.530 --> 00:01:43.120
Now, this is fantastic.

00:01:43.120 --> 00:01:45.460
But unfortunately,
the source data

00:01:45.460 --> 00:01:47.560
is a little bit out of step.

00:01:47.560 --> 00:01:51.010
And so what we want is we want
this amazing, tight iteration

00:01:51.010 --> 00:01:54.040
for inputs and targets
in train data set.

00:01:54.040 --> 00:01:56.620
But, of course, what's hidden
from there is, how did you

00:01:56.620 --> 00:01:58.630
get the data into a format
that you can actually

00:01:58.630 --> 00:02:01.150
feed into your model
in the first place?

00:02:01.150 --> 00:02:05.560
Every single source data set,
whether it's MNIST or IMDB

00:02:05.560 --> 00:02:08.577
sentiment classification, you
have the IMDB reviews data set,

00:02:08.577 --> 00:02:09.910
it's all a little bit different.

00:02:09.910 --> 00:02:11.577
And of course, this
makes perfect sense.

00:02:11.577 --> 00:02:13.960
When you're distributing
data at rest,

00:02:13.960 --> 00:02:17.260
there's some natural format that
you'd like to distribute it in.

00:02:17.260 --> 00:02:20.087
But we're not really
interested in data at rest.

00:02:20.087 --> 00:02:22.170
We're interested in having
data in a format that's

00:02:22.170 --> 00:02:27.190
sort of ready to move
into a TF data pipeline.

00:02:27.190 --> 00:02:32.720
And what we really want is
something that's good together.

00:02:32.720 --> 00:02:34.160
We want these two pups--

00:02:34.160 --> 00:02:38.300
data and models-- to
really be simpatico.

00:02:38.300 --> 00:02:41.660
And I'm happy to tell you that
TensorFlow Data sets is really

00:02:41.660 --> 00:02:44.360
just about this one
piece of getting data

00:02:44.360 --> 00:02:48.410
into a format that's ready to
move into a TF data pipeline.

00:02:48.410 --> 00:02:50.210
And so here's
TensorFlow data sets.

00:02:50.210 --> 00:02:55.280
You import TensorFlow data sets
and this load API for MNIST.

00:02:55.280 --> 00:02:57.140
Because that's our
canonical first example,

00:02:57.140 --> 00:03:00.290
always and forever, I hope.

00:03:00.290 --> 00:03:03.470
We're grabbing the split
train, so the training split

00:03:03.470 --> 00:03:04.940
of the data set.

00:03:04.940 --> 00:03:07.010
We're asking for
supervised tuples.

00:03:07.010 --> 00:03:09.150
So in this case, MNIST
is a supervised data set.

00:03:09.150 --> 00:03:10.310
It has inputs and targets.

00:03:10.310 --> 00:03:13.010
And so we'll get inputs
and target tuples.

00:03:13.010 --> 00:03:17.550
We get a TF data data set
right back from the load call,

00:03:17.550 --> 00:03:19.390
and we can build a
complex pipeline.

00:03:19.390 --> 00:03:22.668
In this case, we're adding some
shuffling and some batching,

00:03:22.668 --> 00:03:23.460
and then we're off.

00:03:23.460 --> 00:03:25.650
Now we can iterate
through our inputs

00:03:25.650 --> 00:03:28.890
and do all the modeling
things that we need.

00:03:28.890 --> 00:03:32.737
And load actually hides
a lot of the complexity.

00:03:32.737 --> 00:03:34.320
And so what's happening
here with load

00:03:34.320 --> 00:03:37.500
is that MNIST is actually
being fetched from the source.

00:03:37.500 --> 00:03:38.745
It's being pre-processed.

00:03:38.745 --> 00:03:40.980
It's being put into
a standard format.

00:03:40.980 --> 00:03:43.650
It's being documented with
statistics and everything,

00:03:43.650 --> 00:03:46.170
which I'll show you in a second.

00:03:46.170 --> 00:03:48.540
And then you're building
sort of the front end.

00:03:48.540 --> 00:03:51.840
So you have pre-processed
data on disk, ready to move.

00:03:51.840 --> 00:03:53.250
Typically you'd
only do this once

00:03:53.250 --> 00:03:56.630
and, of course, TFDS will cache
all those results for you.

00:03:56.630 --> 00:04:00.370
And then you're getting a
TF data pipeline out of it.

00:04:00.370 --> 00:04:02.100
And we don't just have MNIST.

00:04:02.100 --> 00:04:05.220
So ImageNet-- complex data set.

00:04:05.220 --> 00:04:07.200
Most places, when you
go look at the tutorial

00:04:07.200 --> 00:04:09.960
for how do I use ImageNet,
first step is, like,

00:04:09.960 --> 00:04:14.610
run this batch script, muck with
the files over here, make sure

00:04:14.610 --> 00:04:16.510
you filter out these records--

00:04:16.510 --> 00:04:17.850
things like that.

00:04:17.850 --> 00:04:20.820
And of course, this is
all taken care of for you.

00:04:20.820 --> 00:04:28.020
Now, ImageNet is gated with
a password and username.

00:04:28.020 --> 00:04:30.765
And so currently,
TensorFlow Data

00:04:30.765 --> 00:04:32.650
sets just asks you
to download the data,

00:04:32.650 --> 00:04:35.647
put it in a certain place,
and TensorFlow Data sets

00:04:35.647 --> 00:04:36.480
takes it from there.

00:04:36.480 --> 00:04:38.400
And we're actually
working with Kaggle

00:04:38.400 --> 00:04:43.530
to get ImageNet able to be
auto-downloaded without having

00:04:43.530 --> 00:04:47.210
any user intervention at all.

00:04:47.210 --> 00:04:48.870
We go beyond images.

00:04:48.870 --> 00:04:50.020
So we have text data sets.

00:04:50.020 --> 00:04:52.830
So in this case, we
have IMDB Reviews.

00:04:52.830 --> 00:04:55.150
Note, again, that the
API doesn't change.

00:04:55.150 --> 00:04:58.292
This is just a tfds.load
call to get a data set.

00:04:58.292 --> 00:05:00.250
So in this case, we'd be
getting the plain text

00:05:00.250 --> 00:05:01.950
IMDB Reviews data set.

00:05:01.950 --> 00:05:04.980
But of course, text is a
notoriously difficult format

00:05:04.980 --> 00:05:05.852
to work with.

00:05:05.852 --> 00:05:07.560
When you actually are
interested in doing

00:05:07.560 --> 00:05:10.020
machine learning on it,
you have to do encodings.

00:05:10.020 --> 00:05:12.930
And so we're actually shipping
with three really powerful

00:05:12.930 --> 00:05:13.590
encodings.

00:05:13.590 --> 00:05:18.060
One is byte-level, so you
have character-level encoding,

00:05:18.060 --> 00:05:19.890
and another is subwords.

00:05:19.890 --> 00:05:22.350
So we have a really great
subword text encoder

00:05:22.350 --> 00:05:25.390
that was originally developed
in the Tensor2Tensor Library.

00:05:25.390 --> 00:05:28.290
And so in this case, we'd be
getting IMDB reviews already

00:05:28.290 --> 00:05:32.400
encoded into integers
using a subword vocabulary

00:05:32.400 --> 00:05:37.330
with about 8,000 tokens.

00:05:37.330 --> 00:05:40.000
So each of these data
sets is packaged together

00:05:40.000 --> 00:05:41.283
as a data set builder.

00:05:41.283 --> 00:05:42.700
So if you wanted
to add a data set

00:05:42.700 --> 00:05:45.670
to TensorFlow data sets, which
I hope every single one of you

00:05:45.670 --> 00:05:46.230
does.

00:05:46.230 --> 00:05:48.063
I think there should
be about 1,000 new data

00:05:48.063 --> 00:05:49.450
sets within a week.

00:05:49.450 --> 00:05:51.460
That's my hope.

00:05:51.460 --> 00:05:53.990
So Data set Builder is
actually really simple.

00:05:53.990 --> 00:05:55.750
It does exactly
those three things

00:05:55.750 --> 00:05:57.970
that load was sort of hiding.

00:05:57.970 --> 00:06:00.880
The first, it has a method
called download_and_prepare,

00:06:00.880 --> 00:06:03.880
which takes source data
out on the open web

00:06:03.880 --> 00:06:07.850
or on local directory and
produces pre-processed files.

00:06:07.850 --> 00:06:09.550
So it takes data at
rest and it puts it

00:06:09.550 --> 00:06:12.100
into a format that's
data ready to move.

00:06:12.100 --> 00:06:13.870
The second is as_data set.

00:06:13.870 --> 00:06:17.740
As_data set takes those
pre-processed files on disk

00:06:17.740 --> 00:06:20.200
and produces a tf.data.data set.

00:06:20.200 --> 00:06:23.350
And the third is really
useful information-- metadata

00:06:23.350 --> 00:06:26.870
about the data set, which can be
programmatically really useful.

00:06:26.870 --> 00:06:31.280
So data set info is
the documenting object.

00:06:31.280 --> 00:06:34.755
So in this case, it has the
features in the data set.

00:06:34.755 --> 00:06:36.130
And each of the
features, you can

00:06:36.130 --> 00:06:39.620
see, documents the name,
the shape, and the type.

00:06:39.620 --> 00:06:41.350
So in this case,
the image for MNIST

00:06:41.350 --> 00:06:46.540
is 28 by 28 by 1, type uint8.

00:06:46.540 --> 00:06:49.390
The class labels, the
number of classes,

00:06:49.390 --> 00:06:51.700
is documented in the
class label feature.

00:06:51.700 --> 00:06:54.103
You also get statistics
on the data set itself.

00:06:54.103 --> 00:06:56.020
What are the splits that
are actually exposed,

00:06:56.020 --> 00:06:57.950
in this case, train and test?

00:06:57.950 --> 00:06:59.770
And how many records
are in each one?

00:06:59.770 --> 00:07:02.320
MNIST has 60,000 images
in the training set

00:07:02.320 --> 00:07:05.350
and 10,000 images
in the testing set.

00:07:05.350 --> 00:07:08.720
For supervised tasks, you
also get supervised keys.

00:07:08.720 --> 00:07:10.990
So if you wanted to be able
to programmatically grab

00:07:10.990 --> 00:07:14.050
a data set from TensorFlow
Data sets and sort of filter

00:07:14.050 --> 00:07:18.430
for all the ones that are
trivially supervised data sets,

00:07:18.430 --> 00:07:21.910
you can look for supervised
keys and pick out the features

00:07:21.910 --> 00:07:23.370
that you need.

00:07:23.370 --> 00:07:25.800
And for text, again,
one of these things

00:07:25.800 --> 00:07:30.150
that's a bit annoying to work
with often, so this is IMDB

00:07:30.150 --> 00:07:30.690
Reviews.

00:07:30.690 --> 00:07:34.110
We see that it has a label
feature and a text feature,

00:07:34.110 --> 00:07:37.320
and the text feature actually
contains the encoder.

00:07:37.320 --> 00:07:39.930
We see here that it's
a subword texting coder

00:07:39.930 --> 00:07:42.690
with a vocab size of 8,185.

00:07:42.690 --> 00:07:46.420
You get the same
statistics for the splits.

00:07:46.420 --> 00:07:47.990
We also support NumPy usage.

00:07:47.990 --> 00:07:49.510
Now, this is
TensorFlow Dev Summit,

00:07:49.510 --> 00:07:52.060
so maybe this isn't
so useful, but we

00:07:52.060 --> 00:07:55.090
wanted to make TensorFlow
Data sets really portable.

00:07:55.090 --> 00:07:58.810
And so you can actually just
call this tfds.as_numpy,

00:07:58.810 --> 00:08:00.760
and it hides all the
TensorFlow from you.

00:08:00.760 --> 00:08:02.290
And you pass it a
data set, it hands

00:08:02.290 --> 00:08:06.280
you back an Python generator
over NumPy arrays, which

00:08:06.280 --> 00:08:08.870
is great.

00:08:08.870 --> 00:08:11.760
So coming back to
our initial example,

00:08:11.760 --> 00:08:15.795
we had this beautiful
model and data.

00:08:15.795 --> 00:08:18.170
But of course, trained data
set, where did this come from

00:08:18.170 --> 00:08:20.510
and how did you
actually get the data?

00:08:20.510 --> 00:08:23.110
And with TensorFlow data
sets, simple change,

00:08:23.110 --> 00:08:26.480
and you actually have something
that works end to end, out

00:08:26.480 --> 00:08:30.860
of the box, which is something
we're really happy about,

00:08:30.860 --> 00:08:32.539
and we hope you are too.

00:08:32.539 --> 00:08:35.659
You can come find us on
PyPI, the TensorFlow website,

00:08:35.659 --> 00:08:37.100
and GitHub.

00:08:37.100 --> 00:08:39.150
We have over 30
data sets already,

00:08:39.150 --> 00:08:41.179
and there's more
being added every day.

00:08:41.179 --> 00:08:43.250
The community on
GitHub has actually

00:08:43.250 --> 00:08:45.740
gotten surprisingly active
surprisingly quickly,

00:08:45.740 --> 00:08:47.600
so come join us.

00:08:47.600 --> 00:08:50.300
We absolutely could
use your data.

00:08:50.300 --> 00:08:52.510
If you're distributing
some data set

00:08:52.510 --> 00:08:54.560
and you'd like to make
your data set famous,

00:08:54.560 --> 00:08:56.900
please add it to
TensorFlow data sets.

00:08:56.900 --> 00:09:00.800
It's a great way to make it
accessible to a huge community.

00:09:00.800 --> 00:09:02.210
And come help us on GitHub.

00:09:02.210 --> 00:09:03.980
There are data set requests.

00:09:03.980 --> 00:09:06.410
You can help implement
a data set that's

00:09:06.410 --> 00:09:10.340
been requested many times
or help us just develop.

00:09:10.340 --> 00:09:12.020
We developed fully
out in the open.

00:09:12.020 --> 00:09:15.470
All of our internal changes
are actually external changes.

00:09:15.470 --> 00:09:18.290
We just develop
straight out on GitHub.

00:09:18.290 --> 00:09:20.220
So thanks very much.

00:09:20.220 --> 00:09:24.050
Hopefully, you guys find this
useful and come help us out.

00:09:24.050 --> 00:09:27.400
[MUSIC PLAYING]

