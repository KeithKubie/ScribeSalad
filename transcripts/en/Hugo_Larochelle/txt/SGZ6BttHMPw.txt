Speaker 1:          00:00          In this video, we'll introduce the concept of an artificial neuron, which is going to be the basic building block we'll be using to construct complicated neural networks. So in artificial neuron is simply computational unit, which will make a particular computation based on other units it's connected to. So, uh, in the case of a single artificial neuron like this one here, uh, it's going to be connected directly on an input description of some object we want to extract information from. So this input description, we'll call it x. So here, uh, both ex is a sense essentially a vector which contains the scaler x one up to x d where these the size of the vector. Okay. So notice that I'll be using bold as uh, when I'm denoting vectors and matrices as well. And uh, whenever it's not involved and it corresponds to a scalar value.

Speaker 1:          01:01          Okay. So x is the element of Vector X. And so artificial neuron is going to read the information from this x vector and perform a particular computation, which will dictate its value, the value of the neuron. And this valley will essentially indicate whether some particular feature or information about the subject that we're manipulating is present or not in the object, uh, described by the vector x. Now the computation it will perform, can be decomposed into two steps. There's going to be first a, uh, the computation of a pre activation of the neuron. Sometimes when you look at the literature, you see the expression instead input activation. Uh, but I'll avoid using input activation because four x we often use also the, uh, for the word input to describe it. So instead, I'll, I'll talk about this as the pre activation and the pre activation, which I'll note a of x for a given neuron is simply going to be some scale or B, which we're going to call a refer to as the bias, the neuron bias plus the, uh, product between a weight factor w with some, uh, the input vector x.

Speaker 1:          02:24          So we can write it into vector form here or we can write it into a more scalar form where we have B plus the summation over all ISO, all indices within the vector of the IMF element of w Times d a l I f element of X. And then from this pre activation there neuron Luke compute its activation, which is sometimes referred to as the output activation of the neuron by simply taking the pre activation and passing it through a, uh, activation function. So this z function here is what we're going to call activation function and are going to be different choices of potential activation functions we will be able to use. So the UPC code activation, uh, I will often know that that's h of x and that will be the activation. If I don't say output, uh, if I just say activation, I need the output activation of a given neuron.

Speaker 1:          03:24          And so if we just put the expression for Amx, uh, explicitly, it will be the activation function applied on this linear transformation of the input vector, a made of the elements XII. So w we will refer to it as the, uh, a vector of connection weights. And that's because, uh, as in this visualization here, we can think of the elements of w as the strength of the connection between the neurons to which are a neuron is connected to, in this case these neurons would be input neurons. They would be, uh, neurons that take the value of each of the element in the input vector. So w will be the vector of connection weights be will be a bias. It's a bias because it's, uh, if we have no input, uh, B would be the pre activation. So by observing a particular input, then we are essentially going away from this initial value be, uh, uh, of the pre activation of the neuron. And then as I said, gee, will be the activation function.

Speaker 1:          04:36          So there is a two d visualization of the activation of a neuron. So imagine we have a vector made of two element x one and x two, and then we'll note as why. So I could have written instead h of x here, uh, as the, uh, uh, output a activation of the neuron. Okay. So first thing we see here is that on this axis here, uh, we get values dialogue between minus one and one. And that will be because in this specific example, the range of values we can get once we pass the pre activation through the activation function is a number between minus one and one. And now what we see is that for a different values of x, we get a different output. And specifically for, uh, all the values that Lee here we get an output for the neuron and activation of minus one, uh, oh, very close to minus one.

Speaker 1:          05:34          And for all values of x, here we get a value of one. So in this particular case, we have a not to official neuron that detects whether, uh, some given input point x one x two belongs to this part of the space or this part of this space. So he can think of it as a, of the artificial neuron as a binary classifier that separates points in one region. And some other region. For now, we won't discuss how we actually find the parameters of the neuron that determine what regions it's actually separating. So for now, we'll just assume that someone has given us the value of the wave vector w and the bias vector B, which determines the shape of dysfunction. Uh, and we'll see how we can actually have neurons that will train to, uh, discover good values of these parameters. So for now, we'll just assume that someone has given us these values.

Speaker 1:          06:31          Also, geometrically, it turns out that the vector w we'll actually be perpendicular with respect to the, uh, hybrid plane in this space that separates these two regions, that the neuron, uh, is distinguishing is separating. So this vector w determines essentially the orientation of this ridge between the two parts of this space that, uh, the neuron, the separating, and then the bias be, we'll essentially determine along this direction where the rich will be. So if the bias is, uh, uh, if the biases large, so when the bias is increasing, then the ridge will be moving in the opposite direction of w. So this would correspond to a, uh, perhaps negative value of beam. And if we had the, uh, more strongly positive value of B than the ridge would be a moving in this direction. Okay. So I want to exactly explain why this is, but you can sit down and try to figure out why it is that w is perpendicular to this ridge. And why is it that when bees decree is increasing, then the rich would be moving, uh, this way. So this is the description of the computation that the artificial neuron is performing.