WEBVTT
Kind: captions
Language: en

00:00:06.390 --> 00:00:07.930
MICHAEL HANDLER: All right,
good afternoon everybody.

00:00:07.930 --> 00:00:09.920
I'm ready to get started.

00:00:09.920 --> 00:00:11.970
So this is a pretty
good turnout.

00:00:11.970 --> 00:00:15.210
This is a little better than I
was expecting for a talk where

00:00:15.210 --> 00:00:18.090
I'm not going to actually show
you any code or anything new

00:00:18.090 --> 00:00:19.920
at the end of the second
day of the conference.

00:00:19.920 --> 00:00:22.260
If I knew it was going to be
this good, I wouldn't have

00:00:22.260 --> 00:00:23.810
bribed all of my coworkers
to show up.

00:00:23.810 --> 00:00:27.055
But thank you very
much for coming.

00:00:27.055 --> 00:00:32.130
So this in Life in App Engine
Production for Google IO 2011.

00:00:32.130 --> 00:00:36.680
And we've got some hash tags up
here suggested if you want

00:00:36.680 --> 00:00:39.220
to Twitter during the talk.

00:00:39.220 --> 00:00:41.440
And we've got a very convenient
and memorable

00:00:41.440 --> 00:00:43.980
feedback URL that you can visit
if you'd like to give us

00:00:43.980 --> 00:00:45.450
feedback during the
talk or after.

00:00:45.450 --> 00:00:49.230
And we'd very much appreciate
if you do that.

00:00:49.230 --> 00:00:50.620
So I'm ready.

00:00:50.620 --> 00:00:52.500
So let's get started.

00:00:52.500 --> 00:00:55.010
So who am I?

00:00:55.010 --> 00:00:56.090
I'm Michael Handler.

00:00:56.090 --> 00:00:57.710
I'll be your cruise director
for this talk.

00:00:57.710 --> 00:01:00.540
I'm the Tech Lead for the
App Engine SRE team.

00:01:00.540 --> 00:01:02.510
I'm based out of
San Francisco.

00:01:02.510 --> 00:01:06.400
I'll be co-presenting with my
first mate, Alan Green, also

00:01:06.400 --> 00:01:08.210
from the App Engine SRE team.

00:01:08.210 --> 00:01:09.570
He's based out of the
Sydney office.

00:01:09.570 --> 00:01:12.090
And we've got the hash tags and
the feedback URL up there

00:01:12.090 --> 00:01:17.220
again, just in case you missed
it in the last five minutes.

00:01:17.220 --> 00:01:19.080
So everybody says we should
have a table of contents.

00:01:19.080 --> 00:01:20.870
So I'm going to get through
this as quickly as I can.

00:01:20.870 --> 00:01:24.130
So who am I, what do I do,
what do I do all day?

00:01:24.130 --> 00:01:25.660
How does that affect
App Engine?

00:01:25.660 --> 00:01:27.790
What has happened to App Engine
that we expected?

00:01:27.790 --> 00:01:30.030
What's happened to App Engine
that we didn't expect?

00:01:30.030 --> 00:01:32.360
What do we do to make App Engine
more reliable when

00:01:32.360 --> 00:01:34.780
those things happen to us, both
the stuff we expected and

00:01:34.780 --> 00:01:36.110
the stuff we didn't expect?

00:01:36.110 --> 00:01:38.670
And based on all of that, if
you're building a production

00:01:38.670 --> 00:01:41.470
system on App Engine or off site
of App Engine, what can

00:01:41.470 --> 00:01:45.820
you do to make your application
more reliable?

00:01:45.820 --> 00:01:48.280
And then, we've got a demo of
some monitoring technology

00:01:48.280 --> 00:01:50.080
that we're really happy about
and that we're entering

00:01:50.080 --> 00:01:51.630
Trusted Tester with.

00:01:51.630 --> 00:01:54.430
So I've said SRE twice and
I haven't defined it.

00:01:54.430 --> 00:01:56.220
So I should probably fix that.

00:01:56.220 --> 00:01:59.300
So SRE is Site Reliability
Engineer.

00:01:59.300 --> 00:02:02.920
So this is a program that
started in Google some number

00:02:02.920 --> 00:02:06.090
of years ago, and here you
can see our mascot

00:02:06.090 --> 00:02:07.100
with our logo on it.

00:02:07.100 --> 00:02:09.449
We are concerned with the
lack of down time,

00:02:09.449 --> 00:02:11.520
or rather, up time.

00:02:11.520 --> 00:02:13.870
So Google Site Reliability
Engineering.

00:02:13.870 --> 00:02:16.510
So we have a very simple mission
which is, Google

00:02:16.510 --> 00:02:18.830
products need to be fast, they
need to be available.

00:02:18.830 --> 00:02:21.510
Whenever you put google.com into
your browser, whenever

00:02:21.510 --> 00:02:25.000
your website requests content
ads or whatever, that has to

00:02:25.000 --> 00:02:28.370
happen, it has to happen all of
the time, 24 hours a day,

00:02:28.370 --> 00:02:31.910
365 days a year, or 366,
just like that.

00:02:31.910 --> 00:02:34.600
As fast as possible, so that
the users are happy.

00:02:37.815 --> 00:02:42.750
So what we end up doing doesn't
show up in new product

00:02:42.750 --> 00:02:46.590
releases, new SDKs, change
lists, et cetera.

00:02:46.590 --> 00:02:49.010
A lot of the work that
we do is invisible.

00:02:49.010 --> 00:02:51.650
And we wanted to say, we want
to get out there and let

00:02:51.650 --> 00:02:54.950
people know that Google has
SRE teams because we want

00:02:54.950 --> 00:02:57.400
people to know that we
exist because we do.

00:02:57.400 --> 00:03:00.150
And for you to understand the
kind of work that we're doing,

00:03:00.150 --> 00:03:03.620
both for you on App Engine and
for all other Google products.

00:03:03.620 --> 00:03:06.360
So what happens when Google
products launch is that the

00:03:06.360 --> 00:03:07.190
developers run them.

00:03:07.190 --> 00:03:09.100
So they're spending all of their
time getting the product

00:03:09.100 --> 00:03:11.950
stable and directing traffic
to it and saying, OK, let's

00:03:11.950 --> 00:03:13.410
encourage people to use it.

00:03:13.410 --> 00:03:15.130
Let's get out there and
advocate for it.

00:03:15.130 --> 00:03:17.310
And then it gets a certain
level of stability and a

00:03:17.310 --> 00:03:18.310
certain level of growth.

00:03:18.310 --> 00:03:19.740
And we're saying, OK, wow.

00:03:19.740 --> 00:03:22.850
People actually like this
product and want to use it.

00:03:22.850 --> 00:03:25.450
And the developers are splitting
their time between

00:03:25.450 --> 00:03:28.760
well, let's make the product
as stable as we can while

00:03:28.760 --> 00:03:30.760
growing it and also adding
these new features.

00:03:30.760 --> 00:03:33.210
And this doesn't end up
working out really

00:03:33.210 --> 00:03:34.250
well that we found.

00:03:34.250 --> 00:03:37.110
It's better if you can let
people focus on adding new

00:03:37.110 --> 00:03:41.510
features and let other people
specialize on making it stable

00:03:41.510 --> 00:03:44.280
for the existing users
and enabling growth.

00:03:44.280 --> 00:03:46.150
And this is where the
SREs come in.

00:03:46.150 --> 00:03:50.440
So we originally had SRE teams
docketed to Search and Ads.

00:03:50.440 --> 00:03:52.330
You know, the original
core Google products.

00:03:52.330 --> 00:03:55.310
That tells you how old this
program goes back.

00:03:55.310 --> 00:03:58.545
Because the Search developers
and the Ads developers needed

00:03:58.545 --> 00:04:01.760
to spend all of their time
adding the features and they

00:04:01.760 --> 00:04:04.650
had lost scope of all the things
that we needed to do to

00:04:04.650 --> 00:04:06.660
deal with the volume of Google
search queries that were

00:04:06.660 --> 00:04:08.460
coming in every day.

00:04:08.460 --> 00:04:10.690
So we're talking about App
Engine reliability.

00:04:13.550 --> 00:04:16.920
And the SRE team, we're the
people that do that.

00:04:16.920 --> 00:04:18.600
So you may sort of think, well,
what does that day look

00:04:18.600 --> 00:04:19.140
like for us?

00:04:19.140 --> 00:04:21.725
Well, we're sitting around and
then the alarm goes off and we

00:04:21.725 --> 00:04:22.170
go, oh my God.

00:04:22.170 --> 00:04:23.030
Something's wrong.

00:04:23.030 --> 00:04:26.030
We all pile out, we fix the
problem, put the fire out,

00:04:26.030 --> 00:04:28.270
everybody's back and happy in
doing what they're doing, and

00:04:28.270 --> 00:04:31.700
we go back into the fire house
and we sit around, play cards,

00:04:31.700 --> 00:04:34.480
and wait till the alarm
goes off again.

00:04:34.480 --> 00:04:38.120
So if you've been App Engine
for a long time and we had

00:04:38.120 --> 00:04:40.510
some instability problems in
the past, you might think a

00:04:40.510 --> 00:04:41.300
little differently of us.

00:04:41.300 --> 00:04:43.290
You might say, like, wow,
there's a reliability team?

00:04:43.290 --> 00:04:45.840
They should take off their clown
noses and they should

00:04:45.840 --> 00:04:47.110
get back to work.

00:04:47.110 --> 00:04:49.140
But the reality of this is
actually a little different.

00:04:49.140 --> 00:04:51.710
Which is to say that we're
technical professionals, we're

00:04:51.710 --> 00:04:54.520
engineers, and we look at
this from an engineering

00:04:54.520 --> 00:04:55.170
perspective.

00:04:55.170 --> 00:04:58.210
We certainly respond with a
priority when there's a

00:04:58.210 --> 00:05:02.030
problem, but what we need to do
in addition to fixing it,

00:05:02.030 --> 00:05:03.410
is make sure that it doesn't
happen again.

00:05:03.410 --> 00:05:05.550
It's not just enough to
do crisis response.

00:05:05.550 --> 00:05:08.200
You actually have to do full
engineering to understand the

00:05:08.200 --> 00:05:10.450
problems to understand
what's going on.

00:05:10.450 --> 00:05:13.320
What we end up spending most of
our time doing is managing

00:05:13.320 --> 00:05:16.100
complexity.

00:05:16.100 --> 00:05:17.890
You've all designed and
architected systems, and

00:05:17.890 --> 00:05:19.280
you've put diagrams up there.

00:05:19.280 --> 00:05:21.140
And they're beautiful and simple
and you go, wow, this

00:05:21.140 --> 00:05:21.780
is fantastic.

00:05:21.780 --> 00:05:22.670
It's going to work.

00:05:22.670 --> 00:05:24.840
And that diagram never
survives first

00:05:24.840 --> 00:05:25.900
contact with the enemy.

00:05:25.900 --> 00:05:29.260
And that's even more true at
Google than it is outside at

00:05:29.260 --> 00:05:30.170
smaller scale.

00:05:30.170 --> 00:05:33.710
So all of these things, your
perfect plan gets overridden

00:05:33.710 --> 00:05:34.570
by reality.

00:05:34.570 --> 00:05:36.630
So where does that complexity
come from?

00:05:36.630 --> 00:05:39.490
Well, we all run on computers
and we all know that computers

00:05:39.490 --> 00:05:42.200
are very reliable things that
never do anything that is at

00:05:42.200 --> 00:05:44.020
all mysterious or that
you don't expect.

00:05:47.910 --> 00:05:50.150
That might actually be slightly
true when they're

00:05:50.150 --> 00:05:53.580
standing alone, but we put
them in datacenters and

00:05:53.580 --> 00:05:55.730
they're all interconnected
between each other.

00:05:55.730 --> 00:05:57.510
And a computer that was having
a great time just sort of

00:05:57.510 --> 00:05:59.950
doing it's thing, now it's
getting bothered by its

00:05:59.950 --> 00:06:02.360
neighbor down the row and
it's not as reliable

00:06:02.360 --> 00:06:02.810
as it used to be.

00:06:02.810 --> 00:06:05.270
It's asking it to do things,
crazy things are happening,

00:06:05.270 --> 00:06:07.380
traffic patterns are going on.

00:06:07.380 --> 00:06:09.280
It's just confusing.

00:06:09.280 --> 00:06:11.040
And then, we make the situation
worse because we've

00:06:11.040 --> 00:06:13.350
got datacenters every where
all around the world.

00:06:13.350 --> 00:06:15.810
And they're all interconnected
at high speed, and this means

00:06:15.810 --> 00:06:19.550
that not only can your nice,
reliable computer be bothered

00:06:19.550 --> 00:06:21.650
by a computer near it, it can
be bothered by a computer

00:06:21.650 --> 00:06:22.900
halfway around the world.

00:06:25.950 --> 00:06:27.790
We've got the infrastructure,
the support infrastructure.

00:06:27.790 --> 00:06:31.860
That if you built this global
network of applications,

00:06:31.860 --> 00:06:34.510
servers, datacenters, and all
of these other things, you

00:06:34.510 --> 00:06:35.820
need a bunch of support
infrastructure.

00:06:35.820 --> 00:06:37.610
I'm going to touch on exactly
what that looks like in a

00:06:37.610 --> 00:06:38.420
little bit.

00:06:38.420 --> 00:06:40.930
And then, you've got the traffic
coming from the users.

00:06:40.930 --> 00:06:44.270
More traffic then you could
ever expect that you could

00:06:44.270 --> 00:06:47.530
ever deal, that you ever thought
that you would deal.

00:06:47.530 --> 00:06:50.810
And it comes every day without
fail with peaks and valleys

00:06:50.810 --> 00:06:52.280
and you have to be ready.

00:06:52.280 --> 00:06:54.800
You know, whatever happens
with you today at peak

00:06:54.800 --> 00:06:57.900
traffic, if you started having
problems, it's going to happen

00:06:57.900 --> 00:06:59.830
to you just as badly tomorrow
and you need to be ready to

00:06:59.830 --> 00:07:02.140
deal with it.

00:07:02.140 --> 00:07:03.540
And we haven't gone into the

00:07:03.540 --> 00:07:05.050
circumstances of random chance.

00:07:05.050 --> 00:07:06.370
We have a lot of computers.

00:07:06.370 --> 00:07:07.530
We have a lot of datacenters.

00:07:07.530 --> 00:07:10.220
We have a lot of complicated
systems. They all interact

00:07:10.220 --> 00:07:13.240
with each other and each one of
those systems, every day,

00:07:13.240 --> 00:07:15.940
is a chance for the universe
to roll the dice and say,

00:07:15.940 --> 00:07:18.530
maybe something weird is
going to happen to you.

00:07:18.530 --> 00:07:21.890
So you have to be ready for
these kind of things as well.

00:07:21.890 --> 00:07:23.680
So I had a very simple balancing
act over here.

00:07:23.680 --> 00:07:27.070
So we've got stability on one
side and over here we've got

00:07:27.070 --> 00:07:28.690
the new features that the
developers are carefully

00:07:28.690 --> 00:07:29.290
stacking on.

00:07:29.290 --> 00:07:30.680
And we'll just balance
this out.

00:07:30.680 --> 00:07:31.600
It's not that simple.

00:07:31.600 --> 00:07:36.420
It's not that simple on the
reliability side at all.

00:07:36.420 --> 00:07:38.945
So this is a quote from Leslie
Lamport, a computer scientist

00:07:38.945 --> 00:07:40.420
who invented the Paxos

00:07:40.420 --> 00:07:42.010
distributed consensus algorithm.

00:07:42.010 --> 00:07:43.720
We use this inside
App Engine on the

00:07:43.720 --> 00:07:45.670
high replication datastore.

00:07:45.670 --> 00:07:48.350
"A distributed system is one
in which the failure of a

00:07:48.350 --> 00:07:50.930
computer you didn't even know
existed can render your own

00:07:50.930 --> 00:07:53.240
computer unusable." And that's
more or less, a description of

00:07:53.240 --> 00:07:55.080
my day every day.

00:07:55.080 --> 00:07:56.560
Which is to say that
there are--

00:07:56.560 --> 00:07:58.360
we've got the Rumsfeld
reference up here.

00:07:58.360 --> 00:07:59.980
There are unknown unknowns.

00:07:59.980 --> 00:08:02.230
I come to work and something
weird has happened.

00:08:02.230 --> 00:08:03.740
It might be from a computer
I've heard of.

00:08:03.740 --> 00:08:04.480
It might not.

00:08:04.480 --> 00:08:08.560
And if it's one I've heard of,
I probably know what to do.

00:08:08.560 --> 00:08:10.810
If it's one I've never heard
of before, I need to figure

00:08:10.810 --> 00:08:13.020
out quickly, restore everything
to service, and

00:08:13.020 --> 00:08:16.390
then go on making sure that
that doesn't happen again.

00:08:16.390 --> 00:08:18.490
So we're talking about
life in production.

00:08:18.490 --> 00:08:20.220
And that's the Google production
environment.

00:08:20.220 --> 00:08:21.990
And what does that look like?

00:08:21.990 --> 00:08:23.980
Where can I get one of these
fantastic hard hats and ear

00:08:23.980 --> 00:08:26.920
protection for my
day-to-day job?

00:08:26.920 --> 00:08:29.620
So App Engine's a cloud
computing environment.

00:08:29.620 --> 00:08:30.500
You all know this.

00:08:30.500 --> 00:08:32.460
And Google has a cloud computing
environment.

00:08:32.460 --> 00:08:35.090
We don't have a dedicated
datacenter for App Engine.

00:08:35.090 --> 00:08:37.510
We don't have a dedicated set
of machines for App Engine.

00:08:37.510 --> 00:08:39.840
We run App Engine inside the
same cloud computing

00:08:39.840 --> 00:08:42.650
environment that Google does.

00:08:42.650 --> 00:08:44.990
And so there's all of these
layers of infrastructure

00:08:44.990 --> 00:08:47.520
inside Google's cloud computing
environment.

00:08:47.520 --> 00:08:49.790
You've got the part the
environment itself, you've got

00:08:49.790 --> 00:08:52.130
lock services, storage
services, monitoring,

00:08:52.130 --> 00:08:54.170
configuration and reporting,
binary

00:08:54.170 --> 00:08:55.370
packages, and there's like--

00:08:55.370 --> 00:08:57.960
I ran out of space before I
could list all the ones that I

00:08:57.960 --> 00:08:59.070
thought were really important.

00:08:59.070 --> 00:09:02.020
I know there are computers and
they need power , networking,

00:09:02.020 --> 00:09:03.665
and cooling if they're going to
operate, if they're going

00:09:03.665 --> 00:09:06.670
to be at all useful to us.

00:09:06.670 --> 00:09:10.460
So it's interesting that we find
ourselves somewhat in the

00:09:10.460 --> 00:09:14.300
same situation that our
customers do in terms of

00:09:14.300 --> 00:09:14.960
running App Engine.

00:09:14.960 --> 00:09:18.410
So you come to us and say, well,
we want you to run this

00:09:18.410 --> 00:09:19.320
application for us.

00:09:19.320 --> 00:09:20.770
And we say, that's fantastic.

00:09:20.770 --> 00:09:22.380
We'll totally do that.

00:09:22.380 --> 00:09:24.910
But we don't run all of that,
all of those things that I

00:09:24.910 --> 00:09:26.080
just mentioned, ourselves.

00:09:26.080 --> 00:09:26.620
We can't.

00:09:26.620 --> 00:09:30.990
There's not enough of us and the
specialization that would

00:09:30.990 --> 00:09:33.220
be required for us to run
every part of the

00:09:33.220 --> 00:09:35.830
infrastructure all the way down
to the computers would be

00:09:35.830 --> 00:09:36.530
astronomical.

00:09:36.530 --> 00:09:39.260
We would need a huge team of
people, and we probably

00:09:39.260 --> 00:09:41.910
wouldn't be consistently good
at running all parts of it.

00:09:41.910 --> 00:09:44.710
So in the same way that you
depend on us, we depend on

00:09:44.710 --> 00:09:47.870
further down the stack, a bunch
of people, a bunch of

00:09:47.870 --> 00:09:51.980
SREs running the storage
surface, the lock service, the

00:09:51.980 --> 00:09:54.070
cloud computing environment,
all the way down to

00:09:54.070 --> 00:09:56.410
datacenters and hardware
ops who keep

00:09:56.410 --> 00:09:59.360
on running the machines.

00:09:59.360 --> 00:10:02.670
Now, we can't operate with them
on a handshake basis in

00:10:02.670 --> 00:10:04.310
the same way that we can't
operate with you on a

00:10:04.310 --> 00:10:06.260
handshake basis.

00:10:06.260 --> 00:10:08.090
There have been some discussion
about the changes

00:10:08.090 --> 00:10:12.150
in App Engine pricing model.

00:10:12.150 --> 00:10:13.170
We're changing the
pricing model.

00:10:13.170 --> 00:10:15.770
It's going to be a little more
expensive for people.

00:10:15.770 --> 00:10:17.980
We're asking this money of you
because we're offering a

00:10:17.980 --> 00:10:19.380
certain amount of reliability.

00:10:19.380 --> 00:10:22.590
And in the same way, I can't
just say well, I depend on

00:10:22.590 --> 00:10:24.170
that infrastructure.

00:10:24.170 --> 00:10:27.650
But you guys are going to mostly
run it for us, right?

00:10:27.650 --> 00:10:30.180
Because they understand the
infrastructure and they can

00:10:30.180 --> 00:10:33.870
look at it and say, it's got
this much complexity, we've

00:10:33.870 --> 00:10:35.270
got our own dependencies.

00:10:35.270 --> 00:10:37.560
And we think it can be
available this much

00:10:37.560 --> 00:10:38.240
amount of the time.

00:10:38.240 --> 00:10:41.330
This many 9's or this much
down time per year.

00:10:41.330 --> 00:10:44.400
And we look at what they
offer us and say, we

00:10:44.400 --> 00:10:45.190
can work with that.

00:10:45.190 --> 00:10:46.240
We'll use your service.

00:10:46.240 --> 00:10:48.290
Or if it's below our
threshold, we

00:10:48.290 --> 00:10:48.910
say, you know what?

00:10:48.910 --> 00:10:50.640
We'd love to use it if you
could bring it up to this

00:10:50.640 --> 00:10:51.980
level of reliability.

00:10:51.980 --> 00:10:55.100
Can we pay you in internal
currency or favors, or

00:10:55.100 --> 00:10:57.000
whatever it is?

00:10:57.000 --> 00:10:59.630
And sometimes they say yes and
sometimes they say no.

00:10:59.630 --> 00:11:01.220
And the reason that this
is important is that

00:11:01.220 --> 00:11:02.550
mathematically, we're
depending on

00:11:02.550 --> 00:11:03.720
all of these services.

00:11:03.720 --> 00:11:06.990
And you can not be more reliable
than the least

00:11:06.990 --> 00:11:09.060
reliable service that
you rely upon.

00:11:09.060 --> 00:11:11.990
So we have to make a decision
based on the reliability of

00:11:11.990 --> 00:11:15.650
the underlying services
that we rely upon.

00:11:15.650 --> 00:11:18.000
And so what's some of the
sources of this unreliability?

00:11:18.000 --> 00:11:19.590
Well, this software
is not static.

00:11:19.590 --> 00:11:20.700
It's always changing.

00:11:20.700 --> 00:11:22.100
There's always bugs
being found.

00:11:22.100 --> 00:11:23.740
There's always features
being added.

00:11:23.740 --> 00:11:27.400
We're also always worried
about efficiency and

00:11:27.400 --> 00:11:29.060
increasing efficiency.

00:11:29.060 --> 00:11:30.820
We have our computers.

00:11:30.820 --> 00:11:34.100
If we can make a change across
the board that brings 5%

00:11:34.100 --> 00:11:38.000
efficiency improvement in this
sub system across all of

00:11:38.000 --> 00:11:41.600
Google, that's a lot of spare
computing cycles, that's a lot

00:11:41.600 --> 00:11:45.400
of power, that's a lot of money
that we can save that we

00:11:45.400 --> 00:11:48.710
can use to use the same
infrastructure to scale

00:11:48.710 --> 00:11:50.720
everything else.

00:11:50.720 --> 00:11:54.530
So this is software and it
needs to be upgraded.

00:11:54.530 --> 00:11:56.920
It can't be like your laptop
where it says, well, I want to

00:11:56.920 --> 00:11:58.020
install this software.

00:11:58.020 --> 00:11:59.940
Hold on while I reboot.

00:11:59.940 --> 00:12:02.040
So we have to get this software
out there into

00:12:02.040 --> 00:12:07.910
production while having 24 by 7
service for all these Google

00:12:07.910 --> 00:12:09.410
applications.

00:12:09.410 --> 00:12:12.680
So we design from the beginning
for in place updates

00:12:12.680 --> 00:12:15.110
transparently as possible, which
is to say that there's

00:12:15.110 --> 00:12:17.190
an update going on right
now in the background.

00:12:17.190 --> 00:12:19.900
We're slowly rolling it out
and you shouldn't notice

00:12:19.900 --> 00:12:23.790
anything except that Gmail, all
of a sudden you're Gmail's

00:12:23.790 --> 00:12:25.310
chugging along and then
it says, oh wait.

00:12:25.310 --> 00:12:26.230
I need to reload.

00:12:26.230 --> 00:12:27.680
And you get your mailbox
back again.

00:12:27.680 --> 00:12:28.950
And the UI's a little
different.

00:12:28.950 --> 00:12:31.010
It wasn't like Gmail was like,
we're going to be down for six

00:12:31.010 --> 00:12:32.120
hours while we're doing
an upgrade.

00:12:32.120 --> 00:12:34.590
We do this upgrade in place
right up until the moment that

00:12:34.590 --> 00:12:35.940
we expose it to you.

00:12:35.940 --> 00:12:38.370
Te infrastructure underlying
it works the same way.

00:12:38.370 --> 00:12:40.370
When there's a new version of
the story subsystem, we just

00:12:40.370 --> 00:12:44.890
say, roll it out slowly in the
background, slowly enough that

00:12:44.890 --> 00:12:47.690
the services that are using on
it are engineered to deal with

00:12:47.690 --> 00:12:48.980
that unreliability.

00:12:48.980 --> 00:12:51.290
But you can't do this with
everything because some

00:12:51.290 --> 00:12:52.420
maintenance is too intrusive.

00:12:52.420 --> 00:12:55.130
Sometimes it requires intrusive
software work.

00:12:55.130 --> 00:12:57.780
You actually have to take down
part of a stack, rework it a

00:12:57.780 --> 00:12:59.120
little bit, bring it back up.

00:12:59.120 --> 00:13:01.930
Similarly, if they say well, we
want to take down half the

00:13:01.930 --> 00:13:04.280
datacenter to rework the power
distribution units.

00:13:04.280 --> 00:13:08.440
Fix the generator, oh wait, the
cooling's having problems.

00:13:08.440 --> 00:13:12.100
You can't just say like we're
going to do this and hopefully

00:13:12.100 --> 00:13:14.190
everything will be all right.

00:13:14.190 --> 00:13:16.420
So what we do is we get out our
calendar and we mark out,

00:13:16.420 --> 00:13:20.150
and we say, OK, this given
datacenter, we need to do a

00:13:20.150 --> 00:13:22.860
bunch of work on it and we do
this on a regular schedule.

00:13:22.860 --> 00:13:26.080
And we're going to gang up all
of the really intrusive

00:13:26.080 --> 00:13:29.080
disruptive work that we need to
do, put it all together and

00:13:29.080 --> 00:13:31.660
say, this datacenter
is going to be out

00:13:31.660 --> 00:13:33.720
during this time period.

00:13:33.720 --> 00:13:35.430
And there's an interesting
design pattern here, which is

00:13:35.430 --> 00:13:38.610
to say that we don't say
that every datacenter--

00:13:38.610 --> 00:13:40.030
we're going to try and
keep it up as much as

00:13:40.030 --> 00:13:42.150
possible all of the time.

00:13:42.150 --> 00:13:44.400
When you start launching a
product at Google, you

00:13:44.400 --> 00:13:47.470
understand that the datacenter
the you land in is going to

00:13:47.470 --> 00:13:49.270
blink out of existence
for some amount

00:13:49.270 --> 00:13:50.680
of time every year.

00:13:50.680 --> 00:13:52.670
And what that means is that
you have to be nimble.

00:13:52.670 --> 00:13:55.970
You have to be able to say, OK,
when that happens, I need

00:13:55.970 --> 00:13:58.370
to be able to serve my user
capacity somewhere else.

00:13:58.370 --> 00:14:00.570
I need to do it as transparently
as possible.

00:14:00.570 --> 00:14:02.850
This has to be baked into your
application from the beginning

00:14:02.850 --> 00:14:05.030
or you're going to take
embarrassing outages every

00:14:05.030 --> 00:14:07.140
time this happens.

00:14:07.140 --> 00:14:11.610
So speaking of architecting for
datacenter failovers, how

00:14:11.610 --> 00:14:12.900
does this apply to App Engine?

00:14:12.900 --> 00:14:14.465
Specifically the master
slave configuration

00:14:14.465 --> 00:14:15.660
that we launched in.

00:14:15.660 --> 00:14:19.200
So I've seen this slide given
a couple other times, so I'm

00:14:19.200 --> 00:14:21.410
going to sort of blaze through
it a tiny bit.

00:14:21.410 --> 00:14:24.530
So this is a very complicated
App Engine architectural

00:14:24.530 --> 00:14:27.735
diagram that are intellectual
property that I'm revealing to

00:14:27.735 --> 00:14:30.150
you here for the price
of admission.

00:14:30.150 --> 00:14:32.970
Which is that your application
is serving out of datacenter A

00:14:32.970 --> 00:14:35.000
and reads and writes are
happening to data store, and

00:14:35.000 --> 00:14:35.820
everything's great.

00:14:35.820 --> 00:14:38.440
And we are asynchronously
replicating from the data

00:14:38.440 --> 00:14:41.780
store in A to the data
store over in B.

00:14:41.780 --> 00:14:43.770
And so everything's sort
of well and good.

00:14:43.770 --> 00:14:45.840
And then they say, hey,
datacenter A is going to be on

00:14:45.840 --> 00:14:48.070
available for time period X.

00:14:48.070 --> 00:14:50.850
And we pick a time a little in
advance of time period X.

00:14:50.850 --> 00:14:53.520
And if you've launched an
application on master slave,

00:14:53.520 --> 00:14:55.140
this should be sort of
familiar to you with

00:14:55.140 --> 00:14:55.780
what's going on.

00:14:55.780 --> 00:14:58.390
But I'll show you under
the covers.

00:14:58.390 --> 00:15:01.970
So we disable writes so that you
can continue reading, but

00:15:01.970 --> 00:15:03.710
there's no changes coming
into the data store.

00:15:03.710 --> 00:15:05.960
And this is so that we can
engage this bulk data copy

00:15:05.960 --> 00:15:09.190
process, synchronizing
datacenter A and datacenter's

00:15:09.190 --> 00:15:10.040
B data store.

00:15:10.040 --> 00:15:11.750
And once that's done,
we are guaranteed

00:15:11.750 --> 00:15:13.170
that A and B are identical.

00:15:13.170 --> 00:15:14.750
We can say all right,
we're ready to go.

00:15:14.750 --> 00:15:18.290
And we flip the switch
and we move

00:15:18.290 --> 00:15:19.870
datacenter B to be the master.

00:15:19.870 --> 00:15:21.040
Reads and writes are
happening there.

00:15:21.040 --> 00:15:23.120
The direction of replication
is reversed.

00:15:23.120 --> 00:15:25.930
And at this point, we can take
datacenter A offline for some

00:15:25.930 --> 00:15:29.390
amount of time and they can do
whatever they need to it with

00:15:29.390 --> 00:15:31.360
the power and the cooling
and the hey, hey, hey.

00:15:31.360 --> 00:15:34.350
And you know, your application
continues to serve.

00:15:34.350 --> 00:15:36.520
Other than the fact that with
no further outage other than

00:15:36.520 --> 00:15:40.190
the read-only period we just
made you go through.

00:15:40.190 --> 00:15:42.160
So that's all well and good,
but it's also not very

00:15:42.160 --> 00:15:44.100
complicated.

00:15:44.100 --> 00:15:46.780
If this was all that we had to
deal with, they wouldn't need

00:15:46.780 --> 00:15:49.840
SREs and I wouldn't have this
job, this very interesting and

00:15:49.840 --> 00:15:50.680
well-paying job.

00:15:50.680 --> 00:15:52.660
So I want to talk a little bit
about things that happen to us

00:15:52.660 --> 00:15:55.090
that we sort of didn't
anticipate.

00:15:55.090 --> 00:15:59.510
And there's a bunch of good
stories here, and I really

00:15:59.510 --> 00:16:00.570
wish I could tell.

00:16:00.570 --> 00:16:02.430
And a lot of them I was like,
how about this one?

00:16:02.430 --> 00:16:04.960
And they said, no.

00:16:04.960 --> 00:16:07.030
That exposes a little too much
of our infrastructure.

00:16:07.030 --> 00:16:12.250
This is what I could get cleared
and it's oh, so good.

00:16:12.250 --> 00:16:16.600
All right, so we exposed a
long time ago that Google

00:16:16.600 --> 00:16:18.760
doesn't have UPS's in
our datacenters.

00:16:18.760 --> 00:16:21.810
We don'y have a big room
full of batteries.

00:16:21.810 --> 00:16:24.580
Each server has a small UPS
battery next to it, connected

00:16:24.580 --> 00:16:28.240
to it, providing battery back up
in case the power goes out.

00:16:28.240 --> 00:16:31.160
And so, here's a server--
simplified--

00:16:31.160 --> 00:16:32.810
that is running the
storage process.

00:16:32.810 --> 00:16:36.650
And it's accepting writes from
the network and committing

00:16:36.650 --> 00:16:37.120
them to disk.

00:16:37.120 --> 00:16:38.300
And it's all really good.

00:16:38.300 --> 00:16:41.120
But you note that the storage
process is talking to a power

00:16:41.120 --> 00:16:41.980
management process.

00:16:41.980 --> 00:16:44.180
And you say sort of,
well, why is that?

00:16:44.180 --> 00:16:46.060
It's so that we can do this,
which is an amazing

00:16:46.060 --> 00:16:48.450
optimization and I really,
really like it.

00:16:48.450 --> 00:16:50.900
Which is to say that if the
power goes out, we're running

00:16:50.900 --> 00:16:51.540
off of battery.

00:16:51.540 --> 00:16:54.650
And rather than doing some kind
of broadcast notification

00:16:54.650 --> 00:16:57.150
from a central location that
oh hey, the datacenter has

00:16:57.150 --> 00:17:00.610
lost power, the server knows
that as soon as it's happened

00:17:00.610 --> 00:17:02.650
that the power's off because
it's running off a battery.

00:17:02.650 --> 00:17:05.310
The wall power has gone away.

00:17:05.310 --> 00:17:07.480
And the great thing that can
happen is that the storage

00:17:07.480 --> 00:17:10.910
process asks the power
management process.

00:17:10.910 --> 00:17:13.109
It's been doing this
periodically and it's saying,

00:17:13.109 --> 00:17:14.604
what's the state of the
power infrastructure?

00:17:14.604 --> 00:17:17.040
And you say, why would
it bother?

00:17:17.040 --> 00:17:20.460
It's so it can do this, which is
when the power's out, it'll

00:17:20.460 --> 00:17:21.599
say, oh wow.

00:17:21.599 --> 00:17:23.760
I shouldn't accept any more
writes from the network.

00:17:23.760 --> 00:17:26.300
And the reason it does that is
because we've accepted writes

00:17:26.300 --> 00:17:27.810
and they're maybe cached
in memory.

00:17:27.810 --> 00:17:29.700
They may be in the application
memory.

00:17:29.700 --> 00:17:30.910
They may be in the
kernel memory.

00:17:30.910 --> 00:17:33.100
They may be in write
cache on the disk.

00:17:33.100 --> 00:17:35.630
The write has not necessarily
been synchronously accepted

00:17:35.630 --> 00:17:37.810
from the network all the
way down to the disk.

00:17:37.810 --> 00:17:39.980
So we want to buy ourselves
some time to get

00:17:39.980 --> 00:17:41.050
that flushed to happen.

00:17:41.050 --> 00:17:45.540
Which if it's server is going to
lose power, then we want to

00:17:45.540 --> 00:17:47.130
have a guarantee
that we have--

00:17:47.130 --> 00:17:48.770
or a high guaranteed
chance that the

00:17:48.770 --> 00:17:50.060
data was all committed.

00:17:50.060 --> 00:17:52.500
So the storage process says,
I'm not going to accept any

00:17:52.500 --> 00:17:53.290
other writes.

00:17:53.290 --> 00:17:54.200
And this works beautifully.

00:17:54.200 --> 00:17:56.320
So imagine you have a datacenter
that looks like

00:17:56.320 --> 00:17:58.810
this with an array
of storage nodes.

00:17:58.810 --> 00:18:00.180
And writes are happening
everywhere.

00:18:00.180 --> 00:18:01.900
A couple of the servers are
down for maintenance, so

00:18:01.900 --> 00:18:03.690
they're denying writes and
everything's happy.

00:18:03.690 --> 00:18:06.950
And then the power goes out
and it looks like this.

00:18:06.950 --> 00:18:07.970
It's beautiful.

00:18:07.970 --> 00:18:10.110
Everybody says, I'm not
accepting writes.

00:18:10.110 --> 00:18:12.580
And as quickly as that happens,
all of the services

00:18:12.580 --> 00:18:15.250
that were trying to write to
them realize, wow, nothing

00:18:15.250 --> 00:18:17.140
here in this datacenter
is accepting writes.

00:18:17.140 --> 00:18:20.360
Let me return a deferral back
to the client JavaScript

00:18:20.360 --> 00:18:21.880
that's running in the
user's browser.

00:18:21.880 --> 00:18:24.110
And they say, hey, we're
having a problem.

00:18:24.110 --> 00:18:25.420
Let's try again in a minute.

00:18:25.420 --> 00:18:27.970
So the state that the user's
trying to submit isn't lost.

00:18:27.970 --> 00:18:28.640
We didn't say, oh, yeah.

00:18:28.640 --> 00:18:33.940
We accepted it and
it's all good.

00:18:33.940 --> 00:18:36.220
The minute this happens, we can
ripple that error back out

00:18:36.220 --> 00:18:39.780
to the user in a way that allows
them to cleanly retry.

00:18:39.780 --> 00:18:40.940
It's beautiful.

00:18:40.940 --> 00:18:43.000
And of course, I'm telling you
this story because it didn't

00:18:43.000 --> 00:18:45.360
work out that way.

00:18:45.360 --> 00:18:48.370
Which is to say that the power
management process is talking

00:18:48.370 --> 00:18:49.380
to the power supply.

00:18:49.380 --> 00:18:53.050
Power supplies are spooky,
spooky analog crap.

00:18:53.050 --> 00:18:55.680
And occasionally, the power
management process would talk

00:18:55.680 --> 00:18:59.090
to the battery or talk to power
supply and say, so hey,

00:18:59.090 --> 00:18:59.660
how's the battery?

00:18:59.660 --> 00:19:01.320
And the power supply would
say, there's no battery

00:19:01.320 --> 00:19:04.500
attached to me.

00:19:04.500 --> 00:19:05.220
Really?

00:19:05.220 --> 00:19:06.460
OK.

00:19:06.460 --> 00:19:07.970
You know, I thought I just asked
you a minute ago and you

00:19:07.970 --> 00:19:08.470
said there was.

00:19:08.470 --> 00:19:10.020
But maybe they disconnected
it.

00:19:10.020 --> 00:19:11.250
OK, that's fine.

00:19:11.250 --> 00:19:12.935
And then if you asked it
again a minute later it

00:19:12.935 --> 00:19:13.870
would say, oh, right.

00:19:13.870 --> 00:19:14.330
No sorry.

00:19:14.330 --> 00:19:15.210
There's a battery in here.

00:19:15.210 --> 00:19:19.210
It's sort of like voltage and
charge and general health.

00:19:19.210 --> 00:19:21.320
But there was a race condition
in the middle here where the

00:19:21.320 --> 00:19:24.720
storage process would ask the
power management process, so

00:19:24.720 --> 00:19:25.930
what's the state of the power?

00:19:25.930 --> 00:19:29.430
And the power manager process
would say, it's good.

00:19:29.430 --> 00:19:29.900
We're up.

00:19:29.900 --> 00:19:30.620
We clearly have power.

00:19:30.620 --> 00:19:32.040
But I don't have a battery
installed.

00:19:32.040 --> 00:19:33.170
Just so you know.

00:19:33.170 --> 00:19:36.540
And the power managed storage
process made a decision that

00:19:36.540 --> 00:19:38.810
was not actually very good,
which is it decided well, if

00:19:38.810 --> 00:19:40.920
there's no battery, I don't
need to keep on

00:19:40.920 --> 00:19:42.160
talking to you any more.

00:19:42.160 --> 00:19:45.430
That thread that I was running
that was talking to the power

00:19:45.430 --> 00:19:48.900
management process, I'm going
to save some CPU by just

00:19:48.900 --> 00:19:49.590
shutting that down.

00:19:49.590 --> 00:19:50.430
Because there's no point.

00:19:50.430 --> 00:19:52.620
If there's no battery when
the power goes out, I'm

00:19:52.620 --> 00:19:53.410
just going to crash.

00:19:53.410 --> 00:19:56.070
Why should I bother?

00:19:56.070 --> 00:19:57.960
You can think about the
consequences of this, which is

00:19:57.960 --> 00:19:59.830
to say that the power goes
out and the server keeps

00:19:59.830 --> 00:20:01.320
on going yep, sure.

00:20:01.320 --> 00:20:02.180
I'll take some writes.

00:20:02.180 --> 00:20:03.310
Do whatever you want.

00:20:03.310 --> 00:20:07.370
I'll take them into
and continually

00:20:07.370 --> 00:20:08.370
call them in memory.

00:20:08.370 --> 00:20:12.690
And then we had a power outage
that looked like this, which

00:20:12.690 --> 00:20:17.770
is to say that most of the
servers said, oh wait.

00:20:17.770 --> 00:20:19.470
I'm not going to accept
any writes.

00:20:19.470 --> 00:20:21.950
But a handful of them had
triggered this bug.

00:20:21.950 --> 00:20:24.800
And when they triggered
this bug they

00:20:24.800 --> 00:20:26.140
continued accepting writes.

00:20:26.140 --> 00:20:28.105
And so this was fine because--
well OK, we

00:20:28.105 --> 00:20:30.510
went on to the generator.

00:20:30.510 --> 00:20:31.100
And that was great.

00:20:31.100 --> 00:20:33.100
And the generator came back and
I have another slide of

00:20:33.100 --> 00:20:35.410
all green and it's
all fantastic.

00:20:35.410 --> 00:20:39.070
And then the generator's turbo
started glowing cherry red.

00:20:39.070 --> 00:20:43.130
And the hardware ops guys onsite
said, well, I don't

00:20:43.130 --> 00:20:45.110
know what the procedure is for
this, but it looks like it's

00:20:45.110 --> 00:20:46.330
about to explode.

00:20:46.330 --> 00:20:48.455
And when it's about to explode,
I think we're going

00:20:48.455 --> 00:20:49.360
to turn it off.

00:20:49.360 --> 00:20:51.080
So they went over and they
hit the big red button.

00:20:51.080 --> 00:20:52.740
And they really have a big
red button for this.

00:20:52.740 --> 00:20:55.260
And you know, I'm glad they got
to hit it at least once.

00:20:55.260 --> 00:20:56.470
And they turned the
generator off.

00:20:56.470 --> 00:20:57.690
So we're back on
battery again.

00:20:57.690 --> 00:21:00.580
And utility power hasn't
come back on.

00:21:00.580 --> 00:21:03.320
So we're in this situation and
all of these servers are

00:21:03.320 --> 00:21:05.600
accepting writes and accepting
writes and accepting writes,

00:21:05.600 --> 00:21:06.850
and then they all crash.

00:21:09.430 --> 00:21:12.430
So that didn't really sort of
work out so well for us.

00:21:12.430 --> 00:21:14.850
I'm going to point out that we
didn't have any data loss in

00:21:14.850 --> 00:21:18.740
this situation because when the
power went out, we drained

00:21:18.740 --> 00:21:19.750
the datacenter of traffic.

00:21:19.750 --> 00:21:21.770
We have the ability to globally
redirect traffic away

00:21:21.770 --> 00:21:22.450
from that datacenter.

00:21:22.450 --> 00:21:24.480
We said, OK, something's
going on.

00:21:24.480 --> 00:21:26.280
So user traffic went away.

00:21:26.280 --> 00:21:28.060
But there's still some
background processes running

00:21:28.060 --> 00:21:32.520
doing batch processing
of various stuff.

00:21:32.520 --> 00:21:34.380
Those things are still running
even though the front end

00:21:34.380 --> 00:21:37.180
traffic had gone away.

00:21:37.180 --> 00:21:38.400
They were happily
writing, happily

00:21:38.400 --> 00:21:39.340
writing, happily writing.

00:21:39.340 --> 00:21:41.930
And then we lost that
data temporarily

00:21:41.930 --> 00:21:45.250
when the servers crashed.

00:21:45.250 --> 00:21:48.640
This drastically increased our
return to service time for

00:21:48.640 --> 00:21:49.760
this datacenter.

00:21:49.760 --> 00:21:52.780
It took it out for much longer
than statistically it takes us

00:21:52.780 --> 00:21:55.350
to return a datacenter to
service after we've had a

00:21:55.350 --> 00:21:56.180
power outage.

00:21:56.180 --> 00:21:59.360
That's no good for us,
so we fixed that.

00:21:59.360 --> 00:22:00.320
So we learned some
things from this.

00:22:00.320 --> 00:22:03.100
Which is that you can't always
trust your monitoring.

00:22:03.100 --> 00:22:04.760
It'll lie to you and sometimes
it'll lie to you

00:22:04.760 --> 00:22:07.090
intermittently, which
is awesome.

00:22:10.540 --> 00:22:13.040
This is a trust, but verify.

00:22:13.040 --> 00:22:16.460
Ronald Reagan's excellent
one quote.

00:22:16.460 --> 00:22:19.680
And you say that don't make
any decisions permanently

00:22:19.680 --> 00:22:21.520
based on one data point.

00:22:21.520 --> 00:22:24.630
And like I said, the system
may lie to you.

00:22:24.630 --> 00:22:28.000
And some optimizations
are not worth doing.

00:22:28.000 --> 00:22:30.220
It was very nice of the storage
process to say, I'm

00:22:30.220 --> 00:22:32.660
going to save you this
fractional amount of CPU by

00:22:32.660 --> 00:22:36.760
not running this query to the
power management process.

00:22:36.760 --> 00:22:40.790
But it cost us all of this time
in clean up and debugging

00:22:40.790 --> 00:22:43.480
and general frustration
afterwards.

00:22:43.480 --> 00:22:47.380
So this is a good lesson
for there.

00:22:47.380 --> 00:22:50.280
So I've got a quote here from
Clay Caviness, who runs

00:22:50.280 --> 00:22:52.010
Google's MacOps organization.

00:22:52.010 --> 00:22:54.870
These numbers in here are
about our laptop fleet.

00:22:54.870 --> 00:22:56.100
They don't tell you anything
about our production

00:22:56.100 --> 00:22:56.630
datacenters.

00:22:56.630 --> 00:22:59.320
I'm not disclosing anything
that has not been

00:22:59.320 --> 00:23:01.300
tightly held before.

00:23:01.300 --> 00:23:03.890
"A large fleet like ours means
that a bug that crops up only

00:23:03.890 --> 00:23:07.400
0.5% of the time affects
over 100 people.

00:23:07.400 --> 00:23:09.720
It becomes difficult to file
bugs when the "how to

00:23:09.720 --> 00:23:15.040
reproduce" section is do
this 25,000 times."

00:23:15.040 --> 00:23:18.180
And I tell you this story
because the next story is

00:23:18.180 --> 00:23:20.720
really sort of all about that.

00:23:20.720 --> 00:23:22.990
So one day somebody's pager
goes off and it

00:23:22.990 --> 00:23:25.860
says the big table--

00:23:25.860 --> 00:23:28.140
a big table, not the entire big
table, but a table inside

00:23:28.140 --> 00:23:29.290
of a big table is gone.

00:23:29.290 --> 00:23:32.870
And he comes up online and says,
well, what's going on?

00:23:32.870 --> 00:23:35.460
We're serving errors out of
this application now.

00:23:35.460 --> 00:23:37.450
And he looks and why
did this go away?

00:23:37.450 --> 00:23:40.440
And he looks at the big table
master and he says, oh OK.

00:23:40.440 --> 00:23:43.630
It says that there was an RPC to
delete that table from this

00:23:43.630 --> 00:23:46.030
server and this user
at this time.

00:23:46.030 --> 00:23:49.370
And they went back and looked
at that server.

00:23:49.370 --> 00:23:51.670
And that server had recently--
there was a process that had

00:23:51.670 --> 00:23:53.510
run by that user on
that server and

00:23:53.510 --> 00:23:54.360
it had a stack trace.

00:23:54.360 --> 00:23:56.680
And it said function A, function
B, function C,

00:23:56.680 --> 00:23:58.120
destroy table.

00:23:58.120 --> 00:23:59.070
And they went, huh.

00:23:59.070 --> 00:24:00.720
And it was funny because we
looked at the code for

00:24:00.720 --> 00:24:01.670
function C.

00:24:01.670 --> 00:24:03.740
It never should have called
destroy cable.

00:24:03.740 --> 00:24:06.040
So at this point,
we're going, OK.

00:24:06.040 --> 00:24:06.790
What's going on?

00:24:06.790 --> 00:24:09.620
Is this enemy action?

00:24:09.620 --> 00:24:11.710
What the hell?

00:24:11.710 --> 00:24:14.300
So this was a language with
virtual message dispatch

00:24:14.300 --> 00:24:16.020
unfortunately.

00:24:16.020 --> 00:24:19.610
And we pulled that server out of
commission and we put it in

00:24:19.610 --> 00:24:23.260
through the equivalent of Google
memtestx86 and after

00:24:23.260 --> 00:24:25.940
long enough we found out that oh
hey, if you let it run long

00:24:25.940 --> 00:24:28.530
enough under high enough load,
it starts corrupting contents

00:24:28.530 --> 00:24:30.240
of memory occasionally.

00:24:30.240 --> 00:24:33.430
And as nearest we can tell
what happened here was in

00:24:33.430 --> 00:24:36.620
function C there was an index
into whatever function it was

00:24:36.620 --> 00:24:37.600
going to call.

00:24:37.600 --> 00:24:39.560
Give big table a hug
or something.

00:24:39.560 --> 00:24:45.510
And the processor corrupted the
memory just perfectly to

00:24:45.510 --> 00:24:48.450
instead of calling that
function, let's call destroy

00:24:48.450 --> 00:24:50.090
table instead.

00:24:50.090 --> 00:24:52.670
And again, this was no data loss
here because after a big

00:24:52.670 --> 00:24:54.000
table is deleted, it's not

00:24:54.000 --> 00:24:57.180
instantaneously garbage collected.

00:24:57.180 --> 00:24:59.560
We have facilities where we can
go, OK, let's recreate it

00:24:59.560 --> 00:25:00.990
at the state of the last--

00:25:00.990 --> 00:25:02.870
right before the deletion.

00:25:02.870 --> 00:25:04.640
On the other hand, that's not
acceptable to us because the

00:25:04.640 --> 00:25:07.360
minute that this happened we
started serving errors out of

00:25:07.360 --> 00:25:09.900
that datacenter for
that product.

00:25:09.900 --> 00:25:12.480
We're serving 500s or whatever
the error code was or please

00:25:12.480 --> 00:25:14.370
try again later, et cetera.

00:25:17.800 --> 00:25:19.280
Yeah, it's good times.

00:25:19.280 --> 00:25:21.060
It's really good times.

00:25:21.060 --> 00:25:23.480
So what did we learn
about this?

00:25:23.480 --> 00:25:26.450
Again, trust, but verify.

00:25:26.450 --> 00:25:30.450
I have seen every piece of
hardware at Google attempt to

00:25:30.450 --> 00:25:31.700
corrupt data.

00:25:35.410 --> 00:25:37.310
I'm almost not kidding.

00:25:37.310 --> 00:25:41.640
Processors, memory, hard
drives, power supplies,

00:25:41.640 --> 00:25:43.680
network switches, network
cables, I have not heard a

00:25:43.680 --> 00:25:48.110
story where the dolly that we
use to move machines around or

00:25:48.110 --> 00:25:50.570
a bad power cable has corrupted
data, but I wouldn't

00:25:50.570 --> 00:25:52.870
be surprised if somebody came
to me and told me a story

00:25:52.870 --> 00:25:54.910
where that had happened.

00:25:54.910 --> 00:25:57.170
When you have enough
pieces of hardware,

00:25:57.170 --> 00:25:58.220
they're out to get you.

00:25:58.220 --> 00:26:00.090
I don't believe that there are
bad people, but I believe that

00:26:00.090 --> 00:26:01.495
there is actually
bad hardware.

00:26:05.620 --> 00:26:07.580
You need checksums. You
need checksums on data

00:26:07.580 --> 00:26:08.130
transmission.

00:26:08.130 --> 00:26:11.260
Checksums for data at rest.
And verify them.

00:26:11.260 --> 00:26:14.660
Because if you transmit the data
and it comes out at the

00:26:14.660 --> 00:26:17.700
other end, it's not what you
expected, somewhere in there,

00:26:17.700 --> 00:26:20.080
something in there, there's
a bad piece of hardware.

00:26:20.080 --> 00:26:22.970
You need to find it and get it
out of your infrastructure as

00:26:22.970 --> 00:26:24.220
quickly as possible.

00:26:26.500 --> 00:26:28.540
It may be causing data loss,
potential data loss, or

00:26:28.540 --> 00:26:29.960
something that you're going
to have recover.

00:26:29.960 --> 00:26:31.660
Find these things and
get rid of them.

00:26:31.660 --> 00:26:34.990
They are the bad apples.

00:26:34.990 --> 00:26:39.860
And when these things happen,
you can't be sure that--

00:26:39.860 --> 00:26:40.810
like I said, that story.

00:26:40.810 --> 00:26:42.700
Suddenly a big table was
gone and nobody copped

00:26:42.700 --> 00:26:43.810
to having done it.

00:26:43.810 --> 00:26:46.460
And were we hacked?

00:26:46.460 --> 00:26:48.150
Was some disgruntled employee
just doing it?

00:26:48.150 --> 00:26:48.950
No.

00:26:48.950 --> 00:26:52.950
This is the evidence that if you
give an infinite number of

00:26:52.950 --> 00:26:55.420
monkeys a typewriter, one of
them will write Shakespeare.

00:26:55.420 --> 00:26:57.670
If you put enough pieces of
Google hardware and you roll

00:26:57.670 --> 00:27:00.710
enough dice over days and days
and days, eventually one of

00:27:00.710 --> 00:27:02.940
them will spontaneously attempt
to delete your data

00:27:02.940 --> 00:27:04.190
out from under you.

00:27:07.640 --> 00:27:10.050
So I've told you some stories
about planned maintenance and

00:27:10.050 --> 00:27:10.770
unplanned maintenance.

00:27:10.770 --> 00:27:14.650
And what we learn here is that
any given datacenter can blink

00:27:14.650 --> 00:27:18.110
out of existence or be rendered
unfit for serving.

00:27:18.110 --> 00:27:19.730
It doesn't matter that the
datacenter was still there

00:27:19.730 --> 00:27:21.180
when we deleted that
big table.

00:27:21.180 --> 00:27:24.470
Because at the minute that
that happened, we started

00:27:24.470 --> 00:27:25.710
serving errors.

00:27:25.710 --> 00:27:28.700
This wasn't App Engine, this
was a different product.

00:27:28.700 --> 00:27:30.350
And you need to be able
to deal with that.

00:27:30.350 --> 00:27:32.400
There's an unending
list of things.

00:27:32.400 --> 00:27:34.790
I mean we haven't even gotten
into like people stealing

00:27:34.790 --> 00:27:37.990
fiber optic cable or people
digging up-- accidentally

00:27:37.990 --> 00:27:39.400
trenching up fiber
optic cable.

00:27:39.400 --> 00:27:41.440
Which is the bane of any

00:27:41.440 --> 00:27:43.630
networking company's existence.

00:27:43.630 --> 00:27:46.530
And you can attempt to harden
each site to make them

00:27:46.530 --> 00:27:49.330
resilient against these
things, but it

00:27:49.330 --> 00:27:50.290
doesn't always work.

00:27:50.290 --> 00:27:55.000
It doesn't matter how much error
correcting RAM I had in

00:27:55.000 --> 00:27:57.880
that machine if the processor
was doing the corruption.

00:27:57.880 --> 00:28:02.970
It doesn't matter how many
generators I had if we roll

00:28:02.970 --> 00:28:04.060
the dice and eventually
the data

00:28:04.060 --> 00:28:06.480
spontaneously gets deleted.

00:28:06.480 --> 00:28:10.350
Situations will arise that will
make a mockery of it.

00:28:10.350 --> 00:28:11.840
It's like the Maginot Line.

00:28:11.840 --> 00:28:13.280
They're just going
to go around it.

00:28:13.280 --> 00:28:14.730
And you're going to be like,
oh, well, now I have

00:28:14.730 --> 00:28:16.370
to deal with this.

00:28:16.370 --> 00:28:19.740
So architecturally, what have
we learned about App Engine

00:28:19.740 --> 00:28:23.640
and making it more reliant and
making it more reliable?

00:28:23.640 --> 00:28:26.110
And so philosophically, we found
that people through the

00:28:26.110 --> 00:28:30.160
ages have the right sort of
instinct for this kind of

00:28:30.160 --> 00:28:32.850
thing that we like
to find in SRE.

00:28:32.850 --> 00:28:34.440
So here's again, the master
slave datastore.

00:28:34.440 --> 00:28:35.605
And I'm not going to go through
a bunch of failure

00:28:35.605 --> 00:28:37.920
slides, but we can just talk
about it a little bit.

00:28:37.920 --> 00:28:41.070
Which is to say that imagine
this circumstance.

00:28:41.070 --> 00:28:42.720
Datacenter A can go
away at any time.

00:28:42.720 --> 00:28:47.025
The power can go out and we
can't get it back on in time.

00:28:50.390 --> 00:28:52.180
The big tables that we're
relying upon can get

00:28:52.180 --> 00:28:54.350
spontaneously deleted and
we can restore it.

00:28:54.350 --> 00:28:57.120
But again, that's going to
take some amount of time.

00:28:57.120 --> 00:28:58.740
There could be a network
partition that just takes the

00:28:58.740 --> 00:28:59.620
datacenter off the map.

00:28:59.620 --> 00:29:02.730
And all of these things conspire
that say, OK, well we

00:29:02.730 --> 00:29:04.180
can't serve your application
out of here, we

00:29:04.180 --> 00:29:05.340
need to move it.

00:29:05.340 --> 00:29:07.930
But unfortunately, there's
this little line mark to

00:29:07.930 --> 00:29:09.580
asynchronous replication
on there.

00:29:09.580 --> 00:29:12.190
That makes it hard for us
because we've committed

00:29:12.190 --> 00:29:14.730
certain amounts of data to the
datastore in A and if we

00:29:14.730 --> 00:29:17.430
promote B, that data's
not there.

00:29:17.430 --> 00:29:20.330
And it's not lost because the
datacenter's going to come

00:29:20.330 --> 00:29:23.040
back and we're going to
be able to move it.

00:29:23.040 --> 00:29:26.300
Move it back over it, but in
the meantime, if we promote

00:29:26.300 --> 00:29:28.980
your B to be serving, all of a
sudden your application has

00:29:28.980 --> 00:29:31.350
jumped backwards in time a
couple of minutes for all of

00:29:31.350 --> 00:29:32.060
the writes.

00:29:32.060 --> 00:29:34.800
And once we do get datacenter
A back online, we say, OK,

00:29:34.800 --> 00:29:37.630
here's the uncommitted data.

00:29:37.630 --> 00:29:40.250
I can't say to you just like,
oh, we'll shove it into

00:29:40.250 --> 00:29:43.600
datacenter B because you may
have rewritten that data.

00:29:43.600 --> 00:29:45.240
You may deleted it, whatever.

00:29:45.240 --> 00:29:48.220
We have to say, here's your
data, you can decide what to

00:29:48.220 --> 00:29:51.650
do with it and we're
going away.

00:29:51.650 --> 00:29:53.710
That's not a good
story for you.

00:29:53.710 --> 00:29:55.640
That's the kind of thing we're
trying to hide from you and

00:29:55.640 --> 00:29:56.360
that's not a good
story for us.

00:29:56.360 --> 00:29:57.790
We don't want to spend our
time doing that either.

00:29:57.790 --> 00:30:01.860
There's way more interesting
problems to be solving for us.

00:30:01.860 --> 00:30:04.180
And like I, you can decide that
OK, what we should do is

00:30:04.180 --> 00:30:04.950
armor the sites.

00:30:04.950 --> 00:30:08.140
And when we do, when we find
things that-- we find some

00:30:08.140 --> 00:30:09.900
kinds of fragility that
we think we can sort

00:30:09.900 --> 00:30:12.210
of make less common.

00:30:12.210 --> 00:30:14.420
We looked at that and we said,
all right, we can deal with

00:30:14.420 --> 00:30:16.380
this kind of thing.

00:30:16.380 --> 00:30:18.160
But you can't anticipate
everything.

00:30:18.160 --> 00:30:20.850
And really what we've found
out is that you

00:30:20.850 --> 00:30:22.060
need to become agile.

00:30:22.060 --> 00:30:23.730
You need to be able
to leap around.

00:30:23.730 --> 00:30:26.600
Things are going to
come at you like

00:30:26.600 --> 00:30:27.680
your Jackie Chan movie.

00:30:27.680 --> 00:30:29.550
Things are going to be coming
at you from all directions:

00:30:29.550 --> 00:30:31.800
above, behind You know,
things are going to

00:30:31.800 --> 00:30:32.880
try and fall on you.

00:30:32.880 --> 00:30:35.990
And the best thing you can do is
be able to move quickly, so

00:30:35.990 --> 00:30:39.130
that you can deal with while
you see that coming at you,

00:30:39.130 --> 00:30:42.070
get out of the way, then figure
out what you do next.

00:30:42.070 --> 00:30:44.180
And so we thought about this for
a while and we looked at

00:30:44.180 --> 00:30:45.840
sort of the things that
improved inside Google

00:30:45.840 --> 00:30:49.180
infrastructure and we said,
what can we do about this?

00:30:49.180 --> 00:30:51.300
And the end product is this.

00:30:51.300 --> 00:30:52.810
It's the high replication
datastore

00:30:52.810 --> 00:30:55.160
configuration to App Engine.

00:30:55.160 --> 00:30:59.880
And what it is is we run
multiple datacenters.

00:30:59.880 --> 00:31:03.040
And three is not the
actual number.

00:31:03.040 --> 00:31:05.370
And we serve your application
out of one of them, but all of

00:31:05.370 --> 00:31:07.580
your writes happens
synchronously to all of them.

00:31:07.580 --> 00:31:10.700
Before we would do a majority
of them using the Paxos

00:31:10.700 --> 00:31:12.700
algorithm, which we talked about
in the datastore talk

00:31:12.700 --> 00:31:15.050
and also Leslie Lamport's
paper.

00:31:15.050 --> 00:31:17.890
So we commit your writes to a
majority of them before we

00:31:17.890 --> 00:31:20.120
return success to you,
your application.

00:31:20.120 --> 00:31:22.580
Before you return success to the
user and the web browser.

00:31:22.580 --> 00:31:26.440
And the beauty of this is that
if we instantaneously have to

00:31:26.440 --> 00:31:29.080
take your traffic away from
datacenter A to datacenter B,

00:31:29.080 --> 00:31:31.160
and it really doesn't take too
much more effort for me to do

00:31:31.160 --> 00:31:35.870
that then when I just did, then
we can move that without

00:31:35.870 --> 00:31:37.690
a read-only period.

00:31:37.690 --> 00:31:40.400
We are certain that all of your
data is replicated to a

00:31:40.400 --> 00:31:42.430
majority of all of the
other datacenters.

00:31:42.430 --> 00:31:45.200
If we missed it for whatever
reason, there's this missed

00:31:45.200 --> 00:31:48.310
write replication happening
in the background.

00:31:48.310 --> 00:31:51.170
It'll keep all of them up
to date eventually.

00:31:51.170 --> 00:31:54.490
And if you have an outage
unplanned or planned, then we

00:31:54.490 --> 00:31:57.050
can instantaneously shift the
traffic over and know that

00:31:57.050 --> 00:32:00.530
we're not trying to you backup
reads from the offline

00:32:00.530 --> 00:32:02.190
datacenter and the missed write

00:32:02.190 --> 00:32:03.290
replication is now active.

00:32:03.290 --> 00:32:04.970
So we're queuing up
these things once

00:32:04.970 --> 00:32:05.960
A comes back online.

00:32:05.960 --> 00:32:08.310
Whether it's after scheduled
maintenance or we've drained

00:32:08.310 --> 00:32:11.920
traffic away in an emergency,
or the network is

00:32:11.920 --> 00:32:12.410
disconnected.

00:32:12.410 --> 00:32:15.850
Whatever's happened, we're ready
to handle it when that

00:32:15.850 --> 00:32:19.150
datacenter comes back online.

00:32:19.150 --> 00:32:21.680
This isolates us from problems
in the underlying storage

00:32:21.680 --> 00:32:24.260
stack, which is to say that
if you were serving your

00:32:24.260 --> 00:32:28.080
application out of A and
something happens, something

00:32:28.080 --> 00:32:29.110
hiccups in that stack.

00:32:29.110 --> 00:32:29.863
Remember, we don't run it.

00:32:29.863 --> 00:32:31.090
We share it with other people.

00:32:31.090 --> 00:32:34.440
And we attempt to isolate it,
but isolation is really hard.

00:32:34.440 --> 00:32:38.220
So if something happens, if some
amount of traffic, some

00:32:38.220 --> 00:32:42.060
amount of disruption, some bug,
something's going on that

00:32:42.060 --> 00:32:48.160
disrupts this situation, the
megastore layer that handles

00:32:48.160 --> 00:32:51.270
all the writes down to the
datastore can look at that and

00:32:51.270 --> 00:32:52.140
say, you know what?

00:32:52.140 --> 00:32:54.890
I'm not actually getting a quick
response to the reads

00:32:54.890 --> 00:32:56.590
and the writes out of
my local datastore.

00:32:56.590 --> 00:32:58.020
I prefer that one because
it's local.

00:32:58.020 --> 00:33:01.020
I don't have to go to another
datacenter, but if that one's

00:33:01.020 --> 00:33:04.480
not performing adequately for
me, I'm going to ignore it and

00:33:04.480 --> 00:33:06.730
I'm going to commit my writes
over there to the other

00:33:06.730 --> 00:33:07.240
datacenters.

00:33:07.240 --> 00:33:09.680
Which even though I'm sending
it over the network and

00:33:09.680 --> 00:33:11.840
there's the speed of light issue
with how fast it gets

00:33:11.840 --> 00:33:13.880
there and how fast it gets back,
it's still better than

00:33:13.880 --> 00:33:15.720
waiting several seconds
for the local

00:33:15.720 --> 00:33:17.360
storage option to return.

00:33:17.360 --> 00:33:20.530
And this goes on the minute that
there's any performance

00:33:20.530 --> 00:33:25.130
problems. We are aware of what
that happened and we monitor

00:33:25.130 --> 00:33:26.530
it, but I don't have
to do anything.

00:33:26.530 --> 00:33:27.830
The system handles
this for me.

00:33:27.830 --> 00:33:30.700
In the middle of the night,
right now at Google IO, and it

00:33:30.700 --> 00:33:32.310
handles it faster than
I can react.

00:33:35.110 --> 00:33:39.990
So we see that the perennial
theme throughout all of this,

00:33:39.990 --> 00:33:43.790
is that if you think about
modern thinking and systems

00:33:43.790 --> 00:33:45.760
thinking, you need to expect
the unexpected.

00:33:45.760 --> 00:33:50.280
And this is really what
SRE tries to do.

00:33:50.280 --> 00:33:51.750
Oh right, there's a story
I want to tell you.

00:33:51.750 --> 00:33:53.110
All right, so I'll
back up to that.

00:33:55.900 --> 00:33:56.870
I heard that big table story.

00:33:56.870 --> 00:33:58.260
And again, that wasn't something
that happened to me

00:33:58.260 --> 00:34:00.560
about the spontaneously
deleted big table.

00:34:00.560 --> 00:34:04.250
And I thought about that and
my relatives will sometimes

00:34:04.250 --> 00:34:08.350
say to me, so what exactly is
it that you do all day?

00:34:08.350 --> 00:34:10.830
And I thought about that story
and sort of how to

00:34:10.830 --> 00:34:12.790
explain it to them.

00:34:12.790 --> 00:34:15.889
How would I explain that?

00:34:15.889 --> 00:34:16.940
There's a large system.

00:34:16.940 --> 00:34:19.949
You could think of it sort
of like a steam engine.

00:34:19.949 --> 00:34:20.930
You know, it's very elaborate.

00:34:20.930 --> 00:34:22.040
It's stories tall.

00:34:22.040 --> 00:34:24.570
I understand most of it.

00:34:24.570 --> 00:34:26.340
Some parts of it are fuzzy
around the edge unless I get

00:34:26.340 --> 00:34:28.159
close to it.

00:34:28.159 --> 00:34:30.949
And we're working on it trying
to keep it working and some

00:34:30.949 --> 00:34:34.389
days gremlins just pop out of
the system and do something

00:34:34.389 --> 00:34:36.090
really unexpected.

00:34:36.090 --> 00:34:39.560
And you chase them off and
you get rid of them.

00:34:39.560 --> 00:34:43.030
OK, I'm not likely to see that
exact gremlin again, but maybe

00:34:43.030 --> 00:34:44.469
he's got a cousin.

00:34:44.469 --> 00:34:46.510
And I should make sure that
he and his cousin

00:34:46.510 --> 00:34:47.600
can't come back again.

00:34:47.600 --> 00:34:54.420
And every day it's
a new surprise.

00:34:54.420 --> 00:34:56.040
So how do you survive
life in production?

00:34:56.040 --> 00:34:59.020
And here we see our SRE mascot
fulfilling its intended

00:34:59.020 --> 00:35:00.270
purpose in life.

00:35:03.510 --> 00:35:06.960
So I made a couple of points to
you earlier on, which is to

00:35:06.960 --> 00:35:08.590
say we take these
things as given.

00:35:08.590 --> 00:35:10.940
A service running in a given
datacenter can be no more

00:35:10.940 --> 00:35:12.450
reliable than the
least reliable

00:35:12.450 --> 00:35:13.930
component it utilizes.

00:35:13.930 --> 00:35:16.720
We love the SRE teams who run
the infrastructure under us,

00:35:16.720 --> 00:35:19.410
but they have outages too.

00:35:19.410 --> 00:35:22.880
They're conforming to the SLAs
that they promised us, but

00:35:22.880 --> 00:35:26.080
that doesn't mean that there's
not going to be an outage.

00:35:26.080 --> 00:35:28.420
And also, that you can rely on
serving your application of a

00:35:28.420 --> 00:35:30.810
datacenter, but you have to
handle both scheduled

00:35:30.810 --> 00:35:32.860
maintenance and surprises
that will make it

00:35:32.860 --> 00:35:34.110
unusable at any time.

00:35:38.760 --> 00:35:40.930
If you're designing something
like Google, what are the

00:35:40.930 --> 00:35:44.130
design principles
that you need?

00:35:44.130 --> 00:35:45.430
So everybody get ready
to take notes.

00:35:45.430 --> 00:35:48.540
I'm going to give you away
all of Google's secrets.

00:35:48.540 --> 00:35:52.890
So here's what we have found is
the best option if you want

00:35:52.890 --> 00:35:57.300
to build a worldwide reliable
set of applications.

00:35:57.300 --> 00:35:59.050
Put the application in
multiple datacenters.

00:35:59.050 --> 00:36:03.600
You know, basic but many
people get this wrong.

00:36:03.600 --> 00:36:05.820
Your datacenters have
to be distributed.

00:36:05.820 --> 00:36:07.830
They can't be behind
the same power.

00:36:07.830 --> 00:36:09.680
They can't be behind
the same network.

00:36:09.680 --> 00:36:11.150
They can't be on the
same flood plain.

00:36:11.150 --> 00:36:12.610
They can't be on the
same fault line.

00:36:12.610 --> 00:36:14.300
It doesn't matter if you've
a hundred datacenters.

00:36:14.300 --> 00:36:17.100
If they're behind shared
infrastructure like that,

00:36:17.100 --> 00:36:19.700
they're all going to
go offline at once.

00:36:19.700 --> 00:36:22.430
Be capable of absorbing more
than one failure at a time.

00:36:22.430 --> 00:36:25.500
Having two is not enough because
you'll say well, I'm

00:36:25.500 --> 00:36:27.655
serving out of here and then oh,
I'm having problems. Let

00:36:27.655 --> 00:36:29.480
me move my traffic over here.

00:36:29.480 --> 00:36:31.850
Certainly nothing is going to
happen to this datacenter over

00:36:31.850 --> 00:36:35.400
here before you get the first
datacenter back online, right?

00:36:35.400 --> 00:36:37.600
So be capable of absorbing
more than one

00:36:37.600 --> 00:36:38.720
failure at a time.

00:36:38.720 --> 00:36:41.730
Because the law of perversity
will make sure that it'll

00:36:41.730 --> 00:36:43.790
happen to you.

00:36:43.790 --> 00:36:45.800
Write synchronously, don't
write asynchronously.

00:36:45.800 --> 00:36:47.740
You will say OK, well I can get
better performance out of

00:36:47.740 --> 00:36:50.630
it and it won't be so bad, and
my users will get used to it.

00:36:50.630 --> 00:36:53.420
And I'll find ways around it.

00:36:53.420 --> 00:36:54.920
We wanted that to be the case.

00:36:54.920 --> 00:36:56.620
It doesn't actually work.

00:36:56.620 --> 00:36:58.980
There's a ceiling to the
threshold of reliability that

00:36:58.980 --> 00:36:59.810
we can offer.

00:36:59.810 --> 00:37:02.900
We talked about App Engine
coming out of preview and how

00:37:02.900 --> 00:37:05.160
we're now going to have an
SLA for all paid users.

00:37:05.160 --> 00:37:08.020
That SLA only applies to high
replication, to apps using the

00:37:08.020 --> 00:37:10.170
high replication datastore
configuration.

00:37:10.170 --> 00:37:12.330
There's a very specific
reason for that.

00:37:12.330 --> 00:37:14.910
There's only so much reliability
we can give you

00:37:14.910 --> 00:37:16.280
given the asynchronous
replication.

00:37:16.280 --> 00:37:19.870
We try really hard and we're
spending a lot of our time

00:37:19.870 --> 00:37:21.440
always trying to improve
these things.

00:37:21.440 --> 00:37:23.560
But there's only so
much we can do.

00:37:23.560 --> 00:37:26.700
Empower the infrastructure.

00:37:26.700 --> 00:37:28.460
Don't make it only manual
intervention

00:37:28.460 --> 00:37:29.480
that changes something.

00:37:29.480 --> 00:37:33.270
Let the megastore layer, or the
writing layer, make some

00:37:33.270 --> 00:37:37.900
interesting decisions about when
to abandon writing to the

00:37:37.900 --> 00:37:39.690
local one and write
somewhere else.

00:37:39.690 --> 00:37:40.610
Don't have that be permanent.

00:37:40.610 --> 00:37:42.330
Like I said, don't make
permanently dangerous

00:37:42.330 --> 00:37:44.800
decisions based on anything.

00:37:44.800 --> 00:37:48.070
And when there is a problem,
give me the switch that I can

00:37:48.070 --> 00:37:51.330
throw that says, move this
traffic from here to here as

00:37:51.330 --> 00:37:54.730
quickly as possible, so that I
can return your application to

00:37:54.730 --> 00:37:58.590
service and we can all stand
around the problem and go,

00:37:58.590 --> 00:38:01.650
huh, we weren't expecting
that to happen.

00:38:01.650 --> 00:38:04.690
And also, if you're trying to
build all of this, a bunch of

00:38:04.690 --> 00:38:09.070
SREs really doesn't hurt who
done all these things.

00:38:09.070 --> 00:38:13.470
So this is a lot of a complexity
and people talk

00:38:13.470 --> 00:38:14.680
about the private cloud.

00:38:14.680 --> 00:38:18.800
And it's kind of difficult
unfortunately, to do all of

00:38:18.800 --> 00:38:20.185
these things on a small scale.

00:38:20.185 --> 00:38:22.470
The efficiencies only come when
you're doing them on a

00:38:22.470 --> 00:38:24.640
large scale and they're really
difficult and they're really

00:38:24.640 --> 00:38:25.250
complicated.

00:38:25.250 --> 00:38:28.350
There are unending piles of
bugs that I could tell you

00:38:28.350 --> 00:38:30.540
about that we found while
we were debugging our

00:38:30.540 --> 00:38:30.845
implementation.

00:38:30.845 --> 00:38:34.280
And we have incredibly smart
people anticipating almost all

00:38:34.280 --> 00:38:35.230
of these things.

00:38:35.230 --> 00:38:38.550
And yet still, we had a lot
of problems. So if you're

00:38:38.550 --> 00:38:42.200
interested in building an
application that is scalable

00:38:42.200 --> 00:38:47.440
and globally reliable and not
a manual pain in the ass to

00:38:47.440 --> 00:38:49.010
administer and failover.

00:38:49.010 --> 00:38:50.930
So these are about the things
you have to do.

00:38:50.930 --> 00:38:53.870
Or well, we think we have
a better option.

00:38:53.870 --> 00:38:57.680
And as people like to say,
it's available right now.

00:38:57.680 --> 00:38:59.180
And in fact, more than
be available right

00:38:59.180 --> 00:39:00.530
now, it's the default.

00:39:00.530 --> 00:39:04.520
As of App Engine 1.5.0, we have
said, high replication is

00:39:04.520 --> 00:39:04.980
the default.

00:39:04.980 --> 00:39:07.130
You have to manually make a
choice if you want master

00:39:07.130 --> 00:39:09.010
slave. We don't want
you to do that.

00:39:09.010 --> 00:39:14.570
We are cutting the prices on
master slave. I'm sorry,

00:39:14.570 --> 00:39:19.630
cutting the prices on high
replication datastore storage

00:39:19.630 --> 00:39:21.260
costs to encourage people.

00:39:21.260 --> 00:39:24.090
There are already migration
processes that we have to move

00:39:24.090 --> 00:39:27.110
your existing apps from master
slave to high replication.

00:39:27.110 --> 00:39:28.260
We are working on
improving those.

00:39:28.260 --> 00:39:30.680
That's one of our very top
priorities, and we expect to

00:39:30.680 --> 00:39:32.850
have something about that
out very shortly.

00:39:32.850 --> 00:39:35.970
Probably on the order of
weeks, not months.

00:39:35.970 --> 00:39:40.070
We want you to move your
applications over.

00:39:40.070 --> 00:39:42.360
I mean, we really
believe in this.

00:39:42.360 --> 00:39:46.440
And I hope that you've taken
away from this, this is all of

00:39:46.440 --> 00:39:51.470
the stuff that we learned
about implementing

00:39:51.470 --> 00:39:53.860
applications on top of Google's
infrastructure.

00:39:53.860 --> 00:39:57.700
And the stuff that we
experienced, taught us what we

00:39:57.700 --> 00:39:59.210
needed to do to make
things better.

00:39:59.210 --> 00:40:01.330
And we think it works fine if
your application is just

00:40:01.330 --> 00:40:05.990
starting out, it's a great
platform to design to if

00:40:05.990 --> 00:40:08.190
you've got the next great
idea that you want

00:40:08.190 --> 00:40:09.310
to build and monetize.

00:40:09.310 --> 00:40:12.440
Or, if you have already got a
tremendous amount of traffic

00:40:12.440 --> 00:40:15.500
and you want a better scaling
story then, well, I guess it's

00:40:15.500 --> 00:40:18.660
time to send somebody out to
the store to buy a bunch of

00:40:18.660 --> 00:40:21.010
more machines and put them
in the datacenter.

00:40:21.010 --> 00:40:24.220
And the beauty of this system is
again, the dynamic scaling.

00:40:24.220 --> 00:40:26.520
If you get mentioned on Oprah
in the middle of the night

00:40:26.520 --> 00:40:29.080
when you're sleeping and all of
a sudden everybody in the

00:40:29.080 --> 00:40:31.290
world wants to go to your
website or Justin Bieber

00:40:31.290 --> 00:40:34.860
twitters about it, or whatever
it is the kids do these days.

00:40:34.860 --> 00:40:39.360
Then App Engine will allow you
to scale this automatically.

00:40:39.360 --> 00:40:42.090
You'll be ready for whatever
traffic and all that will

00:40:42.090 --> 00:40:43.260
happen is we'll hand
you a slightly

00:40:43.260 --> 00:40:46.240
higher bill that month.

00:40:46.240 --> 00:40:47.640
Hope you got that monetized
properly.

00:40:47.640 --> 00:40:51.660
You know, very enthusiastic
about that.

00:40:51.660 --> 00:40:55.500
And with that comes
an army of us.

00:40:55.500 --> 00:40:58.680
We've got developers support
people, we've got forums that

00:40:58.680 --> 00:41:00.060
you can interact with.

00:41:00.060 --> 00:41:02.890
Coming out of preview, we've got
paid support options that

00:41:02.890 --> 00:41:10.010
are paid support options for
incidence and ticket responses

00:41:10.010 --> 00:41:13.960
that you can take advantage
of if you need this.

00:41:13.960 --> 00:41:16.780
We're taking the availability
of this very seriously.

00:41:16.780 --> 00:41:19.040
And you get me and my team.

00:41:19.040 --> 00:41:20.610
We're follow the sun
on-call coverage.

00:41:20.610 --> 00:41:22.660
I don't get woken up in the
middle of the night by pages.

00:41:22.660 --> 00:41:25.680
I've got people in other time
zones so that you we can all

00:41:25.680 --> 00:41:29.110
be really fresh when we're
working on these problems.

00:41:29.110 --> 00:41:30.930
It's not crisis for us.

00:41:30.930 --> 00:41:33.730
All of that stuff that I
mentioned, we'll take care of

00:41:33.730 --> 00:41:35.440
it for you.

00:41:35.440 --> 00:41:37.200
I like that kind of stuff.

00:41:37.200 --> 00:41:41.510
I mean I wake up and I go well,
something weird is going

00:41:41.510 --> 00:41:42.900
to happen to me today
most likely.

00:41:42.900 --> 00:41:44.800
And it'll probably be
pretty exciting.

00:41:44.800 --> 00:41:48.350
And I accept that there's
probably something wrong with

00:41:48.350 --> 00:41:50.870
me and my team that
like there's

00:41:50.870 --> 00:41:52.530
something wrong with us.

00:41:52.530 --> 00:41:55.320
But if you don't want to have
to do it, we'll take care of

00:41:55.320 --> 00:41:56.830
it for you.

00:41:56.830 --> 00:41:59.570
Because you can focus on what
you're excited about, your

00:41:59.570 --> 00:42:01.760
domain knowledge, your
application.

00:42:01.760 --> 00:42:04.340
On the other hand, if that kind
of stuff does excite you,

00:42:04.340 --> 00:42:08.620
I am bound by contract to say we
are hiring for SRE all over

00:42:08.620 --> 00:42:14.290
the globe, many projects,
many datacenters--

00:42:14.290 --> 00:42:15.630
not datacenters--
many offices.

00:42:15.630 --> 00:42:19.680
And there are so many things
that we could teach you.

00:42:19.680 --> 00:42:23.420
So if that interests you,
please come talk to me.

00:42:23.420 --> 00:42:25.530
So I'm folding up what
I've got to say here.

00:42:25.530 --> 00:42:28.860
And I'm going to take a moment
and get very serious.

00:42:28.860 --> 00:42:31.370
And so we need to talk a little
bit about transparency.

00:42:31.370 --> 00:42:34.750
And I don't need to tell you
all that cloud computing is

00:42:34.750 --> 00:42:36.820
big business.

00:42:36.820 --> 00:42:41.030
It's not just some website
or people can

00:42:41.030 --> 00:42:42.230
dismiss it all they want.

00:42:42.230 --> 00:42:44.610
But out there in the customer
area we've got web filings,

00:42:44.610 --> 00:42:48.580
and they're taking Fortune 100
companies, their SEC data, and

00:42:48.580 --> 00:42:50.620
getting their quarterly
filings into the SEC

00:42:50.620 --> 00:42:51.470
on time with them.

00:42:51.470 --> 00:42:54.460
That's real money, real jail
time for people for

00:42:54.460 --> 00:42:55.470
non-compliance.

00:42:55.470 --> 00:42:58.000
Congressional investigations,
et cetera.

00:42:58.000 --> 00:43:00.350
We hosted the website for the
recent royal wedding.

00:43:00.350 --> 00:43:05.280
This is once in a lifetime, or
well, maybe twice when Harry

00:43:05.280 --> 00:43:05.720
gets married.

00:43:05.720 --> 00:43:06.970
But whatever.

00:43:09.860 --> 00:43:14.450
A unique event on TV,
all eyes on it.

00:43:14.450 --> 00:43:17.270
And when Accenture
said, we need an

00:43:17.270 --> 00:43:18.270
infrastructure to host it.

00:43:18.270 --> 00:43:19.810
They said, App Engine.

00:43:19.810 --> 00:43:22.550
So there's a lot of money
coming in to this.

00:43:22.550 --> 00:43:24.890
And we know that we have to
treat this very seriously,

00:43:24.890 --> 00:43:27.300
treat it as seriously as
you treat your users.

00:43:27.300 --> 00:43:29.730
When we have problems, you
need to know about it as

00:43:29.730 --> 00:43:30.810
quickly as possible.

00:43:30.810 --> 00:43:33.250
You need to understand what
we're doing to make the App

00:43:33.250 --> 00:43:34.130
Engine more reliable.

00:43:34.130 --> 00:43:36.630
That we're working on
it very seriously.

00:43:36.630 --> 00:43:39.120
And the architectural choices
that we've made, the

00:43:39.120 --> 00:43:41.360
improvements that we're
working on.

00:43:41.360 --> 00:43:43.240
When we start having a problem,
you need to know

00:43:43.240 --> 00:43:44.880
about it as quickly
as possible.

00:43:44.880 --> 00:43:47.260
And like I said, we've got
communication, we've got all

00:43:47.260 --> 00:43:51.150
of these things, SLA for paid
apps on high replication once

00:43:51.150 --> 00:43:52.410
we come out of preview.

00:43:52.410 --> 00:43:54.740
All of this is saying, we
are going to tell you

00:43:54.740 --> 00:43:55.500
what's going on.

00:43:55.500 --> 00:43:57.020
When we have problems we'll
give you a detailed

00:43:57.020 --> 00:43:59.140
postmortem.

00:43:59.140 --> 00:44:01.410
We're bringing out the SREs to
give talks when I don't have

00:44:01.410 --> 00:44:04.580
any features tell you other than
you should use HRD and

00:44:04.580 --> 00:44:06.880
here are some hilarious
stories.

00:44:06.880 --> 00:44:09.380
So the other story about
transparency that's important

00:44:09.380 --> 00:44:12.400
is that you need to know when
you're having problems before

00:44:12.400 --> 00:44:14.190
your users know that you're
having problems. And those

00:44:14.190 --> 00:44:17.320
problems aren't necessarily all
connected to App Engine's

00:44:17.320 --> 00:44:18.200
infrastructure.

00:44:18.200 --> 00:44:20.620
They could also be related
to high load that your

00:44:20.620 --> 00:44:24.050
applications are experiencing,
or a new piece of code.

00:44:24.050 --> 00:44:28.500
And the story for monitoring App
Engine applications hasn't

00:44:28.500 --> 00:44:30.120
historically been great.

00:44:30.120 --> 00:44:32.420
That's true for most cloud
applications honestly.

00:44:32.420 --> 00:44:34.740
And this is something that we
knew that we need to improve

00:44:34.740 --> 00:44:39.070
if you're going to actually,
trust us to say we're as

00:44:39.070 --> 00:44:41.040
available as we are and give
us your money, give us your

00:44:41.040 --> 00:44:42.820
trust, give us your data.

00:44:42.820 --> 00:44:47.540
So Alan Green, an SRE engineer
out of the Sydney office has

00:44:47.540 --> 00:44:51.250
been working on improving the
monetary story for App Engine

00:44:51.250 --> 00:44:51.820
applications.

00:44:51.820 --> 00:44:53.680
And I'm going to turn this over
to him, so that he can

00:44:53.680 --> 00:44:56.246
tell you all about what
he's been working on.

00:44:56.246 --> 00:44:57.496
[APPLAUSE]

00:45:05.530 --> 00:45:06.040
ALAN GREEN: Hi.

00:45:06.040 --> 00:45:09.050
So Michael's been telling you
about how Google looks after

00:45:09.050 --> 00:45:11.180
the App Engine platform.

00:45:11.180 --> 00:45:13.560
And a key part of that is
our monitoring tools.

00:45:13.560 --> 00:45:16.200
So for App Engine we're
monitoring thousands and

00:45:16.200 --> 00:45:18.750
thousands of statistics about
all of the components that

00:45:18.750 --> 00:45:20.300
makeup App Engine.

00:45:20.300 --> 00:45:22.985
And we're able to spot problems
and hopefully, find

00:45:22.985 --> 00:45:24.720
and fix them before your
apps even notice.

00:45:27.960 --> 00:45:32.440
So I'm going to cover the
monitoring API that is going

00:45:32.440 --> 00:45:35.520
to trust tester today.

00:45:35.520 --> 00:45:36.810
I guess the question is,
why do you need to

00:45:36.810 --> 00:45:38.000
monitor your apps?

00:45:38.000 --> 00:45:39.880
Because this team of Google
SREs looking after the

00:45:39.880 --> 00:45:46.870
platform and if you're confident
in their ability to

00:45:46.870 --> 00:45:49.450
keep they platform running,
why do you even need to

00:45:49.450 --> 00:45:51.400
monitor your own apps.

00:45:51.400 --> 00:45:55.250
The answer is, for many simple
apps, you don't need to

00:45:55.250 --> 00:45:56.480
monitor them.

00:45:56.480 --> 00:45:59.455
They're so simple and they use
such a small fraction of App

00:45:59.455 --> 00:46:01.810
Engine's capabilities that so
long as the platform is

00:46:01.810 --> 00:46:04.240
stable, they're going
to keep running.

00:46:04.240 --> 00:46:08.520
But for more complex apps, the
kinds of apps that a business

00:46:08.520 --> 00:46:11.400
might depend upon for critical
functions, there's some

00:46:11.400 --> 00:46:14.250
reasons why you might monitor.

00:46:14.250 --> 00:46:18.620
So first the reason is it gives
you insight into the

00:46:18.620 --> 00:46:19.970
running of the platform.

00:46:19.970 --> 00:46:23.750
So say datastore latency
increases slightly and starts

00:46:23.750 --> 00:46:25.640
to get to the point where it's
affecting your customers.

00:46:25.640 --> 00:46:27.620
It would be good to know that
before your customers

00:46:27.620 --> 00:46:29.930
start to call you.

00:46:29.930 --> 00:46:33.590
Another reason is so that you
can get insight into your own

00:46:33.590 --> 00:46:34.870
application.

00:46:34.870 --> 00:46:37.550
And a good example of
that is mail quotas.

00:46:37.550 --> 00:46:41.680
So if app is sending mail, you
set a budget for it and App

00:46:41.680 --> 00:46:45.830
Engine lets you send mail up
until you hit that budget.

00:46:45.830 --> 00:46:47.570
But if you getting close to that
budget it would be good

00:46:47.570 --> 00:46:50.510
to know because when you
hit it, it stops.

00:46:50.510 --> 00:46:54.140
And if you know, you can do
something about it in advance.

00:46:54.140 --> 00:47:00.770
And finally, you need to monitor
because you're on the

00:47:00.770 --> 00:47:03.920
internet and every crazy
person with a computer

00:47:03.920 --> 00:47:05.765
can see your app.

00:47:05.765 --> 00:47:09.370
So Michael mentioned before
gri.pe, G-R-I-dot-P-E, who

00:47:09.370 --> 00:47:12.790
were mentioned on Oprah and
they're traffic graph kind of

00:47:12.790 --> 00:47:16.660
was going along flat at just a
couple of requests per second.

00:47:16.660 --> 00:47:19.080
And within a few minutes,
they're all of a sudden

00:47:19.080 --> 00:47:22.260
serving 250 requests
per second.

00:47:22.260 --> 00:47:24.660
And it'd be good to know when
something like that happens to

00:47:24.660 --> 00:47:25.910
your app, right?

00:47:29.180 --> 00:47:31.620
So how many people here actually
use a monitoring tool

00:47:31.620 --> 00:47:33.840
in their organisation?

00:47:33.840 --> 00:47:34.710
A few of you, right.

00:47:34.710 --> 00:47:36.020
So you'll know all this.

00:47:36.020 --> 00:47:40.570
But a monitoring tool is a
program that sits there and it

00:47:40.570 --> 00:47:42.650
pulls the things that
it monitors.

00:47:42.650 --> 00:47:45.700
They can be anything from
hardware devices to databases

00:47:45.700 --> 00:47:49.710
to servers, you're
own software.

00:47:49.710 --> 00:47:54.470
But the important thing is that
when something goes wrong

00:47:54.470 --> 00:47:57.630
it tells you about it and lets
you look at the history of

00:47:57.630 --> 00:47:59.070
what's happened to
the service.

00:47:59.070 --> 00:48:03.120
So there's quite a number of
different monitoring tools,

00:48:03.120 --> 00:48:07.440
but three of the big ones out
there are Nagios, Zabbix and

00:48:07.440 --> 00:48:08.040
HP OpenView.

00:48:08.040 --> 00:48:11.390
You might have heard
those names before.

00:48:11.390 --> 00:48:17.580
What this API does is lets you
integrate your App Engine app

00:48:17.580 --> 00:48:19.570
into the monitoring tool.

00:48:19.570 --> 00:48:21.290
So that you can see it the same
way you see everything

00:48:21.290 --> 00:48:23.980
else in your service.

00:48:23.980 --> 00:48:27.460
So driving down a little bit
more into the API, here are

00:48:27.460 --> 00:48:28.780
the things need to
know about it.

00:48:28.780 --> 00:48:29.930
It's experimental.

00:48:29.930 --> 00:48:31.800
We're putting this out there
so that we can get some

00:48:31.800 --> 00:48:33.930
feedback about whether we're
providing the right

00:48:33.930 --> 00:48:37.370
information about your apps to
you, whether we're providing

00:48:37.370 --> 00:48:38.800
it in a good form.

00:48:43.195 --> 00:48:45.880
The information that it exposes
is all the stuff

00:48:45.880 --> 00:48:47.670
that's currently on
the dashboard.

00:48:47.670 --> 00:48:50.740
So just flick over and have a
look at this app's dashboard.

00:48:54.200 --> 00:48:58.140
So the number of requests per
second coming in, all this

00:48:58.140 --> 00:48:58.890
good stuff.

00:48:58.890 --> 00:49:00.810
How many instances you're
currently running.

00:49:00.810 --> 00:49:02.860
How much memory you're
currently using.

00:49:02.860 --> 00:49:06.040
All of that now comes out
through the monitoring API, as

00:49:06.040 --> 00:49:10.120
well as all of the
quota information

00:49:10.120 --> 00:49:11.370
that's on the dashboard.

00:49:17.470 --> 00:49:25.160
Because it's a standard API,
you can use the Google API

00:49:25.160 --> 00:49:32.720
Explorer to investigate
the API.

00:49:32.720 --> 00:49:35.170
So there's two methods
we're providing.

00:49:35.170 --> 00:49:38.890
One is apps.list to list all of
the applications that you

00:49:38.890 --> 00:49:47.910
have. And it's also queued so
let's try to execute this.

00:49:47.910 --> 00:49:50.130
We see the results down here.

00:49:50.130 --> 00:49:51.530
Oh, we need to be
authenticated.

00:49:51.530 --> 00:49:52.780
We must have--

00:50:03.140 --> 00:50:04.220
there we go.

00:50:04.220 --> 00:50:06.200
These are all of the
applications that this user is

00:50:06.200 --> 00:50:11.580
allowed to see that this user
is an administrator of.

00:50:11.580 --> 00:50:12.830
So let's just take
one of these.

00:50:16.950 --> 00:50:18.680
And look at the counters.

00:50:18.680 --> 00:50:21.610
We call them counters, but
it's just the statistics.

00:50:21.610 --> 00:50:23.820
The reason for that is the
most interesting ones are

00:50:23.820 --> 00:50:25.070
usually just simple counters.

00:50:29.890 --> 00:50:32.010
And then the results
come back.

00:50:32.010 --> 00:50:34.520
Lovely JSON list. We
can skip down to an

00:50:34.520 --> 00:50:37.270
interesting one here.

00:50:37.270 --> 00:50:40.190
How many memcache calls
have been made.

00:50:40.190 --> 00:50:47.560
And today, this apps done
486,274 so far.

00:50:47.560 --> 00:50:52.290
So that's the form of API
in just two calls.

00:50:52.290 --> 00:50:53.400
Great.

00:50:53.400 --> 00:50:57.290
If you are programming to it,
here's an example in Python.

00:50:57.290 --> 00:50:58.280
Typical Google API.

00:50:58.280 --> 00:51:01.480
So you get your credentials,
you go and ask for details

00:51:01.480 --> 00:51:04.450
about the service.

00:51:04.450 --> 00:51:07.640
In this case, App Engine
monitoring version alpha 1.

00:51:07.640 --> 00:51:10.110
And then you can do things
with the API.

00:51:10.110 --> 00:51:12.800
So list all of the apps and then
list all the counters.

00:51:17.810 --> 00:51:18.110
OK.

00:51:18.110 --> 00:51:19.940
So that's that API.

00:51:19.940 --> 00:51:21.100
Very simple.

00:51:21.100 --> 00:51:23.120
Let's dive into an example.

00:51:23.120 --> 00:51:25.340
I built an app here.

00:51:25.340 --> 00:51:28.890
This is why they don't let SREs
write user facing code.

00:51:32.790 --> 00:51:34.100
So very simple app.

00:51:34.100 --> 00:51:35.280
There's three URLs.

00:51:35.280 --> 00:51:38.810
There's slash here, which
is the home URL.

00:51:38.810 --> 00:51:44.970
You can write some more news or
you can view news in a way

00:51:44.970 --> 00:51:47.170
that allows you to
page through.

00:51:57.830 --> 00:51:59.990
OK, so let's say you've got
all of this monitoring

00:51:59.990 --> 00:52:04.790
integrated, everything's happy,
it's serving away.

00:52:04.790 --> 00:52:08.000
And then one night, your pager
actually goes off.

00:52:08.000 --> 00:52:11.310
And it says, excuse me, there's
a lot of CPU usage all

00:52:11.310 --> 00:52:13.070
of a sudden.

00:52:13.070 --> 00:52:15.630
So you jump in to Zabbix and you
have a look at the graphs.

00:52:15.630 --> 00:52:18.000
This red line here is
the alerting level.

00:52:18.000 --> 00:52:21.580
So if CPU usage goes over this
line for more than a few

00:52:21.580 --> 00:52:25.870
samples, it will
sound an alert.

00:52:25.870 --> 00:52:29.940
And you can see here that CPU
usage was pegging along at

00:52:29.940 --> 00:52:34.590
under a thousand mega cycles
per minute per second for

00:52:34.590 --> 00:52:35.410
quite a long time.

00:52:35.410 --> 00:52:39.360
And then all of a sudden, it's
jumped up and it's now using

00:52:39.360 --> 00:52:44.020
33,000 mega cycles per second.

00:52:44.020 --> 00:52:46.930
Which is a lot of CPUs.

00:52:46.930 --> 00:52:50.750
So when something like this
goes wrong and CPU usage

00:52:50.750 --> 00:52:53.930
spikes like this, then either
something's changed in the

00:52:53.930 --> 00:52:58.550
code or you're getting some
different stimulus to the

00:52:58.550 --> 00:53:00.950
application.

00:53:00.950 --> 00:53:04.340
And so the next graph you might
want to look at then is

00:53:04.340 --> 00:53:06.750
how much traffic are we
getting on each of the

00:53:06.750 --> 00:53:10.150
different paths coming in
to the application.

00:53:10.150 --> 00:53:14.920
This red line here is requests
to the homepage.

00:53:14.920 --> 00:53:16.250
And you can see they're
pegging along pretty

00:53:16.250 --> 00:53:19.790
consistently at 11 or 12
requests per second.

00:53:19.790 --> 00:53:22.780
But then, just lately we've
started getting requests in

00:53:22.780 --> 00:53:25.790
for slash news, which
is different.

00:53:25.790 --> 00:53:27.860
That's a bit suspicious.

00:53:27.860 --> 00:53:31.200
Then we can dive down and have
a look at underneath the

00:53:31.200 --> 00:53:34.410
application what API
calls it's making.

00:53:34.410 --> 00:53:38.650
This graph is showing the number
of calls per second to

00:53:38.650 --> 00:53:42.900
the datastore in the green line
and to mencache in the

00:53:42.900 --> 00:53:44.160
purple line there.

00:53:44.160 --> 00:53:48.740
And memcache, it's a bit spiky,
but it's generally

00:53:48.740 --> 00:53:52.040
doing somewhere between 10
and 20 calls per seconds.

00:53:52.040 --> 00:53:52.810
That's all good.

00:53:52.810 --> 00:53:56.020
And the datastore is
doing very few

00:53:56.020 --> 00:53:59.720
calls until just recently.

00:53:59.720 --> 00:54:03.200
So that looks like it might
be the problem.

00:54:03.200 --> 00:54:05.510
And so let's put together what
we've discovered through the

00:54:05.510 --> 00:54:07.240
monitoring APIs so far.

00:54:07.240 --> 00:54:13.060
We've got a massive
increase in CPU.

00:54:13.060 --> 00:54:15.480
Massive increase in
datastore usage.

00:54:15.480 --> 00:54:20.240
No increase in memcache usage,
and a little bit of extra

00:54:20.240 --> 00:54:23.930
traffic coming in on one path.

00:54:23.930 --> 00:54:28.985
That would suggest to me that
that one path that's serving a

00:54:28.985 --> 00:54:31.870
tiny little bit of traffic is
serving it uncached, but

00:54:31.870 --> 00:54:34.500
instead going straight to the
datastore and doing something

00:54:34.500 --> 00:54:36.920
very complex.

00:54:36.920 --> 00:54:39.980
not actually caching
the result.

00:54:39.980 --> 00:54:41.970
And in fact, because I wrote
this dodgy app, I know that's

00:54:41.970 --> 00:54:44.930
exactly what's happening.

00:54:44.930 --> 00:54:47.850
Possible solutions here is find
what it is that's sending

00:54:47.850 --> 00:54:52.030
traffic to your app and modify
your code to proper use

00:54:52.030 --> 00:54:53.280
memcache for that URL.

00:54:57.300 --> 00:55:00.090
So that's pretty much the
monitoring API, what it is,

00:55:00.090 --> 00:55:01.170
and how do to use it.

00:55:01.170 --> 00:55:05.190
It exposes dashboard stats
compatible with pretty much

00:55:05.190 --> 00:55:06.210
any monitoring tool.

00:55:06.210 --> 00:55:09.040
They all require a little bit of
stitching up to make it all

00:55:09.040 --> 00:55:10.460
code together.

00:55:10.460 --> 00:55:14.430
It's powered by App Engine
because it turns out that if

00:55:14.430 --> 00:55:15.350
you want to write--

00:55:15.350 --> 00:55:17.270
at Google, if you want to write
a distributed app that

00:55:17.270 --> 00:55:20.320
works across multiple
datacenters, is highly

00:55:20.320 --> 00:55:22.360
reliable and available,
App Engine is a

00:55:22.360 --> 00:55:24.530
great way to do that.

00:55:24.530 --> 00:55:27.010
And we have a Trusted Tester
program for this.

00:55:27.010 --> 00:55:30.820
If you're monitoring currently
or thinking about monitoring

00:55:30.820 --> 00:55:34.140
and are interested in joining
up, you can go to this short

00:55:34.140 --> 00:55:37.640
URL or you can come talk to
us after the presentation.

00:55:37.640 --> 00:55:38.532
Thanks.

00:55:38.532 --> 00:55:39.782
[APPLAUSE]

00:55:44.436 --> 00:55:46.370
MICHAEL HANDLER: All right, so
we're about to fold it up

00:55:46.370 --> 00:55:49.670
here, and just so you know where
you were in case you

00:55:49.670 --> 00:55:50.650
fell asleep.

00:55:50.650 --> 00:55:54.180
This is Life in App Engine
Production Google IO 2011.

00:55:54.180 --> 00:55:56.600
I'm Michael Handler, that was
Alan Green giving you the

00:55:56.600 --> 00:55:58.080
extent of the monitoring demo.

00:55:58.080 --> 00:55:59.830
You can talk about
this, please give

00:55:59.830 --> 00:56:00.660
us feedback on this.

00:56:00.660 --> 00:56:02.640
I'd love to see what
you thought.

00:56:02.640 --> 00:56:08.110
The Trusted Tester API, we're
definitely enthusiastic about

00:56:08.110 --> 00:56:10.360
getting people into that
program and learning

00:56:10.360 --> 00:56:11.530
some more about it.

00:56:11.530 --> 00:56:13.880
There's been some really
interesting talks that

00:56:13.880 --> 00:56:15.770
unfortunately, you have to go
back in time to see them.

00:56:15.770 --> 00:56:19.220
But I've got a link to one
that's incredibly relevant.

00:56:19.220 --> 00:56:22.430
If you want even more
architectural details of the

00:56:22.430 --> 00:56:25.680
higher application datastore
configuration versus the

00:56:25.680 --> 00:56:29.350
master slave datastore
configuration, Alfred Fuller

00:56:29.350 --> 00:56:33.430
and Matt Wilder, Matt's another
SRE on my team, they

00:56:33.430 --> 00:56:35.640
gave a talk earlier today
called More 9's Please.

00:56:35.640 --> 00:56:38.510
Under the Covers of the High
Replication Data Store.

00:56:38.510 --> 00:56:40.360
They've got animated diagrams
in their PowerPoint.

00:56:40.360 --> 00:56:41.540
It's fantastic.

00:56:41.540 --> 00:56:44.450
I thoroughly recommend
you watch the video.

00:56:44.450 --> 00:56:46.720
So we've actually run the clock
out a little bit, so I'm

00:56:46.720 --> 00:56:48.480
not going to take any
questions here.

00:56:48.480 --> 00:56:50.470
But Alan and I are going
to be waiting outside.

00:56:50.470 --> 00:56:51.960
We'll take some questions
out there.

00:56:51.960 --> 00:56:55.870
And we might have some of the
other App Engine developers

00:56:55.870 --> 00:56:57.880
and SREs around if you wanted
to ask questions about

00:56:57.880 --> 00:56:58.790
production.

00:56:58.790 --> 00:57:00.990
Thank you so much for coming
and enjoy the rest of IO.

