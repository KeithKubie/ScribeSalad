WEBVTT
Kind: captions
Language: en

00:00:12.760 --> 00:00:16.136
Automation anxiety
has been spreading lately,

00:00:16.160 --> 00:00:18.816
a fear that in the future,

00:00:18.840 --> 00:00:21.296
many jobs will be performed by machines

00:00:21.320 --> 00:00:22.656
rather than human beings,

00:00:22.680 --> 00:00:25.616
given the remarkable advances
that are unfolding

00:00:25.640 --> 00:00:28.416
in artificial intelligence and robotics.

00:00:28.440 --> 00:00:31.256
What's clear is that
there will be significant change.

00:00:31.280 --> 00:00:34.896
What's less clear
is what that change will look like.

00:00:34.920 --> 00:00:39.856
My research suggests that the future
is both troubling and exciting.

00:00:39.880 --> 00:00:43.616
The threat of technological
unemployment is real,

00:00:43.640 --> 00:00:45.696
and yet it's a good problem to have.

00:00:45.720 --> 00:00:48.936
And to explain
how I came to that conclusion,

00:00:48.960 --> 00:00:51.496
I want to confront three myths

00:00:51.520 --> 00:00:55.800
that I think are currently obscuring
our vision of this automated future.

00:00:56.880 --> 00:00:59.216
A picture that we see
on our television screens,

00:00:59.240 --> 00:01:01.456
in books, in films, in everyday commentary

00:01:01.480 --> 00:01:05.176
is one where an army of robots
descends on the workplace

00:01:05.200 --> 00:01:06.576
with one goal in mind:

00:01:06.600 --> 00:01:09.096
to displace human beings from their work.

00:01:09.120 --> 00:01:11.816
And I call this the Terminator myth.

00:01:11.840 --> 00:01:15.816
Yes, machines displace
human beings from particular tasks,

00:01:15.840 --> 00:01:18.096
but they don't just
substitute for human beings.

00:01:18.120 --> 00:01:20.096
They also complement them in other tasks,

00:01:20.120 --> 00:01:23.736
making that work more valuable
and more important.

00:01:23.760 --> 00:01:27.096
Sometimes they complement
human beings directly,

00:01:27.120 --> 00:01:31.136
making them more productive
or more efficient at a particular task.

00:01:31.160 --> 00:01:35.776
So a taxi driver can use a satnav system
to navigate on unfamiliar roads.

00:01:35.800 --> 00:01:39.136
An architect can use
computer-assisted design software

00:01:39.160 --> 00:01:42.256
to design bigger,
more complicated buildings.

00:01:42.280 --> 00:01:45.976
But technological progress doesn't
just complement human beings directly.

00:01:46.000 --> 00:01:49.336
It also complements them indirectly,
and it does this in two ways.

00:01:49.360 --> 00:01:52.696
The first is if we think
of the economy as a pie,

00:01:52.720 --> 00:01:55.616
technological progress
makes the pie bigger.

00:01:55.640 --> 00:01:59.496
As productivity increases,
incomes rise and demand grows.

00:01:59.520 --> 00:02:01.296
The British pie, for instance,

00:02:01.320 --> 00:02:05.280
is more than a hundred times
the size it was 300 years ago.

00:02:05.920 --> 00:02:09.136
And so people displaced
from tasks in the old pie

00:02:09.160 --> 00:02:11.880
could find tasks to do
in the new pie instead.

00:02:12.800 --> 00:02:16.736
But technological progress
doesn't just make the pie bigger.

00:02:16.760 --> 00:02:19.616
It also changes
the ingredients in the pie.

00:02:19.640 --> 00:02:23.096
As time passes, people spend
their income in different ways,

00:02:23.120 --> 00:02:25.936
changing how they spread it
across existing goods,

00:02:25.960 --> 00:02:29.176
and developing tastes
for entirely new goods, too.

00:02:29.200 --> 00:02:30.976
New industries are created,

00:02:31.000 --> 00:02:32.816
new tasks have to be done

00:02:32.840 --> 00:02:35.376
and that means often
new roles have to be filled.

00:02:35.400 --> 00:02:36.896
So again, the British pie:

00:02:36.920 --> 00:02:39.896
300 years ago,
most people worked on farms,

00:02:39.920 --> 00:02:42.256
150 years ago, in factories,

00:02:42.280 --> 00:02:45.136
and today, most people work in offices.

00:02:45.160 --> 00:02:49.216
And once again, people displaced
from tasks in the old bit of pie

00:02:49.240 --> 00:02:52.040
could tumble into tasks
in the new bit of pie instead.

00:02:52.720 --> 00:02:56.056
Economists call these effects
complementarities,

00:02:56.080 --> 00:02:59.336
but really that's just a fancy word
to capture the different way

00:02:59.360 --> 00:03:02.496
that technological progress
helps human beings.

00:03:02.520 --> 00:03:04.616
Resolving this Terminator myth

00:03:04.640 --> 00:03:06.976
shows us that there are
two forces at play:

00:03:07.000 --> 00:03:10.536
one, machine substitution
that harms workers,

00:03:10.560 --> 00:03:13.440
but also these complementarities
that do the opposite.

00:03:13.960 --> 00:03:15.336
Now the second myth,

00:03:15.360 --> 00:03:17.640
what I call the intelligence myth.

00:03:18.440 --> 00:03:23.336
What do the tasks of driving a car,
making a medical diagnosis

00:03:23.360 --> 00:03:26.280
and identifying a bird
at a fleeting glimpse have in common?

00:03:27.280 --> 00:03:30.256
Well, these are all tasks
that until very recently,

00:03:30.280 --> 00:03:33.616
leading economists thought
couldn't readily be automated.

00:03:33.640 --> 00:03:36.816
And yet today, all of these tasks
can be automated.

00:03:36.840 --> 00:03:40.336
You know, all major car manufacturers
have driverless car programs.

00:03:40.360 --> 00:03:44.336
There's countless systems out there
that can diagnose medical problems.

00:03:44.360 --> 00:03:46.776
And there's even an app
that can identify a bird

00:03:46.800 --> 00:03:48.000
at a fleeting glimpse.

00:03:48.920 --> 00:03:53.296
Now, this wasn't simply a case of bad luck
on the part of economists.

00:03:53.320 --> 00:03:54.616
They were wrong,

00:03:54.640 --> 00:03:57.136
and the reason why
they were wrong is very important.

00:03:57.160 --> 00:03:59.416
They've fallen for the intelligence myth,

00:03:59.440 --> 00:04:02.336
the belief that machines
have to copy the way

00:04:02.360 --> 00:04:04.416
that human beings think and reason

00:04:04.440 --> 00:04:06.216
in order to outperform them.

00:04:06.240 --> 00:04:08.456
When these economists
were trying to figure out

00:04:08.480 --> 00:04:10.336
what tasks machines could not do,

00:04:10.360 --> 00:04:12.496
they imagined the only way
to automate a task

00:04:12.520 --> 00:04:14.336
was to sit down with a human being,

00:04:14.360 --> 00:04:17.896
get them to explain to you
how it was they performed a task,

00:04:17.920 --> 00:04:20.576
and then try and capture that explanation

00:04:20.600 --> 00:04:23.376
in a set of instructions
for a machine to follow.

00:04:23.400 --> 00:04:27.576
This view was popular in artificial
intelligence at one point, too.

00:04:27.600 --> 00:04:29.776
I know this because Richard Susskind,

00:04:29.800 --> 00:04:32.656
who is my dad and my coauthor,

00:04:32.680 --> 00:04:36.736
wrote his doctorate in the 1980s
on artificial intelligence and the law

00:04:36.760 --> 00:04:38.176
at Oxford University,

00:04:38.200 --> 00:04:39.776
and he was part of the vanguard.

00:04:39.800 --> 00:04:42.056
And with a professor called Phillip Capper

00:04:42.080 --> 00:04:44.176
and a legal publisher called Butterworths,

00:04:44.200 --> 00:04:50.096
they produced the world's first
commercially available

00:04:50.120 --> 00:04:52.896
artificial intelligence system in the law.

00:04:52.920 --> 00:04:55.536
This was the home screen design.

00:04:55.560 --> 00:04:58.256
He assures me this was
a cool screen design at the time.

00:04:58.280 --> 00:04:59.296
(Laughter)

00:04:59.320 --> 00:05:01.016
I've never been entirely convinced.

00:05:01.040 --> 00:05:03.656
He published it
in the form of two floppy disks,

00:05:03.680 --> 00:05:07.216
at a time where floppy disks
genuinely were floppy,

00:05:07.240 --> 00:05:09.576
and his approach was the same
as the economists':

00:05:09.600 --> 00:05:10.856
sit down with a lawyer,

00:05:10.880 --> 00:05:14.056
get her to explain to you
how it was she solved a legal problem,

00:05:14.080 --> 00:05:19.456
and then try and capture that explanation
in a set of rules for a machine to follow.

00:05:19.480 --> 00:05:23.096
In economics, if human beings
could explain themselves in this way,

00:05:23.120 --> 00:05:26.416
the tasks are called routine,
and they could be automated.

00:05:26.440 --> 00:05:28.776
But if human beings
can't explain themselves,

00:05:28.800 --> 00:05:33.056
the tasks are called non-routine,
and they're thought to be out reach.

00:05:33.080 --> 00:05:36.376
Today, that routine-nonroutine
distinction is widespread.

00:05:36.400 --> 00:05:38.456
Think how often you hear people say to you

00:05:38.480 --> 00:05:41.736
machines can only perform tasks
that are predictable or repetitive,

00:05:41.760 --> 00:05:43.656
rules-based or well-defined.

00:05:43.680 --> 00:05:46.616
Those are all just
different words for routine.

00:05:46.640 --> 00:05:50.616
And go back to those three cases
that I mentioned at the start.

00:05:50.640 --> 00:05:53.536
Those are all classic cases
of nonroutine tasks.

00:05:53.560 --> 00:05:56.536
Ask a doctor, for instance,
how she makes a medical diagnosis,

00:05:56.560 --> 00:05:59.216
and she might be able
to give you a few rules of thumb,

00:05:59.240 --> 00:06:00.896
but ultimately she'd struggle.

00:06:00.920 --> 00:06:05.736
She'd say it requires things like
creativity and judgment and intuition.

00:06:05.760 --> 00:06:08.136
And these things are
very difficult to articulate,

00:06:08.160 --> 00:06:11.256
and so it was thought these tasks
would be very hard to automate.

00:06:11.280 --> 00:06:13.816
If a human being can't explain themselves,

00:06:13.840 --> 00:06:16.736
where on earth do we begin
in writing a set of instructions

00:06:16.760 --> 00:06:17.960
for a machine to follow?

00:06:18.640 --> 00:06:21.216
Thirty years ago, this view was right,

00:06:21.240 --> 00:06:23.376
but today it's looking shaky,

00:06:23.400 --> 00:06:25.656
and in the future
it's simply going to be wrong.

00:06:25.680 --> 00:06:28.936
Advances in processing power,
in data storage capability

00:06:28.960 --> 00:06:30.616
and in algorithm design

00:06:30.640 --> 00:06:33.136
mean that this
routine-nonroutine distinction

00:06:33.160 --> 00:06:34.896
is diminishingly useful.

00:06:34.920 --> 00:06:38.176
To see this, go back to the case
of making a medical diagnosis.

00:06:38.200 --> 00:06:39.576
Earlier in the year,

00:06:39.600 --> 00:06:42.896
a team of researchers at Stanford
announced they'd developed a system

00:06:42.920 --> 00:06:45.976
which can tell you
whether or not a freckle is cancerous

00:06:46.000 --> 00:06:48.680
as accurately as leading dermatologists.

00:06:49.280 --> 00:06:50.536
How does it work?

00:06:50.560 --> 00:06:55.856
It's not trying to copy the judgment
or the intuition of a doctor.

00:06:55.880 --> 00:06:59.016
It knows or understands
nothing about medicine at all.

00:06:59.040 --> 00:07:01.616
Instead, it's running
a pattern recognition algorithm

00:07:01.640 --> 00:07:06.296
through 129,450 past cases,

00:07:06.320 --> 00:07:09.416
hunting for similarities
between those cases

00:07:09.440 --> 00:07:11.520
and the particular lesion in question.

00:07:12.080 --> 00:07:15.296
It's performing these tasks
in an unhuman way,

00:07:15.320 --> 00:07:17.656
based on the analysis
of more possible cases

00:07:17.680 --> 00:07:20.296
than any doctor could hope
to review in their lifetime.

00:07:20.320 --> 00:07:22.216
It didn't matter that that human being,

00:07:22.240 --> 00:07:25.040
that doctor, couldn't explain
how she'd performed the task.

00:07:25.640 --> 00:07:27.976
Now, there are those
who dwell upon that the fact

00:07:28.000 --> 00:07:30.296
that these machines
aren't built in our image.

00:07:30.320 --> 00:07:32.376
As an example, take IBM's Watson,

00:07:32.400 --> 00:07:37.256
the supercomputer that went
on the US quiz show "Jeopardy!" in 2011,

00:07:37.280 --> 00:07:40.296
and it beat the two
human champions at "Jeopardy!"

00:07:40.320 --> 00:07:42.016
The day after it won,

00:07:42.040 --> 00:07:45.336
The Wall Street Journal ran a piece
by the philosopher John Searle

00:07:45.360 --> 00:07:48.736
with the title "Watson
Doesn't Know It Won on 'Jeopardy!'"

00:07:48.760 --> 00:07:50.736
Right, and it's brilliant, and it's true.

00:07:50.760 --> 00:07:53.216
You know, Watson didn't
let out a cry of excitement.

00:07:53.240 --> 00:07:56.336
It didn't call up its parents
to say what a good job it had done.

00:07:56.360 --> 00:07:58.696
It didn't go down to the pub for a drink.

00:07:58.720 --> 00:08:03.176
This system wasn't trying to copy the way
that those human contestants played,

00:08:03.200 --> 00:08:04.456
but it didn't matter.

00:08:04.480 --> 00:08:06.456
It still outperformed them.

00:08:06.480 --> 00:08:08.056
Resolving the intelligence myth

00:08:08.080 --> 00:08:11.456
shows us that our limited understanding
about human intelligence,

00:08:11.480 --> 00:08:13.376
about how we think and reason,

00:08:13.400 --> 00:08:16.856
is far less of a constraint
on automation than it was in the past.

00:08:16.880 --> 00:08:18.376
What's more, as we've seen,

00:08:18.400 --> 00:08:21.816
when these machines
perform tasks differently to human beings,

00:08:21.840 --> 00:08:23.096
there's no reason to think

00:08:23.120 --> 00:08:25.656
that what human beings
are currently capable of doing

00:08:25.680 --> 00:08:27.136
represents any sort of summit

00:08:27.160 --> 00:08:30.160
in what these machines
might be capable of doing in the future.

00:08:31.040 --> 00:08:32.296
Now the third myth,

00:08:32.320 --> 00:08:34.776
what I call the superiority myth.

00:08:34.800 --> 00:08:37.016
It's often said that those who forget

00:08:37.040 --> 00:08:39.496
about the helpful side
of technological progress,

00:08:39.520 --> 00:08:42.016
those complementarities from before,

00:08:42.040 --> 00:08:45.080
are committing something
known as the lump of labor fallacy.

00:08:45.840 --> 00:08:48.135
Now, the problem is
the lump of labor fallacy

00:08:48.159 --> 00:08:49.655
is itself a fallacy,

00:08:49.679 --> 00:08:52.616
and I call this the lump
of labor fallacy fallacy,

00:08:52.640 --> 00:08:54.960
or LOLFF, for short.

00:08:56.000 --> 00:08:57.416
Let me explain.

00:08:57.440 --> 00:08:59.576
The lump of labor fallacy
is a very old idea.

00:08:59.600 --> 00:09:03.816
It was a British economist, David Schloss,
who gave it this name in 1892.

00:09:03.840 --> 00:09:06.656
He was puzzled
to come across a dock worker

00:09:06.680 --> 00:09:09.016
who had begun to use
a machine to make washers,

00:09:09.040 --> 00:09:12.360
the small metal discs
that fasten on the end of screws.

00:09:13.000 --> 00:09:16.760
And this dock worker
felt guilty for being more productive.

00:09:17.560 --> 00:09:19.736
Now, most of the time,
we expect the opposite,

00:09:19.760 --> 00:09:21.976
that people feel guilty
for being unproductive,

00:09:22.000 --> 00:09:25.016
you know, a little too much time
on Facebook or Twitter at work.

00:09:25.040 --> 00:09:27.576
But this worker felt guilty
for being more productive,

00:09:27.600 --> 00:09:29.896
and asked why, he said,
"I know I'm doing wrong.

00:09:29.920 --> 00:09:31.960
I'm taking away the work of another man."

00:09:32.760 --> 00:09:35.736
In his mind, there was
some fixed lump of work

00:09:35.760 --> 00:09:37.896
to be divided up between him and his pals,

00:09:37.920 --> 00:09:39.976
so that if he used
this machine to do more,

00:09:40.000 --> 00:09:42.016
there'd be less left for his pals to do.

00:09:42.040 --> 00:09:43.896
Schloss saw the mistake.

00:09:43.920 --> 00:09:45.776
The lump of work wasn't fixed.

00:09:45.800 --> 00:09:48.616
As this worker used the machine
and became more productive,

00:09:48.640 --> 00:09:51.616
the price of washers would fall,
demand for washers would rise,

00:09:51.640 --> 00:09:53.336
more washers would have to be made,

00:09:53.360 --> 00:09:55.456
and there'd be more work
for his pals to do.

00:09:55.480 --> 00:09:57.176
The lump of work would get bigger.

00:09:57.200 --> 00:09:59.880
Schloss called this
"the lump of labor fallacy."

00:10:00.560 --> 00:10:03.496
And today you hear people talk
about the lump of labor fallacy

00:10:03.520 --> 00:10:05.736
to think about the future
of all types of work.

00:10:05.760 --> 00:10:08.416
There's no fixed lump of work
out there to be divided up

00:10:08.440 --> 00:10:09.816
between people and machines.

00:10:09.840 --> 00:10:14.496
Yes, machines substitute for human beings,
making the original lump of work smaller,

00:10:14.520 --> 00:10:16.376
but they also complement human beings,

00:10:16.400 --> 00:10:18.496
and the lump of work
gets bigger and changes.

00:10:19.760 --> 00:10:21.376
But LOLFF.

00:10:21.400 --> 00:10:22.776
Here's the mistake:

00:10:22.800 --> 00:10:25.016
it's right to think
that technological progress

00:10:25.040 --> 00:10:27.016
makes the lump of work to be done bigger.

00:10:27.040 --> 00:10:30.056
Some tasks become more valuable.
New tasks have to be done.

00:10:30.080 --> 00:10:32.616
But it's wrong to think that necessarily,

00:10:32.640 --> 00:10:35.896
human beings will be best placed
to perform those tasks.

00:10:35.920 --> 00:10:37.536
And this is the superiority myth.

00:10:37.560 --> 00:10:40.976
Yes, the lump of work
might get bigger and change,

00:10:41.000 --> 00:10:42.976
but as machines become more capable,

00:10:43.000 --> 00:10:46.896
it's likely that they'll take on
the extra lump of work themselves.

00:10:46.920 --> 00:10:50.176
Technological progress,
rather than complement human beings,

00:10:50.200 --> 00:10:52.080
complements machines instead.

00:10:52.920 --> 00:10:55.936
To see this, go back
to the task of driving a car.

00:10:55.960 --> 00:11:00.056
Today, satnav systems
directly complement human beings.

00:11:00.080 --> 00:11:02.360
They make some
human beings better drivers.

00:11:02.920 --> 00:11:04.176
But in the future,

00:11:04.200 --> 00:11:07.296
software is going to displace
human beings from the driving seat,

00:11:07.320 --> 00:11:10.256
and these satnav systems,
rather than complement human beings,

00:11:10.280 --> 00:11:12.816
will simply make these
driverless cars more efficient,

00:11:12.840 --> 00:11:14.376
helping the machines instead.

00:11:14.400 --> 00:11:18.456
Or go to those indirect complementarities
that I mentioned as well.

00:11:18.480 --> 00:11:20.256
The economic pie may get larger,

00:11:20.280 --> 00:11:22.016
but as machines become more capable,

00:11:22.040 --> 00:11:25.183
it's possible that any new demand
will fall on goods that machines,

00:11:25.207 --> 00:11:27.856
rather than human beings,
are best placed to produce.

00:11:27.880 --> 00:11:29.776
The economic pie may change,

00:11:29.800 --> 00:11:31.696
but as machines become more capable,

00:11:31.720 --> 00:11:36.576
it's possible that they'll be best placed
to do the new tasks that have to be done.

00:11:36.600 --> 00:11:40.296
In short, demand for tasks
isn't demand for human labor.

00:11:40.320 --> 00:11:42.256
Human beings only stand to benefit

00:11:42.280 --> 00:11:46.096
if they retain the upper hand
in all these complemented tasks,

00:11:46.120 --> 00:11:49.840
but as machines become more capable,
that becomes less likely.

00:11:50.760 --> 00:11:52.776
So what do these three myths tell us then?

00:11:52.800 --> 00:11:54.496
Well, resolving the Terminator myth

00:11:54.520 --> 00:11:58.216
shows us that the future of work depends
upon this balance between two forces:

00:11:58.240 --> 00:12:01.376
one, machine substitution
that harms workers

00:12:01.400 --> 00:12:03.976
but also those complementarities
that do the opposite.

00:12:04.000 --> 00:12:08.040
And until now, this balance
has fallen in favor of human beings.

00:12:09.120 --> 00:12:10.856
But resolving the intelligence myth

00:12:10.880 --> 00:12:13.376
shows us that that first force,
machine substitution,

00:12:13.400 --> 00:12:14.696
is gathering strength.

00:12:14.720 --> 00:12:16.696
Machines, of course, can't do everything,

00:12:16.720 --> 00:12:17.976
but they can do far more,

00:12:18.000 --> 00:12:22.576
encroaching ever deeper into the realm
of tasks performed by human beings.

00:12:22.600 --> 00:12:24.496
What's more, there's no reason to think

00:12:24.520 --> 00:12:26.736
that what human beings
are currently capable of

00:12:26.760 --> 00:12:28.616
represents any sort of finishing line,

00:12:28.640 --> 00:12:30.896
that machines are going
to draw to a polite stop

00:12:30.920 --> 00:12:32.736
once they're as capable as us.

00:12:32.760 --> 00:12:34.296
Now, none of this matters

00:12:34.320 --> 00:12:37.136
so long as those helpful
winds of complementarity

00:12:37.160 --> 00:12:38.896
blow firmly enough,

00:12:38.920 --> 00:12:40.856
but resolving the superiority myth

00:12:40.880 --> 00:12:43.976
shows us that that process
of task encroachment

00:12:44.000 --> 00:12:47.936
not only strengthens
the force of machine substitution,

00:12:47.960 --> 00:12:51.296
but it wears down
those helpful complementarities too.

00:12:51.320 --> 00:12:53.256
Bring these three myths together

00:12:53.280 --> 00:12:56.216
and I think we can capture a glimpse
of that troubling future.

00:12:56.240 --> 00:12:58.256
Machines continue to become more capable,

00:12:58.280 --> 00:13:01.936
encroaching ever deeper
on tasks performed by human beings,

00:13:01.960 --> 00:13:04.536
strengthening the force
of machine substitution,

00:13:04.560 --> 00:13:08.176
weakening the force
of machine complementarity.

00:13:08.200 --> 00:13:12.496
And at some point, that balance
falls in favor of machines

00:13:12.520 --> 00:13:14.576
rather than human beings.

00:13:14.600 --> 00:13:16.336
This is the path we're currently on.

00:13:16.360 --> 00:13:19.536
I say "path" deliberately,
because I don't think we're there yet,

00:13:19.560 --> 00:13:23.200
but it is hard to avoid the conclusion
that this is our direction of travel.

00:13:24.640 --> 00:13:26.096
That's the troubling part.

00:13:26.120 --> 00:13:29.640
Let me say now why I think actually
this is a good problem to have.

00:13:30.520 --> 00:13:34.056
For most of human history,
one economic problem has dominated:

00:13:34.080 --> 00:13:38.136
how to make the economic pie
large enough for everyone to live on.

00:13:38.160 --> 00:13:40.336
Go back to the turn
of the first century AD,

00:13:40.360 --> 00:13:42.456
and if you took the global economic pie

00:13:42.480 --> 00:13:45.776
and divided it up into equal slices
for everyone in the world,

00:13:45.800 --> 00:13:47.936
everyone would get a few hundred dollars.

00:13:47.960 --> 00:13:50.720
Almost everyone lived
on or around the poverty line.

00:13:51.320 --> 00:13:53.496
And if you roll forward a thousand years,

00:13:53.520 --> 00:13:54.760
roughly the same is true.

00:13:55.680 --> 00:13:59.256
But in the last few hundred years,
economic growth has taken off.

00:13:59.280 --> 00:14:01.656
Those economic pies have exploded in size.

00:14:01.680 --> 00:14:03.736
Global GDP per head,

00:14:03.760 --> 00:14:07.136
the value of those individual
slices of the pie today,

00:14:07.160 --> 00:14:09.976
they're about 10,150 dollars.

00:14:10.000 --> 00:14:12.696
If economic growth continues
at two percent,

00:14:12.720 --> 00:14:14.776
our children will be twice as rich as us.

00:14:14.800 --> 00:14:17.096
If it continues
at a more measly one percent,

00:14:17.120 --> 00:14:19.776
our grandchildren
will be twice as rich as us.

00:14:19.800 --> 00:14:23.480
By and large, we've solved
that traditional economic problem.

00:14:24.200 --> 00:14:27.216
Now, technological unemployment,
if it does happen,

00:14:27.240 --> 00:14:30.456
in a strange way will be
a symptom of that success,

00:14:30.480 --> 00:14:34.336
will have solved one problem --
how to make the pie bigger --

00:14:34.360 --> 00:14:36.176
but replaced it with another --

00:14:36.200 --> 00:14:38.960
how to make sure
that everyone gets a slice.

00:14:39.840 --> 00:14:43.336
As other economists have noted,
solving this problem won't be easy.

00:14:43.360 --> 00:14:45.016
Today, for most people,

00:14:45.040 --> 00:14:47.536
their job is their seat
at the economic dinner table,

00:14:47.560 --> 00:14:49.976
and in a world with less work
or even without work,

00:14:50.000 --> 00:14:52.056
it won't be clear
how they get their slice.

00:14:52.080 --> 00:14:54.416
There's a great deal
of discussion, for instance,

00:14:54.440 --> 00:14:57.136
about various forms
of universal basic income

00:14:57.160 --> 00:14:58.376
as one possible approach,

00:14:58.400 --> 00:15:00.016
and there's trials underway

00:15:00.040 --> 00:15:02.440
in the United States
and in Finland and in Kenya.

00:15:03.000 --> 00:15:06.176
And this is the collective challenge
that's right in front of us,

00:15:06.200 --> 00:15:11.256
to figure out how this material prosperity
generated by our economic system

00:15:11.280 --> 00:15:13.256
can be enjoyed by everyone

00:15:13.280 --> 00:15:15.696
in a world in which
our traditional mechanism

00:15:15.720 --> 00:15:17.576
for slicing up the pie,

00:15:17.600 --> 00:15:19.536
the work that people do,

00:15:19.560 --> 00:15:21.720
withers away and perhaps disappears.

00:15:22.280 --> 00:15:26.640
Solving this problem is going to require
us to think in very different ways.

00:15:27.400 --> 00:15:31.576
There's going to be a lot of disagreement
about what ought to be done,

00:15:31.600 --> 00:15:35.016
but it's important to remember
that this is a far better problem to have

00:15:35.040 --> 00:15:37.856
than the one that haunted
our ancestors for centuries:

00:15:37.880 --> 00:15:41.256
how to make that pie
big enough in the first place.

00:15:41.280 --> 00:15:42.536
Thank you very much.

00:15:42.560 --> 00:15:46.400
(Applause)

