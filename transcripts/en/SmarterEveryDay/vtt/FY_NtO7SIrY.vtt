WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.120
- Facebook is a great way to connect.

00:00:02.120 --> 00:00:04.260
People plan events there; you can sell things.

00:00:04.260 --> 00:00:06.260
People tell me that if they advertise on Facebook,

00:00:06.260 --> 00:00:07.720
they actually get more business.

00:00:07.840 --> 00:00:11.840
But any time you combine two billion people in one location,

00:00:11.860 --> 00:00:13.660
people are going to try to exploit that

00:00:13.660 --> 00:00:15.500
for money, power, and influence.

00:00:15.500 --> 00:00:19.720
This is the third video of a series I'm doing on social media manipulation.

00:00:19.720 --> 00:00:22.740
Today we're going to be looking at the kinds of manipulation that happens on Facebook.

00:00:22.740 --> 00:00:26.680
Then we're going to go to Facebook and talk directly to the people that are trying to fight it.

00:00:26.680 --> 00:00:29.380
Because of how big and how global Facebook is,

00:00:29.380 --> 00:00:30.960
this is a very complicated issue.

00:00:30.960 --> 00:00:32.220
So we're going to start right here

00:00:32.220 --> 00:00:35.000
so that you can understand how serious this issue is.

00:00:35.000 --> 00:00:36.140
- So this is Renee DiResta.

00:00:36.140 --> 00:00:38.220
She has studied social media manipulation

00:00:38.220 --> 00:00:39.260
for years.

00:00:39.260 --> 00:00:40.920
And we're going to talk about Facebook, right?

00:00:41.020 --> 00:00:41.520
- Okay.

00:00:41.520 --> 00:00:43.500
- Do you have specific examples of things you've seen

00:00:43.500 --> 00:00:45.100
on the platform, Facebook,

00:00:45.100 --> 00:00:47.120
where people have tried to manipulate others

00:00:47.120 --> 00:00:49.000
in this coordinated inauthentic behavior?

00:00:49.000 --> 00:00:52.240
- It's such a global platform that it's a global problem.

00:00:52.240 --> 00:00:54.660
So you have things like the genocide in Myanmar

00:00:55.560 --> 00:00:57.540
through the spreading of misinformation about

00:00:57.540 --> 00:00:58.940
particular ethnic minorities, and that was—

00:00:58.940 --> 00:01:01.460
- To be clear, you're saying actual people died

00:01:01.460 --> 00:01:03.660
as a result of misinformation.

00:01:03.660 --> 00:01:05.160
- That seems to be the case, yeah.

00:01:05.260 --> 00:01:09.420
It appears that accounts, fake accounts, tied to the military

00:01:09.620 --> 00:01:12.380
were used to create,

00:01:12.720 --> 00:01:15.520
foment hatred against an ethnic minority.

00:01:15.520 --> 00:01:17.200
You hear genocide and Facebook,

00:01:17.380 --> 00:01:19.060
and you're like, "What‽"

00:01:19.060 --> 00:01:21.300
I mean, the confirmation bias might start

00:01:21.300 --> 00:01:22.960
creeping in, and you start thinking weird things.

00:01:22.960 --> 00:01:24.680
But before we get out the pitchforks,

00:01:24.680 --> 00:01:25.920
let's think about this.

00:01:25.920 --> 00:01:29.180
Most social media platforms don't have a good handle on manipulation,

00:01:29.180 --> 00:01:31.300
but this specific thing is very unique

00:01:31.300 --> 00:01:32.880
from an engineering perspective.

00:01:32.880 --> 00:01:35.980
Think about Myanmar, a country where over 100 languages are spoken

00:18:50.480 --> 00:18:54.000
People see anything they distrust or dislike, they think it's Russia.

00:18:54.000 --> 00:18:56.020
They think it's a foreign government.

00:18:56.020 --> 00:19:00.340
And we try very hard not to play in to that, and not to help with that.

00:19:00.500 --> 00:19:02.660
So on the one hand, it's important that people know the techniques,

00:19:02.660 --> 00:19:06.860
but on the other hand, you don't want to be hyperbolic about you're seeing.

00:19:07.120 --> 00:19:14.500
And you want to be as factual and specific as possible, so that people don't jump to conclusions and you don't feed into that.

00:19:14.500 --> 00:19:15.340
- And they don't freak out.

00:19:15.340 --> 00:19:16.040
- Right.

00:19:16.040 --> 00:19:18.500
- Thank you so much. You've given me a ton to think about.

00:19:18.500 --> 00:19:19.840
- Thank you! I really appreciate it.

00:19:21.640 --> 00:19:24.760
- I feel like this coordinated inauthentic behavior thing

00:19:24.760 --> 00:19:26.020
that's happening on social media

00:19:26.100 --> 00:19:28.900
I think this is, like, one of the big challenges of our day.

00:19:28.900 --> 00:19:32.480
I mean, this isn't just a Facebook or YouTube or Twitter issue, or even Reddit;

00:19:32.500 --> 00:19:35.360
this is a Hotels.com issue, an Amazon.com issue.

00:19:35.700 --> 00:19:37.400
This is a big problem

00:19:37.400 --> 00:19:38.740
and we have to develop countermeasures

00:19:38.900 --> 00:19:40.400
in order to combat this stuff.

00:19:40.400 --> 00:19:42.540
My whole point in this social media manipulation series

00:19:42.620 --> 00:19:45.340
was first, to educate about what the problem is

00:19:45.340 --> 00:19:47.520
and hopefully arm you with a little bit of knowledge,

00:19:47.520 --> 00:19:49.340
so that when you're on the Internet, you can protect yourself.

00:19:49.520 --> 00:19:51.040
Speaking of protection,

00:19:51.040 --> 00:19:53.960
let me just explain why I'm working with ExpressVPN on this.

