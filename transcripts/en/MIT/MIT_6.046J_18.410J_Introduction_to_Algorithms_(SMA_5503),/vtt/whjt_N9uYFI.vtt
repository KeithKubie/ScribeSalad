WEBVTT
Kind: captions
Language: en

00:00:07.880 --> 00:00:09.960
So my name is Erik Demaine. You should

00:00:09.970 --> 00:00:19.530
call me Erik. Welcome back to 6.046. This
is Lecture 2. And today we are going to essentially

00:00:19.530 --> 00:00:26.110
fill in some of the more mathematical underpinnings
of Lecture 1. So, Lecture 1, we just sort

00:00:26.110 --> 00:00:31.150
of barely got our feet wet with some analysis
of algorithms, insertion sort and mergesort.

00:00:31.150 --> 00:00:36.350
And we needed a couple of tools. We had this
big idea of asymptotics and forgetting about

00:00:36.350 --> 00:00:39.740
constants, just looking at the lead term.
And so, today, we're going to develop asymptotic

00:00:39.740 --> 00:00:46.649
notation so that we know that mathematically.
And we also ended up with a recurrence with

00:00:46.649 --> 00:00:50.559
mergesort, the running time of mergesort,
so we need to see how to solve recurrences.

00:00:50.559 --> 00:00:58.540
And we will do those two things today. Question?
Yes, I will speak louder. Thanks. Good. Even

00:00:58.540 --> 00:01:14.970
though I have a microphone, I am not amplified.
OK, so let's start with asymptotic notation.

00:01:14.970 --> 00:01:19.040
We have seen some basic asymptotic notation.
I am sure you have seen it in other classes

00:01:19.040 --> 00:01:27.250
before, things like big O-notation. And today
we are going to really define this rigorously

00:01:27.250 --> 00:01:40.140
so we know what is true and what is not, what
is valid and what is not. We are going to

00:01:40.140 --> 00:01:45.770
define, and unfortunately today is going to be really mathematical

00:01:45.770 --> 00:01:50.600
and really no algorithms today, which is sort
of an anticlimax. But next lecture we will

00:01:50.600 --> 00:01:58.790
talk about real algorithms and will apply
all the things we learned today to real algorithms.

00:01:58.790 --> 00:02:11.110
This is big O-notation, capital O-notation.
We have f(n)=O[g(n)]. This means that there

00:02:11.110 --> 00:02:33.250
are some suitable constants, c and n_o, such
that f is bounded by cg(n) for all sufficiently

00:02:33.250 --> 00:02:40.829
large n. So, this is pretty intuitive notion.
We have seen it before. We are going to assume

00:02:40.829 --> 00:02:48.749
that f(n) is non-negative here. And I just
want f(n) to be bounded above by g(n). We

00:02:48.749 --> 00:03:02.170
have seen a bunch of examples, but something
like 2n^2=O(n^3) defined. And roughly this

00:03:02.170 --> 00:03:07.190
means if you drop leading constants and low
order terms then this is less than or equal

00:03:07.190 --> 00:03:13.959
to that. So, big O corresponds roughly to
less than or equal to. But this is the formalization.

00:03:13.959 --> 00:03:18.999
Another way to think of it formally, a funny
thing about this notation is it is asymmetric.

00:03:18.999 --> 00:03:25.459
Normally, you think of equality being symmetric.
If A=B then B=A. But it's not true here. We

00:03:25.459 --> 00:03:32.109
do not have n^3 being big O of n^2. We don't
even have big O of n^3 equaling n^2. So, we

00:03:32.109 --> 00:03:43.110
will see exactly what that means in a second.
But before we get there, this is a bit bizarre

00:03:43.110 --> 00:03:49.200
notation and you should always think about
what it really means. Another way to think

00:03:49.200 --> 00:03:55.730
about what it really means is that f(n) is
in some set of functions that are like g.

00:03:55.730 --> 00:04:08.359
You could define big O[g(n)] to be a set of
functions, let's call it f(n), such that there

00:04:08.359 --> 00:04:20.190
exist constants. They are the same definition,
I think, fancy here, c and n_o, such that

00:04:20.190 --> 00:04:33.590
we have the bound f(n) is between zero and
cg(n). It is a bit of a long definition, and

00:04:33.590 --> 00:04:39.060
that is why we use the notation, to avoid
having to write this over and over. You can

00:04:39.060 --> 00:04:44.910
think of instead of n^2 being equal to big
O of n^3, what we really mean is that 2n^2

00:04:44.910 --> 00:04:55.720
is in the set big O(n^3). When we write equal
sign, we in some sense mean this in the set,

00:04:55.720 --> 00:04:59.310
but we are going to use equal sign. You could
write this. And occasionally you see papers

00:04:59.310 --> 00:05:05.190
that write this, but this is the notation
that we are going to use. That has the consequence

00:05:05.190 --> 00:05:15.310
the equal sign is asymmetric, just like this
operator. We have some nifty ways that we

00:05:15.310 --> 00:05:32.570
actually use big O-notation. And it is using
it as a macro. By the way, we have a lot to

00:05:32.570 --> 00:05:36.780
cover today, so I am going to go relatively
fast. If anything is unclear, just stop, ask

00:05:36.780 --> 00:05:40.720
questions, then I will slow down. Otherwise,
I will take this as all completely obvious

00:05:40.720 --> 00:05:53.750
and I can keep going at full speed. The convention,
this is intuitive, I guess, if you do some

00:05:53.750 --> 00:06:08.810
macro programming or something, but it's a
bit more mathematical. We have defined big

00:06:08.810 --> 00:06:19.220
O-notation and it equals big O of something.
And so we have only defined big O when on

00:06:19.220 --> 00:06:23.760
the equal sign we have big O of some function.
But it is useful to have some general expression

00:06:23.760 --> 00:06:33.630
on the right-hand side that involves big O.
For example, let's say we have f(n) = n^3

00:06:33.630 --> 00:06:42.530
+ O(n^2). This is attempting to get an error
bound. This is saying f(n) is basically n^3

00:06:42.530 --> 00:06:50.380
but there are these lower order terms that
are O(n^2). And so this means that there is

00:06:50.380 --> 00:07:09.530
a function, shorthand for a function, h(n)
which is in O(n^2) or equals O(n^2) such that

00:07:09.530 --> 00:07:19.860
f(n) = n^3 + h(n). It is saying that there
are some lower order terms that are bounded

00:07:19.860 --> 00:07:25.870
above by some constant times n^2 for sufficiently
large n, and that is what is here. And then

00:07:25.870 --> 00:07:32.400
f(n) equals, now this is a true equality,
n^3 plus that error term. This is very useful

00:07:32.400 --> 00:07:36.370
here. Essentially, I am expressing what the
lead constant is and then saying well, there

00:07:36.370 --> 00:07:42.220
is other stuff and it's all at most n^2. Saying
that f(n) therefore is also order n^3, but

00:07:42.220 --> 00:07:46.950
that is a bit weaker of a statement. This
is a bit more refined. We won't need to use

00:07:46.950 --> 00:07:50.590
this too often, but it is useful. Sometimes
we will see, like in last class we even had

00:07:50.590 --> 00:07:54.170
a big O inside a summation. So, you can use
them all over the place. The point is they

00:07:54.170 --> 00:08:06.550
represent some function in that set. A bit
less intuitive, and this is more subtle, is

00:08:06.550 --> 00:08:13.300
what it means to have big O on the left-hand
side. It means the same thing, but there is

00:08:13.300 --> 00:08:22.491
some convention what equality means. And this
is why equal sign is asymmetric. You should

00:08:22.491 --> 00:08:30.870
read equals like "is". Is means that everything
over here is something over here. So, there

00:08:30.870 --> 00:08:34.669
is an implicit for all on the left-hand side
and there exists on the right-hand side. This

00:08:34.669 --> 00:08:41.810
is a true statement. Anything that is n^2
+ O(n) is also O(n^2), but not the other way

00:08:41.810 --> 00:08:51.680
around. So, this is a bit asymmetric. If you
think about it, this is pretty intuitive but

00:08:51.680 --> 00:09:15.839
it is subtle so you should be careful. This
says for any expansion of the macro on the

00:09:15.839 --> 00:09:23.500
left-hand side, which should be f(n), there
is an expansion of the macro on the right-hand

00:09:23.500 --> 00:09:30.920
side such that we get equality. And what this
allows you to do is if you have a chain of

00:09:30.920 --> 00:09:36.110
equal signs relations, a chain of "is"s, then
the very first one is equal to or bounded

00:09:36.110 --> 00:09:40.060
by the very last one. So, you can chain equal
signs the way you normally would. You just

00:09:40.060 --> 00:09:52.190
cannot flip them around. Good. So, that's
big O-notation. Any questions about that?

00:09:52.190 --> 00:10:03.589
So, big O is great for expressing upper bounds.
But we also want to talk about lower bounds.

00:10:03.589 --> 00:10:06.709
For algorithms, we usually care about upper
bounds on their running time. Running times

00:10:06.709 --> 00:10:12.660
at most n^2 is at most n log n up to big O,
but sometimes we need to express functions

00:10:12.660 --> 00:10:17.779
that are at least some quantity. For example,
we will show that sorting requires at least

00:10:17.779 --> 00:10:25.230
n log n time in some model. So, we need some
other notation for that. And the notation

00:10:25.230 --> 00:10:38.769
is big Omega-notation. And it is pretty symmetric.
I will just write out the set definition here.

00:10:38.769 --> 00:10:45.620
And we are going to write f(n)= big Omega[g(n)]
to mean f(n) is at least some constant times

00:10:45.620 --> 00:11:10.829
g(n) -- -- for sufficiently large n. So, I
am basically just reversing the inequality

00:11:10.829 --> 00:11:20.890
relation between f and g, nothing surprising,
just to have it there. A random example, and

00:11:20.890 --> 00:11:26.379
now we will get a little bit more sophisticated,
root n= big Omega(lg n). And you should read

00:11:26.379 --> 00:11:34.550
this that up to constant factors root n is
at least log n for sufficiently large n. So,

00:11:34.550 --> 00:11:43.629
omega sort of corresponds to greater than
or equal to. Let me give you some analogies.

00:11:43.629 --> 00:11:50.389
We have big O, we have big omega, this is
less than or equal to, this is greater than

00:11:50.389 --> 00:12:10.470
or equal to. And I am going to fill in some
more here in a moment. It's nice to have all

00:12:10.470 --> 00:12:14.230
the usual operators we have. Normally we have
strict less than, strict greater than and

00:12:14.230 --> 00:12:19.930
equal sign. And we want those sort of analogs
in the asymptotic world where we ignore constant

00:12:19.930 --> 00:12:27.519
factors and ignore lower order terms. We have,
for example, big Theta[g(n)]. This is a capital

00:12:27.519 --> 00:12:32.879
theta which means you write the horizontal
bar in the middle as opposed to all the way

00:12:32.879 --> 00:12:38.750
through. I didn't invent Greek, so that is
the way it is. Theta means that you are less

00:12:38.750 --> 00:12:44.490
than or equal to and you are greater than
or equal to up to constant factors, so it

00:12:44.490 --> 00:12:54.680
is the inner section of these two sets, big
O and big Omega. That is sort of like equal

00:12:54.680 --> 00:12:59.930
sign but, of course, this is very different.
You have things like n^2 is big Theta of 2(n^2)

00:12:59.930 --> 00:13:09.210
because you ignore constant factors, but all
of these other relations, OK, n^2 + O(n) = Theta(n^2),

00:13:09.210 --> 00:13:13.399
but this does not hold with theta because
square root of n is really asymptotically

00:13:13.399 --> 00:13:20.269
bigger than log n. And some of the other examples
we saw like n^2 versus n^3, those don't hold

00:13:20.269 --> 00:13:35.711
with T. And we have some strict notation which
are the little o-notation and little omega-notation.

00:13:35.711 --> 00:13:44.029
There is no little theta because there is
not notion of strict equality versus unstrict

00:13:44.029 --> 00:13:49.560
equality. Little o is going to correspond
roughly to less than and little omega is going

00:13:49.560 --> 00:13:56.699
to correspond to greater than. This is a notation
you will just have to get used to. And I am

00:13:56.699 --> 00:14:01.470
not going to define it precisely here because
it is almost exactly the same. The difference

00:14:01.470 --> 00:14:06.730
is that instead of saying there exists constant
c and n_o, you have to say for every constant

00:14:06.730 --> 00:14:16.769
c there exists a constant n_o. The relationship
between f and g, this inequality must hold

00:14:16.769 --> 00:14:35.420
for all c instead of just for 1. And so n_o
can now depend on c. You can assume that really

00:14:35.420 --> 00:14:40.129
n is sufficiently large, but this gives you
a strict inequality. No matter what constant

00:14:40.129 --> 00:14:48.119
you put here, in front of g, let's say we
are doing little o, f will be still less than

00:14:48.119 --> 00:15:02.180
c times g for sufficiently large n. We have
some random examples. We are again ignoring

00:15:02.180 --> 00:15:10.360
constants. n^2 is always less than n^3 for
sufficiently large n. And it is a bit subtle

00:15:10.360 --> 00:15:16.700
here. I mean in order to prove something like
this, it will become intuitive after you manipulate

00:15:16.700 --> 00:15:23.100
it a little bit. You have to figure out what
n_o is in terms of c. I think it something

00:15:23.100 --> 00:15:35.379
like 2/c. If we have less than or equal to,
that should be right. As long n is at least

00:15:35.379 --> 00:15:40.869
this big, no matter how small of a c, you
should think of c here as being epsilon now,

00:15:40.869 --> 00:15:46.790
in the usual epsilon and deltas. No matter
how small c gets, still I can bound n^2 in

00:15:46.790 --> 00:15:54.759
terms of n^3, upper bound, but whenever you
have theta you do not have either of these

00:15:54.759 --> 00:16:06.310
relations. For example, Ωn^2 = Theta(n^2)
and it is not little o(n^2) and it not little

00:16:06.310 --> 00:16:11.029
omega(n^2) because it is exactly n^2. You
will get some sense in order relation out

00:16:11.029 --> 00:16:19.620
of this, although there are some messy behaviors
as you will see in your problem set. Any questions

00:16:19.620 --> 00:16:24.259
about asymptotic notation? That is the quick
rundown. Now we are going to use it to solve

00:16:24.259 --> 00:16:30.569
some recurrences. Although we won't use it
that much today, we will use it a lot more

00:16:30.569 --> 00:16:56.699
on Wednesday. OK. We will move onto the second
topic of today, which is solving recurrences.

00:16:56.699 --> 00:17:04.710
You have probably solved some recurrences
before in 6.042 or whatever discrete math

00:17:04.710 --> 00:17:09.600
class you have taken. We are going to do more
and have some techniques here that are particularly

00:17:09.600 --> 00:17:16.750
useful for analyzing recursive algorithms,
and we will see that mostly on Wednesday.

00:17:16.750 --> 00:17:23.390
There are three main methods that we are going
to use here for solving recurrences. The first

00:17:23.390 --> 00:17:31.500
one is the substitution method. There is no
general procedure for solving a recurrence.

00:17:31.500 --> 00:17:36.659
There is no good algorithm for solving recurrences,
unfortunately. We just have a bunch of techniques.

00:17:36.659 --> 00:17:42.030
Some of them work some of the time, and if
you are lucky yours will work for your recurrence,

00:17:42.030 --> 00:17:45.551
but it is sort of like solving an integral.
You have to just know some of them, you have

00:17:45.551 --> 00:17:50.580
to know various methods for solving them.
It is usually easy to check if you have the

00:17:50.580 --> 00:17:54.260
right answer. Just like with integrals, you
just differentiate and say oh, I got the right

00:17:54.260 --> 00:18:00.279
answer. And that is essentially the idea of
substitution method. Substitution method will

00:18:00.279 --> 00:18:09.760
always work, but unfortunately Step 1 is guess
the answer. And you have to guess it correctly.

00:18:09.760 --> 00:18:16.830
That makes it a big difficult. You don't have
to guess it completely. You can usually get

00:18:16.830 --> 00:18:20.090
away with not knowing the constant factors,
which is a good thing because we don't really

00:18:20.090 --> 00:18:24.960
care about the constant factors. You guess
the form. You say oh, it is going to be roughly

00:18:24.960 --> 00:18:30.570
n^2, and so it's some constant times n^2 presumably.
So, you guess that. We are going to figure

00:18:30.570 --> 00:18:39.520
out the constants. You try to verify whether
the recurrence satisfies this bound by induction,

00:18:39.520 --> 00:18:44.700
and that is the key. Substitution uses induction.
And from that you usually get the constants

00:18:44.700 --> 00:18:50.490
for free. You figure out what the constants
have to be in order to make this work. So,

00:18:50.490 --> 00:18:55.800
that is the general idea. You will see a few
examples of this. Actually, the same example

00:18:55.800 --> 00:19:07.350
several times. Unfortunately, this is what
you might call, I don't know. This is an algorithm,

00:19:07.350 --> 00:19:10.960
but it uses an oracle which is knowing the
right answer. But sometimes it is not too

00:19:10.960 --> 00:19:19.159
hard to guess the answer. It depends. If you
look at this recurrence, T(n) = 4T(n/2) +

00:19:19.159 --> 00:19:26.170
n, we should implicitly always have some base
case of T of some constant, usually 1 is a

00:19:26.170 --> 00:19:30.820
constant, so we don't really care about the
base case. For algorithms that is always the

00:19:30.820 --> 00:19:37.230
case. And we want to solve this thing. Does
anyone have a guess to what the solution is?

00:19:37.230 --> 00:19:42.220
Ideally someone who doesn't already know how
to solve this recurrence. OK. How many people

00:19:42.220 --> 00:19:51.120
know how to solve this recurrence? A few,
OK. And, of the rest, any guesses? If you

00:19:51.120 --> 00:19:56.200
look at what is going on here, here you have
T(n/2) and let's ignore this term more or

00:19:56.200 --> 00:20:03.630
less. We have n/2 here. If we double n and
get T(n) then we multiply the value by 4.

00:20:03.630 --> 00:20:07.320
And then there is this additive end, but that
doesn't matter so much. What function do you

00:20:07.320 --> 00:20:15.610
know that when you double the argument the
output goes up by a factor of 4? Sorry? n^2,yeah.

00:20:15.610 --> 00:20:23.640
You should think n^2 and you would be right.
But we won't prove n^2 yet. Let's prove something

00:20:23.640 --> 00:20:28.090
simpler, because it turns out proving that
it is at most n^2 is a bit of a pain. We will

00:20:28.090 --> 00:20:34.190
see that in just a few minutes. But let's
guess that T(n) = O(n^3) first because that

00:20:34.190 --> 00:20:39.950
will be easier to prove by induction. You
sort of see how it is done in the easy case,

00:20:39.950 --> 00:20:45.420
and then we will actually get the right answer,
n^2, later. I need to prove. What I am going

00:20:45.420 --> 00:20:52.380
to do is guess that T(n) is some constant
times n^3 at most, so I will be a little more

00:20:52.380 --> 00:20:58.529
precise. I cannot use the big O-notation in
the substitution method so I have to expand

00:20:58.529 --> 00:21:11.730
it out to use constants. I will show you why
in a little bit, but let me just tell you

00:21:11.730 --> 00:21:17.190
at a high level what is important in not using
big O-notation. Big O-notation is great if

00:21:17.190 --> 00:21:23.490
you have a finite chain of big O relations,
you know, n^2 is big O(n^3) is big O(n^4)

00:21:23.490 --> 00:21:29.920
is big O(n^4) is big O(n^4). That is all true.
And so you get that n^2 is big O(n^4). But

00:21:29.920 --> 00:21:34.870
if you have an infinite chain of those relations
then the first thing is not big O of the last

00:21:34.870 --> 00:21:42.299
thing. You have to be very careful. For example,
this is a total aside on the lecture notes.

00:21:42.299 --> 00:21:47.760
Suppose you want to prove that n = O(1). This
is a great relation. If it were true, every

00:21:47.760 --> 00:21:58.450
algorithm would have constant running time.
This is not true. Not in Wayne's World notation.

00:21:58.450 --> 00:22:07.190
You could "prove this by induction" by saying
well, base case is 1 = O(1). OK, that is true.

00:22:07.190 --> 00:22:14.370
And then the induction step as well, if I
know that n-1, so let's suppose that n-1 = O(1),

00:22:14.370 --> 00:22:26.750
well, that implies that n, which is (n-1)
+1, if this is O(1) and 1 = O(1), the whole

00:22:26.750 --> 00:22:32.611
thing is O(1). And that is true. If you knew
that (n-1) = O(1) and 1 = O(1) then their

00:22:32.611 --> 00:22:38.549
sum is also O(1), but this is a false proof.
You cannot induct over big Os. What is going

00:22:38.549 --> 00:22:42.370
on here is that the constants that are implicit
in here are changing. Here you have some big

00:22:42.370 --> 00:22:47.260
O of 1, here you have some big O of 1. You
are probably doubling the constant in there

00:22:47.260 --> 00:22:51.020
every time you do this relation. If you have
a finite number of doubling of constants,

00:22:51.020 --> 00:22:54.861
no big deal, it is just a constant, two the
power number of doublings. But here you are

00:22:54.861 --> 00:23:01.110
doing n doublings and that is no good. The
constant is now depending on n. So, we are

00:23:01.110 --> 00:23:05.820
avoiding this kind of problem by writing out
the constant. We have to make sure that constant

00:23:05.820 --> 00:23:15.409
doesn't change. Good. Now I have written out
the constant. I should be safe. I am assuming

00:23:15.409 --> 00:23:21.570
it for all k less than n, now I have to prove
it for k equal to n. I am going to take T(n)

00:23:21.570 --> 00:23:28.620
and just expand it. I am going to do the obvious
thing. I have this recurrence how to expand

00:23:28.620 --> 00:23:34.640
T(n). Then it involves T(n/2). And I know
some fact about T(n/2) because n/2 is less

00:23:34.640 --> 00:23:45.220
than n. So, let's expand. T(n) = 4T(n/2) +
n. And now I have an upper bound on this thing

00:23:45.220 --> 00:23:58.130
from the induction hypothesis. This is at
most 4 times c times the argument cubed plus

00:23:58.130 --> 00:24:40.909
n.

00:24:40.909 --> 00:24:52.549
Continuing on here. Let's expand this a little
bit. We have n cubed over 2 cubed. Two cubed

00:24:52.549 --> 00:25:08.480
is 8, so 4 over 8 is a half. So, we have Ωcn^3
+ n. And what I would like this to be is,

00:25:08.480 --> 00:25:14.659
so at the bottom where I would like to go
is that this is at most cn3. That is what

00:25:14.659 --> 00:25:21.120
I would like to prove to reestablish the induction
hypothesis for n. What I will do, in order

00:25:21.120 --> 00:25:27.610
to see when that is case, is just write this
as what I want, so this is sort of the desired

00:25:27.610 --> 00:25:42.250
value, cn3, minus whatever I don't want. This
is called the residual. Now I have to actually

00:25:42.250 --> 00:25:46.940
figure this out. Let's see. We have cn^3,
but only Ωcn^3 here, so I need to subtract

00:25:46.940 --> 00:25:52.909
off Ωcn^3 to get that lead term correct.
And then I have plus n and there is a minus

00:25:52.909 --> 00:26:00.260
here, so it is minus n. And that is the residual.
In order for this to be at most this, I need

00:26:00.260 --> 00:26:09.940
that the residual is non-negative. This is
if the residual part is greater than or equal

00:26:09.940 --> 00:26:16.380
to zero, which is pretty easy to do because
here I have control over c. I get to pick

00:26:16.380 --> 00:26:23.480
c to be whatever I want. And, as long as c
is at least, oh, I don't know, 2, then this

00:26:23.480 --> 00:26:29.429
is a 1 at least. Then I have n^3 should be
greater than or equal to n. And that is always

00:26:29.429 --> 00:26:43.150
the case. For example, this is true if c is
at least what n is, but let's say n is at

00:26:43.150 --> 00:26:53.630
least 1 just for kicks. So, what we have done
is proved that T(n) is at most some constant

00:26:53.630 --> 00:27:00.639
times n^3. And the constant is like 1. So,
that is an upper bound. It is not a tight

00:27:00.639 --> 00:27:06.050
upper bound. We actually believed that it
is n^2, and it is, but you have to be a little

00:27:06.050 --> 00:27:09.700
careful. This does not mean that the answer
is n^3. It just means that at most n^3 is

00:27:09.700 --> 00:27:15.000
big O(n^3). And this is a proof by induction.
Now, technically I should have put a base

00:27:15.000 --> 00:27:18.880
case in this induction, so there is a little
bit missing. The base case is pretty easy

00:27:18.880 --> 00:27:26.159
because T(1) is some constant, but it will
sort of influence things. If the base case

00:27:26.159 --> 00:27:35.330
T(1) is some constant. And what we need is
that it is at most c times one cubed, which

00:27:35.330 --> 00:27:41.669
is c. And that will be true as long as you
choose c to be sufficiently large. So, this

00:27:41.669 --> 00:27:51.470
is true if c is chosen sufficiently large.
Now, we don't care about constants, but the

00:27:51.470 --> 00:27:58.200
point is just to be a little bit careful.
It is not true that T(n) is at most 1 times

00:27:58.200 --> 00:28:02.150
n^2, even though here all we need is that
c is at least 1. For the base case to work,

00:28:02.150 --> 00:28:08.889
c actually might have to be a hundred or whatever
T(1) is. So, be a little bit careful there.

00:28:08.889 --> 00:28:13.450
It doesn't really affect the answer, usually
it won't because we have very simple base

00:28:13.450 --> 00:28:28.220
cases here. OK, so let's try to prove the
tight bound of O(n^2). I am not going to prove

00:28:28.220 --> 00:28:31.840
an omega bound, but you can prove an omega
n squared bound as well using substitution

00:28:31.840 --> 00:28:44.290
method. I will just be satisfied for now proving
an upper bound of n squared. Let's try to

00:28:44.290 --> 00:28:53.720
prove that T(n), this is the same recurrence,
I want to prove that it is O(n^2). I am going

00:28:53.720 --> 00:29:00.990
to do the same thing. And I will write a bit
faster because this is basically copying.

00:29:00.990 --> 00:29:15.950
Except now, instead of three, I have two.
Then I have T(n) = 4T(n/2) + n. I expand this

00:29:15.950 --> 00:29:27.100
T(n/2). This is at most 4c(n/2)^2 + n. And
now, instead of have 2 cubed, I have 2 squared,

00:29:27.100 --> 00:29:35.590
which is only 4. The fours cancel. I get cn^2
+ n. And if you prefer to write it as desired

00:29:35.590 --> 00:29:46.450
minus residual, then I have cn^2 - (-n). And
I want this to be non-negative. And it is

00:29:46.450 --> 00:29:51.750
damn hard for minus n to be non-negative.
If n is zero we are happy, but unfortunately

00:29:51.750 --> 00:29:58.679
this is an induction on n. It's got to hold
for all n greater than or equal to 1. This

00:29:58.679 --> 00:30:06.799
is not less than or equal to cn^2. Notice
the temptation is to write that this equals

00:30:06.799 --> 00:30:16.230
O(n^2), which is true for this one step. cn^2
- (-n), well, these are both order n, or this

00:30:16.230 --> 00:30:20.430
is order n, this is order n squared. Certainly
this thing is O(n^2), that is true, but it

00:30:20.430 --> 00:30:24.190
is not completing the induction. To complete
the induction, you have to prove the induction

00:30:24.190 --> 00:30:30.889
hypothesis for n with this constant c. Here
you are getting a constant c of like c + 1,

00:30:30.889 --> 00:30:40.690
which is not good. This is true but useless.
It does not finish the induction, so you can

00:30:40.690 --> 00:30:45.580
sort of ignore that. This proof doesn't work,
which is kind of annoying because we feel,

00:30:45.580 --> 00:30:52.889
in our heart of hearts, that T(n) = n^2. It
turns out to fix this you need to express

00:30:52.889 --> 00:30:57.630
T(n) in a slightly different form. This is,
again, divine inspiration. And, if you have

00:30:57.630 --> 00:31:02.970
a good connection to some divinity, you are
all set. [LAUGHTER] But it is a little bit

00:31:02.970 --> 00:31:12.279
harder for the rest of us mere mortals. It
turns out, and maybe you could guess this,

00:31:12.279 --> 00:31:19.549
that the idea is we want to strengthen the
induction hypothesis. We assumed this relatively

00:31:19.549 --> 00:31:23.231
weak thing, T(k) is less than or equal to
some constant times k^2. We didn't know what

00:31:23.231 --> 00:31:27.100
the constant was, that is fine, but we assumed
that there were no lower order terms. I want

00:31:27.100 --> 00:31:33.139
to look at lower order terms. Maybe they play
a role. And if you look at this progression

00:31:33.139 --> 00:31:37.220
you say, oh, well, I am getting something
like n^2 and the constants are pretty damn

00:31:37.220 --> 00:31:41.059
tight. I mean the fours are canceling and
the c just is preserved. How am I going to

00:31:41.059 --> 00:31:45.970
get rid of this lower order term plus n? Well,
maybe I could subtract off a linear term in

00:31:45.970 --> 00:31:50.830
here and, if I am lucky, it will cancel with
this one. That is all the intuition we have

00:31:50.830 --> 00:32:01.270
at this point. It turns out it works. We look
at T(n) and this is 4T(n/2) + n as usual.

00:32:01.270 --> 00:32:16.899
Now we expand a slightly messier form. We
have 4[c_1*(n/2)^2 - c_2*(n/2)] + n. This

00:32:16.899 --> 00:32:23.440
part is the same because the fours cancel
again. So, we get c_1*n^2, which is good.

00:32:23.440 --> 00:32:28.210
I mean that is sort of the form we want. Then
we have something times n, so let's figure

00:32:28.210 --> 00:32:38.389
it out. We have a plus 1 times n, so let's
write it 1 minus c_2 over 2 times n. Oops,

00:32:38.389 --> 00:32:48.059
got that wrong. There is four times a two
so, in fact, the two is upstairs. Let me double

00:32:48.059 --> 00:32:55.850
check. Right. OK. Now we can write this as
desired minus residual. And we have to be

00:32:55.850 --> 00:32:59.370
a little careful here because now we have
a stronger induction hypothesis to prove.

00:32:59.370 --> 00:33:03.190
We don't just need it is at most c_1*n^2,
which would be fine here because we could

00:33:03.190 --> 00:33:13.410
choose c_2 to be large, but what we really
need is c_1*n^2 - c_2*n, and then minus some

00:33:13.410 --> 00:33:18.500
other stuff. This is, again, desired minus
residual. And minus residual, let's see, we

00:33:18.500 --> 00:33:41.120
have a minus 1 and we have a minus c_2. That
doesn't look so happy. Plus c_2, thank you,

00:33:41.120 --> 00:33:45.769
because that again looked awfully negative.
It is plus c_2. I am getting my signs, there

00:33:45.769 --> 00:33:53.559
is a minus here and there is one minus here,
so there we go. Again, I want my residual

00:33:53.559 --> 00:34:05.899
to be greater than or equal to zero. And if
I have that I will be all set in making this

00:34:05.899 --> 00:34:18.700
inductive argument. Office hours start this
week, in case you are eager to go. They are

00:34:18.700 --> 00:34:24.119
all held in some room in Building 24, which
is roughly the midpoint between here and Stata,

00:34:24.119 --> 00:34:31.780
I think, for no particular reason. And you
can look at the Web page for details on the

00:34:31.780 --> 00:34:39.510
office hours. Continuing along, when is c_2
- 1 going to be greater than or equal to zero?

00:34:39.510 --> 00:34:46.160
Well, that is true if c_2 is at least 1, which
is no big deal. Again, we get to choose the

00:34:46.160 --> 00:34:50.040
constants however we want. It only has to
hold for some choice of constants. So, we

00:34:50.040 --> 00:34:57.359
can set c_2 greater than or equal to 1. And
then we are happy. That means this whole thing

00:34:57.359 --> 00:35:05.569
is less than or equal to c_1*n^2 - c_2*n if
c_2 is greater than or equal to 1. It is kind

00:35:05.569 --> 00:35:12.579
of funny here. This finishes the induction,
at least the induction step. We proved now

00:35:12.579 --> 00:35:18.510
that for any value of c_1, and provided c_2
is at least one. We have to be a little more

00:35:18.510 --> 00:35:23.760
careful that c_1 does actually have to be
sufficiently large. Any particular reason

00:35:23.760 --> 00:35:35.160
why? c_1 better not be negative, indeed. c_1
has to be positive for this to work, but it

00:35:35.160 --> 00:35:41.520
even has to be larger than positive depending.
Sorry. I have been going so fast, I haven't

00:35:41.520 --> 00:35:45.250
asked you questions. Now you are caught off
guard. Yeah? Because of the base case, exactly.

00:35:45.250 --> 00:35:59.700
So, the base case will have T(1) is c_1 time
1 squared minus c_2, we want to prove that

00:35:59.700 --> 00:36:06.560
it is at most this, and T(1) is some constant
we have assumed. We need to choose c_1 to

00:36:06.560 --> 00:36:11.430
be sufficiently larger than c_2, in fact,
so c_2 has to be at least 1. c_1 may have

00:36:11.430 --> 00:36:24.920
to be at least a hundred more than one if
this is sufficiently large. And sufficiently

00:36:24.920 --> 00:36:32.349
large now means with respect to c_2. You have
to be a little bit careful, but in this case

00:36:32.349 --> 00:36:39.260
it doesn't matter. Any questions about the
substitution method? That was the same example

00:36:39.260 --> 00:36:44.340
three times. In the end, it turned out we
got the right answer. But we sort of had to

00:36:44.340 --> 00:36:48.150
know the answer in order to find it, which
is a bit of a pain. It would certainly be

00:36:48.150 --> 00:36:53.030
nicer to just figure out the answer by some
procedure, and that will be the next two techniques

00:36:53.030 --> 00:37:00.730
we talk about. Sorry? How would you prove
a lower bound? I haven't tried it for this

00:37:00.730 --> 00:37:06.510
recurrence, but you should be able to do exactly
the same form. Argue that T(n) is greater

00:37:06.510 --> 00:37:15.099
than or equal to c_1*n^2 - c_2*n. I didn't
check whether that particular form will work,

00:37:15.099 --> 00:37:24.020
but I think it does. Try it. These other methods
will give you, in some sense, upper and lower

00:37:24.020 --> 00:37:28.800
bounds if you are a little bit careful. But,
to really check things, you pretty much have

00:37:28.800 --> 00:37:33.400
to do the substitution method. And you will
get some practice with that. Usually we only

00:37:33.400 --> 00:37:38.250
care about upper bounds. Proving upper bounds
like this is what we will focus on, but occasionally

00:37:38.250 --> 00:37:42.260
we need lower bounds. It is always nice to
know that you have the right answer by proving

00:37:42.260 --> 00:37:54.930
a matching lower bound. The next method we
will talk about is the recursion-tree method.

00:37:54.930 --> 00:38:04.771
And it is a particular way of adding up a
recurrence, and it is my favorite way. It

00:38:04.771 --> 00:38:09.300
usually just works. That's the great thing
about it. It provides you intuition for free.

00:38:09.300 --> 00:38:13.910
It tells you what the answer is pretty much.
It is slightly nonrigorous, this is a bit

00:38:13.910 --> 00:38:17.359
of a pain, so you have to be really careful
when you apply it. Otherwise, you might get

00:38:17.359 --> 00:38:24.180
the wrong answer. Because it involves dot,
dot, dots, our favorite three characters,

00:38:24.180 --> 00:38:29.861
but dot, dot, dots are always a little bit
nonrigorous so be careful. Technically, what

00:38:29.861 --> 00:38:32.970
you should do is find out what the answer
is with recursion-tree method. Then prove

00:38:32.970 --> 00:38:38.171
that it is actually right with the substitution
method. Usually that is not necessary, but

00:38:38.171 --> 00:38:42.420
you should at least have in your mind that
that is required rigorously. And probably

00:38:42.420 --> 00:38:46.050
the first few recurrences you solve, you should
do it that way. When you really understand

00:38:46.050 --> 00:38:50.550
the recursion-tree method, you can be a little
bit more sloppy if you are really sure you

00:38:50.550 --> 00:38:58.900
have the right answer. Let's do an example.
We saw recursion trees very briefly last time

00:38:58.900 --> 00:39:06.680
with mergesort as the intuition why it was
n log n. And, if you took an example like

00:39:06.680 --> 00:39:11.050
the one we just did with the recursion-tree
method, it is dead simple. Just to make our

00:39:11.050 --> 00:39:16.380
life harder, let's do a more complicated recursion.
Here we imagine we have some algorithm. It

00:39:16.380 --> 00:39:21.140
starts with a problem size n, it recursively
solves a problem of size n/4, it then recursively

00:39:21.140 --> 00:39:27.630
solves a problem of size n/2, and it does
n^2 work on the side without nonrecursive

00:39:27.630 --> 00:39:35.670
work. What is that? I mean that is a bit less
obvious, I would say. What we are going to

00:39:35.670 --> 00:39:44.760
do is draw a picture, and we are just going
to expand out that recursion in tree form

00:39:44.760 --> 00:39:59.230
-- -- 
and then just add everything up. We want the

00:39:59.230 --> 00:40:06.619
general picture, and the general principle
in the recursion-tree method is we just draw

00:40:06.619 --> 00:40:20.850
this as a picture. We say well, T(n) equals
the sum of n^2, T(n/4) and T(n/2). This is

00:40:20.850 --> 00:40:26.650
a weird way of writing a sum but why not write
it that way. This is going to be a tree. And

00:40:26.650 --> 00:40:33.119
it is going to be a tree by recursively expanding
each of these two leaves. I start by expanding

00:40:33.119 --> 00:40:39.510
T(n) to this, then I keep expanding, expanding,
expanding everything. Let's go one more step.

00:40:39.510 --> 00:40:51.630
We have this n^2, T(n/4), T(n/2). If we expand
one more time, this is going to be n^2 plus

00:40:51.630 --> 00:40:59.670
two things. The first thing is going to be
(n/4)^2, the second thing is going to be (n/2)^2.

00:40:59.670 --> 00:41:11.549
Plus their recursive branches. We have T(n/16)
and T(n/8). Here my arithmetic shows thin.

00:41:11.549 --> 00:41:17.280
This better be the same, T(n/8), and this
should be T(n/4), I believe. You just keep

00:41:17.280 --> 00:41:23.369
going forever, I mean, until you get down
to the base case where T is a constant. So,

00:41:23.369 --> 00:41:27.770
I am now going to skip some steps and say
dot, dot, dot. This is where you have to be

00:41:27.770 --> 00:41:40.260
careful. We have n^2, (n/4)^2, (n/2)^2. Now
this is easy because I have already done them

00:41:40.260 --> 00:41:55.059
all. (n/16)^2, (n/8)^2, (n/8)^2 again, (n/4)^2
and et cetera, dot, dot, dot, of various levels

00:41:55.059 --> 00:41:59.599
of recursion here. At the bottom, we are going
to get a bunch of constants. These are the

00:41:59.599 --> 00:42:09.440
leaves. I would like to know how many leaves
there are. One challenge is how many leaves

00:42:09.440 --> 00:42:13.730
in this tree could there be? This is a bit
subtle, unlike mergesort or unlike the previous

00:42:13.730 --> 00:42:17.780
recurrence we solved, the number of leaves
here is a bit funny because we are recursing

00:42:17.780 --> 00:42:22.000
at different speeds. This tree is going to
be much smaller than this tree. It is going

00:42:22.000 --> 00:42:26.720
to have smaller depth because it has already
done down to (n/16). Here it has only gone

00:42:26.720 --> 00:42:36.140
down to (n/4). But how many leaves are there
in this recursion tree? All I need is an upper

00:42:36.140 --> 00:42:41.819
bound, some reasonable upper bound. I can
tell you it is at most T(n^10), but that is

00:42:41.819 --> 00:42:56.950
a bit unreasonable. It should be less than
n, good. Why is it less than n? Exactly. I

00:42:56.950 --> 00:43:01.720
start with a problem of size n. And I recurse
into a problem that n/4 and a problem that

00:43:01.720 --> 00:43:09.260
says n/2. When I get down to one I stop. So,
n/4 + n/2 = æn, which is strictly less than

00:43:09.260 --> 00:43:14.690
n. So, definitely the total number of leaves
has to be at most n. If I start out with n

00:43:14.690 --> 00:43:20.800
sort of stuff and get rid of a quarter of
it and then recurse, it is definitely going

00:43:20.800 --> 00:43:29.110
to be less than n stuff at the bottom. So,
strictly less than n leaves. At this point,

00:43:29.110 --> 00:43:35.589
I have done nothing interesting. And then
the second cool idea in recursion trees is

00:43:35.589 --> 00:43:39.079
you don't just expand this tree and see what
it looks like and then say, well, God, how

00:43:39.079 --> 00:43:44.700
the hell am I going to sum that? You sum it
level by level. That is the only other idea.

00:43:44.700 --> 00:43:49.619
It usually works really, really well. Here
it is a bit complicated and I have to think

00:43:49.619 --> 00:43:56.099
a bit to figure out n^2 is n^2. That is the
first level. Easy. The second level, I have

00:43:56.099 --> 00:44:00.730
to think a lot harder. There are three kinds
of mathematicians, those who can add and those

00:44:00.730 --> 00:44:11.440
who cannot, and I am the latter kind so I
need your help. Can you add these things together?

00:44:11.440 --> 00:44:22.809
It's n^2 over something. Please? (5/16)n^2.
Now I really need your help. I think that

00:44:22.809 --> 00:44:26.470
one I could have done, but this one is a little
bit harder. I will go look at my notes while

00:44:26.470 --> 00:44:44.440
you compute that. Any answers? 73/256. Anyone
else confirm that? It seems a bit high to

00:44:44.440 --> 00:44:56.329
me. 73 does not sound right to me. 64? Closer.
It is actually important that we get this

00:44:56.329 --> 00:45:03.780
right. The 256 is correct. I can tell. Everyone
should know that 16^2 = 256. We are computer

00:45:03.780 --> 00:45:10.870
scientists. 25, good. We have two people saying
25, therefore it is correct by democracy.

00:45:10.870 --> 00:45:16.380
[LAUGHTER] 25 is also what my notes say, and
I computed it at home. (25/256)n^2 is the

00:45:16.380 --> 00:45:25.000
right answer. Now, did anyone notice something
magical about this progression? It squares

00:45:25.000 --> 00:45:33.549
each time, good. And, if we were going to
add these up, you might call it? A geometric

00:45:33.549 --> 00:45:39.140
series, very good. So, it turns out this is
geometric. And we know how to sum geometric

00:45:39.140 --> 00:46:01.000
series, at least you should. We started n^2.
We know that at the bottom, well, this is

00:46:01.000 --> 00:46:09.980
not quite a level, we get something like n,
but we are decreasing geometrically. So, the

00:46:09.980 --> 00:46:14.170
total, I mean the solution to the recurrence
is the sum of all the numbers in this tree.

00:46:14.170 --> 00:46:17.700
If we added it up level by level and then
add up all the levels that is going to give

00:46:17.700 --> 00:46:23.780
us the answer. This is the total computed
level by level. It is just a cute way to compute

00:46:23.780 --> 00:46:31.000
it. It usually gives you nice answers like
geometric answers. We have n^2(1 + 5/16 +

00:46:31.000 --> 00:46:38.609
25/256 + ...). And, if we believe in fate
and we see this three number recurrence, we

00:46:38.609 --> 00:46:46.260
know that we have the right answer. In general,
it is going to be (5/16)k, at least we hope,

00:46:46.260 --> 00:46:51.450
and so on. And it keeps going. It doesn't
go on infinitely, but let's just assume it

00:46:51.450 --> 00:46:57.829
goes on infinitely. That will be an upper
bound that goes on forever. This is all times

00:46:57.829 --> 00:47:05.140
n^2. Now, if you are going to know one thing
about geometric series, you should know that

00:47:05.140 --> 00:47:16.579
1 + Ω + º, if you sum all the powers of
2 you get 2. We are computer scientists. We

00:47:16.579 --> 00:47:22.820
have got to know at least the binary case.
This is like writing 0.1111111 in binary,

00:47:22.820 --> 00:47:32.920
actually, as 1, so this is 2. This is even
smaller. We have 5/16, that is less than a

00:47:32.920 --> 00:47:40.210
half and then we are squaring each time, so
this is even less than 2. If you want, there

00:47:40.210 --> 00:47:44.539
is a nifty formula for solving the general
geometric series, but all we need is that

00:47:44.539 --> 00:47:53.200
it is a constant. This is O(n^2). It is also
O(n^2). It is pretty obvious that it is O(n^2)

00:47:53.200 --> 00:47:58.880
because the top thing is n^2. So, there is
our lower bound of n^2. And we have it within

00:47:58.880 --> 00:48:04.069
a factor of 2, which is pretty good. You actually
get a better factor here. So, that is recursion-tree

00:48:04.069 --> 00:48:08.440
method. It is a little shaky here because
we have these dot, dot, dots, and we just

00:48:08.440 --> 00:48:12.359
believe that it is geometric. It turns out
most of the time it is geometric. No problem

00:48:12.359 --> 00:48:16.340
here. I would definitely check it with the
substitution method because this is not obvious

00:48:16.340 --> 00:48:21.440
to me that it is going to be geometric. In
the cases we will look at in a moment, it

00:48:21.440 --> 00:48:30.580
will be much clearer, so clear that we can
state a theorem that everything is working

00:48:30.580 --> 00:48:43.470
fine. And still time, good. So, that was recursion-trees.
There is one more method we are going to talk

00:48:43.470 --> 00:48:48.260
about, and you could essentially think of
it as an application of the recursion-tree

00:48:48.260 --> 00:49:02.430
method but it is made more precise. And it
is an actual theorem, whereas recursion trees,

00:49:02.430 --> 00:49:08.440
if the dot, dot, dots aren't obvious, you
better check them. The sad part about the

00:49:08.440 --> 00:49:19.120
master method is it is pretty restrictive.
It only applies to a particular family of

00:49:19.120 --> 00:49:35.809
recurrences. It should be T(n) = aT(n/b) +
f(n). Am I going to

00:49:35.809 --> 00:49:41.920
call it f? Yes, I will call it f. In particular,
it will not cover the recurrence I just solved

00:49:41.920 --> 00:49:46.869
because I was recursing on two different problems
of different sizes. Here, every problem you

00:49:46.869 --> 00:49:50.890
recurse on should be of the same size. There
are a subproblems. A way to think of this

00:49:50.890 --> 00:49:56.140
is a recursive algorithm. You have a subproblems.
Each of them is of size n/b, so the total

00:49:56.140 --> 00:50:02.680
costs will be this. Then you are doing f(n)
nonrecursive work. A few constraints. a should

00:50:02.680 --> 00:50:08.559
be at least 1, should have at least 1 recursion.
b should be strictly greater than 1. You better

00:50:08.559 --> 00:50:17.050
make the problem smaller or else it is going
to be infinity. And f should have some nice

00:50:17.050 --> 00:50:32.410
property. f(n) should be asymptotically positive.
How many people know what asymptotically positive

00:50:32.410 --> 00:50:40.039
means? No one. OK, you haven't read the textbook.
That's OK. I haven't read it either, although

00:50:40.039 --> 00:50:47.950
don't tell Charles. And he'd notice. And what
might you think asymptotically positive means?

00:50:47.950 --> 00:51:00.060
That we can do a little bit better. Sorry?
Yes, it means for large enough n, f(n) is

00:51:00.060 --> 00:51:08.700
positive. This means f(n) is greater than
zero for n, at least some n_o, so for some

00:51:08.700 --> 00:51:13.500
constant n_o. Eventually it should be positive.
I mean, we don't care about whether it's negative

00:51:13.500 --> 00:51:19.000
1 for n=1, not a big deal. It won't affect
the answer because we only care about the

00:51:19.000 --> 00:51:29.980
asympotics within. The master method, you
gave it a recurrence of this form, it tells

00:51:29.980 --> 00:51:33.920
you the answer. That is the great thing about
the master method. The annoying thing about

00:51:33.920 --> 00:51:39.579
the master method is that it has three cases.
It is a big long. It takes a little bit longer

00:51:39.579 --> 00:51:43.110
to memorize than all the others because the
others are just ideas. Here we need to actually

00:51:43.110 --> 00:51:52.390
remember a few things. Let me state the theorem.
Well, not quite yet. There is one very simple

00:51:52.390 --> 00:51:58.260
idea, which is we are going to compare this
nonrecursive work f(n) with a very particular

00:51:58.260 --> 00:52:10.391
function n^(log_b(a)). Why n^(log_b(a))? You
will see later. It turns out it is the number

00:52:10.391 --> 00:52:17.480
of leaves in the recursion tree, but that
is foreshadowing. So, it is either less, equal

00:52:17.480 --> 00:52:22.849
or bigger. And here we care about asymptotics.
And we have to be a little bit more precious

00:52:22.849 --> 00:52:27.730
about less, equal or bigger. You might think
well, it means little o, big Theta, or little

00:52:27.730 --> 00:52:32.640
omega. It would be nice if the theorem held
for all of those cases, but it leaves some

00:52:32.640 --> 00:52:41.350
gaps. Let's start with Case 1. Case 1 is when
f is smaller. And not just that it is little

00:52:41.350 --> 00:52:47.880
o, but it is actually quite a bit smaller.
It has got to be polynomially smaller than

00:52:47.880 --> 00:53:04.380
n^(log_b(a)). For some positive epsilon, the
running time should be this n to this constant

00:53:04.380 --> 00:53:11.339
log base b of a minus that epsilon, so it
is really polynomially smaller than n^(log_b(a)).

00:53:11.339 --> 00:53:14.790
We cannot handle the little o case, that's
a little bit too strong. This is saying it

00:53:14.790 --> 00:53:22.049
is really quite a bit smaller. But the answer
then is really simple, T(n) = Theta(n^(log_b(a))).

00:53:22.049 --> 00:53:36.869
Great. That is 
Case 1. Case 2 is when f(n) is pretty much

00:53:36.869 --> 00:53:49.089
equal to n^(log_b(a)). And by pretty much
equal I mean up to poly log factors. This

00:53:49.089 --> 00:53:58.000
is log base 2 of n to the power k. You should
know this notation. For example, k could be

00:53:58.000 --> 00:54:03.710
zero. And then they are equal up to constant
factors, for some k greater than or equal

00:54:03.710 --> 00:54:10.980
to zero. Less than will not work, so it is
really important that k is non-negative. It

00:54:10.980 --> 00:54:16.380
should probably be an integer. It doesn't
actually matter whether there is an integer,

00:54:16.380 --> 00:54:22.790
but there it is. It could n^(log_b(a)) times
log n or just times nothing, whatever. Again,

00:54:22.790 --> 00:54:35.470
the solution is easy here, T(n) = Theta(n^(log_b(a))*
lg^(k+1)(n)). Presumably it has to be at least

00:54:35.470 --> 00:54:49.310
times log k. It turns out it is log to the
k plus 1 of n. That is Case 2. We have one

00:54:49.310 --> 00:54:58.559
more case which is slightly more complicated.
We need to assume slightly more for Case 3.

00:54:58.559 --> 00:55:06.420
But Case 3 is roughly when f(n) grows bigger
than n^(log_b(a)). So, it should be capital

00:55:06.420 --> 00:55:16.530
Omega, here is one place where we get to use
omega, (n^(log_b(a)) + epsilon) for some positive

00:55:16.530 --> 00:55:24.160
epsilon. It should grow not just bigger but
polynomially bigger. Here it was growing just

00:55:24.160 --> 00:55:31.000
a log factor bigger, poly log, and here it
is a polynomial factor. In this case, we need

00:55:31.000 --> 00:55:38.859
another assumption about f because we worry
a little bit about how quickly f grows. We

00:55:38.859 --> 00:55:43.151
want to make sure that as you go down the
recursion f gets smaller. It would be kind

00:55:43.151 --> 00:55:48.220
of nice if f gets smaller as you go down,
otherwise you are, again, trying to sum to

00:55:48.220 --> 00:56:04.560
infinity or whatever. I see why this is for
some epsilon prime greater than zero. What

00:56:04.560 --> 00:56:10.549
I would like is that if I just sort of take
the recurrence, this T(n) and just throw in

00:56:10.549 --> 00:56:19.869
fs instead, f(n) should be somehow related
to af(n/b). What I would like is that f(n),

00:56:19.869 --> 00:56:25.339
which is at the top of the recursion tree,
should be bigger than the thing at the next

00:56:25.339 --> 00:56:31.359
level down. The sum of all the values at the
next level down should be bigger by some constant

00:56:31.359 --> 00:56:37.460
factor. Here I have the next level down is
at most some 1 - e, something strictly less

00:56:37.460 --> 00:56:43.990
than 1, some constant strictly less than 1
times the thing at the top level. I need that

00:56:43.990 --> 00:56:51.050
to make sure things are getting smaller as
I go down. Then T(n) = Theta[f(n)]. And that

00:56:51.050 --> 00:56:58.460
is the theorem. This is the master theorem
or whatever you want to call it. It is not

00:56:58.460 --> 00:57:03.049
named after some guy name Master. It is just
the master of all methods because it is very

00:57:03.049 --> 00:57:15.910
easy to apply. Let's apply it a few times.
It is a bit much to take in all at once. And

00:57:15.910 --> 00:57:21.089
then I will give you a sketch of the proof
to see that it is really not that surprising

00:57:21.089 --> 00:57:29.400
this is true if you look at the recursion-tree.
But first let's just try using it. For example,

00:57:29.400 --> 00:57:46.310
we could take T(n) = 4T(n/2) + n. This is
a, this is b, this is f(n). The first thing

00:57:46.310 --> 00:57:54.339
we should compute is n^(log_b(a)). This I
think even I can do. Log base 2 of 4. Yeah,

00:57:54.339 --> 00:58:02.950
log base 2 I can do. This is n^2. OK, so is
f(n) smaller or bigger than n^2? Well, f(n)

00:58:02.950 --> 00:58:23.119
= n. n^2 is clearly bigger by a polynomial
factor. So, we are in Case 1. What is the

00:58:23.119 --> 00:58:39.180
answer? n^2, yeah. It is T(n^(log_b(a))),
which here it is just n^2. Let's do some slight

00:58:39.180 --> 00:58:50.460
variation. I am going to keep a and b the
same and just change f. Let's say T(n) = 4T(n/2)

00:58:50.460 --> 00:59:01.380
+ n^2. This is like drill spelling. n^2 is
asymptotically the same as n^2 even up to

00:59:01.380 --> 00:59:21.289
constants. What is the answer? This is Case
2. It is slightly harder. What is k in this

00:59:21.289 --> 00:59:46.119
example? Zero. The answer is? Survey says?
n^2 log n. Good. And a couple more. T(n) = 4T(n/2)

00:59:46.119 --> 01:00:04.500
+ n^3. What is the answer? n^3. This is Case
3. I know this is pretty boring. At this point

01:00:04.500 --> 01:00:21.420
we are just applying this stupid theorem.
How about n^2/lg n? What is the answer? Good.

01:00:21.420 --> 01:00:27.901
In this case no one should answer. It is a
big tricky. I forget exactly the answer. I

01:00:27.901 --> 01:00:38.270
think it is like n^2 log log n over log n,
no? Oh, no. n^2 log log n, that's right. Yeah.

01:00:38.270 --> 01:00:42.260
But you shouldn't know that, and this doesn't
follow from the master method. This is something

01:00:42.260 --> 01:00:45.039
you would have to solve, probably with the
recursion-tree would be a good way to do this

01:00:45.039 --> 01:00:51.079
one, and you need to know some properties
of logs to know how that goes. But here the

01:00:51.079 --> 01:01:10.450
master method does not apply. And so you have
to use a different method. OK. The last thing

01:01:10.450 --> 01:01:15.250
I want to do is tell you why the master method
is true, and that makes it much more intuitive,

01:01:15.250 --> 01:01:36.579
especially using recursion-trees, why everything
works. This is a sketch of a proof, not the

01:01:36.579 --> 01:01:41.720
full thing. You should read the proof in the
textbook. It is not that much harder than

01:01:41.720 --> 01:01:45.539
what I will show, but it is good for you to
know the formal details. I don't have time

01:01:45.539 --> 01:01:51.810
here to do all of the details. I will just
tell you the salient parts. This is the proof

01:01:51.810 --> 01:02:05.309
sketch or the intuition behind the master
method. What we are going to do is just take

01:02:05.309 --> 01:02:11.380
the recursion-tree for this recurrence and
add up each level and then add up all the

01:02:11.380 --> 01:02:19.950
levels and see what we get. We start with
f(n) at the top after we have expanded one

01:02:19.950 --> 01:02:29.910
level. Then we get a different problems, each
of n/b. And after we expand them it will f(n/b)

01:02:29.910 --> 01:02:38.039
for each one. They are all the same size.
Then we expand all of those and so on, and

01:02:38.039 --> 01:02:49.991
we get another a subproblems from there. We
are going to get like f((n/b)^2). That is

01:02:49.991 --> 01:02:55.790
sort of decreasing geometrically the size,
and so on and so on and so on, until at the

01:02:55.790 --> 01:03:00.250
bottom we get constant size problems. This
is a bit special because this is the base

01:03:00.250 --> 01:03:04.869
case, but we have some other constant at the
bottom. We would like to know how many leaves

01:03:04.869 --> 01:03:10.680
there are, but that is a little bit tricky
at the moment. Let's first compute the height

01:03:10.680 --> 01:03:19.050
of this tree. Let me draw it over here. What
is the height of this tree? I start with a

01:03:19.050 --> 01:03:28.920
problem of size n. I want to get down to a
problem of size 1. How long does that take?

01:03:28.920 --> 01:03:48.819
How many levels? This is probably too easy
for some and not at your fingertips for others.

01:03:48.819 --> 01:03:54.490
Log base b of n, good. The height of this
tree is n^(log_b(a)), because it is just how

01:03:54.490 --> 01:03:59.700
many times I divide by b until I get down
to 1. That is great. Now I should be able

01:03:59.700 --> 01:04:04.769
to compute the number of leaves because I
have branching factor a, I have height h.

01:04:04.769 --> 01:04:19.529
The number of leaves is a^h, a^log_b(n). Let
me expand that a little bit. a^log_b(n), properties

01:04:19.529 --> 01:04:27.059
of logs, we can take the n downstairs and
put the a upstairs, and we get n^(log_b(a)).

01:04:27.059 --> 01:04:31.349
Our good friend n^(log_b(a)). So, that is
why Our good friend n^(log_b(a)) is so important

01:04:31.349 --> 01:04:37.820
in the master method. What we are doing is
comparing f, which is the top level, to n^(log_b(a)),

01:04:37.820 --> 01:04:41.750
which up to theta is the bottom level. Now
the leaves are all at the same level because

01:04:41.750 --> 01:04:48.180
we are decreasing at the same rate in every
branch. If I add up the cost at the bottom

01:04:48.180 --> 01:04:57.890
level, it is Theta(n^(log_b(a))). I add up
the things at the top level it is f(n), not

01:04:57.890 --> 01:05:04.480
terribly exciting. But the next level, this
is a little bit more interesting, is af(n/b),

01:05:04.480 --> 01:05:09.299
which should look familiar if you had the
master method already memorized, it is that.

01:05:09.299 --> 01:05:18.599
So, we know that af(n/b) has decreased by
some constant factor, 1-epsilon prime. We

01:05:18.599 --> 01:05:23.860
have gone down. This is a constant factor
smaller than this. And then you sum up the

01:05:23.860 --> 01:05:34.059
next level. It is going to be like a^2f(n/b^2).
I see that I actually wrote this wrong, the

01:05:34.059 --> 01:05:46.410
parentheses. Sorry about that. It is not (n/b)^2.
It is (n/b^2). So, this sequence, in Case

01:05:46.410 --> 01:05:52.180
3 at least, is decreasing geometrically. If
it is decreasing geometrically up to constant

01:05:52.180 --> 01:05:59.260
factors, it is dominated by the biggest term,
which is f(n). Therefore, in Case 3, we get

01:05:59.260 --> 01:06:05.960
Theta[f(n)]. Let's look at the other cases,
and let me adapt those cases to how much time

01:06:05.960 --> 01:06:15.819
we have left. Wow, lot's of time. Five minutes.
Tons of time. What to do? Let me write that

01:06:15.819 --> 01:06:23.490
down. Case 3, the costs decrease. Now, this
is a place I would argue where the dot, dot,

01:06:23.490 --> 01:06:31.349
dot is pretty obvious. Here, this is damn
simple, it is a^kf(n/b^k). And, in Case 3,

01:06:31.349 --> 01:06:49.630
we assume that the costs decrease geometrically
as we go down the tree. That was sort of backwards

01:06:49.630 --> 01:06:57.309
to start with Case 3. Let's do Case 1, which
is sort of the other intuitively easy case.

01:06:57.309 --> 01:07:07.660
In Case 1, we know that f(n) is polynomially
smaller than this thing. And we are sort of

01:07:07.660 --> 01:07:11.780
changing by this very simple procedure in
the middle. I am going to wave my hands if

01:07:11.780 --> 01:07:17.440
this is where you need a more formal argument.
I claim that this will increase geometrically.

01:07:17.440 --> 01:07:23.369
It has to increase geometrically because this
f(n) is polynomially smaller than this one,

01:07:23.369 --> 01:07:27.480
you are going to get various polynomials in
the middle which interpret geometrically from

01:07:27.480 --> 01:07:32.180
the small one to the big one. Therefore, the
big one dominates because it is, again, geometric

01:07:32.180 --> 01:07:40.210
series. As I said, this is intuition, not
a formal argument. This one was pretty formal

01:07:40.210 --> 01:07:45.950
because we assumed it, but here you need a
bit more argument. They may not increase geometrically

01:07:45.950 --> 01:07:55.010
but they could increase faster, and that is
also fine. So, in Case 3, you are dominated,

01:07:55.010 --> 01:08:01.339
I mean you are always dominated by the biggest
term in a geometric series. Here it happens

01:08:01.339 --> 01:08:20.960
to be f(n) and here you are dominated by n^(log_b(a))
with a bottom term, oh, Theta. Case 2, here

01:08:20.960 --> 01:08:26.109
it is pretty easy but you need to know some
properties of logs. In Case 2, we assume that

01:08:26.109 --> 01:08:31.620
all of these are basically the same. I mean,
we assume that the top is equal to the bottom.

01:08:31.620 --> 01:08:35.740
And this is changing in this very procedural
way. Therefore, all of the ones in the middle

01:08:35.740 --> 01:08:42.259
have to be pretty much the same. Not quite
because here we don't have the log factor.

01:08:42.259 --> 01:08:47.980
Here we have a log to the k. We have n^(log_b(a))
times log to the kn. Here we don't have the

01:08:47.980 --> 01:08:52.690
log to the k. So, the logs do disappear here.
It turns out the way they disappear is pretty

01:08:52.690 --> 01:09:01.750
slowly. If you look at the top half of these
terms, they will all have log to the k. The

01:09:01.750 --> 01:09:07.421
bottom half they will start to disappear.
I am giving you some oracle information. If

01:09:07.421 --> 01:09:15.500
you take logs and you don't change the argument
by too much, the logs remain. Maybe halfway

01:09:15.500 --> 01:09:22.120
is too far. The claim is that each level is
roughly the same, especially the upper most

01:09:22.120 --> 01:09:35.859
levels are all asymptotically equal. Roughly
the same. And, therefore, the cost is one

01:09:35.860 --> 01:09:48.210
level, here like f(n) times the number of
levels, h. And h is log base b of n. B is

01:09:48.210 --> 01:10:01.659
a constant so we don't care. This is Theta(lg
n). And, therefore, we get T(n) = (n^(log_b(a))

01:10:01.659 --> 01:10:10.739
lg^(k+1)(n)) times another log n. So, we get
[f(n)lg n]. That is the very quick sketch.

01:10:10.739 --> 01:10:15.300
Sorry, I am being pretty fuzzy on Cases 1
and 2. Read the proof because you will have

01:10:15.300 --> 01:10:20.460
to, at some point, manipulate logs in that
way. And that is all. Any questions? Or, you

01:10:20.460 --> 01:10:24.090
are all eager to go. OK. Thanks. See you Wednesday.

