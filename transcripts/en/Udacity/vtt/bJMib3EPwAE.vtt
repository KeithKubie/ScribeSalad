WEBVTT
Kind: captions
Language: en

00:00:00.360 --> 00:00:03.390
Charles I'm going to say that you
invented an algorithm just now

00:00:03.390 --> 00:00:05.270
that we're going to
call policy iteration.

00:00:05.270 --> 00:00:06.500
It's kind of like value iteration,

00:00:06.500 --> 00:00:08.170
except it's going to
iterate a policy space.

00:00:08.170 --> 00:00:09.450
&gt;&gt; I'm so smart
&gt;&gt; Yeah,

00:00:09.450 --> 00:00:11.540
though you invented it
a little bit to late,

00:00:11.540 --> 00:00:15.605
because it dates back to 1960 or
thereabout.

00:00:15.605 --> 00:00:16.195
&gt;&gt; Dijkstra?

00:00:16.195 --> 00:00:18.255
Dijkstra always has ideas before I do.

00:00:18.255 --> 00:00:19.335
&gt;&gt; Yes.
Dijkstra's pretty clever, but

00:00:19.335 --> 00:00:20.895
this is actually Ron Howard.

00:00:20.895 --> 00:00:21.795
&gt;&gt; You mean the Ron Howard?

00:00:21.795 --> 00:00:22.655
The director?

00:00:22.655 --> 00:00:23.175
&gt;&gt; Oh, sorry.

00:00:23.175 --> 00:00:24.795
No.
It's the Ron Howard.

00:00:24.795 --> 00:00:26.773
The algorithm designer.

00:00:26.773 --> 00:00:29.295
[LAUGH]
&gt;&gt; Oh, that one.

00:00:29.295 --> 00:00:30.505
I always get them confused.

00:00:30.505 --> 00:00:32.100
&gt;&gt; One played Opie.

00:00:32.100 --> 00:00:34.750
&gt;&gt; And the other one proved
contraction mappings.

00:00:34.750 --> 00:00:35.710
&gt;&gt; They're very similar things.

00:00:35.710 --> 00:00:37.690
&gt;&gt; So this is what I was
imagining you were explaining.

00:00:37.690 --> 00:00:39.080
Take it step by step.

00:00:39.080 --> 00:00:42.010
So what we're going to do is we're
going to start off picking an arbitrary

00:00:42.010 --> 00:00:43.810
Q function like we often do.

00:00:43.810 --> 00:00:46.170
We'll call that the initialization step.

00:00:46.170 --> 00:00:48.270
Then, we're going to
iterate the following.

00:00:48.270 --> 00:00:51.100
We're going to take the t-th Q function.

00:00:51.100 --> 00:00:53.260
And compute its greedy policy.

00:00:53.260 --> 00:00:56.200
Call that pi sub t, then, that policy,

00:00:56.200 --> 00:01:01.340
we're going to evaluate it to get
a new Q function, Q 2 plus 1, and

00:01:01.340 --> 00:01:04.519
then we're going to repeat, and iterate
this over and over and over again.

00:01:04.519 --> 00:01:08.383
So each time we go around this loop,
we're taking our previous Q function.

00:01:08.383 --> 00:01:11.550
&gt;&gt; Finding it's policy,
taking that policy,

00:01:11.550 --> 00:01:13.620
finding its value function and
repeating.

00:01:13.620 --> 00:01:14.730
&gt;&gt; Lather, rinse, repeat.

00:01:14.730 --> 00:01:16.050
&gt;&gt; Exactly.

00:01:16.050 --> 00:01:20.255
So unlike when I take a shower, we
actually get convergence in finite time.

00:01:20.255 --> 00:01:23.630
&gt;&gt; [LAUGH]
&gt;&gt; So

00:01:23.630 --> 00:01:28.540
in particular the sequence of Q
functions that we get converges to Q*.

00:01:28.540 --> 00:01:31.460
Which is good,
that's like how policy iteration works.

00:01:31.460 --> 00:01:35.360
But even better, convergence is
exact and complete in finite time.

00:01:35.360 --> 00:01:38.870
I guess that was kind of true
of value iteration as well.

00:01:38.870 --> 00:01:42.480
And it converges at least as
fast as value iterationm in that

00:01:42.480 --> 00:01:46.590
if at any point we sync up the Q
functions, we start value iteration and

00:01:46.590 --> 00:01:48.750
policy iteration from
the same Q function.

00:01:48.750 --> 00:01:52.970
Then each step that policy iteration
takes is moving us towards

00:01:52.970 --> 00:01:59.250
the optimal Q function, no more
slowly than valued iteration does.

00:01:59.250 --> 00:01:59.840
&gt;&gt; Okay.

00:01:59.840 --> 00:02:02.670
&gt;&gt; So that kind of suggests
that this is just way better.

00:02:02.670 --> 00:02:03.740
&gt;&gt; Yeah, isn't it way better?

00:02:03.740 --> 00:02:05.440
&gt;&gt; So, why is it not way better?

00:02:05.440 --> 00:02:09.508
There's kind of some excitement going
on in here that we need to take

00:02:09.508 --> 00:02:10.380
into consideration.

00:02:10.380 --> 00:02:12.356
There is a bit of trade-off
as you like to say.

00:02:12.356 --> 00:02:14.510
Mm-hm.
What's the trade-off?

00:02:14.510 --> 00:02:15.680
&gt;&gt; So where's the trade-off here?

00:02:15.680 --> 00:02:22.950
We're getting faster convergence at the
cost of greater computational expense.

00:02:22.950 --> 00:02:26.360
So in particular this step,
this policy evaluation step that says

00:02:26.360 --> 00:02:30.075
take the policy and then work out
the Q function for that policy.

00:02:30.075 --> 00:02:33.420
You can do that by say solving
a system of linear equations.

00:02:33.420 --> 00:02:36.080
Or perhaps more commonly,

00:02:36.080 --> 00:02:38.730
by writing something like
value iteration to completion.

00:02:40.270 --> 00:02:43.090
So in the inner loop of policy
iteration is something that's

00:02:43.090 --> 00:02:44.970
an awful lot like value iteration.

00:02:44.970 --> 00:02:46.040
And so maybe it's not so

00:02:46.040 --> 00:02:48.720
surprising that it goes at least
as fast as value iteration.

00:02:50.360 --> 00:02:52.600
It's doing a lot more work
than value iteration.

00:02:52.600 --> 00:02:55.390
Each iteration of policy iteration
is doing pretty much all the work

00:02:55.390 --> 00:02:55.960
of value iteration.

00:02:55.960 --> 00:02:58.450
Yeah, well, so, it just depends
upon what you're counting.

00:02:58.450 --> 00:03:00.050
I say we just count the outer loop.

00:03:00.050 --> 00:03:01.660
Then we win.

00:03:01.660 --> 00:03:03.250
Or at least we don't lose.

00:03:03.250 --> 00:03:06.070
In fact, this is kind of
an interesting outstanding question.

00:03:06.070 --> 00:03:09.710
So, we don't really know how many
iterations policy iteration takes, so

00:03:09.710 --> 00:03:12.430
an open question is what
the convergence time really is.

00:03:12.430 --> 00:03:15.640
We know a couple things about it,
but it turns out to be fairly weak.

00:03:15.640 --> 00:03:17.950
We know that there are some MDPs.

00:03:17.950 --> 00:03:22.470
Such that the number of iterations,
the policy iteration takes, is linear.

00:03:22.470 --> 00:03:27.330
It's at least as large as the number
of states in the MBP, though

00:03:27.330 --> 00:03:32.220
I don't think anybody's actually shown
like two times the number of states.

00:03:32.220 --> 00:03:35.100
So all we know is something really,
really basic which is that it

00:03:35.100 --> 00:03:38.690
takes at least the number of states
iterations, in theworst case.

00:03:38.690 --> 00:03:41.420
And we know it can't be
any worse than number of

00:03:41.420 --> 00:03:44.380
actions raised to the number
of states an exponential.

00:03:44.380 --> 00:03:47.450
But where is it's in between
we don't really know.

00:03:47.450 --> 00:03:50.950
And so if it's closer to linear,
then it's totally awesome and

00:03:50.950 --> 00:03:53.310
it blows the doors off value iteration.

00:03:53.310 --> 00:03:57.940
If it's more like exponential,
then It's probably still better than

00:03:57.940 --> 00:04:01.420
value iteration, but
it's definitely more of a wash.

