1
00:00:00,960 --> 00:00:02,460
The Joe Rogan experience.

2
00:00:02,760 --> 00:00:05,400
No one one scared me more than
anyone that I've ever read.

3
00:00:05,610 --> 00:00:07,410
I read about this thing that I'm,

4
00:00:08,040 --> 00:00:12,150
Darpa was putting together a
robot called the eater robot,

5
00:00:12,300 --> 00:00:13,860
e a t our robot.

6
00:00:14,790 --> 00:00:18,060
It's a robot that fuels
itself on a biological matter,

7
00:00:18,570 --> 00:00:20,400
so it essentially can eat bodies.

8
00:00:20,910 --> 00:00:23,700
So you've got a murderous
robot that eats people.

9
00:00:24,350 --> 00:00:29,350
There is fuel was kind of things
that human beings could achieve.

10
00:00:31,050 --> 00:00:35,810
It's like people are sat
around Trynna yeah, come up
with them. Well, they're the,

11
00:00:35,820 --> 00:00:36,810
you know,
they,

12
00:00:36,870 --> 00:00:40,440
they're responsible for a lot of really
crazy innovation in terms of like

13
00:00:40,441 --> 00:00:43,770
military stuff, you know, but
Boston dynamics, you know,

14
00:00:43,771 --> 00:00:48,030
they're the ones that make those crazy
robots and they work with Darpa and those

15
00:00:48,031 --> 00:00:51,150
are the ones that make those robots that
you can't kick over. Right. You know?

16
00:00:51,151 --> 00:00:51,391
I mean,

17
00:00:51,391 --> 00:00:54,270
that's what you need one of those that
eats people and you send them to the

18
00:00:54,271 --> 00:00:56,210
battlefield. Kicky Anova. No,

19
00:00:56,220 --> 00:00:59,060
that was the first thing we
established is you can't kick out over.

20
00:00:59,670 --> 00:01:00,720
I just think that's,

21
00:01:00,750 --> 00:01:05,480
that's the big fear is that future
warfare will be our robots versus their

22
00:01:05,481 --> 00:01:06,570
robots.
You know,

23
00:01:07,140 --> 00:01:12,140
if we're starting to bring about the
worst aspects are the worst things that a

24
00:01:12,331 --> 00:01:16,770
human being can conceive of that
channel them through into reality. Yeah.

25
00:01:16,820 --> 00:01:18,910
It does make you feel that
there are pockets is real.

26
00:01:18,930 --> 00:01:23,310
I thought it was bad enough when
in the malaise of my younger days,

27
00:01:23,520 --> 00:01:25,860
I see like a for a,
wow.

28
00:01:25,861 --> 00:01:29,790
Imagine if there was a cleaning service
where the person would come around and

29
00:01:29,791 --> 00:01:33,420
clean dressed scantily.
They do that. They do that.

30
00:01:33,570 --> 00:01:35,820
Why have a devious shit?
You can dream up.

31
00:01:35,821 --> 00:01:40,050
Someone's trying to turn a buck off it
and they've taken it to the extent of the

32
00:01:40,051 --> 00:01:44,610
non kick over the robot.
Fleshy in robots. Yeah. Yeah.

33
00:01:44,611 --> 00:01:48,600
What is this? James is a new one.
New video today. God, watch this.

34
00:01:48,810 --> 00:01:51,630
This is so scary. This
is Boston dynamics. Yeah.

35
00:01:52,290 --> 00:01:55,260
There's something very eerie
about that type of motion.

36
00:01:55,290 --> 00:01:58,860
You know that the way that the movement
of a snake is deeply coded to be

37
00:01:58,861 --> 00:02:01,560
unpleasant when you see it.
Let's talk about your thing that moved me.

38
00:02:01,561 --> 00:02:05,880
Think that ain't good for the
truck. It's telling, wow. Oh my God,

39
00:02:05,881 --> 00:02:08,460
they're pulling a truck when
he's tiny little Tutsis,

40
00:02:08,480 --> 00:02:11,400
those they're that strong to get
pull a truck and a little robots.

41
00:02:11,430 --> 00:02:13,230
That's a giant ass truck.
I mean,

42
00:02:13,240 --> 00:02:17,580
he's also just a Husky sled might
have expensive robots and a truck.

43
00:02:17,740 --> 00:02:22,650
They've spent a lot of time and endeavor
to go backwards, I guess kind of.

44
00:02:22,651 --> 00:02:26,490
But to an Asia Santa close, they're
showing how strong these things are.

45
00:02:26,850 --> 00:02:31,650
I don't like, I don't like their
gate, Joe. That's an unpleasant gate.

46
00:02:31,830 --> 00:02:35,010
Yeah, you should be. You should be
uncomfortable with it. Yeah, that's not,

47
00:02:35,011 --> 00:02:38,030
I'm not at ease with that.
It's not good.

48
00:02:38,520 --> 00:02:43,500
It's not animal and there's no compassion
in it. This is, it's, it's feeling lists,

49
00:02:44,010 --> 00:02:45,120
but that's what you got to worry about.

50
00:02:45,121 --> 00:02:48,090
If you've ever see that episode of black
mirror or the lady gets chased down by

51
00:02:48,091 --> 00:02:52,140
the drones, I have nothing that
will, they've won with a BS. No.

52
00:02:52,141 --> 00:02:54,090
There's a woman who's being haunted.

53
00:02:54,120 --> 00:02:59,120
She's being hunted by a robot and it's
terrifying because if it's remorseless

54
00:02:59,230 --> 00:03:02,950
lack of humanity and empathy, just
like that looks just like those things.

55
00:03:03,190 --> 00:03:07,090
Those are real. Charlie Brooker. He,
yeah, he intubate. That man's got good.

56
00:03:07,091 --> 00:03:09,850
Imagine that. He's amazing. He's
amazing. That show. It was fantastic,

57
00:03:09,880 --> 00:03:11,440
but these things,
look,

58
00:03:11,441 --> 00:03:15,430
what we have to worry
about is once artificial
intelligence become sentient and

59
00:03:15,431 --> 00:03:20,260
you can somehow or another attachment to
these objects that move and they run on

60
00:03:20,261 --> 00:03:22,570
solar power or they have you know,

61
00:03:22,571 --> 00:03:26,530
nuclear fuel cells or some crazy shit
that allows them to exist for a long

62
00:03:26,531 --> 00:03:27,364
period of time.

63
00:03:27,880 --> 00:03:30,910
Maybe you don't have to worry about
them contaminating environments.

64
00:03:30,911 --> 00:03:33,160
If your plan of killing everybody
in the environment, aw man,

65
00:03:33,240 --> 00:03:36,370
I know that there's no means of
regulation. Is there because this,

66
00:03:36,371 --> 00:03:40,590
because this is the apex of
human endeavor there in Wa.

67
00:03:40,620 --> 00:03:44,020
What can govern that?
What can regulate it and like you say,

68
00:03:44,021 --> 00:03:47,770
there'll be a Chinese equivalent
for any of this stuff.

69
00:03:47,771 --> 00:03:50,860
There's nothing that's above it guy and
is this a good idea? Should we pull back?

70
00:03:51,070 --> 00:03:52,110
What did he do?

71
00:03:52,810 --> 00:03:55,990
He just pulled up a thing
that said they're making
that now you can cut that one

72
00:03:55,991 --> 00:03:57,030
I just showed you.
They're good.

73
00:03:57,070 --> 00:03:59,200
Are a hundred different models
of it are going to be available.

74
00:03:59,201 --> 00:04:02,560
Starting production this summer.
Doesn't say how much they're going. Wow.

75
00:04:03,700 --> 00:04:04,900
But available for people to buy.

76
00:04:05,200 --> 00:04:08,080
Well it says a hundred different
models has produced a hundred models.

77
00:04:08,110 --> 00:04:10,330
That probably means it'll
produce a hundred of them.

78
00:04:11,380 --> 00:04:13,300
Like a hundred different
companies are going to want them.

79
00:04:13,630 --> 00:04:16,570
But I bet it's more than that. Yeah.
Depending about how much they cost.

80
00:04:16,630 --> 00:04:18,730
It doesn't say how much it's gonna cost.
They're going down to that later.

81
00:04:18,731 --> 00:04:22,990
But there's showed uh oh like a robot
arm come. Although it looks so creepy.

82
00:04:23,020 --> 00:04:25,990
Look at that thing. Imagine we have one
of those things in the room filming.

83
00:04:26,410 --> 00:04:29,500
We should get one. No, I'm taking
notes. One day we come here,

84
00:04:29,501 --> 00:04:32,050
it's got red eyes and it's
like, fuck you. Fuck you.

85
00:04:32,080 --> 00:04:36,240
What were the first ones to
help it be friendly chat.

86
00:04:36,241 --> 00:04:40,600
Can they listen to us like
Alexa? It begins, isn't it?

87
00:04:40,840 --> 00:04:44,170
There's something arachnoid and
now that it's almost like you know,

88
00:04:44,171 --> 00:04:48,310
not your church and see if this tuned
into their DMT component of what we've

89
00:04:48,311 --> 00:04:52,300
been talking about. It's almost as if
we've already experienced to this reality.

90
00:04:52,300 --> 00:04:56,660
We've already been through the version
where those evil insectoid robots take

91
00:04:56,661 --> 00:05:00,360
over. So when we see on the screen and
say, oh no, we're doing that thing,

92
00:05:01,060 --> 00:05:04,930
that thing where we create those
things that bring about destruction.

93
00:05:05,170 --> 00:05:10,170
And I believe it's because we've become
biased to commerce and a particular type

94
00:05:11,351 --> 00:05:12,184
of progress.

95
00:05:12,260 --> 00:05:17,140
But one narrative has succeeded because
we necessarily have to throw off

96
00:05:17,141 --> 00:05:20,020
religion but you know,
at the dawn of the secular age,

97
00:05:20,021 --> 00:05:24,940
because religion was becoming systems of
bias and systems of oppression and what

98
00:05:25,120 --> 00:05:29,480
systems of a but what do I want to say?
Elevate in certain types of power.

99
00:05:29,481 --> 00:05:32,020
One supporting it at least lets go
hang on a minute. This religion,

100
00:05:32,040 --> 00:05:33,220
a lot of it seems like bullshit.

101
00:05:33,400 --> 00:05:37,570
What we've done is we've abandoned the
sacred and I think if you abandon the

102
00:05:37,571 --> 00:05:40,750
sacred meaning there is more to
life than what we can understand.

103
00:05:40,750 --> 00:05:43,420
I listened to the Brian Cox
episode enough, spoke to Brian Cox,

104
00:05:43,421 --> 00:05:46,540
the British physicist,
astrophysicist on my show as well.

105
00:05:46,810 --> 00:05:48,700
And when he talks about like he said that,
you know,

106
00:05:48,701 --> 00:05:51,610
we know that there's not some additional
component to a human being because we

107
00:05:51,611 --> 00:05:54,970
can break down everything that happens
when you move an arm, you know, whatever.

108
00:05:54,971 --> 00:05:59,630
And I feel like we only have limited,
we only have limited instruments.

109
00:05:59,631 --> 00:06:01,820
There's certain frequencies
that we simply cannot read.

110
00:06:01,821 --> 00:06:05,030
What else is going on when people are
having these transcendent psychedelic

111
00:06:05,031 --> 00:06:08,090
experiences,
we're accessing elements of consciousness,

112
00:06:08,120 --> 00:06:12,290
energies and frequencies that we are
not able to access while we're in this

113
00:06:12,291 --> 00:06:15,230
state and everything were achieving
and everything we're building,

114
00:06:15,231 --> 00:06:18,680
we're building on this platform and the
bias of this platform is towards prior

115
00:06:18,740 --> 00:06:23,740
progress and materialism and I think
the result is fleshy in robots and those

116
00:06:23,991 --> 00:06:27,950
evil monkey warrior.
So Jess and we might want to calm down.

117
00:06:28,260 --> 00:06:31,070
I have a little talk about what it
is we're trying to design. Yeah.

118
00:06:31,071 --> 00:06:34,610
I don't know if I agree with Brian on
that particular point that we think we

119
00:06:34,611 --> 00:06:37,850
know everything about where
consciousness emanates.

120
00:06:38,120 --> 00:06:39,620
I don't think that's necessarily true,

121
00:06:39,621 --> 00:06:42,440
but I like the fact that he thinks
that way because he's such a rigid,

122
00:06:42,441 --> 00:06:46,640
hard liner for science. And
Yeah, we've got works at cern.
I mean, he's a brilliant,

123
00:06:46,641 --> 00:06:48,290
brilliant man.
So of course he thinks that way.

124
00:06:48,530 --> 00:06:51,330
I also don't think he's ever
had a DMT experience. Yeah.

