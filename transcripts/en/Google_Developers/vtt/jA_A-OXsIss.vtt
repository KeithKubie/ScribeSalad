WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.935
JAY JUDKOWITZ: This is a
presentation on persistent

00:00:01.935 --> 00:00:05.030
disk, or as we affectionately
call it, PD.

00:00:05.030 --> 00:00:06.430
My name is Jay Judkowitz.

00:00:06.430 --> 00:00:07.765
I'm a product manager for PD.

00:00:07.765 --> 00:00:10.840
This is Andrew Kadatch,
our tech lead.

00:00:10.840 --> 00:00:13.320
And it's a 30 minute
presentation where we'll go

00:00:13.320 --> 00:00:17.010
through what persistent disk is,
what we use it for, how it

00:00:17.010 --> 00:00:20.786
works, what are some of the main
beneficial properties to

00:00:20.786 --> 00:00:22.400
you as developers.

00:00:22.400 --> 00:00:23.960
And then, we're going to spend
most of our time, actually

00:00:23.960 --> 00:00:26.070
Andrew, talking about
the best practices--

00:00:26.070 --> 00:00:29.710
how you get the most out of it,
how you protect it, how

00:00:29.710 --> 00:00:32.290
you get some advanced
use out of it.

00:00:32.290 --> 00:00:36.210
And then, we'll try to save 10
minutes at the end for Q&amp;A.

00:00:36.210 --> 00:00:39.380
So let's jump right into it.

00:00:39.380 --> 00:00:41.440
So the first thing we like to
say we talk about persistent

00:00:41.440 --> 00:00:45.910
disk is talk about the context
of the rest of cloud storage.

00:00:45.910 --> 00:00:48.800
A lot of our cloud storage
offerings are actually managed

00:00:48.800 --> 00:00:52.880
data, whether it's a blob, or
a NoSQL database with data

00:00:52.880 --> 00:00:56.660
store, or Cloud SQL, which
is kind of a MySQL

00:00:56.660 --> 00:00:59.380
implementation, or Big Query.

00:00:59.380 --> 00:01:01.360
But every now and then, you
don't want managed data.

00:01:01.360 --> 00:01:03.740
Every now and then, you want to
manage the data yourself.

00:01:03.740 --> 00:01:05.870
And you need raw
block devices.

00:01:05.870 --> 00:01:09.900
So Google has two block devices
for this purpose.

00:01:09.900 --> 00:01:12.160
One is the scratch devices,
which we used to call

00:01:12.160 --> 00:01:13.580
ephemeral disk.

00:01:13.580 --> 00:01:16.090
Those live and die with VMs.

00:01:16.090 --> 00:01:17.050
A VM is created.

00:01:17.050 --> 00:01:18.480
You get the scratch disk.

00:01:18.480 --> 00:01:19.720
And then, when you
kill the VM, the

00:01:19.720 --> 00:01:21.100
scratch disk goes away.

00:01:21.100 --> 00:01:24.250
Persistent disk is,
well, persistent.

00:01:24.250 --> 00:01:26.720
You create it independently of
a virtual machine, mount it

00:01:26.720 --> 00:01:28.930
and unmount it from the virtual
machine as you need,

00:01:28.930 --> 00:01:31.850
and can mount it to a new
VM if the first VM dies.

00:01:31.850 --> 00:01:35.160
Basically, persistent disk is a
cloud-wide SAN for your use.

00:01:37.840 --> 00:01:40.730
Three primary use cases
for persistent disk.

00:01:40.730 --> 00:01:43.610
First is as a boot device.

00:01:43.610 --> 00:01:46.640
So you can boot off of
persistent disk if you have a

00:01:46.640 --> 00:01:50.630
stateful application, one that
you need its configuration to

00:01:50.630 --> 00:01:55.340
survive after you shut it down
and turn it on many times.

00:01:55.340 --> 00:01:59.010
The more common use case right
now is as a data drive.

00:01:59.010 --> 00:02:01.700
So regardless of whether you're
booting off of scratch

00:02:01.700 --> 00:02:06.430
or persistent disk, if you're
running a data store like HDFS

00:02:06.430 --> 00:02:09.910
file system for Hadoop, if
you're running an NFS server,

00:02:09.910 --> 00:02:16.580
a MySQL database, a NoSQL
database like Mongo or

00:02:16.580 --> 00:02:20.140
Cassandra, having a persistent
disk as a data drive is the

00:02:20.140 --> 00:02:21.980
way to keep that data
living across

00:02:21.980 --> 00:02:23.850
instantiations of that database.

00:02:23.850 --> 00:02:26.300
And obviously, to keep it highly
available so you never

00:02:26.300 --> 00:02:28.900
lose that data.

00:02:28.900 --> 00:02:32.530
The last use case is actually
really unique to Google.

00:02:32.530 --> 00:02:34.270
And let me start with
the use case here.

00:02:34.270 --> 00:02:36.850
Imagine you're some scientific
computing application

00:02:36.850 --> 00:02:40.050
developer, or you're doing
some sort of render farm.

00:02:40.050 --> 00:02:43.850
And you have data that you
create once, but then need it

00:02:43.850 --> 00:02:45.990
available to many workers.

00:02:45.990 --> 00:02:48.730
Many workers are going to
compute on this genomic data,

00:02:48.730 --> 00:02:50.540
or on this image.

00:02:50.540 --> 00:02:54.220
So persistent disk gives you
an option as opposed to

00:02:54.220 --> 00:02:55.150
copying it everywhere.

00:02:55.150 --> 00:02:57.820
Instead of copying it to every
worker, you can just mount the

00:02:57.820 --> 00:03:01.370
persistent disk to very large
numbers of virtual machines.

00:03:01.370 --> 00:03:03.860
And they'll have instant
access to it.

00:03:03.860 --> 00:03:06.040
So this can scale to at
least hundreds of VMs.

00:03:08.720 --> 00:03:10.990
So before I get into how
persistent disk works, we'll

00:03:10.990 --> 00:03:13.840
talk about one of our partners
who's had a lot of success

00:03:13.840 --> 00:03:16.910
with persistent disk and
their customers.

00:03:16.910 --> 00:03:19.690
People are probably pretty
aware of MapR.

00:03:19.690 --> 00:03:21.810
They're a Hadoop distributor.

00:03:21.810 --> 00:03:24.580
And they basically used the
first two use cases of

00:03:24.580 --> 00:03:25.890
persistent desk.

00:03:25.890 --> 00:03:28.820
They encourage their customers
to use it as a boot drive

00:03:28.820 --> 00:03:31.830
always because they want the
Hadoop cluster configuration

00:03:31.830 --> 00:03:35.680
to survive death and restart
of instances.

00:03:35.680 --> 00:03:38.310
And then, they also will
suggest some of their

00:03:38.310 --> 00:03:41.750
customers use it as
the data disks.

00:03:41.750 --> 00:03:45.390
The MapR FS can, if you lose a
scratch disk, let's say, can

00:03:45.390 --> 00:03:46.560
replicate the data.

00:03:46.560 --> 00:03:49.800
But if you lose the entire
cluster and bring it back up,

00:03:49.800 --> 00:03:51.450
you would have to re-import
the data.

00:03:51.450 --> 00:03:54.010
So if you're going to shut down
and restart entire Hadoop

00:03:54.010 --> 00:03:55.830
clusters, it's better
to use persistent

00:03:55.830 --> 00:03:57.350
disk on the back end.

00:03:57.350 --> 00:04:01.655
Now, the concern that they had
going into it was performance.

00:04:01.655 --> 00:04:04.430
In other clouds, people say, oh,
you have to use ephemeral

00:04:04.430 --> 00:04:06.360
disk for something
like Hadoop.

00:04:06.360 --> 00:04:07.730
They were very pleasantly
surprised.

00:04:07.730 --> 00:04:10.440
They found that persistent disk
performed about as fast

00:04:10.440 --> 00:04:12.840
as ephemeral disk
in other clouds.

00:04:12.840 --> 00:04:16.180
And they were able to
consistently get 120 megabytes

00:04:16.180 --> 00:04:18.779
per second through the
persistent disk using their

00:04:18.779 --> 00:04:22.200
application how they tuned it.

00:04:22.200 --> 00:04:24.350
So let's talk a little bit
about how persistent disk

00:04:24.350 --> 00:04:28.190
works so we can get to why some
of those benefits exist.

00:04:28.190 --> 00:04:31.740
So the first thing to note is
that a persistent disk is not

00:04:31.740 --> 00:04:33.000
actually a real disk.

00:04:33.000 --> 00:04:36.410
It's a virtual volume that is
made up of blocks from many,

00:04:36.410 --> 00:04:39.110
many underlying physical
devices.

00:04:39.110 --> 00:04:44.430
And so one block is cut up,
and striped, and smeared

00:04:44.430 --> 00:04:47.210
across many, many-- it could
be hundreds or thousands of

00:04:47.210 --> 00:04:48.860
physical devices.

00:04:48.860 --> 00:04:51.010
Now, each of those blocks
is encrypted,

00:04:51.010 --> 00:04:52.600
which is critical because--

00:04:52.600 --> 00:04:54.160
I'm sure you've talked about
security in a lot

00:04:54.160 --> 00:04:55.680
of the other sessions.

00:04:55.680 --> 00:04:57.980
You want to make sure there's
absolutely no circumstances

00:04:57.980 --> 00:05:00.930
where somebody else can
get to your data.

00:05:00.930 --> 00:05:03.920
On top of the security, we
also do check summing.

00:05:03.920 --> 00:05:06.890
When you retrieve data that
you've stored, you want to

00:05:06.890 --> 00:05:09.110
make sure that it's exactly
the data that you had.

00:05:09.110 --> 00:05:11.210
So we take care of this all
underneath the covers.

00:05:11.210 --> 00:05:14.340
We will not return bad
data to you ever.

00:05:14.340 --> 00:05:18.380
Now, let's talk about what
we do with that data for

00:05:18.380 --> 00:05:19.190
performance.

00:05:19.190 --> 00:05:21.970
So the first thing is, we're
going to optimize for random

00:05:21.970 --> 00:05:25.110
writes by basically writing
anywhere in the cloud that we

00:05:25.110 --> 00:05:27.960
feel is a good fit for that
block at that time.

00:05:27.960 --> 00:05:29.220
So this is going to help you.

00:05:29.220 --> 00:05:30.980
Let's use a database use case.

00:05:30.980 --> 00:05:33.750
If you're doing a lot of
transactions that are randomly

00:05:33.750 --> 00:05:36.460
writing all across your
database, you don't know who

00:05:36.460 --> 00:05:39.290
the user's going to come in,
what transaction they're going

00:05:39.290 --> 00:05:41.520
to do, or where in the database
you need to write to.

00:05:41.520 --> 00:05:44.890
You want to know that every
write is going to go to a good

00:05:44.890 --> 00:05:47.230
part of the cloud that isn't
affected by a noisy neighbor

00:05:47.230 --> 00:05:49.930
and gets you consistent rate
performance, even with

00:05:49.930 --> 00:05:51.200
randomness.

00:05:51.200 --> 00:05:55.040
And then, moving over to reads,
what you'll notice

00:05:55.040 --> 00:05:57.710
about these copies, what's
interesting is it's not an

00:05:57.710 --> 00:05:59.060
active-passive copy.

00:05:59.060 --> 00:06:02.200
Each one of those copies can
actively take reads.

00:06:02.200 --> 00:06:07.330
So we will get a read from the
first guy that responds.

00:06:07.330 --> 00:06:08.690
We'll put out the
read everywhere.

00:06:08.690 --> 00:06:10.610
Whoever comes back first wins.

00:06:10.610 --> 00:06:13.580
Again, this is going to add to
consistency of performance.

00:06:13.580 --> 00:06:16.130
So now, if you're trying to
figure out, oh, how much data

00:06:16.130 --> 00:06:19.220
and how much I/O do I put to a
particular data volume before

00:06:19.220 --> 00:06:22.050
I create a new database shard,
that doesn't become an

00:06:22.050 --> 00:06:23.240
exercise in randomness.

00:06:23.240 --> 00:06:24.840
That actually--

00:06:24.840 --> 00:06:26.930
you can apply a very specific
formula to that

00:06:26.930 --> 00:06:28.180
based on your workload.

00:06:30.430 --> 00:06:32.830
Now, moving away from data
volumes back to boot volume

00:06:32.830 --> 00:06:38.050
use case, what we do is, again,
we can boot from PD.

00:06:38.050 --> 00:06:42.110
The PD is striped, spread out
across lots of volumes.

00:06:42.110 --> 00:06:45.540
But what happens when we create
another copy of the VM

00:06:45.540 --> 00:06:48.370
that has high commonality
with the previous one?

00:06:48.370 --> 00:06:51.470
Well, very, very soon-- it's
not released this moment.

00:06:51.470 --> 00:06:55.010
But very soon, we'll basically
be able to create a new copy

00:06:55.010 --> 00:06:58.300
with basically virtually
referencing the previously

00:06:58.300 --> 00:07:03.530
copied blocks so that you get an
extremely fast deploy time.

00:07:03.530 --> 00:07:04.630
And the idea--

00:07:04.630 --> 00:07:06.690
like so when you guys
saw [? Urzhi's ?]

00:07:06.690 --> 00:07:09.910
talk this morning and Greg's
demo, the idea that you could

00:07:09.910 --> 00:07:12.800
do like 1,000 VMs in five
minutes, right?

00:07:12.800 --> 00:07:15.590
We can do that, and we're
looking to get even faster.

00:07:15.590 --> 00:07:17.630
And of course, even though
they're reading off of shared

00:07:17.630 --> 00:07:19.450
blocks, we're not going
to corrupt it.

00:07:19.450 --> 00:07:22.600
If you add a block to one or
change a block on another,

00:07:22.600 --> 00:07:25.260
that's going to be a new write
elsewhere in the persistent

00:07:25.260 --> 00:07:29.100
disk cloud and not going to
corrupt the shared blocks.

00:07:33.120 --> 00:07:36.890
This slide gets to another
extremely unique feature of

00:07:36.890 --> 00:07:40.520
Google Cloud, but it's not
persistent disks only.

00:07:40.520 --> 00:07:42.540
It's in teaming up with Google
Cloud Storage, or

00:07:42.540 --> 00:07:45.100
GCS, or blob storage.

00:07:45.100 --> 00:07:48.320
So the common thing that
everybody knows about is if I

00:07:48.320 --> 00:07:51.140
wanted to be able to copy data
to another site, whether it's

00:07:51.140 --> 00:07:54.650
for VM migration, backup,
disaster recovery, I can

00:07:54.650 --> 00:08:00.170
snapshot my block device
into a blob store.

00:08:00.170 --> 00:08:02.300
A lot of other clouds have
the ability to do

00:08:02.300 --> 00:08:03.900
differential snapshots.

00:08:03.900 --> 00:08:05.970
So I only have to copy
the changes.

00:08:05.970 --> 00:08:07.390
This is something we'll
be coming out with

00:08:07.390 --> 00:08:09.370
very soon as well.

00:08:09.370 --> 00:08:12.310
But what's really cool is that
GCS cloud, you notice I didn't

00:08:12.310 --> 00:08:13.790
put that in one of
the data centers

00:08:13.790 --> 00:08:14.740
when I made this slide.

00:08:14.740 --> 00:08:17.190
That's actually globally
available to all data centers

00:08:17.190 --> 00:08:18.250
simultaneously.

00:08:18.250 --> 00:08:21.510
As soon as you get that confirm
back on that write,

00:08:21.510 --> 00:08:25.190
you're able to use that snapshot
somewhere else.

00:08:25.190 --> 00:08:28.660
So now, I can here take this
snapshot that I made from the

00:08:28.660 --> 00:08:31.870
central region in the US and
deploy it in Western Europe

00:08:31.870 --> 00:08:35.710
immediately after
I get the ACC.

00:08:35.710 --> 00:08:39.480
So a couple quick notes on
using persistent disk.

00:08:39.480 --> 00:08:42.419
I'm going to talk very briefly
about the things are kind of

00:08:42.419 --> 00:08:45.750
obvious and well-documented, try
to spend a little bit of

00:08:45.750 --> 00:08:49.180
time on things that are not 100%
obvious, but then I want

00:08:49.180 --> 00:08:53.220
to hand it over to Andrew to
talk about best practices,

00:08:53.220 --> 00:08:55.290
because I think that's why
a lot of people are here.

00:08:55.290 --> 00:08:59.500
So the first thing is we have
a REST interface for

00:08:59.500 --> 00:09:02.400
programmatic use
of the product.

00:09:02.400 --> 00:09:06.280
And we have command line
and UI for more

00:09:06.280 --> 00:09:07.380
administrative use.

00:09:07.380 --> 00:09:09.770
And the commands are what
you'd expect them to be.

00:09:09.770 --> 00:09:13.140
Create volume, delete volume,
mount volume, unmount volume.

00:09:13.140 --> 00:09:16.650
It's all well-documented, and
everybody can see that.

00:09:16.650 --> 00:09:18.030
So what does the volume
look like

00:09:18.030 --> 00:09:19.790
inside the virtual machine?

00:09:19.790 --> 00:09:22.450
The first thing to note is
that it shows up as a

00:09:22.450 --> 00:09:23.940
VirtioSCSI device.

00:09:23.940 --> 00:09:26.960
And it shows up with a symbolic
link to a device so

00:09:26.960 --> 00:09:28.550
that you can easily find it.

00:09:28.550 --> 00:09:32.210
Basically, the name that you
created it with will show up

00:09:32.210 --> 00:09:34.730
in the device model inside the
VM, which'll help you find it

00:09:34.730 --> 00:09:36.160
for mounting.

00:09:36.160 --> 00:09:38.750
We do give you a command for
formatting and mounting the

00:09:38.750 --> 00:09:41.350
volume, , which is actually
pretty useful.

00:09:41.350 --> 00:09:43.100
Andrew will talk about
why we created that.

00:09:43.100 --> 00:09:45.030
Talks basically about
block alignment

00:09:45.030 --> 00:09:47.990
and performance assurance.

00:09:47.990 --> 00:09:50.220
Now, if you want to make sure
that every time you create a

00:09:50.220 --> 00:09:53.580
virtual machine a persistent
disk is mounted into it, a

00:09:53.580 --> 00:09:55.020
couple ways you can do that.

00:09:55.020 --> 00:10:00.170
First, if you're booting from
a PD, you obviously edit the

00:10:00.170 --> 00:10:03.620
FS tab for any of the data
volumes you want to boot, you

00:10:03.620 --> 00:10:05.830
want to mount, and that'll
always mount

00:10:05.830 --> 00:10:07.390
properly the next time.

00:10:07.390 --> 00:10:09.910
If you're going from a scratch
disk, though, you lose the FS

00:10:09.910 --> 00:10:10.950
tab every time.

00:10:10.950 --> 00:10:14.100
So you can use the custom
start-up scripts and the

00:10:14.100 --> 00:10:17.470
custom metadata variables to
basically script your way

00:10:17.470 --> 00:10:22.900
around making sure the data
drive mounts every time.

00:10:22.900 --> 00:10:24.660
I did have some sample
code on that.

00:10:24.660 --> 00:10:26.830
But everybody's embarrassed
of the PM shell scripting.

00:10:26.830 --> 00:10:29.770
So it's in the backup slides.

00:10:29.770 --> 00:10:32.380
Like any system,
PD has limits.

00:10:32.380 --> 00:10:36.180
Limits are, like [? Urzhi ?]
mentioned this morning and I

00:10:36.180 --> 00:10:36.810
think like [? Dabnev ?]

00:10:36.810 --> 00:10:40.180
mentioned in the GCE talk today,
we've increased that.

00:10:40.180 --> 00:10:42.870
Depending on the size of the VM
previously, it was anywhere

00:10:42.870 --> 00:10:46.030
from 128 gig to 1 terabyte
volume limits.

00:10:46.030 --> 00:10:47.740
Now, we've radically
increased that.

00:10:47.740 --> 00:10:50.940
You can create 3 terabyte
volumes on the very small VMs,

00:10:50.940 --> 00:10:55.460
the sub-core VMs, and up to
10 terabyte volumes on the

00:10:55.460 --> 00:10:57.000
previous sized VMs.

00:10:57.000 --> 00:10:59.180
And the number of devices--

00:10:59.180 --> 00:11:01.410
four for the small VMs, 10
for the bigger VMs--

00:11:03.970 --> 00:11:07.110
it's not 10 times 10 terabytes
for 100 terabytes.

00:11:07.110 --> 00:11:10.010
It's 10 volumes adding
up to 100 terabytes--

00:11:10.010 --> 00:11:10.800
I'm sorry.

00:11:10.800 --> 00:11:13.690
10 volumes adding to 10
terabytes is the limit.

00:11:13.690 --> 00:11:17.930
And so that should really take
care of pretty much any size

00:11:17.930 --> 00:11:18.940
needs, I think, people have.

00:11:18.940 --> 00:11:23.140
And compares, I think, very
favorably to other clouds.

00:11:23.140 --> 00:11:25.370
Now, to ensure the consistent
performance, one of the things

00:11:25.370 --> 00:11:28.950
we had to do was limit
the I/O rates.

00:11:28.950 --> 00:11:33.920
So for large I/O blocks, you'll
hit a bandwidth limit

00:11:33.920 --> 00:11:36.020
of somewhere around
120 megabytes.

00:11:36.020 --> 00:11:40.950
And for smaller I/O, you'll
hit read I/O limitations.

00:11:40.950 --> 00:11:45.120
Now, if you want to get more
smaller I/Os, you can add

00:11:45.120 --> 00:11:47.720
multiple volumes to a VM and
stripe them together.

00:11:47.720 --> 00:11:49.150
And Andrew will show some
data on that in the

00:11:49.150 --> 00:11:50.400
best practices session.

00:11:52.740 --> 00:11:56.250
And again, the 120 megabytes
is pretty good for a best

00:11:56.250 --> 00:11:58.980
effort volume, right, as the
MapR guys found and talked

00:11:58.980 --> 00:12:01.620
about in a previous slide.

00:12:01.620 --> 00:12:06.970
Now, in terms of pricing, the
price is publicly listed.

00:12:06.970 --> 00:12:10.690
What's good about this is like
GCE, the billing increment is

00:12:10.690 --> 00:12:12.590
now very small.

00:12:12.590 --> 00:12:16.540
So even though the price is per
month, when you create the

00:12:16.540 --> 00:12:19.410
virtual disk, you don't get
charged for a whole month.

00:12:19.410 --> 00:12:21.800
You only get charged for
the time that it lives.

00:12:21.800 --> 00:12:25.490
And the increment is something
under five minutes.

00:12:25.490 --> 00:12:28.200
If you want to see what your
price is at any given time,

00:12:28.200 --> 00:12:31.750
how much you've accrued over the
course of a month, you can

00:12:31.750 --> 00:12:33.830
go to the cloud console
and check that out.

00:12:33.830 --> 00:12:36.920
One of the things to note is the
cloud console does update

00:12:36.920 --> 00:12:39.070
its display on a daily basis.

00:12:39.070 --> 00:12:40.320
But you're not actually getting

00:12:40.320 --> 00:12:41.510
charged on a daily basis.

00:12:41.510 --> 00:12:43.490
It's still accruing in very
small increments.

00:12:43.490 --> 00:12:46.990
So don't panic when you
see that happen.

00:12:46.990 --> 00:12:49.750
So that's kind of the overview
of what PD is, what it's good

00:12:49.750 --> 00:12:52.100
for, and a little bit
of how it works.

00:12:52.100 --> 00:12:54.870
And let me hand it over to
Andrew for how you guys can

00:12:54.870 --> 00:12:56.440
get the most out of it.

00:12:56.440 --> 00:12:57.420
ANDREW KADATCH: Thank you.

00:12:57.420 --> 00:12:57.460
[? That was ?]

00:12:57.460 --> 00:13:00.260
[? great ?].

00:13:00.260 --> 00:13:03.200
When we started working on
persistent disk a while ago,

00:13:03.200 --> 00:13:06.400
we put a couple of goals
ahead of us.

00:13:06.400 --> 00:13:10.710
And the first and most important
one is durability.

00:13:10.710 --> 00:13:14.000
If you lose data on the left and
on the right, everything

00:13:14.000 --> 00:13:15.820
else doesn't matter.

00:13:15.820 --> 00:13:17.430
Simple.

00:13:17.430 --> 00:13:19.250
Second goal was security.

00:13:19.250 --> 00:13:22.630
If I cannot trust my cloud
provider with my data, I'm not

00:13:22.630 --> 00:13:24.060
going to use it.

00:13:24.060 --> 00:13:26.670
So everything else doesn't
matter either.

00:13:26.670 --> 00:13:30.440
To a [INAUDIBLE]-- so getting
back to durability.

00:13:30.440 --> 00:13:32.920
So we replicate the data.

00:13:32.920 --> 00:13:37.210
And we provide sufficient mean
time to data loss to say that

00:13:37.210 --> 00:13:40.790
the system disk's mean time to
data loss exceeds mean time to

00:13:40.790 --> 00:13:45.130
data loss of typical enterprise
class SATA disk by

00:13:45.130 --> 00:13:47.790
one to two orders
of magnitude.

00:13:47.790 --> 00:13:50.120
Security, what we do,
we encrypt the data.

00:13:50.120 --> 00:13:53.910
And we encrypt it not only at
rest, but over the wire, too.

00:13:53.910 --> 00:13:58.570
So it's extremely difficult
to get to the data.

00:13:58.570 --> 00:14:01.190
And to be honest, I myself
don't have access to the

00:14:01.190 --> 00:14:03.710
encryption keys.

00:14:03.710 --> 00:14:07.860
So we made it as hard as
possible for ourselves.

00:14:07.860 --> 00:14:08.940
High availability.

00:14:08.940 --> 00:14:12.410
I should be able to get the
data when I most need it.

00:14:12.410 --> 00:14:14.670
Typically, Tuesday morning
is the peak

00:14:14.670 --> 00:14:17.600
traffic in cloud services.

00:14:17.600 --> 00:14:23.110
And I they cannot get it,
my disk is unusable.

00:14:23.110 --> 00:14:27.010
Typically, we are able to
achieve [? availability ?]

00:14:27.010 --> 00:14:30.530
in I/O execution latency
within 3%.

00:14:30.530 --> 00:14:36.780
The more cores the VM has and
the smaller the bandwidth, the

00:14:36.780 --> 00:14:39.520
better availability and
consistency is.

00:14:39.520 --> 00:14:42.950
The same applies
to consistency.

00:14:42.950 --> 00:14:43.710
It's [? closely ?]

00:14:43.710 --> 00:14:45.890
related to availability.

00:14:45.890 --> 00:14:49.130
The thing about consistency is
that it's extremely important

00:14:49.130 --> 00:14:50.630
for capacity planning.

00:14:50.630 --> 00:14:55.390
Should I be running two
instances of my SQL services

00:14:55.390 --> 00:14:58.800
or 20 to meet my peak demand?

00:14:58.800 --> 00:15:00.020
I need to know up front.

00:15:00.020 --> 00:15:02.380
And I need to plan
accordingly.

00:15:02.380 --> 00:15:04.800
And finally, performance.

00:15:04.800 --> 00:15:10.680
We bravely wanted to meet or
exceed performance of locally

00:15:10.680 --> 00:15:13.850
attached SATA disks.

00:15:13.850 --> 00:15:17.160
Obviously, mission impossible
because of encryption, network

00:15:17.160 --> 00:15:19.150
latency, replication,
what not.

00:15:19.150 --> 00:15:25.860
But I think overall, for many
services, we achieved pretty

00:15:25.860 --> 00:15:28.920
good performance and it's
suitable for use.

00:15:28.920 --> 00:15:33.740
However, before we'll talk about
specifics of persistent

00:15:33.740 --> 00:15:40.270
disk, let me take a step back
and say that one lesson that

00:15:40.270 --> 00:15:44.870
we all learn at Google
every day is simple.

00:15:44.870 --> 00:15:48.420
The luck favors the prepared.

00:15:48.420 --> 00:15:50.050
It was said by Louis Pasteur.

00:15:50.050 --> 00:15:52.440
And he knew exactly
what he was doing.

00:15:55.060 --> 00:16:00.920
So the problem is that
disasters do happen.

00:16:00.920 --> 00:16:04.960
Human mistakes, software
bugs, natural disasters

00:16:04.960 --> 00:16:06.020
happen all the time.

00:16:06.020 --> 00:16:08.420
And we need to be prepared.

00:16:08.420 --> 00:16:10.150
The way to get prepared
with persistent

00:16:10.150 --> 00:16:13.430
disk is using snapshots.

00:16:13.430 --> 00:16:15.890
Snapshots are a very
efficient way to

00:16:15.890 --> 00:16:18.440
implement backup and restore.

00:16:18.440 --> 00:16:19.960
The data is very distributed.

00:16:19.960 --> 00:16:22.190
Snapshots are non-intrusive.

00:16:22.190 --> 00:16:24.480
They are extremely efficient.

00:16:24.480 --> 00:16:29.660
And most importantly,
they do not affect

00:16:29.660 --> 00:16:31.350
performance of the volume.

00:16:31.350 --> 00:16:34.130
So you can take it any time.

00:16:34.130 --> 00:16:36.190
So I would strongly urge--

00:16:36.190 --> 00:16:39.360
and probably will repeat it
multiple times-- to automate

00:16:39.360 --> 00:16:40.010
the process.

00:16:40.010 --> 00:16:41.610
Test it regularly.

00:16:41.610 --> 00:16:43.830
And have regular snap shots.

00:16:43.830 --> 00:16:47.890
What I would do, I would take
snapshots, say, hourly.

00:16:47.890 --> 00:16:50.440
Keep a couple days of
hourly snapshots.

00:16:50.440 --> 00:16:52.970
Then, I will remove all snap
shots and have a couple of

00:16:52.970 --> 00:16:57.050
daily snapshots, a few weekly,
and a handful of monthly, just

00:16:57.050 --> 00:17:00.060
in case, because sometimes we
don't know it is a problem

00:17:00.060 --> 00:17:00.570
right away.

00:17:00.570 --> 00:17:02.620
Sometimes it takes
time to discover.

00:17:02.620 --> 00:17:06.520
And I should be able to get back
in time and restore my

00:17:06.520 --> 00:17:12.589
database or my data set to
the correct state with

00:17:12.589 --> 00:17:13.819
minimal data loss.

00:17:13.819 --> 00:17:15.869
One thing is extremely
important.

00:17:15.869 --> 00:17:19.250
We really have to use journaling
file systems when

00:17:19.250 --> 00:17:22.650
we use persistent disk because
file systems cache a lot of

00:17:22.650 --> 00:17:24.579
internal data structures
in memory.

00:17:24.579 --> 00:17:27.790
And if those data structures in
memory are not preserved on

00:17:27.790 --> 00:17:33.170
the disk in case of an
undesirable event, we will

00:17:33.170 --> 00:17:34.960
have massive data loss.

00:17:34.960 --> 00:17:38.490
And from our experiments, when
we use non-journaling file

00:17:38.490 --> 00:17:44.115
systems and simulate various
failures under heavy load, we

00:17:44.115 --> 00:17:47.580
are able to cause massive data
losses or irrecoverable file

00:17:47.580 --> 00:17:51.550
system corruptions in
40%, 60% of cases.

00:17:51.550 --> 00:17:53.440
It is very substantial.

00:17:53.440 --> 00:17:57.140
Please, please use journaling
file systems.

00:17:57.140 --> 00:17:59.650
Making snapshots
is really easy.

00:17:59.650 --> 00:18:05.120
The safe way to do it is to
freeze application data files

00:18:05.120 --> 00:18:07.650
and file system, make
a snapshot.

00:18:07.650 --> 00:18:10.020
Making a snapshot is a
two phase operation.

00:18:10.020 --> 00:18:12.800
One is making a snapshot,
effectively taking an

00:18:12.800 --> 00:18:16.560
instantaneous photograph of
the volume state, and then

00:18:16.560 --> 00:18:18.290
copying the data out.

00:18:18.290 --> 00:18:23.290
When you start copying the data
out, the snapshot state

00:18:23.290 --> 00:18:24.400
changes to upload.

00:18:24.400 --> 00:18:26.720
And when we finish copying,
it becomes ready.

00:18:26.720 --> 00:18:32.720
So the moment snapshot state
converts to uploading, it is

00:18:32.720 --> 00:18:35.190
safe to release and then freeze

00:18:35.190 --> 00:18:38.580
application and file systems.

00:18:38.580 --> 00:18:39.720
Two caveats--

00:18:39.720 --> 00:18:44.100
first of all, we need to
separate application data and

00:18:44.100 --> 00:18:44.970
operating systems.

00:18:44.970 --> 00:18:50.260
You cannot freeze OS partitions
or volumes.

00:18:50.260 --> 00:18:51.890
It's as simple as that.

00:18:51.890 --> 00:18:54.365
Secondly, for some applications
and some file

00:18:54.365 --> 00:18:56.770
systems, it may be
not necessary.

00:18:56.770 --> 00:18:59.790
For instance, if we run a SQL
Server which supports

00:18:59.790 --> 00:19:03.440
transactions we run a journaling
file system, if you

00:19:03.440 --> 00:19:06.010
just make a snapshot while
everything is running and

00:19:06.010 --> 00:19:09.870
restore another VM from the
snapshot, it will look as if

00:19:09.870 --> 00:19:11.990
there was a power failure
at the moment the

00:19:11.990 --> 00:19:13.240
snapshot was taken.

00:19:13.240 --> 00:19:15.320
File system will recover
from that.

00:19:15.320 --> 00:19:17.340
SQL Server will recover
from that.

00:19:17.340 --> 00:19:19.420
The problem is that
it may take time.

00:19:19.420 --> 00:19:22.450
And it is different trade-off.

00:19:22.450 --> 00:19:26.220
Sometimes, it's beneficial to
freeze and take a snapshot.

00:19:26.220 --> 00:19:30.380
Sometimes, it's okay
to keep going.

00:19:30.380 --> 00:19:37.350
Today, Google implements two
week maintenance windows every

00:19:37.350 --> 00:19:39.490
quarter per cell.

00:19:39.490 --> 00:19:40.780
And [? ICE ?]

00:19:40.780 --> 00:19:46.190
is a wonderful opportunity to
make sure that your recovery

00:19:46.190 --> 00:19:51.332
and scripts are working and
doing the right thing.

00:19:54.500 --> 00:19:57.410
As a developer, when I joined
Google, I really hated that.

00:19:57.410 --> 00:19:58.610
Now, I really love it.

00:19:58.610 --> 00:20:02.160
It is a nice, forcing function
to ensure that we are doing

00:20:02.160 --> 00:20:05.120
the right things, and our
products work, and they're

00:20:05.120 --> 00:20:07.160
ready to face disasters.

00:20:07.160 --> 00:20:09.930
Because they will happen.

00:20:09.930 --> 00:20:12.720
And I would strongly recommend
visiting the next session.

00:20:12.720 --> 00:20:17.650
They will have it on Friday at
2:00 PM in room number two by

00:20:17.650 --> 00:20:19.160
Mark and Joe.

00:20:19.160 --> 00:20:22.120
Maybe we will talk about
it a little bit more.

00:20:22.120 --> 00:20:27.320
And we provide some tools to
simplify the migration of your

00:20:27.320 --> 00:20:31.870
data and VMs to another cell
or zone using gcutil

00:20:31.870 --> 00:20:32.740
[? moveinstances ?]

00:20:32.740 --> 00:20:33.290
command.

00:20:33.290 --> 00:20:36.000
That will make it fast
and efficient.

00:20:36.000 --> 00:20:40.180
Or, if you prefer, you could
use your own scripts.

00:20:40.180 --> 00:20:42.160
Amateurs back up.

00:20:42.160 --> 00:20:43.920
Professionals restore.

00:20:43.920 --> 00:20:49.140
Please, please, please automate
everything you can.

00:20:49.140 --> 00:20:51.400
Take snapshots regularly.

00:20:51.400 --> 00:20:53.880
Test your recovery routines.

00:20:53.880 --> 00:21:00.130
And make sure that it works as
luck favors the prepared.

00:21:00.130 --> 00:21:03.020
Now, let us proceed to
persistent disk [? repair. ?]

00:21:05.730 --> 00:21:11.170
In many environments, it is a
well-known trick to create a

00:21:11.170 --> 00:21:14.890
lot of volumes, test their
performance, and pick the one

00:21:14.890 --> 00:21:17.100
that works the best and
throw away the rest.

00:21:17.100 --> 00:21:21.910
It is not necessary to do with
persistent disk for a variety

00:21:21.910 --> 00:21:23.020
of reasons.

00:21:23.020 --> 00:21:26.460
We decide the actual data
placement at time of write.

00:21:26.460 --> 00:21:28.460
So the past performance
doesn't matter.

00:21:28.460 --> 00:21:33.200
We do exceptionally well
with noisy neighbors.

00:21:33.200 --> 00:21:38.070
I would say about in 98%
of cases, if there is

00:21:38.070 --> 00:21:42.110
interference at physical disk
level, we resolve it within

00:21:42.110 --> 00:21:43.640
from two to 20 milliseconds.

00:21:43.640 --> 00:21:45.480
So it's extremely efficient.

00:21:45.480 --> 00:21:49.490
So I just took the data from
the last 10 automated

00:21:49.490 --> 00:21:51.920
performance runs that
we run all the time.

00:21:51.920 --> 00:21:54.160
Unfortunately, it's a
very boring slide.

00:21:54.160 --> 00:21:57.700
There was much, much, much less
than 1% [? variability ?]

00:21:57.700 --> 00:22:00.590
between runs.

00:22:00.590 --> 00:22:04.160
Similarly, there is no need to
pre-warm the volume before

00:22:04.160 --> 00:22:07.660
use, filling it with data, doing
full format, et cetera.

00:22:07.660 --> 00:22:10.880
The volume is ready to use
and go from day one.

00:22:10.880 --> 00:22:12.920
That is [? to our ?] run.

00:22:12.920 --> 00:22:18.360
And those two horizontal lines
represent average I/O

00:22:18.360 --> 00:22:21.530
execution latency and bandwidth,
respectively.

00:22:21.530 --> 00:22:25.250
And to make the stuff a little
bit more interesting, we threw

00:22:25.250 --> 00:22:26.370
a curveball.

00:22:26.370 --> 00:22:31.780
That wavy line represents the
latency of the slowest I/O

00:22:31.780 --> 00:22:33.110
during a one second period.

00:22:33.110 --> 00:22:36.410
And as we see at one case about
30 minutes through the

00:22:36.410 --> 00:22:41.560
run, it took about 3.5 times
slower than average, which is

00:22:41.560 --> 00:22:42.810
not too bad.

00:22:42.810 --> 00:22:45.190
So that's exactly [INAUDIBLE]
we did that.

00:22:45.190 --> 00:22:48.480
Finally, we have something
interesting.

00:22:48.480 --> 00:22:54.880
Three classic textbook
scalability drafts.

00:22:54.880 --> 00:22:59.510
What happens if you stripe
volumes, which is yet another

00:22:59.510 --> 00:23:02.000
well-known trick to improve
performance?

00:23:02.000 --> 00:23:06.900
Yes, performance will improve
on small, random writes we

00:23:06.900 --> 00:23:09.340
scale really well.

00:23:09.340 --> 00:23:13.535
And max out at about six volumes
on small random rates,

00:23:13.535 --> 00:23:16.500
will scale practically
linearly.

00:23:16.500 --> 00:23:20.520
With large, sequential writes
which simulate [? bald ?] data

00:23:20.520 --> 00:23:24.910
processing workload, they max
out, and then performance

00:23:24.910 --> 00:23:28.030
drops because those volumes
start to compete with each

00:23:28.030 --> 00:23:32.250
other at process level
and network.

00:23:32.250 --> 00:23:35.480
But what I would like to
say, you can do that.

00:23:35.480 --> 00:23:36.940
Nobody can stop you.

00:23:36.940 --> 00:23:38.960
I would strongly discourage
you from doing

00:23:38.960 --> 00:23:43.240
that because we would--

00:23:43.240 --> 00:23:48.320
I don't like the need to do
something awkward and

00:23:48.320 --> 00:23:52.220
extraordinary to get what
you should be getting.

00:23:52.220 --> 00:23:53.810
We will change that.

00:23:53.810 --> 00:23:57.860
You will get a way to get good
performance without the need

00:23:57.860 --> 00:23:59.690
to go to extreme measures.

00:23:59.690 --> 00:24:01.240
That's problem number one.

00:24:01.240 --> 00:24:04.550
Problem number two, whenever
you stripe volumes,

00:24:04.550 --> 00:24:08.400
reliability of the volume, mean
time to data loss, drops

00:24:08.400 --> 00:24:10.170
dramatically.

00:24:10.170 --> 00:24:13.160
And because we have absolutely
no control of what's going on

00:24:13.160 --> 00:24:15.450
under the hood, unlike the
system disk where we have a

00:24:15.450 --> 00:24:20.670
lot of control and we already
stripe it under the hood, you

00:24:20.670 --> 00:24:25.650
will have degraded durability
of the volume.

00:24:25.650 --> 00:24:29.300
Third, it is extremely difficult
to take consistent

00:24:29.300 --> 00:24:31.270
snapshots of multiple volumes.

00:24:31.270 --> 00:24:33.580
It is difficult and error-prone
to maintain those

00:24:33.580 --> 00:24:36.480
small snapshots in time.

00:24:36.480 --> 00:24:39.670
So if you have to, do it.

00:24:39.670 --> 00:24:44.720
Otherwise, I would suggest
avoiding doing it.

00:24:44.720 --> 00:24:49.995
Performance of persistent
disk supports latency--

00:24:49.995 --> 00:24:52.930
well, mostly throughput and
number of I/Os per second

00:24:52.930 --> 00:24:57.410
depend on I/O size and number
of I/Os in flight.

00:24:57.410 --> 00:24:59.750
The more I/Os in flight we
have, the better the

00:24:59.750 --> 00:25:00.550
performance.

00:25:00.550 --> 00:25:03.400
The larger I/Os, the better
the throughput.

00:25:03.400 --> 00:25:05.940
And as we could see from those
graphs, it is especially

00:25:05.940 --> 00:25:08.310
noticeable in small
I/O blocks.

00:25:08.310 --> 00:25:12.060
And we recommend to
use block size a

00:25:12.060 --> 00:25:14.300
multiple of 4 kilobytes.

00:25:14.300 --> 00:25:16.520
And I will talk about that
a little bit later.

00:25:16.520 --> 00:25:21.740
The thing is that the system
disk internally is implemented

00:25:21.740 --> 00:25:25.220
as a physical device with 4
kilobyte physical block and

00:25:25.220 --> 00:25:27.770
512 byte logical block size.

00:25:27.770 --> 00:25:31.065
It means that the minimal unit
of physical access to the unit

00:25:31.065 --> 00:25:32.890
is 4 kilobytes.

00:25:32.890 --> 00:25:35.820
Which means that if, for
instance, we are writing one

00:25:35.820 --> 00:25:39.150
sector, 512 bytes, what will
happen under the hood?

00:25:39.150 --> 00:25:41.960
We will read an entire 4
kilobyte block, modify a small

00:25:41.960 --> 00:25:44.450
part of it, and write it back.

00:25:44.450 --> 00:25:46.760
Read, modify, write
cycle is slow.

00:25:46.760 --> 00:25:49.250
And actually, a lot of people
who are using modern hard

00:25:49.250 --> 00:25:52.550
disks notice that because they
have 4 kilobyte physical

00:25:52.550 --> 00:25:54.750
blocks, too.

00:25:54.750 --> 00:25:58.080
Another thing that may happen,
even if we are doing

00:25:58.080 --> 00:26:02.780
sequential I/O, if it's
misaligned, we will generate

00:26:02.780 --> 00:26:04.710
not only read, modify,
write cycles.

00:26:04.710 --> 00:26:09.020
But we will get overlapping I/Os
because each I/O will be

00:26:09.020 --> 00:26:11.660
extended to the nearest
4 kilobyte boundary.

00:26:11.660 --> 00:26:14.580
And then, they will
start to overlap.

00:26:14.580 --> 00:26:17.910
And then, they will be
serialized so they don't

00:26:17.910 --> 00:26:19.990
corrupt each other's data.

00:26:19.990 --> 00:26:21.900
It is very slow.

00:26:21.900 --> 00:26:23.780
Please, avoid doing that.

00:26:23.780 --> 00:26:28.150
Typical mistakes people make is
partitioning their disks.

00:26:28.150 --> 00:26:31.040
First of all, I would strongly
discourage doing that until

00:26:31.040 --> 00:26:33.170
you absolutely have to do it.

00:26:33.170 --> 00:26:36.950
There is no need to take a
large persistent disk and

00:26:36.950 --> 00:26:39.810
partition it into multiple
subdisks.

00:26:39.810 --> 00:26:43.350
It's better to just create
multiple disks.

00:26:43.350 --> 00:26:48.140
Secondly, I would recommend
using modern tools, like

00:26:48.140 --> 00:26:50.600
fdisk, one of the latest
generation, who do the right

00:26:50.600 --> 00:26:53.620
things, to partition
if you have to.

00:26:53.620 --> 00:26:57.640
And third thing, once you're
done partitioning a disk,

00:26:57.640 --> 00:27:00.940
please double-check that
everything is right by running

00:27:00.940 --> 00:27:03.560
fdisk -lu command.

00:27:03.560 --> 00:27:08.690
And if you see that line in
red, the partition is

00:27:08.690 --> 00:27:09.390
misaligned.

00:27:09.390 --> 00:27:12.100
And if partition is misaligned,
all I/Os to that

00:27:12.100 --> 00:27:15.120
partition will be misaligned
and will be extremely slow.

00:27:15.120 --> 00:27:17.620
So that red line, it's
not something that I

00:27:17.620 --> 00:27:18.620
tacked on the slide.

00:27:18.620 --> 00:27:21.130
It's output from the command
so it's easy to grab.

00:27:24.020 --> 00:27:27.810
So when we talk about file
system formatting, I would

00:27:27.810 --> 00:27:31.360
recommend aligning partitions
on one megabyte boundaries,

00:27:31.360 --> 00:27:34.890
default second for
modern fdisk, for

00:27:34.890 --> 00:27:37.430
latest versions of fdisk.

00:27:37.430 --> 00:27:39.420
I wouldn't recommend
modifying that.

00:27:39.420 --> 00:27:42.340
I would recommend formatting
the disks with

00:27:42.340 --> 00:27:43.940
four kilobyte blocks.

00:27:43.940 --> 00:27:46.060
Use journaling file system.

00:27:46.060 --> 00:27:49.520
And we provide instructions
and utilities

00:27:49.520 --> 00:27:50.940
how to do that correct.

00:27:54.460 --> 00:27:58.215
Another thing to talk about is
how we could use persistent

00:27:58.215 --> 00:28:01.540
disk to efficiently
distribute data.

00:28:01.540 --> 00:28:04.640
Well, as Jay mentioned, mounting
a persistent disk

00:28:04.640 --> 00:28:09.330
read-only is a very efficient
way to distribute data within

00:28:09.330 --> 00:28:10.790
the same zone.

00:28:10.790 --> 00:28:12.390
What we could also do,
we could take a

00:28:12.390 --> 00:28:14.330
snapshot of the disk.

00:28:14.330 --> 00:28:19.250
And then, recreate a disk in
another zone and mount it read

00:28:19.250 --> 00:28:23.470
only to another large
set of VMs.

00:28:23.470 --> 00:28:27.730
And boom, within a few minutes,
we distributed a very

00:28:27.730 --> 00:28:31.470
large data set across hundreds
of machines.

00:28:31.470 --> 00:28:35.380
When we mount persistent disk
read only, I recommend you

00:28:35.380 --> 00:28:36.840
measure the performance because

00:28:36.840 --> 00:28:38.600
actual performance varies.

00:28:38.600 --> 00:28:41.140
It depends on the
workload a lot.

00:28:41.140 --> 00:28:43.640
We handle edge cases
extremely well.

00:28:43.640 --> 00:28:45.720
But something in the
middle may vary.

00:28:45.720 --> 00:28:49.440
It may scale from few hundreds,
from low hundreds,

00:28:49.440 --> 00:28:50.550
to low thousands.

00:28:50.550 --> 00:28:54.520
But you need to measure on your
specific workload on your

00:28:54.520 --> 00:28:57.390
specific scenario.

00:28:57.390 --> 00:29:00.600
Importing data from PD if you
have data sources somewhere

00:29:00.600 --> 00:29:02.200
else is relatively easy.

00:29:02.200 --> 00:29:05.720
We could use a [? CPO ?] or
gcutil push command to move

00:29:05.720 --> 00:29:12.700
the data from your data
warehouse to a persistent

00:29:12.700 --> 00:29:15.890
disk, from other storages
like blog

00:29:15.890 --> 00:29:18.490
storages, object storages.

00:29:18.490 --> 00:29:22.720
We can use tools provided
with them.

00:29:22.720 --> 00:29:27.780
And for demanding applications
for accelerated data copying,

00:29:27.780 --> 00:29:31.070
transferring huge quantities of
data, you may want to look

00:29:31.070 --> 00:29:35.600
at third-party providers, like
Aspera who do that reliably

00:29:35.600 --> 00:29:36.850
and efficiently.

00:29:38.700 --> 00:29:41.904
JAY JUDKOWITZ: Oh, thank
you, Andrew.

00:29:41.904 --> 00:29:46.330
[APPLAUSE]

00:29:46.330 --> 00:29:48.550
ANDREW KADATCH: I noticed you
got clapping when you

00:29:48.550 --> 00:29:49.886
finished, but not me.

00:29:53.270 --> 00:29:55.860
So thanks for joining.

00:29:55.860 --> 00:29:57.610
We're going to go to Q&amp;A
in a second here.

00:29:57.610 --> 00:30:00.100
But just to summarize, first,
we talked about how

00:30:00.100 --> 00:30:05.210
performance is optimized for
storing large, persistent data

00:30:05.210 --> 00:30:08.420
for databases, whether
it's SQL or NoSQL.

00:30:08.420 --> 00:30:11.530
We get rid of noisy neighbor
problems, whether it be on

00:30:11.530 --> 00:30:14.460
reads or writes by distributing
the data, writing

00:30:14.460 --> 00:30:17.490
to free disks, and reading
from whoever's

00:30:17.490 --> 00:30:19.470
available most quickly.

00:30:19.470 --> 00:30:22.310
Besides scaling on data access,
we scale on the boot

00:30:22.310 --> 00:30:25.590
devices by deploying extremely
quickly and using shared

00:30:25.590 --> 00:30:26.920
blocks wherever we can.

00:30:26.920 --> 00:30:30.440
So we can handle those burst
use cases, like Greg was

00:30:30.440 --> 00:30:32.900
talking about this morning.

00:30:32.900 --> 00:30:35.490
And again, all these services
are better together.

00:30:35.490 --> 00:30:39.540
Using GCS for your backups and
your DR gets data off-site

00:30:39.540 --> 00:30:43.490
super fast, and then also is
able to give you more scale--

00:30:43.490 --> 00:30:46.850
again, this massive read-only
mount use case we talked about

00:30:46.850 --> 00:30:48.910
for scientific computing,
or rendering, or

00:30:48.910 --> 00:30:50.590
other things like that.

00:30:50.590 --> 00:30:53.155
And lastly, hopefully it wasn't
too scary, but we did

00:30:53.155 --> 00:30:55.030
give you a lot of best
practices, a lot of ways to

00:30:55.030 --> 00:30:58.170
keep your data safe, and to
keep it working very fast.

00:30:58.170 --> 00:31:01.040
So follow the best practices
that Andrew shared with you.

00:31:01.040 --> 00:31:02.520
The slides are available,
so you don't have

00:31:02.520 --> 00:31:03.670
to remember it all.

00:31:03.670 --> 00:31:06.290
And I guess let's turn
it over to questions.

00:31:06.290 --> 00:31:10.126
Thank you very much
for joining.

00:31:10.126 --> 00:31:22.918
[APPLAUSE]

00:31:22.918 --> 00:31:23.902
AUDIENCE: Hi.

00:31:23.902 --> 00:31:27.168
So we're running various
Hadoop clusters today,

00:31:27.168 --> 00:31:29.653
currently on [? native disks. ?]

00:31:29.653 --> 00:31:34.520
And we use instant storage,
which is not very

00:31:34.520 --> 00:31:36.845
satisfactory, because they're
quite small volumes, as you

00:31:36.845 --> 00:31:39.060
probably know.

00:31:39.060 --> 00:31:41.090
So two questions, really.

00:31:41.090 --> 00:31:46.080
One is it sounds like you're
saying that you can use PDs as

00:31:46.080 --> 00:31:51.440
disks for HDFS data nodes, and
that the I/O is probably good

00:31:51.440 --> 00:31:53.940
enough, or compatible with
instant storage.

00:31:53.940 --> 00:31:56.140
Is that what you're saying?

00:31:56.140 --> 00:31:57.480
JAY JUDKOWITZ: So yeah.

00:31:57.480 --> 00:31:58.740
What the MapR guys found--

00:31:58.740 --> 00:32:00.620
I haven't used this personally,
but the MapR guys

00:32:00.620 --> 00:32:05.650
basically said, if you use the
scratch disk from Google, it

00:32:05.650 --> 00:32:07.170
will perform better.

00:32:07.170 --> 00:32:09.030
I mean, they basically took--

00:32:09.030 --> 00:32:12.550
when they optimized the MapR
FS, similar to how HDFS was

00:32:12.550 --> 00:32:15.870
done, it was optimized for
locally attached SATA devices.

00:32:15.870 --> 00:32:18.810
Now, they just found some of
their customers couldn't use

00:32:18.810 --> 00:32:20.700
that because they were going to
shut down the whole cluster

00:32:20.700 --> 00:32:22.210
instead of one VM at a time.

00:32:22.210 --> 00:32:25.860
And what they found was PD
was actually fast enough.

00:32:25.860 --> 00:32:27.780
They thought it wasn't going
to be, based on experience

00:32:27.780 --> 00:32:29.000
with other clouds.

00:32:29.000 --> 00:32:31.820
So I'm not going to say it was,
in that use case, as fast

00:32:31.820 --> 00:32:32.720
as scratch.

00:32:32.720 --> 00:32:36.200
But they were able to stream
120 megabytes to a volume.

00:32:36.200 --> 00:32:38.750
And if that's good enough for
you, and if you value the

00:32:38.750 --> 00:32:42.080
flexibility of being able to
shut down the cluster more

00:32:42.080 --> 00:32:46.290
than the raw performance,
then it's a good option.

00:32:46.290 --> 00:32:48.820
It's just a flexibility versus
performance trade-off, but

00:32:48.820 --> 00:32:50.230
you're going to do pretty
good either way.

00:32:50.230 --> 00:32:54.670
AUDIENCE: So my follow up
question is, that third use

00:32:54.670 --> 00:32:57.750
case you talked about, the
parallel read case, actually,

00:32:57.750 --> 00:32:59.480
that's my preferred option
because actually,

00:32:59.480 --> 00:33:01.670
realistically, I'd rather just
be able to shut down the whole

00:33:01.670 --> 00:33:05.275
cluster and bring it
up instantaneously.

00:33:05.275 --> 00:33:07.580
And if I've got HDFS running,
it's hard to do that.

00:33:07.580 --> 00:33:10.613
So are there any limitations to
that parallel read in terms

00:33:10.613 --> 00:33:12.822
of the number of instances and
the combined throughput you

00:33:12.822 --> 00:33:14.550
could have [INAUDIBLE]
with it?

00:33:14.550 --> 00:33:16.280
JAY JUDKOWITZ: So like Andrew
was saying, it is going to be

00:33:16.280 --> 00:33:18.400
a little bit dependent
on the workload.

00:33:18.400 --> 00:33:20.770
If you think about how we
architected it, where all the

00:33:20.770 --> 00:33:23.480
data from a particular volume
is striped out across

00:33:23.480 --> 00:33:26.750
thousands of disks, if you mount
it to a lot of VMs and

00:33:26.750 --> 00:33:28.680
each of those VMs are accessing
different parts of

00:33:28.680 --> 00:33:30.820
the disk, you're actually
going to get additive

00:33:30.820 --> 00:33:31.490
performance.

00:33:31.490 --> 00:33:35.330
I mean, it's going to perform
like-- it's going to seem to

00:33:35.330 --> 00:33:37.110
defy the laws of physics because
you're going to have

00:33:37.110 --> 00:33:39.446
hundreds of VMs accessing
hundreds of blocks on hundreds

00:33:39.446 --> 00:33:41.560
of physical devices over
hundreds of NICs.

00:33:41.560 --> 00:33:44.890
Now, let's say you have the
opposite case where somehow,

00:33:44.890 --> 00:33:47.520
you've synchronized it so that
ever VM reads from the same

00:33:47.520 --> 00:33:49.830
block at the same
exact moment.

00:33:49.830 --> 00:33:52.100
We're smart enough not to go
to disk each time on that.

00:33:52.100 --> 00:33:55.940
But you're going to throttle
the network card on the few

00:33:55.940 --> 00:33:57.950
machines that are holding
onto that block.

00:33:57.950 --> 00:34:00.780
So it's going to depend
on your workload.

00:34:00.780 --> 00:34:03.830
The more random the access, the
more linear that thing's

00:34:03.830 --> 00:34:04.772
going to scale.

00:34:04.772 --> 00:34:08.630
AUDIENCE: All right, thanks.

00:34:08.630 --> 00:34:09.060
AUDIENCE: Hi, there.

00:34:09.060 --> 00:34:11.929
Very briefly, you mentioned
that it behaves very, very

00:34:11.929 --> 00:34:14.730
well with the EXT3 and EXT4.

00:34:14.730 --> 00:34:18.960
Have any of the other funkier
file systems been tried with

00:34:18.960 --> 00:34:19.725
any degree of success?

00:34:19.725 --> 00:34:21.280
JAY JUDKOWITZ: Is "funky"
a technical term?

00:34:21.280 --> 00:34:23.780
AUDIENCE: I believe so, yes.

00:34:23.780 --> 00:34:25.679
ANDREW KADATCH: Excuse me, I
didn't hear the question.

00:34:25.679 --> 00:34:27.642
JAY JUDKOWITZ: He asked, beside
EXT3 and EXT4, are

00:34:27.642 --> 00:34:30.679
there other file systems
we played with?

00:34:30.679 --> 00:34:31.420
ANDREW KADATCH: Yes.

00:34:31.420 --> 00:34:34.929
We played with other
file systems.

00:34:34.929 --> 00:34:40.949
We liked performance of EXT4
in our tests, aggregate

00:34:40.949 --> 00:34:44.159
workloads, aggregate performance
across a variety

00:34:44.159 --> 00:34:47.210
of workloads that we tried
on, which is by no means

00:34:47.210 --> 00:34:50.870
representative or matching
your specific needs.

00:34:50.870 --> 00:34:54.925
It did the best overall.

00:34:54.925 --> 00:34:57.800
AUDIENCE: Okay, thank you.

00:34:57.800 --> 00:35:00.160
JAY JUDKOWITZ: Other
questions?

00:35:00.160 --> 00:35:02.260
Still have five more minutes,
so we'll have them lock the

00:35:02.260 --> 00:35:03.856
doors and make you
ask questions.

00:35:03.856 --> 00:35:04.696
AUDIENCE: Can I ask
a quick question?

00:35:04.696 --> 00:35:05.648
JAY JUDKOWITZ: Hey,
absolutely.

00:35:05.648 --> 00:35:08.742
AUDIENCE: So if you get to the
point where you need a big

00:35:08.742 --> 00:35:11.122
chunk of it to come down, is
there a best practice for

00:35:11.122 --> 00:35:13.270
downloading from the
persistent disk?

00:35:13.270 --> 00:35:13.830
JAY JUDKOWITZ: I apologize.

00:35:13.830 --> 00:35:15.870
You're actually way taller
than the mic.

00:35:15.870 --> 00:35:17.050
Can you--?

00:35:17.050 --> 00:35:18.470
AUDIENCE: I'm just wondering if
you get to the point where

00:35:18.470 --> 00:35:21.620
you actually want to retrieve
the data, are there best

00:35:21.620 --> 00:35:25.965
practices for being able to
download it efficiently to

00:35:25.965 --> 00:35:27.788
your local system?

00:35:30.860 --> 00:35:32.850
ANDREW KADATCH: There are
multiple ways to do it.

00:35:32.850 --> 00:35:35.020
And I think what I'll
say applies

00:35:35.020 --> 00:35:37.670
to most cloud storages.

00:35:37.670 --> 00:35:41.450
Like I said, using similar
means, you would copy the data

00:35:41.450 --> 00:35:42.620
to another machine.

00:35:42.620 --> 00:35:45.110
Or you could copy it
to [? up-cloud ?]

00:35:45.110 --> 00:35:47.740
object storage, or
for instance,

00:35:47.740 --> 00:35:49.220
Google Cloud Storage.

00:35:49.220 --> 00:35:54.140
And we can get your data
available from anywhere.

00:35:54.140 --> 00:35:58.040
So there are a variety of
ways to accomplish that.

00:35:58.040 --> 00:36:00.240
JAY JUDKOWITZ: So Google Cloud
Storage in particular, if

00:36:00.240 --> 00:36:02.710
you're uploading to cloud
storage, you can use the

00:36:02.710 --> 00:36:04.700
parallelism and the object
composition to

00:36:04.700 --> 00:36:07.610
get stuff up quickly.

00:36:07.610 --> 00:36:10.880
In terms of downloading from
Google Cloud Storage, you have

00:36:10.880 --> 00:36:12.130
the ability to--

00:36:16.560 --> 00:36:20.223
what are the things in terms of
download we can do in GCS?

00:36:23.120 --> 00:36:26.460
Gcutil is this optimized for
getting as much data down as

00:36:26.460 --> 00:36:29.150
quickly as possible.

00:36:29.150 --> 00:36:30.690
And they are working--

00:36:30.690 --> 00:36:33.150
yeah, sorry.

00:36:33.150 --> 00:36:35.030
AUDIENCE: How do you guys manage
your encryption keys,

00:36:35.030 --> 00:36:37.380
and can I supply my
own custom ones?

00:36:37.380 --> 00:36:40.170
ANDREW KADATCH: May I ask you
to talk into the microphone?

00:36:40.170 --> 00:36:43.460
JAY JUDKOWITZ: How are we
managing encryption keys?

00:36:43.460 --> 00:36:46.250
ANDREW KADATCH: There is a
service which owns those keys.

00:36:46.250 --> 00:36:48.190
And they are passed to the VM.

00:36:48.190 --> 00:36:53.110
VM itself cannot query those
keys, or ask to give it to it.

00:36:53.110 --> 00:36:56.890
If you know the top layer,
management layer, we just

00:36:56.890 --> 00:37:00.500
trust the one, and transmit on
separate machines and the

00:37:00.500 --> 00:37:02.290
separate accounts.

00:37:02.290 --> 00:37:04.880
If it believes that this VM
should be able to mount the

00:37:04.880 --> 00:37:09.740
disk, it gives it the command to
do so and passes the keys.

00:37:09.740 --> 00:37:12.470
Nobody else is able to
access those keys.

00:37:12.470 --> 00:37:13.970
We restrict the access.

00:37:13.970 --> 00:37:16.645
And it's audited, et
cetera, et cetera.

00:37:22.310 --> 00:37:24.030
JAY JUDKOWITZ: Well, thank
you all very much again.

00:37:24.030 --> 00:37:26.190
I hope you enjoy the
rest of the show.

00:37:26.190 --> 00:37:26.640
Thanks for your time.

00:37:26.640 --> 00:37:30.330
Oh, and if you have any follow
up stuff, anything you want to

00:37:30.330 --> 00:37:33.320
talk about in terms of feedback,
feature requests,

00:37:33.320 --> 00:37:35.990
anything, do feel free to reach
out to Andrew and I

00:37:35.990 --> 00:37:38.970
directly or the mailing list
on the last slide.

00:37:38.970 --> 00:37:39.870
OK, thank you.

00:37:39.870 --> 00:37:45.872
[APPLAUSE]

