WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.862
[MUSIC PLAYING]

00:00:06.209 --> 00:00:07.750
SPEAKER: Fresh from
Google Zeitgeist,

00:00:07.750 --> 00:00:10.330
we have author Jamie
Metzl here to talk to us

00:00:10.330 --> 00:00:13.920
about his latest book, which
is called "Eternal Sonata."

00:00:13.920 --> 00:00:16.190
And so in a world
where biotechnology

00:00:16.190 --> 00:00:19.030
has increased the length
of human lifespan,

00:00:19.030 --> 00:00:20.090
people start dying.

00:00:20.090 --> 00:00:21.270
Researchers start dying.

00:00:21.270 --> 00:00:22.690
What's going on?

00:00:22.690 --> 00:00:24.720
How can we unravel this mystery?

00:00:24.720 --> 00:00:27.072
This work of
fiction-- or is it--

00:00:27.072 --> 00:00:29.280
is what Jamie will be here
to talk to us about today.

00:00:29.280 --> 00:00:31.620
So please join me in
welcoming him to Google.

00:00:31.620 --> 00:00:34.120
[APPLAUSE]

00:00:35.120 --> 00:00:36.970
JAMIE METZL: Thank you so much.

00:00:36.970 --> 00:00:37.890
Thank you so much.

00:00:37.890 --> 00:00:39.520
I'm really delighted
to be at Google

00:00:39.520 --> 00:00:41.900
because for people who are
thinking about and writing

00:00:41.900 --> 00:00:44.160
about the future,
it's pretty exciting

00:00:44.160 --> 00:00:47.610
to be at a company that's
actually building the future.

00:00:47.610 --> 00:00:50.750
I had a tremendous time
at Google Zeitgeist.

00:00:50.750 --> 00:00:55.020
I loved working with the
outstanding team of the people

00:00:55.020 --> 00:00:56.700
who put together Zeitgeist.

00:00:56.700 --> 00:00:59.460
And still, I think, not
just me, but everybody

00:00:59.460 --> 00:01:01.437
who participated in
that event is still

00:01:01.437 --> 00:01:03.770
connected to each other,
talking to each other, and just

00:01:03.770 --> 00:01:07.220
full of excitement
and enthusiasm

00:01:07.220 --> 00:01:09.440
because we live
in a world that is

00:01:09.440 --> 00:01:12.680
in a tremendous state of flux.

00:01:12.680 --> 00:01:16.110
And the challenge that
we have, for a company

00:01:16.110 --> 00:01:18.700
like Google or for
anybody else, is

00:01:18.700 --> 00:01:22.770
that our brains are designed
for linear thinking.

00:01:22.770 --> 00:01:25.420
When our ancestors were
living on the savannas

00:01:25.420 --> 00:01:29.130
in Africa, if your parents
were afraid of a tiger,

00:01:29.130 --> 00:01:32.190
it was pretty good to learn
that lesson from your parents

00:01:32.190 --> 00:01:35.340
and to think, well, I'd better
be afraid of a tiger or a lion,

00:01:35.340 --> 00:01:39.350
too, because the world had a
huge amount of consistency.

00:01:39.350 --> 00:01:43.420
But now we live in a world
of exponential change,

00:01:43.420 --> 00:01:45.900
and it's really
difficult for people

00:01:45.900 --> 00:01:49.430
to internalize what
exponential change means

00:01:49.430 --> 00:01:53.280
and how it applies to them and
how it applies to all of us.

00:01:53.280 --> 00:01:55.650
And because of the great
work that you at Google

00:01:55.650 --> 00:01:58.280
and others have done,
we are now at a place

00:01:58.280 --> 00:02:02.157
where people in many ways
can internalize Moore's law.

00:02:02.157 --> 00:02:04.240
And I don't need to explain
to the Google audience

00:02:04.240 --> 00:02:07.540
what Moore's law is, but the
law that computing power roughly

00:02:07.540 --> 00:02:12.950
doubles between every
18 months and 2 years.

00:02:12.950 --> 00:02:16.780
And that's why we have
an expectation now

00:02:16.780 --> 00:02:18.990
that our iPhones
or other technology

00:02:18.990 --> 00:02:23.530
will get stronger,
smaller, better, faster.

00:02:23.530 --> 00:02:29.040
And we've just internalized that
that's the way the world works.

00:02:29.040 --> 00:02:33.080
But especially since the end
of the Human Genome Project,

00:02:33.080 --> 00:02:35.420
we have a new
information technology,

00:02:35.420 --> 00:02:39.070
and that information
technology is biology.

00:02:39.070 --> 00:02:42.280
And it's really difficult
for people who've already

00:02:42.280 --> 00:02:45.250
internalized that information
technology systems are

00:02:45.250 --> 00:02:49.390
manipulable and hackable,
it's very difficult for them

00:02:49.390 --> 00:02:52.430
and for all of us
to begin to think

00:02:52.430 --> 00:02:57.870
about what it means that the
human biological system, or all

00:02:57.870 --> 00:03:03.460
biological systems, will be
hackable and transformable.

00:03:03.460 --> 00:03:09.360
And it's a big, big idea because
our species has been around

00:03:09.360 --> 00:03:11.700
for about 4 billion years.

00:03:11.700 --> 00:03:13.990
And for about 4 billion
years, from when

00:03:13.990 --> 00:03:18.320
we were single-cell organisms
until now, multicellular

00:03:18.320 --> 00:03:23.400
organisms, we've evolved
by one set of rules.

00:03:23.400 --> 00:03:27.750
And it's random mutation
and competition.

00:03:27.750 --> 00:03:31.370
And what happens if we
enter a world, which

00:03:31.370 --> 00:03:33.710
we are entering now,
where that's not

00:03:33.710 --> 00:03:35.870
the primary driver of evolution?

00:03:35.870 --> 00:03:38.870
It's guided
manipulation and guided

00:03:38.870 --> 00:03:40.710
selection, not
natural selection,

00:03:40.710 --> 00:03:43.620
but guided, managed selection.

00:03:43.620 --> 00:03:45.170
And what does that mean?

00:03:45.170 --> 00:03:49.380
One of the reasons why I write
novels, among other things,

00:03:49.380 --> 00:03:51.730
is that we don't
know what it means.

00:03:51.730 --> 00:03:53.690
But it's not just a
scientific question.

00:03:53.690 --> 00:03:55.470
It's not just a policy question.

00:03:55.470 --> 00:03:58.620
It's an existential,
human question.

00:03:58.620 --> 00:04:00.880
Not just what are we
going to think about,

00:04:00.880 --> 00:04:07.070
but what are we going to be
as a species in 100, 1,000,

00:04:07.070 --> 00:04:09.670
2,000 5,000 years?

00:04:09.670 --> 00:04:11.720
And it's very,
very likely that we

00:04:11.720 --> 00:04:14.310
are going to be a
species significantly

00:04:14.310 --> 00:04:18.029
different from
what we are today.

00:04:18.029 --> 00:04:24.080
In 1865, Jules Verne wrote
a story about humans landing

00:04:24.080 --> 00:04:27.345
on the moon, and it was science
fiction, pure science fiction,

00:04:27.345 --> 00:04:28.720
the best kind of
science fiction.

00:04:28.720 --> 00:04:31.194
We're just imagining some
kind of crazy future.

00:04:31.194 --> 00:04:33.610
You don't have the tools to
make it happen, but you think,

00:04:33.610 --> 00:04:34.590
wouldn't it be neat?

00:04:34.590 --> 00:04:35.756
And I write science fiction.

00:04:35.756 --> 00:04:38.830
Wouldn't it be amazing
if x could be possible?

00:04:38.830 --> 00:04:42.730
But in 1962, when President
Kennedy gave his famous speech

00:04:42.730 --> 00:04:44.770
at Houston that the
United States was

00:04:44.770 --> 00:04:47.870
going to put a man on
the moon in the decade,

00:04:47.870 --> 00:04:49.530
it wasn't science fiction.

00:04:49.530 --> 00:04:51.290
We had all of
these technologies.

00:04:51.290 --> 00:04:52.330
We had missiles.

00:04:52.330 --> 00:04:53.570
We had the physics to do it.

00:04:53.570 --> 00:04:56.570
We hadn't put all of the
pieces together exactly,

00:04:56.570 --> 00:04:59.940
but it wasn't some crazy guess.

00:04:59.940 --> 00:05:01.750
It was real.

00:05:01.750 --> 00:05:06.830
And with genetic technologies
leading to the transformation

00:05:06.830 --> 00:05:11.830
of how we make babies, the
nature of the babies we make,

00:05:11.830 --> 00:05:15.170
how we age, how we do many,
many different things,

00:05:15.170 --> 00:05:17.100
this isn't 1865.

00:05:17.100 --> 00:05:18.640
It's 1962.

00:05:18.640 --> 00:05:21.870
We have all of the tools
that we need to, over time,

00:05:21.870 --> 00:05:24.060
fundamentally
transform our species.

00:05:24.060 --> 00:05:26.420
We haven't put all
the pieces together.

00:05:26.420 --> 00:05:30.890
But we don't need to invent some
new, unimaginable technology

00:05:30.890 --> 00:05:34.640
in order to do these things,
although we will, in fact, do

00:05:34.640 --> 00:05:36.820
that, in part because
of the great work

00:05:36.820 --> 00:05:38.200
that you guys are
doing at Google

00:05:38.200 --> 00:05:39.430
with artificial intelligence.

00:05:39.430 --> 00:05:41.770
And our collective
intelligence as a species

00:05:41.770 --> 00:05:43.910
is going to get
greater and greater

00:05:43.910 --> 00:05:49.740
and be more and more capable of
driving technological change.

00:05:49.740 --> 00:05:52.370
But we have all of
these tools now.

00:05:52.370 --> 00:05:55.480
And the way I like to make
this concrete for people

00:05:55.480 --> 00:05:57.930
is to say that if you had a
time machine, if we had a time

00:05:57.930 --> 00:06:01.210
machine and traveled
1,000 years into the past,

00:06:01.210 --> 00:06:03.350
and we took a baby
from 1,000 years ago

00:06:03.350 --> 00:06:06.200
and brought that
baby back to today

00:06:06.200 --> 00:06:08.540
and placed that
child with a family,

00:06:08.540 --> 00:06:11.290
that child would be
a normal kid, just

00:06:11.290 --> 00:06:15.140
like anybody else at any
school down the street here.

00:06:15.140 --> 00:06:16.490
Take the same time machine.

00:06:16.490 --> 00:06:18.520
Travel 1,000 years
into the future.

00:06:18.520 --> 00:06:22.110
Take that kid, and bring
that kid back to today.

00:06:22.110 --> 00:06:26.270
And that kid would be a
genetic superman or superwoman.

00:06:26.270 --> 00:06:28.020
They'd live longer.

00:06:28.020 --> 00:06:31.070
They'd be immune to
lots of diseases.

00:06:31.070 --> 00:06:32.160
They wouldn't carry.

00:06:32.160 --> 00:06:34.870
They wouldn't have the potential
to have a lot of diseases.

00:06:34.870 --> 00:06:36.740
They'd be more
intelligent, and then we

00:06:36.740 --> 00:06:38.780
could go down the list
of all of these things

00:06:38.780 --> 00:06:42.240
that child in the future
is likely to have.

00:06:42.240 --> 00:06:45.600
And so how are we going
to get from here to there?

00:06:45.600 --> 00:06:50.520
And it's a cliche, but we
will get there in baby steps.

00:06:50.520 --> 00:06:53.250
And there are three
technologies that

00:06:53.250 --> 00:06:57.830
already exist that are going
to be driving this way forward.

00:06:57.830 --> 00:07:02.350
And the first one is IVF,
In Vitro Fertilization,

00:07:02.350 --> 00:07:03.680
and embryo selection.

00:07:03.680 --> 00:07:06.310
So IVF, when it
started in the 1980s,

00:07:06.310 --> 00:07:07.860
was a revolutionary technology.

00:07:07.860 --> 00:07:10.840
When Louise Brown, the first
so-called test-tube baby,

00:07:10.840 --> 00:07:13.450
was born, there were people
at that time who said,

00:07:13.450 --> 00:07:14.970
this is an abomination.

00:07:14.970 --> 00:07:16.620
It's a crime against nature.

00:07:16.620 --> 00:07:19.480
But now everybody accepts,
most people accept,

00:07:19.480 --> 00:07:21.870
that IVF is a very,
very legitimate way

00:07:21.870 --> 00:07:25.430
and an effective way
to have children.

00:07:25.430 --> 00:07:28.490
On top of IVF,
there is a procedure

00:07:28.490 --> 00:07:31.840
called Preimplantation
Genetic Diagnosis, PGD,

00:07:31.840 --> 00:07:35.730
or Preimplantation
Genetic Selection, PGS.

00:07:35.730 --> 00:07:38.340
What that basically
means is in IVF,

00:07:38.340 --> 00:07:40.190
a woman has her eggs extracted.

00:07:40.190 --> 00:07:44.730
And the average number of
eggs extracted in this process

00:07:44.730 --> 00:07:45.670
is about 15.

00:07:45.670 --> 00:07:47.090
Sometimes it's a bit more.

00:07:47.090 --> 00:07:50.840
Sometimes it's a bit less,
but that's the average.

00:07:50.840 --> 00:07:57.290
And in IVF, then those eggs
are fertilized by sperm.

00:07:57.290 --> 00:07:59.210
The average male
ejaculation, I guess

00:07:59.210 --> 00:08:02.520
we can say this at Google, has
hundreds of millions of sperm,

00:08:02.520 --> 00:08:07.040
whereas the egg production,
as I mentioned, is limited.

00:08:07.040 --> 00:08:08.760
So you fertilize
however many eggs

00:08:08.760 --> 00:08:11.440
you have of those that
can be fertilized.

00:08:11.440 --> 00:08:15.420
And let's just say you
have 10 fertilized eggs.

00:08:15.420 --> 00:08:17.860
In PGD or PGS,
what you do is you

00:08:17.860 --> 00:08:22.060
let those fertilized eggs grow
to be 5-day-old zygotes, which

00:08:22.060 --> 00:08:24.370
are early stage embryos.

00:08:24.370 --> 00:08:27.800
And then you extract two
cells from each of those,

00:08:27.800 --> 00:08:31.250
and then you screen those
using genome sequencing.

00:08:31.250 --> 00:08:33.890
So just a little bit of an
aside on genome sequencing,

00:08:33.890 --> 00:08:37.840
in 2003 when the Human
Genome Project ended,

00:08:37.840 --> 00:08:43.880
it had taken 10 years, 13 years
actually, and cost $1 billion.

00:08:43.880 --> 00:08:47.710
Today, we can sequence a
genome in a couple of days,

00:08:47.710 --> 00:08:49.210
and it costs $1,000.

00:08:49.210 --> 00:08:52.770
Within 5 years, it's
going to be around 50

00:08:52.770 --> 00:08:56.140
and take hours, and after that
it's going to be even less

00:08:56.140 --> 00:08:58.080
and take minutes.

00:08:58.080 --> 00:09:03.350
And so what does it mean now
that in the process of PGD

00:09:03.350 --> 00:09:07.550
we can screen using this
low-cost genome sequencing?

00:09:07.550 --> 00:09:11.120
Well, today it means that we can
screen for a number of things,

00:09:11.120 --> 00:09:16.740
the easy and obvious ones-- for
gender, for hair and eye color,

00:09:16.740 --> 00:09:21.511
and for single-gene mutations,
so-called Mendelian diseases.

00:09:21.511 --> 00:09:23.010
And you know what
some of those are.

00:09:23.010 --> 00:09:27.260
It's Tay-Sachs, sickle
cell disease, Huntington's.

00:09:27.260 --> 00:09:29.860
There's a long list of these.

00:09:29.860 --> 00:09:34.610
And so today, people, especially
people undergoing high risk

00:09:34.610 --> 00:09:37.120
pregnancies, either because
of the age of the parents

00:09:37.120 --> 00:09:38.890
or because the parents
may be carriers

00:09:38.890 --> 00:09:42.360
of some type of single-gene
mutation diseases,

00:09:42.360 --> 00:09:45.660
we can screen those embryos
and decide which of the 10

00:09:45.660 --> 00:09:47.550
to implant.

00:09:47.550 --> 00:09:51.190
But as more and more
genomes are sequenced,

00:09:51.190 --> 00:09:53.230
we're going to know
more and more about how

00:09:53.230 --> 00:09:56.180
the genome works, not just
for single-gene mutations,

00:09:56.180 --> 00:09:57.410
but for everything.

00:09:57.410 --> 00:10:00.520
I think all of you have heard
about precision medicine,

00:10:00.520 --> 00:10:02.430
or personalized medicine.

00:10:02.430 --> 00:10:04.170
And everyone kind of
understands that it

00:10:04.170 --> 00:10:06.490
means that your medicine,
your health care,

00:10:06.490 --> 00:10:09.750
is going to be tailored
to who you are.

00:10:09.750 --> 00:10:11.660
But what does that really mean?

00:10:11.660 --> 00:10:14.120
Well, it means that
one of the foundations

00:10:14.120 --> 00:10:18.430
of your medical record will
be your sequenced genome.

00:10:18.430 --> 00:10:22.390
It will be sequenced when
you're a five-day-old embryo.

00:10:22.390 --> 00:10:27.040
And as first millions, and then
hundreds of millions, and then

00:10:27.040 --> 00:10:30.610
billions of people have
their genome sequenced,

00:10:30.610 --> 00:10:33.550
and then we compare the
genotype, what the genes say,

00:10:33.550 --> 00:10:36.960
and the phenotype of
their life experience--

00:10:36.960 --> 00:10:40.350
how old were they when they got
some kind of genetic diseases?

00:10:40.350 --> 00:10:42.050
How tall were they?

00:10:42.050 --> 00:10:43.920
What was their IQ?

00:10:43.920 --> 00:10:47.080
Did they win the Olympics
in the hundred meters?

00:10:47.080 --> 00:10:50.370
Anything that would be
influenced by genetics,

00:10:50.370 --> 00:10:52.240
we're going to know
a lot more than just

00:10:52.240 --> 00:10:54.790
these single-gene mutations,
on-off switches, which

00:10:54.790 --> 00:10:56.200
are relatively rare.

00:10:56.200 --> 00:10:58.220
We're going to be
able to continue

00:10:58.220 --> 00:11:00.400
to use what's called the
genome-wide association

00:11:00.400 --> 00:11:05.600
study to figure out how
complex patterns of genes

00:11:05.600 --> 00:11:10.300
influence certain
behaviors or traits.

00:11:10.300 --> 00:11:16.660
So now in the process of IVF and
PGD, we're going to know a lot.

00:11:16.660 --> 00:11:20.210
And so parents, having a
child, the first question

00:11:20.210 --> 00:11:24.170
the parents are going to ask is,
is my child carrying some kind

00:11:24.170 --> 00:11:26.610
of terrible genetic disease?

00:11:26.610 --> 00:11:29.070
And everybody is going
to want to know that.

00:11:29.070 --> 00:11:31.580
And that's why we're going
to see a significant increase

00:11:31.580 --> 00:11:34.840
in people who don't
have indications

00:11:34.840 --> 00:11:39.430
under the current structure
wanting to have kids, perfectly

00:11:39.430 --> 00:11:42.510
young and healthy people,
wanting to have kids

00:11:42.510 --> 00:11:45.190
through IVF and PGD
because we're going

00:11:45.190 --> 00:11:49.970
to be able to screen out genetic
diseases that nobody will want.

00:11:49.970 --> 00:11:51.980
My last book, "Genesis
Code," one of the reasons

00:11:51.980 --> 00:11:55.670
why it was featured in
"Cosmopolitan Magazine"

00:11:55.670 --> 00:11:59.072
is I was at a party in New
York, and I met the editor.

00:11:59.072 --> 00:12:01.530
And I said, hey, you should
really do something on my book.

00:12:01.530 --> 00:12:03.740
And she said, well,
what's it about?

00:12:03.740 --> 00:12:06.600
And I'd seen how they do
titles in the grocery store.

00:12:06.600 --> 00:12:08.384
I said, well, it's
about the end of sex.

00:12:08.384 --> 00:12:10.300
She goes, the end of
sex, what does that mean?

00:12:10.800 --> 00:12:14.490
I said, well, my contention
is that as people

00:12:14.490 --> 00:12:17.150
become more and more comfortable
with embryo screening

00:12:17.150 --> 00:12:19.270
and recognize what
embryo screening can

00:12:19.270 --> 00:12:22.330
do for the long-term
health of their children,

00:12:22.330 --> 00:12:24.260
that more and more
people are going

00:12:24.260 --> 00:12:28.390
to choose to conceive through
IVF and embryo selection.

00:12:28.390 --> 00:12:32.320
And less people will conceive
through traditional sex,

00:12:32.320 --> 00:12:35.140
ultimately leading to the
end of sex for procreation,

00:12:35.140 --> 00:12:38.770
not for all the other wonderful
reasons why people have sex,

00:12:38.770 --> 00:12:41.400
but not for procreation.

00:12:41.400 --> 00:12:44.170
I certainly think that
over the coming decades,

00:12:44.170 --> 00:12:46.320
that is going to happen.

00:12:46.320 --> 00:12:50.450
It'll start with the more
advantaged people in societies,

00:12:50.450 --> 00:12:52.260
but it will eventually
trickle down.

00:12:52.260 --> 00:12:53.950
And why will it trickle down?

00:12:53.950 --> 00:12:58.090
Because societies
bear a tremendous cost

00:12:58.090 --> 00:13:00.290
of treating and
providing lifetime

00:13:00.290 --> 00:13:04.150
care to people who are born
with these genetic diseases.

00:13:04.150 --> 00:13:08.210
And some of these
people live 80, 90 years

00:13:08.210 --> 00:13:09.470
and very meaningful lives.

00:13:09.470 --> 00:13:12.720
But if society wasn't
paying those costs,

00:13:12.720 --> 00:13:17.590
and you add up all of the costs
of lifetime care for people

00:13:17.590 --> 00:13:19.940
with genetic diseases,
and then said,

00:13:19.940 --> 00:13:22.720
well, what would be the cost of
screening the entire population

00:13:22.720 --> 00:13:25.700
to eliminate those diseases--
and it's very tricky

00:13:25.700 --> 00:13:29.380
because people who already
live have these diseases.

00:13:29.380 --> 00:13:32.990
It's not at all to say that
your life isn't valued.

00:13:32.990 --> 00:13:34.330
It is.

00:13:34.330 --> 00:13:36.170
But if we had a
choice as a society

00:13:36.170 --> 00:13:40.470
to eliminate terrible diseases
that are killing people--

00:13:40.470 --> 00:13:42.470
for example, some diseases
are killing people

00:13:42.470 --> 00:13:43.844
by the time they're
10 years old.

00:13:43.844 --> 00:13:46.350
And we could say, we can
guarantee that your kids

00:13:46.350 --> 00:13:48.970
won't have those diseases.

00:13:48.970 --> 00:13:51.530
Some people will say,
well, it's God's plan,

00:13:51.530 --> 00:13:54.060
and we should just
see what happens.

00:13:54.060 --> 00:13:56.280
But I think many
other people will say,

00:13:56.280 --> 00:13:57.990
they're all my natural children.

00:13:57.990 --> 00:14:01.310
If I can choose one who doesn't
have a terrible disease,

00:14:01.310 --> 00:14:03.240
I think I would do that.

00:14:03.240 --> 00:14:05.390
Once you've opened
that door, once people

00:14:05.390 --> 00:14:09.640
are in the process of selecting
their early-stage embryos,

00:14:09.640 --> 00:14:12.670
then there's more information
that's potentially available.

00:14:12.670 --> 00:14:15.230
Right now, we're
right on the verge

00:14:15.230 --> 00:14:18.490
of being able to predict
the height of a child

00:14:18.490 --> 00:14:20.900
based on the genetic
screening when

00:14:20.900 --> 00:14:22.859
they're a 5-day-old embryo.

00:14:22.859 --> 00:14:24.900
Within a decade-- and my
friend Steve [? Schuh ?]

00:14:24.900 --> 00:14:27.510
has done a lot of work
on this-- it's likely

00:14:27.510 --> 00:14:30.360
that we're going to be able to
have a pretty good estimation,

00:14:30.360 --> 00:14:33.050
again provided that the
child gets proper health care

00:14:33.050 --> 00:14:35.080
and love and all those
things, but a pretty

00:14:35.080 --> 00:14:38.690
good estimation of the
future IQ of a child

00:14:38.690 --> 00:14:44.010
when that child is not a child
but a five-day-old early stage

00:14:44.010 --> 00:14:44.870
embryo.

00:14:44.870 --> 00:14:49.520
So embryo selection is the first
step in this transformation.

00:14:49.520 --> 00:14:53.790
The second step will be
increasing the number of eggs

00:14:53.790 --> 00:14:57.670
and therefore increasing
the range of choice.

00:14:57.670 --> 00:14:59.330
As I mentioned,
the average woman,

00:14:59.330 --> 00:15:03.300
having their eggs extracted,
has about 15 eggs.

00:15:03.300 --> 00:15:06.800
But what if we could increase
that number to 100 or 1,000?

00:15:06.800 --> 00:15:09.980
Again, as I mentioned before,
sperm is a dime a dozen.

00:15:09.980 --> 00:15:13.960
And so we can do that
in animal studies

00:15:13.960 --> 00:15:17.250
now through induced
pluripotent stem cells.

00:15:17.250 --> 00:15:20.420
So basically what you do is
you take any kind of cell,

00:15:20.420 --> 00:15:24.190
but blood and skin
cells are the easiest.

00:15:24.190 --> 00:15:27.030
So you take, let's
say, a skin cell.

00:15:27.030 --> 00:15:30.330
You turn the skin cell into
a stem cell, an induced stem

00:15:30.330 --> 00:15:32.900
cell, and then
turn those induced

00:15:32.900 --> 00:15:35.430
stem cells, which can
become anything-- that's

00:15:35.430 --> 00:15:41.180
why their stem cells-- into egg
cells, and egg cells into eggs.

00:15:41.180 --> 00:15:44.210
And now instead of 15
eggs, you have 100.

00:15:44.210 --> 00:15:44.900
You have 1,000.

00:15:44.900 --> 00:15:46.250
Let's call it 100.

00:15:46.250 --> 00:15:48.684
And then you use your
hundreds of millions of sperm

00:15:48.684 --> 00:15:49.600
that you already have.

00:15:49.600 --> 00:15:51.350
You fertilize 100.

00:15:51.350 --> 00:15:53.750
And then you grow
those to 5 days.

00:15:53.750 --> 00:15:56.530
You take out 2 cells
from each of those 100.

00:15:56.530 --> 00:15:59.490
You sequence each of them,
again, because by this time

00:15:59.490 --> 00:16:02.300
sequencing will
cost almost nothing.

00:16:02.300 --> 00:16:06.070
And then instead of having 10
choices, you have 100 choices.

00:16:06.070 --> 00:16:09.200
And in 100 kids, of
your own natural kids,

00:16:09.200 --> 00:16:11.420
what's the range of IQs?

00:16:11.420 --> 00:16:14.120
What's the range of any kind
of thing that you might select?

00:16:14.120 --> 00:16:15.560
And you'll have a little chart.

00:16:15.560 --> 00:16:19.650
And you'll say, well, we know
that these 10 will definitely

00:16:19.650 --> 00:16:21.480
have Down syndrome.

00:16:21.480 --> 00:16:24.980
And these 5 may have
Huntington's disease.

00:16:24.980 --> 00:16:27.760
And these 20 may have
a high likelihood

00:16:27.760 --> 00:16:30.690
of being mathematical geniuses,
whatever those things are.

00:16:30.690 --> 00:16:33.610
And in jurisdictions
where this is legal--

00:16:33.610 --> 00:16:36.020
and it won't be in
every jurisdiction--

00:16:36.020 --> 00:16:39.260
people will have the ability
to make those choices.

00:16:39.260 --> 00:16:42.290
And as a society, I think we're
more comfortable with making

00:16:42.290 --> 00:16:43.630
choices about health.

00:16:43.630 --> 00:16:46.040
And making choices
about health and making

00:16:46.040 --> 00:16:49.640
choices about other traits
will be the exact same process.

00:16:49.640 --> 00:16:52.130
So it will be a policy issue.

00:16:52.130 --> 00:16:55.090
It will be a personal choice
issue, not a science issue,

00:16:55.090 --> 00:16:58.800
what information we'll
all be able to have.

00:16:58.800 --> 00:17:03.970
And then the third phase of
this, again, already underway,

00:17:03.970 --> 00:17:06.520
is using precision gene editing.

00:17:06.520 --> 00:17:10.520
I think everybody here has
most likely heard of CRISPR.

00:17:10.520 --> 00:17:13.829
And CRISPR is this
amazing, new technology

00:17:13.829 --> 00:17:16.869
that allows scientists
to edit the genome

00:17:16.869 --> 00:17:20.319
with a level of
inexpensive simplicity

00:17:20.319 --> 00:17:23.920
that would have been mind
boggling just a decade ago.

00:17:23.920 --> 00:17:26.190
It's the equivalent of
being at your computer

00:17:26.190 --> 00:17:28.560
with a Microsoft Word program.

00:17:28.560 --> 00:17:31.850
And you see something, a
letter you want to change.

00:17:31.850 --> 00:17:34.870
You put the cursor there, and
you eliminate that letter.

00:17:34.870 --> 00:17:36.580
And you can add another letter.

00:17:36.580 --> 00:17:38.750
So that's what we
can do in the genome.

00:17:38.750 --> 00:17:42.030
We can make a double-strand
cut in the genome,

00:17:42.030 --> 00:17:44.350
and we can either
let it repair itself

00:17:44.350 --> 00:17:46.010
with just a missing space.

00:17:46.010 --> 00:17:49.000
We can add new DNA to
fill that hole that's

00:17:49.000 --> 00:17:51.860
giving it different instruction
to create a different protein.

00:17:51.860 --> 00:17:56.340
We can turn on a gene
or turn off a gene.

00:17:56.340 --> 00:17:58.190
And so what will that mean?

00:17:58.190 --> 00:18:01.990
Well, it's incredible because
it was only three years ago

00:18:01.990 --> 00:18:04.550
that scientists
figured out how to use

00:18:04.550 --> 00:18:09.870
CRISPR to alter the DNA of a
living cell, so three years.

00:18:09.870 --> 00:18:11.100
That's a tool.

00:18:11.100 --> 00:18:17.310
Two years later was the advent
of functional gene drives.

00:18:17.310 --> 00:18:19.810
And basically what
gene drives does is it

00:18:19.810 --> 00:18:22.980
puts this tool, this
molecular scissors,

00:18:22.980 --> 00:18:28.100
inside of the DNA of a
sexually reproducing organism.

00:18:28.100 --> 00:18:30.662
And what it basically
does is it can guarantee--

00:18:30.662 --> 00:18:32.620
and I can go into this
later if anybody wants--

00:18:32.620 --> 00:18:36.630
but it can guarantee that
a genetic trait becomes

00:18:36.630 --> 00:18:39.934
dominant from one parent.

00:18:39.934 --> 00:18:41.600
And so it means, let's
just say that you

00:18:41.600 --> 00:18:43.560
have a fruit fly
with light eyes,

00:18:43.560 --> 00:18:46.980
and you want to push light
eyes through the population.

00:18:46.980 --> 00:18:48.870
You just attach this
molecular scissors

00:18:48.870 --> 00:18:52.400
so that when this one fruit fly
mates with another fruit fly,

00:18:52.400 --> 00:18:55.070
it doesn't matter what the
other fruit fly's genetics are

00:18:55.070 --> 00:18:57.810
because the first
fruit fly is passing

00:18:57.810 --> 00:18:59.860
this little tool, this
little machine, that's

00:18:59.860 --> 00:19:01.750
changing the second.

00:19:01.750 --> 00:19:04.190
And so it guarantees
that you will have

00:19:04.190 --> 00:19:05.990
a specific genetic outcome.

00:19:05.990 --> 00:19:08.240
Now you have two,
and all of their kids

00:19:08.240 --> 00:19:09.880
won't just have that trait.

00:19:09.880 --> 00:19:12.690
They will also have
the tool that is also

00:19:12.690 --> 00:19:14.490
reproducing within the cell.

00:19:14.490 --> 00:19:17.180
And so from one
mosquito, you can

00:19:17.180 --> 00:19:19.355
transform an entire species.

00:19:19.355 --> 00:19:20.730
And that's why
people are talking

00:19:20.730 --> 00:19:23.810
about gene drives
as a potential way

00:19:23.810 --> 00:19:25.700
to wipe out Zika or malaria.

00:19:25.700 --> 00:19:28.790
The upside is you can
wipe out Zika or malaria.

00:19:28.790 --> 00:19:30.290
The downside is if
you get it wrong,

00:19:30.290 --> 00:19:32.050
you could wipe out
all of life on Earth.

00:19:32.050 --> 00:19:33.880
That's the down.

00:19:33.880 --> 00:19:39.460
But one thing is clear, that
precision gene editing is just

00:19:39.460 --> 00:19:43.380
going to fundamentally
transform our world

00:19:43.380 --> 00:19:47.240
at all levels-- on the plant
level, on the animal level,

00:19:47.240 --> 00:19:48.400
and on the us level.

00:19:48.400 --> 00:19:51.710
And when people talk
about different inventions

00:19:51.710 --> 00:19:54.680
in human history, you think,
what were the really, really

00:19:54.680 --> 00:19:55.770
big ones?

00:19:55.770 --> 00:19:57.310
Well, there was fire.

00:19:57.310 --> 00:20:01.700
There was harnessing steam
power, maybe the internet.

00:20:01.700 --> 00:20:03.390
I mean, what are these things?

00:20:03.390 --> 00:20:06.269
It's not like
inventing the glasses

00:20:06.269 --> 00:20:08.060
with the little hook
that goes on your nose

00:20:08.060 --> 00:20:08.810
so it doesn't slip.

00:20:08.810 --> 00:20:09.800
That's a reference to a movie.

00:20:09.800 --> 00:20:11.050
I don't know if anyone saw it.

00:20:11.050 --> 00:20:11.560
No.

00:20:11.560 --> 00:20:19.810
All right, but CRISPR, in
the history of our species,

00:20:19.810 --> 00:20:24.950
it is a steam-power
level innovation.

00:20:24.950 --> 00:20:29.100
And just this week, in Berkeley,
actually, not far from here,

00:20:29.100 --> 00:20:32.350
there was a new approach, which
it seems very, very likely

00:20:32.350 --> 00:20:37.330
will cure sickle cell
disease using CRISPR.

00:20:37.330 --> 00:20:41.230
And the power of gene
editing is incredible.

00:20:41.230 --> 00:20:43.340
And a word of warning,
though, about it

00:20:43.340 --> 00:20:51.480
is that I, for one, believe
that humans are massively,

00:20:51.480 --> 00:20:53.100
but not infinitely, complex.

00:20:53.100 --> 00:20:56.810
We are single-cell organisms
that have gone wild.

00:20:56.810 --> 00:20:58.930
And we know that we can
understand pretty well

00:20:58.930 --> 00:21:00.800
how a single-cell
organism functions.

00:21:00.800 --> 00:21:03.420
So if we can understand
a single-cell organism

00:21:03.420 --> 00:21:06.100
conceptually, we should
ultimately over time

00:21:06.100 --> 00:21:07.980
be able to understand ourselves.

00:21:07.980 --> 00:21:09.680
Right now, our
level of complexity

00:21:09.680 --> 00:21:12.370
is far greater than
our computing power

00:21:12.370 --> 00:21:14.180
and the power of our machines.

00:21:14.180 --> 00:21:18.690
But we are kind of at
maybe a linear evolution,

00:21:18.690 --> 00:21:21.290
but our machines are at
an exponential evolution.

00:21:21.290 --> 00:21:24.690
So there will come a time--
and it will be definitely

00:21:24.690 --> 00:21:27.100
less than 100 years from now,
probably less than 50 years

00:21:27.100 --> 00:21:28.830
from now-- where
our computers will

00:21:28.830 --> 00:21:32.910
be able to understand the
complexity of the human body,

00:21:32.910 --> 00:21:35.550
including of the
human brain, which

00:21:35.550 --> 00:21:40.510
is by far the most complex
thing in the known universe now.

00:21:40.510 --> 00:21:42.270
So there's a lot
that we don't know,

00:21:42.270 --> 00:21:44.780
and we need to be very,
very careful, obviously,

00:21:44.780 --> 00:21:48.400
in messing with complex systems
that we don't fully understand.

00:21:48.400 --> 00:21:51.110
And that's why I think
that embryo selection will

00:21:51.110 --> 00:21:54.280
be the entry level, the
starter drug, because you don't

00:21:54.280 --> 00:21:57.132
need to understand
how the genome works

00:21:57.132 --> 00:21:58.090
to do embryo selection.

00:21:58.090 --> 00:22:00.070
You just have a guess.

00:22:00.070 --> 00:22:04.320
And because all of the kids are
your own natural, unadulterated

00:22:04.320 --> 00:22:09.170
children, people making
those kinds of guesses

00:22:09.170 --> 00:22:12.160
can lead to some pretty
significant changes

00:22:12.160 --> 00:22:13.970
across the population.

00:22:13.970 --> 00:22:15.440
This is a lot of background.

00:22:15.440 --> 00:22:17.650
My new novel,
"Eternal Sonata," is

00:22:17.650 --> 00:22:21.080
about extreme life extension,
and it's the same curve.

00:22:21.080 --> 00:22:22.580
We're going to be
able increasingly

00:22:22.580 --> 00:22:26.100
to understand the genetic
foundation of extreme ages,

00:22:26.100 --> 00:22:27.400
people who live longer.

00:22:27.400 --> 00:22:30.170
And we're going to be able
to select against that.

00:22:30.170 --> 00:22:33.200
We already, as a result of our
understanding of the genome,

00:22:33.200 --> 00:22:36.330
have new classes of
drugs that can regulate

00:22:36.330 --> 00:22:40.000
the balance between cellular
growth and cellular repair

00:22:40.000 --> 00:22:43.150
because if you're emphasizing
repair, you can live longer.

00:22:43.150 --> 00:22:45.250
We have new,
nascent technologies

00:22:45.250 --> 00:22:48.830
for blood transfusions
that share plasma

00:22:48.830 --> 00:22:51.740
and between younger
and older mice

00:22:51.740 --> 00:22:53.360
and younger and older rats.

00:22:53.360 --> 00:22:57.200
And the older rats, getting the
younger blood, become younger.

00:22:57.200 --> 00:22:58.890
Their skin is more supple.

00:22:58.890 --> 00:23:01.010
They remember things better.

00:23:01.010 --> 00:23:04.300
And the younger mice
getting the older blood

00:23:04.300 --> 00:23:07.430
transfused, they become slower.

00:23:07.430 --> 00:23:12.060
And so we're unlocking all
of these new ways of hacking

00:23:12.060 --> 00:23:15.180
into the human system.

00:23:15.180 --> 00:23:17.550
It is going to make all
of these things that

00:23:17.550 --> 00:23:21.000
seemed fixed, especially
our own biology,

00:23:21.000 --> 00:23:25.190
increasingly seem
variable in the future.

00:23:25.190 --> 00:23:29.170
And the adoption rates of
these technologies, they're

00:23:29.170 --> 00:23:30.220
going to normalize.

00:23:30.220 --> 00:23:32.060
Again, societies
will have an interest

00:23:32.060 --> 00:23:34.320
in having healthier populations.

00:23:34.320 --> 00:23:36.730
When people feel
it's safe, people

00:23:36.730 --> 00:23:40.150
will want to have healthier,
maybe even more capable

00:23:40.150 --> 00:23:42.266
children than they
otherwise would have had.

00:23:42.266 --> 00:23:43.640
If given the
choice, people would

00:23:43.640 --> 00:23:49.040
like to live longer,
healthier, more robust lives.

00:23:49.040 --> 00:23:51.330
And there will be competition
on many of these things,

00:23:51.330 --> 00:23:53.900
both within and
between societies.

00:23:53.900 --> 00:23:57.150
That you're in a society,
and somebody else

00:23:57.150 --> 00:23:59.180
is doing something
that potentially

00:23:59.180 --> 00:24:01.680
will give their
kids an advantage,

00:24:01.680 --> 00:24:04.970
will you not consider at least
doing that for your kids?

00:24:04.970 --> 00:24:07.620
I spend a lot of time in
Korea, and in Korea people

00:24:07.620 --> 00:24:10.710
are spending 50% of
their disposable outcome

00:24:10.710 --> 00:24:13.830
on tutors and private education.

00:24:13.830 --> 00:24:16.420
They have their kids staying up
till 2 o'clock in the morning

00:24:16.420 --> 00:24:19.130
every single night, year
after year after year,

00:24:19.130 --> 00:24:22.360
to get just a small advantage
over everybody else.

00:24:22.360 --> 00:24:24.030
There is this arms race.

00:24:24.030 --> 00:24:28.520
If people could make
decisions before birth

00:24:28.520 --> 00:24:31.020
to give their kids
advantage, would they do it?

00:24:31.020 --> 00:24:33.000
Maybe some would,
and some wouldn't.

00:24:33.000 --> 00:24:35.050
Maybe they would do it
in some jurisdictions

00:24:35.050 --> 00:24:36.290
and not in others.

00:24:36.290 --> 00:24:40.450
But the history of our species
is a history of hubris.

00:24:40.450 --> 00:24:42.660
And whatever it is,
name one technology

00:24:42.660 --> 00:24:45.300
that had the potential
to do a lot of good,

00:24:45.300 --> 00:24:47.840
and there were big dangers
associated with it,

00:24:47.840 --> 00:24:49.330
that we haven't adopted.

00:24:49.330 --> 00:24:52.160
We have, and that's our history.

00:24:52.160 --> 00:24:56.030
But there are huge and very,
very challenging issues.

00:24:56.030 --> 00:24:58.380
And if we just say, well,
this is just technology,

00:24:58.380 --> 00:25:00.560
and technology will
find a way, we'll

00:25:00.560 --> 00:25:04.880
have a disaster on our hands
because as I think all of you

00:25:04.880 --> 00:25:07.240
are thinking, and all of
you watching on the screen

00:25:07.240 --> 00:25:11.240
are thinking, there are massive
issues of equity, of diversity.

00:25:11.240 --> 00:25:15.570
Our survival as a species has
been based on our diversity.

00:25:15.570 --> 00:25:17.481
And at any one time,
there are people

00:25:17.481 --> 00:25:19.480
who seemed like they
weren't really contributing

00:25:19.480 --> 00:25:21.940
much who, when
the world changed,

00:25:21.940 --> 00:25:25.410
those were the people who led
to our salvation as groups

00:25:25.410 --> 00:25:26.510
or as a species.

00:25:26.510 --> 00:25:30.390
And that's true
with any organism.

00:25:30.390 --> 00:25:32.610
And so if we were to
make selections and limit

00:25:32.610 --> 00:25:36.530
the diversity of our species, we
would do that at our own peril.

00:25:36.530 --> 00:25:39.090
And there are huge
values issues.

00:25:39.090 --> 00:25:41.770
Many of us come from
ancient traditions

00:25:41.770 --> 00:25:46.310
that have very meaningful
and valuable codes of justice

00:25:46.310 --> 00:25:49.390
and honor and good behavior.

00:25:49.390 --> 00:25:50.950
We can't just throw those away.

00:25:50.950 --> 00:25:55.850
Those are hard-won gains that
our ancestors have fought for

00:25:55.850 --> 00:25:59.690
to try to figure out how to
live in communities where people

00:25:59.690 --> 00:26:01.080
respect one another.

00:26:01.080 --> 00:26:03.680
And just because
there's new technology

00:26:03.680 --> 00:26:07.419
doesn't mean that
we shouldn't think

00:26:07.419 --> 00:26:09.210
of what's the best of
our tradition, what's

00:26:09.210 --> 00:26:11.540
the best of our
values, and deploy

00:26:11.540 --> 00:26:15.760
them to help us navigate
this new and very complicated

00:26:15.760 --> 00:26:16.910
terrain.

00:26:16.910 --> 00:26:21.240
So that finally brings me
to the issue of the novel

00:26:21.240 --> 00:26:23.900
and why I write novels.

00:26:23.900 --> 00:26:26.840
As I said at the outset,
these are very, very complex.

00:26:26.840 --> 00:26:29.520
They're very, very
difficult issues.

00:26:29.520 --> 00:26:31.980
And they're human issues.

00:26:31.980 --> 00:26:37.650
And we, as a species, don't
just respond to science.

00:26:37.650 --> 00:26:41.950
We don't just respond
to policy memos.

00:26:41.950 --> 00:26:44.520
Our history as a species is
that we respond to stories.

00:26:44.520 --> 00:26:45.660
It's how we've lived.

00:26:45.660 --> 00:26:48.650
We tell stories
around the campfire.

00:26:48.650 --> 00:26:51.790
It's how we struggle with
things, and we have parables.

00:26:51.790 --> 00:26:53.760
And for me, I
certainly have been

00:26:53.760 --> 00:26:56.790
working on these issues
for many, many years.

00:26:56.790 --> 00:27:00.100
And I've written a lot
of articles who people

00:27:00.100 --> 00:27:02.050
like the people I know read.

00:27:02.050 --> 00:27:05.250
But this is such a
fundamental issue

00:27:05.250 --> 00:27:08.880
that it's a species-wide
issue, and everybody

00:27:08.880 --> 00:27:11.910
needs to be part
of the conversation

00:27:11.910 --> 00:27:15.450
about our future evolution
and the implications,

00:27:15.450 --> 00:27:18.479
the big-picture implications
of today's technologies.

00:27:18.479 --> 00:27:20.020
And that's why I've
written the book.

00:27:20.020 --> 00:27:22.350
And then I'll just tell a
little bit about the book

00:27:22.350 --> 00:27:24.810
and then just read you
a very, very short bit

00:27:24.810 --> 00:27:26.530
and then would love
to get your thoughts

00:27:26.530 --> 00:27:28.900
and answer any questions.

00:27:28.900 --> 00:27:31.800
So the premise
behind the book is

00:27:31.800 --> 00:27:34.960
that octogenarian,
brilliant scientists, dying

00:27:34.960 --> 00:27:37.460
of terminal cancer,
are disappearing

00:27:37.460 --> 00:27:39.700
from hospices around the world.

00:27:39.700 --> 00:27:41.490
And nobody notices,
and nobody really

00:27:41.490 --> 00:27:45.590
cares because these people
are very old and near death.

00:27:45.590 --> 00:27:49.350
But one reporter from
"The Kansas City Star"

00:27:49.350 --> 00:27:51.870
begins investigating
one disappearance

00:27:51.870 --> 00:27:53.920
and starts piecing
together that there

00:27:53.920 --> 00:27:56.820
are these other disappearances
around the world.

00:27:56.820 --> 00:28:01.580
And that investigation leads him
to a very reclusive scientist

00:28:01.580 --> 00:28:04.090
whose wife had died of
cancer 10 years before.

00:28:04.090 --> 00:28:08.550
He's committed himself to trying
to find a cure for cancer.

00:28:08.550 --> 00:28:11.060
And he finds a way
to reverse age,

00:28:11.060 --> 00:28:15.170
to revert cancer cells to
their pre-cancerous state.

00:28:15.170 --> 00:28:17.800
And he's all excited about it,
and it works in animal models.

00:28:17.800 --> 00:28:21.820
But when he tries it in
humans, the complexity

00:28:21.820 --> 00:28:28.350
of the body's response rejects
this cellular transformation.

00:28:28.350 --> 00:28:30.750
So he experiments
and experiments,

00:28:30.750 --> 00:28:35.210
and he stumbles upon a way
of reverting all of the cells

00:28:35.210 --> 00:28:37.830
in the body to create
an environment where

00:28:37.830 --> 00:28:39.440
the cancer can be reverted.

00:28:39.440 --> 00:28:43.930
And, of course, what this means
is that he's conquered aging.

00:28:43.930 --> 00:28:46.720
And he realizes what he's
done and how dangerous

00:28:46.720 --> 00:28:49.380
the concept of immortality
has been for people

00:28:49.380 --> 00:28:52.050
for so many millennia,
and he realizes

00:28:52.050 --> 00:28:54.410
that he needs to
protect this knowledge.

00:28:54.410 --> 00:28:58.680
And so he compartmentalizes his
research to try to make sure

00:28:58.680 --> 00:29:01.700
it doesn't get into the wrong
hands, and then he's murdered.

00:29:01.700 --> 00:29:05.110
Not to spoil, but that
happens early in the novel.

00:29:05.110 --> 00:29:10.250
And then he's murdered, which
launches a global struggle

00:29:10.250 --> 00:29:14.800
to find and control
the technology enabling

00:29:14.800 --> 00:29:17.250
extreme human life
extension and immortality.

00:29:17.250 --> 00:29:20.960
So it's a thriller
because I just

00:29:20.960 --> 00:29:23.190
feel like what I
want people to do,

00:29:23.190 --> 00:29:24.970
I want people to
be able to explore

00:29:24.970 --> 00:29:28.730
these issues in a way that's fun
and interesting and hopefully

00:29:28.730 --> 00:29:29.730
exciting.

00:29:29.730 --> 00:29:30.940
And it's a love story.

00:29:30.940 --> 00:29:34.310
There's sex, but I can't say
I did a very good job at it.

00:29:34.310 --> 00:29:36.560
But you guys be the judge.

00:29:36.560 --> 00:29:41.510
And maybe I'll just read
you just a little bit,

00:29:41.510 --> 00:29:42.990
just to get a sense of things.

00:29:42.990 --> 00:29:46.390
So this is chapter 26.

00:29:46.390 --> 00:29:49.230
"Compared to the vast expanse
of the 4.5 billion years

00:29:49.230 --> 00:29:51.640
of this planet, or the
nearly 4 billion years

00:29:51.640 --> 00:29:54.180
it took life to morph from
a single-cell organism

00:29:54.180 --> 00:29:58.280
into the hundred trillion or so
cells I lug around today, each

00:29:58.280 --> 00:30:00.740
of our presences is but a blip.

00:30:00.740 --> 00:30:03.180
Blip actually is
too strong a word.

00:30:03.180 --> 00:30:05.350
We're a pimple on
the nose of a speck,

00:30:05.350 --> 00:30:07.710
joyriding on the blip's ass.

00:30:07.710 --> 00:30:10.040
In other words, we are small.

00:30:10.040 --> 00:30:12.700
But each of us is the hero of
our own little play, endowed

00:30:12.700 --> 00:30:14.150
with enough
narcissism to believe

00:30:14.150 --> 00:30:16.630
our lives, our
presence here on Earth

00:30:16.630 --> 00:30:19.460
during the estimated 5 billion
more years our blue marble has

00:30:19.460 --> 00:30:22.350
left to twirl, somehow counts.

00:30:22.350 --> 00:30:24.850
Faced with death two
years ago in a musty hotel

00:30:24.850 --> 00:30:27.190
room in Norman,
Oklahoma, I realized

00:30:27.190 --> 00:30:28.820
that as small as
our lives may be

00:30:28.820 --> 00:30:31.580
in the grand scheme of things,
loving another person is

00:30:31.580 --> 00:30:34.950
our tiny ripple in the universe,
our song into the vastness

00:30:34.950 --> 00:30:37.820
of space, announcing the
inexplicable artistry

00:30:37.820 --> 00:30:40.890
of our one true creation
greater than ourselves.

00:30:40.890 --> 00:30:42.390
But over the past
two years, I've

00:30:42.390 --> 00:30:44.990
gradually lost hold
of this insight.

00:30:44.990 --> 00:30:46.800
Perpetually overthinking
life, I seem

00:30:46.800 --> 00:30:50.200
to have reverted to my ongoing
dialectical battle between--

00:30:50.200 --> 00:30:51.760
and I'll be the
first to roll my eyes

00:30:51.760 --> 00:30:54.070
whenever Jean-Paul
Sartre is referenced--

00:30:54.070 --> 00:30:55.830
being and nothingness.

00:30:55.830 --> 00:30:57.330
In the last three
days, I've learned

00:30:57.330 --> 00:30:59.460
of two men's disappearances
into thin air, come

00:30:59.460 --> 00:31:02.530
across the body of a genius
I just met being grotesquely

00:31:02.530 --> 00:31:05.240
devoured by jellyfish--"
spoiler alert--

00:31:05.240 --> 00:31:08.400
"and essentially witnessed
the deaths of two police

00:31:08.400 --> 00:31:10.710
officers caught in
a massive explosion.

00:31:10.710 --> 00:31:13.660
No wonder my grip on reality,
whatever that strange concept

00:31:13.660 --> 00:31:15.756
actually means, is
feeling tenuous as I

00:31:15.756 --> 00:31:16.880
pull into Tony's driveway."

00:31:16.880 --> 00:31:18.336
Tony is his girlfriend.

00:31:18.336 --> 00:31:20.210
"She's running back and
forth in the kitchen,

00:31:20.210 --> 00:31:23.150
waving her arms, as I enter
the door from the garage.

00:31:23.150 --> 00:31:25.120
An easy smile blankets her face.

00:31:25.120 --> 00:31:26.160
Come on, Sebastian.

00:31:26.160 --> 00:31:28.850
Come on, she says, in
her silly baby voice.

00:31:28.850 --> 00:31:30.910
The dog leaps in
circles with delight.

00:31:30.910 --> 00:31:34.390
Look who it is, she says to
the dog in the same voice.

00:31:34.390 --> 00:31:35.840
It's Daddy.

00:31:35.840 --> 00:31:39.550
I turn, as if to see if
someone is standing behind me.

00:31:39.550 --> 00:31:41.870
Tony seems to forgive
my poor attempt at humor

00:31:41.870 --> 00:31:43.120
and crawls toward me.

00:31:43.120 --> 00:31:45.190
She swats my left
shoe back and forth,

00:31:45.190 --> 00:31:47.080
which the dog
recognizes as placing

00:31:47.080 --> 00:31:48.900
my foot on the edible list.

00:31:48.900 --> 00:31:50.890
Sebastian darts
in with what looks

00:31:50.890 --> 00:31:52.550
like an irrepressible
smile, and I'm

00:31:52.550 --> 00:31:54.890
reminded, as if
I need reminding,

00:31:54.890 --> 00:31:59.600
which I probably sometimes
do, just how special Tony is."

00:31:59.600 --> 00:32:01.770
Anyway, so it's written.

00:32:01.770 --> 00:32:06.750
My hope is-- I was in
Washington for a lot of years

00:32:06.750 --> 00:32:08.350
on the National
Security Council.

00:32:08.350 --> 00:32:11.020
And a lot of my
friends wrote novels

00:32:11.020 --> 00:32:14.810
that were like policy memos
with a murderer and a sex scene.

00:32:14.810 --> 00:32:18.960
And so I'm hoping that this
is a bit better than that.

00:32:18.960 --> 00:32:21.150
But really, it's such
a tremendous honor

00:32:21.150 --> 00:32:24.740
for me to meet with
any number, even one,

00:32:24.740 --> 00:32:28.250
of people who are reading my
book because, as an author,

00:32:28.250 --> 00:32:30.310
it's such a personal experience.

00:32:30.310 --> 00:32:33.260
You have all of your kind
of thoughts and your life

00:32:33.260 --> 00:32:35.300
and eventually kind
of think up a story.

00:32:35.300 --> 00:32:37.920
Then you kind of
gather your soul.

00:32:37.920 --> 00:32:40.380
And you kind of condense
it and condense it.

00:32:40.380 --> 00:32:44.200
And then all of a sudden, it
becomes this physical object,

00:32:44.200 --> 00:32:47.200
and then it has a
life in the world that

00:32:47.200 --> 00:32:48.200
is separate from you.

00:32:48.200 --> 00:32:50.490
And people consume
it as a product.

00:32:50.490 --> 00:32:52.810
And your little piece
of your soul hopefully,

00:32:52.810 --> 00:32:55.050
ideally, enters them.

00:32:55.050 --> 00:32:58.120
And so what I hope is that
if you'll read the book,

00:32:58.120 --> 00:32:59.880
I hope you will enjoy the book.

00:32:59.880 --> 00:33:03.400
And if it just inspires you to
think a little bit differently,

00:33:03.400 --> 00:33:05.540
or a little bit more,
about any of these issues,

00:33:05.540 --> 00:33:07.684
or any other issues, then
for me that's success.

00:33:07.684 --> 00:33:09.850
So it's a real pleasure and
honor for me to be here,

00:33:09.850 --> 00:33:11.766
and I'm happy to answer
any of your questions.

00:33:11.766 --> 00:33:14.122
[APPLAUSE]

00:33:16.930 --> 00:33:19.130
AUDIENCE: Can you talk
a bit about yourself,

00:33:19.130 --> 00:33:21.077
your experience?

00:33:21.077 --> 00:33:23.160
It seems that you have
traveled to many countries.

00:33:23.160 --> 00:33:25.120
JAMIE METZL: Yes [INAUDIBLE].

00:33:25.120 --> 00:33:26.820
AUDIENCE: Can you
talk about that?

00:33:26.820 --> 00:33:30.400
JAMIE METZL: Yeah, sure, so
just a little bit about me,

00:33:30.400 --> 00:33:33.540
this is the condensed
version of my life.

00:33:33.540 --> 00:33:35.870
But I'm originally
from Kansas City.

00:33:35.870 --> 00:33:37.460
But in my freshman
year at Brown--

00:33:37.460 --> 00:33:39.710
this is connected to traveling
to a lot of countries--

00:33:39.710 --> 00:33:41.084
I met a classmate
of mine who was

00:33:41.084 --> 00:33:43.290
a survivor of the
Cambodian genocide.

00:33:43.290 --> 00:33:47.230
And I was so blown away
by his life experience

00:33:47.230 --> 00:33:50.727
that that summer I quit
my job on the first day.

00:33:50.727 --> 00:33:53.060
I had a garage sale of all
the junk in my parents' house

00:33:53.060 --> 00:33:55.830
and bought a ticket and went
to the Thai-Cambodian border

00:33:55.830 --> 00:33:59.130
and got very, very involved
working in a refugee camp

00:33:59.130 --> 00:34:03.250
and then came back and was
very deeply involved in Asia

00:34:03.250 --> 00:34:05.850
and international relations.

00:34:05.850 --> 00:34:09.792
And after my PhD in law school,
then I was in US government,

00:34:09.792 --> 00:34:11.750
on the National Security
Council, and the State

00:34:11.750 --> 00:34:14.530
Department, and Senate
Foreign Relations Committee.

00:34:14.530 --> 00:34:18.020
And my idea was that those kinds
of problems, and the problems

00:34:18.020 --> 00:34:20.650
that we see now in
Syria and other places,

00:34:20.650 --> 00:34:22.900
that the refugee is
the end of the tale.

00:34:22.900 --> 00:34:25.310
The beginning of the tail,
or the head of the animal,

00:34:25.310 --> 00:34:27.949
is bad decisions by leaders.

00:34:27.949 --> 00:34:32.739
And so certainly I have a
long history in government.

00:34:32.739 --> 00:34:35.440
It's a big-- I don't
know if commitment

00:34:35.440 --> 00:34:38.070
is the right word--
but just a belief

00:34:38.070 --> 00:34:41.370
that we all have an
important role to play,

00:34:41.370 --> 00:34:44.190
both in helping our leaders
make good decisions, but also--

00:34:44.190 --> 00:34:46.550
and this comes back
to my novel writing--

00:34:46.550 --> 00:34:49.980
in educating ourselves and
educating each other on what

00:34:49.980 --> 00:34:53.199
the big issues are so that
we can be informed and make

00:34:53.199 --> 00:34:54.340
smart decisions.

00:34:54.340 --> 00:34:57.890
And we can inspire our
leaders, knock wood,

00:34:57.890 --> 00:35:00.510
to make smart decisions as well.

00:35:00.510 --> 00:35:05.350
AUDIENCE: So how much embryo
selection, or even gene

00:35:05.350 --> 00:35:07.890
editing before the
embryo selection,

00:35:07.890 --> 00:35:10.664
would it take to be evolution?

00:35:10.664 --> 00:35:11.455
You used that term.

00:35:11.455 --> 00:35:13.537
JAMIE METZL: Yeah,
that's a great question.

00:35:13.537 --> 00:35:14.870
I don't know the answer to that.

00:35:14.870 --> 00:35:17.200
It's a great question.

00:35:17.200 --> 00:35:20.050
And I think that we'll
know it when we just

00:35:20.050 --> 00:35:22.220
start seeing the differences.

00:35:22.220 --> 00:35:24.100
I think that we're
always evolving.

00:35:24.100 --> 00:35:27.290
We're a constantly
evolving species.

00:35:27.290 --> 00:35:29.380
But we're going to
see people jumping out

00:35:29.380 --> 00:35:32.210
of the range of what
we now consider normal.

00:35:32.210 --> 00:35:36.490
And whether it's living
longer right-- this is a quiz.

00:35:36.490 --> 00:35:39.190
How old was the oldest
person who has ever lived?

00:35:42.780 --> 00:35:44.990
Yes, 122, and she was a smoker.

00:35:44.990 --> 00:35:46.940
She smoked her whole life.

00:35:46.940 --> 00:35:50.400
And to live such a long
time, actually lifestyle

00:35:50.400 --> 00:35:51.350
isn't that important.

00:35:51.350 --> 00:35:54.450
I mean, to live to 95,
lifestyle is really important.

00:35:54.450 --> 00:35:58.472
But to get to 110, you just
have to thank your parents.

00:35:58.472 --> 00:36:00.180
But we're going to
see people jumping out

00:36:00.180 --> 00:36:01.730
of the range of normal.

00:36:01.730 --> 00:36:04.200
And I think that is going to
be one easy indication, when

00:36:04.200 --> 00:36:08.970
we have more and more people
who have beyond genius IQs

00:36:08.970 --> 00:36:13.510
who are living longer, when
we're eliminating diseases.

00:36:13.510 --> 00:36:15.330
And, again, not
every change is good.

00:36:15.330 --> 00:36:19.180
I talked about this cure
for sickle cell disease,

00:36:19.180 --> 00:36:21.440
this prospective cure
for sickle cell disease.

00:36:21.440 --> 00:36:24.300
The reason sickle cell
disease has survived

00:36:24.300 --> 00:36:25.950
is that it's a
bad thing to have,

00:36:25.950 --> 00:36:29.990
but it also gives many
people immunity to malaria.

00:36:29.990 --> 00:36:33.110
And so there are a lot of
genes that do multiple things.

00:36:33.110 --> 00:36:35.480
It's not that every
gene does one thing.

00:36:35.480 --> 00:36:37.740
And there's a lot of complexity.

00:36:37.740 --> 00:36:39.886
So I don't have a
clear way of answering,

00:36:39.886 --> 00:36:41.510
but we're going to
have a lot of people

00:36:41.510 --> 00:36:43.130
in the not-distant
future jumping out

00:36:43.130 --> 00:36:44.088
of the range of normal.

00:36:44.088 --> 00:36:46.900
And that's going to be
relatively easy to identify.

00:36:46.900 --> 00:36:48.900
AUDIENCE: To what extent
have you thought about?

00:36:48.900 --> 00:36:50.620
And to what extent do
you go into, in the book,

00:36:50.620 --> 00:36:52.250
some of the economic
implications?

00:36:52.250 --> 00:36:55.700
I see both short term, where
better differentiable embryos

00:36:55.700 --> 00:36:58.350
eventually become better
differentiable economic assets,

00:36:58.350 --> 00:37:01.310
and long term, where things
like a 200-year-old person,

00:37:01.310 --> 00:37:03.125
compounded interest
in their accounts

00:37:03.125 --> 00:37:05.250
economically differentiates
them substantially more

00:37:05.250 --> 00:37:07.260
than someone maybe
entering the workforce.

00:37:07.260 --> 00:37:09.190
So how much have you
thought about that?

00:37:09.190 --> 00:37:09.800
What are your thoughts?

00:37:09.800 --> 00:37:10.966
How much do you [INAUDIBLE]?

00:37:10.966 --> 00:37:13.420
JAMIE METZL: Yeah, I've
thought a lot about it.

00:37:13.420 --> 00:37:16.230
The prequel to "Eternal
Sonata" is "Genesis Code,"

00:37:16.230 --> 00:37:19.890
where I focus more on
your first question.

00:37:19.890 --> 00:37:22.110
So there will be
huge implications,

00:37:22.110 --> 00:37:25.870
both for competition
between societies.

00:37:25.870 --> 00:37:29.860
There will be big
economic implications.

00:37:29.860 --> 00:37:34.600
So on the economic sense, we're
going to eliminate diseases,

00:37:34.600 --> 00:37:36.647
and that's going to help.

00:37:36.647 --> 00:37:38.480
But we're going to be
a lot more competitive

00:37:38.480 --> 00:37:42.470
between the enhancements
of the human brain, which

00:37:42.470 --> 00:37:46.330
I think are coming, and
either the work that you all

00:37:46.330 --> 00:37:50.740
are doing, the
brain-computer interface.

00:37:50.740 --> 00:37:53.460
We're going to be inventing
things that we can't even

00:37:53.460 --> 00:37:56.980
imagine, and our inventions are
going to be inventing things.

00:37:56.980 --> 00:38:00.370
So I've definitely
thought a lot about that.

00:38:00.370 --> 00:38:04.400
In terms of the implications
of people living longer,

00:38:04.400 --> 00:38:06.340
we have all of
these systems that

00:38:06.340 --> 00:38:09.460
are based on one expectation
of how long people live.

00:38:09.460 --> 00:38:12.440
And already social
security is a good example.

00:38:12.440 --> 00:38:16.280
We're seeing the strains when
our early assumptions are

00:38:16.280 --> 00:38:18.240
no longer correct,
and we're going

00:38:18.240 --> 00:38:19.840
to have more and more of that.

00:38:19.840 --> 00:38:22.120
And that's actually a
good thing to do now.

00:38:22.120 --> 00:38:24.720
If you have a lot
of faith, as I do,

00:38:24.720 --> 00:38:29.962
that we are going to be living
much longer than was the model,

00:38:29.962 --> 00:38:31.920
there are, like, some
pretty smart investments.

00:38:31.920 --> 00:38:34.510
Definitely, everybody should
be putting into your retirement

00:38:34.510 --> 00:38:36.730
account now.

00:38:36.730 --> 00:38:39.540
But all of these things that
are axiomatic-- and that's

00:38:39.540 --> 00:38:40.670
what I started with.

00:38:40.670 --> 00:38:44.170
It's kind of axiomatic
to us that the human body

00:38:44.170 --> 00:38:45.490
isn't that hackable.

00:38:45.490 --> 00:38:47.830
But it turns out it is hackable.

00:38:47.830 --> 00:38:53.250
And it's axiomatic that you live
within a certain range of life,

00:38:53.250 --> 00:38:56.150
but it was also, in earlier
stages of not just evolution,

00:38:56.150 --> 00:38:58.900
of human existence, not too
long ago the expectation was

00:38:58.900 --> 00:39:00.160
much shorter than it is now.

00:39:00.160 --> 00:39:01.876
So we were going
to grow and expand.

00:39:01.876 --> 00:39:03.500
If you want to make
a smart investment,

00:39:03.500 --> 00:39:05.860
just assume, which I think
is a fair investment,

00:39:05.860 --> 00:39:08.320
that everyone in
this room is going

00:39:08.320 --> 00:39:11.960
to live longer than the
actuarial tables would suggest.

00:39:11.960 --> 00:39:13.650
And then if you're
thinking that,

00:39:13.650 --> 00:39:15.740
what is the investment
that you can make?

00:39:15.740 --> 00:39:18.250
And, of course, the answer
is buy more Google stock.

00:39:18.250 --> 00:39:20.500
AUDIENCE: I have a question
about policy and morality,

00:39:20.500 --> 00:39:22.320
since obviously you
worked in DC for a while.

00:39:22.320 --> 00:39:23.510
Where does policy
come into this,

00:39:23.510 --> 00:39:26.051
because I know a lot of times
with technology, policy kind of

00:39:26.051 --> 00:39:27.180
comes after the fact?

00:39:27.180 --> 00:39:28.810
Uber is an example.

00:39:28.810 --> 00:39:31.101
And when you're talking about
picking genes and picking

00:39:31.101 --> 00:39:33.750
genders, like, we need to have
an equal balance of genders.

00:39:33.750 --> 00:39:37.287
Obviously in many countries,
they would favor male embryos.

00:39:37.287 --> 00:39:39.370
JAMIE METZL: That's, like,
the dumbest thing ever.

00:39:39.370 --> 00:39:41.870
My thinking is if you want to
have national competitiveness,

00:39:41.870 --> 00:39:42.560
pick women.

00:39:42.560 --> 00:39:45.410
You're much better off.

00:39:45.410 --> 00:39:47.229
But, yeah, it's
a great question.

00:39:47.229 --> 00:39:49.270
And this is the problem
with a lot of technology.

00:39:49.270 --> 00:39:53.750
The technology is
advancing exponentially.

00:39:53.750 --> 00:39:55.650
But our mindset, as
I mentioned before,

00:39:55.650 --> 00:39:58.690
is only increasing linearly.

00:39:58.690 --> 00:40:00.780
And the regulatory
environment is only

00:40:00.780 --> 00:40:02.380
inching forward glacially.

00:40:02.380 --> 00:40:06.180
And that's why there's
such a dangerous mismatch.

00:40:06.180 --> 00:40:09.590
And we've seen what happens
when there's a mismatch.

00:40:09.590 --> 00:40:13.120
The GMO debate is
a case in point.

00:40:13.120 --> 00:40:18.060
30 years of research has
proven inconclusively

00:40:18.060 --> 00:40:21.390
that there is nothing more
dangerous about GMO crops

00:40:21.390 --> 00:40:23.690
than regular crops.

00:40:23.690 --> 00:40:28.030
And yet, because the information
battle has been lost,

00:40:28.030 --> 00:40:31.830
people just have a feeling
that GM crops are unnatural.

00:40:31.830 --> 00:40:35.090
And there may be other
legit criticisms,

00:40:35.090 --> 00:40:38.300
like monoculture and
corporate control.

00:40:38.300 --> 00:40:41.870
But in terms of safety,
really, GM crops, there's

00:40:41.870 --> 00:40:44.190
nothing at least that's
been proven thus far that's

00:40:44.190 --> 00:40:44.856
wrong with them.

00:40:44.856 --> 00:40:47.770
But once it gets stuck
in people's minds

00:40:47.770 --> 00:40:51.186
that there's something bad, it's
very, very hard to move them.

00:40:51.186 --> 00:40:53.060
And that's the moment
that we're in right now

00:40:53.060 --> 00:40:54.830
with all of these technologies.

00:40:54.830 --> 00:40:57.630
And that's why I'm
pushing for and trying

00:40:57.630 --> 00:40:59.980
to create a species-wide
dialogue on the future

00:40:59.980 --> 00:41:01.830
of human genetic
engineering, because now

00:41:01.830 --> 00:41:03.750
people's minds haven't formed.

00:41:03.750 --> 00:41:05.780
And there is an
opportunity to bring people

00:41:05.780 --> 00:41:09.210
into an inclusive conversation,
an inclusive global dialogue

00:41:09.210 --> 00:41:11.910
that includes people
of faith and people

00:41:11.910 --> 00:41:13.690
from the civil
society groups, people

00:41:13.690 --> 00:41:16.600
from the corporate world,
from anywhere and everywhere.

00:41:16.600 --> 00:41:20.530
But relatively soon, those
things are going to form.

00:41:20.530 --> 00:41:23.910
And there's a danger
of under-regulation

00:41:23.910 --> 00:41:25.890
because what we're
talking about is life.

00:41:25.890 --> 00:41:28.730
And life, there must
be limits on how

00:41:28.730 --> 00:41:30.310
people can manipulate life.

00:41:30.310 --> 00:41:31.840
It's just unacceptable.

00:41:31.840 --> 00:41:32.630
And there are some
people who are

00:41:32.630 --> 00:41:34.370
in the transhumanist
community-- and there they

00:41:34.370 --> 00:41:36.910
live around here-- who say
there should just be no limits.

00:41:36.910 --> 00:41:39.720
Let science do what it will.

00:41:39.720 --> 00:41:42.940
But I don't think that's a
morally defensible position.

00:41:42.940 --> 00:41:45.080
Having said that,
there needs to be

00:41:45.080 --> 00:41:48.150
a high level of
permissiveness because,

00:41:48.150 --> 00:41:52.450
one, if the technology can help
cure disease and enhance life,

00:41:52.450 --> 00:41:58.440
it will find a way, whether in
one jurisdiction or another.

00:41:58.440 --> 00:42:02.170
And then we need to create
an environment where

00:42:02.170 --> 00:42:07.190
this technology can-- thanks,
guys-- where the technology can

00:42:07.190 --> 00:42:08.710
realize its potential.

00:42:08.710 --> 00:42:10.050
But there's a big mismatch.

00:42:10.050 --> 00:42:13.630
And that's why we need
to educate ourselves,

00:42:13.630 --> 00:42:15.600
and we need to
educate our leaders.

00:42:15.600 --> 00:42:17.990
And we need to make
sure that we're neither

00:42:17.990 --> 00:42:20.410
under-regulated,
nor over-regulated.

00:42:20.410 --> 00:42:22.230
And that's a very tall order.

00:42:22.230 --> 00:42:25.230
AUDIENCE: You talk about, like,
how good it is to live longer,

00:42:25.230 --> 00:42:25.900
right?

00:42:25.900 --> 00:42:29.050
But there should be then, like,
a bad side of that, right?

00:42:29.050 --> 00:42:32.350
Like if you live longer
than it's supposed to be,

00:42:32.350 --> 00:42:36.435
then 100 years, maybe
you will live 150 years.

00:42:36.435 --> 00:42:38.665
What will happen
with the work, right?

00:42:38.665 --> 00:42:40.180
There will be so
many people already

00:42:40.180 --> 00:42:42.690
there, all their resources.

00:42:42.690 --> 00:42:44.510
And some of them
maybe don't desire

00:42:44.510 --> 00:42:46.280
to live that much longer.

00:42:46.280 --> 00:42:47.610
So what about that.

00:42:47.610 --> 00:42:51.050
Your genes are modified
to live longer.

00:42:51.050 --> 00:42:54.720
What if that person doesn't
want to live longer?

00:42:54.720 --> 00:42:57.360
JAMIE METZL: So, yeah, there
are two great questions.

00:42:57.360 --> 00:42:59.200
I'll start with the second one.

00:42:59.200 --> 00:43:01.360
If you are an
existentialist-- and I

00:43:01.360 --> 00:43:04.730
mention Sartre--
then living every day

00:43:04.730 --> 00:43:06.090
is an affirmative choice.

00:43:06.090 --> 00:43:08.200
And so if people don't
want to live longer,

00:43:08.200 --> 00:43:09.210
I don't encourage this.

00:43:09.210 --> 00:43:10.630
And life is wonderful.

00:43:10.630 --> 00:43:12.640
Everyone should
live to be 1,000,

00:43:12.640 --> 00:43:17.880
but people can make whatever
personal choice maybe.

00:43:17.880 --> 00:43:19.600
But don't do it.

00:43:19.600 --> 00:43:20.690
Life is great.

00:43:20.690 --> 00:43:24.640
If you're thinking about
that, change your perspective.

00:43:24.640 --> 00:43:27.430
But on the thing of
overpopulation, that's an issue

00:43:27.430 --> 00:43:29.180
I get asked every time.

00:43:29.180 --> 00:43:31.260
And I'm not at all
concerned about it,

00:43:31.260 --> 00:43:34.850
just because when you look
at population trends now,

00:43:34.850 --> 00:43:38.860
we're moving towards 9
billion people in about 2050.

00:43:38.860 --> 00:43:41.300
But then, forgetting
all of this,

00:43:41.300 --> 00:43:43.530
then the global
population is going

00:43:43.530 --> 00:43:46.300
to start going down
pretty significantly.

00:43:46.300 --> 00:43:49.330
And the biggest reason
for that is education.

00:43:49.330 --> 00:43:52.500
Right now, we have big
pockets of the world

00:43:52.500 --> 00:43:57.115
of people who are not as
well educated as elsewhere.

00:43:57.115 --> 00:43:57.990
But that will change.

00:43:57.990 --> 00:43:59.781
That's going to change
over coming decades.

00:44:00.290 --> 00:44:04.250
Every society that
becomes highly educated

00:44:04.250 --> 00:44:06.000
stops having kids.

00:44:06.000 --> 00:44:10.690
And so their replacement rates
most everywhere-- in Korea,

00:44:10.690 --> 00:44:15.620
in Japan, in Singapore,
in most of Europe--

00:44:15.620 --> 00:44:20.470
it's below two
children per mother.

00:44:20.470 --> 00:44:22.429
And the people, at
least in the beginning,

00:44:22.429 --> 00:44:24.720
who are going to be having
access to these technologies

00:44:24.720 --> 00:44:27.460
will be people in that
same class of people

00:44:27.460 --> 00:44:30.250
who have access to modernity.

00:44:30.250 --> 00:44:32.160
So I'm not concerned.

00:44:32.160 --> 00:44:35.200
And I also feel that
we, as a species,

00:44:35.200 --> 00:44:38.990
have the resources now easily
to feed everybody on Earth,

00:44:38.990 --> 00:44:41.530
to clothe everybody on Earth,
to house everybody on Earth.

00:44:41.530 --> 00:44:44.450
We've made a moral decision
that we don't want to do it.

00:44:44.450 --> 00:44:46.520
We've made a moral
decision that we

00:44:46.520 --> 00:44:48.680
are comfortable with
concentrating resources

00:44:48.680 --> 00:44:52.350
in some places and letting
people starve to death

00:44:52.350 --> 00:44:56.400
or live in extremely unsanitary
conditions elsewhere.

00:44:56.400 --> 00:44:58.605
But today, what's
the difference?

00:44:58.605 --> 00:45:00.980
We've seen the life experience
of everybody in this room,

00:45:00.980 --> 00:45:03.320
and the average person in
the Central African Republic

00:45:03.320 --> 00:45:05.400
in the middle of a
horrendous civil war,

00:45:05.400 --> 00:45:08.410
or the people in Haiti
whose lives have been

00:45:08.410 --> 00:45:11.630
wrecked 10 times over
over the last decade.

00:45:11.630 --> 00:45:13.560
And so I definitely
think that there

00:45:13.560 --> 00:45:18.956
is a moral imperative for us
to think about equity issues

00:45:18.956 --> 00:45:19.455
today.

00:45:23.390 --> 00:45:25.500
But I don't think
that as long as we

00:45:25.500 --> 00:45:27.990
have good values
that that's going

00:45:27.990 --> 00:45:32.134
to put a significant
strain on the planet.

00:45:32.134 --> 00:45:33.300
But I could be proven wrong.

00:45:33.300 --> 00:45:36.110
I mean, if everybody has it,
and people have more kids,

00:45:36.110 --> 00:45:38.107
I mean, things
change all the time.

00:45:38.107 --> 00:45:39.690
AUDIENCE: Could you
speak as an author

00:45:39.690 --> 00:45:42.700
about the process of going
from a very society-level,

00:45:42.700 --> 00:45:46.530
local-level vision of the
future to a more isolated story

00:45:46.530 --> 00:45:48.140
with a specific plot?

00:45:48.140 --> 00:45:51.370
JAMIE METZL: So it's
a great question.

00:45:51.370 --> 00:45:56.400
And it's personal, so I don't
know how other people do it.

00:45:56.400 --> 00:45:58.651
But for me, there
will be a big idea,

00:45:58.651 --> 00:45:59.900
and I'll be thinking about it.

00:45:59.900 --> 00:46:02.329
I'll often write articles
about a big idea.

00:46:02.329 --> 00:46:04.620
And then I'll start thinking,
well, what would it mean?

00:46:04.620 --> 00:46:07.190
Like for my last
book, "Genesis Code,"

00:46:07.190 --> 00:46:11.010
I testified before Congress
on the national security

00:46:11.010 --> 00:46:13.189
implications of the
genetics revolution.

00:46:13.189 --> 00:46:15.480
And then I started writing
a book that I didn't finish,

00:46:15.480 --> 00:46:18.110
but it was a nonfiction
book on exactly that topic.

00:46:18.110 --> 00:46:19.940
And I was writing the preface.

00:46:19.940 --> 00:46:22.870
And in the preface, I talked
about a national security

00:46:22.870 --> 00:46:23.740
staffer.

00:46:23.740 --> 00:46:25.770
I imagined a national
security staffer briefing

00:46:25.770 --> 00:46:27.770
the president,
saying, Mr. President,

00:46:27.770 --> 00:46:30.600
we've learned that China has
a secret genetic enhancement

00:46:30.600 --> 00:46:32.900
program where they are
breeding super babies

00:46:32.900 --> 00:46:35.197
and placing them
in the equivalent

00:46:35.197 --> 00:46:37.530
of their Olympic sport schools,
but for science and math

00:46:37.530 --> 00:46:39.120
and engineering and business.

00:46:39.120 --> 00:46:40.870
And the president says,
what does it mean?

00:46:40.870 --> 00:46:42.370
And the staffer
says, well, it means

00:46:42.370 --> 00:46:46.450
that in 30 years, if they
do it and we don't, we won't

00:46:46.450 --> 00:46:48.160
be able to compete with them.

00:46:48.160 --> 00:46:49.160
And so what our options?

00:46:49.160 --> 00:46:53.170
Well, we've decided there's
nothing we can do to stop them.

00:46:53.170 --> 00:46:56.770
But we can't do it
ourselves because we

00:46:56.770 --> 00:47:01.260
have all of these legal and
political and cultural issues.

00:47:01.260 --> 00:47:02.480
And what can we do?

00:47:02.480 --> 00:47:04.500
Well, we have this
idea that if we

00:47:04.500 --> 00:47:06.810
purchase a small chain
of fertility clinics

00:47:06.810 --> 00:47:09.370
and begin seeding the population
with genetically enhanced

00:47:09.370 --> 00:47:11.207
babies, nobody's going to know.

00:47:11.207 --> 00:47:12.790
And parents are going
to be so excited

00:47:12.790 --> 00:47:16.290
that their kid is a genius
or a sports star or whatever.

00:47:16.290 --> 00:47:18.320
So that was what I started with.

00:47:18.320 --> 00:47:19.617
And then I threw away the book.

00:47:19.617 --> 00:47:21.700
And I thought, well, this
is what I want to write.

00:47:21.700 --> 00:47:23.820
And then eventually,
well, how would it happen?

00:47:23.820 --> 00:47:26.860
If a genetically modified
person showed up,

00:47:26.860 --> 00:47:29.080
how would we experience that?

00:47:29.080 --> 00:47:32.450
And it's like little bits.

00:47:32.450 --> 00:47:33.859
It's like building with LEGOs.

00:47:33.859 --> 00:47:36.400
And then you kind of take these
little pieces from your life.

00:47:36.400 --> 00:47:38.590
And it was same with
"Eternal Sonata."

00:47:38.590 --> 00:47:41.009
I was thinking a lot
about life extension.

00:47:41.009 --> 00:47:42.300
Well, what is it going to mean?

00:47:42.300 --> 00:47:43.970
Because all of
these technologies,

00:47:43.970 --> 00:47:46.130
they build and they
build and they build.

00:47:46.130 --> 00:47:49.480
And then for people, like, one
day, you have an interaction.

00:47:49.480 --> 00:47:52.950
Like, one day, I
guarantee you your company

00:47:52.950 --> 00:47:55.683
is going to create
human experience

00:47:55.683 --> 00:47:56.683
artificial intelligence.

00:47:59.134 --> 00:48:00.800
Everyone in this room,
in your lifetime,

00:48:00.800 --> 00:48:06.710
you're going to have one day
where whatever the vehicle is,

00:48:06.710 --> 00:48:10.240
whether it's a robot or
your car or something,

00:48:10.240 --> 00:48:12.110
you're going to be
having a conversation

00:48:12.110 --> 00:48:15.020
with artificial intelligence
in your lifetime.

00:48:15.020 --> 00:48:17.370
And you're going to
think, like, oh, my God.

00:48:17.370 --> 00:48:20.940
I forgot it was
artificial intelligence.

00:48:20.940 --> 00:48:25.350
I've been talking about
philosophy on the 101 for 20

00:48:25.350 --> 00:48:29.240
minutes, and I forgot it
was artificial intelligence.

00:48:29.240 --> 00:48:31.280
You're all going to
have that experience.

00:48:31.280 --> 00:48:34.200
And think, well, when you
have that experience, what's

00:48:34.200 --> 00:48:36.000
it going to mean to
you as a human being?

00:48:36.000 --> 00:48:38.375
How are you going to think
about your future differently?

00:48:38.375 --> 00:48:40.340
So for me, I think a lot
about these questions.

00:48:40.340 --> 00:48:44.330
And then I think, you know, how
can you turn it into stories

00:48:44.330 --> 00:48:47.680
and turn it into characters and
have the characters, as I read

00:48:47.680 --> 00:48:49.900
you that piece,
not have characters

00:48:49.900 --> 00:48:52.360
being kind of wooden
representations,

00:48:52.360 --> 00:48:54.042
but real characters with lives.

00:48:54.042 --> 00:48:55.500
And that's one of
the things that I

00:48:55.500 --> 00:48:58.390
like about fiction is you
kind of create the characters,

00:48:58.390 --> 00:49:01.050
and then the characters
develop a life of their own.

00:49:01.050 --> 00:49:03.950
And they do things
that surprise you.

00:49:03.950 --> 00:49:06.350
And I guess it's you doing
things that surprise you

00:49:06.350 --> 00:49:08.510
because they pick up life.

00:49:08.510 --> 00:49:12.171
And it's like a magnet,
picking up little shavings.

00:49:12.171 --> 00:49:13.670
AUDIENCE: What other
works would you

00:49:13.670 --> 00:49:15.046
say influenced
your current work?

00:49:15.046 --> 00:49:16.295
JAMIE METZL: Oh, that's great.

00:49:16.295 --> 00:49:16.880
Thank you.

00:49:16.880 --> 00:49:18.390
AUDIENCE: What
inspired you, yeah?

00:49:18.390 --> 00:49:19.390
JAMIE METZL: Yeah, yeah,
yeah, you know, it's funny.

00:49:19.390 --> 00:49:21.690
As a science fiction
writer-- I mean,

00:49:21.690 --> 00:49:24.910
it's near-term science
fiction-- I don't primarily

00:49:24.910 --> 00:49:26.180
read science fiction.

00:49:26.180 --> 00:49:30.710
I like it when I read it,
but I love literature.

00:49:30.710 --> 00:49:34.360
And I certainly have a
lot of writers who I like.

00:49:34.360 --> 00:49:35.805
One writer who I
really love, who

00:49:35.805 --> 00:49:38.967
I reference a lot in the
book, is Jorge Luis Borges.

00:49:41.620 --> 00:49:43.006
I highly recommend it.

00:49:43.006 --> 00:49:45.380
It's a short book, although
you could read it 1,000 times

00:49:45.380 --> 00:49:48.150
and learn something new every
time, called "Ficciones,"

00:49:48.150 --> 00:49:55.170
F-I-C-C-I-O-N-E-S. So
Borges is a big inspiration.

00:49:55.170 --> 00:49:59.310
Yasunari Kawabata, who is a
Japanese Nobel Laureate, what

00:49:59.310 --> 00:50:04.970
I really love about him--
it's like I'm 1,000 years away

00:50:04.970 --> 00:50:08.300
from him-- but just, like,
the purity of the words,

00:50:08.300 --> 00:50:13.730
the purity of the language.

00:50:13.730 --> 00:50:17.970
So I have a lot of different
writers who I really like,

00:50:17.970 --> 00:50:21.550
but what I try to do is draw
from a lot of different places

00:50:21.550 --> 00:50:28.930
and then try to infuse it
with some of my own voice.

00:50:28.930 --> 00:50:29.710
Yeah?

00:50:29.710 --> 00:50:32.470
AUDIENCE: If any of your
works were to be optioned

00:50:32.470 --> 00:50:36.830
and to be something
animated or live action,

00:50:36.830 --> 00:50:39.249
who would you want
to team up with?

00:50:39.249 --> 00:50:41.040
JAMIE METZL: Well,
that's a great question.

00:50:41.040 --> 00:50:44.190
So I'm going to answer
that and another question

00:50:44.190 --> 00:50:48.580
because, like, it's a
little bit of a fantasy.

00:50:48.580 --> 00:50:50.330
With all my novels,
people say, well, this

00:50:50.330 --> 00:50:52.080
should definitely--
because when you write

00:50:52.080 --> 00:50:54.580
a book, everybody you know
says, this should definitely

00:50:54.580 --> 00:50:55.420
be a movie.

00:50:55.420 --> 00:50:57.030
And you should be on Oprah.

00:50:57.030 --> 00:51:00.200
And there's 200,000 books that
come out every year in English,

00:51:00.200 --> 00:51:01.960
and every person's
friends tell them that.

00:51:01.960 --> 00:51:04.480
And then they say, like,
why weren't you on Oprah?

00:51:04.480 --> 00:51:06.230
Like, what's wrong with you?

00:51:06.230 --> 00:51:07.994
So anyway, having
said all of that,

00:51:07.994 --> 00:51:10.160
I definitely would love for
this to be made a movie.

00:51:10.160 --> 00:51:13.350
And I've cast all the
roles, and Adrien Brody

00:51:13.350 --> 00:51:15.670
plays the main character.

00:51:15.670 --> 00:51:18.000
And so for who to make it?

00:51:18.000 --> 00:51:19.890
I wouldn't imagine
it as an animation,

00:51:19.890 --> 00:51:22.390
just because the whole reason
I write near-term fiction is I

00:51:22.390 --> 00:51:24.900
want this to feel to people
like it's the world today.

00:51:24.900 --> 00:51:27.350
I don't want people to
feel it's space opera.

00:51:27.350 --> 00:51:29.092
I don't need any Wookies.

00:51:29.092 --> 00:51:30.800
I want people to think,
like, this is me.

00:51:30.800 --> 00:51:32.220
This is my life.

00:51:32.220 --> 00:51:34.980
And I'm going to LA in two days.

00:51:34.980 --> 00:51:40.530
And anyone who will take
me, I'd be happy to have.

00:51:40.530 --> 00:51:41.784
So, anyway, thank you so much.

00:51:41.784 --> 00:51:43.700
It's really been a
pleasure speaking with you.

00:51:43.700 --> 00:51:44.200
Thank you.

00:51:44.200 --> 00:51:47.050
[APPLAUSE]

