WEBVTT
Kind: captions
Language: en

00:00:02.300 --> 00:00:04.650
MALE SPEAKER: All
right, welcome.

00:00:04.650 --> 00:00:06.590
It's my pleasure to
introduce Jerry Kaplan.

00:00:06.590 --> 00:00:09.100
He is a serial entrepreneur.

00:00:09.100 --> 00:00:12.000
I'm not sure what flavor
of cereal, but some flavor.

00:00:12.000 --> 00:00:16.670
And co-founded four Silicon
Valley start-ups, two of which

00:00:16.670 --> 00:00:18.250
became publicly
traded companies.

00:00:18.250 --> 00:00:21.230
He's also written-- and I
don't quite understand this,

00:00:21.230 --> 00:00:23.940
I hope you explain-- a
bestselling nonfiction novel.

00:00:23.940 --> 00:00:24.770
JERRY KAPLAN: Yep.

00:00:24.770 --> 00:00:25.478
MALE SPEAKER: OK.

00:00:25.478 --> 00:00:27.890
Called "Startup: A
Silicon Valley Adventure,"

00:00:27.890 --> 00:00:31.180
selected as Business
Week by one of the top

00:00:31.180 --> 00:00:32.560
10 business books of the year.

00:00:32.560 --> 00:00:35.820
And his latest book, he's
going to talk with us today

00:00:35.820 --> 00:00:38.580
about "Humans Need Not Apply:
A Guide to Wealth and Work,

00:00:38.580 --> 00:00:41.060
the Age of Artificial
Intelligence."

00:00:41.060 --> 00:00:45.990
He's now teaching at Stanford
in Stanford Center for Legal

00:00:45.990 --> 00:00:48.480
Informatics, teaches on the
ethics and impact of artificial

00:00:48.480 --> 00:00:49.855
intelligence in
computer science.

00:00:55.690 --> 00:00:57.470
JERRY KAPLAN: OK, just
so I can calibrate

00:00:57.470 --> 00:00:59.870
to the audience, how many
people here are engineers?

00:00:59.870 --> 00:01:01.990
Can you raise your hands?

00:01:01.990 --> 00:01:02.800
Excellent, OK.

00:01:02.800 --> 00:01:06.536
And how many people
on the live stream?

00:01:06.536 --> 00:01:08.160
They can't see me
either, I discovered.

00:01:08.160 --> 00:01:09.820
They just see the slides.

00:01:09.820 --> 00:01:12.360
So the fact that I'm
standing here buck naked

00:01:12.360 --> 00:01:15.190
is invisible to all
of those people.

00:01:15.190 --> 00:01:16.720
See what you missed?

00:01:16.720 --> 00:01:19.940
They're the lucky
ones, that didn't come.

00:01:19.940 --> 00:01:24.420
OK, I'm going to talk today
about artificial intelligence.

00:01:24.420 --> 00:01:27.880
How many of you
do work that is in

00:01:27.880 --> 00:01:31.010
or related to the field of
artificial intelligence?

00:01:31.010 --> 00:01:32.080
Excellent.

00:01:32.080 --> 00:01:34.110
I hope that I'm going
to give you a new way

00:01:34.110 --> 00:01:35.530
to think about that.

00:01:35.530 --> 00:01:38.840
Now the common wisdom about
artificial intelligence

00:01:38.840 --> 00:01:41.330
is that we're building
increasingly intelligent

00:01:41.330 --> 00:01:43.920
machines that are
ultimately going

00:01:43.920 --> 00:01:47.690
to surpass human capabilities.

00:01:47.690 --> 00:01:51.040
They're going to steal our
jobs and possibly, even

00:01:51.040 --> 00:01:53.830
escape human control
and take over the world.

00:01:53.830 --> 00:01:56.420
Well, I'm going to
present the case today

00:01:56.420 --> 00:02:00.840
that this standard
narrative is both misguided,

00:02:00.840 --> 00:02:03.360
and it's counterproductive.

00:02:03.360 --> 00:02:06.120
I think that a more appropriate
framing of the field,

00:02:06.120 --> 00:02:08.150
which is actually
better supported

00:02:08.150 --> 00:02:11.150
by historical events
and current events,

00:02:11.150 --> 00:02:12.970
is that artificial
intelligence is simply

00:02:12.970 --> 00:02:16.350
a natural extension of
longstanding efforts

00:02:16.350 --> 00:02:18.810
to automate tasks.

00:02:18.810 --> 00:02:23.330
This dates back to the start
of the Industrial Revolution.

00:02:23.330 --> 00:02:26.130
And then I'm going to talk about
the consequences of thinking

00:02:26.130 --> 00:02:28.160
in the field in
this different way,

00:02:28.160 --> 00:02:31.000
rather than the way that
it's typically done.

00:02:31.000 --> 00:02:33.470
So let me start
with this, what is

00:02:33.470 --> 00:02:36.830
artificial intelligence really?

00:02:36.830 --> 00:02:39.000
Can machines really think?

00:02:39.000 --> 00:02:40.430
Now I want to tell
you something.

00:02:40.430 --> 00:02:42.320
After a lifetime of
work in this field

00:02:42.320 --> 00:02:46.430
and a great deal of
reflection on this question,

00:02:46.430 --> 00:02:49.940
my reluctant and disappointing
answer is very simple.

00:02:49.940 --> 00:02:52.826
The answer is no.

00:02:52.826 --> 00:02:56.705
Or at least, I agree with
Alan Turing in his conclusion

00:02:56.705 --> 00:03:02.010
of his 1950 paper where he
propose the Turing test,

00:03:02.010 --> 00:03:03.460
he didn't call it
the Turing test.

00:03:03.460 --> 00:03:05.560
You guys, how many
people-- everybody's

00:03:05.560 --> 00:03:06.950
familiar with the Turing test?

00:03:06.950 --> 00:03:07.150
Great.

00:03:07.150 --> 00:03:07.590
Great.

00:03:07.590 --> 00:03:08.090
Great.

00:03:08.090 --> 00:03:09.830
By the way, if you
read the paper,

00:03:09.830 --> 00:03:12.260
it isn't what the public
thinks the Turing test really

00:03:12.260 --> 00:03:13.674
is, a test of intelligence.

00:03:13.674 --> 00:03:15.840
That's not what he says,
that's not what it's about.

00:03:15.840 --> 00:03:21.060
What he said was, I believe the
question, can machines think,

00:03:21.060 --> 00:03:24.400
to be too meaningless
to deserve discussion.

00:03:24.400 --> 00:03:27.760
That's actually what
he said in the paper.

00:03:27.760 --> 00:03:31.360
Now at best, I think, they don't
think the way people think.

00:03:31.360 --> 00:03:32.930
And it's little
more than an analogy

00:03:32.930 --> 00:03:35.730
to say that they
really think at all.

00:03:35.730 --> 00:03:38.220
Because machines are not
people, and there's simply

00:03:38.220 --> 00:03:41.090
no persuasive
evidence that machines

00:03:41.090 --> 00:03:44.130
are on a path to becoming
generally intelligent sentient

00:03:44.130 --> 00:03:48.660
beings, despite what
everybody sees in movies.

00:03:48.660 --> 00:03:52.490
Now you might say--
here's my straw engineer--

00:03:52.490 --> 00:03:54.670
but wait a minute,
can't they solve

00:03:54.670 --> 00:03:57.740
all sorts of complex reasoning
and perception problems?

00:03:57.740 --> 00:03:59.560
My answer is, sure.

00:03:59.560 --> 00:04:03.660
They can perform tasks that
humans use intelligence

00:04:03.660 --> 00:04:06.700
to solve, but that doesn't mean
that the machines themselves

00:04:06.700 --> 00:04:07.570
are intelligent.

00:04:07.570 --> 00:04:10.400
It only means that
a lot of tasks

00:04:10.400 --> 00:04:14.710
that we thought required
general intelligence,

00:04:14.710 --> 00:04:17.279
are really subject to
solution by other, more

00:04:17.279 --> 00:04:18.834
mechanical means.

00:04:18.834 --> 00:04:21.250
Now there's an old joke in AI,
I like that how many of you

00:04:21.250 --> 00:04:23.100
heard this, once the
problem is solved,

00:04:23.100 --> 00:04:25.830
it's no longer considered AI.

00:04:25.830 --> 00:04:26.370
OK?

00:04:26.370 --> 00:04:27.710
Remember that?

00:04:27.710 --> 00:04:30.880
Well, personally, I no
longer think that's a joke.

00:04:30.880 --> 00:04:33.230
And I'm going to take a look
at some of the signature

00:04:33.230 --> 00:04:35.480
accomplishments of
artificial intelligence

00:04:35.480 --> 00:04:38.060
from this new and
different perspective.

00:04:38.060 --> 00:04:40.340
So let's start with chess.

00:04:40.340 --> 00:04:42.410
For decades, the
archetypal test of coming

00:04:42.410 --> 00:04:43.785
of age of artificial
intelligence

00:04:43.785 --> 00:04:46.076
was whether a machine could
ever beat the world's chess

00:04:46.076 --> 00:04:46.660
champion.

00:04:46.660 --> 00:04:49.140
Now many of you may not have
been engineers at that time,

00:04:49.140 --> 00:04:50.750
but I lived through this.

00:04:50.750 --> 00:04:52.600
For a long time,
chess was considered

00:04:52.600 --> 00:04:55.870
the quintessential demonstration
of human intelligence.

00:04:55.870 --> 00:04:58.420
Surely, people thought,
when a computer

00:04:58.420 --> 00:05:01.170
could beat the world's
champion at chess,

00:05:01.170 --> 00:05:02.900
artificial intelligence
had arrived.

00:05:02.900 --> 00:05:05.990
It was the singularity.

00:05:05.990 --> 00:05:09.950
Well, it happened in
1997 when IBM's Deep Blue

00:05:09.950 --> 00:05:12.780
beat the then world
champion, Garry Kasparov.

00:05:12.780 --> 00:05:15.030
And all kinds of ink was
spilled in the media--

00:05:15.030 --> 00:05:17.820
they did have ink
back then-- lamenting

00:05:17.820 --> 00:05:19.540
the arrival of
superintelligent machines.

00:05:19.540 --> 00:05:21.040
There was all kinds
of hand wringing

00:05:21.040 --> 00:05:22.610
over what this
accomplishment meant

00:05:22.610 --> 00:05:24.130
for the future of humankind.

00:05:24.130 --> 00:05:26.610
But the truth is
it meant nothing,

00:05:26.610 --> 00:05:29.360
other than that you could do
a lot of clever programming

00:05:29.360 --> 00:05:32.530
and use the increased speed
of computers to play chess.

00:05:32.530 --> 00:05:33.860
That's all it meant.

00:05:33.860 --> 00:05:35.690
Now the techniques
that were used

00:05:35.690 --> 00:05:37.960
certainly had application
to other classes' problems,

00:05:37.960 --> 00:05:41.870
but it hardly proved to be
the harbinger of the robot

00:05:41.870 --> 00:05:45.030
apocalypse that
everybody had expected.

00:05:45.030 --> 00:05:45.995
So people said, OK.

00:05:45.995 --> 00:05:47.260
OK.

00:05:47.260 --> 00:05:51.260
The world didn't end that day,
well, how about this other day?

00:05:51.260 --> 00:05:53.750
And they said computers
can play chess, that's

00:05:53.750 --> 00:05:55.530
a limited, well defined domain.

00:05:55.530 --> 00:05:59.630
But they'll never be
able to drive a car.

00:05:59.630 --> 00:06:01.660
OK, that requires a
broad understanding

00:06:01.660 --> 00:06:04.469
of the real world, the ability
to make split second judgments

00:06:04.469 --> 00:06:05.510
in chaotic circumstances.

00:06:05.510 --> 00:06:08.000
And it requires common sense.

00:06:08.000 --> 00:06:11.460
Well as you know, this
bulwark of human supremacy

00:06:11.460 --> 00:06:14.980
was breached in
2004, notably in 2004

00:06:14.980 --> 00:06:19.830
with the DARPA Grand Challenge
that Sebastian Thrun, you guys

00:06:19.830 --> 00:06:22.150
probably know Sebastian was
in charge of that project.

00:06:25.280 --> 00:06:28.670
But our self-driving
cars do just that.

00:06:28.670 --> 00:06:30.660
They drive cars.

00:06:30.660 --> 00:06:31.920
They don't build houses.

00:06:31.920 --> 00:06:32.840
Don't cook meals.

00:06:32.840 --> 00:06:34.490
They don't make beds.

00:06:34.490 --> 00:06:36.220
They drive cars.

00:06:36.220 --> 00:06:39.530
So now, we speak of
autonomous vehicles,

00:06:39.530 --> 00:06:41.120
but here's the problem.

00:06:41.120 --> 00:06:44.630
What that means to an engineer
is very different than what

00:06:44.630 --> 00:06:46.669
it means to the general public.

00:06:46.669 --> 00:06:47.960
These are two different things.

00:06:47.960 --> 00:06:50.730
The public discourse
is dangerously

00:06:50.730 --> 00:06:54.190
disconnected from the academic
or technical discourse.

00:06:54.190 --> 00:06:57.500
To an engineer, it means
the data from sensors

00:06:57.500 --> 00:07:00.910
can be synthesized and
analyzed to formulate

00:07:00.910 --> 00:07:03.080
control instructions that
are sent to effectors

00:07:03.080 --> 00:07:05.490
like brakes and steering.

00:07:05.490 --> 00:07:09.320
But to your mother,
to the general public,

00:07:09.320 --> 00:07:11.640
it means that the car
is looking around.

00:07:11.640 --> 00:07:13.830
It's figuring out
what it's seeing.

00:07:13.830 --> 00:07:15.670
It's understanding,
and then it's

00:07:15.670 --> 00:07:18.120
deciding what it's
going to do next.

00:07:18.120 --> 00:07:21.820
Now these two descriptions
are fundamentally different.

00:07:21.820 --> 00:07:24.170
Anybody here know
Brad Templeton?

00:07:24.170 --> 00:07:24.670
No?

00:07:24.670 --> 00:07:25.169
OK.

00:07:25.169 --> 00:07:27.570
He used to work here on
the Google car project.

00:07:27.570 --> 00:07:29.240
He's at Singularity University.

00:07:29.240 --> 00:07:30.840
So he had a great quote.

00:07:30.840 --> 00:07:35.570
He said, your car will
truly be autonomous

00:07:35.570 --> 00:07:37.880
when you instruct it to
take you to the office,

00:07:37.880 --> 00:07:39.830
but it decides to go
to the beach instead.

00:07:42.680 --> 00:07:47.214
OK, so computers can play chess,
and computers can drive cars

00:07:47.214 --> 00:07:48.630
But they moved the
goalpost again.

00:07:48.630 --> 00:07:52.412
They could never play Jeopardy.

00:07:52.412 --> 00:07:54.620
That requires too much world
knowledge, understanding

00:07:54.620 --> 00:07:56.690
metaphors, and clever wordplay.

00:07:56.690 --> 00:07:59.490
Well thanks again to the
ingenious people at IBM,

00:07:59.490 --> 00:08:02.760
this hurdle has also been
cleared, as you know.

00:08:02.760 --> 00:08:06.095
IBM's Watson system beat Ken
Jennings, the world's champion.

00:08:06.095 --> 00:08:08.470
That was 2011.

00:08:08.470 --> 00:08:11.520
Now what is Watson?

00:08:11.520 --> 00:08:14.760
The reality is, it's a
collection of facts and figures

00:08:14.760 --> 00:08:18.070
that are encoded into
cleverly organized modules

00:08:18.070 --> 00:08:21.150
they can quickly and
accurately answer various types

00:08:21.150 --> 00:08:23.560
of common jeopardy questions.

00:08:23.560 --> 00:08:26.900
Watson's main advantage,
most people don't know,

00:08:26.900 --> 00:08:31.520
was that it could ring in
before the other contestants

00:08:31.520 --> 00:08:33.730
when it thought it
had a high probability

00:08:33.730 --> 00:08:35.610
of a correct answer.

00:08:35.610 --> 00:08:37.289
Don't fight with
a machine that can

00:08:37.289 --> 00:08:39.450
hit a button in a millisecond.

00:08:39.450 --> 00:08:42.970
Now it's a remarkable
and very sophisticated

00:08:42.970 --> 00:08:45.590
knowledge base retrieval
and inference system.

00:08:45.590 --> 00:08:47.440
And it was honed-- at
least at that time--

00:08:47.440 --> 00:08:48.960
to a very particular
problem set,

00:08:48.960 --> 00:08:51.830
and now they're trying to
apply it to other problem sets.

00:08:51.830 --> 00:08:54.586
But I'm going to take a
little aside in my talk

00:08:54.586 --> 00:08:55.710
and rant here for a minute.

00:08:59.230 --> 00:09:01.560
There are some issues that
Watson, as it was presented,

00:09:01.560 --> 00:09:05.450
raises the really disturbed me.

00:09:05.450 --> 00:09:08.300
More than you may think,
Watson's Jeopardy victory

00:09:08.300 --> 00:09:10.690
was something of a magic show.

00:09:10.690 --> 00:09:12.510
In my opinion, there
is way too much

00:09:12.510 --> 00:09:15.750
of this sort of gratuitous
anthropomorphism

00:09:15.750 --> 00:09:17.770
in artificial intelligence.

00:09:17.770 --> 00:09:20.580
IBM didn't do the field
any favors, in my view,

00:09:20.580 --> 00:09:22.740
by wrapping Watson
in a theatrical suite

00:09:22.740 --> 00:09:24.302
of anthropomorphic features.

00:09:24.302 --> 00:09:25.760
There's no technical
reason to have

00:09:25.760 --> 00:09:30.770
the system say it's responses in
a calm, didactic tone of voice

00:09:30.770 --> 00:09:34.855
and have this head-like
swirling graphics, suggesting

00:09:34.855 --> 00:09:36.970
that the machine has a
mind and that it's thinking

00:09:36.970 --> 00:09:39.490
about problem,
much less to set it

00:09:39.490 --> 00:09:41.370
right next to the two
other players that way,

00:09:41.370 --> 00:09:44.790
if you've seen how they do that.

00:09:44.790 --> 00:09:46.840
These are really
incidental adornments

00:09:46.840 --> 00:09:49.400
to what is otherwise
a tremendous technical

00:09:49.400 --> 00:09:50.090
achievement.

00:09:50.090 --> 00:09:52.810
But without a deep understanding
of how these systems work,

00:09:52.810 --> 00:09:57.010
and with humans as the only
available exemplars with which

00:09:57.010 --> 00:09:59.460
to interpret the
results, the temptation

00:09:59.460 --> 00:10:03.090
for the average person to view
these systems as human-like

00:10:03.090 --> 00:10:05.200
is absolutely irresistible.

00:10:05.200 --> 00:10:05.960
But they are not.

00:10:08.790 --> 00:10:12.170
Robots don't have independent
goals and desires.

00:10:12.170 --> 00:10:15.765
A robot that is designed
to wash and fold laundry

00:10:15.765 --> 00:10:18.640
may be plenty sophisticated.

00:10:18.640 --> 00:10:20.880
It might learn not to
put the laundry away

00:10:20.880 --> 00:10:23.960
when you're sleeping
or determine how

00:10:23.960 --> 00:10:25.250
you like your shirts folded.

00:10:25.250 --> 00:10:26.830
It could be very sophisticated.

00:10:26.830 --> 00:10:30.000
But it isn't going to wake up
one day and say, oh my god,

00:10:30.000 --> 00:10:31.020
what a fool I've been!

00:10:31.020 --> 00:10:34.670
I really want to play the
great concert halls of Vienna.

00:10:34.670 --> 00:10:37.000
Not going to happen.

00:10:37.000 --> 00:10:42.720
So just as we can teach
bears to ride bikes,

00:10:42.720 --> 00:10:45.970
and we can teach chimps
to use sign language,

00:10:45.970 --> 00:10:50.150
we can build machines that
perform tasks somewhat

00:10:50.150 --> 00:10:51.060
the way people do.

00:10:51.060 --> 00:10:53.280
And we can even
simulate human emotions.

00:10:53.280 --> 00:10:56.170
We can make them say, ouch,
when you pinch them or wag

00:10:56.170 --> 00:10:58.290
their tails when you pet them.

00:10:58.290 --> 00:11:00.140
But there's simply
no compelling reason

00:11:00.140 --> 00:11:02.830
to believe that this bears
any meaningful relationship

00:11:02.830 --> 00:11:06.050
to human behavior or experience.

00:11:06.050 --> 00:11:07.780
Machines aren't
people, even if we

00:11:07.780 --> 00:11:13.770
build them to talk, and to walk,
and to chew gum like we do.

00:11:13.770 --> 00:11:15.730
OK, let me get
back-- end of rant--

00:11:15.730 --> 00:11:17.520
let me get back to my examples.

00:11:17.520 --> 00:11:20.900
Who's doing machine
learning here?

00:11:20.900 --> 00:11:21.570
[INAUDIBLE] is.

00:11:21.570 --> 00:11:22.740
OK, good.

00:11:26.640 --> 00:11:29.700
So maybe Watson isn't the holy
grail, but what about machine

00:11:29.700 --> 00:11:30.750
learning systems?

00:11:30.750 --> 00:11:33.700
Aren't they more like
human intelligence?

00:11:33.700 --> 00:11:35.090
The answer is not really.

00:11:35.090 --> 00:11:38.610
In reality, the use of
these anthropomorphic terms

00:11:38.610 --> 00:11:41.750
like deep learning
and neural networks

00:11:41.750 --> 00:11:44.690
is really little more than
an analogy, in the same sense

00:11:44.690 --> 00:11:51.620
that airplane wings
were inspired by birds.

00:11:51.620 --> 00:11:53.630
Consider how machines
and people learn.

00:11:53.630 --> 00:11:57.600
You can teach a computer
to recognize cats

00:11:57.600 --> 00:12:01.890
by showing it a million images,
as Andrew [INAUDIBLE] did.

00:12:01.890 --> 00:12:03.400
Do you guys know Andrew?

00:12:03.400 --> 00:12:03.930
Good.

00:12:03.930 --> 00:12:05.910
OK.

00:12:05.910 --> 00:12:07.833
I gave him this talk by the way.

00:12:07.833 --> 00:12:12.080
I don't know how happy he
was to hear my point of view.

00:12:12.080 --> 00:12:16.310
You can do this by showing
the machine a million images

00:12:16.310 --> 00:12:20.100
of cats, or you could
simply point one out

00:12:20.100 --> 00:12:24.980
to a three-year-old and get
same job done That's a cat.

00:12:24.980 --> 00:12:26.570
And now they know what a cat is.

00:12:26.570 --> 00:12:30.850
Obviously, humans and machines
do not learn the same way.

00:12:30.850 --> 00:12:32.740
I'm going to give
you another example.

00:12:32.740 --> 00:12:35.461
Anybody here working
on machine translation?

00:12:35.461 --> 00:12:35.960
Nobody?

00:12:35.960 --> 00:12:36.930
Oh my god.

00:12:36.930 --> 00:12:39.570
OK.

00:12:39.570 --> 00:12:41.570
Tremendous strides have
been made in this field,

00:12:41.570 --> 00:12:43.403
as you probably know,
in the past few years,

00:12:43.403 --> 00:12:46.460
mainly by applying statistical
machine learning techniques

00:12:46.460 --> 00:12:49.940
to very large bodies
of concorded text.

00:12:49.940 --> 00:12:52.420
But how do people perform
this difficult task?

00:12:52.420 --> 00:12:55.040
They learn two or
more languages,

00:12:55.040 --> 00:12:58.090
along with the respective
cultures and conventions.

00:12:58.090 --> 00:13:01.110
And then they read some
text in one language,

00:13:01.110 --> 00:13:03.051
they understand what
it says, and then

00:13:03.051 --> 00:13:04.800
they render the meaning
as closely as they

00:13:04.800 --> 00:13:07.050
can in the other language.

00:13:07.050 --> 00:13:10.020
But machine translation, as
successful as it is today,

00:13:10.020 --> 00:13:14.200
bears almost no relationship to
the human translation process.

00:13:14.200 --> 00:13:16.460
It simply means
there's another way

00:13:16.460 --> 00:13:21.030
to approximate the same
results using a machine.

00:13:21.030 --> 00:13:23.315
Well, now we carry
around a smartphone.

00:13:25.970 --> 00:13:27.840
It's sort of reminiscent
of the capabilities

00:13:27.840 --> 00:13:31.430
of the computer on the
Star Trek's Enterprise,

00:13:31.430 --> 00:13:34.600
or maybe even the
Hal 9000, hopefully

00:13:34.600 --> 00:13:36.690
minus the homicidal intent.

00:13:40.830 --> 00:13:41.870
What is it?

00:13:41.870 --> 00:13:43.990
Google-- What was the term?

00:13:43.990 --> 00:13:45.687
Hey Google Voice?

00:13:45.687 --> 00:13:46.520
AUDIENCE: OK Google.

00:13:46.520 --> 00:13:47.230
JERRY KAPLAN: OK Google.

00:13:47.230 --> 00:13:47.810
OK Google.

00:13:47.810 --> 00:13:48.980
Sorry.

00:13:48.980 --> 00:13:51.658
OK Google.

00:13:51.658 --> 00:13:54.600
Yeah, you can talk to your
phone, and it talks back.

00:13:54.600 --> 00:13:56.220
And it becomes more
capable every day

00:13:56.220 --> 00:13:59.460
as you download new apps and you
upgrade the operating system.

00:13:59.460 --> 00:14:02.000
But do you really think of
your phone as getting smarter,

00:14:02.000 --> 00:14:04.830
in this human sense,
when you download an app?

00:14:04.830 --> 00:14:07.614
Or you enable the
voice recognition?

00:14:07.614 --> 00:14:09.280
Certainly not in the
same sense that you

00:14:09.280 --> 00:14:12.500
get smarter when
you learn calculus

00:14:12.500 --> 00:14:14.680
or you learn philosophy.

00:14:14.680 --> 00:14:19.401
It's the electronic equivalent
of a Swiss army knife.

00:14:19.401 --> 00:14:21.400
It's a bunch of different
information processing

00:14:21.400 --> 00:14:23.680
tools bound together
into a single unit,

00:14:23.680 --> 00:14:25.580
to take advantage of
some commonalities,

00:14:25.580 --> 00:14:29.980
like detailed maps or
like internet access.

00:14:29.980 --> 00:14:32.590
You have one integrated mind.

00:14:32.590 --> 00:14:35.270
Your phone has no mind at all.

00:14:35.270 --> 00:14:38.040
No one's home.

00:14:38.040 --> 00:14:40.610
OK, so what is intelligence?

00:14:40.610 --> 00:14:42.970
Let me go on to this.

00:14:42.970 --> 00:14:47.220
So machines perform an
increasingly diverse array

00:14:47.220 --> 00:14:48.960
tasks that people
perform by applying

00:14:48.960 --> 00:14:50.420
their native intelligence.

00:14:50.420 --> 00:14:52.320
Does that mean
machines are smart?

00:14:52.320 --> 00:14:55.690
Let's look at how we might
measure supposed machine

00:14:55.690 --> 00:14:57.132
intelligence.

00:14:57.132 --> 00:14:58.840
Now I pulled this
image off the internet.

00:14:58.840 --> 00:15:00.115
I did not make this slide.

00:15:00.115 --> 00:15:02.400
I'm Illustrating
a point with it.

00:15:02.400 --> 00:15:05.220
We can start by looking at how
humans measure intelligence.

00:15:05.220 --> 00:15:08.790
And of course, one very
common way is with IQ tests.

00:15:08.790 --> 00:15:13.260
But even for humans, this is
an extremely flawed concept.

00:15:13.260 --> 00:15:16.120
We love to measure and
rank things with numbers,

00:15:16.120 --> 00:15:17.620
but let's face it.

00:15:17.620 --> 00:15:21.040
Reducing human intelligence
to a flat, linear scale

00:15:21.040 --> 00:15:23.560
is highly questionable.

00:15:23.560 --> 00:15:27.050
Little Sally did two more
arithmetic problems than Johnny

00:15:27.050 --> 00:15:28.700
in the time that was allotted.

00:15:28.700 --> 00:15:32.090
So her IQ is 7 points
higher than his.

00:15:32.090 --> 00:15:33.719
Bull.

00:15:33.719 --> 00:15:36.010
This is not to say that some
people aren't smarter than

00:15:36.010 --> 00:15:38.600
others, only that's simple,
linear numerical measures

00:15:38.600 --> 00:15:45.250
provide an inappropriate patina
of objectivity and precision.

00:15:45.250 --> 00:15:47.402
Psychologists are
fond of pointing out,

00:15:47.402 --> 00:15:49.360
there are many different
kinds of intelligence.

00:15:49.360 --> 00:15:54.550
Social, emotional, analytic,
athletic, musical, et cetera.

00:15:54.550 --> 00:15:58.030
What does it mean here to say
that Mozart and Einstein have

00:15:58.030 --> 00:16:00.360
the same IQ.

00:16:00.360 --> 00:16:07.560
Now suppose we gave the
same IQ test to a machine,

00:16:07.560 --> 00:16:09.940
the same ones we
administer to humans.

00:16:09.940 --> 00:16:12.190
After all, how else are we
going to measure their IQs?

00:16:12.190 --> 00:16:13.564
Well it took only
one millisecond

00:16:13.564 --> 00:16:15.810
to accurately complete
the same sums that

00:16:15.810 --> 00:16:18.130
took Sally and Johnny an hour.

00:16:18.130 --> 00:16:21.840
That machine must
be super smart.

00:16:21.840 --> 00:16:26.970
Well, as you may know,
calculating-- some people don't

00:16:26.970 --> 00:16:29.050
know this, it's a really
cool fact-- calculating

00:16:29.050 --> 00:16:31.520
used to be the province of
highly trained specialists,

00:16:31.520 --> 00:16:33.170
and they were known
as calculators.

00:16:33.170 --> 00:16:34.841
It was a profession.

00:16:34.841 --> 00:16:37.240
The profession required
considerable intelligence,

00:16:37.240 --> 00:16:41.250
attention to detail, and skill.

00:16:41.250 --> 00:16:43.590
Now all it takes is
a $0.99 calculator.

00:16:43.590 --> 00:16:47.100
Oh my god, the robots
can out perform us!

00:16:47.100 --> 00:16:49.300
They're super smart
on measures that we

00:16:49.300 --> 00:16:50.496
use for human intelligence.

00:16:50.496 --> 00:16:51.870
What are we going
to do when they

00:16:51.870 --> 00:16:53.380
decide they don't
need us anymore,

00:16:53.380 --> 00:16:56.700
and they're going to take over?

00:16:56.700 --> 00:16:58.996
OK.

00:16:58.996 --> 00:17:00.620
You're probably
wondering how I'm going

00:17:00.620 --> 00:17:04.609
to tie this slide into my talk.

00:17:04.609 --> 00:17:07.150
I'll give it a try.

00:17:07.150 --> 00:17:09.880
The truth is that intelligence
isn't an objective, well

00:17:09.880 --> 00:17:14.069
defined, measurable attribute
like mass or blood pressure.

00:17:14.069 --> 00:17:16.530
It's a subjective,
culturally influenced concept

00:17:16.530 --> 00:17:18.170
that's more like beauty.

00:17:18.170 --> 00:17:19.220
Who's more attractive?

00:17:19.220 --> 00:17:23.710
Angelina Jolie or January Jones?

00:17:23.710 --> 00:17:26.690
Is the answer the
same in Nairobi?

00:17:26.690 --> 00:17:29.260
Or in Karachi?

00:17:29.260 --> 00:17:33.570
Can you say that one is 6.5%
more attractive than the other?

00:17:33.570 --> 00:17:36.290
Is that a meaningful
thing to say?

00:17:36.290 --> 00:17:41.430
Now let's apply this observation
to the superintelligence

00:17:41.430 --> 00:17:45.360
debate, which hopefully,
many of you have heard.

00:17:45.360 --> 00:17:48.764
There's a book by Nick Bostrom,
anybody heard of that book?

00:17:48.764 --> 00:17:49.430
A couple people?

00:17:49.430 --> 00:17:50.120
OK.

00:17:50.120 --> 00:17:52.060
I'm going after him
at a talk next week.

00:17:52.060 --> 00:17:55.477
It's not going to be good.

00:17:55.477 --> 00:17:58.060
Let's apply the same thinking
to the superintelligence debate.

00:17:58.060 --> 00:18:00.570
As anybody can see
from the movies-- here

00:18:00.570 --> 00:18:02.470
it is-- female
robots are getting

00:18:02.470 --> 00:18:05.900
more attractive and more
capable all the time.

00:18:05.900 --> 00:18:09.450
If I were to subscribe to a
concept like superintelligence,

00:18:09.450 --> 00:18:13.390
I could write a scholarly
tome about how robots

00:18:13.390 --> 00:18:14.660
are becoming more beautiful.

00:18:14.660 --> 00:18:16.910
And I could, i don't know,
maybe I could call it like,

00:18:16.910 --> 00:18:19.350
super attractiveness
or something.

00:18:19.350 --> 00:18:22.810
And I could conclude that
this is an existential threat

00:18:22.810 --> 00:18:24.050
to humanity.

00:18:24.050 --> 00:18:28.950
Because when female robots begin
to exceed human attractiveness,

00:18:28.950 --> 00:18:31.300
men will only desire to
mate with them instead

00:18:31.300 --> 00:18:33.510
of other people,
and that's going

00:18:33.510 --> 00:18:37.020
to disrupt our natural
reproductive process.

00:18:37.020 --> 00:18:39.740
OK, that's the same
argument that you

00:18:39.740 --> 00:18:41.970
find in super
intelligence, played out

00:18:41.970 --> 00:18:43.560
in a different domain.

00:18:43.560 --> 00:18:46.470
Now here's a news
flash for you guys,

00:18:46.470 --> 00:18:50.410
there's no such thing as
a female or a male robot,

00:18:50.410 --> 00:18:52.280
despite the what the
average moviegoer

00:18:52.280 --> 00:18:54.227
is encouraged to believe.

00:18:54.227 --> 00:18:56.310
And just like there's no
such thing as a generally

00:18:56.310 --> 00:18:58.310
intelligent robot.

00:18:58.310 --> 00:19:01.670
That's a comparable
slice of fiction.

00:19:01.670 --> 00:19:04.280
Now we might want to
create such a thing,

00:19:04.280 --> 00:19:06.070
but a sober look at
the results so far

00:19:06.070 --> 00:19:09.640
don't suggest at all that
we're on the path to doing so.

00:19:09.640 --> 00:19:12.340
It's a little bit
like climbing a tree

00:19:12.340 --> 00:19:14.590
and claiming progress
on getting to the moon.

00:19:17.530 --> 00:19:19.930
Now there's a long and
undistinguished history

00:19:19.930 --> 00:19:22.180
of false alarms in
artificial intelligence,

00:19:22.180 --> 00:19:24.520
predicting the imminent
emergence of generally

00:19:24.520 --> 00:19:26.200
intelligent machines.

00:19:26.200 --> 00:19:29.440
Anybody here know
what a perceptron is?

00:19:29.440 --> 00:19:31.580
I took this out of
the talk, but you've

00:19:31.580 --> 00:19:34.600
got to be older than-- It's
interesting, gray hairs,

00:19:34.600 --> 00:19:35.460
hands go up.

00:19:35.460 --> 00:19:37.920
This was the beginning
of neural networks,

00:19:37.920 --> 00:19:39.570
very interesting story.

00:19:39.570 --> 00:19:43.200
But neuroperceptrons,
you guys may not

00:19:43.200 --> 00:19:47.799
have heard of symbolic systems,
general problem solvers,

00:19:47.799 --> 00:19:50.340
connection machines-- anybody
remember the connection machine

00:19:50.340 --> 00:19:55.560
was going to take over the
world-- scripts, frames, expert

00:19:55.560 --> 00:19:58.540
systems-- who lived
through that debacle.

00:19:58.540 --> 00:20:00.070
I did.

00:20:00.070 --> 00:20:02.220
I was in one of
those companies--

00:20:02.220 --> 00:20:05.775
a case based reasoning, Doug
Lennit's work-- it's pysch.

00:20:05.775 --> 00:20:09.300
You guys heard of that-- the
fifth generation computers

00:20:09.300 --> 00:20:12.600
and Lisp machines, they were
going to take over the world.

00:20:12.600 --> 00:20:15.670
Luckily, most of these
have been forgotten.

00:20:15.670 --> 00:20:18.640
So are the robots taking over?

00:20:18.640 --> 00:20:20.154
Well by the
superintelligence logic,

00:20:20.154 --> 00:20:21.570
the machines took
over a long time

00:20:21.570 --> 00:20:23.820
ago whether they
were smarter not.

00:20:23.820 --> 00:20:25.060
They move our freight.

00:20:25.060 --> 00:20:25.960
They score a test.

00:20:25.960 --> 00:20:26.990
They explore the cosmos.

00:20:26.990 --> 00:20:28.150
They handle our billing.

00:20:28.150 --> 00:20:29.900
They plant and pick
most of our crops.

00:20:29.900 --> 00:20:30.760
They trade stocks.

00:20:30.760 --> 00:20:32.530
They store and
retrieve our documents.

00:20:32.530 --> 00:20:34.700
They manufacturer
just about everything,

00:20:34.700 --> 00:20:36.310
including themselves.

00:20:36.310 --> 00:20:37.940
Sometimes with human
help, sometimes

00:20:37.940 --> 00:20:40.100
without human intervention.

00:20:40.100 --> 00:20:42.580
All these tasks
that not long ago,

00:20:42.580 --> 00:20:45.600
would have been regarded
as requiring human effort

00:20:45.600 --> 00:20:46.520
and intelligence.

00:20:46.520 --> 00:20:52.020
And yet, they often predicted
demise of human labor,

00:20:52.020 --> 00:20:53.650
never ride.

00:20:53.650 --> 00:20:56.070
And nor have the
machines taking over.

00:20:56.070 --> 00:20:59.010
The robots are
doing these things,

00:20:59.010 --> 00:21:01.440
but they're taking
over our businesses.

00:21:01.440 --> 00:21:03.790
They're not marrying
our children.

00:21:03.790 --> 00:21:06.140
And they're not watching
the SciFi channel

00:21:06.140 --> 00:21:07.530
when you're not home.

00:21:07.530 --> 00:21:09.850
That's not what's going on.

00:21:09.850 --> 00:21:12.430
So what's wrong with the
traditional picture of AI?

00:21:12.430 --> 00:21:14.150
We can build machines
and write programs

00:21:14.150 --> 00:21:17.810
to perform tasks that previously
required human intelligence

00:21:17.810 --> 00:21:18.690
attention.

00:21:18.690 --> 00:21:21.170
But there's nothing
really new about that.

00:21:21.170 --> 00:21:24.690
Plowing a straight
furrow, used to be

00:21:24.690 --> 00:21:27.510
a matter of considerable human
experience and skill, yet

00:21:27.510 --> 00:21:30.910
today, no one marvels at
the ability of a tractor

00:21:30.910 --> 00:21:33.040
to do the same thing.

00:21:33.040 --> 00:21:35.740
More contemporary example,
painting natural looking hair

00:21:35.740 --> 00:21:37.880
used to be a skill
honed through years

00:21:37.880 --> 00:21:41.380
of practice and apprenticeship
by our most talented artists.

00:21:41.380 --> 00:21:44.540
But today, nobody
blinks an eye when

00:21:44.540 --> 00:21:49.110
Disney uses CGI rendering to
animate Rapunzel's flowing

00:21:49.110 --> 00:21:51.029
hair.

00:21:51.029 --> 00:21:52.570
Now both of those
advances are better

00:21:52.570 --> 00:21:55.190
understood as progress
in automation,

00:21:55.190 --> 00:21:58.720
not as re-creation of
human intelligence.

00:21:58.720 --> 00:22:01.350
We can program machines to
solve very complex problems,

00:22:01.350 --> 00:22:05.080
and we can program it to
operate with great independence.

00:22:05.080 --> 00:22:08.170
But to call this a simulated
form of human intelligence

00:22:08.170 --> 00:22:10.570
is little more than a fancy.

00:22:10.570 --> 00:22:13.990
So my point is simple, lots of
problems that we think require

00:22:13.990 --> 00:22:16.110
intelligence, don't.

00:22:16.110 --> 00:22:17.960
There are other
ways to solve them.

00:22:17.960 --> 00:22:21.280
And that's what we're
using machines to do.

00:22:21.280 --> 00:22:23.190
OK.

00:22:23.190 --> 00:22:27.850
Now I hope you can
give me my perspective

00:22:27.850 --> 00:22:31.130
just for a few minutes, and let
me talk about what that means

00:22:31.130 --> 00:22:34.270
in terms of the social
impact and economic impact

00:22:34.270 --> 00:22:35.770
that these things
are going to have.

00:22:40.240 --> 00:22:44.140
It's certainly true that
artificial intelligence

00:22:44.140 --> 00:22:45.950
is going to have a
serious impact on labor

00:22:45.950 --> 00:22:48.033
markets and employment,
but perhaps not in the way

00:22:48.033 --> 00:22:49.060
that you expect.

00:22:49.060 --> 00:22:52.140
If you think of machines as
becoming evermore intelligent

00:22:52.140 --> 00:22:55.440
and threatening our livelihoods,
the obvious solution

00:22:55.440 --> 00:22:57.830
is to prevent them
from getting smarter.

00:22:57.830 --> 00:23:00.860
To lock our doors and arm
ourselves with tasers.

00:23:00.860 --> 00:23:04.010
And you see stuff like this,
even among AI community

00:23:04.010 --> 00:23:06.990
we write stop this research
is dangerous these things are

00:23:06.990 --> 00:23:09.500
going to get loose.

00:23:09.500 --> 00:23:12.570
Well I got news for you,
the robots are coming,

00:23:12.570 --> 00:23:17.890
but they aren't exactly
coming for our jobs.

00:23:17.890 --> 00:23:20.480
Machines and computers
actually don't perform jobs,

00:23:20.480 --> 00:23:21.570
this is misunderstood.

00:23:21.570 --> 00:23:24.350
What they do is
they automate tasks.

00:23:24.350 --> 00:23:26.490
Except in the most
extreme cases,

00:23:26.490 --> 00:23:29.190
you don't roll in a robot
and show an employee here,

00:23:29.190 --> 00:23:30.190
to the door.

00:23:30.190 --> 00:23:32.330
Instead, the new
technology hollows out,

00:23:32.330 --> 00:23:35.800
it changes, the jobs
that people perform.

00:23:35.800 --> 00:23:38.560
Now even experts spend most
their time doing mundane,

00:23:38.560 --> 00:23:41.370
repetitive tasks like
reviewing lab test results,

00:23:41.370 --> 00:23:44.370
drafting contracts, or
writing press releases,

00:23:44.370 --> 00:23:46.922
filling out paperwork, ,
forms, things like that.

00:23:46.922 --> 00:23:48.380
On the blue collar
side, you've got

00:23:48.380 --> 00:23:52.100
lots of workers who drive
cars, load trucks, pack boxes,

00:23:52.100 --> 00:23:56.260
take blood samples, fight fires,
deliver mail, direct traffic.

00:23:56.260 --> 00:24:00.090
And many of those intellectual
and physical tasks

00:24:00.090 --> 00:24:05.010
require straightforward logic
or simple hand-eye coordination.

00:24:05.010 --> 00:24:09.880
And the new technologies,
mainly driven by advances in AI,

00:24:09.880 --> 00:24:13.800
are poised to automate a
lot of these kinds of tasks.

00:24:13.800 --> 00:24:17.260
Now if your job involves
a narrow, well defined set

00:24:17.260 --> 00:24:19.610
of duties, as many
do, then indeed,

00:24:19.610 --> 00:24:21.640
your employment is at risk.

00:24:21.640 --> 00:24:24.070
If you have a broader
set of responsibilities

00:24:24.070 --> 00:24:27.790
or if your job requires
some kind of human touch,

00:24:27.790 --> 00:24:30.960
such as expressing sympathy
or providing companionship,

00:24:30.960 --> 00:24:33.370
I don't think that you really
have much to worry about.

00:24:33.370 --> 00:24:37.160
Check out this comparison
of bricklayers, laid bricks.

00:24:37.160 --> 00:24:39.690
That's what they do, you can
build a machine to lay bricks.

00:24:39.690 --> 00:24:42.380
Licensed practical
nurses-- I got this off

00:24:42.380 --> 00:24:44.291
of an official list
of their duties--

00:24:44.291 --> 00:24:45.790
ensuring patients
and their families

00:24:45.790 --> 00:24:51.150
understand release instructions,
providing emotional support.

00:24:51.150 --> 00:24:55.220
Most jobs require a mix
of general capabilities

00:24:55.220 --> 00:24:56.780
and specific skills.

00:24:56.780 --> 00:24:59.390
And as the machines can perform
more routine activities,

00:24:59.390 --> 00:25:02.370
the plain fact is
that fewer people

00:25:02.370 --> 00:25:04.800
are needed to get those
particular jobs done.

00:25:04.800 --> 00:25:07.450
So one person's
productivity enhancing tool

00:25:07.450 --> 00:25:09.060
is another's pink slip.

00:25:09.060 --> 00:25:12.070
Or more likely, a job opening
that doesn't need to get

00:25:12.070 --> 00:25:12.570
filled.

00:25:12.570 --> 00:25:15.880
There's this big, false
debate between whether we're

00:25:15.880 --> 00:25:17.910
making people more
productive or whether we're

00:25:17.910 --> 00:25:19.440
putting people out of work.

00:25:19.440 --> 00:25:22.170
These are potential future
headlines that capture that.

00:25:22.170 --> 00:25:24.260
The New York Times,
they might say,

00:25:24.260 --> 00:25:27.170
robots steal jobs
at record pace.

00:25:27.170 --> 00:25:29.420
And The Wall Street Journal
might look at the same day

00:25:29.420 --> 00:25:32.370
and say, profits rise as
worker productivity soars.

00:25:35.220 --> 00:25:36.020
OK.

00:25:36.020 --> 00:25:44.150
Now economists call this
process of the hollowing

00:25:44.150 --> 00:25:48.124
out of these jobs, changing the
jobs, structural unemployment.

00:25:48.124 --> 00:25:50.540
Automation, whether it's driven
by artificial intelligence

00:25:50.540 --> 00:25:52.340
or not, changes
the skills that are

00:25:52.340 --> 00:25:54.260
necessary to perform the work.

00:25:54.260 --> 00:25:56.030
If the oncologist
no longer needs

00:25:56.030 --> 00:25:59.490
to read MRIs or the
accountant operates a computer

00:25:59.490 --> 00:26:01.540
program instead of doing
their calculations,

00:26:01.540 --> 00:26:03.100
which they used
to do by hand, you

00:26:03.100 --> 00:26:05.150
have different aptitudes,
different talents,

00:26:05.150 --> 00:26:07.460
and different training
that might be required.

00:26:07.460 --> 00:26:09.790
So this is what they call
structural or technical

00:26:09.790 --> 00:26:10.900
unemployment.

00:26:10.900 --> 00:26:12.620
It's the mismatch
of skills that are

00:26:12.620 --> 00:26:15.900
needed by employers to the
actual skills of the workforce.

00:26:15.900 --> 00:26:17.900
So the more pressing
problem posed

00:26:17.900 --> 00:26:20.140
by artificial
intelligence for workers

00:26:20.140 --> 00:26:24.180
is not so much a lack of jobs
but the way new technology

00:26:24.180 --> 00:26:26.870
transforms the nature of work
and therefore, the training

00:26:26.870 --> 00:26:30.190
that's required to
perform those jobs.

00:26:30.190 --> 00:26:34.030
Now it's helpful to look at
this problem dynamically.

00:26:34.030 --> 00:26:37.210
Historically, as
automation has eliminated

00:26:37.210 --> 00:26:40.170
the need for workers, the
resulting increasing wealth

00:26:40.170 --> 00:26:42.386
has eventually generated
all kinds of new jobs

00:26:42.386 --> 00:26:43.260
to take up the slack.

00:26:43.260 --> 00:26:46.505
And I see no reason why
this won't continue.

00:26:46.505 --> 00:26:49.170
But the keyword
there is eventually.

00:26:49.170 --> 00:26:50.370
Let me give you some data.

00:26:50.370 --> 00:26:52.286
It's very helpful to
look at this dynamically.

00:26:52.286 --> 00:26:57.290
200 years ago, more than
90% of the US population

00:26:57.290 --> 00:26:59.580
worked in agriculture.

00:26:59.580 --> 00:27:01.900
Basically, almost
all anybody ever

00:27:01.900 --> 00:27:03.760
did was grow and prepare food.

00:27:03.760 --> 00:27:05.300
The stuff you see
on Downton Abbey,

00:27:05.300 --> 00:27:09.410
that was a tiny little
sliver of the population.

00:27:09.410 --> 00:27:12.430
Today, less than 2%
of the population

00:27:12.430 --> 00:27:14.460
is required to feed everybody.

00:27:14.460 --> 00:27:14.979
On my god!

00:27:14.979 --> 00:27:16.020
Is everybody out of work?

00:27:16.020 --> 00:27:17.800
Of course not.

00:27:17.800 --> 00:27:19.630
We've had plenty
of time to adapt.

00:27:19.630 --> 00:27:22.210
And as our standard of
living has relentlessly

00:27:22.210 --> 00:27:24.530
increased-- which I'm going
to get to in a minute--

00:27:24.530 --> 00:27:26.560
new opportunities have
always arisen for people

00:27:26.560 --> 00:27:30.750
to fill the ever
expanding expectations

00:27:30.750 --> 00:27:33.430
of our ever richer society.

00:27:33.430 --> 00:27:36.190
Now try this on for size.

00:27:36.190 --> 00:27:38.680
Imagine you are an average
person in the 1800s

00:27:38.680 --> 00:27:41.220
and we time-traveled
them to today.

00:27:41.220 --> 00:27:43.750
They would think
we'd all gone nuts.

00:27:43.750 --> 00:27:47.170
Why not work a few hours a week,
buy a sack of potatoes and jug

00:27:47.170 --> 00:27:48.670
of wine, build a
shack in the woods,

00:27:48.670 --> 00:27:52.450
dig a hole for an outhouse,
and live a life of leisure.

00:27:52.450 --> 00:27:54.900
That's what they would think.

00:27:54.900 --> 00:27:58.970
So somehow, however,
our rising expectations

00:27:58.970 --> 00:28:02.210
seem to magically keep
pace with our wealth.

00:28:02.210 --> 00:28:04.850
So you might say, well, OK,
what's going to happen next?

00:28:04.850 --> 00:28:07.510
What are these jobs of tomorrow?

00:28:07.510 --> 00:28:09.010
Now contrary to
what you might think

00:28:09.010 --> 00:28:12.780
about the end of work or
robots doing everybody's jobs,

00:28:12.780 --> 00:28:15.830
it turns out that there
are many, many, current

00:28:15.830 --> 00:28:18.280
and there are going to be
a lot more future jobs that

00:28:18.280 --> 00:28:20.020
simply can't be automated.

00:28:20.020 --> 00:28:22.400
They inherently involve
a personal touch,

00:28:22.400 --> 00:28:27.590
or demonstration of skill, or
person to person interaction.

00:28:27.590 --> 00:28:29.500
I don't see why we
can't become a society

00:28:29.500 --> 00:28:33.810
of competitive gamers, of
artisans, of personal shoppers,

00:28:33.810 --> 00:28:37.660
of flower arrangers,
of tennis pros,

00:28:37.660 --> 00:28:40.350
of party planners, and no
doubt a lot of other things

00:28:40.350 --> 00:28:42.460
that don't exist yet at all.

00:28:42.460 --> 00:28:47.600
I mean, nobody wants to go to
a robotic undertaker who says,

00:28:47.600 --> 00:28:50.150
I am so sorry for your loss.

00:28:50.150 --> 00:28:52.170
It doesn't work.

00:28:52.170 --> 00:28:55.480
So who's going to do the
real work, you might ask.

00:28:55.480 --> 00:28:59.250
Well our great-grandchildren
may think our idea of real work

00:28:59.250 --> 00:29:00.940
is so 21st century.

00:29:00.940 --> 00:29:03.490
I won't be around, but
a lot of you guys will.

00:29:03.490 --> 00:29:06.300
It may only take 2% of
the Population assisted

00:29:06.300 --> 00:29:08.290
by some pretty
remarkable automation,

00:29:08.290 --> 00:29:11.960
to accomplish what takes
90% of our labor today.

00:29:11.960 --> 00:29:16.650
The jobs you guys
have may not be there.

00:29:16.650 --> 00:29:17.350
So what?

00:29:17.350 --> 00:29:19.400
That's the historical
pattern and it's

00:29:19.400 --> 00:29:21.940
likely to continue
in the future.

00:29:21.940 --> 00:29:24.790
Now our expectations
keep rising,

00:29:24.790 --> 00:29:28.860
like my mythical 1800s
person out the woods,

00:29:28.860 --> 00:29:31.050
today, here's some weird facts.

00:29:31.050 --> 00:29:31.780
It's really fun.

00:29:31.780 --> 00:29:33.113
It's great we have the internet.

00:29:33.113 --> 00:29:36.080
It makes it easy to
pop up crazy stuff.

00:29:36.080 --> 00:29:39.110
Today, 70% percent of
the people in the US

00:29:39.110 --> 00:29:42.370
take a shower every day.

00:29:42.370 --> 00:29:47.470
Now I'm not asking
for a show of hands,

00:29:47.470 --> 00:29:51.680
but in 1900 the average
was once a week.

00:29:51.680 --> 00:29:54.850
That was the old normal.

00:29:54.850 --> 00:29:57.640
Now it may be as important
to our grandchildren

00:29:57.640 --> 00:30:00.010
to have fresh flowers
in the house every day,

00:30:00.010 --> 00:30:02.972
as it is to us to
shower every day.

00:30:02.972 --> 00:30:04.680
That's the kind of
change in expectations

00:30:04.680 --> 00:30:06.440
that we're talking about.

00:30:06.440 --> 00:30:10.300
Two of my kids just
got their first jobs.

00:30:10.300 --> 00:30:12.990
And I couldn't help but notice
that their chosen professions

00:30:12.990 --> 00:30:14.890
didn't exist 10 years ago.

00:30:14.890 --> 00:30:18.410
One does social media
promotion for restaurants,

00:30:18.410 --> 00:30:20.130
and the other
one-- you guys will

00:30:20.130 --> 00:30:23.240
know what this is-- works at
Udacity, the online education

00:30:23.240 --> 00:30:24.820
company.

00:30:24.820 --> 00:30:26.960
So that's the good news.

00:30:26.960 --> 00:30:28.270
But there's bad news.

00:30:28.270 --> 00:30:31.340
The bad news is it takes
time for these transitions

00:30:31.340 --> 00:30:32.220
to happen.

00:30:32.220 --> 00:30:34.950
And a new wave of AI
enabled applications

00:30:34.950 --> 00:30:37.755
is likely to accelerate the
normal cycle of job destruction

00:30:37.755 --> 00:30:39.020
and creation.

00:30:39.020 --> 00:30:44.170
So we have to find new ways
to retrain displaced workers.

00:30:44.170 --> 00:30:47.230
And we don't have the right
mechanisms to do that today.

00:30:47.230 --> 00:30:49.659
So I've written a book.

00:30:49.659 --> 00:30:51.700
So in the book, I talk
about what kinds of things

00:30:51.700 --> 00:30:52.410
we need to do.

00:30:52.410 --> 00:30:55.730
Well, one of them is the idea,
I just call it a job mortgage.

00:30:55.730 --> 00:30:58.165
It's not new to me,
there's been a history

00:30:58.165 --> 00:31:00.290
of talking about this, but
I framed it a little bit

00:31:00.290 --> 00:31:01.122
differently.

00:31:01.122 --> 00:31:02.830
Basically, people
should be able to learn

00:31:02.830 --> 00:31:08.520
new skills by borrowing against
their future earnings capacity.

00:31:08.520 --> 00:31:10.370
Today, vocational
training in the US

00:31:10.370 --> 00:31:14.060
is really messed up--
this is all in the book--

00:31:14.060 --> 00:31:17.340
mainly because the government
is the lender of first resort

00:31:17.340 --> 00:31:18.354
for the student loans.

00:31:18.354 --> 00:31:19.770
And the skills of
people learn are

00:31:19.770 --> 00:31:22.630
disconnected from the immediate
needs of the marketplace.

00:31:22.630 --> 00:31:24.510
So we're not investing
in education,

00:31:24.510 --> 00:31:25.930
we're handing out
money to people

00:31:25.930 --> 00:31:29.650
to learn things that won't help
them to pay that money back.

00:31:29.650 --> 00:31:32.505
If you can't get a job, too bad.

00:31:32.505 --> 00:31:34.710
Your student loan is still due.

00:31:34.710 --> 00:31:35.859
We need to fix that.

00:31:35.859 --> 00:31:37.650
We need to create new
financial instruments

00:31:37.650 --> 00:31:39.390
that tie the
deployment of capital

00:31:39.390 --> 00:31:41.300
to the return on
investment, just

00:31:41.300 --> 00:31:43.630
as we do with home mortgages.

00:31:43.630 --> 00:31:46.660
And the history of that is also
in the book, very interesting.

00:31:46.660 --> 00:31:49.780
That was created and it doubled
home ownership in the US

00:31:49.780 --> 00:31:53.950
by creating special kinds
of financial instruments.

00:31:53.950 --> 00:31:56.000
We can use the discipline
of the marketplace

00:31:56.000 --> 00:31:59.260
to help us productively
repurpose displaced workers.

00:31:59.260 --> 00:32:01.560
And that not only
benefits the ones who

00:32:01.560 --> 00:32:04.550
find their skills obsoleted,
like you programmers,

00:32:04.550 --> 00:32:08.380
but society in general,
it's good for us.

00:32:08.380 --> 00:32:12.800
Now finally, there's
one more dark cloud

00:32:12.800 --> 00:32:16.610
related to advances in
artificial intelligence.

00:32:16.610 --> 00:32:19.820
While it's true that automation
makes society richer,

00:32:19.820 --> 00:32:22.840
there are very serious
questions about whose pockets

00:32:22.840 --> 00:32:26.840
are going to be filled
by that increased wealth.

00:32:26.840 --> 00:32:32.800
Now those of us
in high tech tend

00:32:32.800 --> 00:32:34.920
to believe we're developing
dazzling technologies

00:32:34.920 --> 00:32:36.930
for a needy and grateful world.

00:32:36.930 --> 00:32:39.181
And indeed, we've made
tremendous progress

00:32:39.181 --> 00:32:41.430
on raising the standard of
living for the very poorest

00:32:41.430 --> 00:32:43.370
of the poor around the world.

00:32:43.370 --> 00:32:46.470
But for the developed world,
the news is not so good.

00:32:46.470 --> 00:32:50.320
Many of you may know that up
until around 1970, on and off,

00:32:50.320 --> 00:32:52.110
we found ways to
distribute at least some

00:32:52.110 --> 00:32:54.350
of these economic benefits
of increased productivity

00:32:54.350 --> 00:32:56.260
broadly across society.

00:32:56.260 --> 00:33:00.540
And this supported the rise
of this mythical middle class.

00:33:00.540 --> 00:33:02.436
But it doesn't
take much to see--

00:33:02.436 --> 00:33:04.060
and economists are
lamenting the fact--

00:33:04.060 --> 00:33:06.590
that those days are over.

00:33:06.590 --> 00:33:11.920
As economists know,
automation is the substitution

00:33:11.920 --> 00:33:14.230
of capital for labor.

00:33:14.230 --> 00:33:16.600
And I'm here to tell
you, Karl Marx was right.

00:33:16.600 --> 00:33:18.390
He was an economist.

00:33:18.390 --> 00:33:20.140
The struggle between
capital labor

00:33:20.140 --> 00:33:22.650
is a losing proposition
for the workers.

00:33:22.650 --> 00:33:24.960
What that means is that
the benefits of automation

00:33:24.960 --> 00:33:29.210
naturally accrue to those who
can invest in new systems.

00:33:29.210 --> 00:33:30.590
And why not?

00:33:30.590 --> 00:33:33.650
People aren't really working
any harder than they used to.

00:33:33.650 --> 00:33:38.040
And frankly, we're not really
smarter than people used to be.

00:33:38.040 --> 00:33:40.700
In fact, interesting fact,
is that working hours have

00:33:40.700 --> 00:33:43.250
actually decreased on
average, slowly, but pretty

00:33:43.250 --> 00:33:45.166
consistently, over
the last 100 years,

00:33:45.166 --> 00:33:46.790
it used to be normal
to work an 80 hour

00:33:46.790 --> 00:33:50.930
week in the early 1800s.

00:33:50.930 --> 00:33:54.390
The reason we can
do more with less

00:33:54.390 --> 00:33:57.190
is that the business owners
invest some of their capital

00:33:57.190 --> 00:33:59.940
into the process and
productivity improvements.

00:33:59.940 --> 00:34:03.250
And it makes sense, under
our economic system,

00:34:03.250 --> 00:34:05.590
that they get the rewards.

00:34:05.590 --> 00:34:08.590
So what does all this
got to do with AI?

00:34:08.590 --> 00:34:09.590
Is [INAUDIBLE] on there?

00:34:09.590 --> 00:34:10.969
No.

00:34:10.969 --> 00:34:13.320
Some of your greatest hits?

00:34:13.320 --> 00:34:14.780
All your friends?

00:34:14.780 --> 00:34:16.889
The technologies that
are on the drawing boards

00:34:16.889 --> 00:34:19.170
and are in the labs
here at Google,

00:34:19.170 --> 00:34:22.420
are quickening the hearts of
entrepreneurs, and investors,

00:34:22.420 --> 00:34:26.270
and business owners
everywhere because they're

00:34:26.270 --> 00:34:27.949
the ones that stand to benefit.

00:34:27.949 --> 00:34:30.580
While they're able to
export more and more risks

00:34:30.580 --> 00:34:34.110
to the rest of society, and
workers become less secure,

00:34:34.110 --> 00:34:37.090
wages stagnate,
pension funds go bust.

00:34:37.090 --> 00:34:39.219
We're raising a
generation of contractors

00:34:39.219 --> 00:34:42.350
for the gig economy, whose
variable working hours

00:34:42.350 --> 00:34:45.600
and health benefits
are their problem.

00:34:45.600 --> 00:34:46.870
We need to fix that.

00:34:46.870 --> 00:34:49.739
Now some people, if you're
following the latest

00:34:49.739 --> 00:34:51.810
political debates,
if you've watched

00:34:51.810 --> 00:34:56.719
any of the Republican
debates-- horrifies me--

00:34:56.719 --> 00:34:58.440
people have the
mistaken impression

00:34:58.440 --> 00:35:00.180
that the free market is going
to address these problems,

00:35:00.180 --> 00:35:02.130
if only we can get the
government out of way.

00:35:02.130 --> 00:35:03.750
I'm here to tell
you, our economy

00:35:03.750 --> 00:35:06.570
is hardly an example
unfettered capitalism.

00:35:06.570 --> 00:35:09.340
And believe me, I've been
a beneficiary of that.

00:35:09.340 --> 00:35:11.870
The fact is, there are all
sorts of rules and policies

00:35:11.870 --> 00:35:14.620
that drive where the capital
goes, how it's deployed,

00:35:14.620 --> 00:35:16.000
who gets the returns.

00:35:16.000 --> 00:35:17.670
It's out of whack.

00:35:17.670 --> 00:35:20.180
And the problem is that
our economic and regulatory

00:35:20.180 --> 00:35:25.260
policies have become decoupled
from our social goals.

00:35:25.260 --> 00:35:27.190
And we have to fix that.

00:35:27.190 --> 00:35:28.250
But how do we do that?

00:35:28.250 --> 00:35:29.510
OK.

00:35:29.510 --> 00:35:32.440
Now as with the
labor markets, it's

00:35:32.440 --> 00:35:35.260
helpful to look at this
problem dynamically.

00:35:35.260 --> 00:35:36.805
If you just look
at it today, you

00:35:36.805 --> 00:35:37.890
think well, we go
to raise taxes,

00:35:37.890 --> 00:35:39.190
take the money away
from the rich people,

00:35:39.190 --> 00:35:40.400
give it to poor people.

00:35:40.400 --> 00:35:42.790
That's not necessarily the case.

00:35:42.790 --> 00:35:46.410
The good news is that
economy isn't static.

00:35:46.410 --> 00:35:49.750
It actually doubles, GNP
doubles every 40 years.

00:35:49.750 --> 00:35:51.210
And it's done that
pretty reliably

00:35:51.210 --> 00:35:53.400
since the start of the
Industrial Revolution

00:35:53.400 --> 00:35:55.260
in the 1700s.

00:35:55.260 --> 00:35:59.450
Here's some more weird
facts from the internet.

00:35:59.450 --> 00:36:05.100
In 1800, can anybody guess
what the average household

00:36:05.100 --> 00:36:07.650
annual income was in the US?

00:36:07.650 --> 00:36:11.310
1800, average annual
household income of US,

00:36:11.310 --> 00:36:12.643
anybody want to venture a guess?

00:36:12.643 --> 00:36:13.690
AUDIENCE: $1,000

00:36:13.690 --> 00:36:15.330
JERRY KAPLAN: Exactly.

00:36:15.330 --> 00:36:17.690
People actually get this
right, the people in the front,

00:36:17.690 --> 00:36:18.080
it's interesting.

00:36:18.080 --> 00:36:19.785
The ones in the
front get it right.

00:36:19.785 --> 00:36:22.160
The ones in the back
row are on their phones,

00:36:22.160 --> 00:36:24.650
checking their mail.

00:36:24.650 --> 00:36:28.620
It was $1,000 That's an
inflation adjusted figure.

00:36:28.620 --> 00:36:31.200
This isn't some kind of old
greenbacks, gold standard,

00:36:31.200 --> 00:36:34.304
we're talking about $1,000.

00:36:34.304 --> 00:36:35.720
Now that's about
the same as it is

00:36:35.720 --> 00:36:39.214
today in Malawi and Mozambique.

00:36:39.214 --> 00:36:40.630
And it's probably
not coincidental

00:36:40.630 --> 00:36:42.921
that their economies, the
structure of their economies,

00:36:42.921 --> 00:36:44.460
look surprisingly
similar to what

00:36:44.460 --> 00:36:47.940
it was in the US 200 years ago.

00:36:47.940 --> 00:36:51.390
So I doubt that people
in Ben Franklin's time

00:36:51.390 --> 00:36:53.630
thought of themselves
as dirt poor,

00:36:53.630 --> 00:36:57.370
that they were barely
scratching out an existence.

00:36:57.370 --> 00:37:00.320
Now what does this mean?

00:37:00.320 --> 00:37:03.085
40 years from now,
most likely, there

00:37:03.085 --> 00:37:06.050
will literally be twice as
much wealth to go around.

00:37:06.050 --> 00:37:09.505
And the challenge for us
is to implement policies

00:37:09.505 --> 00:37:11.130
that are going to
encourage that wealth

00:37:11.130 --> 00:37:13.850
to be more broadly distributed.

00:37:13.850 --> 00:37:17.410
We don't have to steal from
the rich and give to the poor.

00:37:17.410 --> 00:37:20.040
We need to provide proper
incentives for entrepreneurs

00:37:20.040 --> 00:37:21.980
and businesses to
find ways to benefit

00:37:21.980 --> 00:37:25.260
an ever wider swath
of society, as opposed

00:37:25.260 --> 00:37:27.630
to their stockholders
and themselves.

00:37:27.630 --> 00:37:30.050
Now in my book-- available
in bookstores everywhere.

00:37:30.050 --> 00:37:35.094
I have one back there-- I
give at least one example.

00:37:35.094 --> 00:37:36.510
And it's just an
example, the kind

00:37:36.510 --> 00:37:40.630
of thinking I believe we should
do to solve this problem.

00:37:40.630 --> 00:37:42.550
Let me give you an idea.

00:37:42.550 --> 00:37:46.150
I suggest that we make
corporate taxes progressive,

00:37:46.150 --> 00:37:51.420
based on how broadly distributed
a company's equity is.

00:37:51.420 --> 00:37:53.930
So the more stockholders
a company has-- which

00:37:53.930 --> 00:37:57.980
is suitably defined, and I
do so in the book-- the lower

00:37:57.980 --> 00:38:00.547
the tax rate that
that company has.

00:38:00.547 --> 00:38:02.630
Microsoft, you might not
know, has one of the most

00:38:02.630 --> 00:38:08.260
broadly distributed
stockholder bases in the world.

00:38:08.260 --> 00:38:08.960
Bechtel?

00:38:08.960 --> 00:38:09.835
Anybody know Bechtel?

00:38:09.835 --> 00:38:11.790
Heard of Bechtel?

00:38:11.790 --> 00:38:14.860
I think it's still owned by one
family, it's a couple people.

00:38:14.860 --> 00:38:17.870
It's a huge corporation.

00:38:17.870 --> 00:38:21.707
Now, if we do
this, the companies

00:38:21.707 --> 00:38:23.540
that have a lower tax
rate have an advantage

00:38:23.540 --> 00:38:24.380
in the marketplace.

00:38:24.380 --> 00:38:25.963
And believe me, these
people are going

00:38:25.963 --> 00:38:29.950
to figure out how to distribute
the wealth more widely in order

00:38:29.950 --> 00:38:33.315
to maximize the return
for the corporation.

00:38:33.315 --> 00:38:34.690
There's another
talk I can give--

00:38:34.690 --> 00:38:37.550
we're going to run out of time
here-- it's very interesting,

00:38:37.550 --> 00:38:40.080
but we used to give away assets
in the United States, when

00:38:40.080 --> 00:38:41.500
we were an agrarian economy.

00:38:41.500 --> 00:38:44.080
In an agrarian economy,
assets were land.

00:38:44.080 --> 00:38:47.150
And If you worked the
land for seven years,

00:38:47.150 --> 00:38:48.460
you got it for free.

00:38:48.460 --> 00:38:50.290
The government
literally gave it away.

00:38:50.290 --> 00:38:55.920
Now today, the total
value, by my research

00:38:55.920 --> 00:38:58.360
of all the real estate
in the United States,

00:38:58.360 --> 00:39:00.810
I think it's 17% of
our assets, mostly we

00:39:00.810 --> 00:39:02.350
have financial instruments.

00:39:02.350 --> 00:39:05.520
So the idea of
giving away money,

00:39:05.520 --> 00:39:08.050
suitably used for
productive purposes,

00:39:08.050 --> 00:39:09.770
is not at all
unreasonable and not

00:39:09.770 --> 00:39:14.240
without historical precedent.

00:39:14.240 --> 00:39:16.470
Thank you.

00:39:16.470 --> 00:39:18.970
So I believe that progressive
policies like this

00:39:18.970 --> 00:39:22.590
can promote our social
goals without stifling

00:39:22.590 --> 00:39:23.620
economic growth.

00:39:23.620 --> 00:39:27.030
We just have to get on with
it and stop believing the myth

00:39:27.030 --> 00:39:29.390
that unfettered capitalism
is the answer to the world's

00:39:29.390 --> 00:39:30.970
problems.

00:39:30.970 --> 00:39:31.900
OK.

00:39:31.900 --> 00:39:34.030
So let me wrap up.

00:39:34.030 --> 00:39:36.690
I don't want you to
think that I'm anti-AI.

00:39:36.690 --> 00:39:38.790
Nothing is further
from the truth.

00:39:38.790 --> 00:39:43.010
I think its potential impact
on the world is similar--

00:39:43.010 --> 00:39:48.250
and I'm not exaggerating-- to
the invention of the wheel,

00:39:48.250 --> 00:39:51.140
possibly to the steam engine.

00:39:51.140 --> 00:39:53.350
But when you think of
it not of some kind

00:39:53.350 --> 00:39:56.340
of magical discontinuity in the
development of intelligent life

00:39:56.340 --> 00:39:59.670
on earth, but rather as
a powerful collection

00:39:59.670 --> 00:40:02.350
of automation tools
with the potential

00:40:02.350 --> 00:40:09.640
to transform our livelihoods and
to vastly increase our wealth.

00:40:09.640 --> 00:40:12.090
The challenge we face is that
our existing institutions,

00:40:12.090 --> 00:40:13.590
without some
enlightened rethinking,

00:40:13.590 --> 00:40:18.570
run a serious risk of making a
mess out of this opportunity.

00:40:18.570 --> 00:40:22.320
I'm supremely confident that
our future is very bright, more

00:40:22.320 --> 00:40:25.970
Star Trek than Terminator.

00:40:25.970 --> 00:40:28.980
But the transition may
be protracted and brutal

00:40:28.980 --> 00:40:33.360
unless we pay attention to the
issues that I've raised today.

00:40:33.360 --> 00:40:34.900
We have to find
new and better ways

00:40:34.900 --> 00:40:38.510
to ensure that our economy
doesn't just motor on,

00:40:38.510 --> 00:40:40.800
going faster and
faster while throwing

00:40:40.800 --> 00:40:44.850
evermore people overboard
to increase its speed.

00:40:44.850 --> 00:40:50.400
Our technology and our
economy should serve us, not

00:40:50.400 --> 00:40:53.067
the other way around.

00:40:53.067 --> 00:40:53.900
Thank you very much.

00:41:00.382 --> 00:41:01.090
MALE SPEAKER: OK.

00:41:01.090 --> 00:41:02.756
Well, thank you very
much for that talk.

00:41:02.756 --> 00:41:06.140
I have to, say I agreed with
almost everything you said.

00:41:06.140 --> 00:41:08.234
I'm not sure I'm such
a good discusser.

00:41:08.234 --> 00:41:10.150
Maybe we should-- Let's
take a question or two

00:41:10.150 --> 00:41:11.979
from the audience.

00:41:11.979 --> 00:41:12.520
AUDIENCE: Hi.

00:41:12.520 --> 00:41:14.685
You mentioned job mortgages,
and the first thing

00:41:14.685 --> 00:41:17.180
that comes to my mind
is indentured servitude,

00:41:17.180 --> 00:41:18.410
when I think of that.

00:41:18.410 --> 00:41:22.160
So how do you draw the line,
necessarily, between the two?

00:41:22.160 --> 00:41:24.930
JERRY KAPLAN: Well, the word,
mortgage, little known fact,

00:41:24.930 --> 00:41:28.535
comes from French, it
means pay until death.

00:41:28.535 --> 00:41:29.880
Mortgage, that's what it means.

00:41:29.880 --> 00:41:31.770
I'm serious about that.

00:41:31.770 --> 00:41:36.200
The answer of course is
if you take out a loan,

00:41:36.200 --> 00:41:37.620
you have to pay it back.

00:41:37.620 --> 00:41:40.379
If we simply tie the loan
to your future income

00:41:40.379 --> 00:41:42.920
so that we limit it-- And there
is a lot of activity going on

00:41:42.920 --> 00:41:45.419
for student loans today, where
the US government has a bunch

00:41:45.419 --> 00:41:48.050
programs where you
can defer payments,

00:41:48.050 --> 00:41:50.640
depends on how much you make.

00:41:50.640 --> 00:41:54.860
Then I think we can do
it without it becoming

00:41:54.860 --> 00:41:58.289
indentured servitude as such.

00:41:58.289 --> 00:41:59.830
It's just better
than what we've got.

00:41:59.830 --> 00:42:03.850
Which is, you work, and
you've got to pay the man,

00:42:03.850 --> 00:42:07.390
regardless of whether you
have the income to do it.

00:42:07.390 --> 00:42:11.020
MALE SPEAKER: They did do
something like this at Yale

00:42:11.020 --> 00:42:14.980
20 or 25 years ago, where
you could take out a loan

00:42:14.980 --> 00:42:18.220
and pay back in equity,
so I would pay back

00:42:18.220 --> 00:42:22.370
5% of my adjusted gross income,
or you could take out the loan

00:42:22.370 --> 00:42:23.580
and pay back a fixed amount.

00:42:23.580 --> 00:42:25.825
So you've got a debt
or equity choice.

00:42:25.825 --> 00:42:28.117
The trouble is, there was
a serious adverse selection

00:42:28.117 --> 00:42:30.700
problem that all the people that
are going to high income jobs

00:42:30.700 --> 00:42:34.160
took the debt contract, and all
the people that are going into,

00:42:34.160 --> 00:42:38.210
let's say humanities or
less well paying paths

00:42:38.210 --> 00:42:38.950
took the equity.

00:42:38.950 --> 00:42:41.077
So they were
certainly sharp enough

00:42:41.077 --> 00:42:42.160
to recognize a difference.

00:42:42.160 --> 00:42:44.770
So it is a little
tricky when you design

00:42:44.770 --> 00:42:46.630
these financial contracts.

00:42:46.630 --> 00:42:49.530
And for some reason, we made the
student loans non-dischargeable

00:42:49.530 --> 00:42:51.870
in bankruptcy for the
same sort of reason.

00:42:51.870 --> 00:42:54.700
Because the smart thing to
do would be to graduate,

00:42:54.700 --> 00:42:57.620
immediately go bankrupt
because you have no income,

00:42:57.620 --> 00:43:02.390
then you remove that obligation
and go out and get a job.

00:43:02.390 --> 00:43:05.800
So it's tricky because of people
responding to the incentives

00:43:05.800 --> 00:43:08.577
you said in terms of
the financial contract.

00:43:08.577 --> 00:43:09.910
JERRY KAPLAN: I agree with that.

00:43:09.910 --> 00:43:11.300
The thing to bear
in mind though,

00:43:11.300 --> 00:43:13.900
is I think these are
all solvable problems.

00:43:13.900 --> 00:43:16.650
They're just issues that need
to be addressed. in some way.

00:43:16.650 --> 00:43:19.420
Mortgages, as you probably
know, are non-recourse loans,

00:43:19.420 --> 00:43:20.460
and that was the key.

00:43:20.460 --> 00:43:21.670
It was tied to property.

00:43:21.670 --> 00:43:23.090
Now somebody who
is going to loan

00:43:23.090 --> 00:43:24.673
you the money for
your house, now they

00:43:24.673 --> 00:43:26.670
care about what
that house is worth.

00:43:26.670 --> 00:43:28.150
If it was the other
way around, you

00:43:28.150 --> 00:43:29.230
could buy any house you want.

00:43:29.230 --> 00:43:31.522
I don't care what it's worth,
we'll loan you the money,

00:43:31.522 --> 00:43:32.480
and you're on the hook.

00:43:32.480 --> 00:43:33.970
It would be a very
different world.

00:43:33.970 --> 00:43:35.820
And there are many
steps in between.

00:43:35.820 --> 00:43:37.940
MALE SPEAKER: Well, I
think part of the problem

00:43:37.940 --> 00:43:41.050
is that you don't have
the same due diligence

00:43:41.050 --> 00:43:44.250
standards for handing out
education loans as you

00:43:44.250 --> 00:43:45.110
do for mortgages.

00:43:45.110 --> 00:43:47.790
There's a serious
question of whether you're

00:43:47.790 --> 00:43:51.780
giving these loans in
cases where they probably

00:43:51.780 --> 00:43:53.602
could be forecast,
they wouldn't pay off.

00:43:53.602 --> 00:43:55.810
And it's very hard to do
that when they're government

00:43:55.810 --> 00:43:57.250
subsidized because
then you're saying,

00:43:57.250 --> 00:43:59.791
oh, you're treating these people
different than those people,

00:43:59.791 --> 00:44:00.770
and so on.

00:44:00.770 --> 00:44:04.230
There was actually just a
piece in either The Wall Street

00:44:04.230 --> 00:44:06.410
Journal or The New York
Times in the last week

00:44:06.410 --> 00:44:09.320
about that exact
topic, that maybe we

00:44:09.320 --> 00:44:13.310
should use more diligence
in awarding the student

00:44:13.310 --> 00:44:14.360
loans in the first place.

00:44:14.360 --> 00:44:15.370
And of course, that's
the way it used

00:44:15.370 --> 00:44:16.990
to be when I was in college.

00:44:16.990 --> 00:44:19.790
I got loans from the
University, but the university

00:44:19.790 --> 00:44:23.780
knew I was a pretty good bet.

00:44:23.780 --> 00:44:26.430
Other questions?

00:44:26.430 --> 00:44:28.125
Here, [INAUDIBLE], over here.

00:44:28.125 --> 00:44:29.750
AUDIENCE: All right,
thanks for coming.

00:44:29.750 --> 00:44:31.890
You were pointing the
finger at the general public

00:44:31.890 --> 00:44:34.980
for not understanding
the limitations

00:44:34.980 --> 00:44:36.860
of artificial
intelligence, but I'm not

00:44:36.860 --> 00:44:38.570
so sure it's just
the general public.

00:44:38.570 --> 00:44:42.000
I think this idea
might be unwelcome even

00:44:42.000 --> 00:44:44.040
among people who work
in software development,

00:44:44.040 --> 00:44:45.560
or work in artificial
intelligence.

00:44:45.560 --> 00:44:47.910
My question for you
is, how unpopular

00:44:47.910 --> 00:44:51.550
is this message, even
for people who work

00:44:51.550 --> 00:44:53.330
in artificial intelligence?

00:44:53.330 --> 00:44:56.587
JERRY KAPLAN:
Well, I'm not sure.

00:44:56.587 --> 00:44:58.420
I mean, I've only given
the talk or versions

00:44:58.420 --> 00:45:00.070
of this a few times.

00:45:00.070 --> 00:45:02.275
And usually what happened,
it happened up at Google

00:45:02.275 --> 00:45:05.740
in Kirkland, I had about
1/3 of the audience going,

00:45:05.740 --> 00:45:09.076
yeah, that's right!

00:45:09.076 --> 00:45:13.160
This guy's Donald Trump,
he's telling the truth.

00:45:13.160 --> 00:45:15.740
And you know, other people
are just kind of, hmmm.

00:45:15.740 --> 00:45:18.350
If somebody comes
in and says, you

00:45:18.350 --> 00:45:20.890
are working on the most
important thing in the world.

00:45:20.890 --> 00:45:24.370
This is the future of evolution.

00:45:24.370 --> 00:45:25.780
You are going to build machines.

00:45:25.780 --> 00:45:27.113
We're going to upload our minds.

00:45:29.890 --> 00:45:32.490
There's a wonderful book called.

00:45:32.490 --> 00:45:33.660
"Apocalyptic AI."

00:45:33.660 --> 00:45:37.330
It is fantastic, and
nobody has read it,

00:45:37.330 --> 00:45:39.420
by a religious
historian, talking

00:45:39.420 --> 00:45:42.240
about the history
of apocalyptic ideas

00:45:42.240 --> 00:45:44.220
in Judeo-Christian societies.

00:45:44.220 --> 00:45:46.340
And then, he maps
that onto this.

00:45:46.340 --> 00:45:49.030
He went to Carnegie Mellon,
he interviewed everybody.

00:45:49.030 --> 00:45:54.680
And I mean, I'm arguing against
the soothsayer who promises

00:45:54.680 --> 00:45:57.140
eternal life and salvation.

00:45:57.140 --> 00:45:58.980
I'm not expecting
to get very far.

00:45:58.980 --> 00:46:02.040
And I say, that's nonsense,
you're going to die.

00:46:02.040 --> 00:46:05.160
But I think that a sensible
person, particularly

00:46:05.160 --> 00:46:08.050
those working in the field, take
a look at what you're doing.

00:46:08.050 --> 00:46:09.480
Think about what you're doing.

00:46:09.480 --> 00:46:11.480
And do you really feel,
just because there's

00:46:11.480 --> 00:46:14.310
some guy telling
you, this is the most

00:46:14.310 --> 00:46:19.280
important thing since sliced
bread, do you believe that?

00:46:19.280 --> 00:46:22.250
Or is it just dressing up?

00:46:22.250 --> 00:46:24.430
And I think it's
just dressing up.

00:46:24.430 --> 00:46:27.450
If they hadn't called-- I've had
to take this out of the talk.

00:46:27.450 --> 00:46:30.240
The term artificial intelligence
is what caused the problem.

00:46:30.240 --> 00:46:32.940
It started in 1958,
by John McCarthy.

00:46:32.940 --> 00:46:34.790
Any of you guys
know John McCarthy?

00:46:34.790 --> 00:46:36.140
Know who he is at least?

00:46:36.140 --> 00:46:36.930
OK.

00:46:36.930 --> 00:46:38.780
I did that at Stanford.

00:46:38.780 --> 00:46:41.196
Only like 1/3 of the graduate
students in Computer Science

00:46:41.196 --> 00:46:43.030
knew who John McCarthy was.

00:46:43.030 --> 00:46:44.930
John McCarthy invented the term.

00:46:44.930 --> 00:46:48.510
And he did it as a reaction
against Norbert Wiener,

00:46:48.510 --> 00:46:49.649
who had cybernetics.

00:46:49.649 --> 00:46:51.690
And he didn't want it to
be known as cybernetics,

00:46:51.690 --> 00:46:52.997
so he came up with the term.

00:46:52.997 --> 00:46:54.330
Now John was an interesting guy.

00:46:54.330 --> 00:46:56.510
He was a mathematician,
but he had

00:46:56.510 --> 00:46:59.320
the greatest inadvertent
marketing coup in history

00:46:59.320 --> 00:47:00.710
by coming up with that term.

00:47:00.710 --> 00:47:02.520
It has driven all this media.

00:47:02.520 --> 00:47:06.240
All this-- If he had just called
it logic programming, which

00:47:06.240 --> 00:47:07.980
is what he meant
by it, by the way,

00:47:07.980 --> 00:47:10.830
back then, which has been
discredited as a basis

00:47:10.830 --> 00:47:14.520
for artificial intelligence, we
wouldn't be in this room today.

00:47:14.520 --> 00:47:16.580
And I wouldn't be
spending my time

00:47:16.580 --> 00:47:18.472
trying to promote these ideas.

00:47:18.472 --> 00:47:19.930
MALE SPEAKER: So
one thing you said

00:47:19.930 --> 00:47:21.680
that I thought
was very important

00:47:21.680 --> 00:47:24.110
is this distinction
between jobs and tasks.

00:47:24.110 --> 00:47:26.150
And as you know, from
the Department of Labor,

00:47:26.150 --> 00:47:30.390
there is this massive listing
of job titles and tasks.

00:47:30.390 --> 00:47:32.660
I think your nurses example
was taken from that.

00:47:32.660 --> 00:47:34.500
You can look this up online.

00:47:34.500 --> 00:47:36.080
And so people have
gone through this.

00:47:36.080 --> 00:47:38.700
And people at Oxford
and actually, McKinsey

00:47:38.700 --> 00:47:40.620
is going through
it now, and trying

00:47:40.620 --> 00:47:45.310
to identify which
tasks are automatable.

00:47:45.310 --> 00:47:47.089
Now what happens,
of course, if you're

00:47:47.089 --> 00:47:49.630
in a job where there are only
a few automatable tasks, that's

00:47:49.630 --> 00:47:53.680
great for you because the
automation compliments

00:47:53.680 --> 00:47:54.440
your services.

00:47:54.440 --> 00:47:56.550
You can do more of
these other things

00:47:56.550 --> 00:47:59.522
and less of these
tedious, unpleasant tasks.

00:47:59.522 --> 00:48:01.480
But on the other hand,
if you're in a job where

00:48:01.480 --> 00:48:03.930
almost all your
tasks in that job

00:48:03.930 --> 00:48:07.690
are automatable, like
the bricklayer example,

00:48:07.690 --> 00:48:10.410
in fact, then, of course,
you're not such good shape.

00:48:10.410 --> 00:48:14.330
So the challenge is to look
both at the fraction of tasks

00:48:14.330 --> 00:48:16.570
or the number of tasks
in your job description

00:48:16.570 --> 00:48:17.870
that are automatable.

00:48:17.870 --> 00:48:22.470
And that's going to give you a
clue of whether the technology

00:48:22.470 --> 00:48:25.830
is going to function as
a complement to your job

00:48:25.830 --> 00:48:29.500
or as a substitute for it, So
McKinsey's working on this,

00:48:29.500 --> 00:48:31.310
and it'll be out in a few weeks.

00:48:31.310 --> 00:48:31.590
JERRY KAPLAN: Yeah.

00:48:31.590 --> 00:48:33.910
The McKinsey report sounds
like it's going to be great.

00:48:33.910 --> 00:48:35.210
I've seen some summaries of it.

00:48:35.210 --> 00:48:38.070
The Frey-Osborne study from
Oxford is terribly flawed.

00:48:38.070 --> 00:48:40.562
I actually had dinner
with them two weeks ago.

00:48:40.562 --> 00:48:41.770
MALE SPEAKER: I looked at it.

00:48:41.770 --> 00:48:43.630
It seemed to me it
was-- Their methodology

00:48:43.630 --> 00:48:45.790
wasn't very clearly described.

00:48:45.790 --> 00:48:47.650
And I think the
McKinsey guys, I'm

00:48:47.650 --> 00:48:49.730
urging them to make
their work reproducible,

00:48:49.730 --> 00:48:51.610
so that you could--
There could be

00:48:51.610 --> 00:48:55.292
other opinions offered on what
tasks are really automatable.

00:48:55.292 --> 00:48:56.750
JERRY KAPLAN: But
the great example

00:48:56.750 --> 00:48:59.336
of how science doesn't
proceed scientifically.

00:48:59.336 --> 00:49:00.460
Or this debate [INAUDIBLE].

00:49:00.460 --> 00:49:04.939
The Oxford study by
Frey-Osborne had a number in it,

00:49:04.939 --> 00:49:05.480
in the title.

00:49:05.480 --> 00:49:08.830
It said 47% of jobs will be
automatable in the next 20

00:49:08.830 --> 00:49:09.430
years.

00:49:09.430 --> 00:49:16.040
I mean, they're right or
wrong, that got total play

00:49:16.040 --> 00:49:17.035
from Oxford.

00:49:17.035 --> 00:49:21.130
So I mean, I have to agree
with Hal completely on this.

00:49:21.130 --> 00:49:22.396
It's about task automation.

00:49:22.396 --> 00:49:24.520
Now there are two things
that have happened in AI--

00:49:24.520 --> 00:49:26.150
I'm not going to
spend a lot of time

00:49:26.150 --> 00:49:28.390
on this-- they're
recent advances,

00:49:28.390 --> 00:49:30.742
so they really are going
to affect the tasks that

00:49:30.742 --> 00:49:31.450
can be automated.

00:49:31.450 --> 00:49:33.207
The first of them
is machine learning,

00:49:33.207 --> 00:49:34.790
which allows us to
take a certain kind

00:49:34.790 --> 00:49:37.370
of perceptual tasks, and
convert that information

00:49:37.370 --> 00:49:40.490
into actionable symbolic form.

00:49:40.490 --> 00:49:45.090
And the second is
basic robotics of being

00:49:45.090 --> 00:49:47.010
able to move things
around, but now

00:49:47.010 --> 00:49:49.377
to control that
using the information

00:49:49.377 --> 00:49:50.710
that we can get in from censors.

00:49:50.710 --> 00:49:53.350
So a great deal of
blue collar tasks

00:49:53.350 --> 00:49:54.610
are going to be automatable.

00:49:54.610 --> 00:49:55.860
That's what's going to happen.

00:49:55.860 --> 00:49:58.440
And a great deal of
intellectual tasks,

00:49:58.440 --> 00:50:01.210
like writing dumb
press releases,

00:50:01.210 --> 00:50:03.160
are going to be
automatable as well.

00:50:03.160 --> 00:50:05.840
So my basic point, which I
didn't really make in the talk

00:50:05.840 --> 00:50:07.220
is I think things are
going to accelerate

00:50:07.220 --> 00:50:08.803
over the next 20 or
30 years, and it's

00:50:08.803 --> 00:50:09.990
going to be a big problem.

00:50:09.990 --> 00:50:11.550
I'm not against doing it.

00:50:11.550 --> 00:50:12.140
It's great.

00:50:12.140 --> 00:50:13.930
Automation is good.

00:50:13.930 --> 00:50:16.260
It just has some
negative side effects

00:50:16.260 --> 00:50:18.169
and we need to deal with those.

00:50:18.169 --> 00:50:20.085
AUDIENCE: So we have
seen prominent academics,

00:50:20.085 --> 00:50:24.231
you know, Stephen Hawking,
Elon Musk, Bill Gates,

00:50:24.231 --> 00:50:25.980
speak out against
artificial intelligence.

00:50:25.980 --> 00:50:27.646
And I think they're
really speaking out,

00:50:27.646 --> 00:50:30.080
not against narrow intelligence,
things like Watson,

00:50:30.080 --> 00:50:34.850
but really general purpose
learning algorithms.

00:50:34.850 --> 00:50:37.310
Things like DeepMind
and Vicarious.

00:50:37.310 --> 00:50:40.287
What do you see to be their
greatest argument for being

00:50:40.287 --> 00:50:41.870
worried about
artificial intelligence,

00:50:41.870 --> 00:50:44.030
and how would you refute it?

00:50:44.030 --> 00:50:45.465
JERRY KAPLAN: Well,
the problem is

00:50:45.465 --> 00:50:46.840
what you're really
talking about,

00:50:46.840 --> 00:50:49.149
is a tweet from Stephen Hawking.

00:50:49.149 --> 00:50:50.940
It's just, this is the
problem with the way

00:50:50.940 --> 00:50:53.020
science and society proceeds.

00:50:53.020 --> 00:50:54.510
He's very prominent.

00:50:54.510 --> 00:50:55.290
He's well known.

00:50:55.290 --> 00:50:59.980
He puts out a tweet, you can
interpret it in multiple ways.

00:50:59.980 --> 00:51:03.430
I was at Oxford, where a
guy actually said to me--

00:51:03.430 --> 00:51:05.450
This is worth 15 seconds.

00:51:05.450 --> 00:51:08.070
It was worth the trip out there
just because a guy looked me

00:51:08.070 --> 00:51:10.720
in the eye and he said to me, I
wasn't going to be able to make

00:51:10.720 --> 00:51:15.237
your talk, but my dinner with
Stephen Hawking was cancelled.

00:51:15.237 --> 00:51:16.695
That was worth the
trip to England,

00:51:16.695 --> 00:51:18.930
to have somebody say that to me.

00:51:18.930 --> 00:51:23.002
I mean, my reaction is
great mind, great man.

00:51:23.002 --> 00:51:24.960
I'm not going to argue
with him about radiation

00:51:24.960 --> 00:51:26.824
from black holes.

00:51:26.824 --> 00:51:28.240
I don't think he
should be arguing

00:51:28.240 --> 00:51:31.340
with the people who are here,
doing artificial intelligence.

00:51:31.340 --> 00:51:34.070
They're hearing it from each
other, it's a big echo chamber.

00:51:34.070 --> 00:51:37.100
It's not that they don't
have something to say.

00:51:37.100 --> 00:51:39.880
And there is arguments
on the other side.

00:51:39.880 --> 00:51:43.230
But it's just-- I heard
somebody say this,

00:51:43.230 --> 00:51:45.060
worrying about
superintelligence, worrying

00:51:45.060 --> 00:51:47.630
about the threat of
AI, it's like worrying

00:51:47.630 --> 00:51:50.380
about overpopulation on Mars.

00:51:50.380 --> 00:51:54.360
We're just about maybe going to
be able to get a man on Mars,

00:51:54.360 --> 00:51:56.100
despite the film.

00:51:56.100 --> 00:52:00.230
And why are we
worrying about this?

00:52:00.230 --> 00:52:01.980
It's sucking the oxygen
out of the debates

00:52:01.980 --> 00:52:04.954
that we really need to be having
about what's really going on

00:52:04.954 --> 00:52:06.370
and what kind of
societal problems

00:52:06.370 --> 00:52:08.509
these things are going to
cause in your lifetime.

00:52:08.509 --> 00:52:10.800
And we shouldn't be spending
the time and energy on it.

00:52:10.800 --> 00:52:12.680
I'm fine with academics
going to conferences

00:52:12.680 --> 00:52:14.740
and having talks about this.

00:52:14.740 --> 00:52:18.280
But we got to get the, gee wiz,
out of artificial intelligence.

00:52:18.280 --> 00:52:21.970
When it starts to
show up, that's great.

00:52:21.970 --> 00:52:25.244
MALE SPEAKER: I would
say I differ slightly

00:52:25.244 --> 00:52:26.660
because I don't
think it's so much

00:52:26.660 --> 00:52:28.360
the artificial
intelligence component,

00:52:28.360 --> 00:52:32.550
but there are
technologies that have,

00:52:32.550 --> 00:52:36.620
let's say military use or
offensive use, that of course,

00:52:36.620 --> 00:52:38.790
will be more broadly available.

00:52:38.790 --> 00:52:41.690
Just like mobile phones
are more broadly available,

00:52:41.690 --> 00:52:45.190
drones that deliver
bombs will be available.

00:52:45.190 --> 00:52:47.890
And so there's
questions, that would

00:52:47.890 --> 00:52:53.170
be the threat the most worries
me, of kind of a homemade,

00:52:53.170 --> 00:52:58.210
small scale attacks of one sort
or another using technology.

00:52:58.210 --> 00:52:59.940
I don't know that
there's a lot we

00:52:59.940 --> 00:53:03.350
can do about that,
except build better

00:53:03.350 --> 00:53:05.480
defenses against such things.

00:53:05.480 --> 00:53:06.930
But I think that is a concern.

00:53:06.930 --> 00:53:10.880
Technology makes
people more powerful,

00:53:10.880 --> 00:53:12.820
and the people it
makes more powerful

00:53:12.820 --> 00:53:16.020
may be good guys or bad guys.

00:53:16.020 --> 00:53:17.479
So it's going to escalate.

00:53:17.479 --> 00:53:18.270
JERRY KAPLAN: Yeah.

00:53:18.270 --> 00:53:19.580
You know Stewart Russell?

00:53:19.580 --> 00:53:20.549
MALE SPEAKER: Yeah.

00:53:20.549 --> 00:53:22.590
JERRY KAPLAN: Now, I don't
always agree with him,

00:53:22.590 --> 00:53:24.947
but at least he's trying.

00:53:24.947 --> 00:53:26.030
[INAUDIBLE] point of view.

00:53:26.030 --> 00:53:27.460
And this goes back
to your question.

00:53:27.460 --> 00:53:29.030
If what you mean
by AI is dangerous,

00:53:29.030 --> 00:53:31.405
is that robots are going to
become sentient and take over

00:53:31.405 --> 00:53:33.250
the world, forget it.

00:53:33.250 --> 00:53:34.440
Not relevant at all.

00:53:34.440 --> 00:53:38.460
But that's not to say that
technology cannot be dangerous

00:53:38.460 --> 00:53:41.010
and represent real
threats to real people.

00:53:41.010 --> 00:53:44.570
And this is very scary because
what's happening-- and Stewart,

00:53:44.570 --> 00:53:47.650
I think when he's
giving in depth talks

00:53:47.650 --> 00:53:50.100
rather than saying the robots
are coming-- what he talks

00:53:50.100 --> 00:53:55.470
about is that the price of the--
The access and price of some

00:53:55.470 --> 00:54:00.370
of these technologies that can
have mass destructive power,

00:54:00.370 --> 00:54:04.540
it used to be, or is now,
confined to very big, hopefully

00:54:04.540 --> 00:54:07.040
responsible, players
like the United States.

00:54:07.040 --> 00:54:09.390
But that may not be
the case in 10 years.

00:54:09.390 --> 00:54:11.170
You know, you could
build little drones

00:54:11.170 --> 00:54:15.272
that will run around, you know,
recognize people, and inject

00:54:15.272 --> 00:54:17.230
some poison in their eye
when they're sleeping.

00:54:17.230 --> 00:54:20.250
I mean, that's the kind of
crazy stuff that can happen.

00:54:20.250 --> 00:54:25.080
And if some despot
or crazy whack

00:54:25.080 --> 00:54:27.400
terrorist types get a
hold of that technology,

00:54:27.400 --> 00:54:28.525
it could be very dangerous.

00:54:28.525 --> 00:54:30.170
So there's a huge
debate going on.

00:54:30.170 --> 00:54:31.960
MALE SPEAKER: If you can
deliver a pizza by drones,

00:54:31.960 --> 00:54:33.330
you can deliver bombs by drones.

00:54:33.330 --> 00:54:34.246
JERRY KAPLAN: Exactly.

00:54:34.246 --> 00:54:35.530
That's exactly right.

00:54:35.530 --> 00:54:38.760
And there's a very big debate
going on inside the military.

00:54:38.760 --> 00:54:40.980
And at the UN there's a
sanding counsel on this.

00:54:40.980 --> 00:54:45.510
And a lot of philosophers
and technology experts.

00:54:45.510 --> 00:54:47.950
That sort of-- They're really
trying to grapple with this.

00:54:47.950 --> 00:54:49.910
But it's like the
biotechnology thing,

00:54:49.910 --> 00:54:51.540
I would use that as an analogy.

00:54:51.540 --> 00:54:52.690
It's very dangerous.

00:54:52.690 --> 00:54:54.760
Much more dangerous
than the AI stuff.

00:54:54.760 --> 00:54:57.870
You guys have probably
heard of CRISPR, Cas9 stuff?

00:54:57.870 --> 00:55:02.440
I mean, they're trying
to deal with that.

00:55:02.440 --> 00:55:03.790
What are the ethics of this?

00:55:03.790 --> 00:55:04.580
What's OK to do?

00:55:04.580 --> 00:55:05.830
How much development?

00:55:05.830 --> 00:55:07.092
What kind of labs do we need?

00:55:07.092 --> 00:55:09.550
We'll have to do the same thing
in artificial intelligence.

00:55:09.550 --> 00:55:13.740
AUDIENCE: So I guess the
first thing is that I never

00:55:13.740 --> 00:55:17.110
really understood
why people always

00:55:17.110 --> 00:55:21.920
treat technological advancement
that improves productivity

00:55:21.920 --> 00:55:25.290
as something that
threatens jobs.

00:55:25.290 --> 00:55:30.080
Because if someone can
produce more of something

00:55:30.080 --> 00:55:33.500
in the same time, it
basically could also

00:55:33.500 --> 00:55:36.750
mean that we can all have
more of that same thing,

00:55:36.750 --> 00:55:39.560
and everybody would
be better off.

00:55:39.560 --> 00:55:42.545
And then my second
point, of my question,

00:55:42.545 --> 00:55:50.910
I guess, is do you think
that the increase in wealth

00:55:50.910 --> 00:55:57.070
disparity is related to the
influence of money on politics,

00:55:57.070 --> 00:56:01.420
and how the accumulation
of wealth, which could then

00:56:01.420 --> 00:56:06.280
be used to impact politics, is
itself an exacerbating cycle,

00:56:06.280 --> 00:56:08.780
and that therefore,
the solution would

00:56:08.780 --> 00:56:13.970
be to kind of
crowd fund politics

00:56:13.970 --> 00:56:17.472
in some sense to
democratize it again?

00:56:17.472 --> 00:56:20.050
JERRY KAPLAN: Well, I'm sure
Hal could probably do a better

00:56:20.050 --> 00:56:26.300
job than I can, but I'm the
speaker, so on the first point

00:56:26.300 --> 00:56:28.734
was relating to?

00:56:28.734 --> 00:56:30.400
I can only remember
one thing at a time.

00:56:30.400 --> 00:56:31.330
Productivity, yeah.

00:56:31.330 --> 00:56:33.480
It's a complicated equation.

00:56:33.480 --> 00:56:35.470
The automation makes
people more productive,

00:56:35.470 --> 00:56:37.330
that means you
need fewer of them.

00:56:37.330 --> 00:56:40.440
But it takes a certain amount
of time, it's like a cycle.

00:56:40.440 --> 00:56:42.240
It's estimated at
10 to 15 years.

00:56:42.240 --> 00:56:46.150
There's process re-engineering,
and the money, the new wealth

00:56:46.150 --> 00:56:47.790
that is created goes
out into society,

00:56:47.790 --> 00:56:52.150
and society then
creates new jobs, often

00:56:52.150 --> 00:56:53.440
in different places.

00:56:53.440 --> 00:56:55.267
So it's the lag that
creates the problem.

00:56:55.267 --> 00:56:57.100
It puts people out of
work in the short run.

00:56:57.100 --> 00:56:58.724
In the long run, it
creates more wealth

00:56:58.724 --> 00:57:01.380
and generates a
lot more new jobs.

00:57:01.380 --> 00:57:07.900
Your second point, sorry, didn't
get what-- Money in politics?

00:57:07.900 --> 00:57:08.620
My opinion?

00:57:08.620 --> 00:57:09.460
Absolutely.

00:57:09.460 --> 00:57:11.020
It's completely ridiculous.

00:57:11.020 --> 00:57:14.660
People look at Iran, and
say why this is terrible,

00:57:14.660 --> 00:57:19.440
the Ayatollah has to approve
everybody who runs for office,

00:57:19.440 --> 00:57:21.730
why would we have
a system like that?

00:57:21.730 --> 00:57:23.340
Well, we have the
same system here.

00:57:23.340 --> 00:57:26.820
If you can't get a very small
group of very wealthy people--

00:57:26.820 --> 00:57:30.860
it's like 150 families, I think,
was in the New York Times--

00:57:30.860 --> 00:57:33.390
to support you, you
can't run for office.

00:57:33.390 --> 00:57:38.480
So that's a gate that's very
sadly has been put in place.

00:57:38.480 --> 00:57:40.930
And my opinion is, you
call it crowd-funding,

00:57:40.930 --> 00:57:42.400
but they used have
a check off box,

00:57:42.400 --> 00:57:43.830
I don't even know if
they still have that

00:57:43.830 --> 00:57:45.171
for presidential campaigns.

00:57:45.171 --> 00:57:45.670
Absolutely.

00:57:45.670 --> 00:57:46.694
We need to get the
money, you don't

00:57:46.694 --> 00:57:48.152
have to get it out
of politics, you

00:57:48.152 --> 00:57:51.340
have to have it
reduce its influence.

00:57:51.340 --> 00:57:55.530
And it's an obvious reason
why the people who are getting

00:57:55.530 --> 00:58:01.810
elected take the point of
view to do the things that

00:58:01.810 --> 00:58:04.144
benefit the wealthy.

00:58:04.144 --> 00:58:06.060
MALE SPEAKER: Let's thank
you again for coming

00:58:06.060 --> 00:58:09.110
and we'll be signing
books in the back.

