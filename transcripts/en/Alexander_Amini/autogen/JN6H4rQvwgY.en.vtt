WEBVTT
Kind: captions
Language: en

00:00:02.040 --> 00:00:04.240
good morning everyone thank you for

00:00:04.240 --> 00:00:04.250
good morning everyone thank you for
 

00:00:04.250 --> 00:00:06.320
good morning everyone thank you for
thank you all for joining us

00:00:06.320 --> 00:00:06.330
thank you all for joining us
 

00:00:06.330 --> 00:00:10.879
thank you all for joining us
this is MIT six s-191 and we'd like to

00:00:10.879 --> 00:00:10.889
this is MIT six s-191 and we'd like to
 

00:00:10.889 --> 00:00:13.070
this is MIT six s-191 and we'd like to
welcome to welcome you to this course on

00:00:13.070 --> 00:00:13.080
welcome to welcome you to this course on
 

00:00:13.080 --> 00:00:15.740
welcome to welcome you to this course on
introduction to deep learning so in this

00:00:15.740 --> 00:00:15.750
introduction to deep learning so in this
 

00:00:15.750 --> 00:00:18.439
introduction to deep learning so in this
course you'll learn how to build

00:00:18.439 --> 00:00:18.449
course you'll learn how to build
 

00:00:18.449 --> 00:00:20.779
course you'll learn how to build
remarkable algorithms intelligent

00:00:20.779 --> 00:00:20.789
remarkable algorithms intelligent
 

00:00:20.789 --> 00:00:23.630
remarkable algorithms intelligent
algorithms capable of solving very

00:00:23.630 --> 00:00:23.640
algorithms capable of solving very
 

00:00:23.640 --> 00:00:25.999
algorithms capable of solving very
complex problems that just a decade ago

00:00:25.999 --> 00:00:26.009
complex problems that just a decade ago
 

00:00:26.009 --> 00:00:29.560
complex problems that just a decade ago
were not even feasible to solve and

00:00:29.560 --> 00:00:29.570
were not even feasible to solve and
 

00:00:29.570 --> 00:00:32.540
were not even feasible to solve and
let's just start with this notion of

00:00:32.540 --> 00:00:32.550
let's just start with this notion of
 

00:00:32.550 --> 00:00:36.100
let's just start with this notion of
intelligence so at a very high level

00:00:36.100 --> 00:00:36.110
intelligence so at a very high level
 

00:00:36.110 --> 00:00:38.720
intelligence so at a very high level
intelligence is the ability to process

00:00:38.720 --> 00:00:38.730
intelligence is the ability to process
 

00:00:38.730 --> 00:00:42.740
intelligence is the ability to process
information so that it can be used to

00:00:42.740 --> 00:00:42.750
information so that it can be used to
 

00:00:42.750 --> 00:00:45.069
information so that it can be used to
inform future predictions and decisions

00:00:45.069 --> 00:00:45.079
inform future predictions and decisions
 

00:00:45.079 --> 00:00:48.250
inform future predictions and decisions
now when this intelligence is not

00:00:48.250 --> 00:00:48.260
now when this intelligence is not
 

00:00:48.260 --> 00:00:51.849
now when this intelligence is not
engineered but rather a biological

00:00:51.849 --> 00:00:51.859
engineered but rather a biological
 

00:00:51.859 --> 00:00:55.910
engineered but rather a biological
inspiration such as in humans it's

00:00:55.910 --> 00:00:55.920
inspiration such as in humans it's
 

00:00:55.920 --> 00:00:57.830
inspiration such as in humans it's
called human intelligence but when it's

00:00:57.830 --> 00:00:57.840
called human intelligence but when it's
 

00:00:57.840 --> 00:00:59.660
called human intelligence but when it's
engineered we refer to it as artificial

00:00:59.660 --> 00:00:59.670
engineered we refer to it as artificial
 

00:00:59.670 --> 00:01:02.240
engineered we refer to it as artificial
intelligence so this course is a course

00:01:02.240 --> 00:01:02.250
intelligence so this course is a course
 

00:01:02.250 --> 00:01:04.009
intelligence so this course is a course
on deep learning which is just a subset

00:01:04.009 --> 00:01:04.019
on deep learning which is just a subset
 

00:01:04.019 --> 00:01:07.160
on deep learning which is just a subset
of artificial intelligence and really

00:01:07.160 --> 00:01:07.170
of artificial intelligence and really
 

00:01:07.170 --> 00:01:08.720
of artificial intelligence and really
it's just a subset of machine learning

00:01:08.720 --> 00:01:08.730
it's just a subset of machine learning
 

00:01:08.730 --> 00:01:10.840
it's just a subset of machine learning
which involves more traditional methods

00:01:10.840 --> 00:01:10.850
which involves more traditional methods
 

00:01:10.850 --> 00:01:15.440
which involves more traditional methods
where we tried to learn representations

00:01:15.440 --> 00:01:15.450
where we tried to learn representations
 

00:01:15.450 --> 00:01:17.240
where we tried to learn representations
directly from data and we'll talk about

00:01:17.240 --> 00:01:17.250
directly from data and we'll talk about
 

00:01:17.250 --> 00:01:20.600
directly from data and we'll talk about
this more in detail later today but let

00:01:20.600 --> 00:01:20.610
this more in detail later today but let
 

00:01:20.610 --> 00:01:22.160
this more in detail later today but let
me first just start by talking about

00:01:22.160 --> 00:01:22.170
me first just start by talking about
 

00:01:22.170 --> 00:01:24.530
me first just start by talking about
some of the amazing successes that deep

00:01:24.530 --> 00:01:24.540
some of the amazing successes that deep
 

00:01:24.540 --> 00:01:28.910
some of the amazing successes that deep
learning has had in the past so in 2012

00:01:28.910 --> 00:01:28.920
learning has had in the past so in 2012
 

00:01:28.920 --> 00:01:31.280
learning has had in the past so in 2012
this competition called imagenet came

00:01:31.280 --> 00:01:31.290
this competition called imagenet came
 

00:01:31.290 --> 00:01:34.250
this competition called imagenet came
out which tasked AI researchers to build

00:01:34.250 --> 00:01:34.260
out which tasked AI researchers to build
 

00:01:34.260 --> 00:01:37.180
out which tasked AI researchers to build
an AI system capable of recognizing

00:01:37.180 --> 00:01:37.190
an AI system capable of recognizing
 

00:01:37.190 --> 00:01:40.490
an AI system capable of recognizing
images objects in images and there was

00:01:40.490 --> 00:01:40.500
images objects in images and there was
 

00:01:40.500 --> 00:01:42.980
images objects in images and there was
millions of examples in this data set

00:01:42.980 --> 00:01:42.990
millions of examples in this data set
 

00:01:42.990 --> 00:01:46.100
millions of examples in this data set
and the winner in 2012 for the first

00:01:46.100 --> 00:01:46.110
and the winner in 2012 for the first
 

00:01:46.110 --> 00:01:47.780
and the winner in 2012 for the first
time ever was a deep learning based

00:01:47.780 --> 00:01:47.790
time ever was a deep learning based
 

00:01:47.790 --> 00:01:49.700
time ever was a deep learning based
system and a when it came out it

00:01:49.700 --> 00:01:49.710
system and a when it came out it
 

00:01:49.710 --> 00:01:51.530
system and a when it came out it
absolutely shattered all other

00:01:51.530 --> 00:01:51.540
absolutely shattered all other
 

00:01:51.540 --> 00:01:54.370
absolutely shattered all other
competitors and crushed the competition

00:01:54.370 --> 00:01:54.380
competitors and crushed the competition
 

00:01:54.380 --> 00:01:56.860
competitors and crushed the competition
across the country crush the challenge

00:01:56.860 --> 00:01:56.870
across the country crush the challenge
 

00:01:56.870 --> 00:02:00.020
across the country crush the challenge
and today these deep learning based

00:02:00.020 --> 00:02:00.030
and today these deep learning based
 

00:02:00.030 --> 00:02:01.970
and today these deep learning based
systems have actually surpassed human

00:02:01.970 --> 00:02:01.980
systems have actually surpassed human
 

00:02:01.980 --> 00:02:04.160
systems have actually surpassed human
level accuracy on the image net

00:02:04.160 --> 00:02:04.170
level accuracy on the image net
 

00:02:04.170 --> 00:02:06.230
level accuracy on the image net
challenge and can actually recognize

00:02:06.230 --> 00:02:06.240
challenge and can actually recognize
 

00:02:06.240 --> 00:02:10.789
challenge and can actually recognize
images even better than humans can now

00:02:10.789 --> 00:02:10.799
images even better than humans can now
 

00:02:10.799 --> 00:02:12.170
images even better than humans can now
in this class you'll actually learn how

00:02:12.170 --> 00:02:12.180
in this class you'll actually learn how
 

00:02:12.180 --> 00:02:14.839
in this class you'll actually learn how
to build complex vision systems building

00:02:14.839 --> 00:02:14.849
to build complex vision systems building
 

00:02:14.849 --> 00:02:15.710
to build complex vision systems building
a computer that

00:02:15.710 --> 00:02:15.720
a computer that
 

00:02:15.720 --> 00:02:18.440
a computer that
how to see and just tomorrow you'll

00:02:18.440 --> 00:02:18.450
how to see and just tomorrow you'll
 

00:02:18.450 --> 00:02:20.540
how to see and just tomorrow you'll
learn how to build an algorithm that

00:02:20.540 --> 00:02:20.550
learn how to build an algorithm that
 

00:02:20.550 --> 00:02:24.620
learn how to build an algorithm that
will take as input x-ray images and as

00:02:24.620 --> 00:02:24.630
will take as input x-ray images and as
 

00:02:24.630 --> 00:02:27.080
will take as input x-ray images and as
output it will detect if that person has

00:02:27.080 --> 00:02:27.090
output it will detect if that person has
 

00:02:27.090 --> 00:02:29.840
output it will detect if that person has
a pneumothorax just from that single

00:02:29.840 --> 00:02:29.850
a pneumothorax just from that single
 

00:02:29.850 --> 00:02:34.250
a pneumothorax just from that single
input image you'll even make the network

00:02:34.250 --> 00:02:34.260
input image you'll even make the network
 

00:02:34.260 --> 00:02:36.890
input image you'll even make the network
explain to you why it decided to

00:02:36.890 --> 00:02:36.900
explain to you why it decided to
 

00:02:36.900 --> 00:02:39.560
explain to you why it decided to
diagnose the way it diagnosed by looking

00:02:39.560 --> 00:02:39.570
diagnose the way it diagnosed by looking
 

00:02:39.570 --> 00:02:41.510
diagnose the way it diagnosed by looking
inside the network and understanding

00:02:41.510 --> 00:02:41.520
inside the network and understanding
 

00:02:41.520 --> 00:02:45.710
inside the network and understanding
exactly why I made that decision deep

00:02:45.710 --> 00:02:45.720
exactly why I made that decision deep
 

00:02:45.720 --> 00:02:47.570
exactly why I made that decision deep
neural networks can also be used to

00:02:47.570 --> 00:02:47.580
neural networks can also be used to
 

00:02:47.580 --> 00:02:49.490
neural networks can also be used to
model sequences where your data points

00:02:49.490 --> 00:02:49.500
model sequences where your data points
 

00:02:49.500 --> 00:02:51.020
model sequences where your data points
are not just single images but rather

00:02:51.020 --> 00:02:51.030
are not just single images but rather
 

00:02:51.030 --> 00:02:53.630
are not just single images but rather
temporally dependent so for this you can

00:02:53.630 --> 00:02:53.640
temporally dependent so for this you can
 

00:02:53.640 --> 00:02:55.730
temporally dependent so for this you can
think of things like predicting the

00:02:55.730 --> 00:02:55.740
think of things like predicting the
 

00:02:55.740 --> 00:02:58.400
think of things like predicting the
stock price translating sentences from

00:02:58.400 --> 00:02:58.410
stock price translating sentences from
 

00:02:58.410 --> 00:03:01.160
stock price translating sentences from
English to Spanish or even generating

00:03:01.160 --> 00:03:01.170
English to Spanish or even generating
 

00:03:01.170 --> 00:03:02.990
English to Spanish or even generating
new music so actually today you'll learn

00:03:02.990 --> 00:03:03.000
new music so actually today you'll learn
 

00:03:03.000 --> 00:03:04.790
new music so actually today you'll learn
how to create and actually you'll create

00:03:04.790 --> 00:03:04.800
how to create and actually you'll create
 

00:03:04.800 --> 00:03:07.970
how to create and actually you'll create
yourselves an algorithm that learns that

00:03:07.970 --> 00:03:07.980
yourselves an algorithm that learns that
 

00:03:07.980 --> 00:03:11.270
yourselves an algorithm that learns that
first listens to hours of music learns

00:03:11.270 --> 00:03:11.280
first listens to hours of music learns
 

00:03:11.280 --> 00:03:13.490
first listens to hours of music learns
the underlying representation of the

00:03:13.490 --> 00:03:13.500
the underlying representation of the
 

00:03:13.500 --> 00:03:15.260
the underlying representation of the
notes that are being played in those

00:03:15.260 --> 00:03:15.270
notes that are being played in those
 

00:03:15.270 --> 00:03:18.020
notes that are being played in those
songs and then learns to build brand new

00:03:18.020 --> 00:03:18.030
songs and then learns to build brand new
 

00:03:18.030 --> 00:03:20.410
songs and then learns to build brand new
songs that have never been heard before

00:03:20.410 --> 00:03:20.420
songs that have never been heard before
 

00:03:20.420 --> 00:03:23.420
songs that have never been heard before
and there are really so many other

00:03:23.420 --> 00:03:23.430
and there are really so many other
 

00:03:23.430 --> 00:03:25.190
and there are really so many other
incredible success stories of deep

00:03:25.190 --> 00:03:25.200
incredible success stories of deep
 

00:03:25.200 --> 00:03:27.470
incredible success stories of deep
learning that I could talk for many

00:03:27.470 --> 00:03:27.480
learning that I could talk for many
 

00:03:27.480 --> 00:03:28.880
learning that I could talk for many
hours about and will try to cover as

00:03:28.880 --> 00:03:28.890
hours about and will try to cover as
 

00:03:28.890 --> 00:03:30.380
hours about and will try to cover as
many of these as possible as part of

00:03:30.380 --> 00:03:30.390
many of these as possible as part of
 

00:03:30.390 --> 00:03:32.210
many of these as possible as part of
this course but I just wanted to give

00:03:32.210 --> 00:03:32.220
this course but I just wanted to give
 

00:03:32.220 --> 00:03:33.979
this course but I just wanted to give
you an overview of some of the amazing

00:03:33.979 --> 00:03:33.989
you an overview of some of the amazing
 

00:03:33.989 --> 00:03:35.270
you an overview of some of the amazing
ones that we'll be covering as part of

00:03:35.270 --> 00:03:35.280
ones that we'll be covering as part of
 

00:03:35.280 --> 00:03:38.000
ones that we'll be covering as part of
the labs that you'll be implementing and

00:03:38.000 --> 00:03:38.010
the labs that you'll be implementing and
 

00:03:38.010 --> 00:03:40.040
the labs that you'll be implementing and
that's really the goal of what we want

00:03:40.040 --> 00:03:40.050
that's really the goal of what we want
 

00:03:40.050 --> 00:03:41.780
that's really the goal of what we want
you to accomplish as part of this class

00:03:41.780 --> 00:03:41.790
you to accomplish as part of this class
 

00:03:41.790 --> 00:03:44.360
you to accomplish as part of this class
firstly we want to provide you with the

00:03:44.360 --> 00:03:44.370
firstly we want to provide you with the
 

00:03:44.370 --> 00:03:46.640
firstly we want to provide you with the
foundation to do deep learn to

00:03:46.640 --> 00:03:46.650
foundation to do deep learn to
 

00:03:46.650 --> 00:03:48.320
foundation to do deep learn to
understand what these algorithms are

00:03:48.320 --> 00:03:48.330
understand what these algorithms are
 

00:03:48.330 --> 00:03:50.750
understand what these algorithms are
doing underneath the hood how they work

00:03:50.750 --> 00:03:50.760
doing underneath the hood how they work
 

00:03:50.760 --> 00:03:54.500
doing underneath the hood how they work
and why they work we will provide you

00:03:54.500 --> 00:03:54.510
and why they work we will provide you
 

00:03:54.510 --> 00:03:56.210
and why they work we will provide you
some of the practical skills to

00:03:56.210 --> 00:03:56.220
some of the practical skills to
 

00:03:56.220 --> 00:03:57.860
some of the practical skills to
implement these algorithms and deploy

00:03:57.860 --> 00:03:57.870
implement these algorithms and deploy
 

00:03:57.870 --> 00:04:00.830
implement these algorithms and deploy
them on your own machines and will talk

00:04:00.830 --> 00:04:00.840
them on your own machines and will talk
 

00:04:00.840 --> 00:04:02.960
them on your own machines and will talk
to you about some of the stating state

00:04:02.960 --> 00:04:02.970
to you about some of the stating state
 

00:04:02.970 --> 00:04:04.910
to you about some of the stating state
of art and cutting edge research that's

00:04:04.910 --> 00:04:04.920
of art and cutting edge research that's
 

00:04:04.920 --> 00:04:07.640
of art and cutting edge research that's
happening in deep learning industries

00:04:07.640 --> 00:04:07.650
happening in deep learning industries
 

00:04:07.650 --> 00:04:10.930
happening in deep learning industries
and deep learning academia institutions

00:04:10.930 --> 00:04:10.940
and deep learning academia institutions
 

00:04:10.940 --> 00:04:13.850
and deep learning academia institutions
finally the main purpose of this course

00:04:13.850 --> 00:04:13.860
finally the main purpose of this course
 

00:04:13.860 --> 00:04:16.219
finally the main purpose of this course
is we want to build a community here at

00:04:16.219 --> 00:04:16.229
is we want to build a community here at
 

00:04:16.229 --> 00:04:19.340
is we want to build a community here at
MIT that is devoted to advancing the

00:04:19.340 --> 00:04:19.350
MIT that is devoted to advancing the
 

00:04:19.350 --> 00:04:20.590
MIT that is devoted to advancing the
state of artificial intelligence

00:04:20.590 --> 00:04:20.600
state of artificial intelligence
 

00:04:20.600 --> 00:04:23.180
state of artificial intelligence
advancing a state of deep learning as

00:04:23.180 --> 00:04:23.190
advancing a state of deep learning as
 

00:04:23.190 --> 00:04:25.219
advancing a state of deep learning as
part of this course we'll cover some of

00:04:25.219 --> 00:04:25.229
part of this course we'll cover some of
 

00:04:25.229 --> 00:04:27.260
part of this course we'll cover some of
the limitations of these algorithms

00:04:27.260 --> 00:04:27.270
the limitations of these algorithms
 

00:04:27.270 --> 00:04:28.980
the limitations of these algorithms
there are many

00:04:28.980 --> 00:04:28.990
there are many
 

00:04:28.990 --> 00:04:30.180
there are many
we need to be mindful of these

00:04:30.180 --> 00:04:30.190
we need to be mindful of these
 

00:04:30.190 --> 00:04:31.710
we need to be mindful of these
limitations so that we as a community

00:04:31.710 --> 00:04:31.720
limitations so that we as a community
 

00:04:31.720 --> 00:04:34.080
limitations so that we as a community
can move forward and create more

00:04:34.080 --> 00:04:34.090
can move forward and create more
 

00:04:34.090 --> 00:04:38.490
can move forward and create more
intelligent systems but before we do

00:04:38.490 --> 00:04:38.500
intelligent systems but before we do
 

00:04:38.500 --> 00:04:39.870
intelligent systems but before we do
that let's just start with some

00:04:39.870 --> 00:04:39.880
that let's just start with some
 

00:04:39.880 --> 00:04:43.559
that let's just start with some
administrative details in this course so

00:04:43.559 --> 00:04:43.569
administrative details in this course so
 

00:04:43.569 --> 00:04:45.990
administrative details in this course so
this course is a one-week course today

00:04:45.990 --> 00:04:46.000
this course is a one-week course today
 

00:04:46.000 --> 00:04:48.360
this course is a one-week course today
is the first lecture we meet every day

00:04:48.360 --> 00:04:48.370
is the first lecture we meet every day
 

00:04:48.370 --> 00:04:52.189
is the first lecture we meet every day
this week 10:30 a.m. to 1:30 p.m. and

00:04:52.189 --> 00:04:52.199
this week 10:30 a.m. to 1:30 p.m. and
 

00:04:52.199 --> 00:04:54.480
this week 10:30 a.m. to 1:30 p.m. and
this during this three hour time slot

00:04:54.480 --> 00:04:54.490
this during this three hour time slot
 

00:04:54.490 --> 00:04:57.600
this during this three hour time slot
were broken down into one and a half

00:04:57.600 --> 00:04:57.610
were broken down into one and a half
 

00:04:57.610 --> 00:05:00.330
were broken down into one and a half
hour time slots around 50% of the course

00:05:00.330 --> 00:05:00.340
hour time slots around 50% of the course
 

00:05:00.340 --> 00:05:06.240
hour time slots around 50% of the course
see each and each of those have half

00:05:06.240 --> 00:05:06.250
see each and each of those have half
 

00:05:06.250 --> 00:05:08.310
see each and each of those have half
sections of this course will consist of

00:05:08.310 --> 00:05:08.320
sections of this course will consist of
 

00:05:08.320 --> 00:05:10.140
sections of this course will consist of
lectures which is what you're in right

00:05:10.140 --> 00:05:10.150
lectures which is what you're in right
 

00:05:10.150 --> 00:05:12.210
lectures which is what you're in right
now and the second part is the labs

00:05:12.210 --> 00:05:12.220
now and the second part is the labs
 

00:05:12.220 --> 00:05:13.529
now and the second part is the labs
where you'll actually get practice

00:05:13.529 --> 00:05:13.539
where you'll actually get practice
 

00:05:13.539 --> 00:05:17.540
where you'll actually get practice
implementing what you learn in lectures

00:05:17.540 --> 00:05:17.550
implementing what you learn in lectures
 

00:05:17.550 --> 00:05:20.070
implementing what you learn in lectures
we have an amazing set of lectures lined

00:05:20.070 --> 00:05:20.080
we have an amazing set of lectures lined
 

00:05:20.080 --> 00:05:20.640
we have an amazing set of lectures lined
up for you

00:05:20.640 --> 00:05:20.650
up for you
 

00:05:20.650 --> 00:05:22.700
up for you
so today we're going to be talking about

00:05:22.700 --> 00:05:22.710
so today we're going to be talking about
 

00:05:22.710 --> 00:05:24.899
so today we're going to be talking about
some of the introduction to neural

00:05:24.899 --> 00:05:24.909
some of the introduction to neural
 

00:05:24.909 --> 00:05:26.670
some of the introduction to neural
networks which is really the backbone of

00:05:26.670 --> 00:05:26.680
networks which is really the backbone of
 

00:05:26.680 --> 00:05:29.370
networks which is really the backbone of
deep learning we're also talking about

00:05:29.370 --> 00:05:29.380
deep learning we're also talking about
 

00:05:29.380 --> 00:05:31.710
deep learning we're also talking about
modeling sequence data so this is what I

00:05:31.710 --> 00:05:31.720
modeling sequence data so this is what I
 

00:05:31.720 --> 00:05:33.649
modeling sequence data so this is what I
was mentioning about the temporally

00:05:33.649 --> 00:05:33.659
was mentioning about the temporally
 

00:05:33.659 --> 00:05:36.779
was mentioning about the temporally
dependent data tomorrow we'll talk about

00:05:36.779 --> 00:05:36.789
dependent data tomorrow we'll talk about
 

00:05:36.789 --> 00:05:38.490
dependent data tomorrow we'll talk about
computer vision and deep generative

00:05:38.490 --> 00:05:38.500
computer vision and deep generative
 

00:05:38.500 --> 00:05:41.040
computer vision and deep generative
models we have one of the inventors of

00:05:41.040 --> 00:05:41.050
models we have one of the inventors of
 

00:05:41.050 --> 00:05:43.560
models we have one of the inventors of
generative adversarial networks coming

00:05:43.560 --> 00:05:43.570
generative adversarial networks coming
 

00:05:43.570 --> 00:05:45.270
generative adversarial networks coming
to give that lecture for us so that's

00:05:45.270 --> 00:05:45.280
to give that lecture for us so that's
 

00:05:45.280 --> 00:05:48.209
to give that lecture for us so that's
going to be a great lecture and the day

00:05:48.209 --> 00:05:48.219
going to be a great lecture and the day
 

00:05:48.219 --> 00:05:49.200
going to be a great lecture and the day
after that we'll touch on deep

00:05:49.200 --> 00:05:49.210
after that we'll touch on deep
 

00:05:49.210 --> 00:05:50.909
after that we'll touch on deep
reinforcement learning and some of the

00:05:50.909 --> 00:05:50.919
reinforcement learning and some of the
 

00:05:50.919 --> 00:05:53.550
reinforcement learning and some of the
open challenges in AI and how we can

00:05:53.550 --> 00:05:53.560
open challenges in AI and how we can
 

00:05:53.560 --> 00:05:56.610
open challenges in AI and how we can
move forward past this course we'll

00:05:56.610 --> 00:05:56.620
move forward past this course we'll
 

00:05:56.620 --> 00:05:59.219
move forward past this course we'll
spend the final two days of this course

00:05:59.219 --> 00:05:59.229
spend the final two days of this course
 

00:05:59.229 --> 00:06:01.860
spend the final two days of this course
talking or hearing from some of the

00:06:01.860 --> 00:06:01.870
talking or hearing from some of the
 

00:06:01.870 --> 00:06:04.709
talking or hearing from some of the
leading industry representatives doing

00:06:04.709 --> 00:06:04.719
leading industry representatives doing
 

00:06:04.719 --> 00:06:07.170
leading industry representatives doing
deep learning in their respective

00:06:07.170 --> 00:06:07.180
deep learning in their respective
 

00:06:07.180 --> 00:06:09.450
deep learning in their respective
companies and these are bound to be

00:06:09.450 --> 00:06:09.460
companies and these are bound to be
 

00:06:09.460 --> 00:06:11.310
companies and these are bound to be
extremely interesting or extremely

00:06:11.310 --> 00:06:11.320
extremely interesting or extremely
 

00:06:11.320 --> 00:06:13.050
extremely interesting or extremely
exciting so I highly recommend attending

00:06:13.050 --> 00:06:13.060
exciting so I highly recommend attending
 

00:06:13.060 --> 00:06:17.249
exciting so I highly recommend attending
these as well for those of you who are

00:06:17.249 --> 00:06:17.259
these as well for those of you who are
 

00:06:17.259 --> 00:06:19.230
these as well for those of you who are
taking this course for credit you have

00:06:19.230 --> 00:06:19.240
taking this course for credit you have
 

00:06:19.240 --> 00:06:21.930
taking this course for credit you have
two options to fulfill your graders

00:06:21.930 --> 00:06:21.940
two options to fulfill your graders
 

00:06:21.940 --> 00:06:25.709
two options to fulfill your graders
assignment the first option is a project

00:06:25.709 --> 00:06:25.719
assignment the first option is a project
 

00:06:25.719 --> 00:06:27.719
assignment the first option is a project
proposal it's a one-minute project pitch

00:06:27.719 --> 00:06:27.729
proposal it's a one-minute project pitch
 

00:06:27.729 --> 00:06:31.100
proposal it's a one-minute project pitch
that will take place during Friday and

00:06:31.100 --> 00:06:31.110
that will take place during Friday and
 

00:06:31.110 --> 00:06:33.270
that will take place during Friday and
for this you have to work in groups of

00:06:33.270 --> 00:06:33.280
for this you have to work in groups of
 

00:06:33.280 --> 00:06:35.100
for this you have to work in groups of
three or four and what you'll be tasked

00:06:35.100 --> 00:06:35.110
three or four and what you'll be tasked
 

00:06:35.110 --> 00:06:37.290
three or four and what you'll be tasked
to do is just come up with interesting

00:06:37.290 --> 00:06:37.300
to do is just come up with interesting
 

00:06:37.300 --> 00:06:40.320
to do is just come up with interesting
deep learning idea and try to show some

00:06:40.320 --> 00:06:40.330
deep learning idea and try to show some
 

00:06:40.330 --> 00:06:42.810
deep learning idea and try to show some
sort of results if possible

00:06:42.810 --> 00:06:42.820
sort of results if possible
 

00:06:42.820 --> 00:06:45.270
sort of results if possible
we understand that one week is extremely

00:06:45.270 --> 00:06:45.280
we understand that one week is extremely
 

00:06:45.280 --> 00:06:47.010
we understand that one week is extremely
short to create any type of results or

00:06:47.010 --> 00:06:47.020
short to create any type of results or
 

00:06:47.020 --> 00:06:49.140
short to create any type of results or
even come up with a interesting idea for

00:06:49.140 --> 00:06:49.150
even come up with a interesting idea for
 

00:06:49.150 --> 00:06:52.140
even come up with a interesting idea for
that matter but we're going to be giving

00:06:52.140 --> 00:06:52.150
that matter but we're going to be giving
 

00:06:52.150 --> 00:06:54.570
that matter but we're going to be giving
out some amazing prizes so including

00:06:54.570 --> 00:06:54.580
out some amazing prizes so including
 

00:06:54.580 --> 00:07:00.210
out some amazing prizes so including
some nvidia gpus and google homes on

00:07:00.210 --> 00:07:00.220
some nvidia gpus and google homes on
 

00:07:00.220 --> 00:07:02.040
some nvidia gpus and google homes on
friday you'll like I said give a

00:07:02.040 --> 00:07:02.050
friday you'll like I said give a
 

00:07:02.050 --> 00:07:04.650
friday you'll like I said give a
one-minute pitch there's somewhat of an

00:07:04.650 --> 00:07:04.660
one-minute pitch there's somewhat of an
 

00:07:04.660 --> 00:07:08.040
one-minute pitch there's somewhat of an
arts to your idea in just one minute

00:07:08.040 --> 00:07:08.050
arts to your idea in just one minute
 

00:07:08.050 --> 00:07:10.260
arts to your idea in just one minute
even though it's extremely short so we

00:07:10.260 --> 00:07:10.270
even though it's extremely short so we
 

00:07:10.270 --> 00:07:11.550
even though it's extremely short so we
will be holding you to a strict deadline

00:07:11.550 --> 00:07:11.560
will be holding you to a strict deadline
 

00:07:11.560 --> 00:07:13.880
will be holding you to a strict deadline
of that one minute

00:07:13.880 --> 00:07:13.890
of that one minute
 

00:07:13.890 --> 00:07:16.050
of that one minute
the second option is a little more

00:07:16.050 --> 00:07:16.060
the second option is a little more
 

00:07:16.060 --> 00:07:17.640
the second option is a little more
boring but you'll be able to write a

00:07:17.640 --> 00:07:17.650
boring but you'll be able to write a
 

00:07:17.650 --> 00:07:19.740
boring but you'll be able to write a
one-page paper about any deep learning

00:07:19.740 --> 00:07:19.750
one-page paper about any deep learning
 

00:07:19.750 --> 00:07:21.620
one-page paper about any deep learning
paper that you find interesting and

00:07:21.620 --> 00:07:21.630
paper that you find interesting and
 

00:07:21.630 --> 00:07:23.670
paper that you find interesting and
really that's if you can't do the

00:07:23.670 --> 00:07:23.680
really that's if you can't do the
 

00:07:23.680 --> 00:07:29.370
really that's if you can't do the
project proposal you can do that this

00:07:29.370 --> 00:07:29.380
project proposal you can do that this
 

00:07:29.380 --> 00:07:31.770
project proposal you can do that this
class has a lot of online resources you

00:07:31.770 --> 00:07:31.780
class has a lot of online resources you
 

00:07:31.780 --> 00:07:33.660
class has a lot of online resources you
can find support on Piazza please post

00:07:33.660 --> 00:07:33.670
can find support on Piazza please post
 

00:07:33.670 --> 00:07:35.730
can find support on Piazza please post
if you have any questions about the

00:07:35.730 --> 00:07:35.740
if you have any questions about the
 

00:07:35.740 --> 00:07:38.340
if you have any questions about the
lectures the labs installing any of the

00:07:38.340 --> 00:07:38.350
lectures the labs installing any of the
 

00:07:38.350 --> 00:07:41.850
lectures the labs installing any of the
software etc also try to keep up to date

00:07:41.850 --> 00:07:41.860
software etc also try to keep up to date
 

00:07:41.860 --> 00:07:43.650
software etc also try to keep up to date
with the course website we'll be posting

00:07:43.650 --> 00:07:43.660
with the course website we'll be posting
 

00:07:43.660 --> 00:07:47.520
with the course website we'll be posting
all of the lectures labs and video

00:07:47.520 --> 00:07:47.530
all of the lectures labs and video
 

00:07:47.530 --> 00:07:52.260
all of the lectures labs and video
recordings online as well we have an

00:07:52.260 --> 00:07:52.270
recordings online as well we have an
 

00:07:52.270 --> 00:07:54.090
recordings online as well we have an
amazing team that you can reach out to

00:07:54.090 --> 00:07:54.100
amazing team that you can reach out to
 

00:07:54.100 --> 00:07:56.310
amazing team that you can reach out to
at any time in case you have any

00:07:56.310 --> 00:07:56.320
at any time in case you have any
 

00:07:56.320 --> 00:07:57.930
at any time in case you have any
problems with anything feel free to

00:07:57.930 --> 00:07:57.940
problems with anything feel free to
 

00:07:57.940 --> 00:07:59.580
problems with anything feel free to
reach out to any of us and we wanted to

00:07:59.580 --> 00:07:59.590
reach out to any of us and we wanted to
 

00:07:59.590 --> 00:08:00.900
reach out to any of us and we wanted to
give a huge thanks to all of our

00:08:00.900 --> 00:08:00.910
give a huge thanks to all of our
 

00:08:00.910 --> 00:08:03.780
give a huge thanks to all of our
sponsors who without this without their

00:08:03.780 --> 00:08:03.790
sponsors who without this without their
 

00:08:03.790 --> 00:08:05.940
sponsors who without this without their
support this class would simply not

00:08:05.940 --> 00:08:05.950
support this class would simply not
 

00:08:05.950 --> 00:08:08.490
support this class would simply not
happened the way the way it's happening

00:08:08.490 --> 00:08:08.500
happened the way the way it's happening
 

00:08:08.500 --> 00:08:11.850
happened the way the way it's happening
this year so now let's start with the

00:08:11.850 --> 00:08:11.860
this year so now let's start with the
 

00:08:11.860 --> 00:08:15.120
this year so now let's start with the
fun stuff and let's start by actually

00:08:15.120 --> 00:08:15.130
fun stuff and let's start by actually
 

00:08:15.130 --> 00:08:18.330
fun stuff and let's start by actually
asking ourselves a question why do we

00:08:18.330 --> 00:08:18.340
asking ourselves a question why do we
 

00:08:18.340 --> 00:08:21.180
asking ourselves a question why do we
even care about deep learning so why now

00:08:21.180 --> 00:08:21.190
even care about deep learning so why now
 

00:08:21.190 --> 00:08:24.780
even care about deep learning so why now
and why do we why do we even sit in this

00:08:24.780 --> 00:08:24.790
and why do we why do we even sit in this
 

00:08:24.790 --> 00:08:28.710
and why do we why do we even sit in this
class today so traditional machine

00:08:28.710 --> 00:08:28.720
class today so traditional machine
 

00:08:28.720 --> 00:08:31.500
class today so traditional machine
learning algorithms typically define

00:08:31.500 --> 00:08:31.510
learning algorithms typically define
 

00:08:31.510 --> 00:08:34.320
learning algorithms typically define
sets of pre-programmed features and the

00:08:34.320 --> 00:08:34.330
sets of pre-programmed features and the
 

00:08:34.330 --> 00:08:36.690
sets of pre-programmed features and the
data and they work to extract these

00:08:36.690 --> 00:08:36.700
data and they work to extract these
 

00:08:36.700 --> 00:08:40.020
data and they work to extract these
features as part of their pipeline now

00:08:40.020 --> 00:08:40.030
features as part of their pipeline now
 

00:08:40.030 --> 00:08:41.940
features as part of their pipeline now
the key differentiating point of deep

00:08:41.940 --> 00:08:41.950
the key differentiating point of deep
 

00:08:41.950 --> 00:08:43.860
the key differentiating point of deep
learning is that it recognizes that in

00:08:43.860 --> 00:08:43.870
learning is that it recognizes that in
 

00:08:43.870 --> 00:08:46.680
learning is that it recognizes that in
many practical situations these features

00:08:46.680 --> 00:08:46.690
many practical situations these features
 

00:08:46.690 --> 00:08:49.470
many practical situations these features
can be extremely brittle so what deep

00:08:49.470 --> 00:08:49.480
can be extremely brittle so what deep
 

00:08:49.480 --> 00:08:52.320
can be extremely brittle so what deep
learning tries to do is learn these

00:08:52.320 --> 00:08:52.330
learning tries to do is learn these
 

00:08:52.330 --> 00:08:54.510
learning tries to do is learn these
features directly from data as opposed

00:08:54.510 --> 00:08:54.520
features directly from data as opposed
 

00:08:54.520 --> 00:08:56.420
features directly from data as opposed
to being hand engineered

00:08:56.420 --> 00:08:56.430
to being hand engineered
 

00:08:56.430 --> 00:09:00.769
to being hand engineered
by the human that is can we learn if we

00:09:00.769 --> 00:09:00.779
by the human that is can we learn if we
 

00:09:00.779 --> 00:09:02.480
by the human that is can we learn if we
want to learn to detect faces can we

00:09:02.480 --> 00:09:02.490
want to learn to detect faces can we
 

00:09:02.490 --> 00:09:04.460
want to learn to detect faces can we
first learn automatically from data that

00:09:04.460 --> 00:09:04.470
first learn automatically from data that
 

00:09:04.470 --> 00:09:07.310
first learn automatically from data that
to detect faces we first need to detect

00:09:07.310 --> 00:09:07.320
to detect faces we first need to detect
 

00:09:07.320 --> 00:09:10.100
to detect faces we first need to detect
edges in the image compose these edges

00:09:10.100 --> 00:09:10.110
edges in the image compose these edges
 

00:09:10.110 --> 00:09:12.949
edges in the image compose these edges
together to detect eyes and ears then

00:09:12.949 --> 00:09:12.959
together to detect eyes and ears then
 

00:09:12.959 --> 00:09:14.870
together to detect eyes and ears then
compose these eyes and ears together to

00:09:14.870 --> 00:09:14.880
compose these eyes and ears together to
 

00:09:14.880 --> 00:09:17.090
compose these eyes and ears together to
form higher-level facial structure and

00:09:17.090 --> 00:09:17.100
form higher-level facial structure and
 

00:09:17.100 --> 00:09:19.400
form higher-level facial structure and
in this way deep learning represents a

00:09:19.400 --> 00:09:19.410
in this way deep learning represents a
 

00:09:19.410 --> 00:09:22.280
in this way deep learning represents a
form of a hierarchical model capable of

00:09:22.280 --> 00:09:22.290
form of a hierarchical model capable of
 

00:09:22.290 --> 00:09:24.740
form of a hierarchical model capable of
representing different levels of

00:09:24.740 --> 00:09:24.750
representing different levels of
 

00:09:24.750 --> 00:09:28.730
representing different levels of
abstraction in the data so actually the

00:09:28.730 --> 00:09:28.740
abstraction in the data so actually the
 

00:09:28.740 --> 00:09:30.230
abstraction in the data so actually the
fundamental building blocks of deep

00:09:30.230 --> 00:09:30.240
fundamental building blocks of deep
 

00:09:30.240 --> 00:09:32.930
fundamental building blocks of deep
learning which are neural networks have

00:09:32.930 --> 00:09:32.940
learning which are neural networks have
 

00:09:32.940 --> 00:09:35.120
learning which are neural networks have
actually been existing have actually

00:09:35.120 --> 00:09:35.130
actually been existing have actually
 

00:09:35.130 --> 00:09:37.790
actually been existing have actually
existed for decades so why are we

00:09:37.790 --> 00:09:37.800
existed for decades so why are we
 

00:09:37.800 --> 00:09:40.910
existed for decades so why are we
studying this now well there's three key

00:09:40.910 --> 00:09:40.920
studying this now well there's three key
 

00:09:40.920 --> 00:09:44.180
studying this now well there's three key
points here the first is that data has

00:09:44.180 --> 00:09:44.190
points here the first is that data has
 

00:09:44.190 --> 00:09:48.350
points here the first is that data has
become much more pervasive we're living

00:09:48.350 --> 00:09:48.360
become much more pervasive we're living
 

00:09:48.360 --> 00:09:49.880
become much more pervasive we're living
in a big data environment these

00:09:49.880 --> 00:09:49.890
in a big data environment these
 

00:09:49.890 --> 00:09:51.889
in a big data environment these
algorithms are hungry for more and more

00:09:51.889 --> 00:09:51.899
algorithms are hungry for more and more
 

00:09:51.899 --> 00:09:56.090
algorithms are hungry for more and more
data and accessing that data has become

00:09:56.090 --> 00:09:56.100
data and accessing that data has become
 

00:09:56.100 --> 00:09:58.180
data and accessing that data has become
easier than ever before

00:09:58.180 --> 00:09:58.190
easier than ever before
 

00:09:58.190 --> 00:10:00.530
easier than ever before
second these algorithms are massively

00:10:00.530 --> 00:10:00.540
second these algorithms are massively
 

00:10:00.540 --> 00:10:02.000
second these algorithms are massively
parallel Liza below and can benefit

00:10:02.000 --> 00:10:02.010
parallel Liza below and can benefit
 

00:10:02.010 --> 00:10:05.660
parallel Liza below and can benefit
tremendously from modern GPU

00:10:05.660 --> 00:10:05.670
tremendously from modern GPU
 

00:10:05.670 --> 00:10:07.579
tremendously from modern GPU
architectures that simply just did not

00:10:07.579 --> 00:10:07.589
architectures that simply just did not
 

00:10:07.589 --> 00:10:10.340
architectures that simply just did not
exist just less more than a decade ago

00:10:10.340 --> 00:10:10.350
exist just less more than a decade ago
 

00:10:10.350 --> 00:10:13.310
exist just less more than a decade ago
and finally due to open-source tool

00:10:13.310 --> 00:10:13.320
and finally due to open-source tool
 

00:10:13.320 --> 00:10:16.820
and finally due to open-source tool
boxes like tensor flow building and

00:10:16.820 --> 00:10:16.830
boxes like tensor flow building and
 

00:10:16.830 --> 00:10:18.829
boxes like tensor flow building and
deploying these algorithms has become so

00:10:18.829 --> 00:10:18.839
deploying these algorithms has become so
 

00:10:18.839 --> 00:10:20.900
deploying these algorithms has become so
streamlined so simple that we can teach

00:10:20.900 --> 00:10:20.910
streamlined so simple that we can teach
 

00:10:20.910 --> 00:10:22.579
streamlined so simple that we can teach
it in a one-week course like this and

00:10:22.579 --> 00:10:22.589
it in a one-week course like this and
 

00:10:22.589 --> 00:10:25.790
it in a one-week course like this and
it's become extremely deployable for the

00:10:25.790 --> 00:10:25.800
it's become extremely deployable for the
 

00:10:25.800 --> 00:10:30.320
it's become extremely deployable for the
massive public so let's start with now

00:10:30.320 --> 00:10:30.330
massive public so let's start with now
 

00:10:30.330 --> 00:10:31.430
massive public so let's start with now
looking at the fundamental building

00:10:31.430 --> 00:10:31.440
looking at the fundamental building
 

00:10:31.440 --> 00:10:33.560
looking at the fundamental building
block of deep learning and that's the

00:10:33.560 --> 00:10:33.570
block of deep learning and that's the
 

00:10:33.570 --> 00:10:36.019
block of deep learning and that's the
perceptron this is really just a single

00:10:36.019 --> 00:10:36.029
perceptron this is really just a single
 

00:10:36.029 --> 00:10:41.269
perceptron this is really just a single
neuron in a neural network so the idea

00:10:41.269 --> 00:10:41.279
neuron in a neural network so the idea
 

00:10:41.279 --> 00:10:43.960
neuron in a neural network so the idea
of a perceptron or a single neuron is

00:10:43.960 --> 00:10:43.970
of a perceptron or a single neuron is
 

00:10:43.970 --> 00:10:47.180
of a perceptron or a single neuron is
extremely simple let's start by talking

00:10:47.180 --> 00:10:47.190
extremely simple let's start by talking
 

00:10:47.190 --> 00:10:48.860
extremely simple let's start by talking
about the forward propagation of

00:10:48.860 --> 00:10:48.870
about the forward propagation of
 

00:10:48.870 --> 00:10:52.040
about the forward propagation of
information through this data unit we

00:10:52.040 --> 00:10:52.050
information through this data unit we
 

00:10:52.050 --> 00:10:54.680
information through this data unit we
define a set of inputs x1 through XM on

00:10:54.680 --> 00:10:54.690
define a set of inputs x1 through XM on
 

00:10:54.690 --> 00:10:58.910
define a set of inputs x1 through XM on
the left and all we do is we multiply

00:10:58.910 --> 00:10:58.920
the left and all we do is we multiply
 

00:10:58.920 --> 00:11:00.680
the left and all we do is we multiply
each of these inputs by their

00:11:00.680 --> 00:11:00.690
each of these inputs by their
 

00:11:00.690 --> 00:11:02.750
each of these inputs by their
corresponding weight theta1 through

00:11:02.750 --> 00:11:02.760
corresponding weight theta1 through
 

00:11:02.760 --> 00:11:06.110
corresponding weight theta1 through
theta m which are those arrows we take

00:11:06.110 --> 00:11:06.120
theta m which are those arrows we take
 

00:11:06.120 --> 00:11:08.740
theta m which are those arrows we take
this weighted we take this weighted

00:11:08.740 --> 00:11:08.750
this weighted we take this weighted
 

00:11:08.750 --> 00:11:10.220
this weighted we take this weighted
combination

00:11:10.220 --> 00:11:10.230
combination
 

00:11:10.230 --> 00:11:12.620
combination
of all of our inputs sum them up and

00:11:12.620 --> 00:11:12.630
of all of our inputs sum them up and
 

00:11:12.630 --> 00:11:14.840
of all of our inputs sum them up and
pass them through a nonlinear activation

00:11:14.840 --> 00:11:14.850
pass them through a nonlinear activation
 

00:11:14.850 --> 00:11:18.290
pass them through a nonlinear activation
function and that produces our output

00:11:18.290 --> 00:11:18.300
function and that produces our output
 

00:11:18.300 --> 00:11:20.660
function and that produces our output
why it's that simple so we have M inputs

00:11:20.660 --> 00:11:20.670
why it's that simple so we have M inputs
 

00:11:20.670 --> 00:11:24.170
why it's that simple so we have M inputs
one output number and you can see it

00:11:24.170 --> 00:11:24.180
one output number and you can see it
 

00:11:24.180 --> 00:11:27.080
one output number and you can see it
summarized on the right-hand side as a

00:11:27.080 --> 00:11:27.090
summarized on the right-hand side as a
 

00:11:27.090 --> 00:11:29.590
summarized on the right-hand side as a
mathematic single mathematical equation

00:11:29.590 --> 00:11:29.600
mathematic single mathematical equation
 

00:11:29.600 --> 00:11:31.760
mathematic single mathematical equation
but actually I left that one important

00:11:31.760 --> 00:11:31.770
but actually I left that one important
 

00:11:31.770 --> 00:11:34.910
but actually I left that one important
detail that makes the previous slide not

00:11:34.910 --> 00:11:34.920
detail that makes the previous slide not
 

00:11:34.920 --> 00:11:35.870
detail that makes the previous slide not
exactly correct

00:11:35.870 --> 00:11:35.880
exactly correct
 

00:11:35.880 --> 00:11:38.750
exactly correct
so I left that this notion of a bias a

00:11:38.750 --> 00:11:38.760
so I left that this notion of a bias a
 

00:11:38.760 --> 00:11:42.290
so I left that this notion of a bias a
bias is a that green term you see on the

00:11:42.290 --> 00:11:42.300
bias is a that green term you see on the
 

00:11:42.300 --> 00:11:44.390
bias is a that green term you see on the
left and this just represents some way

00:11:44.390 --> 00:11:44.400
left and this just represents some way
 

00:11:44.400 --> 00:11:47.510
left and this just represents some way
that we can allow our model to learn or

00:11:47.510 --> 00:11:47.520
that we can allow our model to learn or
 

00:11:47.520 --> 00:11:49.340
that we can allow our model to learn or
we can allow our activation function to

00:11:49.340 --> 00:11:49.350
we can allow our activation function to
 

00:11:49.350 --> 00:11:50.720
we can allow our activation function to
shift to the left or right

00:11:50.720 --> 00:11:50.730
shift to the left or right
 

00:11:50.730 --> 00:11:54.190
shift to the left or right
so it allows if we provide allows us to

00:11:54.190 --> 00:11:54.200
so it allows if we provide allows us to
 

00:11:54.200 --> 00:11:57.290
so it allows if we provide allows us to
when we have no input features to still

00:11:57.290 --> 00:11:57.300
when we have no input features to still
 

00:11:57.300 --> 00:12:02.360
when we have no input features to still
provide a positive output so on this

00:12:02.360 --> 00:12:02.370
provide a positive output so on this
 

00:12:02.370 --> 00:12:04.610
provide a positive output so on this
equation on the right we can actually

00:12:04.610 --> 00:12:04.620
equation on the right we can actually
 

00:12:04.620 --> 00:12:07.120
equation on the right we can actually
rewrite this using linear algebra and

00:12:07.120 --> 00:12:07.130
rewrite this using linear algebra and
 

00:12:07.130 --> 00:12:10.420
rewrite this using linear algebra and
dot products to make this a lot cleaner

00:12:10.420 --> 00:12:10.430
dot products to make this a lot cleaner
 

00:12:10.430 --> 00:12:14.990
dot products to make this a lot cleaner
so let's do that let's say X capital X

00:12:14.990 --> 00:12:15.000
so let's do that let's say X capital X
 

00:12:15.000 --> 00:12:17.690
so let's do that let's say X capital X
is a vector containing all of our inputs

00:12:17.690 --> 00:12:17.700
is a vector containing all of our inputs
 

00:12:17.700 --> 00:12:22.910
is a vector containing all of our inputs
x1 through XM capital theta is now just

00:12:22.910 --> 00:12:22.920
x1 through XM capital theta is now just
 

00:12:22.920 --> 00:12:24.470
x1 through XM capital theta is now just
a vector containing all of our Thetas

00:12:24.470 --> 00:12:24.480
a vector containing all of our Thetas
 

00:12:24.480 --> 00:12:27.770
a vector containing all of our Thetas
theta 1 to theta M we can rewrite that

00:12:27.770 --> 00:12:27.780
theta 1 to theta M we can rewrite that
 

00:12:27.780 --> 00:12:29.330
theta 1 to theta M we can rewrite that
equation that we had before is just

00:12:29.330 --> 00:12:29.340
equation that we had before is just
 

00:12:29.340 --> 00:12:31.130
equation that we had before is just
applying a dot product between X and

00:12:31.130 --> 00:12:31.140
applying a dot product between X and
 

00:12:31.140 --> 00:12:35.150
applying a dot product between X and
theta adding our bias theta 0 and apply

00:12:35.150 --> 00:12:35.160
theta adding our bias theta 0 and apply
 

00:12:35.160 --> 00:12:40.610
theta adding our bias theta 0 and apply
our non-linearity G now you might be

00:12:40.610 --> 00:12:40.620
our non-linearity G now you might be
 

00:12:40.620 --> 00:12:42.530
our non-linearity G now you might be
wondering since I've mentioned this a

00:12:42.530 --> 00:12:42.540
wondering since I've mentioned this a
 

00:12:42.540 --> 00:12:45.170
wondering since I've mentioned this a
couple times now what is this nonlinear

00:12:45.170 --> 00:12:45.180
couple times now what is this nonlinear
 

00:12:45.180 --> 00:12:47.930
couple times now what is this nonlinear
function G well I said it's the

00:12:47.930 --> 00:12:47.940
function G well I said it's the
 

00:12:47.940 --> 00:12:50.090
function G well I said it's the
activation function but let's see an

00:12:50.090 --> 00:12:50.100
activation function but let's see an
 

00:12:50.100 --> 00:12:52.820
activation function but let's see an
example of what in practice G actually

00:12:52.820 --> 00:12:52.830
example of what in practice G actually
 

00:12:52.830 --> 00:12:56.420
example of what in practice G actually
could be so one very popular activation

00:12:56.420 --> 00:12:56.430
could be so one very popular activation
 

00:12:56.430 --> 00:12:58.670
could be so one very popular activation
function is the sigmoid function you can

00:12:58.670 --> 00:12:58.680
function is the sigmoid function you can
 

00:12:58.680 --> 00:13:00.440
function is the sigmoid function you can
see a plot of it here on the bottom

00:13:00.440 --> 00:13:00.450
see a plot of it here on the bottom
 

00:13:00.450 --> 00:13:03.110
see a plot of it here on the bottom
right and this is a function that takes

00:13:03.110 --> 00:13:03.120
right and this is a function that takes
 

00:13:03.120 --> 00:13:06.470
right and this is a function that takes
its input any real number on the x-axis

00:13:06.470 --> 00:13:06.480
its input any real number on the x-axis
 

00:13:06.480 --> 00:13:09.320
its input any real number on the x-axis
and transforms it to an output between 0

00:13:09.320 --> 00:13:09.330
and transforms it to an output between 0
 

00:13:09.330 --> 00:13:12.170
and transforms it to an output between 0
and 1 because all outputs of this

00:13:12.170 --> 00:13:12.180
and 1 because all outputs of this
 

00:13:12.180 --> 00:13:13.760
and 1 because all outputs of this
function are between 0 &amp; 1 it makes it a

00:13:13.760 --> 00:13:13.770
function are between 0 &amp; 1 it makes it a
 

00:13:13.770 --> 00:13:16.070
function are between 0 &amp; 1 it makes it a
very popular choice in deep learning to

00:13:16.070 --> 00:13:16.080
very popular choice in deep learning to
 

00:13:16.080 --> 00:13:21.140
very popular choice in deep learning to
represent probabilities in fact there

00:13:21.140 --> 00:13:21.150
represent probabilities in fact there
 

00:13:21.150 --> 00:13:23.270
represent probabilities in fact there
are many types of nonlinear activation

00:13:23.270 --> 00:13:23.280
are many types of nonlinear activation
 

00:13:23.280 --> 00:13:24.110
are many types of nonlinear activation
functions in

00:13:24.110 --> 00:13:24.120
functions in
 

00:13:24.120 --> 00:13:25.850
functions in
Durrell networks and here are some of

00:13:25.850 --> 00:13:25.860
Durrell networks and here are some of
 

00:13:25.860 --> 00:13:27.800
Durrell networks and here are some of
the common ones throughout this

00:13:27.800 --> 00:13:27.810
the common ones throughout this
 

00:13:27.810 --> 00:13:29.720
the common ones throughout this
presentation you'll also see tensorflow

00:13:29.720 --> 00:13:29.730
presentation you'll also see tensorflow
 

00:13:29.730 --> 00:13:31.850
presentation you'll also see tensorflow
code snippets like the ones you see on

00:13:31.850 --> 00:13:31.860
code snippets like the ones you see on
 

00:13:31.860 --> 00:13:32.990
code snippets like the ones you see on
the bottom here since we'll be using

00:13:32.990 --> 00:13:33.000
the bottom here since we'll be using
 

00:13:33.000 --> 00:13:35.780
the bottom here since we'll be using
tensorflow for our labs and well this is

00:13:35.780 --> 00:13:35.790
tensorflow for our labs and well this is
 

00:13:35.790 --> 00:13:37.730
tensorflow for our labs and well this is
some way that I can provide to you to

00:13:37.730 --> 00:13:37.740
some way that I can provide to you to
 

00:13:37.740 --> 00:13:39.170
some way that I can provide to you to
kind of link the material in our

00:13:39.170 --> 00:13:39.180
kind of link the material in our
 

00:13:39.180 --> 00:13:40.400
kind of link the material in our
lectures with what you'll be

00:13:40.400 --> 00:13:40.410
lectures with what you'll be
 

00:13:40.410 --> 00:13:43.820
lectures with what you'll be
implementing in labs so the sigmoid

00:13:43.820 --> 00:13:43.830
implementing in labs so the sigmoid
 

00:13:43.830 --> 00:13:45.290
implementing in labs so the sigmoid
activation function which I talked about

00:13:45.290 --> 00:13:45.300
activation function which I talked about
 

00:13:45.300 --> 00:13:48.880
activation function which I talked about
in the previous slide now on the left is

00:13:48.880 --> 00:13:48.890
in the previous slide now on the left is
 

00:13:48.890 --> 00:13:51.050
in the previous slide now on the left is
it's just a function like I said it's

00:13:51.050 --> 00:13:51.060
it's just a function like I said it's
 

00:13:51.060 --> 00:13:53.690
it's just a function like I said it's
commonly used to produce probability

00:13:53.690 --> 00:13:53.700
commonly used to produce probability
 

00:13:53.700 --> 00:13:55.610
commonly used to produce probability
outputs each of these activation

00:13:55.610 --> 00:13:55.620
outputs each of these activation
 

00:13:55.620 --> 00:13:57.230
outputs each of these activation
functions has their own advantages and

00:13:57.230 --> 00:13:57.240
functions has their own advantages and
 

00:13:57.240 --> 00:14:00.050
functions has their own advantages and
disadvantages on the right a very common

00:14:00.050 --> 00:14:00.060
disadvantages on the right a very common
 

00:14:00.060 --> 00:14:01.670
disadvantages on the right a very common
activation function is the rectified

00:14:01.670 --> 00:14:01.680
activation function is the rectified
 

00:14:01.680 --> 00:14:04.880
activation function is the rectified
linear unit or Lu this function is very

00:14:04.880 --> 00:14:04.890
linear unit or Lu this function is very
 

00:14:04.890 --> 00:14:07.100
linear unit or Lu this function is very
popular because it's extremely simple to

00:14:07.100 --> 00:14:07.110
popular because it's extremely simple to
 

00:14:07.110 --> 00:14:09.260
popular because it's extremely simple to
compute it's piecewise linear it's zero

00:14:09.260 --> 00:14:09.270
compute it's piecewise linear it's zero
 

00:14:09.270 --> 00:14:12.530
compute it's piecewise linear it's zero
before with inputs less than zero it's X

00:14:12.530 --> 00:14:12.540
before with inputs less than zero it's X
 

00:14:12.540 --> 00:14:16.640
before with inputs less than zero it's X
with any input greater than zero and the

00:14:16.640 --> 00:14:16.650
with any input greater than zero and the
 

00:14:16.650 --> 00:14:18.200
with any input greater than zero and the
gradients are just zero or one with a

00:14:18.200 --> 00:14:18.210
gradients are just zero or one with a
 

00:14:18.210 --> 00:14:21.500
gradients are just zero or one with a
single non-linearity at the origin and

00:14:21.500 --> 00:14:21.510
single non-linearity at the origin and
 

00:14:21.510 --> 00:14:24.500
single non-linearity at the origin and
you might be wondering why we even need

00:14:24.500 --> 00:14:24.510
you might be wondering why we even need
 

00:14:24.510 --> 00:14:26.450
you might be wondering why we even need
activation functions why can't we just

00:14:26.450 --> 00:14:26.460
activation functions why can't we just
 

00:14:26.460 --> 00:14:28.310
activation functions why can't we just
take our dot product at our bias and

00:14:28.310 --> 00:14:28.320
take our dot product at our bias and
 

00:14:28.320 --> 00:14:30.170
take our dot product at our bias and
that's our output why do we need the

00:14:30.170 --> 00:14:30.180
that's our output why do we need the
 

00:14:30.180 --> 00:14:32.720
that's our output why do we need the
activation function activation functions

00:14:32.720 --> 00:14:32.730
activation function activation functions
 

00:14:32.730 --> 00:14:35.060
activation function activation functions
introduce nonlinearities into the

00:14:35.060 --> 00:14:35.070
introduce nonlinearities into the
 

00:14:35.070 --> 00:14:36.640
introduce nonlinearities into the
network that's the whole point of why

00:14:36.640 --> 00:14:36.650
network that's the whole point of why
 

00:14:36.650 --> 00:14:41.240
network that's the whole point of why
activations themselves are nonlinear we

00:14:41.240 --> 00:14:41.250
activations themselves are nonlinear we
 

00:14:41.250 --> 00:14:44.060
activations themselves are nonlinear we
want to model nonlinear data in the

00:14:44.060 --> 00:14:44.070
want to model nonlinear data in the
 

00:14:44.070 --> 00:14:45.710
want to model nonlinear data in the
world because the world is extremely

00:14:45.710 --> 00:14:45.720
world because the world is extremely
 

00:14:45.720 --> 00:14:48.980
world because the world is extremely
nonlinear but suppose I gave you this

00:14:48.980 --> 00:14:48.990
nonlinear but suppose I gave you this
 

00:14:48.990 --> 00:14:51.440
nonlinear but suppose I gave you this
this plot green and red points and I

00:14:51.440 --> 00:14:51.450
this plot green and red points and I
 

00:14:51.450 --> 00:14:54.350
this plot green and red points and I
asked you to draw a single line not a

00:14:54.350 --> 00:14:54.360
asked you to draw a single line not a
 

00:14:54.360 --> 00:14:56.630
asked you to draw a single line not a
curve just a line between the green and

00:14:56.630 --> 00:14:56.640
curve just a line between the green and
 

00:14:56.640 --> 00:14:58.130
curve just a line between the green and
red points to separate them perfectly

00:14:58.130 --> 00:14:58.140
red points to separate them perfectly
 

00:14:58.140 --> 00:15:00.199
red points to separate them perfectly
you'd find this really difficult and

00:15:00.199 --> 00:15:00.209
you'd find this really difficult and
 

00:15:00.209 --> 00:15:02.090
you'd find this really difficult and
probably you could get as best as

00:15:02.090 --> 00:15:02.100
probably you could get as best as
 

00:15:02.100 --> 00:15:04.010
probably you could get as best as
something like this now if your

00:15:04.010 --> 00:15:04.020
something like this now if your
 

00:15:04.020 --> 00:15:05.570
something like this now if your
activation function in your deep neural

00:15:05.570 --> 00:15:05.580
activation function in your deep neural
 

00:15:05.580 --> 00:15:08.390
activation function in your deep neural
network was linear since you're just

00:15:08.390 --> 00:15:08.400
network was linear since you're just
 

00:15:08.400 --> 00:15:10.160
network was linear since you're just
composing linear functions with linear

00:15:10.160 --> 00:15:10.170
composing linear functions with linear
 

00:15:10.170 --> 00:15:11.840
composing linear functions with linear
functions your output will always be

00:15:11.840 --> 00:15:11.850
functions your output will always be
 

00:15:11.850 --> 00:15:13.850
functions your output will always be
linear so the most complicated deep

00:15:13.850 --> 00:15:13.860
linear so the most complicated deep
 

00:15:13.860 --> 00:15:15.350
linear so the most complicated deep
neural network no matter how big or how

00:15:15.350 --> 00:15:15.360
neural network no matter how big or how
 

00:15:15.360 --> 00:15:17.300
neural network no matter how big or how
deep if the activation function is

00:15:17.300 --> 00:15:17.310
deep if the activation function is
 

00:15:17.310 --> 00:15:19.760
deep if the activation function is
linear your output can only look like

00:15:19.760 --> 00:15:19.770
linear your output can only look like
 

00:15:19.770 --> 00:15:21.410
linear your output can only look like
this but once we introduce

00:15:21.410 --> 00:15:21.420
this but once we introduce
 

00:15:21.420 --> 00:15:24.890
this but once we introduce
nonlinearities our network is extremely

00:15:24.890 --> 00:15:24.900
nonlinearities our network is extremely
 

00:15:24.900 --> 00:15:27.440
nonlinearities our network is extremely
more as the capacity of our network has

00:15:27.440 --> 00:15:27.450
more as the capacity of our network has
 

00:15:27.450 --> 00:15:29.750
more as the capacity of our network has
extremely increased we're now able to

00:15:29.750 --> 00:15:29.760
extremely increased we're now able to
 

00:15:29.760 --> 00:15:32.390
extremely increased we're now able to
model much more complex functions we're

00:15:32.390 --> 00:15:32.400
model much more complex functions we're
 

00:15:32.400 --> 00:15:33.829
model much more complex functions we're
able to draw decision boundaries that

00:15:33.829 --> 00:15:33.839
able to draw decision boundaries that
 

00:15:33.839 --> 00:15:36.110
able to draw decision boundaries that
were not possible with only linear

00:15:36.110 --> 00:15:36.120
were not possible with only linear
 

00:15:36.120 --> 00:15:36.680
were not possible with only linear
activation

00:15:36.680 --> 00:15:36.690
activation
 

00:15:36.690 --> 00:15:39.680
activation
options let's understand this with a

00:15:39.680 --> 00:15:39.690
options let's understand this with a
 

00:15:39.690 --> 00:15:41.990
options let's understand this with a
very simple example imagine I gave you a

00:15:41.990 --> 00:15:42.000
very simple example imagine I gave you a
 

00:15:42.000 --> 00:15:43.700
very simple example imagine I gave you a
train to network like the one we saw

00:15:43.700 --> 00:15:43.710
train to network like the one we saw
 

00:15:43.710 --> 00:15:45.740
train to network like the one we saw
before sorry a trained perceptron not in

00:15:45.740 --> 00:15:45.750
before sorry a trained perceptron not in
 

00:15:45.750 --> 00:15:49.370
before sorry a trained perceptron not in
network yet just a single node and the

00:15:49.370 --> 00:15:49.380
network yet just a single node and the
 

00:15:49.380 --> 00:15:51.110
network yet just a single node and the
weights are on the top right so theta 0

00:15:51.110 --> 00:15:51.120
weights are on the top right so theta 0
 

00:15:51.120 --> 00:15:54.200
weights are on the top right so theta 0
is 1 and the theta vector is 3 and

00:15:54.200 --> 00:15:54.210
is 1 and the theta vector is 3 and
 

00:15:54.210 --> 00:15:58.070
is 1 and the theta vector is 3 and
negative 2 the network has two inputs X

00:15:58.070 --> 00:15:58.080
negative 2 the network has two inputs X
 

00:15:58.080 --> 00:16:00.140
negative 2 the network has two inputs X
1 and X 2 and if we want to get the

00:16:00.140 --> 00:16:00.150
1 and X 2 and if we want to get the
 

00:16:00.150 --> 00:16:02.420
1 and X 2 and if we want to get the
output all we have to do is apply the

00:16:02.420 --> 00:16:02.430
output all we have to do is apply the
 

00:16:02.430 --> 00:16:04.970
output all we have to do is apply the
same story as before so we apply the dot

00:16:04.970 --> 00:16:04.980
same story as before so we apply the dot
 

00:16:04.980 --> 00:16:07.610
same story as before so we apply the dot
product of X and theta we add the bias

00:16:07.610 --> 00:16:07.620
product of X and theta we add the bias
 

00:16:07.620 --> 00:16:10.760
product of X and theta we add the bias
and apply our non-linearity but let's

00:16:10.760 --> 00:16:10.770
and apply our non-linearity but let's
 

00:16:10.770 --> 00:16:11.720
and apply our non-linearity but let's
take a look at what's actually inside

00:16:11.720 --> 00:16:11.730
take a look at what's actually inside
 

00:16:11.730 --> 00:16:14.720
take a look at what's actually inside
before we apply that non-linearity this

00:16:14.720 --> 00:16:14.730
before we apply that non-linearity this
 

00:16:14.730 --> 00:16:16.910
before we apply that non-linearity this
looks a lot like just a 2d line because

00:16:16.910 --> 00:16:16.920
looks a lot like just a 2d line because
 

00:16:16.920 --> 00:16:20.780
looks a lot like just a 2d line because
we have two inputs and it is we can

00:16:20.780 --> 00:16:20.790
we have two inputs and it is we can
 

00:16:20.790 --> 00:16:22.340
we have two inputs and it is we can
actually plot this line when it equals

00:16:22.340 --> 00:16:22.350
actually plot this line when it equals
 

00:16:22.350 --> 00:16:24.800
actually plot this line when it equals
zero in feature space so this is space

00:16:24.800 --> 00:16:24.810
zero in feature space so this is space
 

00:16:24.810 --> 00:16:27.140
zero in feature space so this is space
where I'm plotting x1 one of our

00:16:27.140 --> 00:16:27.150
where I'm plotting x1 one of our
 

00:16:27.150 --> 00:16:31.640
where I'm plotting x1 one of our
features on the x-axis and x2 the other

00:16:31.640 --> 00:16:31.650
features on the x-axis and x2 the other
 

00:16:31.650 --> 00:16:34.010
features on the x-axis and x2 the other
feature on the y-axis we plot that line

00:16:34.010 --> 00:16:34.020
feature on the y-axis we plot that line
 

00:16:34.020 --> 00:16:35.420
feature on the y-axis we plot that line
it's just the decision boundary

00:16:35.420 --> 00:16:35.430
it's just the decision boundary
 

00:16:35.430 --> 00:16:37.940
it's just the decision boundary
separating our entire space into two

00:16:37.940 --> 00:16:37.950
separating our entire space into two
 

00:16:37.950 --> 00:16:41.240
separating our entire space into two
subspaces now if I give you a new point

00:16:41.240 --> 00:16:41.250
subspaces now if I give you a new point
 

00:16:41.250 --> 00:16:44.690
subspaces now if I give you a new point
negative 1/2 and plot it on the sub in

00:16:44.690 --> 00:16:44.700
negative 1/2 and plot it on the sub in
 

00:16:44.700 --> 00:16:46.700
negative 1/2 and plot it on the sub in
this feature space depending on which

00:16:46.700 --> 00:16:46.710
this feature space depending on which
 

00:16:46.710 --> 00:16:48.620
this feature space depending on which
side of the line it falls on I can

00:16:48.620 --> 00:16:48.630
side of the line it falls on I can
 

00:16:48.630 --> 00:16:50.000
side of the line it falls on I can
automatically determine whether our

00:16:50.000 --> 00:16:50.010
automatically determine whether our
 

00:16:50.010 --> 00:16:52.760
automatically determine whether our
output is less than 0 or greater than 0

00:16:52.760 --> 00:16:52.770
output is less than 0 or greater than 0
 

00:16:52.770 --> 00:16:54.530
output is less than 0 or greater than 0
since our line represents a decision

00:16:54.530 --> 00:16:54.540
since our line represents a decision
 

00:16:54.540 --> 00:16:58.070
since our line represents a decision
boundary equal to 0 now we can follow

00:16:58.070 --> 00:16:58.080
boundary equal to 0 now we can follow
 

00:16:58.080 --> 00:16:59.740
boundary equal to 0 now we can follow
the math on the bottom and see that

00:16:59.740 --> 00:16:59.750
the math on the bottom and see that
 

00:16:59.750 --> 00:17:01.910
the math on the bottom and see that
computing the inside of this activation

00:17:01.910 --> 00:17:01.920
computing the inside of this activation
 

00:17:01.920 --> 00:17:06.079
computing the inside of this activation
function we get 1 minus 3 minus 2 sorry

00:17:06.079 --> 00:17:06.089
function we get 1 minus 3 minus 2 sorry
 

00:17:06.089 --> 00:17:12.490
function we get 1 minus 3 minus 2 sorry
minus 4 and we get minus 6 at the output

00:17:12.490 --> 00:17:12.500
minus 4 and we get minus 6 at the output
 

00:17:12.500 --> 00:17:14.600
minus 4 and we get minus 6 at the output
before we apply the activation function

00:17:14.600 --> 00:17:14.610
before we apply the activation function
 

00:17:14.610 --> 00:17:16.579
before we apply the activation function
once we apply the activation function we

00:17:16.579 --> 00:17:16.589
once we apply the activation function we
 

00:17:16.589 --> 00:17:20.079
once we apply the activation function we
get zero point zero zero two so negative

00:17:20.079 --> 00:17:20.089
get zero point zero zero two so negative
 

00:17:20.089 --> 00:17:22.730
get zero point zero zero two so negative
what was applied to the activation

00:17:22.730 --> 00:17:22.740
what was applied to the activation
 

00:17:22.740 --> 00:17:24.770
what was applied to the activation
function is negative because we fell on

00:17:24.770 --> 00:17:24.780
function is negative because we fell on
 

00:17:24.780 --> 00:17:29.990
function is negative because we fell on
the negative piece of this subspace well

00:17:29.990 --> 00:17:30.000
the negative piece of this subspace well
 

00:17:30.000 --> 00:17:31.460
the negative piece of this subspace well
if we remember with the sigmoid function

00:17:31.460 --> 00:17:31.470
if we remember with the sigmoid function
 

00:17:31.470 --> 00:17:33.080
if we remember with the sigmoid function
it actually divides our space into two

00:17:33.080 --> 00:17:33.090
it actually divides our space into two
 

00:17:33.090 --> 00:17:35.960
it actually divides our space into two
parts greater than 0.5 and less than 0.5

00:17:35.960 --> 00:17:35.970
parts greater than 0.5 and less than 0.5
 

00:17:35.970 --> 00:17:37.490
parts greater than 0.5 and less than 0.5
since we're modeling probabilities and

00:17:37.490 --> 00:17:37.500
since we're modeling probabilities and
 

00:17:37.500 --> 00:17:40.670
since we're modeling probabilities and
everything is between 0 &amp; 1 so actually

00:17:40.670 --> 00:17:40.680
everything is between 0 &amp; 1 so actually
 

00:17:40.680 --> 00:17:43.160
everything is between 0 &amp; 1 so actually
our decision boundary where the input to

00:17:43.160 --> 00:17:43.170
our decision boundary where the input to
 

00:17:43.170 --> 00:17:46.400
our decision boundary where the input to
our network equals 0 sorry the side the

00:17:46.400 --> 00:17:46.410
our network equals 0 sorry the side the
 

00:17:46.410 --> 00:17:48.050
our network equals 0 sorry the side the
input to our activation function equals

00:17:48.050 --> 00:17:48.060
input to our activation function equals
 

00:17:48.060 --> 00:17:50.460
input to our activation function equals
0 corresponds to

00:17:50.460 --> 00:17:50.470
0 corresponds to
 

00:17:50.470 --> 00:17:52.560
0 corresponds to
the output of our activation function

00:17:52.560 --> 00:17:52.570
the output of our activation function
 

00:17:52.570 --> 00:17:57.210
the output of our activation function
being greater than or less than 0.5 so

00:17:57.210 --> 00:17:57.220
being greater than or less than 0.5 so
 

00:17:57.220 --> 00:17:58.830
being greater than or less than 0.5 so
now that we have an idea of what a

00:17:58.830 --> 00:17:58.840
now that we have an idea of what a
 

00:17:58.840 --> 00:18:01.620
now that we have an idea of what a
perceptron is let's just start now by

00:18:01.620 --> 00:18:01.630
perceptron is let's just start now by
 

00:18:01.630 --> 00:18:04.169
perceptron is let's just start now by
understanding how we can compose these

00:18:04.169 --> 00:18:04.179
understanding how we can compose these
 

00:18:04.179 --> 00:18:06.930
understanding how we can compose these
perceptrons together to actually build

00:18:06.930 --> 00:18:06.940
perceptrons together to actually build
 

00:18:06.940 --> 00:18:10.320
perceptrons together to actually build
neural networks and let's see how this

00:18:10.320 --> 00:18:10.330
neural networks and let's see how this
 

00:18:10.330 --> 00:18:11.760
neural networks and let's see how this
all comes together so let's revisit our

00:18:11.760 --> 00:18:11.770
all comes together so let's revisit our
 

00:18:11.770 --> 00:18:15.210
all comes together so let's revisit our
previous diagram of the perceptron now

00:18:15.210 --> 00:18:15.220
previous diagram of the perceptron now
 

00:18:15.220 --> 00:18:16.409
previous diagram of the perceptron now
if there's a few things that you learned

00:18:16.409 --> 00:18:16.419
if there's a few things that you learned
 

00:18:16.419 --> 00:18:18.270
if there's a few things that you learned
from this class let this be one of them

00:18:18.270 --> 00:18:18.280
from this class let this be one of them
 

00:18:18.280 --> 00:18:19.890
from this class let this be one of them
and we'll keep repeating it over and

00:18:19.890 --> 00:18:19.900
and we'll keep repeating it over and
 

00:18:19.900 --> 00:18:23.370
and we'll keep repeating it over and
over in deep learning you do a dot

00:18:23.370 --> 00:18:23.380
over in deep learning you do a dot
 

00:18:23.380 --> 00:18:25.799
over in deep learning you do a dot
product you apply a bias and you add

00:18:25.799 --> 00:18:25.809
product you apply a bias and you add
 

00:18:25.809 --> 00:18:27.779
product you apply a bias and you add
your non-linearity you keep repeating

00:18:27.779 --> 00:18:27.789
your non-linearity you keep repeating
 

00:18:27.789 --> 00:18:30.539
your non-linearity you keep repeating
that many many times three each node

00:18:30.539 --> 00:18:30.549
that many many times three each node
 

00:18:30.549 --> 00:18:32.640
that many many times three each node
each neuron and your neural network and

00:18:32.640 --> 00:18:32.650
each neuron and your neural network and
 

00:18:32.650 --> 00:18:36.720
each neuron and your neural network and
that's a neural network so it's simplify

00:18:36.720 --> 00:18:36.730
that's a neural network so it's simplify
 

00:18:36.730 --> 00:18:38.630
that's a neural network so it's simplify
this diagram a little I remove the bias

00:18:38.630 --> 00:18:38.640
this diagram a little I remove the bias
 

00:18:38.640 --> 00:18:40.680
this diagram a little I remove the bias
since we were going to always have that

00:18:40.680 --> 00:18:40.690
since we were going to always have that
 

00:18:40.690 --> 00:18:42.510
since we were going to always have that
and we just take you for granted from

00:18:42.510 --> 00:18:42.520
and we just take you for granted from
 

00:18:42.520 --> 00:18:44.310
and we just take you for granted from
now on I'll remove all of the weight

00:18:44.310 --> 00:18:44.320
now on I'll remove all of the weight
 

00:18:44.320 --> 00:18:49.080
now on I'll remove all of the weight
labels for simplicity and note that Z is

00:18:49.080 --> 00:18:49.090
labels for simplicity and note that Z is
 

00:18:49.090 --> 00:18:51.210
labels for simplicity and note that Z is
just the input to our activation

00:18:51.210 --> 00:18:51.220
just the input to our activation
 

00:18:51.220 --> 00:18:53.850
just the input to our activation
function so that's just the dot product

00:18:53.850 --> 00:18:53.860
function so that's just the dot product
 

00:18:53.860 --> 00:18:56.880
function so that's just the dot product
plus our bias if we want the output of

00:18:56.880 --> 00:18:56.890
plus our bias if we want the output of
 

00:18:56.890 --> 00:18:58.919
plus our bias if we want the output of
the network Y we simply take Z and we

00:18:58.919 --> 00:18:58.929
the network Y we simply take Z and we
 

00:18:58.929 --> 00:19:02.669
the network Y we simply take Z and we
apply our non-linearity like before if

00:19:02.669 --> 00:19:02.679
apply our non-linearity like before if
 

00:19:02.679 --> 00:19:04.049
apply our non-linearity like before if
we want to define a multi output

00:19:04.049 --> 00:19:04.059
we want to define a multi output
 

00:19:04.059 --> 00:19:06.419
we want to define a multi output
perceptron it's very simple we just add

00:19:06.419 --> 00:19:06.429
perceptron it's very simple we just add
 

00:19:06.429 --> 00:19:08.490
perceptron it's very simple we just add
another perceptron now we have two

00:19:08.490 --> 00:19:08.500
another perceptron now we have two
 

00:19:08.500 --> 00:19:09.990
another perceptron now we have two
outputs y1 and y2

00:19:09.990 --> 00:19:10.000
outputs y1 and y2
 

00:19:10.000 --> 00:19:13.560
outputs y1 and y2
each one has weight matrices it has

00:19:13.560 --> 00:19:13.570
each one has weight matrices it has
 

00:19:13.570 --> 00:19:16.470
each one has weight matrices it has
weight vector theta corresponding to the

00:19:16.470 --> 00:19:16.480
weight vector theta corresponding to the
 

00:19:16.480 --> 00:19:23.700
weight vector theta corresponding to the
weight of each of the inputs now let's

00:19:23.700 --> 00:19:23.710
weight of each of the inputs now let's
 

00:19:23.710 --> 00:19:25.169
weight of each of the inputs now let's
suppose we want to go the next step

00:19:25.169 --> 00:19:25.179
suppose we want to go the next step
 

00:19:25.179 --> 00:19:28.710
suppose we want to go the next step
deeper we want to create now a single

00:19:28.710 --> 00:19:28.720
deeper we want to create now a single
 

00:19:28.720 --> 00:19:31.890
deeper we want to create now a single
layered neural network single layered

00:19:31.890 --> 00:19:31.900
layered neural network single layered
 

00:19:31.900 --> 00:19:33.570
layered neural network single layered
neural networks are actually not deep

00:19:33.570 --> 00:19:33.580
neural networks are actually not deep
 

00:19:33.580 --> 00:19:36.090
neural networks are actually not deep
networks yet they're only there's still

00:19:36.090 --> 00:19:36.100
networks yet they're only there's still
 

00:19:36.100 --> 00:19:37.740
networks yet they're only there's still
shallow networks they're only one layer

00:19:37.740 --> 00:19:37.750
shallow networks they're only one layer
 

00:19:37.750 --> 00:19:40.320
shallow networks they're only one layer
deep but let's look at the singledom

00:19:40.320 --> 00:19:40.330
deep but let's look at the singledom
 

00:19:40.330 --> 00:19:43.140
deep but let's look at the singledom
layered neural network where now all we

00:19:43.140 --> 00:19:43.150
layered neural network where now all we
 

00:19:43.150 --> 00:19:44.669
layered neural network where now all we
do is we have one hidden layer between

00:19:44.669 --> 00:19:44.679
do is we have one hidden layer between
 

00:19:44.679 --> 00:19:46.590
do is we have one hidden layer between
our inputs and outputs we call this a

00:19:46.590 --> 00:19:46.600
our inputs and outputs we call this a
 

00:19:46.600 --> 00:19:50.130
our inputs and outputs we call this a
hidden layer because it's states are not

00:19:50.130 --> 00:19:50.140
hidden layer because it's states are not
 

00:19:50.140 --> 00:19:51.840
hidden layer because it's states are not
directly observable they're not directly

00:19:51.840 --> 00:19:51.850
directly observable they're not directly
 

00:19:51.850 --> 00:19:56.549
directly observable they're not directly
enforced by by the AI designer we only

00:19:56.549 --> 00:19:56.559
enforced by by the AI designer we only
 

00:19:56.559 --> 00:19:59.039
enforced by by the AI designer we only
enforce the inputs and outputs typically

00:19:59.039 --> 00:19:59.049
enforce the inputs and outputs typically
 

00:19:59.049 --> 00:20:02.450
enforce the inputs and outputs typically
the states in the middle are hidden and

00:20:02.450 --> 00:20:02.460
the states in the middle are hidden and
 

00:20:02.460 --> 00:20:04.409
the states in the middle are hidden and
since we now have a transformer

00:20:04.409 --> 00:20:04.419
since we now have a transformer
 

00:20:04.419 --> 00:20:06.509
since we now have a transformer
to go from our input space to our hidden

00:20:06.509 --> 00:20:06.519
to go from our input space to our hidden
 

00:20:06.519 --> 00:20:08.849
to go from our input space to our hidden
hidden lair space and from our hidden

00:20:08.849 --> 00:20:08.859
hidden lair space and from our hidden
 

00:20:08.859 --> 00:20:11.820
hidden lair space and from our hidden
lair space to our output layer space we

00:20:11.820 --> 00:20:11.830
lair space to our output layer space we
 

00:20:11.830 --> 00:20:14.099
lair space to our output layer space we
actually need two weight matrices theta

00:20:14.099 --> 00:20:14.109
actually need two weight matrices theta
 

00:20:14.109 --> 00:20:16.440
actually need two weight matrices theta
1 and theta 2 corresponding to the

00:20:16.440 --> 00:20:16.450
1 and theta 2 corresponding to the
 

00:20:16.450 --> 00:20:21.450
1 and theta 2 corresponding to the
weight matrices of each layer now if we

00:20:21.450 --> 00:20:21.460
weight matrices of each layer now if we
 

00:20:21.460 --> 00:20:23.249
weight matrices of each layer now if we
look at just a single unit in that

00:20:23.249 --> 00:20:23.259
look at just a single unit in that
 

00:20:23.259 --> 00:20:25.229
look at just a single unit in that
hidden layer it's the exact same story

00:20:25.229 --> 00:20:25.239
hidden layer it's the exact same story
 

00:20:25.239 --> 00:20:27.720
hidden layer it's the exact same story
as before it's one perceptron we take

00:20:27.720 --> 00:20:27.730
as before it's one perceptron we take
 

00:20:27.730 --> 00:20:29.460
as before it's one perceptron we take
its top product with all of the X's that

00:20:29.460 --> 00:20:29.470
its top product with all of the X's that
 

00:20:29.470 --> 00:20:33.450
its top product with all of the X's that
came before it and we apply I'm sorry we

00:20:33.450 --> 00:20:33.460
came before it and we apply I'm sorry we
 

00:20:33.460 --> 00:20:34.739
came before it and we apply I'm sorry we
take the dot product of the X's that

00:20:34.739 --> 00:20:34.749
take the dot product of the X's that
 

00:20:34.749 --> 00:20:36.269
take the dot product of the X's that
came before with the weight matrices

00:20:36.269 --> 00:20:36.279
came before with the weight matrices
 

00:20:36.279 --> 00:20:39.659
came before with the weight matrices
theta is theta one in this case we apply

00:20:39.659 --> 00:20:39.669
theta is theta one in this case we apply
 

00:20:39.669 --> 00:20:42.720
theta is theta one in this case we apply
a bias to get Z 2 and if we look we're

00:20:42.720 --> 00:20:42.730
a bias to get Z 2 and if we look we're
 

00:20:42.730 --> 00:20:44.489
a bias to get Z 2 and if we look we're
to look at a different hidden unit let's

00:20:44.489 --> 00:20:44.499
to look at a different hidden unit let's
 

00:20:44.499 --> 00:20:47.279
to look at a different hidden unit let's
say Z 3 instead we would just take

00:20:47.279 --> 00:20:47.289
say Z 3 instead we would just take
 

00:20:47.289 --> 00:20:50.070
say Z 3 instead we would just take
different weight matrices different our

00:20:50.070 --> 00:20:50.080
different weight matrices different our
 

00:20:50.080 --> 00:20:52.499
different weight matrices different our
dot product to change our bias would

00:20:52.499 --> 00:20:52.509
dot product to change our bias would
 

00:20:52.509 --> 00:20:55.200
dot product to change our bias would
change but and this means that Z would

00:20:55.200 --> 00:20:55.210
change but and this means that Z would
 

00:20:55.210 --> 00:20:56.519
change but and this means that Z would
change which means this activation would

00:20:56.519 --> 00:20:56.529
change which means this activation would
 

00:20:56.529 --> 00:20:59.519
change which means this activation would
also be different so from now on I'm

00:20:59.519 --> 00:20:59.529
also be different so from now on I'm
 

00:20:59.529 --> 00:21:02.519
also be different so from now on I'm
going to use this symbol to denote what

00:21:02.519 --> 00:21:02.529
going to use this symbol to denote what
 

00:21:02.529 --> 00:21:04.259
going to use this symbol to denote what
is called as a fully connected layer and

00:21:04.259 --> 00:21:04.269
is called as a fully connected layer and
 

00:21:04.269 --> 00:21:05.369
is called as a fully connected layer and
that's what we've been talking about so

00:21:05.369 --> 00:21:05.379
that's what we've been talking about so
 

00:21:05.379 --> 00:21:07.619
that's what we've been talking about so
far so that's every node and one layer

00:21:07.619 --> 00:21:07.629
far so that's every node and one layer
 

00:21:07.629 --> 00:21:09.149
far so that's every node and one layer
is connected to every node and another

00:21:09.149 --> 00:21:09.159
is connected to every node and another
 

00:21:09.159 --> 00:21:12.299
is connected to every node and another
layer by these weight matrices and this

00:21:12.299 --> 00:21:12.309
layer by these weight matrices and this
 

00:21:12.309 --> 00:21:14.039
layer by these weight matrices and this
is really just for simplicity so I don't

00:21:14.039 --> 00:21:14.049
is really just for simplicity so I don't
 

00:21:14.049 --> 00:21:17.249
is really just for simplicity so I don't
have to keep redrawing those lines now

00:21:17.249 --> 00:21:17.259
have to keep redrawing those lines now
 

00:21:17.259 --> 00:21:18.389
have to keep redrawing those lines now
if we want to create a deep neural

00:21:18.389 --> 00:21:18.399
if we want to create a deep neural
 

00:21:18.399 --> 00:21:21.450
if we want to create a deep neural
network all we do is keep stacking these

00:21:21.450 --> 00:21:21.460
network all we do is keep stacking these
 

00:21:21.460 --> 00:21:23.549
network all we do is keep stacking these
layers and fully connected weights

00:21:23.549 --> 00:21:23.559
layers and fully connected weights
 

00:21:23.559 --> 00:21:26.279
layers and fully connected weights
between the layers it's that simple but

00:21:26.279 --> 00:21:26.289
between the layers it's that simple but
 

00:21:26.289 --> 00:21:27.899
between the layers it's that simple but
the underlying building block is that

00:21:27.899 --> 00:21:27.909
the underlying building block is that
 

00:21:27.909 --> 00:21:31.039
the underlying building block is that
single perceptron set single dot product

00:21:31.039 --> 00:21:31.049
single perceptron set single dot product
 

00:21:31.049 --> 00:21:33.919
single perceptron set single dot product
non-linearity and bias that's it

00:21:33.919 --> 00:21:33.929
non-linearity and bias that's it
 

00:21:33.929 --> 00:21:36.269
non-linearity and bias that's it
so this is really incredible because

00:21:36.269 --> 00:21:36.279
so this is really incredible because
 

00:21:36.279 --> 00:21:38.639
so this is really incredible because
something so simple at the foundation is

00:21:38.639 --> 00:21:38.649
something so simple at the foundation is
 

00:21:38.649 --> 00:21:40.710
something so simple at the foundation is
still able to create such incredible

00:21:40.710 --> 00:21:40.720
still able to create such incredible
 

00:21:40.720 --> 00:21:43.200
still able to create such incredible
algorithms and now let's see an example

00:21:43.200 --> 00:21:43.210
algorithms and now let's see an example
 

00:21:43.210 --> 00:21:44.940
algorithms and now let's see an example
of how we can actually apply neural

00:21:44.940 --> 00:21:44.950
of how we can actually apply neural
 

00:21:44.950 --> 00:21:47.700
of how we can actually apply neural
networks to a very important question

00:21:47.700 --> 00:21:47.710
networks to a very important question
 

00:21:47.710 --> 00:21:50.159
networks to a very important question
that I know you are all extremely

00:21:50.159 --> 00:21:50.169
that I know you are all extremely
 

00:21:50.169 --> 00:21:51.960
that I know you are all extremely
worried about you care a lot about

00:21:51.960 --> 00:21:51.970
worried about you care a lot about
 

00:21:51.970 --> 00:21:54.930
worried about you care a lot about
here's the question you want to build an

00:21:54.930 --> 00:21:54.940
here's the question you want to build an
 

00:21:54.940 --> 00:21:57.419
here's the question you want to build an
AI system that answers the following

00:21:57.419 --> 00:21:57.429
AI system that answers the following
 

00:21:57.429 --> 00:21:59.909
AI system that answers the following
question will I pass this class yes or

00:21:59.909 --> 00:21:59.919
question will I pass this class yes or
 

00:21:59.919 --> 00:22:03.899
question will I pass this class yes or
no one or zero is the output to do this

00:22:03.899 --> 00:22:03.909
no one or zero is the output to do this
 

00:22:03.909 --> 00:22:06.539
no one or zero is the output to do this
let's start by defining a simple two

00:22:06.539 --> 00:22:06.549
let's start by defining a simple two
 

00:22:06.549 --> 00:22:08.940
let's start by defining a simple two
feature model one feature is the number

00:22:08.940 --> 00:22:08.950
feature model one feature is the number
 

00:22:08.950 --> 00:22:11.070
feature model one feature is the number
of lectures that you attend the second

00:22:11.070 --> 00:22:11.080
of lectures that you attend the second
 

00:22:11.080 --> 00:22:12.299
of lectures that you attend the second
feature is the number of hours that you

00:22:12.299 --> 00:22:12.309
feature is the number of hours that you
 

00:22:12.309 --> 00:22:15.749
feature is the number of hours that you
spend on your final project let's plot

00:22:15.749 --> 00:22:15.759
spend on your final project let's plot
 

00:22:15.759 --> 00:22:18.120
spend on your final project let's plot
this data in our feature space

00:22:18.120 --> 00:22:18.130
this data in our feature space
 

00:22:18.130 --> 00:22:20.340
this data in our feature space
reply Greenpoint's are people who pass

00:22:20.340 --> 00:22:20.350
reply Greenpoint's are people who pass
 

00:22:20.350 --> 00:22:23.580
reply Greenpoint's are people who pass
red points are people I fail we want to

00:22:23.580 --> 00:22:23.590
red points are people I fail we want to
 

00:22:23.590 --> 00:22:26.910
red points are people I fail we want to
know given a new person this guy he

00:22:26.910 --> 00:22:26.920
know given a new person this guy he
 

00:22:26.920 --> 00:22:30.930
know given a new person this guy he
spent ersity they spent five hours on

00:22:30.930 --> 00:22:30.940
spent ersity they spent five hours on
 

00:22:30.940 --> 00:22:33.090
spent ersity they spent five hours on
their final project and we went to four

00:22:33.090 --> 00:22:33.100
their final project and we went to four
 

00:22:33.100 --> 00:22:35.670
their final project and we went to four
lectures we want to know did that person

00:22:35.670 --> 00:22:35.680
lectures we want to know did that person
 

00:22:35.680 --> 00:22:37.740
lectures we want to know did that person
pass or failed a class and we want to

00:22:37.740 --> 00:22:37.750
pass or failed a class and we want to
 

00:22:37.750 --> 00:22:38.970
pass or failed a class and we want to
build a neural network that will

00:22:38.970 --> 00:22:38.980
build a neural network that will
 

00:22:38.980 --> 00:22:43.200
build a neural network that will
determine this so let's do it we have

00:22:43.200 --> 00:22:43.210
determine this so let's do it we have
 

00:22:43.210 --> 00:22:45.720
determine this so let's do it we have
two inputs one is for the others five we

00:22:45.720 --> 00:22:45.730
two inputs one is for the others five we
 

00:22:45.730 --> 00:22:47.160
two inputs one is for the others five we
have one hidden layer with three units

00:22:47.160 --> 00:22:47.170
have one hidden layer with three units
 

00:22:47.170 --> 00:22:49.350
have one hidden layer with three units
and we want to see the final output

00:22:49.350 --> 00:22:49.360
and we want to see the final output
 

00:22:49.360 --> 00:22:51.360
and we want to see the final output
probability of passing this class and we

00:22:51.360 --> 00:22:51.370
probability of passing this class and we
 

00:22:51.370 --> 00:22:55.440
probability of passing this class and we
computed as 0.1 or 10% well that's

00:22:55.440 --> 00:22:55.450
computed as 0.1 or 10% well that's
 

00:22:55.450 --> 00:22:57.120
computed as 0.1 or 10% well that's
really bad news because actually this

00:22:57.120 --> 00:22:57.130
really bad news because actually this
 

00:22:57.130 --> 00:22:59.460
really bad news because actually this
person did pass the class they passed it

00:22:59.460 --> 00:22:59.470
person did pass the class they passed it
 

00:22:59.470 --> 00:23:03.030
person did pass the class they passed it
with probability one now can anyone tell

00:23:03.030 --> 00:23:03.040
with probability one now can anyone tell
 

00:23:03.040 --> 00:23:06.030
with probability one now can anyone tell
me why the neural network got this such

00:23:06.030 --> 00:23:06.040
me why the neural network got this such
 

00:23:06.040 --> 00:23:10.940
me why the neural network got this such
so wrong why I do this yeah it is

00:23:10.940 --> 00:23:10.950
 

00:23:10.950 --> 00:23:14.400
exactly so this network has never been

00:23:14.400 --> 00:23:14.410
exactly so this network has never been
 

00:23:14.410 --> 00:23:16.890
exactly so this network has never been
trained it's never seen any data it's

00:23:16.890 --> 00:23:16.900
trained it's never seen any data it's
 

00:23:16.900 --> 00:23:18.780
trained it's never seen any data it's
basically like a baby it's never learned

00:23:18.780 --> 00:23:18.790
basically like a baby it's never learned
 

00:23:18.790 --> 00:23:21.030
basically like a baby it's never learned
anything so we can't expect it to solve

00:23:21.030 --> 00:23:21.040
anything so we can't expect it to solve
 

00:23:21.040 --> 00:23:24.000
anything so we can't expect it to solve
a problem and knows nothing about so to

00:23:24.000 --> 00:23:24.010
a problem and knows nothing about so to
 

00:23:24.010 --> 00:23:26.070
a problem and knows nothing about so to
do this to tackle this problem of

00:23:26.070 --> 00:23:26.080
do this to tackle this problem of
 

00:23:26.080 --> 00:23:27.780
do this to tackle this problem of
training a neural network we have to

00:23:27.780 --> 00:23:27.790
training a neural network we have to
 

00:23:27.790 --> 00:23:29.220
training a neural network we have to
first define a couple of things so first

00:23:29.220 --> 00:23:29.230
first define a couple of things so first
 

00:23:29.230 --> 00:23:32.310
first define a couple of things so first
we'll talk about the loss the loss of a

00:23:32.310 --> 00:23:32.320
we'll talk about the loss the loss of a
 

00:23:32.320 --> 00:23:37.050
we'll talk about the loss the loss of a
network basically tells our algorithm or

00:23:37.050 --> 00:23:37.060
network basically tells our algorithm or
 

00:23:37.060 --> 00:23:41.070
network basically tells our algorithm or
our model how wrong our predictions are

00:23:41.070 --> 00:23:41.080
our model how wrong our predictions are
 

00:23:41.080 --> 00:23:44.280
our model how wrong our predictions are
from the ground truth so you can think

00:23:44.280 --> 00:23:44.290
from the ground truth so you can think
 

00:23:44.290 --> 00:23:46.500
from the ground truth so you can think
of this as a distance between our

00:23:46.500 --> 00:23:46.510
of this as a distance between our
 

00:23:46.510 --> 00:23:48.630
of this as a distance between our
predicted output and our actual output

00:23:48.630 --> 00:23:48.640
predicted output and our actual output
 

00:23:48.640 --> 00:23:51.150
predicted output and our actual output
if the two are very close if we predict

00:23:51.150 --> 00:23:51.160
if the two are very close if we predict
 

00:23:51.160 --> 00:23:52.680
if the two are very close if we predict
something that is very close to the true

00:23:52.680 --> 00:23:52.690
something that is very close to the true
 

00:23:52.690 --> 00:23:56.130
something that is very close to the true
output our loss is very low if we

00:23:56.130 --> 00:23:56.140
output our loss is very low if we
 

00:23:56.140 --> 00:23:58.680
output our loss is very low if we
predict something that is very far in a

00:23:58.680 --> 00:23:58.690
predict something that is very far in a
 

00:23:58.690 --> 00:24:01.220
predict something that is very far in a
high-level sense far like in distance

00:24:01.220 --> 00:24:01.230
high-level sense far like in distance
 

00:24:01.230 --> 00:24:03.870
high-level sense far like in distance
then our loss is very high and we want

00:24:03.870 --> 00:24:03.880
then our loss is very high and we want
 

00:24:03.880 --> 00:24:05.580
then our loss is very high and we want
to minimize this from happening as much

00:24:05.580 --> 00:24:05.590
to minimize this from happening as much
 

00:24:05.590 --> 00:24:08.520
to minimize this from happening as much
as possible now let's assume we're not

00:24:08.520 --> 00:24:08.530
as possible now let's assume we're not
 

00:24:08.530 --> 00:24:10.290
as possible now let's assume we're not
given just one data point one student

00:24:10.290 --> 00:24:10.300
given just one data point one student
 

00:24:10.300 --> 00:24:11.550
given just one data point one student
but we're given a whole class of

00:24:11.550 --> 00:24:11.560
but we're given a whole class of
 

00:24:11.560 --> 00:24:13.890
but we're given a whole class of
students so as previous data I used this

00:24:13.890 --> 00:24:13.900
students so as previous data I used this
 

00:24:13.900 --> 00:24:16.500
students so as previous data I used this
entire class from last year and if we

00:24:16.500 --> 00:24:16.510
entire class from last year and if we
 

00:24:16.510 --> 00:24:17.730
entire class from last year and if we
want to quantify what's called the

00:24:17.730 --> 00:24:17.740
want to quantify what's called the
 

00:24:17.740 --> 00:24:18.890
want to quantify what's called the
empirical loss

00:24:18.890 --> 00:24:18.900
empirical loss
 

00:24:18.900 --> 00:24:21.360
empirical loss
now we care about how the model did on

00:24:21.360 --> 00:24:21.370
now we care about how the model did on
 

00:24:21.370 --> 00:24:23.580
now we care about how the model did on
average over the entire data set not for

00:24:23.580 --> 00:24:23.590
average over the entire data set not for
 

00:24:23.590 --> 00:24:25.050
average over the entire data set not for
just a single student but across the

00:24:25.050 --> 00:24:25.060
just a single student but across the
 

00:24:25.060 --> 00:24:26.430
just a single student but across the
entire data set and how we do that is

00:24:26.430 --> 00:24:26.440
entire data set and how we do that is
 

00:24:26.440 --> 00:24:27.900
entire data set and how we do that is
very simple we just take the average of

00:24:27.900 --> 00:24:27.910
very simple we just take the average of
 

00:24:27.910 --> 00:24:30.420
very simple we just take the average of
the loss of each data point if we have n

00:24:30.420 --> 00:24:30.430
the loss of each data point if we have n
 

00:24:30.430 --> 00:24:31.620
the loss of each data point if we have n
students it's the

00:24:31.620 --> 00:24:31.630
students it's the
 

00:24:31.630 --> 00:24:35.010
students it's the
average over end data points this has

00:24:35.010 --> 00:24:35.020
average over end data points this has
 

00:24:35.020 --> 00:24:36.570
average over end data points this has
other names besides empirical law

00:24:36.570 --> 00:24:36.580
other names besides empirical law
 

00:24:36.580 --> 00:24:37.710
other names besides empirical law
sometimes people call it the objective

00:24:37.710 --> 00:24:37.720
sometimes people call it the objective
 

00:24:37.720 --> 00:24:42.210
sometimes people call it the objective
function the cost function etc all of

00:24:42.210 --> 00:24:42.220
function the cost function etc all of
 

00:24:42.220 --> 00:24:43.620
function the cost function etc all of
these terms are completely the same

00:24:43.620 --> 00:24:43.630
these terms are completely the same
 

00:24:43.630 --> 00:24:46.800
these terms are completely the same
thing now if we look at the problem of

00:24:46.800 --> 00:24:46.810
thing now if we look at the problem of
 

00:24:46.810 --> 00:24:48.600
thing now if we look at the problem of
binary classification predicting if you

00:24:48.600 --> 00:24:48.610
binary classification predicting if you
 

00:24:48.610 --> 00:24:52.190
binary classification predicting if you
pass or fail this class yes or no 1 or 0

00:24:52.190 --> 00:24:52.200
pass or fail this class yes or no 1 or 0
 

00:24:52.200 --> 00:24:54.540
pass or fail this class yes or no 1 or 0
we can actually use something that's

00:24:54.540 --> 00:24:54.550
we can actually use something that's
 

00:24:54.550 --> 00:24:57.440
we can actually use something that's
called the softmax cross entropy loss

00:24:57.440 --> 00:24:57.450
called the softmax cross entropy loss
 

00:24:57.450 --> 00:24:59.430
called the softmax cross entropy loss
now for those of you who aren't familiar

00:24:59.430 --> 00:24:59.440
now for those of you who aren't familiar
 

00:24:59.440 --> 00:25:02.070
now for those of you who aren't familiar
with cross entropy or entropy this is a

00:25:02.070 --> 00:25:02.080
with cross entropy or entropy this is a
 

00:25:02.080 --> 00:25:04.650
with cross entropy or entropy this is a
extremely powerful notion that was

00:25:04.650 --> 00:25:04.660
extremely powerful notion that was
 

00:25:04.660 --> 00:25:06.630
extremely powerful notion that was
actually developed or first introduced

00:25:06.630 --> 00:25:06.640
actually developed or first introduced
 

00:25:06.640 --> 00:25:11.040
actually developed or first introduced
here at MIT over 50 years ago by Claude

00:25:11.040 --> 00:25:11.050
here at MIT over 50 years ago by Claude
 

00:25:11.050 --> 00:25:14.790
here at MIT over 50 years ago by Claude
Shannon and his master's thesis like I

00:25:14.790 --> 00:25:14.800
Shannon and his master's thesis like I
 

00:25:14.800 --> 00:25:17.130
Shannon and his master's thesis like I
said this was 50 years ago it's huge in

00:25:17.130 --> 00:25:17.140
said this was 50 years ago it's huge in
 

00:25:17.140 --> 00:25:18.810
said this was 50 years ago it's huge in
the field of signal processing

00:25:18.810 --> 00:25:18.820
the field of signal processing
 

00:25:18.820 --> 00:25:21.300
the field of signal processing
thermodynamics really all over computer

00:25:21.300 --> 00:25:21.310
thermodynamics really all over computer
 

00:25:21.310 --> 00:25:23.690
thermodynamics really all over computer
science that seen in information theory

00:25:23.690 --> 00:25:23.700
science that seen in information theory
 

00:25:23.700 --> 00:25:26.940
science that seen in information theory
now instead of predicting a single one

00:25:26.940 --> 00:25:26.950
now instead of predicting a single one
 

00:25:26.950 --> 00:25:28.920
now instead of predicting a single one
or zero output yes or no let's suppose

00:25:28.920 --> 00:25:28.930
or zero output yes or no let's suppose
 

00:25:28.930 --> 00:25:32.100
or zero output yes or no let's suppose
we want to predict a continuous valued

00:25:32.100 --> 00:25:32.110
we want to predict a continuous valued
 

00:25:32.110 --> 00:25:34.740
we want to predict a continuous valued
function not will I pass this class but

00:25:34.740 --> 00:25:34.750
function not will I pass this class but
 

00:25:34.750 --> 00:25:36.390
function not will I pass this class but
what's the grade that I will get and

00:25:36.390 --> 00:25:36.400
what's the grade that I will get and
 

00:25:36.400 --> 00:25:39.060
what's the grade that I will get and
then as a percentage let's say 0 to 100

00:25:39.060 --> 00:25:39.070
then as a percentage let's say 0 to 100
 

00:25:39.070 --> 00:25:40.860
then as a percentage let's say 0 to 100
now we're no longer limited to 0 to 1

00:25:40.860 --> 00:25:40.870
now we're no longer limited to 0 to 1
 

00:25:40.870 --> 00:25:42.930
now we're no longer limited to 0 to 1
but can't actually output any real

00:25:42.930 --> 00:25:42.940
but can't actually output any real
 

00:25:42.940 --> 00:25:46.050
but can't actually output any real
number on the number line now instead of

00:25:46.050 --> 00:25:46.060
number on the number line now instead of
 

00:25:46.060 --> 00:25:47.640
number on the number line now instead of
using cross entropy we might want to use

00:25:47.640 --> 00:25:47.650
using cross entropy we might want to use
 

00:25:47.650 --> 00:25:49.500
using cross entropy we might want to use
a different loss and for this let's

00:25:49.500 --> 00:25:49.510
a different loss and for this let's
 

00:25:49.510 --> 00:25:50.790
a different loss and for this let's
think of something like a mean squared

00:25:50.790 --> 00:25:50.800
think of something like a mean squared
 

00:25:50.800 --> 00:25:53.010
think of something like a mean squared
error loss whereas your predicted and

00:25:53.010 --> 00:25:53.020
error loss whereas your predicted and
 

00:25:53.020 --> 00:25:54.750
error loss whereas your predicted and
your true output diverged from each

00:25:54.750 --> 00:25:54.760
your true output diverged from each
 

00:25:54.760 --> 00:25:57.720
your true output diverged from each
other the loss increases as a quadratic

00:25:57.720 --> 00:25:57.730
other the loss increases as a quadratic
 

00:25:57.730 --> 00:26:03.540
other the loss increases as a quadratic
function ok great so now let's put this

00:26:03.540 --> 00:26:03.550
function ok great so now let's put this
 

00:26:03.550 --> 00:26:05.790
function ok great so now let's put this
new loss information to the test and

00:26:05.790 --> 00:26:05.800
new loss information to the test and
 

00:26:05.800 --> 00:26:07.620
new loss information to the test and
actually learn how we can train a neural

00:26:07.620 --> 00:26:07.630
actually learn how we can train a neural
 

00:26:07.630 --> 00:26:12.690
actually learn how we can train a neural
network by quantifying its loss and

00:26:12.690 --> 00:26:12.700
network by quantifying its loss and
 

00:26:12.700 --> 00:26:15.090
network by quantifying its loss and
really if we go back to what the loss is

00:26:15.090 --> 00:26:15.100
really if we go back to what the loss is
 

00:26:15.100 --> 00:26:17.760
really if we go back to what the loss is
at the very high level the loss tells us

00:26:17.760 --> 00:26:17.770
at the very high level the loss tells us
 

00:26:17.770 --> 00:26:20.970
at the very high level the loss tells us
how the network is performing right that

00:26:20.970 --> 00:26:20.980
how the network is performing right that
 

00:26:20.980 --> 00:26:22.830
how the network is performing right that
loss tells us the accuracy of the

00:26:22.830 --> 00:26:22.840
loss tells us the accuracy of the
 

00:26:22.840 --> 00:26:25.290
loss tells us the accuracy of the
network on a set of examples and what we

00:26:25.290 --> 00:26:25.300
network on a set of examples and what we
 

00:26:25.300 --> 00:26:27.600
network on a set of examples and what we
want to do is basically minimize the

00:26:27.600 --> 00:26:27.610
want to do is basically minimize the
 

00:26:27.610 --> 00:26:31.770
want to do is basically minimize the
loss over our entire training set really

00:26:31.770 --> 00:26:31.780
loss over our entire training set really
 

00:26:31.780 --> 00:26:34.110
loss over our entire training set really
we want to find the set of parameters

00:26:34.110 --> 00:26:34.120
we want to find the set of parameters
 

00:26:34.120 --> 00:26:37.410
we want to find the set of parameters
theta such that that loss J of theta

00:26:37.410 --> 00:26:37.420
theta such that that loss J of theta
 

00:26:37.420 --> 00:26:41.040
theta such that that loss J of theta
that's our empirical loss is minimum so

00:26:41.040 --> 00:26:41.050
that's our empirical loss is minimum so
 

00:26:41.050 --> 00:26:43.470
that's our empirical loss is minimum so
remember J of theta takes as input theta

00:26:43.470 --> 00:26:43.480
remember J of theta takes as input theta
 

00:26:43.480 --> 00:26:45.450
remember J of theta takes as input theta
and theta is just our weight

00:26:45.450 --> 00:26:45.460
and theta is just our weight
 

00:26:45.460 --> 00:26:46.950
and theta is just our weight
so these are the things that actually

00:26:46.950 --> 00:26:46.960
so these are the things that actually
 

00:26:46.960 --> 00:26:55.169
so these are the things that actually
define our network remember that the

00:26:55.169 --> 00:26:55.179
define our network remember that the
 

00:26:55.179 --> 00:26:57.210
define our network remember that the
loss is just a function of these weights

00:26:57.210 --> 00:26:57.220
loss is just a function of these weights
 

00:26:57.220 --> 00:27:00.810
loss is just a function of these weights
if we want to think about the process of

00:27:00.810 --> 00:27:00.820
if we want to think about the process of
 

00:27:00.820 --> 00:27:03.570
if we want to think about the process of
training we can imagine this landscape

00:27:03.570 --> 00:27:03.580
training we can imagine this landscape
 

00:27:03.580 --> 00:27:05.220
training we can imagine this landscape
so if we only have two weights we can

00:27:05.220 --> 00:27:05.230
so if we only have two weights we can
 

00:27:05.230 --> 00:27:08.370
so if we only have two weights we can
plot this nice diagram like this theta

00:27:08.370 --> 00:27:08.380
plot this nice diagram like this theta
 

00:27:08.380 --> 00:27:10.080
plot this nice diagram like this theta
zero and theta one are our two weights

00:27:10.080 --> 00:27:10.090
zero and theta one are our two weights
 

00:27:10.090 --> 00:27:11.789
zero and theta one are our two weights
they're on the four they're on the

00:27:11.789 --> 00:27:11.799
they're on the four they're on the
 

00:27:11.799 --> 00:27:14.310
they're on the four they're on the
planar axis on the bottom J of theta

00:27:14.310 --> 00:27:14.320
planar axis on the bottom J of theta
 

00:27:14.320 --> 00:27:17.909
planar axis on the bottom J of theta
zero and theta one are plotted on the z

00:27:17.909 --> 00:27:17.919
zero and theta one are plotted on the z
 

00:27:17.919 --> 00:27:21.000
zero and theta one are plotted on the z
axis what we want to do is basically

00:27:21.000 --> 00:27:21.010
axis what we want to do is basically
 

00:27:21.010 --> 00:27:23.460
axis what we want to do is basically
find the minimum of this loss of this

00:27:23.460 --> 00:27:23.470
find the minimum of this loss of this
 

00:27:23.470 --> 00:27:25.350
find the minimum of this loss of this
landscape if we can find the minimum

00:27:25.350 --> 00:27:25.360
landscape if we can find the minimum
 

00:27:25.360 --> 00:27:28.110
landscape if we can find the minimum
then this tells us where our loss is the

00:27:28.110 --> 00:27:28.120
then this tells us where our loss is the
 

00:27:28.120 --> 00:27:30.600
then this tells us where our loss is the
smallest and this tells us where theta

00:27:30.600 --> 00:27:30.610
smallest and this tells us where theta
 

00:27:30.610 --> 00:27:33.210
smallest and this tells us where theta
want with where or what values of theta

00:27:33.210 --> 00:27:33.220
want with where or what values of theta
 

00:27:33.220 --> 00:27:35.639
want with where or what values of theta
zero and theta one we can use to attain

00:27:35.639 --> 00:27:35.649
zero and theta one we can use to attain
 

00:27:35.649 --> 00:27:40.789
zero and theta one we can use to attain
that minimum loss so how do we do this

00:27:40.789 --> 00:27:40.799
that minimum loss so how do we do this
 

00:27:40.799 --> 00:27:45.299
that minimum loss so how do we do this
well we start with a random guess we

00:27:45.299 --> 00:27:45.309
well we start with a random guess we
 

00:27:45.309 --> 00:27:47.580
well we start with a random guess we
pick a point theta zero theta one and we

00:27:47.580 --> 00:27:47.590
pick a point theta zero theta one and we
 

00:27:47.590 --> 00:27:50.370
pick a point theta zero theta one and we
start there we compute the gradient of

00:27:50.370 --> 00:27:50.380
start there we compute the gradient of
 

00:27:50.380 --> 00:27:52.740
start there we compute the gradient of
this point on the lost landscape that's

00:27:52.740 --> 00:27:52.750
this point on the lost landscape that's
 

00:27:52.750 --> 00:27:54.269
this point on the lost landscape that's
DJ D theta

00:27:54.269 --> 00:27:54.279
DJ D theta
 

00:27:54.279 --> 00:27:56.610
DJ D theta
it's how the loss is changing with

00:27:56.610 --> 00:27:56.620
it's how the loss is changing with
 

00:27:56.620 --> 00:28:00.480
it's how the loss is changing with
respect to each of the weights now this

00:28:00.480 --> 00:28:00.490
respect to each of the weights now this
 

00:28:00.490 --> 00:28:02.880
respect to each of the weights now this
gradient tells us the direction of

00:28:02.880 --> 00:28:02.890
gradient tells us the direction of
 

00:28:02.890 --> 00:28:05.460
gradient tells us the direction of
highest ascent not descent so this is

00:28:05.460 --> 00:28:05.470
highest ascent not descent so this is
 

00:28:05.470 --> 00:28:07.710
highest ascent not descent so this is
telling us the direction going towards

00:28:07.710 --> 00:28:07.720
telling us the direction going towards
 

00:28:07.720 --> 00:28:10.560
telling us the direction going towards
the top of the mountain so let's take a

00:28:10.560 --> 00:28:10.570
the top of the mountain so let's take a
 

00:28:10.570 --> 00:28:13.019
the top of the mountain so let's take a
small step in the opposite direction so

00:28:13.019 --> 00:28:13.029
small step in the opposite direction so
 

00:28:13.029 --> 00:28:15.899
small step in the opposite direction so
we negate our gradient and we adjust our

00:28:15.899 --> 00:28:15.909
we negate our gradient and we adjust our
 

00:28:15.909 --> 00:28:17.580
we negate our gradient and we adjust our
weight such that we step in the opposite

00:28:17.580 --> 00:28:17.590
weight such that we step in the opposite
 

00:28:17.590 --> 00:28:19.710
weight such that we step in the opposite
direction of that gradient such that we

00:28:19.710 --> 00:28:19.720
direction of that gradient such that we
 

00:28:19.720 --> 00:28:21.899
direction of that gradient such that we
move continuously towards the lowest

00:28:21.899 --> 00:28:21.909
move continuously towards the lowest
 

00:28:21.909 --> 00:28:25.769
move continuously towards the lowest
point in this landscape until we finally

00:28:25.769 --> 00:28:25.779
point in this landscape until we finally
 

00:28:25.779 --> 00:28:27.539
point in this landscape until we finally
converge at a local minima and then we

00:28:27.539 --> 00:28:27.549
converge at a local minima and then we
 

00:28:27.549 --> 00:28:30.870
converge at a local minima and then we
just stop so let's summarize this with

00:28:30.870 --> 00:28:30.880
just stop so let's summarize this with
 

00:28:30.880 --> 00:28:32.130
just stop so let's summarize this with
some pseudocode so we randomly

00:28:32.130 --> 00:28:32.140
some pseudocode so we randomly
 

00:28:32.140 --> 00:28:34.620
some pseudocode so we randomly
initialize our weights we loop until

00:28:34.620 --> 00:28:34.630
initialize our weights we loop until
 

00:28:34.630 --> 00:28:36.990
initialize our weights we loop until
convergence the following we compute the

00:28:36.990 --> 00:28:37.000
convergence the following we compute the
 

00:28:37.000 --> 00:28:39.450
convergence the following we compute the
gradient at that point and simply we

00:28:39.450 --> 00:28:39.460
gradient at that point and simply we
 

00:28:39.460 --> 00:28:41.789
gradient at that point and simply we
apply this update rule where the update

00:28:41.789 --> 00:28:41.799
apply this update rule where the update
 

00:28:41.799 --> 00:28:48.930
apply this update rule where the update
takes as input the negative gradient now

00:28:48.930 --> 00:28:48.940
takes as input the negative gradient now
 

00:28:48.940 --> 00:28:50.639
takes as input the negative gradient now
let's look at this term here this is the

00:28:50.639 --> 00:28:50.649
let's look at this term here this is the
 

00:28:50.649 --> 00:28:53.399
let's look at this term here this is the
gradient like I said it explains how the

00:28:53.399 --> 00:28:53.409
gradient like I said it explains how the
 

00:28:53.409 --> 00:28:55.889
gradient like I said it explains how the
lost changes with respect to each weight

00:28:55.889 --> 00:28:55.899
lost changes with respect to each weight
 

00:28:55.899 --> 00:28:58.139
lost changes with respect to each weight
in the network but I never actually told

00:28:58.139 --> 00:28:58.149
in the network but I never actually told
 

00:28:58.149 --> 00:28:59.310
in the network but I never actually told
you how to compute this

00:28:59.310 --> 00:28:59.320
you how to compute this
 

00:28:59.320 --> 00:29:01.710
you how to compute this
this is actually a big big issue in

00:29:01.710 --> 00:29:01.720
this is actually a big big issue in
 

00:29:01.720 --> 00:29:03.360
this is actually a big big issue in
neural networks I just kind of took it

00:29:03.360 --> 00:29:03.370
neural networks I just kind of took it
 

00:29:03.370 --> 00:29:05.640
neural networks I just kind of took it
for granted so now let's talk about this

00:29:05.640 --> 00:29:05.650
for granted so now let's talk about this
 

00:29:05.650 --> 00:29:07.020
for granted so now let's talk about this
process of actually computing this

00:29:07.020 --> 00:29:07.030
process of actually computing this
 

00:29:07.030 --> 00:29:08.400
process of actually computing this
gradient because it's not that gradient

00:29:08.400 --> 00:29:08.410
gradient because it's not that gradient
 

00:29:08.410 --> 00:29:09.930
gradient because it's not that gradient
you kind of helpless

00:29:09.930 --> 00:29:09.940
you kind of helpless
 

00:29:09.940 --> 00:29:12.540
you kind of helpless
you have no idea which way down is you

00:29:12.540 --> 00:29:12.550
you have no idea which way down is you
 

00:29:12.550 --> 00:29:15.230
you have no idea which way down is you
don't know where to go in your landscape

00:29:15.230 --> 00:29:15.240
don't know where to go in your landscape
 

00:29:15.240 --> 00:29:17.730
don't know where to go in your landscape
so let's consider a very simple neural

00:29:17.730 --> 00:29:17.740
so let's consider a very simple neural
 

00:29:17.740 --> 00:29:19.710
so let's consider a very simple neural
network probably the simplest neural

00:29:19.710 --> 00:29:19.720
network probably the simplest neural
 

00:29:19.720 --> 00:29:22.410
network probably the simplest neural
network in the world it contains one

00:29:22.410 --> 00:29:22.420
network in the world it contains one
 

00:29:22.420 --> 00:29:24.720
network in the world it contains one
hidden unit one hidden layer and one

00:29:24.720 --> 00:29:24.730
hidden unit one hidden layer and one
 

00:29:24.730 --> 00:29:28.980
hidden unit one hidden layer and one
output unit and we want to compute the

00:29:28.980 --> 00:29:28.990
output unit and we want to compute the
 

00:29:28.990 --> 00:29:31.980
output unit and we want to compute the
gradient of our loss J of theta with

00:29:31.980 --> 00:29:31.990
gradient of our loss J of theta with
 

00:29:31.990 --> 00:29:35.460
gradient of our loss J of theta with
respect to theta to just data to for now

00:29:35.460 --> 00:29:35.470
respect to theta to just data to for now
 

00:29:35.470 --> 00:29:38.010
respect to theta to just data to for now
so this tells us how a small change in

00:29:38.010 --> 00:29:38.020
so this tells us how a small change in
 

00:29:38.020 --> 00:29:39.960
so this tells us how a small change in
theta 2 will impact our final loss at

00:29:39.960 --> 00:29:39.970
theta 2 will impact our final loss at
 

00:29:39.970 --> 00:29:43.040
theta 2 will impact our final loss at
the output so let's write this out as a

00:29:43.040 --> 00:29:43.050
the output so let's write this out as a
 

00:29:43.050 --> 00:29:46.140
the output so let's write this out as a
derivative we can start by just applying

00:29:46.140 --> 00:29:46.150
derivative we can start by just applying
 

00:29:46.150 --> 00:29:49.710
derivative we can start by just applying
a chain rule because J of theta is

00:29:49.710 --> 00:29:49.720
a chain rule because J of theta is
 

00:29:49.720 --> 00:29:53.070
a chain rule because J of theta is
dependent on Y right so first we want to

00:29:53.070 --> 00:29:53.080
dependent on Y right so first we want to
 

00:29:53.080 --> 00:29:56.040
dependent on Y right so first we want to
back propagate through Y our output all

00:29:56.040 --> 00:29:56.050
back propagate through Y our output all
 

00:29:56.050 --> 00:30:00.360
back propagate through Y our output all
the way back to theta 2 we can do this

00:30:00.360 --> 00:30:00.370
the way back to theta 2 we can do this
 

00:30:00.370 --> 00:30:05.310
the way back to theta 2 we can do this
because Y our output Y is only dependent

00:30:05.310 --> 00:30:05.320
because Y our output Y is only dependent
 

00:30:05.320 --> 00:30:09.090
because Y our output Y is only dependent
on the input and theta 2 that's it so

00:30:09.090 --> 00:30:09.100
on the input and theta 2 that's it so
 

00:30:09.100 --> 00:30:10.890
on the input and theta 2 that's it so
we're able to just from that perceptron

00:30:10.890 --> 00:30:10.900
we're able to just from that perceptron
 

00:30:10.900 --> 00:30:12.240
we're able to just from that perceptron
equation that we wrote on the previous

00:30:12.240 --> 00:30:12.250
equation that we wrote on the previous
 

00:30:12.250 --> 00:30:15.150
equation that we wrote on the previous
slide compute a closed-form gradient or

00:30:15.150 --> 00:30:15.160
slide compute a closed-form gradient or
 

00:30:15.160 --> 00:30:17.870
slide compute a closed-form gradient or
closed form derivative of that function

00:30:17.870 --> 00:30:17.880
closed form derivative of that function
 

00:30:17.880 --> 00:30:20.640
closed form derivative of that function
now let's suppose I change theta to 2

00:30:20.640 --> 00:30:20.650
now let's suppose I change theta to 2
 

00:30:20.650 --> 00:30:23.520
now let's suppose I change theta to 2
theta 1 and I want to compute the same

00:30:23.520 --> 00:30:23.530
theta 1 and I want to compute the same
 

00:30:23.530 --> 00:30:25.770
theta 1 and I want to compute the same
thing but now for the previous layer and

00:30:25.770 --> 00:30:25.780
thing but now for the previous layer and
 

00:30:25.780 --> 00:30:30.110
thing but now for the previous layer and
the previous weight all we need to do is

00:30:30.110 --> 00:30:30.120
the previous weight all we need to do is
 

00:30:30.120 --> 00:30:32.670
the previous weight all we need to do is
apply the chain rule one more time back

00:30:32.670 --> 00:30:32.680
apply the chain rule one more time back
 

00:30:32.680 --> 00:30:34.980
apply the chain rule one more time back
propagate those gradients that we

00:30:34.980 --> 00:30:34.990
propagate those gradients that we
 

00:30:34.990 --> 00:30:37.490
propagate those gradients that we
previously computed one layer further

00:30:37.490 --> 00:30:37.500
previously computed one layer further
 

00:30:37.500 --> 00:30:41.040
previously computed one layer further
it's the same story again we can do this

00:30:41.040 --> 00:30:41.050
it's the same story again we can do this
 

00:30:41.050 --> 00:30:45.210
it's the same story again we can do this
for the same reason this is because z1

00:30:45.210 --> 00:30:45.220
for the same reason this is because z1
 

00:30:45.220 --> 00:30:49.500
for the same reason this is because z1
our hidden state is only dependent on

00:30:49.500 --> 00:30:49.510
our hidden state is only dependent on
 

00:30:49.510 --> 00:30:53.190
our hidden state is only dependent on
our previous input X and that single

00:30:53.190 --> 00:30:53.200
our previous input X and that single
 

00:30:53.200 --> 00:30:57.990
our previous input X and that single
weight theta one now the process of back

00:30:57.990 --> 00:30:58.000
weight theta one now the process of back
 

00:30:58.000 --> 00:31:02.280
weight theta one now the process of back
propagation is basically you repeat this

00:31:02.280 --> 00:31:02.290
propagation is basically you repeat this
 

00:31:02.290 --> 00:31:04.620
propagation is basically you repeat this
process over and over again for every

00:31:04.620 --> 00:31:04.630
process over and over again for every
 

00:31:04.630 --> 00:31:06.810
process over and over again for every
way in your network until you compute

00:31:06.810 --> 00:31:06.820
way in your network until you compute
 

00:31:06.820 --> 00:31:09.660
way in your network until you compute
that gradient DJ D theta and you can use

00:31:09.660 --> 00:31:09.670
that gradient DJ D theta and you can use
 

00:31:09.670 --> 00:31:11.100
that gradient DJ D theta and you can use
that as part of your optimization

00:31:11.100 --> 00:31:11.110
that as part of your optimization
 

00:31:11.110 --> 00:31:12.330
that as part of your optimization
process

00:31:12.330 --> 00:31:12.340
process
 

00:31:12.340 --> 00:31:16.590
process
to find your local minimum now in theory

00:31:16.590 --> 00:31:16.600
to find your local minimum now in theory
 

00:31:16.600 --> 00:31:19.529
to find your local minimum now in theory
that sounds pretty simple I hope I mean

00:31:19.529 --> 00:31:19.539
that sounds pretty simple I hope I mean
 

00:31:19.539 --> 00:31:21.060
that sounds pretty simple I hope I mean
we just talked about some basic chain

00:31:21.060 --> 00:31:21.070
we just talked about some basic chain
 

00:31:21.070 --> 00:31:23.700
we just talked about some basic chain
rules but let's actually touch on some

00:31:23.700 --> 00:31:23.710
rules but let's actually touch on some
 

00:31:23.710 --> 00:31:26.399
rules but let's actually touch on some
insights on training these networks and

00:31:26.399 --> 00:31:26.409
insights on training these networks and
 

00:31:26.409 --> 00:31:28.019
insights on training these networks and
computing back propagation in practice

00:31:28.019 --> 00:31:28.029
computing back propagation in practice
 

00:31:28.029 --> 00:31:30.720
computing back propagation in practice
now the picture I showed you before is

00:31:30.720 --> 00:31:30.730
now the picture I showed you before is
 

00:31:30.730 --> 00:31:32.279
now the picture I showed you before is
not really accurate for modern deep

00:31:32.279 --> 00:31:32.289
not really accurate for modern deep
 

00:31:32.289 --> 00:31:34.260
not really accurate for modern deep
neural network architectures modern deep

00:31:34.260 --> 00:31:34.270
neural network architectures modern deep
 

00:31:34.270 --> 00:31:35.549
neural network architectures modern deep
neural network architectures are

00:31:35.549 --> 00:31:35.559
neural network architectures are
 

00:31:35.559 --> 00:31:38.850
neural network architectures are
extremely non convex this is an

00:31:38.850 --> 00:31:38.860
extremely non convex this is an
 

00:31:38.860 --> 00:31:41.159
extremely non convex this is an
illustration or a visualization of the

00:31:41.159 --> 00:31:41.169
illustration or a visualization of the
 

00:31:41.169 --> 00:31:43.230
illustration or a visualization of the
landscape like I've plotted before but

00:31:43.230 --> 00:31:43.240
landscape like I've plotted before but
 

00:31:43.240 --> 00:31:45.960
landscape like I've plotted before but
of a real deep neural network of ResNet

00:31:45.960 --> 00:31:45.970
of a real deep neural network of ResNet
 

00:31:45.970 --> 00:31:49.409
of a real deep neural network of ResNet
50 to be precise this was actually taken

00:31:49.409 --> 00:31:49.419
50 to be precise this was actually taken
 

00:31:49.419 --> 00:31:52.260
50 to be precise this was actually taken
from a paper published about a month ago

00:31:52.260 --> 00:31:52.270
from a paper published about a month ago
 

00:31:52.270 --> 00:31:54.210
from a paper published about a month ago
where the authors attempt to visualize

00:31:54.210 --> 00:31:54.220
where the authors attempt to visualize
 

00:31:54.220 --> 00:31:57.120
where the authors attempt to visualize
the lost landscape to show how difficult

00:31:57.120 --> 00:31:57.130
the lost landscape to show how difficult
 

00:31:57.130 --> 00:31:59.370
the lost landscape to show how difficult
gradient descent can actually be so

00:31:59.370 --> 00:31:59.380
gradient descent can actually be so
 

00:31:59.380 --> 00:32:00.840
gradient descent can actually be so
there's a possibility that you can get

00:32:00.840 --> 00:32:00.850
there's a possibility that you can get
 

00:32:00.850 --> 00:32:02.700
there's a possibility that you can get
lost in any one of these local minima

00:32:02.700 --> 00:32:02.710
lost in any one of these local minima
 

00:32:02.710 --> 00:32:03.659
lost in any one of these local minima
there's no guarantee that you'll

00:32:03.659 --> 00:32:03.669
there's no guarantee that you'll
 

00:32:03.669 --> 00:32:07.139
there's no guarantee that you'll
actually find a true global minimum so

00:32:07.139 --> 00:32:07.149
actually find a true global minimum so
 

00:32:07.149 --> 00:32:08.760
actually find a true global minimum so
let's recall that update equation that

00:32:08.760 --> 00:32:08.770
let's recall that update equation that
 

00:32:08.770 --> 00:32:12.090
let's recall that update equation that
we defined during gradient descent let's

00:32:12.090 --> 00:32:12.100
we defined during gradient descent let's
 

00:32:12.100 --> 00:32:13.980
we defined during gradient descent let's
take a look at this term here this is

00:32:13.980 --> 00:32:13.990
take a look at this term here this is
 

00:32:13.990 --> 00:32:15.570
take a look at this term here this is
the learning rate I didn't talk too much

00:32:15.570 --> 00:32:15.580
the learning rate I didn't talk too much
 

00:32:15.580 --> 00:32:18.690
the learning rate I didn't talk too much
about it but this basically determines

00:32:18.690 --> 00:32:18.700
about it but this basically determines
 

00:32:18.700 --> 00:32:21.090
about it but this basically determines
how large of a step we take in the

00:32:21.090 --> 00:32:21.100
how large of a step we take in the
 

00:32:21.100 --> 00:32:23.430
how large of a step we take in the
direction of our gradient and in

00:32:23.430 --> 00:32:23.440
direction of our gradient and in
 

00:32:23.440 --> 00:32:25.529
direction of our gradient and in
practice setting this learning rate it's

00:32:25.529 --> 00:32:25.539
practice setting this learning rate it's
 

00:32:25.539 --> 00:32:27.570
practice setting this learning rate it's
just a number but setting it can be very

00:32:27.570 --> 00:32:27.580
just a number but setting it can be very
 

00:32:27.580 --> 00:32:29.850
just a number but setting it can be very
difficult if we set the learning rate

00:32:29.850 --> 00:32:29.860
difficult if we set the learning rate
 

00:32:29.860 --> 00:32:33.000
difficult if we set the learning rate
too low then the model may get stuck in

00:32:33.000 --> 00:32:33.010
too low then the model may get stuck in
 

00:32:33.010 --> 00:32:34.470
too low then the model may get stuck in
a local minima and may never actually

00:32:34.470 --> 00:32:34.480
a local minima and may never actually
 

00:32:34.480 --> 00:32:36.269
a local minima and may never actually
find its way out of that local minima

00:32:36.269 --> 00:32:36.279
find its way out of that local minima
 

00:32:36.279 --> 00:32:38.519
find its way out of that local minima
because at the bottom a local minima

00:32:38.519 --> 00:32:38.529
because at the bottom a local minima
 

00:32:38.529 --> 00:32:40.230
because at the bottom a local minima
obviously your gradient is 0 so it's

00:32:40.230 --> 00:32:40.240
obviously your gradient is 0 so it's
 

00:32:40.240 --> 00:32:43.169
obviously your gradient is 0 so it's
just going to stop moving if I set the

00:32:43.169 --> 00:32:43.179
just going to stop moving if I set the
 

00:32:43.179 --> 00:32:44.909
just going to stop moving if I set the
learning rate to large it could

00:32:44.909 --> 00:32:44.919
learning rate to large it could
 

00:32:44.919 --> 00:32:47.399
learning rate to large it could
overshoot and actually diverge our model

00:32:47.399 --> 00:32:47.409
overshoot and actually diverge our model
 

00:32:47.409 --> 00:32:52.769
overshoot and actually diverge our model
could blow up ok ideally we want to use

00:32:52.769 --> 00:32:52.779
could blow up ok ideally we want to use
 

00:32:52.779 --> 00:32:54.630
could blow up ok ideally we want to use
learning rates that are large enough to

00:32:54.630 --> 00:32:54.640
learning rates that are large enough to
 

00:32:54.640 --> 00:32:58.590
learning rates that are large enough to
avoid local minima but also still

00:32:58.590 --> 00:32:58.600
avoid local minima but also still
 

00:32:58.600 --> 00:33:00.299
avoid local minima but also still
converge to our global minima so they

00:33:00.299 --> 00:33:00.309
converge to our global minima so they
 

00:33:00.309 --> 00:33:03.180
converge to our global minima so they
can overshoot just enough to avoid some

00:33:03.180 --> 00:33:03.190
can overshoot just enough to avoid some
 

00:33:03.190 --> 00:33:05.220
can overshoot just enough to avoid some
local local minima but then converge to

00:33:05.220 --> 00:33:05.230
local local minima but then converge to
 

00:33:05.230 --> 00:33:08.850
local local minima but then converge to
our global minima now how can we

00:33:08.850 --> 00:33:08.860
our global minima now how can we
 

00:33:08.860 --> 00:33:11.250
our global minima now how can we
actually set the learning rate well one

00:33:11.250 --> 00:33:11.260
actually set the learning rate well one
 

00:33:11.260 --> 00:33:12.720
actually set the learning rate well one
idea is let's just try a lot of

00:33:12.720 --> 00:33:12.730
idea is let's just try a lot of
 

00:33:12.730 --> 00:33:14.399
idea is let's just try a lot of
different things and see what works best

00:33:14.399 --> 00:33:14.409
different things and see what works best
 

00:33:14.409 --> 00:33:16.620
different things and see what works best
but I don't really like the solution

00:33:16.620 --> 00:33:16.630
but I don't really like the solution
 

00:33:16.630 --> 00:33:18.180
but I don't really like the solution
let's try and see if we can be a little

00:33:18.180 --> 00:33:18.190
let's try and see if we can be a little
 

00:33:18.190 --> 00:33:21.510
let's try and see if we can be a little
smarter than that how about we tried to

00:33:21.510 --> 00:33:21.520
smarter than that how about we tried to
 

00:33:21.520 --> 00:33:24.510
smarter than that how about we tried to
build an adaptive algorithm that changes

00:33:24.510 --> 00:33:24.520
build an adaptive algorithm that changes
 

00:33:24.520 --> 00:33:25.480
build an adaptive algorithm that changes
its learning rate

00:33:25.480 --> 00:33:25.490
its learning rate
 

00:33:25.490 --> 00:33:28.690
its learning rate
as training happens so this is a

00:33:28.690 --> 00:33:28.700
as training happens so this is a
 

00:33:28.700 --> 00:33:30.400
as training happens so this is a
learning rate that actually adapts to

00:33:30.400 --> 00:33:30.410
learning rate that actually adapts to
 

00:33:30.410 --> 00:33:33.010
learning rate that actually adapts to
the landscape that it's in so the

00:33:33.010 --> 00:33:33.020
the landscape that it's in so the
 

00:33:33.020 --> 00:33:34.390
the landscape that it's in so the
learning rate is no longer a fixed

00:33:34.390 --> 00:33:34.400
learning rate is no longer a fixed
 

00:33:34.400 --> 00:33:35.980
learning rate is no longer a fixed
number it can change it can go up and

00:33:35.980 --> 00:33:35.990
number it can change it can go up and
 

00:33:35.990 --> 00:33:38.560
number it can change it can go up and
down and this will change depending on

00:33:38.560 --> 00:33:38.570
down and this will change depending on
 

00:33:38.570 --> 00:33:42.280
down and this will change depending on
the location that that the update is

00:33:42.280 --> 00:33:42.290
the location that that the update is
 

00:33:42.290 --> 00:33:44.200
the location that that the update is
currently at the gradient in that

00:33:44.200 --> 00:33:44.210
currently at the gradient in that
 

00:33:44.210 --> 00:33:46.750
currently at the gradient in that
location may be how fast were learning

00:33:46.750 --> 00:33:46.760
location may be how fast were learning
 

00:33:46.760 --> 00:33:48.640
location may be how fast were learning
and many other many other possible

00:33:48.640 --> 00:33:48.650
and many other many other possible
 

00:33:48.650 --> 00:33:53.049
and many other many other possible
situations in fact this process of

00:33:53.049 --> 00:33:53.059
situations in fact this process of
 

00:33:53.059 --> 00:33:55.450
situations in fact this process of
optimization in in deep neural networks

00:33:55.450 --> 00:33:55.460
optimization in in deep neural networks
 

00:33:55.460 --> 00:33:58.060
optimization in in deep neural networks
and non convex situation has been

00:33:58.060 --> 00:33:58.070
and non convex situation has been
 

00:33:58.070 --> 00:34:00.850
and non convex situation has been
extremely explored there's many many

00:34:00.850 --> 00:34:00.860
extremely explored there's many many
 

00:34:00.860 --> 00:34:04.150
extremely explored there's many many
many algorithms for computing adaptive

00:34:04.150 --> 00:34:04.160
many algorithms for computing adaptive
 

00:34:04.160 --> 00:34:06.400
many algorithms for computing adaptive
learning rates and here are some

00:34:06.400 --> 00:34:06.410
learning rates and here are some
 

00:34:06.410 --> 00:34:08.710
learning rates and here are some
examples that we encourage you to try

00:34:08.710 --> 00:34:08.720
examples that we encourage you to try
 

00:34:08.720 --> 00:34:11.050
examples that we encourage you to try
out during your labs to see what works

00:34:11.050 --> 00:34:11.060
out during your labs to see what works
 

00:34:11.060 --> 00:34:13.540
out during your labs to see what works
best and for your problems especially

00:34:13.540 --> 00:34:13.550
best and for your problems especially
 

00:34:13.550 --> 00:34:16.540
best and for your problems especially
real-world problems things can change a

00:34:16.540 --> 00:34:16.550
real-world problems things can change a
 

00:34:16.550 --> 00:34:18.070
real-world problems things can change a
lot depending on what you learn in

00:34:18.070 --> 00:34:18.080
lot depending on what you learn in
 

00:34:18.080 --> 00:34:20.970
lot depending on what you learn in
lecture and what really works in lab and

00:34:20.970 --> 00:34:20.980
lecture and what really works in lab and
 

00:34:20.980 --> 00:34:23.710
lecture and what really works in lab and
we encourage you to just experiment get

00:34:23.710 --> 00:34:23.720
we encourage you to just experiment get
 

00:34:23.720 --> 00:34:25.030
we encourage you to just experiment get
some intuition about each of these

00:34:25.030 --> 00:34:25.040
some intuition about each of these
 

00:34:25.040 --> 00:34:26.470
some intuition about each of these
learning rates and really understand

00:34:26.470 --> 00:34:26.480
learning rates and really understand
 

00:34:26.480 --> 00:34:31.149
learning rates and really understand
them at a higher level so I want to

00:34:31.149 --> 00:34:31.159
them at a higher level so I want to
 

00:34:31.159 --> 00:34:32.669
them at a higher level so I want to
continue this talk and really talk about

00:34:32.669 --> 00:34:32.679
continue this talk and really talk about
 

00:34:32.679 --> 00:34:34.659
continue this talk and really talk about
more of the practice of deep neural

00:34:34.659 --> 00:34:34.669
more of the practice of deep neural
 

00:34:34.669 --> 00:34:37.690
more of the practice of deep neural
networks this incredibly powerful notion

00:34:37.690 --> 00:34:37.700
networks this incredibly powerful notion
 

00:34:37.700 --> 00:34:43.080
networks this incredibly powerful notion
of mini batching and I'll focus for now

00:34:43.080 --> 00:34:43.090
of mini batching and I'll focus for now
 

00:34:43.090 --> 00:34:45.820
of mini batching and I'll focus for now
if we go back to this gradient descent

00:34:45.820 --> 00:34:45.830
if we go back to this gradient descent
 

00:34:45.830 --> 00:34:47.349
if we go back to this gradient descent
algorithm this is the same one that we

00:34:47.349 --> 00:34:47.359
algorithm this is the same one that we
 

00:34:47.359 --> 00:34:49.840
algorithm this is the same one that we
saw before and let's look at this term

00:34:49.840 --> 00:34:49.850
saw before and let's look at this term
 

00:34:49.850 --> 00:34:51.700
saw before and let's look at this term
again so we found out how to compute

00:34:51.700 --> 00:34:51.710
again so we found out how to compute
 

00:34:51.710 --> 00:34:54.760
again so we found out how to compute
this term using back propagation but

00:34:54.760 --> 00:34:54.770
this term using back propagation but
 

00:34:54.770 --> 00:34:56.409
this term using back propagation but
actually what I didn't tell you is that

00:34:56.409 --> 00:34:56.419
actually what I didn't tell you is that
 

00:34:56.419 --> 00:34:58.960
actually what I didn't tell you is that
the computation here is extremely calm

00:34:58.960 --> 00:34:58.970
the computation here is extremely calm
 

00:34:58.970 --> 00:35:01.720
the computation here is extremely calm
is extremely expensive we have a lot of

00:35:01.720 --> 00:35:01.730
is extremely expensive we have a lot of
 

00:35:01.730 --> 00:35:03.940
is extremely expensive we have a lot of
data points potentially in our data set

00:35:03.940 --> 00:35:03.950
data points potentially in our data set
 

00:35:03.950 --> 00:35:06.849
data points potentially in our data set
and this takes as input a summation over

00:35:06.849 --> 00:35:06.859
and this takes as input a summation over
 

00:35:06.859 --> 00:35:08.920
and this takes as input a summation over
all of those data points so if our data

00:35:08.920 --> 00:35:08.930
all of those data points so if our data
 

00:35:08.930 --> 00:35:11.470
all of those data points so if our data
set is millions of examples large which

00:35:11.470 --> 00:35:11.480
set is millions of examples large which
 

00:35:11.480 --> 00:35:13.870
set is millions of examples large which
is not that large and the realm of

00:35:13.870 --> 00:35:13.880
is not that large and the realm of
 

00:35:13.880 --> 00:35:16.540
is not that large and the realm of
today's deep neural networks but this

00:35:16.540 --> 00:35:16.550
today's deep neural networks but this
 

00:35:16.550 --> 00:35:18.220
today's deep neural networks but this
can be extremely expensive just for one

00:35:18.220 --> 00:35:18.230
can be extremely expensive just for one
 

00:35:18.230 --> 00:35:19.510
can be extremely expensive just for one
iteration so we can compute this on

00:35:19.510 --> 00:35:19.520
iteration so we can compute this on
 

00:35:19.520 --> 00:35:22.660
iteration so we can compute this on
every iteration instead let's create a

00:35:22.660 --> 00:35:22.670
every iteration instead let's create a
 

00:35:22.670 --> 00:35:24.040
every iteration instead let's create a
variant of this algorithm called

00:35:24.040 --> 00:35:24.050
variant of this algorithm called
 

00:35:24.050 --> 00:35:26.260
variant of this algorithm called
stochastic gradient descent where we

00:35:26.260 --> 00:35:26.270
stochastic gradient descent where we
 

00:35:26.270 --> 00:35:28.120
stochastic gradient descent where we
compute the gradient just using a single

00:35:28.120 --> 00:35:28.130
compute the gradient just using a single
 

00:35:28.130 --> 00:35:31.660
compute the gradient just using a single
training example now this is nice

00:35:31.660 --> 00:35:31.670
training example now this is nice
 

00:35:31.670 --> 00:35:33.190
training example now this is nice
because it's really easy to compute the

00:35:33.190 --> 00:35:33.200
because it's really easy to compute the
 

00:35:33.200 --> 00:35:34.270
because it's really easy to compute the
gradient for a single training example

00:35:34.270 --> 00:35:34.280
gradient for a single training example
 

00:35:34.280 --> 00:35:36.640
gradient for a single training example
it's not nearly as intense as over the

00:35:36.640 --> 00:35:36.650
it's not nearly as intense as over the
 

00:35:36.650 --> 00:35:39.310
it's not nearly as intense as over the
entire training set but as the name

00:35:39.310 --> 00:35:39.320
entire training set but as the name
 

00:35:39.320 --> 00:35:41.110
entire training set but as the name
might suggest this is a more stochastic

00:35:41.110 --> 00:35:41.120
might suggest this is a more stochastic
 

00:35:41.120 --> 00:35:44.500
might suggest this is a more stochastic
estimate it's much more noisy it can

00:35:44.500 --> 00:35:44.510
estimate it's much more noisy it can
 

00:35:44.510 --> 00:35:46.450
estimate it's much more noisy it can
make us jump around the landscape in

00:35:46.450 --> 00:35:46.460
make us jump around the landscape in
 

00:35:46.460 --> 00:35:48.190
make us jump around the landscape in
ways that we didn't anticipate doesn't

00:35:48.190 --> 00:35:48.200
ways that we didn't anticipate doesn't
 

00:35:48.200 --> 00:35:50.050
ways that we didn't anticipate doesn't
actually represent the true gradient of

00:35:50.050 --> 00:35:50.060
actually represent the true gradient of
 

00:35:50.060 --> 00:35:51.700
actually represent the true gradient of
our data set because it's only a single

00:35:51.700 --> 00:35:51.710
our data set because it's only a single
 

00:35:51.710 --> 00:35:54.760
our data set because it's only a single
point so what's the middle ground how

00:35:54.760 --> 00:35:54.770
point so what's the middle ground how
 

00:35:54.770 --> 00:35:58.750
point so what's the middle ground how
about we define a mini batch of B data

00:35:58.750 --> 00:35:58.760
about we define a mini batch of B data
 

00:35:58.760 --> 00:36:01.330
about we define a mini batch of B data
points compute the average gradient

00:36:01.330 --> 00:36:01.340
points compute the average gradient
 

00:36:01.340 --> 00:36:04.150
points compute the average gradient
across those B data points and actually

00:36:04.150 --> 00:36:04.160
across those B data points and actually
 

00:36:04.160 --> 00:36:06.070
across those B data points and actually
use that as an estimate of our true

00:36:06.070 --> 00:36:06.080
use that as an estimate of our true
 

00:36:06.080 --> 00:36:08.590
use that as an estimate of our true
gradient now this is much faster than

00:36:08.590 --> 00:36:08.600
gradient now this is much faster than
 

00:36:08.600 --> 00:36:10.150
gradient now this is much faster than
computing the estimate over the entire

00:36:10.150 --> 00:36:10.160
computing the estimate over the entire
 

00:36:10.160 --> 00:36:11.500
computing the estimate over the entire
batch because B is usually something

00:36:11.500 --> 00:36:11.510
batch because B is usually something
 

00:36:11.510 --> 00:36:14.770
batch because B is usually something
like 10 to 100 and it's much more

00:36:14.770 --> 00:36:14.780
like 10 to 100 and it's much more
 

00:36:14.780 --> 00:36:17.020
like 10 to 100 and it's much more
accurate than SGD because we're not

00:36:17.020 --> 00:36:17.030
accurate than SGD because we're not
 

00:36:17.030 --> 00:36:18.490
accurate than SGD because we're not
taking a single example but we're

00:36:18.490 --> 00:36:18.500
taking a single example but we're
 

00:36:18.500 --> 00:36:20.680
taking a single example but we're
learning over a smaller batch a larger

00:36:20.680 --> 00:36:20.690
learning over a smaller batch a larger
 

00:36:20.690 --> 00:36:24.250
learning over a smaller batch a larger
batch sorry now the more accurate our

00:36:24.250 --> 00:36:24.260
batch sorry now the more accurate our
 

00:36:24.260 --> 00:36:27.490
batch sorry now the more accurate our
gradient estimation is that means the

00:36:27.490 --> 00:36:27.500
gradient estimation is that means the
 

00:36:27.500 --> 00:36:30.130
gradient estimation is that means the
more or the easier it will be for us to

00:36:30.130 --> 00:36:30.140
more or the easier it will be for us to
 

00:36:30.140 --> 00:36:33.520
more or the easier it will be for us to
converge to the solution faster means

00:36:33.520 --> 00:36:33.530
converge to the solution faster means
 

00:36:33.530 --> 00:36:35.470
converge to the solution faster means
will converge smoother because we'll

00:36:35.470 --> 00:36:35.480
will converge smoother because we'll
 

00:36:35.480 --> 00:36:37.420
will converge smoother because we'll
actually follow the true landscape that

00:36:37.420 --> 00:36:37.430
actually follow the true landscape that
 

00:36:37.430 --> 00:36:39.790
actually follow the true landscape that
exists it also means that we can

00:36:39.790 --> 00:36:39.800
exists it also means that we can
 

00:36:39.800 --> 00:36:41.500
exists it also means that we can
increase our learning rate to trust each

00:36:41.500 --> 00:36:41.510
increase our learning rate to trust each
 

00:36:41.510 --> 00:36:45.520
increase our learning rate to trust each
update more this also allows for

00:36:45.520 --> 00:36:45.530
update more this also allows for
 

00:36:45.530 --> 00:36:47.980
update more this also allows for
massively parallel Liza become petition

00:36:47.980 --> 00:36:47.990
massively parallel Liza become petition
 

00:36:47.990 --> 00:36:50.920
massively parallel Liza become petition
if we split up batches on different

00:36:50.920 --> 00:36:50.930
if we split up batches on different
 

00:36:50.930 --> 00:36:53.350
if we split up batches on different
workers on different GPUs or different

00:36:53.350 --> 00:36:53.360
workers on different GPUs or different
 

00:36:53.360 --> 00:36:56.770
workers on different GPUs or different
threads we can achieve even higher speed

00:36:56.770 --> 00:36:56.780
threads we can achieve even higher speed
 

00:36:56.780 --> 00:36:58.540
threads we can achieve even higher speed
ups because each thread can handle its

00:36:58.540 --> 00:36:58.550
ups because each thread can handle its
 

00:36:58.550 --> 00:36:59.770
ups because each thread can handle its
own batch then they can come back

00:36:59.770 --> 00:36:59.780
own batch then they can come back
 

00:36:59.780 --> 00:37:01.870
own batch then they can come back
together and aggregate together to

00:37:01.870 --> 00:37:01.880
together and aggregate together to
 

00:37:01.880 --> 00:37:04.030
together and aggregate together to
basically create that single learning

00:37:04.030 --> 00:37:04.040
basically create that single learning
 

00:37:04.040 --> 00:37:06.520
basically create that single learning
rate or completely complete that single

00:37:06.520 --> 00:37:06.530
rate or completely complete that single
 

00:37:06.530 --> 00:37:11.140
rate or completely complete that single
training iteration now finally the last

00:37:11.140 --> 00:37:11.150
training iteration now finally the last
 

00:37:11.150 --> 00:37:13.030
training iteration now finally the last
topic I want to talk about is that of

00:37:13.030 --> 00:37:13.040
topic I want to talk about is that of
 

00:37:13.040 --> 00:37:17.410
topic I want to talk about is that of
overfitting and regularization really

00:37:17.410 --> 00:37:17.420
overfitting and regularization really
 

00:37:17.420 --> 00:37:19.480
overfitting and regularization really
this is a problem of generalization

00:37:19.480 --> 00:37:19.490
this is a problem of generalization
 

00:37:19.490 --> 00:37:23.380
this is a problem of generalization
which is one of the most fundamental

00:37:23.380 --> 00:37:23.390
which is one of the most fundamental
 

00:37:23.390 --> 00:37:25.390
which is one of the most fundamental
problems in all of artificial

00:37:25.390 --> 00:37:25.400
problems in all of artificial
 

00:37:25.400 --> 00:37:28.390
problems in all of artificial
intelligence not just deep learning but

00:37:28.390 --> 00:37:28.400
intelligence not just deep learning but
 

00:37:28.400 --> 00:37:31.600
intelligence not just deep learning but
all of artificial intelligence and for

00:37:31.600 --> 00:37:31.610
all of artificial intelligence and for
 

00:37:31.610 --> 00:37:32.890
all of artificial intelligence and for
those of you who aren't familiar let me

00:37:32.890 --> 00:37:32.900
those of you who aren't familiar let me
 

00:37:32.900 --> 00:37:34.630
those of you who aren't familiar let me
just go over in a high level what

00:37:34.630 --> 00:37:34.640
just go over in a high level what
 

00:37:34.640 --> 00:37:36.730
just go over in a high level what
overfitting is what it means to

00:37:36.730 --> 00:37:36.740
overfitting is what it means to
 

00:37:36.740 --> 00:37:39.580
overfitting is what it means to
generalize ideally in machine learning

00:37:39.580 --> 00:37:39.590
generalize ideally in machine learning
 

00:37:39.590 --> 00:37:41.290
generalize ideally in machine learning
we want a model that accurately

00:37:41.290 --> 00:37:41.300
we want a model that accurately
 

00:37:41.300 --> 00:37:44.140
we want a model that accurately
describes our test data not our training

00:37:44.140 --> 00:37:44.150
describes our test data not our training
 

00:37:44.150 --> 00:37:48.100
describes our test data not our training
data but our test data said differently

00:37:48.100 --> 00:37:48.110
data but our test data said differently
 

00:37:48.110 --> 00:37:49.960
data but our test data said differently
we want to build models that can learn

00:37:49.960 --> 00:37:49.970
we want to build models that can learn
 

00:37:49.970 --> 00:37:53.170
we want to build models that can learn
representations from our training data

00:37:53.170 --> 00:37:53.180
representations from our training data
 

00:37:53.180 --> 00:37:55.780
representations from our training data
still generalized well on unseen test

00:37:55.780 --> 00:37:55.790
still generalized well on unseen test
 

00:37:55.790 --> 00:37:58.690
still generalized well on unseen test
data assume we want to build a line to

00:37:58.690 --> 00:37:58.700
data assume we want to build a line to
 

00:37:58.700 --> 00:38:00.940
data assume we want to build a line to
describe these points under fitting

00:38:00.940 --> 00:38:00.950
describe these points under fitting
 

00:38:00.950 --> 00:38:03.040
describe these points under fitting
describes the process on the left where

00:38:03.040 --> 00:38:03.050
describes the process on the left where
 

00:38:03.050 --> 00:38:05.349
describes the process on the left where
the complexity of our model is simply

00:38:05.349 --> 00:38:05.359
the complexity of our model is simply
 

00:38:05.359 --> 00:38:07.210
the complexity of our model is simply
not high enough to capture the nuances

00:38:07.210 --> 00:38:07.220
not high enough to capture the nuances
 

00:38:07.220 --> 00:38:10.089
not high enough to capture the nuances
of our data if we go to overfitting on

00:38:10.089 --> 00:38:10.099
of our data if we go to overfitting on
 

00:38:10.099 --> 00:38:12.849
of our data if we go to overfitting on
the right we're actually having to

00:38:12.849 --> 00:38:12.859
the right we're actually having to
 

00:38:12.859 --> 00:38:14.740
the right we're actually having to
complex of a model and actually just

00:38:14.740 --> 00:38:14.750
complex of a model and actually just
 

00:38:14.750 --> 00:38:16.900
complex of a model and actually just
memorizing our training data which means

00:38:16.900 --> 00:38:16.910
memorizing our training data which means
 

00:38:16.910 --> 00:38:18.400
memorizing our training data which means
that if we introduce a new test data

00:38:18.400 --> 00:38:18.410
that if we introduce a new test data
 

00:38:18.410 --> 00:38:20.319
that if we introduce a new test data
point it's not going to generalize well

00:38:20.319 --> 00:38:20.329
point it's not going to generalize well
 

00:38:20.329 --> 00:38:22.240
point it's not going to generalize well
ideally what we want to something in the

00:38:22.240 --> 00:38:22.250
ideally what we want to something in the
 

00:38:22.250 --> 00:38:24.520
ideally what we want to something in the
middle which is not too complex to

00:38:24.520 --> 00:38:24.530
middle which is not too complex to
 

00:38:24.530 --> 00:38:28.890
middle which is not too complex to
memorize all the training data but still

00:38:28.890 --> 00:38:28.900
memorize all the training data but still
 

00:38:28.900 --> 00:38:31.569
memorize all the training data but still
contains the capacity to learn some of

00:38:31.569 --> 00:38:31.579
contains the capacity to learn some of
 

00:38:31.579 --> 00:38:36.490
contains the capacity to learn some of
these nuances in this in the test set so

00:38:36.490 --> 00:38:36.500
these nuances in this in the test set so
 

00:38:36.500 --> 00:38:38.170
these nuances in this in the test set so
address to address this problem let's

00:38:38.170 --> 00:38:38.180
address to address this problem let's
 

00:38:38.180 --> 00:38:39.730
address to address this problem let's
talk about this technique called

00:38:39.730 --> 00:38:39.740
talk about this technique called
 

00:38:39.740 --> 00:38:42.730
talk about this technique called
regularization now regularization is

00:38:42.730 --> 00:38:42.740
regularization now regularization is
 

00:38:42.740 --> 00:38:45.220
regularization now regularization is
just this way that you can discourage

00:38:45.220 --> 00:38:45.230
just this way that you can discourage
 

00:38:45.230 --> 00:38:47.790
just this way that you can discourage
your models from becoming too complex

00:38:47.790 --> 00:38:47.800
your models from becoming too complex
 

00:38:47.800 --> 00:38:51.700
your models from becoming too complex
and absolutely as we've seen before this

00:38:51.700 --> 00:38:51.710
and absolutely as we've seen before this
 

00:38:51.710 --> 00:38:54.130
and absolutely as we've seen before this
is extremely critical because we don't

00:38:54.130 --> 00:38:54.140
is extremely critical because we don't
 

00:38:54.140 --> 00:38:56.530
is extremely critical because we don't
want our data we don't want our models

00:38:56.530 --> 00:38:56.540
want our data we don't want our models
 

00:38:56.540 --> 00:38:59.920
want our data we don't want our models
to just memorize data and only do well

00:38:59.920 --> 00:38:59.930
to just memorize data and only do well
 

00:38:59.930 --> 00:39:05.230
to just memorize data and only do well
in our training set one of the most

00:39:05.230 --> 00:39:05.240
in our training set one of the most
 

00:39:05.240 --> 00:39:07.660
in our training set one of the most
popular techniques for regularization in

00:39:07.660 --> 00:39:07.670
popular techniques for regularization in
 

00:39:07.670 --> 00:39:10.240
popular techniques for regularization in
neural networks is dropout this is an

00:39:10.240 --> 00:39:10.250
neural networks is dropout this is an
 

00:39:10.250 --> 00:39:12.910
neural networks is dropout this is an
extremely simple idea let's revisit this

00:39:12.910 --> 00:39:12.920
extremely simple idea let's revisit this
 

00:39:12.920 --> 00:39:14.410
extremely simple idea let's revisit this
picture of a deep neural network and

00:39:14.410 --> 00:39:14.420
picture of a deep neural network and
 

00:39:14.420 --> 00:39:16.720
picture of a deep neural network and
then drop out all we do during training

00:39:16.720 --> 00:39:16.730
then drop out all we do during training
 

00:39:16.730 --> 00:39:20.640
then drop out all we do during training
on every iteration we randomly drop some

00:39:20.640 --> 00:39:20.650
on every iteration we randomly drop some
 

00:39:20.650 --> 00:39:23.530
on every iteration we randomly drop some
proportion of the hidden neurons with

00:39:23.530 --> 00:39:23.540
proportion of the hidden neurons with
 

00:39:23.540 --> 00:39:25.960
proportion of the hidden neurons with
some probability P so let's suppose P

00:39:25.960 --> 00:39:25.970
some probability P so let's suppose P
 

00:39:25.970 --> 00:39:28.210
some probability P so let's suppose P
equals 0.5 that means we dropped 50% of

00:39:28.210 --> 00:39:28.220
equals 0.5 that means we dropped 50% of
 

00:39:28.220 --> 00:39:29.890
equals 0.5 that means we dropped 50% of
those neurons like that those

00:39:29.890 --> 00:39:29.900
those neurons like that those
 

00:39:29.900 --> 00:39:32.589
those neurons like that those
activations become zero and effectively

00:39:32.589 --> 00:39:32.599
activations become zero and effectively
 

00:39:32.599 --> 00:39:35.160
activations become zero and effectively
they're no longer part of our network

00:39:35.160 --> 00:39:35.170
they're no longer part of our network
 

00:39:35.170 --> 00:39:38.559
they're no longer part of our network
this forces the network to not rely on

00:39:38.559 --> 00:39:38.569
this forces the network to not rely on
 

00:39:38.569 --> 00:39:41.530
this forces the network to not rely on
any single node but actually find

00:39:41.530 --> 00:39:41.540
any single node but actually find
 

00:39:41.540 --> 00:39:43.240
any single node but actually find
alternative paths through the network

00:39:43.240 --> 00:39:43.250
alternative paths through the network
 

00:39:43.250 --> 00:39:45.460
alternative paths through the network
and not put too much weight on any

00:39:45.460 --> 00:39:45.470
and not put too much weight on any
 

00:39:45.470 --> 00:39:47.589
and not put too much weight on any
single example with any single single

00:39:47.589 --> 00:39:47.599
single example with any single single
 

00:39:47.599 --> 00:39:49.299
single example with any single single
node so it discourages memorization

00:39:49.299 --> 00:39:49.309
node so it discourages memorization
 

00:39:49.309 --> 00:39:53.650
node so it discourages memorization
essentially on every iteration we

00:39:53.650 --> 00:39:53.660
essentially on every iteration we
 

00:39:53.660 --> 00:39:56.589
essentially on every iteration we
randomly drop another 50% of the node so

00:39:56.589 --> 00:39:56.599
randomly drop another 50% of the node so
 

00:39:56.599 --> 00:39:58.150
randomly drop another 50% of the node so
on this iteration I may drop these on

00:39:58.150 --> 00:39:58.160
on this iteration I may drop these on
 

00:39:58.160 --> 00:40:00.099
on this iteration I may drop these on
the next iteration I may drop those and

00:40:00.099 --> 00:40:00.109
the next iteration I may drop those and
 

00:40:00.109 --> 00:40:01.510
the next iteration I may drop those and
since it's different on every iteration

00:40:01.510 --> 00:40:01.520
since it's different on every iteration
 

00:40:01.520 --> 00:40:03.730
since it's different on every iteration
you're encouraging the network to find

00:40:03.730 --> 00:40:03.740
you're encouraging the network to find
 

00:40:03.740 --> 00:40:07.290
you're encouraging the network to find
these different paths to its answer

00:40:07.290 --> 00:40:07.300
these different paths to its answer
 

00:40:07.300 --> 00:40:09.760
these different paths to its answer
the second technique for regularization

00:40:09.760 --> 00:40:09.770
the second technique for regularization
 

00:40:09.770 --> 00:40:11.680
the second technique for regularization
that we'll talk about is this notion of

00:40:11.680 --> 00:40:11.690
that we'll talk about is this notion of
 

00:40:11.690 --> 00:40:14.740
that we'll talk about is this notion of
early stopping now we know that the

00:40:14.740 --> 00:40:14.750
early stopping now we know that the
 

00:40:14.750 --> 00:40:17.020
early stopping now we know that the
definition of overfitting actually is

00:40:17.020 --> 00:40:17.030
definition of overfitting actually is
 

00:40:17.030 --> 00:40:19.090
definition of overfitting actually is
just when our model starts to perform

00:40:19.090 --> 00:40:19.100
just when our model starts to perform
 

00:40:19.100 --> 00:40:22.000
just when our model starts to perform
worse and worse on our test data set so

00:40:22.000 --> 00:40:22.010
worse and worse on our test data set so
 

00:40:22.010 --> 00:40:23.950
worse and worse on our test data set so
let's use that to our advantage to

00:40:23.950 --> 00:40:23.960
let's use that to our advantage to
 

00:40:23.960 --> 00:40:26.260
let's use that to our advantage to
create this early stopping algorithm if

00:40:26.260 --> 00:40:26.270
create this early stopping algorithm if
 

00:40:26.270 --> 00:40:28.150
create this early stopping algorithm if
we set aside some of our training data

00:40:28.150 --> 00:40:28.160
we set aside some of our training data
 

00:40:28.160 --> 00:40:29.950
we set aside some of our training data
and use it only as test data we don't

00:40:29.950 --> 00:40:29.960
and use it only as test data we don't
 

00:40:29.960 --> 00:40:32.590
and use it only as test data we don't
train with that data we can use it to

00:40:32.590 --> 00:40:32.600
train with that data we can use it to
 

00:40:32.600 --> 00:40:34.840
train with that data we can use it to
basically monitor the progress of our

00:40:34.840 --> 00:40:34.850
basically monitor the progress of our
 

00:40:34.850 --> 00:40:37.420
basically monitor the progress of our
model on unseen data so we can plot this

00:40:37.420 --> 00:40:37.430
model on unseen data so we can plot this
 

00:40:37.430 --> 00:40:40.090
model on unseen data so we can plot this
curve we're on the x axis we have the

00:40:40.090 --> 00:40:40.100
curve we're on the x axis we have the
 

00:40:40.100 --> 00:40:41.770
curve we're on the x axis we have the
training iterations on the y axis we

00:40:41.770 --> 00:40:41.780
training iterations on the y axis we
 

00:40:41.780 --> 00:40:43.900
training iterations on the y axis we
have the loss now they start off going

00:40:43.900 --> 00:40:43.910
have the loss now they start off going
 

00:40:43.910 --> 00:40:45.790
have the loss now they start off going
down together this is great because it

00:40:45.790 --> 00:40:45.800
down together this is great because it
 

00:40:45.800 --> 00:40:47.620
down together this is great because it
means that we're learning we're training

00:40:47.620 --> 00:40:47.630
means that we're learning we're training
 

00:40:47.630 --> 00:40:50.830
means that we're learning we're training
right that's great there comes a point

00:40:50.830 --> 00:40:50.840
right that's great there comes a point
 

00:40:50.840 --> 00:40:54.250
right that's great there comes a point
though where the testing data where the

00:40:54.250 --> 00:40:54.260
though where the testing data where the
 

00:40:54.260 --> 00:40:57.160
though where the testing data where the
testing data set and the add the loss

00:40:57.160 --> 00:40:57.170
testing data set and the add the loss
 

00:40:57.170 --> 00:41:01.030
testing data set and the add the loss
for that data set starts to Plateau now

00:41:01.030 --> 00:41:01.040
for that data set starts to Plateau now
 

00:41:01.040 --> 00:41:02.950
for that data set starts to Plateau now
if we look a little further the training

00:41:02.950 --> 00:41:02.960
if we look a little further the training
 

00:41:02.960 --> 00:41:05.260
if we look a little further the training
data set loss will always continue to go

00:41:05.260 --> 00:41:05.270
data set loss will always continue to go
 

00:41:05.270 --> 00:41:07.090
data set loss will always continue to go
down as long as our model has the

00:41:07.090 --> 00:41:07.100
down as long as our model has the
 

00:41:07.100 --> 00:41:09.580
down as long as our model has the
capacity to learn and memorize some of

00:41:09.580 --> 00:41:09.590
capacity to learn and memorize some of
 

00:41:09.590 --> 00:41:11.020
capacity to learn and memorize some of
that data but that doesn't mean that

00:41:11.020 --> 00:41:11.030
that data but that doesn't mean that
 

00:41:11.030 --> 00:41:12.460
that data but that doesn't mean that
it's actually generalizing well because

00:41:12.460 --> 00:41:12.470
it's actually generalizing well because
 

00:41:12.470 --> 00:41:14.620
it's actually generalizing well because
we can see that the testing data set has

00:41:14.620 --> 00:41:14.630
we can see that the testing data set has
 

00:41:14.630 --> 00:41:17.560
we can see that the testing data set has
actually started to increase this

00:41:17.560 --> 00:41:17.570
actually started to increase this
 

00:41:17.570 --> 00:41:18.940
actually started to increase this
pattern continues for the rest of

00:41:18.940 --> 00:41:18.950
pattern continues for the rest of
 

00:41:18.950 --> 00:41:20.680
pattern continues for the rest of
training but I want to focus on this

00:41:20.680 --> 00:41:20.690
training but I want to focus on this
 

00:41:20.690 --> 00:41:23.650
training but I want to focus on this
point here this is the point where you

00:41:23.650 --> 00:41:23.660
point here this is the point where you
 

00:41:23.660 --> 00:41:25.690
point here this is the point where you
need to stop training because after this

00:41:25.690 --> 00:41:25.700
need to stop training because after this
 

00:41:25.700 --> 00:41:28.480
need to stop training because after this
point you are overfitting and your model

00:41:28.480 --> 00:41:28.490
point you are overfitting and your model
 

00:41:28.490 --> 00:41:30.070
point you are overfitting and your model
is no longer performing well on unseen

00:41:30.070 --> 00:41:30.080
is no longer performing well on unseen
 

00:41:30.080 --> 00:41:32.170
is no longer performing well on unseen
data if you stop before that point

00:41:32.170 --> 00:41:32.180
data if you stop before that point
 

00:41:32.180 --> 00:41:34.300
data if you stop before that point
you're actually under fitting and you're

00:41:34.300 --> 00:41:34.310
you're actually under fitting and you're
 

00:41:34.310 --> 00:41:36.220
you're actually under fitting and you're
not utilizing the full potential the

00:41:36.220 --> 00:41:36.230
not utilizing the full potential the
 

00:41:36.230 --> 00:41:40.750
not utilizing the full potential the
full capacity of your network so I'll

00:41:40.750 --> 00:41:40.760
full capacity of your network so I'll
 

00:41:40.760 --> 00:41:43.000
full capacity of your network so I'll
conclude this lecture by summarizing

00:41:43.000 --> 00:41:43.010
conclude this lecture by summarizing
 

00:41:43.010 --> 00:41:45.700
conclude this lecture by summarizing
three key points that we've covered so

00:41:45.700 --> 00:41:45.710
three key points that we've covered so
 

00:41:45.710 --> 00:41:48.580
three key points that we've covered so
far first we've learned about the

00:41:48.580 --> 00:41:48.590
far first we've learned about the
 

00:41:48.590 --> 00:41:50.440
far first we've learned about the
fundamental building blocks of neural

00:41:50.440 --> 00:41:50.450
fundamental building blocks of neural
 

00:41:50.450 --> 00:41:53.350
fundamental building blocks of neural
networks called the perceptron we've

00:41:53.350 --> 00:41:53.360
networks called the perceptron we've
 

00:41:53.360 --> 00:41:56.770
networks called the perceptron we've
learned about stacking these units these

00:41:56.770 --> 00:41:56.780
learned about stacking these units these
 

00:41:56.780 --> 00:41:59.670
learned about stacking these units these
perceptrons together to compose very

00:41:59.670 --> 00:41:59.680
perceptrons together to compose very
 

00:41:59.680 --> 00:42:03.850
perceptrons together to compose very
complex hierarchical models and we've

00:42:03.850 --> 00:42:03.860
complex hierarchical models and we've
 

00:42:03.860 --> 00:42:05.980
complex hierarchical models and we've
learned how to mathematically optimize

00:42:05.980 --> 00:42:05.990
learned how to mathematically optimize
 

00:42:05.990 --> 00:42:08.740
learned how to mathematically optimize
these models using a process called back

00:42:08.740 --> 00:42:08.750
these models using a process called back
 

00:42:08.750 --> 00:42:11.320
these models using a process called back
row back propagation and gradient

00:42:11.320 --> 00:42:11.330
row back propagation and gradient
 

00:42:11.330 --> 00:42:14.410
row back propagation and gradient
descent finally we adjust some of the

00:42:14.410 --> 00:42:14.420
descent finally we adjust some of the
 

00:42:14.420 --> 00:42:16.120
descent finally we adjust some of the
practical challenges of training these

00:42:16.120 --> 00:42:16.130
practical challenges of training these
 

00:42:16.130 --> 00:42:18.940
practical challenges of training these
models in real life that you'll find

00:42:18.940 --> 00:42:18.950
models in real life that you'll find
 

00:42:18.950 --> 00:42:20.260
models in real life that you'll find
useful for the labs today

00:42:20.260 --> 00:42:20.270
useful for the labs today
 

00:42:20.270 --> 00:42:22.030
useful for the labs today
such as using adaptive learning rates

00:42:22.030 --> 00:42:22.040
such as using adaptive learning rates
 

00:42:22.040 --> 00:42:24.730
such as using adaptive learning rates
batching and regularization to combat

00:42:24.730 --> 00:42:24.740
batching and regularization to combat
 

00:42:24.740 --> 00:42:31.150
batching and regularization to combat
overfitting thank you and I'd be happy

00:42:31.150 --> 00:42:31.160
overfitting thank you and I'd be happy
 

00:42:31.160 --> 00:42:32.920
overfitting thank you and I'd be happy
to answer any questions now otherwise

00:42:32.920 --> 00:42:32.930
to answer any questions now otherwise
 

00:42:32.930 --> 00:42:34.120
to answer any questions now otherwise
we'll have Ferrini

00:42:34.120 --> 00:42:34.130
we'll have Ferrini
 

00:42:34.130 --> 00:42:36.640
we'll have Ferrini
talk to us about some of the deep

00:42:36.640 --> 00:42:36.650
talk to us about some of the deep
 

00:42:36.650 --> 00:42:38.680
talk to us about some of the deep
sequence models for modeling temporal

00:42:38.680 --> 00:42:38.690
sequence models for modeling temporal
 

00:42:38.690 --> 00:42:40.900
sequence models for modeling temporal
data

