WEBVTT
Kind: captions
Language: en

00:00:00.179 --> 00:00:02.869
 
in addition to l2 regularization another

00:00:02.869 --> 00:00:02.879
in addition to l2 regularization another
 

00:00:02.879 --> 00:00:05.269
in addition to l2 regularization another
very powerful regularization technique

00:00:05.269 --> 00:00:05.279
very powerful regularization technique
 

00:00:05.279 --> 00:00:07.579
very powerful regularization technique
is called drop out let's see how that

00:00:07.579 --> 00:00:07.589
is called drop out let's see how that
 

00:00:07.589 --> 00:00:09.860
is called drop out let's see how that
works let's say you've trained a neural

00:00:09.860 --> 00:00:09.870
works let's say you've trained a neural
 

00:00:09.870 --> 00:00:11.419
works let's say you've trained a neural
network where the one on the left and is

00:00:11.419 --> 00:00:11.429
network where the one on the left and is
 

00:00:11.429 --> 00:00:13.879
network where the one on the left and is
overfitting just what you do with

00:00:13.879 --> 00:00:13.889
overfitting just what you do with
 

00:00:13.889 --> 00:00:15.890
overfitting just what you do with
dropout let me make a copy of the neural

00:00:15.890 --> 00:00:15.900
dropout let me make a copy of the neural
 

00:00:15.900 --> 00:00:18.740
dropout let me make a copy of the neural
network with dropout what we're going to

00:00:18.740 --> 00:00:18.750
network with dropout what we're going to
 

00:00:18.750 --> 00:00:21.109
network with dropout what we're going to
do is go through each of the layers of

00:00:21.109 --> 00:00:21.119
do is go through each of the layers of
 

00:00:21.119 --> 00:00:24.140
do is go through each of the layers of
the network and set some probability of

00:00:24.140 --> 00:00:24.150
the network and set some probability of
 

00:00:24.150 --> 00:00:26.480
the network and set some probability of
eliminating a node in your network so

00:00:26.480 --> 00:00:26.490
eliminating a node in your network so
 

00:00:26.490 --> 00:00:28.779
eliminating a node in your network so
let's say that for each of these layers

00:00:28.779 --> 00:00:28.789
let's say that for each of these layers
 

00:00:28.789 --> 00:00:31.609
let's say that for each of these layers
we're going to for each note of the coin

00:00:31.609 --> 00:00:31.619
we're going to for each note of the coin
 

00:00:31.619 --> 00:00:34.819
we're going to for each note of the coin
and have a 0.5 challenge of keeping each

00:00:34.819 --> 00:00:34.829
and have a 0.5 challenge of keeping each
 

00:00:34.829 --> 00:00:37.940
and have a 0.5 challenge of keeping each
node and 0.5 cons of removing each node

00:00:37.940 --> 00:00:37.950
node and 0.5 cons of removing each node
 

00:00:37.950 --> 00:00:40.100
node and 0.5 cons of removing each node
so after the coin tosses maybe you

00:00:40.100 --> 00:00:40.110
so after the coin tosses maybe you
 

00:00:40.110 --> 00:00:43.250
so after the coin tosses maybe you
decide to eliminate those nodes then

00:00:43.250 --> 00:00:43.260
decide to eliminate those nodes then
 

00:00:43.260 --> 00:00:46.490
decide to eliminate those nodes then
what you do is actually remove all the

00:00:46.490 --> 00:00:46.500
what you do is actually remove all the
 

00:00:46.500 --> 00:00:48.740
what you do is actually remove all the
in going outgoing links from that node

00:00:48.740 --> 00:00:48.750
in going outgoing links from that node
 

00:00:48.750 --> 00:00:50.690
in going outgoing links from that node
as well so you end up with a much

00:00:50.690 --> 00:00:50.700
as well so you end up with a much
 

00:00:50.700 --> 00:00:53.029
as well so you end up with a much
smaller really much diminished Network

00:00:53.029 --> 00:00:53.039
smaller really much diminished Network
 

00:00:53.039 --> 00:00:55.100
smaller really much diminished Network
and then you do back propagation

00:00:55.100 --> 00:00:55.110
and then you do back propagation
 

00:00:55.110 --> 00:00:57.740
and then you do back propagation
training this one example on this much

00:00:57.740 --> 00:00:57.750
training this one example on this much
 

00:00:57.750 --> 00:01:00.529
training this one example on this much
diminished network and then on different

00:01:00.529 --> 00:01:00.539
diminished network and then on different
 

00:01:00.539 --> 00:01:02.689
diminished network and then on different
example you would toss the set of coins

00:01:02.689 --> 00:01:02.699
example you would toss the set of coins
 

00:01:02.699 --> 00:01:04.460
example you would toss the set of coins
again and keep a different set of nodes

00:01:04.460 --> 00:01:04.470
again and keep a different set of nodes
 

00:01:04.470 --> 00:01:06.410
again and keep a different set of nodes
and they drop out eliminated different

00:01:06.410 --> 00:01:06.420
and they drop out eliminated different
 

00:01:06.420 --> 00:01:08.600
and they drop out eliminated different
set of nodes and so for each training

00:01:08.600 --> 00:01:08.610
set of nodes and so for each training
 

00:01:08.610 --> 00:01:12.080
set of nodes and so for each training
example you would train it using one of

00:01:12.080 --> 00:01:12.090
example you would train it using one of
 

00:01:12.090 --> 00:01:15.050
example you would train it using one of
these newer reduce networks so maybe who

00:01:15.050 --> 00:01:15.060
these newer reduce networks so maybe who
 

00:01:15.060 --> 00:01:16.760
these newer reduce networks so maybe who
seems like a slightly crazy technique to

00:01:16.760 --> 00:01:16.770
seems like a slightly crazy technique to
 

00:01:16.770 --> 00:01:19.940
seems like a slightly crazy technique to
just go go around killing those at

00:01:19.940 --> 00:01:19.950
just go go around killing those at
 

00:01:19.950 --> 00:01:23.120
just go go around killing those at
random but this actually works but you

00:01:23.120 --> 00:01:23.130
random but this actually works but you
 

00:01:23.130 --> 00:01:24.890
random but this actually works but you
can imagine that because you're training

00:01:24.890 --> 00:01:24.900
can imagine that because you're training
 

00:01:24.900 --> 00:01:28.270
can imagine that because you're training
a much smaller network on each example

00:01:28.270 --> 00:01:28.280
a much smaller network on each example
 

00:01:28.280 --> 00:01:31.249
a much smaller network on each example
maybe you know this gives a sense of why

00:01:31.249 --> 00:01:31.259
maybe you know this gives a sense of why
 

00:01:31.259 --> 00:01:33.679
maybe you know this gives a sense of why
you end up able to regularize the

00:01:33.679 --> 00:01:33.689
you end up able to regularize the
 

00:01:33.689 --> 00:01:35.330
you end up able to regularize the
network because these much smaller

00:01:35.330 --> 00:01:35.340
network because these much smaller
 

00:01:35.340 --> 00:01:39.530
network because these much smaller
networks are being trained so let's look

00:01:39.530 --> 00:01:39.540
networks are being trained so let's look
 

00:01:39.540 --> 00:01:41.630
networks are being trained so let's look
at how you can implement dropout there

00:01:41.630 --> 00:01:41.640
at how you can implement dropout there
 

00:01:41.640 --> 00:01:43.219
at how you can implement dropout there
are a few ways of implementing drop-off

00:01:43.219 --> 00:01:43.229
are a few ways of implementing drop-off
 

00:01:43.229 --> 00:01:44.300
are a few ways of implementing drop-off
I'm going to show you the most common

00:01:44.300 --> 00:01:44.310
I'm going to show you the most common
 

00:01:44.310 --> 00:01:46.580
I'm going to show you the most common
one which is technique called inverted

00:01:46.580 --> 00:01:46.590
one which is technique called inverted
 

00:01:46.590 --> 00:01:49.069
one which is technique called inverted
dropout for the sake of completeness

00:01:49.069 --> 00:01:49.079
dropout for the sake of completeness
 

00:01:49.079 --> 00:01:54.160
dropout for the sake of completeness
let's say we want to illustrate this

00:01:54.160 --> 00:01:54.170
let's say we want to illustrate this
 

00:01:54.170 --> 00:02:00.410
let's say we want to illustrate this
wave from layer L equals V so in the

00:02:00.410 --> 00:02:00.420
wave from layer L equals V so in the
 

00:02:00.420 --> 00:02:01.580
wave from layer L equals V so in the
code I'm going to write there'll be a

00:02:01.580 --> 00:02:01.590
code I'm going to write there'll be a
 

00:02:01.590 --> 00:02:03.889
code I'm going to write there'll be a
bunch of threes here that's just you

00:02:03.889 --> 00:02:03.899
bunch of threes here that's just you
 

00:02:03.899 --> 00:02:05.780
bunch of threes here that's just you
know that I'm just a little tree health

00:02:05.780 --> 00:02:05.790
know that I'm just a little tree health
 

00:02:05.790 --> 00:02:08.630
know that I'm just a little tree health
into and drop out in a single layer so

00:02:08.630 --> 00:02:08.640
into and drop out in a single layer so
 

00:02:08.640 --> 00:02:10.469
into and drop out in a single layer so
one thing to do is

00:02:10.469 --> 00:02:10.479
one thing to do is
 

00:02:10.479 --> 00:02:13.890
one thing to do is
a vector D G 3 is going to be the

00:02:13.890 --> 00:02:13.900
a vector D G 3 is going to be the
 

00:02:13.900 --> 00:02:17.160
a vector D G 3 is going to be the
dropout vector for layer 3 that's what 3

00:02:17.160 --> 00:02:17.170
dropout vector for layer 3 that's what 3
 

00:02:17.170 --> 00:02:22.820
dropout vector for layer 3 that's what 3
is to be MP dot random dot R and and

00:02:22.820 --> 00:02:22.830
is to be MP dot random dot R and and
 

00:02:22.830 --> 00:02:26.940
is to be MP dot random dot R and and
then is going to be the same shape as a

00:02:26.940 --> 00:02:26.950
then is going to be the same shape as a
 

00:02:26.950 --> 00:02:30.270
then is going to be the same shape as a
3 and going to see if this is less than

00:02:30.270 --> 00:02:30.280
3 and going to see if this is less than
 

00:02:30.280 --> 00:02:33.110
3 and going to see if this is less than
some number which I'm gonna call keep

00:02:33.110 --> 00:02:33.120
some number which I'm gonna call keep
 

00:02:33.120 --> 00:02:37.319
some number which I'm gonna call keep
problem and so cheap prop is a number it

00:02:37.319 --> 00:02:37.329
problem and so cheap prop is a number it
 

00:02:37.329 --> 00:02:39.330
problem and so cheap prop is a number it
was 0.5 on the previous slide and maybe

00:02:39.330 --> 00:02:39.340
was 0.5 on the previous slide and maybe
 

00:02:39.340 --> 00:02:42.000
was 0.5 on the previous slide and maybe
now I use 0.8 in this example and

00:02:42.000 --> 00:02:42.010
now I use 0.8 in this example and
 

00:02:42.010 --> 00:02:43.880
now I use 0.8 in this example and
there'll be the probability that a given

00:02:43.880 --> 00:02:43.890
there'll be the probability that a given
 

00:02:43.890 --> 00:02:46.650
there'll be the probability that a given
hidden unit will be kept

00:02:46.650 --> 00:02:46.660
hidden unit will be kept
 

00:02:46.660 --> 00:02:49.440
hidden unit will be kept
so if key problem is equal to 0.8 then

00:02:49.440 --> 00:02:49.450
so if key problem is equal to 0.8 then
 

00:02:49.450 --> 00:02:51.869
so if key problem is equal to 0.8 then
this means that there's a 0.2 chance of

00:02:51.869 --> 00:02:51.879
this means that there's a 0.2 chance of
 

00:02:51.879 --> 00:02:55.199
this means that there's a 0.2 chance of
eliminating any hidden unit so what this

00:02:55.199 --> 00:02:55.209
eliminating any hidden unit so what this
 

00:02:55.209 --> 00:02:57.869
eliminating any hidden unit so what this
does is it generates a random matrix um

00:02:57.869 --> 00:02:57.879
does is it generates a random matrix um
 

00:02:57.879 --> 00:02:59.640
does is it generates a random matrix um
and this works as well if you have

00:02:59.640 --> 00:02:59.650
and this works as well if you have
 

00:02:59.650 --> 00:03:02.520
and this works as well if you have
vectorized but so DC will be a matrix

00:03:02.520 --> 00:03:02.530
vectorized but so DC will be a matrix
 

00:03:02.530 --> 00:03:05.069
vectorized but so DC will be a matrix
where for each example and the each

00:03:05.069 --> 00:03:05.079
where for each example and the each
 

00:03:05.079 --> 00:03:07.920
where for each example and the each
hidden unit there's a 0.8 chance that

00:03:07.920 --> 00:03:07.930
hidden unit there's a 0.8 chance that
 

00:03:07.930 --> 00:03:10.440
hidden unit there's a 0.8 chance that
the corresponding DC will be 1 and the

00:03:10.440 --> 00:03:10.450
the corresponding DC will be 1 and the
 

00:03:10.450 --> 00:03:13.500
the corresponding DC will be 1 and the
20% chance will be 0 all right so no

00:03:13.500 --> 00:03:13.510
20% chance will be 0 all right so no
 

00:03:13.510 --> 00:03:16.259
20% chance will be 0 all right so no
this this random number being less than

00:03:16.259 --> 00:03:16.269
this this random number being less than
 

00:03:16.269 --> 00:03:19.890
this this random number being less than
0.8 there's a point a chance at being 1

00:03:19.890 --> 00:03:19.900
0.8 there's a point a chance at being 1
 

00:03:19.900 --> 00:03:22.080
0.8 there's a point a chance at being 1
or being true and at 2015 Johnson

00:03:22.080 --> 00:03:22.090
or being true and at 2015 Johnson
 

00:03:22.090 --> 00:03:23.940
or being true and at 2015 Johnson
playing to charms are being false of

00:03:23.940 --> 00:03:23.950
playing to charms are being false of
 

00:03:23.950 --> 00:03:25.379
playing to charms are being false of
being 0 and then what you're going to do

00:03:25.379 --> 00:03:25.389
being 0 and then what you're going to do
 

00:03:25.389 --> 00:03:27.059
being 0 and then what you're going to do
is take your activations from the 3rd

00:03:27.059 --> 00:03:27.069
is take your activations from the 3rd
 

00:03:27.069 --> 00:03:29.370
is take your activations from the 3rd
layer I'm just call it a fee in this

00:03:29.370 --> 00:03:29.380
layer I'm just call it a fee in this
 

00:03:29.380 --> 00:03:31.530
layer I'm just call it a fee in this
little example so a 3 are the

00:03:31.530 --> 00:03:31.540
little example so a 3 are the
 

00:03:31.540 --> 00:03:33.809
little example so a 3 are the
activations you compute it and I'm going

00:03:33.809 --> 00:03:33.819
activations you compute it and I'm going
 

00:03:33.819 --> 00:03:37.559
activations you compute it and I'm going
to send a 3 to be equal to the old a 3

00:03:37.559 --> 00:03:37.569
to send a 3 to be equal to the old a 3
 

00:03:37.569 --> 00:03:40.319
to send a 3 to be equal to the old a 3
times ok so there's an element-wise

00:03:40.319 --> 00:03:40.329
times ok so there's an element-wise
 

00:03:40.329 --> 00:03:42.539
times ok so there's an element-wise
multiplication or I guess you could also

00:03:42.539 --> 00:03:42.549
multiplication or I guess you could also
 

00:03:42.549 --> 00:03:46.800
multiplication or I guess you could also
write this as a a 3 x equals d3 but what

00:03:46.800 --> 00:03:46.810
write this as a a 3 x equals d3 but what
 

00:03:46.810 --> 00:03:49.020
write this as a a 3 x equals d3 but what
this does is for every element of DV

00:03:49.020 --> 00:03:49.030
this does is for every element of DV
 

00:03:49.030 --> 00:03:51.659
this does is for every element of DV
that's equal to 0 and there's a 20%

00:03:51.659 --> 00:03:51.669
that's equal to 0 and there's a 20%
 

00:03:51.669 --> 00:03:53.400
that's equal to 0 and there's a 20%
chance of each of the elements being 0

00:03:53.400 --> 00:03:53.410
chance of each of the elements being 0
 

00:03:53.410 --> 00:03:55.470
chance of each of the elements being 0
you end up this multiplier operation

00:03:55.470 --> 00:03:55.480
you end up this multiplier operation
 

00:03:55.480 --> 00:03:58.740
you end up this multiplier operation
ends up zeroing out the corresponding

00:03:58.740 --> 00:03:58.750
ends up zeroing out the corresponding
 

00:03:58.750 --> 00:04:01.770
ends up zeroing out the corresponding
element of DC well if you do this in

00:04:01.770 --> 00:04:01.780
element of DC well if you do this in
 

00:04:01.780 --> 00:04:03.990
element of DC well if you do this in
Python technically d3 will be a boolean

00:04:03.990 --> 00:04:04.000
Python technically d3 will be a boolean
 

00:04:04.000 --> 00:04:06.030
Python technically d3 will be a boolean
array what values true and false rather

00:04:06.030 --> 00:04:06.040
array what values true and false rather
 

00:04:06.040 --> 00:04:08.189
array what values true and false rather
than 1 and 0 but it'll multiply the

00:04:08.189 --> 00:04:08.199
than 1 and 0 but it'll multiply the
 

00:04:08.199 --> 00:04:09.900
than 1 and 0 but it'll multiply the
multiplier operation we're going to

00:04:09.900 --> 00:04:09.910
multiplier operation we're going to
 

00:04:09.910 --> 00:04:12.839
multiplier operation we're going to
interpret the true and false values as 1

00:04:12.839 --> 00:04:12.849
interpret the true and false values as 1
 

00:04:12.849 --> 00:04:14.879
interpret the true and false values as 1
and 0 if you try to just open Python you

00:04:14.879 --> 00:04:14.889
and 0 if you try to just open Python you
 

00:04:14.889 --> 00:04:17.969
and 0 if you try to just open Python you
you see then finally we're going to take

00:04:17.969 --> 00:04:17.979
you see then finally we're going to take
 

00:04:17.979 --> 00:04:23.520
you see then finally we're going to take
a 3 and scale it up by dividing by 0.8

00:04:23.520 --> 00:04:23.530
a 3 and scale it up by dividing by 0.8
 

00:04:23.530 --> 00:04:24.000
a 3 and scale it up by dividing by 0.8
or

00:04:24.000 --> 00:04:24.010
or
 

00:04:24.010 --> 00:04:28.490
or
really dividing by our cheap prop

00:04:28.490 --> 00:04:28.500
really dividing by our cheap prop
 

00:04:28.500 --> 00:04:31.230
really dividing by our cheap prop
parameter so let me explain what this

00:04:31.230 --> 00:04:31.240
parameter so let me explain what this
 

00:04:31.240 --> 00:04:33.150
parameter so let me explain what this
final step is doing let's say for the

00:04:33.150 --> 00:04:33.160
final step is doing let's say for the
 

00:04:33.160 --> 00:04:35.940
final step is doing let's say for the
sake of argument then you have 50 units

00:04:35.940 --> 00:04:35.950
sake of argument then you have 50 units
 

00:04:35.950 --> 00:04:39.750
sake of argument then you have 50 units
or 50 neurons in the third hidden layer

00:04:39.750 --> 00:04:39.760
or 50 neurons in the third hidden layer
 

00:04:39.760 --> 00:04:42.090
or 50 neurons in the third hidden layer
so maybe a three is fifty by one

00:04:42.090 --> 00:04:42.100
so maybe a three is fifty by one
 

00:04:42.100 --> 00:04:44.460
so maybe a three is fifty by one
dimensional or if your factorization

00:04:44.460 --> 00:04:44.470
dimensional or if your factorization
 

00:04:44.470 --> 00:04:47.220
dimensional or if your factorization
will be 50 by M dimensional so if you

00:04:47.220 --> 00:04:47.230
will be 50 by M dimensional so if you
 

00:04:47.230 --> 00:04:49.260
will be 50 by M dimensional so if you
have a eighty percent chance of keeping

00:04:49.260 --> 00:04:49.270
have a eighty percent chance of keeping
 

00:04:49.270 --> 00:04:51.300
have a eighty percent chance of keeping
them type is enchanted eliminating them

00:04:51.300 --> 00:04:51.310
them type is enchanted eliminating them
 

00:04:51.310 --> 00:04:53.430
them type is enchanted eliminating them
this means that on average you end up

00:04:53.430 --> 00:04:53.440
this means that on average you end up
 

00:04:53.440 --> 00:04:56.820
this means that on average you end up
with ten units you know shut off for 10

00:04:56.820 --> 00:04:56.830
with ten units you know shut off for 10
 

00:04:56.830 --> 00:05:00.570
with ten units you know shut off for 10
units zero and so now if you look at the

00:05:00.570 --> 00:05:00.580
units zero and so now if you look at the
 

00:05:00.580 --> 00:05:03.270
units zero and so now if you look at the
value of V 4 V 4 is going to be equal to

00:05:03.270 --> 00:05:03.280
value of V 4 V 4 is going to be equal to
 

00:05:03.280 --> 00:05:09.350
value of V 4 V 4 is going to be equal to
W 4 times a 3 plus B 4 and so on

00:05:09.350 --> 00:05:09.360
W 4 times a 3 plus B 4 and so on
 

00:05:09.360 --> 00:05:13.620
W 4 times a 3 plus B 4 and so on
expectation this will be reduced by 20%

00:05:13.620 --> 00:05:13.630
expectation this will be reduced by 20%
 

00:05:13.630 --> 00:05:16.590
expectation this will be reduced by 20%
by which I mean that 20% of the elements

00:05:16.590 --> 00:05:16.600
by which I mean that 20% of the elements
 

00:05:16.600 --> 00:05:19.620
by which I mean that 20% of the elements
of a 3 will be 0 L so in order to not

00:05:19.620 --> 00:05:19.630
of a 3 will be 0 L so in order to not
 

00:05:19.630 --> 00:05:22.110
of a 3 will be 0 L so in order to not
reduce the expected value of B 4

00:05:22.110 --> 00:05:22.120
reduce the expected value of B 4
 

00:05:22.120 --> 00:05:24.240
reduce the expected value of B 4
what you do is you need to take this and

00:05:24.240 --> 00:05:24.250
what you do is you need to take this and
 

00:05:24.250 --> 00:05:29.730
what you do is you need to take this and
divide it by 0.8 because this will you

00:05:29.730 --> 00:05:29.740
divide it by 0.8 because this will you
 

00:05:29.740 --> 00:05:31.680
divide it by 0.8 because this will you
know correct or just bump it back up by

00:05:31.680 --> 00:05:31.690
know correct or just bump it back up by
 

00:05:31.690 --> 00:05:34.350
know correct or just bump it back up by
the roughly 20% a unique so it's to not

00:05:34.350 --> 00:05:34.360
the roughly 20% a unique so it's to not
 

00:05:34.360 --> 00:05:37.830
the roughly 20% a unique so it's to not
change the expected value of a 3 and so

00:05:37.830 --> 00:05:37.840
change the expected value of a 3 and so
 

00:05:37.840 --> 00:05:41.370
change the expected value of a 3 and so
this line here is what's called the

00:05:41.370 --> 00:05:41.380
this line here is what's called the
 

00:05:41.380 --> 00:05:43.770
this line here is what's called the
inverted dropout technique and this

00:05:43.770 --> 00:05:43.780
inverted dropout technique and this
 

00:05:43.780 --> 00:05:45.990
inverted dropout technique and this
effect is that no matter what you said

00:05:45.990 --> 00:05:46.000
effect is that no matter what you said
 

00:05:46.000 --> 00:05:48.150
effect is that no matter what you said
the key prop to whether there's point 8

00:05:48.150 --> 00:05:48.160
the key prop to whether there's point 8
 

00:05:48.160 --> 00:05:51.150
the key prop to whether there's point 8
or 4 9 or even one it deserves a wonder

00:05:51.150 --> 00:05:51.160
or 4 9 or even one it deserves a wonder
 

00:05:51.160 --> 00:05:52.740
or 4 9 or even one it deserves a wonder
there's no drop out because you keeping

00:05:52.740 --> 00:05:52.750
there's no drop out because you keeping
 

00:05:52.750 --> 00:05:55.530
there's no drop out because you keeping
everything 0.5 or whatever this inverted

00:05:55.530 --> 00:05:55.540
everything 0.5 or whatever this inverted
 

00:05:55.540 --> 00:05:57.630
everything 0.5 or whatever this inverted
dropout technique by dividing by the key

00:05:57.630 --> 00:05:57.640
dropout technique by dividing by the key
 

00:05:57.640 --> 00:05:59.730
dropout technique by dividing by the key
prop it ensures that the expected value

00:05:59.730 --> 00:05:59.740
prop it ensures that the expected value
 

00:05:59.740 --> 00:06:03.450
prop it ensures that the expected value
of a3 remains the same and it turns out

00:06:03.450 --> 00:06:03.460
of a3 remains the same and it turns out
 

00:06:03.460 --> 00:06:05.520
of a3 remains the same and it turns out
that at test time when you're trying to

00:06:05.520 --> 00:06:05.530
that at test time when you're trying to
 

00:06:05.530 --> 00:06:07.470
that at test time when you're trying to
be valid in your network we stop on the

00:06:07.470 --> 00:06:07.480
be valid in your network we stop on the
 

00:06:07.480 --> 00:06:09.510
be valid in your network we stop on the
next slide this inverted dropout

00:06:09.510 --> 00:06:09.520
next slide this inverted dropout
 

00:06:09.520 --> 00:06:11.850
next slide this inverted dropout
technique there's this lines etc through

00:06:11.850 --> 00:06:11.860
technique there's this lines etc through
 

00:06:11.860 --> 00:06:14.340
technique there's this lines etc through
the green box around this makes test

00:06:14.340 --> 00:06:14.350
the green box around this makes test
 

00:06:14.350 --> 00:06:16.320
the green box around this makes test
time easier because you have less of a

00:06:16.320 --> 00:06:16.330
time easier because you have less of a
 

00:06:16.330 --> 00:06:18.870
time easier because you have less of a
scaling problem but by far the most

00:06:18.870 --> 00:06:18.880
scaling problem but by far the most
 

00:06:18.880 --> 00:06:21.120
scaling problem but by far the most
common implantation of drop-off today or

00:06:21.120 --> 00:06:21.130
common implantation of drop-off today or
 

00:06:21.130 --> 00:06:22.890
common implantation of drop-off today or
as I know is inverted dropouts I

00:06:22.890 --> 00:06:22.900
as I know is inverted dropouts I
 

00:06:22.900 --> 00:06:24.660
as I know is inverted dropouts I
recommend you just links mentis but

00:06:24.660 --> 00:06:24.670
recommend you just links mentis but
 

00:06:24.670 --> 00:06:26.490
recommend you just links mentis but
there were some very iterations of

00:06:26.490 --> 00:06:26.500
there were some very iterations of
 

00:06:26.500 --> 00:06:29.940
there were some very iterations of
dropout then miss this / g probe line

00:06:29.940 --> 00:06:29.950
dropout then miss this / g probe line
 

00:06:29.950 --> 00:06:32.310
dropout then miss this / g probe line
and so at test time the album became

00:06:32.310 --> 00:06:32.320
and so at test time the album became
 

00:06:32.320 --> 00:06:33.930
and so at test time the album became
involved in more complicated but but

00:06:33.930 --> 00:06:33.940
involved in more complicated but but
 

00:06:33.940 --> 00:06:35.850
involved in more complicated but but
again people tend not to use those other

00:06:35.850 --> 00:06:35.860
again people tend not to use those other
 

00:06:35.860 --> 00:06:39.209
again people tend not to use those other
versions so what you do is you use the D

00:06:39.209 --> 00:06:39.219
versions so what you do is you use the D
 

00:06:39.219 --> 00:06:42.179
versions so what you do is you use the D
vector and you notice that for different

00:06:42.179 --> 00:06:42.189
vector and you notice that for different
 

00:06:42.189 --> 00:06:45.089
vector and you notice that for different
training examples you zero out different

00:06:45.089 --> 00:06:45.099
training examples you zero out different
 

00:06:45.099 --> 00:06:47.369
training examples you zero out different
hidden unions and in fact if you make

00:06:47.369 --> 00:06:47.379
hidden unions and in fact if you make
 

00:06:47.379 --> 00:06:48.990
hidden unions and in fact if you make
multiple passes through the same

00:06:48.990 --> 00:06:49.000
multiple passes through the same
 

00:06:49.000 --> 00:06:51.119
multiple passes through the same
training set then on different pulses

00:06:51.119 --> 00:06:51.129
training set then on different pulses
 

00:06:51.129 --> 00:06:52.469
training set then on different pulses
through the training set

00:06:52.469 --> 00:06:52.479
through the training set
 

00:06:52.479 --> 00:06:54.629
through the training set
you should randomly zero different

00:06:54.629 --> 00:06:54.639
you should randomly zero different
 

00:06:54.639 --> 00:06:56.429
you should randomly zero different
hidden units so it's not that for one

00:06:56.429 --> 00:06:56.439
hidden units so it's not that for one
 

00:06:56.439 --> 00:06:58.469
hidden units so it's not that for one
example you should keep dealing of the

00:06:58.469 --> 00:06:58.479
example you should keep dealing of the
 

00:06:58.479 --> 00:07:01.260
example you should keep dealing of the
drift same hidden units is that on

00:07:01.260 --> 00:07:01.270
drift same hidden units is that on
 

00:07:01.270 --> 00:07:03.570
drift same hidden units is that on
iteration one of gradient descent you

00:07:03.570 --> 00:07:03.580
iteration one of gradient descent you
 

00:07:03.580 --> 00:07:05.700
iteration one of gradient descent you
might zero something in unions and on

00:07:05.700 --> 00:07:05.710
might zero something in unions and on
 

00:07:05.710 --> 00:07:07.679
might zero something in unions and on
the second iteration again this and we

00:07:07.679 --> 00:07:07.689
the second iteration again this and we
 

00:07:07.689 --> 00:07:09.119
the second iteration again this and we
go through the training set a second

00:07:09.119 --> 00:07:09.129
go through the training set a second
 

00:07:09.129 --> 00:07:11.519
go through the training set a second
time maybe you set zero in a different

00:07:11.519 --> 00:07:11.529
time maybe you set zero in a different
 

00:07:11.529 --> 00:07:14.070
time maybe you set zero in a different
pattern of hidden units and the vector D

00:07:14.070 --> 00:07:14.080
pattern of hidden units and the vector D
 

00:07:14.080 --> 00:07:16.529
pattern of hidden units and the vector D
or D three for the third layer is used

00:07:16.529 --> 00:07:16.539
or D three for the third layer is used
 

00:07:16.539 --> 00:07:19.260
or D three for the third layer is used
to decide what's a zero out both in for

00:07:19.260 --> 00:07:19.270
to decide what's a zero out both in for
 

00:07:19.270 --> 00:07:21.719
to decide what's a zero out both in for
prop as well as in that problem just

00:07:21.719 --> 00:07:21.729
prop as well as in that problem just
 

00:07:21.729 --> 00:07:23.519
prop as well as in that problem just
showing forward prop here now having

00:07:23.519 --> 00:07:23.529
showing forward prop here now having
 

00:07:23.529 --> 00:07:25.619
showing forward prop here now having
trained the algorithm at test times

00:07:25.619 --> 00:07:25.629
trained the algorithm at test times
 

00:07:25.629 --> 00:07:27.929
trained the algorithm at test times
here's what you would do at time you're

00:07:27.929 --> 00:07:27.939
here's what you would do at time you're
 

00:07:27.939 --> 00:07:29.610
here's what you would do at time you're
given some X on which you want to make a

00:07:29.610 --> 00:07:29.620
given some X on which you want to make a
 

00:07:29.620 --> 00:07:31.469
given some X on which you want to make a
prediction and using our standard

00:07:31.469 --> 00:07:31.479
prediction and using our standard
 

00:07:31.479 --> 00:07:33.990
prediction and using our standard
notation I'm going to use a zero

00:07:33.990 --> 00:07:34.000
notation I'm going to use a zero
 

00:07:34.000 --> 00:07:36.029
notation I'm going to use a zero
activations of the zero of layer to

00:07:36.029 --> 00:07:36.039
activations of the zero of layer to
 

00:07:36.039 --> 00:07:38.670
activations of the zero of layer to
denote this test example X so what we're

00:07:38.670 --> 00:07:38.680
denote this test example X so what we're
 

00:07:38.680 --> 00:07:41.939
denote this test example X so what we're
going to do is not use dropout at test

00:07:41.939 --> 00:07:41.949
going to do is not use dropout at test
 

00:07:41.949 --> 00:07:44.219
going to do is not use dropout at test
time in particular which is going to set

00:07:44.219 --> 00:07:44.229
time in particular which is going to set
 

00:07:44.229 --> 00:07:51.990
time in particular which is going to set
Z 1 equals W 1 a 0 plus B 1 a 1 equals G

00:07:51.990 --> 00:07:52.000
Z 1 equals W 1 a 0 plus B 1 a 1 equals G
 

00:07:52.000 --> 00:08:03.510
Z 1 equals W 1 a 0 plus B 1 a 1 equals G
1 of Z 1 Z 2 equals W 2 a 1 plus B 2 a 2

00:08:03.510 --> 00:08:03.520
1 of Z 1 Z 2 equals W 2 a 1 plus B 2 a 2
 

00:08:03.520 --> 00:08:06.929
1 of Z 1 Z 2 equals W 2 a 1 plus B 2 a 2
equals and so on until where you get to

00:08:06.929 --> 00:08:06.939
equals and so on until where you get to
 

00:08:06.939 --> 00:08:08.129
equals and so on until where you get to
the last layer in the you make a

00:08:08.129 --> 00:08:08.139
the last layer in the you make a
 

00:08:08.139 --> 00:08:11.040
the last layer in the you make a
prediction Y hat but notice that at test

00:08:11.040 --> 00:08:11.050
prediction Y hat but notice that at test
 

00:08:11.050 --> 00:08:13.379
prediction Y hat but notice that at test
time you're not using dropout explicitly

00:08:13.379 --> 00:08:13.389
time you're not using dropout explicitly
 

00:08:13.389 --> 00:08:15.240
time you're not using dropout explicitly
and then you're not tossing coins around

00:08:15.240 --> 00:08:15.250
and then you're not tossing coins around
 

00:08:15.250 --> 00:08:17.640
and then you're not tossing coins around
them you're not flipping coins to decide

00:08:17.640 --> 00:08:17.650
them you're not flipping coins to decide
 

00:08:17.650 --> 00:08:20.459
them you're not flipping coins to decide
which hidden units to eliminate and

00:08:20.459 --> 00:08:20.469
which hidden units to eliminate and
 

00:08:20.469 --> 00:08:21.629
which hidden units to eliminate and
that's because we're making predictions

00:08:21.629 --> 00:08:21.639
that's because we're making predictions
 

00:08:21.639 --> 00:08:23.339
that's because we're making predictions
there test time you don't really want

00:08:23.339 --> 00:08:23.349
there test time you don't really want
 

00:08:23.349 --> 00:08:25.980
there test time you don't really want
your output to be random if you were

00:08:25.980 --> 00:08:25.990
your output to be random if you were
 

00:08:25.990 --> 00:08:27.749
your output to be random if you were
implementing dropout at test time that

00:08:27.749 --> 00:08:27.759
implementing dropout at test time that
 

00:08:27.759 --> 00:08:29.640
implementing dropout at test time that
just add noise to your predictions in

00:08:29.640 --> 00:08:29.650
just add noise to your predictions in
 

00:08:29.650 --> 00:08:32.939
just add noise to your predictions in
theory one thing you could do is run the

00:08:32.939 --> 00:08:32.949
theory one thing you could do is run the
 

00:08:32.949 --> 00:08:34.949
theory one thing you could do is run the
prediction process many times with

00:08:34.949 --> 00:08:34.959
prediction process many times with
 

00:08:34.959 --> 00:08:36.689
prediction process many times with
different hidden units randomly

00:08:36.689 --> 00:08:36.699
different hidden units randomly
 

00:08:36.699 --> 00:08:38.459
different hidden units randomly
drop-down and then average across them

00:08:38.459 --> 00:08:38.469
drop-down and then average across them
 

00:08:38.469 --> 00:08:41.009
drop-down and then average across them
but that's computationally inefficient

00:08:41.009 --> 00:08:41.019
but that's computationally inefficient
 

00:08:41.019 --> 00:08:43.529
but that's computationally inefficient
and it gives you roughly the same result

00:08:43.529 --> 00:08:43.539
and it gives you roughly the same result
 

00:08:43.539 --> 00:08:45.600
and it gives you roughly the same result
very very similar result to this to the

00:08:45.600 --> 00:08:45.610
very very similar result to this to the
 

00:08:45.610 --> 00:08:48.059
very very similar result to this to the
procedure as well and I just mention the

00:08:48.059 --> 00:08:48.069
procedure as well and I just mention the
 

00:08:48.069 --> 00:08:49.760
procedure as well and I just mention the
inverted dropout theorem

00:08:49.760 --> 00:08:49.770
inverted dropout theorem
 

00:08:49.770 --> 00:08:50.990
inverted dropout theorem
step on a previous slide where we

00:08:50.990 --> 00:08:51.000
step on a previous slide where we
 

00:08:51.000 --> 00:08:53.720
step on a previous slide where we
divided by the cheap problem The effect

00:08:53.720 --> 00:08:53.730
divided by the cheap problem The effect
 

00:08:53.730 --> 00:08:55.820
divided by the cheap problem The effect
of that was ensure that even when you

00:08:55.820 --> 00:08:55.830
of that was ensure that even when you
 

00:08:55.830 --> 00:08:59.000
of that was ensure that even when you
don't implement dropout and test time to

00:08:59.000 --> 00:08:59.010
don't implement dropout and test time to
 

00:08:59.010 --> 00:09:00.829
don't implement dropout and test time to
the scaling the expected value of these

00:09:00.829 --> 00:09:00.839
the scaling the expected value of these
 

00:09:00.839 --> 00:09:02.480
the scaling the expected value of these
activations don't change so you don't

00:09:02.480 --> 00:09:02.490
activations don't change so you don't
 

00:09:02.490 --> 00:09:04.760
activations don't change so you don't
need to add in an extra funny scaling

00:09:04.760 --> 00:09:04.770
need to add in an extra funny scaling
 

00:09:04.770 --> 00:09:06.980
need to add in an extra funny scaling
parameter at test time that's different

00:09:06.980 --> 00:09:06.990
parameter at test time that's different
 

00:09:06.990 --> 00:09:08.570
parameter at test time that's different
than when you had a training time

00:09:08.570 --> 00:09:08.580
than when you had a training time
 

00:09:08.580 --> 00:09:11.120
than when you had a training time
so that's dropouts and when your implant

00:09:11.120 --> 00:09:11.130
so that's dropouts and when your implant
 

00:09:11.130 --> 00:09:12.980
so that's dropouts and when your implant
is in this week's for an exercise you

00:09:12.980 --> 00:09:12.990
is in this week's for an exercise you
 

00:09:12.990 --> 00:09:15.590
is in this week's for an exercise you
gain more first-hand experience with it

00:09:15.590 --> 00:09:15.600
gain more first-hand experience with it
 

00:09:15.600 --> 00:09:18.590
gain more first-hand experience with it
as well but why does it really work what

00:09:18.590 --> 00:09:18.600
as well but why does it really work what
 

00:09:18.600 --> 00:09:20.240
as well but why does it really work what
I want to do in the next video is give

00:09:20.240 --> 00:09:20.250
I want to do in the next video is give
 

00:09:20.250 --> 00:09:21.980
I want to do in the next video is give
you some better intuition about what

00:09:21.980 --> 00:09:21.990
you some better intuition about what
 

00:09:21.990 --> 00:09:24.230
you some better intuition about what
dropout really is doing let's go on to

00:09:24.230 --> 00:09:24.240
dropout really is doing let's go on to
 

00:09:24.240 --> 00:09:26.810
dropout really is doing let's go on to
the next video

