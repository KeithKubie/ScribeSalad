WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:03.429
It’s safe to say that our private information
isn’t private anymore.

00:00:03.429 --> 00:00:07.649
New technologies are collecting data to be
sold or shared between companies… in sometimes

00:00:07.649 --> 00:00:08.780
questionable ways.

00:00:08.780 --> 00:00:10.460
Y’all really think that Black Mirror isn’t
gonna happen?

00:00:10.460 --> 00:00:12.130
Well, it’s not GOING to happen.

00:00:12.130 --> 00:00:14.090
It’s already happening.

00:00:14.090 --> 00:00:18.230
Let’s start in China, where employers are
monitoring their employees’ brain waves.

00:00:18.230 --> 00:00:21.320
Oh yeah, you heard that right, monitoring
their brain waves.

00:00:21.320 --> 00:00:25.150
To be clear, they are not attempting to read
the workers’ thoughts, but their emotions.

00:00:25.150 --> 00:00:29.140
Factories, state-owned enterprises, and sections
of the Chinese military are placing wireless

00:00:29.140 --> 00:00:34.070
sensors in employees’ hats that record and
transmit data similar to an electroencephalogram,

00:00:34.070 --> 00:00:35.360
or EEG.

00:00:35.360 --> 00:00:39.400
By analyzing the incoming sensor data, AI
models can detect anomalies that might indicate

00:00:39.400 --> 00:00:40.970
a spike in anger, depression, or anxiety.

00:00:40.970 --> 00:00:44.530
This system is said to help employers find
out who’s stressed, modulate break times,

00:00:44.530 --> 00:00:48.860
and increase productivity… in turn spiking
company profits by an estimated two billion

00:00:48.860 --> 00:00:51.000
yuan since 2014.

00:00:51.000 --> 00:00:54.510
This tech is being used elsewhere too, like
assessing fatigue in high-speed train drivers

00:00:54.510 --> 00:00:56.100
and monitoring patients in hospitals.

00:00:56.100 --> 00:00:59.290
Sure, it would be dope to find out if your
significant other is really fine when they

00:00:59.290 --> 00:01:01.260
say “I’m fine” after a fight.

00:01:01.260 --> 00:01:03.400
But, how do you regulate something like this?

00:01:03.400 --> 00:01:07.400
If emotional data is mineable, what happens
if companies nefariously use it to abuse their

00:01:07.400 --> 00:01:08.400
power?

00:01:08.400 --> 00:01:10.030
I listen to a lot of SZA and Drake -- I’m
emotional!

00:01:10.030 --> 00:01:12.530
Please don’t use my emotions against me.

00:01:12.530 --> 00:01:17.040
China has a Social Credit score: a clout score
based on your criminal record, donations to

00:01:17.040 --> 00:01:21.549
charity, loyalty to political parties, how
many video games you buy, and even your friends’

00:01:21.549 --> 00:01:22.549
social credit scores.

00:01:22.549 --> 00:01:26.280
This is just like Black Mirror Series three’s
Nosedive, where everyone has a score based

00:01:26.280 --> 00:01:27.820
on all social interactions.

00:01:27.820 --> 00:01:31.270
The Chinese government claims it’s trying
to build “trust” with this score, but

00:01:31.270 --> 00:01:32.640
its implications can be more sinister.

00:01:32.640 --> 00:01:36.970
For instance, in twenty-sixteen, a man was
denied a plane ticket because a judge deemed

00:01:36.970 --> 00:01:41.970
a court apology “insincere” and placed
him on a blacklist, tanking his score.

00:01:41.970 --> 00:01:45.670
“Insincerity” is hella subjective, so
how would we regulate for everyone’s opinions?

00:01:45.670 --> 00:01:49.640
Finally, China is using all this information
to make you into a precog: they’re literally

00:01:49.640 --> 00:01:54.470
trying to predict political instability using
feeds from surveillance cameras, phone usage,

00:01:54.470 --> 00:01:56.120
travel records and religious orientation.

00:01:56.120 --> 00:02:00.280
Extrapolating the negative consequences, this
taps into personal data and can unfairly target

00:02:00.280 --> 00:02:04.460
groups based on prejudice, specifically the
Uyghur, and other predominantly Muslim populations.

00:02:04.460 --> 00:02:07.820
And let’s just say you protest this state-sponsored
measure.

00:02:07.820 --> 00:02:11.560
That affects your social credit score, which
can then deny you things like plane tickets

00:02:11.560 --> 00:02:13.940
and jobs, keeping you trapped by the system.

00:02:13.940 --> 00:02:17.530
Tracking every arena –– personal, professional,
recreational, political, et cetera, –– is

00:02:17.530 --> 00:02:20.950
dangerous, especially in the United States,
where we value life, liberty, and the pursuit

00:02:20.950 --> 00:02:21.950
of happiness.

00:02:21.950 --> 00:02:25.810
&nbsp;Like we don’t already know that the government
is in our webcams, Siris, and Alexa!

00:02:25.810 --> 00:02:27.580
...Thanks.

00:02:27.580 --> 00:02:33.030
It’s pretty spooky to think about how systemic
issues we’re already grappling pretty hard

00:02:33.030 --> 00:02:38.950
with as a society, such as all these biases...
could be magnified by technology we’ve already

00:02:38.950 --> 00:02:39.950
developed.

00:02:39.950 --> 00:02:43.090
America has a lot to deal with right now,
so maybe we should sit this tech out.

00:02:43.090 --> 00:02:47.150
All these tools can have a prosocial end goal,
but it’s too soon to tell if the ends justify

00:02:47.150 --> 00:02:48.209
the means.

00:02:48.209 --> 00:02:50.560
Data will continue to be collected on us,
that’s for sure.

00:02:50.560 --> 00:02:54.430
But, with few regulatory systems in place,
we gotta keep an eye on this new tech that’s

00:02:54.430 --> 00:02:58.420
already just chillin' here, and stop pretending
that this is all happening in some distant

00:02:58.420 --> 00:02:59.420
dystopian future.

00:02:59.420 --> 00:03:03.830
Not only is AI going to run the world someday,
but it's already being used to predict the

00:03:03.830 --> 00:03:05.150
next global pandemic.

00:03:05.150 --> 00:03:06.150
Wanna find out how?

00:03:06.150 --> 00:03:07.150
Bet you do.

00:03:07.150 --> 00:03:08.280
Check out this video right here.

00:03:08.280 --> 00:03:11.870
Thanks for watching, subscribe to Seeker,
and come back for new videos every day so

00:03:11.870 --> 00:03:13.660
you can watch your computer as much as it's
watching you.

