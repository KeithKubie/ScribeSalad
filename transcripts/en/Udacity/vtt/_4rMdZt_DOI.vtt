WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:02.400
So that's really all I wanted to say about genetic algorithms.

00:00:02.400 --> 00:00:04.750
I mean, there's lots of tweaky things that you need to

00:00:04.750 --> 00:00:07.650
do to get this to work very effectively. You have some

00:00:07.650 --> 00:00:09.780
choice about how to represent the input, and you have some

00:00:09.780 --> 00:00:12.600
choice about how you can do your selection, and your fit

00:00:12.600 --> 00:00:15.620
to finding your fitness function. But, at a generic level, this

00:00:15.620 --> 00:00:17.460
is, this is, it's a, it's a useful thing. Some people

00:00:17.460 --> 00:00:20.750
call genetic algorithms the second best solution to any given problem.

00:00:20.750 --> 00:00:21.070
&gt;&gt; Hmm.

00:00:21.070 --> 00:00:23.340
&gt;&gt; So, it's a good thing to have in your toolbox. But

00:00:23.340 --> 00:00:25.420
I think that's, that's really it. That's all I wanted to say about

00:00:25.420 --> 00:00:28.600
randomized optimization. So what have we learned?

00:00:28.600 --> 00:00:34.030
&gt;&gt; Well, we learned about random optimization period. That that

00:00:34.030 --> 00:00:36.570
there is a notion of optimization in the first place.

00:00:36.570 --> 00:00:39.180
&gt;&gt; So, we talked about optimization in general

00:00:39.180 --> 00:00:40.840
and then we, what was the randomized part?

00:00:40.840 --> 00:00:43.310
&gt;&gt; Well, we'd take random steps where we

00:00:43.310 --> 00:00:46.360
start off in random places. Or, we'd do random

00:00:46.360 --> 00:00:48.060
kind, well actually that's really it. You take

00:00:48.060 --> 00:00:51.360
random steps, you start off in random places and

00:00:51.360 --> 00:00:55.960
it's a way to overcome when you can't take a natural gradient step.

00:00:55.960 --> 00:00:58.230
&gt;&gt; That's right. So did we talk, and we

00:00:58.230 --> 00:01:00.150
talked about some particular randomization.

00:01:00.150 --> 00:01:03.170
Er, sorry, randomized optimization algorithms.

00:01:03.170 --> 00:01:05.230
&gt;&gt; Let's see. There was randomized hill climbing.

00:01:05.230 --> 00:01:07.040
&gt;&gt; And we had 2 flavors of that.

00:01:07.040 --> 00:01:12.680
&gt;&gt; Right. We did simulated annealing. And, we did genetic algorithms.

00:01:12.680 --> 00:01:16.520
&gt;&gt; And don' t forget, that we talked a little bit about how this all connects

00:01:16.520 --> 00:01:17.990
back up with learning, because in many

00:01:17.990 --> 00:01:20.660
cases, we're searching some parameter space to

00:01:20.660 --> 00:01:23.940
find a good classifier, a good regression

00:01:23.940 --> 00:01:28.480
function. A later in this particular sub-unit

00:01:28.480 --> 00:01:30.470
we're going to be talking about, finding good

00:01:30.470 --> 00:01:33.250
clusterings. And so, this notion of finding

00:01:33.250 --> 00:01:35.060
something that's good, finding a way to,

00:01:35.060 --> 00:01:38.630
to be optimal is pervasive through apache learning.

00:01:38.630 --> 00:01:41.650
&gt;&gt; Oh that make sense. Well, there, well, if we're trying to remember all

00:01:41.650 --> 00:01:42.960
these other things we learned. We

00:01:42.960 --> 00:01:46.220
also learned that AI researchers like analogies.

00:01:46.220 --> 00:01:47.883
&gt;&gt; [LAUGH]

00:01:47.883 --> 00:01:49.260
&gt;&gt; Both simulating annealing and generic

00:01:49.260 --> 00:01:51.410
algorithms are analogies. And they don't

00:01:51.410 --> 00:01:52.850
just like analogies, they like taking

00:01:52.850 --> 00:01:55.770
analogies and pushing them until they break.

00:01:55.770 --> 00:01:57.670
&gt;&gt; Indeed, actually hill climbing [LAUGH] is an analogy too.

00:01:57.670 --> 00:01:59.350
&gt;&gt; Yeah, actually, right? Every single thing that we

00:01:59.350 --> 00:02:02.390
did is an analogy to something. Okay, that's good. The

00:02:02.390 --> 00:02:03.790
other thing that we learned, which I think is

00:02:03.790 --> 00:02:06.680
important, is that there's no way to talk about cross,

00:02:06.680 --> 00:02:12.244
crossover without getting a bunch of people in the studio to giggle. [LAUGH]

00:02:12.244 --> 00:02:15.150
&gt;&gt; Yeah, genetic algorithms make people blush.

00:02:15.150 --> 00:02:16.230
&gt;&gt; Okay, that's pretty good, but Michael you

00:02:16.230 --> 00:02:17.610
know, I'm, I'm looking at all this and

00:02:17.610 --> 00:02:18.780
now that you put all these words together

00:02:18.780 --> 00:02:21.275
on one slide, I have 2 observations I want to

00:02:21.275 --> 00:02:23.890
make. That that are kind of bothering me. So,

00:02:23.890 --> 00:02:26.145
one is, I'm looking at hill climbing that makes

00:02:26.145 --> 00:02:29.280
sense, hill climbing restarts makes sense, simulated annealing

00:02:29.280 --> 00:02:31.865
makes sense, [INAUDIBLE] but you know what, they don't

00:02:31.865 --> 00:02:34.283
really remember a lot. So, what do I mean by that?

00:02:34.283 --> 00:02:36.263
So you do all this hill climbing and you go 8

00:02:36.263 --> 00:02:39.020
billion steps, and then what happens? You end up with the

00:02:39.020 --> 00:02:43.020
point. You do simulated annealing. You do all this fancy stuff

00:02:43.020 --> 00:02:48.160
with slowly changing your temperature, and creating swords with black smiths,

00:02:48.160 --> 00:02:50.290
and all that other stuff you talked about and in the

00:02:50.290 --> 00:02:53.280
end, at every step, the only thing you remember is, where

00:02:53.280 --> 00:02:56.810
you are and maybe where you last were. And with genetic algorithms,

00:02:56.810 --> 00:02:59.410
it's a little more complicated that because you keep a population,

00:02:59.410 --> 00:03:02.500
but really you're just keeping track of where you are, not

00:03:02.500 --> 00:03:05.672
where you've been. So in some sense, the only difference between

00:03:05.672 --> 00:03:09.110
the 1 millionth iteration, and the 1st iteration is that you might

00:03:09.110 --> 00:03:11.800
be at a better point. And it just feels like, if

00:03:11.800 --> 00:03:15.250
you're going to go through all this trouble of going through what is

00:03:15.250 --> 00:03:18.390
some complicated space that hopefully has some structure, there should be

00:03:18.390 --> 00:03:21.840
some way to communicate information about that structure as you go along.

00:03:21.840 --> 00:03:23.838
And that just, that sort of bothers. So that's

00:03:23.838 --> 00:03:26.140
one thing. The second thing is, what I really

00:03:26.140 --> 00:03:28.710
liked about simulating annealing, other than, you know, the

00:03:28.710 --> 00:03:33.310
analogy and hearing you talk about strong swords, is that

00:03:33.310 --> 00:03:35.700
it came out at the end with a really

00:03:35.700 --> 00:03:38.640
nice result, which is this Boltzmann distribution, that there's

00:03:38.640 --> 00:03:42.540
some probability distribution that we can understand, that is

00:03:42.540 --> 00:03:46.930
actually trying to model. So, here are my questions then.

00:03:46.930 --> 00:03:48.022
It's the long way of asking a real

00:03:48.022 --> 00:03:50.820
simple question. Is there something out there, something

00:03:50.820 --> 00:03:53.550
we can say more about? Not just keeping

00:03:53.550 --> 00:03:55.480
track of points, but keeping track of structure

00:03:55.480 --> 00:03:58.110
and information. And is there some way that

00:03:58.110 --> 00:04:00.080
we can take advantage of the fact that,

00:04:00.080 --> 00:04:03.540
all of these things are somehow tracking probability

00:04:03.540 --> 00:04:05.920
distributions just by their very nature of being randomized.

00:04:05.920 --> 00:04:09.280
&gt;&gt; That's, that's outstanding. Yes, very good, very good question.

00:04:09.280 --> 00:04:11.830
It is true that these are all kind of amnesic,

00:04:11.830 --> 00:04:14.400
right? They kind of just wander around, and forget

00:04:14.400 --> 00:04:16.550
everything about what they learned. They don't really learn about

00:04:16.550 --> 00:04:19.810
the optimization space itself. And use that information to be

00:04:19.810 --> 00:04:23.325
more effective. There are some other algorithms that these are,

00:04:23.325 --> 00:04:26.405
these are kind of the simplest algorithms, but you can re-combine

00:04:26.405 --> 00:04:29.485
these ideas you know, sort of cross-over style, to get

00:04:29.485 --> 00:04:33.690
other more powerful algorithms. There's one that's called taboo search,

00:04:33.690 --> 00:04:37.130
that specifically tries to remember where you've been, and you're

00:04:37.130 --> 00:04:39.660
supposed to avoid it. Right, they become taboo regions.

00:04:40.690 --> 00:04:42.920
To stay, with the idea that you should stay away

00:04:42.920 --> 00:04:45.480
from regions where you've already done a lot of evaluations,

00:04:45.480 --> 00:04:48.090
so you cover the space better. And then there's other

00:04:48.090 --> 00:04:50.170
methods that are, have been popular for a while

00:04:50.170 --> 00:04:53.120
that are gaining in popularity, where they explicitly model the

00:04:53.120 --> 00:04:56.590
probability distribution over where good solutions might be. So they

00:04:56.590 --> 00:04:58.690
might be worth actually talking a little more about that.

00:04:58.690 --> 00:05:03.040
&gt;&gt; Okay, so go ahead. Well, so I kind of added time.

00:05:03.040 --> 00:05:05.030
Maybe maybe you could look into this, I can

00:05:05.030 --> 00:05:07.430
give you some references and maybe you can report back.

00:05:07.430 --> 00:05:08.280
&gt;&gt; Fine.

00:05:08.280 --> 00:05:10.380
&gt;&gt; [LAUGH]

00:05:10.380 --> 00:05:12.290
You'll learn, you'll learn very well by doing that.

00:05:12.290 --> 00:05:12.938
&gt;&gt; Yes sir.

00:05:12.938 --> 00:05:15.610
&gt;&gt; [LAUGH] Alright, we'll see you then, then.

00:05:15.610 --> 00:05:18.330
&gt;&gt; Okay, I'll see you then, then too. Bye, Michael.

00:05:18.330 --> 00:05:19.140
&gt;&gt; Bye Charles.

