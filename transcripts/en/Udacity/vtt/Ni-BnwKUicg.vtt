WEBVTT
Kind: captions
Language: en

00:00:00.440 --> 00:00:04.260
Here is the basic algorithm for
incremental concept learning and

00:00:04.260 --> 00:00:07.660
David has created a visual
illustration of this algorithm.

00:00:07.660 --> 00:00:11.890
We're given an example and we're also
told whether it's a positive example or

00:00:11.890 --> 00:00:12.720
a negative example.

00:00:13.775 --> 00:00:15.338
If this is a positive example,

00:00:15.338 --> 00:00:19.600
then the algorithm comes to the left
branch of this particular tree.

00:00:19.600 --> 00:00:23.090
And it asks does the current
definition of the concept

00:00:23.090 --> 00:00:24.880
cover this positive example?

00:00:24.880 --> 00:00:26.730
We want to cover positive examples.

00:00:28.000 --> 00:00:31.360
If it already covers the positive
example, we don't have to do anything.

00:00:31.360 --> 00:00:34.580
We don't have to devise a current
definition of the concept.

00:00:34.580 --> 00:00:38.420
On the other hand, if the current
definition of the concept does not

00:00:38.420 --> 00:00:41.820
cover the positive example then
we must revise it in some way so

00:00:41.820 --> 00:00:44.460
that it does, so we will generalize it.

00:00:44.460 --> 00:00:45.745
On the other half of the tree,

00:00:45.745 --> 00:00:50.470
if this example is not a positive
instance of the example, then

00:00:50.470 --> 00:00:54.230
we can ask ourselves does the current
definition of the concept cover it?

00:00:55.580 --> 00:00:57.900
If it doesn't cover it,
it shouldn't cover it.

00:00:57.900 --> 00:01:00.910
And if it doesn't cover it,
then we don't have to do anything.

00:01:00.910 --> 00:01:04.800
On the other hand, if the example
is a negative instance and

00:01:04.800 --> 00:01:06.910
if current definition does cover it,

00:01:06.910 --> 00:01:10.560
then we want to define our current
definition to rule it out.

00:01:10.560 --> 00:01:13.170
So we'll specialize in
a current definition.

00:01:13.170 --> 00:01:16.850
&gt;&gt; So, oftentimes, we see children
committing overgeneralization or

00:01:16.850 --> 00:01:18.340
overspecialization.

00:01:18.340 --> 00:01:21.200
So, to take an example of this,
we can imagine a child

00:01:21.200 --> 00:01:23.720
that has a concept of a cat,
but the cat has to be black.

00:01:24.870 --> 00:01:28.150
The child has only ever been around
black cats, so part of their definition,

00:01:28.150 --> 00:01:30.660
part of their concept of
the cat is that cats are black.

00:01:31.730 --> 00:01:33.890
When she goes over to
her friend's house,

00:01:33.890 --> 00:01:37.140
Is introduced to her friend's cat,
and her friend's cat is orange.

00:01:37.140 --> 00:01:40.550
Right now, she's told that this
is an example of a cat, but

00:01:40.550 --> 00:01:43.020
it does not fit her current
definition of a cat so

00:01:43.020 --> 00:01:46.740
she needs to generalize her definition
that cats can be different colors.

00:01:46.740 --> 00:01:51.430
Similarly, we can imagine another child
that has only ever been exposed to dogs.

00:01:51.430 --> 00:01:55.610
Thus the child's concept of a dog is
that a dog is anything that is furry,

00:01:55.610 --> 00:01:58.170
has four legs and that we keep as a pet.

00:01:58.170 --> 00:02:00.040
This child goes over to
the same friend's house and

00:02:00.040 --> 00:02:02.110
is introduced to this orange cat.

00:02:02.110 --> 00:02:04.810
And right now, that orange cat
fits his definition of a dog.

00:02:04.810 --> 00:02:06.860
It's furry, it has four legs,
and they keep it as a pet.

00:02:06.860 --> 00:02:09.320
But he's told that
this cat is not a dog.

00:02:09.320 --> 00:02:14.790
So he needs to specialize his concept
of a dog to exclude this cat.

00:02:14.790 --> 00:02:15.470
&gt;&gt; That's good, David.

00:02:15.470 --> 00:02:17.890
It connects things with
our everyday lives.

