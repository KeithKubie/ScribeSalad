WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:01.880
 
one of the things that might help speed

00:00:01.880 --> 00:00:01.890
one of the things that might help speed
 

00:00:01.890 --> 00:00:04.039
one of the things that might help speed
up your learning algorithm is to slowly

00:00:04.039 --> 00:00:04.049
up your learning algorithm is to slowly
 

00:00:04.049 --> 00:00:06.230
up your learning algorithm is to slowly
reduce your learning rate over time we

00:00:06.230 --> 00:00:06.240
reduce your learning rate over time we
 

00:00:06.240 --> 00:00:08.720
reduce your learning rate over time we
call this learning rate decay let's see

00:00:08.720 --> 00:00:08.730
call this learning rate decay let's see
 

00:00:08.730 --> 00:00:11.089
call this learning rate decay let's see
how you can implement this let's start -

00:00:11.089 --> 00:00:11.099
how you can implement this let's start -
 

00:00:11.099 --> 00:00:12.860
how you can implement this let's start -
an example of why you might want to

00:00:12.860 --> 00:00:12.870
an example of why you might want to
 

00:00:12.870 --> 00:00:14.720
an example of why you might want to
implement learning rate decay

00:00:14.720 --> 00:00:14.730
implement learning rate decay
 

00:00:14.730 --> 00:00:17.210
implement learning rate decay
suppose you're implementing mini batch

00:00:17.210 --> 00:00:17.220
suppose you're implementing mini batch
 

00:00:17.220 --> 00:00:19.130
suppose you're implementing mini batch
gradient descent with a reasonably small

00:00:19.130 --> 00:00:19.140
gradient descent with a reasonably small
 

00:00:19.140 --> 00:00:21.310
gradient descent with a reasonably small
mini batch maybe a mini batch has just

00:00:21.310 --> 00:00:21.320
mini batch maybe a mini batch has just
 

00:00:21.320 --> 00:00:25.939
mini batch maybe a mini batch has just
64 128 examples then as you iterate your

00:00:25.939 --> 00:00:25.949
64 128 examples then as you iterate your
 

00:00:25.949 --> 00:00:28.580
64 128 examples then as you iterate your
steps will be a little bit noisy and it

00:00:28.580 --> 00:00:28.590
steps will be a little bit noisy and it
 

00:00:28.590 --> 00:00:31.099
steps will be a little bit noisy and it
will tend toward this minimum over here

00:00:31.099 --> 00:00:31.109
will tend toward this minimum over here
 

00:00:31.109 --> 00:00:34.130
will tend toward this minimum over here
but it won't exactly converge but your

00:00:34.130 --> 00:00:34.140
but it won't exactly converge but your
 

00:00:34.140 --> 00:00:35.810
but it won't exactly converge but your
algorithm might just end up wandering

00:00:35.810 --> 00:00:35.820
algorithm might just end up wandering
 

00:00:35.820 --> 00:00:39.950
algorithm might just end up wandering
around and never really converge because

00:00:39.950 --> 00:00:39.960
around and never really converge because
 

00:00:39.960 --> 00:00:42.020
around and never really converge because
you're using some fixed value for alpha

00:00:42.020 --> 00:00:42.030
you're using some fixed value for alpha
 

00:00:42.030 --> 00:00:44.869
you're using some fixed value for alpha
and there's just some noise in your

00:00:44.869 --> 00:00:44.879
and there's just some noise in your
 

00:00:44.879 --> 00:00:47.450
and there's just some noise in your
different mini batches but if you were

00:00:47.450 --> 00:00:47.460
different mini batches but if you were
 

00:00:47.460 --> 00:00:51.709
different mini batches but if you were
to slowly reduce your learning rate

00:00:51.709 --> 00:00:51.719
to slowly reduce your learning rate
 

00:00:51.719 --> 00:00:53.930
to slowly reduce your learning rate
alpha then during the initial phases

00:00:53.930 --> 00:00:53.940
alpha then during the initial phases
 

00:00:53.940 --> 00:00:55.939
alpha then during the initial phases
while your learning rate alpha still

00:00:55.939 --> 00:00:55.949
while your learning rate alpha still
 

00:00:55.949 --> 00:00:58.160
while your learning rate alpha still
lasts you can still have it to be fast

00:00:58.160 --> 00:00:58.170
lasts you can still have it to be fast
 

00:00:58.170 --> 00:01:01.189
lasts you can still have it to be fast
learning but then as alpha gets smaller

00:01:01.189 --> 00:01:01.199
learning but then as alpha gets smaller
 

00:01:01.199 --> 00:01:04.179
learning but then as alpha gets smaller
your steps you take would be slower and

00:01:04.179 --> 00:01:04.189
your steps you take would be slower and
 

00:01:04.189 --> 00:01:07.730
your steps you take would be slower and
smaller and so you end up oscillating in

00:01:07.730 --> 00:01:07.740
smaller and so you end up oscillating in
 

00:01:07.740 --> 00:01:10.910
smaller and so you end up oscillating in
a tighter region around this minimum

00:01:10.910 --> 00:01:10.920
a tighter region around this minimum
 

00:01:10.920 --> 00:01:14.090
a tighter region around this minimum
rather than one ring far away even as

00:01:14.090 --> 00:01:14.100
rather than one ring far away even as
 

00:01:14.100 --> 00:01:16.100
rather than one ring far away even as
training goes on and on so the intuition

00:01:16.100 --> 00:01:16.110
training goes on and on so the intuition
 

00:01:16.110 --> 00:01:19.490
training goes on and on so the intuition
behind slowly reducing alpha is that

00:01:19.490 --> 00:01:19.500
behind slowly reducing alpha is that
 

00:01:19.500 --> 00:01:21.320
behind slowly reducing alpha is that
maybe during the initial steps of

00:01:21.320 --> 00:01:21.330
maybe during the initial steps of
 

00:01:21.330 --> 00:01:23.570
maybe during the initial steps of
learning you can afford to take much

00:01:23.570 --> 00:01:23.580
learning you can afford to take much
 

00:01:23.580 --> 00:01:26.170
learning you can afford to take much
bigger steps but then as learning

00:01:26.170 --> 00:01:26.180
bigger steps but then as learning
 

00:01:26.180 --> 00:01:29.510
bigger steps but then as learning
approaches convergence then having a

00:01:29.510 --> 00:01:29.520
approaches convergence then having a
 

00:01:29.520 --> 00:01:31.340
approaches convergence then having a
slower learning rate allows you to take

00:01:31.340 --> 00:01:31.350
slower learning rate allows you to take
 

00:01:31.350 --> 00:01:34.100
slower learning rate allows you to take
smaller steps so here's how you can

00:01:34.100 --> 00:01:34.110
smaller steps so here's how you can
 

00:01:34.110 --> 00:01:36.890
smaller steps so here's how you can
implement learning rate decay recall

00:01:36.890 --> 00:01:36.900
implement learning rate decay recall
 

00:01:36.900 --> 00:01:42.859
implement learning rate decay recall
that one epoch is one class through the

00:01:42.859 --> 00:01:42.869
that one epoch is one class through the
 

00:01:42.869 --> 00:01:46.789
that one epoch is one class through the
data right so if you have them a

00:01:46.789 --> 00:01:46.799
data right so if you have them a
 

00:01:46.799 --> 00:01:50.840
data right so if you have them a
training set as follows maybe break it

00:01:50.840 --> 00:01:50.850
training set as follows maybe break it
 

00:01:50.850 --> 00:01:56.389
training set as follows maybe break it
up into different mini batches then once

00:01:56.389 --> 00:01:56.399
up into different mini batches then once
 

00:01:56.399 --> 00:01:58.100
up into different mini batches then once
the first pass through the training set

00:01:58.100 --> 00:01:58.110
the first pass through the training set
 

00:01:58.110 --> 00:02:01.130
the first pass through the training set
is called the first epoch and then the

00:02:01.130 --> 00:02:01.140
is called the first epoch and then the
 

00:02:01.140 --> 00:02:04.789
is called the first epoch and then the
second pass is the second epoch and so

00:02:04.789 --> 00:02:04.799
second pass is the second epoch and so
 

00:02:04.799 --> 00:02:07.310
second pass is the second epoch and so
on so one thing you could do is set your

00:02:07.310 --> 00:02:07.320
on so one thing you could do is set your
 

00:02:07.320 --> 00:02:10.609
on so one thing you could do is set your
learning rate alpha to be equal to one

00:02:10.609 --> 00:02:10.619
learning rate alpha to be equal to one
 

00:02:10.619 --> 00:02:13.230
learning rate alpha to be equal to one
over one plus a per hour

00:02:13.230 --> 00:02:13.240
over one plus a per hour
 

00:02:13.240 --> 00:02:18.390
over one plus a per hour
originally called the decay rate times

00:02:18.390 --> 00:02:18.400
originally called the decay rate times
 

00:02:18.400 --> 00:02:22.980
originally called the decay rate times
the epoch num and there's going to be

00:02:22.980 --> 00:02:22.990
the epoch num and there's going to be
 

00:02:22.990 --> 00:02:25.680
the epoch num and there's going to be
times some initial learning rate alpha

00:02:25.680 --> 00:02:25.690
times some initial learning rate alpha
 

00:02:25.690 --> 00:02:26.820
times some initial learning rate alpha
zero

00:02:26.820 --> 00:02:26.830
zero
 

00:02:26.830 --> 00:02:29.040
zero
note that the decay rate here it becomes

00:02:29.040 --> 00:02:29.050
note that the decay rate here it becomes
 

00:02:29.050 --> 00:02:31.080
note that the decay rate here it becomes
another hyper parameter which you might

00:02:31.080 --> 00:02:31.090
another hyper parameter which you might
 

00:02:31.090 --> 00:02:33.000
another hyper parameter which you might
need to tune so here's a concrete

00:02:33.000 --> 00:02:33.010
need to tune so here's a concrete
 

00:02:33.010 --> 00:02:36.810
need to tune so here's a concrete
example um if you take several epochs so

00:02:36.810 --> 00:02:36.820
example um if you take several epochs so
 

00:02:36.820 --> 00:02:39.800
example um if you take several epochs so
several passes through your data if

00:02:39.800 --> 00:02:39.810
several passes through your data if
 

00:02:39.810 --> 00:02:42.420
several passes through your data if
alpha zero is equal to zero point two

00:02:42.420 --> 00:02:42.430
alpha zero is equal to zero point two
 

00:02:42.430 --> 00:02:47.040
alpha zero is equal to zero point two
and the decay rate is equal to one then

00:02:47.040 --> 00:02:47.050
and the decay rate is equal to one then
 

00:02:47.050 --> 00:02:50.370
and the decay rate is equal to one then
doing your first epoch alpha will be 1

00:02:50.370 --> 00:02:50.380
doing your first epoch alpha will be 1
 

00:02:50.380 --> 00:02:55.380
doing your first epoch alpha will be 1
over 1 plus 1 times alpha 0 so your

00:02:55.380 --> 00:02:55.390
over 1 plus 1 times alpha 0 so your
 

00:02:55.390 --> 00:02:58.910
over 1 plus 1 times alpha 0 so your
learning rate will be zero point one

00:02:58.910 --> 00:02:58.920
learning rate will be zero point one
 

00:02:58.920 --> 00:03:01.830
learning rate will be zero point one
that's just your evaluating this formula

00:03:01.830 --> 00:03:01.840
that's just your evaluating this formula
 

00:03:01.840 --> 00:03:03.780
that's just your evaluating this formula
when the decay rate is equal to 1 and

00:03:03.780 --> 00:03:03.790
when the decay rate is equal to 1 and
 

00:03:03.790 --> 00:03:06.270
when the decay rate is equal to 1 and
the epochal on this one on the second

00:03:06.270 --> 00:03:06.280
the epochal on this one on the second
 

00:03:06.280 --> 00:03:09.060
the epochal on this one on the second
you pop your learning rate the case to

00:03:09.060 --> 00:03:09.070
you pop your learning rate the case to
 

00:03:09.070 --> 00:03:14.790
you pop your learning rate the case to
0.67 on the third 0.5 on the fourth 0.4

00:03:14.790 --> 00:03:14.800
0.67 on the third 0.5 on the fourth 0.4
 

00:03:14.800 --> 00:03:16.020
0.67 on the third 0.5 on the fourth 0.4
and so on

00:03:16.020 --> 00:03:16.030
and so on
 

00:03:16.030 --> 00:03:17.670
and so on
fearful evaluate well these values

00:03:17.670 --> 00:03:17.680
fearful evaluate well these values
 

00:03:17.680 --> 00:03:19.440
fearful evaluate well these values
yourself and get a sense that you know

00:03:19.440 --> 00:03:19.450
yourself and get a sense that you know
 

00:03:19.450 --> 00:03:22.320
yourself and get a sense that you know
as a function of your epoch number your

00:03:22.320 --> 00:03:22.330
as a function of your epoch number your
 

00:03:22.330 --> 00:03:24.330
as a function of your epoch number your
learning rate gradually decreases

00:03:24.330 --> 00:03:24.340
learning rate gradually decreases
 

00:03:24.340 --> 00:03:27.630
learning rate gradually decreases
whereas this according to this formula

00:03:27.630 --> 00:03:27.640
whereas this according to this formula
 

00:03:27.640 --> 00:03:31.740
whereas this according to this formula
up on top so if you wish to use learning

00:03:31.740 --> 00:03:31.750
up on top so if you wish to use learning
 

00:03:31.750 --> 00:03:34.680
up on top so if you wish to use learning
rate decay what you can do is try to

00:03:34.680 --> 00:03:34.690
rate decay what you can do is try to
 

00:03:34.690 --> 00:03:36.840
rate decay what you can do is try to
provide your values of both hyper

00:03:36.840 --> 00:03:36.850
provide your values of both hyper
 

00:03:36.850 --> 00:03:39.930
provide your values of both hyper
parameter alpha 0 as well as of this

00:03:39.930 --> 00:03:39.940
parameter alpha 0 as well as of this
 

00:03:39.940 --> 00:03:42.270
parameter alpha 0 as well as of this
decay rate hyper parameter and then try

00:03:42.270 --> 00:03:42.280
decay rate hyper parameter and then try
 

00:03:42.280 --> 00:03:44.670
decay rate hyper parameter and then try
to find a value that works well other

00:03:44.670 --> 00:03:44.680
to find a value that works well other
 

00:03:44.680 --> 00:03:46.350
to find a value that works well other
than this formula for learning rate

00:03:46.350 --> 00:03:46.360
than this formula for learning rate
 

00:03:46.360 --> 00:03:47.910
than this formula for learning rate
decay there are a few other ways that

00:03:47.910 --> 00:03:47.920
decay there are a few other ways that
 

00:03:47.920 --> 00:03:50.790
decay there are a few other ways that
people use for example this is called

00:03:50.790 --> 00:03:50.800
people use for example this is called
 

00:03:50.800 --> 00:03:53.370
people use for example this is called
exponential decay where alpha is equal

00:03:53.370 --> 00:03:53.380
exponential decay where alpha is equal
 

00:03:53.380 --> 00:03:59.430
exponential decay where alpha is equal
to some number less than 1 such as 0.9 5

00:03:59.430 --> 00:03:59.440
to some number less than 1 such as 0.9 5
 

00:03:59.440 --> 00:04:04.920
to some number less than 1 such as 0.9 5
times epoch num times alpha 0 so this

00:04:04.920 --> 00:04:04.930
times epoch num times alpha 0 so this
 

00:04:04.930 --> 00:04:09.120
times epoch num times alpha 0 so this
will exponentially quickly decay your

00:04:09.120 --> 00:04:09.130
will exponentially quickly decay your
 

00:04:09.130 --> 00:04:11.460
will exponentially quickly decay your
learning rate other formulas that people

00:04:11.460 --> 00:04:11.470
learning rate other formulas that people
 

00:04:11.470 --> 00:04:14.520
learning rate other formulas that people
use are things like alpha equals some

00:04:14.520 --> 00:04:14.530
use are things like alpha equals some
 

00:04:14.530 --> 00:04:17.080
use are things like alpha equals some
constant over

00:04:17.080 --> 00:04:17.090
constant over
 

00:04:17.090 --> 00:04:21.380
constant over
EPOC numb square root times alpha zero

00:04:21.380 --> 00:04:21.390
EPOC numb square root times alpha zero
 

00:04:21.390 --> 00:04:25.430
EPOC numb square root times alpha zero
or some constants cave another hyper

00:04:25.430 --> 00:04:25.440
or some constants cave another hyper
 

00:04:25.440 --> 00:04:30.110
or some constants cave another hyper
counter over dr.mini Bosch number P

00:04:30.110 --> 00:04:30.120
counter over dr.mini Bosch number P
 

00:04:30.120 --> 00:04:32.660
counter over dr.mini Bosch number P
square root 2 times alpha zero and

00:04:32.660 --> 00:04:32.670
square root 2 times alpha zero and
 

00:04:32.670 --> 00:04:35.440
square root 2 times alpha zero and
sometimes you also see people use a

00:04:35.440 --> 00:04:35.450
sometimes you also see people use a
 

00:04:35.450 --> 00:04:37.880
sometimes you also see people use a
learning rate that decreases and

00:04:37.880 --> 00:04:37.890
learning rate that decreases and
 

00:04:37.890 --> 00:04:40.910
learning rate that decreases and
discrete stats where for some number of

00:04:40.910 --> 00:04:40.920
discrete stats where for some number of
 

00:04:40.920 --> 00:04:43.040
discrete stats where for some number of
steps you have some learning rate and

00:04:43.040 --> 00:04:43.050
steps you have some learning rate and
 

00:04:43.050 --> 00:04:45.140
steps you have some learning rate and
then after a while you decrease it by

00:04:45.140 --> 00:04:45.150
then after a while you decrease it by
 

00:04:45.150 --> 00:04:47.330
then after a while you decrease it by
one half after a while by one half after

00:04:47.330 --> 00:04:47.340
one half after a while by one half after
 

00:04:47.340 --> 00:04:49.540
one half after a while by one half after
a while by one half and so this is a

00:04:49.540 --> 00:04:49.550
a while by one half and so this is a
 

00:04:49.550 --> 00:04:56.780
a while by one half and so this is a
discrete staircase so so far we've

00:04:56.780 --> 00:04:56.790
discrete staircase so so far we've
 

00:04:56.790 --> 00:04:59.390
discrete staircase so so far we've
talked about some using some you know

00:04:59.390 --> 00:04:59.400
talked about some using some you know
 

00:04:59.400 --> 00:05:03.200
talked about some using some you know
formula to govern how alpha the learning

00:05:03.200 --> 00:05:03.210
formula to govern how alpha the learning
 

00:05:03.210 --> 00:05:05.750
formula to govern how alpha the learning
rate changes over time one other thing

00:05:05.750 --> 00:05:05.760
rate changes over time one other thing
 

00:05:05.760 --> 00:05:08.120
rate changes over time one other thing
that people sometimes do is nanyo decay

00:05:08.120 --> 00:05:08.130
that people sometimes do is nanyo decay
 

00:05:08.130 --> 00:05:10.700
that people sometimes do is nanyo decay
and so if you're training just one model

00:05:10.700 --> 00:05:10.710
and so if you're training just one model
 

00:05:10.710 --> 00:05:13.340
and so if you're training just one model
at a time and the dual model takes many

00:05:13.340 --> 00:05:13.350
at a time and the dual model takes many
 

00:05:13.350 --> 00:05:16.070
at a time and the dual model takes many
hours or even many days to Train what

00:05:16.070 --> 00:05:16.080
hours or even many days to Train what
 

00:05:16.080 --> 00:05:17.570
hours or even many days to Train what
some people will do is just wash your

00:05:17.570 --> 00:05:17.580
some people will do is just wash your
 

00:05:17.580 --> 00:05:20.330
some people will do is just wash your
model as this training over your a large

00:05:20.330 --> 00:05:20.340
model as this training over your a large
 

00:05:20.340 --> 00:05:22.850
model as this training over your a large
number of days and then annually say oh

00:05:22.850 --> 00:05:22.860
number of days and then annually say oh
 

00:05:22.860 --> 00:05:24.650
number of days and then annually say oh
it looks like the learning rate slowed

00:05:24.650 --> 00:05:24.660
it looks like the learning rate slowed
 

00:05:24.660 --> 00:05:26.180
it looks like the learning rate slowed
down I'm going to decrease out for a

00:05:26.180 --> 00:05:26.190
down I'm going to decrease out for a
 

00:05:26.190 --> 00:05:28.100
down I'm going to decrease out for a
little bit of course this works this

00:05:28.100 --> 00:05:28.110
little bit of course this works this
 

00:05:28.110 --> 00:05:30.530
little bit of course this works this
manually controlling alpha really tuning

00:05:30.530 --> 00:05:30.540
manually controlling alpha really tuning
 

00:05:30.540 --> 00:05:32.900
manually controlling alpha really tuning
alpha by hand all by hour day by day

00:05:32.900 --> 00:05:32.910
alpha by hand all by hour day by day
 

00:05:32.910 --> 00:05:35.210
alpha by hand all by hour day by day
this works only if you're training only

00:05:35.210 --> 00:05:35.220
this works only if you're training only
 

00:05:35.220 --> 00:05:37.460
this works only if you're training only
a small number of models but sometimes

00:05:37.460 --> 00:05:37.470
a small number of models but sometimes
 

00:05:37.470 --> 00:05:39.560
a small number of models but sometimes
people do that as well so now you have a

00:05:39.560 --> 00:05:39.570
people do that as well so now you have a
 

00:05:39.570 --> 00:05:41.540
people do that as well so now you have a
few more options so how to control the

00:05:41.540 --> 00:05:41.550
few more options so how to control the
 

00:05:41.550 --> 00:05:44.240
few more options so how to control the
learning rate alpha now in case you're

00:05:44.240 --> 00:05:44.250
learning rate alpha now in case you're
 

00:05:44.250 --> 00:05:45.890
learning rate alpha now in case you're
thinking wow this is a lot of hyper

00:05:45.890 --> 00:05:45.900
thinking wow this is a lot of hyper
 

00:05:45.900 --> 00:05:47.630
thinking wow this is a lot of hyper
parameters how that select amongst all

00:05:47.630 --> 00:05:47.640
parameters how that select amongst all
 

00:05:47.640 --> 00:05:49.580
parameters how that select amongst all
these different options I would say

00:05:49.580 --> 00:05:49.590
these different options I would say
 

00:05:49.590 --> 00:05:51.410
these different options I would say
don't worry about it for now in next

00:05:51.410 --> 00:05:51.420
don't worry about it for now in next
 

00:05:51.420 --> 00:05:52.910
don't worry about it for now in next
week we'll talk more about how to

00:05:52.910 --> 00:05:52.920
week we'll talk more about how to
 

00:05:52.920 --> 00:05:55.600
week we'll talk more about how to
systematically choose hyper parameters

00:05:55.600 --> 00:05:55.610
systematically choose hyper parameters
 

00:05:55.610 --> 00:05:59.120
systematically choose hyper parameters
for me I would say that learning rate is

00:05:59.120 --> 00:05:59.130
for me I would say that learning rate is
 

00:05:59.130 --> 00:06:00.950
for me I would say that learning rate is
usually lower down or the list of things

00:06:00.950 --> 00:06:00.960
usually lower down or the list of things
 

00:06:00.960 --> 00:06:03.530
usually lower down or the list of things
I try setting alpha just a fixed value

00:06:03.530 --> 00:06:03.540
I try setting alpha just a fixed value
 

00:06:03.540 --> 00:06:05.480
I try setting alpha just a fixed value
of alpha and getting that to be wealthy

00:06:05.480 --> 00:06:05.490
of alpha and getting that to be wealthy
 

00:06:05.490 --> 00:06:07.370
of alpha and getting that to be wealthy
and has a huge in time learning rate

00:06:07.370 --> 00:06:07.380
and has a huge in time learning rate
 

00:06:07.380 --> 00:06:09.560
and has a huge in time learning rate
decay does help sometimes it can really

00:06:09.560 --> 00:06:09.570
decay does help sometimes it can really
 

00:06:09.570 --> 00:06:11.180
decay does help sometimes it can really
help speed up training but it is a

00:06:11.180 --> 00:06:11.190
help speed up training but it is a
 

00:06:11.190 --> 00:06:13.910
help speed up training but it is a
little bit lower down my list when in

00:06:13.910 --> 00:06:13.920
little bit lower down my list when in
 

00:06:13.920 --> 00:06:16.070
little bit lower down my list when in
terms of the things I would try but next

00:06:16.070 --> 00:06:16.080
terms of the things I would try but next
 

00:06:16.080 --> 00:06:17.480
terms of the things I would try but next
we want to talk about hyper parameter

00:06:17.480 --> 00:06:17.490
we want to talk about hyper parameter
 

00:06:17.490 --> 00:06:19.790
we want to talk about hyper parameter
tuning you see more systematic ways to

00:06:19.790 --> 00:06:19.800
tuning you see more systematic ways to
 

00:06:19.800 --> 00:06:21.620
tuning you see more systematic ways to
organize all of these hyper parameters

00:06:21.620 --> 00:06:21.630
organize all of these hyper parameters
 

00:06:21.630 --> 00:06:23.540
organize all of these hyper parameters
and how to efficiently search amongst

00:06:23.540 --> 00:06:23.550
and how to efficiently search amongst
 

00:06:23.550 --> 00:06:26.300
and how to efficiently search amongst
them so that's it for learning rate is

00:06:26.300 --> 00:06:26.310
them so that's it for learning rate is
 

00:06:26.310 --> 00:06:29.000
them so that's it for learning rate is
hey um finally I want to also want to

00:06:29.000 --> 00:06:29.010
hey um finally I want to also want to
 

00:06:29.010 --> 00:06:30.640
hey um finally I want to also want to
talk a little bit about local

00:06:30.640 --> 00:06:30.650
talk a little bit about local
 

00:06:30.650 --> 00:06:32.439
talk a little bit about local
optimal and saddle points in new

00:06:32.439 --> 00:06:32.449
optimal and saddle points in new
 

00:06:32.449 --> 00:06:34.150
optimal and saddle points in new
networks so you can have a little bit

00:06:34.150 --> 00:06:34.160
networks so you can have a little bit
 

00:06:34.160 --> 00:06:36.010
networks so you can have a little bit
better intuition about the types of

00:06:36.010 --> 00:06:36.020
better intuition about the types of
 

00:06:36.020 --> 00:06:38.379
better intuition about the types of
optimization problems your optimization

00:06:38.379 --> 00:06:38.389
optimization problems your optimization
 

00:06:38.389 --> 00:06:40.150
optimization problems your optimization
algorithm is trying to solve when you're

00:06:40.150 --> 00:06:40.160
algorithm is trying to solve when you're
 

00:06:40.160 --> 00:06:41.320
algorithm is trying to solve when you're
trying to train these in your network

00:06:41.320 --> 00:06:41.330
trying to train these in your network
 

00:06:41.330 --> 00:06:45.520
trying to train these in your network
let's go onto the next video to see that

