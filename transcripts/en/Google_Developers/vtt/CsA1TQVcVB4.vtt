WEBVTT
Kind: captions
Language: en

00:00:01.446 --> 00:00:04.820
[APPLAUSE]

00:00:04.820 --> 00:00:07.720
JEREMIAH HARMSEN: Thank you.

00:00:07.720 --> 00:00:09.370
All right, good
morning, everyone.

00:00:09.370 --> 00:00:12.950
And hello to everyone
watching on the stream.

00:00:12.950 --> 00:00:14.860
My name is Jeremiah Harmsen.

00:00:14.860 --> 00:00:17.410
I am an engineer here in Zurich.

00:00:17.410 --> 00:00:20.890
I lead our applied
machine intelligence team.

00:00:20.890 --> 00:00:24.640
Our general mission is to
advance the impact of machine

00:00:24.640 --> 00:00:27.880
learning here in
Zurich, across Google,

00:00:27.880 --> 00:00:30.980
and throughout the world.

00:00:30.980 --> 00:00:32.470
So why are we here in Zurich?

00:00:32.470 --> 00:00:34.330
I want to take a
few minutes to talk

00:00:34.330 --> 00:00:39.370
about why Zurich is
the hub of AI research

00:00:39.370 --> 00:00:40.550
for Google in Europe.

00:00:45.910 --> 00:00:48.770
So the Zurich office is actually
one of our oldest offices

00:00:48.770 --> 00:00:50.282
in Europe.

00:00:50.282 --> 00:00:52.115
It's one of our largest
engineering offices.

00:00:52.115 --> 00:00:54.300
It's the largest one
outside the States.

00:00:54.300 --> 00:00:56.090
And similarly, our
research center

00:00:56.090 --> 00:00:58.640
here is the largest
one outside the States.

00:00:58.640 --> 00:01:00.920
And those two really
play off each other.

00:01:00.920 --> 00:01:03.530
So the product teams, they have
all these really interesting

00:01:03.530 --> 00:01:04.879
challenges to solve.

00:01:04.879 --> 00:01:07.190
And the research teams
work hand-in-hand with them

00:01:07.190 --> 00:01:08.360
to solve them.

00:01:08.360 --> 00:01:10.171
So as a researcher,
this is fantastic.

00:01:10.171 --> 00:01:12.170
We get to build all these
things and immediately

00:01:12.170 --> 00:01:15.250
impact billions of lives,
whether it's through Chrome,

00:01:15.250 --> 00:01:18.630
or Gmail, or YouTube.

00:01:18.630 --> 00:01:20.650
So this is a really
natural place to do that.

00:01:20.650 --> 00:01:23.490
We've got a lot of great
universities with ETH ad EPFL.

00:01:23.490 --> 00:01:27.090
We have a number of programs
where we support and work

00:01:27.090 --> 00:01:29.500
with researchers there.

00:01:29.500 --> 00:01:32.020
We also have a growing
fundamental research group

00:01:32.020 --> 00:01:32.520
here.

00:01:32.520 --> 00:01:34.860
We have a Brain team that
is growing very quickly.

00:01:34.860 --> 00:01:39.000
They look at things like
reinforcement learning,

00:01:39.000 --> 00:01:42.270
artificial neural networks,
and general optimization

00:01:42.270 --> 00:01:45.649
and some adversarial networks.

00:01:45.649 --> 00:01:47.940
So really quickly, I want to
highlight some of the work

00:01:47.940 --> 00:01:50.590
that teams across
Google do in research.

00:01:50.590 --> 00:01:53.400
The first is the
handwriting team.

00:01:53.400 --> 00:01:55.291
So this team takes
a bunch of squiggles

00:01:55.291 --> 00:01:56.790
across a bunch of
different services

00:01:56.790 --> 00:01:59.910
and turns them into words,
whether it's on a watch

00:01:59.910 --> 00:02:02.847
or on your phone.

00:02:02.847 --> 00:02:05.180
This team uses technology to
build a really neat web app

00:02:05.180 --> 00:02:07.000
called Quick, Draw!

00:02:07.000 --> 00:02:08.440
Has anyone used Quick, Draw!

00:02:08.440 --> 00:02:09.480
before?

00:02:09.480 --> 00:02:10.759
All right, a few folks.

00:02:10.759 --> 00:02:12.020
I think I found the Googlers.

00:02:12.020 --> 00:02:14.050
[LAUGHTER]

00:02:14.050 --> 00:02:15.516
I highly recommend
trying this out.

00:02:15.516 --> 00:02:16.390
This is a lot of fun.

00:02:16.390 --> 00:02:17.900
This is where you get to draw.

00:02:17.900 --> 00:02:20.110
And the machine tries to
guess what you're drawing,

00:02:20.110 --> 00:02:22.570
whether it's a helicopter
or an ice cream cone.

00:02:22.570 --> 00:02:23.980
So this has been a lot of fun.

00:02:23.980 --> 00:02:28.330
But it's also been a really
interesting source of doodles.

00:02:28.330 --> 00:02:30.130
We've gotten over
a billion doodles

00:02:30.130 --> 00:02:32.120
from 100 different countries.

00:02:32.120 --> 00:02:34.061
And we've released this
as a dataset, which

00:02:34.061 --> 00:02:36.060
is really interesting for
people, to researchers

00:02:36.060 --> 00:02:39.360
at Google, and outside
Google to comb through.

00:02:39.360 --> 00:02:43.931
So for instance, here is OK--

00:02:43.931 --> 00:02:45.430
so for instance,
here's what a chair

00:02:45.430 --> 00:02:48.882
looks like as drawn by
different countries.

00:02:48.882 --> 00:02:50.590
And it's interesting
because you can see,

00:02:50.590 --> 00:02:53.350
some countries in general draw
it a little bit differently.

00:02:53.350 --> 00:02:55.450
So this might come
up a little bit later

00:02:55.450 --> 00:02:58.060
as we talk about biases in data.

00:02:58.060 --> 00:02:59.950
For instance, we can
see some countries

00:02:59.950 --> 00:03:03.390
may prefer a little bit of an
ISO view or things like that.

00:03:03.390 --> 00:03:06.280
And so what happens if we leave
them out of our training data?

00:03:06.280 --> 00:03:09.040
And what happens when we try to
classify a chair that's maybe

00:03:09.040 --> 00:03:10.556
drawn from one of
these places where

00:03:10.556 --> 00:03:11.680
we don't have as much data?

00:03:17.150 --> 00:03:20.480
Another really great product
is from the sense team here.

00:03:20.480 --> 00:03:22.190
This team is really
at the forefront

00:03:22.190 --> 00:03:25.920
of putting machine
learning on device.

00:03:25.920 --> 00:03:27.850
This one in particular
is called Now Playing.

00:03:27.850 --> 00:03:30.320
This gives you an
ambient superpower

00:03:30.320 --> 00:03:33.020
where you always know
what song is playing.

00:03:33.020 --> 00:03:35.600
This runs in the background.

00:03:35.600 --> 00:03:38.040
And this is a really
challenging problem.

00:03:38.040 --> 00:03:38.600
Right?

00:03:38.600 --> 00:03:40.740
Because we don't want to
suck everyone's battery down.

00:03:40.740 --> 00:03:42.531
It also has some really
interesting privacy

00:03:42.531 --> 00:03:43.160
implications.

00:03:43.160 --> 00:03:45.290
This model runs
completely on device.

00:03:45.290 --> 00:03:47.740
It doesn't send
any data to Google.

00:03:47.740 --> 00:03:50.180
So I think this is a really
interesting direction we're

00:03:50.180 --> 00:03:51.304
going to see a lot more of.

00:03:53.700 --> 00:03:56.780
So from the same team is
one of my favorite uses

00:03:56.780 --> 00:03:57.900
of machine learning.

00:03:57.900 --> 00:04:01.692
So this is Smart Text Selection.

00:04:01.692 --> 00:04:04.150
I'm sure everybody has tried
to select text on their phone.

00:04:04.150 --> 00:04:05.650
You have this
frustrating experience

00:04:05.650 --> 00:04:09.277
of trying to drop the two little
blue pins with your thumbs.

00:04:09.277 --> 00:04:10.360
I have kind of big thumbs.

00:04:10.360 --> 00:04:13.139
It's especially awkward for me.

00:04:13.139 --> 00:04:14.680
This is actually a
machining learning

00:04:14.680 --> 00:04:16.399
model that runs on the device.

00:04:16.399 --> 00:04:18.709
So when you touch
a piece of text,

00:04:18.709 --> 00:04:21.110
it does a search to figure
out what you're touching.

00:04:21.110 --> 00:04:23.260
If you touch a phone
number, it automatically

00:04:23.260 --> 00:04:24.430
selects it for you.

00:04:24.430 --> 00:04:26.210
If you touch an address,
it automatically

00:04:26.210 --> 00:04:28.370
selects the address for you.

00:04:28.370 --> 00:04:30.870
And again, this is a
model that runs on device.

00:04:30.870 --> 00:04:32.650
It doesn't need to
send data to Google.

00:04:32.650 --> 00:04:34.650
And I think this
is a great example

00:04:34.650 --> 00:04:37.840
of improving one of those
little frustrations for users.

00:04:42.390 --> 00:04:44.260
The last thing I'll
mention is that we

00:04:44.260 --> 00:04:46.847
work on the tools
that are open source.

00:04:46.847 --> 00:04:49.180
I really wanted to put a slide
of what we're working on.

00:04:49.180 --> 00:04:51.120
Unfortunately, it
hasn't been released.

00:04:51.120 --> 00:04:53.680
So I encourage everyone to
watch the TensorFlow developer

00:04:53.680 --> 00:04:56.270
summit a week from Friday.

00:04:56.270 --> 00:04:58.420
The team from
Zurich will be there

00:04:58.420 --> 00:05:04.460
and announcing what we've been
working on for the last year.

00:05:04.460 --> 00:05:07.480
All right, that was
a very quick tour.

00:05:07.480 --> 00:05:10.619
I am excited that everyone
is here in Zurich.

00:05:10.619 --> 00:05:12.160
And I'm excited to
hear the speakers.

00:05:12.160 --> 00:05:16.500
So with that, I'd like to
move on to Fernanda and Jess.

00:05:16.500 --> 00:05:19.500
[APPLAUSE]

00:05:20.875 --> 00:05:22.000
FERNANDA VIÉGAS: Thank you.

00:05:22.000 --> 00:05:23.035
Thanks.

00:05:23.035 --> 00:05:24.510
Hi, everyone.

00:05:24.510 --> 00:05:25.850
I'm so excited.

00:05:25.850 --> 00:05:29.170
This is the first
PAIR UX symposium.

00:05:29.170 --> 00:05:32.200
As Matt and Lucia
were saying before,

00:05:32.200 --> 00:05:35.390
we had the first
symposium last year.

00:05:35.390 --> 00:05:37.190
And it was a general symposium.

00:05:37.190 --> 00:05:40.840
This is the first focused
symposium around UX.

00:05:40.840 --> 00:05:43.370
And I think that matters
a lot because if we

00:05:43.370 --> 00:05:46.900
want to think
seriously about AI,

00:05:46.900 --> 00:05:48.490
and how it affects
people's lives,

00:05:48.490 --> 00:05:50.950
and how users
interact with AI, we

00:05:50.950 --> 00:05:54.100
need to be thinking from a
user experience perspective.

00:05:54.100 --> 00:05:57.920
And so that's why today
is really important.

00:05:57.920 --> 00:05:59.929
So I'm Fernanda Viégas.

00:05:59.929 --> 00:06:01.345
JESS HOLBROOK:
Does this one work?

00:06:01.345 --> 00:06:01.845
Oh, hey.

00:06:01.845 --> 00:06:04.960
Hey, I'm Jess Holbrook.

00:06:04.960 --> 00:06:08.110
FERNANDA VIÉGAS: And we're going
to talk a little bit about PAIR

00:06:08.110 --> 00:06:11.950
So Jess and I and
Martin Wattenberg,

00:06:11.950 --> 00:06:16.810
who couldn't be here today,
launched PAIR last year,

00:06:16.810 --> 00:06:18.320
middle of last year.

00:06:18.320 --> 00:06:21.130
And this is really
the first time

00:06:21.130 --> 00:06:25.180
there was an initiative
at Google for People + AI

00:06:25.180 --> 00:06:28.570
research, and again,
trying to bring

00:06:28.570 --> 00:06:30.640
these different
perspectives to the table,

00:06:30.640 --> 00:06:33.820
so design thinking,
HCI, how do we

00:06:33.820 --> 00:06:37.480
build ML systems
that are designed

00:06:37.480 --> 00:06:41.680
with user needs in mind
from the beginning,

00:06:41.680 --> 00:06:45.110
from the very objective
function that you design.

00:06:45.110 --> 00:06:47.170
How do you design an
objective function that

00:06:47.170 --> 00:06:50.500
takes into account user needs?

00:06:50.500 --> 00:06:53.740
So when we talk
about People + AI

00:06:53.740 --> 00:06:55.159
research, who are the people?

00:06:55.159 --> 00:06:56.450
That's a question we get a lot.

00:06:56.450 --> 00:06:58.750
So who are the people
you're talking about?

00:06:58.750 --> 00:07:00.640
We're talking about
a range of people.

00:07:00.640 --> 00:07:04.420
We're talking about
hardcore machine

00:07:04.420 --> 00:07:06.820
learning researchers
and engineers.

00:07:06.820 --> 00:07:11.470
What are the tools that they
need to develop better machine

00:07:11.470 --> 00:07:13.570
learning systems?

00:07:13.570 --> 00:07:15.490
If you've been
programming for a while,

00:07:15.490 --> 00:07:19.330
you probably know that
traditional development tools

00:07:19.330 --> 00:07:22.400
are not exactly designed
for machine learning.

00:07:22.400 --> 00:07:24.940
So we're going to talk
a little bit about this.

00:07:24.940 --> 00:07:28.180
So what kind of tool do we need?

00:07:28.180 --> 00:07:31.210
And then a different kind of
user is the user in the middle

00:07:31.210 --> 00:07:34.750
here, where we talk about
augmented expert intelligence.

00:07:34.750 --> 00:07:37.300
So imagine professionals
who are going

00:07:37.300 --> 00:07:39.790
to be interacting with
machine-learning-enabled

00:07:39.790 --> 00:07:43.330
systems at a professional
level, at an expert level.

00:07:43.330 --> 00:07:46.111
But they are not themselves
machine learning experts.

00:07:46.111 --> 00:07:46.610
Right?

00:07:46.610 --> 00:07:49.640
So you can think about
doctors, designers, architects,

00:07:49.640 --> 00:07:50.740
musicians.

00:07:50.740 --> 00:07:54.700
How do you actually partner
with an intelligent system

00:07:54.700 --> 00:07:56.830
in a meaningful, productive way?

00:07:56.830 --> 00:08:01.120
So these are some of the
things we're studying in PAIR.

00:08:01.120 --> 00:08:04.330
And then finally, all the way
to the right here, this notion

00:08:04.330 --> 00:08:06.400
of AI for everyone.

00:08:06.400 --> 00:08:12.640
So even if you are a lay
user, you are potentially-- so

00:08:12.640 --> 00:08:13.820
imagine a situation.

00:08:13.820 --> 00:08:15.580
You were a YouTube viewer.

00:08:15.580 --> 00:08:19.270
And you were being recommended
the next video for you

00:08:19.270 --> 00:08:20.380
to watch.

00:08:20.380 --> 00:08:22.540
Maybe there's
something where you

00:08:22.540 --> 00:08:26.650
want an explanation about why
you were recommended a video.

00:08:26.650 --> 00:08:31.210
Those kinds of questions
about explainability,

00:08:31.210 --> 00:08:35.500
trust, how do we surface some
of what the machine learning

00:08:35.500 --> 00:08:38.650
systems are doing in
the backhand for users

00:08:38.650 --> 00:08:42.020
are also research questions
we're looking into.

00:08:42.020 --> 00:08:44.290
There's a whole host
of research questions

00:08:44.290 --> 00:08:46.280
that I think
permeate all of this

00:08:46.280 --> 00:08:50.540
that have to do with fairness
also that we're looking into.

00:08:50.540 --> 00:08:52.810
So when we talk
about People + AI,

00:08:52.810 --> 00:08:58.040
it's a whole range of people
and users we have in mind.

00:08:58.040 --> 00:09:00.400
And so finally,
what does PAIR do?

00:09:00.400 --> 00:09:01.850
We've been very busy.

00:09:01.850 --> 00:09:04.600
We do a lot of different
kinds of things.

00:09:04.600 --> 00:09:07.390
So one of the things
I'm very excited about

00:09:07.390 --> 00:09:10.000
is the first column
here on the left,

00:09:10.000 --> 00:09:13.810
where we get to open
source tools and platforms.

00:09:13.810 --> 00:09:17.230
And so this is the place where
we take a lot of the things

00:09:17.230 --> 00:09:19.960
that we're actually
doing at Google, tools

00:09:19.960 --> 00:09:24.050
that we're using internally at
Google that are very helpful,

00:09:24.050 --> 00:09:25.450
and we externalize them.

00:09:25.450 --> 00:09:26.440
We open source them.

00:09:26.440 --> 00:09:28.520
And that makes me very happy.

00:09:28.520 --> 00:09:32.480
And so I'm very quickly
going to talk about that.

00:09:32.480 --> 00:09:35.300
We also create
educational materials.

00:09:35.300 --> 00:09:37.360
And these are for the lay user.

00:09:37.360 --> 00:09:39.870
These are not for experts.

00:09:39.870 --> 00:09:43.560
We publish and we have
academic publications.

00:09:43.560 --> 00:09:46.750
We are sharing best practices.

00:09:46.750 --> 00:09:48.190
We run symposia.

00:09:48.190 --> 00:09:49.930
We have visiting faculty.

00:09:49.930 --> 00:09:52.330
We give out faculty grants.

00:09:52.330 --> 00:09:55.870
So there's a whole host of
ways in which we interact

00:09:55.870 --> 00:09:59.730
with the broader community
around this notion of human-AI

00:09:59.730 --> 00:10:02.270
interaction.

00:10:02.270 --> 00:10:04.160
And then, as I was
saying, one of the things

00:10:04.160 --> 00:10:06.140
that gives me great
joy is that we

00:10:06.140 --> 00:10:09.490
get to open source
tools like Facets.

00:10:09.490 --> 00:10:12.410
So Facets is a
visualization that

00:10:12.410 --> 00:10:15.140
allows you to look at
your training data.

00:10:15.140 --> 00:10:21.500
Again, one of those gaps in
machine learning tooling--

00:10:21.500 --> 00:10:23.420
your data is super important.

00:10:23.420 --> 00:10:24.530
How do you look at it?

00:10:24.530 --> 00:10:27.320
How do you debug your data?

00:10:27.320 --> 00:10:30.110
And so tools like
Facets allow you

00:10:30.110 --> 00:10:34.040
to do that in a much
more effective way.

00:10:34.040 --> 00:10:37.880
And it's been really great
to see how Facets gets used

00:10:37.880 --> 00:10:40.020
by people outside of Google.

00:10:40.020 --> 00:10:46.010
So for instance, this woman, Joy
Buolamwini from the MIT Media

00:10:46.010 --> 00:10:54.050
Lab, used Facets in her
research to look into fairness

00:10:54.050 --> 00:10:56.840
and how different
facial recognition

00:10:56.840 --> 00:11:01.130
APIs deal with different
races, for instance,

00:11:01.130 --> 00:11:03.350
and different genders.

00:11:03.350 --> 00:11:07.070
And so, again, it's great to
see how these tools make it

00:11:07.070 --> 00:11:11.520
out there, and how they're being
used for very serious research

00:11:11.520 --> 00:11:14.170
work.

00:11:14.170 --> 00:11:21.850
Another focus of research
for us is just fairness.

00:11:21.850 --> 00:11:26.170
And we've done a
number of things here.

00:11:26.170 --> 00:11:29.980
So released tools, so this
is Facets that you saw.

00:11:29.980 --> 00:11:31.750
The one that's
animating here is called

00:11:31.750 --> 00:11:35.230
embedding projector, which
allows you to visualize

00:11:35.230 --> 00:11:38.020
massively high-dimensional
spaces, which is super

00:11:38.020 --> 00:11:40.630
useful in machine learning.

00:11:40.630 --> 00:11:43.300
We've also worked
with Creative Labs

00:11:43.300 --> 00:11:50.530
to create an introductory video
on what is machine bias, what

00:11:50.530 --> 00:11:54.070
is bias in machine learning.

00:11:54.070 --> 00:11:56.200
There are obviously
publications.

00:11:56.200 --> 00:11:58.567
This one here, I'm
very excited about.

00:11:58.567 --> 00:12:01.150
So that everybody knows what I'm
talking about, this one here.

00:12:01.150 --> 00:12:02.790
Can you see my cursor?

00:12:02.790 --> 00:12:03.700
OK, great.

00:12:03.700 --> 00:12:07.190
So this one here was
kind of different.

00:12:07.190 --> 00:12:13.510
We actually translated a
highly mathematical paper

00:12:13.510 --> 00:12:16.540
around definitions of
machine learning fairness

00:12:16.540 --> 00:12:20.020
into a visualization that
people could play with,

00:12:20.020 --> 00:12:23.650
a simulation around
a bank giving out

00:12:23.650 --> 00:12:26.800
loans for different
kinds of populations.

00:12:26.800 --> 00:12:27.790
OK?

00:12:27.790 --> 00:12:29.875
And one of the things that
was really interesting

00:12:29.875 --> 00:12:33.040
was to talk about the fact that
there are different definitions

00:12:33.040 --> 00:12:36.910
of fairness, and you need to
choose a definition of fairness

00:12:36.910 --> 00:12:38.980
to implement in your system.

00:12:38.980 --> 00:12:41.560
Unfortunately,
mathematically speaking,

00:12:41.560 --> 00:12:44.740
you can't put all the
definitions of fairness

00:12:44.740 --> 00:12:46.370
together in one system.

00:12:46.370 --> 00:12:49.120
So you're always going
to have to choose,

00:12:49.120 --> 00:12:51.310
how are you going to be fair.

00:12:51.310 --> 00:12:52.000
Right?

00:12:52.000 --> 00:12:54.910
And so the reason
why I call this out

00:12:54.910 --> 00:12:58.760
is that it was a
simple visualization,

00:12:58.760 --> 00:13:02.650
a simple simulation,
and it went viral.

00:13:02.650 --> 00:13:05.170
And part of the
reason was that it

00:13:05.170 --> 00:13:09.940
created this much more
accessible way of turning

00:13:09.940 --> 00:13:12.670
these mathematical equations
into something that people

00:13:12.670 --> 00:13:15.820
could play with, and could
understand, and could

00:13:15.820 --> 00:13:17.470
start to see the tradeoffs.

00:13:17.470 --> 00:13:21.130
And so we got a lot
of emails from places

00:13:21.130 --> 00:13:24.820
like criminal justice
systems in the US asking us,

00:13:24.820 --> 00:13:26.410
oh, could I create
a visualization

00:13:26.410 --> 00:13:30.770
like this to try to understand
fairness within my own context.

00:13:30.770 --> 00:13:34.270
And so again, I think
it speaks to the need

00:13:34.270 --> 00:13:37.810
of having ways of
broadening the message

00:13:37.810 --> 00:13:44.050
and involving other stakeholders
into this bigger debate of what

00:13:44.050 --> 00:13:47.170
it means to have these
machine learning systems

00:13:47.170 --> 00:13:50.960
and how they interact
with society as a whole.

00:13:50.960 --> 00:13:53.500
The other thing I want
to point out very quickly

00:13:53.500 --> 00:13:58.860
is we also launched
deeplearn.js,

00:13:58.860 --> 00:14:04.440
which is this really exciting
machine learning open source

00:14:04.440 --> 00:14:07.680
library that brings
ML to the web.

00:14:07.680 --> 00:14:10.170
And that matters why?

00:14:10.170 --> 00:14:12.750
Because it means it
lowers the barriers.

00:14:12.750 --> 00:14:14.250
You don't have to
download anything.

00:14:14.250 --> 00:14:15.708
You don't have to
install anything.

00:14:15.708 --> 00:14:18.210
It's JavaScript-based,
so it means

00:14:18.210 --> 00:14:22.530
it opens up machine learning to
a whole new host of developers.

00:14:22.530 --> 00:14:23.980
It runs on your browser.

00:14:23.980 --> 00:14:25.320
It runs on your phone.

00:14:25.320 --> 00:14:27.717
It runs locally.

00:14:27.717 --> 00:14:29.550
You're not sending
information anywhere else

00:14:29.550 --> 00:14:30.870
to any one service.

00:14:30.870 --> 00:14:34.890
So it's been great to see how
people have been using it.

00:14:34.890 --> 00:14:37.972
So Alex Chen, who is going
to speak later today,

00:14:37.972 --> 00:14:39.930
is going to talk about
a teachable machine that

00:14:39.930 --> 00:14:43.770
used deeplearn.js.

00:14:43.770 --> 00:14:45.970
Another group in
Brain, Magenta, used

00:14:45.970 --> 00:14:49.170
it to create music
online to let you partner

00:14:49.170 --> 00:14:51.240
with this
machine-learning-enabled

00:14:51.240 --> 00:14:56.580
jamming session called
Performance RNN.

00:14:56.580 --> 00:15:00.720
We have people here
in the house who--

00:15:00.720 --> 00:15:03.377
so we have Dan Shiffman.

00:15:03.377 --> 00:15:03.960
Where are you?

00:15:03.960 --> 00:15:04.240
Dan?

00:15:04.240 --> 00:15:04.920
Dan?

00:15:04.920 --> 00:15:07.170
OK, go talk to Dan afterwards.

00:15:07.170 --> 00:15:13.440
So Dan is a faculty
member at NYU

00:15:13.440 --> 00:15:23.430
and is creating this entire
GitHub repository called ml5js,

00:15:23.430 --> 00:15:27.570
which is basically the friendly
machine learning for the web

00:15:27.570 --> 00:15:28.470
that you see here.

00:15:28.470 --> 00:15:31.470
This is Dan's work and
his students' work.

00:15:31.470 --> 00:15:37.770
And so the idea, again, is to
use something like deeplearn.js

00:15:37.770 --> 00:15:43.220
to broaden the reach of machine
learning, just in the sense

00:15:43.220 --> 00:15:46.620
to bring folks who may
not have been able to work

00:15:46.620 --> 00:15:50.700
with these systems, outside
of places like Google

00:15:50.700 --> 00:15:53.460
or other academic
institutions that are

00:15:53.460 --> 00:15:55.980
very computer-sciencecentric.

00:15:55.980 --> 00:15:57.120
But how do you broaden it?

00:15:57.120 --> 00:16:00.480
How do you bring it to
a much bigger audience?

00:16:00.480 --> 00:16:04.140
So Dan Shiffman is
doing this work.

00:16:04.140 --> 00:16:10.200
There is also a whole undergrad
class going on right now at MIT

00:16:10.200 --> 00:16:14.550
that uses deeplearn.js
so that students can just

00:16:14.550 --> 00:16:18.380
start building as soon as
possible with machine learning.

00:16:18.380 --> 00:16:19.690
And I'm going to stop here.

00:16:19.690 --> 00:16:20.981
JESS HOLBROOK: Yeah, thank you.

00:16:20.981 --> 00:16:23.394
I'm going to expand on what
Fernanda was saying there.

00:16:23.394 --> 00:16:24.810
One of the things
that really gets

00:16:24.810 --> 00:16:27.226
me excited about PAIR and about
the field in general right

00:16:27.226 --> 00:16:31.620
now is this idea of
expanding the conversation,

00:16:31.620 --> 00:16:35.460
and expanding who's included in
the work that we're doing here,

00:16:35.460 --> 00:16:37.500
and seeing how this is
starting to manifest.

00:16:37.500 --> 00:16:40.500
So one of the stories is--

00:16:40.500 --> 00:16:44.070
I think it was two
or three weeks ago,

00:16:44.070 --> 00:16:46.960
this guy named Oz started
tweeting about deep learning,

00:16:46.960 --> 00:16:51.840
about how he was learning
deeplearn.js to--

00:16:51.840 --> 00:16:53.610
the problem that
he wanted to solve

00:16:53.610 --> 00:16:57.480
was that he wanted to help his
friend who had had a stroke

00:16:57.480 --> 00:17:00.750
and is paralyzed be able
to navigate the web.

00:17:00.750 --> 00:17:02.520
And so how he was
doing it is he was

00:17:02.520 --> 00:17:06.150
trying to build a Chrome
extension using deeplearn.js

00:17:06.150 --> 00:17:10.140
to allow his friend to
navigate the web with his eyes.

00:17:10.140 --> 00:17:11.960
There's a link here.

00:17:11.960 --> 00:17:15.180
Let's click it because it's
pretty cool if you see it.

00:17:15.180 --> 00:17:18.700
And we started forwarding
this around on our team.

00:17:18.700 --> 00:17:22.050
Alex also reached out to Oz.

00:17:22.050 --> 00:17:24.630
I think one of the more
interesting parts of Oz's story

00:17:24.630 --> 00:17:27.119
is Oz is homeless.

00:17:27.119 --> 00:17:30.150
So he's a homeless
veteran in Portland

00:17:30.150 --> 00:17:33.660
who sleeps in a homeless
shelter at night.

00:17:33.660 --> 00:17:36.390
And he goes to the library,
and he codes on deeplearn

00:17:36.390 --> 00:17:40.530
during the day because
he wants to allow people

00:17:40.530 --> 00:17:45.120
with physical disabilities like
his friend access to the web.

00:17:45.120 --> 00:17:48.120
Last week, we called Oz,
kind of talked to him

00:17:48.120 --> 00:17:49.930
about what he wants to get done.

00:17:49.930 --> 00:17:52.370
We're looking at ways
we can partner with him.

00:17:52.370 --> 00:17:54.300
I've actually kind
of randomly been

00:17:54.300 --> 00:17:57.450
thinking about Oz these
last couple of days.

00:17:57.450 --> 00:18:00.480
It's not lost on me that
I'm traveling halfway

00:18:00.480 --> 00:18:03.600
around the world for a
conference in Zurich,

00:18:03.600 --> 00:18:05.030
in the room that we're in now.

00:18:05.030 --> 00:18:09.030
And Oz, it's nighttime on
the West Coast right now.

00:18:09.030 --> 00:18:11.880
So Oz is sleeping in a
homeless shelter right now.

00:18:11.880 --> 00:18:14.310
But he's going to wake
up, and contribute,

00:18:14.310 --> 00:18:16.824
and try to make the world
a better place in the way

00:18:16.824 --> 00:18:18.990
that he wants to, with some
of the things that we're

00:18:18.990 --> 00:18:19.823
building right here.

00:18:19.823 --> 00:18:21.690
And this is really,
really inspiring to me.

00:18:21.690 --> 00:18:22.290
Right?

00:18:22.290 --> 00:18:25.080
I think we're getting at
this inclusion and people

00:18:25.080 --> 00:18:27.540
being able to solve their
own problems with AI

00:18:27.540 --> 00:18:29.350
from many different angles.

00:18:29.350 --> 00:18:32.530
So this is just pretty
amazing stuff to see.

00:18:32.530 --> 00:18:34.860
And I'd recommend everyone
follow him because he's

00:18:34.860 --> 00:18:36.870
a guy on a mission.

00:18:36.870 --> 00:18:41.359
And so I think there's going to
be a lot more to come from Oz.

00:18:41.359 --> 00:18:42.900
Another thing that
we've been working

00:18:42.900 --> 00:18:45.480
on for the last 18
months or so is something

00:18:45.480 --> 00:18:47.500
that we call AIY Projects.

00:18:47.500 --> 00:18:51.510
So this is something that
I was on until December.

00:18:51.510 --> 00:18:55.370
And this is really about
bringing AI to the maker

00:18:55.370 --> 00:18:58.640
and probably going to extend
into the STEM communities.

00:18:58.640 --> 00:19:00.880
So if you haven't
seen these before,

00:19:00.880 --> 00:19:03.130
remember the little
cardboard VR goggles

00:19:03.130 --> 00:19:04.360
we did a couple of years ago?

00:19:04.360 --> 00:19:05.760
It's kind of like
that but for AI

00:19:05.760 --> 00:19:07.390
in a bunch of different ways.

00:19:07.390 --> 00:19:10.120
These little kits, if you
already have a Raspberry Pi,

00:19:10.120 --> 00:19:11.140
are pretty cheap.

00:19:11.140 --> 00:19:12.370
They're like $20.

00:19:12.370 --> 00:19:14.400
I think the vision
kit is like $45.

00:19:14.400 --> 00:19:16.820
And so we have two-- one
that really focuses on audio,

00:19:16.820 --> 00:19:19.910
so audio input and output,
and one that is a camera.

00:19:19.910 --> 00:19:21.644
So it focuses on recognition.

00:19:21.644 --> 00:19:23.560
One of the really cool
things about the camera

00:19:23.560 --> 00:19:25.310
is it does it all on device.

00:19:25.310 --> 00:19:27.087
So all the models run on device.

00:19:27.087 --> 00:19:29.170
This is something that's
come up a couple of times

00:19:29.170 --> 00:19:32.397
so I wanted to just put a
little extra emphasis on it,

00:19:32.397 --> 00:19:34.480
especially for the people
that aren't from Google,

00:19:34.480 --> 00:19:35.730
is that we're investing a lot.

00:19:35.730 --> 00:19:38.200
And you'll hear a lot about
everything running on device,

00:19:38.200 --> 00:19:40.090
not sending anything
to the cloud,

00:19:40.090 --> 00:19:42.040
not making that a
requirement for people

00:19:42.040 --> 00:19:44.830
to get the benefits of AI.

00:19:44.830 --> 00:19:45.770
I don't have it here.

00:19:45.770 --> 00:19:48.340
But one of the tweets from
our original launch that

00:19:48.340 --> 00:19:52.390
really inspired me was this dad
posted a tweet with his kid.

00:19:52.390 --> 00:19:56.260
And he said, "Just did our first
electronics project together."

00:19:56.260 --> 00:19:57.700
And they built our kit.

00:19:57.700 --> 00:20:01.825
And what blew my mind is that
kid's orientation towards what

00:20:01.825 --> 00:20:05.740
an electronics project is
is that it has AI in it.

00:20:05.740 --> 00:20:08.200
The first thing he
put together, he

00:20:08.200 --> 00:20:10.510
was building, and
controlling, and using AI

00:20:10.510 --> 00:20:12.670
to do what he wanted it to do.

00:20:12.670 --> 00:20:15.670
That's a mind-blowing
generational shift

00:20:15.670 --> 00:20:18.570
that I hope that we can actually
put a lot more pressure on.

00:20:18.570 --> 00:20:21.880
And if people don't feel an
intimidation, or an otherness,

00:20:21.880 --> 00:20:23.860
or like, oh, I'm worried
about this thing,

00:20:23.860 --> 00:20:29.690
but helping them control it from
nine years old and on, I think,

00:20:29.690 --> 00:20:32.370
is a big shift in the
mindset that we can make.

00:20:35.480 --> 00:20:38.440
We also reach very fancy people.

00:20:38.440 --> 00:20:42.820
So this is Martin testifying
before the House of Commons

00:20:42.820 --> 00:20:45.220
on the Science and
Technology Committee,

00:20:45.220 --> 00:20:48.120
where he broke out
his best suit and tie.

00:20:48.120 --> 00:20:50.020
And so actually
talking to people

00:20:50.020 --> 00:20:51.750
about algorithmic
decision-making

00:20:51.750 --> 00:20:54.790
and helping to try to inform a
lot of these very big and very

00:20:54.790 --> 00:20:58.570
important policy debates
through people that

00:20:58.570 --> 00:21:00.590
are on the ground trying
to make this work so

00:21:00.590 --> 00:21:05.690
we have the right
policies around the world.

00:21:05.690 --> 00:21:09.100
And then we're also doing a
ton to try to reach as much

00:21:09.100 --> 00:21:11.080
of the UX community as we can.

00:21:11.080 --> 00:21:13.840
So just a little while
ago, through a lot

00:21:13.840 --> 00:21:16.840
of effort of Josh
Lovejoy and others,

00:21:16.840 --> 00:21:19.180
we launched a Google
Design collection really

00:21:19.180 --> 00:21:22.650
focused on HAIR and what
we call the UX of AI.

00:21:22.650 --> 00:21:25.540
So we're trying to get as many
of our resources up and out

00:21:25.540 --> 00:21:26.380
here.

00:21:26.380 --> 00:21:28.780
We don't really talk
about unreleased stuff

00:21:28.780 --> 00:21:30.280
or stuff we do internally.

00:21:30.280 --> 00:21:33.460
But this came out of a
lot of internal efforts

00:21:33.460 --> 00:21:36.790
that we'd done around trying
to educate and mobilize

00:21:36.790 --> 00:21:39.940
the UX community at
Google to really own

00:21:39.940 --> 00:21:44.500
what is the UX of AI, and what
does this next wave of user

00:21:44.500 --> 00:21:46.045
experience we want to create.

00:21:46.045 --> 00:21:48.100
And so we're trying
as rapidly as we

00:21:48.100 --> 00:21:49.960
can to get as much
of this external

00:21:49.960 --> 00:21:52.720
as well because it's a thing
that's a lot bigger than Google

00:21:52.720 --> 00:21:57.980
and a lot bigger than a small
group at Google, for sure.

00:21:57.980 --> 00:22:00.910
And so with that, I've been
thinking about this, like how

00:22:00.910 --> 00:22:04.030
to put a wrapper on what--

00:22:04.030 --> 00:22:05.980
Fernanda and I get the
question a lot of times

00:22:05.980 --> 00:22:07.270
like, what is PAIR doing?

00:22:07.270 --> 00:22:08.710
Or why does PAIR exist?

00:22:08.710 --> 00:22:11.740
Or what is our purpose here?

00:22:11.740 --> 00:22:14.030
And one of the ways
I put it sometimes

00:22:14.030 --> 00:22:16.480
is, I think we're
making human-centered

00:22:16.480 --> 00:22:18.510
AI the easiest option.

00:22:18.510 --> 00:22:20.789
And I think that there's
two big parts there.

00:22:20.789 --> 00:22:22.330
I think one of the
big parts is we're

00:22:22.330 --> 00:22:24.910
trying to define what
human-centered AI is.

00:22:24.910 --> 00:22:27.454
We're trying to define that
through a big conversation

00:22:27.454 --> 00:22:29.620
with a lot of people, and
a lot of different groups,

00:22:29.620 --> 00:22:32.359
and companies, and
technologists, and academics,

00:22:32.359 --> 00:22:33.400
and everything like that.

00:22:33.400 --> 00:22:35.080
And so we're really
trying to define

00:22:35.080 --> 00:22:38.830
what we think that means
with all these improvements.

00:22:38.830 --> 00:22:41.590
And then as a user researcher,
I'm a big believer in people

00:22:41.590 --> 00:22:43.750
do the easiest
thing, not the thing

00:22:43.750 --> 00:22:45.190
they're most motivated to do.

00:22:45.190 --> 00:22:48.250
So we're looking
for ways of, how

00:22:48.250 --> 00:22:49.990
do we make that
human-centered approach

00:22:49.990 --> 00:22:51.670
to AI the easiest option.

00:22:51.670 --> 00:22:54.610
How do we put tools, and
resources, and trainings,

00:22:54.610 --> 00:22:57.340
and options, and everything
in people's hands

00:22:57.340 --> 00:22:58.891
so that if they are
on board with us

00:22:58.891 --> 00:23:00.640
and they want to do
this human-centered AI

00:23:00.640 --> 00:23:03.830
thing that it's the easiest
option for them to do it.

00:23:03.830 --> 00:23:06.640
And with that, I think we're
going to really push that

00:23:06.640 --> 00:23:08.560
into the conversation
and into the world

00:23:08.560 --> 00:23:13.750
through products that we build.

00:23:13.750 --> 00:23:16.150
And with that, I think
it's time to move on

00:23:16.150 --> 00:23:20.020
to the pretty great
speaker lineup we have.

00:23:20.020 --> 00:23:23.490
This is our hashtag,
if you care to tweet.

00:23:23.490 --> 00:23:24.470
I think that's it.

00:23:24.470 --> 00:23:25.670
All right, let's get going.

00:23:25.670 --> 00:23:28.720
[APPLAUSE]

