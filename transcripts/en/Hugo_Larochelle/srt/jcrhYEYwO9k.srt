1
00:00:00,510 --> 00:00:01,051
And does,
we know,

2
00:00:01,051 --> 00:00:05,790
we'll look at the usual preprocessing
steps that we go through to take some text

3
00:00:05,791 --> 00:00:09,420
data in its raw form and
put it in a different form,

4
00:00:09,421 --> 00:00:12,690
which is more amenable for
a neural network processing.

5
00:00:15,720 --> 00:00:19,020
So, um, what is texts really?

6
00:00:19,021 --> 00:00:23,880
What is the raw form for
a actual text? Uh, well,

7
00:00:23,881 --> 00:00:24,600
really it's,

8
00:00:24,600 --> 00:00:29,600
you should think of text in its raw form
as just a long string that contains all

9
00:00:30,211 --> 00:00:35,010
of the characters that, uh, that are
contained in a contained, in the text.

10
00:00:35,280 --> 00:00:39,510
So we have an example here where we have
just a single sentence represented as a

11
00:00:39,511 --> 00:00:43,330
long string, uh, but it
could be a whole webpage. Uh,

12
00:00:43,340 --> 00:00:47,610
so the textual content of a webpage
where the whole string would be, uh,

13
00:00:47,640 --> 00:00:52,320
all the sentences, uh, in order
that they appear in, in the,

14
00:00:52,410 --> 00:00:54,060
uh,
in the webpage and question.

15
00:00:54,900 --> 00:00:59,900
So that's the raw format for our text
data and that we'll see a bunch of steps,

16
00:01:01,720 --> 00:01:05,640
uh, preprocessing steps that we
usually go through to get, uh,

17
00:01:05,730 --> 00:01:10,730
to represent this text in a format that's
going to be easier to manipulate when

18
00:01:11,881 --> 00:01:16,080
we want to feed textual
data. To a neuro network. Um,

19
00:01:16,200 --> 00:01:20,790
so the first thing that we usually
do is that we will tokenize the text.

20
00:01:20,820 --> 00:01:25,700
That is, we will identify essentially
the words or tokens that are, uh,

21
00:01:25,920 --> 00:01:30,840
present in, uh, in the, in
the text itself. So, um,

22
00:01:30,900 --> 00:01:35,070
so for instance, for this sentence here,
he spending seven days in San Francisco,

23
00:01:35,220 --> 00:01:39,310
then we could split it, uh, like this. Um,

24
00:01:39,360 --> 00:01:41,610
so the first token or word would be heat.

25
00:01:41,640 --> 00:01:46,620
The second one would be apostrophe
s then spending seven and so on.

26
00:01:46,621 --> 00:01:51,180
So we'd convert that into a list of
shorter strings where each train is a word

27
00:01:51,570 --> 00:01:55,110
and, um, uh, instead of
word, we often say a token.

28
00:01:56,490 --> 00:02:00,780
So for me, the data sets, if you download
the data, set a NLP Dataset from the web,

29
00:02:00,840 --> 00:02:05,010
this step as often been
done for you already, uh,

30
00:02:05,040 --> 00:02:08,970
but if you were to apply this on where
you would extract by yourself some,

31
00:02:08,971 --> 00:02:10,980
some text data from
the web point instance,

32
00:02:10,981 --> 00:02:15,330
you would have to go through that
step, uh, for English and for French,

33
00:02:15,360 --> 00:02:19,310
usually just splitting words
or tokens based on, uh,

34
00:02:19,410 --> 00:02:24,410
by essentially spinning at spaces
and also separating punctuations.

35
00:02:24,990 --> 00:02:25,621
So,
for instance,

36
00:02:25,621 --> 00:02:30,621
we notice here that the a.here a is
actually a separate it as its own token.

37
00:02:32,851 --> 00:02:37,100
And if there had been a Camo so it
would be considered its own token. Uh,

38
00:02:37,110 --> 00:02:41,880
so if we just use these two simple
rules separate at spaces and make

39
00:02:41,881 --> 00:02:45,390
punctuations their own token,
then for English and French,

40
00:02:45,420 --> 00:02:48,570
that usually works fine. And
we've been able like this to, uh,

41
00:02:48,580 --> 00:02:50,280
essentially separate
everything into words.

42
00:02:50,490 --> 00:02:53,850
It's not as much true for other language
before English and French that works

43
00:02:53,851 --> 00:02:56,970
well. Uh, and there are some other
languages where that works well.

44
00:02:57,480 --> 00:02:59,120
But we have an example
here where our points,

45
00:02:59,260 --> 00:03:04,030
maybe what we would want actually is
consider San Francisco as a single word,

46
00:03:04,031 --> 00:03:05,940
a single token and a,

47
00:03:05,950 --> 00:03:08,860
if you wanted to do some more
fancier preprocessing like this,

48
00:03:08,861 --> 00:03:11,410
you need to define some rules.
Uh,

49
00:03:11,470 --> 00:03:15,090
where are you would identify
that a space here for instance,

50
00:03:15,091 --> 00:03:19,180
is not something that should
separate that token into two. Okay.

51
00:03:19,181 --> 00:03:21,130
But just keep in mind and in general,

52
00:03:21,131 --> 00:03:24,960
just separating by species
is often sufficient. Uh,

53
00:03:24,970 --> 00:03:26,500
in particular for English and French.

54
00:03:30,020 --> 00:03:30,801
Um,
next,

55
00:03:30,801 --> 00:03:35,510
we often limitizing the tokens that it
will put them in some sort of standard

56
00:03:35,511 --> 00:03:37,460
form where we remove some,
uh,

57
00:03:37,461 --> 00:03:42,200
usually syntactic information that is
contained in the words. So for instance,

58
00:03:42,201 --> 00:03:45,650
we could take the word he and
just remove the capital letters,

59
00:03:45,710 --> 00:03:46,780
so uncapitalized it,

60
00:03:47,550 --> 00:03:52,520
or we could also take some verbs and
actually put them in a more standard form.

61
00:03:52,930 --> 00:03:55,040
Uh,
we could take numbers,

62
00:03:55,041 --> 00:03:58,100
it just replace them by
some arbitrary token.

63
00:03:58,480 --> 00:04:01,730
Here I chosen the token number,
uh,

64
00:04:01,790 --> 00:04:05,030
just to say that this was a
number and a pass this step,

65
00:04:05,031 --> 00:04:08,300
you just don't know which number it was.
Um,

66
00:04:08,420 --> 00:04:11,960
we could remove plurals or
days converted today and so on.

67
00:04:12,820 --> 00:04:16,700
So the specific kind of limits ization
that you might want to do for a

68
00:04:16,701 --> 00:04:18,860
particular problem, we'll, we'll vary.

69
00:04:19,100 --> 00:04:23,030
Essentially what you want to do
is remove variations of words, uh,

70
00:04:23,031 --> 00:04:25,820
that are not relevant for
the task you want to solve.

71
00:04:25,821 --> 00:04:28,460
So if knowing that this is seven,

72
00:04:28,490 --> 00:04:31,170
this is just an arbitrary
number is important,

73
00:04:31,220 --> 00:04:36,200
then you'll want to keep seven
here. Uh, in the same way, if, uh,

74
00:04:36,220 --> 00:04:39,140
knowing that over, uh,
uh, now noun for instance,

75
00:04:39,141 --> 00:04:43,370
was singular or plural is important,
then don't remove the plural.

76
00:04:43,371 --> 00:04:47,660
Just keep it in its original form,
in the original text.

77
00:04:48,230 --> 00:04:49,940
Okay.
So that's the monetization.

78
00:04:53,070 --> 00:04:55,980
And then what we'll do is that,
uh,

79
00:04:55,981 --> 00:04:58,470
will not convert ev all of these tokens,

80
00:04:58,471 --> 00:05:01,650
which are strings into an integer id.

81
00:05:02,220 --> 00:05:06,780
And the way we'll do that is that
will first come up with a vocabulary,

82
00:05:07,180 --> 00:05:11,610
uh, which is a list of words that we're
willing to recognize, advanced English.

83
00:05:12,270 --> 00:05:14,520
And, uh, so this vocabulary,

84
00:05:14,670 --> 00:05:18,960
we can think of it as a big long
lists of words. And then, uh,

85
00:05:18,961 --> 00:05:22,920
once we've made up this list
and we can associate, uh,

86
00:05:22,980 --> 00:05:27,200
each word in that list or each
token, uh, to a unique ids offense,

87
00:05:27,210 --> 00:05:31,620
the first one could be id one, the
second one, I need two, three, and so on.

88
00:05:32,850 --> 00:05:33,450
Um,

89
00:05:33,450 --> 00:05:36,990
so how did we come up with that list of
words that are different criteria that

90
00:05:36,991 --> 00:05:38,220
we can use?
Uh,

91
00:05:38,410 --> 00:05:43,200
we can just pick the most frequent words
in the data set that we've collected.

92
00:05:43,620 --> 00:05:47,460
Uh, so we take all the words to
have a frequency of at least five,

93
00:05:47,461 --> 00:05:51,300
or if we want to limit
the number of words,

94
00:05:51,301 --> 00:05:54,240
if we know the number of words
we willing to, to distinguish,

95
00:05:54,330 --> 00:05:59,090
we could say we'll take the
100,000 most frequent words.

96
00:05:59,091 --> 00:06:03,550
So that's a very general, which is
often used sometimes. Also we, uh,

97
00:06:03,800 --> 00:06:08,000
decided to ignore some quote
unquote uninformative words, uh,

98
00:06:08,020 --> 00:06:10,490
which would define using a short list.
For instance,

99
00:06:10,491 --> 00:06:13,480
we could remove articles
like the or, uh, uh,

100
00:06:13,600 --> 00:06:18,410
there might be other words which are not
useful to recognize in the text for the

101
00:06:18,411 --> 00:06:20,360
problem that we want to solve.

102
00:06:21,080 --> 00:06:26,080
And at Dnh then for all the
words or tokens that are not in,

103
00:06:26,941 --> 00:06:30,090
not in our vocabulary that we've defined,
uh,

104
00:06:30,280 --> 00:06:35,280
then all of these other words will be
mapped to a special out of vocabulary id.

105
00:06:35,660 --> 00:06:40,490
So at the very end of our list, we can
think of it as there being an extra word,

106
00:06:40,510 --> 00:06:44,370
Oval v four out of vocabulary.
And then this one will take a,

107
00:06:44,470 --> 00:06:49,130
the last id in our list of,
so if I had a say here,

108
00:06:49,131 --> 00:06:53,720
I had nine words, then this
would take id, 10 points things.

109
00:06:54,780 --> 00:06:55,080
Yeah.

110
00:06:55,080 --> 00:06:56,910
So again,
depending on the problem,

111
00:06:56,940 --> 00:07:01,080
the type of vocabulary we want to define
the rules that we want to choose for

112
00:07:01,081 --> 00:07:05,910
selecting which words we
want to distinguish and
associated that are unique id

113
00:07:05,911 --> 00:07:08,970
to a is going to vary.
Um,

114
00:07:09,030 --> 00:07:13,440
and in practice the vocabulary size
can actually, uh, uh, be quite big.

115
00:07:13,560 --> 00:07:18,560
It can be as little as around 10,000
or as high as ton and a 50,000 or even

116
00:07:20,791 --> 00:07:21,624
more.

117
00:07:24,310 --> 00:07:24,460
Yeah.

118
00:07:24,460 --> 00:07:27,340
So here's an example of the
last preprocessing step.

119
00:07:27,460 --> 00:07:30,310
So we have this list of tokens
that have been limitizing.

120
00:07:30,580 --> 00:07:34,810
And then imagine I've defined
this vocabulary here where
the is associated with

121
00:07:34,811 --> 00:07:38,290
the ID one and two,
two and so on until Oh v,

122
00:07:38,291 --> 00:07:40,360
which is associated with the ID five.

123
00:07:40,600 --> 00:07:45,010
Then I take that and I replace it by one,
the integer one,

124
00:07:45,011 --> 00:07:49,950
because the has the id one
and cat in the list, uh,

125
00:07:49,990 --> 00:07:53,590
is not present. So we'll map
it to the idea of [inaudible].

126
00:07:53,650 --> 00:07:56,280
So that's why we have five here than,

127
00:07:56,460 --> 00:08:00,730
and is going to take the ID to uh,
the one again.

128
00:08:00,790 --> 00:08:03,550
And then dog, she's a,
which has the ID three,

129
00:08:03,551 --> 00:08:05,530
it's going to be associated
with three and so on.

130
00:08:06,710 --> 00:08:07,150
Yeah.

131
00:08:07,150 --> 00:08:11,950
So, um, um, for the rest of the videos,

132
00:08:12,030 --> 00:08:12,590
uh,

133
00:08:12,590 --> 00:08:17,590
w whatever we're going to refer to some
word ideas often I will use this symbol

134
00:08:18,670 --> 00:08:20,200
w um,

135
00:08:20,410 --> 00:08:25,410
so we can think of w as a cartographer
categorical feature that describes the

136
00:08:26,861 --> 00:08:27,671
original word.

137
00:08:27,671 --> 00:08:32,170
It's essentially a feature
description of the original words.

138
00:08:32,171 --> 00:08:33,130
So fun since the,

139
00:08:33,190 --> 00:08:37,600
we can think of one as being a feature
and it's a categorical feature because it

140
00:08:37,601 --> 00:08:42,601
corresponds to which category in many
categories where each category would be a

141
00:08:45,131 --> 00:08:46,690
different word in the vocabulary.

142
00:08:46,691 --> 00:08:51,450
So to which category does
the correspond to and uh,

143
00:08:51,730 --> 00:08:55,140
we could imagine adding
other features and that's,

144
00:08:55,620 --> 00:08:59,390
we'll probably talk about
in the following videos. Um,

145
00:08:59,670 --> 00:09:02,940
but at this point now we
have data which is taking a,

146
00:09:03,070 --> 00:09:07,830
a format which is more numerical,
so that's going to be easier to deal with.

147
00:09:08,580 --> 00:09:13,410
And, uh, still sometimes when I'm
going to talk about and refer to WWE,

148
00:09:13,411 --> 00:09:17,760
I might say the word w, even though
really it corresponds to an id,

149
00:09:17,940 --> 00:09:19,770
but because it's just at this point,

150
00:09:20,130 --> 00:09:23,520
these categories correspond Duker
respond to individual words,

151
00:09:23,880 --> 00:09:28,110
except for o v which correspond to any
other word that's not in the vocabulary.

152
00:09:28,320 --> 00:09:29,460
So, um, you know,

153
00:09:29,880 --> 00:09:34,710
talking about WWE as a word is
not such a stretch really. So,

154
00:09:34,770 --> 00:09:36,550
um,
so that's a 40 preprocessing,

155
00:09:36,560 --> 00:09:39,390
and that's the kind of preprocessing
we'll assume that has been done for the

156
00:09:39,391 --> 00:09:43,950
data, uh, for everything else that
follows in the next [inaudible].

