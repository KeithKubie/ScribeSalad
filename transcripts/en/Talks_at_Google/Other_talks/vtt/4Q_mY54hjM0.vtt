WEBVTT
Kind: captions
Language: en

00:00:04.230 --> 00:00:07.180
PETER NORVIG: Welcome
everybody.

00:00:07.180 --> 00:00:09.550
It's our pleasure to have
with us at Google

00:00:09.550 --> 00:00:11.120
today, Daniel Dennett.

00:00:11.120 --> 00:00:12.990
He has a long and distinguished
career.

00:00:12.990 --> 00:00:15.380
I won't try to summarize
all of it.

00:00:15.380 --> 00:00:16.140
You can Google it.

00:00:16.140 --> 00:00:17.571
[LAUGHTER]

00:00:17.571 --> 00:00:20.110
PETER NORVIG: Instead, I'll
just try to introduce very

00:00:20.110 --> 00:00:24.300
briefly the book that he's
written most recently and is

00:00:24.300 --> 00:00:27.580
talking about today, which
is a collection or a

00:00:27.580 --> 00:00:29.570
tool kit for thinking.

00:00:29.570 --> 00:00:33.260
And I think the reason I think
it's such a great book is

00:00:33.260 --> 00:00:35.860
because a lot of it
is our tool kit.

00:00:35.860 --> 00:00:41.150
It's these computational tools
for thinking that we take for

00:00:41.150 --> 00:00:45.860
granted, but Dan is trying to
introduce to the rest of the

00:00:45.860 --> 00:00:47.960
world who sometimes aren't
aware of how

00:00:47.960 --> 00:00:50.610
to think like that.

00:00:50.610 --> 00:00:51.900
And so what do I mean by that?

00:00:51.900 --> 00:00:58.100
Well, in his 1995 book on
evolution, he wrote that,

00:00:58.100 --> 00:01:00.550
"Life on Earth has been
generated over billions of

00:01:00.550 --> 00:01:04.040
years by an algorithmic
process." And that's kind of a

00:01:04.040 --> 00:01:06.970
natural way for us to think
about it, but it's hard way

00:01:06.970 --> 00:01:08.760
for other people
to think about.

00:01:08.760 --> 00:01:12.240
And some people don't quite
understand that by taking lots

00:01:12.240 --> 00:01:16.740
and lots of small steps that you
could arrange and come up

00:01:16.740 --> 00:01:18.940
with this result.

00:01:18.940 --> 00:01:23.330
And other philosophers, who we
won't name, have taken the

00:01:23.330 --> 00:01:25.590
approach, well look at these
incredible results.

00:01:25.590 --> 00:01:29.370
And if I simulate the steps
for three or four steps I

00:01:29.370 --> 00:01:29.980
don't get there.

00:01:29.980 --> 00:01:32.370
So it must be magic.

00:01:32.370 --> 00:01:36.130
Rather than saying, well, maybe
a billion or 100 billion

00:01:36.130 --> 00:01:39.450
is actually qualitatively
different than a couple steps.

00:01:39.450 --> 00:01:41.830
So Dan reminds us how
to think that way.

00:01:41.830 --> 00:01:46.180
He also reminds us that there
can be emergent properties

00:01:46.180 --> 00:01:51.920
that aren't implicit in any one
piece of an ensemble and

00:01:51.920 --> 00:01:53.320
yet they come out in the end.

00:01:53.320 --> 00:01:55.380
We're all familiar with that.

00:01:55.380 --> 00:01:57.360
Our programs have emergent
properties

00:01:57.360 --> 00:01:59.000
that we didn't expect.

00:01:59.000 --> 00:02:02.730
And we bang our heads
trying to fix them.

00:02:02.730 --> 00:02:03.960
So we're aware of it.

00:02:03.960 --> 00:02:05.230
But the rest of the
world isn't.

00:02:05.230 --> 00:02:08.449
So that's why they need
a book like this.

00:02:08.449 --> 00:02:13.240
I just want to talk about
one more of the tools.

00:02:13.240 --> 00:02:17.140
And this is the using lay
audience as a decoy.

00:02:17.140 --> 00:02:22.200
So this is a trick if you have
a philosophy professor who's

00:02:22.200 --> 00:02:25.470
not explaining all the details,
rather than ask them

00:02:25.470 --> 00:02:27.170
to explain a few you say, oh,
well why did you give a

00:02:27.170 --> 00:02:30.450
lecture to this introductory
class.

00:02:30.450 --> 00:02:33.510
And then it will become more
clear because the professor

00:02:33.510 --> 00:02:35.780
will have to work harder to get
it to that level rather

00:02:35.780 --> 00:02:40.360
than trying to keep it at the
professor to professor level.

00:02:40.360 --> 00:02:44.210
One of our Google employees Rob
Pike wrote a book where he

00:02:44.210 --> 00:02:46.752
talked about a similar
approach.

00:02:46.752 --> 00:02:49.920
Instead of calling it the lay
audience as decoy approach, he

00:02:49.920 --> 00:02:51.840
called it the teddy
bear approach.

00:02:51.840 --> 00:02:56.100
Which is, there was a
introductory class in computer

00:02:56.100 --> 00:02:59.370
programming for which the
TAs were overstocked.

00:02:59.370 --> 00:03:01.510
And there was always a long
waiting line of students

00:03:01.510 --> 00:03:03.790
waiting to get in to talk with
the TAs about why the program

00:03:03.790 --> 00:03:04.680
didn't work.

00:03:04.680 --> 00:03:09.580
So what they instituted was a
chair that held a teddy bear

00:03:09.580 --> 00:03:11.070
outside their office.

00:03:11.070 --> 00:03:13.530
Before you were allowed to talk
to the TAs you had to

00:03:13.530 --> 00:03:15.180
talk to the teddy bear.

00:03:15.180 --> 00:03:18.180
And so the students would come
up and say, oh teddy bear, my

00:03:18.180 --> 00:03:19.310
program doesn't work.

00:03:19.310 --> 00:03:22.560
And it must work because this
loop, see, this could never be

00:03:22.560 --> 00:03:23.510
more than one.

00:03:23.510 --> 00:03:26.660
Oh, except, oh, never mind.

00:03:26.660 --> 00:03:30.210
And it turns out the teddy bear
successfully solved about

00:03:30.210 --> 00:03:31.130
half the problems.

00:03:31.130 --> 00:03:33.030
And people didn't have
to go on to the TA.

00:03:33.030 --> 00:03:36.420
So I think that's sort of
in the same toolbox.

00:03:36.420 --> 00:03:40.390
And Dan will tell you about
the rest of the tools.

00:03:40.390 --> 00:03:40.820
DANIEL DENNETT: Thanks, Peter.

00:03:40.820 --> 00:03:41.990
That's a great introduction.

00:03:41.990 --> 00:03:44.220
I'm going to add
the teddy bear.

00:03:44.220 --> 00:03:46.352
It occurred to me as you were
telling us about that, that

00:03:46.352 --> 00:03:49.350
that probably explains the
success rate of psychoanalysts

00:03:49.350 --> 00:03:51.990
right there.

00:03:51.990 --> 00:03:53.560
You can replace them with
a teddy bear and

00:03:53.560 --> 00:03:54.480
get the same results.

00:03:54.480 --> 00:03:56.370
AUDIENCE: [INAUDIBLE].

00:03:56.370 --> 00:03:58.600
DANIEL DENNETT: Yeah, Teddy
bears are a lot cheaper.

00:04:01.710 --> 00:04:06.140
So I want to talk about better
tools for thinking.

00:04:06.140 --> 00:04:09.860
One of my best students, a Swede
name Bo Dahlbom once

00:04:09.860 --> 00:04:12.980
said, "You can't do much
carpentry with your bare

00:04:12.980 --> 00:04:15.050
hands, and you can't do much
thinking with your bare

00:04:15.050 --> 00:04:22.920
brain." He hits the nail on
the head right there.

00:04:22.920 --> 00:04:24.580
It's obvious, in a way, when
you think of it, even

00:04:24.580 --> 00:04:27.060
something as simple as
a word is a tool.

00:04:27.060 --> 00:04:29.330
It's a thinking tool.

00:04:29.330 --> 00:04:31.600
And these thinking tools have
been around for a long time.

00:04:31.600 --> 00:04:35.300
And recently we've learned
that maybe these thinking

00:04:35.300 --> 00:04:38.130
tools are having a remarkable
effect.

00:04:38.130 --> 00:04:40.440
How many of you know what
the Flynn effect is?

00:04:40.440 --> 00:04:41.510
Oh, a few of you do.

00:04:41.510 --> 00:04:43.040
And the rest of you, all right,
now let me tell you

00:04:43.040 --> 00:04:44.210
about the Flynn effect.

00:04:44.210 --> 00:04:47.860
Jim Flynn, it's named after him,
but he's not really the

00:04:47.860 --> 00:04:48.990
one that did the
first research.

00:04:48.990 --> 00:04:51.730
He's the one who drew people's
attention to it.

00:04:51.730 --> 00:04:59.450
It turns out that IQ
worldwide is up.

00:04:59.450 --> 00:05:02.010
And it's up a significant
amount.

00:05:02.010 --> 00:05:08.620
If you use today's test, if you
gave that test to a person

00:05:08.620 --> 00:05:13.070
who got 100 back in the '30s
they get probably an 80.

00:05:13.070 --> 00:05:16.270
And we'd be viewed as
almost retarded.

00:05:16.270 --> 00:05:18.320
I mean, there's a big jump.

00:05:18.320 --> 00:05:20.200
It's like 20 or 30 IQ points.

00:05:20.200 --> 00:05:24.760
Of course, IQ tests are always
scaled with 100 as average.

00:05:24.760 --> 00:05:29.490
But if you use the same test
items, at the very least we've

00:05:29.490 --> 00:05:33.480
gotten a lot better at
taking IQ tests.

00:05:33.480 --> 00:05:36.920
And it has nothing directly to
do with how much tutoring or

00:05:36.920 --> 00:05:39.140
schooling or your genes.

00:05:39.140 --> 00:05:42.520
It's a very large effect.

00:05:42.520 --> 00:05:46.290
And it can't be due to
improvements in our genes as

00:05:46.290 --> 00:05:49.900
we're talking about such
a short time periods.

00:05:49.900 --> 00:05:52.960
Jim Flynn thinks-- and
I hope he's right.

00:05:52.960 --> 00:05:57.030
That doesn't mean he's right,
but I think there's a lot to

00:05:57.030 --> 00:06:01.190
be said for it-- that what's
happened is that the thinking

00:06:01.190 --> 00:06:04.530
tools that are first developed
and tested and debugged by

00:06:04.530 --> 00:06:10.420
scientists and engineers and
other professional thinkers

00:06:10.420 --> 00:06:17.680
have been filtering down into
every day people's thinking.

00:06:17.680 --> 00:06:21.560
Even as simple an idea
as percentage.

00:06:21.560 --> 00:06:25.766
Which kids, you just pick
it up very fast.

00:06:25.766 --> 00:06:28.960
And 50 years ago, 100 years
ago, that was a pretty

00:06:28.960 --> 00:06:31.030
complicated concept for
a lot of people.

00:06:31.030 --> 00:06:32.950
They just weren't
ready for that.

00:06:32.950 --> 00:06:39.670
And so probably it's all these
thinking tools that we install

00:06:39.670 --> 00:06:42.820
without even realizing
we're doing it.

00:06:42.820 --> 00:06:45.780
We install in on our own brains
and in other brains.

00:06:45.780 --> 00:06:50.670
Probably that is a main source
of our improved thinking.

00:06:50.670 --> 00:06:54.970
And certainly that is the thesis
that I want to explore,

00:06:54.970 --> 00:06:55.510
in any case.

00:06:55.510 --> 00:07:00.100
Because what I'm doing in this
book, as a philosopher, I

00:07:00.100 --> 00:07:04.060
discovered that a lot of my
colleagues are remarkably

00:07:04.060 --> 00:07:07.080
unselfconscious about the
tools of the trade.

00:07:07.080 --> 00:07:10.320
Weird because philosophers are
famously self conscious and

00:07:10.320 --> 00:07:12.010
reflective and navel-gazing.

00:07:12.010 --> 00:07:15.230
But a lot of them don't
ever ask wait a

00:07:15.230 --> 00:07:16.300
minute, how do I do that.

00:07:16.300 --> 00:07:18.780
And why do I think that's a
good way of proceeding?

00:07:18.780 --> 00:07:20.140
What tools am I suing?

00:07:20.140 --> 00:07:22.670
And I wanted them to become
more self conscious.

00:07:22.670 --> 00:07:26.040
I want everybody to become more
self conscious about the

00:07:26.040 --> 00:07:27.520
tools that they use.

00:07:30.610 --> 00:07:32.360
So what kind of thinking
tools are there?

00:07:32.360 --> 00:07:33.290
There's words, as I say.

00:07:33.290 --> 00:07:34.330
And numbers, of course.

00:07:34.330 --> 00:07:36.880
And diagrams and maps
and methods.

00:07:36.880 --> 00:07:39.380
And intuition pumps.

00:07:39.380 --> 00:07:42.620
Which is a term I coined
more than 30

00:07:42.620 --> 00:07:45.610
years ago to describe--

00:07:45.610 --> 00:07:48.800
well in the first instance it
was John Searle's notorious

00:07:48.800 --> 00:07:50.390
Chinese room.

00:07:50.390 --> 00:07:54.440
And since I was beating up on
that intuition pump, a lot of

00:07:54.440 --> 00:07:57.570
people thought I thought that
intuition pumps were bad.

00:07:57.570 --> 00:07:58.530
No, I think they're wonderful.

00:07:58.530 --> 00:08:00.300
It's just got to
have good ones.

00:08:00.300 --> 00:08:02.500
There's defective pumps.

00:08:02.500 --> 00:08:03.320
And there's good ones.

00:08:03.320 --> 00:08:06.440
And the way you tell the
difference is, as you know, by

00:08:06.440 --> 00:08:08.950
reverse engineering.

00:08:08.950 --> 00:08:12.010
Doug Hofstadter came up with the
lovely phrase, you want to

00:08:12.010 --> 00:08:15.090
turn all the knobs on an
intuition pump to see what

00:08:15.090 --> 00:08:16.590
makes it do its work.

00:08:16.590 --> 00:08:18.500
You want to try out
the variation.

00:08:18.500 --> 00:08:20.270
Nothing mysterious about
why that works.

00:08:20.270 --> 00:08:22.540
Is just Mill's method
of differences.

00:08:22.540 --> 00:08:25.650
You're just empirically testing
to see what variations

00:08:25.650 --> 00:08:26.990
make what differences.

00:08:26.990 --> 00:08:31.350
See how robust the intuition
pump is under deformation.

00:08:31.350 --> 00:08:35.049
And sometimes they're remarkably
sensitive to tiny

00:08:35.049 --> 00:08:37.200
variations in the way
you tell a story.

00:08:37.200 --> 00:08:39.820
Then you know it's probably
a defective pump.

00:08:39.820 --> 00:08:42.159
Not doing what you
think it's doing.

00:08:42.159 --> 00:08:45.550
So what intuition pumps
are are persuaders.

00:08:45.550 --> 00:08:48.450
And it's important that
they're persuaders.

00:08:48.450 --> 00:08:55.720
So much good thinking is
accomplished in a friendly but

00:08:55.720 --> 00:09:00.940
competitive environment where
I am trying to persuade an

00:09:00.940 --> 00:09:05.430
interlocutor, a target, that P
and the target is trying to

00:09:05.430 --> 00:09:10.050
persuade me that not P. And
this opponent process is

00:09:10.050 --> 00:09:11.835
actually a good way,
very often, of

00:09:11.835 --> 00:09:14.490
getting at the truth.

00:09:14.490 --> 00:09:16.320
So they're also discoverers.

00:09:16.320 --> 00:09:18.200
They're attention-holders.

00:09:18.200 --> 00:09:20.970
And they're anchors for
a fixed points.

00:09:20.970 --> 00:09:27.330
In my work in philosophy there
are no fixed points.

00:09:27.330 --> 00:09:28.770
That's the point a philosophy.

00:09:28.770 --> 00:09:31.490
Philosophy is what you're doing
when you're not quite

00:09:31.490 --> 00:09:33.760
sure what the right questions
to ask are.

00:09:33.760 --> 00:09:37.680
And everything is up for
reconsideration always.

00:09:37.680 --> 00:09:41.250
But how do you proceed then?

00:09:41.250 --> 00:09:44.090
You say, well, maybe this as
a fixed point for this

00:09:44.090 --> 00:09:44.390
discussion.

00:09:44.390 --> 00:09:47.710
And then you sort of put
a pin down here.

00:09:47.710 --> 00:09:48.630
And here's one here.

00:09:48.630 --> 00:09:49.490
And here's one here.

00:09:49.490 --> 00:09:50.970
And then you say,
well, suppose we

00:09:50.970 --> 00:09:52.110
take those on board.

00:09:52.110 --> 00:09:55.410
What can we do with this idea
with a few of these candidate

00:09:55.410 --> 00:09:57.010
fixed points?

00:09:57.010 --> 00:09:59.415
One thing you may do with them
is throw all three of them out

00:09:59.415 --> 00:10:01.670
or throw out one against
the other two.

00:10:01.670 --> 00:10:08.020
It's in this intellectually
informal.

00:10:08.020 --> 00:10:11.020
There's no algorithms for doing
this, certainly not yet.

00:10:11.020 --> 00:10:16.370
This is an informal exploratory
process for which

00:10:16.370 --> 00:10:19.030
there are really no rules.

00:10:19.030 --> 00:10:23.670
But intuition pumps are
persuaders to use in those

00:10:23.670 --> 00:10:24.920
harrowing circumstances.

00:10:28.140 --> 00:10:30.620
Here's one that isn't in the
book, but I've talked about it

00:10:30.620 --> 00:10:33.160
a lot recently and I think it's
important to understand

00:10:33.160 --> 00:10:34.590
the context.

00:10:34.590 --> 00:10:35.860
The MacCready explosion.

00:10:35.860 --> 00:10:37.510
This is Paul MacCready.

00:10:37.510 --> 00:10:40.190
How many of you know
Paul MacCready?

00:10:40.190 --> 00:10:41.260
Oh well good.

00:10:41.260 --> 00:10:43.030
I can educate you about that.

00:10:43.030 --> 00:10:46.870
You will know him, not by name,
as the inventor of the

00:10:46.870 --> 00:10:51.670
"Gossamer Albatrosss."
A visionary engineer.

00:10:51.670 --> 00:10:53.340
The greenest of green
engineers.

00:10:53.340 --> 00:10:54.700
Wonderful guy.

00:10:54.700 --> 00:10:56.610
Died a few years ago.

00:10:56.610 --> 00:11:01.650
And in a Ted talk a few years
back he came up with the

00:11:01.650 --> 00:11:05.620
following startling
observation.

00:11:05.620 --> 00:11:10.780
Let's go back 10,000 years to
the dawn of agriculture.

00:11:10.780 --> 00:11:13.650
At that point, human beings
all around the world we're

00:11:13.650 --> 00:11:17.670
beginning to domesticate animals
and plants, of course.

00:11:17.670 --> 00:11:21.820
And at that point, our species,
Homo sapiens, and

00:11:21.820 --> 00:11:25.990
these are, of course, entirely
modern Homo sapiens.

00:11:25.990 --> 00:11:29.730
At that point 10,000 years ago,
if you put them on one

00:11:29.730 --> 00:11:32.760
side of the balance scale
together with their pets and

00:11:32.760 --> 00:11:37.340
livestock, all the domesticated
vertebrates on

00:11:37.340 --> 00:11:39.680
one side, including us.

00:11:39.680 --> 00:11:45.030
And you put all the rest of the
terrestrial vertebrates on

00:11:45.030 --> 00:11:45.800
the other side.

00:11:45.800 --> 00:11:50.830
Animals, not the insects, not
the worms, not the fish.

00:11:50.830 --> 00:11:57.110
He calculated that at that time,
10,000 years ago, we and

00:11:57.110 --> 00:12:03.430
our livestock accounted for a
fraction of 1% by weight of

00:12:03.430 --> 00:12:07.390
the terrestrial vertebrate
biomass.

00:12:07.390 --> 00:12:11.510
What do you think the
percentage is today?

00:12:11.510 --> 00:12:12.630
Any guesses?

00:12:12.630 --> 00:12:13.450
AUDIENCE: 10%.

00:12:13.450 --> 00:12:15.064
DANIEL DENNETT: 10%.

00:12:15.064 --> 00:12:16.306
AUDIENCE: Over 50%.

00:12:16.306 --> 00:12:17.650
DANIEL DENNETT: Over 50%.

00:12:17.650 --> 00:12:19.314
AUDIENCE: 99%.

00:12:19.314 --> 00:12:21.630
DANIEL DENNETT: 99%,
are you kidding?

00:12:21.630 --> 00:12:24.940
No it's 98%.

00:12:24.940 --> 00:12:29.770
It is 98%.

00:12:29.770 --> 00:12:34.900
We human beings have engulfed
the planet.

00:12:34.900 --> 00:12:38.460
This is one of the most
astonishing biological

00:12:38.460 --> 00:12:41.280
phenomena that has
ever happened.

00:12:41.280 --> 00:12:44.890
And here's what Paul
said about this.

00:12:44.890 --> 00:12:49.250
"Over billions of years on a
unique sphere, chance has

00:12:49.250 --> 00:12:52.430
painted a thin covering
of life--

00:12:52.430 --> 00:12:55.845
complex, improbable, wonderful,
and fragile.

00:12:58.720 --> 00:13:03.630
Suddenly we humans have grown in
population, technology, and

00:13:03.630 --> 00:13:07.740
intelligence to a position of
terrible power: we now wield

00:13:07.740 --> 00:13:10.470
the paintbrush."

00:13:10.470 --> 00:13:12.840
I've highlighted the technology
and intelligence

00:13:12.840 --> 00:13:14.900
because those are the
thinking tools.

00:13:14.900 --> 00:13:18.360
The only difference between us
and our ancestors of 100,000

00:13:18.360 --> 00:13:20.530
years ago is the
thinking tools.

00:13:20.530 --> 00:13:22.280
We have the same genes.

00:13:22.280 --> 00:13:25.470
We have pretty much the
same justice systems.

00:13:25.470 --> 00:13:27.930
The same muscles.

00:13:27.930 --> 00:13:29.500
The same brains.

00:13:29.500 --> 00:13:33.390
The thinking tools have changed
us and made all of

00:13:33.390 --> 00:13:34.580
this power possible.

00:13:34.580 --> 00:13:37.790
It is the source of our power.

00:13:37.790 --> 00:13:41.110
Which raises a sort of
chicken egg problem.

00:13:41.110 --> 00:13:44.550
Did evolved tools
make us smarter?

00:13:44.550 --> 00:13:47.440
Or did we evolve to become smart
enough to make tools?

00:13:50.260 --> 00:13:53.720
And like all good chicken
and egg questions,

00:13:53.720 --> 00:13:54.970
the answer is yes.

00:13:57.470 --> 00:14:01.850
It's a coevolutionary,
bootstrapping process where

00:14:01.850 --> 00:14:05.010
you have a little evolution
makes us smarter.

00:14:05.010 --> 00:14:08.060
And then we can be smarter
about making tools.

00:14:08.060 --> 00:14:10.350
We became can become self
conscious about

00:14:10.350 --> 00:14:11.490
the tools we make.

00:14:11.490 --> 00:14:13.440
Our ancestors weren't
self conscious.

00:14:13.440 --> 00:14:14.750
They didn't invent words.

00:14:14.750 --> 00:14:16.320
They didn't coin words.

00:14:16.320 --> 00:14:18.870
They just found themselves
using words.

00:14:18.870 --> 00:14:20.660
And it made them smarter.

00:14:20.660 --> 00:14:23.240
Now we are very deliberate.

00:14:23.240 --> 00:14:26.710
We have become intelligent
designers of tools.

00:14:26.710 --> 00:14:28.880
But initially we weren't
intelligent

00:14:28.880 --> 00:14:29.710
designers of tools.

00:14:29.710 --> 00:14:32.840
We were simply the beneficiaries
of tools.

00:14:32.840 --> 00:14:34.340
Well how did they
get there then?

00:14:34.340 --> 00:14:35.300
Not by the genes.

00:14:35.300 --> 00:14:37.690
They evolved, culturally.

00:14:37.690 --> 00:14:39.820
My next book is going
to be on cultural

00:14:39.820 --> 00:14:41.730
evolution and that story.

00:14:41.730 --> 00:14:44.150
But I'm not going to say more
about it now because I'm going

00:14:44.150 --> 00:14:49.960
to be looking today at recent
tools intelligently designed,

00:14:49.960 --> 00:14:52.550
supposedly intelligent designed
thinking tools.

00:14:57.400 --> 00:15:00.490
Everybody knows reductio ad
absurdum where this is the

00:15:00.490 --> 00:15:04.010
sort of crowbar of
logical argument.

00:15:04.010 --> 00:15:05.970
It's what you use.

00:15:05.970 --> 00:15:09.730
You take your opponent,
your target's premise.

00:15:09.730 --> 00:15:13.470
You take your target's
thesis on board,

00:15:13.470 --> 00:15:15.350
for the sake of argument.

00:15:15.350 --> 00:15:17.090
And then you use the crowbar.

00:15:17.090 --> 00:15:21.010
You try to deduce a
contradiction from that.

00:15:21.010 --> 00:15:24.240
And if you succeed you win.

00:15:24.240 --> 00:15:26.670
And we use it all the time
very unselfconsciously.

00:15:26.670 --> 00:15:30.970
Very few people, probably
you've used to sort of

00:15:30.970 --> 00:15:36.002
disguise reductio three or four
times in the last day.

00:15:36.002 --> 00:15:42.260
You've said something like well
if that's a bear then

00:15:42.260 --> 00:15:44.790
bears have antlers.

00:15:44.790 --> 00:15:48.150
That's a reductio.

00:15:48.150 --> 00:15:49.870
You'll find yourself
making them.

00:15:49.870 --> 00:15:52.040
Now why do I bother mentioning
that since it's a very

00:15:52.040 --> 00:15:53.270
familiar tool?

00:15:53.270 --> 00:15:59.150
Because I want to point out that
rhetorical questions, the

00:15:59.150 --> 00:16:01.260
questions you're not
supposed to answer.

00:16:01.260 --> 00:16:04.320
Many of them, not all of them,
but very many of them are, in

00:16:04.320 --> 00:16:08.820
fact, disguised reductio
ad absurdum arguments.

00:16:08.820 --> 00:16:13.370
They imply a reductio ad
absurdum argument that the

00:16:13.370 --> 00:16:16.130
writer doesn't bother
presenting.

00:16:16.130 --> 00:16:16.960
It's too obvious.

00:16:16.960 --> 00:16:18.320
It would be embarrassing.

00:16:18.320 --> 00:16:21.670
So you just get the rhetorical
question and nudge,

00:16:21.670 --> 00:16:23.730
nudge you go on.

00:16:23.730 --> 00:16:29.470
So what I want to do is to
create in you a little alarm.

00:16:29.470 --> 00:16:32.060
Sort of like a Google alert.

00:16:32.060 --> 00:16:36.260
So that whenever you see a
rhetorical question in an

00:16:36.260 --> 00:16:38.500
argument, ding.

00:16:38.500 --> 00:16:42.120
You think, ah rhetorical
question.

00:16:42.120 --> 00:16:46.870
You look to see, maybe
I can just answer it.

00:16:46.870 --> 00:16:50.880
Maybe there is a perfectly
unembarrassing answer.

00:16:50.880 --> 00:16:54.340
I remember years ago seeing a
"Peanuts" cartoon and then the

00:16:54.340 --> 00:16:59.370
next to last frame Charlie Brown
is saying, "Who's to say

00:16:59.370 --> 00:17:02.260
what the difference is between
right and wrong." Lucy in the

00:17:02.260 --> 00:17:09.170
last panel says, "I will." and
think about answering a

00:17:09.170 --> 00:17:11.630
rhetorical question but
just answering it.

00:17:11.630 --> 00:17:15.780
See if you can do that.

00:17:15.780 --> 00:17:17.990
A similar thing is what I
call the surely alarm.

00:17:17.990 --> 00:17:21.380
Whenever you see the
word surely a bell

00:17:21.380 --> 00:17:22.630
should ring, ding.

00:17:25.240 --> 00:17:27.930
Let me just give you one.

00:17:27.930 --> 00:17:31.510
Oh, yes, you could make a
computer they could sort fruit

00:17:31.510 --> 00:17:36.510
by its color, but surely
no computer, ding.

00:17:36.510 --> 00:17:40.070
Surely no computer could ever
have a favorite color or

00:17:40.070 --> 00:17:42.750
appreciate color

00:17:42.750 --> 00:17:46.880
When you hear that surely
you are being--

00:17:46.880 --> 00:17:49.120
now maybe you think, I go
along with that one.

00:17:49.120 --> 00:17:50.390
That's got to be right.

00:17:50.390 --> 00:17:52.910
Actually in this audience I
bet a lot of people say, I

00:17:52.910 --> 00:17:54.080
don't think that's so obvious.

00:17:54.080 --> 00:17:56.110
[LAUGHTER]

00:17:59.491 --> 00:18:01.020
DANIEL DENNETT: Why surely?

00:18:01.020 --> 00:18:06.350
Well the author wants to get
this proposition, the

00:18:06.350 --> 00:18:09.450
proposition that follows
it into the discussion.

00:18:09.450 --> 00:18:11.550
Thinks it's true.

00:18:11.550 --> 00:18:12.660
Thinks it's obvious.

00:18:12.660 --> 00:18:14.990
But it's not quite obvious
enough so that it

00:18:14.990 --> 00:18:17.250
goes without saying.

00:18:17.250 --> 00:18:19.830
If it did you wouldn't
have to say it.

00:18:19.830 --> 00:18:21.930
It would go without saying.

00:18:21.930 --> 00:18:26.560
It goes with saying, but he
doesn't bother arguing for it.

00:18:26.560 --> 00:18:30.230
So he gives you just a little
nudge and says surely.

00:18:30.230 --> 00:18:31.650
So be on the lookout.

00:18:31.650 --> 00:18:33.890
I tested my hunch about
this by going

00:18:33.890 --> 00:18:36.350
through, thanks to Google.

00:18:36.350 --> 00:18:37.910
Thanks to string search.

00:18:37.910 --> 00:18:41.830
Going through some dozens of
philosophy of mind papers that

00:18:41.830 --> 00:18:42.900
were online.

00:18:42.900 --> 00:18:43.940
Counting up the surelys.

00:18:43.940 --> 00:18:46.500
Most papers don't have the
word surely in them.

00:18:46.500 --> 00:18:50.140
But I found several dozen
instances of surely.

00:18:50.140 --> 00:18:55.120
And about 1/3 of them were I
thought clearly cases of this

00:18:55.120 --> 00:18:57.300
was the weakest place on
the whole argument.

00:18:57.300 --> 00:19:00.610
This is the place you
want to look first.

00:19:00.610 --> 00:19:04.180
So now if I've alerted
you on rhetorical

00:19:04.180 --> 00:19:07.640
questions and on surely.

00:19:07.640 --> 00:19:10.710
Another one is rathering,
rather.

00:19:10.710 --> 00:19:14.300
As in, it's not the case
of blah blah blah.

00:19:14.300 --> 00:19:17.200
Rather it's ta da da da.

00:19:17.200 --> 00:19:21.600
Rathering signals a dichotomy.

00:19:21.600 --> 00:19:24.420
And very often it singles a
false dichotomy and what you

00:19:24.420 --> 00:19:25.640
should do is, again,
have a little

00:19:25.640 --> 00:19:27.450
alarm ring for rathering.

00:19:27.450 --> 00:19:29.850
When you see the word rather
take a good hard look and see

00:19:29.850 --> 00:19:32.300
if you can't say, well
maybe both blah blah

00:19:32.300 --> 00:19:34.710
blah and da da da.

00:19:34.710 --> 00:19:36.490
That hasn't been asserted.

00:19:36.490 --> 00:19:39.490
It's just been presupposed
and glossed over

00:19:39.490 --> 00:19:41.040
with the word rather.

00:19:41.040 --> 00:19:46.680
So if I get you to adopt these
habits, look what I've done.

00:19:46.680 --> 00:19:49.360
I've installed an app
on your necktop.

00:19:49.360 --> 00:19:55.070
[LAUGHTER]

00:19:55.070 --> 00:19:56.733
DANIEL DENNETT: And I
mean that literally.

00:20:00.220 --> 00:20:04.620
The thing between your ears
is a kind of computer.

00:20:04.620 --> 00:20:09.960
And I don't have to tell you
folks, but the power of a

00:20:09.960 --> 00:20:12.380
computer is the apps
and the levels.

00:20:12.380 --> 00:20:14.550
And you can build and build and
build and build and build

00:20:14.550 --> 00:20:15.720
apps on top of apps.

00:20:15.720 --> 00:20:18.710
That's what the thinking
tools do.

00:20:18.710 --> 00:20:22.090
And that's what makes
us smarter.

00:20:22.090 --> 00:20:24.660
A CPU without an operating
system, you can't do

00:20:24.660 --> 00:20:26.500
very much with it.

00:20:26.500 --> 00:20:30.020
You start building up all the
software and that's where the

00:20:30.020 --> 00:20:31.270
power comes from.

00:20:36.080 --> 00:20:38.530
I don't have to tell you that
computers are thinking tools

00:20:38.530 --> 00:20:39.710
par excellence.

00:20:39.710 --> 00:20:41.650
And in the book I put
an interlude on

00:20:41.650 --> 00:20:43.380
computers in the book.

00:20:43.380 --> 00:20:50.080
And then Peter, I'm happy to
say, he mentioned that I want

00:20:50.080 --> 00:20:55.010
everybody who reads the book to
come away with a little bit

00:20:55.010 --> 00:20:56.760
more basic understanding
of just where

00:20:56.760 --> 00:20:59.850
computers get their power.

00:20:59.850 --> 00:21:02.000
One of the things I do, and
this is so eccentric.

00:21:02.000 --> 00:21:04.450
I think that's probably the only
trade book in history to

00:21:04.450 --> 00:21:05.740
have this feature.

00:21:05.740 --> 00:21:08.520
In this chapter called, "The
Seven Secrets of Computer

00:21:08.520 --> 00:21:15.940
Power revealed," I teach
everybody how to program a

00:21:15.940 --> 00:21:17.822
register machine.

00:21:17.822 --> 00:21:20.530
A Hao Wang simple register
machine that can just do

00:21:20.530 --> 00:21:22.820
increment and decrement
and branch.

00:21:22.820 --> 00:21:25.130
And they teach it how
to do arithmetic.

00:21:25.130 --> 00:21:27.200
And then they teach it how
to do some other things.

00:21:27.200 --> 00:21:30.320
And there's problem sets.

00:21:30.320 --> 00:21:34.110
I've actually got some simple
problem sets in this book.

00:21:34.110 --> 00:21:35.920
And I say, well you don't have
to do them, of course, but if

00:21:35.920 --> 00:21:37.790
you really want to understand.

00:21:37.790 --> 00:21:40.390
If you want to be a computer
for a while and understand

00:21:40.390 --> 00:21:41.670
what it is.

00:21:41.670 --> 00:21:43.330
And, of course, they
learn about loops.

00:21:43.330 --> 00:21:44.660
They learn about counters.

00:21:44.660 --> 00:21:46.830
They learn about all
kinds of things.

00:21:46.830 --> 00:21:49.260
That they hear these words in
other contexts and they think

00:21:49.260 --> 00:21:50.050
they know what they mean.

00:21:50.050 --> 00:21:52.530
Now they're going to know
what they mean.

00:21:52.530 --> 00:21:55.690
The reason I think that's so
important, and again, I know,

00:21:55.690 --> 00:22:00.100
I'm preaching to the choir
here, is one of the most

00:22:00.100 --> 00:22:03.320
important epistemological facts
about computers is that

00:22:03.320 --> 00:22:05.300
if you can make a computer
simulation of

00:22:05.300 --> 00:22:06.990
something that works.

00:22:06.990 --> 00:22:09.170
You know to a moral certainty,
there's no magic.

00:22:09.170 --> 00:22:10.965
There's no morphic resonance.

00:22:10.965 --> 00:22:15.020
There's no psionic forces
or ectoplasm.

00:22:15.020 --> 00:22:18.230
It's good old-fashioned,
every day,

00:22:18.230 --> 00:22:20.850
hardened, honest causation.

00:22:20.850 --> 00:22:22.570
And you just got a proof
of it because of

00:22:22.570 --> 00:22:24.910
the computer itself.

00:22:24.910 --> 00:22:26.300
That's what it is.

00:22:26.300 --> 00:22:33.250
There's no place for any mystery
forces to play a role.

00:22:33.250 --> 00:22:36.850
But people shouldn't just
take that on faith.

00:22:36.850 --> 00:22:39.250
They should know in their
bones why that's true

00:22:39.250 --> 00:22:40.270
and how it's true.

00:22:40.270 --> 00:22:42.330
And that's what that section
is supposed to show them.

00:22:45.010 --> 00:22:48.070
Some intuition pumps don't
even have words,

00:22:48.070 --> 00:22:49.900
they're just images.

00:22:49.900 --> 00:22:50.740
I love this one.

00:22:50.740 --> 00:22:54.550
This is from my friend Matt
Ridley, wonderful evolutionary

00:22:54.550 --> 00:22:56.740
biologist and journalist.

00:22:56.740 --> 00:23:00.430
And it just startles
for a moment.

00:23:00.430 --> 00:23:01.910
But what points can
you make with it?

00:23:01.910 --> 00:23:04.530
I'll just make one
then move on.

00:23:04.530 --> 00:23:07.610
On the left you see an
Acheulean hand-axe.

00:23:07.610 --> 00:23:10.290
On the right, I don't have
to tell you what that is.

00:23:10.290 --> 00:23:14.070
The Acheulean hand-axe was made
by our ancestors without

00:23:14.070 --> 00:23:17.200
any discernible stylistic
change.

00:23:17.200 --> 00:23:20.150
Without any discernible
design improvement for

00:23:20.150 --> 00:23:23.640
over a million years.

00:23:23.640 --> 00:23:25.960
Absolute stasis.

00:23:25.960 --> 00:23:28.680
Amazing.

00:23:28.680 --> 00:23:31.910
Some people have even thought
maybe it wasn't to be used axe

00:23:31.910 --> 00:23:34.730
at all, maybe it was like
the peacock's tail.

00:23:34.730 --> 00:23:38.270
Maybe it was, hey, you want to
see my hand-axe collection?

00:23:41.300 --> 00:23:47.350
I am so good that I can make
hand-axes all day long and

00:23:47.350 --> 00:23:51.670
still get enough to eat,
take care of myself.

00:23:51.670 --> 00:23:53.820
Bragging.

00:23:53.820 --> 00:23:55.220
Sort of like having
a Lamborghini or

00:23:55.220 --> 00:23:57.760
something like that.

00:23:57.760 --> 00:24:01.620
But notice at the one on the
right, familiar as is it is to

00:24:01.620 --> 00:24:04.255
us, it probably has a rather
short lifetime ahead of it.

00:24:07.850 --> 00:24:10.890
How many bet that there
will still be mice

00:24:10.890 --> 00:24:13.440
in a hundred years?

00:24:13.440 --> 00:24:14.916
Right, point made.

00:24:17.540 --> 00:24:19.573
Now here's a similar
comparison.

00:24:22.120 --> 00:24:27.380
On the left we have
a termite castle.

00:24:27.380 --> 00:24:30.450
Australian termite castle.

00:24:30.450 --> 00:24:33.820
And on the right we have Sagrada
Familia, the great

00:24:33.820 --> 00:24:36.580
Gaudi church in Barcelona.

00:24:36.580 --> 00:24:39.770
As I think you can see, they're
stunningly similar

00:24:39.770 --> 00:24:40.990
structures.

00:24:40.990 --> 00:24:44.660
And even internally, if you look
at the cutaways, there's

00:24:44.660 --> 00:24:47.190
a lot of design features that
they have in common.

00:24:47.190 --> 00:24:53.370
These are two remarkable
artifacts made by animals.

00:24:53.370 --> 00:24:56.800
And yet, for all their
similarities, you can see at a

00:24:56.800 --> 00:25:00.710
glance that they are the
products of profoundly

00:25:00.710 --> 00:25:05.710
different histories of design
and construction.

00:25:05.710 --> 00:25:09.520
The one on the left is, this is
bottom up Darwinian design

00:25:09.520 --> 00:25:10.500
and construction.

00:25:10.500 --> 00:25:12.860
The termite, the individual
termites, could

00:25:12.860 --> 00:25:14.810
hardly be more clueless.

00:25:14.810 --> 00:25:16.040
They don't know what
they're doing.

00:25:16.040 --> 00:25:17.320
There's no boss termite.

00:25:17.320 --> 00:25:19.170
There's no architect.

00:25:19.170 --> 00:25:21.060
There's no blueprint.

00:25:21.060 --> 00:25:25.580
There's no leadership roles.

00:25:25.580 --> 00:25:26.910
There's no hierarchy.

00:25:26.910 --> 00:25:29.570
This is bottom up activity.

00:25:29.570 --> 00:25:30.430
And it produces that.

00:25:30.430 --> 00:25:33.170
Whereas Gaudi, it's
almost comical.

00:25:33.170 --> 00:25:36.480
He was like a cartoon
caricature of

00:25:36.480 --> 00:25:38.990
the inspired genius.

00:25:38.990 --> 00:25:42.010
Autocratic, dictatorial,
laying down the law.

00:25:42.010 --> 00:25:44.380
He had blueprints
and manifestos.

00:25:44.380 --> 00:25:46.180
He had accounts of everything.

00:25:46.180 --> 00:25:47.600
And he bossed people around.

00:25:47.600 --> 00:25:51.180
And he told the bosses how
to boss the people.

00:25:51.180 --> 00:25:55.970
Classic case of hierarchical
design and construction.

00:25:55.970 --> 00:25:59.890
Here is one way of looking at
the problem, I think, that

00:25:59.890 --> 00:26:03.250
evolution and cultural evolution
has to explain.

00:26:03.250 --> 00:26:06.900
How did our planet get from
the left to the right?

00:26:06.900 --> 00:26:15.220
How did we get from termite type
R&amp;D to Gaudi-type R&amp;D?

00:26:15.220 --> 00:26:19.140
I think the answer has to be,
well, Gaudi did not have a

00:26:19.140 --> 00:26:20.710
bare brain.

00:26:20.710 --> 00:26:22.220
He had all those
thinking tools.

00:26:22.220 --> 00:26:26.870
And those thinking tools are
the key to opening up top

00:26:26.870 --> 00:26:31.660
down, as it were, intelligent
design type design.

00:26:31.660 --> 00:26:34.060
What Darwin showed us is we
don't need the intelligent

00:26:34.060 --> 00:26:38.490
designer that lots of bottom up,
mindless grubbing around,

00:26:38.490 --> 00:26:40.720
trial and error, will eventually
produce all the

00:26:40.720 --> 00:26:42.200
wonders of the biosphere.

00:26:42.200 --> 00:26:44.230
But then the process keeps
right on going

00:26:44.230 --> 00:26:45.500
with cultural evolution.

00:26:45.500 --> 00:26:48.110
And pretty soon it starts
to reverse itself.

00:26:48.110 --> 00:26:51.520
And we get more and more
intelligent design.

00:26:51.520 --> 00:26:55.310
To where we now have people like
touring, better example

00:26:55.310 --> 00:26:57.730
than Gaudi, because he
really had the goods.

00:26:57.730 --> 00:27:02.310
Who notice, when he built his
computer, he already had proof

00:27:02.310 --> 00:27:03.280
of concept.

00:27:03.280 --> 00:27:04.200
He had the blueprint.

00:27:04.200 --> 00:27:05.020
He had the design.

00:27:05.020 --> 00:27:05.860
He proved it all.

00:27:05.860 --> 00:27:07.810
Nobody would have ever given
him the money to make that

00:27:07.810 --> 00:27:09.750
computer if he hadn't proved
it in advance.

00:27:09.750 --> 00:27:13.340
This is where the idea really
is leading the way.

00:27:13.340 --> 00:27:15.740
That's intelligent design.

00:27:15.740 --> 00:27:18.960
And we've got to explain how it
has happened to be one of

00:27:18.960 --> 00:27:21.420
the fruits of evolution.

00:27:21.420 --> 00:27:22.670
Because it wasn't the cause.

00:27:25.080 --> 00:27:27.880
The Bible says in the beginning
was the Word.

00:27:27.880 --> 00:27:33.450
No, no, the word, very
recent development.

00:27:33.450 --> 00:27:37.290
The word is the beginning
of intelligent design.

00:27:37.290 --> 00:27:39.020
The Bible is right about that.

00:27:43.320 --> 00:27:45.780
Another thinking tool which is
just a picture, but what a

00:27:45.780 --> 00:27:47.090
lovely picture.

00:27:47.090 --> 00:27:49.630
And, by the way, this is Leonard
Eisenberg's version of

00:27:49.630 --> 00:27:50.250
the Tree of Life.

00:27:50.250 --> 00:27:53.400
And you can get it on t-shirts,
big shiny posters

00:27:53.400 --> 00:27:55.250
that you put up in
your office.

00:27:55.250 --> 00:27:56.890
There's all sorts of ways
you can get it if

00:27:56.890 --> 00:27:58.510
you go to his website.

00:27:58.510 --> 00:28:01.240
And what I love about it,
there's many different ways of

00:28:01.240 --> 00:28:02.510
representing the tree of life.

00:28:02.510 --> 00:28:03.860
This has a few nice features.

00:28:03.860 --> 00:28:06.471
I just want to sort of point
them out to you.

00:28:06.471 --> 00:28:08.820
I ran out of battery
at some point.

00:28:08.820 --> 00:28:09.760
OK.

00:28:09.760 --> 00:28:12.420
There's no other pointer
up here, is there?

00:28:12.420 --> 00:28:16.180
I'll just do it like this.

00:28:16.180 --> 00:28:18.120
Here's a pointer.

00:28:18.120 --> 00:28:19.740
A thinking tool!

00:28:19.740 --> 00:28:21.210
Right here on the end
of my finger.

00:28:21.210 --> 00:28:22.460
How wonderful.

00:28:24.410 --> 00:28:28.150
So time goes out radially.

00:28:28.150 --> 00:28:29.720
Here's the birth of the earth.

00:28:29.720 --> 00:28:31.930
Here's the birth of life,
the origin of life.

00:28:31.930 --> 00:28:35.020
And as you can see, the bacteria
and the archea are

00:28:35.020 --> 00:28:38.110
the first single-celled
organisms.

00:28:38.110 --> 00:28:40.950
And then we have a period of
single-celled eukaryotes.

00:28:40.950 --> 00:28:44.470
This is a great, great, great
moment in the history of life.

00:28:44.470 --> 00:28:46.230
It's the eukaryotic
revolution.

00:28:46.230 --> 00:28:51.560
When the endosymbiotic event
put two of these types

00:28:51.560 --> 00:28:53.800
together they joined
forces and became a

00:28:53.800 --> 00:28:55.660
more powerful thing.

00:28:55.660 --> 00:28:57.150
This is technology transfer.

00:28:57.150 --> 00:29:00.470
This is what happens when you
have two independent lineages.

00:29:00.470 --> 00:29:02.080
They've been evolving
independently

00:29:02.080 --> 00:29:04.150
for a billion years.

00:29:04.150 --> 00:29:05.840
And now they join forces.

00:29:05.840 --> 00:29:07.840
They don't all have to
reinvent the wheel.

00:29:07.840 --> 00:29:10.130
They get the benefit of
each current, and

00:29:10.130 --> 00:29:13.290
those eukaryotic cells.

00:29:13.290 --> 00:29:15.660
All the rest of life
is eukaryotes.

00:29:18.200 --> 00:29:20.640
You've heard of the Cambrion
explosion.

00:29:20.640 --> 00:29:21.720
There it is.

00:29:21.720 --> 00:29:25.890
In a very short period of time
tremendous flourishing of

00:29:25.890 --> 00:29:27.570
different kinds of life.

00:29:27.570 --> 00:29:34.840
A period of fantastic
fecund R&amp;D.

00:29:34.840 --> 00:29:36.280
How was it explained?

00:29:36.280 --> 00:29:38.980
That's a story I'd love to tell,
but for another day.

00:29:38.980 --> 00:29:41.740
You can see mass extinctions.

00:29:41.740 --> 00:29:47.030
And then way over here, this
last little fork here, that's

00:29:47.030 --> 00:29:49.540
about 7 million years
on that scale.

00:29:49.540 --> 00:29:52.340
Which is about the length of
time since we shared an

00:29:52.340 --> 00:29:56.060
ancestor with our nearest
relative, the chimpanzee.

00:29:56.060 --> 00:30:01.870
And, of course, of that seven
million years, only a million

00:30:01.870 --> 00:30:06.870
years at most has
any language.

00:30:06.870 --> 00:30:09.660
Some people will push it
back a little further.

00:30:09.660 --> 00:30:15.060
The Acheulean hand-axe comes
before language.

00:30:15.060 --> 00:30:17.470
Some people think that's
a controversial issue.

00:30:17.470 --> 00:30:21.170
But so now notice, Cambrion
explosion.

00:30:21.170 --> 00:30:23.650
Fabulous.

00:30:23.650 --> 00:30:30.570
The MacCready explosion
happens there.

00:30:30.570 --> 00:30:32.920
And transforms the planet.

00:30:32.920 --> 00:30:35.770
All thanks to thinking tools.

00:30:40.860 --> 00:30:47.870
So now, a few recent intuition
pumps that are in the book.

00:30:47.870 --> 00:30:49.190
Because I haven't really
talked about them.

00:30:49.190 --> 00:30:50.640
These are these little
stories.

00:30:50.640 --> 00:30:54.480
So here's an intuition pump I
recently devised to discomfort

00:30:54.480 --> 00:30:57.375
neuroscientists and cognitive
scientists who have been going

00:30:57.375 --> 00:31:00.510
around saying I think
ill-considered things about

00:31:00.510 --> 00:31:04.730
what neuroscience teaches
us about free will.

00:31:04.730 --> 00:31:07.270
So first a little
science fact.

00:31:07.270 --> 00:31:10.740
Damiaan Denys, a very brilliant
researcher in

00:31:10.740 --> 00:31:13.720
Amsterdam, has made a little
microchip which controls

00:31:13.720 --> 00:31:15.050
obsessive-compulsive disorder.

00:31:15.050 --> 00:31:16.990
You implant it in a
person's brain.

00:31:16.990 --> 00:31:20.520
Really does a very good job.

00:31:20.520 --> 00:31:22.070
Science fiction.

00:31:22.070 --> 00:31:25.910
So one day this guy with OCD
goes to see his neurosurgeon.

00:31:25.910 --> 00:31:28.380
And she implants one
of in these Denys

00:31:28.380 --> 00:31:30.290
microchips in his brain.

00:31:30.290 --> 00:31:32.280
Sews him up.

00:31:32.280 --> 00:31:34.110
The anaesthesia wears off.

00:31:34.110 --> 00:31:37.140
And as he's about to leave she
debriefs him a little bit.

00:31:37.140 --> 00:31:40.110
She says, oh and by the way,
my staff will be monitoring

00:31:40.110 --> 00:31:44.000
you 24 hours a day, seven
days a week from our

00:31:44.000 --> 00:31:45.010
control room here.

00:31:45.010 --> 00:31:46.440
And you're going to think
you have free will.

00:31:46.440 --> 00:31:47.910
You're going to think you're
making your own decisions.

00:31:47.910 --> 00:31:49.565
But, in fact, we're going
to be making all the

00:31:49.565 --> 00:31:51.280
decisions for you.

00:31:51.280 --> 00:31:53.470
Free will is just an
illusion for you.

00:31:53.470 --> 00:31:54.720
Have a nice life, goodbye.

00:31:57.620 --> 00:31:59.400
And he believes her.

00:31:59.400 --> 00:32:03.280
Well, shiny lab, lab
coat, all the rest.

00:32:03.280 --> 00:32:06.960
And as you might imagine he
becomes a little negligent in

00:32:06.960 --> 00:32:07.750
his habits.

00:32:07.750 --> 00:32:09.195
And a little bit aggressive.

00:32:09.195 --> 00:32:14.630
And indulges in some of
his worst behaviors.

00:32:14.630 --> 00:32:17.780
And soon enough he gets in
trouble with the law.

00:32:17.780 --> 00:32:20.400
Gets hauled into court,
charged with a

00:32:20.400 --> 00:32:21.720
fairly serious crime.

00:32:21.720 --> 00:32:23.130
He says, but Your Honor.

00:32:23.130 --> 00:32:25.260
I don't have free will.

00:32:25.260 --> 00:32:26.160
How do know that?

00:32:26.160 --> 00:32:30.090
Well my neurosurgeon told me,
free will is just an illusion.

00:32:30.090 --> 00:32:33.870
It feels like I'm making these
decisions, but I'm not really.

00:32:33.870 --> 00:32:36.250
So they call the neurosurgeon
to the stand

00:32:36.250 --> 00:32:37.260
and ask her to testify.

00:32:37.260 --> 00:32:40.340
And under oath she says, Yeah,
that's what I told him.

00:32:40.340 --> 00:32:42.260
But it was just a joke.

00:32:42.260 --> 00:32:43.510
I was just messing
with his head.

00:32:46.480 --> 00:32:50.540
Now how many of you agree that
she did, not just a mean

00:32:50.540 --> 00:32:52.240
thing, a socially irresponsible
thing.

00:32:52.240 --> 00:32:57.220
But she did very serious
harm to this guy.

00:32:57.220 --> 00:33:02.420
With her language, she did
nonsurgically she claimed to

00:33:02.420 --> 00:33:03.550
do surgically.

00:33:03.550 --> 00:33:07.390
She disabled him as
a moral agent.

00:33:07.390 --> 00:33:11.570
By just convincing him that
he didn't have free will.

00:33:11.570 --> 00:33:17.000
Now I ask my neuroscientist
friends, aren't you just doing

00:33:17.000 --> 00:33:19.760
the same thing wholesale?

00:33:19.760 --> 00:33:21.990
When you write these articles
saying, we have learned in

00:33:21.990 --> 00:33:24.860
neuroscience that we don't
have free will.

00:33:24.860 --> 00:33:29.630
The reason they say that turns
out to be very bad reasons.

00:33:29.630 --> 00:33:32.750
Having to do with, well
everything in your brain is

00:33:32.750 --> 00:33:34.220
determined by what happens
in your brain

00:33:34.220 --> 00:33:34.980
a little bit before.

00:33:34.980 --> 00:33:35.630
Yeah, of course.

00:33:35.630 --> 00:33:36.710
So what?

00:33:36.710 --> 00:33:38.850
That doesn't show you don't
have free will.

00:33:38.850 --> 00:33:41.960
They have a very simplistic
idea of what free will is.

00:33:41.960 --> 00:33:45.580
But it's the idea that would
actually ground moral

00:33:45.580 --> 00:33:47.920
responsibility.

00:33:47.920 --> 00:33:49.570
If you didn't have it you
wouldn't have moral

00:33:49.570 --> 00:33:50.710
responsibility.

00:33:50.710 --> 00:33:52.280
But they're not showing
that at all.

00:33:52.280 --> 00:33:54.750
And I think that's
irresponsible.

00:33:54.750 --> 00:34:00.070
Now you might say, well, if your
brain is determined, and

00:34:00.070 --> 00:34:02.560
then you don't have free will.

00:34:02.560 --> 00:34:05.430
How many of you are inclined
to think, if every event in

00:34:05.430 --> 00:34:09.600
your brain is determined then
you don't have free will?

00:34:09.600 --> 00:34:12.010
The general run of people think
that's sort of, even

00:34:12.010 --> 00:34:13.630
obviously true.

00:34:13.630 --> 00:34:15.500
So here's a little
intuition pump to

00:34:15.500 --> 00:34:16.830
dissuade you from that.

00:34:16.830 --> 00:34:18.040
Two lotteries.

00:34:18.040 --> 00:34:23.070
In lottery A the winning ticket
is chosen after the

00:34:23.070 --> 00:34:24.409
tickets are sold.

00:34:24.409 --> 00:34:27.580
Most lotteries are like that.

00:34:27.580 --> 00:34:30.540
In Lottery B the winning ticket
is chosen before the

00:34:30.540 --> 00:34:31.650
tickets are sold.

00:34:31.650 --> 00:34:35.219
And the stub is secretly put
in a safe for safekeeping

00:34:35.219 --> 00:34:37.739
until the tickets are sold.

00:34:37.739 --> 00:34:42.870
Now how many of you think that
you're a fool to buy a ticket

00:34:42.870 --> 00:34:43.840
in Lottery B?

00:34:43.840 --> 00:34:48.580
That only lottery A gives you
any chance of winning?

00:34:48.580 --> 00:34:53.820
How many think that
they're even?

00:34:53.820 --> 00:34:54.790
Yeah.

00:34:54.790 --> 00:34:57.670
And so does most of the
rest of the world.

00:34:57.670 --> 00:34:59.100
When you put in these terms.

00:34:59.100 --> 00:35:00.600
They recognize that,
no, you've got a

00:35:00.600 --> 00:35:02.140
chance either way.

00:35:02.140 --> 00:35:05.660
And Publishers Clearing House
actually says, you may

00:35:05.660 --> 00:35:06.910
already have won.

00:35:09.510 --> 00:35:14.700
People see that they still have
a chance if the winning

00:35:14.700 --> 00:35:17.150
tickets are chosen in advance.

00:35:17.150 --> 00:35:20.940
But now think, if determinism is
true then all your lottery

00:35:20.940 --> 00:35:24.340
tickets were chosen before
you were born.

00:35:24.340 --> 00:35:26.050
At the Big Bang, if you like.

00:35:26.050 --> 00:35:30.250
And sort of put in on a little
secret safe in your body.

00:35:30.250 --> 00:35:35.710
And as life goes on, whenever
you need a coin flip, whenever

00:35:35.710 --> 00:35:38.252
you need a little chaos, you
just take the next one out of

00:35:38.252 --> 00:35:42.210
the envelope and you win
some you lose some.

00:35:42.210 --> 00:35:45.930
That's just as free as if
there's a quantum miracle that

00:35:45.930 --> 00:35:48.800
happens right then and there.

00:35:48.800 --> 00:35:52.730
So what if they're
chosen first.

00:35:52.730 --> 00:35:56.550
Well somebody might say, well
you see, if all your lottery

00:35:56.550 --> 00:35:59.710
tickets are chosen before you're
born, you're determined

00:35:59.710 --> 00:36:01.320
to have good luck on
some occasions

00:36:01.320 --> 00:36:02.610
and bad luck on others.

00:36:02.610 --> 00:36:03.950
You're determined.

00:36:03.950 --> 00:36:06.390
Well that's true even if
indeterminism is true.

00:36:09.180 --> 00:36:12.310
Even if indeterminism is true,
you're determined on some

00:36:12.310 --> 00:36:14.650
occasions to win and some
occasions to lose, you just

00:36:14.650 --> 00:36:17.530
don't know which.

00:36:17.530 --> 00:36:20.510
Fairness isn't always winning.

00:36:20.510 --> 00:36:23.980
You've got to accept the
fact that there's luck.

00:36:23.980 --> 00:36:26.530
And that luck is consistent
with free will.

00:36:26.530 --> 00:36:28.340
But a lot of philosophers
haven't seen it.

00:36:28.340 --> 00:36:30.630
A very good one, David Wiggins,
once wrote some years

00:36:30.630 --> 00:36:35.070
ago about what he called the
cosmic unfairness of

00:36:35.070 --> 00:36:36.920
determinism.

00:36:36.920 --> 00:36:40.960
And you can see, or many people
think, yeah right, it's

00:36:40.960 --> 00:36:45.220
a bummer if determinism
is true.

00:36:45.220 --> 00:36:48.480
But I want to ask what about
the cosmic unfairness of

00:36:48.480 --> 00:36:51.020
indeterminism?

00:36:51.020 --> 00:36:53.070
Just as bad.

00:36:53.070 --> 00:36:54.320
You know.

00:36:56.550 --> 00:36:58.870
Life isn't always a picnic.

00:36:58.870 --> 00:37:02.080
That doesn't mean that we can't
be responsible agents.

00:37:02.080 --> 00:37:04.870
Being a responsible agent,
having free will in the sense

00:37:04.870 --> 00:37:09.480
that matters, has zip to do
with whether physics is

00:37:09.480 --> 00:37:11.170
deterministic or
indeterministic.

00:37:11.170 --> 00:37:13.860
Or whether your brain
is deterministic or

00:37:13.860 --> 00:37:14.860
indeterministic.

00:37:14.860 --> 00:37:19.250
Has to do with the software
you've got in your brain.

00:37:19.250 --> 00:37:21.920
It has to do with the competence
that you have had

00:37:21.920 --> 00:37:25.390
installed in your brain since
you were a little kid.

00:37:25.390 --> 00:37:28.190
If you were trained with a good,
moral upbringing then

00:37:28.190 --> 00:37:29.770
you make the right decisions
at the right

00:37:29.770 --> 00:37:31.380
time most the time.

00:37:31.380 --> 00:37:34.860
And you are, in general,
competent enough so when you

00:37:34.860 --> 00:37:37.980
screw up, you are held
responsible.

00:37:37.980 --> 00:37:38.220
Why?

00:37:38.220 --> 00:37:42.170
Because you're good enough
to be responsible.

00:37:42.170 --> 00:37:42.915
That's a long story.

00:37:42.915 --> 00:37:45.130
I'm not going to say
more about that.

00:37:45.130 --> 00:37:47.300
Time to quit this talk.

00:37:47.300 --> 00:37:50.990
So the book is sort
of a tapas book.

00:37:50.990 --> 00:37:53.660
There are 77 chapters.

00:37:53.660 --> 00:37:55.300
And some of them are
just a page.

00:37:55.300 --> 00:37:58.620
Some of them there are maybe
as much as 10 pages long.

00:37:58.620 --> 00:38:02.600
And they each introduce a tool
making it autonomous.

00:38:02.600 --> 00:38:05.350
I tried to make them all
portable and accessible.

00:38:05.350 --> 00:38:08.850
So my recommendation
is to try them out.

00:38:08.850 --> 00:38:10.190
Take them for a spin.

00:38:10.190 --> 00:38:14.800
And above all, turn all the
knobs to see how they work.

00:38:14.800 --> 00:38:15.890
Thanks for coming.

00:38:15.890 --> 00:38:20.685
[APPLAUSE]

00:38:20.685 --> 00:38:23.655
DANIEL DENNETT: Questions?

00:38:23.655 --> 00:38:26.130
AUDIENCE: Hello.

00:38:26.130 --> 00:38:29.050
So intelligent design
is better in some

00:38:29.050 --> 00:38:31.570
domains than others.

00:38:31.570 --> 00:38:36.440
We can design a very nice mobile
phone or computer.

00:38:36.440 --> 00:38:41.430
When it comes to an economy,
well, some may argue, but so

00:38:41.430 --> 00:38:43.520
far we've yet to show we can

00:38:43.520 --> 00:38:46.520
intelligently design an economy.

00:38:46.520 --> 00:38:49.980
Do you have, well
two questions.

00:38:49.980 --> 00:38:53.610
First of all, what you think the
profit is of things which

00:38:53.610 --> 00:38:58.295
are not very tractable to
intelligent design?

00:38:58.295 --> 00:39:02.620
And do you think eventually all
problems we'll have the

00:39:02.620 --> 00:39:04.670
right tools in order to
design everything?

00:39:07.640 --> 00:39:10.180
DANIEL DENNETT: No, I think
there's a trade-off which

00:39:10.180 --> 00:39:12.115
other people have described
pretty well.

00:39:14.950 --> 00:39:19.100
Natural selection is
a great engine.

00:39:19.100 --> 00:39:25.070
And it feeds on chaos and
so-called randomness.

00:39:25.070 --> 00:39:26.960
But it works, remember,
by trial and error.

00:39:26.960 --> 00:39:28.340
And there's an awful
lot of errors.

00:39:28.340 --> 00:39:30.840
And it's a very wasteful
product.

00:39:30.840 --> 00:39:33.240
Very wasteful way of
doing business.

00:39:33.240 --> 00:39:38.780
You've got to have a
lot of failures.

00:39:38.780 --> 00:39:43.410
And so that's what happens,
let's say, with an economy.

00:39:43.410 --> 00:39:48.160
And to that extent, that grubby
old social Darwinist

00:39:48.160 --> 00:39:53.500
line about how the survival of
the fittest in the economy has

00:39:53.500 --> 00:39:55.010
some truth to it.

00:39:55.010 --> 00:39:59.810
But that doesn't mean that we
should simply honor that and

00:39:59.810 --> 00:40:02.630
make that a moral principle.

00:40:02.630 --> 00:40:05.790
Because I think we have very
good reasons to think that an

00:40:05.790 --> 00:40:09.230
unfettered, completely chaotic,
laissez-faire,

00:40:09.230 --> 00:40:14.530
dog-eat-dog economy is
not a good way to go.

00:40:14.530 --> 00:40:17.780
Too much harm can accrue.

00:40:17.780 --> 00:40:20.090
You can get these winner
take all situations

00:40:20.090 --> 00:40:22.200
which can be very bad.

00:40:22.200 --> 00:40:24.530
You know about winner
take all algorithms.

00:40:24.530 --> 00:40:28.520
You know about opponent
processes in the brain.

00:40:28.520 --> 00:40:31.550
In the work that I've been
doing on how brains are

00:40:31.550 --> 00:40:37.190
organized, I think, first of
all, they are computers.

00:40:37.190 --> 00:40:41.650
But they're very, very different
from laptops.

00:40:41.650 --> 00:40:47.470
Precisely because you've
got 100 million

00:40:47.470 --> 00:40:50.180
or 200 million neurons.

00:40:50.180 --> 00:40:51.970
Each one is like a
little termite.

00:40:51.970 --> 00:40:53.920
They actually have their
own agendas.

00:40:53.920 --> 00:40:56.010
There's no two alike.

00:40:56.010 --> 00:41:00.410
And the way software works in
the brain is by creating

00:41:00.410 --> 00:41:03.730
coalitions and getting them
to work together.

00:41:03.730 --> 00:41:07.890
And unity is an achievement,
it's possible.

00:41:07.890 --> 00:41:12.080
But it isn't built in
from the top down.

00:41:12.080 --> 00:41:14.640
And once we think about the
brain being organized that

00:41:14.640 --> 00:41:17.470
way, we should think that's
probably economies are best

00:41:17.470 --> 00:41:18.950
built in that way too.

00:41:18.950 --> 00:41:26.310
Without too much top down
central control, but with some

00:41:26.310 --> 00:41:30.340
policing to protect us from the
excesses that otherwise

00:41:30.340 --> 00:41:32.255
would accrue.

00:41:32.255 --> 00:41:36.456
AUDIENCE: Could you give your
opinion on Newcomb's paradox?

00:41:36.456 --> 00:41:37.350
DANIEL DENNETT: Yeah.

00:41:37.350 --> 00:41:39.890
Do you know what Newcomb's
paradox is?

00:41:39.890 --> 00:41:42.690
This is the one boxer
or two boxer.

00:41:42.690 --> 00:41:45.540
OK, so you've got.

00:41:45.540 --> 00:41:47.010
First I'm going to have
to explain it.

00:41:47.010 --> 00:41:48.730
See if I can get it right.

00:41:48.730 --> 00:41:51.670
So you're on a sort of quiz
show and you're given the

00:41:51.670 --> 00:41:53.750
following weird choice.

00:41:53.750 --> 00:41:57.210
You could be a one boxer, or
you can be a two boxer.

00:41:57.210 --> 00:41:58.730
In one box there's a $1,000.

00:41:58.730 --> 00:42:00.500
In the other there's $1
million versus $0.

00:42:04.000 --> 00:42:06.800
It would be hard enough if you
had to choose either box A or

00:42:06.800 --> 00:42:12.460
box B. Would you take a chance
on $1 million, or would you go

00:42:12.460 --> 00:42:16.140
for the sure $1,000?

00:42:16.140 --> 00:42:18.230
But that's not your choice.

00:42:18.230 --> 00:42:20.890
Your choice is either to go
for a box of B which has

00:42:20.890 --> 00:42:23.010
either got the $1 million
or nothing.

00:42:23.010 --> 00:42:25.890
Or to go from boxes.

00:42:25.890 --> 00:42:27.620
Got it?

00:42:27.620 --> 00:42:31.890
Well one line of research, one
line of thinking, says well,

00:42:31.890 --> 00:42:34.610
dummy, of course you
go for both boxes.

00:42:34.610 --> 00:42:38.165
That way at least you get $1,000
and maybe get a $1

00:42:38.165 --> 00:42:39.280
million and $1,000.

00:42:39.280 --> 00:42:42.630
That dominates decision
tree, draw it out.

00:42:42.630 --> 00:42:45.960
That's the one you should do.

00:42:45.960 --> 00:42:49.010
And so it would be, except
for one thing.

00:42:49.010 --> 00:42:54.270
The MC of the program tells you,
oh, by the way, you're

00:42:54.270 --> 00:42:59.380
the 5,000th contestant
on the show.

00:42:59.380 --> 00:43:03.430
And we're very good at
predicting whether you're

00:43:03.430 --> 00:43:06.420
going to be a one boxer
or a two boxer.

00:43:06.420 --> 00:43:10.620
And if we predict that you're
going to make the two boxer

00:43:10.620 --> 00:43:17.170
choice, then there's only the
$1,000 in box A. We never put

00:43:17.170 --> 00:43:18.320
the $1 million.

00:43:18.320 --> 00:43:21.550
We only put the $1 million in
when we predict that you're

00:43:21.550 --> 00:43:24.360
going to go for one box.

00:43:24.360 --> 00:43:26.050
And do you want to know
our track record?

00:43:26.050 --> 00:43:29.070
It's almost perfect.

00:43:29.070 --> 00:43:35.590
They might add, well you know,
one poor devil went for box A

00:43:35.590 --> 00:43:39.060
and it was empty, or box
B and it was empty.

00:43:39.060 --> 00:43:43.720
And one lucky devil out of 500
or whatever, went for both and

00:43:43.720 --> 00:43:45.320
got both rewards.

00:43:45.320 --> 00:43:49.510
But in general it's a very
strong statistical

00:43:49.510 --> 00:43:50.610
correlation.

00:43:50.610 --> 00:43:53.580
So now I ask you.

00:43:53.580 --> 00:43:55.525
How many of you go
for both boxes?

00:43:58.310 --> 00:44:02.780
And how many of you go
for just the one box?

00:44:02.780 --> 00:44:05.460
What's nice about Newcomb's
paradox is that it ain't

00:44:05.460 --> 00:44:07.230
obvious at all.

00:44:07.230 --> 00:44:10.630
And an awful lot of ink has
been spilled trying to get

00:44:10.630 --> 00:44:12.290
clear about this.

00:44:12.290 --> 00:44:15.960
And I have, myself, not spilled
any ink because I

00:44:15.960 --> 00:44:19.365
still don't have a
firm view on it.

00:44:23.170 --> 00:44:28.480
I tend to think there must be
something a little bit screwy

00:44:28.480 --> 00:44:32.250
about the setting up of this
thought experiment.

00:44:32.250 --> 00:44:37.800
But it's very hard
to show that.

00:44:37.800 --> 00:44:40.810
And I highly recommend
it to you.

00:44:40.810 --> 00:44:44.070
There's a lot of good things
have been written about it.

00:44:44.070 --> 00:44:47.960
So I'm sorry I can't answer
the question.

00:44:47.960 --> 00:44:48.780
I go back and forth.

00:44:48.780 --> 00:44:51.450
Some days I'm a one boxer, some
days I'm a two boxer.

00:44:51.450 --> 00:44:53.860
AUDIENCE: I wanted
to ask you about

00:44:53.860 --> 00:44:56.640
determinism versus free will.

00:44:56.640 --> 00:44:57.710
Since you were talking
about earlier.

00:44:57.710 --> 00:45:00.910
And, in particular, I think
Hofstadter would talk about a

00:45:00.910 --> 00:45:02.790
lover's disparity there.

00:45:02.790 --> 00:45:05.680
That determinism is kind of a
totally different question

00:45:05.680 --> 00:45:06.930
then free will.

00:45:06.930 --> 00:45:08.290
I wondered if you could
talk about that.

00:45:08.290 --> 00:45:09.890
DANIEL DENNETT: Yeah Doug
and I have talked about

00:45:09.890 --> 00:45:10.560
this over the years.

00:45:10.560 --> 00:45:12.710
And Doug disagrees with me.

00:45:12.710 --> 00:45:15.570
It's one of the few points where
he and I have a serious

00:45:15.570 --> 00:45:15.910
disagreement.

00:45:15.910 --> 00:45:18.300
And he's actually quite
pessimistic about this and

00:45:18.300 --> 00:45:22.390
thinks that the sense of free
will that really matters is

00:45:22.390 --> 00:45:25.280
one that we can't have because
of determinism.

00:45:25.280 --> 00:45:29.170
And I've done my level best in
several books and many hours

00:45:29.170 --> 00:45:35.630
to convince him otherwise
without success yet.

00:45:35.630 --> 00:45:41.310
Unlike the fellow in my tale, it
doesn't lead him to behave

00:45:41.310 --> 00:45:45.350
in a irresponsible way.

00:45:45.350 --> 00:45:47.750
So there's at least to
could be said for it.

00:45:52.130 --> 00:45:54.550
I think that Doug--

00:45:54.550 --> 00:45:57.020
here's what I think
the problem is.

00:45:57.020 --> 00:45:58.750
That the term free will.

00:45:58.750 --> 00:46:00.050
Where does it come from?

00:46:00.050 --> 00:46:02.830
It comes from every day.

00:46:02.830 --> 00:46:04.790
It comes from the law
and from tradition.

00:46:04.790 --> 00:46:06.720
It's not a scientific term.

00:46:06.720 --> 00:46:09.720
It's a term you get from what's
called the manifest

00:46:09.720 --> 00:46:11.100
image, in philosophy.

00:46:11.100 --> 00:46:14.380
The world of colors
and dollars and

00:46:14.380 --> 00:46:16.310
baseball and so forth.

00:46:16.310 --> 00:46:23.260
Not the world of neurons and
proteins and atoms, or for

00:46:23.260 --> 00:46:26.110
that matter, scientific laws.

00:46:26.110 --> 00:46:33.710
When you look at it's home
territory, free will, you can

00:46:33.710 --> 00:46:36.330
see it does two jobs.

00:46:36.330 --> 00:46:39.140
And they both seem to be
definitive or constitutive.

00:46:39.140 --> 00:46:43.010
On the one hand, it seems
definitive that well free will

00:46:43.010 --> 00:46:45.850
means uncaused, it
means it's free.

00:46:45.850 --> 00:46:50.520
There's no sufficient
causal antecedent.

00:46:50.520 --> 00:46:53.010
For many people I think that's
just obvious that's what free

00:46:53.010 --> 00:46:55.880
will means.

00:46:55.880 --> 00:47:00.310
But the other thing it means is
it's the standard condition

00:47:00.310 --> 00:47:03.490
for being morally competent.

00:47:03.490 --> 00:47:08.260
For instance, how many of you
believe yourself qualified,

00:47:08.260 --> 00:47:13.060
eligible, to buy a car
to take out a loan?

00:47:13.060 --> 00:47:14.620
Yeah.

00:47:14.620 --> 00:47:17.300
And you sign the paperwork
of your own free will.

00:47:17.300 --> 00:47:20.140
Now there are people who don't
have that competence.

00:47:20.140 --> 00:47:23.340
They're senile or they're
too young.

00:47:23.340 --> 00:47:25.300
There's something you've got
that they don't have.

00:47:25.300 --> 00:47:29.370
And it is your very condition of
your living a free life as

00:47:29.370 --> 00:47:32.610
a citizen and conducting
business, making promises that

00:47:32.610 --> 00:47:34.110
count and so forth.

00:47:34.110 --> 00:47:36.580
I think that's the best
notion of free will.

00:47:36.580 --> 00:47:37.880
That's the one that matters.

00:47:37.880 --> 00:47:40.670
And I think that notion
has nothing to do with

00:47:40.670 --> 00:47:41.920
determinism.

00:47:44.780 --> 00:47:48.270
For instance, I don't think
your competence to sign a

00:47:48.270 --> 00:47:54.960
contract would be disparaged
in the slightest if

00:47:54.960 --> 00:47:57.960
determinism were true.

00:47:57.960 --> 00:48:01.510
So I have argued for years
there's just no reason to

00:48:01.510 --> 00:48:05.720
prefer that your brain
be deterministic over

00:48:05.720 --> 00:48:06.620
indeterministic.

00:48:06.620 --> 00:48:07.380
No reason at all.

00:48:07.380 --> 00:48:08.780
No good reason.

00:48:08.780 --> 00:48:14.390
I've recently had to back off
that a little bit by a very

00:48:14.390 --> 00:48:19.570
persistent lover of
indeterminism and free will,

00:48:19.570 --> 00:48:22.220
who finally showed me that there
was, by my own lights, a

00:48:22.220 --> 00:48:24.210
condition where I would
agree with them.

00:48:24.210 --> 00:48:27.090
I would wish for indeterminism
in that case.

00:48:27.090 --> 00:48:29.180
And I'll tell you what it is.

00:48:29.180 --> 00:48:32.190
You all know the game
rock-paper-scissors?

00:48:32.190 --> 00:48:33.580
Well the best way to
play that game, of

00:48:33.580 --> 00:48:35.760
course, is to play random.

00:48:35.760 --> 00:48:39.560
It's very hard to fake
randomness or to put together

00:48:39.560 --> 00:48:40.600
a random series.

00:48:40.600 --> 00:48:44.060
If you're random you give you
give your opponent no pattern

00:48:44.060 --> 00:48:45.400
to track at all.

00:48:45.400 --> 00:48:48.230
So they can't do any better than
play random against you.

00:48:48.230 --> 00:48:49.480
You'll break even.

00:48:55.320 --> 00:49:00.650
So since that's the case, you
might just as well memorize a

00:49:00.650 --> 00:49:07.340
few hundred entries in a table
of random numbers and use that

00:49:07.340 --> 00:49:10.020
to guide your choices.

00:49:10.020 --> 00:49:11.490
You'll do fine as long as
you don't let the other

00:49:11.490 --> 00:49:13.360
guy see your list.

00:49:13.360 --> 00:49:15.450
Which would be terrible.

00:49:15.450 --> 00:49:18.130
So now I can tell you, here's
the condition under which I

00:49:18.130 --> 00:49:20.730
would hope that indeterminism
is true.

00:49:20.730 --> 00:49:23.890
If I were playing
rock-paper-scissors for high

00:49:23.890 --> 00:49:25.265
stakes with God.

00:49:28.700 --> 00:49:33.200
I don't really anticipate that
happening in the future, so I

00:49:33.200 --> 00:49:36.380
am quite content to live out
my life in a deterministic

00:49:36.380 --> 00:49:39.060
world and I'll take
responsibility for my acts,

00:49:39.060 --> 00:49:43.030
even though they are, for all I
know or care, determined by

00:49:43.030 --> 00:49:47.120
the kazillion things that
happened both before

00:49:47.120 --> 00:49:48.370
and after my birth.

00:49:50.880 --> 00:49:51.080
Yes.

00:49:51.080 --> 00:49:55.060
AUDIENCE: Is consciousness an
epiphenomenon of advanced

00:49:55.060 --> 00:49:58.110
biological systems and tends to
disappear as you look back

00:49:58.110 --> 00:49:59.350
across the Tree of Life?

00:49:59.350 --> 00:50:01.770
Or is it a fundamental dimension
to nature and

00:50:01.770 --> 00:50:05.541
reality like the dimensions
of space and time?

00:50:05.541 --> 00:50:07.220
DANIEL DENNETT: OK
you said or.

00:50:07.220 --> 00:50:08.470
This is sort of a rathering.

00:50:10.870 --> 00:50:12.580
You didn't say rather,
but there was a sort

00:50:12.580 --> 00:50:15.390
of rathering there.

00:50:15.390 --> 00:50:21.700
But I'll take it on because I
do not think that it is a

00:50:21.700 --> 00:50:26.580
fundamental category that splits
the universe in two.

00:50:26.580 --> 00:50:32.030
I think that the idea that it
is deeply rooted in culture.

00:50:32.030 --> 00:50:33.410
And I suppose it
might be right.

00:50:33.410 --> 00:50:38.850
But I think that consciousness
is like reproduction,

00:50:38.850 --> 00:50:42.990
metabolism, and self repair,
healing, blood clotting.

00:50:42.990 --> 00:50:48.110
It's a very interestingly
complex and deeply recursive

00:50:48.110 --> 00:50:50.530
biological phenomenon.

00:50:50.530 --> 00:50:54.300
It is not in the interesting
sense, and there's many senses

00:50:54.300 --> 00:50:56.150
of consciousness.

00:50:56.150 --> 00:50:59.940
In the sense that we in this
room are conscious, which is

00:50:59.940 --> 00:51:06.060
the anchoring that's home base
from our own experience and

00:51:06.060 --> 00:51:07.420
our reading of literature.

00:51:07.420 --> 00:51:09.730
We read novels and
things like that.

00:51:09.730 --> 00:51:13.300
We have the sense of what a
conscious life is like.

00:51:13.300 --> 00:51:16.200
I think we're the only creatures
that have that kind

00:51:16.200 --> 00:51:16.860
of conscious life.

00:51:16.860 --> 00:51:17.970
I don't think dogs have it.

00:51:17.970 --> 00:51:19.650
I don't think dolphins
have it.

00:51:19.650 --> 00:51:20.640
They're sentient.

00:51:20.640 --> 00:51:21.420
They're awake.

00:51:21.420 --> 00:51:22.540
They feel pain.

00:51:22.540 --> 00:51:27.900
But they don't have most of what
makes human consciousness

00:51:27.900 --> 00:51:30.870
so important.

00:51:30.870 --> 00:51:40.040
And I think that phenomena
is, in the end, software.

00:51:40.040 --> 00:51:42.760
And I think it has
to be installed.

00:51:42.760 --> 00:51:46.490
So it is an implication,
in my view, and

00:51:46.490 --> 00:51:47.520
don't try this at home.

00:51:47.520 --> 00:51:49.190
Don't test this.

00:51:49.190 --> 00:51:53.570
That if a human baby could be
raised entirely without any

00:51:53.570 --> 00:51:55.530
interaction with other
human beings and

00:51:55.530 --> 00:51:56.640
cut off from language.

00:51:56.640 --> 00:52:02.790
If it could grow to adulthood
without benefit of all the

00:52:02.790 --> 00:52:06.190
thinking tools that would
normally be installed in the

00:52:06.190 --> 00:52:09.600
course of just growing up
in a social community.

00:52:09.600 --> 00:52:15.650
I think that human being would
have a mind so profoundly

00:52:15.650 --> 00:52:19.870
different from ours that we
would all pretty well agree he

00:52:19.870 --> 00:52:22.690
or she wasn't really
conscious.

00:52:22.690 --> 00:52:26.010
Now that may be a very
eccentric notion of

00:52:26.010 --> 00:52:26.900
consciousness.

00:52:26.900 --> 00:52:31.320
But if you're wanting to know,
do I think that, if you look

00:52:31.320 --> 00:52:33.530
for a better notion of
consciousness or one that

00:52:33.530 --> 00:52:35.370
isn't so eccentric.

00:52:35.370 --> 00:52:37.350
And you start thinking
about what it is.

00:52:37.350 --> 00:52:43.450
The capacity to experience color
and flavor and to have

00:52:43.450 --> 00:52:45.230
qualia, as philosophers say.

00:52:45.230 --> 00:52:47.415
I think that's all pretty
badly confused.

00:52:47.415 --> 00:52:52.780
And I've tried to say how
and why over the years.

00:52:52.780 --> 00:52:57.520
But part of what makes it
confusing is it people want

00:52:57.520 --> 00:53:02.340
consciousness to be, to
use a technical term,

00:53:02.340 --> 00:53:03.590
supercalifragili
sticexpialidocious.

00:53:06.030 --> 00:53:08.680
And if you're not talking
about something that is

00:53:08.680 --> 00:53:11.570
supercalifragili
sticexpialidocious then you're

00:53:11.570 --> 00:53:14.920
not talking about
consciousness.

00:53:14.920 --> 00:53:21.800
And there's a quote my friend
Lee Siegel, he's a philsopher

00:53:21.800 --> 00:53:22.550
and a magician.

00:53:22.550 --> 00:53:24.280
A very fine stage magician.

00:53:24.280 --> 00:53:27.430
Wrote a wonderful book on Indian
street magic called,

00:53:27.430 --> 00:53:29.340
"Net of Magic."

00:53:29.340 --> 00:53:30.990
And there's a passage in
there I love to quote.

00:53:30.990 --> 00:53:34.645
He says, I tell my friends I'm
writing a book on magic and

00:53:34.645 --> 00:53:38.420
they ask, do you mean
real magic?

00:53:38.420 --> 00:53:46.200
And by real magic they
mean magical.

00:53:46.200 --> 00:53:52.020
They mean thaumaturgical
acts and so forth.

00:53:52.020 --> 00:53:53.710
And he says, no.

00:53:53.710 --> 00:53:54.940
Not real magic.

00:53:54.940 --> 00:53:56.430
Sleight of hand.

00:53:56.430 --> 00:53:57.980
Stage magic.

00:53:57.980 --> 00:54:02.100
And then he goes on, he says,
in other words, the kind of

00:54:02.100 --> 00:54:06.390
magic you can actually
do is not real magic.

00:54:10.210 --> 00:54:13.610
Real magic is a kind
that doesn't exist.

00:54:13.610 --> 00:54:18.580
Well what I've come to realize,
my whole career, I've

00:54:18.580 --> 00:54:19.840
been confronted with people.

00:54:19.840 --> 00:54:21.860
Because what I want to deal with
are topics that a lot of

00:54:21.860 --> 00:54:23.700
people think of as real magic.

00:54:23.700 --> 00:54:26.400
Free will and consciousness
in particular.

00:54:26.400 --> 00:54:32.770
And if you don't mean something
that as one

00:54:32.770 --> 00:54:36.500
proponent of it says,
every free

00:54:36.500 --> 00:54:39.430
decision is a little miracle.

00:54:39.430 --> 00:54:41.310
If you don't think that that's
what free will is then you're

00:54:41.310 --> 00:54:43.970
not talking about free will.

00:54:43.970 --> 00:54:45.080
Not real free will.

00:54:45.080 --> 00:54:48.530
You're talking about some
cheesy substitute.

00:54:48.530 --> 00:54:51.740
And I'm all for this cheesy
substitutes, I'm afraid.

00:54:51.740 --> 00:54:53.500
I think they're quite
good enough.

00:54:53.500 --> 00:54:56.980
And science can explain why we
have varieties of free will

00:54:56.980 --> 00:55:02.650
that are worth wanting, even if
they aren't real free will.

00:55:02.650 --> 00:55:07.620
They'll secure our lives
as meaningful and

00:55:07.620 --> 00:55:09.860
responsible and so forth.

00:55:09.860 --> 00:55:13.810
And we can have consciousness
even if it isn't something

00:55:13.810 --> 00:55:17.580
that sunders the universe
in twain in this

00:55:17.580 --> 00:55:19.680
very special way.

00:55:19.680 --> 00:55:22.220
Sorry it's such a long answer,
but you got me going.

00:55:22.220 --> 00:55:23.420
AUDIENCE: What do
you think about

00:55:23.420 --> 00:55:26.950
utilitarianism and moral reasoning?

00:55:26.950 --> 00:55:29.170
DANIEL DENNETT: I'm not a
utilitarian, or at least not

00:55:29.170 --> 00:55:31.180
an ordinary utilitarian.

00:55:31.180 --> 00:55:32.870
I don't that any.

00:55:32.870 --> 00:55:35.490
I'm a consequentialist.

00:55:35.490 --> 00:55:38.840
That is to say, I think the
consequences matter.

00:55:38.840 --> 00:55:44.190
I'm not one who takes
a strictly

00:55:44.190 --> 00:55:48.100
deontological view of ethics.

00:55:48.100 --> 00:55:49.370
Using some technical terms.

00:55:49.370 --> 00:55:52.570
I want to try to get
away from them.

00:55:52.570 --> 00:55:55.060
This is an issue that's very
close to the service in work

00:55:55.060 --> 00:56:00.030
that I'm doing right now on
free will and punishment.

00:56:00.030 --> 00:56:02.830
Because I say, no, we don't.

00:56:02.830 --> 00:56:06.250
I notoriously said in my last
book on free will, I don't

00:56:06.250 --> 00:56:08.410
want to live, no in an
article on free will.

00:56:08.410 --> 00:56:12.010
I don't want to live in a world
without punishment.

00:56:12.010 --> 00:56:15.330
And these people jumped all over
me and said oh, you dirty

00:56:15.330 --> 00:56:18.320
retributivist, you.

00:56:18.320 --> 00:56:19.630
I said, no, I'm not
a retributivist.

00:56:22.840 --> 00:56:25.940
Kant was the great
retributivist.

00:56:25.940 --> 00:56:31.750
And if you want to know what
bullet he bit, he said, if we

00:56:31.750 --> 00:56:36.250
know the world was going to
end tomorrow, one of our

00:56:36.250 --> 00:56:39.250
highest duties would be to go
into the prisons and execute

00:56:39.250 --> 00:56:40.500
the people on death row.

00:56:46.010 --> 00:56:49.310
By that he wanted he wanted us
to think that punishment was

00:56:49.310 --> 00:56:50.660
in itself good.

00:56:50.660 --> 00:56:55.530
That a crime unpunished
is a an offense to

00:56:55.530 --> 00:56:57.440
nature or God or something.

00:56:57.440 --> 00:57:00.160
And that's what a retributivist
is one who

00:57:00.160 --> 00:57:04.460
thinks that punishment
is a good in itself.

00:57:04.460 --> 00:57:06.100
I don't.

00:57:06.100 --> 00:57:09.880
But I think it's a very
foundational good for a very

00:57:09.880 --> 00:57:12.350
simple reason.

00:57:12.350 --> 00:57:17.520
I want to live in a world in
which there are promises.

00:57:17.520 --> 00:57:19.950
And you can't really have
promises unless there's

00:57:19.950 --> 00:57:22.960
consequences for not keeping
your promise.

00:57:22.960 --> 00:57:26.770
And those consequences are
not rehabilitation.

00:57:26.770 --> 00:57:27.760
They're not reform.

00:57:27.760 --> 00:57:30.020
They're punishment.

00:57:30.020 --> 00:57:31.670
And it's OK.

00:57:31.670 --> 00:57:35.870
And we should all be willing to
say, no, I'd rather take my

00:57:35.870 --> 00:57:36.390
punishment.

00:57:36.390 --> 00:57:39.400
I'd rather be a free citizen
in a free society.

00:57:39.400 --> 00:57:42.870
I accept that if I do something
really wrong I am

00:57:42.870 --> 00:57:45.500
eligible for punishment
and that's OK.

00:57:45.500 --> 00:57:49.770
I think the world we live in,
when we have a system of law

00:57:49.770 --> 00:57:54.130
and order with punishment, I
think our current system of

00:57:54.130 --> 00:57:57.690
punishment is obscene and should
be completely reformed.

00:57:57.690 --> 00:58:00.390
But I don't think we should
eliminate punishment as some

00:58:00.390 --> 00:58:04.880
of the current crop of cognitive
scientists and

00:58:04.880 --> 00:58:07.280
neuroscientists are saying.

00:58:07.280 --> 00:58:10.290
I don't think they've look
closely at the last time this

00:58:10.290 --> 00:58:12.860
was considered a really
wise move.

00:58:12.860 --> 00:58:18.280
And that was back in the
Stalinist era when people

00:58:18.280 --> 00:58:22.190
weren't punished, they
were treated.

00:58:22.190 --> 00:58:24.890
I don't think we want
to be treated.

00:58:24.890 --> 00:58:28.620
I think we want to
be punished.

00:58:28.620 --> 00:58:30.350
Because then there's rules.

00:58:30.350 --> 00:58:33.010
And you can serve your
time and get out and

00:58:33.010 --> 00:58:35.570
be a citizen again.

00:58:35.570 --> 00:58:38.050
An that dire note, I'll stop.

00:58:38.050 --> 00:58:41.450
AUDIENCE: There was a decision
this week about patents that

00:58:41.450 --> 00:58:44.195
touched on some deeply
philosophical issues.

00:58:44.195 --> 00:58:47.500
The question was whether
software should be patentable.

00:58:47.500 --> 00:58:50.180
And one of the opinions, which
unfortunately, didn't gather

00:58:50.180 --> 00:58:55.120
majority support was that
basically things you do in

00:58:55.120 --> 00:58:57.230
software are like
natural laws.

00:58:57.230 --> 00:58:59.230
They're tools that everybody
can use.

00:58:59.230 --> 00:59:01.290
And some particular arrangements
with these tools

00:59:01.290 --> 00:59:03.130
should not be patentable.

00:59:03.130 --> 00:59:05.760
Other judges felt that if you
have a computer and it's

00:59:05.760 --> 00:59:08.570
programmed with software,
that's a new device.

00:59:08.570 --> 00:59:11.030
And devices are clearly
patentable.

00:59:11.030 --> 00:59:14.895
Do you have some wisdom
to offer us on that?

00:59:14.895 --> 00:59:18.700
DANIEL DENNETT: Well I thought
about it in the past.

00:59:18.700 --> 00:59:20.600
And I've thought about the
difference between patent and

00:59:20.600 --> 00:59:22.740
copyright and trade secret.

00:59:22.740 --> 00:59:29.940
And actually I'm trying to
think about this in quite

00:59:29.940 --> 00:59:30.740
fundamental terms.

00:59:30.740 --> 00:59:35.810
Because I think that what we
need to develop is the sense

00:59:35.810 --> 00:59:38.540
of information, which is
distinct from Shannon-Weaver

00:59:38.540 --> 00:59:39.230
information.

00:59:39.230 --> 00:59:41.650
Which depends on Shannon-Weaver
information,

00:59:41.650 --> 00:59:43.280
but is semantic information.

00:59:43.280 --> 00:59:45.230
It's cognitive information.

00:59:45.230 --> 00:59:50.340
And, to me, the best way of
thinking about this is that

00:59:50.340 --> 00:59:54.090
it's useful information.

00:59:54.090 --> 01:00:00.370
It is information defined as
that which permits the

01:00:00.370 --> 01:00:04.110
transmission of adaptations for
one purpose, one utility

01:00:04.110 --> 01:00:05.260
or another.

01:00:05.260 --> 01:00:10.760
And then under that view then,
of course, software, I think

01:00:10.760 --> 01:00:14.870
intuitively when we're measuring
whether software

01:00:14.870 --> 01:00:22.580
what counts as theft, we have
to look at the utility.

01:00:22.580 --> 01:00:29.920
And if it's the utility that's
been stolen, then we should

01:00:29.920 --> 01:00:32.520
have penalties for that.

01:00:32.520 --> 01:00:35.220
That should be discouraged.

01:00:35.220 --> 01:00:41.840
The trouble is that copyright
law also involves this idea.

01:00:45.750 --> 01:00:46.680
It's a good question.

01:00:46.680 --> 01:00:49.720
And I'm not surprised
that the Justice--

01:00:49.720 --> 01:00:52.800
Is this at the Supreme Court?

01:00:52.800 --> 01:00:54.730
AUDIENCE: It's the Court of
Appeals for the Federal

01:00:54.730 --> 01:00:56.250
Circuit that handles
patent cases.

01:00:56.250 --> 01:00:57.450
DANIEL DENNETT: I want to
know more about it.

01:00:57.450 --> 01:00:59.390
I'm not surprised that
they are divided.

01:00:59.390 --> 01:01:01.000
I think it's a tough issue.

01:01:01.000 --> 01:01:06.540
But obviously when I talk
about apps, as tools for

01:01:06.540 --> 01:01:10.135
thinking, I am thinking of these
as mechanisms that have

01:01:10.135 --> 01:01:10.830
been designed.

01:01:10.830 --> 01:01:15.920
And so on that ground I would
tend to come down on the side

01:01:15.920 --> 01:01:17.530
that you can patent it.

01:01:17.530 --> 01:01:17.830
Thank you.

01:01:17.830 --> 01:01:20.530
[APPLAUSE]

01:01:20.530 --> 01:01:44.348
[MUSIC PLAYING]

