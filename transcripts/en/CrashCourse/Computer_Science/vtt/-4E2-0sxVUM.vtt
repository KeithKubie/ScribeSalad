WEBVTT
Kind: captions
Language: en

00:00:03.200 --> 00:00:05.919
Hi, I’m Carrie Anne, and welcome to Crash
Course Computer Science!

00:00:05.919 --> 00:00:09.290
Today, let’s start by thinking about how
important vision can be.

00:00:09.290 --> 00:00:13.760
Most people rely on it to prepare food, walk
around obstacles, read street signs, watch

00:00:13.760 --> 00:00:16.280
videos like this, and do hundreds of other
tasks.

00:00:16.280 --> 00:00:20.170
Vision is the highest bandwidth sense, and
it provides a firehose of information about

00:00:20.170 --> 00:00:22.110
the state of the world and how to act on it.

00:00:22.110 --> 00:00:26.220
For this reason, computer scientists have
been trying to give computers vision for half

00:00:26.220 --> 00:00:29.420
a century, birthing the sub-field of computer
vision.

00:00:29.420 --> 00:00:33.559
Its goal is to give computers the ability
to extract high-level understanding from digital

00:00:33.559 --> 00:00:35.039
images and videos.

00:00:35.040 --> 00:00:39.170
As everyone with a digital camera or smartphone
knows, computers are already really good at

00:00:39.170 --> 00:00:43.740
capturing photos with incredible fidelity
and detail – much better than humans in fact.

00:00:43.820 --> 00:00:48.300
But as computer vision professor Fei-Fei Li
recently said, “Just like to hear is the

00:00:48.300 --> 00:00:49.880
not the same as to listen.

00:00:49.880 --> 00:00:52.440
To take pictures is not the same as to see.”

00:00:52.440 --> 00:01:01.320
INTRO

00:01:01.320 --> 00:01:05.980
As a refresher, images on computers are most
often stored as big grids of pixels.

00:01:05.980 --> 00:01:10.470
Each pixel is defined by a color, stored as
a combination of three additive primary colors:

00:01:10.470 --> 00:01:11.850
red, green and blue.

00:01:11.850 --> 00:01:14.490
By combining different intensities of these
three colors, what’s called a RGB value,

00:01:14.490 --> 00:01:17.720
we can represent any color.

00:01:17.720 --> 00:01:21.650
Perhaps the simplest computer vision algorithm
– and a good place to start – is to track

00:01:21.650 --> 00:01:23.930
a colored object, like a bright pink ball.

00:01:23.930 --> 00:01:26.890
The first thing we need to do is record the
ball’s color.

00:01:26.890 --> 00:01:30.090
For that, we’ll take the RGB value of the
centermost pixel.

00:01:30.090 --> 00:01:34.410
With that value saved, we can give a computer
program an image, and ask it to find the pixel

00:01:34.410 --> 00:01:35.980
with the closest color match.

00:01:35.980 --> 00:01:39.990
An algorithm like this might start in the
upper right corner, and check each pixel,

00:01:39.990 --> 00:01:43.060
one at time, calculating the difference from
our target color.

00:01:43.060 --> 00:01:47.500
Now, having looked at every pixel, the best
match is very likely a pixel from our ball.

00:01:47.500 --> 00:01:51.461
We’re not limited to running this algorithm
on a single photo; we can do it for every

00:01:51.461 --> 00:01:54.550
frame in a video, allowing us to track the
ball over time.

00:01:54.550 --> 00:01:58.110
Of course, due to variations in lighting,
shadows, and other effects, the ball on the

00:01:58.110 --> 00:02:03.080
field is almost certainly not going to be
the exact same RGB value as our target color,

00:02:03.080 --> 00:02:04.590
but merely the closest match.

00:02:04.590 --> 00:02:08.560
In more extreme cases, like at a game at night,
the tracking might be poor.

00:02:08.560 --> 00:02:12.459
And if one of the team's jerseys used the
same color as the ball, our algorithm would

00:02:12.459 --> 00:02:13.900
get totally confused.

00:02:13.900 --> 00:02:18.510
For these reasons, color marker tracking and
similar algorithms are rarely used, unless

00:02:18.510 --> 00:02:20.569
the environment can be tightly controlled.

00:02:20.569 --> 00:02:25.269
This color tracking example was able to search
pixel-by-pixel, because colors are stored

00:02:25.269 --> 00:02:27.099
inside of single pixels.

00:02:27.099 --> 00:02:31.090
But this approach doesn’t work for features
larger than a single pixel, like edges of

00:02:31.090 --> 00:02:34.239
objects, which are inherently made up of many
pixels.

00:02:34.239 --> 00:02:38.709
To identify these types of features in images,
computer vision algorithms have to consider

00:02:38.709 --> 00:02:40.930
small regions of pixels, called patches.

00:02:40.930 --> 00:02:45.370
As an example, let’s talk about an algorithm
that finds vertical edges in a scene, let’s

00:02:45.370 --> 00:02:48.360
say to help a drone navigate safely through
a field of obstacles.

00:02:48.360 --> 00:02:52.640
To keep things simple, we’re going to convert
our image into grayscale, although most algorithms

00:02:52.640 --> 00:02:53.700
can handle color.

00:02:53.700 --> 00:02:57.250
Now let’s zoom into one of these poles to
see what an edge looks like up close.

00:02:57.250 --> 00:03:01.060
We can easily see where the left edge of the
pole starts, because there’s a change in

00:03:01.060 --> 00:03:04.099
color that persists across many pixels vertically.

00:03:04.099 --> 00:03:08.250
We can define this behavior more formally
by creating a rule that says the likelihood

00:03:08.250 --> 00:03:13.120
of a pixel being a vertical edge is the magnitude
of the difference in color between some pixels

00:03:13.120 --> 00:03:15.310
to its left and some pixels to its right.

00:03:15.310 --> 00:03:19.139
The bigger the color difference between these
two sets of pixels, the more likely the pixel

00:03:19.139 --> 00:03:20.249
is on an edge.

00:03:20.249 --> 00:03:23.359
If the color difference is small, it’s probably
not an edge at all.

00:03:23.359 --> 00:03:27.170
The mathematical notation for this operation
looks like this – it’s called a kernel

00:03:27.170 --> 00:03:28.240
or filter.

00:03:28.240 --> 00:03:33.660
It contains the values for a pixel-wise multiplication, the sum of which is saved into the center pixel.

00:03:33.660 --> 00:03:36.340
Let’s see how this works for our example
pixel.

00:03:36.340 --> 00:03:39.599
I’ve gone ahead and labeled all of the pixels
with their grayscale values.

00:03:39.599 --> 00:03:43.459
Now, we take our kernel, and center it over
our pixel of interest.

00:03:43.459 --> 00:03:46.969
This specifies what each pixel value underneath
should be multiplied by.

00:03:46.969 --> 00:03:49.249
Then, we just add up all those numbers.

00:03:49.249 --> 00:03:51.999
In this example, that gives us 147.

00:03:51.999 --> 00:03:54.020
That becomes our new pixel value.

00:03:54.020 --> 00:03:58.319
This operation, of applying a kernel to a
patch of pixels, is call a convolution.

00:03:58.319 --> 00:04:00.709
Now let’s apply our kernel to another pixel.

00:04:00.709 --> 00:04:02.489
In this case, the result is 1.

00:04:02.489 --> 00:04:03.240
Just 1.

00:04:03.240 --> 00:04:06.640
In other words, it’s a very small color
difference, and not an edge.

00:04:06.640 --> 00:04:10.800
If we apply our kernel to every pixel in the
photo, the result looks like this, where the

00:04:10.800 --> 00:04:13.560
highest pixel values are where there are strong
vertical edges.

00:04:13.560 --> 00:04:18.609
Note that horizontal edges, like those platforms
in the background, are almost invisible.

00:04:18.609 --> 00:04:22.280
If we wanted to highlight those features,
we’d have to use a different kernel – one

00:04:22.280 --> 00:04:23.879
that’s sensitive to horizontal edges.

00:04:23.879 --> 00:04:29.310
Both of these edge enhancing kernels are called
Prewitt Operators, named after their inventor.

00:04:29.310 --> 00:04:33.400
These are just two examples of a huge variety
of kernels, able to perform many different

00:04:33.400 --> 00:04:34.919
image transformations.

00:04:34.919 --> 00:04:37.350
For example, here’s a kernel that sharpens
images.

00:04:37.350 --> 00:04:39.520
And here’s a kernel that blurs them.

00:04:39.520 --> 00:04:43.530
Kernels can also be used like little image
cookie cutters that match only certain shapes.

00:04:43.530 --> 00:04:48.240
So, our edge kernels looked for image patches
with strong differences from right to left

00:04:48.240 --> 00:04:49.240
or up and down.

00:04:49.240 --> 00:04:53.729
But we could also make kernels that are good
at finding lines, with edges on both sides.

00:04:53.729 --> 00:04:57.020
And even islands of pixels surrounded by contrasting
colors.

00:04:57.020 --> 00:05:00.180
These types of kernels can begin to characterize
simple shapes.

00:05:00.180 --> 00:05:04.780
For example, on faces, the bridge of the nose
tends to be brighter than the sides of the

00:05:04.780 --> 00:05:08.040
nose, resulting in higher values for line-sensitive
kernels.

00:05:08.040 --> 00:05:12.920
Eyes are also distinctive – a dark circle
sounded by lighter pixels – a pattern other

00:05:12.930 --> 00:05:14.440
kernels are sensitive to.

00:05:14.450 --> 00:05:18.621
When a computer scans through an image, most
often by sliding around a search window, it

00:05:18.621 --> 00:05:22.039
can look for combinations of features indicative
of a human face.

00:05:22.039 --> 00:05:26.960
Although each kernel is a weak face detector
by itself, combined, they can be quite accurate.

00:05:26.960 --> 00:05:30.430
It’s unlikely that a bunch of face-like
features will cluster together if they’re

00:05:30.430 --> 00:05:31.539
not a face.

00:05:31.539 --> 00:05:35.790
This was the basis of an early and influential
algorithm called Viola-Jones Face Detection.

00:05:35.790 --> 00:05:40.009
Today, the hot new algorithms on the block
are Convolutional Neural Networks.

00:05:40.009 --> 00:05:42.990
We talked about neural nets last episode,
if you need a primer.

00:05:42.990 --> 00:05:47.360
In short, an artificial neuron – which is
the building block of a neural network – takes

00:05:47.360 --> 00:05:52.229
a series of inputs, and multiplies each by
a specified weight, and then sums those values

00:05:52.229 --> 00:05:53.220
all together.

00:05:53.220 --> 00:05:56.360
This should sound vaguely familiar, because
it’s a lot like a convolution.

00:05:56.360 --> 00:06:01.460
In fact, if we pass a neuron 2D pixel data,
rather than a one-dimensional list of inputs,

00:06:01.460 --> 00:06:03.650
it’s exactly like a convolution.

00:06:03.650 --> 00:06:08.669
The input weights are equivalent to kernel
values, but unlike a predefined kernel, neural

00:06:08.669 --> 00:06:12.540
networks can learn their own useful kernels
that are able to recognize interesting features

00:06:12.540 --> 00:06:13.300
in images.

00:06:13.300 --> 00:06:17.740
Convolutional Neural Networks use banks of
these neurons to process image data, each

00:06:17.760 --> 00:06:21.889
outputting a new image, essentially digested
by different learned kernels.

00:06:21.889 --> 00:06:26.229
These outputs are then processed by subsequent
layers of neurons, allowing for convolutions

00:06:26.229 --> 00:06:28.409
on convolutions on convolutions.

00:06:28.409 --> 00:06:32.689
The very first convolutional layer might find
things like edges, as that’s what a single

00:06:32.689 --> 00:06:35.099
convolution can recognize, as we’ve already
discussed.

00:06:35.099 --> 00:06:39.580
The next layer might have neurons that convolve
on those edge features to recognize simple

00:06:39.580 --> 00:06:42.060
shapes, comprised of edges, like corners.

00:06:42.060 --> 00:06:46.960
A layer beyond that might convolve on those
corner features, and contain neurons that

00:06:46.960 --> 00:06:49.800
can recognize simple objects, like mouths
and eyebrows.

00:06:49.800 --> 00:06:54.180
And this keeps going, building up in complexity,
until there’s a layer that does a convolution

00:06:54.180 --> 00:06:58.650
that puts it together: eyes, ears, mouth,
nose, the whole nine yards, and says “ah

00:06:58.650 --> 00:06:59.840
ha, it’s a face!”

00:06:59.840 --> 00:07:04.540
Convolutional neural networks aren’t required
to be many layers deep, but they usually are,

00:07:04.550 --> 00:07:07.020
in order to recognize complex objects and
scenes.

00:07:07.020 --> 00:07:09.860
That’s why the technique is considered deep
learning.

00:07:09.860 --> 00:07:14.180
Both Viola-Jones and Convolutional Neural
Networks can be applied to many image recognition

00:07:14.180 --> 00:07:19.229
problems, beyond faces, like recognizing handwritten
text, spotting tumors in CT scans and monitoring

00:07:19.229 --> 00:07:20.520
traffic flow on roads.

00:07:20.520 --> 00:07:22.560
But we’re going to stick with faces.

00:07:22.560 --> 00:07:26.860
Regardless of what algorithm was used, once
we’ve isolated a face in a photo, we can

00:07:26.860 --> 00:07:31.919
apply more specialized computer vision algorithms
to pinpoint facial landmarks, like the tip

00:07:31.919 --> 00:07:33.970
of the nose and corners of the mouth.

00:07:33.970 --> 00:07:38.030
This data can be used for determining things
like if the eyes are open, which is pretty

00:07:38.030 --> 00:07:41.810
easy once you have the landmarks – it’s
just the distance between points.

00:07:41.810 --> 00:07:45.849
We can also track the position of the eyebrows;
their relative position to the eyes can be

00:07:45.849 --> 00:07:47.949
an indicator of surprise, or delight.

00:07:47.949 --> 00:07:52.789
Smiles are also pretty straightforward to
detect based on the shape of mouth landmarks.

00:07:52.789 --> 00:07:57.210
All of this information can be interpreted
by emotion recognition algorithms, giving

00:07:57.210 --> 00:08:02.280
computers the ability to infer when you’re
happy, sad, frustrated, confused and so on.

00:08:02.280 --> 00:08:07.490
In turn, that could allow computers to intelligently
adapt their behavior... maybe offer tips when

00:08:07.490 --> 00:08:11.110
you’re confused, and not ask to install
updates when you’re frustrated.

00:08:11.110 --> 00:08:16.169
This is just one example of how vision can
give computers the ability to be context sensitive,

00:08:16.169 --> 00:08:18.180
that is, aware of their surroundings.

00:08:18.180 --> 00:08:21.880
And not just the physical surroundings – like
if you're at work or on a train – but also

00:08:21.880 --> 00:08:26.240
your social surroundings – like if you’re
in a formal business meeting versus a friend’s

00:08:26.240 --> 00:08:27.300
birthday party.

00:08:27.420 --> 00:08:32.000
You behave differently in those surroundings, and so should computing devices, if they’re smart.

00:08:32.000 --> 00:08:36.180
Facial landmarks also capture the geometry
of your face, like the distance between your

00:08:36.180 --> 00:08:38.130
eyes and the height of your forehead.

00:08:38.130 --> 00:08:42.530
This is one form of biometric data, and it
allows computers with cameras to recognize

00:08:42.530 --> 00:08:43.200
you.

00:08:43.200 --> 00:08:47.220
Whether it’s your smartphone automatically
unlocking itself when it sees you, or governments

00:08:47.220 --> 00:08:52.510
tracking people using CCTV cameras, the applications
of face recognition seem limitless.

00:08:52.510 --> 00:08:56.690
There have also been recent breakthroughs
in landmark tracking for hands and whole bodies,

00:08:56.690 --> 00:09:00.839
giving computers the ability to interpret
a user’s body language, and what hand gestures

00:09:00.839 --> 00:09:03.570
they’re frantically waving at their internet
connected microwave.

00:09:03.570 --> 00:09:07.550
As we’ve talked about many times in this
series, abstraction is the key to building

00:09:07.550 --> 00:09:10.880
complex systems, and the same is true in computer
vision.

00:09:10.880 --> 00:09:14.610
At the hardware level, you have engineers
building better and better cameras, giving

00:09:14.610 --> 00:09:18.910
computers improved sight with each passing
year, which I can’t say for myself.

00:09:18.910 --> 00:09:23.339
Using that camera data, you have computer
vision algorithms crunching pixels to find

00:09:23.339 --> 00:09:25.149
things like faces and hands.

00:09:25.149 --> 00:09:29.120
And then, using output from those algorithms,
you have even more specialized algorithms

00:09:29.120 --> 00:09:32.930
for interpreting things like user facial expression
and hand gestures.

00:09:32.930 --> 00:09:37.040
On top of that, there are people building
novel interactive experiences, like smart

00:09:37.040 --> 00:09:41.660
TVs and intelligent tutoring systems, that
respond to hand gestures and emotion.

00:09:41.660 --> 00:09:46.080
Each of these levels are active areas of research,
with breakthroughs happening every year.

00:09:46.080 --> 00:09:47.790
And that’s just the tip of the iceberg.

00:09:47.790 --> 00:09:52.930
Today, computer vision is everywhere – whether
it’s barcodes being scanned at stores, self-driving

00:09:52.930 --> 00:09:56.820
cars waiting at red lights, or snapchat filters
superimposing mustaches.

00:09:56.820 --> 00:10:01.440
And, the most exciting thing is that computer
scientists are really just getting started,

00:10:01.440 --> 00:10:05.740
enabled by recent advances in computing, like
super fast GPUs.

00:10:05.740 --> 00:10:10.920
Computers with human-like ability to see is
going to totally change how we interact with them.

00:10:10.920 --> 00:10:14.420
Of course, it’d also be nice if they could
hear and speak, which we’ll discuss next

00:10:14.420 --> 00:10:15.000
week.

00:10:15.040 --> 00:10:15.540
I’ll see you then.

