WEBVTT
Kind: captions
Language: en

00:00:00.290 --> 00:00:02.498
&gt;&gt; Earlier I mentioned sampling and I asked you whether that

00:00:02.498 --> 00:00:05.250
sounded useful, and you said it was. So, let's do a

00:00:05.250 --> 00:00:08.760
little exercise. Why? Why [LAUGH] is that a useful thing? Why

00:00:08.760 --> 00:00:10.800
is it good idea to be able to sample from a distribution?

00:00:10.800 --> 00:00:15.310
&gt;&gt; Well, because it's one of the two things that distributions are for.

00:00:15.310 --> 00:00:16.830
&gt;&gt; What does that mean?

00:00:16.830 --> 00:00:17.775
&gt;&gt; Well so why do you have a

00:00:17.775 --> 00:00:20.940
distribution? A distribution is so that given some

00:00:20.940 --> 00:00:22.978
value, you can, you can tell me what's

00:00:22.978 --> 00:00:25.300
the probability of me seeing that value which

00:00:25.300 --> 00:00:28.460
is kind of what it looks like when you have the probability function, but

00:00:28.460 --> 00:00:29.880
also if you have a nice distribution

00:00:29.880 --> 00:00:32.980
you can generate values according to that distribution.

00:00:32.980 --> 00:00:37.930
&gt;&gt; Okay. That's a little bit circular in the sense that it didn't tell me why

00:00:37.930 --> 00:00:40.550
it was useful to generate them other than it's one of the things you can do.

00:00:40.550 --> 00:00:43.820
&gt;&gt; Well, you didn't ask me to actually make sense. But I mean, this is

00:00:43.820 --> 00:00:46.130
the, the thing that you use distributions for.

00:00:46.130 --> 00:00:47.940
Now why would you want to do that?

00:00:47.940 --> 00:00:49.280
&gt;&gt; Yeah.

00:00:49.280 --> 00:00:50.110
&gt;&gt; So,

00:00:51.150 --> 00:00:55.650
if a distribution represents kind of a process, it would

00:00:55.650 --> 00:00:59.790
be nice if I could duplicate that process, right? So, I

00:00:59.790 --> 00:01:02.190
would have to be able to generate values in the

00:01:02.190 --> 00:01:05.260
right way, consistent with the distribution in order to generate that

00:01:05.260 --> 00:01:07.781
process. So it's like flipping a coin, or I want

00:01:07.781 --> 00:01:10.500
to flip a coin and find out whether I'm going to

00:01:10.500 --> 00:01:13.230
get heads or tails. It would be nice if I can

00:01:13.230 --> 00:01:16.210
do that in a way that's consistent with whatever the underlying

00:01:16.210 --> 00:01:17.305
bias of the coin is.

00:01:17.305 --> 00:01:19.520
&gt;&gt; Okay, so yeah, if this distribution

00:01:19.520 --> 00:01:21.760
represented something complex, we might, you know, for

00:01:21.760 --> 00:01:23.920
whatever reason need to simulate that world

00:01:23.920 --> 00:01:26.498
and, and act according to those probabilities. So,

00:01:26.498 --> 00:01:30.130
yeah, that, that's a reasonable one. What else, what if, what if I showed you

00:01:30.130 --> 00:01:31.980
this, if i took this distribution that

00:01:31.980 --> 00:01:33.730
we used for the lightning and thunder example.

00:01:33.730 --> 00:01:34.373
&gt;&gt; Mm-hm.

00:01:34.373 --> 00:01:36.430
&gt;&gt; What if you wanted to get a handle on it? How can we

00:01:36.430 --> 00:01:38.110
use sampling for the distribution to give

00:01:38.110 --> 00:01:40.940
you some insight into how the storms work?

00:01:40.940 --> 00:01:41.574
&gt;&gt; Okay

00:01:41.574 --> 00:01:43.890
so let's see, I've, I've, I've got this representation

00:01:43.890 --> 00:01:46.478
of the joint distribution, but it's just a representation of

00:01:46.478 --> 00:01:48.310
the joint distribution. If I want to asked a

00:01:48.310 --> 00:01:51.860
question like, well what's the chance that it's, oh let's

00:01:51.860 --> 00:01:56.730
say, storming outside if I've heard thunder, I could

00:01:56.730 --> 00:02:00.840
go through and, and, you know, back compute the reverse

00:02:00.840 --> 00:02:03.850
of the conditional probability tables. And I could do things

00:02:03.850 --> 00:02:06.739
like, or I could just generate a bunch of samples

00:02:08.110 --> 00:02:12.250
where I had thunder and I can just see how

00:02:12.250 --> 00:02:15.030
often the storm was also true. Does that make sense?

00:02:15.030 --> 00:02:16.590
&gt;&gt; It does, though I'm not going to use

00:02:16.590 --> 00:02:18.480
the words that you just used to write that down.

00:02:18.480 --> 00:02:18.820
&gt;&gt; Okay.

00:02:18.820 --> 00:02:22.250
&gt;&gt; I'm going to call that approximate inference. So the basic idea is that

00:02:22.250 --> 00:02:24.460
you would like to do some inference, you'd like to figure out what might

00:02:24.460 --> 00:02:25.830
be true of the world in different

00:02:25.830 --> 00:02:29.110
situations. Instead of doing some complex probability

00:02:29.110 --> 00:02:30.800
calculation, you're just going to imagine a

00:02:30.800 --> 00:02:33.200
bunch of possible worlds and see how often

00:02:33.200 --> 00:02:35.320
is it the case that whatever it is you

00:02:35.320 --> 00:02:38.410
want to figure out is true. So yeah, that, that

00:02:38.410 --> 00:02:39.410
turns out to be a really good way to

00:02:39.410 --> 00:02:41.250
do it. In fact, sometimes I think that's a lot

00:02:41.250 --> 00:02:42.760
of what people are doing when we're, when we're

00:02:42.760 --> 00:02:45.290
making judgments in the world. We're just really, really good

00:02:45.290 --> 00:02:47.710
at this kind of sampling from past realities that

00:02:47.710 --> 00:02:50.770
are relevant, and we can make judgments based on that.

00:02:50.770 --> 00:02:53.180
&gt;&gt; Hm. So, how would you do that?

00:02:53.180 --> 00:02:54.790
&gt;&gt; How would I do what?

00:02:54.790 --> 00:02:56.300
&gt;&gt; How would you do this approximate inference?

00:02:56.300 --> 00:02:57.900
&gt;&gt; We're going to get to that but I wanted to.

00:02:57.900 --> 00:02:58.190
&gt;&gt; Oh, okay, cool.

00:02:58.190 --> 00:02:58.280
&gt;&gt; But

00:02:58.280 --> 00:02:59.240
there, but there's one or two other

00:02:59.240 --> 00:03:01.140
things about sampling that I wanted to mention.

00:03:01.140 --> 00:03:01.580
&gt;&gt; Okay.

00:03:01.580 --> 00:03:04.330
&gt;&gt; Another thing that I could imagine using this for

00:03:04.330 --> 00:03:07.870
is this notion of visualization. Which may be, I mean this

00:03:07.870 --> 00:03:10.910
in a, in a broader way than it sounds, not

00:03:10.910 --> 00:03:13.850
necessarily to actually see what the distribution is like, but to

00:03:13.850 --> 00:03:18.080
kind of get a feel for it. So, I bet if I was to run that if I was to draw

00:03:18.080 --> 00:03:20.750
a bunch of samples from the lightening thundering set, you

00:03:20.750 --> 00:03:23.800
would have a better feel for how likely different things are.

00:03:23.800 --> 00:03:25.990
Just you as a person might get a sense of how these

00:03:25.990 --> 00:03:30.040
things work. So, you can imagine in, in a medical domain a doctor

00:03:30.040 --> 00:03:33.080
who's, who's thinking about prescri, prescribing a particular kind of drug for a

00:03:33.080 --> 00:03:35.100
particular kind of person, if the

00:03:35.100 --> 00:03:37.970
information about drug interactions and so forth

00:03:37.970 --> 00:03:40.680
was, was represented as a big belief net, it might be hard to

00:03:40.680 --> 00:03:43.110
look at it and know anything. But if it ge, if you use

00:03:43.110 --> 00:03:45.550
that to generate a bunch of artificial patients you might start to get

00:03:45.550 --> 00:03:49.710
to feel for oh, you know what, these kinds of people tend to

00:03:49.710 --> 00:03:51.800
react badly in these kinds of circumstances.

00:03:51.800 --> 00:03:55.670
&gt;&gt; That's still a kind of approximate inference, right?

00:03:55.670 --> 00:03:58.380
&gt;&gt; That's right. So this is, this is a kind of an

00:03:58.380 --> 00:04:01.900
in the machine sense, and this is kind of in the human sense.

00:04:01.900 --> 00:04:04.190
&gt;&gt; Okay, I like that. So let's see, let's see

00:04:04.190 --> 00:04:07.000
if I, if I understand this. So the, the nice thing

00:04:07.000 --> 00:04:09.980
about the storm, the thunder, and the lightning example is that

00:04:09.980 --> 00:04:14.840
it has pedagogical value. Because it's easy for a student to

00:04:14.840 --> 00:04:16.620
look at that and go okay, I understand what's

00:04:16.620 --> 00:04:19.103
going on here. One because there's only three nodes

00:04:19.103 --> 00:04:21.708
and two arrows, and the other is because, we

00:04:21.708 --> 00:04:25.670
think we understand how storms, thunder and lightning work. Right.

00:04:25.670 --> 00:04:26.120
&gt;&gt; Yup.

00:04:26.120 --> 00:04:28.220
&gt;&gt; Or most people do. So that makes a lot of sense. Of course

00:04:28.220 --> 00:04:31.860
the downside of it is, we think we understand it. And so it's hard to

00:04:31.860 --> 00:04:34.780
see why you would need to do samples, I mean, there's just a couple of

00:04:34.780 --> 00:04:36.420
probability distributions and we kind of know

00:04:36.420 --> 00:04:39.580
what it means. But in the real world,

00:04:39.580 --> 00:04:42.490
there are perhaps hundreds and hundreds of variables

00:04:42.490 --> 00:04:46.660
with complicated relationships and conditional independencies that, that aren't

00:04:46.660 --> 00:04:50.550
necessary intuitive just by looking at the graph. And

00:04:50.550 --> 00:04:53.100
so picking one conditional probability table and looking at

00:04:53.100 --> 00:04:55.190
it isn't going to tell you much. But by

00:04:55.190 --> 00:04:59.340
sampling I get real examples that are concrete that,

00:04:59.340 --> 00:05:01.920
as a human being, I can understand without having

00:05:01.920 --> 00:05:04.740
to, you know, really glock all the 25 different

00:05:04.740 --> 00:05:07.827
conditional probability tables. Does that sound right? Is that. [CROSSTALK]

00:05:07.827 --> 00:05:08.250
&gt;&gt; Yeah, yeah.

00:05:08.250 --> 00:05:08.950
&gt;&gt; What you're trying to say?

00:05:08.950 --> 00:05:10.010
&gt;&gt; That's exactly right. Thanks.

00:05:10.010 --> 00:05:10.698
&gt;&gt; Okay.

00:05:10.698 --> 00:05:12.690
&gt;&gt; I want to draw your attention to this, this

00:05:12.690 --> 00:05:16.120
word here for a moment. This notion of approximate inference. Now

00:05:16.120 --> 00:05:19.680
generally we don't like approximations when we can do things, things

00:05:19.680 --> 00:05:22.900
exactly. So why are, why are we not doing things exactly?

00:05:22.900 --> 00:05:24.250
&gt;&gt; because it's hard.

00:05:24.250 --> 00:05:27.720
&gt;&gt; It's hard, that's exactly right. So or,

00:05:27.720 --> 00:05:29.700
or, even if it weren't hard, it may,

00:05:29.700 --> 00:05:32.782
it may be in some cases faster. So I would be,

00:05:32.782 --> 00:05:35.076
I'm not going to do it now, but I'd be happy

00:05:35.076 --> 00:05:37.928
if I guess if there's ground swell of support among the

00:05:37.928 --> 00:05:41.600
students. To I can go through the argument as to why

00:05:41.600 --> 00:05:45.390
this inference is hard. There's a nice little reduction to problems,

00:05:45.390 --> 00:05:48.520
N, NP complete problems like satisfiability. But it turns out roughly

00:05:48.520 --> 00:05:51.280
that if you could do inference exactly on any belief net

00:05:51.280 --> 00:05:54.760
that you want, then you could solve very, very hard problems efficiently

00:05:54.760 --> 00:05:58.960
using that idea. So it's, it's cute, but it's kind of takes us

00:05:58.960 --> 00:06:01.520
a little bit off our path, so I'm not going to get into that.

00:06:01.520 --> 00:06:05.850
&gt;&gt; Okay, so sampling is useful, Michael, which I always suspected in my

00:06:05.850 --> 00:06:09.120
heart, and now we've got some good arguments for why it actually is.

