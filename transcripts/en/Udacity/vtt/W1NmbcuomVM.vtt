WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:02.950
In architecting the data store and the computational

00:00:02.950 --> 00:00:06.670
resources that back the data store at a server,

00:00:06.670 --> 00:00:09.300
the system administrator has a choice of either

00:00:09.300 --> 00:00:12.930
replicating or partitioning the data. If you replicating the

00:00:12.930 --> 00:00:15.710
data what that means is. That every data

00:00:15.710 --> 00:00:19.090
server has the full corpus of data that is

00:00:19.090 --> 00:00:22.030
needed to serve a particular request. Think of it

00:00:22.030 --> 00:00:25.740
as say, gmail. If it is gmail, then if

00:00:25.740 --> 00:00:28.010
a gmail request gets sent to anyone of

00:00:28.010 --> 00:00:30.920
these servers. They can handle the request that comes

00:00:30.920 --> 00:00:33.510
in because they have the full corpus of data

00:00:33.510 --> 00:00:36.640
to deal with a incoming request. We mentioned that

00:00:36.640 --> 00:00:39.940
failures are inevitable in giant scale services because

00:00:39.940 --> 00:00:44.550
of the number of computation elements that live within

00:00:44.550 --> 00:00:47.790
a data center. If failures occur, what happens when

00:00:47.790 --> 00:00:51.430
we have replicated data? What they means is that

00:00:51.430 --> 00:00:54.450
the harvest is going to be unchanged because if

00:00:54.450 --> 00:00:57.910
some data repository is failed. Because it is

00:00:57.910 --> 00:01:01.520
replicated, we can redirect a client's request to

00:01:01.520 --> 00:01:04.640
another live server that has full access to

00:01:04.640 --> 00:01:07.890
the later repository and therefore the harvest that

00:01:07.890 --> 00:01:10.450
you're going to get with replication in 100%, it

00:01:10.450 --> 00:01:13.770
is unaffected. On the other hand because the

00:01:13.770 --> 00:01:16.400
server capacity has come down due to these failures,

00:01:16.400 --> 00:01:18.440
the heal is going to come down. In other

00:01:18.440 --> 00:01:22.550
words, with replication. The service is unavailable for

00:01:22.550 --> 00:01:27.370
some users for some amount of time. But all the users that are able to get the

00:01:27.370 --> 00:01:30.420
service, get complete harvest in term of the

00:01:30.420 --> 00:01:34.810
fidelity for the query that is returned and answered

00:01:34.810 --> 00:01:37.570
by the server. The other alternative of course

00:01:37.570 --> 00:01:41.780
for architecting the internal data depository of the server

00:01:41.780 --> 00:01:43.770
is to partition the data. So if you

00:01:43.770 --> 00:01:46.370
partition the data, and let's say that There are

00:01:46.370 --> 00:01:49.110
end partitions of the full compass of data. And

00:01:49.110 --> 00:01:53.880
let's say that some of the service fail, then

00:01:53.880 --> 00:01:57.570
some portion of the corpus of data becomes unavailable.

00:01:57.570 --> 00:02:00.860
If this happens, what it means is that the

00:02:00.860 --> 00:02:03.400
harvest is going to be done because some portion of

00:02:03.400 --> 00:02:07.270
the data corpus. Is unavailable, and therefore the harvest

00:02:07.270 --> 00:02:12.050
is going to come down, but it is possible to keep Q unchanged, so long as the

00:02:12.050 --> 00:02:15.750
computation capacity is unchanged. Then we can serve

00:02:15.750 --> 00:02:19.560
the same number of user community. It's just that

00:02:19.560 --> 00:02:25.290
the fidelity of the result may not be as good as when All of the data corpuses

00:02:25.290 --> 00:02:28.520
available. For example, if this is a server

00:02:28.520 --> 00:02:32.660
that is serving search results and if some portion

00:02:32.660 --> 00:02:35.670
of the data repository is unavailable. In

00:02:35.670 --> 00:02:39.120
that case, each query that comes in will

00:02:39.120 --> 00:02:42.800
get service, but it is going to get. A

00:02:42.800 --> 00:02:45.280
subset of the query results for the search

00:02:45.280 --> 00:02:48.060
that they are doing. So the important message

00:02:48.060 --> 00:02:50.230
that I want to convey through these pictures

00:02:50.230 --> 00:02:54.680
is that dq is independent of whether we

00:02:54.680 --> 00:02:58.000
are replicating or we are partitioning the data.

00:02:58.000 --> 00:03:01.180
Given a certain server capacity, The D Qs

00:03:01.180 --> 00:03:03.840
are constant, because we are assuming that this

00:03:03.840 --> 00:03:06.682
is cheap. This space is cheap and therefore

00:03:06.682 --> 00:03:09.856
processing incoming requests are not disc bound, but

00:03:09.856 --> 00:03:13.770
it is network bound. The only exception is

00:03:13.770 --> 00:03:17.190
if the queries Result in significant in right

00:03:17.190 --> 00:03:20.100
traffic to the disk. Now this is rare

00:03:20.100 --> 00:03:22.940
in giant scale services. But if that happens,

00:03:22.940 --> 00:03:28.780
then replication may require more DQ than partitioning. But as a first order of

00:03:28.780 --> 00:03:32.110
approximation, so long as we assume that

00:03:32.110 --> 00:03:37.060
giant scale services are serving client requests which

00:03:37.060 --> 00:03:42.530
are network bound, The DQ is independent of the strategy that is used by the

00:03:42.530 --> 00:03:45.190
system administrator for managing the data whether

00:03:45.190 --> 00:03:48.210
it is replication or partitioning. Now what this

00:03:48.210 --> 00:03:51.000
also means is that this is giving

00:03:51.000 --> 00:03:54.470
an object lesson to the system administrator. In

00:03:54.470 --> 00:03:57.720
saying that beyond a certain point, you

00:03:57.720 --> 00:04:02.210
probably want to replicate and partition. Why? Because

00:04:02.210 --> 00:04:04.850
failures are bound to happen, and a

00:04:04.850 --> 00:04:07.970
failures are bound to happen if you partition

00:04:07.970 --> 00:04:10.540
the data, then a portion of the

00:04:10.540 --> 00:04:13.890
data corpus becomes unavailable. On the other hand,

00:04:13.890 --> 00:04:17.149
if beyond a certain point Even if

00:04:17.149 --> 00:04:19.470
you have partitioned data, if you replicate it,

00:04:19.470 --> 00:04:25.350
then you're making sure that, even if one replica is down, some of the replica,

00:04:25.350 --> 00:04:28.580
of the same partition is available for

00:04:28.580 --> 00:04:32.260
serving a particular client request. And replication, beyond

00:04:32.260 --> 00:04:35.620
a certain point is important because Users would

00:04:35.620 --> 00:04:39.570
normally prefer complete data or a complete harvest.

00:04:39.570 --> 00:04:42.880
Take for instance, you're accessing your email through

00:04:42.880 --> 00:04:46.250
the Internet. You would rather put up with

00:04:46.250 --> 00:04:48.780
a service being unavailable for some amount of

00:04:48.780 --> 00:04:53.180
time rather than seeing that you only have access

00:04:53.180 --> 00:04:55.700
to a portion of your main box. And

00:04:55.700 --> 00:04:58.420
that's the reason why replication beyond a certain

00:04:58.420 --> 00:05:01.580
point is better for dealing with giant scale

00:05:01.580 --> 00:05:04.650
services. On the other hand, for searches, it may

00:05:04.650 --> 00:05:07.190
be okay to have a harvest which

00:05:07.190 --> 00:05:11.350
is not complete. So in structuring different services

00:05:11.350 --> 00:05:17.650
as a giant scale service, one has to make a decision. Of whether partitioning is

00:05:17.650 --> 00:05:22.320
the right approach or at what point we may want to partition and then replicate

00:05:22.320 --> 00:05:25.800
as well. As a concrete example, Inktomi which

00:05:25.800 --> 00:05:29.680
is a CDN server uses partial replication It

00:05:29.680 --> 00:05:35.070
uses full replication for email because of what I just said, and that is

00:05:35.070 --> 00:05:40.230
a user would expect complete harvest for a service like email.

00:05:40.230 --> 00:05:42.960
But on the other hand, if it is a web cache that is

00:05:42.960 --> 00:05:48.740
being implemented as giant-scale service, it is okay if it is not replicated.

