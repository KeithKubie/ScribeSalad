WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.360
[MUSIC PLAYING]

00:00:05.815 --> 00:00:06.940
JOSH GORDON: Hey, everyone.

00:00:06.940 --> 00:00:08.129
Welcome back.

00:00:08.129 --> 00:00:10.170
Features are the way you
represent your knowledge

00:00:10.170 --> 00:00:12.320
about the world
for the classifier,

00:00:12.320 --> 00:00:14.130
and today I'll walk
you through techniques

00:00:14.130 --> 00:00:16.740
you can use to represent
your features and utilities

00:00:16.740 --> 00:00:19.170
TensorFlow provides to help.

00:00:19.170 --> 00:00:22.320
You use a dataset from the
US census as an example,

00:00:22.320 --> 00:00:24.480
and the goal is to predict
if someone's income is

00:00:24.480 --> 00:00:27.960
greater than $50,000 based
on attributes like their age

00:00:27.960 --> 00:00:29.760
and occupation.

00:00:29.760 --> 00:00:32.310
The dataset is
stored as a CSV file,

00:00:32.310 --> 00:00:35.580
and previously we've seen how to
use the column values directly

00:00:35.580 --> 00:00:37.230
as features.

00:00:37.230 --> 00:00:38.880
But today we'll use
feature engineering

00:00:38.880 --> 00:00:42.360
to transform them into a
more useful representation.

00:00:42.360 --> 00:00:45.210
As we go, I'll visualize
what these transformations do

00:00:45.210 --> 00:00:48.180
using a tool called Facets,
and you can find a link to it

00:00:48.180 --> 00:00:49.800
in the description.

00:00:49.800 --> 00:00:52.740
You'll also find complete code
to train a TensorFlow estimator

00:00:52.740 --> 00:00:54.420
on this dataset.

00:00:54.420 --> 00:00:55.410
OK, let's get started.

00:00:57.960 --> 00:01:00.230
Let's begin with a numeric
attribute like age,

00:01:00.230 --> 00:01:03.230
and think about how we can
use it to predict income.

00:01:03.230 --> 00:01:05.840
Now if you think about how
age correlates with income,

00:01:05.840 --> 00:01:08.300
our first intuition is
that as age increases,

00:01:08.300 --> 00:01:10.400
usually so does income.

00:01:10.400 --> 00:01:12.320
And the simplest way
to represent this

00:01:12.320 --> 00:01:14.510
would just be to take
the raw numeric value

00:01:14.510 --> 00:01:16.197
and use that as a feature.

00:01:16.197 --> 00:01:17.780
Here we're building
a list of features

00:01:17.780 --> 00:01:20.360
we use to train the
model, and each of these

00:01:20.360 --> 00:01:22.340
is stored as a feature column.

00:01:22.340 --> 00:01:25.340
This contains data about
the column from the CSV file

00:01:25.340 --> 00:01:27.160
and how to represent it.

00:01:27.160 --> 00:01:30.620
Here we'll write a feature that
just uses the raw value of age,

00:01:30.620 --> 00:01:34.100
and this string corresponds
to a column in the CSV file.

00:01:34.100 --> 00:01:36.442
Now what can go wrong
with this approach?

00:01:36.442 --> 00:01:38.150
Well, if we think more
closely about age,

00:01:38.150 --> 00:01:41.630
we realize it's not in a linear
relationship with income.

00:01:41.630 --> 00:01:43.980
The curve might look
something like this.

00:01:43.980 --> 00:01:47.000
It's flat for children, then
increases during working age,

00:01:47.000 --> 00:01:49.220
and decreases during retirement.

00:01:49.220 --> 00:01:50.810
A linear classifier,
for example,

00:01:50.810 --> 00:01:53.480
is unable to capture
this relationship.

00:01:53.480 --> 00:01:55.940
That's because it learns a
single weight for each feature.

00:01:58.820 --> 00:02:01.550
To make it easier for the
classifier, one thing we can do

00:02:01.550 --> 00:02:03.200
is bucket the feature.

00:02:03.200 --> 00:02:05.510
And bucketing transforms
a numeric feature

00:02:05.510 --> 00:02:07.700
into several
categorical ones based

00:02:07.700 --> 00:02:11.150
on the range it falls into,
and each of these new features

00:02:11.150 --> 00:02:14.940
indicate whether a person's
age falls into that range.

00:02:14.940 --> 00:02:17.480
And now a linear model can
capture the relationship

00:02:17.480 --> 00:02:20.080
by learning different
weights for each bucket.

00:02:20.080 --> 00:02:22.430
Let's see how this
looks in Facets.

00:02:22.430 --> 00:02:23.870
Conveniently,
there's a live demo

00:02:23.870 --> 00:02:27.350
that runs in the browser with
our census data preloaded,

00:02:27.350 --> 00:02:30.200
and each individual from
the CSV is visualized

00:02:30.200 --> 00:02:32.720
as a dot colored by income.

00:02:32.720 --> 00:02:36.550
If you click on a dot, you can
see stats about the person.

00:02:36.550 --> 00:02:38.690
Now let's bucket
by age, and you can

00:02:38.690 --> 00:02:41.900
adjust the number of buckets to
make it more or less granular.

00:02:41.900 --> 00:02:44.240
How you choose the number
of buckets is up to you,

00:02:44.240 --> 00:02:46.760
and ideally, you'd want to use
your knowledge of the problem

00:02:46.760 --> 00:02:48.560
to do this well.

00:02:48.560 --> 00:02:50.840
In TensorFlow, we can
create a bucketized feature

00:02:50.840 --> 00:02:53.720
by wrapping a numeric
column from the CSV.

00:02:53.720 --> 00:02:55.340
And here we're
specifying the number

00:02:55.340 --> 00:02:58.220
and the ranges of the
buckets we'd like created.

00:02:58.220 --> 00:03:00.440
Once this is done, we can
add the bucketized feature

00:03:00.440 --> 00:03:02.030
to the list used
to train our model.

00:03:05.200 --> 00:03:07.800
Now let's see how to represent
a categorical feature,

00:03:07.800 --> 00:03:10.740
and I'll use the education
column as an example.

00:03:10.740 --> 00:03:12.600
Because there are
only a few values,

00:03:12.600 --> 00:03:16.207
the best way to represent this
is just use the raw value.

00:03:16.207 --> 00:03:17.790
And here we'll create
a feature column

00:03:17.790 --> 00:03:21.420
that says education can be a
single value from this list.

00:03:21.420 --> 00:03:24.360
Of course, you could also read
the values from a file on disk

00:03:24.360 --> 00:03:27.030
rather than writing
them out in code.

00:03:27.030 --> 00:03:28.950
Now using the raw value
is the right thing

00:03:28.950 --> 00:03:32.164
to do when there are only a
small number of possibilities.

00:03:32.164 --> 00:03:34.080
We'll cover the case
where there are thousands

00:03:34.080 --> 00:03:35.716
of possibilities in a moment.

00:03:35.716 --> 00:03:37.590
First, let's take a look
at feature crossing.

00:03:40.372 --> 00:03:42.580
Feature crossing is a way
to create new features that

00:03:42.580 --> 00:03:44.770
are combinations
of existing ones,

00:03:44.770 --> 00:03:47.780
and these can be especially
helpful to linear classifiers,

00:03:47.780 --> 00:03:51.040
which can't model
interactions between features.

00:03:51.040 --> 00:03:53.410
Here's what this
looks like in Facets.

00:03:53.410 --> 00:03:55.150
I'll take our age
buckets from before

00:03:55.150 --> 00:03:57.400
and cross them with education.

00:03:57.400 --> 00:04:00.070
Under the hood, you can think
of a true-false feature being

00:04:00.070 --> 00:04:02.080
created for each
bucket that tells

00:04:02.080 --> 00:04:04.060
the classifier whether
an individual falls

00:04:04.060 --> 00:04:05.890
into that range.

00:04:05.890 --> 00:04:07.690
Now these buckets
can be informative,

00:04:07.690 --> 00:04:10.660
and here we see some groups are
likely to have a high income,

00:04:10.660 --> 00:04:12.220
and others low.

00:04:12.220 --> 00:04:15.850
In code, using a feature cross
works the same way as before.

00:04:15.850 --> 00:04:17.860
We'll cross our age
buckets with education

00:04:17.860 --> 00:04:20.300
and add it to the list
of features to use.

00:04:20.300 --> 00:04:23.345
A feature cross can generate
many possibilities quickly,

00:04:23.345 --> 00:04:24.970
which is why they
are often represented

00:04:24.970 --> 00:04:26.140
under the hood with a hash.

00:04:29.044 --> 00:04:32.440
A hashed feature column is one
way to efficiently represent

00:04:32.440 --> 00:04:35.380
a categorical feature
with a large vocabulary.

00:04:35.380 --> 00:04:37.000
More importantly,
you can use these

00:04:37.000 --> 00:04:38.440
as a way to make
your data easier

00:04:38.440 --> 00:04:40.750
to work with because
they free you from having

00:04:40.750 --> 00:04:43.300
to provide a vocabulary list.

00:04:43.300 --> 00:04:45.760
In this example, we'll
represent the occupation column

00:04:45.760 --> 00:04:48.190
from our CSV file
by using a hash

00:04:48.190 --> 00:04:50.570
with 1,000 possible values.

00:04:50.570 --> 00:04:53.380
Notice we don't have to
provide a vocabulary list,

00:04:53.380 --> 00:04:55.570
and to avoid collisions,
I've set the hash size

00:04:55.570 --> 00:04:58.840
so it's larger than the number
of items in the vocabulary.

00:04:58.840 --> 00:05:01.060
Here's how this
works under the hood.

00:05:01.060 --> 00:05:03.130
Normally, a categorical
feature is represented

00:05:03.130 --> 00:05:04.840
as a one hot encoding.

00:05:04.840 --> 00:05:07.240
That means there's one bit
for each possible value

00:05:07.240 --> 00:05:08.620
in the vocabulary.

00:05:08.620 --> 00:05:11.110
And we can create a lookup
because we know the vocabulary

00:05:11.110 --> 00:05:12.790
list in advance.

00:05:12.790 --> 00:05:14.500
Now if we don't
know the vocab, we

00:05:14.500 --> 00:05:17.680
can use a hash function to
compute the bit automatically.

00:05:17.680 --> 00:05:19.810
The downside is there
could be collisions,

00:05:19.810 --> 00:05:23.020
meaning different items are
mapped to the same value.

00:05:23.020 --> 00:05:25.630
Hashes can also be used
to limit memory usage

00:05:25.630 --> 00:05:28.540
at the cost of adding some
noise to your training data.

00:05:28.540 --> 00:05:30.247
If you have a large
vocabulary, it

00:05:30.247 --> 00:05:32.080
can be memory intensive
to use that as input

00:05:32.080 --> 00:05:33.325
to a neural network.

00:05:33.325 --> 00:05:35.380
A hashed column can
be used to limit

00:05:35.380 --> 00:05:37.540
the maximum number
of possibilities,

00:05:37.540 --> 00:05:39.160
but I prefer them
simply as a tool

00:05:39.160 --> 00:05:40.450
to save you programming time.

00:05:43.370 --> 00:05:45.290
Finally, I'd like to
mention embeddings,

00:05:45.290 --> 00:05:47.780
and these can be less intuitive
than the other techniques,

00:05:47.780 --> 00:05:50.113
but they're a powerful way
to work with categorical data

00:05:50.113 --> 00:05:51.950
in a deep learning setting.

00:05:51.950 --> 00:05:54.620
You can think of an embedding
as a vector that represents

00:05:54.620 --> 00:05:56.270
the meaning of a word.

00:05:56.270 --> 00:05:58.490
And we can visualize a
dataset of word embeddings

00:05:58.490 --> 00:06:00.830
using the TensorFlow
Embedding Projector,

00:06:00.830 --> 00:06:04.070
and there's an online demo you
can find in the description.

00:06:04.070 --> 00:06:07.340
Here we're looking at a dataset
of 10,000 words, each of which

00:06:07.340 --> 00:06:09.830
is represented by a vector
with many dimensions,

00:06:09.830 --> 00:06:12.440
projected down to 3D
so we can see them.

00:06:12.440 --> 00:06:14.612
You can search for words
in the box to the right.

00:06:14.612 --> 00:06:16.070
And if you experiment
a bit, you'll

00:06:16.070 --> 00:06:18.650
find similar words are
often close together.

00:06:18.650 --> 00:06:21.827
For example, all of the words
in this cluster are cities.

00:06:21.827 --> 00:06:23.660
What's neat about
embeddings is that they're

00:06:23.660 --> 00:06:26.900
learned automatically in the
process of training a DNN.

00:06:26.900 --> 00:06:28.700
And to make that happen,
all you need to do

00:06:28.700 --> 00:06:30.587
is write an embedding column.

00:06:30.587 --> 00:06:32.420
Here we'll create an
embedding for education

00:06:32.420 --> 00:06:34.852
with 10 dimensions.

00:06:34.852 --> 00:06:37.310
Now embeddings are helpful if
you have a categorical column

00:06:37.310 --> 00:06:39.140
with a large
vocabulary and you want

00:06:39.140 --> 00:06:42.020
to compress the representation
so the classifier learns

00:06:42.020 --> 00:06:44.420
general concepts
rather than memorizing

00:06:44.420 --> 00:06:46.850
the meaning of specific words.

00:06:46.850 --> 00:06:48.620
For example, imagine
if the census data

00:06:48.620 --> 00:06:50.840
had a column called job title.

00:06:50.840 --> 00:06:52.710
There are thousands
of different jobs,

00:06:52.710 --> 00:06:55.310
and an embedding could be used
to help your classifier learn

00:06:55.310 --> 00:06:57.980
that words like programmer
and software engineer

00:06:57.980 --> 00:06:59.150
often mean the same thing.

00:07:01.930 --> 00:07:04.090
OK, hope this was
a helpful intro,

00:07:04.090 --> 00:07:06.210
and thinking about how to
represent your features

00:07:06.210 --> 00:07:07.960
is one of the most
important contributions

00:07:07.960 --> 00:07:10.397
you can make to a machine
learning experiment.

00:07:10.397 --> 00:07:11.980
Feature columns are
great because they

00:07:11.980 --> 00:07:14.021
let you experiment with
different representations

00:07:14.021 --> 00:07:16.260
in code and make advanced
features like embeddings

00:07:16.260 --> 00:07:17.590
accessible.

00:07:17.590 --> 00:07:19.090
As a next step,
I'd recommend you

00:07:19.090 --> 00:07:21.940
try the code in the description
and see if you can modify it

00:07:21.940 --> 00:07:23.950
for a problem you care about.

00:07:23.950 --> 00:07:26.770
Thanks for watching everyone,
and I'll see you next time.

00:07:26.770 --> 00:07:30.120
[MUSIC PLAYING]

