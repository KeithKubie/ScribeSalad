WEBVTT
Kind: captions
Language: en

00:00:03.120 --> 00:00:06.260
Hi, I’m Adriene Hill, and Welcome back to
Crash Course, Statistics.

00:00:06.260 --> 00:00:10.410
We’ve been talking a lot about how to tell
whether two groups are different like whether

00:00:10.410 --> 00:00:13.549
there’s more car accidents on rainy days
than snowy days.

00:00:13.549 --> 00:00:17.860
or whether the IQ of university students is
actually different from the population.

00:00:17.860 --> 00:00:22.290
Today, we’re going to start a conversation
about statistical inference, which tells us

00:00:22.290 --> 00:00:28.260
how we can go from describing data we already have to making inferences about data we don’t have.

00:00:28.260 --> 00:00:37.620
INTRO

00:00:37.630 --> 00:00:41.120
If you’ve watched any of the other videos
in this series, you’ve heard a lot about

00:00:41.120 --> 00:00:42.120
uncertainty.

00:00:42.120 --> 00:00:44.460
It comes up endlessly in statistics.

00:00:44.460 --> 00:00:49.500
And uncertainty is at the core of what Inferential
Statistics is about: making decisions about

00:00:49.500 --> 00:00:50.840
ideas, or hypotheses.

00:00:50.840 --> 00:00:54.990
I might be interested in whether listening
to Mozart while doing calculus homework improves

00:00:54.990 --> 00:00:56.710
my calculus grades.

00:00:56.710 --> 00:01:00.680
But I need to test my hypothesis, I can’t
just have an idea and claim it’s correct

00:01:00.680 --> 00:01:02.090
without any evidence.

00:01:02.090 --> 00:01:03.780
One thing we need for sure, is data.

00:01:03.780 --> 00:01:07.770
So we could randomly sample two groups of
25 people and make half of them listen to

00:01:07.770 --> 00:01:10.829
Mozart and half to do their homework in silence.

00:01:10.829 --> 00:01:15.360
We collect their calculus grades and see that
those who listened to Mozart scored on average

00:01:15.360 --> 00:01:17.820
3 points higher than those who didn’t.

00:01:17.820 --> 00:01:19.450
So Mozart’s good.

00:01:19.450 --> 00:01:21.480
Problem solved, break out Sonatas, right?

00:01:21.480 --> 00:01:22.760
Unfortunately, no.

00:01:22.770 --> 00:01:26.520
We’ve seen that sample parameters like the
mean are just estimates of the mean of the

00:01:26.520 --> 00:01:28.310
population that they are taken from.

00:01:28.310 --> 00:01:31.990
The sample mean score of the Mozart group
is higher.

00:01:31.990 --> 00:01:36.439
But we don’t have sufficient evidence that
the population mean of Mozart listeners is

00:01:36.440 --> 00:01:38.960
higher than those who did their work in silence.

00:01:38.960 --> 00:01:43.700
We may have gotten an especially high sample mean that isn’t close to the true population mean.

00:01:43.700 --> 00:01:49.240
So we need a way to test our hypothesis while
taking into account the random variation of

00:01:49.240 --> 00:01:50.160
sample means.

00:01:50.160 --> 00:01:54.800
In theory, one way you could test a hypothesis
or model is by how well it predicts the data

00:01:54.820 --> 00:01:55.819
you got.

00:01:55.819 --> 00:01:59.340
For example, you and your best friend really
love giraffes, and you’ve spent a lot of

00:01:59.340 --> 00:02:02.399
time watching them at the zoo and drawing
sketches of them.

00:02:02.400 --> 00:02:07.840
So you both have a hypothesis about the average
number of spots a baby giraffe has, but they’re

00:02:07.840 --> 00:02:08.840
slightly different.

00:02:08.860 --> 00:02:13.460
You think that baby giraffes have an average
of 175 spots, with a standard deviation of

00:02:13.460 --> 00:02:18.740
50 spots, and your best friend thinks that
baby giraffes have an average of 209 spots

00:02:18.750 --> 00:02:21.740
with a standard deviation of 45 spots.

00:02:21.740 --> 00:02:25.700
With the permission of your local zoo, of
course, you begin to collect a random sample

00:02:25.700 --> 00:02:28.900
of baby giraffes and count how many spots
they had.

00:02:28.900 --> 00:02:33.250
Your sample of 25 baby giraffes had a mean
of 200 spots.

00:02:33.250 --> 00:02:37.580
Now that you have data, you can use it to
evaluate which one of you is more likely to

00:02:37.580 --> 00:02:38.580
be right.

00:02:38.580 --> 00:02:43.100
Both you and your friend have a model or idea
about what the population distribution of

00:02:43.100 --> 00:02:45.570
baby giraffe spots is.

00:02:45.570 --> 00:02:49.710
If you’re right, then the sampling distribution
of all the possible sample means we could

00:02:49.710 --> 00:02:51.100
get looks like this: (RED in chart)

00:02:51.100 --> 00:02:54.740
And the distribution of sample means for your friend’s model looks like this: (black in chart)

00:02:54.740 --> 00:02:59.840
Let’s look at where our sample mean of 200
lies on both of these distributions.

00:02:59.840 --> 00:03:04.580
You can see that you’re more likely to see
a mean of 200 spots under your friend’s

00:03:04.580 --> 00:03:05.850
hypothesis than yours.

00:03:05.850 --> 00:03:11.630
If your model were correct, a mean of 200
spots is pretty rare...it’s in the top 1.2%

00:03:11.630 --> 00:03:16.010
most extreme values we’d expect to see,
whereas in your friend’s model, a mean of

00:03:16.010 --> 00:03:21.270
200 spots is only in the top 32%, which means
it’s pretty common that we’d see sample

00:03:21.270 --> 00:03:24.870
means around 200 if your friend’s model
was correct.

00:03:24.870 --> 00:03:29.190
But we don’t always have predictions that
are as specific as you and your friend’s

00:03:29.190 --> 00:03:31.480
predictions about baby giraffe spots.

00:03:31.480 --> 00:03:35.950
We might have a more general hypothesis, like
that the average number of baby giraffe spots

00:03:35.950 --> 00:03:39.140
is more than 200... but that’s all that
you really know.

00:03:39.140 --> 00:03:45.040
In situations like these, one common method of testing ideas is Null Hypothesis Significance Testing (NHST)

00:03:45.060 --> 00:03:46.680
You have a hypothesis.

00:03:46.680 --> 00:03:50.340
That people with a certain gene, we’ll call
it gene X, eat a different amount of calories

00:03:50.350 --> 00:03:52.600
than the general population.

00:03:52.600 --> 00:03:58.040
Null Hypothesis Significance testing asks
you to test a different hypothesis--which

00:03:58.040 --> 00:04:00.710
says there is no difference or effect of this
gene.

00:04:00.710 --> 00:04:04.800
And we’ll see how well this null hypothesis
predicts the data we’ve collected.

00:04:04.800 --> 00:04:09.760
In this case the null hypothesis--or null
model-- is that the population mean caloric

00:04:09.760 --> 00:04:16.910
intake for people with gene X is actually
2,300, the same as the regular population.

00:04:16.910 --> 00:04:21.310
If the null hypothesis is found to be infeasible,
we can “reject” it.

00:04:21.310 --> 00:04:23.719
We can represent this hypothesis like this:

00:04:23.719 --> 00:04:27.169
This might seem like a pretty round about
way to test your theory that people with gene

00:04:27.169 --> 00:04:29.870
X eat differently, and that’s because it
is.

00:04:29.870 --> 00:04:34.370
Null Hypothesis Significance testing is a
form of the reductio ad absurdum argument

00:04:34.370 --> 00:04:38.950
which tries to discredit an idea by assuming
the idea is true, and then showing that if

00:04:38.950 --> 00:04:41.629
you make that assumption, something contradictory
happens.

00:04:41.629 --> 00:04:46.110
For example, you can use reductio ad absurdum
to show that there is no largest positive

00:04:46.110 --> 00:04:47.110
integer.

00:04:47.110 --> 00:04:49.580
Let’s assume there is a largest positive
integer.

00:04:49.580 --> 00:04:52.190
We’ll call it AB for “absurdly big”.

00:04:52.190 --> 00:04:54.250
Now add one to AB.

00:04:54.250 --> 00:04:55.250
shoot.

00:04:55.250 --> 00:05:00.180
That would be a larger positive integer...which
would be absurd since AB is the largest.

00:05:00.180 --> 00:05:04.319
Therefore, by reductio ad absurdum, there
is no largest positive integer.

00:05:04.319 --> 00:05:09.289
By the way, if this kind of argument sounds
familiar, it might because reductio ad absurdum

00:05:09.289 --> 00:05:11.300
is like proof by contradiction.

00:05:11.300 --> 00:05:14.610
Let’s test the null hypothesis for our our
gene X case.

00:05:14.610 --> 00:05:20.270
First, we assume that the mean number of calories
eaten by people with gene X is 2,300, just

00:05:20.270 --> 00:05:22.080
like the regular population.

00:05:22.080 --> 00:05:26.349
If we can show that this assumption makes
something “absurd” happen, then we can

00:05:26.349 --> 00:05:28.370
“reject” the idea that it’s true.

00:05:28.370 --> 00:05:32.860
With data from 60 people with gene X, we see
that the mean number of calories eaten was

00:05:32.860 --> 00:05:37.780
2,400 with a sample standard deviation of
500 calories.

00:05:37.780 --> 00:05:42.749
We have to ask how rare or “absurd” it
would be to get a sample mean that is this

00:05:42.749 --> 00:05:45.580
far away from our assumed mean of 2,300.

00:05:45.580 --> 00:05:50.360
Essentially, we imagine that we take a random
sample of 60 people with gene X over and over

00:05:50.360 --> 00:05:52.909
and over again and calculate the mean.

00:05:52.909 --> 00:05:57.759
Then we ask how many times out of all those
experiments, do we get a sample mean that’s

00:05:57.759 --> 00:06:02.930
as far away from 2,300 as our actual sample
mean of 2,400 is.

00:06:02.930 --> 00:06:07.469
Even if you haven’t heard of the term null
hypothesis significance testing, you may have

00:06:07.469 --> 00:06:10.889
heard of p-values which have been covered
everywhere from academic journals, to Buzzfeed

00:06:10.889 --> 00:06:11.889
articles.

00:06:11.889 --> 00:06:17.699
A p-value answers the question of how “rare”
your data is by telling you the probability

00:06:17.699 --> 00:06:23.129
of getting data that’s as extreme as the
data you observed if the null hypothesis was

00:06:23.129 --> 00:06:23.620
true.

00:06:23.620 --> 00:06:28.440
If your p-value was 0.10 you could say that
your sample is in the top 10% most extreme

00:06:28.440 --> 00:06:32.720
samples we’d expect to see based on the
distribution of sample means.

00:06:32.720 --> 00:06:36.590
If we assume that the null hypothesis is true,
and the mean caloric intake of people with

00:06:36.590 --> 00:06:42.460
gene X is 2,300 with a standard deviation
of 500 calories, the distribution of sample

00:06:42.460 --> 00:06:48.300
means will look like this, and tells us which
means we expect to see and how often we expect

00:06:48.300 --> 00:06:49.309
to see each of them.

00:06:49.309 --> 00:06:53.521
Sample means around 2,300 are most common,
but we’ll also often see sample means a

00:06:53.521 --> 00:06:55.180
little bit further away.

00:06:55.180 --> 00:06:58.889
We can use this distribution to calculate
our p-value.

00:06:58.889 --> 00:07:04.139
This is similar to how we compared the likelihood
of 200 giraffe spots in you and your friend’s

00:07:04.140 --> 00:07:06.410
models, but with only 1 model this time.

00:07:06.410 --> 00:07:09.880
Here’s our sample mean of 2,400 on this
graph.

00:07:09.889 --> 00:07:15.139
Only about 8.99 percent of the possible sample
means are higher than 2,400.

00:07:15.139 --> 00:07:19.759
So it’s not that unlikely that we’d get
a sample mean that’s this high if the true

00:07:19.759 --> 00:07:26.569
population mean was 2,300 calories.This is
called a one-sided p-value since it only tells

00:07:26.569 --> 00:07:31.320
us the probability of getting a sample mean
that’s higher than 2,400.

00:07:31.320 --> 00:07:35.440
Often when we ask scientific questions like
“Does this medicine have a different level

00:07:35.449 --> 00:07:39.920
of efficacy than the existing treatment?”
we don’t know which direction the effect

00:07:39.930 --> 00:07:41.120
will be in.

00:07:41.120 --> 00:07:44.889
The new medicine might be better...or it might
be worse.

00:07:44.889 --> 00:07:47.339
Gene X’ers might eat more, or they might
eat less.

00:07:47.339 --> 00:07:51.551
Because of this--and a few other reasons we’ll
talk about later in the series--p-values are

00:07:51.551 --> 00:07:56.699
often two-sided, meaning that we look at how
far away a value is from the mean, regardless

00:07:56.699 --> 00:08:02.159
of if it’s higher or lower . This allows
us to reject the null hypothesis if our value

00:08:02.159 --> 00:08:07.610
is significantly higher than the mean, or
if the value is significantly lower than the

00:08:07.610 --> 00:08:08.100
mean.

00:08:08.100 --> 00:08:12.500
Because the distribution of sample means is
symmetrical, if 9% of the samples of caloric

00:08:12.500 --> 00:08:18.659
intake are higher than a mean of 2,400, about
18 percent of sample means for calories would

00:08:18.659 --> 00:08:25.249
be as far away or further from the population
mean than 2,400 is in either direction.

00:08:25.249 --> 00:08:30.941
In other words, a two-sided p-value is a measure
of how extreme your sample mean is, because

00:08:30.941 --> 00:08:35.640
it tells you how often you’ll get a value
that’s as or more extreme than the one you

00:08:35.640 --> 00:08:36.240
got.

00:08:36.240 --> 00:08:41.580
The smaller your p-value is, the more “rare”
it would be to get your sample just by random

00:08:41.600 --> 00:08:43.750
chance alone if the null is true.

00:08:43.750 --> 00:08:47.670
In our example, we learned that if we assume
that there is no effect of gene X on caloric

00:08:47.670 --> 00:08:52.620
intake, then there would be an 18% chance,
about 1 in 5, that we’d see a sample like

00:08:52.620 --> 00:08:56.070
this just because of the random variation
of samples.

00:08:56.070 --> 00:09:01.420
To finish our attempt at reductio ad absurdum,
we have to decide whether this sample is “absurd”

00:09:01.420 --> 00:09:06.589
or “extreme” enough to lead us to believe
that this sample probably isn’t from the

00:09:06.589 --> 00:09:07.740
null distribution.

00:09:07.740 --> 00:09:11.050
But that decision isn’t always an easy one
to make...It’s not clear how “rare”

00:09:11.050 --> 00:09:15.320
or “absurd” a sample needs to be before
I decide to “reject” the idea that the

00:09:15.320 --> 00:09:19.760
sample was taken from a population that has
the null distribution.

00:09:19.760 --> 00:09:24.120
Especially since we don’t have another distribution
to compare it to, like we did with the giraffes.

00:09:24.120 --> 00:09:28.959
Our p-value of 0.18 tells us that if we took
a sample like this over and over, about 1

00:09:28.959 --> 00:09:33.470
out of every 5 times we’d get a sample with
a mean caloric intake that’s further from

00:09:33.470 --> 00:09:35.780
the mean than 2,400 calories is.

00:09:35.780 --> 00:09:39.180
1 in 5’s not bad...but a 1 in 20 chance
might be better.

00:09:39.180 --> 00:09:41.709
And 1 in 100 better than that.

00:09:41.709 --> 00:09:45.440
Some statisticians see a p-value as a continuous
measure of evidence.

00:09:45.440 --> 00:09:51.130
A p-value of 0.18 like ours might be considered
pretty weak evidence that our sample isn’t

00:09:51.130 --> 00:09:52.920
taken from the null distribution.

00:09:52.920 --> 00:09:57.960
But it’s better than 0.19, which is in turn
better than 0.20 and so on.

00:09:57.960 --> 00:10:02.680
However, in Null Hypothesis Significance Testing,
p-values need a cutoff.

00:10:02.680 --> 00:10:07.959
We could set a cut of at 0.05 and say that
a p-value that is less than 0.05 is sufficient

00:10:07.959 --> 00:10:12.930
evidence to allow us to “reject” the idea
that the null hypothesis is true.

00:10:12.930 --> 00:10:17.279
When we can reject the null hypothesis, we
consider our result to be “statistically

00:10:17.279 --> 00:10:21.170
significant”, which is basically a phrase
that just means “unlikely due to random

00:10:21.170 --> 00:10:22.500
chance alone”.

00:10:22.500 --> 00:10:26.019
As we’ll see later on, it doesn’t always
mean that it should be “significant” or

00:10:26.019 --> 00:10:27.190
meaningful to you.

00:10:27.190 --> 00:10:33.190
A cutoff of 0.05 means that we want our sample
value to be at least in the top 5% of most

00:10:33.190 --> 00:10:38.380
extreme values in our distribution before
we consider the value evidence against that

00:10:38.380 --> 00:10:39.280
hypothesis.

00:10:39.280 --> 00:10:44.020
And any p-value less than the 0.05 cutoff
counts.

00:10:44.020 --> 00:10:48.560
0.049 leads to the same conclusion as 0.0001.

00:10:48.560 --> 00:10:51.340
Both cause you to reject the null hypothesis.

00:10:51.350 --> 00:10:57.500
The current scientific consensus in most fields
is that your cutoff--or alpha--should be 0.05.

00:10:57.509 --> 00:11:02.199
But there’s huge disagreement in the field
of statistics about whether 0.05 is appropriate,

00:11:02.199 --> 00:11:04.350
and we’re going to dive into later.

00:11:04.350 --> 00:11:09.660
In the meantime I’m going to get 24 more
giraffes so I can compare my model with my

00:11:09.660 --> 00:11:10.480
friends.

00:11:10.480 --> 00:11:12.960
Thanks for watching.

00:11:31.080 --> 00:11:44.840
I’ll see you next time.

