WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.320
[MUSIC PLAYING]

00:00:05.320 --> 00:00:07.990
STEVE GROVE: All right,
hello, everybody.

00:00:07.990 --> 00:00:08.949
My name is Steve Grove.

00:00:08.949 --> 00:00:11.115
And I'm the director of the
News Lab here at Google.

00:00:11.115 --> 00:00:13.260
It's a new team focused on
working with journalists

00:00:13.260 --> 00:00:16.422
and entrepreneurs to build the
future of media with Google.

00:00:16.422 --> 00:00:18.130
And it's my pleasure
to welcome you today

00:00:18.130 --> 00:00:19.970
to this talk with Nick
Kristof and Sheryl

00:00:19.970 --> 00:00:22.280
WuDunn about their new
book, "A Path Appears."

00:00:22.280 --> 00:00:23.240
Before I have them
come up, I'm just

00:00:23.240 --> 00:00:24.460
going to tell you a
little bit about Nick

00:00:24.460 --> 00:00:25.490
and Sheryl and the book.

00:00:25.490 --> 00:00:28.087
And then we'll start
the conversation.

00:00:28.087 --> 00:00:29.920
Nick and Sheryl are a
really dynamic couple.

00:00:29.920 --> 00:00:32.450
They jointly won a Pulitzer
Prize together in 1990

00:00:32.450 --> 00:00:34.200
for their coverage of
the Tiananmen Square

00:00:34.200 --> 00:00:35.634
crisis in China.

00:00:35.634 --> 00:00:37.550
At the time, it was the
first husband and wife

00:00:37.550 --> 00:00:39.920
team to ever win a
Pulitzer Prize together.

00:00:39.920 --> 00:00:41.960
And Sheryl was actually
the first Asian American

00:00:41.960 --> 00:00:44.160
to a Pulitzer Prize as well.

00:00:44.160 --> 00:00:46.490
Sheryl covered energy
and international markets

00:00:46.490 --> 00:00:48.680
for the "New York Times"
among many other topics.

00:00:48.680 --> 00:00:50.221
She's also the
anchor of something

00:00:50.221 --> 00:00:52.220
that used to be called
the "New York Times" Page

00:00:52.220 --> 00:00:54.480
One, which was a look
ahead at the coming stories

00:00:54.480 --> 00:00:55.669
of the next day.

00:00:55.669 --> 00:00:57.460
She's also worked on
the strategic planning

00:00:57.460 --> 00:00:59.580
side of the business and
the circulation department,

00:00:59.580 --> 00:01:01.038
where she ran an
effort to build up

00:01:01.038 --> 00:01:02.560
the next generation of readers.

00:01:02.560 --> 00:01:04.859
So it's really a rare instance
in which Sheryl worked

00:01:04.859 --> 00:01:06.740
both in the journalistic
side of the newsroom

00:01:06.740 --> 00:01:09.270
but also the business side
of a newsroom as well, which

00:01:09.270 --> 00:01:11.670
gives her, I think, a really
unique perspective on media

00:01:11.670 --> 00:01:14.117
and where we're headed.

00:01:14.117 --> 00:01:15.700
She's since grown
an impressive career

00:01:15.700 --> 00:01:17.410
in banking and in
financing, focused

00:01:17.410 --> 00:01:19.960
on technology firms,
new media firms and also

00:01:19.960 --> 00:01:21.520
double bottom line
firms, firms that

00:01:21.520 --> 00:01:23.900
look to do both
profitable things

00:01:23.900 --> 00:01:25.734
but also things that
are good for the world.

00:01:25.734 --> 00:01:27.608
Nick is a columnist for
the "New York Times,"

00:01:27.608 --> 00:01:29.910
as you probably know, where
he focuses on human rights

00:01:29.910 --> 00:01:31.410
and giving a voice
to the voiceless.

00:01:31.410 --> 00:01:32.910
He's really come to
represent, I think,

00:01:32.910 --> 00:01:35.360
the conscience of the "New
York Times" in a lot of ways.

00:01:35.360 --> 00:01:37.526
He started out on the
reporting side of the business

00:01:37.526 --> 00:01:41.380
as a reporter in Los Angeles,
in Hong Kong, and Beijing,

00:01:41.380 --> 00:01:44.485
and in Tokyo, obviously in
China, as we said before.

00:01:44.485 --> 00:01:47.110
And he's the kind of journalist
who will do just about anything

00:01:47.110 --> 00:01:50.820
to tell a story that he
really feels needs to be told.

00:01:50.820 --> 00:01:53.200
During Nick's travels
he's caught malarial,

00:01:53.200 --> 00:01:57.340
he has confronted warlords, he's
encountered an Indonesian mob

00:01:57.340 --> 00:01:59.720
caring heads on
pikes, he's survived

00:01:59.720 --> 00:02:00.790
an African plain crash.

00:02:00.790 --> 00:02:01.831
Nick has been everywhere.

00:02:01.831 --> 00:02:02.930
He has done everything.

00:02:02.930 --> 00:02:04.400
And we're pleased to have
him here at Google today.

00:02:04.400 --> 00:02:06.720
Hopefully, it will be a
little safe time for you

00:02:06.720 --> 00:02:09.052
here in Mountain View
then on the road.

00:02:09.052 --> 00:02:11.260
In addition to the Pulitzer
Prize he won with Sheryl,

00:02:11.260 --> 00:02:12.780
he won another
Pulitzer in the '90s

00:02:12.780 --> 00:02:15.790
for his coverage of the
situation-- sorry, in 2005,

00:02:15.790 --> 00:02:17.820
for his coverage of
the situation in Darfur

00:02:17.820 --> 00:02:19.940
and has also won
the Fred Cuny Prize

00:02:19.940 --> 00:02:21.880
for the Prevention
of Armed Conflict.

00:02:21.880 --> 00:02:24.047
So this is truly a power
couple we have here today.

00:02:24.047 --> 00:02:25.380
We're very excited to have them.

00:02:25.380 --> 00:02:27.310
And I think it's a
testament to the fact

00:02:27.310 --> 00:02:29.310
that they're a power
couple who uses their power

00:02:29.310 --> 00:02:30.647
to do good in the world.

00:02:30.647 --> 00:02:33.230
So please join me in welcoming
Nick Kristof and Sherly WuDunn.

00:02:33.230 --> 00:02:35.182
[APPLAUSE]

00:02:35.182 --> 00:02:36.093
Thanks so much.

00:02:36.093 --> 00:02:37.134
NICK KRISTOF: Hey, Steve.

00:02:37.134 --> 00:02:39.922
STEVE GROVE: Yeah, thanks.

00:02:39.922 --> 00:02:40.880
Well welcome to Google.

00:02:40.880 --> 00:02:43.390
We're so happy to have you
both here with us today.

00:02:43.390 --> 00:02:44.100
NICK KRISTOF: Glad to be here.

00:02:44.100 --> 00:02:46.016
STEVE GROVE: And we want
to dig into the book.

00:02:46.016 --> 00:02:48.540
But I thought before we got
into some of the specific issues

00:02:48.540 --> 00:02:50.140
that you focused
on, I'm curious what

00:02:50.140 --> 00:02:52.844
it's like to write a
book with your spouse.

00:02:52.844 --> 00:02:54.260
SHERYL WUDUNN:
Well, first of all,

00:02:54.260 --> 00:02:55.640
I only had to write half a book.

00:02:55.640 --> 00:02:56.780
So, it was really nice.

00:02:56.780 --> 00:02:58.390
STEVE GROVE: Oh,
you're half-off.

00:02:58.390 --> 00:02:59.960
SHERYL WUDUNN: Well, no,
that's actually nice.

00:02:59.960 --> 00:03:00.990
When you write a book
with someone else,

00:03:00.990 --> 00:03:02.220
you only have to
write half of it.

00:03:02.220 --> 00:03:03.803
NICK KRISTOF: I
mostly wrote subjects.

00:03:03.803 --> 00:03:04.940
Sheryl wrote predicates.

00:03:04.940 --> 00:03:06.523
STEVE GROVE: Oh,
sentence by sentence.

00:03:06.523 --> 00:03:09.112
[LAUGHTER]

00:03:09.112 --> 00:03:10.570
But in your fourth
book together, I

00:03:10.570 --> 00:03:11.490
assume you have a process?

00:03:11.490 --> 00:03:12.240
STEVE GROVE: Yeah.

00:03:12.240 --> 00:03:14.450
No, we actually take subjects.

00:03:14.450 --> 00:03:15.800
And the we carve out subjects.

00:03:15.800 --> 00:03:20.507
And then we each write, and
then we swap, edit and rewrite.

00:03:20.507 --> 00:03:23.090
NICK KRISTOF: But what I think
Steve is actually trying to ask

00:03:23.090 --> 00:03:25.140
is, how do you write a book
together and stay married,

00:03:25.140 --> 00:03:25.710
right?

00:03:25.710 --> 00:03:26.080
[LAUGHTER]

00:03:26.080 --> 00:03:27.205
I think that's the subtext.

00:03:27.205 --> 00:03:28.609
And you know, the
truth is that--

00:03:28.609 --> 00:03:30.150
and we also have
three kids together.

00:03:30.150 --> 00:03:32.140
And if you can
raise kids together,

00:03:32.140 --> 00:03:35.900
it is infinitely easier to
produce a book together.

00:03:35.900 --> 00:03:38.980
You know, you can put
a book to bed at night,

00:03:38.980 --> 00:03:40.420
and it stays asleep.

00:03:40.420 --> 00:03:43.970
A manuscript doesn't
play you off each other.

00:03:43.970 --> 00:03:47.230
If you screw up, the
stakes are lower, you know.

00:03:47.230 --> 00:03:48.730
STEVE GROVE: I've
never heard a book

00:03:48.730 --> 00:03:51.140
writing process sound
so easy, actually,

00:03:51.140 --> 00:03:52.684
comparatively speaking.

00:03:52.684 --> 00:03:54.850
Like I said, you've written
a lot of books together.

00:03:54.850 --> 00:03:57.269
All four of your books
have been best sellers.

00:03:57.269 --> 00:03:59.810
A lot of your past works have
been a little more journalistic

00:03:59.810 --> 00:04:00.310
in nature.

00:04:00.310 --> 00:04:02.530
This book seems to
combine storytelling

00:04:02.530 --> 00:04:03.512
with a call to action.

00:04:03.512 --> 00:04:05.220
Tell us a little bit
about the motivation

00:04:05.220 --> 00:04:06.164
for writing this book.

00:04:06.164 --> 00:04:07.580
SHERYL WUDUNN:
Well, I have to say

00:04:07.580 --> 00:04:10.080
that, first of all, because I'm
not a journalist any longer,

00:04:10.080 --> 00:04:13.720
I sort of felt that I
could leave aside this need

00:04:13.720 --> 00:04:20.720
to be balanced and factual
and sort of not take sides.

00:04:20.720 --> 00:04:22.950
And our first two books
really were like that.

00:04:22.950 --> 00:04:25.570
I mean our first two
books, on China, first,

00:04:25.570 --> 00:04:28.374
and then on Asia, we really
were still the journalists.

00:04:28.374 --> 00:04:30.540
And we were always accountable
for whatever we said.

00:04:30.540 --> 00:04:32.039
And we were very
careful, especially

00:04:32.039 --> 00:04:36.350
writing about China,
not to take sides.

00:04:36.350 --> 00:04:38.100
But it was clear, just
from the reporting,

00:04:38.100 --> 00:04:39.790
it just spoke for itself.

00:04:39.790 --> 00:04:43.490
And then in both "Half the
Sky" and "A Path Appears,"

00:04:43.490 --> 00:04:45.980
we also did focus
on the reporting.

00:04:45.980 --> 00:04:49.146
And I think that was part
of the value in, at least,

00:04:49.146 --> 00:04:51.520
"Half the Sky" and now "A Path
Appears," that we actually

00:04:51.520 --> 00:04:52.478
did a lot of reporting.

00:04:52.478 --> 00:04:54.480
So it wasn't as though
we took a stand first,

00:04:54.480 --> 00:04:58.610
and then everything was
written from that one stand's

00:04:58.610 --> 00:04:59.450
point of view.

00:04:59.450 --> 00:05:01.500
I think the value
is that we turned up

00:05:01.500 --> 00:05:03.530
things that were
just so obviously

00:05:03.530 --> 00:05:06.980
horrific that the facts
spoke for themselves.

00:05:06.980 --> 00:05:09.480
And so, at the end, it was at
the end that we sort of nudge.

00:05:09.480 --> 00:05:11.104
We don't want to
really be overbearing,

00:05:11.104 --> 00:05:13.290
because we want people to
make their own decision.

00:05:13.290 --> 00:05:15.820
But there is a point of view,
much more of a point of view

00:05:15.820 --> 00:05:17.497
in each of those last two books.

00:05:17.497 --> 00:05:19.330
NICK KRISTOF: It sort
of came about, really,

00:05:19.330 --> 00:05:22.030
because, after "Half the
Sky," people kept asking us,

00:05:22.030 --> 00:05:23.320
so what can I do?

00:05:23.320 --> 00:05:26.900
And it does seem to us
that an awful lot of people

00:05:26.900 --> 00:05:28.370
kind of want to do
the right thing,

00:05:28.370 --> 00:05:31.110
want to find a purpose
or meaning in life

00:05:31.110 --> 00:05:35.580
but worry about corruption,
about inefficiency

00:05:35.580 --> 00:05:37.520
in aid groups, or
just the problem

00:05:37.520 --> 00:05:39.680
seemed too vast, so
that anything you do

00:05:39.680 --> 00:05:42.450
is just a kind of meaningless
drop in the bucket.

00:05:42.450 --> 00:05:44.310
And it does seem
to us that there

00:05:44.310 --> 00:05:46.000
are answers to those concerns.

00:05:46.000 --> 00:05:49.150
They're legitimate, that helping
people is harder than it looks.

00:05:49.150 --> 00:05:51.560
But that we have a
much more evidence now

00:05:51.560 --> 00:05:54.640
than we had a decade or
two ago about what works,

00:05:54.640 --> 00:05:56.100
where you get bang for the buck.

00:05:56.100 --> 00:05:57.740
And that there are
things we can do

00:05:57.740 --> 00:06:00.250
where we can have a high
degree of confidence

00:06:00.250 --> 00:06:03.590
that one can really have
a transformational impact.

00:06:03.590 --> 00:06:06.190
We talked about clubfoot
early on in the book.

00:06:06.190 --> 00:06:09.440
1 kid in a 1,000 is
born with clubfoot.

00:06:09.440 --> 00:06:11.100
We never notice it
in this country,

00:06:11.100 --> 00:06:14.160
because if you're born
with a club foot, then

00:06:14.160 --> 00:06:15.720
it gets treated right at birth.

00:06:15.720 --> 00:06:18.275
And you know,
everything is fine.

00:06:18.275 --> 00:06:19.650
Only when we are
writing the book

00:06:19.650 --> 00:06:22.790
did we find out that my mom had
been born with clubfoot, which

00:06:22.790 --> 00:06:25.100
I was completely oblivious
to, because she's

00:06:25.100 --> 00:06:29.200
very active and very athletic.

00:06:29.200 --> 00:06:33.090
And yet in some place
in the developing world,

00:06:33.090 --> 00:06:34.890
that child will never
be able to walk,

00:06:34.890 --> 00:06:38.400
will never be able to go to
school, will become a beggar.

00:06:38.400 --> 00:06:44.430
And with help, you
can cure that child

00:06:44.430 --> 00:06:48.280
so that their life is
completely transformed.

00:06:48.280 --> 00:06:53.190
And boy, when you see
that operation happening,

00:06:53.190 --> 00:06:55.282
it's unbelievable what
can be accomplished.

00:06:55.282 --> 00:06:56.990
STEVE GROVE: I'm sorry,
go ahead, Sheryl.

00:06:56.990 --> 00:06:58.000
SHERYL WUDUNN: And I think
that's really important.

00:06:58.000 --> 00:06:59.500
Because I think
that a lot of people

00:06:59.500 --> 00:07:01.550
really do think, oh,
my gosh, these problems

00:07:01.550 --> 00:07:03.020
are so overwhelming.

00:07:03.020 --> 00:07:05.600
I mean, how can I even
begin to make a difference?

00:07:05.600 --> 00:07:07.930
And what we think
is so essential

00:07:07.930 --> 00:07:11.030
is for people to break down
that problem to little bits

00:07:11.030 --> 00:07:14.070
and to realize that you can have
an extreme amount of control

00:07:14.070 --> 00:07:15.280
over a little bit.

00:07:15.280 --> 00:07:17.530
And you know, I know
how important bits are

00:07:17.530 --> 00:07:19.600
to you all here, too.

00:07:19.600 --> 00:07:22.970
But I really do think
that even transforming one

00:07:22.970 --> 00:07:25.060
life is pretty amazing.

00:07:25.060 --> 00:07:27.890
And if you can do
that from scratch,

00:07:27.890 --> 00:07:30.500
I mean help someone
like Rashid, who

00:07:30.500 --> 00:07:33.570
is the woman in Niger,
a girl in Niger,

00:07:33.570 --> 00:07:36.690
who had clubfoot in both feet.

00:07:36.690 --> 00:07:40.470
And we actually found
someone, an American,

00:07:40.470 --> 00:07:42.870
who was born and
raised in California,

00:07:42.870 --> 00:07:48.350
who had donated $250 to help
someone in the developing world

00:07:48.350 --> 00:07:49.650
get cured of clubfoot.

00:07:49.650 --> 00:07:53.440
And so we followed her check
and found Rashid in Niger.

00:07:53.440 --> 00:08:00.384
And her money really helped
Rashid get cured of clubfoot.

00:08:00.384 --> 00:08:01.800
STEVE GROVE: You
talk about people

00:08:01.800 --> 00:08:04.060
feeling like they don't know
exactly how they can access

00:08:04.060 --> 00:08:05.510
some of these problems
or how they can help.

00:08:05.510 --> 00:08:06.160
Early on in the
book-- I just want

00:08:06.160 --> 00:08:08.297
to read a brief
excerpt that I think

00:08:08.297 --> 00:08:10.130
will resonate with our
crowd here at Google.

00:08:10.130 --> 00:08:12.789
You write that, so many social
problems in the 21st century

00:08:12.789 --> 00:08:14.080
seem intractable and insoluble.

00:08:14.080 --> 00:08:15.330
We explore Mars.

00:08:15.330 --> 00:08:17.124
We embed telephones
in our wrist watches.

00:08:17.124 --> 00:08:19.290
But we can't keep families
safe in the inner cities.

00:08:19.290 --> 00:08:21.920
We can map subatomic
particles such as gluons.

00:08:21.920 --> 00:08:23.685
We can design robots
that drive cars

00:08:23.685 --> 00:08:26.540
and respond to speech and
defeat grandmasters in chess.

00:08:26.540 --> 00:08:28.790
But be grudgingly accept
failure in our struggles

00:08:28.790 --> 00:08:32.000
to keep kids in school off
drugs and out of gangs.

00:08:32.000 --> 00:08:32.730
Why is that?

00:08:32.730 --> 00:08:34.643
Why does it seem easier
for us, as a society,

00:08:34.643 --> 00:08:36.809
to tackle some of these,
what we call around Google,

00:08:36.809 --> 00:08:40.090
moonshot problems than it is to
do the really simple, important

00:08:40.090 --> 00:08:43.190
work of helping other people.

00:08:43.190 --> 00:08:45.420
NICK KRISTOF: You know,
humans are complex.

00:08:45.420 --> 00:08:48.120
And I think that we're beginning
to understand, increasingly,

00:08:48.120 --> 00:08:52.620
that poverty is not just--
the metrics about poverty

00:08:52.620 --> 00:08:55.350
aren't just amounts of money,
of income that you earn,

00:08:55.350 --> 00:09:00.170
but there are pathologies that
interact with poverty that

00:09:00.170 --> 00:09:04.700
are caused by it and then
create self-replicating cycles

00:09:04.700 --> 00:09:06.870
and self-destructive behaviors.

00:09:06.870 --> 00:09:09.640
Poverty can rob you
of cognitive capacity.

00:09:09.640 --> 00:09:13.320
There's some fascinating
studies in India of IQ

00:09:13.320 --> 00:09:15.740
before and after
the rice harvest.

00:09:15.740 --> 00:09:18.110
And before the
rice harvest, when

00:09:18.110 --> 00:09:20.460
people are
economically stressed,

00:09:20.460 --> 00:09:23.430
their IQ is 11 points lower
than it is immediately

00:09:23.430 --> 00:09:25.590
after the rice harvest.

00:09:25.590 --> 00:09:28.650
When people are
stressed in these ways,

00:09:28.650 --> 00:09:34.730
they sometimes engage in
self-destructive behaviors.

00:09:34.730 --> 00:09:39.260
And these cycles can repeat.

00:09:39.260 --> 00:09:43.250
And these are hard problems.

00:09:43.250 --> 00:09:47.440
You look at a child
poverty in the US,

00:09:47.440 --> 00:09:54.880
and I think what Sheryl and I
saw is that poverty in America,

00:09:54.880 --> 00:09:58.800
the basic kind of
material situation,

00:09:58.800 --> 00:10:03.200
was infinitely better off than
if one is poor in Calcutta.

00:10:03.200 --> 00:10:07.330
But there is a sense of
despair and hopelessness,

00:10:07.330 --> 00:10:11.270
if you are a mom in
West Virginia, that

00:10:11.270 --> 00:10:15.170
is quite parallel to the despair
of people in slums in Calcutta.

00:10:15.170 --> 00:10:21.180
And that commonality, I think,
is a reminder of our humanity

00:10:21.180 --> 00:10:24.360
and of the need to try to do
what we can to chip away at it

00:10:24.360 --> 00:10:26.340
but also the complexity
of the challenges.

00:10:26.340 --> 00:10:27.923
SHERYL WUDUNN: I
also think that there

00:10:27.923 --> 00:10:31.320
are more solutions that
are coming to light.

00:10:31.320 --> 00:10:35.730
There's been a lot of
data-driven, evidence-based

00:10:35.730 --> 00:10:39.310
research that shows
this strategy is better

00:10:39.310 --> 00:10:41.330
than that strategy,
because they have done

00:10:41.330 --> 00:10:44.710
randomized controlled trials
to show that a program that

00:10:44.710 --> 00:10:47.530
focuses on early childhood
really can make a broader

00:10:47.530 --> 00:10:50.040
difference and
change the trajectory

00:10:50.040 --> 00:10:51.940
of the life of a kid.

00:10:51.940 --> 00:10:54.470
And so while, in
the past, we have

00:10:54.470 --> 00:10:55.890
done a lot of
these interventions

00:10:55.890 --> 00:11:00.110
with kids in impoverished
homes, often what's happened

00:11:00.110 --> 00:11:02.830
is that we've
intervened too late.

00:11:02.830 --> 00:11:05.080
And now this research
is showing that you've

00:11:05.080 --> 00:11:07.030
got to intervene a lot earlier.

00:11:07.030 --> 00:11:10.100
And of course, when you
put theory into practice,

00:11:10.100 --> 00:11:13.350
and even if you take something
out of the experimental stage,

00:11:13.350 --> 00:11:15.970
from the randomized control
trial, which is everything

00:11:15.970 --> 00:11:18.672
is very much controlled,
into the real world,

00:11:18.672 --> 00:11:19.380
things do change.

00:11:19.380 --> 00:11:20.754
You may not get
as good teachers,

00:11:20.754 --> 00:11:23.360
or the classroom
isn't exactly the same

00:11:23.360 --> 00:11:24.890
as it was in the experiment.

00:11:24.890 --> 00:11:28.400
And maybe it's a
poor construction,

00:11:28.400 --> 00:11:30.830
and so there's noise
outside, so that

00:11:30.830 --> 00:11:33.490
impairs the ability
of the kids to learn.

00:11:33.490 --> 00:11:36.300
So there's a million
different things that go on.

00:11:36.300 --> 00:11:40.160
But at least, we have the
beginnings of strategies

00:11:40.160 --> 00:11:42.200
that we know can work
if we can actually

00:11:42.200 --> 00:11:44.905
get to that sort
of perfect stage.

00:11:44.905 --> 00:11:46.530
And that's really
important, because we

00:11:46.530 --> 00:11:49.000
didn't have all
that in the past.

00:11:49.000 --> 00:11:50.750
STEVE GROVE: I'm
curious, your assessment,

00:11:50.750 --> 00:11:51.700
in all the
conversations you had,

00:11:51.700 --> 00:11:53.110
how far along you think we are?

00:11:53.110 --> 00:11:54.920
I'm reminded, there
was an MIT economist

00:11:54.920 --> 00:11:55.755
you wrote about Esther--

00:11:55.755 --> 00:11:56.050
SHERYL WUDUNN: Esther Duflo.

00:11:56.050 --> 00:11:59.000
STEVE GROVE: --yes, who did a
study determining that actually

00:11:59.000 --> 00:12:00.960
cook stoves aren't as effective
as the State Department thought

00:12:00.960 --> 00:12:02.730
they were going to be, despite
a lot of money and energy

00:12:02.730 --> 00:12:03.950
that went into them.

00:12:03.950 --> 00:12:05.155
And we're obviously a
data-driven company, here

00:12:05.155 --> 00:12:05.655
at Google.

00:12:05.655 --> 00:12:07.490
We measure just about
everything that we do.

00:12:07.490 --> 00:12:10.830
Do you think that the nonprofit
sector is changing because

00:12:10.830 --> 00:12:13.230
of people like Esther
or we have ways to go?

00:12:13.230 --> 00:12:15.460
SHERYL WUDUNN: Absolutely,
it's definitely changing.

00:12:15.460 --> 00:12:17.160
But there are problems.

00:12:17.160 --> 00:12:19.970
I mean, even though we have
these great evidence-based

00:12:19.970 --> 00:12:22.530
strategies, one problem
is that they're expensive.

00:12:22.530 --> 00:12:25.710
You know, someone came
up to me the other day

00:12:25.710 --> 00:12:27.430
and said that we
have a non-profit.

00:12:27.430 --> 00:12:30.649
And we believe in this
data-driven stuff.

00:12:30.649 --> 00:12:32.190
But the problem is
it's so expensive.

00:12:32.190 --> 00:12:33.273
We're this tiny nonprofit.

00:12:33.273 --> 00:12:36.250
And so we want to experiment
with these teaching strategies

00:12:36.250 --> 00:12:38.120
that we've developed,
but I can't

00:12:38.120 --> 00:12:41.680
pay for doing a
randomized control trial

00:12:41.680 --> 00:12:43.400
and have different sites.

00:12:43.400 --> 00:12:44.400
It's just too expensive.

00:12:44.400 --> 00:12:48.100
And so you run into sort of
the practicality of things.

00:12:48.100 --> 00:12:51.250
NICK KRISTOF: Oh, but the
change is just enormous.

00:12:51.250 --> 00:12:53.730
Basically, the nonprofit
sector, in general,

00:12:53.730 --> 00:12:57.350
has hugely lagged the for
profit sector in productivity,

00:12:57.350 --> 00:12:58.280
in metrics.

00:12:58.280 --> 00:13:00.260
It's been much more
about inputs, not enough

00:13:00.260 --> 00:13:01.070
about outputs.

00:13:01.070 --> 00:13:03.680
You haven't had that the
discipline of the market

00:13:03.680 --> 00:13:05.430
to create better productivity.

00:13:05.430 --> 00:13:09.482
But now, partly because of
randomized controlled trials,

00:13:09.482 --> 00:13:11.690
we're just getting a much
better sense of what works,

00:13:11.690 --> 00:13:15.017
at what cost, what our
priorities should be.

00:13:15.017 --> 00:13:16.600
For example, there
was huge excitement

00:13:16.600 --> 00:13:19.840
a few years ago
about microlending.

00:13:19.840 --> 00:13:24.760
And when it was
carefully studied,

00:13:24.760 --> 00:13:26.400
rigorously, it
turned out, frankly,

00:13:26.400 --> 00:13:30.510
that microlending helped but
wasn't nearly as transformative

00:13:30.510 --> 00:13:32.360
as people had hoped.

00:13:32.360 --> 00:13:36.090
But when it was
studied, it turned out

00:13:36.090 --> 00:13:38.530
that another element
of microfinance

00:13:38.530 --> 00:13:39.700
really was transformative.

00:13:39.700 --> 00:13:42.014
And that was microsavings.

00:13:42.014 --> 00:13:43.430
Most people around
the world don't

00:13:43.430 --> 00:13:46.780
have a capacity to save money.

00:13:46.780 --> 00:13:48.340
They don't have bank accounts.

00:13:48.340 --> 00:13:53.390
They typically get money
coming in once or twice a year

00:13:53.390 --> 00:13:55.430
at the end of a
harvest, and then you

00:13:55.430 --> 00:14:00.140
can put money in a jar
somewhere in your house

00:14:00.140 --> 00:14:02.470
or you can buy extra chickens.

00:14:02.470 --> 00:14:04.440
In West Africa, you
can deposit money

00:14:04.440 --> 00:14:06.700
with the Susu money traders.

00:14:06.700 --> 00:14:11.730
And you pay 4o0% annual
interest on your savings.

00:14:11.730 --> 00:14:15.310
So we could give people
a microsavings capacity.

00:14:15.310 --> 00:14:19.110
That really has
proved transformative.

00:14:19.110 --> 00:14:22.290
Getting that kind of
evidence about what

00:14:22.290 --> 00:14:25.530
works, where we
should be investing,

00:14:25.530 --> 00:14:29.910
I think has just far
reaching implications.

00:14:29.910 --> 00:14:33.280
And the real problem has been
that we have evidence now,

00:14:33.280 --> 00:14:35.760
but we don't translate
it into policy.

00:14:35.760 --> 00:14:37.770
SHERYL WUDUNN: And that's
where the rub really

00:14:37.770 --> 00:14:41.000
comes, because you've
got to shift then

00:14:41.000 --> 00:14:45.816
the cultural side of NGOs
and different organizations,

00:14:45.816 --> 00:14:47.690
so that they can actually
move towards things

00:14:47.690 --> 00:14:51.210
that really are shown to work.

00:14:51.210 --> 00:14:54.750
And then also, with policy,
certainly with early childhood

00:14:54.750 --> 00:14:57.050
education, we know that
early childhood education

00:14:57.050 --> 00:14:58.010
is really important.

00:14:58.010 --> 00:15:02.130
So when President Obama
says, I want universal pre-K

00:15:02.130 --> 00:15:04.592
and John Boehner
says, yes, I do, too.

00:15:04.592 --> 00:15:05.800
I mean, he clapped, actually.

00:15:05.800 --> 00:15:10.330
But then it gets stuck in the
dysfunction of Washington.

00:15:10.330 --> 00:15:13.220
And things just
don't move along.

00:15:13.220 --> 00:15:16.710
Even though we all know
that it is the best

00:15:16.710 --> 00:15:18.670
thing for our country's young.

00:15:18.670 --> 00:15:20.920
STEVE GROVE: Is part of that
a problem of measurement,

00:15:20.920 --> 00:15:23.599
because I know that-- and you
write about this in the book--

00:15:23.599 --> 00:15:25.640
some of the benefits of
early childhood education

00:15:25.640 --> 00:15:30.327
come 15, 16, 17 years later,
when prisons aren't as full,

00:15:30.327 --> 00:15:31.910
which is just a
harder message, maybe,

00:15:31.910 --> 00:15:35.010
to convey, especially, maybe, to
policymakers who are thinking,

00:15:35.010 --> 00:15:38.362
maybe, shorter term
than we might like.

00:15:38.362 --> 00:15:39.820
Is there things
that the nonprofits

00:15:39.820 --> 00:15:41.944
can do to communicate that
message more effectively

00:15:41.944 --> 00:15:43.320
or is that a policy problem?

00:15:43.320 --> 00:15:45.194
SHERYL WUDUNN: Well,
those are the long-term,

00:15:45.194 --> 00:15:47.560
some outcomes that
are clear as well.

00:15:47.560 --> 00:15:50.490
But along the way, there
are lots of metrics as well.

00:15:50.490 --> 00:15:52.080
For instance,
staying in school, I

00:15:52.080 --> 00:15:53.538
mean that's one of
the key metrics.

00:15:53.538 --> 00:15:55.250
So for younger kids,
if they actually

00:15:55.250 --> 00:15:59.030
get, not a head start
even, even if they

00:15:59.030 --> 00:16:01.797
are born at the starting
line with everyone else,

00:16:01.797 --> 00:16:02.880
that's a real achievement.

00:16:02.880 --> 00:16:05.360
Because so many kids,
especially if, for instance ,

00:16:05.360 --> 00:16:09.990
a lot of babies are born
with fetal alcohol syndrome,

00:16:09.990 --> 00:16:14.010
and so you're way behind the
starting line when you're born.

00:16:14.010 --> 00:16:15.820
Because that does impair.

00:16:15.820 --> 00:16:17.640
It has huge amounts
of implications

00:16:17.640 --> 00:16:19.090
for the brain development.

00:16:19.090 --> 00:16:20.950
And then smoking
also has an impact.

00:16:20.950 --> 00:16:23.800
We hadn't really
appreciated that years ago.

00:16:23.800 --> 00:16:30.010
But smoking also can actually
lead to more violent children,

00:16:30.010 --> 00:16:33.660
partly because they believe
that smoking creates

00:16:33.660 --> 00:16:35.910
a greater amount
of testosterone.

00:16:35.910 --> 00:16:37.900
So there are implications there.

00:16:37.900 --> 00:16:43.090
And so you have, if the
kid can start-- at least

00:16:43.090 --> 00:16:45.600
be born at the starting
line, and then also

00:16:45.600 --> 00:16:48.160
is talked to, by
the age of four,

00:16:48.160 --> 00:16:52.670
we know that children
of parents on welfare

00:16:52.670 --> 00:16:56.140
have heard 30 million
fewer words than children

00:16:56.140 --> 00:16:57.360
of professional parents.

00:16:57.360 --> 00:17:00.180
So 30 million words
by the age of four,

00:17:00.180 --> 00:17:01.960
that difference,
that gap you can just

00:17:01.960 --> 00:17:06.760
imagine how far behind those
kids are of parents on welfare.

00:17:06.760 --> 00:17:10.980
So we have milestones
along the way, too.

00:17:10.980 --> 00:17:13.900
NICK KRISTOF: I don't think
that the problem is even

00:17:13.900 --> 00:17:15.260
the length of time.

00:17:15.260 --> 00:17:17.410
I think part of the
problem is that,

00:17:17.410 --> 00:17:18.910
if you're a member
of Congress, then

00:17:18.910 --> 00:17:20.869
you are consistently
getting bombarded

00:17:20.869 --> 00:17:25.670
with claims of evidence from
partisans of every side.

00:17:25.670 --> 00:17:28.770
And you don't understand
randomized controlled trials.

00:17:28.770 --> 00:17:32.860
And so you just kind
of shrug at it all and

00:17:32.860 --> 00:17:36.360
go with what your gut tells you.

00:17:36.360 --> 00:17:39.480
And one of the problems, with
some of the most efficient

00:17:39.480 --> 00:17:44.340
interventions, is that you
can't put a human face on them.

00:17:44.340 --> 00:17:49.810
So maybe the single
program in America

00:17:49.810 --> 00:17:52.972
that has the greatest, most
robust evidence of cost

00:17:52.972 --> 00:17:55.180
effectiveness is called the
Nurse Family Partnership.

00:17:55.180 --> 00:17:57.280
And it essentially
involves home visitations,

00:17:57.280 --> 00:18:00.810
beginning in pregnancy, for
very at-risk, low income moms,

00:18:00.810 --> 00:18:01.660
up to age two.

00:18:01.660 --> 00:18:03.464
And it tries to reduce
the number of moms

00:18:03.464 --> 00:18:05.880
who are smoking in pregnancy,
drinking in pregnancy, doing

00:18:05.880 --> 00:18:07.422
drugs, exposing
their kids to lead.

00:18:07.422 --> 00:18:09.130
Then it gets them to
read to their child,

00:18:09.130 --> 00:18:11.850
talk to their child, hug their
child, this kind of thing.

00:18:11.850 --> 00:18:15.540
And the impact is astonishing.

00:18:15.540 --> 00:18:18.545
And at age 15,
those kids, 13 years

00:18:18.545 --> 00:18:20.420
after the program has
ended, are only half as

00:18:20.420 --> 00:18:22.480
likely to have been
arrested as kids randomly

00:18:22.480 --> 00:18:25.350
assigned not to
be in the program.

00:18:25.350 --> 00:18:29.400
But you can't choose
any one particular kid

00:18:29.400 --> 00:18:31.165
and say, look, this
kid is shining,

00:18:31.165 --> 00:18:32.540
because they were
in Nurse Family

00:18:32.540 --> 00:18:34.940
Partnership 13 years ago.

00:18:34.940 --> 00:18:38.310
And in general, I
think why is nutrition,

00:18:38.310 --> 00:18:41.170
why do we under invest in
early nutrition programs

00:18:41.170 --> 00:18:43.400
and micronutrients
and salt iodization?

00:18:43.400 --> 00:18:47.040
Again, because it's very hard
to put a human face on that.

00:18:47.040 --> 00:18:48.710
It's very hard for
me, as a columnist,

00:18:48.710 --> 00:18:53.146
to write a triumphant column
about salt iodization.

00:18:53.146 --> 00:18:54.270
I know, because I've tried.

00:18:54.270 --> 00:18:56.800
[LAUGHTER]

00:18:56.800 --> 00:19:01.910
So I thinks that that
is part of the problem,

00:19:01.910 --> 00:19:03.710
that some of the
most effective things

00:19:03.710 --> 00:19:06.250
work on a statistical
basis rather

00:19:06.250 --> 00:19:08.902
than through individual
human triumphs.

00:19:08.902 --> 00:19:10.610
STEVE GROVE: I mean,
you, in your columns

00:19:10.610 --> 00:19:13.230
and certainly in the book,
use so many stories of people

00:19:13.230 --> 00:19:15.730
that you've met to try to
put a face on these problems.

00:19:15.730 --> 00:19:17.350
Do you feel like that works?

00:19:17.350 --> 00:19:19.725
SHERYL WUDUNN: Well, there's
been some really interesting

00:19:19.725 --> 00:19:24.520
research on what happens
when you put a face in front

00:19:24.520 --> 00:19:27.600
of someone when it
comes to donors.

00:19:27.600 --> 00:19:30.560
Paul Sloic, who's someone we
also talk about in the book,

00:19:30.560 --> 00:19:34.770
really was helping--
I think it was CARE.

00:19:34.770 --> 00:19:37.270
And he was analyzing
what happens

00:19:37.270 --> 00:19:42.890
when a potential donor sees
a photograph of one girl who

00:19:42.890 --> 00:19:43.859
is starving.

00:19:43.859 --> 00:19:44.900
And how much do you give.

00:19:44.900 --> 00:19:46.860
So you give an amount x.

00:19:46.860 --> 00:19:49.710
And then if they just
add one other person

00:19:49.710 --> 00:19:52.990
into the photograph,
let's say her brother,

00:19:52.990 --> 00:19:55.440
well donations drop by half.

00:19:55.440 --> 00:19:56.250
Now, why is that?

00:19:56.250 --> 00:19:59.091
And it seems to be
because you think

00:19:59.091 --> 00:20:00.590
that, if there are
two people there,

00:20:00.590 --> 00:20:02.810
you make less of
a difference then

00:20:02.810 --> 00:20:05.130
if you see only
one person there.

00:20:05.130 --> 00:20:09.330
So there is a lot
going on there when

00:20:09.330 --> 00:20:12.260
it comes to just
visual stimulation.

00:20:12.260 --> 00:20:16.230
And at the same time,
though, researchers

00:20:16.230 --> 00:20:20.590
have found that donations really
are kind of an emotional thing.

00:20:20.590 --> 00:20:22.760
You make a decision
to donate based

00:20:22.760 --> 00:20:25.330
on an emotional
connection, not so

00:20:25.330 --> 00:20:26.810
much a statistical connection.

00:20:26.810 --> 00:20:28.240
They've actually
also found that,

00:20:28.240 --> 00:20:31.720
if you give someone
some statistics,

00:20:31.720 --> 00:20:37.160
oh, if you can give $10,
you'll save 10 kids,

00:20:37.160 --> 00:20:40.510
or if it's $1,000,
you'll save 1,000 kids.

00:20:40.510 --> 00:20:43.380
Well, those numbers just
don't do anything for people.

00:20:43.380 --> 00:20:45.750
It's that one kid
that they care about.

00:20:45.750 --> 00:20:47.416
NICK KRISTOF: In fact,
if you ask people

00:20:47.416 --> 00:20:49.220
to do math problems,
first, to exercise

00:20:49.220 --> 00:20:50.810
the more rational
parts of the brain,

00:20:50.810 --> 00:20:53.304
and then you ask them to
donate, they donate less.

00:20:53.304 --> 00:20:54.470
SHERYL WUDUNN: Yeah, I know.

00:20:54.470 --> 00:20:54.830
STEVE GROVE: Interesting.

00:20:54.830 --> 00:20:57.200
SHERYL WUDUNN: And so
also, if your screensaver

00:20:57.200 --> 00:20:59.850
has like dollar
signs on it, you're

00:20:59.850 --> 00:21:03.362
also less likely to donate
than if it has fish on it.

00:21:03.362 --> 00:21:05.570
STEVE GROVE: Well, this gets
into an interesting area

00:21:05.570 --> 00:21:06.945
that you cover in
the book, where

00:21:06.945 --> 00:21:08.362
you study the
neurology of giving.

00:21:08.362 --> 00:21:10.486
You both actually went to
the University of Oregon.

00:21:10.486 --> 00:21:11.903
You got inside of
a brain scanner.

00:21:11.903 --> 00:21:14.069
Tell our audience a little
bit about that experience

00:21:14.069 --> 00:21:16.070
and what you learned,
both the experience itself

00:21:16.070 --> 00:21:17.569
of getting inside
that brain scanner

00:21:17.569 --> 00:21:19.740
and what you learned about
the psychology of giving.

00:21:19.740 --> 00:21:22.495
SHERYL WUDUNN: Well, that
was actually fascinating.

00:21:22.495 --> 00:21:24.620
So now, because of neuroscience
and the development

00:21:24.620 --> 00:21:26.410
of the FMRI, they
actually can see

00:21:26.410 --> 00:21:28.910
what happens in the brain when
you've give and when you get.

00:21:28.910 --> 00:21:30.493
And so there are
number of researchers

00:21:30.493 --> 00:21:32.810
that have put hundreds of
people through these brain

00:21:32.810 --> 00:21:34.350
scans to come up
with a composite

00:21:34.350 --> 00:21:36.680
and see what happens
on the average.

00:21:36.680 --> 00:21:38.440
And it turns out
that when you give,

00:21:38.440 --> 00:21:39.920
you actually
stimulate the pleasure

00:21:39.920 --> 00:21:42.900
center of your brain, which is
the same pleasure center that

00:21:42.900 --> 00:21:45.810
is stimulated when you
eat chocolate or ice cream

00:21:45.810 --> 00:21:49.680
or when you flirt,
when you fall in love,

00:21:49.680 --> 00:21:52.000
also when you get
addicted to drugs.

00:21:52.000 --> 00:21:54.350
It is that addiction
center, too, which, I guess,

00:21:54.350 --> 00:21:58.030
means that you might be able
to be addicted to giving.

00:21:58.030 --> 00:22:02.910
But they also found that in
half or even sometimes more

00:22:02.910 --> 00:22:07.720
subjects, those people feel
more pleasure when they actually

00:22:07.720 --> 00:22:11.745
give than when they get,
which is very interesting.

00:22:11.745 --> 00:22:13.870
STEVE GROVE: So you were
wheeled into this scanner.

00:22:13.870 --> 00:22:16.370
And you were shown
images of organizations

00:22:16.370 --> 00:22:19.415
you wanted to give to, and
then it just checked your brain

00:22:19.415 --> 00:22:21.540
for how you were responding
to those opportunities.

00:22:21.540 --> 00:22:21.840
NICK KRISTOF: That's right.

00:22:21.840 --> 00:22:24.150
We had a screen in front
of us and a clicker.

00:22:24.150 --> 00:22:28.240
And so we were
told, OK, you have

00:22:28.240 --> 00:22:29.780
been given x amount of money.

00:22:29.780 --> 00:22:31.480
And then they're monitoring
the nucleus accumbens

00:22:31.480 --> 00:22:32.938
in these pleasure
centers, and they

00:22:32.938 --> 00:22:34.094
see how much they light up.

00:22:34.094 --> 00:22:35.510
And then you're
asked, do you want

00:22:35.510 --> 00:22:39.610
to give $20 to Helen Keller
International, for example?

00:22:39.610 --> 00:22:40.791
And you can say yes or no.

00:22:40.791 --> 00:22:43.040
And they see, when you say
yes, what that does, again,

00:22:43.040 --> 00:22:45.630
to the nucleus accumbens and,
again, how much it lights up.

00:22:45.630 --> 00:22:50.080
And this really confirms
this broader lesson,

00:22:50.080 --> 00:22:52.920
which goes back to
ancient scripture,

00:22:52.920 --> 00:22:54.960
that it is more blessed
to give than to receive.

00:22:54.960 --> 00:22:58.380
And it really does have
modern neuroscience behind it

00:22:58.380 --> 00:23:00.780
and also population studies.

00:23:00.780 --> 00:23:03.200
One of my favorite
studies looked

00:23:03.200 --> 00:23:06.040
at a population of
seniors and found

00:23:06.040 --> 00:23:09.950
that those who joined a
religious organization,

00:23:09.950 --> 00:23:11.860
went regularly to
religious services,

00:23:11.860 --> 00:23:15.690
their mortality risk for
that year dropped 29%.

00:23:15.690 --> 00:23:18.710
Those who exercise
regularly drop 30%.

00:23:18.710 --> 00:23:22.110
Those who volunteered for
multiple organizations,

00:23:22.110 --> 00:23:25.590
their mortality risk
that year dropped 44%.

00:23:25.590 --> 00:23:27.740
And presumably,
if you just manage

00:23:27.740 --> 00:23:31.380
to volunteer for a religious
running organization--

00:23:31.380 --> 00:23:32.710
[LAUGHTER]

00:23:32.710 --> 00:23:33.460
--you're immortal.

00:23:33.460 --> 00:23:35.250
STEVE GROVE: You hit
the jackpot, yeah.

00:23:35.250 --> 00:23:37.400
The perfect storm.

00:23:37.400 --> 00:23:40.120
You're talking a lot about
sort of the personal connection

00:23:40.120 --> 00:23:41.720
to the person that
you're giving to.

00:23:41.720 --> 00:23:43.960
And you write in the book
about having proximity

00:23:43.960 --> 00:23:47.190
to someone else's pain.

00:23:47.190 --> 00:23:49.965
You talk about walking through,
for example, camps in Darfur,

00:23:49.965 --> 00:23:52.340
where families are giving
almost their whole life to help

00:23:52.340 --> 00:23:55.040
their neighbor out, when someone
far away might not do that.

00:23:55.040 --> 00:23:56.831
It doesn't make them
more sort of a person,

00:23:56.831 --> 00:24:00.170
they just can't feel
the power of that story.

00:24:00.170 --> 00:24:02.695
Society has also
become more unequal,

00:24:02.695 --> 00:24:05.070
and so people don't have access
to some of those stories.

00:24:05.070 --> 00:24:05.950
I wonder if you
think about internet

00:24:05.950 --> 00:24:08.040
and the power of social
media and the internet

00:24:08.040 --> 00:24:11.270
to give people access to
stories of people who need help?

00:24:11.270 --> 00:24:12.970
Is that increasing,
do you think,

00:24:12.970 --> 00:24:16.547
people's ability to sympathize
or is it desensitizing us?

00:24:16.547 --> 00:24:18.380
What do you make of the
internet's influence

00:24:18.380 --> 00:24:22.100
on people's desire and
propensity to give?

00:24:22.100 --> 00:24:24.880
SHERYL WUDUNN: You know, I
think that it helps, certainly.

00:24:24.880 --> 00:24:28.230
But I think that
also in real life,

00:24:28.230 --> 00:24:31.260
you want to have a personal
sort of visualization

00:24:31.260 --> 00:24:32.860
or meet someone in person.

00:24:32.860 --> 00:24:34.920
I think that really
makes a big difference.

00:24:34.920 --> 00:24:38.370
Because right now, I think
the logic for researchers

00:24:38.370 --> 00:24:40.492
who have examined
this phenomenon,

00:24:40.492 --> 00:24:42.950
they're saying, well, why is
it that people who are wealthy

00:24:42.950 --> 00:24:44.880
tend to be less?

00:24:44.880 --> 00:24:50.790
Actually, it turns out
that 20% of the people,

00:24:50.790 --> 00:24:52.800
with the lowest income,
they basically give

00:24:52.800 --> 00:24:54.970
a larger percentage
of their income

00:24:54.970 --> 00:24:57.515
to charity than
people at the top 20%.

00:24:57.515 --> 00:24:59.640
And so it's not as though
the people at the top 20%

00:24:59.640 --> 00:25:00.598
are less compassionate.

00:25:00.598 --> 00:25:03.430
It's just that one
explanation is that they just

00:25:03.430 --> 00:25:05.640
are not surrounded by need.

00:25:05.640 --> 00:25:09.570
And when we do better,
we become more wealthy,

00:25:09.570 --> 00:25:11.560
we move into houses,
we get bigger houses,

00:25:11.560 --> 00:25:14.700
we get bigger plots of land,
maybe a fence around it,

00:25:14.700 --> 00:25:16.875
so we're kind of fenced
out of the need around us.

00:25:16.875 --> 00:25:18.250
So we live in nice
neighborhoods.

00:25:18.250 --> 00:25:21.400
So we almost never interact
with people in need.

00:25:21.400 --> 00:25:24.860
And so when you are actually
in a poor neighborhood,

00:25:24.860 --> 00:25:28.360
you are on the lower
income spectrum,

00:25:28.360 --> 00:25:29.914
you see need all the time.

00:25:29.914 --> 00:25:31.330
And so you feel
obligated to give.

00:25:31.330 --> 00:25:32.620
You feel compassionate.

00:25:32.620 --> 00:25:34.610
It moves you to give.

00:25:34.610 --> 00:25:37.730
Susan Fiske, who is a researcher
at Princeton University,

00:25:37.730 --> 00:25:40.090
she's looked at
successful people.

00:25:40.090 --> 00:25:41.940
And she looked at
their brain scans

00:25:41.940 --> 00:25:45.820
of what happens when
they are shown pictures

00:25:45.820 --> 00:25:48.040
of people who are homeless.

00:25:48.040 --> 00:25:52.500
And they see those
people as objects

00:25:52.500 --> 00:25:54.260
not even as human beings.

00:25:54.260 --> 00:25:56.680
So in other words, she
calls it the "otherization"

00:25:56.680 --> 00:25:57.647
of these people.

00:25:57.647 --> 00:25:59.480
The wealthy people and
the successful people

00:25:59.480 --> 00:26:03.950
are "otherizing," turning into
objects people who are really

00:26:03.950 --> 00:26:04.590
human beings.

00:26:04.590 --> 00:26:06.339
So that's a phenomena.

00:26:06.339 --> 00:26:07.880
NICK KRISTOF: I'm
skeptical, frankly,

00:26:07.880 --> 00:26:09.226
that the internet changes that.

00:26:09.226 --> 00:26:11.100
I mean I think partly
through self-selection,

00:26:11.100 --> 00:26:13.210
I think maybe empathetic,
compassionate people

00:26:13.210 --> 00:26:22.770
will seek out depictions
of people in need.

00:26:22.770 --> 00:26:25.120
And their empathy
will be reinforced.

00:26:25.120 --> 00:26:28.210
I think less
empathetic people will

00:26:28.210 --> 00:26:30.610
manage to avoid
those depictions.

00:26:30.610 --> 00:26:33.470
We in the news media
could be quite useful.

00:26:33.470 --> 00:26:35.630
Especially television
could be quite powerful

00:26:35.630 --> 00:26:37.700
if we projected
into living rooms

00:26:37.700 --> 00:26:43.670
or onto your screen the story's
of disadvantaged people.

00:26:43.670 --> 00:26:46.707
But I think that we
historically have not

00:26:46.707 --> 00:26:47.790
done a good job with that.

00:26:47.790 --> 00:26:51.340
And I think that now as the
business model of television

00:26:51.340 --> 00:26:53.590
collapses, and everybody
is desperate for ratings,

00:26:53.590 --> 00:26:55.465
I think that's going to
be substantially less

00:26:55.465 --> 00:26:58.640
likely in the next 15 years.

00:26:58.640 --> 00:27:02.840
One of the things I found really
dispiriting, as a journalist,

00:27:02.840 --> 00:27:05.430
was that the Bill and
Melinda Gates Foundation

00:27:05.430 --> 00:27:09.580
had essentially bribed
ABC News to cover

00:27:09.580 --> 00:27:13.145
global nutrition, global health.

00:27:13.145 --> 00:27:14.350
It wasn't called a bribe.

00:27:14.350 --> 00:27:15.300
It was called a grant.

00:27:15.300 --> 00:27:17.000
[LAUGHTER]

00:27:17.000 --> 00:27:19.520
And it was controversial within
the development community,

00:27:19.520 --> 00:27:21.228
because why should
Bill and Melinda Gates

00:27:21.228 --> 00:27:27.554
be giving money not to vaccinate
children or to provide bed nets

00:27:27.554 --> 00:27:28.970
or to save lives
in some other way

00:27:28.970 --> 00:27:31.350
but rather to pay overpaid
television executives to do

00:27:31.350 --> 00:27:32.740
what they should
be doing anyway?

00:27:32.740 --> 00:27:34.702
But in fact, after
a year, it was

00:27:34.702 --> 00:27:35.910
regarded as quite successful.

00:27:35.910 --> 00:27:39.850
ABC News did some really
good coverage of nutrition,

00:27:39.850 --> 00:27:42.120
of maternal health.

00:27:42.120 --> 00:27:44.350
And it projected those
issues on the agenda

00:27:44.350 --> 00:27:46.760
in ways that would get
more resources to them.

00:27:46.760 --> 00:27:48.600
So after a year,
Gates Foundation

00:27:48.600 --> 00:27:50.240
was ready to renew the bribe.

00:27:50.240 --> 00:27:51.425
And ABC News said, no.

00:27:51.425 --> 00:27:52.800
We don't want to
take your money.

00:27:52.800 --> 00:27:57.480
Because we see that when
we air these pieces,

00:27:57.480 --> 00:28:00.680
then that's when viewers
switch the channel.

00:28:00.680 --> 00:28:03.090
I find that just
really discouraging.

00:28:03.090 --> 00:28:06.850
And I think that television
and other news organizations

00:28:06.850 --> 00:28:10.180
are going to take the hint that
that is not what viewers want.

00:28:10.180 --> 00:28:13.291
And that you can make more money
and get higher ratings if you

00:28:13.291 --> 00:28:15.540
put a Democrat and a Republican
in the studio together

00:28:15.540 --> 00:28:17.290
and have them yell
at each other.

00:28:17.290 --> 00:28:20.320
And if that's the
case, then these issues

00:28:20.320 --> 00:28:23.350
just aren't going to
get on the agenda.

00:28:23.350 --> 00:28:26.190
And if that's the case, they're
not going to get addressed.

00:28:26.190 --> 00:28:27.000
STEVE GROVE: What
have you seen that has

00:28:27.000 --> 00:28:29.000
been effective to get
these issues on the agenda

00:28:29.000 --> 00:28:32.180
and cut through the clutter
of the chaos on line

00:28:32.180 --> 00:28:33.910
or on television?

00:28:33.910 --> 00:28:37.170
SHERYL WUDUNN: Well, advocacy
is one, really important tool.

00:28:37.170 --> 00:28:39.510
And I think that
particularly companies, they

00:28:39.510 --> 00:28:42.690
have a huge influence
when they actually

00:28:42.690 --> 00:28:44.610
do advocate for issues.

00:28:44.610 --> 00:28:47.370
And so if companies
are willing to take

00:28:47.370 --> 00:28:49.910
on some of these
issues, that actually

00:28:49.910 --> 00:28:51.580
just improves society
around them, too.

00:28:51.580 --> 00:28:53.940
I mean, it doesn't have to be at
the really, really local level.

00:28:53.940 --> 00:28:55.564
It could be at the
more national level.

00:28:55.564 --> 00:29:00.811
But ultimately, it also helps
enhance the brand of a company

00:29:00.811 --> 00:29:02.310
if they take on
some of these issues

00:29:02.310 --> 00:29:04.200
that most people
don't disagree with.

00:29:04.200 --> 00:29:06.220
It's just that it
needs to get help

00:29:06.220 --> 00:29:09.030
to be pushed through sort
of the Washington mechanism.

00:29:09.030 --> 00:29:12.820
And so I do think that
advocacy is one area.

00:29:12.820 --> 00:29:16.210
And individuals can also
play a role in advocacy.

00:29:16.210 --> 00:29:19.210
We often think, oh, how can one
person make such a difference?

00:29:19.210 --> 00:29:21.690
Well, in the same way
that you go out and vote,

00:29:21.690 --> 00:29:24.234
and your vote, amassed
with all the others,

00:29:24.234 --> 00:29:25.775
it does ultimately
make a difference,

00:29:25.775 --> 00:29:27.120
the same thing with advocacy.

00:29:27.120 --> 00:29:29.370
I mean if you write-- this
sounds very old-fashioned--

00:29:29.370 --> 00:29:33.190
but if you write to your
congressperson, man or woman,

00:29:33.190 --> 00:29:33.940
they want to hear.

00:29:33.940 --> 00:29:37.480
I mean they, obviously, want
to hear a lot of voices.

00:29:37.480 --> 00:29:40.530
And now there are
petition mechanisms,

00:29:40.530 --> 00:29:42.940
through change.org
or moveon.org,

00:29:42.940 --> 00:29:47.210
a lot of ways that you can
actually get lots of signatures

00:29:47.210 --> 00:29:50.250
to send something to
your congressperson,

00:29:50.250 --> 00:29:51.296
to get a message across.

00:29:51.296 --> 00:29:52.920
They need to hear
this, because if they

00:29:52.920 --> 00:29:55.587
don't hear from their
constituents, then, of course,

00:29:55.587 --> 00:29:57.920
they're not going to think
that it's an important issue.

00:29:57.920 --> 00:30:00.372
And they won't put
theirselves on the line.

00:30:00.372 --> 00:30:02.080
NICK KRISTOF: I've
also become a believer

00:30:02.080 --> 00:30:05.840
in selectivism, somewhat
against my better judgment.

00:30:10.610 --> 00:30:15.080
I found the Joseph Kony
campaign kind of fascinating.

00:30:15.080 --> 00:30:18.950
And it was much criticized
for lots of reasons.

00:30:18.950 --> 00:30:20.989
But I found it just
astonishing that

00:30:20.989 --> 00:30:22.530
here's a story, the
Lord's Resistance

00:30:22.530 --> 00:30:26.870
Army in northern Uganda,
a Central African

00:30:26.870 --> 00:30:30.100
Republic, Congo, that
basically we in the news media

00:30:30.100 --> 00:30:30.980
didn't cover.

00:30:30.980 --> 00:30:33.350
And all of the sudden,
every school kid in America

00:30:33.350 --> 00:30:34.930
was passionate about the issue.

00:30:34.930 --> 00:30:40.450
And the upshot
was more resources

00:30:40.450 --> 00:30:44.780
going to the anti LRA campaign.

00:30:44.780 --> 00:30:49.210
The number two under Kony
has just surrendered.

00:30:49.210 --> 00:30:52.300
Attacks by the LRA have
dropped more than 90%.

00:30:55.060 --> 00:31:01.990
And I think that there are
some kinds of storytelling that

00:31:01.990 --> 00:31:03.410
just work from the ground up.

00:31:03.410 --> 00:31:05.380
And as we in the news
media drop the ball

00:31:05.380 --> 00:31:07.130
on some of these
stories, sometimes people

00:31:07.130 --> 00:31:09.130
are able to pick them
up and spread the word.

00:31:09.130 --> 00:31:11.670
And people want to participate.

00:31:11.670 --> 00:31:14.860
The "Bring Back Our Girls"
campaign was another example.

00:31:14.860 --> 00:31:19.080
Now, it obviously did not bring
back those 200 plus Nigerian

00:31:19.080 --> 00:31:20.120
girls.

00:31:20.120 --> 00:31:24.256
I think that it did get
the Nigerian government

00:31:24.256 --> 00:31:25.880
and some other
governments more focused

00:31:25.880 --> 00:31:29.340
on the idea of educating
girls, an incredibly

00:31:29.340 --> 00:31:32.310
disenfranchised part
of those populations.

00:31:32.310 --> 00:31:34.120
So here and there,
there may be ways

00:31:34.120 --> 00:31:39.290
for effective storytelling,
effective grassroots campaigns

00:31:39.290 --> 00:31:46.700
by people to kind of keep issues
alive in ways that raise issues

00:31:46.700 --> 00:31:50.510
on a global agenda, get
resources, and bring

00:31:50.510 --> 00:31:52.265
some results if
imperfect results.

00:31:52.265 --> 00:31:54.640
STEVE GROVE: Yeah, I guess
your point is that there's not

00:31:54.640 --> 00:31:56.220
really a downside
to selectivism.

00:31:56.220 --> 00:31:58.110
Selectivism is doing
something that you

00:31:58.110 --> 00:32:00.610
think is going to help, and
it's fairly sort of empty,

00:32:00.610 --> 00:32:02.735
but it might create some
momentum around an action.

00:32:02.735 --> 00:32:04.568
SHERYL WUDUNN: Well,
certainly an awareness.

00:32:04.568 --> 00:32:06.180
I mean that's
really hard to break

00:32:06.180 --> 00:32:10.419
through all the stuff that
comes out-- sorry-- on the web.

00:32:10.419 --> 00:32:11.960
I mean I think it
really is important

00:32:11.960 --> 00:32:13.793
that, if you can actually
break through that

00:32:13.793 --> 00:32:16.040
and raise awareness
about girls' education

00:32:16.040 --> 00:32:19.984
in Nigeria, after all, I mean
that's a real accomplishment.

00:32:19.984 --> 00:32:22.400
STEVE GROVE: We want to turn
to our audience for questions

00:32:22.400 --> 00:32:22.900
in a moment.

00:32:22.900 --> 00:32:26.040
I'm wondering what you think
tech companies, like Google,

00:32:26.040 --> 00:32:30.410
could be doing more effectively
to help people access

00:32:30.410 --> 00:32:32.225
opportunities to
make a difference?

00:32:32.225 --> 00:32:34.100
SHERYL WUDUNN: Well, I
think that we actually

00:32:34.100 --> 00:32:36.350
have a lot of ideas
in "A Path Appears."

00:32:36.350 --> 00:32:38.932
and I think that, first of
all, at an individual level,

00:32:38.932 --> 00:32:40.640
I think it's important
for people to say,

00:32:40.640 --> 00:32:45.696
hey, if they like what
they learn about or read,

00:32:45.696 --> 00:32:47.570
whatever, then to say,
hey, this is something

00:32:47.570 --> 00:32:48.560
that I do care about.

00:32:48.560 --> 00:32:51.370
I want to play a
little bit of a role.

00:32:51.370 --> 00:32:53.700
You can start out
very, very small.

00:32:53.700 --> 00:32:56.600
And I mean I think that's what's
so important about what we're

00:32:56.600 --> 00:32:58.110
trying to convey
is that this is not

00:32:58.110 --> 00:32:59.800
something you have
to drop everything

00:32:59.800 --> 00:33:02.050
and then go and do, go
off to India, whatever.

00:33:02.050 --> 00:33:04.150
This is something you can
do in bite-sized pieces.

00:33:04.150 --> 00:33:06.140
Because I think that
as long as everyone,

00:33:06.140 --> 00:33:11.469
or many people together, devote
a little bit of their time--

00:33:11.469 --> 00:33:13.760
I mean an hour a month, start
out with an hour a month.

00:33:13.760 --> 00:33:16.800
It can be really,
really manageable.

00:33:16.800 --> 00:33:18.590
Over time, I think
you'll find that you'll

00:33:18.590 --> 00:33:22.352
get a lot more out of it.

00:33:22.352 --> 00:33:23.810
There has been,
also, research that

00:33:23.810 --> 00:33:30.720
shows that, when you give
back, maybe on a Saturday,

00:33:30.720 --> 00:33:33.150
on a typical Saturday,
once a week, you really

00:33:33.150 --> 00:33:37.560
can take on more sort
of work pressure,

00:33:37.560 --> 00:33:42.020
because that gives you a way
to accommodate all the pressure

00:33:42.020 --> 00:33:44.470
you feel in the
rest of your world.

00:33:44.470 --> 00:33:47.810
So that it really does
alleviate some of the burden.

00:33:47.810 --> 00:33:50.380
And that's partly because
of the neuroscience

00:33:50.380 --> 00:33:52.680
that we talked about,
that when you give,

00:33:52.680 --> 00:33:56.250
you're actually doing something
in your brain that actually

00:33:56.250 --> 00:33:57.290
gives you pleasure.

00:33:57.290 --> 00:34:01.190
And so it's kind of a relief
from your workaday world.

00:34:01.190 --> 00:34:04.200
So it's not only
good for society,

00:34:04.200 --> 00:34:05.800
but it's also good for you.

00:34:05.800 --> 00:34:08.210
And I think that our
approach is really

00:34:08.210 --> 00:34:11.690
to start small and
do it with friends.

00:34:11.690 --> 00:34:14.139
And then maybe, if you
can engage the company

00:34:14.139 --> 00:34:18.947
to do something all together,
that's even more powerful.

00:34:18.947 --> 00:34:21.530
NICK KRISTOF: I would just add
that I think corporate America,

00:34:21.530 --> 00:34:24.120
generally, and the tech
sector, in particular,

00:34:24.120 --> 00:34:27.530
has done a pretty lousy
job with corporate social

00:34:27.530 --> 00:34:30.010
responsibility, CSR
programs, that these

00:34:30.010 --> 00:34:32.409
tend to be window dressing,
these tend to be off,

00:34:32.409 --> 00:34:34.860
foisted on the side.

00:34:34.860 --> 00:34:36.620
The people in them,
very often, are

00:34:36.620 --> 00:34:39.050
people who haven't made
it in the mainstream

00:34:39.050 --> 00:34:40.150
of the organization.

00:34:40.150 --> 00:34:44.239
They're not going to then go
on and ever run the company.

00:34:44.239 --> 00:34:48.960
And not only do they not do
much in terms of really making

00:34:48.960 --> 00:34:50.460
a difference in
society, they don't

00:34:50.460 --> 00:34:52.139
do much for recruitment,
for retention,

00:34:52.139 --> 00:34:53.630
for morale of the company.

00:34:53.630 --> 00:34:56.949
And I think especially
for a company that

00:34:56.949 --> 00:35:00.420
cares about its brand, that
cares about millennials--

00:35:00.420 --> 00:35:04.560
I think millennials, much
more than my generation,

00:35:04.560 --> 00:35:06.320
when they're looking
for a place to work,

00:35:06.320 --> 00:35:09.150
they want a company
that has some values.

00:35:09.150 --> 00:35:10.870
I think increasingly
the consumers

00:35:10.870 --> 00:35:16.630
themselves will be making
decisions about charity, if you

00:35:16.630 --> 00:35:20.430
will, not just writing
charitable donation checks

00:35:20.430 --> 00:35:22.680
at the end of the year, but
in terms of their consumer

00:35:22.680 --> 00:35:25.350
choices, maybe in terms of
their investment choices.

00:35:25.350 --> 00:35:29.960
And I don't think this has sunk
into CEOs around the country,

00:35:29.960 --> 00:35:36.540
generally, that CSR could be
a very powerful way, if they

00:35:36.540 --> 00:35:41.230
actually do something, to
really improve, to really change

00:35:41.230 --> 00:35:44.080
the dynamic within the company
as well, at the same time,

00:35:44.080 --> 00:35:46.291
to really make a
difference beyond.

00:35:46.291 --> 00:35:48.415
SHERYL WUDUNN: Of course,
present company excluded.

00:35:52.160 --> 00:35:54.790
There have been some really
interesting examples.

00:35:54.790 --> 00:35:57.240
For instance, a
major accounting firm

00:35:57.240 --> 00:36:01.230
was noticing that the turnover
rate among their millennials

00:36:01.230 --> 00:36:04.050
was much higher
than among everyone

00:36:04.050 --> 00:36:05.050
else in the corporation.

00:36:05.050 --> 00:36:07.100
And they just wondering,
what's going on?

00:36:07.100 --> 00:36:09.770
And so they started doing
interviews and investigating.

00:36:09.770 --> 00:36:13.724
And they discovered that a
lot of the younger recruits

00:36:13.724 --> 00:36:15.640
were saying, well, if
I'm going to be spending

00:36:15.640 --> 00:36:18.440
16 hours of my day
at a company, I

00:36:18.440 --> 00:36:21.600
want to know what kind of value
is it contributing to society.

00:36:21.600 --> 00:36:23.260
And an accounting
firm, what I can say?

00:36:23.260 --> 00:36:24.380
What kind of value?

00:36:24.380 --> 00:36:25.840
And so they had to work at that.

00:36:25.840 --> 00:36:28.140
They actually had
to really create

00:36:28.140 --> 00:36:32.810
programs that allowed
a lot of the young,

00:36:32.810 --> 00:36:35.660
or anybody in the company,
to try and give back

00:36:35.660 --> 00:36:39.040
or to help out, do some
accounting for a nonprofit.

00:36:39.040 --> 00:36:41.890
They had to devise ways to
actually show that there really

00:36:41.890 --> 00:36:46.040
was a place in society
where these organizations

00:36:46.040 --> 00:36:48.724
were playing a really
fundamental, important role.

00:36:48.724 --> 00:36:50.390
And I thought that
was very interesting.

00:36:50.390 --> 00:36:52.837
And I think that
this just happened

00:36:52.837 --> 00:36:54.670
to be a case study for
this accounting firm.

00:36:54.670 --> 00:36:56.253
But I think this is
probably happening

00:36:56.253 --> 00:36:59.939
across the corporate spectrum.

00:36:59.939 --> 00:37:00.730
STEVE GROVE: Great.

00:37:00.730 --> 00:37:03.090
Well, why don't we turn to
our audience for questions

00:37:03.090 --> 00:37:03.750
from those who are here.

00:37:03.750 --> 00:37:04.800
I think we have a
few roving mics.

00:37:04.800 --> 00:37:06.175
Just throw your
hand up in there,

00:37:06.175 --> 00:37:09.300
and we'll pass a mic over.

00:37:09.300 --> 00:37:12.552
NICK KRISTOF: Easy questions to
me, hard questions to Sheryl.

00:37:12.552 --> 00:37:14.260
AUDIENCE: Hi, my name
is Danielle Bauers.

00:37:14.260 --> 00:37:18.980
And I found your comment
about Bill Gates and ABC

00:37:18.980 --> 00:37:20.650
extremely interesting.

00:37:20.650 --> 00:37:22.590
So I'm curious,
with all the studies

00:37:22.590 --> 00:37:26.700
that you guys have done,
what would you recommend ABC?

00:37:26.700 --> 00:37:28.970
How would you recommend
they tell that story that

00:37:28.970 --> 00:37:32.530
might not sadden viewers
and might actually

00:37:32.530 --> 00:37:35.180
inspire them to act?

00:37:35.180 --> 00:37:38.240
NICK KRISTOF: I think there are
two lessons from the data about

00:37:38.240 --> 00:37:38.800
what works.

00:37:38.800 --> 00:37:44.420
One is that you have
to open up the pathway

00:37:44.420 --> 00:37:49.380
with an individual story and
make an emotional connection

00:37:49.380 --> 00:37:50.950
before you make a rational one.

00:37:50.950 --> 00:37:57.640
So you find one, particular,
very, very compelling anecdote.

00:37:57.640 --> 00:37:59.560
And then you could
broaden it out from there.

00:37:59.560 --> 00:38:02.920
And the second one it has is it
has to ultimately be uplifting,

00:38:02.920 --> 00:38:04.580
to show that change is possible.

00:38:04.580 --> 00:38:09.800
And so there can be a
desperate part of it,

00:38:09.800 --> 00:38:12.870
but the end should
be this arc that

00:38:12.870 --> 00:38:16.870
shows that there can be an
impact if people do something.

00:38:16.870 --> 00:38:20.810
So the dog can be desperately
unhappy for a moment,

00:38:20.810 --> 00:38:23.320
but then has to emerge
as this super dog,

00:38:23.320 --> 00:38:25.840
because somebody
cared about that dog.

00:38:25.840 --> 00:38:27.850
And then you have to
show, after people

00:38:27.850 --> 00:38:31.390
are feeling compassionate for
that dog, then you have to say,

00:38:31.390 --> 00:38:34.590
OK, there are x zillion dogs
out there in the same situation.

00:38:34.590 --> 00:38:36.590
And once people have
had that connection,

00:38:36.590 --> 00:38:38.180
then it's OK to
throw in some data.

00:38:38.180 --> 00:38:38.880
SHERYL WUDUNN: And
sometimes that's

00:38:38.880 --> 00:38:40.360
really tricky,
because when you get

00:38:40.360 --> 00:38:43.290
to the news and the
newsrooms, they're

00:38:43.290 --> 00:38:44.970
like, we just want
to report the facts.

00:38:44.970 --> 00:38:48.230
We can't manipulate.

00:38:48.230 --> 00:38:52.400
But there is some
degree of, so to speak,

00:38:52.400 --> 00:38:54.870
manipulation just by
virtue of the facts

00:38:54.870 --> 00:38:56.550
that you select to put in there.

00:38:56.550 --> 00:39:00.490
But it is true that there's been
a hesitancy among reporters.

00:39:00.490 --> 00:39:02.670
They say, well, I'm not
going do this arc thing,

00:39:02.670 --> 00:39:05.392
because it's it isn't
really what's there.

00:39:05.392 --> 00:39:06.850
And we have to
report what's there.

00:39:06.850 --> 00:39:09.240
So there's a little
bit of a tension.

00:39:09.240 --> 00:39:12.240
STEVE GROVE: Other questions?

00:39:12.240 --> 00:39:14.850
AUDIENCE: So, as
you know, Google

00:39:14.850 --> 00:39:18.720
is constantly trying to
optimize user experience based

00:39:18.720 --> 00:39:20.860
on what we know about users.

00:39:20.860 --> 00:39:24.020
And there's always a certain
amount of background noise

00:39:24.020 --> 00:39:27.600
out in the world about Google
being manipulative as opposed

00:39:27.600 --> 00:39:29.592
to doing this as a
service to the users.

00:39:29.592 --> 00:39:31.050
There's always this
sort of tension

00:39:31.050 --> 00:39:34.530
about how this is perceived.

00:39:34.530 --> 00:39:37.050
It might seem like a
no-brainer that trying

00:39:37.050 --> 00:39:39.040
to get to know the users
in order to sell them

00:39:39.040 --> 00:39:41.110
toothpaste is perhaps
not as noble as trying

00:39:41.110 --> 00:39:42.610
to get to know the
user so there you

00:39:42.610 --> 00:39:45.920
can get them to contribute
to worthy causes.

00:39:45.920 --> 00:39:47.630
But is there still
an issue there

00:39:47.630 --> 00:39:50.290
about how far an
organization like Google

00:39:50.290 --> 00:39:53.260
should go before it
becomes manipulative

00:39:53.260 --> 00:39:55.390
and we're basically
convincing people

00:39:55.390 --> 00:39:57.999
to do things that they
otherwise wouldn't do?

00:39:57.999 --> 00:40:00.165
SHERYL WUDUNN: That's a
really interesting question.

00:40:03.400 --> 00:40:06.370
People say, well, everyone has
different definitions of good.

00:40:06.370 --> 00:40:10.640
And so, it'll be, well,
my definition of good

00:40:10.640 --> 00:40:12.330
is very fair and balanced.

00:40:12.330 --> 00:40:14.460
And your definition
of good is not,

00:40:14.460 --> 00:40:17.400
so therefore you shouldn't
be manipulating all the way

00:40:17.400 --> 00:40:18.110
to the end.

00:40:18.110 --> 00:40:21.980
But I think that I would
set that bar pretty high,

00:40:21.980 --> 00:40:24.070
that you really can do
a lot more before you

00:40:24.070 --> 00:40:26.550
get to the point where
you're really hurting.

00:40:26.550 --> 00:40:28.370
I mean, obviously,
at some point,

00:40:28.370 --> 00:40:30.520
you're going to be doing
harm, so whether it's

00:40:30.520 --> 00:40:36.870
in invading privacy
or whether it's

00:40:36.870 --> 00:40:39.180
excluding other voices
from getting in.

00:40:39.180 --> 00:40:40.721
I mean there a number
of things you'd

00:40:40.721 --> 00:40:45.450
have to also analyze as
to what the downside is

00:40:45.450 --> 00:40:48.270
from this extra, so to
speak, manipulation.

00:40:48.270 --> 00:40:52.121
But I think that I would
set that bar pretty high.

00:40:52.121 --> 00:40:54.245
NICK KRISTOF: In general,
I'm wary of manipulation.

00:40:58.939 --> 00:41:01.230
I mean one a rare case where
I think I'd be sympathetic

00:41:01.230 --> 00:41:07.019
would be in dramas
and television dramas.

00:41:07.019 --> 00:41:08.560
I think there's
pretty good evidence.

00:41:08.560 --> 00:41:11.420
I mean "16 and Pregnant,"
for example, the MTV show,

00:41:11.420 --> 00:41:12.910
seems to have
dramatically reduced

00:41:12.910 --> 00:41:16.210
teen pregnancy in America by
kind of reminding teenagers

00:41:16.210 --> 00:41:20.930
just how much of a hassle
it is to be a mom at 16.

00:41:20.930 --> 00:41:24.060
And there is, likewise,
evidence from India

00:41:24.060 --> 00:41:27.120
that television
arriving in a village,

00:41:27.120 --> 00:41:29.520
bringing in kind of middle
class values to a village

00:41:29.520 --> 00:41:33.970
is equivalent to 6 years'
education in terms of improving

00:41:33.970 --> 00:41:36.440
women's rights in that village.

00:41:36.440 --> 00:41:38.900
I must say, as a
scribbler who tends

00:41:38.900 --> 00:41:41.220
to have a great deal of
disdain for television,

00:41:41.220 --> 00:41:43.750
this was really hard
for me to accept.

00:41:43.750 --> 00:41:48.530
But I think you can make a
case in those cases, maybe.

00:41:48.530 --> 00:41:51.110
But in general, I
think that one should

00:41:51.110 --> 00:41:57.550
be wary of being didactic,
of manipulating results

00:41:57.550 --> 00:42:00.150
for kind of social good.

00:42:00.150 --> 00:42:01.955
I think it does breed backlash.

00:42:01.955 --> 00:42:03.330
SHERYL WUDUNN: I
think it depends

00:42:03.330 --> 00:42:06.350
on what you mean
by manipulation.

00:42:06.350 --> 00:42:10.710
We really would have to
sort of set some parameters.

00:42:10.710 --> 00:42:14.220
And it's not as though
you're deceiving people

00:42:14.220 --> 00:42:19.510
or you're crowding out
other voices, other counter

00:42:19.510 --> 00:42:21.951
voices that would say, well,
this isn't really the good

00:42:21.951 --> 00:42:22.700
that you're doing.

00:42:22.700 --> 00:42:24.160
I think it really just depends.

00:42:24.160 --> 00:42:29.310
And maybe manipulation
is too strong a word.

00:42:29.310 --> 00:42:29.880
I don't know.

00:42:29.880 --> 00:42:31.470
STEVE GROVE: Lot of people,
storytelling and manipulation,

00:42:31.470 --> 00:42:32.430
basically.

00:42:32.430 --> 00:42:35.180
SHERYL WUDUNN: Or advocacy.

00:42:35.180 --> 00:42:36.730
STEVE GROVE: Other questions?

00:42:36.730 --> 00:42:38.446
Yeah.

00:42:38.446 --> 00:42:41.070
Actually, where is the-- are we
just sort of passing it around?

00:42:41.070 --> 00:42:41.340
AUDIENCE: I have the mic.

00:42:41.340 --> 00:42:41.690
STEVE GROVE: There we go.

00:42:41.690 --> 00:42:44.049
AUDIENCE: And I'll pass it
to these people right next.

00:42:44.049 --> 00:42:45.590
So first of all, I
wanted to just say

00:42:45.590 --> 00:42:48.490
thank you so much for doing
the work you're doing.

00:42:48.490 --> 00:42:50.580
Just speaking, personally,
I did not use to give,

00:42:50.580 --> 00:42:53.670
and, because of an article
you wrote, I began to give.

00:42:53.670 --> 00:42:55.850
I mean, not that I
have that much money.

00:42:55.850 --> 00:42:59.802
But I think you've really
impacted people in your books

00:42:59.802 --> 00:43:01.010
that you've written together.

00:43:01.010 --> 00:43:02.690
And I've been wondering
this question a lot

00:43:02.690 --> 00:43:03.690
and asking some friends.

00:43:03.690 --> 00:43:05.780
And I'd love to ask you.

00:43:05.780 --> 00:43:08.150
Could you name like
5 or 10 charities

00:43:08.150 --> 00:43:10.806
that you think make
the most impact?

00:43:10.806 --> 00:43:12.430
Because some of the
charities-- the one

00:43:12.430 --> 00:43:15.650
you wrote about in an
article, Fistula Foundation,

00:43:15.650 --> 00:43:17.000
was a great example.

00:43:17.000 --> 00:43:18.980
Could you name 5 or 10 of them?

00:43:18.980 --> 00:43:20.530
SHERYL WUDUNN: In the back
of the book, we have a list.

00:43:20.530 --> 00:43:21.150
We have a whole list--

00:43:21.150 --> 00:43:21.590
AUDIENCE: Sorry.

00:43:21.590 --> 00:43:21.770
Well, I look forward to.

00:43:21.770 --> 00:43:22.030
SHERYL WUDUNN: --of them.

00:43:22.030 --> 00:43:22.738
Yeah, absolutely.

00:43:22.738 --> 00:43:27.520
But like Reach Out and Read
is a great organization

00:43:27.520 --> 00:43:28.740
that helps kids read.

00:43:28.740 --> 00:43:32.930
It basically prescribes books,
just like it does medicines,

00:43:32.930 --> 00:43:35.240
for parents to give
their children.

00:43:35.240 --> 00:43:38.200
And so it says, OK, you've
got to prescribe reading,

00:43:38.200 --> 00:43:41.400
30 minutes a day, to
even your six month old.

00:43:41.400 --> 00:43:43.250
So that's a typical
organization.

00:43:43.250 --> 00:43:44.910
But there are a
whole bunch, based

00:43:44.910 --> 00:43:46.809
on what your interests are.

00:43:46.809 --> 00:43:49.100
And what we really actually--
there are a couple things

00:43:49.100 --> 00:43:50.058
I would say about this.

00:43:50.058 --> 00:43:53.340
One is that it's really
going to focus on outcomes.

00:43:53.340 --> 00:43:56.380
A lot of organizations,
they'll say

00:43:56.380 --> 00:43:58.490
that they send 100 million
books out, whatever.

00:43:58.490 --> 00:44:00.630
But you know, did children
really receive them?

00:44:00.630 --> 00:44:02.060
Did they read the books?

00:44:02.060 --> 00:44:04.790
I think the focus on
outcomes is really important.

00:44:04.790 --> 00:44:06.990
The second thing
is that it really

00:44:06.990 --> 00:44:08.710
depends on what
you're interested in.

00:44:08.710 --> 00:44:10.964
I think that some
people, health care,

00:44:10.964 --> 00:44:12.880
maybe they're not as
interested in health care

00:44:12.880 --> 00:44:13.921
as they are in education.

00:44:13.921 --> 00:44:16.670
And so for us to say, well, you
should give to this health care

00:44:16.670 --> 00:44:19.510
organization, well, you it
may not do much for you.

00:44:19.510 --> 00:44:21.010
And so I think it's
really important

00:44:21.010 --> 00:44:23.679
to figure out what areas
you care about and then

00:44:23.679 --> 00:44:24.720
bring some friends along.

00:44:24.720 --> 00:44:26.220
Because when you
do it together, you

00:44:26.220 --> 00:44:27.980
can discuss some
of these issues.

00:44:27.980 --> 00:44:29.957
You can discuss
which organizations

00:44:29.957 --> 00:44:30.790
you want to give to.

00:44:30.790 --> 00:44:33.111
You can maybe have
rotating organizations.

00:44:33.111 --> 00:44:35.236
STEVE GROVE: Yeah, you
recommend in the book having

00:44:35.236 --> 00:44:36.820
like a giving club
with your friends.

00:44:36.820 --> 00:44:37.990
SHERYL WUDUNN: Exactly.

00:44:37.990 --> 00:44:38.730
Exactly.

00:44:38.730 --> 00:44:39.620
Exactly.

00:44:39.620 --> 00:44:41.120
Start off with a
book club, and then

00:44:41.120 --> 00:44:42.702
turn it into a giving club.

00:44:42.702 --> 00:44:43.910
STEVE GROVE: Other questions?

00:44:43.910 --> 00:44:45.134
Here.

00:44:45.134 --> 00:44:46.300
AUDIENCE: Thank you so much.

00:44:46.300 --> 00:44:48.630
Nick, I heard you speak at
an event, very eloquently,

00:44:48.630 --> 00:44:52.080
about the unfair bifurcation
between the noble nonprofit

00:44:52.080 --> 00:44:53.670
and the greedy for profit.

00:44:53.670 --> 00:44:56.040
And in my experience,
especially recently,

00:44:56.040 --> 00:44:57.760
a lot of people with
financial resources

00:44:57.760 --> 00:45:00.660
are more likely to invest
in social entrepreneurs, who

00:45:00.660 --> 00:45:03.270
may have for profit
companies, but they're

00:45:03.270 --> 00:45:05.480
working on amazing
contributions,

00:45:05.480 --> 00:45:08.910
from eradicating disease
to transforming education.

00:45:08.910 --> 00:45:11.390
That wouldn't move the
needle on that top 20%

00:45:11.390 --> 00:45:15.000
that you mentioned, necessarily
making nonprofit donations,

00:45:15.000 --> 00:45:17.130
but given that it's still
contributing, hopefully,

00:45:17.130 --> 00:45:18.963
to an efficient, more
productive world, what

00:45:18.963 --> 00:45:20.900
are your thoughts on that?

00:45:20.900 --> 00:45:24.170
NICK KRISTOF: This is really
Sheryl's world more than mine.

00:45:24.170 --> 00:45:26.930
Sheryl is precisely
kind of raising capital,

00:45:26.930 --> 00:45:30.830
often for for profit
companies that do good.

00:45:30.830 --> 00:45:33.690
SHERYL WUDUNN: I mean
in general, right now--

00:45:33.690 --> 00:45:36.250
in the past, we always
thought of the nonprofits

00:45:36.250 --> 00:45:38.190
as the engines of good.

00:45:38.190 --> 00:45:40.720
But right now, everything,
it's a free for all.

00:45:40.720 --> 00:45:43.340
There are a lot of corporations
that are doing great good.

00:45:43.340 --> 00:45:45.480
And there are actually
some nonprofits

00:45:45.480 --> 00:45:49.220
who aren't doing good,
maybe even doing harm.

00:45:49.220 --> 00:45:51.500
And so it's really
important that we also

00:45:51.500 --> 00:45:53.870
borrow from the for profit
world, so for instance

00:45:53.870 --> 00:45:55.020
marketing.

00:45:55.020 --> 00:45:56.420
In the nonprofit
world, marketing

00:45:56.420 --> 00:45:58.690
is considered an evil activity.

00:45:58.690 --> 00:46:00.260
Well, if you don't market?

00:46:00.260 --> 00:46:03.130
What better cause to market
than a really good social cause,

00:46:03.130 --> 00:46:03.720
right?

00:46:03.720 --> 00:46:06.220
And so I think that there's
really this melding,

00:46:06.220 --> 00:46:08.960
and there's just a mixing up
of all these different things.

00:46:08.960 --> 00:46:10.790
As long as there's
a social mission

00:46:10.790 --> 00:46:12.760
to a for profit
enterprise, they really

00:46:12.760 --> 00:46:16.130
can accomplish a great deal.

00:46:16.130 --> 00:46:19.130
And if it's incorporated
into the DNA of the company,

00:46:19.130 --> 00:46:22.370
than each widget that they sell
then accomplishes something.

00:46:22.370 --> 00:46:24.930
So for instance, health
care is really much easier

00:46:24.930 --> 00:46:26.767
to do that, because
supposedly, if you've

00:46:26.767 --> 00:46:29.350
developed this new medicine that
is helping people with asthma

00:46:29.350 --> 00:46:31.016
or a device that helps
them with asthma,

00:46:31.016 --> 00:46:33.200
each device that you
sell will help people.

00:46:33.200 --> 00:46:36.099
But at some point, when these
corporations get very large,

00:46:36.099 --> 00:46:37.890
there's a lot of tension,
because maybe you

00:46:37.890 --> 00:46:41.031
want to oversell, or maybe we're
getting into the manipulation

00:46:41.031 --> 00:46:41.530
here.

00:46:41.530 --> 00:46:43.220
Maybe you're trying to
manipulate things in order

00:46:43.220 --> 00:46:44.640
to get more product
out the door,

00:46:44.640 --> 00:46:47.040
because you have these
profit goals to make.

00:46:47.040 --> 00:46:49.080
So it does get tricky.

00:46:49.080 --> 00:46:51.180
But in general,
we think that it's

00:46:51.180 --> 00:46:53.570
a good thing that
for profit companies

00:46:53.570 --> 00:46:55.970
are actually beginning to
think about incorporating

00:46:55.970 --> 00:46:59.754
a social mission
into their companies.

00:46:59.754 --> 00:47:01.920
It's harder, because you've
got a social bottom line

00:47:01.920 --> 00:47:03.544
and you've got a
financial bottom line.

00:47:03.544 --> 00:47:05.264
And some, in the
environmental space,

00:47:05.264 --> 00:47:06.680
have an environmental
bottom line,

00:47:06.680 --> 00:47:08.310
so it's got getting
a lot harder.

00:47:08.310 --> 00:47:12.840
But we think that, hey, we
all have to make this place,

00:47:12.840 --> 00:47:16.030
we have to work to make
this world a better place.

00:47:16.030 --> 00:47:17.910
Because we have to live in it.

00:47:17.910 --> 00:47:20.044
But we think it's really
just a good trend.

00:47:20.044 --> 00:47:21.460
NICK KRISTOF: One
thing I'd add is

00:47:21.460 --> 00:47:28.080
that philanthropy,
whether it's to nonprofits

00:47:28.080 --> 00:47:31.630
or a to for profits,
with a social mission,

00:47:31.630 --> 00:47:33.270
is great and very
important and can

00:47:33.270 --> 00:47:35.795
make a huge difference
for individuals.

00:47:35.795 --> 00:47:37.170
And there are some
areas, though,

00:47:37.170 --> 00:47:39.550
where you really need
government action.

00:47:39.550 --> 00:47:42.310
You would never think about
building an interstate highway

00:47:42.310 --> 00:47:47.450
system financed through bake
sales and volunteers going out

00:47:47.450 --> 00:47:49.520
with a shovel to dig one
more foot of highway.

00:47:49.520 --> 00:47:52.670
In the same way, we desperately
need early childhood

00:47:52.670 --> 00:47:54.320
interventions in this country.

00:47:54.320 --> 00:47:56.990
And it is kind of crazy to
do the equivalent of trying

00:47:56.990 --> 00:48:00.510
to finance it
through bake sales.

00:48:00.510 --> 00:48:01.930
I think one of the
big problems is

00:48:01.930 --> 00:48:05.420
in this country is
we perceive poverty

00:48:05.420 --> 00:48:09.330
through this prism of
personal responsibility.

00:48:09.330 --> 00:48:14.790
And there is something to it.

00:48:14.790 --> 00:48:18.180
There are occasions when
poverty is associated

00:48:18.180 --> 00:48:21.400
with irresponsible behavior.

00:48:21.400 --> 00:48:23.980
We have a huge problem
with teenage pregnancy.

00:48:23.980 --> 00:48:27.100
30% percent of American
girls get pregnant by age 19.

00:48:27.100 --> 00:48:29.490
And that involves plenty
of irresponsibility

00:48:29.490 --> 00:48:33.730
on the part of both the guys and
girls not using birth control,

00:48:33.730 --> 00:48:34.830
whatever it may be.

00:48:34.830 --> 00:48:38.260
But we also, as a
society, we have

00:48:38.260 --> 00:48:40.440
proven tools that
will dramatically

00:48:40.440 --> 00:48:43.270
reduce those teenage
pregnancy rates.

00:48:43.270 --> 00:48:44.580
We'll save money doing it.

00:48:44.580 --> 00:48:47.800
Every dollar invested in
long-acting reversible

00:48:47.800 --> 00:48:49.770
contraceptives aimed
at those teenagers

00:48:49.770 --> 00:48:53.170
will pay itself back
seven times over,

00:48:53.170 --> 00:48:57.980
because every Medicaid
childbirth cost $12,000, which

00:48:57.980 --> 00:49:00.590
is a huge thing to prevent.

00:49:00.590 --> 00:49:03.260
And yet when we have
these tools and we

00:49:03.260 --> 00:49:05.330
as a society don't
implement them,

00:49:05.330 --> 00:49:08.700
that seems a real narrative
of personal irresponsibility

00:49:08.700 --> 00:49:10.300
on the part of all of us.

00:49:10.300 --> 00:49:15.500
And so I think we somehow
have to kind of balance

00:49:15.500 --> 00:49:19.980
the traditional philanthropy
with the new kinds

00:49:19.980 --> 00:49:21.590
of philanthropy and
impact investing,

00:49:21.590 --> 00:49:23.050
which are very
powerful, and also

00:49:23.050 --> 00:49:27.270
advocacy to have government
step up to the plate

00:49:27.270 --> 00:49:31.160
and try to create opportunity
to try to address some

00:49:31.160 --> 00:49:33.087
of these broader
problems we have.

00:49:33.087 --> 00:49:34.670
STEVE GROVE: We're
almost out of time.

00:49:34.670 --> 00:49:37.865
I wanted to, before
we end, ask you.

00:49:37.865 --> 00:49:39.490
If you're sitting in
the audience today

00:49:39.490 --> 00:49:40.600
or you're watching
online or you're

00:49:40.600 --> 00:49:42.080
seeing this clip
on YouTube later

00:49:42.080 --> 00:49:43.215
and you want to
make a difference

00:49:43.215 --> 00:49:45.006
and you just don't
quite know how to start,

00:49:45.006 --> 00:49:47.590
you offer some practical tips
at the end of where to start.

00:49:47.590 --> 00:49:49.500
Give us a synopsis of
that, for our audience

00:49:49.500 --> 00:49:51.360
here, to leave us
with how we can

00:49:51.360 --> 00:49:55.010
take early steps to
make a difference.

00:49:55.010 --> 00:49:56.660
NICK KRISTOF: Well
one thing, I just

00:49:56.660 --> 00:50:00.140
also want to give a plug
that there well be-- please,

00:50:00.140 --> 00:50:02.390
do read the book--
but on January 26,

00:50:02.390 --> 00:50:06.240
also, there'll be a PBS version
of the book that will air,

00:50:06.240 --> 00:50:12.030
beginning January 26 and then
each Monday night after that.

00:50:12.030 --> 00:50:15.200
I guess what I would
say is that too often we

00:50:15.200 --> 00:50:18.150
tend to give because
somebody asks.

00:50:18.150 --> 00:50:20.960
So we get a phone call,
marketing call, and somebody

00:50:20.960 --> 00:50:24.770
mentions children with
cancer or disabled veterans,

00:50:24.770 --> 00:50:27.997
and we kind of freeze and
agree to donate without knowing

00:50:27.997 --> 00:50:30.330
anything about the caller on
the other end of the phone.

00:50:30.330 --> 00:50:33.040
We would never buy a
television like that.

00:50:33.040 --> 00:50:34.840
And in the same
way-- and people who

00:50:34.840 --> 00:50:36.790
are good at raising
money aren't always

00:50:36.790 --> 00:50:38.690
the ones are good
at spending money.

00:50:38.690 --> 00:50:43.030
So I think it behooves us,
where the stakes are so high,

00:50:43.030 --> 00:50:48.170
to think of our
prosocial portfolio,

00:50:48.170 --> 00:50:50.750
if you will, as
something like investing.

00:50:50.750 --> 00:50:53.900
And something that is
important to invest wisely,

00:50:53.900 --> 00:50:56.360
where there will be important
returns, potentially

00:50:56.360 --> 00:50:57.077
lifesaving ones.

00:50:57.077 --> 00:50:59.160
It doesn't seem to me to
matter hugely whether you

00:50:59.160 --> 00:51:03.940
do that in East Palo
Alto or in Congo.

00:51:03.940 --> 00:51:05.730
There are huge needs
in both places.

00:51:05.730 --> 00:51:11.170
But I think it is critical to
try to invest in areas where

00:51:11.170 --> 00:51:12.440
there will be high returns.

00:51:12.440 --> 00:51:14.170
And there are
plenty of those now.

00:51:14.170 --> 00:51:16.460
SHERYL WUDUNN: And I
would suggest, really,

00:51:16.460 --> 00:51:18.670
I am serious about a
book club that you then

00:51:18.670 --> 00:51:24.270
can discuss with some friends
to choose an area that you all

00:51:24.270 --> 00:51:27.340
might want to play a role in
and then perhaps an organization

00:51:27.340 --> 00:51:29.230
that you might want
to affiliate with.

00:51:29.230 --> 00:51:29.900
Do it together.

00:51:29.900 --> 00:51:32.210
It's just a lot more fun.

00:51:32.210 --> 00:51:36.010
And it's great to be able to
do it with a group of friends.

00:51:36.010 --> 00:51:37.510
You might even take
it a lot farther

00:51:37.510 --> 00:51:39.990
or come up with a
new way of doing it.

00:51:39.990 --> 00:51:42.070
So I think that's
what I would suggest.

00:51:42.070 --> 00:51:43.060
STEVE GROVE: Great.

00:51:43.060 --> 00:51:44.780
Nick and Sheryl, thank you
so much for coming to Google.

00:51:44.780 --> 00:51:45.680
It's been such a
pleasure to have you.

00:51:45.680 --> 00:51:45.980
NICK KRISTOF: Thanks so much.

00:51:45.980 --> 00:51:46.580
[APPLAUSE]

00:51:46.580 --> 00:51:48.130
[MUSIC PLAYING]

