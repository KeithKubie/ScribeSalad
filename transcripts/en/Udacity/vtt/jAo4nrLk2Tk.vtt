WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.660
This drives home an important point.

00:00:01.660 --> 00:00:03.889
You need to be aware of branch divergence.

00:00:03.889 --> 00:00:07.455
It's one of the fundamental things about GPU architectures

00:00:07.455 --> 00:00:11.263
or any massively parallel architecture that uses a SIMD approach.

00:00:11.263 --> 00:00:14.931
But don't freak out just because your code has an if statement or for loop.

00:00:14.931 --> 00:00:19.726
Reason it out, profile the code, figure out whether this is a real problem worth optimizing.

00:00:19.726 --> 00:00:22.813
There's no real recipe for reducing branch divergence.

00:00:22.813 --> 00:00:26.981
What to do depends on the algorithm, and sometimes the right answer is an entirely new algorithm.

00:00:26.981 --> 00:00:29.840
There are a couple of general principles, though.

00:00:29.840 --> 00:00:31.410
Try to avoid branchy code.

00:00:31.410 --> 00:00:33.645
If you have a lot of if or switch statements in your code,

00:00:33.645 --> 00:00:37.191
consider whether or not adjacent threads are likely to take different branches

00:00:37.191 --> 00:00:40.028
and if so, try to restructure.

00:00:40.028 --> 00:00:43.862
The second general principle is to beware large imbalance in thread workloads.

00:00:43.862 --> 00:00:48.392
Sometimes 1 thread can take much, much longer than the average thread to complete a task,

00:00:48.392 --> 00:00:51.432
and when this happens, the long-running thread can hold up the rest of the warp

00:00:51.432 --> 00:00:53.241
or even the rest of the thread block.

00:00:53.241 --> 00:00:55.699
We saw this in the example earlier where some threads performed a loop

00:00:55.699 --> 00:00:58.171
much more often than other threads in the same warp.

00:00:58.171 --> 00:01:01.048
So look carefully at loops and recursive calls

00:01:01.048 --> 00:01:04.473
to decide if this kind of work imbalance might be hurting you.

00:01:04.473 --> 00:01:07.376
In the next unit you'll see another example, breadth-first search in a graph,

00:01:07.376 --> 00:01:10.552
where each thread is assigned a work item of random size.

00:01:10.552 --> 00:01:12.677
And if 1 thread gets unlucky and picks up a work item

00:01:12.677 --> 00:01:15.183
that takes a thousand times longer than the rest of the threads,

00:01:15.183 --> 00:01:18.921
you're wasting a lot of computational horsepower as all the rest of those threads sit idle

00:01:18.921 --> 00:01:21.127
waiting for that one long-running thread to finish.

00:01:21.127 --> 00:01:23.994
So there's a lot of simple strategies that could mitigate this imbalance.

00:01:23.994 --> 00:01:25.956
You can restructure your code entirely.

00:01:25.956 --> 00:01:29.331
Or you might, for example, coarsely sort the work items before size.

00:01:29.331 --> 00:01:32.912
And John will dive deeper into parallel strategies for breadth-first search

00:01:32.912 --> 00:01:35.361
and a lot of other interesting problems in the next unit.

