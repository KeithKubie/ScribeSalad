WEBVTT
Kind: captions
Language: en

00:00:00.620 --> 00:00:05.460
Anyway, so I'll give you this
top ten list, and if you

00:00:05.460 --> 00:00:10.260
look at the final review handout, it has
a very comprehensive list of topics.

00:00:10.260 --> 00:00:12.250
But that list may be too long, so

00:00:12.250 --> 00:00:16.380
here's just ten things,
not in any particular order.

00:00:16.380 --> 00:00:18.640
These are not in order of importance.

00:00:21.400 --> 00:00:23.200
Okay, I'll just use the big board.

00:00:23.200 --> 00:00:29.441
All right, so here's my top ten list.

00:00:29.441 --> 00:00:33.340
First is conditioning, right.

00:00:33.340 --> 00:00:37.320
Conditioning is the soul of statistics,
so that did have to come first.

00:00:38.580 --> 00:00:42.050
And conditioning includes a lot of things,
right.

00:00:42.050 --> 00:00:46.330
It includes conditional probability,
it includes conditional expectation,

00:00:46.330 --> 00:00:51.820
it includes Bayes rule, and
even includes Markov trends

00:00:51.820 --> 00:00:55.130
in a certain sense because there was
a certain conditional independence thing.

00:00:55.130 --> 00:01:00.898
Conditional independence versus
independence, that's been everywhere.

00:01:00.898 --> 00:01:04.390
Second, symmetry.

00:01:04.390 --> 00:01:10.350
Symmetry is powerful but
dangerous, I like to say.

00:01:10.350 --> 00:01:13.680
Because it's extremely powerful,
and a lot of problems where you'd

00:01:13.680 --> 00:01:16.970
have to do 100 pages of algebra
if you don't see the symmetry and

00:01:16.970 --> 00:01:18.510
one line if you see the symmetry.

00:01:18.510 --> 00:01:21.870
On the other hand you don't want to start
hallucinating symmetries that are not

00:01:21.870 --> 00:01:24.450
there, so you have to be careful.

00:01:24.450 --> 00:01:28.553
And third, random variables and
their distributions,

00:01:28.553 --> 00:01:32.838
which is one possible five word title for
the whole course.

00:01:36.843 --> 00:01:40.380
Fourth is stories.

00:01:41.500 --> 00:01:45.910
So we spent a lot of time
on not only story proofs,

00:01:45.910 --> 00:01:50.170
but more importantly, the stories for
each distribution, like the normal and

00:01:50.170 --> 00:01:52.250
gamma and Poisson and so on.

00:01:52.250 --> 00:01:54.800
They're extremely important
because otherwise, right,

00:01:54.800 --> 00:01:58.010
there are infinitely many possible
distributions we could have looked at.

00:01:58.010 --> 00:02:04.076
Why do we spend so much time on
the Poissons and exponentials and stuff?

00:02:04.076 --> 00:02:06.519
It's because they had important stories,

00:02:06.519 --> 00:02:11.150
otherwise just write down anything that
integrates to one and then work with that.

00:02:11.150 --> 00:02:13.960
Why are some distributions more useful and
important than others?

00:02:13.960 --> 00:02:16.320
It's because they have stories.

00:02:16.320 --> 00:02:19.258
Okay, fifth is linearity.

00:02:19.258 --> 00:02:22.385
Sixth is indicator random variables,

00:02:22.385 --> 00:02:26.847
which is one of my favorite tricks,
as I think you know.

00:02:26.847 --> 00:02:29.788
It's extremely useful on interview
problems and all kinds of stuff.

00:02:29.788 --> 00:02:35.620
Seventh is LOTUS,

00:02:35.620 --> 00:02:38.770
just an extremely useful tool for
computing expectations.

00:02:39.910 --> 00:02:45.510
And eighth is law of large numbers.

00:02:45.510 --> 00:02:48.810
I just sort of had to put it there
because that, and central limit theorem.

00:02:49.950 --> 00:02:54.420
These are the two most famous theorems
in probability, possibly the two most

00:02:54.420 --> 00:02:57.573
important theorems in probability, but
definitely the two most famous ones.

00:02:57.573 --> 00:03:03.290
And then tenth, Markov chains,
which we spent the last few lectures on.

00:03:06.899 --> 00:03:09.460
Okay, so that's just the top ten list.

00:03:09.460 --> 00:03:12.330
But it kind of partitions
up kind of neatly.

00:03:14.000 --> 00:03:20.030
At the first floor all have to do with

00:03:20.030 --> 00:03:25.800
the big picture question of
what is randomness, right.

00:03:25.800 --> 00:03:30.160
That's why probability is everywhere,
is because randomness and uncertainty.

00:03:31.610 --> 00:03:34.080
And what is uncertainty,
that's everywhere, right.

00:03:35.420 --> 00:03:38.500
How do we think about uncertainty?

00:03:38.500 --> 00:03:43.230
That's these four, these three are all
about computing expected values.

00:03:43.230 --> 00:03:46.140
Indicator random variable you can use for
other purposes as well,

00:03:46.140 --> 00:03:49.820
but we were especially using it for
computing expected values.

00:03:51.540 --> 00:03:54.791
And remember also that expected values
doesn't just mean finding the average of

00:03:54.791 --> 00:03:57.753
the random variable, because if we
want to find the standard deviation,

00:03:57.753 --> 00:03:58.869
then we need the variance.

00:03:58.869 --> 00:04:00.880
And to get the variance,
we need to do expected values.

00:04:00.880 --> 00:04:06.942
So a lot of things are more than just
the average reduced back down to this,

00:04:06.942 --> 00:04:08.940
so computing averages.

00:04:08.940 --> 00:04:13.870
And then these last three, I would
describe these as talking about long run

00:04:13.870 --> 00:04:21.030
behavior as we have, by long run I mean
we have a lot of random variables.

00:04:21.030 --> 00:04:23.960
So here and here, and
we take the average of a large number

00:04:23.960 --> 00:04:28.210
of IID random variables and
see what happens, how does it behave.

00:04:28.210 --> 00:04:32.390
And then for Markov chains,
that's how they were originally designed.

00:04:32.390 --> 00:04:35.810
Markov's motivation was
going one step beyond IID.

00:04:35.810 --> 00:04:40.380
You run this Markov chain just wandering
around for a long time, right.

00:04:40.380 --> 00:04:43.590
It's not IID anymore generally, but

00:04:43.590 --> 00:04:48.399
it has this nice conditional structure
that makes it very nice to work with.

00:04:49.750 --> 00:04:53.760
Okay, well anyway that's top ten list.

00:04:53.760 --> 00:04:57.741
And let me know if any other questions
come to mind in the mean time.

00:05:00.729 --> 00:05:04.390
But mean while I'll draw a little picture.

00:05:06.540 --> 00:05:08.919
So we've been doing Markov chains.

00:05:10.220 --> 00:05:16.430
So it's kind of natural to represent
our current state, is stat 110, right.

00:05:16.430 --> 00:05:21.600
So this is the whole world
of statistics at Harvard

00:05:21.600 --> 00:05:27.150
as a Markov chain, and we're here now, and
it doesn't matter how you got here, right.

00:05:27.150 --> 00:05:29.800
All that matters is that
you're here in Stat 110.

00:05:29.800 --> 00:05:34.020
And then the question is saying, well
where are you gonna be next semester or

00:05:34.020 --> 00:05:35.330
next year or things like that?

00:05:35.330 --> 00:05:36.590
What can you do from here?

00:05:36.590 --> 00:05:41.027
So I want to briefly mention,
if you look at the prerequisites for

00:05:41.027 --> 00:05:44.991
stat courses above 110,
most of them require 110.

00:05:44.991 --> 00:05:49.090
And there's reasons,
there are very good reasons for that.

00:05:49.090 --> 00:05:53.680
And just give a few examples, both
mentioning a little bit about the courses

00:05:53.680 --> 00:05:57.280
just so that you, for some of you,
it may be useful to know.

00:05:57.280 --> 00:06:00.000
But also just to show you a few examples.

00:06:01.130 --> 00:06:04.369
Okay, so where can you go from 110?

00:06:05.790 --> 00:06:09.540
Well the most obvious follow up is 111.

00:06:09.540 --> 00:06:16.702
111 and 110 in a sense should be a full
year course on probability and inference.

00:06:16.702 --> 00:06:20.572
It's going back to the very first day of
class, I won't draw the diagram again, but

00:06:20.572 --> 00:06:24.061
on the very first day of class I drew
a picture of about what's the difference

00:06:24.061 --> 00:06:26.429
between probability and
statistical inference.

00:06:26.429 --> 00:06:29.222
And then they're kind of
two sides of the same coin.

00:06:29.222 --> 00:06:32.350
In probability we're saying,
here's our model, and then we're saying,

00:06:32.350 --> 00:06:34.526
well what's the probability
that this will happen?

00:06:34.526 --> 00:06:36.700
What's the probability that would happen?

00:06:36.700 --> 00:06:40.300
On average what's gonna happen, right,
given that we're using this particular

00:06:40.300 --> 00:06:44.870
model like we're Poisson lambda or
something and assume lambda is known or

00:06:44.870 --> 00:06:48.578
lambda has some distribution itself but
that distribution is known and so on.

00:06:48.578 --> 00:06:50.618
Inference is about going the other way.

00:06:50.618 --> 00:06:55.788
You have data, and then you want to
actually then say, estimate unknown

00:06:55.788 --> 00:07:02.140
parameters, right, theta or lambda or
whatever the parameter is unknown, okay.

00:07:02.140 --> 00:07:05.510
You want to estimate it or
you want to make predictions about future,

00:07:05.510 --> 00:07:08.800
like you have lots of observation so
far, but then in the future you're

00:07:08.800 --> 00:07:12.510
gonna get more data and you want to
predict what that's gonna look like.

00:07:12.510 --> 00:07:14.980
Okay, so those are questions
of statistical inference.

00:07:14.980 --> 00:07:18.220
You can't do statistical inference unless
you have probability cuz it gives you

00:07:18.220 --> 00:07:22.630
both the language you need as
well as a lot of the theorems.

00:07:22.630 --> 00:07:28.860
Universality of the uniform, I didn't
introduce that to torture you or anything.

00:07:28.860 --> 00:07:32.140
I mean, it's a beautiful general result,
but it's also very,

00:07:32.140 --> 00:07:34.430
very useful in statistical inference.

00:07:35.850 --> 00:07:37.860
So things like that.

00:07:37.860 --> 00:07:40.730
Okay, so that's 111, that's every Spring.

00:07:40.730 --> 00:07:46.890
Just from 110 and 111 alone, they're
both more on the mathematical side,

00:07:46.890 --> 00:07:52.270
rather than a course where you're actually
learning how do you work with data.

00:07:52.270 --> 00:07:57.365
So another course I would
especially recommend Is 139,

00:07:57.365 --> 00:08:01.936
which is a linear models, which has some

00:08:01.936 --> 00:08:06.653
overlap with econometrics EC1123.

00:08:06.653 --> 00:08:12.025
And in fact, we, in stat, have been having
discussions of the ec department for

00:08:12.025 --> 00:08:15.344
years about the issue of 139 versus 1123.

00:08:15.344 --> 00:08:19.240
Because the ec department's policy is
that you can't take both for credit.

00:08:19.240 --> 00:08:23.630
And the stat department's policy is
that you can take both for credit.

00:08:23.630 --> 00:08:28.010
So because the stat perspective on this is
that the points of view are sufficiently

00:08:28.010 --> 00:08:30.970
different that you should
be able to do both.

00:08:30.970 --> 00:08:34.690
But that's a difference in point
of view about the points of view.

00:08:34.690 --> 00:08:37.797
So anyway, you could include 1123 or

00:08:37.797 --> 00:08:43.030
1126 here, I'm not going to
list all the ec course numbers.

00:08:43.030 --> 00:08:48.810
You should have some course in regression
in linear models as useful for

00:08:48.810 --> 00:08:50.610
all kinds of things.

00:08:50.610 --> 00:08:54.270
And 139 does not require 110, but

00:08:54.270 --> 00:08:59.930
if you've had 110 it gives you a better
understanding of what's going on.

00:08:59.930 --> 00:09:02.580
And similarly, if you look at like 1126,

00:09:02.580 --> 00:09:07.620
take a look at the course notes for
that sometime.

00:09:07.620 --> 00:09:10.650
Everything is conditional expectation,
all right?

00:09:10.650 --> 00:09:14.480
So if you understand
conditional expectation,

00:09:14.480 --> 00:09:16.780
then that's going to be
extremely advantageous.

00:09:16.780 --> 00:09:20.560
139, you're going to understand what's
going on much better than other people in

00:09:20.560 --> 00:09:23.770
class who haven't had 110,
although it's not required.

00:09:23.770 --> 00:09:27.010
So 139 is just a great course.

00:09:27.010 --> 00:09:30.510
And actually just ways to work with data,

00:09:30.510 --> 00:09:32.950
that you're actually using
data on the computer.

00:09:32.950 --> 00:09:36.970
And one piece of advice if you want,

00:09:36.970 --> 00:09:40.710
in addition to everything else I'm saying,
is to learn R.

00:09:40.710 --> 00:09:45.485
That's one of the best six letter
pieces of advice I can think of.

00:09:45.485 --> 00:09:47.150
Also R is a statistical.

00:09:47.150 --> 00:09:49.888
I also recommend learning C.

00:09:49.888 --> 00:09:54.431
And so also I recommend CS50 or
learn C somewhere else as well.

00:09:54.431 --> 00:09:57.220
But we're talking about
statistics right now, learn R.

00:09:57.220 --> 00:09:59.150
R is very, very different from C.

00:09:59.150 --> 00:10:03.050
R was designed by statisticians for
statistical purposes in mind.

00:10:03.050 --> 00:10:07.570
Completely different from C in flavor,
it's easier in some ways.

00:10:07.570 --> 00:10:11.500
But sometimes that's a misconception,
sometimes people who have been doing C for

00:10:11.500 --> 00:10:14.540
years think that they could just
learn R in like a couple of hours and

00:10:14.540 --> 00:10:17.680
be an R expert, and
they approach it all wrong.

00:10:17.680 --> 00:10:20.270
Cuz it's just a completely
different mindset.

00:10:20.270 --> 00:10:22.050
Very, very different language.

00:10:22.050 --> 00:10:23.742
It's extremely powerful.

00:10:23.742 --> 00:10:28.800
Actually I just saw a really
interesting article yesterday that was

00:10:28.800 --> 00:10:33.660
arguing that learning R is not only
incredibly useful that it actually

00:10:33.660 --> 00:10:39.190
helps you become a bit better statistical
thinker just by studying this language.

00:10:39.190 --> 00:10:42.190
And I hadn't thought about
it that way before but

00:10:42.190 --> 00:10:45.200
actually that article convinced
me that that was true.

00:10:45.200 --> 00:10:47.680
So this is incredibly useful.

00:10:47.680 --> 00:10:54.310
Okay, a lot of the stat courses
in general, actually use R.

00:10:54.310 --> 00:10:59.640
And whether you do more stat or not,
it's just an incredibly useful skill.

00:10:59.640 --> 00:11:01.160
Getting more useful all the time.

00:11:01.160 --> 00:11:02.030
And R is free.

00:11:02.030 --> 00:11:05.460
You can find all kinds of very,
very well written tutorials and

00:11:05.460 --> 00:11:08.720
stuff like that for free,
you can download it for free.

00:11:08.720 --> 00:11:12.170
And because it has this kind
of free software culture,

00:11:12.170 --> 00:11:16.521
a lot of people are writing R packages,
and put them online for free.

00:11:16.521 --> 00:11:22.325
And so it keeps growing like that, because
it has this open source culture to it.

00:11:22.325 --> 00:11:26.518
Okay, that's 111, 139,
these are in no particular order.

00:11:26.518 --> 00:11:29.030
123, finance.

00:11:29.030 --> 00:11:30.650
If you have any interest in finance.

00:11:30.650 --> 00:11:35.416
Not offered this spring but
it will be offered next spring.

00:11:35.416 --> 00:11:39.119
Any interest at all in finance,
you should definitely take 123.

00:11:39.119 --> 00:11:43.296
I don't need to talk more about it now
because Steven Blithe already gave the 123

00:11:43.296 --> 00:11:44.280
preview.

00:11:44.280 --> 00:11:48.520
Although I would mention I really
liked what he said at the end.

00:11:48.520 --> 00:11:52.640
He said the two dimensional lotus was
a key to winning the Nobel Prize.

00:11:52.640 --> 00:11:55.200
So then I was happy I have LOTUS here.

00:11:55.200 --> 00:12:01.692
That includes the 2D one,
right here, all dimensional LOTUSes.

00:12:01.692 --> 00:12:04.763
Okay, that's a finance course.

00:12:04.763 --> 00:12:10.000
115 is computational biology.

00:12:10.000 --> 00:12:11.840
That's every spring.

00:12:11.840 --> 00:12:18.831
It's a really nice mixture of biology,
computer science and statistics.

00:12:18.831 --> 00:12:23.394
To understand what's going on there,
a lot of it relies on Markov chains and

00:12:23.394 --> 00:12:26.546
Bayesian thinking,
the things we've been doing.

00:12:26.546 --> 00:12:29.490
Like that email that I just sent.

00:12:29.490 --> 00:12:34.640
The Markov chains are everywhere
in biology these days.

00:12:34.640 --> 00:12:37.528
So if you're interested in biology and
stat, that would be a good one.

00:12:37.528 --> 00:12:39.403
Well, what else do I wanna mention?

00:12:39.403 --> 00:12:42.380
171 is another natural followup.

00:12:42.380 --> 00:12:46.380
171 is stochastic processes.

00:12:46.380 --> 00:12:49.697
So if you like Markov chains, but
we only had three lectures on it.

00:12:49.697 --> 00:12:53.522
In 171, maybe they spend
a month on Markov chains, and

00:12:53.522 --> 00:12:56.969
then do a lot of other
stochastic processes as well.

00:12:56.969 --> 00:13:01.906
So in a sense, 111 is swapping
to the other side of the coin.

00:13:01.906 --> 00:13:05.600
And 171 is continuing in kind
of probability thinking.

00:13:05.600 --> 00:13:09.340
Where you already have random
variables evolving over time, right?

00:13:09.340 --> 00:13:10.915
That's a stochastic process.

00:13:10.915 --> 00:13:14.430
Okay, so that's a really good
stochastic process course.

00:13:14.430 --> 00:13:16.880
And any others that I wanted
to mention right now?

00:13:18.890 --> 00:13:20.600
I think that's good.

00:13:22.410 --> 00:13:24.560
Okay, so we have that little thing there.

00:13:24.560 --> 00:13:29.440
I want to do a couple,
just a few quick examples of just

00:13:29.440 --> 00:13:34.544
kind of how probability shows
up in some of these areas.

00:13:34.544 --> 00:13:38.738
Okay, so for example,

00:13:38.738 --> 00:13:42.521
let's take 139.

00:13:42.521 --> 00:13:47.369
And by 139, I mean 139 but
I also mean more generally regression,

00:13:47.369 --> 00:13:50.955
which you can also see in econometrics or
elsewhere.

00:13:50.955 --> 00:13:54.840
It's extremely widely used for
analyzing data.

00:13:54.840 --> 00:14:00.600
And the first time I saw regression,
I really, really hated it.

00:14:02.340 --> 00:14:08.230
I thought it was the ugliest thing ever
and I had no idea what was going on.

00:14:08.230 --> 00:14:13.610
To start with a really simple, relatively
simple, linear regression model just for

00:14:13.610 --> 00:14:17.880
the simplest case just to do an example
because it's not a regression course.

00:14:17.880 --> 00:14:26.060
We're looking at models like this,
y equals beta 0 + beta 1x + epsilon.

00:14:26.060 --> 00:14:30.950
Beta 0 and beta 1 are constants which
are generally we would think of

00:14:30.950 --> 00:14:32.350
those as unknown parameters.

00:14:32.350 --> 00:14:34.650
So we're trying to estimate.

00:14:34.650 --> 00:14:38.960
The question is,
can you use x to predict y?

00:14:38.960 --> 00:14:41.500
So you have a pair x and y.

00:14:41.500 --> 00:14:45.420
Maybe you have 100 people in your study or
something like that, or

00:14:45.420 --> 00:14:46.740
it doesn't have to be people obviously but

00:14:46.740 --> 00:14:50.620
you have two variables you wanna
know how does x use x to predict y?

00:14:50.620 --> 00:14:51.320
That's a regression.

00:14:51.320 --> 00:14:54.580
And this would be a simple
example of linear regression

00:14:54.580 --> 00:14:56.660
because this looks linear, right?

00:14:56.660 --> 00:15:01.550
And epsilon is because you're assuming
that x is not going to perfectly tell

00:15:01.550 --> 00:15:07.470
you y,
that there's some error term here epsilon.

00:15:07.470 --> 00:15:12.350
In practice,
people often assume that these epsilons

00:15:12.350 --> 00:15:16.582
are normal with mean zero, but
you don't have to assume that necessarily.

00:15:16.582 --> 00:15:20.128
So sometimes normal
might not be realistic.

00:15:20.128 --> 00:15:24.027
But let's assume,
a common assumption would be that

00:15:24.027 --> 00:15:28.982
at least they're centered at zero,
so the expected value is zero.

00:15:28.982 --> 00:15:30.452
So on average it's not gonna be,

00:15:30.452 --> 00:15:35.710
that there should be zero on average
is a natural uncommon assumption.

00:15:35.710 --> 00:15:36.250
But in particular,

00:15:36.250 --> 00:15:41.140
I want to assume that the expected
value of epsilon given x is 0.

00:15:41.140 --> 00:15:44.610
So there's not certain values of
x that are going to the errors

00:15:44.610 --> 00:15:48.110
tend to be positive rather than negative,
that kind of thing.

00:15:48.110 --> 00:15:50.761
So let's assume that,
I mean this is just regression.

00:15:50.761 --> 00:15:53.470
Okay, and
then I'm not going to do much with this,

00:15:53.470 --> 00:15:59.390
I'm just going to show you very quickly
how to get some simple facts about this.

00:15:59.390 --> 00:16:02.080
So one thing we could do
when we see this equation

00:16:02.080 --> 00:16:05.670
is to take the covariance
of both sides with X.

00:16:05.670 --> 00:16:10.261
So, I'm saying that these
are the same random variable, okay?

00:16:10.261 --> 00:16:13.844
So I can do whatever I want to both sides,
so

00:16:13.844 --> 00:16:18.224
if I take the covariance
of both sides with X, well,

00:16:18.224 --> 00:16:24.590
covariance of a constant with anything
is 0, so that part goes away.

00:16:24.590 --> 00:16:27.550
So we have the covariance of (beta

00:16:27.550 --> 00:16:32.151
1 X with X) + the covariance
of (epsilon with X).

00:16:35.570 --> 00:16:37.690
This is just a quick review
of properties of covariance.

00:16:38.730 --> 00:16:42.730
Beta 1 comes out, covariance of
X with itself is the variance,

00:16:42.730 --> 00:16:46.880
so that's beta 1 times the variance (X),
and

00:16:46.880 --> 00:16:49.590
then let's think about
the covariance of epsilon and X.

00:16:49.590 --> 00:16:54.290
I'll just do that over here and
figure out what this is.

00:16:54.290 --> 00:16:58.660
Well, I'm assuming that
the conditional expectation is 0.

00:16:58.660 --> 00:17:05.014
By Adam's law,
the unconditional expectation is also 0,

00:17:05.014 --> 00:17:09.251
because it's E(E(epsilon|x)),

00:17:09.251 --> 00:17:12.874
that's just Adam's law, = 0.

00:17:12.874 --> 00:17:20.497
And so the covariance of epsilon and
X is just the expected value of epsilon X,

00:17:20.497 --> 00:17:26.730
because the other term is 0,
because epsilon has 0.

00:17:26.730 --> 00:17:30.065
And let's just quickly
compute this expectation.

00:17:30.065 --> 00:17:33.070
Well, the strategy, again,
should be condition on x.

00:17:33.070 --> 00:17:40.650
Adam's law, so
we're just gonna go E(E(epsilon X|X).

00:17:40.650 --> 00:17:45.060
Just a quick review of Adam's law,
take out what's known.

00:17:45.060 --> 00:17:49.790
Once we condition on X, the next is known,
so this X pops out right there.

00:17:49.790 --> 00:17:56.760
But what's left is E(epsilon|X), which
we assumed is 0, so now it's X times 0.

00:17:56.760 --> 00:18:01.318
X times 0 is 0, so this is just 0.

00:18:01.318 --> 00:18:02.560
So actually, it's plus 0 here.

00:18:04.310 --> 00:18:09.520
So that tells us that
beta 1 = the covariance.

00:18:09.520 --> 00:18:13.621
By symmetry, that's the covariance
(X,Y) divided by the variance (X).

00:18:13.621 --> 00:18:14.129
Okay?
So

00:18:14.129 --> 00:18:17.438
that's a very quick
derivation of this fact.

00:18:17.438 --> 00:18:20.854
And if you look in a lot of
books that do regression,

00:18:20.854 --> 00:18:25.551
you're gonna have a very ugly looking
formula with summation signs.

00:18:25.551 --> 00:18:29.210
This is the population version rather
than the sample version, okay?

00:18:29.210 --> 00:18:31.160
And a lot of books,
if they don't wanna assume 1-10,

00:18:31.160 --> 00:18:34.770
then they're gonna give the sample
version of just like, here's the formula,

00:18:34.770 --> 00:18:39.920
the summation of something, xi minus
something, yi minus, that kind of thing.

00:18:41.400 --> 00:18:45.072
And they'll either prove that thing in
some long, tedious, algebraic way, or

00:18:45.072 --> 00:18:48.798
they'll just say, we don't wanna prove
this, here's the formula for you, and

00:18:48.798 --> 00:18:50.041
it just looks really ugly.

00:18:50.041 --> 00:18:51.981
Right?
It's hard to memorize it and

00:18:51.981 --> 00:18:56.510
it's hard to understand where it
came from, but it's just that.

00:18:56.510 --> 00:19:00.040
So understanding this thing, you actually
understand where the things come from.

00:19:00.040 --> 00:19:04.670
Similarly, a lot of these formulas that
looked really ugly at first, once you

00:19:04.670 --> 00:19:07.980
really understand what they're doing,
a lot of times, it's just a conditional

00:19:07.980 --> 00:19:11.790
expectation, and in particular,
a lot of times it's just a projection.

00:19:11.790 --> 00:19:13.920
Right?
We drew a geometric picture.

00:19:13.920 --> 00:19:16.960
What does this conditional
expectation mean, geometrically?

00:19:16.960 --> 00:19:18.510
It's a projection.

00:19:18.510 --> 00:19:20.900
Projections are nice, right?

00:19:20.900 --> 00:19:26.120
So, just understanding things, you
actually can understand what's going on.

00:19:26.120 --> 00:19:32.010
And in particular, a few of you actually
asked me for book recommendations.

00:19:32.010 --> 00:19:35.170
And if any of you want more
specific book recommendations for

00:19:35.170 --> 00:19:38.430
anything, I like recommending books.

00:19:38.430 --> 00:19:40.850
I'm just gonna mention one right now.

00:19:40.850 --> 00:19:41.723
I love this book.

00:19:41.723 --> 00:19:44.438
Mostly Harmless Econometrics.

00:19:44.438 --> 00:19:45.600
Have any of you read this book?

00:19:47.650 --> 00:19:49.380
It's really cheap, too.

00:19:49.380 --> 00:19:51.970
It's like $18 on Amazon,
last time I checked.

00:19:53.190 --> 00:19:54.780
This book is beautifully written.

00:19:54.780 --> 00:19:58.230
And I can't say that that's true
about many econometrics books.

00:19:58.230 --> 00:20:00.210
This one is just beautifully written.

00:20:00.210 --> 00:20:05.750
And if you flip through this
book sometime, what you'll

00:20:05.750 --> 00:20:10.790
see is the conditional expectation, and
Adam's law and Eve's law are everywhere.

00:20:13.120 --> 00:20:16.580
So it's all building on things we did
here, and if you haven't had that

00:20:16.580 --> 00:20:19.720
background, it will be much harder
to understand what's going on there.

00:20:19.720 --> 00:20:24.358
But with that,
then it's just Adam's law, Eve's law.

00:20:24.358 --> 00:20:25.570
It should all come together.

00:20:27.190 --> 00:20:30.040
Okay.
So I wanted to mention one other

00:20:30.040 --> 00:20:34.710
course here, which is more of an obscure
one, but I think it's an important one.

00:20:34.710 --> 00:20:40.145
It's only offered every other year.

00:20:40.145 --> 00:20:43.581
That's that 160.

00:20:43.581 --> 00:20:46.864
So that 160 is survey sampling.

00:20:46.864 --> 00:20:53.105
It's very relevant for any of you
who are interested in government and

00:20:53.105 --> 00:20:57.654
policy and
things that would involve survey data,

00:20:57.654 --> 00:21:02.550
political polls, or
other types of survey sampling.

00:21:02.550 --> 00:21:07.710
So just to give you a quick example
of how you can use 110 ideas,

00:21:07.710 --> 00:21:12.790
and I think this is actually a good
review example too, I'm gonna do one.

00:21:12.790 --> 00:21:13.860
And you can imagine, by the way,

00:21:13.860 --> 00:21:17.120
that stuff like hypergeometrics
would come up a lot there,

00:21:17.120 --> 00:21:20.620
because you have this population and
you're drawing a sample, right?

00:21:20.620 --> 00:21:22.450
Usually, you're just trying
to learn something and

00:21:22.450 --> 00:21:24.870
that's of some policy interest, right?

00:21:24.870 --> 00:21:28.870
Usually, you cannot ask every person in
the population what their opinion is, so

00:21:28.870 --> 00:21:30.110
you get a sample, and

00:21:30.110 --> 00:21:35.050
then you try to infer to
the population what can you do, right?

00:21:35.050 --> 00:21:37.580
So, if you sample without replacement,

00:21:37.580 --> 00:21:42.960
we know that hypergeometrics come up very,
very, very naturally, right?

00:21:42.960 --> 00:21:44.860
So here's just a quick example.

00:21:47.310 --> 00:21:53.541
It's kind of a different mindset from
other parts of statistics where we assume

00:21:53.541 --> 00:21:59.595
IID because we're assuming that we're
sampling from a finite population.

00:22:04.083 --> 00:22:05.540
This is one approach to it.

00:22:05.540 --> 00:22:09.050
There are some kind of
foundational controversies

00:22:10.070 --> 00:22:14.140
about how do you approach this problem.

00:22:14.140 --> 00:22:15.660
We have a finite population.

00:22:15.660 --> 00:22:17.470
Well, of course, it's a finite population.

00:22:17.470 --> 00:22:19.310
It's always a finite population right?

00:22:19.310 --> 00:22:21.090
A lot of times, that gets ignored.

00:22:21.090 --> 00:22:24.791
You have a finite population,
let's say, of people, and

00:22:24.791 --> 00:22:29.383
suppose that their true values of
something that you're interested in,

00:22:29.383 --> 00:22:33.845
that any character, that is for
each person, you have some variable.

00:22:33.845 --> 00:22:38.347
It could be their height, their income,
their opinion on some question,

00:22:38.347 --> 00:22:40.137
whatever you're studying.

00:22:40.137 --> 00:22:43.649
Let's say that the true
values are Y1 through YN,

00:22:43.649 --> 00:22:46.845
where capital N is the size
of the population.

00:22:46.845 --> 00:22:50.716
And let's assume that each person has an
ID number, like social security number, so

00:22:50.716 --> 00:22:52.938
there's some well defined
way to list them out.

00:22:52.938 --> 00:22:54.798
And these are treated as non-random,

00:22:54.798 --> 00:22:57.990
at least in the approach
we're talking about now.

00:22:57.990 --> 00:22:59.450
These are treated as fixed.

00:22:59.450 --> 00:23:00.650
These are just constants.

00:23:02.670 --> 00:23:04.374
Initially, they are unknown constants.

00:23:04.374 --> 00:23:05.025
Right?

00:23:05.025 --> 00:23:07.162
We just assume each person
has some fixed opinion, but

00:23:07.162 --> 00:23:09.130
we don't know what those opinions are yet.

00:23:09.130 --> 00:23:10.350
So what do we do?

00:23:10.350 --> 00:23:13.810
We draw a sample, we get a sample.

00:23:13.810 --> 00:23:14.690
Okay?
And there's many,

00:23:14.690 --> 00:23:20.170
many different ways to do that sampling,
which we discussed in Stat 160.

00:23:20.170 --> 00:23:25.605
You got this sample, and then you
try to infer it to the population.

00:23:25.605 --> 00:23:30.130
And in practice, it may be extremely
difficult to get a simple random sample.

00:23:30.130 --> 00:23:33.220
That is where all samples of a certain
size are equally likely, and

00:23:33.220 --> 00:23:36.790
it's not always necessarily even
desirable to have a simple random sample.

00:23:36.790 --> 00:23:40.810
But anyway, let's just assume we have some
samplings to scheme where we get a sample

00:23:40.810 --> 00:23:46.140
of size lowercase n.

00:23:46.140 --> 00:23:46.890
Okay?

00:23:46.890 --> 00:23:54.330
So suppose our goal is to
estimate the average of these.

00:23:54.330 --> 00:23:58.460
Or, equivalently,
assuming we know N, capital N,

00:23:58.460 --> 00:24:01.210
then it's equivalent to just say,
try to estimate the sum.

00:24:01.210 --> 00:24:05.700
That is, we only get to collect the
sample, but we wanna estimate the total or

00:24:05.700 --> 00:24:07.490
average value of the entire population.

00:24:08.550 --> 00:24:13.518
So we wanna estimate the sum of
all the yjs, j = 1 to N, okay?

00:24:13.518 --> 00:24:15.531
That's the goal.

00:24:15.531 --> 00:24:18.690
Now we have a sample of size n.

00:24:18.690 --> 00:24:24.540
And let's assume that
the probability that person j,

00:24:24.540 --> 00:24:29.480
we have some ordering ID number for
each person, the probability that person j

00:24:31.400 --> 00:24:38.665
is in the sample is pj,
which we assume right now is known.

00:24:38.665 --> 00:24:41.835
And then, of course,
in practice pj might not be known.

00:24:41.835 --> 00:24:43.215
And you might have to estimate pj.

00:24:43.215 --> 00:24:44.775
And then you can ask what happens then.

00:24:44.775 --> 00:24:47.397
How do you deal with the fact
that you don't know the true pj?

00:24:47.397 --> 00:24:48.827
Okay, that's more complicated.

00:24:48.827 --> 00:24:51.897
But let's assume that
the true pj are known.

00:24:51.897 --> 00:24:55.487
So the simple random sampling would be
the case when all the pjs are equal.

00:24:55.487 --> 00:24:59.207
That is, everyone is equally likely
to be collected into your sample.

00:24:59.207 --> 00:25:03.387
But that may not be true, some people may
be much easier to sample than others.

00:25:03.387 --> 00:25:06.497
Or some are just obscure,
hard to reach, don't answer the phone,

00:25:06.497 --> 00:25:09.067
in some obscure part of the country or
something like that.

00:25:09.067 --> 00:25:12.660
Other people are easier to
actually collect into your sample.

00:25:12.660 --> 00:25:15.540
Okay, so pjs may not all be equal.

00:25:15.540 --> 00:25:18.580
Okay, now then the claim is that,

00:25:24.650 --> 00:25:29.378
suppose that our data are we get like X1,
I'll just say like X1,

00:25:29.378 --> 00:25:36.550
Z1 through Xn, Zn be the data,

00:25:38.350 --> 00:25:44.070
That you collect from your sample,
where X is the Y value.

00:25:44.070 --> 00:25:46.460
I'm using a different letter,
X rather than Y,

00:25:47.930 --> 00:25:52.400
just to indicate the Xs are random
variables, the Ys are fixed.

00:25:52.400 --> 00:25:58.000
The Xs are random because this is the
first person you collect into your survey.

00:25:58.000 --> 00:26:00.690
But that person was randomly
chosen with some probabilities.

00:26:00.690 --> 00:26:04.940
So the value has become random
because of the sampling, okay?

00:26:04.940 --> 00:26:07.770
And Z is just their ID number.

00:26:07.770 --> 00:26:11.901
So Xj is the Y value that
you're interested in, and

00:26:11.901 --> 00:26:18.490
Zj is the ID number of that person,
that is who did we actually get.

00:26:21.010 --> 00:26:23.350
That's the setup, okay?

00:26:23.350 --> 00:26:29.736
So a question that is of tremendous
interest is how do we get an unbiased

00:26:29.736 --> 00:26:35.800
estimator for the total?

00:26:35.800 --> 00:26:40.150
And the standard method that's used
that's based on a very, very clever trick

00:26:41.340 --> 00:26:45.390
is to divide by the probability.

00:26:45.390 --> 00:26:52.780
That is, you sum up all your observations,
j = 1 to n of Xj,

00:26:52.780 --> 00:26:57.310
but you divide by the probability
of getting at that person.

00:26:57.310 --> 00:27:01.312
So in other words,
if the values you observed were 5, 10 and

00:27:01.312 --> 00:27:04.490
15 and those had probabilities, A, B, C.

00:27:04.490 --> 00:27:10.290
What you would do is 5 divided by a,
plus 10 divided by b, plus 15 divided c.

00:27:10.290 --> 00:27:12.670
So, these are the probabilities, pzj.

00:27:12.670 --> 00:27:16.938
That's unbiased.

00:27:21.832 --> 00:27:25.833
So this just says take each measurement
and divide by the probability that

00:27:25.833 --> 00:27:30.350
you actually got that person in
your survey, then that is unbiased.

00:27:30.350 --> 00:27:33.625
Okay, so let's prove that is unbiased.

00:27:33.625 --> 00:27:40.850
We'll take the expectation,
but this looks ugly.

00:27:40.850 --> 00:27:43.460
This is kind of an interesting
thing here because

00:27:44.540 --> 00:27:48.210
this denominator seems kind of tricky.

00:27:48.210 --> 00:27:51.385
Because this is P sub Zj,

00:27:51.385 --> 00:27:56.700
where Zj is the ID number of that,
it's a random ID number.

00:27:56.700 --> 00:27:59.389
So in the denominator here we
have a random probability.

00:28:00.670 --> 00:28:01.575
How do we deal with that?

00:28:02.630 --> 00:28:05.960
Well, a simple way around that

00:28:05.960 --> 00:28:09.770
is to rewrite this using
indicator random variables.

00:28:09.770 --> 00:28:12.770
So let's just rewrite this as the sum,

00:28:12.770 --> 00:28:16.960
not just over these values
that you actually had.

00:28:16.960 --> 00:28:19.940
Let's sum over the entire
population j = 1 to N.

00:28:19.940 --> 00:28:25.819
And let's sum yj,
which is just a constant,

00:28:25.819 --> 00:28:30.305
divided by pj times Ij, where Ij is

00:28:30.305 --> 00:28:35.900
the indicator of person j being included.

00:28:40.533 --> 00:28:48.265
So that's exactly the same thing,
In the sample.

00:28:48.265 --> 00:28:52.450
That's the same thing because anyone who's
not included just get zeroed out, and

00:28:52.450 --> 00:28:56.180
anyone who is included that's 1 and
then it's just the same thing here.

00:28:56.180 --> 00:28:59.890
But now it's just Pj in the denominator,
easy to deal with.

00:28:59.890 --> 00:29:05.340
Pzj, that's a random probability,
Pj, very simple, okay?

00:29:05.340 --> 00:29:06.820
So now the expected value.

00:29:13.139 --> 00:29:17.550
I'll just take the expected
value of this linearity, right?

00:29:17.550 --> 00:29:20.850
So I'm just gonna swap the e in the sum

00:29:23.920 --> 00:29:29.320
by linearity, respective value of ij,
by definition, and

00:29:29.320 --> 00:29:34.130
fundamental bridge is the probability
that person j is included in the sample,

00:29:34.130 --> 00:29:36.170
by definition that's pj.

00:29:36.170 --> 00:29:39.448
So this is just pj over pj, yj, done.

00:29:42.390 --> 00:29:44.860
So that's a very, very simple trick.

00:29:45.950 --> 00:29:48.700
Just divide by the probability.

00:29:48.700 --> 00:29:50.420
This is used all over the place now.

00:29:50.420 --> 00:29:54.080
Variations on this trick
is a very important idea.

00:29:54.080 --> 00:29:58.740
Inverse probability, this is either
called Horvitz-Thompson Estimator.

00:29:58.740 --> 00:30:01.720
We just derived the Horvitz-Thompson
Estimator, it wasn't too difficult.

00:30:03.240 --> 00:30:06.580
Or it's called inverse probability
waiting, and you'll find there's tons and

00:30:06.580 --> 00:30:11.050
tons of people using this,
inverse probability waiting.

00:30:14.080 --> 00:30:16.900
Okay, well, I mean so you can do that.

00:30:16.900 --> 00:30:17.960
That's unbiased.

00:30:18.980 --> 00:30:22.934
Now we come to a deeper question,
is unbiased good?

00:30:22.934 --> 00:30:27.120
Well, I mean,
I'd rather be unbiased than biased but

00:30:27.120 --> 00:30:30.490
is that good in and
of itself as a criterion?

00:30:30.490 --> 00:30:37.670
Is that good enough to tell us that
this is actually a good estimator?

00:30:37.670 --> 00:30:40.420
Not so much a harder question.

00:30:40.420 --> 00:30:47.390
And I'll tell you a quick story showing
that this can be horribly, horribly bad.

00:30:47.390 --> 00:30:48.770
I'm not saying it's always bad, either.

00:30:48.770 --> 00:30:49.910
It can be good, it can be bad.

00:30:49.910 --> 00:30:51.850
I'm saying this goes much, much deeper.

00:30:53.750 --> 00:30:56.440
This is one of my favorite examples.

00:30:56.440 --> 00:31:02.840
This is called Basu's Elephant, and
Basu was one of my favorite statisticians.

00:31:04.150 --> 00:31:07.250
So I'll just tell you Basu's
elephant story quickly.

00:31:07.250 --> 00:31:12.610
This is an indication that there's more,
there's a lot going on here.

00:31:12.610 --> 00:31:14.730
Statistics cannot be described as,

00:31:14.730 --> 00:31:17.680
you learn some formulas and
you plug it in and whatever.

00:31:17.680 --> 00:31:20.990
You need to think really hard about,
doesn't make sense?

00:31:20.990 --> 00:31:24.870
This is useful, you can't just say
all time biased so we're happy.

00:31:24.870 --> 00:31:28.840
So the story is there was a circus
owner who had 50 elephants, and

00:31:28.840 --> 00:31:32.920
he wanted to know that the average
weight of his elephants,

00:31:32.920 --> 00:31:36.300
equivalently he wanted to know
the total weight of the 50 elephants.

00:31:36.300 --> 00:31:38.440
And I've never tried weighing an elephant,
but

00:31:38.440 --> 00:31:42.795
apparently it's pretty hard to actually
physically weigh the elephant.

00:31:42.795 --> 00:31:45.570
Too much work to try to
weigh the 50 elephants.

00:31:45.570 --> 00:31:48.620
So the circus said, okay,
I'll just take one that looks average.

00:31:48.620 --> 00:31:51.070
Let's call that elephant Stampie.

00:31:51.070 --> 00:31:54.590
Take Stampie, weigh Stampie,
multiply by 50.

00:31:54.590 --> 00:31:57.402
Sounds like a reasonable thing to do.

00:31:57.402 --> 00:31:58.620
Right?

00:31:58.620 --> 00:32:02.591
But a statistician overhears that and
gets very, very agitated.

00:32:02.591 --> 00:32:05.920
And then says, well but
that would be biased, right?

00:32:05.920 --> 00:32:08.540
And so the circus owner says okay,
you're the statistical expert.

00:32:08.540 --> 00:32:09.209
What should we do?

00:32:09.209 --> 00:32:12.730
And the statistician, this is actually
not a very good statistician.

00:32:12.730 --> 00:32:13.890
It's someone who just kind
of learned some formulas.

00:32:13.890 --> 00:32:17.290
And says,
well you should use the Horvitz–Thompson.

00:32:17.290 --> 00:32:19.688
You know, we do this
inverse-probability-weighting thing.

00:32:19.688 --> 00:32:21.570
And otherwise you're gonna be biased,
okay?

00:32:21.570 --> 00:32:27.320
And the circus owner says well,
I really wanna weigh Stampie's right here.

00:32:27.320 --> 00:32:30.612
The other elephants are off wandering
somewhere else and I really like Stampy.

00:32:30.612 --> 00:32:33.170
H's not gonna kick me and stuff.

00:32:33.170 --> 00:32:36.150
So anyway, then the statistician says,
that's fine.

00:32:36.150 --> 00:32:40.220
And this Horvitz–Thompson,
these probabilities pj.

00:32:40.220 --> 00:32:45.283
I should've written pj &gt; 0 because
then we're not dividing by 0.

00:32:45.283 --> 00:32:51.590
But as long as pj is greater than 0,
this's perfectly fine.

00:32:51.590 --> 00:32:53.304
It's still unbiased, so okay.

00:32:53.304 --> 00:32:57.490
So they decided to give probability,
they're only gonna weigh one elephant.

00:32:57.490 --> 00:33:00.857
They decide to give
probability 0.99 to Stampy.

00:33:00.857 --> 00:33:03.410
And then for
the other 49 elephants take the other 1%.

00:33:03.410 --> 00:33:07.408
And divide that up equally
to the other 49 elephants.

00:33:07.408 --> 00:33:13.710
Sure enough,
99% chance that they get to weigh Stampy.

00:33:13.710 --> 00:33:16.242
So they weigh Stampy and
everyone seems happy.

00:33:16.242 --> 00:33:20.772
Until what the circus owner was
proposing to do is take Stampy's weight,

00:33:20.772 --> 00:33:22.610
multiply it by 50, right?

00:33:22.610 --> 00:33:25.005
It seems like a natural thing to do.

00:33:25.005 --> 00:33:28.233
Well Horvitz–Thompson says
take Stampy's weight.

00:33:28.233 --> 00:33:31.131
Divide by the probability,
which was 99 over 100.

00:33:31.131 --> 00:33:34.276
So just take take Stampy and
multiply by 100 over 99,

00:33:34.276 --> 00:33:36.470
which is just slightly greater than 1.

00:33:36.470 --> 00:33:41.351
Now that doesn't sound like a very
good estimate for 50 elephants,

00:33:41.351 --> 00:33:45.499
just to take one of them and
multiply it by 100 over 99.

00:33:45.499 --> 00:33:47.264
And so the circus owner says,

00:33:47.264 --> 00:33:50.520
well something seems a little
suspicious about that.

00:33:50.520 --> 00:33:51.080
Are you sure that that's good?

00:33:51.080 --> 00:33:55.822
And the statistician says, well if
you'd gotten one of the other ones,

00:33:55.822 --> 00:33:58.580
well then you multiply it by 4,900.

00:33:58.580 --> 00:34:01.550
So then it's unbiased.

00:34:01.550 --> 00:34:04.658
So there are very, very, very,

00:34:04.658 --> 00:34:10.319
very tricky questions about what
are kind of good criteria or

00:34:10.319 --> 00:34:14.870
natural criteria for
when is an estimator good.

00:34:14.870 --> 00:34:16.760
What does that mean?

00:34:16.760 --> 00:34:20.250
That goes back into 111 a lot.

00:34:20.250 --> 00:34:25.160
111 is gonna be a lot about
Bayesian versus frequentist.

00:34:26.960 --> 00:34:31.390
So we've been talking about
some Bayesian stuff, right?

00:34:31.390 --> 00:34:35.638
Like we talked about conjugate priors and
the beta binomial, things like that.

00:34:35.638 --> 00:34:38.780
They'll do a lot of conjugate priors,
things like that.

00:34:38.780 --> 00:34:42.360
Frequent unbiasedness is
inherently a frequentist concept.

00:34:42.360 --> 00:34:44.340
But there's other frequentist
concepts as well.

00:34:44.340 --> 00:34:49.840
You can ask when are these two methods or
ideas.

00:34:49.840 --> 00:34:53.910
Well, first of all you can discuss whether
they even are well defined methods.

00:34:53.910 --> 00:34:59.160
But when would they agree, when they
would disagree, things like that.

00:35:00.610 --> 00:35:06.215
There are very, very,
very subtle questions that come up,

00:35:06.215 --> 00:35:08.400
wery, very surprising.

00:35:08.400 --> 00:35:13.620
There are a lot of paradoxes in where
you write down something that seems

00:35:13.620 --> 00:35:17.489
like a perfectly natural estimator and
you know it.

00:35:17.489 --> 00:35:21.970
And then you can prove that actually
you can always do better than that.

00:35:21.970 --> 00:35:23.310
And things like that.

00:35:25.220 --> 00:35:26.770
Okay well, so let's see.

00:35:26.770 --> 00:35:28.650
Any other courses to mention here?

00:35:28.650 --> 00:35:35.150
All right, so this is our little
Markov chain, well it's not really.

00:35:35.150 --> 00:35:38.790
Well the question is whether
it's irreducible, I guess.

00:35:38.790 --> 00:35:41.360
It's irreducible right now.

00:35:41.360 --> 00:35:43.850
Now, of course,
you could go From 111 to 139.

00:35:43.850 --> 00:35:45.780
You can go from 139 to 111.

00:35:45.780 --> 00:35:49.409
You can go all over the place.

00:35:49.409 --> 00:35:53.620
There are a couple more quick
edges we should draw here.

00:35:53.620 --> 00:35:56.406
Once is 110 goes to jobs.

00:35:56.406 --> 00:36:00.155
&gt;&gt; [LAUGH]
&gt;&gt; So if any of you get any good

00:36:00.155 --> 00:36:04.342
interview questions that relate to
probability, please send them my way.

00:36:04.342 --> 00:36:07.943
I won't get into the whole
Vocational debate right now,

00:36:07.943 --> 00:36:10.540
which has been coming up more.

00:36:10.540 --> 00:36:12.037
And one more.

00:36:12.037 --> 00:36:18.659
The 110 is actually a recurrent
state in this chain.

00:36:18.659 --> 00:36:24.585
And William is here who took the class
last year, so we have an example.

00:36:24.585 --> 00:36:28.415
Someone else just emailed me yesterday
saying she took it last year.

00:36:28.415 --> 00:36:33.002
And really wished, more than anything,
she could just repeat the class over and

00:36:33.002 --> 00:36:33.755
over again.

00:36:33.755 --> 00:36:37.073
Now I'm hoping you're not
gonna repeat it over and

00:36:37.073 --> 00:36:42.183
over again in the sense that you actually
have to literally retake the course.

00:36:42.183 --> 00:36:43.690
&gt;&gt; [LAUGH]
&gt;&gt; But

00:36:45.160 --> 00:36:50.270
in the sense of revisiting the material,
over and over again, is a good thing.

00:36:50.270 --> 00:36:51.013
Okay, well that's all.

00:36:51.013 --> 00:36:58.138
Thank you, it was all fun.

00:36:58.138 --> 00:36:58.638
[APPLAUSE]

