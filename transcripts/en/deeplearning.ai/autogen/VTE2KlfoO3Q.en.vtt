WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.659
 
being effective in developing your deep

00:00:02.659 --> 00:00:02.669
being effective in developing your deep
 

00:00:02.669 --> 00:00:04.370
being effective in developing your deep
neural Nets requires that you not only

00:00:04.370 --> 00:00:04.380
neural Nets requires that you not only
 

00:00:04.380 --> 00:00:06.860
neural Nets requires that you not only
organize your parameters well but also

00:00:06.860 --> 00:00:06.870
organize your parameters well but also
 

00:00:06.870 --> 00:00:09.259
organize your parameters well but also
your hyper parameters so what are hyper

00:00:09.259 --> 00:00:09.269
your hyper parameters so what are hyper
 

00:00:09.269 --> 00:00:11.749
your hyper parameters so what are hyper
parameters let's take a look so the

00:00:11.749 --> 00:00:11.759
parameters let's take a look so the
 

00:00:11.759 --> 00:00:15.160
parameters let's take a look so the
parameters your model our W and B and

00:00:15.160 --> 00:00:15.170
parameters your model our W and B and
 

00:00:15.170 --> 00:00:17.810
parameters your model our W and B and
there are other things you need to tell

00:00:17.810 --> 00:00:17.820
there are other things you need to tell
 

00:00:17.820 --> 00:00:21.710
there are other things you need to tell
your learning algorithm such as the

00:00:21.710 --> 00:00:21.720
your learning algorithm such as the
 

00:00:21.720 --> 00:00:26.240
your learning algorithm such as the
learning rate alpha because um you need

00:00:26.240 --> 00:00:26.250
learning rate alpha because um you need
 

00:00:26.250 --> 00:00:28.910
learning rate alpha because um you need
to set alpha and that in turn will

00:00:28.910 --> 00:00:28.920
to set alpha and that in turn will
 

00:00:28.920 --> 00:00:32.319
to set alpha and that in turn will
determine how your parameters evolve or

00:00:32.319 --> 00:00:32.329
determine how your parameters evolve or
 

00:00:32.329 --> 00:00:34.880
determine how your parameters evolve or
maybe the number of iterations of

00:00:34.880 --> 00:00:34.890
maybe the number of iterations of
 

00:00:34.890 --> 00:00:37.549
maybe the number of iterations of
gradient descent you carry out your

00:00:37.549 --> 00:00:37.559
gradient descent you carry out your
 

00:00:37.559 --> 00:00:40.130
gradient descent you carry out your
learning algorithm has other you know

00:00:40.130 --> 00:00:40.140
learning algorithm has other you know
 

00:00:40.140 --> 00:00:42.619
learning algorithm has other you know
numbers that you need to set such as the

00:00:42.619 --> 00:00:42.629
numbers that you need to set such as the
 

00:00:42.629 --> 00:00:47.330
numbers that you need to set such as the
number of hidden layers so we call that

00:00:47.330 --> 00:00:47.340
number of hidden layers so we call that
 

00:00:47.340 --> 00:00:50.590
number of hidden layers so we call that
capital L or the number of hidden units

00:00:50.590 --> 00:00:50.600
capital L or the number of hidden units
 

00:00:50.600 --> 00:00:56.029
capital L or the number of hidden units
right such as zero and one and two and

00:00:56.029 --> 00:00:56.039
right such as zero and one and two and
 

00:00:56.039 --> 00:00:59.660
right such as zero and one and two and
so on and then you also have the choice

00:00:59.660 --> 00:00:59.670
so on and then you also have the choice
 

00:00:59.670 --> 00:01:03.319
so on and then you also have the choice
of activation function do you want to

00:01:03.319 --> 00:01:03.329
of activation function do you want to
 

00:01:03.329 --> 00:01:05.870
of activation function do you want to
use a value or ten age or Sigma little

00:01:05.870 --> 00:01:05.880
use a value or ten age or Sigma little
 

00:01:05.880 --> 00:01:07.160
use a value or ten age or Sigma little
something especially in the hidden

00:01:07.160 --> 00:01:07.170
something especially in the hidden
 

00:01:07.170 --> 00:01:12.170
something especially in the hidden
layers and so all of these things are

00:01:12.170 --> 00:01:12.180
layers and so all of these things are
 

00:01:12.180 --> 00:01:13.550
layers and so all of these things are
things that you need to tell your

00:01:13.550 --> 00:01:13.560
things that you need to tell your
 

00:01:13.560 --> 00:01:15.920
things that you need to tell your
learning algorithm and so these are

00:01:15.920 --> 00:01:15.930
learning algorithm and so these are
 

00:01:15.930 --> 00:01:19.630
learning algorithm and so these are
parameters that control the ultimate

00:01:19.630 --> 00:01:19.640
parameters that control the ultimate
 

00:01:19.640 --> 00:01:22.190
parameters that control the ultimate
parameters W and B and so we call all of

00:01:22.190 --> 00:01:22.200
parameters W and B and so we call all of
 

00:01:22.200 --> 00:01:25.630
parameters W and B and so we call all of
these things below hyper parameters

00:01:25.630 --> 00:01:25.640
these things below hyper parameters
 

00:01:25.640 --> 00:01:29.330
these things below hyper parameters
because these things like alpha the

00:01:29.330 --> 00:01:29.340
because these things like alpha the
 

00:01:29.340 --> 00:01:30.740
because these things like alpha the
learning rate the number of iterations

00:01:30.740 --> 00:01:30.750
learning rate the number of iterations
 

00:01:30.750 --> 00:01:32.359
learning rate the number of iterations
number of hidden layers and so on these

00:01:32.359 --> 00:01:32.369
number of hidden layers and so on these
 

00:01:32.369 --> 00:01:35.990
number of hidden layers and so on these
are all parameters that control W and B

00:01:35.990 --> 00:01:36.000
are all parameters that control W and B
 

00:01:36.000 --> 00:01:39.280
are all parameters that control W and B
so we call these things hyper parameters

00:01:39.280 --> 00:01:39.290
so we call these things hyper parameters
 

00:01:39.290 --> 00:01:41.960
so we call these things hyper parameters
because it is the hyper parameters that

00:01:41.960 --> 00:01:41.970
because it is the hyper parameters that
 

00:01:41.970 --> 00:01:44.240
because it is the hyper parameters that
you know somehow determine the final

00:01:44.240 --> 00:01:44.250
you know somehow determine the final
 

00:01:44.250 --> 00:01:46.880
you know somehow determine the final
value of the parameters W and B that you

00:01:46.880 --> 00:01:46.890
value of the parameters W and B that you
 

00:01:46.890 --> 00:01:50.090
value of the parameters W and B that you
end up with in fact deep learning has a

00:01:50.090 --> 00:01:50.100
end up with in fact deep learning has a
 

00:01:50.100 --> 00:01:53.510
end up with in fact deep learning has a
lot of different hyper parameters later

00:01:53.510 --> 00:01:53.520
lot of different hyper parameters later
 

00:01:53.520 --> 00:01:55.460
lot of different hyper parameters later
in the later course we'll see other

00:01:55.460 --> 00:01:55.470
in the later course we'll see other
 

00:01:55.470 --> 00:01:57.920
in the later course we'll see other
hyper parameters as well such as the

00:01:57.920 --> 00:01:57.930
hyper parameters as well such as the
 

00:01:57.930 --> 00:02:05.840
hyper parameters as well such as the
momentum term the mini batch size

00:02:05.840 --> 00:02:05.850
 
 

00:02:05.850 --> 00:02:07.210
 
various forms of regularization

00:02:07.210 --> 00:02:07.220
various forms of regularization
 

00:02:07.220 --> 00:02:13.010
various forms of regularization
parameters and so on and if none of

00:02:13.010 --> 00:02:13.020
parameters and so on and if none of
 

00:02:13.020 --> 00:02:14.690
parameters and so on and if none of
these terms of the bottom make sense yet

00:02:14.690 --> 00:02:14.700
these terms of the bottom make sense yet
 

00:02:14.700 --> 00:02:16.010
these terms of the bottom make sense yet
don't worry about it we'll talk about

00:02:16.010 --> 00:02:16.020
don't worry about it we'll talk about
 

00:02:16.020 --> 00:02:18.800
don't worry about it we'll talk about
them in a second pause because deep

00:02:18.800 --> 00:02:18.810
them in a second pause because deep
 

00:02:18.810 --> 00:02:21.860
them in a second pause because deep
learning has so many hyper parameters in

00:02:21.860 --> 00:02:21.870
learning has so many hyper parameters in
 

00:02:21.870 --> 00:02:24.110
learning has so many hyper parameters in
contrast to earlier errors of machine

00:02:24.110 --> 00:02:24.120
contrast to earlier errors of machine
 

00:02:24.120 --> 00:02:26.300
contrast to earlier errors of machine
learning I'm going to try to be very

00:02:26.300 --> 00:02:26.310
learning I'm going to try to be very
 

00:02:26.310 --> 00:02:28.910
learning I'm going to try to be very
consistent in calling the learning rate

00:02:28.910 --> 00:02:28.920
consistent in calling the learning rate
 

00:02:28.920 --> 00:02:31.040
consistent in calling the learning rate
alpha a hyper parameter rather than

00:02:31.040 --> 00:02:31.050
alpha a hyper parameter rather than
 

00:02:31.050 --> 00:02:33.470
alpha a hyper parameter rather than
calling a parameter I think in earlier

00:02:33.470 --> 00:02:33.480
calling a parameter I think in earlier
 

00:02:33.480 --> 00:02:35.240
calling a parameter I think in earlier
eras of machine learning when we didn't

00:02:35.240 --> 00:02:35.250
eras of machine learning when we didn't
 

00:02:35.250 --> 00:02:37.910
eras of machine learning when we didn't
have so many hyper parameters most of us

00:02:37.910 --> 00:02:37.920
have so many hyper parameters most of us
 

00:02:37.920 --> 00:02:39.800
have so many hyper parameters most of us
used to be a bit sloppier and just call

00:02:39.800 --> 00:02:39.810
used to be a bit sloppier and just call
 

00:02:39.810 --> 00:02:42.500
used to be a bit sloppier and just call
alpha a parameter and technically alpha

00:02:42.500 --> 00:02:42.510
alpha a parameter and technically alpha
 

00:02:42.510 --> 00:02:45.020
alpha a parameter and technically alpha
is a parameter but is a parameter that

00:02:45.020 --> 00:02:45.030
is a parameter but is a parameter that
 

00:02:45.030 --> 00:02:47.750
is a parameter but is a parameter that
determines the real programmers or try

00:02:47.750 --> 00:02:47.760
determines the real programmers or try
 

00:02:47.760 --> 00:02:50.600
determines the real programmers or try
to consistent in calling these things

00:02:50.600 --> 00:02:50.610
to consistent in calling these things
 

00:02:50.610 --> 00:02:52.280
to consistent in calling these things
like alpha the number of iterations and

00:02:52.280 --> 00:02:52.290
like alpha the number of iterations and
 

00:02:52.290 --> 00:02:54.470
like alpha the number of iterations and
so on hyper parameters so when you're

00:02:54.470 --> 00:02:54.480
so on hyper parameters so when you're
 

00:02:54.480 --> 00:02:55.880
so on hyper parameters so when you're
training a deep net for your own

00:02:55.880 --> 00:02:55.890
training a deep net for your own
 

00:02:55.890 --> 00:02:58.009
training a deep net for your own
application you find that there may be a

00:02:58.009 --> 00:02:58.019
application you find that there may be a
 

00:02:58.019 --> 00:03:00.140
application you find that there may be a
lot of possible settings for the hyper

00:03:00.140 --> 00:03:00.150
lot of possible settings for the hyper
 

00:03:00.150 --> 00:03:02.390
lot of possible settings for the hyper
parameters that you need to just try out

00:03:02.390 --> 00:03:02.400
parameters that you need to just try out
 

00:03:02.400 --> 00:03:04.970
parameters that you need to just try out
so apply deep learning today is a very

00:03:04.970 --> 00:03:04.980
so apply deep learning today is a very
 

00:03:04.980 --> 00:03:07.460
so apply deep learning today is a very
imperiled process where often you might

00:03:07.460 --> 00:03:07.470
imperiled process where often you might
 

00:03:07.470 --> 00:03:09.949
imperiled process where often you might
have an idea for example you might have

00:03:09.949 --> 00:03:09.959
have an idea for example you might have
 

00:03:09.959 --> 00:03:12.110
have an idea for example you might have
an idea for the best value for the

00:03:12.110 --> 00:03:12.120
an idea for the best value for the
 

00:03:12.120 --> 00:03:13.539
an idea for the best value for the
learning rate you might say well maybe

00:03:13.539 --> 00:03:13.549
learning rate you might say well maybe
 

00:03:13.549 --> 00:03:16.759
learning rate you might say well maybe
alpha equals 0.01 I want to try that

00:03:16.759 --> 00:03:16.769
alpha equals 0.01 I want to try that
 

00:03:16.769 --> 00:03:20.660
alpha equals 0.01 I want to try that
then you implemented try it out and then

00:03:20.660 --> 00:03:20.670
then you implemented try it out and then
 

00:03:20.670 --> 00:03:22.520
then you implemented try it out and then
see how that works and then based on

00:03:22.520 --> 00:03:22.530
see how that works and then based on
 

00:03:22.530 --> 00:03:23.900
see how that works and then based on
that outcome you might say you know what

00:03:23.900 --> 00:03:23.910
that outcome you might say you know what
 

00:03:23.910 --> 00:03:25.910
that outcome you might say you know what
I've changed online I want to increase

00:03:25.910 --> 00:03:25.920
I've changed online I want to increase
 

00:03:25.920 --> 00:03:29.000
I've changed online I want to increase
the learning rate to 0.05 and so if

00:03:29.000 --> 00:03:29.010
the learning rate to 0.05 and so if
 

00:03:29.010 --> 00:03:30.920
the learning rate to 0.05 and so if
you're not sure what's the best value

00:03:30.920 --> 00:03:30.930
you're not sure what's the best value
 

00:03:30.930 --> 00:03:32.930
you're not sure what's the best value
for the learning ready to use you might

00:03:32.930 --> 00:03:32.940
for the learning ready to use you might
 

00:03:32.940 --> 00:03:35.000
for the learning ready to use you might
try one value of the learning rate alpha

00:03:35.000 --> 00:03:35.010
try one value of the learning rate alpha
 

00:03:35.010 --> 00:03:37.880
try one value of the learning rate alpha
and see the cost function J go down like

00:03:37.880 --> 00:03:37.890
and see the cost function J go down like
 

00:03:37.890 --> 00:03:40.009
and see the cost function J go down like
this then you might try a larger value

00:03:40.009 --> 00:03:40.019
this then you might try a larger value
 

00:03:40.019 --> 00:03:42.380
this then you might try a larger value
for the learning rate alpha and see the

00:03:42.380 --> 00:03:42.390
for the learning rate alpha and see the
 

00:03:42.390 --> 00:03:44.420
for the learning rate alpha and see the
cost function blow up and diverge then

00:03:44.420 --> 00:03:44.430
cost function blow up and diverge then
 

00:03:44.430 --> 00:03:46.430
cost function blow up and diverge then
you might try another version and see it

00:03:46.430 --> 00:03:46.440
you might try another version and see it
 

00:03:46.440 --> 00:03:48.170
you might try another version and see it
go down really fast the converse to

00:03:48.170 --> 00:03:48.180
go down really fast the converse to
 

00:03:48.180 --> 00:03:50.090
go down really fast the converse to
higher value you might try another

00:03:50.090 --> 00:03:50.100
higher value you might try another
 

00:03:50.100 --> 00:03:52.460
higher value you might try another
version and see it you know see the cost

00:03:52.460 --> 00:03:52.470
version and see it you know see the cost
 

00:03:52.470 --> 00:03:54.800
version and see it you know see the cost
function J do that then after trial so

00:03:54.800 --> 00:03:54.810
function J do that then after trial so
 

00:03:54.810 --> 00:03:56.449
function J do that then after trial so
the values you might say okay looks like

00:03:56.449 --> 00:03:56.459
the values you might say okay looks like
 

00:03:56.459 --> 00:03:59.569
the values you might say okay looks like
this the value of alpha gives me a

00:03:59.569 --> 00:03:59.579
this the value of alpha gives me a
 

00:03:59.579 --> 00:04:01.910
this the value of alpha gives me a
pretty fast learning and allows me to

00:04:01.910 --> 00:04:01.920
pretty fast learning and allows me to
 

00:04:01.920 --> 00:04:03.800
pretty fast learning and allows me to
converge to a lower cost function J so

00:04:03.800 --> 00:04:03.810
converge to a lower cost function J so
 

00:04:03.810 --> 00:04:05.569
converge to a lower cost function J so
I'm going to use this value of alpha you

00:04:05.569 --> 00:04:05.579
I'm going to use this value of alpha you
 

00:04:05.579 --> 00:04:07.699
I'm going to use this value of alpha you
saw on the previous slide that there are

00:04:07.699 --> 00:04:07.709
saw on the previous slide that there are
 

00:04:07.709 --> 00:04:09.740
saw on the previous slide that there are
a lot of different hyper parameters and

00:04:09.740 --> 00:04:09.750
a lot of different hyper parameters and
 

00:04:09.750 --> 00:04:10.970
a lot of different hyper parameters and
it turns out that when you're starting

00:04:10.970 --> 00:04:10.980
it turns out that when you're starting
 

00:04:10.980 --> 00:04:13.580
it turns out that when you're starting
on a new application I should find it

00:04:13.580 --> 00:04:13.590
on a new application I should find it
 

00:04:13.590 --> 00:04:15.190
on a new application I should find it
very difficult to know

00:04:15.190 --> 00:04:15.200
very difficult to know
 

00:04:15.200 --> 00:04:17.710
very difficult to know
exactly what's the best value of the

00:04:17.710 --> 00:04:17.720
exactly what's the best value of the
 

00:04:17.720 --> 00:04:20.229
exactly what's the best value of the
hyper parameters so what often happens

00:04:20.229 --> 00:04:20.239
hyper parameters so what often happens
 

00:04:20.239 --> 00:04:21.670
hyper parameters so what often happens
is you just have to try out many

00:04:21.670 --> 00:04:21.680
is you just have to try out many
 

00:04:21.680 --> 00:04:23.860
is you just have to try out many
different values and go around this

00:04:23.860 --> 00:04:23.870
different values and go around this
 

00:04:23.870 --> 00:04:25.720
different values and go around this
cycle your try out some value maybe

00:04:25.720 --> 00:04:25.730
cycle your try out some value maybe
 

00:04:25.730 --> 00:04:27.700
cycle your try out some value maybe
retry five hidden layers with different

00:04:27.700 --> 00:04:27.710
retry five hidden layers with different
 

00:04:27.710 --> 00:04:29.740
retry five hidden layers with different
number of hidden unions implement that

00:04:29.740 --> 00:04:29.750
number of hidden unions implement that
 

00:04:29.750 --> 00:04:33.550
number of hidden unions implement that
Steven works and then iterate so the

00:04:33.550 --> 00:04:33.560
Steven works and then iterate so the
 

00:04:33.560 --> 00:04:35.409
Steven works and then iterate so the
title of this slide is that apply deep

00:04:35.409 --> 00:04:35.419
title of this slide is that apply deep
 

00:04:35.419 --> 00:04:37.629
title of this slide is that apply deep
learning is very empirical process and

00:04:37.629 --> 00:04:37.639
learning is very empirical process and
 

00:04:37.639 --> 00:04:39.700
learning is very empirical process and
empirical process is maybe a fancy way

00:04:39.700 --> 00:04:39.710
empirical process is maybe a fancy way
 

00:04:39.710 --> 00:04:41.620
empirical process is maybe a fancy way
of saying you just have to try a lot of

00:04:41.620 --> 00:04:41.630
of saying you just have to try a lot of
 

00:04:41.630 --> 00:04:44.470
of saying you just have to try a lot of
things and see what works another effect

00:04:44.470 --> 00:04:44.480
things and see what works another effect
 

00:04:44.480 --> 00:04:46.450
things and see what works another effect
I've seen is that deep learning today is

00:04:46.450 --> 00:04:46.460
I've seen is that deep learning today is
 

00:04:46.460 --> 00:04:48.400
I've seen is that deep learning today is
applied to so many problems ranging from

00:04:48.400 --> 00:04:48.410
applied to so many problems ranging from
 

00:04:48.410 --> 00:04:51.490
applied to so many problems ranging from
computer vision to speech recognition to

00:04:51.490 --> 00:04:51.500
computer vision to speech recognition to
 

00:04:51.500 --> 00:04:53.800
computer vision to speech recognition to
natural language processing to a lot of

00:04:53.800 --> 00:04:53.810
natural language processing to a lot of
 

00:04:53.810 --> 00:04:55.330
natural language processing to a lot of
structured data applications such as

00:04:55.330 --> 00:04:55.340
structured data applications such as
 

00:04:55.340 --> 00:04:58.960
structured data applications such as
maybe a online advertising or on web

00:04:58.960 --> 00:04:58.970
maybe a online advertising or on web
 

00:04:58.970 --> 00:05:01.960
maybe a online advertising or on web
search or product recommendations and so

00:05:01.960 --> 00:05:01.970
search or product recommendations and so
 

00:05:01.970 --> 00:05:05.140
search or product recommendations and so
on and what I've seen is that first I've

00:05:05.140 --> 00:05:05.150
on and what I've seen is that first I've
 

00:05:05.150 --> 00:05:07.900
on and what I've seen is that first I've
seen researchers from one discipline any

00:05:07.900 --> 00:05:07.910
seen researchers from one discipline any
 

00:05:07.910 --> 00:05:09.940
seen researchers from one discipline any
one of these try to go to a different

00:05:09.940 --> 00:05:09.950
one of these try to go to a different
 

00:05:09.950 --> 00:05:11.890
one of these try to go to a different
one and sometimes the intuitions about

00:05:11.890 --> 00:05:11.900
one and sometimes the intuitions about
 

00:05:11.900 --> 00:05:13.600
one and sometimes the intuitions about
hyper parameters carries over and

00:05:13.600 --> 00:05:13.610
hyper parameters carries over and
 

00:05:13.610 --> 00:05:16.330
hyper parameters carries over and
sometimes it doesn't so I often advise

00:05:16.330 --> 00:05:16.340
sometimes it doesn't so I often advise
 

00:05:16.340 --> 00:05:17.830
sometimes it doesn't so I often advise
people especially when starting on a new

00:05:17.830 --> 00:05:17.840
people especially when starting on a new
 

00:05:17.840 --> 00:05:20.920
people especially when starting on a new
problem to just try out a range of

00:05:20.920 --> 00:05:20.930
problem to just try out a range of
 

00:05:20.930 --> 00:05:23.500
problem to just try out a range of
values and see what works and then next

00:05:23.500 --> 00:05:23.510
values and see what works and then next
 

00:05:23.510 --> 00:05:25.480
values and see what works and then next
course will see a systematic way we'll

00:05:25.480 --> 00:05:25.490
course will see a systematic way we'll
 

00:05:25.490 --> 00:05:27.909
course will see a systematic way we'll
see some systematic ways for trying out

00:05:27.909 --> 00:05:27.919
see some systematic ways for trying out
 

00:05:27.919 --> 00:05:30.969
see some systematic ways for trying out
a range of values right and second even

00:05:30.969 --> 00:05:30.979
a range of values right and second even
 

00:05:30.979 --> 00:05:32.920
a range of values right and second even
if you're working on one application for

00:05:32.920 --> 00:05:32.930
if you're working on one application for
 

00:05:32.930 --> 00:05:33.909
if you're working on one application for
a long time you know maybe you're

00:05:33.909 --> 00:05:33.919
a long time you know maybe you're
 

00:05:33.919 --> 00:05:37.029
a long time you know maybe you're
working on online advertising as you

00:05:37.029 --> 00:05:37.039
working on online advertising as you
 

00:05:37.039 --> 00:05:38.830
working on online advertising as you
make progress on the problem it's quite

00:05:38.830 --> 00:05:38.840
make progress on the problem it's quite
 

00:05:38.840 --> 00:05:40.630
make progress on the problem it's quite
possible to the best value for the

00:05:40.630 --> 00:05:40.640
possible to the best value for the
 

00:05:40.640 --> 00:05:41.830
possible to the best value for the
learning rate of number of hidden units

00:05:41.830 --> 00:05:41.840
learning rate of number of hidden units
 

00:05:41.840 --> 00:05:44.380
learning rate of number of hidden units
and so on might change so even if you

00:05:44.380 --> 00:05:44.390
and so on might change so even if you
 

00:05:44.390 --> 00:05:47.350
and so on might change so even if you
tune your system to the best value of

00:05:47.350 --> 00:05:47.360
tune your system to the best value of
 

00:05:47.360 --> 00:05:50.170
tune your system to the best value of
hyper parameters today it's possible you

00:05:50.170 --> 00:05:50.180
hyper parameters today it's possible you
 

00:05:50.180 --> 00:05:52.450
hyper parameters today it's possible you
find that the best value might change a

00:05:52.450 --> 00:05:52.460
find that the best value might change a
 

00:05:52.460 --> 00:05:54.370
find that the best value might change a
year from now maybe because of the

00:05:54.370 --> 00:05:54.380
year from now maybe because of the
 

00:05:54.380 --> 00:05:56.650
year from now maybe because of the
computer infrastructure it you know CPUs

00:05:56.650 --> 00:05:56.660
computer infrastructure it you know CPUs
 

00:05:56.660 --> 00:05:58.450
computer infrastructure it you know CPUs
or the type of GPU running on or

00:05:58.450 --> 00:05:58.460
or the type of GPU running on or
 

00:05:58.460 --> 00:06:00.430
or the type of GPU running on or
something has changed but so maybe one

00:06:00.430 --> 00:06:00.440
something has changed but so maybe one
 

00:06:00.440 --> 00:06:02.200
something has changed but so maybe one
rule of thumb is you know every now and

00:06:02.200 --> 00:06:02.210
rule of thumb is you know every now and
 

00:06:02.210 --> 00:06:04.210
rule of thumb is you know every now and
then or maybe every few months if you're

00:06:04.210 --> 00:06:04.220
then or maybe every few months if you're
 

00:06:04.220 --> 00:06:05.800
then or maybe every few months if you're
working on a problem for an extended

00:06:05.800 --> 00:06:05.810
working on a problem for an extended
 

00:06:05.810 --> 00:06:07.930
working on a problem for an extended
period of time for many years just try a

00:06:07.930 --> 00:06:07.940
period of time for many years just try a
 

00:06:07.940 --> 00:06:10.000
period of time for many years just try a
few values for the hyper parameters and

00:06:10.000 --> 00:06:10.010
few values for the hyper parameters and
 

00:06:10.010 --> 00:06:11.950
few values for the hyper parameters and
double check if there's a better value

00:06:11.950 --> 00:06:11.960
double check if there's a better value
 

00:06:11.960 --> 00:06:14.110
double check if there's a better value
for the hyper parameters and as you do

00:06:14.110 --> 00:06:14.120
for the hyper parameters and as you do
 

00:06:14.120 --> 00:06:16.270
for the hyper parameters and as you do
so you slowly gain intuition as well

00:06:16.270 --> 00:06:16.280
so you slowly gain intuition as well
 

00:06:16.280 --> 00:06:18.279
so you slowly gain intuition as well
about the hyper parameters that work

00:06:18.279 --> 00:06:18.289
about the hyper parameters that work
 

00:06:18.289 --> 00:06:19.899
about the hyper parameters that work
best for your problems

00:06:19.899 --> 00:06:19.909
best for your problems
 

00:06:19.909 --> 00:06:21.820
best for your problems
and I know that this might seem like an

00:06:21.820 --> 00:06:21.830
and I know that this might seem like an
 

00:06:21.830 --> 00:06:23.980
and I know that this might seem like an
unsatisfying part of deep learning that

00:06:23.980 --> 00:06:23.990
unsatisfying part of deep learning that
 

00:06:23.990 --> 00:06:25.480
unsatisfying part of deep learning that
you just have to try out other values

00:06:25.480 --> 00:06:25.490
you just have to try out other values
 

00:06:25.490 --> 00:06:28.209
you just have to try out other values
for these hyper answers but maybe this

00:06:28.209 --> 00:06:28.219
for these hyper answers but maybe this
 

00:06:28.219 --> 00:06:30.760
for these hyper answers but maybe this
is one area where deep learning research

00:06:30.760 --> 00:06:30.770
is one area where deep learning research
 

00:06:30.770 --> 00:06:32.739
is one area where deep learning research
is still advancing and maybe over time

00:06:32.739 --> 00:06:32.749
is still advancing and maybe over time
 

00:06:32.749 --> 00:06:34.389
is still advancing and maybe over time
we'll be able to give better guidance

00:06:34.389 --> 00:06:34.399
we'll be able to give better guidance
 

00:06:34.399 --> 00:06:37.359
we'll be able to give better guidance
for the best hyper parameters to use but

00:06:37.359 --> 00:06:37.369
for the best hyper parameters to use but
 

00:06:37.369 --> 00:06:39.969
for the best hyper parameters to use but
it's also possible that because CPUs and

00:06:39.969 --> 00:06:39.979
it's also possible that because CPUs and
 

00:06:39.979 --> 00:06:42.070
it's also possible that because CPUs and
GPUs and networks and data cells are all

00:06:42.070 --> 00:06:42.080
GPUs and networks and data cells are all
 

00:06:42.080 --> 00:06:44.949
GPUs and networks and data cells are all
changing and it is possible that the

00:06:44.949 --> 00:06:44.959
changing and it is possible that the
 

00:06:44.959 --> 00:06:46.989
changing and it is possible that the
guidance won't to converge for some time

00:06:46.989 --> 00:06:46.999
guidance won't to converge for some time
 

00:06:46.999 --> 00:06:48.429
guidance won't to converge for some time
and you just need to keep trying out

00:06:48.429 --> 00:06:48.439
and you just need to keep trying out
 

00:06:48.439 --> 00:06:50.499
and you just need to keep trying out
different values and evaluate them on a

00:06:50.499 --> 00:06:50.509
different values and evaluate them on a
 

00:06:50.509 --> 00:06:51.999
different values and evaluate them on a
hold on cross-validation set or

00:06:51.999 --> 00:06:52.009
hold on cross-validation set or
 

00:06:52.009 --> 00:06:53.799
hold on cross-validation set or
something and pick the value that works

00:06:53.799 --> 00:06:53.809
something and pick the value that works
 

00:06:53.809 --> 00:06:55.749
something and pick the value that works
for your problems so that was a brief

00:06:55.749 --> 00:06:55.759
for your problems so that was a brief
 

00:06:55.759 --> 00:06:58.239
for your problems so that was a brief
discussion of hyper parameters in the

00:06:58.239 --> 00:06:58.249
discussion of hyper parameters in the
 

00:06:58.249 --> 00:07:00.249
discussion of hyper parameters in the
second course we'll also give some

00:07:00.249 --> 00:07:00.259
second course we'll also give some
 

00:07:00.259 --> 00:07:02.109
second course we'll also give some
suggestions to how to systematically

00:07:02.109 --> 00:07:02.119
suggestions to how to systematically
 

00:07:02.119 --> 00:07:04.230
suggestions to how to systematically
explore the space of hyper parameters

00:07:04.230 --> 00:07:04.240
explore the space of hyper parameters
 

00:07:04.240 --> 00:07:06.669
explore the space of hyper parameters
but by now you actually have pretty much

00:07:06.669 --> 00:07:06.679
but by now you actually have pretty much
 

00:07:06.679 --> 00:07:08.199
but by now you actually have pretty much
all the tools you need to do their

00:07:08.199 --> 00:07:08.209
all the tools you need to do their
 

00:07:08.209 --> 00:07:09.999
all the tools you need to do their
programming sir sighs before you do that

00:07:09.999 --> 00:07:10.009
programming sir sighs before you do that
 

00:07:10.009 --> 00:07:12.399
programming sir sighs before you do that
just share view one more set of ideas

00:07:12.399 --> 00:07:12.409
just share view one more set of ideas
 

00:07:12.409 --> 00:07:14.980
just share view one more set of ideas
which is I often ask what does deep

00:07:14.980 --> 00:07:14.990
which is I often ask what does deep
 

00:07:14.990 --> 00:07:18.669
which is I often ask what does deep
learning have to do the human brain

