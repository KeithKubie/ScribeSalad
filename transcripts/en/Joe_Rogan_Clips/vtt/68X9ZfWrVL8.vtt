WEBVTT

1
00:00:00.990 --> 00:00:05.950
The Joe Rogan experience.
So what do you picture like,

2
00:00:06.150 --> 00:00:10.560
cause we have to look at Boston dynamics robots.
Cause you said walking around.

3
00:00:10.590 --> 00:00:15.540
Yeah,
I'd like to get to a sense of how you think about,

4
00:00:15.690 --> 00:00:20.580
and maybe I can talk about too where the technology is of what that artificial

5
00:00:20.581 --> 00:00:25.500
intelligence looks like in 20 years.
In 30 years,
they'll surprise you.

6
00:00:25.890 --> 00:00:28.860
So you have a sense that it has a humanlike form.

7
00:00:28.960 --> 00:00:29.470
<v 1>No,</v>

8
00:00:29.470 --> 00:00:32.650
I have a sense that it's going to take on the form the same way the automobile

9
00:00:32.651 --> 00:00:36.100
has.
If you go back and look at it like CTE has a CT,

10
00:00:36.101 --> 00:00:38.240
Fletcher has a um,

11
00:00:38.320 --> 00:00:41.770
beautiful old patina pickup truck.

12
00:00:41.800 --> 00:00:46.800
What do you say it was from like 58 or some shit 60 anyway.

13
00:00:47.080 --> 00:00:50.830
Old Ass cool.
Heavy metal,

14
00:00:50.831 --> 00:00:53.650
you know those sweeping round curves,

15
00:00:53.651 --> 00:00:57.940
those old school pickup trucks had now look at that and look at a Tesla
roadster.

16
00:00:58.330 --> 00:01:02.470
What in the fuck happened when the fuck happened?
I'll tell you what happened.

17
00:01:02.471 --> 00:01:03.880
They got better and better and better at it.

18
00:01:03.881 --> 00:01:05.410
They figured out the most effective shape.

19
00:01:05.650 --> 00:01:08.650
If you want a motherfucker to boot that,
that,
that little car.

20
00:01:08.710 --> 00:01:12.970
Have you seen that video where they have the Tesla roadster in a drag race or in

21
00:01:12.971 --> 00:01:16.210
a race against a Nissan Gtr?
It's a simulated video,

22
00:01:16.360 --> 00:01:19.090
but it's based on the actual horsepower of each car.

23
00:01:19.330 --> 00:01:23.920
I don't know if you've ever driven a Nissan Gtr,
but it is a fucking insane car.

24
00:01:24.070 --> 00:01:26.440
It's insane.
This is a,
um,

25
00:01:27.000 --> 00:01:31.720
a CGI version of what it would look like if these two cars raised against each

26
00:01:31.721 --> 00:01:36.400
other.
So the car on the,
the Nissan Gtr do it from the beginning,

27
00:01:36.401 --> 00:01:38.440
their coast.
Look how fast this thing pulls away.

28
00:01:38.620 --> 00:01:43.480
The Nissan GTR is fucking insanely fast man.
Insanely fast.

29
00:01:43.900 --> 00:01:47.170
But this Tesla is so on another level,

30
00:01:47.171 --> 00:01:50.590
it's so in the future that it's not even close.

31
00:01:50.650 --> 00:01:53.710
As the video gets further and further,
you see how ridiculous it is?

32
00:01:53.830 --> 00:01:56.950
It's essentially lapping that car.
It's going to go

33
00:01:57.000 --> 00:01:59.760
<v 0>look how far away it is.
Bye.
See Ya.
So the,</v>

34
00:01:59.761 --> 00:02:03.510
you're saying the human races will be the Nissan here.
Exactly.
Then

35
00:02:03.660 --> 00:02:05.030
<v 1>we're not even going to be the Nissan.</v>

36
00:02:05.040 --> 00:02:09.210
We're going to be CT Fletcher's pickup truck.
This is the future.

37
00:02:09.510 --> 00:02:14.370
There's not gonna be any limitations in terms of bipedal form or wings or not

38
00:02:14.371 --> 00:02:15.870
having wings.
If you can walk on it.

39
00:02:15.871 --> 00:02:18.990
I mean there's not gonna be any of that shit there and we might have a

40
00:02:18.991 --> 00:02:22.560
propulsion system or it might,
it's not going to be us.
And they might,

41
00:02:22.561 --> 00:02:26.730
they might design some sort of organic propulsion system like the way squid have

42
00:02:26.731 --> 00:02:28.630
and shit.
Who the fuck knows?

43
00:02:28.720 --> 00:02:32.400
<v 0>They could also operate in the space of language and ideas.
So for example,</v>

44
00:02:32.700 --> 00:02:34.890
I don't know if you're familiar with,
you know,
open Ai,

45
00:02:35.730 --> 00:02:40.730
it's a company they created a system called gpt to which does language modeling.

46
00:02:41.041 --> 00:02:45.390
This is something in machine learning where you basically unsupervised let the

47
00:02:45.391 --> 00:02:49.140
system just read a bunch of texts and then learns to generate new texts and

48
00:02:49.141 --> 00:02:54.141
they've created this system called gpt to that is able to generate very

49
00:02:55.321 --> 00:03:00.050
realistic texts.
A very realistic sounding text,

50
00:03:00.100 --> 00:03:03.580
not sounding,
but when you read it,
it makes,
it seems like a person.

51
00:03:03.581 --> 00:03:05.930
It seems like a person.
And the question there is,

52
00:03:05.931 --> 00:03:07.990
it raises a really interesting question.

53
00:03:08.320 --> 00:03:11.380
So talking about AI existing in our world,
it,

54
00:03:11.381 --> 00:03:13.720
it paints a picture of a world in five,

55
00:03:13.721 --> 00:03:18.721
10 years plus where most of the texts on the Internet is generated by Ai.

56
00:03:19.660 --> 00:03:22.810
And it's very difficult to know who's real and who's not.

57
00:03:23.660 --> 00:03:25.290
And one of the interesting things,

58
00:03:25.310 --> 00:03:28.120
I'd be curious from your perspective to get what your thoughts I,

59
00:03:28.130 --> 00:03:32.050
what open AI did is they didn't release the code for the full system.

60
00:03:32.051 --> 00:03:35.170
They only released a much weaker version of it publicly.

61
00:03:35.740 --> 00:03:37.450
So they only demonstrated it.

62
00:03:37.730 --> 00:03:41.860
And so they felt that it was their responsibility to hold back.

63
00:03:42.790 --> 00:03:46.030
Prior to that date,
everybody in the community,
including them,

64
00:03:46.031 --> 00:03:50.260
had open sourced everything.
But they felt that now at this point,

65
00:03:50.740 --> 00:03:54.520
part of it whiffs for publicity they wanted to raise the question is,

66
00:03:54.940 --> 00:03:59.940
when do we hold back on these systems when they're so strong,

67
00:04:00.460 --> 00:04:02.500
when they're so good at generating texts?
For example,

68
00:04:02.501 --> 00:04:06.090
in this case or at deep fakes at uh,

69
00:04:06.400 --> 00:04:10.510
generating fake Joe Rogan faces.
Jamie just did one you showed me earlier,

70
00:04:10.540 --> 00:04:15.370
Donald Trump's head.
Yeah,
it's crazy.
And this is something that Jamie can do.

71
00:04:15.371 --> 00:04:18.520
He's not even a video editor.
Yeah.
We were talking about it before the show.

72
00:04:18.760 --> 00:04:23.050
We could go crazy if you want.
It is one of those things where you go,

73
00:04:23.051 --> 00:04:25.000
where is this going to be in five years?

74
00:04:25.270 --> 00:04:27.130
Because five years ago we didn't have anything like this.

75
00:04:27.670 --> 00:04:30.580
Five years ago was a joke.
Right,
exactly.

76
00:04:30.610 --> 00:04:35.440
And then now it's still in the gray area between joke and something that could

77
00:04:35.441 --> 00:04:38.500
be at scale transformed the way we communicate.

78
00:04:38.501 --> 00:04:40.840
Do you ever go to Kyle Donovan's Instagram page?
Of course,

79
00:04:41.020 --> 00:04:42.340
one of the best look at that's made,

80
00:04:45.880 --> 00:04:50.770
it's killing me.
It's killing me.
This is,

81
00:04:51.240 --> 00:04:56.140
and that it looks so much like I'm really talking and it looks like what I would

82
00:04:56.141 --> 00:04:58.900
look like if I was fat and it could,
you know,

83
00:04:58.930 --> 00:05:02.210
of course that's really good and that it could be improved significantly and it

84
00:05:02.220 --> 00:05:06.280
can make you say anything.
So there's a lot of variants of this.
Yeah.

85
00:05:06.550 --> 00:05:10.180
We can take,
like for example,
uh,
full disclosure,

86
00:05:10.181 --> 00:05:14.410
I downloaded your face the entire like have a data set or your face.

87
00:05:14.411 --> 00:05:19.090
I'm sure other hackers do as how dare you.
Yeah.
So,
uh,
for this exact purpose,

88
00:05:19.150 --> 00:05:21.550
I mean,
if I'm thinking like this and I'm very busy,

89
00:05:21.640 --> 00:05:25.630
then there's other people doing exactly this for sure.
Because you happen,

90
00:05:25.631 --> 00:05:29.830
your podcasts happens to be one of the biggest data sets in the world of people

91
00:05:29.831 --> 00:05:34.831
talking and really high quality audio with high quality 10 80 p for most,

92
00:05:35.380 --> 00:05:40.150
for a few hundred episodes of people's faces.
The lighting could be better,

93
00:05:41.380 --> 00:05:43.870
not quite as we're making it degraded.

94
00:05:45.410 --> 00:05:49.400
Hackers and the Mike gets in,
it blocks part of your face from the top.

95
00:05:49.430 --> 00:05:49.950
And that's right.

96
00:05:49.950 --> 00:05:53.500
So the best guest of the ones where they keep the mic love the defect stuff I've

97
00:05:53.501 --> 00:05:57.650
been using removes the microphone within about a thousand iterations.
It does.

98
00:05:57.651 --> 00:05:58.370
It instantly,

99
00:05:58.370 --> 00:06:02.030
<v 1>it gets the,
gets rid of it paints over the face.
Wow.
Yeah.
So,</v>

100
00:06:02.440 --> 00:06:06.040
so you could basically make Joe Rogan say anything that,

101
00:06:06.300 --> 00:06:10.880
I think this is just one step before they finagle us into having a nuclear war

102
00:06:10.881 --> 00:06:12.620
against each other so they could take over the earth.

103
00:06:12.950 --> 00:06:15.830
What they're going to do is they're gonna design artificial intelligence that

104
00:06:15.831 --> 00:06:17.780
survives off of nuclear waste.

105
00:06:18.110 --> 00:06:21.360
And so then they encourage these stupid assholes to,
uh,

106
00:06:21.440 --> 00:06:24.740
go into a war with North Korea and Russia and we blow each other up,

107
00:06:24.980 --> 00:06:29.980
but we leave behind all this precious radioactive material that they use to them

108
00:06:30.261 --> 00:06:31.370
fashion,
their new world.

109
00:06:31.371 --> 00:06:34.670
And we'd come a thousand years from now and it's just fucking beautiful and

110
00:06:34.671 --> 00:06:37.860
pristine with artificial life everywhere.
No more,
no more biological.

111
00:06:37.861 --> 00:06:41.570
It was too messy.
Are you saying the current president is artificial life?

112
00:06:41.960 --> 00:06:44.270
I didn't say that.
Okay.
Which is that with that,

113
00:06:44.750 --> 00:06:48.980
cause you're saying start nuclear war?
No,
I don't think so.
He said there's,

114
00:06:49.100 --> 00:06:50.570
imagine if they did do that,

115
00:06:50.571 --> 00:06:54.800
they would have to started with him in the 70s I mean he's been around for a

116
00:06:54.801 --> 00:06:56.960
long time and talking about being president for a long time,

117
00:06:56.990 --> 00:07:00.770
maybe electronics have been playing the long game and they got him to the

118
00:07:00.771 --> 00:07:04.550
position and then they get to use all this grand scale of time.

119
00:07:04.580 --> 00:07:07.700
It's not really long game seventies well you know all about that Internet

120
00:07:07.701 --> 00:07:10.130
research agency.
Right.
You know about that?
Uh,

121
00:07:10.160 --> 00:07:13.050
that's the Russian company that,
uh,

122
00:07:13.130 --> 00:07:17.210
they're responsible for all these different Facebook pages where they would make

123
00:07:17.211 --> 00:07:20.390
people fight against each other.
It was really,
it's really kind of interesting.

124
00:07:20.480 --> 00:07:23.990
Um,
Sam Harris had a podcast on it with,
um,

125
00:07:24.620 --> 00:07:29.400
Renee,
how do I say her name?
Renee de Resta.
And uh,

126
00:07:29.450 --> 00:07:33.620
then she came on our podcast and talked about it as well.
And they were,

127
00:07:33.980 --> 00:07:37.370
they were pitting these people against each other.
Like they would have a,

128
00:07:37.371 --> 00:07:42.371
a pro Texas secession rally and directly across the street from a pro Muslim

129
00:07:44.001 --> 00:07:44.391
rally.

130
00:07:44.391 --> 00:07:47.980
And they would do it on purpose and they would have these people meet there and

131
00:07:48.020 --> 00:07:49.970
get angry at each other and they would,

132
00:07:50.060 --> 00:07:53.720
they would pretend to be a black lives matter page.

133
00:07:53.870 --> 00:07:58.870
They would pretend to be a white southern pride page and they were just trying

134
00:07:59.541 --> 00:08:03.680
to make people angry at people.
Now that's human driven manipulation.
Yeah.

135
00:08:03.980 --> 00:08:08.930
Now imagine this is my biggest way of AI is what Jack is working on is that

136
00:08:09.170 --> 00:08:14.150
algorithm driven manipulation of people unintentional trying to do good,

137
00:08:14.630 --> 00:08:18.610
but like those people,
uh,
Jack needs to do some Jujitsu.
I used to be,

138
00:08:19.390 --> 00:08:22.610
it needs to be some open minded,
uh,
you know,
uh,

139
00:08:22.910 --> 00:08:27.910
like really understand society transparency to where they can talk to us is uh,

140
00:08:28.460 --> 00:08:32.360
uh,
to the people in general how they're thinking about,
uh,
uh,

141
00:08:32.750 --> 00:08:36.290
managing these conversations.
Because you talk about these groups,

142
00:08:36.500 --> 00:08:41.120
very small number of Russians are able to control very large amounts of other

143
00:08:41.300 --> 00:08:45.810
people's opinions and arguments.
Yeah.
An algorithm can do that.
10 X,

144
00:08:46.100 --> 00:08:50.100
Oh yeah.
More and more of us will go on Twitter and Facebook and,
sure.
Yeah,

145
00:08:50.540 --> 00:08:53.090
for sure.
I think it's coming.
I think,
um,

146
00:08:53.120 --> 00:08:56.130
once people figured out how to manipulate that effectively and really

147
00:08:56.130 --> 00:09:01.130
<v 0>create like an army of fake bots that will assume stances on a variety of</v>

148
00:09:03.001 --> 00:09:07.860
different issues and just argue into infinity,
we were not going to know.

149
00:09:07.861 --> 00:09:09.990
We're not going to know who's real and who's not.
Well,

150
00:09:09.991 --> 00:09:13.650
it'll change the nature of our communication online.
I think it might,

151
00:09:13.860 --> 00:09:16.050
it might have affects this.
This is the problem,
the future.

152
00:09:16.051 --> 00:09:17.220
It's hard to predict the future.

153
00:09:17.221 --> 00:09:22.040
It might have affects where we'll stop taking anything online.

154
00:09:22.050 --> 00:09:23.220
Seriously.
Yeah.

155
00:09:23.370 --> 00:09:28.370
And we might get retract back to communicating in person more.

156
00:09:28.951 --> 00:09:31.230
I mean,
there,
there could be effects that we're not anticipating totally.

157
00:09:31.231 --> 00:09:32.130
And there might be some,

158
00:09:32.480 --> 00:09:36.920
at some ways in virtual reality we can authenticate our identity butter.

159
00:09:37.620 --> 00:09:42.300
So it will change the nature of communication.
I think the more,

160
00:09:42.301 --> 00:09:45.810
the more you can generate fake text,
uh,

161
00:09:46.140 --> 00:09:50.070
than the more the will distrust the information online.

162
00:09:50.220 --> 00:09:53.730
And the way that changes society is totally an open question.
We don't know.

163
00:09:54.360 --> 00:09:58.620
But your,
um,
what are your thoughts about the open AI?

164
00:09:58.621 --> 00:10:03.030
Do you think they should release or hold back on it?
Because this is,

165
00:10:03.031 --> 00:10:06.870
we're talking about Ai.
So artificial life,
there's stuff you're concerned about.

166
00:10:06.960 --> 00:10:08.310
Some company will create it.

167
00:10:08.760 --> 00:10:11.850
The question is what does the responsibility of that,
uh,

168
00:10:12.240 --> 00:10:13.590
short video where it looks like when it,

169
00:10:13.600 --> 00:10:17.670
this type of small paragraph in here hit a button.
It says how open AI rights,

170
00:10:17.671 --> 00:10:22.350
what was it today?
What did say Jimmy and signature stories.
Okay,

171
00:10:22.980 --> 00:10:27.980
so you give it a desserty across the UK economy at least 80 billion since,

172
00:10:28.620 --> 00:10:32.130
and then many industry actually believe so much time.
So they just,

173
00:10:32.190 --> 00:10:36.750
it just fills in those things.
Yeah.
So basically you give it,
you start the text,

174
00:10:36.930 --> 00:10:41.670
oh well can say,
uh,
Joe Rogan experience is the greatest podcast ever.

175
00:10:41.671 --> 00:10:46.671
And then let it finish the rest and it'll start explaining stuff about why it's

176
00:10:46.861 --> 00:10:49.950
the greatest podcast.
Is it accurate?
Oh look at this,

177
00:10:49.951 --> 00:10:53.820
it says a move that threatens to push many of our most talented young brains out

178
00:10:53.821 --> 00:10:57.690
of the country,
not to campuses in the developing world.

179
00:10:57.750 --> 00:11:01.530
This is a particularly costly blow research by Oxford University warns that the

180
00:11:01.531 --> 00:11:06.090
UK would have to spend nearly 1 trillion on post-Brexit infrastructure.

181
00:11:06.091 --> 00:11:10.950
That's crazy that that's all done by an AI that's like spelling this out in this

182
00:11:10.951 --> 00:11:15.240
very convincing argument.
The thing is the,
the way it actually works,

183
00:11:15.570 --> 00:11:19.500
algorithmic is fascinating cause this January is generating it one character at

184
00:11:19.501 --> 00:11:22.650
a time.
It has,
as far as,
you know,

185
00:11:23.100 --> 00:11:26.460
you don't want to discriminate against the AI,
but as far as we understand,

186
00:11:26.700 --> 00:11:30.600
it doesn't have any understanding of what it is to have,
what it's doing.

187
00:11:30.640 --> 00:11:34.050
If any ideas it's expressing,
it's simply stealing idea.

188
00:11:34.051 --> 00:11:37.680
It's like the largest scale plagiarizer of all time.
Right.

189
00:11:37.830 --> 00:11:41.430
It's basically just pulling out ideas from elsewhere in an automated way.

190
00:11:41.760 --> 00:11:42.900
And the question is,

191
00:11:43.320 --> 00:11:46.890
you could argue us humans are exactly that were just really good plagiarize

192
00:11:46.891 --> 00:11:51.750
years of what our parents taught us of what our previous so on.
Uh,
yeah,

193
00:11:51.751 --> 00:11:53.980
we are for sure.
Yeah.
So

194
00:11:53.980 --> 00:11:57.370
<v 1>the,
the question is whether you hold that back,
they,
their decision was to say,</v>

195
00:11:57.970 --> 00:12:01.000
uh,
let's hold it.
Let's not release it.

196
00:12:01.150 --> 00:12:05.020
That scares me to not release it.
Yeah.
Yeah.
You know why it scares me.

197
00:12:05.021 --> 00:12:09.940
It scares me that they would think that that's like this mindset that they,

198
00:12:10.030 --> 00:12:12.730
they sensed the inevitable,
the inevitable,

199
00:12:12.760 --> 00:12:15.610
meaning that someone's gonna come along with a version of this,

200
00:12:15.611 --> 00:12:19.810
it's going to be used for evil,
but it bothers him that much.
That seems so,

201
00:12:19.840 --> 00:12:24.840
it seems almost irresponsible for the technology to prevail for the technology

202
00:12:25.211 --> 00:12:25.540
to,

203
00:12:25.540 --> 00:12:30.540
to continue to be more and more powerful.

204
00:12:31.150 --> 00:12:34.390
Yeah.
They scared of it.
They're scared of it getting out,
right.
Yeah.

205
00:12:34.450 --> 00:12:37.510
That's scares the shit out of me.
Like if they're scared of it,

206
00:12:37.511 --> 00:12:41.230
they're the people that make it and there they are called open Ai.
I mean,

207
00:12:41.231 --> 00:12:44.410
this is the idea behind the group where everybody kind of agrees that you're

208
00:12:44.411 --> 00:12:47.320
going to use the brightest minds and have this open source.

209
00:12:47.320 --> 00:12:51.460
Everybody can understand it and everybody work at it and you don't miss out on

210
00:12:51.461 --> 00:12:55.360
any genius contributions.
And they're like,
no,
no,
no,
no,
no more.

211
00:12:56.170 --> 00:12:59.290
And there obviously their system currently is not that dangerous.

212
00:12:59.320 --> 00:13:02.620
They're not dangerous.
Well not,
yes,

213
00:13:02.621 --> 00:13:06.070
not that dangerous that if you just saw that,
that it can do that.

214
00:13:06.430 --> 00:13:09.160
But if you think through like what that would actually create,
I mean,

215
00:13:09.220 --> 00:13:11.240
it's possible.
It will be dangerous,
but it's not the,

216
00:13:11.430 --> 00:13:14.740
the point is they're doing it.
They try to do it early.
Right.

217
00:13:15.040 --> 00:13:19.090
To raise the question,
what do we do here?
Because yeah,
what do we do?

218
00:13:19.360 --> 00:13:23.170
Because they're directly going to be able to improve this.
Now.
Like if,
if there,

219
00:13:23.200 --> 00:13:28.200
if we can generate basically a 10 times more content of your face saying a bunch

220
00:13:29.681 --> 00:13:33.700
of stuff,
uh,
what does that,
what do we do with that?
If,

221
00:13:33.730 --> 00:13:38.730
if a Jamie all of a sudden on the side develops a much better generator and has

222
00:13:39.280 --> 00:13:40.210
your face,

223
00:13:40.360 --> 00:13:45.360
does an offshoot podcast essentially fake Joe Rogan experience and what do we do

224
00:13:46.361 --> 00:13:49.450
to,
does he release that?
You know,
does he,

225
00:13:49.510 --> 00:13:52.900
because now we can have,
uh,

226
00:13:53.300 --> 00:13:58.300
basically generated content and a much larger scale that will just be completely

227
00:13:58.571 --> 00:13:59.081
fake.
Well,

228
00:13:59.081 --> 00:14:01.910
I think what they're worried about is not just generating content that's faked

229
00:14:01.920 --> 00:14:05.530
or they're worried about manipulation of opinion.
Right?
Right.

230
00:14:05.531 --> 00:14:07.890
If they have all these people that are like that,

231
00:14:08.070 --> 00:14:13.070
that little sentence that led to that enormous paragraph in that video was just

232
00:14:13.121 --> 00:14:15.880
a sentence that showed a certain amount of outrage and then it led him,

233
00:14:15.881 --> 00:14:18.490
Phil let the AI fill in the blanks.
Yes,

234
00:14:19.030 --> 00:14:21.520
you could do that with fucking anything.

235
00:14:21.880 --> 00:14:24.010
Like you could just set those things loose.

236
00:14:24.011 --> 00:14:29.011
If they're that good and that convincing and they're that logical man,

237
00:14:29.920 --> 00:14:33.360
this is,
this is not real.
I'll just tell you,

238
00:14:33.370 --> 00:14:38.160
Ben Shapiro all creates Ai,
creates fake Ben Shapiro.

239
00:14:38.180 --> 00:14:39.130
Get the sound.
Sorry.

240
00:14:39.290 --> 00:14:43.780
<v 2>She has boards.
How Other?
This is a fake Ben Shapiro with this technology,</v>

241
00:14:43.960 --> 00:14:47.620
they can make me say anything such as,
for example,
I love socialism.

242
00:14:47.890 --> 00:14:52.070
Healthcare is a right,
not just a privilege.
It ending will solve crime.

243
00:14:52.110 --> 00:14:56.270
<v 1>Facts.
Care about your feelings.
I support Bernie Sanders.
Okay.
Yeah.
Yeah.</v>

244
00:14:56.330 --> 00:15:00.350
That's crazy.
It's crude,
but it's crude,
but it's on the way.
Yeah,
it's on the way.

245
00:15:00.351 --> 00:15:03.220
It's all in the way and we have to.
This is the time to talk about it.

246
00:15:03.221 --> 00:15:04.360
This is the time to think about it.

247
00:15:04.440 --> 00:15:07.730
One of the funny things about Kyle Donovan's Instagram was that it's obviously

248
00:15:07.731 --> 00:15:11.150
fake.
That's one of the funny things about it.
It's like South Park's animation.

249
00:15:11.340 --> 00:15:12.860
It's like the animation sucks.

250
00:15:13.160 --> 00:15:16.940
That's half the reason why it's so funny cause they're just like the circles.

251
00:15:17.150 --> 00:15:19.430
You know,
these weird looking creature things.

252
00:15:19.490 --> 00:15:22.940
Then we went and won the Canadians when their heads pop off at the top.

253
00:15:25.210 --> 00:15:26.690
And,
and,
uh,
my,

254
00:15:26.691 --> 00:15:31.250
my hope is this kind of technology will ultimately just be used from memes as

255
00:15:31.251 --> 00:15:34.700
opposed to something that's going to get wars.
Putin is going to be,

256
00:15:35.090 --> 00:15:40.090
he's going to be a bang and Mother Teresa on the White House desk in a video.

257
00:15:40.410 --> 00:15:43.700
We're going to be outraged.
We're going really go to war over this shit.

