WEBVTT

1
00:00:00.600 --> 00:00:01.141
In this video,

2
00:00:01.141 --> 00:00:06.060
we'll discuss two other operations that are useful to have in a a convolutional

3
00:00:06.061 --> 00:00:09.660
neural network,
full solving,
harder object recognition problems.

4
00:00:11.640 --> 00:00:15.960
So the architecture that have been described so far where we have convolutional

5
00:00:15.961 --> 00:00:20.961
layers followed by a pooling layers and supplanting pulling sub sampling layers

6
00:00:21.241 --> 00:00:24.560
that alternate before we reach some output layer,
uh,

7
00:00:24.630 --> 00:00:29.460
gives a neural network that actually works really well for somewhat simple

8
00:00:29.461 --> 00:00:33.720
problem like Henry [inaudible] character recognition.
Uh,
in fact,

9
00:00:34.030 --> 00:00:38.900
such architectures I've been used for reading,
uh,
checks in the u s uh,

10
00:00:38.910 --> 00:00:41.130
with a lot of success.
Uh,

11
00:00:41.160 --> 00:00:45.090
but it turns out that when we apply it on a harder problem like object

12
00:00:45.091 --> 00:00:48.840
recognition of the type that we find in the Caltech one on one data set,

13
00:00:48.841 --> 00:00:50.100
for instance,
uh,

14
00:00:50.170 --> 00:00:54.540
we have a lot of background variations and a lot of variations with respect to

15
00:00:54.541 --> 00:00:59.340
the appearance of the objects.
Uh,
and we have colored images and so on.

16
00:01:00.900 --> 00:01:03.960
It turns out that these types of networks actually perform,
uh,

17
00:01:03.990 --> 00:01:07.230
not so well with this particular architecture.
Uh,

18
00:01:07.260 --> 00:01:11.990
compared to other more standard approaches in computer vision.
Uh,

19
00:01:12.120 --> 00:01:17.070
but to get better performance,
we actually need to introduce to other operations,

20
00:01:17.250 --> 00:01:18.600
which we'll describe in this video.

21
00:01:21.630 --> 00:01:26.160
The first operation is very simple.
It's called the rectification layer.

22
00:01:26.820 --> 00:01:31.680
Uh,
and it consists of taking the previous layer and just computing it's absolute

23
00:01:31.681 --> 00:01:34.880
value and that becomes the new layer.
Uh,

24
00:01:35.280 --> 00:01:40.280
we will usually do this a rectification right after the convolutional layer

25
00:01:41.341 --> 00:01:42.174
operation.

26
00:01:42.900 --> 00:01:47.790
And one advantage that it has is that it introduces variants in Varian,

27
00:01:47.791 --> 00:01:52.710
sorry to the sign of the unit or the previous layer.
And so,

28
00:01:52.770 --> 00:01:55.470
uh,
for instance,
if,
uh,

29
00:01:55.650 --> 00:02:00.650
we have a feature map that computes the activation of something like,

30
00:02:01.440 --> 00:02:06.150
uh,
you know,
uh,
uh,
uh,
filter of connections that looks something like this.

31
00:02:06.151 --> 00:02:11.151
So that's looking for a high intensity here and low intensity here.

32
00:02:11.610 --> 00:02:12.210
Um,

33
00:02:12.210 --> 00:02:17.010
well if we have an edge that if we applied this filter around the neck,

34
00:02:17.040 --> 00:02:20.040
which has high activation here and low activation here,

35
00:02:20.130 --> 00:02:24.180
it's actually going to have a large but negative sign when we performed the

36
00:02:24.181 --> 00:02:28.350
convolution and compute the pre activation.
So by taking the absolute value,

37
00:02:28.440 --> 00:02:33.440
it means that we will now lose the information about whether the polarity of the

38
00:02:34.051 --> 00:02:38.310
edge was high intensity,
low intensity or low intensity.

39
00:02:38.311 --> 00:02:40.200
High intensity will confuse the two.

40
00:02:40.890 --> 00:02:45.890
And if we think that detecting certain objects is actually in Varian to the

41
00:02:45.961 --> 00:02:50.280
polarity of the intensity of the pixels across edges,

42
00:02:50.460 --> 00:02:55.460
then this is a kind of operation that should be useful and for object where the

43
00:02:55.501 --> 00:02:58.500
actual color of the object might not be that informative.
For instance,

44
00:02:58.501 --> 00:03:01.540
we can imagine something like this.
This is actually useful.

45
00:03:04.640 --> 00:03:05.473
<v 1>Yeah.</v>

46
00:03:05.800 --> 00:03:10.210
<v 0>And is,
and the second operation we'll introduce is call a local contrast</v>

47
00:03:10.211 --> 00:03:13.570
normalizations.
So we have a layer that just computes that,
uh,

48
00:03:13.600 --> 00:03:17.560
the local contrast normalization.
I give the full equations here,

49
00:03:17.561 --> 00:03:20.990
but I think I'll just describe intuitively means,
cause,
uh,

50
00:03:21.070 --> 00:03:24.610
just the description should make it clear what,
uh,
it's performing.

51
00:03:25.660 --> 00:03:30.250
So we just take the previous input,
uh,
the previous layer,

52
00:03:31.510 --> 00:03:33.640
and then we subtract some local average,

53
00:03:33.641 --> 00:03:38.110
which is computed locally in some neighborhood and a,

54
00:03:38.111 --> 00:03:42.830
and we can also compute the average across the channels of the layer.

55
00:03:42.831 --> 00:03:45.130
So this is illustrated by this sort of,

56
00:03:45.680 --> 00:03:48.310
<v 2>uh,
uh,
little,
uh,
uh,</v>

57
00:03:48.410 --> 00:03:52.130
<v 0>three Vq here,
uh,
that goes across the filter maps.</v>

58
00:03:52.520 --> 00:03:56.860
So we might have a,
an average debt is computer across,

59
00:03:56.980 --> 00:03:58.670
<v 2>uh,
uh,</v>

60
00:03:58.840 --> 00:04:03.290
<v 0>feature maps or I guess,
uh,
input channels.
If we're considering this to be there,</v>

61
00:04:03.291 --> 00:04:04.180
the previous layer,

62
00:04:04.420 --> 00:04:08.950
we training it as a input channels and then we just take the local average and

63
00:04:08.951 --> 00:04:12.010
subtract.
So that's what I'm calling v I j.
K.

64
00:04:12.940 --> 00:04:16.900
And then next we'll actually divided by a local standard deviation.

65
00:04:17.230 --> 00:04:19.640
So we are taking whatever value that we sent.
Uh,

66
00:04:19.660 --> 00:04:24.660
we centered using this local average and then divided by e d a local standard

67
00:04:25.901 --> 00:04:30.730
deviation,
which,
so we just take our centered values.
We take the squared,

68
00:04:31.050 --> 00:04:35.980
uh,
use the same weighting that we use for the local averaging here.
A,

69
00:04:35.981 --> 00:04:37.860
we take the sum of the squared and then,
uh,

70
00:04:37.870 --> 00:04:42.370
applied the square root on top of the,
of that song.
Uh,
so this,

71
00:04:42.430 --> 00:04:46.810
this is a standard deviation because we were taking the sum average of the

72
00:04:46.811 --> 00:04:49.600
square difference with the,
uh,
with the average.

73
00:04:49.660 --> 00:04:53.950
And it's local because it's only computed at some neighborhood index by p and Q

74
00:04:53.951 --> 00:04:57.160
here where we see that,
yeah,
applied the offset here.

75
00:04:57.940 --> 00:04:58.990
So we divide by that.

76
00:04:58.991 --> 00:05:03.760
The Max with C is just to make sure that if we have a standard deviation of

77
00:05:03.790 --> 00:05:07.510
zero,
we don't actually divide by zero and,
uh,
get uh,

78
00:05:08.020 --> 00:05:10.060
uh,
numerical problems.

79
00:05:11.260 --> 00:05:11.450
<v 1>Okay.</v>

80
00:05:11.450 --> 00:05:15.520
<v 0>So,
um,
what this is doing is that,
uh,</v>

81
00:05:15.620 --> 00:05:20.000
it reduces the units activation if its neighbors in the local neighborhood,

82
00:05:20.480 --> 00:05:24.620
uh,
are also very active.
And so it sort of encouraging competition,

83
00:05:25.220 --> 00:05:26.220
<v 2>uh,</v>

84
00:05:26.390 --> 00:05:28.610
<v 0>within a neighborhood of a,</v>

85
00:05:28.700 --> 00:05:33.700
of a single feature map and or even the cross a feature map if we're allowing to

86
00:05:34.431 --> 00:05:36.410
take all local average across feature maps,

87
00:05:36.680 --> 00:05:40.640
then we actually also encouraging competition across feature maps so that these

88
00:05:40.730 --> 00:05:45.200
different features perhaps tried to,
uh,
learn instead things that are,

89
00:05:45.550 --> 00:05:48.500
uh,
that are different.
And,
uh,

90
00:05:48.530 --> 00:05:51.650
one thing you can also do is that it can introduce some,

91
00:05:51.690 --> 00:05:56.690
a normalization which respect to local changes in the illumination because with

92
00:05:57.441 --> 00:06:01.940
essentially just change the scaling of the intensity of the pixels.

93
00:06:01.941 --> 00:06:05.350
And so by dividing by this,
uh,
um,

94
00:06:05.580 --> 00:06:06.950
at the standard deviation,

95
00:06:07.160 --> 00:06:11.330
we essentially get to cancel out the impact of the elimination,

96
00:06:11.780 --> 00:06:16.760
uh,
in the computation of the original feature map.
Okay.

97
00:06:16.761 --> 00:06:20.630
So that's another operation that's very useful for convolutional neural
networks.

98
00:06:22.760 --> 00:06:25.160
So these two operations here,
uh,

99
00:06:25.190 --> 00:06:29.210
are usually introduced between the convolution and the pooling.

100
00:06:29.211 --> 00:06:32.780
So we introduced them right here and uh,

101
00:06:32.840 --> 00:06:37.840
to finish up other types of preprocessing that's useful directly on the images

102
00:06:38.330 --> 00:06:41.060
is to,
well first convert them to gray scale.

103
00:06:41.061 --> 00:06:45.890
If we think that color information is not useful for detecting the objects were

104
00:06:45.891 --> 00:06:48.260
interested in detecting,
um,

105
00:06:48.440 --> 00:06:52.820
often all the images are resized to the same economical size.

106
00:06:53.060 --> 00:06:55.970
So 150 by 150 something that's often used.

107
00:06:56.180 --> 00:07:00.260
And then we use zero panning for images that are not squared so that they cannot

108
00:07:00.590 --> 00:07:04.430
be.
So when we're resizing one or respect the aspect ratio of the original image,

109
00:07:04.431 --> 00:07:08.510
so we apply some zero padding to get the same aspect ratio.

110
00:07:09.170 --> 00:07:12.950
Um,
often we remove the intra image means.

111
00:07:12.951 --> 00:07:14.870
So the mean of the pixels within,

112
00:07:14.890 --> 00:07:18.980
they give an image and divide by the standard deviation.
Also of that image.

113
00:07:19.190 --> 00:07:23.570
So that will also make us more invariant to eliminations,

114
00:07:24.470 --> 00:07:26.810
variations in the illumination in the images.

115
00:07:27.680 --> 00:07:32.510
And often we also applied local contrast normalization so that we get,
uh,

116
00:07:32.660 --> 00:07:33.540
also,
uh,

117
00:07:33.590 --> 00:07:38.210
some amount of invariance to local variation in the,

118
00:07:38.300 --> 00:07:39.133
um,

119
00:07:39.290 --> 00:07:43.520
in the elimination in the scene where the image was taken.

120
00:07:44.600 --> 00:07:49.530
So those are the other two types of layers that we often add as well as pre

121
00:07:49.531 --> 00:07:54.050
processing that are useful to get a good system.
Uh,
object recognition,

122
00:07:54.500 --> 00:07:58.190
object recognition system for using a convolutional neural network.

