1
00:00:01,040 --> 00:00:04,920
The Joe Rogan experience. When
you think about the future? Uh,

2
00:00:05,180 --> 00:00:08,660
at least me though one, let
me tell you my concern. Yeah.

3
00:00:08,750 --> 00:00:12,290
I'm worried that rich people are going
to get a hold of this technology quick

4
00:00:12,560 --> 00:00:17,540
and they're going to have massive
unfair advantages in terms of intellect,

5
00:00:17,541 --> 00:00:20,540
in terms of physical, athletic
ability, all, I mean we,

6
00:00:20,750 --> 00:00:25,750
we really can have a grossly imbalanced
world radically quickly if this happens

7
00:00:26,931 --> 00:00:31,750
fast, when we don't understand exactly
what the consequences of these actions are

8
00:00:31,751 --> 00:00:32,630
until it's too late.

9
00:00:32,631 --> 00:00:36,890
And then we'd try to play catch up with
rules and regulations and laws. Yeah,

10
00:00:36,980 --> 00:00:40,910
that's a very, very real danger. And
that's why I've written this book.

11
00:00:40,911 --> 00:00:43,190
That's why I'm out speaking
everyday about this,

12
00:00:43,191 --> 00:00:46,400
this topic because we need
to recognize that if we have,

13
00:00:46,401 --> 00:00:51,401
if we approach these revolutionary
technologies using the same values that we

14
00:00:51,441 --> 00:00:55,250
experience today where, you know,
we're here and very comfortable,

15
00:00:55,251 --> 00:00:57,050
but just down the road there
are people who are just,

16
00:00:57,051 --> 00:00:58,940
who are living without many opportunities.

17
00:00:59,180 --> 00:01:02,480
There are people in parts of the world
like Central African Republic where

18
00:01:02,481 --> 00:01:05,560
there's just a warzone kids
are born malnourished. Um,

19
00:01:05,750 --> 00:01:07,940
if those are our values today,

20
00:01:08,180 --> 00:01:12,650
we can expect that when these
future technologies arrive,

21
00:01:12,651 --> 00:01:15,950
we'll use those, those
same values. So it's real.

22
00:01:16,130 --> 00:01:18,260
And right now we have an
opportunity to say, all right,

23
00:01:18,620 --> 00:01:21,740
these technologies are coming whenever
we do these technologies that are coming,

24
00:01:22,040 --> 00:01:25,370
there's a better possible future
and a worst possible future.

25
00:01:25,550 --> 00:01:29,960
And how can we infuse our best values
into the process to optimize the good

26
00:01:29,961 --> 00:01:31,280
stuff and minimize the bad stuff.

27
00:01:31,281 --> 00:01:35,210
And certainly what you're saying is a
real risk thing of what happened when

28
00:01:35,240 --> 00:01:39,500
European countries had slightly better
weapons and slightly better ships than

29
00:01:39,501 --> 00:01:43,910
everybody else. They took over the world
and dominated everybody. And so, yeah,

30
00:01:43,911 --> 00:01:45,050
that's very real.

31
00:01:45,110 --> 00:01:49,550
That's the government's need to play
a role in ensuring broad access in

32
00:01:49,551 --> 00:01:50,810
regulating these,
uh,

33
00:01:50,811 --> 00:01:55,340
these technologies to make sure we don't
get to that kind of dystopian scenario

34
00:01:55,341 --> 00:01:57,070
that you've laid down. Well, it's also

35
00:01:57,200 --> 00:02:00,740
in terms of governments regulating things,
like why are they qualified?

36
00:02:01,160 --> 00:02:04,040
Who are they? Who are the
governments? So just people, right?

37
00:02:04,070 --> 00:02:07,300
It's people that are either elected
or the, uh, you know, or, yeah,

38
00:02:07,490 --> 00:02:09,590
some sort of a monarchy earn.
You're,

39
00:02:09,800 --> 00:02:14,390
you're dealing with either kings and
queens and sheikhs or you're dealing with

40
00:02:14,391 --> 00:02:15,051
presidents.

41
00:02:15,051 --> 00:02:18,680
And we've seen in this
country that sometimes our
presidents don't know what the

42
00:02:18,681 --> 00:02:20,000
fuck they're talking about.
Right?

43
00:02:20,210 --> 00:02:25,210
So who are they to disrupt science
to s to disrupt this natural flow of

44
00:02:25,251 --> 00:02:26,010
technology?

45
00:02:26,010 --> 00:02:29,010
Well,
we decide we need somebody to do it.

46
00:02:29,011 --> 00:02:32,160
We need some representation
of our collective will,

47
00:02:32,430 --> 00:02:34,200
but just to avoid some of the things,

48
00:02:34,201 --> 00:02:37,500
like you're just like you just mentioned
that that's the reason why humans

49
00:02:37,501 --> 00:02:41,580
banded together and made these kinds of,
the of made created governments.

50
00:02:41,581 --> 00:02:43,170
And the reason for democracy,

51
00:02:43,171 --> 00:02:45,900
especially if you have more
functioning democracies,

52
00:02:45,901 --> 00:02:50,880
is that your government in some ways
reflects the will of the people in the

53
00:02:50,881 --> 00:02:54,330
government does things
that individuals can't do.

54
00:02:54,331 --> 00:02:56,490
And I know there are a
leather libertarian arguments.

55
00:02:56,491 --> 00:02:58,950
Well everyone should just like, if you
want a little road in front of your house,

56
00:02:58,960 --> 00:03:02,680
either go build the road
and pay some somebody. But
there are a lot of things in,

57
00:03:02,681 --> 00:03:04,870
even in that model that won't get done.

58
00:03:04,900 --> 00:03:09,100
There are a lot of kind of big national
and even global concerns that you need

59
00:03:09,101 --> 00:03:10,630
some kind of,
uh,

60
00:03:10,640 --> 00:03:15,640
of regulation because what we're talking
about is the future of life and life on

61
00:03:16,061 --> 00:03:18,000
earth. And there have to
be some kind of guardrails.

62
00:03:18,100 --> 00:03:21,640
And that's why what I'm arguing
for is we really need a bottom up.

63
00:03:21,700 --> 00:03:23,260
And I think we every person,

64
00:03:23,261 --> 00:03:25,450
and that's why I'm so thrilled
to be here with you today, Joe.

65
00:03:26,050 --> 00:03:29,890
Every person needs to really understand
these revolutionary technologies like

66
00:03:29,891 --> 00:03:30,311
genetics,

67
00:03:30,311 --> 00:03:35,311
like AI and all of our responsibilities
and opportunities to say,

68
00:03:35,321 --> 00:03:39,600
hey, this is really important. Here
are the values that I think that, uh,

69
00:03:39,601 --> 00:03:41,770
that I cherish.
And just like you said,

70
00:03:41,771 --> 00:03:46,270
I don't want it to be that the wealthiest
people are the ones who have kids with

71
00:03:46,300 --> 00:03:49,720
higher iqs and live longer and healthier
than, than everybody else. And then,

72
00:03:50,110 --> 00:03:54,640
so we have to raise our voice and our
needs to be a bottom up process and a,

73
00:03:54,641 --> 00:03:57,220
in a top down process.
And it's re it's really hard.

