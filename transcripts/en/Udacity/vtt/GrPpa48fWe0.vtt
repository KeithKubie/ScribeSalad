WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:02.280
So first,
to be able to talk about generalization.

00:00:02.280 --> 00:00:05.640
It helps to think about
states as actually being

00:00:05.640 --> 00:00:08.940
collections of features instead
of just unanalyzed blobs.

00:00:08.940 --> 00:00:12.590
&gt;&gt; So we're no longer to refer to
states as state 17 and state 19?

00:00:12.590 --> 00:00:13.340
&gt;&gt; We can.

00:00:13.340 --> 00:00:16.450
But I think it's helpful actually to
give them additional information like

00:00:16.450 --> 00:00:16.970
these features.

00:00:16.970 --> 00:00:19.710
So here's a concrete example to
make this, try to make some sense.

00:00:19.710 --> 00:00:22.350
Do you remember our taxi problem
that we talked about before?

00:00:22.350 --> 00:00:23.180
&gt;&gt; How could I forget.

00:00:23.180 --> 00:00:27.500
&gt;&gt; In this particular environment,
there was something like, I don't know,

00:00:27.500 --> 00:00:32.119
500 states and in standard q learning
type approaches, we would just think of

00:00:32.119 --> 00:00:34.720
all those different states as being
completely different from one another.

00:00:34.720 --> 00:00:40.250
But we can also imagine tagging each of
the states with some particular value.

00:00:40.250 --> 00:00:41.880
Like whether or not well,

00:00:41.880 --> 00:00:45.140
we don't really have fuel when we talked
about it, but whether we're low on fuel.

00:00:45.140 --> 00:00:49.510
What is the X coordinate of
the taxi cab, this thing here.

00:00:49.510 --> 00:00:51.270
What's the Y coordinate the taxi cab?

00:00:52.390 --> 00:00:54.120
Is the taxi cab currently near a wall?

00:00:54.120 --> 00:00:57.630
Say to the you know to some direction,
north, south, east, west.

00:00:57.630 --> 00:01:01.280
Each of these different features
give us a little bit of a handle on

00:01:01.280 --> 00:01:03.180
the states and
how they relate to each other right.

00:01:03.180 --> 00:01:07.950
So if we have this state that we've got
right here and we say move the passenger

00:01:07.950 --> 00:01:11.920
to some other position,
we're still in a related state.

00:01:11.920 --> 00:01:14.610
The taxi hasn't moved and it would be
nice to be able to capture that and so

00:01:14.610 --> 00:01:17.780
this notion of features
gives us that idea.

00:01:17.780 --> 00:01:18.530
Does that makes sense?

00:01:18.530 --> 00:01:21.470
&gt;&gt; So that's a lot like how we
talked about machine learning and

00:01:21.470 --> 00:01:23.820
supervised learning in our
other machine learning class.

00:01:23.820 --> 00:01:24.470
&gt;&gt; Exactly.

00:01:24.470 --> 00:01:26.510
Yeah, in fact we had words for
these things.

00:01:26.510 --> 00:01:30.370
We talked about the,
boy what were those words?

00:01:30.370 --> 00:01:33.570
It's sort of the notion that
different machine learning

00:01:33.570 --> 00:01:36.800
representations cause different
things to be similar to each other.

00:01:36.800 --> 00:01:37.850
&gt;&gt; Right.

00:01:37.850 --> 00:01:40.620
And there are lots of different ways
that we could represent the state space

00:01:40.620 --> 00:01:43.540
other than whatever
the different 500 states are.

00:01:43.540 --> 00:01:47.280
You have some of them up here, but
we could have the coordinates of

00:01:47.280 --> 00:01:50.270
where the passenger is, not just
the coordinates of where the taxi is.

00:01:50.270 --> 00:01:53.790
We could have things represented and
sort of an egocentric way.

00:01:53.790 --> 00:01:58.850
What's the, I don't know, Manhattan
distance between a passenger and

00:01:58.850 --> 00:02:00.490
where the passenger wants to end up.

00:02:00.490 --> 00:02:04.560
There are all kinds of things that we
could the ways we could describe things

00:02:04.560 --> 00:02:07.410
and some of those things
make some states look

00:02:07.410 --> 00:02:11.645
more similar than other
representations would.

00:02:11.645 --> 00:02:12.935
&gt;&gt; I think I remember the term.

00:02:12.935 --> 00:02:15.295
I think it was inductive bias.

00:02:15.295 --> 00:02:18.625
&gt;&gt; Inductive bias was something that
referred to the way algorithms preferred

00:02:18.625 --> 00:02:20.245
one solution over another.

00:02:20.245 --> 00:02:21.465
&gt;&gt; But I think you're right, though,

00:02:21.465 --> 00:02:25.915
because different features
sort of create different

00:02:25.915 --> 00:02:29.855
kind of inductive biases by making
some states look more similar.

00:02:29.855 --> 00:02:34.180
So, for example,
if we had this egocentric notion of how

00:02:34.180 --> 00:02:38.960
far away a person is from a taxi,
then so

00:02:38.960 --> 00:02:44.790
long as a person is say two steps
away from a taxi, any state where

00:02:44.790 --> 00:02:49.670
a person is two steps away from a taxi
looks the same or looks very similar.

00:02:49.670 --> 00:02:52.490
In fact, if you look at this one you
have now if you just drew that right

00:02:52.490 --> 00:02:56.340
the red circle is close
to the orange square.

00:02:56.340 --> 00:02:58.140
If you were to move
both the red circle and

00:02:58.140 --> 00:03:02.580
the orange square up, those two states
would still be very similar because

00:03:02.580 --> 00:03:05.820
they're still two away from one
another or have two in between them.

00:03:05.820 --> 00:03:11.060
But even though I don't move the red
circle, if I take the orange square and

00:03:11.060 --> 00:03:16.600
move it to the right, then that's less
similar because they're farther apart.

00:03:18.660 --> 00:03:21.730
&gt;&gt; Okay, and they were ignoring
walls the way he described it right?

00:03:21.730 --> 00:03:22.730
&gt;&gt; Right, I'm ignoring walls.

00:03:22.730 --> 00:03:25.250
&gt;&gt; We move both of these up by two
we're so yeah we're two apart but

00:03:25.250 --> 00:03:27.720
it does feel kind of different
to go around the walls.

00:03:27.720 --> 00:03:28.560
&gt;&gt; And if it's different or

00:03:28.560 --> 00:03:30.890
not different depends upon
the state features that you use.

00:03:30.890 --> 00:03:33.290
&gt;&gt; Okay do we have a name for that or.

00:03:33.290 --> 00:03:35.390
Is inductive bias an okay stand in?

00:03:35.390 --> 00:03:37.010
&gt;&gt; It's a representation, right?

00:03:37.010 --> 00:03:38.020
It's a representation.

00:03:38.020 --> 00:03:42.560
Some representations are better for
some things than others and

00:03:42.560 --> 00:03:45.190
you can learn differently depending
upon which features you choose.

00:03:45.190 --> 00:03:46.800
&gt;&gt; Okay.
&gt;&gt; For example, here's a great feature.

00:03:46.800 --> 00:03:48.000
For every state,

00:03:48.000 --> 00:03:50.450
here's the set of actions you
should take to accomplish the goal.

00:03:50.450 --> 00:03:53.720
That's one representation that
makes learning really easy.

00:03:53.720 --> 00:03:54.450
&gt;&gt; Yes, good point.

00:03:54.450 --> 00:03:56.130
So let's talk about what we're
actually going to learn.

00:03:56.130 --> 00:03:56.630
&gt;&gt; Okay.

