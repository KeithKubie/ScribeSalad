WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.220
In the realm of
data compression,

00:00:02.220 --> 00:00:04.950
there's been many rulers over
many lands of information.

00:00:04.950 --> 00:00:06.960
But since the early
days of the realm,

00:00:06.960 --> 00:00:09.150
there's been one family
that's reigned over it

00:00:09.150 --> 00:00:10.500
with supreme authority.

00:00:10.500 --> 00:00:12.810
This ruling class is known
as the Lempel-Ziv family

00:00:12.810 --> 00:00:13.765
of compressors.

00:00:13.765 --> 00:00:16.780
And it's got bloodlines
that span generations

00:00:16.780 --> 00:00:18.910
with feuding
brothers, and sisters,

00:00:18.910 --> 00:00:22.410
and cousins, even twice-removed
aunts, a family tree that's

00:00:22.410 --> 00:00:25.310
so dynamic and interwoven that
it makes "Game of Thrones"

00:00:25.310 --> 00:00:27.270
look like a gathering
at a local pizza parlor.

00:00:27.270 --> 00:00:29.210
Not, this family
is more than that.

00:00:29.210 --> 00:00:31.760
It's cunning, and dynamic,
and efficient, traits

00:00:31.760 --> 00:00:34.270
that have allowed it to reign
over the realm of compression

00:00:34.270 --> 00:00:37.130
for more than 30
years undisputed.

00:00:37.130 --> 00:00:40.335
But to truly understand what
gives this family it's power,

00:00:40.335 --> 00:00:42.710
we're going to have to go back
and change the way that we

00:00:42.710 --> 00:00:45.660
approach our data and challenge
our fundamental understanding

00:00:45.660 --> 00:00:46.500
of entropy.

00:00:46.500 --> 00:00:48.250
Now, this is going to
be a difficult task.

00:00:48.250 --> 00:00:50.140
But fear not, young programmer.

00:00:50.140 --> 00:00:51.060
I'm here to help.

00:00:51.060 --> 00:00:52.170
My name is Colt McAnlis.

00:00:52.170 --> 00:00:54.880
And this is Compressor Head.

00:00:54.880 --> 00:00:56.820
[MUSIC PLAYING]

00:01:01.190 --> 00:01:04.660
What Claude Shannon developed
with the concept of entropy

00:01:04.660 --> 00:01:07.380
was a measurement of
information content in a stream.

00:01:07.380 --> 00:01:09.340
Or put another way,
entropy defines

00:01:09.340 --> 00:01:12.740
the minimum number of bits
needed per symbol on average

00:01:12.740 --> 00:01:14.780
to represent your data stream.

00:01:14.780 --> 00:01:17.090
So for example, you take
the entropy of a stream

00:01:17.090 --> 00:01:18.930
and multiply it by the
length of the stream,

00:01:18.930 --> 00:01:20.660
and you get the smallest
size that stream

00:01:20.660 --> 00:01:23.160
can be compressed according
to information theory.

00:01:23.160 --> 00:01:25.990
But there's a reason it's called
information theory, not a law.

00:01:25.990 --> 00:01:27.540
We can break the rules.

00:01:27.540 --> 00:01:31.340
So for example, if we took this
sorted, linearly increasing

00:01:31.340 --> 00:01:33.440
numbers as our data
stream, the entropy

00:01:33.440 --> 00:01:37.010
is about four bits per symbol.

00:01:37.010 --> 00:01:38.870
However, if we apply
delta encoding, which

00:01:38.870 --> 00:01:41.650
is the process of encoding the
next symbol as the difference

00:01:41.650 --> 00:01:43.660
with the previous
symbol, we end up

00:01:43.660 --> 00:01:47.670
with a data stream full of ones,
whose entropy is effectively

00:01:47.670 --> 00:01:48.770
zero.

00:01:48.770 --> 00:01:50.920
Now, what we did here
was transform the stream

00:01:50.920 --> 00:01:53.020
into something with a
different symbol frequency,

00:01:53.020 --> 00:01:55.860
dominantly ones, which
changed the entropy, making

00:01:55.860 --> 00:01:57.040
it more compressible.

00:01:57.040 --> 00:01:59.300
This idea to transform
data into state

00:01:59.300 --> 00:02:01.530
which can be better
compressed lies the heart

00:02:01.530 --> 00:02:03.330
of all modern
compression theory.

00:02:03.330 --> 00:02:06.490
But it's not as
straightforward as it seems.

00:02:06.490 --> 00:02:09.080
So for example, take this
stream of ones and twos.

00:02:09.080 --> 00:02:11.009
Applying a delta
transform here doesn't

00:02:11.009 --> 00:02:13.120
change the frequency
of the symbols.

00:02:13.120 --> 00:02:15.770
The entropy remains
about one bit

00:02:15.770 --> 00:02:18.490
per symbol, give or
take a quarter of a bit.

00:02:18.490 --> 00:02:20.870
As such, we know that
not every data transform

00:02:20.870 --> 00:02:22.947
is suited for
every type of data.

00:02:22.947 --> 00:02:25.280
You need to know something
about the data in your stream

00:02:25.280 --> 00:02:28.090
to properly understand
what valid transforms work

00:02:28.090 --> 00:02:30.160
on it to produce lower
entropy, which, trust me,

00:02:30.160 --> 00:02:32.085
is a lot trickier
than it sounds.

00:02:32.085 --> 00:02:34.010
Now, if we focus
the eye of Sauron

00:02:34.010 --> 00:02:36.287
on compressing
textual data, we find

00:02:36.287 --> 00:02:37.870
that one of the more
popular transform

00:02:37.870 --> 00:02:39.830
is known as symbol grouping.

00:02:39.830 --> 00:02:41.900
You see, up until
now, we've really only

00:02:41.900 --> 00:02:43.940
been looking at single
symbols within a stream.

00:02:43.940 --> 00:02:45.650
But in reality,
there's often quite

00:02:45.650 --> 00:02:48.320
a lot of correlation
between adjacent symbols.

00:02:48.320 --> 00:02:50.190
For example, you
typically see the letter

00:02:50.190 --> 00:02:53.360
Q followed by the letter
U. Grouping symbols means

00:02:53.360 --> 00:02:55.670
looking at the stream in
terms of pairs or more

00:02:55.670 --> 00:03:00.660
of adjacent symbols instead
of just the single ones.

00:03:00.660 --> 00:03:03.250
Guys, my clicker's broken.

00:03:03.250 --> 00:03:07.310
Can-- thanks.

00:03:07.310 --> 00:03:09.070
So for that example
data set, you

00:03:09.070 --> 00:03:11.570
can see that by grouping
the symbols together,

00:03:11.570 --> 00:03:14.170
a new pattern emerges,
which, of course, as we know,

00:03:14.170 --> 00:03:16.220
changes entropy.

00:03:16.220 --> 00:03:17.560
Thank you, please.

00:03:17.560 --> 00:03:20.200
But sadly, we don't get a chance
to look at compressed data

00:03:20.200 --> 00:03:20.730
manually.

00:03:20.730 --> 00:03:23.050
So writing a grouping
algorithm gets tough

00:03:23.050 --> 00:03:24.730
when you get complex strings.

00:03:24.730 --> 00:03:26.160
But just for
giggles, we're going

00:03:26.160 --> 00:03:28.000
to take a swing at
this one anyway.

00:03:28.000 --> 00:03:30.610
Are you taking a selfie?

00:03:30.610 --> 00:03:33.480
Just turn the thing.

00:03:33.480 --> 00:03:36.360
So if we simply define
each letter as a symbol

00:03:36.360 --> 00:03:38.950
as we have been doing,
we end up with an entropy

00:03:38.950 --> 00:03:42.650
that's around 2.38,
which isn't that

00:03:42.650 --> 00:03:44.680
bad for a string like this.

00:03:44.680 --> 00:03:49.505
But I think we can do
a little bit better.

00:03:49.505 --> 00:03:52.276
Can we speed this
up next time maybe?

00:03:52.276 --> 00:03:53.650
Actually, looking
at this stream,

00:03:53.650 --> 00:03:56.660
we realize that probably,
words, or known words,

00:03:56.660 --> 00:03:58.370
would be a little bit
easier to group by.

00:03:58.370 --> 00:04:01.440
So for to be or not to
be, for this stream,

00:04:01.440 --> 00:04:06.060
we can get a little bit better
entropy at 1.98, which is much,

00:04:06.060 --> 00:04:06.810
much better.

00:04:06.810 --> 00:04:09.990
But I think we
can still do more.

00:04:09.990 --> 00:04:11.410
Now we're getting it.

00:04:11.410 --> 00:04:14.590
In fact, a naive approach may
be to do something a little bit

00:04:14.590 --> 00:04:15.090
different.

00:04:15.090 --> 00:04:17.690
Perhaps we should actually
take the longest string

00:04:17.690 --> 00:04:18.779
that can be matched.

00:04:18.779 --> 00:04:21.519
So in this case, to
be or not is actually

00:04:21.519 --> 00:04:23.220
matched twice in this string.

00:04:23.220 --> 00:04:24.980
And it actually has
the longest length.

00:04:24.980 --> 00:04:26.980
The problem here is
that we actually end up

00:04:26.980 --> 00:04:29.640
with a whole group
of single symbols

00:04:29.640 --> 00:04:33.070
that really don't have any
duplicates and no clear skewed

00:04:33.070 --> 00:04:33.830
probability.

00:04:33.830 --> 00:04:38.090
And the entropy
reflects this at 2.5.

00:04:38.090 --> 00:04:40.975
You see, what we really need
is more of a dynamic grouping

00:04:40.975 --> 00:04:42.350
algorithm, one
that tries to find

00:04:42.350 --> 00:04:45.220
a balance between the longest
chains and the smallest

00:04:45.220 --> 00:04:45.800
entropy.

00:04:45.800 --> 00:04:49.560
And as you can imagine, that's
a pretty difficult problem.

00:04:49.560 --> 00:04:50.060
Thanks.

00:04:52.991 --> 00:04:53.490
Really?

00:04:56.767 --> 00:04:58.100
Let's just go to the next scene.

00:05:00.910 --> 00:05:02.660
And with that in mind,
it's time to reveal

00:05:02.660 --> 00:05:04.660
the founders of the LZ dynasty.

00:05:04.660 --> 00:05:07.376
You see, back in 1977, right
after an accident took him

00:05:07.376 --> 00:05:09.000
out of running for
the winter Olympics,

00:05:09.000 --> 00:05:12.290
Abraham Lempel and Jacob
Ziv invented an algorithm

00:05:12.290 --> 00:05:15.420
which identifies the optimal
way to tokenize a stream.

00:05:15.420 --> 00:05:19.560
The original two transforms,
named LZ77 and LZ78,

00:05:19.560 --> 00:05:22.170
are so good at finding
optimal tokenization

00:05:22.170 --> 00:05:25.340
that in 30 plus years, there
hasn't been another algorithm

00:05:25.340 --> 00:05:26.150
to replace them.

00:05:26.150 --> 00:05:28.390
Nope, the LZ family
still makes up

00:05:28.390 --> 00:05:32.390
the backbone of all
modern, useful compressors.

00:05:32.390 --> 00:05:34.810
The LZ algorithm works
by splitting the stream

00:05:34.810 --> 00:05:36.660
into two segments.

00:05:36.660 --> 00:05:39.779
The left side of the stream
is dubbed the search buffer

00:05:39.779 --> 00:05:41.570
and contains the symbols
that we've already

00:05:41.570 --> 00:05:43.130
seen and already processed.

00:05:43.130 --> 00:05:46.610
The right side of the stream is
denoted the look ahead buffer

00:05:46.610 --> 00:05:48.830
and contains symbols
we haven't seen yet.

00:05:48.830 --> 00:05:50.900
In practical matters,
the search buffer

00:05:50.900 --> 00:05:53.330
is some thousands
of kilosymbols long,

00:05:53.330 --> 00:05:56.500
while the look ahead buffer
is some 10s of symbols long.

00:05:56.500 --> 00:05:58.930
The encoder will read
a symbol from the look

00:05:58.930 --> 00:06:01.430
ahead buffer and attempt
to find a match for it

00:06:01.430 --> 00:06:04.320
in the search buffer.

00:06:04.320 --> 00:06:06.020
If the symbol is
found, then the encoder

00:06:06.020 --> 00:06:09.320
will go to read more symbols
from the look ahead buffer

00:06:09.320 --> 00:06:12.280
and continue searching
backwards in the search buffer

00:06:12.280 --> 00:06:15.120
until it finds
the longest match.

00:06:15.120 --> 00:06:17.340
When a match is
finally settled on,

00:06:17.340 --> 00:06:19.750
the encoder will output a token.

00:06:19.750 --> 00:06:25.250
The token has three parts,
an offset, a length,

00:06:25.250 --> 00:06:27.760
and the next token
in the stream.

00:06:27.760 --> 00:06:30.490
In this case, it would
be nine backwards

00:06:30.490 --> 00:06:33.846
from the window with a
length of two symbols

00:06:33.846 --> 00:06:35.220
where the next
symbol in the look

00:06:35.220 --> 00:06:39.520
ahead is the letter
B. Once this is done,

00:06:39.520 --> 00:06:41.490
the window is then
shifted to the right.

00:06:45.580 --> 00:06:49.810
If the backward search yields no
match for a given input symbol,

00:06:49.810 --> 00:06:52.222
or we're seeing a symbol
for the first time,

00:06:52.222 --> 00:06:54.430
instead of outputting a
token with length and offset,

00:06:54.430 --> 00:06:58.120
we actually output a null
token, which is 0, 0, and then

00:06:58.120 --> 00:07:01.360
the token itself that we
actually just encountered.

00:07:01.360 --> 00:07:04.880
The compression process comes
from creating lists, and lists,

00:07:04.880 --> 00:07:08.570
and lists of these token values
across the entire data set

00:07:08.570 --> 00:07:11.100
and then combining
each of the channels

00:07:11.100 --> 00:07:13.390
in their own associative
lanes to create compression.

00:07:15.734 --> 00:07:17.650
Now that we've actually
been through encoding,

00:07:17.650 --> 00:07:20.080
it's time to meet the
rest of the family.

00:07:20.080 --> 00:07:23.460
At the top of our tree
lie LZ78 and LZ77,

00:07:23.460 --> 00:07:25.730
our matriarch and
patriarch algorithms.

00:07:25.730 --> 00:07:29.090
Each one has given rise to
a variant that changes just

00:07:29.090 --> 00:07:31.360
slightly how you search
through your search buffer

00:07:31.360 --> 00:07:33.100
or how you output tokens.

00:07:33.100 --> 00:07:35.690
For example, LZW will
actually maintain

00:07:35.690 --> 00:07:38.600
a running variable length
dictionary of symbols,

00:07:38.600 --> 00:07:41.730
always trying to optimize for
the longest match possible.

00:07:41.730 --> 00:07:44.010
LZSS, on the other
hand, will not

00:07:44.010 --> 00:07:46.900
omit a token if the token
itself is longer than the string

00:07:46.900 --> 00:07:48.120
you're trying to compress.

00:07:48.120 --> 00:07:51.910
Or my favorite, LZMA, actually
combines the LZ algorithm

00:07:51.910 --> 00:07:54.980
with Markov chains to
produce better compression.

00:07:54.980 --> 00:07:57.680
In fact, LZMA is actually
one of the cooler algorithms

00:07:57.680 --> 00:07:58.810
in this entire tree.

00:07:58.810 --> 00:08:01.470
Not only is it more
modern, but it actually

00:08:01.470 --> 00:08:04.740
combines a statistical
encoder with the LZ transform.

00:08:04.740 --> 00:08:06.960
And in fact, most
of these algorithms

00:08:06.960 --> 00:08:09.970
do not compress as well as
you would think and need

00:08:09.970 --> 00:08:13.200
statistical encoding to actually
get the values you want.

00:08:13.200 --> 00:08:16.130
In fact, if you look under the
hood at most modern compression

00:08:16.130 --> 00:08:19.060
algorithms, you'll actually see
an LZ variant sitting around.

00:08:19.060 --> 00:08:21.830
For example, LZW is
actually the backbone

00:08:21.830 --> 00:08:25.870
of PKZIP, a popular compression
algorithm in the early '90s.

00:08:25.870 --> 00:08:30.060
On the other hand, LZSS makes
up most of what WIN RAR uses.

00:08:30.060 --> 00:08:33.080
And this deflate algorithm over
here-- you may recognize it.

00:08:33.080 --> 00:08:35.289
It powers everything
behind GZIP,

00:08:35.289 --> 00:08:37.179
which runs most of the
internet right now.

00:08:37.179 --> 00:08:39.929
Or you may know it as a
better name of WINZIP,

00:08:39.929 --> 00:08:41.970
which, of course, is the
Windows variant of that.

00:08:41.970 --> 00:08:43.850
And of course, my
favorite LZMA actually

00:08:43.850 --> 00:08:45.700
comes from a modern
archiver known

00:08:45.700 --> 00:08:48.490
as 7 Zip that's slowly
gaining popularity.

00:08:48.490 --> 00:08:51.120
The truth of all
of this is that LZ

00:08:51.120 --> 00:08:53.260
is such a dynamic and
efficient algorithm

00:08:53.260 --> 00:08:57.210
that it powers the backbone
of the fastest, most

00:08:57.210 --> 00:09:00.300
used archivers out there
in the public today.

00:09:00.300 --> 00:09:01.830
Even if something
else comes along,

00:09:01.830 --> 00:09:03.750
the LZ family's reign
is going to last

00:09:03.750 --> 00:09:06.946
much longer than
you and I realize.

00:09:06.946 --> 00:09:08.320
In the realm of
data compression,

00:09:08.320 --> 00:09:10.745
the LZ family still
reigns supreme.

00:09:10.745 --> 00:09:13.670
It's the backbone of the
most popular and aggressive

00:09:13.670 --> 00:09:14.970
compressors out there.

00:09:14.970 --> 00:09:17.490
But there's trouble
in the realm.

00:09:17.490 --> 00:09:20.480
A new breed of algorithms
known as the Markov models

00:09:20.480 --> 00:09:22.570
are actually
threatening its reign.

00:09:22.570 --> 00:09:24.970
But that's a story
for a different show.

00:09:24.970 --> 00:09:26.107
My name is Colt McAnlis.

00:09:26.107 --> 00:09:26.940
Thanks for watching.

00:09:26.940 --> 00:09:31.190
[MUSIC PLAYING]

