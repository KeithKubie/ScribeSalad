WEBVTT
Kind: captions
Language: en

00:00:03.040 --> 00:00:05.900
Hi, I’m Adriene Hill and Welcome to Crash
Course Statistics.

00:00:05.910 --> 00:00:09.700
We’ve spent out a lot of time talking about
data visualization and different kinds of

00:00:09.710 --> 00:00:15.830
frequency plots--like dot-plots and histograms--that
tell us how frequently things occur in data

00:00:15.830 --> 00:00:17.130
we actually have.

00:00:17.130 --> 00:00:20.570
But so far in this series, the data we have
talked about usually isn’t ALL the data

00:00:20.570 --> 00:00:21.570
that exists.

00:00:21.570 --> 00:00:26.550
If I want to know about student loan debt
in America, I am definitely not going to ask

00:00:26.550 --> 00:00:28.280
over 300 million Americans.

00:00:28.280 --> 00:00:29.900
I’m lazy like that.

00:00:29.900 --> 00:00:32.890
But maybe I can find the time to ask 2,000
of them.

00:00:32.890 --> 00:00:37.140
Samples...and the shapes they give us...are
shadows of what all the data would look like.

00:00:37.140 --> 00:00:41.030
We collect samples because we think they’ll
give us a glimpse of the bigger picture.

00:00:41.030 --> 00:00:44.340
They’ll tell us something about the shape
of all the data.

00:00:44.340 --> 00:00:49.200
Because it turns out we can learn almost everything we need to know about data from its shape.

00:00:49.220 --> 00:00:58.380
Intro

00:00:58.380 --> 00:01:01.280
Picture a histogram of every single person’s
height.

00:01:01.280 --> 00:01:06.140
Now imagine the bars getting thinner, and
thinner, and thinner ...as the bins get smaller

00:01:06.149 --> 00:01:07.159
and smaller.

00:01:07.159 --> 00:01:12.090
Till they are so thin that the outline of
our histogram looks like a smooth line ...since

00:01:12.090 --> 00:01:14.829
this is a distribution of continuous numbers.

00:01:14.829 --> 00:01:17.200
And there’s an infinite possibility of heights.

00:01:17.200 --> 00:01:21.490
I am 1.67642… (and on and on) meters tall.

00:01:21.490 --> 00:01:26.631
If we let our bars be infinitely small, we
get a smooth curve, also known as the distribution

00:01:26.631 --> 00:01:27.631
of the data.

00:01:27.631 --> 00:01:33.099
A distribution represents all possible values
for a set of data...and how often those values

00:01:33.100 --> 00:01:33.960
occur.

00:01:33.960 --> 00:01:35.500
Distributions can also be discrete.

00:01:35.509 --> 00:01:37.500
Like the number of countries people have visited.

00:01:37.500 --> 00:01:41.060
That means they only have a few set values--that
they can take on.

00:01:41.060 --> 00:01:45.440
These distributions look a lot more like the
histograms we’re used to seeing.

00:01:45.440 --> 00:01:49.540
Like a histogram, the distribution tells us
about the shape and spread of data.

00:01:49.540 --> 00:01:53.220
We can think of distributions as a set of
instructions for a machine that generates

00:01:53.220 --> 00:01:54.219
random numbers.

00:01:54.219 --> 00:01:56.969
Let’s say it generates the number of leaves
on a tree.

00:01:56.969 --> 00:02:00.679
You may well be wondering why we’d have
a tree-leaf-number generating machine.

00:02:00.679 --> 00:02:03.310
The idea here is that EVERYTHING can generate
data.

00:02:03.310 --> 00:02:04.530
It’s not just mechanical stuff.

00:02:04.530 --> 00:02:07.000
It’s leaves and animals and even people.

00:02:07.000 --> 00:02:11.950
The distribution is what specifies how the
knobs and dials on our machine are set.

00:02:11.950 --> 00:02:15.700
Once the machine is set, every time there’s
a new tree, the machine pops out a random

00:02:15.700 --> 00:02:17.880
number of leaves from the distribution.

00:02:17.880 --> 00:02:20.160
It won’t be the same number each time though.

00:02:20.160 --> 00:02:24.170
That’s because it’s a random selection
based on the information the knobs and dials

00:02:24.170 --> 00:02:26.660
tell us about our distribution of leaves.

00:02:26.660 --> 00:02:30.420
When we look at samples of data generated
by our leaf machine, we’re trying to guess

00:02:30.420 --> 00:02:35.600
the shape of the distribution and how that
machine’s knobs and dials are set.

00:02:35.600 --> 00:02:39.960
But remember, samples of data are not all
the data, so when we compare the shapes of

00:02:39.960 --> 00:02:44.740
two samples of data, we’re really asking
whether the same distribution--these two machine

00:02:44.751 --> 00:02:49.320
settings--could have produced these two different--but
sorta similar--shapes.

00:02:49.320 --> 00:02:53.010
If you got an especially expensive electricity
bill last month, you may want to look at the

00:02:53.010 --> 00:02:59.900
histogram of your average daily energy consumption
this month, and the same month last year side-by-side.

00:02:59.900 --> 00:03:05.400
It’s not that realistic to expect that you
consumed energy at EXACTLY the same rate this

00:03:05.400 --> 00:03:07.000
month as you did the year before.

00:03:07.000 --> 00:03:08.570
There are probably some differences.

00:03:08.570 --> 00:03:12.700
But your question is whether there’s enough
difference to conclude that your energy consuming

00:03:12.700 --> 00:03:14.370
behaviors have changed.

00:03:14.370 --> 00:03:18.970
When we think about data samples as being
just some of the data made using a certain

00:03:18.970 --> 00:03:23.060
distribution shape, it helps us compare samples
in a more meaningful way.

00:03:23.060 --> 00:03:27.800
Because we know that the samples approximate
some theoretical shape, we can draw connections

00:03:27.800 --> 00:03:32.120
between the sample and the theoretical machine
that generated it, which is what we really

00:03:32.120 --> 00:03:33.290
care about.

00:03:33.290 --> 00:03:38.170
While data come in all sorts of shapes, let’s
take a look at a few of the most common, starting

00:03:38.170 --> 00:03:39.390
with the normal distribution.

00:03:39.390 --> 00:03:42.730
We mentioned the Normal distribution when
we talked about the different ways to measure

00:03:42.730 --> 00:03:46.940
the center of data--since the mean, median,
and mode of a normal distribution are the

00:03:46.940 --> 00:03:47.940
same.

00:03:47.940 --> 00:03:52.380
This tells us that the distribution is symmetric,
meaning you could fold it in half and those

00:03:52.380 --> 00:03:56.760
halves would be the same and that it’s unimodal,
meaning there’s only one peak.

00:03:56.760 --> 00:04:01.930
The shape of a normal distribution is set
by two familiar statistics: the mean and standard

00:04:01.930 --> 00:04:02.990
deviation.

00:04:02.990 --> 00:04:05.950
The mean tells us where the center of the
distribution is.

00:04:05.950 --> 00:04:10.440
The standard deviation tells us how thin or
squished the normal distribution is.

00:04:10.440 --> 00:04:15.050
Since the standard deviation is the average
distance between any point and the mean, the

00:04:15.050 --> 00:04:18.650
smaller it is the closer all the data will
be to the mean.

00:04:18.650 --> 00:04:20.960
We’ll have a skinnier normal distribution.

00:04:20.960 --> 00:04:26.960
Most of the data in the normal distribution--about
68%--is within 1 standard deviation of the

00:04:26.960 --> 00:04:28.870
mean on either side.

00:04:28.870 --> 00:04:33.620
Just like the quartiles in a boxplot, the
smaller the range that 68% of the data has

00:04:33.620 --> 00:04:35.820
to occupy, the more squished it gets.

00:04:35.820 --> 00:04:40.319
Speaking of boxplots here’s what the boxplot
for normally distributed data looks like.

00:04:40.319 --> 00:04:45.099
The two halves of our box are exactly the
same because the normal distribution is symmetric.

00:04:45.099 --> 00:04:48.749
You’ve probably seen the normal distribution
in a lot of different places, it gets called

00:04:48.749 --> 00:04:50.210
a Bell Curve sometimes.

00:04:50.210 --> 00:04:54.940
Attributes like IQ and the number of Fruit
Loops you get in a box are approximately normally

00:04:54.940 --> 00:04:55.880
distributed.

00:04:55.880 --> 00:05:00.120
Normal distributions come up a lot when we
look at groups of things, like the total value

00:05:00.120 --> 00:05:02.660
rolled after 10 dice rolls, or birth weights.

00:05:02.660 --> 00:05:06.600
We’ll talk more about why the normal distribution
is so useful in the future.

00:05:06.600 --> 00:05:10.660
As we’ve seen in this series, data isn’t
always normal or symmetric, often times it

00:05:10.660 --> 00:05:14.460
has some extreme values on one side making
it a little bit skewed.

00:05:14.460 --> 00:05:19.500
Age at death during the middle ages is left-skewed...cause lots of people died young while the time it

00:05:19.500 --> 00:05:23.779
takes to fill out the Nerdfighteria survey
was right skewed because some people lolly-gagged.

00:05:23.779 --> 00:05:29.180
In a boxplot of data from a skewed distribution,
the median will not usually split the box

00:05:29.180 --> 00:05:31.199
into two even pieces.

00:05:31.199 --> 00:05:35.560
Instead the side with the skewed tail will
tend to be stretched out, and often, we’ll

00:05:35.560 --> 00:05:40.529
see a lot of outliers on that side, just like
the boxplot of the Nerdfighteria survey times.

00:05:40.529 --> 00:05:44.819
When we see those features in our sample of
data, it suggests that the distribution that

00:05:44.819 --> 00:05:47.930
generated our data also has some kind of skewed
tail.

00:05:47.930 --> 00:05:49.900
Skew can be a useful way to compare data.

00:05:49.900 --> 00:05:54.319
For example, teachers often look at the distribution
of scores on a test to see how difficult the

00:05:54.320 --> 00:05:55.500
test was.

00:05:55.500 --> 00:06:00.420
Really difficult tests tend to generate skewed scores, with most students doing pretty poorly

00:06:00.430 --> 00:06:02.520
and a few who still ace it.

00:06:02.520 --> 00:06:06.520
Say we flashed pictures of 20 pokemon and
asked people to name them.

00:06:06.520 --> 00:06:07.599
Here are their grades.

00:06:07.599 --> 00:06:12.449
Or another sample from a test asking people
to list all 195 countries.

00:06:12.449 --> 00:06:16.909
We can compare the shapes and centers of these
two groups of tests, as well as any other

00:06:16.909 --> 00:06:18.300
notable features.

00:06:18.300 --> 00:06:20.760
First of all, these two samples look pretty
similar.

00:06:20.760 --> 00:06:22.080
Both have a right skew.

00:06:22.080 --> 00:06:26.560
Both have a pretty low center, but the second
test has a more extreme skew.

00:06:26.560 --> 00:06:30.720
Bigger skewed tails usually mean that the
data--and therefore the distribution--has

00:06:30.729 --> 00:06:36.039
both a larger range, and a bigger standard
deviation than data with a smaller tail.

00:06:36.039 --> 00:06:40.589
The standard deviation is higher because not
only are extreme data further away from the

00:06:40.589 --> 00:06:45.409
mean, they drag the mean toward them, making
most of the other points just a little further

00:06:45.409 --> 00:06:46.659
from the mean too.

00:06:46.659 --> 00:06:50.969
While the direction of the skew tells you
where most of the data is--always on the opposite

00:06:50.969 --> 00:06:55.430
side of the skewed tail--the extremeness of
the skew can help you mentally compare the

00:06:55.430 --> 00:06:58.990
approximate measures of spread, like range
and standard deviation.

00:06:58.990 --> 00:07:03.289
But we compare the shapes of two samples in
order to ask whether the shape of the distributions

00:07:03.289 --> 00:07:08.340
that generated them are different, or whether
ONE shape could have randomly created both

00:07:08.340 --> 00:07:08.920
samples.

00:07:09.000 --> 00:07:14.240
In terms of our machine analogy, we ask whether
one machine with its knob settings could have

00:07:14.249 --> 00:07:18.460
spit out two sets of scores, one that looks
like test A, and one that looks like test

00:07:18.460 --> 00:07:22.400
B. Answering that question get’s complicated,
but we’ll get there.

00:07:22.400 --> 00:07:26.800
Now that we’ve examined the tails, let’s
look at the middle of some distributions.

00:07:26.800 --> 00:07:31.880
Almost all the distributions we’ve seen
so far are unimodal--they only have one peak.

00:07:31.889 --> 00:07:34.800
But there’s many times when data might have
two or more peaks.

00:07:34.810 --> 00:07:37.099
We call it bimodal or multimodal data.

00:07:37.099 --> 00:07:40.931
And it looks like the back of a camel, or
maybe like two of our unimodal distributions

00:07:40.931 --> 00:07:42.469
pasted side by side.

00:07:42.469 --> 00:07:46.240
And, that’s probably what’s happening--the
unimodal distributions, not the camel thing.

00:07:46.240 --> 00:07:50.060
Often when you see multimodal data in the
world it’s because there are two different

00:07:50.120 --> 00:07:54.580
machines with two different distributions
that are both generating data that is being--for

00:07:54.589 --> 00:07:56.779
some reason or other--measured together.

00:07:56.780 --> 00:08:01.720
One possible example of this is the length
in minutes that the geyser Old Faithful erupts.

00:08:01.720 --> 00:08:06.660
Most eruptions last either about 2 minutes,
or about 4 minutes, with few eruptions around

00:08:06.660 --> 00:08:09.420
the 3-minute mark, giving us a bimodal distribution.

00:08:09.420 --> 00:08:13.520
It’s entirely possible that there are two
different mechanisms behind the data, even

00:08:13.520 --> 00:08:15.080
though they’re being measured together.

00:08:15.099 --> 00:08:19.119
For example, one set of conditions may lead
to an eruption that’s about 2 minutes long,

00:08:19.119 --> 00:08:23.529
and another--maybe a different temperature
or latency--leads to a different kind of eruption

00:08:23.529 --> 00:08:25.680
which lasts on average 4 minutes.

00:08:25.680 --> 00:08:29.909
Since these two potentially different types
of eruptions are being measured together,

00:08:29.909 --> 00:08:35.390
the data look like they come from one distribution
with two bumps, but it is likely that there’s

00:08:35.390 --> 00:08:38.330
two unimodal distributions being measured
at the same time.

00:08:38.330 --> 00:08:41.650
Another example that you don’t need to be
a geologist to understand is the race times

00:08:41.650 --> 00:08:42.810
for some marathons.

00:08:42.810 --> 00:08:46.910
While this data may look like it comes from
a unimodal distribution, in reality there’s

00:08:46.910 --> 00:08:51.040
two big groups of people who run a marathon:
those that are competing, and those that do

00:08:51.040 --> 00:08:52.410
it to prove they can do it.

00:08:52.410 --> 00:08:55.950
There’s usually one peak around the time
that all the professional runners cross the

00:08:55.950 --> 00:08:58.190
finish line, and another when the amateurs do.

00:08:58.190 --> 00:09:03.440
While we don’t know for sure that bimodal
data is secretly two distributions disguised

00:09:03.440 --> 00:09:06.200
as one, it is a good reason to look at things
more closely.

00:09:06.200 --> 00:09:09.250
We’ll finish today with uniform distribution.

00:09:09.250 --> 00:09:12.320
Even though we haven’t mentioned uniform
distributions yet, you’ve probably come

00:09:12.320 --> 00:09:14.060
across them in your everyday life.

00:09:14.060 --> 00:09:18.350
Each value in a uniform distribution has the
same frequency, just like each number on a

00:09:18.350 --> 00:09:20.960
die has exactly the same chance of being rolled.

00:09:20.960 --> 00:09:24.150
When you need to decide something fairly,
like which of your 6 roomates has to do dishes

00:09:24.150 --> 00:09:28.310
tonight, or which friend to take to the Jay-Z
concert--the best thing you can do is use

00:09:28.310 --> 00:09:31.310
something--like a die--that has a uniform
distribution.

00:09:31.310 --> 00:09:33.720
That gives everyone an equal chance of being
picked.

00:09:33.720 --> 00:09:37.400
And you can have uniform distributions with
any number of outcomes.

00:09:37.400 --> 00:09:38.950
There are 20-sided dice.

00:09:38.950 --> 00:09:42.550
When you’re in Vegas playing a round of
roulette the ball is equally likely to land

00:09:42.550 --> 00:09:44.480
in any of 38 slots.

00:09:44.480 --> 00:09:48.630
There’s a difference between the shape of
all the data, and the shape of a sample of

00:09:48.630 --> 00:09:49.240
the data.

00:09:49.240 --> 00:09:52.260
When we talk about a uniform distribution,
we’re talking about the settings of that

00:09:52.260 --> 00:09:56.260
data generating machine, it doesn’t mean
that every sample--or even most samples--

00:09:56.261 --> 00:09:59.950
of our data will have exactly the same frequency
for each outcome.

00:09:59.950 --> 00:10:05.490
It’s entirely possible that rolling a die
60 times results in a sample shaped like this:

00:10:05.490 --> 00:10:08.160
Even if we know the theoretical distribution
looks like this:

00:10:08.160 --> 00:10:13.280
Using statistics allow us to take the shape
of samples that has some randomness and uncertainty,

00:10:13.280 --> 00:10:17.480
and make a guess about the true distribution
that created that sample of data.

00:10:17.480 --> 00:10:20.740
Statistics is all about making decisions when
we’re not sure.

00:10:20.740 --> 00:10:24.700
It allows us to look at the shape of 60 dice
rolls and figure out whether we believe the

00:10:24.700 --> 00:10:28.380
die is fair... or whether the die is loaded
or whether we need to keep rolling.

00:10:28.380 --> 00:10:32.140
Whether it’s finding the true distribution
of eruption times at Old Faithful, or showing

00:10:32.140 --> 00:10:36.010
evidence that a company is discriminating
based on age, gender, or race.

00:10:36.010 --> 00:10:39.020
The shape of data gives us a glimpse into
the true nature

00:10:39.020 --> 00:10:40.780
of what is happening in the world.

00:10:40.900 --> 00:10:43.400
Thanks for watching and DFTBAQ.

00:10:43.400 --> 00:10:44.820
I’ll see you next time.

