WEBVTT
Kind: captions
Language: en

00:00:00.850 --> 00:00:02.798
All right, let's add a step now.

00:00:02.798 --> 00:00:05.900
I provided some fixed server log data.

00:00:05.900 --> 00:00:09.430
What we are doing here is counting the
number of hits to the main web page for

00:00:09.430 --> 00:00:11.450
each day in the logs.

00:00:11.450 --> 00:00:14.610
You can use the mapper or reducer
files I provided or write your own.

00:00:15.650 --> 00:00:21.420
Upload the extracted data, the mapper,
and reducer files to your S3 bucket.

00:00:21.420 --> 00:00:24.710
The mapper and reducer are written
in Python for Hadoop streaming.

00:00:24.710 --> 00:00:27.998
So, for the step 2 streaming program.

00:00:27.998 --> 00:00:30.940
So here is where you link to your
mapper and reducer programs.

00:00:30.940 --> 00:00:34.657
Click the folder icons to select
the files on your S3 bucket.

00:00:36.060 --> 00:00:40.573
And same thing for the input,
and find the output.

00:00:40.573 --> 00:00:43.470
And give this a name,
something like Hits.

00:00:43.470 --> 00:00:44.980
And run it, just hit Add.

00:00:46.730 --> 00:00:48.200
Down here you can see the step running.

00:00:49.520 --> 00:00:53.140
You can also view the logs to see
what's happening on the cluster and

00:00:53.140 --> 00:00:55.870
you can also see the jobs running
to make sure they're not failing.

00:00:55.870 --> 00:01:00.719
If they do fail you can look at the job
logs to help to bug your MapReduce code.

00:01:00.719 --> 00:01:02.607
All right, now that is done running,

00:01:02.607 --> 00:01:05.090
you should see the output
data on your S3 bucket.

00:01:06.260 --> 00:01:08.970
The process is similar for
high events spark jobs.

00:01:08.970 --> 00:01:11.700
You put your analysis code
input data on this three,

00:01:11.700 --> 00:01:13.190
then link to them when adding the step.

00:01:13.190 --> 00:01:15.330
And don't forget to
terminate the cluster.

00:01:15.330 --> 00:01:17.520
If you leave it running,
it can cost you quite a bit.

