WEBVTT
Kind: captions
Language: en

00:00:00.450 --> 00:00:04.990
Let's apply this to face recognition and
it's referred to as using Eigenfaces and

00:00:04.990 --> 00:00:08.745
it comes from this paper by Matt Turk
and Alex Pentland, Sandy Pentland.

00:00:08.745 --> 00:00:12.000
It's Alex Pentland, but
his name is, he goes by Sandy.

00:00:12.000 --> 00:00:13.830
They were at the MIT
media lab at the time.

00:00:15.770 --> 00:00:18.870
And one of the reasons this paper,
besides the fact that it works so

00:00:18.870 --> 00:00:20.880
well, is just known so

00:00:20.880 --> 00:00:25.300
well, in my humble opinion, is because
of this term, Eigenfaces and face space.

00:00:25.300 --> 00:00:26.575
What a great, what a great phrase.

00:00:26.575 --> 00:00:29.049
In fact, I can show you the Eigenfaces,
I will in just a minute.

00:00:29.049 --> 00:00:30.640
All right.

00:00:30.640 --> 00:00:32.280
So, here's what they do.

00:00:32.280 --> 00:00:36.925
They assume that most face
images are going to lie on some

00:00:36.925 --> 00:00:41.730
low-dimensional subspace
in the big image space.

00:00:41.730 --> 00:00:46.220
Determine by, I don't know,
let's say, k eigenvectors.

00:00:46.220 --> 00:00:48.910
Okay?
K directions of maximum variance where k

00:00:48.910 --> 00:00:51.280
is going to be way smaller than d.

00:00:51.280 --> 00:00:53.190
So, d might be 10,000 or a million.

00:00:53.190 --> 00:00:55.217
K is going to be like 20 or 200.

00:00:55.217 --> 00:00:58.008
Both of those numbers are way
smaller than 10,000 or a million.

00:00:58.008 --> 00:01:01.591
So, what it does is they use PCA,
like I just showed you,

00:01:01.591 --> 00:01:05.190
to find the vectors, or
what are called the Eigenfaces.

00:01:05.190 --> 00:01:09.740
We'll look at them in a minute,
u1 through uk, that span, that subspace.

00:01:09.740 --> 00:01:10.830
Okay?
So, you take all your images,

00:01:10.830 --> 00:01:12.730
you find your eigenvectors.

00:01:12.730 --> 00:01:15.584
And now, what you're going to do,
and this is the really cool part,

00:01:15.584 --> 00:01:20.120
is you're going to represent your
face images in that data set

00:01:20.120 --> 00:01:25.150
as just the linear combination
of those eigenvectors.

00:01:25.150 --> 00:01:26.690
Or another way of saying it is,

00:01:26.690 --> 00:01:30.670
I'm just going to have the coefficients
of the 20, of the 20 eigenvectors.

00:01:30.670 --> 00:01:32.750
If it's 200, 200 eigenvectors.

00:01:32.750 --> 00:01:37.090
And I'm going to represent my
entire image, all 10,000 numbers,

00:01:37.090 --> 00:01:40.030
by just the coefficients
of these eigenvectors.

00:01:40.030 --> 00:01:43.473
And I multiply those coefficients
times the eigenvector, sum them up,

00:01:43.473 --> 00:01:44.778
that would be my new image.

00:01:44.778 --> 00:01:45.920
All right.

00:01:45.920 --> 00:01:48.120
So, it's a tremendous data reduction.

00:01:48.120 --> 00:01:49.980
So, take you through an example here.

00:01:49.980 --> 00:01:53.050
So, and some of these images come
directly from the old papers,

00:01:53.050 --> 00:01:55.350
some from some newer work,
but makes the same point.

00:01:55.350 --> 00:01:58.180
So, suppose I have some
training images X1 through XM.

00:01:58.180 --> 00:01:59.370
So, here's a picture of faces.

00:01:59.370 --> 00:02:02.260
Now, notice,
it's just the cropped part of the face.

00:02:02.260 --> 00:02:04.540
So, they're going to try to do
recognition just on the cropped

00:02:04.540 --> 00:02:05.150
part of the face.

00:02:05.150 --> 00:02:11.048
So, the first thing that's kind of cool
to look at is this is the mean image,

00:02:11.048 --> 00:02:12.620
mean face.

00:02:12.620 --> 00:02:15.820
Not the mean face,
the mean face, the average face.

00:02:15.820 --> 00:02:17.960
Now, nobody wants to
have the average face.

00:02:17.960 --> 00:02:18.810
Too bad.

00:02:18.810 --> 00:02:21.470
Whoever that is, he or
she has the average face.

00:02:21.470 --> 00:02:23.500
Why did I say he or she?

00:02:23.500 --> 00:02:26.560
Well, by definition, it's hard to tell.

00:02:26.560 --> 00:02:28.750
Right?
because it's an average over everybody.

00:02:28.750 --> 00:02:32.390
There is some interesting work
done on showing how average faces

00:02:32.390 --> 00:02:35.440
appear more beautiful than real faces.

00:02:35.440 --> 00:02:36.550
This was done a long time ago.

00:02:36.550 --> 00:02:38.450
It's just a study of
the appearance of beauty, anyway.

00:02:38.450 --> 00:02:41.730
It was interesting image processing
approach to thinking about,

00:02:41.730 --> 00:02:43.970
could you predict when a face
would be deemed beautiful.

00:02:43.970 --> 00:02:44.470
All right.

00:02:44.470 --> 00:02:45.127
You can imagine here's a question.

00:02:45.127 --> 00:02:48.694
Show a picture to a computer,
face image, part of the digression, and

00:02:48.694 --> 00:02:52.795
say on the average, on a scale of one to
ten, how likely are people to rate this

00:02:52.795 --> 00:02:55.363
thing as beautiful,
as this person is beautiful?

00:02:55.363 --> 00:02:56.727
And could you do that?

00:02:56.727 --> 00:03:00.537
I think these days using machine
learning might be a different approach.

00:03:00.537 --> 00:03:04.500
But at the time, there was work
done looking at average faces.

00:03:04.500 --> 00:03:05.830
Sure you wanted to know that.

00:03:05.830 --> 00:03:08.500
Anyway, oh, and so
what do you do with the mean face?

00:03:08.500 --> 00:03:10.690
Like what you should do with
a mean face, you throw it away.

00:03:10.690 --> 00:03:13.690
Well, what you really do is
subtract it out of your population.

00:03:13.690 --> 00:03:14.440
Isn't that great?

00:03:14.440 --> 00:03:16.750
We subtract all the [LAUGH] mean
faces out of the population.

00:03:16.750 --> 00:03:19.220
No, we subtract the mean
face from all the, and

00:03:19.220 --> 00:03:21.640
what we have left is our distribution.

00:03:21.640 --> 00:03:24.540
And then,
we start computing the eigenvectors.

00:03:24.540 --> 00:03:25.564
And here they are.
Okay?

00:03:25.564 --> 00:03:29.409
So, here's some top eigenvectors,
and one of the things you'll notice

00:03:29.409 --> 00:03:33.620
right away, fact, let's take
a look at this first eigenvector.

00:03:33.620 --> 00:03:35.150
Actually, the, the second one.

00:03:35.150 --> 00:03:37.300
Is that it's much brighter
on one side than the other,

00:03:37.300 --> 00:03:40.550
because some of the variation might just
be due to the lighting coming from one

00:03:40.550 --> 00:03:41.728
side or the other.

00:03:41.728 --> 00:03:44.660
But later, you'll have these
different eigenvectors,

00:03:44.660 --> 00:03:47.330
which look like these
kind of ghostly images.

00:03:47.330 --> 00:03:49.760
But these are eigenvectors.

00:03:49.760 --> 00:03:54.485
So, they look to you like pictures and
they are, but what they are is

00:03:54.485 --> 00:04:00.950
they're a 10,000 dimensional vector,
which is just an image.

00:04:00.950 --> 00:04:05.810
Remember, I can go between the image
space 10,000, right, and the images, so

00:04:05.810 --> 00:04:09.890
I'm just showing you these
eigenvectors as images.

00:04:09.890 --> 00:04:12.190
Okay?
And if I want to take a dot product

00:04:12.190 --> 00:04:14.580
of a real picture and these?

00:04:14.580 --> 00:04:15.140
That's easy.

00:04:15.140 --> 00:04:18.589
All I do is overlay them and multiply.

00:04:18.589 --> 00:04:20.130
Multiply every pixel and sum them up.

00:04:20.130 --> 00:04:21.940
because that's what a dot product is.

00:04:21.940 --> 00:04:23.439
So, when I show this to students,

00:04:23.439 --> 00:04:26.665
sometimes they don't understand how
come the images are eigenvectors.

00:04:26.665 --> 00:04:30.544
And that's because in image space,
that's what it means,

00:04:30.544 --> 00:04:31.920
a vector is an image.

