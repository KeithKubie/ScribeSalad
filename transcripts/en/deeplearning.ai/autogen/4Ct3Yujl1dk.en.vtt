WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:01.760
in the last video you learned about

00:00:01.760 --> 00:00:01.770
in the last video you learned about
 

00:00:01.770 --> 00:00:04.100
in the last video you learned about
gradient checking in this video I want

00:00:04.100 --> 00:00:04.110
gradient checking in this video I want
 

00:00:04.110 --> 00:00:06.499
gradient checking in this video I want
to share you some practical tips or some

00:00:06.499 --> 00:00:06.509
to share you some practical tips or some
 

00:00:06.509 --> 00:00:08.629
to share you some practical tips or some
notes on how to actually go about

00:00:08.629 --> 00:00:08.639
notes on how to actually go about
 

00:00:08.639 --> 00:00:10.100
notes on how to actually go about
implementing this for your neural

00:00:10.100 --> 00:00:10.110
implementing this for your neural
 

00:00:10.110 --> 00:00:12.350
implementing this for your neural
network first don't use graduating

00:00:12.350 --> 00:00:12.360
network first don't use graduating
 

00:00:12.360 --> 00:00:15.110
network first don't use graduating
training or me to debug so what I mean

00:00:15.110 --> 00:00:15.120
training or me to debug so what I mean
 

00:00:15.120 --> 00:00:18.830
training or me to debug so what I mean
is that computing D theta or procs eyes

00:00:18.830 --> 00:00:18.840
is that computing D theta or procs eyes
 

00:00:18.840 --> 00:00:21.109
is that computing D theta or procs eyes
all the values of ID is a very slow

00:00:21.109 --> 00:00:21.119
all the values of ID is a very slow
 

00:00:21.119 --> 00:00:23.599
all the values of ID is a very slow
computation so to implement gradient

00:00:23.599 --> 00:00:23.609
computation so to implement gradient
 

00:00:23.609 --> 00:00:25.759
computation so to implement gradient
descent use backprop to compute D theta

00:00:25.759 --> 00:00:25.769
descent use backprop to compute D theta
 

00:00:25.769 --> 00:00:27.859
descent use backprop to compute D theta
and just use back prop to compute the

00:00:27.859 --> 00:00:27.869
and just use back prop to compute the
 

00:00:27.869 --> 00:00:29.839
and just use back prop to compute the
derivative and as only when you're

00:00:29.839 --> 00:00:29.849
derivative and as only when you're
 

00:00:29.849 --> 00:00:32.240
derivative and as only when you're
debugging that you would compute this to

00:00:32.240 --> 00:00:32.250
debugging that you would compute this to
 

00:00:32.250 --> 00:00:34.520
debugging that you would compute this to
Mitchell as close to D theta but once

00:00:34.520 --> 00:00:34.530
Mitchell as close to D theta but once
 

00:00:34.530 --> 00:00:36.380
Mitchell as close to D theta but once
you've done that then you would turn off

00:00:36.380 --> 00:00:36.390
you've done that then you would turn off
 

00:00:36.390 --> 00:00:37.910
you've done that then you would turn off
the grant check and don't run this

00:00:37.910 --> 00:00:37.920
the grant check and don't run this
 

00:00:37.920 --> 00:00:39.530
the grant check and don't run this
during every iteration being a sentence

00:00:39.530 --> 00:00:39.540
during every iteration being a sentence
 

00:00:39.540 --> 00:00:42.139
during every iteration being a sentence
it's just much too slow second if the

00:00:42.139 --> 00:00:42.149
it's just much too slow second if the
 

00:00:42.149 --> 00:00:44.180
it's just much too slow second if the
never fails brag check look at the

00:00:44.180 --> 00:00:44.190
never fails brag check look at the
 

00:00:44.190 --> 00:00:45.650
never fails brag check look at the
components look at the individual

00:00:45.650 --> 00:00:45.660
components look at the individual
 

00:00:45.660 --> 00:00:48.020
components look at the individual
components to try to identify the bug so

00:00:48.020 --> 00:00:48.030
components to try to identify the bug so
 

00:00:48.030 --> 00:00:51.350
components to try to identify the bug so
what I mean by that is is the faith in

00:00:51.350 --> 00:00:51.360
what I mean by that is is the faith in
 

00:00:51.360 --> 00:00:53.959
what I mean by that is is the faith in
aprox is very far from DSA so what I

00:00:53.959 --> 00:00:53.969
aprox is very far from DSA so what I
 

00:00:53.969 --> 00:00:55.549
aprox is very far from DSA so what I
would do is look at the different values

00:00:55.549 --> 00:00:55.559
would do is look at the different values
 

00:00:55.559 --> 00:00:57.770
would do is look at the different values
of I to see which are the values of D

00:00:57.770 --> 00:00:57.780
of I to see which are the values of D
 

00:00:57.780 --> 00:00:59.599
of I to see which are the values of D
theta aprox they're really very

00:00:59.599 --> 00:00:59.609
theta aprox they're really very
 

00:00:59.609 --> 00:01:02.360
theta aprox they're really very
different than the values of D theta so

00:01:02.360 --> 00:01:02.370
different than the values of D theta so
 

00:01:02.370 --> 00:01:04.399
different than the values of D theta so
for example um if you find that the

00:01:04.399 --> 00:01:04.409
for example um if you find that the
 

00:01:04.409 --> 00:01:07.250
for example um if you find that the
values of theta or D theta they're very

00:01:07.250 --> 00:01:07.260
values of theta or D theta they're very
 

00:01:07.260 --> 00:01:10.340
values of theta or D theta they're very
far off all corresponding to D BL for

00:01:10.340 --> 00:01:10.350
far off all corresponding to D BL for
 

00:01:10.350 --> 00:01:12.800
far off all corresponding to D BL for
some layer or for some layers but the

00:01:12.800 --> 00:01:12.810
some layer or for some layers but the
 

00:01:12.810 --> 00:01:16.219
some layer or for some layers but the
components for DW are quite close right

00:01:16.219 --> 00:01:16.229
components for DW are quite close right
 

00:01:16.229 --> 00:01:17.929
components for DW are quite close right
remember different components of theta

00:01:17.929 --> 00:01:17.939
remember different components of theta
 

00:01:17.939 --> 00:01:20.270
remember different components of theta
correspond to different components of B

00:01:20.270 --> 00:01:20.280
correspond to different components of B
 

00:01:20.280 --> 00:01:22.460
correspond to different components of B
P and W but you find this is the case

00:01:22.460 --> 00:01:22.470
P and W but you find this is the case
 

00:01:22.470 --> 00:01:25.460
P and W but you find this is the case
then maybe you find that some the bug is

00:01:25.460 --> 00:01:25.470
then maybe you find that some the bug is
 

00:01:25.470 --> 00:01:27.380
then maybe you find that some the bug is
in how you're computing DP the

00:01:27.380 --> 00:01:27.390
in how you're computing DP the
 

00:01:27.390 --> 00:01:30.469
in how you're computing DP the
derivative respect to parameters B and

00:01:30.469 --> 00:01:30.479
derivative respect to parameters B and
 

00:01:30.479 --> 00:01:32.990
derivative respect to parameters B and
then similarly vice versa we find that

00:01:32.990 --> 00:01:33.000
then similarly vice versa we find that
 

00:01:33.000 --> 00:01:35.480
then similarly vice versa we find that
the values they're very far you know the

00:01:35.480 --> 00:01:35.490
the values they're very far you know the
 

00:01:35.490 --> 00:01:37.940
the values they're very far you know the
values from D theta aprox that are very

00:01:37.940 --> 00:01:37.950
values from D theta aprox that are very
 

00:01:37.950 --> 00:01:40.160
values from D theta aprox that are very
far from D theta and you find that all

00:01:40.160 --> 00:01:40.170
far from D theta and you find that all
 

00:01:40.170 --> 00:01:43.310
far from D theta and you find that all
those components came from GW or from GW

00:01:43.310 --> 00:01:43.320
those components came from GW or from GW
 

00:01:43.320 --> 00:01:45.410
those components came from GW or from GW
and certain layer then that might help

00:01:45.410 --> 00:01:45.420
and certain layer then that might help
 

00:01:45.420 --> 00:01:47.840
and certain layer then that might help
you hone in on the location of the bug

00:01:47.840 --> 00:01:47.850
you hone in on the location of the bug
 

00:01:47.850 --> 00:01:49.880
you hone in on the location of the bug
there doesn't always let you identify

00:01:49.880 --> 00:01:49.890
there doesn't always let you identify
 

00:01:49.890 --> 00:01:52.100
there doesn't always let you identify
the bug right away but sometimes it

00:01:52.100 --> 00:01:52.110
the bug right away but sometimes it
 

00:01:52.110 --> 00:01:53.660
the bug right away but sometimes it
helps you give you some guesses about

00:01:53.660 --> 00:01:53.670
helps you give you some guesses about
 

00:01:53.670 --> 00:01:56.990
helps you give you some guesses about
other where they track down the bug next

00:01:56.990 --> 00:01:57.000
other where they track down the bug next
 

00:01:57.000 --> 00:02:00.170
other where they track down the bug next
um when doing grad check remember your

00:02:00.170 --> 00:02:00.180
um when doing grad check remember your
 

00:02:00.180 --> 00:02:01.999
um when doing grad check remember your
regularization term if you're using

00:02:01.999 --> 00:02:02.009
regularization term if you're using
 

00:02:02.009 --> 00:02:04.459
regularization term if you're using
regularization so if your cost function

00:02:04.459 --> 00:02:04.469
regularization so if your cost function
 

00:02:04.469 --> 00:02:08.779
regularization so if your cost function
is J of theta equals 1 over m sum of

00:02:08.779 --> 00:02:08.789
is J of theta equals 1 over m sum of
 

00:02:08.789 --> 00:02:11.090
is J of theta equals 1 over m sum of
your losses

00:02:11.090 --> 00:02:11.100
your losses
 

00:02:11.100 --> 00:02:13.820
your losses
um and then plus this regularization

00:02:13.820 --> 00:02:13.830
um and then plus this regularization
 

00:02:13.830 --> 00:02:18.310
um and then plus this regularization
term right some of the hell of wll

00:02:18.310 --> 00:02:18.320
term right some of the hell of wll
 

00:02:18.320 --> 00:02:21.230
term right some of the hell of wll
Frobenius norm squared then this is the

00:02:21.230 --> 00:02:21.240
Frobenius norm squared then this is the
 

00:02:21.240 --> 00:02:23.660
Frobenius norm squared then this is the
definition of J and you should have that

00:02:23.660 --> 00:02:23.670
definition of J and you should have that
 

00:02:23.670 --> 00:02:27.740
definition of J and you should have that
D theta is gradients of J or respect to

00:02:27.740 --> 00:02:27.750
D theta is gradients of J or respect to
 

00:02:27.750 --> 00:02:28.910
D theta is gradients of J or respect to
theta

00:02:28.910 --> 00:02:28.920
theta
 

00:02:28.920 --> 00:02:30.890
theta
including the regularization term so

00:02:30.890 --> 00:02:30.900
including the regularization term so
 

00:02:30.900 --> 00:02:32.900
including the regularization term so
just remember to include that term next

00:02:32.900 --> 00:02:32.910
just remember to include that term next
 

00:02:32.910 --> 00:02:35.540
just remember to include that term next
Grouch egg doesn't work with dropouts

00:02:35.540 --> 00:02:35.550
Grouch egg doesn't work with dropouts
 

00:02:35.550 --> 00:02:37.580
Grouch egg doesn't work with dropouts
because in every iteration dropout is

00:02:37.580 --> 00:02:37.590
because in every iteration dropout is
 

00:02:37.590 --> 00:02:40.070
because in every iteration dropout is
randomly eliminating different subsets

00:02:40.070 --> 00:02:40.080
randomly eliminating different subsets
 

00:02:40.080 --> 00:02:42.890
randomly eliminating different subsets
or the fit in humans there isn't a easy

00:02:42.890 --> 00:02:42.900
or the fit in humans there isn't a easy
 

00:02:42.900 --> 00:02:45.620
or the fit in humans there isn't a easy
to compute cost function J the dropout

00:02:45.620 --> 00:02:45.630
to compute cost function J the dropout
 

00:02:45.630 --> 00:02:48.530
to compute cost function J the dropout
is doing gradient descent on it turns

00:02:48.530 --> 00:02:48.540
is doing gradient descent on it turns
 

00:02:48.540 --> 00:02:50.480
is doing gradient descent on it turns
out that dropout can be viewed as

00:02:50.480 --> 00:02:50.490
out that dropout can be viewed as
 

00:02:50.490 --> 00:02:53.540
out that dropout can be viewed as
optimizing some cost function J but its

00:02:53.540 --> 00:02:53.550
optimizing some cost function J but its
 

00:02:53.550 --> 00:02:56.000
optimizing some cost function J but its
cost function J is defined by summing

00:02:56.000 --> 00:02:56.010
cost function J is defined by summing
 

00:02:56.010 --> 00:02:58.700
cost function J is defined by summing
over all exponentially large subsets of

00:02:58.700 --> 00:02:58.710
over all exponentially large subsets of
 

00:02:58.710 --> 00:03:00.140
over all exponentially large subsets of
nodes they could eliminate in any

00:03:00.140 --> 00:03:00.150
nodes they could eliminate in any
 

00:03:00.150 --> 00:03:02.750
nodes they could eliminate in any
iteration so the cost function J is very

00:03:02.750 --> 00:03:02.760
iteration so the cost function J is very
 

00:03:02.760 --> 00:03:05.590
iteration so the cost function J is very
difficult to compute menu just sampling

00:03:05.590 --> 00:03:05.600
difficult to compute menu just sampling
 

00:03:05.600 --> 00:03:08.660
difficult to compute menu just sampling
the cost function every time you live in

00:03:08.660 --> 00:03:08.670
the cost function every time you live in
 

00:03:08.670 --> 00:03:10.370
the cost function every time you live in
a different random subset and military

00:03:10.370 --> 00:03:10.380
a different random subset and military
 

00:03:10.380 --> 00:03:13.070
a different random subset and military
use gravel so it's difficult to use grad

00:03:13.070 --> 00:03:13.080
use gravel so it's difficult to use grad
 

00:03:13.080 --> 00:03:15.260
use gravel so it's difficult to use grad
chair to double-check your computation

00:03:15.260 --> 00:03:15.270
chair to double-check your computation
 

00:03:15.270 --> 00:03:17.900
chair to double-check your computation
with dropouts so what I usually do is

00:03:17.900 --> 00:03:17.910
with dropouts so what I usually do is
 

00:03:17.910 --> 00:03:20.420
with dropouts so what I usually do is
implement grad check without dropout so

00:03:20.420 --> 00:03:20.430
implement grad check without dropout so
 

00:03:20.430 --> 00:03:23.120
implement grad check without dropout so
you if you want in set key prop in

00:03:23.120 --> 00:03:23.130
you if you want in set key prop in
 

00:03:23.130 --> 00:03:26.150
you if you want in set key prop in
dropout to be equal to 1.0 and then turn

00:03:26.150 --> 00:03:26.160
dropout to be equal to 1.0 and then turn
 

00:03:26.160 --> 00:03:27.440
dropout to be equal to 1.0 and then turn
on dropout and hope that my

00:03:27.440 --> 00:03:27.450
on dropout and hope that my
 

00:03:27.450 --> 00:03:29.800
on dropout and hope that my
implementation of dropout was correct

00:03:29.800 --> 00:03:29.810
implementation of dropout was correct
 

00:03:29.810 --> 00:03:31.970
implementation of dropout was correct
there are some other things you could do

00:03:31.970 --> 00:03:31.980
there are some other things you could do
 

00:03:31.980 --> 00:03:34.550
there are some other things you could do
like fix the pattern of nose dropped and

00:03:34.550 --> 00:03:34.560
like fix the pattern of nose dropped and
 

00:03:34.560 --> 00:03:37.150
like fix the pattern of nose dropped and
verify that grad check for that a

00:03:37.150 --> 00:03:37.160
verify that grad check for that a
 

00:03:37.160 --> 00:03:40.070
verify that grad check for that a
pattern of unis killed off is correct

00:03:40.070 --> 00:03:40.080
pattern of unis killed off is correct
 

00:03:40.080 --> 00:03:42.220
pattern of unis killed off is correct
but in practice I don't usually do that

00:03:42.220 --> 00:03:42.230
but in practice I don't usually do that
 

00:03:42.230 --> 00:03:45.680
but in practice I don't usually do that
so my recommendation is turn off dropout

00:03:45.680 --> 00:03:45.690
so my recommendation is turn off dropout
 

00:03:45.690 --> 00:03:47.930
so my recommendation is turn off dropout
use drag check to double-check that your

00:03:47.930 --> 00:03:47.940
use drag check to double-check that your
 

00:03:47.940 --> 00:03:49.310
use drag check to double-check that your
algorithm is at least correct without

00:03:49.310 --> 00:03:49.320
algorithm is at least correct without
 

00:03:49.320 --> 00:03:52.220
algorithm is at least correct without
dropout and then turn on dropout so

00:03:52.220 --> 00:03:52.230
dropout and then turn on dropout so
 

00:03:52.230 --> 00:03:56.150
dropout and then turn on dropout so
finally this is the subtlety it is not

00:03:56.150 --> 00:03:56.160
finally this is the subtlety it is not
 

00:03:56.160 --> 00:03:58.250
finally this is the subtlety it is not
impossible rarely happens with not

00:03:58.250 --> 00:03:58.260
impossible rarely happens with not
 

00:03:58.260 --> 00:04:00.290
impossible rarely happens with not
impossible that your implementation of

00:04:00.290 --> 00:04:00.300
impossible that your implementation of
 

00:04:00.300 --> 00:04:03.260
impossible that your implementation of
gradient descent is correct when W and B

00:04:03.260 --> 00:04:03.270
gradient descent is correct when W and B
 

00:04:03.270 --> 00:04:05.180
gradient descent is correct when W and B
are close to zero so at random

00:04:05.180 --> 00:04:05.190
are close to zero so at random
 

00:04:05.190 --> 00:04:07.370
are close to zero so at random
initialization but that as you run grain

00:04:07.370 --> 00:04:07.380
initialization but that as you run grain
 

00:04:07.380 --> 00:04:09.950
initialization but that as you run grain
descent and W and B become bigger maybe

00:04:09.950 --> 00:04:09.960
descent and W and B become bigger maybe
 

00:04:09.960 --> 00:04:12.040
descent and W and B become bigger maybe
your implementation of back prop is

00:04:12.040 --> 00:04:12.050
your implementation of back prop is
 

00:04:12.050 --> 00:04:14.900
your implementation of back prop is
correct only when W and B is close to 0

00:04:14.900 --> 00:04:14.910
correct only when W and B is close to 0
 

00:04:14.910 --> 00:04:17.270
correct only when W and B is close to 0
but it gives more inaccurate when W and

00:04:17.270 --> 00:04:17.280
but it gives more inaccurate when W and
 

00:04:17.280 --> 00:04:19.760
but it gives more inaccurate when W and
B become large so one thing you could do

00:04:19.760 --> 00:04:19.770
B become large so one thing you could do
 

00:04:19.770 --> 00:04:21.890
B become large so one thing you could do
I don't do this very often but one thing

00:04:21.890 --> 00:04:21.900
I don't do this very often but one thing
 

00:04:21.900 --> 00:04:23.990
I don't do this very often but one thing
you could do is run drag check your

00:04:23.990 --> 00:04:24.000
you could do is run drag check your
 

00:04:24.000 --> 00:04:24.640
you could do is run drag check your
randomness

00:04:24.640 --> 00:04:24.650
randomness
 

00:04:24.650 --> 00:04:26.710
randomness
elevation and then train the network for

00:04:26.710 --> 00:04:26.720
elevation and then train the network for
 

00:04:26.720 --> 00:04:29.050
elevation and then train the network for
a while so the wmb had some time to

00:04:29.050 --> 00:04:29.060
a while so the wmb had some time to
 

00:04:29.060 --> 00:04:31.810
a while so the wmb had some time to
wonder away from zero from the small

00:04:31.810 --> 00:04:31.820
wonder away from zero from the small
 

00:04:31.820 --> 00:04:33.969
wonder away from zero from the small
random initial values and then run drat

00:04:33.969 --> 00:04:33.979
random initial values and then run drat
 

00:04:33.979 --> 00:04:35.800
random initial values and then run drat
check again after you've trained for

00:04:35.800 --> 00:04:35.810
check again after you've trained for
 

00:04:35.810 --> 00:04:38.200
check again after you've trained for
some number of innovations so that's it

00:04:38.200 --> 00:04:38.210
some number of innovations so that's it
 

00:04:38.210 --> 00:04:40.210
some number of innovations so that's it
so gradient checking and congratulations

00:04:40.210 --> 00:04:40.220
so gradient checking and congratulations
 

00:04:40.220 --> 00:04:41.680
so gradient checking and congratulations
are coming to the end of this week's

00:04:41.680 --> 00:04:41.690
are coming to the end of this week's
 

00:04:41.690 --> 00:04:43.840
are coming to the end of this week's
materials in this week you learned about

00:04:43.840 --> 00:04:43.850
materials in this week you learned about
 

00:04:43.850 --> 00:04:45.909
materials in this week you learned about
how to set up your trained jab intersect

00:04:45.909 --> 00:04:45.919
how to set up your trained jab intersect
 

00:04:45.919 --> 00:04:48.640
how to set up your trained jab intersect
how to analyze bias and variance and

00:04:48.640 --> 00:04:48.650
how to analyze bias and variance and
 

00:04:48.650 --> 00:04:50.110
how to analyze bias and variance and
what things to do if you have high bias

00:04:50.110 --> 00:04:50.120
what things to do if you have high bias
 

00:04:50.120 --> 00:04:52.450
what things to do if you have high bias
and Siberians versus maybe high by 9

00:04:52.450 --> 00:04:52.460
and Siberians versus maybe high by 9
 

00:04:52.460 --> 00:04:56.080
and Siberians versus maybe high by 9
high variance you also saw how to apply

00:04:56.080 --> 00:04:56.090
high variance you also saw how to apply
 

00:04:56.090 --> 00:04:58.510
high variance you also saw how to apply
different forms of regularization like

00:04:58.510 --> 00:04:58.520
different forms of regularization like
 

00:04:58.520 --> 00:05:00.879
different forms of regularization like
l2 regularization and drop on your

00:05:00.879 --> 00:05:00.889
l2 regularization and drop on your
 

00:05:00.889 --> 00:05:02.830
l2 regularization and drop on your
neural network so some tricks for

00:05:02.830 --> 00:05:02.840
neural network so some tricks for
 

00:05:02.840 --> 00:05:04.960
neural network so some tricks for
speeding up the training video network

00:05:04.960 --> 00:05:04.970
speeding up the training video network
 

00:05:04.970 --> 00:05:08.230
speeding up the training video network
and then finally gradient checking so I

00:05:08.230 --> 00:05:08.240
and then finally gradient checking so I
 

00:05:08.240 --> 00:05:10.330
and then finally gradient checking so I
think you've seen a lot in this week and

00:05:10.330 --> 00:05:10.340
think you've seen a lot in this week and
 

00:05:10.340 --> 00:05:12.159
think you've seen a lot in this week and
you get to exercise all these ideas in

00:05:12.159 --> 00:05:12.169
you get to exercise all these ideas in
 

00:05:12.169 --> 00:05:14.680
you get to exercise all these ideas in
this week's program exercise so best of

00:05:14.680 --> 00:05:14.690
this week's program exercise so best of
 

00:05:14.690 --> 00:05:16.360
this week's program exercise so best of
luck exact and I look forward to seeing

00:05:16.360 --> 00:05:16.370
luck exact and I look forward to seeing
 

00:05:16.370 --> 00:05:19.900
luck exact and I look forward to seeing
you in the week 2 materials

