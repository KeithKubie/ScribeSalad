WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.952
[MUSIC PLAYING]

00:00:08.233 --> 00:00:09.400
CHRIS LATTNER: Hi, everyone.

00:00:09.400 --> 00:00:09.850
I'm Chris.

00:00:09.850 --> 00:00:10.720
And this is Brennan.

00:00:10.720 --> 00:00:13.090
And we're super excited to
tell you about a new approach

00:00:13.090 --> 00:00:14.943
to machine learning.

00:00:14.943 --> 00:00:16.360
So here in the
TensorFlow team, it

00:00:16.360 --> 00:00:18.745
is our jobs to push
the state of the art

00:00:18.745 --> 00:00:19.982
in machine learning forward.

00:00:19.982 --> 00:00:21.940
And we've learned a lot
over the last few years

00:00:21.940 --> 00:00:23.390
with deep learning.

00:00:23.390 --> 00:00:25.880
And we've incorporated most
of that all into TensorFlow 2.

00:00:25.880 --> 00:00:27.590
And we're really
excited about it.

00:00:27.590 --> 00:00:29.620
But, here, we're looking
a little bit further

00:00:29.620 --> 00:00:31.510
beyond TensorFlow 2.

00:00:31.510 --> 00:00:33.010
And what do I mean by further?

00:00:33.010 --> 00:00:37.720
Well, eager mode makes it really
easy to train a dynamic model.

00:00:37.720 --> 00:00:41.080
But deploying it still requires
you take that and then write

00:00:41.080 --> 00:00:43.450
a bunch of C++ code
to help drive it.

00:00:43.450 --> 00:00:45.930
And that could be better.

00:00:45.930 --> 00:00:48.430
Similarly, some researchers are
interested in taking machine

00:00:48.430 --> 00:00:51.470
learning models and integrating
them into larger applications.

00:00:51.470 --> 00:00:54.700
That also often requires
writing C++ code.

00:00:54.700 --> 00:00:58.660
We always want more flexible and
expressive autodifferentiation

00:00:58.660 --> 00:00:59.428
mechanisms.

00:00:59.428 --> 00:01:00.970
And one of things
we're excited about

00:01:00.970 --> 00:01:04.030
is being able to define
reusable types that

00:01:04.030 --> 00:01:07.720
then can be put into
new places and used

00:01:07.720 --> 00:01:09.760
with automatic differentiation.

00:01:09.760 --> 00:01:12.400
And we always love improving
your developer workflow.

00:01:12.400 --> 00:01:14.282
We want to make
you more productive

00:01:14.282 --> 00:01:16.240
by taking errors in your
code and bringing them

00:01:16.240 --> 00:01:21.043
to your source and also by just
improving your iteration time.

00:01:21.043 --> 00:01:22.710
Now, what we're really
trying to do here

00:01:22.710 --> 00:01:24.840
is lift TensorFlow to
entirely new heights.

00:01:24.840 --> 00:01:26.820
And to do that, we need
to be able to innovate

00:01:26.820 --> 00:01:28.620
at all levels of the stack.

00:01:28.620 --> 00:01:31.170
This includes the
compiler and the language.

00:01:31.170 --> 00:01:34.110
And that's what Swift for
TensorFlow is all about.

00:01:34.110 --> 00:01:37.020
We think that applying new
solutions to old problems

00:01:37.020 --> 00:01:40.470
can help push machine learning
even further than before.

00:01:40.470 --> 00:01:42.550
Well, let's jump into some code.

00:01:42.550 --> 00:01:43.910
So first, what is Swift?

00:01:43.910 --> 00:01:47.190
Swift is a modern and
cross-platform programming

00:01:47.190 --> 00:01:51.210
language that's designed to
be easy to learn and use.

00:01:51.210 --> 00:01:52.038
Swift uses types.

00:01:52.038 --> 00:01:53.580
And types are great,
because they can

00:01:53.580 --> 00:01:55.630
help you catch errors earlier.

00:01:55.630 --> 00:01:58.670
And also, they encourage
good API design.

00:01:58.670 --> 00:02:01.460
Now, Swift uses type inference,
so it's really easy to use

00:02:01.460 --> 00:02:03.350
and very elegant.

00:02:03.350 --> 00:02:07.220
But it's also open source and
has an open language evolution

00:02:07.220 --> 00:02:09.259
process, which allows us
to change the language

00:02:09.259 --> 00:02:12.790
and make it better for machine
learning which is really great.

00:02:12.790 --> 00:02:14.950
Let's jump into a
more relevant example.

00:02:14.950 --> 00:02:18.220
This is how you define a simple
model in Swift for TensorFlow.

00:02:18.220 --> 00:02:21.670
As you can see, we're
laying out our layers here.

00:02:21.670 --> 00:02:24.250
And then we can find a forward
function, which composes them

00:02:24.250 --> 00:02:26.260
together in a linear sequence.

00:02:26.260 --> 00:02:28.770
You've probably noticed that
this looks a lot like Keras.

00:02:28.770 --> 00:02:30.227
That's no accident, of course.

00:02:30.227 --> 00:02:32.560
We want you to be able to
take what you know about Keras

00:02:32.560 --> 00:02:35.600
and bring it forward
into this world as well.

00:02:35.600 --> 00:02:38.180
Now, once we have a simple
model, let's train it.

00:02:38.180 --> 00:02:39.390
How do we do that?

00:02:39.390 --> 00:02:42.060
All we have to is
instantiate our model,

00:02:42.060 --> 00:02:44.870
pick an optimizer and
some random input data,

00:02:44.870 --> 00:02:46.130
and then pick a training loop.

00:02:46.130 --> 00:02:47.650
And, here, we'll
write it by hand.

00:02:47.650 --> 00:02:49.400
One of the reasons we
like writing by hand

00:02:49.400 --> 00:02:51.233
is that it gives you
the maximum flexibility

00:02:51.233 --> 00:02:53.300
to play with different
kinds of constructs.

00:02:53.300 --> 00:02:56.337
And you can do whatever you
want, which is really great.

00:02:56.337 --> 00:02:58.670
But some of the major advantages
of Swift for TensorFlow

00:02:58.670 --> 00:03:00.033
are the workflow.

00:03:00.033 --> 00:03:02.450
And so instead of telling you
about it, what do you think,

00:03:02.450 --> 00:03:03.658
Brennan, should be show them?

00:03:03.658 --> 00:03:06.320
BRENNAN SAETA: Let's do it.

00:03:06.320 --> 00:03:08.750
All right, the team has
thought long and hard

00:03:08.750 --> 00:03:11.420
about what's the easiest way
for people to get started

00:03:11.420 --> 00:03:13.280
using Swift for TensorFlow.

00:03:13.280 --> 00:03:16.960
And what could be easier than
just opening up a browser tab?

00:03:16.960 --> 00:03:21.190
This is Google Colab,
hosted Jupyter notebooks.

00:03:21.190 --> 00:03:24.860
And it comes with Swift for
TensorFlow built right in.

00:03:24.860 --> 00:03:26.590
Let's see it in action.

00:03:26.590 --> 00:03:28.750
Here is the layer
model, the model

00:03:28.750 --> 00:03:31.780
that Chris just showed you
a couple of slides ago.

00:03:31.780 --> 00:03:34.690
And we're going to run it
using some random training

00:03:34.690 --> 00:03:36.945
data right here in the browser.

00:03:36.945 --> 00:03:39.010
So we're going to
instantiate the model.

00:03:39.010 --> 00:03:41.260
We're going to use the
stochastic gradient descent SGD

00:03:41.260 --> 00:03:43.180
optimizer.

00:03:43.180 --> 00:03:45.130
And here we go.

00:03:45.130 --> 00:03:46.720
We have now just
trained a model using

00:03:46.720 --> 00:03:50.020
Swift for TensorFlow in our
browser on some training data

00:03:50.020 --> 00:03:51.670
right here.

00:03:51.670 --> 00:03:54.530
Now, we can see the training
loss is decreasing over time.

00:03:54.530 --> 00:03:56.000
So that's great.

00:03:56.000 --> 00:03:58.510
But if you're ever like me
and whenever I try and use

00:03:58.510 --> 00:04:01.300
machine learning
in any application,

00:04:01.300 --> 00:04:02.710
I start with a simple model.

00:04:02.710 --> 00:04:03.760
And I've got to iterate.

00:04:03.760 --> 00:04:06.430
I've got to tweak the
model to make it fit better

00:04:06.430 --> 00:04:09.350
to the task at hand.

00:04:09.350 --> 00:04:11.650
So since we're trying to
show you the workflow,

00:04:11.650 --> 00:04:13.300
let's actually edit this model.

00:04:13.300 --> 00:04:16.010
Let's make it more accurate.

00:04:16.010 --> 00:04:17.500
So here we are.

00:04:17.500 --> 00:04:19.600
Now, let's think a
little for a moment.

00:04:19.600 --> 00:04:21.709
What changes do we want
to make to our model?

00:04:21.709 --> 00:04:23.410
Well, this is deep
learning after all.

00:04:23.410 --> 00:04:26.530
So the answer is always
to go deeper, right?

00:04:26.530 --> 00:04:30.190
But you may have been following
the recent literature in state

00:04:30.190 --> 00:04:32.470
of the art in that not
just sequential layers,

00:04:32.470 --> 00:04:35.320
but skip connections
or residual connections

00:04:35.320 --> 00:04:37.660
are a really good idea to
make sure your model continues

00:04:37.660 --> 00:04:39.260
to train effectively.

00:04:39.260 --> 00:04:42.880
So let's go through and
actually add an extra layer

00:04:42.880 --> 00:04:44.390
to our model.

00:04:44.390 --> 00:04:46.340
Let's add some skip connections.

00:04:46.340 --> 00:04:49.070
And we're going to do it all
right now in under 90 seconds.

00:04:49.070 --> 00:04:50.320
Are you ready?

00:04:50.320 --> 00:04:51.658
All right, here we go.

00:04:51.658 --> 00:04:53.200
So the first thing
that we want to do

00:04:53.200 --> 00:04:55.400
is we need to define
our additional layer.

00:04:55.400 --> 00:04:58.231
So we're going to fill
in this dense layer.

00:04:58.231 --> 00:04:59.173
Whoops.

00:04:59.173 --> 00:05:01.055
Flow.

00:05:01.055 --> 00:05:02.680
And one thing you
can see is that we're

00:05:02.680 --> 00:05:04.750
using Tab autocomplete
to help fill

00:05:04.750 --> 00:05:09.490
in code as we're trying to
develop and modify our model.

00:05:09.490 --> 00:05:12.820
Now, we're going to fix up
the shapes right here really

00:05:12.820 --> 00:05:17.512
quick, so that the residual
connections will all work.

00:05:17.512 --> 00:05:21.040
If I can type properly,
that would go better.

00:05:21.040 --> 00:05:21.790
All right, great.

00:05:21.790 --> 00:05:26.260
We have now defined our model
with the additional layers.

00:05:26.260 --> 00:05:28.522
All we need to do is
modify the forward pass,

00:05:28.522 --> 00:05:30.230
so that we add those
skipped connections.

00:05:30.230 --> 00:05:32.242
So here we go.

00:05:32.242 --> 00:05:33.700
The first thing we
need to do is we

00:05:33.700 --> 00:05:38.940
need to store in a
temporary variable

00:05:38.940 --> 00:05:40.780
the output of the
flattened layer.

00:05:40.780 --> 00:05:43.270
Then we're going to feed the
output of the flattened layer

00:05:43.270 --> 00:05:44.710
to our first dense layer.

00:05:44.710 --> 00:05:51.670
So dense.applied
to tmp in context.

00:05:51.670 --> 00:05:56.290
Now, for the coup de grace,
here is our residual connection.

00:05:56.290 --> 00:06:04.740
So dense2.applied to
tmp + tmp2 in context.

00:06:04.740 --> 00:06:05.320
Run that.

00:06:05.320 --> 00:06:07.980
And, yes, that works.

00:06:07.980 --> 00:06:10.620
We have now just
defined a new model

00:06:10.620 --> 00:06:12.330
that has residual
connections and is

00:06:12.330 --> 00:06:14.910
one additional layer deeper.

00:06:14.910 --> 00:06:16.180
Let's see how it does.

00:06:16.180 --> 00:06:18.510
So we're going to
reinstantiate our model

00:06:18.510 --> 00:06:20.120
and rerun the training loop.

00:06:20.120 --> 00:06:22.440
And if you recall from the
loss that we saw before,

00:06:22.440 --> 00:06:24.930
this one is now
substantially lower.

00:06:24.930 --> 00:06:26.523
This is great.

00:06:26.523 --> 00:06:28.565
This is an example of what
it's like to use Swift

00:06:28.565 --> 00:06:32.270
for TensorFlow to develop and
iterate as you apply models

00:06:32.270 --> 00:06:35.137
to applications and challenges.

00:06:35.137 --> 00:06:36.470
But Swift for TensorFlow-- thank

00:06:36.470 --> 00:06:37.584
[APPLAUSE]

00:06:41.320 --> 00:06:44.920
But Swift for TensorFlow was
designed for researchers.

00:06:44.920 --> 00:06:48.220
And researchers often need to
do more than just change models

00:06:48.220 --> 00:06:51.310
and change the way the
architecture fits together.

00:06:51.310 --> 00:06:53.230
Researchers often need
to define entirely

00:06:53.230 --> 00:06:56.230
new abstractions or layers.

00:06:56.230 --> 00:06:59.530
And so let's actually
see that live right now.

00:06:59.530 --> 00:07:03.520
Let's define a new custom layer.

00:07:03.520 --> 00:07:05.400
So let's say we had
the brilliant idea

00:07:05.400 --> 00:07:07.950
that we wanted to modify the
standard dense layer that

00:07:07.950 --> 00:07:09.510
takes a weights
and biases and we

00:07:09.510 --> 00:07:14.730
wanted to add an additional
bias set of parameters, OK?

00:07:14.730 --> 00:07:18.710
So we're going to define this
double bias dense layer right

00:07:18.710 --> 00:07:19.210
here.

00:07:19.210 --> 00:07:21.580
So I'm going to type
this really quickly.

00:07:21.580 --> 00:07:23.020
Stand by 15 seconds.

00:07:23.020 --> 00:07:24.679
Here we go.

00:07:24.679 --> 00:07:26.531
[LAUGHTER]

00:07:26.531 --> 00:07:29.510
Woo, all right, that was great.

00:07:29.510 --> 00:07:31.730
So let's actually
walk through the codes

00:07:31.730 --> 00:07:33.990
that you can see
what's going on.

00:07:33.990 --> 00:07:37.890
So the first thing that we have
is we define our parameters.

00:07:37.890 --> 00:07:40.610
So these are a W, like our
weights for our neurons,

00:07:40.610 --> 00:07:44.480
and B1, bias one, and
B2, our second bias.

00:07:44.480 --> 00:07:46.340
We defined an
initializer that takes

00:07:46.340 --> 00:07:50.600
an input size and an output
size just like dense does.

00:07:50.600 --> 00:07:54.412
We use that to initialize
our parameters.

00:07:54.412 --> 00:07:56.120
The forward pass is
very simple to write.

00:07:56.120 --> 00:07:58.370
So here's just applied to.

00:07:58.370 --> 00:08:01.070
And we just take the matrix
multiplication of input

00:08:01.070 --> 00:08:04.300
by our weights, and we
add in our bias terms.

00:08:04.300 --> 00:08:06.030
That's it.

00:08:06.030 --> 00:08:09.670
We've now just defined a
custom layer right in Colab

00:08:09.670 --> 00:08:12.000
in just a few lines of code.

00:08:12.000 --> 00:08:13.940
All right, let's
see how it goes.

00:08:13.940 --> 00:08:15.080
Here's model two.

00:08:15.080 --> 00:08:18.332
And so we're going to use
our double bias dense layer.

00:08:18.332 --> 00:08:19.790
And we're going to
instantiate and.

00:08:19.790 --> 00:08:22.040
We're going to train it
using, again, our custom

00:08:22.040 --> 00:08:24.543
handwritten training loop.

00:08:24.543 --> 00:08:26.210
Here's an example of
another way that we

00:08:26.210 --> 00:08:29.360
think Swift for TensorFlow
makes your life easier.

00:08:29.360 --> 00:08:32.520
Because Swift for TensorFlow can
statically analyze your code,

00:08:32.520 --> 00:08:34.730
it can be really helpful to you.

00:08:34.730 --> 00:08:38.280
I don't know about you, but I
regularly put typos in my code.

00:08:38.280 --> 00:08:39.890
I don't if you saw
me typing earlier.

00:08:39.890 --> 00:08:42.770
And Swift for TensorFlow here
is helping you out, right?

00:08:42.770 --> 00:08:46.550
It's saying, look, you
mistyped softmaxCrossEntropy.

00:08:46.550 --> 00:08:49.340
This should be labels, OK?

00:08:49.340 --> 00:08:50.480
All right, so we run it.

00:08:50.480 --> 00:08:51.470
We train it.

00:08:51.470 --> 00:08:53.990
And our loss isn't as good.

00:08:53.990 --> 00:08:55.680
This was not the right idea.

00:08:55.680 --> 00:08:57.170
But this is an
example of how easy

00:08:57.170 --> 00:09:00.950
it is for researchers to
experiment with new ideas

00:09:00.950 --> 00:09:05.730
really easily in
Swift for TensorFlow.

00:09:05.730 --> 00:09:08.600
But let's go deeper.

00:09:08.600 --> 00:09:12.060
Swift for TensorFlow is, again,
designed for researchers.

00:09:12.060 --> 00:09:15.670
And researchers need to be able
to customize everything, right?

00:09:15.670 --> 00:09:17.500
That's the whole
point of research.

00:09:17.500 --> 00:09:20.460
And so let's show
an example of how

00:09:20.460 --> 00:09:24.130
to customize something other
than just a model or a layer.

00:09:24.130 --> 00:09:28.720
So you may have heard that large
GPU clusters or TPU super pods

00:09:28.720 --> 00:09:32.460
are, like, delivering massive
breakthroughs in research

00:09:32.460 --> 00:09:35.690
and advancing the state of the
art in certain applications

00:09:35.690 --> 00:09:36.700
and domains.

00:09:36.700 --> 00:09:39.190
And you may have also
heard that, as you scale up

00:09:39.190 --> 00:09:42.760
to effectively utilize these
massive hardware pools,

00:09:42.760 --> 00:09:45.200
you need to increase
your batch size.

00:09:45.200 --> 00:09:46.880
And so let's say
you're a researcher,

00:09:46.880 --> 00:09:48.650
and you want to
try and figure out

00:09:48.650 --> 00:09:52.010
what are the best ways to train
deep neural networks at larger

00:09:52.010 --> 00:09:54.410
batch sizes.

00:09:54.410 --> 00:09:56.450
Well, if you're a
researcher, you probably

00:09:56.450 --> 00:09:59.870
can't buy a whole GPU cluster or
rent a whole TPU super pod all

00:09:59.870 --> 00:10:01.940
the time for your experiments.

00:10:01.940 --> 00:10:04.010
But you often have a
GPU under your desk.

00:10:04.010 --> 00:10:07.880
So let's see how we can simulate
running on a super large data

00:10:07.880 --> 00:10:13.210
parallel GPU or TPU cluster
on a single machine.

00:10:13.210 --> 00:10:17.020
We're going to do it all in a
few lines of code right here.

00:10:17.020 --> 00:10:19.687
So here's our custom
training loop.

00:10:19.687 --> 00:10:21.270
Well, here's the
standard part, right?

00:10:21.270 --> 00:10:24.150
This is 1 to 10 training epics.

00:10:24.150 --> 00:10:27.210
And what we're going to do
is, instead of just applying

00:10:27.210 --> 00:10:30.690
our model forward once, we
have an additional inner loop,

00:10:30.690 --> 00:10:31.230
right?

00:10:31.230 --> 00:10:33.510
So we're going to
run our forward pass.

00:10:33.510 --> 00:10:35.700
We're going to run our model--

00:10:35.700 --> 00:10:38.220
whoops-- four times.

00:10:38.220 --> 00:10:40.560
And we're going to take the
gradients for each step.

00:10:40.560 --> 00:10:43.650
And we're going to aggregate
them in this grads variable.

00:10:43.650 --> 00:10:44.340
OK?

00:10:44.340 --> 00:10:48.090
This simulates running on four
independent accelerators, four

00:10:48.090 --> 00:10:51.540
GPUs or four TPUs in a
data parallel fashion

00:10:51.540 --> 00:10:53.640
on a batch that's actually
four times as large

00:10:53.640 --> 00:10:55.248
as what we actually run.

00:10:55.248 --> 00:10:56.790
We're going to then
use our optimizer

00:10:56.790 --> 00:11:00.450
to update our model along
these aggregated gradients,

00:11:00.450 --> 00:11:05.200
again simulating a data parallel
synchronous training process.

00:11:05.200 --> 00:11:06.440
That's it.

00:11:06.440 --> 00:11:07.850
That's all there is to it.

00:11:07.850 --> 00:11:09.850
We're really excited by
this sort of flexibility

00:11:09.850 --> 00:11:12.010
and capabilities that
Swift for TensorFlow

00:11:12.010 --> 00:11:13.750
brings to researchers.

00:11:13.750 --> 00:11:15.160
Back over to you, Chris.

00:11:15.160 --> 00:11:16.452
CHRIS LATTNER: Thanks, Brennan.

00:11:16.452 --> 00:11:17.094
[APPLAUSE]

00:11:21.490 --> 00:11:24.820
So I think that the focus on
catching errors early and also

00:11:24.820 --> 00:11:27.070
productivity enhancements
like code completion

00:11:27.070 --> 00:11:28.380
can help you in a lot of ways.

00:11:28.380 --> 00:11:32.080
And it's not just about, like,
automating typing of code.

00:11:32.080 --> 00:11:35.448
But it can also be
about discovery of APIs.

00:11:35.448 --> 00:11:37.990
So another thing that's really
cool about Swift as a language

00:11:37.990 --> 00:11:40.960
is that it has really good
interoperability with C code.

00:11:40.960 --> 00:11:42.790
And so in Swift, you
can literally just

00:11:42.790 --> 00:11:46.000
import a C header file
and call symbols directly

00:11:46.000 --> 00:11:48.910
from C without wrappers,
without boilerplate or anything

00:11:48.910 --> 00:11:49.810
involved.

00:11:49.810 --> 00:11:51.290
It just works.

00:11:51.290 --> 00:11:52.630
So we've taken this approach.

00:11:52.630 --> 00:11:54.730
In the TensorFlow team,
we've taken this approach

00:11:54.730 --> 00:11:56.395
and brought it to
the world of Python.

00:11:56.395 --> 00:11:58.030
And one of the cool
things about this

00:11:58.030 --> 00:12:00.290
is that that allows you to
combine the power of Swift

00:12:00.290 --> 00:12:03.250
for TensorFlow with all the
advantages of the Python

00:12:03.250 --> 00:12:04.218
ecosystem.

00:12:04.218 --> 00:12:05.260
How about we take a look?

00:12:08.360 --> 00:12:11.150
BRENNAN SAETA: Thanks, Chris.

00:12:11.150 --> 00:12:13.100
The Python data
science ecosystem

00:12:13.100 --> 00:12:15.500
is incredibly
powerful and vibrant.

00:12:15.500 --> 00:12:17.390
And we wanted to make
sure that, as you start

00:12:17.390 --> 00:12:19.220
using Swift for
TensorFlow, you didn't

00:12:19.220 --> 00:12:21.620
miss all your favorite
libraries and utilities that you

00:12:21.620 --> 00:12:23.670
were used to.

00:12:23.670 --> 00:12:26.600
And so we've built a seamless
Python interoperability

00:12:26.600 --> 00:12:28.672
capability to Swift
for TensorFlow.

00:12:28.672 --> 00:12:30.380
And let's see how it
works in the context

00:12:30.380 --> 00:12:34.325
of my favorite Python data
science library, NumPy.

00:12:34.325 --> 00:12:35.700
So the first thing
you need to do

00:12:35.700 --> 00:12:38.125
is import TensorFlow
and import Python.

00:12:38.125 --> 00:12:41.340
And once you do that, that
defines this Python object

00:12:41.340 --> 00:12:44.430
that allows you to import
arbitrary Python libraries.

00:12:44.430 --> 00:12:48.390
So here we import pyplot
from the matplotlib library

00:12:48.390 --> 00:12:49.200
and NumPy.

00:12:49.200 --> 00:12:52.770
And we assign it to np, OK?

00:12:52.770 --> 00:12:56.220
After that, we can just use np
just as if we were in Python.

00:12:56.220 --> 00:12:57.660
So, here, we call linspace.

00:12:57.660 --> 00:12:59.835
We're going to call
sine and cosine.

00:12:59.835 --> 00:13:03.100
And we're going to pass
those values to pyplot.

00:13:03.100 --> 00:13:08.580
When we run the cell, it just
works exactly as you'd expect.

00:13:08.580 --> 00:13:09.560
[APPLAUSE]

00:13:09.560 --> 00:13:10.540
Thank you.

00:13:10.540 --> 00:13:13.480
[APPLAUSE]

00:13:13.480 --> 00:13:15.780
Now, this sort of kind of
looks like the Python code

00:13:15.780 --> 00:13:19.380
you're used to writing, but
this is actually pure Swift.

00:13:19.380 --> 00:13:21.690
It just works seamlessly.

00:13:21.690 --> 00:13:23.500
But this is maybe a
bit of a toy example.

00:13:23.500 --> 00:13:25.458
So let's see this a little
bit more in context.

00:13:28.340 --> 00:13:31.520
OpenAI has done a lot
of work in the area

00:13:31.520 --> 00:13:32.838
of reinforcement learning.

00:13:32.838 --> 00:13:34.380
And in order to help
that along, they

00:13:34.380 --> 00:13:39.140
developed a Python
library called OpenAI Gym.

00:13:39.140 --> 00:13:42.585
Gym contains a collection
of environments

00:13:42.585 --> 00:13:44.210
that are very useful
when you're trying

00:13:44.210 --> 00:13:46.190
to train a reinforcement
learning agent

00:13:46.190 --> 00:13:48.920
across a variety of
different challenges.

00:13:48.920 --> 00:13:52.310
Let's use OpenAI Gym to train
a reinforcement learning

00:13:52.310 --> 00:13:56.420
agent in Swift for TensorFlow
right now our browsers.

00:13:56.420 --> 00:14:00.020
So the first thing we need to
do is we need to import Gym.

00:14:00.020 --> 00:14:02.570
We're going to define a
few hyperparameters here.

00:14:02.570 --> 00:14:04.710
And, now, we define
our neural network.

00:14:04.710 --> 00:14:06.085
In this case,
we're going to pick

00:14:06.085 --> 00:14:08.360
a simple two-layer
dense network.

00:14:08.360 --> 00:14:11.900
And it's just a
sequential model, OK?

00:14:11.900 --> 00:14:13.400
After that, we have
some helper code

00:14:13.400 --> 00:14:17.060
to filter out bad or short
episodes and whatnot.

00:14:17.060 --> 00:14:20.560
But here's the real meat of it.

00:14:20.560 --> 00:14:22.360
We're going to use
Gym to instantiate

00:14:22.360 --> 00:14:25.360
the CartPole v0 environment.

00:14:25.360 --> 00:14:26.500
So that's our env.

00:14:26.500 --> 00:14:28.690
We're going to then
instantiate our network right

00:14:28.690 --> 00:14:30.250
here and our optimizer.

00:14:30.250 --> 00:14:32.170
And here's our training loop.

00:14:32.170 --> 00:14:33.320
There we go.

00:14:33.320 --> 00:14:35.530
So we're going to get
a bunch of episodes.

00:14:35.530 --> 00:14:38.180
We're going to run our
model, get the gradients.

00:14:38.180 --> 00:14:40.180
And we're going to apply
those to our optimizer.

00:14:40.180 --> 00:14:43.990
And we're going to record the
mean rewards as we train, OK?

00:14:43.990 --> 00:14:46.270
It's all very simple,
straightforward Swift.

00:14:46.270 --> 00:14:49.840
And here you can see us training
a Swift for TensorFlow model

00:14:49.840 --> 00:14:54.120
in an OpenAI Gym environment
using the Python bridge,

00:14:54.120 --> 00:14:55.863
totally seamless.

00:14:55.863 --> 00:14:57.280
And of course,
afterwards, you can

00:14:57.280 --> 00:15:01.930
keep track of the
parameters of the rewards.

00:15:01.930 --> 00:15:04.060
In this case, we're going
to plot the mean rewards

00:15:04.060 --> 00:15:08.860
as the model trained using
Python NumPy, totally seamless.

00:15:08.860 --> 00:15:11.080
You can get started using
Swift for TensorFlow using

00:15:11.080 --> 00:15:12.850
all the libraries
you know and love

00:15:12.850 --> 00:15:16.510
and take advantage of
what Swift for TensorFlow

00:15:16.510 --> 00:15:17.860
brings to the table.

00:15:17.860 --> 00:15:19.088
Back over to you, Chris.

00:15:19.088 --> 00:15:20.380
CHRIS LATTNER: Thanks, Brennan.

00:15:20.380 --> 00:15:22.505
So one of the things that
I love about this is it's

00:15:22.505 --> 00:15:24.730
not just about being
able to leverage

00:15:24.730 --> 00:15:26.800
big important
libraries like NumPy.

00:15:26.800 --> 00:15:29.770
We're working on the ability to
integrate Swift for TensorFlow

00:15:29.770 --> 00:15:31.850
and Python for
TensorFlow code together,

00:15:31.850 --> 00:15:34.090
which we think will
provide a nice transition

00:15:34.090 --> 00:15:37.090
path to make you able to
incrementally move code

00:15:37.090 --> 00:15:39.420
from one world to the other.

00:15:39.420 --> 00:15:42.940
Now, I think it's fair
to say that calculus

00:15:42.940 --> 00:15:45.080
is an integral part
of machine learning.

00:15:45.080 --> 00:15:45.580
[LAUGHTER]

00:15:45.580 --> 00:15:48.640
And we think that
differentiable programming

00:15:48.640 --> 00:15:51.880
is so important that we've built
it right into the language.

00:15:51.880 --> 00:15:53.660
This has a number
of huge advantages,

00:15:53.660 --> 00:15:56.530
including enabling more
flexible and custom work

00:15:56.530 --> 00:16:01.010
with differentiables,
with derivatives.

00:16:01.010 --> 00:16:02.710
And we think this
is really cool.

00:16:02.710 --> 00:16:06.543
So I'd like to take a look.

00:16:06.543 --> 00:16:07.960
BRENNAN SAETA: So
we've been using

00:16:07.960 --> 00:16:09.918
Swift for TensorFlow's
differential programming

00:16:09.918 --> 00:16:12.350
capabilities throughout
all of our demos so far.

00:16:12.350 --> 00:16:15.010
But let's really break it
down and see what's going on

00:16:15.010 --> 00:16:17.090
at a fundamental level.

00:16:17.090 --> 00:16:19.390
So here we define
my function that

00:16:19.390 --> 00:16:24.640
takes two doubles and returns a
double based on some products,

00:16:24.640 --> 00:16:27.720
and sums, and quotients.

00:16:27.720 --> 00:16:30.810
If we want Swift for TensorFlow
to automatically compute

00:16:30.810 --> 00:16:36.870
the derivative for us, we just
annotate it at differential.

00:16:36.870 --> 00:16:40.200
Swift for TensorFlow will
then derive the derivative

00:16:40.200 --> 00:16:43.890
for this function right
when we run the cell.

00:16:43.890 --> 00:16:48.340
To use this autogenerated
derivative, use gradient.

00:16:48.340 --> 00:16:49.840
So gradient takes two things.

00:16:49.840 --> 00:16:53.490
It takes a closure to evaluate
and a point that you want

00:16:53.490 --> 00:16:56.360
to evaluate your closure at.

00:16:56.360 --> 00:16:58.020
So here we go.

00:16:58.020 --> 00:17:01.610
This is what it is to take
the derivative of a function

00:17:01.610 --> 00:17:02.730
at a particular point.

00:17:02.730 --> 00:17:04.460
So we can change it surround.

00:17:04.460 --> 00:17:07.490
This one's my
favorite tasty number.

00:17:07.490 --> 00:17:09.380
And that works nicely.

00:17:09.380 --> 00:17:11.270
Now, one thing to
note, we've just

00:17:11.270 --> 00:17:14.060
been taking the partial
derivatives of my function

00:17:14.060 --> 00:17:15.650
with respect to a.

00:17:15.650 --> 00:17:17.960
But, of course, you can
take the partial derivatives

00:17:17.960 --> 00:17:22.880
and get a full gradient
of my function, like so.

00:17:22.880 --> 00:17:24.859
Often with neural
networks, however, you

00:17:24.859 --> 00:17:27.829
want to get not just the
gradients for your network

00:17:27.829 --> 00:17:30.380
as you're trying to train it
and optimize your loss function.

00:17:30.380 --> 00:17:32.780
You often want what the
network predicted, right?

00:17:32.780 --> 00:17:36.470
This is really useful to compute
accuracy or other debugging

00:17:36.470 --> 00:17:37.550
sort of information.

00:17:37.550 --> 00:17:40.640
And for that you can
use value with gradient.

00:17:40.640 --> 00:17:43.520
And that returns
a tuple containing

00:17:43.520 --> 00:17:46.100
both the value and the
gradient, shockingly enough.

00:17:46.100 --> 00:17:48.980
Now, one thing to
note, in Swift,

00:17:48.980 --> 00:17:50.990
tuples can actually
have named parameters.

00:17:50.990 --> 00:17:52.560
They aren't just ordered.

00:17:52.560 --> 00:17:56.150
And so you can actually see that
it prints out really nicely.

00:17:56.150 --> 00:17:57.320
And you can access values.

00:17:57.320 --> 00:18:00.920
We think this is, again, another
nice little thing that helps

00:18:00.920 --> 00:18:03.500
makes writing and debugging
code and, more importantly,

00:18:03.500 --> 00:18:07.560
reading it and understanding
it later a little bit easier.

00:18:07.560 --> 00:18:10.130
But the one thing that
I want to call out

00:18:10.130 --> 00:18:13.760
is that throughout this we've
been using just normal types.

00:18:13.760 --> 00:18:16.040
These aren't tensor
of something.

00:18:16.040 --> 00:18:18.750
It's just plain old double.

00:18:18.750 --> 00:18:20.960
This is because
automatic differentiation

00:18:20.960 --> 00:18:23.600
is built right into the language
in Swift for TensorFlow.

00:18:23.600 --> 00:18:26.540
It makes it really easy
to express your thoughts

00:18:26.540 --> 00:18:29.160
very clearly.

00:18:29.160 --> 00:18:31.080
But even though it's
built into the language,

00:18:31.080 --> 00:18:33.210
we've actually worked
very hard to make sure

00:18:33.210 --> 00:18:36.510
that automatic differentiation
is totally flexible so that you

00:18:36.510 --> 00:18:40.050
can customize it to
whatever it needs you have.

00:18:40.050 --> 00:18:44.370
And instead of telling you
about that, let's show you.

00:18:44.370 --> 00:18:48.200
So let's say you want to
define an algebra in 2D space.

00:18:48.200 --> 00:18:51.870
Well, you're certainly going
to need a point data type.

00:18:51.870 --> 00:18:54.140
So here we define a point
struct with x and y.

00:18:54.140 --> 00:18:57.110
And we just market
differentiable.

00:18:57.110 --> 00:19:01.010
We can define helper functions
on it like dot or other helper

00:19:01.010 --> 00:19:02.070
functions.

00:19:02.070 --> 00:19:04.920
And Swift for TensorFlow, when
you try and use your code,

00:19:04.920 --> 00:19:06.950
will often
automatically infer when

00:19:06.950 --> 00:19:10.520
you need gradients to be
automatically computed

00:19:10.520 --> 00:19:13.300
for you by the compiler.

00:19:13.300 --> 00:19:17.060
But often, it's a good idea
to document your intentions.

00:19:17.060 --> 00:19:19.990
And so you can annotate
your helper functions

00:19:19.990 --> 00:19:23.810
as @differentiable.

00:19:23.810 --> 00:19:25.820
The other reason why
we recommend doing this

00:19:25.820 --> 00:19:28.226
is because this
helps catch errors.

00:19:28.226 --> 00:19:31.490
So here, Swift for
TensorFlow is actually

00:19:31.490 --> 00:19:35.510
telling you that, hey, you can
only differentiate functions

00:19:35.510 --> 00:19:39.140
that return values that
conform to differentiable.

00:19:39.140 --> 00:19:41.780
But int doesn't conform
to differentiable, right?

00:19:41.780 --> 00:19:44.360
What this is telling you
is that my helper function

00:19:44.360 --> 00:19:45.190
returns an int.

00:19:45.190 --> 00:19:48.230
And int is all about taking
infinitesimally small steps

00:19:48.230 --> 00:19:50.660
as you optimize and
take gradients, right?

00:19:50.660 --> 00:19:53.600
And integers just
are very discrete.

00:19:53.600 --> 00:19:57.500
And so Swift for TensorFlow
is helping to catch errors,

00:19:57.500 --> 00:19:59.930
you know, right when you
write the code very easily

00:19:59.930 --> 00:20:01.952
and tell you what's going on.

00:20:01.952 --> 00:20:03.410
So the solution,
of course, is just

00:20:03.410 --> 00:20:05.810
to not mark that
as @differentiable.

00:20:05.810 --> 00:20:06.940
The cell runs just fine.

00:20:09.650 --> 00:20:13.580
But let's say we also wanted to
go beyond just defining the dot

00:20:13.580 --> 00:20:14.080
product.

00:20:14.080 --> 00:20:17.195
Let's say we also wanted to
define the magnitude helper

00:20:17.195 --> 00:20:17.695
function.

00:20:20.320 --> 00:20:22.900
That is the magnitude of the
vector defined by the origin

00:20:22.900 --> 00:20:25.640
to the point in question.

00:20:25.640 --> 00:20:28.445
So to do that, we can
use the distance formula

00:20:28.445 --> 00:20:30.200
if you're going to do
Euclidean distance.

00:20:30.200 --> 00:20:35.240
And we can define an extension
on point that does this.

00:20:35.240 --> 00:20:37.240
But we're going to
pretend for a moment

00:20:37.240 --> 00:20:40.780
that Swift doesn't include
a square root function,

00:20:40.780 --> 00:20:43.090
because I want a
good excuse for you

00:20:43.090 --> 00:20:46.480
to see the interoperability
with C, OK?

00:20:46.480 --> 00:20:50.290
So we're actually going to use
C's square root function that

00:20:50.290 --> 00:20:53.170
operates on doubles, OK?

00:20:53.170 --> 00:20:55.780
So based on the definition
of Euclidean distance,

00:20:55.780 --> 00:20:57.310
we can define the magnitude.

00:20:57.310 --> 00:20:59.240
And it totally just--

00:20:59.240 --> 00:21:00.390
no, it doesn't quite work.

00:21:00.390 --> 00:21:00.890
OK.

00:21:00.890 --> 00:21:03.410
Let's see what's going on.

00:21:03.410 --> 00:21:05.910
So we wanted magnitude
to be differentiable.

00:21:05.910 --> 00:21:08.410
And it's saying that you can't
differentiate the square root

00:21:08.410 --> 00:21:11.800
function, because this is an
external function that hasn't

00:21:11.800 --> 00:21:13.460
been marked as differentiable.

00:21:13.460 --> 00:21:14.050
OK.

00:21:14.050 --> 00:21:15.350
What's that saying?

00:21:15.350 --> 00:21:17.380
Well, the square root,
it's a C function.

00:21:17.380 --> 00:21:19.330
It was compiled
by the C compiler.

00:21:19.330 --> 00:21:22.090
And as of today, the C
compiler can't automatically

00:21:22.090 --> 00:21:23.900
compute derivatives for you.

00:21:23.900 --> 00:21:26.260
So Swift for TensorFlow
is saying like, hey, this

00:21:26.260 --> 00:21:28.072
isn't going to work.

00:21:28.072 --> 00:21:30.280
This is excellent, because
it gives me a great excuse

00:21:30.280 --> 00:21:32.560
to show you how to
write custom gradients.

00:21:32.560 --> 00:21:38.050
All right, so here we
define a wrapper function,

00:21:38.050 --> 00:21:39.970
mySqrt square root,
that just calls down

00:21:39.970 --> 00:21:43.990
in the forward pass to the
C square root function.

00:21:43.990 --> 00:21:47.380
In the backwards pass,
we take our double

00:21:47.380 --> 00:21:50.170
and we return a
tuple of two values.

00:21:50.170 --> 00:21:52.900
Rather, the first
element in the tuple

00:21:52.900 --> 00:21:55.980
is the normal value
in the forward pass.

00:21:55.980 --> 00:21:59.780
And the second is
a pullback closure.

00:21:59.780 --> 00:22:02.590
And this is where you
define the backwards pass

00:22:02.590 --> 00:22:05.790
capturing whatever values you
need from the forward pass.

00:22:05.790 --> 00:22:06.820
OK?

00:22:06.820 --> 00:22:08.200
So we're going to run that.

00:22:08.200 --> 00:22:10.780
We're going to go back up to
our definition of magnitude

00:22:10.780 --> 00:22:13.680
and change it from square
root to my square root,

00:22:13.680 --> 00:22:17.330
rerun the cell, and it works.

00:22:17.330 --> 00:22:19.750
We've now defined
point and two methods

00:22:19.750 --> 00:22:21.250
on it, dot and magnitude.

00:22:21.250 --> 00:22:23.410
And we can now combine
these in arbitrary

00:22:23.410 --> 00:22:25.850
other silly
differentiable functions.

00:22:25.850 --> 00:22:28.690
So, here, I've defined
the silly function.

00:22:28.690 --> 00:22:30.760
And we've marked it
as differentiable.

00:22:30.760 --> 00:22:32.922
And we're going to
take two points.

00:22:32.922 --> 00:22:34.630
We're also going to
take a double, right?

00:22:34.630 --> 00:22:37.450
You can mix and match
differentiable data types

00:22:37.450 --> 00:22:38.375
totally fluidly.

00:22:38.375 --> 00:22:40.000
We're going to return
double, and we're

00:22:40.000 --> 00:22:42.340
going to take magnitudes
and do dot products.

00:22:42.340 --> 00:22:44.800
It's a silly function after all.

00:22:44.800 --> 00:22:49.720
And we can then use it, compute
the gradient of this function

00:22:49.720 --> 00:22:52.390
at arbitrary data points.

00:22:52.390 --> 00:22:57.040
Just like you'd expect, you can
get the value of the function,

00:22:57.040 --> 00:22:59.680
get full gradients in addition
to partial derivatives

00:22:59.680 --> 00:23:02.950
with respect to
individual values.

00:23:02.950 --> 00:23:04.630
That's been a quick
run through of how

00:23:04.630 --> 00:23:08.260
to use customization, custom
gradients, custom data

00:23:08.260 --> 00:23:11.740
types with a language integrated
automatic differentiation built

00:23:11.740 --> 00:23:14.050
into Swift for TensorFlow.

00:23:14.050 --> 00:23:16.310
But let's go one step further.

00:23:16.310 --> 00:23:18.010
Let's put all this
together and show

00:23:18.010 --> 00:23:20.350
how you can write
your own debuggers

00:23:20.350 --> 00:23:25.550
as an example of how this
power is all in your hands.

00:23:25.550 --> 00:23:30.353
So often when you're
debugging models,

00:23:30.353 --> 00:23:32.270
you often want to be
able to see the gradients

00:23:32.270 --> 00:23:34.910
at different points
within your model.

00:23:34.910 --> 00:23:37.760
And so here, we can just
define in regular Swift code

00:23:37.760 --> 00:23:39.600
a gradient debugger.

00:23:39.600 --> 00:23:43.547
Now, it's going to
take as input a double.

00:23:43.547 --> 00:23:45.380
And it's going to return
it just like normal

00:23:45.380 --> 00:23:46.547
for the forward pass, right?

00:23:46.547 --> 00:23:47.938
It's an identity function.

00:23:47.938 --> 00:23:50.230
On the backwards pass, we're
going to get the gradient.

00:23:50.230 --> 00:23:51.620
We're going to
print the gradient.

00:23:51.620 --> 00:23:53.037
And then we're
going to return it.

00:23:53.037 --> 00:23:55.530
So we're just passing it
through just printing it out.

00:23:55.530 --> 00:23:58.290
Now that we've defined this
gradient debugger ourselves,

00:23:58.290 --> 00:24:00.060
we can use it in
our silly function

00:24:00.060 --> 00:24:03.990
to see what's going on
as we take derivatives.

00:24:03.990 --> 00:24:10.650
So gradient debugger,
there we go.

00:24:10.650 --> 00:24:12.150
We can rerun that.

00:24:12.150 --> 00:24:16.830
And when we take the gradients,
we can now see that for that

00:24:16.830 --> 00:24:24.130
point in the silly function of
a dot b, the gradient is 3.80.

00:24:24.130 --> 00:24:26.980
That's been a brief tour through
how automatic differentiation

00:24:26.980 --> 00:24:30.820
works in Swift for TensorFlow
and how it's customizable

00:24:30.820 --> 00:24:33.940
so that you can harness the
power in whatever abstractions

00:24:33.940 --> 00:24:36.070
or systems you need to build.

00:24:36.070 --> 00:24:37.270
Back over to you, Chris.

00:24:37.270 --> 00:24:38.562
CHRIS LATTNER: Thanks, Brennan.

00:24:38.562 --> 00:24:39.126
[APPLAUSE]

00:24:42.135 --> 00:24:43.510
So the funny thing
about all this

00:24:43.510 --> 00:24:45.385
is that the algorithms
that we're building on

00:24:45.385 --> 00:24:47.300
were defined back in the 1970s.

00:24:47.300 --> 00:24:49.120
And so it really took
language integration

00:24:49.120 --> 00:24:50.787
to be able to bring
these things forward

00:24:50.787 --> 00:24:53.400
into the world of
machine learning.

00:24:53.400 --> 00:24:55.150
There's a tremendous
amount of depth here.

00:24:55.150 --> 00:24:57.170
And I'm really excited to see
what you all can do with it.

00:24:57.170 --> 00:24:59.440
And we think that this is going
to enable new kinds of research

00:24:59.440 --> 00:25:00.983
which we're very excited about.

00:25:00.983 --> 00:25:02.150
There's also a ton of depth.

00:25:02.150 --> 00:25:03.160
And if you're interested
in learning more,

00:25:03.160 --> 00:25:04.993
we have a bunch of
detailed design documents

00:25:04.993 --> 00:25:06.700
available online.

00:25:06.700 --> 00:25:08.795
Let's talk about
performance a little bit.

00:25:08.795 --> 00:25:10.087
Now, Swift is fast.

00:25:10.087 --> 00:25:12.670
And this comes from a number of
different things, one of which

00:25:12.670 --> 00:25:14.878
is that the language itself
has really good low level

00:25:14.878 --> 00:25:16.100
performance.

00:25:16.100 --> 00:25:19.473
There's also no GIL to get
in the way of concurrency.

00:25:19.473 --> 00:25:21.640
Swift for TensorFlow also
has some advanced compiler

00:25:21.640 --> 00:25:24.130
techniques to automatically
identify graphs for you

00:25:24.130 --> 00:25:25.370
and extract them.

00:25:25.370 --> 00:25:27.070
So you don't have
to think about that.

00:25:27.070 --> 00:25:28.570
The consequence of
all this together

00:25:28.570 --> 00:25:30.153
is that we think
Swift has the world's

00:25:30.153 --> 00:25:32.017
most advanced eager mode.

00:25:32.017 --> 00:25:34.100
Now, you may not care that
much about performance.

00:25:34.100 --> 00:25:37.305
You may wonder, like, why
do we care about this stuff?

00:25:37.305 --> 00:25:38.680
Well, we're seeing
various trends

00:25:38.680 --> 00:25:41.020
in the industry where people
are defining neural nets

00:25:41.020 --> 00:25:44.770
and then want to integrate them
into other larger applications.

00:25:44.770 --> 00:25:48.040
And typically what this requires
is this requires you to export

00:25:48.040 --> 00:25:51.220
graphs and then write a
bunch of C++ code to load

00:25:51.220 --> 00:25:53.810
and orchestrate them
in various ways.

00:25:53.810 --> 00:25:56.050
So let's take a look
at an example of this.

00:25:56.050 --> 00:25:59.380
AlphaGo Zero is
really impressive work

00:25:59.380 --> 00:26:02.740
that combines three major
classes of techniques.

00:26:02.740 --> 00:26:04.990
Of course, you have deep
learning on the one hand.

00:26:04.990 --> 00:26:08.950
But it also drives it through
Monte Carlo tracers to actually

00:26:08.950 --> 00:26:11.080
find and evaluate these spaces.

00:26:11.080 --> 00:26:13.180
And then it runs them
at scale on industry

00:26:13.180 --> 00:26:15.017
leading TPU accelerators.

00:26:15.017 --> 00:26:17.350
And so it's the combination
of all three of these things

00:26:17.350 --> 00:26:19.600
that make AlphaGo Zero possible.

00:26:19.600 --> 00:26:22.090
Now, this is possible today.

00:26:22.090 --> 00:26:24.400
And if you're an advanced
team like DeepMind,

00:26:24.400 --> 00:26:25.472
you can totally do this.

00:26:25.472 --> 00:26:27.430
But it's much more
difficult than it should be.

00:26:27.430 --> 00:26:29.513
And we think that breaking
down barriers like this

00:26:29.513 --> 00:26:31.900
can lead to new
breakthroughs in science.

00:26:31.900 --> 00:26:34.637
And we think that this is what
can drive progress forward.

00:26:34.637 --> 00:26:36.970
So instead of talking about
it again, let's take a look.

00:26:39.598 --> 00:26:41.850
BRENNAN SAETA: MiniGo
is an open source go

00:26:41.850 --> 00:26:46.020
player inspired by DeepMind's
AlphaGo Zero project.

00:26:46.020 --> 00:26:47.132
It's available on GitHub.

00:26:47.132 --> 00:26:48.840
And you can certainly
check out the code.

00:26:48.840 --> 00:26:49.890
And I encourage you to.

00:26:49.890 --> 00:26:51.910
They're also going to be here.

00:26:51.910 --> 00:26:55.413
And they have some other
presentations tomorrow.

00:26:55.413 --> 00:26:57.330
But the MiniGo project,
when they started out,

00:26:57.330 --> 00:26:59.413
they were getting everything
in normal TensorFlow.

00:26:59.413 --> 00:27:01.040
And it was working
great until they

00:27:01.040 --> 00:27:05.770
started trying to run at scale
on large clusters of TPUs.

00:27:05.770 --> 00:27:08.480
There, they ran into performance
problems and had to rewrite

00:27:08.480 --> 00:27:12.140
things like Monte Carlo tree
search into C++ in order

00:27:12.140 --> 00:27:15.220
to effectively utilize
modern accelerators.

00:27:15.220 --> 00:27:18.650
Here, we've reimplemented
Monte Carlo tree

00:27:18.650 --> 00:27:25.280
search and the rest of the
MiniGo self-play in pure Swift.

00:27:25.280 --> 00:27:29.540
And we're going to let you see
it running right here in Colab.

00:27:29.540 --> 00:27:31.220
So here we define
a helper function

00:27:31.220 --> 00:27:32.870
where we take in a
game configuration

00:27:32.870 --> 00:27:34.340
and a couple of participants.

00:27:34.340 --> 00:27:36.710
These are our white
and black players.

00:27:36.710 --> 00:27:39.590
And we're going to
run, basically, play

00:27:39.590 --> 00:27:43.520
the game until we have
a winner or loser.

00:27:43.520 --> 00:27:45.368
And so let's actually run this.

00:27:45.368 --> 00:27:46.910
Here, we define a
game configuration.

00:27:46.910 --> 00:27:50.480
We're going to play between a
Monte Carlo tree search powered

00:27:50.480 --> 00:27:53.660
by neural networks versus
just a random player just

00:27:53.660 --> 00:27:56.330
to see how easy it is
to flip back and forth

00:27:56.330 --> 00:27:58.630
or mix and match
between deep learning

00:27:58.630 --> 00:28:03.590
and other arbitrary
machine learning algorithms

00:28:03.590 --> 00:28:05.570
right here in Swift.

00:28:05.570 --> 00:28:06.840
So here you go.

00:28:06.840 --> 00:28:10.580
You can see them playing white,
black, playing different moves

00:28:10.580 --> 00:28:12.110
back and forth.

00:28:12.110 --> 00:28:14.150
And it just goes.

00:28:14.150 --> 00:28:16.400
We think that Swift for
TensorFlow is going to unlock

00:28:16.400 --> 00:28:19.160
whole new classes of
algorithms and research,

00:28:19.160 --> 00:28:22.340
because of how easy it is to do
everything in one language with

00:28:22.340 --> 00:28:25.690
no barriers, no having to
rewrite things into C++.

00:28:25.690 --> 00:28:26.690
Back over to you, Chris.

00:28:26.690 --> 00:28:28.248
CHRIS LATTNER: Thank, Brennan.

00:28:28.248 --> 00:28:29.790
The cool thing about
this, of course,

00:28:29.790 --> 00:28:31.332
is that you can
actually do something

00:28:31.332 --> 00:28:34.610
like this in a workbook,
which is pretty phenomenal.

00:28:34.610 --> 00:28:37.310
And we've seen many different
families of new techniques

00:28:37.310 --> 00:28:40.410
that can be combined together
and fused in different ways.

00:28:40.410 --> 00:28:42.410
And bringing this to
more people we think

00:28:42.410 --> 00:28:45.530
will lead to new kinds
of interesting research.

00:28:45.530 --> 00:28:47.930
Now, our work on
usability and design

00:28:47.930 --> 00:28:50.240
is not just about
high-end researchers.

00:28:50.240 --> 00:28:53.030
So we love them,
but Swift is also

00:28:53.030 --> 00:28:56.210
widely used to teach new
programmers how to code.

00:28:56.210 --> 00:28:58.200
And education is very
close to our hearts.

00:28:58.200 --> 00:29:00.560
And so I'm very excited to
announce a collaboration

00:29:00.560 --> 00:29:03.590
that we're embarking on with
none other than Jeremy Howard.

00:29:03.590 --> 00:29:05.810
But instead of
talking about this,

00:29:05.810 --> 00:29:09.595
I'd rather have Jeremy
speak about it now.

00:29:09.595 --> 00:29:11.220
JEREMY HOWARD: At
Fast.AI, we're always

00:29:11.220 --> 00:29:12.887
looking to push the
boundaries of what's

00:29:12.887 --> 00:29:15.210
possible with deep
learning, especially

00:29:15.210 --> 00:29:18.820
pushing to make recent
advances more accessible.

00:29:18.820 --> 00:29:20.820
We've been involved with
setting image net speed

00:29:20.820 --> 00:29:24.690
records at a cost of just $25
and building the world's best

00:29:24.690 --> 00:29:26.482
document classifier.

00:29:26.482 --> 00:29:28.440
Hundreds of thousands
have become deep learning

00:29:28.440 --> 00:29:30.300
practitioners
through our courses

00:29:30.300 --> 00:29:33.990
and are producing state of the
art results with our library.

00:29:33.990 --> 00:29:36.390
We think that with
Swift for TensorFlow,

00:29:36.390 --> 00:29:37.840
we can go even further.

00:29:37.840 --> 00:29:41.490
So we're announcing today that
our next course will include

00:29:41.490 --> 00:29:44.340
a big Swift component
co-taught by someone

00:29:44.340 --> 00:29:46.350
that knows Swift pretty well.

00:29:48.990 --> 00:29:50.940
BRENNAN SAETA: Chris,
I think he means you.

00:29:50.940 --> 00:29:54.040
CHRIS LATTNER: Yeah,
we'll see how this goes.

00:29:54.040 --> 00:29:56.220
So I'm super excited
to be able to help

00:29:56.220 --> 00:29:58.260
teach the next
generation of learners.

00:29:58.260 --> 00:30:00.150
But I'm also really
excited that Jeremy

00:30:00.150 --> 00:30:02.262
will be bringing his
expertise in API design

00:30:02.262 --> 00:30:03.720
and helping us
shape the high level

00:30:03.720 --> 00:30:06.410
APIs in Swift for TensorFlow.

00:30:06.410 --> 00:30:08.910
So we've talked
about many things.

00:30:08.910 --> 00:30:10.425
But the most important
part is Swift

00:30:10.425 --> 00:30:13.418
for TensorFlow is really
TensorFlow at its core.

00:30:13.418 --> 00:30:14.960
And we think this
is super important,

00:30:14.960 --> 00:30:17.627
because we've worked really hard
to make sure that it integrates

00:30:17.627 --> 00:30:21.290
with all the things going on
in the big TensorFlow family.

00:30:21.290 --> 00:30:23.930
And we're very
excited about that.

00:30:23.930 --> 00:30:26.170
Now, you may be wondering
where you could get this.

00:30:26.170 --> 00:30:28.033
So Swift for TensorFlow
is open source.

00:30:28.033 --> 00:30:29.450
You can find out
it on GitHub now.

00:30:29.450 --> 00:30:31.910
And you can join our community.

00:30:31.910 --> 00:30:34.760
It also works great in
Colab as you've seen today.

00:30:34.760 --> 00:30:35.870
We have tutorials.

00:30:35.870 --> 00:30:36.798
We have examples.

00:30:36.798 --> 00:30:38.840
And all the demos you saw
today are available now

00:30:38.840 --> 00:30:40.830
in Colab, which is great.

00:30:40.830 --> 00:30:43.258
We've also released
our 0.2 release, which

00:30:43.258 --> 00:30:44.800
includes all the
basic infrastructure

00:30:44.800 --> 00:30:47.840
and underlying technology to
power these demos and examples.

00:30:47.840 --> 00:30:51.420
And we're actively working
on high level APIs right now.

00:30:51.420 --> 00:30:53.300
So this is not
ready for production

00:30:53.300 --> 00:30:55.160
yet as you could guess.

00:30:55.160 --> 00:30:58.527
But we're very excited
about shaping this future,

00:30:58.527 --> 00:31:00.860
building this out, exploring
this new programming model.

00:31:00.860 --> 00:31:04.100
And this is a great opportunity
for advanced researchers

00:31:04.100 --> 00:31:06.785
to get involved and help shape
the future of this platform.

00:31:06.785 --> 00:31:09.140
So we'd love it for you
to try it out and let

00:31:09.140 --> 00:31:10.340
us know what you think.

00:31:10.340 --> 00:31:10.850
Thank you.

00:31:10.850 --> 00:31:12.050
[APPLAUSE]

00:31:12.050 --> 00:31:14.500
[MUSIC PLAYING]

