WEBVTT
Kind: captions
Language: en

00:00:00.220 --> 00:00:02.830
Creating an ensemble of learners

00:00:02.830 --> 00:00:07.030
is one way to make the learners
you've got better.

00:00:07.030 --> 00:00:09.940
So we're not talking about
creating a new algorithm, but

00:00:09.940 --> 00:00:14.020
instead assembling together
several different algorithms or

00:00:14.020 --> 00:00:17.590
several different models to
create an ensemble learner.

00:00:17.590 --> 00:00:21.230
One thing I want to emphasize here is
that you can take what you learn here

00:00:21.230 --> 00:00:25.670
about ensemble learners and plug it
right in to what you're already doing

00:00:25.670 --> 00:00:28.730
with your KNN and
linear regression models.

00:00:28.730 --> 00:00:32.000
Now, what we've been doing so far,
is that we've had one kind of

00:00:32.000 --> 00:00:37.780
learning method, say KNN, we plug our
data into there and we learn a model.

00:00:37.780 --> 00:00:42.670
We can query our model with an X and
it will give us a Y.

00:00:42.670 --> 00:00:46.420
So this is not an ensemble learner,
this is just a single learner.

00:00:46.420 --> 00:00:49.290
And the idea with ensemble
learners is that we have

00:00:49.290 --> 00:00:51.420
several additional learners.

00:00:51.420 --> 00:00:55.380
So, we might have a linear regression
based model, we might have a decision

00:00:55.380 --> 00:00:59.550
tree based model, we might have
a support vector machine based model.

00:00:59.550 --> 00:01:02.810
You could continue this on with any
different number of algorithms.

00:01:02.810 --> 00:01:06.270
They're all trained using the same data,
and so now we have,

00:01:06.270 --> 00:01:08.710
in this case, four different models.

00:01:08.710 --> 00:01:13.880
To query this ensemble of learners,
we query each model by itself and

00:01:13.880 --> 00:01:15.260
combine the answers.

00:01:15.260 --> 00:01:19.325
So if we wanted to query this model
with X, we plug X into each model,

00:01:19.325 --> 00:01:22.680
the same X and then our Ys come out.

00:01:22.680 --> 00:01:27.400
So we have a Y output from each of
these models, how do we combine them?

00:01:27.400 --> 00:01:30.580
If we're doing classification where for
instance we're trying to identify

00:01:30.580 --> 00:01:35.560
what the thing is, we might have
each of these Ys vote on what it is.

00:01:35.560 --> 00:01:38.780
But we're doing regression, and so
the typical thing to do here is to

00:01:38.780 --> 00:01:43.860
take the mean, and that is the result
for this ensemble learner.

00:01:43.860 --> 00:01:46.380
We can then test this
overall ensemble learner

00:01:46.380 --> 00:01:48.970
using this test data that we set aside.

00:01:48.970 --> 00:01:50.270
Why ensembles?

00:01:50.270 --> 00:01:52.540
Why do we use them,
why might they be better?

00:01:52.540 --> 00:01:53.910
Well, there's a few reasons.

00:01:53.910 --> 00:01:54.890
First of all,

00:01:54.890 --> 00:02:01.110
ensembles often have lower error than
any individual method by themselves.

00:02:01.110 --> 00:02:04.520
Ensemble learners offer
less overfitting.

00:02:04.520 --> 00:02:09.038
The ensemble of learners typically
does not overfit as much as any

00:02:09.038 --> 00:02:10.788
individual learner by itself.

00:02:10.788 --> 00:02:12.490
Now why is that?

00:02:13.690 --> 00:02:15.890
Here's at least an intuitive answer.

00:02:15.890 --> 00:02:21.020
As each kind of learner that you
might use has a sort of bias,

00:02:21.020 --> 00:02:24.760
it's easiest to talk about that
in terms of linear regression

00:02:24.760 --> 00:02:27.200
in terms of what do I mean by bias.

00:02:27.200 --> 00:02:32.340
So clearly, with linear regression
our bias is that the data is linear.

00:02:33.550 --> 00:02:37.580
KNN has its own kind of bias, decision
trees have their own kind of bias, but

00:02:37.580 --> 00:02:43.020
when you put them together you tend
to reduce the biases because they're

00:02:43.020 --> 00:02:45.740
fighting against each
other in some sort of way.

00:02:45.740 --> 00:02:48.500
Anyways that's what
an ensemble learner is like

00:02:48.500 --> 00:02:50.750
if we use multiple types of learners.

