WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:01.270
KRZYSZTOF GRYGIEL: Hi.

00:00:01.270 --> 00:00:04.250
I'm Krzysztof from the
Google Cloud Autoscaler team.

00:00:04.250 --> 00:00:05.800
In this tutorial,
I'm going to talk

00:00:05.800 --> 00:00:08.820
about how to build scalable
applications on Google Compute

00:00:08.820 --> 00:00:12.120
Engine using the Compute Engine
autoscaling and load balancing

00:00:12.120 --> 00:00:13.280
services.

00:00:13.280 --> 00:00:15.530
The load balancing service
lets you distribute traffic

00:00:15.530 --> 00:00:18.170
across many virtual machines,
while the autoscaling

00:00:18.170 --> 00:00:19.890
service ensures that
there are always

00:00:19.890 --> 00:00:22.020
enough virtual machines
to handle an increase

00:00:22.020 --> 00:00:23.800
or decrease in load.

00:00:23.800 --> 00:00:25.330
By the end of this
video, you will

00:00:25.330 --> 00:00:27.080
know how to create
a simple scalable

00:00:27.080 --> 00:00:30.150
service using both autoscaling
and load balancing.

00:00:30.150 --> 00:00:32.240
First, we'll set the stage
and discuss a scenario

00:00:32.240 --> 00:00:35.160
where a user requires to scale
the application front end.

00:00:35.160 --> 00:00:37.630
Then, we'll go over
how Compute Engine can

00:00:37.630 --> 00:00:39.130
address some of these needs.

00:00:39.130 --> 00:00:41.920
Finally, we will run
some actual examples.

00:00:41.920 --> 00:00:43.630
Now let's get started.

00:00:43.630 --> 00:00:45.370
Imagine you're a
developer who has just

00:00:45.370 --> 00:00:48.169
built a new web application,
and you're about to launch it.

00:00:48.169 --> 00:00:49.960
Let's assume that your
application consists

00:00:49.960 --> 00:00:51.730
of the following components.

00:00:51.730 --> 00:00:54.650
A front end web server with an
external IP address associated

00:00:54.650 --> 00:00:57.280
with your service; a
back end server handling

00:00:57.280 --> 00:00:59.025
all of the business
logic and computation

00:00:59.025 --> 00:01:02.170
for the application; and
a persistent storage,

00:01:02.170 --> 00:01:04.580
such as a MySQL database,
where you will store

00:01:04.580 --> 00:01:07.102
all permanent application data.

00:01:07.102 --> 00:01:09.060
So we have your serving
infrastructure running,

00:01:09.060 --> 00:01:11.850
and are now receiving
requests from your users.

00:01:11.850 --> 00:01:14.560
As the user base and traffic
to your application grows,

00:01:14.560 --> 00:01:16.580
your front end running
on a single instance

00:01:16.580 --> 00:01:18.780
will soon reach
its capacity limit

00:01:18.780 --> 00:01:20.910
and won't have enough
CPU power to process

00:01:20.910 --> 00:01:24.005
more parallel requests
coming from your users.

00:01:24.005 --> 00:01:26.005
However, since the front
end of your application

00:01:26.005 --> 00:01:28.350
is a stateless server,
it can be replicated

00:01:28.350 --> 00:01:30.270
to multiple virtual
machines to handle

00:01:30.270 --> 00:01:32.620
many simultaneous requests.

00:01:32.620 --> 00:01:34.580
Knowing this can help
address potential

00:01:34.580 --> 00:01:36.280
serving capacity problems.

00:01:36.280 --> 00:01:38.060
If you provision more
front end instances

00:01:38.060 --> 00:01:39.980
and hook it up to
your back end system,

00:01:39.980 --> 00:01:42.897
you can serve more
requests in parallel.

00:01:42.897 --> 00:01:44.480
Of course, the back
end instance could

00:01:44.480 --> 00:01:46.530
be scaled in a similar
way, as long as it is also

00:01:46.530 --> 00:01:47.700
a stateless server.

00:01:47.700 --> 00:01:51.350
That is, if all persistent state
lives only in the database.

00:01:51.350 --> 00:01:55.020
We now have more capacity, and
no request should be dropped.

00:01:55.020 --> 00:01:56.750
There is one issue, however.

00:01:56.750 --> 00:01:59.030
Right now, you have
multiple front end servers,

00:01:59.030 --> 00:02:01.320
each with a distinct IP address.

00:02:01.320 --> 00:02:03.980
So you have enough capacity
to serve the user traffic,

00:02:03.980 --> 00:02:06.920
but you need a way to distribute
it across these servers.

00:02:06.920 --> 00:02:09.770
What you need is a single IP
address for all your users

00:02:09.770 --> 00:02:11.066
to connect to.

00:02:11.066 --> 00:02:14.030
To do this, you can set up a
network load balancer, which

00:02:14.030 --> 00:02:16.290
exposes a single
external IP to the world

00:02:16.290 --> 00:02:19.390
and forwards the traffic across
multiple front end instances,

00:02:19.390 --> 00:02:22.270
ensuring your instances
experience uniform load.

00:02:22.270 --> 00:02:24.130
As the traffic to your
service increases,

00:02:24.130 --> 00:02:25.836
you would keep adding
front end servers

00:02:25.836 --> 00:02:27.460
so that the serving
capacity can always

00:02:27.460 --> 00:02:29.220
handle the observed traffic.

00:02:29.220 --> 00:02:31.630
While this process might
work well in the beginning,

00:02:31.630 --> 00:02:34.190
you might begin to notice that
the volume of incoming traffic

00:02:34.190 --> 00:02:36.700
varies depending
on time of the day.

00:02:36.700 --> 00:02:38.460
You'll also notice
that on some days,

00:02:38.460 --> 00:02:40.811
you have more traffic
than other days.

00:02:40.811 --> 00:02:42.310
While you keep
adding machines to be

00:02:42.310 --> 00:02:44.870
able to serve the maximum
number of user connections,

00:02:44.870 --> 00:02:46.570
you quickly realize
that you also

00:02:46.570 --> 00:02:50.090
want to remove servers during
periods of low traffic.

00:02:50.090 --> 00:02:53.050
So, in fact, your serving needs
to look roughly like this.

00:02:53.050 --> 00:02:55.500
And you could save money
overall if you added and removed

00:02:55.500 --> 00:02:57.932
servers in response
to changes in traffic,

00:02:57.932 --> 00:03:00.140
rather than maintaining the
maximum number of servers

00:03:00.140 --> 00:03:01.430
all the time.

00:03:01.430 --> 00:03:04.720
Here is where an autoscaler
can come to the rescue.

00:03:04.720 --> 00:03:06.480
Instead of keeping
track of traffic

00:03:06.480 --> 00:03:08.850
and adjusting the number
of servers manually,

00:03:08.850 --> 00:03:11.280
you could configure autoscaler
to do the job for you

00:03:11.280 --> 00:03:12.910
automatically.

00:03:12.910 --> 00:03:15.020
An autoscaler
constantly keeps track

00:03:15.020 --> 00:03:18.080
of how busy a group of instances
are by watching the utilization

00:03:18.080 --> 00:03:20.140
level of instances.

00:03:20.140 --> 00:03:22.720
You define what utilization
metric the autoscaler watches,

00:03:22.720 --> 00:03:24.770
such as average CPU
load, and define

00:03:24.770 --> 00:03:27.390
a specific per-instance
utilization target,

00:03:27.390 --> 00:03:30.544
and the autoscaler automatically
works to maintain this target.

00:03:30.544 --> 00:03:32.460
For example, you might
the set the utilization

00:03:32.460 --> 00:03:36.520
target of your instances
at 60% CPU utilization.

00:03:36.520 --> 00:03:39.050
When the CPU utilization
exceeds this target,

00:03:39.050 --> 00:03:40.840
the autoscaler
adds more instances

00:03:40.840 --> 00:03:43.457
to handle the load
automatically.

00:03:43.457 --> 00:03:45.290
Let's assume you have
one instance receiving

00:03:45.290 --> 00:03:48.730
one request per second,
which consumes 30% of the CPU

00:03:48.730 --> 00:03:49.785
on the machine.

00:03:49.785 --> 00:03:53.000
If the traffic suddenly grows
to three requests per second,

00:03:53.000 --> 00:03:56.960
the machine will run
hot, using 90% of CPU.

00:03:56.960 --> 00:03:59.640
At this point, the autoscaler
will add as many instances

00:03:59.640 --> 00:04:02.900
as necessary to get it below
the target utilization, which

00:04:02.900 --> 00:04:06.070
in our example is just
one extra instance.

00:04:06.070 --> 00:04:07.700
With two instances,
the load drops

00:04:07.700 --> 00:04:10.810
to 1.5 requests per
second per instance,

00:04:10.810 --> 00:04:15.130
and the CPU utilization drops
to 45% on each instance, which

00:04:15.130 --> 00:04:17.940
is below the target 60%
utilization that we defined

00:04:17.940 --> 00:04:19.200
earlier.

00:04:19.200 --> 00:04:21.637
If the utilization remains
low for a long time

00:04:21.637 --> 00:04:23.220
and it is possible
to keep utilization

00:04:23.220 --> 00:04:26.170
below the target with a
fewer number of instances,

00:04:26.170 --> 00:04:29.120
autoscaler will start
removing instances.

00:04:29.120 --> 00:04:31.320
Now that we have
defined our scenario,

00:04:31.320 --> 00:04:32.946
let's go over the
configuration steps

00:04:32.946 --> 00:04:34.570
you would need to
set up your resources

00:04:34.570 --> 00:04:36.480
like we just described.

00:04:36.480 --> 00:04:38.830
First, you need a
group of instances.

00:04:38.830 --> 00:04:41.140
To create it, you must
define an instance template

00:04:41.140 --> 00:04:44.470
which describes the image,
machine type, startup script,

00:04:44.470 --> 00:04:47.767
and other properties of the
virtual machines in the group.

00:04:47.767 --> 00:04:49.850
This template will be used
to create new instances

00:04:49.850 --> 00:04:53.010
in the future, as well, either
by the autoscaling service,

00:04:53.010 --> 00:04:56.470
or if you manually add a
new instance to the group.

00:04:56.470 --> 00:04:59.310
Then, create your
group of instances.

00:04:59.310 --> 00:05:01.160
Use the instance
template from step one

00:05:01.160 --> 00:05:03.030
to create a managed
instance group which

00:05:03.030 --> 00:05:06.220
contains any number of
identical instances.

00:05:06.220 --> 00:05:08.330
Once you have created the
managed instance group,

00:05:08.330 --> 00:05:11.020
you can enable
autoscaling for the group.

00:05:11.020 --> 00:05:13.860
Finally, create the network
load balancer to handle traffic

00:05:13.860 --> 00:05:15.846
from a single external IP.

00:05:15.846 --> 00:05:18.560
The network load balancer
talks to a group of back ends

00:05:18.560 --> 00:05:20.360
called the target pool.

00:05:20.360 --> 00:05:23.400
A target pool can contain
individual instances or groups

00:05:23.400 --> 00:05:24.780
of instances.

00:05:24.780 --> 00:05:27.280
In this case, it would just
consist of the single managed

00:05:27.280 --> 00:05:29.570
instance group you create.

00:05:29.570 --> 00:05:32.030
The network load balancer
uses a forwarding rule

00:05:32.030 --> 00:05:34.970
which defines the protocol,
IP address, and port

00:05:34.970 --> 00:05:36.820
the users will talk to.

00:05:36.820 --> 00:05:37.890
That's it.

00:05:37.890 --> 00:05:40.260
Now it's time to try
it out in practice.

00:05:40.260 --> 00:05:42.100
We are going to use
the developer's console

00:05:42.100 --> 00:05:44.000
to perform all the steps.

00:05:44.000 --> 00:05:46.380
We assume that you already
have a Google Cloud Platform

00:05:46.380 --> 00:05:48.290
project set up.

00:05:48.290 --> 00:05:50.060
For the purpose of
this demonstration,

00:05:50.060 --> 00:05:52.000
we'll only create
the front end layer,

00:05:52.000 --> 00:05:54.120
which will respond
to HTTP requests

00:05:54.120 --> 00:05:56.220
without talking to a back end.

00:05:56.220 --> 00:05:59.540
Step one, create an
instance template.

00:05:59.540 --> 00:06:02.290
Go to Instance Templates
pane, and choose New Instance

00:06:02.290 --> 00:06:03.470
Template.

00:06:03.470 --> 00:06:06.480
At this step, you are selecting
machine type, operating system,

00:06:06.480 --> 00:06:08.370
and disk for your servers.

00:06:08.370 --> 00:06:10.660
Use the default settings.

00:06:10.660 --> 00:06:14.430
Give a name to your template,
and check Allow HTTP Traffic.

00:06:14.430 --> 00:06:16.390
Finally, add a startup
script for your VM

00:06:16.390 --> 00:06:19.120
in the Instance Metadata field.

00:06:19.120 --> 00:06:21.250
For this example, we are
adding a startup script

00:06:21.250 --> 00:06:22.916
that downloads the
front end application

00:06:22.916 --> 00:06:25.120
code from cloud storage.

00:06:25.120 --> 00:06:28.020
The front end is a super simple
web server written in Python

00:06:28.020 --> 00:06:31.310
which only simulates the effect
of real request processing

00:06:31.310 --> 00:06:33.930
by running an idle loop
that consumes some CPU

00:06:33.930 --> 00:06:37.020
power in response to
HTTP get requests.

00:06:37.020 --> 00:06:39.850
In a real application, we
would replace this idle loop

00:06:39.850 --> 00:06:42.470
with some logic to do request
processing, communication

00:06:42.470 --> 00:06:46.370
with the back end, and to
generate HTTP responses.

00:06:46.370 --> 00:06:48.360
The code for this demo
is available under

00:06:48.360 --> 00:06:48.692
storage.googleap
is.com/autoscale

00:06:48.692 --> 00:06:49.483
r-demo/frontend.py.

00:06:55.090 --> 00:06:57.710
Based on the code, notice
that our front end will serve

00:06:57.710 --> 00:07:00.460
requests under
the path /service.

00:07:00.460 --> 00:07:02.950
So we will later be
appending it to the HTTP

00:07:02.950 --> 00:07:05.040
address of the front end.

00:07:05.040 --> 00:07:07.630
Of course, if you prefer,
instead of using the Python

00:07:07.630 --> 00:07:10.220
app, you could install a
standard Apache web server

00:07:10.220 --> 00:07:14.130
and try the PHP script
for a similar result.

00:07:14.130 --> 00:07:16.810
Step two, create
an instance group

00:07:16.810 --> 00:07:20.890
using the template that you just
defined, and enable autoscaler.

00:07:20.890 --> 00:07:22.690
Select a preferred
zone for your service

00:07:22.690 --> 00:07:26.130
to operate, and give
your group a name.

00:07:26.130 --> 00:07:29.430
The autoscaler configuration
is pre-filled with defaults.

00:07:29.430 --> 00:07:32.510
Here we will use the average
CPU utilization of the VMs

00:07:32.510 --> 00:07:34.880
as the utilization
metric to measure.

00:07:34.880 --> 00:07:38.690
The target utilization
is set to 60%.

00:07:38.690 --> 00:07:40.680
Also keep in mind that
the autoscaling policy

00:07:40.680 --> 00:07:42.060
can be customized.

00:07:42.060 --> 00:07:44.690
Instead of CPU usage, you
could choose other utilization

00:07:44.690 --> 00:07:47.250
metrics, such as rate of
requests going through the load

00:07:47.250 --> 00:07:51.240
balancer, or arbitrary
cloud monitoring metrics.

00:07:51.240 --> 00:07:53.930
For now, we'll just
use the defaults.

00:07:53.930 --> 00:07:56.820
Click Create to create
your instance group.

00:07:56.820 --> 00:07:59.732
Creating the instance group
may take about two minutes.

00:07:59.732 --> 00:08:01.440
Check when the group
is ready by watching

00:08:01.440 --> 00:08:05.240
the Activities tab in the bottom
right corner of the screen.

00:08:05.240 --> 00:08:07.350
To see the details of
the instance group,

00:08:07.350 --> 00:08:10.150
select it in the
Instance Groups pane.

00:08:10.150 --> 00:08:13.800
Here we can see that the first
instance has been created.

00:08:13.800 --> 00:08:17.160
Step three, configure
load balancer.

00:08:17.160 --> 00:08:19.760
Select Network Load Balancing.

00:08:19.760 --> 00:08:21.920
You may notice that there
is another option called

00:08:21.920 --> 00:08:24.310
HTTP Load Balancing next to it.

00:08:24.310 --> 00:08:27.280
This is a more advanced form
of load balancing, which

00:08:27.280 --> 00:08:30.850
offers options specific to
HTTP traffic such as balancing

00:08:30.850 --> 00:08:32.970
based on URL map.

00:08:32.970 --> 00:08:34.970
As we don't need these
advanced features,

00:08:34.970 --> 00:08:37.059
we are going to use the
generic network load

00:08:37.059 --> 00:08:39.690
balancing for this example.

00:08:39.690 --> 00:08:43.460
Go to the Basic Setup tab of
the Network Load Balancing page.

00:08:43.460 --> 00:08:45.600
Under Region, choose
the same region

00:08:45.600 --> 00:08:47.510
where the instance group lives.

00:08:47.510 --> 00:08:50.710
The forwarding rule section
specifies the endpoint

00:08:50.710 --> 00:08:53.500
that your users will
send the traffic to.

00:08:53.500 --> 00:08:55.000
Leave it set to the
default options.

00:08:55.000 --> 00:08:59.940
That this, ephemeral IP,
port number 80, TCP traffic.

00:08:59.940 --> 00:09:02.192
Then we define the target pool.

00:09:02.192 --> 00:09:04.400
Recall that the target pool
is the group of instances

00:09:04.400 --> 00:09:08.320
that the network load balancer
will forward traffic to.

00:09:08.320 --> 00:09:11.280
We choose Select
Existing Instance Group,

00:09:11.280 --> 00:09:15.090
and pick the instance group we
created in the previous step.

00:09:15.090 --> 00:09:16.110
That's it.

00:09:16.110 --> 00:09:19.370
You're done setting
up your configuration.

00:09:19.370 --> 00:09:21.610
You can test your stack
by entering the IP address

00:09:21.610 --> 00:09:23.730
of the forwarding rule
in the web browser,

00:09:23.730 --> 00:09:26.350
followed by the /service path.

00:09:26.350 --> 00:09:28.970
To get the external IP address
of the forwarding rule,

00:09:28.970 --> 00:09:32.300
go to the Forwarding Rules
tab of the same menu.

00:09:32.300 --> 00:09:34.170
It works.

00:09:34.170 --> 00:09:37.080
So far, the load balancer
didn't have to do much work,

00:09:37.080 --> 00:09:39.660
since the front end
only had one instance.

00:09:39.660 --> 00:09:43.300
For a real test, send some
more traffic to the instance.

00:09:43.300 --> 00:09:47.100
Let's run a shell script to
send HTTP get requests to the IP

00:09:47.100 --> 00:09:49.420
address of the forwarding rule.

00:09:49.420 --> 00:09:53.650
This command generates one
HTTP request every second.

00:09:53.650 --> 00:09:56.800
The front end was implemented to
consume one second of CPU time

00:09:56.800 --> 00:09:58.750
to serve a single request.

00:09:58.750 --> 00:10:00.460
So on a single core
machine, it will

00:10:00.460 --> 00:10:03.255
be running at 100% utilization.

00:10:03.255 --> 00:10:06.210
The autoscaler should
start another instance soon

00:10:06.210 --> 00:10:08.830
to handle the increase in load.

00:10:08.830 --> 00:10:10.940
Let's return to the
Instance Group page

00:10:10.940 --> 00:10:13.080
and select our instance group.

00:10:13.080 --> 00:10:14.020
Here we go.

00:10:14.020 --> 00:10:16.460
The Instance Group page now
shows an additional instance

00:10:16.460 --> 00:10:18.395
added by the autoscaler.

00:10:18.395 --> 00:10:20.680
Let's try to double their load.

00:10:20.680 --> 00:10:22.750
Simply decrease the delay
between the requests

00:10:22.750 --> 00:10:24.620
to half a second.

00:10:24.620 --> 00:10:27.810
We should see the response
time immediately going up.

00:10:27.810 --> 00:10:30.600
Some requests are
even timing out.

00:10:30.600 --> 00:10:33.770
In a short while, autoscaler
adds another two instances.

00:10:33.770 --> 00:10:36.920
The response time
drops down to normal.

00:10:36.920 --> 00:10:39.770
I encourage you to experiment
more with this setup.

00:10:39.770 --> 00:10:41.910
You may notice that
when the traffic stops,

00:10:41.910 --> 00:10:44.060
it takes a while before
the autoscaler starts

00:10:44.060 --> 00:10:45.890
deleting the instances.

00:10:45.890 --> 00:10:47.940
This is to reduce
disruption to the service,

00:10:47.940 --> 00:10:49.840
and to avoid the cost
of frequent restarts

00:10:49.840 --> 00:10:52.230
of virtual machines.

00:10:52.230 --> 00:10:54.890
We hope that this presentation
demonstrated how easily you

00:10:54.890 --> 00:10:57.330
can start building your
own scalable applications

00:10:57.330 --> 00:10:59.340
with Google Cloud Platform.

00:10:59.340 --> 00:11:02.480
Autoscaler is designed to work
equally well for Google scale

00:11:02.480 --> 00:11:05.360
applications, running on
hundreds of virtual machines,

00:11:05.360 --> 00:11:08.350
to let you take full advantage
of the Google Cloud Platform

00:11:08.350 --> 00:11:08.950
scale.

00:11:08.950 --> 00:11:12.600
[MUSIC PLAYING]

