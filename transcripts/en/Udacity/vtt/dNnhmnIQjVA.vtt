WEBVTT
Kind: captions
Language: en

00:00:00.240 --> 00:00:06.330
So one way of speeding up hit times
is to overlap one hit with another,

00:00:06.330 --> 00:00:09.930
and we can achieve this, for example,
through pipelining the cache.

00:00:09.930 --> 00:00:13.860
If the cache takes
multiple cycles to access,

00:00:13.860 --> 00:00:18.870
we can have a situation where an access
come in cycle N, and it's a hit.

00:00:18.870 --> 00:00:23.778
And now if another access comes in
cycle N+1, and it would be a hit, too,

00:00:23.778 --> 00:00:28.940
in a non-pipeline cache, this second
access has to wait until the first

00:00:28.940 --> 00:00:32.750
access is done using the cache, and
it takes multiple cycles to do that.

00:00:32.750 --> 00:00:37.290
So in this situation, the hit time,
as seen by each access,

00:00:37.290 --> 00:00:42.310
is the actual hit time of a cache
plus the wait time that the access

00:00:42.310 --> 00:00:46.940
suffers because it cannot access the
cache until the previous one is done.

00:00:46.940 --> 00:00:50.650
And in that situation,
pipelining the cache so that we can send

00:00:50.650 --> 00:00:55.590
in accesses one after the other
will improve the overall hit time.

00:00:55.590 --> 00:00:59.011
Now it may sound straightforward
how to pipeline the cache.

00:00:59.011 --> 00:01:01.320
You just divide it in,
let's say, three stages.

00:01:01.320 --> 00:01:04.989
But how do you split
what amounts to basically

00:01:04.989 --> 00:01:08.990
a read from a large array because
that's really what the cache is?

00:01:08.990 --> 00:01:13.780
Remember that the cache access consists
of using the index part of the address

00:01:13.780 --> 00:01:14.870
to find the set.

00:01:15.890 --> 00:01:21.250
Reading out the tags and valid bits that
correspond to the blocks in that set.

00:01:21.250 --> 00:01:23.830
Comparing the tags, and
checking the valid bits for

00:01:23.830 --> 00:01:26.580
each of these to see
whether it has a hit.

00:01:26.580 --> 00:01:31.180
Combining these so that we know
whether we have an overall hit and

00:01:31.180 --> 00:01:32.850
where in our set.

00:01:32.850 --> 00:01:36.200
Once we know where,
we read out the data and

00:01:36.200 --> 00:01:41.460
use the offset to choose the right
part of the large cache block.

00:01:41.460 --> 00:01:44.300
At which point, we have the data
that the processor wants, and

00:01:44.300 --> 00:01:46.020
our cache access is done.

00:01:46.020 --> 00:01:51.020
So one example of a cache pipeline
would be to have this part,

00:01:51.020 --> 00:01:55.600
reading out the tags and so
on from the cache array, be stage one.

00:01:55.600 --> 00:01:57.740
Determining the hits and

00:01:57.740 --> 00:02:02.080
beginning the data read would be
stage two, and finishing the actual

00:02:02.080 --> 00:02:05.660
data read all the way to getting
the data would be stage three.

00:02:05.660 --> 00:02:10.740
So as you can see, we can pipeline
the cache access, even if we

00:02:10.740 --> 00:02:16.510
don't know how to actually break down
the reading from the cache array,

00:02:16.510 --> 00:02:21.540
especially if the tags and the valid
bits are read before we determine

00:02:21.540 --> 00:02:26.520
the hit, and we only read the data part
of the cache after we determine the hit.

00:02:26.520 --> 00:02:29.810
In that case,
just those two can be separate stages.

00:02:29.810 --> 00:02:34.340
Usually, the actual cache hit time for
level one caches will be one, two or

00:02:34.340 --> 00:02:35.875
three cycles.

00:02:35.875 --> 00:02:39.390
One-cycle caches don't need pipelining,
and two- and three-cycle

00:02:39.390 --> 00:02:43.330
caches can be relatively easily
pipelined into two or three stages.

00:02:43.330 --> 00:02:46.780
So usually,
level one caches will be pipelined.

