WEBVTT

1
00:00:00.860 --> 00:00:05.580
In this video,
we'll start,
uh,
uh,
investigating how we can train neural networks.

2
00:00:05.820 --> 00:00:10.620
And a four that we'll look at the main principle behind training neural
networks,

3
00:00:10.790 --> 00:00:13.350
uh,
which is known as empirical risk minimization.

4
00:00:15.470 --> 00:00:17.540
Now we've seen the mathematical,

5
00:00:17.810 --> 00:00:22.810
a derivation or mathematical expression for what we mean by a multilayer neural

6
00:00:23.511 --> 00:00:24.710
network.
Uh,

7
00:00:24.711 --> 00:00:29.570
and in this description there are these parameters which are for instance,

8
00:00:29.571 --> 00:00:34.190
the connections between the unions and the input layer.
The first one there,

9
00:00:34.191 --> 00:00:38.470
the connections between the first and then layer in second hidden layer.

10
00:00:38.480 --> 00:00:40.320
Then the connections with,
uh,

11
00:00:40.370 --> 00:00:45.370
between the Lassen in layer and the output layer as well as the bias vectors for

12
00:00:45.650 --> 00:00:49.610
each of the layers or the hidden layers and the output layer of the neural

13
00:00:49.611 --> 00:00:54.170
network.
And so far we haven't talked about how we can actually,
uh,

14
00:00:54.560 --> 00:00:58.910
find good values for these parameters such that on your own network will have

15
00:00:58.911 --> 00:01:03.380
the correct behavior,
will solve our problem that we're trying to solve.
So,

16
00:01:03.381 --> 00:01:04.790
um,
uh,

17
00:01:04.791 --> 00:01:08.630
what we'll see in this video in the following videos is how we can train a

18
00:01:08.631 --> 00:01:13.610
neural network to develop the right behavior to solve a given problem.

19
00:01:14.510 --> 00:01:17.060
One of the problem will often talk about them,

20
00:01:17.061 --> 00:01:19.220
we'll focus on is a problem classification.

21
00:01:19.520 --> 00:01:23.240
And a good problem to keep in mind when we're discussing a training neural

22
00:01:23.241 --> 00:01:27.200
network is one where we would have an image where there is a particular

23
00:01:27.201 --> 00:01:31.070
character,
say a three in this example and a,

24
00:01:31.071 --> 00:01:34.820
and we'll assume that we have some data where someone has told us that for

25
00:01:34.821 --> 00:01:39.500
instance,
for this image,
this actually curse bonds to the character,
uh,
three.

26
00:01:39.950 --> 00:01:42.590
So we'll assume that we have some data where we have a,

27
00:01:42.591 --> 00:01:45.290
an input x and the associated target.

28
00:01:45.320 --> 00:01:50.320
Why that we must predict based on the input x and for a character prediction

29
00:01:50.840 --> 00:01:55.840
than x would be the a row wise vectorization of all the pixels in the image.

30
00:01:56.151 --> 00:02:00.630
That would be an example of an input to use an input representation for an
image.

31
00:02:01.040 --> 00:02:04.380
Okay.
So that's a good problem to keep in mind while we're talking about training

32
00:02:04.381 --> 00:02:05.214
neural networks.

33
00:02:07.290 --> 00:02:08.123
<v 1>Yeah.</v>

34
00:02:08.210 --> 00:02:09.140
<v 0>So,
uh,</v>

35
00:02:09.170 --> 00:02:13.250
the specific algorithm that we'll be looking at for training neural network is

36
00:02:13.251 --> 00:02:17.720
going to be based on a very general principle in machine learning,
uh,

37
00:02:17.750 --> 00:02:21.760
known as empirical risk minimization.
So in a nutshell,

38
00:02:21.770 --> 00:02:26.120
the ideas that will convert the problem of training a neural network into an

39
00:02:26.121 --> 00:02:28.970
optimization problem.
Um,

40
00:02:29.210 --> 00:02:34.210
so I'm going to call [inaudible] the set of all my parameters in a given model

41
00:02:35.811 --> 00:02:40.250
in,
in my,
uh,
the case were interesting.
Uh,
Tara is going to be the,

42
00:02:40.310 --> 00:02:43.010
all the parameters in the connection matrices,

43
00:02:43.011 --> 00:02:47.900
the WWE matrices as well as the bias vectors be,
uh,
for each,

44
00:02:47.901 --> 00:02:49.760
uh,
for all the different layers.

45
00:02:50.920 --> 00:02:54.910
I also note x t as the teeth,

46
00:02:55.370 --> 00:03:00.370
a training example.
In some set of training images we'll use for,
uh,

47
00:03:00.430 --> 00:03:01.750
to train on neural network.

48
00:03:02.470 --> 00:03:07.470
So XD here is a vector and his DTF input vector in my training set.

49
00:03:08.560 --> 00:03:11.410
And then why tea is the associated.

50
00:03:11.650 --> 00:03:16.630
So the TF label a or target,
they have to predict based on x.

51
00:03:17.500 --> 00:03:22.420
So again,
x t could be an image of a character and why it could be the categories

52
00:03:22.421 --> 00:03:27.160
associated with that character.
So in empirical risk minimization,

53
00:03:27.220 --> 00:03:31.240
we'll frame the problem of learning or training a model as the problem of

54
00:03:31.241 --> 00:03:35.950
finding the parameters which minimized this objective.

55
00:03:36.220 --> 00:03:39.100
Here,
this objective has two parts.

56
00:03:39.160 --> 00:03:44.160
The first part is the average of a loss function l that compares the output of

57
00:03:47.861 --> 00:03:52.510
my neural network with the expected or the correct answer.

58
00:03:52.580 --> 00:03:54.370
Yt Okay.

59
00:03:54.371 --> 00:03:58.900
Al is going to be what is known as a loss function and it compares the output

60
00:03:59.230 --> 00:04:00.100
with the label.

61
00:04:01.480 --> 00:04:06.480
And also I'm going to add a term or mega data which is known as irregular riser.

62
00:04:10.480 --> 00:04:13.420
And what it does is that it penalizes certain values of data,

63
00:04:13.421 --> 00:04:15.970
certain values of my parameter,
which uh,

64
00:04:16.120 --> 00:04:20.460
for some reason we think are not good values that will not work well On.
Uh,

65
00:04:20.920 --> 00:04:24.550
uh,
when we try to solve,
uh,
problems on new images,

66
00:04:24.551 --> 00:04:26.290
say on new examples.

67
00:04:27.370 --> 00:04:31.840
And then we have a parameter here,
lambda,
which is going to be hyper parameter,

68
00:04:31.841 --> 00:04:35.700
which is going to control the,
uh,

69
00:04:35.701 --> 00:04:40.070
a balance between optimizing the average loss and uh,

70
00:04:40.180 --> 00:04:44.440
optimizing the regularization burglarized or function.

71
00:04:45.670 --> 00:04:47.920
So here,
because we have this argument,

72
00:04:47.921 --> 00:04:51.280
really we're converting our problem into an optimization problem,

73
00:04:51.281 --> 00:04:53.290
we're trying to minimize a particular objective.

74
00:04:54.370 --> 00:04:58.750
Ideally we'd like to optimize classification error if we're doing
classification.

75
00:04:58.780 --> 00:05:03.780
So we'd like to optimize the actual percentage of errors that our model is,

76
00:05:04.060 --> 00:05:07.450
uh,
makes on the training set.
Howard,

77
00:05:07.460 --> 00:05:11.260
this function is not as smooth function.
Uh,
so,
uh,

78
00:05:11.290 --> 00:05:16.290
if we wanted to optimize the number of times that the,

79
00:05:16.390 --> 00:05:19.570
uh,
outputs class by our model,

80
00:05:19.571 --> 00:05:24.070
which is the class which is associated with the biggest probability based on the

81
00:05:24.071 --> 00:05:26.890
output probability given by our neural network.

82
00:05:27.670 --> 00:05:29.080
So the number of times at this,

83
00:05:29.370 --> 00:05:32.710
the most likely class according to our model is wrong.

84
00:05:33.100 --> 00:05:35.950
This is actually a function which is a not smooth.

85
00:05:35.980 --> 00:05:40.870
So it essentially looks something like this where this is when we have an error

86
00:05:40.871 --> 00:05:45.640
of zero,
when we have,
uh,
uh,
where the correct class as the most,

87
00:05:45.670 --> 00:05:46.690
the highest probability.

88
00:05:46.691 --> 00:05:51.580
And then we have an error one when the correct class as doesn't have the highest

89
00:05:51.581 --> 00:05:56.500
probability.
So this function,
which looks like this is not smooth.
So it's a,

90
00:05:56.501 --> 00:05:58.910
it's a very hard function to,
uh,
to optimize.

91
00:05:59.360 --> 00:06:02.810
So instead what we do is we usually use a loss function,

92
00:06:02.811 --> 00:06:07.490
which is going to be a surrogate of what we truly want to optimize.
So,

93
00:06:07.491 --> 00:06:07.971
for instance,

94
00:06:07.971 --> 00:06:11.900
we might try to actually upper bound the real thing that we aren't optimized

95
00:06:12.070 --> 00:06:15.950
with something smooth.
Uh,
and uh,
so you know,

96
00:06:15.951 --> 00:06:19.040
if we could somehow take this,
uh,
functions,

97
00:06:19.041 --> 00:06:22.940
they are lost function and look like this and have a smooth version which may be

98
00:06:22.941 --> 00:06:27.520
looked like this,
then we might try to optimize this as well and hope that,
uh,

99
00:06:27.590 --> 00:06:30.080
you know,
because this is fairly close to that,

100
00:06:30.081 --> 00:06:35.030
then we are sort of optimizing the real thing.
Okay.
And,
uh,

101
00:06:35.031 --> 00:06:38.810
so this is the general principle for empirical risk minimization that we'll try

102
00:06:38.811 --> 00:06:40.610
to follow when training neural networks.

103
00:06:43.790 --> 00:06:46.400
And because we've cast it our problem as an optimization problem,

104
00:06:46.401 --> 00:06:49.760
what's great is that we can leverage optimization algorithms from the

105
00:06:49.761 --> 00:06:53.710
optimization literature.
And the one algorithm that will,
uh,

106
00:06:53.840 --> 00:06:57.260
focus on is these stochastic gradient descent algorithm.

107
00:06:57.770 --> 00:06:59.210
And this is not going,

108
00:06:59.211 --> 00:07:01.940
you've probably seen if you've taken a machine learning class.

109
00:07:02.060 --> 00:07:05.360
So I'll just briefly describe it here.
Um,

110
00:07:05.390 --> 00:07:10.390
so what the algorithm does is at first it initializes the parameters of our

111
00:07:10.611 --> 00:07:11.331
neural networks.

112
00:07:11.331 --> 00:07:14.960
So all the connections and biases in our neural network in some way,

113
00:07:16.190 --> 00:07:19.730
often it's initialized randomly.
And then for a son,

114
00:07:19.740 --> 00:07:24.290
a number of iterations with we have to specify,
we have to specify end here.

115
00:07:24.860 --> 00:07:29.860
What we'll do is that will iterate over our training set and for each training

116
00:07:29.871 --> 00:07:31.840
example,
each pair x,
t,
y,

117
00:07:31.841 --> 00:07:36.520
t we'll figure out a direction,
uh,

118
00:07:36.590 --> 00:07:41.590
that we'll explore for a that will consider for updating our parameters and send

119
00:07:41.811 --> 00:07:46.811
then and natural direction to use for a given training example is the opposite

120
00:07:46.911 --> 00:07:51.250
direction of the gradient of the loss and,
uh,

121
00:07:51.380 --> 00:07:54.140
plus the gradient of the irregular riser.

122
00:07:54.620 --> 00:07:57.080
So we go in the opposite directions who have a minus here.

123
00:07:57.770 --> 00:08:01.910
So remember that the gradient is telling us in what directions do,

124
00:08:01.940 --> 00:08:03.680
uh,
I do,

125
00:08:03.860 --> 00:08:07.340
I get the biggest increase if I change a certain number of variables.

126
00:08:07.760 --> 00:08:12.620
So if I take my loss plus my regular riser,

127
00:08:13.490 --> 00:08:17.670
if I think the great end of that,
then this will give me in which direction they,

128
00:08:17.700 --> 00:08:19.790
I think the grain in retrospect to my parameter.

129
00:08:19.970 --> 00:08:24.680
This gives me the direction in which if I changed my,
if I increase a,
oh,

130
00:08:24.681 --> 00:08:28.150
sorry,
if I follow this direction in changing my parameter,
I'm getting,

131
00:08:28.151 --> 00:08:31.870
I'm going to get the biggest increase in the loss plus the way that regular

132
00:08:31.871 --> 00:08:35.510
riser.
So if I go in the opposite direction,
I should get the locally,

133
00:08:35.511 --> 00:08:39.530
the biggest decrease in the last plus my weighted regularized,
sir.

134
00:08:39.560 --> 00:08:43.880
So I use that as my direction.
So that's going to be my direction,

135
00:08:43.970 --> 00:08:44.803
Delta.

136
00:08:45.350 --> 00:08:50.350
And then I'll just take my parameter and I updated by adding a step size or

137
00:08:51.321 --> 00:08:53.750
learning rate Alpha,
which is another parameter.

138
00:08:53.751 --> 00:08:58.751
I have to specify a hyper parameter a times dis direction delta of four,

139
00:09:00.000 --> 00:09:03.780
updating my privater.
So I'm taking one step into,

140
00:09:03.810 --> 00:09:08.370
in the direction that is locally most likely to decrease,

141
00:09:08.371 --> 00:09:10.620
that's going to decrease the most my,

142
00:09:10.950 --> 00:09:13.980
some of loss plus record riser for my training example.

143
00:09:14.770 --> 00:09:19.770
And so I repeat this operation here for each training example in my training

144
00:09:20.251 --> 00:09:23.910
set.
And then I repeat this whole loop and times,

145
00:09:23.911 --> 00:09:26.190
which is the number of iterations,

146
00:09:26.191 --> 00:09:31.100
or sometimes we'll call that a number of training epoch,
uh,

147
00:09:31.320 --> 00:09:35.730
uh,
over all my,
uh,
training examples.
Okay.

148
00:09:35.731 --> 00:09:39.210
So if we want to apply this general algorithm of stochastic gradient descent,

149
00:09:39.630 --> 00:09:43.320
I need the following items in this recipe.

150
00:09:43.950 --> 00:09:47.490
I need to define my loss function.
This function owl.

151
00:09:48.180 --> 00:09:51.840
I need the procedure for computing the gradients.
Ideally,

152
00:09:51.910 --> 00:09:54.570
I want a procedure that's efficient because I'm going to,

153
00:09:54.780 --> 00:09:57.720
I'm going to do a lot of updates.
I'm going to compute this term here.

154
00:09:57.721 --> 00:10:01.860
Often I need to choose a particular form for my regular riser.

155
00:10:02.250 --> 00:10:06.780
And given that form else,
we'll need to compute the gradients with respect to,
uh,

156
00:10:06.810 --> 00:10:09.720
have that regularize her with respect to my parameter ETA.

157
00:10:10.530 --> 00:10:13.890
And then also need to specify a way of initializing my parameters.

158
00:10:14.610 --> 00:10:15.990
So what we'll do in the next videos,

159
00:10:15.991 --> 00:10:20.010
and we'll go over all of these different items in this recipe so that we have

160
00:10:20.011 --> 00:10:24.460
all the items we need all the ingredients to applies to Casta great in the

161
00:10:24.461 --> 00:10:25.770
center and train the neural network.

