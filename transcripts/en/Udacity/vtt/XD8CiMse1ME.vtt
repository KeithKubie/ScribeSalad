WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:04.490
Instead of just making a one
variable kernel density estimate,

00:00:04.490 --> 00:00:07.720
we could use multivariate
density estimates.

00:00:07.720 --> 00:00:12.590
Multivariate kernel density estimates
will give us joint probability density

00:00:12.590 --> 00:00:17.020
functions from which we can
draw conditional probabilities.

00:00:17.020 --> 00:00:22.940
In this case, we can use another
second variable denoted as

00:00:22.940 --> 00:00:28.530
x bar which could be the difference
between f0 and f2 divided by f0.

00:00:29.560 --> 00:00:35.010
This will then be the difference
of the submitted charge amount and

00:00:35.010 --> 00:00:39.270
the Medicare allowed amount,
divided by the submitted charge amount.

00:00:40.280 --> 00:00:44.020
Let's take a look at what
the kernel density estimate for

00:00:44.020 --> 00:00:49.020
the joint distribution of x and
x bar looks like.

00:00:49.020 --> 00:00:51.190
Back to your iPython notebook.

00:00:51.190 --> 00:00:56.178
We have defined a new variable called
xbar which is the absolute value

00:00:56.178 --> 00:01:01.030
of the difference between f0 and
f2 divided by f0.

00:01:01.030 --> 00:01:06.653
We can use a very convenient
joint plot function included in

00:01:08.330 --> 00:01:14.340
to make the joint kernel density
estimate of the variables x and x bar.

00:01:14.340 --> 00:01:16.920
Let's run this piece of code.

00:01:16.920 --> 00:01:19.910
This might take a while to run.

00:01:19.910 --> 00:01:24.400
The star here indicates that the
computer is still working on your plot.

00:01:24.400 --> 00:01:27.860
Have patience and
you will get a nice looking figure.

00:01:29.310 --> 00:01:31.410
Once that code finishes running,

00:01:31.410 --> 00:01:35.760
you will get a two-dimensional
plot looking like this.

00:01:35.760 --> 00:01:40.230
On each axis we have plotted
the one dimensional PDFs.

00:01:41.920 --> 00:01:46.690
The joint distribution is
given as this heat map here.

00:01:46.690 --> 00:01:48.611
Looking at the joint PDF,

00:01:48.611 --> 00:01:52.811
you can see that there is
a cluster around the point 00.

00:01:52.811 --> 00:01:56.563
There is also another one around 0.8.

00:01:56.563 --> 00:02:00.110
There could be another
cluster somewhere around 0.6.

00:02:00.110 --> 00:02:07.000
You can ask several different
questions after you have a joint KDE.

00:02:07.000 --> 00:02:12.170
Notice that the multivariate kernel
density estimation was made behind

00:02:12.170 --> 00:02:17.910
the scene, and all you see is
the plot of the probability density.

00:02:17.910 --> 00:02:22.930
You can use multivariate
KDEs to do regressions.

00:02:22.930 --> 00:02:27.929
You can also use them to do pattern
classification using Bayes' decision

00:02:27.929 --> 00:02:29.020
rules.

00:02:29.020 --> 00:02:34.250
The usefulness of KDEs is also to
determine the shape of the data

00:02:34.250 --> 00:02:37.800
very quickly without having
to collect a lot of data and

00:02:37.800 --> 00:02:41.860
can be used for online models and
high velocity data.

00:02:41.860 --> 00:02:45.350
In the next lessons,
we're going to look at some ways

00:02:45.350 --> 00:02:49.050
to determine if we have
outliers in the data.

00:02:49.050 --> 00:02:52.860
We covered a little bit of kernel
density estimates as part of

00:02:52.860 --> 00:02:56.030
nonparametric methods in statistics.

00:02:56.030 --> 00:02:59.110
However, the subject is really vast.

00:02:59.110 --> 00:03:02.740
We recommend the book All of
Nonparametric Statistics by

00:03:02.740 --> 00:03:07.190
Larry Wasserman if you want to
learn more about the subject.

00:03:07.190 --> 00:03:11.080
A link to the book is given
in the instructors notes.

00:03:11.080 --> 00:03:12.360
In the next lesson,

00:03:12.360 --> 00:03:17.490
we're going to look at a simple way to
see if there are outliers in our data.

