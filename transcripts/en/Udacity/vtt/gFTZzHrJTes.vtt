WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:03.250
The difference between using
reinforcement learning for control and

00:00:03.250 --> 00:00:06.850
using reinforcement learning without
control is basically whether or

00:00:06.850 --> 00:00:10.260
not there's actions that
are being chosen by the learner.

00:00:10.260 --> 00:00:12.570
And so just to make that make sense,

00:00:12.570 --> 00:00:15.330
I'm going to swap back in
the notion of Bellman equation.

00:00:15.330 --> 00:00:17.010
So this is the Bellman
equations without actions,

00:00:17.010 --> 00:00:20.070
this is what we've been talking
about for the last few videos.

00:00:20.070 --> 00:00:21.340
Do you recognize the Bellman Equation?

00:00:21.340 --> 00:00:24.210
The value of a state is the reward

00:00:24.210 --> 00:00:27.210
plus the discounted expected
value of the next state.

00:00:27.210 --> 00:00:30.270
&gt;&gt; Yeah, I mean usually you
write that with an a in there.

00:00:30.270 --> 00:00:32.530
&gt;&gt; Right, right, but I said no actions.

00:00:32.530 --> 00:00:33.290
&gt;&gt; Okay.
&gt;&gt; We're going to

00:00:33.290 --> 00:00:34.760
put an a back in in just a moment.

00:00:34.760 --> 00:00:35.780
&gt;&gt; Okay, that'll make me feel better.

00:00:35.780 --> 00:00:38.830
&gt;&gt; Certainly when we were talking
about TD we didn't really have

00:00:38.830 --> 00:00:39.820
much of an ocean of actions.

00:00:39.820 --> 00:00:43.350
We were really talking about sequence of
states that were being experienced and

00:00:43.350 --> 00:00:44.570
the rewards associated with them.

00:00:44.570 --> 00:00:47.280
And in fact the update
rule that we talked about,

00:00:47.280 --> 00:00:50.840
this is one of the update
rules that we talked about.

00:00:50.840 --> 00:00:54.130
Says that if we go from some state and
we get some reward and

00:00:54.130 --> 00:00:55.110
we end up in some new state,

00:00:55.110 --> 00:00:59.010
then what we're going to do is our new
value function estimate is going to be.

00:01:00.160 --> 00:01:02.540
The, well, it's going to be
the same as the old one, except for

00:01:02.540 --> 00:01:04.120
in the state that we just left.

00:01:04.120 --> 00:01:08.950
Where we're going to move a little bit
towards the reward we got plus our

00:01:08.950 --> 00:01:12.345
prediction, our discounted prediction
of the state that we just landed in.

00:01:12.345 --> 00:01:15.433
So what update rule is this of the
different update rules we talked about?

00:01:15.433 --> 00:01:16.472
&gt;&gt; TD(0).

00:01:16.472 --> 00:01:18.705
&gt;&gt; Yeah, TD(0), factorial.

00:01:18.705 --> 00:01:22.335
So what we need to do now is get control
back into the picture here, and so

00:01:22.335 --> 00:01:24.055
we're going to do that now.

