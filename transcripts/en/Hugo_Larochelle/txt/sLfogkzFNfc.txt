Speaker 1:          00:00          In this video, we'll look at a common way of initializing the parameters of a neural network.

Speaker 1:          00:06          So we're not the final step in, uh, our recipe for obtaining the whole cicastic gradient descent training algorithm for neural net. Uh, so I have to look at the initialization methods. So how do we initialize the, uh, initial value of all of our parameters, all of our, a hidden layer weights and biases as well as the output weights and biases? Okay. Well, um, typically, uh, we often first initialized devices to a zero. Uh, so we don't express any preference for the, uh, hidden units taking values that are above or below, uh, you know, closer to the saturation point, the lower saturation point or the higher saturation point. Um, if, uh, if you're using a, a sigmoid activation function and a, if you would like most hidden needs to be close to zero, taps us more sparsity in the activation of, uh, your, uh, layers. Uh, it's sparsity something we haven't discussed so much, uh, in later videos we'll talk about that and why that might be desirable.

Speaker 1:          01:15          But if you do this, then you might want to initialize the biases to a large negative violin. Like say, you know, the bias could initially be minus 10 plus, you know, the activation that would give you the value of a unit. And so because of this, uh, if this term is not too large, initially, most units would be close to zero. So that's one way of initializing and neural network in a situation where it's initially sparse in its hidden activations. Uh, but, uh, that's for special cases where you think that's a good idea, just in general a good recipes to just initialized to zero. Um, now for the weights, uh, well let's see. First, uh, could we initialize them to zero? Well, uh, if we use the, uh, tench activation function, we can actually show that all gradients will be zero at the first time.

Speaker 1:          02:05          You compute your gradients. And so, uh, if the grain size zero, then you're updating your weights with me in the direction that the vector of zero, so you're not changing the weights. Uh, and so this essentially corresponds to settle point. You've converged, quote unquote, and uh, you can't move away from that initialization. So that's obviously a bad idea. Um, well, okay, could we, should I initialize all the weights to non zero value but to exactly that same non zero value, but it's also a bad idea. And that's because in that case we can show that all hidden units in all layers, uh, we'll always behave the same. All the Hinns will compute exactly the same, uh, uh, activation function and a, essentially all of the units there are connections with the layer below will always stay the same. And, uh, and intuitively though, we want the neural net where we have different than Munis that do different things.

Speaker 1:          03:04          And so we need to break the initial symmetry that we've, uh, that would have enforced if we had used the same identical value for all weights of all the units. And so the recipe that people use, the usually follow is that they're going to samples the cast actually the initial value for all the weights. Uh, and one thing that we propose here is to use a uniform distribution in some interval minus B two B. So, uh, around centered at zero. And the value that suggested that we propose here is the square root of six divided by the square root of the sum of the number of units in layer k and number of units in layer came on this one. Uh, and um, so that might seem like a strange formula. Uh, let's first no, this, that, um, one of assemble around zero because we want the initially small weights.

Speaker 1:          03:59          We want to start with a simple neural network that's not to nonlinear. And because the probability that I'll weights are sampled, you know, uh, get a initial value that's exactly the same as essentially zero. Uh, then we can break symmetry. So now each in the unit is going to be initialized with a slightly different, uh, initials, state initial set of weights. Um, so other values for B could work well, this is just one proposal and this isn't like, this isn't an exact science determining good initialization for their own that, uh, the parameters of the neural net can look at this paper. This is where I got this formula here. And, uh, they can show under certain conditions that the hidden layer values at the end layer activations and also the gradients are back propagated, will tend to have a similar ranges of values, similar variants across the different layers.

Speaker 1:          04:56          And that's something that, uh, means, uh, so if you have very big gradients at say the top hidden layer in small green and set the Laura hidden layer, then uh, intuitively this means that the top in layer we'll train much faster because it's, it's pushed further away from its current value because the gradients are bigger than the, uh, lower hidden there if they had those smaller gradient. And so they tried to correct for this at least initially by using this value in, they show under certain conditions that this is well behaved. All right, so this is a very reasonable, uh, initialization recipe to use and I encourage you to use that. And, uh, but, uh, as I stated, there are other papers that might suggest other values which might work well. One thing that she would want to do is perhaps use always the same formula, but at least try different seed for how you, Jay, for your random number generator that generates, uh, uh, numbers between minus B and B. And so we'll try different values and then see which one works better, uh, as a solution for, in terms of the quality of the neural net you get. All right, so this is a good recipe to follow for initializing your neural network.