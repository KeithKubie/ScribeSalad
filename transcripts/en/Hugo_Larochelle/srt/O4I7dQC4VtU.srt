1
00:00:00,450 --> 00:00:00,690
Yeah.

2
00:00:00,690 --> 00:00:04,890
In this video we'll look at what is the
capacity of a multilayer neural network.

3
00:00:07,160 --> 00:00:11,270
Now we are already seeing that and
artificial single artificial neuron was

4
00:00:11,271 --> 00:00:12,680
limited at its capacity.

5
00:00:12,681 --> 00:00:16,550
There are certain classification problems
that it cannot actually model if it's,

6
00:00:16,680 --> 00:00:19,700
if they're not linearly separable.
However,

7
00:00:19,701 --> 00:00:24,701
we've looked at an approach where we
would first learn a representation or

8
00:00:24,770 --> 00:00:29,300
transformation of our original
space, uh, in such a way that,

9
00:00:29,570 --> 00:00:34,430
uh, we could get a new
representation where the data
was then linearly separable.

10
00:00:37,100 --> 00:00:41,350
And then we've used that as
a justification for having a,

11
00:00:41,360 --> 00:00:45,070
what we call the single hidden
layer neural network, uh,

12
00:00:45,140 --> 00:00:50,000
where we would first compute a
representation that hidden representation.

13
00:00:50,001 --> 00:00:55,001
So I transformation of our input space
and hopefully in such a way that then the

14
00:00:55,400 --> 00:01:00,250
output of our neural network could
then perform its jobs, its job more,

15
00:01:00,270 --> 00:01:04,790
more, much more easily. So in the
context of classification, uh, so that,

16
00:01:04,880 --> 00:01:09,620
uh, it could actually perform
the classification, uh,

17
00:01:09,680 --> 00:01:12,290
in the, uh, original space,

18
00:01:12,740 --> 00:01:15,750
perfectly distinguishing between the,
uh,

19
00:01:15,830 --> 00:01:20,540
inputs in the class zero
on class one. Now, uh,

20
00:01:20,810 --> 00:01:25,280
can we actually do a show anything
formal about that? So, in other words,

21
00:01:25,340 --> 00:01:27,770
if we assume that we have a function of f,

22
00:01:27,830 --> 00:01:32,590
f of x that takes this particular
form that computes a hidden,

23
00:01:32,591 --> 00:01:35,870
they are pre activation
applies an activation function,

24
00:01:36,170 --> 00:01:38,670
computes and output linear combination.

25
00:01:38,671 --> 00:01:43,610
So literary combining the new
representation, linear representation, uh,

26
00:01:43,670 --> 00:01:46,580
sorry,
hidden representation of uh,

27
00:01:46,610 --> 00:01:51,610
the input and then applying
some activation function
at the output to have a

28
00:01:51,801 --> 00:01:55,190
function that performs all
of these transformations.

29
00:01:55,340 --> 00:02:00,340
What kind of functions can we actually
model with this particular form and other

30
00:02:00,411 --> 00:02:02,660
functions that we cannot model?

31
00:02:03,170 --> 00:02:06,860
So let's look at this a little bit and
try to get some intuition about around

32
00:02:06,861 --> 00:02:07,640
this problem.

33
00:02:07,640 --> 00:02:08,473
Cool.

34
00:02:09,550 --> 00:02:09,821
No,

35
00:02:09,821 --> 00:02:14,821
here's the first example where we have
a single layer neural network with two

36
00:02:16,420 --> 00:02:17,290
hidden units,

37
00:02:18,340 --> 00:02:22,330
which a compute these two functions here.

38
00:02:22,720 --> 00:02:23,553
Okay.

39
00:02:24,180 --> 00:02:28,440
Now, uh, you should be able
to convince yourself that, uh,

40
00:02:28,470 --> 00:02:32,700
add the output layer.
If you have an output learner neuron,

41
00:02:33,000 --> 00:02:38,000
then we can actually combine these two
hidden units to get a new function,

42
00:02:38,971 --> 00:02:40,560
which respect to our original input.

43
00:02:40,830 --> 00:02:42,810
That would actually look
something like this.

44
00:02:43,110 --> 00:02:46,950
We'd get a ridge that goes up
here and then that goes down here.

45
00:02:47,580 --> 00:02:52,320
And essentially the way we can get this
is by taking this neuron and subtracting

46
00:02:52,321 --> 00:02:56,040
from it. Disney Rod.
So by getting a linear,

47
00:02:56,070 --> 00:02:59,440
a combination of [inaudible]
with this neurons specifically,

48
00:02:59,680 --> 00:03:02,260
essentially this neuron minus that neuron,

49
00:03:02,560 --> 00:03:07,560
we can actually get a ridge that is a
localized in a particular range of a

50
00:03:08,470 --> 00:03:12,580
combinations of x and x one
and x two values. So indeed,

51
00:03:12,581 --> 00:03:16,570
if we're here,
then this neuron as a small value,

52
00:03:16,630 --> 00:03:19,780
but this neuron too.
So we get a small value at the output.

53
00:03:20,500 --> 00:03:25,330
Now if we're in this range,
sorry, excuse another color,

54
00:03:25,360 --> 00:03:30,070
if we are in this range,
then this neuron that has a large value,

55
00:03:30,071 --> 00:03:33,130
but there's some,
this neuron also still has a low value.

56
00:03:33,460 --> 00:03:36,730
So because of that we get a
large value minus a low value.

57
00:03:36,731 --> 00:03:39,310
So we get at the output
a fairly large value.

58
00:03:40,030 --> 00:03:44,860
And then if we reach this part here,
then this is still large,

59
00:03:44,861 --> 00:03:48,850
but this is now the second
neuron has not become larger.

60
00:03:48,970 --> 00:03:53,680
So the value at the output we'll now
drop because we're not subtracting value

61
00:03:53,700 --> 00:03:57,880
that has become larger at the
second, uh, hidden neuron.

62
00:03:58,450 --> 00:04:00,280
So we see, so this is, you know,

63
00:04:00,760 --> 00:04:03,520
intuitively how we can get
from these two simple neurons.

64
00:04:03,550 --> 00:04:05,260
A more complicated output,

65
00:04:05,530 --> 00:04:09,760
which is localized in a particular
range of x one and x two values.

66
00:04:12,880 --> 00:04:15,160
And our weekend go,
uh,

67
00:04:15,190 --> 00:04:19,420
into a context that's a bit
more complicated where we
could get for neurons that

68
00:04:19,421 --> 00:04:21,190
would compute these four functions.

69
00:04:21,250 --> 00:04:24,910
And by combining them in
a fairly similar fashion,

70
00:04:25,060 --> 00:04:29,720
we can actually get a d output, a
localized bump. So four values, uh,

71
00:04:29,980 --> 00:04:33,250
of these, uh, inputs x one and x two.

72
00:04:33,460 --> 00:04:37,180
Within this part here,
we could get an output,

73
00:04:37,210 --> 00:04:41,350
which actually is large and
a low everywhere else here.

74
00:04:41,890 --> 00:04:45,160
Okay. And then we could actually, uh,

75
00:04:45,580 --> 00:04:50,580
continue making a more complex output
by having four other neurons that would

76
00:04:51,581 --> 00:04:55,000
allow us to compute, say,
another bump maybe here,

77
00:04:55,150 --> 00:04:59,650
or maybe we could get a bump that's here
with these, uh, with four other neurons.

78
00:05:00,010 --> 00:05:03,390
So we can see that by combining
in two dimensions, these, uh,

79
00:05:03,530 --> 00:05:08,530
for neurons together and having sets of
four neurons for different parts of the

80
00:05:09,071 --> 00:05:13,630
input space, we could actually add a
bunch of different bumps in different, uh,

81
00:05:13,690 --> 00:05:16,900
parts of the space and
sort of carve out a, uh,

82
00:05:16,960 --> 00:05:18,580
more and more complicated function.

83
00:05:21,460 --> 00:05:23,430
So if we think of this,
uh,

84
00:05:23,440 --> 00:05:28,270
if we go in the sending
of a classification, this
means that we could have,

85
00:05:28,670 --> 00:05:33,610
uh, chunks of regions that
are actually separated and,

86
00:05:33,640 --> 00:05:35,740
but that correspond to the same class.

87
00:05:35,741 --> 00:05:40,741
So say that our two corresponds to
class one and our one class zero.

88
00:05:41,380 --> 00:05:44,300
Then by having uh,
uh,

89
00:05:44,320 --> 00:05:49,320
several bumps here and bumps here and
no bumps in the [inaudible] region here

90
00:05:53,710 --> 00:05:54,543
and here,

91
00:05:54,610 --> 00:05:59,610
we can actually get there
fairly complicated decision
and a fairly complicated

92
00:05:59,991 --> 00:06:01,400
decision,
uh,

93
00:06:01,490 --> 00:06:05,450
classification decision with a neural
network that could have enough,

94
00:06:06,020 --> 00:06:10,370
um, hidden units to place
bumps at the right places.

95
00:06:10,580 --> 00:06:13,820
Uh, uh, to model our
complicated decision function.

96
00:06:16,390 --> 00:06:20,480
Not can we actually be formal about
this? Uh, well it turns out we can.

97
00:06:20,481 --> 00:06:25,481
So we can show that a neural network with
a single hidden layer is actually what

98
00:06:25,821 --> 00:06:30,740
is known as a universal
approximator. Uh, so, uh,

99
00:06:30,770 --> 00:06:32,330
more specifically,

100
00:06:32,540 --> 00:06:37,280
we can say that a single hidden layer
neural network with a linear output can

101
00:06:37,281 --> 00:06:42,281
approximate any continuous function
arbitrarily well as long as it has enough

102
00:06:42,680 --> 00:06:46,430
hidden units.
So this was shown by Hornik in 1991.

103
00:06:46,460 --> 00:06:49,610
So there are more details and
technical details behind this theorem.

104
00:06:49,611 --> 00:06:52,460
So I encourage you to
consult the original paper,

105
00:06:53,390 --> 00:06:57,980
but this is essentially saying that if
you have a function which is continuous,

106
00:06:58,040 --> 00:07:01,640
so that does not vary too much,
um,

107
00:07:01,820 --> 00:07:06,140
then we can actually model it as long
as we give our neural network enough

108
00:07:06,141 --> 00:07:09,200
hidden units. So there are
configurations of these, uh,

109
00:07:09,500 --> 00:07:11,090
hidden units and the output unit,

110
00:07:11,091 --> 00:07:15,590
there are values of their weights and
biases such that we can model any function

111
00:07:15,591 --> 00:07:17,440
arbitrarily. Well, uh,

112
00:07:17,450 --> 00:07:21,680
this was all actually applies whether
you're using sigmoid hidden units or tench

113
00:07:21,770 --> 00:07:24,600
hidden units. And there
are also many other, uh,

114
00:07:24,710 --> 00:07:28,610
hidden layer activation functions
that, uh, makes this result true that,

115
00:07:28,611 --> 00:07:30,080
that complies with this result.

116
00:07:30,890 --> 00:07:35,660
And so what this is saying is that
by using a neural networks, we,

117
00:07:35,730 --> 00:07:36,170
uh,

118
00:07:36,170 --> 00:07:41,170
are actually allowing ourselves to model
many types of arbitrarily complicated,

119
00:07:41,930 --> 00:07:46,190
fairly arbitrarily complicated function.
So this is a pretty good result.

120
00:07:46,820 --> 00:07:51,820
Now notice that it actually doesn't
mean that there is a learning algorithm.

121
00:07:52,610 --> 00:07:57,270
So an algorithm that would find a
behavior, the parameters of these, uh,

122
00:07:57,280 --> 00:08:02,240
of all the units in the neural network
that would actually, uh, allow us to,

123
00:08:02,510 --> 00:08:06,800
uh, um, model any function.
So in other words,

124
00:08:07,320 --> 00:08:11,030
uh, there might exists values
of the connections, uh,

125
00:08:11,100 --> 00:08:15,410
weights and the biases for
a neural network with enough
hidden units that would

126
00:08:15,411 --> 00:08:17,660
allow us to model any functional HR well,

127
00:08:17,750 --> 00:08:21,380
but it doesn't mean that we actually have
the learning algorithm that will find

128
00:08:21,381 --> 00:08:25,340
the values of these connections
and biases for any given function.

129
00:08:26,150 --> 00:08:28,690
And, uh, this is, uh, an in fact,

130
00:08:28,710 --> 00:08:31,820
there aren't really any known algorithm
that can do this for any types of

131
00:08:31,821 --> 00:08:35,240
function. And, uh, we'll see that, uh,

132
00:08:35,270 --> 00:08:39,950
actually this is a whole field of research
trying to develop more and more, uh,

133
00:08:39,980 --> 00:08:43,520
improve learning algorithms
that can learn, uh,

134
00:08:43,550 --> 00:08:48,550
more and more types of functions and
more specifically that can solve more and

135
00:08:48,591 --> 00:08:51,740
more, uh, types of, uh,

136
00:08:51,800 --> 00:08:54,680
problems usually related
to artificial intelligence.

