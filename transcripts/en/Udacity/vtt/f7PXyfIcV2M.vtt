WEBVTT
Kind: captions
Language: en

00:00:00.750 --> 00:00:03.774
So meta reasoning can be very difficult
to talk about because it's circular.

00:00:03.774 --> 00:00:07.391
So it's often unclear what exactly
we're talking about at a given time.

00:00:07.391 --> 00:00:10.200
So to try to make this process more
explicit, let's take an example.

00:00:11.290 --> 00:00:14.988
Let's imagine a robot whose task is
to assemble a camera, and all day,

00:00:14.988 --> 00:00:18.455
every day, the robot sits there
assembling camera after camera.

00:00:18.455 --> 00:00:21.681
Then one day, the boss walks in and
says instead of assembling that camera

00:00:21.681 --> 00:00:24.401
disassemble this camera,
it's already been put together.

00:00:24.401 --> 00:00:25.744
So what does the robot do?

00:00:25.744 --> 00:00:28.240
It wasn't programmed to
disassemble cameras.

00:00:28.240 --> 00:00:30.630
So how does it figure
out how to do this task?

00:00:30.630 --> 00:00:32.930
Well, the robot might have a rule,
that says given a new goal or

00:00:32.930 --> 00:00:34.160
given a new problem.

00:00:34.160 --> 00:00:37.290
Choose the method for
which I have the most knowledge.

00:00:37.290 --> 00:00:40.100
The robot might then look at its
memory and say, well, the only thing

00:00:40.100 --> 00:00:44.230
I have is a bunch of cases of assembling
camera after camera, after camera.

00:00:44.230 --> 00:00:47.640
So this is Metacognition, the robot has
now thought about it what it knows,

00:00:47.640 --> 00:00:49.320
it's thought about its new goal, and

00:00:49.320 --> 00:00:52.530
it's decided on a method to use
to approach this new problem.

00:00:52.530 --> 00:00:55.930
But now that the robot has selected a
method, it then needs to reason further.

00:00:55.930 --> 00:00:58.720
Its chosen case based reasoning
of its reasoning method.

00:00:58.720 --> 00:01:00.650
But how will it do the adaptation?

00:01:00.650 --> 00:01:04.110
Well, the robot might then select
a rule out of its memory, it says,

00:01:04.110 --> 00:01:07.270
to reverse the process
do the steps in reverse.

00:01:07.270 --> 00:01:08.960
So now we've done,
not only strategy selection,

00:01:08.960 --> 00:01:11.930
when we select a case based reasoning,
but we've done strategy integration.

00:01:13.090 --> 00:01:17.213
It is now using a rule based approach,
to take care of the case adaptation.

00:01:17.213 --> 00:01:21.557
So, the robot uses that rule, reverses
the steps, and dissembles the camera.

00:01:21.557 --> 00:01:24.854
Now the boss comes back by and says,
well, yeah, you disassembled the camera,

00:01:24.854 --> 00:01:25.982
but why did it take so long?

00:01:25.982 --> 00:01:27.970
You could have done it so much faster.

00:01:27.970 --> 00:01:29.710
But the boss doesn't give
any feedback as to how,

00:01:29.710 --> 00:01:31.390
the robot could have done it faster.

00:01:31.390 --> 00:01:33.650
Then later,
the boss brings back another camera and

00:01:33.650 --> 00:01:37.000
says here, disassemble this camera,
but do it faster this time.

00:01:37.000 --> 00:01:39.370
So now the robot needs to
select a method again.

00:01:39.370 --> 00:01:43.900
It thinks, last time, I used case-based
reasoning to select my method for

00:01:43.900 --> 00:01:44.852
disassembling the camera.

00:01:44.852 --> 00:01:48.290
But case-based reasoning didn't
give me an optimal solution,

00:01:48.290 --> 00:01:50.150
the boss complained it took me too long.

00:01:50.150 --> 00:01:55.320
So instead, the robot chunks a rule that
says, given a new task of disassembling

00:01:55.320 --> 00:01:59.450
a camera, suggest strategies
other than case-based reasoning.

00:01:59.450 --> 00:02:01.440
So now,
it's used a form of meta Metacognition,

00:02:01.440 --> 00:02:03.320
it's thought about its Metacognition.

00:02:03.320 --> 00:02:06.170
It's looked at the way it selected
a strategy in the past and

00:02:06.170 --> 00:02:09.330
decided the way I selected a strategy
in the past must not have worked,

00:02:09.330 --> 00:02:11.770
because the strategy I
selected wasn't ideal.

00:02:11.770 --> 00:02:13.730
So next time select
a different strategy.

00:02:14.830 --> 00:02:16.910
So here we've seen everything we've
talked about today in action.

00:02:17.940 --> 00:02:20.010
First, we saw using
Metacognition to resolve a gap.

00:02:21.090 --> 00:02:23.770
The agent didn't have any way
of disassembling a camera so

00:02:23.770 --> 00:02:26.460
used to select a strategy for
resolving that gap.

00:02:26.460 --> 00:02:29.220
That's also strategy selection,
it use case-based reasoning

00:02:29.220 --> 00:02:32.380
to decide what method it should
use to resolve this gap.

00:02:32.380 --> 00:02:36.270
We then saw strategy integration where
we use the rule-based approach to adapt

00:02:36.270 --> 00:02:37.880
its prior cases.

00:02:37.880 --> 00:02:40.470
We didn't thought correcting mistakes
where the boss told the robot they

00:02:40.470 --> 00:02:41.950
didn't do it fast enough, so

00:02:41.950 --> 00:02:43.740
its reasoning method must
have been suboptimal.

00:02:43.740 --> 00:02:45.970
So next time if you try
something different.

00:02:45.970 --> 00:02:48.300
Now, last part is also an example
of Meta Metacognition or

00:02:48.300 --> 00:02:50.260
metacognition about Metacognition.

00:02:50.260 --> 00:02:53.000
It looked at the strategy selection
method and decided next time,

00:02:53.000 --> 00:02:54.560
I should select a strategy differently.

00:02:55.600 --> 00:03:00.340
&gt;&gt; David's example of a robot that knows
how to assemble cameras but then is

00:03:00.340 --> 00:03:05.639
given the goal of disassembling a camera
is a good example of goal base autonomy.

00:03:05.639 --> 00:03:08.839
Earlier we had looked at, how an agent
can go about repairing this knowledge

00:03:08.839 --> 00:03:11.791
a reasonable or learning when it
makes a mistake or reaches a failure.

00:03:11.791 --> 00:03:16.629
But sometimes it is not so much that
agent reaches a failure as much as it

00:03:16.629 --> 00:03:19.310
is that it is given a new goal.

00:03:19.310 --> 00:03:21.114
When the agent is given a new goal,

00:03:21.114 --> 00:03:24.985
we do not want the agent to just fall
apart, we do not want brittle agents.

00:03:24.985 --> 00:03:28.405
We want agents that can then
adapt their reasoning methods and

00:03:28.405 --> 00:03:31.439
their learning methods to
try to achieve the new goal.

00:03:31.439 --> 00:03:35.520
Even if they were not necessarily
programmed to achieve that goal for the.

00:03:35.520 --> 00:03:38.830
We know that human cognition
is really robust and flexible.

00:03:38.830 --> 00:03:41.342
You and
I address a very large number of task,

00:03:41.342 --> 00:03:45.570
a very large number of problems and
achieve a very large number of goals.

00:03:45.570 --> 00:03:49.070
If you had to design human level,
human like AI agents and

00:03:49.070 --> 00:03:52.380
those AI agents will have to be
equally robust and flexible.

00:03:52.380 --> 00:03:56.190
Metacognition provides a powerful
way of achieving that robustness and

00:03:56.190 --> 00:03:57.200
flexibility.

00:03:57.200 --> 00:04:03.080
It does so, by flexibly, dynamically
selecting among competing strategies.

00:04:03.080 --> 00:04:05.310
It does so, by flexibility and

00:04:05.310 --> 00:04:09.900
dynamically integrating multiple
strategies as a problem solving evolves.

00:04:09.900 --> 00:04:12.550
It does so,
by using reasoning strategies and

00:04:12.550 --> 00:04:16.050
knowledge that were programmed
into it to achieve new goals

