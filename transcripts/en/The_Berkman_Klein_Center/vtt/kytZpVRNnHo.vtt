WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.909
My name's Chris Bavitz, I'm a clinical
professor of law at Harvard Law School.

00:00:02.909 --> 00:00:06.569
I'm one of the faculty co-directors at
the Berkman Klein Center for Internet

00:00:06.569 --> 00:00:10.469
and Society. We're really focused on
specific situations where the government

00:00:10.469 --> 00:00:15.179
is using algorithmic tools. Are we doing
so thoughtfully? Are we understanding the

00:00:15.179 --> 00:00:20.210
ramifications? Are we satisfied with the
level of transparency or the level of

00:00:20.210 --> 00:00:24.240
interpretability of a particular outcome from one of

00:00:24.240 --> 00:00:29.550
these tools? The most obvious place where this is happening is in the

00:00:29.550 --> 00:00:34.140
criminal justice system with respect to
tools that help support decisions around

00:00:34.140 --> 00:00:38.969
things like sentencing, bail, and parole.
These are decisions that have

00:00:38.969 --> 00:00:43.379
traditionally been left to judges to
make, weighing factors related to the

00:00:43.379 --> 00:00:47.070
defendant that's before them. In the
context of a decision about bail for

00:00:47.070 --> 00:00:50.250
example, whether the defendant is likely to flee the

00:00:50.250 --> 00:00:56.640
jurisdiction if we let him or her go
after arrest and before their trial. How

00:00:56.640 --> 00:00:59.579
dangerous they are, whether they're
likely to cause harm to the community if

00:00:59.579 --> 00:01:03.930
we release them. Some of those decisions now are being, I don't want to say

00:01:03.930 --> 00:01:06.950
they're being "handed over" to these
technical tools, but they're being

00:01:06.950 --> 00:01:11.030
facilitated by risk scores that are
being given to defendants.

00:01:11.030 --> 00:01:17.000
And I think that there is enormous
potential in the use of these kinds of

00:01:17.000 --> 00:01:22.759
tools to help eliminate some of the
biases that creep in when we have human

00:01:22.759 --> 00:01:26.780
decision-makers who themselves are
biased, or making decisions about

00:01:26.780 --> 00:01:32.390
defendants in the criminal justice
system. I also think that if designed

00:01:32.390 --> 00:01:37.520
poorly or designed without adequate
consideration to context there's a risk

00:01:37.520 --> 00:01:43.550
that we might actually entrench existing
biases and in fact make it harder to get

00:01:43.550 --> 00:01:47.690
at those biases, make these decisions
less transparent, less understandable,

00:01:47.690 --> 00:01:52.190
both to the defendants and to society as
a whole. We're starting out by just

00:01:52.190 --> 00:01:55.490
trying to gather as much information as
we can about these tools in the way

00:01:55.490 --> 00:02:01.429
they're being used out there in the
world. We're also doing a bunch of legal

00:02:01.429 --> 00:02:07.700
research and policy analysis kinds of
projects that relate to figuring out

00:02:07.700 --> 00:02:12.050
good ways, reasonable ways, to regulate
the use of these tools so we're trying

00:02:12.050 --> 00:02:16.670
to think about whether there are
existing regulatory models out there

00:02:16.670 --> 00:02:21.079
that might be helpful when the question
comes up "how can we regulate government

00:02:21.079 --> 00:02:26.060
use of these algorithms?" Do we want them to be subject to some sort of government

00:02:26.060 --> 00:02:30.650
oversight? Do we want to be providing the
government actors and procure these

00:02:30.650 --> 00:02:33.860
tools with some sets of best practices
or standards that they should be

00:02:33.860 --> 00:02:40.010
following when they're facing an
invitation from a vendor to purchase one

00:02:40.010 --> 00:02:45.890
of these tools? What kinds of questions
should they be asking? And we're also

00:02:45.890 --> 00:02:50.630
trying to do some work on the deployment side. So we talked about sort of the

00:02:50.630 --> 00:02:54.000
development, the procurement of the tools by the

00:02:54.000 --> 00:02:58.110
government officers who use them, and
then the way that they're deployed in

00:02:58.110 --> 00:03:03.030
courts, typically a judge who actually
has in front of of him or her a risk

00:03:03.030 --> 00:03:07.110
score that he or she can consider in
making a particular decision, we're

00:03:07.110 --> 00:03:10.410
trying to examine the ways in which judges are

00:03:10.410 --> 00:03:19.010
using these scores and make sure that
they're being used fairly, reasonably. That the parties that actually make use of them understand what they mean.

00:03:19.010 --> 00:03:28.590
That, for example, a factor that went into the assessment of a risk score does not then

00:03:28.590 --> 00:03:32.040
get double counted by the judge who goes ahead and considers that same factor

00:03:32.040 --> 00:03:36.180
again as part of the analysis. That the
judge understands that the risk score is

00:03:36.180 --> 00:03:40.170
accomplishing these certain goals and
that the judge is then supposed to

00:03:40.170 --> 00:03:43.560
exercise his or her judgment with
respect to a set of other goals and kind

00:03:43.560 --> 00:03:46.709
of trying to map the system so that we
make sure judges are understanding where

00:03:46.709 --> 00:03:50.820
these are being used. By using the use of
these tools in the criminal justice

00:03:50.820 --> 00:03:55.230
system as our case study we could get
at a bunch of other questions about

00:03:55.230 --> 00:03:58.830
places where government might use these tools in general when it comes again to

00:03:58.830 --> 00:04:03.720
allocating resources, deploying law
enforcement personnel or health

00:04:03.720 --> 00:04:09.989
inspectors, helping to draw voting
districts, those kinds of things. So I

00:04:09.989 --> 00:04:13.650
think we're using the criminal justice piece again as a bit of

00:04:13.650 --> 00:04:16.830
a case study to try and get at these
broader questions, all of which have in

00:04:16.830 --> 00:04:22.040
common the fact that it's a government
that's making use of these tools.

