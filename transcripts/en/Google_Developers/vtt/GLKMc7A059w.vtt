WEBVTT
Kind: captions
Language: en

00:00:12.307 --> 00:00:13.640
MIKE DAHLIN: So I'm Mike Dahlin.

00:00:13.640 --> 00:00:15.880
I'm an engineer working
on Google Cloud Platform.

00:00:15.880 --> 00:00:18.580
And I'd like to take a few
minutes to talk about some

00:00:18.580 --> 00:00:22.880
of the things that makes
our cloud stand apart.

00:00:22.880 --> 00:00:24.850
The common theme you're
going to hear today

00:00:24.850 --> 00:00:27.389
is that we're built on Google.

00:00:27.389 --> 00:00:29.430
This means that being an
engineer on this project

00:00:29.430 --> 00:00:31.270
is a fun job.

00:00:31.270 --> 00:00:33.566
To take-- to having a
great global network that

00:00:33.566 --> 00:00:34.940
will go to the
ends of the earth,

00:00:34.940 --> 00:00:37.940
will go even beyond
the ends of the earth,

00:00:37.940 --> 00:00:40.440
put lasers down--
and fiber-- down

00:00:40.440 --> 00:00:42.380
with where the
sharks are to ensure

00:00:42.380 --> 00:00:44.660
that we have that great
global connectivity

00:00:44.660 --> 00:00:48.090
that so many of our
products depend on.

00:00:48.090 --> 00:00:48.725
So number nine.

00:00:52.000 --> 00:00:55.240
Let's see if this will advance.

00:00:55.240 --> 00:00:57.490
Free, fast
connections to Google.

00:00:57.490 --> 00:01:00.250
So when you build on
Google's Cloud platform,

00:01:00.250 --> 00:01:04.390
one of the great features you
get is the rest of Google.

00:01:04.390 --> 00:01:08.340
We give you high performance,
low latency, high bandwidth,

00:01:08.340 --> 00:01:12.620
and no charge connection
to other Google APIs.

00:01:12.620 --> 00:01:18.000
Other Google services like Ad
Exchange, YouTube, Drive, Docs,

00:01:18.000 --> 00:01:21.940
apps, the Maps API.

00:01:21.940 --> 00:01:24.780
This makes Google a great
place to build real time

00:01:24.780 --> 00:01:27.364
bidding, a great place to
build video processing,

00:01:27.364 --> 00:01:29.655
where you can do your processing
and upload immediately

00:01:29.655 --> 00:01:32.200
to YouTube, a great
place to build

00:01:32.200 --> 00:01:34.390
productivity apps,
interactive apps,

00:01:34.390 --> 00:01:36.790
collaborate and work
with Docs and Drive,

00:01:36.790 --> 00:01:39.590
a great place to build
geolocated services.

00:01:39.590 --> 00:01:42.230
So there's a wide range
of other Google properties

00:01:42.230 --> 00:01:44.460
and Google services
that can make your cloud

00:01:44.460 --> 00:01:47.290
applications better.

00:01:47.290 --> 00:01:48.720
Number eight.

00:01:48.720 --> 00:01:50.920
Google Scale.

00:01:50.920 --> 00:01:52.940
So Google understands scale.

00:01:52.940 --> 00:01:55.260
And when you build on
Google Cloud Platform,

00:01:55.260 --> 00:01:58.620
you get to take advantage
of the technologies we've

00:01:58.620 --> 00:02:03.130
built that allow us to
deliver services at scale.

00:02:03.130 --> 00:02:06.720
Not to brag, but what you might
regard as a success disaster is

00:02:06.720 --> 00:02:09.214
likely something that
we handle every day.

00:02:09.214 --> 00:02:12.650
As an example of this,
we ran an experiment

00:02:12.650 --> 00:02:17.430
where we set up on the cloud
platform a bunch of servers.

00:02:17.430 --> 00:02:20.320
We put a load balancer in
front of those servers.

00:02:20.320 --> 00:02:22.840
That load balancer is
the same infrastructure

00:02:22.840 --> 00:02:26.380
that we use to deliver
our other services,

00:02:26.380 --> 00:02:28.310
so it knows how to scale.

00:02:28.310 --> 00:02:30.770
We then fired up a
bunch of clients.

00:02:30.770 --> 00:02:34.030
And we just had them start
blasting at the server

00:02:34.030 --> 00:02:35.200
as hard as they could.

00:02:35.200 --> 00:02:38.490
They were delivering a
million requests per second.

00:02:38.490 --> 00:02:40.680
So with no pre-warming,
no warning,

00:02:40.680 --> 00:02:44.320
just turned on the load balancer
and fired up the clients.

00:02:44.320 --> 00:02:46.990
Within four seconds, those
clients were ramped up.

00:02:46.990 --> 00:02:48.990
They're firing their
million requests per second

00:02:48.990 --> 00:02:53.719
at the server, and the load
balancer is handling it.

00:02:53.719 --> 00:02:55.510
After about a minute,
there is a little bit

00:02:55.510 --> 00:02:57.470
of an adjustment as
things rebalance.

00:02:57.470 --> 00:02:59.080
That all steadies out.

00:02:59.080 --> 00:03:01.880
Even during that time, you're
getting very good throughput.

00:03:01.880 --> 00:03:03.070
That all steadies out.

00:03:03.070 --> 00:03:06.120
And by two minutes, things
have gotten completely boring.

00:03:06.120 --> 00:03:06.620
Right?

00:03:06.620 --> 00:03:10.140
And the million requests
per second just continue on.

00:03:10.140 --> 00:03:13.960
This entire experiment
cost less than $10.

00:03:13.960 --> 00:03:15.810
That was prior to
the price drop,

00:03:15.810 --> 00:03:17.715
so it would cost
even less today.

00:03:17.715 --> 00:03:19.590
Which means that this
technology is something

00:03:19.590 --> 00:03:22.630
that really anyone can have.

00:03:22.630 --> 00:03:24.680
Now, not everyone
needs to handle

00:03:24.680 --> 00:03:25.930
a million requests per second.

00:03:25.930 --> 00:03:27.670
In fact, that's
fairly unusual to do.

00:03:27.670 --> 00:03:31.400
The way to think of this as
an insurance policy, at least

00:03:31.400 --> 00:03:33.282
for most applications.

00:03:33.282 --> 00:03:35.240
You may not need a million
requests per second,

00:03:35.240 --> 00:03:38.310
but you may not know exactly
what you're going to need.

00:03:38.310 --> 00:03:41.030
And so what you can do is
build on Google Cloud Platform

00:03:41.030 --> 00:03:45.410
and know that the infrastructure
is capable of handling whatever

00:03:45.410 --> 00:03:47.896
you throw at it, whatever
your customers throw at you,

00:03:47.896 --> 00:03:49.520
you've got an underlying
infrastructure

00:03:49.520 --> 00:03:52.420
that knows how to scale.

00:03:52.420 --> 00:03:55.540
An example of this,
we saw last year

00:03:55.540 --> 00:03:58.740
at the Eurovision
song competition.

00:03:58.740 --> 00:04:01.350
Our friends at Scalar
worked with Eurovision.

00:04:01.350 --> 00:04:04.920
What they wanted-- so Eurosong,
if you're not familiar with it,

00:04:04.920 --> 00:04:08.094
is one of the most popular
television events in the world.

00:04:08.094 --> 00:04:10.010
Because if you don't
look at sporting events--

00:04:10.010 --> 00:04:11.218
sporting events are very big.

00:04:11.218 --> 00:04:15.160
But then behind sporting events,
this is really right up there.

00:04:15.160 --> 00:04:19.560
They see about 125
million viewers each year.

00:04:19.560 --> 00:04:21.010
It's a big event.

00:04:21.010 --> 00:04:25.280
They decided last year to start
an app for a second screen

00:04:25.280 --> 00:04:26.810
experience.

00:04:26.810 --> 00:04:29.010
But they face this problem.

00:04:29.010 --> 00:04:31.340
How many users are they
going to get from the app?

00:04:31.340 --> 00:04:34.180
How much load are they
going to get from the app?

00:04:34.180 --> 00:04:36.725
You've got 125 million viewers.

00:04:36.725 --> 00:04:38.800
Are you going to see
100,000 users, a million

00:04:38.800 --> 00:04:39.960
users, 10 million users?

00:04:39.960 --> 00:04:42.220
It's hard to know ahead of time.

00:04:42.220 --> 00:04:44.240
You're launching a
brand new feature.

00:04:44.240 --> 00:04:46.530
Really hard to predict
what's going to happen.

00:04:46.530 --> 00:04:49.250
So they took advantage of
the flexibility of the cloud

00:04:49.250 --> 00:04:51.960
platform, the scalability
of the cloud platform,

00:04:51.960 --> 00:04:55.040
to deal with the fact
that they had uncertainty

00:04:55.040 --> 00:04:57.255
about what they were
going to need to do.

00:04:57.255 --> 00:04:58.630
Because they're
on this platform,

00:04:58.630 --> 00:05:00.338
they were able to deal
with, for example,

00:05:00.338 --> 00:05:02.200
in one phase of the
competition, they

00:05:02.200 --> 00:05:04.530
saw load spikes of about 5x
what they were originally

00:05:04.530 --> 00:05:05.386
anticipating.

00:05:05.386 --> 00:05:06.760
And that wasn't
really a problem.

00:05:06.760 --> 00:05:08.260
They could just scale
up the extra capacity.

00:05:08.260 --> 00:05:09.280
It took a couple
minutes, and then they're

00:05:09.280 --> 00:05:11.670
serving everything
that they-- they were

00:05:11.670 --> 00:05:13.200
able to handle that load.

00:05:13.200 --> 00:05:17.670
By the finals, they were dealing
with 50,000 requests per second

00:05:17.670 --> 00:05:22.460
and serving those requests
with a 99 percentile latency

00:05:22.460 --> 00:05:24.640
of under 35 milliseconds.

00:05:24.640 --> 00:05:27.191
So really great performance,
great scalability.

00:05:27.191 --> 00:05:28.940
And they were able to
successfully deliver

00:05:28.940 --> 00:05:29.815
this new application.

00:05:33.010 --> 00:05:34.820
Let's go to number seven.

00:05:34.820 --> 00:05:37.420
Google's Cloud is green.

00:05:37.420 --> 00:05:41.330
So this might matter to you
because you're green too,

00:05:41.330 --> 00:05:43.950
and you'd like to have
a cloud provider that

00:05:43.950 --> 00:05:45.500
thinks about these things.

00:05:45.500 --> 00:05:47.600
Nothing else that you don't have
to worry about making your data

00:05:47.600 --> 00:05:48.140
center green.

00:05:48.140 --> 00:05:49.640
Let Google worry
about these things.

00:05:49.640 --> 00:05:52.480
Let Google worry about
being very efficient

00:05:52.480 --> 00:05:55.690
and finding green
sources of energy.

00:05:55.690 --> 00:05:58.024
Green is something that's
very deep in our values,

00:05:58.024 --> 00:06:00.190
and it's something that
we've worked on and invested

00:06:00.190 --> 00:06:01.725
heavily in over time.

00:06:01.725 --> 00:06:05.170
As an example of that,
this upper right picture

00:06:05.170 --> 00:06:08.340
is from the Ivan Pa
solar thermal plant

00:06:08.340 --> 00:06:11.700
that just went online earlier
this year in the Mojave Desert.

00:06:11.700 --> 00:06:16.720
Ivan Pa is a 392 megawatt
generating plant.

00:06:16.720 --> 00:06:21.020
So that's enough green energy
to power over 100,000 homes

00:06:21.020 --> 00:06:23.555
or dozens of 10
megawatt data centers.

00:06:26.640 --> 00:06:29.580
Another example here, this
is one of the wind farms

00:06:29.580 --> 00:06:33.299
that we source some of
our renewable energy from.

00:06:33.299 --> 00:06:34.340
There's a bunch of these.

00:06:34.340 --> 00:06:37.620
You can go to our website to
see more examples of that.

00:06:37.620 --> 00:06:40.700
As just another data
point, Google as a company

00:06:40.700 --> 00:06:44.376
has been carbon
neutral since 2007.

00:06:44.376 --> 00:06:46.000
So something that's
deep in our values,

00:06:46.000 --> 00:06:48.140
we think it's very important.

00:06:48.140 --> 00:06:51.129
But being green is
all well and good,

00:06:51.129 --> 00:06:52.920
but at the end of the
day, some people just

00:06:52.920 --> 00:06:54.362
have green eye shades.

00:06:54.362 --> 00:06:55.820
And my argument is
that this should

00:06:55.820 --> 00:06:59.110
matter for the
accountants as well.

00:06:59.110 --> 00:07:01.530
That by putting these types
of investments in place,

00:07:01.530 --> 00:07:04.100
we're able to lower
our cost of energy.

00:07:04.100 --> 00:07:06.530
We're able to insulate
ourselves from volatility

00:07:06.530 --> 00:07:07.400
in energy prices.

00:07:07.400 --> 00:07:10.150
We're able to insulate ourselves
from potential increases

00:07:10.150 --> 00:07:12.510
in the cost of carbon over time.

00:07:12.510 --> 00:07:15.340
And so this is the
type of thing that

00:07:15.340 --> 00:07:17.610
has allowed us to
reduce our costs

00:07:17.610 --> 00:07:19.000
and pass those
savings on to you,

00:07:19.000 --> 00:07:22.750
as you saw earlier today
as we dropped our prices.

00:07:22.750 --> 00:07:27.700
Let me give a concrete example
of how the accountants should

00:07:27.700 --> 00:07:31.900
love a green cloud
as much as we do.

00:07:31.900 --> 00:07:36.300
So what this graph shows is the
power utilization effectiveness

00:07:36.300 --> 00:07:39.870
across Google's data centers,
across our entire fleet.

00:07:39.870 --> 00:07:43.470
Shows that year by
year since 2008,

00:07:43.470 --> 00:07:44.900
power utilization
effectiveness is

00:07:44.900 --> 00:07:46.790
a measure of data
center efficiency.

00:07:46.790 --> 00:07:51.360
What it measures is to
deliver one watt of power

00:07:51.360 --> 00:07:54.110
to the compute
infrastructure, how many watts

00:07:54.110 --> 00:07:57.150
do you need to actually
put into the data center.

00:07:57.150 --> 00:08:01.770
So in 2013, our power
utilization effectiveness,

00:08:01.770 --> 00:08:05.940
our PUE, was 1.12 over the year.

00:08:05.940 --> 00:08:10.450
That meant that over 88% of the
energy that went into a data

00:08:10.450 --> 00:08:13.380
center actually went to
the compute, the storage,

00:08:13.380 --> 00:08:16.440
and the networks, the things
that you actually care about.

00:08:16.440 --> 00:08:20.470
Less than 12% was
spread across cooling,

00:08:20.470 --> 00:08:23.420
across electrical
transformation,

00:08:23.420 --> 00:08:26.020
across electrical
transmission losses,

00:08:26.020 --> 00:08:29.029
even across keeping the
lights on in the data centers.

00:08:29.029 --> 00:08:30.820
So this is a pretty
remarkable achievement.

00:08:30.820 --> 00:08:33.760
If you look, for
example, in 2012,

00:08:33.760 --> 00:08:35.350
that's the last
year I've got data,

00:08:35.350 --> 00:08:40.200
the average across the
industry, the PUE was about 1.8.

00:08:40.200 --> 00:08:43.099
So on average, in industries,
and probably those of you

00:08:43.099 --> 00:08:44.640
that walk into a
typical data center,

00:08:44.640 --> 00:08:47.280
the big crack units are running,
this is not too surprising,

00:08:47.280 --> 00:08:50.270
on average almost half
of the electricity that

00:08:50.270 --> 00:08:55.176
goes into a data center goes to
these other types of overheads.

00:08:55.176 --> 00:08:56.550
Google is able to
drive that cost

00:08:56.550 --> 00:08:59.112
down through really long
term investments and building

00:08:59.112 --> 00:09:01.320
state of the art data centers,
has been able to drive

00:09:01.320 --> 00:09:05.760
that down to 1.12, where
88% or better of the energy

00:09:05.760 --> 00:09:07.890
is actually going
to what you want.

00:09:10.850 --> 00:09:12.280
Number six.

00:09:12.280 --> 00:09:14.550
Supercharged disks.

00:09:14.550 --> 00:09:18.990
So physical disk
drives really haven't

00:09:18.990 --> 00:09:22.884
changed much since the late
1950s when they were invented.

00:09:22.884 --> 00:09:24.300
You've got some
spinning platters,

00:09:24.300 --> 00:09:26.470
you've got an arm that
you move back and forth.

00:09:26.470 --> 00:09:29.740
They're getting bigger, they're
not getting much faster.

00:09:29.740 --> 00:09:33.224
Moore's law is great for
moving and storing electrons,

00:09:33.224 --> 00:09:34.640
but it doesn't
really do much when

00:09:34.640 --> 00:09:38.460
you need to move a piece
of metal to read your data.

00:09:38.460 --> 00:09:40.300
And as disks have
gotten bigger, you

00:09:40.300 --> 00:09:41.740
get fewer I/Os per gigabyte.

00:09:41.740 --> 00:09:45.770
Your data becomes trapped
inside of these disks.

00:09:45.770 --> 00:09:50.330
PD, Persistent Disk, is the disk
drive system, the block drive

00:09:50.330 --> 00:09:53.600
system that we provide as
part of Compute Engine.

00:09:53.600 --> 00:09:56.620
Instead of relying
directly on this kind

00:09:56.620 --> 00:09:59.910
of ancient technology,
on the left,

00:09:59.910 --> 00:10:02.720
it's built on
Colossus and Spanner.

00:10:02.720 --> 00:10:08.500
Colossus and Spanner are two
of the large scale data storage

00:10:08.500 --> 00:10:12.140
systems that we've built inside
of Google for storing lots

00:10:12.140 --> 00:10:14.560
of data, spreading it
across a bunch of disks,

00:10:14.560 --> 00:10:18.850
and reorganizing that data
for more efficient access.

00:10:18.850 --> 00:10:21.950
As an example of what
you get by moving

00:10:21.950 --> 00:10:26.350
from the old physical device
to our new virtual device,

00:10:26.350 --> 00:10:28.650
if you take a two terabyte
physical hard drive

00:10:28.650 --> 00:10:31.940
and compare it to a two terabyte
persistent disk, both of them

00:10:31.940 --> 00:10:34.350
can get very good sequential
read and write bandwidth.

00:10:34.350 --> 00:10:37.250
If you're not having to move the
metal, you can do pretty well.

00:10:37.250 --> 00:10:39.290
But as soon as you start
having to move metal

00:10:39.290 --> 00:10:42.812
to read your data, to do
random reads and random writes,

00:10:42.812 --> 00:10:44.270
traditional disks
really fall down.

00:10:44.270 --> 00:10:48.500
You can get dozens, maybe
75 read and write IOPS.

00:10:48.500 --> 00:10:52.700
Whereas on PD, with this
ability to reorganize

00:10:52.700 --> 00:10:54.620
the data for more
efficient access,

00:10:54.620 --> 00:11:00.060
with this ability to paralyze
the access across large numbers

00:11:00.060 --> 00:11:04.611
of underlying physical devices,
you can do 8 to 32 times

00:11:04.611 --> 00:11:05.110
better.

00:11:09.080 --> 00:11:12.190
So other advantages
of our PD device

00:11:12.190 --> 00:11:15.560
virtualizing this old
technology, first of all,

00:11:15.560 --> 00:11:18.204
you can move PD between
machines easily.

00:11:18.204 --> 00:11:20.870
So you don't have to pull up the
disk and put it somewhere else.

00:11:20.870 --> 00:11:23.770
You send a command, you hook
it up to another machine.

00:11:23.770 --> 00:11:26.580
You can actually simultaneously
mount a PD device

00:11:26.580 --> 00:11:28.560
across multiple
virtual machines.

00:11:28.560 --> 00:11:31.690
So in this example, we've got a
web service, a bunch of clients

00:11:31.690 --> 00:11:33.690
are accessing it, we've
got a load balancer that

00:11:33.690 --> 00:11:36.600
spreads the load across a bunch
of Compute Engine front ends.

00:11:36.600 --> 00:11:40.150
They're all sharing the
same backend storage

00:11:40.150 --> 00:11:42.350
of all the data they're
actually serving.

00:11:42.350 --> 00:11:44.097
This is nice because
it saves you money.

00:11:44.097 --> 00:11:45.680
You only have to pay
for the data once

00:11:45.680 --> 00:11:47.060
to store the data once.

00:11:47.060 --> 00:11:50.410
And it also can simplify
your administration.

00:11:50.410 --> 00:11:53.007
You do one update, and
all your different servers

00:11:53.007 --> 00:11:53.840
can see that update.

00:11:53.840 --> 00:11:57.830
You don't have to keep a
bunch of copies in sync.

00:11:57.830 --> 00:11:59.670
Another nice thing
about PD is that there's

00:11:59.670 --> 00:12:01.432
no separate charge for IOs.

00:12:01.432 --> 00:12:03.890
This goes to the theme of trying
to give you a simple model

00:12:03.890 --> 00:12:05.670
to think about pricing.

00:12:05.670 --> 00:12:08.030
So we think this
makes sense that when

00:12:08.030 --> 00:12:12.600
you buy a physical disk drive,
you get not just the platters,

00:12:12.600 --> 00:12:13.750
but you also get the arms.

00:12:13.750 --> 00:12:15.220
You don't have to buy
the arms separately.

00:12:15.220 --> 00:12:16.980
We're going to do the
same thing with PD.

00:12:16.980 --> 00:12:20.322
When you buy PD, when you
rent PD, you get the IOs.

00:12:20.322 --> 00:12:22.030
There's not going to
be a separate charge

00:12:22.030 --> 00:12:24.540
for actually using those arms.

00:12:24.540 --> 00:12:26.940
This is both simpler
for you, but also

00:12:26.940 --> 00:12:28.785
makes it easier to
predict your bills.

00:12:28.785 --> 00:12:30.660
You don't have to try
to figure out, OK, I've

00:12:30.660 --> 00:12:32.200
got a bunch of web servers.

00:12:32.200 --> 00:12:35.629
How many IOs per month
are they going to do?

00:12:35.629 --> 00:12:37.670
So you can predict what
your bill is going to be.

00:12:37.670 --> 00:12:40.510
Instead, when you buy the
PD, you look at the size,

00:12:40.510 --> 00:12:44.480
tells you what the price is
going to be and you're done.

00:12:44.480 --> 00:12:46.230
Finally, just another
nice property of PD,

00:12:46.230 --> 00:12:48.771
and there's a bunch of other's
you can go read more about it,

00:12:48.771 --> 00:12:51.020
is that we get rid of
some of the limitations

00:12:51.020 --> 00:12:52.530
of the underlying
physical devices.

00:12:52.530 --> 00:12:56.200
So for example, if
you've got a big database

00:12:56.200 --> 00:12:58.540
and you want to put 10
terabytes for your database

00:12:58.540 --> 00:13:00.270
onto a single volume,
that's something

00:13:00.270 --> 00:13:02.100
that we can accommodate.

00:13:02.100 --> 00:13:05.310
No problem for us.

00:13:05.310 --> 00:13:06.680
Number five.

00:13:06.680 --> 00:13:09.600
Consistent performance.

00:13:09.600 --> 00:13:12.050
Multi-tenancy is hard.

00:13:12.050 --> 00:13:14.680
When you build a
shared infrastructure

00:13:14.680 --> 00:13:17.290
and you want lots of
diverse applications

00:13:17.290 --> 00:13:19.940
to use that infrastructure,
you have this challenge

00:13:19.940 --> 00:13:23.620
of making sure that one
application doesn't interfere

00:13:23.620 --> 00:13:26.730
with another application.

00:13:26.730 --> 00:13:29.800
Google has lots of experience
dealing with this problem.

00:13:29.800 --> 00:13:33.130
We've been running multi-tenant
data centers for a long time.

00:13:33.130 --> 00:13:35.980
And we've built up a
reservoir of technologies

00:13:35.980 --> 00:13:38.840
that we use internally, and
now we can use in our cloud,

00:13:38.840 --> 00:13:40.740
to help provide that
type of isolation

00:13:40.740 --> 00:13:43.570
to give you consistent
performance.

00:13:43.570 --> 00:13:47.550
We've built cluster management
systems, and schedulers,

00:13:47.550 --> 00:13:52.410
in Borg and Omega, that help us
provide this type of isolation.

00:13:52.410 --> 00:13:54.400
As we just mentioned
in the earlier talk,

00:13:54.400 --> 00:13:57.754
we've been really some of
the pioneers in containers

00:13:57.754 --> 00:13:59.420
and contributed some
of those techniques

00:13:59.420 --> 00:14:01.100
back to the open
source community

00:14:01.100 --> 00:14:04.400
for isolating different
groups of Linux processes

00:14:04.400 --> 00:14:07.170
and giving them
consistent performance.

00:14:07.170 --> 00:14:09.770
We've been pushing the
boundaries of software defined

00:14:09.770 --> 00:14:13.260
networking and taking
advantage of that flexibility

00:14:13.260 --> 00:14:16.390
to improve the isolation
in our clusters

00:14:16.390 --> 00:14:18.680
and across our clusters.

00:14:18.680 --> 00:14:20.340
As I mentioned a
little bit ago, we've

00:14:20.340 --> 00:14:24.150
built internally a big
collection of shared storage

00:14:24.150 --> 00:14:24.970
systems.

00:14:24.970 --> 00:14:29.020
Spanner and Dremel and Colossus.

00:14:29.020 --> 00:14:31.730
And those systems
have been designed

00:14:31.730 --> 00:14:34.680
to allow different applications
to take advantage of a shared

00:14:34.680 --> 00:14:38.860
service, but get the consistent
performance that they need.

00:14:38.860 --> 00:14:41.120
And then finally,
we've just built up

00:14:41.120 --> 00:14:45.370
over the time operational
practice and tools.

00:14:45.370 --> 00:14:47.780
We've got the tools to
monitor what's going on,

00:14:47.780 --> 00:14:49.780
to react when we see
something that's not right,

00:14:49.780 --> 00:14:52.280
and to fix those problems and
improve our systems over time.

00:14:52.280 --> 00:14:54.960
And we've been doing that to get
better and better consistency,

00:14:54.960 --> 00:14:58.360
both internally
and now externally.

00:14:58.360 --> 00:15:00.130
We hear over and over
from our customers

00:15:00.130 --> 00:15:02.350
that they value this
type of consistency.

00:15:02.350 --> 00:15:04.650
This is one example
of one of the quotes,

00:15:04.650 --> 00:15:06.920
but we hear this from a lot
of customers all the time.

00:15:06.920 --> 00:15:08.794
And it's something we
take pride in and think

00:15:08.794 --> 00:15:12.180
is a really important
thing to be delivering.

00:15:12.180 --> 00:15:14.420
As an example of
the types of things

00:15:14.420 --> 00:15:17.460
that we do to improve the
consistency of our service,

00:15:17.460 --> 00:15:21.160
I just wanted to show
this very recent example.

00:15:21.160 --> 00:15:24.300
What this graph
on the right shows

00:15:24.300 --> 00:15:26.170
is what kind of
performance do you

00:15:26.170 --> 00:15:31.500
get for the CoreMark benchmark
as you spin up two, four,

00:15:31.500 --> 00:15:36.160
or eight CPU virtual
machines on a platform.

00:15:36.160 --> 00:15:38.130
In the left hand side
of each pair of these

00:15:38.130 --> 00:15:40.420
is what we've been
delivering until recently.

00:15:40.420 --> 00:15:43.740
These are all-- these
are called violin graphs,

00:15:43.740 --> 00:15:44.890
this is a violin graph.

00:15:44.890 --> 00:15:46.800
And so the way to
read this is you could

00:15:46.800 --> 00:15:49.930
look at one of these
clusters of points.

00:15:49.930 --> 00:15:51.960
It says it's four CPUs.

00:15:51.960 --> 00:15:56.250
And the thickness at any given
level is basically a histogram.

00:15:56.250 --> 00:15:58.480
The number of samples,
the number of machines

00:15:58.480 --> 00:16:01.530
out of the benchmarks
that we ran that

00:16:01.530 --> 00:16:05.040
delivered this level
of performance.

00:16:05.040 --> 00:16:09.320
Ideally what you'd see would
be perfectly flat, thin lines

00:16:09.320 --> 00:16:11.220
for each of the configurations.

00:16:11.220 --> 00:16:14.270
And we're pretty
good if you look

00:16:14.270 --> 00:16:16.610
at our recent version
of the system.

00:16:16.610 --> 00:16:19.230
So for example, if you
look at the four CPU case,

00:16:19.230 --> 00:16:22.390
the vast majority of the
VMs that we've spun up

00:16:22.390 --> 00:16:24.650
had an expected
level of performance.

00:16:24.650 --> 00:16:27.380
But there was a little bit of a
tail and every once in a while,

00:16:27.380 --> 00:16:29.500
we got an outlier going
one way or the other.

00:16:29.500 --> 00:16:31.000
This is probably
not something you'd

00:16:31.000 --> 00:16:33.540
notice unless you were looking
pretty carefully, running

00:16:33.540 --> 00:16:34.709
a lot of VMs.

00:16:34.709 --> 00:16:36.250
But it is something
that we detected.

00:16:36.250 --> 00:16:38.166
One of our engineers
noticed this and thought,

00:16:38.166 --> 00:16:39.360
this is not OK.

00:16:39.360 --> 00:16:41.420
We want to have
consistent performance.

00:16:41.420 --> 00:16:44.720
This engineer spent
several weeks, actually

00:16:44.720 --> 00:16:47.320
a couple of months tracking
down what was going on.

00:16:47.320 --> 00:16:49.700
Did a bunch of
statistical analysis,

00:16:49.700 --> 00:16:51.659
found out when did you
get better than expected

00:16:51.659 --> 00:16:53.116
performance, when
did you get worse

00:16:53.116 --> 00:16:54.300
than expected performance.

00:16:54.300 --> 00:16:56.890
Tracked it down, found an
issue in the Scheduler,

00:16:56.890 --> 00:17:00.800
and was able to provide
a better Scheduler.

00:17:00.800 --> 00:17:03.720
And that gives us the
performance shown on the right.

00:17:03.720 --> 00:17:08.280
We're very nearly back to that
ideal of a perfectly flat,

00:17:08.280 --> 00:17:11.000
thin line when we spin
up these machines.

00:17:11.000 --> 00:17:13.347
That change has been
rolling out to our clusters

00:17:13.347 --> 00:17:14.930
over the last couple
of weeks and will

00:17:14.930 --> 00:17:16.450
continue to roll out
over the coming weeks.

00:17:16.450 --> 00:17:18.569
And so if you've been happy with
the consistency of performance

00:17:18.569 --> 00:17:20.240
that you've seen in
the past, it should

00:17:20.240 --> 00:17:23.910
be getting even
better in the future.

00:17:23.910 --> 00:17:25.550
Now, this may seem
like a small thing.

00:17:25.550 --> 00:17:26.050
Right?

00:17:26.050 --> 00:17:28.133
I mean, the vast majority
of the time in the past,

00:17:28.133 --> 00:17:29.820
it was working just fine.

00:17:29.820 --> 00:17:31.609
Those tails didn't
happen very often.

00:17:31.609 --> 00:17:33.900
And a lot of people probably
wouldn't even notice them.

00:17:33.900 --> 00:17:37.260
But if you ever tried to
debug a weird performance

00:17:37.260 --> 00:17:40.300
anomaly in your system,
you've got enough problems

00:17:40.300 --> 00:17:42.390
with your own code
without that underlying

00:17:42.390 --> 00:17:43.682
system shifting underneath you.

00:17:43.682 --> 00:17:45.556
Or even if you're just
trying to do something

00:17:45.556 --> 00:17:47.950
as simple as predict the
number of virtual machines

00:17:47.950 --> 00:17:50.670
that you need to serve
a given level of load,

00:17:50.670 --> 00:17:52.870
you really want to know
what each of those machines

00:17:52.870 --> 00:17:53.704
is going to deliver.

00:17:53.704 --> 00:17:55.244
That's something
that we value, we've

00:17:55.244 --> 00:17:56.500
learned how important this is.

00:17:56.500 --> 00:17:58.260
We've got some great
engineers working

00:17:58.260 --> 00:18:02.060
to make that better
and better over time.

00:18:02.060 --> 00:18:02.980
Number four.

00:18:02.980 --> 00:18:04.780
Security.

00:18:04.780 --> 00:18:08.290
So this picture was taken
at one of our data centers

00:18:08.290 --> 00:18:09.200
a couple years ago.

00:18:09.200 --> 00:18:11.010
It was posted as a Street View.

00:18:11.010 --> 00:18:13.080
It perhaps tipped our
hand a little bit more

00:18:13.080 --> 00:18:15.690
than we intended about
how far away we're

00:18:15.690 --> 00:18:21.250
willing to go-- sorry--
to improve our security.

00:18:21.250 --> 00:18:25.800
But suffice it to say, we
take security very seriously.

00:18:25.800 --> 00:18:31.390
We physically protect our sites
with fences, barriers, guards.

00:18:31.390 --> 00:18:33.090
We have strict access control.

00:18:33.090 --> 00:18:35.730
My badge will not get
me into a data center.

00:18:35.730 --> 00:18:38.970
And we have video surveillance
with both human monitoring

00:18:38.970 --> 00:18:40.640
and also programmatic
monitoring.

00:18:40.640 --> 00:18:42.510
Programs that actually
analyze the video,

00:18:42.510 --> 00:18:44.460
detect anomalies,
and raise alarms

00:18:44.460 --> 00:18:46.290
so people can have a
look and figure out

00:18:46.290 --> 00:18:48.790
what more needs to be done.

00:18:48.790 --> 00:18:51.410
We do automatic
encryption at rest

00:18:51.410 --> 00:18:54.210
across PD cloud
storage and Cloud SQL.

00:18:54.210 --> 00:18:56.710
This is something that you don't
have to configure anything,

00:18:56.710 --> 00:18:59.030
the data gets encrypted
automatically.

00:18:59.030 --> 00:19:03.640
We provide secure communication
with both physical security

00:19:03.640 --> 00:19:06.600
and encryption across
our various networks.

00:19:06.600 --> 00:19:09.670
We have a growing list of
sort of certifications,

00:19:09.670 --> 00:19:12.760
kind of attesting to the sorts
of things that we're doing.

00:19:12.760 --> 00:19:14.750
Here are some of these.

00:19:14.750 --> 00:19:16.750
But I think just broadly
the thing to understand

00:19:16.750 --> 00:19:19.270
is that, again, high
security is something

00:19:19.270 --> 00:19:21.160
that Google as a
whole depends on.

00:19:21.160 --> 00:19:22.180
It's part of our values.

00:19:22.180 --> 00:19:24.180
It's something that we
work on all the time.

00:19:24.180 --> 00:19:27.540
We intend to lead and drive
security for the industry.

00:19:27.540 --> 00:19:29.490
We depend on it for all
of our other products.

00:19:29.490 --> 00:19:32.550
We provide it in Cloud, and
we take securing your data

00:19:32.550 --> 00:19:34.620
as seriously as we
take securing our own.

00:19:37.910 --> 00:19:39.460
Number three.

00:19:39.460 --> 00:19:42.270
0 to 100 in 38.

00:19:42.270 --> 00:19:44.840
So this graph is, again,
another violin graph.

00:19:44.840 --> 00:19:48.700
In this case, showing the time
that it takes to start up VMs

00:19:48.700 --> 00:19:51.440
that are part of various
sizes of clusters.

00:19:51.440 --> 00:19:55.370
So we start up clusters
of 1, 10, 25, 50, 100 VMs.

00:19:55.370 --> 00:19:57.100
And then what the
violin graph is showing

00:19:57.100 --> 00:20:01.240
is the histogram of how long
it takes for each individual VM

00:20:01.240 --> 00:20:02.290
to get started.

00:20:02.290 --> 00:20:04.930
We run this trial a bunch of
times to get a distribution.

00:20:04.930 --> 00:20:07.440
The histogram shows that.

00:20:07.440 --> 00:20:08.930
So the first thing
you might notice

00:20:08.930 --> 00:20:12.930
is that spinning up
a single VM is fast.

00:20:12.930 --> 00:20:15.100
Looks like the first ones
are showing up-- typically

00:20:15.100 --> 00:20:16.884
one will show up in 21 seconds.

00:20:16.884 --> 00:20:18.550
In all of our experiments
they showed up

00:20:18.550 --> 00:20:21.200
between 21, 22, and 23 seconds.

00:20:21.200 --> 00:20:22.640
So the VMs come very quickly.

00:20:22.640 --> 00:20:24.139
And that's from the
time that we say

00:20:24.139 --> 00:20:26.955
go until you can
SSH into the VM.

00:20:26.955 --> 00:20:28.580
The second thing
you're going to notice

00:20:28.580 --> 00:20:32.800
is that spinning up a bunch of
VMs doesn't take much longer.

00:20:32.800 --> 00:20:35.330
So in the case where we
kick off 100 VMs, spin them

00:20:35.330 --> 00:20:37.970
all up at once,
again, we measure

00:20:37.970 --> 00:20:40.440
how long until
individual VMs arrive.

00:20:40.440 --> 00:20:42.660
And in this example,
some of the VMs

00:20:42.660 --> 00:20:45.490
are showing up as quickly
as looks like 18 seconds.

00:20:45.490 --> 00:20:49.340
All of them have arrived in all
of our trials in 38 seconds.

00:20:49.340 --> 00:20:53.516
So you can spin up a bunch of
VMs, and it comes very quickly.

00:20:53.516 --> 00:20:57.460
What this lets you do is
adjust to changing demands.

00:20:57.460 --> 00:20:59.810
As the load on your
system changes,

00:20:59.810 --> 00:21:01.690
the resources you're
using to serve that load

00:21:01.690 --> 00:21:05.140
you can quickly move up to
match what you need to do.

00:21:05.140 --> 00:21:08.590
It was this type of agility
that our friends at Scalar

00:21:08.590 --> 00:21:11.960
used to help Eurovision meet
their demands for the Eurosong

00:21:11.960 --> 00:21:14.400
contest that I talked
about earlier today.

00:21:18.740 --> 00:21:20.560
Per minute.

00:21:20.560 --> 00:21:24.300
So as some of you may know,
we have a sister business

00:21:24.300 --> 00:21:25.930
that sells ads.

00:21:25.930 --> 00:21:29.230
And a big part of what they
need to do to make that business

00:21:29.230 --> 00:21:31.450
work is micro billing.

00:21:31.450 --> 00:21:34.960
And so once again, we've
taken technology that

00:21:34.960 --> 00:21:37.020
we've-- an institutional
experience,

00:21:37.020 --> 00:21:42.210
and applied it to our Cloud
to improve our platform.

00:21:42.210 --> 00:21:45.530
So in particular, we
offer per minute billing

00:21:45.530 --> 00:21:49.660
with a 10 minute minimum for
our Compute Engine instances.

00:21:49.660 --> 00:21:51.470
Now, this will both
save you money,

00:21:51.470 --> 00:21:54.707
especially for very short
running instances, but any time

00:21:54.707 --> 00:21:56.290
that you're running
an instance and it

00:21:56.290 --> 00:21:58.314
doesn't take exactly
an hour worth of work.

00:21:58.314 --> 00:22:00.480
So if your work takes a
little more or a little less

00:22:00.480 --> 00:22:02.063
than a certain number
of hours, you're

00:22:02.063 --> 00:22:04.320
going to save money with
this per minute billing.

00:22:04.320 --> 00:22:07.280
What this graph shows is on
the x-axis, the number of hours

00:22:07.280 --> 00:22:10.320
that a VM runs, on the y-axis,
the price that you get.

00:22:10.320 --> 00:22:12.060
The cost that you
pay to run that VM.

00:22:12.060 --> 00:22:16.920
And as you can see, being able
to pay for just what you're

00:22:16.920 --> 00:22:19.380
using can actually result
in some pretty significant

00:22:19.380 --> 00:22:21.150
savings.

00:22:21.150 --> 00:22:24.950
Again, this gives you
more ability to one,

00:22:24.950 --> 00:22:28.000
deal with changes in
demand, but also two,

00:22:28.000 --> 00:22:29.720
take advantage of our scale.

00:22:29.720 --> 00:22:33.150
So for example, if you're doing
a analysis of a big data set

00:22:33.150 --> 00:22:36.420
on Compute Engine, so you're
spinning up a big Hadoop job,

00:22:36.420 --> 00:22:40.860
rather than run on 100
machines for an hour,

00:22:40.860 --> 00:22:42.900
run on 600 machines
and get the same answer

00:22:42.900 --> 00:22:44.275
for the same price
in 10 minutes.

00:22:47.980 --> 00:22:49.710
Finally, number one.

00:22:49.710 --> 00:22:53.630
If per minute billing is--
renting our infrastructure

00:22:53.630 --> 00:22:56.250
by the minute is cool,
renting it by the second

00:22:56.250 --> 00:22:56.964
is even better.

00:22:56.964 --> 00:22:58.380
And that's effectively
what you're

00:22:58.380 --> 00:23:01.760
doing when you use
the BigQuery service.

00:23:01.760 --> 00:23:05.720
What BigQuery does is it spreads
out analysis of your data

00:23:05.720 --> 00:23:08.487
across hundreds or thousands
of machines without, of course,

00:23:08.487 --> 00:23:09.820
you having to manually shard it.

00:23:09.820 --> 00:23:12.000
We take care of all that.

00:23:12.000 --> 00:23:16.130
It allows you to use a familiar
SQL-like query interface

00:23:16.130 --> 00:23:20.410
to this data across
these massive data sets.

00:23:20.410 --> 00:23:22.995
This graph shows-- this actually
comes from the Dremel paper

00:23:22.995 --> 00:23:24.495
that we published
a couple years ago

00:23:24.495 --> 00:23:28.630
to describe a lot of the core
techniques inside of BigQuery,

00:23:28.630 --> 00:23:30.840
inside of Dremel at that time.

00:23:30.840 --> 00:23:33.130
What it shows was that
they were able to do

00:23:33.130 --> 00:23:36.120
an analysis of a
trillion row table, doing

00:23:36.120 --> 00:23:39.440
a top-k query across
that trillion row table,

00:23:39.440 --> 00:23:41.690
and by spreading out the
analysis of this trillion row

00:23:41.690 --> 00:23:44.715
table across thousands
of nodes, they

00:23:44.715 --> 00:23:47.410
were able to get their
answers in under a minute.

00:23:47.410 --> 00:23:51.420
Now of course, we didn't
stop working on Dremel

00:23:51.420 --> 00:23:53.120
when we published the paper.

00:23:53.120 --> 00:23:54.990
We continued to
work on this system

00:23:54.990 --> 00:23:58.205
and giving you new features
like the continuous queries that

00:23:58.205 --> 00:23:59.970
were described earlier today.

00:23:59.970 --> 00:24:04.180
So that you upload data, you
can keep streaming up uploads

00:24:04.180 --> 00:24:07.020
continuously, and as soon
as the data hits BigQuery,

00:24:07.020 --> 00:24:09.390
you can issue your request,
and now your queries

00:24:09.390 --> 00:24:11.390
are going to return answers
reflecting that most

00:24:11.390 --> 00:24:13.185
recent data.

00:24:13.185 --> 00:24:15.310
So we develop these
technologies as part of Dremel.

00:24:15.310 --> 00:24:16.910
Internally, it's a great tool.

00:24:16.910 --> 00:24:20.130
And we're really happy that now
with BigQuery, you have access

00:24:20.130 --> 00:24:21.370
to this technology as well.

00:24:24.480 --> 00:24:27.110
So thanks for letting
me talk about some

00:24:27.110 --> 00:24:29.712
of the things that make
the platform special.

00:24:29.712 --> 00:24:31.920
We've been able to bring
you some neat things so far.

00:24:31.920 --> 00:24:36.060
Things from sharks to scale,
consistency to security,

00:24:36.060 --> 00:24:39.120
from green to the
rest of Google,

00:24:39.120 --> 00:24:42.300
and I'm really excited to be
an engineer on this project

00:24:42.300 --> 00:24:45.470
because I feel like we're
really just getting started.

00:24:45.470 --> 00:24:46.560
Thank you.

00:24:46.560 --> 00:24:53.181
[APPLAUSE]

