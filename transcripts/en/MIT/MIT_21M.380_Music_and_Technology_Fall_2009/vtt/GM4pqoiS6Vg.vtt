WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:01.700
The following
content is provided

00:00:01.700 --> 00:00:03.840
under a Creative
Commons license.

00:00:03.840 --> 00:00:06.570
Your support will help MIT
OpenCourseWare continue

00:00:06.570 --> 00:00:10.180
to offer high quality,
educational resources for free.

00:00:10.180 --> 00:00:12.720
To make a donation, or to
view additional materials

00:00:12.720 --> 00:00:15.180
from hundreds of
MIT courses, "visit

00:00:15.180 --> 00:00:21.280
MIT OpenCourseWare
at ocw.mit.edu."

00:00:21.280 --> 00:00:25.260
PROFESSOR: So today we are going
to spend a little bit of time

00:00:25.260 --> 00:00:28.650
discussing the Bimber
reading, which introduces

00:00:28.650 --> 00:00:30.910
a really important
concept for us.

00:00:30.910 --> 00:00:33.410
That is the concept of
technological determinism.

00:00:33.410 --> 00:00:37.770
I think this is particularly
important in our look at music

00:00:37.770 --> 00:00:41.580
and technology because
music technologies, I think,

00:00:41.580 --> 00:00:43.180
raise really
interesting questions,

00:00:43.180 --> 00:00:45.500
and pose really
interesting examples

00:00:45.500 --> 00:00:47.470
and counter examples
of this question

00:00:47.470 --> 00:00:48.940
of technological determinism.

00:00:48.940 --> 00:00:50.990
So we'll spend a little
time talking about that.

00:00:50.990 --> 00:00:52.448
Then we're going
to experiment with

00:00:52.448 --> 00:00:56.370
some electromagnetic
properties of sound generation.

00:00:56.370 --> 00:01:01.540
So we're going to generate
a mini Telharmonium light,

00:01:01.540 --> 00:01:04.830
as Collins likes to say,
using a simple motor.

00:01:04.830 --> 00:01:08.990
We're going to experiment
with humbucker pickup,

00:01:08.990 --> 00:01:11.330
and see what kind of sounds
we can pick up with that.

00:01:11.330 --> 00:01:15.520
And then we're going to build
a little oscillator using

00:01:15.520 --> 00:01:21.810
the Schmitt trigger model, that
Collins nicely provides for us.

00:01:21.810 --> 00:01:22.590
Good?

00:01:22.590 --> 00:01:24.540
Questions?

00:01:24.540 --> 00:01:27.310
In general, keep in mind
we will have another quiz

00:01:27.310 --> 00:01:33.190
next Thursday, a week from
today, so keep that in mind.

00:01:33.190 --> 00:01:33.690
OK, good.

00:01:33.690 --> 00:01:39.000
So let's start with the Bimber
article, and I believe Jillian,

00:01:39.000 --> 00:01:41.827
you were responsible
for this one.

00:01:41.827 --> 00:01:42.368
AUDIENCE: OK.

00:01:42.368 --> 00:01:44.701
So, basically this article
is talking about, as he said,

00:01:44.701 --> 00:01:46.030
technological determinism.

00:01:46.030 --> 00:01:49.360
Which he kind of describes
as the views on relationships

00:01:49.360 --> 00:01:51.460
between technology
and human activity,

00:01:51.460 --> 00:01:52.915
or the significance
of technology

00:01:52.915 --> 00:01:55.050
to this social change.

00:01:55.050 --> 00:01:57.920
But he kind of talks
about how there's

00:01:57.920 --> 00:02:00.070
a huge debate on what the
concept actually means.

00:02:02.810 --> 00:02:06.540
So he goes through
the other three types

00:02:06.540 --> 00:02:08.796
of technological
determinism, that

00:02:08.796 --> 00:02:11.256
would be norm-based
accounts, logical sequence

00:02:11.256 --> 00:02:14.240
accounts, and unintended
consequence accounts.

00:02:17.970 --> 00:02:23.141
He kind of describes those, and
goes through different views

00:02:23.141 --> 00:02:24.090
on each of those.

00:02:24.090 --> 00:02:24.930
PROFESSOR: OK, good.

00:02:24.930 --> 00:02:27.260
And the big picture-- his
goal-- his big question

00:02:27.260 --> 00:02:30.230
for this article is what?

00:02:30.230 --> 00:02:31.890
AUDIENCE: Basically,
how technology

00:02:31.890 --> 00:02:35.060
relates to social aspects.

00:02:35.060 --> 00:02:36.257
Or, does it really?

00:02:36.257 --> 00:02:36.840
PROFESSOR: OK.

00:02:36.840 --> 00:02:40.790
So how technology relates
to culture and society--

00:02:40.790 --> 00:02:43.640
what action-- what
power technology has

00:02:43.640 --> 00:02:45.990
in relation to
society and culture.

00:02:45.990 --> 00:02:48.570
And specifically this article--
on the biggest picture,

00:02:48.570 --> 00:02:50.821
he's asking a question that
doesn't really concern us.

00:02:50.821 --> 00:02:52.445
But what's the big
question he's trying

00:02:52.445 --> 00:02:53.710
to answer with this article?

00:02:53.710 --> 00:02:57.441
AUDIENCE: Did what Marx say--
was it really technological--

00:02:57.441 --> 00:02:59.440
PROFESSOR: Was Marx a
technological determinist?

00:02:59.440 --> 00:03:01.231
That's sort of the big
question, but that's

00:03:01.231 --> 00:03:04.030
the least interesting for us.

00:03:04.030 --> 00:03:06.560
People debate that a lot, and
that's an interesting issue,

00:03:06.560 --> 00:03:08.190
but that's not really
relevant for us.

00:03:08.190 --> 00:03:09.737
This article, for
us, provides us

00:03:09.737 --> 00:03:12.320
with a really nice way of just
getting an introduction to some

00:03:12.320 --> 00:03:14.861
of the different approaches to
what technological determinism

00:03:14.861 --> 00:03:15.950
is.

00:03:15.950 --> 00:03:18.560
So let's first discuss
a little bit about what

00:03:18.560 --> 00:03:21.620
is technological determinism,
without looking at his three

00:03:21.620 --> 00:03:24.540
cases, let's just talk
about this in general.

00:03:24.540 --> 00:03:26.670
What is this idea of
technological determinism?

00:03:30.219 --> 00:03:31.386
AUDIENCE: Which one of them?

00:03:31.386 --> 00:03:32.510
PROFESSOR: Just in general.

00:03:32.510 --> 00:03:34.825
Try from the highest, most
broadest description first.

00:03:37.652 --> 00:03:39.860
AUDIENCE: So you have this
aspect of, does technology

00:03:39.860 --> 00:03:40.880
affect us?

00:03:40.880 --> 00:03:43.700
And how-- in that case,
how much does it affect us?

00:03:43.700 --> 00:03:46.880
There's also the deterministic
part , where it's like,

00:03:46.880 --> 00:03:49.030
is what's going to happen
in technology really

00:03:49.030 --> 00:03:50.540
deterministic?

00:03:50.540 --> 00:03:53.060
And so that's at
times, do we affect it?

00:03:53.060 --> 00:03:55.260
Do we actually have control
over that technology?

00:03:55.260 --> 00:03:55.968
PROFESSOR: Right.

00:03:55.968 --> 00:03:57.780
I think in the
biggest sense, that's

00:03:57.780 --> 00:03:59.960
the question we're
trying to answer here.

00:03:59.960 --> 00:04:04.890
Is technology an independent
force that acts on its own,

00:04:04.890 --> 00:04:07.009
or do we have some control?

00:04:07.009 --> 00:04:08.550
I think, in the
biggest sense, that's

00:04:08.550 --> 00:04:09.840
the question this gets to.

00:04:09.840 --> 00:04:13.190
What is this concept
of determinism?

00:04:13.190 --> 00:04:14.130
In general.

00:04:14.130 --> 00:04:15.704
Determinism?

00:04:15.704 --> 00:04:17.339
AUDIENCE: Do you have free will?

00:04:17.339 --> 00:04:18.190
PROFESSOR: Yeah,
it basically gets

00:04:18.190 --> 00:04:19.398
to the question of free will.

00:04:19.398 --> 00:04:24.005
Determinism-- I mean, how
could we not have free will?

00:04:24.005 --> 00:04:25.380
Well, what are
you talking about?

00:04:25.380 --> 00:04:27.940
I make choices.

00:04:27.940 --> 00:04:30.385
AUDIENCE: It's to say
that possibly the course

00:04:30.385 --> 00:04:32.570
of human history, or
any event that happens,

00:04:32.570 --> 00:04:34.995
is all predetermined
by the choices

00:04:34.995 --> 00:04:36.450
that we made in the past.

00:04:36.450 --> 00:04:39.521
So, it's hard to say whether or
not your free will is actually

00:04:39.521 --> 00:04:41.785
your decision, or if
that's just a conclusion

00:04:41.785 --> 00:04:44.372
of some other
decision that's made.

00:04:44.372 --> 00:04:45.080
PROFESSOR: Right.

00:04:45.080 --> 00:04:46.620
But then that case,
then you still

00:04:46.620 --> 00:04:48.720
have decisions playing a part.

00:04:48.720 --> 00:04:51.310
That might be a sort
of soft determinism.

00:04:51.310 --> 00:04:56.212
A hard determinism, taken
further would be what?

00:04:56.212 --> 00:04:57.610
AUDIENCE: You have no choice.

00:04:57.610 --> 00:04:57.880
PROFESSOR: Right.

00:04:57.880 --> 00:04:58.510
And why not?

00:04:58.510 --> 00:04:59.801
Why don't you have any choices?

00:05:03.984 --> 00:05:06.400
AUDIENCE: Everything's just a
bunch of chemical reactions,

00:05:06.400 --> 00:05:06.730
right?

00:05:06.730 --> 00:05:07.800
It's just all physics.

00:05:07.800 --> 00:05:08.520
PROFESSOR: It's all physics.

00:05:08.520 --> 00:05:09.686
It's all chemical reactions.

00:05:09.686 --> 00:05:13.860
It's all causal change,
causality and causal change.

00:05:13.860 --> 00:05:19.080
One reaction, one physical law
interacts, results in another.

00:05:19.080 --> 00:05:23.020
And that chain of causal
actions, leads in a result.

00:05:23.020 --> 00:05:24.550
That's the idea of determinism.

00:05:24.550 --> 00:05:26.900
And if that is taken
to its logical extreme,

00:05:26.900 --> 00:05:29.330
then we don't have any
room for free will.

00:05:29.330 --> 00:05:33.100
And that's a debate we'll
let the philosophers discuss,

00:05:33.100 --> 00:05:35.420
it doesn't really bother us.

00:05:35.420 --> 00:05:36.320
Well it does.

00:05:36.320 --> 00:05:39.620
Sometimes it bothers me, but
I feel like I have free will,

00:05:39.620 --> 00:05:41.110
and you probably
all do too, so we

00:05:41.110 --> 00:05:42.533
can work with that
assumption just because it's

00:05:42.533 --> 00:05:43.533
a little more practical.

00:05:43.533 --> 00:05:45.485
AUDIENCE: So then
the other major

00:05:45.485 --> 00:05:48.890
in technological determinism
is not determinism in general,

00:05:48.890 --> 00:05:52.220
but does technology
determine social things?

00:05:52.220 --> 00:05:54.100
Does it cause social
things to happen?

00:05:54.100 --> 00:05:57.960
So does the fact that
the typewriter came about

00:05:57.960 --> 00:06:00.470
at a certain time,
make certain things

00:06:00.470 --> 00:06:02.326
happen because of
the typewriter?

00:06:02.326 --> 00:06:03.610
Not the other way around.

00:06:03.610 --> 00:06:05.110
Not, did the
typewriter get invented

00:06:05.110 --> 00:06:07.344
because of social things?

00:06:07.344 --> 00:06:08.010
PROFESSOR: Yeah.

00:06:08.010 --> 00:06:12.150
I mean, the idea that
technologies on their own,

00:06:12.150 --> 00:06:15.590
just by existing, play a
part in the causal chain,

00:06:15.590 --> 00:06:17.890
independent of human
actions, that's

00:06:17.890 --> 00:06:20.460
really at the core of the issue.

00:06:20.460 --> 00:06:24.480
We'll talk about that more, but
let's look at Bimber's three--

00:06:24.480 --> 00:06:28.350
he categorizes three groups
of approaches, accounts

00:06:28.350 --> 00:06:29.920
of technological determinism.

00:06:29.920 --> 00:06:32.620
He's trying to narrow the field,
because so many people have

00:06:32.620 --> 00:06:33.390
talked about this.

00:06:33.390 --> 00:06:37.330
He's trying to narrow the field
down, and sort of categorize

00:06:37.330 --> 00:06:38.680
the ideas about this.

00:06:38.680 --> 00:06:40.730
Let's start with the
norm-based account.

00:06:40.730 --> 00:06:42.710
What's the norm-based account?

00:06:42.710 --> 00:06:44.980
What are the norms that
he's talking about?

00:06:44.980 --> 00:06:47.440
AUDIENCE: So basically,
that technology

00:06:47.440 --> 00:06:49.116
is like a human activity.

00:06:49.116 --> 00:06:50.235
People create technology.

00:06:50.235 --> 00:06:53.162
They get together
and make things,

00:06:53.162 --> 00:06:55.410
then their actions are
governed by whatever

00:06:55.410 --> 00:06:56.904
political body is ruling.

00:07:01.744 --> 00:07:05.132
And also he talks about
the fear of productivity

00:07:05.132 --> 00:07:07.132
over ethical things.

00:07:07.132 --> 00:07:08.590
PROFESSOR: That's
the key-- I think

00:07:08.590 --> 00:07:10.880
that's the key point
in norm-based accounts.

00:07:10.880 --> 00:07:12.750
Somebody else want to
build on that idea?

00:07:12.750 --> 00:07:14.390
Norm-based?

00:07:14.390 --> 00:07:17.540
So the norms he's
talking about-- he's

00:07:17.540 --> 00:07:20.080
basically talking
about where people

00:07:20.080 --> 00:07:22.500
give up some control
to technology,

00:07:22.500 --> 00:07:25.240
right, giving up some
control to technology.

00:07:25.240 --> 00:07:27.770
And he's talking about
that based on these norms.

00:07:27.770 --> 00:07:28.930
And what are the norms?

00:07:28.930 --> 00:07:31.470
You mentioned one of the
norms that he mentions,

00:07:31.470 --> 00:07:33.812
which was-- which
one did you say?

00:07:33.812 --> 00:07:34.770
AUDIENCE: Productivity.

00:07:34.770 --> 00:07:35.720
PROFESSOR: Productivity.

00:07:35.720 --> 00:07:36.844
Productivity, that's right.

00:07:36.844 --> 00:07:40.480
What are some of the other
norms that he describes?

00:07:40.480 --> 00:07:41.200
AUDIENCE: Logic.

00:07:41.200 --> 00:07:42.300
PROFESSOR: Logic.

00:07:42.300 --> 00:07:42.850
Logic.

00:07:42.850 --> 00:07:48.750
And often with logic is reason.

00:07:48.750 --> 00:07:52.220
Logic, reason, productivity,
efficiency, those

00:07:52.220 --> 00:07:55.310
are the norms that he's talking
in the norms-based account.

00:07:55.310 --> 00:07:57.160
And what he's
talking about-- how

00:07:57.160 --> 00:08:00.600
this is a sort of technological
determinism is that people say,

00:08:00.600 --> 00:08:04.740
well, I don't know if
I like this technology,

00:08:04.740 --> 00:08:06.710
but it makes my life easier.

00:08:06.710 --> 00:08:13.890
It's more efficient, or
it's a reasonable approach.

00:08:13.890 --> 00:08:16.680
That's sort of a giving up
of some control, but not all

00:08:16.680 --> 00:08:17.470
control.

00:08:17.470 --> 00:08:19.970
Ultimately, does he say that
the norm-based account is

00:08:19.970 --> 00:08:21.620
a technological determinism?

00:08:21.620 --> 00:08:22.120
No.

00:08:22.120 --> 00:08:25.650
He rejects it because
humans still play some role,

00:08:25.650 --> 00:08:30.320
but don't have complete control.

00:08:30.320 --> 00:08:32.020
OK, the second one.

00:08:32.020 --> 00:08:35.100
Let's go to-- let's jump
to unintended consequences.

00:08:35.100 --> 00:08:37.329
What are the unintended
consequences?

00:08:37.329 --> 00:08:39.120
AUDIENCE: Basically,
you have a technology,

00:08:39.120 --> 00:08:41.020
and you're creating
it for some purpose.

00:08:41.020 --> 00:08:44.044
And then, way down the road, it
has some completely different

00:08:44.044 --> 00:08:45.710
affect that you
couldn't have predicted.

00:08:45.710 --> 00:08:46.560
PROFESSOR: Good.

00:08:46.560 --> 00:08:48.890
Now how is that a
technological-- suggests

00:08:48.890 --> 00:08:51.470
perhaps a technological
determinism?

00:08:51.470 --> 00:08:54.380
AUDIENCE: For instance, he
brought up the example cars.

00:08:54.380 --> 00:08:57.330
They were created to
clean up the streets

00:08:57.330 --> 00:09:00.644
so there wouldn't be
horses everywhere.

00:09:00.644 --> 00:09:04.480
And then it kind of led to
this whole Co2 emissions,

00:09:04.480 --> 00:09:07.180
and that's kind of a
technological determinism,

00:09:07.180 --> 00:09:11.033
because we couldn't have
prevented that because we

00:09:11.033 --> 00:09:13.402
couldn't have predicted
it in the first place.

00:09:13.402 --> 00:09:15.110
PROFESSOR: Yeah,
unintended consequences.

00:09:15.110 --> 00:09:18.850
Technologies have results
that we may not foresee.

00:09:18.850 --> 00:09:21.537
And that gives us a sign
that the technology--

00:09:21.537 --> 00:09:23.370
it suggests perhaps,
that the technology has

00:09:23.370 --> 00:09:24.869
some sort of agency,
that technology

00:09:24.869 --> 00:09:27.040
is doing something
out of our control.

00:09:27.040 --> 00:09:29.170
That's another
reason why it might

00:09:29.170 --> 00:09:30.880
be seen as a
technological determinism.

00:09:30.880 --> 00:09:32.560
Does Bimber ultimately
say that it's

00:09:32.560 --> 00:09:34.340
a technological determinism?

00:09:34.340 --> 00:09:35.850
No, he cuts that one out too.

00:09:35.850 --> 00:09:36.496
And why?

00:09:36.496 --> 00:09:38.120
AUDIENCE: He makes
the distinction that

00:09:38.120 --> 00:09:41.200
because it's-- just because it's
uncontrollable and unintended,

00:09:41.200 --> 00:09:42.840
doesn't mean that
it's deterministic.

00:09:42.840 --> 00:09:44.090
PROFESSOR: Right, that's true.

00:09:44.090 --> 00:09:46.970
And also that humans
still have agency rights.

00:09:46.970 --> 00:09:50.760
Humans still played a role in
creating those technologies,

00:09:50.760 --> 00:09:53.230
introducing them, so
it doesn't completely

00:09:53.230 --> 00:09:55.930
remove the human agency.

00:09:55.930 --> 00:09:58.260
Finally, we get to the
logical sequence account,

00:09:58.260 --> 00:09:59.980
which is the one that
he wants to hold up

00:09:59.980 --> 00:10:04.380
as the real, pure
technological determinism.

00:10:04.380 --> 00:10:06.738
And what is the logical
sequence account?

00:10:06.738 --> 00:10:09.777
AUDIENCE: The technology itself,
creates the social change.

00:10:09.777 --> 00:10:11.610
PROFESSOR: Technology
creates social change,

00:10:11.610 --> 00:10:14.200
that's one aspect of the
logical sequence account.

00:10:14.200 --> 00:10:17.840
What are some other aspects of
the logical sequence account?

00:10:17.840 --> 00:10:20.210
AUDIENCE: That the existing
technologies will always

00:10:20.210 --> 00:10:23.184
entirely determine what next
technologies [INAUDIBLE]

00:10:23.184 --> 00:10:24.060
emerge.

00:10:24.060 --> 00:10:27.020
PROFESSOR: Right, and that
there's a logical sequence.

00:10:27.020 --> 00:10:29.220
Both it's logical,
one leads to another,

00:10:29.220 --> 00:10:31.820
and that there's a sequence.

00:10:31.820 --> 00:10:34.084
Anybody want to flesh out
that idea a little bit more?

00:10:34.084 --> 00:10:35.500
With this logical
sequence, can we

00:10:35.500 --> 00:10:37.840
think of an example of how
somebody might interpret

00:10:37.840 --> 00:10:40.490
this logical sequence
in our world,

00:10:40.490 --> 00:10:43.220
or in some sort of
practical context?

00:10:43.220 --> 00:10:44.807
AUDIENCE: Steam
before combustion.

00:10:44.807 --> 00:10:47.390
PROFESSOR: Yeah, like you have
to go through the steam engine,

00:10:47.390 --> 00:10:49.014
before we go through
combustion engine,

00:10:49.014 --> 00:10:51.738
before we move through
different technologies.

00:10:51.738 --> 00:10:53.602
AUDIENCE: Bronze Age.

00:10:53.602 --> 00:10:54.310
PROFESSOR: Right.

00:10:54.310 --> 00:10:56.960
We see this all the time
in simulation games,

00:10:56.960 --> 00:10:58.800
like Sim World or
something, where

00:10:58.800 --> 00:11:01.230
you're building a little
artificial culture,

00:11:01.230 --> 00:11:03.030
and you have to go
through the Iron Age

00:11:03.030 --> 00:11:05.160
before you do this
and that, and that's

00:11:05.160 --> 00:11:08.762
an idea of a logical
sequence of technology.

00:11:08.762 --> 00:11:09.970
Does it have to be like that?

00:11:09.970 --> 00:11:10.511
Do you agree?

00:11:10.511 --> 00:11:11.117
Is that--

00:11:11.117 --> 00:11:12.492
AUDIENCE: I was
just going to say

00:11:12.492 --> 00:11:17.470
that it was regardless
of where it was and who--

00:11:17.470 --> 00:11:18.470
PROFESSOR: That's right.

00:11:18.470 --> 00:11:20.170
The logical sequence
happens regardless

00:11:20.170 --> 00:11:24.990
of geography, regardless of
background, ethnicity, climate,

00:11:24.990 --> 00:11:28.416
cultural context,
economics, politics.

00:11:28.416 --> 00:11:29.790
AUDIENCE: Personally,
I disagree.

00:11:29.790 --> 00:11:33.080
I may be biased because we just
read the [INAUDIBLE] article.

00:11:33.080 --> 00:11:36.210
But basically this is
making everything linear,

00:11:36.210 --> 00:11:39.480
which, as we found out,
isn't really the best model.

00:11:39.480 --> 00:11:41.950
PROFESSOR: Right, so it does
suggest a linear trajectory.

00:11:41.950 --> 00:11:42.760
That's absolutely right.

00:11:42.760 --> 00:11:43.884
I think that's a good flaw.

00:11:43.884 --> 00:11:49.750
What else makes you wonder
about this logical sequence?

00:11:49.750 --> 00:11:52.930
AUDIENCE: It's very
Western Eurocentric.

00:11:52.930 --> 00:11:54.919
It's about our
sequence that we've

00:11:54.919 --> 00:11:56.710
gone through
[INTERPOSING VOICES] sequences

00:11:56.710 --> 00:11:58.470
happen elsewhere.

00:11:58.470 --> 00:12:00.912
So the idea that you need to
go through certain steps--

00:12:00.912 --> 00:12:02.370
and like now with
globalization you

00:12:02.370 --> 00:12:04.200
see in Africa they
use cell phones,

00:12:04.200 --> 00:12:05.760
but they don't use
landline phones.

00:12:05.760 --> 00:12:07.950
PROFESSOR: That's
a great example.

00:12:07.950 --> 00:12:09.865
AUDIENCE: It's not
necessarily, basically

00:12:09.865 --> 00:12:11.250
leapfrogging all these things.

00:12:11.250 --> 00:12:11.980
PROFESSOR: That's true.

00:12:11.980 --> 00:12:13.271
We have great examples of that.

00:12:13.271 --> 00:12:15.700
And of course, the idea
that the logical sequence is

00:12:15.700 --> 00:12:20.044
our sequence is
pretty, I think, true.

00:12:20.044 --> 00:12:23.090
AUDIENCE: [INAUDIBLE] logical,
because once you look back

00:12:23.090 --> 00:12:25.203
and you see which
steps have occurred,

00:12:25.203 --> 00:12:29.370
it's hard to imagine another
set of steps occurring.

00:12:29.370 --> 00:12:32.206
I [INAUDIBLE] with the
deterministic quality

00:12:32.206 --> 00:12:35.630
of the universe, because
we assume that everything--

00:12:35.630 --> 00:12:37.472
if you assume determinism
in the universe,

00:12:37.472 --> 00:12:39.992
then obviously technological
determinism holds.

00:12:39.992 --> 00:12:40.700
PROFESSOR: Right.

00:12:40.700 --> 00:12:42.080
But of course we
have people that

00:12:42.080 --> 00:12:44.570
are embracing
technological determinism,

00:12:44.570 --> 00:12:48.150
without really being
philosophical determinists.

00:12:52.180 --> 00:12:53.421
Great, so that's Bimber.

00:12:53.421 --> 00:12:54.920
And Bimber sets
this out, and that's

00:12:54.920 --> 00:12:56.336
the really important
thing for us.

00:12:56.336 --> 00:12:59.360
Now that we have a good idea
of technological determinism,

00:12:59.360 --> 00:13:01.360
he doesn't want to say
these other things are

00:13:01.360 --> 00:13:02.810
technological
determinism, but I think

00:13:02.810 --> 00:13:04.851
we can think of them as
technological determinism

00:13:04.851 --> 00:13:06.160
light.

00:13:06.160 --> 00:13:08.560
They're there, they happen.

00:13:08.560 --> 00:13:11.687
So the reason why I find
this concept so interesting

00:13:11.687 --> 00:13:14.270
and relevant to us is because
we find aspects of technological

00:13:14.270 --> 00:13:17.810
determinism all around
us in our lives, .

00:13:17.810 --> 00:13:22.890
in commentary that people offer,
both in the media and in casual

00:13:22.890 --> 00:13:23.740
discourse.

00:13:23.740 --> 00:13:25.300
And I'm wondering
if you guys can

00:13:25.300 --> 00:13:30.430
think of some casual ways
in which people reinforce

00:13:30.430 --> 00:13:32.585
or suggest technological
determinism?

00:13:35.710 --> 00:13:39.680
Casual ways people
talk about technology--

00:13:39.680 --> 00:13:42.020
AUDIENCE: Just our
idea of the future.

00:13:42.020 --> 00:13:44.600
We've predetermined
what future is supposed

00:13:44.600 --> 00:13:46.680
to look like in science fiction.

00:13:46.680 --> 00:13:49.390
And so now if someone wants
to design a product that

00:13:49.390 --> 00:13:52.810
looks futuristic, then they have
a model to work with already.

00:13:52.810 --> 00:13:54.402
Even though it
hasn't happened yet.

00:13:54.402 --> 00:13:55.360
PROFESSOR: Interesting.

00:13:55.360 --> 00:13:55.859
Interesting.

00:13:59.616 --> 00:14:01.240
Are people making
choices there though?

00:14:01.240 --> 00:14:03.297
Or are they--

00:14:03.297 --> 00:14:05.130
AUDIENCE: I guess we've
made a choice that's

00:14:05.130 --> 00:14:10.580
supposed to drive some type of
aesthetics of our development.

00:14:10.580 --> 00:14:14.185
These are the products that
people develop and have

00:14:14.185 --> 00:14:14.980
[INAUDIBLE].

00:14:14.980 --> 00:14:16.940
PROFESSOR: I think that's-- I
think there's two layers there.

00:14:16.940 --> 00:14:18.660
One is not technological
determinism,

00:14:18.660 --> 00:14:21.700
that it's sort of the
result of creativity,

00:14:21.700 --> 00:14:24.494
of fancy, that
imagines that future.

00:14:24.494 --> 00:14:25.160
That's one part.

00:14:25.160 --> 00:14:27.410
The part that's technological
deterministic about that

00:14:27.410 --> 00:14:31.170
is the idea that, that
future looks a certain way

00:14:31.170 --> 00:14:36.810
and is often better, or good,
or more efficient, or more

00:14:36.810 --> 00:14:39.310
Star Trek like.

00:14:39.310 --> 00:14:43.510
AUDIENCE: Do you think that you
can consider a social aesthetic

00:14:43.510 --> 00:14:47.342
that's [INAUDIBLE] your norm,
or would that maybe not qualify?

00:14:47.342 --> 00:14:49.300
PROFESSOR: What do you
mean a social aesthetic?

00:14:49.300 --> 00:14:53.035
AUDIENCE: Like he was saying,
we had this predetermined way,

00:14:53.035 --> 00:14:56.590
just because of how popular
the idea of the future was,

00:14:56.590 --> 00:15:00.960
[INAUDIBLE] maybe [INAUDIBLE]
the stereotypical Star Trek

00:15:00.960 --> 00:15:01.460
idea.

00:15:01.460 --> 00:15:04.245
And then you have an idea of
what a spaceship looks like,

00:15:04.245 --> 00:15:06.800
even though it does not exist.

00:15:06.800 --> 00:15:09.257
Can you call that--
you can define

00:15:09.257 --> 00:15:10.525
that as a social aesthetic?

00:15:10.525 --> 00:15:12.600
Can you call that a norm
for certain [INAUDIBLE]

00:15:12.600 --> 00:15:13.600
PROFESSOR: Yeah,
that's interesting.

00:15:13.600 --> 00:15:14.440
I think--

00:15:14.440 --> 00:15:16.410
AUDIENCE: File that under
norm-based accounts?

00:15:16.410 --> 00:15:17.340
PROFESSOR: Yeah,
that's interesting.

00:15:17.340 --> 00:15:18.640
I think it is a norm.

00:15:18.640 --> 00:15:21.531
I think it is-- there's some
freedom there though, right?

00:15:21.531 --> 00:15:23.030
Because we can
imagine other things.

00:15:23.030 --> 00:15:24.450
So I think science
fiction offers

00:15:24.450 --> 00:15:26.680
some freedom and
actually some ways out

00:15:26.680 --> 00:15:29.880
of a technological
deterministic approach.

00:15:29.880 --> 00:15:31.370
Let me give you an example.

00:15:31.370 --> 00:15:34.470
One way in which I think that we
see technological determinism,

00:15:34.470 --> 00:15:37.890
is whenever people give
agency to technologies.

00:15:37.890 --> 00:15:40.250
So remember when we are
reading the Stern article,

00:15:40.250 --> 00:15:44.170
and he kept on saying
the MP3 does this.

00:15:44.170 --> 00:15:46.680
The MP3 has changed
the way that we

00:15:46.680 --> 00:15:49.270
listen to-- That sentence,
in and of itself,

00:15:49.270 --> 00:15:52.950
I think is problematic,
because it implies

00:15:52.950 --> 00:15:55.040
that the MP3 is doing something.

00:15:55.040 --> 00:15:56.555
Is the MP3 doing something?

00:15:59.230 --> 00:16:01.480
Do you think it does something?

00:16:01.480 --> 00:16:04.810
Gets up, has a nice day,
forces you to listen to music

00:16:04.810 --> 00:16:07.020
at a lower cost and
a lower quality?

00:16:07.020 --> 00:16:07.720
AUDIENCE: Technically
we were talking

00:16:07.720 --> 00:16:09.220
about how it's not
the MP3, but it's

00:16:09.220 --> 00:16:12.472
the person who developed the
MP3 coding that does [INAUDIBLE]

00:16:12.472 --> 00:16:13.180
PROFESSOR: Right.

00:16:13.180 --> 00:16:16.140
Either the developer or
the user are real agents.

00:16:16.140 --> 00:16:19.560
But this casual use of language,
this attribution of agency

00:16:19.560 --> 00:16:24.300
to technologies, I think
is a significant point

00:16:24.300 --> 00:16:27.480
of how these technological
deterministic ideas creep

00:16:27.480 --> 00:16:29.700
into our thinking
about technology.

00:16:29.700 --> 00:16:31.130
Can you think of any others?

00:16:31.130 --> 00:16:31.820
AUDIENCE: I was going to
say there is something

00:16:31.820 --> 00:16:35.340
to be said though for the idea
that these technologies embody,

00:16:35.340 --> 00:16:37.650
in certain ways,
the ideas of people.

00:16:37.650 --> 00:16:41.360
So, the people who made
the MP3, had certain ideas

00:16:41.360 --> 00:16:43.435
about how people
hear and listen,

00:16:43.435 --> 00:16:45.414
and so that sort of
built into the MP3.

00:16:45.414 --> 00:16:46.330
PROFESSOR: Absolutely.

00:16:46.330 --> 00:16:50.050
AUDIENCE: So in a sense, we
can say the MP3 does do stuff.

00:16:50.050 --> 00:16:53.390
You play it and it plays music
back, and it has properties.

00:16:53.390 --> 00:16:56.610
So in a certain sense, those
properties can shape things.

00:16:56.610 --> 00:16:59.510
There isn't agency
necessarily of the MP3,

00:16:59.510 --> 00:17:03.825
but they are reflective of
human agency cycled back.

00:17:03.825 --> 00:17:04.825
PROFESSOR: That's right.

00:17:04.825 --> 00:17:06.116
AUDIENCE: It's a bunch of math.

00:17:09.890 --> 00:17:12.710
PROFESSOR: Yeah, and it's sort
of this casual use of language

00:17:12.710 --> 00:17:17.089
that sort of overlooks the
humans behind the process.

00:17:17.089 --> 00:17:20.290
And I think that it's not-- that
that's important to recognize,

00:17:20.290 --> 00:17:22.050
that that process is happening.

00:17:22.050 --> 00:17:26.599
Can you think of other examples
of this, where we casually

00:17:26.599 --> 00:17:32.140
sort of give technology a power
that it may not really have?

00:17:32.140 --> 00:17:35.636
AUDIENCE: The nuclear
bomb overturned ordinary

00:17:35.636 --> 00:17:39.200
in the 20th century.

00:17:39.200 --> 00:17:40.650
It's completely different now.

00:17:40.650 --> 00:17:42.770
All political
science has changed,

00:17:42.770 --> 00:17:44.400
international relations.

00:17:44.400 --> 00:17:48.670
And so I don't know if you give
the agency to the bomb itself,

00:17:48.670 --> 00:17:49.970
or to it's inventors.

00:17:49.970 --> 00:17:52.470
But it changed a lot.

00:17:52.470 --> 00:17:54.580
PROFESSOR: That's
true, that's true.

00:17:54.580 --> 00:17:59.740
And it-- very casually, people
say things like the Nuclear Age

00:17:59.740 --> 00:18:02.760
transformed geopolitics.

00:18:02.760 --> 00:18:06.410
But again, the question I
raise is, well, was it that?

00:18:06.410 --> 00:18:09.200
Or was it the people
behind it, and the culture,

00:18:09.200 --> 00:18:10.400
that led to that?

00:18:10.400 --> 00:18:12.200
And is it significant
in our language

00:18:12.200 --> 00:18:13.772
if we make a distinction?

00:18:13.772 --> 00:18:15.230
AUDIENCE: At the
same time, I don't

00:18:15.230 --> 00:18:18.420
know if any of the people who
developed it had a choice.

00:18:18.420 --> 00:18:21.250
Because at the time,
there was a competition.

00:18:21.250 --> 00:18:24.700
And so you could look at that
as a deterministic factor

00:18:24.700 --> 00:18:28.510
also, that once
some spark goes off

00:18:28.510 --> 00:18:31.780
and there's this idea
of the nuclear bomb,

00:18:31.780 --> 00:18:35.150
the superpowers have no choice
but to develop it, and have

00:18:35.150 --> 00:18:36.859
no choice but to go
into the Nuclear Age.

00:18:36.859 --> 00:18:38.441
PROFESSOR: That's
one of the arguments

00:18:38.441 --> 00:18:39.840
for technological determinism.

00:18:39.840 --> 00:18:40.680
Yeah, that's right

00:18:40.680 --> 00:18:42.388
AUDIENCE: Once you're
in the Nuclear Age,

00:18:42.388 --> 00:18:43.217
you can't leave it.

00:18:43.217 --> 00:18:45.300
PROFESSOR: That some
technologies sort of push you

00:18:45.300 --> 00:18:46.110
like this.

00:18:46.110 --> 00:18:48.420
Some would say, and
I would probably say,

00:18:48.420 --> 00:18:50.950
that we still have control.

00:18:50.950 --> 00:18:54.240
That we can still make a choice,
and getting to that position

00:18:54.240 --> 00:18:55.972
that we were in,
was still a choice.

00:18:55.972 --> 00:18:57.430
I mean now we're
trying for-- we're

00:18:57.430 --> 00:18:59.730
attempting to do
nuclear disarmament.

00:18:59.730 --> 00:19:02.715
And that's a choice to take that
away, whether that can work--

00:19:02.715 --> 00:19:04.340
AUDIENCE: But what
you can't take away,

00:19:04.340 --> 00:19:06.120
is the knowledge of
how to build bomb.

00:19:06.120 --> 00:19:08.730
So even if everyone
disarms, then

00:19:08.730 --> 00:19:12.580
should things get tense
again, will people

00:19:12.580 --> 00:19:14.198
start building again?

00:19:14.198 --> 00:19:15.830
You really can never go back.

00:19:15.830 --> 00:19:18.700
PROFESSOR: Yeah that's
an argument for--

00:19:18.700 --> 00:19:21.586
AUDIENCE: [INAUDIBLE] I
think it's optimistic,

00:19:21.586 --> 00:19:24.070
in a sense, that some
people theorize that you can

00:19:24.070 --> 00:19:26.925
in fact destroy knowledge
by rewriting history

00:19:26.925 --> 00:19:31.500
books [INAUDIBLE] 1,000
years [INAUDIBLE].

00:19:31.500 --> 00:19:32.660
PROFESSOR: Yeah.

00:19:32.660 --> 00:19:35.350
Well, and there's
contemporary examples

00:19:35.350 --> 00:19:38.970
of countries doing that in
explicit and inexplicit ways.

00:19:38.970 --> 00:19:40.925
AUDIENCE: [INAUDIBLE]
any knowledge globally?

00:19:40.925 --> 00:19:41.800
PROFESSOR: It's hard.

00:19:41.800 --> 00:19:43.508
AUDIENCE: [INAUDIBLE]
it's hard to prove.

00:19:43.508 --> 00:19:44.780
PROFESSOR: Yeah.

00:19:44.780 --> 00:19:47.520
So I think again another sort
of telltale sign for this

00:19:47.520 --> 00:19:51.080
is when people say-- talk
about technology evolving.

00:19:51.080 --> 00:19:54.270
That's another use of language
that raises a question for me,

00:19:54.270 --> 00:19:57.480
because technology
itself in my view,

00:19:57.480 --> 00:19:59.200
I don't think actually evolves.

00:19:59.200 --> 00:20:02.310
Humans change their
uses of technology,

00:20:02.310 --> 00:20:03.960
and technology changes.

00:20:03.960 --> 00:20:06.070
But to say that
technology itself evolves,

00:20:06.070 --> 00:20:09.560
I think this is a
problematic thing.

00:20:09.560 --> 00:20:13.300
Another common way is this idea
that the technologies we have

00:20:13.300 --> 00:20:14.250
are the best.

00:20:14.250 --> 00:20:16.170
That's a very common idea.

00:20:16.170 --> 00:20:17.730
People think oh,
the technologies we

00:20:17.730 --> 00:20:20.085
have are the best.

00:20:20.085 --> 00:20:21.990
Are they the best?

00:20:21.990 --> 00:20:23.105
We don't know.

00:20:23.105 --> 00:20:25.650
AUDIENCE: His whole umbrella,
technological determinism,

00:20:25.650 --> 00:20:28.320
and technology evolves in this
direction and the current one

00:20:28.320 --> 00:20:29.310
is the best.

00:20:29.310 --> 00:20:31.050
I think people
spend a lot of money

00:20:31.050 --> 00:20:33.830
to make us all believe
that particular idea.

00:20:33.830 --> 00:20:36.220
Particularly some
technological tycoons

00:20:36.220 --> 00:20:40.075
out in Seattle, and
maybe in Silicon Valley.

00:20:40.075 --> 00:20:41.700
There's a couple of
guys I have in mind

00:20:41.700 --> 00:20:44.240
who thought it was certainly
in their best interest

00:20:44.240 --> 00:20:47.700
to make you think that in
order to get best access

00:20:47.700 --> 00:20:50.470
the internet, you need this best
technology browser, which works

00:20:50.470 --> 00:20:53.450
with this best operating
system and best platform,

00:20:53.450 --> 00:20:54.670
and it goes in this way.

00:20:54.670 --> 00:20:56.959
And thankfully, we make it.

00:20:56.959 --> 00:20:57.750
PROFESSOR: Exactly.

00:20:57.750 --> 00:21:00.670
And those ideas, I think seep
into the culture and seep

00:21:00.670 --> 00:21:01.640
into the discourse.

00:21:01.640 --> 00:21:06.490
And people who don't engage in
technology, tend to think that.

00:21:06.490 --> 00:21:08.810
And they have a
sort of resignation

00:21:08.810 --> 00:21:12.010
that they don't play an
active role in technology.

00:21:12.010 --> 00:21:13.840
That technology is just there.

00:21:13.840 --> 00:21:15.890
OK, well they buy the
product, or they're

00:21:15.890 --> 00:21:18.730
sort of passive consumers
of technology, and not

00:21:18.730 --> 00:21:20.790
active agents in technology.

00:21:20.790 --> 00:21:23.460
And I think this idea of
technological determinism

00:21:23.460 --> 00:21:26.930
helps us remind ourselves
that we are active agents.

00:21:26.930 --> 00:21:29.440
Well, it may be a little
easier for you guys.

00:21:29.440 --> 00:21:32.730
But it's important
that everyone realize,

00:21:32.730 --> 00:21:34.500
even those unskilled
and untrained

00:21:34.500 --> 00:21:37.770
are active agents in shaping
what technologies happen,

00:21:37.770 --> 00:21:40.590
and what technologies
move forward.

00:21:40.590 --> 00:21:42.870
A lot of people don't
see that all the time.

00:21:42.870 --> 00:21:45.060
OK, so the last
case now, in terms

00:21:45.060 --> 00:21:46.960
of music technology--
in our look

00:21:46.960 --> 00:21:50.565
so far at music technologies,
do we have examples?

00:21:53.530 --> 00:21:55.110
Does our look at
music technology

00:21:55.110 --> 00:21:57.800
support this idea of
technological determinism,

00:21:57.800 --> 00:21:59.910
or does it counter it?

00:21:59.910 --> 00:22:02.530
Can we think of some
things we've seen so far?

00:22:06.060 --> 00:22:08.830
AUDIENCE: Could the development
of a certain type of instrument

00:22:08.830 --> 00:22:11.953
follow a logical
sequence approach?

00:22:11.953 --> 00:22:13.844
Starting at the lute,
moving all the way

00:22:13.844 --> 00:22:15.010
down to the electric guitar?

00:22:15.010 --> 00:22:16.676
PROFESSOR: A determinist
might say, yes,

00:22:16.676 --> 00:22:19.000
the electric guitar
is the logical outcome

00:22:19.000 --> 00:22:21.752
of the [? Ude ?].

00:22:21.752 --> 00:22:22.960
A determinist might say that.

00:22:22.960 --> 00:22:24.002
Do you think that's true?

00:22:24.002 --> 00:22:24.960
AUDIENCE: I don't know.

00:22:24.960 --> 00:22:26.590
I feel like that
probably wouldn't

00:22:26.590 --> 00:22:29.510
be the case, because
it's either looking

00:22:29.510 --> 00:22:34.200
at it like the development and
evolution of the instrument

00:22:34.200 --> 00:22:35.860
affected the course
of music history,

00:22:35.860 --> 00:22:38.235
or it's the other way around,
that music history affected

00:22:38.235 --> 00:22:40.820
the evolution, if you want to
call it, of the instrument.

00:22:40.820 --> 00:22:44.290
So I have to give more
credit to [INAUDIBLE]

00:22:44.290 --> 00:22:45.320
PROFESSOR: Me too.

00:22:45.320 --> 00:22:47.930
AUDIENCE: Quite a bit
of music technology

00:22:47.930 --> 00:22:50.510
follows competing power,
which is determined

00:22:50.510 --> 00:22:53.870
by literally a law,
Moore's law, so--

00:22:53.870 --> 00:22:56.130
PROFESSOR: It's not
determined by Moore's law.

00:22:56.130 --> 00:22:59.470
Moore's law is empirical,
based on an estimation

00:22:59.470 --> 00:23:00.160
of what happens.

00:23:00.160 --> 00:23:01.240
It's not determined--

00:23:01.240 --> 00:23:02.489
AUDIENCE: It is a law, though.

00:23:05.250 --> 00:23:07.450
PROFESSOR: I don't
think it's a law.

00:23:07.450 --> 00:23:09.658
AUDIENCE: If you're not
coming out with a better chip

00:23:09.658 --> 00:23:11.960
every 18 months, you're
breaking the law.

00:23:11.960 --> 00:23:14.562
It's pretty important that you--

00:23:14.562 --> 00:23:15.520
PROFESSOR: Is it a law?

00:23:15.520 --> 00:23:16.340
I don't think--

00:23:16.340 --> 00:23:20.642
AUDIENCE: You get booted out of
Silicon Valley, if you don't.

00:23:20.642 --> 00:23:22.070
It's a real thing.

00:23:22.070 --> 00:23:25.036
PROFESSOR: It's a
economic practice.

00:23:25.036 --> 00:23:26.160
So music techno-- go ahead.

00:23:28.720 --> 00:23:30.428
AUDIENCE: [INAUDIBLE]
seems very confused

00:23:30.428 --> 00:23:33.130
that they have no
way of measuring

00:23:33.130 --> 00:23:35.483
how successful a theory is.

00:23:35.483 --> 00:23:37.191
I feel like what we
need to be looking at

00:23:37.191 --> 00:23:38.720
is their predictive power.

00:23:38.720 --> 00:23:42.440
So if we have a theory that
says technologies are going

00:23:42.440 --> 00:23:44.065
to determine what
happens next, then we

00:23:44.065 --> 00:23:46.564
should be able to kind of put
these building blocks together

00:23:46.564 --> 00:23:47.620
from where we are now.

00:23:47.620 --> 00:23:49.453
And say, OK, ten years
down the line, here's

00:23:49.453 --> 00:23:51.610
exactly what's
going to be there.

00:23:51.610 --> 00:23:54.910
And then we can measure that.

00:23:54.910 --> 00:23:56.140
This hasn't happened.

00:23:56.140 --> 00:23:58.140
These theories have been
around for a long time.

00:23:58.140 --> 00:24:00.580
So I feel like the
ones that claim

00:24:00.580 --> 00:24:04.426
that things are deterministic,
that they should really

00:24:04.426 --> 00:24:05.632
be able to back that up.

00:24:05.632 --> 00:24:07.269
And they haven't.

00:24:07.269 --> 00:24:08.060
PROFESSOR: I agree.

00:24:08.060 --> 00:24:09.726
And that makes it
very clear and simple.

00:24:09.726 --> 00:24:11.820
But what I find
really interesting

00:24:11.820 --> 00:24:15.040
is that these ideas of
technological determinism

00:24:15.040 --> 00:24:17.990
sort of seep into our culture,
and seep into our discourse.

00:24:17.990 --> 00:24:19.644
Even though, as you
point out there,

00:24:19.644 --> 00:24:22.060
there's some ways where, very
clearly, it could be tested,

00:24:22.060 --> 00:24:24.709
and could be shown
to not always follow.

00:24:24.709 --> 00:24:25.625
And so in many cases--

00:24:25.625 --> 00:24:27.468
AUDIENCE: They do
say-- he mentioned

00:24:27.468 --> 00:24:28.905
Robert Heilbroner in there.

00:24:28.905 --> 00:24:31.940
And Heilbroner has this thing
about technological determinism

00:24:31.940 --> 00:24:34.960
that one of the reasons that you
might [INAUDIBLE] or anything,

00:24:34.960 --> 00:24:37.730
is that people have
predicted historically

00:24:37.730 --> 00:24:39.160
technological developments.

00:24:39.160 --> 00:24:40.826
[INTERPOSING VOICES] which
is sort of a crazy idea,

00:24:40.826 --> 00:24:43.325
I think, because it's not like
they printed a lot of things.

00:24:43.325 --> 00:24:46.380
And so it's sort of
hard, that's [INAUDIBLE].

00:24:46.380 --> 00:24:49.630
PROFESSOR: Nostradamus is
right three out of 1,000 times.

00:24:49.630 --> 00:24:52.734
AUDIENCE: People say there's
going to be flying skateboards.

00:24:52.734 --> 00:24:55.044
There's all these things
that people predict,

00:24:55.044 --> 00:24:56.592
and [INAUDIBLE].

00:24:56.592 --> 00:24:57.300
PROFESSOR: Right.

00:24:57.300 --> 00:25:00.300
Well, there is the issue of a
sort of simultaneous discovery,

00:25:00.300 --> 00:25:03.340
where people separated in
different regions come up

00:25:03.340 --> 00:25:05.810
with the same inventions
at the same time.

00:25:05.810 --> 00:25:07.880
There's many examples
of that, which

00:25:07.880 --> 00:25:10.000
is kind of an interesting case.

00:25:10.000 --> 00:25:11.500
But again, to
music technologies.

00:25:11.500 --> 00:25:14.740
I think music technologies
pose an interesting challenge

00:25:14.740 --> 00:25:16.670
to this idea of
technological determinism

00:25:16.670 --> 00:25:21.340
because, one, we see the role of
aesthetics and cultural factors

00:25:21.340 --> 00:25:23.870
making the choices that
determine what technologies are

00:25:23.870 --> 00:25:26.710
successful and
how they are used.

00:25:26.710 --> 00:25:32.330
Nobody intended the disk
based player of a gramophone

00:25:32.330 --> 00:25:34.220
to be used as a
musical instrument

00:25:34.220 --> 00:25:37.210
for altering the playback speed.

00:25:37.210 --> 00:25:39.330
Nobody intended the
components of a radio

00:25:39.330 --> 00:25:42.410
to be hacked together
to build a synthesizer.

00:25:42.410 --> 00:25:46.790
Nobody intended
countless other examples

00:25:46.790 --> 00:25:50.520
of repurposing-- that
was one of our articles,

00:25:50.520 --> 00:25:52.750
talked about the idea of
repurposing technologies.

00:25:52.750 --> 00:25:55.540
We see many examples of
that in music technology.

00:25:55.540 --> 00:25:57.110
We were talking
about the attraction

00:25:57.110 --> 00:25:58.570
to 8-bit technologies.

00:25:58.570 --> 00:26:01.000
We see people going back
to older technologies,

00:26:01.000 --> 00:26:04.030
inferior technologies,
for aesthetic reasons,

00:26:04.030 --> 00:26:07.730
not for a technological
logical sequence.

00:26:07.730 --> 00:26:09.940
And I think those give us
some interesting examples

00:26:09.940 --> 00:26:13.650
of deviating from the
deterministic approach.

00:26:13.650 --> 00:26:15.520
AUDIENCE: Yeah, I
absolutely agree.

00:26:15.520 --> 00:26:17.960
And especially-- you
were saying that in order

00:26:17.960 --> 00:26:19.970
to subscribe to the
deterministic view,

00:26:19.970 --> 00:26:22.900
you need to have some
sort of predictive power.

00:26:22.900 --> 00:26:24.810
It enumerates that
you're actually

00:26:24.810 --> 00:26:27.050
committing some kind of
fallacy that's really common.

00:26:27.050 --> 00:26:30.430
Which is to say, because it
did happen in this direction,

00:26:30.430 --> 00:26:32.600
you forget that there were
other possible directions

00:26:32.600 --> 00:26:33.830
that it could have
gone into, that

00:26:33.830 --> 00:26:35.330
were dependent on
a bunch of factors

00:26:35.330 --> 00:26:36.698
you didn't even think about.

00:26:36.698 --> 00:26:39.370
I wonder how dependent
rock and roll, and all

00:26:39.370 --> 00:26:42.190
of the technologies
that go along with it,

00:26:42.190 --> 00:26:44.520
be it distortion pedals or
different kinds of guitars

00:26:44.520 --> 00:26:48.600
and pickups, is dependant on
World War II, and the baby

00:26:48.600 --> 00:26:51.460
boomer generation happening, and
the rebellious kids and stuff.

00:26:51.460 --> 00:26:52.080
PROFESSOR: You mean
just independent

00:26:52.080 --> 00:26:52.815
of the technologies?

00:26:52.815 --> 00:26:53.850
AUDIENCE: If World
War II never happened,

00:26:53.850 --> 00:26:56.177
we'd never have
distortion pedals, right?

00:26:56.177 --> 00:26:57.488
It's probably true.

00:27:00.520 --> 00:27:03.541
PROFESSOR: Well, the Scott model
talks about an interconnected

00:27:03.541 --> 00:27:04.040
web.

00:27:04.040 --> 00:27:06.797
And I don't like to
give agency to machines,

00:27:06.797 --> 00:27:08.130
that's just my personal opinion.

00:27:08.130 --> 00:27:10.713
If you guys want to give agency
to the machines in your lives,

00:27:10.713 --> 00:27:12.216
that's good for you.

00:27:12.216 --> 00:27:14.027
AUDIENCE: [INAUDIBLE]
music though?

00:27:14.027 --> 00:27:15.110
PROFESSOR: No, not at all.

00:27:15.110 --> 00:27:17.270
In fact, I've argued
very strenuously

00:27:17.270 --> 00:27:20.430
for the opposite
of that, and that's

00:27:20.430 --> 00:27:23.310
a case where it becomes
really, really interesting.

00:27:23.310 --> 00:27:26.030
But this is, I think,
a useful thing for us

00:27:26.030 --> 00:27:28.450
to think about as
we move forward.

00:27:28.450 --> 00:27:30.280
Other comments?

00:27:30.280 --> 00:27:33.030
OK, let's make some noise.

