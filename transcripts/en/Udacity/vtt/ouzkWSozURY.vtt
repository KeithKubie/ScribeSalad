WEBVTT
Kind: captions
Language: en

00:00:00.380 --> 00:00:02.450
So in order to elaborate on all of that,

00:00:02.450 --> 00:00:06.130
Michael, I want to ask
you a simple question.

00:00:06.130 --> 00:00:07.310
And here it is.

00:00:07.310 --> 00:00:10.210
What makes reinforcement learning hard?

00:00:10.210 --> 00:00:12.790
&gt;&gt; I'm going to say
living on the streets.

00:00:12.790 --> 00:00:14.270
&gt;&gt; Makes reinforcement learning hard?

00:00:14.270 --> 00:00:15.840
&gt;&gt; Yeah, it sort of hardens it.

00:00:15.840 --> 00:00:19.580
&gt;&gt; Let me let me ask that
question slightly differently.

00:00:19.580 --> 00:00:20.110
&gt;&gt; Okay.

00:00:20.110 --> 00:00:24.470
&gt;&gt; From a computational point of view,
from an algorithmic point of view,

00:00:24.470 --> 00:00:26.150
what makes reinforcement learning hard?

00:00:26.150 --> 00:00:29.400
&gt;&gt; So well there's a lot of
things we've been talking about.

00:00:29.400 --> 00:00:32.530
There's the notion of delayed
reward is a very tricky thing.

00:00:32.530 --> 00:00:35.600
&gt;&gt; I like that one, I like that one
enough for me to write it down.

00:00:35.600 --> 00:00:36.700
&gt;&gt; You're not going to wait?

00:00:36.700 --> 00:00:37.210
&gt;&gt; No, I'm not going to wait,

00:00:37.210 --> 00:00:41.030
I'm just going to write it down right
now because it's ironic that way.

00:00:41.030 --> 00:00:44.210
&gt;&gt; And a related issue well
there's two pieces in that.

00:00:44.210 --> 00:00:45.680
There's a delay and there's a reward.

00:00:45.680 --> 00:00:49.050
The reward part is like unlike
say supervised learning

00:00:49.050 --> 00:00:52.810
reinforcement learning is trying to do
its job based on very weak feedback.

00:00:52.810 --> 00:00:57.770
That it's only told how it's doing and
not necessarily what it should be doing.

00:00:57.770 --> 00:01:01.730
&gt;&gt; Right which makes it very different
from normal function approximation,

00:01:01.730 --> 00:01:03.790
in the way that we think
about in supervised learning.

00:01:03.790 --> 00:01:04.519
Okay, what else?

00:01:04.519 --> 00:01:06.590
&gt;&gt; If we do try to use something
like supervised learning,

00:01:06.590 --> 00:01:10.140
one thing that makes the function
approximation hard is

00:01:10.140 --> 00:01:12.160
the idea that it's kind
of a moving target.

00:01:12.160 --> 00:01:16.640
We tend to base our estimates
on our current estimates,

00:01:16.640 --> 00:01:21.470
as opposed to anything kind of
legitimately valid in the world.

00:01:21.470 --> 00:01:24.050
Right, so doing estimates
based on estimates is, well,

00:01:24.050 --> 00:01:25.070
like averaging averages.

00:01:27.810 --> 00:01:30.250
&gt;&gt; Okay, I want to say
bootstrapping is the issue.

00:01:30.250 --> 00:01:31.770
&gt;&gt; Yeah I agree with that.

00:01:31.770 --> 00:01:33.700
Bootstrapping is sort of a problem.

00:01:33.700 --> 00:01:39.060
I think I would probably summarize that
as the problem with the need to do

00:01:39.060 --> 00:01:40.060
good exploration.

00:01:40.060 --> 00:01:42.920
And that's a little different
from the normal supervised

00:01:42.920 --> 00:01:45.360
learning set up where you've got
a bunch of training examples.

00:01:45.360 --> 00:01:49.550
Here because you only have estimates
in the reinforcement learning context,

00:01:49.550 --> 00:01:52.940
you have to do a bunch of extra
things to actually figure

00:01:52.940 --> 00:01:55.560
out how to get good estimates so
that you can actually do good learning.

00:01:55.560 --> 00:02:00.180
So one more thing I think sort of worth
mentioning that makes this sort of hard

00:02:00.180 --> 00:02:03.420
and it's kind of very basic kind
of computational problems, right.

00:02:03.420 --> 00:02:06.330
You know when we describe computation
complexity we talk about it in terms

00:02:06.330 --> 00:02:06.860
of big o.

00:02:06.860 --> 00:02:09.090
And the question is big o of what?

00:02:09.090 --> 00:02:10.370
So what's the what here?

00:02:10.370 --> 00:02:13.170
What's the in,
in our big o that makes ROR?

00:02:13.170 --> 00:02:14.540
&gt;&gt; Well there's a lot of them.

00:02:14.540 --> 00:02:15.070
&gt;&gt; Like what?

00:02:15.070 --> 00:02:17.560
&gt;&gt; Well I mean if we're
thinking about sort of M.V.P.

00:02:17.560 --> 00:02:21.841
style reinforcement learning then
the size of the state space is a big o.

00:02:21.841 --> 00:02:23.920
&gt;&gt; Mm hm.

00:02:23.920 --> 00:02:26.530
&gt;&gt; The size of the temporal
window in some sense,

00:02:26.530 --> 00:02:28.440
like how how delayed is the reward.

00:02:28.440 --> 00:02:29.550
&gt;&gt; Mm hm.
&gt;&gt; There's

00:02:29.550 --> 00:02:31.640
issues on what could be
the actions as well.

00:02:31.640 --> 00:02:34.160
There's lots of things that could scale.

00:02:34.160 --> 00:02:38.120
But I guess what makes state super hard
is that you can very easily specify

00:02:38.120 --> 00:02:42.410
a problem that has a state space
with an exponential number of states

00:02:42.410 --> 00:02:46.710
that is still relatively simple, but
algorithms that don't generalize

00:02:46.710 --> 00:02:50.230
between states are not going to be able
to learning them very effectively.

00:02:50.230 --> 00:02:51.250
&gt;&gt; I think all of what you said is true.

00:02:51.250 --> 00:02:53.920
Some of it, I think, is kind of captured
not necessarily as explicitly in

00:02:53.920 --> 00:02:55.130
the first two bullet points.

00:02:55.130 --> 00:02:57.940
But I'd like to focus on states and
actions.

00:02:57.940 --> 00:03:02.630
The problem is that the learning that
we need to do grows super linearly

00:03:02.630 --> 00:03:07.070
in general, we aren't very, very careful
and smart in the number of states and

00:03:07.070 --> 00:03:08.660
the number of actions.

00:03:08.660 --> 00:03:12.150
And so we really need to have kind
of clever ways of thinking about

00:03:12.150 --> 00:03:15.270
learning across multiple states and
perhaps multiple actions.

00:03:15.270 --> 00:03:17.940
And in fact when you talked
about function approximation and

00:03:17.940 --> 00:03:21.810
generalization, I would claim that
at least in sort of this discussion,

00:03:21.810 --> 00:03:25.580
we could talk about that as a way of
dealing with generalizing overstates,

00:03:25.580 --> 00:03:29.910
that you did function
approximation over value functions.

00:03:29.910 --> 00:03:34.560
Right, so here you tried one way of
dealing with the problem of scale

00:03:34.560 --> 00:03:36.650
when it comes to states.

00:03:36.650 --> 00:03:39.400
Is well,
if I learn something about one state,

00:03:39.400 --> 00:03:43.110
I want to somehow have that teach me
something about a bunch of other states.

00:03:43.110 --> 00:03:46.870
And then I won't have to visit every
single state an infinite number of

00:03:46.870 --> 00:03:49.460
times, and take every action
an infinite number of times.

00:03:49.460 --> 00:03:51.380
And that's really good,
that's really important, right, and

00:03:51.380 --> 00:03:56.930
the trick here is I can see many, many
states by just seeing a single state and

00:03:56.930 --> 00:04:00.980
that's what function approximation over
the value functions actually gives me.

00:04:00.980 --> 00:04:02.430
Agreed?
&gt;&gt; Yeah I don't know that I'd say it

00:04:02.430 --> 00:04:05.800
quite that way, but
that we learn about many states

00:04:05.800 --> 00:04:09.120
when we learn about other states or
something like that.

00:04:09.120 --> 00:04:11.900
But yeah, that's absolutely the way
I was thinking about function

00:04:11.900 --> 00:04:14.570
approximation is helping us to
kind of fill in the gaps for

00:04:14.570 --> 00:04:16.959
states that we haven't
visited nearly as often.

00:04:16.959 --> 00:04:20.690
&gt;&gt; So in fact, I could be aggressive and
ask you the question,

00:04:20.690 --> 00:04:22.950
why did you do all those function
approximation over states but

00:04:22.950 --> 00:04:25.840
you didn't do any function
approximation over actions, Michael?

00:04:25.840 --> 00:04:26.948
&gt;&gt; That would be aggressive
&gt;&gt; And

00:04:26.948 --> 00:04:32.430
the [LAUGH] I don't have
a wonderful answer to that.

00:04:32.430 --> 00:04:37.120
I mean that it's tradition, in the sense
that a lot of the MDPs that people have

00:04:37.120 --> 00:04:40.530
been working with really do try to keep
the action space relatively small and

00:04:40.530 --> 00:04:44.250
expand in the state space but that's
not really that good of an answer.

00:04:44.250 --> 00:04:49.130
I think action, large actions bases are
really important tom we just don't have

00:04:49.130 --> 00:04:50.510
as good ways to deal with them.

00:04:50.510 --> 00:04:54.650
For one thing the computing the max
ends up being hard in q learning.

00:04:54.650 --> 00:04:57.510
&gt;&gt; Right so in fact what I'm going
to try to talk about in the rest of

00:04:57.510 --> 00:05:00.610
the lesson is trying to do To
generalizations of the sort of

00:05:00.610 --> 00:05:02.360
generalization that you
are talking about and

00:05:02.360 --> 00:05:06.200
think about how we can learn
about actions and how we can

00:05:06.200 --> 00:05:10.690
abstract over actions in the same way
that you are abstracting over states.

00:05:10.690 --> 00:05:12.760
&gt;&gt; That seems great and
a little bit meta.

00:05:12.760 --> 00:05:13.570
&gt;&gt; A little bit meta.

00:05:13.570 --> 00:05:17.190
And so that's why the lessons
generalizing over generalizing and

00:05:17.190 --> 00:05:20.220
the whole point in the end I'm
going to claim other than just sort of

00:05:20.220 --> 00:05:23.640
it's interesting in and of itself is
that it's going to allow us to scale.

00:05:23.640 --> 00:05:26.960
So all of these tricks ultimately
have a kind of practical use

00:05:26.960 --> 00:05:29.100
including function approximation
of allowing us to scale.

