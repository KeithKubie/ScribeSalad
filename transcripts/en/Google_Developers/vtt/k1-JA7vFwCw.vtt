WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.832
[MUSIC PLAYING]

00:00:01.832 --> 00:00:03.290
NATALIE VILLALOBOS:
Good afternoon.

00:00:03.290 --> 00:00:04.915
Well, actually, it's
still the morning.

00:00:04.915 --> 00:00:07.440
So good morning, Google
I/O Live Stream viewers.

00:00:07.440 --> 00:00:08.219
Welcome.

00:00:08.219 --> 00:00:10.010
I'm Natalie Villalobos,
Women in Technology

00:00:10.010 --> 00:00:11.410
advocate here at Google.

00:00:11.410 --> 00:00:15.020
And I'm sitting with
Aheri Stanford-Asiyo.

00:00:15.020 --> 00:00:17.070
She is an incredible
woman who has recently

00:00:17.070 --> 00:00:19.250
been working with the
Hack4Congress competition

00:00:19.250 --> 00:00:21.250
to solve for amazing
things in the world.

00:00:21.250 --> 00:00:21.970
Welcome, Aheri.

00:00:21.970 --> 00:00:23.260
AHERI STANFORD-ASIYO: Thank
you so much for having me.

00:00:23.260 --> 00:00:23.590
It's a pleasure.

00:00:23.590 --> 00:00:24.850
NATALIE VILLALOBOS: So
how did you get started?

00:00:24.850 --> 00:00:26.705
Actually, tell me a little
bit about your background.

00:00:26.705 --> 00:00:28.300
You have a fascinating
background.

00:00:28.300 --> 00:00:29.960
AHERI STANFORD-ASIYO:
Well, I agree.

00:00:29.960 --> 00:00:32.189
It's not entirely common.

00:00:32.189 --> 00:00:36.710
I was actually recruited
by the federal government

00:00:36.710 --> 00:00:41.070
back in-- this was back in 2005.

00:00:41.070 --> 00:00:43.570
They were hiring for
security professionals.

00:00:43.570 --> 00:00:46.320
And I had an international
studies degree,

00:00:46.320 --> 00:00:50.570
was teachable, and passed
a number of intelligence

00:00:50.570 --> 00:00:52.590
and writing and tests.

00:00:52.590 --> 00:00:55.256
NATALIE VILLALOBOS: Did you have
physical stamina tests as well?

00:00:56.599 --> 00:00:58.890
AHERI STANFORD-ASIYO: We'll
get into that really later.

00:00:58.890 --> 00:00:59.460
NATALIE VILLALOBOS: OK, OK.

00:00:59.460 --> 00:01:02.095
I just don't know if Hollywood
is depicting it-- in my mind,

00:01:02.095 --> 00:01:03.860
you're, like, running
miles with bricks.

00:01:03.860 --> 00:01:05.360
AHERI STANFORD-ASIYO:
You've totally

00:01:05.360 --> 00:01:06.984
got to go through
the clearance process

00:01:06.984 --> 00:01:09.290
before they even get you
physically out there.

00:01:09.290 --> 00:01:10.373
NATALIE VILLALOBOS: Ready.

00:01:10.373 --> 00:01:11.220
OK.

00:01:11.220 --> 00:01:14.035
AHERI STANFORD-ASIYO: But in
case, they went, hey, you know,

00:01:14.035 --> 00:01:15.410
she's trainable,
she's teachable,

00:01:15.410 --> 00:01:18.980
and we need some people to train
up as security professionals.

00:01:18.980 --> 00:01:22.930
And so I took a role
fresh out of college.

00:01:22.930 --> 00:01:26.350
And really just learned how
to protect people, facilities,

00:01:26.350 --> 00:01:27.090
and information.

00:01:27.090 --> 00:01:29.340
And that involved a lot
of really cool technology.

00:01:29.340 --> 00:01:31.340
NATALIE VILLALOBOS: And
what three-letter agency

00:01:31.340 --> 00:01:32.191
was this, again?

00:01:32.191 --> 00:01:34.190
AHERI STANFORD-ASIYO: So
I was hired by the CIA.

00:01:34.190 --> 00:01:34.550
NATALIE VILLALOBOS: CIA.

00:01:34.550 --> 00:01:34.890
AHERI STANFORD-ASIYO: Yes.

00:01:34.890 --> 00:01:37.060
A lot of people go,
oh, is that NSA?

00:01:37.060 --> 00:01:38.230
Very hot topic, of course.

00:01:38.230 --> 00:01:40.146
NATALIE VILLALOBOS:
There's a lot of acronyms.

00:01:40.146 --> 00:01:42.419
So when you bring this
up, what do people do?

00:01:42.419 --> 00:01:43.585
Do they usually shrink away?

00:01:43.585 --> 00:01:44.500
Do they lean in?

00:01:44.500 --> 00:01:46.400
Do they want to know
classified secrets

00:01:46.400 --> 00:01:47.650
and you've just got to,
like, shut them down?

00:01:47.650 --> 00:01:48.930
AHERI STANFORD-ASIYO: Wow.

00:01:48.930 --> 00:01:52.250
So as a security officer, I am
very attentive to other folks'

00:01:52.250 --> 00:01:54.150
reactions to this.

00:01:54.150 --> 00:01:59.840
And frankly, we live
in a world now where

00:01:59.840 --> 00:02:06.780
people are really becoming
very, very sensitized to issues

00:02:06.780 --> 00:02:10.642
of government surveillance.

00:02:10.642 --> 00:02:12.850
NATALIE VILLALOBOS: Yeah,
Big Brother-y type feeling.

00:02:12.850 --> 00:02:13.130
Yeah.

00:02:13.130 --> 00:02:14.921
AHERI STANFORD ASIYO:
Big Brother thoughts.

00:02:14.921 --> 00:02:16.706
And so naturally, they
kind of want to go,

00:02:16.706 --> 00:02:22.980
OK, are you a part of
the Empire, which is sort

00:02:22.980 --> 00:02:24.502
of covertly plotting against--

00:02:24.502 --> 00:02:26.460
NATALIE VILLALOBOS: Which
may mean "Star Wars."

00:02:26.460 --> 00:02:27.000
AHERI STANFORD-ASIYO:
That's fair.

00:02:27.000 --> 00:02:27.840
That's fair.

00:02:27.840 --> 00:02:29.750
Right.

00:02:29.750 --> 00:02:32.130
No, there's a fair
amount of curiosity.

00:02:32.130 --> 00:02:39.070
But frankly, I'm accustomed to
being more observant of where

00:02:39.070 --> 00:02:42.330
that curiosity is
coming from than sort

00:02:42.330 --> 00:02:48.000
of taking a perspective
of what's more common.

00:02:48.000 --> 00:02:50.585
I would say the most common
thing is just curiosity.

00:02:50.585 --> 00:02:52.210
And then maybe a
little protectiveness.

00:02:52.210 --> 00:02:53.620
Is there anything that
you can't tell me?

00:02:53.620 --> 00:02:55.150
NATALIE VILLALOBOS: Or do
you know everything about me?

00:02:55.150 --> 00:02:56.410
AHERI STANFORD-ASIYO: Right.

00:02:56.410 --> 00:02:58.580
NATALIE VILLALOBOS: So you
did that for many years,

00:02:58.580 --> 00:02:59.330
working with them.

00:02:59.330 --> 00:03:00.230
AHERI STANFORD-ASIYO:
About eight years.

00:03:00.230 --> 00:03:01.100
NATALIE VILLALOBOS:
About eight years.

00:03:01.100 --> 00:03:02.570
And now you're in
the tech world.

00:03:02.570 --> 00:03:03.821
How did you land here and why?

00:03:03.821 --> 00:03:05.320
AHERI STANFORD
ASIYO: Good question.

00:03:05.320 --> 00:03:05.910
Sure.

00:03:05.910 --> 00:03:08.951
Well, first of all, it's
always been a passion for me.

00:03:08.951 --> 00:03:11.450
I was lucky enough to grow up
in the Research Triangle Park,

00:03:11.450 --> 00:03:13.030
an area of North Carolina.

00:03:13.030 --> 00:03:15.030
NATALIE VILLALOBOS:
Yeah, I've been there.

00:03:15.030 --> 00:03:18.012
AHERI STANFORD-ASIYO: It's
a great place to grow up.

00:03:18.012 --> 00:03:19.470
Actually, they say
it's the highest

00:03:19.470 --> 00:03:21.210
per capita PhDs in the country.

00:03:21.210 --> 00:03:22.720
Maybe that's an old statistic.

00:03:22.720 --> 00:03:25.949
But we're talking about Raleigh,
Durham, and Chapel Hill.

00:03:25.949 --> 00:03:27.990
And they, of course, have
excellent universities,

00:03:27.990 --> 00:03:30.120
one of which is Duke University.

00:03:30.120 --> 00:03:33.350
And as a result of
really just interacting

00:03:33.350 --> 00:03:38.570
with a lot of folks, including,
well, both of my parents--

00:03:38.570 --> 00:03:42.080
my father, a student, and my
mom working in the IT department

00:03:42.080 --> 00:03:43.170
at Duke.

00:03:43.170 --> 00:03:46.725
Just it was easy for it to be
sort of dinnertime conversation

00:03:46.725 --> 00:03:53.800
to talk about how binary
works or token ring, which

00:03:53.800 --> 00:03:57.570
IBM and token ring, I've just
dated to, like, the early '80s.

00:03:57.570 --> 00:03:59.130
And that's a little
embarrassing.

00:03:59.130 --> 00:04:01.100
I was eight in 1988.

00:04:01.100 --> 00:04:01.850
This is awkward.

00:04:01.850 --> 00:04:02.720
NATALIE VILLALOBOS:
It's all good.

00:04:02.720 --> 00:04:04.261
AHERI STANFORD-ASIYO:
But the reality

00:04:04.261 --> 00:04:09.240
was that I was exposed to
kind of the realities of how

00:04:09.240 --> 00:04:13.090
problem-solving can be done
with computers very early.

00:04:13.090 --> 00:04:17.120
And later on, I think that
really made it possible

00:04:17.120 --> 00:04:21.540
for once I had come out of
my undergraduate education,

00:04:21.540 --> 00:04:24.400
it was easy for my
supervisors and colleagues

00:04:24.400 --> 00:04:28.160
to see that there was some
complex technology that

00:04:28.160 --> 00:04:30.700
made sense to me in a
different way than perhaps

00:04:30.700 --> 00:04:34.310
a lot of my predecessors
in security positions.

00:04:34.310 --> 00:04:36.620
So that really kind
of, I think, led

00:04:36.620 --> 00:04:39.928
them to shift me over to
supporting more advanced work.

00:04:39.928 --> 00:04:41.303
NATALIE VILLALOBOS:
So it's cool.

00:04:41.303 --> 00:04:43.810
So you had mentors or
people who saw that in you

00:04:43.810 --> 00:04:45.665
and wanted to give you more.

00:04:45.665 --> 00:04:48.282
It's a success problem to take
on more responsibility, right?

00:04:48.282 --> 00:04:49.990
AHERI STANFORD-ASIYO:
That's always true.

00:04:49.990 --> 00:04:51.760
And it's natural for
employers to want

00:04:51.760 --> 00:04:54.817
to push you to be of the
highest use and value.

00:04:54.817 --> 00:04:55.900
NATALIE VILLALOBOS: Right.

00:04:55.900 --> 00:04:57.950
And so the Hack4Congress
competition,

00:04:57.950 --> 00:05:01.090
you were saying it was your
first competition and foray

00:05:01.090 --> 00:05:02.130
into the tech industry.

00:05:02.130 --> 00:05:02.530
AHERI STANFORD-ASIYO:
That's correct.

00:05:02.530 --> 00:05:03.905
NATALIE VILLALOBOS:
How did it go

00:05:03.905 --> 00:05:05.307
and what were you solving for?

00:05:05.307 --> 00:05:07.390
AHERI STANFORD-ASIYO: So
it was a wonderful event.

00:05:07.390 --> 00:05:12.810
This was over two days over a
weekend in March of this year.

00:05:12.810 --> 00:05:17.590
And the first thing
that I think of

00:05:17.590 --> 00:05:22.580
is kind of the crowd
that convened there.

00:05:22.580 --> 00:05:24.700
We're not talking
about folks who

00:05:24.700 --> 00:05:31.160
were sort of independently
coming up with how they wanted

00:05:31.160 --> 00:05:33.060
to build something,
and then coming,

00:05:33.060 --> 00:05:35.490
and sort of presenting
these as solutions.

00:05:35.490 --> 00:05:39.730
These were folks who are
committed to civic tech, people

00:05:39.730 --> 00:05:44.250
who recognize that our
government lacks efficiencies,

00:05:44.250 --> 00:05:47.850
and that there are
civil servants who

00:05:47.850 --> 00:05:52.000
want to do the right thing, and
yet may not have the ability

00:05:52.000 --> 00:05:54.847
to do that, or are
barraged with information.

00:05:54.847 --> 00:05:56.430
And that goes all
the way up the line,

00:05:56.430 --> 00:05:58.770
from congressional
staffers who are

00:05:58.770 --> 00:06:01.750
running around trying to
assist people, representatives

00:06:01.750 --> 00:06:03.260
themselves.

00:06:03.260 --> 00:06:08.190
Darrell Issa was a huge
proponent for Hack4Congress.

00:06:08.190 --> 00:06:11.120
And he was very frank
about the big problem,

00:06:11.120 --> 00:06:16.130
which is there's stuff
that human beings miss.

00:06:16.130 --> 00:06:19.490
And yes, I'm a congressman,
but guess what?

00:06:19.490 --> 00:06:22.550
I need regular folks'
help to see where

00:06:22.550 --> 00:06:23.940
attention needs to be put.

00:06:26.720 --> 00:06:31.050
So convening a group of people
who kind of have their antennas

00:06:31.050 --> 00:06:34.810
up about these
realities really meant

00:06:34.810 --> 00:06:39.049
that we ended up kind of
hacking some cool ideas.

00:06:39.049 --> 00:06:40.590
NATALIE VILLALOBOS:
Is there anything

00:06:40.590 --> 00:06:42.860
you can share about
what you successfully

00:06:42.860 --> 00:06:44.270
found in these projects?

00:06:44.270 --> 00:06:44.720
AHERI STANFORD-ASIYO: Of course.

00:06:44.720 --> 00:06:45.400
Of course.

00:06:45.400 --> 00:06:49.880
So the team that I
ended up on really

00:06:49.880 --> 00:06:53.740
hyper-focused on the issue
of campaign finance reform.

00:06:53.740 --> 00:06:58.530
Now, that is really
a behemoth issue

00:06:58.530 --> 00:07:00.730
and one that is not
something that can be solved

00:07:00.730 --> 00:07:03.130
in a weekend, so to speak.

00:07:03.130 --> 00:07:05.530
But what we did do and
what was exciting about

00:07:05.530 --> 00:07:14.220
it was that we essentially
designed a mobile solution,

00:07:14.220 --> 00:07:18.400
through which folks can very
easily connect their own values

00:07:18.400 --> 00:07:23.960
with the values that are
being shown through voting

00:07:23.960 --> 00:07:29.350
and bringing bills forward for
votes by area representatives.

00:07:29.350 --> 00:07:31.500
So for instance, if I'm
very, very concerned

00:07:31.500 --> 00:07:34.390
about public schools in
my area and that's really

00:07:34.390 --> 00:07:37.710
my top concern, then I can
drag and drop that issue

00:07:37.710 --> 00:07:39.710
to the top of my
profile and choose

00:07:39.710 --> 00:07:41.080
two other issues under that.

00:07:41.080 --> 00:07:45.330
And I can immediately
hit Match Me.

00:07:45.330 --> 00:07:47.330
And through some very
interesting recommendation

00:07:47.330 --> 00:07:50.790
engines, can see
representatives who

00:07:50.790 --> 00:07:53.070
match that profile through
their voting history,

00:07:53.070 --> 00:07:55.160
through some of
the Sunshine APIs

00:07:55.160 --> 00:07:58.560
that have been invented just
by some of the nonprofits,

00:07:58.560 --> 00:08:01.180
especially in the DC area.

00:08:01.180 --> 00:08:03.730
So the whole point
is really to be

00:08:03.730 --> 00:08:05.460
able to find a
matching point, where

00:08:05.460 --> 00:08:10.700
people are engaged through
issues with representatives

00:08:10.700 --> 00:08:13.490
or candidates who are
interested in the same ones

00:08:13.490 --> 00:08:16.104
or who have priorities that
are aligned with theirs.

00:08:16.104 --> 00:08:18.270
NATALIE VILLALOBOS: And I
think that's really great.

00:08:18.270 --> 00:08:20.855
You expressed a user
interface that seemed

00:08:20.855 --> 00:08:22.912
really intuitive, really easy.

00:08:22.912 --> 00:08:24.370
And it's really
nice because I know

00:08:24.370 --> 00:08:26.020
that just even
when I'm voting, I

00:08:26.020 --> 00:08:28.590
get sent, like, paper
pamphlets and update pamphlets.

00:08:28.590 --> 00:08:29.750
And I'm just overwhelmed.

00:08:29.750 --> 00:08:32.390
Like, I don't know what
is the latest version.

00:08:32.390 --> 00:08:36.900
And I just end up going
with estimated guesses,

00:08:36.900 --> 00:08:40.270
maybe whoever's name
I've heard the most.

00:08:40.270 --> 00:08:43.020
And a lot of my friends
will actually get together,

00:08:43.020 --> 00:08:45.709
and they'll host kind
of get-togethers,

00:08:45.709 --> 00:08:48.250
and you have to defend topics
that you normally wouldn't want

00:08:48.250 --> 00:08:50.280
to defend in a voting season.

00:08:50.280 --> 00:08:52.830
And we try to have
these education parties.

00:08:52.830 --> 00:08:54.870
Sometimes I just
can't get to them.

00:08:54.870 --> 00:08:57.780
I love what you're doing-- is
you're making this process more

00:08:57.780 --> 00:08:59.910
accessible to everyone
in a way that we're all

00:08:59.910 --> 00:09:01.480
starting to come
into, and having

00:09:01.480 --> 00:09:02.952
these mobile technologies.

00:09:02.952 --> 00:09:04.410
And so I think it's
really awesome.

00:09:04.410 --> 00:09:05.600
AHERI STANFORD-ASIYO: Thank you.

00:09:05.600 --> 00:09:07.770
What you're talking about,
I think, is a growing trend.

00:09:07.770 --> 00:09:09.144
But there are a
lot of folks that

00:09:09.144 --> 00:09:11.210
are being left behind
who aren't in crowds

00:09:11.210 --> 00:09:12.550
where that's more common.

00:09:12.550 --> 00:09:16.762
And to make that possible is an
interest that I have, for sure.

00:09:16.762 --> 00:09:18.470
NATALIE VILLALOBOS:
And so at Google I/O,

00:09:18.470 --> 00:09:20.390
are there specific
platforms or tools

00:09:20.390 --> 00:09:24.530
that you feel are good for these
civic innovation ideas that

00:09:24.530 --> 00:09:28.000
are going to drive the future
of getting more of the public

00:09:28.000 --> 00:09:29.784
to participate with
the government?

00:09:29.784 --> 00:09:31.700
AHERI STANFORD-ASIYO:
That's a great question.

00:09:31.700 --> 00:09:35.090
So there are some
technologies that I was really

00:09:35.090 --> 00:09:37.170
excited about.

00:09:37.170 --> 00:09:41.030
In particular,
during the keynote,

00:09:41.030 --> 00:09:42.810
there was some discussion
around something

00:09:42.810 --> 00:09:47.107
called Loon, which is
really-- I mean, anyone who

00:09:47.107 --> 00:09:49.190
wasn't able to catch the
keynote or what have you,

00:09:49.190 --> 00:09:51.510
should Google it.

00:09:51.510 --> 00:09:55.520
But why it's so exciting
to me is for instance,

00:09:55.520 --> 00:09:59.250
my parents live in an area
where their internet service

00:09:59.250 --> 00:10:02.830
is pretty much hit or miss.

00:10:02.830 --> 00:10:05.750
And in the age of
technology, it's

00:10:05.750 --> 00:10:07.942
imperative that they be
able to be in access.

00:10:07.942 --> 00:10:10.275
NATALIE VILLALOBOS: Access
to information is important--

00:10:10.275 --> 00:10:10.600
AHERI STANFORD-ASIYO: Exactly.

00:10:10.600 --> 00:10:11.170
That's key.

00:10:11.170 --> 00:10:14.410
That's part of our evolution
as humans, I believe.

00:10:14.410 --> 00:10:17.210
So what Loon is
capable of doing is

00:10:17.210 --> 00:10:22.160
providing a service through--
really, it's a physical device.

00:10:22.160 --> 00:10:26.240
It's hey, we're going
to put this in the air,

00:10:26.240 --> 00:10:29.380
and it lasts for about 100 days.

00:10:29.380 --> 00:10:32.600
It's around the same ideas
that the government uses

00:10:32.600 --> 00:10:35.900
for transmitting communications
data using satellites,

00:10:35.900 --> 00:10:37.820
but it's below the stratosphere.

00:10:37.820 --> 00:10:41.290
It's lower down to the ground.

00:10:41.290 --> 00:10:46.600
So it's basically allowing
low or no connectivity areas

00:10:46.600 --> 00:10:49.200
to be able to be online.

00:10:49.200 --> 00:10:52.520
So to me, that is where
the democratization

00:10:52.520 --> 00:10:55.120
of technology and
civic involvement

00:10:55.120 --> 00:10:56.520
are permitted to meet.

00:10:56.520 --> 00:11:02.610
When everyday people become
linked in with their neighbors,

00:11:02.610 --> 00:11:05.890
with their families,
with their neighborhoods,

00:11:05.890 --> 00:11:08.680
and then out into
the wider areas,

00:11:08.680 --> 00:11:10.065
they're already
being represented

00:11:10.065 --> 00:11:14.130
and laws are being passed
that affect them every day.

00:11:14.130 --> 00:11:19.140
But if they don't have
that two-way communication

00:11:19.140 --> 00:11:22.347
mechanism, then democracy
really isn't working.

00:11:22.347 --> 00:11:24.680
NATALIE VILLALOBOS: And so
it's this pervasive knowledge

00:11:24.680 --> 00:11:26.965
economy that you're
creating with the work

00:11:26.965 --> 00:11:29.090
that you're doing,
and blending it

00:11:29.090 --> 00:11:32.030
with this innovative
technology that we're starting

00:11:32.030 --> 00:11:33.800
to see pop up in the industry.

00:11:33.800 --> 00:11:36.570
So last question.

00:11:36.570 --> 00:11:39.476
And you've already solved for so
much, I feel, in your lifetime

00:11:39.476 --> 00:11:41.100
that you've got a
whole lifetime to go.

00:11:41.100 --> 00:11:42.474
So what do you
want to see solved

00:11:42.474 --> 00:11:46.407
for in the next 50 years?

00:11:46.407 --> 00:11:47.490
AHERI STANFORD-ASIYO: Wow.

00:11:47.490 --> 00:11:48.060
Wow.

00:11:48.060 --> 00:11:48.590
Wow.

00:11:48.590 --> 00:11:48.930
NATALIE VILLALOBOS: Or 10.

00:11:48.930 --> 00:11:49.720
We can go to two.

00:11:49.720 --> 00:11:50.560
We can go to one.

00:11:50.560 --> 00:11:53.170
What is the thing that
you're most passionate to see

00:11:53.170 --> 00:11:53.720
solved for?

00:11:56.370 --> 00:12:01.530
AHERI STANFORD-ASIYO: So I
have two diverging responses

00:12:01.530 --> 00:12:07.910
to that, truly, because
the world is represented

00:12:07.910 --> 00:12:10.360
by very different populations.

00:12:10.360 --> 00:12:12.250
And we see a very
narrow field of it,

00:12:12.250 --> 00:12:15.890
being in a developed country,
where we are in access easily

00:12:15.890 --> 00:12:21.990
to food, water, shelter,
and information.

00:12:21.990 --> 00:12:24.310
There's another
part of the world

00:12:24.310 --> 00:12:26.630
that we don't see every day,
where those things are not

00:12:26.630 --> 00:12:27.850
taken for granted.

00:12:27.850 --> 00:12:31.960
So my initial
response is, well, I

00:12:31.960 --> 00:12:37.110
would love to see materials
design ideas really

00:12:37.110 --> 00:12:41.890
coalesce into products
that can allow

00:12:41.890 --> 00:12:44.940
us to have flat screens
this small on my wrist.

00:12:47.930 --> 00:12:51.640
But on the other side of
it, perhaps material science

00:12:51.640 --> 00:12:57.450
can help me to create a water
bottle that very, very quickly

00:12:57.450 --> 00:13:00.710
filters what would otherwise
be non-potable water

00:13:00.710 --> 00:13:02.200
to make it potable.

00:13:02.200 --> 00:13:04.840
Those are the kinds of
technologies or tools

00:13:04.840 --> 00:13:07.180
that I'm also interested in.

00:13:07.180 --> 00:13:08.430
NATALIE VILLALOBOS: Very cool.

00:13:08.430 --> 00:13:09.846
AHERI STANFORD-ASIYO:
And that is,

00:13:09.846 --> 00:13:15.274
I think, at the core of
what I would call tech,

00:13:15.274 --> 00:13:16.690
because we're
talking about tools.

00:13:16.690 --> 00:13:21.360
We're talking about using
existing materials to make

00:13:21.360 --> 00:13:23.580
something new that's useful.

00:13:23.580 --> 00:13:26.610
And that's really kind of the
intersection of my interest

00:13:26.610 --> 00:13:27.587
in making things.

00:13:27.587 --> 00:13:29.170
NATALIE VILLALOBOS:
Well, thank you so

00:13:29.170 --> 00:13:30.378
much for being with us today.

00:13:30.378 --> 00:13:32.720
It's been a pleasure getting
to spend time with you.

00:13:32.720 --> 00:13:34.510
I'm certainly inspired by
the way that you're doing.

00:13:34.510 --> 00:13:36.020
AHERI STANFORD-ASIYO: Thank
you so much for having me.

00:13:36.020 --> 00:13:37.230
NATALIE VILLALOBOS: Please come
back again to I/O next year,

00:13:37.230 --> 00:13:39.300
and perhaps you'll be
on a stage talking to us

00:13:39.300 --> 00:13:40.200
about what you've created.

00:13:40.200 --> 00:13:40.710
AHERI STANFORD-ASIYO:
You're so kind.

00:13:40.710 --> 00:13:40.970
Thank you very much.

00:13:40.970 --> 00:13:41.400
NATALIE VILLALOBOS:
Thank you so much.

00:13:41.400 --> 00:13:42.800
Thanks.

