WEBVTT
Kind: captions
Language: en

00:00:00.070 --> 00:00:02.029
&gt;&gt; All right, Michael. What's the answer?

00:00:02.029 --> 00:00:04.725
&gt;&gt; All right. Well, let me start off with what I think the

00:00:04.725 --> 00:00:09.190
answer isn't. So, the last one, boosting tends to overfit, if boosting trains

00:00:09.190 --> 00:00:12.420
too long. You just told me a story about that not being true.

00:00:12.420 --> 00:00:14.000
So I'm going to eliminate that

00:00:14.000 --> 00:00:16.260
one from consideration. Boosting training too long.

00:00:16.260 --> 00:00:17.430
&gt;&gt; Oh, nice to know you were listening.

00:00:17.430 --> 00:00:19.670
&gt;&gt; [LAUGH] Boosting training too long, seems like

00:00:19.670 --> 00:00:21.810
not a good reason for it to overfit.

00:00:21.810 --> 00:00:22.430
&gt;&gt; You're correct.

00:00:22.430 --> 00:00:25.155
&gt;&gt; All right. Boosting tends to overfit if it's

00:00:25.155 --> 00:00:31.290
a nonlinear problem. So, that doesn't seem right. I

00:00:31.290 --> 00:00:33.250
mean I guess, no, this one just doesn't seem right

00:00:33.250 --> 00:00:34.860
at all. Like I don't see why, why the problem

00:00:34.860 --> 00:00:37.180
being linear or nonlinear, has anything to do with overfitting.,

00:00:37.180 --> 00:00:37.770
&gt;&gt; Okay.

00:00:37.770 --> 00:00:41.260
&gt;&gt; A whole lot of data is the opposite of what tends

00:00:41.260 --> 00:00:44.160
to cause overfitting. If there's lots of data then you'd think that it

00:00:44.160 --> 00:00:47.030
would actually do a pretty reasonable job of, you know, there's a

00:00:47.030 --> 00:00:50.120
lot to fit. There's a lot going on there. It's unlikely to overfit.

00:00:50.120 --> 00:00:50.410
&gt;&gt; Right, and

00:00:50.410 --> 00:00:54.840
in fact if a whole lot of data included all of the data, and you actually

00:00:54.840 --> 00:00:56.700
could get zero training error over it, then

00:00:56.700 --> 00:01:00.820
you know you have zero training zero generalization error.

00:01:00.820 --> 00:01:02.770
&gt;&gt; because it'll work on the testing data as well, because it's in there.

00:01:02.770 --> 00:01:03.282
&gt;&gt; Right.

00:01:03.282 --> 00:01:08.080
&gt;&gt; All right. Weak learner uses artificial neural network with

00:01:08.080 --> 00:01:10.520
many layers and nodes. So I'm guessing that you wanted

00:01:10.520 --> 00:01:13.170
me to think about that being something that, on its

00:01:13.170 --> 00:01:15.570
own, is prone to overfitting, because it's got a lot

00:01:15.570 --> 00:01:16.320
of parameters.

00:01:16.320 --> 00:01:17.010
&gt;&gt; Sure.

00:01:17.010 --> 00:01:19.584
&gt;&gt; So, if, and now we're doing boosting

00:01:19.584 --> 00:01:21.970
over that. So we fit a neural net, and

00:01:21.970 --> 00:01:24.140
then we fit another neural net, and we

00:01:24.140 --> 00:01:26.280
fit another neural net. And we're combining all the

00:01:26.280 --> 00:01:29.770
outputs together in the correct, weighted way. It's

00:01:29.770 --> 00:01:32.310
not obvious to me that that should be a

00:01:32.310 --> 00:01:36.170
good thing to do. I'm not sure it would overfit, but it seem like it sure could.

00:01:36.170 --> 00:01:38.770
&gt;&gt; OK, so you're, you're, so for now let's

00:01:38.770 --> 00:01:40.600
put a little question mark to it. You think that

00:01:40.600 --> 00:01:42.170
might be the right answer, but you want to think about it some more?

00:01:42.170 --> 00:01:43.440
&gt;&gt; Yeah let me, let me look at the first

00:01:43.440 --> 00:01:48.400
one. Weak learner chooses the weakest output. Well, I mean

00:01:48.400 --> 00:01:50.820
boosting is supposed to work as long as we have

00:01:50.820 --> 00:01:52.780
a weak learner. . And it doesn't matter if it

00:01:52.780 --> 00:01:55.630
chooses the weakest or the strongest. All that matters is

00:01:55.630 --> 00:01:58.130
it does significantly better than a half. So, like I

00:01:58.130 --> 00:01:59.850
feel like the only one, the only one of these

00:01:59.850 --> 00:02:02.800
choices that is likely to be true is the second one.

00:02:02.800 --> 00:02:05.770
&gt;&gt; And that is, in fact, correct. So let me give you an example

00:02:05.770 --> 00:02:07.590
of when that would be correct. So let's imagine

00:02:07.590 --> 00:02:09.940
I have a big powerful neural network that could represent

00:02:09.940 --> 00:02:12.730
any arbitrary functions. Okay, got lots of layers and lots

00:02:12.730 --> 00:02:16.440
of nodes. So, boosting calls it, and it perfectly fits

00:02:16.440 --> 00:02:19.720
the training data, but of course overfits. So then it

00:02:19.720 --> 00:02:23.750
returns, and it's got no error, which means all of

00:02:23.750 --> 00:02:27.760
the examples will have equal weight. And when you go

00:02:27.760 --> 00:02:31.290
through the loop again, you will just call the same

00:02:31.290 --> 00:02:32.850
learner, which will use the same neural

00:02:32.850 --> 00:02:35.030
network, and will return the same neural network.

00:02:35.030 --> 00:02:38.080
So every time you call the learner, you'll get zero training error, but you will

00:02:38.080 --> 00:02:42.230
just get the same neural network over and over and over again. And a weighted

00:02:42.230 --> 00:02:45.090
sum of the same function is just that

00:02:45.090 --> 00:02:47.700
function. So if it overfit, boosting will overfit.

00:02:47.700 --> 00:02:50.790
&gt;&gt; Interesting. And not only will it overfit, but it'll

00:02:50.790 --> 00:02:53.510
just, it'll be stuck in a horrible loop of error.

00:02:53.510 --> 00:02:56.390
&gt;&gt; Right. So that's why this

00:02:56.390 --> 00:02:58.700
is the sort of situation where you can imagine

00:02:58.700 --> 00:03:00.935
boosting a lower fit. If the underlying learners all

00:03:00.935 --> 00:03:02.955
overfit and you can never get them to stop

00:03:02.955 --> 00:03:04.990
overfitting, then there's really not much you can do.

00:03:04.990 --> 00:03:05.650
&gt;&gt; Interesting.

00:03:05.650 --> 00:03:08.028
&gt;&gt; Now, I do want to have a little semantic argument

00:03:08.028 --> 00:03:12.370
with you for a moment, Michael. You used the word strongest at

00:03:12.370 --> 00:03:15.450
some point, when you were talking about using the weakest output. And

00:03:15.450 --> 00:03:17.950
I just want to point out that, that doesn't really mean anything.

00:03:17.950 --> 00:03:19.980
&gt;&gt; What do you mean, it doesn't mean anything?

00:03:19.980 --> 00:03:21.570
&gt;&gt; Well, so what's a strong, what would you call

00:03:21.570 --> 00:03:22.510
a strong learner?

00:03:22.510 --> 00:03:24.830
&gt;&gt; One that is far away from it. If a

00:03:24.830 --> 00:03:26.900
weak learner just has to do a little bit better

00:03:26.900 --> 00:03:28.810
than a half, it seems like a strong learner would

00:03:28.810 --> 00:03:31.780
be something that would be very close to being accurate.

00:03:31.780 --> 00:03:33.900
&gt;&gt; Right. Of course, on the other hand, if

00:03:33.900 --> 00:03:36.289
by that definition all strong learners are also weak learners.

00:03:36.289 --> 00:03:36.774
&gt;&gt; Sure.

00:03:36.774 --> 00:03:39.230
&gt;&gt; Because anything that does better than a half is still doing better

00:03:39.230 --> 00:03:41.420
than a half, which is all it requires to be a weak learner.

00:03:41.420 --> 00:03:43.290
&gt;&gt; Yeah, but that's kind of true of people

00:03:43.290 --> 00:03:46.880
too. Like a strong person is also a weak person.

00:03:46.880 --> 00:03:47.610
&gt;&gt; No.

00:03:47.610 --> 00:03:49.030
&gt;&gt; Well it depends how you define it. So,

00:03:49.030 --> 00:03:51.060
if you say a weak person is someone who can

00:03:51.060 --> 00:03:54.660
at least lift their own arms, then strong people are

00:03:54.660 --> 00:03:56.810
also weak people in that they can lift their arms.

00:03:56.810 --> 00:03:58.750
&gt;&gt; Yes if you define it that way and if I define

00:03:58.750 --> 00:04:00.880
blue to be purple, then I can say blue is purple. But that's

00:04:00.880 --> 00:04:04.570
not how people define weak people. They define weak people, by saying they

00:04:04.570 --> 00:04:08.000
can't lift more than, not that they can lift at least as much.

00:04:08.000 --> 00:04:11.890
&gt;&gt; I see. So it's this piece of terminology that boosting uses that is in

00:04:11.890 --> 00:04:13.034
error, not me.

00:04:13.034 --> 00:04:16.300
&gt;&gt; That's one interpretation. It's not the one that I would use, but

00:04:16.300 --> 00:04:20.022
it's one interpretation. When you say something like a strong learner, I mean,

00:04:20.022 --> 00:04:22.466
it makes sense to use that kind of term, and sort of throw

00:04:22.466 --> 00:04:25.586
it around, and say, well, by a strong learner I mean someone who's,

00:04:25.586 --> 00:04:28.290
or a learner that's going to overfit, or is going to always do

00:04:28.290 --> 00:04:31.580
really well on the training data. But in kind of a technical definition

00:04:31.580 --> 00:04:34.531
it's very difficult to sort of pin down. So don't get too caught

00:04:34.531 --> 00:04:37.090
up what a strong learner means if you want to write a proof.

00:04:37.090 --> 00:04:37.890
Seems fair?

00:04:37.890 --> 00:04:42.570
&gt;&gt; Good point yeah, also, also that this whole notion that strong

00:04:42.570 --> 00:04:45.720
is sometimes defined as not weak. And it is not the case that

00:04:45.720 --> 00:04:47.970
if you have something that's not a weak learner that it's, then

00:04:47.970 --> 00:04:50.880
it's a strong learner. In fact, it's no lear, no learner at all.

00:04:50.880 --> 00:04:53.840
&gt;&gt; Exactly. So, a weak learner's just defined in a way that

00:04:53.840 --> 00:04:57.280
basically says, it gives me at least some information. Good. Let me

00:04:57.280 --> 00:04:59.040
just throw one more thing in here and then we can stop

00:04:59.040 --> 00:05:02.260
talking about this. There's another, a couple of other cases where boosting

00:05:02.260 --> 00:05:04.470
tends to overfit. The one that matters the most, or

00:05:04.470 --> 00:05:07.920
comes up the most, is in the case of pink noise.

00:05:07.920 --> 00:05:10.890
&gt;&gt; Did you say, peak noise?

00:05:10.890 --> 00:05:14.450
&gt;&gt; I said, pink noise. I even wrote it in red, which

00:05:14.450 --> 00:05:17.330
looks like pink. It's a strong pink as opposed to a weak pink.

00:05:17.330 --> 00:05:18.921
&gt;&gt; [LAUGH]

00:05:18.921 --> 00:05:20.910
&gt;&gt; I'm sorry. There's no way for that to be obvious from what we've

00:05:20.910 --> 00:05:22.500
talked about, but as a practical matter,

00:05:22.500 --> 00:05:25.710
pink noise tends to, cause boosting overfit.

00:05:25.710 --> 00:05:27.290
&gt;&gt; Okay, but this is not a term I'm familiar with

00:05:27.290 --> 00:05:31.830
unless you're critiquing the musical stylings of a particular performer.

00:05:31.830 --> 00:05:35.880
&gt;&gt; [LAUGH] No. Although I did recently see, see them in concert. But

00:05:35.880 --> 00:05:39.449
that's a whole other conversation. Okay, so pink noise just means uniform noise.

00:05:40.870 --> 00:05:42.510
&gt;&gt; I thought white noise was uniform noise.

00:05:42.510 --> 00:05:49.600
&gt;&gt; No, white noise is Gaussian noise. Okay, so pink noise is uniform

00:05:49.600 --> 00:05:52.410
noise and white noise is Gaussian noise. This is why, Michael, by the way,

00:05:52.410 --> 00:05:57.580
if you ever try to set up a studio or a cool stereo system in your house, you

00:05:57.580 --> 00:05:59.700
want a pink noise generator. So that it covers

00:05:59.700 --> 00:06:02.780
all the frequencies equally, not just the white noise. generated.

00:06:02.780 --> 00:06:03.520
&gt;&gt; Hm.

00:06:03.520 --> 00:06:08.400
&gt;&gt; But boosting tends to overfit in those sorts of circumstances. And you

00:06:08.400 --> 00:06:10.220
can read more about it in the notes if you want to. But

00:06:10.220 --> 00:06:13.180
the one that I want I really want people to get is, that

00:06:13.180 --> 00:06:17.590
if you have an underlying weak learner that overfits, then it is difficult

00:06:17.590 --> 00:06:20.490
for boosting to overcome that. Because fundamentally you've already done all of

00:06:20.490 --> 00:06:23.060
your overfitting and it's, there's really not much for those things to do.

00:06:23.060 --> 00:06:24.420
&gt;&gt; Okay. Got it?

00:06:24.420 --> 00:06:24.830
&gt;&gt; Got it.

00:06:24.830 --> 00:06:27.940
&gt;&gt; Excellent. It all ties back into margins, and it's all one

00:06:27.940 --> 00:06:30.828
big story, which I think is the lesson of all of machine learning.

