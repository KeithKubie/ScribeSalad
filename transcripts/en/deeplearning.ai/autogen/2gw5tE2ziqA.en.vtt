WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:01.610
 
welcome to the fourth week of this

00:00:01.610 --> 00:00:01.620
welcome to the fourth week of this
 

00:00:01.620 --> 00:00:03.619
welcome to the fourth week of this
course by now you've seen forward

00:00:03.619 --> 00:00:03.629
course by now you've seen forward
 

00:00:03.629 --> 00:00:05.599
course by now you've seen forward
propagation and back propagation in the

00:00:05.599 --> 00:00:05.609
propagation and back propagation in the
 

00:00:05.609 --> 00:00:07.610
propagation and back propagation in the
context of a neural network with a

00:00:07.610 --> 00:00:07.620
context of a neural network with a
 

00:00:07.620 --> 00:00:09.320
context of a neural network with a
single hidden layer as well as logistic

00:00:09.320 --> 00:00:09.330
single hidden layer as well as logistic
 

00:00:09.330 --> 00:00:11.540
single hidden layer as well as logistic
regression and you've learned about

00:00:11.540 --> 00:00:11.550
regression and you've learned about
 

00:00:11.550 --> 00:00:13.610
regression and you've learned about
vectorization and when it's important

00:00:13.610 --> 00:00:13.620
vectorization and when it's important
 

00:00:13.620 --> 00:00:15.650
vectorization and when it's important
initialize the weights randomly if

00:00:15.650 --> 00:00:15.660
initialize the weights randomly if
 

00:00:15.660 --> 00:00:17.870
initialize the weights randomly if
you've done the past company's homework

00:00:17.870 --> 00:00:17.880
you've done the past company's homework
 

00:00:17.880 --> 00:00:19.790
you've done the past company's homework
we've also implemented and seen some of

00:00:19.790 --> 00:00:19.800
we've also implemented and seen some of
 

00:00:19.800 --> 00:00:22.099
we've also implemented and seen some of
these ideas work for yourself so by now

00:00:22.099 --> 00:00:22.109
these ideas work for yourself so by now
 

00:00:22.109 --> 00:00:23.779
these ideas work for yourself so by now
you've actually seen most of the ideas

00:00:23.779 --> 00:00:23.789
you've actually seen most of the ideas
 

00:00:23.789 --> 00:00:25.820
you've actually seen most of the ideas
you need to implement a deep neural

00:00:25.820 --> 00:00:25.830
you need to implement a deep neural
 

00:00:25.830 --> 00:00:27.470
you need to implement a deep neural
network what we're going to do in this

00:00:27.470 --> 00:00:27.480
network what we're going to do in this
 

00:00:27.480 --> 00:00:29.269
network what we're going to do in this
week is take those ideas and put them

00:00:29.269 --> 00:00:29.279
week is take those ideas and put them
 

00:00:29.279 --> 00:00:30.890
week is take those ideas and put them
together so that you'll be able to

00:00:30.890 --> 00:00:30.900
together so that you'll be able to
 

00:00:30.900 --> 00:00:33.100
together so that you'll be able to
implement your own deep neural network

00:00:33.100 --> 00:00:33.110
implement your own deep neural network
 

00:00:33.110 --> 00:00:36.950
implement your own deep neural network
because the following exercise is longer

00:00:36.950 --> 00:00:36.960
because the following exercise is longer
 

00:00:36.960 --> 00:00:38.630
because the following exercise is longer
and just has a bit more work going to

00:00:38.630 --> 00:00:38.640
and just has a bit more work going to
 

00:00:38.640 --> 00:00:40.549
and just has a bit more work going to
keep the video so this week short as you

00:00:40.549 --> 00:00:40.559
keep the video so this week short as you
 

00:00:40.559 --> 00:00:42.619
keep the video so this week short as you
get through the videos a little bit more

00:00:42.619 --> 00:00:42.629
get through the videos a little bit more
 

00:00:42.629 --> 00:00:45.069
get through the videos a little bit more
quickly and then have more time to do a

00:00:45.069 --> 00:00:45.079
quickly and then have more time to do a
 

00:00:45.079 --> 00:00:47.060
quickly and then have more time to do a
significant programming exercise at the

00:00:47.060 --> 00:00:47.070
significant programming exercise at the
 

00:00:47.070 --> 00:00:49.670
significant programming exercise at the
end which I hope will leave you having

00:00:49.670 --> 00:00:49.680
end which I hope will leave you having
 

00:00:49.680 --> 00:00:51.170
end which I hope will leave you having
built a deep neural network that you

00:00:51.170 --> 00:00:51.180
built a deep neural network that you
 

00:00:51.180 --> 00:00:54.619
built a deep neural network that you
feel proud of so what is a deep neural

00:00:54.619 --> 00:00:54.629
feel proud of so what is a deep neural
 

00:00:54.629 --> 00:00:56.869
feel proud of so what is a deep neural
network you've seen this picture for a

00:00:56.869 --> 00:00:56.879
network you've seen this picture for a
 

00:00:56.879 --> 00:01:00.110
network you've seen this picture for a
literacy regression and you've also seen

00:01:00.110 --> 00:01:00.120
literacy regression and you've also seen
 

00:01:00.120 --> 00:01:02.389
literacy regression and you've also seen
new networks sort of a single hidden

00:01:02.389 --> 00:01:02.399
new networks sort of a single hidden
 

00:01:02.399 --> 00:01:05.750
new networks sort of a single hidden
layer so here is an example of a neural

00:01:05.750 --> 00:01:05.760
layer so here is an example of a neural
 

00:01:05.760 --> 00:01:08.000
layer so here is an example of a neural
network with two hidden layers and in

00:01:08.000 --> 00:01:08.010
network with two hidden layers and in
 

00:01:08.010 --> 00:01:10.880
network with two hidden layers and in
your network with five hidden layers we

00:01:10.880 --> 00:01:10.890
your network with five hidden layers we
 

00:01:10.890 --> 00:01:12.740
your network with five hidden layers we
should say that logistic regression is a

00:01:12.740 --> 00:01:12.750
should say that logistic regression is a
 

00:01:12.750 --> 00:01:17.929
should say that logistic regression is a
very shallow model whereas this model

00:01:17.929 --> 00:01:17.939
very shallow model whereas this model
 

00:01:17.939 --> 00:01:21.320
very shallow model whereas this model
here is a much deeper model and shallow

00:01:21.320 --> 00:01:21.330
here is a much deeper model and shallow
 

00:01:21.330 --> 00:01:23.240
here is a much deeper model and shallow
versus depth is a matter of degree

00:01:23.240 --> 00:01:23.250
versus depth is a matter of degree
 

00:01:23.250 --> 00:01:25.190
versus depth is a matter of degree
so neural network of a single hidden

00:01:25.190 --> 00:01:25.200
so neural network of a single hidden
 

00:01:25.200 --> 00:01:28.789
so neural network of a single hidden
layer this would be a two layer neural

00:01:28.789 --> 00:01:28.799
layer this would be a two layer neural
 

00:01:28.799 --> 00:01:31.730
layer this would be a two layer neural
network remember when we count layers in

00:01:31.730 --> 00:01:31.740
network remember when we count layers in
 

00:01:31.740 --> 00:01:33.469
network remember when we count layers in
neural network we don't count the input

00:01:33.469 --> 00:01:33.479
neural network we don't count the input
 

00:01:33.479 --> 00:01:36.170
neural network we don't count the input
layer we just count the hidden layers as

00:01:36.170 --> 00:01:36.180
layer we just count the hidden layers as
 

00:01:36.180 --> 00:01:39.200
layer we just count the hidden layers as
was the output layer so this would be a

00:01:39.200 --> 00:01:39.210
was the output layer so this would be a
 

00:01:39.210 --> 00:01:42.770
was the output layer so this would be a
two layer neural network is so quite

00:01:42.770 --> 00:01:42.780
two layer neural network is so quite
 

00:01:42.780 --> 00:01:44.990
two layer neural network is so quite
shallow but not as shallow as logistic

00:01:44.990 --> 00:01:45.000
shallow but not as shallow as logistic
 

00:01:45.000 --> 00:01:47.060
shallow but not as shallow as logistic
regression technically logistic

00:01:47.060 --> 00:01:47.070
regression technically logistic
 

00:01:47.070 --> 00:01:49.370
regression technically logistic
regression is a you know one layer

00:01:49.370 --> 00:01:49.380
regression is a you know one layer
 

00:01:49.380 --> 00:01:51.649
regression is a you know one layer
neural network but over the last several

00:01:51.649 --> 00:01:51.659
neural network but over the last several
 

00:01:51.659 --> 00:01:52.100
neural network but over the last several
years

00:01:52.100 --> 00:01:52.110
years
 

00:01:52.110 --> 00:01:54.740
years
dai on the machine learning community

00:01:54.740 --> 00:01:54.750
dai on the machine learning community
 

00:01:54.750 --> 00:01:56.899
dai on the machine learning community
has realized that there are functions

00:01:56.899 --> 00:01:56.909
has realized that there are functions
 

00:01:56.909 --> 00:01:59.149
has realized that there are functions
that very deep neural networks can learn

00:01:59.149 --> 00:01:59.159
that very deep neural networks can learn
 

00:01:59.159 --> 00:02:02.450
that very deep neural networks can learn
that shallower models are often unable

00:02:02.450 --> 00:02:02.460
that shallower models are often unable
 

00:02:02.460 --> 00:02:05.300
that shallower models are often unable
to although for any given problem it

00:02:05.300 --> 00:02:05.310
to although for any given problem it
 

00:02:05.310 --> 00:02:06.950
to although for any given problem it
might be hard to predict in advance

00:02:06.950 --> 00:02:06.960
might be hard to predict in advance
 

00:02:06.960 --> 00:02:09.529
might be hard to predict in advance
exactly how deep a neural network you

00:02:09.529 --> 00:02:09.539
exactly how deep a neural network you
 

00:02:09.539 --> 00:02:11.150
exactly how deep a neural network you
will want so it would be reasonable to

00:02:11.150 --> 00:02:11.160
will want so it would be reasonable to
 

00:02:11.160 --> 00:02:13.320
will want so it would be reasonable to
try logistic regression

00:02:13.320 --> 00:02:13.330
try logistic regression
 

00:02:13.330 --> 00:02:16.080
try logistic regression
one and two hidden layers and view the

00:02:16.080 --> 00:02:16.090
one and two hidden layers and view the
 

00:02:16.090 --> 00:02:17.790
one and two hidden layers and view the
number of hidden layers is another hyper

00:02:17.790 --> 00:02:17.800
number of hidden layers is another hyper
 

00:02:17.800 --> 00:02:20.340
number of hidden layers is another hyper
parameter that you could try a variety

00:02:20.340 --> 00:02:20.350
parameter that you could try a variety
 

00:02:20.350 --> 00:02:24.060
parameter that you could try a variety
of values of and evaluate on holdout

00:02:24.060 --> 00:02:24.070
of values of and evaluate on holdout
 

00:02:24.070 --> 00:02:25.890
of values of and evaluate on holdout
cross validation data or all your

00:02:25.890 --> 00:02:25.900
cross validation data or all your
 

00:02:25.900 --> 00:02:28.050
cross validation data or all your
development set say more about that

00:02:28.050 --> 00:02:28.060
development set say more about that
 

00:02:28.060 --> 00:02:30.330
development set say more about that
later as well let's now go through the

00:02:30.330 --> 00:02:30.340
later as well let's now go through the
 

00:02:30.340 --> 00:02:32.370
later as well let's now go through the
notation we're used to describe deep

00:02:32.370 --> 00:02:32.380
notation we're used to describe deep
 

00:02:32.380 --> 00:02:36.830
notation we're used to describe deep
neural networks here is a one two three

00:02:36.830 --> 00:02:36.840
neural networks here is a one two three
 

00:02:36.840 --> 00:02:42.000
neural networks here is a one two three
four layer neural network with three

00:02:42.000 --> 00:02:42.010
four layer neural network with three
 

00:02:42.010 --> 00:02:45.000
four layer neural network with three
thin layers and the number of units in

00:02:45.000 --> 00:02:45.010
thin layers and the number of units in
 

00:02:45.010 --> 00:02:46.980
thin layers and the number of units in
these hidden layers are I guess five

00:02:46.980 --> 00:02:46.990
these hidden layers are I guess five
 

00:02:46.990 --> 00:02:49.680
these hidden layers are I guess five
five three almond and it's one output

00:02:49.680 --> 00:02:49.690
five three almond and it's one output
 

00:02:49.690 --> 00:02:52.140
five three almond and it's one output
unit so the notation we're going to use

00:02:52.140 --> 00:02:52.150
unit so the notation we're going to use
 

00:02:52.150 --> 00:02:55.260
unit so the notation we're going to use
is going to use capital L to denote the

00:02:55.260 --> 00:02:55.270
is going to use capital L to denote the
 

00:02:55.270 --> 00:02:56.820
is going to use capital L to denote the
number of layers in the network so in

00:02:56.820 --> 00:02:56.830
number of layers in the network so in
 

00:02:56.830 --> 00:02:59.940
number of layers in the network so in
this case L is equal to four and so

00:02:59.940 --> 00:02:59.950
this case L is equal to four and so
 

00:02:59.950 --> 00:03:04.260
this case L is equal to four and so
that's the number of layers and we're

00:03:04.260 --> 00:03:04.270
that's the number of layers and we're
 

00:03:04.270 --> 00:03:08.540
that's the number of layers and we're
going to use n superscript L to denote

00:03:08.540 --> 00:03:08.550
going to use n superscript L to denote
 

00:03:08.550 --> 00:03:11.190
going to use n superscript L to denote
the number of notes or the number of

00:03:11.190 --> 00:03:11.200
the number of notes or the number of
 

00:03:11.200 --> 00:03:17.729
the number of notes or the number of
units in they are lowercase L so if we

00:03:17.729 --> 00:03:17.739
units in they are lowercase L so if we
 

00:03:17.739 --> 00:03:22.800
units in they are lowercase L so if we
index this the input as layer 0 this is

00:03:22.800 --> 00:03:22.810
index this the input as layer 0 this is
 

00:03:22.810 --> 00:03:26.280
index this the input as layer 0 this is
layer 1 this is layer 2 this is layer 3

00:03:26.280 --> 00:03:26.290
layer 1 this is layer 2 this is layer 3
 

00:03:26.290 --> 00:03:30.240
layer 1 this is layer 2 this is layer 3
and this is layer 4 then we have that

00:03:30.240 --> 00:03:30.250
and this is layer 4 then we have that
 

00:03:30.250 --> 00:03:34.830
and this is layer 4 then we have that
for example n 1 that would be this the

00:03:34.830 --> 00:03:34.840
for example n 1 that would be this the
 

00:03:34.840 --> 00:03:36.900
for example n 1 that would be this the
first isn't layer would be equal to 5

00:03:36.900 --> 00:03:36.910
first isn't layer would be equal to 5
 

00:03:36.910 --> 00:03:40.229
first isn't layer would be equal to 5
because we have 500 units there for this

00:03:40.229 --> 00:03:40.239
because we have 500 units there for this
 

00:03:40.239 --> 00:03:42.930
because we have 500 units there for this
one without that n 2 the number of units

00:03:42.930 --> 00:03:42.940
one without that n 2 the number of units
 

00:03:42.940 --> 00:03:45.210
one without that n 2 the number of units
in the second sitting there is also

00:03:45.210 --> 00:03:45.220
in the second sitting there is also
 

00:03:45.220 --> 00:03:52.790
in the second sitting there is also
equal to 5 n 3 is equal to 3 and n 4

00:03:52.790 --> 00:03:52.800
equal to 5 n 3 is equal to 3 and n 4
 

00:03:52.800 --> 00:03:56.640
equal to 5 n 3 is equal to 3 and n 4
which is n capital L this number of

00:03:56.640 --> 00:03:56.650
which is n capital L this number of
 

00:03:56.650 --> 00:03:59.250
which is n capital L this number of
units is this number of output units is

00:03:59.250 --> 00:03:59.260
units is this number of output units is
 

00:03:59.260 --> 00:04:01.740
units is this number of output units is
equal to 1 because here our capital L is

00:04:01.740 --> 00:04:01.750
equal to 1 because here our capital L is
 

00:04:01.750 --> 00:04:04.979
equal to 1 because here our capital L is
equal to 4 and we're also going to have

00:04:04.979 --> 00:04:04.989
equal to 4 and we're also going to have
 

00:04:04.989 --> 00:04:08.940
equal to 4 and we're also going to have
here therefore the input layer n 0 is

00:04:08.940 --> 00:04:08.950
here therefore the input layer n 0 is
 

00:04:08.950 --> 00:04:12.840
here therefore the input layer n 0 is
just equal to n X is equal to 3 okay so

00:04:12.840 --> 00:04:12.850
just equal to n X is equal to 3 okay so
 

00:04:12.850 --> 00:04:14.970
just equal to n X is equal to 3 okay so
that's the notation we use to describe

00:04:14.970 --> 00:04:14.980
that's the notation we use to describe
 

00:04:14.980 --> 00:04:17.099
that's the notation we use to describe
the number of nodes we have in different

00:04:17.099 --> 00:04:17.109
the number of nodes we have in different
 

00:04:17.109 --> 00:04:20.670
the number of nodes we have in different
layers so each layer L also also going

00:04:20.670 --> 00:04:20.680
layers so each layer L also also going
 

00:04:20.680 --> 00:04:24.980
layers so each layer L also also going
to use a L to denote D

00:04:24.980 --> 00:04:24.990
to use a L to denote D
 

00:04:24.990 --> 00:04:30.830
to use a L to denote D
observations in there l so we'll see

00:04:30.830 --> 00:04:30.840
observations in there l so we'll see
 

00:04:30.840 --> 00:04:33.499
observations in there l so we'll see
later that in for propagation you end up

00:04:33.499 --> 00:04:33.509
later that in for propagation you end up
 

00:04:33.509 --> 00:04:38.450
later that in for propagation you end up
computing al as the activation G applied

00:04:38.450 --> 00:04:38.460
computing al as the activation G applied
 

00:04:38.460 --> 00:04:42.200
computing al as the activation G applied
to ZL and then perhaps the activations

00:04:42.200 --> 00:04:42.210
to ZL and then perhaps the activations
 

00:04:42.210 --> 00:04:46.580
to ZL and then perhaps the activations
index by the layer l as well and then

00:04:46.580 --> 00:04:46.590
index by the layer l as well and then
 

00:04:46.590 --> 00:04:50.659
index by the layer l as well and then
we'll use WL to denote you know the

00:04:50.659 --> 00:04:50.669
we'll use WL to denote you know the
 

00:04:50.669 --> 00:04:55.249
we'll use WL to denote you know the
weights for computing the values VL in

00:04:55.249 --> 00:04:55.259
weights for computing the values VL in
 

00:04:55.259 --> 00:04:59.180
weights for computing the values VL in
the ARL and similarly VL that's used to

00:04:59.180 --> 00:04:59.190
the ARL and similarly VL that's used to
 

00:04:59.190 --> 00:05:02.210
the ARL and similarly VL that's used to
compute ZL finally just to wrap up on

00:05:02.210 --> 00:05:02.220
compute ZL finally just to wrap up on
 

00:05:02.220 --> 00:05:04.370
compute ZL finally just to wrap up on
the notation the input features are

00:05:04.370 --> 00:05:04.380
the notation the input features are
 

00:05:04.380 --> 00:05:07.730
the notation the input features are
called X but X is also the activations

00:05:07.730 --> 00:05:07.740
called X but X is also the activations
 

00:05:07.740 --> 00:05:12.170
called X but X is also the activations
of layer 0 so a 0 is equal to X and the

00:05:12.170 --> 00:05:12.180
of layer 0 so a 0 is equal to X and the
 

00:05:12.180 --> 00:05:15.290
of layer 0 so a 0 is equal to X and the
activation of the final layer a capital

00:05:15.290 --> 00:05:15.300
activation of the final layer a capital
 

00:05:15.300 --> 00:05:18.230
activation of the final layer a capital
L is equal to Y hat so a superscript

00:05:18.230 --> 00:05:18.240
L is equal to Y hat so a superscript
 

00:05:18.240 --> 00:05:21.499
L is equal to Y hat so a superscript
square bracket capital L is equal to the

00:05:21.499 --> 00:05:21.509
square bracket capital L is equal to the
 

00:05:21.509 --> 00:05:24.020
square bracket capital L is equal to the
predicted output to prediction y hats of

00:05:24.020 --> 00:05:24.030
predicted output to prediction y hats of
 

00:05:24.030 --> 00:05:26.149
predicted output to prediction y hats of
the neural network so you now know what

00:05:26.149 --> 00:05:26.159
the neural network so you now know what
 

00:05:26.159 --> 00:05:28.309
the neural network so you now know what
a deep neural network looks like as well

00:05:28.309 --> 00:05:28.319
a deep neural network looks like as well
 

00:05:28.319 --> 00:05:30.140
a deep neural network looks like as well
as the notation will use to this drive

00:05:30.140 --> 00:05:30.150
as the notation will use to this drive
 

00:05:30.150 --> 00:05:32.209
as the notation will use to this drive
and to compute with teeth networks I

00:05:32.209 --> 00:05:32.219
and to compute with teeth networks I
 

00:05:32.219 --> 00:05:34.189
and to compute with teeth networks I
never introduced a lot of notation in

00:05:34.189 --> 00:05:34.199
never introduced a lot of notation in
 

00:05:34.199 --> 00:05:36.230
never introduced a lot of notation in
this video but if you ever forget what

00:05:36.230 --> 00:05:36.240
this video but if you ever forget what
 

00:05:36.240 --> 00:05:38.629
this video but if you ever forget what
some symbol means we've also posted on

00:05:38.629 --> 00:05:38.639
some symbol means we've also posted on
 

00:05:38.639 --> 00:05:41.089
some symbol means we've also posted on
the course website a notation sheet or a

00:05:41.089 --> 00:05:41.099
the course website a notation sheet or a
 

00:05:41.099 --> 00:05:43.070
the course website a notation sheet or a
notation guide that you can use to look

00:05:43.070 --> 00:05:43.080
notation guide that you can use to look
 

00:05:43.080 --> 00:05:44.450
notation guide that you can use to look
up what these different symbols means

00:05:44.450 --> 00:05:44.460
up what these different symbols means
 

00:05:44.460 --> 00:05:46.939
up what these different symbols means
mix elect to describe what forward

00:05:46.939 --> 00:05:46.949
mix elect to describe what forward
 

00:05:46.949 --> 00:05:49.070
mix elect to describe what forward
propagation in this type of network look

00:05:49.070 --> 00:05:49.080
propagation in this type of network look
 

00:05:49.080 --> 00:05:52.550
propagation in this type of network look
like let's go into the next video

