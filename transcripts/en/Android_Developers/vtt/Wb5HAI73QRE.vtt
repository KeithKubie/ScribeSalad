WEBVTT
Kind: captions
Language: en

00:00:05.210 --> 00:00:08.500
DAN GALPIN: Android Internals--
Writing Performant Native Code.

00:00:08.500 --> 00:00:11.140
If you haven't heard
enough about me already,

00:00:11.140 --> 00:00:15.130
I have spent 5 plus years
talking to developers just

00:00:15.130 --> 00:00:17.030
like you around
the world, and it

00:00:17.030 --> 00:00:19.120
is awesome to be here in Hurst.

00:00:19.120 --> 00:00:21.210
And I spent 15 years
as a software developer

00:00:21.210 --> 00:00:21.961
before doing that.

00:00:21.961 --> 00:00:23.543
So I have a little
bit of street cred.

00:00:23.543 --> 00:00:25.600
I actually started
developing Android right

00:00:25.600 --> 00:00:27.500
around Android 1.1.

00:00:27.500 --> 00:00:30.080
Seriously actually, I started
developing commercially then.

00:00:30.080 --> 00:00:32.430
I was working with
it ever since 1.0.

00:00:32.430 --> 00:00:34.320
I wear a lot of hats.

00:00:34.320 --> 00:00:37.140
This is like one of the
smallest hats that I wear.

00:00:37.140 --> 00:00:38.830
And I have no shame.

00:00:38.830 --> 00:00:41.320
And I think I'm kind of
funny sometimes, especially

00:00:41.320 --> 00:00:43.320
with lack of sleep like now.

00:00:43.320 --> 00:00:44.000
All right.

00:00:44.000 --> 00:00:48.160
Performant is an
entirely invented word.

00:00:48.160 --> 00:00:49.367
It is not a real word.

00:00:49.367 --> 00:00:51.700
How many of you knew that
performant wasn't a real word?

00:00:51.700 --> 00:00:52.415
OK.

00:00:52.415 --> 00:00:54.873
Good, I got a bunch of English
majors here, that's awesome.

00:00:54.873 --> 00:00:58.530
So actually according to
Urban Dictionary and most

00:00:58.530 --> 00:01:01.770
of the research that I did--
because I do extensive research

00:01:01.770 --> 00:01:04.280
on a talk like this--
performant was actually

00:01:04.280 --> 00:01:06.860
invented by software developers.

00:01:06.860 --> 00:01:07.870
OK?

00:01:07.870 --> 00:01:10.530
And there's some
theories behind this.

00:01:10.530 --> 00:01:12.870
But the Urban
Dictionary defines it

00:01:12.870 --> 00:01:16.160
is having adequate performance.

00:01:16.160 --> 00:01:20.003
And but really this would
not be nearly as cool

00:01:20.003 --> 00:01:24.000
of a talk-- native code
having adequate performance.

00:01:24.000 --> 00:01:25.811
So this is why the
word was created, OK?

00:01:25.811 --> 00:01:27.310
It just doesn't
have the ring to it.

00:01:27.310 --> 00:01:29.150
We don't like adequate
in our industry.

00:01:29.150 --> 00:01:32.080
We like awesome.

00:01:32.080 --> 00:01:35.510
So yeah, so rather than title it
that, I use the invented word.

00:01:35.510 --> 00:01:37.960
And we're going to be
talking about really, really

00:01:37.960 --> 00:01:39.480
tiny benchmarks.

00:01:39.480 --> 00:01:43.150
Because in order for your
app to actually perform well,

00:01:43.150 --> 00:01:45.750
you have to do everything
and make sure everything

00:01:45.750 --> 00:01:49.130
happens within
16.67 milliseconds.

00:01:49.130 --> 00:01:51.480
That is how you get
60 frames per second.

00:01:51.480 --> 00:01:53.133
But most of the
benchmarks that I'm

00:01:53.133 --> 00:01:55.300
going to be talking
about in this lecture

00:01:55.300 --> 00:01:56.700
are in nanoseconds.

00:01:56.700 --> 00:01:58.660
So we take this
into nanoseconds.

00:01:58.660 --> 00:02:00.580
That's a lot of nanoseconds.

00:02:00.580 --> 00:02:03.140
So this stuff is
really, really fast.

00:02:03.140 --> 00:02:06.150
So don't worry when I tell you
it takes five times as long

00:02:06.150 --> 00:02:08.180
to do something on one
version as the other.

00:02:08.180 --> 00:02:10.530
Because it's really fast still.

00:02:10.530 --> 00:02:13.100
But it's good for you to know.

00:02:13.100 --> 00:02:14.870
All right.

00:02:14.870 --> 00:02:17.370
So again, do not panic.

00:02:17.370 --> 00:02:19.184
Remember the clock
speed of the Nexus,

00:02:19.184 --> 00:02:21.100
which is what I used to
do most of my testing,

00:02:21.100 --> 00:02:25.120
because I wanted something that
could run KitKat as well as run

00:02:25.120 --> 00:02:30.120
L&amp;M, is about 2.3 gigahertz,
tops, with four cores.

00:02:30.120 --> 00:02:33.770
So that's like billions and
billions of instructions.

00:02:33.770 --> 00:02:36.290
I know, I'm like sounding
like Carl Sagan here.

00:02:36.290 --> 00:02:39.580
So we're talking in very
small micro benchmark terms.

00:02:39.580 --> 00:02:42.714
Now Android Internals is this
kind of thing I'm working up.

00:02:42.714 --> 00:02:44.380
I want to see what
you guys think of it.

00:02:44.380 --> 00:02:46.560
So afterwards we'll have a quiz.

00:02:46.560 --> 00:02:50.030
And it really is about the
unique voyages of discovery

00:02:50.030 --> 00:02:53.520
we can take in an open
source platform like Android.

00:02:53.520 --> 00:02:56.350
So the idea is not
just to understand

00:02:56.350 --> 00:02:59.054
how to code Android, but
understand how it works,

00:02:59.054 --> 00:03:00.470
so that when you
run into problems

00:03:00.470 --> 00:03:02.094
you have a better
idea of actually what

00:03:02.094 --> 00:03:02.929
you're dealing with.

00:03:02.929 --> 00:03:05.220
And we're going to do it kind
of this way-- we're gonna

00:03:05.220 --> 00:03:08.010
actually test our assumptions.

00:03:08.010 --> 00:03:09.270
We're going to benchmark.

00:03:09.270 --> 00:03:10.810
We're going to look
at source code.

00:03:10.810 --> 00:03:14.130
And we're going to
debug, even in native.

00:03:14.130 --> 00:03:17.070
So this voyage one
takes us into the land

00:03:17.070 --> 00:03:18.560
of optimizations in ART.

00:03:18.560 --> 00:03:21.467
If you say my talk yesterday,
I got a bit into that.

00:03:21.467 --> 00:03:23.675
But this time we're going
to be a bit more pragmatic,

00:03:23.675 --> 00:03:26.333
because we're really going to
talk about how the world of J

00:03:26.333 --> 00:03:29.780
and I has changed as we've
moved from a world of Dalvik

00:03:29.780 --> 00:03:30.806
to a world of art.

00:03:30.806 --> 00:03:32.180
But I am getting
ahead of myself.

00:03:32.180 --> 00:03:33.810
So let's talk about native code.

00:03:33.810 --> 00:03:36.510
Most of you guys have
actually done native code.

00:03:36.510 --> 00:03:39.200
But I'm talking about code
written using the NDK.

00:03:39.200 --> 00:03:42.780
And we're talking about
primarily C and C++ code that

00:03:42.780 --> 00:03:45.750
interfaces with the Android
runtime using J and I.

00:03:45.750 --> 00:03:48.830
Here is a really, really
abbreviated architecture

00:03:48.830 --> 00:03:51.150
diagram of what this looks like.

00:03:51.150 --> 00:03:54.505
Applications written with the
NDK take the form of these,

00:03:54.505 --> 00:03:59.540
you know, decks classes, that
execute on the Android Runtime.

00:03:59.540 --> 00:04:01.340
They interact with
system libraries

00:04:01.340 --> 00:04:03.780
by the SDK framework classes.

00:04:03.780 --> 00:04:06.180
And SDK application
code is written

00:04:06.180 --> 00:04:08.920
in a language like Java,
that the runtime can support.

00:04:08.920 --> 00:04:12.240
So the Linux kernel was
written primarily in C and C++,

00:04:12.240 --> 00:04:14.680
and so are the system libraries.

00:04:14.680 --> 00:04:16.490
The framework and
the Java runtime

00:04:16.490 --> 00:04:19.190
call into these libraries
using the Java Native Interface

00:04:19.190 --> 00:04:20.380
or JNI.

00:04:20.380 --> 00:04:22.720
Now the NDK
essentially allows you

00:04:22.720 --> 00:04:25.910
to write a dynamically
linked native library.

00:04:25.910 --> 00:04:29.220
But it can't run directly
against the system libraries,

00:04:29.220 --> 00:04:33.484
because these ABIs or
APIs aren't stable.

00:04:33.484 --> 00:04:35.150
So the purpose of the
NDK is to give you

00:04:35.150 --> 00:04:38.950
a stable application
binary interface to run

00:04:38.950 --> 00:04:42.130
your own compiled code
against, that provides access

00:04:42.130 --> 00:04:45.340
to only the most critical OS
features, so the platform can

00:04:45.340 --> 00:04:47.750
still continue to grow
and expand, and change

00:04:47.750 --> 00:04:49.980
how they implement
things, and be awesome.

00:04:49.980 --> 00:04:53.145
But your application
code is talking to this

00:04:53.145 --> 00:04:54.415
through this ABI.

00:04:54.415 --> 00:04:56.990
It's all important stuff.

00:04:56.990 --> 00:04:58.240
And that's what it looks like.

00:04:58.240 --> 00:05:00.810
Boom, your application code now
talks to your library, which

00:05:00.810 --> 00:05:02.690
is going straight to native.

00:05:02.690 --> 00:05:05.830
We talked to you a little bit
about the history of the NDK.

00:05:05.830 --> 00:05:10.490
The original first versions of
Android did not even have it.

00:05:10.490 --> 00:05:13.360
But we got it in Cupcake.

00:05:13.360 --> 00:05:16.350
And we've been slowly
expanding it ever since.

00:05:16.350 --> 00:05:20.450
So in the first versions
of it, you got a C runtime,

00:05:20.450 --> 00:05:25.100
really minimal C++ support,
Zlib compression, logging,

00:05:25.100 --> 00:05:29.210
networking, dynamic linking,
some math-- not a lot,

00:05:29.210 --> 00:05:30.700
but enough.

00:05:30.700 --> 00:05:31.760
We then added graphics.

00:05:31.760 --> 00:05:35.014
So the first version
couldn't even talk to OpenGL.

00:05:35.014 --> 00:05:36.180
But we added graphics there.

00:05:36.180 --> 00:05:37.790
And you know what took the
longest part about this slide

00:05:37.790 --> 00:05:39.915
was actually trying to find
all these images again.

00:05:39.915 --> 00:05:43.650
It's like, when have I used
a slide with these images?

00:05:43.650 --> 00:05:45.640
And then Gingerbread
really expanded things.

00:05:45.640 --> 00:05:47.210
Gingerbread got
much more serious

00:05:47.210 --> 00:05:50.050
about gaming and multimedia.

00:05:50.050 --> 00:05:52.000
So we added our native
application API.

00:05:52.000 --> 00:05:53.170
So you can actually build.

00:05:53.170 --> 00:05:54.110
That was the first
version of Android

00:05:54.110 --> 00:05:56.630
that you could actually
build a native application

00:05:56.630 --> 00:06:00.910
without needing to use
any Java whatsoever.

00:06:00.910 --> 00:06:03.100
And also sound,
which is really cool,

00:06:03.100 --> 00:06:05.260
like Open SL was
really nice to have.

00:06:05.260 --> 00:06:06.695
We continued to evolve it.

00:06:06.695 --> 00:06:12.619
In Ice Cream Sandwich, we added
the OpenMAX AL media layer.

00:06:12.619 --> 00:06:14.410
Not many people know
this, but you actually

00:06:14.410 --> 00:06:18.930
can access renderscript directly
from the NDK, as of KitKat.

00:06:18.930 --> 00:06:20.775
And it's pretty cool stuff.

00:06:20.775 --> 00:06:22.990
It was a long request.

00:06:22.990 --> 00:06:24.945
And we also did a bunch
of graphic stuff here.

00:06:24.945 --> 00:06:26.320
That's why these
aren't in order.

00:06:26.320 --> 00:06:29.120
But in Jelly Bean
MR2, we added ES 3.

00:06:29.120 --> 00:06:33.779
And in Lollipop, we added 3.1,
as well as 64-bit support.

00:06:33.779 --> 00:06:34.695
So that's pretty cool.

00:06:34.695 --> 00:06:36.850
So let's talk about
some assumptions.

00:06:36.850 --> 00:06:40.250
So our assumptions are basically
to follow the suggestions

00:06:40.250 --> 00:06:41.850
that the Perf-JNI article.

00:06:41.850 --> 00:06:43.350
If you have not
read this article,

00:06:43.350 --> 00:06:46.750
it is the gospel
for looking at how

00:06:46.750 --> 00:06:49.510
to deal with JNI on Android.

00:06:49.510 --> 00:06:51.650
But do they still
make sense today?

00:06:51.650 --> 00:06:54.330
We haven't updated the
article since we shipped ART.

00:06:54.330 --> 00:06:57.350
So here are the basic
things you have to do, OK?

00:06:57.350 --> 00:07:00.490
Absolutely critical when you're
trying to make JNI performant.

00:07:00.490 --> 00:07:03.730
One is you're going to
cache field and method IDs.

00:07:03.730 --> 00:07:05.480
You're going to do
it intelligently.

00:07:05.480 --> 00:07:11.690
Two, you're going to
GetStrings in a reasonable way.

00:07:11.690 --> 00:07:14.060
And you're going to
copy things in native.

00:07:14.060 --> 00:07:16.030
These are the only
three real tips we gave.

00:07:16.030 --> 00:07:18.080
But I'll go more into details.

00:07:18.080 --> 00:07:19.590
So how did we benchmark this?

00:07:19.590 --> 00:07:21.639
We actually used
something called Caliper.

00:07:21.639 --> 00:07:23.680
Now how many of you have
actually have ever heard

00:07:23.680 --> 00:07:25.620
of Caliper in this room?

00:07:25.620 --> 00:07:26.810
No one, that's good.

00:07:26.810 --> 00:07:28.380
Oh, one person, sorry.

00:07:28.380 --> 00:07:30.270
I had never heard of
it before doing this.

00:07:30.270 --> 00:07:32.200
But I was interested
in doing benchmarking.

00:07:32.200 --> 00:07:35.490
It turns out if you
actually look at AOSP,

00:07:35.490 --> 00:07:37.480
we have Caliper
tests checked in.

00:07:37.480 --> 00:07:40.927
This is actually how we
benchmark our VM ourselves.

00:07:40.927 --> 00:07:42.385
And we use this
thing called Vogar.

00:07:42.385 --> 00:07:44.830
And if you actually look at
what's checked into Vogar,

00:07:44.830 --> 00:07:46.780
it's a really ancient
version of Caliper.

00:07:46.780 --> 00:07:49.640
I'm hoping some day we
actually update that.

00:07:49.640 --> 00:07:51.750
It would have made my
life a little easier.

00:07:51.750 --> 00:07:53.520
But Caliper is a
really cool framework

00:07:53.520 --> 00:07:55.650
for running micro benchmarks.

00:07:55.650 --> 00:07:56.680
All right.

00:07:56.680 --> 00:07:59.280
So let's get to the first thing.

00:07:59.280 --> 00:08:02.020
This-- if you haven't
use NDK before--

00:08:02.020 --> 00:08:06.180
is how you access a
class from native code.

00:08:06.180 --> 00:08:08.760
So once you have the class,
and I'm passing this class

00:08:08.760 --> 00:08:15.030
in from Java-- you can see
jclass type-- that is actually

00:08:15.030 --> 00:08:16.690
the class information.

00:08:16.690 --> 00:08:18.980
And that's what we
call it GetFieldID,

00:08:18.980 --> 00:08:22.440
the name of the field, the type
of the field-- so in this case,

00:08:22.440 --> 00:08:23.200
integer.

00:08:23.200 --> 00:08:25.100
And then finally I
can call GetItField

00:08:25.100 --> 00:08:27.120
to actually pull the value.

00:08:27.120 --> 00:08:30.380
So that's how we actually
access an integer that's

00:08:30.380 --> 00:08:32.850
inside of a Java class
from native code.

00:08:32.850 --> 00:08:33.350
All right.

00:08:33.350 --> 00:08:36.520
So the first suggestion, which
is a really, really good one,

00:08:36.520 --> 00:08:38.960
is to cache field
and method IDs.

00:08:38.960 --> 00:08:40.130
And here's why.

00:08:40.130 --> 00:08:43.390
Those field and method
IDs are just numbers.

00:08:43.390 --> 00:08:47.100
They don't actually change
once the class has been loaded.

00:08:47.100 --> 00:08:49.600
And if you want to be really,
really good about when

00:08:49.600 --> 00:08:52.385
you actually grab them, inside
of the static initialize

00:08:52.385 --> 00:08:54.700
is a class in Java.

00:08:54.700 --> 00:08:57.980
You can actually call some
JNI code-- in this case,

00:08:57.980 --> 00:09:00.010
I'm calling ir nativeInIt.

00:09:00.010 --> 00:09:02.718
And inside of
nativeInIt-- it really

00:09:02.718 --> 00:09:04.426
shouldn't have been
named nativeInIt, now

00:09:04.426 --> 00:09:07.680
that I'm looking at this
slide, but that's OK.

00:09:07.680 --> 00:09:09.890
You could see I'm
getting that field ID.

00:09:09.890 --> 00:09:11.570
And that field ID
will be good as long

00:09:11.570 --> 00:09:12.570
as this class is loaded.

00:09:12.570 --> 00:09:13.669
So that's pretty awesome.

00:09:13.669 --> 00:09:14.960
I don't have to think about it.

00:09:14.960 --> 00:09:17.680
I'm just storing it in
a little variable there

00:09:17.680 --> 00:09:20.680
that's associated
with my native class.

00:09:20.680 --> 00:09:21.250
All right.

00:09:21.250 --> 00:09:22.870
Let's talk about performance.

00:09:22.870 --> 00:09:26.640
So here is how it
benchmarks on a Nexus 5

00:09:26.640 --> 00:09:29.440
running KitKat and marshmallow.

00:09:29.440 --> 00:09:31.680
And you'll notice something.

00:09:31.680 --> 00:09:33.580
ART takes longer.

00:09:33.580 --> 00:09:38.600
That's going to be in
general, a common theme.

00:09:38.600 --> 00:09:42.630
ART is more complicated
than Dalvik in general.

00:09:42.630 --> 00:09:45.060
And so it's even
more important today

00:09:45.060 --> 00:09:46.990
than it was initially
to cache these things.

00:09:46.990 --> 00:09:49.420
Because devices
are running faster.

00:09:49.420 --> 00:09:52.497
And ART is already
faster doing most things.

00:09:52.497 --> 00:09:54.580
So you'll even notice this
maybe a little bit more

00:09:54.580 --> 00:09:56.946
than even these
benchmarks should say.

00:09:56.946 --> 00:09:58.320
So let's look at
the code and try

00:09:58.320 --> 00:10:02.280
to figure out why this is
faster, or slower I should say.

00:10:02.280 --> 00:10:04.790
And the really key thing
is this thing here,

00:10:04.790 --> 00:10:08.120
this ScopedJNIThreadState
and ScopedObjectAccess.

00:10:08.120 --> 00:10:10.950
This is why JNI
actually does not

00:10:10.950 --> 00:10:12.500
run at lightning warp Speed.

00:10:12.500 --> 00:10:18.730
And that's because every
single thread in Android

00:10:18.730 --> 00:10:19.980
can be in one of two states.

00:10:19.980 --> 00:10:21.482
It could be in running state.

00:10:21.482 --> 00:10:23.690
Actually it can be in more
than that-- but two states

00:10:23.690 --> 00:10:24.481
that we care about.

00:10:24.481 --> 00:10:26.270
It can be in running state.

00:10:26.270 --> 00:10:29.462
That's actually when we're
in the Java virtual machine,

00:10:29.462 --> 00:10:30.920
and we're actually
executing stuff.

00:10:30.920 --> 00:10:33.570
And it has access to
all that great-- sorry,

00:10:33.570 --> 00:10:35.349
I should say the runtime.

00:10:35.349 --> 00:10:37.390
We do not have a Java
virtual machine in Android.

00:10:37.390 --> 00:10:39.580
You can strike that
from your memory.

00:10:39.580 --> 00:10:43.570
The Android runtime--
that's when it's in there.

00:10:43.570 --> 00:10:48.200
Or it can be in the non-running
state or native state.

00:10:48.200 --> 00:10:50.860
So when we're actually
accessing a variable

00:10:50.860 --> 00:10:53.250
like this, which is an
int field, all of these

00:10:53.250 --> 00:10:56.940
are variables that are
inside of the runtime.

00:10:56.940 --> 00:10:59.110
We actually have to switch
the state of our thread

00:10:59.110 --> 00:11:00.110
in order to do that.

00:11:00.110 --> 00:11:03.420
And that means we're doing a
whole bunch of synchronization.

00:11:03.420 --> 00:11:05.500
And that synchronization
is expensive.

00:11:05.500 --> 00:11:10.490
It's expensive on the order
of about 300 nanoseconds.

00:11:10.490 --> 00:11:13.070
Now to give you some context,
because 300 nanoseconds

00:11:13.070 --> 00:11:14.490
is a really small number.

00:11:14.490 --> 00:11:18.850
In the average function call,
an average function call in ART

00:11:18.850 --> 00:11:20.830
is about five nanoseconds.

00:11:20.830 --> 00:11:22.250
In Dolvic, it's more like 10.

00:11:24.507 --> 00:11:26.340
So once again, we're
talking about something

00:11:26.340 --> 00:11:28.850
that is a really tiny number.

00:11:28.850 --> 00:11:32.970
But it's still like 60 times
longer than a standard function

00:11:32.970 --> 00:11:33.940
call.

00:11:33.940 --> 00:11:35.564
So it's still something
to think about.

00:11:35.564 --> 00:11:37.220
So let's look at
our first work hard.

00:11:37.220 --> 00:11:39.850
And yes, based upon our
benchmark caching field

00:11:39.850 --> 00:11:41.780
and method IDs is great
for Dolvic and ART.

00:11:41.780 --> 00:11:43.920
It's even better in ART.

00:11:43.920 --> 00:11:45.740
All right, so let's
look at the suggestion

00:11:45.740 --> 00:11:51.200
two of this, which was
use GetStringChars.

00:11:51.200 --> 00:11:54.140
Now this was kind
of interesting.

00:11:54.140 --> 00:11:59.370
So basically as you
probably all know,

00:11:59.370 --> 00:12:03.420
the standard for Java--
which ART also follows--

00:12:03.420 --> 00:12:08.370
is to treat all strings
as double byte character

00:12:08.370 --> 00:12:11.620
strings, UCS2.

00:12:11.620 --> 00:12:14.330
And this is important
because we're

00:12:14.330 --> 00:12:16.430
in a world that's
highly international,

00:12:16.430 --> 00:12:19.920
single byte strings are kind
of passe, et cetera, et cetera.

00:12:19.920 --> 00:12:24.120
Not to mention as it
turns out, the VM actually

00:12:24.120 --> 00:12:26.152
doesn't particularly
have great instructions

00:12:26.152 --> 00:12:27.110
for dealing with bytes.

00:12:27.110 --> 00:12:35.810
So it's actually kind of nice to
have these things in these two

00:12:35.810 --> 00:12:37.640
byte characters, thank you.

00:12:37.640 --> 00:12:41.220
So the suggestion here is that
rather than getting string UTF

00:12:41.220 --> 00:12:43.890
characters, like we have
there at the bottom,

00:12:43.890 --> 00:12:46.390
we actually call it
GetStringChars, which actually

00:12:46.390 --> 00:12:49.710
takes our string it gets
us the closest to being

00:12:49.710 --> 00:12:52.650
a native representation of
it that you could imagine.

00:12:52.650 --> 00:12:55.370
And we would expect this
to always outperform

00:12:55.370 --> 00:12:57.450
the UTF equivalent,
where it actually

00:12:57.450 --> 00:12:59.890
has to do a copy of memory.

00:12:59.890 --> 00:13:01.410
All right, so
let's look at this.

00:13:01.410 --> 00:13:03.750
I took a 15 character string.

00:13:03.750 --> 00:13:06.450
I ran some benchmarks on this.

00:13:06.450 --> 00:13:11.350
And I was actually really
astonished to see two things.

00:13:11.350 --> 00:13:13.260
One, as we expected,
ART is actually slow.

00:13:13.260 --> 00:13:15.050
But it's actually much slower.

00:13:15.050 --> 00:13:18.280
And two, this was a real shock.

00:13:18.280 --> 00:13:20.840
if you look at those
two blue lines,

00:13:20.840 --> 00:13:25.280
on a 15 character string,
GetStringUTFChars actually

00:13:25.280 --> 00:13:28.680
performs faster
than GetStringChars.

00:13:28.680 --> 00:13:30.710
How can that possibly happen?

00:13:30.710 --> 00:13:32.920
Because we already said
GetStringChars doesn't

00:13:32.920 --> 00:13:34.660
have to copy the
string, it doesn't

00:13:34.660 --> 00:13:37.120
have to translate the
string between UTFA.

00:13:37.120 --> 00:13:39.010
So something is happening
to actually make

00:13:39.010 --> 00:13:41.240
on this very short string.

00:13:41.240 --> 00:13:45.640
It's actually faster to do all
that copying and translation.

00:13:45.640 --> 00:13:48.930
So let's try a longer string,
just to see if I'm crazy here.

00:13:48.930 --> 00:13:51.300
So this is a string that's
100 characters long.

00:13:51.300 --> 00:13:52.966
And we see more of
what we would expect.

00:13:52.966 --> 00:13:58.980
So GetStringUTFChars is now
slower than GetStringChars.

00:13:58.980 --> 00:14:01.800
So the question is,
why was GetStringUTF

00:14:01.800 --> 00:14:04.420
ever faster under ART?

00:14:04.420 --> 00:14:07.830
So let's look at
some source code.

00:14:07.830 --> 00:14:10.570
So you can see here that
GetStringUTFChars always

00:14:10.570 --> 00:14:11.630
has to copy.

00:14:11.630 --> 00:14:14.210
So what it does,
it goes through,

00:14:14.210 --> 00:14:17.260
and it actualy just goes
to that copy operation.

00:14:17.260 --> 00:14:19.940
Well, GetStringChars actually
has to go and check, huh?

00:14:19.940 --> 00:14:21.830
Well, can I actually
avoid this copy?

00:14:21.830 --> 00:14:24.070
So it actually goes,
looks at the key,

00:14:24.070 --> 00:14:27.975
and checks to see if
that's a movable object.

00:14:27.975 --> 00:14:32.290
And it turns out that's actually
somewhat of an expensive call.

00:14:32.290 --> 00:14:35.690
So you can see here, there
is this fine continuous space

00:14:35.690 --> 00:14:37.380
from object.

00:14:37.380 --> 00:14:42.090
That just sounds dangerous and
what is this actually doing

00:14:42.090 --> 00:14:43.860
in real life?

00:14:43.860 --> 00:14:46.490
Well, it actually
calls this, which

00:14:46.490 --> 00:14:47.980
has a for loop into
it, which looks

00:14:47.980 --> 00:14:49.380
to find continuous spaces.

00:14:49.380 --> 00:14:51.610
So you can see already
here, even though the VN

00:14:51.610 --> 00:14:55.670
is doing all of this work to
try to avoid this little tiny 15

00:14:55.670 --> 00:15:00.210
character, 30 bite mem
copy, it's actually

00:15:00.210 --> 00:15:03.860
failing to run this
particular case optimally.

00:15:03.860 --> 00:15:07.010
And so some point in between
100 characters in 15 characters

00:15:07.010 --> 00:15:08.752
happens to be the
break even point.

00:15:08.752 --> 00:15:10.210
What does this
really, really mean?

00:15:10.210 --> 00:15:12.930
Is it unless you're passing
very, very large strings

00:15:12.930 --> 00:15:17.700
around, do what's ever the most
convenient to you, honestly.

00:15:17.700 --> 00:15:19.390
It's not a big deal.

00:15:19.390 --> 00:15:22.210
You have to do a whole bunch
of crazy stuff in native code

00:15:22.210 --> 00:15:27.620
to actually make your code
handle two byte strings-- two

00:15:27.620 --> 00:15:30.530
byte characters, it might
or may not be worth it.

00:15:30.530 --> 00:15:33.100
You'll probably want to look
at actually profiling it.

00:15:33.100 --> 00:15:34.185
So here's our scorecard.

00:15:34.185 --> 00:15:37.800
So in general, yes,
GetStringUTFChars

00:15:37.800 --> 00:15:43.020
is going to be faster for large
characters, but not always.

00:15:43.020 --> 00:15:46.560
I'll give it 3/4
of a star for ART.

00:15:46.560 --> 00:15:49.070
Here's another suggestion
that came out of there,

00:15:49.070 --> 00:15:52.370
which is use GetStringRegion.

00:15:52.370 --> 00:15:54.340
This is kind of interesting.

00:15:54.340 --> 00:15:58.410
So here is what that looks like.

00:15:58.410 --> 00:16:03.020
So normally if you want to copy
a string into a native buffer--

00:16:03.020 --> 00:16:05.880
in this case in my native, I
mean just literally a buffer

00:16:05.880 --> 00:16:07.010
of characters.

00:16:07.010 --> 00:16:10.200
You're going to
call GetStringChars,

00:16:10.200 --> 00:16:15.410
and then you'll mem copy it,
and then et cetera, et cetera.

00:16:15.410 --> 00:16:18.400
And you'll see I'm also doing
some memory deallocation here,

00:16:18.400 --> 00:16:19.970
just to be fair on both sides.

00:16:19.970 --> 00:16:23.180
You can see it's actually
several lines of code

00:16:23.180 --> 00:16:25.047
and several more accesses.

00:16:25.047 --> 00:16:26.880
Because every time you
actually do something

00:16:26.880 --> 00:16:30.680
like GetStringChars
or GetStringRegion,

00:16:30.680 --> 00:16:33.440
you're actually talking
to the VM as well.

00:16:33.440 --> 00:16:35.630
Well, this is
actually kind of cool.

00:16:35.630 --> 00:16:38.442
Sorry, you're talking to native
code as well and to the VM.

00:16:38.442 --> 00:16:39.650
So this is kind of cool here.

00:16:39.650 --> 00:16:41.399
And you could actually
use GetStringRegion

00:16:41.399 --> 00:16:43.260
and GetStringRegion
does the copy for you.

00:16:46.040 --> 00:16:49.000
That's kind of nice.

00:16:49.000 --> 00:16:50.700
Also one thing I'm
doing here, which

00:16:50.700 --> 00:16:52.800
is a nice little
optimization is I'm actually

00:16:52.800 --> 00:16:55.710
passing the length of
the string into this.

00:16:55.710 --> 00:16:56.840
And that's kind of cool.

00:16:56.840 --> 00:17:00.380
Because as it turns out, passing
extra parameters into JNI

00:17:00.380 --> 00:17:02.100
is almost free.

00:17:02.100 --> 00:17:05.050
It takes literally on the order
of a couple of nanoseconds

00:17:05.050 --> 00:17:08.400
for every single additional
parameter you want to use.

00:17:08.400 --> 00:17:10.000
So that's awesome.

00:17:10.000 --> 00:17:13.069
And if I were going to
actually query the string,

00:17:13.069 --> 00:17:15.319
and say give me the
size of the string,

00:17:15.319 --> 00:17:18.930
that would be another
300 nanosecond round trip

00:17:18.930 --> 00:17:19.910
through the machine.

00:17:19.910 --> 00:17:22.609
So adding additional
parameters is a great way

00:17:22.609 --> 00:17:24.170
of optimizing your JNI.

00:17:24.170 --> 00:17:25.569
So I thought I'd point this out.

00:17:25.569 --> 00:17:27.870
This is sort of a little
minor optimization here.

00:17:27.870 --> 00:17:29.870
But these things are what
you're thinking about.

00:17:29.870 --> 00:17:32.490
Again you're trying to avoid
round trips on both sides.

00:17:32.490 --> 00:17:35.460
You're trying to avoid
extra calls into the VM

00:17:35.460 --> 00:17:39.479
or into the runtimes, I
should say, from native code.

00:17:39.479 --> 00:17:41.270
And you're also trying
to do the other way.

00:17:41.270 --> 00:17:44.740
You're trying to avoid
extra calls into native code

00:17:44.740 --> 00:17:46.300
from the run time.

00:17:46.300 --> 00:17:46.800
All right.

00:17:46.800 --> 00:17:49.008
So what does this really
look like after all of this?

00:17:49.008 --> 00:17:51.600
Well, it's kind of
as you'd expect.

00:17:51.600 --> 00:17:55.230
GetStringRegion is way faster.

00:17:55.230 --> 00:17:56.980
You're avoiding doing
an extra allocation.

00:18:01.020 --> 00:18:03.290
And so that's going to,
in general, be good.

00:18:08.676 --> 00:18:10.300
And you can also see
in ART is actually

00:18:10.300 --> 00:18:12.310
a lot slower than in Dalvik.

00:18:12.310 --> 00:18:13.700
And a lot is all relative.

00:18:13.700 --> 00:18:15.420
Again these are all
little tiny things.

00:18:15.420 --> 00:18:18.690
You might think after this
talk that ART isn't very fast.

00:18:18.690 --> 00:18:21.540
And I don't want to give that
impression to you at all.

00:18:21.540 --> 00:18:26.910
In fact, ART is scary fast at
doing almost anything but this.

00:18:26.910 --> 00:18:30.420
So in almost any other way it
is going to blow away Dalvik.

00:18:30.420 --> 00:18:33.780
So do not take this any kind
of indictment against ART.

00:18:33.780 --> 00:18:36.820
In fact, you could
also-- when I was

00:18:36.820 --> 00:18:39.570
asking one of the
internal guys about why

00:18:39.570 --> 00:18:41.950
this case, ART
actually was written

00:18:41.950 --> 00:18:45.140
in a time when we had multiple
processor cores in a system.

00:18:45.140 --> 00:18:47.870
So when they started
designing it and writing it,

00:18:47.870 --> 00:18:52.170
they were thinking the entire
time about deadlock problems.

00:18:52.170 --> 00:18:55.290
And I would say that ART
takes incredibly conservative

00:18:55.290 --> 00:18:56.940
approach to make
sure that you're not

00:18:56.940 --> 00:18:58.680
going to have deadlock.

00:18:58.680 --> 00:19:01.850
And if you look in the
list of bugs on AOSP,

00:19:01.850 --> 00:19:04.820
you will find deadlock bugs
in Dalvik-- most of which

00:19:04.820 --> 00:19:06.300
have been fixed.

00:19:06.300 --> 00:19:08.880
But I think part of
what you're seeing

00:19:08.880 --> 00:19:11.057
is the art team wanted this
to be incredibly robust.

00:19:11.057 --> 00:19:13.140
And that's why you're
seeing a little bit of this.

00:19:13.140 --> 00:19:14.987
So maybe in the
future, we can actually

00:19:14.987 --> 00:19:17.070
figure out how to make
these even closer together.

00:19:17.070 --> 00:19:19.350
But that's what it's like today.

00:19:19.350 --> 00:19:20.160
All right.

00:19:20.160 --> 00:19:23.445
So another big win on ART,
and a big win on Dalvik

00:19:23.445 --> 00:19:25.070
to use GetStringRegion.

00:19:25.070 --> 00:19:25.570
All right.

00:19:25.570 --> 00:19:27.986
Let's talk about a problem
that a lot of people have which

00:19:27.986 --> 00:19:30.451
is sharing raw data
with native code.

00:19:30.451 --> 00:19:31.700
And this is also part of this.

00:19:31.700 --> 00:19:34.350
Now if you haven't figured
this out by the talk,

00:19:34.350 --> 00:19:36.550
JNI calls are
relatively expensive.

00:19:36.550 --> 00:19:38.870
And you know again,
this is relative.

00:19:38.870 --> 00:19:43.560
We're talking about five
nanoseconds for regular a call,

00:19:43.560 --> 00:19:45.970
to about 300 nanoseconds--
on a Nexus 5,

00:19:45.970 --> 00:19:47.820
to be fair-- of a JNI call.

00:19:47.820 --> 00:19:50.150
So what are we really
talking about, the overhead

00:19:50.150 --> 00:19:51.660
of a one-way call?

00:19:55.232 --> 00:19:56.440
Or I'm sorry, a two-way call.

00:19:56.440 --> 00:19:57.500
This is a two-way call.

00:19:57.500 --> 00:20:01.650
So on Dalvik our overhead was
about a little less than 130

00:20:01.650 --> 00:20:02.600
nanoseconds.

00:20:02.600 --> 00:20:05.730
On ART, it's almost twice that.

00:20:05.730 --> 00:20:09.040
And good thing that
devices are getting faster.

00:20:09.040 --> 00:20:10.930
You can see I've
also benchmarked

00:20:10.930 --> 00:20:14.310
the Nexus 6P and a Nexus
9, both in 64 bit mode.

00:20:14.310 --> 00:20:17.160
And you can see they're
actually pretty fast.

00:20:17.160 --> 00:20:20.300
But even the Nexus
9 actually doesn't

00:20:20.300 --> 00:20:22.580
outscore Dalvik
running on a Nexus 5,

00:20:22.580 --> 00:20:24.260
for doing these kinds of things.

00:20:24.260 --> 00:20:25.705
So JNI is expensive.

00:20:25.705 --> 00:20:27.580
And the real goal of
all this, and if there's

00:20:27.580 --> 00:20:31.450
any takeback from this entire
lecture is, avoid chattiness.

00:20:31.450 --> 00:20:35.100
Every bit of chattiness
you add adds extra time.

00:20:35.100 --> 00:20:37.226
And a lot of that is stuff
you don't even think of.

00:20:37.226 --> 00:20:38.975
So for example, let's
say you're like, you

00:20:38.975 --> 00:20:41.540
know what, I'm going to avoid
writing a whole bunch of code.

00:20:41.540 --> 00:20:42.480
If you've ever
played with Unity--

00:20:42.480 --> 00:20:44.313
how many people here
have played with Unity?

00:20:44.313 --> 00:20:49.595
So one of the ways in which
you talk to Android from Unity

00:20:49.595 --> 00:20:51.990
is to use something
called Android Java Proxy.

00:20:51.990 --> 00:20:53.647
And Android Java
Proxy is really cool.

00:20:53.647 --> 00:20:55.980
Because basically it takes
it in under proxy interfaces,

00:20:55.980 --> 00:20:57.960
and it creates a dynamic
class, essentially

00:20:57.960 --> 00:21:01.130
on the fly, that's used to fill
out some interface that you can

00:21:01.130 --> 00:21:03.770
then use to talk to a whole
bunch of internal systems--

00:21:03.770 --> 00:21:05.870
which realizes
that by doing that,

00:21:05.870 --> 00:21:08.620
you are getting the
chattiest possible interface

00:21:08.620 --> 00:21:09.190
into Android.

00:21:09.190 --> 00:21:10.740
And so if you're
trying to do something

00:21:10.740 --> 00:21:12.280
over and over and
over again, that's

00:21:12.280 --> 00:21:14.800
going to actually
impact your performance.

00:21:14.800 --> 00:21:17.400
So for example, let's say
you're trying to read bytes out

00:21:17.400 --> 00:21:21.400
of some class in
Java one at a time,

00:21:21.400 --> 00:21:23.850
you realize this is going
to very, very quickly

00:21:23.850 --> 00:21:26.405
exhaust all of your CPU
time on the main thread.

00:21:26.405 --> 00:21:28.280
So you really do have
to be careful with what

00:21:28.280 --> 00:21:29.980
you do on this.

00:21:29.980 --> 00:21:31.850
And think about
the interfaces you

00:21:31.850 --> 00:21:34.450
have between your
native code and the VM.

00:21:34.450 --> 00:21:35.040
All right.

00:21:35.040 --> 00:21:37.220
Let's go back over
to this thing.

00:21:37.220 --> 00:21:39.250
So how do we actually
deal with sending

00:21:39.250 --> 00:21:45.620
big chunks of data between
native code and the runtime?

00:21:45.620 --> 00:21:49.096
And there's this cool thing
called a direct byte buffer.

00:21:49.096 --> 00:21:51.470
I don't know how many people
have played with direct byte

00:21:51.470 --> 00:21:52.920
buffers here before.

00:21:52.920 --> 00:21:55.534
You pretty much only ever want
to deal with a direct byte

00:21:55.534 --> 00:21:57.200
buffer if you're
working in native code.

00:21:57.200 --> 00:21:58.824
There's really no
other reason for them

00:21:58.824 --> 00:22:00.330
to exist, as far as I can tell.

00:22:00.330 --> 00:22:04.180
Although the VM might
choose to not actually

00:22:04.180 --> 00:22:06.510
allocate this memory out
of its normal page pool.

00:22:06.510 --> 00:22:11.930
So on some VMs, it
actually might get you

00:22:11.930 --> 00:22:13.920
memory you don't
normally have access to.

00:22:13.920 --> 00:22:15.950
But in our runtimes,
it does not.

00:22:18.570 --> 00:22:21.180
And you get this
nice allocate direct.

00:22:21.180 --> 00:22:23.022
And then when you're
inside native code,

00:22:23.022 --> 00:22:25.230
you can just get an address
for that chunk of memory,

00:22:25.230 --> 00:22:27.270
and start writing to it--
which is really cool.

00:22:27.270 --> 00:22:29.630
And there's no like,
I want to free this.

00:22:29.630 --> 00:22:31.040
There's no like,
release address.

00:22:31.040 --> 00:22:32.390
It's one call.

00:22:32.390 --> 00:22:34.390
So that's nice and fast right?

00:22:34.390 --> 00:22:35.239
In theory.

00:22:35.239 --> 00:22:37.030
So this is what this
looks like when you're

00:22:37.030 --> 00:22:38.930
using direct byte buffer.

00:22:38.930 --> 00:22:40.631
So let's look at
the performance.

00:22:40.631 --> 00:22:42.005
Again we're looking
at benchmarks

00:22:42.005 --> 00:22:44.250
here all the time
on these things--

00:22:44.250 --> 00:22:46.530
keep me up sleepless
nights doing these--

00:22:46.530 --> 00:22:48.290
and what that
actually looks like.

00:22:48.290 --> 00:22:49.920
And as you can see
once again, this

00:22:49.920 --> 00:22:52.080
is a very, very slow access.

00:22:52.080 --> 00:22:55.480
Because DirectBuffers
actually involve even more

00:22:55.480 --> 00:22:56.780
synchronization.

00:22:56.780 --> 00:22:59.540
And so then actually we're
talking about something

00:22:59.540 --> 00:23:03.060
that aren't running
a Nexus 5, is almost

00:23:03.060 --> 00:23:05.080
in the 600 nanoseconds range.

00:23:05.080 --> 00:23:07.230
So you really want-- once
you actually grab this,

00:23:07.230 --> 00:23:09.480
the answer is you really
want to use it for something.

00:23:09.480 --> 00:23:11.780
If you're using it to pass
an integer, not a good idea.

00:23:11.780 --> 00:23:14.700
You want to actually use
it to pass lots of data.

00:23:14.700 --> 00:23:15.390
All right.

00:23:15.390 --> 00:23:17.790
But there's another
side to this.

00:23:17.790 --> 00:23:22.460
Once you're inside of code
that's running on the runtime,

00:23:22.460 --> 00:23:26.400
what's the performance of byte
buffer-- direct byte buffer

00:23:26.400 --> 00:23:27.760
versus regular byte buffer?

00:23:27.760 --> 00:23:30.840
So let's take a look at that.

00:23:30.840 --> 00:23:32.690
What?

00:23:32.690 --> 00:23:37.170
OK, now let me just back
up a little bit here.

00:23:37.170 --> 00:23:41.660
Because you're seeing something
really, really strange here.

00:23:41.660 --> 00:23:44.640
You're seeing that
first of all Dalvik

00:23:44.640 --> 00:23:47.770
Nexus 5 direct byte
buffer is the slowest

00:23:47.770 --> 00:23:50.280
call by a substantial
amount, compared

00:23:50.280 --> 00:23:52.250
to all of these other calls.

00:23:52.250 --> 00:23:52.750
OK?

00:23:52.750 --> 00:23:55.750
Takes it takes 300 nanoseconds.

00:23:55.750 --> 00:23:58.220
The other thing you're noticing
is a direct byte buffer

00:23:58.220 --> 00:24:00.280
is way slower than the
standard one, which

00:24:00.280 --> 00:24:04.870
is backed by a standard
byte array in Java.

00:24:04.870 --> 00:24:07.465
So once again, two things
that are kind of weird.

00:24:07.465 --> 00:24:10.380
And wherever we see really
weird stuff like this,

00:24:10.380 --> 00:24:13.270
other than scratching
my head, it

00:24:13.270 --> 00:24:15.060
is time to go and
explore some code,

00:24:15.060 --> 00:24:17.640
and try to figure out
why that's the case.

00:24:17.640 --> 00:24:18.560
All right.

00:24:18.560 --> 00:24:21.410
So here is what actually
happens when you do

00:24:21.410 --> 00:24:22.762
allocate and allocate direct.

00:24:22.762 --> 00:24:24.220
You actually get
a different class.

00:24:24.220 --> 00:24:26.665
We're using polymorphism
here, it's awesome.

00:24:26.665 --> 00:24:29.300
You either get ByteArrayBuffer
or DirectByteBuffer,

00:24:29.300 --> 00:24:31.610
one of the two, OK?

00:24:31.610 --> 00:24:33.270
And as you can see,
ByteArrayBuffer

00:24:33.270 --> 00:24:35.920
is backed by an array.

00:24:35.920 --> 00:24:39.020
And DirectByteBuffer is actually
backed by this class called

00:24:39.020 --> 00:24:40.420
memory block.

00:24:40.420 --> 00:24:42.620
All right.

00:24:42.620 --> 00:24:44.850
And here's how we
start reading integer.

00:24:44.850 --> 00:24:47.090
We use the call and get Int.

00:24:47.090 --> 00:24:49.560
And in ByteArrayBuffer,
it's pretty standard.

00:24:49.560 --> 00:24:53.920
It actually goes into another
class called MemoryPeekInt.

00:24:53.920 --> 00:24:56.750
And inside of memory block,
we have a little bit--

00:24:56.750 --> 00:24:58.280
an extra bit of indirection.

00:24:58.280 --> 00:25:00.570
We actually have to
call into the block

00:25:00.570 --> 00:25:03.210
class, which calls
into Memory.PeakInt,

00:25:03.210 --> 00:25:04.720
but a different call.

00:25:04.720 --> 00:25:06.990
It's that PeakInt is
taking a backing array,

00:25:06.990 --> 00:25:09.800
and that other one is taking
an address plus offset.

00:25:09.800 --> 00:25:12.760
And yes, you are actually
looking essentially

00:25:12.760 --> 00:25:16.400
at pointer arithmetic
inside of the runtime right

00:25:16.400 --> 00:25:20.160
here-- not something
you see very often.

00:25:20.160 --> 00:25:22.724
So what does this mean?

00:25:22.724 --> 00:25:24.140
Well, when you're
actually looking

00:25:24.140 --> 00:25:26.190
at how this is implemented--
if you try to find the source

00:25:26.190 --> 00:25:27.520
code, this is what you'll see.

00:25:27.520 --> 00:25:31.330
You'll see probably the
most classic implementation

00:25:31.330 --> 00:25:35.560
of how to pull
data from an array

00:25:35.560 --> 00:25:39.140
and get it into an integer that
you see inside of the ByteArray

00:25:39.140 --> 00:25:39.640
class.

00:25:39.640 --> 00:25:43.755
And inside of memory block,
it actually calls into JNI.

00:25:43.755 --> 00:25:44.630
All right, all right.

00:25:44.630 --> 00:25:48.820
So now, remember-- let's
go back to this graph here.

00:25:48.820 --> 00:25:55.950
So we saw that ART is way,
way faster than Dalvik

00:25:55.950 --> 00:25:56.640
at doing this.

00:25:56.640 --> 00:25:59.215
And yet we just demonstrated
looking at the source code,

00:25:59.215 --> 00:26:02.730
that it's actually
calling into JNI.

00:26:02.730 --> 00:26:03.790
So that's really weird.

00:26:03.790 --> 00:26:05.590
Why is it so much faster?

00:26:05.590 --> 00:26:06.960
All right.

00:26:06.960 --> 00:26:09.970
So once again, here's
what actually happens

00:26:09.970 --> 00:26:12.150
inside of that native code.

00:26:12.150 --> 00:26:13.520
But that really doesn't matter.

00:26:13.520 --> 00:26:16.241
Because we've shown almost all
of the cost of this operation

00:26:16.241 --> 00:26:18.740
is going to be in synchronizing
between the different thread

00:26:18.740 --> 00:26:21.035
states, between the two VMS.

00:26:21.035 --> 00:26:26.400
So it turns out that ART is
actually doing a little trick.

00:26:26.400 --> 00:26:28.670
And that is, when it
actually declares the method,

00:26:28.670 --> 00:26:30.690
it's declaring it with
this little exclamation

00:26:30.690 --> 00:26:33.930
point on it-- which is a
flag to the VM that says,

00:26:33.930 --> 00:26:38.097
well, this is a
dangerous function.

00:26:38.097 --> 00:26:39.930
Actually it's a flag
to the person coding it

00:26:39.930 --> 00:26:40.760
that it's a dangerous function.

00:26:40.760 --> 00:26:41.880
It's a flag to the
VM saying this is

00:26:41.880 --> 00:26:43.480
a very non-dangerous function.

00:26:43.480 --> 00:26:45.610
It's not going to try
to do anything in Java.

00:26:45.610 --> 00:26:47.420
It's not going to
last very long.

00:26:47.420 --> 00:26:49.870
So let's not actually
go through and change

00:26:49.870 --> 00:26:51.240
the state of the thread at all.

00:26:51.240 --> 00:26:56.010
Let's just run this code
as quickly as possible.

00:26:56.010 --> 00:27:01.460
So once again this is
how long it actually

00:27:01.460 --> 00:27:04.880
takes to read that
integer from a ByteBuffer.

00:27:04.880 --> 00:27:12.330
Now it's still about half
the speed-- even on ART--

00:27:12.330 --> 00:27:15.130
of our StandardByteBuffer call--
even with all of that, even

00:27:15.130 --> 00:27:16.790
with this fast switching.

00:27:16.790 --> 00:27:20.960
And that's because if you
actually go and throw this

00:27:20.960 --> 00:27:23.760
into a debugger, you realize
that that whole statement

00:27:23.760 --> 00:27:26.010
about where it's using
lots and lots of shifts

00:27:26.010 --> 00:27:28.260
in order to do it, is actually
not getting run at all.

00:27:28.260 --> 00:27:29.610
It's actually an intrinsic.

00:27:29.610 --> 00:27:31.800
And so that's how this
is speeding it up.

00:27:31.800 --> 00:27:33.965
And also even if it
was running that code,

00:27:33.965 --> 00:27:35.760
ART is just really fast.

00:27:35.760 --> 00:27:37.510
Like you know, it's
really, really fast.

00:27:37.510 --> 00:27:41.460
And it turns out, there is
some overhead in doing even

00:27:41.460 --> 00:27:43.096
this fast JNI call.

00:27:43.096 --> 00:27:44.720
Because it still has
to set up the call

00:27:44.720 --> 00:27:46.303
stack and all the
other things that it

00:27:46.303 --> 00:27:48.820
would have to do
to actually switch

00:27:48.820 --> 00:27:52.370
from running in the
runtime to running native.

00:27:52.370 --> 00:27:55.040
and that takes about
50 to 60 nanoseconds,

00:27:55.040 --> 00:27:59.070
according to my benchmarks
just to do that, in fast JNI.

00:27:59.070 --> 00:27:59.940
All right.

00:27:59.940 --> 00:28:03.440
So is there anything
we can do to avoid

00:28:03.440 --> 00:28:06.750
having to make a JNI
call for every single int

00:28:06.750 --> 00:28:08.490
we want to read?

00:28:08.490 --> 00:28:10.590
It turns out there is.

00:28:10.590 --> 00:28:15.100
We can actually get it all
at once using something

00:28:15.100 --> 00:28:16.890
like this-- so we
can get buffer,

00:28:16.890 --> 00:28:20.219
we can allocate an array, we can
wrap that in a new ByteBuffer,

00:28:20.219 --> 00:28:21.260
and then we can get that.

00:28:21.260 --> 00:28:23.301
OK, and then we have to
fiddle with the position,

00:28:23.301 --> 00:28:25.070
because otherwise
there's an overflow,

00:28:25.070 --> 00:28:27.680
there's no fast call to
actually just give me

00:28:27.680 --> 00:28:31.090
the contents of that buffer
that'll actually work.

00:28:31.090 --> 00:28:33.700
So believe it or not, this
is what you have to do--

00:28:33.700 --> 00:28:35.500
and what does that
look like, if you

00:28:35.500 --> 00:28:36.750
do all of that, allegations?

00:28:36.750 --> 00:28:38.740
And this is even including
deallocations and stuff

00:28:38.740 --> 00:28:39.240
like that.

00:28:39.240 --> 00:28:41.470
And the answer is of
course, it's pretty slow.

00:28:41.470 --> 00:28:43.530
It's really, really
slow on Dalvik.

00:28:43.530 --> 00:28:45.810
You can see like--
this is where you

00:28:45.810 --> 00:28:49.250
start getting into multiple
levels of optimization here.

00:28:49.250 --> 00:28:52.914
But if you're going to
be moving a lot, lot,

00:28:52.914 --> 00:28:57.060
lot of data around-- big,
big, big chunk of data.

00:28:57.060 --> 00:29:00.310
And you're going to be
accessing that from with inside

00:29:00.310 --> 00:29:03.310
of the run time, then yes.

00:29:03.310 --> 00:29:06.252
This is a strategy that
might make sense for you.

00:29:06.252 --> 00:29:07.710
Like for example,
one of the things

00:29:07.710 --> 00:29:10.780
you might want to be doing
is using like FlatBuffers,

00:29:10.780 --> 00:29:15.450
to move big C structures
to the runtime.

00:29:15.450 --> 00:29:17.510
How many of you are
familiar with FlatBuffers,

00:29:17.510 --> 00:29:19.100
first of all, when I say that?

00:29:19.100 --> 00:29:20.849
All right, so FlatBuffers
are really cool.

00:29:20.849 --> 00:29:24.830
They're an open source
project that my team created.

00:29:24.830 --> 00:29:31.910
And it basically allows you
to do really, really efficient

00:29:31.910 --> 00:29:34.400
translation from
stuff that's coming in

00:29:34.400 --> 00:29:36.740
either from disk
or from network,

00:29:36.740 --> 00:29:38.310
into structures
that you can use.

00:29:38.310 --> 00:29:41.090
It is about as efficient as
you can get, given the amount

00:29:41.090 --> 00:29:43.037
of flexibility that it has.

00:29:43.037 --> 00:29:45.286
It's actually very similar
to protobufs, if any of you

00:29:45.286 --> 00:29:48.090
have used that-- except that
it's designed from the start

00:29:48.090 --> 00:29:53.145
to run on mobile, and to
run really, really fast.

00:29:53.145 --> 00:29:54.770
So if you're doing
something like that,

00:29:54.770 --> 00:29:57.610
you might actually get some
performance out of this.

00:29:57.610 --> 00:29:59.970
All right, so now since
we have a little time,

00:29:59.970 --> 00:30:02.200
I wanted to show you
just a little bit of how

00:30:02.200 --> 00:30:06.310
you use JNI in Android Studio.

00:30:06.310 --> 00:30:06.810
All right.

00:30:06.810 --> 00:30:08.640
So once again, how
many of you here

00:30:08.640 --> 00:30:12.170
have actually tried doing
this in Android Studio?

00:30:12.170 --> 00:30:14.915
OK, so that's not an enormous,
enormous number of people.

00:30:14.915 --> 00:30:16.010
But that's OK.

00:30:16.010 --> 00:30:18.600
Because this is
actually really cool.

00:30:18.600 --> 00:30:25.940
This made my life so
much easier than trying

00:30:25.940 --> 00:30:30.260
to actually deal with
the various things that

00:30:30.260 --> 00:30:30.980
go on in JNI.

00:30:30.980 --> 00:30:33.400
Here's a whole bunch
of native declarations

00:30:33.400 --> 00:30:35.700
that are inside of my
JNI benchmark class.

00:30:35.700 --> 00:30:37.570
And you can see
the kinds of things

00:30:37.570 --> 00:30:39.950
you'd expect, like
these ByteArrayCalls

00:30:39.950 --> 00:30:41.750
and these string calls.

00:30:41.750 --> 00:30:43.960
So let's say I wanted to
add another native method.

00:30:43.960 --> 00:30:44.460
OK.

00:30:44.460 --> 00:30:47.700
So I'm going to type Native.

00:30:47.700 --> 00:30:50.200
And let's have it
return an Int--

00:30:50.200 --> 00:30:52.850
I don't have to call it JNI,
but just for consistency, I'm

00:30:52.850 --> 00:31:02.140
going to call it JNI pass
a bunch of stuff to native.

00:31:02.140 --> 00:31:13.360
So we're going to pass let's
say a string, a ByteBuffer,

00:31:13.360 --> 00:31:25.847
an integer, a long
et cetera, et cetera.

00:31:25.847 --> 00:31:27.930
And you see a couple of
things have happened here.

00:31:27.930 --> 00:31:30.830
Probably the most useful
thing is that we actually now

00:31:30.830 --> 00:31:34.900
are compiling the native
code and the Java code,

00:31:34.900 --> 00:31:37.260
all in one Gradle build--
which is really awesome.

00:31:37.260 --> 00:31:39.950
Because we can do
stuff like say, hey,

00:31:39.950 --> 00:31:42.420
this function
actually isn't found.

00:31:42.420 --> 00:31:43.400
We can't resolve this.

00:31:43.400 --> 00:31:45.000
So you see, it shows up red.

00:31:45.000 --> 00:31:46.260
It knows that it's
not in my native code.

00:31:46.260 --> 00:31:47.926
So here's the really,
really cool trick.

00:31:47.926 --> 00:31:50.484
For anyone who's done
a lot of JNI code,

00:31:50.484 --> 00:31:51.900
the ability to do
this is awesome.

00:31:51.900 --> 00:31:56.690
I can do create function
here, click on this,

00:31:56.690 --> 00:32:01.800
and now I have a native function
that's been created inside

00:32:01.800 --> 00:32:03.980
of that C file.

00:32:03.980 --> 00:32:05.560
And this is really cool.

00:32:05.560 --> 00:32:08.340
First of all, it's also done
some helper things for me.

00:32:08.340 --> 00:32:11.920
It thinks I might want to get
this string interesting enough,

00:32:11.920 --> 00:32:15.150
into UTF rather than
double byte characters.

00:32:15.150 --> 00:32:19.560
But hey, you know, it's
probably what your code wants.

00:32:19.560 --> 00:32:22.374
And then it's also gotten
the ByteArryElements for me.

00:32:22.374 --> 00:32:23.790
And it's released
them at the end.

00:32:23.790 --> 00:32:25.280
Because it's
assuming that you're

00:32:25.280 --> 00:32:26.570
actually going to want
to use these things.

00:32:26.570 --> 00:32:28.590
And so it actually puts
in that code for you.

00:32:28.590 --> 00:32:30.090
So this is really, really cool.

00:32:30.090 --> 00:32:32.770
And the best part is when I
go back to my benchmark class

00:32:32.770 --> 00:32:34.870
here, you'll see
it's no longer red.

00:32:34.870 --> 00:32:38.780
It's actually done the
compile and we are golden.

00:32:38.780 --> 00:32:42.136
We are actually ready to now
run that inside of this class.

00:32:42.136 --> 00:32:44.010
So if you haven't had
a chance to play around

00:32:44.010 --> 00:32:46.460
with Android Studio and
in support of the NDK,

00:32:46.460 --> 00:32:47.610
I highly recommend it.

00:32:47.610 --> 00:32:50.260
It's still a little bit of work
to get your Gradle project up

00:32:50.260 --> 00:32:50.760
and running.

00:32:50.760 --> 00:32:52.480
Because you've still got to
use the experimental version

00:32:52.480 --> 00:32:52.980
of Gradle.

00:32:52.980 --> 00:32:56.390
But you don't actually have to
use the experimental version

00:32:56.390 --> 00:32:57.580
of Android Studio.

00:32:57.580 --> 00:32:58.840
It is now in mainline.

00:32:58.840 --> 00:33:01.030
So go check it
out, play with it,

00:33:01.030 --> 00:33:02.530
and make sure that
you're not making

00:33:02.530 --> 00:33:05.680
your applications that actually
use the NDK very chatty.

00:33:05.680 --> 00:33:09.150
If there's anything you can take
back from this entire lecture.

00:33:09.150 --> 00:33:11.030
All right, so I'm
going to switch back

00:33:11.030 --> 00:33:15.590
into non-mirroring
mode, so I can

00:33:15.590 --> 00:33:18.800
finish all of the
citing slides that

00:33:18.800 --> 00:33:25.270
are left in my presentation,
which is really just this.

00:33:25.270 --> 00:33:29.340
If you need to get in touch
with me, this is how you do it.

00:33:29.340 --> 00:33:31.112
And I hope you have
enjoyed the talk.

00:33:31.112 --> 00:33:32.570
I hope you've
learned a little bit.

00:33:32.570 --> 00:33:36.180
I have time for some questions,
if anyone wants to stump me,

00:33:36.180 --> 00:33:38.430
this is a really
good chance to do it.

00:33:38.430 --> 00:33:41.460
Because you most likely will.

00:33:41.460 --> 00:33:44.150
But other than that,
again, it's not

00:33:44.150 --> 00:33:45.525
that it's scary to use the NDK.

00:33:45.525 --> 00:33:48.040
It is really cool to use the
new Android Studio stuff.

00:33:48.040 --> 00:33:50.460
And you just have
to be cognizant

00:33:50.460 --> 00:33:52.360
of the kinds of
performance problems

00:33:52.360 --> 00:33:53.475
you could create with it.

00:33:53.475 --> 00:33:55.600
And I hope you've got a
little bit of it from this.

00:33:55.600 --> 00:33:58.090
And also once again,
what's wonderful

00:33:58.090 --> 00:33:59.970
about a platform like
Android, an open source

00:33:59.970 --> 00:34:02.520
platform like Android, you
can go and explore the code.

00:34:02.520 --> 00:34:05.660
You can actually understand how
we solve these very difficult

00:34:05.660 --> 00:34:06.905
problems in many cases.

00:34:06.905 --> 00:34:09.030
And you can learn something
and take something back

00:34:09.030 --> 00:34:11.929
with your engineering
career, and use it again.

00:34:11.929 --> 00:34:14.949
And that to me is half the fun
of working in an open source

00:34:14.949 --> 00:34:15.449
project.

00:34:15.449 --> 00:34:18.080
I mean, wouldn't it be awesome
if everyone could simply say,

00:34:18.080 --> 00:34:20.496
you know, here's the reason
why that doesn't perform well.

00:34:20.496 --> 00:34:21.872
Let's go look at
the source code.

00:34:21.872 --> 00:34:23.829
And I think everyone
should be able to do that.

00:34:23.829 --> 00:34:26.449
So I'm super excited to be
able to work on a development

00:34:26.449 --> 00:34:29.830
project that actually does
have an open source backend.

00:34:29.830 --> 00:34:31.750
So that being said,
thank you very

00:34:31.750 --> 00:34:33.608
much for coming this morning.

00:34:33.608 --> 00:34:36.098
[APPLAUSE]

00:34:39.090 --> 00:34:40.765
And I will take questions now.

00:34:40.765 --> 00:34:41.264
OK?

00:34:41.264 --> 00:34:47.820
AUDIENCE: [INAUDIBLE]

00:34:48.320 --> 00:34:49.999
DAN GALPIN: I do not know.

00:34:49.999 --> 00:34:51.290
That is a really good question.

00:34:51.290 --> 00:34:52.310
You've now stumped me.

00:34:52.310 --> 00:34:54.429
I'm so embarrassed.

00:34:54.429 --> 00:34:56.780
That's OK.

00:34:56.780 --> 00:34:57.485
And yes?

00:34:57.485 --> 00:34:58.360
AUDIENCE: [INAUDIBLE]

00:35:02.970 --> 00:35:03.886
DAN GALPIN: Uh huh.

00:35:03.886 --> 00:35:04.761
AUDIENCE: [INAUDIBLE]

00:35:07.218 --> 00:35:08.170
DAN GALPIN: Mm hm.

00:35:08.170 --> 00:35:09.045
AUDIENCE: [INAUDIBLE]

00:35:12.470 --> 00:35:16.190
DAN GALPIN: So if it
is, like let's say

00:35:16.190 --> 00:35:21.274
you're doing like Java getString
critical, which would mark that

00:35:21.274 --> 00:35:21.940
as being in use.

00:35:21.940 --> 00:35:23.670
That will never be moved.

00:35:23.670 --> 00:35:25.170
That is fixed in memory.

00:35:25.170 --> 00:35:29.300
DirectByteBuffers are also fixed
in memory, they can't be moved.

00:35:29.300 --> 00:35:32.610
There is a little bit of
weirdness around that.

00:35:32.610 --> 00:35:37.440
Because if you look at the
way they are allocated, ,

00:35:37.440 --> 00:35:40.764
there is a little bit of code
that checks around moving them.

00:35:40.764 --> 00:35:42.430
But once you're
actually accessing them,

00:35:42.430 --> 00:35:47.160
getting that direct byte buffer
address, it is fixed in memory.

00:35:47.160 --> 00:35:49.900
So it can be moved, however
outside of that call.

00:35:49.900 --> 00:35:52.920
So once that call goes
away, my understanding

00:35:52.920 --> 00:35:54.800
is that it can be moved.

00:35:54.800 --> 00:36:00.730
So again, it's protected for the
lifetime of that call, I think.

00:36:00.730 --> 00:36:01.980
That's a really good question.

00:36:01.980 --> 00:36:05.800
I think that's what I remember,
and don't quote me on that one.

00:36:05.800 --> 00:36:06.560
I might be wrong.

00:36:06.560 --> 00:36:08.030
It might be always protected.

00:36:08.030 --> 00:36:13.360
But in looking at
the allocator, there

00:36:13.360 --> 00:36:16.120
are actually two different kinds
of allocation that can happen.

00:36:16.120 --> 00:36:19.020
And for a very, very small--
like less than three pages,

00:36:19.020 --> 00:36:21.770
it goes into the
movable allocation pool.

00:36:21.770 --> 00:36:24.060
And for things that are
larger than that, at least

00:36:24.060 --> 00:36:26.660
in the current implementation,
it's not movable ever.

00:36:26.660 --> 00:36:31.990
So yeah, kind of yes and no.

00:36:31.990 --> 00:36:32.874
Yeah?

00:36:32.874 --> 00:36:33.749
AUDIENCE: [INAUDIBLE]

00:36:45.900 --> 00:36:47.650
DAN GALPIN: It depends
on whether or not--

00:36:47.650 --> 00:36:51.470
so the question is, if you're
using a backend to deal with

00:36:51.470 --> 00:36:55.737
this data, and you're
talking to C++ code,

00:36:55.737 --> 00:36:58.070
ultimately you need to get
that data into your C++ code,

00:36:58.070 --> 00:37:01.180
is it better to just use the
networking services that are

00:37:01.180 --> 00:37:04.929
built into the NDK, or is it
better to actually use and do

00:37:04.929 --> 00:37:05.720
everything in Java?

00:37:05.720 --> 00:37:10.830
And there's kind of two
questions I have about this.

00:37:10.830 --> 00:37:13.340
Is the performance of your
networking something you

00:37:13.340 --> 00:37:15.612
actually even care
that much about?

00:37:15.612 --> 00:37:16.570
That's the first thing.

00:37:16.570 --> 00:37:18.950
If you're not on
the main thread,

00:37:18.950 --> 00:37:23.070
and you're processing
some stuff in native code,

00:37:23.070 --> 00:37:26.082
you might not care that's it's
a little bit more expensive.

00:37:26.082 --> 00:37:28.040
Because you're not actually
affecting the frame

00:37:28.040 --> 00:37:29.670
rate of your application.

00:37:29.670 --> 00:37:32.990
And you might be saving
an enormous amount of time

00:37:32.990 --> 00:37:35.290
by actually using the
implementations that

00:37:35.290 --> 00:37:36.550
are in Java.

00:37:36.550 --> 00:37:38.660
So as a general
rule, you really want

00:37:38.660 --> 00:37:40.410
to look to see whether
or not you actually

00:37:40.410 --> 00:37:43.110
care about that particular
performance loss,

00:37:43.110 --> 00:37:44.822
and then weigh it.

00:37:44.822 --> 00:37:46.280
Yes, for a performance
you're going

00:37:46.280 --> 00:37:48.420
to do way better if
you parse something

00:37:48.420 --> 00:37:51.270
in completely in native code--
especially if you're not using

00:37:51.270 --> 00:37:54.440
it on the Java side of things.

00:37:54.440 --> 00:37:55.940
Then yeah, that
would make sense.

00:37:55.940 --> 00:37:57.481
But the real question
you have to ask

00:37:57.481 --> 00:37:59.100
is what's the cost of that?

00:37:59.100 --> 00:38:00.930
What's the cost in
terms of opportunity?

00:38:00.930 --> 00:38:02.820
How much more time is
it going to take me?

00:38:02.820 --> 00:38:03.830
Is it really worthwhile?

00:38:03.830 --> 00:38:06.760
And that's-- with all of these
things, that's what I say.

00:38:06.760 --> 00:38:09.500
If it's an easy optimization,
like let's throw a couple

00:38:09.500 --> 00:38:11.830
parameters into a JNI
call, by all means do it.

00:38:11.830 --> 00:38:14.410
Don't waste more time,
don't waste more battery.

00:38:14.410 --> 00:38:17.930
But if it's going to mean
rewriting an entire library,

00:38:17.930 --> 00:38:20.340
then really look closely at
it and say, how much am I

00:38:20.340 --> 00:38:22.510
really gaining out of this?

00:38:22.510 --> 00:38:23.010
Mm hm?

00:38:23.010 --> 00:38:23.885
AUDIENCE: [INAUDIBLE]

00:38:32.788 --> 00:38:33.537
DAN GALPIN: Mm hm.

00:38:33.537 --> 00:38:34.412
AUDIENCE: [INAUDIBLE]

00:38:38.510 --> 00:38:40.360
DAN GALPIN: OK, so the
question is about JNI

00:38:40.360 --> 00:38:42.041
versus using Renderscript.

00:38:42.041 --> 00:38:43.040
When does it make sense?

00:38:43.040 --> 00:38:45.980
So what's really cool about
Renderscript first of all,

00:38:45.980 --> 00:38:49.310
is that Renderscript is
actually LLVM byte code that

00:38:49.310 --> 00:38:51.110
gets compiled on the device.

00:38:51.110 --> 00:38:53.790
And there's some beautiful
things that you get from that.

00:38:53.790 --> 00:38:56.520
One is that it can
be actually optimized

00:38:56.520 --> 00:39:00.050
for that particular CPU
that's running on that device,

00:39:00.050 --> 00:39:00.940
to some degree.

00:39:00.940 --> 00:39:02.523
They're certain kinds
of optimizations

00:39:02.523 --> 00:39:03.550
you can't do for LLVM.

00:39:03.550 --> 00:39:05.175
But there's a whole
bunch that you can.

00:39:05.175 --> 00:39:08.100
There's intrinsics that you
can actually swap in and out.

00:39:08.100 --> 00:39:10.240
There's like peephole
optimizations

00:39:10.240 --> 00:39:13.930
that are specific to actually
how that particular device

00:39:13.930 --> 00:39:14.970
works.

00:39:14.970 --> 00:39:18.000
So one of the secrets
of Renderscript

00:39:18.000 --> 00:39:22.120
is that it actually can generate
better code than the Compiler

00:39:22.120 --> 00:39:23.180
can, in some cases.

00:39:23.180 --> 00:39:25.360
The second thing is it's
also running in a kernel.

00:39:25.360 --> 00:39:28.210
It's actually running in
its own little tiny machine,

00:39:28.210 --> 00:39:31.230
that is used to run
massively parallel stuff.

00:39:31.230 --> 00:39:34.190
And it's really set up to
do that very, very well.

00:39:34.190 --> 00:39:39.870
So if your problem space
falls into Renderscript,

00:39:39.870 --> 00:39:42.900
something that's really
helped my parallelization,

00:39:42.900 --> 00:39:48.320
and something that
is also helped

00:39:48.320 --> 00:39:51.180
by using these intrinsics that
you get from the LLVM byte

00:39:51.180 --> 00:39:53.110
code, then by all means use it.

00:39:53.110 --> 00:39:56.360
But as a general
rule, I would say

00:39:56.360 --> 00:39:59.420
that again you're looking at
opportunity, time and cost.

00:39:59.420 --> 00:40:01.800
If you're not seeing that it's
a performance issue that's

00:40:01.800 --> 00:40:05.700
impacting you, it may not make
sense to go through to that.

00:40:05.700 --> 00:40:11.270
Part of the reason we have JNI
is to be able to reuse all this

00:40:11.270 --> 00:40:14.630
crazy amount of C and C++
code that's out there.

00:40:14.630 --> 00:40:17.666
And so for me, it's you always
have to balance these things.

00:40:17.666 --> 00:40:19.290
But from a true
performance standpoint,

00:40:19.290 --> 00:40:21.250
it is very possible
that Renderscript

00:40:21.250 --> 00:40:23.360
will be the highest
performing way

00:40:23.360 --> 00:40:24.880
to do certain kinds
of operations,

00:40:24.880 --> 00:40:28.170
because it can just do
a whole bunch of things

00:40:28.170 --> 00:40:29.937
that the Compiler can't
do because it just

00:40:29.937 --> 00:40:32.020
doesn't know enough about
the system architecture.

00:40:32.020 --> 00:40:34.990
And it really depends on how
well the individual OEMs have

00:40:34.990 --> 00:40:37.360
actually managed to-- or
chip providers have actually

00:40:37.360 --> 00:40:39.370
managed to optimize the
Renderscript Compiler

00:40:39.370 --> 00:40:42.080
on their particular chipsets.

00:40:42.080 --> 00:40:43.850
So there's a lot
of variables here.

00:40:43.850 --> 00:40:45.669
I wish there was a
cut and dry answer.

00:40:45.669 --> 00:40:47.210
But what's great
about Renderscript--

00:40:47.210 --> 00:40:49.940
the really cool reason you might
want to use it anyway-- even

00:40:49.940 --> 00:40:51.890
despite all that, is
because as I said,

00:40:51.890 --> 00:40:54.950
the LLVM byte code gets compiled
on the individual system.

00:40:54.950 --> 00:40:57.320
So you only have to ship
one copy of the byte code.

00:40:57.320 --> 00:40:59.540
You don't have to use a
dependency on the NDK.

00:40:59.540 --> 00:41:02.910
Or you don't have to worry about
it bloating the size of your

00:41:02.910 --> 00:41:04.880
build with a bunch of
different executables.

00:41:04.880 --> 00:41:06.910
And that by itself
might be worth

00:41:06.910 --> 00:41:09.870
investigating Renderscript,
just for that one reason.

00:41:09.870 --> 00:41:12.810
Now with 64-bit, I
believe you actually

00:41:12.810 --> 00:41:16.720
do need to ship 64-bit byte
code so it not completely

00:41:16.720 --> 00:41:19.370
transparent to
architecture, I think.

00:41:19.370 --> 00:41:20.620
I haven't actually tried this.

00:41:20.620 --> 00:41:23.282
But I vaguely remember
reading that somewhere.

00:41:23.282 --> 00:41:24.970
AUDIENCE: [INAUDIBLE]

00:41:24.970 --> 00:41:25.695
DAN GALPIN: Sure.

00:41:25.695 --> 00:41:26.570
AUDIENCE: [INAUDIBLE]

00:41:34.730 --> 00:41:37.090
DAN GALPIN: So if you do
[? Maleks ?] and Freeze,

00:41:37.090 --> 00:41:37.830
it's separate.

00:41:37.830 --> 00:41:39.580
It's actually using a
different allocator.

00:41:39.580 --> 00:41:43.070
It's using JE [? Malek ?]
when you're doing stuff from

00:41:43.070 --> 00:41:45.300
the NDK, and you're
using [? Rozalek, ?] when

00:41:45.300 --> 00:41:48.250
you're in the virtual machine.

00:41:48.250 --> 00:41:51.442
And the reason is that if you
went to the talk yesterday,

00:41:51.442 --> 00:41:53.400
[? Rozalek ?] is really,
really good at garbage

00:41:53.400 --> 00:41:55.730
collecting in the background.

00:41:55.730 --> 00:42:00.050
And we're trying to avoid heap
fragmentation by bucketing

00:42:00.050 --> 00:42:02.260
all of our memory allocations.

00:42:02.260 --> 00:42:04.971
[? JE Malek ?] is not trying
to have everything cleaned up

00:42:04.971 --> 00:42:05.720
in the background.

00:42:05.720 --> 00:42:07.380
It doesn't have to
be as paralyzed.

00:42:07.380 --> 00:42:11.990
So it's a slightly faster
allocator then [? Rozalek ?],

00:42:11.990 --> 00:42:13.440
when you're in native.

00:42:13.440 --> 00:42:15.910
So yeah, they don't share space.

00:42:15.910 --> 00:42:17.980
It's been a long
time thing in Android

00:42:17.980 --> 00:42:19.560
that if you
desperately, desperately

00:42:19.560 --> 00:42:22.100
need to run something that
couldn't run inside of the heap

00:42:22.100 --> 00:42:24.650
space that we give
you, in the run time

00:42:24.650 --> 00:42:26.044
you can add native code.

00:42:26.044 --> 00:42:27.460
There's other ways
to do that too.

00:42:27.460 --> 00:42:28.680
You can run multiple VMs.

00:42:28.680 --> 00:42:32.425
Like by launching each activity
into a different process.

00:42:32.425 --> 00:42:34.860
There's all sorts of ways
of getting around this.

00:42:34.860 --> 00:42:40.539
But even using ashmem is a last
ditch resort, if you actually

00:42:40.539 --> 00:42:42.080
are completely out
of all the memory,

00:42:42.080 --> 00:42:43.079
we allow you to do that.

00:42:43.079 --> 00:42:45.860
But realistically, yes, they're
entirely separate heaps.

00:42:49.616 --> 00:42:50.115
Yes?

00:42:50.115 --> 00:42:50.990
AUDIENCE: [INAUDIBLE]

00:42:57.250 --> 00:42:59.000
DAN GALPIN: Oh, it's
not that complicated.

00:42:59.000 --> 00:43:01.570
It's just if you
are-- so the reason

00:43:01.570 --> 00:43:04.530
I say-- complicated was
probably not the right term

00:43:04.530 --> 00:43:06.690
to use there.

00:43:06.690 --> 00:43:09.010
It's mostly that we've changed
the structure of the way

00:43:09.010 --> 00:43:10.789
the Gradle files look.

00:43:10.789 --> 00:43:12.580
So if you actually look
at what we've done,

00:43:12.580 --> 00:43:16.070
we've added the concept of model
into the experimental version

00:43:16.070 --> 00:43:17.410
of Gradle.

00:43:17.410 --> 00:43:18.410
Oh sorry, you can't see.

00:43:18.410 --> 00:43:22.330
Let me mirror, let me
mirror the display.

00:43:22.330 --> 00:43:25.281
I just did like the dumb
Californian thing here.

00:43:25.281 --> 00:43:25.780
All right.

00:43:25.780 --> 00:43:30.090
So now you can see
what I'm seeing.

00:43:30.090 --> 00:43:34.630
So if you take a look at
the Build.Gradle here,

00:43:34.630 --> 00:43:37.810
you'll notice that we've
added this concept of model.

00:43:37.810 --> 00:43:40.620
So now Android is now at the
top, model is at the top.

00:43:40.620 --> 00:43:43.374
So basically you need to
go through and restructure

00:43:43.374 --> 00:43:45.040
your Gradle build a
little bit, in order

00:43:45.040 --> 00:43:46.590
to take advantage of this.

00:43:46.590 --> 00:43:50.040
There are some pretty
good stuff online.

00:43:50.040 --> 00:43:52.350
You also see like all the
kind of standard things you'd

00:43:52.350 --> 00:43:55.370
expect to see in the
old NDK build is there.

00:43:55.370 --> 00:44:01.010
You can actually
add libraries here.

00:44:01.010 --> 00:44:05.870
And also turn on-- sorry,
static libraries as well as

00:44:05.870 --> 00:44:07.210
dynamic libraries here.

00:44:07.210 --> 00:44:09.170
So pretty basic stuff.

00:44:09.170 --> 00:44:11.350
You can see I'm not using
any of this in that.

00:44:11.350 --> 00:44:13.650
This is also how you build
different product flavors.

00:44:13.650 --> 00:44:18.910
I'm building x86 arm
seven and arm eight.

00:44:18.910 --> 00:44:20.410
Actually I'm building
all, because I

00:44:20.410 --> 00:44:21.760
have all of these here.

00:44:21.760 --> 00:44:22.700
It's hilarious.

00:44:22.700 --> 00:44:28.350
But any case, this is how
you would do product flavors,

00:44:28.350 --> 00:44:29.690
and dependencies.

00:44:29.690 --> 00:44:32.300
Again just like
normal Gradle stuff.

00:44:32.300 --> 00:44:35.110
So it's a little
different in structure.

00:44:35.110 --> 00:44:36.690
but it's really
not hard to set up

00:44:36.690 --> 00:44:38.080
once you've actually set it up.

00:44:38.080 --> 00:44:41.310
If you want, I can even show
you debugging, it's really cool.

00:44:41.310 --> 00:44:43.990
I think actually-- I may not
be able to show you debugging.

00:44:43.990 --> 00:44:45.230
We're basically out of time.

00:44:45.230 --> 00:44:46.897
But if you want I'll
show you debugging.

00:44:46.897 --> 00:44:48.771
If anyone wants to come
to a table out there,

00:44:48.771 --> 00:44:50.569
I can show you how
the debugger works.

00:44:50.569 --> 00:44:51.444
AUDIENCE: [INAUDIBLE]

00:44:59.035 --> 00:45:00.410
DAN GALPIN: So if
[AUDIO OUT] two

00:45:00.410 --> 00:45:03.790
options [AUDIO OUT] Google Play.

00:45:03.790 --> 00:45:07.200
You can either upload
each individual variant

00:45:07.200 --> 00:45:10.560
as a separate multi-APK chunk.

00:45:10.560 --> 00:45:12.730
Basically those are all
separated by version codes.

00:45:12.730 --> 00:45:17.860
Or your other option is, you
can put them all into one APK,

00:45:17.860 --> 00:45:22.210
and it will do the right
thing when it actually

00:45:22.210 --> 00:45:24.010
launches the applications.

00:45:24.010 --> 00:45:25.191
You've got two options.

00:45:25.191 --> 00:45:27.190
It really depends how
much native code you have,

00:45:27.190 --> 00:45:29.140
and what percentage of
your APK size it is.

00:45:29.140 --> 00:45:33.220
For some people, even having six
flavors of their NDK libraries

00:45:33.220 --> 00:45:35.544
will only be a negligible
amount of their space.

00:45:35.544 --> 00:45:37.460
For others, let's say
you're running something

00:45:37.460 --> 00:45:39.249
really big and
heavy like Unity--

00:45:39.249 --> 00:45:41.540
you now, it has its own
runtime and all sorts of stuff.

00:45:41.540 --> 00:45:44.340
You're definitely going to
have to seriously consider

00:45:44.340 --> 00:45:46.080
[AUDIO OUT]
distribute it on Play.

00:45:46.080 --> 00:45:47.735
So a multi-APK is
really the way to go

00:45:47.735 --> 00:45:49.860
if you want [AUDIO OUT]
lots of different versions.

00:45:49.860 --> 00:45:53.660
And [AUDIO OUT] native
machine like that, and I

00:45:53.660 --> 00:45:55.470
highly recommend doing it.

00:45:55.470 --> 00:46:02.550
[AUDIO OUT] use our translator
to actually arm code.

00:46:02.550 --> 00:46:05.390
It's pretty fast, but it's not
nearly as battery efficient

00:46:05.390 --> 00:46:06.510
as x86 code.

00:46:06.510 --> 00:46:11.380
So I highly recommend
doing an x86 build as well.

00:46:11.380 --> 00:46:14.010
And I think one of the
big things I hope we do

00:46:14.010 --> 00:46:15.850
is make multiple APK
even easier to use.

00:46:15.850 --> 00:46:17.990
Because right now, there's
sort of a partitioning

00:46:17.990 --> 00:46:19.780
scheme we suggest.

00:46:19.780 --> 00:46:22.460
And it's a little bit
more of a challenge

00:46:22.460 --> 00:46:24.649
to walk through the first
time on the Play Store.

00:46:24.649 --> 00:46:26.440
So I'm hoping we actually
make that better.

00:46:26.440 --> 00:46:27.750
I think out of time though.

00:46:27.750 --> 00:46:30.512
So I can totally take
questions afterwards.

00:46:30.512 --> 00:46:31.720
But thank you all for coming.

00:46:31.720 --> 00:46:32.900
I hope this was fun.

00:46:32.900 --> 00:46:33.600
[APPLAUSE]

00:46:33.600 --> 00:46:36.850
And enjoy the rest
of your barbecue.

00:46:36.850 --> 00:46:39.900
[MUSIC PLAYING]

