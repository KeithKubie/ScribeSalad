WEBVTT
Kind: captions
Language: en

00:00:00.300 --> 00:00:02.560
Hi, my name is Bushger, and today we are going

00:00:02.560 --> 00:00:06.140
to talk about information theory. Now, information theory is not

00:00:06.140 --> 00:00:09.580
really a machine learning algorithm, so we'll, kind of, first

00:00:09.580 --> 00:00:12.954
understand why we need to learn information theory. Usually, you

00:00:12.954 --> 00:00:16.149
could teach a whole course on information theory, but for

00:00:16.149 --> 00:00:19.300
now, it is sufficient to know the basics. So first

00:00:19.300 --> 00:00:22.090
we'll try to understand where information theory is used in

00:00:22.090 --> 00:00:25.800
motion learning. So consider this to be any machine learning algorithm.

00:00:25.800 --> 00:00:28.940
For example, let this learner be, or a decision learner,

00:00:28.940 --> 00:00:32.189
now we have several inputs, x1, x2, x3, and one

00:00:32.189 --> 00:00:35.460
output. For simplification, let's assume that this is a regression

00:00:35.460 --> 00:00:38.380
problem. That's why we have one output. We want to

00:00:38.380 --> 00:00:41.670
ask interesting questions like how is x1 related to y,

00:00:41.670 --> 00:00:44.760
x2 related to y, x3 related to y. Why do

00:00:44.760 --> 00:00:47.526
you want to ask such questions? If you remember, from

00:00:47.526 --> 00:00:50.856
our IDT algorithm, the first step is to find out which

00:00:50.856 --> 00:00:56.830
input best splits our output. So we need to find out which of these, x1, x2,

00:00:56.830 --> 00:00:59.600
or x3 gives you the most information about

00:00:59.600 --> 00:01:02.310
why. So we have to first understand what the

00:01:02.310 --> 00:01:07.410
word information in information theory means. In general

00:01:07.410 --> 00:01:10.100
every input vector and output vector, in the

00:01:10.100 --> 00:01:12.848
machine learning context, can be considered as a

00:01:12.848 --> 00:01:16.420
probability density function. So, information theory is a mathematical

00:01:16.420 --> 00:01:20.150
framework which allows us to compare these density

00:01:20.150 --> 00:01:22.390
functions, so that we can ask interesting questions

00:01:22.390 --> 00:01:25.600
like are these input vectors similar? If they're

00:01:25.600 --> 00:01:28.000
not similar, then how different they are? And so

00:01:28.000 --> 00:01:32.740
on. We call this measure as mutual information.

00:01:32.740 --> 00:01:35.170
Or we could ask if this feature has any

00:01:35.170 --> 00:01:38.920
information at all. So we'll call this measure

00:01:38.920 --> 00:01:41.652
entropy. So we are going to find out what

00:01:41.652 --> 00:01:44.370
these terms mean, how they're related to information learning in

00:01:44.370 --> 00:01:47.880
general, and we'll briefly look at the history of this field.

