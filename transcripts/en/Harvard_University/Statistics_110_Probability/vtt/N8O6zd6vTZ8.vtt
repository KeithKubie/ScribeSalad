WEBVTT
Kind: captions
Language: en

00:00:00.080 --> 00:00:07.250
So we were talking about the exponential
distribution, and if I remember

00:00:07.250 --> 00:00:11.420
correctly we were talking about something
called the memoryless property, right?

00:00:11.420 --> 00:00:16.560
So we showed the last time that the
exponential distribution is memoryless but

00:00:16.560 --> 00:00:20.360
at this point as far as we know there
could be infinitely other memoryless

00:00:20.360 --> 00:00:21.420
distributions.

00:00:21.420 --> 00:00:27.220
So what I want to talk about now is just
kinda the fact that I find pretty amazing,

00:00:27.220 --> 00:00:30.660
which is that the exponential is
the only memoryless distribution

00:00:30.660 --> 00:00:32.100
in continuous time.

00:00:32.100 --> 00:00:34.040
In discrete time we have the geometric.

00:00:34.040 --> 00:00:38.940
So in a very deep sense, the geometric is
the discrete analog of the exponential.

00:00:38.940 --> 00:00:42.390
The exponential is the continuous
analog of the geometric.

00:00:42.390 --> 00:00:46.200
So those two distributions
are very closely related.

00:00:46.200 --> 00:00:50.830
So just to remind you what
the memoryless properties said, and

00:00:50.830 --> 00:00:54.550
also just cuz I saw some
news article recently

00:00:54.550 --> 00:00:59.140
that completely misunderstood
the concept of a life expectancy.

00:00:59.140 --> 00:01:02.820
And that's not the first
time that that happened.

00:01:02.820 --> 00:01:07.450
So basically it's a mistake of
not understanding the difference

00:01:07.450 --> 00:01:10.150
between expectation and
conditional expectation.

00:01:10.150 --> 00:01:13.480
And we haven't formally done
conditional expectation yet.

00:01:13.480 --> 00:01:17.270
But I claim that you already know
how to do conditional expectation.

00:01:17.270 --> 00:01:18.960
Because you just do expectation,

00:01:18.960 --> 00:01:21.940
except you use conditional
probabilities instead of probability.

00:01:21.940 --> 00:01:24.800
But we talked about the fact
that conditional probabilities

00:01:24.800 --> 00:01:28.530
are probabilities, so
it's completely analogous.

00:01:28.530 --> 00:01:31.090
So we will spend a lot of time
on conditional expectation

00:01:31.090 --> 00:01:33.200
later as a topic in its own right.

00:01:33.200 --> 00:01:35.024
But it's already something
that's familiar, right,

00:01:35.024 --> 00:01:37.080
just use conditional probability.

00:01:37.080 --> 00:01:39.660
Okay, so for this life expectancy thing,

00:01:39.660 --> 00:01:44.150
here's like the common misconception
that I've seen in various news articles.

00:01:45.430 --> 00:01:51.000
Last time I looked the life
expectancy in the US was 76 years for

00:01:51.000 --> 00:01:54.050
men, 81 years for women.

00:01:54.050 --> 00:01:56.640
And it's different in different
countries and whatever.

00:01:56.640 --> 00:02:00.260
And it's kind of an interesting
statistical problem.

00:02:00.260 --> 00:02:02.150
How do you actually come
up with those numbers?

00:02:02.150 --> 00:02:03.960
So I'm not saying those numbers
aren't exactly correct.

00:02:03.960 --> 00:02:06.365
I'm saying those are the latest
numbers that I've seen reported.

00:02:06.365 --> 00:02:09.930
Now how do you get the those numbers?

00:02:09.930 --> 00:02:14.300
Because in principle, you think, if you
wanna know what's the life expectancy of

00:02:14.300 --> 00:02:17.410
a baby who's born tomorrow, right,

00:02:17.410 --> 00:02:23.380
then I guess in principle what you would
is take all the babies born tomorrow,

00:02:23.380 --> 00:02:28.900
wait and wait until they all die, and
then take the average lifetime, okay?

00:02:28.900 --> 00:02:31.600
Well, first of all,
that's gonna take over 100 years, and

00:02:31.600 --> 00:02:34.020
you might want an answer now.

00:02:34.020 --> 00:02:37.830
But secondly, at some point in
time you want an answer, right?

00:02:37.830 --> 00:02:41.020
But if you only look at the ones
who've died up to that point and

00:02:41.020 --> 00:02:44.670
average those, that's gonna be
a very biased answer, right?

00:02:44.670 --> 00:02:48.440
Because you're ignoring all
the ones who have longer lifetimes.

00:02:48.440 --> 00:02:51.710
Okay, so that's an example of
what's called censored data.

00:02:51.710 --> 00:02:52.960
That's a good kind of censoring.

00:02:52.960 --> 00:02:56.800
It's censored because they're still alive.

00:02:56.800 --> 00:03:00.540
So anyway, that's a hard statistical
problem, and an interesting one.

00:03:00.540 --> 00:03:04.070
The reason I'm mentioning it now
is kind of like good news and

00:03:04.070 --> 00:03:07.390
bad news about life expectancy.

00:03:07.390 --> 00:03:12.550
So let's just assume it's 80 years for
simplicity.

00:03:12.550 --> 00:03:17.425
The mistake that I saw on this
news article is basically assuming

00:03:17.425 --> 00:03:20.594
that it's like 80 years for everyone.

00:03:20.594 --> 00:03:25.387
It was about Social Security and Medicare,
stuff like that, even still assuming

00:03:25.387 --> 00:03:29.780
80 years even for people who
are already in their 50s and 60s, okay?

00:03:29.780 --> 00:03:33.710
But the fact is,
the longer you've lived then

00:03:33.710 --> 00:03:38.186
you're expected how long you're going
to live total becomes longer, okay?

00:03:38.186 --> 00:03:44.270
So if I wanted to write that as
an equation, I would say, for

00:03:44.270 --> 00:03:49.990
example, if we let T be how
long someone's gonna live,

00:03:49.990 --> 00:03:54.981
given that that person
lives to be at least 20,

00:03:54.981 --> 00:04:01.101
that's gonna be greater than
just the expected value of T.

00:04:01.101 --> 00:04:04.660
It's kind of intuitively clear that
that's the conditional expectation.

00:04:04.660 --> 00:04:06.970
It just means given this information.

00:04:06.970 --> 00:04:10.980
And we compute our expectation based on
conditional probabilities rather than

00:04:10.980 --> 00:04:12.580
unconditional probabilities.

00:04:12.580 --> 00:04:14.430
This should be pretty intuitive, right?

00:04:14.430 --> 00:04:17.260
But the case where that would not
hold would be if everyone lives

00:04:17.260 --> 00:04:21.240
exactly the same lifespan,
then this is irrelevant information.

00:04:21.240 --> 00:04:22.840
And as soon as there's
variability in that,

00:04:22.840 --> 00:04:27.020
given that you live this long,
that's a good thing, that's the good news.

00:04:27.020 --> 00:04:31.280
The bad news is that human lifetimes
are not memoryless, right?

00:04:31.280 --> 00:04:34.455
People get older and decay with age.

00:04:34.455 --> 00:04:39.230
And so this is just to illustrate what
the memoryless property would say.

00:04:40.330 --> 00:04:46.790
If human lifetimes were memoryless, it
would say, and if the average is 80 years,

00:04:46.790 --> 00:04:51.080
then it would say, if you lived to be 20,
then your new expectation is 100, right?

00:04:51.080 --> 00:04:53.672
Because memoryless says
you're good as new, right?

00:04:53.672 --> 00:04:58.486
So no matter how long you live you
get an extra 80 years on average,

00:04:58.486 --> 00:05:01.032
and that's not true empirically.

00:05:01.032 --> 00:05:03.522
So the memoryless property would say.

00:05:03.522 --> 00:05:07.600
I'll say if memoryless,
cuz that's realistic for human beings, but

00:05:07.600 --> 00:05:09.879
it's realistic in some other applications.

00:05:11.420 --> 00:05:13.140
If memoryless, we would have,

00:05:17.921 --> 00:05:20.700
I just wanna translate what I
just said into an equation.

00:05:20.700 --> 00:05:23.160
It would say that E of T.

00:05:24.310 --> 00:05:31.110
Given T greater than 20, well, you have
those 20 years plus you're as good as new.

00:05:31.110 --> 00:05:33.680
So you get an extra E of T.

00:05:33.680 --> 00:05:36.340
That's what the memoryless
property would say, okay?

00:05:36.340 --> 00:05:38.730
So the truth is somewhere in between.

00:05:38.730 --> 00:05:42.250
And so, we actually have upper and
lower bounds at this point.

00:05:42.250 --> 00:05:46.033
It's gonna be somewhere between E(T) and
E(T)+20, okay?

00:05:46.033 --> 00:05:48.520
So that's the memoryless property.

00:05:48.520 --> 00:05:52.640
So of course, you could ask, since it's
not very realistic for human beings,

00:05:52.640 --> 00:05:54.500
why do we care so much about it?

00:05:54.500 --> 00:05:59.100
Well, first of all,
it's used a lot in different science,

00:05:59.100 --> 00:06:01.570
chemistry, physics, types of applications.

00:06:03.400 --> 00:06:08.490
See, sometimes using economics, in some
application basically it's realistic

00:06:08.490 --> 00:06:14.140
in problems where things
don't decay with age.

00:06:14.140 --> 00:06:17.060
Or another way of thinking of it like
there's a homework problem about doing

00:06:17.060 --> 00:06:18.099
homework problems.

00:06:19.330 --> 00:06:22.410
And there are sort of two sorts of
homework problems, roughly speaking.

00:06:22.410 --> 00:06:25.770
One is a type where just like you
have to do a certain calculation,

00:06:25.770 --> 00:06:28.450
and you do the calculation,
and then you're done.

00:06:28.450 --> 00:06:31.100
And it sort of takes
a fixed amount of time.

00:06:31.100 --> 00:06:33.660
Or at least that you can make
partial progress, right?

00:06:33.660 --> 00:06:36.840
And then there's another type of homework
problem where you could just stare at it

00:06:36.840 --> 00:06:39.480
for hours and
have absolutely nothing, right?

00:06:39.480 --> 00:06:42.540
And then, at some point, eventually,
you have to be very determined,

00:06:42.540 --> 00:06:43.780
very persistent.

00:06:43.780 --> 00:06:47.250
At some point, you get this a-ha moment,
you get the breakthrough, and

00:06:47.250 --> 00:06:49.010
you get it, okay?

00:06:49.010 --> 00:06:52.100
So memory lets you restore,
like you can't make partial progress.

00:06:52.100 --> 00:06:55.180
Either you get it eventually,
or you don't.

00:06:55.180 --> 00:06:59.308
Whereas another type of problem you saw
is more of a fixed gradual progress,

00:06:59.308 --> 00:07:02.090
progress, progress until
we finish the problem.

00:07:03.700 --> 00:07:06.160
Okay, so
there are cases where it's realistic.

00:07:06.160 --> 00:07:09.670
But the other big reason for
studying it is that it's a building block.

00:07:09.670 --> 00:07:14.200
So if you go in and study what
distribution do people actually use

00:07:14.200 --> 00:07:18.850
in practice for
something like this T survival time?

00:07:18.850 --> 00:07:23.590
The most popular distribution that's used
in practice is what's called the Weibull.

00:07:23.590 --> 00:07:26.870
And you don't need to know Weibulls right
now, but just to mention it right now,

00:07:26.870 --> 00:07:31.580
a Weibull is just obtained by
taking a exponential to a power.

00:07:31.580 --> 00:07:35.470
And as soon as if you take an exponential,
random variable and then cube it, that's

00:07:35.470 --> 00:07:38.090
not going to be exponential anymore,
it's not going to be memoryless anymore.

00:07:38.090 --> 00:07:39.900
That's called a Weibull.

00:07:39.900 --> 00:07:41.400
It's not gonna be memoryless anymore.

00:07:41.400 --> 00:07:44.050
And it actually turns out
to be extremely useful.

00:07:44.050 --> 00:07:47.860
So exponentials are a crucial
building block.

00:07:47.860 --> 00:07:51.376
But in some cases memoryless is
not an unreasonable assumption or

00:07:51.376 --> 00:07:56.230
it may be a reasonable approximation
even if it's not exactly true.

00:07:56.230 --> 00:07:59.074
Okay, so that's just the intuition
of the memoryless property.

00:07:59.074 --> 00:08:02.154
We proved that it was true last time for
the exponential distribution.

00:08:02.154 --> 00:08:05.566
But now, let's show that it's only
the exponential distribution that has

00:08:05.566 --> 00:08:06.760
that property.

00:08:06.760 --> 00:08:10.622
So I'll state that as a theorem.

00:08:10.622 --> 00:08:17.280
So if X is a continuous random variable,

00:08:19.712 --> 00:08:23.641
And we're thinking about applications
like lifetime, survival time.

00:08:23.641 --> 00:08:31.951
So we're thinking of positive-valued
random variables, so I'll say positive.

00:08:31.951 --> 00:08:38.203
That is, it takes values from zero to
infinity, with the memoryless property,

00:08:41.763 --> 00:08:44.683
So the memoryless property is
a property of the distribution,

00:08:44.683 --> 00:08:47.050
not of the random variable itself, per se.

00:08:47.050 --> 00:08:50.890
But we would say the random variable
has the memoryless property if its

00:08:50.890 --> 00:08:52.610
distribution has the memoryless property.

00:08:54.130 --> 00:08:57.999
Okay, and then the claim is just
that it has to be exponential.

00:09:00.780 --> 00:09:03.679
So with exponential lambda for
some lambda.

00:09:10.121 --> 00:09:13.650
So this is a characterization
of the exponential distribution.

00:09:15.110 --> 00:09:19.441
Okay, so
now let's try to prove this result.

00:09:19.441 --> 00:09:22.801
And it's kind of an unusual proof,

00:09:22.801 --> 00:09:27.171
compared to most that
you've probably seen.

00:09:27.171 --> 00:09:31.145
Because we're gonna write down
an equation, but we're solving for

00:09:31.145 --> 00:09:33.690
a function, not solving for the variable.

00:09:33.690 --> 00:09:36.700
So it's what's called
a functional equation, okay?

00:09:36.700 --> 00:09:41.580
So let's let F be

00:09:41.580 --> 00:09:47.513
the CDF, as usual.

00:09:47.513 --> 00:09:50.663
But as we saw last time with
the exponential distribution,

00:09:50.663 --> 00:09:54.191
it's easier to think in terms of
1- CDF for this kind of problem.

00:09:54.191 --> 00:09:57.480
Because that's the probability
of surviving longer than time T.

00:09:58.630 --> 00:10:04.015
So that's the CDF of X, and let's say

00:10:04.015 --> 00:10:09.249
G(x) = P(x &gt; x) = 1- F(x).

00:10:09.249 --> 00:10:16.740
So it's easier to do this
problem in terms of G(x).

00:10:16.740 --> 00:10:24.501
Now, the memoryless property says, Says,

00:10:24.501 --> 00:10:30.086
in terms of G, it's easy to write.

00:10:32.230 --> 00:10:35.785
It's just the equation
G(s + t) = G(s) G(t).

00:10:35.785 --> 00:10:38.862
And we saw this, I'm not gonna
repeat the argument for this.

00:10:38.862 --> 00:10:41.087
Because, the same thing
as we did last time for

00:10:41.087 --> 00:10:42.982
the specific case of the exponential.

00:10:42.982 --> 00:10:43.815
Just write down,

00:10:43.815 --> 00:10:47.210
memoryless property is defined in
terms of a conditional probability.

00:10:47.210 --> 00:10:50.284
But just write down the definition
of conditional probability, and

00:10:50.284 --> 00:10:52.301
just in one line you can
rewrite it like this.

00:10:52.301 --> 00:10:55.001
Notice this is true for
the exponential, right,

00:10:55.001 --> 00:10:59.320
because e to the -(s + t) is (e
to the -s)(e to the -t), okay?

00:10:59.320 --> 00:11:02.910
We basically want to show that
this is not like a usual equation,

00:11:02.910 --> 00:11:06.150
where we're trying to solve for s or
solve for t or something like that.

00:11:06.150 --> 00:11:07.212
We're trying to solve for G,

00:11:07.212 --> 00:11:13.170
as we want to show that only exponential
functions can satisfy this identity, okay?

00:11:14.420 --> 00:11:19.880
So, the way to approach this
kind of equation is just to

00:11:19.880 --> 00:11:24.900
start plugging some stuff in, and
try to learn more and more about G.

00:11:24.900 --> 00:11:31.062
So we're trying to solve for G,
And it's not like something where,

00:11:31.062 --> 00:11:34.990
just plug it into the quadratic formula or
something, and then you just solve for G.

00:11:34.990 --> 00:11:37.180
We're trying to solve for
this function G, so

00:11:37.180 --> 00:11:43.090
we just got to try to gradually learn
more and more stuff about G, right?

00:11:43.090 --> 00:11:45.630
Okay, so the first thing I can think of,
so this has to be true for

00:11:45.630 --> 00:11:47.850
all positive numbers s and t, okay?

00:11:47.850 --> 00:11:51.020
So, let's see, what can we learn about G?

00:11:51.020 --> 00:11:55.032
Well, one thing I can do is let s = t,
just to see what that says.

00:11:55.032 --> 00:11:58.571
Okay, so we can choose s and

00:11:58.571 --> 00:12:03.790
t to be whatever we want, so we may as
well derive some consequences of this.

00:12:03.790 --> 00:12:08.603
So one choice would be let s = t, and

00:12:08.603 --> 00:12:13.740
then that says that G(2t) = G(t)

00:12:13.740 --> 00:12:18.670
squared, I just rewrote that.

00:12:18.670 --> 00:12:25.482
Okay, so that's nice to know, what else
can we see, well, let's try G(3t).

00:12:25.482 --> 00:12:29.647
G(3t), well, I could replace s by 2t,

00:12:29.647 --> 00:12:35.690
that would be G(3t), G(3t) = G(2t) G(t).

00:12:35.690 --> 00:12:39.638
But we know G(2t) = G(t) squared,
so this is G(t) cubed.

00:12:42.651 --> 00:12:45.526
And you can keep repeating that,
formally by induction,

00:12:45.526 --> 00:12:49.070
by just repeat a few of them and
you immediately see the pattern,OK ay?

00:12:49.070 --> 00:12:55.260
So we immediately have
the G(kt) = G(t) to the k,

00:12:55.260 --> 00:12:59.930
If k is a positive integer, so
that seems like a useful property to know.

00:13:01.310 --> 00:13:02.990
Now what we went the other way around?

00:13:04.060 --> 00:13:07.893
What if we want to know what's not G(2t),
but G(t/2)?

00:13:10.439 --> 00:13:14.559
Well actually, if I take this equation and
replace t by t/2,

00:13:14.559 --> 00:13:18.309
cuz this is true for all t's,
so I can plug in t/2 for t.

00:13:18.309 --> 00:13:23.600
That's G(t/2) and that's G(t),
take the square root of both sides.

00:13:23.600 --> 00:13:27.352
Then we get the G(t/2) is
the square root of G(t).

00:13:27.352 --> 00:13:32.415
Similarly, G(t/3)

00:13:32.415 --> 00:13:40.133
is the cube root of G(t), and so on.

00:13:40.133 --> 00:13:44.011
So now, we've figured out that
this equation is true if k is

00:13:44.011 --> 00:13:48.990
a positive integer, or if k is the
reciprocal of a positive integer, okay?

00:13:48.990 --> 00:13:53.455
Well then the next step would be,
what if k is a rational number?

00:13:53.455 --> 00:13:59.424
That is, a rational number by
definition is just a ratio of integers,

00:13:59.424 --> 00:14:02.366
so let's say we have G(m/n t).

00:14:02.366 --> 00:14:07.400
Well, if we just apply these two
properties, that this is true.

00:14:07.400 --> 00:14:12.692
All right,
this one has G(t/k) = G(t) to the 1/k,

00:14:12.692 --> 00:14:16.420
where k is a positive integer.

00:14:16.420 --> 00:14:21.314
We apply these two properties,
then we immediately

00:14:21.314 --> 00:14:25.763
get that if we multiply
by any rational number,

00:14:25.763 --> 00:14:30.562
the same thing holds,
that's G(t) to the m/n.

00:14:30.562 --> 00:14:34.303
So then this m over n is
any rational number, okay?

00:14:34.303 --> 00:14:36.661
Now, if we have any real number,

00:14:36.661 --> 00:14:42.330
we can always treat a real number as
a limit of rational numbers, right?

00:14:42.330 --> 00:14:46.920
Like pi, you could approximate pi,
you could say that pi is the limit of

00:14:46.920 --> 00:14:50.910
the sequence, 3, 3,1,
3.14 and so on, all right?

00:14:52.070 --> 00:14:56.489
So you can pick a sequence of rational
numbers that converges to pi or

00:14:56.489 --> 00:14:57.893
any number you want.

00:14:57.893 --> 00:14:59.930
So just take the limit of both sides.

00:14:59.930 --> 00:15:05.546
Imagine, replace this m/n by
a sequence of rational numbers,

00:15:05.546 --> 00:15:08.152
take the limit of both sides.

00:15:08.152 --> 00:15:11.031
We're using the fact that
capital G is continuous.

00:15:11.031 --> 00:15:14.169
Continuous means that if we take
the limit of something like this,

00:15:14.169 --> 00:15:15.692
you can swap the limit and the G.

00:15:15.692 --> 00:15:20.790
So by continuity, this is true for
any positive real numbers.

00:15:20.790 --> 00:15:26.158
Let's say, G(xt) = G(t)

00:15:26.158 --> 00:15:31.010
to the x for any real x &gt; 0.

00:15:31.010 --> 00:15:36.222
Just by taking the limit of rational
numbers, we can get real numbers.

00:15:36.222 --> 00:15:42.044
Okay, now we're basically done,
because this is true for all x and t.

00:15:42.044 --> 00:15:46.974
So to simplify it,
let's just let t equal 1.

00:15:48.361 --> 00:15:55.070
And if we let t = 1,
this just says that G(x) = G(1) to the x.

00:15:57.430 --> 00:15:59.340
That looks like an exponential function.

00:15:59.340 --> 00:16:02.700
In particular,
let's write it in terms of base E.

00:16:02.700 --> 00:16:10.051
That's the same thing
as E to the X log G(1).

00:16:10.051 --> 00:16:13.370
So this thing, now G(1) is a probability.

00:16:13.370 --> 00:16:16.460
So G(1) is clearly between 0 and 1.

00:16:16.460 --> 00:16:18.770
If you take the log of
a number between 0 and 1,

00:16:18.770 --> 00:16:23.010
you'll just get some negative real number,
so this is just some negative number here.

00:16:23.010 --> 00:16:28.070
So we could call this thing -lambda,
but a lambda is a positive number.

00:16:29.920 --> 00:16:31.410
This is just a constant, right, so

00:16:31.410 --> 00:16:35.320
I'm just calling it -lambda,
it happens to be a negative constant.

00:16:35.320 --> 00:16:39.990
So that's just e to the -lambda x,
which is 1- the CDF for

00:16:39.990 --> 00:16:43.080
the exponential, so
that's the only possibility, okay?

00:16:43.080 --> 00:16:47.270
So exponential is the only
continuous memoryless distribution.

00:16:47.270 --> 00:16:50.520
That's the proof of that fact, okay?

00:16:52.460 --> 00:16:55.609
So, we'll use the memoryless property and

00:16:55.609 --> 00:17:00.840
more stuff with the exponential
distribution from time to time.

00:17:00.840 --> 00:17:07.995
But there's another kind of key
tool that we need at this point.

00:17:07.995 --> 00:17:10.912
You'll need it for the homework,
and you'll need it in general, and

00:17:10.912 --> 00:17:13.000
that's called the moment
generating function.

00:17:14.550 --> 00:17:16.880
So let's talk about moment
generating functions.

00:17:18.890 --> 00:17:24.840
Moment generating function,
which sometimes seems mysterious at first.

00:17:24.840 --> 00:17:28.690
But if you think carefully about what it
means, you'll see why it's useful and

00:17:28.690 --> 00:17:30.550
what it actually means.

00:17:30.550 --> 00:17:34.017
Moment generating function,
which we abbreviate to MGF,

00:17:36.545 --> 00:17:41.820
Is just another way of describing
a distribution rather than CDFs and PDF.

00:17:41.820 --> 00:17:48.140
MGF is another alternative way
to describe a distribution.

00:17:48.140 --> 00:17:49.961
So the definition is that,

00:17:53.165 --> 00:17:57.828
A random variable x has

00:17:57.828 --> 00:18:03.228
MGF M(t) = the expected

00:18:03.228 --> 00:18:08.393
value of (e to the tx).

00:18:11.149 --> 00:18:13.229
This is as a function of t.

00:18:19.298 --> 00:18:24.894
And we say that it exists,
this is not a useful concept unless

00:18:24.894 --> 00:18:30.295
this thing is actually finite
on some interval around 0.

00:18:30.295 --> 00:18:36.719
So we would just say if this
is finite on some interval,

00:18:36.719 --> 00:18:42.620
let's say, -a to a,
where a is greater than 0.

00:18:42.620 --> 00:18:47.045
It could be that this thing is finite for
all numbers to you, which is great.

00:18:47.045 --> 00:18:52.003
But we're not requiring that to be true,
but we are requiring at least having

00:18:52.003 --> 00:18:55.911
at least some tiny interval
about 0 on which this is finite.

00:18:55.911 --> 00:19:00.200
All right, well, this definition at first
seems to come out of nowhere, I think.

00:19:00.200 --> 00:19:03.440
And students sometimes really wonder,
what's t?

00:19:03.440 --> 00:19:05.733
What does t mean, okay?

00:19:05.733 --> 00:19:09.686
The first thing to understand about this
is that this letter t is just a dummy

00:19:09.686 --> 00:19:11.760
variable, right?

00:19:11.760 --> 00:19:15.899
Conventionally, we call it t, but we could
could have called that s or q or w or

00:19:15.899 --> 00:19:19.939
anything else that wouldn't clash
with the rest of the notation, right?

00:19:19.939 --> 00:19:23.757
I wouldn't have called it E or
M or capital X, but

00:19:23.757 --> 00:19:27.220
anything that doesn't clash is fine.

00:19:27.220 --> 00:19:35.170
So t is just a placeholder, Secondly
what does this thing actually mean?

00:19:35.170 --> 00:19:41.340
Well, this is a well defined thing
because for any number t, we talked

00:19:41.340 --> 00:19:45.140
many times about the fact that a function
of a random variable is a random variable.

00:19:45.140 --> 00:19:48.020
So that's a function of a random variable,
that's a random variable.

00:19:48.020 --> 00:19:49.820
We can look at its expectation.

00:19:49.820 --> 00:19:54.660
That doesn't yet show why we would want
to do that, but we can do that, right?

00:19:54.660 --> 00:19:58.137
So this is a function of t,
that is for any number t, we could

00:19:58.137 --> 00:20:02.677
imagine computing this expectation,
so it's a well defined function of t.

00:20:02.677 --> 00:20:06.710
It might be that it's infinity for
some values of t, okay?

00:20:06.710 --> 00:20:09.730
But at least you know it's something
we can write down and study.

00:20:10.870 --> 00:20:14.700
Okay, so think of t is kind of
like a book keeping device.

00:20:16.530 --> 00:20:18.000
All the MGF is,

00:20:18.000 --> 00:20:22.130
is a fancy book keeping device for keeping
track of the moments of a distribution.

00:20:23.450 --> 00:20:25.500
So let's see why that's true.

00:20:26.800 --> 00:20:29.745
So why is it called moment generating?

00:20:32.679 --> 00:20:36.915
Well, to see why it's called that,
all we have to do is say,

00:20:36.915 --> 00:20:41.461
we have e to a power here, let's use
the Taylor series for e to a power,

00:20:41.461 --> 00:20:45.250
right, which we've been using many times.

00:20:45.250 --> 00:20:49.010
So if we expand this thing out,
expected value of e to the tx,

00:20:49.010 --> 00:20:53.770
that's the expected value,
just the Taylor series for e to the x.

00:20:53.770 --> 00:20:59.260
So that's x to the n, t to the n
over n factorial, n= 0 to infinity.

00:21:02.410 --> 00:21:06.470
This is always valid cuz the Taylor series
for e to the x converges everywhere.

00:21:06.470 --> 00:21:09.090
x is a random variable,
but this is always true.

00:21:09.090 --> 00:21:11.630
So that's a valid expansion.

00:21:11.630 --> 00:21:17.410
Now intuitively at this point,
we wanna swap the E and the sum.

00:21:17.410 --> 00:21:22.285
And that's where some technical
conditions come in, in particular,

00:21:22.285 --> 00:21:27.826
that's where this fact matters that we
have this interval, n=0 to infinity.

00:21:27.826 --> 00:21:32.857
So just suppose for a second that
we can swap this sum and the e.

00:21:32.857 --> 00:21:35.825
Then we would get this thing,

00:21:35.825 --> 00:21:41.412
expected value of x to the n,
t to the n over n factorial.

00:21:41.412 --> 00:21:45.657
This thing here, E(x to the n),
is called the nth moment.

00:21:45.657 --> 00:21:47.620
So the first moment is the mean.

00:21:47.620 --> 00:21:51.526
The second moment is not the variance
unless the mean is 0, but the second

00:21:51.526 --> 00:21:55.633
moment and the first moment are what
we need to compute the variance, right?

00:21:55.633 --> 00:21:58.395
And then there are higher moments,
that's called the nth moment.

00:22:00.305 --> 00:22:03.900
And higher moments have different
interpretations that are more complicated

00:22:03.900 --> 00:22:07.340
than mean and variance, but they turn
out to be useful for a lot of reasons.

00:22:08.530 --> 00:22:10.880
So assuming that we can do this swap,

00:22:10.880 --> 00:22:15.540
bringing the E inside the sum, then
what we've really done is just capture

00:22:15.540 --> 00:22:20.076
all the moments of x into this
Taylor series, all right?

00:22:20.076 --> 00:22:23.010
But that's why it's called the moment
generating function cuz you see all

00:22:23.010 --> 00:22:25.260
the moments are just sitting
there in the Taylor series.

00:22:26.770 --> 00:22:31.680
As far as showing why you can swap the E
and the sum, if this were a finite sum,

00:22:31.680 --> 00:22:34.610
that would just be immediately
true by linearity, right?

00:22:34.610 --> 00:22:39.525
Since it's an infinite sum, that requires
more justification, and for that kind of

00:22:39.525 --> 00:22:44.730
justification, either you need to take a
certain real analysis course, or step 210.

00:22:44.730 --> 00:22:49.500
So we need some more analysis and
math to do that.

00:22:49.500 --> 00:22:55.040
But this is valid,
under fairly mild assumption

00:22:56.120 --> 00:22:59.450
that this exists on some
interval like that.

00:22:59.450 --> 00:23:00.550
So it turns out that we can,

00:23:00.550 --> 00:23:05.302
it's kind of like an infinite
version of linearity, right?

00:23:05.302 --> 00:23:08.215
Swap the E and the sum,
and we get that thing,

00:23:08.215 --> 00:23:12.077
okay, so that's called
the moment generating function.

00:23:12.077 --> 00:23:13.371
All right, now,

00:23:13.371 --> 00:23:19.077
I guess this shows that it would be useful
if we were interested in the moments.

00:23:19.077 --> 00:23:22.095
But what if we don't
care about the moments?

00:23:22.095 --> 00:23:25.320
I mean, usually we might care about
the mean and the variance, but

00:23:25.320 --> 00:23:28.560
we haven't yet worried that much
about higher moments than that.

00:23:29.580 --> 00:23:34.368
So let me just tell you three
reasons why the MGF is important.

00:23:38.293 --> 00:23:41.877
Okay, so why is the MGF important?

00:23:46.069 --> 00:23:48.360
Well, the first reason is the moments.

00:23:48.360 --> 00:23:49.785
That's what we just talked about.

00:23:49.785 --> 00:23:52.460
Cuz sometimes we do want the moments.

00:23:52.460 --> 00:23:58.220
So if x, so we're gonna let x have MGF.

00:23:59.882 --> 00:24:03.948
Capital M(t).

00:24:03.948 --> 00:24:07.245
If necessary for clarity,
we might subscript the x here.

00:24:07.245 --> 00:24:10.124
But right now, we're just talking
about one random variable.

00:24:10.124 --> 00:24:13.996
And here is it's MGF, so
we don't need a subscript.

00:24:13.996 --> 00:24:17.594
So the nth moment.

00:24:19.743 --> 00:24:27.215
That is, E(x to the n) is,
there's two ways to think of it.

00:24:27.215 --> 00:24:30.675
The nicer way is that
it's the coefficient.

00:24:32.645 --> 00:24:38.423
Of t to the n over n factorial
in the Taylor expansion.

00:24:42.013 --> 00:24:46.790
Of M about zero,
from Maclaurin series if you like.

00:24:46.790 --> 00:24:49.720
That's what we just showed over there,
assuming we could do that swap.

00:24:51.450 --> 00:24:52.950
Another way to say this,

00:24:52.950 --> 00:24:57.470
just remember from your basic Taylor
series from integral calculus that

00:24:59.910 --> 00:25:03.790
if you have some functions and you wanna
compute its Taylor series, what do you do?

00:25:03.790 --> 00:25:05.920
You take the sum like this.

00:25:05.920 --> 00:25:07.783
And right here in the circle place,

00:25:07.783 --> 00:25:10.525
you put the nth derivative evaluated at 0,
right?

00:25:10.525 --> 00:25:15.348
That's just how you do Taylor series,
right, take derivatives evaluated at 0.

00:25:15.348 --> 00:25:20.207
So another way to say it is
that it's the nth derivative,

00:25:20.207 --> 00:25:23.050
which I'll write like that.

00:25:23.050 --> 00:25:24.706
So if we want the first moment,

00:25:24.706 --> 00:25:28.287
we could take the first derivative,
evaluate it at 0, and so on.

00:25:31.493 --> 00:25:37.120
Okay, so the coefficient,
and it is this thing.

00:25:38.460 --> 00:25:42.940
I'll just write that =E(x to
the n) again just for emphasis.

00:25:42.940 --> 00:25:44.998
So to get the nth moment,

00:25:44.998 --> 00:25:49.904
we could take the nth derivative
of the MGF evaluated at 0.

00:25:49.904 --> 00:25:53.631
But as we'll see, sometimes it's
a lot easier to just directly

00:25:53.631 --> 00:25:56.470
work out the Taylor series
by some other method.

00:25:56.470 --> 00:26:00.250
For example, we already know the Taylor
series for E to the x, right?

00:26:00.250 --> 00:26:04.610
Rather than going through take derivative,
take derivative, take derivative.

00:26:04.610 --> 00:26:06.710
Just write down the Taylor series.

00:26:06.710 --> 00:26:08.750
Okay, so that's the first reason.

00:26:08.750 --> 00:26:12.240
Second reason,
which is probably even more important,

00:26:12.240 --> 00:26:17.487
the other two reasons are more important
even if we don't care about moments, okay?

00:26:17.487 --> 00:26:20.937
The second reason is that the MGF
determines the distribution.

00:26:28.786 --> 00:26:31.026
So another way to say that would be.

00:26:34.250 --> 00:26:37.553
If you have two random variables, X and
Y, and they both have the same MGF,

00:26:37.553 --> 00:26:39.500
then they must have the same distribution.

00:26:41.650 --> 00:26:44.250
So if X and Y have the same MGF,

00:26:44.250 --> 00:26:49.872
then they have the same distribution,
for example, same CDF.

00:26:49.872 --> 00:26:54.456
If they're continuous,
they have the same PDF and so on.

00:26:54.456 --> 00:26:58.555
Then, they have the same CDF.

00:27:00.649 --> 00:27:04.370
This fact is very difficult to prove.

00:27:04.370 --> 00:27:05.910
So I'm not gonna try to prove this.

00:27:05.910 --> 00:27:08.075
But it's useful to know.

00:27:08.075 --> 00:27:13.095
If you compute some MGF, and
you recognize, that's a Poisson of 3 MGF,

00:27:13.095 --> 00:27:18.320
then you can conclude that that's
a Poisson of three random variables.

00:27:18.320 --> 00:27:23.174
There isn't some other distribution that
kind of pretends to be a Poisson of 3

00:27:23.174 --> 00:27:24.726
at the same MGF, right?

00:27:24.726 --> 00:27:29.550
Once you know the MGF, you know
the distribution, at least in principle.

00:27:29.550 --> 00:27:33.580
Okay, and then the other reason why
they're important aside from 1 and 2,

00:27:33.580 --> 00:27:37.400
is that they make the sums much,
much easier to handle.

00:27:38.710 --> 00:27:41.600
So we've dealt a little bit with
sums of random variables before and

00:27:41.600 --> 00:27:43.780
we'll deal more with it later.

00:27:43.780 --> 00:27:49.200
In general, finding the distribution of
a sum of independent random variables

00:27:49.200 --> 00:27:53.930
is complicated,
that's called a convolution.

00:27:53.930 --> 00:27:58.955
But if we have access to MGFs,
things are a lot easier.

00:27:58.955 --> 00:28:01.650
Convolution, you have to
do this convolution sum or

00:28:01.650 --> 00:28:04.540
convolution integral which
we'll deal with somewhat later.

00:28:04.540 --> 00:28:06.420
We've done a little bit of it before.

00:28:06.420 --> 00:28:07.120
It's complicated.

00:28:08.290 --> 00:28:09.624
So suppose we have MGFs.

00:28:09.624 --> 00:28:14.850
So let's say, if x has

00:28:14.850 --> 00:28:20.633
MGF Mx and if Y has MGF My.

00:28:22.740 --> 00:28:24.538
And they're independent.

00:28:27.122 --> 00:28:30.990
Then we want the MGF of the sum.

00:28:30.990 --> 00:28:33.979
A lot of times, we're interested in
sums of random variables, right,

00:28:33.979 --> 00:28:35.890
just adding things up,
comes up all the time.

00:28:35.890 --> 00:28:40.512
Then the MGF of X+Y.

00:28:45.457 --> 00:28:51.882
Or by definition, it's the expected
value of e to the t(x+y).

00:28:51.882 --> 00:28:53.997
That's e to the t(x+y).

00:28:56.046 --> 00:28:57.610
And we haven't proven this yet.

00:28:57.610 --> 00:29:00.230
But this is another fact
that we're gonna prove soon.

00:29:00.230 --> 00:29:04.622
That if we have expected value of
two things, a product of things that

00:29:04.622 --> 00:29:09.410
are independent, then they're
the product of the expectations, okay?

00:29:10.440 --> 00:29:11.420
So we haven't shown that yet.

00:29:11.420 --> 00:29:17.520
That would be false in general,
if they are not independent, so it's this.

00:29:17.520 --> 00:29:20.358
Crucial that we have independence or

00:29:20.358 --> 00:29:24.852
some other condition here,
but we'll prove that later.

00:29:24.852 --> 00:29:28.490
I'm writing this as e to
the t X times e to the t Y.

00:29:28.490 --> 00:29:33.460
Since X and Y are independent, then, e to
the t X and e to the t Y are independent.

00:29:34.500 --> 00:29:37.420
And according to that fact,

00:29:37.420 --> 00:29:41.630
then I can write e of the product of
the product of e, using that assumption.

00:29:41.630 --> 00:29:45.795
But then notice that that's
just the product of MGFs.

00:29:45.795 --> 00:29:52.078
So that's Mx of t by definition,
Mx, My of t.

00:29:52.078 --> 00:29:56.477
So this is really simple in the sense
that if we have both of those MGFs,

00:29:56.477 --> 00:29:58.690
we just multiply them, right?

00:29:58.690 --> 00:29:59.726
We didn't have to do an integral.

00:29:59.726 --> 00:30:01.830
We didn't have to do some complicated sum.

00:30:01.830 --> 00:30:03.890
We just multiplied the two MGFs.

00:30:03.890 --> 00:30:06.860
So that's really convenient, okay?

00:30:06.860 --> 00:30:08.693
Let's do a couple quick examples.

00:30:10.979 --> 00:30:14.178
Of MGFs for specific distributions.

00:30:17.259 --> 00:30:21.540
So the easiest example to start
with is Bernoulli, right?

00:30:21.540 --> 00:30:26.200
Bernoulli is the easiest,
simplest distribution.

00:30:26.200 --> 00:30:27.835
So let's just start with the Bernoulli.

00:30:30.499 --> 00:30:36.313
So if X is Bern(p) Then

00:30:36.313 --> 00:30:41.700
the MGF M(t) = E(e the tx).

00:30:42.790 --> 00:30:47.490
Now we could use LOTUS, but we don't even
need LOTUShere, because we could just say

00:30:48.930 --> 00:30:52.540
either the tx, X is just 0 or 1.

00:30:52.540 --> 00:30:59.476
So e to the tx is either e to the t or
1, right, only two possibilities.

00:30:59.476 --> 00:31:04.458
The probabilities of those two cases are,
with probably p, it's e to the t.

00:31:07.861 --> 00:31:15.540
Pe to the t, and with probability q,
it's 1, where q is 1-p as usual.

00:31:15.540 --> 00:31:18.390
So I'm just stating a weighted
average of the two possible values,

00:31:18.390 --> 00:31:20.210
it's either e to the t or it's 1.

00:31:20.210 --> 00:31:21.980
So just take a weighted average.

00:31:21.980 --> 00:31:24.587
So that's really easy calculation.

00:31:24.587 --> 00:31:31.143
But because of that, now we can
immediately get the MGF of a binomial.

00:31:32.920 --> 00:31:34.202
Because.

00:31:38.905 --> 00:31:40.410
Well, if we write down the definition,

00:31:40.410 --> 00:31:42.550
then we know we're gonna have
to do some big LOTUS thing.

00:31:42.550 --> 00:31:46.140
But we don't have to do that
because if we think of the binomial

00:31:46.140 --> 00:31:49.980
as the sum of iid Bernoulli p,
then you use fact 3 there.

00:31:51.190 --> 00:31:55.141
Then we immediately know that the M(t)

00:31:55.141 --> 00:32:00.520
equals the MGF of
a Bernoulli To the nth power.

00:32:01.740 --> 00:32:07.153
So we just write it down immediately just
by taking the nth power of that, okay?

00:32:07.153 --> 00:32:11.650
So for practice you could check
remember the binomial has

00:32:11.650 --> 00:32:16.720
the mean of the binomial,
NP is NP and the variants is NPQ.

00:32:16.720 --> 00:32:20.120
And if you wanna check that
statement one there is true,

00:32:20.120 --> 00:32:24.980
you could take this thing and check that
the first derivative, evaluate it at 0,

00:32:24.980 --> 00:32:27.840
you get the mean, second derivative,
evaluate it at 0,

00:32:27.840 --> 00:32:30.480
you'll get the second moment,
from there you get the variance.

00:32:31.760 --> 00:32:34.010
Okay, so that's the binomial.

00:32:34.010 --> 00:32:37.282
One more that we need
right now is the normal.

00:32:37.282 --> 00:32:41.865
And that's a bit more
involved of a calculation.

00:32:41.865 --> 00:32:44.488
So let's let z be standard normal.

00:32:48.143 --> 00:32:52.219
Notice that once we have the MGF of the
standard normal, then we know the MGF of

00:32:52.219 --> 00:32:56.360
any normal we want, cuz remember
this thing about location and scale.

00:32:56.360 --> 00:33:01.061
So once we know this answer MGF,
then we can take any normal we want

00:33:01.061 --> 00:33:05.865
then go mu plus sigma z figure out
its MGF in a straightforward way.

00:33:05.865 --> 00:33:09.060
And that's good practice to make
sure you know how to do that.

00:33:09.060 --> 00:33:12.734
So let's just talk about the standard
normal first cuz that's simpler.

00:33:12.734 --> 00:33:15.081
And we wanna compute the MGF.

00:33:17.701 --> 00:33:24.103
So by LOTUS, this is 1 over square root of
2pi integral minus infinity to infinity.

00:33:24.103 --> 00:33:27.417
So we want the expected
value of e over tz.

00:33:27.417 --> 00:33:32.130
So by LOTUS it's e over tz, and then
we just have to write down the standard

00:33:32.130 --> 00:33:35.874
normal density,
which is e to the minus z squared over 2.

00:33:35.874 --> 00:33:38.183
I already put the normalizing
constant out there.

00:33:38.183 --> 00:33:42.277
So this is minus z squared over 2 dz.

00:33:44.228 --> 00:33:46.988
Now looks like a pretty nasty integral,
but

00:33:46.988 --> 00:33:51.023
we will bravely try to do it anyway
cuz you wanna know the answer.

00:33:53.630 --> 00:33:55.728
So that was kind of nasty.

00:33:55.728 --> 00:33:58.364
Now if t is 0, then that's just 1,

00:33:58.364 --> 00:34:04.640
because then it's just saying integrate
the standard normal PDF, we'd get one.

00:34:04.640 --> 00:34:07.260
So we know how to do this
without this linear, right?

00:34:07.260 --> 00:34:10.090
We have a linear term,
we have a quadratic term.

00:34:10.090 --> 00:34:13.550
It's only the linear term
that's annoying here, right?

00:34:13.550 --> 00:34:16.850
Without the linear term,
this is just really easy.

00:34:16.850 --> 00:34:18.340
It's not easy, but it's one that we did.

00:34:18.340 --> 00:34:21.207
So it's easy given what we've done before.

00:34:21.207 --> 00:34:23.587
So the question is just how to
get rid of the linear term.

00:34:23.587 --> 00:34:27.430
Well then you have to think like all
the way, way back to algebra class.

00:34:27.430 --> 00:34:29.350
And how do you solve quadratic formulas,

00:34:29.350 --> 00:34:32.240
stuff like that,
completing the square, right?

00:34:32.240 --> 00:34:37.406
Something you probably thought you'd never
see again, because once you know like

00:34:37.406 --> 00:34:42.662
the quadratic formula, you rarely bother
to complete the square anymore, right?

00:34:42.662 --> 00:34:45.912
But that's what we need here
because if we complete the square,

00:34:45.912 --> 00:34:50.420
then it's gonna look like a quadratic and
not look like a linear thing anymore.

00:34:50.420 --> 00:34:53.174
Okay, so
we just need a little bit of algebra.

00:34:53.174 --> 00:34:56.295
So this is e to the minus one-half.

00:34:56.295 --> 00:34:59.419
Let's see if I can complete
the square correctly.

00:34:59.419 --> 00:35:02.040
And then we can factor
out a minus one-half.

00:35:02.040 --> 00:35:04.150
We attempt to complete the square.

00:35:04.150 --> 00:35:08.359
Well, it's gonna be z-t squared, I think.

00:35:08.359 --> 00:35:10.771
Then I have to adjust it.

00:35:10.771 --> 00:35:12.885
All right,
I'm just trying to do some algebra here.

00:35:12.885 --> 00:35:16.732
And it's hard to do algebra on a board,
but let's try.

00:35:16.732 --> 00:35:19.920
So we have this thing, and
then we need to fix it.

00:35:19.920 --> 00:35:22.460
But let's see if this gets at
least the beginning right.

00:35:22.460 --> 00:35:26.350
If we multiply this side is either
minus half z squared, that's good.

00:35:26.350 --> 00:35:32.850
Then we have minus 2tz times negative
one-half, so that matches this.

00:35:32.850 --> 00:35:35.846
And then the only part that we
need to fix up is this t squared.

00:35:35.846 --> 00:35:39.231
So this is gonna be
minus t squared over 2.

00:35:39.231 --> 00:35:41.817
So if we also multiply by
either the t squared over 2,

00:35:41.817 --> 00:35:45.730
an I'll do that on the outside because
that's just a constant with respect to z.

00:35:47.120 --> 00:35:48.650
Then that's gonna cancel out that part.

00:35:48.650 --> 00:35:51.767
So that's just completing the square, dz.

00:35:53.897 --> 00:35:57.668
It's just algebra.

00:35:57.668 --> 00:36:00.559
But by writing it this way,
it's much nicer than this,

00:36:00.559 --> 00:36:03.770
because now we don't have this
linear term anymore, right?

00:36:03.770 --> 00:36:05.990
Now it's just something squared, okay?

00:36:07.330 --> 00:36:08.955
All right, so whats this integral?

00:36:14.934 --> 00:36:16.890
Yeah, root 2pi.

00:36:16.890 --> 00:36:21.035
Because, well, and
if we include this part, then we get 1.

00:36:22.620 --> 00:36:23.420
That's just a normal.

00:36:24.650 --> 00:36:27.001
So this is either the t squared over 2.

00:36:29.937 --> 00:36:36.250
If we include the 1 over 2pi,
this is exactly the normal set.

00:36:36.250 --> 00:36:38.706
We recentered it at t.

00:36:38.706 --> 00:36:41.170
We didn't change the variants, okay?

00:36:41.170 --> 00:36:45.373
So we're integrating a normal density,
we must get 1.

00:36:45.373 --> 00:36:50.004
So just by recognizing that's a normal
except centered at t rather centered at 0,

00:36:50.004 --> 00:36:51.790
we immediately know that's 1.

00:36:51.790 --> 00:36:53.469
So we get e to the t squared over 2.

00:36:53.469 --> 00:36:57.893
And from this you can derive
the nonstandard normal as well,

00:36:57.893 --> 00:37:00.941
which you should do for practice, okay?

00:37:00.941 --> 00:37:07.490
So we're gonna come back to MGFs later and
use them.

00:37:07.490 --> 00:37:12.168
But I want to do one more kind of
like famous probability example

00:37:12.168 --> 00:37:17.022
that's not exactly related to MGFs,
but which will be useful for

00:37:17.022 --> 00:37:20.568
the homework, and for
what we're doing next.

00:37:20.568 --> 00:37:25.626
And that's called Laplace's
rule of succession.

00:37:27.589 --> 00:37:32.979
A famous old problem, Laplace was
a great mathematician and physicist.

00:37:35.762 --> 00:37:39.714
And so
there's this calculation that he did.

00:37:41.689 --> 00:37:45.453
And he phrased it in terms of what's
the probability that the sun will rise

00:37:45.453 --> 00:37:46.070
tomorrow?

00:37:47.200 --> 00:37:52.064
So suppose that the sun has risen for
the last n days in a row,

00:37:52.064 --> 00:37:56.630
and suppose we've observed and
we've been alive for n days,

00:37:56.630 --> 00:38:01.610
every day the sun came up,
n times in a row, right?

00:38:01.610 --> 00:38:04.110
What's the probability that
the sun will rise tomorrow?

00:38:04.110 --> 00:38:07.890
And he kind of got ridiculed for
working on that problem,

00:38:07.890 --> 00:38:11.650
probably cuz it seemed like
kind of a crazy thing.

00:38:11.650 --> 00:38:18.780
And partly because the assumption is
that we're considering random variables

00:38:18.780 --> 00:38:23.730
X1, X2, blah, blah, blah, blah, blah, iid,

00:38:25.810 --> 00:38:30.880
Bernoulli p,
where think of those as the indicators,

00:38:30.880 --> 00:38:34.090
random variable for the sun rising
on the first day, the second day.

00:38:34.090 --> 00:38:38.812
So suppose that each day the sun
either rises or it fails to rise,

00:38:38.812 --> 00:38:42.667
probability p that the sun rises,
iid Bernoullis.

00:38:42.667 --> 00:38:46.821
The other thing that's kind of fishy
about this is that, well I have never

00:38:46.821 --> 00:38:51.377
experienced a day where the sun didn't
rise, but if I did then I started thinking

00:38:51.377 --> 00:38:56.143
probably that's the end of the world, and
it's not gonna rise the next day either.

00:38:56.143 --> 00:38:59.252
So it doesn't seem very realistic
to assume that they're independent.

00:38:59.252 --> 00:39:03.150
But it's kind of fun to think
about the problem in those terms.

00:39:03.150 --> 00:39:07.720
That's just one story, and you can
easily think of other problems that have

00:39:07.720 --> 00:39:09.940
the exact same structure as this.

00:39:09.940 --> 00:39:13.680
So even if you wouldn't apply this to
the actual question of the sun rising,

00:39:13.680 --> 00:39:16.085
it's just a useful structure.

00:39:16.085 --> 00:39:20.749
So this is something we've dealt with
before, just iid Bernoulli trials.

00:39:20.749 --> 00:39:25.048
Okay, but the twist to this
problem is that Laplace is saying

00:39:25.048 --> 00:39:28.434
the probability that
the sun rises is unknown.

00:39:28.434 --> 00:39:31.475
So p is actually unknown, and

00:39:31.475 --> 00:39:36.598
the question is how do we
deal with that unknown p.

00:39:36.598 --> 00:39:44.185
So more precisely, let's say,
given p, this is true.

00:39:44.185 --> 00:39:48.769
If P is known, we're assuming this, okay?

00:39:48.769 --> 00:39:54.785
So all of this is conditional,
this is conditional independence, okay?

00:39:54.785 --> 00:39:59.834
So this is all given p We're
assuming there are iid.

00:39:59.834 --> 00:40:03.022
But now we're saying we don't know, right?

00:40:03.022 --> 00:40:07.123
We have evidence that the sun
has risen n times in a row, but

00:40:07.123 --> 00:40:11.310
we don't know for
sure what the value of p is, right?

00:40:11.310 --> 00:40:13.130
So we're going to treat p as unknown.

00:40:15.790 --> 00:40:19.010
And then kind of one of
the deep philosophical

00:40:19.010 --> 00:40:23.860
debates in statistics is how do
we deal with unknowns, right?

00:40:23.860 --> 00:40:28.020
And for many decades, there's been
this controversy between Bayesians and

00:40:28.020 --> 00:40:29.220
Frequentists.

00:40:29.220 --> 00:40:33.810
That's a big topic for 111, and I'm not
saying we'll talk much about it here.

00:40:33.810 --> 00:40:36.550
The question is, how do we deal with
the fact that this p is unknown?

00:40:37.750 --> 00:40:42.690
Well, the Bayesian point
of view is to say,

00:40:42.690 --> 00:40:47.140
well, since it's unknown,
we're gonna quantify its uncertainty

00:40:47.140 --> 00:40:50.360
by treating it as a random variable
that has some distribution, okay?

00:40:50.360 --> 00:40:54.420
Now, distribution is just
a reflection of our uncertainty.

00:40:54.420 --> 00:41:00.020
So the Bayesian approach is
treat P as a random variable.

00:41:04.560 --> 00:41:09.610
The reason it's called Bayesian is
because then we can use Bayes' rule to

00:41:09.610 --> 00:41:14.900
say what's the distribution of p given
all the evidence we have, right?

00:41:14.900 --> 00:41:19.800
So we start with some prior beliefs about
p, that is before we have any data or

00:41:19.800 --> 00:41:23.460
any evidence,
we have some prior uncertainty.

00:41:23.460 --> 00:41:25.120
Then we collect data and

00:41:25.120 --> 00:41:29.130
we use Baye's rule, Baye's rule is how
do we update based on evidence, okay?

00:41:29.130 --> 00:41:33.730
So update using Baye's rule and
then we have some new uncertainty, okay?

00:41:34.950 --> 00:41:36.600
So, that's the idea.

00:41:36.600 --> 00:41:39.820
So this is gonna look a little bit
strange because we're not used to

00:41:39.820 --> 00:41:42.000
treating lowercase P as a random variable,
but

00:41:42.000 --> 00:41:46.540
this is just good practice in thinking
carefully in what a random variable means.

00:41:46.540 --> 00:41:50.300
So now we're going to treat
p as a random variable and

00:41:50.300 --> 00:41:54.650
we're going to say Laplace said,
let p be uniform.

00:41:54.650 --> 00:41:58.560
Of course you don't have to use
the uniform, but Laplace, and

00:41:58.560 --> 00:42:02.000
this is also a bit controversial,
so this is called the prior,

00:42:03.590 --> 00:42:07.200
that's also a bit controversial,
right, like why uniform?

00:42:07.200 --> 00:42:11.400
Well Laplace basically said well, uniform
should reflect complete uncertainty,

00:42:11.400 --> 00:42:15.370
just completely random,
we know nothing about p, okay?

00:42:15.370 --> 00:42:18.200
But there are definitely some
controversial issues about that.

00:42:18.200 --> 00:42:19.980
Well, let's assume that for now.

00:42:19.980 --> 00:42:25.460
So the structure of the problem,
and let's let Sn be the sum

00:42:25.460 --> 00:42:30.910
of the first n of them, okay?

00:42:30.910 --> 00:42:32.680
So here's the structure of the problem.

00:42:35.160 --> 00:42:44.190
The structure is that
Sn given p is binomial.

00:42:44.190 --> 00:42:48.170
That is the conditional on p,
this just means p as known constant.

00:42:48.170 --> 00:42:51.280
Which is what we've been doing
most of the time, but not always.

00:42:51.280 --> 00:42:56.310
This relates back to the problem about the
random coins and things like that, right?

00:42:56.310 --> 00:42:59.040
The difference between independence and
conditional independence, so

00:42:59.040 --> 00:43:01.250
this is just another example of that.

00:43:01.250 --> 00:43:06.789
If we know which coin we have,
if we know the probability of the sun

00:43:06.789 --> 00:43:11.727
rising the we're the assumption
here is that it's IID And

00:43:11.727 --> 00:43:15.988
so some of id Bernoulli p
is we know is binomial np.

00:43:15.988 --> 00:43:20.120
But p itself is random with
the uniform distribution.

00:43:23.243 --> 00:43:26.955
That's the structure, okay?

00:43:26.955 --> 00:43:34.140
And the problem is defined, first of
all find the posterior distribution.

00:43:34.140 --> 00:43:34.930
By definition,

00:43:34.930 --> 00:43:40.140
posterior distribution means the
distribution after we collect the data.

00:43:40.140 --> 00:43:44.625
This part is the prior, and
the posterior is P given,

00:43:44.625 --> 00:43:47.827
Sn.

00:43:50.260 --> 00:43:53.500
As we assume we observe S.

00:43:53.500 --> 00:43:58.090
You could also assume that you observe
X1 through Xn, and it turns out that you

00:43:58.090 --> 00:44:01.000
would get the same, this is what's
called a sufficient statistic,

00:44:01.000 --> 00:44:04.720
which again is a 111 topic that you
don't have to worry about Per se.

00:44:04.720 --> 00:44:11.150
But it turns out that just observing how
many of these are one is enough, so,

00:44:11.150 --> 00:44:17.040
we can condition on Sn or we can condition
on X1 through Xn and it won't Will matter.

00:44:18.480 --> 00:44:22.860
And the other question would be,
what's the probability, the more practical

00:44:22.860 --> 00:44:25.190
question is what's the probability
the sun it's gonna rise tomorrow?

00:44:25.190 --> 00:44:29.530
So that's the probability
that Xn + 1 = 1 given,

00:44:31.140 --> 00:44:34.530
Sn equals n let's say.

00:44:34.530 --> 00:44:37.050
So the sun is rasing for the last n days,

00:44:37.050 --> 00:44:39.560
what's the probability that
it would rise tomorrow, okay?

00:44:39.560 --> 00:44:41.840
So that's what we wanna do.

00:44:41.840 --> 00:44:43.600
So how do we do that?

00:44:43.600 --> 00:44:47.170
Well, the answer is just Bayes' rule.

00:44:47.170 --> 00:44:52.770
It's just that it's an unfamiliar
form of Bayes' rule but

00:44:52.770 --> 00:44:56.240
it's completely analagous, okay?

00:44:56.240 --> 00:45:00.213
So define this posterior distribution,

00:45:00.213 --> 00:45:09.310
I'm just gonna write down something that
looks like exactly like Baye's rule, okay?

00:45:09.310 --> 00:45:11.770
So we wanna find the, f(p),

00:45:11.770 --> 00:45:17.178
I'm just using f to kind of
generically mean pdf, p is continuous.

00:45:17.178 --> 00:45:21.309
Before we have data, we're treating as
uniform, then after we have data it's just

00:45:21.309 --> 00:45:25.800
gonna have some density, a PDF, but it's
conditional, okay, it's a conditional PDF.

00:45:25.800 --> 00:45:31.670
So I'm calling that f (p),
let's say, given that Sn = k, okay?

00:45:31.670 --> 00:45:36.860
So we're especially interested
in the case where k is n.

00:45:36.860 --> 00:45:39.430
That is the sun has risen for
the last ten days, but

00:45:39.430 --> 00:45:41.500
we may as well consider more generally.

00:45:41.500 --> 00:45:44.480
The sun has risen on k of the last n days,
okay?

00:45:44.480 --> 00:45:49.166
Given that information how do we
update our uncertainty about p?

00:45:49.166 --> 00:45:54.290
It's just Baye's rule, it's just
a form that we haven't seen before

00:45:54.290 --> 00:45:57.610
because on the one hand, these are PDFs.

00:45:57.610 --> 00:45:59.410
PDFs are not probabilities, okay?

00:45:59.410 --> 00:46:04.890
But PDFs, we can think of it intuitively
as a probability or, at least,

00:46:04.890 --> 00:46:10.270
if we multiply it by if we took a PDF
times some little increment then that's

00:46:10.270 --> 00:46:14.450
going to be approximately the probability
of being in that little interval.

00:46:14.450 --> 00:46:18.531
So Baye's rule in this case,
looks Just the same as if you

00:46:18.531 --> 00:46:23.210
ignored the fact that that's
a density in our probability.

00:46:23.210 --> 00:46:24.900
So we're gonna swap those two things,

00:46:24.900 --> 00:46:28.750
I'm gonna say it's probability
of Sn = k given p.

00:46:28.750 --> 00:46:32.540
The notation,

00:46:32.540 --> 00:46:38.030
it takes a little while though to get
used to because we try, when possible to

00:46:38.030 --> 00:46:42.550
distinguishing random variables and their
values of lower case and capital letters.

00:46:42.550 --> 00:46:46.210
But that's a little hard to, you know,
to do here because we let p be,

00:46:46.210 --> 00:46:49.290
we let a lower case p be
a random variable and

00:46:49.290 --> 00:46:51.980
I don't want to start letting
capital p be a random variable.

00:46:51.980 --> 00:46:54.870
So you can make up some
new notation if you want.

00:46:54.870 --> 00:46:58.670
But it's easier to just think
about what things mean.

00:46:58.670 --> 00:47:01.980
So, this given p means we're just
treating p as a known constant,

00:47:01.980 --> 00:47:04.030
even though it's a random variable.

00:47:04.030 --> 00:47:09.220
So, Baye's role, we swap these things,
times f (p), that's the prior.

00:47:12.882 --> 00:47:16.884
And it's also just equal to one,
because we used a uniform prior, so

00:47:16.884 --> 00:47:18.860
that makes it easy.

00:47:18.860 --> 00:47:23.700
Divided by the probability that Sn=k,

00:47:23.700 --> 00:47:28.310
this thing here in the denominator
is an unconditional,

00:47:28.310 --> 00:47:33.448
this is conditional given p, this thing
does not depend on p, it's not allowed to.

00:47:33.448 --> 00:47:37.176
But this thing, in the numerator,
it's a function of p,

00:47:37.176 --> 00:47:40.079
in the denominator,
it does not depend on p.

00:47:43.880 --> 00:47:45.600
If we want to define it directly,

00:47:45.600 --> 00:47:48.860
we would use something that looks
like the Law of Total Probability.

00:47:50.950 --> 00:47:53.560
This is completely analogous to
the Law of Total Probability.

00:47:53.560 --> 00:47:56.070
This is the continuous version
of the Law of Total Probability

00:47:56.070 --> 00:47:57.270
which we haven't done yet so

00:47:57.270 --> 00:48:01.210
I'm doing it now, but it's completely
analogous to the Law of Total Probability.

00:48:01.210 --> 00:48:05.600
That if we want the probability of this
event, then rather than doing a sum,

00:48:05.600 --> 00:48:07.500
we do an integral cuz it's continuous.

00:48:10.060 --> 00:48:15.624
We just do the integral that given p,
f(p)dp.

00:48:15.624 --> 00:48:20.982
So that's a continuous version
of the law of total probability.

00:48:20.982 --> 00:48:25.848
We don't actually need this, we don't
need it now, and I'll show you why.

00:48:25.848 --> 00:48:29.310
But I wanted to just tell you a little
bit more about what this denominator

00:48:29.310 --> 00:48:30.064
means, okay?

00:48:30.064 --> 00:48:33.890
This denominator is a constant
that doesn't depend on p.

00:48:33.890 --> 00:48:37.290
So let's just look at this thing
up to proportionality, right,

00:48:37.290 --> 00:48:39.370
that's a proportionality symbol.

00:48:39.370 --> 00:48:43.420
We're gonna ignore the denominator,
because it doesn't depend on p, right?

00:48:45.320 --> 00:48:52.142
And for the numerator, that's just
from the binomial, that's n choose k.

00:48:52.142 --> 00:48:55.212
n choose k is also a constant,
that is, it doesn't depend on p.

00:48:55.212 --> 00:48:57.602
So I'm gonna ignore the n choose k.

00:48:57.602 --> 00:49:01.891
That's just p to the k q,

00:49:01.891 --> 00:49:07.213
(1-p) to the n-k, okay?

00:49:07.213 --> 00:49:10.760
So that part's one, so
that's actually easy.

00:49:10.760 --> 00:49:13.570
To get the constant in front,
then we'd have to integrate this thing.

00:49:13.570 --> 00:49:16.370
And we're gonna do that
much later in the course.

00:49:16.370 --> 00:49:22.671
But now let's just do the easier case,
f(p) given Sn=n.

00:49:22.671 --> 00:49:26.410
So that's the case where the sun
did rise for the last n days.

00:49:26.410 --> 00:49:28.430
Now this is just p to the n.

00:49:30.500 --> 00:49:33.150
Now this one is easy to normalize, right?

00:49:33.150 --> 00:49:39.924
Because the integral of this, from 0 to 1,
is just p to the n + 1 over n + 1.

00:49:39.924 --> 00:49:45.921
So it's 1 over n + 1, so to normalize it,
we'll just stick an n + 1 there.

00:49:45.921 --> 00:49:48.574
Now this is a valid PDF, okay?

00:49:48.574 --> 00:49:50.152
And now lastly to get this thing, so

00:49:50.152 --> 00:49:53.264
in other words we got this thing
without evaluating the denominator.

00:49:53.264 --> 00:49:58.717
And then lastly if we want p(xn

00:49:58.717 --> 00:50:03.339
+ 1) = 1 given Sn=n.

00:50:03.339 --> 00:50:08.585
Well just think of it this way that
this is our, we want the expected

00:50:08.585 --> 00:50:14.780
value of a random variable with this
distribution by the fundamental bridge.

00:50:14.780 --> 00:50:18.445
We just want the expected value of
a random variable with this distribution.

00:50:18.445 --> 00:50:23.900
So that's just going to be integral
0 to 1, (n + 1) p p to the n, dp.

00:50:27.924 --> 00:50:32.791
Integral p to the n + 1 is
p to the n + 2 over n + 2.

00:50:32.791 --> 00:50:37.195
So we get n + 1 over n + 2.

00:50:37.195 --> 00:50:40.634
So according to Laplace,
if the sun rose 100 days in a row,

00:50:40.634 --> 00:50:44.090
then it would probably be 101 over 102 for
the next day.

