WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.689
 this capsule we will be reducing the 

00:00:01.860 --> 00:00:05.279
 dilemma of exploration versus 

00:00:03.689 --> 00:00:08.820
 exploitation learning parents 

00:00:05.279 --> 00:00:10.320
 we have seen previously 

00:00:08.820 --> 00:00:12.690
 our first algorithms 

00:00:10.320 --> 00:00:15.630
 active reinforcement learning 

00:00:12.690 --> 00:00:18.480
 based on the close parent programming 

00:00:15.630 --> 00:00:20.490
 adaptive dynamics where finally we had 

00:00:18.480 --> 00:00:24.810
 an agent who could take his own 

00:00:20.490 --> 00:00:27.320
 decisions and action chosen a state 

00:00:24.810 --> 00:00:31.830
 given in fact was based on 

00:00:27.320 --> 00:00:33.809
 the current estimate of the sum of 

00:00:31.830 --> 00:00:36.059
 rewards future hope actually 

00:00:33.809 --> 00:00:37.649
 the money what is that is a 

00:00:36.059 --> 00:00:40.879
 index that knows the environment 

00:00:37.649 --> 00:00:43.440
 then his calculation that can make 

00:00:40.879 --> 00:00:45.510
 the sum of future rewards 

00:00:43.440 --> 00:00:48.809
 hope will just learn the action 

00:00:45.510 --> 00:00:51.149
 who maximizes that which maximizes the sum 

00:00:48.809 --> 00:00:53.010
 rewards future hope according to this 

00:00:51.149 --> 00:00:55.079
 he's currently experiencing in relation to 

00:00:53.010 --> 00:00:59.000
 to environments so only according to 

00:00:55.079 --> 00:00:59.000
 his estimate of the transition model 

00:01:00.410 --> 00:01:05.850
 and made it this approached to choose the 

00:01:04.260 --> 00:01:08.729
 next action if an approach that 

00:01:05.850 --> 00:01:10.220
 will say voracious or greedy in English 

00:01:08.729 --> 00:01:14.100
 we will say that it is an agent who is 

00:01:10.220 --> 00:01:16.350
 glee is young so major bad and 

00:01:14.100 --> 00:01:19.320
 we say that because in fact in this case 

00:01:16.350 --> 00:01:23.820
 also the junta will update its plan 

00:01:19.320 --> 00:01:25.790
 so choose new decisions to 

00:01:23.820 --> 00:01:28.229
 from what they consider to be 

00:01:25.790 --> 00:01:29.909
 optimal now ie to 

00:01:28.229 --> 00:01:31.950
 from the information that collected it 

00:01:29.909 --> 00:01:33.960
 so far so in other words 

00:01:31.950 --> 00:01:35.970
 it's an age and will exploit the most 

00:01:33.960 --> 00:01:37.680
 possible information that has been 

00:01:35.970 --> 00:01:39.720
 collected so far compared 

00:01:37.680 --> 00:01:41.939
 to the environment so the information that 

00:01:39.720 --> 00:01:45.000
 is troubled in his estimation of the model 

00:01:41.939 --> 00:01:47.100
 transition and to make practical 

00:01:45.000 --> 00:01:49.170
 voracious approaches are very rarely going 

00:01:47.100 --> 00:01:52.259
 found the final plan for a 

00:01:49.170 --> 00:01:53.670
 men's environment and reason 

00:01:52.259 --> 00:01:55.649
 it actually only takes into account 

00:01:53.670 --> 00:01:59.250
 information that is accumulated 

00:01:55.649 --> 00:02:00.960
 until now and not want to do 

00:01:59.250 --> 00:02:03.899
 account the fact that this 

00:02:00.960 --> 00:02:06.540
 information there and only partial is 

00:02:03.899 --> 00:02:08.910
 so in phase has consisted of jean the 

00:02:06.540 --> 00:02:10.410
 not consider the possibility of exploring 

00:02:08.910 --> 00:02:13.290
 a little bit more the environment for 

00:02:10.410 --> 00:02:15.989
 just amassed more information 

00:02:13.290 --> 00:02:17.909
 on the environment take it to this 

00:02:15.989 --> 00:02:20.310
 moment website better decisions by 

00:02:17.909 --> 00:02:21.659
 compared to the best action after a 

00:02:20.310 --> 00:02:25.859
 makes an interesting parallel to make 

00:02:21.659 --> 00:02:27.989
 here between just the next forest 

00:02:25.859 --> 00:02:32.069
 it's an approach that will explore a 

00:02:27.989 --> 00:02:33.269
 little more and the hill climbing so who 

00:02:32.069 --> 00:02:37.079
 extends the local search war 

00:02:33.269 --> 00:02:39.200
 we saw earlier versus 

00:02:37.079 --> 00:02:41.730
 animated similarities so we saw that 

00:02:39.200 --> 00:02:42.810
 the uimm climbing say we have a 

00:02:41.730 --> 00:02:43.260
 function as what we want 

00:02:42.810 --> 00:02:47.489
 minimize 

00:02:43.260 --> 00:02:50.310
 if here we are currently at this 

00:02:47.489 --> 00:02:51.989
 position there in our function the islands 

00:02:50.310 --> 00:02:55.139
 climbing that will try to do that is to 

00:02:51.989 --> 00:02:57.930
 this place so by a comparison to 

00:02:55.139 --> 00:03:00.359
 left then right and take the 

00:02:57.930 --> 00:03:01.980
 transition so move to the state 

00:03:00.359 --> 00:03:05.310
 who is the best 

00:03:01.980 --> 00:03:08.909
 at this position also because if I 

00:03:05.310 --> 00:03:10.889
 successful here institute because the 

00:03:08.909 --> 00:03:12.120
 liing simulator will allow to 

00:03:10.889 --> 00:03:16.079
 make a transition to a less good 

00:03:12.120 --> 00:03:19.889
 state to possibly afford 

00:03:16.079 --> 00:03:22.290
 to align an overcoat optima 

00:03:19.889 --> 00:03:24.959
 mountain species like those in 

00:03:22.290 --> 00:03:30.540
 our function may be reaching 

00:03:24.959 --> 00:03:32.069
 a better a better optima that is going 

00:03:30.540 --> 00:03:34.590
 maybe be dense that if even a 

00:03:32.069 --> 00:03:36.150
 global optimum so this is no longer the 

00:03:34.590 --> 00:03:37.470
 same thing here is that we will 

00:03:36.150 --> 00:03:38.699
 allow to take actions that 

00:03:37.470 --> 00:03:40.769
 according to the information we had 

00:03:38.699 --> 00:03:43.680
 now does not seem to be the 

00:03:40.769 --> 00:03:46.440
 better to consider the possibility 

00:03:43.680 --> 00:03:47.849
 in fact we are not the right one 

00:03:46.440 --> 00:03:49.889
 information to make our decision 

00:03:47.849 --> 00:03:50.579
 and that by exploring actions that are 

00:03:49.889 --> 00:03:53.549
 not optimal 

00:03:50.579 --> 00:03:55.500
 currently we may be in fact 

00:03:53.549 --> 00:03:58.459
 discover that there are other actions that 

00:03:55.500 --> 00:03:58.459
 they are better 

00:04:00.600 --> 00:04:04.620
 when an example here on in a 

00:04:03.060 --> 00:04:07.110
 environment we saw previously a 

00:04:04.620 --> 00:04:10.380
 environment where we have a grid iii 

00:04:07.110 --> 00:04:11.640
 paris iv and so we have an example of 

00:04:10.380 --> 00:04:13.710
 the execution of the business cure 

00:04:11.640 --> 00:04:15.750
 learning not reinforcement that 

00:04:13.710 --> 00:04:22.410
 precisely was wrong and in this case 

00:04:15.750 --> 00:04:24.600
 in state 1 2 it was also the 

00:04:22.410 --> 00:04:26.970
 policy so the decision that 

00:04:24.600 --> 00:04:28.230
 corresponds to that state corresponds to 

00:04:26.970 --> 00:04:30.240
 winter the boss is to say that 

00:04:28.230 --> 00:04:32.970
 ultimately this agency he prefers 

00:04:30.240 --> 00:04:34.850
 go through here to surrender 

00:04:32.970 --> 00:04:37.650
 possibly +1 

00:04:34.850 --> 00:04:40.260
 when in fact the best policy to 

00:04:37.650 --> 00:04:42.390
 take this would be our best 

00:04:40.260 --> 00:04:43.950
 decision will take the twisted that would 

00:04:42.390 --> 00:04:45.960
 to go up to finally the 

00:04:43.950 --> 00:04:49.470
 here we would have less reinforcement 

00:04:45.960 --> 00:04:53.130
 negative associated with going through these states 

00:04:49.470 --> 00:04:54.690
 there here is actually what happened 

00:04:53.130 --> 00:04:57.390
 it's that in the execution of 

00:04:54.690 --> 00:05:00.210
 the algorithm possibly at 7 is 

00:04:57.390 --> 00:05:01.920
 also the agent made the decision 

00:05:00.210 --> 00:05:04.440
 to go do it the boss she led him 

00:05:01.920 --> 00:05:06.780
 towards a reinforcement that was 

00:05:04.440 --> 00:05:08.670
 was not 0 that was positive because 

00:05:06.780 --> 00:05:10.560
 that eventually 

00:05:08.670 --> 00:05:12.510
 policies to travel to an eto'o fnac 

00:05:10.560 --> 00:05:15.570
 in operation the more we were very 

00:05:12.510 --> 00:05:18.360
 high and therefore son aja do not know 

00:05:15.570 --> 00:05:19.350
 that so he was tempted to take this 

00:05:18.360 --> 00:05:22.020
 action because they appear 

00:05:19.350 --> 00:05:23.640
 better but never tried to go 

00:05:22.020 --> 00:05:27.120
 here up so to make a 

00:05:23.640 --> 00:05:30.800
 upward transition to this state there 

00:05:27.120 --> 00:05:33.120
 here or there politics bring him back up 

00:05:30.800 --> 00:05:37.320
 necessarily positive and better 

00:05:33.120 --> 00:05:39.510
 are future rewards hope so 

00:05:37.320 --> 00:05:41.289
 we see it here if we look we will 

00:05:39.510 --> 00:05:45.610
 measure that 

00:05:41.289 --> 00:05:46.509
 we go through tests on which we 

00:05:45.610 --> 00:05:48.849
 does learning 

00:05:46.509 --> 00:05:50.499
 we see that if we look at what is 

00:05:48.849 --> 00:05:51.879
 written here as being the silent part 

00:05:50.499 --> 00:05:54.309
 that the difference between politics 

00:05:51.879 --> 00:05:56.949
 optimal policy has taken so 

00:05:54.309 --> 00:05:58.389
 that we never reach 0 because of this 

00:05:56.949 --> 00:06:00.909
 because we never allowed ourselves in this 

00:05:58.389 --> 00:06:02.439
 case also to explore actions that at 

00:06:00.909 --> 00:06:09.369
 beginning of learning did not appear 

00:06:02.439 --> 00:06:12.669
 not be the best so too 

00:06:09.369 --> 00:06:14.229
 exploited to be voracious it goes often 

00:06:12.669 --> 00:06:16.990
 led to plans that are not optimal 

00:06:14.229 --> 00:06:19.270
 on the other hand, to explore too much 

00:06:16.990 --> 00:06:20.919
 actions that are currently not the 

00:06:19.270 --> 00:06:22.779
 better it could slow down 

00:06:20.919 --> 00:06:23.949
 needlessly learning who 

00:06:22.779 --> 00:06:25.959
 maybe in fact we have the right 

00:06:23.949 --> 00:06:29.649
 information at some point for 

00:06:25.959 --> 00:06:32.409
 take it and that enough information 

00:06:29.649 --> 00:06:34.240
 to take the action that according to our 

00:06:32.409 --> 00:06:35.110
 information is the best maybe 

00:06:34.240 --> 00:06:37.919
 it's the best thing to do 

00:06:35.110 --> 00:06:39.969
 so found this balance between 

00:06:37.919 --> 00:06:41.770
 exploit the information up 

00:06:39.969 --> 00:06:43.689
 now and explore to accumulate 

00:06:41.770 --> 00:06:45.580
 more information this problem actually 

00:06:43.689 --> 00:06:47.860
 that they would leave fundamental in 

00:06:45.580 --> 00:06:49.689
 learning by inevitably and on 

00:06:47.860 --> 00:06:53.379
 which there is a lot of research that 

00:06:49.689 --> 00:06:55.120
 make a better decor easier 

00:06:53.379 --> 00:06:57.370
 or in fact one can determine 

00:06:55.120 --> 00:07:00.009
 strategies that are optimal sure that 

00:06:57.370 --> 00:07:01.240
 according to certain conditions and a sublet 

00:07:00.009 --> 00:07:04.839
 learn more I invite you to go 

00:07:01.240 --> 00:07:09.159
 see the code of the huge bendit in the 

00:07:04.839 --> 00:07:10.749
 resource book of energy in 

00:07:09.159 --> 00:07:12.399
 practice so what we will do is 

00:07:10.749 --> 00:07:16.330
 only for more complex cases 

00:07:12.399 --> 00:07:17.889
 we will use heuristics the pocket 

00:07:16.330 --> 00:07:19.779
 which is often followed is to define 

00:07:17.889 --> 00:07:22.089
 a function that we will call the 

00:07:19.779 --> 00:07:24.639
 exploration function so this is a 

00:07:22.089 --> 00:07:26.769
 function that takes as input our 

00:07:24.639 --> 00:07:28.539
 current estimate of rewards 

00:07:26.769 --> 00:07:31.059
 future hoped for as well as the number of 

00:07:28.539 --> 00:07:32.559
 once I find myself in that I have 

00:07:31.059 --> 00:07:33.069
 visited the state in which I find myself 

00:07:32.559 --> 00:07:34.479
 currently 

00:07:33.069 --> 00:07:35.800
 and the idea of ​​this function 

00:07:34.479 --> 00:07:39.129
 of exploration there is that she is going 

00:07:35.800 --> 00:07:44.979
 artificially increased the value of 

00:07:39.129 --> 00:07:47.649
 future rewards is so in fact for 

00:07:44.979 --> 00:07:49.300
 the close relative pda 

00:07:47.649 --> 00:07:51.080
 what we are going to do is that value very 

00:07:49.300 --> 00:07:54.680
 young 

00:07:51.080 --> 00:07:57.340
 will be based on an update where the 

00:07:54.680 --> 00:08:00.039
 new value for a state s 

00:07:57.340 --> 00:08:02.629
 I will update it from the 

00:08:00.039 --> 00:08:05.389
 next formula where did I do here 

00:08:02.629 --> 00:08:07.969
 there is that I added this part if 

00:08:05.389 --> 00:08:12.199
 I applied my exploration function 

00:08:07.969 --> 00:08:14.750
 on my current estimate of the 

00:08:12.199 --> 00:08:16.939
 future reward hope and this 

00:08:14.750 --> 00:08:19.789
 function there we could define 

00:08:16.939 --> 00:08:22.909
 different shapes but often ski 

00:08:19.789 --> 00:08:25.699
 used as exploration functions 

00:08:22.909 --> 00:08:31.580
 it's a function that if the number of 

00:08:25.699 --> 00:08:35.000
 once I reached my state s is in 

00:08:31.580 --> 00:08:38.779
 does I go back a bit so here what I 

00:08:35.000 --> 00:08:41.390
 gives in my cns exploration function 

00:08:38.779 --> 00:08:43.510
 acquired the number of times the action has 

00:08:41.390 --> 00:08:45.200
 been chosen for the state s 

00:08:43.510 --> 00:08:47.690
 since I started doing 

00:08:45.200 --> 00:08:50.000
 learning so that's my argument 

00:08:47.690 --> 00:08:51.890
 nbc emma search functions this 

00:08:50.000 --> 00:08:55.300
 case is that if the number of times 

00:08:51.890 --> 00:08:59.810
 that I chose the state s 

00:08:55.300 --> 00:09:02.720
 and while I chose the action has to 

00:08:59.810 --> 00:09:04.820
 my condition is smaller than a certain 

00:09:02.720 --> 00:09:06.230
 threshold that I have to define their very selves 

00:09:04.820 --> 00:09:10.070
 it's a species different settings 

00:09:06.230 --> 00:09:11.779
 so if this smaller than this one down there 

00:09:10.070 --> 00:09:13.880
 I will return a reward 

00:09:11.779 --> 00:09:15.800
 optimistic i call r + that is 

00:09:13.880 --> 00:09:18.230
 so the father allows sakin so 

00:09:15.800 --> 00:09:20.839
 rewards that almost certainly 

00:09:18.230 --> 00:09:24.529
 higher than it really is 

00:09:20.839 --> 00:09:25.970
 and if not if I actually reached exceeded 

00:09:24.529 --> 00:09:28.279
 the threshold of the number of times I have 

00:09:25.970 --> 00:09:30.949
 visited the state s is running the action to 

00:09:28.279 --> 00:09:33.199
 this state I'm going to return the real 

00:09:30.949 --> 00:09:37.910
 future reward hope they have 

00:09:33.199 --> 00:09:40.250
 given an argument here but which are so 

00:09:37.910 --> 00:09:44.660
 these guarantees that the action havas be 

00:09:40.250 --> 00:09:46.490
 chosen in the state is at least n 1 x 

00:09:44.660 --> 00:09:48.829
 who are getting stuck with which I'm doing a 

00:09:46.490 --> 00:09:49.610
 comparison when learning cij 

00:09:48.829 --> 00:09:51.829
 visit 

00:09:49.610 --> 00:09:54.170
 obviously the state ace at least this number 

00:09:51.829 --> 00:09:55.820
 of times there so that allows me to guarantee 

00:09:54.170 --> 00:09:58.990
 that I will try all the actions 

00:09:55.820 --> 00:10:01.430
 at least a number of times in 

00:09:58.990 --> 00:10:02.940
 finally deceiving my algorithms into 

00:10:01.430 --> 00:10:05.460
 giving him does not answer 

00:10:02.940 --> 00:10:07.320
 this inspired photo that is optimistic by 

00:10:05.460 --> 00:10:11.130
 report to the information that I have up 

00:10:07.320 --> 00:10:13.980
 it's a way of controlling 

00:10:11.130 --> 00:10:15.450
 finally my dilemma between 

00:10:13.980 --> 00:10:19.680
 exploration and exploitation 

00:10:15.450 --> 00:10:22.110
 this number of times here this soldier with 

00:10:19.680 --> 00:10:23.190
 which I'm going to know will determine the 

00:10:22.110 --> 00:10:24.540
 number of false notes I'm going to 

00:10:23.190 --> 00:10:26.340
 to explore actions that 

00:10:24.540 --> 00:10:26.760
 are potentially not the best 

00:10:26.340 --> 00:10:29.010
 action 

00:10:26.760 --> 00:10:31.610
 according to my estimate of the rewards 

00:10:29.010 --> 00:10:31.610
 future s 

