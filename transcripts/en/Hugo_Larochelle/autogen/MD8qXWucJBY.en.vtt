WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.960
in this video we'll see an efficient

00:00:02.960 --> 00:00:02.970
in this video we'll see an efficient
 

00:00:02.970 --> 00:00:04.519
in this video we'll see an efficient
algorithm for training a restricted

00:00:04.519 --> 00:00:04.529
algorithm for training a restricted
 

00:00:04.529 --> 00:00:06.470
algorithm for training a restricted
Boltzmann machine known as contrastive

00:00:06.470 --> 00:00:06.480
Boltzmann machine known as contrastive
 

00:00:06.480 --> 00:00:11.270
Boltzmann machine known as contrastive
divergence so we've seen there's the

00:00:11.270 --> 00:00:11.280
divergence so we've seen there's the
 

00:00:11.280 --> 00:00:13.009
divergence so we've seen there's the
post machine how it defines the

00:00:13.009 --> 00:00:13.019
post machine how it defines the
 

00:00:13.019 --> 00:00:15.140
post machine how it defines the
distribution over X and it's hidden

00:00:15.140 --> 00:00:15.150
distribution over X and it's hidden
 

00:00:15.150 --> 00:00:18.470
distribution over X and it's hidden
layer H we've seen that we could also

00:00:18.470 --> 00:00:18.480
layer H we've seen that we could also
 

00:00:18.480 --> 00:00:22.340
layer H we've seen that we could also
write down P of X using the free

00:00:22.340 --> 00:00:22.350
write down P of X using the free
 

00:00:22.350 --> 00:00:25.790
write down P of X using the free
energies and we've looked at how exactly

00:00:25.790 --> 00:00:25.800
energies and we've looked at how exactly
 

00:00:25.800 --> 00:00:27.410
energies and we've looked at how exactly
it tries to model and increase

00:00:27.410 --> 00:00:27.420
it tries to model and increase
 

00:00:27.420 --> 00:00:29.599
it tries to model and increase
probability for certain values of the

00:00:29.599 --> 00:00:29.609
probability for certain values of the
 

00:00:29.609 --> 00:00:32.810
probability for certain values of the
input vector let's actually see how we

00:00:32.810 --> 00:00:32.820
input vector let's actually see how we
 

00:00:32.820 --> 00:00:34.430
input vector let's actually see how we
can actually train the restricted

00:00:34.430 --> 00:00:34.440
can actually train the restricted
 

00:00:34.440 --> 00:00:36.190
can actually train the restricted
Boltzmann machine on some training data

00:00:36.190 --> 00:00:36.200
Boltzmann machine on some training data
 

00:00:36.200 --> 00:00:38.540
Boltzmann machine on some training data
in order to obtain a restricted

00:00:38.540 --> 00:00:38.550
in order to obtain a restricted
 

00:00:38.550 --> 00:00:40.069
in order to obtain a restricted
Boltzmann machine that models well that

00:00:40.069 --> 00:00:40.079
Boltzmann machine that models well that
 

00:00:40.079 --> 00:00:44.410
Boltzmann machine that models well that
data and assigns high probability to it

00:00:44.410 --> 00:00:44.420
data and assigns high probability to it
 

00:00:44.420 --> 00:00:48.020
data and assigns high probability to it
so like we've seen before we'll treat

00:00:48.020 --> 00:00:48.030
so like we've seen before we'll treat
 

00:00:48.030 --> 00:00:50.389
so like we've seen before we'll treat
this problem of training as an empirical

00:00:50.389 --> 00:00:50.399
this problem of training as an empirical
 

00:00:50.399 --> 00:00:53.930
this problem of training as an empirical
Muniz asian problem we'll do without the

00:00:53.930 --> 00:00:53.940
Muniz asian problem we'll do without the
 

00:00:53.940 --> 00:00:56.270
Muniz asian problem we'll do without the
regularization here and just try to

00:00:56.270 --> 00:00:56.280
regularization here and just try to
 

00:00:56.280 --> 00:00:59.779
regularization here and just try to
minimize the average loss where the loss

00:00:59.779 --> 00:00:59.789
minimize the average loss where the loss
 

00:00:59.789 --> 00:01:01.849
minimize the average loss where the loss
is going to be the most natural thing

00:01:01.849 --> 00:01:01.859
is going to be the most natural thing
 

00:01:01.859 --> 00:01:04.789
is going to be the most natural thing
here to do which is to use the negative

00:01:04.789 --> 00:01:04.799
here to do which is to use the negative
 

00:01:04.799 --> 00:01:07.969
here to do which is to use the negative
log probability as our loss for this

00:01:07.969 --> 00:01:07.979
log probability as our loss for this
 

00:01:07.979 --> 00:01:10.670
log probability as our loss for this
model and so what we'd like to do is

00:01:10.670 --> 00:01:10.680
model and so what we'd like to do is
 

00:01:10.680 --> 00:01:13.940
model and so what we'd like to do is
optimize this average negative log

00:01:13.940 --> 00:01:13.950
optimize this average negative log
 

00:01:13.950 --> 00:01:16.820
optimize this average negative log
probability of the training data using

00:01:16.820 --> 00:01:16.830
probability of the training data using
 

00:01:16.830 --> 00:01:18.830
probability of the training data using
an optimization procedure in particular

00:01:18.830 --> 00:01:18.840
an optimization procedure in particular
 

00:01:18.840 --> 00:01:20.120
an optimization procedure in particular
we'd like to use to cast a gradient

00:01:20.120 --> 00:01:20.130
we'd like to use to cast a gradient
 

00:01:20.130 --> 00:01:22.249
we'd like to use to cast a gradient
descent because works well and scales

00:01:22.249 --> 00:01:22.259
descent because works well and scales
 

00:01:22.259 --> 00:01:24.890
descent because works well and scales
well to large data sets and so it means

00:01:24.890 --> 00:01:24.900
well to large data sets and so it means
 

00:01:24.900 --> 00:01:27.710
well to large data sets and so it means
that to achieve this I need the partial

00:01:27.710 --> 00:01:27.720
that to achieve this I need the partial
 

00:01:27.720 --> 00:01:30.350
that to achieve this I need the partial
derivative of any parameter theta of my

00:01:30.350 --> 00:01:30.360
derivative of any parameter theta of my
 

00:01:30.360 --> 00:01:33.350
derivative of any parameter theta of my
loss now if I make the derivation and

00:01:33.350 --> 00:01:33.360
loss now if I make the derivation and
 

00:01:33.360 --> 00:01:35.690
loss now if I make the derivation and
write down the expression for it it

00:01:35.690 --> 00:01:35.700
write down the expression for it it
 

00:01:35.700 --> 00:01:38.330
write down the expression for it it
actually has a an expression which looks

00:01:38.330 --> 00:01:38.340
actually has a an expression which looks
 

00:01:38.340 --> 00:01:40.190
actually has a an expression which looks
kind of similar to other things we've

00:01:40.190 --> 00:01:40.200
kind of similar to other things we've
 

00:01:40.200 --> 00:01:41.929
kind of similar to other things we've
seen before in terms of partial

00:01:41.929 --> 00:01:41.939
seen before in terms of partial
 

00:01:41.939 --> 00:01:43.789
seen before in terms of partial
derivatives of parameters for neural

00:01:43.789 --> 00:01:43.799
derivatives of parameters for neural
 

00:01:43.799 --> 00:01:46.219
derivatives of parameters for neural
networks we have again a difference of

00:01:46.219 --> 00:01:46.229
networks we have again a difference of
 

00:01:46.229 --> 00:01:48.889
networks we have again a difference of
two terms one which depends on the

00:01:48.889 --> 00:01:48.899
two terms one which depends on the
 

00:01:48.899 --> 00:01:50.690
two terms one which depends on the
observation and another one which

00:01:50.690 --> 00:01:50.700
observation and another one which
 

00:01:50.700 --> 00:01:53.480
observation and another one which
depends on more explicitly only on the

00:01:53.480 --> 00:01:53.490
depends on more explicitly only on the
 

00:01:53.490 --> 00:01:56.450
depends on more explicitly only on the
model and so this partial derivative is

00:01:56.450 --> 00:01:56.460
model and so this partial derivative is
 

00:01:56.460 --> 00:02:00.490
model and so this partial derivative is
going to be the expec expectation over

00:02:00.490 --> 00:02:00.500
going to be the expec expectation over
 

00:02:00.500 --> 00:02:03.560
going to be the expec expectation over
what we never see which is the hidden

00:02:03.560 --> 00:02:03.570
what we never see which is the hidden
 

00:02:03.570 --> 00:02:06.950
what we never see which is the hidden
layer value of the partial derivative of

00:02:06.950 --> 00:02:06.960
layer value of the partial derivative of
 

00:02:06.960 --> 00:02:08.889
layer value of the partial derivative of
the energy with respect to my parameter

00:02:08.889 --> 00:02:08.899
the energy with respect to my parameter
 

00:02:08.899 --> 00:02:13.610
the energy with respect to my parameter
given my observation so and expect

00:02:13.610 --> 00:02:13.620
given my observation so and expect
 

00:02:13.620 --> 00:02:15.770
given my observation so and expect
partial derivative we have to need an

00:02:15.770 --> 00:02:15.780
partial derivative we have to need an
 

00:02:15.780 --> 00:02:17.780
partial derivative we have to need an
expectation because H we never know what

00:02:17.780 --> 00:02:17.790
expectation because H we never know what
 

00:02:17.790 --> 00:02:18.800
expectation because H we never know what
its value is

00:02:18.800 --> 00:02:18.810
its value is
 

00:02:18.810 --> 00:02:22.310
its value is
and so it's the expectation of this

00:02:22.310 --> 00:02:22.320
and so it's the expectation of this
 

00:02:22.320 --> 00:02:23.990
and so it's the expectation of this
partial derivative condition on the

00:02:23.990 --> 00:02:24.000
partial derivative condition on the
 

00:02:24.000 --> 00:02:26.960
partial derivative condition on the
observation and then we subtract that

00:02:26.960 --> 00:02:26.970
observation and then we subtract that
 

00:02:26.970 --> 00:02:30.530
observation and then we subtract that
the expectation over H and I and now

00:02:30.530 --> 00:02:30.540
the expectation over H and I and now
 

00:02:30.540 --> 00:02:35.090
the expectation over H and I and now
also X of the again the partial

00:02:35.090 --> 00:02:35.100
also X of the again the partial
 

00:02:35.100 --> 00:02:36.770
also X of the again the partial
derivitive the energy function with

00:02:36.770 --> 00:02:36.780
derivitive the energy function with
 

00:02:36.780 --> 00:02:38.630
derivitive the energy function with
respect to our parameter but notice now

00:02:38.630 --> 00:02:38.640
respect to our parameter but notice now
 

00:02:38.640 --> 00:02:40.910
respect to our parameter but notice now
we do an expectation over X as well and

00:02:40.910 --> 00:02:40.920
we do an expectation over X as well and
 

00:02:40.920 --> 00:02:44.150
we do an expectation over X as well and
this is an expectation according to our

00:02:44.150 --> 00:02:44.160
this is an expectation according to our
 

00:02:44.160 --> 00:02:48.070
this is an expectation according to our
model so under our models distribution

00:02:48.070 --> 00:02:48.080
model so under our models distribution
 

00:02:48.080 --> 00:02:50.990
model so under our models distribution
so we often refer to that part computing

00:02:50.990 --> 00:02:51.000
so we often refer to that part computing
 

00:02:51.000 --> 00:02:53.540
so we often refer to that part computing
this part of the gradient as the

00:02:53.540 --> 00:02:53.550
this part of the gradient as the
 

00:02:53.550 --> 00:02:55.760
this part of the gradient as the
positive phase and this the negative

00:02:55.760 --> 00:02:55.770
positive phase and this the negative
 

00:02:55.770 --> 00:02:59.510
positive phase and this the negative
phase for obvious reasons and but now

00:02:59.510 --> 00:02:59.520
phase for obvious reasons and but now
 

00:02:59.520 --> 00:03:02.750
phase for obvious reasons and but now
the problem is that this part here is

00:03:02.750 --> 00:03:02.760
the problem is that this part here is
 

00:03:02.760 --> 00:03:04.940
the problem is that this part here is
actually hard to compute it's generally

00:03:04.940 --> 00:03:04.950
actually hard to compute it's generally
 

00:03:04.950 --> 00:03:08.090
actually hard to compute it's generally
intractable and the reason is that we

00:03:08.090 --> 00:03:08.100
intractable and the reason is that we
 

00:03:08.100 --> 00:03:10.130
intractable and the reason is that we
have to make an exponential sum over

00:03:10.130 --> 00:03:10.140
have to make an exponential sum over
 

00:03:10.140 --> 00:03:14.960
have to make an exponential sum over
both H and X and so perhaps you'll be

00:03:14.960 --> 00:03:14.970
both H and X and so perhaps you'll be
 

00:03:14.970 --> 00:03:17.210
both H and X and so perhaps you'll be
convinced that if I give you a value of

00:03:17.210 --> 00:03:17.220
convinced that if I give you a value of
 

00:03:17.220 --> 00:03:19.010
convinced that if I give you a value of
the visible layer so if I give you an

00:03:19.010 --> 00:03:19.020
the visible layer so if I give you an
 

00:03:19.020 --> 00:03:22.040
the visible layer so if I give you an
observation X then summing over H is

00:03:22.040 --> 00:03:22.050
observation X then summing over H is
 

00:03:22.050 --> 00:03:24.320
observation X then summing over H is
actually something that's tractable to

00:03:24.320 --> 00:03:24.330
actually something that's tractable to
 

00:03:24.330 --> 00:03:27.110
actually something that's tractable to
do for this partial derivative in

00:03:27.110 --> 00:03:27.120
do for this partial derivative in
 

00:03:27.120 --> 00:03:29.690
do for this partial derivative in
particular for the RB m and E we can

00:03:29.690 --> 00:03:29.700
particular for the RB m and E we can
 

00:03:29.700 --> 00:03:31.610
particular for the RB m and E we can
again leverage the fact that we get an

00:03:31.610 --> 00:03:31.620
again leverage the fact that we get an
 

00:03:31.620 --> 00:03:36.980
again leverage the fact that we get an
expression of nested ik nested sums over

00:03:36.980 --> 00:03:36.990
expression of nested ik nested sums over
 

00:03:36.990 --> 00:03:38.840
expression of nested ik nested sums over
something that factorizes with respect

00:03:38.840 --> 00:03:38.850
something that factorizes with respect
 

00:03:38.850 --> 00:03:40.790
something that factorizes with respect
to each hidden unit and then we can

00:03:40.790 --> 00:03:40.800
to each hidden unit and then we can
 

00:03:40.800 --> 00:03:42.530
to each hidden unit and then we can
actually write this down into an

00:03:42.530 --> 00:03:42.540
actually write this down into an
 

00:03:42.540 --> 00:03:43.640
actually write this down into an
expression that's linear in the number

00:03:43.640 --> 00:03:43.650
expression that's linear in the number
 

00:03:43.650 --> 00:03:46.070
expression that's linear in the number
of hidden units so we'll see that more

00:03:46.070 --> 00:03:46.080
of hidden units so we'll see that more
 

00:03:46.080 --> 00:03:50.210
of hidden units so we'll see that more
and more details later in this video but

00:03:50.210 --> 00:03:50.220
and more details later in this video but
 

00:03:50.220 --> 00:03:52.310
and more details later in this video but
now if you have to do is sum over both X

00:03:52.310 --> 00:03:52.320
now if you have to do is sum over both X
 

00:03:52.320 --> 00:03:55.280
now if you have to do is sum over both X
and H now this becomes intractable and

00:03:55.280 --> 00:03:55.290
and H now this becomes intractable and
 

00:03:55.290 --> 00:03:57.380
and H now this becomes intractable and
so we'll have to approximate that term

00:03:57.380 --> 00:03:57.390
so we'll have to approximate that term
 

00:03:57.390 --> 00:04:01.820
so we'll have to approximate that term
somehow in order to perform stochastic

00:04:01.820 --> 00:04:01.830
somehow in order to perform stochastic
 

00:04:01.830 --> 00:04:07.130
somehow in order to perform stochastic
gradient descent efficiently so to

00:04:07.130 --> 00:04:07.140
gradient descent efficiently so to
 

00:04:07.140 --> 00:04:08.030
gradient descent efficiently so to
address this problem

00:04:08.030 --> 00:04:08.040
address this problem
 

00:04:08.040 --> 00:04:10.460
address this problem
Jeff fintan in 2002 proposed the

00:04:10.460 --> 00:04:10.470
Jeff fintan in 2002 proposed the
 

00:04:10.470 --> 00:04:13.100
Jeff fintan in 2002 proposed the
contrastive divergence learning

00:04:13.100 --> 00:04:13.110
contrastive divergence learning
 

00:04:13.110 --> 00:04:15.530
contrastive divergence learning
algorithm so there's some theory for

00:04:15.530 --> 00:04:15.540
algorithm so there's some theory for
 

00:04:15.540 --> 00:04:18.349
algorithm so there's some theory for
what this is actually doing I'm not

00:04:18.349 --> 00:04:18.359
what this is actually doing I'm not
 

00:04:18.359 --> 00:04:20.330
what this is actually doing I'm not
going to go over that I'm just going to

00:04:20.330 --> 00:04:20.340
going to go over that I'm just going to
 

00:04:20.340 --> 00:04:22.640
going to go over that I'm just going to
describe what the algorithm is and the

00:04:22.640 --> 00:04:22.650
describe what the algorithm is and the
 

00:04:22.650 --> 00:04:24.860
describe what the algorithm is and the
more intuitive terms and

00:04:24.860 --> 00:04:24.870
more intuitive terms and
 

00:04:24.870 --> 00:04:28.040
more intuitive terms and
try to give more intuition for why it

00:04:28.040 --> 00:04:28.050
try to give more intuition for why it
 

00:04:28.050 --> 00:04:30.710
try to give more intuition for why it
should actually work so really the idea

00:04:30.710 --> 00:04:30.720
should actually work so really the idea
 

00:04:30.720 --> 00:04:33.680
should actually work so really the idea
is to try to do without this double

00:04:33.680 --> 00:04:33.690
is to try to do without this double
 

00:04:33.690 --> 00:04:36.950
is to try to do without this double
expectation and instead estimated so

00:04:36.950 --> 00:04:36.960
expectation and instead estimated so
 

00:04:36.960 --> 00:04:38.330
expectation and instead estimated so
there are really three main components

00:04:38.330 --> 00:04:38.340
there are really three main components
 

00:04:38.340 --> 00:04:41.960
there are really three main components
to contrastive divergence the first idea

00:04:41.960 --> 00:04:41.970
to contrastive divergence the first idea
 

00:04:41.970 --> 00:04:44.750
to contrastive divergence the first idea
is that the expectation over X and H in

00:04:44.750 --> 00:04:44.760
is that the expectation over X and H in
 

00:04:44.760 --> 00:04:47.270
is that the expectation over X and H in
the negative phase will actually replace

00:04:47.270 --> 00:04:47.280
the negative phase will actually replace
 

00:04:47.280 --> 00:04:50.630
the negative phase will actually replace
it by a point estimate at a single

00:04:50.630 --> 00:04:50.640
it by a point estimate at a single
 

00:04:50.640 --> 00:04:53.210
it by a point estimate at a single
observation X still so the expectation

00:04:53.210 --> 00:04:53.220
observation X still so the expectation
 

00:04:53.220 --> 00:04:56.630
observation X still so the expectation
over X we'll replace it by a point

00:04:56.630 --> 00:04:56.640
over X we'll replace it by a point
 

00:04:56.640 --> 00:04:59.150
over X we'll replace it by a point
estimate at X still because if you have

00:04:59.150 --> 00:04:59.160
estimate at X still because if you have
 

00:04:59.160 --> 00:05:00.530
estimate at X still because if you have
that point estimate if we give me a

00:05:00.530 --> 00:05:00.540
that point estimate if we give me a
 

00:05:00.540 --> 00:05:02.150
that point estimate if we give me a
value of the visible layer then I can do

00:05:02.150 --> 00:05:02.160
value of the visible layer then I can do
 

00:05:02.160 --> 00:05:05.660
value of the visible layer then I can do
the expectation over H and then get an

00:05:05.660 --> 00:05:05.670
the expectation over H and then get an
 

00:05:05.670 --> 00:05:10.910
the expectation over H and then get an
estimate of the double expectation so

00:05:10.910 --> 00:05:10.920
estimate of the double expectation so
 

00:05:10.920 --> 00:05:12.680
estimate of the double expectation so
that's just really saying we'll do

00:05:12.680 --> 00:05:12.690
that's just really saying we'll do
 

00:05:12.690 --> 00:05:14.690
that's just really saying we'll do
multicolor estimate of the expectation

00:05:14.690 --> 00:05:14.700
multicolor estimate of the expectation
 

00:05:14.700 --> 00:05:18.620
multicolor estimate of the expectation
with a single data point then we'll we

00:05:18.620 --> 00:05:18.630
with a single data point then we'll we
 

00:05:18.630 --> 00:05:21.740
with a single data point then we'll we
need to obtain that X tilt somehow and

00:05:21.740 --> 00:05:21.750
need to obtain that X tilt somehow and
 

00:05:21.750 --> 00:05:24.200
need to obtain that X tilt somehow and
ideally we like to sample from the true

00:05:24.200 --> 00:05:24.210
ideally we like to sample from the true
 

00:05:24.210 --> 00:05:25.700
ideally we like to sample from the true
distribution not the true distribution

00:05:25.700 --> 00:05:25.710
distribution not the true distribution
 

00:05:25.710 --> 00:05:27.620
distribution not the true distribution
but our model distribution and then

00:05:27.620 --> 00:05:27.630
but our model distribution and then
 

00:05:27.630 --> 00:05:28.400
but our model distribution and then
we'll do this by

00:05:28.400 --> 00:05:28.410
we'll do this by
 

00:05:28.410 --> 00:05:31.790
we'll do this by
Gibbs sampling so remember that Gibbs

00:05:31.790 --> 00:05:31.800
Gibbs sampling so remember that Gibbs
 

00:05:31.800 --> 00:05:35.260
Gibbs sampling so remember that Gibbs
sampling corresponds to sampling each

00:05:35.260 --> 00:05:35.270
sampling corresponds to sampling each
 

00:05:35.270 --> 00:05:39.410
sampling corresponds to sampling each
variable in my model given the others

00:05:39.410 --> 00:05:39.420
variable in my model given the others
 

00:05:39.420 --> 00:05:41.810
variable in my model given the others
and specifically for a restricted

00:05:41.810 --> 00:05:41.820
and specifically for a restricted
 

00:05:41.820 --> 00:05:43.220
and specifically for a restricted
Boltzmann machine performing that

00:05:43.220 --> 00:05:43.230
Boltzmann machine performing that
 

00:05:43.230 --> 00:05:44.930
Boltzmann machine performing that
sampling is actually quite as efficient

00:05:44.930 --> 00:05:44.940
sampling is actually quite as efficient
 

00:05:44.940 --> 00:05:47.150
sampling is actually quite as efficient
because conditioned on one layer all the

00:05:47.150 --> 00:05:47.160
because conditioned on one layer all the
 

00:05:47.160 --> 00:05:49.460
because conditioned on one layer all the
other elements in the other layer are

00:05:49.460 --> 00:05:49.470
other elements in the other layer are
 

00:05:49.470 --> 00:05:52.700
other elements in the other layer are
independent then I can sample all the

00:05:52.700 --> 00:05:52.710
independent then I can sample all the
 

00:05:52.710 --> 00:05:55.100
independent then I can sample all the
values in one layer in parallel given

00:05:55.100 --> 00:05:55.110
values in one layer in parallel given
 

00:05:55.110 --> 00:05:58.160
values in one layer in parallel given
the value of the opposite layer and then

00:05:58.160 --> 00:05:58.170
the value of the opposite layer and then
 

00:05:58.170 --> 00:06:01.130
the value of the opposite layer and then
alternate between each layer like this

00:06:01.130 --> 00:06:01.140
alternate between each layer like this
 

00:06:01.140 --> 00:06:03.500
alternate between each layer like this
and so this is actually very efficient

00:06:03.500 --> 00:06:03.510
and so this is actually very efficient
 

00:06:03.510 --> 00:06:06.590
and so this is actually very efficient
to do in practice and then the third

00:06:06.590 --> 00:06:06.600
to do in practice and then the third
 

00:06:06.600 --> 00:06:08.900
to do in practice and then the third
idea which is perhaps the most important

00:06:08.900 --> 00:06:08.910
idea which is perhaps the most important
 

00:06:08.910 --> 00:06:10.220
idea which is perhaps the most important
contribution behind contrasted

00:06:10.220 --> 00:06:10.230
contribution behind contrasted
 

00:06:10.230 --> 00:06:14.120
contribution behind contrasted
divergence is to perform Gibbs sampling

00:06:14.120 --> 00:06:14.130
divergence is to perform Gibbs sampling
 

00:06:14.130 --> 00:06:19.190
divergence is to perform Gibbs sampling
but by starting our sampling at a state

00:06:19.190 --> 00:06:19.200
but by starting our sampling at a state
 

00:06:19.200 --> 00:06:22.730
but by starting our sampling at a state
where the visible layer is set to the

00:06:22.730 --> 00:06:22.740
where the visible layer is set to the
 

00:06:22.740 --> 00:06:25.040
where the visible layer is set to the
training example for which I'm trying to

00:06:25.040 --> 00:06:25.050
training example for which I'm trying to
 

00:06:25.050 --> 00:06:27.320
training example for which I'm trying to
compute the gradient and do an update

00:06:27.320 --> 00:06:27.330
compute the gradient and do an update
 

00:06:27.330 --> 00:06:29.420
compute the gradient and do an update
and so instead of starting like we

00:06:29.420 --> 00:06:29.430
and so instead of starting like we
 

00:06:29.430 --> 00:06:31.520
and so instead of starting like we
usually do in Gibbs sampling at the

00:06:31.520 --> 00:06:31.530
usually do in Gibbs sampling at the
 

00:06:31.530 --> 00:06:34.940
usually do in Gibbs sampling at the
configuration of my my layers my random

00:06:34.940 --> 00:06:34.950
configuration of my my layers my random
 

00:06:34.950 --> 00:06:37.820
configuration of my my layers my random
variables that is sample

00:06:37.820 --> 00:06:37.830
variables that is sample
 

00:06:37.830 --> 00:06:39.700
variables that is sample
perhaps uniformly and just randomly

00:06:39.700 --> 00:06:39.710
perhaps uniformly and just randomly
 

00:06:39.710 --> 00:06:42.680
perhaps uniformly and just randomly
according to some initial distribution

00:06:42.680 --> 00:06:42.690
according to some initial distribution
 

00:06:42.690 --> 00:06:45.590
according to some initial distribution
I'll actually use the value of the

00:06:45.590 --> 00:06:45.600
I'll actually use the value of the
 

00:06:45.600 --> 00:06:47.990
I'll actually use the value of the
training observation for the value of

00:06:47.990 --> 00:06:48.000
training observation for the value of
 

00:06:48.000 --> 00:06:50.600
training observation for the value of
the visible layer when I'm performing

00:06:50.600 --> 00:06:50.610
the visible layer when I'm performing
 

00:06:50.610 --> 00:06:53.210
the visible layer when I'm performing
Gibbs sampling and the other thing is

00:06:53.210 --> 00:06:53.220
Gibbs sampling and the other thing is
 

00:06:53.220 --> 00:06:55.190
Gibbs sampling and the other thing is
that I'm not actually going to do Gibbs

00:06:55.190 --> 00:06:55.200
that I'm not actually going to do Gibbs
 

00:06:55.200 --> 00:06:56.810
that I'm not actually going to do Gibbs
sampling for long I'm actually going to

00:06:56.810 --> 00:06:56.820
sampling for long I'm actually going to
 

00:06:56.820 --> 00:06:59.960
sampling for long I'm actually going to
do it for one two or just a few

00:06:59.960 --> 00:06:59.970
do it for one two or just a few
 

00:06:59.970 --> 00:07:02.990
do it for one two or just a few
iterations and as we'll see this

00:07:02.990 --> 00:07:03.000
iterations and as we'll see this
 

00:07:03.000 --> 00:07:05.240
iterations and as we'll see this
actually works well in practice so to

00:07:05.240 --> 00:07:05.250
actually works well in practice so to
 

00:07:05.250 --> 00:07:08.120
actually works well in practice so to
illustrate this process more visually so

00:07:08.120 --> 00:07:08.130
illustrate this process more visually so
 

00:07:08.130 --> 00:07:11.150
illustrate this process more visually so
there's air in my figure there's below

00:07:11.150 --> 00:07:11.160
there's air in my figure there's below
 

00:07:11.160 --> 00:07:13.910
there's air in my figure there's below
the same circle here so what we'll do is

00:07:13.910 --> 00:07:13.920
the same circle here so what we'll do is
 

00:07:13.920 --> 00:07:16.220
the same circle here so what we'll do is
that at training time for a given

00:07:16.220 --> 00:07:16.230
that at training time for a given
 

00:07:16.230 --> 00:07:18.970
that at training time for a given
training example I'll take the value of

00:07:18.970 --> 00:07:18.980
training example I'll take the value of
 

00:07:18.980 --> 00:07:22.640
training example I'll take the value of
the input vector X and I'll set it as my

00:07:22.640 --> 00:07:22.650
the input vector X and I'll set it as my
 

00:07:22.650 --> 00:07:25.370
the input vector X and I'll set it as my
value for my visible layer then I'll

00:07:25.370 --> 00:07:25.380
value for my visible layer then I'll
 

00:07:25.380 --> 00:07:28.730
value for my visible layer then I'll
sample all the hidden units conditioned

00:07:28.730 --> 00:07:28.740
sample all the hidden units conditioned
 

00:07:28.740 --> 00:07:31.790
sample all the hidden units conditioned
on observing this particular value of

00:07:31.790 --> 00:07:31.800
on observing this particular value of
 

00:07:31.800 --> 00:07:33.500
on observing this particular value of
the visible layer this training example

00:07:33.500 --> 00:07:33.510
the visible layer this training example
 

00:07:33.510 --> 00:07:37.820
the visible layer this training example
so out sampled from P of H given that X

00:07:37.820 --> 00:07:37.830
so out sampled from P of H given that X
 

00:07:37.830 --> 00:07:43.130
so out sampled from P of H given that X
is equal to X T in this case here just a

00:07:43.130 --> 00:07:43.140
is equal to X T in this case here just a
 

00:07:43.140 --> 00:07:46.160
is equal to X T in this case here just a
note on performing that sampling so each

00:07:46.160 --> 00:07:46.170
note on performing that sampling so each
 

00:07:46.170 --> 00:07:47.930
note on performing that sampling so each
neuron condition independent so they

00:07:47.930 --> 00:07:47.940
neuron condition independent so they
 

00:07:47.940 --> 00:07:50.360
neuron condition independent so they
each a Bernoulli for which I can compute

00:07:50.360 --> 00:07:50.370
each a Bernoulli for which I can compute
 

00:07:50.370 --> 00:07:53.410
each a Bernoulli for which I can compute
what's the probability for that

00:07:53.410 --> 00:07:53.420
what's the probability for that
 

00:07:53.420 --> 00:07:55.640
what's the probability for that
Bernoulli random variable that hidden

00:07:55.640 --> 00:07:55.650
Bernoulli random variable that hidden
 

00:07:55.650 --> 00:07:58.670
Bernoulli random variable that hidden
unit being called to 1 given X now to

00:07:58.670 --> 00:07:58.680
unit being called to 1 given X now to
 

00:07:58.680 --> 00:08:01.010
unit being called to 1 given X now to
obtain a sample from a Bernoulli with

00:08:01.010 --> 00:08:01.020
obtain a sample from a Bernoulli with
 

00:08:01.020 --> 00:08:02.660
obtain a sample from a Bernoulli with
that probability of being equal to 1

00:08:02.660 --> 00:08:02.670
that probability of being equal to 1
 

00:08:02.670 --> 00:08:05.090
that probability of being equal to 1
what I can do is just sample from a

00:08:05.090 --> 00:08:05.100
what I can do is just sample from a
 

00:08:05.100 --> 00:08:09.550
what I can do is just sample from a
uniform distribution between 0 and 1 and

00:08:09.550 --> 00:08:09.560
uniform distribution between 0 and 1 and
 

00:08:09.560 --> 00:08:13.460
uniform distribution between 0 and 1 and
then if that value that uniform value

00:08:13.460 --> 00:08:13.470
then if that value that uniform value
 

00:08:13.470 --> 00:08:18.230
then if that value that uniform value
that I sample is actually sorry it

00:08:18.230 --> 00:08:18.240
that I sample is actually sorry it
 

00:08:18.240 --> 00:08:20.540
that I sample is actually sorry it
should be the other way around so if

00:08:20.540 --> 00:08:20.550
should be the other way around so if
 

00:08:20.550 --> 00:08:26.570
should be the other way around so if
this value is greater than so greater

00:08:26.570 --> 00:08:26.580
this value is greater than so greater
 

00:08:26.580 --> 00:08:30.200
this value is greater than so greater
than the value the probability is

00:08:30.200 --> 00:08:30.210
than the value the probability is
 

00:08:30.210 --> 00:08:32.180
than the value the probability is
greater than the value of sample from my

00:08:32.180 --> 00:08:32.190
greater than the value of sample from my
 

00:08:32.190 --> 00:08:36.860
greater than the value of sample from my
uniform then I'm gonna set the that I'm

00:08:36.860 --> 00:08:36.870
uniform then I'm gonna set the that I'm
 

00:08:36.870 --> 00:08:39.410
uniform then I'm gonna set the that I'm
going to set the hidden layer layer unit

00:08:39.410 --> 00:08:39.420
going to set the hidden layer layer unit
 

00:08:39.420 --> 00:08:42.650
going to set the hidden layer layer unit
DG hidden layer the unit to 1 and

00:08:42.650 --> 00:08:42.660
DG hidden layer the unit to 1 and
 

00:08:42.660 --> 00:08:44.570
DG hidden layer the unit to 1 and
otherwise I'm gonna set it to 0 so in

00:08:44.570 --> 00:08:44.580
otherwise I'm gonna set it to 0 so in
 

00:08:44.580 --> 00:08:46.700
otherwise I'm gonna set it to 0 so in
other words the value of the hidden unit

00:08:46.700 --> 00:08:46.710
other words the value of the hidden unit
 

00:08:46.710 --> 00:08:48.650
other words the value of the hidden unit
is going to be the identity function of

00:08:48.650 --> 00:08:48.660
is going to be the identity function of
 

00:08:48.660 --> 00:08:50.960
is going to be the identity function of
whether the probability

00:08:50.960 --> 00:08:50.970
whether the probability
 

00:08:50.970 --> 00:08:54.020
whether the probability
the hidden unit is greater than a random

00:08:54.020 --> 00:08:54.030
the hidden unit is greater than a random
 

00:08:54.030 --> 00:08:56.680
the hidden unit is greater than a random
sample from a uniform between 0 &amp; 1 so

00:08:56.680 --> 00:08:56.690
sample from a uniform between 0 &amp; 1 so
 

00:08:56.690 --> 00:09:00.980
sample from a uniform between 0 &amp; 1 so
we can see that this will be equal to 1

00:09:00.980 --> 00:09:00.990
we can see that this will be equal to 1
 

00:09:00.990 --> 00:09:05.720
we can see that this will be equal to 1
with a probability of this value because

00:09:05.720 --> 00:09:05.730
with a probability of this value because
 

00:09:05.730 --> 00:09:08.720
with a probability of this value because
the mass of a uniformly distributed

00:09:08.720 --> 00:09:08.730
the mass of a uniformly distributed
 

00:09:08.730 --> 00:09:12.070
the mass of a uniformly distributed
random variable between 0 and this value

00:09:12.070 --> 00:09:12.080
random variable between 0 and this value
 

00:09:12.080 --> 00:09:15.110
random variable between 0 and this value
is going to be exactly because it's

00:09:15.110 --> 00:09:15.120
is going to be exactly because it's
 

00:09:15.120 --> 00:09:18.440
is going to be exactly because it's
uniform it's going to be P of H equal to

00:09:18.440 --> 00:09:18.450
uniform it's going to be P of H equal to
 

00:09:18.450 --> 00:09:22.820
uniform it's going to be P of H equal to
1 given X so so indeed the property that

00:09:22.820 --> 00:09:22.830
1 given X so so indeed the property that
 

00:09:22.830 --> 00:09:26.390
1 given X so so indeed the property that
this is 1 is going to be P of HD equal 1

00:09:26.390 --> 00:09:26.400
this is 1 is going to be P of HD equal 1
 

00:09:26.400 --> 00:09:31.220
this is 1 is going to be P of HD equal 1
given X okay so now I've taken my

00:09:31.220 --> 00:09:31.230
given X okay so now I've taken my
 

00:09:31.230 --> 00:09:33.350
given X okay so now I've taken my
training sample I've sampled each of the

00:09:33.350 --> 00:09:33.360
training sample I've sampled each of the
 

00:09:33.360 --> 00:09:37.910
training sample I've sampled each of the
hidden units conditionally conditioned

00:09:37.910 --> 00:09:37.920
hidden units conditionally conditioned
 

00:09:37.920 --> 00:09:40.130
hidden units conditionally conditioned
on the visible layer taking that value

00:09:40.130 --> 00:09:40.140
on the visible layer taking that value
 

00:09:40.140 --> 00:09:44.140
on the visible layer taking that value
and then I'm going to reconstruct a

00:09:44.140 --> 00:09:44.150
and then I'm going to reconstruct a
 

00:09:44.150 --> 00:09:48.140
and then I'm going to reconstruct a
visible layer by sampling from P of X

00:09:48.140 --> 00:09:48.150
visible layer by sampling from P of X
 

00:09:48.150 --> 00:09:50.720
visible layer by sampling from P of X
given the current value of my hidden

00:09:50.720 --> 00:09:50.730
given the current value of my hidden
 

00:09:50.730 --> 00:09:56.240
given the current value of my hidden
layer so I could call this a x1 and then

00:09:56.240 --> 00:09:56.250
layer so I could call this a x1 and then
 

00:09:56.250 --> 00:09:58.670
layer so I could call this a x1 and then
I'm going to do that for K step so I'm

00:09:58.670 --> 00:09:58.680
I'm going to do that for K step so I'm
 

00:09:58.680 --> 00:10:00.620
I'm going to do that for K step so I'm
going to take X 1 and then sample a new

00:10:00.620 --> 00:10:00.630
going to take X 1 and then sample a new
 

00:10:00.630 --> 00:10:02.540
going to take X 1 and then sample a new
value of the hidden there given my

00:10:02.540 --> 00:10:02.550
value of the hidden there given my
 

00:10:02.550 --> 00:10:04.610
value of the hidden there given my
previously sample value of the visible

00:10:04.610 --> 00:10:04.620
previously sample value of the visible
 

00:10:04.620 --> 00:10:06.260
previously sample value of the visible
layer I'm going to alternate like this

00:10:06.260 --> 00:10:06.270
layer I'm going to alternate like this
 

00:10:06.270 --> 00:10:08.690
layer I'm going to alternate like this
performing Gibbs sampling for K steps

00:10:08.690 --> 00:10:08.700
performing Gibbs sampling for K steps
 

00:10:08.700 --> 00:10:12.320
performing Gibbs sampling for K steps
and now this last value after I've done

00:10:12.320 --> 00:10:12.330
and now this last value after I've done
 

00:10:12.330 --> 00:10:17.300
and now this last value after I've done
my K steps is going to be the negative

00:10:17.300 --> 00:10:17.310
my K steps is going to be the negative
 

00:10:17.310 --> 00:10:20.330
my K steps is going to be the negative
sample X tilde X still we often refer to

00:10:20.330 --> 00:10:20.340
sample X tilde X still we often refer to
 

00:10:20.340 --> 00:10:22.100
sample X tilde X still we often refer to
it as a negative sample which is used to

00:10:22.100 --> 00:10:22.110
it as a negative sample which is used to
 

00:10:22.110 --> 00:10:25.040
it as a negative sample which is used to
estimate the negative phase part of the

00:10:25.040 --> 00:10:25.050
estimate the negative phase part of the
 

00:10:25.050 --> 00:10:27.710
estimate the negative phase part of the
gradient and so I'm going to use that as

00:10:27.710 --> 00:10:27.720
gradient and so I'm going to use that as
 

00:10:27.720 --> 00:10:29.870
gradient and so I'm going to use that as
my negative sample to perform my point

00:10:29.870 --> 00:10:29.880
my negative sample to perform my point
 

00:10:29.880 --> 00:10:36.860
my negative sample to perform my point
estimates of the expectation over X okay

00:10:36.860 --> 00:10:36.870
estimates of the expectation over X okay
 

00:10:36.870 --> 00:10:39.260
estimates of the expectation over X okay
so visually what does this look like so

00:10:39.260 --> 00:10:39.270
so visually what does this look like so
 

00:10:39.270 --> 00:10:43.040
so visually what does this look like so
I get a training example and in the

00:10:43.040 --> 00:10:43.050
I get a training example and in the
 

00:10:43.050 --> 00:10:44.750
I get a training example and in the
positive phase I had to estimate this

00:10:44.750 --> 00:10:44.760
positive phase I had to estimate this
 

00:10:44.760 --> 00:10:47.060
positive phase I had to estimate this
conditional expectation to sample if I

00:10:47.060 --> 00:10:47.070
conditional expectation to sample if I
 

00:10:47.070 --> 00:10:48.680
conditional expectation to sample if I
imagine I actually don't perform that

00:10:48.680 --> 00:10:48.690
imagine I actually don't perform that
 

00:10:48.690 --> 00:10:52.520
imagine I actually don't perform that
expectation I just sample and H given

00:10:52.520 --> 00:10:52.530
expectation I just sample and H given
 

00:10:52.530 --> 00:10:57.010
expectation I just sample and H given
this XT and I'll call that H till T and

00:10:57.010 --> 00:10:57.020
this XT and I'll call that H till T and
 

00:10:57.020 --> 00:10:59.630
this XT and I'll call that H till T and
I've also performed K steps of Gibbs

00:10:59.630 --> 00:10:59.640
I've also performed K steps of Gibbs
 

00:10:59.640 --> 00:11:01.130
I've also performed K steps of Gibbs
sampling to have an X tilde and

00:11:01.130 --> 00:11:01.140
sampling to have an X tilde and
 

00:11:01.140 --> 00:11:04.490
sampling to have an X tilde and
similarly to perform that expect

00:11:04.490 --> 00:11:04.500
similarly to perform that expect
 

00:11:04.500 --> 00:11:08.730
similarly to perform that expect
estimated at X still and H still where H

00:11:08.730 --> 00:11:08.740
estimated at X still and H still where H
 

00:11:08.740 --> 00:11:12.269
estimated at X still and H still where H
tilled would be sample based on the

00:11:12.269 --> 00:11:12.279
tilled would be sample based on the
 

00:11:12.279 --> 00:11:13.769
tilled would be sample based on the
conditional distribution of the hidden

00:11:13.769 --> 00:11:13.779
conditional distribution of the hidden
 

00:11:13.779 --> 00:11:18.829
conditional distribution of the hidden
layer given that X is equal to X still

00:11:18.829 --> 00:11:18.839
layer given that X is equal to X still
 

00:11:18.839 --> 00:11:22.320
layer given that X is equal to X still
so that I get these two pairs and if I

00:11:22.320 --> 00:11:22.330
so that I get these two pairs and if I
 

00:11:22.330 --> 00:11:24.000
so that I get these two pairs and if I
look at the gradient descent procedure

00:11:24.000 --> 00:11:24.010
look at the gradient descent procedure
 

00:11:24.010 --> 00:11:26.389
look at the gradient descent procedure
what it's telling me is that I should

00:11:26.389 --> 00:11:26.399
what it's telling me is that I should
 

00:11:26.399 --> 00:11:29.579
what it's telling me is that I should
decrease the energy at the up training

00:11:29.579 --> 00:11:29.589
decrease the energy at the up training
 

00:11:29.589 --> 00:11:32.250
decrease the energy at the up training
observation and I should increase it at

00:11:32.250 --> 00:11:32.260
observation and I should increase it at
 

00:11:32.260 --> 00:11:35.610
observation and I should increase it at
the sample value X still and its

00:11:35.610 --> 00:11:35.620
the sample value X still and its
 

00:11:35.620 --> 00:11:39.690
the sample value X still and its
associated hidden layer because low

00:11:39.690 --> 00:11:39.700
associated hidden layer because low
 

00:11:39.700 --> 00:11:44.070
associated hidden layer because low
energy means high probability then this

00:11:44.070 --> 00:11:44.080
energy means high probability then this
 

00:11:44.080 --> 00:11:46.800
energy means high probability then this
means that I'm going to increase really

00:11:46.800 --> 00:11:46.810
means that I'm going to increase really
 

00:11:46.810 --> 00:11:50.760
means that I'm going to increase really
the probability of observing XT when

00:11:50.760 --> 00:11:50.770
the probability of observing XT when
 

00:11:50.770 --> 00:11:52.800
the probability of observing XT when
it's in a layer and I'm going to

00:11:52.800 --> 00:11:52.810
it's in a layer and I'm going to
 

00:11:52.810 --> 00:11:54.660
it's in a layer and I'm going to
decrease at the same time the

00:11:54.660 --> 00:11:54.670
decrease at the same time the
 

00:11:54.670 --> 00:11:57.329
decrease at the same time the
probability that X still is going to be

00:11:57.329 --> 00:11:57.339
probability that X still is going to be
 

00:11:57.339 --> 00:11:59.880
probability that X still is going to be
observed under my models distribution

00:11:59.880 --> 00:11:59.890
observed under my models distribution
 

00:11:59.890 --> 00:12:02.880
observed under my models distribution
and so if my training example

00:12:02.880 --> 00:12:02.890
and so if my training example
 

00:12:02.890 --> 00:12:05.850
and so if my training example
correspondent two images of digits then

00:12:05.850 --> 00:12:05.860
correspondent two images of digits then
 

00:12:05.860 --> 00:12:07.590
correspondent two images of digits then
I'd be increasing the probability of

00:12:07.590 --> 00:12:07.600
I'd be increasing the probability of
 

00:12:07.600 --> 00:12:09.780
I'd be increasing the probability of
observing this particular digit here

00:12:09.780 --> 00:12:09.790
observing this particular digit here
 

00:12:09.790 --> 00:12:13.590
observing this particular digit here
under my model and then say initially my

00:12:13.590 --> 00:12:13.600
under my model and then say initially my
 

00:12:13.600 --> 00:12:15.750
under my model and then say initially my
restricted Boltzmann machine is randomly

00:12:15.750 --> 00:12:15.760
restricted Boltzmann machine is randomly
 

00:12:15.760 --> 00:12:17.220
restricted Boltzmann machine is randomly
initialized so it essentially

00:12:17.220 --> 00:12:17.230
initialized so it essentially
 

00:12:17.230 --> 00:12:18.510
initialized so it essentially
corresponds to a uniform distribution

00:12:18.510 --> 00:12:18.520
corresponds to a uniform distribution
 

00:12:18.520 --> 00:12:22.800
corresponds to a uniform distribution
over binary vectors then initially one

00:12:22.800 --> 00:12:22.810
over binary vectors then initially one
 

00:12:22.810 --> 00:12:24.090
over binary vectors then initially one
I'm going to sample is really going to

00:12:24.090 --> 00:12:24.100
I'm going to sample is really going to
 

00:12:24.100 --> 00:12:25.860
I'm going to sample is really going to
look like noise essentially it's going

00:12:25.860 --> 00:12:25.870
look like noise essentially it's going
 

00:12:25.870 --> 00:12:27.990
look like noise essentially it's going
to look like something like this and so

00:12:27.990 --> 00:12:28.000
to look like something like this and so
 

00:12:28.000 --> 00:12:29.790
to look like something like this and so
what I'll be doing is then making the

00:12:29.790 --> 00:12:29.800
what I'll be doing is then making the
 

00:12:29.800 --> 00:12:31.560
what I'll be doing is then making the
probability of sampling something like

00:12:31.560 --> 00:12:31.570
probability of sampling something like
 

00:12:31.570 --> 00:12:34.710
probability of sampling something like
this from my model much smaller and then

00:12:34.710 --> 00:12:34.720
this from my model much smaller and then
 

00:12:34.720 --> 00:12:36.690
this from my model much smaller and then
I'll continue iterating like this so

00:12:36.690 --> 00:12:36.700
I'll continue iterating like this so
 

00:12:36.700 --> 00:12:38.519
I'll continue iterating like this so
next time around it'll be less likely

00:12:38.519 --> 00:12:38.529
next time around it'll be less likely
 

00:12:38.529 --> 00:12:40.380
next time around it'll be less likely
that X still would look like a random

00:12:40.380 --> 00:12:40.390
that X still would look like a random
 

00:12:40.390 --> 00:12:43.230
that X still would look like a random
image and the sample value for X still

00:12:43.230 --> 00:12:43.240
image and the sample value for X still
 

00:12:43.240 --> 00:12:45.210
image and the sample value for X still
are gonna be looking more and more like

00:12:45.210 --> 00:12:45.220
are gonna be looking more and more like
 

00:12:45.220 --> 00:12:47.579
are gonna be looking more and more like
actual training examples because I keep

00:12:47.579 --> 00:12:47.589
actual training examples because I keep
 

00:12:47.589 --> 00:12:49.920
actual training examples because I keep
pushing down the probability of anything

00:12:49.920 --> 00:12:49.930
pushing down the probability of anything
 

00:12:49.930 --> 00:12:52.590
pushing down the probability of anything
that doesn't look like a digit and so we

00:12:52.590 --> 00:12:52.600
that doesn't look like a digit and so we
 

00:12:52.600 --> 00:12:54.420
that doesn't look like a digit and so we
can see that as we keep sampling like

00:12:54.420 --> 00:12:54.430
can see that as we keep sampling like
 

00:12:54.430 --> 00:12:56.400
can see that as we keep sampling like
this then eventually the gradient should

00:12:56.400 --> 00:12:56.410
this then eventually the gradient should
 

00:12:56.410 --> 00:12:59.760
this then eventually the gradient should
become smaller because the value of XT

00:12:59.760 --> 00:12:59.770
become smaller because the value of XT
 

00:12:59.770 --> 00:13:02.699
become smaller because the value of XT
is gonna be more more similar to the

00:13:02.699 --> 00:13:02.709
is gonna be more more similar to the
 

00:13:02.709 --> 00:13:05.670
is gonna be more more similar to the
sample value of X still and so intuitive

00:13:05.670 --> 00:13:05.680
sample value of X still and so intuitive
 

00:13:05.680 --> 00:13:07.319
sample value of X still and so intuitive
ly this algorithm what it's doing is

00:13:07.319 --> 00:13:07.329
ly this algorithm what it's doing is
 

00:13:07.329 --> 00:13:10.710
ly this algorithm what it's doing is
that it's increasing the energy sorry

00:13:10.710 --> 00:13:10.720
that it's increasing the energy sorry
 

00:13:10.720 --> 00:13:12.870
that it's increasing the energy sorry
decreasing the energy of things that

00:13:12.870 --> 00:13:12.880
decreasing the energy of things that
 

00:13:12.880 --> 00:13:14.250
decreasing the energy of things that
look like what's in the training set

00:13:14.250 --> 00:13:14.260
look like what's in the training set
 

00:13:14.260 --> 00:13:16.199
look like what's in the training set
while increasing the energy of things

00:13:16.199 --> 00:13:16.209
while increasing the energy of things
 

00:13:16.209 --> 00:13:18.000
while increasing the energy of things
that are as

00:13:18.000 --> 00:13:18.010
that are as
 

00:13:18.010 --> 00:13:20.520
that are as
loosen a door sample by the model and we

00:13:20.520 --> 00:13:20.530
loosen a door sample by the model and we
 

00:13:20.530 --> 00:13:22.470
loosen a door sample by the model and we
keep doing this until the model fits out

00:13:22.470 --> 00:13:22.480
keep doing this until the model fits out
 

00:13:22.480 --> 00:13:25.830
keep doing this until the model fits out
or generates observations that are very

00:13:25.830 --> 00:13:25.840
or generates observations that are very
 

00:13:25.840 --> 00:13:27.900
or generates observations that are very
similar to what's in the model in other

00:13:27.900 --> 00:13:27.910
similar to what's in the model in other
 

00:13:27.910 --> 00:13:31.440
similar to what's in the model in other
words it's become a good model of our

00:13:31.440 --> 00:13:31.450
words it's become a good model of our
 

00:13:31.450 --> 00:13:34.080
words it's become a good model of our
dataset

