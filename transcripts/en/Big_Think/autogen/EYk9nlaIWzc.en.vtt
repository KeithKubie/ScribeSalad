WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.550
 
you

00:00:04.550 --> 00:00:04.560
 
 

00:00:04.560 --> 00:00:06.330
 
everybody is concerned about killer

00:00:06.330 --> 00:00:06.340
everybody is concerned about killer
 

00:00:06.340 --> 00:00:09.299
everybody is concerned about killer
robots we should ban them you know isn't

00:00:09.299 --> 00:00:09.309
robots we should ban them you know isn't
 

00:00:09.309 --> 00:00:11.129
robots we should ban them you know isn't
doing any research into them it may be

00:00:11.129 --> 00:00:11.139
doing any research into them it may be
 

00:00:11.139 --> 00:00:13.350
doing any research into them it may be
unethical to do so it's a wonderful

00:00:13.350 --> 00:00:13.360
unethical to do so it's a wonderful
 

00:00:13.360 --> 00:00:18.240
unethical to do so it's a wonderful
paper in fact by a professor at the post

00:00:18.240 --> 00:00:18.250
paper in fact by a professor at the post
 

00:00:18.250 --> 00:00:21.630
paper in fact by a professor at the post
naval graduate school in Monterey a

00:00:21.630 --> 00:00:21.640
naval graduate school in Monterey a
 

00:00:21.640 --> 00:00:24.450
naval graduate school in Monterey a
balut bj's trouser I believe the title

00:00:24.450 --> 00:00:24.460
balut bj's trouser I believe the title
 

00:00:24.460 --> 00:00:27.450
balut bj's trouser I believe the title
is the moral requirement to deploy

00:00:27.450 --> 00:00:27.460
is the moral requirement to deploy
 

00:00:27.460 --> 00:00:31.770
is the moral requirement to deploy
autonomous drones and his basic point in

00:00:31.770 --> 00:00:31.780
autonomous drones and his basic point in
 

00:00:31.780 --> 00:00:33.690
autonomous drones and his basic point in
that is is really pretty straightforward

00:00:33.690 --> 00:00:33.700
that is is really pretty straightforward
 

00:00:33.700 --> 00:00:36.750
that is is really pretty straightforward
we have obligations to our military

00:00:36.750 --> 00:00:36.760
we have obligations to our military
 

00:00:36.760 --> 00:00:40.829
we have obligations to our military
forces to protect them and things that

00:00:40.829 --> 00:00:40.839
forces to protect them and things that
 

00:00:40.839 --> 00:00:43.770
forces to protect them and things that
we can do which may protect them failure

00:00:43.770 --> 00:00:43.780
we can do which may protect them failure
 

00:00:43.780 --> 00:00:45.509
we can do which may protect them failure
to do that is itself in an ethical

00:00:45.509 --> 00:00:45.519
to do that is itself in an ethical
 

00:00:45.519 --> 00:00:48.869
to do that is itself in an ethical
decision which may cause maybe the wrong

00:00:48.869 --> 00:00:48.879
decision which may cause maybe the wrong
 

00:00:48.879 --> 00:00:52.649
decision which may cause maybe the wrong
thing to do if you have technologies so

00:00:52.649 --> 00:00:52.659
thing to do if you have technologies so
 

00:00:52.659 --> 00:00:55.229
thing to do if you have technologies so
let me give you an interesting scale

00:00:55.229 --> 00:00:55.239
let me give you an interesting scale
 

00:00:55.239 --> 00:00:56.669
let me give you an interesting scale
that whole thing down to show you this

00:00:56.669 --> 00:00:56.679
that whole thing down to show you this
 

00:00:56.679 --> 00:00:58.319
that whole thing down to show you this
doesn't have to be about you know

00:00:58.319 --> 00:00:58.329
doesn't have to be about you know
 

00:00:58.329 --> 00:01:00.479
doesn't have to be about you know
terminator like robots coming in and you

00:01:00.479 --> 00:01:00.489
terminator like robots coming in and you
 

00:01:00.489 --> 00:01:01.829
terminator like robots coming in and you
know shooting at people and things like

00:01:01.829 --> 00:01:01.839
know shooting at people and things like
 

00:01:01.839 --> 00:01:04.710
know shooting at people and things like
that think about a land mine now land

00:01:04.710 --> 00:01:04.720
that think about a land mine now land
 

00:01:04.720 --> 00:01:08.160
that think about a land mine now land
mine has a sensor will switch you step

00:01:08.160 --> 00:01:08.170
mine has a sensor will switch you step
 

00:01:08.170 --> 00:01:10.830
mine has a sensor will switch you step
on it it blows up there's a sensor

00:01:10.830 --> 00:01:10.840
on it it blows up there's a sensor
 

00:01:10.840 --> 00:01:12.600
on it it blows up there's a sensor
there's an action that's taken as a

00:01:12.600 --> 00:01:12.610
there's an action that's taken as a
 

00:01:12.610 --> 00:01:13.920
there's an action that's taken as a
result of a change in its environment

00:01:13.920 --> 00:01:13.930
result of a change in its environment
 

00:01:13.930 --> 00:01:16.830
result of a change in its environment
now it's a fairly straightforward matter

00:01:16.830 --> 00:01:16.840
now it's a fairly straightforward matter
 

00:01:16.840 --> 00:01:19.530
now it's a fairly straightforward matter
to take some artificial intelligence

00:01:19.530 --> 00:01:19.540
to take some artificial intelligence
 

00:01:19.540 --> 00:01:21.270
to take some artificial intelligence
technologies right off the shelf today

00:01:21.270 --> 00:01:21.280
technologies right off the shelf today
 

00:01:21.280 --> 00:01:23.040
technologies right off the shelf today
and just put a little camera on that's

00:01:23.040 --> 00:01:23.050
and just put a little camera on that's
 

00:01:23.050 --> 00:01:25.020
and just put a little camera on that's
not expensive Sam Khan you having your

00:01:25.020 --> 00:01:25.030
not expensive Sam Khan you having your
 

00:01:25.030 --> 00:01:26.550
not expensive Sam Khan you having your
in your cell phone a little bit of

00:01:26.550 --> 00:01:26.560
in your cell phone a little bit of
 

00:01:26.560 --> 00:01:28.800
in your cell phone a little bit of
processing power that could look at

00:01:28.800 --> 00:01:28.810
processing power that could look at
 

00:01:28.810 --> 00:01:31.350
processing power that could look at
what's actually happening around that

00:01:31.350 --> 00:01:31.360
what's actually happening around that
 

00:01:31.360 --> 00:01:34.910
what's actually happening around that
landline and you might think well okay I

00:01:34.910 --> 00:01:34.920
landline and you might think well okay I
 

00:01:34.920 --> 00:01:38.670
landline and you might think well okay I
can see that the person who is nearby me

00:01:38.670 --> 00:01:38.680
can see that the person who is nearby me
 

00:01:38.680 --> 00:01:41.250
can see that the person who is nearby me
is carrying a gun I can see that they're

00:01:41.250 --> 00:01:41.260
is carrying a gun I can see that they're
 

00:01:41.260 --> 00:01:43.470
is carrying a gun I can see that they're
wearing a military uniform so I'm going

00:01:43.470 --> 00:01:43.480
wearing a military uniform so I'm going
 

00:01:43.480 --> 00:01:46.410
wearing a military uniform so I'm going
to blow up but if you see it's just some

00:01:46.410 --> 00:01:46.420
to blow up but if you see it's just some
 

00:01:46.420 --> 00:01:49.440
to blow up but if you see it's just some
peasant out in a field with a rake or a

00:01:49.440 --> 00:01:49.450
peasant out in a field with a rake or a
 

00:01:49.450 --> 00:01:52.560
peasant out in a field with a rake or a
hoe we can avoid blowing up under those

00:01:52.560 --> 00:01:52.570
hoe we can avoid blowing up under those
 

00:01:52.570 --> 00:01:54.630
hoe we can avoid blowing up under those
circumstances oh that's a child I don't

00:01:54.630 --> 00:01:54.640
circumstances oh that's a child I don't
 

00:01:54.640 --> 00:01:56.340
circumstances oh that's a child I don't
want to blow up I'm being stepped on by

00:01:56.340 --> 00:01:56.350
want to blow up I'm being stepped on by
 

00:01:56.350 --> 00:01:58.320
want to blow up I'm being stepped on by
an animal okay I'm not going to blow up

00:01:58.320 --> 00:01:58.330
an animal okay I'm not going to blow up
 

00:01:58.330 --> 00:02:02.210
an animal okay I'm not going to blow up
now that is an autonomous military

00:02:02.210 --> 00:02:02.220
now that is an autonomous military
 

00:02:02.220 --> 00:02:05.670
now that is an autonomous military
technology of just the sort there was a

00:02:05.670 --> 00:02:05.680
technology of just the sort there was a
 

00:02:05.680 --> 00:02:09.029
technology of just the sort there was a
recent letter signed by a great many

00:02:09.029 --> 00:02:09.039
recent letter signed by a great many
 

00:02:09.039 --> 00:02:11.640
recent letter signed by a great many
scientists this falls into that that

00:02:11.640 --> 00:02:11.650
scientists this falls into that that
 

00:02:11.650 --> 00:02:15.309
scientists this falls into that that
class and in their urging

00:02:15.309 --> 00:02:15.319
class and in their urging
 

00:02:15.319 --> 00:02:18.699
class and in their urging
devices like that be banned but I give

00:02:18.699 --> 00:02:18.709
devices like that be banned but I give
 

00:02:18.709 --> 00:02:21.149
devices like that be banned but I give
this as an example of the device which

00:02:21.149 --> 00:02:21.159
this as an example of the device which
 

00:02:21.159 --> 00:02:23.559
this as an example of the device which
for which there's a good argument that

00:02:23.559 --> 00:02:23.569
for which there's a good argument that
 

00:02:23.569 --> 00:02:25.539
for which there's a good argument that
if we can deploy that technology it's

00:02:25.539 --> 00:02:25.549
if we can deploy that technology it's
 

00:02:25.549 --> 00:02:28.569
if we can deploy that technology it's
more humane it's more targeted and it's

00:02:28.569 --> 00:02:28.579
more humane it's more targeted and it's
 

00:02:28.579 --> 00:02:30.879
more humane it's more targeted and it's
more ethical to do so now that isn't

00:02:30.879 --> 00:02:30.889
more ethical to do so now that isn't
 

00:02:30.889 --> 00:02:33.970
more ethical to do so now that isn't
always the case my point is not that I'm

00:02:33.970 --> 00:02:33.980
always the case my point is not that I'm
 

00:02:33.980 --> 00:02:35.979
always the case my point is not that I'm
that's right and we should just go ahead

00:02:35.979 --> 00:02:35.989
that's right and we should just go ahead
 

00:02:35.989 --> 00:02:38.440
that's right and we should just go ahead
willy-nilly and develop killer robots my

00:02:38.440 --> 00:02:38.450
willy-nilly and develop killer robots my
 

00:02:38.450 --> 00:02:40.809
willy-nilly and develop killer robots my
point is this is a much more subtle area

00:02:40.809 --> 00:02:40.819
point is this is a much more subtle area
 

00:02:40.819 --> 00:02:42.849
point is this is a much more subtle area
which requires considerable more thought

00:02:42.849 --> 00:02:42.859
which requires considerable more thought
 

00:02:42.859 --> 00:02:45.520
which requires considerable more thought
and research and we should let the

00:02:45.520 --> 00:02:45.530
and research and we should let the
 

00:02:45.530 --> 00:02:47.319
and research and we should let the
people who are working on it think

00:02:47.319 --> 00:02:47.329
people who are working on it think
 

00:02:47.329 --> 00:02:48.759
people who are working on it think
through these problems and make sure

00:02:48.759 --> 00:02:48.769
through these problems and make sure
 

00:02:48.769 --> 00:02:51.099
through these problems and make sure
that they understand the kinds of

00:02:51.099 --> 00:02:51.109
that they understand the kinds of
 

00:02:51.109 --> 00:02:53.890
that they understand the kinds of
sensitivities and concerns that we have

00:02:53.890 --> 00:02:53.900
sensitivities and concerns that we have
 

00:02:53.900 --> 00:02:56.439
sensitivities and concerns that we have
as a society about the use and

00:02:56.439 --> 00:02:56.449
as a society about the use and
 

00:02:56.449 --> 00:02:57.670
as a society about the use and
deployment of these types of

00:02:57.670 --> 00:02:57.680
deployment of these types of
 

00:02:57.680 --> 00:03:03.710
deployment of these types of
technologies

00:03:03.710 --> 00:03:03.720
 
 

00:03:03.720 --> 00:03:05.780
 
you

