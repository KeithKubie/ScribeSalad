WEBVTT
Kind: captions
Language: en

00:00:01.000 --> 00:00:02.960
Alright so, the, the whole goal of what we're

00:00:02.960 --> 00:00:04.760
going to add for boosting here is we're going to, we're going to

00:00:04.760 --> 00:00:07.400
expand on this notion of hardest examples and weighted

00:00:07.400 --> 00:00:09.790
mean. But before I can do that, I'm going to have

00:00:09.790 --> 00:00:11.770
to define a couple of terms. Okay. And you

00:00:11.770 --> 00:00:14.460
let me know Michael if, if these terms make sense.

00:00:14.460 --> 00:00:17.790
So, here's the first one. The first one is

00:00:17.790 --> 00:00:21.570
error. So how have we been defining error so far?

00:00:21.570 --> 00:00:26.370
&gt;&gt; Usually we take the square difference between the

00:00:26.370 --> 00:00:29.420
correct labels and the, what's produced

00:00:29.420 --> 00:00:31.650
by our classifier or regression algorithm.

00:00:31.650 --> 00:00:32.930
&gt;&gt; That's true. That is how we've been using

00:00:32.930 --> 00:00:36.020
error when we're thinking about regression error. How about, a

00:00:36.020 --> 00:00:39.500
notion of accuracy. About how good we are at,

00:00:39.500 --> 00:00:43.100
say, classifying examples. So let's, let's stick with classification formulas.

00:00:43.100 --> 00:00:44.740
&gt;&gt; Well, that would be the same as

00:00:44.740 --> 00:00:47.270
squared areas, except that it's not really meeting the

00:00:47.270 --> 00:00:49.070
whole powers [INAUDIBLE] area. That is to say, if

00:00:49.070 --> 00:00:51.790
the outputs are zeroes and ones, the squared area

00:00:51.790 --> 00:00:53.620
is just whether or not there's a mismatch. So

00:00:53.620 --> 00:00:56.620
it could just be the total number of wrong answers.

00:00:56.620 --> 00:00:57.720
&gt;&gt; Right. So, what we've been doing so

00:00:57.720 --> 00:01:00.430
far is counting mismatches. I like that word, mismatches.

00:01:00.430 --> 00:01:02.210
And we might call an error raid or

00:01:02.210 --> 00:01:05.610
an error percentage as the total number of mismatches

00:01:05.610 --> 00:01:07.920
over the total number of examples. And that

00:01:07.920 --> 00:01:11.370
tells us whether we're at 85% or 92%, or,

00:01:11.370 --> 00:01:13.100
or whatever, right? So that's what we've been doing

00:01:13.100 --> 00:01:16.820
so far. But implicit in that, Michael, is the

00:01:16.820 --> 00:01:20.600
idea that every single example is equally as important.

00:01:20.600 --> 00:01:23.900
So, that's not always the case. Now you might remember

00:01:23.900 --> 00:01:27.130
from the first talk that we had. We talked

00:01:27.130 --> 00:01:31.250
about distributions over examples. We said that, you know, learning

00:01:31.250 --> 00:01:33.530
only happens if you're training set has the same

00:01:33.530 --> 00:01:37.330
distribution as your future testing set. And if it doesn't,

00:01:37.330 --> 00:01:38.980
then all bets are off. And it's very difficult

00:01:38.980 --> 00:01:42.390
to talk about induction or learning. That notion of distribution

00:01:42.390 --> 00:01:44.850
is implicit in everything that we've been doing so far, and

00:01:44.850 --> 00:01:47.410
we haven't really been taking into account when we've been talking

00:01:47.410 --> 00:01:50.320
about error. So here's another definition of error and you tell

00:01:50.320 --> 00:01:51.850
me if you think it makes sense, given what we just

00:01:51.850 --> 00:01:55.700
said. So, this is my definition of error. So the subscript

00:01:55.700 --> 00:02:00.510
D, stands for distribution. So we don't know how new examples

00:02:00.510 --> 00:02:03.090
are being drawn, but however they're being drawn they're being drawn

00:02:03.090 --> 00:02:06.300
from some distribution, and I'm just going to call that distribution" D", okay?

00:02:06.300 --> 00:02:06.860
&gt;&gt; mhm

00:02:06.860 --> 00:02:09.639
Right. So H is our old friend the

00:02:09.639 --> 00:02:12.580
hypothesis. That's the specific hypothesis that our learner

00:02:12.580 --> 00:02:14.970
has output. That's what we think is the

00:02:14.970 --> 00:02:18.250
true concept, and C is whatever the true underlying

00:02:18.250 --> 00:02:21.770
concept is. So I'm going to define error as

00:02:21.770 --> 00:02:25.740
the probability, given the underlined distribution that I

00:02:25.740 --> 00:02:28.800
will disagree with the true concept on some

00:02:28.800 --> 00:02:31.590
particular instance X. Does that make sense for you?

00:02:31.590 --> 00:02:32.260
&gt;&gt; Yeah

00:02:32.260 --> 00:02:35.250
but I'm not seeing why that's different from number of mismatches in the

00:02:35.250 --> 00:02:39.750
sense that if we count mismatches on a sample drawn from D, which is

00:02:39.750 --> 00:02:42.660
how we would get our testing set anyway. Then I would think that would

00:02:42.660 --> 00:02:46.340
be you know if it's large enough a pretty good approximation of this value.

00:02:46.340 --> 00:02:48.590
&gt;&gt; So here Michael, let me give you a specific example.

00:02:48.590 --> 00:02:52.360
I'm going to draw four, four possible values of X. And when

00:02:52.360 --> 00:02:54.660
I say I'm going to draw four possible values of X, I

00:02:54.660 --> 00:02:56.330
mean I'm just going to put four dots on the the screen.

00:02:56.330 --> 00:02:56.940
&gt;&gt; Hm.

00:02:56.940 --> 00:02:58.070
&gt;&gt; Okay? And

00:02:58.070 --> 00:03:03.390
then I'm going to tell you this particular learner output a hypothesis.

00:03:03.390 --> 00:03:06.750
Output you know, a, a potential function that ends up getting

00:03:06.750 --> 00:03:09.750
the first one and the third one right, but gets the

00:03:09.750 --> 00:03:12.600
second and the fourth one wrong. So what's the error here?

00:03:12.600 --> 00:03:13.310
&gt;&gt; Mm.

00:03:13.310 --> 00:03:16.270
&gt;&gt; So let's just make sure that, that

00:03:16.270 --> 00:03:18.020
everybody's with us. Let's do this as a quiz.

00:03:18.020 --> 00:03:20.330
&gt;&gt; Okay, so let's ask the students what they

00:03:20.330 --> 00:03:23.620
think. So here's the question again. You've output some hypothesis

00:03:23.620 --> 00:03:27.270
over the four possible values of x, and it turns

00:03:27.270 --> 00:03:29.580
out that you get the first and the third one right,

00:03:29.580 --> 00:03:32.170
and you get the second and the fourth one wrong.

00:03:32.170 --> 00:03:34.330
If I look at it like this, what's the error rate?

