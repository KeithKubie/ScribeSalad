1
00:00:00,510 --> 00:00:05,510
You are listening to radio
lab radio from W N Y.

2
00:00:06,780 --> 00:00:07,613
S. T. E.

3
00:00:11,250 --> 00:00:15,270
I'm Jad Abumrad. I'm Robert Krulwich.
And you know what this is, is radio that,

4
00:00:15,271 --> 00:00:16,081
yeah. Okay.

5
00:00:16,081 --> 00:00:19,080
So we're going to play you a little
bit of tape first just to set up the,

6
00:00:19,081 --> 00:00:20,010
what we're going to do today.

7
00:00:20,220 --> 00:00:23,210
About a month ago we were doing the
thing about the fake news. Yeah.

8
00:00:23,400 --> 00:00:25,410
We're very worried about a lot of
fake news. A lot of people are.

9
00:00:25,540 --> 00:00:27,000
But in the middle of
doing that reporting, we,

10
00:00:27,320 --> 00:00:30,930
we were talking with a fellow from
Vanity Fair. My name is Nick Dalton.

11
00:00:30,931 --> 00:00:32,760
I'm a special correspondent
for Vanity Fair.

12
00:00:32,790 --> 00:00:34,440
And in the course of our conversation,

13
00:00:34,470 --> 00:00:36,690
nick and this had nothing to do what
we were talking about, by the way,

14
00:00:36,720 --> 00:00:38,760
nick just got into a sort of a,

15
00:00:40,350 --> 00:00:43,920
well he went into a kind of a nervous
reverie, I'd say. Yeah. He was like,

16
00:00:43,921 --> 00:00:47,050
you know, you guys want to talk about
fake news, but that's not actually,

17
00:00:47,210 --> 00:00:48,080
it's eating at me.

18
00:00:48,170 --> 00:00:51,650
The thing that I've been pretty obsessed
with lately is actually not fake news,

19
00:00:51,651 --> 00:00:54,340
but it's automation and
artificial intelligence. And uhm,

20
00:00:54,560 --> 00:00:59,560
and driverless cars because it's going
to have a larger effect on society than

21
00:01:02,000 --> 00:01:05,270
any technology that I think has ever
been created in the history of mankind.

22
00:01:05,690 --> 00:01:08,870
I know that's kind of a
bold statement, but, uh,

23
00:01:08,990 --> 00:01:10,700
but you've got to imagine that you know,

24
00:01:11,180 --> 00:01:14,180
that there will be in the next 10 years,

25
00:01:14,840 --> 00:01:19,280
20 to 50 million jobs that will just
vanish, uh, to automation. Um, you've got,

26
00:01:19,400 --> 00:01:23,930
you know, million truckers that
will lose their jobs. Um, uh,

27
00:01:23,931 --> 00:01:25,250
the, but it's not,

28
00:01:25,340 --> 00:01:29,000
we think about like automation
and driverless cars and
we think about the fact

29
00:01:29,001 --> 00:01:32,240
that, um, they are going to, uh,

30
00:01:32,270 --> 00:01:34,010
the people that just drive the cars,

31
00:01:34,040 --> 00:01:36,240
like the taxi drivers and the
truckers are going to lose their jobs.

32
00:01:36,300 --> 00:01:39,800
But what we don't realize is that
there, there are entire industries that,

33
00:01:39,860 --> 00:01:43,280
that are built around
just cars. So for example,

34
00:01:43,730 --> 00:01:46,670
if you're not driving the car,
why do you need insurance?

35
00:01:46,820 --> 00:01:49,670
There's no parking tickets because you're
driving this car knows where it can

36
00:01:49,671 --> 00:01:52,620
and cannot park and goes and finds
a spot and moves and so on. Um,

37
00:01:52,820 --> 00:01:56,480
if there are truckers that are no longer
using rest stops because driverless

38
00:01:56,481 --> 00:01:58,730
cars don't have to stop
and pee or take a nap,

39
00:01:58,760 --> 00:02:03,410
then all of those little rest stops
all across America are effected.

40
00:02:03,440 --> 00:02:06,710
People aren't stopping to use the
restrooms, they're not buying burgers,

41
00:02:06,711 --> 00:02:09,290
they're not staying in these
hotels and so on and so forth. And,

42
00:02:09,640 --> 00:02:12,620
and then you look at driverless
cars to a next level.

43
00:02:12,890 --> 00:02:16,160
The whole concept of what a car is,
is going to change. So for example,

44
00:02:16,161 --> 00:02:20,180
right now a car has five seats in
a wheel, but if I'm not driving,

45
00:02:20,390 --> 00:02:23,030
well what's the point of
having five seats in a wheel?

46
00:02:23,030 --> 00:02:25,160
You could imagine that
you take different cars.

47
00:02:25,250 --> 00:02:29,240
And maybe when I was on my way here to
this interview, I wanted to work out,

48
00:02:29,241 --> 00:02:33,440
so I called a driverless gym card or I
have a meeting out in Santa Monica after

49
00:02:33,441 --> 00:02:34,340
this and it's an hour.

50
00:02:34,341 --> 00:02:38,840
So I call a movie car to watch a movie
on the way out there or office car and I

51
00:02:38,841 --> 00:02:41,960
pick up someone else and we have a
meeting on the way and all of these things

52
00:02:41,961 --> 00:02:45,510
are going to happen, not in a vacuum,
but simultaneously. This, you know,

53
00:02:45,560 --> 00:02:48,890
pizza delivery drivers are going to
replace by robots that will actually cook

54
00:02:48,891 --> 00:02:52,910
your pizza on the way to your house in a
little box and then deliver it. And so,

55
00:02:53,540 --> 00:02:57,200
kind of a little bit of a along with an
answer, but I truly do think that, um,

56
00:02:57,560 --> 00:03:00,190
that it's gonna have a
massive effect on society.

57
00:03:01,300 --> 00:03:02,800
Am I stressing you guys out? Are you,

58
00:03:02,830 --> 00:03:05,480
are you having heart palpitations over
there? My blood pressure has gone.

59
00:03:05,650 --> 00:03:10,230
Doesn't that? So that's a fairly
compelling description of, uh, of a, um,

60
00:03:10,330 --> 00:03:13,780
of a very dangerous future. Yes.
But you know what? It's funny, uh,

61
00:03:13,930 --> 00:03:15,330
one of the things that, I mean, we,

62
00:03:15,340 --> 00:03:17,920
we couldn't use that tape
initially at least, right?

63
00:03:17,950 --> 00:03:22,570
But we kept thinking about it because
it actually weirdly points us back to a

64
00:03:22,571 --> 00:03:24,730
story we did about a decade ago.

65
00:03:25,300 --> 00:03:30,300
The story of a moral problem that's
about to get totally re-imagined.

66
00:03:30,550 --> 00:03:34,450
It may be that what nick is worried
about and what we were worried about 10

67
00:03:34,451 --> 00:03:37,960
years ago have now come
dangerously close together.

68
00:03:37,990 --> 00:03:39,380
So what we thought we'd do is we are,

69
00:03:39,390 --> 00:03:44,390
we're going to play you this story as
we did it then sort of the full segment

70
00:03:45,550 --> 00:03:47,140
and then we're going to
amend it on the back end.

71
00:03:47,140 --> 00:03:48,400
And by way of just disclaiming,

72
00:03:48,401 --> 00:03:52,180
this was at a moment in our development
where there's just like way too many

73
00:03:52,181 --> 00:03:54,820
sound effects. Just gratuitous.
You don't have to apologize.

74
00:03:55,300 --> 00:03:58,810
I'm going to apologize cause it
was just too much, just too much.

75
00:03:59,140 --> 00:04:01,810
And also like we talk
about the FMSI machine,

76
00:04:01,811 --> 00:04:06,640
like it's this like amazing thing when
was, it's sort of commonplace now.

77
00:04:06,700 --> 00:04:09,350
Anyhow, it doesn't matter. We're
going to play it for you. Uh,

78
00:04:09,400 --> 00:04:11,080
and then talk about it
on the back end. This,

79
00:04:11,380 --> 00:04:15,580
we start with a description of
something called the trolley problem.

80
00:04:17,340 --> 00:04:19,800
You Ready? Yup. Alright.
You're near some train tracks.

81
00:04:21,650 --> 00:04:25,880
Go there and your mind. Okay. There
are five workers on the tracks working.

82
00:04:26,480 --> 00:04:30,560
You've got their backs turned to the
trolley, which is coming in the distance.

83
00:04:30,750 --> 00:04:32,830
I mean they were repairing the track,
they were repairing the descent.

84
00:04:32,890 --> 00:04:35,650
Unbeknownst to them, the trolley
is approaching. They don't see it.

85
00:04:35,890 --> 00:04:40,720
You can't shout to them. Okay. And if
you do nothing, here's what will happen.

86
00:04:44,920 --> 00:04:49,480
I think workers will die. God,
I was a horrible experience.

87
00:04:49,481 --> 00:04:51,700
I don't want that to happen. But
you didn't. But you have a choice.

88
00:04:52,620 --> 00:04:55,410
You can do a nothing or B.

89
00:04:55,880 --> 00:05:00,330
It's so happens next to you is a lever
pull the lever trolley will jump onto

90
00:05:00,331 --> 00:05:04,940
some sidetracks where there is
only one person working. So if the

91
00:05:07,800 --> 00:05:10,350
send the trolley goes on the second
track, it will kill the one guy. Yeah.

92
00:05:10,390 --> 00:05:11,171
So there's your choice.

93
00:05:11,171 --> 00:05:16,171
Do you kill one man by pulling a lever
or do you kill five men by doing nothing?

94
00:05:18,210 --> 00:05:22,740
Well, I'm going to pull the lever
naturally. Alright, here's part two.

95
00:05:23,750 --> 00:05:25,290
You stand in near some train tracks.

96
00:05:25,440 --> 00:05:29,760
Five guys are in the tracks just as
before and there is the trolley company.

97
00:05:30,190 --> 00:05:33,750
Same five guys working and five guys
back to the train. They can't see. Yeah,

98
00:05:33,751 --> 00:05:35,490
exactly. However, and I
made a couple changes.

99
00:05:35,491 --> 00:05:39,580
Now you're standing on a foot
bridge. The past is over the tracks.

100
00:05:39,760 --> 00:05:41,680
You're looking down onto the tracks.

101
00:05:41,800 --> 00:05:45,400
There's no lever anywhere to be seen
except next to you. There is a guy.

102
00:05:45,550 --> 00:05:47,110
What do you mean as a guy, a large guy,

103
00:05:47,170 --> 00:05:50,860
large individuals standing next to you
on the bridge looking down with you over

104
00:05:50,861 --> 00:05:52,240
the tracks and you realize, wait,

105
00:05:52,270 --> 00:05:57,080
I can save those five workers if I push
this man, give him a little tap. No,

106
00:05:57,540 --> 00:06:02,540
no hills land on the tracks.

107
00:06:03,460 --> 00:06:05,480
Hey, stop the train. Oh man,

108
00:06:05,830 --> 00:06:07,550
got to do that. I'm not going to do that.

109
00:06:07,580 --> 00:06:10,130
But surely you realize
the math is the same.

110
00:06:10,580 --> 00:06:12,830
You mean I'll say four
people this way? Yeah. Yeah,

111
00:06:12,831 --> 00:06:15,770
but this time I'm pushing
the guy. Are you insane?

112
00:06:16,590 --> 00:06:18,910
All right. Here's the thing. If
you ask people these questions,

113
00:06:18,911 --> 00:06:20,290
and we did starting with the first,

114
00:06:20,410 --> 00:06:23,800
is it okay to kill one man
to save five using a lever?

115
00:06:24,040 --> 00:06:28,720
Nine out of 10 people will say yes,
yes, yes. Yeah. But if you ask them,

116
00:06:28,750 --> 00:06:32,680
is it okay to kill one man to
say five by pushing the guy,

117
00:06:33,460 --> 00:06:37,420
nine out of 10 people
will say no, no, no, no.

118
00:06:37,840 --> 00:06:40,000
It is practically universal.

119
00:06:40,510 --> 00:06:45,280
And the thing is if you ask
people why is it okay to murder?

120
00:06:45,430 --> 00:06:46,001
Cause that's what it is.

121
00:06:46,001 --> 00:06:49,780
Murder a man with a lever and not
okay to do it with your hands.

122
00:06:50,740 --> 00:06:55,480
People don't really know. Pulling
the library to save the five, no,

123
00:06:55,540 --> 00:06:59,230
that feels better than pushing
the one to save the five.

124
00:06:59,770 --> 00:07:04,570
But I don't really know why. So that's
a good, there's a good moral quandary,

125
00:07:04,571 --> 00:07:06,240
Maria. Huh?

126
00:07:09,140 --> 00:07:14,140
And if having a moral sense is a unique
and special human quality than maybe we,

127
00:07:14,930 --> 00:07:16,640
we us to humans anyway,

128
00:07:16,641 --> 00:07:20,060
you and me should at least
inquire as to why this happens.

129
00:07:20,480 --> 00:07:21,920
And I happen to have met somebody

130
00:07:22,490 --> 00:07:23,330
who has a hunch.

131
00:07:25,550 --> 00:07:29,810
He's a young guy at Princeton
University, wild, uh, curly here,

132
00:07:29,840 --> 00:07:33,410
but the mischief in his eye,
his name is Josh Goon. Alrighty.

133
00:07:34,040 --> 00:07:38,030
And he spent the last few years trying
to figure out where this inconsistency

134
00:07:38,031 --> 00:07:40,360
comes from. How do people
make this judgment,

135
00:07:40,361 --> 00:07:42,760
forget whether or not these
judgements are right or wrong,

136
00:07:42,850 --> 00:07:47,170
just what's going on in the brain that
makes people distinguish so naturally and

137
00:07:47,171 --> 00:07:48,760
intuitively between these two cases,

138
00:07:48,761 --> 00:07:53,500
which from an actuarial point of
view are very, very, very similar,

139
00:07:53,501 --> 00:07:54,334
if not identical.

140
00:07:55,620 --> 00:07:59,210
Josh is by the way a philosopher
and in neuroscientists.

141
00:07:59,460 --> 00:08:03,030
So this gives him special powers. He
doesn't sort of sit back in a chair,

142
00:08:03,031 --> 00:08:06,480
smoke a pipe and think, now why do you
have these differences? He's just, no,

143
00:08:06,510 --> 00:08:11,510
I would like to look inside people's
heads because in our heads we may find

144
00:08:11,911 --> 00:08:16,380
clues as to where these feelings of
revulsion or acceptance come from

145
00:08:17,660 --> 00:08:18,830
in our brains. Hi.

146
00:08:18,960 --> 00:08:22,670
So we're here in the control room where
you basically just see and it just so

147
00:08:22,671 --> 00:08:25,450
happens that in the basement of
Princeton, different things, one,

148
00:08:25,490 --> 00:08:29,090
there was this, um, yeah, yeah.
Wow. Big circular thing. Yeah.

149
00:08:29,091 --> 00:08:30,680
It looks kind of like an
airplane and [inaudible]

150
00:08:30,820 --> 00:08:34,650
180,000 pound brains.

151
00:08:35,110 --> 00:08:36,190
I'll tell you a funny story.

152
00:08:37,390 --> 00:08:40,210
You can't have any metal in
there because the magnet,

153
00:08:40,510 --> 00:08:43,150
so we have this long list of questions
that we ask people to make sure they can

154
00:08:43,151 --> 00:08:45,710
go in. Do you have a pacemaker or have
you ever worked with metal, blah, blah,

155
00:08:45,711 --> 00:08:48,040
blah blah. Have you ever with metal? Yeah.

156
00:08:48,220 --> 00:08:51,220
Cause you could have little flecks of
metal in your eyes that you would never

157
00:08:51,221 --> 00:08:53,190
even know are there from
having done metal working.

158
00:08:53,480 --> 00:08:56,550
And one of the questions is whether or
not you wear wig or anything like that

159
00:08:56,610 --> 00:08:58,890
because they often have
metal wires in with that.

160
00:08:58,891 --> 00:09:01,620
And there was this very nice woman
who does brain research here,

161
00:09:01,740 --> 00:09:05,660
who's Italian and she's asking
her subjects over the phone, all,

162
00:09:05,700 --> 00:09:07,970
all these screening questions. And
so I have this person over to dinner.

163
00:09:07,980 --> 00:09:10,440
She said, yeah, you know, I
ended up, uh, doing this study,

164
00:09:10,441 --> 00:09:12,750
but it's asked you the weirdest
questions. This woman's like,

165
00:09:12,870 --> 00:09:16,080
do you have a head of, and, and I'm like,

166
00:09:16,170 --> 00:09:18,270
what does it have to do with
if I have herpes or not?

167
00:09:18,560 --> 00:09:22,470
And I want to say it anyway. And she
said, you know, she asked you, she said,

168
00:09:22,471 --> 00:09:24,210
do you have a hairpiece? But she, uh,

169
00:09:24,300 --> 00:09:26,430
so now she asks people if
you wear a wig or whatever.

170
00:09:26,960 --> 00:09:30,260
Anyhow, what Josh does is he
invites people into this room,

171
00:09:30,261 --> 00:09:35,110
has them lie down on what is essentially
a cot on rollers and he rules them into

172
00:09:35,140 --> 00:09:38,510
the machine. Their heads are braced.
So they're sort of stuck in there.

173
00:09:38,980 --> 00:09:42,650
They were done this, oh yeah, Yup.
Several times and many tells them stories.

174
00:09:42,651 --> 00:09:45,830
He tells him the same too, you know,
trolley tales that you told before.

175
00:09:46,190 --> 00:09:49,700
And then at the very instant that they're
deciding whether I should push the

176
00:09:49,701 --> 00:09:51,350
lever or whether I should push the man.

177
00:09:51,420 --> 00:09:56,370
That instant the scanner snaps
pictures of their brains.

178
00:09:56,530 --> 00:10:00,490
And what he found in those pictures
was a frankly a little startling. Uh,

179
00:10:00,520 --> 00:10:04,270
he showed us some, I, I'll, I'll,
I'll show you some, some, some stuff.

180
00:10:05,710 --> 00:10:10,240
Okay. Let me think. The picture that
I'm looking at is this sort of, uh,

181
00:10:10,940 --> 00:10:12,850
it's, it's a brain looked,
I guess from the top down,

182
00:10:13,280 --> 00:10:16,960
top down and sort of sliced, you know,
like, like a, like a Deli slicer.

183
00:10:16,990 --> 00:10:20,020
And the first slide that he showed
me was a human brain being asked the

184
00:10:20,021 --> 00:10:23,860
question, would you pull the lever?
And the answer, in most cases was yes,

185
00:10:24,280 --> 00:10:26,320
but pull the level when
the brain saying yes,

186
00:10:26,321 --> 00:10:29,890
you'd see little pint of
peanut shaped spots of yellow.

187
00:10:29,950 --> 00:10:33,100
This little guy right here and
these two guys right there,

188
00:10:33,101 --> 00:10:36,880
the green was being active in these
places and oddly enough when ever people

189
00:10:37,140 --> 00:10:41,740
said yes, yes, yes to the lever
question, the very same pattern lit up.

190
00:10:42,710 --> 00:10:46,640
Then he showed me another slide. This
was a side of brain saying, no, no,

191
00:10:46,641 --> 00:10:51,230
I would not push the man. I will not
push the large man. And in this picture,

192
00:10:51,350 --> 00:10:52,880
this one we're looking at here, this,

193
00:10:52,910 --> 00:10:56,570
it was a totally different constellation
of regions that lit up. This is the,

194
00:10:56,571 --> 00:10:58,940
no, no, no crowd. I think this is

195
00:10:59,000 --> 00:11:00,190
part of the no, no, no crowd.

196
00:11:00,610 --> 00:11:04,390
So when people answer yes to the
level of question, there are,

197
00:11:04,420 --> 00:11:07,090
there are places in their brain
which glow, but when they answer, no,

198
00:11:07,091 --> 00:11:08,680
I will not push the man,

199
00:11:08,681 --> 00:11:10,840
then you get a completely different
part of the brain lighting up.

200
00:11:10,990 --> 00:11:13,670
Even though the questions
are basically the same,

201
00:11:14,290 --> 00:11:17,320
what does that mean and what
does Josh make this? Oh,

202
00:11:17,321 --> 00:11:20,260
he has a theory about
this. A theory, not proven,

203
00:11:20,261 --> 00:11:22,280
but I think that this is what
I think the evidence suggests.

204
00:11:22,430 --> 00:11:27,220
Suggest that the human brain doesn't
hum along like one big unified system.

205
00:11:27,340 --> 00:11:28,800
Instead he says, maybe in your brain,

206
00:11:28,870 --> 00:11:32,770
in every brain you'll find little
warring tribes, little subgroups,

207
00:11:33,040 --> 00:11:37,510
one that is sort of doing a logical
sort of counting kind of thing.

208
00:11:39,400 --> 00:11:43,420
You've got one part of the brain that
says, Huh, five lives versus one life.

209
00:11:43,420 --> 00:11:45,070
Wouldn't it be better
to say five versus one?

210
00:11:45,280 --> 00:11:48,610
And that's the part that would glow
when you answer yes, I'd pull the lever.

211
00:11:48,640 --> 00:11:52,150
Yeah, pull the lever, but there's
this other part of the brain.

212
00:11:52,340 --> 00:11:52,811
Would you really,

213
00:11:52,811 --> 00:11:57,040
really like personally killing another
human being and gets very upset that the

214
00:11:57,041 --> 00:12:00,140
fat man ks and shouts in effect? Oh No,

215
00:12:00,450 --> 00:12:04,750
you don't understand it on that
level and says, no, no, no, Dad,

216
00:12:04,780 --> 00:12:07,750
don't do never do everything.
A couple of books now

217
00:12:09,520 --> 00:12:12,940
instead of having sort of one system that
just sort of churns out the answer and

218
00:12:13,330 --> 00:12:14,860
bing, we have multiple systems,

219
00:12:14,980 --> 00:12:17,620
they give different answers and they
Duke it out and hopefully out of that

220
00:12:17,621 --> 00:12:20,440
competition comes morality.

221
00:12:25,520 --> 00:12:30,260
This is not a trivial discovery that
you struggle to find right and wrong

222
00:12:30,610 --> 00:12:34,750
depending upon what part of your brain
is shouting the loudest. This is,

223
00:12:35,250 --> 00:12:39,010
it's like bleachers, morality.
Do you buy this? Hmm.

224
00:12:40,940 --> 00:12:43,650
Ah, you know, I, I just don't know.

225
00:12:44,760 --> 00:12:49,620
I always kind of suspected that a sense
of right and wrong is mostly stuff that

226
00:12:49,621 --> 00:12:54,080
you get from your mom and your dad and
from experience that it's culturally

227
00:12:54,081 --> 00:12:57,770
learned. For the most part, Josh is
kind of a radical in this respect.

228
00:12:57,771 --> 00:12:59,030
He thinks it's biological.

229
00:12:59,031 --> 00:13:03,050
I mean deeply biological that somehow
we inherit from the deep past,

230
00:13:03,051 --> 00:13:04,070
a sense of right and wrong.

231
00:13:04,071 --> 00:13:07,970
That's already in our brains from
the get go before mom and dad

232
00:13:08,690 --> 00:13:12,470
are our primate ancestors,
before we were full blown,

233
00:13:12,471 --> 00:13:15,020
humans had intensely social lives.

234
00:13:15,200 --> 00:13:20,000
They have social mechanisms that prevent
them from doing all the nasty things

235
00:13:20,001 --> 00:13:23,510
that they might otherwise be interested
in doing. And so deepen our brain.

236
00:13:23,511 --> 00:13:28,511
We have what you might call basic primate
morality and basic primate morality

237
00:13:28,790 --> 00:13:30,770
doesn't understand
things like tax evasion,

238
00:13:31,160 --> 00:13:35,180
but it does understand things like
pushing your buddy off of a cliff. Ah,

239
00:13:35,270 --> 00:13:39,410
so you're thinking then that the man on
the bridge that I'm on the bridge next

240
00:13:39,411 --> 00:13:44,360
to the large man and then I have hundreds
of thousands of years of training in

241
00:13:44,361 --> 00:13:48,170
my brain that says don't
murder the large man. Right?

242
00:13:48,200 --> 00:13:50,960
Whereas even if I'm thinking
if I murdered the large man,

243
00:13:50,961 --> 00:13:52,910
I'm going to save five lives
and only kill the woman,

244
00:13:53,060 --> 00:13:54,380
but there's something
deeper down that says,

245
00:13:54,381 --> 00:13:57,290
don't murder the large
man right now, that case,

246
00:13:57,291 --> 00:14:01,310
I think it's a pretty easy case even
though it's five versus one. In that case,

247
00:14:01,311 --> 00:14:05,600
people just go with what we might call
the inner chimp, but there are other,

248
00:14:05,880 --> 00:14:10,070
but their unfortunate way
of describing an act of,

249
00:14:10,170 --> 00:14:13,100
of deep goodness. That's interesting.

250
00:14:13,460 --> 00:14:16,160
10 Commandments forgot
inner chimp, right? Well,

251
00:14:16,161 --> 00:14:20,360
what's interesting is that we think of
of basic human morality as being handed

252
00:14:20,361 --> 00:14:21,530
down from on high,

253
00:14:21,680 --> 00:14:24,500
and it's probably better to say
that it was handed up from below,

254
00:14:25,010 --> 00:14:30,010
that our most basic core moral values
are not the things that we humans have

255
00:14:31,191 --> 00:14:34,280
invented, but the things that we've
actually inherited from other people,

256
00:14:34,400 --> 00:14:37,700
the stuff that we humans have invented
or the things that seem more peripheral

257
00:14:37,701 --> 00:14:38,420
and variable,

258
00:14:38,420 --> 00:14:42,890
but something as basic
as thou shalt not kill,

259
00:14:43,430 --> 00:14:47,570
which many people think was handed
down in tablet form from a mountaintop,

260
00:14:47,720 --> 00:14:51,290
from God directly to humans.
No chimps involved, right?

261
00:14:51,890 --> 00:14:56,090
You're suggesting that
hundreds of thousands of years
of on the ground training

262
00:14:56,091 --> 00:15:00,980
have gotten our brains to
think, don't kill your kin,

263
00:15:01,010 --> 00:15:04,790
don't kill your these, you know, that
should be your default response. I mean,

264
00:15:04,791 --> 00:15:08,690
certainly chimpanzees are extremely
violent and they do kill each other,

265
00:15:08,810 --> 00:15:11,840
but they don't do it. As a matter
of, of course, they, so to speak,

266
00:15:11,841 --> 00:15:15,510
have to have some context sensitive
reason for doing so. So, hey,

267
00:15:15,520 --> 00:15:20,520
how are we getting to the rubber that
you think that profound moral positions

268
00:15:21,680 --> 00:15:26,270
may be somehow embedded in
brain chemistry? Um, yeah,

269
00:15:27,750 --> 00:15:28,970
and Josh thinks, uh,

270
00:15:29,010 --> 00:15:32,550
there are times when these different
moral positions that we have embedded

271
00:15:32,551 --> 00:15:37,050
inside of us in our brains when
they can come into conflict.

272
00:15:37,620 --> 00:15:40,710
And in the original episode
we went into one more story.

273
00:15:41,220 --> 00:15:43,800
This one you might call
the crying baby dilemma.

274
00:15:44,420 --> 00:15:45,780
The situation, uh, is,

275
00:15:45,800 --> 00:15:50,490
is somewhat similar to the last episode
of Mash for people who are familiar with

276
00:15:50,491 --> 00:15:52,440
that. But the way we told
the story, it goes like this.

277
00:15:52,740 --> 00:15:55,290
It's wartime [inaudible]
patrol coming down the road.

278
00:15:55,640 --> 00:15:58,560
You are hiding in the basement
with some of your fellow villagers.

279
00:15:58,740 --> 00:16:02,790
Let's kill those lights. And
the enemy soldiers are outside.

280
00:16:02,970 --> 00:16:05,310
They have orders to kill
anyone that they find

281
00:16:05,610 --> 00:16:09,920
quiet. Please nobody make a
sound until the pastors. So

282
00:16:10,170 --> 00:16:10,441
you are,

283
00:16:10,441 --> 00:16:15,060
you're huddled in the basement all around
your enemy troops and you're holding

284
00:16:15,061 --> 00:16:17,550
your baby in your arms,

285
00:16:19,570 --> 00:16:23,430
your baby with a coat, a bit
of a sniffle. And you know,

286
00:16:24,180 --> 00:16:27,200
the good baby could cough
at any moment. [inaudible]

287
00:16:27,450 --> 00:16:30,780
they hear your baby. They're going to
find you and the baby and everyone else,

288
00:16:30,781 --> 00:16:31,920
and they're going to kill everybody.

289
00:16:32,880 --> 00:16:36,120
And the only way you can stop this from
happening is covered the baby's mouth.

290
00:16:37,200 --> 00:16:39,300
But if you do that, the baby's
going to smother and die.

291
00:16:39,720 --> 00:16:41,160
If you don't cover the baby's mouth,

292
00:16:41,460 --> 00:16:43,800
soldiers are going to find everybody
and everybody's going to be killed,

293
00:16:43,801 --> 00:16:47,310
including you, including your
baby, and you have the choice.

294
00:16:48,090 --> 00:16:53,090
Would you smother your own baby to save
the village or would you like your baby

295
00:16:53,521 --> 00:16:55,860
cough? Knowing the consequences,

296
00:16:58,260 --> 00:17:00,060
and this is a very tough question.

297
00:17:00,570 --> 00:17:05,100
People take a long time to think about
it and some people say yes and some

298
00:17:05,101 --> 00:17:05,934
people say no

299
00:17:06,040 --> 00:17:08,550
and children are a blessing
and a gift from God.

300
00:17:08,760 --> 00:17:11,070
And we do not do that to children. Yes,

301
00:17:11,071 --> 00:17:15,570
I think I would heal my baby to
save everyone else and myself. No,

302
00:17:15,630 --> 00:17:18,300
I would not kill the baby.
I feel because it's my baby.

303
00:17:18,301 --> 00:17:20,970
I have the right to terminate the life.

304
00:17:21,030 --> 00:17:22,620
I'd like to say that
I would kill the baby,

305
00:17:22,621 --> 00:17:24,390
but I don't know if I'd
have the inner strength.

306
00:17:24,630 --> 00:17:27,930
Know if it comes down to killing my own
child, my own daughter or my own son.

307
00:17:28,380 --> 00:17:30,410
Then I choose that.
Yeah, if you have to do,

308
00:17:30,870 --> 00:17:34,440
because it was done in World War Two
when the Germans were coming around,

309
00:17:34,920 --> 00:17:38,670
there was a mother that had a baby that
was crying and rather than be found,

310
00:17:38,671 --> 00:17:42,130
she actually suffocated the
baby, but the other people live.

311
00:17:42,250 --> 00:17:46,770
Sounds like an old mash. They know
you do not kill your baby. Okay.

312
00:17:46,910 --> 00:17:48,950
In the final mash episode,

313
00:17:50,340 --> 00:17:52,890
the Korean woman who is a
character in this piece,

314
00:17:54,980 --> 00:17:59,520
she later, sure baby. She
killed it. She killed it.

315
00:18:00,360 --> 00:18:01,193
Okay.

316
00:18:01,580 --> 00:18:04,190
Oh my God. Oh my God.

317
00:18:10,640 --> 00:18:12,530
I didn't mean for it to kill it.

318
00:18:17,480 --> 00:18:19,820
I just wanted that to be quiet.

319
00:18:21,960 --> 00:18:23,420
It was a baby.

320
00:18:24,740 --> 00:18:27,290
Great kids mothered around bay rate

321
00:18:29,670 --> 00:18:30,410
[inaudible].

322
00:18:30,410 --> 00:18:34,760
What Josh did is he asked
people the question,

323
00:18:34,761 --> 00:18:39,761
would you murder your own child
while they were in the brain scanner

324
00:18:41,780 --> 00:18:45,350
and adjust the moment when they were
trying to decide what they would do.

325
00:18:46,520 --> 00:18:48,040
He took pictures of their brains

326
00:18:50,930 --> 00:18:51,950
and what he saw,

327
00:18:52,190 --> 00:18:56,630
the contest we described
before was global in the brain.

328
00:18:56,750 --> 00:19:00,250
It was like a world war.
That gang of accountants,

329
00:19:00,310 --> 00:19:02,850
that part of the brain was
busy [inaudible] calculating,

330
00:19:03,030 --> 00:19:05,000
cut a whole village could die
of old village, could die.

331
00:19:05,390 --> 00:19:10,310
But the older and deeper reflects also
was lit up shouting, don't kill the baby.

332
00:19:10,470 --> 00:19:15,080
No, don't kill the baby. Go inside.
The brain was literally divided.

333
00:19:15,200 --> 00:19:19,910
Do the calculations. Don't kill
two different tribes in the brain.

334
00:19:19,911 --> 00:19:23,310
Literally trying to just
shout each other out. And Jen,

335
00:19:23,340 --> 00:19:26,820
this was a different kind of contest
than the ones we talked about before.

336
00:19:26,830 --> 00:19:29,970
Remember before when people were
pushing a man off of a bridge,

337
00:19:30,270 --> 00:19:34,170
overwhelmingly their brains
yelled, no, no, don't push the man.

338
00:19:34,380 --> 00:19:37,250
And when people were pulling the
lever overwhelmingly, yeah, yeah,

339
00:19:37,540 --> 00:19:39,720
pull the lever right there.
It was distinct here.

340
00:19:40,070 --> 00:19:41,840
I don't think really anybody wins.

341
00:19:42,740 --> 00:19:45,400
Well who breaks the time and they had
to answer something, right? Yeah. Yeah.

342
00:19:45,410 --> 00:19:49,970
That's a good question.
And now is there a,

343
00:19:50,910 --> 00:19:54,490
do you, what happens, is it just two
cries that fight each other out or

344
00:19:54,610 --> 00:19:55,490
is there a judge?

345
00:19:55,580 --> 00:19:58,340
Well that's an interesting question
and that's one of the things that we're

346
00:19:58,341 --> 00:19:59,050
looking at

347
00:19:59,050 --> 00:20:03,490
when you are in this moment with
parts of your brain contesting,

348
00:20:04,060 --> 00:20:07,930
there are two brain regions, these
two areas here towards the front,

349
00:20:07,960 --> 00:20:10,840
right behind your eyebrows,
left and right that light up

350
00:20:12,140 --> 00:20:12,750
[inaudible]

351
00:20:12,750 --> 00:20:15,750
and this is particular to us.
He showed me a slide that's

352
00:20:15,760 --> 00:20:16,420
those uh,

353
00:20:16,420 --> 00:20:19,570
sort of areas that are very highly
developed in humans as compared to other

354
00:20:19,571 --> 00:20:20,200
species.

355
00:20:20,200 --> 00:20:25,090
So when we have a problem that we
need to deliberate over the light,

356
00:20:25,140 --> 00:20:29,150
the front of the brain, this is
above my eyebrow sort of. Yeah. Yeah,

357
00:20:29,260 --> 00:20:31,300
right about there. And
there's two of them.

358
00:20:31,301 --> 00:20:33,340
One on the left and one
on the right, bilateral.

359
00:20:33,670 --> 00:20:37,990
And they are the things that monkeys
don't have as much of that. We have,

360
00:20:38,080 --> 00:20:41,050
certainly these parts of the brain are
more highly developed in humans. Oh,

361
00:20:41,051 --> 00:20:44,770
looking at these two flashes of
like at the front of a human brain,

362
00:20:44,980 --> 00:20:48,310
you could say, we are looking
at what makes us special.

363
00:20:49,530 --> 00:20:49,910
No.

364
00:20:49,910 --> 00:20:53,700
So that's a fair statement. They
human being wrestling with a problem.

365
00:20:53,760 --> 00:20:56,400
That's what that is. Yeah.
Where it's both emotional,

366
00:20:56,401 --> 00:20:59,310
but there's also a sort of irrational
attempt to sort of sort through those

367
00:20:59,311 --> 00:21:02,610
emotions. Those are the cases that are
showing more activity in that area.

368
00:21:03,210 --> 00:21:06,990
So in those cases, when these dots
above our eyebrows become active,

369
00:21:06,991 --> 00:21:07,750
what are they doing?

370
00:21:07,750 --> 00:21:12,400
Well, he doesn't know for sure, but what
he found is in these close contests,

371
00:21:12,401 --> 00:21:15,940
whenever those nodes
are very, very active,

372
00:21:16,030 --> 00:21:21,000
it appears that the calculating section
of the brain gets a bit of a boost.

373
00:21:22,210 --> 00:21:26,560
And the visceral inner chimp section
of the brain is kind of muffled.

374
00:21:27,160 --> 00:21:28,240
Oh Hey

375
00:21:29,700 --> 00:21:29,880
[inaudible]

376
00:21:29,880 --> 00:21:32,520
people who chose to kill their children,

377
00:21:32,940 --> 00:21:37,940
who made what is essentially a logical
decision over and over those subjects had

378
00:21:39,420 --> 00:21:44,380
brighter glows in these two areas
and longer glows in these two areas.

379
00:21:44,381 --> 00:21:48,880
So there is a definite association between
these two dots above the eyebrow and

380
00:21:48,881 --> 00:21:53,340
the power of the logical brain over
the inner chamber or the visceral

381
00:21:53,600 --> 00:21:55,620
right. Well, you know,
that's the hypothesis.

382
00:21:55,621 --> 00:21:59,280
So it's going to take a lot of more
research to sort of tease apart what these

383
00:21:59,281 --> 00:22:00,690
different parts of the brain are doing.

384
00:22:00,691 --> 00:22:03,780
Or if some of these are just sort of
activated in incidental kind of way.

385
00:22:03,781 --> 00:22:06,840
I mean we really don't know.
This is all, all very new

386
00:22:14,140 --> 00:22:14,240
[inaudible].

387
00:22:14,240 --> 00:22:17,870
Okay. So that was the story we put
together many, many, many years ago,

388
00:22:17,871 --> 00:22:19,010
about a decade ago.

389
00:22:19,870 --> 00:22:23,420
And at that point the whole idea of
thinking of morality is kind of purely a

390
00:22:23,421 --> 00:22:24,254
brain thing.

391
00:22:24,830 --> 00:22:29,090
It's relatively new and certainly the
idea of philosophers working with MRI

392
00:22:29,091 --> 00:22:30,380
machines, that was super new.

393
00:22:30,560 --> 00:22:33,920
But now here we are 10 years later
and some updates, uh, first of all,

394
00:22:33,921 --> 00:22:37,420
Josh Green. So in the, in the long,
long [inaudible] the theme of times,

395
00:22:37,421 --> 00:22:42,220
I assume now you have a three
giraffes, two Bob's rats and children.

396
00:22:42,490 --> 00:22:46,540
Yeah. So two kids and we're, we're close
to adding a cat. We talk to them again.

397
00:22:46,541 --> 00:22:51,130
He has started a family. He's switched
labs from Princeton to Harvard.

398
00:22:51,670 --> 00:22:53,770
But that whole time, that interim decade,

399
00:22:53,771 --> 00:22:57,130
he has still been thinking and
working on the trolley problem.

400
00:22:57,131 --> 00:22:59,560
Did you ever write the story
differently? Absolutely.

401
00:22:59,690 --> 00:23:02,830
So for years he's been trying out
different permutations of the scenario on

402
00:23:02,831 --> 00:23:03,664
people. Like okay,

403
00:23:03,860 --> 00:23:05,030
by firing freshman in college,

404
00:23:06,050 --> 00:23:08,450
instead of pushing the guy off
the bridge with your hands,

405
00:23:08,451 --> 00:23:10,610
what if you did it but
not with your hands.

406
00:23:10,611 --> 00:23:15,230
So in one version we asked people about
hitting a switch that opens a trap door

407
00:23:15,231 --> 00:23:18,440
on the foot bridge and drops the
person in one version of that.

408
00:23:18,441 --> 00:23:22,040
The switches right next to the person
in another version of the switch is far

409
00:23:22,041 --> 00:23:24,110
away and in yet another version,

410
00:23:24,111 --> 00:23:27,380
you're right next to the person and you
don't push them off with your hands,

411
00:23:27,560 --> 00:23:31,470
but you push them with a pole.
Oh, and to cut to the chase, uh,

412
00:23:31,640 --> 00:23:36,640
what Josh has found is it
the basic results that we
talked about that's roughly

413
00:23:36,681 --> 00:23:40,790
held up still the case that people would
like to save the most number of lives,

414
00:23:40,791 --> 00:23:45,410
but not if it means pushing somebody
with their own hands or with a Paul that

415
00:23:45,411 --> 00:23:47,960
matter. Now here's something
kind of interesting. Uh,

416
00:23:48,020 --> 00:23:53,020
he and others have found that there are
two groups that are more willing to push

417
00:23:53,401 --> 00:23:54,570
the guy off the bridge.

418
00:23:55,150 --> 00:23:58,450
They are Buddhist monks and psychopaths.

419
00:23:58,510 --> 00:24:01,930
I mean some people just don't care
very much about hurting other people.

420
00:24:01,931 --> 00:24:03,850
They don't have that kind
of an emotional response.

421
00:24:03,910 --> 00:24:04,960
That would be the cycle bass,

422
00:24:04,961 --> 00:24:09,640
where's the Buddhist monks presumably
are really good at shushing their inner

423
00:24:10,030 --> 00:24:12,790
champs. He called it and just
saying to themselves, you know,

424
00:24:12,820 --> 00:24:16,840
I'm aware that this is that killing
somebody is a terrible thing to do and I

425
00:24:16,841 --> 00:24:17,561
feel that,

426
00:24:17,561 --> 00:24:21,650
but I recognize that this has done
for a noble reason and therefore it's,

427
00:24:21,670 --> 00:24:22,690
it's okay,

428
00:24:23,220 --> 00:24:26,310
so there's all kinds of interesting things
you can say about the trolley problem

429
00:24:26,820 --> 00:24:27,990
as a thought experiment,

430
00:24:27,991 --> 00:24:30,270
but at the end of the day it's just
that it's a thought experiment.

431
00:24:30,570 --> 00:24:35,570
What got us interested in revisiting
it is that it seems like the thought

432
00:24:36,331 --> 00:24:38,170
experiment is about to get real.

433
00:24:45,200 --> 00:24:46,730
That's coming up right after the break.

434
00:24:58,140 --> 00:25:01,020
This is Amanda Darby calling
from Rockville, Maryland.

435
00:25:01,260 --> 00:25:06,150
Radiolab is supported in part by the
Alfred p Sloan Foundation enhancing public

436
00:25:06,151 --> 00:25:09,090
understanding of science and
technology in the modern world.

437
00:25:09,210 --> 00:25:13,920
More information
aboutSloan@wwwdotsloan.org

438
00:25:17,580 --> 00:25:19,230
I'm forming you as attorney Prepara.

439
00:25:19,500 --> 00:25:22,200
My new podcast is called
stay tuned with pre,

440
00:25:22,740 --> 00:25:24,840
it's about how power works in American.

441
00:25:25,170 --> 00:25:27,810
I talked to justice department
officials to federal judges,

442
00:25:27,960 --> 00:25:29,430
even to a former defense secretary.

443
00:25:29,700 --> 00:25:32,820
And I tell the story of what
happened when Donald Trump fired me.

444
00:25:33,090 --> 00:25:37,980
It's a collaboration between Wny c
studios, cafe and Pineapple Street media.

445
00:25:38,520 --> 00:25:40,120
Get it wherever you get your pocket.

446
00:25:40,170 --> 00:25:41,003
Yes,

447
00:25:50,090 --> 00:25:51,080
Chad, Robert Radiolab. Okay,

448
00:25:51,081 --> 00:25:53,930
so where we left it is it the
trolley problems about to get real.

449
00:25:54,410 --> 00:25:55,580
Here's how Josh Green put it.

450
00:25:55,670 --> 00:25:59,850
You know now as we're entering the
age of self driving cars, oh this is,

451
00:25:59,851 --> 00:26:02,510
this is like the trolley
problem. Now finally come to life

452
00:26:03,090 --> 00:26:04,940
owner's card. Oh,

453
00:26:05,300 --> 00:26:08,850
the future of the automobile
is here on us cars,

454
00:26:09,420 --> 00:26:13,170
autonomous vehicles. It's
here. Build this part first.

455
00:26:13,200 --> 00:26:18,200
Self-Driving Volvo will be offered
to customers in 2021 ah oh.

456
00:26:20,040 --> 00:26:24,090
Where's it going with this legislation
or the first of its kind focused on the

457
00:26:24,091 --> 00:26:27,090
car of the future. That is more
of a supercomputer on wheels.

458
00:26:29,580 --> 00:26:33,300
Okay, so self driving cars, unless
you've been living under a muffler,

459
00:26:33,470 --> 00:26:36,660
they are coming. It's going to be a little
bit of an adjustment for some of us.

460
00:26:37,760 --> 00:26:40,890
We hit the brakes. But what Josh meant

461
00:26:41,000 --> 00:26:44,190
when he said it's the trolley problem
come to life is basically this.

462
00:26:45,310 --> 00:26:49,230
Imagine this scenario.
Self-Driving card now is, uh,

463
00:26:49,300 --> 00:26:52,060
headed towards a bunch of
pedestrians in the road.

464
00:26:52,090 --> 00:26:54,160
The only way to save them
as to swerve out of the way,

465
00:26:54,161 --> 00:26:58,270
but that will run the car into a concrete
wall and it will kill the passenger in

466
00:26:58,271 --> 00:26:59,104
the car.

467
00:26:59,570 --> 00:26:59,710
Ah,

468
00:26:59,710 --> 00:27:02,200
what should the car do? Should
the car go straight and run over,

469
00:27:02,201 --> 00:27:04,730
say those five people or should
it swerve and, and, and, and,

470
00:27:04,740 --> 00:27:05,980
and kill the one person

471
00:27:06,440 --> 00:27:10,310
that suddenly is a real world question.

472
00:27:14,940 --> 00:27:15,030
[inaudible]

473
00:27:15,030 --> 00:27:19,170
if you ask people in the abstract like
what theoretically should a car in this

474
00:27:19,171 --> 00:27:21,600
situation do, they're
much more likely to say,

475
00:27:21,630 --> 00:27:24,360
I think you should sacrifice
one for the good of the many.

476
00:27:24,420 --> 00:27:27,240
They should just try to do the
most good or avoid the most harm.

477
00:27:27,420 --> 00:27:29,670
So if it's between one
driver and five pedestrians,

478
00:27:29,671 --> 00:27:33,060
logically it would be the
driver driver. Be Selfless.

479
00:27:33,270 --> 00:27:34,740
I think it should kill the driver.

480
00:27:36,210 --> 00:27:39,120
But when you ask people,
forget the theory,

481
00:27:39,150 --> 00:27:41,460
would you want to drive
in a car that would

482
00:27:41,810 --> 00:27:45,200
potentially sacrifice you to save the
lives of more people in order to minimize

483
00:27:45,201 --> 00:27:46,034
the total amount of harm?

484
00:27:46,190 --> 00:27:49,850
No, I wouldn't buy it. No. Absolutely not.

485
00:27:50,170 --> 00:27:53,380
That would kill me in
it. No. So I'm not going,

486
00:27:53,400 --> 00:27:57,020
I'm not going to buy a car that's
going to purposely kill me. Hell No.

487
00:27:57,650 --> 00:28:00,680
I wouldn't buy it for sure. No,

488
00:28:02,590 --> 00:28:03,680
I'll sell it, but I wouldn't buy it.

489
00:28:05,620 --> 00:28:06,660
So there's your problem.

490
00:28:07,050 --> 00:28:12,050
People would sell a car and an idea of
moral reasoning that they themselves

491
00:28:12,871 --> 00:28:17,370
wouldn't buy. And last fall
and exec at Mercedes-Benz

492
00:28:18,790 --> 00:28:19,623
face planted

493
00:28:22,320 --> 00:28:25,740
right into the middle
of this contradiction.

494
00:28:26,000 --> 00:28:28,320
Welcome to Paris, one of the most
beautiful cities in the world.

495
00:28:28,321 --> 00:28:31,980
And Welcome to the 2016 Paris motor show
home to some of the most beautiful cars

496
00:28:31,981 --> 00:28:32,490
in the world.

497
00:28:32,490 --> 00:28:36,150
Okay. October, 2016, uh,
the Paris motor show,

498
00:28:36,151 --> 00:28:40,110
you had something like a million people
coming in over the course of a few days.

499
00:28:40,111 --> 00:28:41,220
All the major car makers

500
00:28:41,630 --> 00:28:42,960
here is Ferrari. You can see

501
00:28:44,880 --> 00:28:48,420
the LaFerrari Abeta and of course
the new gt four seat Luso t

502
00:28:48,580 --> 00:28:51,190
everybody was debuting
their new cars. Endo.

503
00:28:51,191 --> 00:28:53,680
One of the big presenters in
this whole affair was this guy.

504
00:28:53,740 --> 00:28:57,130
In the future you'll have cars where you
don't even have to have your hands on

505
00:28:57,131 --> 00:28:58,440
the steering wheel anymore, but,

506
00:28:58,470 --> 00:29:02,020
but maybe you watch a movie on the head
up display or maybe you want to do your

507
00:29:02,021 --> 00:29:04,300
emails. That's really
what we're striving for.

508
00:29:04,500 --> 00:29:08,330
This is Christoph on Hugo, a senior
safety manager at Mercedes Benz.

509
00:29:08,331 --> 00:29:12,500
He was at the show sort of demonstrating
a prototype of a car that could sort of

510
00:29:12,501 --> 00:29:15,770
self-drive its way through traffic
and this e-class today, for example,

511
00:29:15,890 --> 00:29:18,770
he's got a maximum of
comfort and support systems.

512
00:29:18,800 --> 00:29:23,030
You'll actually look forward to being
stuck in traffic jams when you, of course,

513
00:29:23,031 --> 00:29:23,690
of course,

514
00:29:23,690 --> 00:29:26,270
he's doing dozens and dozens of interviews
through the show and in one of those

515
00:29:26,271 --> 00:29:29,420
interviews, unfortunately this one
we don't have on tape. He was asked,

516
00:29:29,421 --> 00:29:34,421
what would your driverless car do in a
trolley problem type dilemma or maybe you

517
00:29:35,691 --> 00:29:40,130
have to choose between one or
many and he answered quote,

518
00:29:40,900 --> 00:29:42,190
know you can save

519
00:29:42,940 --> 00:29:45,490
one person at least say that one.

520
00:29:45,790 --> 00:29:48,520
If you know you can save one
person, save that one person.

521
00:29:48,840 --> 00:29:50,140
I use the one in the car.

522
00:29:50,610 --> 00:29:53,610
This is Michael Taylor corresponder
for car and driver magazine.

523
00:29:53,611 --> 00:29:56,490
He was the one that Christoph
on. Hugo said that too.

524
00:29:56,630 --> 00:30:01,630
If you know for sure that one thing one
day is can be prevented and that's your

525
00:30:02,181 --> 00:30:03,170
first priority.

526
00:30:04,000 --> 00:30:06,900
Now, when he said this to you [inaudible],

527
00:30:06,970 --> 00:30:09,520
did it seem controversial
at all in the moment?

528
00:30:09,620 --> 00:30:12,020
In the moment it seems incredibly logical.

529
00:30:12,320 --> 00:30:17,020
I mean all he's really doing is saying
what's on people's minds, which is that,

530
00:30:17,430 --> 00:30:20,060
no, I wouldn't buy it.
Who's going to buy a car?

531
00:30:21,000 --> 00:30:25,460
The chooses somebody else over
them. Anyhow, he makes that comment.

532
00:30:26,060 --> 00:30:27,590
Michael Prince it and

533
00:30:29,280 --> 00:30:32,320
a kerfuffle ensues. Say
the one in the car. That's

534
00:30:33,850 --> 00:30:36,160
Christoph von Hugo from Mercedes. But
then when you look at the questions,

535
00:30:36,161 --> 00:30:39,700
you sound like a bit of a heel because
you want to save yourself as opposed to

536
00:30:39,701 --> 00:30:44,350
the pedestrian. Doesn't it ring though
of like just privilege? It does. Yeah,

537
00:30:44,490 --> 00:30:49,090
it does. What would you do? It's you or
a pedestrian and it's just, you know,

538
00:30:49,091 --> 00:30:51,640
I don't know anything about
this pedestrian. It's just
you were a pedestrian,

539
00:30:51,641 --> 00:30:53,170
just a regular guy
walking down the street,

540
00:30:53,850 --> 00:30:55,930
screw everyone who's not in a Mercedes.

541
00:30:56,060 --> 00:30:58,910
And there was this kind
of uproar about that, uh,

542
00:30:59,060 --> 00:31:02,570
how dare you drive these selfish, you
know, make these selfish cars. Uh,

543
00:31:02,720 --> 00:31:06,210
and then he walked it back and he
said, no, no. What I mean is that, uh,

544
00:31:06,230 --> 00:31:10,210
just that we, that we have a better chance
of protecting the people in the car,

545
00:31:10,211 --> 00:31:13,670
so we're going to protect them because
they're easier to protect. But of course,

546
00:31:14,060 --> 00:31:16,160
yeah, there's always going
to be trade offs. Yeah.

547
00:31:16,590 --> 00:31:19,950
And those tradeoffs could
get really, really tricky

548
00:31:22,890 --> 00:31:27,330
and subtle because obviously
these cars have sensors,

549
00:31:27,390 --> 00:31:31,200
sensors, like cameras, radars, lasers,

550
00:31:31,350 --> 00:31:34,110
and ultrasound sensors.
This is Raj Rajkumar.

551
00:31:34,111 --> 00:31:37,890
He's a professor at Carnegie Mellon.
I'm the, uh, code director of the GM,

552
00:31:37,891 --> 00:31:41,280
CMU connected and autonomous
driving collaborative research lab.

553
00:31:41,430 --> 00:31:45,870
He is one of the guys that is writing
the code that will go inside gms a

554
00:31:45,900 --> 00:31:47,310
driverless car. And he says, yeah,

555
00:31:47,700 --> 00:31:51,450
the sensors at the moment on these
cars still evolving. Pretty basic.

556
00:31:51,540 --> 00:31:55,500
We are very happy if today can
actually detect a pedestrian,

557
00:31:55,650 --> 00:31:59,280
can detect a bicyclist or motorcyclist,
different vehicles of different shapes,

558
00:31:59,281 --> 00:32:00,330
sizes and colors.

559
00:32:00,470 --> 00:32:04,740
But he says it won't be long before you
can actually know a lot more about who

560
00:32:04,741 --> 00:32:05,574
these people are.

561
00:32:05,610 --> 00:32:09,090
Eventually they will be able to detect
people of different sizes, shapes,

562
00:32:09,091 --> 00:32:12,060
and colors. Like, oh, that's a
skinny person. That's a small person,

563
00:32:12,061 --> 00:32:14,850
tall person, black person, white
person. That's a little boy.

564
00:32:14,851 --> 00:32:17,490
That's a little girl. So
forget the basic moral math.

565
00:32:17,491 --> 00:32:20,010
Like what does a car do
if it has to decide, oh,

566
00:32:20,011 --> 00:32:21,870
do I save this boy or this girl?

567
00:32:22,110 --> 00:32:25,680
What about two girls versus one boy
and an adult, but a cat versus a dog,

568
00:32:25,710 --> 00:32:29,850
a 75 year old guy in a suit versus that
person over there who might be homeless.

569
00:32:30,420 --> 00:32:33,750
You can see where this is going and
it's conceivable the cars will know our

570
00:32:33,751 --> 00:32:38,670
medical records. And back at the car
show we've also had that term car to car.

571
00:32:40,040 --> 00:32:44,240
Uh, also one of the enabling
technologies in highly automated driving.

572
00:32:44,270 --> 00:32:47,930
Mercedes guy basically said in a couple
of years the cars will be network,

573
00:32:47,931 --> 00:32:49,070
there'll be talking to each other.

574
00:32:49,250 --> 00:32:52,220
So just imagine a scenario where like
cars are about to get into accidents and

575
00:32:52,221 --> 00:32:54,770
right at the decision point
they're like conferring.

576
00:32:54,771 --> 00:32:56,150
Well who do you have in your car? Me,

577
00:32:56,290 --> 00:32:59,380
I've got a 70 year old Wall Street guy
makes eight figures. How about you? Well,

578
00:32:59,510 --> 00:33:02,480
I'm a bus full of kids. Kids have
more years left. You need to move.

579
00:33:02,720 --> 00:33:03,260
We'll hold up.

580
00:33:03,260 --> 00:33:05,660
I see that your kids come from a
poor neighborhood and have asthma.

581
00:33:05,661 --> 00:33:06,710
So I don't know.

582
00:33:06,810 --> 00:33:11,430
So you can basically tie yourself up and
knots a wrap yourself around an axle.

583
00:33:11,940 --> 00:33:16,940
We do not think that any programmer
should be given this major a burden of

584
00:33:17,191 --> 00:33:19,120
deciding who survives and who gets killed.

585
00:33:19,200 --> 00:33:23,610
I think these are a very
fundamental, deep, uh,

586
00:33:24,030 --> 00:33:27,270
issues that society
has to decide at large.

587
00:33:27,510 --> 00:33:32,020
I don't think a programmer eating pizza
and sipping coke should be making the

588
00:33:32,021 --> 00:33:32,854
call.

589
00:33:33,740 --> 00:33:36,860
How does society decide? I
mean, help me imagine that.

590
00:33:36,980 --> 00:33:38,840
I think it clearly has to
be an evolutionary process.

591
00:33:38,841 --> 00:33:42,170
I believe Raj told us two things
basically need to happen. First.

592
00:33:42,171 --> 00:33:44,000
We need to get these
Robo cars on the road,

593
00:33:44,001 --> 00:33:47,900
get more experienced with how they
interact with us human drivers and how we

594
00:33:47,901 --> 00:33:52,901
interact with them and to their need
to be like industry wide summit.

595
00:33:53,270 --> 00:33:55,340
No one company is going to solve that.

596
00:33:55,590 --> 00:33:58,510
This is Bill Ford Jr of
the Ford Company, uh,

597
00:33:58,590 --> 00:34:02,400
giving a speech in October of
2016 at the Economic Club of DC.

598
00:34:02,520 --> 00:34:03,660
And we have to have,

599
00:34:03,690 --> 00:34:08,070
because he could you imagine if we had
one algorithm and Toyota had another and

600
00:34:08,071 --> 00:34:09,210
general motors had another,

601
00:34:09,211 --> 00:34:11,430
I mean it would be amiable
obviously couldn't do that.

602
00:34:11,870 --> 00:34:15,650
What if the Tibetan cards
make one decision in the
American cars make another.

603
00:34:15,760 --> 00:34:20,320
So we need to have a national
discussion on ethics I think of,

604
00:34:20,321 --> 00:34:23,320
because we've never had to
think of these things before,

605
00:34:23,321 --> 00:34:26,110
but the cars will have the
time and the ability to do that

606
00:34:26,860 --> 00:34:27,693
live a colleague,

607
00:34:28,930 --> 00:34:30,520
boys this pasta so far,

608
00:34:30,521 --> 00:34:34,930
Germany is the only country that we
know of that has tackled this head on.

609
00:34:36,520 --> 00:34:41,470
One of the most significant points the
ethics commission made is that autonomous

610
00:34:41,500 --> 00:34:44,340
and connected driving is
an ethical imperative.

611
00:34:44,770 --> 00:34:48,430
The government has released a code of
ethics that says among other things that

612
00:34:48,431 --> 00:34:53,431
self driving cars are forbidden to
discriminate between humans and almost any

613
00:34:53,441 --> 00:34:56,650
way not on race, not
on gender, not on age,

614
00:34:56,651 --> 00:34:59,560
nothing shouldn't be
programmed into the cars.

615
00:34:59,650 --> 00:35:03,870
One can imagine a few
classes being added, uh,

616
00:35:04,600 --> 00:35:06,100
in the Geneva Convention, if you will,

617
00:35:06,101 --> 00:35:09,670
of what these automated vehicles should
do. A globally accepted standard,

618
00:35:09,760 --> 00:35:10,360
if you will.

619
00:35:10,360 --> 00:35:15,360
How we get there to that globally accepted
standard is anyone's guess and what

620
00:35:15,491 --> 00:35:16,390
it will look like.

621
00:35:16,450 --> 00:35:21,100
Whether it'll be like a coherent set
of rules or like rife with the kind of

622
00:35:21,101 --> 00:35:25,270
contradictions we see in our own
brain. That also remains to be seen,

623
00:35:25,880 --> 00:35:27,140
but one thing is clear.

624
00:35:31,330 --> 00:35:35,080
Oh, there are cars coming.

625
00:35:35,800 --> 00:35:38,100
Build this with their
questions. Let me back from

626
00:35:38,280 --> 00:35:42,300
me controlling it. Oh
dear Jesus. I could never,

627
00:35:44,250 --> 00:35:49,150
oh, where's it going? Oh my God.

628
00:35:50,020 --> 00:35:50,201
Okay.

629
00:35:50,201 --> 00:35:54,040
We do need to caveat all this by saying
that the moral dilemma we're talking

630
00:35:54,041 --> 00:35:57,670
about in the case of these driverless
cars is going to be super rare.

631
00:35:58,210 --> 00:36:03,210
Mostly what will probably happen is
that like the plane loads full of people

632
00:36:04,001 --> 00:36:07,930
that die every day from car accidents.

633
00:36:07,960 --> 00:36:12,400
Well that's just going to hit the floor
and so you have to balance the few cases

634
00:36:12,760 --> 00:36:16,570
where a car might make a decision you
don't like against the massive number of

635
00:36:16,571 --> 00:36:17,470
lives saved.

636
00:36:17,900 --> 00:36:20,030
I think I actually have a a
different thing I was thinking,

637
00:36:20,840 --> 00:36:24,650
even though you dramatically
bring down the, the,

638
00:36:25,190 --> 00:36:27,800
the number of bad things
that happen on road,

639
00:36:27,950 --> 00:36:29,960
you dramatically bring
down the collisions.

640
00:36:29,961 --> 00:36:31,760
You dramatically bring down the mortality.

641
00:36:32,540 --> 00:36:36,770
You dramatically lower the number of
people who are drunk coming home from a

642
00:36:36,771 --> 00:36:40,610
party and just ram someone sideways and
killing three of them and injuring two

643
00:36:40,611 --> 00:36:43,970
of them for the rest of their lives.
Those kinds of things go way down,

644
00:36:44,720 --> 00:36:47,030
but they're the ones that remain,

645
00:36:47,060 --> 00:36:50,610
are engineered like
they are calculated, uh,

646
00:36:50,930 --> 00:36:52,310
almost with foresight.

647
00:36:52,990 --> 00:36:56,540
[inaudible] so here's the difference and
this is just an interesting difference.

648
00:36:56,541 --> 00:36:58,610
Like Ah Damn,

649
00:36:58,611 --> 00:37:02,330
that's so sad that happened that that
guy got drunk and kind of that maybe he

650
00:37:02,331 --> 00:37:03,164
should go to jail.

651
00:37:03,770 --> 00:37:07,610
But you mean that this
society engineered this in.

652
00:37:08,540 --> 00:37:09,920
That is a big difference.

653
00:37:09,921 --> 00:37:14,690
One is operatic and seems like the
forces of destiny and the other seems

654
00:37:14,691 --> 00:37:17,790
mechanical and pre fought through.

655
00:37:17,870 --> 00:37:22,870
He meditated and there's something dark
about a premeditated expected death and

656
00:37:23,931 --> 00:37:26,940
I don't know what you do about that.
Everybody's on the hook for that.

657
00:37:27,030 --> 00:37:27,500
The killers

658
00:37:27,500 --> 00:37:30,710
in the particular as it feels dark,
it's a little bit like when you know,

659
00:37:30,711 --> 00:37:32,660
should you kill your own
baby? Did you save the villas?

660
00:37:32,690 --> 00:37:35,630
Like in the particular instance
of that one child, it's dark,

661
00:37:35,960 --> 00:37:38,630
but against the backdrop of the
life saved. It's just a tiny bit

662
00:37:38,670 --> 00:37:41,310
pinprick of darkness. That's all.
Yeah, but you know how humans are.

663
00:37:41,370 --> 00:37:44,910
If you argue back that yes,

664
00:37:44,911 --> 00:37:48,390
a bunch of smarty pants is
concocted a mathematical formula,

665
00:37:48,391 --> 00:37:50,610
which meant that some people
had to die in here they are.

666
00:37:50,820 --> 00:37:54,450
There are many fewer than
before a human being.

667
00:37:54,451 --> 00:37:56,040
Just like Josh would tell you,

668
00:37:56,580 --> 00:37:59,040
we'd have a roar of feeling
and of anger and saying,

669
00:37:59,220 --> 00:38:02,820
how dare you engineer this
in? No, no, no, no, no.

670
00:38:02,900 --> 00:38:07,640
And that human being needs to meditate
like the monks to silence that feeling

671
00:38:07,670 --> 00:38:10,460
because the feeling in that case,
it's just getting in the way.

672
00:38:10,590 --> 00:38:15,540
Yes and no, and that may be impossible
unless you're a monk for God's sake.

673
00:38:17,130 --> 00:38:21,870
See, we're right back where we started
down. Yeah. All right. We should go.

674
00:38:21,930 --> 00:38:23,520
Chad, you have to thank some people. No,

675
00:38:23,730 --> 00:38:24,000
yes.

676
00:38:24,000 --> 00:38:27,720
A this piece was produced by Amanda
Arran check with help from Bethel Hob Tay

677
00:38:28,200 --> 00:38:29,970
special thanks to e odd Rawan,

678
00:38:30,000 --> 00:38:33,630
Edmond Awad and Sidney Levine from
the moral machine group at MIT.

679
00:38:33,930 --> 00:38:36,940
Also thanks to Suretec Karaman Chin Chang

680
00:38:36,940 --> 00:38:40,900
and robo race for all their help,
and I guess we should go now. Yeah,

681
00:38:41,770 --> 00:38:45,400
I'll, um, I'm Jad Abumrad
not coming into your car.

682
00:38:46,930 --> 00:38:48,610
You don't mind? Just take my own,

683
00:38:49,300 --> 00:38:52,150
I'm going to rig up an autonomous
vehicle to the bottom of your bed,

684
00:38:53,520 --> 00:38:57,100
so you're going to go to bed and suddenly
find yourself on the highway driving

685
00:38:57,101 --> 00:38:59,010
you wherever I want

686
00:39:01,750 --> 00:39:04,900
anyhow. Okay. We should go. Yeah,
I'm [inaudible] I'm Robert Krulwich.

687
00:39:05,170 --> 00:39:05,800
Thanks for listening.

