WEBVTT
Kind: captions
Language: en

00:00:07.440 --> 00:00:11.970
 
we really as engineers need to take

00:00:11.970 --> 00:00:11.980
we really as engineers need to take
 

00:00:11.980 --> 00:00:13.890
we really as engineers need to take
responsibility to work on the things

00:00:13.890 --> 00:00:13.900
responsibility to work on the things
 

00:00:13.900 --> 00:00:15.480
responsibility to work on the things
that will make the world a better place

00:00:15.480 --> 00:00:15.490
that will make the world a better place
 

00:00:15.490 --> 00:00:18.480
that will make the world a better place
taking responsibility having a research

00:00:18.480 --> 00:00:18.490
taking responsibility having a research
 

00:00:18.490 --> 00:00:21.089
taking responsibility having a research
strategy instead of just going wherever

00:00:21.089 --> 00:00:21.099
strategy instead of just going wherever
 

00:00:21.099 --> 00:00:23.460
strategy instead of just going wherever
your curiosity takes you AI and

00:00:23.460 --> 00:00:23.470
your curiosity takes you AI and
 

00:00:23.470 --> 00:00:25.380
your curiosity takes you AI and
optimization have very different

00:00:25.380 --> 00:00:25.390
optimization have very different
 

00:00:25.390 --> 00:00:27.929
optimization have very different
strengths than humans do humans are

00:00:27.929 --> 00:00:27.939
strengths than humans do humans are
 

00:00:27.939 --> 00:00:29.790
strengths than humans do humans are
really good at actually figuring out

00:00:29.790 --> 00:00:29.800
really good at actually figuring out
 

00:00:29.800 --> 00:00:32.249
really good at actually figuring out
what the ins are what the value

00:00:32.249 --> 00:00:32.259
what the ins are what the value
 

00:00:32.259 --> 00:00:34.170
what the ins are what the value
judgments are in general how it should

00:00:34.170 --> 00:00:34.180
judgments are in general how it should
 

00:00:34.180 --> 00:00:35.730
judgments are in general how it should
trade-off between different things like

00:00:35.730 --> 00:00:35.740
trade-off between different things like
 

00:00:35.740 --> 00:00:37.770
trade-off between different things like
various forms of efficiency and various

00:00:37.770 --> 00:00:37.780
various forms of efficiency and various
 

00:00:37.780 --> 00:00:41.190
various forms of efficiency and various
forms of fairness but he must think of

00:00:41.190 --> 00:00:41.200
forms of fairness but he must think of
 

00:00:41.200 --> 00:00:43.680
forms of fairness but he must think of
them in the context of special cases and

00:00:43.680 --> 00:00:43.690
them in the context of special cases and
 

00:00:43.690 --> 00:00:46.950
them in the context of special cases and
humans are very bad as actually sifting

00:00:46.950 --> 00:00:46.960
humans are very bad as actually sifting
 

00:00:46.960 --> 00:00:48.990
humans are very bad as actually sifting
through all of the possible special

00:00:48.990 --> 00:00:49.000
through all of the possible special
 

00:00:49.000 --> 00:00:51.330
through all of the possible special
cases of which there are more than the

00:00:51.330 --> 00:00:51.340
cases of which there are more than the
 

00:00:51.340 --> 00:00:53.189
cases of which there are more than the
number of atoms in the universe we

00:00:53.189 --> 00:00:53.199
number of atoms in the universe we
 

00:00:53.199 --> 00:00:54.869
number of atoms in the universe we
developed this new framework which we

00:00:54.869 --> 00:00:54.879
developed this new framework which we
 

00:00:54.879 --> 00:00:57.630
developed this new framework which we
call future match where we take as input

00:00:57.630 --> 00:00:57.640
call future match where we take as input
 

00:00:57.640 --> 00:01:00.599
call future match where we take as input
the humans value judgments and then

00:01:00.599 --> 00:01:00.609
the humans value judgments and then
 

00:01:00.609 --> 00:01:05.060
the humans value judgments and then
using AI based simulation and past data

00:01:05.060 --> 00:01:05.070
using AI based simulation and past data
 

00:01:05.070 --> 00:01:08.010
using AI based simulation and past data
we can actually optimize the policy

00:01:08.010 --> 00:01:08.020
we can actually optimize the policy
 

00:01:08.020 --> 00:01:12.480
we can actually optimize the policy
parameters as to how do you best achieve

00:01:12.480 --> 00:01:12.490
parameters as to how do you best achieve
 

00:01:12.490 --> 00:01:14.520
parameters as to how do you best achieve
those goals so we're separating the

00:01:14.520 --> 00:01:14.530
those goals so we're separating the
 

00:01:14.530 --> 00:01:16.620
those goals so we're separating the
means and the ends the humans are

00:01:16.620 --> 00:01:16.630
means and the ends the humans are
 

00:01:16.630 --> 00:01:18.660
means and the ends the humans are
talking about ends and the AI is

00:01:18.660 --> 00:01:18.670
talking about ends and the AI is
 

00:01:18.670 --> 00:01:20.850
talking about ends and the AI is
figuring out the means and I think that

00:01:20.850 --> 00:01:20.860
figuring out the means and I think that
 

00:01:20.860 --> 00:01:24.450
figuring out the means and I think that
this is a very important separation for

00:01:24.450 --> 00:01:24.460
this is a very important separation for
 

00:01:24.460 --> 00:01:35.550
this is a very important separation for
the future in many different areas of AI

00:01:35.550 --> 00:01:35.560
 
 

00:01:35.560 --> 00:01:37.619
 
you

