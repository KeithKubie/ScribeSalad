WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.819
and we're going to have now a panel

00:00:01.819 --> 00:00:01.829
and we're going to have now a panel
 

00:00:01.829 --> 00:00:04.070
and we're going to have now a panel
addressing the question of whether we

00:00:04.070 --> 00:00:04.080
addressing the question of whether we
 

00:00:04.080 --> 00:00:06.289
addressing the question of whether we
should fear or welcome the coming

00:00:06.289 --> 00:00:06.299
should fear or welcome the coming
 

00:00:06.299 --> 00:00:08.419
should fear or welcome the coming
singularity or indeed whether it will

00:00:08.419 --> 00:00:08.429
singularity or indeed whether it will
 

00:00:08.429 --> 00:00:12.230
singularity or indeed whether it will
come so may I welcome the panelists for

00:00:12.230 --> 00:00:12.240
come so may I welcome the panelists for
 

00:00:12.240 --> 00:00:14.419
come so may I welcome the panelists for
that we have as magnetars moderator

00:00:14.419 --> 00:00:14.429
that we have as magnetars moderator
 

00:00:14.429 --> 00:00:16.010
that we have as magnetars moderator
maggie Bowden you've already met this

00:00:16.010 --> 00:00:16.020
maggie Bowden you've already met this
 

00:00:16.020 --> 00:00:19.580
maggie Bowden you've already met this
morning and then we have Ray Kurzweil

00:00:19.580 --> 00:00:19.590
morning and then we have Ray Kurzweil
 

00:00:19.590 --> 00:00:21.710
morning and then we have Ray Kurzweil
who you know Harry Shum who is vice

00:00:21.710 --> 00:00:21.720
who you know Harry Shum who is vice
 

00:00:21.720 --> 00:00:24.620
who you know Harry Shum who is vice
president for research at Microsoft Max

00:00:24.620 --> 00:00:24.630
president for research at Microsoft Max
 

00:00:24.630 --> 00:00:27.380
president for research at Microsoft Max
tegmark from MIT and the future of life

00:00:27.380 --> 00:00:27.390
tegmark from MIT and the future of life
 

00:00:27.390 --> 00:00:32.209
tegmark from MIT and the future of life
Institute max flex video and Stuart

00:00:32.209 --> 00:00:32.219
Institute max flex video and Stuart
 

00:00:32.219 --> 00:00:35.569
Institute max flex video and Stuart
Russell from Berkeley so Maggie over to

00:00:35.569 --> 00:00:35.579
Russell from Berkeley so Maggie over to
 

00:00:35.579 --> 00:00:43.369
Russell from Berkeley so Maggie over to
you okay well hello again everybody

00:00:43.369 --> 00:00:43.379
you okay well hello again everybody
 

00:00:43.379 --> 00:00:47.600
you okay well hello again everybody
what is the singularity it's an idea

00:00:47.600 --> 00:00:47.610
what is the singularity it's an idea
 

00:00:47.610 --> 00:00:50.090
what is the singularity it's an idea
it's the supposed point at which

00:00:50.090 --> 00:00:50.100
it's the supposed point at which
 

00:00:50.100 --> 00:00:52.189
it's the supposed point at which
artificial intelligence will not only

00:00:52.189 --> 00:00:52.199
artificial intelligence will not only
 

00:00:52.199 --> 00:00:54.979
artificial intelligence will not only
reach but actually surpass human

00:00:54.979 --> 00:00:54.989
reach but actually surpass human
 

00:00:54.989 --> 00:00:57.470
reach but actually surpass human
intelligence and thereby become able to

00:00:57.470 --> 00:00:57.480
intelligence and thereby become able to
 

00:00:57.480 --> 00:01:02.209
intelligence and thereby become able to
make itself even better and the idea is

00:01:02.209 --> 00:01:02.219
make itself even better and the idea is
 

00:01:02.219 --> 00:01:05.119
make itself even better and the idea is
that after that time most of these

00:01:05.119 --> 00:01:05.129
that after that time most of these
 

00:01:05.129 --> 00:01:07.219
that after that time most of these
science certainly the exciting science

00:01:07.219 --> 00:01:07.229
science certainly the exciting science
 

00:01:07.229 --> 00:01:10.550
science certainly the exciting science
will be done by AI much of the art will

00:01:10.550 --> 00:01:10.560
will be done by AI much of the art will
 

00:01:10.560 --> 00:01:14.359
will be done by AI much of the art will
be done by AI and the problems of not

00:01:14.359 --> 00:01:14.369
be done by AI and the problems of not
 

00:01:14.369 --> 00:01:15.980
be done by AI and the problems of not
only myths and but government

00:01:15.980 --> 00:01:15.990
only myths and but government
 

00:01:15.990 --> 00:01:19.030
only myths and but government
international politics anything really

00:01:19.030 --> 00:01:19.040
international politics anything really
 

00:01:19.040 --> 00:01:22.940
international politics anything really
will be taken over certainly helped

00:01:22.940 --> 00:01:22.950
will be taken over certainly helped
 

00:01:22.950 --> 00:01:25.719
will be taken over certainly helped
perhaps taken over by AI now this is a

00:01:25.719 --> 00:01:25.729
perhaps taken over by AI now this is a
 

00:01:25.729 --> 00:01:30.890
perhaps taken over by AI now this is a
hugely controversial idea people

00:01:30.890 --> 00:01:30.900
hugely controversial idea people
 

00:01:30.900 --> 00:01:32.780
hugely controversial idea people
disagree very much about whether it

00:01:32.780 --> 00:01:32.790
disagree very much about whether it
 

00:01:32.790 --> 00:01:35.929
disagree very much about whether it
could happen they disagree about whether

00:01:35.929 --> 00:01:35.939
could happen they disagree about whether
 

00:01:35.939 --> 00:01:39.410
could happen they disagree about whether
it will happen they disagree as you've

00:01:39.410 --> 00:01:39.420
it will happen they disagree as you've
 

00:01:39.420 --> 00:01:42.620
it will happen they disagree as you've
just heard in the question time about

00:01:42.620 --> 00:01:42.630
just heard in the question time about
 

00:01:42.630 --> 00:01:45.410
just heard in the question time about
whether it might sorry when it might

00:01:45.410 --> 00:01:45.420
whether it might sorry when it might
 

00:01:45.420 --> 00:01:48.710
whether it might sorry when it might
happen and they disagree very much about

00:01:48.710 --> 00:01:48.720
happen and they disagree very much about
 

00:01:48.720 --> 00:01:50.120
happen and they disagree very much about
whether it would be a bad or a good

00:01:50.120 --> 00:01:50.130
whether it would be a bad or a good
 

00:01:50.130 --> 00:01:52.249
whether it would be a bad or a good
thing and that's what we're going to be

00:01:52.249 --> 00:01:52.259
thing and that's what we're going to be
 

00:01:52.259 --> 00:01:54.410
thing and that's what we're going to be
focusing on should we welcome it or

00:01:54.410 --> 00:01:54.420
focusing on should we welcome it or
 

00:01:54.420 --> 00:01:57.980
focusing on should we welcome it or
should we fear it and Ray's going to

00:01:57.980 --> 00:01:57.990
should we fear it and Ray's going to
 

00:01:57.990 --> 00:02:00.109
should we fear it and Ray's going to
kick us off I suspect Ray he's going to

00:02:00.109 --> 00:02:00.119
kick us off I suspect Ray he's going to
 

00:02:00.119 --> 00:02:02.359
kick us off I suspect Ray he's going to
say that bad laws we should welcome it

00:02:02.359 --> 00:02:02.369
say that bad laws we should welcome it
 

00:02:02.369 --> 00:02:04.700
say that bad laws we should welcome it
and perhaps the others are going to jump

00:02:04.700 --> 00:02:04.710
and perhaps the others are going to jump
 

00:02:04.710 --> 00:02:05.890
and perhaps the others are going to jump
in and say well

00:02:05.890 --> 00:02:05.900
in and say well
 

00:02:05.900 --> 00:02:08.080
in and say well
not so fast so let's see what happens

00:02:08.080 --> 00:02:08.090
not so fast so let's see what happens
 

00:02:08.090 --> 00:02:11.620
not so fast so let's see what happens
right well I've been accused of being an

00:02:11.620 --> 00:02:11.630
right well I've been accused of being an
 

00:02:11.630 --> 00:02:14.710
right well I've been accused of being an
optimist before I guess you have to be

00:02:14.710 --> 00:02:14.720
optimist before I guess you have to be
 

00:02:14.720 --> 00:02:16.420
optimist before I guess you have to be
an optimist to be an inventor and an

00:02:16.420 --> 00:02:16.430
an optimist to be an inventor and an
 

00:02:16.430 --> 00:02:18.430
an optimist to be an inventor and an
entrepreneur but actually wrote

00:02:18.430 --> 00:02:18.440
entrepreneur but actually wrote
 

00:02:18.440 --> 00:02:20.830
entrepreneur but actually wrote
extensively starting in the 1990s about

00:02:20.830 --> 00:02:20.840
extensively starting in the 1990s about
 

00:02:20.840 --> 00:02:22.840
extensively starting in the 1990s about
the downsides and of course every

00:02:22.840 --> 00:02:22.850
the downsides and of course every
 

00:02:22.850 --> 00:02:26.440
the downsides and of course every
technology back to fire has had promise

00:02:26.440 --> 00:02:26.450
technology back to fire has had promise
 

00:02:26.450 --> 00:02:29.770
technology back to fire has had promise
in peril fire kept us warm cooked our

00:02:29.770 --> 00:02:29.780
in peril fire kept us warm cooked our
 

00:02:29.780 --> 00:02:31.840
in peril fire kept us warm cooked our
food we're also burned down our houses

00:02:31.840 --> 00:02:31.850
food we're also burned down our houses
 

00:02:31.850 --> 00:02:34.150
food we're also burned down our houses
we have much more powerful technologies

00:02:34.150 --> 00:02:34.160
we have much more powerful technologies
 

00:02:34.160 --> 00:02:36.580
we have much more powerful technologies
today so the promise in parellel are are

00:02:36.580 --> 00:02:36.590
today so the promise in parellel are are
 

00:02:36.590 --> 00:02:41.560
today so the promise in parellel are are
much more powerful recently this was a

00:02:41.560 --> 00:02:41.570
much more powerful recently this was a
 

00:02:41.570 --> 00:02:43.690
much more powerful recently this was a
lot of concern expressed by Stephen

00:02:43.690 --> 00:02:43.700
lot of concern expressed by Stephen
 

00:02:43.700 --> 00:02:45.460
lot of concern expressed by Stephen
Hawking and Elon Musk and others about

00:02:45.460 --> 00:02:45.470
Hawking and Elon Musk and others about
 

00:02:45.470 --> 00:02:47.890
Hawking and Elon Musk and others about
the dire dangers of artificial

00:02:47.890 --> 00:02:47.900
the dire dangers of artificial
 

00:02:47.900 --> 00:02:50.980
the dire dangers of artificial
intelligence those dangers are real

00:02:50.980 --> 00:02:50.990
intelligence those dangers are real
 

00:02:50.990 --> 00:02:53.980
intelligence those dangers are real
I wrote a response really along these

00:02:53.980 --> 00:02:53.990
I wrote a response really along these
 

00:02:53.990 --> 00:02:56.170
I wrote a response really along these
lines that first of all we need to

00:02:56.170 --> 00:02:56.180
lines that first of all we need to
 

00:02:56.180 --> 00:02:59.380
lines that first of all we need to
pursue AI we've heard a lot today about

00:02:59.380 --> 00:02:59.390
pursue AI we've heard a lot today about
 

00:02:59.390 --> 00:03:01.810
pursue AI we've heard a lot today about
how we're diagnosing disease curing

00:03:01.810 --> 00:03:01.820
how we're diagnosing disease curing
 

00:03:01.820 --> 00:03:04.560
how we're diagnosing disease curing
disease cleaning up the environment

00:03:04.560 --> 00:03:04.570
disease cleaning up the environment
 

00:03:04.570 --> 00:03:06.790
disease cleaning up the environment
overcoming poverty with artificial

00:03:06.790 --> 00:03:06.800
overcoming poverty with artificial
 

00:03:06.800 --> 00:03:10.270
overcoming poverty with artificial
intelligence of the three new

00:03:10.270 --> 00:03:10.280
intelligence of the three new
 

00:03:10.280 --> 00:03:12.010
intelligence of the three new
information technologies that are going

00:03:12.010 --> 00:03:12.020
information technologies that are going
 

00:03:12.020 --> 00:03:14.650
information technologies that are going
to transform the world biotechnology

00:03:14.650 --> 00:03:14.660
to transform the world biotechnology
 

00:03:14.660 --> 00:03:16.540
to transform the world biotechnology
which I talked about reprogramming agent

00:03:16.540 --> 00:03:16.550
which I talked about reprogramming agent
 

00:03:16.550 --> 00:03:18.580
which I talked about reprogramming agent
software in our bodies nanotechnology

00:03:18.580 --> 00:03:18.590
software in our bodies nanotechnology
 

00:03:18.590 --> 00:03:21.190
software in our bodies nanotechnology
reprogramming materials and artificial

00:03:21.190 --> 00:03:21.200
reprogramming materials and artificial
 

00:03:21.200 --> 00:03:23.920
reprogramming materials and artificial
intelligence the one that's actually an

00:03:23.920 --> 00:03:23.930
intelligence the one that's actually an
 

00:03:23.930 --> 00:03:25.860
intelligence the one that's actually an
existential risk already is

00:03:25.860 --> 00:03:25.870
existential risk already is
 

00:03:25.870 --> 00:03:28.300
existential risk already is
biotechnology the same technologies

00:03:28.300 --> 00:03:28.310
biotechnology the same technologies
 

00:03:28.310 --> 00:03:31.240
biotechnology the same technologies
we're using to reprogram biology away

00:03:31.240 --> 00:03:31.250
we're using to reprogram biology away
 

00:03:31.250 --> 00:03:32.979
we're using to reprogram biology away
from cancer and disease could also be

00:03:32.979 --> 00:03:32.989
from cancer and disease could also be
 

00:03:32.989 --> 00:03:35.140
from cancer and disease could also be
used by bioterrorist to create a new

00:03:35.140 --> 00:03:35.150
used by bioterrorist to create a new
 

00:03:35.150 --> 00:03:37.840
used by bioterrorist to create a new
super weapon so this has recognized a

00:03:37.840 --> 00:03:37.850
super weapon so this has recognized a
 

00:03:37.850 --> 00:03:40.930
super weapon so this has recognized a
couple of decades ago the industry in

00:03:40.930 --> 00:03:40.940
couple of decades ago the industry in
 

00:03:40.940 --> 00:03:42.970
couple of decades ago the industry in
the field developed the so-called

00:03:42.970 --> 00:03:42.980
the field developed the so-called
 

00:03:42.980 --> 00:03:45.970
the field developed the so-called
Asilomar guidelines we're now actually

00:03:45.970 --> 00:03:45.980
Asilomar guidelines we're now actually
 

00:03:45.980 --> 00:03:47.890
Asilomar guidelines we're now actually
seeing the benefits of biotechnology the

00:03:47.890 --> 00:03:47.900
seeing the benefits of biotechnology the
 

00:03:47.900 --> 00:03:49.449
seeing the benefits of biotechnology the
beginning to trickle out this will be a

00:03:49.449 --> 00:03:49.459
beginning to trickle out this will be a
 

00:03:49.459 --> 00:03:52.300
beginning to trickle out this will be a
flood over the next decade so far we've

00:03:52.300 --> 00:03:52.310
flood over the next decade so far we've
 

00:03:52.310 --> 00:03:57.430
flood over the next decade so far we've
seen no dangers as max pointed out the

00:03:57.430 --> 00:03:57.440
seen no dangers as max pointed out the
 

00:03:57.440 --> 00:03:59.199
seen no dangers as max pointed out the
dinner last night that doesn't mean we

00:03:59.199 --> 00:03:59.209
dinner last night that doesn't mean we
 

00:03:59.209 --> 00:04:01.990
dinner last night that doesn't mean we
can cross it off our concern list was it

00:04:01.990 --> 00:04:02.000
can cross it off our concern list was it
 

00:04:02.000 --> 00:04:03.970
can cross it off our concern list was it
was pointed out at dinner that these are

00:04:03.970 --> 00:04:03.980
was pointed out at dinner that these are
 

00:04:03.980 --> 00:04:05.800
was pointed out at dinner that these are
seldom our guidelines are well one side

00:04:05.800 --> 00:04:05.810
seldom our guidelines are well one side
 

00:04:05.810 --> 00:04:06.699
seldom our guidelines are well one side
of the table said they're already

00:04:06.699 --> 00:04:06.709
of the table said they're already
 

00:04:06.709 --> 00:04:08.979
of the table said they're already
obsolete another part of the table said

00:04:08.979 --> 00:04:08.989
obsolete another part of the table said
 

00:04:08.989 --> 00:04:10.540
obsolete another part of the table said
well they're about to be obsolete but

00:04:10.540 --> 00:04:10.550
well they're about to be obsolete but
 

00:04:10.550 --> 00:04:12.580
well they're about to be obsolete but
that's always the case we have to

00:04:12.580 --> 00:04:12.590
that's always the case we have to
 

00:04:12.590 --> 00:04:15.490
that's always the case we have to
recreate these ethical standards in

00:04:15.490 --> 00:04:15.500
recreate these ethical standards in
 

00:04:15.500 --> 00:04:18.250
recreate these ethical standards in
these technology defense systems that's

00:04:18.250 --> 00:04:18.260
these technology defense systems that's
 

00:04:18.260 --> 00:04:19.750
these technology defense systems that's
actually the biggest challenge

00:04:19.750 --> 00:04:19.760
actually the biggest challenge
 

00:04:19.760 --> 00:04:21.880
actually the biggest challenge
of the 21st century it's how do we reap

00:04:21.880 --> 00:04:21.890
of the 21st century it's how do we reap
 

00:04:21.890 --> 00:04:23.920
of the 21st century it's how do we reap
the promise which I think we have a

00:04:23.920 --> 00:04:23.930
the promise which I think we have a
 

00:04:23.930 --> 00:04:25.600
the promise which I think we have a
moral imperative to do because it's

00:04:25.600 --> 00:04:25.610
moral imperative to do because it's
 

00:04:25.610 --> 00:04:26.650
moral imperative to do because it's
still a lot of suffering in the world

00:04:26.650 --> 00:04:26.660
still a lot of suffering in the world
 

00:04:26.660 --> 00:04:30.370
still a lot of suffering in the world
while we control the peril the Asilomar

00:04:30.370 --> 00:04:30.380
while we control the peril the Asilomar
 

00:04:30.380 --> 00:04:32.860
while we control the peril the Asilomar
guidelines have worked so far it's a

00:04:32.860 --> 00:04:32.870
guidelines have worked so far it's a
 

00:04:32.870 --> 00:04:34.990
guidelines have worked so far it's a
good blueprint for how we can keep AI

00:04:34.990 --> 00:04:35.000
good blueprint for how we can keep AI
 

00:04:35.000 --> 00:04:37.990
good blueprint for how we can keep AI
safe but we're going to have to

00:04:37.990 --> 00:04:38.000
safe but we're going to have to
 

00:04:38.000 --> 00:04:40.480
safe but we're going to have to
continually reconsider them and I agree

00:04:40.480 --> 00:04:40.490
continually reconsider them and I agree
 

00:04:40.490 --> 00:04:42.100
continually reconsider them and I agree
with max that we need to actually talk

00:04:42.100 --> 00:04:42.110
with max that we need to actually talk
 

00:04:42.110 --> 00:04:43.000
with max that we need to actually talk
about this now

00:04:43.000 --> 00:04:43.010
about this now
 

00:04:43.010 --> 00:04:45.310
about this now
even though the existential dangers are

00:04:45.310 --> 00:04:45.320
even though the existential dangers are
 

00:04:45.320 --> 00:04:52.390
even though the existential dangers are
still off in the future I do want to go

00:04:52.390 --> 00:04:52.400
still off in the future I do want to go
 

00:04:52.400 --> 00:04:55.980
still off in the future I do want to go
first um so I think it's worth a

00:04:55.980 --> 00:04:55.990
first um so I think it's worth a
 

00:04:55.990 --> 00:04:59.230
first um so I think it's worth a
expanding a little further on why one

00:04:59.230 --> 00:04:59.240
expanding a little further on why one
 

00:04:59.240 --> 00:05:01.060
expanding a little further on why one
might think that that better machines

00:05:01.060 --> 00:05:01.070
might think that that better machines
 

00:05:01.070 --> 00:05:03.760
might think that that better machines
would be a problem so what exactly is

00:05:03.760 --> 00:05:03.770
would be a problem so what exactly is
 

00:05:03.770 --> 00:05:06.700
would be a problem so what exactly is
the fear of the singularity and if you

00:05:06.700 --> 00:05:06.710
the fear of the singularity and if you
 

00:05:06.710 --> 00:05:09.970
the fear of the singularity and if you
open a newspaper and just page through

00:05:09.970 --> 00:05:09.980
open a newspaper and just page through
 

00:05:09.980 --> 00:05:11.440
open a newspaper and just page through
it until you find a picture of

00:05:11.440 --> 00:05:11.450
it until you find a picture of
 

00:05:11.450 --> 00:05:13.290
it until you find a picture of
Terminator robots taking over the world

00:05:13.290 --> 00:05:13.300
Terminator robots taking over the world
 

00:05:13.300 --> 00:05:16.090
Terminator robots taking over the world
and then read that article the the

00:05:16.090 --> 00:05:16.100
and then read that article the the
 

00:05:16.100 --> 00:05:18.070
and then read that article the the
journalists story is usually that

00:05:18.070 --> 00:05:18.080
journalists story is usually that
 

00:05:18.080 --> 00:05:20.590
journalists story is usually that
somehow machines will spontaneously wake

00:05:20.590 --> 00:05:20.600
somehow machines will spontaneously wake
 

00:05:20.600 --> 00:05:22.690
somehow machines will spontaneously wake
up and you know they'll be in a bad mood

00:05:22.690 --> 00:05:22.700
up and you know they'll be in a bad mood
 

00:05:22.700 --> 00:05:24.760
up and you know they'll be in a bad mood
because they just woke up and they'll

00:05:24.760 --> 00:05:24.770
because they just woke up and they'll
 

00:05:24.770 --> 00:05:26.080
because they just woke up and they'll
hate the human race and they'll try to

00:05:26.080 --> 00:05:26.090
hate the human race and they'll try to
 

00:05:26.090 --> 00:05:29.850
hate the human race and they'll try to
kill us all and they'll they'll be armed

00:05:29.850 --> 00:05:29.860
kill us all and they'll they'll be armed
 

00:05:29.860 --> 00:05:31.930
kill us all and they'll they'll be armed
obviously because we just go around

00:05:31.930 --> 00:05:31.940
obviously because we just go around
 

00:05:31.940 --> 00:05:34.120
obviously because we just go around
arming our computers a lot my laptop has

00:05:34.120 --> 00:05:34.130
arming our computers a lot my laptop has
 

00:05:34.130 --> 00:05:37.270
arming our computers a lot my laptop has
a machine gun dead so it's not gonna

00:05:37.270 --> 00:05:37.280
a machine gun dead so it's not gonna
 

00:05:37.280 --> 00:05:41.560
a machine gun dead so it's not gonna
happen like that the the process is

00:05:41.560 --> 00:05:41.570
happen like that the the process is
 

00:05:41.570 --> 00:05:44.920
happen like that the the process is
going to be a little bit like what

00:05:44.920 --> 00:05:44.930
going to be a little bit like what
 

00:05:44.930 --> 00:05:47.260
going to be a little bit like what
happens when when you rub the lamp and

00:05:47.260 --> 00:05:47.270
happens when when you rub the lamp and
 

00:05:47.270 --> 00:05:51.100
happens when when you rub the lamp and
the genie comes out and the genie grants

00:05:51.100 --> 00:05:51.110
the genie comes out and the genie grants
 

00:05:51.110 --> 00:05:54.310
the genie comes out and the genie grants
you three wishes and it's very very

00:05:54.310 --> 00:05:54.320
you three wishes and it's very very
 

00:05:54.320 --> 00:05:56.710
you three wishes and it's very very
difficult to state the first two wishes

00:05:56.710 --> 00:05:56.720
difficult to state the first two wishes
 

00:05:56.720 --> 00:05:58.930
difficult to state the first two wishes
in exactly the right way and in all of

00:05:58.930 --> 00:05:58.940
in exactly the right way and in all of
 

00:05:58.940 --> 00:06:01.870
in exactly the right way and in all of
these stories and the story of King

00:06:01.870 --> 00:06:01.880
these stories and the story of King
 

00:06:01.880 --> 00:06:04.870
these stories and the story of King
Midas as well you asked for something

00:06:04.870 --> 00:06:04.880
Midas as well you asked for something
 

00:06:04.880 --> 00:06:06.520
Midas as well you asked for something
and then you realize that you haven't

00:06:06.520 --> 00:06:06.530
and then you realize that you haven't
 

00:06:06.530 --> 00:06:08.560
and then you realize that you haven't
asked for it quite right and the third

00:06:08.560 --> 00:06:08.570
asked for it quite right and the third
 

00:06:08.570 --> 00:06:10.180
asked for it quite right and the third
wish is always you know please can you

00:06:10.180 --> 00:06:10.190
wish is always you know please can you
 

00:06:10.190 --> 00:06:11.860
wish is always you know please can you
undo the first two wishes because I

00:06:11.860 --> 00:06:11.870
undo the first two wishes because I
 

00:06:11.870 --> 00:06:14.680
undo the first two wishes because I
didn't quite say them right I in case of

00:06:14.680 --> 00:06:14.690
didn't quite say them right I in case of
 

00:06:14.690 --> 00:06:17.140
didn't quite say them right I in case of
King Midas it was too late he died of

00:06:17.140 --> 00:06:17.150
King Midas it was too late he died of
 

00:06:17.150 --> 00:06:18.940
King Midas it was too late he died of
starvation and thirst because everything

00:06:18.940 --> 00:06:18.950
starvation and thirst because everything
 

00:06:18.950 --> 00:06:20.950
starvation and thirst because everything
he tried to eat and drink turned into

00:06:20.950 --> 00:06:20.960
he tried to eat and drink turned into
 

00:06:20.960 --> 00:06:25.590
he tried to eat and drink turned into
gold and so it was irreversible and the

00:06:25.590 --> 00:06:25.600
gold and so it was irreversible and the
 

00:06:25.600 --> 00:06:31.170
gold and so it was irreversible and the
the issue that goes has been raised for

00:06:31.170 --> 00:06:31.180
the issue that goes has been raised for
 

00:06:31.180 --> 00:06:33.060
the issue that goes has been raised for
many years going back at least to

00:06:33.060 --> 00:06:33.070
many years going back at least to
 

00:06:33.070 --> 00:06:34.710
many years going back at least to
Norbert Wiener the famous control

00:06:34.710 --> 00:06:34.720
Norbert Wiener the famous control
 

00:06:34.720 --> 00:06:37.440
Norbert Wiener the famous control
theorist in 1960 if you put a purpose

00:06:37.440 --> 00:06:37.450
theorist in 1960 if you put a purpose
 

00:06:37.450 --> 00:06:40.710
theorist in 1960 if you put a purpose
into a machine and that machine is

00:06:40.710 --> 00:06:40.720
into a machine and that machine is
 

00:06:40.720 --> 00:06:42.450
into a machine and that machine is
capable enough that it's going to be

00:06:42.450 --> 00:06:42.460
capable enough that it's going to be
 

00:06:42.460 --> 00:06:45.960
capable enough that it's going to be
hard to reverse that decision then you

00:06:45.960 --> 00:06:45.970
hard to reverse that decision then you
 

00:06:45.970 --> 00:06:47.580
hard to reverse that decision then you
better be sure that the purpose is

00:06:47.580 --> 00:06:47.590
better be sure that the purpose is
 

00:06:47.590 --> 00:06:49.680
better be sure that the purpose is
exactly the purpose that you desire and

00:06:49.680 --> 00:06:49.690
exactly the purpose that you desire and
 

00:06:49.690 --> 00:06:52.170
exactly the purpose that you desire and
at the moment we don't know how to do

00:06:52.170 --> 00:06:52.180
at the moment we don't know how to do
 

00:06:52.180 --> 00:06:55.590
at the moment we don't know how to do
that so it's a technological problem in

00:06:55.590 --> 00:06:55.600
that so it's a technological problem in
 

00:06:55.600 --> 00:06:58.230
that so it's a technological problem in
much the same way that the containment

00:06:58.230 --> 00:06:58.240
much the same way that the containment
 

00:06:58.240 --> 00:07:00.540
much the same way that the containment
of nuclear fusion is a technological

00:07:00.540 --> 00:07:00.550
of nuclear fusion is a technological
 

00:07:00.550 --> 00:07:03.240
of nuclear fusion is a technological
problem and fusion is a source of

00:07:03.240 --> 00:07:03.250
problem and fusion is a source of
 

00:07:03.250 --> 00:07:05.580
problem and fusion is a source of
unlimited energy that could save our

00:07:05.580 --> 00:07:05.590
unlimited energy that could save our
 

00:07:05.590 --> 00:07:07.470
unlimited energy that could save our
environment and perhaps even save our

00:07:07.470 --> 00:07:07.480
environment and perhaps even save our
 

00:07:07.480 --> 00:07:12.000
environment and perhaps even save our
race but it's not useful until it's

00:07:12.000 --> 00:07:12.010
race but it's not useful until it's
 

00:07:12.010 --> 00:07:12.570
race but it's not useful until it's
contained

00:07:12.570 --> 00:07:12.580
contained
 

00:07:12.580 --> 00:07:15.450
contained
and I think artificial intelligence may

00:07:15.450 --> 00:07:15.460
and I think artificial intelligence may
 

00:07:15.460 --> 00:07:18.950
and I think artificial intelligence may
have some of the same characteristics

00:07:18.950 --> 00:07:18.960
have some of the same characteristics
 

00:07:18.960 --> 00:07:22.200
have some of the same characteristics
the question that was posed to us here

00:07:22.200 --> 00:07:22.210
the question that was posed to us here
 

00:07:22.210 --> 00:07:24.720
the question that was posed to us here
was should we welcome a singularity or

00:07:24.720 --> 00:07:24.730
was should we welcome a singularity or
 

00:07:24.730 --> 00:07:27.540
was should we welcome a singularity or
should we fear it and I agree with both

00:07:27.540 --> 00:07:27.550
should we fear it and I agree with both
 

00:07:27.550 --> 00:07:30.930
should we fear it and I agree with both
Ray and do it and saying we should

00:07:30.930 --> 00:07:30.940
Ray and do it and saying we should
 

00:07:30.940 --> 00:07:32.910
Ray and do it and saying we should
welcome it and fear it we should welcome

00:07:32.910 --> 00:07:32.920
welcome it and fear it we should welcome
 

00:07:32.920 --> 00:07:35.250
welcome it and fear it we should welcome
it because every single thing that we

00:07:35.250 --> 00:07:35.260
it because every single thing that we
 

00:07:35.260 --> 00:07:37.230
it because every single thing that we
love that I love about civilization

00:07:37.230 --> 00:07:37.240
love that I love about civilization
 

00:07:37.240 --> 00:07:38.880
love that I love about civilization
there's a product of intelligence and

00:07:38.880 --> 00:07:38.890
there's a product of intelligence and
 

00:07:38.890 --> 00:07:40.230
there's a product of intelligence and
obviously if we can amplify our

00:07:40.230 --> 00:07:40.240
obviously if we can amplify our
 

00:07:40.240 --> 00:07:43.020
obviously if we can amplify our
intelligence thanks machines we can

00:07:43.020 --> 00:07:43.030
intelligence thanks machines we can
 

00:07:43.030 --> 00:07:45.570
intelligence thanks machines we can
create a much more wonderful future at

00:07:45.570 --> 00:07:45.580
create a much more wonderful future at
 

00:07:45.580 --> 00:07:47.010
create a much more wonderful future at
the same time we should fear it because

00:07:47.010 --> 00:07:47.020
the same time we should fear it because
 

00:07:47.020 --> 00:07:48.420
the same time we should fear it because
only if we fear it are we actually going

00:07:48.420 --> 00:07:48.430
only if we fear it are we actually going
 

00:07:48.430 --> 00:07:50.340
only if we fear it are we actually going
to take the right precautions and get

00:07:50.340 --> 00:07:50.350
to take the right precautions and get
 

00:07:50.350 --> 00:07:52.470
to take the right precautions and get
things right this is going to be the

00:07:52.470 --> 00:07:52.480
things right this is going to be the
 

00:07:52.480 --> 00:07:54.450
things right this is going to be the
most powerful technology ever we don't

00:07:54.450 --> 00:07:54.460
most powerful technology ever we don't
 

00:07:54.460 --> 00:07:56.550
most powerful technology ever we don't
want to just bumble into it unprepared

00:07:56.550 --> 00:07:56.560
want to just bumble into it unprepared
 

00:07:56.560 --> 00:07:59.910
want to just bumble into it unprepared
that would be ridiculous and I think you

00:07:59.910 --> 00:07:59.920
that would be ridiculous and I think you
 

00:07:59.920 --> 00:08:01.620
that would be ridiculous and I think you
said something very wise there ray or

00:08:01.620 --> 00:08:01.630
said something very wise there ray or
 

00:08:01.630 --> 00:08:04.080
said something very wise there ray or
you can you mentioned fire in the past

00:08:04.080 --> 00:08:04.090
you can you mentioned fire in the past
 

00:08:04.090 --> 00:08:07.770
you can you mentioned fire in the past
oh we found all technologies to be

00:08:07.770 --> 00:08:07.780
oh we found all technologies to be
 

00:08:07.780 --> 00:08:09.570
oh we found all technologies to be
double-edged swords of course to give us

00:08:09.570 --> 00:08:09.580
double-edged swords of course to give us
 

00:08:09.580 --> 00:08:11.310
double-edged swords of course to give us
new ways of doing good and new ways of

00:08:11.310 --> 00:08:11.320
new ways of doing good and new ways of
 

00:08:11.320 --> 00:08:12.930
new ways of doing good and new ways of
messing up but we've always used a

00:08:12.930 --> 00:08:12.940
messing up but we've always used a
 

00:08:12.940 --> 00:08:14.490
messing up but we've always used a
strategy in the past of learning from

00:08:14.490 --> 00:08:14.500
strategy in the past of learning from
 

00:08:14.500 --> 00:08:18.660
strategy in the past of learning from
mistakes that's what must change we

00:08:18.660 --> 00:08:18.670
mistakes that's what must change we
 

00:08:18.670 --> 00:08:20.550
mistakes that's what must change we
learn from mistake with fire we stood up

00:08:20.550 --> 00:08:20.560
learn from mistake with fire we stood up
 

00:08:20.560 --> 00:08:22.680
learn from mistake with fire we stood up
a bunch of times and now there's a fire

00:08:22.680 --> 00:08:22.690
a bunch of times and now there's a fire
 

00:08:22.690 --> 00:08:24.510
a bunch of times and now there's a fire
exit there and a fire extinguisher over

00:08:24.510 --> 00:08:24.520
exit there and a fire extinguisher over
 

00:08:24.520 --> 00:08:27.450
exit there and a fire extinguisher over
there and things are fine but with more

00:08:27.450 --> 00:08:27.460
there and things are fine but with more
 

00:08:27.460 --> 00:08:28.950
there and things are fine but with more
powerful tech like nuclear weapons

00:08:28.950 --> 00:08:28.960
powerful tech like nuclear weapons
 

00:08:28.960 --> 00:08:32.909
powerful tech like nuclear weapons
synthetic biology and AI we do not learn

00:08:32.909 --> 00:08:32.919
synthetic biology and AI we do not learn
 

00:08:32.919 --> 00:08:34.980
synthetic biology and AI we do not learn
from mistakes we want to get things

00:08:34.980 --> 00:08:34.990
from mistakes we want to get things
 

00:08:34.990 --> 00:08:36.279
from mistakes we want to get things
right the first time because

00:08:36.279 --> 00:08:36.289
right the first time because
 

00:08:36.289 --> 00:08:38.230
right the first time because
is perhaps the only time we'll have it

00:08:38.230 --> 00:08:38.240
is perhaps the only time we'll have it
 

00:08:38.240 --> 00:08:40.089
is perhaps the only time we'll have it
that's why I think it's so valuable to

00:08:40.089 --> 00:08:40.099
that's why I think it's so valuable to
 

00:08:40.099 --> 00:08:42.909
that's why I think it's so valuable to
get away from this thing about us let's

00:08:42.909 --> 00:08:42.919
get away from this thing about us let's
 

00:08:42.919 --> 00:08:44.680
get away from this thing about us let's
whine about whether we should be worried

00:08:44.680 --> 00:08:44.690
whine about whether we should be worried
 

00:08:44.690 --> 00:08:46.720
whine about whether we should be worried
we should be and ask what concrete

00:08:46.720 --> 00:08:46.730
we should be and ask what concrete
 

00:08:46.730 --> 00:08:48.850
we should be and ask what concrete
things can we do now to prepare these

00:08:48.850 --> 00:08:48.860
things can we do now to prepare these
 

00:08:48.860 --> 00:08:51.310
things can we do now to prepare these
are exciting and challenging research

00:08:51.310 --> 00:08:51.320
are exciting and challenging research
 

00:08:51.320 --> 00:08:53.199
are exciting and challenging research
questions that we don't know the answer

00:08:53.199 --> 00:08:53.209
questions that we don't know the answer
 

00:08:53.209 --> 00:08:54.610
questions that we don't know the answer
to yet and we want to know the answer to

00:08:54.610 --> 00:08:54.620
to yet and we want to know the answer to
 

00:08:54.620 --> 00:08:59.170
to yet and we want to know the answer to
them so I feel that to be able to

00:08:59.170 --> 00:08:59.180
them so I feel that to be able to
 

00:08:59.180 --> 00:09:01.090
them so I feel that to be able to
welcome this and feel really optimistic

00:09:01.090 --> 00:09:01.100
welcome this and feel really optimistic
 

00:09:01.100 --> 00:09:02.769
welcome this and feel really optimistic
and excited about the future which I do

00:09:02.769 --> 00:09:02.779
and excited about the future which I do
 

00:09:02.779 --> 00:09:05.530
and excited about the future which I do
we need to win this race between the

00:09:05.530 --> 00:09:05.540
we need to win this race between the
 

00:09:05.540 --> 00:09:07.120
we need to win this race between the
growing power of the technology of AI

00:09:07.120 --> 00:09:07.130
growing power of the technology of AI
 

00:09:07.130 --> 00:09:10.030
growing power of the technology of AI
and the growing wisdom with which you

00:09:10.030 --> 00:09:10.040
and the growing wisdom with which you
 

00:09:10.040 --> 00:09:11.920
and the growing wisdom with which you
manage it and so far it's very natural

00:09:11.920 --> 00:09:11.930
manage it and so far it's very natural
 

00:09:11.930 --> 00:09:14.470
manage it and so far it's very natural
that the investments have been mainly

00:09:14.470 --> 00:09:14.480
that the investments have been mainly
 

00:09:14.480 --> 00:09:17.319
that the investments have been mainly
into just making the thing work that's

00:09:17.319 --> 00:09:17.329
into just making the thing work that's
 

00:09:17.329 --> 00:09:18.639
into just making the thing work that's
the natural thing to do before there is

00:09:18.639 --> 00:09:18.649
the natural thing to do before there is
 

00:09:18.649 --> 00:09:20.410
the natural thing to do before there is
much impact on society but now is the

00:09:20.410 --> 00:09:20.420
much impact on society but now is the
 

00:09:20.420 --> 00:09:22.030
much impact on society but now is the
time to also invest in the other kind of

00:09:22.030 --> 00:09:22.040
time to also invest in the other kind of
 

00:09:22.040 --> 00:09:24.460
time to also invest in the other kind of
research developing the wisdom figuring

00:09:24.460 --> 00:09:24.470
research developing the wisdom figuring
 

00:09:24.470 --> 00:09:27.129
research developing the wisdom figuring
out how we can create not pure

00:09:27.129 --> 00:09:27.139
out how we can create not pure
 

00:09:27.139 --> 00:09:29.079
out how we can create not pure
undirected intelligence as you said in

00:09:29.079 --> 00:09:29.089
undirected intelligence as you said in
 

00:09:29.089 --> 00:09:31.379
undirected intelligence as you said in
your nice nuclear metaphor but

00:09:31.379 --> 00:09:31.389
your nice nuclear metaphor but
 

00:09:31.389 --> 00:09:35.650
your nice nuclear metaphor but
beneficial artificial intelligence I mix

00:09:35.650 --> 00:09:35.660
beneficial artificial intelligence I mix
 

00:09:35.660 --> 00:09:38.259
beneficial artificial intelligence I mix
make some most necessary comments here I

00:09:38.259 --> 00:09:38.269
make some most necessary comments here I
 

00:09:38.269 --> 00:09:40.420
make some most necessary comments here I
follow what max just said I think before

00:09:40.420 --> 00:09:40.430
follow what max just said I think before
 

00:09:40.430 --> 00:09:42.790
follow what max just said I think before
we worry about should we fear should we

00:09:42.790 --> 00:09:42.800
we worry about should we fear should we
 

00:09:42.800 --> 00:09:45.069
we worry about should we fear should we
welcome singularity or the AI technology

00:09:45.069 --> 00:09:45.079
welcome singularity or the AI technology
 

00:09:45.079 --> 00:09:47.800
welcome singularity or the AI technology
let's just be a little bit specific you

00:09:47.800 --> 00:09:47.810
let's just be a little bit specific you
 

00:09:47.810 --> 00:09:50.379
let's just be a little bit specific you
know what we really mean by AI what are

00:09:50.379 --> 00:09:50.389
know what we really mean by AI what are
 

00:09:50.389 --> 00:09:52.750
know what we really mean by AI what are
those things we already developed and we

00:09:52.750 --> 00:09:52.760
those things we already developed and we
 

00:09:52.760 --> 00:09:56.019
those things we already developed and we
believe in the next five to 50 years we

00:09:56.019 --> 00:09:56.029
believe in the next five to 50 years we
 

00:09:56.029 --> 00:09:58.509
believe in the next five to 50 years we
will be able to do there are few things

00:09:58.509 --> 00:09:58.519
will be able to do there are few things
 

00:09:58.519 --> 00:10:00.879
will be able to do there are few things
that we generally say they actually are

00:10:00.879 --> 00:10:00.889
that we generally say they actually are
 

00:10:00.889 --> 00:10:03.819
that we generally say they actually are
about AI is this speech this computer

00:10:03.819 --> 00:10:03.829
about AI is this speech this computer
 

00:10:03.829 --> 00:10:06.189
about AI is this speech this computer
vision is natural language and we also

00:10:06.189 --> 00:10:06.199
vision is natural language and we also
 

00:10:06.199 --> 00:10:08.800
vision is natural language and we also
talk about robots but one thing we don't

00:10:08.800 --> 00:10:08.810
talk about robots but one thing we don't
 

00:10:08.810 --> 00:10:10.750
talk about robots but one thing we don't
talk too much is actually the general

00:10:10.750 --> 00:10:10.760
talk too much is actually the general
 

00:10:10.760 --> 00:10:12.490
talk too much is actually the general
intelligence that general understanding

00:10:12.490 --> 00:10:12.500
intelligence that general understanding
 

00:10:12.500 --> 00:10:13.990
intelligence that general understanding
what's going on it's just very very

00:10:13.990 --> 00:10:14.000
what's going on it's just very very
 

00:10:14.000 --> 00:10:16.720
what's going on it's just very very
difficult to talk about you know did I

00:10:16.720 --> 00:10:16.730
difficult to talk about you know did I
 

00:10:16.730 --> 00:10:18.250
difficult to talk about you know did I
say something wrong or something these

00:10:18.250 --> 00:10:18.260
say something wrong or something these
 

00:10:18.260 --> 00:10:20.379
say something wrong or something these
kind of general questions I don't really

00:10:20.379 --> 00:10:20.389
kind of general questions I don't really
 

00:10:20.389 --> 00:10:22.120
kind of general questions I don't really
did the speech for many years you

00:10:22.120 --> 00:10:22.130
did the speech for many years you
 

00:10:22.130 --> 00:10:23.620
did the speech for many years you
definitely would agree with me I think

00:10:23.620 --> 00:10:23.630
definitely would agree with me I think
 

00:10:23.630 --> 00:10:25.629
definitely would agree with me I think
we are only about a few years to several

00:10:25.629 --> 00:10:25.639
we are only about a few years to several
 

00:10:25.639 --> 00:10:27.610
we are only about a few years to several
years away that computer speech

00:10:27.610 --> 00:10:27.620
years away that computer speech
 

00:10:27.620 --> 00:10:30.519
years away that computer speech
recognition error rates will match human

00:10:30.519 --> 00:10:30.529
recognition error rates will match human
 

00:10:30.529 --> 00:10:32.350
recognition error rates will match human
level we're talking about only a few

00:10:32.350 --> 00:10:32.360
level we're talking about only a few
 

00:10:32.360 --> 00:10:34.389
level we're talking about only a few
years out if you look at a computer

00:10:34.389 --> 00:10:34.399
years out if you look at a computer
 

00:10:34.399 --> 00:10:36.579
years out if you look at a computer
vision in a visual recognition we also

00:10:36.579 --> 00:10:36.589
vision in a visual recognition we also
 

00:10:36.589 --> 00:10:39.040
vision in a visual recognition we also
talk about probably somewhere several

00:10:39.040 --> 00:10:39.050
talk about probably somewhere several
 

00:10:39.050 --> 00:10:41.829
talk about probably somewhere several
years a couple of tens of years we

00:10:41.829 --> 00:10:41.839
years a couple of tens of years we
 

00:10:41.839 --> 00:10:44.860
years a couple of tens of years we
actually for certain limited areas like

00:10:44.860 --> 00:10:44.870
actually for certain limited areas like
 

00:10:44.870 --> 00:10:46.900
actually for certain limited areas like
if you have only ten thousands of

00:10:46.900 --> 00:10:46.910
if you have only ten thousands of
 

00:10:46.910 --> 00:10:48.569
if you have only ten thousands of
categories of objects

00:10:48.569 --> 00:10:48.579
categories of objects
 

00:10:48.579 --> 00:10:50.369
categories of objects
you know computers so they can almost do

00:10:50.369 --> 00:10:50.379
you know computers so they can almost do
 

00:10:50.379 --> 00:10:52.109
you know computers so they can almost do
as well if not slightly better than

00:10:52.109 --> 00:10:52.119
as well if not slightly better than
 

00:10:52.119 --> 00:10:54.689
as well if not slightly better than
human now if you like talk about natural

00:10:54.689 --> 00:10:54.699
human now if you like talk about natural
 

00:10:54.699 --> 00:10:56.489
human now if you like talk about natural
language we're still far away from

00:10:56.489 --> 00:10:56.499
language we're still far away from
 

00:10:56.499 --> 00:10:59.400
language we're still far away from
really understanding natural language we

00:10:59.400 --> 00:10:59.410
really understanding natural language we
 

00:10:59.410 --> 00:11:00.840
really understanding natural language we
still have problems translating

00:11:00.840 --> 00:11:00.850
still have problems translating
 

00:11:00.850 --> 00:11:03.689
still have problems translating
languages from Swedish to English or to

00:11:03.689 --> 00:11:03.699
languages from Swedish to English or to
 

00:11:03.699 --> 00:11:04.850
languages from Swedish to English or to
Chinese

00:11:04.850 --> 00:11:04.860
Chinese
 

00:11:04.860 --> 00:11:07.739
Chinese
robots are particularly interesting now

00:11:07.739 --> 00:11:07.749
robots are particularly interesting now
 

00:11:07.749 --> 00:11:09.509
robots are particularly interesting now
if we look at in the robots in let's do

00:11:09.509 --> 00:11:09.519
if we look at in the robots in let's do
 

00:11:09.519 --> 00:11:11.910
if we look at in the robots in let's do
it talked about and you watch the movies

00:11:11.910 --> 00:11:11.920
it talked about and you watch the movies
 

00:11:11.920 --> 00:11:13.379
it talked about and you watch the movies
the science fiction's you have robots

00:11:13.379 --> 00:11:13.389
the science fiction's you have robots
 

00:11:13.389 --> 00:11:15.419
the science fiction's you have robots
running around shoot at us I think

00:11:15.419 --> 00:11:15.429
running around shoot at us I think
 

00:11:15.429 --> 00:11:17.129
running around shoot at us I think
that's probably where we have most of

00:11:17.129 --> 00:11:17.139
that's probably where we have most of
 

00:11:17.139 --> 00:11:20.100
that's probably where we have most of
fear and that's to me it's not really

00:11:20.100 --> 00:11:20.110
fear and that's to me it's not really
 

00:11:20.110 --> 00:11:22.350
fear and that's to me it's not really
justified today and as we think about

00:11:22.350 --> 00:11:22.360
justified today and as we think about
 

00:11:22.360 --> 00:11:25.710
justified today and as we think about
this what we really need is actually no

00:11:25.710 --> 00:11:25.720
this what we really need is actually no
 

00:11:25.720 --> 00:11:27.889
this what we really need is actually no
spending more investing more and

00:11:27.889 --> 00:11:27.899
spending more investing more and
 

00:11:27.899 --> 00:11:30.239
spending more investing more and
developing better and the better AI

00:11:30.239 --> 00:11:30.249
developing better and the better AI
 

00:11:30.249 --> 00:11:33.600
developing better and the better AI
technologies right now well let me

00:11:33.600 --> 00:11:33.610
technologies right now well let me
 

00:11:33.610 --> 00:11:35.489
technologies right now well let me
follow up on the comment about movies

00:11:35.489 --> 00:11:35.499
follow up on the comment about movies
 

00:11:35.499 --> 00:11:37.049
follow up on the comment about movies
because I think a lot of the fear of AI

00:11:37.049 --> 00:11:37.059
because I think a lot of the fear of AI
 

00:11:37.059 --> 00:11:40.079
because I think a lot of the fear of AI
comes from the typical future dystopian

00:11:40.079 --> 00:11:40.089
comes from the typical future dystopian
 

00:11:40.089 --> 00:11:42.689
comes from the typical future dystopian
movie where it's the AI versus the

00:11:42.689 --> 00:11:42.699
movie where it's the AI versus the
 

00:11:42.699 --> 00:11:44.249
movie where it's the AI versus the
humans or there's two groups of humans

00:11:44.249 --> 00:11:44.259
humans or there's two groups of humans
 

00:11:44.259 --> 00:11:47.669
humans or there's two groups of humans
fighting for control over the AI we

00:11:47.669 --> 00:11:47.679
fighting for control over the AI we
 

00:11:47.679 --> 00:11:50.009
fighting for control over the AI we
don't have one or two AIS such as they

00:11:50.009 --> 00:11:50.019
don't have one or two AIS such as they
 

00:11:50.019 --> 00:11:52.590
don't have one or two AIS such as they
are today we have two or three billion

00:11:52.590 --> 00:11:52.600
are today we have two or three billion
 

00:11:52.600 --> 00:11:55.439
are today we have two or three billion
of them projections are well have six

00:11:55.439 --> 00:11:55.449
of them projections are well have six
 

00:11:55.449 --> 00:11:57.150
of them projections are well have six
billion smartphones which are AIS

00:11:57.150 --> 00:11:57.160
billion smartphones which are AIS
 

00:11:57.160 --> 00:12:00.449
billion smartphones which are AIS
circuit today in the world they're

00:12:00.449 --> 00:12:00.459
circuit today in the world they're
 

00:12:00.459 --> 00:12:02.369
circuit today in the world they're
deeply integrated with us we use them

00:12:02.369 --> 00:12:02.379
deeply integrated with us we use them
 

00:12:02.379 --> 00:12:04.049
deeply integrated with us we use them
constantly they're going to become even

00:12:04.049 --> 00:12:04.059
constantly they're going to become even
 

00:12:04.059 --> 00:12:06.809
constantly they're going to become even
more intimately integrated in the future

00:12:06.809 --> 00:12:06.819
more intimately integrated in the future
 

00:12:06.819 --> 00:12:09.199
more intimately integrated in the future
the primary application is going to be

00:12:09.199 --> 00:12:09.209
the primary application is going to be
 

00:12:09.209 --> 00:12:11.970
the primary application is going to be
humans and machines working in very

00:12:11.970 --> 00:12:11.980
humans and machines working in very
 

00:12:11.980 --> 00:12:15.090
humans and machines working in very
close concert together so for one thing

00:12:15.090 --> 00:12:15.100
close concert together so for one thing
 

00:12:15.100 --> 00:12:17.970
close concert together so for one thing
I think keeping AI safe we need to keep

00:12:17.970 --> 00:12:17.980
I think keeping AI safe we need to keep
 

00:12:17.980 --> 00:12:19.970
I think keeping AI safe we need to keep
humans safe and while the reasons were

00:12:19.970 --> 00:12:19.980
humans safe and while the reasons were
 

00:12:19.980 --> 00:12:22.379
humans safe and while the reasons were
distrustful of AI is because we're in

00:12:22.379 --> 00:12:22.389
distrustful of AI is because we're in
 

00:12:22.389 --> 00:12:24.090
distrustful of AI is because we're in
distrustful of humans when you look at

00:12:24.090 --> 00:12:24.100
distrustful of humans when you look at
 

00:12:24.100 --> 00:12:26.519
distrustful of humans when you look at
human history but I mean that's the

00:12:26.519 --> 00:12:26.529
human history but I mean that's the
 

00:12:26.529 --> 00:12:28.530
human history but I mean that's the
vision is there's going to be a very

00:12:28.530 --> 00:12:28.540
vision is there's going to be a very
 

00:12:28.540 --> 00:12:30.539
vision is there's going to be a very
deeply integrated society we're already

00:12:30.539 --> 00:12:30.549
deeply integrated society we're already
 

00:12:30.549 --> 00:12:34.829
deeply integrated society we're already
a human technological society it's

00:12:34.829 --> 00:12:34.839
a human technological society it's
 

00:12:34.839 --> 00:12:36.689
a human technological society it's
coming actually from one of the unlock

00:12:36.689 --> 00:12:36.699
coming actually from one of the unlock
 

00:12:36.699 --> 00:12:39.210
coming actually from one of the unlock
online audience they say how will they I

00:12:39.210 --> 00:12:39.220
online audience they say how will they I
 

00:12:39.220 --> 00:12:42.419
online audience they say how will they I
be able to cope with the current lack of

00:12:42.419 --> 00:12:42.429
be able to cope with the current lack of
 

00:12:42.429 --> 00:12:44.249
be able to cope with the current lack of
development of our ethical and moral

00:12:44.249 --> 00:12:44.259
development of our ethical and moral
 

00:12:44.259 --> 00:12:48.800
development of our ethical and moral
norms how do you put that in

00:12:48.800 --> 00:12:48.810
norms how do you put that in
 

00:12:48.810 --> 00:12:50.960
norms how do you put that in
I think that's a that's a great question

00:12:50.960 --> 00:12:50.970
I think that's a that's a great question
 

00:12:50.970 --> 00:12:56.300
I think that's a that's a great question
so one of the approaches that may be

00:12:56.300 --> 00:12:56.310
so one of the approaches that may be
 

00:12:56.310 --> 00:12:59.810
so one of the approaches that may be
able to eliminate the risk from from

00:12:59.810 --> 00:12:59.820
able to eliminate the risk from from
 

00:12:59.820 --> 00:13:02.930
able to eliminate the risk from from
super intelligent AI is that the

00:13:02.930 --> 00:13:02.940
super intelligent AI is that the
 

00:13:02.940 --> 00:13:04.910
super intelligent AI is that the
machines have to come to understand what

00:13:04.910 --> 00:13:04.920
machines have to come to understand what
 

00:13:04.920 --> 00:13:08.200
machines have to come to understand what
human values are and they have to learn

00:13:08.200 --> 00:13:08.210
human values are and they have to learn
 

00:13:08.210 --> 00:13:11.540
human values are and they have to learn
that for example if you ask them to cure

00:13:11.540 --> 00:13:11.550
that for example if you ask them to cure
 

00:13:11.550 --> 00:13:15.530
that for example if you ask them to cure
cancer a good solution to the problem of

00:13:15.530 --> 00:13:15.540
cancer a good solution to the problem of
 

00:13:15.540 --> 00:13:18.710
cancer a good solution to the problem of
cancer is not to to wipe out the human

00:13:18.710 --> 00:13:18.720
cancer is not to to wipe out the human
 

00:13:18.720 --> 00:13:20.240
cancer is not to to wipe out the human
race and then there won't be any cancer

00:13:20.240 --> 00:13:20.250
race and then there won't be any cancer
 

00:13:20.250 --> 00:13:22.790
race and then there won't be any cancer
anymore alright that's not quite what we

00:13:22.790 --> 00:13:22.800
anymore alright that's not quite what we
 

00:13:22.800 --> 00:13:26.420
anymore alright that's not quite what we
meant to to eliminating cancer you know

00:13:26.420 --> 00:13:26.430
meant to to eliminating cancer you know
 

00:13:26.430 --> 00:13:28.220
meant to to eliminating cancer you know
limiting human suffering you know you

00:13:28.220 --> 00:13:28.230
limiting human suffering you know you
 

00:13:28.230 --> 00:13:29.750
limiting human suffering you know you
might say well you know humans are

00:13:29.750 --> 00:13:29.760
might say well you know humans are
 

00:13:29.760 --> 00:13:30.950
might say well you know humans are
always going to suffer because that's

00:13:30.950 --> 00:13:30.960
always going to suffer because that's
 

00:13:30.960 --> 00:13:32.720
always going to suffer because that's
just the way they are so we have to you

00:13:32.720 --> 00:13:32.730
just the way they are so we have to you
 

00:13:32.730 --> 00:13:34.310
just the way they are so we have to you
know I I have to eliminate human

00:13:34.310 --> 00:13:34.320
know I I have to eliminate human
 

00:13:34.320 --> 00:13:36.640
know I I have to eliminate human
suffering so I have to eliminate humans

00:13:36.640 --> 00:13:36.650
suffering so I have to eliminate humans
 

00:13:36.650 --> 00:13:39.140
suffering so I have to eliminate humans
so machines have to understand our

00:13:39.140 --> 00:13:39.150
so machines have to understand our
 

00:13:39.150 --> 00:13:41.120
so machines have to understand our
values you know more more parochially in

00:13:41.120 --> 00:13:41.130
values you know more more parochially in
 

00:13:41.130 --> 00:13:44.780
values you know more more parochially in
a very very foreseeable future when we

00:13:44.780 --> 00:13:44.790
a very very foreseeable future when we
 

00:13:44.790 --> 00:13:46.820
a very very foreseeable future when we
have domestic robots and self-driving

00:13:46.820 --> 00:13:46.830
have domestic robots and self-driving
 

00:13:46.830 --> 00:13:49.910
have domestic robots and self-driving
cars they have to be able to make value

00:13:49.910 --> 00:13:49.920
cars they have to be able to make value
 

00:13:49.920 --> 00:13:51.769
cars they have to be able to make value
judgments and understand the things we

00:13:51.769 --> 00:13:51.779
judgments and understand the things we
 

00:13:51.779 --> 00:13:54.800
judgments and understand the things we
care about so a domestic robot that is

00:13:54.800 --> 00:13:54.810
care about so a domestic robot that is
 

00:13:54.810 --> 00:13:56.540
care about so a domestic robot that is
supposed to be preparing dinner for the

00:13:56.540 --> 00:13:56.550
supposed to be preparing dinner for the
 

00:13:56.550 --> 00:13:58.880
supposed to be preparing dinner for the
kids before the parents get home when

00:13:58.880 --> 00:13:58.890
kids before the parents get home when
 

00:13:58.890 --> 00:14:00.410
kids before the parents get home when
there's nothing in the fridge we don't

00:14:00.410 --> 00:14:00.420
there's nothing in the fridge we don't
 

00:14:00.420 --> 00:14:02.750
there's nothing in the fridge we don't
want the robot to cook the cat for

00:14:02.750 --> 00:14:02.760
want the robot to cook the cat for
 

00:14:02.760 --> 00:14:05.630
want the robot to cook the cat for
dinner not understanding that that the

00:14:05.630 --> 00:14:05.640
dinner not understanding that that the
 

00:14:05.640 --> 00:14:07.190
dinner not understanding that that the
cat is a very valuable part of the

00:14:07.190 --> 00:14:07.200
cat is a very valuable part of the
 

00:14:07.200 --> 00:14:10.040
cat is a very valuable part of the
family it certainly cheap source of meat

00:14:10.040 --> 00:14:10.050
family it certainly cheap source of meat
 

00:14:10.050 --> 00:14:15.460
family it certainly cheap source of meat
but it's not quite the right one so if a

00:14:15.460 --> 00:14:15.470
but it's not quite the right one so if a
 

00:14:15.470 --> 00:14:18.320
but it's not quite the right one so if a
if a machine makes a mistake like that

00:14:18.320 --> 00:14:18.330
if a machine makes a mistake like that
 

00:14:18.330 --> 00:14:22.070
if a machine makes a mistake like that
if a domestic robot does that you can

00:14:22.070 --> 00:14:22.080
if a domestic robot does that you can
 

00:14:22.080 --> 00:14:23.780
if a domestic robot does that you can
only imagine what the newspapers will

00:14:23.780 --> 00:14:23.790
only imagine what the newspapers will
 

00:14:23.790 --> 00:14:25.700
only imagine what the newspapers will
say and you can only imagine the

00:14:25.700 --> 00:14:25.710
say and you can only imagine the
 

00:14:25.710 --> 00:14:27.440
say and you can only imagine the
reaction of people who might have been

00:14:27.440 --> 00:14:27.450
reaction of people who might have been
 

00:14:27.450 --> 00:14:29.120
reaction of people who might have been
considering buying one of those robots

00:14:29.120 --> 00:14:29.130
considering buying one of those robots
 

00:14:29.130 --> 00:14:31.430
considering buying one of those robots
there's no way I would have a robot in

00:14:31.430 --> 00:14:31.440
there's no way I would have a robot in
 

00:14:31.440 --> 00:14:32.810
there's no way I would have a robot in
my house that was capable of doing

00:14:32.810 --> 00:14:32.820
my house that was capable of doing
 

00:14:32.820 --> 00:14:35.720
my house that was capable of doing
something like that so it would wipe out

00:14:35.720 --> 00:14:35.730
something like that so it would wipe out
 

00:14:35.730 --> 00:14:37.550
something like that so it would wipe out
the industry overnight there's an

00:14:37.550 --> 00:14:37.560
the industry overnight there's an
 

00:14:37.560 --> 00:14:39.910
the industry overnight there's an
enormous ly strong economic incentive

00:14:39.910 --> 00:14:39.920
enormous ly strong economic incentive
 

00:14:39.920 --> 00:14:43.490
enormous ly strong economic incentive
for companies that are building AI to

00:14:43.490 --> 00:14:43.500
for companies that are building AI to
 

00:14:43.500 --> 00:14:45.199
for companies that are building AI to
take these questions very seriously

00:14:45.199 --> 00:14:45.209
take these questions very seriously
 

00:14:45.209 --> 00:14:48.140
take these questions very seriously
because otherwise any company any

00:14:48.140 --> 00:14:48.150
because otherwise any company any
 

00:14:48.150 --> 00:14:50.960
because otherwise any company any
startup company that that doesn't pay

00:14:50.960 --> 00:14:50.970
startup company that that doesn't pay
 

00:14:50.970 --> 00:14:52.730
startup company that that doesn't pay
attention to this could could ruin it

00:14:52.730 --> 00:14:52.740
attention to this could could ruin it
 

00:14:52.740 --> 00:14:54.650
attention to this could could ruin it
for everybody else so they're going to

00:14:54.650 --> 00:14:54.660
for everybody else so they're going to
 

00:14:54.660 --> 00:14:56.480
for everybody else so they're going to
have to figure out how to make machines

00:14:56.480 --> 00:14:56.490
have to figure out how to make machines
 

00:14:56.490 --> 00:14:59.660
have to figure out how to make machines
behave ethically avoid doing things even

00:14:59.660 --> 00:14:59.670
behave ethically avoid doing things even
 

00:14:59.670 --> 00:15:01.770
behave ethically avoid doing things even
if they're told to do something

00:15:01.770 --> 00:15:01.780
if they're told to do something
 

00:15:01.780 --> 00:15:04.170
if they're told to do something
by their human master they have to know

00:15:04.170 --> 00:15:04.180
by their human master they have to know
 

00:15:04.180 --> 00:15:06.450
by their human master they have to know
what's right and wrong so that they

00:15:06.450 --> 00:15:06.460
what's right and wrong so that they
 

00:15:06.460 --> 00:15:09.540
what's right and wrong so that they
don't do something catastrophic to the

00:15:09.540 --> 00:15:09.550
don't do something catastrophic to the
 

00:15:09.550 --> 00:15:12.030
don't do something catastrophic to the
discussion here that human society is

00:15:12.030 --> 00:15:12.040
discussion here that human society is
 

00:15:12.040 --> 00:15:13.590
discussion here that human society is
getting more ethical the number of

00:15:13.590 --> 00:15:13.600
getting more ethical the number of
 

00:15:13.600 --> 00:15:16.050
getting more ethical the number of
democracies we had a hundred years ago

00:15:16.050 --> 00:15:16.060
democracies we had a hundred years ago
 

00:15:16.060 --> 00:15:17.310
democracies we had a hundred years ago
you could count on the fingers of one

00:15:17.310 --> 00:15:17.320
you could count on the fingers of one
 

00:15:17.320 --> 00:15:19.440
you could count on the fingers of one
hand I was certainly not every nation as

00:15:19.440 --> 00:15:19.450
hand I was certainly not every nation as
 

00:15:19.450 --> 00:15:21.150
hand I was certainly not every nation as
a perfect democracy but we've definitely

00:15:21.150 --> 00:15:21.160
a perfect democracy but we've definitely
 

00:15:21.160 --> 00:15:23.870
a perfect democracy but we've definitely
moved dramatically in that direction

00:15:23.870 --> 00:15:23.880
moved dramatically in that direction
 

00:15:23.880 --> 00:15:26.910
moved dramatically in that direction
human life it's very harsh and unjust if

00:15:26.910 --> 00:15:26.920
human life it's very harsh and unjust if
 

00:15:26.920 --> 00:15:29.220
human life it's very harsh and unjust if
you read writers like Thomas Hobbes only

00:15:29.220 --> 00:15:29.230
you read writers like Thomas Hobbes only
 

00:15:29.230 --> 00:15:31.710
you read writers like Thomas Hobbes only
a few centuries ago when human life

00:15:31.710 --> 00:15:31.720
a few centuries ago when human life
 

00:15:31.720 --> 00:15:35.100
a few centuries ago when human life
expectancy was 37 better communication

00:15:35.100 --> 00:15:35.110
expectancy was 37 better communication
 

00:15:35.110 --> 00:15:37.170
expectancy was 37 better communication
technology has led to more democracy

00:15:37.170 --> 00:15:37.180
technology has led to more democracy
 

00:15:37.180 --> 00:15:40.950
technology has led to more democracy
that people think that's surprising that

00:15:40.950 --> 00:15:40.960
that people think that's surprising that
 

00:15:40.960 --> 00:15:43.040
that people think that's surprising that
for example we have the lowest rate of

00:15:43.040 --> 00:15:43.050
for example we have the lowest rate of
 

00:15:43.050 --> 00:15:45.960
for example we have the lowest rate of
violence today your chance of being

00:15:45.960 --> 00:15:45.970
violence today your chance of being
 

00:15:45.970 --> 00:15:49.590
violence today your chance of being
killed is hundreds of times less than it

00:15:49.590 --> 00:15:49.600
killed is hundreds of times less than it
 

00:15:49.600 --> 00:15:52.320
killed is hundreds of times less than it
was a few centuries ago although max

00:15:52.320 --> 00:15:52.330
was a few centuries ago although max
 

00:15:52.330 --> 00:15:54.750
was a few centuries ago although max
points out if we ever had an incident

00:15:54.750 --> 00:15:54.760
points out if we ever had an incident
 

00:15:54.760 --> 00:15:57.570
points out if we ever had an incident
with a nuclear war or abuse of

00:15:57.570 --> 00:15:57.580
with a nuclear war or abuse of
 

00:15:57.580 --> 00:15:59.070
with a nuclear war or abuse of
biotechnology that those statistics

00:15:59.070 --> 00:15:59.080
biotechnology that those statistics
 

00:15:59.080 --> 00:16:02.070
biotechnology that those statistics
would change quickly but we are we are

00:16:02.070 --> 00:16:02.080
would change quickly but we are we are
 

00:16:02.080 --> 00:16:04.560
would change quickly but we are we are
making progress the reason we don't

00:16:04.560 --> 00:16:04.570
making progress the reason we don't
 

00:16:04.570 --> 00:16:06.480
making progress the reason we don't
think so is we have exponentially better

00:16:06.480 --> 00:16:06.490
think so is we have exponentially better
 

00:16:06.490 --> 00:16:08.070
think so is we have exponentially better
information about what's wrong with the

00:16:08.070 --> 00:16:08.080
information about what's wrong with the
 

00:16:08.080 --> 00:16:11.910
information about what's wrong with the
world yeah this question of how to have

00:16:11.910 --> 00:16:11.920
world yeah this question of how to have
 

00:16:11.920 --> 00:16:14.850
world yeah this question of how to have
get machines to learn human goals and

00:16:14.850 --> 00:16:14.860
get machines to learn human goals and
 

00:16:14.860 --> 00:16:18.030
get machines to learn human goals and
values is so central to this because of

00:16:18.030 --> 00:16:18.040
values is so central to this because of
 

00:16:18.040 --> 00:16:19.590
values is so central to this because of
course as you Maggie mentioned in the

00:16:19.590 --> 00:16:19.600
course as you Maggie mentioned in the
 

00:16:19.600 --> 00:16:21.060
course as you Maggie mentioned in the
morning panel one way of defining

00:16:21.060 --> 00:16:21.070
morning panel one way of defining
 

00:16:21.070 --> 00:16:22.800
morning panel one way of defining
intelligence is simply how good you are

00:16:22.800 --> 00:16:22.810
intelligence is simply how good you are
 

00:16:22.810 --> 00:16:25.620
intelligence is simply how good you are
at accomplishing your goals so a chess

00:16:25.620 --> 00:16:25.630
at accomplishing your goals so a chess
 

00:16:25.630 --> 00:16:26.820
at accomplishing your goals so a chess
computer is very good at winning in

00:16:26.820 --> 00:16:26.830
computer is very good at winning in
 

00:16:26.830 --> 00:16:29.430
computer is very good at winning in
chess a more general system to be very

00:16:29.430 --> 00:16:29.440
chess a more general system to be very
 

00:16:29.440 --> 00:16:32.760
chess a more general system to be very
good at doing whatever its goals are we

00:16:32.760 --> 00:16:32.770
good at doing whatever its goals are we
 

00:16:32.770 --> 00:16:34.500
good at doing whatever its goals are we
cannot ignore the question of what goals

00:16:34.500 --> 00:16:34.510
cannot ignore the question of what goals
 

00:16:34.510 --> 00:16:38.010
cannot ignore the question of what goals
to give the systems and and right now we

00:16:38.010 --> 00:16:38.020
to give the systems and and right now we
 

00:16:38.020 --> 00:16:40.530
to give the systems and and right now we
largely have you know when when ante has

00:16:40.530 --> 00:16:40.540
largely have you know when when ante has
 

00:16:40.540 --> 00:16:42.360
largely have you know when when ante has
Lubitz flew this Germanwings airplane

00:16:42.360 --> 00:16:42.370
Lubitz flew this Germanwings airplane
 

00:16:42.370 --> 00:16:45.230
Lubitz flew this Germanwings airplane
into the Alps killing all the passengers

00:16:45.230 --> 00:16:45.240
into the Alps killing all the passengers
 

00:16:45.240 --> 00:16:48.030
into the Alps killing all the passengers
AI could have prevented this if the

00:16:48.030 --> 00:16:48.040
AI could have prevented this if the
 

00:16:48.040 --> 00:16:50.370
AI could have prevented this if the
flight computer had realized that there

00:16:50.370 --> 00:16:50.380
flight computer had realized that there
 

00:16:50.380 --> 00:16:52.860
flight computer had realized that there
is no reasonable reason for why an

00:16:52.860 --> 00:16:52.870
is no reasonable reason for why an
 

00:16:52.870 --> 00:16:54.630
is no reasonable reason for why an
airplane should be flown into a mountain

00:16:54.630 --> 00:16:54.640
airplane should be flown into a mountain
 

00:16:54.640 --> 00:16:56.040
airplane should be flown into a mountain
it could have just gone into safe mode

00:16:56.040 --> 00:16:56.050
it could have just gone into safe mode
 

00:16:56.050 --> 00:16:58.580
it could have just gone into safe mode
and autopilot to the airport right and

00:16:58.580 --> 00:16:58.590
and autopilot to the airport right and
 

00:16:58.590 --> 00:17:02.520
and autopilot to the airport right and
similarly future AI systems we need to

00:17:02.520 --> 00:17:02.530
similarly future AI systems we need to
 

00:17:02.530 --> 00:17:04.050
similarly future AI systems we need to
make them such that they can understand

00:17:04.050 --> 00:17:04.060
make them such that they can understand
 

00:17:04.060 --> 00:17:06.330
make them such that they can understand
what humans really want not that's what

00:17:06.330 --> 00:17:06.340
what humans really want not that's what
 

00:17:06.340 --> 00:17:08.670
what humans really want not that's what
they say they want and also it's basic

00:17:08.670 --> 00:17:08.680
they say they want and also it's basic
 

00:17:08.680 --> 00:17:11.430
they say they want and also it's basic
human values if if you ask for it

00:17:11.430 --> 00:17:11.440
human values if if you ask for it
 

00:17:11.440 --> 00:17:13.230
human values if if you ask for it
ask your future self-driving car to take

00:17:13.230 --> 00:17:13.240
ask your future self-driving car to take
 

00:17:13.240 --> 00:17:14.100
ask your future self-driving car to take
you to land vet

00:17:14.100 --> 00:17:14.110
you to land vet
 

00:17:14.110 --> 00:17:16.500
you to land vet
Airport as fast as possible you're gonna

00:17:16.500 --> 00:17:16.510
Airport as fast as possible you're gonna
 

00:17:16.510 --> 00:17:18.270
Airport as fast as possible you're gonna
get there chased by helicopters and

00:17:18.270 --> 00:17:18.280
get there chased by helicopters and
 

00:17:18.280 --> 00:17:19.199
get there chased by helicopters and
covered in vomit

00:17:19.199 --> 00:17:19.209
covered in vomit
 

00:17:19.209 --> 00:17:21.030
covered in vomit
you're gonna be like no no this isn't

00:17:21.030 --> 00:17:21.040
you're gonna be like no no this isn't
 

00:17:21.040 --> 00:17:24.179
you're gonna be like no no this isn't
quite what I wanted and your car will be

00:17:24.179 --> 00:17:24.189
quite what I wanted and your car will be
 

00:17:24.189 --> 00:17:27.390
quite what I wanted and your car will be
like this is exactly what you asked for

00:17:27.390 --> 00:17:27.400
like this is exactly what you asked for
 

00:17:27.400 --> 00:17:30.570
like this is exactly what you asked for
and you know our children learn very

00:17:30.570 --> 00:17:30.580
and you know our children learn very
 

00:17:30.580 --> 00:17:33.360
and you know our children learn very
quickly they'll take us so literally

00:17:33.360 --> 00:17:33.370
quickly they'll take us so literally
 

00:17:33.370 --> 00:17:36.750
quickly they'll take us so literally
they understand what we mean when we ask

00:17:36.750 --> 00:17:36.760
they understand what we mean when we ask
 

00:17:36.760 --> 00:17:39.030
they understand what we mean when we ask
them for things not so later than the

00:17:39.030 --> 00:17:39.040
them for things not so later than the
 

00:17:39.040 --> 00:17:40.380
them for things not so later than the
funny examples you gave with a cat

00:17:40.380 --> 00:17:40.390
funny examples you gave with a cat
 

00:17:40.390 --> 00:17:41.789
funny examples you gave with a cat
they're exactly in that same category so

00:17:41.789 --> 00:17:41.799
they're exactly in that same category so
 

00:17:41.799 --> 00:17:44.299
they're exactly in that same category so
this is this is another one example of a

00:17:44.299 --> 00:17:44.309
this is this is another one example of a
 

00:17:44.309 --> 00:17:46.830
this is this is another one example of a
technical fascinating research problem

00:17:46.830 --> 00:17:46.840
technical fascinating research problem
 

00:17:46.840 --> 00:17:49.049
technical fascinating research problem
how do you make machines able to observe

00:17:49.049 --> 00:17:49.059
how do you make machines able to observe
 

00:17:49.059 --> 00:17:51.299
how do you make machines able to observe
human behavior figure out what humans

00:17:51.299 --> 00:17:51.309
human behavior figure out what humans
 

00:17:51.309 --> 00:17:52.890
human behavior figure out what humans
really want and also what the sort of

00:17:52.890 --> 00:17:52.900
really want and also what the sort of
 

00:17:52.900 --> 00:17:54.600
really want and also what the sort of
consensus is among humans about what's a

00:17:54.600 --> 00:17:54.610
consensus is among humans about what's a
 

00:17:54.610 --> 00:17:58.049
consensus is among humans about what's a
good thing and the important ethical

00:17:58.049 --> 00:17:58.059
good thing and the important ethical
 

00:17:58.059 --> 00:17:59.580
good thing and the important ethical
question I think always the AI

00:17:59.580 --> 00:17:59.590
question I think always the AI
 

00:17:59.590 --> 00:18:02.280
question I think always the AI
technology today we must address is

00:18:02.280 --> 00:18:02.290
technology today we must address is
 

00:18:02.290 --> 00:18:04.799
technology today we must address is
really about the beta is really about

00:18:04.799 --> 00:18:04.809
really about the beta is really about
 

00:18:04.809 --> 00:18:06.990
really about the beta is really about
the data privacy and the use of privacy

00:18:06.990 --> 00:18:07.000
the data privacy and the use of privacy
 

00:18:07.000 --> 00:18:08.970
the data privacy and the use of privacy
related to all the data we have the

00:18:08.970 --> 00:18:08.980
related to all the data we have the
 

00:18:08.980 --> 00:18:10.680
related to all the data we have the
reason we actually have a lot of AI

00:18:10.680 --> 00:18:10.690
reason we actually have a lot of AI
 

00:18:10.690 --> 00:18:13.260
reason we actually have a lot of AI
today largely reason is because we now

00:18:13.260 --> 00:18:13.270
today largely reason is because we now
 

00:18:13.270 --> 00:18:15.000
today largely reason is because we now
can collect a lot more data it's not

00:18:15.000 --> 00:18:15.010
can collect a lot more data it's not
 

00:18:15.010 --> 00:18:16.590
can collect a lot more data it's not
only just about your own data is

00:18:16.590 --> 00:18:16.600
only just about your own data is
 

00:18:16.600 --> 00:18:19.230
only just about your own data is
actually about the data from so many

00:18:19.230 --> 00:18:19.240
actually about the data from so many
 

00:18:19.240 --> 00:18:21.990
actually about the data from so many
people so many you know human AI systems

00:18:21.990 --> 00:18:22.000
people so many you know human AI systems
 

00:18:22.000 --> 00:18:24.090
people so many you know human AI systems
that we actually have today the reason

00:18:24.090 --> 00:18:24.100
that we actually have today the reason
 

00:18:24.100 --> 00:18:26.190
that we actually have today the reason
we have intelligence better today is

00:18:26.190 --> 00:18:26.200
we have intelligence better today is
 

00:18:26.200 --> 00:18:27.539
we have intelligence better today is
because we have this collective

00:18:27.539 --> 00:18:27.549
because we have this collective
 

00:18:27.549 --> 00:18:29.940
because we have this collective
intelligence that we are able to learn

00:18:29.940 --> 00:18:29.950
intelligence that we are able to learn
 

00:18:29.950 --> 00:18:32.130
intelligence that we are able to learn
from each other with all this kind of

00:18:32.130 --> 00:18:32.140
from each other with all this kind of
 

00:18:32.140 --> 00:18:34.260
from each other with all this kind of
power with this kind of data now we have

00:18:34.260 --> 00:18:34.270
power with this kind of data now we have
 

00:18:34.270 --> 00:18:36.390
power with this kind of data now we have
to be very careful as we design what

00:18:36.390 --> 00:18:36.400
to be very careful as we design what
 

00:18:36.400 --> 00:18:39.030
to be very careful as we design what
kind of systems we have and related to

00:18:39.030 --> 00:18:39.040
kind of systems we have and related to
 

00:18:39.040 --> 00:18:40.470
kind of systems we have and related to
that is what I actually call the

00:18:40.470 --> 00:18:40.480
that is what I actually call the
 

00:18:40.480 --> 00:18:43.020
that is what I actually call the
emotional AI when you design at the Year

00:18:43.020 --> 00:18:43.030
emotional AI when you design at the Year
 

00:18:43.030 --> 00:18:44.520
emotional AI when you design at the Year
system that you actually have to take

00:18:44.520 --> 00:18:44.530
system that you actually have to take
 

00:18:44.530 --> 00:18:47.820
system that you actually have to take
care of that okay well there's a lot of

00:18:47.820 --> 00:18:47.830
care of that okay well there's a lot of
 

00:18:47.830 --> 00:18:49.919
care of that okay well there's a lot of
thought there a lot of thought there

00:18:49.919 --> 00:18:49.929
thought there a lot of thought there
 

00:18:49.929 --> 00:18:52.440
thought there a lot of thought there
going on and will be going on and must

00:18:52.440 --> 00:18:52.450
going on and will be going on and must
 

00:18:52.450 --> 00:18:54.480
going on and will be going on and must
be going on from you youngsters

00:18:54.480 --> 00:18:54.490
be going on from you youngsters
 

00:18:54.490 --> 00:18:56.039
be going on from you youngsters
especially in the audience we're going

00:18:56.039 --> 00:18:56.049
especially in the audience we're going
 

00:18:56.049 --> 00:18:58.110
especially in the audience we're going
to be living with this stuff throughout

00:18:58.110 --> 00:18:58.120
to be living with this stuff throughout
 

00:18:58.120 --> 00:19:00.390
to be living with this stuff throughout
your lives and your children too there

00:19:00.390 --> 00:19:00.400
your lives and your children too there
 

00:19:00.400 --> 00:19:03.630
your lives and your children too there
really are a lot of very important and

00:19:03.630 --> 00:19:03.640
really are a lot of very important and
 

00:19:03.640 --> 00:19:06.390
really are a lot of very important and
very difficult issues to think about so

00:19:06.390 --> 00:19:06.400
very difficult issues to think about so
 

00:19:06.400 --> 00:19:08.310
very difficult issues to think about so
thank you ever so much the panel from

00:19:08.310 --> 00:19:08.320
thank you ever so much the panel from
 

00:19:08.320 --> 00:19:10.980
thank you ever so much the panel from
raising many of them thank you

00:19:10.980 --> 00:19:10.990
raising many of them thank you
 

00:19:10.990 --> 00:19:23.180
raising many of them thank you
you

00:19:23.180 --> 00:19:23.190
 

00:19:23.190 --> 00:19:25.250
you

