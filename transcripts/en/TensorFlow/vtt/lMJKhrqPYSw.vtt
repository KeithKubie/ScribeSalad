WEBVTT
Kind: captions
Language: en

00:00:00.255 --> 00:00:02.733
♪ (intro music) ♪

00:00:05.653 --> 00:00:10.634
It's really just in all of --
what machine learning is capable of,

00:00:10.634 --> 00:00:14.823
in how we can extend human capabilities.

00:00:14.823 --> 00:00:18.810
And we want to think more than just
about discovering new approaches

00:00:18.810 --> 00:00:23.673
and new ways of using technology;
we want to see how it's being used

00:00:23.673 --> 00:00:28.724
and how it impacts
the human creative process.

00:00:28.724 --> 00:00:34.143
So imagine, you need to find
or compose a drum pattern,

00:00:34.143 --> 00:00:40.914
and you have some idea of a drum beat
that you would like to compose,

00:00:41.724 --> 00:00:44.862
and all you need to do now
is go to a website

00:00:44.862 --> 00:00:49.299
where there's a pre-trained model
of drum patterns

00:00:49.299 --> 00:00:53.293
sitting on the online--
you just need a web browser.

00:00:53.293 --> 00:00:59.319
You give it some human input and you can
generate a space of expressive variations.

00:00:59.319 --> 00:01:02.352
You can tune and control
the type of outputs

00:01:02.352 --> 00:01:04.874
that you're getting
from this generative model.

00:01:04.874 --> 00:01:07.622
And if you don't like it,
you can continue going through it

00:01:07.622 --> 00:01:10.873
exploring this generative space.

00:01:13.255 --> 00:01:17.418
So this is the type of work
that Project Magenta focuses on.

00:01:18.276 --> 00:01:21.944
To give you a bird's eye view
of what Project Magenta is about,

00:01:21.944 --> 00:01:27.464
it basically is a group of researchers,
and developers and creative technologists

00:01:27.464 --> 00:01:29.864
that engage in generative models research.

00:01:29.864 --> 00:01:32.804
So you'll see this work published
in machine learning conferences,

00:01:32.804 --> 00:01:34.223
you'll see the workshops,

00:01:34.223 --> 00:01:38.095
you'll see a lot of research contributions

00:01:38.095 --> 00:01:39.325
from Magenta.

00:01:39.325 --> 00:01:43.414
You'll also see the code
that after it's been published

00:01:43.414 --> 00:01:49.672
put into open source repository
on GitHub in the Magenta repo.

00:01:49.672 --> 00:01:54.013
And then from there we'll see ways
of thinking and designing creative tools

00:01:54.013 --> 00:01:58.671
that can enhance and extend the human
expressive creative process.

00:01:59.384 --> 00:02:06.235
And eventually, ending up into the hands
of artists and musicians

00:02:06.235 --> 00:02:10.934
inventing new ways we can create
and inventing new types of artists.

00:02:10.934 --> 00:02:14.782
So, I'm going to give three brief
overviews of the highlights

00:02:14.782 --> 00:02:16.354
of some of our recent work.

00:02:16.354 --> 00:02:17.983
So this is PerformanceRNN.

00:02:17.983 --> 00:02:21.274
How many people have seen this?
This is one of the demos earlier today.

00:02:21.274 --> 00:02:23.535
A lot of people have seen
and heard of this kind of work,

00:02:23.535 --> 00:02:26.063
and this is what people typically think
of when they're thinking

00:02:26.063 --> 00:02:30.004
of a generative model, they're thinking,
"How can we build a computer

00:02:30.004 --> 00:02:32.954
that has the kind of intuition
to know the qualities

00:02:32.954 --> 00:02:38.138
of things like melody and harmony,
but also expressive timing and dynamics?"

00:02:38.138 --> 00:02:41.526
And what's even more--
it's even more interesting now

00:02:41.526 --> 00:02:46.607
to be able to explore this for yourself
in the browser enabled by TensorFlow.js.

00:02:46.607 --> 00:02:48.634
So,this is a demo we have running online.

00:02:48.634 --> 00:02:54.230
We have the ability to tune and control
some of the output that we're getting.

00:02:54.230 --> 00:02:58.015
So in a second, I'm going to show you
this video of what that looks like,

00:02:58.015 --> 00:02:59.780
you would have seen
it out on the demo floor

00:02:59.780 --> 00:03:02.493
but we will show you
and all of you watching online,

00:03:02.493 --> 00:03:07.024
and we were also able to bring it
even more alive by connecting

00:03:07.024 --> 00:03:11.894
a baby grand piano Disklavier
that is also a midi controller

00:03:11.894 --> 00:03:17.669
and we have the ability to perform
alongside the generative model

00:03:17.669 --> 00:03:21.004
reading in the inputs
from the human playing the piano.

00:03:21.004 --> 00:03:22.579
So, let's take a look.

00:03:23.006 --> 00:03:26.292
♪ (piano) ♪

00:03:26.292 --> 00:03:32.584
So this is trained on classical music
data from actual live performers.

00:03:32.584 --> 00:03:37.967
This is from a data set that we got,
from a piano competition.

00:03:37.967 --> 00:03:41.777
♪ (piano) ♪

00:03:41.777 --> 00:03:44.645
I don't know if you noticed,
this is Nikhil from earlier today.

00:03:44.645 --> 00:03:47.180
He's actually quite a talented young man.

00:03:48.045 --> 00:03:53.253
He helped build out the browser version
of PerformanceRNN.

00:03:53.253 --> 00:03:56.570
♪ (piano) ♪

00:04:02.502 --> 00:04:06.522
And so we're thinking of ways
that we take bodies of work,

00:04:06.522 --> 00:04:11.914
we train a model off of the data,
then we create these open source tools

00:04:11.914 --> 00:04:16.613
that enable new forms of interaction
of creativity and of expression.

00:04:16.613 --> 00:04:20.328
And this is all these points of engagement
are enabled by TensorFlow.

00:04:21.415 --> 00:04:24.141
The next tool I want to talk about
that we've been working on

00:04:24.141 --> 00:04:26.072
is Variational Autoencoders.

00:04:26.072 --> 00:04:28.952
How many people are familiar
Layton's space interpolation?

00:04:28.952 --> 00:04:30.392
Okay, quite a few of you.

00:04:30.392 --> 00:04:34.772
And if you're not, it's quite simple--
you take human inputs

00:04:34.772 --> 00:04:36.562
and you train it through
on your own network,

00:04:36.562 --> 00:04:38.502
compressing it down to an embedding space.

00:04:38.502 --> 00:04:41.746
So you compress it down
to some dimensionality

00:04:41.746 --> 00:04:43.250
and then you reconstruct it.

00:04:43.250 --> 00:04:48.286
So you're comparing the reconstruction
with the original and trying to train,

00:04:48.286 --> 00:04:51.197
build a space around that,
and what that does is that creates

00:04:51.197 --> 00:04:55.806
the ability to interpolate
from one point to another

00:04:55.806 --> 00:05:00.215
touching on the intermediate points
where a human may have not given input.

00:05:00.215 --> 00:05:02.805
So the machine learning model
may have never seen an example

00:05:02.805 --> 00:05:05.461
that it's able to generate,
because it's building an intuition

00:05:05.461 --> 00:05:07.614
off of these examples.

00:05:07.614 --> 00:05:09.339
So, you can imagine if you're an animator,

00:05:09.339 --> 00:05:11.717
there's so many ways
of going from cat to pig.

00:05:11.717 --> 00:05:13.316
How would you animate that?

00:05:13.316 --> 00:05:16.167
So, we're train--there's an intuition
that the artist would have

00:05:16.167 --> 00:05:18.706
in creating that sort of morphing
from one to the other.

00:05:18.706 --> 00:05:22.109
So we're able to have the machine learning
model now also do this.

00:05:23.026 --> 00:05:25.508
We can also do this with sound, right?

00:05:25.508 --> 00:05:28.726
This technology actually carries
over to multiple domains.

00:05:28.726 --> 00:05:32.777
So, this is NSynth,
and we've released this,

00:05:32.777 --> 00:05:34.607
I think some time last year.

00:05:34.607 --> 00:05:36.597
And what it does is
it takes that same idea

00:05:36.597 --> 00:05:38.265
of moving one input to another.

00:05:38.265 --> 00:05:40.800
So, let's take a look.
You'll get a sense of it.

00:05:40.800 --> 00:05:42.294
Piccolo to electric guitar.

00:05:42.294 --> 00:05:45.619
(electric guitar sound to piccolo)

00:05:45.619 --> 00:05:49.250
(piccolo sound to electric guitar)

00:05:49.250 --> 00:05:52.436
(piccolo and electric guitar
sound together)

00:05:52.436 --> 00:05:57.475
So, rather than recomposing
or fading from one sound to the other,

00:05:57.475 --> 00:06:01.485
what we're actually able to do
is we're able to find these intermediary,

00:06:01.485 --> 00:06:05.246
recomposed sound samples
and produce that.

00:06:05.246 --> 00:06:08.312
So, it looks, you know,
there's a lot of components to that.

00:06:08.312 --> 00:06:10.983
There's a wave and a decoder,
but really it's the same technology

00:06:10.983 --> 00:06:15.732
underlying the encoder-decoder
variational autoencoder.

00:06:15.732 --> 00:06:18.642
But when we think about the types
of tools that musicians use,

00:06:18.642 --> 00:06:21.882
we think less about training
machine learning models.

00:06:21.882 --> 00:06:25.516
We see drum pedals right? --
I mean not drum pedals.

00:06:25.516 --> 00:06:27.764
Guitar pedals, these knobs
and these pedals

00:06:27.764 --> 00:06:30.984
that are used to tune and refine sound
to cultivate the kind of art

00:06:30.984 --> 00:06:33.292
and flavor a musician is looking for.

00:06:33.292 --> 00:06:36.609
We don't think so much
about setting parameter flags

00:06:36.609 --> 00:06:41.503
or trying to write lines of python code

00:06:41.503 --> 00:06:44.993
to create this sort of art in general.

00:06:44.993 --> 00:06:46.449
So what we've done--

00:06:46.449 --> 00:06:49.824
Not just are we interested
in finding and discovering new things.

00:06:49.824 --> 00:06:54.192
We're also interested in how those things
get used in general--

00:06:54.192 --> 00:06:56.677
used by practitioners,
used by specialists.

00:06:56.677 --> 00:07:02.369
And so we've created hardware,
we've taken a piece of hardware

00:07:02.369 --> 00:07:05.301
where we've taken the machine learning
model, we've put it into a box

00:07:05.301 --> 00:07:07.341
where a musician can just plug in

00:07:07.341 --> 00:07:09.781
and explore this latent space
in performance.

00:07:09.781 --> 00:07:14.813
So take a look on how musicians feel,
what they think in this process.

00:07:15.683 --> 00:07:19.502
♪ (music) ♪

00:07:20.733 --> 00:07:22.666
<i>(woman) I just feel like
we're turning a corner</i>

00:07:22.666 --> 00:07:25.332
<i>of what could be new possibility.</i>

00:07:25.332 --> 00:07:27.982
<i>It could generate a sound
that might inspire us.</i>

00:07:28.942 --> 00:07:31.762
<i>(man) The fun part is even though
you think you know what you're doing,</i>

00:07:31.762 --> 00:07:33.504
<i>there's some weird interaction happening</i>

00:07:33.504 --> 00:07:37.059
<i>that can give you something
totally unexpected.</i>

00:07:40.139 --> 00:07:43.002
I mean, it's great research,
and it's really fun,

00:07:43.002 --> 00:07:46.510
and it's amazing to discover new things,
but it's even more amazing to see

00:07:46.510 --> 00:07:51.627
how it gets used and what people
think to create with alongside it.

00:07:51.627 --> 00:07:55.617
And so, what's even better
is that it's just released, NSynth Super,

00:07:55.617 --> 00:07:58.251
in collaboration
with the Creative Lab London.

00:07:58.251 --> 00:08:01.127
It's an open source hardware project.

00:08:01.127 --> 00:08:03.404
All the information
and the specs are on GitHub.

00:08:03.404 --> 00:08:05.931
We talk about everything
from potentiometers,

00:08:05.931 --> 00:08:11.163
to the touch panel, to the code
and what hardware it's running on.

00:08:11.738 --> 00:08:14.916
And this is all available
to everyone here today.

00:08:15.480 --> 00:08:18.153
You just go online
and you can check it out yourself.

00:08:18.661 --> 00:08:21.352
Now music is more than just sound right?

00:08:21.352 --> 00:08:23.492
It's actually a sequence
of things that goes on.

00:08:23.492 --> 00:08:27.981
So when we think about this idea
of what it means

00:08:27.981 --> 00:08:32.260
to have a generative music space,
we think also about melodies,

00:08:32.260 --> 00:08:35.084
and so just like we have cat to pig,

00:08:35.084 --> 00:08:37.633
what is it like to go
from one melody to the next?

00:08:38.302 --> 00:08:42.071
And moreover, once we have
that technology, how does it --

00:08:42.071 --> 00:08:43.790
what does it look like
to create with that?

00:08:43.790 --> 00:08:46.592
You have this expressive space
of variations--

00:08:46.592 --> 00:08:49.519
how do we design an expressive tool
that takes advantage of that?

00:08:49.519 --> 00:08:50.777
And what will we get out of it?

00:08:50.777 --> 00:08:52.750
So this is also another tool

00:08:52.750 --> 00:08:55.620
that's developed
by another team at Google,

00:08:55.620 --> 00:08:57.985
to make use of melodies in a latent space,

00:08:57.985 --> 00:08:59.566
so how interpolation works,

00:08:59.566 --> 00:09:03.343
and then building a song
or some sort of composition with it.

00:09:03.343 --> 00:09:06.076
So let's take a listen.
Say you have two melodies....

00:09:06.466 --> 00:09:10.530
♪ (<i>"Twinkle Twinkle Little Star"</i>) ♪

00:09:15.750 --> 00:09:17.085
And in the middle....

00:09:17.265 --> 00:09:21.341
♪ (piano playing variation) ♪

00:09:23.961 --> 00:09:25.152
You can extend it....

00:09:25.272 --> 00:09:28.719
♪ (piano playing variation) ♪

00:09:40.980 --> 00:09:44.658
And we really are just scratching
the surface of what's possible.

00:09:44.658 --> 00:09:47.779
How do we continue
to have the machine learn

00:09:47.779 --> 00:09:51.079
and have a better intuition
for what melodies are about.

00:09:51.079 --> 00:09:55.871
So again to bring it back full circle,
using different compositions

00:09:55.871 --> 00:10:01.520
and musical works, we're able to train
a variational autoencoder

00:10:01.520 --> 00:10:05.110
to create an embedding space
that builds tools that enable

00:10:05.110 --> 00:10:08.709
open source communities
to design creative artist's tools

00:10:08.709 --> 00:10:12.260
to look at new ways of pushing
the expressive boundaries

00:10:12.260 --> 00:10:14.029
that we currently have.

00:10:14.029 --> 00:10:17.888
This is, again, just released!
It's on our blog.

00:10:17.888 --> 00:10:22.139
All the code is an open source
and made available to you,

00:10:22.139 --> 00:10:24.470
and also enabled by TensorFlow.

00:10:24.470 --> 00:10:28.808
In addition to all these other things,
including Nikhil,

00:10:28.808 --> 00:10:34.488
here, enabled by the type of work
and creativity and expressivity.

00:10:35.107 --> 00:10:37.182
And so, in wrapping up
I want to take us back

00:10:37.182 --> 00:10:38.529
to this demo that we saw.

00:10:38.529 --> 00:10:42.622
Now the most interesting and maybe
the coolest thing about this demo,

00:10:42.622 --> 00:10:45.459
was that we didn't even know
that it was being built

00:10:45.459 --> 00:10:50.128
until it was tweeted by Tero,
a developer from Finland.

00:10:50.829 --> 00:10:55.600
And the fact of the matter is that
we're just barely scratching the surface.

00:10:55.600 --> 00:10:58.589
There's so much to do,
so much to engage in,

00:10:58.589 --> 00:10:59.969
and so much to discover.

00:10:59.969 --> 00:11:02.063
And we want to see so much more of this.

00:11:02.063 --> 00:11:04.858
We want to see more developers,
more people sharing things

00:11:04.858 --> 00:11:06.569
and more people getting engaged.

00:11:06.569 --> 00:11:09.469
Not just developers,
but artists and creatives as well.

00:11:09.469 --> 00:11:13.379
We want to explore and invent
and imagine what we can do

00:11:13.379 --> 00:11:17.005
with machine learning together
as an expressive tool.

00:11:17.005 --> 00:11:21.080
And so, go to our website,
<i>g.co/magenta</i>.

00:11:21.080 --> 00:11:24.399
There you'll find our publications
and these demos,

00:11:24.399 --> 00:11:26.758
you can experience it yourself, and more.

00:11:26.758 --> 00:11:30.869
And you can also join
our discussion group.

00:11:30.869 --> 00:11:33.179
So here's <i>g.co/magenta.</i>

00:11:33.179 --> 00:11:35.617
Join our discussion group,
become part of the community,

00:11:35.617 --> 00:11:39.219
and share the things that you're building,
so we can do this alongside together.

00:11:39.219 --> 00:11:40.539
Thank you so much.

00:11:40.539 --> 00:11:43.586
(applause)

00:11:48.213 --> 00:11:50.249
So that's it for the talks today.

00:11:50.249 --> 00:11:55.510
We had an amazing, amazing show,
amazing spread of speakers and topics.

00:11:55.510 --> 00:11:59.459
Now, let's take a look at
a highlight review of the day.

00:12:01.970 --> 00:12:04.270
♪ (music) ♪

00:12:29.169 --> 00:12:32.917
Earlier this year we hit the milestone
of 11 million downloads.

00:12:32.917 --> 00:12:36.778
We really excited to see how much users
are using this and how much impact

00:12:36.778 --> 00:12:38.776
it's having in the world.

00:12:39.479 --> 00:12:43.958
We're very excited today
to announce that deeplearn.js

00:12:43.958 --> 00:12:46.274
is joining the TensorFlow family.

00:12:46.274 --> 00:12:47.753
♪ (music) ♪

00:12:50.200 --> 00:12:52.930
<i>(man) The software TensorFlow,
is also an early-stage project.</i>

00:12:52.930 --> 00:12:54.890
<i>And so we'd really love for you
to get interested</i>

00:12:54.890 --> 00:12:57.221
<i>and help us to build this future.</i>

00:12:57.221 --> 00:12:59.290
♪ (music) ♪

00:13:00.695 --> 00:13:03.182
<i>(man) I told you at the beginning
that our mission for TF data</i>

00:13:03.182 --> 00:13:08.300
was to make a live [inaudible] processing
that is fast, flexible and easy to use.

00:13:08.300 --> 00:13:09.630
♪ (music) ♪

00:13:10.292 --> 00:13:12.702
<i>(woman) I'm very excited to say
that we have been working</i>

00:13:12.702 --> 00:13:17.106
<i>with other teams in Google
to bring TensorFlow Lite to Google Labs.</i>

00:13:17.106 --> 00:13:18.810
♪ (music) ♪

00:13:19.930 --> 00:13:21.965
(man) In general the Google
Brain Team's mission

00:13:21.965 --> 00:13:23.742
is to make machines intelligent,

00:13:23.742 --> 00:13:26.330
and then use that ability
to improve people's lives.

00:13:26.330 --> 00:13:29.321
I think these are good examples of where
there's real opportunity for this.

00:13:29.321 --> 00:13:30.948
♪ (music ends) ♪

00:13:31.922 --> 00:13:33.454
(applause)

00:13:33.864 --> 00:13:36.986
♪ (music) ♪

