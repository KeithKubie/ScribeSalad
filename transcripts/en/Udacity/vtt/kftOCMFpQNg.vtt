WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:01.880
So that brings us to the end of the

00:00:01.880 --> 00:00:03.590
topics we are going to talk about in terms of

00:00:03.590 --> 00:00:05.290
neural nets. There's going to be some interesting stuff

00:00:05.290 --> 00:00:08.130
for you to do in terms of the homework where

00:00:08.130 --> 00:00:10.210
you'll be exposed to some other important concepts. But

00:00:10.210 --> 00:00:12.590
that's, that's all we're going to lecture about for now.

00:00:12.590 --> 00:00:17.300
So let's just remind ourselves what exactly we covered in

00:00:17.300 --> 00:00:20.020
the neural net section. So Charles what do you remember?

00:00:21.110 --> 00:00:25.020
&gt;&gt; I remember perceptrons. I remember.

00:00:25.020 --> 00:00:25.830
&gt;&gt; And

00:00:25.830 --> 00:00:28.960
perceptron was a threshold unit, a linear threshold

00:00:28.960 --> 00:00:31.310
unit, and we could put networks of them together.

00:00:31.310 --> 00:00:31.970
&gt;&gt; Yes.

00:00:31.970 --> 00:00:36.520
&gt;&gt; To produce any Boolean function. What else?

00:00:36.520 --> 00:00:38.970
Oh, we had a learning rule for perceptrons.

00:00:38.970 --> 00:00:43.830
&gt;&gt; Mm-hm. Which runs in finite time for linearly separable data sets.

00:00:43.830 --> 00:00:47.430
&gt;&gt; And we learned a general differentiable rule. Adding

00:00:47.430 --> 00:00:50.360
general we learned about propagation using a gradient set.

00:00:50.360 --> 00:00:51.460
&gt;&gt; And

00:00:51.460 --> 00:00:54.480
we talked a little bit about the, about the preference

00:00:54.480 --> 00:00:58.420
and restriction by c's of neural networks. Alright, til next time.

00:00:58.420 --> 00:00:59.180
&gt;&gt; See you Michael.

