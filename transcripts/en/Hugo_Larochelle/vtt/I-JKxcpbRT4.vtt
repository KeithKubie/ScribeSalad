WEBVTT

1
00:00:00.920 --> 00:00:01.070
Okay.

2
00:00:01.070 --> 00:00:04.460
<v 1>In this video we'll introduce the concept of pooling in sub sampling from</v>

3
00:00:04.461 --> 00:00:05.660
convolutional neural networks.

4
00:00:07.670 --> 00:00:12.080
So we're ready now to look at this third idea that characterizes a convolutional

5
00:00:12.081 --> 00:00:12.914
neural networks,

6
00:00:13.170 --> 00:00:17.150
which is the idea of pooling and sub sampling hidden units in a convolutional

7
00:00:17.151 --> 00:00:17.984
neural net.

8
00:00:19.530 --> 00:00:19.960
<v 0>Yeah,</v>

9
00:00:19.960 --> 00:00:20.980
<v 1>so we talked before,</v>

10
00:00:21.010 --> 00:00:25.390
thanks to the local connectivity of the hidden units and with parameters sharing

11
00:00:25.720 --> 00:00:30.340
that we could compute,
uh,
the activation of a hidden there,

12
00:00:30.420 --> 00:00:35.200
a convolutional neural network by,
uh,
operating,

13
00:00:35.201 --> 00:00:37.090
doing multiple convolutions,

14
00:00:37.091 --> 00:00:41.170
one for each input channel between the input channel and,
uh,

15
00:00:41.171 --> 00:00:45.910
some feature map summing over all the input channels to aggregate all the pre

16
00:00:45.911 --> 00:00:47.830
activations from all the input channels.

17
00:00:48.130 --> 00:00:51.010
And then applying like we usually do in feed for known network,

18
00:00:51.100 --> 00:00:54.490
some nonlinearity on the,
uh,
p activations.

19
00:00:54.730 --> 00:00:58.930
And then that would give us the activation of the hidden units in some given

20
00:00:58.931 --> 00:00:59.764
feature map.

21
00:01:00.040 --> 00:01:04.990
So this formula might characterize for one j the value of the feature map.

22
00:01:04.991 --> 00:01:08.230
And then we would have in the whole hidden layer,

23
00:01:08.500 --> 00:01:12.250
usually multiple feature maps.
So we'd repeat this operation,

24
00:01:12.251 --> 00:01:16.240
but with different convolutional kernels.
So with different hidden weights,

25
00:01:16.241 --> 00:01:21.241
matrix w and so this operation of multiple convolutions with nonlinearity with

26
00:01:23.020 --> 00:01:26.860
defined the activation value of a convolutional,

27
00:01:27.040 --> 00:01:28.390
a hidden layer.

28
00:01:31.300 --> 00:01:31.730
<v 0>Yeah.</v>

29
00:01:31.730 --> 00:01:34.220
<v 1>On top of this,
we'll usually add another operation,</v>

30
00:01:34.221 --> 00:01:37.230
which we call pooling and sub sampling.
So,
uh,

31
00:01:37.240 --> 00:01:41.030
our first define what it does and then I'll describe a intuitively why it's a,

32
00:01:41.031 --> 00:01:42.020
it's a good thing to have.

33
00:01:42.890 --> 00:01:47.890
The idea of fooling is dia essentially corresponds to taking a set of a hidden

34
00:01:49.761 --> 00:01:54.410
units and aggregating their activation somehow to obtain just a single number.

35
00:01:55.160 --> 00:01:58.520
So one thing we could do,
and this is referred to as Max pooling,

36
00:01:58.760 --> 00:02:03.760
is to take all the hidden units in some given feature map in a dollar in the

37
00:02:05.211 --> 00:02:08.300
same local region.
Uh,
so remember feature map.

38
00:02:08.480 --> 00:02:13.000
We can think of it as an image or we can,
they're there locally,
uh,
uh,

39
00:02:13.010 --> 00:02:15.760
placed in the,
uh,
in the feature maps.

40
00:02:15.761 --> 00:02:18.440
So we can take just a passionate for,

41
00:02:19.050 --> 00:02:22.550
if I want to think about as a receptive field.
And then,
uh,

42
00:02:22.580 --> 00:02:27.110
in that region taking all of the activations of the hidden unit with Max pooling

43
00:02:27.111 --> 00:02:29.780
would be just taking,
uh,
so,
uh,

44
00:02:29.870 --> 00:02:34.100
only taking the maximum value of all the hidden units,
which is illustrated here.

45
00:02:34.100 --> 00:02:37.340
So with some index p and Q,

46
00:02:37.430 --> 00:02:41.660
which indexes some local neighborhood in,
uh,
the,
uh,

47
00:02:41.661 --> 00:02:43.490
in some region in the feature map,

48
00:02:43.820 --> 00:02:48.620
we just take the maximum value in that local neighborhood and that would then

49
00:02:48.621 --> 00:02:53.270
become the new value at a given position in the subsequent,

50
00:02:53.640 --> 00:02:56.510
uh,
layer of our conclusion or neural network.

51
00:02:57.830 --> 00:02:59.180
And this pooling,

52
00:02:59.890 --> 00:03:03.400
we will normally do it over non overlapping neighborhoods.

53
00:03:03.610 --> 00:03:08.610
So if we have a big feature map we could say divided into three like this.

54
00:03:10.660 --> 00:03:15.460
And then in each neighborhood who would actually take individually for each

55
00:03:15.461 --> 00:03:18.910
neighborhood,
the maximum,
and that would give us,
say maybe the,

56
00:03:19.040 --> 00:03:23.350
it was like nine by five,
a nine by nine for this,

57
00:03:23.790 --> 00:03:26.350
say this was a feature map that was done by nine by nine.

58
00:03:26.351 --> 00:03:29.830
So there were actually three activations within each neighborhood.

59
00:03:30.100 --> 00:03:33.910
Then after the pooling operation,
because we did a makeover,

60
00:03:34.120 --> 00:03:38.380
none overlapping neighborhoods who would actually get a matrix.

61
00:03:38.381 --> 00:03:42.010
That's only three by three.
Okay.

62
00:03:42.011 --> 00:03:45.430
And then each value here would be the maximum activation within the curse

63
00:03:45.431 --> 00:03:47.110
funding local neighborhood.

64
00:03:48.310 --> 00:03:50.830
So that's why we say it's pooling.

65
00:03:50.831 --> 00:03:55.540
The pooling corresponds to taking the Max and then sub sampling because we have

66
00:03:55.760 --> 00:03:56.030
a,

67
00:03:56.030 --> 00:04:01.030
a smaller matrix with fewer rows and columns after we've done this operation.

68
00:04:03.290 --> 00:04:06.920
So just know this also,
again,
I'm using the rotation from Jared.

69
00:04:07.100 --> 00:04:10.720
The way that the finding different operations is that the input of the operation

70
00:04:10.721 --> 00:04:14.180
is always x and then the output is always y.

71
00:04:14.181 --> 00:04:18.680
So x here would actually be from my previous slide,

72
00:04:19.220 --> 00:04:22.120
the,
uh,
computation of this,
uh,

73
00:04:22.140 --> 00:04:24.560
of these convolutions plus nonlinearity.

74
00:04:25.370 --> 00:04:27.920
So this would be the x that we see here.

75
00:04:28.070 --> 00:04:31.730
And then we'd get an additional subsequent layer here,

76
00:04:31.880 --> 00:04:34.250
which would be the pooling,
some sampling layer,

77
00:04:34.400 --> 00:04:37.400
which Chris Carr with curse bond in this case to doing Max pooling

78
00:04:39.640 --> 00:04:42.190
and other,
uh,
variants of,

79
00:04:42.250 --> 00:04:45.640
or an alternative of the Max pooling is to do average pooling.

80
00:04:46.360 --> 00:04:50.410
So if the size of the neighborhood over which I'm doing pooling is m,

81
00:04:50.800 --> 00:04:55.240
then I just take the average of all the activations in that neighborhood.

82
00:04:55.241 --> 00:05:00.220
So I would take the sum and divided by m squared.
So that's uh,

83
00:05:00.250 --> 00:05:03.130
called average pooing instead of Max pooling?

84
00:05:06.790 --> 00:05:08.710
No.
Um,
there are,
uh,

85
00:05:08.740 --> 00:05:13.360
certain advantages to pooling some sampling operation.
The first one,

86
00:05:13.361 --> 00:05:14.710
which is perhaps the most obvious,

87
00:05:14.711 --> 00:05:19.690
is that it reduces the number of hidden units in the next hidden layer.

88
00:05:20.350 --> 00:05:24.310
And a,
this is because of the sub sampling operation essentially,

89
00:05:24.970 --> 00:05:29.260
and a f on that hidden layer.
We were to then perform some more,

90
00:05:29.650 --> 00:05:34.330
uh,
convolutions with nonlinearity.
Then this means that computing,

91
00:05:34.331 --> 00:05:38.680
this subsequent convolutional layer will be more efficient because there will be

92
00:05:39.010 --> 00:05:41.900
a,
the size of the,
uh,
uh,

93
00:05:41.920 --> 00:05:45.760
the feature maps that are fed as input to these new convolutions,
uh,

94
00:05:45.790 --> 00:05:48.280
will be smaller so there will be more efficient to compute.

95
00:05:48.430 --> 00:05:49.930
So that's a first advantage.

96
00:05:50.860 --> 00:05:55.540
Another advantage is that it introduces some invariants to local translation.

97
00:05:55.541 --> 00:06:00.200
So that's actually mostly true for the Max operation.
So,
uh,

98
00:06:00.240 --> 00:06:05.210
essentially if the maximum value in some neighborhood,
uh,

99
00:06:05.240 --> 00:06:10.220
doesn't change after we've apply on the image is small translation.

100
00:06:10.850 --> 00:06:11.301
In other words,

101
00:06:11.301 --> 00:06:16.010
if the maximum value in the neighborhoods still remains in the same,
uh,

102
00:06:16.040 --> 00:06:19.460
in the same neighborhood after the trends,
the translation,

103
00:06:19.490 --> 00:06:24.120
it means that after pooling the value of the,
uh,

104
00:06:24.160 --> 00:06:28.190
the pool,
the pool value will not change because the Max is still the same.

105
00:06:28.910 --> 00:06:33.620
And so intuitively it means that if we have local translations like this where

106
00:06:33.621 --> 00:06:37.730
the Max remains in the same our local neighborhoods everywhere,

107
00:06:37.731 --> 00:06:42.731
that it means we get exactly the same pool than subsample a hidden layer in the

108
00:06:42.831 --> 00:06:44.090
compulsion of neural network.

109
00:06:45.210 --> 00:06:49.530
So this way we were able to build some sort of local translation in variants,

110
00:06:49.740 --> 00:06:53.450
which is useful for convolutional neural networks because,
uh,

111
00:06:53.520 --> 00:06:54.810
for object recognition say,

112
00:06:54.811 --> 00:06:58.740
because the position of the object does not necessarily define the actual

113
00:06:58.741 --> 00:06:59.880
identity of the object.

114
00:07:02.130 --> 00:07:07.080
So here's an example of two images over which we,

115
00:07:07.100 --> 00:07:11.190
uh,
perform a convolution followed by Max pooling.
Uh,

116
00:07:11.191 --> 00:07:16.191
so this is actually a next step after the conclusion we saw in the previous

117
00:07:16.231 --> 00:07:21.231
video where we did a couple solution with a filter with a zero zero,

118
00:07:22.350 --> 00:07:26.280
0.5 0.5,
and then we applied to nonlinearity.
Well,

119
00:07:26.281 --> 00:07:31.281
turns out if I do this convolution plus nonlinearity and then I do a,

120
00:07:31.560 --> 00:07:33.990
uh,
a Max pooling operation,

121
00:07:34.380 --> 00:07:38.660
I get exactly the same pool,
the value of a pool,

122
00:07:38.661 --> 00:07:43.661
and some sample value of the hidden layer for both these images and a,

123
00:07:44.470 --> 00:07:45.780
and so,
you know,

124
00:07:45.810 --> 00:07:49.830
in this case it's hard to see how both would be somewhat equivalent.

125
00:07:50.030 --> 00:07:53.400
They are cool in the sense that both have nothing happening here.

126
00:07:53.401 --> 00:07:56.970
The only thing that moved really is this,
that went up.
Uh,

127
00:07:56.971 --> 00:08:00.630
and so this was a,
this,
and then we see that the hidden,

128
00:08:00.631 --> 00:08:04.890
they are sort of reflected the fact that nothing changed really up to very small

129
00:08:04.891 --> 00:08:07.590
local translation.
Uh,
operations.

