WEBVTT
Kind: captions
Language: en

00:00:01.984 --> 00:00:02.900
MALE SPEAKER: Welcome.

00:00:02.900 --> 00:00:05.900
It's my pleasure to
introduce Gil Weinberg.

00:00:05.900 --> 00:00:09.180
He's a professor at Georgia
Tech for the Center of Music

00:00:09.180 --> 00:00:10.650
and Technology.

00:00:10.650 --> 00:00:13.780
And I went to Georgia
Tech a long, long time ago

00:00:13.780 --> 00:00:16.180
and we didn't have a music
department back then.

00:00:16.180 --> 00:00:21.930
So when I visited recently, the
folks I was meeting with in ECE

00:00:21.930 --> 00:00:24.410
sent me over to
meet Gil and he's

00:00:24.410 --> 00:00:29.590
doing some really cool stuff
with interactive computing

00:00:29.590 --> 00:00:31.910
and musicianship.

00:00:31.910 --> 00:00:36.640
So building machines that
you can interact with,

00:00:36.640 --> 00:00:38.820
not just from a
dialogue perspective,

00:00:38.820 --> 00:00:40.570
like the kind of thing
I'm thinking about,

00:00:40.570 --> 00:00:42.080
but more from a
music perspective

00:00:42.080 --> 00:00:44.170
where you want to
jam and do some jazz

00:00:44.170 --> 00:00:46.421
improvisation with a robot.

00:00:46.421 --> 00:00:48.670
And he's going to talk about
that in many other things

00:00:48.670 --> 00:00:49.170
today.

00:00:49.170 --> 00:00:49.870
So welcome.

00:00:49.870 --> 00:00:51.484
GIL WEINBERG: Thank you.

00:00:51.484 --> 00:00:53.770
[APPLAUSE]

00:00:53.770 --> 00:00:54.340
Thank you.

00:00:54.340 --> 00:00:56.510
Thank you for coming.

00:00:56.510 --> 00:01:00.947
So I'm going to talk about
three or four main projects.

00:01:00.947 --> 00:01:02.780
Only three of them I
showed in these slides.

00:01:02.780 --> 00:01:05.920
The last one is a surprise.

00:01:05.920 --> 00:01:09.280
We did the project which we just
finished and I have some very,

00:01:09.280 --> 00:01:11.910
very fresh new slides.

00:01:11.910 --> 00:01:14.970
The reason I started
to be interested

00:01:14.970 --> 00:01:16.580
in robotic musicianship
is that I'm

00:01:16.580 --> 00:01:19.250
a musician before
I became interested

00:01:19.250 --> 00:01:24.740
in computation or in robotics,
I was a musician and still am.

00:01:24.740 --> 00:01:27.850
And I was always fascinated
by playing in a group.

00:01:27.850 --> 00:01:32.530
By being constantly ready
to change what I'm doing.

00:01:32.530 --> 00:01:35.100
By trying to build
on what other people

00:01:35.100 --> 00:01:36.520
are playing in real time.

00:01:36.520 --> 00:01:41.120
Improvising in a group, a visual
cues, auditory cues, of course.

00:01:41.120 --> 00:01:44.060
So when I started to be
interested in robotics,

00:01:44.060 --> 00:01:45.949
I wanted to capture
this experience.

00:01:45.949 --> 00:01:47.990
Before I show you some of
the efforts that I did,

00:01:47.990 --> 00:01:50.510
maybe I'll show a short clip
of me playing with a trumpet

00:01:50.510 --> 00:01:53.830
player and trying to see
the kind of experiences

00:01:53.830 --> 00:01:55.435
that I was trying to recreate.

00:01:55.435 --> 00:01:56.101
[VIDEO PLAYBACK]

00:01:56.101 --> 00:01:59.586
[JAZZ MUSIC]

00:02:09.257 --> 00:02:09.840
[END PLAYBACK]

00:02:09.840 --> 00:02:13.480
So I think you've seen
a lot of eye contact,

00:02:13.480 --> 00:02:16.100
trying to build a motif that
I'm not sure what is going to be

00:02:16.100 --> 00:02:18.266
and trying to create something
interesting with them

00:02:18.266 --> 00:02:18.910
back and forth.

00:02:18.910 --> 00:02:22.110
And the first robots that
I tried to develop it

00:02:22.110 --> 00:02:24.070
was building on these ideas.

00:02:24.070 --> 00:02:28.140
But what I wanted to do is to
have the robot understand music

00:02:28.140 --> 00:02:29.700
like humans do.

00:02:29.700 --> 00:02:34.000
The big idea that
I started with,

00:02:34.000 --> 00:02:36.380
was to create robots
that listen like humans,

00:02:36.380 --> 00:02:37.960
but improvise like machines.

00:02:37.960 --> 00:02:40.140
Because I felt that if I
want to push what music

00:02:40.140 --> 00:02:44.010
is about through new,
novel improvisation

00:02:44.010 --> 00:02:46.520
algorithms and new
acoustic playing,

00:02:46.520 --> 00:02:49.650
I first have to have connection
between humans and robots,

00:02:49.650 --> 00:02:52.474
and that's why there is the
listening like human part.

00:02:52.474 --> 00:02:54.640
Only then would I be able
to start to make the robot

00:02:54.640 --> 00:02:58.780
play like a machine in order to
create this kind of connection

00:02:58.780 --> 00:03:01.819
and relationship.

00:03:01.819 --> 00:03:03.360
I'll start with
something very simple

00:03:03.360 --> 00:03:07.420
that probably many of
you are familiar with.

00:03:07.420 --> 00:03:09.000
If I want a robot
to understand me,

00:03:09.000 --> 00:03:11.720
maybe the first simple
thing that I can make him do

00:03:11.720 --> 00:03:14.470
is understand the beat
of the music that I play.

00:03:14.470 --> 00:03:19.250
And here we use auto-correlation
and self-similarity algorithms.

00:03:19.250 --> 00:03:20.960
This is a piece from Bach.

00:03:20.960 --> 00:03:23.680
You see the time is both
on the x and on the y.

00:03:23.680 --> 00:03:26.360
And you see that it's
symmetric and by comparing

00:03:26.360 --> 00:03:27.860
the [INAUDIBLE] to
the algorithm you

00:03:27.860 --> 00:03:29.940
can try to find sections
that are similar

00:03:29.940 --> 00:03:32.620
and detect the beat from that.

00:03:32.620 --> 00:03:35.290
But what we try to do is to
actually have it in real time.

00:03:35.290 --> 00:03:37.500
And you see here my
student Scott Driscoll

00:03:37.500 --> 00:03:41.640
used this algorithm based
on Davies and Plumbley

00:03:41.640 --> 00:03:43.420
from Queen Mary.

00:03:43.420 --> 00:03:45.690
And you see that's
it becomes much more

00:03:45.690 --> 00:03:50.035
sophisticated because it's not
just analyzing the beat in Bach

00:03:50.035 --> 00:03:54.900
or in the Beatles-- Scott is
playing in real-time so he

00:03:54.900 --> 00:03:57.370
is trying to get the
beat, but then with Haile,

00:03:57.370 --> 00:03:58.530
start to play with it.

00:03:58.530 --> 00:04:00.421
Scott is trying
to fit what he is

00:04:00.421 --> 00:04:02.920
doing to what Haile-- Haile is
a robot-- and back and forth.

00:04:02.920 --> 00:04:04.669
So it's a little more
complicated you see.

00:04:04.669 --> 00:04:07.870
Sometimes they escape from the
beat and get back to the beat.

00:04:07.870 --> 00:04:09.815
And here's a short example.

00:04:13.810 --> 00:04:14.870
Got the beat.

00:04:14.870 --> 00:04:16.924
Now Scott will start
faster and slower.

00:04:20.170 --> 00:04:22.718
Faster , got it.

00:04:33.470 --> 00:04:35.980
So as you can see, it
loses it, it gets it back.

00:04:35.980 --> 00:04:43.450
I think in a second, it
will play slower which

00:04:43.450 --> 00:04:44.860
shows how the system fails.

00:04:47.655 --> 00:04:49.280
The next thing that
I was trying to get

00:04:49.280 --> 00:04:53.290
is to have the robot understand
other things at a high level

00:04:53.290 --> 00:04:54.070
musically.

00:04:54.070 --> 00:04:56.677
Not just the beat but
concepts that we humans

00:04:56.677 --> 00:04:58.510
understand such as
stability and similarity.

00:04:58.510 --> 00:05:02.560
And basically we
had a huge database

00:05:02.560 --> 00:05:06.170
of rhythm generated almost
randomly-- with some rules,

00:05:06.170 --> 00:05:07.630
some stochastic rules.

00:05:07.630 --> 00:05:11.800
And then we had a coefficient
for stability and similarity.

00:05:11.800 --> 00:05:14.809
Whenever Haile listened
to a particular beat

00:05:14.809 --> 00:05:16.350
there's some settings
and coefficient

00:05:16.350 --> 00:05:17.620
for stability and similarities.

00:05:17.620 --> 00:05:19.750
At some point the robot
actually decided by itself.

00:05:19.750 --> 00:05:23.550
First the human on the side
can change the similarity

00:05:23.550 --> 00:05:27.190
and stability and create some
output to bring rhythm back.

00:05:27.190 --> 00:05:31.970
And this similarity is based
on Tanguiane from 1993,

00:05:31.970 --> 00:05:33.700
basically looking
at the overlapping

00:05:33.700 --> 00:05:34.850
onset between beats.

00:05:34.850 --> 00:05:39.470
I can get more into this
if you want, maybe later.

00:05:39.470 --> 00:05:42.070
And this is as
similarity algorithm

00:05:42.070 --> 00:05:44.550
based on Desain and Honing.

00:05:44.550 --> 00:05:46.350
[INAUDIBLE] between
adjunct intervals

00:05:46.350 --> 00:05:49.660
is what set how
stable the rhythm is.

00:05:49.660 --> 00:05:51.560
This is based on music
perception studies

00:05:51.560 --> 00:05:52.470
that I've been doing.

00:05:52.470 --> 00:05:54.470
And basically, there is
a mathematical procedure

00:05:54.470 --> 00:05:55.940
where you compare
each one of the notes

00:05:55.940 --> 00:05:57.390
to the note that comes after it.

00:05:57.390 --> 00:06:03.010
And at some point, after giving
preference to one and two,

00:06:03.010 --> 00:06:07.209
which are stable, you can get
for every particular rhythem--

00:06:07.209 --> 00:06:09.000
for example, this one,
the quarter quarter,

00:06:09.000 --> 00:06:12.000
two-eighths quarter-- a
particular onset stability

00:06:12.000 --> 00:06:15.620
by combining all of the ratios
and getting something stable.

00:06:15.620 --> 00:06:18.710
And here's a short
example of Scott

00:06:18.710 --> 00:06:21.470
playing with Haile
and Haile trying

00:06:21.470 --> 00:06:25.480
to understand the stability
of Scott's rhythms.

00:06:25.480 --> 00:06:28.080
And then based on a
curve of similarity,

00:06:28.080 --> 00:06:31.440
starting most similar
then going less similar.

00:06:31.440 --> 00:06:33.450
And basically a
human put a curve.

00:06:33.450 --> 00:06:35.780
But I can see a scenario
where Haile could come up

00:06:35.780 --> 00:06:38.140
with a curve by itself,
trying to start with something

00:06:38.140 --> 00:06:40.110
that Scott understands.

00:06:40.110 --> 00:06:42.160
Scott is playing seven
quarters, and then slowly

00:06:42.160 --> 00:06:43.190
introduce new ideas.

00:06:43.190 --> 00:06:45.920
And you can see how Scott
actually listens to the robot

00:06:45.920 --> 00:06:48.540
and at some point, building
on what the robot is doing.

00:06:48.540 --> 00:06:51.939
So Haile is building on what
Scott is doing, obviously,

00:06:51.939 --> 00:06:53.730
by looking at the
stability and similarity.

00:06:53.730 --> 00:06:56.900
But at some point, Scott
is almost inspired.

00:06:56.900 --> 00:06:58.580
Maybe inspired is
too big of a word,

00:06:58.580 --> 00:07:01.070
but that's a goal that Scott
will come up with an idea

00:07:01.070 --> 00:07:04.384
that he would come up with
if he played with humans.

00:07:04.384 --> 00:07:05.050
[VIDEO PLAYBACK]

00:07:05.050 --> 00:07:07.930
[DRUMMING]

00:07:18.860 --> 00:07:19.970
That's almost a new idea.

00:07:19.970 --> 00:07:20.970
Scott is building on it.

00:07:24.010 --> 00:07:27.770
[END PLAYBACK]

00:07:27.770 --> 00:07:34.002
And this is a darbuka
drum player concert.

00:07:34.002 --> 00:07:35.710
You will see how the
professional darbuka

00:07:35.710 --> 00:07:37.840
player, actually from
Israel, is kind of surprised.

00:07:37.840 --> 00:07:42.120
But I think his facial gestures
were interesting for me

00:07:42.120 --> 00:07:46.110
because I think he was
surprised for the better.

00:07:46.110 --> 00:07:51.170
And at some point you'll see
how all of us are playing

00:07:51.170 --> 00:07:52.730
and Haile tries to get to beat.

00:07:52.730 --> 00:07:54.540
So we combine the
stability similarity

00:07:54.540 --> 00:07:57.086
and beat detection
into a drum circle.

00:07:57.086 --> 00:07:58.082
[VIDEO PLAYBACK]

00:07:58.082 --> 00:08:00.080
[DRUMMING]

00:08:00.080 --> 00:08:01.217
This is call and response.

00:08:01.217 --> 00:08:02.550
Later it will be simultaneously.

00:08:02.550 --> 00:08:06.738
[DRUMMING]

00:08:21.980 --> 00:08:26.260
And now, what it does, it
listened to these two drummers,

00:08:26.260 --> 00:08:28.680
and tricked the pitch
from one drummer,

00:08:28.680 --> 00:08:31.580
and the rhythm from another.

00:08:31.580 --> 00:08:34.299
And the other arm,
most from pitch

00:08:34.299 --> 00:08:37.125
and the timbre of
the two players.

00:08:37.125 --> 00:08:38.010
[END PLAYBACK]

00:08:38.010 --> 00:08:39.969
So we played the rhythm
that one player played,

00:08:39.969 --> 00:08:42.551
and the pitch-- well, it's not
really pitch, it's in the drum,

00:08:42.551 --> 00:08:44.890
but this is lower and this
is higher next to the rim,

00:08:44.890 --> 00:08:46.490
and tried to create
something that is really

00:08:46.490 --> 00:08:47.615
morphing between these two.

00:08:47.615 --> 00:08:51.020
Again, things that humans cannot
do and maybe shouldn't do,

00:08:51.020 --> 00:08:55.610
but here something
interesting can come up.

00:08:55.610 --> 00:08:58.300
Another thing that I was
interested in is polyrhythm.

00:08:58.300 --> 00:09:02.280
It's very easy for a robot to
do things that humans cannot.

00:09:02.280 --> 00:09:04.010
Sometimes I'll ask
my student to clap.

00:09:04.010 --> 00:09:05.135
I will not ask you to clap.

00:09:05.135 --> 00:09:06.176
I'll give you an example.

00:09:06.176 --> 00:09:07.810
I think there is two
main reasons here.

00:09:07.810 --> 00:09:09.180
[SPEAKING RHYTHM]

00:09:13.480 --> 00:09:14.550
This is nine.

00:09:14.550 --> 00:09:15.750
[SPEAKING RHYTHM]

00:09:19.370 --> 00:09:22.620
I don't ask you to clap
but sometimes I would.

00:09:22.620 --> 00:09:27.690
It was [SPEAKING RHYTHM] seven.

00:09:27.690 --> 00:09:29.274
And then I asked
my students to do

00:09:29.274 --> 00:09:30.690
the nine in one
hand and the seven

00:09:30.690 --> 00:09:32.860
in another hand which I
will definitely not ask you.

00:09:32.860 --> 00:09:37.000
But see how Haile
here captured--

00:09:37.000 --> 00:09:38.392
decided to record the rhythm.

00:09:38.392 --> 00:09:39.350
So it records the nine.

00:09:39.350 --> 00:09:41.474
He choose the nine and the
seven, and at some point

00:09:41.474 --> 00:09:43.760
he introduced them in
polyrhythmic, interesting

00:09:43.760 --> 00:09:44.721
rhythms.

00:09:44.721 --> 00:09:45.703
[VIDEO PLAYBACK]

00:09:45.703 --> 00:09:48.649
[DRUMMING]

00:10:05.305 --> 00:10:07.660
And we add more and
more rhythms over it.

00:10:07.660 --> 00:10:10.070
[END PLAYBACK]

00:10:10.070 --> 00:10:13.330
And I don't know if know,
but Pat Metheny had a project

00:10:13.330 --> 00:10:14.320
that he used robots in.

00:10:14.320 --> 00:10:16.729
He came to our lab and
I explained it to him,

00:10:16.729 --> 00:10:17.520
I showed it to him.

00:10:17.520 --> 00:10:18.880
I said, this is something
no humans can do.

00:10:18.880 --> 00:10:19.880
And he said, sure, my
drummers can do it.

00:10:19.880 --> 00:10:22.670
And he can also do the four
with his leg, and another three

00:10:22.670 --> 00:10:23.490
with another leg.

00:10:23.490 --> 00:10:26.740
So I think everyone can do
it except maybe Pat Metheny's

00:10:26.740 --> 00:10:28.040
drummer.

00:10:28.040 --> 00:10:30.820
And here is something that's
at the end of concert,

00:10:30.820 --> 00:10:33.870
it's obviously, you
know, fishing for cheers.

00:10:33.870 --> 00:10:36.065
We just have a little
short MIDI file where

00:10:36.065 --> 00:10:37.260
we playing with it together.

00:10:37.260 --> 00:10:39.350
Unison always works,
so I'll just play this.

00:10:39.350 --> 00:10:40.600
We grabbed the nine and seven.

00:10:40.600 --> 00:10:41.343
[VIDEO PLAYBACK]

00:10:41.343 --> 00:10:44.181
[DRUMMING]

00:10:47.970 --> 00:10:51.020
And that's from a performance
in Odense in Denmark.

00:10:51.020 --> 00:10:52.810
They had a robot festival.

00:10:52.810 --> 00:10:55.720
[END PLAYBACK]

00:10:55.720 --> 00:10:59.066
So the next project was Shimon.

00:10:59.066 --> 00:11:00.690
And actually I have
an story about this

00:11:00.690 --> 00:11:04.920
because I put a video of
Haile and Scott playing,

00:11:04.920 --> 00:11:06.129
the first one, on my website.

00:11:06.129 --> 00:11:08.378
It was before YouTube, or
before I knew about YouTube.

00:11:08.378 --> 00:11:10.280
And someone grabbed it
and put it on YouTube,

00:11:10.280 --> 00:11:14.810
and then CNN saw it and they
asked to come and do a piece.

00:11:14.810 --> 00:11:17.140
And when they put a
piece, the next day

00:11:17.140 --> 00:11:20.350
I got an email from the
NSF, from the NSF director

00:11:20.350 --> 00:11:24.220
who said we saw your piece,
please submit a proposal

00:11:24.220 --> 00:11:26.205
and continue to develop that.

00:11:26.205 --> 00:11:26.830
Rarely happens.

00:11:26.830 --> 00:11:27.660
Never happened since.

00:11:27.660 --> 00:11:27.990
I tried.

00:11:27.990 --> 00:11:28.980
I put so many videos--

00:11:31.510 --> 00:11:34.250
And this is the next
robot that we came up

00:11:34.250 --> 00:11:36.180
which adds multiple things.

00:11:36.180 --> 00:11:39.210
The main thing that it adds
is the concept of pitch.

00:11:39.210 --> 00:11:40.920
It plays a marimba.

00:11:40.920 --> 00:11:43.110
And the second aspect
it adds-- we're

00:11:43.110 --> 00:11:45.420
talking about the personal
connection, gestures,

00:11:45.420 --> 00:11:48.219
visual cues-- is the head.

00:11:48.219 --> 00:11:49.760
And many people ask
me, why the head?

00:11:49.760 --> 00:11:50.880
It doesn't play any music.

00:11:50.880 --> 00:11:52.780
In a second I'll show
some of the utilization

00:11:52.780 --> 00:11:54.060
of what the head does.

00:11:54.060 --> 00:11:56.016
And of course it has a camera.

00:11:56.016 --> 00:11:57.390
That's the first
question you get

00:11:57.390 --> 00:11:59.460
about any head, robotic head.

00:11:59.460 --> 00:12:03.265
So the name is Shimon, and
here is the first example

00:12:03.265 --> 00:12:05.470
of my post-doc, Guy Hoffman.

00:12:05.470 --> 00:12:07.130
He's now a professor in Israel.

00:12:07.130 --> 00:12:08.090
[VIDEO PLAYBACK]

00:12:08.090 --> 00:12:10.490
[PLAYING PIANO]

00:12:11.470 --> 00:12:16.290
It shows how a headless
Shimon responds

00:12:16.290 --> 00:12:20.345
to dynamics, to expression,
and to improvise.

00:12:31.191 --> 00:12:32.911
Look at Guy, how
he looks at him as

00:12:32.911 --> 00:12:34.160
if his kid did something nice.

00:12:34.160 --> 00:12:34.743
[END PLAYBACK]

00:12:34.743 --> 00:12:36.440
[LAUGHTER]

00:12:36.440 --> 00:12:39.830
So when Guy played fast,
the robot responded.

00:12:39.830 --> 00:12:41.230
Simple, just tempo detection.

00:12:41.230 --> 00:12:43.200
And then loud and
louder, and tried

00:12:43.200 --> 00:12:45.280
to get some
understanding of tonality

00:12:45.280 --> 00:12:47.170
and of giving the right codes.

00:12:47.170 --> 00:12:50.250
And we had some
studies done about it.

00:12:50.250 --> 00:12:54.660
If you remember the first
code was [SPEAKING CODE]

00:12:54.660 --> 00:12:55.800
Three codes.

00:12:55.800 --> 00:12:58.520
And we tried to see if,
by looking at the robot,

00:12:58.520 --> 00:13:03.590
at the gestures, with
the arms before the head,

00:13:03.590 --> 00:13:06.350
if the synchronization between
the human and the robot

00:13:06.350 --> 00:13:07.640
is better.

00:13:07.640 --> 00:13:12.500
And you see that the first
code is maybe easier.

00:13:12.500 --> 00:13:15.130
The second code, very
difficult to synchronize

00:13:15.130 --> 00:13:16.240
in terms of milliseconds.

00:13:16.240 --> 00:13:18.790
The third code, you
kind of adjust yourself.

00:13:18.790 --> 00:13:21.030
So it's a little better.

00:13:21.030 --> 00:13:24.870
And we have multiple conditions,
one only visual, one audio,

00:13:24.870 --> 00:13:26.310
and one with synthesized sound.

00:13:26.310 --> 00:13:31.065
And you can see that
the visual is better

00:13:31.065 --> 00:13:32.690
because it's less
milliseconds of delay

00:13:32.690 --> 00:13:35.750
between the human and the robot
in both of the conditions,

00:13:35.750 --> 00:13:36.710
precise and imprecise.

00:13:36.710 --> 00:13:39.290
Especially in imprecise,
you'll see it's better.

00:13:39.290 --> 00:13:42.570
For imprecise, the robot
could play 50 milliseconds,

00:13:42.570 --> 00:13:43.780
before or after.

00:13:43.780 --> 00:13:47.920
And just looking
at the robot helped

00:13:47.920 --> 00:13:50.410
it become a much
more synchronized,

00:13:50.410 --> 00:13:52.190
tight performance.

00:13:52.190 --> 00:13:55.810
And it works much better at
slower tempos, less than 100

00:13:55.810 --> 00:13:57.980
BPM as in more than that.

00:14:00.570 --> 00:14:03.020
Then we added some
other aspects.

00:14:03.020 --> 00:14:05.670
We said, OK, we have a head,
let's do what the head can do.

00:14:05.670 --> 00:14:06.730
Actually we didn't
think of this way.

00:14:06.730 --> 00:14:09.188
We thought of what the head
can do and then built the head,

00:14:09.188 --> 00:14:11.827
but for the story.

00:14:11.827 --> 00:14:13.660
And what it can do is
multi-modal detection.

00:14:13.660 --> 00:14:15.640
We had a camera
there and we tried

00:14:15.640 --> 00:14:19.670
to detect the beats
of the hip hop artist

00:14:19.670 --> 00:14:22.960
to coordinate with the beat
detection of the music.

00:14:22.960 --> 00:14:25.140
And you see how the
multi-modal detection work.

00:14:25.140 --> 00:14:28.330
And also just to help
them feel together--

00:14:28.330 --> 00:14:32.030
and this is Guy Hoffman's
work-- and develop gestures

00:14:32.030 --> 00:14:34.410
that will make everyone
feel that they are

00:14:34.410 --> 00:14:35.928
synchronized with each other.

00:14:35.928 --> 00:14:36.884
[VIDEO PLAYBACK]

00:14:36.884 --> 00:14:40.230
[MUSIC PLAYING]

00:14:40.230 --> 00:14:42.396
So that's just the audio.

00:14:42.396 --> 00:14:48.770
[MUSIC PLAYING]

00:14:48.770 --> 00:14:55.050
Now double-checking with
the beat of the head.

00:14:55.050 --> 00:14:57.456
And of course
blinking never hurts.

00:14:57.456 --> 00:15:00.357
The blinking is random.

00:15:00.357 --> 00:15:00.940
[END PLAYBACK]

00:15:00.940 --> 00:15:03.600
All right.

00:15:03.600 --> 00:15:05.380
OK.

00:15:05.380 --> 00:15:07.990
So that was a little bit
about listening like a human

00:15:07.990 --> 00:15:10.240
and maybe grooving like a human.

00:15:10.240 --> 00:15:15.700
And then I started to think more
about creativity, or machine

00:15:15.700 --> 00:15:17.880
creativity and what it means.

00:15:17.880 --> 00:15:21.390
And actually, this is the
first time I've showed it.

00:15:21.390 --> 00:15:23.630
I hope it's not streamed,
I think it's not streamed.

00:15:23.630 --> 00:15:25.850
This is a TED Ed video
that I just finished doing.

00:15:25.850 --> 00:15:28.020
I didn't put in online
yet, so it's a part of it.

00:15:28.020 --> 00:15:29.190
I didn't show it yet.

00:15:29.190 --> 00:15:32.740
But it shows one
of the ideas I had

00:15:32.740 --> 00:15:37.050
for creating machine creativity
using genetic algorithms.

00:15:37.050 --> 00:15:41.330
And the idea was that if have
a population of motifs, maybe

00:15:41.330 --> 00:15:45.760
random motifs, and then
I have a set of mutations

00:15:45.760 --> 00:15:48.030
and cross-breeding
between these motifs,

00:15:48.030 --> 00:15:52.180
and have a fitness function
that is based on human's melody,

00:15:52.180 --> 00:15:56.840
then I can create
new, interesting music

00:15:56.840 --> 00:15:59.260
that has some aesthetics
for humans in them.

00:15:59.260 --> 00:16:02.610
Because it's easy
to create melodies

00:16:02.610 --> 00:16:05.050
that were never created before.

00:16:05.050 --> 00:16:07.310
Most of them will
not be listenable.

00:16:07.310 --> 00:16:10.930
The question is, how do you
make meaningful and valuable

00:16:10.930 --> 00:16:11.492
melodies?

00:16:11.492 --> 00:16:13.450
And in the video, you'll
see a little reference

00:16:13.450 --> 00:16:15.560
to the Lovelace Test,
which probably some of you

00:16:15.560 --> 00:16:18.143
know about maybe I'll just say
that the Lovelace Test is based

00:16:18.143 --> 00:16:19.860
on Ad Lovelace, the
first programmer,

00:16:19.860 --> 00:16:22.530
or is considered to be
the first programmer,

00:16:22.530 --> 00:16:25.400
trying to-- instead of
intelligence like the Turing

00:16:25.400 --> 00:16:29.180
test, try to determine
when the robot is creative.

00:16:29.180 --> 00:16:32.310
And she said something like,
a robot will never be creative

00:16:32.310 --> 00:16:36.430
before it generates ideas that
its own programmers cannot

00:16:36.430 --> 00:16:38.420
understand how they came to be.

00:16:38.420 --> 00:16:41.960
And then in 2001 there
was a group of scientists,

00:16:41.960 --> 00:16:44.060
some of them continued to
develop [INAUDIBLE] that

00:16:44.060 --> 00:16:46.100
came up with this Lovelace
test, which is mostly

00:16:46.100 --> 00:16:50.140
a theoretical thought
experiment of creating something

00:16:50.140 --> 00:16:50.640
like that.

00:16:50.640 --> 00:16:52.750
But even if we do
something like that,

00:16:52.750 --> 00:16:55.200
I didn't think that the music
that would come be valuable.

00:16:55.200 --> 00:16:57.540
And I'll show this
little TED Ed clip

00:16:57.540 --> 00:16:58.864
and see how I went about it.

00:16:58.864 --> 00:16:59.530
[VIDEO PLAYBACK]

00:16:59.530 --> 00:17:03.290
-So how can evolution make a
machine musically creative?

00:17:03.290 --> 00:17:05.670
Well, instead of
organisms, can start

00:17:05.670 --> 00:17:08.630
with an initial population
of musical phrases.

00:17:08.630 --> 00:17:13.588
[MUSIC PLAYING]

00:17:13.588 --> 00:17:15.129
GIL WEINBERG: They
put the drum beat.

00:17:15.129 --> 00:17:16.829
It's not mine.

00:17:16.829 --> 00:17:19.650
-And a basic algorithm
that mimics reproduction

00:17:19.650 --> 00:17:23.240
and random mutations by
switching some parts,

00:17:23.240 --> 00:17:27.420
combining others, and
replacing random notes.

00:17:27.420 --> 00:17:30.310
Now that we have a new
generation of phrases,

00:17:30.310 --> 00:17:33.970
we can apply selection using
an operation called a fitness

00:17:33.970 --> 00:17:35.180
function.

00:17:35.180 --> 00:17:37.380
Just as biological
fitness is determined

00:17:37.380 --> 00:17:39.760
by external,
environmental pressures,

00:17:39.760 --> 00:17:42.130
our fitness function
can be determined

00:17:42.130 --> 00:17:46.840
by an external melody, chosen
by human musicians or music fans

00:17:46.840 --> 00:17:49.667
to represent the ultimate
beautiful melody.

00:17:49.667 --> 00:17:53.159
[MUSIC PLAYING]

00:17:53.659 --> 00:17:54.657
GIL WEINBERG: Chopin.

00:17:54.657 --> 00:17:58.460
-The algorithm can then compare
between our musical phrases

00:17:58.460 --> 00:18:01.780
and that beautiful melody,
and select only the phrases

00:18:01.780 --> 00:18:04.090
that are most similar to it.

00:18:04.090 --> 00:18:07.180
Once the least similar
sequences are weeded out,

00:18:07.180 --> 00:18:10.550
the algorithm can re-apply
mutation and recombination

00:18:10.550 --> 00:18:14.790
to what's left, select the most
similar or fitted ones, again

00:18:14.790 --> 00:18:18.340
from the new generation and
repeat for many generations.

00:18:18.340 --> 00:18:21.623
[MUSIC PLAYING]

00:18:23.500 --> 00:18:26.480
The process that got us
there has so much randomness

00:18:26.480 --> 00:18:29.130
and complexity built
in, that the result

00:18:29.130 --> 00:18:31.420
might pass the Lovelace Test.

00:18:31.420 --> 00:18:34.640
More importantly, thanks to
the presence of human aesthetic

00:18:34.640 --> 00:18:36.850
in the process,
we'll theoretically

00:18:36.850 --> 00:18:41.100
generate melodies we
would consider beautiful.

00:18:41.100 --> 00:18:43.132
GIL WEINBERG: All right.

00:18:43.132 --> 00:18:44.590
[END PLAYBACK] So
here's an example

00:18:44.590 --> 00:18:47.100
of how this is
used in real-time.

00:18:47.100 --> 00:18:49.684
Chopin is not [INAUDIBLE]
with fitness function

00:18:49.684 --> 00:18:50.350
as you see here.

00:18:50.350 --> 00:18:52.700
But actually, the
saxophone player and myself

00:18:52.700 --> 00:18:57.310
playing piano-- I programmed
Haile with multiple motifs

00:18:57.310 --> 00:19:02.200
and every time it listened
to one of our musical motifs,

00:19:02.200 --> 00:19:04.310
it used dynamic
time warping to see

00:19:04.310 --> 00:19:07.530
what's most similar and
come up with a variation.

00:19:07.530 --> 00:19:09.900
And it stopped after
something like 50 generations

00:19:09.900 --> 00:19:11.870
because, of course,
if it ran more than 50

00:19:11.870 --> 00:19:13.550
it would become
exactly what we played.

00:19:13.550 --> 00:19:15.508
So we just stopped one
time to create something

00:19:15.508 --> 00:19:17.890
that is similar to what we
did, but a little different

00:19:17.890 --> 00:19:21.400
and based on the, let's call it,
original DNA of the population

00:19:21.400 --> 00:19:22.600
that we started with.

00:19:22.600 --> 00:19:26.100
And [INAUDIBLE] Haile will
never use it with Shimon.

00:19:26.100 --> 00:19:29.550
This is Haile when we first
tried to have Haile with pitch.

00:19:29.550 --> 00:19:31.799
So the whole idea of
creating rich, acoustic sound

00:19:31.799 --> 00:19:32.340
is not there.

00:19:32.340 --> 00:19:34.930
We put the little toy
vibraphone below Haile.

00:19:34.930 --> 00:19:37.854
So excuse the
quality of the sound.

00:19:37.854 --> 00:19:38.520
[VIDEO PLAYBACK]

00:19:38.520 --> 00:19:42.020
[MUSIC PLAYING]

00:19:52.020 --> 00:19:54.090
And listen to how
all three of us

00:19:54.090 --> 00:19:56.510
are building on
each other's ideas.

00:19:56.510 --> 00:19:59.550
Saxophone, piano, and robot.

00:19:59.550 --> 00:20:02.910
[MUSIC PLAYING]

00:20:06.647 --> 00:20:07.230
[END PLAYBACK]

00:20:07.230 --> 00:20:07.720
Yeah.

00:20:07.720 --> 00:20:10.136
It's difficult to evaluate it
because of quality of sound.

00:20:10.136 --> 00:20:12.090
But that was the idea
of trying to put it

00:20:12.090 --> 00:20:14.470
in a real-time scenario.

00:20:14.470 --> 00:20:16.695
Another idea where
I tried to have

00:20:16.695 --> 00:20:20.440
it play more like a
machine was style morphing.

00:20:20.440 --> 00:20:23.640
And as you can understand by
now, I play jazz, I like jazz,

00:20:23.640 --> 00:20:27.840
I like Monk on the left
and Coltrane on the right.

00:20:27.840 --> 00:20:30.360
And I thought,
wouldn't it be awesome

00:20:30.360 --> 00:20:33.630
if we tried to morph
between Coltrane and Monk

00:20:33.630 --> 00:20:35.910
and create something that
they themselves will never

00:20:35.910 --> 00:20:36.750
be able to create.

00:20:36.750 --> 00:20:38.207
They cannot morph
into each other.

00:20:38.207 --> 00:20:40.040
And maybe something
interesting will come up

00:20:40.040 --> 00:20:43.160
that is just a combination
of both their styles.

00:20:43.160 --> 00:20:46.730
And we used HMMs here,
basically analyzing

00:20:46.730 --> 00:20:48.772
a lot of improvisation by
both of these musicians

00:20:48.772 --> 00:20:51.188
and trying to see what the
likelihood is that at any given

00:20:51.188 --> 00:20:53.970
note, what the next note would
be, what the next rhythm would

00:20:53.970 --> 00:20:56.690
be, just a second order.

00:20:56.690 --> 00:21:00.380
And then with an iPhone I
can play a little melody.

00:21:00.380 --> 00:21:01.120
[VIDEO PLAYBACK]

00:21:01.120 --> 00:21:04.080
And this is a little app
that we built regardless.

00:21:04.080 --> 00:21:06.350
And at some point when
the melodies that I play,

00:21:06.350 --> 00:21:08.750
of course it can be
piano too, is ready,

00:21:08.750 --> 00:21:11.090
I will send it to
Shimon, and then I

00:21:11.090 --> 00:21:15.320
will be able to morph
between my own melody, Monk's

00:21:15.320 --> 00:21:19.330
style, Coltrane's style and
with two styles with saw

00:21:19.330 --> 00:21:24.018
a different kind of
balances between these two

00:21:24.018 --> 00:21:26.491
improvisations and myself.

00:21:26.491 --> 00:21:30.458
[MUSIC PLAYING]

00:21:40.430 --> 00:21:42.710
If you know Monk's
syncopation in clusters.

00:21:45.852 --> 00:21:48.395
If you know Coltrane, you
know the crazy, fast licks.

00:21:53.510 --> 00:21:57.295
And this is
somewhere in between.

00:21:57.295 --> 00:21:59.519
And of course, one doesn't
play saxophone over piano,

00:21:59.519 --> 00:22:01.060
so it doesn't sound
like any of them.

00:22:01.060 --> 00:22:01.643
[END PLAYBACK]

00:22:01.643 --> 00:22:06.080
But at least the notes are
based on the improvisation.

00:22:06.080 --> 00:22:09.580
Some other quick examples
of things that we tried.

00:22:09.580 --> 00:22:11.466
Here we had the
connect because we

00:22:11.466 --> 00:22:12.840
wanted to get more
than just what

00:22:12.840 --> 00:22:14.940
this simple camera
in the head had.

00:22:14.940 --> 00:22:18.790
And we tried to have
motion-based improvisation.

00:22:18.790 --> 00:22:22.380
You see the Atlanta symphony
percussionist, Thomas Sherwood,

00:22:22.380 --> 00:22:24.910
doing some tai chi-like
movement and trying

00:22:24.910 --> 00:22:27.010
to have Shimon improvise.

00:22:27.010 --> 00:22:31.070
And actually Tom is playing
an electronic marimba

00:22:31.070 --> 00:22:33.845
and Shimon has a system
in which sometimes it will

00:22:33.845 --> 00:22:34.970
repeat what is listened to.

00:22:34.970 --> 00:22:37.280
Sometimes it's
[INAUDIBLE], sometimes

00:22:37.280 --> 00:22:39.650
it will introduce new ideas,
a stochastic system that's

00:22:39.650 --> 00:22:41.890
supposed to inspire
Tom to play differently

00:22:41.890 --> 00:22:43.200
than he would otherwise.

00:22:43.200 --> 00:22:45.412
[VIDEO PLAYBACK] So
that's the tai chi part.

00:22:45.412 --> 00:22:49.348
[MUSIC PLAYING]

00:23:08.580 --> 00:23:14.500
And it wasn't easy for it to
repeat, exactly this pattern.

00:23:14.500 --> 00:23:17.630
It's very limited with the four
arms and where they can be.

00:23:17.630 --> 00:23:20.740
I'll show some path
planning strategies later.

00:23:20.740 --> 00:23:22.840
So not every motif was
captured perfectly.

00:23:31.040 --> 00:23:33.021
And now it starts to
introduce new motifs.

00:23:33.021 --> 00:23:34.687
It will repeat and
then create some kind

00:23:34.687 --> 00:23:35.728
of a musical interaction.

00:23:35.728 --> 00:23:37.200
[END PLAYBACK]

00:23:37.200 --> 00:23:39.900
This is my student, Ryan
Nikolaidis, the entire motif.

00:23:39.900 --> 00:23:42.050
[VIDEO PLAYBACK]

00:23:42.050 --> 00:23:44.976
And Shimon is
developing this motif.

00:23:44.976 --> 00:23:49.170
This is part of the US
Science and Technology Fair

00:23:49.170 --> 00:23:51.122
in Washington.

00:23:51.122 --> 00:23:54.050
[MUSIC PLAYING]

00:24:01.297 --> 00:24:01.880
[END PLAYBACK]

00:24:01.880 --> 00:24:05.249
And for something else,
some of them just, you know,

00:24:05.249 --> 00:24:06.040
they love dubsteps.

00:24:06.040 --> 00:24:07.920
And the machine learning
maybe interesting or not,

00:24:07.920 --> 00:24:08.900
but dubsteps are there.

00:24:08.900 --> 00:24:11.785
So it's just a dubstep piece
that one of our students

00:24:11.785 --> 00:24:12.660
developed for Shimon.

00:24:12.660 --> 00:24:13.904
[VIDEO PLAYBACK]

00:24:13.904 --> 00:24:16.886
[MUSIC PLAYING]

00:24:36.269 --> 00:24:40.742
So it can do all kinds of stuff.

00:24:40.742 --> 00:24:42.280
[END PLAYBACK]

00:24:42.280 --> 00:24:45.950
Here we used vision-- and
you see this green ball

00:24:45.950 --> 00:24:48.110
on the conductor's
baton-- and tried

00:24:48.110 --> 00:24:52.280
to combine the
input from conductor

00:24:52.280 --> 00:24:55.270
and from the audio analysis
from the orchestra to improvise,

00:24:55.270 --> 00:24:57.716
based on the chords that
the orchestra is playing.

00:24:57.716 --> 00:25:02.280
[VIDEO PLAYBACK] That's the
Georgia Tech student orchestra.

00:25:02.280 --> 00:25:06.115
[MUSIC PLAYING]

00:25:08.152 --> 00:25:08.735
[END PLAYBACK]

00:25:08.735 --> 00:25:10.570
All right.

00:25:10.570 --> 00:25:13.510
And I think this is
my favorite clip.

00:25:13.510 --> 00:25:16.030
This actually was a PSA of
Georgia Tech for three or four

00:25:16.030 --> 00:25:17.020
years.

00:25:17.020 --> 00:25:18.670
You know that we
come from Georgia

00:25:18.670 --> 00:25:23.540
where dueling banjos are a big
thing, at least in some movies.

00:25:23.540 --> 00:25:28.070
So we tried to do dueling
marine buzz if you like,

00:25:28.070 --> 00:25:32.880
and present kind of the
dueling aspect and Shimon

00:25:32.880 --> 00:25:34.520
with the gestures--
and this is again,

00:25:34.520 --> 00:25:38.650
Guy Hoffman's work--
project some personality.

00:25:38.650 --> 00:25:41.396
Hopefully better than the movie.

00:25:41.396 --> 00:25:42.062
[VIDEO PLAYBACK]

00:25:42.062 --> 00:25:44.888
[CALL AND RESPONSE MUSIC
 PLAYING]

00:26:03.462 --> 00:26:04.959
[END PLAYBACK]

00:26:09.466 --> 00:26:11.090
I think I have two
more clips before we

00:26:11.090 --> 00:26:14.820
go to the next project.

00:26:14.820 --> 00:26:17.330
I just got the request, how
about having a robot band?

00:26:17.330 --> 00:26:20.160
They listen to each other
and create something.

00:26:20.160 --> 00:26:23.250
I'm sure robotics people
here are being asked about,

00:26:23.250 --> 00:26:25.950
will robots replace
humans often?

00:26:25.950 --> 00:26:28.090
And in my case it's
really painful sometimes

00:26:28.090 --> 00:26:32.580
because they come to me
and tell me, how dare you?

00:26:32.580 --> 00:26:34.430
Music, really?

00:26:34.430 --> 00:26:36.760
Like, OK cars, OK
cleaning carpets,

00:26:36.760 --> 00:26:38.710
but you're going to
take human from us

00:26:38.710 --> 00:26:40.270
and let the robots play it.

00:26:40.270 --> 00:26:44.509
So I never tried, I'm not
interested in having robots

00:26:44.509 --> 00:26:45.800
playing together by themselves.

00:26:45.800 --> 00:26:49.650
It's only and always have
been about inspiring human

00:26:49.650 --> 00:26:51.890
and creating something
interesting for humans.

00:26:51.890 --> 00:26:55.680
And here you can
see a quick clip

00:26:55.680 --> 00:26:58.780
where you see bit detection
and [INAUDIBLE] improvisation

00:26:58.780 --> 00:27:00.744
between two students
and two robots.

00:27:00.744 --> 00:27:01.410
[VIDEO PLAYBACK]

00:27:01.410 --> 00:27:04.439
[MUSIC PLAYING]

00:27:04.439 --> 00:27:06.230
He's not so good with
the beat, by the way,

00:27:06.230 --> 00:27:08.985
so-- strictly for
Haile to detect it.

00:27:11.920 --> 00:27:13.100
But it got it.

00:27:13.100 --> 00:27:15.225
And now it start to
improvise with the other arm.

00:27:15.225 --> 00:27:17.100
What you hear here is
a student that we don't

00:27:17.100 --> 00:27:18.550
see, playing the synthesizer.

00:27:18.550 --> 00:27:23.975
[MUSIC PLAYING]

00:27:23.975 --> 00:27:27.030
And now Shimon,
before we had a head,

00:27:27.030 --> 00:27:33.190
actually we used
someone else's CGI head.

00:27:33.190 --> 00:27:35.396
Andrea Tomaz from Georgia Tech.

00:27:38.402 --> 00:27:40.110
I'm dropping back and
forth in time here.

00:27:45.493 --> 00:27:46.076
[END PLAYBACK]

00:27:46.076 --> 00:27:47.070
All right.

00:27:47.070 --> 00:27:49.310
So that's a robot-robot
interaction for you.

00:27:52.760 --> 00:27:55.340
I promised to talk a little
bit about path playing.

00:27:57.880 --> 00:28:01.260
There are multiple ways
in which these forums

00:28:01.260 --> 00:28:07.030
can be next to the piano
or next to the marimba.

00:28:07.030 --> 00:28:08.990
In this case I would
just put a piano.

00:28:08.990 --> 00:28:13.880
And we have state machines
that represent each one

00:28:13.880 --> 00:28:16.170
off these states.

00:28:16.170 --> 00:28:20.980
Given any melody, what would
be the most reasonable path

00:28:20.980 --> 00:28:22.150
to take?

00:28:22.150 --> 00:28:24.512
And the inputs that
we have is a curve

00:28:24.512 --> 00:28:27.640
that humans play for
tension-- melodic tension

00:28:27.640 --> 00:28:30.440
or harmonic tension,
or rhythmic stability.

00:28:30.440 --> 00:28:32.570
And I'll show it here.

00:28:32.570 --> 00:28:36.790
This is some the
pitch or note density.

00:28:36.790 --> 00:28:40.390
And using the Viterbi algorithm,
actually [INAUDIBLE] Viterbi,

00:28:40.390 --> 00:28:43.320
we're trying reduce
the space and try

00:28:43.320 --> 00:28:48.690
to come up a reasonable path
to create improvisation,

00:28:48.690 --> 00:28:50.780
following these curves.

00:28:50.780 --> 00:28:54.210
So it's basically
developing, as I say,

00:28:54.210 --> 00:28:58.210
an embodied musical mind,
where not only of the music

00:28:58.210 --> 00:29:00.180
it has to play but
also it's own body.

00:29:00.180 --> 00:29:01.930
The whole idea of
embodiment is a bit part

00:29:01.930 --> 00:29:05.680
that robot musicians have
and that software musicians

00:29:05.680 --> 00:29:08.030
or machine musicians don't have.

00:29:08.030 --> 00:29:09.404
Only robotic musicianship has.

00:29:09.404 --> 00:29:12.070
So we are trying to come up with
these algorithms, and actually,

00:29:12.070 --> 00:29:16.100
my student, Mason Bretan
programmed really extensive

00:29:16.100 --> 00:29:18.410
jazz theory into Shimon.

00:29:18.410 --> 00:29:21.600
So now it can listen to
chords that Mason, in a second

00:29:21.600 --> 00:29:24.500
will play, and then
try to improvise,

00:29:24.500 --> 00:29:31.130
using this both music
and physical limitations

00:29:31.130 --> 00:29:33.770
that he has using Viterbi.

00:29:33.770 --> 00:29:37.580
He tried some other, but the
Viterbi worked pretty well

00:29:37.580 --> 00:29:38.880
and this is his improvisation.

00:29:38.880 --> 00:29:41.590
This clip actually went
viral kind of for our fill.

00:29:41.590 --> 00:29:44.816
There were around 50k
viewers last month.

00:29:44.816 --> 00:29:45.482
[VIDEO PLAYBACK]

00:29:45.482 --> 00:29:49.740
[MUSIC PLAYING]

00:30:01.490 --> 00:30:03.534
Mason fed all the
chords offline.

00:30:03.534 --> 00:30:05.450
All the calculation of
what's going to be play

00:30:05.450 --> 00:30:06.158
was done offline.

00:30:06.158 --> 00:30:14.960
[MUSIC PLAYING]

00:30:17.847 --> 00:30:18.430
[END PLAYBACK]

00:30:18.430 --> 00:30:20.410
So I don't know if it's
got [INAUDIBLE] quality

00:30:20.410 --> 00:30:23.540
but it's not bad.

00:30:23.540 --> 00:30:24.730
I was happy.

00:30:24.730 --> 00:30:28.690
All right, so we all think
it's very important, the path

00:30:28.690 --> 00:30:31.120
planning and
creativity and so on.

00:30:31.120 --> 00:30:34.730
But it's important to
realize that this can

00:30:34.730 --> 00:30:36.630
be taken in a different light.

00:30:36.630 --> 00:30:38.990
And I think if you're being
made fun of for what you do,

00:30:38.990 --> 00:30:42.820
this guy is probably the
best guy to make fun of you.

00:30:42.820 --> 00:30:44.390
Some people tell
me, don't show it,

00:30:44.390 --> 00:30:48.104
but I think-- I'm proud of being
made fun of by Stephen Colbert.

00:30:48.104 --> 00:30:48.910
[VIDEO PLAYBACK]

00:30:48.910 --> 00:30:52.800
-Jazz robots, which
combines two of the biggest

00:30:52.800 --> 00:30:57.340
threats to our nation--
jazz and robots.

00:30:57.340 --> 00:30:59.720
He's been programmed
to dig it too.

00:30:59.720 --> 00:31:00.524
[LAUGHTER]

00:31:00.524 --> 00:31:03.180
Bobbing his head to the
beat and checking in

00:31:03.180 --> 00:31:05.240
with this fellow players.

00:31:05.240 --> 00:31:08.760
Meaning that long after
the robot uprising kills

00:31:08.760 --> 00:31:11.030
all humans on earth,
there will still

00:31:11.030 --> 00:31:14.876
be someone on the planet
pretending to enjoy jazz.

00:31:14.876 --> 00:31:16.212
[LAUGHTER]

00:31:16.212 --> 00:31:17.174
[END PLAYBACK]

00:31:21.510 --> 00:31:23.050
OK.

00:31:23.050 --> 00:31:27.660
Shimi was an effort to
take Shimon and make

00:31:27.660 --> 00:31:28.830
it a little smaller.

00:31:28.830 --> 00:31:31.200
Actually it almost
became a product.

00:31:31.200 --> 00:31:34.680
There was a failed
Kickstarter there.

00:31:34.680 --> 00:31:38.304
And it focused on the
gestures, not so much

00:31:38.304 --> 00:31:39.970
from the acoustic
reproduction of sound,

00:31:39.970 --> 00:31:42.595
which is part of the reason that
I went into robotics because I

00:31:42.595 --> 00:31:46.810
kind of grew tired by sound
from speakers, no matter

00:31:46.810 --> 00:31:51.310
how great your hifi system is.

00:31:51.310 --> 00:31:53.190
I was missing the
acoustic sound which

00:31:53.190 --> 00:31:55.390
is one of the reasons
I went to robotics.

00:31:55.390 --> 00:31:57.010
So here we don't
have acoustic sound,

00:31:57.010 --> 00:31:59.060
but we have a bunch
of other applications.

00:31:59.060 --> 00:32:01.950
We call it a robot companion
that can understand music.

00:32:01.950 --> 00:32:03.590
It can connect to your Spotify.

00:32:03.590 --> 00:32:05.820
It can dance, it can
do the bit detection.

00:32:05.820 --> 00:32:10.630
It has a set of algorithms
to get data from the audio

00:32:10.630 --> 00:32:11.910
and respond accordingly.

00:32:11.910 --> 00:32:14.948
And here's a clip.

00:32:14.948 --> 00:32:16.896
[VIDEO PLAYBACK]

00:32:23.714 --> 00:32:26.160
[MUSIC COLDPLAY, "VIVA LA VIDA"]

00:32:26.160 --> 00:32:28.790
GIL WEINBERG: In this case it
takes the music from the phone,

00:32:28.790 --> 00:32:31.955
but we also have an application
that takes it from Spotify.

00:32:31.955 --> 00:32:35.140
And the camera and the phone
can see if you don't like it.

00:32:35.140 --> 00:32:38.778
And better to say no,
because the music is loud.

00:32:38.778 --> 00:32:41.517
[TAPPING "DROP IT LIKE IT'S
 HOT"]

00:32:41.517 --> 00:32:45.990
[MUSIC SNOOP DOGG, "DROP IT LIKE
 IT'S HOT"]

00:32:52.785 --> 00:32:54.410
GIL WEINBERG:
[INAUDIBLE] speaker dock.

00:32:54.410 --> 00:32:56.718
You can see the
dancing speaker dock.

00:32:56.718 --> 00:33:01.160
[MUSIC SNOOP DOGG, "DROP IT LIKE
 IT'S HOT"]

00:33:03.220 --> 00:33:06.349
[END PLAYBACK]

00:33:07.540 --> 00:33:10.360
Here is a couple of other
applications, some using

00:33:10.360 --> 00:33:17.395
speech, and some
using audio analysis.

00:33:20.660 --> 00:33:22.120
That stuff is audio analysis.

00:33:22.120 --> 00:33:24.050
Looking for a beat in
what Mason is playing.

00:33:24.050 --> 00:33:24.675
[VIDEO PLAYING]

00:33:24.675 --> 00:33:27.938
[PLAYING GUITAR]

00:33:33.424 --> 00:33:35.340
GIL WEINBERG: No, there
wasn't anything there.

00:33:35.340 --> 00:33:39.228
[PLAYING GUITAR]

00:34:01.460 --> 00:34:02.420
You get it.

00:34:02.420 --> 00:34:03.940
It has a system of gestures.

00:34:06.850 --> 00:34:08.310
[END PLAYBACK]

00:34:08.310 --> 00:34:13.409
GIL WEINBERG: And here,
using Google Voice.

00:34:13.409 --> 00:34:14.800
Mason is connecting to Spotify.

00:34:14.800 --> 00:34:15.466
[VIDEO PLAYBACK]

00:34:15.466 --> 00:34:18.219
-Look at me.

00:34:18.219 --> 00:34:19.530
Can you play something happy?

00:34:21.604 --> 00:34:23.270
GIL WIENBERG: And we
have some algorithm

00:34:23.270 --> 00:34:27.336
to determine happy, disco, rock.

00:34:27.336 --> 00:34:30.324
[HAPPY MUSIC PLAYING]

00:34:45.534 --> 00:34:46.939
-Can you play disco?

00:34:53.756 --> 00:34:55.422
[MUSIC BEE GEES, "SATURDAY NIGHT
 LIVE"]

00:34:55.422 --> 00:34:57.430
GIL WIENBERG: Of course this.

00:34:57.430 --> 00:34:59.550
So a different set gestures
for different genres.

00:35:02.882 --> 00:35:03.856
All right.

00:35:03.856 --> 00:35:05.720
[END PLAYBACK]

00:35:05.720 --> 00:35:10.190
Annie Zhang, another student,
worked on the Shimi band,

00:35:10.190 --> 00:35:14.090
trying to create collaborations
between three Shimis.

00:35:14.090 --> 00:35:17.840
And she looked at a lot
of [INAUDIBLE], music

00:35:17.840 --> 00:35:20.256
[INAUDIBLE] elements you
would get from the audio.

00:35:20.256 --> 00:35:22.505
Anything for the beat detection
to the beat to the arm

00:35:22.505 --> 00:35:24.480
is for the energy.

00:35:24.480 --> 00:35:29.480
MFCC for vocalist to see if
there's vocals there or melody.

00:35:29.480 --> 00:35:31.910
And then this is her interface.

00:35:31.910 --> 00:35:36.072
And she then came up from
the low level features,

00:35:36.072 --> 00:35:37.530
detecting some high
level features,

00:35:37.530 --> 00:35:39.470
beats, energetics,
melodics, vocalness,

00:35:39.470 --> 00:35:41.350
percussiveness of the sound.

00:35:41.350 --> 00:35:45.700
And then she actually went
and studied [INAUDIBLE] system

00:35:45.700 --> 00:35:48.770
and tried to create
a dancing grammar

00:35:48.770 --> 00:35:50.750
for all of these gestures
that will connect

00:35:50.750 --> 00:35:54.310
between how Shimi is dancing.

00:35:54.310 --> 00:35:58.450
And here are some of
her ideas for that.

00:35:58.450 --> 00:36:02.020
And a whole system of different
ways in which three Shimis can

00:36:02.020 --> 00:36:07.240
dance together-- solo,
having the wave, two

00:36:07.240 --> 00:36:12.800
in synchronicity from both
sides, and the main one

00:36:12.800 --> 00:36:13.410
is different.

00:36:13.410 --> 00:36:17.735
So a full system that later
allowed her to have this.

00:36:17.735 --> 00:36:18.401
[VIDEO PLAYBACK]

00:36:18.401 --> 00:36:21.720
[MUSIC PLAYING]

00:36:21.720 --> 00:36:24.103
And every time it moved
different, the gestures.

00:36:28.913 --> 00:36:29.875
[END PLAYBACK]

00:36:29.875 --> 00:36:33.512
And even in a very noisy
environment in a demo--

00:36:33.512 --> 00:36:34.178
[VIDEO PLAYBACK]

00:36:34.178 --> 00:36:37.636
[MUSIC PLAYING]

00:36:55.914 --> 00:36:57.396
[END PLAYBACK]

00:37:00.900 --> 00:37:05.100
Here too, we try to have
an embodied musical mind

00:37:05.100 --> 00:37:08.820
and have an understanding of
what are the important music

00:37:08.820 --> 00:37:10.640
notes in a piece.

00:37:10.640 --> 00:37:12.520
Tonally, maybe
they're more stable,

00:37:12.520 --> 00:37:14.780
maybe it's the end of a
phrase that goes up and so on.

00:37:14.780 --> 00:37:16.950
And then combine it
with some understanding

00:37:16.950 --> 00:37:20.680
of what are the
positions in which what

00:37:20.680 --> 00:37:24.270
can move to particular
places, being

00:37:24.270 --> 00:37:26.530
aware of its own
limitation in terms

00:37:26.530 --> 00:37:29.090
of the time of the gestures.

00:37:29.090 --> 00:37:31.340
So here you see
the red dots that

00:37:31.340 --> 00:37:34.260
represents a
particular end point

00:37:34.260 --> 00:37:39.740
in gestures that try to be
based on the important notes

00:37:39.740 --> 00:37:40.430
melodically.

00:37:40.430 --> 00:37:43.770
Again, the combination of
being aware of the embodiment

00:37:43.770 --> 00:37:47.200
and of musical theory
and creating a system

00:37:47.200 --> 00:37:49.895
that, I think Mason
here put some titles

00:37:49.895 --> 00:37:51.830
that you can read as he plays.

00:37:51.830 --> 00:37:53.315
[VIDEO PLAYBACK]

00:37:53.315 --> 00:37:58.265
[PLAYING GUITAR]

00:38:39.845 --> 00:38:41.870
[END PLAYBACK]

00:38:41.870 --> 00:38:44.140
And as you can see, Mason
is extremely talented,

00:38:44.140 --> 00:38:46.706
and he is looking for an
internship this summer.

00:38:46.706 --> 00:38:48.990
[LAUGHTER]

00:38:48.990 --> 00:38:51.990
Just to end the
Shimi part-- maybe

00:38:51.990 --> 00:38:54.100
I'll just play this little clip.

00:38:54.100 --> 00:38:56.946
I couldn't make a Shimi without
letting it dance to this song.

00:38:56.946 --> 00:38:57.612
[VIDEO PLAYBACK]

00:38:57.612 --> 00:39:01.580
[MUSIC STAR WARS, "CANTINA
 SONG"]

00:39:29.852 --> 00:39:30.900
[END PLAYBACK]

00:39:30.900 --> 00:39:34.130
And that's the last project
that we just finished last year.

00:39:37.030 --> 00:39:39.760
I actually took a break.

00:39:39.760 --> 00:39:41.510
I took a sabbatical year off.

00:39:41.510 --> 00:39:43.750
And just when I was about
to go to the sabbatical

00:39:43.750 --> 00:39:46.860
I got an email
from Jason Barnes.

00:39:46.860 --> 00:39:49.770
He's an amputee drummer.

00:39:49.770 --> 00:39:52.480
He lost his arm, now it would
be like three years ago.

00:39:52.480 --> 00:39:55.150
When we met it was two.

00:39:55.150 --> 00:39:57.340
And he saw some of
the videos online

00:39:57.340 --> 00:40:00.330
and he was really devastated.

00:40:00.330 --> 00:40:01.900
There's a clip online.

00:40:01.900 --> 00:40:05.910
You can see him talking about
how he thought he had nothing

00:40:05.910 --> 00:40:09.330
to live for if he cannot drum
when he lost his arm when he

00:40:09.330 --> 00:40:14.080
was electrocuted in some
weird accidents on a roof.

00:40:14.080 --> 00:40:17.714
And then he saw these video
and he emailed me and asked me,

00:40:17.714 --> 00:40:19.130
can you use some
of the technology

00:40:19.130 --> 00:40:21.031
to build me a robotic arm?

00:40:21.031 --> 00:40:23.030
And, of course, that was
very intriguing for me.

00:40:23.030 --> 00:40:27.100
What he really wanted was to
be able to control the grip.

00:40:27.100 --> 00:40:29.650
Because he had the arm up
all the way up until here so

00:40:29.650 --> 00:40:30.630
he can still hit.

00:40:30.630 --> 00:40:32.588
But for a drummer, it's
very important for them

00:40:32.588 --> 00:40:36.830
to have control of the
grip, tight or loose

00:40:36.830 --> 00:40:40.170
to create different
kinds of drum strokes.

00:40:40.170 --> 00:40:42.700
And that was very
interesting for me.

00:40:42.700 --> 00:40:44.550
And the people I
usually collaborate

00:40:44.550 --> 00:40:47.446
with couldn't come
through, so I-- I

00:40:47.446 --> 00:40:49.070
don't know if anyone
here is from Meka,

00:40:49.070 --> 00:40:52.280
but I did contact Meka
who built this arm.

00:40:52.280 --> 00:40:59.620
And that was great but because
I'm interested in improvisation

00:40:59.620 --> 00:41:04.630
and creativity, I really
wanted to have this aspect

00:41:04.630 --> 00:41:08.370
of my research in this arm.

00:41:08.370 --> 00:41:10.840
So I asked Jason if he
minded that we had two

00:41:10.840 --> 00:41:13.070
different sticks, not only one.

00:41:13.070 --> 00:41:15.860
One was all about grip, so
you can have your expression

00:41:15.860 --> 00:41:16.930
and play it as he wants.

00:41:16.930 --> 00:41:20.570
The other one actually
would listen and improvise

00:41:20.570 --> 00:41:24.650
and will listen to the music
and listen to other musicians

00:41:24.650 --> 00:41:27.120
and try to maybe inspire
you to create something else

00:41:27.120 --> 00:41:31.055
and become better or try
to create music that he

00:41:31.055 --> 00:41:32.900
would never create otherwise.

00:41:32.900 --> 00:41:34.580
Let me show you
a couple of clips

00:41:34.580 --> 00:41:38.790
first of the basic interaction
of just controlling the group.

00:41:38.790 --> 00:41:42.220
And what we discovered
after Phillip from Meka

00:41:42.220 --> 00:41:47.905
built the arm is that we don't
need just to hold the stick,

00:41:47.905 --> 00:41:50.470
we can actually use
it to generate hits.

00:41:50.470 --> 00:41:52.910
And after some work
on noise reduction,

00:41:52.910 --> 00:41:56.310
we were able to some
pretty quick responses,

00:41:56.310 --> 00:41:58.610
very low latency
so Jason could just

00:41:58.610 --> 00:42:01.920
flex his muscle, in the
muscle and actually create

00:42:01.920 --> 00:42:04.830
a beat, in addition
to using his own arm.

00:42:04.830 --> 00:42:06.920
So there's some
aspect of a cyborg

00:42:06.920 --> 00:42:09.460
there because part
of the gesture

00:42:09.460 --> 00:42:11.650
is his own body and part
of it is electronically

00:42:11.650 --> 00:42:14.150
based on his muscle and
maybe other muscles,

00:42:14.150 --> 00:42:17.390
not only the forearm muscle.

00:42:17.390 --> 00:42:19.250
So actually, this
took eight months.

00:42:19.250 --> 00:42:22.330
It was amazingly fast,
again, thanks to Meka.

00:42:22.330 --> 00:42:25.870
And this is the first
time Jason tried it.

00:42:25.870 --> 00:42:27.838
MALE SPEAKER: Next.

00:42:27.838 --> 00:42:30.782
Next.

00:42:30.782 --> 00:42:31.282
Next.

00:42:46.544 --> 00:42:48.210
GIL WIENBERG: And
that's where we really

00:42:48.210 --> 00:42:53.840
got-- I didn't expect that,
that we can create 20 hertz.

00:42:53.840 --> 00:42:55.830
And you know, my favorite
polyrhythm games.

00:43:10.980 --> 00:43:13.320
I'll show more of the
musical context later.

00:43:13.320 --> 00:43:15.450
But let's first--
see, here we didn't

00:43:15.450 --> 00:43:16.860
have the EMG working yet.

00:43:16.860 --> 00:43:20.150
It's a little knob that changes
between tight and loose,

00:43:20.150 --> 00:43:21.610
but just to see the effect.

00:43:21.610 --> 00:43:22.585
This was a tight hit.

00:43:22.585 --> 00:43:23.410
[VIDEO PLAYBACK]

00:43:23.410 --> 00:43:26.700
[DRUMMING]

00:43:26.700 --> 00:43:29.070
Assuming the holding
of the stick tight,

00:43:29.070 --> 00:43:30.892
this is a loose hit.

00:43:30.892 --> 00:43:34.700
[DRUMMING]

00:43:34.700 --> 00:43:37.530
And this is a role tight.

00:43:37.530 --> 00:43:44.390
[DRUMMING]

00:43:44.390 --> 00:43:45.380
[END PLAYBACK]

00:43:45.380 --> 00:43:47.175
Many more expression
than he had before.

00:43:47.175 --> 00:43:48.300
He couldn't do all of this.

00:43:48.300 --> 00:43:51.010
And actually, he was in
tears the first time.

00:43:51.010 --> 00:43:54.859
He said, I didn't play this
well since I lost my arm.

00:43:54.859 --> 00:43:57.150
Which was the first time I
actually worked with people.

00:43:57.150 --> 00:44:01.370
And it was pretty powerful.

00:44:01.370 --> 00:44:04.960
But, here, you can
see the second stick.

00:44:04.960 --> 00:44:06.340
The kick is very short.

00:44:06.340 --> 00:44:09.490
Look here, we have
an accelerometer

00:44:09.490 --> 00:44:11.460
and when he wants--
only when he wants,

00:44:11.460 --> 00:44:14.900
he can lift the arm quickly and
the second stick will pop up.

00:44:14.900 --> 00:44:17.714
It will be really fast.

00:44:17.714 --> 00:44:18.690
OK.

00:44:18.690 --> 00:44:21.710
And then we can start
with the improvisation

00:44:21.710 --> 00:44:25.310
and the creativity,
the machine creativity.

00:44:25.310 --> 00:44:29.210
Before we do that we
can just have a computer

00:44:29.210 --> 00:44:31.690
generate strikes.

00:44:31.690 --> 00:44:32.864
So nothing to do with him.

00:44:32.864 --> 00:44:34.280
Even though it's
part of his body,

00:44:34.280 --> 00:44:37.097
suddenly someone else is
generating hits for him.

00:44:50.540 --> 00:44:53.660
And even Pat Metheny's
drummer cannot do.

00:44:53.660 --> 00:44:56.740
Not with one arm.

00:44:56.740 --> 00:44:59.320
And this is, again,
playing with roll,

00:44:59.320 --> 00:45:02.760
with tumble, because it's so
fast it becomes actually color

00:45:02.760 --> 00:45:03.690
and not even rhythm.

00:45:14.386 --> 00:45:16.260
And this is what I call
cyborg drumming where

00:45:16.260 --> 00:45:19.380
you combine some of the
computer generated hits

00:45:19.380 --> 00:45:20.270
with his own hits.

00:45:20.270 --> 00:45:23.280
He decides where to put the
arm, where not, what to choose.

00:45:23.280 --> 00:45:24.890
Create a human
machine interaction

00:45:24.890 --> 00:45:27.160
and musical collaboration.

00:45:36.710 --> 00:45:39.770
And at some point-- actually
I wrote an NSF proposal

00:45:39.770 --> 00:45:44.290
about it-- we want to create
some human brain machine

00:45:44.290 --> 00:45:45.320
interaction.

00:45:45.320 --> 00:45:47.001
But now some of the
most sophisticated

00:45:47.001 --> 00:45:48.250
stuff we're doing with pedals.

00:45:52.840 --> 00:45:58.920
With his leg, it controls
dynamics and expressive

00:45:58.920 --> 00:46:01.810
expression elements.

00:46:01.810 --> 00:46:04.900
And when this first
clip went online,

00:46:04.900 --> 00:46:06.290
it also kind of became viral.

00:46:06.290 --> 00:46:08.160
And I got a lot of
emails and one of them

00:46:08.160 --> 00:46:12.560
was from a guy called
Tom Leighton who actually

00:46:12.560 --> 00:46:14.045
played with Richard Feynman.

00:46:14.045 --> 00:46:17.730
I didn't know that Richard
Feynman was a drummer,

00:46:17.730 --> 00:46:19.100
playing hand drums.

00:46:19.100 --> 00:46:21.960
And he said, I have a bunch of
recordings from Richard Feynman

00:46:21.960 --> 00:46:24.561
and I would love for
you to play with it

00:46:24.561 --> 00:46:26.310
and see if you could
do something with it.

00:46:26.310 --> 00:46:29.010
And of course I jumped
on this opportunity.

00:46:29.010 --> 00:46:31.740
And what we did here is
we put some of the rhythm

00:46:31.740 --> 00:46:34.080
that Richard Feynman
played in his arm,

00:46:34.080 --> 00:46:38.210
so he had a chance to play with
Richard Feynman in a weird way.

00:46:38.210 --> 00:46:40.430
And first you'll see him
hovering over the drum,

00:46:40.430 --> 00:46:42.730
just listening to the
rhythm, and then he

00:46:42.730 --> 00:46:47.274
starts to decide what drum, when
he whats to lift the symbol,

00:46:47.274 --> 00:46:47.940
and so on so on.

00:46:47.940 --> 00:46:49.730
So it created all
kinds of interaction

00:46:49.730 --> 00:46:53.180
with someone who is
not with us anymore.

00:46:53.180 --> 00:46:57.154
And I think it also was a
powerful experience for him.

00:46:57.154 --> 00:46:57.820
[VIDEO PLAYBACK]

00:46:57.820 --> 00:46:58.680
[DRUMMING]

00:46:58.680 --> 00:47:02.130
That's the rhythm by
itself, just hovering.

00:47:02.130 --> 00:47:05.687
And now he adds to it
with the other arm.

00:47:05.687 --> 00:47:12.992
[DRUMMING]

00:47:18.840 --> 00:47:21.350
Now he starts to
decide where to put it.

00:47:21.350 --> 00:47:24.280
He's hovering but
now we drew this

00:47:24.280 --> 00:47:27.140
as part of his
gestures and movements.

00:47:27.140 --> 00:47:28.480
[END PLAYBACK]

00:47:28.480 --> 00:47:30.742
Here we have analysis
of the chords

00:47:30.742 --> 00:47:32.450
and we try to create
similar connections,

00:47:32.450 --> 00:47:37.110
a very intimate connection
between the guitar player,

00:47:37.110 --> 00:47:39.840
Andrew, and Jason.

00:47:39.840 --> 00:47:41.090
Different chord that he plays.

00:47:41.090 --> 00:47:43.220
We analyze the chord
and change the rhythms

00:47:43.220 --> 00:47:44.630
that Jason is playing.

00:47:44.630 --> 00:47:46.470
So suddenly he has
to respond, not only

00:47:46.470 --> 00:47:48.720
to the arm that is
generated by the computer

00:47:48.720 --> 00:47:51.730
or generated by his muscle,
but generated or controlled

00:47:51.730 --> 00:47:53.010
by a different musician.

00:47:53.010 --> 00:47:55.680
And you see the whole eye
contact and the whole,

00:47:55.680 --> 00:47:57.640
trying to create
something together,

00:47:57.640 --> 00:47:59.776
happening between them two.

00:47:59.776 --> 00:48:00.762
[VIDEO PLAYBACK]

00:48:00.762 --> 00:48:04.213
[DRUMMING AND GUITAR]

00:48:12.359 --> 00:48:13.900
It was one of the
first times that he

00:48:13.900 --> 00:48:18.010
did it and it's not tight,
but it becomes tighter

00:48:18.010 --> 00:48:20.530
with every rehearsal
and every performance.

00:48:20.530 --> 00:48:24.410
[DRUMMING AND GUITAR]

00:48:33.140 --> 00:48:34.610
[END PLAYBACK]

00:48:34.610 --> 00:48:39.146
And here he is, in context,
trading four for those of you

00:48:39.146 --> 00:48:40.140
who play jazz.

00:48:40.140 --> 00:48:42.220
You play four bars
and then someone else

00:48:42.220 --> 00:48:43.886
improvises four bars
and back and forth.

00:48:43.886 --> 00:48:47.160
Here, the arm listens to the
improvisation for four bars

00:48:47.160 --> 00:48:48.970
and then Jason
gets to improvise.

00:48:48.970 --> 00:48:51.640
And the arm is responding,
improvising, [INAUDIBLE]

00:48:51.640 --> 00:48:55.124
classic manipulation of what it
heard in the four bars before.

00:48:55.124 --> 00:48:55.790
[VIDEO PLAYBACK]

00:48:55.790 --> 00:48:58.730
[JAZZ MUSIC]

00:49:14.410 --> 00:49:15.410
[END PLAYBACK]

00:49:15.410 --> 00:49:19.720
And I have longer clips
for all of this online.

00:49:19.720 --> 00:49:20.870
This is the latest project.

00:49:20.870 --> 00:49:25.930
I just received this videos
from Deepak, my student.

00:49:25.930 --> 00:49:28.380
Just yesterday he just
finished doing that.

00:49:28.380 --> 00:49:31.630
And if you may
have noticed, a lot

00:49:31.630 --> 00:49:34.140
of the expressions that
if you are a drummer

00:49:34.140 --> 00:49:38.870
you probably know that you can
hit in much more versatile ways

00:49:38.870 --> 00:49:40.366
than just a hit or not a hit.

00:49:40.366 --> 00:49:41.740
There's a lot of
multiple bounces

00:49:41.740 --> 00:49:43.090
and a lot of expression in it.

00:49:43.090 --> 00:49:45.780
Here is Deepak who is a drummer.

00:49:45.780 --> 00:49:47.240
My students need
to be musicians,

00:49:47.240 --> 00:49:49.070
and in this case roboticist.

00:49:49.070 --> 00:49:52.100
Actually he was just accepted
to a robotics program

00:49:52.100 --> 00:49:53.550
at Northwestern.

00:49:53.550 --> 00:49:56.130
The first master student
from music technologies

00:49:56.130 --> 00:49:59.060
that actually got into
a robotics program.

00:49:59.060 --> 00:50:01.310
It shows the different
kind of drumming techniques

00:50:01.310 --> 00:50:02.874
that he has.

00:50:02.874 --> 00:50:04.365
[VIDEO PLAYBACK]

00:50:29.230 --> 00:50:33.670
So all of this richness and
expression, we couldn't have.

00:50:33.670 --> 00:50:37.570
So Deepak had to go and
learn PID control, which

00:50:37.570 --> 00:50:39.510
again, is not one of
the classes that we

00:50:39.510 --> 00:50:40.870
offered in music technology.

00:50:40.870 --> 00:50:42.220
[END PLAYBACK]

00:50:42.220 --> 00:50:46.980
And he tried to come up
with a model for the talk,

00:50:46.980 --> 00:50:50.100
taking into account gravity.

00:50:50.100 --> 00:50:53.834
And something he had to know,
it's a very complex system.

00:50:53.834 --> 00:50:55.500
The bounciness of the
drum, for example,

00:50:55.500 --> 00:50:57.010
is not being taken into account.

00:50:57.010 --> 00:51:00.420
But he tried to put
everything in simulation.

00:51:00.420 --> 00:51:04.220
And with some coefficient
of restitution

00:51:04.220 --> 00:51:07.010
here is 0.85 just as a constant.

00:51:07.010 --> 00:51:10.500
And he tried to create all
kind of bounce behaviors.

00:51:10.500 --> 00:51:13.187
Some of them are just based on
humans and bouncing balls kind

00:51:13.187 --> 00:51:13.770
of techniques.

00:51:13.770 --> 00:51:17.760
But some of them are actually
bounces that humans will not

00:51:17.760 --> 00:51:19.820
create or are not likely to.

00:51:19.820 --> 00:51:21.350
You see this is much faster.

00:51:21.350 --> 00:51:24.300
At its end, it's lifting
for the next hit.

00:51:24.300 --> 00:51:28.130
And here you have a high to
low force profile you see.

00:51:28.130 --> 00:51:30.940
This hit is softer
than the next one.

00:51:30.940 --> 00:51:36.000
So you really have to change--
it's time varying talk changes.

00:51:36.000 --> 00:51:43.190
And he put it into a model
and created a PID control.

00:51:43.190 --> 00:51:45.500
And this is the first clip.

00:51:45.500 --> 00:51:48.410
He just did it yesterday
morning and sent it to me

00:51:48.410 --> 00:51:50.210
just before I left.

00:51:50.210 --> 00:51:51.635
[VIDEO PLAYBACK]

00:51:57.340 --> 00:51:59.770
So hopefully this will
be embedded into some

00:51:59.770 --> 00:52:03.680
of the new performances.

00:52:03.680 --> 00:52:05.730
And we're going to
France to perform in May.

00:52:05.730 --> 00:52:12.520
Hopefully Jason can play a much
more wide range of expression.

00:52:12.520 --> 00:52:15.890
This is some of the future works
that the NSF proposal that I

00:52:15.890 --> 00:52:17.980
have wrote is going to address.

00:52:17.980 --> 00:52:20.290
Am I'm working with
a neuroscientist

00:52:20.290 --> 00:52:23.170
from Northwestern, [INAUDIBLE].

00:52:23.170 --> 00:52:28.680
And we are trying to
get some understanding

00:52:28.680 --> 00:52:31.240
about anticipation.

00:52:31.240 --> 00:52:33.750
Because there are
latencies in the system,

00:52:33.750 --> 00:52:38.410
and no one isn't focusing
on volition detection, which

00:52:38.410 --> 00:52:41.250
is basically-- your
brain is shooting,

00:52:41.250 --> 00:52:43.190
sometimes seconds
before your motor cortex

00:52:43.190 --> 00:52:46.682
actually knows that
you're about to hit.

00:52:46.682 --> 00:52:48.640
In a way, you know you're
going to do something

00:52:48.640 --> 00:52:49.970
before you know you're
going to do something,

00:52:49.970 --> 00:52:51.594
or a part of your
brain knows about it.

00:52:51.594 --> 00:52:53.370
I tried to get this data.

00:52:53.370 --> 00:52:58.810
We did some experiments
with 36 EEG electrodes,

00:52:58.810 --> 00:53:01.300
and there maybe some
way to get information

00:53:01.300 --> 00:53:04.790
before the hit is going
to be made and then

00:53:04.790 --> 00:53:06.450
better anticipation algorithms.

00:53:06.450 --> 00:53:09.100
So if he hits with
both hands, or hits

00:53:09.100 --> 00:53:11.180
with both times-- this
hand hits by itself,

00:53:11.180 --> 00:53:13.346
it will be on time because
we have some information.

00:53:15.660 --> 00:53:18.690
We did some machinery to detect
different kind of strokes

00:53:18.690 --> 00:53:22.740
and we saw that the
sum-- it's promising.

00:53:22.740 --> 00:53:25.640
It might be tricky and
we have some outliers,

00:53:25.640 --> 00:53:28.350
but we can pretty much
detect single strokes

00:53:28.350 --> 00:53:30.760
versus double strokes, which
is something else that we

00:53:30.760 --> 00:53:32.375
can try to implement.

00:53:35.160 --> 00:53:38.880
And this is one way it will
probably never look like.

00:53:38.880 --> 00:53:40.720
But this picture
is in the proposal.

00:53:40.720 --> 00:53:42.670
Justin is an example.

00:53:42.670 --> 00:53:44.920
The trick here, when
you create such cyborgs

00:53:44.920 --> 00:53:49.610
is how not to interfere with
what you're doing anyhow.

00:53:49.610 --> 00:53:53.040
But as you can see, I'm jumping
into third arm scenarios.

00:53:53.040 --> 00:53:55.790
So half of the proposal
would be working with Jason

00:53:55.790 --> 00:53:57.190
on his own arm, creating this.

00:53:57.190 --> 00:54:00.230
But the second part is, let's
try, available to anyone,

00:54:00.230 --> 00:54:03.540
whether you are disabled or
abled, and have a third arm.

00:54:03.540 --> 00:54:05.420
And the promise, the
spiel is that if we

00:54:05.420 --> 00:54:09.420
have anticipation algorithms
and timing and synchronization,

00:54:09.420 --> 00:54:12.950
that works in music-- and music
is a very, very time demanding

00:54:12.950 --> 00:54:15.340
and medium.

00:54:15.340 --> 00:54:16.840
You know, you hear
five milliseconds

00:54:16.840 --> 00:54:19.820
that what you heard is two.

00:54:19.820 --> 00:54:22.390
So if we can pass
synchronization in music,

00:54:22.390 --> 00:54:24.930
we can anticipate and help
in all kind of scenarios--

00:54:24.930 --> 00:54:30.660
on operation rooms or fixing a
space station with a third arm.

00:54:30.660 --> 00:54:32.780
So how to put it on
the body in a way

00:54:32.780 --> 00:54:35.657
that you can be
completely-- first,

00:54:35.657 --> 00:54:37.240
the cognitive load,
how to control it.

00:54:37.240 --> 00:54:39.910
So we thought about the
brain, the organ of muscles

00:54:39.910 --> 00:54:41.540
that we can still try to hit.

00:54:41.540 --> 00:54:44.290
But the muscles are used
for legs for the drums

00:54:44.290 --> 00:54:45.960
and for arms for the drums.

00:54:45.960 --> 00:54:49.040
This is a different kind of
design for the future brain

00:54:49.040 --> 00:54:50.360
machine control.

00:54:50.360 --> 00:54:53.580
And I'll be happy to go
for questions right now.

00:54:53.580 --> 00:54:55.458
Thank you very much.

00:54:55.458 --> 00:54:59.177
[APPLAUSE]

00:54:59.177 --> 00:55:00.760
AUDIENCE: I think
you alluded to this,

00:55:00.760 --> 00:55:02.176
but can you say
more about why you

00:55:02.176 --> 00:55:04.730
think the physical manifestation
of a striking thing

00:55:04.730 --> 00:55:06.640
is important.

00:55:06.640 --> 00:55:08.910
Are there other artists
actually getting information

00:55:08.910 --> 00:55:12.730
from the motion of the robot
or is it just for looks.

00:55:12.730 --> 00:55:14.960
GIL WIENBERG: So one
study showed that it

00:55:14.960 --> 00:55:16.660
helped with synchronization.

00:55:16.660 --> 00:55:19.470
That's the graphs that I showed
when you see the arm moving,

00:55:19.470 --> 00:55:20.430
when you see the beat.

00:55:23.010 --> 00:55:26.230
You'll better synchronize
your gestures with it.

00:55:26.230 --> 00:55:27.930
I'm a big believer
in embodiment.

00:55:27.930 --> 00:55:30.920
I believe that that's why
we go to concerts as opposed

00:55:30.920 --> 00:55:34.820
to listening to music at home.

00:55:34.820 --> 00:55:38.000
We want to see how people
interact, not only by music

00:55:38.000 --> 00:55:40.920
but by gestures.

00:55:40.920 --> 00:55:45.600
A drummer and a guitar
will end the song together.

00:55:45.600 --> 00:55:48.800
And there's a lot about
interacting with gestures

00:55:48.800 --> 00:55:52.910
and being aware of your body
as a musician that I think you

00:55:52.910 --> 00:55:54.010
cannot do without it.

00:55:54.010 --> 00:55:56.539
Of course, all of it can two
iPhones that play this music.

00:55:56.539 --> 00:55:59.080
But I think we're going to miss
the whole musical experience.

00:55:59.080 --> 00:56:02.480
It's something really deep
and strong in embodiment

00:56:02.480 --> 00:56:04.080
in the musical experience.

00:56:06.610 --> 00:56:09.640
AUDIENCE: So two sort of related
questions, if you don't mind.

00:56:09.640 --> 00:56:11.670
The first is, have
you tried sort

00:56:11.670 --> 00:56:15.400
of persisting some information
across several sessions for any

00:56:15.400 --> 00:56:18.060
of these to try to
have the robot develop

00:56:18.060 --> 00:56:20.820
some kind of style
more incrementally,

00:56:20.820 --> 00:56:22.730
and sort of lock
into certain patterns

00:56:22.730 --> 00:56:25.905
that it prefers systematically
over longer periods of time?

00:56:25.905 --> 00:56:27.280
GIL WIENBERG: I
thought about it.

00:56:27.280 --> 00:56:28.470
We didn't get to it.

00:56:28.470 --> 00:56:30.017
It's a great idea
because musicians--

00:56:30.017 --> 00:56:30.850
that's what they do.

00:56:30.850 --> 00:56:33.330
There's length, there's
lifelong learning.

00:56:33.330 --> 00:56:36.190
And we're doing
just small episodes.

00:56:36.190 --> 00:56:39.190
There was a student that was
interested in starting to do

00:56:39.190 --> 00:56:42.960
but then he decided that
a master's is enough.

00:56:42.960 --> 00:56:44.490
He doesn't need a PhD.

00:56:44.490 --> 00:56:47.220
But it's a great suggestion
and it's definitely

00:56:47.220 --> 00:56:49.800
something important for music.

00:56:49.800 --> 00:56:52.000
AUDIENCE: And then
related to that,

00:56:52.000 --> 00:56:54.550
some composers or
musicians have--

00:56:54.550 --> 00:56:57.770
you talked about
embodiment-- certain sort

00:56:57.770 --> 00:57:01.495
of different bodily
features like, I don't know,

00:57:01.495 --> 00:57:04.230
someone like Django Reinhardt
in jazz had only two fingers

00:57:04.230 --> 00:57:05.290
to use on the guitar.

00:57:05.290 --> 00:57:07.580
Rachmaninoff had very big hands.

00:57:07.580 --> 00:57:11.882
So have you thought about
sort of creating robots

00:57:11.882 --> 00:57:14.090
that have different variations
on what kind of motion

00:57:14.090 --> 00:57:16.715
is available to them and seeing
how that guides things, or sort

00:57:16.715 --> 00:57:18.900
of, how more
constraint can change?

00:57:18.900 --> 00:57:21.020
GIL WIENBERG: So that's
the latest work by Mason

00:57:21.020 --> 00:57:22.287
with the path planning.

00:57:22.287 --> 00:57:23.870
And he tried to come
up with a system.

00:57:23.870 --> 00:57:28.680
His PhD thesis will come up
with a model of other musicians.

00:57:28.680 --> 00:57:30.440
That's one of his
main goals to continue

00:57:30.440 --> 00:57:32.315
to develop the field of
robotic musicianship.

00:57:32.315 --> 00:57:34.190
Other people who
build robots will

00:57:34.190 --> 00:57:37.370
be able to use this kind
of path planning ideas

00:57:37.370 --> 00:57:41.976
and put as a coefficient
the different limitations

00:57:41.976 --> 00:57:42.600
that they have.

00:57:42.600 --> 00:57:43.740
AUDIENCE: OK.

00:57:43.740 --> 00:57:44.240
Thank you.

00:57:44.240 --> 00:57:45.230
GIL WIENBERG: Yeah.

00:57:45.230 --> 00:57:47.420
AUDIENCE: There was
only one instance

00:57:47.420 --> 00:57:49.330
where you mentioned
a robot knowing

00:57:49.330 --> 00:57:53.660
anything context-specific
or genre-specific.

00:57:53.660 --> 00:57:56.600
For most of the
examples, it was instead,

00:57:56.600 --> 00:57:59.070
everything that the
robot should know

00:57:59.070 --> 00:58:00.680
about how to
generate the content

00:58:00.680 --> 00:58:03.779
comes from the content
it's receiving.

00:58:03.779 --> 00:58:05.820
But then there was one
where one of your students

00:58:05.820 --> 00:58:08.650
programmed all of these
jazz theories into it.

00:58:08.650 --> 00:58:11.360
Do you have an
aversion to telling

00:58:11.360 --> 00:58:14.500
a robot genre-specific or
context-specific information?

00:58:14.500 --> 00:58:18.100
Or do you want it to be as
blank of a slate as possible?

00:58:18.100 --> 00:58:22.240
GIL WIENBERG: I don't have
aversion for anything.

00:58:22.240 --> 00:58:23.950
I come from jazz.

00:58:23.950 --> 00:58:25.150
Jazz is about improvisation.

00:58:25.150 --> 00:58:27.649
A lot of what I'm trying to do
is [INAUDIBLE] improvisation.

00:58:27.649 --> 00:58:30.370
So it doesn't necessarily
fit maybe other genres.

00:58:30.370 --> 00:58:33.850
Like with classical, yes,
Bach can be improvised

00:58:33.850 --> 00:58:35.720
and you can have all
kind of counterpoints

00:58:35.720 --> 00:58:36.678
based on improvisation.

00:58:36.678 --> 00:58:40.360
But I can see a scenario
when you're trying

00:58:40.360 --> 00:58:44.750
to put classical theory in it.

00:58:44.750 --> 00:58:48.840
I less see how it's going to
be used for improvisation.

00:58:48.840 --> 00:58:51.685
I think that it should
be very interesting, yes.

00:58:51.685 --> 00:58:54.310
AUDIENCE: Do you think you would
ever do it negatively and say,

00:58:54.310 --> 00:58:56.940
OK, well, these things, if
you ever come across this,

00:58:56.940 --> 00:58:58.370
this is not idiomatic?

00:58:58.370 --> 00:58:59.850
You should avoid it?

00:58:59.850 --> 00:59:02.990
I'm thinking, for example,
in jazz of course,

00:59:02.990 --> 00:59:05.890
but also for the hand drums,
like, I don't know anything

00:59:05.890 --> 00:59:07.430
about that style
of music but would

00:59:07.430 --> 00:59:10.620
you ever want to say
like, in whatever mutation

00:59:10.620 --> 00:59:13.190
or combination that comes
up, this particular pattern

00:59:13.190 --> 00:59:17.072
is seen as ugly?

00:59:17.072 --> 00:59:18.780
GIL WIENBERG: I don't
know if ugly, but--

00:59:18.780 --> 00:59:19.796
AUDIENCE: Or not--

00:59:19.796 --> 00:59:22.130
GIL WIENBERG: Not
part of the grammar.

00:59:22.130 --> 00:59:23.949
Yes, we have it in
the jazz system.

00:59:23.949 --> 00:59:24.490
AUDIENCE: OK.

00:59:24.490 --> 00:59:25.420
Cool.

00:59:25.420 --> 00:59:26.240
GIL WIENBERG: And
I think it will

00:59:26.240 --> 00:59:28.156
be interesting to try
to see what happens when

00:59:28.156 --> 00:59:31.040
we go to other genres,
with maybe a different kind

00:59:31.040 --> 00:59:33.226
of tonal theory.

00:59:33.226 --> 00:59:37.480
AUDIENCE: I have a question
back to the pathing analysis.

00:59:37.480 --> 00:59:41.000
I'm curious if you have a
measure of how much less you

00:59:41.000 --> 00:59:44.000
are able to do when you do have
these physical constraints,

00:59:44.000 --> 00:59:47.580
versus if you could have the
machine improvise it and watch

00:59:47.580 --> 00:59:50.260
other people, and it would
just have to produce tones.

00:59:50.260 --> 00:59:53.357
Is there some sort of measure
as to the limitations,

00:59:53.357 --> 00:59:55.690
qualitatively versus what it
could do, if it didn't have

00:59:55.690 --> 00:59:58.030
the physical limitations
of forearms that

00:59:58.030 --> 01:00:00.337
can't cross over one another?

01:00:00.337 --> 01:00:01.920
GIL WIENBERG: I think
we can deliver--

01:00:01.920 --> 01:00:02.970
I don't have a measure.

01:00:02.970 --> 01:00:05.890
But I think a measure
can be developed

01:00:05.890 --> 01:00:09.900
and I think a measure can
also be used for future robot

01:00:09.900 --> 01:00:14.250
designers to build robots that
have less of these limitations.

01:00:14.250 --> 01:00:16.800
And when we came up
with the four arms,

01:00:16.800 --> 01:00:20.430
that this arm can never play
this octave, we knew that.

01:00:20.430 --> 01:00:24.250
And we knew that, in a way, it
might play faster than human

01:00:24.250 --> 01:00:28.380
and create 16 chords
simultaneously which humans

01:00:28.380 --> 01:00:28.990
cannot do.

01:00:28.990 --> 01:00:32.400
But it will also be more
limited than humans.

01:00:32.400 --> 01:00:36.177
And I think when I play piano,
I do path planning in a way.

01:00:36.177 --> 01:00:37.760
If I don't have the
fingers, if I just

01:00:37.760 --> 01:00:39.600
to look at notes
with the fingers,

01:00:39.600 --> 01:00:41.350
I constantly,
probably, do something

01:00:41.350 --> 01:00:44.460
similar to Viterbi's beam search
job algorithm or something.

01:00:49.220 --> 01:00:52.360
Creating a measure in terms of
coming up with different kind

01:00:52.360 --> 01:00:54.750
of chords that you
can or cannot play--

01:00:54.750 --> 01:00:59.020
different kind of speed of
phrases that you can or cannot,

01:00:59.020 --> 01:00:59.520
you know.

01:00:59.520 --> 01:01:03.050
They're what, if you noticed,
one for the black keys,

01:01:03.050 --> 01:01:06.586
for the accidentals and one line
for-- so the limitation just

01:01:06.586 --> 01:01:08.210
there and see if you
can play clusters.

01:01:08.210 --> 01:01:12.820
Can it play a cluster
of C C sharp D E flat E?

01:01:12.820 --> 01:01:14.822
It can't.

01:01:14.822 --> 01:01:16.530
We don't have a measure
to really come up

01:01:16.530 --> 01:01:18.897
with a coefficient, but
I think it's a good idea.

01:01:18.897 --> 01:01:20.730
I think all of you have
a lot of good ideas.

01:01:20.730 --> 01:01:21.563
AUDIENCE: Thank you.

01:01:24.600 --> 01:01:25.850
AUDIENCE: First, awesome talk.

01:01:25.850 --> 01:01:26.683
Thank you very much.

01:01:26.683 --> 01:01:28.710
The question I have
has to do with,

01:01:28.710 --> 01:01:33.560
kind of, wanting to play
with these robots myself.

01:01:33.560 --> 01:01:35.550
There's a bit of a
bottleneck in that

01:01:35.550 --> 01:01:38.227
your team is doing the hardware
and also all the software.

01:01:38.227 --> 01:01:40.060
Have you thought about
moving towards a more

01:01:40.060 --> 01:01:43.100
platform-based approach where
other labs could write code

01:01:43.100 --> 01:01:45.827
against your robot and
be able to try it out?

01:01:45.827 --> 01:01:47.160
GIL WIENBERG: That's a good one.

01:01:47.160 --> 01:01:50.550
And by the way I'd like to
give credit when credit is due.

01:01:50.550 --> 01:01:53.860
My students are mostly
computer science students.

01:01:53.860 --> 01:01:55.800
The platform itself--
I have colleagues

01:01:55.800 --> 01:01:57.200
come from the MIT Media Lab.

01:01:57.200 --> 01:02:00.180
Guy Hoffman designed
and Rob [INAUDIBLE]

01:02:00.180 --> 01:02:04.060
designed the robots and Meka
designed the latest one.

01:02:04.060 --> 01:02:06.170
So this is not our research.

01:02:08.750 --> 01:02:12.820
Probably the Meka robot
is the best candidate

01:02:12.820 --> 01:02:13.890
to become a platform.

01:02:13.890 --> 01:02:17.630
The others are too
flaky, and too specific.

01:02:17.630 --> 01:02:20.580
But part of what
I'm interested in

01:02:20.580 --> 01:02:23.770
is also the art and the
design and the coolness

01:02:23.770 --> 01:02:24.770
of creating a new robot.

01:02:24.770 --> 01:02:26.670
So it's just because I love it.

01:02:26.670 --> 01:02:34.606
But I think it can be a good
idea to try to, for the arm,

01:02:34.606 --> 01:02:37.570
for example, to try
to make a platform,

01:02:37.570 --> 01:02:40.660
create code, create, I don't
know, an API for other people

01:02:40.660 --> 01:02:41.940
to try to play with.

01:02:41.940 --> 01:02:43.697
Definitely.

01:02:43.697 --> 01:02:45.530
I think this field is
pretty new and there's

01:02:45.530 --> 01:02:47.760
all kinds of [INAUDIBLE].

01:02:47.760 --> 01:02:52.780
At some point, if enough
people think it's important,

01:02:52.780 --> 01:02:54.730
maybe we will use a
robotic operating system.

01:02:54.730 --> 01:03:00.290
But for now, we just focus on
creating cool musical one-offs.

01:03:00.290 --> 01:03:01.700
AUDIENCE: Thanks.

01:03:01.700 --> 01:03:04.310
AUDIENCE: In the
physical gesture robot,

01:03:04.310 --> 01:03:06.020
you showed a picture
of a score where

01:03:06.020 --> 01:03:09.520
you had analyzed important
points in the melody where

01:03:09.520 --> 01:03:12.470
you might end a gesture.

01:03:12.470 --> 01:03:15.330
But it was taking
as input, raw audio.

01:03:15.330 --> 01:03:16.640
Is that right?

01:03:16.640 --> 01:03:19.812
As opposed to a MIDI file
or a digital encoded score.

01:03:19.812 --> 01:03:21.270
GIL WIENBERG: The
input was a chord

01:03:21.270 --> 01:03:23.720
that Mason and played
from a MIDI piano.

01:03:23.720 --> 01:03:26.600
So it got the symbolic
notes from the MIDI.

01:03:26.600 --> 01:03:27.800
AUDIENCE: Oh, OK.

01:03:27.800 --> 01:03:30.727
And he was playing a guitar I
thought at one point as well?

01:03:30.727 --> 01:03:32.310
GIL WIENBERG: Yes,
that was different.

01:03:32.310 --> 01:03:34.240
The guitar was only
played with shimmies.

01:03:34.240 --> 01:03:35.610
Piano plays with Shimon.

01:03:35.610 --> 01:03:39.510
The guitar for Shimi
to depict the right--

01:03:39.510 --> 01:03:44.660
and yes, it did repeat--
actually, as I mentioned,

01:03:44.660 --> 01:03:46.110
this is offline.

01:03:46.110 --> 01:03:47.920
Mason played this
melody, it did it offline

01:03:47.920 --> 01:03:49.050
and then it recreated it.

01:03:49.050 --> 01:03:49.830
AUDIENCE: Oh, OK.

01:03:49.830 --> 01:03:50.210
OK.

01:03:50.210 --> 01:03:50.530
Thank you.

01:03:50.530 --> 01:03:51.363
And one other thing.

01:03:51.363 --> 01:03:55.770
About the auto-correlation
beat detection.

01:03:55.770 --> 01:03:58.880
Does that infer any time
signature information?

01:03:58.880 --> 01:04:03.810
Or how good is that at detecting
strange time signatures?

01:04:03.810 --> 01:04:05.889
If at all?

01:04:05.889 --> 01:04:07.180
GIL WIENBERG: It's not as good.

01:04:07.180 --> 01:04:09.015
Four quarters is much easier.

01:04:09.015 --> 01:04:13.050
A big estimation,
so it pretty much

01:04:13.050 --> 01:04:16.420
focused on regular
time, three, four.

01:04:16.420 --> 01:04:18.330
Others would be more difficult.

01:04:18.330 --> 01:04:20.480
AUDIENCE: Thank you.

01:04:20.480 --> 01:04:23.350
MALE SPEAKER: I
had one question.

01:04:23.350 --> 01:04:27.530
I'm a classical musician and
this is about online learning.

01:04:27.530 --> 01:04:30.030
I mean, yeah, online real-time
learning from the instructor.

01:04:30.030 --> 01:04:33.530
So as a classical musician,
you sit down, you have a lesson

01:04:33.530 --> 01:04:36.550
and you have a instructor
that teaches you,

01:04:36.550 --> 01:04:38.820
based on what they're
hearing, giving you feedback

01:04:38.820 --> 01:04:43.445
and then they correct or
improve your playing right then.

01:04:43.445 --> 01:04:45.070
I don't know anything
about instruction

01:04:45.070 --> 01:04:46.740
for jazz improvisation.

01:04:46.740 --> 01:04:48.380
I guess I've seen a bit of that.

01:04:48.380 --> 01:04:51.170
Wynton Marsalis has
a program right now

01:04:51.170 --> 01:04:55.184
that you can watch where he's
working with some young jazz

01:04:55.184 --> 01:04:56.850
musicians and he's
giving them feedback.

01:04:56.850 --> 01:04:59.580
A lot of it's about
communication and listening.

01:04:59.580 --> 01:05:01.330
Have you thought about
that sort of thing?

01:05:01.330 --> 01:05:03.250
Not so much-- what
you're showing here

01:05:03.250 --> 01:05:07.620
is the actual improvisation, but
if you wanted to give feedback

01:05:07.620 --> 01:05:10.720
to the robot and say, you know,
hey, why don't you try this.

01:05:10.720 --> 01:05:12.760
Or that's a little bit too busy.

01:05:12.760 --> 01:05:17.780
You maybe should be kind
of simpler at this time

01:05:17.780 --> 01:05:20.050
and then add some more
color later or something.

01:05:20.050 --> 01:05:21.371
I don't know.

01:05:21.371 --> 01:05:22.870
GIL WIENBERG: The
same students that

01:05:22.870 --> 01:05:27.050
decided that MS is fine were
thinking about learning.

01:05:27.050 --> 01:05:29.440
Starting from nothing and
learning getting some input

01:05:29.440 --> 01:05:31.390
and reinforced.

01:05:31.390 --> 01:05:37.220
And then develop a style
that will develop over time.

01:05:37.220 --> 01:05:38.450
We were thinking about.

01:05:38.450 --> 01:05:39.700
We didn't get into it.

01:05:39.700 --> 01:05:42.680
But yes, in jazz too, I think
similar to classical music,

01:05:42.680 --> 01:05:43.740
that's how it works.

01:05:43.740 --> 01:05:45.470
First production of the sound.

01:05:45.470 --> 01:05:47.050
You know, when I
first played piano,

01:05:47.050 --> 01:05:49.990
it was a lot of hits on my
fingers and holding turtles.

01:05:54.110 --> 01:05:56.180
If I have some
more students using

01:05:56.180 --> 01:06:00.029
learning-- not in the
machinery applications of it,

01:06:00.029 --> 01:06:02.320
but actually your robot that
knows nothing and starting

01:06:02.320 --> 01:06:04.265
to teach it slowly.

01:06:04.265 --> 01:06:05.390
It can be very interesting.

01:06:05.390 --> 01:06:07.470
Maybe very interesting
new music perhaps that

01:06:07.470 --> 01:06:08.927
wouldn't come this way.

01:06:08.927 --> 01:06:10.510
MALE SPEAKER: OK,
thank you very much.

01:06:10.510 --> 01:06:10.880
That was awesome.

01:06:10.880 --> 01:06:11.050
GIL WIENBERG: All right.

01:06:11.050 --> 01:06:11.650
Thank you.

01:06:11.650 --> 01:06:13.200
[APPLAUSE]

