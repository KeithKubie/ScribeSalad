WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.712
[APPLAUSE]

00:00:04.712 --> 00:00:05.420
SPEAKER 1: Hello.

00:00:05.420 --> 00:00:07.340
I'm going to welcome Sarah.

00:00:07.340 --> 00:00:11.870
Sarah Gold is an expert in
data management and privacy.

00:00:11.870 --> 00:00:14.640
She sits on the board
of Tech for Good.

00:00:14.640 --> 00:00:17.360
And she is going to talk
to us about the trust

00:00:17.360 --> 00:00:19.874
and digital rights
in learning systems.

00:00:19.874 --> 00:00:20.790
SARAH GOLD: Thank you.

00:00:26.246 --> 00:00:31.330
At IF we believe that services
should respect our rights.

00:00:31.330 --> 00:00:34.690
So we work with
the organizations

00:00:34.690 --> 00:00:37.480
shaping our digital
world to show how

00:00:37.480 --> 00:00:40.630
they can be trusted with data.

00:00:40.630 --> 00:00:43.000
Over the last couple
of years, we've

00:00:43.000 --> 00:00:46.600
worked with a whole range
of different organizations.

00:00:46.600 --> 00:00:49.030
And our work is
increasingly taking us

00:00:49.030 --> 00:00:53.470
to quite a specific part
of machine intelligence,

00:00:53.470 --> 00:00:57.630
looking specifically at how
people can trust services that

00:00:57.630 --> 00:01:00.790
are powered by machine
intelligence technologies

00:01:00.790 --> 00:01:04.180
but also how machine
intelligence technologies can

00:01:04.180 --> 00:01:06.130
be trustworthy in
the first place.

00:01:09.150 --> 00:01:11.780
This is a really
important area of work

00:01:11.780 --> 00:01:14.240
because services
are increasingly

00:01:14.240 --> 00:01:18.050
being powered by machine
intelligence technologies.

00:01:18.050 --> 00:01:20.150
And those services
are beginning to creep

00:01:20.150 --> 00:01:23.120
into really important
areas of our life,

00:01:23.120 --> 00:01:26.240
whether that be helping
you to find a job

00:01:26.240 --> 00:01:30.560
or to manage your finances
or, as we found out last week,

00:01:30.560 --> 00:01:32.390
deciding on who to vote for.

00:01:35.730 --> 00:01:39.000
At this point, I
want to re-frame

00:01:39.000 --> 00:01:42.930
what I mean by machine
intelligence technologies.

00:01:42.930 --> 00:01:47.260
Matt Jones uses the
phrase learned systems.

00:01:47.260 --> 00:01:49.030
And I think it's
really effective

00:01:49.030 --> 00:01:54.240
because I think it helps us
to refocus the issue on people

00:01:54.240 --> 00:01:59.520
and society, which are the
most important things, really.

00:01:59.520 --> 00:02:04.200
It also reminds us that we
actively teach these systems.

00:02:04.200 --> 00:02:07.410
And they operate in
a real world context

00:02:07.410 --> 00:02:09.080
that we must never forget about.

00:02:11.850 --> 00:02:14.810
So I'm going to talk to
you about two things, trust

00:02:14.810 --> 00:02:19.340
and rights in learned systems.

00:02:19.340 --> 00:02:22.370
To start, I'm going
to talk about trust.

00:02:22.370 --> 00:02:27.050
And I'm going to give you
an overview of six insights

00:02:27.050 --> 00:02:31.490
that we have found from our
work at IF working directly

00:02:31.490 --> 00:02:33.720
on these issues.

00:02:33.720 --> 00:02:38.210
And whilst I can't show you
exact examples, because I'd

00:02:38.210 --> 00:02:41.450
be in breach of
confidentiality agreements,

00:02:41.450 --> 00:02:44.420
I have found examples to
illustrate these points

00:02:44.420 --> 00:02:47.780
from the public domain
or illustrations

00:02:47.780 --> 00:02:50.020
from IF's blog to show you.

00:02:52.860 --> 00:02:58.440
So to start off, design
for non-determinism.

00:02:58.440 --> 00:03:00.870
As designers, we are
used to designing

00:03:00.870 --> 00:03:03.480
for fixed circumstances.

00:03:03.480 --> 00:03:06.600
It's kind of like
if this, then that.

00:03:06.600 --> 00:03:10.230
With learned systems,
that completely changes.

00:03:10.230 --> 00:03:12.450
As designers, we
don't necessarily

00:03:12.450 --> 00:03:17.250
know what a user will see once
the system is showing them

00:03:17.250 --> 00:03:20.370
a service because there's
an infinite number

00:03:20.370 --> 00:03:26.280
of possible interfaces
that a user might be shown.

00:03:26.280 --> 00:03:30.450
As a user, that creates a
real challenge for trust.

00:03:30.450 --> 00:03:32.850
Because how can you start
to have conversations

00:03:32.850 --> 00:03:36.510
with one another to check
is my thing working right,

00:03:36.510 --> 00:03:38.520
does this look good
to you, is this

00:03:38.520 --> 00:03:41.730
what it should be showing me,
if everyone's service is showing

00:03:41.730 --> 00:03:43.080
them something different.

00:03:43.080 --> 00:03:46.180
That's a real challenge.

00:03:46.180 --> 00:03:49.480
At IF, we think that
designers should

00:03:49.480 --> 00:03:53.080
start thinking about
designing for frameworks

00:03:53.080 --> 00:03:58.840
for learned systems to express
a range of opportunities within.

00:03:58.840 --> 00:04:01.870
That's why as of today,
we've launched a project

00:04:01.870 --> 00:04:05.530
with Alison Powell from the
London School of Economics

00:04:05.530 --> 00:04:07.300
looking at just that.

00:04:07.300 --> 00:04:09.340
What do these
frameworks look like?

00:04:09.340 --> 00:04:11.950
And how do you
design transparency

00:04:11.950 --> 00:04:14.380
into the frameworks
so that users

00:04:14.380 --> 00:04:17.649
are able to understand
and challenge

00:04:17.649 --> 00:04:20.950
what a learned system
presents them with?

00:04:20.950 --> 00:04:22.690
If you'd like to
follow that project,

00:04:22.690 --> 00:04:26.130
please do take a photograph of
that blog post linked there.

00:04:26.130 --> 00:04:30.290
We'll be working on it
over the next few months.

00:04:34.800 --> 00:04:37.710
Design for legibility
and understanding

00:04:37.710 --> 00:04:40.770
is another principle.

00:04:40.770 --> 00:04:43.050
Just like handwriting,
if someone's handwriting

00:04:43.050 --> 00:04:45.570
is really messy, it's
really difficult for you

00:04:45.570 --> 00:04:48.030
to read what they've written.

00:04:48.030 --> 00:04:50.790
In the same way, I
think that services are

00:04:50.790 --> 00:04:54.140
really hard for users to read.

00:04:54.140 --> 00:04:57.490
And not only that, they're
super difficult to understand

00:04:57.490 --> 00:04:59.790
even if you can read them.

00:04:59.790 --> 00:05:04.840
I'd like to ask the audience how
many of use Spotify or iTunes

00:05:04.840 --> 00:05:05.920
or Google Play?

00:05:05.920 --> 00:05:08.210
Hands up.

00:05:08.210 --> 00:05:12.140
And keep your hands up if
you've read the terms of service

00:05:12.140 --> 00:05:15.400
updates recently.

00:05:15.400 --> 00:05:19.199
No one kept their hand up.

00:05:19.199 --> 00:05:19.740
That's right.

00:05:19.740 --> 00:05:22.800
We know that terms of service
or terms and conditions

00:05:22.800 --> 00:05:25.380
are not good enough for
the digital services

00:05:25.380 --> 00:05:29.756
that we use today, let
alone learned systems.

00:05:29.756 --> 00:05:33.420
At IF, we think it's possible
to begin designing terms

00:05:33.420 --> 00:05:39.700
of service out by putting the
information about when services

00:05:39.700 --> 00:05:44.470
are learning, what data is being
used, and what powers a user

00:05:44.470 --> 00:05:48.910
has to change that, all
at the point of use.

00:05:48.910 --> 00:05:52.160
You can see on the
example to the left,

00:05:52.160 --> 00:05:56.260
this is an example of a
fictional benefits service

00:05:56.260 --> 00:05:58.470
that we prototyped at IF.

00:05:58.470 --> 00:06:01.540
And you can see that this
individual has received

00:06:01.540 --> 00:06:03.970
a sanction on their benefit.

00:06:03.970 --> 00:06:05.710
What we were interested
in doing here

00:06:05.710 --> 00:06:09.730
is showing why the individual
received that sanction,

00:06:09.730 --> 00:06:13.570
to give them that information
at the point of use

00:06:13.570 --> 00:06:16.120
and give them an option to
appeal that decision if they

00:06:16.120 --> 00:06:18.650
think that something's
not quite right.

00:06:18.650 --> 00:06:21.100
This is all about the
service explaining itself

00:06:21.100 --> 00:06:22.630
as it's being used.

00:06:22.630 --> 00:06:24.940
And we think this
is something that

00:06:24.940 --> 00:06:28.420
will be really important as we
see more and more services are

00:06:28.420 --> 00:06:31.330
powered by learned systems.

00:06:31.330 --> 00:06:35.590
In the meantime, before we
ditch the terms of service,

00:06:35.590 --> 00:06:38.800
we think it's possible
to make terms of service

00:06:38.800 --> 00:06:42.580
more legible and
easier to understand.

00:06:42.580 --> 00:06:44.560
We've blogged about
some of our thinking

00:06:44.560 --> 00:06:47.500
on that at understanding
terms and conditions.

00:06:47.500 --> 00:06:50.830
And I'd encourage you to look
there at some of the techniques

00:06:50.830 --> 00:06:53.800
that we think are important
to make terms of service

00:06:53.800 --> 00:07:00.770
more machine readable and more
understandable to you and I.

00:07:00.770 --> 00:07:04.020
Design for override is
another interesting area.

00:07:04.020 --> 00:07:07.010
This is because trust is
a two-way relationship.

00:07:07.010 --> 00:07:10.640
So it really wouldn't be any
good if a learned system simply

00:07:10.640 --> 00:07:14.090
presented a user with
decisions that it

00:07:14.090 --> 00:07:18.380
had made without the user
being able to give feedback

00:07:18.380 --> 00:07:20.770
or to take over.

00:07:20.770 --> 00:07:22.750
I think it's really
important that users

00:07:22.750 --> 00:07:26.830
are given the ability to
take over critical automated

00:07:26.830 --> 00:07:28.210
processes.

00:07:28.210 --> 00:07:30.700
And that means that the
user has to know they can

00:07:30.700 --> 00:07:34.260
take over and understand how.

00:07:34.260 --> 00:07:36.850
This is important for
a host of reasons.

00:07:36.850 --> 00:07:39.510
One of them is skills fade.

00:07:39.510 --> 00:07:44.160
There's a really nice parallel
in the transport for London.

00:07:44.160 --> 00:07:47.520
Every Sunday the
London Underground

00:07:47.520 --> 00:07:52.860
turn off automatic braking to
make sure that drivers of tubes

00:07:52.860 --> 00:07:57.176
don't lose their skills from
the automated safety system.

00:07:57.176 --> 00:07:59.262
I think that's a
really nice parallel.

00:07:59.262 --> 00:08:01.470
And I think there are things
we should draw from that

00:08:01.470 --> 00:08:02.820
in our work on services.

00:08:05.840 --> 00:08:08.300
Design for recovery is another.

00:08:08.300 --> 00:08:12.380
So systems are
going to go wrong.

00:08:12.380 --> 00:08:13.710
That will happen.

00:08:13.710 --> 00:08:15.500
And it's really
important for the user

00:08:15.500 --> 00:08:20.210
to be able to remove any
autonomy that the system has

00:08:20.210 --> 00:08:25.760
or to revert back to a previous
system that didn't fail.

00:08:25.760 --> 00:08:27.980
And that should be
possible without having

00:08:27.980 --> 00:08:30.960
systemic failure.

00:08:30.960 --> 00:08:32.990
This is a bit like
cruise control in a car.

00:08:32.990 --> 00:08:35.720
You can have cruise
control on for a long time

00:08:35.720 --> 00:08:39.110
and enjoy the car kind
of driving you somewhere.

00:08:39.110 --> 00:08:42.230
But as soon as you see
something quite unusual up ahead

00:08:42.230 --> 00:08:45.890
or a difficult junction, you
should be able to take over.

00:08:45.890 --> 00:08:48.290
That's really important
because there are some actions

00:08:48.290 --> 00:08:51.110
that you'll be able to navigate
better than any system.

00:08:55.650 --> 00:08:57.400
Design for
respectability, this is

00:08:57.400 --> 00:09:01.150
all about making
sure that a user can

00:09:01.150 --> 00:09:06.070
see what a learned
system is doing and why.

00:09:06.070 --> 00:09:09.190
It's a bit like view source
on the web that we have now.

00:09:09.190 --> 00:09:12.050
Or, apologies for
another car analogy,

00:09:12.050 --> 00:09:15.400
but if you've ever checked
the oil in your engine,

00:09:15.400 --> 00:09:16.060
it's like that.

00:09:16.060 --> 00:09:19.381
You should have the ability
to check what's happening.

00:09:23.310 --> 00:09:25.620
And then there's design
for collective action.

00:09:25.620 --> 00:09:28.940
So whilst more technology
and more design

00:09:28.940 --> 00:09:32.570
is needed in this area,
they're not the only answers.

00:09:32.570 --> 00:09:37.040
And in fact, the answer
isn't just about more design,

00:09:37.040 --> 00:09:39.650
more technology, because we
have to think about the wider

00:09:39.650 --> 00:09:40.700
system.

00:09:40.700 --> 00:09:42.530
And it's really
important that we

00:09:42.530 --> 00:09:47.510
think about the needs of
different levels of society.

00:09:47.510 --> 00:09:51.440
We really can't fall into
the trap of this Californian

00:09:51.440 --> 00:09:55.280
ideology that to
understand a learned system

00:09:55.280 --> 00:09:59.570
or to trust it will be purely
an individualistic action.

00:09:59.570 --> 00:10:01.790
I think it's really
important to think

00:10:01.790 --> 00:10:05.690
about collective action, by
which I mean things we can only

00:10:05.690 --> 00:10:08.720
do together or with the
help of organizations

00:10:08.720 --> 00:10:11.650
that can represent us.

00:10:11.650 --> 00:10:15.210
At IF, we've been looking at
how consumer groups, unions

00:10:15.210 --> 00:10:18.090
or medical charities
can help people

00:10:18.090 --> 00:10:24.436
they represent understand if a
learned system can be trusted.

00:10:24.436 --> 00:10:28.590
But also those organizations can
help society as a whole course

00:10:28.590 --> 00:10:31.040
correct.

00:10:31.040 --> 00:10:33.380
You can see the
example on the left,

00:10:33.380 --> 00:10:35.980
going back to that
fictional benefits

00:10:35.980 --> 00:10:39.350
service I showed earlier, that
the individual that received

00:10:39.350 --> 00:10:40.990
that sanction was
quite concerned

00:10:40.990 --> 00:10:42.930
that that sanction was wrong.

00:10:42.930 --> 00:10:46.730
So they went to an organization
called Citizens Advice.

00:10:46.730 --> 00:10:50.090
Citizens Advice is an
organization in the UK

00:10:50.090 --> 00:10:54.710
that helps citizens navigate
complexity and hard situations

00:10:54.710 --> 00:10:56.700
in government services.

00:10:56.700 --> 00:10:58.700
So this individual who
has received the sanction

00:10:58.700 --> 00:11:00.680
goes to Citizens Advice.

00:11:00.680 --> 00:11:03.530
And the advisor
at Citizens Advice

00:11:03.530 --> 00:11:07.020
is able to see why
that section was given

00:11:07.020 --> 00:11:10.700
and if anyone else in that area
was given a similar sanction

00:11:10.700 --> 00:11:12.590
for the same reason.

00:11:12.590 --> 00:11:14.000
What's important
here, as you can

00:11:14.000 --> 00:11:17.810
see the kind of
highlighted decision

00:11:17.810 --> 00:11:19.580
at the top, this
question, which is

00:11:19.580 --> 00:11:23.060
that 153 other people
in the Salford area

00:11:23.060 --> 00:11:25.160
have missed
appointments in April.

00:11:25.160 --> 00:11:27.470
Would you like to
include Mary's case?

00:11:27.470 --> 00:11:30.440
Because maybe it's that the
bus timetable has changed

00:11:30.440 --> 00:11:32.300
in Salford, so no
one in that area

00:11:32.300 --> 00:11:35.280
can make the right
appointment time.

00:11:35.280 --> 00:11:39.060
So letting organizations
help us means we can also

00:11:39.060 --> 00:11:40.780
have collective action.

00:11:40.780 --> 00:11:44.280
So we can course correct
so we can see as a society

00:11:44.280 --> 00:11:46.040
where these systems
might go wrong

00:11:46.040 --> 00:11:48.270
and, hopefully, early
enough before they

00:11:48.270 --> 00:11:49.605
cause huge problems.

00:11:52.375 --> 00:11:53.750
Of course, that
means that you're

00:11:53.750 --> 00:11:57.050
designing a learned
system, you really

00:11:57.050 --> 00:12:01.280
need to think about where your
learned system fits in society

00:12:01.280 --> 00:12:04.250
and with civic institutions.

00:12:04.250 --> 00:12:07.130
If you'd like to look at more
about what we've been thinking

00:12:07.130 --> 00:12:11.030
about at IF on that, we
wrote a blog post last year

00:12:11.030 --> 00:12:16.300
on making it clear how
machines make decisions.

00:12:16.300 --> 00:12:20.450
This leads onto the
next part of my talk

00:12:20.450 --> 00:12:22.120
which is all about rights.

00:12:26.630 --> 00:12:30.140
So I think that these are
three areas that I would argue

00:12:30.140 --> 00:12:33.860
urgently need addressing by us.

00:12:33.860 --> 00:12:38.825
The first is GDPR
learned systems.

00:12:38.825 --> 00:12:42.650
GDPR is the General Data
Protection Regulation.

00:12:42.650 --> 00:12:46.270
It comes into force
in May this year.

00:12:46.270 --> 00:12:50.960
It affects every company
that holds any information

00:12:50.960 --> 00:12:53.270
about any EU citizen.

00:12:53.270 --> 00:12:56.550
Essentially, for
us as individuals,

00:12:56.550 --> 00:13:00.320
it gives us a whole
suite of digital rights.

00:13:00.320 --> 00:13:02.120
Some of them are
really interesting.

00:13:02.120 --> 00:13:05.510
And if you'd like to see
what those rights could mean

00:13:05.510 --> 00:13:08.270
for services that you
might use every day,

00:13:08.270 --> 00:13:10.920
we did some sketching
a couple of years ago.

00:13:10.920 --> 00:13:13.610
And you can find those
under newdigitalrights

00:13:13.610 --> 00:13:14.810
.projectsbyIF.com.

00:13:17.610 --> 00:13:22.440
GDPR's rights directly
challenge some

00:13:22.440 --> 00:13:25.140
of the ways that learned
systems are built.

00:13:25.140 --> 00:13:28.020
And that's where these
challenges and the questions

00:13:28.020 --> 00:13:31.460
I have for you come in.

00:13:31.460 --> 00:13:35.270
So what is an individual's
right to deletion

00:13:35.270 --> 00:13:39.370
in a federated learning model?

00:13:39.370 --> 00:13:43.360
Because people are complex,
data that they give a service

00:13:43.360 --> 00:13:44.790
could be wrong.

00:13:44.790 --> 00:13:47.080
Or the data they
give a service could

00:13:47.080 --> 00:13:51.670
be right at one moment
in time and wrong now.

00:13:51.670 --> 00:13:55.030
For instance, if someone
has a divorce, that

00:13:55.030 --> 00:13:58.720
will have an impact on maybe
how they use their bank account,

00:13:58.720 --> 00:14:02.290
perhaps the kind of invitations
they make in their calendar,

00:14:02.290 --> 00:14:04.480
or simply what
images it would not

00:14:04.480 --> 00:14:07.150
be OK to show them in
their social media feed.

00:14:09.830 --> 00:14:13.830
These are kind of everyday
problems that everyone has

00:14:13.830 --> 00:14:17.700
and these systems need
to be able to respond to.

00:14:17.700 --> 00:14:22.670
So it needs to be possible for
this kind of ephemeral design

00:14:22.670 --> 00:14:26.810
to work where a system can
explain the learning it's

00:14:26.810 --> 00:14:31.130
got to a user, and the user can
choose to keep it or throw it

00:14:31.130 --> 00:14:32.580
away.

00:14:32.580 --> 00:14:34.610
So my question is
how can we make

00:14:34.610 --> 00:14:40.130
ephemeral learning possible
in a privacy preserving way?

00:14:40.130 --> 00:14:42.360
And how can we make
ephemeral design work?

00:14:45.545 --> 00:14:49.810
Another one is how can
you give a user the right

00:14:49.810 --> 00:14:53.230
to understand in this world
that's nondeterministic

00:14:53.230 --> 00:14:54.820
that I explained earlier?

00:14:57.500 --> 00:15:00.170
I think this is a
really tricky question.

00:15:00.170 --> 00:15:02.420
And I think we have to
think about the way we deal

00:15:02.420 --> 00:15:05.390
with data permissions
and collective action

00:15:05.390 --> 00:15:09.650
to answer some of this because
I think data privacy is just

00:15:09.650 --> 00:15:15.410
far too complex a thing to be
regulated by individuals who

00:15:15.410 --> 00:15:19.640
are really busy doing important
things like making their money

00:15:19.640 --> 00:15:23.700
stretch or finding
food at a food bank.

00:15:23.700 --> 00:15:26.630
They don't have time to
tweak data permissions,

00:15:26.630 --> 00:15:30.140
or let alone the skills, to
check that their choices are,

00:15:30.140 --> 00:15:32.666
in fact, being respected.

00:15:32.666 --> 00:15:34.040
And this is where
I think we have

00:15:34.040 --> 00:15:37.510
to think about groups that
can represent many of us

00:15:37.510 --> 00:15:42.290
to help us understand whether
certain data should be given

00:15:42.290 --> 00:15:47.120
to a service and if that
service is respecting

00:15:47.120 --> 00:15:48.510
the choices that we made.

00:15:51.590 --> 00:15:54.380
Testing in learned systems
is the second area,

00:15:54.380 --> 00:15:57.560
so any system of any
complexity in society

00:15:57.560 --> 00:15:59.330
has required testing.

00:15:59.330 --> 00:16:03.380
If we take an example of a
fridge, a fridge in your home

00:16:03.380 --> 00:16:06.320
will have been tested by
consumer rights organizations

00:16:06.320 --> 00:16:08.480
to make sure that that
fridge doesn't set alight

00:16:08.480 --> 00:16:09.860
in your home.

00:16:09.860 --> 00:16:12.050
The manufacturer
will have also tested

00:16:12.050 --> 00:16:14.120
that fridge to make
sure that it keeps

00:16:14.120 --> 00:16:17.270
your food at a temperature
that's safe to eat at.

00:16:17.270 --> 00:16:19.340
The manufacturer also
will have designed it

00:16:19.340 --> 00:16:22.070
to standards that have been
iterated and decided that this

00:16:22.070 --> 00:16:24.410
is how we build a fridge.

00:16:24.410 --> 00:16:27.420
Now, if you take that
testing ecosystem question,

00:16:27.420 --> 00:16:30.600
that will be necessary
in learned systems.

00:16:30.600 --> 00:16:34.470
But how can you create a testing
ecosystem that doesn't just

00:16:34.470 --> 00:16:36.895
test a kind of inanimate
object like a fridge

00:16:36.895 --> 00:16:38.670
that its performance
only changes

00:16:38.670 --> 00:16:42.555
with age but a learned system
that can change in minutes?

00:16:42.555 --> 00:16:46.740
So how can we create a testing
ecosystem that keeps pace

00:16:46.740 --> 00:16:49.751
with the changes of software?

00:16:49.751 --> 00:16:53.060
Within this as well, we'll
have different people running

00:16:53.060 --> 00:16:56.340
different models on devices.

00:16:56.340 --> 00:16:58.970
So how do we verify
the authenticity

00:16:58.970 --> 00:17:01.520
of different
versions of models so

00:17:01.520 --> 00:17:04.485
that people can make sure
that the model they're using

00:17:04.485 --> 00:17:04.985
is trusted?

00:17:07.520 --> 00:17:11.190
DeepMind are developing
verifiable data audit,

00:17:11.190 --> 00:17:13.260
which is a technology
that essentially

00:17:13.260 --> 00:17:16.319
logs all the different
kinds of health information

00:17:16.319 --> 00:17:20.940
that DeepMind use to provide
their services that are used

00:17:20.940 --> 00:17:25.530
by clinicians to make better
decisions about people's care

00:17:25.530 --> 00:17:26.819
or treatment.

00:17:26.819 --> 00:17:29.010
Verifiable data
audit is essentially

00:17:29.010 --> 00:17:30.960
a log of lots of
information you'd

00:17:30.960 --> 00:17:33.300
need to look back
at to understand

00:17:33.300 --> 00:17:36.300
how did the clinician make
the decision at the time?

00:17:36.300 --> 00:17:40.390
What information was available
to them to make that decision?

00:17:40.390 --> 00:17:43.660
Verifiable data audit is
just one kind of technology

00:17:43.660 --> 00:17:47.560
that could help us verify
complex learning systems.

00:17:47.560 --> 00:17:49.300
But I think there
will be other things

00:17:49.300 --> 00:17:52.945
we need too, like, for instance,
open registries, perhaps,

00:17:52.945 --> 00:17:55.600
of verified models.

00:17:55.600 --> 00:17:58.240
Or maybe we need to start
verifying the training data

00:17:58.240 --> 00:17:59.710
that we use in the first place.

00:18:03.370 --> 00:18:05.820
So how do we publish
that other data?

00:18:05.820 --> 00:18:07.860
How do we verify
the authenticity

00:18:07.860 --> 00:18:09.660
of different training models?

00:18:09.660 --> 00:18:11.430
And then, what
training is needed

00:18:11.430 --> 00:18:13.470
for the individuals
who will be running

00:18:13.470 --> 00:18:16.110
those testing institutions
or organizations

00:18:16.110 --> 00:18:17.790
in the first place?

00:18:17.790 --> 00:18:19.470
And something I'm
really interested in

00:18:19.470 --> 00:18:22.720
is how do we give testing
suites to individuals

00:18:22.720 --> 00:18:25.560
so they don't just have to
run on our organizations,

00:18:25.560 --> 00:18:27.260
but they can self
test things too.

00:18:30.960 --> 00:18:33.106
The last area I
want to talk about

00:18:33.106 --> 00:18:34.980
is something that's
really close to my heart.

00:18:34.980 --> 00:18:38.370
And this is ownership and
control in learned systems

00:18:38.370 --> 00:18:42.930
because these systems
don't operate in isolation.

00:18:42.930 --> 00:18:45.850
They exist as part
of a wider system.

00:18:45.850 --> 00:18:48.720
And we've seen with Facebook
and Cambridge Analytica what

00:18:48.720 --> 00:18:52.860
happens if the wrong parts of a
learned system and the platform

00:18:52.860 --> 00:18:58.140
it operates on is closed,
obfuscated, illegible,

00:18:58.140 --> 00:19:01.290
or has perverse
incentives baked into it.

00:19:04.030 --> 00:19:07.380
So when you use a service
that learns from your behavior

00:19:07.380 --> 00:19:09.190
and makes decisions
on your behalf,

00:19:09.190 --> 00:19:12.315
whether that be about deciding
what brand of loo paper to buy

00:19:12.315 --> 00:19:15.420
or maybe which school
to send your child to,

00:19:15.420 --> 00:19:17.790
how can you know
that that system is

00:19:17.790 --> 00:19:21.726
working in your best interests?

00:19:21.726 --> 00:19:25.140
Then as those decisions are
made, not just about you

00:19:25.140 --> 00:19:27.630
but maybe about
your community, how

00:19:27.630 --> 00:19:32.820
can you know that that system
understands right from wrong?

00:19:32.820 --> 00:19:35.490
What is right, anyway?

00:19:35.490 --> 00:19:38.100
As humans, we have morality.

00:19:38.100 --> 00:19:40.140
But when we're talking
about, essentially, what

00:19:40.140 --> 00:19:44.670
is a bunch of numbers, how
do we make sure that that can

00:19:44.670 --> 00:19:48.720
understand or know what's best?

00:19:48.720 --> 00:19:52.240
When you use a doctor or
an engineer or a lawyer,

00:19:52.240 --> 00:19:57.420
they have a professional code of
ethics that they must work to.

00:19:57.420 --> 00:20:00.090
So how can we make the
intent of a learned system

00:20:00.090 --> 00:20:02.670
clear to a user?

00:20:02.670 --> 00:20:06.180
And where do you think a
system's ethical premise should

00:20:06.180 --> 00:20:06.860
be logged?

00:20:11.500 --> 00:20:13.120
The other question
I have is which

00:20:13.120 --> 00:20:17.230
parts of a learned system and
the platform it operates on

00:20:17.230 --> 00:20:19.420
can be of the market?

00:20:19.420 --> 00:20:21.910
Which parts of the learned
system and the platform

00:20:21.910 --> 00:20:27.400
it operates on has to be open,
co-operative, commonly owned,

00:20:27.400 --> 00:20:29.980
and not part of the market?

00:20:29.980 --> 00:20:32.110
That's really important
because if there's not

00:20:32.110 --> 00:20:36.070
enough transparency baked into
the way we design and build

00:20:36.070 --> 00:20:41.930
these systems, how can
society ever course correct?

00:20:41.930 --> 00:20:44.630
You'll remember the
financial crisis.

00:20:44.630 --> 00:20:47.390
That was a systems
failure of huge proportion

00:20:47.390 --> 00:20:50.180
that affected everyone
around the world.

00:20:50.180 --> 00:20:52.910
But it most affected
those individuals

00:20:52.910 --> 00:20:54.855
who have least most severely.

00:20:58.130 --> 00:21:01.430
I think these areas are
the most pertinent issues

00:21:01.430 --> 00:21:05.870
to be investigating in design
because what you design

00:21:05.870 --> 00:21:09.170
affects the
understanding and access

00:21:09.170 --> 00:21:11.900
that an individual
has to their rights

00:21:11.900 --> 00:21:16.850
and their ability to
have action on them.

00:21:16.850 --> 00:21:19.310
It is our responsibility
to make sure

00:21:19.310 --> 00:21:23.480
that we design learned systems
so that they can be trusted

00:21:23.480 --> 00:21:26.832
and that they give
people digital rights.

00:21:26.832 --> 00:21:27.780
Thank you.

00:21:27.780 --> 00:21:36.127
[APPLAUSE]

00:21:36.127 --> 00:21:37.035
SPEAKER 2: Questions?

00:21:51.930 --> 00:21:54.600
AUDIENCE: I share, probably,
most people's intimidation

00:21:54.600 --> 00:21:58.530
of legal terminology
in contracts and stuff.

00:21:58.530 --> 00:22:03.570
And it strikes me that
perhaps there are,

00:22:03.570 --> 00:22:09.164
as you say, something
that can be parsed or made

00:22:09.164 --> 00:22:10.080
simpler to understand.

00:22:10.080 --> 00:22:13.170
But at the same time,
a big part of what

00:22:13.170 --> 00:22:15.960
I think makes them complex
is that all of that language

00:22:15.960 --> 00:22:17.510
is there for specificity.

00:22:17.510 --> 00:22:19.690
It's details, basically, right?

00:22:19.690 --> 00:22:22.350
And so in a certain
way, it strikes me

00:22:22.350 --> 00:22:23.777
that it's kind of
like saying, why

00:22:23.777 --> 00:22:25.860
do we need all this code
in this computer program?

00:22:25.860 --> 00:22:29.927
Why can't it just do the thing
that it's supposed to do?

00:22:29.927 --> 00:22:31.510
I don't know if
that's a fair analogy.

00:22:31.510 --> 00:22:35.760
So my question is,
basically, can you

00:22:35.760 --> 00:22:39.780
elaborate a little bit on how
to kind of bubble that up?

00:22:39.780 --> 00:22:42.090
And I guess the
final point on this

00:22:42.090 --> 00:22:46.260
is that it seems like often
the thing that screws a person

00:22:46.260 --> 00:22:50.130
with legal language
is not the top level

00:22:50.130 --> 00:22:52.800
point but the thing that's
kind of buried deep in there.

00:23:00.487 --> 00:23:01.945
SARAH GOLD: So I
think the question

00:23:01.945 --> 00:23:07.150
was about the relevance of
terms of service to a designer.

00:23:07.150 --> 00:23:10.660
And then the second part
was what are the ways

00:23:10.660 --> 00:23:14.336
to design out terms of service?

00:23:14.336 --> 00:23:15.560
Is that kind of right?

00:23:22.200 --> 00:23:23.080
AUDIENCE: I'm sorry.

00:23:23.080 --> 00:23:26.500
My question was basically
how do you envision

00:23:26.500 --> 00:23:31.510
clarifying this kind of
language or this kind of system

00:23:31.510 --> 00:23:39.370
to laypeople without losing the
details in those documents that

00:23:39.370 --> 00:23:41.020
are sort of crucial to--

00:23:41.020 --> 00:23:42.770
SARAH GOLD: OK, I see.

00:23:42.770 --> 00:23:48.070
So I think that having terms
of service that sometimes you

00:23:48.070 --> 00:23:51.170
can't even find very easily
on a service, I mean,

00:23:51.170 --> 00:23:52.420
that just doesn't work at all.

00:23:52.420 --> 00:23:54.878
Because even if you wanted to
try and go back to understand

00:23:54.878 --> 00:23:55.930
something, you can't.

00:23:55.930 --> 00:23:57.471
Or you're not even
sure which version

00:23:57.471 --> 00:24:00.050
it was that you saw when you
signed up to the service.

00:24:00.050 --> 00:24:01.960
So I think that there are--

00:24:01.960 --> 00:24:05.230
it's really important that
an individual understands

00:24:05.230 --> 00:24:07.930
what information is being
collected, when a service is

00:24:07.930 --> 00:24:13.330
learning, what that data is used
to do, all the details of it,

00:24:13.330 --> 00:24:16.150
actually, when it's
relevant to them.

00:24:16.150 --> 00:24:17.950
So I think as you go
through a service,

00:24:17.950 --> 00:24:19.720
I think there are
points of asking

00:24:19.720 --> 00:24:25.420
for consent as a service is
running that is a good idea as

00:24:25.420 --> 00:24:27.140
opposed to asking
for all the consent

00:24:27.140 --> 00:24:29.470
up front, which is
what currently happens.

00:24:29.470 --> 00:24:30.970
So I think getting
smarter about how

00:24:30.970 --> 00:24:33.940
we can bring in consent
throughout a service.

00:24:33.940 --> 00:24:36.730
But we have to do that
with a lot of research

00:24:36.730 --> 00:24:40.270
because I think a
speaker earlier mentioned

00:24:40.270 --> 00:24:41.500
about getting in the way.

00:24:41.500 --> 00:24:43.000
And that's often
part of the tension

00:24:43.000 --> 00:24:45.940
here is you need to add
friction points to help

00:24:45.940 --> 00:24:46.780
someone understand.

00:24:46.780 --> 00:24:50.230
But at the same time, someone
may just fundamentally not

00:24:50.230 --> 00:24:53.426
understand why that could
be relevant to them.

00:24:53.426 --> 00:24:55.050
And I think that's
part of the problem,

00:24:55.050 --> 00:24:58.330
actually, is that with learned
systems the quantity of data

00:24:58.330 --> 00:25:01.340
that is being
extracted is so huge,

00:25:01.340 --> 00:25:04.000
I think it's going to be almost
impossible for the majority

00:25:04.000 --> 00:25:05.650
of people to understand that.

00:25:05.650 --> 00:25:11.980
So I think that looking at how
the issues of terms of service

00:25:11.980 --> 00:25:15.082
gets designed across the service
is one way to look at it.

00:25:15.082 --> 00:25:17.290
But I think, ultimately,
what we should be looking at

00:25:17.290 --> 00:25:19.960
and, Tom Steinberg blogged
about this recently,

00:25:19.960 --> 00:25:22.660
is what are the kind of
collection action points

00:25:22.660 --> 00:25:24.320
to understand this stuff?

00:25:24.320 --> 00:25:26.790
You just want to be
able to use a service,

00:25:26.790 --> 00:25:29.440
to receive the utility
it provides you,

00:25:29.440 --> 00:25:31.000
and not have to worry.

00:25:31.000 --> 00:25:34.550
And the majority of people
just don't have time to check.

00:25:34.550 --> 00:25:36.130
So I think we have
to look to that.

00:25:36.130 --> 00:25:38.560
But also I think there's a
question about how machines

00:25:38.560 --> 00:25:42.160
can also help us check
because of the quantities

00:25:42.160 --> 00:25:45.680
of information we're looking at.

00:25:45.680 --> 00:25:49.310
AUDIENCE: So to extend this
notion of legal systems

00:25:49.310 --> 00:25:52.640
as code or code as legal
systems, as you well know,

00:25:52.640 --> 00:25:55.940
regulatory environments and
laws are often in conflict

00:25:55.940 --> 00:25:57.750
within a nation.

00:25:57.750 --> 00:26:00.650
And so for example, a program
would crash at that point

00:26:00.650 --> 00:26:02.360
through a memory error.

00:26:02.360 --> 00:26:04.250
But legal systems
keep churning on.

00:26:04.250 --> 00:26:06.260
And of course, a lot
of lawyer activity

00:26:06.260 --> 00:26:08.600
is applying the
correct regulation

00:26:08.600 --> 00:26:10.820
for the correct
circumstance and ignoring

00:26:10.820 --> 00:26:12.840
that one that's in conflict.

00:26:12.840 --> 00:26:15.740
Now, of course, we are
developing technology

00:26:15.740 --> 00:26:18.440
that has to live in an
international environment

00:26:18.440 --> 00:26:19.910
where it's not
enough to just have

00:26:19.910 --> 00:26:23.030
consistent laws within a
nation, but we, the technology

00:26:23.030 --> 00:26:24.680
providers and
builders, have to build

00:26:24.680 --> 00:26:26.255
things that are
consistent across

00:26:26.255 --> 00:26:28.025
international boundaries.

00:26:28.025 --> 00:26:30.805
Copyright, for example, varies
tremendously from country

00:26:30.805 --> 00:26:31.320
to country.

00:26:31.320 --> 00:26:36.420
Australia regulations are very
different than US and so on.

00:26:36.420 --> 00:26:38.160
Tell us what we
should be thinking

00:26:38.160 --> 00:26:41.010
about when one country
demands that you delete

00:26:41.010 --> 00:26:44.836
this data and another country
demands you maintain that data.

00:26:44.836 --> 00:26:46.480
Suggestions?

00:26:46.480 --> 00:26:49.930
SARAH GOLD: Well, so this is
an area that I'm really, really

00:26:49.930 --> 00:26:50.650
interested in.

00:26:50.650 --> 00:26:52.540
And I think that
technology companies--

00:26:52.540 --> 00:26:56.090
I mean, there's a question
about your experiences

00:26:56.090 --> 00:26:57.550
as you go from
country to country,

00:26:57.550 --> 00:26:59.484
or as I've come here,
the fact that there

00:26:59.484 --> 00:27:01.900
was at that point in time where
there was a lot of concern

00:27:01.900 --> 00:27:04.060
that at an airport
you could be asked

00:27:04.060 --> 00:27:05.730
to give over your passwords.

00:27:05.730 --> 00:27:07.960
And there was no way of
me going dark on my phone,

00:27:07.960 --> 00:27:09.415
so to speak, right?

00:27:09.415 --> 00:27:14.020
Maybe that should have been an
affordance that an organization

00:27:14.020 --> 00:27:16.074
should have given me.

00:27:16.074 --> 00:27:17.740
So this is a question
that I'm not going

00:27:17.740 --> 00:27:19.460
to say I have an answer to.

00:27:19.460 --> 00:27:21.790
But I think it's something
where we need to be

00:27:21.790 --> 00:27:25.060
designing with the legal teams.

00:27:25.060 --> 00:27:27.670
I think that when you see
design and development teams,

00:27:27.670 --> 00:27:30.540
they're often in completely
different buildings to where

00:27:30.540 --> 00:27:31.240
legal are.

00:27:31.240 --> 00:27:33.490
And so I think far
more collaboration

00:27:33.490 --> 00:27:37.115
between those groups to
understand these issues.

00:27:37.115 --> 00:27:39.570
AUDIENCE: Although there
is a bug with that, right?

00:27:39.570 --> 00:27:42.130
The legal universe
that I deal with

00:27:42.130 --> 00:27:45.340
is way behind the technology
that we're developing.

00:27:45.340 --> 00:27:48.670
I don't know how many times I've
had to explain how HTML works

00:27:48.670 --> 00:27:49.960
and how a web browser works.

00:27:49.960 --> 00:27:52.860
It actually makes a copy
on your local device.

00:27:52.860 --> 00:27:55.210
That's a crazy hard notion
for lawyers to understand.

00:27:55.210 --> 00:27:56.710
SARAH GOLD: But I
think that's where

00:27:56.710 --> 00:27:59.740
we have to look at this as
being a cultural question.

00:27:59.740 --> 00:28:02.710
So when you look at
programs like Oxfam's Data

00:28:02.710 --> 00:28:04.690
Responsibility Program
that they brought

00:28:04.690 --> 00:28:07.210
in with the engine
[INAUDIBLE] that was something

00:28:07.210 --> 00:28:11.680
that wasn't about more design.

00:28:11.680 --> 00:28:14.965
It was actually about
facilitating workshops

00:28:14.965 --> 00:28:17.740
to bring different parts of
the organization together

00:28:17.740 --> 00:28:21.625
to discuss really knotty
issues of things like consent

00:28:21.625 --> 00:28:23.230
in somewhere like Yemen.

00:28:23.230 --> 00:28:25.857
So I think it has to come from
a place of bringing people

00:28:25.857 --> 00:28:27.690
together that have not
been together before.

00:28:27.690 --> 00:28:29.080
And as designers,
remember, we're

00:28:29.080 --> 00:28:30.640
really good at doing that.

00:28:30.640 --> 00:28:32.600
You know, we are facilitators.

00:28:32.600 --> 00:28:35.380
So I think that
has to come first.

00:28:35.380 --> 00:28:36.985
But I'd love to do
more work on that.

00:28:36.985 --> 00:28:38.860
And I'd love to talk to
you after about that.

00:28:38.860 --> 00:28:41.068
I think it's not a question
I can answer super quick.

00:28:43.362 --> 00:28:43.945
AUDIENCE: Yes.

00:28:43.945 --> 00:28:46.120
I'm from DeepMind
Ethics and Society.

00:28:46.120 --> 00:28:48.130
So I was very
interested in your idea

00:28:48.130 --> 00:28:49.810
that certain parts
of the system should

00:28:49.810 --> 00:28:52.600
operate outside of the market.

00:28:52.600 --> 00:28:54.580
And I was wondering if
you could flesh that out

00:28:54.580 --> 00:28:56.435
for us a little bit more?

00:28:56.435 --> 00:28:58.810
SARAH GOLD: Yeah, I think this
is one of the big research

00:28:58.810 --> 00:29:02.620
questions that, actually,
we need to explore and needs

00:29:02.620 --> 00:29:04.960
steady academic research,
as well, so that this

00:29:04.960 --> 00:29:06.880
can be in the open
because I think

00:29:06.880 --> 00:29:09.560
it's a really important one.

00:29:09.560 --> 00:29:14.000
I think that it's all
about incentives, I think.

00:29:14.000 --> 00:29:17.242
So if you have all
parts of learned systems

00:29:17.242 --> 00:29:18.700
and the platforms
that they operate

00:29:18.700 --> 00:29:23.080
on built to a model
of capitalism,

00:29:23.080 --> 00:29:26.410
we know what happens
with that model.

00:29:26.410 --> 00:29:29.207
And we also know what
happens with parts

00:29:29.207 --> 00:29:31.540
of the infrastructure that
are critical to running that,

00:29:31.540 --> 00:29:35.320
the open-source infrastructure,
that may not receive

00:29:35.320 --> 00:29:38.260
any funding or has
security issues in it

00:29:38.260 --> 00:29:40.169
because it's not well resourced.

00:29:40.169 --> 00:29:41.710
We're going to see
those issues again

00:29:41.710 --> 00:29:44.980
but at a much, much bigger
scale with learned systems.

00:29:44.980 --> 00:29:47.440
I think we have to learn
from what we know and think

00:29:47.440 --> 00:29:50.720
about which parts of
these systems have to be,

00:29:50.720 --> 00:29:52.950
what I would call,
of the commons.

00:29:52.950 --> 00:29:55.720
And I hope that Dan Hill, later,
might speak a little bit more

00:29:55.720 --> 00:29:59.050
to some of these
issues of ownership.

00:29:59.050 --> 00:30:01.480
And that's really important
because when something

00:30:01.480 --> 00:30:04.030
is of the commons or
cooperatively owned,

00:30:04.030 --> 00:30:07.900
there are rules that you set
as a group of individuals

00:30:07.900 --> 00:30:10.120
that you all get to decide on.

00:30:10.120 --> 00:30:12.790
So it's really going down
to fundamentals of power

00:30:12.790 --> 00:30:14.110
in these systems.

00:30:14.110 --> 00:30:17.950
And by power, I mean what kind
of rights and capabilities

00:30:17.950 --> 00:30:21.445
these systems give
the majority of us.

00:30:21.445 --> 00:30:23.570
And I think that's incredibly
important to look at.

00:30:23.570 --> 00:30:25.870
And it's something we
have to see, particularly

00:30:25.870 --> 00:30:28.330
at the speed of software change.

00:30:28.330 --> 00:30:30.160
And we just won't
be able to keep up.

00:30:30.160 --> 00:30:31.170
We really won't.

00:30:31.170 --> 00:30:34.600
And I think there is a real risk
that we will see huge systems

00:30:34.600 --> 00:30:36.340
failure again and
again and again

00:30:36.340 --> 00:30:39.520
if we don't give society
the ability to see ahead

00:30:39.520 --> 00:30:40.600
and course correct.

00:30:40.600 --> 00:30:42.190
That's only going
to be possible if we

00:30:42.190 --> 00:30:46.120
make the important parts of
these platforms and systems

00:30:46.120 --> 00:30:48.339
open.

00:30:48.339 --> 00:30:49.130
That's what I mean.

00:30:52.420 --> 00:30:55.960
AUDIENCE: Mine is more of a
comment to a conversation I'd

00:30:55.960 --> 00:30:56.830
like to provoke.

00:30:56.830 --> 00:30:59.890
I think I actually see a
really connective thread

00:30:59.890 --> 00:31:01.920
through the three talks
that we've heard today.

00:31:01.920 --> 00:31:06.490
And I would characterize
it as the reemergence

00:31:06.490 --> 00:31:10.220
of ego or self-perception
in UX and UI.

00:31:10.220 --> 00:31:12.940
So a lot of the threads
especially with work,

00:31:12.940 --> 00:31:15.130
especially about what it
means to be user-friendly,

00:31:15.130 --> 00:31:20.020
especially what it means to have
a consent where for a long time

00:31:20.020 --> 00:31:21.610
there was a lot of
incentive alignment

00:31:21.610 --> 00:31:23.990
between the technology, the
UX, and the business model,

00:31:23.990 --> 00:31:27.490
which is essentially to
obfuscate all of this, right?

00:31:27.490 --> 00:31:29.560
We all get the same UI
no matter who we are.

00:31:29.560 --> 00:31:31.510
We all click on the things.

00:31:31.510 --> 00:31:33.010
And for that, we
get things that are

00:31:33.010 --> 00:31:35.380
really fast, that get
cheaper over time,

00:31:35.380 --> 00:31:37.900
that I don't actually have
to give as much information

00:31:37.900 --> 00:31:40.037
to use.

00:31:40.037 --> 00:31:41.620
And what we're seeing
now is, it seems

00:31:41.620 --> 00:31:43.190
like, the role of identity.

00:31:43.190 --> 00:31:44.770
So if you go to
the doctors, a lot

00:31:44.770 --> 00:31:47.020
of times the reason the
system's getting rejected

00:31:47.020 --> 00:31:48.478
is because you're
trying to replace

00:31:48.478 --> 00:31:51.790
part of what they consider
into their core identity.

00:31:51.790 --> 00:31:54.179
For me to have representation
in the systems,

00:31:54.179 --> 00:31:55.720
I have to give some
kind of feedback.

00:31:55.720 --> 00:31:59.140
I really liked your comments
about one of the things

00:31:59.140 --> 00:32:00.850
is when you have
a certain result

00:32:00.850 --> 00:32:03.100
and I have a certain
result as a UX designer,

00:32:03.100 --> 00:32:05.320
we need to design for us
to be able to compare them.

00:32:05.320 --> 00:32:07.010
And what is that
experience like?

00:32:07.010 --> 00:32:10.930
That's also moving a lot
of work that was obfuscated

00:32:10.930 --> 00:32:14.200
and was taken care of by
the technology to people.

00:32:14.200 --> 00:32:19.030
And I think of Cliff's comment
or thinking on user-friendly.

00:32:19.030 --> 00:32:21.010
And user-friendly used
to mean get as much

00:32:21.010 --> 00:32:22.970
of it out of the
way as possible.

00:32:22.970 --> 00:32:24.820
And I think there's
this shift going

00:32:24.820 --> 00:32:28.250
on where people are raising
their hand, perhaps, to say

00:32:28.250 --> 00:32:30.040
I want to participate more.

00:32:30.040 --> 00:32:31.960
And I actually
want more friction

00:32:31.960 --> 00:32:34.790
because of what
I'll get in the end.

00:32:34.790 --> 00:32:37.150
Sorry, I just wanted to
kind of connect those.

00:32:37.150 --> 00:32:39.400
And if anyone wants to
talk about that topic,

00:32:39.400 --> 00:32:40.870
I'd love to talk more about it.

00:32:40.870 --> 00:32:43.630
SARAH GOLD: I think it's one of
the parts where user interface

00:32:43.630 --> 00:32:47.200
design has kind of stayed the
same whilst the technology's

00:32:47.200 --> 00:32:49.970
underneath these services
have radically changed.

00:32:49.970 --> 00:32:51.190
And that's what we're facing.

00:32:51.190 --> 00:32:53.440
Big questions are kind of
centered around that tension

00:32:53.440 --> 00:32:58.730
point, really, that we've not
changed the way we design.

00:32:58.730 --> 00:33:01.380
AUDIENCE: [INAUDIBLE] don't
make me think entitled Make Me

00:33:01.380 --> 00:33:04.114
Think a Lot, All the Time.

00:33:04.114 --> 00:33:06.030
SARAH GOLD: Yeah, I think
that's where there's

00:33:06.030 --> 00:33:09.930
a real tension there, which is
that I guarantee 99% of people

00:33:09.930 --> 00:33:11.890
just won't have the time.

00:33:11.890 --> 00:33:13.740
And so then you get a
new design challenge,

00:33:13.740 --> 00:33:15.300
which is OK, how
do we start to look

00:33:15.300 --> 00:33:17.820
at cooperative models, which
historically are places

00:33:17.820 --> 00:33:21.940
that are really successful at
helping us do stuff together.

00:33:21.940 --> 00:33:25.500
So I think looking to that
as well, which I don't think

00:33:25.500 --> 00:33:29.301
has been part of the UX
language but should be.

00:33:29.301 --> 00:33:30.300
SPEAKER 1: Thanks a lot.

00:33:30.300 --> 00:33:30.840
SARAH GOLD: Thank you.

00:33:30.840 --> 00:33:31.380
SPEAKER 1: Good.

00:33:31.380 --> 00:33:32.190
Time for a break.

00:33:32.190 --> 00:33:35.361
[APPLAUSE]

00:33:35.361 --> 00:33:37.250
SPEAKER 1: Thanks
to all our speakers.

00:33:37.250 --> 00:33:40.606
And we'll start at 11:45 sharp.

00:33:40.606 --> 00:33:41.400
Thank you.

00:33:41.400 --> 00:33:44.420
Snacks are on this side.

