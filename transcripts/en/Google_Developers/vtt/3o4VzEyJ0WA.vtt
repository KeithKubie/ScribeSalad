WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.458
[MUSIC PLAYING]

00:00:05.364 --> 00:00:07.530
LAURENCE MORONEY: So
Christopher, welcome to "Coffee

00:00:07.530 --> 00:00:07.980
with a Googler."

00:00:07.980 --> 00:00:08.720
It's great to have you on.

00:00:08.720 --> 00:00:10.030
CHRISTOPHER OLAH: Thank
you, for having me.

00:00:10.030 --> 00:00:11.070
LAURENCE MORONEY:
So I know you are

00:00:11.070 --> 00:00:12.903
one of the people-- you
work on Google Brain

00:00:12.903 --> 00:00:15.500
and you're behind Distill for
great dissemination of machine

00:00:15.500 --> 00:00:16.000
learning.

00:00:16.000 --> 00:00:17.220
Could you tell us all about it?

00:00:17.220 --> 00:00:18.178
CHRISTOPHER OLAH: Yeah.

00:00:18.178 --> 00:00:21.510
So I guess maybe I'd split
it sort of into two parts.

00:00:21.510 --> 00:00:23.937
So there's the thing
that we're doing,

00:00:23.937 --> 00:00:26.020
and then there's how
Distill's a vehicle for that.

00:00:26.020 --> 00:00:28.870
So often we have this idea of
there being technical debt,

00:00:28.870 --> 00:00:29.370
right?

00:00:29.370 --> 00:00:32.400
You can only write software, and
maybe you don't always use good

00:00:32.400 --> 00:00:34.440
variable names, you don't
document it very well,

00:00:34.440 --> 00:00:35.060
you don't--

00:00:35.060 --> 00:00:37.080
you end up with all of these--

00:00:37.080 --> 00:00:38.882
you know, you don't refactor it.

00:00:38.882 --> 00:00:40.590
And your software
becomes hard for people

00:00:40.590 --> 00:00:42.880
to go and build
on and work with.

00:00:42.880 --> 00:00:44.760
I actually think a
kind of analogous thing

00:00:44.760 --> 00:00:47.610
can happen in research where,
if we don't explain things well,

00:00:47.610 --> 00:00:50.699
if we don't develop the right
ways for thinking about things,

00:00:50.699 --> 00:00:52.740
it can become harder to
go and build on research.

00:00:52.740 --> 00:00:55.470
And it sort of
becomes this mountain

00:00:55.470 --> 00:00:57.539
of papers where
people-- and I think

00:00:57.539 --> 00:00:58.830
people take pride in it, right?

00:00:58.830 --> 00:01:00.870
They're like, you know,
I spent all these years

00:01:00.870 --> 00:01:04.055
going and climbing this
mountain, and I got to the top.

00:01:04.055 --> 00:01:05.430
And I'm going to
do more research

00:01:05.430 --> 00:01:07.309
and build up this mountain.

00:01:07.309 --> 00:01:08.850
But I think, often,
we actually could

00:01:08.850 --> 00:01:10.391
have done a much
better job of making

00:01:10.391 --> 00:01:11.912
the mountain easy to climb.

00:01:11.912 --> 00:01:12.870
LAURENCE MORONEY: Yeah.

00:01:12.870 --> 00:01:14.411
CHRISTOPHER OLAH:
And so what Distill

00:01:14.411 --> 00:01:16.830
is really trying to do is to
create a vehicle where people

00:01:16.830 --> 00:01:19.620
can do that kind of
work, and do really

00:01:19.620 --> 00:01:23.220
amazing work of that kind,
and have it be acknowledged

00:01:23.220 --> 00:01:26.190
as a real contribution
to the community,

00:01:26.190 --> 00:01:29.580
and be the sort of thing that
can then help them get support

00:01:29.580 --> 00:01:31.260
in doing more of that work.

00:01:31.260 --> 00:01:32.220
LAURENCE MORONEY: So it
becomes like a, really,

00:01:32.220 --> 00:01:33.590
a best of both worlds
kind of thing, right?

00:01:33.590 --> 00:01:33.840
CHRISTOPHER OLAH: Yeah.

00:01:33.840 --> 00:01:36.131
LAURENCE MORONEY: So very
accessible to the nonacademic

00:01:36.131 --> 00:01:38.070
community, but it's
also referentiable--

00:01:38.070 --> 00:01:38.904
if that's the word--

00:01:38.904 --> 00:01:40.653
you know, something
that can be referenced

00:01:40.653 --> 00:01:42.960
by the academic community
so you get the best of both.

00:01:42.960 --> 00:01:43.440
CHRISTOPHER OLAH: Yeah.

00:01:43.440 --> 00:01:45.310
Many of these things
that were already just

00:01:45.310 --> 00:01:48.096
a better version of
scientific papers,

00:01:48.096 --> 00:01:49.720
but they weren't
being counted as such.

00:01:49.720 --> 00:01:51.810
And so I think Distill
is a legitimizing

00:01:51.810 --> 00:01:53.932
vehicle that allows--

00:01:53.932 --> 00:01:55.890
and I think we also
provide some help to people

00:01:55.890 --> 00:01:57.270
in doing good work in this way.

00:01:57.270 --> 00:02:00.180
So there's all this sort of
weird, bureaucratic stuff.

00:02:00.180 --> 00:02:02.370
And we just sort of
crossed all our t's, dotted

00:02:02.370 --> 00:02:05.780
all our i's, and set up--

00:02:05.780 --> 00:02:07.680
Distill's also peer
reviewed --and created

00:02:07.680 --> 00:02:10.110
this scientific journal.

00:02:10.110 --> 00:02:12.990
Except it's a scientific journal
full of interactive diagrams

00:02:12.990 --> 00:02:14.520
where we really--

00:02:14.520 --> 00:02:17.250
really, if somebody's writing
in a very academic style,

00:02:17.250 --> 00:02:21.660
we might encourage them to try
to do a little bit less of that

00:02:21.660 --> 00:02:24.330
and subvert what a
paper is, in some sense,

00:02:24.330 --> 00:02:26.850
and I think explore
what's possible.

00:02:26.850 --> 00:02:27.850
LAURENCE MORONEY: Right.

00:02:27.850 --> 00:02:30.058
I mean, I'll talk about from
my personal perspective,

00:02:30.058 --> 00:02:33.514
and that is that I have been
out of academia for a long time.

00:02:33.514 --> 00:02:35.430
And I've been working
in software and software

00:02:35.430 --> 00:02:36.510
development, that kind of stuff.

00:02:36.510 --> 00:02:38.310
But as I've been getting into
machine learning, of course

00:02:38.310 --> 00:02:40.380
I have to start reading
more and more papers.

00:02:40.380 --> 00:02:42.240
And as I'm reading
those papers, as soon

00:02:42.240 --> 00:02:44.260
as I start getting into
mathematical notation,

00:02:44.260 --> 00:02:46.843
and sigmas, and phis, and that
kind of stuff, part of my brain

00:02:46.843 --> 00:02:48.034
begins to go to sleep.

00:02:48.034 --> 00:02:49.700
And it's not that I
don't understand it,

00:02:49.700 --> 00:02:51.420
it's just so alien to me.

00:02:51.420 --> 00:02:52.462
It's so long in the past.

00:02:52.462 --> 00:02:54.003
And you used the
image of a mountain.

00:02:54.003 --> 00:02:56.220
And it's like I used to be
kind of 3/4 of the way up

00:02:56.220 --> 00:02:56.761
the mountain.

00:02:56.761 --> 00:02:58.140
Now, I'm just in the foothills.

00:02:58.140 --> 00:03:00.150
And it's a lot of effort
to get climbing again.

00:03:00.150 --> 00:03:02.064
So with something
like Distill and what

00:03:02.064 --> 00:03:03.480
you've been doing
with Distill has

00:03:03.480 --> 00:03:05.015
been great for
me, personally, so

00:03:05.015 --> 00:03:06.390
that I can start
really beginning

00:03:06.390 --> 00:03:07.945
to grasp some of the concepts.

00:03:07.945 --> 00:03:10.320
And speaking of those concepts,
another thing you've been

00:03:10.320 --> 00:03:12.450
working on is Lucid, right?

00:03:12.450 --> 00:03:14.730
So I know you're using
Distill to disseminate Lucid.

00:03:14.730 --> 00:03:16.860
Could you tell us a
little bit about that?

00:03:16.860 --> 00:03:17.818
CHRISTOPHER OLAH: Yeah.

00:03:17.818 --> 00:03:23.250
So I think a lot of this really
traces back to Deep Dream,

00:03:23.250 --> 00:03:26.584
or maybe even a
little bit further.

00:03:26.584 --> 00:03:29.000
There's a lot of people, but
there's a small cluster of us

00:03:29.000 --> 00:03:30.666
who've working together
who've been very

00:03:30.666 --> 00:03:32.380
interested for a
number of years in know

00:03:32.380 --> 00:03:34.620
how do neural
networks really work

00:03:34.620 --> 00:03:37.530
and how can humans
understand these really

00:03:37.530 --> 00:03:40.290
complicated systems?

00:03:40.290 --> 00:03:43.170
And a couple years ago,
we had these really

00:03:43.170 --> 00:03:44.770
exciting results where we--

00:03:44.770 --> 00:03:46.270
we came up with all
these techniques

00:03:46.270 --> 00:03:49.100
for visualizing, what do neural
networks think are interesting?

00:03:49.100 --> 00:03:50.850
LAURENCE MORONEY: What
do they really see?

00:03:50.850 --> 00:03:52.550
CHRISTOPHER OLAH: Yeah.

00:03:52.550 --> 00:03:54.925
Really, I think the right way
to think about those images

00:03:54.925 --> 00:03:57.450
is they're trying to make
the most interesting image

00:03:57.450 --> 00:03:58.554
for a network as possible.

00:03:58.554 --> 00:04:00.720
And so it gets filled with
all those sorts of things

00:04:00.720 --> 00:04:04.730
the network finds very
interesting in some sense,

00:04:04.730 --> 00:04:06.180
that activates it.

00:04:06.180 --> 00:04:10.990
And so it gets full of dog
slugs and who knows what else.

00:04:10.990 --> 00:04:12.000
And it's a lot of fun.

00:04:12.000 --> 00:04:15.690
And it's even become
this artistic movement.

00:04:15.690 --> 00:04:17.700
There was this event at
the Gray Area Foundation

00:04:17.700 --> 00:04:20.210
where people were
auctioning off art

00:04:20.210 --> 00:04:21.459
that was made with Deep Dream.

00:04:21.459 --> 00:04:22.800
And I was like, whoa.

00:04:22.800 --> 00:04:23.310
LAURENCE MORONEY: This is great.

00:04:23.310 --> 00:04:24.420
CHRISTOPHER OLAH: This
thing that I was involved in

00:04:24.420 --> 00:04:25.410
is like--

00:04:25.410 --> 00:04:27.840
there's real artists who
are doing things, which is--

00:04:27.840 --> 00:04:29.840
LAURENCE MORONEY: I hope
you have some of those.

00:04:29.840 --> 00:04:30.600
[LAUGHTER]

00:04:30.600 --> 00:04:33.280
CHRISTOPHER OLAH: They were all
too expensive for me to buy.

00:04:33.280 --> 00:04:35.790
So Deep Dream, I think
part of what's exciting

00:04:35.790 --> 00:04:38.460
about it is all this art.

00:04:38.460 --> 00:04:42.240
And I think we were
really excited or really

00:04:42.240 --> 00:04:45.930
lucky to see this-- and there's
all these amazing videos.

00:04:45.930 --> 00:04:50.430
But in addition to
that, I think we--

00:04:50.430 --> 00:04:53.820
it was also really an
attempt to understand what's

00:04:53.820 --> 00:04:55.100
going on inside neural nets.

00:04:55.100 --> 00:04:55.480
LAURENCE MORONEY: Right.

00:04:55.480 --> 00:04:57.450
CHRISTOPHER OLAH: And
it didn't stop there.

00:04:57.450 --> 00:05:01.350
We continued thinking
about this a lot.

00:05:01.350 --> 00:05:06.470
And then about a year ago,
or like a couple months ago,

00:05:06.470 --> 00:05:08.660
we published this
article on feature

00:05:08.660 --> 00:05:11.002
visualization, which
is really exploring,

00:05:11.002 --> 00:05:13.460
how can we take these techniques
that we started developing

00:05:13.460 --> 00:05:18.620
with Deep Dream and then go and
turn them into tools that are--

00:05:18.620 --> 00:05:20.720
instead of just trying to
make interesting images,

00:05:20.720 --> 00:05:22.780
really understand what
individual neurons

00:05:22.780 --> 00:05:24.530
in the network are looking for.

00:05:24.530 --> 00:05:29.610
And we discovered in early
layers, it's looking for edges.

00:05:29.610 --> 00:05:33.360
And then it starts
to look for textures.

00:05:33.360 --> 00:05:35.280
And then it uses the
textures to describe

00:05:35.280 --> 00:05:38.660
simple patterns, like stripes,
and fluffy balls, and logos,

00:05:38.660 --> 00:05:41.930
and hexagons, and clusters
of circles, and these--

00:05:41.930 --> 00:05:44.660
I don't even know what-- like
putty, like sphere-like things.

00:05:44.660 --> 00:05:47.480
And then those get turned in
to simple parts of objects,

00:05:47.480 --> 00:05:49.610
like a button
detector, or a flower

00:05:49.610 --> 00:05:51.830
detector, or a patches-of-fluff
detector, or a bubble

00:05:51.830 --> 00:05:54.760
detector, or a chain
detector, or a nose detector.

00:05:54.760 --> 00:05:59.270
And those get turned
into partial objects,

00:05:59.270 --> 00:06:01.610
like parts of buildings,
or people walking around,

00:06:01.610 --> 00:06:05.420
or helmets, or dogs with
floppy ears, and little

00:06:05.420 --> 00:06:06.291
insect-like things.

00:06:06.291 --> 00:06:07.290
LAURENCE MORONEY: Right.

00:06:07.290 --> 00:06:10.340
CHRISTOPHER OLAH: And so
that was a really exciting

00:06:10.340 --> 00:06:11.240
transition.

00:06:11.240 --> 00:06:14.030
And it was the result
of us fiddling a lot

00:06:14.030 --> 00:06:15.690
with ideas around Deep Dream.

00:06:15.690 --> 00:06:17.306
And not just us, you know?

00:06:17.306 --> 00:06:18.680
There's a big
community of people

00:06:18.680 --> 00:06:21.680
who've been doing really
exciting work in this area.

00:06:21.680 --> 00:06:23.840
And we have been
building on that

00:06:23.840 --> 00:06:26.540
and building up infrastructure.

00:06:26.540 --> 00:06:29.330
Really, that to us is
just a means to an end.

00:06:29.330 --> 00:06:31.910
What that work was giving us
was it was giving us a way

00:06:31.910 --> 00:06:34.040
to go and understand, what
are individual neurons

00:06:34.040 --> 00:06:35.784
doing, the individual
neurons in the network,

00:06:35.784 --> 00:06:38.242
so those individual neurons
that seem to detect floppy ears

00:06:38.242 --> 00:06:39.620
and things like this?

00:06:39.620 --> 00:06:42.200
But then, what we've
been doing lately

00:06:42.200 --> 00:06:44.480
is we've been going and
taking that a step further

00:06:44.480 --> 00:06:47.390
and going and saying,
how can we then use

00:06:47.390 --> 00:06:50.690
that as a building block in
conjunction with other things

00:06:50.690 --> 00:06:54.550
to try to really explore how
the network makes decisions?

00:06:54.550 --> 00:06:57.750
And so I'll start
with a simple example.

00:06:57.750 --> 00:07:00.470
So over here, we have an image--

00:07:00.470 --> 00:07:02.810
right now we're looking at
an image of this Labrador

00:07:02.810 --> 00:07:05.720
Retriever and this tiger cat.

00:07:05.720 --> 00:07:08.120
That's actually apparently,
a species of cat.

00:07:08.120 --> 00:07:09.000
LAURENCE MORONEY: They
call them tiger cats?

00:07:09.000 --> 00:07:09.290
CHRISTOPHER OLAH: Yeah.

00:07:09.290 --> 00:07:10.730
I mean, there's real
tigers, and then

00:07:10.730 --> 00:07:12.620
there's also tiger cats,
which I guess maybe they have

00:07:12.620 --> 00:07:13.730
stripes on them or something.

00:07:13.730 --> 00:07:15.070
But there's also, you know,
like you have tabby cats,

00:07:15.070 --> 00:07:15.770
you have tiger cats.

00:07:15.770 --> 00:07:17.061
Apparently, that's a tiger cat.

00:07:17.061 --> 00:07:18.045
I wouldn't have known.

00:07:18.045 --> 00:07:19.670
LAURENCE MORONEY:
But the network does.

00:07:19.670 --> 00:07:20.280
CHRISTOPHER OLAH:
But the network does.

00:07:20.280 --> 00:07:21.260
It's really amazing.

00:07:21.260 --> 00:07:23.301
Actually, I wouldn't have
known it was a Labrador

00:07:23.301 --> 00:07:24.810
Retriever either.

00:07:24.810 --> 00:07:28.340
The network-- really,
it's pretty phenomenal.

00:07:28.340 --> 00:07:31.100
It classifies dogs into
more than 100 species.

00:07:31.100 --> 00:07:33.950
And I don't even know the
names of 100 species of dogs,

00:07:33.950 --> 00:07:36.300
let alone, how to go
and tell them apart.

00:07:36.300 --> 00:07:37.220
But the network can.

00:07:37.220 --> 00:07:39.080
And one thing you
might wonder is,

00:07:39.080 --> 00:07:40.301
how does the network do that?

00:07:40.301 --> 00:07:41.300
LAURENCE MORONEY: Right.

00:07:41.300 --> 00:07:44.000
CHRISTOPHER OLAH: And one really
exciting thing that we'll see

00:07:44.000 --> 00:07:44.525
is--

00:07:44.525 --> 00:07:46.400
well, let's just dive
into this for a second.

00:07:46.400 --> 00:07:47.359
LAURENCE MORONEY: Sure.

00:07:47.359 --> 00:07:49.066
CHRISTOPHER OLAH: This
interface here, it

00:07:49.066 --> 00:07:50.842
allows us to go and
look at the vectors.

00:07:50.842 --> 00:07:52.550
So the neural net runs
the seam detectors

00:07:52.550 --> 00:07:54.720
at every position in the image.

00:07:54.720 --> 00:07:56.750
And normally, those
detectors, they all

00:07:56.750 --> 00:07:58.557
give a number output
that sort of describes

00:07:58.557 --> 00:07:59.390
how much they fired.

00:07:59.390 --> 00:08:02.110
And normally, you get this
so-called activation vector.

00:08:02.110 --> 00:08:03.650
And it's just a list of numbers.

00:08:03.650 --> 00:08:05.090
And it's really inscrutable.

00:08:05.090 --> 00:08:08.280
You're like, neuron
53 was firing a lot.

00:08:08.280 --> 00:08:09.572
Well, what does that mean?

00:08:09.572 --> 00:08:10.780
That's not very useful to me.

00:08:10.780 --> 00:08:12.488
That doesn't tell me
very much, you know?

00:08:12.488 --> 00:08:14.710
Neuron 134 also fired a bunch.

00:08:14.710 --> 00:08:17.470
But neuron 12, it didn't fire.

00:08:17.470 --> 00:08:18.750
Well, great.

00:08:18.750 --> 00:08:19.550
Thanks.

00:08:19.550 --> 00:08:20.430
That doesn't tell me anything.

00:08:20.430 --> 00:08:21.865
LAURENCE MORONEY: Neuron
12 was having a bad day.

00:08:21.865 --> 00:08:23.420
CHRISTOPHER OLAH: [LAUGHS] Yep.

00:08:23.420 --> 00:08:26.690
But what we can do
is we can combine it

00:08:26.690 --> 00:08:30.110
with these feature
visualizations that

00:08:30.110 --> 00:08:33.862
gives us a sense of what
the neurons are looking for.

00:08:33.862 --> 00:08:35.570
And we can combine--
we can create this--

00:08:35.570 --> 00:08:37.236
we call them semantic
dictionaries where

00:08:37.236 --> 00:08:40.340
we have these things
that sort of give us

00:08:40.340 --> 00:08:41.320
a name for the neuron.

00:08:41.320 --> 00:08:43.022
They give us sort
of a visual symbol

00:08:43.022 --> 00:08:44.480
that describes what
they're looking

00:08:44.480 --> 00:08:46.130
for with how much they fire.

00:08:46.130 --> 00:08:47.670
And that's a lot
more informative.

00:08:47.670 --> 00:08:49.760
So if we look over
here, and all over here,

00:08:49.760 --> 00:08:52.490
we see that there's these
floppy ear detector neurons that

00:08:52.490 --> 00:08:54.770
are firing really intensely.

00:08:54.770 --> 00:08:57.950
It turns out that this
neural net, GoogLeNet,

00:08:57.950 --> 00:09:00.740
has a really rich vocabulary
of different kinds of ears.

00:09:00.740 --> 00:09:03.198
And that's a really big part
of how it tells apart animals.

00:09:03.198 --> 00:09:04.970
So it has different
kinds of floppy ears,

00:09:04.970 --> 00:09:06.020
not just floppy
ears, but there's

00:09:06.020 --> 00:09:07.590
these longer floppy
ears, there's

00:09:07.590 --> 00:09:08.944
these mid-length floppy ears.

00:09:08.944 --> 00:09:10.610
And then it has all
sorts of pointy ears

00:09:10.610 --> 00:09:13.910
as well, and ears
that are sort of

00:09:13.910 --> 00:09:15.932
in between pointy and floppy.

00:09:15.932 --> 00:09:18.390
Here, we're seeing that there's
really these two floppy ear

00:09:18.390 --> 00:09:21.122
detectors that are
firing pretty strongly.

00:09:21.122 --> 00:09:22.580
That's really
interesting, but it's

00:09:22.580 --> 00:09:24.980
something that would have
been completely opaque to me

00:09:24.980 --> 00:09:27.810
without going and combining
these techniques together.

00:09:27.810 --> 00:09:28.810
LAURENCE MORONEY: Right.

00:09:28.810 --> 00:09:30.320
CHRISTOPHER OLAH: And so that's
the first thing we're doing.

00:09:30.320 --> 00:09:31.140
And we can look in other places.

00:09:31.140 --> 00:09:32.681
So there's a snout
detector over here

00:09:32.681 --> 00:09:34.790
that really is firing
pretty strongly.

00:09:34.790 --> 00:09:37.520
And if we go over
here, now we're

00:09:37.520 --> 00:09:40.430
seeing some pointy ear detectors
and a cat face-like detector.

00:09:40.430 --> 00:09:43.060
And it almost looks a little
bit like an ape detector that's

00:09:43.060 --> 00:09:44.780
firing a little bit over here.

00:09:44.780 --> 00:09:48.020
If we go down here, we see these
fur detectors and these leg

00:09:48.020 --> 00:09:49.520
detectors that are firing.

00:09:49.520 --> 00:09:50.980
If you go down
here to the grass,

00:09:50.980 --> 00:09:52.100
there's some grass detectors.

00:09:52.100 --> 00:09:53.600
But they don't fire very
strongly, because that's not

00:09:53.600 --> 00:09:54.547
very interesting.

00:09:54.547 --> 00:09:55.880
And we can look at other images.

00:09:55.880 --> 00:09:58.400
So over here, we
have this fellow

00:09:58.400 --> 00:10:03.220
with sunglasses and a bow tie.

00:10:03.220 --> 00:10:06.047
And it turns out there's
a bow tie detector.

00:10:06.047 --> 00:10:08.380
And there's also a neck without
a bow tie detector and--

00:10:08.380 --> 00:10:09.610
LAURENCE MORONEY: What I
really liked about this one

00:10:09.610 --> 00:10:12.240
was that the bow tie and
sunglasses look similar.

00:10:12.240 --> 00:10:12.740
Right?

00:10:12.740 --> 00:10:14.590
It's two dark oblongs
joined by a small bridge.

00:10:14.590 --> 00:10:14.850
CHRISTOPHER OLAH: Oh, yeah.

00:10:14.850 --> 00:10:15.910
Wow.

00:10:15.910 --> 00:10:16.720
LAURENCE MORONEY:
But this was actually

00:10:16.720 --> 00:10:18.226
able to detect the
difference between a bow

00:10:18.226 --> 00:10:18.910
tie and sunglasses.

00:10:18.910 --> 00:10:19.076
CHRISTOPHER OLAH: Oh, yeah.

00:10:19.076 --> 00:10:21.117
No, it's really-- you
know, you can see here.

00:10:21.117 --> 00:10:22.950
And you can see that
part of what it's doing

00:10:22.950 --> 00:10:28.510
is it's looking for the
chin above the bow tie.

00:10:28.510 --> 00:10:31.810
And I think, also, for the
shirt with buttons a little bit

00:10:31.810 --> 00:10:32.310
below it.

00:10:32.310 --> 00:10:34.226
So it's these other
things that are cueing it.

00:10:34.226 --> 00:10:36.400
But it actually has a
pretty sophisticated sense

00:10:36.400 --> 00:10:38.140
of what a bow tie is.

00:10:38.140 --> 00:10:41.150
And it also has the suit
detector and the other neck

00:10:41.150 --> 00:10:41.650
detector.

00:10:41.650 --> 00:10:42.150
And yeah.

00:10:42.150 --> 00:10:43.750
And Here's some kind
of face detector

00:10:43.750 --> 00:10:45.250
that seems to mostly
be about noses,

00:10:45.250 --> 00:10:46.580
and mouths, and skin texture.

00:10:46.580 --> 00:10:50.459
And there's the sunglasses
detector up here.

00:10:50.459 --> 00:10:51.000
I don't know.

00:10:51.000 --> 00:10:51.958
Let's look at this one.

00:10:51.958 --> 00:10:57.060
It turns out that this has
a top of vase detector, body

00:10:57.060 --> 00:10:58.240
of vase detector.

00:10:58.240 --> 00:11:00.270
There's sort of a
flower detector.

00:11:00.270 --> 00:11:02.729
There's an antler detector that
helps it detect the handle.

00:11:02.729 --> 00:11:04.145
LAURENCE MORONEY:
How interesting.

00:11:04.145 --> 00:11:05.590
CHRISTOPHER OLAH:
Yellow spheres.

00:11:05.590 --> 00:11:07.000
LAURENCE MORONEY: And that
was another thing that

00:11:07.000 --> 00:11:08.830
was interesting in this
one is because the lemons

00:11:08.830 --> 00:11:10.755
at the bottom of the
image are yellow oblongs,

00:11:10.755 --> 00:11:12.630
and the tulips at the
top are yellow oblongs.

00:11:12.630 --> 00:11:12.882
CHRISTOPHER OLAH: Uh-huh.

00:11:12.882 --> 00:11:13.010
Uh-huh.

00:11:13.010 --> 00:11:14.634
LAURENCE MORONEY:
But despite those two

00:11:14.634 --> 00:11:16.550
being very similar,
that it's able to detect

00:11:16.550 --> 00:11:17.800
that they're different things.

00:11:17.800 --> 00:11:18.758
CHRISTOPHER OLAH: Yeah.

00:11:18.758 --> 00:11:21.770
And in fact, if we-- well,
I'll go to that in a second.

00:11:21.770 --> 00:11:23.324
But there's some other one--

00:11:23.324 --> 00:11:24.740
we can see some
interesting things

00:11:24.740 --> 00:11:27.321
about how these all play
together in a second.

00:11:27.321 --> 00:11:27.820
But yeah.

00:11:27.820 --> 00:11:29.560
So I think the
first thing is just

00:11:29.560 --> 00:11:31.760
this is allowing us to
understand-- you know,

00:11:31.760 --> 00:11:33.610
sort of look at which
neurons are firing

00:11:33.610 --> 00:11:34.710
and see what's going on.

00:11:34.710 --> 00:11:36.220
Oh, I wanna show you
the pig for a second.

00:11:36.220 --> 00:11:37.600
The pig is a great
example of something

00:11:37.600 --> 00:11:39.180
that has lots of
pointy ear detectors.

00:11:39.180 --> 00:11:40.210
I might have to move
around a little bit

00:11:40.210 --> 00:11:41.920
to find a place
where we're really

00:11:41.920 --> 00:11:43.920
getting all of the pointy
ear detectors to fire.

00:11:43.920 --> 00:11:47.259
There's two, but there's
some places where more fire.

00:11:47.259 --> 00:11:48.800
Well, maybe I'm not
gonna be patient.

00:11:48.800 --> 00:11:50.360
Oh, yeah, there's
a bunny-- here.

00:11:50.360 --> 00:11:50.860
OK, look.

00:11:50.860 --> 00:11:52.776
So we have pointy ear,
pointy ear, pointy ear,

00:11:52.776 --> 00:11:54.640
and then some kind
of rabbit-like thing

00:11:54.640 --> 00:11:56.244
with another kind of pointy ear.

00:11:56.244 --> 00:11:57.910
It has this whole
vocabulary for talking

00:11:57.910 --> 00:11:59.330
about different kinds of ears.

00:11:59.330 --> 00:12:01.454
And I don't have anything
like that, and I love it.

00:12:01.454 --> 00:12:02.590
[LAUGHTER]

00:12:02.590 --> 00:12:07.260
Right, so that's the first step,
but we wanted to go and take

00:12:07.260 --> 00:12:08.344
that a little bit further.

00:12:08.344 --> 00:12:10.426
So another thing that you
can do is you could ask,

00:12:10.426 --> 00:12:12.310
what do all these neurons
together represent?

00:12:12.310 --> 00:12:13.720
And there's all these
neurons that are

00:12:13.720 --> 00:12:15.040
firing to different extents.

00:12:15.040 --> 00:12:16.415
And if we, instead
of just trying

00:12:16.415 --> 00:12:18.940
to visualize individual ones,
we try to visualize them

00:12:18.940 --> 00:12:21.565
all together, we see what
the network collectively

00:12:21.565 --> 00:12:23.117
saw at that position.

00:12:23.117 --> 00:12:24.700
And then if you
stitch those together,

00:12:24.700 --> 00:12:27.430
you can kind of see how the
network saw the whole image.

00:12:27.430 --> 00:12:34.030
So here's our pig,
or hog, or whatever

00:12:34.030 --> 00:12:36.590
it is, or our cat and our dog.

00:12:36.590 --> 00:12:39.700
And you can see, really seeing
the ear here, and the snout,

00:12:39.700 --> 00:12:42.960
and the cat head, and
the legs, and the grass,

00:12:42.960 --> 00:12:45.710
or over here, the
top of the vase,

00:12:45.710 --> 00:12:48.610
and the handle, and the flowers,
and the lemons down here,

00:12:48.610 --> 00:12:51.210
and the wooden surface
in the background.

00:12:51.210 --> 00:12:53.440
Or over here, there's
the bird on the chain.

00:12:53.440 --> 00:12:55.890
And it really sees the
bird's beak and the feathers,

00:12:55.890 --> 00:12:57.580
the chain, and all of this.

00:12:57.580 --> 00:12:59.120
I mean, that's kind
of interesting.

00:12:59.120 --> 00:13:00.320
LAURENCE MORONEY:
Pretty incredible.

00:13:00.320 --> 00:13:00.940
CHRISTOPHER OLAH: And
then we could actually

00:13:00.940 --> 00:13:01.829
go a step further.

00:13:01.829 --> 00:13:04.120
And the way that these networks
work, remember, is they

00:13:04.120 --> 00:13:05.020
have multiple layers.

00:13:05.020 --> 00:13:06.394
They build up
their understanding

00:13:06.394 --> 00:13:07.990
over the course of
a bunch of layers.

00:13:07.990 --> 00:13:09.760
And so initially,
they're really--

00:13:09.760 --> 00:13:11.190
let's zoom in here--

00:13:11.190 --> 00:13:13.780
you know, initially,
they're really focused

00:13:13.780 --> 00:13:16.549
on relatively simple things.

00:13:16.549 --> 00:13:18.340
In fact, we don't show
you the first layer,

00:13:18.340 --> 00:13:19.930
but it would really
be about edges.

00:13:19.930 --> 00:13:21.290
LAURENCE MORONEY: Right.

00:13:21.290 --> 00:13:23.440
CHRISTOPHER OLAH: But
here, it's getting

00:13:23.440 --> 00:13:25.900
really simple
combinations of edges,

00:13:25.900 --> 00:13:27.910
or maybe the
beginnings of textures.

00:13:27.910 --> 00:13:32.489
But if we go down just two
more layers, we now have--

00:13:32.489 --> 00:13:34.280
we're skipping a layer
in the middle here--

00:13:34.280 --> 00:13:36.010
we now have this thing
where it's really

00:13:36.010 --> 00:13:39.520
got a much richer
sense of textures.

00:13:39.520 --> 00:13:41.560
And it's starting to get
a bit of 3D structure.

00:13:41.560 --> 00:13:43.226
And if you go a little
bit further down,

00:13:43.226 --> 00:13:45.920
now we're seeing the chain,
and the beak, and so on.

00:13:45.920 --> 00:13:46.750
And if we go a
little bit further,

00:13:46.750 --> 00:13:48.125
it sort of becomes
more abstract,

00:13:48.125 --> 00:13:50.230
and it's sort of
birds and chains.

00:13:50.230 --> 00:13:51.370
And similarly, if we go--

00:13:51.370 --> 00:13:52.040
let's look at the dog.

00:13:52.040 --> 00:13:53.290
I love this dog and this cat.

00:13:53.290 --> 00:13:57.070
So here, again, very simple
patterns with-- really

00:13:57.070 --> 00:13:57.970
mostly about edges.

00:13:57.970 --> 00:14:01.580
But you go just a step up,
and now there's fur textures.

00:14:01.580 --> 00:14:03.610
There's grass textures.

00:14:03.610 --> 00:14:05.930
You can really see that
there's this 3D structure where

00:14:05.930 --> 00:14:10.052
there's boundaries and
surfaces, a little bit.

00:14:10.052 --> 00:14:12.010
And then you go another
step, and it's starting

00:14:12.010 --> 00:14:13.960
to understand eyes too.

00:14:13.960 --> 00:14:16.080
But now, it's
really got the snout

00:14:16.080 --> 00:14:19.980
with eyes, the ears, the leg.

00:14:19.980 --> 00:14:21.527
And so you can really
see these jumps

00:14:21.527 --> 00:14:23.110
in its sophistication
of understanding

00:14:23.110 --> 00:14:24.500
the object as you go up.

00:14:24.500 --> 00:14:26.000
LAURENCE MORONEY: And this is
part of the magic of Distill,

00:14:26.000 --> 00:14:26.500
right?

00:14:26.500 --> 00:14:28.240
So this isn't just
a dry academic paper

00:14:28.240 --> 00:14:29.962
with illustrations in it.

00:14:29.962 --> 00:14:31.420
We can see you
interacting with it.

00:14:31.420 --> 00:14:34.000
We can see you actually
seeing what a neuron is seeing

00:14:34.000 --> 00:14:35.650
and getting hands-on to that.

00:14:35.650 --> 00:14:38.199
So it helped me to learn a lot
more about what's going on.

00:14:38.199 --> 00:14:39.157
CHRISTOPHER OLAH: Yeah.

00:14:39.157 --> 00:14:41.360
I think there's just
something extremely

00:14:41.360 --> 00:14:44.320
powerful about
allowing people to--

00:14:44.320 --> 00:14:48.775
you know, there's these pictures
that I have in my head and--

00:14:48.775 --> 00:14:51.150
I think there's actually two
interesting things going on.

00:14:51.150 --> 00:14:53.275
So one is there's these
pictures I have in my head.

00:14:53.275 --> 00:14:56.350
And by really turn
them into interfaces,

00:14:56.350 --> 00:14:59.140
I'm able to crystallize
those pictures into things

00:14:59.140 --> 00:15:01.600
that I can really test
my intuitions against

00:15:01.600 --> 00:15:04.992
and that can sort of
offload some of the thinking

00:15:04.992 --> 00:15:07.450
that I have in imagining what
this would be what's going on

00:15:07.450 --> 00:15:09.490
in the network and really
reifying it, really

00:15:09.490 --> 00:15:11.980
turning it into this thing
that I can interact with.

00:15:11.980 --> 00:15:13.900
And then not only do
I get that for myself,

00:15:13.900 --> 00:15:15.710
but I get to share
it with other people.

00:15:15.710 --> 00:15:18.040
And so they get to now
see this thing that's, I

00:15:18.040 --> 00:15:19.900
think, deeper than
what one would normally

00:15:19.900 --> 00:15:22.310
get where it's not just
me telling you results,

00:15:22.310 --> 00:15:25.880
but I'm really sharing this
way of thinking about it

00:15:25.880 --> 00:15:30.250
and interacting with
this type of problem.

00:15:30.250 --> 00:15:32.395
And I think that's
really, really exciting.

00:15:32.395 --> 00:15:33.353
LAURENCE MORONEY: Yeah.

00:15:33.353 --> 00:15:35.500
I mean, because to me, there's
two levels of opaqueness

00:15:35.500 --> 00:15:36.330
with machine learning, right?

00:15:36.330 --> 00:15:38.164
First of all there's
learning to begin with,

00:15:38.164 --> 00:15:39.163
which is already opaque.

00:15:39.163 --> 00:15:41.800
And then secondly is like, when
you start dealing with models,

00:15:41.800 --> 00:15:43.200
it's a black box, right?

00:15:43.200 --> 00:15:45.640
There's a trained
graph in this thing

00:15:45.640 --> 00:15:47.350
that you give it a
picture, and it tells

00:15:47.350 --> 00:15:48.433
you what's in the picture.

00:15:48.433 --> 00:15:50.192
So it's not like
source code that you

00:15:50.192 --> 00:15:52.400
can open up and step through
and see what's going on.

00:15:52.400 --> 00:15:55.031
So between those two
levels of opaqueness,

00:15:55.031 --> 00:15:57.280
it's really, really hard for
somebody to get into this

00:15:57.280 --> 00:15:57.990
and to learn it.

00:15:57.990 --> 00:16:00.370
And what I like about this
is that this is cracking open

00:16:00.370 --> 00:16:01.450
the second of those.

00:16:01.450 --> 00:16:03.550
So I can begin to
really see what's

00:16:03.550 --> 00:16:06.080
happening in the network,
and help me understand

00:16:06.080 --> 00:16:08.080
the network, and then
later on, help me to build

00:16:08.080 --> 00:16:11.960
and tune my own networks,
which is super cool.

00:16:11.960 --> 00:16:12.544
So now, Lucid.

00:16:12.544 --> 00:16:14.168
Where does Lucid fit
into this picture?

00:16:14.168 --> 00:16:15.340
Right.

00:16:15.340 --> 00:16:19.892
So with all of this, we've been
building up all of these tools,

00:16:19.892 --> 00:16:21.850
and we've been building
a lot of infrastructure

00:16:21.850 --> 00:16:22.820
to go along with it.

00:16:22.820 --> 00:16:24.361
And so one of the
things we're really

00:16:24.361 --> 00:16:26.290
excited about with this
paper is we're also

00:16:26.290 --> 00:16:28.180
open-sourcing all of
the infrastructure

00:16:28.180 --> 00:16:31.070
that we've built up to
go and do this research.

00:16:31.070 --> 00:16:33.520
And so that includes both that
first feature visualization

00:16:33.520 --> 00:16:37.150
article that I was showing
you where all of the tools

00:16:37.150 --> 00:16:38.740
you need to go
and produce images

00:16:38.740 --> 00:16:41.760
like this where you visualize
what a neuron's looking for,

00:16:41.760 --> 00:16:44.170
and then also, to go and--

00:16:44.170 --> 00:16:46.900
you know, that's also what
underlies all of these tools

00:16:46.900 --> 00:16:48.160
that we have here.

00:16:48.160 --> 00:16:48.940
LAURENCE MORONEY:
And the neurons

00:16:48.940 --> 00:16:50.898
in this one, are they
just trained on something

00:16:50.898 --> 00:16:51.850
like inception?

00:16:51.850 --> 00:16:52.870
CHRISTOPHER OLAH: Yeah.

00:16:52.870 --> 00:16:56.001
For both articles, we're
using this network, GoogLeNet,

00:16:56.001 --> 00:16:58.000
which was at one point a
state-of-the-art model.

00:16:58.000 --> 00:17:00.340
It's now several
years out of date,

00:17:00.340 --> 00:17:04.359
but it's one that we have used
for a while as a standard test

00:17:04.359 --> 00:17:07.044
for visualization.

00:17:07.044 --> 00:17:09.460
It seems like it actually--
there's something a little bit

00:17:09.460 --> 00:17:10.876
mysterious about
it where it seems

00:17:10.876 --> 00:17:14.140
like the neurons in GoogLeNet
especially correspond to ideas

00:17:14.140 --> 00:17:17.220
that are meaningful to humans.

00:17:17.220 --> 00:17:18.220
LAURENCE MORONEY: I see.

00:17:18.220 --> 00:17:20.260
CHRISTOPHER OLAH: It's a fun one
to start playing around with.

00:17:20.260 --> 00:17:20.552
LAURENCE MORONEY: Cool.

00:17:20.552 --> 00:17:20.650
Cool.

00:17:20.650 --> 00:17:22.359
CHRISTOPHER OLAH: But you can
go and plug in whatever model

00:17:22.359 --> 00:17:24.359
you want and try and use
these techniques on it.

00:17:24.359 --> 00:17:25.317
LAURENCE MORONEY: Nice.

00:17:25.317 --> 00:17:25.900
Nice.

00:17:25.900 --> 00:17:27.579
CHRISTOPHER OLAH:
We actually made

00:17:27.579 --> 00:17:30.340
these notebooks that
reproduce each one

00:17:30.340 --> 00:17:31.750
of the diagrams in the paper.

00:17:31.750 --> 00:17:32.320
LAURENCE MORONEY: Oh, cool.

00:17:32.320 --> 00:17:33.100
CHRISTOPHER OLAH: So
you can go and then--

00:17:33.100 --> 00:17:34.220
LAURENCE MORONEY:
You can get hands on.

00:17:34.220 --> 00:17:35.620
CHRISTOPHER OLAH:
Yeah, so you can just--

00:17:35.620 --> 00:17:38.036
you now, there's sort of this
continuum where you can just

00:17:38.036 --> 00:17:39.100
read the paper passively.

00:17:39.100 --> 00:17:40.690
You can engage
with the diagrams.

00:17:40.690 --> 00:17:43.720
And you can go a step deeper
and go and start playing around

00:17:43.720 --> 00:17:46.750
with one of these notebooks.

00:17:46.750 --> 00:17:50.920
And so here we have a notebook
for these activation grid

00:17:50.920 --> 00:17:52.460
visualizations.

00:17:52.460 --> 00:17:55.945
And then you can go and just--

00:17:55.945 --> 00:17:57.790
I guess we have to open
it in a playground.

00:17:57.790 --> 00:17:59.080
LAURENCE MORONEY: But it used
to be like, once upon a time

00:17:59.080 --> 00:18:01.120
that you'd read a paper,
and you try to understand

00:18:01.120 --> 00:18:01.810
what was in the paper.

00:18:01.810 --> 00:18:02.800
And you try to figure it out.

00:18:02.800 --> 00:18:04.590
And maybe there'd be
a bit of source code.

00:18:04.590 --> 00:18:06.220
And you'd take that source
code, but then you'd

00:18:06.220 --> 00:18:08.440
have to go and find a data
set of images to train.

00:18:08.440 --> 00:18:09.670
But you'd have a
different data set

00:18:09.670 --> 00:18:11.080
than the people
in the paper had.

00:18:11.080 --> 00:18:13.916
And there are all these
concepts, these frictions,

00:18:13.916 --> 00:18:15.790
these little bumps that
you have to get over.

00:18:15.790 --> 00:18:16.100
CHRISTOPHER OLAH: Right.

00:18:16.100 --> 00:18:18.310
And now we can make it this
continuous transition where

00:18:18.310 --> 00:18:19.910
you can be reading the
paper, you can be like,

00:18:19.910 --> 00:18:22.076
I want to play around with
this diagram a little bit

00:18:22.076 --> 00:18:23.290
and start doing that.

00:18:23.290 --> 00:18:25.060
I want to go a little step
further and start actually

00:18:25.060 --> 00:18:27.040
playing around with the
code and fiddling with it.

00:18:27.040 --> 00:18:28.664
And then if you want
to really dive in,

00:18:28.664 --> 00:18:30.440
we have all of our
code's open source.

00:18:30.440 --> 00:18:31.600
And then you can
learn the library

00:18:31.600 --> 00:18:32.890
a little bit by playing
around with the notebooks,

00:18:32.890 --> 00:18:33.842
and then go and--

00:18:33.842 --> 00:18:35.050
LAURENCE MORONEY: Super cool.

00:18:35.050 --> 00:18:36.865
And as you mentioned, if you
want to start playing with it,

00:18:36.865 --> 00:18:38.031
all the code is open source.

00:18:38.031 --> 00:18:40.600
So say I'm a developer, and
I want to learn this stuff,

00:18:40.600 --> 00:18:42.850
and I want to get started
now, where would I go?

00:18:42.850 --> 00:18:45.850
CHRISTOPHER OLAH: Go to
tensorflow/lucid on GitHub.

00:18:45.850 --> 00:18:47.380
That's our repository.

00:18:47.380 --> 00:18:51.620
And from there, you can get
access to tutorials on using it

00:18:51.620 --> 00:18:54.490
and a list of notebooks to
go and play around with.

00:18:54.490 --> 00:18:57.365
And then the next step
after that is you could--

00:18:57.365 --> 00:18:58.990
well, you might come
up with some ideas

00:18:58.990 --> 00:19:00.310
after you've gone and
played with some notebooks.

00:19:00.310 --> 00:19:01.900
And you might want to
just use code that's

00:19:01.900 --> 00:19:03.160
very similar to what's
in the notebooks

00:19:03.160 --> 00:19:05.200
so you could start writing
your own fun things.

00:19:05.200 --> 00:19:08.660
And a lot of this could also
be used for artistic purposes

00:19:08.660 --> 00:19:09.522
where--

00:19:09.522 --> 00:19:10.480
LAURENCE MORONEY: Cool.

00:19:10.480 --> 00:19:11.938
CHRISTOPHER OLAH:
You can certainly

00:19:11.938 --> 00:19:14.129
do traditional Deep
Dream-style stuff, as well.

00:19:14.129 --> 00:19:16.420
LAURENCE MORONEY: And also
your papers on Distill, I'll

00:19:16.420 --> 00:19:17.980
put links to them in the
description of this video.

00:19:17.980 --> 00:19:18.295
CHRISTOPHER OLAH: Fantastic.

00:19:18.295 --> 00:19:18.610
Yeah.

00:19:18.610 --> 00:19:19.770
LAURENCE MORONEY: So
that people can go there,

00:19:19.770 --> 00:19:20.330
and they can do it.

00:19:20.330 --> 00:19:20.750
CHRISTOPHER OLAH: Yeah.

00:19:20.750 --> 00:19:22.541
I think that probably
what I would do first

00:19:22.541 --> 00:19:23.740
is I would read the papers.

00:19:23.740 --> 00:19:26.980
And then from the papers,
you can go and jump

00:19:26.980 --> 00:19:28.200
to the notebooks.

00:19:28.200 --> 00:19:29.200
LAURENCE MORONEY: Right.

00:19:29.200 --> 00:19:30.610
CHRISTOPHER OLAH: Or
after you've read them,

00:19:30.610 --> 00:19:32.710
you can also look at
the Lucid repository

00:19:32.710 --> 00:19:35.590
and look at more tutorials and
stuff like that to play around.

00:19:35.590 --> 00:19:36.290
LAURENCE MORONEY: Sounds good.

00:19:36.290 --> 00:19:37.390
Thank you so much, Christopher.

00:19:37.390 --> 00:19:38.090
This has been so much fun.

00:19:38.090 --> 00:19:39.506
CHRISTOPHER OLAH:
Oh, my pleasure.

00:19:39.506 --> 00:19:41.061
LAURENCE MORONEY:
You know, we've

00:19:41.061 --> 00:19:43.060
been sitting here for a
little while geeking out

00:19:43.060 --> 00:19:45.476
about this stuff, and it's one
of the things I love to do.

00:19:45.476 --> 00:19:46.570
So thank you, so much.

00:19:46.570 --> 00:19:48.850
And thank you, everybody,
for watching this episode

00:19:48.850 --> 00:19:49.780
of "Coffee with a Googler."

00:19:49.780 --> 00:19:51.250
If you've any questions
for me, or if you've

00:19:51.250 --> 00:19:53.249
any questions for
Christopher, please leave them

00:19:53.249 --> 00:19:54.190
in the comments below.

00:19:54.190 --> 00:19:56.689
We'll also have links in the
description below to everything

00:19:56.689 --> 00:19:57.970
that we spoke about today.

00:19:57.970 --> 00:19:59.624
So thank you so much, again.

00:19:59.624 --> 00:20:01.540
And don't forget to hit
that Subscribe button.

00:20:01.540 --> 00:20:03.914
And remember, we'll have a
TensorFlow channel on YouTube,

00:20:03.914 --> 00:20:04.990
so go check it out.

00:20:04.990 --> 00:20:06.960
Thank you, so much.

