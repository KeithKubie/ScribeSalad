WEBVTT
Kind: captions
Language: en

00:00:00.229 --> 00:00:02.120
Are computers gonna take the world over?

00:00:02.120 --> 00:00:03.820
Stephen Hawking says yes!

00:00:03.820 --> 00:00:07.130
And I say, "Who cares, I'll be dead by then!"

00:00:07.130 --> 00:00:16.340
Hey guys, Tara here for Dnews - and if you're
excited by all the new advancements AI has

00:00:16.340 --> 00:00:18.859
made in the past few decades - you're not
alone.

00:00:18.859 --> 00:00:23.230
We've got smartphones that tell us where to
go, self-driving cars that get us there with

00:00:23.230 --> 00:00:27.890
no effort, and computers that can teach other
computers how to play Pac-man.

00:00:27.890 --> 00:00:30.240
Yes, it is an exciting time we live in.

00:00:30.240 --> 00:00:34.710
And the person you'd think would be most excited
- would be Stephen Hawking, a man who understands

00:00:34.710 --> 00:00:39.590
more about the universe we live in, than most
of us can even conceptualize with our tiny,

00:00:39.590 --> 00:00:40.590
plebeian brains.

00:00:40.590 --> 00:00:42.520
But, Stephen Hawking not excited.

00:00:42.520 --> 00:00:44.879
In fact, he's the opposite of excited.

00:00:44.879 --> 00:00:49.170
In an article he penned for the Independent
last week, he warns about the dangers of artificial

00:00:49.170 --> 00:00:53.399
intelligence - calling it "potentially our
worst mistake in history."

00:00:53.399 --> 00:00:57.530
He begins the article by acknowledging the
potential benefits of AI - like eradicating

00:00:57.530 --> 00:00:59.039
war, disease, and poverty.

00:00:59.039 --> 00:01:02.530
But he also says that there are things we
simply can't predict.

00:01:02.530 --> 00:01:07.830
And unless we learn how to avoid the potential
risks, we could be signing our own death sentence.

00:01:07.830 --> 00:01:11.520
Things we should be concerned about in the
short term, he says, is AI that allows us

00:01:11.520 --> 00:01:14.170
to effectively capitalize on our own greed.

00:01:14.170 --> 00:01:17.950
Autonomous-weapon systems that can choose
and eliminate targets with ease.

00:01:17.950 --> 00:01:22.750
Economical changes, that disrupt our way of
life, creating great wealth - and even greater

00:01:22.750 --> 00:01:24.180
wealth disparity.

00:01:24.180 --> 00:01:26.680
But these are things that can potentially
be avoided.

00:01:26.680 --> 00:01:29.110
The scariest part, is what comes after.

00:01:29.110 --> 00:01:33.460
Hawking cites Irving Good, one of the great
mathematicians of the 20th century, who pioneered

00:01:33.460 --> 00:01:39.610
the concept of "technological singularity"
- a hypothetical moment in time when AI finally

00:01:39.610 --> 00:01:43.851
surpasses human intelligence, radically changing
civilization as we know it.

00:01:43.851 --> 00:01:47.250
It might seem like something out of a sci-fi
movie, but according to Hawking - there are

00:01:47.250 --> 00:01:50.750
no physical laws that prevent such a thing
from happening.

00:01:50.750 --> 00:01:55.610
People aren't concerned about it now, because
we still have control over AI - but in a post-singularity

00:01:55.610 --> 00:01:59.450
world, the biggest question becomes - can
we control it at all?

00:01:59.450 --> 00:02:02.890
If computers become smarter than humans, then
there's no fundamental limit to what they

00:02:02.890 --> 00:02:03.890
can do.

00:02:03.890 --> 00:02:08.350
"Outsmarting financial markets, out-inventing
human researchers, out-manipulating human

00:02:08.350 --> 00:02:13.040
leaders, and developing weapons we cannot
even understand" - are just a few of the examples

00:02:13.040 --> 00:02:14.040
Hawking cites.

00:02:14.040 --> 00:02:17.810
With that in mind, he says - we should all
be asking ourselves - "what we can do now,

00:02:17.810 --> 00:02:22.430
to improve the chances of reaping the benefits
and avoiding the risks."

00:02:22.430 --> 00:02:23.430
What do you guys think?

00:02:23.430 --> 00:02:27.379
Is the potential destruction of all mankind
REALLY worth having an iPhone app that can

00:02:27.379 --> 00:02:34.939
fart on command?

00:02:34.939 --> 00:02:38.150
Leave YOUR thoughts in the comments below
- and hey - if you're a Sourcefed fan and

00:02:38.150 --> 00:02:42.349
you happen to live on the west coast, tickets
just went on sale for the DeFranco Does LA

00:02:42.349 --> 00:02:44.700
show on May 9th in Glendale, CA.

00:02:44.700 --> 00:02:47.670
This is a live show starring Philly D and
some of your favorite SourceFed regulars,

00:02:47.670 --> 00:02:51.469
so if you wanna grab tickets - just hit up
the bit.ly link in the description of this

00:02:51.469 --> 00:02:52.469
episode.

00:02:52.469 --> 00:03:03.489
Otherwise, happy farting!

