WEBVTT
Kind: captions
Language: en

00:00:07.000 --> 00:00:10.560
-- shortest paths.
This is the finale.

00:00:10.560 --> 00:00:13.832
Hopefully it was worth waiting
for.

00:00:13.832 --> 00:00:17.682
Remind you there's a quiz
coming up soon,

00:00:17.682 --> 00:00:23.359
you should be studying for it.
There's no problem set due at

00:00:23.359 --> 00:00:28.652
the same time as the quiz
because you should be studying

00:00:28.652 --> 00:00:32.315
now.
It's a take-home exam.

00:00:32.315 --> 00:00:37.157
It's required that you come to
class on Monday.

00:00:37.157 --> 00:00:43.052
Of course, you'll all come,
but everyone watching at home

00:00:43.052 --> 00:00:47.684
should also come next Monday to
get the quiz.

00:00:47.684 --> 00:00:53.684
It's the required lecture.
So, we need a bit of a recap in

00:00:53.684 --> 00:00:58.421
the trilogy so far.
So, the last two lectures,

00:00:58.421 --> 00:01:04.000
the last two episodes,
or about single source shortest

00:01:04.000 --> 00:01:08.338
paths.
So, we wanted to find the

00:01:08.338 --> 00:01:13.575
shortest path from a source
vertex to every other vertex.

00:01:13.575 --> 00:01:17.035
And, we saw a few algorithms
for this.

00:01:17.035 --> 00:01:21.525
Here's some recap.
We saw in the unweighted case,

00:01:21.525 --> 00:01:27.043
that was sort of the easiest
where all the edge weights were

00:01:27.043 --> 00:01:30.316
one.
Then we could use breadth first

00:01:30.316 --> 00:01:34.988
search.
And this costs what we call

00:01:34.988 --> 00:01:41.849
linear time in the graph world,
the number of vertices plus the

00:01:41.849 --> 00:01:46.166
number of edges.
The next simplest case,

00:01:46.166 --> 00:01:50.150
perhaps, is nonnegative edge
weights.

00:01:50.150 --> 00:01:54.798
And in that case,
what algorithm do we use?

00:01:54.798 --> 00:02:00.000
Dijkstra, all right,
everyone's awake.

00:02:00.000 --> 00:02:04.169
Several answers at once,
great.

00:02:04.169 --> 00:02:11.675
So this takes almost linear
time if you use a good heap

00:02:11.675 --> 00:02:15.706
structure, so,
V log V plus E.

00:02:15.706 --> 00:02:21.405
And, in the general case,
general weights,

00:02:21.405 --> 00:02:26.826
we would use Bellman-Ford which
you saw.

00:02:26.826 --> 00:02:33.498
And that costs VE,
good, OK, which is quite a bit

00:02:33.498 --> 00:02:38.375
worse.
This is ignoring log factors.

00:02:38.375 --> 00:02:42.716
Dijkstra is basically linear
time, Bellman-Ford you're

00:02:42.716 --> 00:02:45.911
quadratic if you have a
connected graph.

00:02:45.911 --> 00:02:49.269
So, in the sparse case,
when E is order V,

00:02:49.269 --> 00:02:52.955
this is about linear.
This is about quadratic.

00:02:52.955 --> 00:02:56.068
In the dense case,
when E is about V^2,

00:02:56.068 --> 00:03:00.000
this is quadratic,
and this is cubic.

00:03:00.000 --> 00:03:06.164
So, Dijkstra and Bellman-Ford
are separated by about an order

00:03:06.164 --> 00:03:09.452
of V factor, which is pretty
bad.

00:03:09.452 --> 00:03:15.410
OK, but that's the best we know
how to do for single source

00:03:15.410 --> 00:03:19.212
shortest paths,
negative edge weights,

00:03:19.212 --> 00:03:24.760
Bellman-Ford is the best.
We also saw in recitation the

00:03:24.760 --> 00:03:30.000
case of a DAG.
And there, what do you do?

00:03:30.000 --> 00:03:32.477
Topological sort,
yeah.

00:03:32.477 --> 00:03:39.010
So, you can do a topological
sort to get an ordering on the

00:03:39.010 --> 00:03:42.952
vertices.
That you run Bellman-Ford,

00:03:42.952 --> 00:03:47.119
one round.
This is one way to think of

00:03:47.119 --> 00:03:51.962
what's going on.
You run Bellman-Ford in the

00:03:51.962 --> 00:03:57.593
order given by the topological
sort, which is once,

00:03:57.593 --> 00:04:03.000
and you get a linear time
algorithm.

00:04:03.000 --> 00:04:06.567
So, DAG is another case where
we know how to do well even with

00:04:06.567 --> 00:04:08.556
weights.
Unweighted, we can also do

00:04:08.556 --> 00:04:10.486
linear time.
But most of the time,

00:04:10.486 --> 00:04:13.001
though, will be,
so you should keep these in

00:04:13.001 --> 00:04:15.633
mind in the quiz.
When you get a shortest path

00:04:15.633 --> 00:04:19.084
problem, or what you end up
determining is the shortest path

00:04:19.084 --> 00:04:22.594
problem, think about what's the
best algorithm you can use in

00:04:22.594 --> 00:04:24.758
that case?
OK, so that's single source

00:04:24.758 --> 00:04:27.449
shortest paths.
And so, in our evolution of the

00:04:27.449 --> 00:04:30.841
Death Star, initially it was
just nonnegative edge weights.

00:04:30.841 --> 00:04:34.000
Then we got negative edge
weights.

00:04:34.000 --> 00:04:37.333
Today, the Death Star
challenges us with all pair

00:04:37.333 --> 00:04:40.111
shortest paths,
where we want to know the

00:04:40.111 --> 00:04:44.000
shortest path weight between
every pair of vertices.

00:04:59.000 --> 00:05:03.650
OK, so let's get some quick
results.

00:05:03.650 --> 00:05:07.769
What could we do with this
case?

00:05:07.769 --> 00:05:13.615
So, for example,
suppose I have an unweighted

00:05:13.615 --> 00:05:18.531
graph.
Any suggestions of how I should

00:05:18.531 --> 00:05:26.902
compute all pair shortest paths?
Between every pair of vertices,

00:05:26.902 --> 00:05:32.083
I want to know the shortest
path weight.

00:05:32.083 --> 00:05:37.787
BFS, a couple more words?
Yeah?

00:05:37.787 --> 00:05:44.721
Right, BFS V times.
OK, I'll say V times BFS,

00:05:44.721 --> 00:05:49.763
OK?
So, the running time would be

00:05:49.763 --> 00:05:57.169
V^2 plus V times E,
yeah, which is assuming your

00:05:57.169 --> 00:06:03.000
graph is connected,
V times E.

00:06:03.000 --> 00:06:05.401
OK, good.
That's probably about the best

00:06:05.401 --> 00:06:07.802
algorithm we know for unweighted
graphs.

00:06:07.802 --> 00:06:11.558
So, a lot of these are going to
sort of be the obvious answer.

00:06:11.558 --> 00:06:15.067
You take your single source
algorithm, you run it V times.

00:06:15.067 --> 00:06:18.577
That's the best you can do,
OK, or the best we know how to

00:06:18.577 --> 00:06:19.932
do.
This is not so bad.

00:06:19.932 --> 00:06:22.518
This is like one iteration of
Bellman-Ford,

00:06:22.518 --> 00:06:25.165
for comparison.
We definitely need at least,

00:06:25.165 --> 00:06:27.936
like, V^2 time,
because the size of the output

00:06:27.936 --> 00:06:32.000
is V^2, shortest path weight we
have to compute.

00:06:32.000 --> 00:06:37.052
So, this is not perfect,
but pretty good.

00:06:37.052 --> 00:06:41.978
And we are not going to improve
on that.

00:06:41.978 --> 00:06:49.305
So, nonnegative edge weights:
the natural thing to do is to

00:06:49.305 --> 00:06:54.484
run Dijkstra V times,
OK, no big surprise.

00:06:54.484 --> 00:07:01.305
And the running time of that
is, well, V times E again,

00:07:01.305 --> 00:07:08.000
plus V^2, log V,
which is also not too bad.

00:07:08.000 --> 00:07:10.766
I mean, it's basically the same
as running BFS.

00:07:10.766 --> 00:07:12.691
And then, there's the log
factor.

00:07:12.691 --> 00:07:16.000
If you ignore the log factor,
this is the dominant term.

00:07:16.000 --> 00:07:18.345
And, I mean,
this had an [added?] V^2 as

00:07:18.345 --> 00:07:20.511
well.
So, these are both pretty good.

00:07:20.511 --> 00:07:22.977
I mean, this is kind of neat.
Essentially,

00:07:22.977 --> 00:07:26.526
the time it takes to run one
Bellman-Ford plus a log factor,

00:07:26.526 --> 00:07:29.533
you can compute all pair
shortest paths if you have

00:07:29.533 --> 00:07:35.311
nonnegative edge weights.
So, I mean, comparing all pairs

00:07:35.311 --> 00:07:39.691
to signal source,
this seems a lot better,

00:07:39.691 --> 00:07:45.032
except we can only handle
nonnegative edge weights.

00:07:45.032 --> 00:07:49.839
OK, so now let's think about
the general case.

00:07:49.839 --> 00:07:55.821
Well, this is the focus of
today, and here's where we can

00:07:55.821 --> 00:08:02.231
actually make an improvement.
So the obvious thing is V times

00:08:02.231 --> 00:08:08.000
Bellman-Ford,
which would cost V^2 times E.

00:08:08.000 --> 00:08:11.517
And that's pretty pitiful,
and we're going to try to

00:08:11.517 --> 00:08:15.862
improve that to something closer
to that nonnegative edge weight

00:08:15.862 --> 00:08:17.379
bound.
So it turns out,

00:08:17.379 --> 00:08:21.310
here, we can actually make an
improvement whereas in these

00:08:21.310 --> 00:08:24.413
special cases,
we really can't do much better.

00:08:24.413 --> 00:08:26.965
OK, I don't have a good
intuition why,

00:08:26.965 --> 00:08:30.275
but it's the case.
So, we'll cover something like

00:08:30.275 --> 00:08:34.000
three algorithms today for this
problem.

00:08:34.000 --> 00:08:37.241
The last one will be the best,
but along the way we'll see

00:08:37.241 --> 00:08:40.368
some nice connections between
shortest paths and dynamic

00:08:40.368 --> 00:08:42.927
programming, which we haven't
really seen yet.

00:08:42.927 --> 00:08:46.054
We've seen shortest path,
and applying greedy algorithms

00:08:46.054 --> 00:08:49.068
to it, but today will actually
do dynamic programming.

00:08:49.068 --> 00:08:51.911
The intuition is that with all
pair shortest paths,

00:08:51.911 --> 00:08:54.129
there's more potential
subproblem reuse.

00:08:54.129 --> 00:08:57.143
We've got to compute the
shortest path from x to y for

00:08:57.143 --> 00:08:59.190
all x and y.
Maybe we can reuse those

00:08:59.190 --> 00:09:03.000
shortest paths in computing
other shortest paths.

00:09:03.000 --> 00:09:07.090
OK, there's a bit more
reusability, let's say.

00:09:07.090 --> 00:09:12.363
OK, let me quickly define all
pair shortest paths formally,

00:09:12.363 --> 00:09:17.000
because we're going to change
our notation slightly.

00:09:17.000 --> 00:09:20.272
It's because we care about all
pairs.

00:09:20.272 --> 00:09:24.000
So, as usual,
the input is directed graph,

00:09:24.000 --> 00:09:29.724
so, vertices and edges.
We're going to say that the

00:09:29.724 --> 00:09:35.880
vertices are labeled one to n
for convenience because with all

00:09:35.880 --> 00:09:42.036
pairs, we're going to think of
things more as an n by n matrix

00:09:42.036 --> 00:09:48.293
instead of edges in some sense
because it doesn't help to think

00:09:48.293 --> 00:09:51.926
any more in terms of adjacency
lists.

00:09:51.926 --> 00:09:55.458
And, you have edge weights as
usual.

00:09:55.458 --> 00:10:00.000
This is what makes it
interesting.

00:10:00.000 --> 00:10:05.456
Some of them are going to be
negative.

00:10:05.456 --> 00:10:13.714
So, w maps to every real
number, and the target output is

00:10:13.714 --> 00:10:20.792
a shortest path matrix.
So, this is now an n by n

00:10:20.792 --> 00:10:25.806
matrix.
So, n is just the number of

00:10:25.806 --> 00:10:32.000
vertices of shortest path
weights.

00:10:32.000 --> 00:10:37.820
So, delta of i,
j is the shortest path weight

00:10:37.820 --> 00:10:42.714
from i to j for all pairs of
vertices.

00:10:42.714 --> 00:10:50.915
So this, you could represent as
an n by n matrix in particular.

00:10:50.915 --> 00:10:57.000
OK, so now let's start doing
algorithms.

00:10:57.000 --> 00:11:02.363
So, we have this very simple
algorithm, V times Bellman-Ford,

00:11:02.363 --> 00:11:06.206
V^2 times E,
and just for comparison's sake,

00:11:06.206 --> 00:11:09.513
I'm going to say,
let me rewrite that,

00:11:09.513 --> 00:11:14.519
V times Bellman-Ford gives us
this running time of V^2 E,

00:11:14.519 --> 00:11:18.363
and I'm going to think about
the case where,

00:11:18.363 --> 00:11:23.458
let's just say the graph is
dense, meeting that the number

00:11:23.458 --> 00:11:29.000
of edges is quadratic,
and the number of vertices.

00:11:29.000 --> 00:11:33.130
So in that case,
this will take V^4 time,

00:11:33.130 --> 00:11:37.675
which is pretty slow.
We'd like to do better.

00:11:37.675 --> 00:11:43.251
So, first goal would just be to
beat V^4, V hypercubed,

00:11:43.251 --> 00:11:46.866
I guess.
OK, and we are going to use

00:11:46.866 --> 00:11:52.856
dynamic programming to do that.
Or at least that's what the

00:11:52.856 --> 00:11:58.639
motivation will come from.
It will take us a while before

00:11:58.639 --> 00:12:03.906
we can even beat V^4,
which is maybe a bit pathetic,

00:12:03.906 --> 00:12:10.000
but it takes some clever
insights, let's say.

00:12:10.000 --> 00:12:19.576
OK, so I'm going to introduce a
bit more notation for this

00:12:19.576 --> 00:12:25.288
graph.
So, I'm going to think about

00:12:25.288 --> 00:12:33.301
the weighted adjacency matrix.
So, I don't think we've really

00:12:33.301 --> 00:12:37.598
seen this in lecture before,
although I think it's in the

00:12:37.598 --> 00:12:39.516
appendix.
What that means,

00:12:39.516 --> 00:12:44.043
so normally adjacency matrix is
like one if there's an edge,

00:12:44.043 --> 00:12:47.803
and zero if there isn't.
And this is in a digraph,

00:12:47.803 --> 00:12:50.718
so you have to be a little bit
careful.

00:12:50.718 --> 00:12:54.171
Here, these values,
the entries in the matrix,

00:12:54.171 --> 00:12:57.240
are going to be the weights of
the edges.

00:12:57.240 --> 00:13:01.000
OK, this is this if ij is an
edge.

00:13:01.000 --> 00:13:04.689
So, if ij is an edge in the
graph, and it's going to be

00:13:04.689 --> 00:13:08.788
infinity if there is no edge.
OK, in terms of shortest paths,

00:13:08.788 --> 00:13:12.068
this is a more useful way to
represent the graph.

00:13:12.068 --> 00:13:16.099
All right, and so this includes
everything that we need from

00:13:16.099 --> 00:13:18.422
here.
And now we just have to think

00:13:18.422 --> 00:13:21.906
about it as a matrix.
Matrices will be a useful tool

00:13:21.906 --> 00:13:25.186
in a little while.
OK, so now I'm going to define

00:13:25.186 --> 00:13:28.260
some sub problems.
And, there's different ways

00:13:28.260 --> 00:13:32.223
that you could define what's
going on in the shortest paths

00:13:32.223 --> 00:13:35.791
problem.
OK, the natural thing is I want

00:13:35.791 --> 00:13:39.084
to go from vertex i to vertex j.
What's the shortest path?

00:13:39.084 --> 00:13:42.551
OK, we need to refine the sub
problems a little but more than

00:13:42.551 --> 00:13:43.706
that.
Not surprising.

00:13:43.706 --> 00:13:46.537
And if you think about my
analogy to Bellman-Ford,

00:13:46.537 --> 00:13:50.004
what Bellman-Ford does is it
tries to build longer and longer

00:13:50.004 --> 00:13:52.662
shortest paths.
But here, length is in terms of

00:13:52.662 --> 00:13:55.493
the number of edges.
So, first, it builds shortest

00:13:55.493 --> 00:13:58.440
paths of length one.
We've proven the first round it

00:13:58.440 --> 00:14:01.789
does that.
The second round,

00:14:01.789 --> 00:14:06.421
it provides all shortest paths
of length two,

00:14:06.421 --> 00:14:08.842
of count two,
and so on.

00:14:08.842 --> 00:14:14.842
We'd like to do that sort of
analogously, and try to reuse

00:14:14.842 --> 00:14:20.526
things a little bit more.
So, I'm going to say d_ij^(m)

00:14:20.526 --> 00:14:26.315
is the weight of the shortest
path from i to j with some

00:14:26.315 --> 00:14:33.012
restriction involving m.
So: shortest path from i to j

00:14:33.012 --> 00:14:36.961
using at most m edges.
OK, for example,

00:14:36.961 --> 00:14:41.220
if m is zero,
then we don't have to really

00:14:41.220 --> 00:14:47.142
think very hard to find all
shortest paths of length zero.

00:14:47.142 --> 00:14:50.987
OK, they use zero edges,
I should say.

00:14:50.987 --> 00:14:57.116
So, Bellman-Ford sort of tells
us how to go from m to m plus

00:14:57.116 --> 00:15:02.000
one.
So, let's just figure that out.

00:15:02.000 --> 00:15:05.806
So one thing we know from the
Bellman-Ford analysis is if we

00:15:05.806 --> 00:15:08.967
look at d_ij^(m-1),
we know that in some sense the

00:15:08.967 --> 00:15:12.193
longest shortest path of
relevance, unless you have

00:15:12.193 --> 00:15:15.419
negative weight cycle,
the longest shortest path of

00:15:15.419 --> 00:15:19.032
relevance is when m equals n
minus one because that's the

00:15:19.032 --> 00:15:21.096
longest simple path you can
have.

00:15:21.096 --> 00:15:24.516
So, this should be a shortest
path weight from i to j,

00:15:24.516 --> 00:15:28.193
and it would be no matter what
larger value you put in the

00:15:28.193 --> 00:15:32.312
superscript.
This should be delta of i comma

00:15:32.312 --> 00:15:35.146
j if there's no negative weight
cycles.

00:15:35.146 --> 00:15:38.578
OK, so this feels good for
dynamic programming.

00:15:38.578 --> 00:15:43.128
This will give us the answer if
we can compute this for all m.

00:15:43.128 --> 00:15:47.230
Then we'll have the shortest
path weights in particular.

00:15:47.230 --> 00:15:50.662
We need a way to detect
negative weight cycles,

00:15:50.662 --> 00:15:54.167
but let's not worry about that
too much for now.

00:15:54.167 --> 00:15:58.344
There are negative weights,
but let's just assume for now

00:15:58.344 --> 00:16:02.000
there's no negative weight
cycles.

00:16:02.000 --> 00:16:06.333
OK, and we get a recursion
recurrence.

00:16:06.333 --> 00:16:10.900
And the base case is when m
equals zero.

00:16:10.900 --> 00:16:16.522
This is pretty easy.
They have the same vertices,

00:16:16.522 --> 00:16:22.027
the weight of zero,
and otherwise it's infinity.

00:16:22.027 --> 00:16:28.000
OK, and then the actual
recursion is for m.

00:16:57.000 --> 00:17:00.833
OK, if I got this right,
this is a pretty easy,

00:17:00.833 --> 00:17:05.083
intuitive recursion for
d_ij^(m) is a min of smaller

00:17:05.083 --> 00:17:10.338
things in terms of n minus one.
I'll just show the picture,

00:17:10.338 --> 00:17:14.669
and then the proof of that
claim should be obvious.

00:17:14.669 --> 00:17:19.606
So, this is proof by picture.
So, we have on the one hand,

00:17:19.606 --> 00:17:22.031
I over here,
and j over here.

00:17:22.031 --> 00:17:25.929
We want to know the shortest
path from i to j.

00:17:25.929 --> 00:17:30.000
And, we want to use,
at most, m edges.

00:17:30.000 --> 00:17:34.862
So, the idea is,
well, you could use m minus one

00:17:34.862 --> 00:17:39.310
edges to get somewhere.
So this is, at most,

00:17:39.310 --> 00:17:42.931
m minus one edges,
some other place,

00:17:42.931 --> 00:17:48.000
and we'll call it k.
So this is a candidate for k.

00:17:48.000 --> 00:17:53.482
And then you could take the
edge directly from k to j.

00:17:53.482 --> 00:18:00.000
So, this costs A_k^j,
and this costs DIK m minus one.

00:18:00.000 --> 00:18:02.878
OK, and that's a candidate path
of length that uses,

00:18:02.878 --> 00:18:06.095
at most, m edges from I to j.
And this is essentially just

00:18:06.095 --> 00:18:08.973
considering all of them.
OK, so there's sort of many

00:18:08.973 --> 00:18:11.851
paths we are considering.
All of these are candidate

00:18:11.851 --> 00:18:14.222
values of k.
We are taking them in over all

00:18:14.222 --> 00:18:16.084
k as intermediate nodes,
whatever.

00:18:16.084 --> 00:18:18.624
So there they are.
We take the best such path.

00:18:18.624 --> 00:18:20.881
That should encompass all
shortest paths.

00:18:20.881 --> 00:18:24.155
And this is essentially sort of
what Bellman-Ford is doing,

00:18:24.155 --> 00:18:26.977
although not exactly.
We also sort of want to think

00:18:26.977 --> 00:18:29.460
about, well, what if I just go
directly with,

00:18:29.460 --> 00:18:34.111
say, m minus one edges?
What if there is no edge here

00:18:34.111 --> 00:18:36.514
that I want to use,
in some sense?

00:18:36.514 --> 00:18:40.810
Well, we always think about
there being, and the way the A's

00:18:40.810 --> 00:18:45.252
are defined, there's always this
zero weight edge to yourself.

00:18:45.252 --> 00:18:48.529
So, you could just take a path
that's shorter,

00:18:48.529 --> 00:18:51.951
go from d i to j,
and j is a particular value of

00:18:51.951 --> 00:18:55.737
k that we might consider,
and then take a zero weight

00:18:55.737 --> 00:19:00.106
edge at the end from A and jj.
OK, so this really encompasses

00:19:00.106 --> 00:19:03.873
everything.
So that's a pretty trivial

00:19:03.873 --> 00:19:06.252
claim.
OK, now once we have such a

00:19:06.252 --> 00:19:08.774
recursion, we get a dynamic
program.

00:19:08.774 --> 00:19:11.585
I mean, there,
this is it in some sense.

00:19:11.585 --> 00:19:15.261
It's written recursively.
You can write a bottom up.

00:19:15.261 --> 00:19:19.513
And I would like to write it
bottom up it little bit because

00:19:19.513 --> 00:19:23.189
while it doesn't look like it,
this is a relaxation.

00:19:23.189 --> 00:19:26.072
This is yet another relaxation
algorithm.

00:19:26.072 --> 00:19:29.027
So, I'll give you,
so, this is sort of the

00:19:29.027 --> 00:19:31.909
algorithm.
This is not a very interesting

00:19:31.909 --> 00:19:35.948
algorithm.
So, you don't have to write it

00:19:35.948 --> 00:19:38.155
all down if you don't feel like
it.

00:19:38.155 --> 00:19:40.363
It's probably not even in the
book.

00:19:40.363 --> 00:19:42.506
This is just an intermediate
step.

00:19:42.506 --> 00:19:45.818
So, we loop over all m.
That's sort of the outermost

00:19:45.818 --> 00:19:48.285
thing to do.
I want to build longer and

00:19:48.285 --> 00:19:51.142
longer paths,
and this vaguely corresponds to

00:19:51.142 --> 00:19:53.805
Bellman-Ford,
although it's actually worse

00:19:53.805 --> 00:19:56.467
than Bellman-Ford.
But hey, what the heck?

00:19:56.467 --> 00:20:03.591
It's a stepping stone.
OK, then for all i and j,

00:20:03.591 --> 00:20:10.204
and then we want to compute
this min.

00:20:10.204 --> 00:20:17.734
So, we'll just loop over all k,
and relax.

00:20:17.734 --> 00:20:26.918
And, here's where we're
actually computing the min.

00:20:26.918 --> 00:20:35.000
And, it's a relaxation,
is the point.

00:20:35.000 --> 00:20:38.118
This is our good friend,
the relaxation step,

00:20:38.118 --> 00:20:40.173
relaxing edge.
Well, it's not,

00:20:40.173 --> 00:20:42.724
yeah.
I guess we're relaxing edge kj,

00:20:42.724 --> 00:20:45.700
or something,
except we don't have the same

00:20:45.700 --> 00:20:48.818
clear notion.
I mean, it's a particular thing

00:20:48.818 --> 00:20:52.149
that we're relaxing.
It's not just a single edge

00:20:52.149 --> 00:20:55.338
because we don't have a single
source anymore.

00:20:55.338 --> 00:20:59.448
It's now relative to source I,
we are relaxing the edge kj,

00:20:59.448 --> 00:21:03.307
something like that.
But this is clearly a

00:21:03.307 --> 00:21:05.923
relaxation.
We are just making the triangle

00:21:05.923 --> 00:21:08.103
inequality true if it wasn't
before.

00:21:08.103 --> 00:21:11.528
The tribal inequality has got
to hold between all pairs.

00:21:11.528 --> 00:21:14.268
And that's just implementing
this min, right?

00:21:14.268 --> 00:21:17.382
You're taking d ij.
You take the min of what it was

00:21:17.382 --> 00:21:19.873
before in some sense.
That was one of the

00:21:19.873 --> 00:21:23.610
possibilities we considered when
we looked at the zero weight

00:21:23.610 --> 00:21:24.731
edge.
We say, well,

00:21:24.731 --> 00:21:28.467
or you could go from i to some
k in some way that we knew how

00:21:28.467 --> 00:21:32.204
to before, and then add on the
edge, and check whether that's

00:21:32.204 --> 00:21:35.256
better if it's better,
set our current estimate to

00:21:35.256 --> 00:21:38.584
that.
And, you do this for all k.

00:21:38.584 --> 00:21:40.931
In particular,
you might actually compute

00:21:40.931 --> 00:21:43.924
something smaller than this min
because I didn't put

00:21:43.924 --> 00:21:46.799
superscripts up here.
But that's just making paths

00:21:46.799 --> 00:21:49.205
even better.
OK, so you have to argue that

00:21:49.205 --> 00:21:51.493
relaxation is always a good
thing to do.

00:21:51.493 --> 00:21:53.312
So, by not putting
superscripts,

00:21:53.312 --> 00:21:56.305
maybe I do some more
relaxation, but more relaxation

00:21:56.305 --> 00:21:59.004
never hurts us.
You can still argue correctness

00:21:59.004 --> 00:22:03.285
using this claim.
So, it's not quite the direct

00:22:03.285 --> 00:22:05.807
implementation,
but there you go,

00:22:05.807 --> 00:22:10.456
dynamic programming algorithm.
The main reason I'll write it

00:22:10.456 --> 00:22:14.790
down: so you see that it's a
relaxation, and you see the

00:22:14.790 --> 00:22:18.336
running time is n^4,
OK, which is certainly no

00:22:18.336 --> 00:22:22.513
better than Bellman-Ford.
Bellman-Ford was n^4 even in

00:22:22.513 --> 00:22:26.138
the dense case,
and it's a little better in the

00:22:26.138 --> 00:22:30.000
sparse case.
So: not doing so great.

00:22:30.000 --> 00:22:34.740
But it's a start.
OK, it gets our dynamic

00:22:34.740 --> 00:22:41.614
programming minds thinking.
And, we'll get a better dynamic

00:22:41.614 --> 00:22:47.185
program in a moment.
But first, there's actually

00:22:47.185 --> 00:22:52.874
something useful we can do with
this formulation,

00:22:52.874 --> 00:22:59.037
and I guess I'll ask,
but I'll be really impressed if

00:22:59.037 --> 00:23:04.907
anyone can see.
Does this formula look like

00:23:04.907 --> 00:23:09.753
anything else that you've seen
in any context,

00:23:09.753 --> 00:23:15.892
mathematical or algorithmic?
Have you seen that recurrence

00:23:15.892 --> 00:23:20.200
anywhere else?
OK, not exactly as stated,

00:23:20.200 --> 00:23:24.615
but similar.
I'm sure if you thought about

00:23:24.615 --> 00:23:30.000
it for awhile,
you could come up with it.

00:23:30.000 --> 00:23:33.162
Any answers?
I didn't think you would be

00:23:33.162 --> 00:23:36.324
very intuitive,
but the answer is matrix

00:23:36.324 --> 00:23:39.810
multiplication.
And it may now be obvious to

00:23:39.810 --> 00:23:43.459
you, or it may not.
You have to think with the

00:23:43.459 --> 00:23:47.108
right quirky mind.
Then it's obvious that it's

00:23:47.108 --> 00:23:50.189
matrix multiplication.
Remember, matrix

00:23:50.189 --> 00:23:52.459
multiplication,
we have A, B,

00:23:52.459 --> 00:23:55.216
and C.
They're all n by n matrices.

00:23:55.216 --> 00:24:00.000
And, we want to compute C
equals A times B.

00:24:00.000 --> 00:24:04.852
And what that meant was,
well, c_ij was a sum over all k

00:24:04.852 --> 00:24:08.558
of a_ik times b_kj.
All right, that was our

00:24:08.558 --> 00:24:11.647
definition of matrix
multiplication.

00:24:11.647 --> 00:24:15.529
And that formula looks kind of
like this one.

00:24:15.529 --> 00:24:19.058
I mean, notice the subscripts:
ik and kj.

00:24:19.058 --> 00:24:22.676
Now, the operators are a little
different.

00:24:22.676 --> 00:24:27.617
Here, we're multiplying the
inside things and adding them

00:24:27.617 --> 00:24:34.444
all together.
There, we're adding the inside

00:24:34.444 --> 00:24:41.259
things and taking them in.
But other than that,

00:24:41.259 --> 00:24:47.185
it's the same.
OK, weird, but here we go.

00:24:47.185 --> 00:24:55.481
So, the connection to shortest
paths is you replace these

00:24:55.481 --> 00:25:00.129
operators.
So, let's take matrix

00:25:00.129 --> 00:25:05.198
multiplication and replace,
what should I do first,

00:25:05.198 --> 00:25:10.470
plus this thing with min.
So, why not just change the

00:25:10.470 --> 00:25:13.714
operators, replace dot with
plus?

00:25:13.714 --> 00:25:18.073
This is just a different
algebra to work in,

00:25:18.073 --> 00:25:23.953
where plus actually means min,
and dot actually means plus.

00:25:23.953 --> 00:25:29.732
So, you have to check that
things sort of work out in that

00:25:29.732 --> 00:25:35.308
context, but if we do that,
then we get that c_ij is the

00:25:35.308 --> 00:25:39.769
min overall k of a_ik plus,
a bit messy here,

00:25:39.769 --> 00:25:44.464
b_kj.
And that looks like what we

00:25:44.464 --> 00:25:49.029
actually want to compute,
here, for one value of m,

00:25:49.029 --> 00:25:52.224
you have to sort of do this m
times.

00:25:52.224 --> 00:25:56.331
But this conceptually is
d_ij^(m), and this is

00:25:56.331 --> 00:25:59.709
d_ik^(m-1).
So, this is looking like a

00:25:59.709 --> 00:26:04.000
matrix product,
which is kind of cool.

00:26:04.000 --> 00:26:11.616
So, if we sort of plug in this
claim, then, and think about

00:26:11.616 --> 00:26:17.262
things as matrices,
the recurrence gives us,

00:26:17.262 --> 00:26:25.535
and I'll just write this now at
matrix form, that d^(m) is d^(m)

00:26:25.535 --> 00:26:30.000
minus one, funny product,
A.

00:26:30.000 --> 00:26:32.187
All right, so these are the
weights.

00:26:32.187 --> 00:26:34.687
These were the weighted
adjacency matrix.

00:26:34.687 --> 00:26:38.062
This was the previous d value.
This is the new d value.

00:26:38.062 --> 00:26:41.375
So, I'll just rewrite that in
matrix form with capital

00:26:41.375 --> 00:26:43.812
letters.
OK, I have the circle up things

00:26:43.812 --> 00:26:47.062
that are using this funny
algebra, so, in particular,

00:26:47.062 --> 00:26:49.812
circled product.
OK, so that's kind of nifty.

00:26:49.812 --> 00:26:52.250
We know something about
computing matrix

00:26:52.250 --> 00:26:54.812
multiplications.
We can do it in n^3 time.

00:26:54.812 --> 00:26:57.812
If we were a bit fancier,
maybe we could do it in

00:26:57.812 --> 00:27:02.956
sub-cubic time.
So, we could try to sort of use

00:27:02.956 --> 00:27:07.343
this connection.
And, well, think about what we

00:27:07.343 --> 00:27:10.491
are computing here.
We are saying,

00:27:10.491 --> 00:27:14.687
well, d to the m is the
previous one times A.

00:27:14.687 --> 00:27:19.075
So, what is d^(m)?
Is that some other algebraic

00:27:19.075 --> 00:27:23.271
notion that we know?
Yeah, it's the exponent.

00:27:23.271 --> 00:27:27.563
We're taking A,
and we want to raise it to the

00:27:27.563 --> 00:27:33.000
power, m, with this funny notion
of product.

00:27:33.000 --> 00:27:36.857
So, in other words,
d to the m is really just A to

00:27:36.857 --> 00:27:40.005
the m in a funny way.
So, I'll circle it,

00:27:40.005 --> 00:27:41.895
OK?
So, that sounds good.

00:27:41.895 --> 00:27:46.145
We also know how to compute
powers of things relatively

00:27:46.145 --> 00:27:50.002
quickly, if you remember how.
OK, for this notion,

00:27:50.002 --> 00:27:52.521
this power notion,
to make sense,

00:27:52.521 --> 00:27:55.434
I should say what A to the zero
means.

00:27:55.434 --> 00:28:00.000
And so, I need some kind of
identity matrix.

00:28:00.000 --> 00:28:02.945
And for here,
the identity matrix is this

00:28:02.945 --> 00:28:06.553
one, if I get it right.
So, it has zeros along the

00:28:06.553 --> 00:28:09.498
diagonal, and infinities
everywhere else.

00:28:09.498 --> 00:28:12.885
OK, that sort of just to match
this definition.

00:28:12.885 --> 00:28:16.862
d_ij zero should be zeros on
the diagonals and infinity

00:28:16.862 --> 00:28:19.881
everywhere else.
But you can check this is

00:28:19.881 --> 00:28:23.489
actually an identity.
If you multiply it with this

00:28:23.489 --> 00:28:26.802
funny multiplication against any
other matrix,

00:28:26.802 --> 00:28:31.000
you get the matrix back.
Nothing changes.

00:28:31.000 --> 00:28:34.749
This really is a valid identity
matrix.

00:28:34.749 --> 00:28:40.175
And, I should mention that for
A to the m to make sense,

00:28:40.175 --> 00:28:44.615
you really knew that your
product operation is

00:28:44.615 --> 00:28:48.858
associative.
So, actually A to the m circled

00:28:48.858 --> 00:28:54.482
makes sense because circled
multiplication is associative,

00:28:54.482 --> 00:28:58.429
and you can check that;
not hard because,

00:28:58.429 --> 00:29:03.855
I mean, min is associative,
and addition is associative,

00:29:03.855 --> 00:29:10.632
and all sorts of good stuff.
And, you have some kind of

00:29:10.632 --> 00:29:14.379
distributivity property.
And, this is,

00:29:14.379 --> 00:29:18.227
in turn, because the real
numbers with,

00:29:18.227 --> 00:29:23.696
and get the right order here,
with min as your addition

00:29:23.696 --> 00:29:29.367
operation, and plus as your
multiplication operation is a

00:29:29.367 --> 00:29:34.305
closed semi-ring.
So, if ever you want to know

00:29:34.305 --> 00:29:37.845
when powers make sense,
this is a good rule.

00:29:37.845 --> 00:29:42.702
If you have a closed semi-ring,
then matrix products on that

00:29:42.702 --> 00:29:46.571
semi-ring will give you an
associative operator,

00:29:46.571 --> 00:29:49.617
and then, good,
you can take products.

00:29:49.617 --> 00:29:54.721
OK, that's just some formalism.
So now, we have some intuition.

00:29:54.721 --> 00:29:57.438
The question is,
what's the right.

00:29:57.438 --> 00:30:00.154
Algorithm?
There are many possible

00:30:00.154 --> 00:30:06.000
answers, some of which are
right, some of which are not.

00:30:06.000 --> 00:30:09.714
So, we have this connection to
matrix products,

00:30:09.714 --> 00:30:13.024
and we have a connection to
matrix powers.

00:30:13.024 --> 00:30:15.608
And, we have algorithms for
both.

00:30:15.608 --> 00:30:18.354
The question is,
what should we do?

00:30:18.354 --> 00:30:23.198
So, all we need to do now is to
compute A to the funny power,

00:30:23.198 --> 00:30:26.267
n minus one.
n minus one is when we get

00:30:26.267 --> 00:30:29.739
shortest paths,
assuming we have no negative

00:30:29.739 --> 00:30:34.267
weight cycles.
In fact, we could compute a

00:30:34.267 --> 00:30:39.055
larger power than n minus one.
Once you get beyond n minus

00:30:39.055 --> 00:30:43.002
one, multipling by A doesn't
change you anymore.

00:30:43.002 --> 00:30:47.622
So, how should we do it?
OK, you're not giving any smart

00:30:47.622 --> 00:30:50.645
answers.
I'll give the stupid answer.

00:30:50.645 --> 00:30:53.081
You could say,
well, I take A.

00:30:53.081 --> 00:30:56.692
I multiply it by A.
Then I multiply it by A,

00:30:56.692 --> 00:31:00.052
and I multiply it by A,
and I use normal,

00:31:00.052 --> 00:31:04.000
boring matrix to
multiplication.

00:31:04.000 --> 00:31:07.038
So, I do, like,
n minus two,

00:31:07.038 --> 00:31:13.229
standard matrix multiplies.
So, standard multiply costs,

00:31:13.229 --> 00:31:17.056
like, n^3.
And I'm doing n of them.

00:31:17.056 --> 00:31:23.134
So, this gives me an n^4
algorithm, and compute all the

00:31:23.134 --> 00:31:26.735
shortest pathways in n^4.
Woohoo!

00:31:26.735 --> 00:31:31.920
OK, no improvement.
So, how can I do better?

00:31:31.920 --> 00:31:36.160
Right, natural thing to try
which sadly does not work,

00:31:36.160 --> 00:31:40.079
is to use the sub cubic matrix
multiply algorithm.

00:31:40.079 --> 00:31:44.160
We will, in some sense,
get there in a moment with a

00:31:44.160 --> 00:31:48.400
somewhat simpler problem.
But, it's actually not known

00:31:48.400 --> 00:31:53.279
how to compute shortest paths
using fast matrix multiplication

00:31:53.279 --> 00:31:55.839
like Strassen's system
algorithm.

00:31:55.839 --> 00:32:00.000
But, good suggestion.
OK, you have to think about why

00:32:00.000 --> 00:32:04.000
it doesn't work,
and I'll tell you.

00:32:04.000 --> 00:32:07.257
It's not obvious,
so it's a perfectly reasonable

00:32:07.257 --> 00:32:10.099
suggestion.
But in this context it doesn't

00:32:10.099 --> 00:32:12.524
quite work.
It will come up in a few

00:32:12.524 --> 00:32:14.118
moments.
The problem is,

00:32:14.118 --> 00:32:17.099
Strassen requires the notion of
subtraction.

00:32:17.099 --> 00:32:21.049
And here, addition is min.
And, there's no inverse to min.

00:32:21.049 --> 00:32:25.000
Once you take the arguments,
you can't sort of undo a min.

00:32:25.000 --> 00:32:28.950
OK, so there's no notion of
subtraction, so it's not known

00:32:28.950 --> 00:32:32.000
how to pull that off,
sadly.

00:32:32.000 --> 00:32:35.812
So, what other tricks do we
have up our sleeve?

00:32:35.812 --> 00:32:37.801
Yeah?
Divide and conquer,

00:32:37.801 --> 00:32:41.033
log n powering,
yeah, repeated squaring.

00:32:41.033 --> 00:32:44.016
That works.
Good, we had a fancy way.

00:32:44.016 --> 00:32:47.911
If you had a number n,
you sort of looked at the

00:32:47.911 --> 00:32:52.883
binary number representation of
n, and you either squared the

00:32:52.883 --> 00:32:57.027
number or squared it and added
another factor of A.

00:32:57.027 --> 00:33:02.000
Here, we don't even have to be
smart about it.

00:33:02.000 --> 00:33:07.306
OK, we can just compute,
we really only have to think

00:33:07.306 --> 00:33:11.489
about powers of two.
What we want to know,

00:33:11.489 --> 00:33:17.102
and I'm going to need a bigger
font here because there's

00:33:17.102 --> 00:33:22.510
multiple levels of subscripts,
A to the circled power,

00:33:22.510 --> 00:33:28.428
two to the ceiling of log n.
Actually, n minus one would be

00:33:28.428 --> 00:33:32.000
enough.
But there you go.

00:33:32.000 --> 00:33:35.852
You can write n if you didn't
leave yourself enough space like

00:33:35.852 --> 00:33:37.810
me, n the ceiling,
n the circle.

00:33:37.810 --> 00:33:41.284
This just means the next power
of two after n minus one,

00:33:41.284 --> 00:33:44.189
two to the ceiling log.
So, we don't have to go

00:33:44.189 --> 00:33:47.284
directly to n minus one.
We can go further because

00:33:47.284 --> 00:33:51.010
anything farther than n minus
one is still just the shortest

00:33:51.010 --> 00:33:53.473
pathways.
If you look at the definition,

00:33:53.473 --> 00:33:57.263
and you know that your paths
are simple, which is true if you

00:33:57.263 --> 00:34:02.000
have no negative weight cycles,
then fine, just go farther.

00:34:02.000 --> 00:34:04.930
Why not?
And so, to compute this,

00:34:04.930 --> 00:34:09.142
we just do ceiling of log n
minus one products,

00:34:09.142 --> 00:34:13.537
just take A squared,
and then take the result and

00:34:13.537 --> 00:34:17.199
square it; take the result and
square it.

00:34:17.199 --> 00:34:20.038
So, this is order log n
squares.

00:34:20.038 --> 00:34:25.257
And, we don't know how to use
Strassen, but we can use the

00:34:25.257 --> 00:34:30.751
boring, standard multiply of
n^3, and that gives us n^3 log n

00:34:30.751 --> 00:34:34.689
running time,
OK, which finally is something

00:34:34.689 --> 00:34:40.000
that beats Bellman-Ford in the
dense case.

00:34:40.000 --> 00:34:43.190
OK, in the dense case,
Bellman-Ford was n^4.

00:34:43.190 --> 00:34:46.677
Here we get n^3 log n,
finally something better.

00:34:46.677 --> 00:34:49.570
In the sparse case,
it's about the same,

00:34:49.570 --> 00:34:52.093
maybe a little worse.
E is order V.

00:34:52.093 --> 00:34:55.803
Then we're going to get,
like, V3 for Bellman-Ford.

00:34:55.803 --> 00:34:59.141
Here, we get n^3 log n.
OK, after log factors,

00:34:59.141 --> 00:35:03.000
this is an improvement some of
the time.

00:35:03.000 --> 00:35:05.963
OK, it's about the same the
other times.

00:35:05.963 --> 00:35:09.990
Another nifty thing that you
get for free out of this,

00:35:09.990 --> 00:35:13.029
is you can detect negative
weight cycles.

00:35:13.029 --> 00:35:16.676
So, here's a bit of a puzzle.
How would I detect,

00:35:16.676 --> 00:35:21.235
after I compute this product,
A to the power to ceiling log n

00:35:21.235 --> 00:35:25.946
minus one, how would I know if I
found a negative weight cycle?

00:35:25.946 --> 00:35:30.200
What would that mean it this
matrix of all their shortest

00:35:30.200 --> 00:35:34.000
paths of, at most,
a certain length?

00:35:34.000 --> 00:35:36.875
If I found a cycle,
what would have to be in that

00:35:36.875 --> 00:35:37.594
matrix?
Yeah?

00:35:37.594 --> 00:35:39.391
Right, so I could,
for example,

00:35:39.391 --> 00:35:41.368
take this thing,
multiply it by A,

00:35:41.368 --> 00:35:43.285
see if the matrix changed at
all.

00:35:43.285 --> 00:35:45.861
Right, that works fine.
That's what we do in

00:35:45.861 --> 00:35:48.258
Bellman-Ford.
It's an even simpler thing.

00:35:48.258 --> 00:35:51.013
It's already there.
You don't have to multiply.

00:35:51.013 --> 00:35:52.930
But that's the same running
time.

00:35:52.930 --> 00:35:55.686
That's a good answer.
The diagonal would have a

00:35:55.686 --> 00:35:56.884
negative value,
yeah.

00:35:56.884 --> 00:36:04.909
So, this is just a cute thing.
Both approaches would work,

00:36:04.909 --> 00:36:15.090
can detect a negative weight
cycle just by looking at the

00:36:15.090 --> 00:36:24.363
diagonal of the matrix.
You just look for a negative

00:36:24.363 --> 00:36:30.000
value in the diagonal.
OK.

00:36:30.000 --> 00:36:32.644
So, that's algorithm one,
let's say.

00:36:32.644 --> 00:36:37.329
I mean, we've seen several that
are all bad, but I'll call this

00:36:37.329 --> 00:36:39.899
number one.
OK, we'll see two more.

00:36:39.899 --> 00:36:44.282
This is the only one that will,
well, I shouldn't say that.

00:36:44.282 --> 00:36:47.984
Fine, there we go.
So, this is one dynamic program

00:36:47.984 --> 00:36:51.309
that wasn't so helpful,
except it showed us a

00:36:51.309 --> 00:36:53.954
connection to matrix
multiplication,

00:36:53.954 --> 00:36:57.581
which is interesting.
We'll see why it's useful a

00:36:57.581 --> 00:37:02.265
little bit more.
But, it bled to this nasty four

00:37:02.265 --> 00:37:04.822
nested loops.
And, using this trick,

00:37:04.822 --> 00:37:08.402
we got down to n^3 log n.
Let's try, just for n^3.

00:37:08.402 --> 00:37:11.544
OK, just get rid of that log.
It's annoying.

00:37:11.544 --> 00:37:15.125
It makes you a little bit worse
than Bellman-Ford,

00:37:15.125 --> 00:37:18.559
and the sparse case.
So, let's just erase one of

00:37:18.559 --> 00:37:21.555
these nested loops.
OK, I want to do that.

00:37:21.555 --> 00:37:25.720
OK, obviously that algorithm
doesn't work because it's for

00:37:25.720 --> 00:37:28.496
first decay, and it's not
defined, but,

00:37:28.496 --> 00:37:31.054
you know, I've got enough
variables.

00:37:31.054 --> 00:37:35.000
Why don't I just define k to
the m?

00:37:35.000 --> 00:37:39.555
OK, it turns out that works.
I'll do it from scratch,

00:37:39.555 --> 00:37:42.885
but why not?
I don't know if that's how

00:37:42.885 --> 00:37:47.003
Floyd and Warshall came up with
their algorithm,

00:37:47.003 --> 00:37:50.332
but here you go.
Here's Floyd-Warshall.

00:37:50.332 --> 00:37:55.063
The idea is to define the
subproblems a little bit more

00:37:55.063 --> 00:37:59.181
cleverly so that to compute one
of these values,

00:37:59.181 --> 00:38:04.000
you don't have to take the min
of n things.

00:38:04.000 --> 00:38:06.720
I just want to take the min of
two things.

00:38:06.720 --> 00:38:09.639
If I could do that,
and I still only have n^3

00:38:09.639 --> 00:38:12.227
subproblems, then I would have
n^3 time.

00:38:12.227 --> 00:38:14.947
So, all right,
the running time of dynamic

00:38:14.947 --> 00:38:18.995
program is number of subproblems
times the time to compute the

00:38:18.995 --> 00:38:22.843
recurrence for one subproblem.
So, here's linear times n^3,

00:38:22.843 --> 00:38:26.161
and we want n^3 times constant.
That would be good.

00:38:26.161 --> 00:38:29.810
So that's Floyd-Warshall.
So, here's the way we're going

00:38:29.810 --> 00:38:35.369
to redefine c_ij.
Or I guess, there it was called

00:38:35.369 --> 00:38:39.173
d_ij.
Good, so we're going to define

00:38:39.173 --> 00:38:43.847
something new.
So, c_ij superscript k is now

00:38:43.847 --> 00:38:50.043
going to be the weight of the
shortest path from I to j as

00:38:50.043 --> 00:38:54.173
before.
Notice I used the superscript k

00:38:54.173 --> 00:39:00.260
instead of m because I want k
and m to be the same thing.

00:39:00.260 --> 00:39:03.612
Deep.
OK, now, here's the new

00:39:03.612 --> 00:39:05.995
constraint.
I want all intermediate

00:39:05.995 --> 00:39:09.850
vertices along the path,
meeting all vertices except for

00:39:09.850 --> 00:39:13.915
I and j at the beginning and the
end to have a small label.

00:39:13.915 --> 00:39:17.140
So, they should be in the set
from one up to k.

00:39:17.140 --> 00:39:21.205
And this is where we are really
using that our vertices are

00:39:21.205 --> 00:39:24.079
labeled one up to m.
So, I'm going to say,

00:39:24.079 --> 00:39:28.004
well, first think about the
shortest paths that don't use

00:39:28.004 --> 00:39:32.000
any other vertices.
That's when k is zero.

00:39:32.000 --> 00:39:35.506
Then think about all the
shortest paths that maybe they

00:39:35.506 --> 00:39:38.038
use vertex one.
And then think about the

00:39:38.038 --> 00:39:41.545
shortest paths that maybe use
vertex one or vertex two.

00:39:41.545 --> 00:39:43.818
Why not?
You could define it in this

00:39:43.818 --> 00:39:44.922
way.
It turns out,

00:39:44.922 --> 00:39:48.363
then when you increase k,
you only have to think about

00:39:48.363 --> 00:39:51.220
one new vertex.
Here, we had to take min over

00:39:51.220 --> 00:39:53.623
all k.
Now we know which k to look at.

00:39:53.623 --> 00:39:57.129
OK, maybe that made sense.
Maybe it's not quite obvious

00:39:57.129 --> 00:39:59.077
yet.
But I'm going to redo this

00:39:59.077 --> 00:40:04.000
claim, redo a recurrence.
So, maybe first I should say

00:40:04.000 --> 00:40:07.428
some obvious things.
So, if I want delta of ij of

00:40:07.428 --> 00:40:10.571
the shortest pathway,
well, just take all the

00:40:10.571 --> 00:40:13.214
vertices.
So, take c_ij superscript n.

00:40:13.214 --> 00:40:15.928
That's everything.
And this even works,

00:40:15.928 --> 00:40:19.714
this is true even if you have a
negative weight cycle.

00:40:19.714 --> 00:40:22.928
Although, again,
we're going to sort of ignore

00:40:22.928 --> 00:40:26.642
negative weight cycles as long
as we can detect them.

00:40:26.642 --> 00:40:29.857
And, another simple case is if
you have, well,

00:40:29.857 --> 00:40:35.995
c_ij to zero.
Let me put that in the claim to

00:40:35.995 --> 00:40:40.635
be a little bit more consistent
here.

00:40:40.635 --> 00:40:47.208
So, here's the new claim.
If we want to compute c_ij

00:40:47.208 --> 00:40:50.817
superscript zero,
what is it?

00:40:50.817 --> 00:40:58.679
Superscript zero means I really
shouldn't use any intermediate

00:40:58.679 --> 00:41:03.918
vertices.
So, this has a very simple

00:41:03.918 --> 00:41:09.306
answer, a three letter answer.
So, it's not zero.

00:41:09.306 --> 00:41:12.673
It's four letters.
What's that?

00:41:12.673 --> 00:41:15.367
Nil.
No, not working yet.

00:41:15.367 --> 00:41:18.397
It has some subscripts,
too.

00:41:18.397 --> 00:41:25.020
So, the definition would be,
what's the shortest path weight

00:41:25.020 --> 00:41:31.530
from I to j when you're not
allowed to use any intermediate

00:41:31.530 --> 00:41:34.596
vertices?
Sorry?

00:41:34.596 --> 00:41:38.073
So, yeah, it has a very simple
name.

00:41:38.073 --> 00:41:43.141
That's the tricky part.
All right, so if i equals j,

00:41:43.141 --> 00:41:48.605
[LAUGHTER] you're clever,
right, open bracket i equals j

00:41:48.605 --> 00:41:50.493
means one, well,
OK.

00:41:50.493 --> 00:41:54.666
It sort of works,
but it's not quite right.

00:41:54.666 --> 00:41:59.237
In fact, I want infinity if i
does not equal j.

00:41:59.237 --> 00:42:05.000
And I want to zero if i equals
j, a_ij, good.

00:42:05.000 --> 00:42:07.363
I think it's a_ij.
It should be,

00:42:07.363 --> 00:42:09.041
right?
Maybe I'm wrong.

00:42:09.041 --> 00:42:12.243
Right, a_ij.
So it's essentially not what I

00:42:12.243 --> 00:42:13.920
said.
That's the point.

00:42:13.920 --> 00:42:17.961
If i does not equal j,
you still have to think about a

00:42:17.961 --> 00:42:20.706
single edge connecting i to j,
right?

00:42:20.706 --> 00:42:23.222
OK, so that's a bit of a
subtlety.

00:42:23.222 --> 00:42:27.492
This is only intermediate
vertices, so you could still go

00:42:27.492 --> 00:42:32.574
from i to j via a single edge.
That will cost a_ij.

00:42:32.574 --> 00:42:34.858
If there is an edge:
infinity.

00:42:34.858 --> 00:42:37.377
If there isn't one:
that is a_ij.

00:42:37.377 --> 00:42:42.102
So, OK, that gets us started.
And then, we want a recurrence.

00:42:42.102 --> 00:42:46.196
And, the recurrence is,
well, maybe you get away with

00:42:46.196 --> 00:42:49.031
all the vertices that you had
before.

00:42:49.031 --> 00:42:52.889
So, if you want to know paths
that you had before,

00:42:52.889 --> 00:42:56.748
so if you want to know paths
that use one up to k,

00:42:56.748 --> 00:43:01.000
maybe I just use one up to k
minus one.

00:43:01.000 --> 00:43:04.559
You could try that.
Or, you could try using k.

00:43:04.559 --> 00:43:07.169
So, either you use k or you
don't.

00:43:07.169 --> 00:43:09.779
If you don't,
it's got to be this.

00:43:09.779 --> 00:43:12.706
If you do, then you've got to
go to k.

00:43:12.706 --> 00:43:17.293
So why not go to k at the end?
So, you go from I to k using

00:43:17.293 --> 00:43:21.248
the previous vertices.
Obviously, you don't want to

00:43:21.248 --> 00:43:24.887
repeat k in there.
And then, you go from k to j

00:43:24.887 --> 00:43:29.000
somehow using vertices that are
not k.

00:43:29.000 --> 00:43:31.389
This should be pretty
intuitive.

00:43:31.389 --> 00:43:35.783
Again, I can draw a picture.
So, either you never go to k,

00:43:35.783 --> 00:43:40.331
and that's this wiggly line.
You go from i to j using things

00:43:40.331 --> 00:43:43.569
only one up to k minus one.
In other words,

00:43:43.569 --> 00:43:45.959
here we have to use one up to
k.

00:43:45.959 --> 00:43:48.349
So, this just means don't use
k.

00:43:48.349 --> 00:43:52.357
So, that's this thing.
Or, you use k somewhere in the

00:43:52.357 --> 00:43:55.595
middle there.
OK, it's got to be one of the

00:43:55.595 --> 00:43:57.214
two.
And in this case,

00:43:57.214 --> 00:44:00.760
you go from i to k using only
smaller vertices,

00:44:00.760 --> 00:44:05.000
because you don't want to
repeat k.

00:44:05.000 --> 00:44:10.778
And here, you go from k to j
using only smaller labeled

00:44:10.778 --> 00:44:14.738
vertices.
So, every path is one of the

00:44:14.738 --> 00:44:18.055
two.
So, we take the shortest of

00:44:18.055 --> 00:44:22.335
these two subproblems.
That's the answer.

00:44:22.335 --> 00:44:26.081
So, now we have a min of two
things.

00:44:26.081 --> 00:44:29.612
It takes constant time to
compute.

00:44:29.612 --> 00:44:36.766
So, we get a cubic algorithm.
So, let me write it down.

00:44:36.766 --> 00:44:41.192
So, this is the Floyd-Warshall
algorithm.

00:44:41.192 --> 00:44:46.614
I'll write the name again.
You give it a matrix A.

00:44:46.614 --> 00:44:50.377
That's all it really needs to
know.

00:44:50.377 --> 00:44:54.360
It codes everything.
You copy C to A.

00:44:54.360 --> 00:44:58.565
That's the warm up.
Right at time zero,

00:44:58.565 --> 00:45:03.183
C equals A.
And then you just have these

00:45:03.183 --> 00:45:07.394
three loops for every value of
k, for every value of i,

00:45:07.394 --> 00:45:10.981
and for every value of j.
You compute that min.

00:45:10.981 --> 00:45:15.036
And if you think about it a
little bit, that min is a

00:45:15.036 --> 00:45:18.000
relaxation.
Surprise, surprise.

00:45:47.000 --> 00:45:51.628
So, that is the Floyd-Warshall
algorithm.

00:45:51.628 --> 00:45:58.107
And, the running time is
clearly n^3, three nested loops,

00:45:58.107 --> 00:46:02.408
constant time inside.
So, we're finally getting

00:46:02.408 --> 00:46:05.056
something that is never worse
than Bellman-Ford.

00:46:05.056 --> 00:46:06.915
In the sparse case,
it's the same.

00:46:06.915 --> 00:46:09.619
And anything denser,
the number of edges is super

00:46:09.619 --> 00:46:11.591
linear.
This is strictly better than

00:46:11.591 --> 00:46:13.507
Bellman-Ford.
And, it's better than

00:46:13.507 --> 00:46:16.718
everything we've seen so far for
all pair, shortest paths.

00:46:16.718 --> 00:46:19.929
And, this handles negative
weights; very simple algorithm,

00:46:19.929 --> 00:46:21.732
even simpler than the one
before.

00:46:21.732 --> 00:46:23.929
It's just relaxation within
three loops.

00:46:23.929 --> 00:46:27.197
What more could you ask for?
And we need to check that this

00:46:27.197 --> 00:46:29.394
is indeed what min we're
computing here,

00:46:29.394 --> 00:46:33.000
except that the superscripts
are omitted.

00:46:33.000 --> 00:46:35.654
That's, again,
a bit of hand waving a bit.

00:46:35.654 --> 00:46:39.343
It's OK to omit subscripts
because that can only mean that

00:46:39.343 --> 00:46:42.515
you're doing more relaxation
techniques should be.

00:46:42.515 --> 00:46:45.169
Doing more relaxations can
never hurt you.

00:46:45.169 --> 00:46:48.082
In particular,
we do all the ones that we have

00:46:48.082 --> 00:46:50.283
to.
Therefore, we find the shortest

00:46:50.283 --> 00:46:52.225
path weights.
And, again, here,

00:46:52.225 --> 00:46:55.720
we're assuming that there is no
negative weight cycles.

00:46:55.720 --> 00:46:59.281
It shouldn't be hard to find
them, but you have to think

00:46:59.281 --> 00:47:04.408
about that a little bit.
OK, you could run another round

00:47:04.408 --> 00:47:07.672
of Bellman-Ford,
see if it relaxes in a new

00:47:07.672 --> 00:47:09.537
edges again.
For example,

00:47:09.537 --> 00:47:13.190
I think there's no nifty trick
for that version.

00:47:13.190 --> 00:47:17.619
And, we're going to cover,
that's our second algorithm for

00:47:17.619 --> 00:47:21.738
all pairs shortest paths.
Before we go up to the third

00:47:21.738 --> 00:47:26.167
algorithm, which is going to be
the cleverest of them all,

00:47:26.167 --> 00:47:30.053
the one Ring to rule them all,
to switch trilogies,

00:47:30.053 --> 00:47:33.705
we're going to take a little
bit of a diversion,

00:47:33.705 --> 00:47:37.280
side story, whatever,
and talk about transitive

00:47:37.280 --> 00:47:42.592
closure briefly.
This is just a good thing to

00:47:42.592 --> 00:47:45.648
know about.
And, it relates to the

00:47:45.648 --> 00:47:51.203
algorithms we've seen so far.
So, here's a transitive closure

00:47:51.203 --> 00:47:54.537
problem.
I give you a directed graph,

00:47:54.537 --> 00:47:59.814
and for all pair vertices,
i and j, I want to compute this

00:47:59.814 --> 00:48:03.333
number.
It's one if there's a path from

00:48:03.333 --> 00:48:06.670
i to j.
From i to j,

00:48:06.670 --> 00:48:14.185
OK, and then zero otherwise.
OK, this is sort of like a

00:48:14.185 --> 00:48:22.953
boring adjacency matrix with no
weights, except it's about paths

00:48:22.953 --> 00:48:32.000
instead of being about edges.
OK, so how can I compute this?

00:48:32.000 --> 00:48:39.325
That's very simple.
How should I compute this?

00:48:39.325 --> 00:48:45.023
This gives me a graph in some
sense.

00:48:45.023 --> 00:48:54.790
This is adjacency matrix of a
new graph called the transitive

00:48:54.790 --> 00:49:01.966
closure of my input graph.
So, breadth first search,

00:49:01.966 --> 00:49:05.033
yeah, good.
So, all I need to do is find

00:49:05.033 --> 00:49:08.337
shortest paths,
and if the weights come out

00:49:08.337 --> 00:49:12.898
infinity, then there's no path.
If it's less than infinity,

00:49:12.898 --> 00:49:15.415
that there's a path.
And so here,

00:49:15.415 --> 00:49:19.662
so you are saying maybe I don't
care about the weights,

00:49:19.662 --> 00:49:22.887
so I can run breadth first
search n times,

00:49:22.887 --> 00:49:27.134
and that will work indeed.
So, if we do B times B of S,

00:49:27.134 --> 00:49:31.539
so it's maybe weird that I'm
covering here in the middle,

00:49:31.539 --> 00:49:36.244
but it's just an interlude.
So, we have,

00:49:36.244 --> 00:49:42.361
then, something like V times E.
OK, you can run any of these

00:49:42.361 --> 00:49:46.508
algorithms.
You could take Floyd-Warshall

00:49:46.508 --> 00:49:48.581
for example.
Why not?

00:49:48.581 --> 00:49:54.698
OK, then it would just be V^ I
mean, you could run in any of

00:49:54.698 --> 00:50:00.816
these algorithms with weights of
one or zero, and just check

00:50:00.816 --> 00:50:06.000
whether the values are infinity
or not.

00:50:06.000 --> 00:50:10.075
So, I mean, t_ij equals zero,
if and only if the shortest

00:50:10.075 --> 00:50:12.622
path weight from i to j is
infinity.

00:50:12.622 --> 00:50:16.261
So, just solve this.
This is an easier problem than

00:50:16.261 --> 00:50:18.444
shortest paths.
It is, in fact,

00:50:18.444 --> 00:50:22.665
strictly easier in a certain
sense, because what's going on

00:50:22.665 --> 00:50:26.668
with transitive closure,
and I just want to mention this

00:50:26.668 --> 00:50:30.525
out of interest because
transitive closure is a useful

00:50:30.525 --> 00:50:33.865
thing to know about.
Essentially,

00:50:33.865 --> 00:50:36.751
what we are doing,
let me get this right,

00:50:36.751 --> 00:50:39.420
is using a different set of
operators.

00:50:39.420 --> 00:50:43.315
We're using or and and,
a logical or and and instead of

00:50:43.315 --> 00:50:46.273
min and plus,
OK, because we want to know,

00:50:46.273 --> 00:50:49.592
if you think about a
relaxation, in some sense,

00:50:49.592 --> 00:50:53.199
maybe I should think about it
in terms of this min.

00:50:53.199 --> 00:50:56.950
So, if I want to know,
is there a pathway from I to j

00:50:56.950 --> 00:51:02.000
that uses vertices labeled one
through k in the middle?

00:51:02.000 --> 00:51:05.762
Well, either there is a path
that doesn't use the vertex k,

00:51:05.762 --> 00:51:09.720
or there is a path that uses k,
and then it would have to look

00:51:09.720 --> 00:51:12.380
like that.
OK, so there would have to be a

00:51:12.380 --> 00:51:15.624
path here, and there would have
to be a path there.

00:51:15.624 --> 00:51:18.803
So, the min and plus get
replaced with or and and.

00:51:18.803 --> 00:51:21.463
And if you remember,
this used to be plus,

00:51:21.463 --> 00:51:24.512
and this used to be product in
the matrix world.

00:51:24.512 --> 00:51:28.015
So, plus is now like or.
And, multiply is now like and,

00:51:28.015 --> 00:51:31.000
which sounds very good,
right?

00:51:31.000 --> 00:51:35.994
Plus does feel like or,
and multiply does feel like and

00:51:35.994 --> 00:51:40.063
if you live in a zero-one world.
So, in fact,

00:51:40.063 --> 00:51:45.335
this is not quite the field Z
mod two, but this is a good,

00:51:45.335 --> 00:51:49.867
nice, field to work in.
This is the Boolean world.

00:51:49.867 --> 00:51:55.046
So, I'll just write Boole.
Good old Boole knows all about

00:51:55.046 --> 00:51:58.283
this.
It's like his master's thesis,

00:51:58.283 --> 00:52:03.000
I think, talking about Boolean
algebra.

00:52:03.000 --> 00:52:06.906
And, this actually means that
you can use fast matrix

00:52:06.906 --> 00:52:09.235
multiply.
You can use Strassen's

00:52:09.235 --> 00:52:13.518
algorithm, and the fancier
algorithms, and you can compute

00:52:13.518 --> 00:52:16.448
the transitive closure in
subcubic time.

00:52:16.448 --> 00:52:19.829
So, this is sub cubic if the
edges are sparse.

00:52:19.829 --> 00:52:24.336
But, it's cubic in the worst
case if there are lots of edges.

00:52:24.336 --> 00:52:27.341
This is cubic.
You can actually do better

00:52:27.341 --> 00:52:30.572
using Strassen.
So, I'll just say you can do

00:52:30.572 --> 00:52:33.180
it.
No details here.

00:52:33.180 --> 00:52:37.019
I think it should be,
so in fact, there is a theorem.

00:52:37.019 --> 00:52:41.300
This is probably not in the
textbook, but there's a theorem

00:52:41.300 --> 00:52:45.949
that says transitive closure is
just as hard as matrix multiply.

00:52:45.949 --> 00:52:49.714
OK, they are equivalent.
Their running times are the

00:52:49.714 --> 00:52:52.371
same.
We don't know how long it takes

00:52:52.371 --> 00:52:55.028
to do a matrix multiply over a
field.

00:52:55.028 --> 00:52:57.685
It's somewhere between n^2 and
n^2.3.

00:52:57.685 --> 00:53:03.000
But, whatever the answer is:
same for transitive closure.

00:53:03.000 --> 00:53:09.403
OK, there's the interlude.
And that's where we actually

00:53:09.403 --> 00:53:16.873
get to use Strassen and friends.
Remember, Strassen was n to the

00:53:16.873 --> 00:53:22.328
log base two of seven algorithm.
Remember that,

00:53:22.328 --> 00:53:28.375
especially on the final.
Those are things you should

00:53:28.375 --> 00:53:35.043
have at the tip of your tongue.
OK, the last algorithm we're

00:53:35.043 --> 00:53:39.275
going to cover is really going
to build on what we saw last

00:53:39.275 --> 00:53:43.070
time: Johnson's algorithm.
And, I've lost some of the

00:53:43.070 --> 00:53:46.427
running times here.
But, when we had unweighted

00:53:46.427 --> 00:53:50.732
graphs, we could do all pairs
really fast, just as fast as a

00:53:50.732 --> 00:53:54.235
single source Bellman-Ford.
That's kind of nifty.

00:53:54.235 --> 00:53:58.175
We don't know how to improve
Bellman-Ford in the single

00:53:58.175 --> 00:54:02.517
source case.
So, we can't really help to get

00:54:02.517 --> 00:54:07.472
anything better than V times E.
And, if you remember running V

00:54:07.472 --> 00:54:11.126
times Dijkstra,
V times Dijkstra was about the

00:54:11.126 --> 00:54:14.050
same.
So, just put this in the recall

00:54:14.050 --> 00:54:19.005
bubble here: V times Dijkstra
would give us V times E plus V^2

00:54:19.005 --> 00:54:21.685
log V.
And, if you ignore that log

00:54:21.685 --> 00:54:25.908
factor, this is just VE.
OK, so this was really good.

00:54:25.908 --> 00:54:29.725
Dijkstra was great.
And this was for nonnegative

00:54:29.725 --> 00:54:34.453
edge weights.
So, with negative edge weights,

00:54:34.453 --> 00:54:38.094
somehow we'd like to get the
same running time.

00:54:38.094 --> 00:54:41.419
Now, how might I get the same
running time?

00:54:41.419 --> 00:54:45.614
Well, it would be really nice
if I could use Dijkstra.

00:54:45.614 --> 00:54:49.889
Of course, Dijkstra doesn't
work with negative weights.

00:54:49.889 --> 00:54:53.292
So what could I do?
What would I hope to do?

00:54:53.292 --> 00:54:56.142
What could I hope to?
Suppose I want,

00:54:56.142 --> 00:55:02.000
in the middle of the algorithm,
it says run Dijkstra n times.

00:55:02.000 --> 00:55:05.665
Then, what should I do to
prepare for that?

00:55:05.665 --> 00:55:09.591
Make all the weights positive,
or nonnegative.

00:55:09.591 --> 00:55:13.431
Why not, right?
We're being wishful thinking.

00:55:13.431 --> 00:55:17.358
That's what we'll do.
So, this is called graph

00:55:17.358 --> 00:55:21.198
re-weighting.
And, what's cool is we actually

00:55:21.198 --> 00:55:26.172
already know how to do it.
We just don't know that we know

00:55:26.172 --> 00:55:30.011
how to do it.
But I know that we know that we

00:55:30.011 --> 00:55:34.287
know how to do it.
You don't yet know that we know

00:55:34.287 --> 00:55:39.000
that I know that we know how to
do it.

00:55:39.000 --> 00:55:41.643
So, it turns out you can
re-weight the vertices.

00:55:41.643 --> 00:55:44.398
So, at the end of the last
class someone asked me,

00:55:44.398 --> 00:55:46.704
can you just,
like, add the same weight to

00:55:46.704 --> 00:55:48.503
all the edges?
That doesn't work.

00:55:48.503 --> 00:55:51.821
Not quite, because different
paths have different numbers of

00:55:51.821 --> 00:55:53.845
edges.
What we are going to do is add

00:55:53.845 --> 00:55:55.757
a particular weight to each
vertex.

00:55:55.757 --> 00:55:58.457
What does that mean?
Well, because we really only

00:55:58.457 --> 00:56:02.000
have weights on the edges,
here's what well do.

00:56:02.000 --> 00:56:06.738
We'll re-weight each edge,
so, (u,v), let's say,

00:56:06.738 --> 00:56:12.485
going to go back into graph
speak instead of matrix speak,

00:56:12.485 --> 00:56:17.828
(u,v) instead of I and j,
and we'll call this modified

00:56:17.828 --> 00:56:20.752
weight w_h.
h is our function.

00:56:20.752 --> 00:56:24.482
It gives us a number for every
vertex.

00:56:24.482 --> 00:56:30.732
And, it's just going to be the
old weight of that edge plus the

00:56:30.732 --> 00:56:36.882
weight of the start vertex minus
the weight of the terminating

00:56:36.882 --> 00:56:40.829
vertex.
I'm sure these have good names.

00:56:40.829 --> 00:56:43.838
One of these is the head,
and the other is the tail,

00:56:43.838 --> 00:56:47.201
but I can never remember which.
OK, so we've directed edge

00:56:47.201 --> 00:56:48.795
(u,v).
Just add one of them;

00:56:48.795 --> 00:56:51.450
subtract the other.
And, it's a directed edge,

00:56:51.450 --> 00:56:53.397
so that's a consistent
definition.

00:56:53.397 --> 00:56:55.344
OK, so that's called
re-weighting.

00:56:55.344 --> 00:56:58.000
Now, this is actually a
theorem.

00:56:58.000 --> 00:57:03.391
If you do this,
then, let's say,

00:57:03.391 --> 00:57:10.000
for any vertices,
u and v in the graph,

00:57:10.000 --> 00:57:18.869
for any two vertices,
all paths from u to v have the

00:57:18.869 --> 00:57:27.043
same weight as they did before,
well, not quite.

00:57:27.043 --> 00:57:34.000
They have the same
re-weighting.

00:57:34.000 --> 00:57:37.265
So, if you look at all the
different paths and you say,

00:57:37.265 --> 00:57:39.925
well, what's the difference
between vh, well,

00:57:39.925 --> 00:57:42.827
sorry, let's say delta,
which is the old shortest

00:57:42.827 --> 00:57:45.790
paths, and deltas of h,
which is the shortest path

00:57:45.790 --> 00:57:48.511
weights according to this new
weight function,

00:57:48.511 --> 00:57:50.446
then that difference is the
same.

00:57:50.446 --> 00:57:53.832
So, we'll say that all these
paths are re-weighted by the

00:57:53.832 --> 00:57:55.948
same amounts.
OK, this is actually a

00:57:55.948 --> 00:58:00.000
statement about all paths,
not just shortest paths.

00:58:00.000 --> 00:58:05.286
There we go.
OK, to how many people is this

00:58:05.286 --> 00:58:08.811
obvious already?
A few, yeah,

00:58:08.811 --> 00:58:12.587
it is.
And what's the one word?

00:58:12.587 --> 00:58:16.489
OK, it's maybe not that
obvious.

00:58:16.489 --> 00:58:23.034
All right, shout out the word
when you figure it out.

00:58:23.034 --> 00:58:29.454
Meanwhile, I'll write out this
rather verbose proof.

00:58:29.454 --> 00:58:36.000
There's a one word proof,
still waiting.

00:58:36.000 --> 00:58:41.601
So, let's just take one of
these paths that starts at u and

00:58:41.601 --> 00:58:43.919
ends at v.
Take any path.

00:58:43.919 --> 00:58:49.423
We're just going to see what
its new weight is relative to

00:58:49.423 --> 00:58:53.576
its old weight.
And so, let's just write out

00:58:53.576 --> 00:58:57.825
w_h of the path,
which we define in the usual

00:58:57.825 --> 00:59:03.909
way as the sum over all edges of
the new weight of the edge from

00:59:03.909 --> 00:59:09.229
v_i to v_i plus one.
Do you have the word?

00:59:09.229 --> 00:59:11.458
No?
Tough puzzle then,

00:59:11.458 --> 00:59:15.068
OK.
So that's the definition of the

00:59:15.068 --> 00:59:20.164
weight of a path.
And, then we know this thing is

00:59:20.164 --> 00:59:23.030
just w of v_i,
v_i plus one.

00:59:23.030 --> 00:59:27.914
I'll get it right,
plus the weight of the first

00:59:27.914 --> 00:59:32.479
vertex, plus,
sorry, the re-weighting of v_i

00:59:32.479 --> 00:59:38.000
minus the re-weighting of v_i
plus one.

00:59:38.000 --> 00:59:42.510
This is all in parentheses
that's summed over I.

00:59:42.510 --> 00:59:46.637
Now I need the magic word.
Telescopes, good.

00:59:46.637 --> 00:59:51.340
Now this is obvious:
each of these telescopes with

00:59:51.340 --> 00:59:55.851
an extra previous,
except the very beginning and

00:59:55.851 --> 00:59:59.786
the very end.
So, this is the sum of these

00:59:59.786 --> 01:00:03.817
weights of edges,
but then outside the sum,

01:00:03.817 --> 01:00:09.000
we have plus h of v_1,
and minus h of v_k.

01:00:09.000 --> 01:00:11.933
OK, those guys don't quite
cancel.

01:00:11.933 --> 01:00:15.577
We're not looking at a cycle,
just a path.

01:00:15.577 --> 01:00:20.822
And, this thing is just w of
the path, as this is the normal

01:00:20.822 --> 01:00:24.111
weight of the path.
And so the change,

01:00:24.111 --> 01:00:29.088
the difference between w_h of P
and w of P is this thing,

01:00:29.088 --> 01:00:33.000
which is just h of u minus h of
v.

01:00:33.000 --> 01:00:36.744
And, the point is that's the
same as long as you fix the

01:00:36.744 --> 01:00:39.468
endpoints, u and v,
of the shortest path,

01:00:39.468 --> 01:00:43.348
you're changing this path
weight by the same thing for all

01:00:43.348 --> 01:00:45.800
paths.
This is for any path from u to

01:00:45.800 --> 01:00:49.612
v, and that proves the theorem.
So, the one word here was

01:00:49.612 --> 01:00:51.927
telescopes.
These change in weights

01:00:51.927 --> 01:00:55.536
telescope over any path.
Therefore, if we want to find

01:00:55.536 --> 01:00:58.327
shortest paths,
you just find the shortest

01:00:58.327 --> 01:01:01.800
paths in this re-weighted
version, and then you just

01:01:01.800 --> 01:01:06.848
change it by this one amount.
You subtract off this amount

01:01:06.848 --> 01:01:10.281
instead of adding it.
That will give you the shortest

01:01:10.281 --> 01:01:12.591
path weight in the original
weights.

01:01:12.591 --> 01:01:15.694
OK, so this is a tool.
We now know how to change

01:01:15.694 --> 01:01:18.995
weights in the graph.
But what we really want is to

01:01:18.995 --> 01:01:22.889
change weights in the graph so
that the weights all come out

01:01:22.889 --> 01:01:25.134
nonnegative.
OK, how do we do that?

01:01:25.134 --> 01:01:28.105
Why in the world would there be
a function, h,

01:01:28.105 --> 01:01:32.000
that makes all the edge weights
nonnegative?

01:01:32.000 --> 01:01:42.851
It doesn't make sense.
It turns out we already know.

01:01:42.851 --> 01:01:52.000
So, I should write down this
consequence.

01:02:12.000 --> 01:02:14.193
Let me get this in the right
order.

01:02:14.193 --> 01:02:17.096
So in particular,
the shortest path changes by

01:02:17.096 --> 01:02:19.677
this amount.
And if you want to know this

01:02:19.677 --> 01:02:22.774
value, you just move the stuff
to the other side.

01:02:22.774 --> 01:02:26.193
So, we compute deltas of h,
then we can compute delta.

01:02:26.193 --> 01:02:29.935
That's the consequence here.
How many people here pronounce

01:02:29.935 --> 01:02:33.981
this word corollary?
OK, and how many people

01:02:33.981 --> 01:02:37.599
pronounce it corollary?
Yeah, we are alone.

01:02:37.599 --> 01:02:42.596
Usually get at least one other
student, and they're usually

01:02:42.596 --> 01:02:45.353
Canadian or British or
something.

01:02:45.353 --> 01:02:50.006
I think that the accent.
So, I always avoid pronouncing

01:02:50.006 --> 01:02:53.969
his word unless I really think,
it's corollary,

01:02:53.969 --> 01:02:57.587
and get it right.
I at least say Z not Zed.

01:02:57.587 --> 01:03:03.428
OK, here we go.
So, what we want to do is find

01:03:03.428 --> 01:03:09.371
one of these functions.
I mean, let's just write down

01:03:09.371 --> 01:03:15.771
what we could hope to have.
We want to find a re-weighted

01:03:15.771 --> 01:03:22.971
function, h, the signs of weight
to each vertex such that w_h of

01:03:22.971 --> 01:03:28.457
(u,v) is nonnegative.
That would be great for all

01:03:28.457 --> 01:03:34.735
edges, all (u,v) in E.
OK, then we could run Dijkstra.

01:03:34.735 --> 01:03:38.264
We could run Dijkstra,
get the delta h's,

01:03:38.264 --> 01:03:41.352
and then just undo the
re-weighting,

01:03:41.352 --> 01:03:45.147
and get what we want.
And, that is Johnson's

01:03:45.147 --> 01:03:48.235
algorithm.
The claim is that this is

01:03:48.235 --> 01:03:52.029
always possible.
OK, why should it always be

01:03:52.029 --> 01:03:54.941
possible?
Well, let's look at this

01:03:54.941 --> 01:03:57.764
constraint.
w_h of (u,v) is that.

01:03:57.764 --> 01:04:02.441
So, it's w of (u,v) plus h of u
minus h of V should be

01:04:02.441 --> 01:04:09.691
nonnegative.
Let me rewrite this a little

01:04:09.691 --> 01:04:14.886
bit.
I'm going to put these guys

01:04:14.886 --> 01:04:21.589
over here.
That would be the right thing,

01:04:21.589 --> 01:04:30.805
h of v minus h of u is less
than or equal to w of (u,v).

01:04:30.805 --> 01:04:39.068
Does that look familiar?
Did I get it right?

01:04:39.068 --> 01:04:46.496
It should be right.
Anyone seen that inequality

01:04:46.496 --> 01:04:51.826
before?
Yeah, yes, correct answer.

01:04:51.826 --> 01:04:56.993
OK, where?
In a previous lecture?

01:04:56.993 --> 01:05:06.000
In the previous lecture.
What is this called if I

01:05:06.000 --> 01:05:11.166
replace h with x?
Charles knows.

01:05:11.166 --> 01:05:20.833
Good, anyone else remember all
the way back to episode two?

01:05:20.833 --> 01:05:31.000
I know there was a weekend.
What's this operator called?

01:05:31.000 --> 01:05:34.058
Not subtraction but,
I think I heard it,

01:05:34.058 --> 01:05:36.568
oh man.
All right, I'll tell you.

01:05:36.568 --> 01:05:39.627
It's a difference constraint,
all right?

01:05:39.627 --> 01:05:42.058
This is the difference
operator.

01:05:42.058 --> 01:05:45.745
OK, it's our good friend
difference constraints.

01:05:45.745 --> 01:05:48.490
So, this is what we want to
satisfy.

01:05:48.490 --> 01:05:51.784
We have a system of difference
constraints.

01:05:51.784 --> 01:05:55.862
h of V minus h of u should be,
we want to find these.

01:05:55.862 --> 01:05:59.941
These are our unknowns.
Subject to these constraints,

01:05:59.941 --> 01:06:05.845
we are given the w's.
Now, we know in these

01:06:05.845 --> 01:06:10.995
difference constraints are
satisfiable.

01:06:10.995 --> 01:06:18.855
Can someone tell me when these
constraints are satisfiable?

01:06:18.855 --> 01:06:26.714
We know exactly when for any
set of difference constraints.

01:06:26.714 --> 01:06:32.000
You've got to remember the
math.

01:06:32.000 --> 01:06:37.649
Terminology,
I can understand.

01:06:37.649 --> 01:06:47.779
It's hard to remember words
unless you're a linguist,

01:06:47.779 --> 01:06:54.207
perhaps.
So, when is the system of

01:06:54.207 --> 01:07:02.000
different constraints
satisfiable?

01:07:02.000 --> 01:07:08.341
All right, you should
definitely, very good.

01:07:08.341 --> 01:07:12.027
[LAUGHTER] Yes,
very good.

01:07:12.027 --> 01:07:21.023
Someone brought their lecture
notes: when the constraint graph

01:07:21.023 --> 01:07:27.806
has no negative weight cycles.
Good, thank you.

01:07:27.806 --> 01:07:34.000
Now, what is the constraint
graph?

01:07:34.000 --> 01:07:37.726
OK, this has a one letter
answer more or less.

01:07:37.726 --> 01:07:40.458
I'll accept the one letter
answer.

01:07:40.458 --> 01:07:41.038
What?
A?

01:07:41.038 --> 01:07:41.949
A: close.
G.

01:07:41.949 --> 01:07:43.936
Yeah, I mean,
same thing.

01:07:43.936 --> 01:07:47.745
Yeah, so the constraint graph
is essentially G.

01:07:47.745 --> 01:07:51.388
Actually, it is G.
The constraint graph is G,

01:07:51.388 --> 01:07:54.286
good.
And, we prove this by adding a

01:07:54.286 --> 01:07:57.764
new source for text,
and connecting that to

01:07:57.764 --> 01:08:01.766
everyone.
But that's sort of beside the

01:08:01.766 --> 01:08:03.898
point.
That was in order to actually

01:08:03.898 --> 01:08:05.604
satisfy them.
But this is our

01:08:05.604 --> 01:08:08.527
characterization.
So, if we assume that there are

01:08:08.527 --> 01:08:12.243
no negative weight cycles in our
graph, which we've been doing

01:08:12.243 --> 01:08:14.923
all the time,
then we know that this thing is

01:08:14.923 --> 01:08:16.994
satisfiable.
Therefore, there is an

01:08:16.994 --> 01:08:20.101
assignment of this h's.
There is a re-weighting that

01:08:20.101 --> 01:08:22.111
makes all the weights
nonnegative.

01:08:22.111 --> 01:08:24.548
Then we can run Dijkstra.
OK, we're done.

01:08:24.548 --> 01:08:27.167
Isn't that cool?
And how do we satisfy these

01:08:27.167 --> 01:08:29.786
constraints?
We know how to do that with one

01:08:29.786 --> 01:08:32.284
run of Bellman-Ford,
which costs order VE,

01:08:32.284 --> 01:08:36.000
which is less than V times
Dijkstra.

01:08:36.000 --> 01:08:39.750
So, that's it,
write down the details

01:08:39.750 --> 01:08:41.000
somewhere.

01:09:00.000 --> 01:09:03.902
So, this is Johnson's
algorithm.

01:09:03.902 --> 01:09:07.931
This is the fanciest of them
all.

01:09:07.931 --> 01:09:13.723
It will be our fastest,
all pairs shortest path

01:09:13.723 --> 01:09:17.122
algorithm.
So, the claim is,

01:09:17.122 --> 01:09:23.543
we can find a function,
h, from V to R such that the

01:09:23.543 --> 01:09:30.971
modified weight of every edge is
nonnegative for every edge,

01:09:30.971 --> 01:09:37.366
(u,v), in our graph.
And, we do that using

01:09:37.366 --> 01:09:43.000
Bellman-Ford to solve the
difference constraints.

01:09:57.000 --> 01:10:01.075
These are exactly the different
constraints that we were born to

01:10:01.075 --> 01:10:03.663
solve that we learned to solve
last time.

01:10:03.663 --> 01:10:06.704
The graphs here are
corresponding exactly if you

01:10:06.704 --> 01:10:10.391
look back at the definition.
Or, Bellman-Ford will tell us

01:10:10.391 --> 01:10:12.785
that there is a negative weight
cycle.

01:10:12.785 --> 01:10:16.796
OK, great, so it's not that we
really have to assume that there

01:10:16.796 --> 01:10:19.772
is no negative weight cycle.
We'll get to know.

01:10:19.772 --> 01:10:22.942
And if your fancy,
you can actually figure out the

01:10:22.942 --> 01:10:25.918
minus infinities from this.
But, at this point,

01:10:25.918 --> 01:10:29.865
I just want to think about the
case where there is no negative

01:10:29.865 --> 01:10:33.696
weight cycle.
But if there is,

01:10:33.696 --> 01:10:39.954
we can find out that it exists,
and that just tell the user.

01:10:39.954 --> 01:10:45.257
OK, then we'd stop.
Otherwise, there is no negative

01:10:45.257 --> 01:10:48.969
weight cycle.
Therefore, there is an

01:10:48.969 --> 01:10:54.166
assignment that gives is
nonnegative edge weights.

01:10:54.166 --> 01:11:00.000
So, we just use it.
We use it to run Dijkstra.

01:11:00.000 --> 01:11:02.744
So, step two is,
oh, I should say the running

01:11:02.744 --> 01:11:05.987
time of all this is V times E.
So, we're just running

01:11:05.987 --> 01:11:08.419
Bellman-Ford on exactly the
input graph.

01:11:08.419 --> 01:11:10.665
Plus, we add a source,
if you recall,

01:11:10.665 --> 01:11:13.160
to solve a set of difference
constraints.

01:11:13.160 --> 01:11:16.340
You add a source vertex,
S, connected to everyone at

01:11:16.340 --> 01:11:20.145
weight zero, run Bellman-Ford
from there because we don't have

01:11:20.145 --> 01:11:22.328
a source here.
We just have a graph.

01:11:22.328 --> 01:11:25.758
We want to know all pairs.
So, this, you can use to find

01:11:25.758 --> 01:11:30.000
whether there is a negative
weight cycle anywhere.

01:11:30.000 --> 01:11:33.428
Or, we get this magic
assignment.

01:11:33.428 --> 01:11:39.535
So now, w_h is nonnegative,
so we can run Dijkstra on w_h.

01:11:39.535 --> 01:11:43.821
We'll say, using w_h,
so you compute w_h.

01:11:43.821 --> 01:11:49.392
That takes linear time.
And, we run Dijkstra for each

01:11:49.392 --> 01:11:54.428
possible source.
I'll write this out explicitly.

01:11:54.428 --> 01:12:00.000
We've had this in our minds
several times.

01:12:00.000 --> 01:12:05.368
But, when we said n times
Dijkstra over n times BFS,

01:12:05.368 --> 01:12:09.684
here it is.
We want to compute delta sub h

01:12:09.684 --> 01:12:15.263
now, of (u,v) for all V,
and we do this separately for

01:12:15.263 --> 01:12:18.947
all u.
And so, the running time here

01:12:18.947 --> 01:12:23.684
is VE plus V^2 log V.
This is just V times the

01:12:23.684 --> 01:12:30.000
running time of Dijkstra,
which is E plus V log V.

01:12:30.000 --> 01:12:35.084
OK, it happens that this term
is the same as this one,

01:12:35.084 --> 01:12:39.017
which is nice,
because that means step one

01:12:39.017 --> 01:12:43.334
costs us nothing asymptotically.
OK, and then,

01:12:43.334 --> 01:12:47.075
last step is,
well, now we know delta h.

01:12:47.075 --> 01:12:52.831
We just need to compute delta.
So, for each pair of vertices,

01:12:52.831 --> 01:12:57.052
we'll call it (u,v),
we just compute what the

01:12:57.052 --> 01:13:03.000
original weights would be,
so what delta (u,v) is.

01:13:03.000 --> 01:13:07.471
And we can do that using this
corollary.

01:13:07.471 --> 01:13:13.777
It's just delta sub h of (u,v)
minus h of u plus h of v.

01:13:13.777 --> 01:13:19.624
I got the signs right.
Yeah, so this takes V^2 time,

01:13:19.624 --> 01:13:24.668
also dwarfed by the running
time of Dijkstra.

01:13:24.668 --> 01:13:31.777
So, the overall running time of
Johnson's algorithm is just the

01:13:31.777 --> 01:13:39.000
running time of step two,
running Dijkstra n times --

01:13:51.000 --> 01:13:54.951
-- which is pretty cool.
When it comes to single source

01:13:54.951 --> 01:13:58.243
shortest paths,
Bellman-Ford is the best thing

01:13:58.243 --> 01:14:01.990
for general weights.
Dijkstra is the best thing for

01:14:01.990 --> 01:14:04.976
nonnegative weights.
But for all pair shortest

01:14:04.976 --> 01:14:08.890
paths, we can skirt the whole
negative weight issue by using

01:14:08.890 --> 01:14:11.213
this magic we saw from
Bellman-Ford.

01:14:11.213 --> 01:14:14.995
But now, running Dijkstra n
times, which is still the best

01:14:14.995 --> 01:14:17.383
thing we know how to do,
pretty much,

01:14:17.383 --> 01:14:21.232
for the all pairs nonnegative
weights, now we can do it for

01:14:21.232 --> 01:14:24.018
general weights too,
which is a pretty nice

01:14:24.018 --> 01:14:28.000
combination of all the
techniques we've seen.

01:14:28.000 --> 01:14:30.217
In the trilogy,
and along the way,

01:14:30.217 --> 01:14:33.578
we saw lots of dynamic
programming, which is always

01:14:33.578 --> 01:14:35.459
good practice.
Any questions?

01:14:35.459 --> 01:14:38.954
This is the last new content
lecture before the quiz.

01:14:38.954 --> 01:14:42.852
On Wednesday it will be quiz
review, if I recall correctly.

01:14:42.852 --> 01:14:46.347
And then it's Thanksgiving,
so there's no recitation.

01:14:46.347 --> 01:14:48.632
And then the quiz starts on
Monday.

01:14:48.632 --> 01:14:50.365
So, study up.
See you then.

