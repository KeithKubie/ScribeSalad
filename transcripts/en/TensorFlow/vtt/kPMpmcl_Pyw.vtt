WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.470
[MUSIC PLAYING]

00:00:04.470 --> 00:00:08.010
MARTIN GORNER: Hi, everyone,
and thank you for being here

00:00:08.010 --> 00:00:11.340
at 8:30 in the morning,
and welcome to this session

00:00:11.340 --> 00:00:15.210
about TPUs and TPU pods.

00:00:15.210 --> 00:00:17.310
So those are custom
made accelerators

00:00:17.310 --> 00:00:20.430
that Google has designed to
accelerate machine learning

00:00:20.430 --> 00:00:21.600
workloads.

00:00:21.600 --> 00:00:27.180
And before I tell you everything
about them, me and Kaz,

00:00:27.180 --> 00:00:29.940
I would like to do something.

00:00:29.940 --> 00:00:33.510
Of course, this is live, so
you want to see a live demo.

00:00:33.510 --> 00:00:37.740
And I would like to train
with you here, onstage, using

00:00:37.740 --> 00:00:40.020
a TPU pod, one of
those big models

00:00:40.020 --> 00:00:42.090
that used to take days to train.

00:00:42.090 --> 00:00:45.840
And we'll see if we
can finish the training

00:00:45.840 --> 00:00:47.320
within this session.

00:00:47.320 --> 00:00:50.730
So let me start the training.

00:00:50.730 --> 00:00:54.850
I will come back to explaining
exactly what I'm doing here.

00:00:54.850 --> 00:00:59.190
I'm just starting it.

00:00:59.190 --> 00:01:00.840
Run all cells.

00:01:00.840 --> 00:01:01.770
Seems to be running.

00:01:04.440 --> 00:01:07.490
OK, I'm just checking.

00:01:07.490 --> 00:01:12.420
I'm running this on
a 128 core TPU pod.

00:01:12.420 --> 00:01:16.460
So one of the things
you see in the logs here

00:01:16.460 --> 00:01:18.530
is that I have all
my TPUs appearing.

00:01:18.530 --> 00:01:23.230
0, 1, 2, 6, and all
the way down to 128.

00:01:23.230 --> 00:01:25.350
All right, so this is running.

00:01:25.350 --> 00:01:27.320
I'm happy with it.

00:01:27.320 --> 00:01:28.970
Let's hear more about TPUs.

00:01:31.880 --> 00:01:37.680
So first of all, what is
this piece of silicon?

00:01:37.680 --> 00:01:41.260
And this is the demo
that I've just launched.

00:01:41.260 --> 00:01:43.680
It's an object
detection demo that

00:01:43.680 --> 00:01:48.420
is training on a wildlife
data set of 300,000 images.

00:01:48.420 --> 00:01:49.290
Why wildlife?

00:01:49.290 --> 00:01:53.020
Because I can show
you cute pandas.

00:01:53.020 --> 00:01:56.050
And I can show you cute
electronics as well.

00:01:56.050 --> 00:01:58.620
So this is a TPU v2.

00:01:58.620 --> 00:02:01.560
And we have a second
version now, a TPU v3.

00:02:01.560 --> 00:02:03.360
Those are fairly large boards.

00:02:03.360 --> 00:02:05.850
It's large like this, roughly.

00:02:05.850 --> 00:02:08.340
And as you can see, they
have four chips on them.

00:02:08.340 --> 00:02:10.650
Each chip is dual core,
so each of these boards

00:02:10.650 --> 00:02:14.620
has 8 TPU cores on them.

00:02:14.620 --> 00:02:17.810
And each core has two units.

00:02:17.810 --> 00:02:19.510
That is a vector
processing unit.

00:02:19.510 --> 00:02:23.110
That is a fairly standard
data-oriented processor,

00:02:23.110 --> 00:02:24.820
general purpose processor.

00:02:24.820 --> 00:02:27.640
What makes this special
for machine learning

00:02:27.640 --> 00:02:29.750
is the matrix multiply unit.

00:02:29.750 --> 00:02:33.700
TPUs have a built-in
hardware-based matrix

00:02:33.700 --> 00:02:39.500
multiplier that can multiply to
128 by 128 matrices in one go.

00:02:39.500 --> 00:02:43.430
So what is special
about this architecture?

00:02:43.430 --> 00:02:45.370
There are two
tricks that we used

00:02:45.370 --> 00:02:48.700
to make it fast and efficient.

00:02:48.700 --> 00:02:52.150
The first one is, I
would say, semi-standard.

00:02:52.150 --> 00:02:53.710
It's reduced precision.

00:02:53.710 --> 00:02:56.830
When you train neural networks,
reducing the precision

00:02:56.830 --> 00:03:01.570
from 32-bit floating
points to 16-bit

00:03:01.570 --> 00:03:05.920
is something that people
quite frequently do,

00:03:05.920 --> 00:03:09.460
because neural networks are
quite resistant to the loss

00:03:09.460 --> 00:03:10.600
of precision.

00:03:10.600 --> 00:03:12.350
Actually, it even
happens sometimes

00:03:12.350 --> 00:03:16.660
that the noise that is
introduced by reduced precision

00:03:16.660 --> 00:03:20.210
acts as a kind of regularizer
and helps with convergence.

00:03:20.210 --> 00:03:23.500
So sometimes you're even lucky
when you reduce precision.

00:03:23.500 --> 00:03:28.880
But then, as you see on this
chart, float16 and float32,

00:03:28.880 --> 00:03:31.970
the floating point formats,
they don't have the same number

00:03:31.970 --> 00:03:34.600
of exponent bits,
which means that they

00:03:34.600 --> 00:03:36.740
don't cover the same range.

00:03:36.740 --> 00:03:40.210
So when you take a model and
downgrade all your float32s

00:03:40.210 --> 00:03:43.900
into float16s, you might get
into underflow or overflow

00:03:43.900 --> 00:03:45.430
problems.

00:03:45.430 --> 00:03:48.190
And if it is your
model, it's usually not

00:03:48.190 --> 00:03:50.750
so hard to go in and fix.

00:03:50.750 --> 00:03:53.020
But if you're using
code from GitHub

00:03:53.020 --> 00:03:54.790
and you don't know
where to fix stuff,

00:03:54.790 --> 00:03:56.960
this might be very problematic.

00:03:56.960 --> 00:04:02.260
So that's why on TPUs,
we chose a different--

00:04:02.260 --> 00:04:04.570
actually we designed a
different floating point

00:04:04.570 --> 00:04:08.740
format called bfloat16.

00:04:08.740 --> 00:04:10.540
And as you can
see, it's actually

00:04:10.540 --> 00:04:15.790
exactly the same as float32
with just the fractional bits

00:04:15.790 --> 00:04:17.140
cut off.

00:04:17.140 --> 00:04:19.779
So the point is it has
exactly the same number

00:04:19.779 --> 00:04:22.390
of exponent bits,
exactly the same range.

00:04:22.390 --> 00:04:26.230
And therefore, usually,
it's a drop-in replacement

00:04:26.230 --> 00:04:30.100
for float32 and
reduced precision.

00:04:30.100 --> 00:04:32.500
So typically for
you, there is nothing

00:04:32.500 --> 00:04:37.150
to do on your model to benefit
from the speed of reduced

00:04:37.150 --> 00:04:37.990
precision.

00:04:37.990 --> 00:04:40.960
The TPU will do this
automatically, on ship,

00:04:40.960 --> 00:04:43.850
in hardware.

00:04:43.850 --> 00:04:46.580
And the second trick
is architectural.

00:04:46.580 --> 00:04:49.980
It's the design of this
matrix multiply unit.

00:04:49.980 --> 00:04:51.950
So that you understand
how this works,

00:04:51.950 --> 00:04:55.310
try to picture, in your
head, how to perform a matrix

00:04:55.310 --> 00:04:56.930
multiplication.

00:04:56.930 --> 00:05:00.980
And one result, one
point of the resulting

00:05:00.980 --> 00:05:07.160
matrix, try to remember calculus
from school, is a dot product.

00:05:07.160 --> 00:05:10.100
A dot product of one line
of one matrix and one column

00:05:10.100 --> 00:05:11.640
of the second matrix.

00:05:11.640 --> 00:05:13.210
Now what is a dot product?

00:05:13.210 --> 00:05:17.520
A dot product is a series of
multiply-accumulate operations,

00:05:17.520 --> 00:05:19.700
which means that the
only operation you

00:05:19.700 --> 00:05:22.100
need to perform a
matrix multiplication

00:05:22.100 --> 00:05:24.770
is multiply and accumulate.

00:05:24.770 --> 00:05:27.260
And multiply-accumulate
in 16 bits,

00:05:27.260 --> 00:05:31.460
because we're using
bfloat16 reduced precision.

00:05:31.460 --> 00:05:34.340
That is a tiny, tiny
piece of silicon.

00:05:34.340 --> 00:05:38.360
A 16-bit multiply-accumulator
is a tiny piece of silicon.

00:05:38.360 --> 00:05:44.040
And if you wire them together
as an array, as you see here.

00:05:44.040 --> 00:05:48.200
So this in real life would
be a 128 by 128 array.

00:05:48.200 --> 00:05:50.150
It's called a systolic array.

00:05:50.150 --> 00:05:53.450
Systolic in Greek means flow.

00:05:53.450 --> 00:05:56.150
Because you will flow
the data through it.

00:05:56.150 --> 00:05:59.570
So the way it works is that you
load one matrix into the array,

00:05:59.570 --> 00:06:03.410
and then you flow the second
matrix through the array.

00:06:03.410 --> 00:06:05.930
And you'll have to
believe me, or maybe

00:06:05.930 --> 00:06:08.390
spend a little bit more
time with the animation,

00:06:08.390 --> 00:06:11.660
by the time the gray
dots have finished

00:06:11.660 --> 00:06:14.150
flowing through those
multiply-accumulators,

00:06:14.150 --> 00:06:17.780
out of the right side come
all the dot products that

00:06:17.780 --> 00:06:19.700
make the resulting matrix.

00:06:19.700 --> 00:06:22.020
So it's a one-shot operation.

00:06:22.020 --> 00:06:25.400
There are no intermediate values
to store anywhere, in memory,

00:06:25.400 --> 00:06:26.570
in registers.

00:06:26.570 --> 00:06:28.970
All the intermediate
values flow on the wires

00:06:28.970 --> 00:06:31.850
from one compute unit to
the second compute units.

00:06:31.850 --> 00:06:33.440
It's very efficient.

00:06:33.440 --> 00:06:35.480
And what is more,
it's only made of

00:06:35.480 --> 00:06:38.070
those tiny 16-bit
multiply-accumulators,

00:06:38.070 --> 00:06:42.140
which means that we can cram
a lot of those into one chip.

00:06:42.140 --> 00:06:47.240
128 by 128 is 16,000
multiply-accumulators.

00:06:47.240 --> 00:06:50.440
And that's how much you get
in one TPU core, twice that

00:06:50.440 --> 00:06:52.990
in two TPU cores.

00:06:52.990 --> 00:06:56.140
So this is what makes it dense.

00:06:56.140 --> 00:06:58.150
Density means power efficiency.

00:06:58.150 --> 00:07:03.120
And power efficiency in
the data center means cost.

00:07:03.120 --> 00:07:05.620
And of course, you
want to know how cheap

00:07:05.620 --> 00:07:08.590
or how fast these things are.

00:07:08.590 --> 00:07:11.560
Some people might
remember from last year

00:07:11.560 --> 00:07:15.430
I did a talk about what I
built, this planespotting model,

00:07:15.430 --> 00:07:19.020
so I'm using this as
a benchmark today.

00:07:19.020 --> 00:07:21.820
And on Google
Cloud's AI platform,

00:07:21.820 --> 00:07:23.780
it's very easy to get
different configurations,

00:07:23.780 --> 00:07:27.190
so I can test how
fast this trains.

00:07:27.190 --> 00:07:30.090
My baseline is-- on a
fast GPU, this model

00:07:30.090 --> 00:07:33.510
trains in half in
four and a half hours.

00:07:33.510 --> 00:07:37.020
But I can also get 5
machines with powerful GPUs

00:07:37.020 --> 00:07:38.100
in a cluster.

00:07:38.100 --> 00:07:40.860
And on those five
machines, five GPUs,

00:07:40.860 --> 00:07:42.780
this model will
train in one hour.

00:07:42.780 --> 00:07:45.450
And I've chosen this number
because one hour is exactly

00:07:45.450 --> 00:07:51.240
the time it takes for this
model to train on one TPU v2.

00:07:51.240 --> 00:07:54.630
So the rule of thumb
I want you to remember

00:07:54.630 --> 00:08:00.270
is that roughly 1 TPU
v2, with its 4 chips,

00:08:00.270 --> 00:08:04.590
is roughly as fast as
five powerful GPUs.

00:08:04.590 --> 00:08:06.400
That's in terms of speed.

00:08:06.400 --> 00:08:10.290
But as you can see, it's
almost three times cheaper.

00:08:10.290 --> 00:08:12.930
And that's the point of
optimizing the architecture

00:08:12.930 --> 00:08:15.450
specifically for neural
network workloads.

00:08:18.730 --> 00:08:21.890
You might want to know how
this works in software as well.

00:08:21.890 --> 00:08:28.270
So when you're using TensorFlow,
or Keras in TensorFlow,

00:08:28.270 --> 00:08:31.720
your Python code
TensorFlow Python code

00:08:31.720 --> 00:08:34.030
generates a computational graph.

00:08:34.030 --> 00:08:35.860
That is how TensorFlow works.

00:08:35.860 --> 00:08:40.409
So your entire neural network
is represented as a graph.

00:08:40.409 --> 00:08:44.770
Now, this graph is what
is sent to the TPU.

00:08:44.770 --> 00:08:47.830
Your TPU does not
execute Python code.

00:08:47.830 --> 00:08:51.940
This graph is processed through
XLA, the Accelerated Linear

00:08:51.940 --> 00:08:54.820
Algebra compiler,
and that is how

00:08:54.820 --> 00:08:59.980
it becomes TPU microcode
to be executed on the TPU.

00:08:59.980 --> 00:09:03.790
And one nice side-effect
of this architecture

00:09:03.790 --> 00:09:06.940
is that if, in your
TensorFlow code,

00:09:06.940 --> 00:09:11.140
you load your data through
the standard tf.data.Dataset

00:09:11.140 --> 00:09:16.690
API, as you should, and as
is required with TPUs, then

00:09:16.690 --> 00:09:20.980
even the data loading part, or
imagery resizing, or whatever

00:09:20.980 --> 00:09:24.160
is in your data pipeline,
ends up in the graph,

00:09:24.160 --> 00:09:25.670
ends up executed on the TPU.

00:09:25.670 --> 00:09:29.440
And the TPU will be pulling
data from Google Cloud Storage

00:09:29.440 --> 00:09:30.740
directly during training.

00:09:30.740 --> 00:09:32.250
So that is very efficient.

00:09:35.540 --> 00:09:38.630
How do you actually
write this with code?

00:09:38.630 --> 00:09:40.810
So let me show you in Keras.

00:09:40.810 --> 00:09:45.760
And one caveat, this is
Keras in TensorFlow 1.14,

00:09:45.760 --> 00:09:48.340
which should be out
in these next days.

00:09:48.340 --> 00:09:52.300
The API is slightly
different in TensorFlow 1.13

00:09:52.300 --> 00:09:56.320
today, but I'd rather show
you the one that will be--

00:09:56.320 --> 00:09:59.660
the new one, as of
tomorrow or next week.

00:09:59.660 --> 00:10:02.380
So it's only a couple
of lines of code.

00:10:02.380 --> 00:10:05.020
There is the first line,
TPUClusterResolver.

00:10:05.020 --> 00:10:07.570
You can call it without
parameters on most platforms,

00:10:07.570 --> 00:10:09.850
and that finds
the connected TPU.

00:10:09.850 --> 00:10:13.800
The TPU is a
remotely-connected accelerator.

00:10:13.800 --> 00:10:15.020
This finds it.

00:10:15.020 --> 00:10:19.480
You initialize the TPU and then
you use the new distribution

00:10:19.480 --> 00:10:26.260
API in TensorFlow to define a
TPU strategy based on this TPU.

00:10:26.260 --> 00:10:29.500
And then you say
with strategy.scope,

00:10:29.500 --> 00:10:33.350
and everything that follows is
perfectly normal Keras code.

00:10:33.350 --> 00:10:35.860
Then you define your
model, you compile it,

00:10:35.860 --> 00:10:40.090
you do model.fit,
model.evaluate, model.predict,

00:10:40.090 --> 00:10:42.200
anything you're used
to doing in Keras.

00:10:42.200 --> 00:10:45.340
So in Keras, it's literally
these four lines of code

00:10:45.340 --> 00:10:46.120
to add--

00:10:46.120 --> 00:10:47.470
to work on a TPU.

00:10:47.470 --> 00:10:50.950
And I would like to point out
that these four lines of code

00:10:50.950 --> 00:10:55.600
also transform your model
into a distributed model.

00:10:55.600 --> 00:10:58.930
Remember a TPU,
even a single GPU,

00:10:58.930 --> 00:11:00.700
is a board with eight cores.

00:11:00.700 --> 00:11:02.860
So from the get go it's
distributed computing.

00:11:02.860 --> 00:11:06.760
And these four lines
of code put in place

00:11:06.760 --> 00:11:10.500
all the machinery of
distributed computing for you.

00:11:10.500 --> 00:11:12.760
One parameter to notice.

00:11:12.760 --> 00:11:14.350
You see in the TPU
strategy, there

00:11:14.350 --> 00:11:17.930
is the steps_per_run equals 100.

00:11:17.930 --> 00:11:19.420
So that's an optimization.

00:11:19.420 --> 00:11:25.120
This tells the TPU, please run
100 batches worth of training

00:11:25.120 --> 00:11:28.180
and don't report back
until you're finished.

00:11:28.180 --> 00:11:33.350
Because it's a network
attached accelerator,

00:11:33.350 --> 00:11:35.470
you don't want the TPU
to be reporting back

00:11:35.470 --> 00:11:39.200
after each batch for
performance reasons.

00:11:39.200 --> 00:11:41.780
So this is the software.

00:11:41.780 --> 00:11:44.630
If you don't want to
write your own code,

00:11:44.630 --> 00:11:46.050
I encourage you to do so.

00:11:46.050 --> 00:11:48.800
But if you don't, we
have a whole library

00:11:48.800 --> 00:11:51.450
of TPU optimized models.

00:11:51.450 --> 00:11:56.600
So you will find them on
the TensorFlow/tpu GitHub

00:11:56.600 --> 00:11:57.980
repository.

00:11:57.980 --> 00:12:02.000
And there is everything
in the image--

00:12:02.000 --> 00:12:05.000
in the vision space, in
the machine translation,

00:12:05.000 --> 00:12:08.870
and language, and NLP space,
in speech recognition.

00:12:08.870 --> 00:12:13.310
Even you can play
with GaN models.

00:12:13.310 --> 00:12:18.000
The one that we are
demoing on stage,

00:12:18.000 --> 00:12:22.470
remember we are training the
model right now, is RetinaNet.

00:12:22.470 --> 00:12:26.100
So this one is an
object detection model.

00:12:26.100 --> 00:12:30.310
And I like this model,
so let me say a few words

00:12:30.310 --> 00:12:32.320
about how this works.

00:12:32.320 --> 00:12:35.860
In object detection, you put
an image, and what you get

00:12:35.860 --> 00:12:37.030
is not just the label--

00:12:37.030 --> 00:12:38.780
this is a dog, this is a panda--

00:12:38.780 --> 00:12:43.220
but you actually get boxes
around where those objects are.

00:12:43.220 --> 00:12:45.940
In object detection
models, you have two kinds.

00:12:45.940 --> 00:12:47.680
There are one shot
detectors that

00:12:47.680 --> 00:12:50.590
are usually fast but
kind of inaccurate,

00:12:50.590 --> 00:12:54.190
and then two-stage
detectors that are much more

00:12:54.190 --> 00:12:56.110
accurate but much slower.

00:12:56.110 --> 00:13:00.100
And I like RetinaNet
because they actually

00:13:00.100 --> 00:13:04.210
found a trick to make
this both the fastest

00:13:04.210 --> 00:13:06.010
and the most accurate
model that you can

00:13:06.010 --> 00:13:08.840
find in object detection today.

00:13:08.840 --> 00:13:10.970
And it's a very simple trick.

00:13:10.970 --> 00:13:12.980
I'm not going to explain
all the math behind it,

00:13:12.980 --> 00:13:15.870
but basically in these
detection models,

00:13:15.870 --> 00:13:19.910
you start with
candidate detections.

00:13:19.910 --> 00:13:23.200
And then you prune them to
find only the detections--

00:13:23.200 --> 00:13:27.630
the boxes that have
actual objects in them.

00:13:27.630 --> 00:13:31.610
And the thing is that all
those blue boxes that you see,

00:13:31.610 --> 00:13:33.240
there is nothing in them.

00:13:33.240 --> 00:13:35.540
So even during training,
they will very easily

00:13:35.540 --> 00:13:40.310
be classified as nothing
to see, move along boxes,

00:13:40.310 --> 00:13:43.580
with a fairly small error.

00:13:43.580 --> 00:13:45.830
But you've got
loads of them, which

00:13:45.830 --> 00:13:50.210
means that when you compute the
loss of this model, in the loss

00:13:50.210 --> 00:13:53.540
you have a huge sum
of very small errors.

00:13:53.540 --> 00:13:56.090
And that huge sum of very
small errors might in the end

00:13:56.090 --> 00:13:59.580
be very big and overwhelm
the useful signal.

00:13:59.580 --> 00:14:02.900
So the two-stage
detectors resolve that

00:14:02.900 --> 00:14:06.110
by being much more careful
about those candidate boxes.

00:14:06.110 --> 00:14:07.760
In one-stage
detectors, you start

00:14:07.760 --> 00:14:10.250
with a host of candidate boxes.

00:14:10.250 --> 00:14:12.560
And the trick they
found in RetinaNet

00:14:12.560 --> 00:14:15.080
is a little mathematical
trick on the loss

00:14:15.080 --> 00:14:17.810
to make sure that the
contribution of all

00:14:17.810 --> 00:14:21.070
those easy boxes stays small.

00:14:21.070 --> 00:14:23.882
The upshot, it's both
fast and accurate.

00:14:27.920 --> 00:14:31.020
So let me go back here.

00:14:31.020 --> 00:14:36.320
I actually want to say a word
about now what I did, exactly,

00:14:36.320 --> 00:14:39.940
when I launched this demo.

00:14:39.940 --> 00:14:44.130
I guess most of you are familiar
with the Google Cloud Platform.

00:14:44.130 --> 00:14:48.060
So here I am opening the
Google Cloud Platform console.

00:14:48.060 --> 00:14:50.370
And in the Google
Cloud Platform,

00:14:50.370 --> 00:14:55.650
I have a tool called
AI platform, which,

00:14:55.650 --> 00:14:59.760
for those who know it, has had
a facility for running training

00:14:59.760 --> 00:15:03.270
jobs and for deploying
models behind the REST API

00:15:03.270 --> 00:15:04.600
for serving.

00:15:04.600 --> 00:15:07.900
But there is a new
functionality called Notebooks.

00:15:07.900 --> 00:15:12.600
In AI platform, you can today
provision ready all-installed

00:15:12.600 --> 00:15:16.220
notebook for working in--

00:15:19.690 --> 00:15:21.550
yeah, so let me
switch to this one--

00:15:21.550 --> 00:15:23.110
for working either
in TensorFlow,

00:15:23.110 --> 00:15:25.400
in PyTorch, with GPUs.

00:15:25.400 --> 00:15:27.340
It's literally a
one click operation.

00:15:27.340 --> 00:15:29.560
NEW INSTANCE, I want
a TensorFlow instance

00:15:29.560 --> 00:15:33.280
with Jupyter notebook
installed, and what you get

00:15:33.280 --> 00:15:37.930
is here an instance that is
running but with the link

00:15:37.930 --> 00:15:39.628
to open Jupyter.

00:15:39.628 --> 00:15:41.670
For example, this one--
and it will open Jupyter,

00:15:41.670 --> 00:15:42.770
but it's already open.

00:15:42.770 --> 00:15:46.240
So it's asking me to select
something else, but it's here.

00:15:46.240 --> 00:15:48.580
And here, you can
actually work normally

00:15:48.580 --> 00:15:52.750
in your Jupyter environment
with a powerful accelerator.

00:15:52.750 --> 00:15:56.380
You might have noticed
that I don't have a TPU

00:15:56.380 --> 00:15:58.660
option, actually
not here, but here,

00:15:58.660 --> 00:16:00.400
for adding an accelerator.

00:16:00.400 --> 00:16:01.930
That's coming.

00:16:01.930 --> 00:16:08.350
But here I am using
Jupyter notebook instances

00:16:08.350 --> 00:16:13.720
that are powered by a
TPU v3 128-core pod.

00:16:13.720 --> 00:16:14.950
How did I do it?

00:16:14.950 --> 00:16:17.200
It's actually possible
on the command line.

00:16:17.200 --> 00:16:18.580
I give you the
command line here.

00:16:18.580 --> 00:16:20.410
There is nothing fancy about it.

00:16:20.410 --> 00:16:22.870
There is one gcloud
compute command line

00:16:22.870 --> 00:16:26.530
to start to the instance and a
second gcloud compute command

00:16:26.530 --> 00:16:28.730
line to start the TPU.

00:16:28.730 --> 00:16:32.620
You provision a TPU just as
you would a virtual machine

00:16:32.620 --> 00:16:34.300
in Google's cloud.

00:16:34.300 --> 00:16:36.040
So this is what I've done.

00:16:36.040 --> 00:16:39.080
And that is what is
running right now.

00:16:39.080 --> 00:16:43.450
So let's see if what we are.

00:16:43.450 --> 00:16:46.150
Here it's still running.

00:16:46.150 --> 00:16:48.250
As you see enqueue
next 100 batches.

00:16:48.250 --> 00:16:49.000
And it's training.

00:16:49.000 --> 00:16:52.330
We are step 4,000
out of 6,000 roughly.

00:16:52.330 --> 00:16:59.470
So we'll check back on this
demo at the end of the session.

00:16:59.470 --> 00:17:03.890
This demo, when I was doing
it, to run it on stage,

00:17:03.890 --> 00:17:08.260
I've been able also to run a
comparison between how fast our

00:17:08.260 --> 00:17:11.460
TPU v3s versus v2s.

00:17:11.460 --> 00:17:17.410
In theory, v3s are roughly
twice as powerful as v2s,

00:17:17.410 --> 00:17:20.829
but that only works if
you feed them enough

00:17:20.829 --> 00:17:24.760
work to make use of
all the hardware.

00:17:24.760 --> 00:17:27.280
So here on RetinaNet,
you can train

00:17:27.280 --> 00:17:28.780
on images of various sizes.

00:17:28.780 --> 00:17:31.330
Of course, if you train
on smaller images, 256

00:17:31.330 --> 00:17:33.700
pixel images, it
will be much faster,

00:17:33.700 --> 00:17:36.260
in terms of images per second.

00:17:36.260 --> 00:17:37.620
And I've tried both--

00:17:37.620 --> 00:17:39.220
TPU v2s and v3s.

00:17:39.220 --> 00:17:41.860
You see with small images,
you get a little bump

00:17:41.860 --> 00:17:48.640
in performance from TPU v3s,
but nowhere near double.

00:17:48.640 --> 00:17:51.790
But as you get to bigger
and bigger images,

00:17:51.790 --> 00:17:54.790
you are feeding the
hardware with more work.

00:17:54.790 --> 00:17:59.800
And on 640 pixel images, the
speed up you get from TPU v3

00:17:59.800 --> 00:18:03.050
is getting close to the
theoretical x2 factor.

00:18:03.050 --> 00:18:07.210
So for this reason, I am
running this demo here

00:18:07.210 --> 00:18:12.110
at the 512 pixel image
size on a TPU v3 pod.

00:18:15.470 --> 00:18:17.570
I'm talking about pods.

00:18:17.570 --> 00:18:20.280
But what are these
pods, exactly?

00:18:20.280 --> 00:18:24.240
To show you more
about TPU pods, I

00:18:24.240 --> 00:18:27.900
would like to give
the lectern to Kaz.

00:18:27.900 --> 00:18:28.760
Thank you Kaz.

00:18:28.760 --> 00:18:29.927
KAZ SATO: Thank you, Martin.

00:18:29.927 --> 00:18:32.412
[APPLAUSE]

00:18:35.400 --> 00:18:39.540
So in my part, I directly
introduce Cloud TPU pods.

00:18:39.540 --> 00:18:41.550
What are pods?

00:18:41.550 --> 00:18:44.000
It's a large cluster
of Cloud TPUs.

00:18:44.000 --> 00:18:47.490
The version two pod is now
available as public beta, which

00:18:47.490 --> 00:18:53.890
provides 11.6 petaflops,
with 512 TPU cores.

00:18:53.890 --> 00:18:55.910
The next generation
version three pod

00:18:55.910 --> 00:18:59.000
is also public beta
now, which achieves

00:18:59.000 --> 00:19:04.110
over 100 petaflops
with 2,048 TPU cores

00:19:04.110 --> 00:19:06.590
So those performance numbers
are as high as the greatest

00:19:06.590 --> 00:19:07.680
supercomputers.

00:19:07.680 --> 00:19:10.910
So Cloud TPU pods
are AI supercomputer

00:19:10.910 --> 00:19:14.270
that Google have
built from scratch.

00:19:14.270 --> 00:19:15.950
But some of you
might think, what's

00:19:15.950 --> 00:19:19.730
the difference between a bunch
of TPU instances and a Cloud

00:19:19.730 --> 00:19:21.810
TPU pod?

00:19:21.810 --> 00:19:24.530
The difference is
the interconnect.

00:19:24.530 --> 00:19:27.500
Google has developed ultra
high-speed interconnect

00:19:27.500 --> 00:19:31.220
hardware derived from a
supercomputer technology,

00:19:31.220 --> 00:19:36.410
for connecting thousands of
TPUs with very short latency.

00:19:36.410 --> 00:19:38.810
What does it do for you?

00:19:38.810 --> 00:19:41.450
As you can see on the
animation, every time you update

00:19:41.450 --> 00:19:43.600
a single parameter
on a single TPU,

00:19:43.600 --> 00:19:46.550
that will be synchronized
with all the other thousands

00:19:46.550 --> 00:19:50.090
of TPUs, in an instant,
by the hardware.

00:19:50.090 --> 00:19:53.960
So in short, TensorFlow
users can use the whole pod

00:19:53.960 --> 00:19:56.440
as a single giant
machine with thousands

00:19:56.440 --> 00:19:58.160
of TPU cores inside it.

00:19:58.160 --> 00:20:02.090
It's as easy as using
a single computer.

00:20:02.090 --> 00:20:05.960
And you may wonder, because
it's an AI supercomputer,

00:20:05.960 --> 00:20:08.310
you may also take
super high cost.

00:20:08.310 --> 00:20:09.160
But it does not.

00:20:12.170 --> 00:20:14.200
You can get started
with using TPU pods

00:20:14.200 --> 00:20:19.790
with 32 cores at $24 per hour,
without any initial cost.

00:20:19.790 --> 00:20:22.160
So you don't have to
pay millions of dollars

00:20:22.160 --> 00:20:24.650
to build your own
supercomputer from scratch.

00:20:24.650 --> 00:20:29.680
You can just rent it for a
couple of hours from the cloud.

00:20:29.680 --> 00:20:33.500
Version three pod also can
be provisioned with 32 cores.

00:20:33.500 --> 00:20:37.170
That costs only $32 per hour.

00:20:37.170 --> 00:20:40.580
For larger sizes, you can
ask our service contact

00:20:40.580 --> 00:20:43.480
for the pricing.

00:20:43.480 --> 00:20:47.050
What is the cost benefit
of a TPU pods over GPUs?

00:20:47.050 --> 00:20:50.650
Here's a comparison result.
With a full version two pod,

00:20:50.650 --> 00:20:56.200
with 512 TPU cores, you can
train the same ResNet-50 models

00:20:56.200 --> 00:21:02.270
at 27 times faster
speed at 38% lower cost.

00:21:02.270 --> 00:21:06.040
This shows the clear
advantage of the TPU pods

00:21:06.040 --> 00:21:09.260
to a typical
GPU-based solutions.

00:21:09.260 --> 00:21:12.730
And there are other benefits
you could get from the TPU pods.

00:21:12.730 --> 00:21:16.360
Let's take a look
at eBay's case.

00:21:16.360 --> 00:21:20.050
eBay has over 1 billion
product listings.

00:21:20.050 --> 00:21:22.820
And to make it easier to
search specific products

00:21:22.820 --> 00:21:26.170
from 1 billion products, they
built a new visual search

00:21:26.170 --> 00:21:27.810
feature.

00:21:27.810 --> 00:21:31.310
And to train the models, they
have used 55 million images.

00:21:31.310 --> 00:21:34.080
So it's a really large
scale training for them.

00:21:34.080 --> 00:21:36.610
And they have used
Cloud TPU pods, and eBay

00:21:36.610 --> 00:21:40.030
was able to get a 100
times faster training time,

00:21:40.030 --> 00:21:43.120
compared with
existing GPU service.

00:21:43.120 --> 00:21:46.510
And they will also get
a 10% accuracy boost.

00:21:46.510 --> 00:21:47.980
Why is that?

00:21:47.980 --> 00:21:51.850
TPU itself is not designed
to increase the accuracy

00:21:51.850 --> 00:21:52.990
that much.

00:21:52.990 --> 00:21:56.830
But because if you can't
increase the training speed

00:21:56.830 --> 00:21:59.200
10 times or 100 times,
that means the data

00:21:59.200 --> 00:22:02.530
scientists or researchers can
have 10 times 100 times more

00:22:02.530 --> 00:22:05.690
iterations for the
trials, such as trying out

00:22:05.690 --> 00:22:08.110
a different combination
of the hyperparameters

00:22:08.110 --> 00:22:10.790
or different
preprocessings and so on.

00:22:10.790 --> 00:22:15.530
So that ended up at least 10%
accuracy boost in eBay's case.

00:22:15.530 --> 00:22:17.510
Let's see what kind
of TensorFlow code

00:22:17.510 --> 00:22:21.170
you would write to get those
benefits from TPU pods.

00:22:21.170 --> 00:22:23.020
And before taking a
look at the actual code,

00:22:23.020 --> 00:22:24.470
I try to look back.

00:22:24.470 --> 00:22:27.190
What are the efforts
required, in the past,

00:22:27.190 --> 00:22:31.930
to implement the large
scale distributed training?

00:22:31.930 --> 00:22:34.750
Using many GPUs or TPUs
for a single machine

00:22:34.750 --> 00:22:37.600
running training, that is
so-called distributed training.

00:22:37.600 --> 00:22:38.960
And there are two ways.

00:22:38.960 --> 00:22:42.370
One is data parallel and
another is model parallel.

00:22:42.370 --> 00:22:44.630
Let's talk about the
data parallel first.

00:22:44.630 --> 00:22:47.770
With data parallel, as you
can see on the diagram,

00:22:47.770 --> 00:22:50.580
you have to split the training
data into the multiple GPU

00:22:50.580 --> 00:22:52.300
or TPU nodes.

00:22:52.300 --> 00:22:56.040
And also you have to share the
same parameter set, the model.

00:22:56.040 --> 00:23:00.190
And to do that, you have to set
up a cluster of GPUs or TPUs

00:23:00.190 --> 00:23:01.080
by yourself.

00:23:01.080 --> 00:23:03.580
And also you have to set
up a parameter server that

00:23:03.580 --> 00:23:06.760
shares all the updates
of our parameters

00:23:06.760 --> 00:23:09.410
among all the GPU or TPUs.

00:23:09.410 --> 00:23:11.230
So it's a complex setup.

00:23:11.230 --> 00:23:13.790
And also in many
cases, you have to--

00:23:13.790 --> 00:23:17.030
there's going to be
synchronization overhead.

00:23:17.030 --> 00:23:18.940
So if you have hundreds
or as thousands

00:23:18.940 --> 00:23:21.850
of the TPUs or GPUs
in a single cluster,

00:23:21.850 --> 00:23:23.710
that's going to be a
huge overhead for that.

00:23:23.710 --> 00:23:27.910
And that limits the scalability.

00:23:27.910 --> 00:23:32.020
But with TPU pods, the
hardware takes care of it.

00:23:32.020 --> 00:23:33.920
Your high-speed
interconnect synchronizes

00:23:33.920 --> 00:23:36.520
all of the parameter
updates in a single TPU

00:23:36.520 --> 00:23:39.880
with the other thousands
of TPUs in an instant,

00:23:39.880 --> 00:23:41.180
with very short latency.

00:23:41.180 --> 00:23:43.570
So there's no need to set
up the parameter server,

00:23:43.570 --> 00:23:46.550
or there's no need to set
up the large cluster of GPUs

00:23:46.550 --> 00:23:48.230
by yourself.

00:23:48.230 --> 00:23:51.260
And also you can get
almost linear scalability

00:23:51.260 --> 00:23:55.100
to add more on the more
TPU cores in your training.

00:23:55.100 --> 00:23:57.200
And Martin will show you
the actual scalability

00:23:57.200 --> 00:23:59.410
result later.

00:23:59.410 --> 00:24:01.490
And as I mentioned
earlier, TensorFlow users

00:24:01.490 --> 00:24:06.260
can use the whole TPU pods
as a single giant computer

00:24:06.260 --> 00:24:09.720
and with thousands of
TPU cores inside it.

00:24:09.720 --> 00:24:14.090
So it's as easy as
using a single computer.

00:24:14.090 --> 00:24:17.060
For example, if you have Keras
code running on a single TPU,

00:24:17.060 --> 00:24:21.630
it also runs on a 2,000 TPU
cores without any changes.

00:24:21.630 --> 00:24:26.060
This is exactly the same
code Martin showed earlier.

00:24:26.060 --> 00:24:28.340
So under the hood, all the
complexity for the data

00:24:28.340 --> 00:24:30.650
parallel training, such
as splitting the training

00:24:30.650 --> 00:24:34.070
data into the multiple
TPUs, or the sharing

00:24:34.070 --> 00:24:36.260
the same parameters,
those are all

00:24:36.260 --> 00:24:39.080
taken care of by the
TPU pods' interconnect,

00:24:39.080 --> 00:24:42.080
and XLA compilers, and
the new TPUStrategy

00:24:42.080 --> 00:24:46.390
API in the TensorFlow 1.14.

00:24:46.390 --> 00:24:49.460
The one thing you may want
to change is the batch size.

00:24:49.460 --> 00:24:53.360
As Martin mentioned, a TPU
core has a matrix processor

00:24:53.360 --> 00:24:59.435
that has 128 by 128
matrix multipliers.

00:24:59.435 --> 00:25:01.310
So usually, you will
get the best performance

00:25:01.310 --> 00:25:05.200
by setting in the batch size
to 128 times the number of TPU

00:25:05.200 --> 00:25:06.140
cores.

00:25:06.140 --> 00:25:12.240
So if you have 10 TPU cores,
that's going to be 1,280.

00:25:12.240 --> 00:25:15.040
The benefit of TPU pods is
not only the training times.

00:25:15.040 --> 00:25:17.620
It also enables the
training of giant modules

00:25:17.620 --> 00:25:22.130
by using gear the
Mesh TensorFlow.

00:25:22.130 --> 00:25:25.040
Data parallel has been a popular
way of distributed training,

00:25:25.040 --> 00:25:27.350
but there's one downside.

00:25:27.350 --> 00:25:29.660
It cannot train a big model.

00:25:29.660 --> 00:25:32.690
Because all departments are
shared with all the GPUs

00:25:32.690 --> 00:25:36.530
or TPUs, you cannot bring a
big model that doesn't fit

00:25:36.530 --> 00:25:40.900
into the memory of a
single GPU or a TPU.

00:25:40.900 --> 00:25:43.380
So there's another way of
distributed training called

00:25:43.380 --> 00:25:44.930
a model parallel.

00:25:44.930 --> 00:25:47.230
With model parallel, you
can split the giant model

00:25:47.230 --> 00:25:49.960
into the multiple GPUs
or TPUs so that you

00:25:49.960 --> 00:25:51.790
can train much larger models.

00:25:51.790 --> 00:25:54.410
But that has not
been a popular way.

00:25:54.410 --> 00:25:55.030
Why?

00:25:55.030 --> 00:25:58.210
Because it's much
harder to implement.

00:25:58.210 --> 00:26:00.053
As you can see on
the diagrams, you

00:26:00.053 --> 00:26:01.720
have to implement all
the communications

00:26:01.720 --> 00:26:03.990
between the fraction
of the models.

00:26:03.990 --> 00:26:06.320
It's like stitching
between the models.

00:26:06.320 --> 00:26:09.220
And again, you have to set
up the complex cluster,

00:26:09.220 --> 00:26:10.900
and in many cases,
the communication

00:26:10.900 --> 00:26:12.550
between the models.

00:26:12.550 --> 00:26:15.190
Because if you have
hundreds of thousands

00:26:15.190 --> 00:26:18.370
of CPU or GPU or TPU
cores, then that's

00:26:18.370 --> 00:26:20.330
going to be a huge
overhead for that.

00:26:20.330 --> 00:26:22.780
So those are the reasons
why model parallel has not

00:26:22.780 --> 00:26:24.910
been so popular.

00:26:24.910 --> 00:26:27.190
To solve those problems,
TensorFlow team

00:26:27.190 --> 00:26:30.180
has developed a new library
called Mesh TensorFlow.

00:26:30.180 --> 00:26:32.313
It's a new way of
distributed training,

00:26:32.313 --> 00:26:33.730
with the multiple
computing nodes,

00:26:33.730 --> 00:26:38.690
such as TPU pods, or multiple
GPUs, or multiple CPUs.

00:26:38.690 --> 00:26:40.540
TensorFlow provides
an abstraction layer

00:26:40.540 --> 00:26:43.930
that sees those computing nodes
as a logical n-dimensional

00:26:43.930 --> 00:26:45.260
mesh.

00:26:45.260 --> 00:26:48.130
Mesh TensorFlow is now
available as open source

00:26:48.130 --> 00:26:51.790
code on the TensorFlow
GitHub repository.

00:26:51.790 --> 00:26:53.980
To see how it
works with imaging,

00:26:53.980 --> 00:26:56.200
you could have a
simple neural network

00:26:56.200 --> 00:26:59.140
like this for recognizing
the MNIST model.

00:26:59.140 --> 00:27:02.680
This network has the
batch size as 512,

00:27:02.680 --> 00:27:06.870
and data dimension as
784, and one hidden layer

00:27:06.870 --> 00:27:12.320
with 100 nodes, and
output as 10 classes.

00:27:12.320 --> 00:27:15.160
And if you want to train
that network with the model

00:27:15.160 --> 00:27:16.900
parallel, you can
just specify, I

00:27:16.900 --> 00:27:20.770
want to split the parameters
into four TPUs to the Mesh

00:27:20.770 --> 00:27:22.000
TensorFlow, and that's it.

00:27:22.000 --> 00:27:23.680
You don't have to
think about how

00:27:23.680 --> 00:27:26.980
to implement the communication
between the split model

00:27:26.980 --> 00:27:32.030
and how to worry about the
communication overhead.

00:27:32.030 --> 00:27:33.620
What kind of a code
you would write?

00:27:33.620 --> 00:27:36.378
Here is the code to
use the model parallel.

00:27:36.378 --> 00:27:38.170
At first, you have to
define the dimensions

00:27:38.170 --> 00:27:40.330
of both data and the model.

00:27:40.330 --> 00:27:44.980
In this code, you are defining
the batch dimension as 512,

00:27:44.980 --> 00:27:47.800
and the data has
a 784 dimensions,

00:27:47.800 --> 00:27:51.670
and hidden layer has 100
nodes, and the 10 classes.

00:27:51.670 --> 00:27:53.980
And then you define
your own network

00:27:53.980 --> 00:27:58.540
by using Mesh TensorFlow APIs,
such as two sets of weights

00:27:58.540 --> 00:28:02.470
and one hidden layers, and
one logits and loss function,

00:28:02.470 --> 00:28:04.660
by using those dimensions.

00:28:04.660 --> 00:28:09.430
Finally, you define how many
TPU or GPUs have in the mesh,

00:28:09.430 --> 00:28:12.090
and what is the layout
rule you want to use.

00:28:12.090 --> 00:28:15.190
In this code example, it
is defining a hidden layer

00:28:15.190 --> 00:28:18.860
dimensions for splitting the
model parameters into the four

00:28:18.860 --> 00:28:19.360
TPUs.

00:28:19.360 --> 00:28:21.400
And that's it.

00:28:21.400 --> 00:28:24.340
So that the Mesh TensorFlow
can take a look at this code

00:28:24.340 --> 00:28:28.150
and automatically split the
model parameters into the four

00:28:28.150 --> 00:28:28.910
TPUs.

00:28:28.910 --> 00:28:33.200
And it shares the same training
data with all the TPUs.

00:28:33.200 --> 00:28:36.480
You can also combine both
data and the model parallel.

00:28:36.480 --> 00:28:40.610
For example, you can define
the 2D mesh like this.

00:28:40.610 --> 00:28:43.730
And you use the rows of the
mesh for the data parallel

00:28:43.730 --> 00:28:46.820
and use the column of the
mesh or the model parallel,

00:28:46.820 --> 00:28:48.950
so that you can get the
benefits from both of them.

00:28:51.720 --> 00:28:54.210
And again, it's easy to
define with Mesh TensorFlow.

00:28:54.210 --> 00:28:56.150
You can just specify
batch dimension

00:28:56.150 --> 00:29:02.340
for the rows and hidden layer
dimensions for the columns.

00:29:02.340 --> 00:29:07.160
This is an example where
you are using the Mesh

00:29:07.160 --> 00:29:09.990
TensorFlow for training
a transformer model.

00:29:09.990 --> 00:29:13.170
Transformer model is a very
popular language model,

00:29:13.170 --> 00:29:17.380
and I don't go deeper into
the transformer model.

00:29:17.380 --> 00:29:19.860
But as you can see,
it's so easy to map

00:29:19.860 --> 00:29:22.740
each layer of a
transformer model

00:29:22.740 --> 00:29:25.380
to the layer load
of Mesh TensorFlow

00:29:25.380 --> 00:29:29.310
so that you can efficiently map
the large data and large model

00:29:29.310 --> 00:29:32.040
into the hundreds of
thousands of TPU cores

00:29:32.040 --> 00:29:34.810
by using Mesh TensorFlow.

00:29:34.810 --> 00:29:36.330
So what's the benefit?

00:29:36.330 --> 00:29:39.950
By using the Mesh TensorFlow
running with to TPU pods,

00:29:39.950 --> 00:29:43.230
the Google AI team was able
to train the language module

00:29:43.230 --> 00:29:47.130
and translation model with
the billion word scale.

00:29:47.130 --> 00:29:50.250
And they were able to achieve
the state-of-the-art scores,

00:29:50.250 --> 00:29:52.870
as you can see on those numbers.

00:29:52.870 --> 00:29:56.460
So for those use cases, the
larger the model, the better

00:29:56.460 --> 00:29:58.170
accuracy you get.

00:29:58.170 --> 00:30:01.800
The model parallel with TPU
pods give the big advantage

00:30:01.800 --> 00:30:06.330
on achieving those
state-of-the-art scores.

00:30:06.330 --> 00:30:09.430
Let's take a look at another use
case of the large scale model

00:30:09.430 --> 00:30:12.130
parallel I just call BigGAN.

00:30:12.130 --> 00:30:17.230
And I don't go deeper into what
is GAN or how the GAN works.

00:30:17.230 --> 00:30:19.660
But here's the basic idea.

00:30:19.660 --> 00:30:21.810
You have the two
defined networks.

00:30:21.810 --> 00:30:24.430
One is called
discriminator D and another

00:30:24.430 --> 00:30:28.230
is called generator G. And
you define a loss function

00:30:28.230 --> 00:30:33.900
so that the D to be trained to
recognize whether an image is

00:30:33.900 --> 00:30:36.680
a fake image or real image.

00:30:36.680 --> 00:30:39.990
And at the same time, the
generator will be trained

00:30:39.990 --> 00:30:43.200
to generate a realistic
image so that a D cannot find

00:30:43.200 --> 00:30:44.490
it's a fake.

00:30:44.490 --> 00:30:47.760
It's like a minimax game you
are playing with those two

00:30:47.760 --> 00:30:48.570
networks.

00:30:48.570 --> 00:30:51.300
And eventually, you
will have a generic G

00:30:51.300 --> 00:30:54.480
that can generate a
photo-realistic fake images,

00:30:54.480 --> 00:30:56.310
artificial images.

00:30:56.310 --> 00:30:59.740
Let's take a look
at the demo video.

00:30:59.740 --> 00:31:01.400
So this is not big spoiler.

00:31:01.400 --> 00:31:03.750
I have already loaded
the bigger models

00:31:03.750 --> 00:31:06.880
that is trained on the TPU pod.

00:31:06.880 --> 00:31:08.490
And as you can
see, these are all

00:31:08.490 --> 00:31:12.520
the artificial synthesized
image at high quality.

00:31:12.520 --> 00:31:14.340
You can also
specify the category

00:31:14.340 --> 00:31:17.040
of the generated
images, such as ostrich,

00:31:17.040 --> 00:31:20.680
so that you can generate
the ostrich images.

00:31:20.680 --> 00:31:23.370
These are all synthesized
artificial images.

00:31:23.370 --> 00:31:25.140
None of them are real.

00:31:25.140 --> 00:31:29.880
And because BigGAN can have
the so-called latent space that

00:31:29.880 --> 00:31:32.490
has the seeds to
generate those images,

00:31:32.490 --> 00:31:34.950
you can interpolate
between two seeds.

00:31:34.950 --> 00:31:37.620
In this example,
it is interpolating

00:31:37.620 --> 00:31:40.230
between golden
retriever and Lhasa.

00:31:40.230 --> 00:31:42.420
And you can try out a
different combination

00:31:42.420 --> 00:31:47.700
of the interpolation, such as
west highland white terrier

00:31:47.700 --> 00:31:49.360
and golden retriever.

00:31:49.360 --> 00:31:51.430
Again, those are
all fake images.

00:31:54.920 --> 00:31:58.330
So this bigger model was trained
with the TPU version three

00:31:58.330 --> 00:32:01.240
pod with 512 cores.

00:32:01.240 --> 00:32:05.580
And that took 24
hours to 48 hours.

00:32:05.580 --> 00:32:09.220
Why BigGAN takes so many
TPU cores and so long time?

00:32:09.220 --> 00:32:12.770
The reasons are the model
size and the batch size.

00:32:12.770 --> 00:32:16.740
The quality of a GAN model,
measured by GAN model,

00:32:16.740 --> 00:32:20.140
are measured by the
inception score, or IS score.

00:32:20.140 --> 00:32:22.566
That represents how
much an inception model

00:32:22.566 --> 00:32:25.270
thinks those images are real.

00:32:25.270 --> 00:32:29.575
And that also represents the
variety of generated images.

00:32:29.575 --> 00:32:32.470
The BigGAN paper
says that you get

00:32:32.470 --> 00:32:34.330
better IS score
when you are having

00:32:34.330 --> 00:32:36.670
more parameters in
the model and when

00:32:36.670 --> 00:32:39.670
you are using the larger
batch size for the training.

00:32:39.670 --> 00:32:42.100
So that means the larger
scale model parallel

00:32:42.100 --> 00:32:46.030
on the hundreds of TPU cores
is crucial for BigGAN model

00:32:46.030 --> 00:32:50.710
to increase the quality
of those generated images.

00:32:50.710 --> 00:32:52.930
So we have seen two use cases.

00:32:52.930 --> 00:32:56.020
A BigGAN use case and
language model use cases.

00:32:56.020 --> 00:32:58.480
And those are the first
applications of the model

00:32:58.480 --> 00:33:00.050
parallel on TPU pods.

00:33:00.050 --> 00:33:02.170
But they are only the starters.

00:33:02.170 --> 00:33:05.780
So TPU pods are available
to everyone from now.

00:33:05.780 --> 00:33:08.170
So we expect to see
more and more exciting

00:33:08.170 --> 00:33:11.920
use cases coming from
the new TPU pods users

00:33:11.920 --> 00:33:14.110
and also from the applications.

00:33:14.110 --> 00:33:15.580
So that's it for my part.

00:33:15.580 --> 00:33:16.210
Back to Martin.

00:33:21.330 --> 00:33:23.640
MARTIN GORNER: So now it's
time to check on our demo.

00:33:23.640 --> 00:33:26.360
Did our model actually train?

00:33:26.360 --> 00:33:31.730
Checking here, yeah, it looks
like it has finished training.

00:33:31.730 --> 00:33:34.660
A saved model has been saved.

00:33:34.660 --> 00:33:36.780
So the only thing
that is to do is

00:33:36.780 --> 00:33:40.110
to verify if this model can
actually predict something.

00:33:40.110 --> 00:33:47.640
So on a second machine I will
reload the exact same model.

00:33:47.640 --> 00:33:48.140
OK.

00:33:48.140 --> 00:33:50.570
I believe that's the one.

00:33:50.570 --> 00:33:53.590
And let's go and reload it.

00:33:53.590 --> 00:33:57.210
So I'll skip training
this time and just go here

00:33:57.210 --> 00:33:58.460
to inference and loading.

00:33:58.460 --> 00:33:59.710
Whoops, sorry about that.

00:34:04.230 --> 00:34:06.900
I just hope the demo gods
will be with me today.

00:34:14.040 --> 00:34:15.350
All right.

00:34:15.350 --> 00:34:18.620
That's because I'm loading
the wrong directory.

00:34:18.620 --> 00:34:22.760
The demo gods are
almost with me.

00:34:22.760 --> 00:34:25.940
It's this one where my
model has been saved.

00:34:30.653 --> 00:34:31.627
All right.

00:34:34.790 --> 00:34:35.290
Yes.

00:34:35.290 --> 00:34:35.830
Indeed.

00:34:35.830 --> 00:34:38.620
It wasn't the same.

00:34:38.620 --> 00:34:39.440
Sorry about that.

00:34:43.449 --> 00:34:46.590
No training, just inference.

00:34:46.590 --> 00:34:51.150
And this time, it looks
like my model is loading.

00:34:51.150 --> 00:34:55.290
And once it's loaded, I
will see if it can actually

00:34:55.290 --> 00:34:58.440
detect animals in
images, and here we are.

00:34:58.440 --> 00:35:00.300
So this leopard is
actually a leopard.

00:35:00.300 --> 00:35:02.190
This bird is a bird.

00:35:02.190 --> 00:35:03.330
The lion is a lion.

00:35:03.330 --> 00:35:04.690
This is a very tricky image.

00:35:04.690 --> 00:35:07.510
So I'm showing you not
cherry-picked images.

00:35:07.510 --> 00:35:10.260
This is a model I have trained
on stage, here with you.

00:35:10.260 --> 00:35:11.650
No model is perfect.

00:35:11.650 --> 00:35:14.360
We will see bad
detections, like this one.

00:35:14.360 --> 00:35:15.450
But that's a tricky one.

00:35:15.450 --> 00:35:16.570
It's artwork.

00:35:16.570 --> 00:35:19.050
It's not an actual lion.

00:35:19.050 --> 00:35:20.310
The leopard is spot on.

00:35:20.310 --> 00:35:21.270
The lion is spot on.

00:35:21.270 --> 00:35:24.880
And see that the boxing
actually works very well.

00:35:24.880 --> 00:35:29.380
The leopard has been perfectly
identified in the image.

00:35:29.380 --> 00:35:31.860
So let's move to something
more challenging.

00:35:31.860 --> 00:35:36.520
Even this inflatable artwork
lion has been identified,

00:35:36.520 --> 00:35:38.770
which is not always the case.

00:35:38.770 --> 00:35:40.170
This is a complicated image--

00:35:40.170 --> 00:35:41.515
a flock of birds.

00:35:41.515 --> 00:35:43.140
So you see it's not
seeing all of them.

00:35:43.140 --> 00:35:45.240
But all of them at
least are birds,

00:35:45.240 --> 00:35:47.340
which is a pretty good job.

00:35:47.340 --> 00:35:48.780
The leopard is fine.

00:35:48.780 --> 00:35:51.140
Oh, and this is the
most complex we have.

00:35:51.140 --> 00:35:53.430
There is a horse and cattle.

00:35:53.430 --> 00:35:57.180
Well, we start seeing a
couple of bad detections here.

00:35:57.180 --> 00:36:00.310
Of course, that
cow is not a pig.

00:36:00.310 --> 00:36:02.380
As I said, no model is perfect.

00:36:02.380 --> 00:36:04.500
But here the tiger
is the tiger, and we

00:36:04.500 --> 00:36:06.318
have our two cute pandas.

00:36:06.318 --> 00:36:08.610
And those two cute pandas
are actually quite difficult,

00:36:08.610 --> 00:36:10.332
because those are baby pandas.

00:36:10.332 --> 00:36:11.790
And I don't believe
that this model

00:36:11.790 --> 00:36:16.000
has had a lot of baby animals
in its 300,000 images data set.

00:36:16.000 --> 00:36:22.440
So I'm quite glad that it
managed to find the two pandas.

00:36:22.440 --> 00:36:27.270
So moving back, let me
finish by giving you

00:36:27.270 --> 00:36:30.580
a couple of feeds and
speeds on those models.

00:36:30.580 --> 00:36:35.730
So here, this model has
a RetinaNet 50 backbone,

00:36:35.730 --> 00:36:40.150
plus all the detection layers
that produced the boxes.

00:36:40.150 --> 00:36:47.080
And we have been training it
on a TPU v3 pod with 128 cores.

00:36:47.080 --> 00:36:49.930
It did finish in 20 minutes.

00:36:49.930 --> 00:36:52.380
You don't have to just
believe me for that.

00:36:52.380 --> 00:36:54.530
Let me show you.

00:36:54.530 --> 00:36:58.650
Here I had a timer
read my script.

00:36:58.650 --> 00:37:01.250
Yep, 19 minutes and 18 seconds.

00:37:01.250 --> 00:37:02.790
So I'm not cheating.

00:37:02.790 --> 00:37:04.590
This was live.

00:37:04.590 --> 00:37:07.860
But I could also have run
this model on a smaller pod.

00:37:07.860 --> 00:37:10.620
Actually, I tried
on a TPU v2-32.

00:37:10.620 --> 00:37:13.410
On this chart, you see
the speed on this axis

00:37:13.410 --> 00:37:16.770
and the time on this axis.

00:37:16.770 --> 00:37:20.280
This is to show you that
a TPU v2-32 is actually

00:37:20.280 --> 00:37:21.450
a very useful tool to have.

00:37:21.450 --> 00:37:24.240
We've been talking about
huge models up to now.

00:37:24.240 --> 00:37:28.190
But it's debatable whether
this is a huge model.

00:37:28.190 --> 00:37:32.160
This definitely was a
huge model a year ago.

00:37:32.160 --> 00:37:34.550
Today, with better
tools, I can train it

00:37:34.550 --> 00:37:38.560
in an hour on a fairly
modest TPU v2 32-core pod.

00:37:38.560 --> 00:37:43.170
So even as an individual
data scientist,

00:37:43.170 --> 00:37:46.740
that is a very useful tool
for me to have handy when

00:37:46.740 --> 00:37:50.490
I need to do a round of
trainings on a model like this,

00:37:50.490 --> 00:37:53.550
because someone wants an
animal detection model.

00:37:53.550 --> 00:37:56.700
And bringing the training
down to the one hour

00:37:56.700 --> 00:37:58.950
space, or 20 minutes
space, allows

00:37:58.950 --> 00:38:01.980
me to work a lot faster
and iterate a lot faster

00:38:01.980 --> 00:38:07.210
on the hyperparemeters, on
the fine tuning, and so on.

00:38:07.210 --> 00:38:11.290
You see on a single TPU
v3, it's the bottom line.

00:38:11.290 --> 00:38:15.040
And if we were to
train this on a GPU--

00:38:15.040 --> 00:38:18.750
so remember our rule of
thumb from the beginning.

00:38:18.750 --> 00:38:21.660
One TPU v2, roughly five GPUs.

00:38:21.660 --> 00:38:24.960
Therefore 1 TPU v3,
roughly 10 GPUs.

00:38:24.960 --> 00:38:28.200
So the GPU line would be
one tenth of the lowest

00:38:28.200 --> 00:38:29.040
line on this graph.

00:38:29.040 --> 00:38:31.500
I didn't put it because it
would barely register there.

00:38:31.500 --> 00:38:34.680
That shows you the
change of scale

00:38:34.680 --> 00:38:39.690
at which you can be training
your models using TPUs.

00:38:39.690 --> 00:38:41.980
You might be
wondering about this.

00:38:41.980 --> 00:38:45.990
So as you scale, one
thing that might happen

00:38:45.990 --> 00:38:49.300
is that you have to adjust
your learning rate schedule.

00:38:49.300 --> 00:38:51.540
So this is actually the
learning rate schedule

00:38:51.540 --> 00:38:57.360
I have used to train the
model on the 128 core TPU pod.

00:38:57.360 --> 00:38:59.310
Just a couple of words,
because it might not

00:38:59.310 --> 00:39:03.510
be the most usual learning rate
schedule you have ever seen.

00:39:03.510 --> 00:39:05.230
There is this ramp up.

00:39:05.230 --> 00:39:07.590
So the second part
is exponential decay.

00:39:07.590 --> 00:39:08.940
That's fairly standard.

00:39:08.940 --> 00:39:12.210
But the ramp up part,
that is because we

00:39:12.210 --> 00:39:16.290
are starting from
ResNet-50, initialized

00:39:16.290 --> 00:39:18.360
with pre-trained weights.

00:39:18.360 --> 00:39:21.090
But we still leave
those weights trainable.

00:39:21.090 --> 00:39:23.370
So we are training
the whole thing.

00:39:23.370 --> 00:39:25.090
It's not transfer learning.

00:39:25.090 --> 00:39:28.950
It's just fine tuning of
pre-trained ResNet-50.

00:39:28.950 --> 00:39:32.910
And when you do
that, and you train

00:39:32.910 --> 00:39:36.330
very fast, using big
batches, as we do here,

00:39:36.330 --> 00:39:40.900
the batch size here
is 64 times 128.

00:39:40.900 --> 00:39:43.840
So it's a very big batch size.

00:39:43.840 --> 00:39:46.960
You might actually break
those pre-trained weights

00:39:46.960 --> 00:39:49.850
in ways that harm
your precision.

00:39:49.850 --> 00:39:53.890
So that's why it's quite
usual to have a ramp up period

00:39:53.890 --> 00:39:57.190
to make sure that the network,
in its initial training

00:39:57.190 --> 00:39:59.710
phases, when it doesn't
know what it's doing,

00:39:59.710 --> 00:40:02.080
does not completely
destroy the information

00:40:02.080 --> 00:40:05.510
in the pre-trained weights.

00:40:05.510 --> 00:40:07.220
So we did it.

00:40:07.220 --> 00:40:11.010
We did train this model
here on stage in 20 minutes.

00:40:11.010 --> 00:40:15.060
And the demo worked, I'm
really glad about that.

00:40:15.060 --> 00:40:16.400
So this is the end.

00:40:16.400 --> 00:40:20.780
What we have seen is
TPUs and TPU pods.

00:40:20.780 --> 00:40:21.890
Fast, yes.

00:40:21.890 --> 00:40:23.990
But mostly cost effective.

00:40:23.990 --> 00:40:25.850
Very cost effective
way and a good tool

00:40:25.850 --> 00:40:28.220
to have for any data scientist.

00:40:28.220 --> 00:40:31.810
Also, and more specifically
for very large models,

00:40:31.810 --> 00:40:34.910
but for what used to be
large models in the past

00:40:34.910 --> 00:40:38.370
and which are normal models
today, such as a ResNet-50

00:40:38.370 --> 00:40:39.620
[INAUDIBLE].

00:40:39.620 --> 00:40:41.720
It's a very useful tools.

00:40:41.720 --> 00:40:46.460
And then Cloud TPU pods,
where you can actually

00:40:46.460 --> 00:40:50.240
enable not only data,
but model parallelism,

00:40:50.240 --> 00:40:54.750
using this new library
called Mesh TensorFlow.

00:40:54.750 --> 00:40:58.460
A couple of links here
with more information

00:40:58.460 --> 00:41:01.280
if you would like to know more.

00:41:01.280 --> 00:41:04.830
Yes, you can take a picture.

00:41:04.830 --> 00:41:08.660
And if you have
more questions, we

00:41:08.660 --> 00:41:12.620
will be at the AI
ML pod, the red one,

00:41:12.620 --> 00:41:16.840
in front of one TPU rack.

00:41:16.840 --> 00:41:19.730
So you can see this
one live and get

00:41:19.730 --> 00:41:22.290
a feel for what kind
of computer it is.

00:41:22.290 --> 00:41:24.470
And with that,
thank you very much.

00:41:24.470 --> 00:41:25.970
[APPLAUSE]

00:41:27.470 --> 00:41:31.120
[MUSIC PLAYING]

