WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.388
[LOGO MUSIC]

00:00:05.245 --> 00:00:06.870
KONSTANTINOS KATSIAPIS:
Hello everyone.

00:00:06.870 --> 00:00:08.240
My name is Gus Katsiapis.

00:00:08.240 --> 00:00:11.990
And together with my colleagues
Kevin Haas and Tulsee Doshi,

00:00:11.990 --> 00:00:15.680
we will talk to you today
about TensorFlow Extended,

00:00:15.680 --> 00:00:18.900
and the topic covers two areas.

00:00:18.900 --> 00:00:21.260
Let's go into that

00:00:21.260 --> 00:00:24.200
I think most of you here that
have used ML in the real world

00:00:24.200 --> 00:00:28.490
realize that machine learning
is a lot more than just a model.

00:00:28.490 --> 00:00:31.250
It's a lot more than
just training a model.

00:00:31.250 --> 00:00:34.190
And especially, when machine
learning powers your business

00:00:34.190 --> 00:00:37.520
or powers a product that
actually affects your users,

00:00:37.520 --> 00:00:40.010
you absolutely need
the reliability.

00:00:40.010 --> 00:00:43.160
So, today, we'll talk
about how, in the face

00:00:43.160 --> 00:00:45.420
of massive data
and the real world,

00:00:45.420 --> 00:00:47.088
how do you build
applications that

00:00:47.088 --> 00:00:49.130
use machine learning that
are robust to the world

00:00:49.130 --> 00:00:52.190
that they operate in?

00:00:52.190 --> 00:00:55.350
And today's talk will
actually have two parts.

00:00:55.350 --> 00:00:58.400
The first part will be about
TensorFlow Extended, otherwise

00:00:58.400 --> 00:00:59.700
known as TFX.

00:00:59.700 --> 00:01:01.970
This is an end-to-end
machine learning platform.

00:01:01.970 --> 00:01:03.470
And the second part
of the talk will

00:01:03.470 --> 00:01:06.560
talk about model understanding
and how you can actually

00:01:06.560 --> 00:01:10.280
get insights into your
business by understanding

00:01:10.280 --> 00:01:13.370
how your model performs
in real-world situations.

00:01:16.050 --> 00:01:16.550
OK.

00:01:16.550 --> 00:01:19.220
So let's get started with
the first part, TensorFlow

00:01:19.220 --> 00:01:22.130
Extended, otherwise
known as TFX.

00:01:22.130 --> 00:01:23.815
We built TFX at Google.

00:01:23.815 --> 00:01:25.940
We started building it
approximately two and a half

00:01:25.940 --> 00:01:26.970
years ago.

00:01:26.970 --> 00:01:30.350
And a lot of the knowledge that
went into building TFX actually

00:01:30.350 --> 00:01:32.750
came from experience we
had building other machine

00:01:32.750 --> 00:01:35.180
learning platforms within
Google that preceded it,

00:01:35.180 --> 00:01:37.160
that preceded TensorFlow even.

00:01:37.160 --> 00:01:40.550
So TFX has had a profound
impact to Google,

00:01:40.550 --> 00:01:44.000
and it's used throughout
several Alphabet companies

00:01:44.000 --> 00:01:48.002
and also in several products
within Google itself.

00:01:48.002 --> 00:01:50.210
And several of those products
that you can see here--

00:01:50.210 --> 00:01:53.240
like Gmail or Add, et
cetera, or YouTube--

00:01:53.240 --> 00:01:54.350
have pretty large scale.

00:01:54.350 --> 00:01:57.060
So they affect billions
of users, et cetera.

00:01:57.060 --> 00:01:58.790
So this is one
more reason for us

00:01:58.790 --> 00:02:01.490
to pay extra attention
to building systems that

00:02:01.490 --> 00:02:02.885
use ML reliably.

00:02:06.440 --> 00:02:09.750
Now, when we started
building TFX,

00:02:09.750 --> 00:02:11.440
we had published
a paper about it,

00:02:11.440 --> 00:02:13.260
and we promised we
would eventually

00:02:13.260 --> 00:02:15.270
make it available to
the rest of the world.

00:02:15.270 --> 00:02:16.860
So over the last
few years, we've

00:02:16.860 --> 00:02:20.400
been open sourcing aspects of
it, and several of our partners

00:02:20.400 --> 00:02:22.470
externally have actually
been pretty successful

00:02:22.470 --> 00:02:24.720
deploying this technology,
several of the libraries we

00:02:24.720 --> 00:02:26.290
have offered over time.

00:02:26.290 --> 00:02:30.300
So just to call out an
interesting case study

00:02:30.300 --> 00:02:33.990
that Twitter made, they actually
made a fascinating blog post

00:02:33.990 --> 00:02:36.990
where they spoke about how they
ranked tweets with TensorFlow

00:02:36.990 --> 00:02:39.300
and how they used
TensorFlow Hub in order

00:02:39.300 --> 00:02:43.680
to do transfer learning
and shared word embeddings

00:02:43.680 --> 00:02:45.660
and share them within
their organization.

00:02:45.660 --> 00:02:48.600
And they also showcased how they
use TensorFlow Model Analysis

00:02:48.600 --> 00:02:51.450
in order to have a better
understanding of their model--

00:02:51.450 --> 00:02:54.060
how their model performs
not just globally

00:02:54.060 --> 00:02:57.690
over the population, but several
slices of their population

00:02:57.690 --> 00:02:59.280
that were important
to the business.

00:02:59.280 --> 00:03:00.540
So we'll be talking
about more of this

00:03:00.540 --> 00:03:02.707
later, especially with the
model understanding talk.

00:03:06.600 --> 00:03:10.620
Now I think most of you here
are either software developers

00:03:10.620 --> 00:03:12.720
or software engineers
or are very much

00:03:12.720 --> 00:03:15.370
familiar with software
processes and technologies.

00:03:15.370 --> 00:03:17.160
So I think most of
you probably recognize

00:03:17.160 --> 00:03:19.520
several of the themes
presented in this slide,

00:03:19.520 --> 00:03:23.610
like scalability, extensibility,
modularity, et cetera.

00:03:23.610 --> 00:03:26.340
But, my conjecture is
that most people think

00:03:26.340 --> 00:03:31.690
about those concepts in terms of
code and how to build software.

00:03:31.690 --> 00:03:35.420
Now, with the advent
of machine learning,

00:03:35.420 --> 00:03:36.970
we are building
applications that

00:03:36.970 --> 00:03:38.553
are powered by machine
learning, which

00:03:38.553 --> 00:03:41.200
means that those applications
are powered by data--

00:03:41.200 --> 00:03:43.250
fundamentally are
powered by data.

00:03:43.250 --> 00:03:47.172
So if you just think about code
and you don't think about data,

00:03:47.172 --> 00:03:49.130
you're only thinking
about half of the picture,

00:03:49.130 --> 00:03:50.620
50% of the picture.

00:03:50.620 --> 00:03:52.447
So you can optimize
one amazingly.

00:03:52.447 --> 00:03:54.280
But if you don't think
about the other half,

00:03:54.280 --> 00:03:56.020
you cannot be better
than the half--

00:03:56.020 --> 00:03:57.260
than the half itself.

00:03:57.260 --> 00:03:59.920
So I would actually
encourage everyone just

00:03:59.920 --> 00:04:02.350
to take each of those
concepts and see,

00:04:02.350 --> 00:04:06.490
how does this concept apply to
data as opposed to just code?

00:04:06.490 --> 00:04:09.370
And if you can apply these
concepts to both data and code,

00:04:09.370 --> 00:04:11.710
you can build a
holistic application

00:04:11.710 --> 00:04:15.462
that is actually robust
and powers your products.

00:04:15.462 --> 00:04:17.170
So we will actually
go into each of those

00:04:17.170 --> 00:04:20.660
individually and see how they
apply to machine learning.

00:04:20.660 --> 00:04:21.160
OK.

00:04:21.160 --> 00:04:24.010
Let's start with scalability.

00:04:24.010 --> 00:04:27.505
Most of you know that, when
you start your business,

00:04:27.505 --> 00:04:29.060
it might be small.

00:04:29.060 --> 00:04:31.310
But the reality is that,
as your business grows,

00:04:31.310 --> 00:04:33.040
so might your data.

00:04:33.040 --> 00:04:34.660
So, ideally, you
want a solution that

00:04:34.660 --> 00:04:40.000
is able to over time scale
together with your business.

00:04:40.000 --> 00:04:42.820
Ideally, you would be able
to write a single library

00:04:42.820 --> 00:04:44.230
or software--
piece of software--

00:04:44.230 --> 00:04:46.450
and that's also could
operate on a laptop

00:04:46.450 --> 00:04:48.340
because you want to
experiment quickly,

00:04:48.340 --> 00:04:50.620
but it could also operate
on a beefy machine

00:04:50.620 --> 00:04:54.070
with tons of processors
or a ton of accelerators.

00:04:54.070 --> 00:04:56.560
And you could also scale it
over hundreds or thousands

00:04:56.560 --> 00:05:00.020
of machines if you need to.

00:05:00.020 --> 00:05:03.580
So this flexibility in terms
of scale is quite important.

00:05:03.580 --> 00:05:07.300
And ideally, each time you hop
from kilobytes to megabytes

00:05:07.300 --> 00:05:09.460
to gigabytes to
terabytes, ideally

00:05:09.460 --> 00:05:12.400
you wouldn't have to use
different tools because you

00:05:12.400 --> 00:05:15.520
have a huge learning curve each
time you change your technology

00:05:15.520 --> 00:05:16.810
under the covers.

00:05:16.810 --> 00:05:19.780
So the ideal here is to have a
machine learning platform that

00:05:19.780 --> 00:05:23.600
is able to work on your
laptop but can also scale it

00:05:23.600 --> 00:05:24.790
on any cloud you would like.

00:05:27.800 --> 00:05:29.090
OK.

00:05:29.090 --> 00:05:31.010
Now, let's talk
about accessibility.

00:05:31.010 --> 00:05:36.173
So everyone here
understands that you

00:05:36.173 --> 00:05:38.090
can have libraries and
components that make up

00:05:38.090 --> 00:05:40.100
your system, and
you can have things

00:05:40.100 --> 00:05:41.448
that work out of the box.

00:05:41.448 --> 00:05:43.490
But, you always want to
customize it a little bit

00:05:43.490 --> 00:05:44.630
to meet your goals.

00:05:44.630 --> 00:05:47.180
You always want to put custom
business logic in some part

00:05:47.180 --> 00:05:49.420
of your application.

00:05:49.420 --> 00:05:51.890
And this is similar
for machine learning.

00:05:51.890 --> 00:05:54.123
So if you think about
the concrete example,

00:05:54.123 --> 00:05:56.040
when you fit data into
machine learning model,

00:05:56.040 --> 00:05:57.530
you need to do multiple
transformations

00:05:57.530 --> 00:05:59.655
to put the data in a format
that the model expects.

00:06:02.010 --> 00:06:05.430
So as a developer of
an ML application,

00:06:05.430 --> 00:06:07.470
you want to have the
transformation flexibility

00:06:07.470 --> 00:06:10.177
that an ML platform
can provide to you--

00:06:10.177 --> 00:06:12.260
whether that's bucketizing,
creating vocabularies,

00:06:12.260 --> 00:06:12.760
et cetera.

00:06:12.760 --> 00:06:15.780
And that's just one example,
but this applies pervasively

00:06:15.780 --> 00:06:19.100
throughout the ML process.

00:06:19.100 --> 00:06:19.600
OK.

00:06:19.600 --> 00:06:22.315
Let's talk a little
bit about modularity.

00:06:22.315 --> 00:06:24.190
All of you probably
understand the importance

00:06:24.190 --> 00:06:26.800
of having nice APIs and
reusable libraries that

00:06:26.800 --> 00:06:30.160
allow you to build bigger
and bigger systems.

00:06:30.160 --> 00:06:32.450
But, going back to
our original question,

00:06:32.450 --> 00:06:36.520
how does this apply to artifacts
produced by machine learning

00:06:36.520 --> 00:06:37.580
pipelines?

00:06:37.580 --> 00:06:40.000
How does this apply to data?

00:06:40.000 --> 00:06:44.410
So ideally, I would be able to
reuse the reusable components

00:06:44.410 --> 00:06:48.080
of a model that was trained
to recognize images and take

00:06:48.080 --> 00:06:48.580
that part--

00:06:48.580 --> 00:06:49.960
the reusable part of it--

00:06:49.960 --> 00:06:54.340
and put it in my model that
predicts kinds of chairs.

00:06:54.340 --> 00:06:59.770
So ideally, we would be able to
reuse parts of models as easy

00:06:59.770 --> 00:07:01.720
as it would be to
reuse libraries.

00:07:01.720 --> 00:07:04.810
So check out TensorFlow
Hub, which actually allows

00:07:04.810 --> 00:07:08.680
you to reuse the reusable parts
of machine learning models

00:07:08.680 --> 00:07:11.200
and plug them into your
own infrastructure.

00:07:11.200 --> 00:07:15.490
And going a step further, how
does this apply to artifacts?

00:07:15.490 --> 00:07:17.230
So machine learning
platforms usually

00:07:17.230 --> 00:07:19.540
produce lots of data
artifacts, whether that's

00:07:19.540 --> 00:07:22.000
statistics about your
data or something else.

00:07:22.000 --> 00:07:25.150
And many times, those operate
in a continuous fashion.

00:07:25.150 --> 00:07:27.340
So data continuously
arrives into the system,

00:07:27.340 --> 00:07:28.840
and you have to
continuously produce

00:07:28.840 --> 00:07:30.940
models that mimic
reality quickly,

00:07:30.940 --> 00:07:33.620
that understand reality faster.

00:07:33.620 --> 00:07:36.560
And, if you have to redo
computation from scratch,

00:07:36.560 --> 00:07:40.000
then it means that you can
sometimes not follow real time.

00:07:40.000 --> 00:07:42.520
So somehow you need to be
able to take artifacts that

00:07:42.520 --> 00:07:45.100
are produced over time
and merge them easily,

00:07:45.100 --> 00:07:46.990
quickly together so
that you can follow

00:07:46.990 --> 00:07:51.020
real time as new data arrives.

00:07:51.020 --> 00:07:51.520
OK.

00:07:51.520 --> 00:07:53.470
Moving on to the
stability, most of us

00:07:53.470 --> 00:07:56.650
are familiar with unit tests,
integration tests, regression

00:07:56.650 --> 00:07:58.290
tests, performance tests.

00:07:58.290 --> 00:08:00.580
All of those are about code.

00:08:00.580 --> 00:08:04.790
What does it mean to
write a test about data?

00:08:04.790 --> 00:08:07.780
ML and data are very
much intertwined.

00:08:07.780 --> 00:08:11.587
What is the equivalent of a unit
test or an integration test?

00:08:11.587 --> 00:08:13.420
If we take a step back,
when we write tests,

00:08:13.420 --> 00:08:17.270
we encode expectations
of what happens in code.

00:08:17.270 --> 00:08:19.810
So when we deal with
applications that have both

00:08:19.810 --> 00:08:23.200
code and data, we need to write
expectations both in terms

00:08:23.200 --> 00:08:25.240
of the code-- how
the code behaves--

00:08:25.240 --> 00:08:28.540
and in terms of what is
the shape of the data

00:08:28.540 --> 00:08:32.600
that goes into this black
box, this black process.

00:08:32.600 --> 00:08:35.830
So I would say that the
equivalent of a unit test

00:08:35.830 --> 00:08:38.590
or an integration test for
data is writing expectations

00:08:38.590 --> 00:08:41.710
about types of the data
that goes into your system,

00:08:41.710 --> 00:08:45.108
the distributions you expect,
the values that are allowed,

00:08:45.108 --> 00:08:46.900
the values that are
not allowed, et cetera.

00:08:46.900 --> 00:08:49.960
And we'll be discussing
about those later.

00:08:49.960 --> 00:08:53.380
If we take this a step
further, code oftentimes

00:08:53.380 --> 00:08:56.110
gives us very strong contracts.

00:08:56.110 --> 00:08:57.610
When you have a
sorting algorithm,

00:08:57.610 --> 00:08:59.290
you can expect that
it will do exactly

00:08:59.290 --> 00:09:01.170
what the contract promises.

00:09:01.170 --> 00:09:03.760
When you have a
machine learning model,

00:09:03.760 --> 00:09:06.820
basically you can think of it
as a function that was generated

00:09:06.820 --> 00:09:08.590
automatically through data.

00:09:08.590 --> 00:09:13.150
So you don't have as strong
contracts as we had before.

00:09:13.150 --> 00:09:18.910
So in order to set expectations
about how those black box

00:09:18.910 --> 00:09:22.270
functions that were
created by data behave,

00:09:22.270 --> 00:09:25.300
we need to set expectations
about what the data that

00:09:25.300 --> 00:09:27.100
went into them was.

00:09:27.100 --> 00:09:29.350
And this relates a lot
to the previous stability

00:09:29.350 --> 00:09:30.502
point we mentioned.

00:09:33.740 --> 00:09:35.330
OK.

00:09:35.330 --> 00:09:38.240
Moving on a little bit more
from a systems view perspective,

00:09:38.240 --> 00:09:40.230
ideally when you
use an application,

00:09:40.230 --> 00:09:42.260
you won't have to code
everything yourself.

00:09:42.260 --> 00:09:45.140
You would be able to reuse
several components out

00:09:45.140 --> 00:09:48.170
of the box that accept
well-defined configuration,

00:09:48.170 --> 00:09:49.820
and the configuration
is ideally also

00:09:49.820 --> 00:09:52.460
flexible enough for you to
parameterize it a little bit

00:09:52.460 --> 00:09:54.450
and customize it to your needs.

00:09:54.450 --> 00:09:57.560
So reuse as many black boxes
as possible, but touch them up

00:09:57.560 --> 00:09:58.490
when you need to.

00:09:58.490 --> 00:10:01.403
This is very similar
to machine learning.

00:10:01.403 --> 00:10:03.320
Ideally, I wouldn't have
to rebuild everything

00:10:03.320 --> 00:10:04.130
from scratch.

00:10:04.130 --> 00:10:07.190
I would be able to reuse
components, libraries, modules,

00:10:07.190 --> 00:10:08.120
pipelines.

00:10:08.120 --> 00:10:09.560
And I would be
able to share them

00:10:09.560 --> 00:10:14.680
with multiple folks within
my business or publicly.

00:10:14.680 --> 00:10:16.785
OK.

00:10:16.785 --> 00:10:18.660
Now, once you have a
system and it's actually

00:10:18.660 --> 00:10:20.880
responsible for the
performance of your product

00:10:20.880 --> 00:10:23.730
or your business, you need
to know what's going on.

00:10:23.730 --> 00:10:26.190
So you need to be able to
continuously monitor it and get

00:10:26.190 --> 00:10:29.490
alerted when things are not
moving as one would expect.

00:10:29.490 --> 00:10:31.740
And, once again, here
I would say that data

00:10:31.740 --> 00:10:33.720
is a first-class citizen.

00:10:33.720 --> 00:10:38.290
So unexpected changes to the
data coming into your system--

00:10:38.290 --> 00:10:40.800
whether that's your training
system or your serving system--

00:10:40.800 --> 00:10:42.750
need to be monitored
because, otherwise,

00:10:42.750 --> 00:10:45.960
you cannot know what the
behavior of the system will be

00:10:45.960 --> 00:10:48.630
especially in the absence
of those strong contracts we

00:10:48.630 --> 00:10:49.410
discussed earlier.

00:10:52.960 --> 00:10:56.570
So both data need to be
first-class citizens,

00:10:56.570 --> 00:10:59.135
and both models need to
be first-class citizens.

00:10:59.135 --> 00:11:00.510
And we need to be
able to monitor

00:11:00.510 --> 00:11:02.510
the performance of the
models as well over time.

00:11:05.400 --> 00:11:10.470
Now, if we apply those concepts,
we can build better software.

00:11:10.470 --> 00:11:11.970
And if we apply the
concepts we just

00:11:11.970 --> 00:11:13.530
discussed to
machine learning, we

00:11:13.530 --> 00:11:17.770
can build better software
that employs machine learning.

00:11:17.770 --> 00:11:19.950
So this should, in
principle, allow

00:11:19.950 --> 00:11:22.830
us to build software
that is safer.

00:11:22.830 --> 00:11:25.830
And safe is a very
generic word, but let

00:11:25.830 --> 00:11:27.780
me try to define it
a little bit here.

00:11:27.780 --> 00:11:29.400
I would call it
maybe a robustness

00:11:29.400 --> 00:11:31.400
to environment changes.

00:11:31.400 --> 00:11:34.663
So, as the data that
undergirds your system changes,

00:11:34.663 --> 00:11:36.330
you should be able
to have a system that

00:11:36.330 --> 00:11:39.300
is robust to it, that
performs properly

00:11:39.300 --> 00:11:42.570
in light of those changes, or
at least you get notified when

00:11:42.570 --> 00:11:45.240
those changes happen so that
you change the system to become

00:11:45.240 --> 00:11:48.060
more robust in the future.

00:11:48.060 --> 00:11:50.410
And how do we do this?

00:11:50.410 --> 00:11:52.398
It's actually hard.

00:11:52.398 --> 00:11:53.940
I think the reality
is that all of us

00:11:53.940 --> 00:11:55.815
build on collective
knowledge and experience.

00:11:55.815 --> 00:11:57.750
When we reuse
libraries, we build

00:11:57.750 --> 00:12:00.540
on the shoulders of giants
that built those libraries.

00:12:00.540 --> 00:12:03.160
And machine learning
is not any different.

00:12:03.160 --> 00:12:05.620
I think the world
outside is complex.

00:12:05.620 --> 00:12:08.190
And if you build any system
that tries to mimic it,

00:12:08.190 --> 00:12:10.260
it has to mirror some
of the complexities.

00:12:10.260 --> 00:12:12.420
Or, if you build a system
that tries to predict it,

00:12:12.420 --> 00:12:15.220
it has to be able to mirror
some of those complexities.

00:12:15.220 --> 00:12:18.210
So what comes to the rescue
here is, I would say,

00:12:18.210 --> 00:12:19.590
automation and best practices.

00:12:19.590 --> 00:12:23.460
So if we apply some best
practices for machine learning

00:12:23.460 --> 00:12:27.030
that have been proven to
be useful in various other

00:12:27.030 --> 00:12:29.550
circumstances, they're probably
useful for your business

00:12:29.550 --> 00:12:30.610
as well.

00:12:30.610 --> 00:12:33.060
And many of those
are difficult. We

00:12:33.060 --> 00:12:37.448
oftentimes spend days or
months debugging situations.

00:12:37.448 --> 00:12:39.990
And then, once we are able to
do that, we can encode the best

00:12:39.990 --> 00:12:42.630
practices we learn
into the ML software

00:12:42.630 --> 00:12:44.910
so that you don't need
to reinvent the wheel,

00:12:44.910 --> 00:12:46.920
basically, in this area.

00:12:46.920 --> 00:12:48.900
And the key here
is that learning

00:12:48.900 --> 00:12:52.050
from the pitfalls of others
is very important to building

00:12:52.050 --> 00:12:55.120
your own system.

00:12:55.120 --> 00:12:56.550
OK.

00:12:56.550 --> 00:12:58.620
So how do we achieve
those in machine learning?

00:12:58.620 --> 00:13:00.760
Let's look into a
typical user journey

00:13:00.760 --> 00:13:04.210
of using ML in a product
and what this looks like.

00:13:04.210 --> 00:13:05.960
In the beginning, you
have data ingestion.

00:13:05.960 --> 00:13:08.270
As we discussed,
data and code are

00:13:08.270 --> 00:13:10.470
tightly intertwined
in machine learning.

00:13:10.470 --> 00:13:13.490
And sometimes, in order to
change your machine algorithm,

00:13:13.490 --> 00:13:15.843
you have to change your
data and vice versa.

00:13:15.843 --> 00:13:17.510
So these need to be
tightly intertwined,

00:13:17.510 --> 00:13:20.390
and you need something that
brings the data into the system

00:13:20.390 --> 00:13:23.090
and applies the best practices
we discussed earlier.

00:13:23.090 --> 00:13:26.000
So it shuffles the training data
so that downstream processes

00:13:26.000 --> 00:13:28.610
can operate efficiently
and learn faster.

00:13:28.610 --> 00:13:30.680
Or, it splits your
data into the training

00:13:30.680 --> 00:13:32.700
and evolved set in
a way that make sure

00:13:32.700 --> 00:13:36.590
is that there is no leakage of
information during this split.

00:13:36.590 --> 00:13:38.660
For example, if you have
a financial application,

00:13:38.660 --> 00:13:40.970
you want to make sure
that the future does not

00:13:40.970 --> 00:13:43.070
look into the past where
you're training, just

00:13:43.070 --> 00:13:44.740
as a current example.

00:13:44.740 --> 00:13:45.650
OK.

00:13:45.650 --> 00:13:47.347
Once we are able to
generate some data,

00:13:47.347 --> 00:13:49.430
we need to make sure that
data is of good quality.

00:13:49.430 --> 00:13:50.447
And why is that?

00:13:50.447 --> 00:13:51.530
The answer is very simple.

00:13:51.530 --> 00:13:53.420
As we discussed,
garbage in, garbage out.

00:13:53.420 --> 00:13:55.520
This applies especially
well to machine

00:13:55.520 --> 00:13:58.560
learning because ML is
this kind of black box

00:13:58.560 --> 00:13:59.970
that is very complicated.

00:13:59.970 --> 00:14:02.520
So if you put things in that
you don't quite understand,

00:14:02.520 --> 00:14:04.520
there is no way for you
to be able to understand

00:14:04.520 --> 00:14:06.110
the output of the model.

00:14:06.110 --> 00:14:07.970
So data understanding
is actually

00:14:07.970 --> 00:14:10.100
required for model
understanding.

00:14:10.100 --> 00:14:12.840
We'll be talking about
this later as well.

00:14:12.840 --> 00:14:15.228
Another thing here is
that, if you catch errors

00:14:15.228 --> 00:14:17.520
as early as possible, that
is critical for your machine

00:14:17.520 --> 00:14:19.610
learning application.

00:14:19.610 --> 00:14:21.950
You reduce your wasted
time because now you've

00:14:21.950 --> 00:14:23.660
identified the
error early on where

00:14:23.660 --> 00:14:26.000
it's easier to actually spot.

00:14:26.000 --> 00:14:28.850
And you actually decrease
the amount of computation

00:14:28.850 --> 00:14:30.080
you perform.

00:14:30.080 --> 00:14:32.300
I don't need to train
a very expensive model

00:14:32.300 --> 00:14:34.490
if the data that's going
into it is garbage.

00:14:34.490 --> 00:14:36.210
So this is key.

00:14:36.210 --> 00:14:38.840
And I think the TL;DR here
is that you should treat data

00:14:38.840 --> 00:14:39.960
as you treat code.

00:14:39.960 --> 00:14:42.810
They're a first-class citizen
in your ML application.

00:14:42.810 --> 00:14:43.310
OK.

00:14:43.310 --> 00:14:44.750
Once you have your
data, sometimes you

00:14:44.750 --> 00:14:46.310
need to massage it
in order to fit it

00:14:46.310 --> 00:14:48.140
into your machine
learning algorithm.

00:14:48.140 --> 00:14:54.080
So oftentimes, you might
need to build vocabularies

00:14:54.080 --> 00:14:56.450
or you need to normalize
your constants in order

00:14:56.450 --> 00:14:59.032
to fit into your neural network
or your linear algorithm.

00:14:59.032 --> 00:15:00.490
And, in order to
do that, you often

00:15:00.490 --> 00:15:03.060
need full passes over the data.

00:15:03.060 --> 00:15:05.315
So the key thing is
that, when you train,

00:15:05.315 --> 00:15:06.940
you do these full
passes over the data.

00:15:06.940 --> 00:15:09.680
But when you serve, when
you evaluate your model,

00:15:09.680 --> 00:15:11.760
you actually have one
prediction at a time.

00:15:11.760 --> 00:15:14.860
So how can we create the
hermetic representation

00:15:14.860 --> 00:15:17.110
of a transformation that
requires the full possibility

00:15:17.110 --> 00:15:21.140
of data and be able to apply
that hermetic presentation

00:15:21.140 --> 00:15:23.960
at serving time so that
my training and serving

00:15:23.960 --> 00:15:25.940
way of doing things
is not different?

00:15:25.940 --> 00:15:28.230
If those are different, my
model would predict bogus.

00:15:28.230 --> 00:15:30.920
So we need to have processes
that ensure those things are

00:15:30.920 --> 00:15:32.450
hermetic and equivalent.

00:15:32.450 --> 00:15:36.360
And ideally, you would have a
system that does that for you.

00:15:36.360 --> 00:15:36.860
OK.

00:15:36.860 --> 00:15:38.780
Now that we have
data, now that we

00:15:38.780 --> 00:15:40.250
have data that is
of good quality

00:15:40.250 --> 00:15:41.540
because we've
validated it, and now

00:15:41.540 --> 00:15:42.915
that we have
transformations that

00:15:42.915 --> 00:15:44.670
allows us to fill the
data into the model,

00:15:44.670 --> 00:15:45.740
let's train the model.

00:15:45.740 --> 00:15:48.020
That's where the magic
happens, or so we think,

00:15:48.020 --> 00:15:52.080
when in reality everything else
before it is actually needed.

00:15:52.080 --> 00:15:54.117
But the chain doesn't stop here.

00:15:54.117 --> 00:15:55.700
Once you produce a
model, you actually

00:15:55.700 --> 00:15:57.450
need to validate the
quality of the model.

00:15:57.450 --> 00:15:59.630
You need to make sure
it passes a threshold

00:15:59.630 --> 00:16:01.588
that you think are
sufficient for your business

00:16:01.588 --> 00:16:03.500
to operate in.

00:16:03.500 --> 00:16:06.440
And ideally, you would do
this not just globally--

00:16:06.440 --> 00:16:09.800
how does this model perform on
the total population of data--

00:16:09.800 --> 00:16:13.670
but also how it performs on each
individual slice of the user

00:16:13.670 --> 00:16:15.320
base you care about--

00:16:15.320 --> 00:16:16.938
whether that's
different countries

00:16:16.938 --> 00:16:18.980
or different populations
or different geographies

00:16:18.980 --> 00:16:19.873
or whatever.

00:16:19.873 --> 00:16:21.290
So, ideally, you
would have a view

00:16:21.290 --> 00:16:23.300
not just how the model
does holistically,

00:16:23.300 --> 00:16:26.870
but in each slice where
you're interested in.

00:16:26.870 --> 00:16:28.910
Once you've performed
this validation,

00:16:28.910 --> 00:16:32.010
you now have something that
we think is of good quality,

00:16:32.010 --> 00:16:34.730
but we want to have a separation
between our training system--

00:16:34.730 --> 00:16:37.160
which oftentimes
operates at large scale

00:16:37.160 --> 00:16:40.520
and at high throughput--
from our serving system that

00:16:40.520 --> 00:16:42.500
actually operates
with low latency.

00:16:42.500 --> 00:16:44.060
When you try to
evaluate the model,

00:16:44.060 --> 00:16:45.935
sometimes you want a
prediction immediately--

00:16:45.935 --> 00:16:47.993
within seconds or
milliseconds even.

00:16:47.993 --> 00:16:49.910
So you need a separation
between your training

00:16:49.910 --> 00:16:53.180
part of the system and your
serving part of the system,

00:16:53.180 --> 00:16:56.370
and you need clear
boundaries between those two.

00:16:56.370 --> 00:16:58.160
So you need something
that takes a model,

00:16:58.160 --> 00:16:59.840
decides whether
it's good or not,

00:16:59.840 --> 00:17:02.150
and then pushes it
into production.

00:17:02.150 --> 00:17:04.010
Once your model
is in production,

00:17:04.010 --> 00:17:06.290
you're now able to make
predictions and improve

00:17:06.290 --> 00:17:07.410
your business.

00:17:07.410 --> 00:17:09.482
So as you can see,
machine learning

00:17:09.482 --> 00:17:10.940
is not about the
middle part there.

00:17:10.940 --> 00:17:12.530
This is not about
training your model.

00:17:12.530 --> 00:17:14.930
Machine learning is about
the end-to-end thing

00:17:14.930 --> 00:17:17.119
of using it in order to
improve an application

00:17:17.119 --> 00:17:20.010
or improve a product.

00:17:20.010 --> 00:17:24.740
So TFX has, over the
course of several years,

00:17:24.740 --> 00:17:26.900
open sourced several
libraries and components

00:17:26.900 --> 00:17:28.700
that make use of
those libraries.

00:17:28.700 --> 00:17:30.408
And the libraries are
very, very modular,

00:17:30.408 --> 00:17:32.367
going back to the previous
things we discussed.

00:17:32.367 --> 00:17:34.580
And they can be stitched
together into your existing

00:17:34.580 --> 00:17:35.730
infrastructure.

00:17:35.730 --> 00:17:38.390
But, we also offer components
that understand the context--

00:17:38.390 --> 00:17:41.097
and Kevin will be talking
about this later--

00:17:41.097 --> 00:17:42.680
we understand the
context they are in,

00:17:42.680 --> 00:17:43.820
and they can connect
to each other

00:17:43.820 --> 00:17:46.580
and operate in unison as opposed
to operating as single things.

00:17:46.580 --> 00:17:52.160
And we also offer
some horizontal layers

00:17:52.160 --> 00:17:55.680
that allow you to do those
connections, both in terms

00:17:55.680 --> 00:17:57.600
of configuration-- like
you have a single way

00:17:57.600 --> 00:18:00.870
to configure your pipeline--
and in terms of data

00:18:00.870 --> 00:18:02.520
storage and metadata storage.

00:18:02.520 --> 00:18:05.280
So those components
understand both the state

00:18:05.280 --> 00:18:07.530
of the world, what exists
there, and how they can

00:18:07.530 --> 00:18:08.738
be connected with each other.

00:18:11.420 --> 00:18:13.365
And we also offer
an entrance system.

00:18:13.365 --> 00:18:14.740
So we give you
the libraries that

00:18:14.740 --> 00:18:16.930
allow you to build your
system if you want, build

00:18:16.930 --> 00:18:20.092
your car if you will, but
we also offer a car itself.

00:18:20.092 --> 00:18:21.550
So we offer an
entrance system that

00:18:21.550 --> 00:18:22.880
gives you well-defined
configuration,

00:18:22.880 --> 00:18:24.380
simple configuration
for you to use.

00:18:24.380 --> 00:18:26.680
It has several
components that work out

00:18:26.680 --> 00:18:29.450
of the box to help you with the
user journey we just discussed,

00:18:29.450 --> 00:18:31.450
which as we saw has
multiple phases.

00:18:31.450 --> 00:18:34.785
So we have components that
work for each of those phases.

00:18:34.785 --> 00:18:36.160
And we also have
a metadata store

00:18:36.160 --> 00:18:39.310
in the bottom that allows
you to track basically

00:18:39.310 --> 00:18:42.707
everything that's happening in
this machine learning platform.

00:18:42.707 --> 00:18:44.290
And, with this, I
would like to invite

00:18:44.290 --> 00:18:45.850
Kevin Haas to talk
a little bit more

00:18:45.850 --> 00:18:47.300
about the effects
and the details.

00:18:47.300 --> 00:18:48.400
KEVIN HAAS: Thanks, Gus.

00:18:48.400 --> 00:18:49.775
So my name is
Kevin Haas, and I'm

00:18:49.775 --> 00:18:51.740
going to talk about
the internals of TFX.

00:18:51.740 --> 00:18:53.620
First a little bit of
audience participation.

00:18:53.620 --> 00:18:55.150
How many out there know Python?

00:18:55.150 --> 00:18:56.600
Raise your hands.

00:18:56.600 --> 00:18:57.100
All right.

00:18:57.100 --> 00:18:58.400
Quite a few.

00:18:58.400 --> 00:18:59.530
How many know TensorFlow?

00:18:59.530 --> 00:19:00.813
Raise your hands.

00:19:00.813 --> 00:19:01.480
Oh, quite a few.

00:19:01.480 --> 00:19:02.290
That's really good.

00:19:02.290 --> 00:19:03.832
And how many of you
want to see code?

00:19:03.832 --> 00:19:05.138
Raise your hands.

00:19:05.138 --> 00:19:06.930
A lot less than the
people who know Python.

00:19:06.930 --> 00:19:07.270
OK.

00:19:07.270 --> 00:19:09.353
So I'm going to get to
code in about five minutes.

00:19:09.353 --> 00:19:12.490
First, I want to talk a
little bit about what TFX is.

00:19:12.490 --> 00:19:14.830
First off, let's talk
about the component.

00:19:14.830 --> 00:19:16.540
The component is the
basic building block

00:19:16.540 --> 00:19:17.890
of all of our pipelines.

00:19:17.890 --> 00:19:21.530
When you think about a pipeline,
it's an assembly of components.

00:19:21.530 --> 00:19:24.250
So, using this, we
have ModelValidator.

00:19:24.250 --> 00:19:26.200
ModelValidator is one
of our components,

00:19:26.200 --> 00:19:28.468
and it is responsible
for taking two models--

00:19:28.468 --> 00:19:30.760
the model that we just trained
as part of this pipeline

00:19:30.760 --> 00:19:33.400
execution and the model
that runs in production.

00:19:33.400 --> 00:19:36.010
We then measure the accuracy
of both of these models.

00:19:36.010 --> 00:19:37.690
And if the
newly-trained model is

00:19:37.690 --> 00:19:39.250
better than the
model in production,

00:19:39.250 --> 00:19:40.708
we then tell a
downstream component

00:19:40.708 --> 00:19:42.500
to push that model
to production.

00:19:42.500 --> 00:19:45.070
So what we have here is
two inputs-- two models--

00:19:45.070 --> 00:19:47.470
and an output-- a decision
whether or not to push.

00:19:47.470 --> 00:19:49.360
The challenge here
is the model that's

00:19:49.360 --> 00:19:50.950
in production has
not been trained

00:19:50.950 --> 00:19:52.840
by this execution
of the pipeline.

00:19:52.840 --> 00:19:55.240
It may not have been trained
by this pipeline at all.

00:19:55.240 --> 00:19:57.100
So we don't have a
reference to this.

00:19:57.100 --> 00:19:58.892
Now, an easy answer
would be, oh, just hard

00:19:58.892 --> 00:20:00.267
code it somewhere
in your config,

00:20:00.267 --> 00:20:02.273
but we want to
avoid that as well.

00:20:02.273 --> 00:20:04.690
So we get around this by using
another project like Google

00:20:04.690 --> 00:20:06.780
put together called ML Metadata.

00:20:06.780 --> 00:20:08.560
With ML Metadata,
we're able to get

00:20:08.560 --> 00:20:10.750
the context of all
prior executions

00:20:10.750 --> 00:20:12.490
and all prior artifacts.

00:20:12.490 --> 00:20:14.720
This really helps us a
lot in being able to say,

00:20:14.720 --> 00:20:16.220
what is the current
production model

00:20:16.220 --> 00:20:18.400
because, we can query
the ML Metadata store

00:20:18.400 --> 00:20:20.470
and ask for the
URI, the artifact,

00:20:20.470 --> 00:20:21.860
of the production model.

00:20:21.860 --> 00:20:24.190
Once we have that, then
now we have both inputs

00:20:24.190 --> 00:20:27.880
that are able to go to the
validator to do our testing.

00:20:27.880 --> 00:20:30.910
Interestingly enough, it's
not just the production model

00:20:30.910 --> 00:20:34.000
that we get from ML Metadata,
but we get all of our inputs

00:20:34.000 --> 00:20:35.230
from ML Metadata.

00:20:35.230 --> 00:20:38.240
If you look here, the trainer
when it emits a new model

00:20:38.240 --> 00:20:39.372
writes it to ML Metadata.

00:20:39.372 --> 00:20:41.455
It does not pass it directly
to the next component

00:20:41.455 --> 00:20:42.760
of ModelValidator.

00:20:42.760 --> 00:20:45.400
When ModelValidator starts
up, it gets both of the models

00:20:45.400 --> 00:20:47.935
from ML Metadata, and then
it writes back the validation

00:20:47.935 --> 00:20:50.708
outcome back to the
ML Metadata store.

00:20:50.708 --> 00:20:52.250
What this does is
it does two things.

00:20:52.250 --> 00:20:55.390
One, it allows us to compose
our pipelines a lot more loosely

00:20:55.390 --> 00:20:57.120
and so have tightly
coupled pipelines.

00:20:57.120 --> 00:20:59.782
And two, it separates
the orchestration layer

00:20:59.782 --> 00:21:01.990
from how we manage state
and the rest of our metadata

00:21:01.990 --> 00:21:04.610
management.

00:21:04.610 --> 00:21:07.840
So a little bit more about
how a component is configured.

00:21:07.840 --> 00:21:11.060
So we have three main
phases of our component.

00:21:11.060 --> 00:21:12.610
It's a very specific
design pattern

00:21:12.610 --> 00:21:14.380
that we've used for
all of TFX, and we

00:21:14.380 --> 00:21:17.110
find it works really well with
machine learning pipelines.

00:21:17.110 --> 00:21:18.520
First, we have the driver.

00:21:18.520 --> 00:21:20.710
The driver is responsible
for additional scheduling

00:21:20.710 --> 00:21:22.900
and orchestration that
we may use to determine

00:21:22.900 --> 00:21:24.340
how the executor behaves.

00:21:24.340 --> 00:21:25.840
For example, if
we're asked to train

00:21:25.840 --> 00:21:28.990
a model using the very
same examples as before,

00:21:28.990 --> 00:21:31.090
the very same inputs
as before, the very

00:21:31.090 --> 00:21:32.830
same runtime
parameters as before,

00:21:32.830 --> 00:21:34.570
and the same version
of the estimator,

00:21:34.570 --> 00:21:35.670
we can kind of guess
that we're going

00:21:35.670 --> 00:21:37.870
to end up with the same
model that we started with.

00:21:37.870 --> 00:21:41.230
As a result, we may choose
to skip the execution

00:21:41.230 --> 00:21:43.160
and return back a
cached artifact.

00:21:43.160 --> 00:21:45.640
This saves us both
time and compute.

00:21:45.640 --> 00:21:47.440
Next, we have the executor.

00:21:47.440 --> 00:21:49.120
What the executor
is responsible for

00:21:49.120 --> 00:21:50.980
is the business logic
of the component.

00:21:50.980 --> 00:21:52.480
In the case of the
ModelValidator,

00:21:52.480 --> 00:21:54.892
this is where we test both
models and make a decision.

00:21:54.892 --> 00:21:56.350
In the case of the
trainer, this is

00:21:56.350 --> 00:21:58.240
where we train the
model using TensorFlow.

00:21:58.240 --> 00:22:00.640
Going all the way back to the
beginning of the pipeline,

00:22:00.640 --> 00:22:02.500
in the case of
example.jm, this is

00:22:02.500 --> 00:22:05.620
where we extract the data
out of either a data store

00:22:05.620 --> 00:22:08.072
or out of BigQuery or
out of the file system

00:22:08.072 --> 00:22:10.030
in order to generate the
examples that the rest

00:22:10.030 --> 00:22:11.770
of the pipeline trains with.

00:22:11.770 --> 00:22:13.653
Finally we have the
publishing phase.

00:22:13.653 --> 00:22:15.820
The publishing phase is
responsible for writing back

00:22:15.820 --> 00:22:17.860
whatever happened
in this component

00:22:17.860 --> 00:22:19.630
back to the metadata store.

00:22:19.630 --> 00:22:23.530
So if the driver decided to
skip work, we write that back.

00:22:23.530 --> 00:22:25.690
If an executor created
one or more new artifacts,

00:22:25.690 --> 00:22:29.450
we also write that
back to ML Metadata.

00:22:29.450 --> 00:22:31.310
So artifacts and
metadata management

00:22:31.310 --> 00:22:34.863
are a crucial element of
managing our ML pipelines.

00:22:34.863 --> 00:22:37.280
If you look at this, this is
a very simple task dependency

00:22:37.280 --> 00:22:37.780
graph.

00:22:37.780 --> 00:22:38.780
We have transform.

00:22:38.780 --> 00:22:40.340
When it's done,
it calls trainer.

00:22:40.340 --> 00:22:42.230
It's a very simple
finish-to-start dependency

00:22:42.230 --> 00:22:43.130
graph.

00:22:43.130 --> 00:22:45.020
So while we can model
our pipelines this way,

00:22:45.020 --> 00:22:47.690
we don't because task
dependencies alone

00:22:47.690 --> 00:22:50.360
are not the right way
to model our goals.

00:22:50.360 --> 00:22:53.030
Instead, we actually look
at this as a data dependency

00:22:53.030 --> 00:22:53.888
graph.

00:22:53.888 --> 00:22:55.430
We're aware of all
the artifacts that

00:22:55.430 --> 00:22:57.320
are going into these
various components,

00:22:57.320 --> 00:23:01.590
and the components will be
creating new artifacts as well.

00:23:01.590 --> 00:23:04.190
In some cases, we can
actually use components

00:23:04.190 --> 00:23:06.530
that were not created by
the current pipeline or even

00:23:06.530 --> 00:23:08.490
this pipeline configuration.

00:23:08.490 --> 00:23:10.760
So what we need is
some sort of system

00:23:10.760 --> 00:23:13.760
that's able to both schedule and
execute components but also be

00:23:13.760 --> 00:23:15.830
task-aware and maintain
a history of all

00:23:15.830 --> 00:23:19.950
the previous executions.

00:23:19.950 --> 00:23:21.530
So, what's the metadata store?

00:23:21.530 --> 00:23:24.530
First off, the metadata
store will keep information--

00:23:24.530 --> 00:23:26.523
I guess I click here--

00:23:26.523 --> 00:23:28.190
the metadata store
will keep information

00:23:28.190 --> 00:23:29.760
about the trained models--

00:23:29.760 --> 00:23:32.520
so, for example, the artifacts,
the type of the artifacts

00:23:32.520 --> 00:23:34.880
that have been trained
by previous components.

00:23:34.880 --> 00:23:37.010
Second, we keep a list
of all the components

00:23:37.010 --> 00:23:39.080
and all the versions,
all the inputs and all

00:23:39.080 --> 00:23:41.880
the runtime parameters that were
provided to these components.

00:23:41.880 --> 00:23:44.510
So this gives us history
of what's happened.

00:23:44.510 --> 00:23:45.950
Now that we have
this, we actually

00:23:45.950 --> 00:23:48.590
have this bi-directional graph
between all of our artifacts

00:23:48.590 --> 00:23:50.090
and all of our components.

00:23:50.090 --> 00:23:53.120
What this does is it gives us
a lot of extra capabilities

00:23:53.120 --> 00:23:56.540
where we can do advanced
analytics and metrics off

00:23:56.540 --> 00:23:57.840
of our system.

00:23:57.840 --> 00:23:59.990
An example of this is
something called lineage.

00:23:59.990 --> 00:24:02.900
Lineage is where you want to
know where all your data has

00:24:02.900 --> 00:24:05.430
passed through the system
or all of the components.

00:24:05.430 --> 00:24:07.640
So, for example, what
were all the models that

00:24:07.640 --> 00:24:09.830
were created by a
particular data set

00:24:09.830 --> 00:24:12.050
is something that we could
use with ML Metadata?

00:24:12.050 --> 00:24:14.480
On the flip side, what
were all the components

00:24:14.480 --> 00:24:16.220
or all the artifacts
that were created

00:24:16.220 --> 00:24:18.830
by a very particular
version of a component?

00:24:18.830 --> 00:24:21.830
We can answer that question
as well using ML Metadata.

00:24:21.830 --> 00:24:23.450
This is very important
for debugging,

00:24:23.450 --> 00:24:25.430
and it's also very
important for our enquiries

00:24:25.430 --> 00:24:27.890
when somebody says, how
has this data been consumed

00:24:27.890 --> 00:24:30.620
by any of your pipelines?

00:24:30.620 --> 00:24:32.870
Another example where
prior state helps us

00:24:32.870 --> 00:24:34.190
is warm starting.

00:24:34.190 --> 00:24:36.140
Warm starting is a
case in TensorFlow

00:24:36.140 --> 00:24:38.083
where you incrementally
train a model using

00:24:38.083 --> 00:24:40.250
the checkpoints in the
weights of a previous version

00:24:40.250 --> 00:24:41.335
of the model.

00:24:41.335 --> 00:24:43.460
So here, because we know
that the model has already

00:24:43.460 --> 00:24:45.693
been warm started by
using ML Metadata,

00:24:45.693 --> 00:24:48.110
we're able to go ahead and
warm start the model, saving us

00:24:48.110 --> 00:24:51.340
both time and compute.

00:24:51.340 --> 00:24:54.640
Here's an example of TensorFlow
Model Analysis, also known

00:24:54.640 --> 00:24:56.080
as TFMA.

00:24:56.080 --> 00:24:58.420
This allows us to
visualize how a model's

00:24:58.420 --> 00:24:59.960
been performing over time.

00:24:59.960 --> 00:25:02.272
This is a single model
on a time series graph,

00:25:02.272 --> 00:25:03.730
so we can see
whether the model has

00:25:03.730 --> 00:25:05.890
been improving or degrading.

00:25:05.890 --> 00:25:08.500
Tulsee is going to be talking
a little bit more about TFMA

00:25:08.500 --> 00:25:10.120
in a bit.

00:25:10.120 --> 00:25:13.540
Finally, we have the ability
to reuse the components.

00:25:13.540 --> 00:25:15.430
Now, I mentioned before
caching is great.

00:25:15.430 --> 00:25:17.560
Caching is great in
production, but it's also great

00:25:17.560 --> 00:25:19.420
when you're building your
model the first time.

00:25:19.420 --> 00:25:21.010
When you think about
you have your pipeline,

00:25:21.010 --> 00:25:22.580
you're probably working
in the static data set,

00:25:22.580 --> 00:25:24.610
and you're probably not working
on your features that much.

00:25:24.610 --> 00:25:27.110
You're probably spending most
of your time on the estimator.

00:25:27.110 --> 00:25:30.050
So with TFX, we more or less
cache through all that part.

00:25:30.050 --> 00:25:31.750
So as you're running
your pipeline,

00:25:31.750 --> 00:25:36.587
your critical path is
on the model itself.

00:25:36.587 --> 00:25:39.045
So you probably wondering, how
do I develop models in this?

00:25:39.045 --> 00:25:41.080
This is where the code comes in.

00:25:41.080 --> 00:25:44.470
So at the core, we use
TensorFlow estimators.

00:25:44.470 --> 00:25:47.133
You can build an estimator
using a Keras inference graph.

00:25:47.133 --> 00:25:48.800
You can build it using
a cant estimator,

00:25:48.800 --> 00:25:50.425
or you can go ahead
and create your own

00:25:50.425 --> 00:25:51.580
using the low-level ops.

00:25:51.580 --> 00:25:52.450
We don't care.

00:25:52.450 --> 00:25:54.530
All we need is an estimator.

00:25:54.530 --> 00:25:56.680
Once we have the estimator,
the next thing we do

00:25:56.680 --> 00:25:59.620
is we need to put it back
into a callback function.

00:25:59.620 --> 00:26:02.470
In the case of trainer, we have
a trainer callback function.

00:26:02.470 --> 00:26:04.360
And you'll see
here the estimator

00:26:04.360 --> 00:26:06.580
plus a couple more
parameters are passed back

00:26:06.580 --> 00:26:09.460
to the caller of the
callback function.

00:26:09.460 --> 00:26:13.240
This is how we call TensorFlow
for the train and evaluate.

00:26:13.240 --> 00:26:15.640
Finally, you add your
code, pretty much

00:26:15.640 --> 00:26:18.700
the file that had the callback
function and the estimator,

00:26:18.700 --> 00:26:20.870
into this TFX pipeline.

00:26:20.870 --> 00:26:23.290
So you'll see there in the
trainer, there's a module file,

00:26:23.290 --> 00:26:25.630
and the module file has
all the information.

00:26:25.630 --> 00:26:28.400
This is what we use to
call back to your function.

00:26:28.400 --> 00:26:31.100
This will generate
the save model.

00:26:31.100 --> 00:26:33.530
I was also talking about
data dependency graphs.

00:26:33.530 --> 00:26:35.390
If you notice here
on the graph, we

00:26:35.390 --> 00:26:38.180
don't actually explicitly
say transform runs

00:26:38.180 --> 00:26:40.280
and then the trainer runs.

00:26:40.280 --> 00:26:43.190
Instead, we're implied
dependency graph

00:26:43.190 --> 00:26:45.530
based on the fact that
the outputs of transform

00:26:45.530 --> 00:26:48.390
are required as inputs
for the trainer.

00:26:48.390 --> 00:26:50.690
So this allows us to couple
task scheduling along

00:26:50.690 --> 00:26:53.150
with our metadata awareness.

00:26:53.150 --> 00:26:55.280
At this point, we've
talked about components,

00:26:55.280 --> 00:26:56.940
we've talked about
the metadata store,

00:26:56.940 --> 00:26:59.157
and we've talked about how
to configure a pipeline.

00:26:59.157 --> 00:27:00.740
So now, what I'll
do is talk about how

00:27:00.740 --> 00:27:03.410
to schedule and execute
using some of the open source

00:27:03.410 --> 00:27:04.820
orchestrators.

00:27:04.820 --> 00:27:08.150
So we've modified our
internal version of TFX

00:27:08.150 --> 00:27:11.150
to support Airflow and Kubeflow,
two popular open source

00:27:11.150 --> 00:27:12.290
orchestrators.

00:27:12.290 --> 00:27:16.040
We know that there's additional
orchestrators, not all of which

00:27:16.040 --> 00:27:18.230
are in open source, so
we built an interface

00:27:18.230 --> 00:27:21.800
that allows us to add additional
orchestrators as possible.

00:27:21.800 --> 00:27:23.037
And we'd love contributions.

00:27:23.037 --> 00:27:25.370
So if somebody out there wants
to go ahead and implement

00:27:25.370 --> 00:27:28.250
on a third or a fourth
orchestrator, please let us

00:27:28.250 --> 00:27:31.020
know via GitHub, and
we'll help you out.

00:27:31.020 --> 00:27:33.103
Our focus is primarily
on extensibility.

00:27:33.103 --> 00:27:34.520
We want to be able
to allow people

00:27:34.520 --> 00:27:38.450
to extend ML pipelines,
build new graphs as opposed

00:27:38.450 --> 00:27:40.500
to the ones that we've
been showing today,

00:27:40.500 --> 00:27:41.967
and then also adding
new components

00:27:41.967 --> 00:27:44.300
because not all the components
that you need for machine

00:27:44.300 --> 00:27:48.190
learning for your particular
machine learning environments

00:27:48.190 --> 00:27:50.690
are the ones that
we're providing.

00:27:50.690 --> 00:27:52.850
So putting it all back
together, this is the slide

00:27:52.850 --> 00:27:54.560
that Gus showed earlier.

00:27:54.560 --> 00:27:57.080
At the very beginning, we
have the ExampleGen component

00:27:57.080 --> 00:27:59.270
that's going to extract
and generate examples.

00:27:59.270 --> 00:28:01.670
Then, we go through the
phases of data validation.

00:28:01.670 --> 00:28:03.230
And assuming that
the data is good,

00:28:03.230 --> 00:28:05.320
then we go ahead and
do feature engineering.

00:28:05.320 --> 00:28:07.320
Provided that the feature
engineering completes,

00:28:07.320 --> 00:28:08.990
we train a model,
validate the model,

00:28:08.990 --> 00:28:11.090
and then push it to
one or more systems.

00:28:11.090 --> 00:28:13.640
This is our typical ML pipeline.

00:28:13.640 --> 00:28:15.380
While some of the
components can change--

00:28:15.380 --> 00:28:17.210
the draft structure can change--

00:28:17.210 --> 00:28:19.240
that's pretty much how we do it.

00:28:19.240 --> 00:28:21.470
And what you see here
on the right is Airflow,

00:28:21.470 --> 00:28:22.970
and on the left is Kubeflow.

00:28:22.970 --> 00:28:25.250
We use the very
same configuration

00:28:25.250 --> 00:28:27.650
for both of these
implementations.

00:28:27.650 --> 00:28:29.810
And the key thing that
we're looking for in TFX

00:28:29.810 --> 00:28:30.950
is portability.

00:28:30.950 --> 00:28:33.800
We want portability where
the same configuration

00:28:33.800 --> 00:28:36.500
of a pipeline can move
between orchestrators.

00:28:36.500 --> 00:28:39.620
We also want to be portable
across the on-prem,

00:28:39.620 --> 00:28:42.110
local machine, and
public cloud boundaries.

00:28:42.110 --> 00:28:46.400
So going from my single machine
up to running in Google Cloud

00:28:46.400 --> 00:28:48.350
with, for example,
Dataflow is really

00:28:48.350 --> 00:28:52.520
a one-line change in my pipeline
just to reconfigure Beam.

00:28:52.520 --> 00:28:55.547
So that's it for how the
internals of TFX works.

00:28:55.547 --> 00:28:57.380
Next up, my colleague
Tulsee will be talking

00:28:57.380 --> 00:28:59.403
about model understanding.

00:28:59.403 --> 00:29:00.320
TULSEE DOSHI: Awesome.

00:29:00.320 --> 00:29:01.520
Thank you.

00:29:01.520 --> 00:29:02.640
Hi, everyone.

00:29:02.640 --> 00:29:05.720
My name is Tulsee, and I lead
product for the ML Fairness

00:29:05.720 --> 00:29:07.520
Effort here at Google.

00:29:07.520 --> 00:29:10.280
ML Fairness, like many
goals related to modeling,

00:29:10.280 --> 00:29:12.200
benefits from debugging
and understanding

00:29:12.200 --> 00:29:13.760
model performance.

00:29:13.760 --> 00:29:16.700
So, today, my goal is to
walk through a brief example

00:29:16.700 --> 00:29:18.710
in which we leverage
the facets of TFX

00:29:18.710 --> 00:29:22.760
that Gus and Kevin spoke about
earlier to better understand

00:29:22.760 --> 00:29:25.970
our model performance.

00:29:25.970 --> 00:29:28.240
So let's imagine that
you're an e-tailer,

00:29:28.240 --> 00:29:30.110
and you're selling shoes online.

00:29:30.110 --> 00:29:31.750
So you have a model,
and this model

00:29:31.750 --> 00:29:33.220
predicts click-through
rates that

00:29:33.220 --> 00:29:36.340
help inform how much
inventory you should order.

00:29:36.340 --> 00:29:38.560
A higher click-through
rate implies a higher need

00:29:38.560 --> 00:29:40.480
for inventory.

00:29:40.480 --> 00:29:42.130
But, all of a
sudden, you discover

00:29:42.130 --> 00:29:44.230
that the AUC and
prediction accuracy have

00:29:44.230 --> 00:29:45.860
dropped on men's dress shoes.

00:29:45.860 --> 00:29:46.840
Oops.

00:29:46.840 --> 00:29:50.110
This means that you may have
over-ordered certain shoes

00:29:50.110 --> 00:29:52.060
or under-ordered
certain inventory.

00:29:52.060 --> 00:29:56.750
Both cases could have direct
impact on your business.

00:29:56.750 --> 00:29:59.140
So what went wrong?

00:29:59.140 --> 00:30:00.940
As you heard in the
keynote yesterday,

00:30:00.940 --> 00:30:03.040
model understanding
is an important part

00:30:03.040 --> 00:30:05.350
of being able to
understand and improve

00:30:05.350 --> 00:30:08.330
possible causes of
these kinds of issues.

00:30:08.330 --> 00:30:11.620
And today, we want to
walk you through how

00:30:11.620 --> 00:30:14.280
you can leverage TFX to think
more about these problems.

00:30:19.790 --> 00:30:22.990
So first things first,
you can check your inputs

00:30:22.990 --> 00:30:26.320
with the built-in TF Data
Validation component.

00:30:26.320 --> 00:30:28.910
This component allows you
to ask questions like,

00:30:28.910 --> 00:30:30.430
are there outliers?

00:30:30.430 --> 00:30:31.850
Are some features missing?

00:30:31.850 --> 00:30:33.190
Is there something broken?

00:30:33.190 --> 00:30:34.780
Or, is there a shift
in distribution

00:30:34.780 --> 00:30:37.150
in the real world that changes
the ways your users might

00:30:37.150 --> 00:30:41.633
be behaving that might be
leading to this output?

00:30:41.633 --> 00:30:43.300
For example, here's
a screenshot of what

00:30:43.300 --> 00:30:46.570
TensorFlow Data Validation might
look like for your example.

00:30:46.570 --> 00:30:48.310
Here, you can see
all the features

00:30:48.310 --> 00:30:49.840
that you're using
in your data set--

00:30:49.840 --> 00:30:52.300
for example, price or shoe size.

00:30:52.300 --> 00:30:54.940
You can see how many examples
these features cover.

00:30:54.940 --> 00:30:57.430
You can see if any percent
of them are missing.

00:30:57.430 --> 00:31:00.280
You can see the mean, the
standard deviation, min,

00:31:00.280 --> 00:31:01.560
median, max.

00:31:01.560 --> 00:31:03.430
And you can also
see the distribution

00:31:03.430 --> 00:31:06.250
of how those features
map across your data set.

00:31:06.250 --> 00:31:08.440
For example, if you
look at shoe size,

00:31:08.440 --> 00:31:11.710
you can see the distribution
from sizes 0 to 14.

00:31:11.710 --> 00:31:14.500
And you can see
that sizes 3 to 7

00:31:14.500 --> 00:31:16.340
seem to be a little bit missing.

00:31:16.340 --> 00:31:21.080
So maybe we don't actually have
that much data for kids' shoes.

00:31:21.080 --> 00:31:23.230
You can now take
this a step further--

00:31:23.230 --> 00:31:25.370
so going beyond the
data to actually ask

00:31:25.370 --> 00:31:27.740
questions about your model
and your model performance

00:31:27.740 --> 00:31:31.010
using the TensorFlow
Model Analysis.

00:31:31.010 --> 00:31:33.150
This allows you to
ask questions like,

00:31:33.150 --> 00:31:36.770
how does the model perform
on different slices of data?

00:31:36.770 --> 00:31:38.570
How does the current
model performance

00:31:38.570 --> 00:31:41.950
compare to previous versions?

00:31:41.950 --> 00:31:43.870
With TensorFlow
Model Analysis, you

00:31:43.870 --> 00:31:46.600
get to dive deep into slices.

00:31:46.600 --> 00:31:49.480
One slice could be
men's dress shoes.

00:31:49.480 --> 00:31:51.670
Another example could
be what you see here,

00:31:51.670 --> 00:31:55.420
where you can actually slice
over different colors of shoes.

00:31:55.420 --> 00:31:58.330
The graph in this example
showcases how many examples

00:31:58.330 --> 00:32:00.170
have a particular feature.

00:32:00.170 --> 00:32:02.860
and the table below allows you
to deep dive into the metrics

00:32:02.860 --> 00:32:05.260
that you care about to
understand not just how

00:32:05.260 --> 00:32:07.120
your model is
performing overall,

00:32:07.120 --> 00:32:08.740
but actually taking
that next step

00:32:08.740 --> 00:32:11.650
to understand where your
performance may be skewed.

00:32:11.650 --> 00:32:14.290
For example, for
the color brick,

00:32:14.290 --> 00:32:17.050
you can actually see
an accuracy of 74%.

00:32:17.050 --> 00:32:19.175
Whereas, for shoes
of color light gray,

00:32:19.175 --> 00:32:20.425
we have an accuracy about 79%.

00:32:23.600 --> 00:32:25.550
Once you find a
slice that you think

00:32:25.550 --> 00:32:27.890
may not be performing the
way you think it should,

00:32:27.890 --> 00:32:30.270
you may want to dive
in a bit deeper.

00:32:30.270 --> 00:32:32.180
You actually now want
to start understanding

00:32:32.180 --> 00:32:33.770
why this performance went off.

00:32:33.770 --> 00:32:35.360
Where is the skew?

00:32:35.360 --> 00:32:38.360
Here, you can extend this
with the what-if tool.

00:32:38.360 --> 00:32:40.160
The what-if tool allows
you to understand

00:32:40.160 --> 00:32:41.870
the input your
model is receiving

00:32:41.870 --> 00:32:44.550
and ask and answer
what if questions.

00:32:44.550 --> 00:32:46.650
What if the shoe was
a different color?

00:32:46.650 --> 00:32:49.700
What if we were using
a different feature?

00:32:49.700 --> 00:32:52.820
With the what if tool, you
can select a data point

00:32:52.820 --> 00:32:55.380
and actually look at the
features and change them.

00:32:55.380 --> 00:32:57.350
You can play with
their feature values

00:32:57.350 --> 00:33:00.110
to be able to see how
the example might change.

00:33:00.110 --> 00:33:03.140
Here, for example, we're
selecting a viewed data point.

00:33:03.140 --> 00:33:05.820
You can go in and change
the value of the feature

00:33:05.820 --> 00:33:08.270
and actually see how the
classification output would

00:33:08.270 --> 00:33:10.940
change as you
change these values.

00:33:10.940 --> 00:33:13.280
This allows you to
test your assumptions

00:33:13.280 --> 00:33:15.770
to understand where there
might be correlations

00:33:15.770 --> 00:33:17.870
that you didn't expect
the model to pick up

00:33:17.870 --> 00:33:20.690
and how you might go
about tackling them.

00:33:20.690 --> 00:33:23.570
The what-if tool is available
as part of the TFX platform,

00:33:23.570 --> 00:33:26.930
and it's part of the
TensorBoard dashboard.

00:33:26.930 --> 00:33:29.510
You can also use it
as a Jupyter widget.

00:33:29.510 --> 00:33:32.120
Training data, test
data, and trained models

00:33:32.120 --> 00:33:33.860
can be provided to
the what-if tool

00:33:33.860 --> 00:33:35.382
directly from the
TFX Metadata store

00:33:35.382 --> 00:33:36.590
that you heard about earlier.

00:33:39.200 --> 00:33:40.660
But, the interesting
thing here is

00:33:40.660 --> 00:33:44.230
CTR is really just the
model's proxy objective.

00:33:44.230 --> 00:33:45.940
You want to really
understand CTR

00:33:45.940 --> 00:33:48.190
so that you can think about
the supply you should buy,

00:33:48.190 --> 00:33:49.660
your inventory.

00:33:49.660 --> 00:33:51.400
And so, your actual
business objectives

00:33:51.400 --> 00:33:53.390
depend on something much larger.

00:33:53.390 --> 00:33:54.490
They depend on revenue.

00:33:54.490 --> 00:33:55.750
They depend on cost.

00:33:55.750 --> 00:33:57.850
They depend on your supply.

00:33:57.850 --> 00:33:59.680
So you don't just
want to understand

00:33:59.680 --> 00:34:01.300
when your CTR is wrong.

00:34:01.300 --> 00:34:04.030
You actually want to understand
how getting this wrong

00:34:04.030 --> 00:34:06.730
could actually affect
your broader business--

00:34:06.730 --> 00:34:09.239
this misprediction cost.

00:34:09.239 --> 00:34:10.870
So in order to
figure this out, you

00:34:10.870 --> 00:34:13.000
decide you want to join
your model predictions

00:34:13.000 --> 00:34:14.469
with the rest of
your business data

00:34:14.469 --> 00:34:17.550
to understand that
larger impact.

00:34:17.550 --> 00:34:20.190
This is where some of the
component functionality of TFX

00:34:20.190 --> 00:34:21.790
comes in.

00:34:21.790 --> 00:34:23.880
You could actually
create a new component

00:34:23.880 --> 00:34:26.070
with a custom executor.

00:34:26.070 --> 00:34:27.960
You can customize
this executor such

00:34:27.960 --> 00:34:30.239
that you can actually join
your model predictions

00:34:30.239 --> 00:34:32.190
with your business data.

00:34:32.190 --> 00:34:35.179
Let's break that down a bit.

00:34:35.179 --> 00:34:37.070
So you have your trainer.

00:34:37.070 --> 00:34:38.690
You train a model.

00:34:38.690 --> 00:34:42.620
And then, you run the evaluator
to be able to get the results.

00:34:42.620 --> 00:34:44.570
You can then leverage
your custom component

00:34:44.570 --> 00:34:47.570
to join in business data
and export every prediction

00:34:47.570 --> 00:34:52.570
to a SQL database where you can
actually quantify this cost.

00:34:52.570 --> 00:34:54.685
You can then take
this a step further.

00:34:54.685 --> 00:34:56.560
With the what-if tool,
we talked a little bit

00:34:56.560 --> 00:34:58.727
about the importance of
assumptions, of testing what

00:34:58.727 --> 00:35:00.185
you think might be going wrong.

00:35:00.185 --> 00:35:01.810
You can take this
farther with the idea

00:35:01.810 --> 00:35:04.150
of understandable baselines.

00:35:04.150 --> 00:35:05.687
Usually, there are
a few assumptions

00:35:05.687 --> 00:35:08.020
we make about the ways we
believe that our models should

00:35:08.020 --> 00:35:09.250
be performing.

00:35:09.250 --> 00:35:12.310
For example, I might believe
that weekends and holidays

00:35:12.310 --> 00:35:16.630
are likely to have a higher CTR
with my shoes than weekdays.

00:35:16.630 --> 00:35:19.240
Or, I may have the hypothesis
that certain geographic regions

00:35:19.240 --> 00:35:21.370
are more likely to click
on certain types of shoes

00:35:21.370 --> 00:35:23.720
than others.

00:35:23.720 --> 00:35:26.770
These assumptions are rules
that I have attributed to how

00:35:26.770 --> 00:35:28.540
my models should perform.

00:35:28.540 --> 00:35:31.690
So I could actually create a
very simple rule-based model

00:35:31.690 --> 00:35:34.360
that could encode
these prior beliefs.

00:35:34.360 --> 00:35:36.920
This baseline
expresses my priors,

00:35:36.920 --> 00:35:38.920
so I can use them to
tell when my model might

00:35:38.920 --> 00:35:41.260
be uncertain or
wrong, or to dive in

00:35:41.260 --> 00:35:44.170
deeper if my expectations
were in fact wrong.

00:35:44.170 --> 00:35:47.290
They can help inform when the
deep model is overgeneralizing

00:35:47.290 --> 00:35:51.610
or when maybe my expectations
are overgeneralizing.

00:35:51.610 --> 00:35:54.340
So here again, custom
components can help you.

00:35:54.340 --> 00:35:56.410
You can actually build
a baseline trainer

00:35:56.410 --> 00:35:59.020
with these simple rules
whose evaluator also

00:35:59.020 --> 00:36:01.210
exports to your SQL database.

00:36:01.210 --> 00:36:03.100
Basically, you
stamp each example

00:36:03.100 --> 00:36:04.840
with its baseline prediction.

00:36:04.840 --> 00:36:07.960
So now, you can run queries not
just over the model predictions

00:36:07.960 --> 00:36:11.145
and your business data, but also
over your baseline assumptions

00:36:11.145 --> 00:36:13.270
to understand where your
expectations are violated.

00:36:15.880 --> 00:36:17.920
Understandable
baselines are one way

00:36:17.920 --> 00:36:19.840
of understanding
your models, and you

00:36:19.840 --> 00:36:22.930
can extend even beyond this
by building custom components

00:36:22.930 --> 00:36:25.030
that leverage new and
amazing research techniques

00:36:25.030 --> 00:36:26.890
in model understanding.

00:36:26.890 --> 00:36:28.600
These techniques
include [INAUDIBLE],,

00:36:28.600 --> 00:36:29.975
which Sundir
touched on yesterday

00:36:29.975 --> 00:36:32.320
in the keynote, but also
path-integrated gradients,

00:36:32.320 --> 00:36:34.150
for example.

00:36:34.150 --> 00:36:36.640
Hopefully, this gave you
one example of many ways

00:36:36.640 --> 00:36:38.740
that we hope that the
extensibility of TFX

00:36:38.740 --> 00:36:41.320
will allow you to go deeper
and extend the functionality

00:36:41.320 --> 00:36:44.350
for your own business goals.

00:36:44.350 --> 00:36:47.260
Overall, with TensorFlow
Extended you can leverage many

00:36:47.260 --> 00:36:50.060
out-of-the-box components for
your production model needs.

00:36:50.060 --> 00:36:52.540
This includes things like
TensorFlow Data Validation

00:36:52.540 --> 00:36:54.920
and TensorFlow Model Analysis.

00:36:54.920 --> 00:36:58.390
TFX also provides flexible
orchestration and metadata,

00:36:58.390 --> 00:37:01.270
and building on top of the
out-of-the-box components can

00:37:01.270 --> 00:37:03.760
help you further your
understanding of the model such

00:37:03.760 --> 00:37:06.790
that you can truly achieve
your business goals.

00:37:06.790 --> 00:37:08.800
We're excited to continue
to expand TensorFlow

00:37:08.800 --> 00:37:11.980
Extended for more of your use
cases and see how you extend

00:37:11.980 --> 00:37:13.960
and expand it as well.

00:37:13.960 --> 00:37:15.910
We're excited to
continue to grow the TFX

00:37:15.910 --> 00:37:16.978
community with you.

00:37:16.978 --> 00:37:18.520
And come to our
office hours tomorrow

00:37:18.520 --> 00:37:20.890
if you're still around I/O
to be able to talk to us

00:37:20.890 --> 00:37:22.810
and learn more and for
us to be able to learn

00:37:22.810 --> 00:37:25.870
from you about your needs
and use cases as well.

00:37:25.870 --> 00:37:27.770
Thank you so much
for joining us today,

00:37:27.770 --> 00:37:29.200
and we hope this was helpful.

00:37:29.200 --> 00:37:30.700
[APPLAUSE]

00:37:30.700 --> 00:37:34.050
[MUSIC PLAYING]

