WEBVTT
Kind: captions
Language: en

00:00:00.430 --> 00:00:03.550
So, how do we find the correct values of theta to minimize our

00:00:03.550 --> 00:00:05.470
cost function J of big theta?

00:00:05.470 --> 00:00:09.200
We'll use a search algorithm that takes some initial guesswork data, and

00:00:09.200 --> 00:00:13.310
iteratively changes theta, so that J of theta keeps on getting smalle,r until it

00:00:13.310 --> 00:00:14.820
converges on some minimum value.

00:00:15.900 --> 00:00:18.608
The algorithm we're going to discuss, is called gradient descent.

00:00:18.608 --> 00:00:21.720
Here's a one-dimensional depiction of what gradient descent might look like.

00:00:22.770 --> 00:00:26.420
We have some starting point where J of theta is large, and we continue to

00:00:26.420 --> 00:00:30.730
try new values of theta, and J of theta keeps on getting smaller and smaller.

00:00:30.730 --> 00:00:33.710
Until we arrive here, the final value.

00:00:33.710 --> 00:00:36.630
This theta value gives us our smallest value of J of theta.

00:00:36.630 --> 00:00:38.170
And so, we know that we've minimized J of theta.

00:00:38.170 --> 00:00:41.940
This is what gradient descent might look like in one dimension.

00:00:41.940 --> 00:00:44.130
Let's see what it looks like in two dimensions.

00:00:44.130 --> 00:00:47.340
Here again we see that we start with some high value of J of theta.

00:00:47.340 --> 00:00:49.560
And we continue to iteratively update theta one and

00:00:49.560 --> 00:00:51.870
theta two until we arrive here.

00:00:51.870 --> 00:00:54.040
The global minimum of J of big theta.

00:00:54.040 --> 00:00:56.000
Gradient descent updates the values of theta,

00:00:56.000 --> 00:01:01.740
according to the following equation, Theta n is equal to theta n itself,

00:01:01.740 --> 00:01:05.480
minus alpha, times d, d theta n, J of big theta.

00:01:06.950 --> 00:01:10.630
Note the derivative here, d, d theta n of J of theta.

00:01:10.630 --> 00:01:14.800
If you're less familiar with calculus, this is calculating the rate of change of

00:01:14.800 --> 00:01:17.420
J, with respect to this particular theta n.

00:01:18.500 --> 00:01:22.790
So, if we change this theta n a little bit, how much does J of big theta change?

00:01:24.110 --> 00:01:27.860
We simultaneously perform this update for every theta n, in our big theta.

00:01:28.900 --> 00:01:31.940
That is, every single theta n in the entire set of thetas.

00:01:33.470 --> 00:01:34.800
The alpha in this equation,

00:01:34.800 --> 00:01:38.120
is a parameter of the algorithm called the learning rate.

00:01:38.120 --> 00:01:40.010
What we're essentially saying here is that,

00:01:40.010 --> 00:01:44.160
in the space of all possible values of theta, what's the smallest step I

00:01:44.160 --> 00:01:47.860
can make in a certain direction, such that I'll make the value of J smaller?

00:01:49.290 --> 00:01:50.850
So, each time I do this,

00:01:50.850 --> 00:01:55.020
I'm taking one of these small steps towards the minimum value of J of theta.

00:01:55.020 --> 00:01:57.450
I'm not going to go into the calculus here, but

00:01:57.450 --> 00:02:01.500
based of the cost function J's dependence on the various theta n's,

00:02:01.500 --> 00:02:03.990
if we actually perform this differentiation.

00:02:03.990 --> 00:02:07.560
The actual equation that we'll use to update our theta n values is this.

00:02:08.770 --> 00:02:11.265
Theta n equals theta n minus alpha,

00:02:11.265 --> 00:02:16.450
times the sum from i equals 1 to m, of y predicted of x i.

00:02:16.450 --> 00:02:21.910
Minus y observed x i time x sub n i.

00:02:21.910 --> 00:02:25.560
You may be wondering how we set a value of the learning parameter, alpha.

00:02:25.560 --> 00:02:28.460
A regress discussion is beyond the scope of this course.

00:02:28.460 --> 00:02:31.170
However, the important thing to note is that the smaller learning rate,

00:02:31.170 --> 00:02:35.430
will cause the algorithm to take longer to converge on the optimal theta values.

00:02:35.430 --> 00:02:37.940
Since you're taking smaller and smaller steps down the curve.

00:02:39.050 --> 00:02:41.430
Larger values of alpha, can converge more quickly.

00:02:41.430 --> 00:02:44.410
But they're more prone to skip over the cost function's known value.

00:02:45.430 --> 00:02:49.520
This can cause J to increase, rather than decrease monotonically.

00:02:49.520 --> 00:02:52.320
The best way to make sure that your alpha is suitable, is simply to

00:02:52.320 --> 00:02:56.450
keep track of the cost functions value as you iteratively update it, and

00:02:56.450 --> 00:02:58.500
make sure it's always going down.

00:02:58.500 --> 00:03:00.530
If not, you learning rate is probably too high.

