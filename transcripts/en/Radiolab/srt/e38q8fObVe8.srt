1
00:00:02,050 --> 00:00:03,190
Oh wait, wait. You're letting

2
00:00:09,320 --> 00:00:14,130
listening to radio lab radio from W N y.

3
00:00:14,130 --> 00:00:15,070
S t.

4
00:00:19,170 --> 00:00:22,520
All right. Hello? Hello? Hello?
Hello? Hello? Hello. I can hear you.

5
00:00:22,790 --> 00:00:25,250
They can't hear me? No, we
can hear you. We can hear you.

6
00:00:25,740 --> 00:00:30,650
We can also hear it twice. Twice.
Yeah. Hey, I'm Jad, I boom. Rod.

7
00:00:30,680 --> 00:00:35,350
I'm Robert Krulwich, Radiolab, and today,
oops. Oops. You don't, you don't hear.

8
00:00:35,410 --> 00:00:36,243
You don't hear us.

9
00:00:36,290 --> 00:00:41,290
We have a story about how the echoes of
you can go out into the world and come

10
00:00:41,421 --> 00:00:46,190
back and bite you in all
of this really in the bud.

11
00:00:47,140 --> 00:00:52,100
Hmm. Uh, oh wait, wait. Maybe
we're fine now. Is My echo gone?

12
00:00:53,030 --> 00:00:54,920
Yes. Okay. Okay. We're good. We're good.

13
00:00:55,070 --> 00:00:58,970
[inaudible] and it comes to us from our
producers. Simon Adler. Yeah. Like nick.

14
00:00:58,971 --> 00:01:03,530
Hello? I'm sorry. Okay,
so this is Nick Bilton.

15
00:01:03,560 --> 00:01:07,220
My name is Nick Bilton. I'm a special
correspondent for Vanity Fair and is beat,

16
00:01:07,280 --> 00:01:11,060
you could say is trying to predict the
future of technology to look into the

17
00:01:11,061 --> 00:01:11,630
future.

18
00:01:11,630 --> 00:01:16,430
I'm into this kind of crystal ball
and try to predict what the next five,

19
00:01:16,520 --> 00:01:19,100
10, 15 years would look
like for the media industry.

20
00:01:19,160 --> 00:01:21,440
Do you have a good batting
record? Like did you,

21
00:01:21,470 --> 00:01:23,450
did you call some big well yeah, you know,

22
00:01:23,570 --> 00:01:27,410
phones in our pockets that would be like
super computers that social media would

23
00:01:27,411 --> 00:01:31,650
drive news, not newspapers and so
on and things like that. So yeah,

24
00:01:32,040 --> 00:01:34,490
it's been pretty good. I
reached out to you because I,

25
00:01:34,491 --> 00:01:37,910
I came across this article that
you wrote an article that, uh,

26
00:01:38,330 --> 00:01:43,190
sent shivers down my spine and I'm not
one to typically be given shivers by

27
00:01:43,191 --> 00:01:44,024
articles. So it,

28
00:01:44,570 --> 00:01:48,410
I guess how did you stumble
into all of this and where did,

29
00:01:48,411 --> 00:01:51,180
where does this start for you? So, uh,

30
00:01:51,181 --> 00:01:55,530
I was sitting around with some friends
in my living room and friend of mine

31
00:01:55,531 --> 00:01:57,800
mentioned, Oh, did you see this
thing that Adobe put out recently?

32
00:02:00,510 --> 00:02:04,960
We live in a time when more people than
ever before believe that they can change

33
00:02:04,961 --> 00:02:05,820
the world. And that

34
00:02:05,890 --> 00:02:08,230
conversation led nick to a video,

35
00:02:08,410 --> 00:02:11,940
a video online of the Adobe Max 2016.

36
00:02:14,060 --> 00:02:16,080
There are tons and tons of
people in the audience. Yeah,

37
00:02:16,180 --> 00:02:18,160
he's amazing. And up in front of them,

38
00:02:18,340 --> 00:02:22,150
it looks like this stage
of a apple product launch,

39
00:02:22,180 --> 00:02:26,170
but sort of beach themed. Why
Beach? I have absolutely no idea.

40
00:02:26,610 --> 00:02:28,660
That's a little bit to
your mind. I got, Hey,

41
00:02:29,200 --> 00:02:32,830
there are two hosts that are sitting
in these like lifeguard chairs,

42
00:02:34,270 --> 00:02:38,350
comedian Jordan Peele, Jordan Peele is
in key and Peele, Jordan Peele. Yes.

43
00:02:38,590 --> 00:02:42,880
And then the other host is this woman Kim
Chambers who is a marathon swimmer and

44
00:02:43,170 --> 00:02:47,650
an Adobe employee. And then please
welcome to the stage on walks,

45
00:02:49,050 --> 00:02:49,883
Zay, UGA.

46
00:02:49,930 --> 00:02:52,660
Hello everyone. Young guy glasses.

47
00:02:52,690 --> 00:02:56,220
You guys have been making
wear to staff online.

48
00:02:58,260 --> 00:02:59,580
Great photo chain.

49
00:02:59,900 --> 00:03:02,480
And he says Adobe is known for Photoshop.

50
00:03:02,600 --> 00:03:05,410
We're known for editing
photos and doing that

51
00:03:05,440 --> 00:03:06,940
magical things visually.

52
00:03:07,360 --> 00:03:10,720
Well we'll do the nectar thing today.

53
00:03:10,721 --> 00:03:15,610
Let's do something to no speech pulls
up the screen on a Mac computer.

54
00:03:15,780 --> 00:03:19,560
Wow. I have overtaken today's
piece ball deal where um,

55
00:03:19,720 --> 00:03:24,400
there's mech key talking to peel about
his feeling after getting nominated.

56
00:03:24,930 --> 00:03:27,720
Keegan, Michael Key had been
nominated for an Emmy and a,

57
00:03:27,730 --> 00:03:29,760
he and Jordan Peele were talking about it.

58
00:03:30,260 --> 00:03:34,620
Ah, there's a pretty interesting, uh,
joke here. So let's, uh, let's, uh,

59
00:03:34,640 --> 00:03:39,200
just here yet. Uh, I jumped
out of the bed and, um,

60
00:03:39,410 --> 00:03:43,130
and uh, uh, I kissed my dogs
and my wife in that order.

61
00:03:44,360 --> 00:03:47,360
Not a bad joke. So that's uh,
that's do something here. Okay.

62
00:03:47,450 --> 00:03:51,440
So suppose a Michael K one, two,
send this audio to his life.

63
00:03:51,770 --> 00:03:52,400
In other words,

64
00:03:52,400 --> 00:03:56,750
what if Keegan Michael Key was feeling
like that was a little bit rough on my

65
00:03:56,751 --> 00:03:59,060
wife? That was a little
bit mean. Uh, you know,

66
00:03:59,090 --> 00:04:04,090
maybe he wanted to go and rewrite history
and say that he kissed his wife before

67
00:04:04,700 --> 00:04:05,533
the dog.

68
00:04:05,650 --> 00:04:09,590
Okay. Actually want his wife
to go before the dogs. So,

69
00:04:11,010 --> 00:04:12,210
okay, so what are we doing

70
00:04:12,320 --> 00:04:12,891
easily?

71
00:04:12,891 --> 00:04:17,060
So you clicks a button in the program
automatically generates a transcript of

72
00:04:17,061 --> 00:04:19,940
the audio and projects it up on
the screen behind him. You know,

73
00:04:19,970 --> 00:04:22,790
just text of what Keegan
Michael Key said. Okay,

74
00:04:22,990 --> 00:04:25,790
let me zooming a little
bit and then copy paste.

75
00:04:26,060 --> 00:04:29,870
He just highlights the word wife
and pastes it over in front of dogs.

76
00:04:30,150 --> 00:04:35,100
Okay, let's listen to it. Clicks play.
And uh, uh, I kissed my wife and my dogs.

77
00:04:35,960 --> 00:04:36,793
Woo.

78
00:04:37,680 --> 00:04:37,940
[inaudible]

79
00:04:37,940 --> 00:04:40,730
oh, so he was able to move the,

80
00:04:40,760 --> 00:04:44,900
edit the audio by moving the text around
in a text box. Yes, exactly. Okay.

81
00:04:44,901 --> 00:04:46,490
Well that's kind of
cool. Kind of impressive.

82
00:04:46,820 --> 00:04:51,790
Wait, but then here's more, here's
more. Uh, we can talk to types.

83
00:04:52,280 --> 00:04:56,500
Onfi. That's not here. So, wait, wait,
what? Just hang on. Just hang on.

84
00:04:56,580 --> 00:05:01,560
I have heard that, um,
actually a dot on that day. Oh,

85
00:05:01,561 --> 00:05:04,590
actually kissed our Jordan.

86
00:05:05,430 --> 00:05:08,720
So sorry to recover the
truth. Let's do this.

87
00:05:08,820 --> 00:05:12,960
He goes back into that little word box
though. Let's remove the word my here.

88
00:05:13,320 --> 00:05:18,060
Your secrets out. Just
type the word Jordan.

89
00:05:18,280 --> 00:05:21,280
So he typed it out. J. O. R.
D. A. N. N. Just to be clear,

90
00:05:21,550 --> 00:05:24,370
Keegan Michael Key did not see
Jordan anywhere in this clip.

91
00:05:24,590 --> 00:05:28,550
And here we go. And, uh, uh,
I kissed Jordan and my dogs.

92
00:05:31,920 --> 00:05:32,160
Wait,

93
00:05:32,160 --> 00:05:36,690
he just typed in a word that the guy never
said and it made the guy say the word

94
00:05:37,030 --> 00:05:40,020
he never said as if he
actually said it exactly.

95
00:05:40,380 --> 00:05:44,750
Well you will, which
jumps out of his lifeguard

96
00:05:44,830 --> 00:05:47,260
chair started sort of
stomping around the stage.

97
00:05:47,660 --> 00:05:49,170
You were, you were demon. Oh yeah.

98
00:05:49,890 --> 00:05:54,890
I have a magic and to the lot magic I
cannot show you guys is we calculate small

99
00:05:55,451 --> 00:05:58,670
phrases. So let's say,

100
00:05:59,090 --> 00:06:02,090
okay, what were your mood? He
deletes the words, my dogs.

101
00:06:02,150 --> 00:06:04,700
Any types three times.

102
00:06:07,550 --> 00:06:11,090
Oh Man. Tea Bag. And uh, I
kissed Jordan three times.

103
00:06:14,810 --> 00:06:15,643
Oh,

104
00:06:15,750 --> 00:06:20,670
all right. Wait, you are saying
that Keegan Michael Key never said,

105
00:06:20,850 --> 00:06:25,460
ever said Jordan. Never said
three. Never said times. Never,

106
00:06:25,770 --> 00:06:29,670
never said any of those words. And
somehow just from the typing in of it,

107
00:06:29,671 --> 00:06:33,450
the guy is now saying them and
we're hearing them in his voice.

108
00:06:33,480 --> 00:06:37,080
That's what just happened. Yup. That
is exactly what the demo claims.

109
00:06:37,140 --> 00:06:41,930
It's essentially Photoshop for audio.
Nick built. And again, you could take, uh,

110
00:06:42,240 --> 00:06:42,280
uh,

111
00:06:42,280 --> 00:06:47,280
as little as 20 minutes of someone's voice
type the words and it creates in that

112
00:06:47,551 --> 00:06:52,470
voice, um, that sentence with just 20
minutes of the guy talking. Yes. What,

113
00:06:52,490 --> 00:06:55,580
how, how in heaven do you do this?

114
00:06:57,200 --> 00:06:59,890
Okay. Well, and so we're,
we're here to Doby. Um, what,

115
00:06:59,891 --> 00:07:03,590
what exactly do you do here? Sure.
I'm the product manager for audio.

116
00:07:03,650 --> 00:07:05,180
This is during Gleaves. Um,

117
00:07:05,270 --> 00:07:08,780
I flew out to Seattle and tracked him
down to ask him exactly that question.

118
00:07:08,780 --> 00:07:12,350
So essentially what it does is it does
an analysis of the speech and it creates

119
00:07:12,351 --> 00:07:16,400
a models and it, it basically, uh, and
he explained to me that this program,

120
00:07:16,401 --> 00:07:18,420
which they called Voco by the way, um,

121
00:07:18,710 --> 00:07:21,920
what it does is it takes
20 minutes or actually 40,

122
00:07:21,921 --> 00:07:24,230
if you want the best
results of you talking.

123
00:07:24,260 --> 00:07:26,480
And it figures out all of
the phonetics of your speech.

124
00:07:26,510 --> 00:07:29,960
All of the sounds that you make finds
each little block of sound and speech that

125
00:07:29,961 --> 00:07:31,880
is in the recordings, chops them all up.

126
00:07:31,910 --> 00:07:35,870
And then when you go and type things
in, it will recombine those, um,

127
00:07:35,900 --> 00:07:40,460
into that new word. But what
if it encounters a sound
that I'd never made? Well,

128
00:07:40,910 --> 00:07:43,400
uh, the theory is in 40 minutes of speech,

129
00:07:43,430 --> 00:07:45,620
which is the amount they
recommend you feed in,

130
00:07:45,650 --> 00:07:50,450
you're gonna probably say just about every
sound really in the English language.

131
00:07:50,480 --> 00:07:53,450
So if, if really sort of
like phonetically I go,

132
00:07:53,451 --> 00:07:57,290
I run through the gamut and
in 40 minutes, yes. Wow.

133
00:07:58,440 --> 00:08:02,120
Well in, uh, like what would you,
or what are you hoping people will,

134
00:08:02,150 --> 00:08:05,960
would use a product like Foco for?

135
00:08:07,190 --> 00:08:11,120
Uh, so for the video production tools and
for what audition is used for a lot is

136
00:08:11,540 --> 00:08:12,110
dialogue. Editing.

137
00:08:12,110 --> 00:08:16,190
The whole idea during said is to help
people that work in movies and TV.

138
00:08:16,191 --> 00:08:21,191
A lot of our customers record great
audio on set the actors and the dialogues

139
00:08:21,321 --> 00:08:24,080
and everything. Uh, and
when they come back,

140
00:08:24,410 --> 00:08:26,630
if sometimes there's a
mistake or they make a change,

141
00:08:26,930 --> 00:08:29,560
like the actor on set said shoe,

142
00:08:30,230 --> 00:08:33,830
but what he was pointing out with
obviously a boot. And right now there's,

143
00:08:33,831 --> 00:08:36,800
they do what's called ADR.
They'll bring the actor in,

144
00:08:37,070 --> 00:08:40,670
they'll rerecord some lines and they'll
try and drop that into the video.

145
00:08:40,790 --> 00:08:43,280
But there's a lot of, you're
not using the same microphones,

146
00:08:43,281 --> 00:08:46,400
you're not in the same location.
The actor might be sick that day.

147
00:08:46,401 --> 00:08:48,670
So up his voice sounds
different and things you,

148
00:08:48,770 --> 00:08:51,440
a lot of times you can really hear
that standout. And in productions,

149
00:08:51,441 --> 00:08:55,530
if they don't get it just right or with
Voco you just delete the word shoe type

150
00:08:55,531 --> 00:08:56,910
in boot and boom,

151
00:08:57,180 --> 00:09:01,710
there it is using the same source media
and the same characteristics and have it

152
00:09:01,711 --> 00:09:06,711
just sounds seamless and natural and
[inaudible] it's going to be a sort of,

153
00:09:07,810 --> 00:09:11,580
the hope is that it will make the lives
of professional postproduction editors

154
00:09:11,610 --> 00:09:14,610
easier the world over. That's
our hope right now. Yeah,

155
00:09:16,200 --> 00:09:20,190
but that's not exactly,
well, it's, I mean it's, um,

156
00:09:20,490 --> 00:09:24,630
what Nick Bilton thought when,
um, when he saw this video,

157
00:09:24,690 --> 00:09:27,660
it could be Donald Trump's
voice or Vladimir Putin. Um,

158
00:09:28,110 --> 00:09:30,480
so I saw that and I thought, wow,

159
00:09:30,540 --> 00:09:34,710
if imagine if audio clips start
getting shared around the Internet,

160
00:09:35,640 --> 00:09:40,170
um, as fake news about fake
conversation between, you know,

161
00:09:40,230 --> 00:09:44,130
Vladimir Putin and Paul Manafort about
trying to get Trump into the white house

162
00:09:44,131 --> 00:09:47,130
or something like that right
now. It was like, Whoa, this is,

163
00:09:47,370 --> 00:09:48,540
this is scary stuff.

164
00:09:49,590 --> 00:09:54,060
But we're just getting started in the
words of John Raymond Arnold played by

165
00:09:54,061 --> 00:09:58,110
Samuel L. Jackson in the movie Jurassic
Park in his own voice. Hold on to you.

166
00:09:58,111 --> 00:10:02,250
But things that are
about to get a lot crazy

167
00:10:04,720 --> 00:10:05,553
[inaudible]

168
00:10:10,580 --> 00:10:14,330
so forget voices for a second because now
one, two, three, four, five, one, two,

169
00:10:14,331 --> 00:10:17,880
three, four, five. It's facetime.
All right. We are at the Paul G.

170
00:10:17,880 --> 00:10:20,420
Allen Center at the University
of Washington in Seattle.

171
00:10:20,450 --> 00:10:25,450
So I left Adobe and went across town to
talk to the head of the grail lab euro.

172
00:10:27,840 --> 00:10:31,590
Yeah. Very nice to meet Dr Era
Chem Omakase Schlitz Herman.

173
00:10:31,960 --> 00:10:34,610
I'm a professor in the computer
science department at the University of

174
00:10:34,611 --> 00:10:37,710
Washington and Elsa work at Facebook.
Can I just have you come a little closer?

175
00:10:38,630 --> 00:10:43,250
Okay. Just to back up for a second. When
Nick first saw the VOCO demonstration,

176
00:10:43,251 --> 00:10:46,550
he started to wonder, okay, like how
could this be used down the road?

177
00:10:46,640 --> 00:10:48,320
And my original thesis was, oh,

178
00:10:48,321 --> 00:10:53,321
well maybe what will happen is that you
will be able to create three d actors

179
00:10:54,110 --> 00:10:55,490
just like you did in star wars.

180
00:10:55,610 --> 00:11:00,610
Then join it with the voco stuff to create
a fake Hillary Clinton and you know,

181
00:11:01,010 --> 00:11:04,580
Donald Trump having a conversation or
making out or whatever it is you want to

182
00:11:04,581 --> 00:11:08,120
do. And that led him to investigate
the type of work that era does.

183
00:11:08,390 --> 00:11:12,140
Should I've been using these terms
like facial reenactment and facial

184
00:11:12,141 --> 00:11:14,120
manipulation? Are those the,
are those the right words?

185
00:11:14,121 --> 00:11:18,020
And then what the hell do
these words mean? Yeah. So, um,

186
00:11:18,950 --> 00:11:19,611
I mean it's all,

187
00:11:19,611 --> 00:11:24,611
it's all a way of animating faces
and it started from the movies,

188
00:11:25,071 --> 00:11:28,980
right? The concept is to drive these
remotely controlled bodies called avatars.

189
00:11:29,110 --> 00:11:31,580
I think like the aptly named movie Avatar

190
00:11:33,470 --> 00:11:36,320
or a sergeant. Yes. Going
a little further back.

191
00:11:36,410 --> 00:11:39,170
No sign of intelligent
life anywhere, toy story,

192
00:11:40,190 --> 00:11:41,840
and to make the characters come alive.

193
00:11:42,130 --> 00:11:44,810
What you need is the expressions
of the actors playing them.

194
00:11:45,080 --> 00:11:49,160
This is the movie space. It means that
you will bring a person to his studio.

195
00:11:49,550 --> 00:11:53,350
Then you covered their face with these
sticky sensory marker things and then

196
00:11:53,351 --> 00:11:54,700
they will spend hours, hours,

197
00:11:54,730 --> 00:11:59,280
hours capturing the person's little
dynamics like smiled up in miles teeth,

198
00:11:59,530 --> 00:12:03,970
claws, mas, no teeth had a surprised
disturbance. Like that. Angry, bloated,

199
00:12:04,190 --> 00:12:05,023
yeah, frustrated.

200
00:12:05,200 --> 00:12:10,200
And from that they create a virtual
character capable of emoting all those

201
00:12:10,211 --> 00:12:12,820
expressions. And to make
that character believable,

202
00:12:13,030 --> 00:12:17,440
the animators sometimes have to
model a bone structure and muscles.

203
00:12:17,590 --> 00:12:20,350
And as you can imagine, this
can get very, very expensive.

204
00:12:20,351 --> 00:12:22,690
And so what people like eras
started to wonder was like,

205
00:12:22,870 --> 00:12:24,220
can this be done on a budget?

206
00:12:24,430 --> 00:12:29,380
So she and others in the field started
feeding videos of faces into computers

207
00:12:29,381 --> 00:12:33,670
and trained those computers to break
down the face into a series of points.

208
00:12:33,730 --> 00:12:38,730
I models are about a two 50 by two 50
that is 62,500 points on one human face.

209
00:12:41,380 --> 00:12:43,810
And then once we know that, right,
we can track the planes. Okay.

210
00:12:44,020 --> 00:12:49,020
So once you can track how my face moves
through a video clip by these 250 by 250

211
00:12:50,710 --> 00:12:55,300
points, what can you then do
with that information? Well,

212
00:12:55,480 --> 00:13:00,480
I can apply the points on the face on a
different model of a different person.

213
00:13:01,330 --> 00:13:04,370
Now this is, this is where
things get quite strange, uh,

214
00:13:04,600 --> 00:13:09,600
because instead of being able to map
all of your facial movements onto a

215
00:13:09,881 --> 00:13:14,260
computer generated a virtual
character or person, uh,

216
00:13:14,620 --> 00:13:18,020
with era and others in this
field of facial reenactment have,

217
00:13:18,040 --> 00:13:23,040
have figured out how to do is to map
your facial movements onto a real person,

218
00:13:24,690 --> 00:13:28,690
a, a prerecorded real person.
What, what does that even mean?

219
00:13:28,840 --> 00:13:31,660
How does that work? The, the
best example of this is a,

220
00:13:31,661 --> 00:13:33,910
is this piece of software
that nick showed us a,

221
00:13:34,260 --> 00:13:38,170
this software that I found from these
university students called face to face

222
00:13:38,230 --> 00:13:38,891
sweet present,

223
00:13:38,891 --> 00:13:42,700
a novel realtime facial reenactment method
that works with any commodity Webcam.

224
00:13:43,010 --> 00:13:45,700
There's a video demo of this.
And, um, when you open it up, uh,

225
00:13:45,701 --> 00:13:48,790
this very monotone voice comes in saying,

226
00:13:48,791 --> 00:13:52,450
since our method only uses RGB data
for both the source and target actor,

227
00:13:52,480 --> 00:13:54,760
and you're like, what the heck is
this? And the screen pops up here,

228
00:13:54,761 --> 00:13:57,250
we demonstrate our method in
a live setup on the right,

229
00:13:57,251 --> 00:14:01,420
you've got this heavyset man
goatee, spiked hair on the right,

230
00:14:01,450 --> 00:14:05,170
a source actor is captured with the
standard webcam arching his eyebrows.

231
00:14:05,171 --> 00:14:08,980
He's pursing his lips, he's opening
his mouth widely. The sort of like a,

232
00:14:09,010 --> 00:14:11,980
like if you're making funny faces for
a two year old kind of thing. Yeah.

233
00:14:12,220 --> 00:14:15,700
And then this input drives the animation
of the face and the video shown on the

234
00:14:15,701 --> 00:14:16,660
Monitor to the left.

235
00:14:16,770 --> 00:14:21,770
On the left you've got this Dell computer
screen displaying a CNN clip of George

236
00:14:22,260 --> 00:14:27,240
Bush. This is a real clip of Bush
back from 2013. And his face is,

237
00:14:27,600 --> 00:14:30,950
they're looking right at the camera.
Occupies most of that screen. Yeah.

238
00:14:30,980 --> 00:14:32,600
Significant difference
to previous methods.

239
00:14:32,601 --> 00:14:36,880
What you start to notice is when
the man with the goatee smiles,

240
00:14:37,150 --> 00:14:40,600
George Bush in the CNN clip also smiles.

241
00:14:40,601 --> 00:14:42,430
And when the man raises his eyebrows,

242
00:14:42,431 --> 00:14:47,431
George Bush raises his eyebrows and you
realize this man is controlling George

243
00:14:48,520 --> 00:14:50,230
Bush's face. Wait, so this is a guy in,

244
00:14:50,380 --> 00:14:54,140
in the present controlling
a past George Bush.

245
00:14:54,141 --> 00:14:56,210
A real George Bush from an old video clip.

246
00:14:56,330 --> 00:14:59,930
Yeah. Okay. I pulled up a
video for you here. Okay, cool.

247
00:14:59,990 --> 00:15:02,210
And a little while back when we
were just learning about this,

248
00:15:02,360 --> 00:15:05,720
we happened to have our friend Andrew
[inaudible] who writes for the New Yorker

249
00:15:05,721 --> 00:15:06,554
in the studio.

250
00:15:07,290 --> 00:15:11,970
So that is George Bushes
face. [inaudible] what?

251
00:15:12,870 --> 00:15:16,080
Oh God. Oh God.

252
00:15:16,380 --> 00:15:20,070
That's terrifying. His,

253
00:15:20,970 --> 00:15:24,870
okay. So yeah, I cannot stop
watching George Bush's face. Oh,

254
00:15:24,871 --> 00:15:26,430
they're doing it with Putin now. Holy God.

255
00:15:26,431 --> 00:15:29,490
So I just have a guy just sort of going

256
00:15:31,720 --> 00:15:36,210
[inaudible] and then that's what Putin
is doing. Yeah. Oh, now it's Trump.

257
00:15:36,270 --> 00:15:40,920
You know, I mean, those videos
online had my mouth agape again,

258
00:15:41,010 --> 00:15:45,930
Nick Bilton. This is this, this is a
form of puppetry where your face is the,

259
00:15:46,050 --> 00:15:49,540
is the puppeteer than the only
thing is, is, is that George W.

260
00:15:49,540 --> 00:15:52,950
Bush is the puppet. So I sit
in front of a camera, I smile,

261
00:15:52,951 --> 00:15:55,530
and the business is taken
care of. That's real time.

262
00:15:55,560 --> 00:15:58,590
This isn't like you have to render
some software on your computer.

263
00:15:58,620 --> 00:16:03,620
It's literally you download a clip or
you take a clip from the cable news and

264
00:16:03,991 --> 00:16:07,500
you turn on your Webcam and however long
it takes you to do it and you're done.

265
00:16:08,070 --> 00:16:12,250
It's the same as to shooting a video
on your phone. What is this for? What,

266
00:16:12,340 --> 00:16:13,350
what are the applications that

267
00:16:13,730 --> 00:16:15,320
Jason, um,

268
00:16:15,380 --> 00:16:20,270
I want to be able to help cal develop
telepresence. This is era again.

269
00:16:20,450 --> 00:16:25,100
So I love Pres telepresence.
Yeah. So for example,

270
00:16:25,520 --> 00:16:28,430
so my mom leaves in Israel, um,

271
00:16:28,760 --> 00:16:33,750
and I'm here and I'm, what is
it? Be Cool if I could, um,

272
00:16:34,130 --> 00:16:36,230
have some guy, it's kind
of crazy, but right.

273
00:16:36,231 --> 00:16:41,231
But if I could have some kind of Hologram
of her sitting on my couch here and we

274
00:16:42,981 --> 00:16:44,450
can have a conversation

275
00:16:44,670 --> 00:16:46,890
and going one step further.
One of your colleagues,

276
00:16:46,891 --> 00:16:48,600
a guy by the name of Steve Sights.

277
00:16:48,601 --> 00:16:51,540
I'm a professor at the University of
Washington and I also work part time at

278
00:16:51,541 --> 00:16:52,020
Google.

279
00:16:52,020 --> 00:16:56,460
He told me that they see this technology
as like a building block that could one

280
00:16:56,461 --> 00:17:00,780
day be used to essentially virtually
bring someone back from the dead.

281
00:17:01,020 --> 00:17:05,370
I just think this technology combined
with the virtual reality and other

282
00:17:05,371 --> 00:17:07,170
innovations could help me, you know,

283
00:17:07,200 --> 00:17:11,460
just be there in the room with
Albert Einstein or Carl Sagan.

284
00:17:11,690 --> 00:17:15,000
Know that that's sort of the
motivation. That's what they want to do.

285
00:17:15,001 --> 00:17:18,930
That's the motivation. Talk to
ghosts. Well for them, yes. And uh,

286
00:17:19,230 --> 00:17:21,720
when I was talking to some
folks who work in commercials,

287
00:17:21,721 --> 00:17:26,470
they're developing their own
version of this and the idea is, uh,

288
00:17:26,570 --> 00:17:30,120
that they're gonna make a million
or $1 billion off of this because a,

289
00:17:30,121 --> 00:17:32,700
say you bring, I don't know, uh,

290
00:17:34,200 --> 00:17:39,120
Jennifer Aniston in to film some makeup
commercial and in the makeup commercial

291
00:17:39,240 --> 00:17:41,610
in English, she says, so
come and buy this product.

292
00:17:41,610 --> 00:17:45,750
This is the best sort of whatever product
around right now you've got China,

293
00:17:45,751 --> 00:17:46,920
which is a booming market.

294
00:17:47,070 --> 00:17:50,070
You may be want to market things to
China and really like to be able to use

295
00:17:50,071 --> 00:17:54,630
Jennifer Anniston. Problem is Jennifer
Annison doesn't speak Mandarin,

296
00:17:54,870 --> 00:17:59,870
so either you use the same audio clip
and you have someone come in and speak

297
00:18:00,570 --> 00:18:03,030
Mandarin over her and
the lips don't line up.

298
00:18:03,240 --> 00:18:08,160
Or You have to hire a mandarin speaking
a actor to come in and do the part of

299
00:18:08,161 --> 00:18:10,500
Jennifer Anniston with this technology.

300
00:18:10,710 --> 00:18:13,050
All you have to do is
record Jennifer Aniston.

301
00:18:13,051 --> 00:18:18,051
Once you can hire a mandarin speaker
and the mandarin speakers voice will be

302
00:18:18,361 --> 00:18:22,980
coming out of Jennifer Aniston's mouth
is if she had said it in front of the

303
00:18:22,981 --> 00:18:27,330
camera, her lips would be moving as if
she were a perfect mandarin speaker.

304
00:18:27,390 --> 00:18:31,430
Exactly, exactly. Wow. I think
that part of it is actually,

305
00:18:32,320 --> 00:18:34,890
that's amazing. Yeah. Oh my God.

306
00:18:35,640 --> 00:18:39,270
I'm amazed and completely frightened
by what you're telling me.

307
00:18:39,450 --> 00:18:42,520
And that's the whole point of what
nick was writing about that, uh,

308
00:18:42,540 --> 00:18:47,540
that gave me shivers that someday if
you join the video manipulation with the

309
00:18:48,451 --> 00:18:52,200
Voco voice manipulation,
you're, you're the,

310
00:18:52,320 --> 00:18:54,480
the ultimate puppeteer. You can create

311
00:18:55,460 --> 00:18:59,840
anyone talking about anything that you
want in their own voice and having any

312
00:18:59,841 --> 00:19:00,950
kind of emotion around it.

313
00:19:01,190 --> 00:19:04,930
And you'd have it right there
for everyone to see in video.

314
00:19:05,180 --> 00:19:09,440
And all you need to do is take that
and put it on Twitter or Facebook.

315
00:19:10,130 --> 00:19:13,760
And if it's shocking enough, I
minutes later it's everywhere.

316
00:19:22,080 --> 00:19:26,940
Like the timing of you guys making this
thing and then this explosion of fake

317
00:19:26,941 --> 00:19:31,230
news like, Huh? How do
you guys think about,

318
00:19:31,260 --> 00:19:32,530
about how it could be used for, for

319
00:19:33,110 --> 00:19:37,250
nefarious purposes? Uh,
yeah, it's a good question.

320
00:19:38,600 --> 00:19:42,740
Um, again, you're a [inaudible] I feel
like when every technology is developed,

321
00:19:42,741 --> 00:19:46,930
then there, there is this danger of
uh, these are technology. You, um,

322
00:19:47,010 --> 00:19:50,540
you can create fake videos and so on
or I don't want to call it fake videos,

323
00:19:50,541 --> 00:19:53,930
but I'd like to create video from audio.
Right. But they are fake videos. Yeah.

324
00:19:54,200 --> 00:19:57,500
Yeah. But the way that I
think about it is that like,

325
00:19:57,501 --> 00:19:59,480
scientists are doing their job in science,

326
00:19:59,510 --> 00:20:02,480
like inventing the
technology and sign it off.

327
00:20:02,510 --> 00:20:06,650
And then we all need to like think
about the next steps. Obviously. I mean,

328
00:20:06,890 --> 00:20:10,430
people should work on that. Um,
and the answer is not clear.

329
00:20:10,431 --> 00:20:12,470
Maybe it's education. Maybe it's, um,

330
00:20:12,530 --> 00:20:16,070
every video should come up with
some code now that this is,

331
00:20:16,310 --> 00:20:20,270
this is like authentic video or authentic
attacks and you don't believe anything

332
00:20:20,271 --> 00:20:24,860
else. I mean, yeah, but like, it maybe
it was the timing more than anything,

333
00:20:24,861 --> 00:20:28,520
but I saw this video and it
really felt like, oh my God,

334
00:20:28,521 --> 00:20:33,260
like America can't handle this right now.
Like we're in a moment where w we're,

335
00:20:33,440 --> 00:20:36,680
we're truth seems to be
sort of a, an open desk.

336
00:20:36,710 --> 00:20:41,710
What is true is has become
an open discussion and this
seems to be adding fuel

337
00:20:41,781 --> 00:20:43,970
on the fire of sort of, uh,

338
00:20:45,020 --> 00:20:48,070
competing narratives in a way that I find

339
00:20:48,070 --> 00:20:50,680
troubling. And I'm just
curious that you don't,

340
00:20:51,540 --> 00:20:52,373
yeah.

341
00:20:53,620 --> 00:20:55,030
Um, a thing that

342
00:20:57,130 --> 00:21:01,150
I think that people, if people
know the touch technology exists,

343
00:21:01,810 --> 00:21:05,260
then they will be more skeptical.
My guess, I don't know.

344
00:21:05,710 --> 00:21:09,610
But if people know that fake news exists,
if they know that fake text exists,

345
00:21:09,910 --> 00:21:11,890
fake videos exist, fake photos exist,

346
00:21:11,891 --> 00:21:16,060
then everyone is more skeptical
than what they read them see.

347
00:21:16,890 --> 00:21:19,680
But like [inaudible] a
man in North Carolina,

348
00:21:19,681 --> 00:21:20,880
I think he was from North Carolina,

349
00:21:20,881 --> 00:21:24,960
believed from a fake print article that
Hillary Clinton was running a sex ring

350
00:21:24,961 --> 00:21:28,920
out of a pizza parlor in
DC, which is like insane.

351
00:21:29,250 --> 00:21:31,110
This man believed it and
showed up with a gun.

352
00:21:31,380 --> 00:21:35,100
And if people are at a moment where
they are willing to believe stories as

353
00:21:35,101 --> 00:21:36,810
ludicrous, is that like,

354
00:21:36,840 --> 00:21:41,060
I don't expect them to wonder
if this video is real or not.

355
00:21:44,300 --> 00:21:45,133
Yeah.

356
00:21:46,480 --> 00:21:48,190
Um, so whether you asking,

357
00:21:48,940 --> 00:21:52,510
I'm asking, well, I'm asking do you, are
you afraid of the power of [inaudible]

358
00:21:52,880 --> 00:21:55,190
and if not, why?

359
00:21:57,900 --> 00:22:01,890
Just I'm just giving my, I don't know it
just, Ah, I'm answering your questions,

360
00:22:01,891 --> 00:22:06,720
but I'm a technologist. I'm a
computer scientist, so I'm not really,

361
00:22:06,721 --> 00:22:09,780
because I know how to read and I know
that because I know that this technology

362
00:22:09,781 --> 00:22:14,580
is reversible. I mean,
nobody, well, there is not,

363
00:22:18,840 --> 00:22:22,980
not worried too much

364
00:22:30,360 --> 00:22:30,590
[inaudible]

365
00:22:30,590 --> 00:22:34,070
have you seen these videos? Otherwise I
can text it. Yeah. Okay. Yeah, I have.

366
00:22:34,520 --> 00:22:37,040
And as we were feeling
worried and more than that,

367
00:22:37,100 --> 00:22:40,280
surprised that the folks making
these technologies weren't.

368
00:22:40,940 --> 00:22:42,950
We decided to do a third gut check.

369
00:22:43,010 --> 00:22:46,610
See if we were totally off base and get
in touch with one of the guys who's on

370
00:22:46,640 --> 00:22:47,900
the front lines of this.

371
00:22:48,380 --> 00:22:51,200
Can you describe what was going through
your head when you were watching Bush's

372
00:22:51,740 --> 00:22:54,830
face? I can tell you exactly what
I was thinking. I was thinking,

373
00:22:55,100 --> 00:22:57,800
how are we going to develop a
forensic technique to detect this?

374
00:23:00,170 --> 00:23:01,490
This is honey for reed.

375
00:23:01,550 --> 00:23:04,880
I am a professor of computer
science at Dartmouth College.

376
00:23:04,990 --> 00:23:08,120
He's sort of like a Sherlock
Holmes of digital misdeeds,

377
00:23:08,121 --> 00:23:11,150
which means that spends a lot of time
sitting around looking at pictures and

378
00:23:11,151 --> 00:23:13,910
videos, trying to understand
where has this come from?

379
00:23:13,911 --> 00:23:15,860
Has it been manipulated
and should we trust it?

380
00:23:16,040 --> 00:23:18,770
He's done work for all sorts
of organizations. The AP,

381
00:23:18,771 --> 00:23:22,970
The Times Reuters who want to know
if say a picture is fake or not,

382
00:23:23,090 --> 00:23:24,850
they often will ask me, you have to leave.

383
00:23:24,851 --> 00:23:27,020
When this just happened at two yesterday,

384
00:23:27,021 --> 00:23:31,400
images came out of North Korea and every
time images come out of these regimes

385
00:23:31,580 --> 00:23:33,320
where there was a history
of photo manipulation,

386
00:23:33,321 --> 00:23:34,970
there are real concerns about this.

387
00:23:35,030 --> 00:23:38,750
So I was asked to determine if they'd
been manipulated in some way and if so,

388
00:23:38,751 --> 00:23:42,470
how had they been manipulated and what,
how, how the heck would you do that?

389
00:23:42,500 --> 00:23:46,040
Wow. Every time you manipulate data
you're going to leave something

390
00:23:46,040 --> 00:23:49,280
behind. Um, so, so let's say you
do some funny business to a photo,

391
00:23:49,340 --> 00:23:52,700
you might create some noticeable
distortion in the picture itself,

392
00:23:52,730 --> 00:23:55,100
but you also might distort the data.

393
00:23:55,130 --> 00:23:59,390
And we're in the business of basically
finding those distortions in the data.

394
00:23:59,990 --> 00:24:03,980
For example, imagine he gets sent a
photo, it's probably a Jpeg, jpeg,

395
00:24:03,981 --> 00:24:08,510
which now is 99% of the image formats
that we see out there is what is called

396
00:24:08,600 --> 00:24:10,250
the lossy compression scheme.

397
00:24:10,520 --> 00:24:14,900
Just a fancy way to say that when a
photo is taken and stored as a Jpeg,

398
00:24:15,170 --> 00:24:19,190
the camera, you know, just to save space
throws a little bit of the data away.

399
00:24:19,520 --> 00:24:20,780
So for example,

400
00:24:20,810 --> 00:24:23,900
if I went out to the Dartmouth green
right now took a picture of the grass,

401
00:24:24,680 --> 00:24:28,820
the camera isn't going to store all those
millions of little variations of green

402
00:24:28,821 --> 00:24:31,880
hidden in the grass because
that would be just a huge file.

403
00:24:32,120 --> 00:24:35,660
It's going to save space by
throwing some of those greens away.

404
00:24:36,200 --> 00:24:40,340
You just don't notice if it changes like
a lot or a little bit less than that.

405
00:24:40,400 --> 00:24:43,280
It just grass as far as you can
tell. Now, here's how these trick.

406
00:24:43,310 --> 00:24:47,270
Every camera has a subtly different
Palette of Greens that's going to keep and

407
00:24:47,271 --> 00:24:48,650
greens that's going to throw away.

408
00:24:48,680 --> 00:24:51,680
This varies tremendously
from device to device.

409
00:24:51,920 --> 00:24:56,570
An iPhone compresses the image much more
so less Greens in a high end icon or a

410
00:24:56,571 --> 00:24:59,540
high on cannon, which would keep
more of the variations of green.

411
00:24:59,870 --> 00:25:02,870
Now if you hold these two
pictures side by side,

412
00:25:02,871 --> 00:25:06,200
you might not be able to tell
the difference, but honey says,

413
00:25:06,201 --> 00:25:08,450
when you look at the underlying pixels,

414
00:25:08,690 --> 00:25:13,160
there are different recognizable patterns.
You take an image off of your iPhone.

415
00:25:13,190 --> 00:25:16,700
I should be able to go into that jpeg
and look at the packaging and say, ah,

416
00:25:16,701 --> 00:25:18,620
yes, this should have
come out of an iPhone.

417
00:25:18,710 --> 00:25:22,790
But if that image is uploaded to Facebook
and then re downloaded or put into

418
00:25:22,791 --> 00:25:26,930
Photoshop and received, it will not look
like jpeg consistent with an iPhone.

419
00:25:27,020 --> 00:25:29,720
So basically he can see it, the
level of the pixels or data,

420
00:25:29,721 --> 00:25:33,500
whether the picture has been messed with
in any way. Huh. And this is of course,

421
00:25:33,501 --> 00:25:37,610
just one of many different ways that
honey can spot a fake. Yeah. Let me ask,

422
00:25:37,880 --> 00:25:42,880
if you could go up against the
top 100 best counterfeiters,

423
00:25:44,750 --> 00:25:47,750
do you think you'd catch
them 10% of the time,

424
00:25:47,780 --> 00:25:50,950
50% of the time just to have
kids? What's your sense? Um,

425
00:25:51,170 --> 00:25:54,620
I would say we could probably
catch 75% of the fakes,

426
00:25:54,650 --> 00:25:58,640
but I would say that would take a long
time to do. This is not an easy task.

427
00:25:58,970 --> 00:25:59,960
And so, you know, the,

428
00:25:59,961 --> 00:26:04,961
the pace at which the media moves does
not lend itself to careful forensic

429
00:26:05,721 --> 00:26:09,530
analysis of images. Um, I'm always amazed
that, you know, you get these emails,

430
00:26:09,531 --> 00:26:12,740
you're like, all right, you got 20
minutes and um, you would need, you know,

431
00:26:12,741 --> 00:26:17,270
half a day, a day per image, still a
very manual in a very human process.

432
00:26:17,680 --> 00:26:18,630
It's so are,

433
00:26:18,950 --> 00:26:23,690
is this video editing and this audio
editing that's coming down the pipeline

434
00:26:23,691 --> 00:26:27,740
here? Yeah, I guess, should
I be, should I be terrified?

435
00:26:29,000 --> 00:26:33,680
Um, yes, you should. Um, oh no.
Did you, you really mean that?

436
00:26:34,190 --> 00:26:35,000
Yeah, I think it's,

437
00:26:35,000 --> 00:26:38,150
I think it's going to raise the fake
news thing to a whole new level.

438
00:26:38,540 --> 00:26:41,900
I did see some artifacts, by the way
in the videos, they are not perfect,

439
00:26:41,960 --> 00:26:46,050
but that's neither here nor there
because the ability of technology to

440
00:26:46,051 --> 00:26:50,400
manipulate and alter reality is
growing at a breakneck speed. Um,

441
00:26:50,490 --> 00:26:54,270
and the ability to disseminate that
information is phenomenal. So I,

442
00:26:54,271 --> 00:26:56,910
I can't stop that by the way,
because at the end of the day,

443
00:26:56,911 --> 00:27:00,660
it's always going to be easier to create
a fake than to detect a fake. Hmm.

444
00:27:06,440 --> 00:27:07,210
[inaudible]

445
00:27:07,210 --> 00:27:09,210
right. Thank you very
much. We're going to,

446
00:27:10,240 --> 00:27:13,030
Chad himself just handed
me a cup of water,

447
00:27:13,031 --> 00:27:16,270
which shows none of you have gotten too
big for your britches and that could be

448
00:27:16,271 --> 00:27:17,830
a serious problem.

449
00:27:17,860 --> 00:27:21,550
I would like to have seen Peter
Jennings do that ever for this guy.

450
00:27:21,610 --> 00:27:26,230
My name is John Klein, Co founder
and CEO of tap media. Before that,

451
00:27:26,231 --> 00:27:27,250
president of CNN,

452
00:27:27,251 --> 00:27:32,251
u s before that I was
executive vice president of
CBS News where I was executive

453
00:27:32,651 --> 00:27:34,540
in charge of 60 minutes,

454
00:27:34,541 --> 00:27:39,400
48 hours and a bunch of other things
and he's had to react to some serious

455
00:27:39,401 --> 00:27:41,560
evolutions in the media industry. Uh,

456
00:27:41,710 --> 00:27:45,700
he was manning the helm as social
media exploded as smartphones became

457
00:27:45,701 --> 00:27:46,534
ubiquitous.

458
00:27:46,540 --> 00:27:51,540
And consequently he had to deal with
figuring out how and if to trust thousands

459
00:27:51,940 --> 00:27:55,270
of hours of video taken on these
smart phones and sended by viewers,

460
00:27:55,510 --> 00:27:57,530
what to broadcast and what not.

461
00:27:57,820 --> 00:28:01,870
And so we wanted to know how someone
in his position would think about these

462
00:28:01,871 --> 00:28:02,704
fake videos.

463
00:28:02,950 --> 00:28:07,000
So we sent him all of the different
demos and videos we'd come across just to

464
00:28:07,001 --> 00:28:07,810
see what he thought.

465
00:28:07,810 --> 00:28:12,810
First thought was that this is the kind
of thing that a James Bond villain would

466
00:28:13,390 --> 00:28:18,390
put to use or a the joker in Batman
or an eighth grade girl who right.

467
00:28:19,080 --> 00:28:22,540
Wants to be most popular. Exactly.
Yeah. You know, I mean, this is,

468
00:28:22,810 --> 00:28:26,830
there's so many ways to
abuse this blows your mind.

469
00:28:28,180 --> 00:28:29,680
I mean, it,

470
00:28:30,520 --> 00:28:35,520
it goes to the very core of
communication of any sort,

471
00:28:38,090 --> 00:28:42,670
whether it's television
or radio or interpersonal.

472
00:28:43,720 --> 00:28:48,630
Um, it is what I'm seeing. True
is what I'm hearing real, uh,

473
00:28:48,720 --> 00:28:50,680
in, in your, over the
course of your career,

474
00:28:50,681 --> 00:28:55,510
you've seen multiple
technological developments
that have impacted the media in

475
00:28:55,511 --> 00:28:59,110
rather profound ways. Where is
your terror level right now,

476
00:28:59,111 --> 00:29:03,070
or your fear level caused by this
relative to all of the other sorts of

477
00:29:03,071 --> 00:29:07,450
advancements that have occurred over,
over your career? It's terrifying.

478
00:29:08,650 --> 00:29:09,940
And it,

479
00:29:10,130 --> 00:29:14,850
it hurdles us even faster toward that
point where no one believes anything.

480
00:29:17,260 --> 00:29:22,260
How do you have a democracy in a country
where people can't trust anything that

481
00:29:23,321 --> 00:29:25,770
they see or read anymore? What,

482
00:29:26,170 --> 00:29:30,910
what we saw happen with the fake news
during the election cycle was that, um,

483
00:29:31,000 --> 00:29:35,290
all the, the, it doesn't, it didn't
even need to matter if anyone, you know,

484
00:29:35,500 --> 00:29:38,890
would rebuff it afterwards.
This is nick built. And again,

485
00:29:39,100 --> 00:29:44,050
it would reach millions and
millions of in me or seconds and,

486
00:29:44,320 --> 00:29:47,290
and you, and that was it.
You'd done it had done its job.

487
00:29:47,291 --> 00:29:51,370
And I think that with this audio
stuff, uh, and the video stuff,

488
00:29:51,371 --> 00:29:55,630
that's gonna that's going to come down,
uh, uh, on online in the next few years.

489
00:29:55,990 --> 00:29:57,220
Um, it's gonna do the same thing, but,

490
00:29:57,550 --> 00:29:59,140
but no one's gonna know
what's real and what's not.

491
00:29:59,300 --> 00:30:01,670
I moved on her actually, you
know, she was down in Palm Beach.

492
00:30:01,671 --> 00:30:02,990
I moved on her NFL.

493
00:30:03,020 --> 00:30:04,400
And what's more, Nick says,

494
00:30:04,490 --> 00:30:08,690
if you think about the video that
came out of Donald Trump from access,

495
00:30:08,691 --> 00:30:09,360
Hollywood

496
00:30:09,360 --> 00:30:11,280
automatically attracted to
beautiful. I just started,

497
00:30:12,250 --> 00:30:14,270
the thing that was really
interesting about that video

498
00:30:14,350 --> 00:30:16,900
when you were started, they let
you do it. You can do anything,

499
00:30:16,930 --> 00:30:18,160
whatever you want, grab him by the,

500
00:30:18,550 --> 00:30:23,470
you don't actually see Donald Trump until
the very last second when he gets off

501
00:30:23,471 --> 00:30:27,280
the bus. How were you high? You
only hear him make me a soapstone.

502
00:30:27,310 --> 00:30:31,420
I'm a little hugged with Donnelley and
so if that technology existed today,

503
00:30:31,540 --> 00:30:36,100
I can guarantee you that Donald Trump
would have responded by saying, oh,

504
00:30:36,101 --> 00:30:39,280
it's fake. It's fake news.
Fake audio. You can't see me.

505
00:30:39,281 --> 00:30:43,660
I didn't say that and it would just
be, this video is word against his

506
00:30:45,700 --> 00:30:46,160
[inaudible].

507
00:30:46,160 --> 00:30:48,890
Actually, that's kind of like for me,
that's sort of the real problem here.

508
00:30:48,980 --> 00:30:49,850
Like you create this,

509
00:30:50,090 --> 00:30:54,260
this possibility for like
plausible deniability.

510
00:30:55,020 --> 00:30:58,010
That's so broad. You know what
I mean? It's like, you know,

511
00:30:58,011 --> 00:31:01,760
it's like the tobacco industry in the
sixties and seventies I was just reading

512
00:31:01,761 --> 00:31:02,860
this great article by a,

513
00:31:02,910 --> 00:31:07,040
the writer Tim Harford about this in
the sixties and seventies the tobacco

514
00:31:07,041 --> 00:31:12,041
industry led this very calculated effort
to sort of push back against cancer

515
00:31:12,861 --> 00:31:17,861
science by just injecting
a little bit of doubt here.

516
00:31:18,141 --> 00:31:21,590
A little bit of doubt there, right there
on the other hand. On the other hand,

517
00:31:21,591 --> 00:31:23,000
this and on the other hand that,

518
00:31:23,001 --> 00:31:26,240
and the idea was to create just enough
wiggle room that nothing happens.

519
00:31:26,450 --> 00:31:28,820
They do that with climate
change too. Exactly. Yeah.

520
00:31:29,000 --> 00:31:33,110
And it's that little bit of
doubt that creates paralysis.

521
00:31:33,560 --> 00:31:36,770
And is that what's going to happen that
like there's going to be paralysis now

522
00:31:37,340 --> 00:31:41,390
writ large because now we're talking
about the very things we see,

523
00:31:41,391 --> 00:31:42,470
the very things we hear

524
00:31:43,570 --> 00:31:43,700
[inaudible]

525
00:31:43,700 --> 00:31:44,091
but wait,

526
00:31:44,091 --> 00:31:47,540
but we don't you think that before we
get completely carried away with the

527
00:31:47,541 --> 00:31:49,610
threat of this technology
because, you know,

528
00:31:49,640 --> 00:31:53,900
maybe we should just find out
literally where we are now. Yeah.

529
00:31:53,930 --> 00:31:57,230
We should give it a spin.
Yeah. So at this moment,

530
00:31:57,231 --> 00:32:01,000
do you think making one of these
eclipses is, is possible? Okay.

531
00:32:01,090 --> 00:32:04,610
I think it's entirely possible. Um, it
just, I would be careful what it is.

532
00:32:11,990 --> 00:32:12,150
[inaudible]

533
00:32:12,150 --> 00:32:13,830
after the break, things get

534
00:32:15,200 --> 00:32:16,033
fake.

535
00:32:30,080 --> 00:32:30,600
[inaudible]

536
00:32:30,600 --> 00:32:34,230
hi everyone. It's Angela
calling from Dallas, Texas.

537
00:32:34,380 --> 00:32:39,210
Radiolab is supported in part by
the Alfred p Sloan Foundation,

538
00:32:39,260 --> 00:32:43,340
enhancing public understanding of science
and technology in the modern world.

539
00:32:43,520 --> 00:32:48,520
More information
aboutSloan@wwwdotsloan.org.

540
00:32:50,030 --> 00:32:51,530
Thanks Radiolab.

541
00:32:56,840 --> 00:33:00,980
Jad Robert Radiolab. So we're back.
We're going to now fake something.

542
00:33:01,040 --> 00:33:04,820
We're going to build our own video
from scratch. Fake words, fake faces,

543
00:33:05,000 --> 00:33:07,280
because we want to know like in use,

544
00:33:07,340 --> 00:33:12,050
how dangerous are these technologies
really? Can they make a convincing fake?

545
00:33:12,080 --> 00:33:14,390
Are they as easy as advertised?

546
00:33:14,450 --> 00:33:19,450
So we will find out by
giving the assignment as
always to our long suffering.

547
00:33:20,060 --> 00:33:23,720
Simon Adler. So while I was in
Seattle talking to during Gleaves,

548
00:33:23,990 --> 00:33:27,950
I not so subtly hinted that
I would really like to, uh,

549
00:33:28,160 --> 00:33:31,680
to give vocal a world. Let's say
I had my hands on it somehow.

550
00:33:32,070 --> 00:33:34,980
What can I do with it?
Well, right now, nothing,

551
00:33:34,981 --> 00:33:38,040
because we haven't shared
it with them. At first,

552
00:33:38,041 --> 00:33:40,740
I just thought he didn't want me
to be able to play around with it.

553
00:33:40,741 --> 00:33:44,550
But then I realized that a, and I don't
even have a personal copy for myself,

554
00:33:44,551 --> 00:33:48,060
but yet I, so it's not even
on the premises here. No,

555
00:33:48,061 --> 00:33:51,240
it's still very much
contained to research.

556
00:33:52,030 --> 00:33:54,370
Oh, but

557
00:33:55,430 --> 00:33:58,930
hi. Hi there. Hey Matt. I,
yeah, yeah, here. Great.

558
00:33:59,240 --> 00:34:02,750
Eventually I got in touch with
this guy. So I'm Dr Matthew Elites.

559
00:34:02,780 --> 00:34:05,660
I'm the Chief Science Officer
at Sarah Proc Limited,

560
00:34:05,690 --> 00:34:08,900
which is a vocal sentences research
company based in Edinburgh. Yeah,

561
00:34:09,810 --> 00:34:14,810
I called you up because I was hoping that
you could help me to make a video clip

562
00:34:15,811 --> 00:34:16,770
that has, I don't know,

563
00:34:16,771 --> 00:34:21,771
like George Bush or Barack Obama saying
things that they have never sent.

564
00:34:23,960 --> 00:34:28,460
Yep. That sounds great. That's it. It was
just game. Yeah, no. See the thing is,

565
00:34:28,461 --> 00:34:31,670
what his company does is
not quite the same as Voco.

566
00:34:31,940 --> 00:34:33,650
What they do is like for a client,

567
00:34:33,890 --> 00:34:37,730
they'll create a voice that you can
then just type in words or sentences and

568
00:34:37,731 --> 00:34:40,940
make that voice say whatever
you want it to say. I feel sad.

569
00:34:40,941 --> 00:34:42,950
That's an interesting idea. They've, uh,

570
00:34:43,010 --> 00:34:45,020
created voices with a variety of accents.

571
00:34:45,021 --> 00:34:47,840
Great for you said last summer
in a variety of languages.

572
00:34:50,420 --> 00:34:50,690
[inaudible]

573
00:34:50,690 --> 00:34:53,750
and in their spare time when they're not
making voices for clients [inaudible]

574
00:34:54,070 --> 00:34:55,060
governor knowledge. What's negative?

575
00:34:55,130 --> 00:35:00,130
I think they're building celebrity voices
future and it just so happens they've

576
00:35:00,651 --> 00:35:03,620
got a Barack Obama and
a George Bush Bot. Yeah.

577
00:35:03,800 --> 00:35:07,240
How did you create a
George Bush robot? Well,

578
00:35:07,390 --> 00:35:11,050
a great thing about George Bush is that
he was president of the United States

579
00:35:11,051 --> 00:35:13,240
for some time. The morning,
good morning, good morning,

580
00:35:13,270 --> 00:35:17,200
which means he had to give the weekly
presidential address. We can go today.

581
00:35:17,201 --> 00:35:20,410
I received a great honor and the
other great thing about the addresses,

582
00:35:20,600 --> 00:35:22,000
it's completely copyright free,

583
00:35:22,001 --> 00:35:25,090
so we were allowed to do anything we
liked with that audio for the people of

584
00:35:25,091 --> 00:35:28,510
America, maybe things that they haven't
envisaged that we going to do with it.

585
00:35:28,630 --> 00:35:31,450
Real quick digression here just
because it's absolutely fascinating.

586
00:35:31,930 --> 00:35:36,280
It looks like we're actually about to
enter this really sticky gray area when it

587
00:35:36,281 --> 00:35:39,220
comes to ownership. For
example, in Rodeo book,

588
00:35:39,310 --> 00:35:43,320
If you record an audio book and you've
signed over the rights to those audio

589
00:35:43,321 --> 00:35:46,340
files to the publisher, the publisher
has the copyright. You, you,

590
00:35:46,350 --> 00:35:50,190
you don't own it. You do not own your
own voice. Is that really true? Yeah.

591
00:35:50,940 --> 00:35:52,570
Anyway, back to Bush,

592
00:35:52,630 --> 00:35:56,110
so I took all those weekly
addresses about six hours worth,

593
00:35:56,140 --> 00:35:58,660
which is a lot more tape
than Volvo's 20 minutes,

594
00:35:58,690 --> 00:36:01,210
but what he did with it
is pretty similar, right?

595
00:36:01,211 --> 00:36:04,840
You Fed them into this machine learning
algorithm along with their transcripts.

596
00:36:05,020 --> 00:36:08,010
And then with the program, we'll do it.

597
00:36:08,200 --> 00:36:11,890
We'll take the text and it will analyze
it in terms of the linguistics that will

598
00:36:11,891 --> 00:36:14,860
say, this is the word,
social security, social,

599
00:36:15,430 --> 00:36:19,780
what social is made out of
the sound, sir. Oh right.

600
00:36:20,200 --> 00:36:25,200
And so we'll cut those sounds out
into lots of little tiny pieces.

601
00:36:25,360 --> 00:36:27,340
And it did that for all of the words

602
00:36:28,870 --> 00:36:33,010
in all of these addresses
around 80,000 in total.

603
00:36:34,330 --> 00:36:38,290
Put them all in this database with tons
of info about what sound came before it

604
00:36:38,440 --> 00:36:43,150
after, et cetera. And once that database
is built, all that's left to do,

605
00:36:43,170 --> 00:36:44,890
I type in some text and then I push,

606
00:36:44,980 --> 00:36:48,790
go and try and find a
set of little sounds,

607
00:36:48,791 --> 00:36:50,650
which will join together really nicely.

608
00:36:51,520 --> 00:36:54,460
And then I push play in the
see how well they came out.

609
00:36:56,310 --> 00:36:59,960
So what we did was we found an
old video of former presidents,

610
00:36:59,970 --> 00:37:02,130
George Bush and Barack Obama together,

611
00:37:04,010 --> 00:37:06,300
they're shaking hands,
making generic statements.

612
00:37:06,330 --> 00:37:07,860
The exact clip isn't important,

613
00:37:07,950 --> 00:37:12,950
but we wondered could we turn that
clip from a boring meet and greet to a

614
00:37:13,501 --> 00:37:16,650
scenario where Bush is
telling Obama a joke.

615
00:37:16,950 --> 00:37:19,860
So we convinced a comedy
writer Rachel Axler,

616
00:37:19,861 --> 00:37:23,820
who works for the show veep to write as
few jokes and then sent it off to Matt

617
00:37:23,821 --> 00:37:25,920
and this is what the computer spat out

618
00:37:26,590 --> 00:37:31,200
and well it goes to something like
knock knock. Who's there? Oval. Oval.

619
00:37:31,770 --> 00:37:35,730
Oval. I think it's something
about the Oval Office. Probably.

620
00:37:36,150 --> 00:37:41,070
That was a a very good joke. Mr
President, my wife Laurie tells it better.

621
00:37:44,430 --> 00:37:48,240
What the hell is that? Wait,
what was it that was terrible?

622
00:37:50,100 --> 00:37:53,270
That was like, was it,
I don't even get, Hey,

623
00:37:53,280 --> 00:37:54,690
I don't understand that joke at all.

624
00:37:55,080 --> 00:37:57,450
And that's literally what
the computer's flat out.

625
00:37:57,510 --> 00:37:59,850
That is what the computer
spit out. And truth be told,

626
00:37:59,851 --> 00:38:01,480
I don't think it's anywhere it, it did.

627
00:38:01,481 --> 00:38:04,920
It is not worthy of the negative
response that you are getting. Terrible.

628
00:38:05,190 --> 00:38:07,200
Terrible. Let me show you another one.

629
00:38:07,510 --> 00:38:11,830
So happy to be joining forces with
this good man to put cortisone in your

630
00:38:11,831 --> 00:38:12,664
drinking water.

631
00:38:13,450 --> 00:38:17,200
This all gets to help protect people's
teeth so they don't get fillings.

632
00:38:17,590 --> 00:38:21,340
Isn't that fluoride? Oh shoot.
I think I signed the wrong bill.

633
00:38:22,750 --> 00:38:26,050
That's good. That's a legitimate
no, that the robots are terrible.

634
00:38:26,730 --> 00:38:30,400
Joke is funny. Yeah. I liked the joke,
but the robot just massacred that joke,

635
00:38:30,460 --> 00:38:33,820
which is in itself kind of a joke. Well,
I do think it, yeah, let me get into,

636
00:38:33,821 --> 00:38:33,911
well,

637
00:38:33,911 --> 00:38:36,700
I think you two are far more than you
should be and you're far more critical

638
00:38:36,701 --> 00:38:41,020
than the average listener. However,
Matt is so wrong about it. But anyway.

639
00:38:41,200 --> 00:38:45,750
Anyhow, Matt did tell me that, uh,
conversations I getting people to see the,

640
00:38:45,800 --> 00:38:49,570
to talk back and forth to each other
are still really difficult for a

641
00:38:49,571 --> 00:38:50,404
synthesizer to do.

642
00:38:50,510 --> 00:38:53,390
But you know, conversational stuff
is always difficult and in fact,

643
00:38:53,391 --> 00:38:56,210
we're going to see, it's going to
be a long time before we get really,

644
00:38:56,330 --> 00:38:59,690
really easy conversational synthesis.
There's all sorts of barriers to that.

645
00:39:00,300 --> 00:39:03,820
There's a human quality, uh, to a,
to a conversation that, uh, that,

646
00:39:03,880 --> 00:39:07,700
that the synthesizers can't quite capture
yet. But he also told us that, um,

647
00:39:07,940 --> 00:39:11,270
you know, if we add, once we add the
video or if we add a video to this, uh,

648
00:39:11,510 --> 00:39:14,180
it will smooth out a lot
of, a lot of the problems

649
00:39:14,510 --> 00:39:16,580
when you have the faces as well speaking.

650
00:39:16,581 --> 00:39:20,870
People are not focusing in the audio and
you can't hear that the arrows in the

651
00:39:20,871 --> 00:39:21,704
same way.

652
00:39:21,730 --> 00:39:22,563
So

653
00:39:26,190 --> 00:39:26,550
hello?

654
00:39:26,550 --> 00:39:30,150
Hey, is this Kyle? Oh yeah, great,
great. I found these two Grad students.

655
00:39:30,180 --> 00:39:33,560
My name is [inaudible]. I
am Tyler Shefsky from uh,

656
00:39:33,600 --> 00:39:36,240
the University of southern California USC.

657
00:39:36,300 --> 00:39:40,380
They also do a lot of facial reenactment
research and agreed to help us. But,

658
00:39:40,670 --> 00:39:45,270
uh, making these visuals also turned
out to be way harder than we thought.

659
00:39:45,520 --> 00:39:49,500
It turned out the clip we chose
posed some serious challenges. Uh,

660
00:39:49,710 --> 00:39:52,260
there were too many side
shots of Obama's face. Uh,

661
00:39:52,261 --> 00:39:54,450
the lighting was all wrong and, uh,

662
00:39:54,480 --> 00:39:59,430
eventually I got an email one late Sunday
night saying, uh, it's not gonna work.

663
00:39:59,580 --> 00:40:04,020
Okay. So now I think I can draw a line
here and I can point out that this,

664
00:40:04,060 --> 00:40:07,380
that we maybe got overexcited
about this technology.

665
00:40:07,710 --> 00:40:10,500
It is not yet ready for true deceit.

666
00:40:10,530 --> 00:40:13,980
You have been fumbling and
fumbling and fumbling here.

667
00:40:14,010 --> 00:40:17,340
I have with them fumbling. I am not. Okay.

668
00:40:18,240 --> 00:40:21,390
I find it interesting psychologically
that Simon feels like it's a personal

669
00:40:21,391 --> 00:40:26,280
failure. I don't like to fail.
You should. This is failure,

670
00:40:26,580 --> 00:40:27,370
so, okay.

671
00:40:27,370 --> 00:40:29,980
Just just uh, just on Simon's behalf,

672
00:40:30,010 --> 00:40:32,710
on the behalf of actually trying to
answer the question, we felt like, okay,

673
00:40:32,711 --> 00:40:36,190
maybe, maybe we should
try this one last time.

674
00:40:36,400 --> 00:40:40,720
Let's find a simpler Obama video and
with the audio rather than like whole

675
00:40:40,721 --> 00:40:44,200
phrases. Let's just do a couple of word
replacements here or there. By the way,

676
00:40:44,201 --> 00:40:46,510
the only reason we're using Obama
is that he seems to be the cow.

677
00:40:46,511 --> 00:40:48,470
All of these technologies
are built around it.

678
00:40:48,500 --> 00:40:53,500
In case we chose the video of Obama's
last weekly address and we chose the audio

679
00:40:53,621 --> 00:40:56,920
from a talk he'd given in
Chicago after he left off.

680
00:40:59,130 --> 00:41:00,090
What's been known.

681
00:41:00,091 --> 00:41:04,120
I want to know in this speech he sort of
talks about what he's going to do next,

682
00:41:04,121 --> 00:41:06,790
how he's still going to keep fighting
for what he believes is right,

683
00:41:06,940 --> 00:41:11,940
filled with idealism and absolutely
certain that somehow I was going to change

684
00:41:12,491 --> 00:41:14,020
the world. But we thought,

685
00:41:14,021 --> 00:41:16,840
what if in an alternate reality
he didn't want to keep fighting.

686
00:41:17,350 --> 00:41:20,920
What if he could at that moment see the
divisions ahead and he was just like,

687
00:41:20,980 --> 00:41:24,040
that's too much. I give up now.

688
00:41:24,041 --> 00:41:26,830
Truth is we didn't think too hard about
this cause we didn't have much time.

689
00:41:26,831 --> 00:41:29,290
We just whipped it together.
Did a script based on words.

690
00:41:29,291 --> 00:41:32,260
Obama used with a few changes,
send it off to the guys at USC.

691
00:41:32,590 --> 00:41:36,790
All right. And I videotaped myself,
uh, seeing this new script so that, uh,

692
00:41:36,980 --> 00:41:41,750
we can use that video of my face to pop
a ties, the former president. And uh,

693
00:41:41,780 --> 00:41:43,780
when, when we got the final video back,

694
00:41:47,100 --> 00:41:48,270
I have to say it was,

695
00:41:50,910 --> 00:41:54,090
I was expecting it to be horrible
and where did it have a good laugh?

696
00:41:54,120 --> 00:41:58,530
But I it went from like Laffy Kalitta to,

697
00:41:58,860 --> 00:42:02,370
oh wait, this is creepy.

698
00:42:02,490 --> 00:42:06,360
Well, yeah, no, I, I was suddenly,
uh, I had been gang busters.

699
00:42:06,480 --> 00:42:07,860
We got to release this thing, uh,

700
00:42:08,250 --> 00:42:11,550
and not tell anybody and try to fake
out the entire world. Uh, but when,

701
00:42:11,580 --> 00:42:15,870
when I saw it, there was,
uh, a reluctancy. You mean
you went, oh no, I went,

702
00:42:15,871 --> 00:42:18,780
Oh God, yeah, yeah. I
thought, oh, this, this,

703
00:42:18,990 --> 00:42:20,400
and you know, my personal
thought was like,

704
00:42:20,401 --> 00:42:23,640
it was convincing enough that I got
genuinely spooked. But you know,

705
00:42:23,641 --> 00:42:24,331
just in fairness,

706
00:42:24,331 --> 00:42:27,120
we shouldn't sit around talking
about something people can't see.

707
00:42:27,780 --> 00:42:32,520
Go to future of fake news.com and check
it out for yourself. It's all one word,

708
00:42:32,521 --> 00:42:37,500
future of fake news.com and
it'll pop right up. You can see,

709
00:42:37,530 --> 00:42:42,120
tell us what you think. You can see
how Simon made the video. Check it out.

710
00:42:43,200 --> 00:42:47,620
Anyhow, the whole process got
us all thinking like, oh wow.

711
00:42:47,780 --> 00:42:52,660
Well if we bunch of idiots can do this,
uh, for no money very, very quickly,

712
00:42:53,410 --> 00:42:58,090
what will this mean to like a newsroom?
For example, just to start there.

713
00:42:58,710 --> 00:43:00,200
Uh, we're at the level now with,

714
00:43:00,230 --> 00:43:05,230
with this kind of thing where
we need technologists to,

715
00:43:05,450 --> 00:43:10,070
to verify or knocked down. And
again, news executive, John Klein,

716
00:43:10,310 --> 00:43:15,310
I don't think journalists, English majors
are, are, are going to be the ones to,

717
00:43:15,380 --> 00:43:19,130
to solve this. You know, you, you may
have been editor of your school paper,

718
00:43:19,131 --> 00:43:22,100
but this is beyond your, your capability.

719
00:43:22,101 --> 00:43:27,101
But if you're good at collaborating
with engineers and scientists,

720
00:43:28,120 --> 00:43:29,480
uh, you know, you, you'll,

721
00:43:29,510 --> 00:43:33,230
you'll have a good chance of working
together to you to figure it out.

722
00:43:33,320 --> 00:43:34,100
So we need,

723
00:43:34,100 --> 00:43:38,540
we need technical expertise
more than we ever have.

724
00:43:38,960 --> 00:43:41,490
Okay. Can I ask you in
your heart, this isn't,

725
00:43:41,610 --> 00:43:44,910
and they let me compare your heart to
my heart for a second. In my heart,

726
00:43:45,630 --> 00:43:50,190
I want somebody to tell the researchers
he has, sorry, you can't do that.

727
00:43:50,460 --> 00:43:52,850
Sorry. You know, I know.
It's really cool. I know you,

728
00:43:52,860 --> 00:43:56,550
I know you probably are really
proud of that algorithm, but um,

729
00:43:57,300 --> 00:43:59,700
some men in black are going to walk in
right now and they're gonna take your

730
00:43:59,701 --> 00:44:03,120
computers away and, and
you, you just can't, sorry.

731
00:44:03,180 --> 00:44:06,330
Society is going to overrule
you right now. Did you,

732
00:44:06,420 --> 00:44:08,400
is there a part of you
that just dictatorial,

733
00:44:08,401 --> 00:44:10,510
he wants to just like
squash this [inaudible]

734
00:44:10,820 --> 00:44:14,050
well sure. But wouldn't you
still have the, what are they,

735
00:44:14,070 --> 00:44:19,070
the FSB and Moscow or the CIA utilizing
this and developing it anyway,

736
00:44:20,540 --> 00:44:23,300
weaponizing it so to speak.

737
00:44:24,290 --> 00:44:29,290
Probably and I think that the top
down model could never contain that.

738
00:44:30,110 --> 00:44:34,380
John says ultimately what's happening
is probably going to be bigger than any

739
00:44:34,381 --> 00:44:37,770
one organization or any
one newsroom can solve.

740
00:44:38,220 --> 00:44:43,020
He said it'll probably end up coming down
to the 14 and 15 year olds of tomorrow

741
00:44:43,200 --> 00:44:46,950
who will grow up using this
technology, making fake videos,

742
00:44:46,951 --> 00:44:51,951
being the victims of fake videos and
that maybe in the maze of them having to

743
00:44:53,131 --> 00:44:56,160
parse truth from fiction
in such a personal way.

744
00:44:56,700 --> 00:45:01,260
Some kind of code will develop.
I'm an optimist by nature. I do.

745
00:45:01,320 --> 00:45:03,990
I look at this and I say, well,
somebody's going to figure it out.

746
00:45:04,120 --> 00:45:09,120
What worries me is the larger context
within this state within which this takes

747
00:45:09,181 --> 00:45:14,130
place. Uh, this is all occurring
within a context of massive news,

748
00:45:14,131 --> 00:45:16,410
illiteracy and the,

749
00:45:16,470 --> 00:45:21,470
the consumers seem to be just throwing
their hands up and tiring of trying to

750
00:45:21,961 --> 00:45:25,620
even figure it out. And
so just the, the, the,

751
00:45:25,950 --> 00:45:30,950
the work involved in getting to the
bottom of the truth is unappealing to a

752
00:45:32,941 --> 00:45:36,150
growing percentage of the audience.

753
00:45:36,210 --> 00:45:41,210
And I'm not sure where Gen z the
teenagers of today come out on this.

754
00:45:42,840 --> 00:45:46,380
Let's hope that they are
more willing to do the work.

755
00:45:46,590 --> 00:45:50,980
Maybe out of self interest, maybe so
that they're not dissed by, you know,

756
00:45:51,390 --> 00:45:56,280
the girl in social studies. But that's
our best hope for overcoming it.

757
00:45:56,810 --> 00:45:57,050
Okay.

758
00:45:57,050 --> 00:46:00,100
Because everybody else
seems to be sick of trying

759
00:46:08,420 --> 00:46:09,253
[inaudible]

760
00:46:15,430 --> 00:46:16,030
uh Huh.

761
00:46:16,030 --> 00:46:20,650
Reporters, Simon Adler. This piece
was produced by Simon and Amy McCune.

762
00:46:31,970 --> 00:46:32,240
[inaudible]

763
00:46:32,240 --> 00:46:37,240
very special thanks to Kyle o Shefsky
and the entire team at USC Institute for

764
00:46:38,420 --> 00:46:40,010
Creative Technology for all their work,

765
00:46:40,160 --> 00:46:43,010
manipulating that video
of President Obama.

766
00:46:43,550 --> 00:46:45,970
And thanks to Matthew Aylett for Cindy,

767
00:46:46,020 --> 00:46:48,450
the sizing. So, so many words for us.

768
00:46:49,440 --> 00:46:52,020
Rachel Axler for writing us the
jokes that we tried to use. Uh,

769
00:46:52,130 --> 00:46:56,130
so from power for billing, it's an
amazing website, Angus Neil, Amy parle,

770
00:46:56,160 --> 00:47:01,160
everybody in the w NYC
newsroom for advising us and
giving us reaction shots to

771
00:47:01,801 --> 00:47:05,040
the face to face video and to David
Carroll for putting us in touch with Nick

772
00:47:05,040 --> 00:47:05,880
Bilton in the first place.

773
00:47:06,060 --> 00:47:09,540
And to Nick Dalton for inspiring
this whole story with his article,

774
00:47:09,900 --> 00:47:10,733
he's got a new one,

775
00:47:10,860 --> 00:47:15,240
a book actually American kingpin about
the founder of a black market website

776
00:47:15,241 --> 00:47:19,230
called the Silk Road and to superstar
in switching the coin computer scientist

777
00:47:19,231 --> 00:47:22,950
who works in eras lab, who helped us
understand what the heck was going on.

778
00:47:23,040 --> 00:47:26,610
And finally you can see the video that
we created as well as a bunch of other

779
00:47:27,360 --> 00:47:30,970
kind of a crazy that we mentioned
throughout this episode.

780
00:47:30,971 --> 00:47:34,240
It's at future of fake
news.com. All one word,

781
00:47:34,540 --> 00:47:39,250
future of fake news.com.
And uh, with that,

782
00:47:40,390 --> 00:47:44,980
my real cohosts and I will
bet you a Jew. I'm jet Booman.

783
00:47:45,160 --> 00:47:48,340
I'm rob crow, which who we really are.

784
00:47:49,750 --> 00:47:52,840
I'm glad we could finally be
honest about that all these years.

