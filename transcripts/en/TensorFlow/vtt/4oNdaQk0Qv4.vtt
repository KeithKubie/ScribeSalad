WEBVTT
Kind: captions
Language: en

00:00:00.170 --> 00:00:01.374
♪ (intro music) ♪

00:00:05.831 --> 00:00:07.005
Hello, everybody.

00:00:07.005 --> 00:00:08.025
(applause)

00:00:08.035 --> 00:00:09.465
My name is Mustafa.

00:00:09.465 --> 00:00:12.003
Today I'll talk about high level APIs.

00:00:12.003 --> 00:00:15.054
But I'll keep practitioners
like you in mind.

00:00:15.494 --> 00:00:19.972
So to keep the practitioners in mind,
we'll define an example

00:00:19.972 --> 00:00:23.934
where you can improve user happiness
by power of Mission Learning.

00:00:25.154 --> 00:00:31.453
And after defining our example project,
we'll use Pre-made Estimators

00:00:31.763 --> 00:00:33.814
to start our first experiment.

00:00:34.684 --> 00:00:38.836
Then we'll experiment more
with every feature we have

00:00:39.196 --> 00:00:43.857
by Feature columns and we'll introduce
a couple of Pre-made Estimators

00:00:43.857 --> 00:00:45.820
that you can experiment more.

00:00:46.420 --> 00:00:51.003
And we'll learn how can you experiment
other modeling ideas too.

00:00:51.003 --> 00:00:54.571
So there are a lot of topics
we will cover in this talk.

00:00:54.571 --> 00:00:57.273
And we'll also talk about
how can you scale it up

00:00:57.753 --> 00:01:01.201
and serving so that you can use it
in your products.

00:01:02.591 --> 00:01:04.762
Let's talk about the Estimators.

00:01:04.762 --> 00:01:07.960
So it's a library that lets you focus
on your experiment.

00:01:07.960 --> 00:01:11.823
There are thousands of engineers,
this is not a small number,

00:01:11.823 --> 00:01:17.154
and hundreds of projects in <i>Google</i>
who use Estimators.

00:01:17.154 --> 00:01:19.773
So we learn a lot from their experiments.

00:01:20.033 --> 00:01:25.682
And we created our APIs
so that the time from an idea

00:01:25.682 --> 00:01:28.709
to an experiment will be
as short as possible.

00:01:29.939 --> 00:01:34.064
And I'm really happy to share
all these experience with all of you.

00:01:34.864 --> 00:01:37.800
Whatever we are using internally
like <i>Google</i>

00:01:37.800 --> 00:01:39.812
is the same as the open source one.

00:01:39.812 --> 00:01:41.815
So you all have the same things.

00:01:43.145 --> 00:01:46.513
Estimators keeps the model function.

00:01:46.513 --> 00:01:48.873
We'll talk about
what's the model function later.

00:01:48.873 --> 00:01:52.751
But it defines your network
and how can you train

00:01:52.751 --> 00:01:55.332
or what is the behavior
during their evaluation

00:01:55.332 --> 00:01:57.171
or during the export time.

00:01:57.811 --> 00:02:01.795
And it provides you some loops
such as training, evaluation

00:02:01.795 --> 00:02:07.103
and it provides you some interface
to integrate with TF Serving.

00:02:07.813 --> 00:02:11.774
Also Estimators keep sessions
so that you don't need to learn

00:02:11.774 --> 00:02:13.502
what is tf.Session.

00:02:13.502 --> 00:02:14.991
It handles it for you.

00:02:15.351 --> 00:02:19.132
But you need to provide data
and as Derek mentioned,

00:02:19.132 --> 00:02:23.562
you can return a tf.Dataset
from your input function.

00:02:24.822 --> 00:02:28.889
So let's define our project
and start with our experiment.

00:02:28.889 --> 00:02:33.333
I love hiking, this is one of the pictures
I took in one of my hikes

00:02:33.333 --> 00:02:38.096
and let's imagine there is a website,
hiking website similar to <i>IMDB</i>

00:02:38.096 --> 00:02:43.455
but it's for hiking
and that website has information

00:02:43.805 --> 00:02:47.832
for each hike
and users are labeling those hikes

00:02:47.832 --> 00:02:49.244
by saying,
"I like this hike."

00:02:49.244 --> 00:02:51.221
"I don't like this hike."
"This is my rating."

00:02:51.221 --> 00:02:52.602
And other stuff.

00:02:52.602 --> 00:02:56.802
And we want to use this data,
let's imagine you have this data

00:02:56.802 --> 00:03:01.203
from that website
to recommend hikes for users.

00:03:02.143 --> 00:03:03.522
How can we do that?

00:03:03.522 --> 00:03:05.342
There are many ways of doing it.

00:03:05.952 --> 00:03:09.613
Let's define one way
that Mission Learning can help us.

00:03:10.063 --> 00:03:14.271
In this case, we want to predict
probability of a like,

00:03:14.271 --> 00:03:18.001
whether a given user will like
a given hike or not.

00:03:19.481 --> 00:03:23.214
What you have, you have hike features
and user features.

00:03:23.214 --> 00:03:25.130
And where can you learn from?

00:03:25.130 --> 00:03:28.081
You learn from the label data,
in this case, that website

00:03:28.081 --> 00:03:31.912
has whether that users like
that hike or not.

00:03:31.912 --> 00:03:36.483
So what can we use to predict
probability of a like?

00:03:37.023 --> 00:03:39.392
You change one of the Pre-made Estimators

00:03:39.392 --> 00:03:43.399
in this case the DNNClassifier
because this is a more strict edition

00:03:43.399 --> 00:03:46.526
or you can think
it's a binary classification problem.

00:03:46.526 --> 00:03:51.103
So with design Pre-made Estimators,
so that it can fit well

00:03:51.103 --> 00:03:55.402
to this kind of problems,
this means you can use it

00:03:55.402 --> 00:03:57.068
as a black box solution.

00:03:58.251 --> 00:04:03.191
Pre-made estimators are surprisingly
popular within <i>Google</i>

00:04:03.751 --> 00:04:05.172
and in many projects.

00:04:05.882 --> 00:04:10.731
Why that many engineers are using
pre-made solutions

00:04:10.731 --> 00:04:13.544
instead of building their own models?

00:04:14.304 --> 00:04:16.553
I think first of all, it works.

00:04:16.973 --> 00:04:21.572
It handles many implementation details
so you can focus on your experiment.

00:04:22.462 --> 00:04:25.996
It has reasonable defaults
for initialization, partitioning,

00:04:25.996 --> 00:04:29.723
or optimization
so that you have a reasonable baseline

00:04:29.973 --> 00:04:31.503
as quick as possible.

00:04:32.573 --> 00:04:35.292
And it is easy to experiment
with many features.

00:04:35.572 --> 00:04:37.002
So we learn about that.

00:04:37.002 --> 00:04:43.402
You can experiment with all of your data
by using the same estimators

00:04:43.402 --> 00:04:44.793
without changing it.

00:04:45.553 --> 00:04:49.922
So let's jump into our first experiment
so that we can have a baseline

00:04:50.542 --> 00:04:52.053
that we will improve.

00:04:54.033 --> 00:04:56.594
I will talk about embedded column
but in this case, let's--

00:04:56.594 --> 00:04:59.092
you are using hike ID.

00:04:59.092 --> 00:05:04.272
It may be a hike name
or an identification as a single feature

00:05:04.272 --> 00:05:05.262
to your model.

00:05:06.422 --> 00:05:09.192
And let's say you instantiate
the DNNClassifier

00:05:09.192 --> 00:05:10.602
with hidden units one.

00:05:10.602 --> 00:05:13.922
So what this will learn,
it will learn other each label

00:05:13.922 --> 00:05:16.420
for each hike ideas
so that may be a good baseline

00:05:16.420 --> 00:05:18.250
for your overall progress.

00:05:19.840 --> 00:05:22.171
You need to say
what is your evaluation data,

00:05:22.171 --> 00:05:26.104
what is your training data,
then you can code train and evaluate.

00:05:26.104 --> 00:05:28.641
Just by these,
a couple of lines, of course,

00:05:29.251 --> 00:05:33.544
you should be able to experiment
and you can see the results

00:05:33.544 --> 00:05:34.902
on the task report.

00:05:35.492 --> 00:05:38.614
For example you can see training
and evaluation loss

00:05:38.614 --> 00:05:42.023
or how accuracy metric is moving.

00:05:42.023 --> 00:05:45.982
So since this is a classification problem
you will see accuracy metric.

00:05:46.612 --> 00:05:49.064
And since this is binary classification,

00:05:49.064 --> 00:05:51.676
you will see
Area Under Curve for Precision-Recall.

00:05:51.676 --> 00:05:55.900
So all of these things are free
and ready to be used.

00:05:57.130 --> 00:05:58.667
Let's experiment more.

00:05:59.547 --> 00:06:02.599
Let's start with the data,
experimenting with the data, itself.

00:06:03.299 --> 00:06:05.920
The design feature columns
with the same mindset,

00:06:05.920 --> 00:06:06.944
we want to keep--

00:06:08.694 --> 00:06:13.850
make it easy to experiment
with your features, with your data.

00:06:14.380 --> 00:06:17.300
And based on our experience,
internal experience,

00:06:17.300 --> 00:06:21.399
it introduces the lines, of course
and may improve the model quality.

00:06:21.919 --> 00:06:25.785
There are a bunch of transformations
you can handle via feature columns.

00:06:25.785 --> 00:06:29.817
These are bucketing, crossing,
hashing, and embedding.

00:06:30.089 --> 00:06:32.938
Each of these needs a careful explanation.

00:06:32.938 --> 00:06:35.217
Unfortunately, I don't have
enough time here

00:06:35.217 --> 00:06:40.158
but you can check Magnus' tutorial
and [Jess' media].

00:06:40.158 --> 00:06:41.379
They are really good.

00:06:43.239 --> 00:06:45.909
Let's experiment
with all the hike features we have.

00:06:46.839 --> 00:06:48.369
Each hike may have tags

00:06:48.369 --> 00:06:51.528
such as kid-friendly,
dog-friendly, birding.

00:06:52.088 --> 00:06:56.780
You may choose indicator column
instead of embedding column in this case

00:06:56.780 --> 00:07:01.129
because you don't have
a huge number of tags

00:07:01.129 --> 00:07:03.739
so you don't need
to read this through dimensionality.

00:07:04.879 --> 00:07:07.367
And for a numerical column,
such as each hike

00:07:07.367 --> 00:07:11.814
may have elevation gain,
you need to normalize

00:07:11.814 --> 00:07:14.966
so that optimization
will be well-conditioned,

00:07:14.966 --> 00:07:16.947
your problem will be well-conditioned.

00:07:16.947 --> 00:07:20.314
And you can use normalizer function here.

00:07:21.284 --> 00:07:25.576
Or you may choose bucketizing,
in this case, the distance of a hike

00:07:25.576 --> 00:07:28.486
we bucketize it
so that it will help the model

00:07:28.486 --> 00:07:30.646
to learn different things
for different segments.

00:07:30.646 --> 00:07:33.995
You can consider it
as a different kind of normalization too.

00:07:35.695 --> 00:07:38.073
How can you use
all of these things together?

00:07:38.073 --> 00:07:40.075
Just putting them into a list?

00:07:40.075 --> 00:07:44.876
That's it, then your system
should work as it is.

00:07:45.716 --> 00:07:47.964
So let's experiment with personalization.

00:07:48.614 --> 00:07:51.286
What we mean by personalization
is instead of recommending

00:07:51.286 --> 00:07:54.906
the same hikes to all users,
let's recommend different hikes

00:07:54.906 --> 00:07:57.646
for different users
based on their interests.

00:07:58.906 --> 00:08:03.205
And one way to do that
is using user features.

00:08:03.205 --> 00:08:05.505
In this case, we are using user embedding

00:08:06.185 --> 00:08:07.804
by embedding column.

00:08:07.804 --> 00:08:11.105
So this will let the model to learn

00:08:11.855 --> 00:08:13.995
a vector for each user

00:08:14.305 --> 00:08:18.563
and put the users closer
if their hike preferences are similar.

00:08:20.687 --> 00:08:22.039
And how can you use that?

00:08:22.039 --> 00:08:24.507
Again, it's just depending into your list.

00:08:24.507 --> 00:08:28.718
And you need to also play with
a number of hidden units

00:08:29.228 --> 00:08:31.418
because you have many more features now

00:08:31.418 --> 00:08:35.639
and you need to let your model
to learn different transformations.

00:08:37.349 --> 00:08:39.916
And the rest of the pipeline should work.

00:08:39.916 --> 00:08:42.769
You will hear this sentence a lot
during this talk

00:08:42.769 --> 00:08:44.156
because it's based on that.

00:08:44.156 --> 00:08:46.065
Rest of the pipeline should work

00:08:46.065 --> 00:08:49.197
and you should be able
to analyze your experiments.

00:08:50.657 --> 00:08:52.137
Let's experiment more.

00:08:52.137 --> 00:08:56.157
We have a couple of pre-made solutions
I mentioned to you it's very popular.

00:08:56.157 --> 00:08:59.667
And I picked on the two of them here
to show.

00:08:59.667 --> 00:09:01.247
One of them is Wide-n-Deep.

00:09:01.247 --> 00:09:04.590
It's a joint training of Neural model
and Neural Network.

00:09:04.590 --> 00:09:07.305
It may have your product or it may not.

00:09:07.305 --> 00:09:08.248
You need to experiment.

00:09:08.248 --> 00:09:09.917
So let's start our the experiment.

00:09:12.307 --> 00:09:17.275
You need to define what are the features
you want to feed to Neural Network.

00:09:17.655 --> 00:09:20.371
Again, we have feature column,
here's the list.

00:09:20.711 --> 00:09:23.147
And you need to define
what are the features

00:09:23.147 --> 00:09:24.938
you need to feed into linear part.

00:09:24.938 --> 00:09:30.636
In this case, I intentionally put user ID
and tags crossing so that,

00:09:30.636 --> 00:09:36.155
for example, if a user always picks
dog-friendly hikes,

00:09:37.025 --> 00:09:41.785
the model will explicitly learn
via this feature, via this cross.

00:09:42.995 --> 00:09:46.613
And you can instantiate the
DNNLinearCombinedClassifier

00:09:46.613 --> 00:09:48.722
instead of the DNNClassifier.

00:09:48.722 --> 00:09:51.195
And the rest of the pipeline should work.

00:09:53.310 --> 00:09:56.825
Based on the Kaggle survey,
2017 Kaggle survey,

00:09:56.825 --> 00:09:58.635
trees are extremely popular.

00:09:59.455 --> 00:10:03.377
And we are introducing
Gradient Boosted Trees

00:10:03.377 --> 00:10:05.024
as a pre-made estimator.

00:10:06.448 --> 00:10:09.547
This means you can experiment
with Neural Network

00:10:09.547 --> 00:10:13.246
and Gradient Boosted Trees
without changing your pipeline.

00:10:14.206 --> 00:10:17.438
Let's start with our experimentation
with Gradient Boosted Trees.

00:10:18.628 --> 00:10:21.547
In the current version,
we only support bucketized column

00:10:21.547 --> 00:10:25.307
and we are working on to support
numerical column

00:10:25.307 --> 00:10:27.378
and categorical column too.

00:10:28.138 --> 00:10:32.008
Here we use height distance
and high elevation gain

00:10:32.628 --> 00:10:36.389
and we bucketize them
and then you can instantiate

00:10:36.389 --> 00:10:38.136
BoostedTreesClassifier.

00:10:38.776 --> 00:10:41.082
And the rest of the pipeline should work.

00:10:41.672 --> 00:10:46.638
We know that trees are not
as computationally expensive as--

00:10:46.948 --> 00:10:49.936
or training trees are not
as computationally expensive

00:10:49.936 --> 00:10:51.637
as training Neural Networks.

00:10:52.347 --> 00:10:54.745
And many of datasets fit into memory.

00:10:55.235 --> 00:10:58.296
So by leveraging that,
we provide you a utility

00:10:58.296 --> 00:11:04.461
so that you can train your model
more than an Order of Magnitude faster

00:11:04.791 --> 00:11:06.171
than the usual one.

00:11:08.071 --> 00:11:10.447
And the rest of the pipeline should work.

00:11:11.727 --> 00:11:15.776
So let's say this already
pre-made solutions

00:11:15.776 --> 00:11:19.178
are not enough for you
and you want to experiment more ideas.

00:11:19.178 --> 00:11:20.867
Let's talk about that.

00:11:20.867 --> 00:11:25.746
Before diving into this
high-level solutions that you can use,

00:11:26.266 --> 00:11:28.935
let's look at the anatomy of the
Feedforward network

00:11:28.935 --> 00:11:30.625
in a supervised setting.

00:11:30.625 --> 00:11:35.205
In this case you have a network
which you'll fit the features

00:11:35.205 --> 00:11:39.845
and based on the output of network
and the labels, you need to decide

00:11:39.845 --> 00:11:43.476
what is the loss, what is the objective
you want to minimize

00:11:43.866 --> 00:11:49.096
and what are the metrics
that you will use as a success metric

00:11:49.096 --> 00:11:50.677
for your evaluation.

00:11:50.677 --> 00:11:55.324
And your predictions on serving time
may be different than in training time

00:11:55.324 --> 00:11:59.365
for example, if you have a large
multi-class setting,

00:11:59.365 --> 00:12:03.286
you may want to use
just ranking of the classes

00:12:03.286 --> 00:12:05.914
instead of the probabilities
in the serving time.

00:12:05.914 --> 00:12:07.955
For that, you don't need
to calculate probability.

00:12:07.955 --> 00:12:12.025
You can use the logistics
out of network to rank them.

00:12:12.695 --> 00:12:18.196
So for all of this decisions,
we obstructed them out under Head API.

00:12:18.486 --> 00:12:23.065
So Head API expects you to give the label

00:12:23.625 --> 00:12:27.363
and out of your network
and provides all of these things for you.

00:12:27.363 --> 00:12:29.102
We'll see in action.

00:12:30.642 --> 00:12:35.443
And model function is an implementation
of this head and network together.

00:12:36.643 --> 00:12:38.605
We'll talk about DNNClassifier.

00:12:38.605 --> 00:12:41.574
So DNNClassifier has a model function,

00:12:41.574 --> 00:12:44.393
so it's an implementation,
it's specifically an implementation

00:12:44.393 --> 00:12:45.826
for head and network.

00:12:46.846 --> 00:12:51.157
Let's implement this with the Head API,
in this case instead of DNNClassifier

00:12:51.157 --> 00:12:52.944
we use DNNEstimator.

00:12:53.384 --> 00:12:56.375
And we can instantiate ahead, in this case

00:12:56.375 --> 00:13:00.335
it's a binary classification head
because we are trying to predict

00:13:00.335 --> 00:13:02.363
whether it is liked or not liked.

00:13:03.553 --> 00:13:08.153
And why are we introducing this head
since these two lines is the same

00:13:08.153 --> 00:13:11.126
as DNNClassifier,
why are we introducing this head?

00:13:11.126 --> 00:13:14.432
So that you can experiment many more ideas

00:13:14.432 --> 00:13:17.611
by combining different
network architectures

00:13:17.611 --> 00:13:19.305
and different heads.

00:13:19.795 --> 00:13:23.722
For example, you can use Wide-n-Deep
with the Poisson regression head

00:13:23.722 --> 00:13:26.983
or the DNNEstimator
with a multi-label head.

00:13:27.993 --> 00:13:30.584
You can even combine
different heads together,

00:13:30.584 --> 00:13:32.793
we introduce multi-head here.

00:13:32.793 --> 00:13:36.734
So it's a one way of experimenting
with multi-task learning

00:13:37.254 --> 00:13:38.703
with a couple of lines of code,

00:13:38.703 --> 00:13:40.503
you can experiment
with multi-task learning.

00:13:40.503 --> 00:13:41.741
Please check it out.

00:13:45.261 --> 00:13:48.142
Let's say this pre-made architectures
are not enough for you

00:13:48.142 --> 00:13:52.584
and you want to experiment more,
you can write your own model function.

00:13:53.604 --> 00:13:57.663
We strongly recommend to use
TF Keras Layers

00:13:58.333 --> 00:13:59.821
to build your network.

00:14:00.601 --> 00:14:05.343
So you can do whatever you want,
you can be as creative as possible.

00:14:05.883 --> 00:14:08.421
And after you have the output of network,

00:14:08.771 --> 00:14:11.172
you need to pick one
of the optimizers available

00:14:11.172 --> 00:14:14.980
in <i>TensorFlow</i>
and you can use one of the head

00:14:14.980 --> 00:14:16.491
that we mentioned

00:14:16.515 --> 00:14:22.305
which will convert your network output
and the labels

00:14:22.305 --> 00:14:25.121
into training behavior
or evaluation metrics

00:14:25.121 --> 00:14:26.741
or export behaviors.

00:14:27.991 --> 00:14:30.551
Then you can fit this model function
to the estimator,

00:14:30.551 --> 00:14:33.389
again, the rest of the pipeline
should work.

00:14:34.409 --> 00:14:40.028
Keras Model is another way
of creating your model.

00:14:40.528 --> 00:14:43.166
And it's very popular,
it's very intuitive to use.

00:14:43.166 --> 00:14:47.318
For example, this is one
of the Keras Model you can build.

00:14:48.198 --> 00:14:50.047
So how can you get the estimator

00:14:50.047 --> 00:14:52.827
so that the rest
of the pipeline should work?

00:14:52.827 --> 00:14:56.296
You can use model to estimators
which gives to the estimator

00:14:56.296 --> 00:15:00.027
so you can run your experiments
without changing your pipeline.

00:15:02.597 --> 00:15:05.336
Transfer Learning
is another popular technique

00:15:05.336 --> 00:15:07.417
we do experiment with.

00:15:07.877 --> 00:15:12.534
One way of extending is using model A
which is already the trend,

00:15:12.534 --> 00:15:16.075
to improve the prediction of model B.

00:15:17.235 --> 00:15:18.423
How can you do that?

00:15:18.423 --> 00:15:22.236
Surprisingly, just copying
and transferring the variables

00:15:22.236 --> 00:15:24.575
from model A to model B works.

00:15:25.205 --> 00:15:27.423
That's simple but it works.

00:15:27.423 --> 00:15:30.834
And we provide the utility for you.

00:15:32.504 --> 00:15:34.453
You can use WARM start from--

00:15:34.453 --> 00:15:38.326
For example this one line 
is saying that transfer all of the model A

00:15:38.326 --> 00:15:41.326
into model B

00:15:41.326 --> 00:15:47.435
or you can define a subset of model A
to transfer from model A to model B.

00:15:50.155 --> 00:15:53.136
Let's talk about image features,
we talked about embedding,

00:15:53.136 --> 00:15:55.593
categorical column and numerical features

00:15:55.593 --> 00:15:57.924
but what if you have image features?

00:15:58.364 --> 00:16:02.234
How can you use them in your pipeline
without doing--

00:16:02.624 --> 00:16:04.491
it will take up a lot of lines, of course.

00:16:04.991 --> 00:16:09.604
You can implement one
of the state data image classifier

00:16:09.604 --> 00:16:11.564
which is not a couple of lines, of course.

00:16:11.564 --> 00:16:17.003
Or you can, thanks to TF-Hub,
you will learn a lot later,

00:16:18.043 --> 00:16:21.792
you can use this one line,
just one line from the hub

00:16:21.792 --> 00:16:23.290
to instantiate the features column

00:16:23.290 --> 00:16:25.512
which is called 
the image embedding column.

00:16:25.942 --> 00:16:30.694
In this case, you may remember
Jeff mentioned OptiML.

00:16:30.694 --> 00:16:34.843
NASNet is one of the OptiML model

00:16:35.210 --> 00:16:37.033
and it's really good,

00:16:37.033 --> 00:16:39.584
it's one of the state of the art model
you can use.

00:16:39.584 --> 00:16:43.777
We will use NASNet as a featurizer

00:16:43.825 --> 00:16:47.554
which means it will use only 
the output layer of NASNet

00:16:47.554 --> 00:16:49.590
as a feature in your pipeline.

00:16:49.590 --> 00:16:53.664
How can you use that
in the DNNClassifier we talked about?

00:16:53.664 --> 00:16:57.960
Same, you are just depending it
into your feature columns, done.

00:16:59.050 --> 00:17:00.824
Then you can experiment it.

00:17:02.584 --> 00:17:05.691
So let's say you experimented
and you find some models

00:17:05.691 --> 00:17:08.671
but you need to scale it up,
not all of you but some of you

00:17:08.671 --> 00:17:10.611
may need to scale your training.

00:17:10.611 --> 00:17:13.411
So you can use multi-GPU
which means the application

00:17:13.411 --> 00:17:17.461
on different GPUs,
Igor will talk about that after my talk,

00:17:18.431 --> 00:17:20.771
the key point here
you don't need to change the estimator

00:17:20.771 --> 00:17:22.500
or you don't need
to change the model code.

00:17:23.290 --> 00:17:26.362
Everything should work
in just a single line

00:17:26.362 --> 00:17:27.820
of configuration change.

00:17:28.830 --> 00:17:32.960
Or you may want to distribute
your training to multiple missions

00:17:32.960 --> 00:17:35.320
by saying these are workers,
these are parameter servers,

00:17:35.320 --> 00:17:37.362
and there's one evaluator going on.

00:17:38.022 --> 00:17:41.860
Same, you don't need to change
your estimator or model code.

00:17:42.330 --> 00:17:44.940
Everything should work
based on the configuration.

00:17:46.240 --> 00:17:50.439
Or you may want to use
TPUEstimator or TPU.

00:17:50.439 --> 00:17:53.650
In this case, there's a minimal change
in the model function,

00:17:53.650 --> 00:17:58.489
hopefully later we will fix that too
but now there's a minimal change

00:17:58.489 --> 00:18:00.629
in your model function you need to do.

00:18:03.939 --> 00:18:08.230
To use this in your production,
you need to export.

00:18:08.230 --> 00:18:10.317
Or you can-- you need to serve.

00:18:10.317 --> 00:18:12.669
And we recommend you to use TF Serving.

00:18:12.669 --> 00:18:14.816
In the serving time, instead of reading

00:18:15.756 --> 00:18:18.939
data from the files, you have a request.

00:18:18.939 --> 00:18:21.329
And you need to define
the receiver function

00:18:21.329 --> 00:18:26.147
which is defining how can you connect
that request into the model.

00:18:26.147 --> 00:18:30.758
So in this case-- and after that
what will be the output of that model.

00:18:31.368 --> 00:18:34.109
Which is defined by signature definition.

00:18:34.109 --> 00:18:37.170
So here, again,
a couple of lines of code.

00:18:37.170 --> 00:18:40.959
You will export your train model
with TF Serving [trend relay].

00:18:40.959 --> 00:18:45.229
For example, if your request
is TF data example proto,

00:18:45.229 --> 00:18:49.501
you can use this utility function
to get your receiver function.

00:18:50.421 --> 00:18:53.530
And then you can use export said model

00:18:54.120 --> 00:18:58.120
so that it will be used by TF Serving.

00:18:59.580 --> 00:19:03.317
These are the modules I mentioned,
tf.estimator, feature column,

00:19:03.317 --> 00:19:06.358
TF Keras, contrib.estimator,
contrib.feature column.

00:19:06.358 --> 00:19:10.058
Please do not use tf.contrib.learn,
we are deprecating it.

00:19:10.058 --> 00:19:12.780
And these are a couple of talks
and videos I picked.

00:19:15.000 --> 00:19:16.048
You can check it out.

00:19:16.048 --> 00:19:19.259
Thank you, I hope some of you
will improve your products

00:19:19.259 --> 00:19:21.278
with the tools that we mentioned.

00:19:21.278 --> 00:19:22.059
(greeting in foreign language)

00:19:22.059 --> 00:19:22.848
(applause)

00:19:22.848 --> 00:19:24.020
♪ (ending music) ♪

