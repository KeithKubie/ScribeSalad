WEBVTT
Kind: captions
Language: en

00:00:00.380 --> 00:00:02.890
We're not going to get a chance to talk
about a lot of the details on this

00:00:02.890 --> 00:00:06.970
though it's just a great example of
reinforcement learning applied to

00:00:06.970 --> 00:00:09.730
a difficult problem and
clever engineering tricks.

00:00:09.730 --> 00:00:13.340
The one thing that's worth noting
is that in addition to using deep

00:00:13.340 --> 00:00:15.750
nets which are kind of
in vogue at the moment,

00:00:15.750 --> 00:00:19.530
they also did a couple other things in
changing the way that training happened.

00:00:19.530 --> 00:00:21.630
That I think were really,
really important for

00:00:21.630 --> 00:00:23.660
getting the kinds of
results that they got.

00:00:23.660 --> 00:00:26.210
And so it wasn't just a matter of hey,
let's just use Deep Nets.

00:00:26.210 --> 00:00:29.380
It was like Deep Nets plus
really careful training so

00:00:29.380 --> 00:00:32.159
that you don't get these kinds
of catastrophic death spirals,

00:00:32.159 --> 00:00:35.160
like again what my students and I have
tended to see when we try this stuff.

00:00:35.160 --> 00:00:39.350
So this kind of doesn't
work successes story,

00:00:39.350 --> 00:00:41.520
is not really the end of the story.

00:00:41.520 --> 00:00:44.690
In the sense that, well, so
we assign some some reading.

00:00:44.690 --> 00:00:47.680
I think these were reading
associated with this current lesson.

00:00:47.680 --> 00:00:49.500
Three papers, Tesauro,
Boyan and Moore and

00:00:49.500 --> 00:00:53.360
Sutton which kind of tell
an interesting story which is, I think,

00:00:53.360 --> 00:00:57.560
a really nice summary of what
the state of the art is here.

00:00:57.560 --> 00:01:00.130
So, Gerry Tesauro got
TD-Gammon to work really well.

00:01:00.130 --> 00:01:02.140
So that's a three layer
back prop net for

00:01:02.140 --> 00:01:05.690
learning to choose moves in
Backgammon and it worked great.

00:01:05.690 --> 00:01:07.340
So everybody thought,
hey everything works great.

00:01:07.340 --> 00:01:10.090
So then Boyan and Moore wrote
a paper where they showed that

00:01:10.090 --> 00:01:14.410
there's some very simple examples
where things ought to work well but

00:01:14.410 --> 00:01:17.430
they need not,
they can actually work extremely poorly.

00:01:17.430 --> 00:01:20.040
Using this general approach
of substituting in

00:01:20.040 --> 00:01:24.280
a function approximator for a Q
function can lead to terrible results.

00:01:24.280 --> 00:01:25.506
So, it need not work.

00:01:25.506 --> 00:01:30.240
Then Sutton had a reply to that where
he published a paper that said, yeah,

00:01:30.240 --> 00:01:32.850
but you can,
you can do it if you do it right.

00:01:32.850 --> 00:01:35.990
So, he actually replicated
the same results that Boyan and

00:01:35.990 --> 00:01:39.710
Moore showed, not working and showed
that if you change the way the training

00:01:39.710 --> 00:01:42.190
happens you can get those
particular things to work, but

00:01:42.190 --> 00:01:46.085
this really is just, the argument is it
need not work, but it need not not work.

00:01:46.085 --> 00:01:49.620
[LAUGH] Which is not really
a very strong kind of guarantee.

00:01:49.620 --> 00:01:51.880
It's not like it has to work or
it has to not work.

00:01:51.880 --> 00:01:55.940
So this is kind of where we are now
in terms of our understanding.

00:01:55.940 --> 00:01:57.620
&gt;&gt; So
what's the Pollack thing at the bottom?

00:01:57.620 --> 00:01:58.240
&gt;&gt; Pollack?

00:01:58.240 --> 00:02:00.030
Sorry, so yes.

00:02:00.030 --> 00:02:03.390
I just wanted to point out that
why did backgammon work so well?

00:02:03.390 --> 00:02:06.630
It might not have just been the fact
that it was reinforcement learning,

00:02:06.630 --> 00:02:10.100
it might have been, well backgammon is
kind of a good thing to learn this way.

00:02:10.100 --> 00:02:14.810
So, Jordan Pollack had a paper where he
showed that genetic algorithms can also

00:02:14.810 --> 00:02:17.460
do particularly well in
backgammon in particular.

00:02:17.460 --> 00:02:19.940
So, it might not have been
that Jerry Tesauro discovered

00:02:19.940 --> 00:02:21.330
that TD is really great,

00:02:21.330 --> 00:02:25.270
he might have discovered that backgammon
is really great as a test bed for TD.

00:02:25.270 --> 00:02:26.398
&gt;&gt; Yeah, I remember that.

00:02:26.398 --> 00:02:28.976
In fact the argument that Jordan
made was that because it's so

00:02:28.976 --> 00:02:31.209
random it forces you to do
all kinds of exploration and

00:02:31.209 --> 00:02:33.008
keep you from falling
into certain traps.

00:02:33.008 --> 00:02:39.076
That made generalization powerful and
in fact if we compared to Isabelle's

00:02:39.076 --> 00:02:46.040
master thesis work on something that you
thought was simpler, like tic tac toe.

00:02:46.040 --> 00:02:49.230
It actually doesn't do terribly well on
tic tac toe, and that's because it's so

00:02:49.230 --> 00:02:51.580
deterministic there's huge
chunks of the state space,

00:02:51.580 --> 00:02:55.310
you kind of never see even when you're
trying to do this generalization.

00:02:55.310 --> 00:02:55.940
&gt;&gt; Yeah.
Okay.

00:02:55.940 --> 00:02:59.900
And I would summarize that by saying
that nearby states in tic tac toe don't

00:02:59.900 --> 00:03:01.490
necessarily have nearby values.

00:03:01.490 --> 00:03:04.840
You just move one x to
a different place on the grid.

00:03:04.840 --> 00:03:06.926
And the whole gameplay
is very very different.

00:03:06.926 --> 00:03:09.760
Backgammon, because it has such
a tremendously influential random

00:03:09.760 --> 00:03:10.580
component.

00:03:10.580 --> 00:03:13.450
If you move one piece,
it can change things but

00:03:13.450 --> 00:03:15.750
it's not going to change
things you know night and day,

00:03:15.750 --> 00:03:19.550
it's going to change things gradually
and so that kind of gradualness means

00:03:19.550 --> 00:03:22.890
that function approximation is probably
going to, well could at least work out.

00:03:22.890 --> 00:03:24.800
&gt;&gt; Maybe,
it's certainly a good argument.

