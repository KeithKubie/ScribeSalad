WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.903
[APPLAUSE]

00:00:01.903 --> 00:00:02.778
SPEAKER 1: Thank you.

00:00:05.560 --> 00:00:08.770
So the next speaker
is Cliff Kuang.

00:00:08.770 --> 00:00:13.680
Cliff is author of an upcoming
book titled "User Friendly"

00:00:13.680 --> 00:00:14.680
around--

00:00:14.680 --> 00:00:16.480
kind of like a
retrospective on the role

00:00:16.480 --> 00:00:18.670
of interaction designers.

00:00:18.670 --> 00:00:22.910
He was head of product at Fast
Company, author and editor

00:00:22.910 --> 00:00:29.220
of thousands of articles around
design and direction design.

00:00:29.220 --> 00:00:31.730
And yeah, thank you.

00:00:31.730 --> 00:00:32.780
CLIFF KUANG: Thanks.

00:00:32.780 --> 00:00:35.280
Hopefully this is working OK.

00:00:35.280 --> 00:00:38.382
So yeah, thank you
for the introduction.

00:00:38.382 --> 00:00:39.840
As mentioned, I
used to be the head

00:00:39.840 --> 00:00:40.965
of product at Fast Company.

00:00:40.965 --> 00:00:44.130
Before that I was the founder
of a site that some of you guys

00:00:44.130 --> 00:00:47.280
may know called
Fast Company Design.

00:00:47.280 --> 00:00:48.740
And for the last
five years, I've

00:00:48.740 --> 00:00:51.390
been working on a book about
the history of the idea of user

00:00:51.390 --> 00:00:52.990
friendliness.

00:00:52.990 --> 00:00:56.160
Where that's taken me in
the last year is thinking--

00:00:56.160 --> 00:01:00.390
I think more and more deeply
about how interaction design

00:01:00.390 --> 00:01:06.720
relates to AI and how it relates
to anticipatory interfaces

00:01:06.720 --> 00:01:09.040
that we're seeing more
and more of today.

00:01:09.040 --> 00:01:14.370
So I'll just start by rolling
the tape back a little bit.

00:01:14.370 --> 00:01:18.510
In the beginning there
was interaction design.

00:01:18.510 --> 00:01:21.000
We focus on making things
predictable, right?

00:01:21.000 --> 00:01:23.550
This has been the mode
of the last 40 years

00:01:23.550 --> 00:01:24.990
or so of interaction
design, which

00:01:24.990 --> 00:01:27.970
is like making a screen
that doesn't have to change,

00:01:27.970 --> 00:01:30.420
even if the software
can be updated, right?

00:01:30.420 --> 00:01:33.030
These things are fairly
static, predictable.

00:01:33.030 --> 00:01:35.970
They work off of
cognitive principles

00:01:35.970 --> 00:01:37.740
that relate to the
most basic ways

00:01:37.740 --> 00:01:41.670
that we actually
process information.

00:01:41.670 --> 00:01:46.980
But I think in the last four
or five years, a lot of you,

00:01:46.980 --> 00:01:48.720
if you're UX-trained,
you probably

00:01:48.720 --> 00:01:51.510
went to school in
an era in which

00:01:51.510 --> 00:01:56.220
things like modeling,
things like consistency,

00:01:56.220 --> 00:01:58.440
things like affordances
were drilled into your head.

00:01:58.440 --> 00:02:01.140
They were beaten into your mind.

00:02:01.140 --> 00:02:02.700
I personally never
thought that I'd

00:02:02.700 --> 00:02:05.199
see the day when a lot of those
things started to disappear,

00:02:05.199 --> 00:02:06.720
and yet they are, right?

00:02:06.720 --> 00:02:10.479
So right now, if you look at
a mobile game, for example,

00:02:10.479 --> 00:02:13.230
there are no affordances
on that screen.

00:02:13.230 --> 00:02:16.860
There is the assumption that you
discover what that interface is

00:02:16.860 --> 00:02:19.620
and how it works, right,
but it's not necessarily

00:02:19.620 --> 00:02:22.470
laid out for you in a way
that classical interaction

00:02:22.470 --> 00:02:24.750
design would have done so.

00:02:24.750 --> 00:02:27.990
Conversational interfaces,
another layer of complexity

00:02:27.990 --> 00:02:30.930
in which the affordances,
the basic affordances

00:02:30.930 --> 00:02:32.700
are not there anymore.

00:02:32.700 --> 00:02:35.430
And the mental model
of what that thing does

00:02:35.430 --> 00:02:37.260
is no longer stable,
right, because it's

00:02:37.260 --> 00:02:40.710
changing all the time,
because its capabilities are

00:02:40.710 --> 00:02:43.470
being augmented and
updated by the cloud,

00:02:43.470 --> 00:02:46.620
and because a lot of the
modeling itself is changing.

00:02:46.620 --> 00:02:49.740
So the point is
that affordances,

00:02:49.740 --> 00:02:52.020
where they used to
live in the interface,

00:02:52.020 --> 00:02:54.730
now increasingly
live in our head.

00:02:54.730 --> 00:02:58.840
They are the assumptions you
make about how things work.

00:02:58.840 --> 00:03:01.950
They are the expectations you
bring to what a computer should

00:03:01.950 --> 00:03:03.360
be able to do.

00:03:03.360 --> 00:03:07.810
And therefore, the surface
of design is changing now.

00:03:07.810 --> 00:03:12.070
And so I would say design now
lives in our expectations,

00:03:12.070 --> 00:03:15.480
our assumptions, our biases
about interfaces, right?

00:03:15.480 --> 00:03:18.630
And that in turn is the
new material of design.

00:03:18.630 --> 00:03:21.330
What we're talking about
is less expectations

00:03:21.330 --> 00:03:23.640
about how things perform,
let's say, physically,

00:03:23.640 --> 00:03:27.425
where you might have talked
about the animation metaphors

00:03:27.425 --> 00:03:29.509
that underlie an interface.

00:03:29.509 --> 00:03:31.800
We're talking much more about
vaguer notions about what

00:03:31.800 --> 00:03:35.280
people aspire to, what
they assume something will

00:03:35.280 --> 00:03:38.760
be able to do, and things
that are much harder to map

00:03:38.760 --> 00:03:39.940
these days.

00:03:39.940 --> 00:03:47.520
And so around that insight,
I wanted to introduce you--

00:03:47.520 --> 00:03:50.330
I'm sorry, hopefully the
resolution is showing up.

00:03:50.330 --> 00:03:52.400
But what I wanted
to introduce you to

00:03:52.400 --> 00:03:53.900
is a simple framework
for thinking

00:03:53.900 --> 00:03:58.460
about how to design for
assistive AI systems

00:03:58.460 --> 00:03:59.660
that I think--

00:03:59.660 --> 00:04:02.600
and the reason I thought
about this framework is

00:04:02.600 --> 00:04:04.700
that I think we're
mashing together

00:04:04.700 --> 00:04:07.250
a lot of different
ways AI can help us

00:04:07.250 --> 00:04:09.980
and a lot of different
ways that AI can

00:04:09.980 --> 00:04:12.340
work as an interface right now.

00:04:12.340 --> 00:04:15.650
And what this is
showing is there's

00:04:15.650 --> 00:04:19.730
this sphere of human conception,
this sphere of how humans view

00:04:19.730 --> 00:04:22.019
themselves and what
they can do, and there

00:04:22.019 --> 00:04:23.894
is a sphere of human
capability, which

00:04:23.894 --> 00:04:25.310
is the things that
they're good at

00:04:25.310 --> 00:04:27.590
and the things that
they're capable of doing.

00:04:27.590 --> 00:04:29.870
These things don't
overlap, right?

00:04:29.870 --> 00:04:34.010
As we all know, looking at users
and looking at what users do,

00:04:34.010 --> 00:04:36.320
humans often overestimate
what they can do

00:04:36.320 --> 00:04:38.812
and sometimes they
underestimate as well.

00:04:38.812 --> 00:04:41.270
Sometimes these things overlap,
but they're not necessarily

00:04:41.270 --> 00:04:42.950
perfect overlaps.

00:04:42.950 --> 00:04:45.950
So what I want to talk about
is basically each section

00:04:45.950 --> 00:04:48.020
of this Venn diagram.

00:04:48.020 --> 00:04:51.530
So let's talk about
one, problem one.

00:04:51.530 --> 00:04:53.870
I call i t the meh problem.

00:04:53.870 --> 00:04:59.920
And this relates to the leftmost
section on this Venn diagram,

00:04:59.920 --> 00:05:00.420
right?

00:05:00.420 --> 00:05:05.780
This is where human capability
exceeds human self-conception

00:05:05.780 --> 00:05:07.670
about what you do.

00:05:07.670 --> 00:05:12.280
So this is a section where
we're basically talking about--

00:05:12.280 --> 00:05:14.900
well, actually, I
think I switched this.

00:05:14.900 --> 00:05:17.030
Yes, this is human
capability exceeding

00:05:17.030 --> 00:05:18.380
human self-conception.

00:05:18.380 --> 00:05:20.940
These are the things that you
take for granted every day.

00:05:20.940 --> 00:05:24.350
These are things you don't
assume, that you don't even

00:05:24.350 --> 00:05:26.360
think about because
they're so embedded

00:05:26.360 --> 00:05:28.999
in your typical way
of doing things.

00:05:28.999 --> 00:05:30.290
And this is actually the work--

00:05:30.290 --> 00:05:32.206
I'm not going to talk a
ton about this sphere,

00:05:32.206 --> 00:05:34.100
because this is
what we familiarly

00:05:34.100 --> 00:05:37.880
know is the work of AI
and AI design right now.

00:05:37.880 --> 00:05:40.310
This is offloading the work
that we would rather not

00:05:40.310 --> 00:05:42.440
be doing for ourselves, right?

00:05:42.440 --> 00:05:46.010
And in this case, actually, some
classical interaction design

00:05:46.010 --> 00:05:47.090
principles hold.

00:05:47.090 --> 00:05:50.810
Things need to be reliable,
accountable, reversible,

00:05:50.810 --> 00:05:52.010
transparent.

00:05:52.010 --> 00:05:54.680
I think I would argue right
now that the dialogue is

00:05:54.680 --> 00:05:58.050
a little bit frozen right now
on these specific problems.

00:05:58.050 --> 00:06:00.352
But as I'll get
to in a second, I

00:06:00.352 --> 00:06:02.060
think we're moving
beyond that, and we're

00:06:02.060 --> 00:06:04.067
moving to more
complicated problems

00:06:04.067 --> 00:06:05.400
relating to those other spheres.

00:06:05.400 --> 00:06:08.280
So let's get into problem two.

00:06:08.280 --> 00:06:10.860
I call this the duh problem.

00:06:10.860 --> 00:06:14.450
This is where human
self-conception actually

00:06:14.450 --> 00:06:16.560
exceeds human capability.

00:06:16.560 --> 00:06:18.770
These are things where
you think that you're

00:06:18.770 --> 00:06:22.460
better than you are actually
are at doing something, right?

00:06:22.460 --> 00:06:24.950
And this is something
that has not

00:06:24.950 --> 00:06:28.910
been talked about a ton related
to interaction design and AI,

00:06:28.910 --> 00:06:33.950
because these are things where
a machine makes a suggestion,

00:06:33.950 --> 00:06:35.480
and you're just like, so what?

00:06:35.480 --> 00:06:36.770
Like, that was stupid.

00:06:36.770 --> 00:06:39.350
Like, I didn't need a
suggestion for that.

00:06:39.350 --> 00:06:41.900
But that actually
suggests a different mode

00:06:41.900 --> 00:06:44.540
of having to design
for that interaction.

00:06:44.540 --> 00:06:47.250
And so we're going to
get into some examples.

00:06:47.250 --> 00:06:49.460
These are obviously
augmented capabilities

00:06:49.460 --> 00:06:52.280
that we put too much stock in.

00:06:52.280 --> 00:06:53.210
Sorry for the lorem.

00:06:53.210 --> 00:06:55.770
I thought I cleared that out.

00:06:55.770 --> 00:07:00.140
So an example that I would
give you guys is IBM Watson.

00:07:00.140 --> 00:07:03.110
IBM Watson has been
a troubled product,

00:07:03.110 --> 00:07:06.870
I think, in a lot of ways,
for various different reasons.

00:07:06.870 --> 00:07:09.660
But one reason that I
actually want to talk about

00:07:09.660 --> 00:07:13.280
is actually less publicized, and
it's a super interesting one.

00:07:13.280 --> 00:07:16.370
It specifically relates to
IBM Watson Cancer, which

00:07:16.370 --> 00:07:20.960
in some ways is their,
I guess, keystone

00:07:20.960 --> 00:07:22.670
product, the one
that they've actually

00:07:22.670 --> 00:07:26.900
had the most success in creating
more robust data sets, and have

00:07:26.900 --> 00:07:29.710
productized first.

00:07:29.710 --> 00:07:31.460
So the way the product
works is basically,

00:07:31.460 --> 00:07:35.840
you enter in diagnosis data
if you're an oncologist.

00:07:35.840 --> 00:07:38.750
So it's much like a database
interface, pretty simple,

00:07:38.750 --> 00:07:43.010
pretty well-trod path
in interaction design.

00:07:43.010 --> 00:07:44.480
And then you get
treatment options.

00:07:44.480 --> 00:07:46.580
These are presented
in bands for you.

00:07:46.580 --> 00:07:49.019
I couldn't present a
screenshot here, unfortunately.

00:07:49.019 --> 00:07:51.560
They're presented in bands for
you that are basically related

00:07:51.560 --> 00:07:53.450
to confidence levels,
right, the top one

00:07:53.450 --> 00:07:57.170
is basically the
most likely course

00:07:57.170 --> 00:08:00.014
of treatment and the
most likely diagnosis,

00:08:00.014 --> 00:08:01.805
given the information
that you had entered.

00:08:05.160 --> 00:08:08.550
I went and actually
watched for a while

00:08:08.550 --> 00:08:10.890
a team of UX researchers
watching somebody

00:08:10.890 --> 00:08:14.640
use this product in the
wild and being introduced

00:08:14.640 --> 00:08:16.500
to it for the first time.

00:08:16.500 --> 00:08:18.830
And one of the most interesting
things that I heard,

00:08:18.830 --> 00:08:20.790
it was this duh problem, right?

00:08:20.790 --> 00:08:22.500
It's like, so the
doctor had gone

00:08:22.500 --> 00:08:26.610
through the problem of
entering all this data in,

00:08:26.610 --> 00:08:28.980
and it essentially had been
presented with something

00:08:28.980 --> 00:08:32.220
that they thought
was no added value.

00:08:32.220 --> 00:08:34.409
This was something that
was not that interesting

00:08:34.409 --> 00:08:37.710
because the doctor was
like, this is familiar.

00:08:37.710 --> 00:08:40.919
And so given this thing
that could actually

00:08:40.919 --> 00:08:45.060
scrape tens of thousands
of journal articles

00:08:45.060 --> 00:08:46.590
from the best
journals in the world

00:08:46.590 --> 00:08:51.300
and collapse that understanding
into three clear, concise

00:08:51.300 --> 00:08:54.450
recommendations, right, this
is an incredibly powerful thing

00:08:54.450 --> 00:08:55.500
to be doing.

00:08:55.500 --> 00:08:57.270
The doctor was like, so what?

00:08:57.270 --> 00:08:59.070
Like, so what?

00:08:59.070 --> 00:09:02.182
Even though that doctor
could never in their lifetime

00:09:02.182 --> 00:09:04.140
read all those journals,
it was presenting them

00:09:04.140 --> 00:09:08.304
more information than they could
possibly gather on their own,

00:09:08.304 --> 00:09:10.470
but the doctor, when presented
with that information

00:09:10.470 --> 00:09:15.210
said, this, you know, being
a doctor is like an art.

00:09:15.210 --> 00:09:16.620
This is just the recipe.

00:09:16.620 --> 00:09:18.470
I don't need this.

00:09:18.470 --> 00:09:21.540
So that's a super
interesting place to be,

00:09:21.540 --> 00:09:23.760
in which the doctor
actually doesn't

00:09:23.760 --> 00:09:27.320
recognize the power of the
system and the capabilities

00:09:27.320 --> 00:09:28.980
it affords.

00:09:28.980 --> 00:09:33.660
So it presented this problem,
which went unsaid in the room,

00:09:33.660 --> 00:09:35.010
in which--

00:09:35.010 --> 00:09:38.670
IBM, I could tell, was super
embarrassed that I saw,

00:09:38.670 --> 00:09:41.490
in real time, which
is, why would somebody

00:09:41.490 --> 00:09:43.990
ever use something that
was just a help meet?

00:09:43.990 --> 00:09:46.185
They wouldn't, right?

00:09:46.185 --> 00:09:46.920
[COUGHING]

00:09:46.920 --> 00:09:48.830
Excuse me.

00:09:48.830 --> 00:09:50.597
Getting over a cold.

00:09:50.597 --> 00:09:52.180
So the question is
how do you actually

00:09:52.180 --> 00:09:56.060
show that AI is adding value?

00:09:56.060 --> 00:10:01.240
So you might think you do things
like progressive disclosure,

00:10:01.240 --> 00:10:04.990
evolving interfaces,
discovered capabilities, right?

00:10:04.990 --> 00:10:08.320
pretty That would be your
pretty standard strategy, right,

00:10:08.320 --> 00:10:10.750
taking somebody along
for the ride, basically.

00:10:10.750 --> 00:10:12.970
Showing them your
homework, showing

00:10:12.970 --> 00:10:17.200
how you arrived at an answer and
involving them at the arrival

00:10:17.200 --> 00:10:19.120
to that answer, right?

00:10:19.120 --> 00:10:22.780
But I would say this
uncovers a different problem.

00:10:22.780 --> 00:10:26.140
This is a case in which
the interface as designed

00:10:26.140 --> 00:10:31.139
was focusing on the rule
rather than the exception.

00:10:31.139 --> 00:10:32.680
This is a case where
it may have been

00:10:32.680 --> 00:10:36.370
more interesting to reverse the
paradigm of what interaction

00:10:36.370 --> 00:10:39.880
design typically does and
instead focus on the exception

00:10:39.880 --> 00:10:43.630
So you can imagine an interface
that would have instead only

00:10:43.630 --> 00:10:47.040
notified you when something
was out of the ordinary,

00:10:47.040 --> 00:10:50.200
when something did not fit your
mental model of how something

00:10:50.200 --> 00:10:51.640
should work, right?

00:10:51.640 --> 00:10:55.440
And that would be a completely
different type of interface.

00:10:55.440 --> 00:10:59.170
And so you might move
from a different scenario.

00:10:59.170 --> 00:11:02.890
So instead of going from
entering diagnosis data

00:11:02.890 --> 00:11:04.810
to getting treatment
options, you

00:11:04.810 --> 00:11:08.050
might instead move from
something that was passively

00:11:08.050 --> 00:11:11.290
collecting diagnosis
data and then showing

00:11:11.290 --> 00:11:13.840
useful exceptions, right?

00:11:13.840 --> 00:11:17.140
But you may have noticed, in
switching this paradigm we now

00:11:17.140 --> 00:11:20.250
have a completely different
problem the interaction design

00:11:20.250 --> 00:11:22.660
has had up to date, right?

00:11:22.660 --> 00:11:27.610
Because it's not about entering
in information actively.

00:11:27.610 --> 00:11:30.520
It's about collecting
information passively.

00:11:30.520 --> 00:11:32.260
That's an entirely
different problem

00:11:32.260 --> 00:11:34.900
that comes with it a
sociological setting,

00:11:34.900 --> 00:11:38.260
psychological setting, and
all kinds of different issues

00:11:38.260 --> 00:11:41.680
about how you gather information
and how you gather information

00:11:41.680 --> 00:11:43.630
passively.

00:11:43.630 --> 00:11:44.770
So that the doctor--

00:11:44.770 --> 00:11:47.350
because, I guess what I'm--
backing up what I'm saying is

00:11:47.350 --> 00:11:50.530
that no doctor would ever go
through the process of sitting

00:11:50.530 --> 00:11:54.040
for 30 minutes to enter in data
on the off chance they might

00:11:54.040 --> 00:11:55.790
get an exception, right?

00:11:55.790 --> 00:11:58.979
So that actually has to
be a passive interface.

00:11:58.979 --> 00:12:00.520
And that's a completely
different way

00:12:00.520 --> 00:12:03.460
of thinking about how to
build interfaces altogether.

00:12:03.460 --> 00:12:06.165
So that's one
problem that comes up

00:12:06.165 --> 00:12:08.290
when you deal with that
different part of that Venn

00:12:08.290 --> 00:12:10.510
diagram that I showed
you, this question

00:12:10.510 --> 00:12:11.980
of how do you
capture information

00:12:11.980 --> 00:12:15.280
invisibly in a way
that actually doesn't

00:12:15.280 --> 00:12:17.570
put any burden on the user.

00:12:17.570 --> 00:12:21.720
Now moving on,
problem three, I think

00:12:21.720 --> 00:12:24.750
is probably more familiarly
known as the killer robots

00:12:24.750 --> 00:12:26.250
problem, right?

00:12:26.250 --> 00:12:29.820
So this is basically
where the human being says

00:12:29.820 --> 00:12:33.900
I can do something, and, in
fact, their capability lines

00:12:33.900 --> 00:12:36.100
up with the fact that
they can do something.

00:12:36.100 --> 00:12:38.460
And so they think, like,
I'm going to be replaced.

00:12:38.460 --> 00:12:41.590
I'm going to be threatened
by this interface.

00:12:41.590 --> 00:12:44.110
This is doing something
that I should do myself

00:12:44.110 --> 00:12:46.110
but it's not letting me do.

00:12:46.110 --> 00:12:49.120
And this is a different
problem entirely as well.

00:12:49.120 --> 00:12:51.300
So this relates
to helping us when

00:12:51.300 --> 00:12:53.250
we don't know that we
need it, right, this

00:12:53.250 --> 00:12:56.150
relates to interfaces that
are trying to help you

00:12:56.150 --> 00:12:59.040
when you think that
you've solved a problem

00:12:59.040 --> 00:13:01.170
but you may not have.

00:13:01.170 --> 00:13:04.560
And so I want to give you an
example of an interface that

00:13:04.560 --> 00:13:06.700
was developed and
still being developed

00:13:06.700 --> 00:13:09.960
at Carnegie Mellon
by a group there

00:13:09.960 --> 00:13:13.740
that's actually looked at how
do you develop a interface that

00:13:13.740 --> 00:13:17.100
would help doctors make
decisions about when

00:13:17.100 --> 00:13:20.460
to insert an artificial heart.

00:13:20.460 --> 00:13:25.470
And what's interesting, when
you talk about decision systems

00:13:25.470 --> 00:13:29.220
for doctors is that we see
all this kind of promise,

00:13:29.220 --> 00:13:33.150
we see all this kind of hype
being told about what AI can do

00:13:33.150 --> 00:13:37.350
and the concept in helping
doctors make decisions.

00:13:37.350 --> 00:13:39.180
The thing that
belies this there's

00:13:39.180 --> 00:13:41.850
probably about 30 years of
academic research showing

00:13:41.850 --> 00:13:45.180
that every single
decision-assistive system

00:13:45.180 --> 00:13:47.100
for doctors has failed.

00:13:47.100 --> 00:13:49.140
They fail all the time.

00:13:49.140 --> 00:13:52.990
We do not know how to build
these interfaces at all.

00:13:52.990 --> 00:13:55.260
We still don't know
how do you get a doctor

00:13:55.260 --> 00:14:00.090
to accept a recommendation
or assistance from a machine.

00:14:00.090 --> 00:14:02.110
And part of that
is physical, right?

00:14:02.110 --> 00:14:06.990
It's very hard to actually have
log-in systems in hospitals

00:14:06.990 --> 00:14:10.800
in which not just one person
accesses a piece of data.

00:14:10.800 --> 00:14:14.390
That's an entirely
hard UX problem.

00:14:14.390 --> 00:14:16.590
There's physical problems
relating to doctors

00:14:16.590 --> 00:14:18.870
just can't use touch screens.

00:14:18.870 --> 00:14:21.510
But putting those aside,
there's actually something

00:14:21.510 --> 00:14:23.400
deeper at work here.

00:14:23.400 --> 00:14:25.500
When that team at
Carnegie Mellon

00:14:25.500 --> 00:14:28.880
actually looked
at heart surgeons

00:14:28.880 --> 00:14:33.380
and why they might not
use such interfaces,

00:14:33.380 --> 00:14:36.350
the things that we uncovered
was this had to do with ego,

00:14:36.350 --> 00:14:38.330
it had to do with
hierarchy, and it

00:14:38.330 --> 00:14:39.920
had to do with self-perception.

00:14:39.920 --> 00:14:44.830
So doctors by their nature
thought I am the exception.

00:14:44.830 --> 00:14:46.460
Nobody is going to
tell me because I'm

00:14:46.460 --> 00:14:50.360
a high performer that does not
need to be told the basics.

00:14:50.360 --> 00:14:54.200
So automatically a huge hill to
climb if you are an interaction

00:14:54.200 --> 00:14:55.340
designer.

00:14:55.340 --> 00:14:59.010
The second thing was hierarchy.

00:14:59.010 --> 00:15:03.150
Doctors often, they work
in very regimented places,

00:15:03.150 --> 00:15:07.650
where if you're a nurse, if you
are a physician's assistant,

00:15:07.650 --> 00:15:09.340
to say, hey, doctor,
I think you missed

00:15:09.340 --> 00:15:11.610
something, that is insane.

00:15:11.610 --> 00:15:13.890
You could get fired for
that very easily, at least

00:15:13.890 --> 00:15:15.360
in the US.

00:15:15.360 --> 00:15:18.210
The way that the
hierarchy works,

00:15:18.210 --> 00:15:21.750
it just doesn't allow
for that sort of thing.

00:15:21.750 --> 00:15:24.030
So an interesting thing
that they discovered,

00:15:24.030 --> 00:15:33.040
though, is there's this quirk in
the way doctors present highly

00:15:33.040 --> 00:15:35.620
difficult cases, which
is a conference board.

00:15:35.620 --> 00:15:37.720
So doctors get together
and they basically

00:15:37.720 --> 00:15:41.980
go over their toughest cases
and they ask other doctors

00:15:41.980 --> 00:15:44.110
for advice about how to do that.

00:15:44.110 --> 00:15:49.280
And what they realized
is the person that

00:15:49.280 --> 00:15:52.550
is the most overlooked there,
that has the most power there,

00:15:52.550 --> 00:15:56.090
is the person that puts together
the slides for the doctors.

00:15:56.090 --> 00:15:58.540
It's the person who
nobody appreciates

00:15:58.540 --> 00:16:00.890
that has to get all this
data and put it on a slide

00:16:00.890 --> 00:16:03.350
that doctors then
all have to look at.

00:16:03.350 --> 00:16:05.600
They're all looking at
the same information.

00:16:05.600 --> 00:16:10.250
And what they realized
is we can slip in some AI

00:16:10.250 --> 00:16:13.310
into a PowerPoint program.

00:16:13.310 --> 00:16:16.670
You might have thought that like
there was some great solution

00:16:16.670 --> 00:16:20.150
having to do with some
great interface in the OR

00:16:20.150 --> 00:16:23.180
about making decisions
about whether or not

00:16:23.180 --> 00:16:26.810
you should implant a heart, but
instead they went to PowerPoint

00:16:26.810 --> 00:16:29.900
and they said, like, we
can slip in this completely

00:16:29.900 --> 00:16:32.720
non-threatening graph,
that some nurse can just

00:16:32.720 --> 00:16:37.640
say like, oh, this is just
what the machine spat out.

00:16:37.640 --> 00:16:39.350
I'm not taking you on.

00:16:39.350 --> 00:16:41.690
I'm not trying to
question your expertise.

00:16:41.690 --> 00:16:43.700
This is just what it did.

00:16:43.700 --> 00:16:48.137
So as this team is
actually going forward,

00:16:48.137 --> 00:16:49.970
the interesting thing
is they're discovering

00:16:49.970 --> 00:16:52.520
different strategies.

00:16:52.520 --> 00:16:54.800
Their next phase
of work is actually

00:16:54.800 --> 00:16:57.920
having to do with trying
to almost trick doctors

00:16:57.920 --> 00:17:00.620
into thinking that the
recommendations of the machine

00:17:00.620 --> 00:17:02.220
are their own.

00:17:02.220 --> 00:17:04.130
So in this case,
you would actually

00:17:04.130 --> 00:17:10.849
have a Potemkin AI model in
which the doctor supposedly

00:17:10.849 --> 00:17:15.109
trains the AI about what
the right recommendation is.

00:17:15.109 --> 00:17:17.480
But it's not really training it.

00:17:17.480 --> 00:17:20.240
I mean, a doctor is going
to give it a couple hundred

00:17:20.240 --> 00:17:22.339
data points,
whereas this machine

00:17:22.339 --> 00:17:26.599
can take in 35,000 from
incredibly difficult cases.

00:17:26.599 --> 00:17:28.430
So it's really like--

00:17:28.430 --> 00:17:32.240
what you're actually talking
about is the Ikea effect.

00:17:32.240 --> 00:17:34.070
If you don't know,
this guy before

00:17:34.070 --> 00:17:37.072
was Richard Thaler, the
famous behavioral economist.

00:17:40.130 --> 00:17:41.930
This is not-- the research--

00:17:41.930 --> 00:17:44.092
I'm adding a little bit
of interpretation here.

00:17:44.092 --> 00:17:45.800
But what the researchers
are discovering,

00:17:45.800 --> 00:17:47.030
this is the Ikea effect.

00:17:47.030 --> 00:17:48.620
For those of you
who don't know, this

00:17:48.620 --> 00:17:50.840
is a study done by
behavioral economists that

00:17:50.840 --> 00:17:55.850
show that human beings, when
asked to co-craft something,

00:17:55.850 --> 00:17:57.320
value it more.

00:17:57.320 --> 00:17:59.810
So this study showed
that, basically,

00:17:59.810 --> 00:18:02.870
after assembling an
Ikea chair, people

00:18:02.870 --> 00:18:05.990
valued it economically
higher than from before,

00:18:05.990 --> 00:18:09.200
or another chair that
somebody else had made.

00:18:09.200 --> 00:18:12.290
So these are instances
where again, going back

00:18:12.290 --> 00:18:15.380
to the introduction
to my talk, these

00:18:15.380 --> 00:18:22.720
are situations where the
biases, the psychological quirks

00:18:22.720 --> 00:18:26.620
of our minds, are actually the
fodder for design and design

00:18:26.620 --> 00:18:28.000
thinking.

00:18:28.000 --> 00:18:31.270
And so in this case, you
might think co-creation,

00:18:31.270 --> 00:18:33.160
understanding
context, deference,

00:18:33.160 --> 00:18:36.340
example-based learning, these
are just some of the quirks

00:18:36.340 --> 00:18:38.930
about psychology,
about how we learn.

00:18:38.930 --> 00:18:42.160
But if you might
have noticed, these

00:18:42.160 --> 00:18:45.396
are the ways humans have
always used to build trust.

00:18:45.396 --> 00:18:47.020
These are the ways
that you've always--

00:18:47.020 --> 00:18:49.780
if you think about how
you get to know somebody

00:18:49.780 --> 00:18:53.870
and how you actually create
a relationship with them,

00:18:53.870 --> 00:18:55.360
you do something together.

00:18:55.360 --> 00:18:57.520
You have this trade-off
of information.

00:18:57.520 --> 00:19:01.481
You have the shared space
of understanding, right?

00:19:01.481 --> 00:19:03.480
And the point that I want
to leave you guys with

00:19:03.480 --> 00:19:04.938
is that we'll get
to the next phase

00:19:04.938 --> 00:19:07.590
by watching people in new ways.

00:19:07.590 --> 00:19:11.460
Too often I think UX research
is focused on end users

00:19:11.460 --> 00:19:14.210
actually using a baked
product, whereas where

00:19:14.210 --> 00:19:17.550
I think we're going is
finding new ways to look

00:19:17.550 --> 00:19:20.820
at analogies that
we can learn from,

00:19:20.820 --> 00:19:25.000
instructive ways in which the
ways people talk to each other,

00:19:25.000 --> 00:19:27.750
the way they structure
their social interaction

00:19:27.750 --> 00:19:31.570
can then change the
way we design products.

00:19:31.570 --> 00:19:35.100
And interestingly enough,
Microsoft got to the same point

00:19:35.100 --> 00:19:36.600
with their own AI
principles, which

00:19:36.600 --> 00:19:39.390
closely mirror the principles
that I laid out before.

00:19:39.390 --> 00:19:44.730
But the way they got there was
not by some interaction design

00:19:44.730 --> 00:19:50.460
framework they dug
out from some paper,

00:19:50.460 --> 00:19:53.250
like presented at
[INAUDIBLE] or something.

00:19:53.250 --> 00:19:56.970
The way they got there was
watching actual real assistants

00:19:56.970 --> 00:19:59.430
and how they developed
a relationship

00:19:59.430 --> 00:20:01.350
with their clients.

00:20:01.350 --> 00:20:03.900
Real personal assistants,
how they developed

00:20:03.900 --> 00:20:06.120
trust over time
with their clients.

00:20:06.120 --> 00:20:08.430
And what they did is try
to abstract the principles

00:20:08.430 --> 00:20:13.140
for how personal assistants
with low capability

00:20:13.140 --> 00:20:17.820
up front try to garner
trust over time.

00:20:17.820 --> 00:20:21.300
So the point is, we're
no longer watching

00:20:21.300 --> 00:20:23.970
the basic of
information processes,

00:20:23.970 --> 00:20:27.630
and how human minds
process information and all

00:20:27.630 --> 00:20:29.610
those things that got
remapped to things

00:20:29.610 --> 00:20:33.330
like models and the
consistency in interfaces.

00:20:33.330 --> 00:20:36.290
Rather, the structure
of our relationships

00:20:36.290 --> 00:20:38.350
is now the fodder of design.

00:20:38.350 --> 00:20:40.360
And this is what we need
to be understanding,

00:20:40.360 --> 00:20:42.120
and this is what we
need to be watching,

00:20:42.120 --> 00:20:44.675
is interaction designers,
is UX designers,

00:20:44.675 --> 00:20:49.440
is engineers thinking about
how do you trust a system.

00:20:49.440 --> 00:20:50.650
And that's it.

00:20:50.650 --> 00:20:51.150
Thank you.

00:20:51.150 --> 00:20:58.410
[APPLAUSE]

00:20:58.410 --> 00:21:02.060
SPEAKER 2: We've got time for a
couple of questions for Cliff.

00:21:02.060 --> 00:21:02.740
One over there.

00:21:09.388 --> 00:21:10.631
AUDIENCE: Hi.

00:21:10.631 --> 00:21:13.650
I'm a PhD student that,
in a previous life,

00:21:13.650 --> 00:21:15.289
I worked at a cancer hospital.

00:21:15.289 --> 00:21:16.830
And I was just
wondering, when you're

00:21:16.830 --> 00:21:19.770
talking about decision
assistance systems failing,

00:21:19.770 --> 00:21:23.370
are you considering external
ones or like in-house ones?

00:21:23.370 --> 00:21:25.536
Because we had quite
a successful one.

00:21:25.536 --> 00:21:27.660
And I think part-- the
problem isn't just that they

00:21:27.660 --> 00:21:29.490
don't trust it,
it's that it doesn't

00:21:29.490 --> 00:21:30.822
do what they need it to do.

00:21:30.822 --> 00:21:33.030
CLIFF KUANG: Yeah I think
that, actually, the greater

00:21:33.030 --> 00:21:35.130
problem in a lot of the
peer-reviewed literature

00:21:35.130 --> 00:21:38.520
has to do more with the fact
that a lot of times when

00:21:38.520 --> 00:21:40.230
somebody comes in
outside to build

00:21:40.230 --> 00:21:43.380
a system for other people,
they don't have UX people

00:21:43.380 --> 00:21:44.070
at the fore.

00:21:44.070 --> 00:21:47.940
They don't have any sort of
user research at the fore.

00:21:47.940 --> 00:21:51.457
Typically, I mean, the sad
thing is with medical systems is

00:21:51.457 --> 00:21:53.400
that it's a little bit
of a backwater in terms

00:21:53.400 --> 00:21:56.184
of current thinking about
how to build software, right?

00:21:56.184 --> 00:21:57.600
And that's really
the point there,

00:21:57.600 --> 00:21:59.820
is that very rarely
do you assemble

00:21:59.820 --> 00:22:03.290
the right people to identify
what the real problem is

00:22:03.290 --> 00:22:05.430
and what the actual
context for use is.

00:22:05.430 --> 00:22:08.940
So things that we take it for
granted in software development

00:22:08.940 --> 00:22:11.430
today actually don't happen in
a lot of parts of the world.

00:22:18.929 --> 00:22:19.470
AUDIENCE: Hi.

00:22:19.470 --> 00:22:24.650
So if we research people
and their relationships,

00:22:24.650 --> 00:22:28.160
are we limiting ourselves
to just what we know today

00:22:28.160 --> 00:22:29.810
in human forms?

00:22:29.810 --> 00:22:33.070
So can we-- are there--

00:22:33.070 --> 00:22:36.250
I mean, it's a bit rhetorical,
but are there things to learn

00:22:36.250 --> 00:22:41.860
from human-robot relationships,
because we're going to find new

00:22:41.860 --> 00:22:42.520
ways--

00:22:42.520 --> 00:22:44.890
CLIFF KUANG: I would say as
fast as things are speeding

00:22:44.890 --> 00:22:48.550
up right now, we
have a couple hundred

00:22:48.550 --> 00:22:52.000
million years of evolution to
catch up on that has actually

00:22:52.000 --> 00:22:57.520
done a very good job and a very
difficult job of figuring out

00:22:57.520 --> 00:23:01.360
how our brains, which run
at like, the wattage less

00:23:01.360 --> 00:23:03.980
than a light bulb, can
actually take in all

00:23:03.980 --> 00:23:08.011
of these complicated pieces
of information and signals

00:23:08.011 --> 00:23:10.510
and actually structure them in
a way that doesn't offend you

00:23:10.510 --> 00:23:13.240
and it doesn't offend me
and allows us to create

00:23:13.240 --> 00:23:15.760
some common space where
we agree that we're

00:23:15.760 --> 00:23:18.309
having a conversation,
we're following each other.

00:23:18.309 --> 00:23:19.850
These are incredibly
difficult things

00:23:19.850 --> 00:23:22.270
that I would argue we are so--

00:23:22.270 --> 00:23:24.280
at least as
interaction designers,

00:23:24.280 --> 00:23:26.740
at least as UX
designers-- we're well

00:23:26.740 --> 00:23:29.474
behind the curve of
understanding what that is

00:23:29.474 --> 00:23:30.265
and how that works.

00:23:37.790 --> 00:23:38.830
AUDIENCE: Great talk.

00:23:38.830 --> 00:23:41.330
I did UX design
for about 15 years,

00:23:41.330 --> 00:23:44.150
and when you started
your talk, talking

00:23:44.150 --> 00:23:47.690
about conversational
agents, bots,

00:23:47.690 --> 00:23:49.850
my first reaction regularly
when I talked to people

00:23:49.850 --> 00:23:53.515
who try to design those and user
tested them, most of them suck.

00:23:53.515 --> 00:23:54.890
I mean, they're
really bad, yeah.

00:23:54.890 --> 00:23:57.530
So I kind of feel
like part of our job

00:23:57.530 --> 00:24:01.490
as UX designers might be to
say you're on a hype bandwagon

00:24:01.490 --> 00:24:03.392
with trying to build a
bot for your product.

00:24:03.392 --> 00:24:04.850
And then we have
to kind of educate

00:24:04.850 --> 00:24:06.890
the company, the
product managers,

00:24:06.890 --> 00:24:09.710
the technologist about how
hard it is to get that right

00:24:09.710 --> 00:24:13.520
and that it might be a cheaper,
easier, better UX experience

00:24:13.520 --> 00:24:14.730
to not go that way.

00:24:14.730 --> 00:24:17.570
CLIFF KUANG: And that's why I
think that a lot of the bot--

00:24:17.570 --> 00:24:19.370
that stuff has
gone away and it's

00:24:19.370 --> 00:24:21.590
been replaced with these
hybrid interfaces that

00:24:21.590 --> 00:24:24.350
are a little bit of UI
inside of a bot, right?

00:24:24.350 --> 00:24:26.950
I also work as a UX and
a product consultant.

00:24:26.950 --> 00:24:30.020
I was just on a project
where the team was

00:24:30.020 --> 00:24:33.080
hellbent on suggesting a chat
bot as a universal interface,

00:24:33.080 --> 00:24:34.760
and I just wanted to throw up.

00:24:34.760 --> 00:24:38.150
Like I just could not
get my head around it,

00:24:38.150 --> 00:24:40.260
and I was just like, this
is never going to work.

00:24:40.260 --> 00:24:42.562
But the thing is,
people just have

00:24:42.562 --> 00:24:44.270
to go through this
learning curve, right?

00:24:44.270 --> 00:24:47.480
We're going through-- and it's
a real disservice right now,

00:24:47.480 --> 00:24:50.660
that I think a lot of the
tech companies are actually

00:24:50.660 --> 00:24:51.210
pushing--

00:24:51.210 --> 00:24:52.710
they're actually
part of the problem

00:24:52.710 --> 00:24:55.430
in thinking that these
solutions-- and here, you

00:24:55.430 --> 00:24:57.050
know, I would not
necessarily exempt,

00:24:57.050 --> 00:24:59.012
like I know that I'm
here on Google's dime,

00:24:59.012 --> 00:25:01.220
but I wouldn't exempt Google
from that either, right?

00:25:04.682 --> 00:25:06.140
We're still struggling
with how you

00:25:06.140 --> 00:25:09.650
create a mental model for what
something does when you have

00:25:09.650 --> 00:25:12.980
no interface that's graphical.

00:25:12.980 --> 00:25:14.300
And I don't know--

00:25:14.300 --> 00:25:16.610
I don't think Google's
not alone in that.

00:25:16.610 --> 00:25:18.770
I don't know what the
solution to that is.

00:25:18.770 --> 00:25:20.270
We're still figuring it out.

00:25:20.270 --> 00:25:22.970
It's going to take like
two decades of negotiation

00:25:22.970 --> 00:25:26.990
about what machines do in
order for us to get that right.

00:25:26.990 --> 00:25:27.560
AUDIENCE: Hi.

00:25:27.560 --> 00:25:28.461
Oh, can you hear me?

00:25:28.461 --> 00:25:28.960
Yeah.

00:25:28.960 --> 00:25:30.820
Hi my name's Ilana.

00:25:30.820 --> 00:25:34.230
I'm from the
DeepMind house team.

00:25:34.230 --> 00:25:36.730
I was curious, do you
know what the team

00:25:36.730 --> 00:25:38.560
motto was for the Watson team?

00:25:38.560 --> 00:25:40.051
Did they involve UXR--

00:25:40.051 --> 00:25:41.050
CLIFF KUANG: Absolutely.

00:25:41.050 --> 00:25:42.220
AUDIENCE: --later on?

00:25:42.220 --> 00:25:43.070
Just to test?

00:25:43.070 --> 00:25:45.670
CLIFF KUANG: No, they involved
it upfront, very early.

00:25:45.670 --> 00:25:47.720
But the point I
wanted to bring out

00:25:47.720 --> 00:25:51.090
with that is there are blind
spots in the way we assume,

00:25:51.090 --> 00:25:51.874
like--

00:25:51.874 --> 00:25:54.040
there are just like-- there's
almost like, you know,

00:25:54.040 --> 00:25:56.020
when the CIA actually
does analysis,

00:25:56.020 --> 00:25:59.950
they actually have two teams
working in separate walls,

00:25:59.950 --> 00:26:01.990
going at a problem,
hoping that they will

00:26:01.990 --> 00:26:03.460
come to different solutions.

00:26:03.460 --> 00:26:07.120
And the idea is that if they
converge, you've got something,

00:26:07.120 --> 00:26:07.840
right?

00:26:07.840 --> 00:26:09.980
That's how the
CIA does analysis.

00:26:09.980 --> 00:26:12.400
I don't think right now,
UX doesn't do that, right?

00:26:12.400 --> 00:26:16.060
You don't have adversarial
UX teams basically saying,

00:26:16.060 --> 00:26:18.949
like, I'm going to fight you
about what the solution is

00:26:18.949 --> 00:26:19.615
to this problem.

00:26:19.615 --> 00:26:21.406
SPEAKER 2: You haven't
worked with us, so--

00:26:21.406 --> 00:26:22.787
[LAUGHTER]

00:26:22.787 --> 00:26:24.370
CLIFF KUANG: Fair
enough, fair enough.

00:26:24.370 --> 00:26:27.340
But my point is,
in a lot of ways,

00:26:27.340 --> 00:26:29.440
we don't know the assumptions
that we're making.

00:26:29.440 --> 00:26:30.814
They assumed that
they were going

00:26:30.814 --> 00:26:33.140
to build a pretty simple
database interface, right?

00:26:33.140 --> 00:26:37.210
That's a perfectly
fine assumption, right?

00:26:37.210 --> 00:26:39.130
That's a perfectly fine thing.

00:26:39.130 --> 00:26:42.810
And you would have assumed
from user data, as a person,

00:26:42.810 --> 00:26:45.347
like creating a monetizable
piece of software,

00:26:45.347 --> 00:26:47.430
you would have thought the
more I can get somebody

00:26:47.430 --> 00:26:50.710
to use this software,
the more valuable it is.

00:26:50.710 --> 00:26:53.334
So those types of
assumptions, sometimes they

00:26:53.334 --> 00:26:54.250
have to be thrown out.

00:26:54.250 --> 00:26:58.270
And maybe it actually takes you
going all the way down the path

00:26:58.270 --> 00:27:00.080
to actually realizing
that it didn't work,

00:27:00.080 --> 00:27:02.020
but you have to be able
to throw something out

00:27:02.020 --> 00:27:05.970
that you actually
committed to upfront.

00:27:05.970 --> 00:27:09.635
SPEAKER 2: Got
time for one more.

00:27:09.635 --> 00:27:11.010
AUDIENCE: Yeah I
have a question.

00:27:11.010 --> 00:27:13.530
What would you think
about a Hippocratic oath

00:27:13.530 --> 00:27:14.800
for developers?

00:27:14.800 --> 00:27:17.260
Part of the reason that
people trust their doctors

00:27:17.260 --> 00:27:18.940
is because they
really feel like they

00:27:18.940 --> 00:27:20.930
have the best in mind for them.

00:27:20.930 --> 00:27:24.430
I would say that that sort of
relationship with technologists

00:27:24.430 --> 00:27:26.290
is becoming increasingly broken.

00:27:26.290 --> 00:27:27.760
So would such an oath help?

00:27:27.760 --> 00:27:30.190
CLIFF KUANG: I would agree,
and speaking in Europe,

00:27:30.190 --> 00:27:32.680
obviously the amount of
skepticism about technology

00:27:32.680 --> 00:27:36.070
is, actually, you guys have been
much more aggressive about it

00:27:36.070 --> 00:27:38.950
with things like the GDPR.

00:27:38.950 --> 00:27:41.560
I would agree that there needs
to be a Hippocratic oath.

00:27:41.560 --> 00:27:43.390
What I would say
as a provocation is

00:27:43.390 --> 00:27:47.140
that what companies have been
relying on right now heretofore

00:27:47.140 --> 00:27:49.870
is marketing and
public sentiment

00:27:49.870 --> 00:27:52.900
to take the place of
a Hippocratic oath.

00:27:52.900 --> 00:27:55.060
So how you actually
get a Hippocratic oath

00:27:55.060 --> 00:28:00.070
to seem meaningful in the
wake of the way public tech

00:28:00.070 --> 00:28:02.920
companies have been
handling their--

00:28:02.920 --> 00:28:06.220
getting basically public
buy-in is a hard problem.

00:28:06.220 --> 00:28:09.820
I have no idea how somebody
would trust a Hippocratic oath

00:28:09.820 --> 00:28:14.710
coming from the same bodies
that are governing themselves,

00:28:14.710 --> 00:28:15.282
right?

00:28:15.282 --> 00:28:16.490
It's going to be really hard.

00:28:16.490 --> 00:28:18.940
I don't know the mechanism
that you create for that.

00:28:18.940 --> 00:28:20.330
I don't think it's a bad idea.

00:28:20.330 --> 00:28:23.170
I just think that
there is context there

00:28:23.170 --> 00:28:26.170
and there is a way that we got
to here that you can't just,

00:28:26.170 --> 00:28:28.420
like, say that we're going
to do this Hippocratic oath

00:28:28.420 --> 00:28:30.760
and then it's just
going to change things.

00:28:30.760 --> 00:28:34.345
And I haven't seen anybody
make that case yet.

00:28:34.345 --> 00:28:35.970
SPEAKER 2: While
we're just do a check,

00:28:35.970 --> 00:28:38.300
I'm going to [INAUDIBLE]
the privilege

00:28:38.300 --> 00:28:39.750
a position of
having a microphone

00:28:39.750 --> 00:28:42.780
and ask a question myself.

00:28:42.780 --> 00:28:44.170
I Could you speak a little bit--

00:28:44.170 --> 00:28:45.360
I think one of the
things that's implied

00:28:45.360 --> 00:28:46.970
in some ways in
what you're saying

00:28:46.970 --> 00:28:49.540
and the answer to the
last but one question

00:28:49.540 --> 00:28:53.580
about often the
metrics that are used

00:28:53.580 --> 00:28:58.260
by people who make decisions,
designers or otherwise, maybe

00:28:58.260 --> 00:28:59.734
force some of the
design decisions.

00:28:59.734 --> 00:29:01.150
Could you speak a
little bit-- you

00:29:01.150 --> 00:29:03.025
talked about the sort
of change in perception

00:29:03.025 --> 00:29:05.430
around design frameworks.

00:29:05.430 --> 00:29:07.620
Have you thought at
all about metrics?

00:29:07.620 --> 00:29:10.600
Daily active users,
monthly active users

00:29:10.600 --> 00:29:15.390
is sort of the mantra,
particularly in the Valley

00:29:15.390 --> 00:29:16.580
and amongst investors.

00:29:16.580 --> 00:29:18.830
How do you think that
should change in this--

00:29:18.830 --> 00:29:22.410
CLIFF KUANG: So I had this
evolving understanding.

00:29:22.410 --> 00:29:25.170
I think that this is kind
of implied in the talk

00:29:25.170 --> 00:29:27.547
that I gave, but I think
that we're moving up--

00:29:27.547 --> 00:29:29.130
if you think about
Maslow's hierarchy,

00:29:29.130 --> 00:29:32.270
where we're fulfilling
basic needs and where--

00:29:32.270 --> 00:29:35.670
you think about interaction
design in its early phases,

00:29:35.670 --> 00:29:39.840
having fulfilled basic
ideas and basic needs,

00:29:39.840 --> 00:29:43.500
those things were relatively
easy to measure, right?

00:29:43.500 --> 00:29:45.450
Were you able to
do what you want?

00:29:45.450 --> 00:29:47.610
As UX designers,
we figured out how

00:29:47.610 --> 00:29:49.330
to measure that pretty well.

00:29:49.330 --> 00:29:51.210
But I think that
interfaces are now

00:29:51.210 --> 00:29:53.760
moving higher up
Maslow's hierarchy

00:29:53.760 --> 00:29:55.980
in the sense that they're
now faced with problems

00:29:55.980 --> 00:29:58.230
like how do you trust something?

00:29:58.230 --> 00:29:59.730
Did I find this satisfying?

00:29:59.730 --> 00:30:01.620
Did this make me happier?

00:30:01.620 --> 00:30:04.320
Those are things that we
don't have metrics yet.

00:30:04.320 --> 00:30:07.260
But I would say to
you that right now, we

00:30:07.260 --> 00:30:09.210
don't have a way to
actually-- for people

00:30:09.210 --> 00:30:11.870
to actually express that
information in the interfaces

00:30:11.870 --> 00:30:13.290
that they deal with.

00:30:13.290 --> 00:30:15.210
An example that I would
give you, for example,

00:30:15.210 --> 00:30:17.940
is that when you walk into
a travel agent, a really

00:30:17.940 --> 00:30:21.690
good travel agent, or almost
any expensive high-end service,

00:30:21.690 --> 00:30:24.840
you can specify what kind of
experience you want to have.

00:30:24.840 --> 00:30:28.649
You can say to a
travel agent, like, I

00:30:28.649 --> 00:30:29.940
want a little bit of adventure.

00:30:29.940 --> 00:30:33.090
Not too much, mostly I want to
hang out on the beach, right?

00:30:33.090 --> 00:30:36.240
You can give that
kind of like heuristic

00:30:36.240 --> 00:30:38.730
about what kind of
experience you want to have.

00:30:38.730 --> 00:30:42.090
Right now as a user
of computer software,

00:30:42.090 --> 00:30:43.590
you cannot do that at all.

00:30:43.590 --> 00:30:46.350
I can't tell my phone, hey,
phone, I want to chill out

00:30:46.350 --> 00:30:48.000
today, right?

00:30:48.000 --> 00:30:50.464
Hey, phone, keep me
super plugged in,

00:30:50.464 --> 00:30:52.380
because I've got a lot
of really intense stuff

00:30:52.380 --> 00:30:55.050
and I want to make sure that
I don't miss anything, right?

00:30:55.050 --> 00:30:57.540
We're not designing for any
of that opt-in behavior that

00:30:57.540 --> 00:31:00.150
would give us that data
to be able to measure

00:31:00.150 --> 00:31:01.654
any of those things.

00:31:01.654 --> 00:31:03.570
And so what I want to
say is there is probably

00:31:03.570 --> 00:31:07.620
a future for an interface that
is about expressing high level

00:31:07.620 --> 00:31:13.440
human desire as almost the
guidelines that a machine

00:31:13.440 --> 00:31:17.490
learning algorithm can fill in
by providing some sort of usage

00:31:17.490 --> 00:31:20.640
pattern that fits with
the high level heuristics

00:31:20.640 --> 00:31:22.560
that you've offered as a user.

00:31:22.560 --> 00:31:24.030
And until we get
to that point, I'm

00:31:24.030 --> 00:31:27.200
just not sure how we're going
to figure that stuff out,

00:31:27.200 --> 00:31:28.605
but I hope we do.

00:31:28.605 --> 00:31:29.480
SPEAKER 2: Thank you.

00:31:29.480 --> 00:31:31.050
Thank you, Cliff.

00:31:31.050 --> 00:31:34.410
[APPLAUSE]

00:31:36.192 --> 00:31:37.900
SPEAKER 1: So I'm
going to welcome Sarah.

00:31:37.900 --> 00:31:41.400
Sarah is-- Sarah Gold
is an expert in--

