WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.450
[MUSIC PLAYING]

00:00:06.422 --> 00:00:07.880
LAURENCE MORONEY:
Running inference

00:00:07.880 --> 00:00:10.400
on compute-heavy
machine learning models

00:00:10.400 --> 00:00:14.150
such as face and contour
detection on mobile devices

00:00:14.150 --> 00:00:17.840
can be very demanding due to
the device's limited processing

00:00:17.840 --> 00:00:21.450
power as well as considerations
such as battery life.

00:00:21.450 --> 00:00:23.630
While converting to
a fixed point model

00:00:23.630 --> 00:00:26.000
is one way to
achieve acceleration,

00:00:26.000 --> 00:00:29.660
we've also had lots of
feedback about GPU support.

00:00:29.660 --> 00:00:31.460
So now, we're
excited to announce

00:00:31.460 --> 00:00:36.230
a developer preview of a GPU
back end for TensorFlow Lite.

00:00:36.230 --> 00:00:38.960
This back end
leverages OpenGL ES

00:00:38.960 --> 00:00:43.160
3.1 compute shaders on
Android, and Metal compute

00:00:43.160 --> 00:00:45.560
shaders on iOS.

00:00:45.560 --> 00:00:48.590
This release is a
pre-compiled binary preview

00:00:48.590 --> 00:00:52.010
of the new GPU back end,
allowing you an early chance

00:00:52.010 --> 00:00:53.360
to play with it.

00:00:53.360 --> 00:00:57.100
We're working on a full open
source release later in 2019,

00:00:57.100 --> 00:00:59.990
and would love to hear your
feedback as we shape it.

00:00:59.990 --> 00:01:01.940
Let's take a look
at its performance.

00:01:01.940 --> 00:01:04.220
We've experimented
with a GPU back end

00:01:04.220 --> 00:01:08.330
on a number of models, including
four public ones and two

00:01:08.330 --> 00:01:11.450
internal models that we
use in Google products.

00:01:11.450 --> 00:01:15.140
The results showed that the GPU
back end worked two to seven

00:01:15.140 --> 00:01:20.310
times faster than a floating
point CPU implementation.

00:01:20.310 --> 00:01:24.080
You can see the GPU inference
time in orange and the CPU

00:01:24.080 --> 00:01:26.030
inference time in gray.

00:01:26.030 --> 00:01:29.780
We tested this on an iPhone
7, a Pixel 2, and a Pixel 3,

00:01:29.780 --> 00:01:33.410
amongst others, and this was
the average inference time

00:01:33.410 --> 00:01:36.740
across 100 tests on each
of these devices for each

00:01:36.740 --> 00:01:38.270
of these models.

00:01:38.270 --> 00:01:41.420
The speed up was most
significant on more complex

00:01:41.420 --> 00:01:43.760
models, the kind
that lead themselves

00:01:43.760 --> 00:01:46.160
better to GPU utilization.

00:01:46.160 --> 00:01:47.950
On smaller and
simpler models, you

00:01:47.950 --> 00:01:50.730
may not see such a
benefit because of,

00:01:50.730 --> 00:01:53.480
for example, the time
cost of transferring data

00:01:53.480 --> 00:01:55.290
into GPU memory.

00:01:55.290 --> 00:01:59.460
So I'm sure your next question
is, how do I get started?

00:01:59.460 --> 00:02:01.790
Well, the easiest way
is to try our demo

00:02:01.790 --> 00:02:04.370
apps that use the GPU delegate.

00:02:04.370 --> 00:02:07.220
We have written a tutorial
to walk you through this,

00:02:07.220 --> 00:02:10.389
and I'll put a link to it in
the description for this video.

00:02:10.389 --> 00:02:12.180
There's also a couple
of really nice screen

00:02:12.180 --> 00:02:14.900
casts showing you how to
get up and running quickly.

00:02:14.900 --> 00:02:16.910
There's one for Android
and one for iOS,

00:02:16.910 --> 00:02:18.530
and I've linked to them too.

00:02:18.530 --> 00:02:21.080
For Android, we've
prepared a complete Android

00:02:21.080 --> 00:02:25.640
archive that includes TensorFlow
Lite with the GPU back end.

00:02:25.640 --> 00:02:28.880
When you use this, you can then
initialize the TensorFlow Lite

00:02:28.880 --> 00:02:33.200
interpreter using the GPU
back end with this code.

00:02:33.200 --> 00:02:38.150
On iOS and C++, you can use
modify graph with delegate

00:02:38.150 --> 00:02:40.760
after creating your
model with this code.

00:02:40.760 --> 00:02:43.040
All of this is done for
you in the sample app,

00:02:43.040 --> 00:02:45.420
so download it
and give it a try.

00:02:45.420 --> 00:02:48.560
Now, not all operations are
supported by the GPU back end

00:02:48.560 --> 00:02:49.790
at this moment.

00:02:49.790 --> 00:02:52.370
Your model will run
fastest when it uses

00:02:52.370 --> 00:02:55.070
only the GPU supported ops.

00:02:55.070 --> 00:02:58.150
Others will automatically
fall back to the CPU.

00:02:58.150 --> 00:03:00.740
To learn more about GPU,
including a deep dive

00:03:00.740 --> 00:03:03.380
into how it works, check
out the documentation

00:03:03.380 --> 00:03:05.600
on TensorFlow.org,
which also has

00:03:05.600 --> 00:03:08.480
a bunch of details
on optimizations,

00:03:08.480 --> 00:03:11.220
performance best practices,
and a whole lot more.

00:03:11.220 --> 00:03:14.840
This is just the beginning
of our GPU support efforts.

00:03:14.840 --> 00:03:17.510
We're continuing to add more
optimizations, performance

00:03:17.510 --> 00:03:20.060
improvements, and API
updates all the time,

00:03:20.060 --> 00:03:23.630
but would love to hear your
feedback on our GitHub page.

00:03:23.630 --> 00:03:25.640
We'd also love to hear
your feedback on YouTube,

00:03:25.640 --> 00:03:27.574
so don't forget to hit
that Subscribe button.

00:03:27.574 --> 00:03:29.240
And if you've any
questions or comments,

00:03:29.240 --> 00:03:31.170
just please leave in
the comments below.

00:03:31.170 --> 00:03:33.020
Thank you.

00:03:33.020 --> 00:03:36.590
Are you excited to see what's
new with TensorFlow in 2019?

00:03:36.590 --> 00:03:38.614
We've got lots of great
new content coming,

00:03:38.614 --> 00:03:41.030
and it's all going to be covered
right here on the YouTube

00:03:41.030 --> 00:03:41.630
channel.

00:03:41.630 --> 00:03:44.270
So whatever you do, don't forget
to hit that Subscribe button,

00:03:44.270 --> 00:03:46.270
and we'll see you there.

