WEBVTT
Kind: captions
Language: en

00:00:00.420 --> 00:00:02.270
 
one of the problems with training your

00:00:02.270 --> 00:00:02.280
one of the problems with training your
 

00:00:02.280 --> 00:00:04.039
one of the problems with training your
network especially very deep neural

00:00:04.039 --> 00:00:04.049
network especially very deep neural
 

00:00:04.049 --> 00:00:06.230
network especially very deep neural
networks is that are vanishing and

00:00:06.230 --> 00:00:06.240
networks is that are vanishing and
 

00:00:06.240 --> 00:00:08.059
networks is that are vanishing and
exploding gradients what that means is

00:00:08.059 --> 00:00:08.069
exploding gradients what that means is
 

00:00:08.069 --> 00:00:09.650
exploding gradients what that means is
that when you're training a very deep

00:00:09.650 --> 00:00:09.660
that when you're training a very deep
 

00:00:09.660 --> 00:00:11.629
that when you're training a very deep
network you're derivatives or your

00:00:11.629 --> 00:00:11.639
network you're derivatives or your
 

00:00:11.639 --> 00:00:13.549
network you're derivatives or your
slopes can sometimes get you to very

00:00:13.549 --> 00:00:13.559
slopes can sometimes get you to very
 

00:00:13.559 --> 00:00:16.250
slopes can sometimes get you to very
very big or very very small maybe even

00:00:16.250 --> 00:00:16.260
very big or very very small maybe even
 

00:00:16.260 --> 00:00:17.930
very big or very very small maybe even
exponentially small and this makes

00:00:17.930 --> 00:00:17.940
exponentially small and this makes
 

00:00:17.940 --> 00:00:20.300
exponentially small and this makes
training difficult in this video you see

00:00:20.300 --> 00:00:20.310
training difficult in this video you see
 

00:00:20.310 --> 00:00:22.880
training difficult in this video you see
what this problem of exploding or that

00:00:22.880 --> 00:00:22.890
what this problem of exploding or that
 

00:00:22.890 --> 00:00:25.580
what this problem of exploding or that
vanishing gradients really means as well

00:00:25.580 --> 00:00:25.590
vanishing gradients really means as well
 

00:00:25.590 --> 00:00:28.580
vanishing gradients really means as well
as how you can use careful choices of

00:00:28.580 --> 00:00:28.590
as how you can use careful choices of
 

00:00:28.590 --> 00:00:30.589
as how you can use careful choices of
the random way the initialization to

00:00:30.589 --> 00:00:30.599
the random way the initialization to
 

00:00:30.599 --> 00:00:33.440
the random way the initialization to
significantly reduce this problem less

00:00:33.440 --> 00:00:33.450
significantly reduce this problem less
 

00:00:33.450 --> 00:00:35.299
significantly reduce this problem less
your training very deep neural network

00:00:35.299 --> 00:00:35.309
your training very deep neural network
 

00:00:35.309 --> 00:00:37.069
your training very deep neural network
like this the same space on this slide

00:00:37.069 --> 00:00:37.079
like this the same space on this slide
 

00:00:37.079 --> 00:00:38.540
like this the same space on this slide
I've drawn it as if you have only two

00:00:38.540 --> 00:00:38.550
I've drawn it as if you have only two
 

00:00:38.550 --> 00:00:41.450
I've drawn it as if you have only two
hidden units per layer but it could be

00:00:41.450 --> 00:00:41.460
hidden units per layer but it could be
 

00:00:41.460 --> 00:00:43.220
hidden units per layer but it could be
more as well but this neural network

00:00:43.220 --> 00:00:43.230
more as well but this neural network
 

00:00:43.230 --> 00:00:49.040
more as well but this neural network
will have parameters W 1 W 2 W 3 and so

00:00:49.040 --> 00:00:49.050
will have parameters W 1 W 2 W 3 and so
 

00:00:49.050 --> 00:00:52.430
will have parameters W 1 W 2 W 3 and so
on up to WL for the sake of simplicity

00:00:52.430 --> 00:00:52.440
on up to WL for the sake of simplicity
 

00:00:52.440 --> 00:00:54.860
on up to WL for the sake of simplicity
let's say we're using an activation

00:00:54.860 --> 00:00:54.870
let's say we're using an activation
 

00:00:54.870 --> 00:00:57.200
let's say we're using an activation
function G of Z equals Z so a linear

00:00:57.200 --> 00:00:57.210
function G of Z equals Z so a linear
 

00:00:57.210 --> 00:01:00.260
function G of Z equals Z so a linear
activation function and let's ignore be

00:01:00.260 --> 00:01:00.270
activation function and let's ignore be
 

00:01:00.270 --> 00:01:04.070
activation function and let's ignore be
the set B of l equals 0 so in that case

00:01:04.070 --> 00:01:04.080
the set B of l equals 0 so in that case
 

00:01:04.080 --> 00:01:08.120
the set B of l equals 0 so in that case
you can show that the output Y will be W

00:01:08.120 --> 00:01:08.130
you can show that the output Y will be W
 

00:01:08.130 --> 00:01:14.120
you can show that the output Y will be W
L times W 0 minus 1 times WL minus 2 dot

00:01:14.120 --> 00:01:14.130
L times W 0 minus 1 times WL minus 2 dot
 

00:01:14.130 --> 00:01:21.770
L times W 0 minus 1 times WL minus 2 dot
dot down to w3 W 2 W 1 times X that

00:01:21.770 --> 00:01:21.780
dot down to w3 W 2 W 1 times X that
 

00:01:21.780 --> 00:01:23.390
dot down to w3 W 2 W 1 times X that
means if you want to just check my math

00:01:23.390 --> 00:01:23.400
means if you want to just check my math
 

00:01:23.400 --> 00:01:28.070
means if you want to just check my math
W 1 times X is going to be Z 1 right

00:01:28.070 --> 00:01:28.080
W 1 times X is going to be Z 1 right
 

00:01:28.080 --> 00:01:31.010
W 1 times X is going to be Z 1 right
because B where is equal to 0 so Z 1 is

00:01:31.010 --> 00:01:31.020
because B where is equal to 0 so Z 1 is
 

00:01:31.020 --> 00:01:36.440
because B where is equal to 0 so Z 1 is
equal to I guess W 1 times X and then

00:01:36.440 --> 00:01:36.450
equal to I guess W 1 times X and then
 

00:01:36.450 --> 00:01:40.310
equal to I guess W 1 times X and then
plug V which is 0 but then a 1 is equal

00:01:40.310 --> 00:01:40.320
plug V which is 0 but then a 1 is equal
 

00:01:40.320 --> 00:01:43.700
plug V which is 0 but then a 1 is equal
to G of Z 1 but because you use a linear

00:01:43.700 --> 00:01:43.710
to G of Z 1 but because you use a linear
 

00:01:43.710 --> 00:01:46.310
to G of Z 1 but because you use a linear
activation function this is just equal

00:01:46.310 --> 00:01:46.320
activation function this is just equal
 

00:01:46.320 --> 00:01:49.819
activation function this is just equal
to Z 1 so this first term W 1 X is equal

00:01:49.819 --> 00:01:49.829
to Z 1 so this first term W 1 X is equal
 

00:01:49.829 --> 00:01:52.039
to Z 1 so this first term W 1 X is equal
to a 1 and then by still everything you

00:01:52.039 --> 00:01:52.049
to a 1 and then by still everything you
 

00:01:52.049 --> 00:01:55.340
to a 1 and then by still everything you
can figure out that W 2 times W 1 times

00:01:55.340 --> 00:01:55.350
can figure out that W 2 times W 1 times
 

00:01:55.350 --> 00:01:58.639
can figure out that W 2 times W 1 times
X is equal to a 2 because that's going

00:01:58.639 --> 00:01:58.649
X is equal to a 2 because that's going
 

00:01:58.649 --> 00:02:04.840
X is equal to a 2 because that's going
to be G of Z 2 is going to be G of W 2

00:02:04.840 --> 00:02:04.850
to be G of Z 2 is going to be G of W 2
 

00:02:04.850 --> 00:02:09.279
to be G of Z 2 is going to be G of W 2
times a 1

00:02:09.279 --> 00:02:09.289
times a 1
 

00:02:09.289 --> 00:02:13.929
times a 1
which implies that in here so this thing

00:02:13.929 --> 00:02:13.939
which implies that in here so this thing
 

00:02:13.939 --> 00:02:17.170
which implies that in here so this thing
is going to be equal to a two and then

00:02:17.170 --> 00:02:17.180
is going to be equal to a two and then
 

00:02:17.180 --> 00:02:22.809
is going to be equal to a two and then
yo this thing is going to be a three and

00:02:22.809 --> 00:02:22.819
yo this thing is going to be a three and
 

00:02:22.819 --> 00:02:25.030
yo this thing is going to be a three and
so on until the products all these

00:02:25.030 --> 00:02:25.040
so on until the products all these
 

00:02:25.040 --> 00:02:29.440
so on until the products all these
matrices gives you the Y hat not Y now

00:02:29.440 --> 00:02:29.450
matrices gives you the Y hat not Y now
 

00:02:29.450 --> 00:02:31.930
matrices gives you the Y hat not Y now
let's say that each of your weight

00:02:31.930 --> 00:02:31.940
let's say that each of your weight
 

00:02:31.940 --> 00:02:36.339
let's say that each of your weight
matrices WL is equal to let's say is

00:02:36.339 --> 00:02:36.349
matrices WL is equal to let's say is
 

00:02:36.349 --> 00:02:38.890
matrices WL is equal to let's say is
just a little bit larger than one time's

00:02:38.890 --> 00:02:38.900
just a little bit larger than one time's
 

00:02:38.900 --> 00:02:40.720
just a little bit larger than one time's
the identity so it's one point five one

00:02:40.720 --> 00:02:40.730
the identity so it's one point five one
 

00:02:40.730 --> 00:02:44.259
the identity so it's one point five one
point five zero zero right technically

00:02:44.259 --> 00:02:44.269
point five zero zero right technically
 

00:02:44.269 --> 00:02:45.970
point five zero zero right technically
the last one has different dimensions so

00:02:45.970 --> 00:02:45.980
the last one has different dimensions so
 

00:02:45.980 --> 00:02:47.710
the last one has different dimensions so
maybe this is just the rest of these

00:02:47.710 --> 00:02:47.720
maybe this is just the rest of these
 

00:02:47.720 --> 00:02:51.940
maybe this is just the rest of these
void matrices then Y hat will be you

00:02:51.940 --> 00:02:51.950
void matrices then Y hat will be you
 

00:02:51.950 --> 00:02:53.860
void matrices then Y hat will be you
know ignoring those last ones different

00:02:53.860 --> 00:02:53.870
know ignoring those last ones different
 

00:02:53.870 --> 00:02:55.899
know ignoring those last ones different
dimension will be this one point five

00:02:55.899 --> 00:02:55.909
dimension will be this one point five
 

00:02:55.909 --> 00:02:58.960
dimension will be this one point five
zero zero one point five matrix to the

00:02:58.960 --> 00:02:58.970
zero zero one point five matrix to the
 

00:02:58.970 --> 00:03:02.949
zero zero one point five matrix to the
power of L minus one times X because if

00:03:02.949 --> 00:03:02.959
power of L minus one times X because if
 

00:03:02.959 --> 00:03:04.420
power of L minus one times X because if
we assume that each one of these

00:03:04.420 --> 00:03:04.430
we assume that each one of these
 

00:03:04.430 --> 00:03:07.390
we assume that each one of these
matrices you know is equal to this thing

00:03:07.390 --> 00:03:07.400
matrices you know is equal to this thing
 

00:03:07.400 --> 00:03:09.430
matrices you know is equal to this thing
is really one point five times the

00:03:09.430 --> 00:03:09.440
is really one point five times the
 

00:03:09.440 --> 00:03:11.440
is really one point five times the
identity matrix then you end up with

00:03:11.440 --> 00:03:11.450
identity matrix then you end up with
 

00:03:11.450 --> 00:03:14.400
identity matrix then you end up with
this calculation and so Y hat will be

00:03:14.400 --> 00:03:14.410
this calculation and so Y hat will be
 

00:03:14.410 --> 00:03:17.800
this calculation and so Y hat will be
essentially one point five to the power

00:03:17.800 --> 00:03:17.810
essentially one point five to the power
 

00:03:17.810 --> 00:03:22.449
essentially one point five to the power
of L mm mm minus one times X and if L is

00:03:22.449 --> 00:03:22.459
of L mm mm minus one times X and if L is
 

00:03:22.459 --> 00:03:25.059
of L mm mm minus one times X and if L is
large for very deep neural network Y has

00:03:25.059 --> 00:03:25.069
large for very deep neural network Y has
 

00:03:25.069 --> 00:03:27.460
large for very deep neural network Y has
will be very large in fact this grows

00:03:27.460 --> 00:03:27.470
will be very large in fact this grows
 

00:03:27.470 --> 00:03:29.379
will be very large in fact this grows
exponentially it grows like one point

00:03:29.379 --> 00:03:29.389
exponentially it grows like one point
 

00:03:29.389 --> 00:03:32.740
exponentially it grows like one point
five to the number of layers and so if

00:03:32.740 --> 00:03:32.750
five to the number of layers and so if
 

00:03:32.750 --> 00:03:34.390
five to the number of layers and so if
you have a very deep neural network the

00:03:34.390 --> 00:03:34.400
you have a very deep neural network the
 

00:03:34.400 --> 00:03:37.420
you have a very deep neural network the
value of y will explode now conversely

00:03:37.420 --> 00:03:37.430
value of y will explode now conversely
 

00:03:37.430 --> 00:03:40.599
value of y will explode now conversely
if we replace this with zero point five

00:03:40.599 --> 00:03:40.609
if we replace this with zero point five
 

00:03:40.609 --> 00:03:43.030
if we replace this with zero point five
so something less than one then this

00:03:43.030 --> 00:03:43.040
so something less than one then this
 

00:03:43.040 --> 00:03:45.250
so something less than one then this
becomes zero point five to the power of

00:03:45.250 --> 00:03:45.260
becomes zero point five to the power of
 

00:03:45.260 --> 00:03:46.780
becomes zero point five to the power of
L where this matrix

00:03:46.780 --> 00:03:46.790
L where this matrix
 

00:03:46.790 --> 00:03:49.689
L where this matrix
um becomes zero point five to the o

00:03:49.689 --> 00:03:49.699
um becomes zero point five to the o
 

00:03:49.699 --> 00:03:53.379
um becomes zero point five to the o
minus one times X we can ignoring WL but

00:03:53.379 --> 00:03:53.389
minus one times X we can ignoring WL but
 

00:03:53.389 --> 00:03:56.740
minus one times X we can ignoring WL but
so each of your matrices are less than

00:03:56.740 --> 00:03:56.750
so each of your matrices are less than
 

00:03:56.750 --> 00:04:00.039
so each of your matrices are less than
one then if let's say X 1 X 2 where 1 1

00:04:00.039 --> 00:04:00.049
one then if let's say X 1 X 2 where 1 1
 

00:04:00.049 --> 00:04:03.300
one then if let's say X 1 X 2 where 1 1
then the activations would be 1/2 1/2

00:04:03.300 --> 00:04:03.310
then the activations would be 1/2 1/2
 

00:04:03.310 --> 00:04:08.530
then the activations would be 1/2 1/2
1/4 1/4 1/8 1/8 and so on until this

00:04:08.530 --> 00:04:08.540
1/4 1/4 1/8 1/8 and so on until this
 

00:04:08.540 --> 00:04:11.530
1/4 1/4 1/8 1/8 and so on until this
becomes a right 1 over 2 to the L so the

00:04:11.530 --> 00:04:11.540
becomes a right 1 over 2 to the L so the
 

00:04:11.540 --> 00:04:13.629
becomes a right 1 over 2 to the L so the
activation values will decrease

00:04:13.629 --> 00:04:13.639
activation values will decrease
 

00:04:13.639 --> 00:04:16.479
activation values will decrease
exponentially as a function of the deaf

00:04:16.479 --> 00:04:16.489
exponentially as a function of the deaf
 

00:04:16.489 --> 00:04:17.400
exponentially as a function of the deaf
as a function

00:04:17.400 --> 00:04:17.410
as a function
 

00:04:17.410 --> 00:04:20.100
as a function
the number of layers elves in network so

00:04:20.100 --> 00:04:20.110
the number of layers elves in network so
 

00:04:20.110 --> 00:04:22.130
the number of layers elves in network so
they be very deep network these

00:04:22.130 --> 00:04:22.140
they be very deep network these
 

00:04:22.140 --> 00:04:24.650
they be very deep network these
activations end up decreasing

00:04:24.650 --> 00:04:24.660
activations end up decreasing
 

00:04:24.660 --> 00:04:27.300
activations end up decreasing
exponentially so the intuition I hope

00:04:27.300 --> 00:04:27.310
exponentially so the intuition I hope
 

00:04:27.310 --> 00:04:29.190
exponentially so the intuition I hope
you can take away from this is that if

00:04:29.190 --> 00:04:29.200
you can take away from this is that if
 

00:04:29.200 --> 00:04:31.920
you can take away from this is that if
the weights W if they're all you know

00:04:31.920 --> 00:04:31.930
the weights W if they're all you know
 

00:04:31.930 --> 00:04:33.840
the weights W if they're all you know
just a little bit bigger than one I'll

00:04:33.840 --> 00:04:33.850
just a little bit bigger than one I'll
 

00:04:33.850 --> 00:04:35.970
just a little bit bigger than one I'll
just work with bigger then the identity

00:04:35.970 --> 00:04:35.980
just work with bigger then the identity
 

00:04:35.980 --> 00:04:39.390
just work with bigger then the identity
matrix then with a very deep network the

00:04:39.390 --> 00:04:39.400
matrix then with a very deep network the
 

00:04:39.400 --> 00:04:43.590
matrix then with a very deep network the
activations can explode and if W is you

00:04:43.590 --> 00:04:43.600
activations can explode and if W is you
 

00:04:43.600 --> 00:04:44.640
activations can explode and if W is you
know just a little bit that's the

00:04:44.640 --> 00:04:44.650
know just a little bit that's the
 

00:04:44.650 --> 00:04:47.220
know just a little bit that's the
identity right so this was maybe is 0.9

00:04:47.220 --> 00:04:47.230
identity right so this was maybe is 0.9
 

00:04:47.230 --> 00:04:50.160
identity right so this was maybe is 0.9
0.9 right then if a very deep network

00:04:50.160 --> 00:04:50.170
0.9 right then if a very deep network
 

00:04:50.170 --> 00:04:52.080
0.9 right then if a very deep network
the activations will decrease

00:04:52.080 --> 00:04:52.090
the activations will decrease
 

00:04:52.090 --> 00:04:54.300
the activations will decrease
exponentially and even though I went

00:04:54.300 --> 00:04:54.310
exponentially and even though I went
 

00:04:54.310 --> 00:04:55.940
exponentially and even though I went
through this argument in terms of

00:04:55.940 --> 00:04:55.950
through this argument in terms of
 

00:04:55.950 --> 00:04:58.220
through this argument in terms of
activations increasing or decreasing

00:04:58.220 --> 00:04:58.230
activations increasing or decreasing
 

00:04:58.230 --> 00:05:01.320
activations increasing or decreasing
exponentially as a function of level the

00:05:01.320 --> 00:05:01.330
exponentially as a function of level the
 

00:05:01.330 --> 00:05:02.910
exponentially as a function of level the
similar argument can be used to show

00:05:02.910 --> 00:05:02.920
similar argument can be used to show
 

00:05:02.920 --> 00:05:04.740
similar argument can be used to show
that the derivatives or the gradients

00:05:04.740 --> 00:05:04.750
that the derivatives or the gradients
 

00:05:04.750 --> 00:05:06.690
that the derivatives or the gradients
you compete with we understand will also

00:05:06.690 --> 00:05:06.700
you compete with we understand will also
 

00:05:06.700 --> 00:05:09.030
you compete with we understand will also
increase exponentially or decrease

00:05:09.030 --> 00:05:09.040
increase exponentially or decrease
 

00:05:09.040 --> 00:05:10.590
increase exponentially or decrease
exponentially as a function of the

00:05:10.590 --> 00:05:10.600
exponentially as a function of the
 

00:05:10.600 --> 00:05:12.480
exponentially as a function of the
number of layers where some of the

00:05:12.480 --> 00:05:12.490
number of layers where some of the
 

00:05:12.490 --> 00:05:14.130
number of layers where some of the
modern neural networks you actually have

00:05:14.130 --> 00:05:14.140
modern neural networks you actually have
 

00:05:14.140 --> 00:05:16.620
modern neural networks you actually have
l equals hundred and fifty Microsoft

00:05:16.620 --> 00:05:16.630
l equals hundred and fifty Microsoft
 

00:05:16.630 --> 00:05:18.360
l equals hundred and fifty Microsoft
basically got great results of

00:05:18.360 --> 00:05:18.370
basically got great results of
 

00:05:18.370 --> 00:05:20.490
basically got great results of
encountering 52 layer in your network

00:05:20.490 --> 00:05:20.500
encountering 52 layer in your network
 

00:05:20.500 --> 00:05:22.680
encountering 52 layer in your network
but whether such a deep neural network

00:05:22.680 --> 00:05:22.690
but whether such a deep neural network
 

00:05:22.690 --> 00:05:24.180
but whether such a deep neural network
if your activations your gradient

00:05:24.180 --> 00:05:24.190
if your activations your gradient
 

00:05:24.190 --> 00:05:26.490
if your activations your gradient
increase or decrease exponentially as a

00:05:26.490 --> 00:05:26.500
increase or decrease exponentially as a
 

00:05:26.500 --> 00:05:29.160
increase or decrease exponentially as a
function of L then these values could

00:05:29.160 --> 00:05:29.170
function of L then these values could
 

00:05:29.170 --> 00:05:32.040
function of L then these values could
get really big or really small and this

00:05:32.040 --> 00:05:32.050
get really big or really small and this
 

00:05:32.050 --> 00:05:34.320
get really big or really small and this
makes training difficult especially if

00:05:34.320 --> 00:05:34.330
makes training difficult especially if
 

00:05:34.330 --> 00:05:36.330
makes training difficult especially if
your gradients are exponentially small

00:05:36.330 --> 00:05:36.340
your gradients are exponentially small
 

00:05:36.340 --> 00:05:38.970
your gradients are exponentially small
in elm then you know gradient descents

00:05:38.970 --> 00:05:38.980
in elm then you know gradient descents
 

00:05:38.980 --> 00:05:40.920
in elm then you know gradient descents
will take tiny little steps they'll take

00:05:40.920 --> 00:05:40.930
will take tiny little steps they'll take
 

00:05:40.930 --> 00:05:43.050
will take tiny little steps they'll take
a long time for gradient descent to

00:05:43.050 --> 00:05:43.060
a long time for gradient descent to
 

00:05:43.060 --> 00:05:43.830
a long time for gradient descent to
learn anything

00:05:43.830 --> 00:05:43.840
learn anything
 

00:05:43.840 --> 00:05:46.200
learn anything
to summarize you've seen how deep

00:05:46.200 --> 00:05:46.210
to summarize you've seen how deep
 

00:05:46.210 --> 00:05:48.150
to summarize you've seen how deep
networks suffer from the problems of

00:05:48.150 --> 00:05:48.160
networks suffer from the problems of
 

00:05:48.160 --> 00:05:51.270
networks suffer from the problems of
vanishing or exploding gradients in fact

00:05:51.270 --> 00:05:51.280
vanishing or exploding gradients in fact
 

00:05:51.280 --> 00:05:53.010
vanishing or exploding gradients in fact
for a long time this problem was a huge

00:05:53.010 --> 00:05:53.020
for a long time this problem was a huge
 

00:05:53.020 --> 00:05:55.170
for a long time this problem was a huge
barrier to training deep neural networks

00:05:55.170 --> 00:05:55.180
barrier to training deep neural networks
 

00:05:55.180 --> 00:05:57.750
barrier to training deep neural networks
it turns out there's a partial solution

00:05:57.750 --> 00:05:57.760
it turns out there's a partial solution
 

00:05:57.760 --> 00:05:59.700
it turns out there's a partial solution
that doesn't completely solve this

00:05:59.700 --> 00:05:59.710
that doesn't completely solve this
 

00:05:59.710 --> 00:06:01.500
that doesn't completely solve this
problem but that helps a lot which is

00:06:01.500 --> 00:06:01.510
problem but that helps a lot which is
 

00:06:01.510 --> 00:06:03.810
problem but that helps a lot which is
careful choice of how you initialize the

00:06:03.810 --> 00:06:03.820
careful choice of how you initialize the
 

00:06:03.820 --> 00:06:06.240
careful choice of how you initialize the
weights to see that let's go on to the

00:06:06.240 --> 00:06:06.250
weights to see that let's go on to the
 

00:06:06.250 --> 00:06:08.790
weights to see that let's go on to the
next video

