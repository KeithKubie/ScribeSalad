WEBVTT

1
00:00:00.960 --> 00:00:03.270
In this video,
we'll see set cop processing,

2
00:00:03.271 --> 00:00:08.070
which is a very useful preprocessing to apply to our data before we run a sparse

3
00:00:08.071 --> 00:00:08.904
coding algorithm.

4
00:00:11.760 --> 00:00:15.290
So sparse coding is trying to learn about the structure,

5
00:00:15.310 --> 00:00:17.910
the statistical structure of our input data,

6
00:00:18.360 --> 00:00:23.190
and we find in practice that it's often beneficial to remove a part of the

7
00:00:23.191 --> 00:00:26.360
structure,
which is a quote unquote obvious,
uh,

8
00:00:26.370 --> 00:00:30.000
which is not particularly interesting and,
uh,
would not correspond to,

9
00:00:30.001 --> 00:00:34.470
will not allow us to extract,
uh,
uh,
interesting features.
Uh,

10
00:00:34.471 --> 00:00:35.370
so for instance,

11
00:00:35.400 --> 00:00:39.570
the mean of the data is not something that's particularly interesting for

12
00:00:39.571 --> 00:00:43.470
representing our different data points into a feature vector Leighton

13
00:00:43.680 --> 00:00:48.240
representation and also some of the,
uh,
linner,

14
00:00:48.420 --> 00:00:49.220
uh,
covert.

15
00:00:49.220 --> 00:00:54.220
So linear correlations or the covariants essentially of our data is also not a

16
00:00:54.720 --> 00:00:58.560
kind of statistical structure of dependence that's particularly interesting.

17
00:00:58.920 --> 00:01:02.870
And so for that reason,
what we often do in that ca preprocessing,

18
00:01:03.000 --> 00:01:07.880
it's just removed a mean and also,
uh,
uh,
changed the,
uh,

19
00:01:07.890 --> 00:01:12.420
do preprocessing such that the empirical covariance of our data is going to be

20
00:01:12.421 --> 00:01:13.750
the identity,
uh,

21
00:01:13.770 --> 00:01:18.150
which would mean that the covariance between the preprocess data,
uh,

22
00:01:18.390 --> 00:01:21.870
between the elements of the vectors in the process data is going to be zero.

23
00:01:22.020 --> 00:01:26.160
And then the variance of each element in RP process data,

24
00:01:26.460 --> 00:01:30.760
uh,
is going to be one.
And so that's a process known as,

25
00:01:30.761 --> 00:01:34.680
as whitening.
And so,
uh,
and uh,

26
00:01:34.710 --> 00:01:39.060
statistical jargon will say that by doing this we'll remove the first and second

27
00:01:39.061 --> 00:01:43.530
order statistical structure in our data.
And so,
uh,

28
00:01:43.531 --> 00:01:47.070
it means that our sparse calling algorithm will then perhaps extract structure

29
00:01:47.071 --> 00:01:52.020
that's a bit more complicated than higher order.
And so,

30
00:01:52.021 --> 00:01:55.260
uh,
does that say be Ross things very simple.
I just described the procedure.

31
00:01:55.740 --> 00:02:00.740
So what we do is first we compute the empirical mean of our training data as

32
00:02:00.991 --> 00:02:03.390
well as the empirical covariance Matrix.

33
00:02:03.570 --> 00:02:07.020
So new hat will be empirical mean and the Sigma Hat,

34
00:02:07.021 --> 00:02:10.650
the empirical co-parents Matrix.
And we also assume that,
uh,

35
00:02:10.680 --> 00:02:13.590
we have computed a Egan value,
Egan vector,

36
00:02:13.650 --> 00:02:17.220
a decomposition of our matrix.
So here the,

37
00:02:17.360 --> 00:02:21.720
and vectors are the columns of the Matrix,
you and then,
uh,
here,

38
00:02:21.721 --> 00:02:26.160
lambda has a zero on it's off diagonal terms.

39
00:02:26.400 --> 00:02:27.630
It's a square matrix with zero,

40
00:02:27.631 --> 00:02:30.860
and it's off diagonal terms and the diagonal terms,
uh,
uh,

41
00:02:31.020 --> 00:02:33.600
correspond to the EIG and values that are sorted.

42
00:02:35.040 --> 00:02:39.810
And now's it CEO said,Z is simply a linear transform that takes,

43
00:02:39.860 --> 00:02:44.160
I give an input and pre process it as follows.
It takes the input,

44
00:02:44.400 --> 00:02:46.410
subtracts the empirical mean,

45
00:02:46.740 --> 00:02:50.010
and then multiply by this transformation here,

46
00:02:50.250 --> 00:02:52.560
which is similar to this one here.

47
00:02:53.040 --> 00:02:58.040
So the difference is only that the diagonal matrix in the middle is going to be

48
00:02:58.111 --> 00:03:02.640
lambda where taken the diagonal terms.
And then,
uh,
we've,
uh,

49
00:03:02.680 --> 00:03:04.960
taken the exponential,
uh,

50
00:03:04.961 --> 00:03:09.400
as so exponentiate it by minus one hat.
So in other words,

51
00:03:09.401 --> 00:03:14.401
the diagonal terms are going to be one over the square root of the diagonal

52
00:03:15.311 --> 00:03:19.150
terms of this lambda matrix here.
Uh,

53
00:03:19.180 --> 00:03:23.560
so the Egan values of our empirical covariance Matrix.
Okay.

54
00:03:24.100 --> 00:03:28.810
So now let's see how this transformation actually removes the mean and also make

55
00:03:28.811 --> 00:03:33.160
sure that the empirical covariance matrix is now the identity matrix.

56
00:03:36.460 --> 00:03:38.680
So first with the empirical mean,
now,

57
00:03:38.681 --> 00:03:41.050
if we take the empirical mean of our preprocess data,

58
00:03:41.051 --> 00:03:46.051
it means we take one over t times the sum over all of our training data of the

59
00:03:46.481 --> 00:03:47.890
transformed data.

60
00:03:47.891 --> 00:03:52.550
So x t minus the empirical mean multiply by the transformation matrix.
Uh,

61
00:03:53.020 --> 00:03:55.090
now because this term,

62
00:03:55.091 --> 00:03:59.350
this matrix here does not depend on t and nor does the empirical mean that I can

63
00:03:59.351 --> 00:04:02.500
put the sum right inside here.

64
00:04:03.070 --> 00:04:04.360
So I can have that.

65
00:04:04.361 --> 00:04:08.880
This is equal to d linear transformation times the,
uh,

66
00:04:09.400 --> 00:04:12.280
average of x t minus the empirical me.

67
00:04:12.730 --> 00:04:15.340
And now the empirical means just the average of the XT.

68
00:04:15.370 --> 00:04:19.660
So this is just sigma,
uh,
sorry,

69
00:04:19.661 --> 00:04:23.240
a new hat.
And so we have new hat minus when you have to,

70
00:04:23.250 --> 00:04:26.980
which is zero times distance formation,
that's going to be zero.

71
00:04:26.981 --> 00:04:31.030
So we do have that,
the empirical mean is equal to zero.

72
00:04:31.031 --> 00:04:32.680
Once we've performed this transformation,

73
00:04:34.850 --> 00:04:35.410
<v 1>yeah.</v>

74
00:04:35.410 --> 00:04:40.200
<v 0>Now let's show that the empirical covariance matrix is also now the identity.</v>

75
00:04:40.210 --> 00:04:45.100
Once we've done this preprocessing.
So,
uh,
here's the expression for the,

76
00:04:45.130 --> 00:04:49.600
uh,
empirical covariance Matrix for our transformed data.

77
00:04:49.601 --> 00:04:53.130
So it's the outer product of the transformed data with itself.
Uh,

78
00:04:53.140 --> 00:04:57.180
so normally you'd expect minus the empirical mean,
uh,

79
00:04:57.220 --> 00:05:00.750
but we've shown that it's zero.
So we just have the other products of the,
uh,

80
00:05:00.920 --> 00:05:05.070
transform factors.
And now,
uh,
similarly,
uh,

81
00:05:05.140 --> 00:05:09.280
because this matrix here and you a hat,

82
00:05:09.370 --> 00:05:12.280
this matrix here and you had here does not depend on tea,

83
00:05:12.281 --> 00:05:14.880
then I can take the sum and uh,

84
00:05:14.950 --> 00:05:17.290
introduce it a further into the expression.

85
00:05:17.770 --> 00:05:22.770
So I'll have one over t minus one times some of our t of these terms here.

86
00:05:25.900 --> 00:05:29.610
Uh,
which,
so where x t depends on,
uh,
on,

87
00:05:29.680 --> 00:05:31.750
<v 1>uh,
on team.
Um,</v>

88
00:05:32.580 --> 00:05:33.720
<v 0>and so we have this expression.</v>

89
00:05:33.721 --> 00:05:38.721
So essentially I've been able to factor out that transformation here out and

90
00:05:38.851 --> 00:05:40.710
hear out and put this,
um,

91
00:05:41.240 --> 00:05:45.930
<v 1>with Eh,
within here or right here.
Uh,
so</v>

92
00:05:45.940 --> 00:05:50.390
<v 0>I actually haven't had to use the fact that you have,
doesn't depend on TV.</v>

93
00:05:51.700 --> 00:05:56.700
And now we notice that this is actually the empirical covariance Matrix.

94
00:05:57.260 --> 00:06:02.160
So this is a similar hat.
Now,

95
00:06:02.310 --> 00:06:06.080
Sigma Hat in its,
uh,
Egan vector,

96
00:06:06.081 --> 00:06:10.790
Egan value decomposition is equal to this.
And now,

97
00:06:10.850 --> 00:06:14.690
well,
we have a bunch of terms are going to cancel out.
So first,

98
00:06:14.840 --> 00:06:17.990
because the Egan vectors are auto normal.

99
00:06:18.020 --> 00:06:22.610
Then if I take the,
uh,
a matrix,
you,

100
00:06:22.750 --> 00:06:25.760
uh,
you transpose,
so you transpose,

101
00:06:25.790 --> 00:06:30.170
it's rows are the Egan vectors times the Matrix.

102
00:06:30.171 --> 00:06:34.930
You whose columns are the agon vectors,
then that's going to be the entity.

103
00:06:35.030 --> 00:06:36.290
So zero off diagonal,

104
00:06:36.291 --> 00:06:40.460
then one on the diagonal because the vectors are a orthogonal and they're

105
00:06:40.490 --> 00:06:43.730
normalized.
So this disappears.

106
00:06:44.600 --> 00:06:46.250
This disappears for the same reason.

107
00:06:47.420 --> 00:06:50.900
Now I have the a minus.
Uh,

108
00:06:50.901 --> 00:06:55.901
so the inverse square root of the elements of a Lambda Matrix Times dilemma

109
00:06:56.210 --> 00:07:01.070
matrix.
Well,
it means that I can make that disappear and have just one half here.

110
00:07:01.850 --> 00:07:06.150
Now I have Landon one half,
uh,
times,
uh,

111
00:07:06.200 --> 00:07:09.440
lambda minus one half.
So again,
this is going to cancel out.

112
00:07:09.890 --> 00:07:11.150
That's going to be the identity.

113
00:07:12.350 --> 00:07:16.580
So now I have you times you transpose a,

114
00:07:16.581 --> 00:07:20.560
which is equivalent to the Egan Value Egan vectors.

115
00:07:20.561 --> 00:07:23.390
So if I have the essentially have in the middle,

116
00:07:23.391 --> 00:07:26.150
right here as if I had the identity.

117
00:07:26.660 --> 00:07:30.820
So I have you times the identity times you transpose,
which is,
which gets,

118
00:07:31.010 --> 00:07:34.540
can be understood as the a possible Egan value.

119
00:07:34.541 --> 00:07:37.820
We can vector decomposition of the identity matrix.

120
00:07:38.180 --> 00:07:43.180
And so that means that you times and entity times you transpose is really just

121
00:07:43.550 --> 00:07:45.590
the identity matrix.
Okay.

122
00:07:45.591 --> 00:07:49.610
So we've shown that if we take the covariance Matrix,

123
00:07:49.760 --> 00:07:54.760
the empirical covariance Matrix of the transformed data with Etsy processing,

124
00:07:55.040 --> 00:07:58.730
we get that.
Now this covariance Matrix has,
uh,
is the identity.

125
00:07:59.090 --> 00:08:04.090
So the variants of all the elements in the transform vectors in our training set

126
00:08:04.340 --> 00:08:05.173
is one,

127
00:08:05.420 --> 00:08:10.360
the empirical barons and the covariance between each a pair of,
uh,
uh,

128
00:08:10.400 --> 00:08:14.610
different elements in these transform vectors is also zero,
right?

129
00:08:14.630 --> 00:08:19.190
So we've seen that desi pre processing removes the mean and in terms of our data

130
00:08:19.191 --> 00:08:23.480
so that the elements are El linearly independent,

131
00:08:23.481 --> 00:08:27.350
so they're not core linearly correlated.
And if we do this,

132
00:08:27.470 --> 00:08:31.460
then empirically we find that we,
after performing district processing,

133
00:08:31.461 --> 00:08:34.550
we run sparse coding.
We tend to get a better,

134
00:08:34.920 --> 00:08:37.340
a better feature representation of our data.

