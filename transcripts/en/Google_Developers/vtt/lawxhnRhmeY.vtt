WEBVTT
Kind: captions
Language: en

00:00:00.690 --> 00:00:03.690
We have built a sensor platform
for mobile devices that

00:00:03.690 --> 00:00:07.420
uses vision to track motion,
recognize faces, and scan

00:00:07.420 --> 00:00:09.980
the surroundings in 3D.

00:00:09.980 --> 00:00:13.160
My name is Nikolai,
and in this episode,

00:00:13.160 --> 00:00:15.420
we will talk about the
depth perception concepts

00:00:15.420 --> 00:00:18.530
and tools available
for you to get started.

00:00:18.530 --> 00:00:20.780
With the 3D depth
sensor, you can

00:00:20.780 --> 00:00:23.030
scan the space
that surrounds you,

00:00:23.030 --> 00:00:27.090
and use the data to build
models of your reality.

00:00:27.090 --> 00:00:30.150
If you need to fit a
new TV on your wall,

00:00:30.150 --> 00:00:33.020
or place a virtual
castle on the floor,

00:00:33.020 --> 00:00:36.120
or see how much ice cream
you can fit in that freezer,

00:00:36.120 --> 00:00:38.850
you may want to
continue watching.

00:00:38.850 --> 00:00:41.170
Depth perception can
create static models

00:00:41.170 --> 00:00:43.630
of the surroundings
as you scan them.

00:00:43.630 --> 00:00:46.860
If you combine depth perception
with motion tracking,

00:00:46.860 --> 00:00:49.650
you get the power to
have virtual objects

00:00:49.650 --> 00:00:54.080
coexist with physical objects
right in front of you.

00:00:54.080 --> 00:00:57.580
This can be used to detect
surfaces, obstacles,

00:00:57.580 --> 00:01:00.580
or shapes of objects.

00:01:00.580 --> 00:01:05.500
Let's say we wanted to simulate
a gecko walking on the ceiling.

00:01:05.500 --> 00:01:07.470
We would use depth
perception to detect

00:01:07.470 --> 00:01:10.890
the surface of the ceiling, and
use motion tracking to follow

00:01:10.890 --> 00:01:13.950
the gecko around the house.

00:01:13.950 --> 00:01:16.930
You won't need to know
all the details of how

00:01:16.930 --> 00:01:19.340
depth sensors work, but
it's helpful to have

00:01:19.340 --> 00:01:22.750
a general understanding to
make the best use of them.

00:01:22.750 --> 00:01:24.910
There are several
technologies available,

00:01:24.910 --> 00:01:27.780
which we will mention
three major ones.

00:01:27.780 --> 00:01:30.600
First, we have
structured light systems.

00:01:30.600 --> 00:01:32.480
They send out an
infrared dot pattern

00:01:32.480 --> 00:01:35.930
to illuminate the contours
of the environment.

00:01:35.930 --> 00:01:38.910
The device that measure
the size of the dots,

00:01:38.910 --> 00:01:43.090
the larger the dot gets,
the further away it is.

00:01:43.090 --> 00:01:47.060
Second, time-of-flight systems
use specialized sensors

00:01:47.060 --> 00:01:49.140
to measure depth,
based on the time

00:01:49.140 --> 00:01:50.970
it takes to transmit
an infrared beam

00:01:50.970 --> 00:01:53.020
and capture its reflection.

00:01:53.020 --> 00:01:54.980
At short distances,
it only takes

00:01:54.980 --> 00:01:58.640
the light a few nanoseconds, or
a few billionths of a second,

00:01:58.640 --> 00:02:01.000
to travel back and forth.

00:02:01.000 --> 00:02:04.080
Third, there are
stereo systems, which

00:02:04.080 --> 00:02:06.480
captures the scene
with two cameras that

00:02:06.480 --> 00:02:09.180
are a short distance
apart, and infer depth

00:02:09.180 --> 00:02:11.560
from using triangulation.

00:02:11.560 --> 00:02:12.940
Does that sound complicated?

00:02:12.940 --> 00:02:15.850
Well, this is very much
how human eyes work.

00:02:15.850 --> 00:02:18.140
Two eyes, or cameras
for that matter, that

00:02:18.140 --> 00:02:21.280
are able to see the same scene,
but from slightly different

00:02:21.280 --> 00:02:24.780
angles, enables the viewer
to gain a perception of depth

00:02:24.780 --> 00:02:27.800
using trigonometry.

00:02:27.800 --> 00:02:30.150
The infrared-based
depth sensors,

00:02:30.150 --> 00:02:33.000
such as structured light or
time-of-flight based systems,

00:02:33.000 --> 00:02:36.180
have the benefit of working
particularly well indoors,

00:02:36.180 --> 00:02:38.460
in low light and
where there isn't

00:02:38.460 --> 00:02:40.690
any textures on the walls.

00:02:40.690 --> 00:02:42.500
On the other hand,
infrared light

00:02:42.500 --> 00:02:44.380
is less reliable
in direct sunlight

00:02:44.380 --> 00:02:48.000
and on very dark surfaces, which
tend to absorb infrared light,

00:02:48.000 --> 00:02:52.700
because it reduces the amount
of depth data you will acquire.

00:02:52.700 --> 00:02:54.870
The sensors in Project
Tango are currently

00:02:54.870 --> 00:02:58.090
optimized to scan room size
and static environments,

00:02:58.090 --> 00:03:00.010
which makes it well
suited for detecting

00:03:00.010 --> 00:03:03.170
surfaces, large objects,
like walls, doors, tables,

00:03:03.170 --> 00:03:05.310
and chairs.

00:03:05.310 --> 00:03:08.140
Other requirements to bear in
mind as you design your program

00:03:08.140 --> 00:03:11.390
is the need for resolution,
range, and speed.

00:03:11.390 --> 00:03:14.970
Resolution determines how
much detail you can capture.

00:03:14.970 --> 00:03:17.930
Range, how close or far
you can be to an object.

00:03:17.930 --> 00:03:22.170
And speed is how fast you
can capture the motion.

00:03:22.170 --> 00:03:25.160
Project Tango returns depth
data as a point cloud.

00:03:25.160 --> 00:03:28.920
It is the raw representation
of what the sensor detects.

00:03:28.920 --> 00:03:30.560
You can use Project
Tango Explorer

00:03:30.560 --> 00:03:35.040
to see what data looks like
in the point cloud mode.

00:03:35.040 --> 00:03:38.580
Each of the displaced dots is
associated with a precision

00:03:38.580 --> 00:03:41.390
and a distance to the device.

00:03:41.390 --> 00:03:43.650
As you move around,
Explorer applies

00:03:43.650 --> 00:03:47.380
motion tracking to combine
multiple point clouds into one.

00:03:47.380 --> 00:03:51.490
The raw data is great for
calculating floor height,

00:03:51.490 --> 00:03:55.560
distances to nearby objects,
or to find surfaces.

00:03:55.560 --> 00:03:59.490
If you instead want to create
complete 3D models, or images,

00:03:59.490 --> 00:04:03.250
you can use an app like
Project Tango Constructor.

00:04:03.250 --> 00:04:07.140
Let's have a look
at how that works.

00:04:07.140 --> 00:04:10.430
As you scan an area with
Constructor, the app combines

00:04:10.430 --> 00:04:13.390
depth data with colored
images from the camera,

00:04:13.390 --> 00:04:16.320
while you can watch the 3D
model of the surroundings

00:04:16.320 --> 00:04:20.500
as it's being built. When
you're done scanning the area,

00:04:20.500 --> 00:04:24.680
the constructor can export a
file of the generated 3D models

00:04:24.680 --> 00:04:25.790
to the SD card.

00:04:25.790 --> 00:04:27.720
From there, the
file can be imported

00:04:27.720 --> 00:04:30.110
by other apps or games.

00:04:30.110 --> 00:04:33.380
If you want to test drive a
micro version of the Mars Rover

00:04:33.380 --> 00:04:36.010
in a sandbox, we
could use Constructor

00:04:36.010 --> 00:04:39.490
to create 3D models of the
hills and valleys in the sand,

00:04:39.490 --> 00:04:41.910
export the data, and
then build an app that

00:04:41.910 --> 00:04:47.130
uses the exported data as
the surface for the Rover.

00:04:47.130 --> 00:04:50.020
Project Tango is all about
connecting the virtual

00:04:50.020 --> 00:04:51.940
with the physical experience.

00:04:51.940 --> 00:04:55.660
And the depth sensor literally
brings the physical world

00:04:55.660 --> 00:04:57.710
into the virtual
world, so you can

00:04:57.710 --> 00:05:00.920
build virtual biology
experiments or Mars Rover

00:05:00.920 --> 00:05:02.560
simulators.

00:05:02.560 --> 00:05:05.550
Visit our Google Plus community,
and join us in our journey.

00:05:05.550 --> 00:05:08.590
We are excited to see what you
will build with Project Tango.

00:05:08.590 --> 00:05:12.080
[MUSIC PLAYING]

