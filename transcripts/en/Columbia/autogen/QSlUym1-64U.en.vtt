WEBVTT
Kind: captions
Language: en

00:00:07.210 --> 00:00:11.980
 
everybody naked sample I'm honestly I'm

00:00:11.980 --> 00:00:11.990
everybody naked sample I'm honestly I'm
 

00:00:11.990 --> 00:00:18.390
everybody naked sample I'm honestly I'm
picture Andrea's car work it's needs

00:00:18.390 --> 00:00:18.400
picture Andrea's car work it's needs
 

00:00:18.400 --> 00:00:21.460
picture Andrea's car work it's needs
special prep theory so what that means

00:00:21.460 --> 00:00:21.470
special prep theory so what that means
 

00:00:21.470 --> 00:00:24.939
special prep theory so what that means
is for analyzing students properties

00:00:24.939 --> 00:00:24.949
is for analyzing students properties
 

00:00:24.949 --> 00:00:28.689
is for analyzing students properties
that graph by housing the spectrum which

00:00:28.689 --> 00:00:28.699
that graph by housing the spectrum which
 

00:00:28.699 --> 00:00:32.049
that graph by housing the spectrum which
are operators to Latin America simply

00:00:32.049 --> 00:00:32.059
are operators to Latin America simply
 

00:00:32.059 --> 00:00:35.530
are operators to Latin America simply
our case we're analyzing connectedness

00:00:35.530 --> 00:00:35.540
our case we're analyzing connectedness
 

00:00:35.540 --> 00:00:40.479
our case we're analyzing connectedness
and by partek is by analyzing the

00:00:40.479 --> 00:00:40.489
and by partek is by analyzing the
 

00:00:40.489 --> 00:00:43.619
and by partek is by analyzing the
spectrum of these will plus one script

00:00:43.619 --> 00:00:43.629
spectrum of these will plus one script
 

00:00:43.629 --> 00:00:48.300
spectrum of these will plus one script
so it would want free

00:00:48.300 --> 00:00:48.310
so it would want free
 

00:00:48.310 --> 00:00:50.490
so it would want free
one interesting question on my task is

00:00:50.490 --> 00:00:50.500
one interesting question on my task is
 

00:00:50.500 --> 00:00:53.490
one interesting question on my task is
how how connected or we're going to have

00:00:53.490 --> 00:00:53.500
how how connected or we're going to have
 

00:00:53.500 --> 00:00:58.140
how how connected or we're going to have
cluster whoa like rat is so you could

00:00:58.140 --> 00:00:58.150
cluster whoa like rat is so you could
 

00:00:58.150 --> 00:01:00.500
cluster whoa like rat is so you could
have say a graph that is completely

00:01:00.500 --> 00:01:00.510
have say a graph that is completely
 

00:01:00.510 --> 00:01:04.170
have say a graph that is completely
discounted to set their quantities which

00:01:04.170 --> 00:01:04.180
discounted to set their quantities which
 

00:01:04.180 --> 00:01:06.420
discounted to set their quantities which
would be completely but you might also

00:01:06.420 --> 00:01:06.430
would be completely but you might also
 

00:01:06.430 --> 00:01:12.210
would be completely but you might also
have two obvious clusters like this with

00:01:12.210 --> 00:01:12.220
have two obvious clusters like this with
 

00:01:12.220 --> 00:01:16.290
have two obvious clusters like this with
a few edges one or one edge connected so

00:01:16.290 --> 00:01:16.300
a few edges one or one edge connected so
 

00:01:16.300 --> 00:01:19.020
a few edges one or one edge connected so
this is a graph that is not completely

00:01:19.020 --> 00:01:19.030
this is a graph that is not completely
 

00:01:19.030 --> 00:01:22.590
this is a graph that is not completely
disconnected but is very very nearly so

00:01:22.590 --> 00:01:22.600
disconnected but is very very nearly so
 

00:01:22.600 --> 00:01:27.780
disconnected but is very very nearly so
and so another fact that is interesting

00:01:27.780 --> 00:01:27.790
and so another fact that is interesting
 

00:01:27.790 --> 00:01:32.130
and so another fact that is interesting
is that mention the laplacian which is a

00:01:32.130 --> 00:01:32.140
is that mention the laplacian which is a
 

00:01:32.140 --> 00:01:34.219
is that mention the laplacian which is a
matrix that will define on this graph

00:01:34.219 --> 00:01:34.229
matrix that will define on this graph
 

00:01:34.229 --> 00:01:50.380
matrix that will define on this graph
which has a spectrum

00:01:50.380 --> 00:01:50.390
 
 

00:01:50.390 --> 00:01:53.649
 
as sine value that went from zero will

00:01:53.649 --> 00:01:53.659
as sine value that went from zero will
 

00:01:53.659 --> 00:01:55.500
as sine value that went from zero will
be the first eigenvalue all the way to

00:01:55.500 --> 00:01:55.510
be the first eigenvalue all the way to
 

00:01:55.510 --> 00:01:58.870
be the first eigenvalue all the way to
get the plug language Michael too it

00:01:58.870 --> 00:01:58.880
get the plug language Michael too it
 

00:01:58.880 --> 00:02:19.250
get the plug language Michael too it
turns out that

00:02:19.250 --> 00:02:19.260
 
 

00:02:19.260 --> 00:02:22.250
 
the second eigen value will be 0 if and

00:02:22.250 --> 00:02:22.260
the second eigen value will be 0 if and
 

00:02:22.260 --> 00:02:25.960
the second eigen value will be 0 if and
only if McGrath is disconnected so then

00:02:25.960 --> 00:02:25.970
only if McGrath is disconnected so then
 

00:02:25.970 --> 00:02:28.910
only if McGrath is disconnected so then
one might ask themselves is what if we

00:02:28.910 --> 00:02:28.920
one might ask themselves is what if we
 

00:02:28.920 --> 00:02:31.369
one might ask themselves is what if we
have very nearly discounting graph like

00:02:31.369 --> 00:02:31.379
have very nearly discounting graph like
 

00:02:31.379 --> 00:02:34.220
have very nearly discounting graph like
this might you have the technical you re

00:02:34.220 --> 00:02:34.230
this might you have the technical you re
 

00:02:34.230 --> 00:02:38.270
this might you have the technical you re
new year 0 but not quite is there like a

00:02:38.270 --> 00:02:38.280
new year 0 but not quite is there like a
 

00:02:38.280 --> 00:02:40.580
new year 0 but not quite is there like a
relation between the two it turns out

00:02:40.580 --> 00:02:40.590
relation between the two it turns out
 

00:02:40.590 --> 00:02:44.270
relation between the two it turns out
the thin effect is a form of achieving

00:02:44.270 --> 00:02:44.280
the thin effect is a form of achieving
 

00:02:44.280 --> 00:02:45.450
the thin effect is a form of achieving
equality

00:02:45.450 --> 00:02:45.460
equality
 

00:02:45.460 --> 00:02:57.559
equality
which is

00:02:57.559 --> 00:02:57.569
 
 

00:02:57.569 --> 00:02:59.690
 
so

00:02:59.690 --> 00:02:59.700
so
 

00:02:59.700 --> 00:03:02.390
so
this is the result of a key pretty much

00:03:02.390 --> 00:03:02.400
this is the result of a key pretty much
 

00:03:02.400 --> 00:03:03.950
this is the result of a key pretty much
started with

00:03:03.950 --> 00:03:03.960
started with
 

00:03:03.960 --> 00:03:08.010
started with
but then we realized something something

00:03:08.010 --> 00:03:08.020
but then we realized something something
 

00:03:08.020 --> 00:03:10.490
but then we realized something something
else that is true which is

00:03:10.490 --> 00:03:10.500
else that is true which is
 

00:03:10.500 --> 00:03:13.720
else that is true which is
it's brilliant a bipartite graphs so

00:03:13.720 --> 00:03:13.730
it's brilliant a bipartite graphs so
 

00:03:13.730 --> 00:03:17.210
it's brilliant a bipartite graphs so
bipartite graph is a graph where all the

00:03:17.210 --> 00:03:17.220
bipartite graph is a graph where all the
 

00:03:17.220 --> 00:03:20.840
bipartite graph is a graph where all the
notes can be separated into two two

00:03:20.840 --> 00:03:20.850
notes can be separated into two two
 

00:03:20.850 --> 00:03:23.540
notes can be separated into two two
different sets and all the edges in the

00:03:23.540 --> 00:03:23.550
different sets and all the edges in the
 

00:03:23.550 --> 00:03:25.850
different sets and all the edges in the
graph go between the two cents none

00:03:25.850 --> 00:03:25.860
graph go between the two cents none
 

00:03:25.860 --> 00:03:29.190
graph go between the two cents none
within yourself

00:03:29.190 --> 00:03:29.200
 
 

00:03:29.200 --> 00:03:30.930
 
this is an example of a bipartite graph

00:03:30.930 --> 00:03:30.940
this is an example of a bipartite graph
 

00:03:30.940 --> 00:03:32.760
this is an example of a bipartite graph
so those two sets here to be the

00:03:32.760 --> 00:03:32.770
so those two sets here to be the
 

00:03:32.770 --> 00:03:34.550
so those two sets here to be the
agreement degree notes the red nodes

00:03:34.550 --> 00:03:34.560
agreement degree notes the red nodes
 

00:03:34.560 --> 00:03:37.020
agreement degree notes the red nodes
there are no edges between green is

00:03:37.020 --> 00:03:37.030
there are no edges between green is
 

00:03:37.030 --> 00:03:40.020
there are no edges between green is
there no edges between threatens my part

00:03:40.020 --> 00:03:40.030
there no edges between threatens my part
 

00:03:40.030 --> 00:03:45.300
there no edges between threatens my part
I graphs are kind of important one of

00:03:45.300 --> 00:03:45.310
I graphs are kind of important one of
 

00:03:45.310 --> 00:03:48.240
I graphs are kind of important one of
the main reasons for protective diagrams

00:03:48.240 --> 00:03:48.250
the main reasons for protective diagrams
 

00:03:48.250 --> 00:03:48.860
the main reasons for protective diagrams
that

00:03:48.860 --> 00:03:48.870
that
 

00:03:48.870 --> 00:03:50.489
that
computationally speaking there's a

00:03:50.489 --> 00:03:50.499
computationally speaking there's a
 

00:03:50.499 --> 00:03:54.149
computationally speaking there's a
number of algorithms that just seemed it

00:03:54.149 --> 00:03:54.159
number of algorithms that just seemed it
 

00:03:54.159 --> 00:03:56.129
number of algorithms that just seemed it
work very well to wrap this by the car

00:03:56.129 --> 00:03:56.139
work very well to wrap this by the car
 

00:03:56.139 --> 00:04:01.440
work very well to wrap this by the car
tech but in any case again question on

00:04:01.440 --> 00:04:01.450
tech but in any case again question on
 

00:04:01.450 --> 00:04:05.160
tech but in any case again question on
my task is how bipartite is so even my

00:04:05.160 --> 00:04:05.170
my task is how bipartite is so even my
 

00:04:05.170 --> 00:04:07.140
my task is how bipartite is so even my
given graph is not completely bipartite

00:04:07.140 --> 00:04:07.150
given graph is not completely bipartite
 

00:04:07.150 --> 00:04:10.020
given graph is not completely bipartite
is in visiting close to a bit bipartite

00:04:10.020 --> 00:04:10.030
is in visiting close to a bit bipartite
 

00:04:10.030 --> 00:04:11.670
is in visiting close to a bit bipartite
graph right

00:04:11.670 --> 00:04:11.680
graph right
 

00:04:11.680 --> 00:04:29.140
graph right
so again we say this

00:04:29.140 --> 00:04:29.150
 
 

00:04:29.150 --> 00:04:33.070
 
so the end I valuably to is only at gsi

00:04:33.070 --> 00:04:33.080
so the end I valuably to is only at gsi
 

00:04:33.080 --> 00:04:37.660
so the end I valuably to is only at gsi
part a or lying a little bit here she

00:04:37.660 --> 00:04:37.670
part a or lying a little bit here she
 

00:04:37.670 --> 00:04:39.040
part a or lying a little bit here she
also has to be connected for this to be

00:04:39.040 --> 00:04:39.050
also has to be connected for this to be
 

00:04:39.050 --> 00:04:43.200
also has to be connected for this to be
true but the g's connected

00:04:43.200 --> 00:04:43.210
true but the g's connected
 

00:04:43.210 --> 00:04:46.320
true but the g's connected
like values to that you'll bipartite

00:04:46.320 --> 00:04:46.330
like values to that you'll bipartite
 

00:04:46.330 --> 00:04:48.330
like values to that you'll bipartite
privacy you can I kind of do the exact

00:04:48.330 --> 00:04:48.340
privacy you can I kind of do the exact
 

00:04:48.340 --> 00:04:51.059
privacy you can I kind of do the exact
same thing if I have a nearly bipartite

00:04:51.059 --> 00:04:51.069
same thing if I have a nearly bipartite
 

00:04:51.069 --> 00:04:57.689
same thing if I have a nearly bipartite
graph do I have Linda an equal to almost

00:04:57.689 --> 00:04:57.699
graph do I have Linda an equal to almost
 

00:04:57.699 --> 00:05:26.490
graph do I have Linda an equal to almost
two and so we have this

00:05:26.490 --> 00:05:26.500
 
 

00:05:26.500 --> 00:05:29.190
 
so

00:05:29.190 --> 00:05:29.200
 
 

00:05:29.200 --> 00:05:30.540
 
but the question right here because

00:05:30.540 --> 00:05:30.550
but the question right here because
 

00:05:30.550 --> 00:05:33.000
but the question right here because
we're not fit any purpose as strong

00:05:33.000 --> 00:05:33.010
we're not fit any purpose as strong
 

00:05:33.010 --> 00:05:34.850
we're not fit any purpose as strong
reason to believe this is true

00:05:34.850 --> 00:05:34.860
reason to believe this is true
 

00:05:34.860 --> 00:05:36.870
reason to believe this is true
fortunately the same group that means

00:05:36.870 --> 00:05:36.880
fortunately the same group that means
 

00:05:36.880 --> 00:05:39.330
fortunately the same group that means
despite there being a lot of symmetry

00:05:39.330 --> 00:05:39.340
despite there being a lot of symmetry
 

00:05:39.340 --> 00:05:42.120
despite there being a lot of symmetry
between these two cases Center

00:05:42.120 --> 00:05:42.130
between these two cases Center
 

00:05:42.130 --> 00:05:44.490
between these two cases Center
techniques do not quite carry over into

00:05:44.490 --> 00:05:44.500
techniques do not quite carry over into
 

00:05:44.500 --> 00:05:49.890
techniques do not quite carry over into
this case and modifying them to work for

00:05:49.890 --> 00:05:49.900
this case and modifying them to work for
 

00:05:49.900 --> 00:05:53.430
this case and modifying them to work for
this case sniffing the trigger than

00:05:53.430 --> 00:05:53.440
this case sniffing the trigger than
 

00:05:53.440 --> 00:06:00.250
this case sniffing the trigger than
expected nonetheless for

00:06:00.250 --> 00:06:00.260
expected nonetheless for
 

00:06:00.260 --> 00:06:04.080
expected nonetheless for
based on the analogy here and for given

00:06:04.080 --> 00:06:04.090
based on the analogy here and for given
 

00:06:04.090 --> 00:06:09.450
based on the analogy here and for given
numerous computational experiments we do

00:06:09.450 --> 00:06:09.460
numerous computational experiments we do
 

00:06:09.460 --> 00:06:11.490
numerous computational experiments we do
believe that this is true

00:06:11.490 --> 00:06:11.500
believe that this is true
 

00:06:11.500 --> 00:06:14.520
believe that this is true
happen this time we have so this

00:06:14.520 --> 00:06:14.530
happen this time we have so this
 

00:06:14.530 --> 00:06:17.850
happen this time we have so this
identical balance

00:06:17.850 --> 00:06:17.860
 
 

00:06:17.860 --> 00:06:22.190
 
so there is at least some relation

00:06:22.190 --> 00:06:22.200
 
 

00:06:22.200 --> 00:06:24.460
 
d by partek test

00:06:24.460 --> 00:06:24.470
d by partek test
 

00:06:24.470 --> 00:06:39.580
d by partek test
highest

00:06:39.580 --> 00:06:39.590
 
 

00:06:39.590 --> 00:06:44.020
 
so while status quo to about one

00:06:44.020 --> 00:06:44.030
so while status quo to about one
 

00:06:44.030 --> 00:06:49.230
so while status quo to about one
approach to

00:06:49.230 --> 00:06:49.240
 
 

00:06:49.240 --> 00:06:51.480
 
are we talking about page rank vectors

00:06:51.480 --> 00:06:51.490
are we talking about page rank vectors
 

00:06:51.490 --> 00:06:54.659
are we talking about page rank vectors
and basically there's nulls would have

00:06:54.659 --> 00:06:54.669
and basically there's nulls would have
 

00:06:54.669 --> 00:06:56.219
and basically there's nulls would have
approached to the graph clustering

00:06:56.219 --> 00:06:56.229
approached to the graph clustering
 

00:06:56.229 --> 00:06:59.960
approached to the graph clustering
problem and they're based on random box

00:06:59.960 --> 00:06:59.970
problem and they're based on random box
 

00:06:59.970 --> 00:07:03.170
problem and they're based on random box
so

00:07:03.170 --> 00:07:03.180
 
 

00:07:03.180 --> 00:07:32.950
 
it's important to talk about

00:07:32.950 --> 00:07:32.960
 
 

00:07:32.960 --> 00:07:39.730
 
so it was just some

00:07:39.730 --> 00:07:39.740
 
 

00:07:39.740 --> 00:07:42.909
 
the way around and walk words is started

00:07:42.909 --> 00:07:42.919
the way around and walk words is started
 

00:07:42.919 --> 00:07:48.550
the way around and walk words is started
some British

00:07:48.550 --> 00:07:48.560
 
 

00:07:48.560 --> 00:07:51.050
 
12

00:07:51.050 --> 00:07:51.060
12
 

00:07:51.060 --> 00:07:55.310
12
and with some probability right back at

00:07:55.310 --> 00:07:55.320
and with some probability right back at
 

00:07:55.320 --> 00:07:59.889
and with some probability right back at
you

00:07:59.889 --> 00:07:59.899
 
 

00:07:59.899 --> 00:08:05.230
 
fell apart

00:08:05.230 --> 00:08:05.240
 
 

00:08:05.240 --> 00:08:08.800
 
so good morning so now as you do random

00:08:08.800 --> 00:08:08.810
so good morning so now as you do random
 

00:08:08.810 --> 00:08:09.480
so good morning so now as you do random
walk

00:08:09.480 --> 00:08:09.490
walk
 

00:08:09.490 --> 00:08:17.030
walk
it'll keep track of the vertices that

00:08:17.030 --> 00:08:17.040
 
 

00:08:17.040 --> 00:08:19.999
 
so

00:08:19.999 --> 00:08:20.009
 
 

00:08:20.009 --> 00:08:25.129
 
fridge light like a real world example

00:08:25.129 --> 00:08:25.139
 
 

00:08:25.139 --> 00:08:27.619
 
Google let's explore exploited greater

00:08:27.619 --> 00:08:27.629
Google let's explore exploited greater
 

00:08:27.629 --> 00:08:34.610
Google let's explore exploited greater
works in one way by using them to

00:08:34.610 --> 00:08:34.620
 
 

00:08:34.620 --> 00:08:36.310
 
of their website sanders and

00:08:36.310 --> 00:08:36.320
of their website sanders and
 

00:08:36.320 --> 00:08:42.550
of their website sanders and
granted early stage it's ultimately the

00:08:42.550 --> 00:08:42.560
granted early stage it's ultimately the
 

00:08:42.560 --> 00:08:52.040
granted early stage it's ultimately the
pagerank vector is

00:08:52.040 --> 00:08:52.050
 
 

00:08:52.050 --> 00:08:53.700
 
um

00:08:53.700 --> 00:08:53.710
um
 

00:08:53.710 --> 00:09:22.820
um
with the vector probability within

00:09:22.820 --> 00:09:22.830
 
 

00:09:22.830 --> 00:09:25.570
 
so the way we can use payment vectors is

00:09:25.570 --> 00:09:25.580
so the way we can use payment vectors is
 

00:09:25.580 --> 00:09:28.550
so the way we can use payment vectors is
one we have to communicate I major and

00:09:28.550 --> 00:09:28.560
one we have to communicate I major and
 

00:09:28.560 --> 00:09:33.050
one we have to communicate I major and
two we have to look for vertices with

00:09:33.050 --> 00:09:33.060
two we have to look for vertices with
 

00:09:33.060 --> 00:09:36.830
two we have to look for vertices with
unusually high probability of being

00:09:36.830 --> 00:09:36.840
unusually high probability of being
 

00:09:36.840 --> 00:09:42.580
unusually high probability of being
visited and by and usually I mean that

00:09:42.580 --> 00:09:42.590
 
 

00:09:42.590 --> 00:09:46.100
 
the higher than we can expect by simply

00:09:46.100 --> 00:09:46.110
the higher than we can expect by simply
 

00:09:46.110 --> 00:09:50.990
the higher than we can expect by simply
looking at

00:09:50.990 --> 00:09:51.000
 
 

00:09:51.000 --> 00:09:56.180
 
so now what are the advantages of using

00:09:56.180 --> 00:09:56.190
so now what are the advantages of using
 

00:09:56.190 --> 00:09:59.160
so now what are the advantages of using
I

00:09:59.160 --> 00:09:59.170
 
 

00:09:59.170 --> 00:10:02.040
 
the one they're local which means that

00:10:02.040 --> 00:10:02.050
the one they're local which means that
 

00:10:02.050 --> 00:10:08.290
the one they're local which means that
you need only a small subset

00:10:08.290 --> 00:10:08.300
 
 

00:10:08.300 --> 00:10:11.259
 
part of interest

00:10:11.259 --> 00:10:11.269
part of interest
 

00:10:11.269 --> 00:10:13.519
part of interest
there are spread conversational class

00:10:13.519 --> 00:10:13.529
there are spread conversational class
 

00:10:13.529 --> 00:10:21.830
there are spread conversational class
because they're very

00:10:21.830 --> 00:10:21.840
 
 

00:10:21.840 --> 00:10:25.430
 
alright so a little talk a little bit

00:10:25.430 --> 00:10:25.440
alright so a little talk a little bit
 

00:10:25.440 --> 00:10:26.530
alright so a little talk a little bit
more

00:10:26.530 --> 00:10:26.540
more
 

00:10:26.540 --> 00:10:29.540
more
our analysis of bipartite graphs

00:10:29.540 --> 00:10:29.550
our analysis of bipartite graphs
 

00:10:29.550 --> 00:10:32.930
our analysis of bipartite graphs
typically in

00:10:32.930 --> 00:10:32.940
 
 

00:10:32.940 --> 00:10:35.519
 
instructions with the music that we've

00:10:35.519 --> 00:10:35.529
instructions with the music that we've
 

00:10:35.529 --> 00:10:55.290
instructions with the music that we've
been calling to rest

00:10:55.290 --> 00:10:55.300
 
 

00:10:55.300 --> 00:11:02.710
 
and

00:11:02.710 --> 00:11:02.720
 
 

00:11:02.720 --> 00:11:12.280
 
always go with so by partek this

00:11:12.280 --> 00:11:12.290
 
 

00:11:12.290 --> 00:11:19.360
 
two graphs in here is redone yeah

00:11:19.360 --> 00:11:19.370
 
 

00:11:19.370 --> 00:11:24.450
 
okay so um

00:11:24.450 --> 00:11:24.460
 
 

00:11:24.460 --> 00:11:26.260
 
what we're talking about kind of

00:11:26.260 --> 00:11:26.270
what we're talking about kind of
 

00:11:26.270 --> 00:11:28.150
what we're talking about kind of
clustering or finding almost

00:11:28.150 --> 00:11:28.160
clustering or finding almost
 

00:11:28.160 --> 00:11:30.910
clustering or finding almost
disconnected sets there's kind of an

00:11:30.910 --> 00:11:30.920
disconnected sets there's kind of an
 

00:11:30.920 --> 00:11:33.910
disconnected sets there's kind of an
immediate intuition as to why the

00:11:33.910 --> 00:11:33.920
immediate intuition as to why the
 

00:11:33.920 --> 00:11:35.740
immediate intuition as to why the
PageRank algorithm in random walks would

00:11:35.740 --> 00:11:35.750
PageRank algorithm in random walks would
 

00:11:35.750 --> 00:11:37.990
PageRank algorithm in random walks would
be useful for this and this is the idea

00:11:37.990 --> 00:11:38.000
be useful for this and this is the idea
 

00:11:38.000 --> 00:11:40.090
be useful for this and this is the idea
that if we started a vertex you and we

00:11:40.090 --> 00:11:40.100
that if we started a vertex you and we
 

00:11:40.100 --> 00:11:42.490
that if we started a vertex you and we
do a random traversal of the graph the

00:11:42.490 --> 00:11:42.500
do a random traversal of the graph the
 

00:11:42.500 --> 00:11:44.500
do a random traversal of the graph the
vertices were really likely to visit our

00:11:44.500 --> 00:11:44.510
vertices were really likely to visit our
 

00:11:44.510 --> 00:11:46.090
vertices were really likely to visit our
kind of the vertices that are in the

00:11:46.090 --> 00:11:46.100
kind of the vertices that are in the
 

00:11:46.100 --> 00:11:49.570
kind of the vertices that are in the
same cluster as our vertex you right so

00:11:49.570 --> 00:11:49.580
same cluster as our vertex you right so
 

00:11:49.580 --> 00:11:53.200
same cluster as our vertex you right so
in the sense if we do a page rank for

00:11:53.200 --> 00:11:53.210
in the sense if we do a page rank for
 

00:11:53.210 --> 00:11:55.720
in the sense if we do a page rank for
vertex u it will kind of give us a set

00:11:55.720 --> 00:11:55.730
vertex u it will kind of give us a set
 

00:11:55.730 --> 00:11:57.250
vertex u it will kind of give us a set
of vertices that were really likely to

00:11:57.250 --> 00:11:57.260
of vertices that were really likely to
 

00:11:57.260 --> 00:11:58.960
of vertices that were really likely to
visit and those should probably be the

00:11:58.960 --> 00:11:58.970
visit and those should probably be the
 

00:11:58.970 --> 00:12:00.820
visit and those should probably be the
vertices that are in the cluster

00:12:00.820 --> 00:12:00.830
vertices that are in the cluster
 

00:12:00.830 --> 00:12:04.090
vertices that are in the cluster
containing so the page rank kind of has

00:12:04.090 --> 00:12:04.100
containing so the page rank kind of has
 

00:12:04.100 --> 00:12:08.050
containing so the page rank kind of has
this immediately to ative thinking as to

00:12:08.050 --> 00:12:08.060
this immediately to ative thinking as to
 

00:12:08.060 --> 00:12:09.430
this immediately to ative thinking as to
why it would make sense that a

00:12:09.430 --> 00:12:09.440
why it would make sense that a
 

00:12:09.440 --> 00:12:11.170
why it would make sense that a
clustering problem or finding these

00:12:11.170 --> 00:12:11.180
clustering problem or finding these
 

00:12:11.180 --> 00:12:14.500
clustering problem or finding these
nearly disconnected sex unfortunately

00:12:14.500 --> 00:12:14.510
nearly disconnected sex unfortunately
 

00:12:14.510 --> 00:12:17.290
nearly disconnected sex unfortunately
there's not really a clear intuition as

00:12:17.290 --> 00:12:17.300
there's not really a clear intuition as
 

00:12:17.300 --> 00:12:19.420
there's not really a clear intuition as
to why the page rank and random walks

00:12:19.420 --> 00:12:19.430
to why the page rank and random walks
 

00:12:19.430 --> 00:12:21.610
to why the page rank and random walks
would translate really well to

00:12:21.610 --> 00:12:21.620
would translate really well to
 

00:12:21.620 --> 00:12:24.610
would translate really well to
understanding the bipartite graph so in

00:12:24.610 --> 00:12:24.620
understanding the bipartite graph so in
 

00:12:24.620 --> 00:12:26.890
understanding the bipartite graph so in
order to kind of translate our

00:12:26.890 --> 00:12:26.900
order to kind of translate our
 

00:12:26.900 --> 00:12:28.480
order to kind of translate our
understanding of the clustering problem

00:12:28.480 --> 00:12:28.490
understanding of the clustering problem
 

00:12:28.490 --> 00:12:30.290
understanding of the clustering problem
to where I understand

00:12:30.290 --> 00:12:30.300
to where I understand
 

00:12:30.300 --> 00:12:32.450
to where I understand
our debt problem we came up with this

00:12:32.450 --> 00:12:32.460
our debt problem we came up with this
 

00:12:32.460 --> 00:12:34.510
our debt problem we came up with this
construction that we call the to graph

00:12:34.510 --> 00:12:34.520
construction that we call the to graph
 

00:12:34.520 --> 00:12:37.670
construction that we call the to graph
so the way that we constructed in graft

00:12:37.670 --> 00:12:37.680
so the way that we constructed in graft
 

00:12:37.680 --> 00:12:42.370
so the way that we constructed in graft
is we start with some original right so

00:12:42.370 --> 00:12:42.380
is we start with some original right so
 

00:12:42.380 --> 00:12:45.170
is we start with some original right so
we'll look at this really simple one

00:12:45.170 --> 00:12:45.180
we'll look at this really simple one
 

00:12:45.180 --> 00:12:47.180
we'll look at this really simple one
here and then you can see the bigger

00:12:47.180 --> 00:12:47.190
here and then you can see the bigger
 

00:12:47.190 --> 00:12:49.790
here and then you can see the bigger
examples are so this is our original

00:12:49.790 --> 00:12:49.800
examples are so this is our original
 

00:12:49.800 --> 00:12:52.220
examples are so this is our original
graph and for it just has three vertices

00:12:52.220 --> 00:12:52.230
graph and for it just has three vertices
 

00:12:52.230 --> 00:12:55.730
graph and for it just has three vertices
and tricks for the two dropper we do is

00:12:55.730 --> 00:12:55.740
and tricks for the two dropper we do is
 

00:12:55.740 --> 00:12:58.100
and tricks for the two dropper we do is
which is a duplicate duplicate the same

00:12:58.100 --> 00:12:58.110
which is a duplicate duplicate the same
 

00:12:58.110 --> 00:13:00.680
which is a duplicate duplicate the same
set of vertices and we're going to draw

00:13:00.680 --> 00:13:00.690
set of vertices and we're going to draw
 

00:13:00.690 --> 00:13:03.260
set of vertices and we're going to draw
an edge between two vertices if there's

00:13:03.260 --> 00:13:03.270
an edge between two vertices if there's
 

00:13:03.270 --> 00:13:05.360
an edge between two vertices if there's
kind of a two-edged path between them in

00:13:05.360 --> 00:13:05.370
kind of a two-edged path between them in
 

00:13:05.370 --> 00:13:07.940
kind of a two-edged path between them in
the original graph so what that's going

00:13:07.940 --> 00:13:07.950
the original graph so what that's going
 

00:13:07.950 --> 00:13:10.550
the original graph so what that's going
to mean is in this original graph you

00:13:10.550 --> 00:13:10.560
to mean is in this original graph you
 

00:13:10.560 --> 00:13:12.650
to mean is in this original graph you
can see that we can get from vertex 1 2

00:13:12.650 --> 00:13:12.660
can see that we can get from vertex 1 2
 

00:13:12.660 --> 00:13:15.620
can see that we can get from vertex 1 2
3 X 3 by taking two steps you can go one

00:13:15.620 --> 00:13:15.630
3 X 3 by taking two steps you can go one
 

00:13:15.630 --> 00:13:18.350
3 X 3 by taking two steps you can go one
two two and two to three so in the to

00:13:18.350 --> 00:13:18.360
two two and two to three so in the to
 

00:13:18.360 --> 00:13:19.430
two two and two to three so in the to
graph what we're going to do is

00:13:19.430 --> 00:13:19.440
graph what we're going to do is
 

00:13:19.440 --> 00:13:21.350
graph what we're going to do is
going to draw vertex between one and

00:13:21.350 --> 00:13:21.360
going to draw vertex between one and
 

00:13:21.360 --> 00:13:25.430
going to draw vertex between one and
three we can also get from one to itself

00:13:25.430 --> 00:13:25.440
three we can also get from one to itself
 

00:13:25.440 --> 00:13:28.340
three we can also get from one to itself
by going from 1 a 2 and 2 back to one so

00:13:28.340 --> 00:13:28.350
by going from 1 a 2 and 2 back to one so
 

00:13:28.350 --> 00:13:31.730
by going from 1 a 2 and 2 back to one so
we can draw a self edge here and we saw

00:13:31.730 --> 00:13:31.740
we can draw a self edge here and we saw
 

00:13:31.740 --> 00:13:34.730
we can draw a self edge here and we saw
same way and we can get from 2 to itself

00:13:34.730 --> 00:13:34.740
same way and we can get from 2 to itself
 

00:13:34.740 --> 00:13:37.130
same way and we can get from 2 to itself
twice so we're going to start with

00:13:37.130 --> 00:13:37.140
twice so we're going to start with
 

00:13:37.140 --> 00:13:39.680
twice so we're going to start with
simple graphs where you can't have any

00:13:39.680 --> 00:13:39.690
simple graphs where you can't have any
 

00:13:39.690 --> 00:13:42.440
simple graphs where you can't have any
self edges and there will exist 1 edges

00:13:42.440 --> 00:13:42.450
self edges and there will exist 1 edges
 

00:13:42.450 --> 00:13:44.750
self edges and there will exist 1 edges
one edge uniting two pairs of vertices

00:13:44.750 --> 00:13:44.760
one edge uniting two pairs of vertices
 

00:13:44.760 --> 00:13:46.940
one edge uniting two pairs of vertices
and for the to graph we're going to end

00:13:46.940 --> 00:13:46.950
and for the to graph we're going to end
 

00:13:46.950 --> 00:13:50.480
and for the to graph we're going to end
up with I'm not simple graph that can't

00:13:50.480 --> 00:13:50.490
up with I'm not simple graph that can't
 

00:13:50.490 --> 00:13:53.990
up with I'm not simple graph that can't
have waited edges and sulfa news so this

00:13:53.990 --> 00:13:54.000
have waited edges and sulfa news so this
 

00:13:54.000 --> 00:13:55.220
have waited edges and sulfa news so this
is the to graph that we end up with

00:13:55.220 --> 00:13:55.230
is the to graph that we end up with
 

00:13:55.230 --> 00:13:58.040
is the to graph that we end up with
there's one edge between one to three an

00:13:58.040 --> 00:13:58.050
there's one edge between one to three an
 

00:13:58.050 --> 00:14:00.110
there's one edge between one to three an
edge between one in itself an edge

00:14:00.110 --> 00:14:00.120
edge between one in itself an edge
 

00:14:00.120 --> 00:14:01.790
edge between one in itself an edge
between three and itself and two edges

00:14:01.790 --> 00:14:01.800
between three and itself and two edges
 

00:14:01.800 --> 00:14:03.920
between three and itself and two edges
between two minute stuff and you can

00:14:03.920 --> 00:14:03.930
between two minute stuff and you can
 

00:14:03.930 --> 00:14:06.020
between two minute stuff and you can
imagine expanding this process to be in

00:14:06.020 --> 00:14:06.030
imagine expanding this process to be in
 

00:14:06.030 --> 00:14:11.580
imagine expanding this process to be in
chi regret so kind of

00:14:11.580 --> 00:14:11.590
 
 

00:14:11.590 --> 00:14:17.010
 
the intuition behind doing this and what

00:14:17.010 --> 00:14:17.020
the intuition behind doing this and what
 

00:14:17.020 --> 00:14:25.410
the intuition behind doing this and what
we ended up proving is this statement so

00:14:25.410 --> 00:14:25.420
we ended up proving is this statement so
 

00:14:25.420 --> 00:14:35.689
we ended up proving is this statement so
FG is connected to begin with then

00:14:35.689 --> 00:14:35.699
 
 

00:14:35.699 --> 00:14:39.199
 
these quantities are true and almost

00:14:39.199 --> 00:14:39.209
these quantities are true and almost
 

00:14:39.209 --> 00:14:41.269
these quantities are true and almost
kind of kind of like this game so we

00:14:41.269 --> 00:14:41.279
kind of kind of like this game so we
 

00:14:41.279 --> 00:14:44.809
kind of kind of like this game so we
have our G equals one if and only of hg

00:14:44.809 --> 00:14:44.819
have our G equals one if and only of hg
 

00:14:44.819 --> 00:14:48.979
have our G equals one if and only of hg
2 equals 0 RG is kind of a constant that

00:14:48.979 --> 00:14:48.989
2 equals 0 RG is kind of a constant that
 

00:14:48.989 --> 00:14:51.499
2 equals 0 RG is kind of a constant that
talks about how bipartite graph is it's

00:14:51.499 --> 00:14:51.509
talks about how bipartite graph is it's
 

00:14:51.509 --> 00:14:53.449
talks about how bipartite graph is it's
going to be a bipartite is going to be

00:14:53.449 --> 00:14:53.459
going to be a bipartite is going to be
 

00:14:53.459 --> 00:14:55.280
going to be a bipartite is going to be
one if the graph is bipartite and the

00:14:55.280 --> 00:14:55.290
one if the graph is bipartite and the
 

00:14:55.290 --> 00:14:56.629
one if the graph is bipartite and the
closer to being by part type is the

00:14:56.629 --> 00:14:56.639
closer to being by part type is the
 

00:14:56.639 --> 00:14:59.809
closer to being by part type is the
closer it is one and it should never be

00:14:59.809 --> 00:14:59.819
closer it is one and it should never be
 

00:14:59.819 --> 00:15:01.789
closer it is one and it should never be
zero but essentially for lower again so

00:15:01.789 --> 00:15:01.799
zero but essentially for lower again so
 

00:15:01.799 --> 00:15:03.739
zero but essentially for lower again so
the closer it gets to zero the further

00:15:03.739 --> 00:15:03.749
the closer it gets to zero the further
 

00:15:03.749 --> 00:15:05.629
the closer it gets to zero the further
away from being bipartite your graphics

00:15:05.629 --> 00:15:05.639
away from being bipartite your graphics
 

00:15:05.639 --> 00:15:09.619
away from being bipartite your graphics
a tree tree is a constant that kind of

00:15:09.619 --> 00:15:09.629
a tree tree is a constant that kind of
 

00:15:09.629 --> 00:15:11.210
a tree tree is a constant that kind of
talks about how close to being

00:15:11.210 --> 00:15:11.220
talks about how close to being
 

00:15:11.220 --> 00:15:12.799
talks about how close to being
disconnected your graph is if it's

00:15:12.799 --> 00:15:12.809
disconnected your graph is if it's
 

00:15:12.809 --> 00:15:14.809
disconnected your graph is if it's
really if it is zero it means your graph

00:15:14.809 --> 00:15:14.819
really if it is zero it means your graph
 

00:15:14.819 --> 00:15:17.150
really if it is zero it means your graph
is disconnected and it's close to zero

00:15:17.150 --> 00:15:17.160
is disconnected and it's close to zero
 

00:15:17.160 --> 00:15:18.559
is disconnected and it's close to zero
it means your graph is really close to

00:15:18.559 --> 00:15:18.569
it means your graph is really close to
 

00:15:18.569 --> 00:15:20.059
it means your graph is really close to
being disconnected it's really clustered

00:15:20.059 --> 00:15:20.069
being disconnected it's really clustered
 

00:15:20.069 --> 00:15:22.879
being disconnected it's really clustered
in a sense so what this tells us is that

00:15:22.879 --> 00:15:22.889
in a sense so what this tells us is that
 

00:15:22.889 --> 00:15:25.849
in a sense so what this tells us is that
our original graph would be bipartite if

00:15:25.849 --> 00:15:25.859
our original graph would be bipartite if
 

00:15:25.859 --> 00:15:30.349
our original graph would be bipartite if
and only if our to graph is completely

00:15:30.349 --> 00:15:30.359
and only if our to graph is completely
 

00:15:30.359 --> 00:15:33.309
and only if our to graph is completely
disconnected so we get kind of a graph

00:15:33.309 --> 00:15:33.319
disconnected so we get kind of a graph
 

00:15:33.319 --> 00:15:36.229
disconnected so we get kind of a graph
originally as part time as possible only

00:15:36.229 --> 00:15:36.239
originally as part time as possible only
 

00:15:36.239 --> 00:15:39.019
originally as part time as possible only
if our to graph is as clustered as

00:15:39.019 --> 00:15:39.029
if our to graph is as clustered as
 

00:15:39.029 --> 00:15:41.210
if our to graph is as clustered as
possible so we kind of see here our

00:15:41.210 --> 00:15:41.220
possible so we kind of see here our
 

00:15:41.220 --> 00:15:43.189
possible so we kind of see here our
relationship between by partek nests of

00:15:43.189 --> 00:15:43.199
relationship between by partek nests of
 

00:15:43.199 --> 00:15:46.999
relationship between by partek nests of
our original graph and the end the

00:15:46.999 --> 00:15:47.009
our original graph and the end the
 

00:15:47.009 --> 00:15:49.400
our original graph and the end the
cluster ability of our to graph so we

00:15:49.400 --> 00:15:49.410
cluster ability of our to graph so we
 

00:15:49.410 --> 00:15:50.960
cluster ability of our to graph so we
wanted to expand that relation because

00:15:50.960 --> 00:15:50.970
wanted to expand that relation because
 

00:15:50.970 --> 00:15:53.499
wanted to expand that relation because
this is only in a very specific case

00:15:53.499 --> 00:15:53.509
this is only in a very specific case
 

00:15:53.509 --> 00:15:57.769
this is only in a very specific case
because finding the best part debt set

00:15:57.769 --> 00:15:57.779
because finding the best part debt set
 

00:15:57.779 --> 00:16:00.169
because finding the best part debt set
for the best clustered set is an np-hard

00:16:00.169 --> 00:16:00.179
for the best clustered set is an np-hard
 

00:16:00.179 --> 00:16:02.210
for the best clustered set is an np-hard
problem which is very computationally

00:16:02.210 --> 00:16:02.220
problem which is very computationally
 

00:16:02.220 --> 00:16:04.700
problem which is very computationally
expensive so actually

00:16:04.700 --> 00:16:04.710
expensive so actually
 

00:16:04.710 --> 00:16:07.760
expensive so actually
finding these sets with the computers

00:16:07.760 --> 00:16:07.770
finding these sets with the computers
 

00:16:07.770 --> 00:16:10.010
finding these sets with the computers
very hard so generalizing our results to

00:16:10.010 --> 00:16:10.020
very hard so generalizing our results to
 

00:16:10.020 --> 00:16:12.950
very hard so generalizing our results to
kind of any subset is very helpful in

00:16:12.950 --> 00:16:12.960
kind of any subset is very helpful in
 

00:16:12.960 --> 00:16:18.620
kind of any subset is very helpful in
the sense and so in order to do that I'm

00:16:18.620 --> 00:16:18.630
the sense and so in order to do that I'm
 

00:16:18.630 --> 00:16:20.510
the sense and so in order to do that I'm
going to kind of introduce some notation

00:16:20.510 --> 00:16:20.520
going to kind of introduce some notation
 

00:16:20.520 --> 00:16:25.630
going to kind of introduce some notation
so we talked about size of us which is

00:16:25.630 --> 00:16:25.640
so we talked about size of us which is
 

00:16:25.640 --> 00:16:31.580
so we talked about size of us which is
kind of how bipartite

00:16:31.580 --> 00:16:31.590
 
 

00:16:31.590 --> 00:16:36.249
 
apartment is

00:16:36.249 --> 00:16:36.259
 
 

00:16:36.259 --> 00:16:39.519
 
of us has a very loose definition we

00:16:39.519 --> 00:16:39.529
of us has a very loose definition we
 

00:16:39.529 --> 00:16:41.619
of us has a very loose definition we
have something stricter to just kind of

00:16:41.619 --> 00:16:41.629
have something stricter to just kind of
 

00:16:41.629 --> 00:16:44.019
have something stricter to just kind of
course this is a measure we have a pipe

00:16:44.019 --> 00:16:44.029
course this is a measure we have a pipe
 

00:16:44.029 --> 00:16:46.629
course this is a measure we have a pipe
our tennis it works very much like this

00:16:46.629 --> 00:16:46.639
our tennis it works very much like this
 

00:16:46.639 --> 00:16:49.210
our tennis it works very much like this
measure it fits one it means that that's

00:16:49.210 --> 00:16:49.220
measure it fits one it means that that's
 

00:16:49.220 --> 00:16:52.569
measure it fits one it means that that's
a partition or subset of vertices that

00:16:52.569 --> 00:16:52.579
a partition or subset of vertices that
 

00:16:52.579 --> 00:16:54.939
a partition or subset of vertices that
acts as like a proper partition of the

00:16:54.939 --> 00:16:54.949
acts as like a proper partition of the
 

00:16:54.949 --> 00:16:57.729
acts as like a proper partition of the
graph into like a bipartite graph in

00:16:57.729 --> 00:16:57.739
graph into like a bipartite graph in
 

00:16:57.739 --> 00:17:00.789
graph into like a bipartite graph in
this case savings like s would be the

00:17:00.789 --> 00:17:00.799
this case savings like s would be the
 

00:17:00.799 --> 00:17:03.129
this case savings like s would be the
red vertices then this would be one

00:17:03.129 --> 00:17:03.139
red vertices then this would be one
 

00:17:03.139 --> 00:17:05.980
red vertices then this would be one
because we can see in this graph of the

00:17:05.980 --> 00:17:05.990
because we can see in this graph of the
 

00:17:05.990 --> 00:17:08.289
because we can see in this graph of the
green hardest seasonal repertories kind

00:17:08.289 --> 00:17:08.299
green hardest seasonal repertories kind
 

00:17:08.299 --> 00:17:10.899
green hardest seasonal repertories kind
of divided the graph intently bipartite

00:17:10.899 --> 00:17:10.909
of divided the graph intently bipartite
 

00:17:10.909 --> 00:17:14.799
of divided the graph intently bipartite
success and the lower this constant is

00:17:14.799 --> 00:17:14.809
success and the lower this constant is
 

00:17:14.809 --> 00:17:16.990
success and the lower this constant is
closer to zero the best buy parking and

00:17:16.990 --> 00:17:17.000
closer to zero the best buy parking and
 

00:17:17.000 --> 00:17:20.549
closer to zero the best buy parking and
subsidies and then we have this constant

00:17:20.549 --> 00:17:20.559
subsidies and then we have this constant
 

00:17:20.559 --> 00:17:29.990
subsidies and then we have this constant
pms which is kind of the cluster ability

00:17:29.990 --> 00:17:30.000
 
 

00:17:30.000 --> 00:17:31.670
 
of s

00:17:31.670 --> 00:17:31.680
of s
 

00:17:31.680 --> 00:17:34.400
of s
and it works just like this each

00:17:34.400 --> 00:17:34.410
and it works just like this each
 

00:17:34.410 --> 00:17:37.280
and it works just like this each
constant except for a general subset if

00:17:37.280 --> 00:17:37.290
constant except for a general subset if
 

00:17:37.290 --> 00:17:39.020
constant except for a general subset if
it's zero it means that set is

00:17:39.020 --> 00:17:39.030
it's zero it means that set is
 

00:17:39.030 --> 00:17:41.210
it's zero it means that set is
completely disconnected from the rest of

00:17:41.210 --> 00:17:41.220
completely disconnected from the rest of
 

00:17:41.220 --> 00:17:44.450
completely disconnected from the rest of
the graph and the closer to one it is or

00:17:44.450 --> 00:17:44.460
the graph and the closer to one it is or
 

00:17:44.460 --> 00:17:47.120
the graph and the closer to one it is or
the larger it gets the less disconnected

00:17:47.120 --> 00:17:47.130
the larger it gets the less disconnected
 

00:17:47.130 --> 00:17:48.620
the larger it gets the less disconnected
it is so the less of like its own

00:17:48.620 --> 00:17:48.630
it is so the less of like its own
 

00:17:48.630 --> 00:17:53.000
it is so the less of like its own
cluster it will be in the graph so what

00:17:53.000 --> 00:17:53.010
cluster it will be in the graph so what
 

00:17:53.010 --> 00:18:01.030
cluster it will be in the graph so what
we

00:18:01.030 --> 00:18:01.040
 
 

00:18:01.040 --> 00:18:05.720
 
improve on her sets that locally

00:18:05.720 --> 00:18:05.730
improve on her sets that locally
 

00:18:05.730 --> 00:18:09.220
improve on her sets that locally
maximize this measure of x park atmos is

00:18:09.220 --> 00:18:09.230
maximize this measure of x park atmos is
 

00:18:09.230 --> 00:18:18.770
maximize this measure of x park atmos is
that one minus sine of s squared is the

00:18:18.770 --> 00:18:18.780
that one minus sine of s squared is the
 

00:18:18.780 --> 00:18:20.190
that one minus sine of s squared is the
best

00:18:20.190 --> 00:18:20.200
best
 

00:18:20.200 --> 00:18:23.159
best
the giraffe or the number of vertices in

00:18:23.159 --> 00:18:23.169
the giraffe or the number of vertices in
 

00:18:23.169 --> 00:18:28.080
the giraffe or the number of vertices in
a graph over 2 times 5 20 s which is

00:18:28.080 --> 00:18:28.090
a graph over 2 times 5 20 s which is
 

00:18:28.090 --> 00:18:30.990
a graph over 2 times 5 20 s which is
kind of messy but essentially what it's

00:18:30.990 --> 00:18:31.000
kind of messy but essentially what it's
 

00:18:31.000 --> 00:18:34.710
kind of messy but essentially what it's
saying is that four sets that locally

00:18:34.710 --> 00:18:34.720
saying is that four sets that locally
 

00:18:34.720 --> 00:18:36.990
saying is that four sets that locally
maximize the resistance if their

00:18:36.990 --> 00:18:37.000
maximize the resistance if their
 

00:18:37.000 --> 00:18:40.740
maximize the resistance if their
conductance in the to graph is small it

00:18:40.740 --> 00:18:40.750
conductance in the to graph is small it
 

00:18:40.750 --> 00:18:42.149
conductance in the to graph is small it
means that their resistance in the

00:18:42.149 --> 00:18:42.159
means that their resistance in the
 

00:18:42.159 --> 00:18:44.460
means that their resistance in the
original graph needs to be fairly large

00:18:44.460 --> 00:18:44.470
original graph needs to be fairly large
 

00:18:44.470 --> 00:18:46.649
original graph needs to be fairly large
so we kind of have this immediate

00:18:46.649 --> 00:18:46.659
so we kind of have this immediate
 

00:18:46.659 --> 00:18:47.990
so we kind of have this immediate
relation that tells us that our

00:18:47.990 --> 00:18:48.000
relation that tells us that our
 

00:18:48.000 --> 00:18:51.060
relation that tells us that our
conductance in the to graph will give us

00:18:51.060 --> 00:18:51.070
conductance in the to graph will give us
 

00:18:51.070 --> 00:18:54.029
conductance in the to graph will give us
a good resistance in the original graph

00:18:54.029 --> 00:18:54.039
a good resistance in the original graph
 

00:18:54.039 --> 00:18:56.789
a good resistance in the original graph
for specific sex s what we want to

00:18:56.789 --> 00:18:56.799
for specific sex s what we want to
 

00:18:56.799 --> 00:19:00.060
for specific sex s what we want to
generalize this to any set because we

00:19:00.060 --> 00:19:00.070
generalize this to any set because we
 

00:19:00.070 --> 00:19:01.500
generalize this to any set because we
can't always guarantee that we're going

00:19:01.500 --> 00:19:01.510
can't always guarantee that we're going
 

00:19:01.510 --> 00:19:02.909
can't always guarantee that we're going
to find these sets if we're looking for

00:19:02.909 --> 00:19:02.919
to find these sets if we're looking for
 

00:19:02.919 --> 00:19:06.299
to find these sets if we're looking for
them so we're looking to prove an

00:19:06.299 --> 00:19:06.309
them so we're looking to prove an
 

00:19:06.309 --> 00:19:19.710
them so we're looking to prove an
inequality of this

00:19:19.710 --> 00:19:19.720
 
 

00:19:19.720 --> 00:19:22.529
 
and we're fairly confident that this or

00:19:22.529 --> 00:19:22.539
and we're fairly confident that this or
 

00:19:22.539 --> 00:19:24.299
and we're fairly confident that this or
something very similar is true and

00:19:24.299 --> 00:19:24.309
something very similar is true and
 

00:19:24.309 --> 00:19:28.230
something very similar is true and
essentially what this says is that if we

00:19:28.230 --> 00:19:28.240
essentially what this says is that if we
 

00:19:28.240 --> 00:19:31.500
essentially what this says is that if we
find a set in the to graph that has a

00:19:31.500 --> 00:19:31.510
find a set in the to graph that has a
 

00:19:31.510 --> 00:19:34.890
find a set in the to graph that has a
very good cluster ability it means that

00:19:34.890 --> 00:19:34.900
very good cluster ability it means that
 

00:19:34.900 --> 00:19:38.130
very good cluster ability it means that
we found a set that is either very

00:19:38.130 --> 00:19:38.140
we found a set that is either very
 

00:19:38.140 --> 00:19:41.399
we found a set that is either very
bipartite in the original graph or not

00:19:41.399 --> 00:19:41.409
bipartite in the original graph or not
 

00:19:41.409 --> 00:19:43.560
bipartite in the original graph or not
bipartite at all and garlic clustering

00:19:43.560 --> 00:19:43.570
bipartite at all and garlic clustering
 

00:19:43.570 --> 00:19:47.039
bipartite at all and garlic clustering
so the two graph kind of takes by partek

00:19:47.039 --> 00:19:47.049
so the two graph kind of takes by partek
 

00:19:47.049 --> 00:19:49.020
so the two graph kind of takes by partek
sets and it makes them very well cluster

00:19:49.020 --> 00:19:49.030
sets and it makes them very well cluster
 

00:19:49.030 --> 00:19:53.460
sets and it makes them very well cluster
in the imogen graph but it takes the bus

00:19:53.460 --> 00:19:53.470
in the imogen graph but it takes the bus
 

00:19:53.470 --> 00:19:55.380
in the imogen graph but it takes the bus
good sense and keeps them all clustered

00:19:55.380 --> 00:19:55.390
good sense and keeps them all clustered
 

00:19:55.390 --> 00:19:57.299
good sense and keeps them all clustered
as well so if we find a wall cost except

00:19:57.299 --> 00:19:57.309
as well so if we find a wall cost except
 

00:19:57.309 --> 00:19:59.279
as well so if we find a wall cost except
in the two craft we can't guarantee that

00:19:59.279 --> 00:19:59.289
in the two craft we can't guarantee that
 

00:19:59.289 --> 00:20:01.440
in the two craft we can't guarantee that
it was quite partite we kind of know

00:20:01.440 --> 00:20:01.450
it was quite partite we kind of know
 

00:20:01.450 --> 00:20:03.180
it was quite partite we kind of know
that it was either very close to being

00:20:03.180 --> 00:20:03.190
that it was either very close to being
 

00:20:03.190 --> 00:20:06.149
that it was either very close to being
bipartite or very close are very well

00:20:06.149 --> 00:20:06.159
bipartite or very close are very well
 

00:20:06.159 --> 00:20:10.140
bipartite or very close are very well
clustered to begin with so we have these

00:20:10.140 --> 00:20:10.150
clustered to begin with so we have these
 

00:20:10.150 --> 00:20:12.539
clustered to begin with so we have these
two results and we're looking to prove

00:20:12.539 --> 00:20:12.549
two results and we're looking to prove
 

00:20:12.549 --> 00:20:13.950
two results and we're looking to prove
this one still and that's kind of where

00:20:13.950 --> 00:20:13.960
this one still and that's kind of where
 

00:20:13.960 --> 00:20:15.120
this one still and that's kind of where
we are on

00:20:15.120 --> 00:20:15.130
we are on
 

00:20:15.130 --> 00:20:19.529
we are on
you're able travel to the tooth

00:20:19.529 --> 00:20:19.539
 
 

00:20:19.539 --> 00:20:22.919
 
ok so we've heard a lot about serious

00:20:22.919 --> 00:20:22.929
ok so we've heard a lot about serious
 

00:20:22.929 --> 00:20:24.899
ok so we've heard a lot about serious
i'm going to be talking about for the

00:20:24.899 --> 00:20:24.909
i'm going to be talking about for the
 

00:20:24.909 --> 00:20:29.580
i'm going to be talking about for the
algorithmic aspects so one question is

00:20:29.580 --> 00:20:29.590
algorithmic aspects so one question is
 

00:20:29.590 --> 00:20:32.430
algorithmic aspects so one question is
why even bother with algorithms because

00:20:32.430 --> 00:20:32.440
why even bother with algorithms because
 

00:20:32.440 --> 00:20:36.539
why even bother with algorithms because
for granny Brad it's true it's possible

00:20:36.539 --> 00:20:36.549
for granny Brad it's true it's possible
 

00:20:36.549 --> 00:20:38.969
for granny Brad it's true it's possible
to white interior defines the gear

00:20:38.969 --> 00:20:38.979
to white interior defines the gear
 

00:20:38.979 --> 00:20:43.080
to white interior defines the gear
constant which is the weight set that

00:20:43.080 --> 00:20:43.090
constant which is the weight set that
 

00:20:43.090 --> 00:20:46.830
constant which is the weight set that
has the lowest bound over volume ratio

00:20:46.830 --> 00:20:46.840
has the lowest bound over volume ratio
 

00:20:46.840 --> 00:20:49.799
has the lowest bound over volume ratio
but I've been 2 to the N hospital

00:20:49.799 --> 00:20:49.809
but I've been 2 to the N hospital
 

00:20:49.809 --> 00:20:52.589
but I've been 2 to the N hospital
subsets we're at with size and that you

00:20:52.589 --> 00:20:52.599
subsets we're at with size and that you
 

00:20:52.599 --> 00:20:54.629
subsets we're at with size and that you
have to be right over this is

00:20:54.629 --> 00:20:54.639
have to be right over this is
 

00:20:54.639 --> 00:20:57.299
have to be right over this is
impractical for large graphs so what we

00:20:57.299 --> 00:20:57.309
impractical for large graphs so what we
 

00:20:57.309 --> 00:21:01.460
impractical for large graphs so what we
did achieve with me the algorithms is

00:21:01.460 --> 00:21:01.470
did achieve with me the algorithms is
 

00:21:01.470 --> 00:21:06.330
did achieve with me the algorithms is
begin run times that are like the only

00:21:06.330 --> 00:21:06.340
begin run times that are like the only
 

00:21:06.340 --> 00:21:09.029
begin run times that are like the only
talking or hopefully close to linear and

00:21:09.029 --> 00:21:09.039
talking or hopefully close to linear and
 

00:21:09.039 --> 00:21:11.289
talking or hopefully close to linear and
the theory guarantees

00:21:11.289 --> 00:21:11.299
the theory guarantees
 

00:21:11.299 --> 00:21:12.639
the theory guarantees
results we get are not going to be very

00:21:12.639 --> 00:21:12.649
results we get are not going to be very
 

00:21:12.649 --> 00:21:15.999
results we get are not going to be very
far off from the optimum cut so let's

00:21:15.999 --> 00:21:16.009
far off from the optimum cut so let's
 

00:21:16.009 --> 00:21:21.220
far off from the optimum cut so let's
first talk about sex divide a tradition

00:21:21.220 --> 00:21:21.230
first talk about sex divide a tradition
 

00:21:21.230 --> 00:21:25.930
first talk about sex divide a tradition
you can graph into two subsets that have

00:21:25.930 --> 00:21:25.940
you can graph into two subsets that have
 

00:21:25.940 --> 00:21:28.789
you can graph into two subsets that have
boundary between them at maximum

00:21:28.789 --> 00:21:28.799
boundary between them at maximum
 

00:21:28.799 --> 00:21:29.550
boundary between them at maximum
internal

00:21:29.550 --> 00:21:29.560
internal
 

00:21:29.560 --> 00:21:33.760
internal
so we heard about I get vectors and page

00:21:33.760 --> 00:21:33.770
so we heard about I get vectors and page
 

00:21:33.770 --> 00:21:36.970
so we heard about I get vectors and page
rank vectors so if you take earlier so

00:21:36.970 --> 00:21:36.980
rank vectors so if you take earlier so
 

00:21:36.980 --> 00:21:40.480
rank vectors so if you take earlier so
if you take the eigenvector associated

00:21:40.480 --> 00:21:40.490
if you take the eigenvector associated
 

00:21:40.490 --> 00:21:44.980
if you take the eigenvector associated
to the second smallest eigenvalue or

00:21:44.980 --> 00:21:44.990
to the second smallest eigenvalue or
 

00:21:44.990 --> 00:21:47.470
to the second smallest eigenvalue or
patron factor what you'll do is you can

00:21:47.470 --> 00:21:47.480
patron factor what you'll do is you can
 

00:21:47.480 --> 00:21:49.480
patron factor what you'll do is you can
get an ordering of the vertices based on

00:21:49.480 --> 00:21:49.490
get an ordering of the vertices based on
 

00:21:49.490 --> 00:21:52.690
get an ordering of the vertices based on
this so if the third entry of the

00:21:52.690 --> 00:21:52.700
this so if the third entry of the
 

00:21:52.700 --> 00:21:55.810
this so if the third entry of the
specter say is the has the greatest

00:21:55.810 --> 00:21:55.820
specter say is the has the greatest
 

00:21:55.820 --> 00:21:58.690
specter say is the has the greatest
value then you'll rank the third vertex

00:21:58.690 --> 00:21:58.700
value then you'll rank the third vertex
 

00:21:58.700 --> 00:22:00.970
value then you'll rank the third vertex
first in your ordering the seventh

00:22:00.970 --> 00:22:00.980
first in your ordering the seventh
 

00:22:00.980 --> 00:22:03.130
first in your ordering the seventh
vertex has the second greatest century

00:22:03.130 --> 00:22:03.140
vertex has the second greatest century
 

00:22:03.140 --> 00:22:08.950
vertex has the second greatest century
will rank the SEC 73 x second so for

00:22:08.950 --> 00:22:08.960
will rank the SEC 73 x second so for
 

00:22:08.960 --> 00:22:30.480
will rank the SEC 73 x second so for
example supposing we have this graph

00:22:30.480 --> 00:22:30.490
 
 

00:22:30.490 --> 00:22:32.110
 
um

00:22:32.110 --> 00:22:32.120
um
 

00:22:32.120 --> 00:22:39.430
um
and we might shed this day

00:22:39.430 --> 00:22:39.440
 
 

00:22:39.440 --> 00:22:44.590
 
courtesy

00:22:44.590 --> 00:22:44.600
 
 

00:22:44.600 --> 00:22:49.090
 
it's either a drink for the eigenvector

00:22:49.090 --> 00:22:49.100
it's either a drink for the eigenvector
 

00:22:49.100 --> 00:22:52.250
it's either a drink for the eigenvector
and so now once we have this ordering

00:22:52.250 --> 00:22:52.260
and so now once we have this ordering
 

00:22:52.260 --> 00:22:54.740
and so now once we have this ordering
the problem is not linear because all we

00:22:54.740 --> 00:22:54.750
the problem is not linear because all we
 

00:22:54.750 --> 00:22:56.510
the problem is not linear because all we
have to do is integrate over what are

00:22:56.510 --> 00:22:56.520
have to do is integrate over what are
 

00:22:56.520 --> 00:22:58.610
have to do is integrate over what are
called a sweet set all the sweeps that's

00:22:58.610 --> 00:22:58.620
called a sweet set all the sweeps that's
 

00:22:58.620 --> 00:23:03.080
called a sweet set all the sweeps that's
our is the end sweet set is just the set

00:23:03.080 --> 00:23:03.090
our is the end sweet set is just the set
 

00:23:03.090 --> 00:23:08.600
our is the end sweet set is just the set
of the first n elements and the ordering

00:23:08.600 --> 00:23:08.610
of the first n elements and the ordering
 

00:23:08.610 --> 00:23:12.680
of the first n elements and the ordering
above so our first sweeps up would be

00:23:12.680 --> 00:23:12.690
above so our first sweeps up would be
 

00:23:12.690 --> 00:23:15.620
above so our first sweeps up would be
just this point and then it will be just

00:23:15.620 --> 00:23:15.630
just this point and then it will be just
 

00:23:15.630 --> 00:23:19.880
just this point and then it will be just
these two points 1 in 3 refers to 135

00:23:19.880 --> 00:23:19.890
these two points 1 in 3 refers to 135
 

00:23:19.890 --> 00:23:25.370
these two points 1 in 3 refers to 135
and so and as you can kind of see from

00:23:25.370 --> 00:23:25.380
and so and as you can kind of see from
 

00:23:25.380 --> 00:23:29.690
and so and as you can kind of see from
this poorly drawn grab the the best cut

00:23:29.690 --> 00:23:29.700
this poorly drawn grab the the best cut
 

00:23:29.700 --> 00:23:32.540
this poorly drawn grab the the best cut
will be right here because the boundary

00:23:32.540 --> 00:23:32.550
will be right here because the boundary
 

00:23:32.550 --> 00:23:35.660
will be right here because the boundary
is minimized and so what we're looking

00:23:35.660 --> 00:23:35.670
is minimized and so what we're looking
 

00:23:35.670 --> 00:23:39.560
is minimized and so what we're looking
to optimizes we want to minimize the

00:23:39.560 --> 00:23:39.570
to optimizes we want to minimize the
 

00:23:39.570 --> 00:23:44.060
to optimizes we want to minimize the
ratio of bounds over the volume of the

00:23:44.060 --> 00:23:44.070
ratio of bounds over the volume of the
 

00:23:44.070 --> 00:23:48.860
ratio of bounds over the volume of the
smaller and so that's very minimized

00:23:48.860 --> 00:23:48.870
smaller and so that's very minimized
 

00:23:48.870 --> 00:23:54.740
smaller and so that's very minimized
here so this is not quite linear for the

00:23:54.740 --> 00:23:54.750
here so this is not quite linear for the
 

00:23:54.750 --> 00:23:55.440
here so this is not quite linear for the
eigen

00:23:55.440 --> 00:23:55.450
eigen
 

00:23:55.450 --> 00:23:57.389
eigen
method because computing eigenvectors

00:23:57.389 --> 00:23:57.399
method because computing eigenvectors
 

00:23:57.399 --> 00:24:00.659
method because computing eigenvectors
takes on the order and cubed but this is

00:24:00.659 --> 00:24:00.669
takes on the order and cubed but this is
 

00:24:00.669 --> 00:24:02.960
takes on the order and cubed but this is
still a vast improvement over

00:24:02.960 --> 00:24:02.970
still a vast improvement over
 

00:24:02.970 --> 00:24:05.399
still a vast improvement over
exponential time and for the PageRank

00:24:05.399 --> 00:24:05.409
exponential time and for the PageRank
 

00:24:05.409 --> 00:24:07.860
exponential time and for the PageRank
process it is very close to linear and

00:24:07.860 --> 00:24:07.870
process it is very close to linear and
 

00:24:07.870 --> 00:24:10.450
process it is very close to linear and
in addition it's

00:24:10.450 --> 00:24:10.460
in addition it's
 

00:24:10.460 --> 00:24:13.149
in addition it's
very

00:24:13.149 --> 00:24:13.159
very
 

00:24:13.159 --> 00:24:17.529
very
and then the process is very similar

00:24:17.529 --> 00:24:17.539
and then the process is very similar
 

00:24:17.539 --> 00:24:22.369
and then the process is very similar
when we're trying to do try so this was

00:24:22.369 --> 00:24:22.379
when we're trying to do try so this was
 

00:24:22.379 --> 00:24:25.609
when we're trying to do try so this was
for finding sets that have minimum

00:24:25.609 --> 00:24:25.619
for finding sets that have minimum
 

00:24:25.619 --> 00:24:28.460
for finding sets that have minimum
boundary connections and maximal

00:24:28.460 --> 00:24:28.470
boundary connections and maximal
 

00:24:28.470 --> 00:24:30.080
boundary connections and maximal
internal connections but if we're doing

00:24:30.080 --> 00:24:30.090
internal connections but if we're doing
 

00:24:30.090 --> 00:24:32.710
internal connections but if we're doing
more of what pink was talking about

00:24:32.710 --> 00:24:32.720
more of what pink was talking about
 

00:24:32.720 --> 00:24:34.879
more of what pink was talking about
trying to find in two sets that have

00:24:34.879 --> 00:24:34.889
trying to find in two sets that have
 

00:24:34.889 --> 00:24:36.950
trying to find in two sets that have
that are maximally connected along the

00:24:36.950 --> 00:24:36.960
that are maximally connected along the
 

00:24:36.960 --> 00:24:38.659
that are maximally connected along the
boundary but have very few internal

00:24:38.659 --> 00:24:38.669
boundary but have very few internal
 

00:24:38.669 --> 00:24:41.690
boundary but have very few internal
connections the process is very

00:24:41.690 --> 00:24:41.700
connections the process is very
 

00:24:41.700 --> 00:24:45.049
connections the process is very
analogous to this so from k drink

00:24:45.049 --> 00:24:45.059
analogous to this so from k drink
 

00:24:45.059 --> 00:24:47.869
analogous to this so from k drink
instead of looking at the original graph

00:24:47.869 --> 00:24:47.879
instead of looking at the original graph
 

00:24:47.879 --> 00:24:50.749
instead of looking at the original graph
if you look at the two graph instead and

00:24:50.749 --> 00:24:50.759
if you look at the two graph instead and
 

00:24:50.759 --> 00:24:54.289
if you look at the two graph instead and
then you do this exact same process so

00:24:54.289 --> 00:24:54.299
then you do this exact same process so
 

00:24:54.299 --> 00:24:56.169
then you do this exact same process so
you use if you get the patron sector

00:24:56.169 --> 00:24:56.179
you use if you get the patron sector
 

00:24:56.179 --> 00:24:59.680
you use if you get the patron sector
looking at the two graph um and then you

00:24:59.680 --> 00:24:59.690
looking at the two graph um and then you
 

00:24:59.690 --> 00:25:02.899
looking at the two graph um and then you
try to minimize the bound over the

00:25:02.899 --> 00:25:02.909
try to minimize the bound over the
 

00:25:02.909 --> 00:25:04.720
try to minimize the bound over the
volume look again at the to graph um

00:25:04.720 --> 00:25:04.730
volume look again at the to graph um
 

00:25:04.730 --> 00:25:08.450
volume look again at the to graph um
then that will ensure that in the to rap

00:25:08.450 --> 00:25:08.460
then that will ensure that in the to rap
 

00:25:08.460 --> 00:25:11.239
then that will ensure that in the to rap
book two sets are very disconnected

00:25:11.239 --> 00:25:11.249
book two sets are very disconnected
 

00:25:11.249 --> 00:25:15.799
book two sets are very disconnected
which kind of suggests that the original

00:25:15.799 --> 00:25:15.809
which kind of suggests that the original
 

00:25:15.809 --> 00:25:17.779
which kind of suggests that the original
graph in the original graph the two sets

00:25:17.779 --> 00:25:17.789
graph in the original graph the two sets
 

00:25:17.789 --> 00:25:21.950
graph in the original graph the two sets
will be very common and you can also do

00:25:21.950 --> 00:25:21.960
will be very common and you can also do
 

00:25:21.960 --> 00:25:25.039
will be very common and you can also do
a similar method but using eigenvectors

00:25:25.039 --> 00:25:25.049
a similar method but using eigenvectors
 

00:25:25.049 --> 00:25:27.109
a similar method but using eigenvectors
but instead of using the second smallest

00:25:27.109 --> 00:25:27.119
but instead of using the second smallest
 

00:25:27.119 --> 00:25:29.509
but instead of using the second smallest
eigen vector of the original graph now

00:25:29.509 --> 00:25:29.519
eigen vector of the original graph now
 

00:25:29.519 --> 00:25:31.519
eigen vector of the original graph now
if you use the largest eigen vector of

00:25:31.519 --> 00:25:31.529
if you use the largest eigen vector of
 

00:25:31.529 --> 00:25:34.549
if you use the largest eigen vector of
the original graph you'll get an

00:25:34.549 --> 00:25:34.559
the original graph you'll get an
 

00:25:34.559 --> 00:25:39.649
the original graph you'll get an
ordering and then if you use the measure

00:25:39.649 --> 00:25:39.659
ordering and then if you use the measure
 

00:25:39.659 --> 00:25:42.950
ordering and then if you use the measure
of my part cut bipartite this which is

00:25:42.950 --> 00:25:42.960
of my part cut bipartite this which is
 

00:25:42.960 --> 00:25:46.190
of my part cut bipartite this which is
up there which redefines to be the bound

00:25:46.190 --> 00:25:46.200
up there which redefines to be the bound
 

00:25:46.200 --> 00:25:50.450
up there which redefines to be the bound
at the side tends to

00:25:50.450 --> 00:25:50.460
at the side tends to
 

00:25:50.460 --> 00:25:55.440
at the side tends to
all divided by our volume of the entire

00:25:55.440 --> 00:25:55.450
all divided by our volume of the entire
 

00:25:55.450 --> 00:26:00.509
all divided by our volume of the entire
graph and make cuts based on the largest

00:26:00.509 --> 00:26:00.519
graph and make cuts based on the largest
 

00:26:00.519 --> 00:26:05.850
graph and make cuts based on the largest
possible value that you get you get to

00:26:05.850 --> 00:26:05.860
possible value that you get you get to
 

00:26:05.860 --> 00:26:08.159
possible value that you get you get to
partition which guarantees or which

00:26:08.159 --> 00:26:08.169
partition which guarantees or which
 

00:26:08.169 --> 00:26:12.139
partition which guarantees or which
comes close to maximal bipartite this

00:26:12.139 --> 00:26:12.149
comes close to maximal bipartite this
 

00:26:12.149 --> 00:26:16.930
comes close to maximal bipartite this
yeah and so we

00:26:16.930 --> 00:26:16.940
yeah and so we
 

00:26:16.940 --> 00:26:20.350
yeah and so we
can we haven't like quite proven that

00:26:20.350 --> 00:26:20.360
can we haven't like quite proven that
 

00:26:20.360 --> 00:26:22.660
can we haven't like quite proven that
this works for all sets but the computer

00:26:22.660 --> 00:26:22.670
this works for all sets but the computer
 

00:26:22.670 --> 00:26:27.250
this works for all sets but the computer
experiments are very promising so as you

00:26:27.250 --> 00:26:27.260
experiments are very promising so as you
 

00:26:27.260 --> 00:26:30.430
experiments are very promising so as you
go farther I guess for you guys it's

00:26:30.430 --> 00:26:30.440
go farther I guess for you guys it's
 

00:26:30.440 --> 00:26:34.480
go farther I guess for you guys it's
right the graph is more bipartite which

00:26:34.480 --> 00:26:34.490
right the graph is more bipartite which
 

00:26:34.490 --> 00:26:36.490
right the graph is more bipartite which
just means that we increase the

00:26:36.490 --> 00:26:36.500
just means that we increase the
 

00:26:36.500 --> 00:26:39.670
just means that we increase the
probability of connections along the

00:26:39.670 --> 00:26:39.680
probability of connections along the
 

00:26:39.680 --> 00:26:45.250
probability of connections along the
boundary and you notice that twice the

00:26:45.250 --> 00:26:45.260
boundary and you notice that twice the
 

00:26:45.260 --> 00:26:47.260
boundary and you notice that twice the
maximum boundary of the sweep set

00:26:47.260 --> 00:26:47.270
maximum boundary of the sweep set
 

00:26:47.270 --> 00:26:49.540
maximum boundary of the sweep set
divided by the volume of G which is just

00:26:49.540 --> 00:26:49.550
divided by the volume of G which is just
 

00:26:49.550 --> 00:26:52.060
divided by the volume of G which is just
the bipartite this measure gets closer

00:26:52.060 --> 00:26:52.070
the bipartite this measure gets closer
 

00:26:52.070 --> 00:26:54.910
the bipartite this measure gets closer
and closer to one if the graph is

00:26:54.910 --> 00:26:54.920
and closer to one if the graph is
 

00:26:54.920 --> 00:26:57.910
and closer to one if the graph is
completely bipartite we'd expect the

00:26:57.910 --> 00:26:57.920
completely bipartite we'd expect the
 

00:26:57.920 --> 00:27:00.810
completely bipartite we'd expect the
boundary to be exactly half the volume

00:27:00.810 --> 00:27:00.820
boundary to be exactly half the volume
 

00:27:00.820 --> 00:27:03.730
boundary to be exactly half the volume
so this looks very promising the trend

00:27:03.730 --> 00:27:03.740
so this looks very promising the trend
 

00:27:03.740 --> 00:27:09.700
so this looks very promising the trend
is very there's a very clear trend and

00:27:09.700 --> 00:27:09.710
is very there's a very clear trend and
 

00:27:09.710 --> 00:27:14.500
is very there's a very clear trend and
similarly for the bike for the PageRank

00:27:14.500 --> 00:27:14.510
similarly for the bike for the PageRank
 

00:27:14.510 --> 00:27:17.350
similarly for the bike for the PageRank
algorithm on the to graph you notice

00:27:17.350 --> 00:27:17.360
algorithm on the to graph you notice
 

00:27:17.360 --> 00:27:20.080
algorithm on the to graph you notice
that now it's getting smaller but that's

00:27:20.080 --> 00:27:20.090
that now it's getting smaller but that's
 

00:27:20.090 --> 00:27:21.250
that now it's getting smaller but that's
because we're looking at the minimum

00:27:21.250 --> 00:27:21.260
because we're looking at the minimum
 

00:27:21.260 --> 00:27:24.580
because we're looking at the minimum
Keuka ratio so as the graph becomes more

00:27:24.580 --> 00:27:24.590
Keuka ratio so as the graph becomes more
 

00:27:24.590 --> 00:27:30.880
Keuka ratio so as the graph becomes more
bipartite then on the bound of the two

00:27:30.880 --> 00:27:30.890
bipartite then on the bound of the two
 

00:27:30.890 --> 00:27:34.300
bipartite then on the bound of the two
graph divided by the volume will get

00:27:34.300 --> 00:27:34.310
graph divided by the volume will get
 

00:27:34.310 --> 00:27:38.980
graph divided by the volume will get
closer and closer to zero and so these

00:27:38.980 --> 00:27:38.990
closer and closer to zero and so these
 

00:27:38.990 --> 00:27:43.540
closer and closer to zero and so these
things both indicate that are what we're

00:27:43.540 --> 00:27:43.550
things both indicate that are what we're
 

00:27:43.550 --> 00:27:49.890
things both indicate that are what we're
trying to prove looks promising

00:27:49.890 --> 00:27:49.900
 
 

00:27:49.900 --> 00:27:53.820
 
I think that's okay acid but we learn to

00:27:53.820 --> 00:27:53.830
I think that's okay acid but we learn to
 

00:27:53.830 --> 00:27:55.400
I think that's okay acid but we learn to
think they probably Oh

00:27:55.400 --> 00:27:55.410
think they probably Oh
 

00:27:55.410 --> 00:27:58.380
think they probably Oh
follow thomas or better focus all this

00:27:58.380 --> 00:27:58.390
follow thomas or better focus all this
 

00:27:58.390 --> 00:28:10.060
follow thomas or better focus all this
research

00:28:10.060 --> 00:28:10.070
 
 

00:28:10.070 --> 00:28:14.480
 
relationship between I event I a vector

00:28:14.480 --> 00:28:14.490
relationship between I event I a vector
 

00:28:14.490 --> 00:28:16.159
relationship between I event I a vector
say

00:28:16.159 --> 00:28:16.169
say
 

00:28:16.169 --> 00:28:20.910
say
classical classical

00:28:20.910 --> 00:28:20.920
 
 

00:28:20.920 --> 00:28:22.860
 
just between

00:28:22.860 --> 00:28:22.870
just between
 

00:28:22.870 --> 00:28:25.290
just between
so so is it you had two different you

00:28:25.290 --> 00:28:25.300
so so is it you had two different you
 

00:28:25.300 --> 00:28:26.940
so so is it you had two different you
have two different vectors but again

00:28:26.940 --> 00:28:26.950
have two different vectors but again
 

00:28:26.950 --> 00:28:30.259
have two different vectors but again
this page rank danger and an item 10 30

00:28:30.259 --> 00:28:30.269
this page rank danger and an item 10 30
 

00:28:30.269 --> 00:28:31.680
this page rank danger and an item 10 30
lucasi a mattress

00:28:31.680 --> 00:28:31.690
lucasi a mattress
 

00:28:31.690 --> 00:28:34.510
lucasi a mattress
they're election and listen

00:28:34.510 --> 00:28:34.520
they're election and listen
 

00:28:34.520 --> 00:28:40.130
they're election and listen
so they're like computationally like a

00:28:40.130 --> 00:28:40.140
so they're like computationally like a
 

00:28:40.140 --> 00:28:41.250
so they're like computationally like a
dependent

00:28:41.250 --> 00:28:41.260
dependent
 

00:28:41.260 --> 00:28:45.410
dependent
but they're they're both similar comment

00:28:45.410 --> 00:28:45.420
but they're they're both similar comment
 

00:28:45.420 --> 00:28:48.750
but they're they're both similar comment
so they're just differently the random

00:28:48.750 --> 00:28:48.760
so they're just differently the random
 

00:28:48.760 --> 00:28:51.480
so they're just differently the random
walk matrix is actually similar to the

00:28:51.480 --> 00:28:51.490
walk matrix is actually similar to the
 

00:28:51.490 --> 00:28:53.910
walk matrix is actually similar to the
laplacian so if you're looking at the

00:28:53.910 --> 00:28:53.920
laplacian so if you're looking at the
 

00:28:53.920 --> 00:28:55.500
laplacian so if you're looking at the
spectrums they have a lot of like

00:28:55.500 --> 00:28:55.510
spectrums they have a lot of like
 

00:28:55.510 --> 00:28:57.270
spectrums they have a lot of like
relationships and similarities but

00:28:57.270 --> 00:28:57.280
relationships and similarities but
 

00:28:57.280 --> 00:29:03.030
relationships and similarities but
computationally the mathematical seminar

00:29:03.030 --> 00:29:03.040
computationally the mathematical seminar
 

00:29:03.040 --> 00:29:06.180
computationally the mathematical seminar
not like maple yeah yeah actually

00:29:06.180 --> 00:29:06.190
not like maple yeah yeah actually
 

00:29:06.190 --> 00:29:12.510
not like maple yeah yeah actually
similar up to my head to the ad

