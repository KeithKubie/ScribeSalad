WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.318
[MUSIC PLAYING]

00:00:06.318 --> 00:00:08.550
MALE SPEAKER: Shanghai
GDG is a very

00:00:08.550 --> 00:00:10.350
interesting developer community.

00:00:10.350 --> 00:00:11.470
FEMALE SPEAKER: I'm
glad somebody

00:00:11.470 --> 00:00:12.315
has asked this question.

00:00:12.315 --> 00:00:14.010
MALE SPEAKER: This is where
the magic happens.

00:00:14.010 --> 00:00:15.200
FEMALE SPEAKER: This is
primarily a question and

00:00:15.200 --> 00:00:16.840
answer show, so if any
of you out there

00:00:16.840 --> 00:00:18.750
would like to ask questions.

00:00:22.705 --> 00:00:23.940
RYAN BOYD: Hi, everyone.

00:00:23.940 --> 00:00:27.510
My name is Ryan Boyd, and I'm a
developer advocate at Google

00:00:27.510 --> 00:00:31.050
on our cloud data services
such as Google Big Query.

00:00:31.050 --> 00:00:31.580
MICHAEL MANOOCHEHRI: And, hi.

00:00:31.580 --> 00:00:32.720
I'm Michael Manoochehri.

00:00:32.720 --> 00:00:34.590
I'm a developer programs
engineer working with the

00:00:34.590 --> 00:00:35.645
BigQuery product.

00:00:35.645 --> 00:00:37.710
RYAN BOYD: And today, we're
going to talk about best

00:00:37.710 --> 00:00:40.560
practices for querying
massive data sets.

00:00:40.560 --> 00:00:44.110
This is a video that's
particular to the querying

00:00:44.110 --> 00:00:45.290
aspect of BigQuery.

00:00:45.290 --> 00:00:49.010
We have other videos that talk
about BigQuery as an overview,

00:00:49.010 --> 00:00:52.280
and how the API works, and
how to get your data into

00:00:52.280 --> 00:00:55.360
BigQuery, but today we're going
to really concentrate on

00:00:55.360 --> 00:00:57.390
querying BigQuery.

00:00:57.390 --> 00:00:59.840
If you have any questions after
watching this video,

00:00:59.840 --> 00:01:03.360
feel free to reach out to
us on Stack Overflow.

00:01:03.360 --> 00:01:06.750
There's a google-bigquery tag
there on Stack Overflow, and

00:01:06.750 --> 00:01:09.430
ask your questions, and
we'll be happy to jump

00:01:09.430 --> 00:01:10.400
in and answer them.

00:01:10.400 --> 00:01:14.320
We also have our Google+
profiles, as well as our

00:01:14.320 --> 00:01:17.290
Twitter handles on this slide
if you have any followup

00:01:17.290 --> 00:01:20.190
questions that you want to ask
to us individually, maybe, to

00:01:20.190 --> 00:01:24.560
clarify what one of us said
during the episode here today.

00:01:24.560 --> 00:01:24.930
All right.

00:01:24.930 --> 00:01:29.090
Before diving into some example
queries with BigQuery,

00:01:29.090 --> 00:01:33.600
I want to actually show you the
way that BigQuery works,

00:01:33.600 --> 00:01:37.260
the way that BigQuery processes
its queries.

00:01:37.260 --> 00:01:40.410
So let's jump over to
this slide here.

00:01:40.410 --> 00:01:43.690
And this is basically the
BigQuery processing tree.

00:01:43.690 --> 00:01:46.450
This is how BigQuery queries
are processed.

00:01:46.450 --> 00:01:48.640
And I'm going to talk a little
bit about how this is

00:01:48.640 --> 00:01:53.270
different between BigQuery and a
typical relational database.

00:01:53.270 --> 00:01:55.630
So a typical relational
database, you load your data

00:01:55.630 --> 00:01:58.330
in, and the relational
database makes an

00:01:58.330 --> 00:02:00.050
index of that data.

00:02:00.050 --> 00:02:04.380
And then any time that you're
performing simple queries that

00:02:04.380 --> 00:02:07.020
are easy to select from that
index, a relational database

00:02:07.020 --> 00:02:08.330
is fairly quick.

00:02:08.330 --> 00:02:12.040
But if you're trying to do
really aggregate queries over

00:02:12.040 --> 00:02:16.170
those large data sets, the
indexes oftentimes

00:02:16.170 --> 00:02:17.480
won't find a match.

00:02:17.480 --> 00:02:19.920
The aggregates aren't stored
in the index, especially if

00:02:19.920 --> 00:02:22.410
they're more complicated
aggregates or over a

00:02:22.410 --> 00:02:25.930
sub-selection of the data,
so it needs to do

00:02:25.930 --> 00:02:27.640
a full table scan.

00:02:27.640 --> 00:02:30.680
And a relational database doing
a full table scan is

00:02:30.680 --> 00:02:35.250
sort of like the heart
attack for the DBA.

00:02:35.250 --> 00:02:37.680
DBAs hate the idea of full table
scans, because they're

00:02:37.680 --> 00:02:41.450
really, really slow when doing
it on a relational database

00:02:41.450 --> 00:02:43.340
that's typically hosted
on a single server.

00:02:43.340 --> 00:02:47.380
There's only so much I/O
bandwidth for that server, and

00:02:47.380 --> 00:02:49.400
so you have really
bad performance.

00:02:49.400 --> 00:02:52.780
And BigQuery is actually the
absolute opposite of this.

00:02:52.780 --> 00:02:56.360
We embrace the concept of full
table scans and allow you to

00:02:56.360 --> 00:03:01.320
query over terabytes of
data in just seconds.

00:03:01.320 --> 00:03:04.460
So we're going to show you some
of those big queries here

00:03:04.460 --> 00:03:06.730
soon, but first I want
to really give you an

00:03:06.730 --> 00:03:10.560
understanding of what happens
underneath the covers when

00:03:10.560 --> 00:03:13.120
BigQuery processes
its queries, for

00:03:13.120 --> 00:03:14.180
those geeks out there.

00:03:14.180 --> 00:03:16.910
If you don't care to know about
some of this stuff, you

00:03:16.910 --> 00:03:17.640
don't have to.

00:03:17.640 --> 00:03:19.440
That's the awesomeness
of BigQuery

00:03:19.440 --> 00:03:21.960
being a web API service.

00:03:21.960 --> 00:03:24.610
It's really easy to use
without knowing this.

00:03:24.610 --> 00:03:28.360
But knowing some of this helps
your geek inner heart, but

00:03:28.360 --> 00:03:32.930
also it makes it a little bit
easier to understand how you

00:03:32.930 --> 00:03:35.300
can optimize your queries.

00:03:35.300 --> 00:03:37.000
So this is the tree structure.

00:03:37.000 --> 00:03:40.470
Again, very similar to like
a relational database.

00:03:40.470 --> 00:03:44.400
And in this tree structure, we
basically have the storage at

00:03:44.400 --> 00:03:51.340
the bottom, and each leaf node
in this tree is basically

00:03:51.340 --> 00:03:56.030
assigned to deal with a very
small subset of your data from

00:03:56.030 --> 00:03:57.640
that distributed storage.

00:03:57.640 --> 00:04:00.890
So think about something on the
order of tens of megabytes

00:04:00.890 --> 00:04:04.050
of data that each leaf node
is reading off of that

00:04:04.050 --> 00:04:05.240
distributed storage.

00:04:05.240 --> 00:04:08.650
Now, the distributed storage
is distributed, as its name

00:04:08.650 --> 00:04:13.000
implies, so it basically has a
lot of different disks, a lot

00:04:13.000 --> 00:04:14.590
of I/O bandwidth.

00:04:14.590 --> 00:04:17.610
So each leaf code can
simultaneously, in parallel,

00:04:17.610 --> 00:04:21.120
read off this distributed
storage really quickly.

00:04:21.120 --> 00:04:25.220
And they're loading in the data
that they are tasked with

00:04:25.220 --> 00:04:25.770
dealing with.

00:04:25.770 --> 00:04:29.040
So think about the leaf on
the left is dealing with

00:04:29.040 --> 00:04:32.600
essentially rows 1 through 10,
and then the next leaf over is

00:04:32.600 --> 00:04:36.430
dealing with rows 11 through
20, or something like that.

00:04:36.430 --> 00:04:39.990
So each can independently
process its own a chunk of

00:04:39.990 --> 00:04:43.880
data and do the aggregate
queries on its

00:04:43.880 --> 00:04:45.200
own chunk of data.

00:04:45.200 --> 00:04:47.600
So you think about something
like a count query, so the

00:04:47.600 --> 00:04:50.700
first leaf would count
the relevant rows.

00:04:50.700 --> 00:04:53.700
The second leaf would count its
relevant rows, and then

00:04:53.700 --> 00:04:57.240
basically it does this partial
reduction as it moves up the

00:04:57.240 --> 00:05:00.570
tree where the mixer above it
would add those two counts,

00:05:00.570 --> 00:05:03.970
would sum those two counts
together, and that keeps on

00:05:03.970 --> 00:05:07.090
going up the tree until you
get the final result.

00:05:07.090 --> 00:05:09.930
Now, one thing that I really
should emphasize here--

00:05:09.930 --> 00:05:13.890
actually two things-- is one,
our reading from the disk with

00:05:13.890 --> 00:05:17.720
BigQuery is only reading the
columns that you're accessing.

00:05:17.720 --> 00:05:21.290
This reduces the cost to us to
run the queries, and thus

00:05:21.290 --> 00:05:23.110
reduces the cost to you.

00:05:23.110 --> 00:05:25.790
And this is all because of the
column or data storage, so we

00:05:25.790 --> 00:05:30.240
only need to read the set of
data that's in the columns

00:05:30.240 --> 00:05:31.400
that we're touching.

00:05:31.400 --> 00:05:33.940
The second thing I should note
is that this is really

00:05:33.940 --> 00:05:36.760
different from like a MapReduce
based system which

00:05:36.760 --> 00:05:39.300
people often try--

00:05:39.300 --> 00:05:40.040
frustratingly--

00:05:40.040 --> 00:05:46.070
to compare to BigQuery, and one
of the big ways that it's

00:05:46.070 --> 00:05:50.060
different is that we are only
reading from disk at the very

00:05:50.060 --> 00:05:52.280
beginning of this
processing tree.

00:05:52.280 --> 00:05:55.720
We're not actually reading and
writing from disk constantly

00:05:55.720 --> 00:05:58.320
throughout this operation,
which is what a typical

00:05:58.320 --> 00:06:00.080
MapReduce operation will do.

00:06:00.080 --> 00:06:03.040
MapReduce, in between the map,
shuffle, and reduce steps will

00:06:03.040 --> 00:06:07.100
often write and read a lot of
data from disk, and disks are

00:06:07.100 --> 00:06:10.400
the slowest point in
this whole tree.

00:06:10.400 --> 00:06:13.650
And also, with a typical
MapReduce operation, you're

00:06:13.650 --> 00:06:15.990
going to be performing multiple
map steps, multiple

00:06:15.990 --> 00:06:18.780
reduce steps, and that
just gets really

00:06:18.780 --> 00:06:20.370
heavy on those disks.

00:06:20.370 --> 00:06:23.750
Whereas with BigQuery, it's able
to take advantage of the

00:06:23.750 --> 00:06:28.440
super fast network bandwidth
that BigQuery has to offer.

00:06:28.440 --> 00:06:29.020
All right.

00:06:29.020 --> 00:06:33.240
So that's kind of covering the
true geek side of BigQuery,

00:06:33.240 --> 00:06:34.700
but now let's show
this in action.

00:06:34.700 --> 00:06:38.700
Let's show what happens when you
run a query and see some

00:06:38.700 --> 00:06:41.800
queries over very
large data sets.

00:06:41.800 --> 00:06:44.790
So the first table that I'm
going to query over here is

00:06:44.790 --> 00:06:50.420
basically all of the page view
data from Wikipedia for a

00:06:50.420 --> 00:06:51.680
particular month.

00:06:51.680 --> 00:06:56.020
So what each of the rows in this
table represents is an

00:06:56.020 --> 00:07:03.270
hour-by-hour look at all of the
Wikipedia page views that

00:07:03.270 --> 00:07:06.940
happen in that hour, grouped
by the title.

00:07:06.940 --> 00:07:10.160
So in this case here, we
actually have the Wikimedia

00:07:10.160 --> 00:07:12.170
projects, so this actually
includes stuff other than

00:07:12.170 --> 00:07:15.020
Wikipedia, but all the Wikimedia
project, and the

00:07:15.020 --> 00:07:18.560
language of the page, and then
the title of the page and how

00:07:18.560 --> 00:07:22.730
many views it had, as well as
some bites transferred data.

00:07:22.730 --> 00:07:28.490
And as Michael we would say,
there's a long tail here.

00:07:28.490 --> 00:07:29.760
There's an extreme long tail.

00:07:29.760 --> 00:07:32.300
There's a lot of pages that are
just viewed once an hour

00:07:32.300 --> 00:07:33.590
or things like that.

00:07:33.590 --> 00:07:35.300
MICHAEL MANOOCHEHRI: But all
together, this table size,

00:07:35.300 --> 00:07:37.680
it's a terabyte per month, and
we're looking at something

00:07:37.680 --> 00:07:40.470
like 650 million rows
for an entire month.

00:07:40.470 --> 00:07:41.970
RYAN BOYD: Actually, I
think that row count

00:07:41.970 --> 00:07:43.220
is off here a second.

00:07:43.220 --> 00:07:44.430
We have a bug in our UI.

00:07:44.430 --> 00:07:45.940
This is actually 13
billion rows.

00:07:45.940 --> 00:07:46.550
MICHAEL MANOOCHEHRI:
Oh, 13 billion.

00:07:46.550 --> 00:07:47.280
My mistake.

00:07:47.280 --> 00:07:49.280
RYAN BOYD: And we can actually
show you here--

00:07:49.280 --> 00:07:51.430
I'll do a very quick
count star query.

00:07:55.660 --> 00:08:03.270
So these are the page views in
Wikipedia for July 2010.

00:08:03.270 --> 00:08:08.880
And it's 13.5 billion rows
that are in this table.

00:08:08.880 --> 00:08:10.035
MICHAEL MANOOCHEHRI:
For a single month.

00:08:10.035 --> 00:08:11.740
RYAN BOYD: For a single
month, and it's over

00:08:11.740 --> 00:08:13.090
a terabyte of data.

00:08:13.090 --> 00:08:14.470
So let's show the type
of query that we're

00:08:14.470 --> 00:08:16.900
going to run on this.

00:08:16.900 --> 00:08:19.650
And the query that we have for
you to run on this is actually

00:08:19.650 --> 00:08:23.770
kind of an interesting query,
and it's really showing the

00:08:23.770 --> 00:08:26.570
power of BigQuery.

00:08:26.570 --> 00:08:27.850
I'm showing a couple
different ways.

00:08:27.850 --> 00:08:31.770
First of all here, the
simple aggregates.

00:08:31.770 --> 00:08:33.150
Michael's going to be talking
about some of the other

00:08:33.150 --> 00:08:36.539
aggregates here shortly, but the
simple aggregate of a sum

00:08:36.539 --> 00:08:39.179
is a very common operation,
especially with log files and

00:08:39.179 --> 00:08:40.750
things like that.

00:08:40.750 --> 00:08:41.860
And then we're basically
getting

00:08:41.860 --> 00:08:44.450
the top viewed titles--

00:08:44.450 --> 00:08:46.480
top viewed pages,
essentially--

00:08:46.480 --> 00:08:49.260
from Wikipedia, and
we're getting the

00:08:49.260 --> 00:08:50.610
100 top viewed pages.

00:08:50.610 --> 00:08:53.550
But we're restricting the set
down a little bit, and we're

00:08:53.550 --> 00:08:56.750
restricting the set with this
regular expression.

00:08:56.750 --> 00:08:59.630
Now, think about on a relational
database trying to

00:08:59.630 --> 00:09:03.600
perform a regular expression
on 13 and 1/2 billion rows.

00:09:03.600 --> 00:09:07.480
If you guys want to try that out
there and give me how long

00:09:07.480 --> 00:09:10.290
your relational database takes
on doing a regular expression

00:09:10.290 --> 00:09:12.500
on 13 billion rows when
it's unindexed.

00:09:12.500 --> 00:09:14.750
These are ad hoc regular
expressions.

00:09:14.750 --> 00:09:17.130
I can change this
however I want.

00:09:17.130 --> 00:09:20.820
Try running that, and give us
a shout out on Google+, and

00:09:20.820 --> 00:09:22.000
let us know how long it took.

00:09:22.000 --> 00:09:22.930
MICHAEL MANOOCHEHRI: Imagine
the horror of building an

00:09:22.930 --> 00:09:24.709
index on something you don't
know yet with the regular

00:09:24.709 --> 00:09:25.420
expression itself.

00:09:25.420 --> 00:09:25.910
RYAN BOYD: Exactly.

00:09:25.910 --> 00:09:28.260
You could probably, in some
relational databases, build an

00:09:28.260 --> 00:09:30.820
index on a regular expression,
but you wouldn't be able to

00:09:30.820 --> 00:09:33.000
change the regular expression
then, so it would take a long

00:09:33.000 --> 00:09:36.210
time to build up that index each
time if you wanted to do

00:09:36.210 --> 00:09:37.950
different types of analyses.

00:09:37.950 --> 00:09:40.660
And that's what BigQuery is
really good at, this ad hoc

00:09:40.660 --> 00:09:42.520
iterative analysis.

00:09:42.520 --> 00:09:45.720
So in here, we're basically
doing a regular expression on

00:09:45.720 --> 00:09:50.990
those 13 and 1/2 billion rows
and finding all the pages that

00:09:50.990 --> 00:09:55.850
start with a capital G, and with
lowercase e, and have two

00:09:55.850 --> 00:09:56.770
o's in between.

00:09:56.770 --> 00:10:00.640
So, think about a page that
looks something like Google.

00:10:00.640 --> 00:10:02.890
We're finding the top
100 of those pages.

00:10:02.890 --> 00:10:04.660
So this is kind of
our inner query.

00:10:04.660 --> 00:10:07.600
We also actually do something in
our outer query to do more

00:10:07.600 --> 00:10:10.800
regular expression magic, and
that is kind of formatting the

00:10:10.800 --> 00:10:12.540
titles of these pages.

00:10:12.540 --> 00:10:14.850
So if you notice when you visit
Wikipedia, the actual

00:10:14.850 --> 00:10:19.620
URL has underscores in place of
all the spaces in the URL.

00:10:19.620 --> 00:10:22.160
We want to strip those out to
make this very easy for us to

00:10:22.160 --> 00:10:24.390
view, so we're showing how we're
doing a sub-select in

00:10:24.390 --> 00:10:26.420
this query as well.

00:10:26.420 --> 00:10:29.805
So I'm going to hit Run Query,
and I will note that this

00:10:29.805 --> 00:10:33.030
query takes a little bit longer
than the typical query

00:10:33.030 --> 00:10:35.190
in BigQuery because it
is going over such a

00:10:35.190 --> 00:10:36.890
massive data set.

00:10:36.890 --> 00:10:39.930
Typically if you're querying
hundreds of millions to a few

00:10:39.930 --> 00:10:43.060
billion rows, you're getting
responses in like 7 to 10

00:10:43.060 --> 00:10:44.570
seconds here.

00:10:44.570 --> 00:10:47.050
But in this case, we're actually
querying over this 13

00:10:47.050 --> 00:10:50.360
and 1/2 billion rows and over
a terabyte of data.

00:10:50.360 --> 00:10:55.310
So it finished in
22.9 seconds.

00:10:55.310 --> 00:10:58.460
And actually, I got rid of that
here, but 22.9 seconds,

00:10:58.460 --> 00:11:02.110
505 gigabytes of data
that it processed.

00:11:02.110 --> 00:11:04.230
And the reason that's different
than the one

00:11:04.230 --> 00:11:06.990
terabyte of data number that
we saw earlier is we're

00:11:06.990 --> 00:11:09.680
actually only accessing
particular columns.

00:11:09.680 --> 00:11:11.530
Remember we mentioned
about the column or

00:11:11.530 --> 00:11:12.990
data store in BigQuery.

00:11:12.990 --> 00:11:16.170
We're only accessing the
title, the views, the

00:11:16.170 --> 00:11:18.400
Wikipedia project,
the language.

00:11:18.400 --> 00:11:21.830
We're only accessing a subset of
the columns in this table,

00:11:21.830 --> 00:11:24.220
so we're only hitting about
half of the data.

00:11:24.220 --> 00:11:28.390
So 505 gigabytes, I think was
23.9 seconds or something like

00:11:28.390 --> 00:11:32.800
that, and we can see here all
the page views of the pages

00:11:32.800 --> 00:11:34.620
that matched that regular
expression.

00:11:34.620 --> 00:11:35.920
MICHAEL MANOOCHEHRI: This
is an amazing query

00:11:35.920 --> 00:11:36.585
with a lot of value.

00:11:36.585 --> 00:11:38.620
You can see that it's looking
at products, and you can see

00:11:38.620 --> 00:11:41.040
which products have been more
viewed on Wikipedia.

00:11:41.040 --> 00:11:42.510
There's some information
about health.

00:11:42.510 --> 00:11:43.630
There's some disease here.

00:11:43.630 --> 00:11:45.490
So you can actually look and see
what people are actually

00:11:45.490 --> 00:11:48.110
searching for in a matter
of seconds, really.

00:11:48.110 --> 00:11:51.300
RYAN BOYD: So we often use this
Wikipedia data set to

00:11:51.300 --> 00:11:53.950
show off some of the power of
BigQuery, because it's a great

00:11:53.950 --> 00:11:57.840
open data set, freely
available to access.

00:11:57.840 --> 00:12:01.040
We actually downloaded all
of this data off of

00:12:01.040 --> 00:12:04.810
dumps.wikimedia.org, but you
can see the analogy here,

00:12:04.810 --> 00:12:07.370
probably, between this and some
of your business problems

00:12:07.370 --> 00:12:08.300
that you may have.

00:12:08.300 --> 00:12:13.360
So you think about views on
advertising and the click

00:12:13.360 --> 00:12:15.390
throughs on your ads.

00:12:15.390 --> 00:12:19.380
That can really add up as you
have your ads on hundreds of

00:12:19.380 --> 00:12:22.370
millions of mobile devices
and web browsers.

00:12:22.370 --> 00:12:26.800
That can really add up, and
this same type of query is

00:12:26.800 --> 00:12:28.250
often used for things
like that.

00:12:28.250 --> 00:12:31.390
So for instance, if the title of
the Wikipedia page here was

00:12:31.390 --> 00:12:34.590
the ad campaign, and you could
figure out which ad campaign

00:12:34.590 --> 00:12:38.480
got the most views when that ad
campaign was a particular

00:12:38.480 --> 00:12:41.810
type, and maybe that type could
be a simple regular

00:12:41.810 --> 00:12:43.830
expression or something
like that.

00:12:43.830 --> 00:12:45.130
So these queries are really--

00:12:45.130 --> 00:12:47.680
we're doing them in Wikipedia,
but you can think about your

00:12:47.680 --> 00:12:51.140
own business challenges and what
types of queries you want

00:12:51.140 --> 00:12:51.930
to be able to do.

00:12:51.930 --> 00:12:56.130
But a terabyte of data
in 23.9 seconds?

00:12:56.130 --> 00:13:00.500
It's not really a bad query
time here, especially with

00:13:00.500 --> 00:13:02.790
being able to do regular
expressions on all the rows.

00:13:02.790 --> 00:13:04.890
MICHAEL MANOOCHEHRI: In this
query, we had a simple sum

00:13:04.890 --> 00:13:07.420
aggregate, so let's take a look
at some other aggregate

00:13:07.420 --> 00:13:10.290
queries that are possible to
do with Google BigQuery.

00:13:10.290 --> 00:13:11.760
So when I talk about aggregates,
I'm not just

00:13:11.760 --> 00:13:12.430
talking about sums.

00:13:12.430 --> 00:13:15.780
I'm talking about things like
counts and basic statistical

00:13:15.780 --> 00:13:18.560
measures like standard
deviation, average, variance,

00:13:18.560 --> 00:13:19.950
and things like that.

00:13:19.950 --> 00:13:21.640
Like Ryan said, in a relational
database, sometimes

00:13:21.640 --> 00:13:24.690
we try to solve the speed
problem with querying for

00:13:24.690 --> 00:13:26.260
aggregates with indexes.

00:13:26.260 --> 00:13:28.080
Of course, that's challenging
in itself.

00:13:28.080 --> 00:13:30.270
With a document store, something
like CouchDB, you

00:13:30.270 --> 00:13:32.260
could run aggregate queries,
but oftentimes you have to

00:13:32.260 --> 00:13:34.010
write your own custom
MapReduce.

00:13:34.010 --> 00:13:35.900
What's great about BigQuery
is it combines the

00:13:35.900 --> 00:13:36.940
best of both worlds.

00:13:36.940 --> 00:13:39.880
You can express these aggregate
queries in terms of

00:13:39.880 --> 00:13:41.570
a SQL-style language.

00:13:41.570 --> 00:13:42.960
So let's take a look some
of these examples.

00:13:42.960 --> 00:13:44.310
This is a really interesting
query.

00:13:44.310 --> 00:13:47.270
We're going to look at birth
weights by state-- so we'll

00:13:47.270 --> 00:13:48.730
just pick Ohio in this case--

00:13:48.730 --> 00:13:51.240
by a particular year, and we're
going to look at how

00:13:51.240 --> 00:13:54.545
cigarette use of the mother
affects the birth weight

00:13:54.545 --> 00:13:56.210
RYAN BOYD: And this is actually
a public data set

00:13:56.210 --> 00:13:57.670
that you can play around with.

00:13:57.670 --> 00:14:00.800
All of our public data sets,
many of the things that we're

00:14:00.800 --> 00:14:03.680
talking about here, are
available in the public data

00:14:03.680 --> 00:14:04.750
sets on BigQuery.

00:14:04.750 --> 00:14:06.400
You can look in the
table of contents.

00:14:06.400 --> 00:14:10.400
I think it says public data sets
or sample data sets, I

00:14:10.400 --> 00:14:12.620
believe, and all these
are available.

00:14:12.620 --> 00:14:15.530
Play around with them, and let
us know what interesting

00:14:15.530 --> 00:14:17.620
queries that you come up with.

00:14:17.620 --> 00:14:19.910
But let's show this
example here.

00:14:19.910 --> 00:14:21.110
And if there's any of the things
that we're talking

00:14:21.110 --> 00:14:24.040
about that aren't there yet, we
will work on coming up with

00:14:24.040 --> 00:14:26.240
some other data as well.

00:14:26.240 --> 00:14:27.570
MICHAEL MANOOCHEHRI: This is a
good opportunity, too, to tell

00:14:27.570 --> 00:14:29.950
you that you have a query quota
that's free, so you can

00:14:29.950 --> 00:14:32.420
actually try this out for free,
and do some queries, and

00:14:32.420 --> 00:14:35.210
learn how to use it without
billing turned on.

00:14:35.210 --> 00:14:38.260
So let's check out this query.

00:14:38.260 --> 00:14:41.380
Let's see if there is a
difference between mothers who

00:14:41.380 --> 00:14:43.430
smoke and baby weight for
this particular year.

00:14:43.430 --> 00:14:43.840
So wow.

00:14:43.840 --> 00:14:44.650
That was a very fast query.

00:14:44.650 --> 00:14:47.430
In just three seconds, we
returned this data.

00:14:47.430 --> 00:14:49.820
It looks like we processed
about almost 3 and 1/2

00:14:49.820 --> 00:14:51.210
gigabytes in just
three seconds.

00:14:51.210 --> 00:14:55.640
And you can see when cigarette
use is true, it looks like the

00:14:55.640 --> 00:14:58.200
baby weight is slightly
smaller.

00:14:58.200 --> 00:14:59.350
It's actually quite a--

00:14:59.350 --> 00:15:01.340
RYAN BOYD: Half a pound
difference.

00:15:01.340 --> 00:15:04.770
We should warn we're not
statisticians here.

00:15:04.770 --> 00:15:07.130
We're not defining a
causation, but just

00:15:07.130 --> 00:15:07.500
correlation.

00:15:07.500 --> 00:15:10.935
But a half a pound difference
when the mothers said that

00:15:10.935 --> 00:15:11.800
they smoked cigarettes.

00:15:11.800 --> 00:15:14.690
Now, not sure that those answers
are always accurate to

00:15:14.690 --> 00:15:18.030
that question, but a half a
pound difference-- and we can

00:15:18.030 --> 00:15:20.140
actually see here, the mother
age is different, too.

00:15:20.140 --> 00:15:24.420
So it looks like the mothers
who are giving birth to

00:15:24.420 --> 00:15:27.900
children who smoked cigarettes
are actually about two years

00:15:27.900 --> 00:15:30.380
younger than the mothers who
said that they didn't smoke

00:15:30.380 --> 00:15:31.270
cigarettes.

00:15:31.270 --> 00:15:33.320
And I don't know if that has
anything to do with cigarette

00:15:33.320 --> 00:15:35.280
use in general, or if that just
has to do with the ones

00:15:35.280 --> 00:15:38.820
that would admit to smoking
cigarettes, but it's still an

00:15:38.820 --> 00:15:41.560
interesting type of ad hoc
analysis that you could do

00:15:41.560 --> 00:15:44.330
here, and especially with the
group buys, averages, standard

00:15:44.330 --> 00:15:44.720
deviations.

00:15:44.720 --> 00:15:46.280
MICHAEL MANOOCHEHRI: Imagine
piping this data into a

00:15:46.280 --> 00:15:48.570
statistics package to do a
little bit more analysis on

00:15:48.570 --> 00:15:49.140
that as well.

00:15:49.140 --> 00:15:50.560
It's very simple.

00:15:50.560 --> 00:15:53.600
RYAN BOYD: We don't have this
prepared here today, but since

00:15:53.600 --> 00:15:57.000
you brought up statistics
packages, oftentimes people

00:15:57.000 --> 00:15:59.270
will take data out of BigQuery
and do like a

00:15:59.270 --> 00:16:01.350
hash function on it.

00:16:01.350 --> 00:16:04.070
And they can do that hash
function on it to select a

00:16:04.070 --> 00:16:06.660
random sampling of their
data, then, to use in

00:16:06.660 --> 00:16:08.070
a statistics package.

00:16:08.070 --> 00:16:10.850
Most statistics packages won't
support if you have hundreds

00:16:10.850 --> 00:16:14.030
of millions or billions of rows
of data, but you can use

00:16:14.030 --> 00:16:18.350
that BigQuery hash function to
select your random subset.

00:16:18.350 --> 00:16:21.160
Use hash, and then [? mob ?]
that by a particular number.

00:16:21.160 --> 00:16:22.840
MICHAEL MANOOCHEHRI: So let's
look at another example.

00:16:22.840 --> 00:16:24.680
This is an example actually
that Ryan came up with.

00:16:24.680 --> 00:16:30.005
It's an interesting use of
BigQuery to bucket birth by

00:16:30.005 --> 00:16:31.300
sex and by state.

00:16:33.890 --> 00:16:35.180
So what's great about
this is it uses

00:16:35.180 --> 00:16:36.030
a conditional statement.

00:16:36.030 --> 00:16:37.540
So if you can see
the statement--

00:16:37.540 --> 00:16:40.870
we're selecting by state, and
we're saying if Ismail, which

00:16:40.870 --> 00:16:45.330
is a Boolean, label that as a
male birth, otherwise female.

00:16:45.330 --> 00:16:45.880
[INAUDIBLE]

00:16:45.880 --> 00:16:47.990
do a count, [INAUDIBLE]
and group by state.

00:16:47.990 --> 00:16:50.780
The other interesting thing is
we're using a function called

00:16:50.780 --> 00:16:55.190
having, and having is a lot
like aware statement, but

00:16:55.190 --> 00:16:57.710
we're using the aggregate data,
in this case the count,

00:16:57.710 --> 00:16:59.750
to compare.

00:16:59.750 --> 00:17:01.620
So this is a very fast query.

00:17:01.620 --> 00:17:03.760
As you can see, some of the
more populous states, like

00:17:03.760 --> 00:17:07.910
California, you can see how
many births were male, how

00:17:07.910 --> 00:17:08.839
many were female.

00:17:08.839 --> 00:17:11.170
And we're counting only states
with over, is it

00:17:11.170 --> 00:17:12.365
five million births?

00:17:12.365 --> 00:17:14.150
RYAN BOYD: Yeah, over
five million births.

00:17:14.150 --> 00:17:16.540
Well, only states that have
five million births of

00:17:16.540 --> 00:17:17.819
children of the same sex.

00:17:17.819 --> 00:17:21.990
So it looks like Texas had more
than five million births

00:17:21.990 --> 00:17:25.490
of males but less births
of females.

00:17:25.490 --> 00:17:26.650
But Texas and California,

00:17:26.650 --> 00:17:27.885
apparently, are quite populous.

00:17:27.885 --> 00:17:29.090
MICHAEL MANOOCHEHRI: This
is really interesting.

00:17:29.090 --> 00:17:31.720
So let's take a look at another
statistics query.

00:17:31.720 --> 00:17:33.670
We're going to look at a
standard deviation query.

00:17:36.320 --> 00:17:38.660
I was really interested to see
if there are any outliers, and

00:17:38.660 --> 00:17:42.390
standard deviation is a great
way to look for outliers.

00:17:42.390 --> 00:17:46.630
So what we want to do is figure
out what birth weights

00:17:46.630 --> 00:17:50.670
are several times a standard
deviation higher than average,

00:17:50.670 --> 00:17:52.796
and sort of get a baseline.

00:17:52.796 --> 00:17:54.070
RYAN BOYD: So I'm just
trying to find

00:17:54.070 --> 00:17:56.050
that query here quickly.

00:17:56.050 --> 00:17:58.926
Let's see here--

00:17:58.926 --> 00:17:59.990
there we go.

00:17:59.990 --> 00:18:02.460
MICHAEL MANOOCHEHRI: This query
is looking at the entire

00:18:02.460 --> 00:18:03.530
natality data set.

00:18:03.530 --> 00:18:07.000
This is a full table scan, and
we're finding the average plus

00:18:07.000 --> 00:18:09.710
four times the standard
deviation away.

00:18:09.710 --> 00:18:10.250
And here we go.

00:18:10.250 --> 00:18:11.410
In just a few seconds,
we found that

00:18:11.410 --> 00:18:13.020
it's about 12.7 pounds.

00:18:13.020 --> 00:18:14.710
So that's several standard
deviations

00:18:14.710 --> 00:18:15.910
away from the average.

00:18:15.910 --> 00:18:21.250
Now we can take that value and
run a similar query, and find

00:18:21.250 --> 00:18:25.190
if there are any outlying months
and states where babies

00:18:25.190 --> 00:18:28.528
were a little bit heftier
than that.

00:18:28.528 --> 00:18:30.800
RYAN BOYD: And Michael's going
to have to point out this

00:18:30.800 --> 00:18:32.110
query for me here again.

00:18:32.110 --> 00:18:33.560
There we go.

00:18:33.560 --> 00:18:34.780
MICHAEL MANOOCHEHRI: We're going
to take that value that

00:18:34.780 --> 00:18:37.160
we got in the standard deviation
query and rerun it

00:18:37.160 --> 00:18:39.110
on the entire data set,
and look for outliers.

00:18:39.110 --> 00:18:39.720
Oh, look.

00:18:39.720 --> 00:18:43.450
So it looks like Maryland in
1990 in December had 12 babies

00:18:43.450 --> 00:18:46.670
that were very far above that
four times the standard

00:18:46.670 --> 00:18:47.740
deviation away from
the average.

00:18:47.740 --> 00:18:49.600
So I'm not sure exactly what's
going on there, but it's

00:18:49.600 --> 00:18:51.900
interesting that that's a
slightly smaller state than

00:18:51.900 --> 00:18:54.340
some of these other ones, like
New York and California, so it

00:18:54.340 --> 00:18:55.810
looks like an outlier
case to me.

00:18:55.810 --> 00:18:58.740
So you can use BigQuery to slam
through tons of data to

00:18:58.740 --> 00:19:03.320
find cases like this with these
statistical queries.

00:19:03.320 --> 00:19:05.370
And I just wanted to highlight
another query.

00:19:05.370 --> 00:19:06.620
It's the--

