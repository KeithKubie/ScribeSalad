WEBVTT
Kind: captions
Language: en

00:00:00.320 --> 00:00:03.090
To compute the parameter
updates of a recurrent network,

00:00:03.090 --> 00:00:05.670
we need to backpropagate
the derivative through time,

00:00:05.670 --> 00:00:07.700
all the way to the beginning
of the sequence.

00:00:07.700 --> 00:00:11.450
Or in practice, more often, for
as many steps as we can afford.

00:00:11.450 --> 00:00:15.510
All these derivatives are going to
be applied to the same parameters.

00:00:15.510 --> 00:00:19.950
That's a lot of correlated updates
all at once, for the same weights.

00:00:19.950 --> 00:00:22.315
This is bad for
stochastic gradient descent.

00:00:22.315 --> 00:00:25.560
Stochastic gradient descent,
prefers to have uncorrelated updates,

00:00:25.560 --> 00:00:28.820
to its parameters, for
the stability of the training.

00:00:28.820 --> 00:00:31.300
This makes the math very unstable.

00:00:31.300 --> 00:00:35.170
Either the gradients grow exponentially
and you end up with infinities.

00:00:35.170 --> 00:00:37.550
Or they go down to zero very quickly.

00:00:37.550 --> 00:00:38.970
And you end up not training anything.

