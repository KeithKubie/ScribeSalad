WEBVTT
Kind: captions
Language: en

00:00:00.210 --> 00:00:02.420
At this point I'm going to
skip over a bunch of details.

00:00:02.420 --> 00:00:07.248
For example which features are we
going to use to make their code words?

00:00:07.248 --> 00:00:10.561
What size should there be when
you do a vector quantization?

00:00:10.561 --> 00:00:14.455
Sometimes it'll tell you how many
code words it thinks it needs or

00:00:14.455 --> 00:00:16.241
sometimes you give it a number.

00:00:16.241 --> 00:00:18.764
We're not going to talk about that,
and we're not even going to talk about

00:00:18.764 --> 00:00:21.255
the clustering or
the vector quantization algorithm.

00:00:21.255 --> 00:00:25.405
Instead we're going to focus on how
would you use that in a way that relates

00:00:25.405 --> 00:00:31.270
to the SVMs that we just talked
about in order to do recognition.

00:00:31.270 --> 00:00:34.630
So, what we do is we're going to
continue with our analogy to documents.

00:00:34.630 --> 00:00:36.670
So here I have two different documents.

00:00:36.670 --> 00:00:39.330
And if you take a look at the document
on the left you'll see a whole bunch of

00:00:39.330 --> 00:00:40.090
words in here.

00:00:40.090 --> 00:00:42.950
Right, sensory, brain, visual,
perception, et cetera, nerve,

00:00:42.950 --> 00:00:44.580
image Hubel, Wiesel.

00:00:44.580 --> 00:00:46.930
Hubel and Wiesel, by the way,
are people who won Nobel Prize for

00:00:46.930 --> 00:00:51.010
understanding what goes on in the visual
cortex of the cat, all right?

00:00:51.010 --> 00:00:52.510
And here's another document.

00:00:52.510 --> 00:00:57.920
China, trade, surplus exports, domestic,
foreign, increase, trade, value.

00:00:57.920 --> 00:01:02.190
Notice that what's in the magnifying
glasses are not ordered.

00:01:02.190 --> 00:01:06.580
It's just the words that
happened to be in that document.

00:01:07.590 --> 00:01:09.510
It's like you stuck all
the words in a bag.

00:01:10.530 --> 00:01:13.910
And that's where we get this
notion of bag of words.

00:01:13.910 --> 00:01:17.851
Okay, and the goal of what we're
going to talk about now is to go from

00:01:17.851 --> 00:01:21.015
object recognition and
map that onto a bag of words.

00:01:21.015 --> 00:01:24.585
And in the document world, you know, one
way of doing this is I have my bag of

00:01:24.585 --> 00:01:27.889
words and I say, what is the
distribution of words in this document?

00:01:27.889 --> 00:01:30.339
And if I wanted to do
document retrieval,

00:01:30.339 --> 00:01:34.609
I want to find other similar documents,
a simple thing you might do is find

00:01:34.609 --> 00:01:39.160
other documents that have sort of the
same histogram distribution of words.

00:01:39.160 --> 00:01:41.910
And that,
that's likely to be a matching document.

00:01:41.910 --> 00:01:44.500
Things are much more sophisticated
now in terms of how that works.

00:01:44.500 --> 00:01:48.140
But it's, the basic idea is that I've
got these code words that represent or

00:01:48.140 --> 00:01:50.758
these words that represent
something about the semantics.

00:01:50.758 --> 00:01:54.170
And so a similarity in
distribution tells me that,

00:01:54.170 --> 00:01:57.250
maybe there's a good chance that
it's a similar type of document.

00:01:57.250 --> 00:01:59.690
So we want to do this
in computer vision.

00:01:59.690 --> 00:02:01.250
So that's notionally represented here.

00:02:01.250 --> 00:02:03.705
So I've got a bunch of
different objects and

00:02:03.705 --> 00:02:07.032
I compute my features over all
of these different objects.

00:02:07.032 --> 00:02:09.627
And I come up with what's
shown down here, and

00:02:09.627 --> 00:02:12.233
I think this side is
originally from [FOREIGN].

00:02:12.233 --> 00:02:13.140
I'm not sure.

00:02:13.140 --> 00:02:17.500
And all of these,
these are all of my code words for

00:02:17.500 --> 00:02:19.410
all of my different objects, right?

00:02:19.410 --> 00:02:22.150
So I take all my objects, and
it's kind of like if you take

00:02:22.150 --> 00:02:26.020
all the documents in a,
a library in United States,

00:02:26.020 --> 00:02:29.660
and if they're all in English, you
took all of those documents together.

00:02:29.660 --> 00:02:32.530
All of the words would be
the set of the words that span

00:02:32.530 --> 00:02:34.310
all of those documents, right?

00:02:34.310 --> 00:02:38.890
And in fact there are many more
documents than there are words.

00:02:38.890 --> 00:02:41.630
And in fact if you think of important
words, words that actually have a lot of

00:02:41.630 --> 00:02:45.440
meaning, there's way more
documents than words.

00:02:45.440 --> 00:02:50.870
So given a collection of words,
visual words,

00:02:50.870 --> 00:02:58.000
for every document, I can say how many
of each word is in each document.

00:02:58.000 --> 00:03:01.500
And that's what these histograms
are meant to represent, right?

00:03:01.500 --> 00:03:06.470
So you know, the bicycle has got
these little bicycle pieces in it.

00:03:06.470 --> 00:03:10.220
All right, and something that
might kind of look like this.

00:03:10.220 --> 00:03:11.820
I don't know, maybe, you know,

00:03:11.820 --> 00:03:15.560
this brown part down here looks a little
bit like that brown part down there.

00:03:15.560 --> 00:03:18.730
But the idea is that there's many more
of the bicycle elements than there

00:03:18.730 --> 00:03:21.568
are of the, the violin elements.

00:03:21.568 --> 00:03:25.080
This is referred to as a bag of
visual words, for obvious reasons.

00:03:25.080 --> 00:03:28.050
And the idea is that you're
describing the entire image

00:03:28.050 --> 00:03:31.630
just by the histogram or
the collection of those words.

00:03:31.630 --> 00:03:34.080
And the, and you want to do access
the same way that you did in

00:03:34.080 --> 00:03:34.930
the documents world.

