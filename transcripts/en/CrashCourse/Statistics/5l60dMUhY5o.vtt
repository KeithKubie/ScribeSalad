WEBVTT
Kind: captions
Language: en

00:00:02.980 --> 00:00:06.040
Hi, I’m Adriene Hill, and Welcome back to
Crash Course Statistics.

00:00:06.040 --> 00:00:09.410
We’ve learned a lot about how statistics
can help us understand the world better, make

00:00:09.410 --> 00:00:13.080
better decisions, and guess what will happen
in the future.

00:00:13.080 --> 00:00:17.661
Prediction is a huge part of how modern statistical
analysis is used, and it’s helped us make

00:00:17.661 --> 00:00:19.110
improvements to our lives.

00:00:19.110 --> 00:00:20.410
Big AND small.

00:00:20.410 --> 00:00:22.590
But predictions are just educated guesses.

00:00:22.590 --> 00:00:26.710
We use the information that we have to build
up a model of how the world works.

00:00:26.710 --> 00:00:30.600
A lot of the example we talked about earlier
in the series were making predictions about

00:00:30.600 --> 00:00:31.600
the present.

00:00:31.600 --> 00:00:35.600
Things like “which coffee shop has better
coffee” or “How much does an increase

00:00:35.600 --> 00:00:38.070
in cigarette smoking decrease heart health”.

00:00:38.070 --> 00:00:41.899
But in this episode, we’re going to focus
more on using statistics to make predictions

00:00:41.899 --> 00:00:42.980
about the future.

00:00:42.980 --> 00:00:47.680
Like who will win the next world series, or
what stock will do well next month.

00:00:47.680 --> 00:00:51.340
Looking back at times when we’ve failed
to make accurate predictions can help us understand

00:00:51.340 --> 00:00:55.030
more about how to get it right or whether
we just don’t have enough information.

00:00:55.030 --> 00:00:59.870
Today, we’re going to talk about three areas
of prediction: markets, Earthquakes, and Elections.

00:00:59.870 --> 00:01:04.420
We’ll look at why predicting these events
can be tricky why we get it wrong.

00:01:04.420 --> 00:01:13.440
INTRO

00:01:13.440 --> 00:01:19.140
Banks were influential in creating the perfect
storm that lead to the 2008 financial crisis.

00:01:19.140 --> 00:01:22.560
If you’ve seen the Big Short, or read the
book it’s based on, you know that.

00:01:22.560 --> 00:01:25.659
You also know that Steve Carell should never
go blonde again.

00:01:25.659 --> 00:01:29.670
The financial crisis is really complicated
we’re about to simplify a lot….but if

00:01:29.670 --> 00:01:33.619
you are interested you can check out Episode
12 of our Economics series.

00:01:33.619 --> 00:01:39.029
For now, we’re going to focus on two prediction
issues related to the crisis: 1.

00:01:39.029 --> 00:01:42.590
overestimating the independence of loan failures
and 2.

00:01:42.590 --> 00:01:44.350
Economists who didn’t see the crisis coming.

00:01:44.350 --> 00:01:48.009
So before the crisis, banks were giving out
mortgages to pretty-much anyone.

00:01:48.009 --> 00:01:52.789
Normally, banks--and lenders in general--are
choosy about who they lend to.

00:01:52.789 --> 00:01:56.939
If you give someone a million dollar loan,
and they can’t pay it back, you lose out.

00:01:56.939 --> 00:02:00.239
But banks weren’t hanging on to the debt
they were selling it to others.

00:02:00.239 --> 00:02:05.709
They combined mortgages into groups and sold
shares of the loans as mortgage backed securities.

00:02:05.709 --> 00:02:08.819
The banks knew some people wouldn’t pay
their loan in full, but when the mortgages

00:02:08.819 --> 00:02:11.810
were packaged together, the risk was supposedly
mitigated.

00:02:11.810 --> 00:02:17.129
Say that there’s a 10% chance that each
borrower will default on--or fail to repay--their

00:02:17.129 --> 00:02:18.129
loan.

00:02:18.129 --> 00:02:21.090
While not totally risky, it’s not ideal
for investors.

00:02:21.090 --> 00:02:26.290
But if you packaged even 5 similar loans together,
the probability that all of them will default

00:02:26.290 --> 00:02:29.530
is now only 0.001%.

00:02:29.530 --> 00:02:34.760
Because the probability of all of them failing--if
each loan failing is independent of another

00:02:34.760 --> 00:02:38.019
loan failing--is 0.1 to the 5th power.

00:02:38.019 --> 00:02:39.709
But we just made a prediction mistake.

00:02:39.709 --> 00:02:43.599
Many investors overestimated the independence
of loan failures.

00:02:43.599 --> 00:02:48.160
They didn’t take into account that if the
then-overvalued housing market and subsequently

00:02:48.160 --> 00:02:54.880
the economy began to crumble, the probability
of loans going unpaid would shoot way up.

00:02:54.880 --> 00:02:59.129
They also had bad estimates for just how risky
some of these loans were.

00:02:59.129 --> 00:03:03.810
Families were losing homes, and the unemployment
rate in the U.S. steadily increased from around

00:03:03.810 --> 00:03:07.849
5% to as high as 10% in just a couple years.

00:03:07.849 --> 00:03:11.579
There was a global recession that most economists’
models hadn’t predicted.

00:03:11.579 --> 00:03:16.730
To this day, they’re still debating exactly
why Economist John T. Harvey claims, “Economics

00:03:16.730 --> 00:03:22.760
is skewed towards rewarding people for building
complex mathematical models, not for explaining

00:03:22.760 --> 00:03:25.209
how the actual economy works.”

00:03:25.209 --> 00:03:30.379
Others theorize that we need to focus more
on people and their sometimes irrational behavior.

00:03:30.379 --> 00:03:35.040
Wharton Finance professor Franklin Allen partly
attributes our inability to predict the financial

00:03:35.040 --> 00:03:38.680
crisis to models that underplayed the impact
of banks.

00:03:38.680 --> 00:03:42.900
The same banks that were involved in the lending
practices that helped create--and then deflate--the

00:03:42.900 --> 00:03:43.900
housing bubble.

00:03:43.900 --> 00:03:46.540
He claims,
“That’s a large part of the issue.

00:03:46.540 --> 00:03:49.510
They simply didn’t believe the banks were
important.”

00:03:49.510 --> 00:03:50.829
But they were.

00:03:50.829 --> 00:03:53.750
Prediction depends a lot on whether or not
you have enough data available.

00:03:53.750 --> 00:03:57.610
But it also depends on what your model deems
as “important”.

00:03:57.610 --> 00:04:01.459
You can collect a HUGE amount of data predicting
the rates of diabetes in each country.

00:04:01.459 --> 00:04:05.349
But if your model only considers hair color,
whether or not a person drives a hybrid, and

00:04:05.349 --> 00:04:09.670
the number of raccoons they think they can
fight it probably won’t be a good model.

00:04:09.670 --> 00:04:14.260
When we create a model to predict things,
we’re trying to use data, math, and statistics

00:04:14.260 --> 00:04:16.530
in order to approximate how the world works.

00:04:16.530 --> 00:04:20.259
We’re never going to get it perfect, but
if we include most of the important things,

00:04:20.259 --> 00:04:22.330
we can usually get pretty close.

00:04:22.330 --> 00:04:25.259
Even if we can tell what features will be
important, it might be hard to get enough

00:04:25.259 --> 00:04:26.630
data.

00:04:26.630 --> 00:04:29.440
Earthquakes are particularly difficult to
predict.

00:04:29.440 --> 00:04:34.470
The United States Geological Survey even has
a webpage dedicated to telling the public

00:04:34.470 --> 00:04:37.759
that currently, earthquakes just aren’t
predictable.

00:04:37.759 --> 00:04:41.639
Clusters of smaller earthquakes often happen
before larger ones.

00:04:41.639 --> 00:04:46.360
But these pre-quakes aren’t that helpful
in predicting when a big earthquake will hit,

00:04:46.360 --> 00:04:49.420
because they’re almost just as often followed
by NOTHING.

00:04:49.420 --> 00:04:53.539
In order to accurately predict an earthquake
you would need three pieces of information:

00:04:53.539 --> 00:04:56.690
its location, magnitude, and time.

00:04:56.690 --> 00:05:00.050
It can be relatively easy to get two out of
three of those.

00:05:00.050 --> 00:05:03.800
For example, I predict that there will be
an earthquake in the future in Los Angeles,

00:05:03.800 --> 00:05:04.800
California.

00:05:04.800 --> 00:05:05.800
And I’d be right.

00:05:05.800 --> 00:05:10.440
But unless I can also specify an exact time,
no one’s going to be handing me any honorary

00:05:10.440 --> 00:05:11.729
degrees in seismology.

00:05:11.729 --> 00:05:16.919
We’re not bad at earthquake forecasting
even if we struggle with accurate earthquake

00:05:16.919 --> 00:05:18.509
prediction.

00:05:18.509 --> 00:05:23.660
Earthquake forecasting focuses on the probabilities
of earthquakes, usually over longer periods

00:05:23.660 --> 00:05:24.660
of time.

00:05:24.660 --> 00:05:27.210
It can also help predict likely effects and
damage.

00:05:27.210 --> 00:05:32.199
This forecasting work is incredibly important
for mitigating the sometimes devastating effects

00:05:32.199 --> 00:05:33.789
of larger earthquakes.

00:05:33.789 --> 00:05:37.500
For example, scientists might look at the
likelihood of severe earthquakes along the

00:05:37.500 --> 00:05:39.250
San Andreas fault.

00:05:39.250 --> 00:05:43.400
Their estimates can help inform building codes,
disaster plans for the area, and even earthquake

00:05:43.400 --> 00:05:44.710
insurance rates.

00:05:44.710 --> 00:05:47.319
And earthquakes are not without some kind
of pattern.

00:05:47.319 --> 00:05:53.540
They do tend to occur in clusters, with aftershocks
following quakes in a pretty predictable pattern.

00:05:53.540 --> 00:05:57.620
But in his book The Signal and the Noise,
Nate Silver warns about looking so hard at

00:05:57.620 --> 00:06:02.569
the data, that we see noise--random variation
with no pattern--as a signal.

00:06:02.569 --> 00:06:05.500
The causes of earthquakes are incredibly complex.

00:06:05.500 --> 00:06:09.190
And the truth is, we’re not in a place where
we can accurately predict when, where, and

00:06:09.190 --> 00:06:10.520
how they’ll occur.

00:06:10.520 --> 00:06:13.830
Especially the larger, particularly destructive
earthquakes.

00:06:13.830 --> 00:06:18.960
To predict a magnitude 9 earthquake, we’d
need to look at data on other similar earthquakes.

00:06:18.960 --> 00:06:21.600
But there just isn’t that much out there.

00:06:21.600 --> 00:06:26.099
Realistically it could be centuries before
we have enough to make solid predictions.

00:06:26.099 --> 00:06:29.540
Even for more common magnitude earthquakes,
it could a lot of data before we have enough

00:06:29.540 --> 00:06:32.510
to see the pattern amidst all the randomness.

00:06:32.510 --> 00:06:37.800
Some experts have written off the possibility
of accurate earthquake prediction almost entirely,

00:06:37.800 --> 00:06:41.830
but others hold on to the hope that with enough
data and time we’ll figure it out.

00:06:41.830 --> 00:06:47.190
Speaking of earthquakes, the 2016 US presidential
election results have been described as a

00:06:47.190 --> 00:06:48.409
political earthquake.

00:06:48.409 --> 00:06:51.880
Many experts didn’t predict the election
of President Donald Trump.

00:06:51.880 --> 00:06:55.009
It’s easy to forget that predictions are
not certain.

00:06:55.009 --> 00:06:58.849
If we could be 100% certain about anything,
we wouldn’t really need predictions.

00:06:58.849 --> 00:07:02.560
In the past, we’ve talked about the fact
that when predicting percentages, like how

00:07:02.560 --> 00:07:06.990
many people will vote for one candidate vs.
the other, there are margins of error.

00:07:06.990 --> 00:07:13.090
If candidate A is predicted to get 54 +/-
2% of the vote, that means that experts predict

00:07:13.090 --> 00:07:19.810
that candidate A will get 54% of the vote,
but wouldn’t be surprised by 52 or 55%.

00:07:19.810 --> 00:07:22.250
These margins help communicate uncertainty.

00:07:22.250 --> 00:07:26.610
But when predictions are discrete--like “will
win” or “won’t win”--it can be easier

00:07:26.610 --> 00:07:29.070
to misunderstand this uncertainty.

00:07:29.070 --> 00:07:32.910
It’s possible for predictions to fail without
models being bad.

00:07:32.910 --> 00:07:38.220
Nate Silver discusses the fact that many predictions
put Trump’s chance of winning the 2016 presidential

00:07:38.220 --> 00:07:40.680
election at about 1 in 100.

00:07:40.680 --> 00:07:44.629
Silver’s prediction on his website, FiveThirtyEight,
put Trump at a much higher chance of about

00:07:44.629 --> 00:07:45.930
3 in 10.

00:07:45.930 --> 00:07:50.030
If you had forced statisticians to predict
a winner, the smart choice according to these

00:07:50.030 --> 00:07:52.289
numbers would have been Hillary Clinton.

00:07:52.289 --> 00:07:56.439
But here’s the problem: many people see
1 in 100 odds against an event, and take it

00:07:56.439 --> 00:07:59.410
to mean that the event is essentially impossible.

00:07:59.410 --> 00:08:04.719
By the numbers, a 1 in 100 chance--even though
low-still says the event will happen 1 every

00:08:04.719 --> 00:08:05.719
100 times.

00:08:05.719 --> 00:08:09.129
There’s been a lot of debate about how these
polls and predictions “got it wrong”.

00:08:09.129 --> 00:08:13.460
But one thing that we should take away from
the election prediction is that low probabilities

00:08:13.460 --> 00:08:15.479
don’t equal impossible events.

00:08:15.479 --> 00:08:20.599
If a meticulously curated prediction gives
a 1 in 100 chance for a candidate to win,

00:08:20.599 --> 00:08:24.790
and that candidate wins, it doesn’t mean
that the prediction was wrong.

00:08:24.790 --> 00:08:27.639
Unlikely things do happen, and we need to
take that into account.

00:08:27.639 --> 00:08:30.400
But we still should keep striving to make
our polls better.

00:08:30.400 --> 00:08:34.430
Many who have done post-mortems on the 2016
election polls and predictions attribute some

00:08:34.430 --> 00:08:36.970
blame to biases in the polls themselves.

00:08:36.970 --> 00:08:42.250
According to the New York Times, “Well-educated
voters are much likelier to take surveys than

00:08:42.250 --> 00:08:44.190
less educated ones.”

00:08:44.190 --> 00:08:48.590
That means we had a non-response bias from
those less educated voters.

00:08:48.590 --> 00:08:52.270
Because of that, Nate Silver argues that pollsters
put too much emphasis on the responses of

00:08:52.270 --> 00:08:55.750
college-educated voters, who were more likely
to vote for Clinton.

00:08:55.750 --> 00:09:00.410
By improperly weighting them, they overestimated
her chance of winning.

00:09:00.410 --> 00:09:01.670
Prediction isn’t easy.

00:09:01.670 --> 00:09:03.370
Well making bad predictions is easy.

00:09:03.370 --> 00:09:07.440
I predict that at the end of this episode,
Brandon will bring me 10 German Chocolate

00:09:07.440 --> 00:09:10.120
Cakes and I will eating them with my raccoons.

00:09:10.120 --> 00:09:12.470
But making good predictions is hard.

00:09:12.470 --> 00:09:15.610
And even good predictions can be hard to interpret.

00:09:15.610 --> 00:09:18.670
In order to make accurate predictions a lot
of things need to go right.

00:09:18.670 --> 00:09:21.290
First, we need good, accurate, and unbiased
data.

00:09:21.290 --> 00:09:22.550
And lots of it.

00:09:22.550 --> 00:09:24.330
And second, we need a good model.

00:09:24.330 --> 00:09:26.960
One that takes into account all the important
variables.

00:09:26.960 --> 00:09:32.610
There’s a quote attributed Confucius that
I’m not really sure he said that goes something

00:09:32.610 --> 00:09:38.680
like: To know what you know and what you do
not know, that is true knowledge.

00:09:38.680 --> 00:09:42.930
For example, I know that I don’t know that
he said that, so I am quite knowledgeable.

00:09:42.930 --> 00:09:46.660
There’s great value in knowing what we can
and can’t predict.

00:09:46.660 --> 00:09:50.450
While we shouldn’t stop trying to make good
predictions, there’s wisdom in recognizing

00:09:50.450 --> 00:09:52.680
that we won’t always be able to get it right.

00:09:52.680 --> 00:09:58.360
Knowing what we can’t accurately predict
may be just as important as making accurate predictions.

00:09:58.360 --> 00:10:00.440
Thanks for watching. I'll see you next time.

