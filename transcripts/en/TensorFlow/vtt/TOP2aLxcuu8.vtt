WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.381
[MUSIC PLAYING]

00:00:06.770 --> 00:00:09.230
KARMEL ALLISON: Hi and
welcome to Coding TensorFlow.

00:00:09.230 --> 00:00:11.120
I'm Karmel Allison, and
I'm here to guide you

00:00:11.120 --> 00:00:14.450
through a scenario using
TensorFlow's high-level APIs.

00:00:14.450 --> 00:00:17.380
This video is the second
in a three-part series.

00:00:17.380 --> 00:00:19.940
In this, we'll dig deeper into
preparing the data for machine

00:00:19.940 --> 00:00:21.314
learning, including
using feature

00:00:21.314 --> 00:00:24.399
columns, categorical
data, and much more.

00:00:24.399 --> 00:00:25.940
We'll also explore
a machine learning

00:00:25.940 --> 00:00:29.120
model built using Keras that
can be trained with this data.

00:00:29.120 --> 00:00:32.060
In the previous video, we
spoke about a complex data set

00:00:32.060 --> 00:00:33.500
and how you can
load that and get

00:00:33.500 --> 00:00:35.510
it ready to use in TensorFlow.

00:00:35.510 --> 00:00:38.570
We used the Covertype data set
from the US Forestry Service

00:00:38.570 --> 00:00:40.670
and Colorado State
University, which

00:00:40.670 --> 00:00:43.880
has about 500,000 rows of
geophysical data collected

00:00:43.880 --> 00:00:46.850
from particular regions
in National Forest areas.

00:00:46.850 --> 00:00:49.160
We are going to use the
features in this data set

00:00:49.160 --> 00:00:52.920
to try to predict the soil type
that was found in each region.

00:00:52.920 --> 00:00:55.460
We took the raw data and put
it into a TensorFlow data

00:00:55.460 --> 00:00:59.450
set that generates dictionaries
of feature tensors and labels,

00:00:59.450 --> 00:01:01.340
but we still have
lots of feature types.

00:01:01.340 --> 00:01:03.590
Some are continuous,
some are categorical,

00:01:03.590 --> 00:01:05.239
some are one-hot encoded.

00:01:05.239 --> 00:01:06.980
We need to represent
these in a way that

00:01:06.980 --> 00:01:09.080
is meaningful to an ML model.

00:01:09.080 --> 00:01:10.920
You'll learn how to
do that in this video,

00:01:10.920 --> 00:01:12.780
so let's get started.

00:01:12.780 --> 00:01:14.930
We are going to use
feature columns for that.

00:01:14.930 --> 00:01:18.110
In TensorFlow, a feature column
is a configuration class.

00:01:18.110 --> 00:01:21.110
It doesn't itself hold any
data but it tells our model

00:01:21.110 --> 00:01:24.860
how to transform the raw data so
that it matches the expectation

00:01:24.860 --> 00:01:28.280
in many ML models that the
data is numeric and continuous.

00:01:28.280 --> 00:01:30.590
If you're working with data
that is already numeric,

00:01:30.590 --> 00:01:34.800
image data, for example, feature
columns may not be necessary,

00:01:34.800 --> 00:01:37.040
but for many real-world
applications,

00:01:37.040 --> 00:01:38.720
data is structured
and represents

00:01:38.720 --> 00:01:41.210
vocabularies or human
concepts that we

00:01:41.210 --> 00:01:44.030
need to transform before we can
use them in machine learning

00:01:44.030 --> 00:01:45.020
models.

00:01:45.020 --> 00:01:48.050
Feature columns are a
great way to do that.

00:01:48.050 --> 00:01:50.930
Let's take, for example, our
Covertype category, which

00:01:50.930 --> 00:01:53.420
is an integer between
1 and 7 that represents

00:01:53.420 --> 00:01:55.580
the type of tree in the region.

00:01:55.580 --> 00:01:57.230
You'll note that
all we've done here

00:01:57.230 --> 00:01:59.240
is define a type
of feature, and we

00:01:59.240 --> 00:02:02.450
haven't passed any of our
data into that feature yet.

00:02:02.450 --> 00:02:04.520
It is just a
configuration object

00:02:04.520 --> 00:02:06.410
that will tell our
model to expect

00:02:06.410 --> 00:02:11.000
categorical IDs less than
the outer range value of 8.

00:02:11.000 --> 00:02:13.100
Now we have to
configure how we want

00:02:13.100 --> 00:02:15.920
to transform our categorical
data for use in a model that

00:02:15.920 --> 00:02:17.930
expects continuous data.

00:02:17.930 --> 00:02:19.790
Using feature columns,
we can trivially

00:02:19.790 --> 00:02:22.490
build a set of instructions
that allow the model to convert

00:02:22.490 --> 00:02:26.250
the categories into an
embedding column, as shown here.

00:02:26.250 --> 00:02:28.730
Now, we could have done this
processing in our data parsing

00:02:28.730 --> 00:02:31.250
function ourselves,
converting the categorical IDs

00:02:31.250 --> 00:02:33.380
to a one-hot vector manually.

00:02:33.380 --> 00:02:35.090
The advantage of
using feature columns

00:02:35.090 --> 00:02:37.100
is that the
transformations they encode

00:02:37.100 --> 00:02:39.980
become part of the model's
graph and can therefore be

00:02:39.980 --> 00:02:41.900
exported with the saved model.

00:02:41.900 --> 00:02:43.700
So you should push
any transformations

00:02:43.700 --> 00:02:46.490
that you want to apply to
data both during training

00:02:46.490 --> 00:02:49.400
and at inference time
into feature columns.

00:02:49.400 --> 00:02:52.040
We can define columns
for each of our features.

00:02:52.040 --> 00:02:54.270
Data that is already
numeric is straightforward.

00:02:54.270 --> 00:02:56.360
We just use a numeric column.

00:02:56.360 --> 00:02:59.910
Sometimes, as in the case
of soil type data here,

00:02:59.910 --> 00:03:01.970
data is spread
out over a vector,

00:03:01.970 --> 00:03:04.580
and numeric feature columns
allow us to easily capture

00:03:04.580 --> 00:03:06.740
that relationship with
the shape argument

00:03:06.740 --> 00:03:09.650
so that our model understands
wilderness area as a length 40

00:03:09.650 --> 00:03:13.340
tensor rather than 40
independent tensors.

00:03:13.340 --> 00:03:16.400
All right, so we configure all
of our features, and then what?

00:03:16.400 --> 00:03:18.050
Well, these become
the first layer

00:03:18.050 --> 00:03:20.150
of our model using
a feature layer.

00:03:20.150 --> 00:03:22.070
When we train our
model, this first layer

00:03:22.070 --> 00:03:24.020
will act like any
other Keras layer,

00:03:24.020 --> 00:03:27.260
but its primary role will be to
take in the raw data, including

00:03:27.260 --> 00:03:29.780
the categorical indices,
and transform it

00:03:29.780 --> 00:03:31.430
into the appropriate
representations

00:03:31.430 --> 00:03:33.440
that our neural
net is expecting.

00:03:33.440 --> 00:03:35.870
This layer will also handle
creating and training

00:03:35.870 --> 00:03:38.010
our embedding Covertype.

00:03:38.010 --> 00:03:40.130
So if you have data that
needs transformation

00:03:40.130 --> 00:03:41.820
before it fits into a model--

00:03:41.820 --> 00:03:44.150
maybe it's categorical
like ours or even has

00:03:44.150 --> 00:03:46.100
string names and vocabularies--

00:03:46.100 --> 00:03:48.860
you can use feature columns to
handle those transformations,

00:03:48.860 --> 00:03:51.320
batch by batch, in
TensorFlow, rather

00:03:51.320 --> 00:03:53.570
than having a whole separate
pipeline to do feature

00:03:53.570 --> 00:03:55.430
transformations in memory.

00:03:55.430 --> 00:03:58.220
TensorFlow provides many
feature columns and even ways

00:03:58.220 --> 00:04:00.170
to combine individual
columns into more

00:04:00.170 --> 00:04:04.370
complex representations of the
data that your model can learn.

00:04:04.370 --> 00:04:06.860
So, before we wrap
up, let me quickly

00:04:06.860 --> 00:04:09.950
show you how this would be a
layer in a Keras model, which

00:04:09.950 --> 00:04:12.620
we'll go into in more
detail in the next video.

00:04:12.620 --> 00:04:15.060
Note that we are
using tf.keras here,

00:04:15.060 --> 00:04:17.450
which implements
the Keras API spec

00:04:17.450 --> 00:04:19.610
but adds additional
TensorFlow-specific

00:04:19.610 --> 00:04:21.589
features on top of
it, like support

00:04:21.589 --> 00:04:24.260
for TensorFlow's eager
execution, optimizers,

00:04:24.260 --> 00:04:25.640
and so on.

00:04:25.640 --> 00:04:27.290
Since the first
thing I want to try

00:04:27.290 --> 00:04:29.810
is a simple sequence of
deep learning layers,

00:04:29.810 --> 00:04:32.480
Keras is the easiest
way to start.

00:04:32.480 --> 00:04:34.855
We will start with a
simple sequential model,

00:04:34.855 --> 00:04:36.230
but what I want
to focus on right

00:04:36.230 --> 00:04:38.300
now is just this first layer.

00:04:38.300 --> 00:04:39.900
Our first layer
is a feature layer

00:04:39.900 --> 00:04:41.900
that will do all the data
transformation we just

00:04:41.900 --> 00:04:44.150
discussed and feed
the transformed data

00:04:44.150 --> 00:04:46.430
into the rest of the model.

00:04:46.430 --> 00:04:48.830
We'll do that in part three
of this series, where we'll

00:04:48.830 --> 00:04:52.080
look at adding the data and
training the model with it,

00:04:52.080 --> 00:04:54.830
including choosing loss
functions and optimizers.

00:04:54.830 --> 00:04:57.230
It will be right here on the
TensorFlow YouTube channel,

00:04:57.230 --> 00:04:59.270
so don't forget to hit
that Subscribe button,

00:04:59.270 --> 00:05:00.410
and I'll see you there.

00:05:00.410 --> 00:05:03.460
[MUSIC PLAYING]

