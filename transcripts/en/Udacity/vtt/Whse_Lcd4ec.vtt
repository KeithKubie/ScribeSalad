WEBVTT
Kind: captions
Language: en

00:00:00.310 --> 00:00:03.510
The second source of latency, as I mentioned,

00:00:03.510 --> 00:00:06.689
is the kernel preemption latency. Now how do

00:00:06.689 --> 00:00:10.240
we reduce the kernel preemption latency? Timer goes

00:00:10.240 --> 00:00:12.110
off when the kernel is in the middle of

00:00:12.110 --> 00:00:16.660
something and so we have to wait for the kernel to be ready to take that

00:00:16.660 --> 00:00:21.950
interrupt. The first approach is the explicitly insert

00:00:21.950 --> 00:00:25.330
preemption points in the kernel. So that it can

00:00:25.330 --> 00:00:28.470
actually look for events that may have gone off

00:00:28.470 --> 00:00:31.000
and take action. So that is the first approach.

00:00:31.000 --> 00:00:34.840
The second approach is to allow preemption of the

00:00:34.840 --> 00:00:40.130
kernel anytime the kernel is not manipulating shared data structures.

00:00:40.130 --> 00:00:42.020
Now the reason why you don't want the kernel

00:00:42.020 --> 00:00:44.820
to be preempted is because it is manipulating some

00:00:44.820 --> 00:00:47.330
shared data structures and if you preempt that that

00:00:47.330 --> 00:00:50.500
can result in some race conditions in the kernel. Bad

00:00:50.500 --> 00:00:54.800
news. So what we want to do is, we want to make sure that the kernel is

00:00:54.800 --> 00:00:57.800
not preempted while it is manipulating shared data

00:00:57.800 --> 00:01:00.270
structures. But if it is not, then we

00:01:00.270 --> 00:01:05.050
want to allow preemption. So these are two different approaches that can be

00:01:05.050 --> 00:01:09.870
combined in order to reduce the latency associated with kernel

00:01:09.870 --> 00:01:15.586
preemption. A technique, due to Robert Love, called the lock-breaking

00:01:15.586 --> 00:01:19.940
preemptible kernel combines these two ideas. And by

00:01:19.940 --> 00:01:23.710
combining these two ideas it reduces the spin lock

00:01:23.710 --> 00:01:27.020
holding time in the kernel. The idea is

00:01:27.020 --> 00:01:29.960
the following. So there will be a long critical

00:01:29.960 --> 00:01:32.530
section in the kernel where in it is

00:01:32.530 --> 00:01:35.910
manipulating some shared data structures, but it also doing

00:01:35.910 --> 00:01:38.330
other things within this critical section. So what

00:01:38.330 --> 00:01:41.120
we're going to do is apply these two principles and

00:01:41.120 --> 00:01:44.480
break this long critical section as follows.

00:01:44.480 --> 00:01:48.030
So we're going to change this critical section code

00:01:48.030 --> 00:01:51.740
into two sections: you acquire the lock, you

00:01:51.740 --> 00:01:54.560
manipulate the shared data structures, and you release

00:01:54.560 --> 00:01:56.720
the lock. And the reason you're releasing the

00:01:56.720 --> 00:01:58.820
lock is because you're done with manipulating the

00:01:58.820 --> 00:02:02.500
shared data structures, and then we will reacquire

00:02:02.500 --> 00:02:06.300
the lock, and finally release it. So essentially

00:02:06.300 --> 00:02:09.710
we replaced this long critical section by two

00:02:09.710 --> 00:02:14.200
shorter critical sections, one manipulating shared data and the

00:02:14.200 --> 00:02:16.690
rest of the code here. What is the advantage

00:02:16.690 --> 00:02:19.320
of doing this? What we can do is, at

00:02:19.320 --> 00:02:22.450
this point, once we are done with manipulating

00:02:22.450 --> 00:02:25.300
shared data, we release the lock and at this

00:02:25.300 --> 00:02:29.360
point we can preempt the kernel safetly. And since

00:02:29.360 --> 00:02:31.780
we can preempt the kernel at this point it's

00:02:31.780 --> 00:02:35.270
a great opportunity to check for expired timers

00:02:35.270 --> 00:02:37.920
at this point. If there are expired timers,

00:02:37.920 --> 00:02:41.450
dispatch them, reprogram one-shot timers if need be.

00:02:41.450 --> 00:02:43.730
All of that stuff can be done at this

00:02:43.730 --> 00:02:46.500
point, and then you can come back, reacquire

00:02:46.500 --> 00:02:49.560
the lock, and continue with the critical section.

00:02:49.560 --> 00:02:51.950
So that's the idea of the lock breaking

00:02:51.950 --> 00:02:57.460
preemptible kernel that combines these two ideas of explicit

00:02:57.460 --> 00:03:02.240
insertion of preemption points and allowing preemption any time

00:03:02.240 --> 00:03:05.840
the kernel is not manipulating shared data structures. The

00:03:05.840 --> 00:03:10.040
third source of time latency I mentioned is the

00:03:10.040 --> 00:03:14.320
scheduling latency, that is, the timer event goes off,

00:03:14.320 --> 00:03:16.040
and we want to schedule the app that is

00:03:16.040 --> 00:03:18.060
going to deal with the timer event as quickly

00:03:18.060 --> 00:03:20.790
as possible, but the scheduler is in the way.

00:03:20.790 --> 00:03:22.590
How do we quickly make sure that that app

00:03:22.590 --> 00:03:29.485
gets scheduled? Firm timer implementation in TS Linux uses a combination of two

00:03:29.485 --> 00:03:34.080
principles. One is called Proportional Period Scheduling

00:03:34.080 --> 00:03:36.430
and in Proportional Period Scheduling what is

00:03:36.430 --> 00:03:42.140
being done is that, when a task, so these things represent tasks T1

00:03:42.140 --> 00:03:48.370
and T2, when a task starts up it is going to say that it needs

00:03:48.370 --> 00:03:53.090
a certain proportion of the CPU time to be allocated to it in every

00:03:53.090 --> 00:03:57.840
time quantum. So there are two parameters associated with proportional period

00:03:57.840 --> 00:04:03.420
scheduling, Q and T. T is a time window, a time

00:04:03.420 --> 00:04:09.450
quantum, and this is the time quantum that is exposed to an application.

00:04:09.450 --> 00:04:14.160
And the application can say, within the time quantum T, I

00:04:14.160 --> 00:04:19.230
need a certain proportion of the time for my task. So T1

00:04:19.230 --> 00:04:24.240
might say that, in any time T, I need two thirds of the time to be

00:04:24.240 --> 00:04:29.810
devoted to my task and if another task, T2, says in any time

00:04:29.810 --> 00:04:35.180
period T, I need one-third of the CPU to be devoted to my task,

00:04:35.180 --> 00:04:39.845
then these two requests can be obviously satisfied by

00:04:39.845 --> 00:04:46.330
TS Linux because the two add to the periodicity of scheduling T. And this is

00:04:46.330 --> 00:04:50.200
the idea behind proportional period scheduling, in which

00:04:50.200 --> 00:04:53.020
what the scheduler does is admission control at

00:04:53.020 --> 00:04:58.640
the time that a process starts up. If it asks for a certain proportion of time

00:04:58.640 --> 00:05:01.460
it sees whether it is possible to satisfy

00:05:01.460 --> 00:05:05.290
that request. So, for example, if already the

00:05:05.290 --> 00:05:11.700
scheduler has promised T1 two-thirds of the time, and T2 comes along and says I

00:05:11.700 --> 00:05:13.290
also need two-thirds of the time in

00:05:13.290 --> 00:05:16.480
every period TS Linux is going to say uh-huh,

00:05:16.480 --> 00:05:19.220
no, cannot do that, because it doesn't

00:05:19.220 --> 00:05:22.800
have the capacity to accommodate both these requests

00:05:22.800 --> 00:05:26.050
simultaneously. So that's the idea behind Proportional

00:05:26.050 --> 00:05:30.640
Period Scheduling. So it provides temporal protection by

00:05:30.640 --> 00:05:37.490
allocating each task a fixed proportion of the CPU during each task

00:05:37.490 --> 00:05:43.210
period T. And both this Q and T parameters are adjustable parameters using a

00:05:43.210 --> 00:05:46.060
feedback control mechanism. And so this,

00:05:46.060 --> 00:05:49.560
in essence, improves the accuracy of scheduling

00:05:49.560 --> 00:05:51.800
analysis that you can do on behalf

00:05:51.800 --> 00:05:55.990
of processes that are time-sensitive. The second

00:05:55.990 --> 00:06:02.630
technique that is used in TS Linux for reducing scheduling latency is to use

00:06:02.630 --> 00:06:09.290
priority scheduling. And let me motivate that by introducing a problem

00:06:09.290 --> 00:06:15.750
that plagues real time tasks and that is what is called priority inversion.

00:06:15.750 --> 00:06:21.240
So here is a high priority task C1 and it needs some service.

00:06:21.240 --> 00:06:26.240
And it gets that service by calling a server. And this call is a blocking

00:06:26.240 --> 00:06:31.730
call to the server. And the server itself may be a low priority server.

00:06:31.730 --> 00:06:36.800
For example, this client may contact a window manager to

00:06:36.800 --> 00:06:39.730
say, I need a portion of the window and this

00:06:39.730 --> 00:06:41.360
is what I want you to do in terms of

00:06:41.360 --> 00:06:43.616
painting that portion of the window. That might be a

00:06:43.616 --> 00:06:46.244
call that a high priority task is making to a

00:06:46.244 --> 00:06:49.091
low priority server and this is a blocking call

00:06:49.091 --> 00:06:51.500
and till the server is done with its work

00:06:51.500 --> 00:06:55.216
the high priority task cannot continue execution. So if

00:06:55.216 --> 00:06:58.472
you look at the timeline, C1 is running and

00:06:58.472 --> 00:07:01.464
it makes a blocking call at this point and

00:07:01.464 --> 00:07:04.986
the server takes over. And this is the service

00:07:04.986 --> 00:07:08.626
time for the server to execute the blocking call

00:07:08.626 --> 00:07:11.574
made by C1. So at the end of this service

00:07:11.574 --> 00:07:17.300
time, C1 is ready to run again. But not so fast. It could be

00:07:17.300 --> 00:07:22.380
that during the service time of this low priority server, on behalf of

00:07:22.380 --> 00:07:29.695
C1, some of the higher priority tasks C2, which perhaps was waiting for some

00:07:29.695 --> 00:07:32.850
I/O completion, becomes runnable again. If it

00:07:32.850 --> 00:07:36.960
becomes runnable again, then this higher priority

00:07:36.960 --> 00:07:40.370
task, compared to this server, is going to

00:07:40.370 --> 00:07:43.310
preempt the server and take over the CPU.

00:07:43.310 --> 00:07:46.120
So what happens at this point is that

00:07:46.120 --> 00:07:49.300
this medium priority task, because it is higher

00:07:49.300 --> 00:07:52.860
than the servers priority, it's going to take over,

00:07:52.860 --> 00:07:56.150
preempting the server, and that is essentially a

00:07:56.150 --> 00:07:59.260
priority inversion as far as C1 is concerned

00:07:59.260 --> 00:08:02.500
because C1 is higher in priority than C2.

00:08:02.500 --> 00:08:06.570
But unfortunately, at this point of time, the

00:08:06.570 --> 00:08:09.360
server is the one that is scheduled and

00:08:09.360 --> 00:08:12.320
that is lower priority than either of these

00:08:12.320 --> 00:08:16.530
two guys. And therefore, C2 happily preempts the

00:08:16.530 --> 00:08:21.300
server and takes over the CPU. But from the point of view of C1, that's a

00:08:21.300 --> 00:08:24.954
priority inversion. And it is a time-sensitive task

00:08:24.954 --> 00:08:29.970
that affects the sensitivity of the time-sensitive tasks.

00:08:29.970 --> 00:08:35.990
This is where the priority-based scheduling of TSL saves the day for us.

00:08:35.990 --> 00:08:42.640
Basically, the idea is that when C1 makes a request to the server, the server

00:08:42.640 --> 00:08:48.710
is going to assume the priority of C1. Even though normally, it has a static

00:08:48.710 --> 00:08:55.020
priority which is lower than the priority of C1. Because C1 is making this call,

00:08:55.020 --> 00:08:59.520
the server's priority is going to be boosted to be the priority of C1

00:08:59.520 --> 00:09:04.560
itself. So for the duration of the service time of the server the

00:09:04.560 --> 00:09:09.890
priority of the server task is going to be the same as the priority

00:09:09.890 --> 00:09:14.910
of C1, which is higher. Now, C2, when it becomes

00:09:14.910 --> 00:09:20.400
runnable, it cannot preempt the server because the server is now running

00:09:20.400 --> 00:09:26.780
at the priority of C1. And therefore, we avoid the priority inversion that

00:09:26.780 --> 00:09:29.115
would have normally happened because of the

00:09:29.115 --> 00:09:32.720
priority-based scheduling in TS linux, so the

00:09:32.720 --> 00:09:34.640
upside is that there will be no

00:09:34.640 --> 00:09:38.710
priority inversion with this priority-based scheduling mechanism

00:09:38.710 --> 00:09:46.170
that is there in TS linux. So to recap, TS linux avoids scheduling latency

00:09:46.170 --> 00:09:52.740
to shrink the distance between the event happening and event activation by doing

00:09:52.740 --> 00:09:55.915
this admission control through the proportion period

00:09:55.915 --> 00:10:00.530
scheduler and also it avoids priority inversion

00:10:00.530 --> 00:10:03.070
by using this priority-based scheduling. So

00:10:03.070 --> 00:10:05.900
these two mechanisms allow shrinking that distance

00:10:05.900 --> 00:10:08.690
as well. The other advantage of the

00:10:08.690 --> 00:10:12.990
Proportion Period Scheduling is that TS Linux

00:10:12.990 --> 00:10:19.560
can have a control over how much of the CPU time is devoted to time-sensitive

00:10:19.560 --> 00:10:21.580
tasks so that it can reserve a

00:10:21.580 --> 00:10:25.580
portion of the time for throughput-oriented tasks. So

00:10:25.580 --> 00:10:31.420
for instance, it could say, within any period T, I'm going to reserve a third of

00:10:31.420 --> 00:10:35.330
the time for throughput-oriented tasks, so that

00:10:35.330 --> 00:10:38.790
even if there are time-sensitive tasks running throughput-oriented

00:10:38.790 --> 00:10:43.260
tasks are going to get their dibs for running on the CPU. And that way, we can

00:10:43.260 --> 00:10:50.320
make sure that while supporting the timeliness of time sensitive tasks, TS-Linux

00:10:50.320 --> 00:10:54.820
can also ensure that throughput oriented tasks

00:10:54.820 --> 00:10:57.050
are able to make forward progress. So

00:10:57.050 --> 00:11:00.795
the three ideas that are enshrined in

00:11:00.795 --> 00:11:04.000
TS Linux, for dealing with time-sensitive tasks are,

00:11:04.000 --> 00:11:10.030
first of all, coming up with the firm timer design that increases the

00:11:10.030 --> 00:11:15.980
accuracy of the timer without exorbitant overhead in dealing with

00:11:15.980 --> 00:11:21.830
one shock timer interrupts. Second, using a preemptible kernel to reduce

00:11:21.830 --> 00:11:25.140
the kernel preemption latency and third,

00:11:25.140 --> 00:11:29.160
using priority-based scheduling to avoid priority

00:11:29.160 --> 00:11:35.282
inversion and guaranteeing a portion of the CPU time to be allowed for

00:11:35.282 --> 00:11:38.760
time-sensitive tasks. Those are the ways

00:11:38.760 --> 00:11:42.600
by which the distance between event happening

00:11:42.600 --> 00:11:48.055
and event activation can be deduced and we can get good performance for

00:11:48.055 --> 00:11:51.290
time-sensitive applications even though the operating system

00:11:51.290 --> 00:11:53.425
is a commodity operating system like Linux.

