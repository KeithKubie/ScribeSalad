WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:05.089
&gt;&gt; AGARWAL: My name is Amit Agarwal. I'm a
product manager at Google. And today, I'm

00:00:05.089 --> 00:00:10.710
excited to talk about two new products that
we announced yesterday. The first one is BigQuery

00:00:10.710 --> 00:00:16.920
and the second one is Prediction API. We hope
both of these products will allow you to get

00:00:16.920 --> 00:00:26.999
more from your data with Google. Before we
deep dive into the session, we have this covered

00:00:26.999 --> 00:00:34.040
on Google Wave where you can view notes and
ask us any questions. We have a bitly link

00:00:34.040 --> 00:00:41.680
here, it's http://bit.ly/dr01ws, once again,
dr01ws. Organization and businesses today

00:00:41.680 --> 00:00:46.670
live and breathe data. Data is being used
for the variety of different purposes such

00:00:46.670 --> 00:00:54.500
as understanding and predicting, user traffic
and trends, improve supply chain management,

00:00:54.500 --> 00:01:02.470
better product marketing, sentiment analysis,
and the list pretty much goes on and on and

00:01:02.470 --> 00:01:10.120
on. The other increasing walling of data,
a couple of the need for more real-time or

00:01:10.120 --> 00:01:18.280
near real-time processing of data and analyzing
the data interactively is what makes this

00:01:18.280 --> 00:01:27.060
really very challenging. At Google, we have
built a business on storing, analyzing, and

00:01:27.060 --> 00:01:37.520
understanding big data. This is clearly reflecting
the work that we do, for example, in the relevancy

00:01:37.520 --> 00:01:44.280
of ads, in the precision of a web search results,
and of course a global infrastructure. Today,

00:01:44.280 --> 00:01:51.280
what we'd like to do is to offer the same
data analytics capability and make them available

00:01:51.280 --> 00:01:58.430
to all of you. Thousands of Googlers have
been using it everyday and now you can too.

00:01:58.430 --> 00:02:04.760
I'm happy to announce two new products, BigQuery
and Prediction API. BigQuery allows you to

00:02:04.760 --> 00:02:10.360
interactively analyze large datasets so it
literally helps you find the needle in the

00:02:10.360 --> 00:02:18.800
haystack where the haystack can be 10, 20,
50, hundred billion, or even a trillion drivers.

00:02:18.800 --> 00:02:25.530
The next move, we're going to Prediction API.
So while BigQuery allows you to analyze historic

00:02:25.530 --> 00:02:30.360
datasets, Prediction API allows you to dig
these datasets and predict future outcomes.

00:02:30.360 --> 00:02:38.460
It's based on the underlying technology on
machine learning. To top it all, both of these

00:02:38.460 --> 00:02:43.980
products are offered as web services. So what
does that mean? Well, they run on Google infrastructure,

00:02:43.980 --> 00:02:50.620
they're running on thousands of machines everyday.
So you really don't have to worry about troubleshooting

00:02:50.620 --> 00:02:57.370
machines, making sure that services are up
and running, or scaling it. We take care of

00:02:57.370 --> 00:03:06.730
all of these. So what do you guys do? Well,
you use it. So what we hope is both these

00:03:06.730 --> 00:03:13.430
products will enable us to get Google's deep
data analytics capability to all of you. And

00:03:13.430 --> 00:03:19.470
we hope you guys will use it in any and every
business. So what are some of the benefits

00:03:19.470 --> 00:03:25.930
of these two products? We talk a little bit
about Google technology. Thousands of Googlers

00:03:25.930 --> 00:03:30.959
use it everyday. This is not something we
just got it right now. They've been using

00:03:30.959 --> 00:03:36.700
it over a multiple years. I wouldn't touch
much along scalability. Today, we take it

00:03:36.700 --> 00:03:42.300
for granted, running analysis on terabytes
of datasets, trillions of those, and getting

00:03:42.300 --> 00:03:48.000
the results in seconds. We'll touch more up
on security and sharing as the presentation

00:03:48.000 --> 00:03:53.060
goes on, but we pretty much get very similar
security and sharing as you guys are used

00:03:53.060 --> 00:03:58.640
to with all Google products. And lastly, we
offer a very easy integration with other Google

00:03:58.640 --> 00:04:07.129
products such as Google App Engine and also
Google Spreadsheets. So now you guys must

00:04:07.129 --> 00:04:14.021
be wondering, "How can we use your data with
BigQuery and Prediction API?" Well, it's very,

00:04:14.021 --> 00:04:21.760
very simple. It's a three-step process; upload,
process, and act. The first part is getting

00:04:21.760 --> 00:04:30.120
a data into Google. We announced yesterday,
Google Storage for developers. We have a bunch

00:04:30.120 --> 00:04:35.080
of APIs, some tools called GSUtil, and you
can pretty much use any of the compatible

00:04:35.080 --> 00:04:41.250
tools to get your dataset into Google. The
next step in the process is actually processing

00:04:41.250 --> 00:04:47.020
a data. So once you import your large datasets,
you want to take that data and import into

00:04:47.020 --> 00:04:52.610
tables for BigQuery. Or if you're looking
at Prediction API, you want to train a model

00:04:52.610 --> 00:04:57.851
based on the dataset. Once the processing
is over, pretty much you're all ready to go.

00:04:57.851 --> 00:05:05.130
So in the case of BigQuery you can start issuing
queries against the tables that you just built.

00:05:05.130 --> 00:05:10.860
In the case of Prediction API, you can make
predictions based on the model that you just

00:05:10.860 --> 00:05:17.130
trained, so very, very simple. Some of you
guys here must be having questions on security

00:05:17.130 --> 00:05:24.080
and privacy. Well, we take both of these very
seriously. First of all, for both of these

00:05:24.080 --> 00:05:31.000
products, we offer SSL access so that data
is secured within transit. And also, it's

00:05:31.000 --> 00:05:38.410
you who owns your data, your model, and your
tables, so you actually decide who actually

00:05:38.410 --> 00:05:45.820
gets to use your data, your tables, your model.
Of course, Google really offers flexible and

00:05:45.820 --> 00:05:53.090
easy user and group-based ACLs with which
you can decide to share the tables, the data,

00:05:53.090 --> 00:05:57.650
and the model. So once again, just recapping
once again, it's a very, very simple process;

00:05:57.650 --> 00:06:04.070
upload, process, and act. Now that you guys
understand a high-level view of what we are

00:06:04.070 --> 00:06:10.030
doing here, how you guys use the system, let's
deep dive into the details. I'm going to invite

00:06:10.030 --> 00:06:15.979
Siddartha Naidu, he's from the BigQuery team.
He'll be deep diving into BigQuery, Siddarth.

00:06:15.979 --> 00:06:23.230
&gt;&gt; NAIDU: Hi, Amit. Thank you. Hi, everyone.
As Amit introduced me, I'm Siddartha Naidu

00:06:23.230 --> 00:06:28.280
and I work with the BigQuery team and I'm
really excited to be able to share what we've

00:06:28.280 --> 00:06:34.780
built with you today. So let me get started.
I want to spend a little time motivating the

00:06:34.780 --> 00:06:40.850
product and then diving into what exactly
it is and how you can go about using it. So

00:06:40.850 --> 00:06:45.410
big data is now everywhere, there's lot of
it, all of us want to analyze it, and there's

00:06:45.410 --> 00:06:51.470
a lot of value in it. But the basic problem
that it presents is scale. And a lot of the

00:06:51.470 --> 00:06:59.590
normal tools that we use become either [INDISTINCT]
or often useless. So there have been frameworks

00:06:59.590 --> 00:07:04.590
built to make this more manageable to run
your processing across multiple machines.

00:07:04.590 --> 00:07:11.240
We use MapReduce at Google. We love it. It's
powerful. It's sophisticated. But it's also

00:07:11.240 --> 00:07:17.180
complex. It's also generally used in a batch-oriented
way. You either run a [INDISTINCT] job, you

00:07:17.180 --> 00:07:23.730
run a [INDISTINCT] job, you can, you know,
run it and get a cup of coffee and come back.

00:07:23.730 --> 00:07:28.420
I mean if you're lucky, that's about how fast
it runs. The things you can do to make it

00:07:28.420 --> 00:07:33.960
simpler, you can add layers to simplify how
you access it, how you drive it. But what

00:07:33.960 --> 00:07:40.950
we want to talk about today is a product that
is exceedingly simple and really fast. Where

00:07:40.950 --> 00:07:48.760
do we find this useful? Well, within Google,
we found a lot of use-cases. And frankly,

00:07:48.760 --> 00:07:55.020
this slide doesn't do justice. I just listed
a couple out there and, you know, we are able

00:07:55.020 --> 00:08:00.250
to build all sorts of useful tools, all sorts
of useful applications on top of this technology.

00:08:00.250 --> 00:08:07.440
And I want to pick one particular example
that we can use to understand it. Take, for

00:08:07.440 --> 00:08:11.310
example, if you're running a large fleet of
computers. And all of these computers are

00:08:11.310 --> 00:08:14.013
connected to each other. Now, one of the things
you want to do is make sure that they're all

00:08:14.013 --> 00:08:18.890
healthy and working as expected. They can
all see each other. So you monitor the network.

00:08:18.890 --> 00:08:21.670
You're monitoring various properties of the
connections between them. You're pinging different

00:08:21.670 --> 00:08:27.190
host periodically. And you're recording all
of this. So you have this large amount of

00:08:27.190 --> 00:08:31.280
data that you've recorded and now you want
to analyze. And here is where a tool like

00:08:31.280 --> 00:08:35.570
this would be useful, right? So you're having
a problem and you want to diagnose the problem.

00:08:35.570 --> 00:08:39.690
You don't want to be running jobs, waiting
for like half an hour, an hour for things

00:08:39.690 --> 00:08:46.040
to run. You want to explore, quickly find
out what's going on. Now, as luck would have

00:08:46.040 --> 00:08:51.840
it, there is a group called M-Lab, it's an
open collaborative project that's doing sort

00:08:51.840 --> 00:08:57.060
of this kind of data collection for their
Internet. Their goal is to understand the

00:08:57.060 --> 00:09:01.480
performance of the public Internet, understand
the performance of broadband usage. And what

00:09:01.480 --> 00:09:06.150
they have done is they have collected data
from a number of nodes distributed around

00:09:06.150 --> 00:09:11.880
the Internet, pinging hosts, and measuring
how good the performance is, how good the

00:09:11.880 --> 00:09:15.690
connectivity is whether there are issues and
so on. And what we've done is import their

00:09:15.690 --> 00:09:21.100
data into BigQuery so that they can analyze
the data. And I'm going to take you for a

00:09:21.100 --> 00:09:27.310
little demo to their data. So here I switch
to the demo. And let me just describe what

00:09:27.310 --> 00:09:32.670
you're seeing. It's a little web application
that I wrote via an API, so in a little--basic

00:09:32.670 --> 00:09:37.630
UI on top of it. I'm not proud of the UI,
but that's okay. Well, what we are focused

00:09:37.630 --> 00:09:41.920
on is being able to run some queries against
the data. So let's talk with the first thing.

00:09:41.920 --> 00:09:46.270
Let's see how big this data. How much did
they actually collect? And what--I have down

00:09:46.270 --> 00:09:50.720
here is my cheat sheet because I don't want
to type too much under pressure. There's not

00:09:50.720 --> 00:09:57.500
a whole lot to type, but let's go ahead and
count up the size of our data. So that's a

00:09:57.500 --> 00:10:05.330
big number. That's 60 billion rows of data.
And we just counted it up. Sixty billion is

00:10:05.330 --> 00:10:11.190
a big number. A US Census--I mean the US Census
has collected data this year. Well, if you

00:10:11.190 --> 00:10:17.810
have hundred times that data, it still wouldn't
quite be, you know 60 billion records. So

00:10:17.810 --> 00:10:21.080
that's a big number. And hopefully, we can
do more than count it. Because if, you know,

00:10:21.080 --> 00:10:27.820
all you can do is count it, that's not going
to really do--you know, buy us all that much.

00:10:27.820 --> 00:10:35.530
So here is another query I'm going run. And
let me tell you what I'm trying to do. So

00:10:35.530 --> 00:10:41.270
what they measure is a ping and they measure
the TCP timeouts that occur against each ping

00:10:41.270 --> 00:10:45.110
so that they can see if the hosts are having
connectivity problems. Timeout is one of the

00:10:45.110 --> 00:10:49.530
things that get measured. But, before I could
explain like the full query to you, we already

00:10:49.530 --> 00:10:54.260
got that answered. This is computed over those
same 60 billion rows. We went over them, aggregated

00:10:54.260 --> 00:10:59.970
them up, found their codes where there were
timeouts, counted up all those timeouts, group

00:10:59.970 --> 00:11:05.910
them by host and found the top 10 offending
or top 20 host that had problems, right? And

00:11:05.910 --> 00:11:10.270
I also normalized it and so on so that it's
really, you know, it's the correct number

00:11:10.270 --> 00:11:16.220
not just the most frequent host pinged or
anything, so all of that on 60 billion rows

00:11:16.220 --> 00:11:24.380
while I was talking to you. So let me--I want
to show you more but I just want to recap

00:11:24.380 --> 00:11:29.440
what we've seen so far, give you a couple
of the details and then we can spend a little

00:11:29.440 --> 00:11:34.500
more time exploring data. So BigQuery is a
scalable system. It scales to billions of

00:11:34.500 --> 00:11:41.970
rows, many billions of rows, and it's fast.
It's designed to be interactively fast. You

00:11:41.970 --> 00:11:45.060
can explore your data, you can formulate queries,
you can fix your queries and run them again.

00:11:45.060 --> 00:11:49.770
It's fast enough to do all of that. And you
probably recognize what I was typing there,

00:11:49.770 --> 00:11:55.660
it was SQL. The language of interaction is
SQL. It's familiar. And all of this is wrapped

00:11:55.660 --> 00:12:01.760
up and exposed via a Web API. I'm making it
available as a Web API, makes it easy to use

00:12:01.760 --> 00:12:05.029
over a number of different places, over a
number of different applications in the way

00:12:05.029 --> 00:12:11.610
that's most convenient. Amit covered sort
of the brief framework. I want to cover a

00:12:11.610 --> 00:12:16.461
couple details about the BigQuery portion
of like using the system end-to-end. You upload

00:12:16.461 --> 00:12:20.720
your data to Google Storage for developers.
That part is the same. Now, you import your

00:12:20.720 --> 00:12:26.700
data into BigTable--sorry--into BigQuery table.
And what that does is allow us to put it in

00:12:26.700 --> 00:12:30.550
a format that is efficient to run our queries
on so that we can, you know, handle all your

00:12:30.550 --> 00:12:34.530
queries. One important aspect of simplicity
I want to call out here is you don't need

00:12:34.530 --> 00:12:37.330
to worry about indexes. You don't need to
worry about sharding your data. You don't

00:12:37.330 --> 00:12:41.180
need to worry about partitioning your data
in ways that are efficient for access. Just

00:12:41.180 --> 00:12:46.090
point us at the full dataset and say, you
know, import it and we'll take care of those

00:12:46.090 --> 00:12:51.850
details. Secondly, you query it. And again,
there's an aspect of querying out here that

00:12:51.850 --> 00:12:57.400
is simple. You don't have to worry about provisioning
machines like, commonly, there's an issue

00:12:57.400 --> 00:13:01.800
of like, you know, do I need 100, 200, 500
machines to run my thing? Do I need the machines

00:13:01.800 --> 00:13:06.210
to be close to my data? All of that is hidden
from you. You construct your queries, send

00:13:06.210 --> 00:13:11.140
them to BigQuery. We will run them across
your data and give you an answer as quickly

00:13:11.140 --> 00:13:19.380
as we can. The queries themselves, well, we've
chosen SQL because it's familiar and we use

00:13:19.380 --> 00:13:24.760
the subset of SQL that's useful for analyzing
data, stuff that you can slice, filter, aggregate

00:13:24.760 --> 00:13:31.940
by all the common operations that, you know,
normally occur. And we have a bunch of functions,

00:13:31.940 --> 00:13:37.400
the usual that you expect, mathematical functions,
time functions, that idea of things to massage

00:13:37.400 --> 00:13:41.920
data into formats like human readable, or
to extract, you know, parts of your deals

00:13:41.920 --> 00:13:49.830
that you need to group by or order by. One
additional thing we've added which is a little

00:13:49.830 --> 00:13:53.600
different is statistical versions of certain
functions. So when you're dealing with very

00:13:53.600 --> 00:13:58.790
large datasets, sometimes it's useful to trade
a little bit of accuracy for a lot of speed.

00:13:58.790 --> 00:14:02.580
And so, what we have introduced as a couple
of statistical operators that allows you to

00:14:02.580 --> 00:14:07.760
do common operations like top most, frequent,
or count distinct, and so on. So these are

00:14:07.760 --> 00:14:16.550
a sort of optimization features that are available.
What about API? Well, the API is a standard

00:14:16.550 --> 00:14:22.730
restful API. We have an endpoint that allows
you to find out what the scheme of your table

00:14:22.730 --> 00:14:28.450
is with--once when you've imported your data.
So it'll come back with a list of fields and

00:14:28.450 --> 00:14:32.460
their types, and then, endpoint for queries.
You pass it to your query, referring to your

00:14:32.460 --> 00:14:36.061
tables in the SQL statement, and then you--it'll
come back with an answer. You can see there

00:14:36.061 --> 00:14:41.870
the first--there's a little header section
that describes the result set and then, an

00:14:41.870 --> 00:14:48.810
entry for each row. There's also a corresponding
JSON-RPC interface of mapping from field to--I

00:14:48.810 --> 00:14:53.661
mean, sorry, address endpoint to JSON method
and mapping for parameters. You can use that

00:14:53.661 --> 00:15:00.760
if it's more convenient. And a few words about
security and privacy. The API is protected

00:15:00.760 --> 00:15:08.370
with the standard Google authentication mechanisms.
We accept all the common Google credentials.

00:15:08.370 --> 00:15:14.230
We use HTTPS to protect your credentials on
the way in and your data on the way back out.

00:15:14.230 --> 00:15:19.170
And finally, we expose the tables as objects
in Google Storage for developers' namespace,

00:15:19.170 --> 00:15:25.960
you know, in the namespace of Google Storage.
And you can manage your ACLs through the same

00:15:25.960 --> 00:15:31.800
tools that you use for Google Storage. You--by
default, we create the tables with private

00:15:31.800 --> 00:15:36.580
access so that only you can access and query
those tables. But you can add other owners

00:15:36.580 --> 00:15:42.480
or other people who can access and manage
your data, or query your data as you see fit

00:15:42.480 --> 00:15:48.560
for your application. So now, enough with
the description, let's get back to playing

00:15:48.560 --> 00:15:53.000
with some more data or working with some more
data. Now, a very common--many applications

00:15:53.000 --> 00:15:59.360
these days are generating a lot of data to
user-generated content. And Wikipedia is a

00:15:59.360 --> 00:16:04.430
fantastic example of that. What they've been
doing is recording the history of their articles

00:16:04.430 --> 00:16:09.850
since sort of their inception in a database.
And they've collected the whole history of

00:16:09.850 --> 00:16:16.040
edits to each article of each page on the
Wikipedia site. We've downloaded that data

00:16:16.040 --> 00:16:24.160
from their public site and imported it into
a BigQuery table so that we can analyze the

00:16:24.160 --> 00:16:29.130
Wikipedia data. Now before I get to that,
I've mentioned that there's an API that can

00:16:29.130 --> 00:16:35.170
embed in different places. So, I actually
like this shell access to our BigQuery tool

00:16:35.170 --> 00:16:40.040
a lot. I've used the Brian M.--Brian Clapper
SQL command tool. It's a nice cross-platform

00:16:40.040 --> 00:16:45.300
Python command line for accessing databases.
And what we've done is written a Python DB-API

00:16:45.300 --> 00:16:52.490
driver around BigQuery so that we can just
plug it straight into his tool and it can

00:16:52.490 --> 00:17:03.360
talk to BigQuery tables. So let's go over
and fire up our demo. So that's the tool.

00:17:03.360 --> 00:17:10.870
And this is the table that we're going to
work with today. It was the data imported

00:17:10.870 --> 00:17:18.540
earlier this year, a little bit--a little
while ago. So, I want to just list the fields.

00:17:18.540 --> 00:17:27.440
Hopefully this will come true. Well, we seem
to be connect [INDISTINCT]. Oh, there you

00:17:27.440 --> 00:17:47.360
go, okay, probably just a hiccup in connectivity.
Anyway, so the table has a couple of fields

00:17:47.360 --> 00:17:50.270
describing properties of the division, the
title, the database device which namespace

00:17:50.270 --> 00:17:56.470
of Wikipedia, it lived in and so on. And the
first--once we had this data in, the first

00:17:56.470 --> 00:18:01.540
query that occurred to me was to look for
the most frequently edited articles in Wikipedia.

00:18:01.540 --> 00:18:05.251
Now, I can define a little shortcut for the
tables, so I don't have to type quite as much.

00:18:05.251 --> 00:18:13.790
And let's do that. So let's look at the top
five most edited articles in Wikipedia. Now,

00:18:13.790 --> 00:18:22.110
if--yeah, great. So, it looks like everything
is working fine. Now, Wikipedia has a lot

00:18:22.110 --> 00:18:26.850
of administrator pages. So this is not quite
what I wanted because I didn't want to find

00:18:26.850 --> 00:18:32.460
out how frequently administrators are, sort
of, you know, making edits. I want to find

00:18:32.460 --> 00:18:38.059
out how frequently users are editing articles
about interesting topics, or about topics.

00:18:38.059 --> 00:18:45.580
And, Wikipedia divides their articles into
namespaces and so it turns what I want is

00:18:45.580 --> 00:18:53.200
actually articles in namespace zero. So all
I've done is go back, edit the same query

00:18:53.200 --> 00:18:57.300
that I had out there, put in a new filter
and now I'm looking at just articles that

00:18:57.300 --> 00:19:02.340
are namespace zero. So there we go. We see,
you know, George Bush holds the record at

00:19:02.340 --> 00:19:08.590
least, as of like January of this year for
the most edited article. And we have some

00:19:08.590 --> 00:19:14.740
very passionate wrestling fans out there.
So that's one way to look at the data. Well,

00:19:14.740 --> 00:19:18.870
the question is how many people are contributing
to Wikipedia? How many people are making these

00:19:18.870 --> 00:19:26.870
edits? So I've a query that does that, and
let me just describe what I'm doing out here.

00:19:26.870 --> 00:19:33.080
What I'm doing is I'm doing a sub-select where
I'm looking at the number of distinct contributors

00:19:33.080 --> 00:19:41.630
contributing at any given month, and aggregating
that up since overall the data. One--sorry,

00:19:41.630 --> 00:19:46.810
one thing I forgot to do which is to tell
you how big this dataset is. I should have

00:19:46.810 --> 00:19:57.429
done that, sorry, but I can easily fix that.
So, 313 million rows, so it's done as big

00:19:57.429 --> 00:20:02.480
as a 60 billion raw dataset but it's still
a very big piece of data and I've just looked

00:20:02.480 --> 00:20:07.040
at the top titles. And now, I'm going to look
at the top contributors, I mean, the number

00:20:07.040 --> 00:20:12.380
of distinct contributors in each month since
Wikipedia has sort of been running. And we

00:20:12.380 --> 00:20:18.070
can see how that's varied over time. And there
you go. So you can see this clear increasing

00:20:18.070 --> 00:20:22.020
the number of distinct contributors going
year-over-year this growth as Wikipedia sort

00:20:22.020 --> 00:20:29.309
off and, you know, has been a huge success.
And not just that, I can--knowing that George

00:20:29.309 --> 00:20:34.130
Bush is sort of like the most popular article
edited, I can limit it to just articles about

00:20:34.130 --> 00:20:38.429
Bush. You can see I have a little statement
there that says, "That the title should contain

00:20:38.429 --> 00:20:41.920
Bush," and just looking at those articles,
I can run through and see what that is. And

00:20:41.920 --> 00:20:51.331
you see the increase of--between 2006 to 2007
when he was, you know, the President. So,

00:20:51.331 --> 00:20:57.370
one thing I want to call out here, you noticed
that I started out by counting up rows. I

00:20:57.370 --> 00:21:01.950
selected top by title. I then looked at contributor
ID, so it doesn't matter which way we slice

00:21:01.950 --> 00:21:06.049
and dice the data. All these queries run in
about the same amount of time which is only

00:21:06.049 --> 00:21:11.720
10 seconds, like, you know, in a matter of,
like, seconds. And I can add in filters, try

00:21:11.720 --> 00:21:18.670
out different variations, limit my query to
fewer rows all interactively while looking

00:21:18.670 --> 00:21:25.540
at this data. Finally, I want to do one last
demo to sort of show you where we're going.

00:21:25.540 --> 00:21:29.840
This is actually some--it's relatively new
things that our colleagues at--who're working

00:21:29.840 --> 00:21:36.020
on Google Apps scripts on making possible,
which is to allow BigQuery to be embedded

00:21:36.020 --> 00:21:44.450
into Google Docs, particularly Google Spreadsheets,
so that we can access 300 million row tables

00:21:44.450 --> 00:21:47.720
within our spreadsheet. So what I have here--and
hopefully this will work, this is pretty experimental,

00:21:47.720 --> 00:21:58.450
is a little tool that I can put in a word
like Euler. I--that's, is a mathematician

00:21:58.450 --> 00:22:09.030
that I've--and I can run a query. And what
it's doing underneath is running a BigQuery

00:22:09.030 --> 00:22:13.230
query fetching the data from our table and
filling it into the spreadsheet. And then

00:22:13.230 --> 00:22:23.660
I have a little graph built off of that data
so that stuff can go in there. So you can

00:22:23.660 --> 00:22:26.171
imagine like if you don't want to design a
full blown Web App nor do you want to like

00:22:26.171 --> 00:22:30.160
expose people to like a shell, you know, developers
like, you know, not everyone likes it. You

00:22:30.160 --> 00:22:35.280
can cobble up one of these spreadsheets and
I'll--let me give you a quick peek of what

00:22:35.280 --> 00:22:42.040
the script look like. It's not very complicated
but you have a little script--sorry--that

00:22:42.040 --> 00:22:45.640
can talk to BigQuery, construct your query
around it, and now you can share this spreadsheet

00:22:45.640 --> 00:22:49.800
with anybody else who wants to try their own
word. If anything has a suggestion, I'm happy

00:22:49.800 --> 00:23:02.420
to take it and give it a crack in here, anyone?
Cheese, that's a good one. I didn't think

00:23:02.420 --> 00:23:14.840
of that. And so, there you have it, well,
the most popular articles on Wikipedia on

00:23:14.840 --> 00:23:20.190
cheese. Okay, so I've shown you a couple of
different use-cases. I've shown you the kinds

00:23:20.190 --> 00:23:21.490
of data that it can work with, large datasets,
billions of rows, hundreds of millions of

00:23:21.490 --> 00:23:27.320
rows, and how you can get them easily where
you want them to have them. So just a recap

00:23:27.320 --> 00:23:32.929
on what BigQuery is. It's a tool to do interactive
analysis or interactive speed analysis of

00:23:32.929 --> 00:23:37.130
very large datasets. It is easy to use because
it's in a simple familiar language, it uses

00:23:37.130 --> 00:23:43.930
SQL for its queries and it has APIs wrapping
it so that it's accessible from everywhere

00:23:43.930 --> 00:23:48.960
and embeddable into a variety of context which,
you know, whichever one best suit your application.

00:23:48.960 --> 00:23:58.750
So with that, let me hand it back to Amit
and he can continue the presentation.

00:23:58.750 --> 00:24:08.010
&gt;&gt; AGARWAL: So how many of you guys like the
spreadsheet demo? Just a raise of hands. And

00:24:08.010 --> 00:24:12.570
it makes my job really easy as a product manager.
I don't want to go and type in SQL commands.

00:24:12.570 --> 00:24:17.200
I can just take the spreadsheet, share it
with customers, share it with other people,

00:24:17.200 --> 00:24:22.680
and just say, "Guys, this is what the results
look like." So what's Siddartha showed you

00:24:22.680 --> 00:24:28.500
was how to do interactive analysis over really
large datasets. And oftentimes when we're

00:24:28.500 --> 00:24:34.160
analyzing large datasets, we see trends and
patterns which opens up a whole new world,

00:24:34.160 --> 00:24:40.230
a whole new way of doing analysis based on
the underlying technology called machine learning.

00:24:40.230 --> 00:24:44.320
And this is what Prediction API is about.
I'd like to call our next speaker; we invited

00:24:44.320 --> 00:24:50.160
the next speaker Gideon. He's from the Google
research team to talk about the Prediction

00:24:50.160 --> 00:25:09.250
API.
&gt;&gt; MANN: Thanks. Is this the right one? There

00:25:09.250 --> 00:25:12.890
we go. Ahh, there we go. Thanks, Amit. So
if BigQuery was about analyzing the past data,

00:25:12.890 --> 00:25:26.700
Prediction APIs is about using your data and
make predictions on new situations using your

00:25:26.700 --> 00:25:52.720
old data. So it's not a crystal ball--that's
not what it is. But it is a tool that we found

00:25:52.720 --> 00:26:14.679
very useful inside the company. We're excited
to be able to open up to you guys. We hope

00:26:14.679 --> 00:26:27.010
you find it useful. Let me give you some background.
Max and I are in the research group and kind

00:26:27.010 --> 00:26:35.540
of our day job is using machine learning to
work on problems supply it and solve problems

00:26:35.540 --> 00:26:42.641
that are digging the company. And one thing
that we noticed as we worked on this is that

00:26:42.641 --> 00:26:50.760
there is a particular class of machine learning
tools. In particular, supervise classifications

00:26:50.760 --> 00:26:57.650
which are broadly, applicable and very easy
to use. So once we had this realization, we

00:26:57.650 --> 00:27:16.550
kind of we were off running, and so I'm very
excited to be able to talk to you guys today

00:27:16.550 --> 00:27:30.040
about Prediction API which is a supervised
classification Web service that runs over

00:27:30.040 --> 00:27:50.880
our Cloud and allows you to embed machine
learning into your Web Apps. So let me give

00:27:50.880 --> 00:28:06.170
you some of the details, Prediction API is
a RESTful HTTP service. There are three steps;

00:28:06.170 --> 00:28:17.050
you upload your data, you train, and then
you predict. So, it's--you know, it's--none

00:28:17.050 --> 00:28:28.100
of the machine learning that we're offering
is really groundbreaking. But I think what's

00:28:28.100 --> 00:28:42.090
really interesting is that it's a Web service
and it runs on a Cloud. And what that means

00:28:42.090 --> 00:28:51.490
is really three things; one, is that it's
very easy to integrate into your applications,

00:28:51.490 --> 00:29:06.980
it's a Web service, it's an HTTP and you can
call it from anywhere; two, it runs on our

00:29:06.980 --> 00:29:18.740
cloud and so it scales with your data and
your needs; and three, a lot of the research

00:29:18.740 --> 00:29:32.780
that happens within the company, we can expose
to you directly and kind of give you a lot

00:29:32.780 --> 00:29:47.059
of leverage quickly. And I think all of those
together would make it really compelling and

00:29:47.059 --> 00:30:12.440
interesting. So let me tell you a little bit
of how it works. The data that you give it

00:30:12.440 --> 00:30:27.490
is
a set of instance. Each instance is a pair,

00:30:27.490 --> 00:30:46.220
a pair of input which is composed of input
features and an output, an output label. The

00:30:46.220 --> 00:30:57.770
Prediction API takes that input after you've
uploaded to the Google Storage and built a

00:30:57.770 --> 00:31:10.110
model. And what the model does is it maps
input or input features to an output. And

00:31:10.110 --> 00:31:25.870
so now with that model, you can issue new
request with a set of input or input features

00:31:25.870 --> 00:31:46.130
and then the model will predict the likely
output. So, okay, what is it? It's not magic.

00:31:46.130 --> 00:32:05.630
Let me tell you a story. When I was 12 or
13, I went to a computer user's conference

00:32:05.630 --> 00:32:12.820
in my hometown and I learned a bunch of things.
Probably, the most important was that beards

00:32:12.820 --> 00:32:20.580
are really cool. But after that, I also learned
someone there said garbage in, garbage out

00:32:20.580 --> 00:32:32.170
and that really stuck with me and it's really
true and it applies here. You know, the prediction

00:32:32.170 --> 00:32:38.990
is only going to be as good as the predictive
features that you give as input. Without that,

00:32:38.990 --> 00:32:44.650
the whole thing is sunk. So this example,
it's a language identification task. You're

00:32:44.650 --> 00:32:50.770
trying to figure out what language this particular
sentence is in, so we have English and Spanish.

00:32:50.770 --> 00:32:57.070
So what the Prediction API does, where really
any supervised classification model does,

00:32:57.070 --> 00:33:02.610
is it looks for relevant features in the input
and then it--during training, and it tries

00:33:02.610 --> 00:33:07.900
to figure out how those features relate to
our particular output. And then at prediction

00:33:07.900 --> 00:33:16.640
time, it looks for those features and tries
to figure out what the likely output is for

00:33:16.640 --> 00:33:24.200
those given features. So in case of language
identification function words, determiners

00:33:24.200 --> 00:33:31.170
like "the" or "la" are pretty good predictive
features. And so it will learn that those

00:33:31.170 --> 00:33:41.630
features are correlated. So "the" it usually
means English and "la" usually means Spanish.

00:33:41.630 --> 00:33:57.000
Now, if you'll notice "la" is also actually
an English word, it's the name of the note.

00:33:57.000 --> 00:34:00.539
So, features aren't always 100% predictive.
If they were, there wouldn't be anything interesting.

00:34:00.539 --> 00:34:06.730
But usually, they're ambiguous and infrequent.
And so a lot of the work in the last 20 years

00:34:06.730 --> 00:34:12.429
on machine learning research has been figuring
out how to use ambiguous features and figure

00:34:12.429 --> 00:34:21.899
out how to combine them. A lot of the work
has, you know, has been done, it's really

00:34:21.899 --> 00:34:27.849
fascinating. Some of the people who have done
the groundbreaking work are at Google, [INDISTINCT].

00:34:27.849 --> 00:34:37.629
But actually, in addition to their work, one
of the big changes nowadays is simply the

00:34:37.629 --> 00:34:49.169
amount of data that's available. It's like
the fire hose has been turned up to eleven.

00:34:49.169 --> 00:34:58.719
There's just so much data out there and that
kind of leads to another saying that in the

00:34:58.719 --> 00:35:08.499
field, which is there's no data like more
data. So we're really hoping that your data

00:35:08.499 --> 00:35:23.640
on our cloud with our system can really let
you do things that really have never been

00:35:23.640 --> 00:35:30.000
done before. So what? What kind of applications?
I mean, really there are an endless number

00:35:30.000 --> 00:35:35.999
of applications we expect that this could
be used for. Here are some examples and in

00:35:35.999 --> 00:35:43.249
term of the company, we use this kind of technology
for spam classification; good email, bad email

00:35:43.249 --> 00:36:04.230
also sentiment analysis. So if you look and
you look for restaurant reviews, they have

00:36:04.230 --> 00:36:16.599
like stars next to it, that's all done automatically
using supervised classification, so you have

00:36:16.599 --> 00:36:30.229
one star, two stars, three stars. But, you
know, the goal of the API isn't just to give

00:36:30.229 --> 00:36:48.200
you these classifiers. It's not to, you know--give
you a language identification classifier or

00:36:48.200 --> 00:37:00.910
a sentiment analysis classifier. And the goal
is to give you the tools to build your own.

00:37:00.910 --> 00:37:15.910
It's like the old saying goes, you know, "Give
a man a fish. He'll eat for a day. Teach a

00:37:15.910 --> 00:37:30.539
man to fish. He'll soon develop a hankering
for chicken cutlet." But seriously, I mean,

00:37:30.539 --> 00:37:46.089
these tools are really exciting and we expect
to be surprised by the things that you guys

00:37:46.089 --> 00:38:02.950
use it for. So here, there are three simple
steps. You upload the data to Google Storage,

00:38:02.950 --> 00:38:12.190
and when you upload, you specify which is
the output label and what are the input features,

00:38:12.190 --> 00:38:23.020
and then you make a request to a particular
HTTP end--HTTP Endpoint. And then after some

00:38:23.020 --> 00:38:30.069
amount of time, the model is finished, it's
usually pretty quick. And then you can make

00:38:30.069 --> 00:38:39.969
predictions using that endpoint. So thank
you for your time. And now, I'm going to hand

00:38:39.969 --> 00:38:55.739
you off to Max, and he's going to run through
a demo on how to use it.

00:38:55.739 --> 00:39:09.031
&gt;&gt; LIN: Thank you, Gideon. Hi. I'm Max Lin.
I'm the tech layout Prediction API team. So

00:39:09.031 --> 00:39:47.550
I'm going to show you how to use a Prediction
API, and I'm going to walk you through on

00:39:47.550 --> 00:39:52.650
real examples. So, now, imagine you are working
for--in a multinational company. Not only

00:39:52.650 --> 00:40:00.279
you have multiple regional office, you have
multinational customers. So suppose you have

00:40:00.279 --> 00:40:09.119
an email address. A lot of international customers
send emails to you. They can either respond--they

00:40:09.119 --> 00:40:19.400
can also send tweets. They can make a blogpost,
so you got a lot of customers response for

00:40:19.400 --> 00:40:28.989
all sorts of languages. Now, you have a problem
or you want to solve the problem that given

00:40:28.989 --> 00:40:37.469
an email, a tweet, a blogpost, how am I going
to respond that? If it's--I would like to--if

00:40:37.469 --> 00:40:39.750
it's an English email, I want to forward to
our American regional office and let them

00:40:39.750 --> 00:41:17.029
to take care of that. If it's an email written
in Spanish, I may want our Mexico regional

00:41:17.029 --> 00:41:35.099
office to take care of that. So the decision
you want to make here is a predictive. Given

00:41:35.099 --> 00:41:39.739
an email, a tweet, a blogpost, you want to
predict the language. You want to know what

00:41:39.739 --> 00:41:51.999
language it's written in. So think a little
bit, what would you do? Well, you will be

00:41:51.999 --> 00:41:56.770
lucky if you have in-house machinery experts
like Google here, but if what if you don't.

00:41:56.770 --> 00:42:06.619
You're just a stop or you are working in an
enterprise focused on total different things,

00:42:06.619 --> 00:42:17.190
you want a solution. So if you already collected
some sort of data and we will show you to

00:42:17.190 --> 00:42:43.700
how you do that right now from this at the
top. So I will show you how. The first step

00:42:43.700 --> 00:42:47.651
is very easy one. Upload a set of training
data to Google Storage. So let me show you

00:42:47.651 --> 00:42:58.920
how--what are the data you need to provide.
It's a very simple format. It's called comma-separated

00:42:58.920 --> 00:43:14.299
value format, so everything is just separate,
by commas and you just provide input and output.

00:43:14.299 --> 00:43:26.089
So what's the input here? It can be emails,
it can be blogpost, it can be tweets, you

00:43:26.089 --> 00:43:28.279
collect before in different language. And
what's an output? Output is in the first column.

00:43:28.279 --> 00:43:38.200
So you see here, we have like 10 instances.
Every instance, the first column, you either

00:43:38.200 --> 00:43:47.960
see the French or Spanish, and they are the
target, the output you want to predict. And

00:43:47.960 --> 00:43:56.160
what's the input? Input is just the text.
So you can--it can be emails, it can be tweets.

00:43:56.160 --> 00:44:12.609
And you just put that in the second column
and that's it. That's--as simple as that.

00:44:12.609 --> 00:44:26.969
You just prepare a bunch of these, and you
don't worry about how much value here we have

00:44:26.969 --> 00:44:40.519
scaled. We have scalable algorithms. I have
to work for that. So let me upload this right

00:44:40.519 --> 00:44:47.670
now to Google Storage. We use GSUtil, and
we copy this file locally to Google Storage.

00:44:47.670 --> 00:45:00.780
I already create a bucket called "io10" and
so that's our bucket. And I also call this

00:45:00.780 --> 00:45:34.221
data "language_id". So that's it. So your
first step, you upload a set of training that

00:45:34.221 --> 00:45:42.969
comes in separate value format to Google Storage.
And the second step is you want to run machine

00:45:42.969 --> 00:45:50.319
algorithms and how to invoke that call? You
just make a very simple HTTP post call. As

00:45:50.319 --> 00:45:56.921
you see, you will see "/predictions/v1/train",
and you just put the name, you just upload

00:45:56.921 --> 00:46:07.440
the big store--the Google Storage. So you'll
see? They're the parameter here, data. So

00:46:07.440 --> 00:46:26.380
that's the name we just upload to Google Storage
and that's it. That's the only parameter.

00:46:26.380 --> 00:46:40.700
You just tell Prediction API, that's the data,
say, you want to walk through. So let me invoke

00:46:40.700 --> 00:46:54.039
that and so that's the name we just upload
to Google Storage, it's just io10/language_id.

00:46:54.039 --> 00:47:05.390
And you won't get a response until the--the
training is going out. And our training is

00:47:05.390 --> 00:47:19.510
asynchronous, so if you upload like a millions
of records to our--Google Storage, it can

00:47:19.510 --> 00:47:32.140
take a couple of minutes. So how do you know
it's done or not? You just make another very

00:47:32.140 --> 00:47:40.959
simple HTTP call. Let me show you. So here,
you will still see the prediction. And this

00:47:40.959 --> 00:47:50.019
time, you call query. And you say, "I want
those check if training is finished or not."

00:47:50.019 --> 00:48:01.539
So, again, it's very simple, you make a HTTP
call and you check the status of the training.

00:48:01.539 --> 00:48:04.589
Check training. And, again, the only parameter
is that it did not say it upload to Google

00:48:04.589 --> 00:48:05.589
Storage, so this, again, is io10/language_id.
So, it's done. And you will see--we'll also

00:48:05.589 --> 00:48:08.999
try to estimate the accuracy. So based on
the data you provide to us, we will try to

00:48:08.999 --> 00:48:15.029
estimate how accurate that will be in the
future. So here, we got a number like a 0.95,

00:48:15.029 --> 00:48:19.660
so you can kind of use beta. You'll do a pretty
good job based on the data you provide. So

00:48:19.660 --> 00:48:27.859
that's second step. So the final step that
once you get a new email, a new tweet, and

00:48:27.859 --> 00:48:41.359
you want to know what language that is written
in, so that's--let me show you how to do that.

00:48:41.359 --> 00:48:53.660
Again, it's very simple. If you look at it
down here, highlight here, it still is a very

00:48:53.660 --> 00:49:05.450
simple HTTP call just like query. And the
only parameter, the data there, is the--again,

00:49:05.450 --> 00:49:12.450
the dataset you just upload to a Google Storage.
And so where do you put the new dataset? You

00:49:12.450 --> 00:49:25.980
put in a body. So you specify that your data
is set in a JSON format here. I highlight

00:49:25.980 --> 00:49:52.180
here. So we have a very simple just JSON format
for you to put your inputs. So here, if you

00:49:52.180 --> 00:50:09.150
want to do a prediction on language, well
you can use the particular type called text.

00:50:09.150 --> 00:50:12.589
And inside the text, you can put whatever,
new email, tweet, post, the blogpost there.

00:50:12.589 --> 00:50:31.799
So let's run that. So if we want to predict--we
get a new English email and we want to use

00:50:31.799 --> 00:50:49.319
our training say io10/language_id. I want
to use the patterns we learn from this historical

00:50:49.319 --> 00:50:54.569
data from io10/language_id. And, of course,
you want to put the new email or new tweet

00:50:54.569 --> 00:51:05.690
you want to classify with. So let's think
of one English sentence say, "What is the

00:51:05.690 --> 00:51:14.869
answer to the ultimate question of life, the
universe, and everything?" Here we go. So

00:51:14.869 --> 00:51:28.119
we try to make a real time prediction. So
every time you see a new thing, you make a

00:51:28.119 --> 00:51:34.569
call, you get a prediction. And we got it
right. And you see up here, so that's English.

00:51:34.569 --> 00:51:37.890
So based on your historical data you provide,
we think this is English Prediction API based

00:51:37.890 --> 00:51:43.550
on your trainings says that thinks this is
an English sentence. And I don't know if you

00:51:43.550 --> 00:51:53.710
look at our example carefully on that, actually,
our trainings that are all in English sentence

00:51:53.710 --> 00:52:14.339
come from Alice in Wonderland. So I can bet
in the English--in the Alice in Wonderland,

00:52:14.339 --> 00:52:17.119
you never see this sentence in that data.
So what's cool and what's so powerful about

00:52:17.119 --> 00:52:23.380
this is right now you have a tool in your
hand and you can use this tool to do all sorts

00:52:23.380 --> 00:52:27.319
of prediction. And the key thing is that you
try to predict something you never see before,

00:52:27.319 --> 00:52:32.430
so like here. This sentence, you've never
seen in your data, but you can do a right

00:52:32.430 --> 00:52:43.539
prediction. Le me just try something Spanish.
Say, we use

00:52:43.539 --> 00:52:50.190
the same training say, io10/language_id and
let's try a Spanish sentence, "coda nino es

00:52:50.190 --> 00:52:57.440
un artista"--oh, sorry. Yeah, thanks. There
you go. So there you see the input is--you

00:52:57.440 --> 00:53:26.530
just send to over where to Prediction API
and you got a prediction, it's a Spanish one.

00:53:26.530 --> 00:53:33.459
So, again, I want--the [INDISTINCT] message
here is not a barrier language at all. If

00:53:33.459 --> 00:53:41.509
you are looking for a language [INDISTINCT]
Russian, I strongly recommend you to look

00:53:41.509 --> 00:53:42.519
at Google language API. There's a much large
number of languages, and much more than just

00:53:42.519 --> 00:53:53.130
three language we showed here. The cool thing
is that, right now, you have a tool, and you

00:53:53.130 --> 00:53:59.191
can think of--you just think of you're--at
whatever application you are developing, it

00:53:59.191 --> 00:54:17.880
doesn't matter if it's a desktop App or it's
a mobile App. And there is a lot of decision

00:54:17.880 --> 00:54:22.799
you want to make in your App to help your
users, to help your customers. So if you want

00:54:22.799 --> 00:54:26.670
to predict on language which is the [INDISTINCT]
of a language_id is pretty complex, but if

00:54:26.670 --> 00:54:44.709
you have training data, you can--like here,
in five minutes, you can right now have a

00:54:44.709 --> 00:54:51.609
model you can use to predict on your data.
If you have a lot of transaction history,

00:54:51.609 --> 00:55:00.900
you want to look through that and you want
to know, "Oh, this is a new customer, are

00:55:00.900 --> 00:55:09.480
they going to buy this product or not?" That's
another predictive decision you want to make.

00:55:09.480 --> 00:55:21.789
And, again, if you can prepare data, send
over to Prediction API, you can use our service.

00:55:21.789 --> 00:55:23.799
So this--I showed you the first step, uploaded
the dataset; the second step, you just co--make

00:55:23.799 --> 00:55:24.799
a very simple HTTP call to training data;
the third step is to predict on a new dataset.

00:55:24.799 --> 00:55:28.049
So they are all very easy just, a very simple
HTTP calls. And because our service is very

00:55:28.049 --> 00:55:32.000
simple to use, I just show you how to do that
from [INDISTINCT], but I'm sure you can do

00:55:32.000 --> 00:55:36.589
that almost from everywhere, it's very simple.
So here, you can see a simple code. You can

00:55:36.589 --> 00:55:40.989
do this from Python. And again, it's very
simple. You can do this at, not just Python,

00:55:40.989 --> 00:55:41.989
it can be from Python, Java. You can host
this, say, Google App Engine. It's--the possibility

00:55:41.989 --> 00:55:42.989
is everywhere. So let me do a recap on all
the key capability right now for Prediction

00:55:42.989 --> 00:55:44.799
API. For the dataset, we take input features.
So I showed you the input you can send to

00:55:44.799 --> 00:55:54.489
Prediction API. I just showed you a text,
the free phone text and actually you can send

00:55:54.489 --> 00:55:57.349
also numerical values. So if you wanted to
create an analysis, you can or you may want

00:55:57.349 --> 00:55:58.349
to send over how much money this guy have
in his checking--his or her checking account.

00:55:58.349 --> 00:55:59.349
How long is the [INDISTINCT] history or something
like that? So they can be numerical. An output

00:55:59.349 --> 00:56:00.349
there right now--I just showed you three categories,
and we support up to hundreds of categories,

00:56:00.349 --> 00:56:01.420
so that's the output. For the training, we're
actually [INDISTINCT] many, many state of

00:56:01.420 --> 00:56:02.420
the art machine learning algorithm behind
the scene. So you don't see that, but we actually

00:56:02.420 --> 00:56:03.420
run many machine-only algorithms. And try
to figure out which one is the best for your

00:56:03.420 --> 00:56:04.799
data and give you the best one. And we automatically
start that--and we also perform--the training

00:56:04.799 --> 00:56:10.029
is done asynchronously, so you don't have
to wait and you can just go back and check

00:56:10.029 --> 00:56:17.309
if the training is done or not. And the last
thing is that, we really want to make this

00:56:17.309 --> 00:56:31.090
easy as [INDISTINCT]. So it's an HTTP web
service, and you can call very easily for

00:56:31.090 --> 00:56:34.589
either it was--like we said it can be from
Web App, it can be from Google Spreadsheet

00:56:34.589 --> 00:56:35.589
uses App Script, it can call from desktop
app, mobile app, everywhere. So with that,

00:56:35.589 --> 00:56:39.420
I will give this back to Amit. Thank you.
&gt;&gt; AGARWAL: Thank you, Max and Gideon. So

00:56:39.420 --> 00:56:51.499
what do you guys think? So both BigQuery and
Prediction API is in the preview mode. We

00:56:51.499 --> 00:56:57.799
open it up to a limited number of developers
and enterprises. And you can pretty much go

00:56:57.799 --> 00:57:02.329
to these URLs and sign up for the services,
code.google.com/apis/bigquery and code.google.com/apis/prediction.

00:57:02.329 --> 00:57:03.509
So we do want you guys to sign up. We've been
seeing a lot of positive feedback and we hope

00:57:03.509 --> 00:57:05.199
you guys will enjoy these tools. Now, I would
like to invite the speakers up on the stage

00:57:05.199 --> 00:57:08.569
so that we can take any questions and answer
any question that you guys think. So Siddhartha,

00:57:08.569 --> 00:57:11.339
Max, and Gideon, can you guys come up? One
quick question in the URL, we had a title,

00:57:11.339 --> 00:57:16.019
so the Prediction API, it's code.google.com/apis/predict.
So it's not prediction, /predict. So we can

00:57:16.019 --> 00:57:19.359
probably start. Yes, sir.
&gt;&gt; This question is on BigQuery. So we already

00:57:19.359 --> 00:57:22.590
have, you know, on App Engine, the BigTable
and GQL, which takes care of two characteristics

00:57:22.590 --> 00:57:23.590
working on large dataset and also taking care
of filter condition, I mean, the dimension.

00:57:23.590 --> 00:57:24.590
Why is that we built another one for aggregate
queries? Can't we merge both of them because

00:57:24.590 --> 00:57:25.590
aggregate queries are something that we need
badly in Google App Engine on GQL last week?

00:57:25.590 --> 00:57:26.590
&gt;&gt; NAIDU: So let me try and answer that question
as best as I can. Both do deal with large

00:57:26.590 --> 00:57:27.590
datasets. BigQuery is probably targeted at
even bigger datasets than they're traditionally

00:57:27.590 --> 00:57:28.590
used to using in data store. One is more geared
towards building Web Apps where you have user

00:57:28.590 --> 00:57:29.590
data sessions you need to query along certain
dimensions, right? The other is more geared

00:57:29.590 --> 00:57:30.590
along data analysis. And we're going to work
hard to make them both as internal [INDISTINCT]

00:57:30.590 --> 00:57:31.590
as possible keeping, you know, possibly bringing
your data between the two in ways that makes

00:57:31.590 --> 00:57:32.590
aggregate queries possible easily and data
store. We're not there yet, but our plan is

00:57:32.590 --> 00:57:33.590
to sort of go in the direction of having a
variety of data stores that are appropriate

00:57:33.590 --> 00:57:34.590
but accessible to a common interface. So that,
you know, the App I built was built through

00:57:34.590 --> 00:57:35.590
App Engine, the demo that I had.
&gt;&gt; Okay.

00:57:35.590 --> 00:57:36.590
&gt;&gt; NAIDU: So we're going to make this easy
to get to Map Engine and, you know, use in

00:57:36.590 --> 00:57:37.590
a way that's appropriate for you.
&gt;&gt; Meaning you will first have to export all

00:57:37.590 --> 00:57:38.590
data into the query data store and then make
the query?

00:57:38.590 --> 00:57:39.590
&gt;&gt; NAIDU: So, I don't want to look too forward
into this, but it's more like we're going

00:57:39.590 --> 00:57:40.590
to allow you to--let me give you one, like,
possible use-case, right. So, let's leave

00:57:40.590 --> 00:57:41.590
data store out, but if you say App Engine
logs. Like there is, you know, there's a question

00:57:41.590 --> 00:57:42.590
of do you import the App Engine logs into
BigQuery or not?

00:57:42.590 --> 00:57:43.590
&gt;&gt; Logs, you mean?
&gt;&gt; NAIDU: Yeah, logs.

00:57:43.590 --> 00:57:44.590
&gt;&gt; AGARWAL: One quick question. We do have
a Sandbox area.

00:57:44.590 --> 00:57:45.590
&gt;&gt; NAIDU: Okay.
&gt;&gt; AGARWAL: And we're happy to address any

00:57:45.590 --> 00:57:46.590
question in detail.
&gt;&gt; Okay.

00:57:46.590 --> 00:57:47.590
&gt;&gt; AGARWAL: It's located very close to the
Google Storage for developers' booth. And

00:57:47.590 --> 00:57:48.590
we do have people from the engineering team
there who can answer your questions in detail.

00:57:48.590 --> 00:57:49.590
&gt;&gt; Okay, thank you.
&gt;&gt; AGARWAL: All right. Moving on to you, sir.

00:57:49.590 --> 00:57:50.590
&gt;&gt; Similar question, actually.
&gt;&gt; NAIDU: Okay.

00:57:50.590 --> 00:57:51.590
&gt;&gt; I have an App Engine app that already has
over a years worth of survey data and collected

00:57:51.590 --> 00:57:52.590
in it from customers. If I want to add a real-time
analytics to my dashboard, do I have to move

00:57:52.590 --> 00:57:53.590
all that data to BigQuery or can I access
the BigQuery functionality from my App Engine

00:57:53.590 --> 00:57:54.590
data store?
&gt;&gt; AGARWAL: To make it easy, we'll make it

00:57:54.590 --> 00:57:55.590
very easy for you to do it, so as a user you
would not have to import it. We'll take care

00:57:55.590 --> 00:57:56.590
of all of that for you.
&gt;&gt; Okay.

00:57:56.590 --> 00:57:57.590
&gt;&gt; NAIDU: You probably just--yeah, so we're
going to make it easy to do the same things.

00:57:57.590 --> 00:57:58.590
&gt;&gt; Okay, but it's no there yet.
&gt;&gt; NAIDU: It's not there yet.

00:57:58.590 --> 00:57:59.590
&gt;&gt; Okay. Cool. Thanks.
&gt;&gt; AGARWA: Yes.

00:57:59.590 --> 00:58:00.590
&gt;&gt; I have a question about the Prediction
API. I didn't see it in a demo so I'm just

00:58:00.590 --> 00:58:01.590
curious if it's there. Do you offer confidence
levels on the Prediction?

00:58:01.590 --> 00:58:02.590
&gt;&gt; MANN: Confidence level on the Predictions
I think that was the question, so, we don't

00:58:02.590 --> 00:58:03.590
have that yet. I mean, I think the, you know,
as I was telling you somewhat earlier. I think

00:58:03.590 --> 00:58:04.590
the Prediction API is the beginning of a conversation
that we're going to have with kind of all

00:58:04.590 --> 00:58:05.590
of you guys in terms of what features are
the most important and where we should push.

00:58:05.590 --> 00:58:06.590
That's something that we have internally,
I could absolutely imagine exposing that.

00:58:06.590 --> 00:58:07.590
&gt;&gt; AGRAWAL: So, Gideon, we can probably take
one more online question here. What kind of

00:58:07.590 --> 00:58:08.590
data-mining algorithms will be included in
the Prediction API?

00:58:08.590 --> 00:58:09.590
&gt;&gt; MANN: So it is the question I think everybody
wants to know and we've been a little cagey

00:58:09.590 --> 00:58:10.590
about it. Part of the reason is that, we'd
try things out. We have a lot of things that

00:58:10.590 --> 00:58:11.590
we do internally, some are public--most are
public, some aren't. And so we're kind of

00:58:11.590 --> 00:58:12.590
going through and trying to figure out what
is giving the most value to you guys and what

00:58:12.590 --> 00:58:13.590
we can allow it. But, you know, I--unfortunately,
I don't have a good answer now. I expect in

00:58:13.590 --> 00:58:14.590
the coming months, we're going to have a much
better answer.

00:58:14.590 --> 00:58:15.590
&gt;&gt; AGRAWAL: Yes, sir?
&gt;&gt; Can you talk a little bit about the commercial

00:58:15.590 --> 00:58:16.590
model and price poll availability, that kind
of thing?

00:58:16.590 --> 00:58:17.590
&gt;&gt; AGRAWAL: So right now, we're in a preview
model, so we don't really have any pricing

00:58:17.590 --> 00:58:18.590
and billing numbers. We hope we'll have--a
lot of you guys trying it out. So then in

00:58:18.590 --> 00:58:19.590
sometime, we can come up with pricing and
billing models, and provide you the numbers

00:58:19.590 --> 00:58:20.590
that you're actually looking for. Yes, sir?
&gt;&gt; Hi. For BigQuery, can you append or update

00:58:20.590 --> 00:58:21.590
data in BigQuery tables? And can you run queries
on tables while the import is still running?

00:58:21.590 --> 00:58:22.590
&gt;&gt; NAIDU: The short answer is append, yes.
Run queries while appending, yes. Both need

00:58:22.590 --> 00:58:23.590
more detailed explanations of like how exactly
import works and so on, but briefly, yes.

00:58:23.590 --> 00:58:24.590
Update, no. I mean, this is more for data
analysis, so we're not looking it like sort

00:58:24.590 --> 00:58:25.590
of updates to the table but, append, yes.
&gt;&gt; AGRAWAL: And this is exactly why we are

00:58:25.590 --> 00:58:26.590
opening it up for preview period. We really
want to understand your features and request

00:58:26.590 --> 00:58:27.590
and accordingly prioritize, so that we can
solve you needs and problems. Yes, sir?

00:58:27.590 --> 00:58:28.590
&gt;&gt; So you guys have the language API available
using all of your datasets you've used for

00:58:28.590 --> 00:58:29.590
language so far. You showed a bunch of other
examples like consumer's sentiment and everything

00:58:29.590 --> 00:58:30.590
else like that. Do we have access to those
through any API calls from all the research

00:58:30.590 --> 00:58:31.590
you've done on figuring out, like what I mentioned,
customer sentiment or species recognition

00:58:31.590 --> 00:58:32.590
or anything else like that?
&gt;&gt; AGRAWAL: Max, Gideon, do you guys want

00:58:32.590 --> 00:58:33.590
to do that?
&gt;&gt; LIN: So Prediction API is more like a work

00:58:33.590 --> 00:58:34.590
from your data. So, for example, like a language
I just showed before--actually, I don't, actually,

00:58:34.590 --> 00:58:35.590
encourage you to use our data. I mean, if
you really want to use language identification,

00:58:35.590 --> 00:58:36.590
you should go to Google Language API, that's
the one. We want to show this example and

00:58:36.590 --> 00:58:37.590
you should prove--I like to use--see how--what
kind of data you upload to us and actually

00:58:37.590 --> 00:58:38.590
what kind of prediction to use.
&gt;&gt; Yeah, I know that's available through the

00:58:38.590 --> 00:58:39.590
language API. I'm talking about, are those
other examples you mentioned available throughout

00:58:39.590 --> 00:58:40.590
the other APIs like consumer sentiment?
&gt;&gt; AGRAWAL: Well, some of them, we have released,

00:58:40.590 --> 00:58:41.590
if you look at Translation API, that is one
of the ways we're just saying, "Use Google."

00:58:41.590 --> 00:58:42.590
Whatever Google has learned including translation.
The language which Max talked about is another

00:58:42.590 --> 00:58:43.590
one. This one is more of a genetic framework
where it can use your data to train the model

00:58:43.590 --> 00:58:44.590
and then do predictive analysis.
&gt;&gt; MANN: Actually, can I jump in and just

00:58:44.590 --> 00:58:45.590
say one thing to speak to that. I think we're
very interested in providing a really high

00:58:45.590 --> 00:58:46.590
quality APIs, and I think, on for particular
datasets. I think there's going to be a lot

00:58:46.590 --> 00:58:47.590
of action both from us and hopefully from
all of you guys in figuring out what are really

00:58:47.590 --> 00:58:48.590
useful APIs and how to make them available.
So we're thinking about it. Not, yet.

00:58:48.590 --> 00:58:49.590
&gt;&gt; SAGRAWAL: So I'll take one online question.
Is BigQuery generally fast enough for generating

00:58:49.590 --> 00:58:50.590
interactive data for user on demand? That
is, are we talking about sub-second response

00:58:50.590 --> 00:58:51.590
time or tens of seconds?
&gt;&gt; NAIDU: Well, I think that demo sort of

00:58:51.590 --> 00:58:52.590
given a feel for that. We're looking at like
order of seconds responses for datasets up

00:58:52.590 --> 00:58:53.590
to hundreds of millions of those, right? The
60 billion row dataset, it can take a little

00:58:53.590 --> 00:58:54.590
bit longer, but they're looking at order of
seconds, tens of seconds. Now, how exactly

00:58:54.590 --> 00:58:55.590
to use that, depends on your application.
In some cases, it's--within Google, we frequently

00:58:55.590 --> 00:58:56.590
use it as a live backing to our user facing
application where a user comes in and generates

00:58:56.590 --> 00:58:57.590
a custom dashboard around the query to find
some issue or some track down, some--especially

00:58:57.590 --> 00:58:58.590
problem, might as well vary depending on your
application.

00:58:58.590 --> 00:58:59.590
&gt;&gt; SAGRAWAL: So, again, I think what you want
to do is very much you want to be very, very

00:58:59.590 --> 00:59:00.590
interactive. And the whole benefit of this
is the interactivity and the really fast response

00:59:00.590 --> 00:59:01.590
times. Yes, on the right?
&gt;&gt; Yes. So I have a question for the Prediction

00:59:01.590 --> 00:59:02.590
API for the training process, do you guys
provide like training feedback so like a confusion

00:59:02.590 --> 00:59:03.590
matrix or, you know, like, if you guys do
cross validation, which one's working? Which

00:59:03.590 --> 00:59:04.590
one didn't work?
&gt;&gt; AGRAWAL: Max? Gideon?

00:59:04.590 --> 00:59:05.590
&gt;&gt; LIN: As you can see here, we try to estimate
that the accuracy and there are various sort

00:59:05.590 --> 00:59:06.590
of ways to do that. And we are actually kind
of experiment on that. We--that we have like

00:59:06.590 --> 00:59:07.590
a bunch of different methods that are in-house.
And, we like to--here in this period-over-period,

00:59:07.590 --> 00:59:08.590
we'd like to see what kind of data you actually
upload to us, what kind of--it ask people

00:59:08.590 --> 00:59:09.590
what's you're doing, and we're going to see
what kind of feedback we can actually provide--better

00:59:09.590 --> 00:59:10.590
feedback for you. So like--accuracy is one
thing and we have been taking a lot of feedback

00:59:10.590 --> 00:59:11.590
we can provide to you. And we love to hear
from you and what kind of feedback you like

00:59:11.590 --> 00:59:12.590
and send it to us.
&gt;&gt; Okay. So what's the best way to use? Any

00:59:12.590 --> 00:59:13.590
suggestions?
&gt;&gt; AGRAWAL: So we do have the mailing list

00:59:13.590 --> 00:59:14.590
and the groups lists on our code site. So
if you go to code.google.com/apis/predict

00:59:14.590 --> 00:59:15.590
or code.google.com/apis/bigquery, we have
links for discussion forums where you can

00:59:15.590 --> 00:59:16.590
give us feedback and also share concerns and
issues with other fellow members.

00:59:16.590 --> 00:59:17.590
&gt;&gt; LIN: Yeah, please send to us.
&gt;&gt; Great. Thank you.

00:59:17.590 --> 00:59:18.590
&gt;&gt; AGRAWAL: Thanks. Yes, sir?
&gt;&gt; Yeah, with BigQuery, are the results going

00:59:18.590 --> 00:59:19.590
to be compatible with like Visualization API,
the Charts API, or...?

00:59:19.590 --> 00:59:20.590
&gt;&gt; NAIDU: So--with the Charts API and so on?
&gt;&gt; Yeah.

00:59:20.590 --> 00:59:21.590
&gt;&gt; NAIDU: Yeah. Actually, what we've been
looking--we've made the response basically

00:59:21.590 --> 00:59:22.590
match the Chart API as closely as possible.
And what we're actually going to do is look

00:59:22.590 --> 00:59:23.590
at providing libraries, you know, expanding
with Chart API Library so I can directly do

00:59:23.590 --> 00:59:24.590
stuff like passing like a query and a connection
we'll be creating and it all hides the details

00:59:24.590 --> 00:59:25.590
of that from you.
&gt;&gt; Is that--is the query language itself close

00:59:25.590 --> 00:59:26.590
to what they specify as data source or, you
know, for making data search query or is it

00:59:26.590 --> 00:59:27.590
exactly the same or just a little bit off?
&gt;&gt; NAIDU: It's quite different from the data

00:59:27.590 --> 00:59:28.590
store thing. It's actually closer to regular
SQL. It's more, I mean--yeah, it's closer

00:59:28.590 --> 00:59:29.590
to regular SQL let me put it that way.
&gt;&gt; So things like pivoting, are you going

00:59:29.590 --> 00:59:30.590
to have that?
&gt;&gt; NAIDU: I haven't--we haven't looked that

00:59:30.590 --> 00:59:31.590
far, so I don't know yet to answer the question.
&gt;&gt; Okay, thanks.

00:59:31.590 --> 00:59:32.590
&gt;&gt; AGRAWAL: So since we're running out of
time, we'll probably take a last question.

00:59:32.590 --> 00:59:33.590
And once again, we do have a booth in the
Sandbox area. It's very close to Google Storage

00:59:33.590 --> 00:59:34.590
for Developers. Please join us there, and
that we will have members from the Prediction

00:59:34.590 --> 00:59:35.590
API and the BigQuery team who'll be there
to answer your questions. Yes, sir?

00:59:35.590 --> 00:59:36.590
&gt;&gt; Are the results from the Prediction API
continuous or they, which is stored and all?

00:59:36.590 --> 00:59:37.590
So if I want to know relative score between,
you know, zero and one with the full range

00:59:37.590 --> 00:59:38.590
of decimals, could I get that or is it only
binary, like English, Spanish, French?

00:59:38.590 --> 00:59:39.590
&gt;&gt; LIN: And for now, we support apps. It's
not binary, it's discreet, so we can--right

00:59:39.590 --> 00:59:40.590
now, it can support hundreds of categories.
And we have been thinking a lot on the continuous

00:59:40.590 --> 00:59:41.590
aspect and that's definitely something we
consider, but it's not there yet.

00:59:41.590 --> 00:59:42.590
&gt;&gt; Thank you.
&gt;&gt; AGRAWAL: Thank you all for coming here.

00:59:42.590 --> 00:59:42.608
I hope you enjoyed the session. We look forward
to you in the Sandbox area. Thanks a lot.

