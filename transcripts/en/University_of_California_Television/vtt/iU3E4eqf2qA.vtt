WEBVTT
Kind: captions
Language: en

00:00:01.060 --> 00:00:02.580
- [Narrator] This program is presented by

00:00:02.580 --> 00:00:04.810
University of California Television.

00:00:04.810 --> 00:00:05.960
Like what you learn?

00:00:05.960 --> 00:00:09.260
Visit our website or follow
us on Facebook and Twitter

00:00:09.260 --> 00:00:12.090
to keep up with the latest UCTV programs.

00:00:12.090 --> 00:00:14.550
Also, make sure to check out and subscribe

00:00:14.550 --> 00:00:17.960
to our YouTube original
channel, UCTV Prime,

00:00:17.960 --> 00:00:19.833
available only on YouTube.

00:00:25.769 --> 00:00:28.352
(techno music)

00:00:50.084 --> 00:00:51.910
- Welcome to a Conversation with History,

00:00:51.910 --> 00:00:52.990
I'm Harry Kreisler

00:00:52.990 --> 00:00:55.030
of the Institute of International Studies.

00:00:55.030 --> 00:00:57.330
Our guest today is Jeff Hawkins,

00:00:57.330 --> 00:01:02.330
who is the Hitchcock professor
on the Berkeley campus

00:01:02.630 --> 00:01:06.630
in the spring of 2012.

00:01:06.630 --> 00:01:10.570
Jeff Hawkins is an inventor,
engineer, neuroscientist,

00:01:10.570 --> 00:01:12.300
author and entrepreneur.

00:01:12.300 --> 00:01:16.070
He founded both Palm
Computing and Handspring,

00:01:16.070 --> 00:01:19.550
created the Redwood Neuroscience Institute

00:01:19.550 --> 00:01:22.110
to promote research on
the brain and cognition,

00:01:22.110 --> 00:01:24.760
and is a member of the scientific board of

00:01:24.760 --> 00:01:26.800
Cold Spring Harbor Laboratory.

00:01:26.800 --> 00:01:29.850
He authored on Intelligence,
which was written

00:01:29.850 --> 00:01:31.440
with Sandra Blakeslee.

00:01:31.440 --> 00:01:34.180
And his latest company is Numenta,

00:01:34.180 --> 00:01:37.740
which brings his two passions together:

00:01:37.740 --> 00:01:40.650
study of the brain and the development

00:01:40.650 --> 00:01:42.910
of intelligent machines.

00:01:42.910 --> 00:01:44.240
Jeff, welcome to Berkeley.

00:01:44.240 --> 00:01:45.470
- Thank you, it's a pleasure to be here.

00:01:45.470 --> 00:01:46.830
- Where were you born and raised?

00:01:46.830 --> 00:01:49.410
- I was born on Long Island in New York.

00:01:49.410 --> 00:01:52.530
- And looking back, how
do you think your parents

00:01:52.530 --> 00:01:54.800
shaped your thinking about the world?

00:01:54.800 --> 00:01:56.673
- I had a pretty unusual childhood.

00:01:57.840 --> 00:02:02.480
My father was sort of
the consummate inventor,

00:02:02.480 --> 00:02:04.480
not particularly successful,

00:02:04.480 --> 00:02:08.610
but I grew up in a house
where we were just building

00:02:08.610 --> 00:02:11.580
crazy things all the time,
crazy boats and so on

00:02:11.580 --> 00:02:16.580
and exposed to a lot of
different things as a child.

00:02:19.720 --> 00:02:23.240
- Reading up on you, your father

00:02:23.240 --> 00:02:26.310
and the family together
made floating boats?

00:02:26.310 --> 00:02:28.920
- Well, he did lots of
things, my father did,

00:02:28.920 --> 00:02:31.050
but one of the things he did,

00:02:31.050 --> 00:02:33.990
he built a lot of unusual
craft, floating craft.

00:02:33.990 --> 00:02:37.570
One was, at the time, the
largest air cushion craft,

00:02:37.570 --> 00:02:40.300
it wasn't a hovercraft
but it was a round boat.

00:02:40.300 --> 00:02:41.950
We eventually did some research,

00:02:41.950 --> 00:02:43.637
eventually we sold it to an Orchestra

00:02:43.637 --> 00:02:44.830
and a tour of an Orchestra
around New York City.

00:02:44.830 --> 00:02:48.180
On this unusual thing, it
looked like a space craft.

00:02:48.180 --> 00:02:49.640
And that's one of many projects,

00:02:49.640 --> 00:02:52.050
but it was very influential as a child.

00:02:52.050 --> 00:02:55.460
- And this must've made an important

00:02:56.400 --> 00:02:58.530
both impression, but also,

00:02:58.530 --> 00:03:00.360
way of learning a lot of skills

00:03:00.360 --> 00:03:01.530
that became useful later.

00:03:01.530 --> 00:03:02.490
- It did.

00:03:02.490 --> 00:03:04.460
There's no question about it

00:03:04.460 --> 00:03:06.710
that I had exposure to a lot of tools,

00:03:06.710 --> 00:03:08.910
a lot of different environments.

00:03:08.910 --> 00:03:10.710
We weren't a wealthy family at all,

00:03:10.710 --> 00:03:12.530
we were kinda living on the edge

00:03:12.530 --> 00:03:14.300
of lower-middle class I would say.

00:03:14.300 --> 00:03:17.230
But I had a wealth of
different experiences.

00:03:17.230 --> 00:03:20.350
At the time I didn't really
understand the value of that,

00:03:20.350 --> 00:03:22.310
but later in life it became clear

00:03:22.310 --> 00:03:24.480
that it really did help me in many ways.

00:03:24.480 --> 00:03:28.110
- And it's not just the practical element

00:03:28.110 --> 00:03:32.530
of working with tools,
but to the extent that

00:03:32.530 --> 00:03:35.290
you seem to be saying
that your father would

00:03:35.290 --> 00:03:38.470
envision things that weren't there before.

00:03:38.470 --> 00:03:39.720
- Yeah, I'll tell you a story.

00:03:39.720 --> 00:03:41.300
When I was in third grade,

00:03:41.300 --> 00:03:42.570
there was a thing called a Weekly Reader,

00:03:42.570 --> 00:03:43.662
I don't know if that still exists.

00:03:43.662 --> 00:03:44.860
- [Harry] Oh yeah, of course.

00:03:44.860 --> 00:03:47.810
- And third grade, my father's
on the cover of Weekly Reader

00:03:49.006 --> 00:03:50.690
and he had an invention at
the time called a Septron,

00:03:50.690 --> 00:03:52.557
which was a type of pattern recognizer,

00:03:52.557 --> 00:03:55.357
and there he was with a
microphone talking to a dolphin.

00:03:56.270 --> 00:03:57.660
You know, this was very
unusual for a third-grader

00:03:57.660 --> 00:03:59.360
to have this kind of thing happen,

00:04:00.210 --> 00:04:02.440
it was always exposed to
these kind of unusual things.

00:04:02.440 --> 00:04:06.190
- And so what was the discussion
at the dinner table like?

00:04:06.190 --> 00:04:08.720
Was it about "Well how
do we fix this boat?"

00:04:08.720 --> 00:04:12.560
or "How do we design this project?"

00:04:12.560 --> 00:04:15.093
- It was a mixed thing.

00:04:17.390 --> 00:04:19.037
It was definitely a lot of, like,

00:04:19.037 --> 00:04:22.030
"Let's go back out to the
garage and work on something

00:04:22.030 --> 00:04:23.030
right after dinner."

00:04:24.000 --> 00:04:26.820
And in our house, actually the garage

00:04:26.820 --> 00:04:27.900
was the warmest part of the house,

00:04:27.900 --> 00:04:29.900
so in the winter that was where we went.

00:04:32.230 --> 00:04:35.010
I wouldn't say it was a very
philosophical upbringing.

00:04:35.010 --> 00:04:37.594
I was brought up in a
secular, atheist home

00:04:37.594 --> 00:04:40.950
and we just discussed world matters a bit.

00:04:40.950 --> 00:04:43.880
My father subscribed to
Scientific American from a child,

00:04:43.880 --> 00:04:45.140
and so those were always in the house

00:04:45.140 --> 00:04:47.480
and I read them religiously every month.

00:04:47.480 --> 00:04:48.583
I still do that.

00:04:50.076 --> 00:04:51.780
There was a culture of,

00:04:51.780 --> 00:04:54.180
hey, the world is
interesting, let's explore it.

00:04:56.040 --> 00:04:58.070
- Where did you do your...

00:04:58.070 --> 00:05:00.520
Well let's talk about when
you were in high school.

00:05:00.520 --> 00:05:02.470
Did you do a lot of
science in high school?

00:05:02.470 --> 00:05:04.780
And where did you do
your undergraduate work?

00:05:04.780 --> 00:05:07.180
- In high school I was...

00:05:07.180 --> 00:05:09.630
I never really worked particularly hard.

00:05:09.630 --> 00:05:11.720
I always did well.

00:05:11.720 --> 00:05:13.810
And so to me high school was
more of a matter of, like,

00:05:13.810 --> 00:05:15.050
hey, let's find something fun to do

00:05:15.050 --> 00:05:16.850
while I have to take these classes.

00:05:16.850 --> 00:05:19.340
So I did computer projects,

00:05:19.340 --> 00:05:21.410
I was often given leeway to do very,

00:05:21.410 --> 00:05:23.730
actually sometimes excuse
myself from things in physics.

00:05:23.730 --> 00:05:26.480
And so I could just do what I wanted to.

00:05:26.480 --> 00:05:28.110
So I kinda cruised through that,

00:05:28.110 --> 00:05:30.810
and then I went to Cornell University.

00:05:30.810 --> 00:05:32.030
It really wasn't much of a choice.

00:05:32.030 --> 00:05:33.817
I just said "Hey, I am living in New York.

00:05:33.817 --> 00:05:35.167
"That's a place I can get scholarships

00:05:35.167 --> 00:05:36.757
"and it's a good university,

00:05:36.757 --> 00:05:38.780
"and so that's where I'm gonna go."

00:05:38.780 --> 00:05:40.418
- And what did you major in?

00:05:40.418 --> 00:05:41.753
- Electrical engineering.

00:05:44.455 --> 00:05:45.660
I didn't have a passion for that,

00:05:45.660 --> 00:05:47.739
but I remember, again, this is

00:05:47.739 --> 00:05:48.960
something from my father's influential.

00:05:48.960 --> 00:05:51.100
I was thinking about getting
into alternative energy,

00:05:51.100 --> 00:05:54.360
this is in, you know, the mid-seventies.

00:05:54.360 --> 00:05:55.687
And he said, "You know,
this computer stuff

00:05:55.687 --> 00:05:57.447
"looks pretty interesting,
these microelectronics.

00:05:57.447 --> 00:05:58.741
"Why don't you think about that?"

00:05:58.741 --> 00:05:59.574
Okay.

00:06:01.127 --> 00:06:05.900
- And then, from there, but
was it there that you got

00:06:05.900 --> 00:06:08.250
what we might call the neuroscience bug?

00:06:08.250 --> 00:06:09.083
- No.
- [Harry] No?

00:06:09.083 --> 00:06:10.380
- No, it wasn't.

00:06:10.380 --> 00:06:12.540
It was right after I graduated,

00:06:12.540 --> 00:06:14.870
and I graduated in the spring of 1979,

00:06:14.870 --> 00:06:17.810
in the fall of 1979, there
was a single topic issue

00:06:17.810 --> 00:06:19.890
of Scientific American,
every September issue,

00:06:19.890 --> 00:06:21.640
and it was about the brain.

00:06:21.640 --> 00:06:23.120
This was one of the best
issues they've ever had,

00:06:23.120 --> 00:06:25.684
lots of other neuroscientists
will say the same.

00:06:25.684 --> 00:06:28.520
Reading that issue is
really what ignited my

00:06:28.520 --> 00:06:29.470
interest in brains.

00:06:32.710 --> 00:06:34.610
- Anything in particular that struck you?

00:06:34.610 --> 00:06:37.350
You seem to be saying that the Crick essay

00:06:37.350 --> 00:06:38.510
was really important.
- Yes, it was.

00:06:38.510 --> 00:06:40.880
So, like a lot of people I read it.

00:06:40.880 --> 00:06:42.320
We're all interested in the brain, right?

00:06:42.320 --> 00:06:44.120
Who wouldn't be interested in the brain?

00:06:44.120 --> 00:06:44.953
It's who we are.

00:06:44.953 --> 00:06:46.530
And so with all these details,

00:06:46.530 --> 00:06:48.040
and you can read about the
neurons and this and this,

00:06:48.040 --> 00:06:50.450
and then the last essay
was by Francis Crick,

00:06:50.450 --> 00:06:54.130
of DNA fame, and he wrote an
essay of something called like

00:06:54.130 --> 00:06:55.930
Thinking About Thinking,
I think it was called.

00:06:55.930 --> 00:06:57.277
And he said, "This is all well and good,

00:06:57.277 --> 00:06:59.290
"we have all this data,
lots of data, data, data,"

00:06:59.290 --> 00:07:01.190
he says, "but we have no theory here."

00:07:02.680 --> 00:07:04.430
There's been no assimilation to this data

00:07:04.430 --> 00:07:06.010
into a theoretical framework,

00:07:06.010 --> 00:07:08.530
and it struck me like a bolt of lightning.

00:07:08.530 --> 00:07:10.640
I said, "Wow, what a great problem."

00:07:10.640 --> 00:07:13.730
We have the data, we
don't have the theory,

00:07:13.730 --> 00:07:15.780
it's not like we're starting from scratch.

00:07:15.780 --> 00:07:17.790
I said, "This is something
we can do in my lifetime."

00:07:17.790 --> 00:07:18.813
And I just decided right then and there

00:07:18.813 --> 00:07:20.840
that I was gonna dedicate my life to that.

00:07:20.840 --> 00:07:25.840
- And then as you're looking
to do advanced graduate work,

00:07:26.170 --> 00:07:29.460
you had interesting experiences both with

00:07:29.460 --> 00:07:31.730
MIT and with Berkeley.

00:07:31.730 --> 00:07:35.120
Talk a little about that,
because it seems to be important.

00:07:35.120 --> 00:07:37.081
- Yeah, so I said, okay.

00:07:37.081 --> 00:07:40.340
I was working in industry
as a computer engineer

00:07:40.340 --> 00:07:41.767
and I said, "Well, I
really wanna do brains,

00:07:41.767 --> 00:07:43.287
"so I need to go back to school.

00:07:43.287 --> 00:07:46.780
"I need a research curriculum in my life."

00:07:46.780 --> 00:07:48.510
So I first applied to MIT,

00:07:48.510 --> 00:07:49.787
which had the AI Lab, and I said,

00:07:49.787 --> 00:07:51.217
"Well they wanna build
intelligent machines,

00:07:51.217 --> 00:07:53.530
"they must be interested
in how intelligence works."

00:07:53.530 --> 00:07:55.657
So I applied there, and they said,

00:07:55.657 --> 00:07:57.167
"Yeah, we wanna build
intelligent machines,

00:07:57.167 --> 00:07:58.930
"but brains don't matter."

00:07:58.930 --> 00:08:01.637
And, literally, one of
the faculty said to me,

00:08:01.637 --> 00:08:03.557
"Brains are just messy computers,

00:08:03.557 --> 00:08:05.432
"so why bother studying them?

00:08:05.432 --> 00:08:06.937
"We're not interested in brains here,

00:08:06.937 --> 00:08:08.490
"you can't do that here."

00:08:08.490 --> 00:08:10.020
And I thought that was wrong,

00:08:10.020 --> 00:08:14.050
but as a young person I was
devastated of being rejected.

00:08:14.050 --> 00:08:16.980
But I said, "Okay, pick
myself up and I'll try again."

00:08:16.980 --> 00:08:20.050
A few years later I got myself into

00:08:20.050 --> 00:08:22.332
a biophysics program at Berkeley,

00:08:22.332 --> 00:08:23.200
which was the closest I could get

00:08:23.200 --> 00:08:24.033
to theoretical neuroscience,

00:08:24.033 --> 00:08:26.851
there wasn't any theoretical
neuroscience back then.

00:08:26.851 --> 00:08:28.890
And I to study a lot of biology,

00:08:28.890 --> 00:08:30.060
I didn't have that in my background,

00:08:30.060 --> 00:08:31.150
so I did a lot of prep for this.

00:08:31.150 --> 00:08:34.670
I got in here and so I
was a graduate student

00:08:34.670 --> 00:08:38.620
at Berkeley starting in
1986, very beginning of 1986.

00:08:38.620 --> 00:08:41.230
But I quickly found out,
about five months into it,

00:08:41.230 --> 00:08:42.270
that I wouldn't be able to do

00:08:42.270 --> 00:08:43.920
what I wanted to do here, either.

00:08:45.700 --> 00:08:50.700
- You reached some conclusions
about the academy from that.

00:08:50.840 --> 00:08:52.473
Is that fair, a fair statement?

00:08:53.563 --> 00:08:54.803
- You mean the academy of Berkeley or--?

00:08:54.803 --> 00:08:57.130
- Just the academy
generally, in other words,

00:08:57.130 --> 00:08:59.622
here you are, a young guy who's saying,

00:08:59.622 --> 00:09:02.377
says, "Hey, I wanna do this from

00:09:02.377 --> 00:09:04.750
"an entirely different approach,"

00:09:04.750 --> 00:09:08.417
and two of the top places
in the country said,

00:09:08.417 --> 00:09:09.500
"Well we can't do that here."

00:09:09.500 --> 00:09:12.100
- Yeah, what was interesting
about Berkeley was

00:09:12.100 --> 00:09:13.790
it was a bit more sympathetic.

00:09:13.790 --> 00:09:15.817
The reaction I got from faculty was,

00:09:15.817 --> 00:09:17.267
"This is great!

00:09:17.267 --> 00:09:19.394
"We need to be doing this.

00:09:19.394 --> 00:09:22.267
"It's great that you wanna,
and you have good ideas.

00:09:22.267 --> 00:09:23.930
"But you can't do it."

00:09:23.930 --> 00:09:26.390
And so I realized at that time,

00:09:26.390 --> 00:09:27.223
and the reason why I couldn't do it

00:09:27.223 --> 00:09:29.400
was because there was no faculty doing

00:09:29.400 --> 00:09:31.750
very close to what I wanted to do.

00:09:31.750 --> 00:09:35.430
And I didn't really understand
that when I came to Berkeley.

00:09:35.430 --> 00:09:38.377
And so, it struck me as, "Oh wow,

00:09:38.377 --> 00:09:40.250
"there's a whole different
aspect to the problem."

00:09:40.250 --> 00:09:42.410
I realized I wasn't just up
against a scientific problem,

00:09:42.410 --> 00:09:44.920
but I had an institutional problem.

00:09:44.920 --> 00:09:47.630
If people agreed that this
was a good thing to do,

00:09:47.630 --> 00:09:49.130
and they tell you you can't do it,

00:09:49.130 --> 00:09:50.350
that's an institutional problem.

00:09:50.350 --> 00:09:52.728
And I have to address that issue as well.

00:09:52.728 --> 00:09:56.730
- Somewhere, maybe it was
in the book on intelligence,

00:09:56.730 --> 00:09:59.340
you suggest that one of the things

00:09:59.340 --> 00:10:01.140
about the academic environment was

00:10:01.140 --> 00:10:03.586
it's reluctant to take risk.

00:10:03.586 --> 00:10:05.550
Is that a fair statement?

00:10:05.550 --> 00:10:08.163
- That was something I was
told over and over again.

00:10:08.163 --> 00:10:09.580
I might've concluded that on my own,

00:10:09.580 --> 00:10:11.863
but it's not something I felt.

00:10:11.863 --> 00:10:15.660
Other people told me
that, they'd explain why.

00:10:15.660 --> 00:10:17.260
It was interesting, later in life

00:10:17.260 --> 00:10:19.730
when I started the Redwood
Neuroscience Institute,

00:10:19.730 --> 00:10:23.930
I had a chance to go visit
NIH and NSF and DARPA.

00:10:23.930 --> 00:10:26.300
I had a good inside person on my board,

00:10:26.300 --> 00:10:28.370
so we got to meet with all
the top program directors

00:10:28.370 --> 00:10:29.550
in neuroscience at NIH,

00:10:29.550 --> 00:10:31.200
and the program directors at NSF,

00:10:32.280 --> 00:10:34.240
and I told them about my
interest in, you know,

00:10:34.240 --> 00:10:35.280
why I'm starting an institute.

00:10:35.280 --> 00:10:36.367
I said, "I'm not raising money,

00:10:36.367 --> 00:10:37.477
"I want to talk to you about working,

00:10:37.477 --> 00:10:39.750
"about how we're gonna develop
models of the neocortex."

00:10:39.750 --> 00:10:41.537
And they were like, "This is great!

00:10:41.537 --> 00:10:42.743
"This is wonderful.

00:10:42.743 --> 00:10:43.750
"We need this, we need
this, we need this."

00:10:43.750 --> 00:10:46.557
And then the NIH program
director started saying,

00:10:46.557 --> 00:10:48.757
"Unfortunately, we can't do any of this.

00:10:48.757 --> 00:10:50.737
"And here's why we are
unable to fund these

00:10:50.737 --> 00:10:51.850
"kinds of research."

00:10:51.850 --> 00:10:53.340
Not just me, I wasn't asking for funding.

00:10:53.340 --> 00:10:54.680
They're saying why they can't fund

00:10:54.680 --> 00:10:56.334
other people who want to do this.

00:10:56.334 --> 00:10:57.770
And it was very, very interesting

00:10:57.770 --> 00:11:00.230
to learn about why it was so difficult

00:11:00.230 --> 00:11:02.420
for neuroscientists to incorporate

00:11:02.420 --> 00:11:04.450
a theoretical paradigm into their work.

00:11:04.450 --> 00:11:06.000
And there's all these different

00:11:07.975 --> 00:11:10.810
forces which were making it
difficult for people to do this.

00:11:10.810 --> 00:11:12.093
- What in particular?

00:11:13.941 --> 00:11:16.170
- I think there's several things.

00:11:16.170 --> 00:11:19.190
One is, at this time, things
have changed now a bit,

00:11:19.190 --> 00:11:22.860
but back then there wasn't a long history

00:11:22.860 --> 00:11:25.350
of theory in neuroscience.

00:11:25.350 --> 00:11:27.860
There wasn't a long history of
theory in biology in general,

00:11:27.860 --> 00:11:29.740
but that changed,

00:11:29.740 --> 00:11:31.978
but in neuroscience it
was later to change.

00:11:31.978 --> 00:11:34.453
It was originally a
classification science.

00:11:36.670 --> 00:11:38.480
So a lot of the old guard, if you will,

00:11:38.480 --> 00:11:39.890
looked down upon theoretical things,

00:11:39.890 --> 00:11:42.010
like, this is about animal studies,

00:11:42.010 --> 00:11:45.120
write labs, there's no
room for theory here.

00:11:45.120 --> 00:11:47.210
Then, the way funding came about,

00:11:47.210 --> 00:11:49.760
this is what I heard from
the NIH directors is that

00:11:50.730 --> 00:11:52.440
the way their funding, it's a very

00:11:52.440 --> 00:11:54.980
consensus, peer-review process.

00:11:54.980 --> 00:11:56.850
And since money is tight,

00:11:56.850 --> 00:12:00.140
nobody wants to take a risk on anything,

00:12:00.140 --> 00:12:02.043
which is slightly speculative.

00:12:04.060 --> 00:12:07.603
I can totally understand this,
there's no anger about this.

00:12:07.603 --> 00:12:09.220
Just, this is the way the world was.

00:12:09.220 --> 00:12:10.640
So it was interesting to learn all these

00:12:10.640 --> 00:12:12.753
forces that were at play there.

00:12:12.753 --> 00:12:16.500
- And at this point in
your career you decide

00:12:16.500 --> 00:12:21.457
to go into the world of computers,

00:12:23.210 --> 00:12:27.890
Silicon Valley, and to
become an entrepreneur,

00:12:27.890 --> 00:12:29.770
inventor, and thinker there.

00:12:29.770 --> 00:12:34.210
And your goal here was to, what?

00:12:34.210 --> 00:12:35.414
- So let me tell you,

00:12:35.414 --> 00:12:37.290
So I was a graduate student at Berkeley.

00:12:37.290 --> 00:12:39.230
And I'd been here for a while,

00:12:39.230 --> 00:12:42.090
and basically I was told, "You
can't do what you wanna do."

00:12:42.090 --> 00:12:43.810
I had a choice, I could do something else

00:12:43.810 --> 00:12:46.150
as a graduate student
related in neuroscience.

00:12:46.150 --> 00:12:47.617
But I looked at that and I said,

00:12:47.617 --> 00:12:48.947
"That's not gonna work for me.

00:12:48.947 --> 00:12:51.087
"I'm gonna get stuck
doing this other thing,

00:12:51.087 --> 00:12:52.647
"and I won't be able to actually end up

00:12:52.647 --> 00:12:54.070
"doing the thing I wanted to do."

00:12:54.070 --> 00:12:55.640
So I was very bummed out about this,

00:12:55.640 --> 00:12:57.587
and so I had to sort of reset my life.

00:12:57.587 --> 00:12:58.757
And I said, "What am I gonna do?

00:12:58.757 --> 00:13:00.120
"I have to do something."

00:13:00.120 --> 00:13:03.210
So I said, "Okay, I'm
going to go back and work."

00:13:03.210 --> 00:13:04.930
I'd already been working in computers

00:13:04.930 --> 00:13:06.170
for a number of years before

00:13:06.170 --> 00:13:07.500
I became a graduate student at Berkeley.

00:13:07.500 --> 00:13:11.030
So I'd given up my career to
become a graduate student.

00:13:11.030 --> 00:13:13.570
And I said, "I'm gonna go
back into the computer field."

00:13:13.570 --> 00:13:14.987
I thought I would do it for four years,

00:13:14.987 --> 00:13:17.140
and I had several goals.

00:13:17.140 --> 00:13:18.180
One goal, as I said, look,

00:13:18.180 --> 00:13:20.240
I'm up against an institutional problem.

00:13:20.240 --> 00:13:22.270
So I need to learn about institution.

00:13:22.270 --> 00:13:24.460
How do I affect broader issues?

00:13:24.460 --> 00:13:25.957
So I said, "Well, you
know, building a business

00:13:25.957 --> 00:13:26.947
"or running a company is a good way

00:13:26.947 --> 00:13:28.127
"of sorta learning how to be

00:13:28.127 --> 00:13:30.800
"a more influential person in the world."

00:13:30.800 --> 00:13:31.907
The second thing I said,

00:13:31.907 --> 00:13:33.357
"Well neuroscience is gonna progress,

00:13:33.357 --> 00:13:34.857
"and maybe people will
become more amenable

00:13:34.857 --> 00:13:36.900
"to the approaches that I wanted to do."

00:13:36.900 --> 00:13:37.897
Another thing, I just said,

00:13:37.897 --> 00:13:39.887
"Well, maybe if I make a
little bit of a name for myself

00:13:39.887 --> 00:13:41.377
"it'll open some doors."

00:13:41.377 --> 00:13:43.727
"And finally I said, "If I
could put some money in the bank

00:13:43.727 --> 00:13:45.347
"then I can afford to be a student again

00:13:45.347 --> 00:13:46.397
"'cause I'm starting to raise a family

00:13:46.397 --> 00:13:50.000
"and I can't really, you know,
be... I have to live a life."

00:13:50.000 --> 00:13:52.960
So I said, "Okay, I'll try
to do that in four years."

00:13:52.960 --> 00:13:57.669
And I, very lucky, I was lucky in my work

00:13:57.669 --> 00:13:59.360
and I was able to be
successful in many ways.

00:13:59.360 --> 00:14:02.180
It turned out that, and
I loved normal computing,

00:14:02.180 --> 00:14:03.930
and I thought, "this is a
great thing to work on."

00:14:03.930 --> 00:14:06.550
After I got into it, it was very difficult

00:14:06.550 --> 00:14:07.910
to extract myself from it.

00:14:07.910 --> 00:14:09.280
I'll tell you a little story.

00:14:09.280 --> 00:14:10.820
When I started Palm,

00:14:10.820 --> 00:14:14.540
and I had to raise money
from venture capitalists,

00:14:14.540 --> 00:14:17.400
I actually put it in the
founding documents of Palm

00:14:17.400 --> 00:14:20.220
that my real interest was neuroscience.

00:14:20.220 --> 00:14:23.720
And that my goal in a
few years was to go back

00:14:23.720 --> 00:14:24.970
and work in neuroscience.

00:14:26.510 --> 00:14:28.871
They were worried that I was
gonna abandon this right away,

00:14:28.871 --> 00:14:31.567
so I said, "Okay, I'm gonna
put in a number of years here,

00:14:31.567 --> 00:14:33.697
"and then, but everyone
knows after those years

00:14:33.697 --> 00:14:35.760
"I can start spending some
time in neuroscience again."

00:14:35.760 --> 00:14:37.670
Well it turned out that
the success of Palm,

00:14:37.670 --> 00:14:40.820
and those ventures really kinda built

00:14:40.820 --> 00:14:43.090
and I got swept away and
it was very difficult

00:14:43.090 --> 00:14:43.923
to extract myself from it.

00:14:43.923 --> 00:14:45.720
It took me quite a few
years to really get myself

00:14:45.720 --> 00:14:47.530
completely out of it again.

00:14:47.530 --> 00:14:49.650
- When you look at your career,

00:14:49.650 --> 00:14:53.963
you had an important insight.

00:14:56.418 --> 00:14:59.150
As I understand it, that insight was

00:14:59.150 --> 00:15:02.860
on the one hand I wanna study and make

00:15:02.860 --> 00:15:05.340
and build intelligent machines.

00:15:05.340 --> 00:15:10.340
But to do that, the model to
look at is how the brain works.

00:15:11.580 --> 00:15:16.480
Is that a fair statement of the two ideas

00:15:16.480 --> 00:15:17.580
that have motivated you?

00:15:17.580 --> 00:15:18.710
- It's a fair statement of the two ideas,

00:15:18.710 --> 00:15:20.460
but they came in a different order.

00:15:20.460 --> 00:15:23.570
My first love was, and is, the brain.

00:15:23.570 --> 00:15:25.510
I wanna understand how my brain works.

00:15:25.510 --> 00:15:27.920
I wanna understand how your brain works.

00:15:27.920 --> 00:15:29.940
Humanity, we are brains.

00:15:29.940 --> 00:15:32.560
Everything we do, our
conversation, our science,

00:15:32.560 --> 00:15:33.770
our literature, our history, our art

00:15:33.770 --> 00:15:35.700
it's a product of brains.

00:15:35.700 --> 00:15:37.060
And our knowledge of the world,

00:15:37.060 --> 00:15:38.500
our ability to ask questions of the world

00:15:38.500 --> 00:15:40.440
is what brains do.

00:15:40.440 --> 00:15:42.860
If I wanna understand
our place in the universe

00:15:42.860 --> 00:15:44.900
I have to understand what brains are.

00:15:44.900 --> 00:15:49.700
So this to me was a
pure love of knowledge.

00:15:49.700 --> 00:15:50.667
I then quickly said,

00:15:50.667 --> 00:15:52.717
"Well, if we figure out how brains work,

00:15:52.717 --> 00:15:54.880
"we can build machines that
work on those principles."

00:15:54.880 --> 00:15:57.243
And that's the way to build
truly intelligent machines.

00:15:57.243 --> 00:15:59.893
That wasn't my number one
motivation, it still isn't,

00:16:02.453 --> 00:16:03.310
and now I've come to understand that's a

00:16:03.310 --> 00:16:04.630
very important thing to do as well,

00:16:04.630 --> 00:16:06.870
so the order is a little different,

00:16:06.870 --> 00:16:07.930
but the two go hand-in-hand.

00:16:07.930 --> 00:16:10.030
You can't really do one without the other.

00:16:12.380 --> 00:16:14.880
- Setting out on this course in that order

00:16:14.880 --> 00:16:18.380
that you just described,
meant you wound up

00:16:19.450 --> 00:16:21.130
wearing a lot of hats.

00:16:21.130 --> 00:16:23.690
And I'm curious because
students watch this interview

00:16:23.690 --> 00:16:28.690
so I'm curious as to what the
skills required, on one hand,

00:16:32.179 --> 00:16:34.680
to be a theorist, then an engineer,

00:16:34.680 --> 00:16:36.250
an inventor, and an entrepreneur.

00:16:36.250 --> 00:16:41.050
And the other hand, the
temperament that's involved.

00:16:41.050 --> 00:16:43.230
Now, what I wanna ask you is,

00:16:43.230 --> 00:16:48.230
is there overlap in these different roles

00:16:48.380 --> 00:16:50.550
are not as different as we think?

00:16:50.550 --> 00:16:52.140
- The roles are pretty different.

00:16:52.140 --> 00:16:54.230
But there is a common theme.

00:16:54.230 --> 00:16:58.793
And the common theme is I had
very clear long-term goals.

00:16:59.760 --> 00:17:01.543
And I was persistent.

00:17:02.570 --> 00:17:05.080
And if, to get to my long-term goals

00:17:05.080 --> 00:17:07.420
I had to start a company,
I'm gonna start a company.

00:17:07.420 --> 00:17:08.770
It's not easy.

00:17:08.770 --> 00:17:10.670
It is a lot of work.

00:17:10.670 --> 00:17:15.350
It is filled with anxiety
and problems and bad days.

00:17:15.350 --> 00:17:18.310
To see my goals, I had to
start a neuroscience institute.

00:17:18.310 --> 00:17:20.563
How do you start a new institute?

00:17:21.450 --> 00:17:22.780
Who's gonna be the first one to show up?

00:17:22.780 --> 00:17:24.010
Do you put a sign on the street

00:17:24.010 --> 00:17:25.890
and say, "Come to my institute"?

00:17:25.890 --> 00:17:28.380
So these are difficult problems,

00:17:28.380 --> 00:17:33.380
but in the end I had
clear goals in my life

00:17:34.420 --> 00:17:35.760
and it's just a matter of persistence

00:17:35.760 --> 00:17:37.210
if I have to learn something totally new.

00:17:37.210 --> 00:17:38.730
I had to learn how to make cell phones.

00:17:38.730 --> 00:17:40.160
I didn't know how to make cell phones.

00:17:40.160 --> 00:17:41.660
I built one of the first smart phones.

00:17:41.660 --> 00:17:43.689
I had to learn how to do that.

00:17:43.689 --> 00:17:44.522
I had to learn how to start companies.

00:17:44.522 --> 00:17:46.440
I had to learn how to run an institute.

00:17:46.440 --> 00:17:51.440
I had to learn how to interact
with academic environments

00:17:52.470 --> 00:17:55.140
and give talks and so on.

00:17:55.140 --> 00:17:56.310
But it's all towards a goal.

00:17:56.310 --> 00:17:58.720
I won't say it was all easy,

00:17:58.720 --> 00:18:00.680
it was actually quite difficult.

00:18:00.680 --> 00:18:02.687
But you have that
long-term goal and you say,

00:18:02.687 --> 00:18:05.657
"Okay, life's challenging,
these are difficult obstacles,

00:18:05.657 --> 00:18:07.363
"but I think I can overcome them."

00:18:07.363 --> 00:18:08.470
Just keep going.

00:18:08.470 --> 00:18:09.450
Just don't give up.

00:18:09.450 --> 00:18:13.070
- So what you're describing in addition is

00:18:13.070 --> 00:18:15.354
it takes courage to do this, is that--?

00:18:15.354 --> 00:18:16.990
- I won't use that word,

00:18:16.990 --> 00:18:18.530
if you want to use that word that's fine.

00:18:18.530 --> 00:18:20.470
I think persistence is the right thing.

00:18:20.470 --> 00:18:21.833
It's the never give up.

00:18:24.290 --> 00:18:29.090
I remember once I gave a talk at UC Davis.

00:18:29.090 --> 00:18:33.470
And I rode in the car with
a friend of mine who is a

00:18:33.470 --> 00:18:35.850
scientist who studies earthquakes.

00:18:35.850 --> 00:18:39.630
And here I was, a computer entrepreneur,

00:18:39.630 --> 00:18:41.220
and I was going to give a talk to a

00:18:41.220 --> 00:18:43.860
neuroscience department about brains.

00:18:43.860 --> 00:18:45.780
And he was like, he was astounded.

00:18:45.780 --> 00:18:49.120
He said, "You feel
comfortable doing that?"

00:18:49.120 --> 00:18:51.050
And he says, "Are they gonna respect you?"

00:18:51.050 --> 00:18:54.280
You know, "What are your
credentials for doing this?"

00:18:54.280 --> 00:18:59.280
And he thought this is
like so gutsy or so crazy.

00:18:59.600 --> 00:19:03.120
And I'm like, well, what
else am I going to do, right?

00:19:03.120 --> 00:19:04.780
So there's this sort of attitude,

00:19:04.780 --> 00:19:06.220
and it's a little awkward at first.

00:19:06.220 --> 00:19:07.710
You get up there and, "Who's this guy?"

00:19:07.710 --> 00:19:08.700
and "Why is he talking to us?"

00:19:08.700 --> 00:19:11.423
and maybe I don't speak
the same language they do.

00:19:12.517 --> 00:19:13.350
That's often the case,

00:19:13.350 --> 00:19:14.810
especially in a complex
field like neuroscience

00:19:14.810 --> 00:19:15.860
there's lots of different languages

00:19:15.860 --> 00:19:17.570
and different approaches people take.

00:19:17.570 --> 00:19:19.160
But you just plow forward

00:19:19.160 --> 00:19:21.505
and you just have to go with the punches.

00:19:21.505 --> 00:19:26.360
- What in your formative
experiences made you this way?

00:19:26.360 --> 00:19:27.780
- I remember my parents trying

00:19:27.780 --> 00:19:29.050
to tell me about Santa Claus as a kid.

00:19:29.050 --> 00:19:30.487
And I immediately said,
"That's not possible.

00:19:30.487 --> 00:19:32.260
"I just don't believe that."

00:19:32.260 --> 00:19:34.030
And--
(Harry laughs)

00:19:34.030 --> 00:19:35.910
I had a problem as a child,

00:19:35.910 --> 00:19:37.330
which my mother took me to a doctor

00:19:37.330 --> 00:19:39.620
to see if it was something wrong with me.

00:19:39.620 --> 00:19:43.570
The problem was that I would
hear what I expected to hear

00:19:43.570 --> 00:19:44.840
and not what people were actually saying.

00:19:44.840 --> 00:19:47.020
I literally, you'd say a sentence to me

00:19:47.020 --> 00:19:50.170
and I would hear it as what I
thought you were going to say.

00:19:50.170 --> 00:19:51.917
It was so problematic
at times my mother said,

00:19:51.917 --> 00:19:54.030
"Maybe there's something
wrong with your hearing."

00:19:54.030 --> 00:19:55.967
And I internalized this, said,

00:19:55.967 --> 00:19:57.247
"You know what, I just
have a model of the world.

00:19:57.247 --> 00:20:00.260
"I have this expectation
of what's going to happen,

00:20:00.260 --> 00:20:02.310
and that's so strong it drives me."

00:20:02.310 --> 00:20:05.050
So it was a very internally
focused view of the world

00:20:05.050 --> 00:20:07.923
and I think that stuck
with me for my whole life.

00:20:09.150 --> 00:20:14.043
- You are a theorist in a field
that didn't have a theory.

00:20:15.310 --> 00:20:18.210
So in understanding you
would actually get some

00:20:18.210 --> 00:20:21.060
important insights about theorizing.

00:20:21.060 --> 00:20:23.980
And going back to the Crick article,

00:20:23.980 --> 00:20:28.540
in other words, the theorist's problem is

00:20:29.520 --> 00:20:31.910
that there's a lot of data coming in

00:20:31.910 --> 00:20:34.760
from different streams of information.

00:20:34.760 --> 00:20:39.760
And in the end, there is no
big explanatory understanding.

00:20:42.530 --> 00:20:44.693
Talk about theorizing, 'cause this is

00:20:44.693 --> 00:20:47.030
a very different cult
than being an engineer.

00:20:47.030 --> 00:20:49.920
- So, when I realized I had
this institutional problem,

00:20:49.920 --> 00:20:52.890
like being a theorist in a
field that doesn't have theory,

00:20:52.890 --> 00:20:56.010
I went back and studied the
history of scientific ideas.

00:20:56.010 --> 00:20:57.620
I went back and read Thomas Kuhn

00:20:57.620 --> 00:20:59.890
and The History of Scientific Revolutions.

00:20:59.890 --> 00:21:01.127
I tried to figure, "What happens here?

00:21:01.127 --> 00:21:01.990
"How do you go about this?"

00:21:01.990 --> 00:21:04.080
And that was actually very useful for me.

00:21:04.080 --> 00:21:07.810
Because Kuhn talks about what
you do differently in science

00:21:07.810 --> 00:21:10.330
when you're working in a field
that doesn't have a paradigm.

00:21:10.330 --> 00:21:12.550
And you have to sorta be
a little bit different.

00:21:12.550 --> 00:21:14.260
He talks about how the way you publish

00:21:14.260 --> 00:21:15.780
and the way you interact

00:21:15.780 --> 00:21:17.580
and how scientists conduct in science.

00:21:17.580 --> 00:21:18.857
So I took that as sort of,

00:21:18.857 --> 00:21:20.877
"Great, I can be a little different here."

00:21:20.877 --> 00:21:23.370
I might have to take different approaches.

00:21:23.370 --> 00:21:25.770
I might have to publish
this as a popular book

00:21:25.770 --> 00:21:28.940
as opposed to a scientific
paper and so on.

00:21:28.940 --> 00:21:31.380
And so I took that process,

00:21:31.380 --> 00:21:32.900
I really sorta thought about the process

00:21:32.900 --> 00:21:34.829
of what you'd have to do about this.

00:21:34.829 --> 00:21:38.180
And it led me down various paths

00:21:39.046 --> 00:21:40.760
that I might not have done otherwise.

00:21:40.760 --> 00:21:44.710
- In your lecture yesterday,
in the introduction

00:21:44.710 --> 00:21:47.070
you said that when you realized

00:21:47.070 --> 00:21:49.230
you couldn't do a PhD at Berkeley,

00:21:49.230 --> 00:21:52.390
you took a year to go to the library.

00:21:52.390 --> 00:21:53.870
Talk a little about that, because--

00:21:53.870 --> 00:21:54.760
- [Jeff] Yeah.
- ...in other words,

00:21:54.760 --> 00:21:57.410
you had to master the field--

00:21:57.410 --> 00:21:58.243
- [Jeff] Yeah.

00:21:58.243 --> 00:21:59.880
- ...that is the information side.

00:21:59.880 --> 00:22:02.096
- So I came to Berkeley, the
first thing I wanted to do

00:22:02.096 --> 00:22:02.929
is I wanted to sign up.

00:22:02.929 --> 00:22:03.790
I said, "These are all the neuroscience

00:22:03.790 --> 00:22:05.050
classes I wanna take."

00:22:05.050 --> 00:22:06.287
And my first disappointment was they said,

00:22:06.287 --> 00:22:07.477
"You can't take them.

00:22:07.477 --> 00:22:09.177
"You have to take this
biochemistry class."

00:22:09.177 --> 00:22:12.490
As I said I couldn't
craft my own curriculum.

00:22:12.490 --> 00:22:13.820
So I said, "All right,
I'll do those things."

00:22:13.820 --> 00:22:15.860
And then I took Jeffrey
Weiner's anatomy class,

00:22:15.860 --> 00:22:16.693
which was like, "Great,

00:22:16.693 --> 00:22:17.526
"this is the best thing in the world."

00:22:17.526 --> 00:22:19.490
And I just immersed myself in it.

00:22:19.490 --> 00:22:20.960
But when I found that I couldn't really do

00:22:20.960 --> 00:22:22.757
what I wanted to do, I said,

00:22:22.757 --> 00:22:27.100
"Okay, well I've quit my
job, I'm here, what do I do?"

00:22:27.100 --> 00:22:29.800
I said, "Let me make my own education."

00:22:29.800 --> 00:22:34.379
And so I spent a year, about 12 months,

00:22:34.379 --> 00:22:37.610
coming, I had a student pass
but I wasn't taking any classes

00:22:37.610 --> 00:22:38.750
or had very little interaction

00:22:38.750 --> 00:22:40.050
with anyone at the university.

00:22:40.050 --> 00:22:42.410
But I'd go to the
libraries here once a week

00:22:42.410 --> 00:22:44.460
with a list of papers I wanna read.

00:22:44.460 --> 00:22:45.601
I would look up those papers,

00:22:45.601 --> 00:22:48.030
cause there was no Internet, right?

00:22:48.030 --> 00:22:50.090
It's all paper in some library someplace,

00:22:50.090 --> 00:22:51.377
and I look at the journal and I say,

00:22:51.377 --> 00:22:52.290
"Okay, that's a good one."

00:22:52.290 --> 00:22:53.360
Or, "That's not a good one."

00:22:53.360 --> 00:22:55.270
And then I photocopied
a whole bunch of these.

00:22:55.270 --> 00:22:57.870
I had a card and I'd come
up with a stack of papers,

00:22:57.870 --> 00:23:00.270
literally a stack of papers
several inches thick every week.

00:23:00.270 --> 00:23:02.600
I'd go home and I'd read them.

00:23:02.600 --> 00:23:04.430
And some of them were
very, very hard to read.

00:23:04.430 --> 00:23:07.770
Some papers are very obtuse
and the language isn't simple.

00:23:07.770 --> 00:23:11.360
And so I did this over
and over again every week,

00:23:11.360 --> 00:23:13.640
and then I would mark which references

00:23:13.640 --> 00:23:14.870
I wanted to follow up on.

00:23:14.870 --> 00:23:17.250
Then I'd go back to the library

00:23:17.250 --> 00:23:18.850
and I would go through those again,

00:23:18.850 --> 00:23:20.890
I would spend the day
reading here, basically.

00:23:20.890 --> 00:23:24.036
And then I'd go home again,
I lived about an hour away.

00:23:24.036 --> 00:23:28.930
I did this for about a year,
and I just built my own view

00:23:28.930 --> 00:23:30.550
of what the world of neuroscience is.

00:23:30.550 --> 00:23:32.770
I also read linguistics
and some philosophy

00:23:32.770 --> 00:23:34.530
and some psychology.

00:23:34.530 --> 00:23:35.860
Anyone who I thought might have

00:23:35.860 --> 00:23:38.300
tangential relationship
to how the brain works.

00:23:38.300 --> 00:23:41.370
And I made my own view of
all these different fields.

00:23:41.370 --> 00:23:43.300
Which ones are speaking to me as like,

00:23:43.300 --> 00:23:44.430
yeah, these are on the right track

00:23:44.430 --> 00:23:46.650
and these are not on the right track.

00:23:46.650 --> 00:23:47.960
I think that was a luxury that

00:23:47.960 --> 00:23:51.291
most young people don't
have an option to do.

00:23:51.291 --> 00:23:54.400
They get down a track
and they're in some lab

00:23:54.400 --> 00:23:56.230
and they learn the language of that lab.

00:23:56.230 --> 00:23:58.580
But to be able to do this
sort of self directed

00:24:00.560 --> 00:24:02.610
study was, and there's a huge amount

00:24:02.610 --> 00:24:03.580
of neuroscience research, so

00:24:03.580 --> 00:24:06.100
that was a really unusual thing.

00:24:06.100 --> 00:24:09.890
- So in a way, opting out
or being forced to opt out

00:24:09.890 --> 00:24:12.390
of graduate school was a blessing.

00:24:12.390 --> 00:24:14.173
- Well, it didn't look it at the time.

00:24:14.173 --> 00:24:16.141
I'll be honest with you.

00:24:16.141 --> 00:24:18.300
In hindsight you could say that.

00:24:18.300 --> 00:24:21.540
I was kinda hoping I would be
able to do that as a student,

00:24:21.540 --> 00:24:22.940
as a graduate student, right?

00:24:22.940 --> 00:24:24.830
I would be encouraged to do this.

00:24:24.830 --> 00:24:27.817
I would be someone saying,
"Here's some suggestions."

00:24:28.720 --> 00:24:30.630
But in the end it was what I needed to do,

00:24:30.630 --> 00:24:33.310
and it was a great thing to do.

00:24:33.310 --> 00:24:36.863
I just read voraciously in the field.

00:24:38.907 --> 00:24:40.530
- So you leave graduate school,

00:24:40.530 --> 00:24:41.790
you already mentioned you went out

00:24:41.790 --> 00:24:44.410
and developed some products.

00:24:44.410 --> 00:24:46.790
I wanna, as we are understanding

00:24:46.790 --> 00:24:48.340
the different hats you've worn,

00:24:49.480 --> 00:24:53.620
you had a sense of the
importance of mobile devices

00:24:53.620 --> 00:24:56.760
and what they could mean
for business and the world

00:24:56.760 --> 00:25:00.570
if people in the field could actually

00:25:00.570 --> 00:25:03.380
gather this data and it could be mastered.

00:25:03.380 --> 00:25:06.110
- Well, I did have, in about 1991,

00:25:07.730 --> 00:25:10.700
I sorta had an epiphany
about mobile devices.

00:25:10.700 --> 00:25:13.040
Now I had worked for a
company called Grid Systems,

00:25:13.040 --> 00:25:14.560
and we had built mobile computers,

00:25:14.560 --> 00:25:15.900
they invented a laptop.

00:25:15.900 --> 00:25:18.420
But I had this sort of
epiphany about the future of

00:25:18.420 --> 00:25:20.850
pocket-sized computers, mobile devices.

00:25:20.850 --> 00:25:22.837
And I said "Oh my goodness, in the future

00:25:22.837 --> 00:25:25.640
"everyone's primary computer
is gonna be in their pocket."

00:25:25.640 --> 00:25:28.590
And this is the way we
can bring the benefits

00:25:28.590 --> 00:25:29.920
of computers to billions of people.

00:25:29.920 --> 00:25:32.100
'Cause in that time it
was very, very expensive

00:25:32.100 --> 00:25:33.240
and difficult to maintain these things.

00:25:33.240 --> 00:25:36.450
It took lots of power and
they're very complex machines.

00:25:36.450 --> 00:25:37.557
I said, "No, it's gonna be simple.

00:25:37.557 --> 00:25:38.427
"They're gonna be in your pocket.

00:25:38.427 --> 00:25:39.297
"They're gonna be inexpensive,

00:25:39.297 --> 00:25:40.950
"billions of people around
the world can have them."

00:25:40.950 --> 00:25:42.910
This sounded crazy at the time.

00:25:42.910 --> 00:25:44.800
Because back then there was no Internet,

00:25:44.800 --> 00:25:47.440
no web browsers, there was
no wireless data networks,

00:25:47.440 --> 00:25:48.740
no one was using a cellular phone,

00:25:48.740 --> 00:25:50.277
in the United States at least.

00:25:50.277 --> 00:25:53.454
There was no technology for doing this,

00:25:53.454 --> 00:25:55.730
there was no battery, the
memory technology didn't exist.

00:25:55.730 --> 00:25:57.518
None of this existed.

00:25:57.518 --> 00:25:59.357
And people said, "What are you gonna do

00:25:59.357 --> 00:26:00.430
"with this mobile device?"

00:26:00.430 --> 00:26:01.707
I couldn't even answer
that question, I said,

00:26:01.707 --> 00:26:03.080
"I just know it's gonna happen."

00:26:03.080 --> 00:26:04.240
I could see this.

00:26:04.240 --> 00:26:05.630
Inevitable more and more,

00:26:05.630 --> 00:26:06.503
people are gonna access information

00:26:06.503 --> 00:26:08.230
with these things in their pocket.

00:26:08.230 --> 00:26:10.020
So it sounded kinda crazy.

00:26:10.020 --> 00:26:12.220
And I used to talk about
this in my VP Marketing,

00:26:12.220 --> 00:26:13.107
they said, "Jeff, stop talking about that.

00:26:13.107 --> 00:26:14.397
"It sounds stupid."

00:26:15.365 --> 00:26:18.400
But I believed it and I
said, "It's gonna happen."

00:26:18.400 --> 00:26:19.847
I said, "This is a good thing.

00:26:19.847 --> 00:26:21.777
"And this is a good thing to go after.

00:26:21.777 --> 00:26:22.630
"This is a good thing to do."

00:26:22.630 --> 00:26:24.620
I thought it was gonna be
really good for the world.

00:26:24.620 --> 00:26:25.577
And I thought, "Okay, that's a thing

00:26:25.577 --> 00:26:27.237
"to work on for a few years.

00:26:27.237 --> 00:26:29.019
"And let's see if we can
make progress on that."

00:26:29.019 --> 00:26:32.430
And we succeeded, I think we succeeded

00:26:32.430 --> 00:26:34.410
where a lot of other companies failed.

00:26:34.410 --> 00:26:36.410
There's a lot of failures in this space.

00:26:36.410 --> 00:26:39.360
We succeeded I believe because
I had that long-term vision.

00:26:39.360 --> 00:26:41.510
It wasn't like I was
building this or that,

00:26:41.510 --> 00:26:43.497
I was like, "No, we're
bringing computers to billions

00:26:43.497 --> 00:26:45.877
"of people and this is the
attributes it has to have.

00:26:45.877 --> 00:26:47.770
"Therefore this is going to happen."

00:26:47.770 --> 00:26:49.640
So, regardless of our failures

00:26:49.640 --> 00:26:50.803
we can continue working on this

00:26:50.803 --> 00:26:52.900
because it's going to happen.

00:26:52.900 --> 00:26:55.680
And we were lucky we actually did succeed,

00:26:55.680 --> 00:26:57.420
in a big way, actually.

00:26:57.420 --> 00:26:59.180
- It's funny, as I listen to you,

00:26:59.180 --> 00:27:04.170
you combine academic skills
without being an academic.

00:27:04.170 --> 00:27:09.170
And then with a very interesting
set of people skills,

00:27:11.780 --> 00:27:16.450
and a kind of sonar
mechanism for understanding

00:27:16.450 --> 00:27:18.270
what's going on in the
world and changing it.

00:27:18.270 --> 00:27:20.460
Is that fair?

00:27:20.460 --> 00:27:21.716
- I guess in hindsight you could say,

00:27:21.716 --> 00:27:23.420
it's like, yeah, I think it's fair.

00:27:23.420 --> 00:27:24.823
I didn't view it that way.

00:27:24.823 --> 00:27:27.047
I was viewing it like,
"I wanna do something.

00:27:27.047 --> 00:27:28.607
"This is important.

00:27:28.607 --> 00:27:29.687
"What do I gotta do?"

00:27:30.670 --> 00:27:35.040
- So, you founded this institute

00:27:35.040 --> 00:27:38.710
and ultimately you gave
the institute to Berkeley.

00:27:38.710 --> 00:27:42.070
But what were the difficulties
of founding an institute

00:27:42.070 --> 00:27:45.530
in a field that was not yet recognized?

00:27:45.530 --> 00:27:46.380
- Well, it was interesting,

00:27:46.380 --> 00:27:49.160
the idea for setting the
institute was not mine.

00:27:49.160 --> 00:27:50.750
I had a bunch of neuroscience friends

00:27:50.750 --> 00:27:54.220
and they were sympathetic with my goals

00:27:54.220 --> 00:27:55.790
of understanding neocortical theory.

00:27:55.790 --> 00:27:57.970
They understand the difficulties,
why this is hard to do,

00:27:57.970 --> 00:27:59.610
why you can't get funding for this.

00:27:59.610 --> 00:28:01.627
And some of them suggested, "You know,

00:28:01.627 --> 00:28:03.470
"the way to do this is
to create an institute."

00:28:03.470 --> 00:28:06.890
And I said, "Oh yeah," like,
"Sure, that's gonna be easy."

00:28:06.890 --> 00:28:08.610
That sounds like impossible, right?

00:28:08.610 --> 00:28:11.694
It wasn't impossible from
a funding point of view,

00:28:11.694 --> 00:28:12.830
because I could actually fund it myself.

00:28:12.830 --> 00:28:17.000
I was lucky to be wealthy
at that time from my work.

00:28:17.000 --> 00:28:19.897
But it was more like,
"How do I structure this?"

00:28:19.897 --> 00:28:21.927
"How do I get respect for this?"

00:28:21.927 --> 00:28:23.360
"Who's gonna wanna work there?"

00:28:23.360 --> 00:28:25.900
So on, so I said to a
bunch of neuroscientists,

00:28:25.900 --> 00:28:29.147
I said, "Look, I will
do this if you help me.

00:28:29.147 --> 00:28:32.058
"If you will bring people and ideas

00:28:32.058 --> 00:28:35.620
"and help me get through these problems."

00:28:35.620 --> 00:28:38.320
And one of the people I talked to was

00:28:38.320 --> 00:28:40.360
Bob Knight at Berkeley,
who was the head of the

00:28:40.360 --> 00:28:41.980
Helen Wills Neuroscience
Institute at the time.

00:28:41.980 --> 00:28:43.730
And he said, "Okay, I'll help you."

00:28:43.730 --> 00:28:44.960
And a few other people did the same,

00:28:44.960 --> 00:28:47.650
so I said, "Okay, we'll get going."

00:28:47.650 --> 00:28:50.480
The problems were, first of all there's

00:28:50.480 --> 00:28:51.790
all these logistical things,

00:28:51.790 --> 00:28:52.970
which is like starting a business.

00:28:52.970 --> 00:28:53.900
We've gotta have a location,

00:28:53.900 --> 00:28:55.120
you gotta have a corporate structure,

00:28:55.120 --> 00:28:56.770
you gotta have boards, advisory boards,

00:28:56.770 --> 00:28:59.830
you know, legal documents,
and pay salaries,

00:28:59.830 --> 00:29:01.860
and these things take a lot of time.

00:29:01.860 --> 00:29:03.140
And then there's like,

00:29:03.140 --> 00:29:04.600
how are you gonna structure the science?

00:29:04.600 --> 00:29:05.960
And who's going to work there?

00:29:05.960 --> 00:29:07.669
Under what terms?

00:29:07.669 --> 00:29:11.250
So, it got going in fits and starts.

00:29:11.250 --> 00:29:12.900
After we got going it turned out that the

00:29:12.900 --> 00:29:15.400
Redwood Neuroscience Institute became

00:29:15.400 --> 00:29:17.400
the hot place to be.

00:29:17.400 --> 00:29:19.640
People started hearing
about it around the world

00:29:19.640 --> 00:29:21.930
and the best thing we did is we had

00:29:21.930 --> 00:29:23.810
every week, we brought
in outside speakers.

00:29:23.810 --> 00:29:24.887
We'd fly people and we'd say,

00:29:24.887 --> 00:29:26.800
"Who do we wanna hear from?"

00:29:26.800 --> 00:29:28.120
We wanna study the basal ganglia?

00:29:28.120 --> 00:29:29.930
Who's the best basal gangliar person?

00:29:29.930 --> 00:29:30.763
Who's gonna tell us?

00:29:30.763 --> 00:29:33.290
And we'd say, "We wanna
hear from this person."

00:29:33.290 --> 00:29:35.247
Sometimes we'd fly them in,
we'd pay their way, we'd say,

00:29:35.247 --> 00:29:37.753
"You guys are gonna give a talk
and we're gonna grill you."

00:29:37.753 --> 00:29:38.670
They were very unusual the talks at the

00:29:38.670 --> 00:29:39.630
Redwood Neuroscience Institute

00:29:39.630 --> 00:29:41.508
because usually scientists
are very polite.

00:29:41.508 --> 00:29:44.590
So they'd come in and we'd have
20 or 25 people in the room,

00:29:44.590 --> 00:29:46.160
and we would just tear into them.

00:29:46.160 --> 00:29:47.690
Not in a mean way, but in a great way.

00:29:47.690 --> 00:29:49.110
Like, explain this more.

00:29:49.110 --> 00:29:50.750
We didn't understand this in your paper.

00:29:50.750 --> 00:29:51.583
And people over and over said,

00:29:51.583 --> 00:29:53.747
"My God, you really care
about what I'm doing.

00:29:53.747 --> 00:29:55.386
"You've read my papers."

00:29:55.386 --> 00:29:59.190
Word spread that getting an invitation

00:29:59.190 --> 00:30:00.380
to the Redwood Neuroscience Institute

00:30:00.380 --> 00:30:02.480
was something to be had.

00:30:02.480 --> 00:30:04.610
And so over the course of
the three years that I ran it

00:30:04.610 --> 00:30:08.223
we had over 100 visitors,
about 120 I think it was.

00:30:09.060 --> 00:30:13.640
And it became sort of
a cool little hot spot

00:30:13.640 --> 00:30:16.570
in the neuroscience world for
people who cared about this.

00:30:16.570 --> 00:30:20.150
- And interestingly,
obviously it was your money,

00:30:20.150 --> 00:30:24.170
the fact that you put
in place an institution.

00:30:24.170 --> 00:30:28.420
But really, it was the mission statement

00:30:28.420 --> 00:30:29.900
that was really important.

00:30:29.900 --> 00:30:32.560
We're gonna look at what you're saying,

00:30:32.560 --> 00:30:36.290
but not tied to the detail.

00:30:36.290 --> 00:30:39.959
How can what you're saying
impact the big picture?

00:30:39.959 --> 00:30:40.922
Is that fair?

00:30:40.922 --> 00:30:42.196
- Yeah, yeah it is fair.

00:30:42.196 --> 00:30:43.660
And what we did, everyone knew
the mission of the institute.

00:30:43.660 --> 00:30:45.420
It is neocortical theory.

00:30:45.420 --> 00:30:46.940
I also told them right upfront,

00:30:46.940 --> 00:30:49.110
this is not a place to retire.

00:30:49.110 --> 00:30:50.840
This is a place to work on that problem.

00:30:50.840 --> 00:30:53.680
And I'm not guaranteeing your
job for any amount of time.

00:30:53.680 --> 00:30:55.370
'Cause I knew I didn't
wanna run an institute

00:30:55.370 --> 00:30:56.746
for the rest of my life.

00:30:56.746 --> 00:30:58.227
I said, "As long as we're
making progress in this problem,

00:30:58.227 --> 00:31:00.030
"you know you're welcome to be here."

00:31:00.030 --> 00:31:01.877
And I said, "You can do anything you want,

00:31:01.877 --> 00:31:03.123
"but here's the things,

00:31:03.123 --> 00:31:05.557
"you have to attend every
lecture from an outside speaker.

00:31:05.557 --> 00:31:07.177
"You have to journal club.

00:31:07.177 --> 00:31:08.537
"And you have to
participate in these things,

00:31:08.537 --> 00:31:10.487
"it's not optional to do those things."

00:31:13.550 --> 00:31:14.887
And everyone collectively is gonna say,

00:31:14.887 --> 00:31:17.487
"Is your work contributing
to that broader mission?"

00:31:18.350 --> 00:31:19.790
But other than that, you
can do whatever you want.

00:31:19.790 --> 00:31:21.040
You can publish or not publish,

00:31:21.040 --> 00:31:23.750
you can travel, you can
do whatever you want.

00:31:23.750 --> 00:31:25.270
So that's how I set it up,

00:31:25.270 --> 00:31:30.000
and it was an unusual structure
for a research institute.

00:31:30.000 --> 00:31:31.760
And some of it worked and
some of it didn't work,

00:31:31.760 --> 00:31:34.310
but some parts were very successful.

00:31:34.310 --> 00:31:39.310
- Your mission statement involved
a focus on the neocortex.

00:31:40.090 --> 00:31:44.140
Explain to our audience: why that focus?

00:31:44.140 --> 00:31:47.313
Why is the neocortex so
important, and what is it?

00:31:49.000 --> 00:31:51.750
- Mammals have a neocortex, all mammals.

00:31:51.750 --> 00:31:53.013
Non-mammals don't.

00:31:54.391 --> 00:31:57.330
In humans and other animals
of high intelligence

00:31:57.330 --> 00:31:58.560
it's very big.

00:31:58.560 --> 00:31:59.790
So in the human brain it's about

00:31:59.790 --> 00:32:01.210
60 percent of the volume of your brain.

00:32:01.210 --> 00:32:03.020
If you look at a brain
it's the big wrinkly thing

00:32:03.020 --> 00:32:03.920
that covers everything else.

00:32:03.920 --> 00:32:04.930
It's like a big sheet of cells

00:32:04.930 --> 00:32:06.260
that wraps around the rest of the brain,

00:32:06.260 --> 00:32:08.150
which is actually pretty small.

00:32:08.150 --> 00:32:10.860
And why it's so important is because

00:32:10.860 --> 00:32:13.872
it is the locus of intelligence.

00:32:13.872 --> 00:32:18.050
All language, spoken,
written, mathematics, physics,

00:32:18.050 --> 00:32:21.890
music, whatever, it's all
a product of the neocortex.

00:32:21.890 --> 00:32:23.770
Both production of it
and understanding it.

00:32:23.770 --> 00:32:25.692
All high level vision

00:32:25.692 --> 00:32:26.820
and all high level motor planning.

00:32:26.820 --> 00:32:28.180
The rest of the brain
is very, very important

00:32:28.180 --> 00:32:29.950
to being human, it does a lot of stuff.

00:32:29.950 --> 00:32:31.270
But when we think about, you know,

00:32:31.270 --> 00:32:32.560
when we're talking about discovering

00:32:32.560 --> 00:32:33.680
the properties of the world

00:32:33.680 --> 00:32:35.470
or communicating through language

00:32:35.470 --> 00:32:38.610
or the conversation you
and I are having right now,

00:32:38.610 --> 00:32:40.070
it's the neocortex.

00:32:40.070 --> 00:32:43.970
And so my goal is a, that
was the most interesting part

00:32:43.970 --> 00:32:45.380
of the brain to focus on

00:32:45.380 --> 00:32:47.350
if you wanna understand
what intelligence is,

00:32:47.350 --> 00:32:50.760
and also, it turns out
that it is, in some ways,

00:32:50.760 --> 00:32:52.320
one of the most regular
parts of the brain.

00:32:52.320 --> 00:32:53.766
That is, even though
it's very, very large,

00:32:53.766 --> 00:32:57.560
it's much more, it's not homogenous,

00:32:57.560 --> 00:33:02.010
but it is a repeated structure
over and over and over again.

00:33:02.010 --> 00:33:03.220
With the other parts of the brain

00:33:03.220 --> 00:33:05.590
have been around for a much longer time

00:33:05.590 --> 00:33:07.820
in evolutionary time and
they're highly evolved.

00:33:07.820 --> 00:33:09.690
So you've got highly
evolved things for emotions

00:33:09.690 --> 00:33:12.147
and highly evolved things for
auditory location and so on,

00:33:12.147 --> 00:33:14.870
but the neocortex appeared big recently.

00:33:14.870 --> 00:33:16.360
It just like, boom!

00:33:16.360 --> 00:33:18.090
And so it had this regular structure,

00:33:18.090 --> 00:33:20.290
so the idea was like, you could figure out

00:33:20.290 --> 00:33:22.784
principles of how the neocortex works

00:33:22.784 --> 00:33:24.860
which supplies to all high
level thought processes.

00:33:24.860 --> 00:33:26.240
I don't need to have a theory of vision

00:33:26.240 --> 00:33:28.240
and a theory of language and a theory of

00:33:29.210 --> 00:33:30.950
how the brain does physics.

00:33:30.950 --> 00:33:32.477
It's really all the same thing,

00:33:32.477 --> 00:33:34.190
this is a miraculous
discovery, it's not mine,

00:33:34.190 --> 00:33:35.380
but it's a miraculous discovery.

00:33:35.380 --> 00:33:36.580
And I said, "Wow, great!

00:33:37.537 --> 00:33:40.220
"This is the center and it's a possibility

00:33:40.220 --> 00:33:41.840
for finding out a few key principles,

00:33:41.840 --> 00:33:43.250
and when you figure out those principles

00:33:43.250 --> 00:33:45.200
you can explain a lot.

00:33:45.200 --> 00:33:47.148
- And so how did you go about

00:33:47.148 --> 00:33:51.860
discovering those principles or finding

00:33:51.860 --> 00:33:53.910
who had talked about those principles?

00:33:53.910 --> 00:33:55.993
It's not as if you found them new.

00:33:56.860 --> 00:34:00.123
- Well, there's clues everywhere.

00:34:01.310 --> 00:34:04.640
First of all, very, very
few people have that focus.

00:34:04.640 --> 00:34:06.360
They're not even looking for those things.

00:34:06.360 --> 00:34:08.270
And so the clues are embedded

00:34:08.270 --> 00:34:09.710
in all these different disciplines

00:34:09.710 --> 00:34:11.370
about when people write
a little bit about this

00:34:11.370 --> 00:34:12.230
and a little bit about that,

00:34:12.230 --> 00:34:15.190
and a large part of the
problem that I faced

00:34:15.190 --> 00:34:16.880
was what to ignore.

00:34:16.880 --> 00:34:19.560
The field of neuroscience has so much data

00:34:19.560 --> 00:34:22.290
on so many things and
so many different ideas.

00:34:22.290 --> 00:34:25.110
It's unbelievable how
much has been published

00:34:25.110 --> 00:34:26.720
in this field, you can't get a sense of it

00:34:26.720 --> 00:34:30.906
until you try to do a broad
research literature study.

00:34:30.906 --> 00:34:34.760
So a lot of it was, like,
sorting through the noise

00:34:34.760 --> 00:34:36.580
and figuring out where are the gems here.

00:34:36.580 --> 00:34:38.820
Where are the key ideas?

00:34:38.820 --> 00:34:40.517
And then you take those key ideas,

00:34:40.517 --> 00:34:43.570
and that's a hit-and-miss process.

00:34:43.570 --> 00:34:44.790
Perhaps it involves some skill,

00:34:44.790 --> 00:34:46.620
it certainly involves an opinion

00:34:46.620 --> 00:34:48.580
about how to go about that, and my opinion

00:34:48.580 --> 00:34:49.700
is different than other people's opinion,

00:34:49.700 --> 00:34:51.493
but I trusted my instincts on this.

00:34:52.410 --> 00:34:55.790
And then I'd say, "Okay, now
we have these core pieces that

00:34:55.790 --> 00:34:57.550
I'm gonna ignore all these
other things for now,

00:34:57.550 --> 00:34:58.640
How do I assemble those

00:34:58.640 --> 00:35:00.390
into a certain theoretical framework?

00:35:00.390 --> 00:35:02.040
Some of it was out there, kinda hidden.

00:35:02.040 --> 00:35:03.968
Some of it I had to come up with myself.

00:35:03.968 --> 00:35:08.968
- And the purpose of a
theory, going back to Kuhn,

00:35:09.830 --> 00:35:14.830
a paradigm, is to in a way
give us a simple statement

00:35:15.040 --> 00:35:19.290
that tells us which data is important

00:35:19.290 --> 00:35:22.240
and lays out a model or a paradigm.

00:35:22.240 --> 00:35:23.723
- Yes, yes.

00:35:24.590 --> 00:35:27.140
And that's what I'm trying
to do in my life's work.

00:35:28.817 --> 00:35:30.170
And my talk yesterday, I talked about it.

00:35:30.170 --> 00:35:31.670
I said, "What is the framework?"

00:35:31.670 --> 00:35:33.070
And I even listed out principles.

00:35:33.070 --> 00:35:33.903
I said, "These are the some

00:35:33.903 --> 00:35:36.008
of the key principles, I believe in."

00:35:36.008 --> 00:35:40.133
And here's the language
I use to describe them,

00:35:40.133 --> 00:35:41.240
because that's another problem.

00:35:41.240 --> 00:35:42.810
Because everybody has different language

00:35:42.810 --> 00:35:44.823
for the same, similar ideas.

00:35:45.780 --> 00:35:46.880
That's what I'm trying to do,

00:35:46.880 --> 00:35:51.880
and I view it that way
and I often begin my talks

00:35:52.580 --> 00:35:54.460
with that quote from Francis Crick about

00:35:54.460 --> 00:35:56.860
where he talks about
we're missing a framework,

00:35:56.860 --> 00:35:58.610
which is essentially
the same as principle.

00:35:58.610 --> 00:36:01.180
We're missing a framework for
understanding these ideas.

00:36:01.180 --> 00:36:04.010
And I constantly ask myself,
"How are we doing on that?"

00:36:04.010 --> 00:36:05.720
I think we're making a lot of progress,

00:36:05.720 --> 00:36:09.053
I'm very pleased with it from
where I started I'm thrilled.

00:36:10.430 --> 00:36:12.670
- Give us a rundown of some of the

00:36:12.670 --> 00:36:14.290
core principles you've come upon.

00:36:14.290 --> 00:36:19.290
We don't have visuals and our
audience is a public audience

00:36:20.310 --> 00:36:22.490
and it may be students
who would be interested.

00:36:22.490 --> 00:36:25.810
So simply stated, what are the core--

00:36:25.810 --> 00:36:29.230
- Let me go down a few of them,
some of the very basic ones.

00:36:29.230 --> 00:36:31.150
One thing that a lot of
people confuse about it

00:36:31.150 --> 00:36:33.410
is the brain is not like a computer.

00:36:33.410 --> 00:36:35.090
The brain is a memory system.

00:36:35.090 --> 00:36:36.970
It's about storing
patterns from the world,

00:36:36.970 --> 00:36:39.590
not computing, you don't compute

00:36:39.590 --> 00:36:41.810
how to move your arm to catch a ball.

00:36:41.810 --> 00:36:44.220
You actually recall sequences of patterns

00:36:44.220 --> 00:36:45.410
that have been stored previously

00:36:45.410 --> 00:36:47.330
and you're recalling them
in a sort of novel way.

00:36:47.330 --> 00:36:49.630
So it's all about memory,
it's a memory organ.

00:36:50.630 --> 00:36:51.770
And it has to be trained,

00:36:51.770 --> 00:36:53.460
and it has to be trained
from sensory data.

00:36:53.460 --> 00:36:56.060
No one goes in and drops
the memory in there,

00:36:56.060 --> 00:36:58.310
it's like you have these
senses of the world

00:36:58.310 --> 00:36:59.920
and our problem is figuring out how

00:36:59.920 --> 00:37:02.840
from the sensory stream
coming into the neocortex

00:37:02.840 --> 00:37:05.050
how it builds a model of the world.

00:37:05.050 --> 00:37:07.061
The second really big insight,

00:37:07.061 --> 00:37:10.050
which was at first understood,

00:37:10.050 --> 00:37:11.480
well a couple of people involved in this.

00:37:11.480 --> 00:37:14.630
Mountcastle, Edelman,
and Van Essen and others.

00:37:14.630 --> 00:37:18.630
Essentially, the neocortex
has its most basic structure

00:37:18.630 --> 00:37:20.340
it's a hierarchy of memory regions.

00:37:20.340 --> 00:37:23.190
And so, even though it
looks like a sheet of cells,

00:37:23.190 --> 00:37:24.740
actually these sheets
are connected together

00:37:24.740 --> 00:37:26.860
and in certain regions you can logically

00:37:26.860 --> 00:37:28.310
think of them as a hierarchy.

00:37:28.310 --> 00:37:30.450
And that's a real thing,
that's not a debatable thing.

00:37:30.450 --> 00:37:31.630
It's a physical fact.

00:37:31.630 --> 00:37:32.800
And so now we have, oh,

00:37:32.800 --> 00:37:34.020
it's a different type of memory system.

00:37:34.020 --> 00:37:35.020
It's a hierarchical memory.

00:37:35.020 --> 00:37:36.800
We don't really have
hierarchial memory systems

00:37:36.800 --> 00:37:38.252
in computers or elsewhere,
that's a new idea.

00:37:38.252 --> 00:37:41.440
And I'll tell you two more ideas.

00:37:41.440 --> 00:37:44.730
The next one is that each of
those regions in the hierarchy

00:37:44.730 --> 00:37:45.700
no matter what they're doing,

00:37:45.700 --> 00:37:49.240
vision, language, touch,
you know, motor, whatever,

00:37:49.240 --> 00:37:51.350
they're all doing the same principle.

00:37:51.350 --> 00:37:53.140
This is one of those "aha" moments

00:37:53.140 --> 00:37:54.077
this came from Mountcastle

00:37:54.077 --> 00:37:57.670
and a few other people around 1979,

00:37:57.670 --> 00:37:59.637
which was like, "Oh my goodness,

00:37:59.637 --> 00:38:01.388
"this is one of those beautiful things

00:38:01.388 --> 00:38:02.290
"we've discovered about nature."

00:38:02.290 --> 00:38:04.220
When you think you have
all these different things,

00:38:04.220 --> 00:38:05.660
but they're really one in the same.

00:38:05.660 --> 00:38:08.380
Vision is the same as audition,

00:38:08.380 --> 00:38:10.380
which is the same as somatosensory sets,

00:38:10.380 --> 00:38:11.730
it is the same thing.

00:38:11.730 --> 00:38:13.660
There's huge amounts of evidence for this,

00:38:13.660 --> 00:38:15.730
and yet some people have
trouble believing it.

00:38:15.730 --> 00:38:17.260
They just, "I can't believe it's true."

00:38:17.260 --> 00:38:18.170
It's true.

00:38:18.170 --> 00:38:21.189
And then the final thing
was an insight that I had

00:38:21.189 --> 00:38:23.950
and I'm not saying other
people didn't have it,

00:38:23.950 --> 00:38:26.100
but it was a real new thing for me.

00:38:26.100 --> 00:38:28.480
Was that what's going on in these regions

00:38:28.480 --> 00:38:30.600
in this hierarchy of cortical regions,

00:38:30.600 --> 00:38:32.790
is that it's the fundamental
memory principle,

00:38:32.790 --> 00:38:36.140
the memory property,
is memory of sequences.

00:38:36.140 --> 00:38:38.100
Of how things move
through time in the world.

00:38:38.100 --> 00:38:40.490
What follows what, what words I say next.

00:38:40.490 --> 00:38:42.960
Everything I'm doing here
is a playback of sequences.

00:38:42.960 --> 00:38:45.460
You're recognizing sequences of patterns.

00:38:45.460 --> 00:38:47.450
Audition, touch, and even vision,

00:38:47.450 --> 00:38:49.350
it's all about time-based patterns.

00:38:49.350 --> 00:38:52.280
And so, how does the
brain learn sequences?

00:38:52.280 --> 00:38:55.080
I've been spending the
last seven years on that,

00:38:55.080 --> 00:38:56.860
we've made really good progress on that.

00:38:56.860 --> 00:38:59.517
- You used the term
yesterday in the lecture,

00:38:59.517 --> 00:39:02.530
"spatially distributed representation."

00:39:02.530 --> 00:39:03.833
- [Jeff] Sparsely distributed.

00:39:04.759 --> 00:39:07.433
- Sparse, yeah, sorry, sparsely, yes.

00:39:09.960 --> 00:39:12.040
- We've known for a long time

00:39:12.040 --> 00:39:14.311
that when you look at
the cells in the brain,

00:39:14.311 --> 00:39:15.480
the brain is just made
of a bunch of cells,

00:39:15.480 --> 00:39:18.120
it's just neurons, lots of
'em, but they're just cells.

00:39:18.120 --> 00:39:20.720
When you look at the brain,
a normal, healthy brain,

00:39:20.720 --> 00:39:22.530
you find that at any moment in time,

00:39:22.530 --> 00:39:24.270
very few of those cells are active

00:39:24.270 --> 00:39:26.200
and most of them are silent.

00:39:26.200 --> 00:39:28.420
And this has been known,
there's empirical evidence,

00:39:28.420 --> 00:39:31.420
and not many people
thought too hard about it.

00:39:31.420 --> 00:39:32.277
Some people have and they say,

00:39:32.277 --> 00:39:33.560
"Well maybe there's some efficiencies

00:39:33.560 --> 00:39:36.320
and energy usage and blah blah blah."

00:39:36.320 --> 00:39:40.493
But we now have a deep
theoretical understanding,

00:39:42.211 --> 00:39:43.044
and part of this came from other people,

00:39:43.044 --> 00:39:44.820
part of it came from myself.

00:39:44.820 --> 00:39:48.660
A deep theoretical understanding
of why it's sparse.

00:39:48.660 --> 00:39:50.490
It's not just about saving energy

00:39:50.490 --> 00:39:52.120
or some sort of efficiency,

00:39:52.120 --> 00:39:55.460
there are fundamental
information properties

00:39:55.460 --> 00:39:58.560
that come out of having a
representation of the brain

00:39:58.560 --> 00:40:00.378
where you have a few things active

00:40:00.378 --> 00:40:03.190
and both things inactive and
then you change over time.

00:40:03.190 --> 00:40:04.170
So it's a temporarily point,

00:40:04.170 --> 00:40:06.960
you know all the cells become
active at some point in time,

00:40:06.960 --> 00:40:09.660
but it's very sparse, and you
don't see a lot of the cells

00:40:09.660 --> 00:40:10.800
active at the same time.

00:40:10.800 --> 00:40:13.080
And so there's these deep
information principles,

00:40:13.080 --> 00:40:16.740
which I now understand
the significance of.

00:40:16.740 --> 00:40:20.670
And they tell me now
that all intelligence,

00:40:20.670 --> 00:40:23.050
whether it's biological
or machine or human-made,

00:40:23.050 --> 00:40:24.300
it doesn't matter, all intelligence

00:40:24.300 --> 00:40:26.860
can be based on sparse representations.

00:40:26.860 --> 00:40:28.340
I can be certain of that now.

00:40:28.340 --> 00:40:31.400
And so this has become a
bit of a rallying point

00:40:31.400 --> 00:40:35.080
for me and some other
people to talk about that.

00:40:35.080 --> 00:40:36.890
- Two other points that come out

00:40:36.890 --> 00:40:38.460
in your book and in the lecture.

00:40:38.460 --> 00:40:42.840
One is that it's a hierarchical system,

00:40:42.840 --> 00:40:46.970
but the flows of information go both ways.

00:40:46.970 --> 00:40:51.610
And the upper regions are the keeper

00:40:51.610 --> 00:40:54.850
of the models or the patterns,

00:40:54.850 --> 00:40:58.750
but those can change as
there is input of data.

00:40:58.750 --> 00:41:00.950
- Well, it's not quite
like that, it's close.

00:41:00.950 --> 00:41:02.820
They're all learning
patterns in this hierarchy,

00:41:02.820 --> 00:41:07.040
and the way the system works is

00:41:07.040 --> 00:41:09.400
when I wanna take an input stream,

00:41:09.400 --> 00:41:10.930
the lower the ones that the input,

00:41:10.930 --> 00:41:13.230
parts of the neocortex
closest to the input

00:41:13.230 --> 00:41:15.925
are learning patterns of
small parts of input space.

00:41:15.925 --> 00:41:18.170
They're, like, in primary visual area

00:41:18.170 --> 00:41:21.040
it's learning sequences of
little line movements and so on.

00:41:21.040 --> 00:41:23.360
As you go up the hierarchy
it's sort of aggregating these

00:41:23.360 --> 00:41:25.660
and it builds representations over it.

00:41:25.660 --> 00:41:27.560
You can think of that as
you go up the hierarchy

00:41:27.560 --> 00:41:30.010
it's assimilating knowledge from a broader

00:41:30.010 --> 00:41:31.330
and broader part of the input space,

00:41:31.330 --> 00:41:33.060
and over more and more time.

00:41:33.060 --> 00:41:35.790
So you're ending up with
representations that are

00:41:35.790 --> 00:41:37.840
representing longer periods of time

00:41:37.840 --> 00:41:41.360
and more stable over
variations in the world.

00:41:41.360 --> 00:41:43.130
So at the bottom of the visual hierarchy

00:41:43.130 --> 00:41:44.210
I'll find little line segments,

00:41:44.210 --> 00:41:45.740
but at the top I'll
find cells that respond

00:41:45.740 --> 00:41:47.790
to faces and people and things like this.

00:41:51.252 --> 00:41:52.394
They're all doing the same thing,

00:41:52.394 --> 00:41:53.227
they're all learning sequences,

00:41:53.227 --> 00:41:55.950
but there's this sort of collapsing

00:41:55.950 --> 00:41:59.470
or a converging of these
things as you go up.

00:41:59.470 --> 00:42:01.500
And then it unfolds, too.

00:42:01.500 --> 00:42:03.550
So then think about my speech right now,

00:42:03.550 --> 00:42:06.880
I'm producing a very high velocity,

00:42:06.880 --> 00:42:09.373
meaning rapidly changing pattern.

00:42:09.373 --> 00:42:10.360
In a matter of milliseconds

00:42:10.360 --> 00:42:13.080
my muscles are going (nonsensical
noise) all over the place.

00:42:13.080 --> 00:42:15.060
I'm recalling a sequence, a pattern,

00:42:15.060 --> 00:42:16.930
that's coming out of my neocortex.

00:42:16.930 --> 00:42:18.830
My whole brain is doing this.

00:42:18.830 --> 00:42:21.670
There's parts of the low
down in the hierarchy

00:42:21.670 --> 00:42:23.700
that are playing back
these rapid sequences.

00:42:23.700 --> 00:42:24.840
Then there's parts of the hierarchy

00:42:24.840 --> 00:42:26.590
that are playing out, like,
"You need to say this word."

00:42:26.590 --> 00:42:27.780
And then there's, "You
need to say this phrase."

00:42:27.780 --> 00:42:30.700
Or, "Jeff, talk about sparse
distributed representations."

00:42:30.700 --> 00:42:32.150
So like in a high level concept,

00:42:32.150 --> 00:42:33.740
at the higher part of the hierarchy

00:42:33.740 --> 00:42:35.293
would send unfolding
sequences, unfolding sequences,

00:42:35.293 --> 00:42:39.160
produces this torrent of patterns

00:42:39.160 --> 00:42:41.460
coming out of my musculature.

00:42:41.460 --> 00:42:44.430
- You also talk about the
brain and its placidity.

00:42:44.430 --> 00:42:49.430
So it can adapt, creating
new bundles of information.

00:42:51.330 --> 00:42:53.130
- Yeah, well, it's a
learning system, right?

00:42:53.130 --> 00:42:57.320
So, it has to adapt and
has to take in patterns.

00:42:57.320 --> 00:42:58.967
As you're coming in it
has to constantly say,

00:42:58.967 --> 00:43:01.410
"I'm trying to learn
something new all the time.

00:43:01.410 --> 00:43:03.280
Think about the brain doesn't
get to store it someplace

00:43:03.280 --> 00:43:05.140
and decide later if it wants to learn it.

00:43:05.140 --> 00:43:06.560
It says, "It's here now,

00:43:06.560 --> 00:43:08.300
I gotta learn what's
going on in the world."

00:43:08.300 --> 00:43:11.770
And so, the word plasticity
refers to a couple of things.

00:43:11.770 --> 00:43:14.047
First, the fact that
it's a learning system

00:43:14.047 --> 00:43:16.450
and new connections are
being formed all the time.

00:43:16.450 --> 00:43:18.540
And it also refers in the brain that

00:43:18.540 --> 00:43:20.560
the brain can slowly rewire itself.

00:43:20.560 --> 00:43:21.660
Not just from new connections,

00:43:21.660 --> 00:43:24.290
but it can slowly rewire itself over time

00:43:24.290 --> 00:43:27.696
so that it can form entirely
new sort of architectures.

00:43:27.696 --> 00:43:31.130
And this is very well documented
in many types of studies.

00:43:31.130 --> 00:43:33.457
So we have this system
which essentially says,

00:43:33.457 --> 00:43:35.377
"Look, I'm constantly learning.

00:43:35.377 --> 00:43:36.457
"I'm constantly trying to discover

00:43:36.457 --> 00:43:37.627
"the patterns in the world.

00:43:37.627 --> 00:43:41.040
"I'm constantly adapting
it's very fluid process."

00:43:41.040 --> 00:43:43.773
But it's working on the same
principles all the time.

00:43:43.773 --> 00:43:45.880
- An important point that you make,

00:43:45.880 --> 00:43:49.530
and again I'm going through
some of the high points here

00:43:49.530 --> 00:43:52.240
to interest our audience so
they'll go read your book

00:43:52.240 --> 00:43:53.860
and follow your research.

00:43:53.860 --> 00:43:57.608
But you're suggesting
that in the upper regions,

00:43:57.608 --> 00:44:02.608
conclusions can be reached
with only part of the pattern.

00:44:04.160 --> 00:44:05.679
Talk a little about that.

00:44:05.679 --> 00:44:06.700
- Yeah, that's true.

00:44:06.700 --> 00:44:09.030
Quite a few scientists
understand this issue

00:44:09.030 --> 00:44:09.890
and have written about it

00:44:09.890 --> 00:44:12.550
more eloquently than I have, actually.

00:44:12.550 --> 00:44:15.590
But your perception of the world, like,

00:44:15.590 --> 00:44:16.580
if you're watching this video.

00:44:16.580 --> 00:44:18.980
Me and you sitting here
having this conversation

00:44:19.840 --> 00:44:24.840
is actually largely a product
of the memory in your brain.

00:44:25.560 --> 00:44:26.930
It's largely a product of the model

00:44:26.930 --> 00:44:27.930
you've created of the world.

00:44:27.930 --> 00:44:29.890
So you have a sensory
stream that's just coming in

00:44:29.890 --> 00:44:31.490
and you can't really see that.

00:44:31.490 --> 00:44:34.740
You don't really, you're not
able to perceive that actually.

00:44:34.740 --> 00:44:36.470
We don't have an ability
to actually look at it

00:44:36.470 --> 00:44:38.890
but we know what it looks
like from scientific studies.

00:44:38.890 --> 00:44:41.373
It's a very impoverished
sense of the world.

00:44:42.620 --> 00:44:44.580
It's like my eyes are
darting all over the place

00:44:44.580 --> 00:44:45.690
looking at little pieces here and there.

00:44:45.690 --> 00:44:47.740
But I have a perception
of you being stable.

00:44:47.740 --> 00:44:49.670
Your whole body is here, but
if you actually could see

00:44:49.670 --> 00:44:51.480
what's coming in to my optic nerve

00:44:51.480 --> 00:44:53.330
it's like this highly
distorted little thing

00:44:53.330 --> 00:44:55.250
going (nonsensical noise)
all over the place.

00:44:55.250 --> 00:44:58.570
It's amazing that you seem
stable in front of me.

00:44:58.570 --> 00:45:02.260
So the reason it is, is
that throughout childhood

00:45:02.260 --> 00:45:03.750
and our early years of development,

00:45:03.750 --> 00:45:05.720
we built this model of how the world is.

00:45:05.720 --> 00:45:08.670
And the sensory data invokes the model.

00:45:08.670 --> 00:45:12.600
So my perception of you as a whole human

00:45:13.730 --> 00:45:15.650
is largely the fact of my model.

00:45:15.650 --> 00:45:17.910
I actually am getting little
pieces of data about you,

00:45:17.910 --> 00:45:19.600
which confirm my model all the time.

00:45:19.600 --> 00:45:21.730
Yes, he still looks like a
person, he's got two hands,

00:45:21.730 --> 00:45:23.040
and a head, and feet, and so on.

00:45:23.040 --> 00:45:24.830
You're sitting in a chair.

00:45:24.830 --> 00:45:27.020
And so my perception,

00:45:27.020 --> 00:45:29.870
the highest levels of the
cortex are saying, "Yes."

00:45:29.870 --> 00:45:30.703
You know?

00:45:30.703 --> 00:45:32.220
"There you are."

00:45:32.220 --> 00:45:36.750
And in reality what's invoking that model

00:45:36.750 --> 00:45:39.907
is this sort of like
crazy changing pattern

00:45:39.907 --> 00:45:40.770
and (nonsensical noise)
all over the place,

00:45:40.770 --> 00:45:42.554
which is like invoking the model.

00:45:42.554 --> 00:45:43.590
And the model is
constantly testing itself,

00:45:43.590 --> 00:45:45.631
saying, "Okay, well, if it is a human then

00:45:45.631 --> 00:45:47.257
"his tie oughta have a knot

00:45:47.257 --> 00:45:49.972
"and his glasses oughta
have things over his ears."

00:45:49.972 --> 00:45:52.889
I'm not conscious of this,
I'm not thinking about it,

00:45:52.889 --> 00:45:55.770
but the brain is doing this all the time.

00:45:56.610 --> 00:45:58.450
- The bottom line here is that

00:45:58.450 --> 00:46:00.400
when you take all this together this is

00:46:01.410 --> 00:46:03.620
a computational system.

00:46:03.620 --> 00:46:05.502
Is that the way I wanna say it?

00:46:05.502 --> 00:46:06.800
- I try to avoid to use computation,

00:46:06.800 --> 00:46:08.670
because people think it's like a computer.

00:46:08.670 --> 00:46:09.503
- [Harry] Yeah, I see.

00:46:09.503 --> 00:46:12.150
- In the history, in my talk
today I'll talk about this,

00:46:12.150 --> 00:46:15.280
the history of people thinking about

00:46:15.280 --> 00:46:17.320
building intelligent machines
or what intelligence is,

00:46:17.320 --> 00:46:19.280
is largely why people think of it

00:46:19.280 --> 00:46:21.080
as a computational problem.

00:46:21.080 --> 00:46:23.970
And they say, "Oh, I'll
apply algorithms to this."

00:46:23.970 --> 00:46:25.510
But in reality it's not.

00:46:25.510 --> 00:46:26.700
It's a memory problem.

00:46:26.700 --> 00:46:29.710
Now the memory system is
a complex memory system,

00:46:29.710 --> 00:46:31.890
hierarchy, sparse
distribution representations,

00:46:31.890 --> 00:46:33.240
sequences, and so on.

00:46:33.240 --> 00:46:34.877
It's not simple, it's not
like, "Oh, just put it

00:46:34.877 --> 00:46:36.810
"down on a piece of tape."

00:46:36.810 --> 00:46:39.100
Or, "Put it on a desk
or something like that."

00:46:39.100 --> 00:46:40.280
It's a complex memory system

00:46:40.280 --> 00:46:42.630
and the memory system
itself has properties

00:46:42.630 --> 00:46:45.070
that you could call,
if you wanna call them

00:46:45.070 --> 00:46:46.100
computational properties,

00:46:46.100 --> 00:46:47.480
like the way the memory system works

00:46:47.480 --> 00:46:49.640
it has certain rules and so on.

00:46:49.640 --> 00:46:51.403
Fundamentally it's a memory system.

00:46:53.980 --> 00:46:55.640
If you try to figure out how brains work

00:46:55.640 --> 00:46:57.670
or what intelligence is or
build intelligent machines,

00:46:57.670 --> 00:46:59.217
by saying, "Okay, it's
a bunch of algorithms.

00:46:59.217 --> 00:47:00.570
"We're gonna program these algorithms."

00:47:00.570 --> 00:47:02.490
You're just not gonna succeed.

00:47:02.490 --> 00:47:04.980
- Now where do the emotions come in?

00:47:04.980 --> 00:47:07.840
That's not the part of the system that is

00:47:07.840 --> 00:47:10.410
the brain complex that you're looking at.

00:47:10.410 --> 00:47:11.704
- Well exactly, so I've said, you know

00:47:11.704 --> 00:47:14.390
the neocortex is about 60 percent

00:47:14.390 --> 00:47:15.420
of the volume of a human brain.

00:47:15.420 --> 00:47:17.785
There's a lot of other
important stuff to be a human.

00:47:17.785 --> 00:47:21.423
And there are several
small areas of the brain

00:47:21.423 --> 00:47:23.880
which really dictate emotions.

00:47:23.880 --> 00:47:25.750
One famous one is the amygdala,

00:47:25.750 --> 00:47:27.620
people may have heard about that.

00:47:27.620 --> 00:47:29.639
It's a little almond-shaped type of thing.

00:47:29.639 --> 00:47:31.630
It's not part of the neocortex,

00:47:31.630 --> 00:47:33.653
it's an old structure, so emotions

00:47:33.653 --> 00:47:37.830
are much older on evolutionary time.

00:47:37.830 --> 00:47:40.360
Animals that don't have a
neocortex have emotions.

00:47:40.360 --> 00:47:42.590
A lizard can be or an
alligator can be angry

00:47:42.590 --> 00:47:45.490
or fight and flee and wanna
have sex and things like that.

00:47:48.300 --> 00:47:49.920
And there's a relationship between those

00:47:49.920 --> 00:47:52.040
emotional centers in the neocortex.

00:47:52.040 --> 00:47:53.450
While I didn't talk about it yesterday

00:47:53.450 --> 00:47:55.130
and it's not in most of our models,

00:47:55.130 --> 00:47:57.970
it's important if you
wanted to make a human.

00:47:57.970 --> 00:48:00.070
If you wanna just capture
the essence of intelligence,

00:48:00.070 --> 00:48:02.482
I argue you don't need to model that.

00:48:02.482 --> 00:48:06.260
So, for example, in a human
one thing that'll happen

00:48:06.260 --> 00:48:08.610
if you have a very
emotionally salient event,

00:48:08.610 --> 00:48:10.700
like something happened to
you that was very dangerous

00:48:10.700 --> 00:48:11.773
and you almost died.

00:48:13.051 --> 00:48:14.487
Your emotion centers
of the brain will say,

00:48:14.487 --> 00:48:15.320
"Bing, bing, bing!"

00:48:15.320 --> 00:48:17.567
They'll light up and
they'll say, "Remember this.

00:48:17.567 --> 00:48:18.747
"This is the most important thing,

00:48:18.747 --> 00:48:21.950
"if you ever see this again
you'll never forget this."

00:48:21.950 --> 00:48:23.900
You know if you're ever in
a situation like that again

00:48:23.900 --> 00:48:25.350
you're just gonna be fearful.

00:48:26.220 --> 00:48:29.837
It tells the neocortex,
"Never forget this.

00:48:29.837 --> 00:48:31.270
"Make this a permanent memory."

00:48:31.270 --> 00:48:33.290
The neocortex itself is not evaluating

00:48:33.290 --> 00:48:35.177
the emotional aspects, it's not saying,

00:48:35.177 --> 00:48:36.560
"I know it's emotionally salient."

00:48:36.560 --> 00:48:38.470
So that's the little
thing like the amygdala.

00:48:38.470 --> 00:48:40.257
But it's telling the neocortex,

00:48:40.257 --> 00:48:41.527
"You should recognize this scenario

00:48:41.527 --> 00:48:43.162
"and tell me when you see it again."

00:48:43.162 --> 00:48:46.350
If I wanted to build intelligent machines

00:48:46.350 --> 00:48:50.001
that were very human-like, which I don't.

00:48:50.001 --> 00:48:51.590
If that's what you wanted to do,

00:48:51.590 --> 00:48:53.760
then you'd have to build
emotional capabilities

00:48:53.760 --> 00:48:54.950
plus a lot of other stuff.

00:48:54.950 --> 00:48:56.490
We'd have to build the whole brain.

00:48:56.490 --> 00:48:58.200
If you're just trying to
build intelligent machines

00:48:58.200 --> 00:49:00.023
that are useful tools for humanity,

00:49:01.010 --> 00:49:02.610
then you don't have to necessarily

00:49:02.610 --> 00:49:03.960
model all that, or maybe we'll do it

00:49:03.960 --> 00:49:05.670
in a slightly different way.

00:49:05.670 --> 00:49:10.250
- Your long-term plan was to
apply what you learn here,

00:49:10.250 --> 00:49:15.250
which we've discussed in
a brief presentation here.

00:49:16.960 --> 00:49:19.890
What are you trying to do
with your new company now?

00:49:19.890 --> 00:49:21.640
- The new company is, it's kind of a,

00:49:21.640 --> 00:49:23.368
it's part of the process of,

00:49:23.368 --> 00:49:24.250
you know when I talked to
you earlier about, like,

00:49:24.250 --> 00:49:26.290
okay, I have this big long-term goal.

00:49:26.290 --> 00:49:27.990
I gotta figure out how
the neocortex works,

00:49:27.990 --> 00:49:29.750
and build machines that work like that,

00:49:29.750 --> 00:49:32.420
and promote these theories to
create a paradigm for this.

00:49:32.420 --> 00:49:34.240
And the company is one aspect of that,

00:49:34.240 --> 00:49:36.250
in the same way like I started
the Neuroscience Institute,

00:49:36.250 --> 00:49:37.300
or the same reason why I wrote a book.

00:49:37.300 --> 00:49:39.210
I wrote a book, not because
I wanted to be an author,

00:49:39.210 --> 00:49:41.190
because I felt it was a tool to help me

00:49:41.190 --> 00:49:43.350
towards the end goal.

00:49:43.350 --> 00:49:47.117
And my company Numenta is in some sense,

00:49:47.117 --> 00:49:48.860
and for me it plays another role.

00:49:48.860 --> 00:49:50.680
It's saying, okay, if we figured out how

00:49:50.680 --> 00:49:53.270
part of the neocortex
works, and I think we have,

00:49:53.270 --> 00:49:54.320
a good portion of it.

00:49:55.170 --> 00:49:56.860
How do I get people excited about this?

00:49:56.860 --> 00:49:58.350
How do I get an interest in this?

00:49:58.350 --> 00:50:00.310
Well I can publish scientific papers,

00:50:00.310 --> 00:50:03.130
maybe I can write another
book, which I probably will do.

00:50:03.130 --> 00:50:05.040
But there's a whole 'nother
world of people out there

00:50:05.040 --> 00:50:06.640
I wanna get excited about this.

00:50:06.640 --> 00:50:08.890
These are entrepreneurs, scientists,

00:50:08.890 --> 00:50:10.970
engineers, and computer scientists.

00:50:10.970 --> 00:50:12.840
So by creating a company,

00:50:12.840 --> 00:50:15.270
if you can create a successful product,

00:50:15.270 --> 00:50:16.750
that gets a lot of people's attention.

00:50:16.750 --> 00:50:18.350
It's a real wake up call to people,

00:50:18.350 --> 00:50:19.183
like wow, there's someone that's

00:50:19.183 --> 00:50:20.410
making some money doing this over here.

00:50:20.410 --> 00:50:23.183
As crass as it sounds,
that's the way it worked.

00:50:24.700 --> 00:50:26.453
They'll say, "How did they do that?

00:50:28.057 --> 00:50:29.347
"How did they make that work?

00:50:29.347 --> 00:50:30.477
"That's pretty cool, you know.

00:50:30.477 --> 00:50:31.350
"What's going on there?"

00:50:31.350 --> 00:50:32.220
And I say, "Here it is."

00:50:32.220 --> 00:50:33.330
I explained it, I've written it up,

00:50:33.330 --> 00:50:34.500
you can read it about it today.

00:50:34.500 --> 00:50:35.770
You can read a white paper on a website

00:50:35.770 --> 00:50:37.300
which explains how all of this works.

00:50:37.300 --> 00:50:39.017
And people will study that and they'll go,

00:50:39.017 --> 00:50:41.390
"That's pretty cool, I
wanna understand that."

00:50:41.390 --> 00:50:43.560
So it's a way of
proselytizing, if you will,

00:50:43.560 --> 00:50:45.290
to a different audience.

00:50:45.290 --> 00:50:48.094
Obviously a company has
to be successful to be,

00:50:48.094 --> 00:50:49.230
you know it has to make
money to be successful.

00:50:49.230 --> 00:50:52.070
It has a dual mission as
to be a successful company,

00:50:52.070 --> 00:50:53.880
but the reason I started it

00:50:53.880 --> 00:50:57.530
was as one more vehicle for promoting

00:50:57.530 --> 00:50:59.707
concepts or a way of
thinking about the brain,

00:50:59.707 --> 00:51:01.680
and it's importance for doing that.

00:51:01.680 --> 00:51:06.020
- In the long-term possibilities here are

00:51:06.020 --> 00:51:11.020
streams of data on weather,
demography, all sorts of things

00:51:11.080 --> 00:51:16.080
that could make intelligent decisions

00:51:16.330 --> 00:51:17.530
about the meaning of the data.

00:51:17.530 --> 00:51:20.170
- Yeah, so if we're on a path to

00:51:20.170 --> 00:51:21.760
building truly intelligent machines,

00:51:21.760 --> 00:51:23.230
what can we do today?

00:51:23.230 --> 00:51:25.960
What's really interesting
that I can do today?

00:51:25.960 --> 00:51:28.590
We've identified an
opportunity at Numenta,

00:51:28.590 --> 00:51:30.880
which I think is really exciting.

00:51:30.880 --> 00:51:32.443
The world is full of data.

00:51:34.110 --> 00:51:36.267
People hear about this,
these terms big data,

00:51:36.267 --> 00:51:38.270
awash in data, the
number of amount of bytes

00:51:38.270 --> 00:51:39.470
you've been storing every year is bigger

00:51:39.470 --> 00:51:42.400
than the Library of Congress, it's crazy.

00:51:42.400 --> 00:51:44.190
And people don't know what to do with it.

00:51:44.190 --> 00:51:45.280
Well, think about that.

00:51:45.280 --> 00:51:48.170
Streams of data coming from
sensors or from computers,

00:51:48.170 --> 00:51:50.533
from cars, from streets, from everything.

00:51:51.415 --> 00:51:54.630
That's just like the data
coming into the brain

00:51:54.630 --> 00:51:57.410
from sensors, it's just a
monstrous stream of data.

00:51:57.410 --> 00:51:59.930
So what we can do is we
can apply brain modeling

00:51:59.930 --> 00:52:02.470
to these streams of data to
model the data in the world

00:52:02.470 --> 00:52:04.923
and act on it immediately,
just like humans do.

00:52:06.338 --> 00:52:07.880
So instead of like storing
the data someplace,

00:52:07.880 --> 00:52:09.990
we stream it into product we call GRAU,

00:52:09.990 --> 00:52:11.790
which is essentially a corticomodel.

00:52:12.689 --> 00:52:14.130
The model builds a model of the world

00:52:14.130 --> 00:52:15.920
using a neocortical algorithm,

00:52:15.920 --> 00:52:17.600
and makes predictions about future events

00:52:17.600 --> 00:52:19.190
and we can act on those events.

00:52:19.190 --> 00:52:21.620
So I'll give you just one example.

00:52:21.620 --> 00:52:24.740
Turns out that when we consume electricity

00:52:24.740 --> 00:52:26.410
there's a lot going on behind the scenes,

00:52:26.410 --> 00:52:28.090
most people aren't aware of.

00:52:28.090 --> 00:52:30.396
Large consumers of electricity can

00:52:30.396 --> 00:52:34.394
buy their electricity at different
rates throughout the day.

00:52:34.394 --> 00:52:37.487
They can make future contracts, and say,

00:52:37.487 --> 00:52:39.500
"I'll pay this amount
for this amount of energy

00:52:39.500 --> 00:52:40.370
six hours from now."

00:52:40.370 --> 00:52:42.610
And there's a thing
called the man response,

00:52:42.610 --> 00:52:45.640
which is going on everywhere,
you're not aware of this.

00:52:45.640 --> 00:52:47.300
If I could predict better

00:52:47.300 --> 00:52:49.990
the energy consumption in buildings,

00:52:49.990 --> 00:52:53.060
I can be more efficient
and I can help everyone,

00:52:53.060 --> 00:52:54.540
it makes a more efficient marketplace.

00:52:54.540 --> 00:52:58.244
And we can start doing
this on a more rapid basis.

00:52:58.244 --> 00:53:00.890
This is one of many
areas we're looking at,

00:53:00.890 --> 00:53:03.550
and what we can do is we
can do a better job of this

00:53:03.550 --> 00:53:05.560
predicting, like, okay, how much energy

00:53:05.560 --> 00:53:08.920
is Berkeley campus gonna
use six hours from now?

00:53:08.920 --> 00:53:10.050
How much should they be paying for that?

00:53:10.050 --> 00:53:11.830
Or should they do some things now,

00:53:11.830 --> 00:53:13.680
like pre-cool some buildings,

00:53:13.680 --> 00:53:16.260
which'll be cheaper than
cooling them in useless

00:53:16.260 --> 00:53:17.810
energy than cooling them later.

00:53:18.660 --> 00:53:20.600
When you start automating this,

00:53:20.600 --> 00:53:23.670
then you can do this on a
building-by-building basis,

00:53:23.670 --> 00:53:25.250
you can do it by a four-by-four basis,

00:53:25.250 --> 00:53:27.120
you could it by a room-by-room basis.

00:53:27.120 --> 00:53:29.640
It gives you the sense of
how technology progresses.

00:53:29.640 --> 00:53:32.680
So that's one example,
but the general idea is

00:53:32.680 --> 00:53:34.670
with this world awash of data,

00:53:34.670 --> 00:53:36.220
how do we take advantage of it?

00:53:36.220 --> 00:53:39.680
How do we build smarter
cars and smarter buildings

00:53:39.680 --> 00:53:41.190
and smarter power grids?

00:53:41.190 --> 00:53:43.130
And we can use brain
technology to do that.

00:53:43.130 --> 00:53:44.350
So that's pretty exciting.

00:53:44.350 --> 00:53:46.400
It's a long way from building like,

00:53:46.400 --> 00:53:51.220
fully brilliant robots or, you know,

00:53:51.220 --> 00:53:52.500
super intelligent machines.

00:53:52.500 --> 00:53:53.827
But it's on the path,

00:53:53.827 --> 00:53:56.920
it's not a digression,

00:53:56.920 --> 00:53:59.940
it's on a continuum toward that path.

00:53:59.940 --> 00:54:04.750
- You're uniquely placed
to answer this question,

00:54:04.750 --> 00:54:06.340
reflecting on your own career,

00:54:06.340 --> 00:54:07.880
but also the research you've done.

00:54:07.880 --> 00:54:10.660
So, what is creativity?

00:54:10.660 --> 00:54:12.500
- I have a very clear idea of that.

00:54:12.500 --> 00:54:14.600
Right or wrong, I have a clear idea of it.

00:54:15.880 --> 00:54:17.880
The first thing you have to dispel

00:54:17.880 --> 00:54:21.080
this notion that creativity
is something special.

00:54:21.080 --> 00:54:22.290
You're actually exhibiting it

00:54:22.290 --> 00:54:24.500
every moment of your waking life.

00:54:24.500 --> 00:54:27.480
You are never in the exact
same situation twice.

00:54:27.480 --> 00:54:30.650
If I look at the patterns
coming on your nerve fibers,

00:54:30.650 --> 00:54:32.320
from your eyes, your ears, and your skin,

00:54:32.320 --> 00:54:34.610
it never repeats ever in your entire life.

00:54:34.610 --> 00:54:36.840
You may think you've been sitting here

00:54:36.840 --> 00:54:37.673
talking to me the whole time,

00:54:37.673 --> 00:54:40.711
but if you actually looked at
the patterns on your vibes,

00:54:40.711 --> 00:54:41.930
I'm moving a little bit,
you're changing a little bit.

00:54:41.930 --> 00:54:44.121
It's constantly changing.

00:54:44.121 --> 00:54:46.030
The miraculous thing is we

00:54:46.030 --> 00:54:48.330
perceive the world as not changing.

00:54:48.330 --> 00:54:49.280
We perceive it as something like,

00:54:49.280 --> 00:54:50.290
oh, I know who you are,

00:54:50.290 --> 00:54:51.920
even though I've never met you before.

00:54:51.920 --> 00:54:55.610
I can see you, and you're still here.

00:54:55.610 --> 00:54:58.320
Actually what we're doing all
the time at a very low basis,

00:54:58.320 --> 00:55:00.507
we're saying, "Here's a
new pattern coming in,

00:55:00.507 --> 00:55:03.667
"and I'm making predictions
based on my previous model

00:55:03.667 --> 00:55:06.350
"of the world of what
you should behave like."

00:55:06.350 --> 00:55:08.530
Creativity is that process.

00:55:08.530 --> 00:55:10.810
We tend to think of it
at a much higher level.

00:55:10.810 --> 00:55:12.340
So I'm looking at some patterns coming in

00:55:12.340 --> 00:55:15.800
from some sensory astronomical data.

00:55:15.800 --> 00:55:18.230
And I'm trying to figure
out what's going on here?

00:55:18.230 --> 00:55:19.640
What could explain these shifts

00:55:19.640 --> 00:55:20.697
in the frequencies I'm working on?

00:55:20.697 --> 00:55:21.640
And so on.

00:55:21.640 --> 00:55:24.870
It's a novel pattern, and what
we do when we're creative,

00:55:24.870 --> 00:55:27.270
we make analogies to previous experience.

00:55:27.270 --> 00:55:29.740
It may be a very subtle analogy,

00:55:29.740 --> 00:55:30.930
it may not even be obvious to you,

00:55:30.930 --> 00:55:33.210
but you say, "Oh, that's a
little like a pattern I saw

00:55:33.210 --> 00:55:36.444
three years ago in this
other paper about genetics."

00:55:36.444 --> 00:55:39.327
And you say, "Well, what
they learned in the genetics

00:55:39.327 --> 00:55:40.957
"is that this is
typically what's going on,

00:55:40.957 --> 00:55:43.460
"so maybe that process will apply here."

00:55:43.460 --> 00:55:44.910
And if you look at them,

00:55:44.910 --> 00:55:47.940
take, for example, the
scientific creativity process,

00:55:47.940 --> 00:55:50.362
it's very obvious that's what people do.

00:55:50.362 --> 00:55:53.210
We form knowledge about the world

00:55:53.210 --> 00:55:54.767
and then we go one step further and say,

00:55:54.767 --> 00:55:56.527
"Here's new pattern, I
don't know really understand

00:55:56.527 --> 00:55:57.927
"what's going on in this new pattern."

00:55:57.927 --> 00:56:00.220
And the brain says, "Well,
it's kinda like this one."

00:56:00.220 --> 00:56:01.510
In my talk yesterday I talked about

00:56:01.510 --> 00:56:03.150
the specific mechanisms for this.

00:56:03.150 --> 00:56:05.410
I think I understand the
specific mechanisms for this.

00:56:05.410 --> 00:56:07.090
So creativity is on a spectrum

00:56:07.090 --> 00:56:09.670
from the everyday where
you're not even aware of it,

00:56:09.670 --> 00:56:11.377
where I'm being creative
by recognizing you.

00:56:11.377 --> 00:56:13.120
You wouldn't think this, but
my brain's being creative

00:56:13.120 --> 00:56:14.877
by saying, "Oh, you know, there's Harry.

00:56:14.877 --> 00:56:17.083
"I know what he's gonna do and
I think he's got these things

00:56:17.083 --> 00:56:20.360
"and I know what's on your
papers, it's probably questions."

00:56:20.360 --> 00:56:22.260
On thoughts, even though I can't see it.

00:56:22.260 --> 00:56:23.490
So I'm being creative all the time,

00:56:23.490 --> 00:56:26.010
I'm making predictions about
things I don't really know.

00:56:26.010 --> 00:56:28.760
But when we bring it up to the
high level of the neocortex,

00:56:28.760 --> 00:56:30.100
these very difficult patterns

00:56:30.100 --> 00:56:31.670
which extend over long periods of time

00:56:31.670 --> 00:56:33.040
or in very distant places,

00:56:33.040 --> 00:56:35.350
then we're doing the same process.

00:56:35.350 --> 00:56:37.170
Now, something like artistic creativity

00:56:37.170 --> 00:56:39.081
is a little harder to understand.

00:56:39.081 --> 00:56:41.280
But I think it's basically the same idea.

00:56:41.280 --> 00:56:43.565
You're saying, even an artist will say,

00:56:43.565 --> 00:56:47.007
"You know, this is the way we typically

00:56:47.007 --> 00:56:49.467
"represent something in the art world,

00:56:49.467 --> 00:56:51.927
"but I have another view from this

00:56:51.927 --> 00:56:53.597
"part of my literature I've read,

00:56:53.597 --> 00:56:55.697
"which is something and
I'm gonna combine the two

00:56:55.697 --> 00:56:57.360
"and I'll create something new."

00:56:57.360 --> 00:57:00.900
- One final question, how
would you advise students

00:57:00.900 --> 00:57:02.868
to prepare for the future?

00:57:02.868 --> 00:57:05.306
- I get this question a lot,

00:57:05.306 --> 00:57:07.270
and actually I get students
asking me this a lot.

00:57:07.270 --> 00:57:10.070
I get probably several emails
a week where someone says,

00:57:11.280 --> 00:57:12.787
they'll say, "You've inspired me.

00:57:12.787 --> 00:57:15.127
"What can I do to do these things?"

00:57:16.000 --> 00:57:17.970
It's a very difficult question to answer,

00:57:17.970 --> 00:57:20.010
'cause it's hard to give
very specific answers.

00:57:20.010 --> 00:57:21.357
I can't say, "Well, go to this university

00:57:21.357 --> 00:57:22.437
"and study here."

00:57:23.530 --> 00:57:27.520
The answers I generally, the
general answer to this question

00:57:27.520 --> 00:57:29.120
and for some it's unsatisfactory,

00:57:29.120 --> 00:57:30.447
but it's the best I can do,

00:57:30.447 --> 00:57:32.510
and I said this earlier.

00:57:32.510 --> 00:57:35.573
You need to find something
you're passionate about,

00:57:36.590 --> 00:57:38.987
that you really care about, that you say,

00:57:38.987 --> 00:57:40.530
"You know, this is
really important to me."

00:57:40.530 --> 00:57:43.170
I don't care what it
is, you figure that out.

00:57:43.170 --> 00:57:47.260
And then be persistent and go
do whatever you have to do.

00:57:47.260 --> 00:57:48.480
Because I'll tell you what'll happen,

00:57:48.480 --> 00:57:49.670
you're gonna run into roadblock

00:57:49.670 --> 00:57:51.790
after roadblock after roadblock.

00:57:51.790 --> 00:57:54.900
And success in life is basically,

00:57:54.900 --> 00:57:57.470
success in life, success in
business, and success in science

00:57:57.470 --> 00:58:01.500
is essentially overcoming
roadblocks, one after another.

00:58:01.500 --> 00:58:03.340
Solving problems, one after another.

00:58:03.340 --> 00:58:06.550
And the only way to do that
is to have this bigger vision,

00:58:06.550 --> 00:58:08.377
to say, "You know, I know
I have to put up with this

00:58:08.377 --> 00:58:10.767
"right now because I'm going over there.

00:58:10.767 --> 00:58:12.703
"And this is the way to get there so

00:58:12.703 --> 00:58:15.000
"literally it looks hard right now."

00:58:15.000 --> 00:58:17.490
That's like the biggest
uber advice I could give.

00:58:17.490 --> 00:58:19.490
And I think if you talk to a lot of people

00:58:19.490 --> 00:58:23.440
who have some level of
success in their life,

00:58:23.440 --> 00:58:26.970
or accomplishment, they very
often say the same thing.

00:58:26.970 --> 00:58:29.947
They'll say, "You know, I
knew what I wanted to do,

00:58:29.947 --> 00:58:33.720
"and I just had to really be
persistent to getting there."

00:58:33.720 --> 00:58:36.230
And that's a good life lesson I think.

00:58:36.230 --> 00:58:38.860
- Jeff, on that note, thank you very much

00:58:38.860 --> 00:58:39.900
for coming on our program.

00:58:39.900 --> 00:58:42.320
It was a fascinating intellectual journey.

00:58:42.320 --> 00:58:43.153
- I thank you.

00:58:43.153 --> 00:58:44.600
It was great questions,
I enjoyed it very much.

00:58:44.600 --> 00:58:46.540
- And thank you very much for joining us

00:58:46.540 --> 00:58:49.461
for this Conversation with History.

00:58:49.461 --> 00:58:52.044
(techno music)

