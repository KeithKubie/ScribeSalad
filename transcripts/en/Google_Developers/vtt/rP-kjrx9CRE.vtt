WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:08.640
GUIDO VAN ROSSUM: Good
morning, everyone.

00:00:08.640 --> 00:00:11.010
JUSTIN HAUGH: Hi guys,
so I'm Justin Haugh.

00:00:11.010 --> 00:00:12.350
GUIDO VAN ROSSUM: I'm
Guido van Rossum.

00:00:12.350 --> 00:00:14.040
JUSTIN HAUGH: And we're here
to talk to you guys about

00:00:14.040 --> 00:00:15.290
scaling AppEngine
applications.

00:00:18.490 --> 00:00:20.310
There's the right button.

00:00:20.310 --> 00:00:22.880
So I'm a software engineer on
the AppEngine team, I work on

00:00:22.880 --> 00:00:24.210
system infrastructure.

00:00:24.210 --> 00:00:26.090
GUIDO VAN ROSSUM: I'm also a
software engineer on the

00:00:26.090 --> 00:00:26.690
AppEngine team.

00:00:26.690 --> 00:00:29.880
I work on tools and run times
and everything Python related.

00:00:29.880 --> 00:00:32.549
And here's a bunch of links that
you probably won't have

00:00:32.549 --> 00:00:35.350
time to type in to
your computer.

00:00:35.350 --> 00:00:37.550
JUSTIN HAUGH: But yeah, there's
a feedback link if

00:00:37.550 --> 00:00:40.335
you'd like to give us some
feedback throughout the talk.

00:00:40.335 --> 00:00:42.506
GUIDO VAN ROSSUM: And it's
an AppEngine app.

00:00:42.506 --> 00:00:45.710
JUSTIN HAUGH: Which
means it scales.

00:00:45.710 --> 00:00:47.930
OK so here's the agenda
for today.

00:00:47.930 --> 00:00:50.620
We're going to be talking about
scaling on AppEngine.

00:00:50.620 --> 00:00:54.220
I'll start off by telling you
a little bit about the

00:00:54.220 --> 00:00:57.330
AppEngine platform,
what is scaling?

00:00:57.330 --> 00:00:59.400
Is scaling actually
a hard problem?

00:00:59.400 --> 00:01:02.480
I'm guessing because of this
audience that it probably is.

00:01:02.480 --> 00:01:04.370
And then talk about the
scaling formula

00:01:04.370 --> 00:01:05.950
that AppEngine uses.

00:01:05.950 --> 00:01:09.740
And then Guido will be talking
about how to build a scalable

00:01:09.740 --> 00:01:13.160
application on AppEngine, I
going into some tools advice,

00:01:13.160 --> 00:01:19.040
some pitfalls to avoid and then
we'll wrap up with Q&amp;A.

00:01:19.040 --> 00:01:19.320
so

00:01:19.320 --> 00:01:21.390
So why do we care
about scaling?

00:01:21.390 --> 00:01:24.420
I think it's interesting to go
back and look at the first

00:01:24.420 --> 00:01:27.410
AppEngine blog post
back in 2008.

00:01:27.410 --> 00:01:30.200
"The goal is to make it easy to
scale and get started with

00:01:30.200 --> 00:01:33.580
the new web app, and then make
it easy to scale when that app

00:01:33.580 --> 00:01:35.630
reaches the point where it's
receiving significant traffic

00:01:35.630 --> 00:01:37.830
and has millions of users."

00:01:37.830 --> 00:01:39.850
And that's still
our goal today.

00:01:39.850 --> 00:01:43.470
We want AppEngine to be a really
high performance and

00:01:43.470 --> 00:01:47.210
scalable web framework
for you guys to use.

00:01:47.210 --> 00:01:48.830
So what is scaling?

00:01:48.830 --> 00:01:53.650
It means handling very high
levels of QPS with low latency

00:01:53.650 --> 00:01:56.460
and few or no errors.

00:01:56.460 --> 00:01:59.250
This leads to happy users,
happy engineers, happy

00:01:59.250 --> 00:02:03.430
investors and that's what we're
all looking for here.

00:02:03.430 --> 00:02:06.460
So QPS, that's queries per
second, we use that term a

00:02:06.460 --> 00:02:09.479
lot, it's in the admin
console when you go

00:02:09.479 --> 00:02:11.490
and work with AppEngine.

00:02:11.490 --> 00:02:14.050
Latency, we're referring to the
time it takes to handle a

00:02:14.050 --> 00:02:17.470
request. With errors we're
talking about HTTP response

00:02:17.470 --> 00:02:21.930
code, so 500 plus you consider
a scaling error.

00:02:21.930 --> 00:02:26.400
And then if you think about
numbers, what constitutes a

00:02:26.400 --> 00:02:30.020
very high traffic app that
needs to scale well?

00:02:30.020 --> 00:02:32.920
A query per second, I mean if
you actually just upload an

00:02:32.920 --> 00:02:36.960
app to AppEngine and you are
getting a query per second

00:02:36.960 --> 00:02:39.160
that's actually pretty good
traffic, but that's not really

00:02:39.160 --> 00:02:42.850
that hard to achieve in
terms of scalability.

00:02:42.850 --> 00:02:47.045
When you're starting to get to
100 QPS, 1000 QPS, that's an

00:02:47.045 --> 00:02:48.380
app that needs to scale.

00:02:48.380 --> 00:02:50.760
When you're talking about 10,000
or more, that's a very

00:02:50.760 --> 00:02:53.270
impressive application that's
getting a lot of traffic.

00:02:53.270 --> 00:02:57.732
It needs to be really well
architected and needs to use a

00:02:57.732 --> 00:03:01.220
lot of the techniques that Guido
will be talking about.

00:03:01.220 --> 00:03:03.650
So an example of a scalable app
that's currently running

00:03:03.650 --> 00:03:05.850
on AppEngine is Panoramio.

00:03:05.850 --> 00:03:08.960
This is an application that
allows you to kind of take

00:03:08.960 --> 00:03:13.340
your lens, it's like a way to go
look at photos that people

00:03:13.340 --> 00:03:16.460
have shared around the world.

00:03:16.460 --> 00:03:18.680
It's actually a Google
application

00:03:18.680 --> 00:03:21.440
that runs on AppEngine.

00:03:21.440 --> 00:03:25.910
So they get about 2000
QPS at their peak.

00:03:25.910 --> 00:03:29.070
Nice traffic curve there,
that's four

00:03:29.070 --> 00:03:31.100
days' worth of traffic.

00:03:31.100 --> 00:03:34.663
If you look at the breakdown of
there traffic, most of that

00:03:34.663 --> 00:03:37.370
is a dynamic request that's been
served by AppEngine, some

00:03:37.370 --> 00:03:43.760
of it is cached, and their
latency is very low and

00:03:43.760 --> 00:03:46.110
constant, so this is an example
of an app that's doing

00:03:46.110 --> 00:03:49.000
a really great job
using AppEngine.

00:03:49.000 --> 00:03:53.720
This is their errors, so this
is about between 8 and 30

00:03:53.720 --> 00:03:54.800
errors per second.

00:03:54.800 --> 00:03:56.470
This is actually really
low, this is

00:03:56.470 --> 00:03:58.920
like .01% of the traffic.

00:03:58.920 --> 00:04:01.770
So this is a really high
performance app that is really

00:04:01.770 --> 00:04:04.700
showing what you can
do with AppEngine.

00:04:04.700 --> 00:04:07.660
So is scaling hard?

00:04:07.660 --> 00:04:08.870
There's two schools of
thought about this.

00:04:08.870 --> 00:04:12.030
I would say, to some extent,
the answer's no.

00:04:12.030 --> 00:04:14.320
Most programs are actually
really scalable.

00:04:14.320 --> 00:04:15.910
If you just have a simple
website, if you're just

00:04:15.910 --> 00:04:19.600
serving static content, if
you're reading and writing a

00:04:19.600 --> 00:04:21.680
few form fields on a request,
that's actually

00:04:21.680 --> 00:04:23.080
really easy to do.

00:04:23.080 --> 00:04:25.300
If you just upload that to
AppEngine it'll most likely

00:04:25.300 --> 00:04:27.920
scale really well out the box.

00:04:27.920 --> 00:04:30.390
But when you get to more
interesting programs that are

00:04:30.390 --> 00:04:33.930
doing aggregations, data joins,
fan-ins, fan-outs, if

00:04:33.930 --> 00:04:36.890
you're working with complex data
structures, if you are

00:04:36.890 --> 00:04:39.460
working with large amounts of
data, audio, video, images

00:04:39.460 --> 00:04:43.520
these things are potential
bottlenecks in your

00:04:43.520 --> 00:04:46.990
application, and you need to be
careful about how you are

00:04:46.990 --> 00:04:49.520
using these things
to scale well.

00:04:49.520 --> 00:04:51.220
And so the truth is
that it depends.

00:04:51.220 --> 00:04:52.680
It depends on the problem
you're solving.

00:04:52.680 --> 00:04:55.440
It depends on the infrastructure
you're using.

00:04:55.440 --> 00:04:57.770
If you're using your laptop,
obviously you're going to have

00:04:57.770 --> 00:04:59.830
some limits even if it's
a very well designed

00:04:59.830 --> 00:05:00.990
application.

00:05:00.990 --> 00:05:04.220
If you have a rack of machines
that you're managing, if you

00:05:04.220 --> 00:05:06.350
want to scale up you have to
go buy a computer, plug it

00:05:06.350 --> 00:05:07.850
into the rack.

00:05:07.850 --> 00:05:10.120
If you're on a scalable cloud
like AppEngine, things get a

00:05:10.120 --> 00:05:10.660
lot easier.

00:05:10.660 --> 00:05:12.970
We'll talk about
how that works.

00:05:12.970 --> 00:05:14.870
And of course it depends on how
much time and money you

00:05:14.870 --> 00:05:17.160
want to throw the problem.

00:05:17.160 --> 00:05:20.720
So efficient scaling is what
we're talking about.

00:05:20.720 --> 00:05:21.860
But programs are
born scalable.

00:05:21.860 --> 00:05:25.690
Hello World is a really scalable
app on AppEngine.

00:05:25.690 --> 00:05:29.790
You can get extremely high QPS
levels out of Hello World

00:05:29.790 --> 00:05:31.750
without a lot of work.

00:05:31.750 --> 00:05:33.945
But adding things makes
them less scalable.

00:05:33.945 --> 00:05:38.320
So reading and writing data,
making HTTP requests, large

00:05:38.320 --> 00:05:41.850
requests and responses, again
large amounts of data, can

00:05:41.850 --> 00:05:43.940
slow you down.

00:05:43.940 --> 00:05:46.910
Really this is the interesting
work that most applications

00:05:46.910 --> 00:05:51.380
are doing, so we'll be talking
about strategies to avoid

00:05:51.380 --> 00:05:55.310
these potential hot spots.

00:05:55.310 --> 00:05:58.745
So caching parallel, do things
in parallel using async APIs,

00:05:58.745 --> 00:06:02.130
or threading, can you
do things offline?

00:06:02.130 --> 00:06:04.360
These are things that you should
be thinking about as

00:06:04.360 --> 00:06:07.960
your application starts
to get more complex.

00:06:07.960 --> 00:06:08.930
So let's talk about
the AppEngine

00:06:08.930 --> 00:06:12.810
platform and how it works.

00:06:12.810 --> 00:06:15.220
So AppEngine's all HTTP
based, everything is

00:06:15.220 --> 00:06:17.330
based around requests.

00:06:17.330 --> 00:06:19.980
We have the concept of app
instances, so you upload your

00:06:19.980 --> 00:06:22.740
app to tap AppEngine you just
write some code, hit upload

00:06:22.740 --> 00:06:24.850
and it's there, it's deployed.

00:06:24.850 --> 00:06:27.830
And each instance of your
application is a process that

00:06:27.830 --> 00:06:29.750
can handle any number
of requests.

00:06:29.750 --> 00:06:34.000
So python processes are single
threaded, and handle requests

00:06:34.000 --> 00:06:35.780
one after the other in serial.

00:06:35.780 --> 00:06:40.680
Java just recently added
multithreaded support as of

00:06:40.680 --> 00:06:45.480
1.4.3 and this allows Java to
handle requests in parallel if

00:06:45.480 --> 00:06:47.350
you enable it.

00:06:47.350 --> 00:06:49.620
AppEngine performs
dynamic scaling.

00:06:49.620 --> 00:06:53.400
So you actually don't use any
instances that would take up

00:06:53.400 --> 00:06:56.350
any space in AppEngine if you're
not getting traffic.

00:06:56.350 --> 00:06:59.450
And as your traffic starts to
rise we just automatically are

00:06:59.450 --> 00:07:03.430
creating instances for you based
on a scaling protocol.

00:07:03.430 --> 00:07:04.720
So we look at a lot of
variables, perform a

00:07:04.720 --> 00:07:07.126
calculation, and then if your
traffic levels of are starting

00:07:07.126 --> 00:07:09.250
to increase, we'll add
some more instances.

00:07:09.250 --> 00:07:12.070
So we'll go into a little
more detail about that.

00:07:12.070 --> 00:07:15.560
And our platform both are to
minimize your latency of the

00:07:15.560 --> 00:07:17.520
requests that are coming in
before they get handled by an

00:07:17.520 --> 00:07:20.900
instance, minimize the number
of instances, and then have

00:07:20.900 --> 00:07:22.690
very high utilization
of those.

00:07:25.360 --> 00:07:28.860
So the scaling formula.

00:07:28.860 --> 00:07:31.790
Whenever you get a request,
whenever a request arrives at

00:07:31.790 --> 00:07:35.850
AppEngine, it actually doesn't
immediately get

00:07:35.850 --> 00:07:38.070
handled by an instance.

00:07:38.070 --> 00:07:41.810
It waits for a short period of
time in a pending queue.

00:07:41.810 --> 00:07:44.650
And this is in contrast to
some more traditional

00:07:44.650 --> 00:07:48.700
architectures, where requests
are immediately routed to a

00:07:48.700 --> 00:07:50.030
machine or a server.

00:07:50.030 --> 00:07:52.260
So AppEngine has to
make a decision.

00:07:52.260 --> 00:07:56.860
It has to decide how long to
wait, and what to do with that

00:07:56.860 --> 00:07:59.080
request. Should it give it
to any existing instance?

00:07:59.080 --> 00:08:01.020
Or should it create
a new instance?

00:08:01.020 --> 00:08:03.670
So the inputs to this formula
are: the number of instances

00:08:03.670 --> 00:08:06.590
that you currently have, the
throughput of those instances,

00:08:06.590 --> 00:08:08.400
and the number of requests
that are waiting in the

00:08:08.400 --> 00:08:10.370
fitting queue in order
to be served.

00:08:10.370 --> 00:08:12.910
And when I say waiting, we're
talking on the order of less

00:08:12.910 --> 00:08:13.800
than a second.

00:08:13.800 --> 00:08:17.240
Less than 100 milliseconds.

00:08:17.240 --> 00:08:19.530
And so AppEngine will make a
prediction based on these

00:08:19.530 --> 00:08:23.170
variables of how long requests
are going to have to wait in

00:08:23.170 --> 00:08:26.030
order to be handled by one of
these existing instances, and

00:08:26.030 --> 00:08:29.980
if that prediction is too long
we'll create a new instance.

00:08:29.980 --> 00:08:34.480
So there's a comparison to the
loading time, and you can see

00:08:34.480 --> 00:08:36.580
some sample code that
gives you the

00:08:36.580 --> 00:08:40.000
idea of how that works.

00:08:40.000 --> 00:08:42.880
So let's walk you through
a quick example.

00:08:42.880 --> 00:08:46.040
So let's say you have an
application that has about 100

00:08:46.040 --> 00:08:50.290
milliseconds of latency once
it handles a request. But

00:08:50.290 --> 00:08:52.480
creating a new instance
takes a second.

00:08:52.480 --> 00:08:54.310
So AppEngine is tracking these
variables for this

00:08:54.310 --> 00:08:55.860
application.

00:08:55.860 --> 00:08:58.080
And let's say there's five
instances already.

00:08:58.080 --> 00:09:01.415
So, in the first case, let's say
there's ten requests that

00:09:01.415 --> 00:09:04.240
are sitting in the pending queue
that have just arrived.

00:09:04.240 --> 00:09:07.980
The wait time to handle those
requests for the next request

00:09:07.980 --> 00:09:09.930
that comes in that queue, it'll
have to wait about 200

00:09:09.930 --> 00:09:10.395
milliseconds.

00:09:10.395 --> 00:09:13.360
AppEngine knows this because
it's looking at your latency.

00:09:13.360 --> 00:09:16.140
So it's going to wait.

00:09:16.140 --> 00:09:18.480
It's going to not load a new
instance because it would take

00:09:18.480 --> 00:09:20.480
a second to load up
that instance.

00:09:20.480 --> 00:09:22.520
So it's faster just to wait.

00:09:22.520 --> 00:09:24.840
And so that's the result.

00:09:24.840 --> 00:09:27.050
In the other case, let's take
another example where 100

00:09:27.050 --> 00:09:29.140
requests are sitting in
the pending queue.

00:09:29.140 --> 00:09:31.580
The next request that comes in
would have to wait two seconds

00:09:31.580 --> 00:09:34.500
for those five instances to
churn through the request

00:09:34.500 --> 00:09:38.800
queue in order to handle that
request. So in this case the

00:09:38.800 --> 00:09:42.250
results of the algorithm is
to create a new instance.

00:09:42.250 --> 00:09:43.500
So you can see how this works.

00:09:45.580 --> 00:09:47.980
But there's actually a little
more detail to it.

00:09:47.980 --> 00:09:49.630
It's not enough just to compare
the loading time and

00:09:49.630 --> 00:09:50.530
the waiting time.

00:09:50.530 --> 00:09:53.170
We also need to calculate and to
take into account the warm

00:09:53.170 --> 00:09:55.750
latency, the latency of how
long it takes to handle a

00:09:55.750 --> 00:09:57.980
request once your instance
is ready to go.

00:09:57.980 --> 00:10:04.160
And in this case it was about
100 milliseconds.

00:10:04.160 --> 00:10:06.490
We actually would more
aggressively create new

00:10:06.490 --> 00:10:10.020
instances for this application
if the pending time started to

00:10:10.020 --> 00:10:13.240
get large in comparison to
the warm request latency.

00:10:13.240 --> 00:10:15.480
AppEngine doesn't want to be the
cause of your latency, so

00:10:15.480 --> 00:10:18.750
we do what we can't to increase
your instance count

00:10:18.750 --> 00:10:20.130
and scale you up.

00:10:20.130 --> 00:10:22.460
So in steady state, there's
going to be a certain

00:10:22.460 --> 00:10:25.530
percentage of your instances
that are idle, as they finish

00:10:25.530 --> 00:10:28.330
a request, wait for a short
period of time for a new

00:10:28.330 --> 00:10:31.020
request to come off
the pending queue.

00:10:31.020 --> 00:10:33.720
But the waiting time is going to
be very small in comparison

00:10:33.720 --> 00:10:34.720
to your latency.

00:10:34.720 --> 00:10:38.010
But this is a dynamic process
and new requests are coming in

00:10:38.010 --> 00:10:39.070
all the time.

00:10:39.070 --> 00:10:41.610
Instances may have to be turned
down if they have an

00:10:41.610 --> 00:10:45.010
error or they exceed a certain
number of requests to be

00:10:45.010 --> 00:10:47.680
handled overtime.

00:10:47.680 --> 00:10:49.320
So things are always in flux.

00:10:49.320 --> 00:10:52.690
So AppEngine is doing what it
can to really optimize your

00:10:52.690 --> 00:10:57.040
performance and also
your utilisation.

00:10:57.040 --> 00:11:00.870
Warmup requests are something
I'd also highlight here.

00:11:00.870 --> 00:11:02.940
Warm up requests allow AppEngine
to create new

00:11:02.940 --> 00:11:06.795
instances in the background
without actually handling a

00:11:06.795 --> 00:11:12.540
user request. And so users will
never see the latency

00:11:12.540 --> 00:11:15.560
that would result from loading
up a new instance due to a

00:11:15.560 --> 00:11:18.640
warmup request. So you can
enable this in your app.yaml

00:11:18.640 --> 00:11:23.900
in Python, there's also a Java
version equivalent, by adding

00:11:23.900 --> 00:11:27.230
in-bound services warm up.

00:11:27.230 --> 00:11:29.070
So how quickly can your
app scale up?

00:11:29.070 --> 00:11:31.880
You know you've got a big
mention on Tech Crunch, you're

00:11:31.880 --> 00:11:33.760
getting a ton of traffic.

00:11:33.760 --> 00:11:35.320
It depends on your latency.

00:11:35.320 --> 00:11:37.730
And these are some guidelines
I would give you guys.

00:11:37.730 --> 00:11:40.170
If you latency is about 100
milliseconds this is

00:11:40.170 --> 00:11:41.680
excellent, you're doing
really well.

00:11:41.680 --> 00:11:45.950
If your latency is 250
milliseconds, that's OK.

00:11:45.950 --> 00:11:49.150
When you start to get a second
or higher, AppEngine is going

00:11:49.150 --> 00:11:51.545
to be a little more conservative
about giving you

00:11:51.545 --> 00:11:55.060
new instances, because your
application is not performing

00:11:55.060 --> 00:11:56.380
quite quickly enough.

00:11:56.380 --> 00:11:58.290
And it also depends on
your loading time.

00:11:58.290 --> 00:12:01.600
So if you're less than a second
that's a pretty good

00:12:01.600 --> 00:12:02.140
loading time.

00:12:02.140 --> 00:12:05.040
AppEngine will be willing to
load up new instances.

00:12:05.040 --> 00:12:08.210
Anything longer than that starts
to slow things down.

00:12:08.210 --> 00:12:10.660
And so what we're talking about
in this section of the

00:12:10.660 --> 00:12:15.950
talk is the velocity of adding
and handling more queries per

00:12:15.950 --> 00:12:20.330
second, so let's say your
traffic goes from 0 to 10 QPS

00:12:20.330 --> 00:12:24.060
for let's say a 200 millisecond
application, you

00:12:24.060 --> 00:12:27.680
can you scale very quickly
in about a second.

00:12:27.680 --> 00:12:30.420
When you get to 100 QPS
you're talking just

00:12:30.420 --> 00:12:31.860
maybe about ten seconds.

00:12:31.860 --> 00:12:35.350
When you talk about 1000 QPS,
that's quite a lot of traffic,

00:12:35.350 --> 00:12:40.040
AppEngine needs a little bit
of time to react to that.

00:12:40.040 --> 00:12:43.470
And because throughput is so
important, I just wanted to

00:12:43.470 --> 00:12:45.880
contrast single threaded and
the multi-threaded case.

00:12:45.880 --> 00:12:49.750
So in the single threaded case,
your run times are only

00:12:49.750 --> 00:12:51.830
handling one request
at a time.

00:12:51.830 --> 00:12:55.010
So the QPS really is almost
entirely determined by the

00:12:55.010 --> 00:12:56.780
latency of your application.

00:12:56.780 --> 00:12:58.370
You can see some
examples here.

00:12:58.370 --> 00:13:02.630
In the multithreaded case, and
again this is new in Java,

00:13:02.630 --> 00:13:06.500
that as of 1.4.3 you can mark
your app thread safe and it

00:13:06.500 --> 00:13:08.670
can handle multiple requests
at the same time.

00:13:08.670 --> 00:13:13.180
So this is a really great way to
make your Java applications

00:13:13.180 --> 00:13:14.950
highly scalable and I
recommend you guys

00:13:14.950 --> 00:13:17.050
go check this out.

00:13:17.050 --> 00:13:21.960
And really at this point your
throughput per instance is

00:13:21.960 --> 00:13:25.780
primarily determined by the
amount of CPU usage that

00:13:25.780 --> 00:13:29.390
you're doing to handle a
request. AppEngine will

00:13:29.390 --> 00:13:32.130
continue to give your
multithreaded instances more

00:13:32.130 --> 00:13:36.750
and more requests, as long as
the CPU rate is reasonable.

00:13:36.750 --> 00:13:40.400
And I've given some examples
with 2.4 gigahertz processors

00:13:40.400 --> 00:13:43.920
which is about what we're
using right now.

00:13:43.920 --> 00:13:46.180
And so you can take a look
at the instances console.

00:13:46.180 --> 00:13:49.290
This is what a typical
app might look like.

00:13:49.290 --> 00:13:52.900
You can see the QPS per
instance, the latency, and you

00:13:52.900 --> 00:13:55.550
should go check this console out
pretty regularly to just

00:13:55.550 --> 00:13:58.420
see how your instances are
performing, and see if you can

00:13:58.420 --> 00:14:00.180
do something to make
them more scalable.

00:14:00.180 --> 00:14:03.380
In this example, this app has
enabled always on, and always

00:14:03.380 --> 00:14:05.650
on is another thing that you
guys think could enable to

00:14:05.650 --> 00:14:06.510
help your apps scale.

00:14:06.510 --> 00:14:09.220
Because you'll have three
instances that are always

00:14:09.220 --> 00:14:11.010
ready to go at any time.

00:14:11.010 --> 00:14:14.080
And I'd recommend you do this
if you are thinking you're

00:14:14.080 --> 00:14:16.170
going to expects some
load spikes or

00:14:16.170 --> 00:14:19.150
have erratic traffic.

00:14:19.150 --> 00:14:21.800
But back to reality,
you know we've gone

00:14:21.800 --> 00:14:23.390
through some math here.

00:14:23.390 --> 00:14:26.850
How do things work
in real life?

00:14:26.850 --> 00:14:29.060
These theoretical limits we've
been talking about, they're

00:14:29.060 --> 00:14:31.120
not exactly achievable.

00:14:31.120 --> 00:14:34.610
There's routing overhead,
there's load fluctuations,

00:14:34.610 --> 00:14:36.750
there's some safety limits that
app engine has in place

00:14:36.750 --> 00:14:39.430
to prevent us from just creating
hundreds and hundreds

00:14:39.430 --> 00:14:42.590
of new instances in a very
short period of time.

00:14:42.590 --> 00:14:46.010
So app engine is a very scalable
platform, it does

00:14:46.010 --> 00:14:49.490
need a few seconds, a few
minutes if you're talking

00:14:49.490 --> 00:14:51.770
about thousands of QPS.

00:14:51.770 --> 00:14:54.220
And we do tweak the formula
regularly, so you make

00:14:54.220 --> 00:14:56.710
trade-offs between performance
and utilisation.

00:14:56.710 --> 00:14:58.790
Of course we could just throw
more instances at your

00:14:58.790 --> 00:15:04.630
application, but that wouldn't
provide very good utilization.

00:15:04.630 --> 00:15:06.720
And there's machine upgrades,
infrastructure changes that

00:15:06.720 --> 00:15:09.230
are taking place, we introduce
new features like

00:15:09.230 --> 00:15:11.850
pre-compilation, warming
requests like always on that

00:15:11.850 --> 00:15:15.340
make things scale better.

00:15:15.340 --> 00:15:17.400
So just some take aways
I would mention.

00:15:17.400 --> 00:15:19.480
AppEngine does a lot for you.

00:15:19.480 --> 00:15:23.230
It tracks your latency, your
CPU, it's a scalable cloud

00:15:23.230 --> 00:15:26.320
platform that's always adding
and removing instances.

00:15:26.320 --> 00:15:29.230
It's trying to optimize both
performance and utilization.

00:15:29.230 --> 00:15:32.020
And it responds very quickly
to traffic spikes.

00:15:32.020 --> 00:15:33.700
But it doesn't understand where
your latencies comes

00:15:33.700 --> 00:15:37.350
from, it doesn't understand
where your CP usage is coming

00:15:37.350 --> 00:15:40.670
from, it he can't make your
app more performing.

00:15:40.670 --> 00:15:43.830
So Guido will be talking a
little bit about that in a

00:15:43.830 --> 00:15:46.230
second, but you know it's really
a partnership between

00:15:46.230 --> 00:15:48.390
you and AppEngine.

00:15:48.390 --> 00:15:51.210
And just as an example, here's
a successful partnership.

00:15:51.210 --> 00:15:56.110
This is Matt Mastracci of gri.pe
and he just sent us an

00:15:56.110 --> 00:16:00.320
e-mail last November, saying
just wanted to say thanks for

00:16:00.320 --> 00:16:03.550
making a great product, they got
mentioned on "The View",

00:16:03.550 --> 00:16:06.640
resulted in a lot of traffic,
it looks like they got up to

00:16:06.640 --> 00:16:09.040
about 300 QPS.

00:16:09.040 --> 00:16:11.610
AppEngine scaled wonderfully, we
got about 900 errors on the

00:16:11.610 --> 00:16:13.530
front page while it scaled up,
but compared to the overall

00:16:13.530 --> 00:16:15.370
traffic that was nothing.

00:16:15.370 --> 00:16:17.080
Our app has seen a ton of
traffic but it's amazingly

00:16:17.080 --> 00:16:19.920
fast, you wouldn't even know.

00:16:19.920 --> 00:16:22.130
And I think this really
shows like how

00:16:22.130 --> 00:16:24.000
AppEngine typically works.

00:16:24.000 --> 00:16:26.170
We didn't do anything
special for Matt.

00:16:26.170 --> 00:16:27.480
He's just a typical user.

00:16:27.480 --> 00:16:31.190
So with that I'll turn it over
to Guido, to talk about how to

00:16:31.190 --> 00:16:33.212
build a scalable app.

00:16:33.212 --> 00:16:35.432
GUIDO VAN ROSSUM: Thank
you Justin.

00:16:35.432 --> 00:16:36.682
[APPLAUSE]

00:16:40.900 --> 00:16:43.860
So now that Justin has discussed
some of the theory

00:16:43.860 --> 00:16:47.910
behind AppEngine scaling, I'm
going to discuss what you

00:16:47.910 --> 00:16:50.530
actually have to do to make
your apps scale as well as

00:16:50.530 --> 00:16:53.260
that successful gri.pe
application.

00:16:53.260 --> 00:16:56.690
So there are some techniques
and tools that I want to

00:16:56.690 --> 00:17:00.180
discuss, beginning with loading
testing and then

00:17:00.180 --> 00:17:04.069
focusing a little bit bit on
something called Appstats.

00:17:04.069 --> 00:17:08.800
But before I even start this, I
want to mention that always

00:17:08.800 --> 00:17:13.680
you have to treat this as sort
of experimental science.

00:17:13.680 --> 00:17:16.960
You're looking at a very
complicated system, it behaves

00:17:16.960 --> 00:17:20.569
a certain way, you have to
really poke and prod it in

00:17:20.569 --> 00:17:24.290
various ways to find out
how it really behaves.

00:17:24.290 --> 00:17:27.819
Which is often sort
of unintuitive.

00:17:27.819 --> 00:17:30.250
You you might not actually
understand the performance

00:17:30.250 --> 00:17:32.770
characteristics of your own
application, even if you wrote

00:17:32.770 --> 00:17:34.140
the code yourself.

00:17:34.140 --> 00:17:36.120
At least that is my personal
experience.

00:17:39.590 --> 00:17:45.070
Even so, whatever you learn
today might not be valid

00:17:45.070 --> 00:17:46.990
tomorrow, or next week
or next year.

00:17:46.990 --> 00:17:48.770
Because your users change.

00:17:48.770 --> 00:17:50.830
Maybe they become more
sophisticated, maybe you

00:17:50.830 --> 00:17:54.340
attract different types of
users, also of course your

00:17:54.340 --> 00:17:55.610
application changes.

00:17:55.610 --> 00:17:59.080
And it's very likely that when
you add the new feature in

00:17:59.080 --> 00:18:02.440
this part of your application,
somehow it effects the

00:18:02.440 --> 00:18:06.080
performance of some other part
of your application that you

00:18:06.080 --> 00:18:10.170
didn't think was affected
by that feature at all.

00:18:10.170 --> 00:18:14.290
Also as data accumulates in the
AppEngine data store, the

00:18:14.290 --> 00:18:15.790
performance characteristics
of the data

00:18:15.790 --> 00:18:18.200
store may change somewhat.

00:18:18.200 --> 00:18:20.820
And of course AppEngine
itself changes.

00:18:20.820 --> 00:18:24.720
I mean it's a very complicated
production environment, the

00:18:24.720 --> 00:18:29.730
sort of the network weather
varies by day and by week.

00:18:29.730 --> 00:18:32.990
Also the AppEngine team always
make improvements, as Justin

00:18:32.990 --> 00:18:36.210
mentioned, that complex scaling
formula is regularly

00:18:36.210 --> 00:18:40.620
tweaked and tunes and
sort of adjusted.

00:18:40.620 --> 00:18:42.600
And in general we do
that to make sure

00:18:42.600 --> 00:18:43.660
your apps scale better.

00:18:43.660 --> 00:18:47.370
But sometimes we make changes
that like work out one way for

00:18:47.370 --> 00:18:51.980
90% of the apps and work out a
slightly less positive way for

00:18:51.980 --> 00:18:53.160
some other apps.

00:18:53.160 --> 00:18:56.800
So, sort of be aware, even if
you think you know exactly,

00:18:56.800 --> 00:19:00.570
you understand the performance
of your app, it may change.

00:19:00.570 --> 00:19:02.890
So keep an eye on it.

00:19:02.890 --> 00:19:07.765
So the most important technique
that I recommend

00:19:07.765 --> 00:19:13.400
that anybody who is expecting
or even hoping to reach a

00:19:13.400 --> 00:19:18.380
significant number of QPS, you
have to do this basically.

00:19:18.380 --> 00:19:21.910
Run a synthetic load test. There
are many tools to sort

00:19:21.910 --> 00:19:25.720
of aid you in load testing
that can send synthetic

00:19:25.720 --> 00:19:30.080
traffic to your site from some
other system that is outside

00:19:30.080 --> 00:19:31.710
AppEngine or sometimes
some tools

00:19:31.710 --> 00:19:34.270
actually run on AppEngine.

00:19:34.270 --> 00:19:38.360
And the important reason to
actually try your app with

00:19:38.360 --> 00:19:42.270
synthetic traffic is that when
you when you didn't do this,

00:19:42.270 --> 00:19:47.770
and you hit a sort of sudden
success with live traffic,

00:19:47.770 --> 00:19:51.530
it's too late to fix things or
it's a mad scramble to figure

00:19:51.530 --> 00:19:53.290
out what went wrong.

00:19:53.290 --> 00:19:57.340
And the thing is, in practice
you almost always hit a bump

00:19:57.340 --> 00:20:01.390
if you didn't actually
plan and test.

00:20:01.390 --> 00:20:04.010
So the basic approach
for load testing is

00:20:04.010 --> 00:20:06.060
actually fairly simple.

00:20:06.060 --> 00:20:09.210
You sort, of using the one of
these tools to generate

00:20:09.210 --> 00:20:12.220
synthetic traffic, you generally
increase your

00:20:12.220 --> 00:20:15.280
traffic that your application
receives.

00:20:15.280 --> 00:20:17.910
And I would recommend using a
test instance or at least a

00:20:17.910 --> 00:20:20.760
test version of the application
if you've already

00:20:20.760 --> 00:20:22.665
gone live for a smaller
number of users.

00:20:22.665 --> 00:20:25.870
So you increase your
traffic basically

00:20:25.870 --> 00:20:27.490
until you hit a bump.

00:20:27.490 --> 00:20:31.440
That bump can be that your
application suddenly starts

00:20:31.440 --> 00:20:34.070
return much higher
error rates.

00:20:34.070 --> 00:20:37.870
Or it just sort of starts
responding slower and slower.

00:20:37.870 --> 00:20:40.550
Those two are actually
pretty related.

00:20:40.550 --> 00:20:45.500
Or your load testing tool finds
out that it cannot drive

00:20:45.500 --> 00:20:49.230
traffic at the site faster
than a certain QPS.

00:20:49.230 --> 00:20:53.020
That is the point where you're
going to, unless you say OK

00:20:53.020 --> 00:20:57.000
I'm really happy with 20 QPS,
that's all I want to shoot

00:20:57.000 --> 00:21:01.640
for, but in general you probably
hit the bottle neck

00:21:01.640 --> 00:21:03.360
and you want to get
beyond that point.

00:21:03.360 --> 00:21:06.490
So now you use some other
tools, and there's the

00:21:06.490 --> 00:21:10.200
AppEngine dashboard, the whole
admin console has a number of

00:21:10.200 --> 00:21:11.930
different tools just
already showed up.

00:21:11.930 --> 00:21:14.980
The instances console, there's
a whole bunch of different

00:21:14.980 --> 00:21:18.400
charts of performance over
time that you can use.

00:21:18.400 --> 00:21:24.560
The logs are a very important
tool, also the quoted details

00:21:24.560 --> 00:21:26.780
help you understand
what is going on.

00:21:26.780 --> 00:21:28.950
How much how much of each
type of resource

00:21:28.950 --> 00:21:31.340
your application uses.

00:21:31.340 --> 00:21:33.990
And then there's Appstats which
we will get to it in a

00:21:33.990 --> 00:21:35.290
few slides.

00:21:35.290 --> 00:21:38.810
Using all those tools together,
you have to actually

00:21:38.810 --> 00:21:42.310
sort of perform science and
reasoning and logic about your

00:21:42.310 --> 00:21:46.060
application as if it's a black
box and you don't understand

00:21:46.060 --> 00:21:49.940
exactly how it works, until
you understand why it is

00:21:49.940 --> 00:21:51.130
hitting this bump.

00:21:51.130 --> 00:21:52.960
Is it a data store problem?

00:21:52.960 --> 00:21:56.790
Is your cache not operating
correctly?

00:21:56.790 --> 00:22:00.020
Are you're spending too much
time expanding the template?

00:22:00.020 --> 00:22:02.510
Are you hanging on some
external URL?

00:22:02.510 --> 00:22:04.970
There are many possible
reasons.

00:22:04.970 --> 00:22:10.130
So once you understand what's
going in your application, you

00:22:10.130 --> 00:22:12.650
produce a fix.

00:22:12.650 --> 00:22:15.280
You rewrite some small section
of code-- hopefully it's a

00:22:15.280 --> 00:22:17.220
small section--

00:22:17.220 --> 00:22:21.080
fine tune something, change some
configuration, redeploy

00:22:21.080 --> 00:22:22.610
and then you go at it again.

00:22:22.610 --> 00:22:30.860
It's just a rinse and repeat
approach and hopefully now

00:22:30.860 --> 00:22:32.040
you'll get a little higher.

00:22:32.040 --> 00:22:35.720
And then probably, if you keep
driving more and more traffic

00:22:35.720 --> 00:22:39.220
at as your application, it is
very likely that you'll hit

00:22:39.220 --> 00:22:42.390
another bump in a different part
of your application or a

00:22:42.390 --> 00:22:44.460
different part of the
infrastructure.

00:22:44.460 --> 00:22:48.310
You learn a lot about AppEngine
this way, and you

00:22:48.310 --> 00:22:51.750
learn a lot about sort of
performance tuning in general

00:22:51.750 --> 00:22:55.450
and about your particular
app in specific.

00:22:55.450 --> 00:22:59.770
And hopefully you will
eventually reach sort of a

00:22:59.770 --> 00:23:04.170
smooth trajectory where you can
handle many 100s of QPS or

00:23:04.170 --> 00:23:07.782
whatever your traffic
projections are.

00:23:07.782 --> 00:23:11.310
So there are a number of things
that I want to sort of

00:23:11.310 --> 00:23:13.020
remind you of doing.

00:23:13.020 --> 00:23:16.810
Justin already explained that
the complex scaling formula is

00:23:16.810 --> 00:23:21.060
very nice but it sort of
generates instances gradually.

00:23:21.060 --> 00:23:27.020
So don't run a load test where
you start driving 200 QPS at

00:23:27.020 --> 00:23:31.520
an app that was dormant before
that, and run that for half a

00:23:31.520 --> 00:23:34.400
minute and see how it performs.
Because during that

00:23:34.400 --> 00:23:38.480
half minute all you will see is
errors while some instances

00:23:38.480 --> 00:23:40.630
are being created but not
enough to actually

00:23:40.630 --> 00:23:41.880
deal with the queue.

00:23:41.880 --> 00:23:46.640
Gradually increase your traffic,
so take like three or

00:23:46.640 --> 00:23:51.660
five minutes or so to reach
100 or 200 QPS.

00:23:51.660 --> 00:23:55.460
Then let it run for
a little while.

00:23:55.460 --> 00:23:59.000
So that you can actually observe
the application in its

00:23:59.000 --> 00:24:01.160
ideal steady state.

00:24:01.160 --> 00:24:05.730
Another thing I want to
emphasize is, it's very easy

00:24:05.730 --> 00:24:11.970
to sort of hit one URL with any
of the load testing tools

00:24:11.970 --> 00:24:14.900
or performance testing tools.

00:24:14.900 --> 00:24:19.240
That may not actually be how
your users are going to use

00:24:19.240 --> 00:24:21.510
that application.

00:24:21.510 --> 00:24:26.330
One technique that I like is
invite a bunch of friends to

00:24:26.330 --> 00:24:31.020
use the application, and sort
of just record what they do.

00:24:31.020 --> 00:24:34.120
Keep track of exactly which
URLs they hit, what their

00:24:34.120 --> 00:24:36.110
behavior is.

00:24:36.110 --> 00:24:39.680
You can use the logs, it's
pretty easy to sort of figure

00:24:39.680 --> 00:24:42.786
out on the logs which part
belongs to which user.

00:24:45.600 --> 00:24:48.940
And so if you do this at a very
small scale but enough to

00:24:48.940 --> 00:24:52.530
have some idea of what users do,
and those sessions you can

00:24:52.530 --> 00:24:57.940
then use to sort of tell your
load testing what kind of

00:24:57.940 --> 00:24:59.930
traffic to drive at
your application.

00:24:59.930 --> 00:25:03.190
Like so many hits of the home
page, so many hits of the

00:25:03.190 --> 00:25:05.640
preferences page,
this and that.

00:25:05.640 --> 00:25:08.780
Make sure to also include static
resources like style

00:25:08.780 --> 00:25:12.110
sheets or images,
what have you.

00:25:12.110 --> 00:25:14.670
Also take into account that
there's a certain amount of

00:25:14.670 --> 00:25:15.700
client caching.

00:25:15.700 --> 00:25:18.900
Although if you expect thousands
of different users,

00:25:18.900 --> 00:25:22.640
of course each of those users
is going to request every

00:25:22.640 --> 00:25:25.340
static resource that you have
on your home page at least

00:25:25.340 --> 00:25:29.220
once, so sort of think about all
those things to create a

00:25:29.220 --> 00:25:31.060
realistic synthetic
traffic pattern.

00:25:31.060 --> 00:25:34.450
Because I've seen some people
very disappointed because they

00:25:34.450 --> 00:25:37.220
had a load test that proved
that they could handle 500

00:25:37.220 --> 00:25:42.140
QPS, and them on the actual
users came along they had a

00:25:42.140 --> 00:25:44.320
bump at 100 QPS.

00:25:44.320 --> 00:25:46.420
Because the users were doing
very different things than the

00:25:46.420 --> 00:25:48.170
load test was testing.

00:25:48.170 --> 00:25:51.120
Also another thing is sometimes
load testing tools

00:25:51.120 --> 00:25:55.110
themselves have an obvious
sort of limitation.

00:25:55.110 --> 00:25:58.500
One obvious limitation is of
course is if the load test,

00:25:58.500 --> 00:26:02.220
the load generating tool runs
on a machine that has

00:26:02.220 --> 00:26:05.370
relatively low network
bandwidth, because then it

00:26:05.370 --> 00:26:08.840
can't actually drive a very
significant amount of traffic.

00:26:08.840 --> 00:26:12.130
Anyway load testing can
be a lot of fun.

00:26:12.130 --> 00:26:14.300
Do it.

00:26:14.300 --> 00:26:17.710
Get good at load testing before
you sort of announce

00:26:17.710 --> 00:26:20.070
your first app to
the wide public.

00:26:20.070 --> 00:26:22.140
It's much better than
having to scramble

00:26:22.140 --> 00:26:24.820
once your load comes.

00:26:24.820 --> 00:26:29.310
So one important tool that I
want to call out at least a

00:26:29.310 --> 00:26:33.000
little bit at this point,
because it's so incredibly

00:26:33.000 --> 00:26:37.080
useful both for debugging apps
in general and for developing

00:26:37.080 --> 00:26:39.700
performance like doing
a load test in

00:26:39.700 --> 00:26:41.740
particular, is Appstats.

00:26:41.740 --> 00:26:44.290
Appstats is a tool that--
actually I gave a talk about

00:26:44.290 --> 00:26:46.470
Appstats about a year ago--

00:26:46.470 --> 00:26:48.820
so I'm not going to say a whole
lot about it because you

00:26:48.820 --> 00:26:52.800
can still find that talk on the
web, search for Appstats

00:26:52.800 --> 00:26:55.240
or Appstats AppEngine or
something and you'll find it.

00:26:55.240 --> 00:26:59.340
So Appstats is based on the
theory that most likely if you

00:26:59.340 --> 00:27:02.770
have an app that doesn't perform
adequately or that

00:27:02.770 --> 00:27:07.780
hits some kind of performance
bump, as you try to scale it,

00:27:07.780 --> 00:27:11.280
the most likely reason is
something to do with remote

00:27:11.280 --> 00:27:12.110
procedure calls.

00:27:12.110 --> 00:27:15.040
Now in AppEngine everything you
do with the data store,

00:27:15.040 --> 00:27:19.880
with Memcache, fetching external
URLs, everything like

00:27:19.880 --> 00:27:24.320
that is actually an RPC.

00:27:24.320 --> 00:27:27.430
Traditional CPU based profiling
just counts function

00:27:27.430 --> 00:27:30.670
calls and times them and doesn't
give you very good

00:27:30.670 --> 00:27:33.660
information about what
those RPCs are.

00:27:33.660 --> 00:27:38.060
Appstats focuses entirely on the
RPCs, collects information

00:27:38.060 --> 00:27:41.490
about every RPCs made you during
a request, and then

00:27:41.490 --> 00:27:44.320
visualizes that in very sort
of in your face ways.

00:27:44.320 --> 00:27:48.030
So it's completely obvious what
your request is doing,

00:27:48.030 --> 00:27:51.410
and often then it's also
completely obvious why it's

00:27:51.410 --> 00:27:54.930
not performing the way you
thought it should be.

00:27:54.930 --> 00:27:58.410
And this tool exists both
for Python and Java.

00:27:58.410 --> 00:28:02.750
So the only other thing I want
to say about Appstats, this is

00:28:02.750 --> 00:28:04.710
the kind of chart you
get out of Appstats.

00:28:04.710 --> 00:28:08.920
This is a very simple time
line , each box here

00:28:08.920 --> 00:28:13.030
represents one RPC, so you can
see that here was a data store

00:28:13.030 --> 00:28:18.440
that called for some reason 247
milliseconds, so that's

00:28:18.440 --> 00:28:20.370
interesting I would say.

00:28:20.370 --> 00:28:22.554
So what you can is you can click
on this box, now this is

00:28:22.554 --> 00:28:24.860
not a live demo so I'm not going
to show that, but you

00:28:24.860 --> 00:28:30.240
can click on that box and it
will, somewhere here below, it

00:28:30.240 --> 00:28:33.260
will expand to a stack trace
where you can see exactly

00:28:33.260 --> 00:28:36.240
which sequence of function
calls led up to that

00:28:36.240 --> 00:28:38.440
particular RPC call.

00:28:38.440 --> 00:28:40.730
And at least if you're using
the Python version, you can

00:28:40.730 --> 00:28:44.150
also inspect the contents of
each stack frame so you'll see

00:28:44.150 --> 00:28:46.950
the local variables and
parameters that will pass into

00:28:46.950 --> 00:28:48.260
these functions.

00:28:48.260 --> 00:28:52.320
So if you Appstats shows that
you're making three queries

00:28:52.320 --> 00:28:56.410
like in this case, or three
queries in a row and you sort

00:28:56.410 --> 00:28:59.640
of thinking about what that
request did, you were only

00:28:59.640 --> 00:29:02.510
expecting it would make one
query, you can immediately

00:29:02.510 --> 00:29:04.900
find out where those other
two queries come from.

00:29:04.900 --> 00:29:07.460
And maybe they're totally
expected once

00:29:07.460 --> 00:29:08.800
you understand that.

00:29:08.800 --> 00:29:15.690
This is an incredible eye
opener, so go download--

00:29:15.690 --> 00:29:18.080
you don't actually have to
download it, it's included in

00:29:18.080 --> 00:29:22.660
the standard SDK and also
in the run time.

00:29:22.660 --> 00:29:25.880
So now that we've sort of
discussed the science of

00:29:25.880 --> 00:29:30.420
measuring what's going on in
your app, by sort of using

00:29:30.420 --> 00:29:34.540
either a telescope or a
microscope, look at your app,

00:29:34.540 --> 00:29:37.940
what things can you do to make
your application work better?

00:29:37.940 --> 00:29:41.810
What what sort of typical
techniques can you use to

00:29:41.810 --> 00:29:44.300
solve the problems that
you found using or

00:29:44.300 --> 00:29:46.180
doing a load test?

00:29:46.180 --> 00:29:47.430
Now there are lots
of strategies.

00:29:49.890 --> 00:29:53.750
One strategy I would sort of
summarize as stupid schema

00:29:53.750 --> 00:29:57.010
tricks, the AppEngine data
store is not a relational

00:29:57.010 --> 00:30:01.560
database and there are a bunch
of things you can do.

00:30:01.560 --> 00:30:06.480
Given the focus on RPCs, the
fact that too many RPCs is so

00:30:06.480 --> 00:30:10.390
often a cause of slowness in
your application, batching

00:30:10.390 --> 00:30:15.370
RPCs together is an important
technique to save on latency.

00:30:15.370 --> 00:30:18.960
There's also something called
parallel RPCs where you don't

00:30:18.960 --> 00:30:22.580
reduce the number of RPCs, but
you reduce the time that you

00:30:22.580 --> 00:30:23.720
wait for them.

00:30:23.720 --> 00:30:27.860
And finally of course there's
lots of different places in

00:30:27.860 --> 00:30:30.115
applications and on the Internet
where you can tune

00:30:30.115 --> 00:30:31.110
your caching.

00:30:31.110 --> 00:30:35.160
So let's look at each of these
in a little more detail.

00:30:35.160 --> 00:30:37.970
So stupid schema tricks.

00:30:37.970 --> 00:30:41.800
Basically and there have been
many talks and blog posts and

00:30:41.800 --> 00:30:44.230
articles about this, so I don't
have to explain much of

00:30:44.230 --> 00:30:48.550
this, but AppEngine's data
store is not a relational

00:30:48.550 --> 00:30:51.070
database, it is not SQL.

00:30:51.070 --> 00:30:55.310
And some of the things that
are sort of dogma with SQL

00:30:55.310 --> 00:30:59.790
actually work counter productive
in AppEngine.

00:30:59.790 --> 00:31:06.820
For one example is, in SQL if
you have a table with lots of

00:31:06.820 --> 00:31:10.910
columns, and you query against
some of those columns, the

00:31:10.910 --> 00:31:13.870
data that it is in columns that
you're not looking at is

00:31:13.870 --> 00:31:15.370
never actually fetched.

00:31:15.370 --> 00:31:18.640
Well in AppEngine it works
slightly different.

00:31:18.640 --> 00:31:21.110
All the data that it is
in a particular row--

00:31:21.110 --> 00:31:24.580
if I can sort of carry over
that SQL terminology--

00:31:24.580 --> 00:31:26.780
is actually fetched into
your application.

00:31:26.780 --> 00:31:31.690
So if you search for certain
columns, all the data that is

00:31:31.690 --> 00:31:34.480
in other columns for the same
record will still be fetched

00:31:34.480 --> 00:31:35.440
to application.

00:31:35.440 --> 00:31:37.620
If you have something like
a photo application that

00:31:37.620 --> 00:31:43.050
contains there's a large blob
of jpeg data that you don't

00:31:43.050 --> 00:31:46.270
use except when you want to
display the image of course,

00:31:46.270 --> 00:31:49.100
you would still be paying for
fetching that as part of your

00:31:49.100 --> 00:31:50.420
query results.

00:31:50.420 --> 00:31:55.050
So a very simple solution here
is to break up that particular

00:31:55.050 --> 00:31:57.700
table or entity kind
as we called it in

00:31:57.700 --> 00:31:59.840
AppEngine, into two parts.

00:31:59.840 --> 00:32:02.400
For example the photo metadata,
which would be a

00:32:02.400 --> 00:32:06.520
small entity with just the
columns that describe things

00:32:06.520 --> 00:32:08.760
like date the picture
was taken, title,

00:32:08.760 --> 00:32:10.660
file names, so on.

00:32:10.660 --> 00:32:14.700
And a separate entity containing
the large bulky

00:32:14.700 --> 00:32:17.970
data, like the image, there may
be a thumbnail and some

00:32:17.970 --> 00:32:19.090
other things.

00:32:19.090 --> 00:32:22.100
So now your query results come
in much faster because that

00:32:22.100 --> 00:32:26.610
bulk data doesn't have to
push through the client.

00:32:26.610 --> 00:32:30.460
Pretty much the opposite is sort
of duplicating data in

00:32:30.460 --> 00:32:33.550
multiple entities.

00:32:33.550 --> 00:32:38.350
This is basically sort of going
against the SQL dogma of

00:32:38.350 --> 00:32:40.320
normalize your schema.

00:32:40.320 --> 00:32:43.150
There are many situations in
AppEngine where it's actually

00:32:43.150 --> 00:32:47.120
useful to actually make
redundant copies of a certain

00:32:47.120 --> 00:32:50.430
piece of information in a couple
of different entities

00:32:50.430 --> 00:32:53.710
that you then all have to update
at once, in order to

00:32:53.710 --> 00:32:57.600
make reading those entities more
efficient, in the sense

00:32:57.600 --> 00:33:00.710
that you only need one of those
entities in order to be

00:33:00.710 --> 00:33:02.280
able to display the
information.

00:33:02.280 --> 00:33:04.820
You don't have to sort of
do pointer chasing.

00:33:04.820 --> 00:33:08.160
Because pointer chasing, which
in the AppEngine data store

00:33:08.160 --> 00:33:12.210
turns into reference chasing,
every time you chase one of

00:33:12.210 --> 00:33:17.880
those references is another
RPC that gets very costly.

00:33:17.880 --> 00:33:20.905
So batching, it's a very
simple concept.

00:33:23.510 --> 00:33:28.180
If you're somehow in a loop, or
maybe that loop is sort of

00:33:28.180 --> 00:33:31.210
hidden in your code, if you are
fetching a whole bunch of

00:33:31.210 --> 00:33:33.440
different entities or maybe
you writing a bunch of

00:33:33.440 --> 00:33:38.940
different entities, one entity
at a time, you pay RPC

00:33:38.940 --> 00:33:44.260
overhead for each entity
you read and write.

00:33:44.260 --> 00:33:47.840
And given that all this goes
across various networking

00:33:47.840 --> 00:33:52.010
nodes, and there's some access
control checking going on, and

00:33:52.010 --> 00:33:57.835
various bits of overhead, the
cost of an RPC is sort of the

00:33:57.835 --> 00:34:00.890
cost that it takes to do the
actual work, like find the

00:34:00.890 --> 00:34:02.660
entity in the data store.

00:34:02.660 --> 00:34:05.860
Plus the cost for the network
traffic going back and forth.

00:34:05.860 --> 00:34:10.560
And there's a certain amount of
constant cost per RPC that

00:34:10.560 --> 00:34:13.650
by combining a whole bunch of
get requests or put requests

00:34:13.650 --> 00:34:16.040
in a single call, you
can shave off lots.

00:34:16.040 --> 00:34:19.900
So using Appstats I took two
snapshots of a very simple

00:34:19.900 --> 00:34:25.520
application, it just fetches
20 different keys using 20

00:34:25.520 --> 00:34:29.120
RPCs, and in this case it took
nearly 400 milliseconds

00:34:29.120 --> 00:34:31.520
realtime to fetch
those entities.

00:34:31.520 --> 00:34:34.350
And then I compared that to a
slight modification of the

00:34:34.350 --> 00:34:39.230
same app where it fetches those
same 20 keys using a

00:34:39.230 --> 00:34:41.980
single batch RPC.

00:34:41.980 --> 00:34:44.400
And here it only takes
200 milliseconds.

00:34:44.400 --> 00:34:47.960
So we sort of halved the latency
of the request by

00:34:47.960 --> 00:34:53.600
using a batch request. So both
the data store and Memcache

00:34:53.600 --> 00:34:56.760
are fairly latency sensitive
in this way.

00:34:56.760 --> 00:35:00.070
So you can get a lot of affect
by using the batch APIs and

00:35:00.070 --> 00:35:02.995
the data store has a slightly
different style of API use it

00:35:02.995 --> 00:35:06.700
has to get a put request, a
list of keys or entities

00:35:06.700 --> 00:35:08.930
instead of a single
key or entity.

00:35:08.930 --> 00:35:12.740
Memcache has seperate get
and get null calls.

00:35:12.740 --> 00:35:16.490
But in both cases the effect
is the same, the more you

00:35:16.490 --> 00:35:19.770
batch, the faster it goes.

00:35:19.770 --> 00:35:22.430
So parallel RPCs is a
different approach.

00:35:25.070 --> 00:35:29.270
Until the AppEngine release
1.5.0 that was released this

00:35:29.270 --> 00:35:34.510
very morning, you can't look
at it because the wifi is

00:35:34.510 --> 00:35:35.570
pretty much down.

00:35:35.570 --> 00:35:36.310
[LAUGHTER]

00:35:36.310 --> 00:35:40.000
But I'll tell you, it is out.

00:35:40.000 --> 00:35:44.530
Until now the main application
of parallel RPCs was for URL

00:35:44.530 --> 00:35:49.990
fetch, which is the AppEngine
specific interface for going

00:35:49.990 --> 00:35:52.750
out on the web and fetching
something from another server

00:35:52.750 --> 00:35:54.860
that is not in Google
data center.

00:35:54.860 --> 00:35:58.040
Or maybe it is in Google
data center.

00:35:58.040 --> 00:36:01.650
And since an external server
easily takes 200, 300

00:36:01.650 --> 00:36:05.410
milliseconds or a second or so
for a round trip, if you have

00:36:05.410 --> 00:36:08.480
to fetch more than one external
RPC, that could

00:36:08.480 --> 00:36:09.500
really slow you down.

00:36:09.500 --> 00:36:13.300
So I think about two years
ago, we had a separate

00:36:13.300 --> 00:36:17.790
asynchronous API to URL fetch
where you can sort of say, go

00:36:17.790 --> 00:36:20.940
fetch this one, go fetch this
one, go fetch this one, and

00:36:20.940 --> 00:36:22.050
then wait for all of them.

00:36:22.050 --> 00:36:27.390
Again here is a little example
chart taken with Appstats

00:36:27.390 --> 00:36:30.520
where you see the dramatic
difference between doing those

00:36:30.520 --> 00:36:34.360
URL fetches in series
or in parallel.

00:36:34.360 --> 00:36:38.710
So the new thing in 1.5.0
is there are separate

00:36:38.710 --> 00:36:44.210
asynchronous calls for data
store get, put, and delete.

00:36:44.210 --> 00:36:45.940
Which are the most important
ones for

00:36:45.940 --> 00:36:48.390
which this makes sense.

00:36:48.390 --> 00:36:51.180
So you can start a series of
gets and a series of puts

00:36:51.180 --> 00:36:55.580
simultaneously, those gets
async or put async calls

00:36:55.580 --> 00:36:58.630
return something called
an RPC handle.

00:36:58.630 --> 00:37:02.400
So they return immediately,
they, in the background, start

00:37:02.400 --> 00:37:05.480
sending the stuff to the data
store and the data store

00:37:05.480 --> 00:37:08.140
starts doing the work.

00:37:08.140 --> 00:37:10.750
And then when your application
is ready to consume the

00:37:10.750 --> 00:37:15.150
results it can call get result
on the handle and then it will

00:37:15.150 --> 00:37:19.085
get the same results as the
corresponding synchronous call

00:37:19.085 --> 00:37:21.100
would have returned.

00:37:21.100 --> 00:37:23.740
The advantage here again is that
you can make a bunch of

00:37:23.740 --> 00:37:28.130
these calls in parallel, set up
a bunch of gets and puts.

00:37:28.130 --> 00:37:32.120
In the future you will even get
be able to set up multiple

00:37:32.120 --> 00:37:34.690
independent transactions.

00:37:34.690 --> 00:37:38.850
And your total wait time is
equal to sort of the slowest

00:37:38.850 --> 00:37:42.450
of all the RPCs rather than
the sum of all the RPCs.

00:37:42.450 --> 00:37:46.300
So the more you parallelize
the more you win.

00:37:46.300 --> 00:37:50.370
And you can do this in Java too,
Google for get async data

00:37:50.370 --> 00:37:54.560
store service to find the proper
documentation for that.

00:37:54.560 --> 00:37:59.360
So the last sort of standard
technique that everybody needs

00:37:59.360 --> 00:38:01.160
to know about and hopefully
you have heard

00:38:01.160 --> 00:38:02.480
about it, is caching.

00:38:02.480 --> 00:38:04.280
So there are actually lots
of different places

00:38:04.280 --> 00:38:05.570
where you can cache.

00:38:05.570 --> 00:38:09.800
This is a tiny little diagram
of how requests travel from

00:38:09.800 --> 00:38:12.270
the browser to your application
to the data store

00:38:12.270 --> 00:38:15.360
and then the data comes back.

00:38:15.360 --> 00:38:18.800
So the question is where can
you where put in caching?

00:38:18.800 --> 00:38:20.600
Well it turns out there
is actually catching

00:38:20.600 --> 00:38:23.390
opportunities in every
one of these boxes.

00:38:23.390 --> 00:38:24.770
So let's look at
the first one.

00:38:24.770 --> 00:38:29.750
In the browser, closest to the
user, the browser caches

00:38:29.750 --> 00:38:35.310
stuff, how much the browser
caches depend on various HTTP

00:38:35.310 --> 00:38:37.870
headers and the most recommended
header at this

00:38:37.870 --> 00:38:39.600
point is cache control.

00:38:39.600 --> 00:38:42.130
There's also something called
Etags which is sort of an

00:38:42.130 --> 00:38:43.330
additional thing.

00:38:43.330 --> 00:38:46.990
And then there's expires but
that's pretty much sort of

00:38:46.990 --> 00:38:48.820
replaced by cache control.

00:38:48.820 --> 00:38:52.320
So if you only want status to
be cached in the browser for

00:38:52.320 --> 00:38:56.470
various privacy reasons you can
set cache control private

00:38:56.470 --> 00:38:57.980
and then you set a max age.

00:38:57.980 --> 00:39:00.795
Now the problem with the max age
is, if you set it too low,

00:39:00.795 --> 00:39:02.920
the cache is not
very effective.

00:39:02.920 --> 00:39:06.830
If you set it too high, the
cache is more effective but

00:39:06.830 --> 00:39:08.950
there's also a probability
that the users

00:39:08.950 --> 00:39:10.140
will see stale data.

00:39:10.140 --> 00:39:13.780
Because the browser really sort
of holds on to that data

00:39:13.780 --> 00:39:15.920
in its cache very effectively.

00:39:15.920 --> 00:39:20.290
So what's a good number
for max age?

00:39:20.290 --> 00:39:22.090
Of course that depends
on your application.

00:39:22.090 --> 00:39:24.270
But basically this
is a problem.

00:39:24.270 --> 00:39:30.180
So there's a different place
where you can ask for caching

00:39:30.180 --> 00:39:33.170
and in many cases it will
actually work, and that is

00:39:33.170 --> 00:39:35.640
sort of on the Internet.

00:39:35.640 --> 00:39:37.600
And now you can--

00:39:37.600 --> 00:39:41.860
companies run proxy servers
and ISPs often also run

00:39:41.860 --> 00:39:43.400
Internet caches.

00:39:43.400 --> 00:39:47.650
So that popular content, this
is most effective if you're

00:39:47.650 --> 00:39:52.450
doing something really high
visibility, really high QPS.

00:39:52.450 --> 00:39:57.940
Like say you're planning some
kind of royal wedding, the

00:39:57.940 --> 00:40:03.150
nice thing here is even with a
small cache time out, like

00:40:03.150 --> 00:40:07.745
setting it to a minute or even
10 seconds sometimes, if there

00:40:07.745 --> 00:40:11.500
are enough users that are all
talking to the same cache--

00:40:11.500 --> 00:40:14.090
so if the ISP has positioned
their cache

00:40:14.090 --> 00:40:15.750
in the right spot--

00:40:15.750 --> 00:40:18.800
even such a short caching time
can be very effective.

00:40:18.800 --> 00:40:22.680
Because during that minute that
the data is valid in the

00:40:22.680 --> 00:40:26.220
cache, hundreds or thousands
of users might actually be

00:40:26.220 --> 00:40:31.190
hitting that cache instead of
going all the way to your app.

00:40:31.190 --> 00:40:35.880
So your app sees less traffic
and users are happier.

00:40:35.880 --> 00:40:38.050
Of course at this point you
have to put public in your

00:40:38.050 --> 00:40:40.460
cache control header, because
that cache is shared.

00:40:40.460 --> 00:40:44.280
So make sure that you don't
accidentally use this for per

00:40:44.280 --> 00:40:48.970
user, or otherwise
sensitive data.

00:40:48.970 --> 00:40:53.500
In some cases, Google
itself actually has

00:40:53.500 --> 00:40:55.330
a cache like this.

00:40:55.330 --> 00:40:59.130
Now I have to it to emphasize
that this is not

00:40:59.130 --> 00:41:01.530
a guaranteed service.

00:41:01.530 --> 00:41:05.210
And it only work for paid
AppEngine apps, which is good

00:41:05.210 --> 00:41:07.340
because if you're a free
AppEngine app you probably

00:41:07.340 --> 00:41:10.350
don't have enough traffic to
benefit from this style of

00:41:10.350 --> 00:41:12.730
caching anyway.

00:41:12.730 --> 00:41:18.060
But there is some caching in
Google's Front End servers.

00:41:18.060 --> 00:41:20.650
It's controlled exactly the same
way, just the same cache

00:41:20.650 --> 00:41:23.670
control public header, has the
same nice property there s

00:41:23.670 --> 00:41:26.720
fairly small time out
on the cache.

00:41:26.720 --> 00:41:29.030
It's still sufficient to
get the high benefits.

00:41:31.720 --> 00:41:34.510
The nice thing is if it's being
cached in Google, you

00:41:34.510 --> 00:41:38.390
will still see this traffic on
the dashboard in the AppEngine

00:41:38.390 --> 00:41:40.590
console, and you'll see
it in the logs.

00:41:40.590 --> 00:41:43.990
You can recognize it by
the 204 response code.

00:41:43.990 --> 00:41:47.460
So that's catching sort
of your application.

00:41:47.460 --> 00:41:50.580
Of course you can also put
caches in your application.

00:41:50.580 --> 00:41:54.130
And perhaps the simplest place
to put some caching is just in

00:41:54.130 --> 00:41:57.080
main memory of your
application.

00:41:57.080 --> 00:41:59.500
In Python you just store stuff
in global variables.

00:41:59.500 --> 00:42:00.860
Modular global variables.

00:42:00.860 --> 00:42:02.430
In Java you use static
variables

00:42:02.430 --> 00:42:04.400
for the same purpose.

00:42:04.400 --> 00:42:09.450
The upside is that it's really
fast because it works at

00:42:09.450 --> 00:42:12.850
memory speeds, like 100
nanoseconds or so.

00:42:12.850 --> 00:42:16.290
The downside is of course that
each instance, and we're

00:42:16.290 --> 00:42:18.470
assuming here that your
application's getting enough

00:42:18.470 --> 00:42:20.940
traffic that there are multiple
instances, has its

00:42:20.940 --> 00:42:22.890
own copy of the data.

00:42:22.890 --> 00:42:27.180
So if it takes a lot of effort
to compute that data, each

00:42:27.180 --> 00:42:31.190
instance recomputes that data
when it starts up, or when it

00:42:31.190 --> 00:42:32.730
first needs it.

00:42:32.730 --> 00:42:36.570
And you'd better only use this
for data that essentially

00:42:36.570 --> 00:42:41.540
never changes, because it
changes in one instance,

00:42:41.540 --> 00:42:45.340
there's no effective way for
that instance to communicate

00:42:45.340 --> 00:42:47.040
to the other instances
that what it is

00:42:47.040 --> 00:42:49.510
caching is now invalid.

00:42:49.510 --> 00:42:53.550
So that's a natural segue
to using Memcache.

00:42:53.550 --> 00:42:56.510
Which is a separate service,
that even though it's called

00:42:56.510 --> 00:43:00.060
Memcache, it doesn't use your
application's memory, it uses

00:43:00.060 --> 00:43:03.010
a separate server's memory.

00:43:03.010 --> 00:43:06.170
However because it just
stores the data in

00:43:06.170 --> 00:43:07.450
that server's memory--

00:43:07.450 --> 00:43:10.580
that server has like, oh I don't
know, 16GB of RAM or

00:43:10.580 --> 00:43:14.050
something, or 32 or 64--

00:43:14.050 --> 00:43:17.230
that's shared between different
AppEngine apps.

00:43:17.230 --> 00:43:21.050
But basically with like a
latency of one millisecond or

00:43:21.050 --> 00:43:25.790
a few milliseconds, Memcache
stores one consistent copy

00:43:25.790 --> 00:43:30.220
that can be accessed by every
instance of your app very

00:43:30.220 --> 00:43:31.750
effectively.

00:43:31.750 --> 00:43:34.760
Of course this is not
persistent, so don't use this

00:43:34.760 --> 00:43:37.340
as a sort of substitute
for the datastore.

00:43:37.340 --> 00:43:38.960
I also want to mention
something

00:43:38.960 --> 00:43:41.430
called dog pile effect.

00:43:41.430 --> 00:43:44.380
There are situations in
production where temporarily

00:43:44.380 --> 00:43:47.330
the memcache service
is unavailable.

00:43:47.330 --> 00:43:49.240
I mean there are other
situations where it just sort

00:43:49.240 --> 00:43:52.520
of expires data before you say
it should be expired, but

00:43:52.520 --> 00:43:54.770
sometimes it's unavailable.

00:43:54.770 --> 00:43:58.360
And then if all your instances
go to the data store, instead

00:43:58.360 --> 00:44:00.310
they might actually overwhelm
the data store.

00:44:00.310 --> 00:44:05.670
So there's some tricks with time
outs and keep trying not

00:44:05.670 --> 00:44:07.000
to overwhelm the data store.

00:44:07.000 --> 00:44:09.090
The best thing to do if you want
to learn more about that

00:44:09.090 --> 00:44:12.470
is just Google for
dog pile effect.

00:44:12.470 --> 00:44:15.810
So there's one final spot where
it actually sometimes

00:44:15.810 --> 00:44:19.220
make sense to cache stuff, and
this might be a bit sort of

00:44:19.220 --> 00:44:20.040
unintuitive.

00:44:20.040 --> 00:44:22.490
You can actually use the
data store as a cache.

00:44:22.490 --> 00:44:28.200
A nice example is Nick Johnson
Bloggart application which is

00:44:28.200 --> 00:44:30.980
a very simple blog
management app.

00:44:30.980 --> 00:44:34.670
And basically what he does is,
whenever you create a new blog

00:44:34.670 --> 00:44:38.210
page for when you update an
existing blog page, he

00:44:38.210 --> 00:44:42.120
pre-renders the sort of
resulting output, which is

00:44:42.120 --> 00:44:44.790
probably composed from the blog
entry and info about the

00:44:44.790 --> 00:44:47.440
author and info about
other blog entries.

00:44:47.440 --> 00:44:51.390
He sort of pregenerates all
that and stores the fully

00:44:51.390 --> 00:44:53.760
generated page in
the data store.

00:44:53.760 --> 00:44:57.340
So when a page is requested,
when users are viewing the

00:44:57.340 --> 00:45:01.760
blog pages, it's a single
datastore store request. It

00:45:01.760 --> 00:45:04.940
just fetches the pre-rendered
page and returns it without

00:45:04.940 --> 00:45:07.290
any sort of processing.

00:45:07.290 --> 00:45:10.890
And that way he claims that
they can do sort of 50

00:45:10.890 --> 00:45:16.670
millisecond latency which is a
very nice number I would say.

00:45:16.670 --> 00:45:24.600
So, we're going to cut the
Q&amp;A time a little short

00:45:24.600 --> 00:45:27.170
unfortunately, fortunately
there's a lunch break right

00:45:27.170 --> 00:45:28.610
after this.

00:45:28.610 --> 00:45:32.720
So there are certain things that
I've seen a lot of people

00:45:32.720 --> 00:45:37.560
make mistakes with, and in
general AppEngine team has

00:45:37.560 --> 00:45:42.740
some experience with what
typical user mistakes are.

00:45:42.740 --> 00:45:45.390
There's a bunch of programming
bugs that I actually discussed

00:45:45.390 --> 00:45:47.130
in my Appstats talk last year.

00:45:47.130 --> 00:45:51.970
So search for the Appstats
video from last year.

00:45:51.970 --> 00:45:54.310
I want to call up out two
specific things, and there are

00:45:54.310 --> 00:45:56.960
also blogs about these so I
don't have to explain in too

00:45:56.960 --> 00:45:57.960
much detail.

00:45:57.960 --> 00:46:01.450
The first one is entity
contention.

00:46:01.450 --> 00:46:05.170
When you're writing this same
entity or the same entity

00:46:05.170 --> 00:46:08.635
group if you're using
transactions, the data store

00:46:08.635 --> 00:46:13.420
is actually limited to, let's
say one write per second.

00:46:13.420 --> 00:46:16.480
I think the data store team will
say, well in practice, we

00:46:16.480 --> 00:46:19.360
sometimes support a little more
than one write per second

00:46:19.360 --> 00:46:20.690
but not a whole lot.

00:46:20.690 --> 00:46:26.470
So clearly if you're keeping
sort of a global counter of

00:46:26.470 --> 00:46:31.050
how many requests have been
made to your service.

00:46:31.050 --> 00:46:33.975
It's not a good idea to store
that counter in data store in

00:46:33.975 --> 00:46:38.550
a simplistic way, because you
would be limiting your QPS to

00:46:38.550 --> 00:46:43.270
about one request per second.

00:46:43.270 --> 00:46:46.030
This problem has been known for
a long time and there's a

00:46:46.030 --> 00:46:48.480
fairly straightforward
solution for it.

00:46:48.480 --> 00:46:51.710
Just search for shard
encounters.

00:46:51.710 --> 00:46:54.200
And if you actually do the
search right you'll also see

00:46:54.200 --> 00:46:57.873
some alternatives that actually
don't shard but but

00:46:57.873 --> 00:46:59.123
use various useful
approximations.

00:47:02.040 --> 00:47:04.760
Finally, another thing that can
happen in the data store,

00:47:04.760 --> 00:47:12.140
is if you append data with a
sequential increasing key, or

00:47:12.140 --> 00:47:17.410
actually also same problem if
you're using index property

00:47:17.410 --> 00:47:22.660
value, if you sort of append to
the end of the sequence of

00:47:22.660 --> 00:47:25.800
data, all that data goes
to the same thing

00:47:25.800 --> 00:47:27.100
called a tablet server.

00:47:27.100 --> 00:47:30.240
And if you want to understand
more about tablet servers, go

00:47:30.240 --> 00:47:33.270
to the talk called "More Nines
Please." I believe it's

00:47:33.270 --> 00:47:35.040
tomorrow morning.

00:47:35.040 --> 00:47:38.400
It explains a lot more about how
the AppEngine data store

00:47:38.400 --> 00:47:41.290
works and what kind of things
we've done in the last six

00:47:41.290 --> 00:47:43.920
months or so to make
it work better.

00:47:43.920 --> 00:47:49.740
So anyway, the problem with
sequentially appending data

00:47:49.740 --> 00:47:53.600
that has predictable keys like
this, is that all your data

00:47:53.600 --> 00:47:56.860
always goes to the last tablet
that is assigned to a

00:47:56.860 --> 00:48:01.510
particular entity
kind or table.

00:48:01.510 --> 00:48:05.050
And normally tablets when they
get too full or too busy they

00:48:05.050 --> 00:48:09.030
split up and they sort of
distribute the range of keys

00:48:09.030 --> 00:48:10.760
that they handle in two.

00:48:10.760 --> 00:48:14.480
But because of the append
behavior of the application,

00:48:14.480 --> 00:48:15.650
that doesn't actually help.

00:48:15.650 --> 00:48:18.850
And no matter how often you
split that last tablet, all

00:48:18.850 --> 00:48:23.400
the data will always go to the
last tablet in your list of

00:48:23.400 --> 00:48:25.390
tablets for your data.

00:48:25.390 --> 00:48:27.790
And now tablet servers
are also limited.

00:48:27.790 --> 00:48:33.370
I do not know exactly what the
right value for n is and it

00:48:33.370 --> 00:48:37.020
probably varies a bit on the
exact properties of the data,

00:48:37.020 --> 00:48:39.390
but let's say 10 or 20
writes per second.

00:48:39.390 --> 00:48:43.280
So again that would sort
of limit your QPS

00:48:43.280 --> 00:48:46.495
to a sadly low number.

00:48:46.495 --> 00:48:50.290
Or what it will do is you'll
get lots of data store time

00:48:50.290 --> 00:48:52.410
outs and application errors.

00:48:52.410 --> 00:48:56.440
So what you have to do is
randomize your data.

00:48:56.440 --> 00:48:58.820
And sometimes you don't have
to do something very

00:48:58.820 --> 00:48:59.620
sophisticated.

00:48:59.620 --> 00:49:04.820
Just using the user name or
something is often enough to

00:49:04.820 --> 00:49:06.570
avoid this hot tablet issue.

00:49:06.570 --> 00:49:08.980
If you want to learn more about
this, go to Ikai Lan's

00:49:08.980 --> 00:49:12.230
blog, he's one of our developer

00:49:12.230 --> 00:49:13.390
relationships people.

00:49:13.390 --> 00:49:17.890
He has a blog called Ikai Lan
Says and he has actually

00:49:17.890 --> 00:49:20.930
drawing skills that greatly
surpass mine, so he has a

00:49:20.930 --> 00:49:24.520
bunch of very entertaining
cartoons that explain how sort

00:49:24.520 --> 00:49:27.510
of the tablet splitting works
under good circumstances and

00:49:27.510 --> 00:49:30.460
under not so good
circumstances.

00:49:30.460 --> 00:49:35.540
So all I have left is a little
summary of what we've learned

00:49:35.540 --> 00:49:40.680
today, app engine scales that
can support many thousands of

00:49:40.680 --> 00:49:43.580
QPS, but to get to that
point you still

00:49:43.580 --> 00:49:45.000
have to do some work.

00:49:45.000 --> 00:49:47.940
You have to understand your
application and tune in and

00:49:47.940 --> 00:49:49.980
load test it.

00:49:49.980 --> 00:49:52.760
And one of the reasons is that
sort of new instances are

00:49:52.760 --> 00:49:57.280
created gradually, and sort of
in order to optimally create

00:49:57.280 --> 00:50:02.440
new instances, the request
latency, how long it takes to

00:50:02.440 --> 00:50:06.430
execute one request,
is the key factor.

00:50:06.430 --> 00:50:10.460
So treat your performance
to tuning as a science.

00:50:10.460 --> 00:50:13.740
We discussed some tools, some
techniques, a bunch of

00:50:13.740 --> 00:50:17.660
approaches for speeding up
applications, a whole bunch of

00:50:17.660 --> 00:50:21.150
different places where
you can do caching.

00:50:21.150 --> 00:50:22.840
And we discussed some
common bottlenecks.

00:50:22.840 --> 00:50:26.150
And well here are some links,
and now maybe we do have

00:50:26.150 --> 00:50:29.400
enough time, and if people want
to ask questions, please

00:50:29.400 --> 00:50:30.240
use the microphone.

00:50:30.240 --> 00:50:33.130
And we'll both be available
for answering questions.

00:50:36.946 --> 00:50:38.196
[APPLAUSE]

00:50:42.670 --> 00:50:43.980
JUSTIN HAUGH: Go ahead.

00:50:43.980 --> 00:50:45.850
MATT MASTRACCI: Hi I'm
Matt Mastracci

00:50:45.850 --> 00:50:47.100
from gri.pe, actually.

00:50:48.960 --> 00:50:51.920
We do a lot of continuous
integration, and I was just

00:50:51.920 --> 00:50:55.810
wondering how instance scaling
works when you push a new

00:50:55.810 --> 00:50:56.650
instance of your app?

00:50:56.650 --> 00:51:02.340
Because we push probably
20 to 30 times a day.

00:51:02.340 --> 00:51:05.280
I guess how are new instances
scaled around the

00:51:05.280 --> 00:51:06.492
times when we push?

00:51:06.492 --> 00:51:09.330
JUSTIN HAUGH: Is this
mike still on?

00:51:11.840 --> 00:51:13.240
We can hear you.

00:51:13.240 --> 00:51:17.480
JUSTIN HAUGH: When you deploy
a new version of your

00:51:17.480 --> 00:51:20.450
application or you do an update,
AppEngine remembers

00:51:20.450 --> 00:51:21.700
quite a lot about--

00:51:24.445 --> 00:51:26.400
okay, is the mike not working?

00:51:26.400 --> 00:51:30.470
So AppEngine remembers--
hello, is this working?

00:51:30.470 --> 00:51:33.950
So AppEngine remembers when
you deploy a new version,

00:51:33.950 --> 00:51:38.780
quite a lot about the previous
versions's scale and its

00:51:38.780 --> 00:51:42.640
performance characteristics,
and so generally speaking

00:51:42.640 --> 00:51:44.890
that's usually pretty
seamless.

00:51:44.890 --> 00:51:48.440
It doesn't suffer the same sort
of gradual ramp up effect

00:51:48.440 --> 00:51:51.422
that you would normally
expect.

00:51:51.422 --> 00:51:52.305
MATT MASTRACCI: Thank you.

00:51:52.305 --> 00:51:55.240
JUSTIN HAUGH: Thanks
for your quote.

00:51:55.240 --> 00:51:56.150
GUIDO VAN ROSSUM: Over there?

00:51:56.150 --> 00:51:58.010
ALBERT WONG: Hello, my
name's Albert Wong.

00:51:58.010 --> 00:52:00.742
I saw that Go was offered
as a language for

00:52:00.742 --> 00:52:02.590
AppEngine on the page.

00:52:02.590 --> 00:52:04.910
Would Go allow you to
have multithreaded

00:52:04.910 --> 00:52:07.710
applications also?

00:52:07.710 --> 00:52:09.130
JUSTIN HAUGH: That's a question
for the Go team, I

00:52:09.130 --> 00:52:11.335
think I'll probably
defer it to them.

00:52:11.335 --> 00:52:12.140
ALBERT WONG: Is there going
to be a talk on that?

00:52:12.140 --> 00:52:14.930
GUIDO VAN ROSSUM: Yeah there is
today, it was like at this

00:52:14.930 --> 00:52:17.590
very same time slot in
a different room.

00:52:17.590 --> 00:52:18.506
[LAUGHTER]

00:52:18.506 --> 00:52:19.422
Sorry.

00:52:19.422 --> 00:52:22.470
JUSTIN HAUGH: But if I could
just speak for them, I would I

00:52:22.470 --> 00:52:25.296
guess with high probability
the answer is yes.

00:52:25.296 --> 00:52:28.420
GUIDO VAN ROSSUM: Yeah, Go has
sort of built in concurrency

00:52:28.420 --> 00:52:31.700
support and they would be crazy
not to turn that on.

00:52:31.700 --> 00:52:34.870
It's new for us too, so we don't
have much experience

00:52:34.870 --> 00:52:35.360
with it yet.

00:52:35.360 --> 00:52:36.590
JUSTIN HAUGH: Question
over here?

00:52:36.590 --> 00:52:40.495
Uh yeah, you mentioned for
concurrency support on Python,

00:52:40.495 --> 00:52:42.950
is that something that's going
to come in the future?

00:52:42.950 --> 00:52:45.540
Beause I guess for someone
who's starting off with a

00:52:45.540 --> 00:52:49.015
brand new app, would you say in
terms of scalability would

00:52:49.015 --> 00:52:53.050
Java in fact give you more power
in the sense that it has

00:52:53.050 --> 00:52:55.320
that threading support
on AppEngine?

00:52:55.320 --> 00:52:58.710
JUSTIN HAUGH: I can speak
to that for just a sec.

00:52:58.710 --> 00:53:03.280
We do have some plans around
Python concurrency, they're

00:53:03.280 --> 00:53:05.620
not ready to be announced.

00:53:05.620 --> 00:53:10.590
But Python is quite scalable,
we see some apps that get a

00:53:10.590 --> 00:53:12.726
lot of traffic with Python.

00:53:12.726 --> 00:53:16.800
GUIDO VAN ROSSUM: I would say
that other things to do

00:53:16.800 --> 00:53:20.790
consider besides the concurrency
is that often a

00:53:20.790 --> 00:53:24.330
Python app uses less memory than
a similar Java app and it

00:53:24.330 --> 00:53:28.670
also starts up faster, so
instance creation latency is

00:53:28.670 --> 00:53:32.200
much lower for typical
Python apps.

00:53:32.200 --> 00:53:36.230
And again that is sort of data
that is valid today, I don't

00:53:36.230 --> 00:53:38.940
know how valid that will
be six months from now.

00:53:38.940 --> 00:53:40.810
JUSTIN HAUGH: One one other
thing I would just mention is

00:53:40.810 --> 00:53:44.540
that the load time of your
application really does matter

00:53:44.540 --> 00:53:45.200
quite a lot.

00:53:45.200 --> 00:53:49.320
And Java does have a little bit
of a longer start up time.

00:53:49.320 --> 00:53:50.650
Python starts very quickly.

00:53:54.610 --> 00:53:56.104
I'm Craig [UNINTELLIGIBLE].

00:53:56.104 --> 00:53:59.760
I guess one thing I'd mention on
the concurrency thing is no

00:53:59.760 --> 00:54:02.870
GS is single threaded, so it's
a pretty good at handling a

00:54:02.870 --> 00:54:04.978
lot of requests for server.

00:54:04.978 --> 00:54:07.840
My question is that you
mentioned a test instance.

00:54:07.840 --> 00:54:10.730
I was wondering is there a way
to create a test instance,

00:54:10.730 --> 00:54:13.860
with a test copy of data that
you can write to but not

00:54:13.860 --> 00:54:17.150
affect the production data?

00:54:17.150 --> 00:54:19.170
GUIDO VAN ROSSUM: I would
recommend just copying being

00:54:19.170 --> 00:54:22.800
your production data using the
bulk loader, so you just

00:54:22.800 --> 00:54:25.890
download a copy of your data
using the bulk loader, to your

00:54:25.890 --> 00:54:28.550
own workstation and then
you upload that to a

00:54:28.550 --> 00:54:30.150
different app instance.

00:54:30.150 --> 00:54:33.140
So you'd have to create two app
instances, and you can't

00:54:33.140 --> 00:54:36.050
do that with a test version, but
if you create a separate

00:54:36.050 --> 00:54:39.130
test app instance with just a
different app ID, you can just

00:54:39.130 --> 00:54:40.740
upload your production
data there.

00:54:40.740 --> 00:54:43.691
Great, thank you.

00:54:43.691 --> 00:54:45.580
Hi, I'm [UNINTELLIGIBLE].

00:54:45.580 --> 00:54:48.286
I was wondering, can you share
a little bit more detail of

00:54:48.286 --> 00:54:49.315
your formula?

00:54:49.315 --> 00:54:53.588
So are you using some kind of
dynamic programming approach

00:54:53.588 --> 00:54:58.260
or are you doing some kind
of prediction under QPS?

00:54:58.260 --> 00:55:01.160
And specifically what I want
to know is whether this

00:55:01.160 --> 00:55:02.320
formula, whether
it's a [? VP ?]

00:55:02.320 --> 00:55:03.100
or not?

00:55:03.100 --> 00:55:07.285
Does it apply per application
or for all applications?

00:55:07.285 --> 00:55:08.930
JUSTIN HAUGH: It's
per application.

00:55:08.930 --> 00:55:11.160
It's actually per version
of your application.

00:55:11.160 --> 00:55:13.460
So you can actually have
multiple versions live at the

00:55:13.460 --> 00:55:16.120
same time that are all receiving
traffic and each one

00:55:16.120 --> 00:55:18.740
is evaluated independently
for its performance

00:55:18.740 --> 00:55:20.971
characteristics.

00:55:20.971 --> 00:55:21.770
And then the last question.

00:55:21.770 --> 00:55:25.613
Did you consider having some
kind of an API where the

00:55:25.613 --> 00:55:30.110
application can give
you the hints?

00:55:30.110 --> 00:55:34.650
JUSTIN HAUGH: We do have some
thoughts around that.

00:55:34.650 --> 00:55:35.900
Nothing right now.

00:55:39.090 --> 00:55:42.930
Introducing some more controls
around certain scaling

00:55:42.930 --> 00:55:45.660
variables or the way the formula
works is something

00:55:45.660 --> 00:55:46.470
that we've been thinking
about.

00:55:46.470 --> 00:55:51.520
For instance, allowing,
for example, when your

00:55:51.520 --> 00:55:53.190
application--

00:55:53.190 --> 00:55:55.150
when an instance has been
sitting idle for a while when

00:55:55.150 --> 00:55:58.790
traffic comes down, the
determination of when to turn

00:55:58.790 --> 00:56:00.970
that instance down, that's
example something that we

00:56:00.970 --> 00:56:03.950
might give control over.

00:56:03.950 --> 00:56:05.600
Thank you.

00:56:05.600 --> 00:56:09.040
With regards to Memcache,
how are you handling key

00:56:09.040 --> 00:56:12.368
distribution and how durable
are the instances?

00:56:12.368 --> 00:56:15.214
Like if one goes down, and how
often do they go down?

00:56:15.214 --> 00:56:18.046
Do you lose all your keys
or is that distributed?

00:56:18.046 --> 00:56:21.140
GUIDO VAN ROSSUM: It's
a hash table.

00:56:21.140 --> 00:56:23.920
When the Memcache server
assigned to your app goes

00:56:23.920 --> 00:56:27.340
down, you do lose all your keys.
so that's why I put in

00:56:27.340 --> 00:56:28.670
that warning.

00:56:28.670 --> 00:56:33.400
There's a talk tomorrow about
life in Google AppEngine

00:56:33.400 --> 00:56:36.630
production I believe
where they go into

00:56:36.630 --> 00:56:38.000
a little more detail.

00:56:38.000 --> 00:56:42.610
But basically model you need
to think about is that each

00:56:42.610 --> 00:56:47.960
application typically uses one
specific Memcache server,

00:56:47.960 --> 00:56:50.730
which is shared with
other applications.

00:56:50.730 --> 00:56:53.840
And the Memcache server tries
to give each app its fair

00:56:53.840 --> 00:56:57.400
share of key and data space.

00:56:57.400 --> 00:57:01.320
It does somewhat penalize
applications that use their

00:57:01.320 --> 00:57:02.920
cache inefficiently.

00:57:02.920 --> 00:57:07.600
Like if you have very few cache
hits I think it might

00:57:07.600 --> 00:57:10.550
evict your data a little sooner
than when you're really

00:57:10.550 --> 00:57:13.508
good at cache hits.

00:57:13.508 --> 00:57:15.878
Thank you.

00:57:15.878 --> 00:57:21.562
My question is tangentially
related to scaling because I'm

00:57:21.562 --> 00:57:25.460
finding myself having to create
a lot of reports that

00:57:25.460 --> 00:57:30.960
are generated basically by my
app's own data but aren't from

00:57:30.960 --> 00:57:33.145
a browser-based client.

00:57:33.145 --> 00:57:38.540
And so I've read up on the
Mapper API and some other ways

00:57:38.540 --> 00:57:43.050
to kind of get around my own
mental limitations of big

00:57:43.050 --> 00:57:47.130
table versus SQL and some smart
ways to do that with

00:57:47.130 --> 00:57:48.510
tasks queues and whatnot.

00:57:48.510 --> 00:57:51.678
But when I look at the data and
the kinds of information

00:57:51.678 --> 00:57:55.035
I'm trying to pull out, it's
essentially a replication of

00:57:55.035 --> 00:57:57.555
the kinds of things I would get
if it were a browser-based

00:57:57.555 --> 00:58:01.940
client running on Google
Analytics.

00:58:01.940 --> 00:58:05.455
So my question to you is, are
there any plans or any thought

00:58:05.455 --> 00:58:08.655
or anything in the roadmap
to try to get server-side

00:58:08.655 --> 00:58:12.836
analytics in a nice
clean package?

00:58:12.836 --> 00:58:14.096
JUSTIN HAUGH: Yes.

00:58:14.096 --> 00:58:16.150
[LAUGHTER]

00:58:16.150 --> 00:58:17.240
Come to the next talk.

00:58:17.240 --> 00:58:19.678
Thank you.

00:58:19.678 --> 00:58:23.810
I'm curious if you guys
have any per user

00:58:23.810 --> 00:58:25.430
throttling in the system?

00:58:25.430 --> 00:58:30.036
I've been using BulkLoader to
try and download all of our

00:58:30.036 --> 00:58:30.458
app's data.

00:58:30.458 --> 00:58:34.728
We only have severals gigs of
data, but it still takes eight

00:58:34.728 --> 00:58:35.466
hours plus.

00:58:35.466 --> 00:58:38.740
So I'm wondering if I'm being
throttled on a per user basis

00:58:38.740 --> 00:58:42.044
or if it's just something
in the BulkLoader side.

00:58:44.800 --> 00:58:47.610
JUSTIN HAUGH: I don't have that
much familiarity with the

00:58:47.610 --> 00:58:51.260
BulkLoader but I do know that it
does have some performance

00:58:51.260 --> 00:58:51.870
bottlenecks.

00:58:51.870 --> 00:58:54.510
Because it's generally
considered an offline process

00:58:54.510 --> 00:58:59.190
so it's not something we're
focusing too much thought into

00:58:59.190 --> 00:59:02.010
making that very, very
fast all the time.

00:59:02.010 --> 00:59:04.380
But that is something we're
aware of and I think--

00:59:04.380 --> 00:59:05.750
Is there anything
on the server?

00:59:05.750 --> 00:59:08.390
Then there's nothing on the
server side that's actually

00:59:08.390 --> 00:59:10.480
intentionally throttling a user
to be like, ah, you're

00:59:10.480 --> 00:59:11.390
hitting the server too much?

00:59:11.390 --> 00:59:13.080
That's possibly malicious
and--

00:59:13.080 --> 00:59:14.170
JUSTIN HAUGH: I don't think so.

00:59:14.170 --> 00:59:15.440
Yeah.

00:59:15.440 --> 00:59:17.400
GUIDO VAN ROSSUM: One thing you
mentioned was that it's

00:59:17.400 --> 00:59:19.150
considered an offline process.

00:59:19.150 --> 00:59:25.350
So it may end up being treated
at a lower priority.

00:59:25.350 --> 00:59:30.640
It's also quite possible that
your bulk loading is throttled

00:59:30.640 --> 00:59:34.960
by some limitation on bandwidth
on the network

00:59:34.960 --> 00:59:38.080
between where you run the bulk
loader client and our

00:59:38.080 --> 00:59:39.240
infrastructure.

00:59:39.240 --> 00:59:41.870
Also the bulk loader actually
has a bunch of tuning

00:59:41.870 --> 00:59:45.350
parameters, if you haven't
already looked into those I

00:59:45.350 --> 00:59:47.662
would definitely
recommend that.

00:59:47.662 --> 00:59:51.040
You're welcome.

00:59:51.040 --> 00:59:52.910
JUSTIN HAUGH: Any
more questions?

00:59:52.910 --> 00:59:55.760
All right, thanks everybody, and
we'll be around afterward.

