WEBVTT
Kind: captions
Language: en

00:00:00.380 --> 00:00:03.440
So that's at least in a nut shell the reinforcement learning story.

00:00:03.440 --> 00:00:06.050
There's there's a lot of other topics and I think we are

00:00:06.050 --> 00:00:08.450
planning to get to some of them in a topics lesson a

00:00:08.450 --> 00:00:11.640
little bit later, but there's you know, courses and books worth of stuff

00:00:11.640 --> 00:00:15.040
to study in this area. Just like supervised learning as a whole.

00:00:15.040 --> 00:00:17.800
But, we're going to just kind of end it there with the idea

00:00:17.800 --> 00:00:21.190
that we now have a handle on how we can use learning

00:00:21.190 --> 00:00:25.460
to do decisions with delayed rewards. So, can you help us summarize what

00:00:25.460 --> 00:00:27.000
we what we learned in this lesson?

00:00:27.000 --> 00:00:29.370
&gt;&gt; Okay, sure. I think what did I learn here today,

00:00:29.370 --> 00:00:31.180
I think I learned a lot of things, some of which had

00:00:31.180 --> 00:00:35.530
to do with reinforcement learning Mainly that you can actually learn how

00:00:35.530 --> 00:00:38.670
to solve an MDP. I think that's actually a pretty big deal.

00:00:38.670 --> 00:00:41.300
&gt;&gt; Right. Meaning that we don't know T and R.

00:00:41.300 --> 00:00:42.460
&gt;&gt; Mm-hm.

00:00:42.460 --> 00:00:44.940
&gt;&gt; But we just have access to. The

00:00:44.940 --> 00:00:52.182
ability to interact with the environment and recieve transitions.

00:00:52.182 --> 00:00:53.820
&gt;&gt; And, that's actually that's actually pretty

00:00:53.820 --> 00:00:56.000
impressive. And, a very powerful thing. Because,

00:00:56.000 --> 00:00:59.670
we're often not, if we assume the world is an MDP, but we don't know

00:00:59.670 --> 00:01:03.670
TNR. If we don't have some way of learning in that we don't really have

00:01:03.670 --> 00:01:06.450
much we can do. And, you've showed me that their is something we can do.

00:01:07.610 --> 00:01:07.850
&gt;&gt; Cool.

00:01:07.850 --> 00:01:09.850
&gt;&gt; I think that's the biggest thing. We learned

00:01:09.850 --> 00:01:12.333
some specific things. In particular, we learned about Q-learning.

00:01:12.333 --> 00:01:15.100
&gt;&gt; Several kinds of Q-learning, but one is actually a real word.

00:01:15.100 --> 00:01:18.140
&gt;&gt; Yes [LAUGH]

00:01:18.140 --> 00:01:23.050
indeed they're all real, Michael. We learned about Q-learning. And I think the

00:01:23.050 --> 00:01:24.460
other most important thing that we learned

00:01:24.460 --> 00:01:27.360
about is the exploration versus exploitation trail.

00:01:27.360 --> 00:01:30.910
&gt;&gt; And with Q-learning, we learned a little bit about when it converges.

00:01:30.910 --> 00:01:31.355
&gt;&gt; Mm-hm.

00:01:31.355 --> 00:01:33.340
&gt;&gt; And that it's actually a family of

00:01:33.340 --> 00:01:36.940
algorithms. And different members of that family have different

00:01:36.940 --> 00:01:38.570
behaviors associated with them. Oh, there's one other

00:01:38.570 --> 00:01:40.430
thing I wanted to say on that topic, actually.

00:01:40.430 --> 00:01:43.260
&gt;&gt; Okay. Which is, that one way to

00:01:43.260 --> 00:01:47.330
achieve this exploration-exploitation balance, was to

00:01:47.330 --> 00:01:49.170
randomly choose actions. So to change

00:01:49.170 --> 00:01:52.590
the we're doing action selection. But there's another one too, which is that

00:01:52.590 --> 00:01:58.540
we can actually by manipulating the initialization of the Q function. We can

00:01:58.540 --> 00:02:02.600
actually get another kind of exploration, can you see how that might work?

00:02:02.600 --> 00:02:03.920
&gt;&gt; Oh, I know what you do. If you could, if

00:02:03.920 --> 00:02:08.340
you set, say the Q values to all be the highest

00:02:08.340 --> 00:02:09.650
possible value they could be.

00:02:09.650 --> 00:02:15.640
&gt;&gt; Great, so if we initialized the Q hat to awesome values, then what the

00:02:15.640 --> 00:02:17.170
Q lettering algorithm would do, even with

00:02:17.170 --> 00:02:19.380
greedy exploration, what it will do is it

00:02:19.380 --> 00:02:22.730
will try things that it hasn't tried very much, and it still thinks are awesome.

00:02:22.730 --> 00:02:23.890
And little by little, it gets a more

00:02:23.890 --> 00:02:26.850
realistic sense of how the environment works. And.

00:02:26.850 --> 00:02:27.810
&gt;&gt; So it's very optimistic?

00:02:27.810 --> 00:02:33.410
&gt;&gt; That's right, exactly and it's referred to often as optimism in

00:02:33.410 --> 00:02:37.200
the face of uncertainty and it's a similar kind of idea that's

00:02:37.200 --> 00:02:41.330
used in algorithms like, A*, if you're familiar with search algorithms in AI.

00:02:41.330 --> 00:02:42.940
&gt;&gt; Oh yes, I remember those.

00:02:42.940 --> 00:02:45.200
&gt;&gt; But this is, this is a really powerful idea and it's

00:02:45.200 --> 00:02:46.820
used in, in reinforcement learning and

00:02:46.820 --> 00:02:50.400
banded algorithms and planning and And search.

00:02:50.400 --> 00:02:52.000
&gt;&gt; Okay, and that makes sense because if

00:02:52.000 --> 00:02:54.630
everything is awesome. Then your true key value

00:02:54.630 --> 00:02:56.410
can only go down if awesome is bigger

00:02:56.410 --> 00:02:58.510
than the biggest Q value you could ever have.

00:02:58.510 --> 00:03:01.970
And so that means you're going to look at every single action. And as you learn

00:03:01.970 --> 00:03:02.920
more about them, then you will just

00:03:02.920 --> 00:03:04.960
get more depressed about them. And that's good.

00:03:04.960 --> 00:03:08.620
&gt;&gt; [LAUGH] Yes the world slowly beats you

00:03:08.620 --> 00:03:12.470
down. [LAUGH] So is that it? Is that all

00:03:12.470 --> 00:03:14.430
we really talked about? I guess that's about

00:03:14.430 --> 00:03:16.380
right. We talked about what a que function was.

00:03:16.380 --> 00:03:17.830
&gt;&gt; Right.

00:03:17.830 --> 00:03:20.580
&gt;&gt; And how that kind of binds everything together

00:03:20.580 --> 00:03:23.940
and we talked about different approaches including policy search and

00:03:23.940 --> 00:03:25.090
model base reinforcement learning.

00:03:25.090 --> 00:03:28.190
&gt;&gt; Yeah that was very nice. We tied it all back into planning.

00:03:28.190 --> 00:03:30.660
&gt;&gt; So one, one thing we didn't talk about

00:03:30.660 --> 00:03:34.550
is connecting to function approximation, and the issues in

00:03:34.550 --> 00:03:36.290
machine learning that are really important things things like

00:03:36.290 --> 00:03:39.450
over fitting. They come up in the reinforcement learning setting,

00:03:39.450 --> 00:03:41.330
but not in this simplified setting that were looking

00:03:41.330 --> 00:03:43.480
at here where we learn a separate value. For

00:03:43.480 --> 00:03:45.570
each state action pair, we're going to have to

00:03:45.570 --> 00:03:49.190
start generalizing to see the importance of that. And that's,

00:03:49.190 --> 00:03:50.360
we're going to do in a later lesson.

00:03:50.360 --> 00:03:52.360
&gt;&gt; Okay. I like it. And we also learned

00:03:52.360 --> 00:03:55.550
a bunch of things about letters. Like exploration versus exploitation.

00:03:55.550 --> 00:03:59.060
&gt;&gt; In fact, we know enough that we can now get an A in letters.

00:03:59.060 --> 00:04:01.610
&gt;&gt; [LAUGH]

00:04:01.610 --> 00:04:05.450
I like it. Okay. Well, I think we learned a lot, Michael.

00:04:05.450 --> 00:04:09.740
&gt;&gt; [LAUGH] Okay. Well, good. Well, thanks. It's very nice

00:04:09.740 --> 00:04:11.350
to get a chance to talk to you about this stuff.

00:04:11.350 --> 00:04:14.390
&gt;&gt; Cool. And so I guess. What are we going to talk about next.

00:04:14.390 --> 00:04:17.579
&gt;&gt; Well Whatever it says on the syllabus.

00:04:17.579 --> 00:04:18.829
&gt;&gt; I think it's game theory.

00:04:18.829 --> 00:04:19.870
&gt;&gt; That's pretty cool.

00:04:19.870 --> 00:04:22.370
&gt;&gt; Oh. I see why we're going to do that.

00:04:22.370 --> 00:04:25.400
Because all we've been talking about the world as if

00:04:25.400 --> 00:04:28.340
there were just one agent and nobody else. And now

00:04:28.340 --> 00:04:31.090
we're going to see what happens. When there are other people.

00:04:31.090 --> 00:04:34.070
&gt;&gt; Right. Other people show up at the party, next time.

00:04:34.070 --> 00:04:35.660
&gt;&gt; On, Machine Learning.

