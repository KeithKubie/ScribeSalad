WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.311
[MUSIC PLAYING]

00:00:06.160 --> 00:00:09.670
JONATHAN FRANZEN:
Hello, Google Boston.

00:00:09.670 --> 00:00:11.710
It's nice to be here.

00:00:11.710 --> 00:00:16.090
Yes, I am in town
promoting my new book,

00:00:16.090 --> 00:00:18.970
"The End of the End of the
World," a very scary seagull

00:00:18.970 --> 00:00:19.560
on the cover.

00:00:22.310 --> 00:00:28.000
And this is a weird one, the
belly of the beast for me.

00:00:28.000 --> 00:00:37.420
I'm a long-credentialed tech
critic, going back, certainly

00:00:37.420 --> 00:00:39.520
in print to 1995.

00:00:39.520 --> 00:00:42.640
I've been hard on
Silicon Valley,

00:00:42.640 --> 00:00:48.250
hard on some of the
dopey utopianism,

00:00:48.250 --> 00:00:51.940
trying to call attention
to the intimate connection

00:00:51.940 --> 00:00:53.890
from the beginning of
the commercial internet

00:00:53.890 --> 00:00:55.540
between consumerism and tech.

00:01:01.336 --> 00:01:03.460
And there is some stuff
about tech in the new book,

00:01:03.460 --> 00:01:05.876
not so much in this book as
in some of the previous books,

00:01:05.876 --> 00:01:11.100
but I'm going to
mostly focus on some

00:01:11.100 --> 00:01:15.150
of the other preoccupations
in these new essays.

00:01:15.150 --> 00:01:19.510
I'm going to read a
piece of one of them.

00:01:19.510 --> 00:01:22.560
Let me first say that
it's a bit of a--

00:01:22.560 --> 00:01:26.850
I was thinking on the way over
here, there are the big five,

00:01:26.850 --> 00:01:30.030
and you guys are part
of one of the big five.

00:01:30.030 --> 00:01:33.510
And it's been interesting
to trace almost

00:01:33.510 --> 00:01:38.430
as a parlor game, which of
the five at any moment in time

00:01:38.430 --> 00:01:39.840
has seemed to me the most evil.

00:01:43.050 --> 00:01:45.330
For a long time, because
there weren't five,

00:01:45.330 --> 00:01:50.190
there was really only
Microsoft, there was no contest.

00:01:50.190 --> 00:01:56.340
I was a DOS user and then
a early Windows hater,

00:01:56.340 --> 00:02:02.190
and there was much to dislike
about Bill Gates' company.

00:02:02.190 --> 00:02:06.500
But then others quickly moved
in, beginning with Apple.

00:02:06.500 --> 00:02:09.080
Apple was, for a
long time, easily

00:02:09.080 --> 00:02:13.790
at the top of my most
hated list because

00:02:13.790 --> 00:02:21.260
of its not so veiled contempt
for its passionate fans.

00:02:21.260 --> 00:02:25.610
To me the absence of a
delete key on the keyboard,

00:02:25.610 --> 00:02:32.390
on the famous Apple keyboard
is one striking example,

00:02:32.390 --> 00:02:37.460
and I will not say more, but if
you're an actual writer having

00:02:37.460 --> 00:02:41.720
a backspace key and a delete
key, it's a plus, believe me.

00:02:41.720 --> 00:02:45.040
You don't need to
reposition the cursor.

00:02:45.040 --> 00:02:48.650
You just hit Delete.

00:02:48.650 --> 00:02:51.376
Sucks that character
right up from the right.

00:02:51.376 --> 00:02:53.230
It's nice.

00:02:53.230 --> 00:02:55.600
You'd think a large
company in the business

00:02:55.600 --> 00:02:59.730
of helping academics and
writers might appreciate that,

00:02:59.730 --> 00:03:03.510
but apparently it went
straight to Steve Jobs,

00:03:03.510 --> 00:03:07.180
and he said no, the keyboard
is prettier without that.

00:03:07.180 --> 00:03:08.770
So that was hateable.

00:03:08.770 --> 00:03:12.150
And then there was that
wonderful series of ads,

00:03:12.150 --> 00:03:14.370
and I actually came
to like Microsoft

00:03:14.370 --> 00:03:16.380
because it wasn't
Apple, and Microsoft

00:03:16.380 --> 00:03:18.420
was embodied by the
John Hodgman character

00:03:18.420 --> 00:03:20.760
in the wonderful series of ads.

00:03:20.760 --> 00:03:23.850
John Hodgman, totally
lovable, embodying everything

00:03:23.850 --> 00:03:26.970
that had become weirdly lovable
about the formerly hated

00:03:26.970 --> 00:03:29.280
Microsoft.

00:03:29.280 --> 00:03:31.360
And what's his face?

00:03:31.360 --> 00:03:31.860
Justin?

00:03:31.860 --> 00:03:32.420
AUDIENCE: Justin Long.

00:03:32.420 --> 00:03:34.020
JONATHAN FRANZEN:
Justin Long, yes.

00:03:34.020 --> 00:03:36.011
I'm sure he's a nice guy, but--

00:03:36.011 --> 00:03:38.370
[LAUGHTER]

00:03:38.370 --> 00:03:40.230
So then, yeah, and I still--

00:03:40.230 --> 00:03:47.280
my little tag at the bottom
of my Android Gmail account

00:03:47.280 --> 00:03:48.810
showing that was
sent from a device

00:03:48.810 --> 00:03:51.990
is still sent from
a non-Apple device,

00:03:51.990 --> 00:03:54.210
because it does seem
very important always

00:03:54.210 --> 00:03:55.640
to be stressing that.

00:03:55.640 --> 00:03:58.840
But then Amazon, Facebook.

00:03:58.840 --> 00:04:02.310
Amazon, one of the four
horsemen of the apocalypse

00:04:02.310 --> 00:04:08.400
as far as the book industry
and bookstores are concerned.

00:04:08.400 --> 00:04:15.570
Of course, it's gone beyond that
to become a textbook monopoly,

00:04:15.570 --> 00:04:19.290
behaving in textbook
monopoly fashion,

00:04:19.290 --> 00:04:22.140
and I think it really
occupied number

00:04:22.140 --> 00:04:24.660
one on the most hated
list for many years,

00:04:24.660 --> 00:04:27.825
and it still contents
for that spot.

00:04:30.445 --> 00:04:35.869
But Facebook,
Facebook, Facebook.

00:04:35.869 --> 00:04:37.660
I'm not mentioning
Twitter because it's not

00:04:37.660 --> 00:04:40.120
one of the big five and
because it's not even clear

00:04:40.120 --> 00:04:41.380
that it's worth hating.

00:04:41.380 --> 00:04:46.060
It seems to me more like one
of those RNA viruses that

00:04:46.060 --> 00:04:48.760
does unspeakable harm, but
people are not even sure

00:04:48.760 --> 00:04:49.870
if it's a life form.

00:04:49.870 --> 00:04:54.531
[LAUGHTER]

00:04:54.531 --> 00:04:57.070
Whereas Facebook is
clearly a life form,

00:04:57.070 --> 00:05:01.780
also scarcely behind Amazon
in its monopolistic intentions

00:05:01.780 --> 00:05:04.780
and its monopolistic behavior.

00:05:04.780 --> 00:05:09.700
And what seems really
particularly evil about it,

00:05:09.700 --> 00:05:11.810
I mean, Bezos knows
what he's doing.

00:05:11.810 --> 00:05:14.700
He knows it's bad, but
he kind of owns up to it.

00:05:14.700 --> 00:05:16.480
It's not like he's
hiding the fact

00:05:16.480 --> 00:05:20.650
that he wants to control all
consumer commerce everywhere

00:05:20.650 --> 00:05:22.930
in the world, whereas
Facebook still

00:05:22.930 --> 00:05:24.700
wraps it up in
these gauzy phrases

00:05:24.700 --> 00:05:28.360
and seems just weirdly
immune to all of the harm

00:05:28.360 --> 00:05:31.520
that it's causing with its
incredibly addictive product.

00:05:31.520 --> 00:05:34.180
Its incredibly addictive,
surveillant, civil

00:05:34.180 --> 00:05:38.090
discourse-destroying product.

00:05:38.090 --> 00:05:41.770
So where is Google
in all of this,

00:05:41.770 --> 00:05:44.530
I ask myself, riding the
old red line over here

00:05:44.530 --> 00:05:46.330
from Harvard Square.

00:05:46.330 --> 00:05:48.910
I lived in Somerville
for five years.

00:05:48.910 --> 00:05:53.230
I wrote my first
novel while I was--

00:05:53.230 --> 00:05:55.750
on the strength of
a research position

00:05:55.750 --> 00:06:00.100
I had at Harvard in the
seismology lab there.

00:06:00.100 --> 00:06:01.910
I lived in Somerville
for five years

00:06:01.910 --> 00:06:04.300
and I am not sure I'd
been on a red line train

00:06:04.300 --> 00:06:07.250
since I left in 1987.

00:06:07.250 --> 00:06:08.090
So it was weird.

00:06:08.090 --> 00:06:10.202
It seemed completely unchanged.

00:06:10.202 --> 00:06:11.890
[LAUGHTER]

00:06:11.890 --> 00:06:15.220
The buskers at the Harvard
Street Station were identical,

00:06:15.220 --> 00:06:17.780
the trains looked identical.

00:06:17.780 --> 00:06:20.680
It all seems sort of sadly
small, compared to New York.

00:06:20.680 --> 00:06:23.050
That was identical, too.

00:06:23.050 --> 00:06:24.280
Sweetly identical now.

00:06:24.280 --> 00:06:29.470
At the time, it was sad,
even infuriatingly small.

00:06:29.470 --> 00:06:31.540
That's why I moved to New York.

00:06:31.540 --> 00:06:35.700
Anyway, I was
thinking why don't I

00:06:35.700 --> 00:06:39.270
hate Google as much as I hate
Facebook and Amazon and even

00:06:39.270 --> 00:06:40.957
Apple?

00:06:40.957 --> 00:06:42.540
A large part of it
is because I really

00:06:42.540 --> 00:06:43.740
like some of your products.

00:06:43.740 --> 00:06:46.290
I am on Gmail and it's good.

00:06:46.290 --> 00:06:48.300
I wish someone,
maybe afterwards,

00:06:48.300 --> 00:06:54.840
can explain to me how to get,
on my Android device my emails

00:06:54.840 --> 00:07:00.120
not to display in thread form,
so that when you delete one

00:07:00.120 --> 00:07:02.670
you don't delete
the whole thread.

00:07:02.670 --> 00:07:06.150
Maybe someone has some expertise
can help me configure that.

00:07:06.150 --> 00:07:09.930
I searched and searched all
the settings, can't find it.

00:07:09.930 --> 00:07:12.690
Anyway, but apart from
that, great product.

00:07:12.690 --> 00:07:16.170
I use your search engine.

00:07:16.170 --> 00:07:19.710
I use Google Maps,
that's all very good.

00:07:19.710 --> 00:07:24.810
One could argue that Google's
position on copyright,

00:07:24.810 --> 00:07:28.050
for a writer, ought
to give one pause,

00:07:28.050 --> 00:07:32.292
but I kind of outsource that
outrage to the Authors Guild

00:07:32.292 --> 00:07:33.750
that I'm a proud
member of, and let

00:07:33.750 --> 00:07:38.610
it file the complaints
and even lawsuits

00:07:38.610 --> 00:07:44.970
to try to protect poor writers
from the ever-widening net

00:07:44.970 --> 00:07:48.810
sweeping up copyrighted
material and offering it

00:07:48.810 --> 00:07:50.490
without compensation
to those writers.

00:07:50.490 --> 00:07:52.230
So that's, you know,
it's a small thing.

00:07:52.230 --> 00:07:52.830
But then--

00:07:52.830 --> 00:07:53.360
AUDIENCE: Sorry to interrupt.

00:07:53.360 --> 00:07:54.464
That's really not fair.

00:07:54.464 --> 00:07:55.880
That's not an
accurate character--

00:07:55.880 --> 00:07:57.088
JONATHAN FRANZEN: OK, I know.

00:07:57.088 --> 00:08:05.610
It's-- I'm here to give you
the view how you are seen

00:08:05.610 --> 00:08:07.290
in the writer's community.

00:08:07.290 --> 00:08:12.840
It's true, that's-- and
indeed, this is in the context

00:08:12.840 --> 00:08:16.320
of saying I don't hate
Google like I hate the others

00:08:16.320 --> 00:08:21.450
because I find all sorts of
stuff with the Google search

00:08:21.450 --> 00:08:24.450
engine, books that I would
otherwise not be able

00:08:24.450 --> 00:08:28.740
to access, and it'll take me to
the keyword on page 267 of some

00:08:28.740 --> 00:08:33.495
obscure text that was published
in 1953 or 1927, fantastic.

00:08:33.495 --> 00:08:35.730
So I think we can
actually come to

00:08:35.730 --> 00:08:37.080
some reasonable accommodation.

00:08:37.080 --> 00:08:38.610
I do know that the
Authors Guild has

00:08:38.610 --> 00:08:41.669
had some issues with Google.

00:08:41.669 --> 00:08:45.414
But it's true, I
did, in my not so

00:08:45.414 --> 00:08:46.830
accustomed to
public speaking way,

00:08:46.830 --> 00:08:50.140
perhaps overstate the case.

00:08:50.140 --> 00:08:51.660
Anyway, I was
thinking yesterday,

00:08:51.660 --> 00:08:55.530
as I came in on
the plane, I travel

00:08:55.530 --> 00:08:57.750
with stacks of New Yorkers
that I don't get around

00:08:57.750 --> 00:09:00.450
to reading at home because
I can throw them away

00:09:00.450 --> 00:09:03.000
and my bag gets lighter
and lighter as I travel.

00:09:03.000 --> 00:09:05.730
And I happened to be
reading about Google

00:09:05.730 --> 00:09:10.470
as I was landing in Boston
last night, the Levandowski,

00:09:10.470 --> 00:09:13.920
that case, the Waymo thing.

00:09:13.920 --> 00:09:19.930
And boy, does he sound
like not a great guy.

00:09:19.930 --> 00:09:27.440
And it didn't affect the
standings at all, that piece.

00:09:27.440 --> 00:09:30.575
It was a little hard
on Google and Waymo.

00:09:30.575 --> 00:09:33.010
But I was thinking
about the whole notion

00:09:33.010 --> 00:09:34.580
of the self-driving car.

00:09:37.990 --> 00:09:40.150
The piece ends with
this wonderful quote

00:09:40.150 --> 00:09:49.230
from Levandowski, talking about
how uninteresting history is,

00:09:49.230 --> 00:09:51.700
how the only thing that
matters, at least to someone

00:09:51.700 --> 00:09:54.250
in tech like him, is the future.

00:09:54.250 --> 00:09:58.090
And I was thinking about
the self-driving car

00:09:58.090 --> 00:10:01.921
thing, which was always
bugged me a little bit.

00:10:01.921 --> 00:10:02.420
You

00:10:02.420 --> 00:10:08.720
Make a list of problems we
have as a country, as a world,

00:10:08.720 --> 00:10:17.390
and I struggle to imagine
that anyone, on their top 25

00:10:17.390 --> 00:10:24.350
problems to be solved list has,
need to sit behind the wheel

00:10:24.350 --> 00:10:26.580
while I listen to
podcasts and traffic,

00:10:26.580 --> 00:10:28.400
rather than sitting
in the passenger seat,

00:10:28.400 --> 00:10:30.050
listening to
podcasts and traffic.

00:10:32.760 --> 00:10:37.880
It has always seemed to me that
perhaps the self-driving car

00:10:37.880 --> 00:10:41.210
initiative has underestimated
the extent to which people

00:10:41.210 --> 00:10:43.880
actually enjoy driving.

00:10:43.880 --> 00:10:45.440
For Americans,
particularly, it's

00:10:45.440 --> 00:10:47.810
an expression of
personal freedom.

00:10:47.810 --> 00:10:51.080
In some ways, it's the,
for non-gun owners,

00:10:51.080 --> 00:10:53.090
it's the expression
of personal freedom.

00:10:56.990 --> 00:10:59.360
And then as I was taking a cab--

00:10:59.360 --> 00:11:03.350
an old-fashioned cab, not an
Uber, from Logan last night--

00:11:03.350 --> 00:11:06.030
because I think of
the poor cabbies

00:11:06.030 --> 00:11:09.530
and like to support them
even though it would probably

00:11:09.530 --> 00:11:11.285
have been cheaper to
take Uber or Lyft--

00:11:14.210 --> 00:11:18.500
I was thinking about a problem
that's on everyone's list

00:11:18.500 --> 00:11:23.210
that a lot of the tech oligarchs
seem suddenly very, very

00:11:23.210 --> 00:11:26.180
preoccupied with, and
that's the problem of dying.

00:11:29.030 --> 00:11:32.120
I think we can all agree that
that remains a big problem

00:11:32.120 --> 00:11:34.160
for any human being.

00:11:34.160 --> 00:11:36.690
You're going to die.

00:11:36.690 --> 00:11:41.396
And it's a measure of the--

00:11:41.396 --> 00:11:45.950
you want to know
why people like me

00:11:45.950 --> 00:11:50.060
hold Silicon Valley
in such low esteem,

00:11:50.060 --> 00:11:54.980
and why I personally have
watched all of the show

00:11:54.980 --> 00:11:58.680
"Silicon Valley" at least twice,
every episode at least twice,

00:11:58.680 --> 00:12:00.165
if not three times
or four times,

00:12:00.165 --> 00:12:02.540
and think it's one of the most
profound cultural products

00:12:02.540 --> 00:12:05.030
in the last eight years--

00:12:05.030 --> 00:12:08.510
it has to do with this
notion that tech is going

00:12:08.510 --> 00:12:11.404
to solve the problem of death.

00:12:11.404 --> 00:12:19.840
And As we were whisking
in, wonderful now

00:12:19.840 --> 00:12:24.640
to live in 2018
thanks to the Big Dig,

00:12:24.640 --> 00:12:26.515
how quickly you can
get to your hotel--

00:12:29.045 --> 00:12:33.670
it used to be an adventure to
get to Logan when I lived here,

00:12:33.670 --> 00:12:34.920
before many of you were born--

00:12:38.140 --> 00:12:43.800
I was imagining
being 635 years old

00:12:43.800 --> 00:12:50.850
and wondering what I was going
to do this week and thinking,

00:12:50.850 --> 00:12:56.490
maybe I will re-watch all
7,000 episodes of the Simpsons.

00:12:56.490 --> 00:12:58.800
[LAUGHTER]

00:12:58.800 --> 00:13:02.280
I haven't done that since I did
it as a special birthday treat

00:13:02.280 --> 00:13:07.170
to myself the year I turned 500.

00:13:07.170 --> 00:13:10.720
And it had been like a
couple of hundred years

00:13:10.720 --> 00:13:12.970
since I'd watched even one
episode of The Simpsons

00:13:12.970 --> 00:13:13.480
before then.

00:13:13.480 --> 00:13:15.240
So it's like, I'm fresh.

00:13:15.240 --> 00:13:19.670
It's been 135 years since I
watched all 7,000 episodes.

00:13:19.670 --> 00:13:23.240
And I've got plenty of time
because I'm never going to die.

00:13:23.240 --> 00:13:25.600
And also my options
for what I'm going

00:13:25.600 --> 00:13:28.720
to do with myself otherwise
are rather limited,

00:13:28.720 --> 00:13:32.980
because of course
I can't go outside.

00:13:32.980 --> 00:13:35.680
Because what if some
freakish accident happened

00:13:35.680 --> 00:13:40.210
and an enormous chunk of masonry
or a large tree fell on me

00:13:40.210 --> 00:13:46.450
and obliterated 635
years' worth of living,

00:13:46.450 --> 00:13:51.170
so damaged me that I couldn't
be put back together.

00:13:51.170 --> 00:13:55.010
So basically I have to
wear these padded clothes

00:13:55.010 --> 00:13:57.530
and this big spongy
helmet for fear

00:13:57.530 --> 00:14:00.470
that something might happen to
me, because now I'm 635 years

00:14:00.470 --> 00:14:05.210
and I can live forever,
thanks to technology,

00:14:05.210 --> 00:14:06.920
but I have to be
very, very careful.

00:14:06.920 --> 00:14:11.900
And also, for the first
480 years of my life

00:14:11.900 --> 00:14:14.120
there was the risk that
when I would go outdoors,

00:14:14.120 --> 00:14:16.270
I might run into a
mortal person, somebody

00:14:16.270 --> 00:14:19.100
who didn't have as
much money as I did

00:14:19.100 --> 00:14:22.550
and who was understandably
resentful that some people who

00:14:22.550 --> 00:14:26.720
had a lot of money and could
afford it could be immortal

00:14:26.720 --> 00:14:28.490
but they were not.

00:14:28.490 --> 00:14:30.290
And that had become
such a problem,

00:14:30.290 --> 00:14:33.530
with the rage of mortal people
against the wealthy immortal

00:14:33.530 --> 00:14:37.240
people that finally
the decision had sadly

00:14:37.240 --> 00:14:40.920
been taken to exterminate
all mortal people.

00:14:40.920 --> 00:14:43.930
So that left just us immortals,
but we're still pretty bunkered

00:14:43.930 --> 00:14:48.080
down because of the
risk of physical harm.

00:14:48.080 --> 00:14:50.850
And what do you do?

00:14:50.850 --> 00:14:55.990
You get married 30 times before
you're even 600 years old.

00:14:55.990 --> 00:14:59.040
You keep your eye on that
cute little 180-year-old

00:14:59.040 --> 00:15:04.110
down the hall, one of the
last people ever born.

00:15:04.110 --> 00:15:08.121
Very, very attractive to
many of us older people.

00:15:08.121 --> 00:15:09.870
And you know that in
the fullness of time,

00:15:09.870 --> 00:15:11.880
because you'll live
forever, you'll

00:15:11.880 --> 00:15:15.480
be able to marry that cute
little person down the hall.

00:15:15.480 --> 00:15:17.320
And then divorce that person.

00:15:22.650 --> 00:15:25.410
I've gone on too long
about the silly scenario.

00:15:25.410 --> 00:15:28.680
I'm not a sci-fi writer, and
I'm sure sci-fi writers have

00:15:28.680 --> 00:15:31.350
roasted the whole concept
much more than I. I'm not

00:15:31.350 --> 00:15:33.270
considering the possibility
that our minds can

00:15:33.270 --> 00:15:37.335
be downloaded and exist in
eternal form in digital form.

00:15:40.280 --> 00:15:43.430
I could go into that, if I
had prepared more than riding

00:15:43.430 --> 00:15:54.220
the subway over here, the
inextricability of identity

00:15:54.220 --> 00:15:55.395
from bodily self.

00:15:57.980 --> 00:16:02.690
It's merely to say that the
whole idea of living forever

00:16:02.690 --> 00:16:08.210
doesn't pass the most
rudimentary moral examination.

00:16:08.210 --> 00:16:15.500
And you have to think that
the tech billionaire who's

00:16:15.500 --> 00:16:18.800
deciding to put $100
or $200 million dollars

00:16:18.800 --> 00:16:24.380
into investigating the
possibility of living forever

00:16:24.380 --> 00:16:32.650
is either unbelievably
morally stupid, or worse,

00:16:32.650 --> 00:16:36.010
like, the worst kind
of asshole, imagining

00:16:36.010 --> 00:16:40.700
that, well, other people have
to die but I don't have to die.

00:16:40.700 --> 00:16:43.950
And that's kind of the
definition of an asshole.

00:16:43.950 --> 00:16:45.810
So there's a gulf--

00:16:45.810 --> 00:16:47.410
I've taken an extreme example--

00:16:47.410 --> 00:16:50.820
but there is a gulf
between the way,

00:16:50.820 --> 00:16:55.410
say, literature, considers
the problem of death

00:16:55.410 --> 00:17:01.950
and the way problem-solvers
consider the problem of death.

00:17:01.950 --> 00:17:04.120
Literature doesn't think
it's a solvable problem.

00:17:06.810 --> 00:17:08.730
Much changes when
you acknowledge

00:17:08.730 --> 00:17:11.520
that certain problems
aren't solvable, that people

00:17:11.520 --> 00:17:13.230
are actually not very nice.

00:17:13.230 --> 00:17:15.810
That's the great mistake
that the visionaries

00:17:15.810 --> 00:17:17.250
made in the 90s.

00:17:17.250 --> 00:17:23.220
They were all nice guys and most
of the people in the early days

00:17:23.220 --> 00:17:25.740
of Facebook, they were all
well-educated, clean, happy

00:17:25.740 --> 00:17:27.780
white people with
liberal politics.

00:17:27.780 --> 00:17:29.340
What could the problem be?

00:17:29.340 --> 00:17:32.610
Well, the problem is that
people aren't very good.

00:17:32.610 --> 00:17:37.830
People are at least as
bad as they are good.

00:17:37.830 --> 00:17:40.560
This is not, actually, a
solvable problem anymore

00:17:40.560 --> 00:17:41.970
than death is.

00:17:41.970 --> 00:17:47.820
So the approach I'm arguing
here with literature

00:17:47.820 --> 00:17:53.820
is to accept the insolubility
of problems and say, well, then,

00:17:53.820 --> 00:17:54.380
what then?

00:17:57.150 --> 00:17:58.020
I'm going to die.

00:17:58.020 --> 00:18:00.460
How do I position myself
in relation to that?

00:18:05.477 --> 00:18:07.810
Start thinking about the fact,
well, other people I love

00:18:07.810 --> 00:18:08.900
are going to die too.

00:18:08.900 --> 00:18:11.320
In fact, everyone's
going to die.

00:18:11.320 --> 00:18:13.930
Certain things fall
out that are actually

00:18:13.930 --> 00:18:15.940
very positive in
terms of how you

00:18:15.940 --> 00:18:19.810
lead your life in the moment
and how you treat other people

00:18:19.810 --> 00:18:24.605
when you accept the reality and
the insolubility of a problem.

00:18:24.605 --> 00:18:26.230
I began by saying
that most of the book

00:18:26.230 --> 00:18:27.400
is not about this stuff.

00:18:30.280 --> 00:18:35.760
A lot of it is about
nature and climate change.

00:18:35.760 --> 00:18:40.480
And I'm going to read some
pages from an essay called

00:18:40.480 --> 00:18:41.680
"The Essay In Dark Times".

00:18:44.240 --> 00:18:48.415
Everything I ever write
is arguing for literature.

00:18:48.415 --> 00:18:49.790
Even when I'm
writing about birds

00:18:49.790 --> 00:18:53.420
I'm arguing for literature in
the way I write about birds.

00:18:53.420 --> 00:18:55.040
Books are what I
most care about.

00:19:02.700 --> 00:19:09.940
OK, so I've just
disrupted myself.

00:19:09.940 --> 00:19:10.750
Where was I?

00:19:10.750 --> 00:19:13.570
OK, I'm just going to
launch in and hopefully I

00:19:13.570 --> 00:19:15.430
can make this comprehensible.

00:19:15.430 --> 00:19:18.250
I'm starting right in
the middle of an essay.

00:19:18.250 --> 00:19:19.900
When I came home from Africa--

00:19:19.900 --> 00:19:22.780
where I'd been birdwatching--
to Santa Cruz--

00:19:22.780 --> 00:19:25.996
this was two years ago exactly--

00:19:25.996 --> 00:19:27.370
my progressive
friends were still

00:19:27.370 --> 00:19:28.960
struggling to
understand how Trump

00:19:28.960 --> 00:19:32.650
could have won I remembered
a public event I'd once

00:19:32.650 --> 00:19:35.920
done with the optimistic
social media specialist Clay

00:19:35.920 --> 00:19:39.910
Shirky, who'd recounted to
the audience how shocked

00:19:39.910 --> 00:19:43.180
professional New York restaurant
critics had been when Zagat,

00:19:43.180 --> 00:19:45.310
a crowdsourced
reviewing service,

00:19:45.310 --> 00:19:50.037
had named Union Square Cafe
the best restaurant in town.

00:19:50.037 --> 00:19:51.870
Shirky's point was that
professional critics

00:19:51.870 --> 00:19:53.495
aren't as smart as
they think they are,

00:19:53.495 --> 00:19:56.100
that, in fact, in
the age of big data,

00:19:56.100 --> 00:19:59.520
critics are no longer
even necessary.

00:19:59.520 --> 00:20:02.730
At the event, ignoring the
fact that Union Square Cafe

00:20:02.730 --> 00:20:05.580
was my favorite New
York restaurant--

00:20:05.580 --> 00:20:07.600
the crowd was right--

00:20:07.600 --> 00:20:10.540
I'd sourly wondered if Shirky
believed that critics were also

00:20:10.540 --> 00:20:13.480
stupid to consider Alice Munro
a better writer than James

00:20:13.480 --> 00:20:15.400
Patterson.

00:20:15.400 --> 00:20:19.080
But now Trump's victory, too,
had vindicated Shirky's mockery

00:20:19.080 --> 00:20:20.610
of the pundits.

00:20:20.610 --> 00:20:22.890
Social media had
allowed Trump to bypass

00:20:22.890 --> 00:20:25.140
the critical establishment,
and just enough members

00:20:25.140 --> 00:20:28.800
of the crowd in key swing
states had found his low comedy

00:20:28.800 --> 00:20:32.010
and as incendiary speech
better than Clinton's

00:20:32.010 --> 00:20:36.180
nuanced arguments and
her mastery of policy.

00:20:36.180 --> 00:20:37.650
This follows from that.

00:20:37.650 --> 00:20:40.260
Without Twitter and
Facebook, no Trump.

00:20:45.230 --> 00:20:46.760
After the election
Mark Zuckerberg

00:20:46.760 --> 00:20:48.907
did briefly seem to
take responsibility,

00:20:48.907 --> 00:20:50.990
sort of, for having created
the platform of choice

00:20:50.990 --> 00:20:54.080
for fake news about Clinton, and
to suggest that Facebook could

00:20:54.080 --> 00:20:57.890
become more active in filtering
the news-- good luck with that.

00:20:57.890 --> 00:20:59.870
Twitter, for its part,
kept its head down.

00:20:59.870 --> 00:21:01.820
As Trump's tweeting
continued unabated,

00:21:01.820 --> 00:21:03.950
what could Twitter possibly
say, that it was making

00:21:03.950 --> 00:21:04.991
the world a better place?

00:21:08.370 --> 00:21:12.600
In December my favorite Santa
Cruz radio station, KPIG,

00:21:12.600 --> 00:21:15.360
began running a fake ad
offering counseling services

00:21:15.360 --> 00:21:19.814
to addicts of Trump-hating
tweets and Facebook posts.

00:21:19.814 --> 00:21:22.230
The following month, the week
before Trump's inauguration,

00:21:22.230 --> 00:21:24.120
the PEN American
Center organized events

00:21:24.120 --> 00:21:26.730
around the country to reject
the assault on free speech

00:21:26.730 --> 00:21:29.550
that it claimed
Trump represented.

00:21:29.550 --> 00:21:31.650
Although his administration's
travel restrictions

00:21:31.650 --> 00:21:34.290
did later make it harder for
writers from Muslim countries

00:21:34.290 --> 00:21:37.240
to have their voices heard
in the United States,

00:21:37.240 --> 00:21:41.010
and although he did still later
kick Acosta out of the press

00:21:41.010 --> 00:21:44.130
corps, the one bad
thing that could not

00:21:44.130 --> 00:21:47.280
be said of Trump in January
was that he had in any way

00:21:47.280 --> 00:21:49.530
curtailed free speech.

00:21:49.530 --> 00:21:53.450
His lying, bullying tweets
were free speech on steroids.

00:21:53.450 --> 00:21:55.530
PEN itself, just a
few years earlier,

00:21:55.530 --> 00:21:58.800
had given a free
speech award to Twitter

00:21:58.800 --> 00:22:02.340
for its self-publicized
role in the Arab Spring.

00:22:02.340 --> 00:22:04.560
The actual result
of the Arab Spring

00:22:04.560 --> 00:22:07.350
had been a retrenchment
of autocracy,

00:22:07.350 --> 00:22:10.110
and Twitter had since revealed
itself in Trump's hands

00:22:10.110 --> 00:22:13.350
to be a platform made
to order for autocracy,

00:22:13.350 --> 00:22:15.480
but the ironies
didn't end there.

00:22:15.480 --> 00:22:18.510
During the same week in January,
progressive American bookstores

00:22:18.510 --> 00:22:22.410
and authors proposed a
boycott of Simon and Schuster

00:22:22.410 --> 00:22:24.090
for the crime of
intending to publish

00:22:24.090 --> 00:22:28.350
one book by the dismal
right wing provocateur Milo

00:22:28.350 --> 00:22:30.539
Yiannopoulos.

00:22:30.539 --> 00:22:32.080
The angriest of the
bookstores talked

00:22:32.080 --> 00:22:35.850
of refusing to stop all
titles from S&amp;S, including,

00:22:35.850 --> 00:22:39.930
presumably, the books of Andrew
Solomon, the president of PEN.

00:22:39.930 --> 00:22:42.780
The talk didn't end until
S&amp;S voided its contract

00:22:42.780 --> 00:22:45.140
with Yiannopoulos.

00:22:45.140 --> 00:22:46.669
Trump and his
alt-right supporters

00:22:46.669 --> 00:22:48.960
take pleasure in pushing the
buttons of the politically

00:22:48.960 --> 00:22:51.300
correct, but it only works
because the buttons are there

00:22:51.300 --> 00:22:52.890
to be pushed.

00:22:52.890 --> 00:22:54.660
Students and activists
claiming the right

00:22:54.660 --> 00:22:56.370
to not hear things
that upset them

00:22:56.370 --> 00:22:59.250
and to shout down
ideas that offend them.

00:22:59.250 --> 00:23:02.940
Intolerance particularly
flourishes online,

00:23:02.940 --> 00:23:04.560
where measured
speech is punished

00:23:04.560 --> 00:23:06.225
by not getting clicked on.

00:23:06.225 --> 00:23:09.080
The invisible Facebook
and Google algorithm

00:23:09.080 --> 00:23:10.920
steer you toward
content you agree with,

00:23:10.920 --> 00:23:13.200
and non-conforming voices
stay silent for fear

00:23:13.200 --> 00:23:16.540
of being flamed or
trolled or unfriended.

00:23:16.540 --> 00:23:19.410
The result is a silo in which
whatever side you're on,

00:23:19.410 --> 00:23:22.860
you feel absolutely right
to hate what you hate.

00:23:22.860 --> 00:23:24.630
And here is another
way in which the essay

00:23:24.630 --> 00:23:27.090
differs from superficially
similar kinds

00:23:27.090 --> 00:23:30.030
of subjective speech.

00:23:30.030 --> 00:23:32.220
The essay's roots
are in literature,

00:23:32.220 --> 00:23:34.470
and literature at its best--

00:23:34.470 --> 00:23:37.290
the work of Alice
Munro, for example--

00:23:37.290 --> 00:23:40.440
invites you to ask whether
you might be somewhat wrong,

00:23:40.440 --> 00:23:43.440
maybe entirely wrong,
and to imagine why

00:23:43.440 --> 00:23:46.570
someone else might hate you.

00:23:46.570 --> 00:23:49.670
I'm going to pause just to
say that I wrote this a year

00:23:49.670 --> 00:23:52.250
and a half ago, and
it already feels

00:23:52.250 --> 00:23:57.390
like what I was saying
then, which seemed urgent

00:23:57.390 --> 00:23:59.550
and somewhat fresh, now a
lot of people are saying,

00:23:59.550 --> 00:24:02.370
which has been very
heartening, particularly

00:24:02.370 --> 00:24:08.220
the scrutiny of Facebook that
has unfolded in the past 18

00:24:08.220 --> 00:24:10.260
months, has been
very, very heartening.

00:24:10.260 --> 00:24:12.420
People are starting to get
it, as far as I can see.

00:24:17.890 --> 00:24:19.330
Three years ago,
I was in a state

00:24:19.330 --> 00:24:22.702
of rage about climate change.

00:24:22.702 --> 00:24:24.160
The Republican
Party was continuing

00:24:24.160 --> 00:24:27.280
to lie about the absence
of a scientific consensus

00:24:27.280 --> 00:24:30.940
on climate, Florida's Department
of Environmental Protection

00:24:30.940 --> 00:24:33.640
had gone so far as to forbid
its employees to write the words

00:24:33.640 --> 00:24:36.940
climate change after the
governor, now involved

00:24:36.940 --> 00:24:41.110
in an interesting
ballot recount,

00:24:41.110 --> 00:24:44.380
insisted that it
wasn't a true fact.

00:24:44.380 --> 00:24:48.200
But I wasn't much less
angry at the left.

00:24:48.200 --> 00:24:50.420
I'd read a new book
by Naomi Klein,

00:24:50.420 --> 00:24:52.300
"This Changes Everything",
in which she'd

00:24:52.300 --> 00:24:56.110
assured the reader that
although time is tight,

00:24:56.110 --> 00:24:58.480
we still have 10 years
to radically remake

00:24:58.480 --> 00:25:01.990
the global economy and prevent
global temperatures from rising

00:25:01.990 --> 00:25:05.620
by more than two degrees Celsius
by the end of the century.

00:25:05.620 --> 00:25:09.070
Klein's optimism was
touching, but it, too,

00:25:09.070 --> 00:25:11.694
was a kind of denialism.

00:25:11.694 --> 00:25:13.360
Even before the
election of Donald Trump

00:25:13.360 --> 00:25:16.210
there was no evidence to suggest
that humanity is capable--

00:25:16.210 --> 00:25:18.700
politically,
psychologically, ethically,

00:25:18.700 --> 00:25:21.850
economically-- of slashing
carbon emissions quickly

00:25:21.850 --> 00:25:25.430
and deeply enough to
change everything.

00:25:25.430 --> 00:25:27.970
Even the European Union,
which had taken the early lead

00:25:27.970 --> 00:25:30.640
on climate and was fond
of lecturing other regions

00:25:30.640 --> 00:25:35.320
on their irresponsibility,
needed only a recession in 2009

00:25:35.320 --> 00:25:38.890
to shift its focus
to economic growth.

00:25:38.890 --> 00:25:41.740
Barring a worldwide revolt
against free market capitalism

00:25:41.740 --> 00:25:44.650
in the next 10 years, the
scenario that Klein contended

00:25:44.650 --> 00:25:47.950
could still save us, the most
likely rise in temperature

00:25:47.950 --> 00:25:52.032
this century is on the order
of six degrees Celsius.

00:25:52.032 --> 00:25:54.790
We'll be lucky to avoid a two
degree rise before the year

00:25:54.790 --> 00:25:57.930
2030.

00:25:57.930 --> 00:26:04.422
Again, the new report in
October, starting to get it.

00:26:04.422 --> 00:26:05.630
But this was three years ago.

00:26:05.630 --> 00:26:09.200
Now, actually, four years ago.

00:26:09.200 --> 00:26:11.580
In a polity ever
more starkly divided,

00:26:11.580 --> 00:26:13.200
the truth about
global warming was

00:26:13.200 --> 00:26:18.370
even less convenient to
the left than to the right.

00:26:18.370 --> 00:26:21.135
The right's denials
were odious lies,

00:26:21.135 --> 00:26:22.510
but at least they
were consistent

00:26:22.510 --> 00:26:26.230
with a certain cold-eyed
political realism.

00:26:26.230 --> 00:26:28.930
The left, having
excoriated the right

00:26:28.930 --> 00:26:32.860
for its intellectual dishonesty
and turned climate denialism

00:26:32.860 --> 00:26:35.560
into a political
rallying cry, was now

00:26:35.560 --> 00:26:38.920
in an impossible position.

00:26:38.920 --> 00:26:42.670
The left had to keep insisting
on the truth of climate science

00:26:42.670 --> 00:26:45.370
while persisting in the fiction
that collective world action

00:26:45.370 --> 00:26:47.650
could stave off the worst of it.

00:26:47.650 --> 00:26:49.510
That universal
acceptance of the facts--

00:26:49.510 --> 00:26:52.540
which really might have
changed everything in 1995--

00:26:52.540 --> 00:26:55.150
could still change everything.

00:26:55.150 --> 00:26:56.530
Otherwise, what
difference did it

00:26:56.530 --> 00:27:00.801
make if the Republicans
quibbled with the science?

00:27:00.801 --> 00:27:02.550
Because my sympathies
were with the left--

00:27:02.550 --> 00:27:04.920
reducing carbon emissions
is vastly better than doing

00:27:04.920 --> 00:27:08.160
nothing, every
half degree helps--

00:27:08.160 --> 00:27:09.900
I also held it to
a higher standard.

00:27:13.020 --> 00:27:14.880
Denying the dark
reality, pretending

00:27:14.880 --> 00:27:18.810
that the Paris Accord
could avert catastrophe

00:27:18.810 --> 00:27:21.630
was understandable as a tactic
to keep people motivated,

00:27:21.630 --> 00:27:25.370
to reduce emissions,
to keep hope alive.

00:27:25.370 --> 00:27:29.190
As a strategy, though, it
did more harm than good.

00:27:29.190 --> 00:27:31.620
It ceded the
ethical high ground,

00:27:31.620 --> 00:27:34.660
insulted the intelligence
of unpersuaded voters--

00:27:34.660 --> 00:27:37.770
really, we still have 10 years--

00:27:37.770 --> 00:27:40.320
and precluded frank discussion
of how the global community

00:27:40.320 --> 00:27:42.637
should prepare for
drastic changes,

00:27:42.637 --> 00:27:44.220
and how nations like
Bangladesh should

00:27:44.220 --> 00:27:47.190
be compensated for what
nations like the United States

00:27:47.190 --> 00:27:47.970
have done to them.

00:27:50.736 --> 00:27:56.130
Now we get to one of my
main points in this book.

00:27:56.130 --> 00:27:59.970
Dishonesty also
skewed priorities.

00:27:59.970 --> 00:28:02.610
In the past 20 years, the
environmental movement

00:28:02.610 --> 00:28:05.670
had become captive
to a single issue,

00:28:05.670 --> 00:28:08.190
partly out of genuine
alarm, partly also

00:28:08.190 --> 00:28:11.370
because foregrounding human
problems was politically less

00:28:11.370 --> 00:28:16.080
risky, less elitist, than
talking about nature.

00:28:16.080 --> 00:28:17.910
The big environmental
NGOs had all

00:28:17.910 --> 00:28:21.690
invested their political capital
in fighting climate change,

00:28:21.690 --> 00:28:24.810
a problem with a human face.

00:28:24.810 --> 00:28:28.560
The NGO that particularly
enraged me as a bird lover

00:28:28.560 --> 00:28:30.990
was the National
Audubon Society, once

00:28:30.990 --> 00:28:33.870
an uncompromising
defender of birds, now

00:28:33.870 --> 00:28:38.610
a lethargic institution with
a very large PR department.

00:28:38.610 --> 00:28:41.610
In September 2014,
with much fanfare,

00:28:41.610 --> 00:28:43.830
that PR department had
announced to the world

00:28:43.830 --> 00:28:45.900
that climate change was
the number one threat

00:28:45.900 --> 00:28:49.400
to the birds of North America.

00:28:49.400 --> 00:28:51.926
The announcement was
both narrowly dishonest,

00:28:51.926 --> 00:28:53.300
because its wording
didn't square

00:28:53.300 --> 00:28:56.510
with the conclusions of
Audubon's own scientists,

00:28:56.510 --> 00:28:59.630
and broadly dishonest,
because not one single bird

00:28:59.630 --> 00:29:01.220
death could be
directly attributed

00:29:01.220 --> 00:29:04.530
to human carbon emissions.

00:29:04.530 --> 00:29:07.410
In 2014 the most serious
threats to American birds

00:29:07.410 --> 00:29:11.400
were habitat loss
and outdoor cats.

00:29:11.400 --> 00:29:13.410
By invoking the buzzword
of climate change,

00:29:13.410 --> 00:29:15.810
Audubon got a lot of attention
in the liberal media.

00:29:15.810 --> 00:29:18.510
Another point had been scored
against the science-denying

00:29:18.510 --> 00:29:23.580
right, but it was not at all
clear how this helped birds.

00:29:23.580 --> 00:29:26.070
The only practical effect
of Audubon's announcement,

00:29:26.070 --> 00:29:28.380
it seemed to me, was
to discourage people

00:29:28.380 --> 00:29:31.560
from addressing the real threats
to nature in the present.

00:29:36.620 --> 00:29:40.250
I was so angry that I decided
I better write an essay.

00:29:40.250 --> 00:29:43.910
I began with a jeremiad against
the National Audubon Society,

00:29:43.910 --> 00:29:46.160
broadened it into a
scornful denunciation

00:29:46.160 --> 00:29:49.310
of the environmental
movement generally, and then

00:29:49.310 --> 00:29:51.620
started waking up in the
night in a panic of remorse

00:29:51.620 --> 00:29:53.810
and doubt.

00:29:53.810 --> 00:29:55.610
For the writer, an
essay is a mirror,

00:29:55.610 --> 00:29:58.700
and I didn't what I
was seeing in this one.

00:29:58.700 --> 00:30:00.830
Why was I excoriating
fellow liberals

00:30:00.830 --> 00:30:04.186
when the denialists
were so much worse?

00:30:04.186 --> 00:30:06.560
The prospect of climate change
was every bit as sickening

00:30:06.560 --> 00:30:09.950
to me as to the groups
I was attacking.

00:30:09.950 --> 00:30:12.110
With every additional
degree of global warming,

00:30:12.110 --> 00:30:14.510
further hundreds of millions
of people around the world

00:30:14.510 --> 00:30:15.650
would suffer.

00:30:15.650 --> 00:30:17.270
Wasn't it worth
an all-out effort

00:30:17.270 --> 00:30:21.470
to achieve a reduction of
even half of one degree?

00:30:21.470 --> 00:30:23.270
Wasn't it obscene
to be talking about

00:30:23.270 --> 00:30:27.530
birds when children in
Bangladesh were threatened?

00:30:27.530 --> 00:30:29.290
Yes, the premise of
my essay was that we

00:30:29.290 --> 00:30:31.810
have an ethical responsibility
to other species

00:30:31.810 --> 00:30:37.430
as well as to our own, but
what if that premise was false?

00:30:37.430 --> 00:30:40.700
And even if it was true,
did I really care personally

00:30:40.700 --> 00:30:44.450
about biodiversity, or was I
just a privileged white guy who

00:30:44.450 --> 00:30:46.240
liked to go birding?

00:30:46.240 --> 00:30:51.110
And not even a pure-hearted
birder, a lister?

00:30:51.110 --> 00:30:53.660
After three nights of doubting
my character and motives

00:30:53.660 --> 00:30:55.640
I called my New Yorker
editor Henry Finder

00:30:55.640 --> 00:30:58.890
and told him I couldn't
write the piece.

00:30:58.890 --> 00:31:01.310
I'd done plenty of ranting
about climate to my friends

00:31:01.310 --> 00:31:02.990
and to like-minded
conservationists,

00:31:02.990 --> 00:31:06.230
but it was a lot like the
ranting that happens online

00:31:06.230 --> 00:31:09.370
where you're protected by the
impromptu nature of the writing

00:31:09.370 --> 00:31:12.290
and by the known friendliness
of your audience.

00:31:12.290 --> 00:31:15.500
Trying to write a
finished thing, an essay,

00:31:15.500 --> 00:31:19.520
had made me aware of the
sloppiness of my thinking,

00:31:19.520 --> 00:31:21.560
and it also enormously
increased the risk

00:31:21.560 --> 00:31:25.649
of shame, because the
writing wasn't casual

00:31:25.649 --> 00:31:27.940
and because it was going out
to an audience of probably

00:31:27.940 --> 00:31:30.580
hostile strangers.

00:31:30.580 --> 00:31:36.200
Following Henry's
admonition, I'd

00:31:36.200 --> 00:31:40.040
come to think of the essayist
as a firefighter, whose

00:31:40.040 --> 00:31:43.430
job, while everyone else is
fleeing the flames of shame,

00:31:43.430 --> 00:31:46.130
is to run straight into them.

00:31:46.130 --> 00:31:50.292
But I had a lot more to fear now
than my mother's disapproval.

00:31:50.292 --> 00:31:51.750
Sorry, that's going
over your head.

00:31:54.500 --> 00:31:58.312
The first time I risked
shame in an essay,

00:31:58.312 --> 00:32:00.020
my mother didn't speak
to me for a month.

00:32:02.917 --> 00:32:05.250
My essay might have stayed
abandoned if I hadn't already

00:32:05.250 --> 00:32:06.916
clicked the button
on Audubon's website,

00:32:06.916 --> 00:32:08.670
affirming that yes,
I wanted to join it

00:32:08.670 --> 00:32:10.410
in fighting climate change.

00:32:10.410 --> 00:32:12.630
I'd only done this to
gather rhetorical ammunition

00:32:12.630 --> 00:32:15.300
to use against
Audubon, but a deluge

00:32:15.300 --> 00:32:19.190
of direct mail solicitations
had followed from that click.

00:32:19.190 --> 00:32:21.480
I got at least eight
of them in six weeks,

00:32:21.480 --> 00:32:24.240
all of them asking me
to give money, along

00:32:24.240 --> 00:32:28.104
with a similar deluge
in my email inbox.

00:32:28.104 --> 00:32:29.520
A few days after
speaking to Henry

00:32:29.520 --> 00:32:31.620
I opened one of the
emails and found myself

00:32:31.620 --> 00:32:36.550
looking at a picture of myself,
luckily, a flattering image,

00:32:36.550 --> 00:32:39.310
taken in 2010 for Vogue
Magazine, which had dressed me

00:32:39.310 --> 00:32:41.885
up better than I dressed
myself, and posed me

00:32:41.885 --> 00:32:45.275
in a field with my
binoculars like a birder.

00:32:45.275 --> 00:32:47.650
The headline of the email was
something like, join author

00:32:47.650 --> 00:32:51.484
Jonathan Franzen in
supporting Audubon.

00:32:51.484 --> 00:32:53.650
It was true that a few years
earlier in an interview

00:32:53.650 --> 00:32:55.240
with Audubon Magazine
I had politely

00:32:55.240 --> 00:32:58.990
praised the organization,
or at least its magazine,

00:32:58.990 --> 00:33:00.580
but no one had asked
for my permission

00:33:00.580 --> 00:33:02.830
to use my name and
image for solicitation.

00:33:02.830 --> 00:33:05.410
I wasn't sure the
email was even legal.

00:33:08.314 --> 00:33:12.186
So bad, really bad.

00:33:12.186 --> 00:33:16.059
A more benign impetus to return
to the essay came from Henry.

00:33:16.059 --> 00:33:18.350
As far as I know, Henry
couldn't care less about birds,

00:33:18.350 --> 00:33:20.225
but he seemed to see
something in my argument

00:33:20.225 --> 00:33:23.360
that our preoccupation
with future catastrophes

00:33:23.360 --> 00:33:26.690
discourages us from tackling
solvable environmental problems

00:33:26.690 --> 00:33:28.416
in the here and now.

00:33:28.416 --> 00:33:30.500
In an email to me,
he gently suggested

00:33:30.500 --> 00:33:33.950
that I lose the tone
of prophetic scorn.

00:33:33.950 --> 00:33:36.260
This piece will be
more persuasive,

00:33:36.260 --> 00:33:38.570
he wrote in another,
if, ironically, it's

00:33:38.570 --> 00:33:40.647
more ambivalent
and less political.

00:33:40.647 --> 00:33:42.980
You're not wailing on folks
who want us to pay attention

00:33:42.980 --> 00:33:45.290
to climate change and
emissions reductions,

00:33:45.290 --> 00:33:48.560
but you're attentive to the
costs, to what the discourse

00:33:48.560 --> 00:33:50.810
pushes to the margins.

00:33:50.810 --> 00:33:52.850
Email by email,
revision by revision,

00:33:52.850 --> 00:33:55.640
Henry nudged me toward framing
the essay not as a denunciation

00:33:55.640 --> 00:33:56.960
but as a question.

00:33:56.960 --> 00:33:58.730
How do we find
meaning in our actions

00:33:58.730 --> 00:34:01.070
when the world seems
to be coming to an end?

00:34:05.140 --> 00:34:07.060
Much of the final
draft was devoted

00:34:07.060 --> 00:34:09.460
to a pair of well-conceived
regional conservation

00:34:09.460 --> 00:34:12.010
projects in Peru and Costa
Rica, where the world really

00:34:12.010 --> 00:34:14.110
is being made a
better place, not

00:34:14.110 --> 00:34:15.760
just for wild plants
and wild animals,

00:34:15.760 --> 00:34:19.480
but for the Peruvians and
Costa Rican who live there.

00:34:19.480 --> 00:34:22.290
Work on these projects
is personally meaningful

00:34:22.290 --> 00:34:24.249
and the benefits are
immediate and tangible.

00:34:27.104 --> 00:34:28.520
In writing about
the two projects,

00:34:28.520 --> 00:34:30.895
I hope that one or two of the
big charitable foundations,

00:34:30.895 --> 00:34:32.860
the ones spending tens
of millions of dollars

00:34:32.860 --> 00:34:36.550
on biodiesel development or
on wind farms in Eritrea,

00:34:36.550 --> 00:34:38.500
might read the piece
and consider investing

00:34:38.500 --> 00:34:43.460
in work that produces results.

00:34:43.460 --> 00:34:47.670
What I got instead was a missile
attack from the liberal silo.

00:34:47.670 --> 00:34:49.380
I'm not on social
media, but my friends

00:34:49.380 --> 00:34:52.739
reported that I was being called
all sorts of names, including

00:34:52.739 --> 00:34:58.515
birdbrain and climate
change denier.

00:34:58.515 --> 00:35:02.110
Tweet-sized snippets of my
essay retweeted out of context

00:35:02.110 --> 00:35:04.210
made it sound as if I'd
proposed that we abandon

00:35:04.210 --> 00:35:07.030
the effort to reduce
carbon emissions, which

00:35:07.030 --> 00:35:09.760
was the position of the
Republican Party, which

00:35:09.760 --> 00:35:12.850
by the polarizing logic
of online discourse,

00:35:12.850 --> 00:35:15.670
made me a climate change denier.

00:35:15.670 --> 00:35:17.710
In fact, I'm such a
climate science accepter

00:35:17.710 --> 00:35:20.740
that I don't even bother
having hope for the ice caps.

00:35:20.740 --> 00:35:23.170
All I denied was that a
right-thinking international

00:35:23.170 --> 00:35:25.517
elite meeting in nice
hotels around the world

00:35:25.517 --> 00:35:26.725
could stop them from melting.

00:35:29.710 --> 00:35:32.650
This was my crime
against orthodoxy.

00:35:32.650 --> 00:35:35.230
Climate now has such a lock
on the liberal imagination

00:35:35.230 --> 00:35:37.960
that any attempt to
change the conversation,

00:35:37.960 --> 00:35:40.360
even trying to change it to
the epic extinction event

00:35:40.360 --> 00:35:42.130
that human beings
are already creating

00:35:42.130 --> 00:35:44.530
without the help
of climate change,

00:35:44.530 --> 00:35:46.420
amounts to an offense
against religion.

00:35:53.860 --> 00:35:56.060
It's 1:40, I'm going
to just stop there.

00:35:56.060 --> 00:35:57.310
The essay goes on.

00:35:57.310 --> 00:35:59.677
Hopefully that was a
suspenseful point to stop.

00:35:59.677 --> 00:36:01.760
I'd be happy to take any
questions you might have.

00:36:01.760 --> 00:36:03.135
Thank you very
much for listening

00:36:03.135 --> 00:36:06.838
to me this rainy afternoon.

00:36:06.838 --> 00:36:10.310
[APPLAUSE]

00:36:12.790 --> 00:36:14.220
AUDIENCE: Thank you very much.

00:36:14.220 --> 00:36:16.260
I have two questions,
actually, and apologies

00:36:16.260 --> 00:36:19.410
if I didn't quite understand
some the points that you made.

00:36:19.410 --> 00:36:22.470
My first question
is, given that the--

00:36:22.470 --> 00:36:25.560
and probably I'm speaking on
behalf of the liberal orthodoxy

00:36:25.560 --> 00:36:26.250
here--

00:36:26.250 --> 00:36:28.350
but given that,
as you well know,

00:36:28.350 --> 00:36:32.910
the climate catastrophe is
going to destroy all before it,

00:36:32.910 --> 00:36:36.180
at some level what is the
point of focusing even

00:36:36.180 --> 00:36:40.650
on sixth mass
extinction level events,

00:36:40.650 --> 00:36:42.360
given that anything
that is preserved

00:36:42.360 --> 00:36:46.120
is not likely to last more than
a couple of decades anyway?

00:36:46.120 --> 00:36:46.634
And--

00:36:46.634 --> 00:36:48.550
JONATHAN FRANZEN: Let's
have a back and forth.

00:36:48.550 --> 00:36:49.710
I'll do that one,
and then you can--

00:36:49.710 --> 00:36:51.959
if you remember your question
I'll go to the next one.

00:36:54.430 --> 00:37:02.830
The short answer is I
know my spouse equivalent

00:37:02.830 --> 00:37:04.210
Kathy is going to die.

00:37:04.210 --> 00:37:06.460
What's the point in
doing anything with her?

00:37:06.460 --> 00:37:08.440
What's the point in
trying to help her

00:37:08.440 --> 00:37:11.620
in any way, given that she's
going to die pretty soon?

00:37:11.620 --> 00:37:13.390
It's all going to be over.

00:37:13.390 --> 00:37:15.530
Why would I--

00:37:15.530 --> 00:37:17.380
At a certain level
the question doesn't

00:37:17.380 --> 00:37:23.050
compute, because things I
love right in front of me

00:37:23.050 --> 00:37:25.780
are in trouble.

00:37:25.780 --> 00:37:26.470
I love them.

00:37:26.470 --> 00:37:28.140
I want to help them.

00:37:28.140 --> 00:37:31.420
I-- and yes, I know, it's going
to be tough for a lot of them.

00:37:31.420 --> 00:37:34.270
Although birds are
shockingly resilient,

00:37:34.270 --> 00:37:37.190
and there's some
recent new evidence,

00:37:37.190 --> 00:37:41.440
one study in California showing
incredibly quick adaptation

00:37:41.440 --> 00:37:43.930
in terms of when they
started the breeding cycle,

00:37:43.930 --> 00:37:46.420
to a warming climate.

00:37:46.420 --> 00:37:50.410
But that's not even--
it doesn't even matter.

00:37:50.410 --> 00:37:53.860
The meaning in my life comes
from spending time with Kathy

00:37:53.860 --> 00:37:58.360
and being around things I
love and trying to help them.

00:38:01.446 --> 00:38:03.570
AUDIENCE: I guess it does,
at some level, come down

00:38:03.570 --> 00:38:07.080
to a question of values, in
the sense that if your spouse

00:38:07.080 --> 00:38:08.820
equivalent were going to--

00:38:08.820 --> 00:38:11.280
I would not take an
action to prolong

00:38:11.280 --> 00:38:13.110
her life by a minute or two.

00:38:13.110 --> 00:38:16.170
I guess I would look at
the question of surviving

00:38:16.170 --> 00:38:19.350
extinctions as best measured
in geologic time, where

00:38:19.350 --> 00:38:23.397
it sounds like you're looking
at it at a human time scale.

00:38:23.397 --> 00:38:24.980
JONATHAN FRANZEN:
Well, yeah, in terms

00:38:24.980 --> 00:38:28.370
of meaning in what
I do with my life,

00:38:28.370 --> 00:38:31.480
I have no choice but to view
it at a human time scale.

00:38:37.205 --> 00:38:41.180
The thing is, we don't know
what the future holds exactly.

00:38:41.180 --> 00:38:45.860
What we do know is if we let
the extinction event unfold,

00:38:45.860 --> 00:38:49.030
there will not be anything.

00:38:49.030 --> 00:38:54.980
We will have so radically
destroyed the fabric

00:38:54.980 --> 00:38:59.130
of the natural world as
we were bequeathed it

00:38:59.130 --> 00:39:03.800
that then, someone's coming
along with a blowtorch

00:39:03.800 --> 00:39:06.500
after we've already
completely killed it.

00:39:06.500 --> 00:39:09.950
But weird things happen.

00:39:09.950 --> 00:39:13.210
Maybe the geoengineering
will fix things.

00:39:13.210 --> 00:39:17.900
Maybe some-- even if it's
a relatively small fraction

00:39:17.900 --> 00:39:21.020
of the species that make up
what we call biodiversity,

00:39:21.020 --> 00:39:25.820
even managed to adapt,
evolve, it's like,

00:39:25.820 --> 00:39:27.770
the more we've
got to start with,

00:39:27.770 --> 00:39:29.230
the better chance everyone has.

00:39:29.230 --> 00:39:34.100
So I don't see that
as necessarily--

00:39:37.520 --> 00:39:40.880
I can also think in
a geological scale

00:39:40.880 --> 00:39:45.890
as well about it, which is,
let's separate in our mind

00:39:45.890 --> 00:39:47.570
what we're doing to the planet.

00:39:47.570 --> 00:39:51.590
On the one hand we're destroying
the last remaining ecosystems.

00:39:51.590 --> 00:39:54.740
On the other hand we're doing
this huge additional thing

00:39:54.740 --> 00:39:58.240
that is going to make
everything really,

00:39:58.240 --> 00:39:59.240
really, really terrible.

00:40:01.582 --> 00:40:03.790
It's not that those are
actually two different things

00:40:03.790 --> 00:40:05.020
we're doing.

00:40:05.020 --> 00:40:08.170
And I'm saying one of
them is something--

00:40:08.170 --> 00:40:09.790
and the latter, the
more terrible one

00:40:09.790 --> 00:40:12.940
is something that we
apparently, by all appearances,

00:40:12.940 --> 00:40:14.680
can't do anything about.

00:40:14.680 --> 00:40:17.770
The former, well, in fact, there
are some very concrete things

00:40:17.770 --> 00:40:20.240
we can do still.

00:40:20.240 --> 00:40:22.030
AUDIENCE: I see.

00:40:22.030 --> 00:40:23.760
OK, that makes sense.

00:40:23.760 --> 00:40:25.591
My second question,
you mentioned just now

00:40:25.591 --> 00:40:27.840
that the latter is something
that we can't do anything

00:40:27.840 --> 00:40:30.420
about, and earlier
in your talk you

00:40:30.420 --> 00:40:33.360
spoke a little bit
disparagingly of the notion

00:40:33.360 --> 00:40:36.180
that society might
have a all-out effort

00:40:36.180 --> 00:40:39.870
to reduce the warming by,
say, half a degree Celsius.

00:40:39.870 --> 00:40:42.270
I'm curious to what
extent do you see

00:40:42.270 --> 00:40:44.940
climate change as a binary--

00:40:44.940 --> 00:40:47.640
yes, the catastrophe
happens, or no, it doesn't--

00:40:47.640 --> 00:40:50.040
and to what extent
do you see value

00:40:50.040 --> 00:40:53.914
in reducing by some small amount
the scope of the catastrophe?

00:40:53.914 --> 00:40:55.830
JONATHAN FRANZEN: Oh,
and the cameras rolling,

00:40:55.830 --> 00:40:57.450
and so I really kind
of hate to admit

00:40:57.450 --> 00:41:04.150
that I don't think there is
an appreciable difference

00:41:04.150 --> 00:41:09.040
between 5.5 and 6
by the year 2100.

00:41:09.040 --> 00:41:11.200
AUDIENCE: Do you see an
appreciable difference

00:41:11.200 --> 00:41:15.680
in 800 million people dying
and 801 million people dying?

00:41:15.680 --> 00:41:19.050
JONATHAN FRANZEN:
Yes, of course, yes.

00:41:19.050 --> 00:41:21.960
And if we could--

00:41:21.960 --> 00:41:25.770
if there was some way to imagine
that collective action could

00:41:25.770 --> 00:41:28.440
be taken on the part of
countries of the world

00:41:28.440 --> 00:41:31.290
now and their populations
to save those,

00:41:31.290 --> 00:41:34.890
even one million lives, I
would be all for doing it.

00:41:37.520 --> 00:41:42.300
There are, of course,
other stakeholders.

00:41:42.300 --> 00:41:44.760
And you get into these kind of--

00:41:44.760 --> 00:41:49.800
the question is a
little bit like, well,

00:41:49.800 --> 00:41:54.090
if here's the last pair
of elephants on earth

00:41:54.090 --> 00:41:58.770
and here is a human
child, would you

00:41:58.770 --> 00:42:01.470
let the baby die in order to
save the elephant species?

00:42:01.470 --> 00:42:05.154
It's like, you can set these
things up and it's like, well,

00:42:05.154 --> 00:42:06.570
who wants to admit
that they would

00:42:06.570 --> 00:42:09.765
kill a human child in order to
save some non-human species?

00:42:13.380 --> 00:42:22.150
The whole structure of
that kind of question

00:42:22.150 --> 00:42:23.350
is morally upsetting.

00:42:29.760 --> 00:42:32.670
But what exactly
is your question?

00:42:32.670 --> 00:42:34.470
AUDIENCE: My question
is, is whether--

00:42:34.470 --> 00:42:39.330
it sounds to me like you don't
see any moral value in reducing

00:42:39.330 --> 00:42:41.460
the scope of the catastrophe?

00:42:41.460 --> 00:42:44.165
Is that a correct understanding
of what you are arguing?

00:42:44.165 --> 00:42:45.540
JONATHAN FRANZEN:
No, no, no, no.

00:42:45.540 --> 00:42:46.770
I absolutely do.

00:42:46.770 --> 00:42:50.631
If I thought we could really--

00:42:50.631 --> 00:42:55.140
if we could prevent Bangladesh
from being submerged,

00:42:55.140 --> 00:42:58.480
if there were some like--

00:42:58.480 --> 00:43:01.820
even if it was a narrow
path to achieving that,

00:43:01.820 --> 00:43:04.332
I would say we should
absolutely take that path.

00:43:04.332 --> 00:43:05.540
AUDIENCE: Would you support--

00:43:05.540 --> 00:43:07.456
and I recognize I'm going
on for a long time--

00:43:07.456 --> 00:43:11.330
would you support a path
which would reduce by 10%

00:43:11.330 --> 00:43:15.379
the fraction of Bangladesh
that is going to be submerged?

00:43:15.379 --> 00:43:17.670
JONATHAN FRANZEN: That gets
us back into the, would you

00:43:17.670 --> 00:43:20.780
save one child situation.

00:43:24.930 --> 00:43:26.640
The real answer to
your question is

00:43:26.640 --> 00:43:30.840
that 5.5, when it comes
to radical destabilization

00:43:30.840 --> 00:43:34.620
of everything we consider
civilization, and certainly

00:43:34.620 --> 00:43:38.760
everything we consider liberal
democracy and human rights

00:43:38.760 --> 00:43:44.100
and the rule of law, you're
so far past the point

00:43:44.100 --> 00:43:48.250
where it all breaks down that--

00:43:48.250 --> 00:43:53.140
because remember, there will
be increasing numbers of people

00:43:53.140 --> 00:43:57.060
who die simply
from being too hot.

00:43:57.060 --> 00:43:59.040
So there is a direct--

00:43:59.040 --> 00:44:00.900
summers will be
hotter, especially

00:44:00.900 --> 00:44:03.180
in places like India and Africa.

00:44:03.180 --> 00:44:05.310
People will simply
die of the heat.

00:44:05.310 --> 00:44:11.470
But most but the
catastrophe we're facing

00:44:11.470 --> 00:44:16.240
is a compounding
thing, where frequency

00:44:16.240 --> 00:44:25.420
of extreme climatic events,
drought, water crises, all

00:44:25.420 --> 00:44:29.290
of this begin to combine with
a political situation which

00:44:29.290 --> 00:44:31.090
is deteriorating.

00:44:31.090 --> 00:44:33.760
And then you throw
some gas on the flames

00:44:33.760 --> 00:44:38.050
in terms of what technology
is doing to break down

00:44:38.050 --> 00:44:40.450
the ability of people
to actually speak

00:44:40.450 --> 00:44:43.990
to one another civilly and
come to some sort of agreement,

00:44:43.990 --> 00:44:46.450
even about basic facts.

00:44:46.450 --> 00:44:51.417
And it looks like what we've
got now, this lovely world here

00:44:51.417 --> 00:44:53.500
in Cambridge, Massachusetts,
it's just really hard

00:44:53.500 --> 00:44:55.390
to see how that stands
up, whether it's

00:44:55.390 --> 00:44:58.690
5.5 or 6 degrees warming
by the end of the century.

00:45:02.570 --> 00:45:10.430
Sorry to be a bummer about it,
but that's very good questions.

00:45:10.430 --> 00:45:12.340
I appreciate the
opportunity to clarify.

00:45:15.650 --> 00:45:17.440
AUDIENCE: Let's
move to literature.

00:45:17.440 --> 00:45:19.717
Maybe it's less of an issue.

00:45:19.717 --> 00:45:21.550
JONATHAN FRANZEN:
[INAUDIBLE] has a question

00:45:21.550 --> 00:45:24.260
is what you're saying.

00:45:24.260 --> 00:45:27.610
AUDIENCE: In one of your
essays you mourn the fact

00:45:27.610 --> 00:45:32.200
that readers think
and judge characters

00:45:32.200 --> 00:45:36.580
in books in terms of
likability, how they like them.

00:45:36.580 --> 00:45:39.220
I was wondering, how do
you feel about readers that

00:45:39.220 --> 00:45:43.690
focus on assessing the
morality of characters,

00:45:43.690 --> 00:45:46.960
and furthermore, as a writer,
are you concerned with how

00:45:46.960 --> 00:45:50.350
readers can assess the
morality of your characters

00:45:50.350 --> 00:45:53.200
in an ever-changing world
where the morality changed so

00:45:53.200 --> 00:45:56.650
quickly, where one character
might seem to be quite

00:45:56.650 --> 00:45:59.020
all right, but after a
few years you say, oh,

00:45:59.020 --> 00:46:02.200
he's completely immoral, he
must be distanced from or even

00:46:02.200 --> 00:46:04.949
banned completely.

00:46:04.949 --> 00:46:06.490
JONATHAN FRANZEN:
Yeah, I am a little

00:46:06.490 --> 00:46:08.410
impatient with those
moral judgments.

00:46:08.410 --> 00:46:10.960
I think people who go into
the business of writing

00:46:10.960 --> 00:46:18.610
literary fiction tend to
be at odds with people who

00:46:18.610 --> 00:46:22.120
are morally sure of themselves.

00:46:22.120 --> 00:46:26.710
It's not quite an entry
level qualification

00:46:26.710 --> 00:46:30.250
for being a good novelist,
but it's nearly one,

00:46:30.250 --> 00:46:35.790
that you begin with
the presumption

00:46:35.790 --> 00:46:42.720
that you don't know
what's right and probably

00:46:42.720 --> 00:46:45.510
that what other people
consider right and wrong

00:46:45.510 --> 00:46:47.520
needs some examination.

00:46:50.050 --> 00:46:51.210
You know, it's interesting.

00:46:55.890 --> 00:47:01.090
I take your question
partly be to be pointing,

00:47:01.090 --> 00:47:04.310
in a little larger time
scale, toward the difference

00:47:04.310 --> 00:47:07.370
in the way we read people
from the late 19th century.

00:47:11.330 --> 00:47:19.920
Joseph Conrad, who was a
early and powerful critic

00:47:19.920 --> 00:47:20.940
of colonialism.

00:47:23.640 --> 00:47:25.440
That's how I
remembered "Nostromo".

00:47:25.440 --> 00:47:30.570
It's just like this wipeout
attack on European colonialism

00:47:30.570 --> 00:47:33.270
in South America.

00:47:33.270 --> 00:47:35.970
I went back and I
reread "Victory",

00:47:35.970 --> 00:47:41.760
which had been the novel of his
I admired most after "Nostromo"

00:47:41.760 --> 00:47:46.800
last summer and it was like,
oh dude, you're such a racist,

00:47:46.800 --> 00:47:50.010
you're such a sexist, you
don't understand women at all,

00:47:50.010 --> 00:47:51.180
oh my god, that's--

00:47:51.180 --> 00:47:55.320
what you are, ugh,
like, don't do that.

00:47:55.320 --> 00:48:00.260
And the question is,
was that always there?

00:48:03.010 --> 00:48:06.370
Because there are older
writers than that.

00:48:06.370 --> 00:48:11.890
You go back to Shakespeare,
pick up "Macbeth" and--

00:48:11.890 --> 00:48:14.230
I have no problem
with Shakespeare.

00:48:14.230 --> 00:48:20.410
Yes, there's
anti-Semitism to be found,

00:48:20.410 --> 00:48:26.295
and certain other somewhat
deplorable political aspects

00:48:26.295 --> 00:48:26.920
of Shakespeare.

00:48:26.920 --> 00:48:29.750
By and large, he's OK.

00:48:29.750 --> 00:48:30.530
Tolstoy's OK.

00:48:33.540 --> 00:48:35.470
Most of the women
fare much better,

00:48:35.470 --> 00:48:38.790
although not all of them.

00:48:38.790 --> 00:48:40.480
And to take a more
recent example,

00:48:40.480 --> 00:48:42.930
a lot of the writing that was
so popular and so critically

00:48:42.930 --> 00:48:44.790
privileged in the '60s.

00:48:44.790 --> 00:48:45.540
"Catch-22".

00:48:45.540 --> 00:48:49.540
"Catch-22" was this enormous
book for me, Joseph Heller.

00:48:49.540 --> 00:48:53.940
It was my model for what I
wanted to do with my novels.

00:48:53.940 --> 00:48:55.930
He was telling this crazy story.

00:48:55.930 --> 00:48:58.470
It was funny and it was
line by line wonderful

00:48:58.470 --> 00:49:02.220
and it was inventive
and it actually

00:49:02.220 --> 00:49:04.020
seemed to do something
to change the world.

00:49:04.020 --> 00:49:08.820
It was part of the cresting
of the anti-Vietnam War

00:49:08.820 --> 00:49:11.016
movement in the late
'60s and early '70s.

00:49:11.016 --> 00:49:12.390
Everybody was
reading "Catch-22".

00:49:12.390 --> 00:49:13.920
That was my model of a book.

00:49:13.920 --> 00:49:17.040
I go back and it's like,
oh dude, so sexist.

00:49:17.040 --> 00:49:20.040
So sexist.

00:49:20.040 --> 00:49:23.370
Would it have killed you
to do one female character

00:49:23.370 --> 00:49:25.260
with her own subjectivity?

00:49:25.260 --> 00:49:28.890
Apparently yes, not doable.

00:49:28.890 --> 00:49:30.990
And Pynchon was the same way.

00:49:30.990 --> 00:49:33.270
It's really-- actually,
even at the time

00:49:33.270 --> 00:49:35.100
I knew Pynchon
was that way, when

00:49:35.100 --> 00:49:37.680
I was reading "Gravity's
Rainbow" as a feminist in 1981,

00:49:37.680 --> 00:49:38.518
it was like, ooh.

00:49:42.680 --> 00:49:46.500
But there's other work
from the same time that

00:49:46.500 --> 00:49:50.250
doesn't do that, which gets
into an argument I make

00:49:50.250 --> 00:49:54.074
in this book, which is
that character is not just

00:49:54.074 --> 00:49:55.740
characters on the
page, but there's also

00:49:55.740 --> 00:49:58.350
the character of the writer.

00:49:58.350 --> 00:50:01.620
And I think the stuff
that holds up well

00:50:01.620 --> 00:50:07.930
is the writer has good
character, basically,

00:50:07.930 --> 00:50:10.030
and you can make mistakes.

00:50:10.030 --> 00:50:13.930
And yes, certain
cultural assumptions--

00:50:16.480 --> 00:50:21.970
everyone was so racist in
the 19th century and before.

00:50:21.970 --> 00:50:25.129
There's just no
way that you can--

00:50:25.129 --> 00:50:26.920
it's almost impossible
to find a writer who

00:50:26.920 --> 00:50:30.300
is completely free
of it, because there

00:50:30.300 --> 00:50:34.810
was no penalty to be paid for
letting those attitudes show.

00:50:34.810 --> 00:50:37.180
And yet within that
spectrum, some people,

00:50:37.180 --> 00:50:43.690
you can say that person
had a big soul, big heart,

00:50:43.690 --> 00:50:49.228
and that person,
bit of an asshole.

00:50:49.228 --> 00:50:52.090
Anyway.

00:50:52.090 --> 00:50:54.580
There was a sort of--

00:50:54.580 --> 00:50:58.210
like you're at an auction nod
from the gentlemen in the back.

00:50:58.210 --> 00:51:00.109
AUDIENCE: Curious how
you got into birding.

00:51:00.109 --> 00:51:01.650
JONATHAN FRANZEN:
I got into birding,

00:51:01.650 --> 00:51:04.460
it was like any
religious experience.

00:51:04.460 --> 00:51:06.548
I was converted by some friends.

00:51:06.548 --> 00:51:08.214
[LAUGHTER]

00:51:08.214 --> 00:51:09.880
And there was kind
of a Paul on the road

00:51:09.880 --> 00:51:11.950
to Damascus moment
for me, and that was

00:51:11.950 --> 00:51:15.040
seeing a veery in Central Park.

00:51:15.040 --> 00:51:17.500
I had never heard of veeries.

00:51:17.500 --> 00:51:20.260
There are 100,000 people in the
park, was a beautiful May day,

00:51:20.260 --> 00:51:23.780
and here's this
bird, utterly lovely,

00:51:23.780 --> 00:51:25.390
and one of the first
times I have ever

00:51:25.390 --> 00:51:27.056
had binoculars in my
hand looking at it,

00:51:27.056 --> 00:51:28.882
like, wow, that's
a beautiful thing.

00:51:28.882 --> 00:51:30.090
And I'm surrounded by people.

00:51:30.090 --> 00:51:31.120
What is this?

00:51:31.120 --> 00:51:32.230
No one ever told me.

00:51:32.230 --> 00:51:36.820
So yeah, it was people
taking me out and putting

00:51:36.820 --> 00:51:37.900
binoculars in my hands.

00:51:41.950 --> 00:51:45.460
It had been growing for a while,
but that was the main thing.

00:51:45.460 --> 00:51:47.650
AUDIENCE: Being here at
Google, you kind of led us

00:51:47.650 --> 00:51:51.100
into something that I've asked
myself here, working here,

00:51:51.100 --> 00:51:55.570
and that is what would your
priorities for technology be?

00:51:55.570 --> 00:51:58.270
Do you have a
vision for how could

00:51:58.270 --> 00:52:02.800
a company with a big
social reach help

00:52:02.800 --> 00:52:06.040
foster the kind of society
that we want to live in?

00:52:06.040 --> 00:52:09.880
How do we connect
people without spreading

00:52:09.880 --> 00:52:16.300
this ability for the platform
to be misused by people who

00:52:16.300 --> 00:52:19.270
are spreading misinformation?

00:52:19.270 --> 00:52:21.250
It turns out, from
what I see, it's

00:52:21.250 --> 00:52:25.300
a technologically
difficult problem to solve.

00:52:25.300 --> 00:52:27.040
JONATHAN FRANZEN:
It absolutely is.

00:52:27.040 --> 00:52:29.680
I'd like to mostly slip
away with a promise

00:52:29.680 --> 00:52:32.530
to go home and think about
that, because it's become--

00:52:32.530 --> 00:52:35.200
I've been such a tech
critic for so long--

00:52:35.200 --> 00:52:37.127
and not even a particularly
well-informed one,

00:52:37.127 --> 00:52:39.460
just a kind of intuitive one,
which gets me into trouble

00:52:39.460 --> 00:52:42.640
when I overstate my case.

00:52:42.640 --> 00:52:47.530
But it's true that I perhaps
don't think enough about what

00:52:47.530 --> 00:52:50.410
I wish it would look like.

00:52:53.230 --> 00:52:54.140
There are so many--

00:52:54.140 --> 00:52:56.320
I mean, first of all,
let's talk about-- there's

00:52:56.320 --> 00:53:03.670
so many great things that you
guys do in terms of access

00:53:03.670 --> 00:53:09.490
to information,
access to products.

00:53:09.490 --> 00:53:11.260
I'm not a buying guy,
I'm not a consumer,

00:53:11.260 --> 00:53:12.635
but every once in
a while there's

00:53:12.635 --> 00:53:14.740
something hard to find
that I would like to find,

00:53:14.740 --> 00:53:23.120
and Kathy, my spouse equivalent,
had some old dinner plates

00:53:23.120 --> 00:53:25.580
and they were getting chipped
and we really liked them,

00:53:25.580 --> 00:53:30.330
and I got them at Pier
One, like, in 1992.

00:53:30.330 --> 00:53:35.540
And sure enough,
eBay, there they were.

00:53:35.540 --> 00:53:37.540
I had to go through like
four different sellers,

00:53:37.540 --> 00:53:40.510
but I was able to assemble
this what turned out to be--

00:53:40.510 --> 00:53:42.010
I'm not sure if she
liked it-- but I

00:53:42.010 --> 00:53:44.110
thought it was a
terrific birthday

00:53:44.110 --> 00:53:48.970
present for her for a big
birthday she just had.

00:53:48.970 --> 00:53:51.730
And-- small thing--

00:53:51.730 --> 00:53:54.400
what it's doing just in terms
of accelerating certain kinds

00:53:54.400 --> 00:53:56.200
of science,
particularly, science

00:53:56.200 --> 00:53:58.110
with citizen participation.

00:53:58.110 --> 00:54:01.197
It's fantastic.

00:54:01.197 --> 00:54:03.280
I think email is a superior
form of communication.

00:54:03.280 --> 00:54:06.400
I know it's gotten
out of hand, and I'm--

00:54:06.400 --> 00:54:08.350
could go on and on and on.

00:54:08.350 --> 00:54:11.560
I don't deny the blessing
the tech brings us.

00:54:11.560 --> 00:54:20.610
I feel as if policy-wise, one
of the biggest problems we face

00:54:20.610 --> 00:54:27.480
is that the DNA of Silicon
Valley is so anti-regulation,

00:54:27.480 --> 00:54:31.650
it really comes out of a sort
of '60s and '70s hippie distrust

00:54:31.650 --> 00:54:34.110
of the government, and that
has morphed, in many cases,

00:54:34.110 --> 00:54:36.990
into something closer to a
right wing libertarianism.

00:54:36.990 --> 00:54:39.960
But there's such hostility to
government, such deep distrust

00:54:39.960 --> 00:54:45.570
of government, even as in terms
of power, reach, influence,

00:54:45.570 --> 00:54:47.700
and wealth, the
industry has begun

00:54:47.700 --> 00:54:51.990
to rival the government
in surveillant power

00:54:51.990 --> 00:54:55.110
and in power to
shape public opinion.

00:54:55.110 --> 00:54:57.710
Still there is such a deep
distrust of government

00:54:57.710 --> 00:55:01.100
that it's just
almost unthinkable

00:55:01.100 --> 00:55:04.740
to ask to be regulated, but
something like Facebook, to me,

00:55:04.740 --> 00:55:06.030
cries out for regulation.

00:55:08.850 --> 00:55:11.100
And something like
Amazon cries out

00:55:11.100 --> 00:55:15.210
for quick antitrust work on the
part of the Justice Department.

00:55:15.210 --> 00:55:17.580
There are things that
I think should happen,

00:55:17.580 --> 00:55:18.480
and even though--

00:55:21.240 --> 00:55:25.530
so it's on the one hand, the
part of the problem is a like,

00:55:25.530 --> 00:55:28.890
in your bones resistance to any
form of government intervention

00:55:28.890 --> 00:55:34.800
and distrust of government,
but also almost anything done

00:55:34.800 --> 00:55:36.810
to impede what's
happening now is going

00:55:36.810 --> 00:55:41.300
to have an effect on profits.

00:55:41.300 --> 00:55:48.530
And Sherry Turkle
talks about something

00:55:48.530 --> 00:55:51.830
that has begun to be discussed
a little bit in the social media

00:55:51.830 --> 00:55:54.230
world, building in features
that encourage people

00:55:54.230 --> 00:55:57.020
to disconnect rather
than to addictively

00:55:57.020 --> 00:56:01.250
stay connected 60 hours a week.

00:56:04.010 --> 00:56:07.920
Laughable idea,
because why would you--

00:56:07.920 --> 00:56:11.250
why would you want to
reduce the stream of data

00:56:11.250 --> 00:56:18.035
that your users are providing
as they remain online?

00:56:20.710 --> 00:56:25.580
And yet even from like a
public health standpoint,

00:56:25.580 --> 00:56:27.280
especially for
children, it seems

00:56:27.280 --> 00:56:29.830
like you have to start doing
that, but it's going to cost.

00:56:29.830 --> 00:56:32.060
And that's really,
really-- that to me,

00:56:32.060 --> 00:56:38.200
is the real issue is that is
that Silicon Valley has become

00:56:38.200 --> 00:56:42.470
a shareholder-serving operation.

00:56:42.470 --> 00:56:44.930
And as long as--

00:56:44.930 --> 00:56:47.445
so it's hard for me to--

00:56:47.445 --> 00:56:49.820
I'm not even sure I would want
this, but it's hard for me

00:56:49.820 --> 00:56:51.903
to imagine really radical
change without something

00:56:51.903 --> 00:56:58.620
like nationalization, where it's
public ownership of at least

00:56:58.620 --> 00:57:03.290
two or three of the big five,
treat them as public utilities,

00:57:03.290 --> 00:57:06.180
and yeah, OK, and now there
are going to be certain rules,

00:57:06.180 --> 00:57:08.080
and yes it's not going
to be as much money.

00:57:08.080 --> 00:57:11.550
There's also not going to be as
much surveillance and so forth.

00:57:17.080 --> 00:57:19.525
And I am going to stop after
uttering this sentence.

00:57:24.750 --> 00:57:27.990
I think there are many ways
in which tech is already

00:57:27.990 --> 00:57:34.580
a force for good, in that it
is fundamentally an information

00:57:34.580 --> 00:57:36.050
business.

00:57:36.050 --> 00:57:39.500
And information, if it's handled
right and used appropriately,

00:57:39.500 --> 00:57:42.650
is a very powerful
and wonderful thing,

00:57:42.650 --> 00:57:51.470
and that the main obstacle to it
becoming a true force for good

00:57:51.470 --> 00:57:56.240
is that profit motive.

00:57:56.240 --> 00:57:57.590
Thank you very much.

00:57:57.590 --> 00:58:01.540
[APPLAUSE]

