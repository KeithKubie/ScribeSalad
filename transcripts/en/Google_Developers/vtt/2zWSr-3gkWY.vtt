WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.416
[MUSIC PLAYING]

00:00:08.777 --> 00:00:10.110
ANDREW GASPAROVIC: Hi, everyone.

00:00:10.110 --> 00:00:12.110
My name is Andrew Gasparovic.

00:00:12.110 --> 00:00:15.860
I am an engineer at Google
Research Europe in Zurich,

00:00:15.860 --> 00:00:19.130
working on infrastructure
for machine intelligence.

00:00:19.130 --> 00:00:21.770
And today, I'm excited to give
you an introduction to machine

00:00:21.770 --> 00:00:24.960
learning using TensorFlow.

00:00:24.960 --> 00:00:28.290
I'll start by introducing you
to TensorFlow itself, and then

00:00:28.290 --> 00:00:31.740
show you some examples of how
we've been using it at Google.

00:00:31.740 --> 00:00:34.500
And I'll share with you
a few recent and upcoming

00:00:34.500 --> 00:00:36.300
developments, and
then talk about how

00:00:36.300 --> 00:00:42.310
you can get started solving real
problems with machine learning.

00:00:42.310 --> 00:00:44.850
So first, let's talk
about exactly what

00:00:44.850 --> 00:00:50.430
it is that TensorFlow does and
why you might want to use it.

00:00:50.430 --> 00:00:53.250
TensorFlow lets you
get straight to work

00:00:53.250 --> 00:00:56.670
solving all kinds of
machine learning tasks.

00:00:56.670 --> 00:01:00.540
The goal is that, in general, no
matter what your problem looks

00:01:00.540 --> 00:01:03.490
like TensorFlow should
be able to support it

00:01:03.490 --> 00:01:05.880
at some level of the API.

00:01:05.880 --> 00:01:09.120
And in general, it's
designed to be fast,

00:01:09.120 --> 00:01:13.110
so it's optimized for the
hardware and the platforms

00:01:13.110 --> 00:01:15.970
that you actually
want to use it with.

00:01:15.970 --> 00:01:17.620
One of the things
that I think really

00:01:17.620 --> 00:01:19.990
makes it unique,
in terms of machine

00:01:19.990 --> 00:01:22.690
learning frameworks, is
that you can actually

00:01:22.690 --> 00:01:27.700
build a model in, let's say,
five or 10 lines of code,

00:01:27.700 --> 00:01:30.670
and then take that
model and scale it

00:01:30.670 --> 00:01:32.390
all the way up to production.

00:01:32.390 --> 00:01:36.850
So you can train that
model on a cluster of tens

00:01:36.850 --> 00:01:40.600
or even hundreds of machines,
and then take that model

00:01:40.600 --> 00:01:45.800
and serve it doing super
low latency predictions.

00:01:45.800 --> 00:01:48.130
So let's have a little
discussion about what

00:01:48.130 --> 00:01:51.250
it is that I mean when I
say model, specifically,

00:01:51.250 --> 00:01:53.830
and how machine learning
relates to that.

00:01:56.550 --> 00:01:58.140
Here's a simple problem.

00:01:58.140 --> 00:02:02.280
Predict whether an image
contains a cat or a dog.

00:02:02.280 --> 00:02:06.240
This is something that would be
difficult, or even impossible,

00:02:06.240 --> 00:02:09.330
to do with traditional
programming,

00:02:09.330 --> 00:02:13.920
because how do you make a
set of rules that define

00:02:13.920 --> 00:02:16.170
what is a cat versus a dog?

00:02:16.170 --> 00:02:17.640
And then on top of
that, how do you

00:02:17.640 --> 00:02:22.380
capture all of the variations
like breeds, poses,

00:02:22.380 --> 00:02:25.590
and then the brightness and
the scaling of the image, all

00:02:25.590 --> 00:02:27.790
of that sort of thing?

00:02:27.790 --> 00:02:31.920
So what we can do instead
is make a neural network,

00:02:31.920 --> 00:02:36.000
which is like an extremely
simplified version of how

00:02:36.000 --> 00:02:38.520
neurons in the brain work.

00:02:38.520 --> 00:02:41.610
Each one of the dots in
this image is a neuron

00:02:41.610 --> 00:02:44.700
and they're all connected
together, layer by layer,

00:02:44.700 --> 00:02:49.710
from the input of what we
see to the output of what

00:02:49.710 --> 00:02:52.020
we understand.

00:02:52.020 --> 00:02:54.510
And then what we do is, we
go through lots and lots

00:02:54.510 --> 00:02:57.780
of examples of cat
and dog images.

00:02:57.780 --> 00:03:01.140
They're all labeled with
the correct category.

00:03:01.140 --> 00:03:04.080
And we try to make a prediction.

00:03:04.080 --> 00:03:09.100
Initially all of those neurons
are just randomly initialized,

00:03:09.100 --> 00:03:12.010
so it's a complete guess.

00:03:12.010 --> 00:03:15.120
What we do is, we
calculate how far

00:03:15.120 --> 00:03:18.120
we are from the guess
to the correct answer--

00:03:18.120 --> 00:03:21.360
the error-- and then
we use that error

00:03:21.360 --> 00:03:24.390
to adjust the strength
of the connections

00:03:24.390 --> 00:03:26.070
between the neurons.

00:03:26.070 --> 00:03:31.410
And basically, we want to slowly
move towards the right answer.

00:03:31.410 --> 00:03:35.550
After we repeat that a million
or so times, let's say,

00:03:35.550 --> 00:03:39.120
then what you end up with is
a nice cat and dog prediction

00:03:39.120 --> 00:03:41.370
model.

00:03:41.370 --> 00:03:45.500
But what you actually want
to do is build a cat and dog

00:03:45.500 --> 00:03:47.060
prediction website, right?

00:03:47.060 --> 00:03:50.840
A user gives you
a photo, and you

00:03:50.840 --> 00:03:53.840
have to tell them whether
there is a cat or a dog in it.

00:03:53.840 --> 00:03:56.870
And the connection
strengths that you

00:03:56.870 --> 00:04:00.710
learned during training are
what allows your model now

00:04:00.710 --> 00:04:03.080
to generalize.

00:04:03.080 --> 00:04:06.050
So if you give it this
photo, even though it's never

00:04:06.050 --> 00:04:08.990
seen it before and there's
no label attached to it,

00:04:08.990 --> 00:04:11.750
it can get a dog
prediction out based

00:04:11.750 --> 00:04:15.800
on what the model has learned
via those weights attached

00:04:15.800 --> 00:04:20.450
in the neurons about the
nature of cats versus dogs--

00:04:20.450 --> 00:04:24.620
at least in terms of the
images that it's seen.

00:04:24.620 --> 00:04:26.600
But how much it
can actually learn

00:04:26.600 --> 00:04:30.770
is a function of the model
size and the complexity.

00:04:30.770 --> 00:04:35.360
And we just didn't have
computer power and the tools

00:04:35.360 --> 00:04:38.750
to experiment with really
big and complicated models

00:04:38.750 --> 00:04:40.590
until very recently.

00:04:40.590 --> 00:04:44.060
This picture is basically
what neural networks

00:04:44.060 --> 00:04:47.520
used to look like, maybe
five or 10 years ago.

00:04:47.520 --> 00:04:50.240
And at that point, they had
a small number of neurons.

00:04:50.240 --> 00:04:53.360
They were just fully
connected between the layers.

00:04:53.360 --> 00:04:55.010
There weren't that many layers.

00:04:55.010 --> 00:04:58.880
And the end result was,
they weren't super powerful.

00:04:58.880 --> 00:05:01.880
In fact, for a problem
like computer vision,

00:05:01.880 --> 00:05:07.160
they were almost written off
compared to a specialized, hand

00:05:07.160 --> 00:05:13.430
tune model that was built by
experts for that exact task.

00:05:13.430 --> 00:05:16.910
Now you can compare that
to a neural network model

00:05:16.910 --> 00:05:19.570
that we use today for
image classification.

00:05:19.570 --> 00:05:21.410
This is called Inception.

00:05:21.410 --> 00:05:23.780
And the idea is that
you feed in an image

00:05:23.780 --> 00:05:25.820
and you get a
prediction of what's

00:05:25.820 --> 00:05:28.940
in the image among
thousands of categories--

00:05:28.940 --> 00:05:34.450
I think it's like 17,000
potential classes.

00:05:34.450 --> 00:05:37.030
And with a framework
like TensorFlow,

00:05:37.030 --> 00:05:41.590
you can train a model like
this that has tons of layers

00:05:41.590 --> 00:05:44.890
and is much more complicated
than the early networks.

00:05:44.890 --> 00:05:48.270
That's what we mean when
we say deep learning.

00:05:48.270 --> 00:05:51.190
The deep, in this case,
refers to a deeper arrangement

00:05:51.190 --> 00:05:55.420
of layers, and the more
complicated connections

00:05:55.420 --> 00:05:56.890
that comes with that.

00:05:56.890 --> 00:06:00.850
The end result is that you
have millions or even billions

00:06:00.850 --> 00:06:03.490
of neurons in your model.

00:06:03.490 --> 00:06:06.700
And that's what allows
a deep neural network

00:06:06.700 --> 00:06:13.600
to get results that can actually
vastly outperform the earlier

00:06:13.600 --> 00:06:15.105
hand-built, hand tune models.

00:06:18.290 --> 00:06:20.720
But the specific
reason why TensorFlow

00:06:20.720 --> 00:06:24.320
is so efficient at working
with those huge networks

00:06:24.320 --> 00:06:27.050
is because it turns
the code that you write

00:06:27.050 --> 00:06:29.670
into a graph of operations.

00:06:29.670 --> 00:06:32.640
And it's the graph
that it actually runs.

00:06:32.640 --> 00:06:35.750
The data, by the way, that
flows between those operations

00:06:35.750 --> 00:06:40.390
are called tensors, which is
where the name comes from.

00:06:40.390 --> 00:06:43.640
And because your model is
represented as a graph,

00:06:43.640 --> 00:06:47.300
you can do things like
delaying or removing

00:06:47.300 --> 00:06:52.250
unnecessary operations, or
even reusing partial results.

00:06:52.250 --> 00:06:54.620
The other thing that
you can do really

00:06:54.620 --> 00:06:58.400
easily is a process that's
called back propagation.

00:06:58.400 --> 00:07:01.040
So if you remember,
when we updated

00:07:01.040 --> 00:07:05.030
the strength of connections in
our model based on the examples

00:07:05.030 --> 00:07:08.150
that we saw, and the
error that we calculated,

00:07:08.150 --> 00:07:11.390
that's the process
of back propagation.

00:07:11.390 --> 00:07:14.600
Because the model is represented
as a graph of operations

00:07:14.600 --> 00:07:17.371
instead of code, you don't
have to write additional code

00:07:17.371 --> 00:07:17.870
for that.

00:07:17.870 --> 00:07:22.410
You can just compute and apply
those updates automatically.

00:07:22.410 --> 00:07:25.410
And another nice side effect
of having a graph around

00:07:25.410 --> 00:07:29.160
is that you can, in your
code, say using a one line

00:07:29.160 --> 00:07:32.832
declaration, I want this part
of the graph to run over here.

00:07:32.832 --> 00:07:34.290
I want this other
part of the graph

00:07:34.290 --> 00:07:37.530
to be distributed to a
different set of machines.

00:07:37.530 --> 00:07:41.340
Or you could even say, I want
this part of the graph that's

00:07:41.340 --> 00:07:44.280
very math intensive
to run on a GPU,

00:07:44.280 --> 00:07:50.220
while the data input code
runs back on the CPU.

00:07:50.220 --> 00:07:55.510
And TensorFlow runs on CPUs
and GPUs out of the box.

00:07:55.510 --> 00:08:00.820
It also can load models
and run inference tasks

00:08:00.820 --> 00:08:05.710
like doing a prediction
or a classification on iOS

00:08:05.710 --> 00:08:11.320
and Android devices, and now
even a Raspberry Pi device.

00:08:11.320 --> 00:08:13.570
And then inside of
our data centers,

00:08:13.570 --> 00:08:16.270
we've been serving
TensorFlow graphs

00:08:16.270 --> 00:08:20.260
using this specially built
hardware that we call a Tensor

00:08:20.260 --> 00:08:23.340
Processing Unit, or TPU.

00:08:23.340 --> 00:08:27.750
So doing that back propagation
and then applying forward

00:08:27.750 --> 00:08:31.080
in the network the strengths of
connections between the neurons

00:08:31.080 --> 00:08:32.280
for each layer--

00:08:32.280 --> 00:08:37.230
that is basically very large
matrix math operations.

00:08:37.230 --> 00:08:41.520
And that's something that
TPUs do a lot of very quickly,

00:08:41.520 --> 00:08:43.470
very well.

00:08:43.470 --> 00:08:48.060
Version two of the TPU hardware
we're calling the Cloud TPU,

00:08:48.060 --> 00:08:51.350
and I will talk a little bit
more about that in a bit.

00:08:54.370 --> 00:08:59.340
So once upon a time, Python
was basically the only choice

00:08:59.340 --> 00:09:03.300
if you wanted to build
a TensorFlow graph.

00:09:03.300 --> 00:09:05.250
And it's still a
perfectly great choice.

00:09:05.250 --> 00:09:06.610
It's very simple.

00:09:06.610 --> 00:09:08.880
There's a lot of
example code out there.

00:09:08.880 --> 00:09:11.370
It supports everything
out of the box.

00:09:11.370 --> 00:09:15.120
But there's also support
for a whole variety

00:09:15.120 --> 00:09:17.110
of other languages.

00:09:17.110 --> 00:09:19.800
And since TensorFlow
is open source,

00:09:19.800 --> 00:09:23.760
there are a lot more
additional language choices

00:09:23.760 --> 00:09:27.720
with community support that
are being added all the time.

00:09:27.720 --> 00:09:31.000
And so the end result is
that if you're interested,

00:09:31.000 --> 00:09:34.620
you can try out
TensorFlow in, probably,

00:09:34.620 --> 00:09:36.240
your favorite
language right now,

00:09:36.240 --> 00:09:39.680
and it'll work out of the box

00:09:39.680 --> 00:09:42.980
And I just wanted to mention,
my coworkers on the TensorFlow

00:09:42.980 --> 00:09:47.790
Serving team just announced
their 1.0 release last month,

00:09:47.790 --> 00:09:50.840
which is a really huge
milestone for them

00:09:50.840 --> 00:09:56.240
because TensorFlow Serving is
a very, very high performance

00:09:56.240 --> 00:10:00.650
piece of infrastructure that
you can use to load your models

00:10:00.650 --> 00:10:05.750
and then serve inference
requests, low latency,

00:10:05.750 --> 00:10:08.180
on your servers.

00:10:08.180 --> 00:10:12.410
Internally, we use it
for about 800 products.

00:10:12.410 --> 00:10:16.430
But we really wanted to release
it as part of the open source

00:10:16.430 --> 00:10:20.720
distribution because it's
such an important aspect

00:10:20.720 --> 00:10:23.660
of a real world deployment.

00:10:23.660 --> 00:10:25.520
That's one of the
things that, when

00:10:25.520 --> 00:10:27.410
we say TensorFlow
is production ready,

00:10:27.410 --> 00:10:31.130
it's things like this that make
the difference between code

00:10:31.130 --> 00:10:34.490
that you write for
doing research and being

00:10:34.490 --> 00:10:37.200
able to actually run
it in production,

00:10:37.200 --> 00:10:40.430
solving real problems.

00:10:40.430 --> 00:10:43.990
Another one is a tool
called Tensor Board.

00:10:43.990 --> 00:10:45.880
So this is one of
the visualizers

00:10:45.880 --> 00:10:48.070
that's included in the package.

00:10:48.070 --> 00:10:50.500
This particular one is
showing a clustering

00:10:50.500 --> 00:10:54.490
of hand-written digits
that a model learned

00:10:54.490 --> 00:10:56.830
for that particular task.

00:10:56.830 --> 00:10:59.650
In general, visualizing
what's happening in a model,

00:10:59.650 --> 00:11:02.980
and then trying to
debug predictions

00:11:02.980 --> 00:11:06.220
that you get out of
it, has traditionally

00:11:06.220 --> 00:11:09.490
been a very difficult
aspect of machine learning.

00:11:09.490 --> 00:11:16.450
It's kind of-- one of the
Achilles heels of a lot

00:11:16.450 --> 00:11:19.230
of machine learning frameworks.

00:11:19.230 --> 00:11:21.317
And so just like with
TensorFlow Serving,

00:11:21.317 --> 00:11:22.900
that's something
that we really wanted

00:11:22.900 --> 00:11:25.870
to include because that--

00:11:25.870 --> 00:11:28.120
you wouldn't be able to
serve something in production

00:11:28.120 --> 00:11:30.550
unless you were able
to actually understand

00:11:30.550 --> 00:11:32.860
what's going on
inside of the model

00:11:32.860 --> 00:11:35.350
and figure out if it
made a prediction that

00:11:35.350 --> 00:11:41.480
doesn't match what you
expected, why that's happening.

00:11:41.480 --> 00:11:44.410
And so that production
readiness in general, I think,

00:11:44.410 --> 00:11:47.440
has really been one of
the keys to the success

00:11:47.440 --> 00:11:51.450
of the framework, and one of the
things that makes it different.

00:11:51.450 --> 00:11:56.920
TensorFlow, since its release,
has been the number one machine

00:11:56.920 --> 00:12:00.900
learning repository on GitHub.

00:12:00.900 --> 00:12:03.150
And it's really been
incredible to see the adoption

00:12:03.150 --> 00:12:04.860
since it was released.

00:12:04.860 --> 00:12:08.400
This chart shows the
number of stars on GitHub

00:12:08.400 --> 00:12:09.680
since it was launched.

00:12:09.680 --> 00:12:12.000
And the last time
I checked it was,

00:12:12.000 --> 00:12:16.970
I think over 68,000
at this point.

00:12:16.970 --> 00:12:19.310
And I think one of
the other reasons

00:12:19.310 --> 00:12:23.480
is because we actually take
our place in the open source

00:12:23.480 --> 00:12:27.080
community really seriously.

00:12:27.080 --> 00:12:29.960
It's never been,
for us, a matter

00:12:29.960 --> 00:12:34.820
of throwing code over
the wall or taking

00:12:34.820 --> 00:12:37.640
a dump from our
source code repository

00:12:37.640 --> 00:12:41.660
and just open sourcing that
and that's the end of it.

00:12:41.660 --> 00:12:44.600
Open source and open
source contributors

00:12:44.600 --> 00:12:48.170
have been a totally first
class part of the process

00:12:48.170 --> 00:12:50.120
since it was released.

00:12:50.120 --> 00:12:52.730
At this point we've
had more than 1,000

00:12:52.730 --> 00:12:55.670
external contributors
on TensorFlow.

00:12:55.670 --> 00:12:57.620
And some of those
external commits

00:12:57.620 --> 00:13:01.550
have added huge new features,
like additional languages,

00:13:01.550 --> 00:13:04.430
as I mentioned earlier,
additional hardware

00:13:04.430 --> 00:13:06.850
support, and even
whole new platforms

00:13:06.850 --> 00:13:09.930
that TensorFlow runs on.

00:13:09.930 --> 00:13:14.020
And the other aspect
of our open source work

00:13:14.020 --> 00:13:18.970
is making sure that users are
productive and informed about

00:13:18.970 --> 00:13:21.130
how best to use TensorFlow.

00:13:21.130 --> 00:13:24.460
So to do that, we've answered
thousands of questions

00:13:24.460 --> 00:13:26.920
on stack overflow.

00:13:26.920 --> 00:13:31.360
And we are also very serious
about looking into and fixing

00:13:31.360 --> 00:13:34.900
issues on our
GitHub issues page,

00:13:34.900 --> 00:13:36.670
because in general,
we want to have

00:13:36.670 --> 00:13:39.520
a really seamless
experience from the time

00:13:39.520 --> 00:13:42.130
that you download the
framework to the time

00:13:42.130 --> 00:13:46.320
that you actually launch
a model in production.

00:13:46.320 --> 00:13:52.620
But just to be clear, we use
TensorFlow a lot within Google.

00:13:52.620 --> 00:13:56.220
This graph shows the
number of directories

00:13:56.220 --> 00:14:02.100
in our source control tree
with models in them over time.

00:14:02.100 --> 00:14:06.600
And the orange bar is when we
internally released TensorFlow

00:14:06.600 --> 00:14:09.420
for all projects to use.

00:14:09.420 --> 00:14:12.600
So you can see before that,
there was interest in machine

00:14:12.600 --> 00:14:15.270
learning, maybe there was some
people white listed or using

00:14:15.270 --> 00:14:18.780
our precursor framework.

00:14:18.780 --> 00:14:21.900
And then after it was
released, it just exploded.

00:14:21.900 --> 00:14:25.320
And there are more
than 6,000 products

00:14:25.320 --> 00:14:29.620
at Google using
TensorFlow today.

00:14:29.620 --> 00:14:32.820
And in fact, it's pretty much
every major Google product

00:14:32.820 --> 00:14:35.640
is using TensorFlow and
doing machine learning

00:14:35.640 --> 00:14:38.200
in some form or another.

00:14:38.200 --> 00:14:42.600
And that has given us a ton
of feedback and opportunities

00:14:42.600 --> 00:14:45.090
to make TensorFlow
better by doing things

00:14:45.090 --> 00:14:48.360
like streamlining the
APIs that we provide,

00:14:48.360 --> 00:14:53.980
adding new high level APIs over
time to make it easier to use,

00:14:53.980 --> 00:14:59.580
and also just providing some
of the production ready tools

00:14:59.580 --> 00:15:01.879
that I mentioned.

00:15:01.879 --> 00:15:03.420
So let me show you
some of the things

00:15:03.420 --> 00:15:06.960
that we've been specifically
using TensorFlow for,

00:15:06.960 --> 00:15:11.280
because there's so much variety
in the types of problems.

00:15:11.280 --> 00:15:15.150
It's, I think, a good
demonstration of how

00:15:15.150 --> 00:15:19.610
flexible it is as a framework.

00:15:19.610 --> 00:15:25.190
Google Translate used to use a
model that basically translated

00:15:25.190 --> 00:15:28.550
word by word-- maybe a few
phrases here and there.

00:15:28.550 --> 00:15:31.610
But that was basically
the extent of it.

00:15:31.610 --> 00:15:34.010
And then on top of that, it
had hundreds of thousands

00:15:34.010 --> 00:15:37.430
of lines of hand
tuned code written

00:15:37.430 --> 00:15:41.630
with the input of linguists
and language experts.

00:15:41.630 --> 00:15:44.870
Even so, it had a
lot of difficulty

00:15:44.870 --> 00:15:47.900
accommodating all of the
nuances and differences

00:15:47.900 --> 00:15:49.940
in human languages.

00:15:49.940 --> 00:15:52.340
And so here's an
example, on the right,

00:15:52.340 --> 00:15:56.150
of translating a particular
Chinese phrase into,

00:15:56.150 --> 00:15:58.340
"where will the restroom?"

00:15:58.340 --> 00:16:02.700
Which leaves a lot of
room for improvement.

00:16:02.700 --> 00:16:06.350
We've replaced that
entire previous system

00:16:06.350 --> 00:16:09.920
with a new deep neural
network based system

00:16:09.920 --> 00:16:12.120
called neural
machine translation.

00:16:12.120 --> 00:16:14.660
And that's running
on TensorFlow.

00:16:14.660 --> 00:16:17.450
And the end result is that
many of the language pairs

00:16:17.450 --> 00:16:21.800
have had huge gains in
translation quality, up to 85%

00:16:21.800 --> 00:16:23.300
in some cases.

00:16:23.300 --> 00:16:25.820
And the reason why
is because the model

00:16:25.820 --> 00:16:29.510
works by considering a whole
sequence of words, a sequence

00:16:29.510 --> 00:16:31.740
input and a sequence output.

00:16:31.740 --> 00:16:34.760
And so the end result is that
you get a more natural sounding

00:16:34.760 --> 00:16:39.290
output, much more like a
human translator would do.

00:16:39.290 --> 00:16:42.760
For instance here, "excuse
me, where is the toilet?"

00:16:42.760 --> 00:16:47.220
A much better result. And
sticking with that translation

00:16:47.220 --> 00:16:52.830
theme, we added the word lens
feature to the translate app.

00:16:52.830 --> 00:16:56.730
And this is actually
running on a mobile device.

00:16:56.730 --> 00:16:59.340
It works in airplane mode,
which is pretty incredible

00:16:59.340 --> 00:17:01.830
considering that it's
doing, basically,

00:17:01.830 --> 00:17:04.589
a combination of computer
vision and translation

00:17:04.589 --> 00:17:07.290
all in the same model.

00:17:07.290 --> 00:17:10.079
We had to add
features to TensorFlow

00:17:10.079 --> 00:17:13.890
specifically to make
things like that possible.

00:17:13.890 --> 00:17:17.369
And now, you can train
a model on a cluster

00:17:17.369 --> 00:17:18.940
of servers or one machine--

00:17:18.940 --> 00:17:20.579
however you would
do it normally--

00:17:20.579 --> 00:17:23.609
but then take that model
and reduce the size of it

00:17:23.609 --> 00:17:27.150
to fit on the device while
keeping the quality high.

00:17:30.330 --> 00:17:32.520
And then Google
Photos is an example

00:17:32.520 --> 00:17:35.610
of an already great
product that was

00:17:35.610 --> 00:17:40.330
enhanced by adding a machine
learning functionality to it.

00:17:40.330 --> 00:17:42.570
So in about six
months time, the team

00:17:42.570 --> 00:17:46.860
took that inception based
image classification system

00:17:46.860 --> 00:17:49.380
and got it working
live in Google Photos.

00:17:49.380 --> 00:17:54.690
And the idea is that you can
take a term and search for it

00:17:54.690 --> 00:17:57.040
in your photos for
pretty much anything.

00:17:57.040 --> 00:18:00.390
Like, you can type in beach
and get photos of beaches.

00:18:00.390 --> 00:18:02.460
You can type in umbrella
and get a photo that

00:18:02.460 --> 00:18:04.570
contains an umbrella in it.

00:18:04.570 --> 00:18:09.280
Or even an abstract term like
sunny, without previously,

00:18:09.280 --> 00:18:12.180
having added those
tags to your photos.

00:18:15.210 --> 00:18:18.630
A more difficult
image based task,

00:18:18.630 --> 00:18:21.720
using another deep neural
network, is this show

00:18:21.720 --> 00:18:24.540
and tell model from
Google Research.

00:18:24.540 --> 00:18:27.900
It takes an image
input and it outputs

00:18:27.900 --> 00:18:30.480
a human sounding caption.

00:18:30.480 --> 00:18:32.920
It also starts with
that inception model,

00:18:32.920 --> 00:18:35.910
but in this case, it's not
just classifying the objects

00:18:35.910 --> 00:18:37.170
that appear in the image.

00:18:37.170 --> 00:18:39.330
It's actually writing
a caption that

00:18:39.330 --> 00:18:43.470
sounds natural and captures the
relationship between objects

00:18:43.470 --> 00:18:45.640
that appear in the image.

00:18:45.640 --> 00:18:49.560
To do that, the
model was fine tuned

00:18:49.560 --> 00:18:54.900
on examples of images that
had human generated captions.

00:18:54.900 --> 00:18:57.690
And from that, it learned
about relationships.

00:18:57.690 --> 00:18:59.950
As a side effect
of that process,

00:18:59.950 --> 00:19:02.730
the model actually got
better at describing details

00:19:02.730 --> 00:19:04.740
in the images like
colors, because it

00:19:04.740 --> 00:19:07.980
found that those are things
that humans want to hear

00:19:07.980 --> 00:19:11.070
in a caption that they like.

00:19:11.070 --> 00:19:12.950
And so Google
Research, by the way,

00:19:12.950 --> 00:19:16.740
open sourced the entire model
and there's an in-depth post

00:19:16.740 --> 00:19:19.350
about it on the research blog.

00:19:19.350 --> 00:19:21.840
And you can go in there
and follow the links

00:19:21.840 --> 00:19:23.655
and try it out for yourself.

00:19:26.570 --> 00:19:28.130
One of the other
things that we've

00:19:28.130 --> 00:19:30.380
been doing at Google
Research is working

00:19:30.380 --> 00:19:35.060
on diagnosing a condition called
diabetic retinopathy using

00:19:35.060 --> 00:19:37.190
computer vision.

00:19:37.190 --> 00:19:39.500
Ideally, you go to
an ophthalmologist.

00:19:39.500 --> 00:19:42.500
They take an image like
this, and they analyze it

00:19:42.500 --> 00:19:47.510
for early signs of
diabetic retinopathy.

00:19:47.510 --> 00:19:50.840
That's important, because if you
catch the disease early enough,

00:19:50.840 --> 00:19:52.430
it's easily treatable.

00:19:52.430 --> 00:19:54.530
The problem is in
the developing world,

00:19:54.530 --> 00:19:57.360
there aren't enough
ophthalmologists to go around.

00:19:57.360 --> 00:20:00.500
And so it's hard to
catch it in time.

00:20:00.500 --> 00:20:03.320
The end result is that it's
become the fastest cause

00:20:03.320 --> 00:20:06.020
of blindness in the world.

00:20:06.020 --> 00:20:10.430
So we published an article in
"The Journal of the American

00:20:10.430 --> 00:20:14.270
Medical Association," which
shows that a computer vision

00:20:14.270 --> 00:20:19.130
model can actually be as
good, or even slightly better

00:20:19.130 --> 00:20:22.010
than, the average
ophthalmologist

00:20:22.010 --> 00:20:24.590
at diagnosing the condition.

00:20:24.590 --> 00:20:27.320
And so that's something that
we're really excited about,

00:20:27.320 --> 00:20:29.180
because if we can get
the model out there

00:20:29.180 --> 00:20:34.040
then it'll have a real impact
to help find more of these cases

00:20:34.040 --> 00:20:35.090
before it's too late.

00:20:38.700 --> 00:20:40.950
And one last thing
from the research team

00:20:40.950 --> 00:20:46.410
is this problem of using
a deep neural network

00:20:46.410 --> 00:20:49.050
to actually learn what
kind of architectures

00:20:49.050 --> 00:20:53.110
are good for solving
different types of problems.

00:20:53.110 --> 00:20:56.100
So what we're able
to do is do a search

00:20:56.100 --> 00:20:59.850
from a poor performing
machine learning model

00:20:59.850 --> 00:21:01.440
to one that's much
more accurate,

00:21:01.440 --> 00:21:04.230
without any human
intervention in between.

00:21:04.230 --> 00:21:07.290
The model actually
builds a machine learning

00:21:07.290 --> 00:21:09.660
model that solves the task.

00:21:09.660 --> 00:21:12.960
And those sort of problems
are called learning to learn,

00:21:12.960 --> 00:21:16.700
which is a really exciting
area in the field of research.

00:21:16.700 --> 00:21:19.260
And there's going to
be a lot more happening

00:21:19.260 --> 00:21:23.700
in the next couple of
years in that area.

00:21:23.700 --> 00:21:27.810
But just before you get
the idea that TensorFlow

00:21:27.810 --> 00:21:31.590
is meant for long term research,
or is meant for big budget

00:21:31.590 --> 00:21:36.750
blockbuster apps, I wanted to
show you a Japanese cucumber

00:21:36.750 --> 00:21:37.680
farmer.

00:21:37.680 --> 00:21:40.440
His son, in the back
of the photo there,

00:21:40.440 --> 00:21:43.170
built an automatic
cucumber sorter

00:21:43.170 --> 00:21:46.230
using TensorFlow
along with an Arduino

00:21:46.230 --> 00:21:50.280
controller and a Raspberry Pi.

00:21:50.280 --> 00:21:54.390
He trained the model by showing
it 7000 examples of cucumbers

00:21:54.390 --> 00:21:56.500
in nine different categories.

00:21:56.500 --> 00:21:58.320
And so this is a
job that his mom

00:21:58.320 --> 00:22:03.480
was doing for 10 hours at a time
after every cucumber harvest.

00:22:03.480 --> 00:22:06.060
And in his words,
he said, I want

00:22:06.060 --> 00:22:08.850
to leave sorting to AI
so that we can focus more

00:22:08.850 --> 00:22:10.830
on growing good cucumbers.

00:22:10.830 --> 00:22:14.670
So after the model was
trained, he hooked it up--

00:22:14.670 --> 00:22:18.300
he hooked up a conveyor
belt to the controller,

00:22:18.300 --> 00:22:21.690
and an array of webcams
to the Raspberry Pi.

00:22:21.690 --> 00:22:24.240
And so as each cucumber
comes along the belt,

00:22:24.240 --> 00:22:26.160
it's imaged by the webcams.

00:22:26.160 --> 00:22:30.760
It's classified, and it's
sorted automatically,

00:22:30.760 --> 00:22:35.520
which I think is just a
fantastic example of something

00:22:35.520 --> 00:22:38.940
really outside the box
that you can do practically

00:22:38.940 --> 00:22:41.990
with machine learning.

00:22:41.990 --> 00:22:47.480
And so TensorFlow
was, I think, really

00:22:47.480 --> 00:22:51.410
powerful for solving all
of those sort of problems

00:22:51.410 --> 00:22:54.260
as of the 1.0 release.

00:22:54.260 --> 00:22:59.090
But there have been quite a few
new developments since then.

00:22:59.090 --> 00:23:04.160
So let me just go over a
couple of those with you now.

00:23:04.160 --> 00:23:07.730
First of all, it's gotten
a lot easier to use.

00:23:07.730 --> 00:23:10.910
So I mentioned,
TensorFlow has always

00:23:10.910 --> 00:23:13.390
been extremely
flexible with the goal

00:23:13.390 --> 00:23:15.140
of being able to solve
any sort of problem

00:23:15.140 --> 00:23:16.280
that you throw at it.

00:23:16.280 --> 00:23:21.680
But it wasn't always the
easiest to use, necessarily.

00:23:21.680 --> 00:23:26.390
It has the distributed execution
engine at the bottom there,

00:23:26.390 --> 00:23:29.720
which is what actually executes
the graph of operations

00:23:29.720 --> 00:23:32.930
and distributes them among
the processors and machines.

00:23:32.930 --> 00:23:36.560
That has pretty much
stayed the same.

00:23:36.560 --> 00:23:41.720
As of 1.0, then, we added
a layers API, with the idea

00:23:41.720 --> 00:23:45.410
that you could construct a
model without having to actually

00:23:45.410 --> 00:23:48.950
mess around with the graph
and operations directly.

00:23:48.950 --> 00:23:52.070
But you still had to build the
actual network architecture

00:23:52.070 --> 00:23:54.910
and all of the layers and
that sort of thing-- you still

00:23:54.910 --> 00:23:56.930
had to do that part by hand.

00:23:56.930 --> 00:24:00.500
And then we added on top of
that the estimator API, which

00:24:00.500 --> 00:24:04.010
is a high level
way to take a model

00:24:04.010 --> 00:24:07.520
and combined that with
inputs and do the training

00:24:07.520 --> 00:24:10.310
and evaluation process.

00:24:10.310 --> 00:24:14.300
Now as of 1.3, we
added another layer

00:24:14.300 --> 00:24:18.390
on top of that for what we
call the canned estimators.

00:24:18.390 --> 00:24:21.620
With a canned estimator, you
can create a deep neural network

00:24:21.620 --> 00:24:25.460
classifier in literally
one line of code.

00:24:25.460 --> 00:24:28.280
And then when you use the
high level estimator APIs,

00:24:28.280 --> 00:24:30.860
you get a bunch of things
for free like distributed

00:24:30.860 --> 00:24:34.250
training, automatic
snapshots, and the ability

00:24:34.250 --> 00:24:37.040
to run on a mix of
hardware that you have,

00:24:37.040 --> 00:24:40.500
like the CPUs and GPUs.

00:24:40.500 --> 00:24:42.840
You also get all of
the best guarantees

00:24:42.840 --> 00:24:44.520
that the performance
improvements

00:24:44.520 --> 00:24:47.880
that we've been making will
actually apply to your model.

00:24:47.880 --> 00:24:50.220
We've started
publishing benchmarks

00:24:50.220 --> 00:24:53.130
of different tasks running
on different combinations

00:24:53.130 --> 00:24:54.210
of hardware.

00:24:54.210 --> 00:24:56.700
And that is important
because its going

00:24:56.700 --> 00:24:59.970
to show how we're going to
continue to improve performance

00:24:59.970 --> 00:25:00.840
over time.

00:25:00.840 --> 00:25:04.920
But it's also important
because it can tell you

00:25:04.920 --> 00:25:07.230
how you should expect
TensorFlow to behave

00:25:07.230 --> 00:25:11.080
on the combination of
hardware that you have

00:25:11.080 --> 00:25:14.520
and the problem that
you're trying to solve.

00:25:14.520 --> 00:25:18.020
And as far as uncommon
configurations,

00:25:18.020 --> 00:25:22.090
here's the Cloud TPU that I
was talking about earlier.

00:25:22.090 --> 00:25:25.410
It's the second generation
tensor processing unit.

00:25:25.410 --> 00:25:27.840
The first generation,
we only used

00:25:27.840 --> 00:25:31.320
for accelerating the inference
part of machine learning.

00:25:31.320 --> 00:25:34.290
The second generation
also accelerates training.

00:25:34.290 --> 00:25:36.870
And it's a big
improvement in general,

00:25:36.870 --> 00:25:42.600
because each Cloud TPU does
180 teraflops individually,

00:25:42.600 --> 00:25:46.230
but they're meant to be
connected 64 of them at a time

00:25:46.230 --> 00:25:48.750
into these TPU pods.

00:25:48.750 --> 00:25:54.930
One pod is 11.5 petaflops,
which is just an enormous amount

00:25:54.930 --> 00:25:56.880
of operations.

00:25:56.880 --> 00:26:01.650
And the neural translation model
that I talked about earlier

00:26:01.650 --> 00:26:06.510
used to take a full day to
train on 32 of the best GPUs

00:26:06.510 --> 00:26:08.100
that we could get our hands on.

00:26:08.100 --> 00:26:11.970
And now we train it to the same
accuracy in about half a day

00:26:11.970 --> 00:26:16.380
using one eighth of
one of these TPU pods.

00:26:16.380 --> 00:26:18.402
And the cla-- oh.

00:26:18.402 --> 00:26:19.346
[APPLAUSE]

00:26:23.600 --> 00:26:25.830
The Cloud TPU, by
the way, we're making

00:26:25.830 --> 00:26:30.420
available on the Cloud
platform to external users

00:26:30.420 --> 00:26:31.480
later on in the year.

00:26:31.480 --> 00:26:34.502
And I'll have a little bit
more information about that

00:26:34.502 --> 00:26:35.460
at the end of the talk.

00:26:38.530 --> 00:26:42.370
We also need to make sure that
we're fully taking advantage

00:26:42.370 --> 00:26:46.660
of whatever hardware you give
to the machine learning task,

00:26:46.660 --> 00:26:50.170
whether it is a TPU or
the GPUs that you have,

00:26:50.170 --> 00:26:53.470
or even just the instructions
that your CPU supports.

00:26:53.470 --> 00:26:56.290
So we've been working
on a compiler that

00:26:56.290 --> 00:27:00.190
converts those graphs that
I talked about directly

00:27:00.190 --> 00:27:01.660
into assembly code.

00:27:01.660 --> 00:27:06.820
It's called XLA, which is for
Accelerated Linear Algebra.

00:27:06.820 --> 00:27:09.650
It's a requirement for
running on the TPUs,

00:27:09.650 --> 00:27:14.500
but it also runs in JIT mode
to compile a graph for CPUs

00:27:14.500 --> 00:27:17.440
and GPUs so that it
can choose exactly

00:27:17.440 --> 00:27:19.450
the right kernels
for the hardware

00:27:19.450 --> 00:27:20.921
that you have available.

00:27:20.921 --> 00:27:22.420
And then there's a
third mode that's

00:27:22.420 --> 00:27:25.390
meant for mobile,
where you can compile

00:27:25.390 --> 00:27:30.050
a model ahead of time and run
predictions on a mobile device.

00:27:30.050 --> 00:27:33.400
The advantage is that the
compiled model is much smaller,

00:27:33.400 --> 00:27:37.090
but it still has the possibility
of running more efficiently as

00:27:37.090 --> 00:27:40.620
well on the specific device.

00:27:40.620 --> 00:27:42.720
And one last note
about mobile is,

00:27:42.720 --> 00:27:46.350
we are working on
TensorFlow Light, which

00:27:46.350 --> 00:27:49.410
is a whole new runtime
that's specifically built

00:27:49.410 --> 00:27:51.810
for Android mobile devices.

00:27:51.810 --> 00:27:54.750
The idea is that you
put a very slim engine

00:27:54.750 --> 00:27:58.080
in a mobile app, which supports
exactly the hardware that you

00:27:58.080 --> 00:27:59.610
have on the device.

00:27:59.610 --> 00:28:02.370
And then it leaves out all of
the unnecessary general purpose

00:28:02.370 --> 00:28:03.180
code.

00:28:03.180 --> 00:28:07.030
When you combine that with
an XLA compiled model,

00:28:07.030 --> 00:28:09.480
you get the efficient
use of hardware

00:28:09.480 --> 00:28:11.770
with a small memory footprint.

00:28:11.770 --> 00:28:15.450
And so you can do those
on device inference tasks,

00:28:15.450 --> 00:28:18.450
like I showed earlier, as well
as something new that we're

00:28:18.450 --> 00:28:22.020
calling Federated Learning,
where you can take advantage

00:28:22.020 --> 00:28:27.240
of a model that's been trained
and is running in the Cloud

00:28:27.240 --> 00:28:31.470
somewhere while having your
own individual training

00:28:31.470 --> 00:28:33.760
data on a device.

00:28:33.760 --> 00:28:37.290
So it's never sent to the
Cloud, but you can combine those

00:28:37.290 --> 00:28:38.775
together on device.

00:28:41.630 --> 00:28:44.840
Even if you have a lot of
programming experience,

00:28:44.840 --> 00:28:47.840
I will readily admit that
getting into machine learning

00:28:47.840 --> 00:28:51.560
can be daunting-- very daunting.

00:28:51.560 --> 00:28:54.230
And one of the benefits
of trying out TensorFlow

00:28:54.230 --> 00:28:56.180
is that the stuff that
you play around with,

00:28:56.180 --> 00:28:59.360
you can eventually
actually use in production.

00:28:59.360 --> 00:29:01.850
But still, I would
like to give you

00:29:01.850 --> 00:29:05.000
some tips on how
you can get started,

00:29:05.000 --> 00:29:07.800
no matter what level you're at.

00:29:07.800 --> 00:29:11.790
First recommendation is to
start at TensorFlow.org,

00:29:11.790 --> 00:29:14.250
because there's a Getting
Started section there

00:29:14.250 --> 00:29:18.960
that has a nice hands on
introduction to TensorFlow,

00:29:18.960 --> 00:29:21.630
as well as some machine
learning tasks that you

00:29:21.630 --> 00:29:23.550
would want to do with it.

00:29:23.550 --> 00:29:25.680
And they assume some
knowledge of Python,

00:29:25.680 --> 00:29:29.300
for those particular
intro's, but that's about it.

00:29:29.300 --> 00:29:30.950
And then if you
start with those,

00:29:30.950 --> 00:29:34.100
you can progress all the
way through the tutorials

00:29:34.100 --> 00:29:37.550
to building things like a
convolutional network which

00:29:37.550 --> 00:29:40.100
is good for image
classification tasks,

00:29:40.100 --> 00:29:43.450
and a recurrent network which
is good for those language

00:29:43.450 --> 00:29:46.430
and translation tasks.

00:29:46.430 --> 00:29:49.100
And then there's also a
really interesting demo

00:29:49.100 --> 00:29:52.790
of different neural network
architectures and parameters

00:29:52.790 --> 00:29:56.240
at playground.TensorFlow.org.

00:29:56.240 --> 00:29:58.790
You can try varying
the number of letters,

00:29:58.790 --> 00:30:02.160
or the number of neurons, or
the features, the learning rate,

00:30:02.160 --> 00:30:06.110
and get an intuition about
how neural networks work

00:30:06.110 --> 00:30:11.140
by seeing the effect on a very
simple classification problem.

00:30:11.140 --> 00:30:12.310
And then once you start--

00:30:12.310 --> 00:30:16.420
once you're ready to start
building real models to use

00:30:16.420 --> 00:30:20.920
in production, I recommend
using that high level estimator

00:30:20.920 --> 00:30:24.100
and canned estimator
APIs, because you

00:30:24.100 --> 00:30:27.520
get all of those automatic
benefits like the saved

00:30:27.520 --> 00:30:30.980
and restored checkpoints,
exporting the tensor board,

00:30:30.980 --> 00:30:33.220
and the ability
to do distributed

00:30:33.220 --> 00:30:35.950
training with pretty
much no additional work.

00:30:35.950 --> 00:30:38.830
So nine times out
of 10, or even more

00:30:38.830 --> 00:30:42.190
than that at this point, the
estimators and the canned

00:30:42.190 --> 00:30:46.030
estimators are the way to go.

00:30:46.030 --> 00:30:49.200
And then of course,
by using estimators

00:30:49.200 --> 00:30:52.300
you could, if you wanted
to, move your model

00:30:52.300 --> 00:30:57.090
onto Cloud TPUs down the road,
pretty much automatically.

00:30:57.090 --> 00:31:02.910
And at g.co/TPUsignup,
there is a sign up form

00:31:02.910 --> 00:31:05.820
if you're interested in finding
out more information about

00:31:05.820 --> 00:31:06.780
those.

00:31:06.780 --> 00:31:09.660
That link is also
the place to go

00:31:09.660 --> 00:31:13.680
to find out about the
TensorFlow Research Cloud.

00:31:13.680 --> 00:31:16.510
So we're making
1,000 of those cloud

00:31:16.510 --> 00:31:21.840
TPUs available for free to
machine learning researchers,

00:31:21.840 --> 00:31:26.010
because there are so many
people that have great ideas

00:31:26.010 --> 00:31:28.860
but limited access to
the kind of hardware

00:31:28.860 --> 00:31:31.710
to do the really state
of the art research.

00:31:31.710 --> 00:31:33.630
So check out that
link as well, if you

00:31:33.630 --> 00:31:37.470
would like to apply for that.

00:31:37.470 --> 00:31:42.170
And finally, there are a
number of good online courses

00:31:42.170 --> 00:31:44.150
for machine learning.

00:31:44.150 --> 00:31:47.480
I'd recommend checking out
the Google Developers YouTube

00:31:47.480 --> 00:31:50.510
channel, and also
Udacity has a course

00:31:50.510 --> 00:31:53.450
called Deep Learning which
uses TensorFlow but really gets

00:31:53.450 --> 00:31:57.719
into the theoretical and math
background of machine learning.

00:31:57.719 --> 00:32:00.260
And if you like that course and
you want to continue with it,

00:32:00.260 --> 00:32:03.050
then it's a good step
towards getting their machine

00:32:03.050 --> 00:32:06.110
learning nano degree.

00:32:06.110 --> 00:32:07.700
I really hope,
honestly, that you

00:32:07.700 --> 00:32:10.280
do want to continue with
machine learning because it is

00:32:10.280 --> 00:32:12.290
an incredibly exciting field.

00:32:12.290 --> 00:32:13.820
It's more accessible than ever.

00:32:13.820 --> 00:32:16.910
And there is so much happening.

00:32:16.910 --> 00:32:18.950
I want to thank
you for your time,

00:32:18.950 --> 00:32:22.960
and I hope you enjoy the
rest of your conference.

