WEBVTT
Kind: captions
Language: en

00:00:00.222 --> 00:00:02.469
All right, so that brings us to
temporal difference learning.

00:00:02.469 --> 00:00:05.766
So, temporal difference learning,
the algorithm we're going to look at

00:00:05.766 --> 00:00:07.920
originally by Sutton,
is called TD Lambda.

00:00:07.920 --> 00:00:09.620
This is a little Lambda symbol.

00:00:09.620 --> 00:00:12.310
So TD stands for temporal difference.

00:00:12.310 --> 00:00:13.970
And what really it's about,

00:00:13.970 --> 00:00:17.490
is learning to make predictions
that take place over time.

00:00:17.490 --> 00:00:20.280
And so as a concrete example,
a concrete way of looking at this,

00:00:20.280 --> 00:00:24.245
we're going to look at how we can
from a sequence of states, state

00:00:24.245 --> 00:00:28.470
zero goes to state one, goes to state
two, goes to let's say a final state.

00:00:28.470 --> 00:00:29.890
And each time there's a transition,

00:00:29.890 --> 00:00:32.200
there's some reward that's
associated with that.

00:00:32.200 --> 00:00:35.690
We're going to try to predict
the expected sum of discounted

00:00:35.690 --> 00:00:37.410
rewards from that.

00:00:37.410 --> 00:00:38.630
&gt;&gt; Okay.

00:00:38.630 --> 00:00:41.890
Which is what we're trying to do anyway.

00:00:41.890 --> 00:00:44.900
&gt;&gt; Yes, so this is going to be a really
important subroutine for trying to do

00:00:44.900 --> 00:00:48.050
reinforcement learning, because we're
going to use the notion that we can

00:00:48.050 --> 00:00:54.430
predict future rewards to try to better
choose actions to generate high rewards.

00:00:54.430 --> 00:00:54.950
&gt;&gt; Okay, that's fair.

00:00:54.950 --> 00:00:57.620
&gt;&gt; All right, so for the kind of
prediction problems that we're going to

00:00:57.620 --> 00:01:00.190
be learning with temporal difference
methods, it's helpful to have

00:01:00.190 --> 00:01:03.860
a concrete example to kind of
ground out some of these terms.

00:01:03.860 --> 00:01:06.530
So, let's take this as a Markov chain.

00:01:06.530 --> 00:01:10.200
With states S1, S2, S3, S4,
S5 and SF for the final state.

00:01:10.200 --> 00:01:10.700
&gt;&gt; Okay.

00:01:12.210 --> 00:01:15.040
&gt;&gt; I've labeled each of
the transitions with the reward

00:01:15.040 --> 00:01:18.240
that happens when you make
a transition from that to the next.

00:01:18.240 --> 00:01:23.060
And this particular transition out of
S3 actually is a stochastic transition,

00:01:23.060 --> 00:01:27.310
with 90% probability going to S4, and
ten percent probability going to S5.

00:01:27.310 --> 00:01:30.080
&gt;&gt; Okay, that makes sense.

00:01:30.080 --> 00:01:32.110
&gt;&gt; And what we'd like to learn
here is a value function,

00:01:32.110 --> 00:01:35.100
a function that maps
the state to some number,

00:01:35.100 --> 00:01:40.470
where that number is set to zero for the
final state and for every other state

00:01:40.470 --> 00:01:44.930
it's the expected value of the reward
plus the discounted value of the state

00:01:44.930 --> 00:01:48.880
that we end up in, averaged across all
the different states we might end up in.

00:01:48.880 --> 00:01:50.180
&gt;&gt; Okay that makes sense.

00:01:50.180 --> 00:01:53.070
&gt;&gt; Great, okay, good,
then let's have a quiz.

00:01:53.070 --> 00:01:55.514
&gt;&gt; [LAUGH]

