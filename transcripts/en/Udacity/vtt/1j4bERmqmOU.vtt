WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:05.038
&gt;&gt; So we are going to need a learning algorithm that is more robust to

00:00:05.038 --> 00:00:10.680
non-linear separability or linear non-separability. Does that sound right?

00:00:10.680 --> 00:00:16.320
&gt;&gt; Non-linear separability. Non linear separability.

00:00:16.320 --> 00:00:17.280
&gt;&gt; Non?

00:00:17.280 --> 00:00:17.770
&gt;&gt; Yeah think of it.

00:00:17.770 --> 00:00:20.470
&gt;&gt; Left parenthesis, linear sep, spreadability left parenthesis.

00:00:20.470 --> 00:00:22.340
&gt;&gt; There we go, that's right, negating the whole phrase, very good.

00:00:22.340 --> 00:00:25.350
So and, Gradient descent is going to give us an algorithm for doing

00:00:25.350 --> 00:00:27.920
exactly that. So, what we're going to do now is

00:00:29.340 --> 00:00:32.590
think of things this way. So what we did before

00:00:32.590 --> 00:00:35.400
was we had a summation over all the different input features

00:00:35.400 --> 00:00:38.920
of the activation on that input feature times the weight,

00:00:38.920 --> 00:00:41.480
w, for that input feature. And we sum all

00:00:41.480 --> 00:00:44.450
those up and we get an activation. And then we

00:00:44.450 --> 00:00:48.010
have our estimated output as whether or not that activation

00:00:48.010 --> 00:00:50.530
is greater than or equal to zero. So let's imagine

00:00:50.530 --> 00:00:53.200
that the output is not thresholded when we're doing the training,

00:00:53.200 --> 00:00:55.350
and what we're going to do instead is try to figure out

00:00:55.350 --> 00:00:59.380
the weight so that the Not thresholded value is, as close

00:00:59.380 --> 00:01:01.810
to the target as we can. So this actually kind of brings

00:01:01.810 --> 00:01:04.459
us back to the regression story. We can define an error

00:01:04.459 --> 00:01:08.390
metric on the weight vector w. And the form of that's

00:01:08.390 --> 00:01:12.120
going to be one half, times the sum over all the data

00:01:12.120 --> 00:01:15.750
in the dataset, of what the target was supposed to be for

00:01:15.750 --> 00:01:18.790
that particular example minus what the

00:01:18.790 --> 00:01:21.120
activation actually was. Right? The activation

00:01:21.120 --> 00:01:25.320
being the dot product between the weights and the input and we're

00:01:25.320 --> 00:01:27.540
going to square that. We're going to square that error and we want to try

00:01:27.540 --> 00:01:30.350
to now minimize that. &gt; Hey Michael, can I ask you a question?

00:01:30.350 --> 00:01:31.480
&gt;&gt; Sure.

00:01:31.480 --> 00:01:32.590
&gt;&gt; Why one half of that?

00:01:32.590 --> 00:01:37.900
&gt;&gt; Mm. Yes. It turns out that it turn, in terms of

00:01:37.900 --> 00:01:40.640
minimizing the error this is just a constant and it doesn't matter.

00:01:41.690 --> 00:01:45.450
So why do we stick in a half there? Let's get back to that.

00:01:45.450 --> 00:01:45.820
&gt;&gt; Okay.

00:01:45.820 --> 00:01:47.980
&gt;&gt; Just like in the regression case we're going

00:01:47.980 --> 00:01:50.160
to fall back to calculus. Right, calculus is going to

00:01:50.160 --> 00:01:53.360
tell us how we can push around these weights,

00:01:53.360 --> 00:01:55.820
to try to push this error down. Right, so we

00:01:55.820 --> 00:01:58.100
would like to know. How does changing the weight

00:01:58.100 --> 00:02:00.810
change the error, and let's push the weight in the

00:02:00.810 --> 00:02:03.110
direction that causes the error to go down. So

00:02:03.110 --> 00:02:06.990
we're going to take the partial derivative of the, this error metric

00:02:06.990 --> 00:02:09.220
with respect to each of the individual weights, so that we'll

00:02:09.220 --> 00:02:11.390
know for each weight which way we should push it a

00:02:11.390 --> 00:02:13.980
little bit to move in the direction of the gradient. So

00:02:13.980 --> 00:02:17.730
that's the partial dif, dif, [INAUDIBLE] So that's the partial derivitive

00:02:17.730 --> 00:02:22.540
with respect to weight wi, of exactly this error measure. So

00:02:22.540 --> 00:02:25.210
to take this partial derivitive we just use the chain rule

00:02:25.210 --> 00:02:28.530
as we always do. And what is it to take the

00:02:28.530 --> 00:02:31.920
derivitive of something like this, if you have this quantity here.

00:02:31.920 --> 00:02:35.160
We take the power, move it to the front, keep this

00:02:35.160 --> 00:02:38.110
thing, and then take the derivitive of this thing. But that,

00:02:38.110 --> 00:02:40.460
so this now answers your question, Charles. Why do we put

00:02:40.460 --> 00:02:42.780
a half in there? Because down the line, it's going to be

00:02:42.780 --> 00:02:45.970
really convenient that two and the half canceled out. So, it's

00:02:45.970 --> 00:02:48.700
just going to mean that our partial derivative is going to look simpler,

00:02:48.700 --> 00:02:51.760
even though our error measure looked a little bit more complicated.

00:02:51.760 --> 00:02:56.990
So so what we're left with then, is exactly what I said,

00:02:56.990 --> 00:02:59.020
the sum over all these data points of what

00:02:59.020 --> 00:03:02.790
was inside this. Quantity here times the derivative of

00:03:02.790 --> 00:03:06.240
that, and here I expanded the a to be,

00:03:06.240 --> 00:03:09.700
the definition of the a. Now, we need to take

00:03:09.700 --> 00:03:13.720
the partial derivative with respect to weight w i

00:03:13.720 --> 00:03:15.750
of this sum that involves a bunch of the ws

00:03:15.750 --> 00:03:18.270
in it. So, when don't match the w i,

00:03:18.270 --> 00:03:21.690
that derivative is going to be zero because the, you know,

00:03:21.690 --> 00:03:24.290
changing the weight won't have any impact on it. The only

00:03:24.290 --> 00:03:27.240
place where this, changing this weight has any impact is at

00:03:27.240 --> 00:03:31.020
x of i. So that's what we end up carrying down.

00:03:31.020 --> 00:03:34.390
This summation disappears. And all that's left is just the one term

00:03:34.390 --> 00:03:37.090
that matches the weight that we care about. So this is

00:03:37.090 --> 00:03:39.320
what we're left with. Now the derivative of the error with

00:03:39.320 --> 00:03:43.000
respect to any weight w sub i. Is exactly this, this

00:03:43.000 --> 00:03:46.990
sum. The sum of the difference between the activation and the target

00:03:46.990 --> 00:03:51.130
output times the activation on that input unit

00:03:51.130 --> 00:03:54.900
&gt;&gt; You know? That looks exactly like, almost exactly

00:03:54.900 --> 00:03:57.880
like the rule that we use with the rule that we used perceptron before.

00:03:57.880 --> 00:03:59.780
&gt;&gt; It does indeed! What's the

00:03:59.780 --> 00:04:03.250
difference? Well, actually let's Let's write this

00:04:03.250 --> 00:04:05.670
down. This is now just a derivative, but let's actually write down what

00:04:05.670 --> 00:04:08.460
our weight update is going to be because we're going to take a little step

00:04:08.460 --> 00:04:10.850
in the direction of this derivative and it's going to involve a learning rate.

