WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:03.649
in this video we'll discuss and contrast

00:00:03.649 --> 00:00:03.659
in this video we'll discuss and contrast
 

00:00:03.659 --> 00:00:06.070
in this video we'll discuss and contrast
two different types of learning

00:00:06.070 --> 00:00:06.080
two different types of learning
 

00:00:06.080 --> 00:00:10.629
two different types of learning
discriminative and generative learning

00:00:10.629 --> 00:00:10.639
 

00:00:10.639 --> 00:00:15.140
so we'll say that a learning algorithm

00:00:15.140 --> 00:00:15.150
so we'll say that a learning algorithm
 

00:00:15.150 --> 00:00:17.570
so we'll say that a learning algorithm
corresponds to a discriminative learning

00:00:17.570 --> 00:00:17.580
corresponds to a discriminative learning
 

00:00:17.580 --> 00:00:20.660
corresponds to a discriminative learning
algorithm if it's trying to optimize a

00:00:20.660 --> 00:00:20.670
algorithm if it's trying to optimize a
 

00:00:20.670 --> 00:00:23.269
algorithm if it's trying to optimize a
conditional likelihood that is if it's

00:00:23.269 --> 00:00:23.279
conditional likelihood that is if it's
 

00:00:23.279 --> 00:00:25.939
conditional likelihood that is if it's
trying to optimize the last function

00:00:25.939 --> 00:00:25.949
trying to optimize the last function
 

00:00:25.949 --> 00:00:29.599
trying to optimize the last function
which looks at minus the log of the

00:00:29.599 --> 00:00:29.609
which looks at minus the log of the
 

00:00:29.609 --> 00:00:32.359
which looks at minus the log of the
probability of the true target given X

00:00:32.359 --> 00:00:32.369
probability of the true target given X
 

00:00:32.369 --> 00:00:34.280
probability of the true target given X
so for instance conditional random

00:00:34.280 --> 00:00:34.290
so for instance conditional random
 

00:00:34.290 --> 00:00:36.440
so for instance conditional random
fields are usually trained in a

00:00:36.440 --> 00:00:36.450
fields are usually trained in a
 

00:00:36.450 --> 00:00:38.330
fields are usually trained in a
conditional way and actually always

00:00:38.330 --> 00:00:38.340
conditional way and actually always
 

00:00:38.340 --> 00:00:40.400
conditional way and actually always
train conditionally because a

00:00:40.400 --> 00:00:40.410
train conditionally because a
 

00:00:40.410 --> 00:00:42.740
train conditionally because a
conditional random field only defines a

00:00:42.740 --> 00:00:42.750
conditional random field only defines a
 

00:00:42.750 --> 00:00:44.830
conditional random field only defines a
conditional distribution it actually

00:00:44.830 --> 00:00:44.840
conditional distribution it actually
 

00:00:44.840 --> 00:00:47.560
conditional distribution it actually
doesn't really look at defining a

00:00:47.560 --> 00:00:47.570
doesn't really look at defining a
 

00:00:47.570 --> 00:00:50.630
doesn't really look at defining a
distribution just over X or junk

00:00:50.630 --> 00:00:50.640
distribution just over X or junk
 

00:00:50.640 --> 00:00:55.130
distribution just over X or junk
distribution over Y and X another

00:00:55.130 --> 00:00:55.140
distribution over Y and X another
 

00:00:55.140 --> 00:00:58.010
distribution over Y and X another
alternative actually would be to do

00:00:58.010 --> 00:00:58.020
alternative actually would be to do
 

00:00:58.020 --> 00:01:00.470
alternative actually would be to do
generative learning that is to optimize

00:01:00.470 --> 00:01:00.480
generative learning that is to optimize
 

00:01:00.480 --> 00:01:03.439
generative learning that is to optimize
the joint log likelihood so in other

00:01:03.439 --> 00:01:03.449
the joint log likelihood so in other
 

00:01:03.449 --> 00:01:05.690
the joint log likelihood so in other
words optimize minus the log of the

00:01:05.690 --> 00:01:05.700
words optimize minus the log of the
 

00:01:05.700 --> 00:01:08.870
words optimize minus the log of the
joint probability of observing y and X

00:01:08.870 --> 00:01:08.880
joint probability of observing y and X
 

00:01:08.880 --> 00:01:11.120
joint probability of observing y and X
as opposed to the probability of

00:01:11.120 --> 00:01:11.130
as opposed to the probability of
 

00:01:11.130 --> 00:01:14.920
as opposed to the probability of
observing Y given that I'm observing X

00:01:14.920 --> 00:01:14.930
observing Y given that I'm observing X
 

00:01:14.930 --> 00:01:18.590
observing Y given that I'm observing X
so for instance hmms are usually trained

00:01:18.590 --> 00:01:18.600
so for instance hmms are usually trained
 

00:01:18.600 --> 00:01:20.570
so for instance hmms are usually trained
generatively so if you know about hmms

00:01:20.570 --> 00:01:20.580
generatively so if you know about hmms
 

00:01:20.580 --> 00:01:22.730
generatively so if you know about hmms
what you've probably seen is the

00:01:22.730 --> 00:01:22.740
what you've probably seen is the
 

00:01:22.740 --> 00:01:24.020
what you've probably seen is the
learning algorithm for training them

00:01:24.020 --> 00:01:24.030
learning algorithm for training them
 

00:01:24.030 --> 00:01:26.929
learning algorithm for training them
generatively though we could also train

00:01:26.929 --> 00:01:26.939
generatively though we could also train
 

00:01:26.939 --> 00:01:29.420
generatively though we could also train
it as the in this way because if I give

00:01:29.420 --> 00:01:29.430
it as the in this way because if I give
 

00:01:29.430 --> 00:01:31.340
it as the in this way because if I give
you a joint distribution over y and X I

00:01:31.340 --> 00:01:31.350
you a joint distribution over y and X I
 

00:01:31.350 --> 00:01:34.969
you a joint distribution over y and X I
can then do the inference of what is P

00:01:34.969 --> 00:01:34.979
can then do the inference of what is P
 

00:01:34.979 --> 00:01:36.620
can then do the inference of what is P
of Y given X and then I could try to

00:01:36.620 --> 00:01:36.630
of Y given X and then I could try to
 

00:01:36.630 --> 00:01:40.700
of Y given X and then I could try to
train it discriminative ly but but

00:01:40.700 --> 00:01:40.710
train it discriminative ly but but
 

00:01:40.710 --> 00:01:43.280
train it discriminative ly but but
initially hmm have mostly been developed

00:01:43.280 --> 00:01:43.290
initially hmm have mostly been developed
 

00:01:43.290 --> 00:01:45.080
initially hmm have mostly been developed
and proposed in the context of

00:01:45.080 --> 00:01:45.090
and proposed in the context of
 

00:01:45.090 --> 00:01:48.950
and proposed in the context of
generative training and so let's exploit

00:01:48.950 --> 00:01:48.960
generative training and so let's exploit
 

00:01:48.960 --> 00:01:50.990
generative training and so let's exploit
that formula a little bit so we have

00:01:50.990 --> 00:01:51.000
that formula a little bit so we have
 

00:01:51.000 --> 00:01:53.870
that formula a little bit so we have
that the joint distribution it can be

00:01:53.870 --> 00:01:53.880
that the joint distribution it can be
 

00:01:53.880 --> 00:01:55.850
that the joint distribution it can be
written down as the product of the

00:01:55.850 --> 00:01:55.860
written down as the product of the
 

00:01:55.860 --> 00:01:59.350
written down as the product of the
conditional P of Y given X times the

00:01:59.350 --> 00:01:59.360
conditional P of Y given X times the
 

00:01:59.360 --> 00:02:02.810
conditional P of Y given X times the
marginal P of X and because the log of a

00:02:02.810 --> 00:02:02.820
marginal P of X and because the log of a
 

00:02:02.820 --> 00:02:05.990
marginal P of X and because the log of a
sum is the sum of logs then this is also

00:02:05.990 --> 00:02:06.000
sum is the sum of logs then this is also
 

00:02:06.000 --> 00:02:08.389
sum is the sum of logs then this is also
equal to minus the log of P of Y given X

00:02:08.389 --> 00:02:08.399
equal to minus the log of P of Y given X
 

00:02:08.399 --> 00:02:11.510
equal to minus the log of P of Y given X
minus the log of P of X so we see that

00:02:11.510 --> 00:02:11.520
minus the log of P of X so we see that
 

00:02:11.520 --> 00:02:13.180
minus the log of P of X so we see that
the difference is this term here

00:02:13.180 --> 00:02:13.190
the difference is this term here
 

00:02:13.190 --> 00:02:16.720
the difference is this term here
so we have this here and this here but

00:02:16.720 --> 00:02:16.730
so we have this here and this here but
 

00:02:16.730 --> 00:02:21.070
so we have this here and this here but
we have this term that is added when

00:02:21.070 --> 00:02:21.080
we have this term that is added when
 

00:02:21.080 --> 00:02:24.240
we have this term that is added when
we're performing genitive learning and

00:02:24.240 --> 00:02:24.250
we're performing genitive learning and
 

00:02:24.250 --> 00:02:26.650
we're performing genitive learning and
so what we were thinking about this is

00:02:26.650 --> 00:02:26.660
so what we were thinking about this is
 

00:02:26.660 --> 00:02:29.050
so what we were thinking about this is
that perhaps this term is kind of acting

00:02:29.050 --> 00:02:29.060
that perhaps this term is kind of acting
 

00:02:29.060 --> 00:02:31.750
that perhaps this term is kind of acting
like a regularizer that is it's we're

00:02:31.750 --> 00:02:31.760
like a regularizer that is it's we're
 

00:02:31.760 --> 00:02:36.100
like a regularizer that is it's we're
adding another term which is does not

00:02:36.100 --> 00:02:36.110
adding another term which is does not
 

00:02:36.110 --> 00:02:38.530
adding another term which is does not
involve Y so it's not a term that

00:02:38.530 --> 00:02:38.540
involve Y so it's not a term that
 

00:02:38.540 --> 00:02:40.840
involve Y so it's not a term that
encourages the model to assign high

00:02:40.840 --> 00:02:40.850
encourages the model to assign high
 

00:02:40.850 --> 00:02:43.120
encourages the model to assign high
probability to the true target given X

00:02:43.120 --> 00:02:43.130
probability to the true target given X
 

00:02:43.130 --> 00:02:45.340
probability to the true target given X
it's just a model that requires to do

00:02:45.340 --> 00:02:45.350
it's just a model that requires to do
 

00:02:45.350 --> 00:02:47.380
it's just a model that requires to do
something else it's gonna favor models

00:02:47.380 --> 00:02:47.390
something else it's gonna favor models
 

00:02:47.390 --> 00:02:50.860
something else it's gonna favor models
that also explain well the marginal so I

00:02:50.860 --> 00:02:50.870
that also explain well the marginal so I
 

00:02:50.870 --> 00:02:53.140
that also explain well the marginal so I
assign high probability to observe any

00:02:53.140 --> 00:02:53.150
assign high probability to observe any
 

00:02:53.150 --> 00:02:56.170
assign high probability to observe any
true data so the data just the inputs

00:02:56.170 --> 00:02:56.180
true data so the data just the inputs
 

00:02:56.180 --> 00:02:59.080
true data so the data just the inputs
that we see from the training set and so

00:02:59.080 --> 00:02:59.090
that we see from the training set and so
 

00:02:59.090 --> 00:03:01.390
that we see from the training set and so
for that reason a good intuition is to

00:03:01.390 --> 00:03:01.400
for that reason a good intuition is to
 

00:03:01.400 --> 00:03:03.700
for that reason a good intuition is to
think that this kind of learning will

00:03:03.700 --> 00:03:03.710
think that this kind of learning will
 

00:03:03.710 --> 00:03:06.130
think that this kind of learning will
tend to fit the data better while

00:03:06.130 --> 00:03:06.140
tend to fit the data better while
 

00:03:06.140 --> 00:03:07.660
tend to fit the data better while
genitive learning will be more

00:03:07.660 --> 00:03:07.670
genitive learning will be more
 

00:03:07.670 --> 00:03:12.570
genitive learning will be more
regularized because of this term here

00:03:12.570 --> 00:03:12.580
 

00:03:12.580 --> 00:03:16.960
actually we can show that there are two

00:03:16.960 --> 00:03:16.970
actually we can show that there are two
 

00:03:16.970 --> 00:03:19.390
actually we can show that there are two
scenarios that we can observe in terms

00:03:19.390 --> 00:03:19.400
scenarios that we can observe in terms
 

00:03:19.400 --> 00:03:21.310
scenarios that we can observe in terms
of how generative versus this kind of

00:03:21.310 --> 00:03:21.320
of how generative versus this kind of
 

00:03:21.320 --> 00:03:24.580
of how generative versus this kind of
learning will compare ibly comparatively

00:03:24.580 --> 00:03:24.590
learning will compare ibly comparatively
 

00:03:24.590 --> 00:03:29.410
learning will compare ibly comparatively
come compare so if a model is well

00:03:29.410 --> 00:03:29.420
come compare so if a model is well
 

00:03:29.420 --> 00:03:32.440
come compare so if a model is well
specified so that means that the data in

00:03:32.440 --> 00:03:32.450
specified so that means that the data in
 

00:03:32.450 --> 00:03:36.220
specified so that means that the data in
our training set is that is actually was

00:03:36.220 --> 00:03:36.230
our training set is that is actually was
 

00:03:36.230 --> 00:03:39.160
our training set is that is actually was
actually generated from a model from the

00:03:39.160 --> 00:03:39.170
actually generated from a model from the
 

00:03:39.170 --> 00:03:40.780
actually generated from a model from the
same class as our model so there's a

00:03:40.780 --> 00:03:40.790
same class as our model so there's a
 

00:03:40.790 --> 00:03:42.670
same class as our model so there's a
setting of our parameters for our model

00:03:42.670 --> 00:03:42.680
setting of our parameters for our model
 

00:03:42.680 --> 00:03:44.830
setting of our parameters for our model
that corresponds exactly to the true

00:03:44.830 --> 00:03:44.840
that corresponds exactly to the true
 

00:03:44.840 --> 00:03:47.229
that corresponds exactly to the true
model that generated the data so if the

00:03:47.229 --> 00:03:47.239
model that generated the data so if the
 

00:03:47.239 --> 00:03:50.350
model that generated the data so if the
model is well specified then we can show

00:03:50.350 --> 00:03:50.360
model is well specified then we can show
 

00:03:50.360 --> 00:03:52.090
model is well specified then we can show
a genitive learning essentially always

00:03:52.090 --> 00:03:52.100
a genitive learning essentially always
 

00:03:52.100 --> 00:03:54.790
a genitive learning essentially always
better that is for any size of training

00:03:54.790 --> 00:03:54.800
better that is for any size of training
 

00:03:54.800 --> 00:03:57.040
better that is for any size of training
set that we use there's always going to

00:03:57.040 --> 00:03:57.050
set that we use there's always going to
 

00:03:57.050 --> 00:03:59.800
set that we use there's always going to
be a gap between what generative

00:03:59.800 --> 00:03:59.810
be a gap between what generative
 

00:03:59.810 --> 00:04:02.860
be a gap between what generative
learning gets in terms of generalization

00:04:02.860 --> 00:04:02.870
learning gets in terms of generalization
 

00:04:02.870 --> 00:04:04.990
learning gets in terms of generalization
performance compared to discriminative

00:04:04.990 --> 00:04:05.000
performance compared to discriminative
 

00:04:05.000 --> 00:04:06.850
performance compared to discriminative
training so genitive training is it's

00:04:06.850 --> 00:04:06.860
training so genitive training is it's
 

00:04:06.860 --> 00:04:09.550
training so genitive training is it's
going to perform better they'll they

00:04:09.550 --> 00:04:09.560
going to perform better they'll they
 

00:04:09.560 --> 00:04:11.580
going to perform better they'll they
should both eventually converge to the

00:04:11.580 --> 00:04:11.590
should both eventually converge to the
 

00:04:11.590 --> 00:04:14.310
should both eventually converge to the
two essentially an error zero so a

00:04:14.310 --> 00:04:14.320
two essentially an error zero so a
 

00:04:14.320 --> 00:04:17.020
two essentially an error zero so a
perfect performance but genitive

00:04:17.020 --> 00:04:17.030
perfect performance but genitive
 

00:04:17.030 --> 00:04:19.330
perfect performance but genitive
learning is going to get there much

00:04:19.330 --> 00:04:19.340
learning is going to get there much
 

00:04:19.340 --> 00:04:25.180
learning is going to get there much
faster however if the model is not well

00:04:25.180 --> 00:04:25.190
faster however if the model is not well
 

00:04:25.190 --> 00:04:26.650
faster however if the model is not well
specified which is

00:04:26.650 --> 00:04:26.660
specified which is
 

00:04:26.660 --> 00:04:28.510
specified which is
most of the time so if data was

00:04:28.510 --> 00:04:28.520
most of the time so if data was
 

00:04:28.520 --> 00:04:30.850
most of the time so if data was
generated by some experts that labeled

00:04:30.850 --> 00:04:30.860
generated by some experts that labeled
 

00:04:30.860 --> 00:04:33.520
generated by some experts that labeled
some data then this is a process that

00:04:33.520 --> 00:04:33.530
some data then this is a process that
 

00:04:33.530 --> 00:04:36.610
some data then this is a process that
that is almost most almost certainly

00:04:36.610 --> 00:04:36.620
that is almost most almost certainly
 

00:04:36.620 --> 00:04:39.280
that is almost most almost certainly
more complicated than the model we've

00:04:39.280 --> 00:04:39.290
more complicated than the model we've
 

00:04:39.290 --> 00:04:42.610
more complicated than the model we've
written down in math so so really this

00:04:42.610 --> 00:04:42.620
written down in math so so really this
 

00:04:42.620 --> 00:04:44.980
written down in math so so really this
is essentially for all the interesting

00:04:44.980 --> 00:04:44.990
is essentially for all the interesting
 

00:04:44.990 --> 00:04:49.210
is essentially for all the interesting
problem the model will tend to be will

00:04:49.210 --> 00:04:49.220
problem the model will tend to be will
 

00:04:49.220 --> 00:04:53.320
problem the model will tend to be will
not be well specified then the picture

00:04:53.320 --> 00:04:53.330
not be well specified then the picture
 

00:04:53.330 --> 00:04:56.740
not be well specified then the picture
is not as clear so what we'll see is a

00:04:56.740 --> 00:04:56.750
is not as clear so what we'll see is a
 

00:04:56.750 --> 00:04:58.750
is not as clear so what we'll see is a
picture that is consistent with the view

00:04:58.750 --> 00:04:58.760
picture that is consistent with the view
 

00:04:58.760 --> 00:05:00.910
picture that is consistent with the view
that genitive learning is just more

00:05:00.910 --> 00:05:00.920
that genitive learning is just more
 

00:05:00.920 --> 00:05:03.490
that genitive learning is just more
corresponds to more regularization so

00:05:03.490 --> 00:05:03.500
corresponds to more regularization so
 

00:05:03.500 --> 00:05:07.780
corresponds to more regularization so
the training set is small then we should

00:05:07.780 --> 00:05:07.790
the training set is small then we should
 

00:05:07.790 --> 00:05:09.670
the training set is small then we should
observe that genitive learning will get

00:05:09.670 --> 00:05:09.680
observe that genitive learning will get
 

00:05:09.680 --> 00:05:12.790
observe that genitive learning will get
a better performance in terms of

00:05:12.790 --> 00:05:12.800
a better performance in terms of
 

00:05:12.800 --> 00:05:15.400
a better performance in terms of
generalization but as we increase the

00:05:15.400 --> 00:05:15.410
generalization but as we increase the
 

00:05:15.410 --> 00:05:17.620
generalization but as we increase the
size of our training set eventually we

00:05:17.620 --> 00:05:17.630
size of our training set eventually we
 

00:05:17.630 --> 00:05:19.660
size of our training set eventually we
should reach a point where this creative

00:05:19.660 --> 00:05:19.670
should reach a point where this creative
 

00:05:19.670 --> 00:05:21.160
should reach a point where this creative
learning will catch up with genitive

00:05:21.160 --> 00:05:21.170
learning will catch up with genitive
 

00:05:21.170 --> 00:05:23.860
learning will catch up with genitive
learning and will start being actually

00:05:23.860 --> 00:05:23.870
learning and will start being actually
 

00:05:23.870 --> 00:05:25.690
learning and will start being actually
better in terms of generalization

00:05:25.690 --> 00:05:25.700
better in terms of generalization
 

00:05:25.700 --> 00:05:27.490
better in terms of generalization
performance to get lower generalization

00:05:27.490 --> 00:05:27.500
performance to get lower generalization
 

00:05:27.500 --> 00:05:33.250
performance to get lower generalization
error than a generative learning and in

00:05:33.250 --> 00:05:33.260
error than a generative learning and in
 

00:05:33.260 --> 00:05:36.520
error than a generative learning and in
fact more specifically what we get is

00:05:36.520 --> 00:05:36.530
fact more specifically what we get is
 

00:05:36.530 --> 00:05:39.430
fact more specifically what we get is
that discredit of learning will converge

00:05:39.430 --> 00:05:39.440
that discredit of learning will converge
 

00:05:39.440 --> 00:05:43.810
that discredit of learning will converge
to a smaller asymptotic error asymptotic

00:05:43.810 --> 00:05:43.820
to a smaller asymptotic error asymptotic
 

00:05:43.820 --> 00:05:45.550
to a smaller asymptotic error asymptotic
error being the error your model would

00:05:45.550 --> 00:05:45.560
error being the error your model would
 

00:05:45.560 --> 00:05:47.080
error being the error your model would
get if you had infinite amount of

00:05:47.080 --> 00:05:47.090
get if you had infinite amount of
 

00:05:47.090 --> 00:05:49.990
get if you had infinite amount of
training data so we'll it has a small

00:05:49.990 --> 00:05:50.000
training data so we'll it has a small
 

00:05:50.000 --> 00:05:52.090
training data so we'll it has a small
asymptotic error then generative

00:05:52.090 --> 00:05:52.100
asymptotic error then generative
 

00:05:52.100 --> 00:05:54.400
asymptotic error then generative
learning which won't be able to do as

00:05:54.400 --> 00:05:54.410
learning which won't be able to do as
 

00:05:54.410 --> 00:05:56.980
learning which won't be able to do as
well even with an infinite amount of

00:05:56.980 --> 00:05:56.990
well even with an infinite amount of
 

00:05:56.990 --> 00:06:00.670
well even with an infinite amount of
training data ok so again here a nice

00:06:00.670 --> 00:06:00.680
training data ok so again here a nice
 

00:06:00.680 --> 00:06:03.790
training data ok so again here a nice
way of so a simpler way of interpreting

00:06:03.790 --> 00:06:03.800
way of so a simpler way of interpreting
 

00:06:03.800 --> 00:06:05.200
way of so a simpler way of interpreting
this is that genitive learning means

00:06:05.200 --> 00:06:05.210
this is that genitive learning means
 

00:06:05.210 --> 00:06:07.030
this is that genitive learning means
more regularization so if there's a

00:06:07.030 --> 00:06:07.040
more regularization so if there's a
 

00:06:07.040 --> 00:06:09.430
more regularization so if there's a
small amount of training data genitive

00:06:09.430 --> 00:06:09.440
small amount of training data genitive
 

00:06:09.440 --> 00:06:12.610
small amount of training data genitive
learning will probably do better but if

00:06:12.610 --> 00:06:12.620
learning will probably do better but if
 

00:06:12.620 --> 00:06:14.710
learning will probably do better but if
I have quite a bit of training data then

00:06:14.710 --> 00:06:14.720
I have quite a bit of training data then
 

00:06:14.720 --> 00:06:16.720
I have quite a bit of training data then
now we can expect this punative learning

00:06:16.720 --> 00:06:16.730
now we can expect this punative learning
 

00:06:16.730 --> 00:06:19.900
now we can expect this punative learning
to eventually catch up and do better so

00:06:19.900 --> 00:06:19.910
to eventually catch up and do better so
 

00:06:19.910 --> 00:06:21.190
to eventually catch up and do better so
that's actually a good thing to know

00:06:21.190 --> 00:06:21.200
that's actually a good thing to know
 

00:06:21.200 --> 00:06:23.650
that's actually a good thing to know
about if you're tackling a problem for

00:06:23.650 --> 00:06:23.660
about if you're tackling a problem for
 

00:06:23.660 --> 00:06:25.570
about if you're tackling a problem for
which you either have a lot of data or

00:06:25.570 --> 00:06:25.580
which you either have a lot of data or
 

00:06:25.580 --> 00:06:28.540
which you either have a lot of data or
not a lot of data so you know which kind

00:06:28.540 --> 00:06:28.550
not a lot of data so you know which kind
 

00:06:28.550 --> 00:06:31.750
not a lot of data so you know which kind
of learning you should prefer and for

00:06:31.750 --> 00:06:31.760
of learning you should prefer and for
 

00:06:31.760 --> 00:06:34.150
of learning you should prefer and for
more theoretical details on this invite

00:06:34.150 --> 00:06:34.160
more theoretical details on this invite
 

00:06:34.160 --> 00:06:36.250
more theoretical details on this invite
you to consult this paper on this random

00:06:36.250 --> 00:06:36.260
you to consult this paper on this random
 

00:06:36.260 --> 00:06:39.909
you to consult this paper on this random
versus genitive classifiers so this is a

00:06:39.909 --> 00:06:39.919
versus genitive classifiers so this is a
 

00:06:39.919 --> 00:06:40.360
versus genitive classifiers so this is a
paper

00:06:40.360 --> 00:06:40.370
paper
 

00:06:40.370 --> 00:06:44.410
paper
enjoying and michael jordan 2001

