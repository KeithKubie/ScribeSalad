WEBVTT
Kind: captions
Language: en

00:00:03.742 --> 00:00:05.950
DON TURNER: We're both very
excited to be here today.

00:00:05.950 --> 00:00:07.530
We've got a load of
cool stuff to show you.

00:00:07.530 --> 00:00:09.600
As you can probably see, we've
got a whole load of equipment

00:00:09.600 --> 00:00:10.240
up here.

00:00:10.240 --> 00:00:12.270
So we've got some live demos.

00:00:12.270 --> 00:00:13.832
We've got a musical performance.

00:00:13.832 --> 00:00:15.540
And we've got a whole
lot of live coding,

00:00:15.540 --> 00:00:17.430
so anything might happen.

00:00:17.430 --> 00:00:18.930
Hopefully you're
going to leave here

00:00:18.930 --> 00:00:21.930
today excited about the
opportunities on Android

00:00:21.930 --> 00:00:23.280
and Chrome.

00:00:23.280 --> 00:00:28.950
But, before we start, a talk
about Android and Chrome--

00:00:28.950 --> 00:00:30.850
it's a bit strange, right?

00:00:30.850 --> 00:00:31.990
Are we friends?

00:00:31.990 --> 00:00:34.570
HONGCHAN CHOI: Well, I thought
we were more family as Google.

00:00:34.570 --> 00:00:39.370
But anyhow, we have a lot more
in common than you might think.

00:00:39.370 --> 00:00:41.030
For starters, both
platforms have

00:00:41.030 --> 00:00:43.960
teams who are passionate
about empowering users

00:00:43.960 --> 00:00:46.510
to be more musically creative.

00:00:46.510 --> 00:00:50.230
And we also have a very similar
audio programming model, right?

00:00:50.230 --> 00:00:50.980
DON TURNER: Right.

00:00:50.980 --> 00:00:54.190
So both Android and Chrome
have a callback-based audio

00:00:54.190 --> 00:00:55.240
programming model.

00:00:55.240 --> 00:00:56.830
You have an audio
renderer, which

00:00:56.830 --> 00:01:00.250
is responsible for communicating
to the underlying audio

00:01:00.250 --> 00:01:01.450
hardware.

00:01:01.450 --> 00:01:06.410
And, every time it needs more
data, it gives us a callback.

00:01:06.410 --> 00:01:09.380
There's a similar story with
round trip audio latency.

00:01:09.380 --> 00:01:11.960
And recent advances
on both platforms

00:01:11.960 --> 00:01:15.290
mean that there's a very
similar level of latency.

00:01:15.290 --> 00:01:20.360
On Android, all Pixel devices
have around 20 milliseconds

00:01:20.360 --> 00:01:22.010
input to output latency.

00:01:22.010 --> 00:01:25.280
And we're now seeing devices
from Samsung, Huawei,

00:01:25.280 --> 00:01:28.423
and others reach a similar
level of performance.

00:01:28.423 --> 00:01:30.590
HONGCHAN CHOI: The audio
latency of a Chrome browser

00:01:30.590 --> 00:01:32.450
largely depends on the
underlying operating

00:01:32.450 --> 00:01:34.190
system or hardware.

00:01:34.190 --> 00:01:36.970
But for example, the
device round trip latency

00:01:36.970 --> 00:01:39.420
of a Chrome Pixelbook is
around 20 milliseconds,

00:01:39.420 --> 00:01:43.160
which is good enough for most
of our real time use cases.

00:01:43.160 --> 00:01:44.810
DON TURNER: Again,
with MIDI support,

00:01:44.810 --> 00:01:47.840
very similar capabilities
on the platforms.

00:01:47.840 --> 00:01:50.420
In Android, we have
the MidiManager API

00:01:50.420 --> 00:01:52.430
introduced in Marshmallow.

00:01:52.430 --> 00:01:55.400
And in Android Q,
there's a new API

00:01:55.400 --> 00:01:59.030
called AMidi, which is for high
performance processing of MIDI

00:01:59.030 --> 00:01:59.690
data.

00:01:59.690 --> 00:02:02.930
HONGCHAN CHOI: And Chrome has
had Web MIDI API since 2015

00:02:02.930 --> 00:02:05.360
and it still remains the same.

00:02:05.360 --> 00:02:08.060
DON TURNER: Of course, one
of the biggest features

00:02:08.060 --> 00:02:11.450
of both platforms is the
number of active users.

00:02:11.450 --> 00:02:13.715
Android has over
2 billion users.

00:02:13.715 --> 00:02:15.590
HONGCHAN CHOI: It's
fantastic and Chrome also

00:02:15.590 --> 00:02:17.760
has a similar number.

00:02:17.760 --> 00:02:20.360
DON TURNER: Now, obviously,
some Android users

00:02:20.360 --> 00:02:22.130
are also Chrome users.

00:02:22.130 --> 00:02:24.980
So it's not like two
plus two equals four,

00:02:24.980 --> 00:02:28.370
but we estimate that, by
targeting both these platforms,

00:02:28.370 --> 00:02:32.180
you can reach around half
the world's population, which

00:02:32.180 --> 00:02:34.200
is pretty incredible
when you think about it.

00:02:34.200 --> 00:02:35.617
HONGCHAN CHOI: And
a company which

00:02:35.617 --> 00:02:38.120
has been taking advantage
of this incredible reach

00:02:38.120 --> 00:02:39.800
is BandLab.

00:02:39.800 --> 00:02:43.010
They achieved huge growth
thanks to their cross platform

00:02:43.010 --> 00:02:44.318
approach from the beginning.

00:02:44.318 --> 00:02:45.110
Here's their story.

00:02:48.137 --> 00:02:50.470
SPEAKER 1: This is a story
of how three people connected

00:02:50.470 --> 00:02:53.830
via a free music app
collaborated to create a top 10

00:02:53.830 --> 00:02:55.300
commercial dance track.

00:02:55.300 --> 00:02:57.240
The app is BandLab.

00:02:57.240 --> 00:02:59.230
JOHN IVERS: BandLab is
a social music platform

00:02:59.230 --> 00:03:01.240
that enables our users
to create and collaborate

00:03:01.240 --> 00:03:04.540
on music on all platforms
anywhere in the world all

00:03:04.540 --> 00:03:06.410
with unlimited cloud storage.

00:03:06.410 --> 00:03:08.450
We have a Chrome-based
digital audio workstation

00:03:08.450 --> 00:03:10.000
we call the Mix Editor.

00:03:10.000 --> 00:03:13.450
It allows our users to create
and record music live, create

00:03:13.450 --> 00:03:15.730
their own tracks using
our built in instruments,

00:03:15.730 --> 00:03:17.980
or start a song from the
thousands of free loops

00:03:17.980 --> 00:03:19.510
in our extensive library.

00:03:19.510 --> 00:03:21.800
SPEAKER 1: Here are
our three creatives.

00:03:21.800 --> 00:03:23.500
Andy is an
international producer

00:03:23.500 --> 00:03:25.450
with two decades of experience.

00:03:25.450 --> 00:03:28.570
He's based in San Francisco,
but works around the world.

00:03:28.570 --> 00:03:31.420
Se-Hwang Kim is a pioneering
guitarist, singer songwriter,

00:03:31.420 --> 00:03:33.250
and arranger from South Korea.

00:03:33.250 --> 00:03:35.530
He works with leading
K Pop and US artists,

00:03:35.530 --> 00:03:37.690
and is on Hollywood's
Rock Walk of Fame.

00:03:37.690 --> 00:03:41.170
Linah London is an up and
coming pop and R&amp;B vocalist.

00:03:41.170 --> 00:03:43.130
Proudly repping East
London to the world,

00:03:43.130 --> 00:03:45.400
she's a hopeless romantic
with a golden voice.

00:03:45.400 --> 00:03:47.250
These three wanted
to create music

00:03:47.250 --> 00:03:49.473
and needed tech that
worked for them, no matter

00:03:49.473 --> 00:03:50.140
where they were.

00:04:30.745 --> 00:04:32.620
JOHN IVERS: I love seeing
stories like these.

00:04:32.620 --> 00:04:34.203
What these guys are
doing with BandLab

00:04:34.203 --> 00:04:37.037
is exactly what we had in
mind when we were building it.

00:04:37.037 --> 00:04:39.370
Our vision is for a future
where there are no boundaries

00:04:39.370 --> 00:04:41.290
to making and sharing music.

00:04:41.290 --> 00:04:42.790
This project shows
how musicians can

00:04:42.790 --> 00:04:45.310
use BandLab them to overcome
barriers like distance

00:04:45.310 --> 00:04:47.950
or incompatible technology
to connect and make

00:04:47.950 --> 00:04:49.263
incredible music.

00:04:49.263 --> 00:04:50.680
On mobile, we've
been working hard

00:04:50.680 --> 00:04:52.660
to create music, editing,
and creation tools

00:04:52.660 --> 00:04:55.390
to empower everyone
with access to a phone.

00:04:55.390 --> 00:04:57.640
And as the most popular
operating system in the world,

00:04:57.640 --> 00:04:59.860
Android is a major
engine to drive that.

00:04:59.860 --> 00:05:02.860
Today, we have close to seven
million registered users

00:05:02.860 --> 00:05:05.920
and they're saving almost 2
million new songs every month

00:05:05.920 --> 00:05:07.360
on BandLab.

00:05:07.360 --> 00:05:09.460
A huge portion of our
community is collaborating

00:05:09.460 --> 00:05:10.810
via Android and Chrome.

00:05:10.810 --> 00:05:13.300
We are here to
democratize music,

00:05:13.300 --> 00:05:16.420
and that's now possible thanks
to developments in technology

00:05:16.420 --> 00:05:17.920
and partners like Google.

00:05:22.290 --> 00:05:25.660
DON TURNER: So BandLab has
been successful by targeting

00:05:25.660 --> 00:05:27.880
both Android and Chrome.

00:05:27.880 --> 00:05:29.710
And today, we're
going to show you

00:05:29.710 --> 00:05:31.930
how you can do the
exact same thing.

00:05:31.930 --> 00:05:35.050
To do this, we're going to
build a synthesizer app which

00:05:35.050 --> 00:05:38.770
runs on Android
and runs on Chrome,

00:05:38.770 --> 00:05:40.483
but shares the same code.

00:05:40.483 --> 00:05:41.650
HONGCHAN CHOI: That's right.

00:05:41.650 --> 00:05:44.410
We'll be using the exact
same synthesizer code written

00:05:44.410 --> 00:05:45.670
in C++.

00:05:45.670 --> 00:05:48.310
And Don is going to show you
how to make it work on Android,

00:05:48.310 --> 00:05:50.470
and I'll do the same on Chrome.

00:05:50.470 --> 00:05:52.990
DON TURNER: After that, we'll
add MIDI support to both apps.

00:05:52.990 --> 00:05:57.820
Hongchan will do that for Chrome
and I will do it for Android.

00:05:57.820 --> 00:06:02.680
So I'm going to
start and I'm going

00:06:02.680 --> 00:06:06.080
to start by using a
library called Oboe.

00:06:06.080 --> 00:06:10.530
Now Oboe is a C++ library
for building low latency,

00:06:10.530 --> 00:06:13.640
high performance
audio apps on Android.

00:06:13.640 --> 00:06:18.140
Here's how it fits into
an app architecture.

00:06:18.140 --> 00:06:22.300
So we use the Oboe library
to build an audio stream.

00:06:22.300 --> 00:06:24.100
That's responsible
for communicating

00:06:24.100 --> 00:06:26.260
to the underlying
audio hardware.

00:06:26.260 --> 00:06:29.320
In this case, I'll be
communicating with this Pixel

00:06:29.320 --> 00:06:32.500
XL headphone output.

00:06:32.500 --> 00:06:35.500
Each time the audio
stream requires more data,

00:06:35.500 --> 00:06:37.940
I get a callback into my app.

00:06:37.940 --> 00:06:42.880
And my job is to add in a
synthesizer which will then

00:06:42.880 --> 00:06:46.270
be responsible for
rendering audio frames back

00:06:46.270 --> 00:06:49.090
into my audio stream.

00:06:49.090 --> 00:06:54.390
OK, if we could
switch to my laptop,

00:06:54.390 --> 00:06:55.780
which needs Hongchan's password.

00:07:01.492 --> 00:07:05.570
OK, so I'm in
Android's studio here.

00:07:05.570 --> 00:07:08.520
Hopefully you can read the code.

00:07:08.520 --> 00:07:12.540
So I've already built kind
of a Hello World audio app.

00:07:12.540 --> 00:07:16.455
And it has this one
class audio engine.

00:07:16.455 --> 00:07:17.830
In audio engine,
we have a couple

00:07:17.830 --> 00:07:21.970
of methods, which the
most important ones are

00:07:21.970 --> 00:07:25.210
start, which is called when
the application starts,

00:07:25.210 --> 00:07:28.150
and this onAudioReady
method, which

00:07:28.150 --> 00:07:31.850
is called every time my
audio stream needs more data.

00:07:31.850 --> 00:07:36.280
So that's going to be being
called every few milliseconds.

00:07:36.280 --> 00:07:38.850
And I'll cover the
rest a bit later on.

00:07:38.850 --> 00:07:43.380
So Hongchan and I are going
to be sharing source code.

00:07:43.380 --> 00:07:45.310
And here's the source here.

00:07:45.310 --> 00:07:51.150
So my first stage is I'm going
to include the synthesizer

00:07:51.150 --> 00:07:53.940
header.

00:07:53.940 --> 00:07:57.580
I'm now going to create
a synthesizer object.

00:07:57.580 --> 00:08:02.690
And we'll just make that
a unique pointer of type

00:08:02.690 --> 00:08:03.650
synthesizer.

00:08:07.760 --> 00:08:11.150
OK, now I'm going to go into the
implementation of this class.

00:08:13.890 --> 00:08:19.420
So in my start method, I'm
going to create an audio stream.

00:08:19.420 --> 00:08:22.020
And this class is
provided by Oboe.

00:08:22.020 --> 00:08:24.800
It allows me to build
an audio stream.

00:08:24.800 --> 00:08:27.280
So I set a few
properties on my stream.

00:08:27.280 --> 00:08:30.910
For example, low latency to
get a low latency audio stream.

00:08:30.910 --> 00:08:34.780
And I also set the format
to floating point samples

00:08:34.780 --> 00:08:38.551
because that's the
format of my synthesizer.

00:08:38.551 --> 00:08:41.020
I then open the
stream and I can now

00:08:41.020 --> 00:08:45.180
create my synthesizer object.

00:08:45.180 --> 00:08:48.195
So I'll just do make unique.

00:08:51.460 --> 00:08:54.680
OK, so in the constructor
for my synthesizer,

00:08:54.680 --> 00:08:57.620
I have to specify
the sample rate.

00:08:57.620 --> 00:09:00.290
And that's the rate at which
I'm going to be generating

00:09:00.290 --> 00:09:01.940
frames of audio data.

00:09:01.940 --> 00:09:03.380
And I get that from my stream.

00:09:06.140 --> 00:09:08.150
Now, the reason I have
to do this at runtime

00:09:08.150 --> 00:09:10.580
is because the sample
rate of my audio stream

00:09:10.580 --> 00:09:13.220
will depend on the
underlying audio hardware.

00:09:16.760 --> 00:09:20.450
OK, so that's my
synthesizer created.

00:09:20.450 --> 00:09:22.000
Here's my audio
callback, the one

00:09:22.000 --> 00:09:24.820
that's being called
every few milliseconds.

00:09:24.820 --> 00:09:27.970
At the moment, I'm just
outputting silence.

00:09:27.970 --> 00:09:34.580
So I'm going to get rid of that
and have my synthesizer render

00:09:34.580 --> 00:09:37.190
its data into the audio stream.

00:09:37.190 --> 00:09:38.690
And the way that I
do that is I have

00:09:38.690 --> 00:09:44.540
to pass data into this container
array here called audio data.

00:09:44.540 --> 00:09:47.990
Now, you'll notice that
it's of type void star,

00:09:47.990 --> 00:09:51.808
so I need to cast
it to type float.

00:09:51.808 --> 00:09:52.850
So I'll just do that now.

00:09:59.563 --> 00:10:01.230
And my synthesizer
also it needs to know

00:10:01.230 --> 00:10:06.040
how much data to render, which
is this num frames parameter

00:10:06.040 --> 00:10:07.020
here.

00:10:07.020 --> 00:10:10.530
Right, so almost there.

00:10:10.530 --> 00:10:13.050
My synthesizer is now
able to render information

00:10:13.050 --> 00:10:16.710
into my audio stream, but I
need to add a bit of control.

00:10:16.710 --> 00:10:19.000
So I've added a couple
of methods here.

00:10:19.000 --> 00:10:20.730
The first one is
on touch, which is

00:10:20.730 --> 00:10:24.150
going to be called each
time I push my finger down

00:10:24.150 --> 00:10:27.580
onto the screen and each time
I lift it off the screen.

00:10:27.580 --> 00:10:31.470
So this parameter
here is down will

00:10:31.470 --> 00:10:34.680
be true if I'm touching down
and false if I'm lifting off.

00:10:34.680 --> 00:10:40.310
So I'll just say if the
touch event is down,

00:10:40.310 --> 00:10:44.920
then tell my synthesizer
to switch a note on.

00:10:44.920 --> 00:10:48.970
And I'll just specify kind
of a random note here,

00:10:48.970 --> 00:10:55.725
note 60 which is middle C.

00:10:55.725 --> 00:10:57.350
Otherwise, if I'm
lifting off, then I'm

00:10:57.350 --> 00:10:59.890
going to switch the note off.

00:10:59.890 --> 00:11:01.630
So the last thing
I'm going to do

00:11:01.630 --> 00:11:03.922
is add a bit more
control to my synthesizer

00:11:03.922 --> 00:11:05.380
and I thought it
would be nice if I

00:11:05.380 --> 00:11:10.390
tied the rotation of this
device with the filter

00:11:10.390 --> 00:11:12.620
cutoff of my synthesizer.

00:11:12.620 --> 00:11:16.540
So I've got this method here,
on sensor exchange, which

00:11:16.540 --> 00:11:22.640
is going to be called each time
the value of rotation changes.

00:11:22.640 --> 00:11:25.850
This comes in-- it'll
be a range of values

00:11:25.850 --> 00:11:27.420
between minus 1 and 1.

00:11:27.420 --> 00:11:31.160
So I have kind of some
scaling and some mapping

00:11:31.160 --> 00:11:35.420
here to get it into the value
range expected by my synth.

00:11:39.132 --> 00:11:42.190
So here we go.

00:11:42.190 --> 00:11:43.220
Ignore the red.

00:11:43.220 --> 00:11:45.280
That's just Android Studio.

00:11:45.280 --> 00:11:49.690
OK, so I'm going to run this
app and what should happen

00:11:49.690 --> 00:11:53.950
is, when I tap down on the
screen, we hear a sound.

00:11:53.950 --> 00:11:57.310
And when I rotate the device,
that sound should change.

00:11:57.310 --> 00:12:00.013
So moment of truth.

00:12:00.013 --> 00:12:03.450
[SYNTHESIZER NOISE]

00:12:05.490 --> 00:12:07.934
OK, first part done.

00:12:07.934 --> 00:12:09.802
[APPLAUSE]

00:12:11.830 --> 00:12:13.330
HONGCHAN CHOI: OK,
it's my turn now.

00:12:13.330 --> 00:12:14.038
DON TURNER: Yeah.

00:12:14.038 --> 00:12:16.340
HONGCHAN CHOI: So before we
move onto the code eater,

00:12:16.340 --> 00:12:18.720
I'd like to talk a little bit
about how this whole thing

00:12:18.720 --> 00:12:19.780
works on web browsers.

00:12:19.780 --> 00:12:22.020
So let me go back to the slide.

00:12:24.890 --> 00:12:29.160
So obviously we cannot use the
same C++ source code as is,

00:12:29.160 --> 00:12:32.670
so we need to go through a
few steps to make it work.

00:12:32.670 --> 00:12:36.080
So first, we start from the
source code synthesizer h.

00:12:36.080 --> 00:12:39.710
This class has methods like
not on and not off for render,

00:12:39.710 --> 00:12:42.770
and that needs to be
compiled with Emscripten,

00:12:42.770 --> 00:12:46.840
the LLVM-based compiler that
compiles C++ source code

00:12:46.840 --> 00:12:50.180
into WebAssembly
or wasm for short.

00:12:50.180 --> 00:12:53.740
So ultimately, we will end
up with a synthesizer wasm JS

00:12:53.740 --> 00:12:54.560
file.

00:12:54.560 --> 00:12:57.560
And the JavaScript object
from this wasm module

00:12:57.560 --> 00:13:02.060
will have the same method that
we exposed, which is not on,

00:13:02.060 --> 00:13:04.590
not off, and render.

00:13:04.590 --> 00:13:07.260
But this mapping just
doesn't happen automatically,

00:13:07.260 --> 00:13:11.120
and that's what Emscript
and binding API's for.

00:13:11.120 --> 00:13:15.350
So I'll be creating a new C++
file for this binding work.

00:13:15.350 --> 00:13:18.680
And this file will include
the original source code

00:13:18.680 --> 00:13:22.780
and will be compiled
by Emscripten.

00:13:22.780 --> 00:13:25.660
This whole process will give
us the wasm source code,

00:13:25.660 --> 00:13:27.850
but how do we make
sound from it?

00:13:27.850 --> 00:13:30.250
That's where Audio
Worklet comes in.

00:13:30.250 --> 00:13:33.400
With Audio Worklet, you can
use Web Audio API's rendering

00:13:33.400 --> 00:13:37.510
thread to run JavaScript
code and make sound from it.

00:13:37.510 --> 00:13:39.830
It exposes low level
audio callback,

00:13:39.830 --> 00:13:42.220
so we can hook up our
synthesizer's render function

00:13:42.220 --> 00:13:43.560
in there.

00:13:43.560 --> 00:13:47.380
You can search Audio Worklet
for more information.

00:13:47.380 --> 00:13:49.020
So there are
several moving parts

00:13:49.020 --> 00:13:53.140
in Audio Worklet's system so
let me illustrate how it works.

00:13:53.140 --> 00:13:55.800
First, we're going to use
Audio Worklet processes

00:13:55.800 --> 00:13:58.440
to wrap this wasm synth module.

00:13:58.440 --> 00:14:01.720
This is because this class
has the audio callback.

00:14:01.720 --> 00:14:04.470
And this callback gets fired
about every three milliseconds

00:14:04.470 --> 00:14:07.030
depending on your separate.

00:14:07.030 --> 00:14:09.040
And the process
side the whole thing

00:14:09.040 --> 00:14:10.840
is handled by the audio thread.

00:14:10.840 --> 00:14:12.500
And the scope is
isolated, so it's

00:14:12.500 --> 00:14:15.190
very good for audio processing.

00:14:15.190 --> 00:14:17.770
And all of this stuff
goes into a separate file

00:14:17.770 --> 00:14:20.050
since process.js.

00:14:20.050 --> 00:14:22.750
But at this point, we
cannot control the processor

00:14:22.750 --> 00:14:25.870
because it's in the other
thread, the audio thread.

00:14:25.870 --> 00:14:28.870
That's why we have something
called the Audio Worklet node.

00:14:28.870 --> 00:14:31.540
It's a main thread
counterpart of the processor.

00:14:34.220 --> 00:14:36.980
But then, how do they
talk to each other?

00:14:36.980 --> 00:14:40.080
And for that, we have something
called Message Import.

00:14:40.080 --> 00:14:42.740
So this Message Import
allows these two objects

00:14:42.740 --> 00:14:46.350
to talk to each
other asynchronously.

00:14:46.350 --> 00:14:49.830
For all the main thread
meetings like UI and MIDI

00:14:49.830 --> 00:14:54.630
that goes into index.js file,
and that's our main script

00:14:54.630 --> 00:14:56.850
file for our demo application.

00:14:56.850 --> 00:14:59.100
OK, I'm going to switch
back to my code editor.

00:15:04.070 --> 00:15:06.160
So let's talk about
this file first.

00:15:06.160 --> 00:15:08.060
This file is our binding file.

00:15:08.060 --> 00:15:10.690
So, first, I need to do--

00:15:10.690 --> 00:15:14.890
I need to include these
synthesizers h source file here

00:15:14.890 --> 00:15:17.530
that's happening right here.

00:15:17.530 --> 00:15:20.680
Actually, I'm extending the
original synthesizer class

00:15:20.680 --> 00:15:23.770
to create a small,
thin replicas.

00:15:23.770 --> 00:15:25.570
But why do I need this?

00:15:25.570 --> 00:15:28.270
That's because
Emscripten's binding API

00:15:28.270 --> 00:15:31.060
is a little bit picky
about having raw pointer

00:15:31.060 --> 00:15:32.590
to the floating point array.

00:15:32.590 --> 00:15:34.540
So actually what
I'm doing right here

00:15:34.540 --> 00:15:39.320
is I'm doing manual typecasting
to walk around this issue.

00:15:39.320 --> 00:15:43.120
It's a little bit odd,
but please bear with me.

00:15:43.120 --> 00:15:46.180
You can look up Emscripten's
binding API documentation

00:15:46.180 --> 00:15:47.410
for more detail.

00:15:47.410 --> 00:15:48.790
But what I'm doing
right here is,

00:15:48.790 --> 00:15:50.860
after the manual
typecasting, I'm

00:15:50.860 --> 00:15:55.280
calling the original
synthesizer's render function.

00:15:55.280 --> 00:15:58.390
So let's take a look at the
second half, the bottom half,

00:15:58.390 --> 00:15:59.090
of the file.

00:15:59.090 --> 00:16:03.550
This is where actual
binding happens.

00:16:03.550 --> 00:16:05.770
So for the first
four lines of code,

00:16:05.770 --> 00:16:07.300
basically what I'm
doing here is I'm

00:16:07.300 --> 00:16:10.840
exposing the function of
original synthesizer class.

00:16:10.840 --> 00:16:14.490
So I'm exposing constructor
and not on function and not

00:16:14.490 --> 00:16:16.000
on function, right?

00:16:16.000 --> 00:16:20.360
All of those functions is
already used in Don's demo.

00:16:20.360 --> 00:16:23.780
Then I'm exposing
the render function

00:16:23.780 --> 00:16:26.030
from the wrapper class,
which this function is

00:16:26.030 --> 00:16:29.090
with a little walk around
that we wrote above.

00:16:29.090 --> 00:16:31.320
So that's our binding power.

00:16:31.320 --> 00:16:33.290
So I think we're ready
for the compilation.

00:16:33.290 --> 00:16:35.150
And for the compilation
I already set up

00:16:35.150 --> 00:16:37.790
the Emscripten
compiler on my machine

00:16:37.790 --> 00:16:39.980
so we can just do
the compilation.

00:16:39.980 --> 00:16:43.560
Let me check out
our directory first.

00:16:43.560 --> 00:16:47.790
So I'm going to move on to
our wasm file directory.

00:16:47.790 --> 00:16:49.910
So, for the easier
compilation, I already

00:16:49.910 --> 00:16:51.210
set up my make file there.

00:16:51.210 --> 00:16:52.640
So I'll just type make here.

00:16:55.190 --> 00:16:57.330
And compilation is completed.

00:16:57.330 --> 00:16:59.560
So, if I click this
link over here,

00:16:59.560 --> 00:17:02.540
actually we can take a look
at what's happening inside

00:17:02.540 --> 00:17:04.345
of this wasm file.

00:17:04.345 --> 00:17:05.720
In the beginning
of file, it just

00:17:05.720 --> 00:17:07.950
looks like a random
JavaScript file.

00:17:07.950 --> 00:17:10.815
But if I scroll
down through enough,

00:17:10.815 --> 00:17:12.940
kind of interesting things
are happening over here.

00:17:12.940 --> 00:17:14.780
They just look
like assembly code.

00:17:14.780 --> 00:17:17.760
So, anyway, this is
our wasm synth module,

00:17:17.760 --> 00:17:20.300
so let's just move on.

00:17:20.300 --> 00:17:24.230
And then this is our
synth process dot js file.

00:17:24.230 --> 00:17:29.030
And this is where all the audio
related operation is happening.

00:17:29.030 --> 00:17:31.420
So first step I
need to do is I need

00:17:31.420 --> 00:17:35.210
to import wasm since module
that we just compile.

00:17:35.210 --> 00:17:37.720
So now we have the
dependency setup.

00:17:37.720 --> 00:17:40.000
Then I will touch
the constructor

00:17:40.000 --> 00:17:41.570
of audio clip processor.

00:17:41.570 --> 00:17:44.210
This is the class that
has audio callbacks.

00:17:44.210 --> 00:17:47.560
So we need to set up a
instance of wasm synth.

00:17:47.560 --> 00:17:49.600
So let me do that.

00:17:49.600 --> 00:17:52.210
Here we have two lines of code.

00:17:52.210 --> 00:17:56.230
The first line is constructor,
which is from C++ wall.

00:17:56.230 --> 00:17:58.750
And so this is
literally constructor

00:17:58.750 --> 00:18:01.100
that Don used in his demo.

00:18:01.100 --> 00:18:04.760
So I need to provide the
argument for this constructor,

00:18:04.760 --> 00:18:06.610
which is separate.

00:18:06.610 --> 00:18:09.130
And this separate is a global
property of this Audio Worklet

00:18:09.130 --> 00:18:10.020
global scale.

00:18:10.020 --> 00:18:12.950
It's not just random
magic variable.

00:18:12.950 --> 00:18:15.820
And, a the next line,
I'm using little helper

00:18:15.820 --> 00:18:20.380
called wasm audio buffer and for
easier wasm memory management.

00:18:20.380 --> 00:18:23.080
I'll get to that a little later.

00:18:23.080 --> 00:18:25.390
And this function right
here is a process function.

00:18:25.390 --> 00:18:28.490
This is our audio callback
in Audio Worklet system.

00:18:28.490 --> 00:18:33.510
So here I need to call
my render function there.

00:18:33.510 --> 00:18:36.210
This render function is
also from the C++ wall.

00:18:36.210 --> 00:18:37.860
So first, the argument
of this function

00:18:37.860 --> 00:18:41.400
is the actual roll pointed
to the floating point array.

00:18:41.400 --> 00:18:44.980
But there is no way to get
that roll pointer here,

00:18:44.980 --> 00:18:45.840
so what I'm doing--

00:18:45.840 --> 00:18:48.630
that's why I'm using my little
high profile wasm memory

00:18:48.630 --> 00:18:49.470
management.

00:18:49.470 --> 00:18:53.370
Because this utility function
will give me the address

00:18:53.370 --> 00:18:57.810
of wasm allocated memory block.

00:18:57.810 --> 00:19:01.950
Then I'm doing one more
step because the memory

00:19:01.950 --> 00:19:05.910
space provided by rebooting
API and wasm allocated memory

00:19:05.910 --> 00:19:07.770
and they're
completely separated.

00:19:07.770 --> 00:19:11.880
So I need to actually copy the
data rendered by synthesizer

00:19:11.880 --> 00:19:14.280
to the web audio
API's output buffer.

00:19:14.280 --> 00:19:18.050
So it's just one more
step to clone the data.

00:19:18.050 --> 00:19:20.930
For that, I'm using
typed array's set method.

00:19:20.930 --> 00:19:23.990
And this method uses
mem copy internally.

00:19:23.990 --> 00:19:27.270
So it is supposed
to be super fast.

00:19:27.270 --> 00:19:28.100
So that's done.

00:19:30.820 --> 00:19:33.430
Let me write one more
function because I

00:19:33.430 --> 00:19:36.370
like to have the similar
functionality with Don's demo.

00:19:36.370 --> 00:19:40.660
Don uses an on touch function
to create the test tune, right?

00:19:40.660 --> 00:19:41.440
So here I am.

00:19:41.440 --> 00:19:43.690
I'm creating play
test tone function.

00:19:43.690 --> 00:19:48.130
It's pretty much the same thing
with what Don did in his demo.

00:19:48.130 --> 00:19:50.460
So when it's down, is come--

00:19:50.460 --> 00:19:54.030
basically I can trigger not on.

00:19:54.030 --> 00:19:56.940
And when button is up,
I can trigger not off.

00:19:56.940 --> 00:20:01.120
I'll be using the
same 60 middle C.

00:20:01.120 --> 00:20:05.440
So now we have this
handling function setup.

00:20:05.440 --> 00:20:09.560
The very last step here is I
need to setup my message port.

00:20:09.560 --> 00:20:11.630
So if any messages
from the main thread--

00:20:11.630 --> 00:20:13.420
if it comes from
the main thread,

00:20:13.420 --> 00:20:18.280
I can trigger my play
test tune function.

00:20:18.280 --> 00:20:20.830
So I think we are almost ready
with the synth's process that

00:20:20.830 --> 00:20:22.240
JS.

00:20:22.240 --> 00:20:24.610
This is the very last step.

00:20:24.610 --> 00:20:26.110
What I'm doing here
right now is I'm

00:20:26.110 --> 00:20:28.340
registering the
synth's processor

00:20:28.340 --> 00:20:32.950
the custom class definition
under the name of my synth.

00:20:32.950 --> 00:20:36.850
So this will be a
keyword for my class.

00:20:36.850 --> 00:20:38.500
So let's remember that.

00:20:38.500 --> 00:20:40.630
So I'm going to move
onto my index.js.

00:20:40.630 --> 00:20:42.650
This is the main script file.

00:20:42.650 --> 00:20:46.080
So first step what
I'm doing here

00:20:46.080 --> 00:20:48.840
is I'm creating audio
context, which is a gateway

00:20:48.840 --> 00:20:50.690
object for Web Audio API.

00:20:50.690 --> 00:20:53.000
So that's what I have.

00:20:53.000 --> 00:20:55.970
Then I'm going to setup
audio-- the minimum audio graph

00:20:55.970 --> 00:20:57.510
to test this demo.

00:20:57.510 --> 00:21:01.010
So here I'm using Audio
Worklet add module.

00:21:01.010 --> 00:21:03.980
This is how to load
custom process definition

00:21:03.980 --> 00:21:06.110
into Web Audio API side.

00:21:06.110 --> 00:21:12.050
So I'm going to type the
file name of synth processor.

00:21:12.050 --> 00:21:14.390
And once the module
loading is completed,

00:21:14.390 --> 00:21:18.200
I can create Audio Worklet
node based on the keyword

00:21:18.200 --> 00:21:20.510
that I used for
the registration.

00:21:20.510 --> 00:21:22.520
And then I'm using
a small gain node

00:21:22.520 --> 00:21:24.380
because I found that
my synthesizer was

00:21:24.380 --> 00:21:25.770
too loud for the demo.

00:21:25.770 --> 00:21:30.650
So I'm cranking-- I'm little
bit lowering my volume there.

00:21:30.650 --> 00:21:35.300
Then I'm creating audio graph
from synthesizer to my volume

00:21:35.300 --> 00:21:38.890
controller and the
output of this laptop.

00:21:38.890 --> 00:21:42.240
That's kind of really
minimum audio graph.

00:21:42.240 --> 00:21:44.430
And lets look at the
next function right here.

00:21:44.430 --> 00:21:46.080
This is on button change.

00:21:46.080 --> 00:21:50.820
And I already set up this button
with the page on this test

00:21:50.820 --> 00:21:52.540
pages button over here.

00:21:52.540 --> 00:21:56.310
So right now if I do
anything like this,

00:21:56.310 --> 00:21:59.310
nothing happens because
it's on button change

00:21:59.310 --> 00:22:01.120
has nothing there.

00:22:01.120 --> 00:22:05.070
So what I want to do
here is I have to--

00:22:05.070 --> 00:22:08.880
I have to send this ease
down variable to message port

00:22:08.880 --> 00:22:12.120
so it can be delivered in the
Audio Worklet processor site.

00:22:12.120 --> 00:22:14.440
It's really simple.

00:22:14.440 --> 00:22:17.290
That's pretty much
what I will do here.

00:22:17.290 --> 00:22:19.760
So I think we are ready
for the test tone.

00:22:19.760 --> 00:22:22.310
So I'm going to
refresh the browser,

00:22:22.310 --> 00:22:25.478
and we'll activate the
page and audio engine.

00:22:25.478 --> 00:22:27.950
[COMPUTER NOISE]

00:22:27.950 --> 00:22:29.890
That's our test tone.

00:22:29.890 --> 00:22:31.427
And let's move
onto the next step.

00:22:31.427 --> 00:22:32.635
So I'll switch back to slide.

00:22:37.570 --> 00:22:41.860
So we just set up Audio Worklet
node an Audio Worklet processor

00:22:41.860 --> 00:22:43.370
and played a test tone.

00:22:43.370 --> 00:22:46.120
So next step is web
MIDI integration.

00:22:46.120 --> 00:22:50.440
And web MIDI API provides
us with the different types

00:22:50.440 --> 00:22:51.370
of object.

00:22:51.370 --> 00:22:54.880
But what we'll be using
today is actually MIDI input.

00:22:54.880 --> 00:22:58.460
MIDI input literally receives
a message from a MIDI device.

00:22:58.460 --> 00:23:01.810
And when that happens, it will
fire on MIDI Message Event

00:23:01.810 --> 00:23:02.560
Handler.

00:23:02.560 --> 00:23:05.620
So our job here
today is basically

00:23:05.620 --> 00:23:09.010
sending any incoming messages
from this on MIDI Message Event

00:23:09.010 --> 00:23:12.880
Handler and just pass them
back through Audio Worklet

00:23:12.880 --> 00:23:14.500
processor.

00:23:14.500 --> 00:23:18.310
And all of our MIDI integration
happens on the index.js.

00:23:18.310 --> 00:23:21.490
But we still need to go
back for audio clip process

00:23:21.490 --> 00:23:25.450
just a little bit because our
synthesizer processor is just

00:23:25.450 --> 00:23:28.402
still not ready for
incoming video messages.

00:23:28.402 --> 00:23:30.610
OK, I'm going to switch back
to my code editor again.

00:23:33.650 --> 00:23:36.690
So this is our
index.js file again.

00:23:36.690 --> 00:23:39.060
And here's a place
for the web MIDI API.

00:23:39.060 --> 00:23:42.320
So I'm going to create
MIDI access object.

00:23:42.320 --> 00:23:45.570
That's the gateway
for the web MIDI API.

00:23:45.570 --> 00:23:47.690
And once I have that
object, basically what

00:23:47.690 --> 00:23:49.760
I'm doing here in
three lines of code,

00:23:49.760 --> 00:23:52.400
is I'm iterating all
the detective MIDI input

00:23:52.400 --> 00:23:55.670
in the system and I'm assigned
the same event handling

00:23:55.670 --> 00:24:01.620
function to all of MIDI input
on MIDI Message Event Handler.

00:24:01.620 --> 00:24:03.750
And the content of the
event handling function

00:24:03.750 --> 00:24:05.250
is really simple as well.

00:24:05.250 --> 00:24:07.040
We're not doing any
pre processing here.

00:24:07.040 --> 00:24:10.140
We will just take the data,
just send to the Audio Worklet

00:24:10.140 --> 00:24:13.010
processor.

00:24:13.010 --> 00:24:14.360
So that's done.

00:24:14.360 --> 00:24:16.463
I think index.js file
is pretty much done.

00:24:16.463 --> 00:24:18.130
So I'm going to go
back to synth process

00:24:18.130 --> 00:24:22.540
that JS because we need to fill
this function in because handle

00:24:22.540 --> 00:24:25.520
media event is
currently doing nothing.

00:24:25.520 --> 00:24:29.590
So what I'm going to do here
is basically these few lines

00:24:29.590 --> 00:24:31.690
of code because we
are only interested

00:24:31.690 --> 00:24:36.250
in node on and node off,
two types of MIDI event.

00:24:36.250 --> 00:24:39.160
When MIDI event
node on comes in,

00:24:39.160 --> 00:24:42.430
basically I'm going
to trigger note

00:24:42.430 --> 00:24:45.970
on function in the
synthesizer just like this.

00:24:45.970 --> 00:24:49.730
And my argument
will be mode number,

00:24:49.730 --> 00:24:52.580
which is a second bite of
incoming media messages.

00:24:52.580 --> 00:24:55.040
And, similarly,
with node off case,

00:24:55.040 --> 00:24:56.780
I can trigger node off method.

00:24:59.360 --> 00:25:01.780
All right.

00:25:01.780 --> 00:25:03.580
One last step.

00:25:03.580 --> 00:25:05.890
Currently our own
Message Event Handler

00:25:05.890 --> 00:25:08.380
is tied to play
test on function,

00:25:08.380 --> 00:25:11.450
but this is something--
this is not what we want.

00:25:11.450 --> 00:25:15.550
So we're going to replace
with handle media event.

00:25:15.550 --> 00:25:18.690
So if there's any incoming MIDI
message to the process site,

00:25:18.690 --> 00:25:22.610
our handle media event
function gets called.

00:25:22.610 --> 00:25:24.610
All right, that's done I think.

00:25:24.610 --> 00:25:29.260
So I'm going to move on to my
test page, activate the web

00:25:29.260 --> 00:25:29.760
page.

00:25:29.760 --> 00:25:32.190
With some luck I should be
able to play my synthesizer

00:25:32.190 --> 00:25:35.454
with little MIDI
keyboard over here.

00:25:35.454 --> 00:25:39.200
[COMPUTER NOISES]

00:25:39.200 --> 00:25:42.240
All right, that's
the end of my demo.

00:25:42.240 --> 00:25:44.355
[APPLAUSE]

00:25:44.355 --> 00:25:46.230
I'm going to switch back
to my slide, please.

00:25:50.390 --> 00:25:52.300
Let me remind you
one more time that we

00:25:52.300 --> 00:25:55.120
started from the same
source code, right?

00:25:55.120 --> 00:25:58.690
And it comes with amazing
benefits like less code

00:25:58.690 --> 00:26:04.240
to maintain and also identical
sound across two platforms

00:26:04.240 --> 00:26:05.820
and so on.

00:26:05.820 --> 00:26:06.820
You ready for the MIDI?

00:26:06.820 --> 00:26:07.528
DON TURNER: Sure.

00:26:07.528 --> 00:26:09.120
Great.

00:26:09.120 --> 00:26:12.290
OK, so Hongchan created
a synthesizer app

00:26:12.290 --> 00:26:13.610
and added MIDI support to it.

00:26:13.610 --> 00:26:16.200
I've already created
my synthesizer app,

00:26:16.200 --> 00:26:18.020
I just need to add MIDI support.

00:26:18.020 --> 00:26:20.750
And to do that, I'm going
to use the new Native MIDI

00:26:20.750 --> 00:26:25.730
API in Android Q. So this
is a high performance

00:26:25.730 --> 00:26:30.988
API that's designed to use
inside your audio callback.

00:26:30.988 --> 00:26:32.530
But there's still
a bit of setup work

00:26:32.530 --> 00:26:36.250
that you need to do inside Java,
so I'll just talk through that.

00:26:36.250 --> 00:26:39.950
In Java, you need to listen
for MIDI device connections.

00:26:39.950 --> 00:26:43.850
So when you actually plug-in
your MIDI controller,

00:26:43.850 --> 00:26:45.860
you will receive
a callback which

00:26:45.860 --> 00:26:48.680
allows you to open this device.

00:26:48.680 --> 00:26:50.820
You then get a Java
MIDI device object,

00:26:50.820 --> 00:26:55.410
which you pass through
JNI to the Native API.

00:26:55.410 --> 00:26:59.380
You convert that from a Java
object to a Native object.

00:26:59.380 --> 00:27:01.660
And then you open
an output port--

00:27:01.660 --> 00:27:04.390
so an output port on the
device itself, which you

00:27:04.390 --> 00:27:08.520
can receive MIDI messages from.

00:27:08.520 --> 00:27:10.610
And then you can start
receiving MIDI messages.

00:27:10.610 --> 00:27:12.560
Now, because we're
short on time,

00:27:12.560 --> 00:27:15.770
I'm just going to focus on
this very last part here,

00:27:15.770 --> 00:27:17.930
the received MIDI messages.

00:27:17.930 --> 00:27:20.390
But there's source code
online which shows you

00:27:20.390 --> 00:27:21.590
how to do the rest this.

00:27:21.590 --> 00:27:28.670
So if we could switch back
to the machine, please.

00:27:28.670 --> 00:27:33.270
OK, so I'm back in
my audio engine class

00:27:33.270 --> 00:27:35.550
and here's my audio callback.

00:27:35.550 --> 00:27:38.600
So this is the one that's being
called every few milliseconds.

00:27:38.600 --> 00:27:42.730
And I've got this
process MIDI method here,

00:27:42.730 --> 00:27:47.200
which is actually
implemented down here.

00:27:50.008 --> 00:27:52.070
And this is going to be called--

00:27:52.070 --> 00:27:53.690
sorry, I'll just
scroll back up there.

00:27:53.690 --> 00:27:56.810
This is going to be
called before I change--

00:27:56.810 --> 00:27:59.960
before I render my
synthesizer frames.

00:27:59.960 --> 00:28:07.782
So what I need to do is call
a MIDI output port receive.

00:28:07.782 --> 00:28:09.240
So this is the main
method that you

00:28:09.240 --> 00:28:11.040
use for receiving MIDI data.

00:28:11.040 --> 00:28:16.674
I've already set up an
output port somewhere.

00:28:24.580 --> 00:28:27.250
So everything is in red because
Android Studio's indexer

00:28:27.250 --> 00:28:29.740
has currently broken,
which is going

00:28:29.740 --> 00:28:32.180
to make this incredibly fun.

00:28:32.180 --> 00:28:36.100
So the second parameter to
this method is an opt code,

00:28:36.100 --> 00:28:39.580
and that can be
either a data type

00:28:39.580 --> 00:28:42.230
or it can be a flush message.

00:28:42.230 --> 00:28:44.890
So if the app code
is set to data,

00:28:44.890 --> 00:28:46.630
it means there's new
MIDI data available.

00:28:46.630 --> 00:28:49.330
If it's flush, it means
that any MIDI data

00:28:49.330 --> 00:28:51.880
that you received before,
you need to get rid of it

00:28:51.880 --> 00:28:54.540
because it's now stale.

00:28:54.540 --> 00:28:58.620
I also specify a buffer to
actually store the MIDI message

00:28:58.620 --> 00:28:59.480
in.

00:28:59.480 --> 00:29:03.300
I tell the method
how large that buffer

00:29:03.300 --> 00:29:05.210
is so that I don't overflow it.

00:29:05.210 --> 00:29:09.880
And I also specify a
message size which tells me

00:29:09.880 --> 00:29:12.130
how big that message was.

00:29:12.130 --> 00:29:14.380
Typically, this would be
three bytes for a note

00:29:14.380 --> 00:29:16.890
on or note off message.

00:29:16.890 --> 00:29:20.850
And the last parameter
is a timestamp.

00:29:20.850 --> 00:29:24.860
And that allows me to
reorder MIDI messages

00:29:24.860 --> 00:29:26.750
based on the order
that they were actually

00:29:26.750 --> 00:29:30.480
tapped by the musician.

00:29:30.480 --> 00:29:36.620
OK, so this gives me everything
I need to receive MIDI data.

00:29:36.620 --> 00:29:38.750
So I just need to
check my opt code

00:29:38.750 --> 00:29:39.980
to see whether it was data.

00:29:43.260 --> 00:29:45.060
And I'll also check
the message size just

00:29:45.060 --> 00:29:48.510
to check that it
is greater than 0.

00:29:48.510 --> 00:29:52.260
So we now know we definitely
have a MIDI message.

00:29:52.260 --> 00:29:55.590
And as Hongchan did
in his app, I just

00:29:55.590 --> 00:29:58.440
need to read the first two
bytes of my MIDI message

00:29:58.440 --> 00:30:01.050
to understand exactly
what the message was.

00:30:01.050 --> 00:30:04.380
So message.

00:30:04.380 --> 00:30:07.620
And I'm actually only
interested in the first four

00:30:07.620 --> 00:30:10.540
bits of this MIDI message.

00:30:10.540 --> 00:30:13.410
So I just use a mask
here and we'll just

00:30:13.410 --> 00:30:18.640
have a quick look at that
just to get the MIDI status.

00:30:18.640 --> 00:30:21.345
The second thing that I'm
interested in is the note,

00:30:21.345 --> 00:30:26.400
and that just comes straight
from the second byte.

00:30:26.400 --> 00:30:34.140
So I can now say, if the
status was a MIDI note on,

00:30:34.140 --> 00:30:36.170
then I'm going to switch
my synthesizer on.

00:30:43.800 --> 00:30:46.820
And I pass in the notes.

00:30:46.820 --> 00:30:53.650
Otherwise, if the
status was a note off,

00:30:53.650 --> 00:30:57.230
then I'll just send in a
note off with the same note.

00:31:00.480 --> 00:31:04.270
OK, I think that should work.

00:31:04.270 --> 00:31:06.240
So I'm going to rerun this now.

00:31:10.410 --> 00:31:12.750
So this is deploying
over this USB cable here.

00:31:12.750 --> 00:31:16.068
When I plug-in this
MIDI controller,

00:31:16.068 --> 00:31:17.610
I won't know if
anything's gone wrong

00:31:17.610 --> 00:31:20.170
because I won't be able to
receive lockout information.

00:31:20.170 --> 00:31:25.970
So what should happen
is when I plug this in

00:31:25.970 --> 00:31:30.024
and I tap on a key, we
should hear a sound.

00:31:30.024 --> 00:31:32.952
[COMPUTER NOISE]

00:31:41.760 --> 00:31:45.070
And add a stock note
there, so there we go.

00:31:45.070 --> 00:31:48.380
Yeah, that's MIDI working
on the Android app.

00:31:48.380 --> 00:31:51.220
[APPLAUSE]

00:31:51.220 --> 00:31:54.930
HONGCHAN CHOI: So can we
switch back to slide, please.

00:31:54.930 --> 00:31:56.850
So I think you're
ready for the jam now.

00:31:56.850 --> 00:31:58.950
So I prepared a little
backing track for us.

00:32:02.027 --> 00:32:03.860
DON TURNER: Yeah, we've
got a backing track.

00:32:03.860 --> 00:32:05.850
We're just going to
do a very short jam.

00:32:05.850 --> 00:32:06.910
See what it sounds like.

00:32:06.910 --> 00:32:09.490
Basically, because we've got
a big sound system so why not?

00:32:09.490 --> 00:32:12.440
HONGCHAN CHOI: Yeah, and Don
will be playing the bass line.

00:32:12.440 --> 00:32:14.240
And I'll be playing
the lead synths

00:32:14.240 --> 00:32:16.640
with our pager activated.

00:32:16.640 --> 00:32:19.460
So let me change my
setting real quick.

00:32:19.460 --> 00:32:22.450
[COMPUTER NOISES]

00:32:22.450 --> 00:32:22.950
All right.

00:32:22.950 --> 00:32:24.325
I think you're
ready for the jam.

00:32:24.325 --> 00:32:24.920
So you ready?

00:32:24.920 --> 00:32:27.066
DON TURNER: OK, yep.

00:32:27.066 --> 00:32:30.060
[MUSIC PLAYING]

00:33:18.770 --> 00:33:21.734
[APPLAUSE]

00:33:25.240 --> 00:33:28.253
HONGCHAN CHOI: Can we switch
back to slides, please?

00:33:28.253 --> 00:33:29.670
DON TURNER: So I'm
not sure that's

00:33:29.670 --> 00:33:32.290
going to be a number one hit.

00:33:32.290 --> 00:33:33.760
But it was OK.

00:33:33.760 --> 00:33:34.290
It was OK.

00:33:34.290 --> 00:33:35.280
HONGCHAN CHOI: Yeah, it was OK.

00:33:35.280 --> 00:33:37.155
DON TURNER: I think we
can definitely benefit

00:33:37.155 --> 00:33:39.505
from some expert advice.

00:33:39.505 --> 00:33:41.880
HONGCHAN CHOI: Sure, I mean
we can always improve, right?

00:33:41.880 --> 00:33:44.190
So how about asking
someone who has

00:33:44.190 --> 00:33:46.380
been working on software
synthesizer for more

00:33:46.380 --> 00:33:47.370
than a decade.

00:33:47.370 --> 00:33:49.890
Let's have Magnus Berger
from Propellerhead Software

00:33:49.890 --> 00:33:51.308
on stage.

00:33:51.308 --> 00:33:54.940
[APPLAUSE]

00:33:54.940 --> 00:33:56.780
MAGNUS BERGER: Thank you.

00:33:56.780 --> 00:33:58.790
OK, so my name is Magnus Berger.

00:33:58.790 --> 00:34:02.070
I'm the CTO of Propellerhead
Software in Stockholm, Sweden.

00:34:02.070 --> 00:34:05.960
Now, we've been around for about
25 years or exactly 25 years

00:34:05.960 --> 00:34:08.090
making music-making
applications.

00:34:08.090 --> 00:34:10.370
Our most well-known
software is called Reason.

00:34:10.370 --> 00:34:13.179
It's a virtual studio
for Windows and Mac OS.

00:34:13.179 --> 00:34:17.052
And you can create any kind
of music imaginable in it.

00:34:17.052 --> 00:34:18.469
So can we please
have my computer?

00:34:22.260 --> 00:34:24.630
This is what Reason looks like.

00:34:24.630 --> 00:34:26.293
It's essentially a
virtual rack, that's

00:34:26.293 --> 00:34:28.460
the most well-known feature,
into which you can drag

00:34:28.460 --> 00:34:31.489
and drop synthesizer's effects.

00:34:31.489 --> 00:34:33.980
And the interaction model is
actually you can push buttons.

00:34:33.980 --> 00:34:35.000
You can turn the knobs.

00:34:35.000 --> 00:34:37.670
Let's change the
filter frequency here.

00:34:37.670 --> 00:34:39.380
You can see it change.

00:34:39.380 --> 00:34:42.968
And let's load a preset.

00:34:42.968 --> 00:34:45.453
[COMPUTER NOISE]

00:34:48.940 --> 00:34:53.600
And that must be the
shortest Reason demo ever.

00:34:53.600 --> 00:34:57.050
So about 10 years ago,
we wanted to open up

00:34:57.050 --> 00:35:00.220
Reason for third party
developers to create plugins.

00:35:00.220 --> 00:35:03.230
We wanted to create a plugin
format that's future proof,

00:35:03.230 --> 00:35:05.000
and we call that
Rack Extensions.

00:35:05.000 --> 00:35:07.498
We did that by
making it sandboxed.

00:35:07.498 --> 00:35:10.040
But we didn't know exactly what
kind of hardware architecture

00:35:10.040 --> 00:35:12.120
we were going to run
on in the future.

00:35:12.120 --> 00:35:16.400
So, you see, DSP code is
usually written in C++.

00:35:16.400 --> 00:35:20.450
In our SDK, we hand out
an LLVM based tool chain.

00:35:20.450 --> 00:35:23.610
But instead of developers giving
us the final binary output,

00:35:23.610 --> 00:35:26.090
they're handing us LLVM
intermediate representation.

00:35:26.090 --> 00:35:27.050
The LLVM bit code.

00:35:27.050 --> 00:35:32.070
It's sort of a semi compiled
platform agnostic code,

00:35:32.070 --> 00:35:34.070
and then they upload
that to our servers.

00:35:34.070 --> 00:35:37.070
And we run this app store model
with more than 500 plug-ins.

00:35:37.070 --> 00:35:40.130
And we do the file completion
towards the target architecture

00:35:40.130 --> 00:35:42.053
of the customer.

00:35:42.053 --> 00:35:43.470
And then we have
a user interface.

00:35:43.470 --> 00:35:46.620
It's a declarative, retain
mode user interface, not

00:35:46.620 --> 00:35:47.920
unlike a web page.

00:35:47.920 --> 00:35:49.830
But this is written in Lua.

00:35:49.830 --> 00:35:52.830
So along comes all of
this cool web technology

00:35:52.830 --> 00:35:55.890
with Audio Worklets
and Web Audio web MIDI.

00:35:55.890 --> 00:35:58.740
And the technology fit there
is pretty much one to one

00:35:58.740 --> 00:36:00.660
with our philosophy
of Rack Extensions,

00:36:00.660 --> 00:36:03.540
so we just had to make
this internal tech demo.

00:36:03.540 --> 00:36:05.085
So this is not really a product.

00:36:05.085 --> 00:36:06.210
It's an internal tech demo.

00:36:06.210 --> 00:36:08.590
We had a lot of fun creating it.

00:36:08.590 --> 00:36:11.190
So we took this Lua code, the
user interface definitions,

00:36:11.190 --> 00:36:12.410
we made it transpilers.

00:36:12.410 --> 00:36:16.470
We transpiled that code
into JavaScript and HTML.

00:36:16.470 --> 00:36:20.790
We're taking LLVM bit code and
building that into WebAssembly.

00:36:20.790 --> 00:36:22.200
And this is what you get.

00:36:26.910 --> 00:36:28.860
Opening up Chrome instead.

00:36:28.860 --> 00:36:32.440
So this looks sort of
familiar, doesn't it?

00:36:32.440 --> 00:36:34.620
Vanilla Chrome, let's
drag and drop Europa.

00:36:34.620 --> 00:36:39.230
Europa, it's a rack extension
so we can board that here.

00:36:39.230 --> 00:36:40.025
And--

00:36:40.025 --> 00:36:42.300
[COMPUTER NOISES]

00:36:42.300 --> 00:36:43.951
Sweeping the filter frequency.

00:36:51.020 --> 00:36:53.370
Let's open a preset
that you heard before.

00:36:58.340 --> 00:37:01.322
[COMPUTER NOISES]

00:37:03.820 --> 00:37:06.723
Now, this wouldn't be half
as cool if it wasn't that we

00:37:06.723 --> 00:37:08.890
can take pretty much anything
from our own app store

00:37:08.890 --> 00:37:11.930
and just move it to the web
using this build tool chain.

00:37:11.930 --> 00:37:14.320
Let's try something else here.

00:37:14.320 --> 00:37:19.450
This is Polysix from Korg from
Japan, a very nice synthesizer.

00:37:19.450 --> 00:37:25.410
And let's stack a player that's
a MIDI effect on top of it.

00:37:25.410 --> 00:37:27.480
It's also a rack extension,
so it's fairly easy

00:37:27.480 --> 00:37:28.970
to move that to the web.

00:37:28.970 --> 00:37:31.170
[ELECTRONIC MUSIC]

00:37:31.170 --> 00:37:32.946
Changing the cutoff frequency.

00:37:37.320 --> 00:37:40.250
Add the rest to it.

00:37:40.250 --> 00:37:42.281
Pretty cool stuff.

00:37:42.281 --> 00:37:45.090
[APPLAUSE]

00:37:45.090 --> 00:37:49.220
Can you go to the presentation?

00:37:49.220 --> 00:37:51.650
So what could be
better than helping

00:37:51.650 --> 00:37:52.970
people express themselves?

00:37:52.970 --> 00:37:55.700
Well, just maybe possibly
helping even more people

00:37:55.700 --> 00:37:57.200
express themselves.

00:37:57.200 --> 00:37:58.850
And being backed
by a platform that

00:37:58.850 --> 00:38:00.620
literally supports
billions of users,

00:38:00.620 --> 00:38:02.570
that's definitely an enabler.

00:38:02.570 --> 00:38:07.040
And, by the way, this also
runs natively on phones.

00:38:07.040 --> 00:38:08.203
Back to Hongchan.

00:38:08.203 --> 00:38:10.000
[APPLAUSE]

00:38:10.000 --> 00:38:11.750
HONGCHAN CHOI: That
was really impressive.

00:38:11.750 --> 00:38:13.152
Thank you so much, Magnus.

00:38:13.152 --> 00:38:14.332
DON TURNER: Thanks, buddy.

00:38:14.332 --> 00:38:16.540
HONGCHAN CHOI: And having
partners like Propellerhead

00:38:16.540 --> 00:38:18.390
makes me really,
really inspiring.

00:38:18.390 --> 00:38:20.470
And I hope this can
be a positive signal

00:38:20.470 --> 00:38:22.330
to the other audio
developer as well.

00:38:22.330 --> 00:38:24.063
We have some stories
to share, right?

00:38:24.063 --> 00:38:25.480
DON TURNER: Yes,
I am lucky enough

00:38:25.480 --> 00:38:28.580
to work with the leading names
in the pro audio industry.

00:38:28.580 --> 00:38:30.340
And, over the past
12 months, we've

00:38:30.340 --> 00:38:32.270
had some fantastic
success stories.

00:38:32.270 --> 00:38:34.690
And I just wanted to share
very quickly two of them

00:38:34.690 --> 00:38:36.630
with you now.

00:38:36.630 --> 00:38:40.680
The first one is from
Izotope, longtime big player

00:38:40.680 --> 00:38:43.110
in the world of audio
processing plugins.

00:38:43.110 --> 00:38:44.820
They recently launched
a product called

00:38:44.820 --> 00:38:48.720
Spire Studio, which is basically
a mobile recording studio.

00:38:48.720 --> 00:38:50.770
And it comes with
a companion app,

00:38:50.770 --> 00:38:53.610
which allows you to do
multi-track recording.

00:38:53.610 --> 00:38:58.140
When they launched on
Android, they saw a nearly 40%

00:38:58.140 --> 00:39:00.240
increase in sales.

00:39:00.240 --> 00:39:02.700
And this is absolutely
fantastic because it

00:39:02.700 --> 00:39:06.270
shows that Android users have
a demand for high end audio

00:39:06.270 --> 00:39:08.320
hardware.

00:39:08.320 --> 00:39:11.970
Another success story comes
from Music World Media.

00:39:11.970 --> 00:39:15.420
Just a few years ago, there
were a tiny startup in Paris.

00:39:15.420 --> 00:39:18.060
And since then, they've
launched an incredible number

00:39:18.060 --> 00:39:23.580
of successful apps and games in
the musically creative space.

00:39:23.580 --> 00:39:25.470
And over the past
12 months, they've

00:39:25.470 --> 00:39:29.100
been able to acquire
an astonishing 45

00:39:29.100 --> 00:39:32.320
million new Android users
all across the globe.

00:39:32.320 --> 00:39:35.670
And for me, this shows
the incredible reach

00:39:35.670 --> 00:39:39.190
that Android has throughout
the entire world.

00:39:39.190 --> 00:39:40.490
So some great stories there.

00:39:40.490 --> 00:39:42.240
You've got some stories
from the web side?

00:39:42.240 --> 00:39:43.200
HONGCHAN CHOI: Sure.

00:39:43.200 --> 00:39:46.050
So things have been pretty
great on the web world.

00:39:46.050 --> 00:39:49.530
So the web music ecosystem
has gotten a lot more diverse,

00:39:49.530 --> 00:39:52.170
and these are my three favorite
digital audio workstation

00:39:52.170 --> 00:39:53.280
on the web.

00:39:53.280 --> 00:39:55.380
Audiotool is a fully
fledged music studio

00:39:55.380 --> 00:39:56.950
with open ended workflow.

00:39:56.950 --> 00:39:58.500
It's user interface is amazing.

00:39:58.500 --> 00:40:00.580
You can collaborate
in real time.

00:40:00.580 --> 00:40:03.120
And Amped Studio, which I
used for my backing track

00:40:03.120 --> 00:40:06.450
right there, comes with high
quality sample libraries

00:40:06.450 --> 00:40:07.710
and instrument.

00:40:07.710 --> 00:40:10.590
It's foundation was featured
in Chrome Developer Summit

00:40:10.590 --> 00:40:13.410
last year and they have
been pioneering cutting

00:40:13.410 --> 00:40:15.600
edge features for WebAssembly.

00:40:15.600 --> 00:40:17.160
So it's amazing
all of these tools

00:40:17.160 --> 00:40:19.420
are freely available
today on the web.

00:40:19.420 --> 00:40:21.000
And, lastly, the Ableton.

00:40:21.000 --> 00:40:22.740
With their learning
music web app,

00:40:22.740 --> 00:40:24.450
you can instantly
start learning how

00:40:24.450 --> 00:40:26.610
to create song by going
through masterfully

00:40:26.610 --> 00:40:29.610
crafted content step by step.

00:40:29.610 --> 00:40:33.180
And you can even export your
work to Ableton live from here.

00:40:33.180 --> 00:40:35.760
Ableton put the potential
of the web music platform,

00:40:35.760 --> 00:40:38.550
where platform as an
educational medium.

00:40:38.550 --> 00:40:41.520
And this project right here
is a visual realization

00:40:41.520 --> 00:40:43.060
of such a vision.

00:40:43.060 --> 00:40:45.890
OK, let's wrap up what
we talked about today.

00:40:45.890 --> 00:40:47.640
We showed you that
both Chrome and Android

00:40:47.640 --> 00:40:49.380
are more than capable
of doing real time

00:40:49.380 --> 00:40:52.170
audio with MIDI support.

00:40:52.170 --> 00:40:53.733
And through the
live coding demo,

00:40:53.733 --> 00:40:55.650
we showed you that you
can use the same source

00:40:55.650 --> 00:40:57.340
code on both platforms.

00:40:57.340 --> 00:41:02.380
And finally, the enormous
reach of Android and Chrome.

00:41:02.380 --> 00:41:04.130
DON TURNER: Yeah,
so I just wanted

00:41:04.130 --> 00:41:06.860
to make one final
point, which is

00:41:06.860 --> 00:41:09.500
I think it's very important
to remember that music

00:41:09.500 --> 00:41:11.940
is a universal language.

00:41:11.940 --> 00:41:14.782
And it's something that's
understood by everyone,

00:41:14.782 --> 00:41:16.490
regardless of where
you are in the world,

00:41:16.490 --> 00:41:19.340
your cultural background,
or economic status.

00:41:19.340 --> 00:41:22.100
And it can be a
powerful force for good,

00:41:22.100 --> 00:41:23.900
for people to communicate.

00:41:23.900 --> 00:41:25.840
And, with your
programming skills,

00:41:25.840 --> 00:41:28.550
and Android and Chrome's
incredible reach,

00:41:28.550 --> 00:41:30.710
then we can give
everyone the tools

00:41:30.710 --> 00:41:33.590
that they need to be musically
creative so that they can not

00:41:33.590 --> 00:41:36.350
just listen to this
universal language,

00:41:36.350 --> 00:41:39.420
but they can speak it as well.

00:41:39.420 --> 00:41:44.030
Search for Audio Worklet and
Oboe Library to get started.

00:41:44.030 --> 00:41:45.400
Thank you all very much.

00:41:45.400 --> 00:41:50.250
[GOOGLE LOGO MUSIC]

