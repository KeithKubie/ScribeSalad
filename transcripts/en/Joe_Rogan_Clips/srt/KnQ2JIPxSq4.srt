1
00:00:00,960 --> 00:00:04,590
The Joe Rogan experience,
they back to your core point.

2
00:00:04,740 --> 00:00:08,790
I mean that's the reason why I also write
science fiction is that the world of

3
00:00:08,791 --> 00:00:13,791
science is changing so fast that we really
need to apply a lot of imagination to

4
00:00:15,121 --> 00:00:18,570
imagine where it's going.
Because if you're just looking
at what's happening now,

5
00:00:18,900 --> 00:00:21,390
it's like this train is
going to speed by you.

6
00:00:21,391 --> 00:00:23,130
We have to kind of imagine
it's like Wayne Gretzky.

7
00:00:23,131 --> 00:00:27,120
We have to imagine where the puck is
going to be, not where it is now. And I,

8
00:00:27,450 --> 00:00:30,460
I mentioned George Church who's
like, he's at Harvard, he's like the,

9
00:00:30,461 --> 00:00:32,010
the Living Charles Darwin.

10
00:00:32,011 --> 00:00:36,090
And I do a lot of speaking alongside
George and it's become our little thing

11
00:00:36,091 --> 00:00:40,290
that he says that he reads,
um, science fiction like mine.

12
00:00:40,291 --> 00:00:42,720
And then imagine says, well, that's
pretty cool. How can we do that?

13
00:00:43,230 --> 00:00:47,340
And what I do is I look at the research
coming out of labs like Georges and I,

14
00:00:47,341 --> 00:00:49,080
sorry,
well that's where we are now.

15
00:00:49,380 --> 00:00:53,370
What's that going to mean in 2050
a hundred years? And so we have to,

16
00:00:53,400 --> 00:00:58,400
that the science fiction plays a more
important role than it ever has and kind

17
00:00:58,561 --> 00:01:03,561
of imagining where we're going and it's
that imagining that allows us to try to

18
00:01:04,261 --> 00:01:07,110
say, well, what if that's one of
the options of where we're going?

19
00:01:07,470 --> 00:01:11,730
What are the decisions that we need to
make now so that we can have a better

20
00:01:11,731 --> 00:01:14,340
outcome rather than a worse
than if you are gambling person.

21
00:01:14,341 --> 00:01:15,240
I don't know if you are,

22
00:01:15,450 --> 00:01:20,450
but if I had to give you 100 bucks
to put on something in 20 years,

23
00:01:21,031 --> 00:01:26,031
it's going to be
profoundly just eh change.

24
00:01:26,250 --> 00:01:31,250
It's going to change us in a way that
is something that we're not really

25
00:01:31,771 --> 00:01:34,230
prepared to understand or deal with.
What do you think that's going to be?

26
00:01:34,260 --> 00:01:38,100
I think it's going to be predictive
genetics that we're going to have.

27
00:01:38,100 --> 00:01:41,700
All right now it's like you go
to your doctor when you're sick.

28
00:01:42,240 --> 00:01:42,961
You could have been,

29
00:01:42,961 --> 00:01:47,400
this could have been some genetic disorder
that you had from the moment you were

30
00:01:47,401 --> 00:01:52,350
conceived and it was ticking and it was
ticking and you showed up 50 years later

31
00:01:52,500 --> 00:01:55,890
when that's been manifest.
So it's going to be very different.

32
00:01:55,891 --> 00:02:00,870
You're taking your kid home
from the hospital, your
newborn, and the doctor says,

33
00:02:00,871 --> 00:02:04,860
Hey, congratulations. This is
really great. Um, but just Fyi,

34
00:02:05,310 --> 00:02:09,120
your kid has a 50% greater than
average chance of getting early onset

35
00:02:09,300 --> 00:02:12,590
Alzheimer's 50 years
from now and your cat,

36
00:02:12,600 --> 00:02:17,430
your kid has a really great chance
of being phenomenal at abstract math.

37
00:02:17,640 --> 00:02:20,400
Like how are we going to think about how
we're gonna think about what it means

38
00:02:20,460 --> 00:02:23,520
to be human when we have
all of that information?

39
00:02:23,521 --> 00:02:26,630
And there are things now that we
call fate and it's, it's, it's, uh,

40
00:02:26,700 --> 00:02:30,330
just a different model. And so I
think that, and once we have that,

41
00:02:30,331 --> 00:02:31,710
that's going to change a
lot of a lot of things.

42
00:02:31,711 --> 00:02:34,010
It's going to fundamentally
transform our healthcare.

43
00:02:34,080 --> 00:02:36,240
What we call healthcare
now is really sick care.

44
00:02:36,241 --> 00:02:38,910
You show up with a symptom and
this is going to be predictive.

45
00:02:38,940 --> 00:02:40,050
And it's going to change the we,

46
00:02:40,051 --> 00:02:45,051
the way we make babies because people are
going to have real choices about which

47
00:02:45,241 --> 00:02:49,070
embryos to implant and we're going
to have a lot of information about,

48
00:02:49,090 --> 00:02:50,730
I read a lot of really intimate stuff.

49
00:02:50,790 --> 00:02:54,090
So you feel like genetic
manipulation and genetic engineering,

50
00:02:54,091 --> 00:02:58,860
genetic understanding, genetic knowledge,
and then applied genetic medicine,

51
00:02:59,120 --> 00:03:01,300
those are going to be the big
changes in the next 20 years,

52
00:03:01,301 --> 00:03:04,960
even more so than technology,
which it's interconnected because these,

53
00:03:04,990 --> 00:03:08,260
there's really, it's like a super
convergence of these technologies. So the,

54
00:03:08,650 --> 00:03:12,250
the genetics revolution is the artificial
intelligence revolution in a sense

55
00:03:12,251 --> 00:03:15,310
that the complexity of
genetics is so great,

56
00:03:15,311 --> 00:03:19,660
it's way beyond what our brains
on their own could process.

57
00:03:19,690 --> 00:03:23,860
And so really with all of these
technologies are touching each other.

58
00:03:23,861 --> 00:03:28,660
And so the biological models
are now influencing the AI.

59
00:03:28,690 --> 00:03:33,520
So for example, we are coming to
the limits of silicon storage,

60
00:03:34,000 --> 00:03:37,840
but DNA has unlimited storage capacity.

61
00:03:37,841 --> 00:03:40,390
So it's the,
as I've said before,

62
00:03:40,391 --> 00:03:45,391
the kind of the boundaries between biology
and AI or genetics and AI is going to

63
00:03:46,241 --> 00:03:49,540
be very blurry. Yeah. That is a,
an interesting concept, right?

64
00:03:49,541 --> 00:03:54,130
The idea of storing information in
DNA and that has been discussed. Yeah.

65
00:03:54,131 --> 00:03:59,131
Which the great DNA is the
greatest information storage
mechanism ever imagined.

66
00:03:59,470 --> 00:04:02,560
But the question is what happens when
you do store things in there and how does

67
00:04:02,561 --> 00:04:05,140
that information interact with all the
rest of the stuff that's in your body

68
00:04:05,141 --> 00:04:07,520
already? Well, I mean, if you can
do it in your body, you could,

69
00:04:07,560 --> 00:04:10,620
doesn't have to be in,
doesn't your body just think of like your,

70
00:04:10,630 --> 00:04:15,630
your DNA has 4 billion years of history
and it's done a great job of recording

71
00:04:16,121 --> 00:04:19,300
it. It's incredible. Like am I
have old eight track tapes? They,

72
00:04:19,310 --> 00:04:20,470
they haven't lasted.

73
00:04:20,890 --> 00:04:24,700
That is a squirrely concept that you have
all that data inside your head. I mean,

74
00:04:24,701 --> 00:04:27,400
that's also when people make,

75
00:04:28,120 --> 00:04:31,270
when they try to understand
instincts yeah. That people have,

76
00:04:31,370 --> 00:04:35,620
these are some sort of genetically
encoded memories or some understanding of

77
00:04:35,621 --> 00:04:37,840
things that are dangerous and that these,

78
00:04:37,900 --> 00:04:42,170
they're in there because this is how
we've learned over the years without

79
00:04:42,190 --> 00:04:45,630
actually having experienced these things
personally. Yeah. Yeah. No. So it's,

80
00:04:46,060 --> 00:04:50,890
it's baked in. Our genetics are
baked into us. And so, you know,

81
00:04:51,610 --> 00:04:52,990
I don't know if you've been to Indonesia,

82
00:04:53,090 --> 00:04:56,950
I was in Indonesia and they went to
this place called Komodo island. Oh Wow.

83
00:04:56,951 --> 00:04:58,240
The dragons Komodo Dragon.

84
00:04:58,450 --> 00:05:01,270
And it was fascinating because it's
like you can tell they don't have

85
00:05:01,271 --> 00:05:03,030
plaintiff's attorney.
So here's walking around there,

86
00:05:03,040 --> 00:05:04,300
all these Komodo dragons and so yeah,

87
00:05:04,301 --> 00:05:07,090
these are like the most deadly creatures
on earth and there's like some little

88
00:05:07,091 --> 00:05:10,150
guy with a little stick and it's like,
well how effective is that stick?

89
00:05:10,570 --> 00:05:11,291
But the way it works,

90
00:05:11,291 --> 00:05:14,980
so you're just walking around walking
around cause they come checks when they're

91
00:05:14,981 --> 00:05:18,010
not killing people or killing animals.
They're just sitting there.

92
00:05:18,090 --> 00:05:21,060
And so it's pretty scary.
Do they ever get Jack,

93
00:05:21,061 --> 00:05:23,380
do people ever go there and
get bitten? Yes. And they say,

94
00:05:23,381 --> 00:05:25,600
oh it's only a few times a year.
It's like, well a few times. Yeah,

95
00:05:25,601 --> 00:05:27,550
that seems like a lot of times.
A year is a lot.

96
00:05:27,610 --> 00:05:29,890
But the way it works for a Komodo Dragon,

97
00:05:30,420 --> 00:05:35,420
a mother lays the egg and then buries the
egg and then forgets where the egg is.

98
00:05:36,940 --> 00:05:41,940
And then let's just say that this egg
hatches and this little Komodo dragon

99
00:05:42,010 --> 00:05:45,730
comes out and the mother sees
her own baby Komodo Dragon.

100
00:05:45,970 --> 00:05:50,110
She'll eat it in a second. Oh Jeez.
And so if you're a Komodo Dragon,

101
00:05:50,440 --> 00:05:55,440
you better have your entire survival
strategy baked into your DNA because

102
00:05:55,780 --> 00:05:59,600
nobody's teaching you anything.
And so for us,

103
00:05:59,850 --> 00:06:03,020
we have this sense that it's like
parenting is really important.

104
00:06:03,021 --> 00:06:04,970
It is environment. It is
really important. It is.

105
00:06:05,180 --> 00:06:09,170
But so much of who and what we
are is baked in to our genetics.

106
00:06:09,290 --> 00:06:11,270
And I think that's,
that's going to be this challenge.

107
00:06:11,280 --> 00:06:14,330
We're going to see ourselves
as increasingly genetic beings.

108
00:06:14,600 --> 00:06:18,020
We can't become genetic determinist
think that we're just genetics,

109
00:06:18,050 --> 00:06:19,730
but we're going to know a lot more.

110
00:06:19,731 --> 00:06:23,930
We're going to demystify a lot of what
it means to be a human. Poof. Yeah,

111
00:06:24,370 --> 00:06:27,900
tiff is right, but are we going
to lose the romance and the,

112
00:06:27,901 --> 00:06:30,760
the just the randomness
of life because of that.

113
00:06:30,761 --> 00:06:33,410
That's what people are
concerned with. Right? Like if,

114
00:06:33,650 --> 00:06:38,650
if we have some sort of genetic uniformity
mean especially in particularly with

115
00:06:39,051 --> 00:06:41,600
like things like intelligence
and athletic performance,

116
00:06:41,840 --> 00:06:45,590
we're not gonna appreciate freaks as
much or maybe we'll all want to be freaks

117
00:06:45,591 --> 00:06:49,790
because the freaks are the ones who push
us into I am not going to want to be a

118
00:06:49,791 --> 00:06:54,560
moron. Yeah. Well your your question,
it's, it's the essential question.

119
00:06:54,561 --> 00:06:58,310
It's like what makes a human a human isn't
to some way of Higher Iq that doesn't

120
00:06:58,311 --> 00:07:00,680
make you a better human that makes
you someone with a higher Iq.

121
00:07:01,160 --> 00:07:05,810
But how are we going to think about
constructing societies when it's up to us?

122
00:07:05,811 --> 00:07:09,890
Like if we are going to say
we value certain people,

123
00:07:09,891 --> 00:07:12,770
certain ideas,
I think we're going to need artists.

124
00:07:12,771 --> 00:07:16,420
Like right now people like artists
are sometimes in the mainstream,

125
00:07:16,450 --> 00:07:17,420
sometime they're on the fringe,

126
00:07:17,421 --> 00:07:20,840
but artists are going to be maybe the
most important people in this new world

127
00:07:20,841 --> 00:07:21,170
then,
right?

128
00:07:21,170 --> 00:07:24,980
Like right now in hospitals we have
kind of a hierarchy and like the most

129
00:07:24,981 --> 00:07:28,640
technical people are the people who are
valued the most and the least technical

130
00:07:28,641 --> 00:07:29,150
people.

131
00:07:29,150 --> 00:07:33,160
Like some of the nurses or nurse's aides
are the people who were often valued

132
00:07:33,170 --> 00:07:34,130
and paid the least.

133
00:07:34,700 --> 00:07:39,590
But when technology can do
these technological feats,

134
00:07:40,040 --> 00:07:44,390
what's going to be left is how can
we be great humans? How can we emote?

135
00:07:44,391 --> 00:07:46,460
How can we connect?
How can we create art?

136
00:07:46,850 --> 00:07:51,550
And if we get swept away by
this tide of science, as in,

137
00:07:51,600 --> 00:07:53,600
you know,
how excited I am about the size a bit,

138
00:07:53,610 --> 00:07:58,310
if we could really undermine our humanity.
But as for humans,

139
00:07:58,311 --> 00:08:03,100
what humans value is many aspects of
that humanity. Art, the curations.

140
00:08:03,120 --> 00:08:07,930
Yeah, yeah. Literature things we, we,

141
00:08:08,060 --> 00:08:10,350
when you read someone's great pros,
you're,

142
00:08:10,351 --> 00:08:12,380
you're reading like an
insight into their mind.

143
00:08:12,381 --> 00:08:14,600
And that's what's interesting
about it, right? You're like, yes,

144
00:08:14,630 --> 00:08:18,140
you're not going to get that
from just ones and Zeros.
Yeah. And that's, and there,

145
00:08:18,350 --> 00:08:21,620
there will always be this
way we call it mystery.

146
00:08:21,830 --> 00:08:26,830
And even if we can do a genetic analysis
of Shakespeare and Mozart and and

147
00:08:27,411 --> 00:08:32,300
whatever, like it's still miraculous and
we need to celebrate that and we can't,

148
00:08:32,660 --> 00:08:36,740
we can't allow us to say that we are just
our genetics or even just our biology.

149
00:08:36,920 --> 00:08:40,270
But we also can't just say
biology has nothing to do with it.

150
00:08:40,271 --> 00:08:43,930
And especially because we're going to
know more about our biology. You know,

151
00:08:43,931 --> 00:08:46,820
about our, our, our differences.
And that's, that's normal.

152
00:08:46,821 --> 00:08:48,680
I mean it used to be in the old days they,
everyone thought,

153
00:08:48,681 --> 00:08:52,190
well God is weather and now we
understand weather pretty much.

154
00:08:52,191 --> 00:08:55,470
And nobody's saying, oh that lightening,
that's God is delivering message.

155
00:08:55,710 --> 00:09:00,090
It could be, but we still like, we still
have that mystery and I think that it's,

156
00:09:00,091 --> 00:09:02,220
in some ways it's about our orientation.

157
00:09:02,400 --> 00:09:06,480
Like how do we make sure that
we keep this view of life,

158
00:09:06,510 --> 00:09:11,510
that we have artists and humanists who
are just at the core of this conversation

159
00:09:12,301 --> 00:09:13,110
about where we're going.

160
00:09:13,110 --> 00:09:16,680
What if that mystery ultimately turns
out to just be ignorance and that as you

161
00:09:16,681 --> 00:09:19,320
develop more and more understanding,
there's less and less mystery?

162
00:09:19,680 --> 00:09:22,960
Would we like to be less smart?
Well, we like to be, yeah,

163
00:09:23,020 --> 00:09:27,600
or overwhelmed by possibility.
I mean that could be,

164
00:09:27,601 --> 00:09:29,340
I think those would be
part of what romance is.

165
00:09:29,550 --> 00:09:34,290
It could be and certainly like the unknown
every we wake up every morning. Sure.

166
00:09:34,350 --> 00:09:35,760
And we just don't know the answer.

167
00:09:35,761 --> 00:09:39,270
And there are some people like to going
back to the issues of of life extension.

168
00:09:39,720 --> 00:09:41,190
There are some people who say,

169
00:09:41,400 --> 00:09:45,630
well that death is essential
for appreciating life.

170
00:09:45,631 --> 00:09:49,340
I talk about this stuff all around and
then there are people who say, you know,

171
00:09:49,350 --> 00:09:52,080
you're talking about eliminating
these terrible diseases,

172
00:09:52,410 --> 00:09:57,410
but I know somebody who had that terrible
disease and their suffering was a gift

173
00:09:57,900 --> 00:10:01,770
to everybody else because we all had
more humanity in response to their

174
00:10:01,771 --> 00:10:03,450
suffering. Like, well
that's kind of screwed up.

175
00:10:03,451 --> 00:10:07,680
I'd prefer them to not have
that suffering, but those
people are thinking wacky.

176
00:10:07,710 --> 00:10:09,570
It's Wacky,
but we need to,

177
00:10:10,080 --> 00:10:14,070
I totally agree with you that if we allow
ourselves to get swept away with this

178
00:10:14,071 --> 00:10:15,570
kind of scientific determinism,

179
00:10:15,571 --> 00:10:20,571
if we don't say we really value
our humanistic traditions,

180
00:10:20,730 --> 00:10:22,770
our artists,
our cultures,

181
00:10:23,310 --> 00:10:27,630
we could get lost and we had to become
obsolete. We could become obsolete,

182
00:10:27,631 --> 00:10:29,370
but we could also just become less human.

183
00:10:29,371 --> 00:10:31,380
And there's something
wonderful and there's magic.

184
00:10:31,670 --> 00:10:35,130
But do you think that monkeys used
to think, man can become a human?

185
00:10:35,131 --> 00:10:37,740
We become less monkey.
You know what I'm saying?

186
00:10:37,741 --> 00:10:42,741
But no being looks in the mirror and
recognizes that they are evolving.

187
00:10:42,990 --> 00:10:47,460
Yeah. We've, well, we've only
been, homo sapiens were about
300,000 years. Right. Um,

188
00:10:47,580 --> 00:10:50,480
so we just, it's hard. We know
where we've come from. Cause you,

189
00:10:50,481 --> 00:10:53,100
you see all those little charts
from high school biology,

190
00:10:53,550 --> 00:10:57,870
but it's really hard for people to imagine
being something else in the future.

191
00:10:57,871 --> 00:11:01,800
It's, it's, it's outside of our, of our
consciousness. And so say she will square.

192
00:11:01,820 --> 00:11:05,520
Yeah. And we are monkeys. It's just
that we've redefined our monkey thing.

193
00:11:05,620 --> 00:11:08,580
You know, we, we do it with
a little different way.

