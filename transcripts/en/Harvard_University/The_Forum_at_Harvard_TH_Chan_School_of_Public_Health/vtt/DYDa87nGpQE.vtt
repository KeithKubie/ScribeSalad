WEBVTT
Kind: captions
Language: en

00:00:04.614 --> 00:00:06.280
MICHELLE WILLIAMS:
Welcome to The Forum,

00:00:06.280 --> 00:00:09.160
live streamed worldwide
from the Leadership Studio

00:00:09.160 --> 00:00:12.220
at the Harvard TH Chan
School of Public Health.

00:00:12.220 --> 00:00:14.410
I'm Dean Michelle Williams.

00:00:14.410 --> 00:00:17.800
The Forum is a collaboration
between the Harvard Chan School

00:00:17.800 --> 00:00:20.050
and independent news media.

00:00:20.050 --> 00:00:23.020
Each program features
a panel of experts

00:00:23.020 --> 00:00:26.860
addressing some of today's most
pressing public health issues.

00:00:26.860 --> 00:00:29.410
The Forum is one way
the school advances

00:00:29.410 --> 00:00:32.170
the frontiers of
public health and makes

00:00:32.170 --> 00:00:37.090
scientific insights accessible
to policymakers and the public.

00:00:37.090 --> 00:00:40.700
I hope you find this program
engaging and informative.

00:00:40.700 --> 00:00:42.924
Thank you for joining us.

00:00:42.924 --> 00:00:46.403
[MUSIC PLAYING]

00:00:59.350 --> 00:01:01.440
PHILLIP MARTIN: Well,
welcome, everyone.

00:01:01.440 --> 00:01:05.010
This is going to be an
extraordinary discussion

00:01:05.010 --> 00:01:07.650
and one that is timely,
to say the least.

00:01:07.650 --> 00:01:09.460
My name is Phillip Martin.

00:01:09.460 --> 00:01:13.170
I'm a senior investigative
reporter with WGBH in Boston

00:01:13.170 --> 00:01:16.320
and also contribute to The
World, PRI's The World.

00:01:16.320 --> 00:01:18.090
And I'm today's moderator.

00:01:18.090 --> 00:01:20.730
Our panelists starting
from my immediate right

00:01:20.730 --> 00:01:22.980
here are Oren Segal.

00:01:22.980 --> 00:01:25.440
He's the director of the
Anti-Defamation League

00:01:25.440 --> 00:01:27.180
Center on Extremism.

00:01:27.180 --> 00:01:30.255
We also have David Williams,
my friend David Williams,

00:01:30.255 --> 00:01:32.880
who's been on the panel with me,
or I've been on panel with him

00:01:32.880 --> 00:01:33.835
several times.

00:01:33.835 --> 00:01:36.210
He's the chair of the Department
of Social and Behavioral

00:01:36.210 --> 00:01:39.300
Sciences at the Harvard
Chan School here.

00:01:39.300 --> 00:01:44.190
We have Dipayan Ghosh, who is
a fellow at the Harvard Kennedy

00:01:44.190 --> 00:01:48.250
School and an expert
on social media.

00:01:48.250 --> 00:01:49.590
We'll be talking about that.

00:01:49.590 --> 00:01:53.830
Maureen Costello, all the way in
from Montgomery and Birmingham,

00:01:53.830 --> 00:01:56.940
Alabama, director of
Teaching Tolerance

00:01:56.940 --> 00:01:59.490
and member of the Southern
Poverty Law Center's

00:01:59.490 --> 00:02:01.740
senior leadership team.

00:02:01.740 --> 00:02:04.620
And my friend joining
us remotely again,

00:02:04.620 --> 00:02:08.190
Jim Doyle, former
senior Menschel fellow

00:02:08.190 --> 00:02:10.740
and former governor
and Attorney General

00:02:10.740 --> 00:02:14.110
of Wisconsin, a state that's
really in the news these days,

00:02:14.110 --> 00:02:15.240
isn't it?

00:02:15.240 --> 00:02:19.080
This event is being presented
jointly with PRI's The World

00:02:19.080 --> 00:02:20.950
and WGBH.

00:02:20.950 --> 00:02:23.220
And we're streaming live.

00:02:23.220 --> 00:02:26.576
Just want everyone to
get to your Facebook

00:02:26.576 --> 00:02:30.030
and your social media
because we're streaming live

00:02:30.030 --> 00:02:32.220
on Facebook and YouTube.

00:02:32.220 --> 00:02:35.940
This program will include
brief questions and answers.

00:02:35.940 --> 00:02:38.910
And you folks, you
can email questions

00:02:38.910 --> 00:02:40.800
to theforum@hsph.harvard.edu.

00:02:44.010 --> 00:02:45.690
I'm going to repeat that again--

00:02:45.690 --> 00:02:47.527
theforum@hsph.harvard.edu.

00:02:51.570 --> 00:02:53.640
And you can also
participate in a live chat

00:02:53.640 --> 00:02:57.360
that's happening on The Forum
site at this very moment.

00:02:57.360 --> 00:02:58.860
We've seen it.

00:02:58.860 --> 00:03:01.500
All you have to do is
turn on your television,

00:03:01.500 --> 00:03:03.390
turn on your radio.

00:03:03.390 --> 00:03:05.250
You've heard people
and seen people

00:03:05.250 --> 00:03:11.190
marching in Virginia shouting,
"The Jews will not replace us."

00:03:11.190 --> 00:03:14.430
We've seen a synagogue
where a massacre took place.

00:03:14.430 --> 00:03:18.420
We saw supermarket-- people
going out shopping and shot

00:03:18.420 --> 00:03:21.270
to death because of the
color of their skin.

00:03:21.270 --> 00:03:26.520
Acts of hate and racism,
whether online or in person,

00:03:26.520 --> 00:03:29.190
are painfully
visible these days.

00:03:29.190 --> 00:03:31.500
Statistics from
the FBI, as well as

00:03:31.500 --> 00:03:34.440
from organizations such as
the Southern Poverty Law

00:03:34.440 --> 00:03:37.560
Center and the
Anti-Defamation League,

00:03:37.560 --> 00:03:40.320
confirm that hate
crimes are on the rise.

00:03:40.320 --> 00:03:43.230
Such acts are, of course, not
only in the United States.

00:03:43.230 --> 00:03:45.840
We see this in Europe and
other places globally.

00:03:45.840 --> 00:03:48.030
Just take a look at
the streets of Poland.

00:03:48.030 --> 00:03:52.470
Or look at Hungary or the
Philippines or Brazil.

00:03:52.470 --> 00:03:57.400
And you see what they have in
common besides the populism--

00:03:57.400 --> 00:04:00.150
that is the term of choice--

00:04:00.150 --> 00:04:02.910
is a lot of hate.

00:04:02.910 --> 00:04:07.500
And today, during Black
History Month, we pause.

00:04:07.500 --> 00:04:13.620
We take a moment to ask,
what forces are fueling

00:04:13.620 --> 00:04:16.140
the rise of hate and racism?

00:04:16.140 --> 00:04:19.899
What's contributing to
what we're seeing out here?

00:04:19.899 --> 00:04:21.970
And what can we do about it?

00:04:21.970 --> 00:04:23.970
That's the operative question.

00:04:23.970 --> 00:04:27.000
To give us a snapshot
of one disconcerting

00:04:27.000 --> 00:04:30.600
facet of extremism,
let's just take a look

00:04:30.600 --> 00:04:33.090
at a clip from the
Anti-Defamation League.

00:04:33.090 --> 00:04:36.990
This clip illustrates
the explosive growth

00:04:36.990 --> 00:04:41.732
of white supremacist
propaganda on college campuses.

00:04:41.732 --> 00:04:45.204
[MUSIC PLAYING]

00:05:41.611 --> 00:05:43.110
I'm going to turn
to Oren right now.

00:05:43.110 --> 00:05:46.230
Oren, your organization's
been tracking--

00:05:46.230 --> 00:05:48.300
looking at the data for years.

00:05:48.300 --> 00:05:49.950
What exactly is going on?

00:05:49.950 --> 00:05:53.790
I've seen posters, for
example, at local colleges

00:05:53.790 --> 00:05:57.780
in Boston of basically
where folks have essentially

00:05:57.780 --> 00:05:58.800
been recruiting.

00:05:58.800 --> 00:06:01.649
Can you talk about
what the data shows

00:06:01.649 --> 00:06:03.690
and how you've been tracking
these organizations?

00:06:03.690 --> 00:06:04.398
OREN SEGAL: Sure.

00:06:04.398 --> 00:06:07.680
So at ADL, we're tracking
both extremist-related data,

00:06:07.680 --> 00:06:11.740
but also hate incidents and
hate crimes of all types.

00:06:11.740 --> 00:06:14.160
And I think the discussion
today about what

00:06:14.160 --> 00:06:17.640
is driving hate and racism and
what we can do about it really

00:06:17.640 --> 00:06:19.720
needs to start with the data.

00:06:19.720 --> 00:06:22.560
So what I'd like to do is
just provide some of that.

00:06:22.560 --> 00:06:24.390
FBI hate crime
statistics recently

00:06:24.390 --> 00:06:27.690
came out for 2017,
which demonstrated

00:06:27.690 --> 00:06:31.800
a 17% increase in hate
crimes around the country.

00:06:31.800 --> 00:06:34.800
That was over 7,000
incidents that were reported.

00:06:34.800 --> 00:06:37.410
Now one of the key pieces
of data that, I think,

00:06:37.410 --> 00:06:40.800
was not necessarily
underscored but is as critical

00:06:40.800 --> 00:06:45.990
is that in over 90 cities with
people that have over 100,000

00:06:45.990 --> 00:06:49.470
in population either
reported zero hate crimes

00:06:49.470 --> 00:06:51.460
or didn't report
hate crimes at all.

00:06:51.460 --> 00:06:54.930
So even that number,
we know, is much lower.

00:06:54.930 --> 00:06:57.630
At ADL, we also track
anti-Semitic incidents, not

00:06:57.630 --> 00:07:01.400
just crimes but other forms of
harassment and intimidation.

00:07:01.400 --> 00:07:05.760
And in 2017, we
saw a 57% increase

00:07:05.760 --> 00:07:08.610
in those anti-Semitic incidents
from the previous year.

00:07:08.610 --> 00:07:11.490
And that included
an over 90% increase

00:07:11.490 --> 00:07:13.680
in just k-through-12
through 12 schools.

00:07:13.680 --> 00:07:16.830
Right, our kids are watching
the public discussion.

00:07:16.830 --> 00:07:20.130
They are viewing how the
headlines on the news

00:07:20.130 --> 00:07:23.010
and in your newspapers and
on your social media feed

00:07:23.010 --> 00:07:25.710
are constantly filled
with hate and extremism.

00:07:25.710 --> 00:07:29.040
And the last point about this
to the video that we just saw,

00:07:29.040 --> 00:07:33.690
we've actually seen a 500%
increase in white supremacist

00:07:33.690 --> 00:07:35.970
propaganda on the ground.

00:07:35.970 --> 00:07:38.370
Not just college campuses
where we saw a spike

00:07:38.370 --> 00:07:40.270
and we continue to
see that increase.

00:07:40.270 --> 00:07:44.010
But what that means is
this post-Charlottesville

00:07:44.010 --> 00:07:46.440
environment that we're
in, white supremacists

00:07:46.440 --> 00:07:50.370
may not be as comfortable
showing their faces at a rally.

00:07:50.370 --> 00:07:53.700
But what they're doing are
finding other clandestine ways

00:07:53.700 --> 00:07:57.510
to amplify and spread their
messages through propaganda

00:07:57.510 --> 00:08:00.040
spread out not only
in small towns,

00:08:00.040 --> 00:08:02.940
but literally in every
major city that we see.

00:08:02.940 --> 00:08:06.420
And last point, we also track
extremist-related murders

00:08:06.420 --> 00:08:10.410
at ADL, and we've been
doing so since about 1970.

00:08:10.410 --> 00:08:14.580
2018 was the fourth
deadliest year related

00:08:14.580 --> 00:08:16.500
to extremist-related murders.

00:08:16.500 --> 00:08:19.650
We saw over 50 people
killed by extremists,

00:08:19.650 --> 00:08:23.400
and 98% of those were by
right-wing extremists--

00:08:23.400 --> 00:08:25.920
white supremacists,
antigovernment types.

00:08:25.920 --> 00:08:27.660
And when you look at
the past 10 years,

00:08:27.660 --> 00:08:32.039
of all the extremist-related
murders-- about 427--

00:08:32.039 --> 00:08:35.789
73% have been carried
out by white supremacists

00:08:35.789 --> 00:08:39.090
and other homegrown
antigovernment type extremists.

00:08:39.090 --> 00:08:41.640
That may not necessarily connect
with some of the narratives

00:08:41.640 --> 00:08:43.260
we hear in the news every day.

00:08:43.260 --> 00:08:46.410
But these are the statistics
we need to account for

00:08:46.410 --> 00:08:50.845
as we have this discussion about
whether hate is rising or not.

00:08:50.845 --> 00:08:53.220
PHILLIP MARTIN: I'd like to
see how those statistics also

00:08:53.220 --> 00:08:56.340
connect with David
Harris's research.

00:08:56.340 --> 00:08:59.160
David, your work
is evidence-based,

00:08:59.160 --> 00:09:00.660
as you've described it.

00:09:00.660 --> 00:09:03.100
You've studied how
discrimination,

00:09:03.100 --> 00:09:05.970
including racist
language and attitudes

00:09:05.970 --> 00:09:09.057
on prejudice, the effects
of health of those

00:09:09.057 --> 00:09:10.140
who are in the crosshairs.

00:09:10.140 --> 00:09:12.510
You've talked about that impact.

00:09:12.510 --> 00:09:14.460
Tell us more about
what you and what

00:09:14.460 --> 00:09:16.230
your colleagues have found.

00:09:16.230 --> 00:09:18.630
DAVID WILLIAMS: Well, there's
a large body of research--

00:09:18.630 --> 00:09:20.880
and it's a global
body of research--

00:09:20.880 --> 00:09:24.360
that indicates that
exposure to discrimination,

00:09:24.360 --> 00:09:28.350
both kind of virulent
forms but even little

00:09:28.350 --> 00:09:30.600
indignities on a
day to day basis,

00:09:30.600 --> 00:09:34.380
has pervasive adverse
negative effects on health.

00:09:34.380 --> 00:09:36.900
There's studies
showing an increase

00:09:36.900 --> 00:09:39.510
in the risk of
premature death linked

00:09:39.510 --> 00:09:41.370
to exposure to discrimination.

00:09:41.370 --> 00:09:43.880
What the research
is also showing us

00:09:43.880 --> 00:09:46.500
that is very relevant
for our conversation,

00:09:46.500 --> 00:09:48.420
that it's not just
the incidents that are

00:09:48.420 --> 00:09:50.850
targeted at you individually.

00:09:50.850 --> 00:09:53.880
But if you live in a
community with higher levels

00:09:53.880 --> 00:09:55.980
of prejudice, studies
show African Americans who

00:09:55.980 --> 00:09:58.290
live in such communities
across the United States,

00:09:58.290 --> 00:10:00.150
such counties across
the United States

00:10:00.150 --> 00:10:02.050
have higher rates of death.

00:10:02.050 --> 00:10:03.560
It's not just African Americans.

00:10:03.560 --> 00:10:06.300
There's a study looking
at anti-gay prejudice

00:10:06.300 --> 00:10:07.750
in the United States.

00:10:07.750 --> 00:10:11.340
And for LGBT populations
who live in communities

00:10:11.340 --> 00:10:14.490
of higher anti-gay
prejudice, their death rates

00:10:14.490 --> 00:10:16.530
are three times
higher than those

00:10:16.530 --> 00:10:22.620
who live in communities of low
levels of anti-gay prejudice.

00:10:22.620 --> 00:10:24.970
And this is pervasive
throughout society.

00:10:24.970 --> 00:10:26.910
One of the studies
I was involved with

00:10:26.910 --> 00:10:29.940
documented that among high
school and middle school

00:10:29.940 --> 00:10:33.540
students who are exposed
to racial discrimination

00:10:33.540 --> 00:10:36.900
in online contexts,
their levels of anxiety,

00:10:36.900 --> 00:10:39.570
their levels of
depression are higher,

00:10:39.570 --> 00:10:43.310
even after you take into account
other adolescent stresses

00:10:43.310 --> 00:10:45.110
and discrimination offline.

00:10:45.110 --> 00:10:48.110
So documenting the
discrimination online

00:10:48.110 --> 00:10:51.890
is a unique contributor
to their poor health.

00:10:51.890 --> 00:10:54.950
There's research documenting
the negative effects

00:10:54.950 --> 00:11:01.470
of anti-immigrant rhetoric, but
also anti-immigrant policies.

00:11:01.470 --> 00:11:03.350
So for example, a
study I was involved

00:11:03.350 --> 00:11:06.500
with in the state of
Arizona after SB1070

00:11:06.500 --> 00:11:08.510
was passed, a law
that authorized

00:11:08.510 --> 00:11:10.580
local officials to
stop anyone who looked

00:11:10.580 --> 00:11:12.710
as if they might be illegal.

00:11:12.710 --> 00:11:17.000
And we documented among
Mexican American mothers,

00:11:17.000 --> 00:11:20.360
there was a decrease in the
use of preventive health

00:11:20.360 --> 00:11:25.170
services for their children and
the access of social services.

00:11:25.170 --> 00:11:27.170
And most strikingly
in that study,

00:11:27.170 --> 00:11:32.550
that effect was strongest among
US-born Mexican American women.

00:11:32.550 --> 00:11:35.720
So those women who were
citizens of the United States,

00:11:35.720 --> 00:11:38.540
we suspect, were so--

00:11:38.540 --> 00:11:41.900
it was such an assault
to their dignity

00:11:41.900 --> 00:11:44.450
that they could be stopped
simply because of how

00:11:44.450 --> 00:11:47.030
they looked that there was
these negative effects.

00:11:47.030 --> 00:11:49.890
And there's research
documenting that--

00:11:49.890 --> 00:11:56.540
a study out of Los Angeles
that shows 11th graders who,

00:11:56.540 --> 00:11:58.340
in the year before
the election, were

00:11:58.340 --> 00:12:03.260
concerned about the hate and the
discrimination in the society.

00:12:03.260 --> 00:12:06.670
A year later, they have
higher levels of depression,

00:12:06.670 --> 00:12:10.530
higher levels of anxiety, higher
levels of substance abuse.

00:12:10.530 --> 00:12:12.350
So the bottom line
is we are finding

00:12:12.350 --> 00:12:15.470
pervasive negative
effects on the health

00:12:15.470 --> 00:12:18.980
of multiple stigmatized
populations linked

00:12:18.980 --> 00:12:22.100
to the exposure, not only
the personal targeting

00:12:22.100 --> 00:12:26.244
but this broader context of
hostility in our environment.

00:12:26.244 --> 00:12:28.160
PHILLIP MARTIN: One thing
is this conversation

00:12:28.160 --> 00:12:30.290
is such that, no, we
don't have to struggle

00:12:30.290 --> 00:12:34.110
to connect the dots between
your different expertise.

00:12:34.110 --> 00:12:37.940
Dipayan, something David
just talked about--

00:12:37.940 --> 00:12:40.430
the role of online
media, the role

00:12:40.430 --> 00:12:45.715
of social media in promulgating
hate is fairly clear right now.

00:12:45.715 --> 00:12:47.340
But you've done
research on this topic.

00:12:47.340 --> 00:12:49.640
What have been your findings?

00:12:49.640 --> 00:12:53.210
DIPAYAN GHOSH: Well,
I think both Oren's

00:12:53.210 --> 00:12:57.440
and David's comments and
yours really resonate for me.

00:12:57.440 --> 00:13:00.890
When we think about
some of the memes that

00:13:00.890 --> 00:13:04.430
have risen over
the past few years

00:13:04.430 --> 00:13:06.590
or even particular
instances of hate-- so,

00:13:06.590 --> 00:13:10.940
for example, the frog
meme or instances

00:13:10.940 --> 00:13:18.620
of hate against particular
classes of the US population

00:13:18.620 --> 00:13:21.095
really pushed by others groups.

00:13:25.752 --> 00:13:27.790
The thing that we really
have to think about

00:13:27.790 --> 00:13:29.430
is, why is this happening?

00:13:29.430 --> 00:13:32.240
What is the infrastructure
that is enabling

00:13:32.240 --> 00:13:35.510
the spread and the
pervasiveness, as David

00:13:35.510 --> 00:13:40.220
has suggested, of all of
this content being consumed?

00:13:40.220 --> 00:13:44.540
Why is it having such an
impact on the internet?

00:13:44.540 --> 00:13:50.480
And what I'd suggest is that
it is about the infrastructure.

00:13:50.480 --> 00:13:52.990
It is about the
commercial regime

00:13:52.990 --> 00:13:55.550
that sits behind these
internet platforms,

00:13:55.550 --> 00:14:00.470
from YouTube to Facebook
to Twitter to even

00:14:00.470 --> 00:14:02.400
some of the newer platforms.

00:14:02.400 --> 00:14:06.710
And when we think about
that infrastructure,

00:14:06.710 --> 00:14:10.899
it's sometimes difficult to get
our heads around how it works

00:14:10.899 --> 00:14:13.190
and what connects them all
and how they're all similar.

00:14:13.190 --> 00:14:16.595
But deep down, I think
that infrastructure,

00:14:16.595 --> 00:14:18.470
that commercial
regime that defines

00:14:18.470 --> 00:14:23.250
these platforms is actually
fairly simple at a high level.

00:14:23.250 --> 00:14:26.930
First of all, these
platforms, they

00:14:26.930 --> 00:14:29.900
develop very
compelling services--

00:14:29.900 --> 00:14:33.500
like Messenger or like the News
Feed or like the Twitter feed

00:14:33.500 --> 00:14:36.560
or like the YouTube system--

00:14:36.560 --> 00:14:39.770
to such an extent that
they're, as some psychologists

00:14:39.770 --> 00:14:41.930
have suggested, addictive.

00:14:41.930 --> 00:14:44.360
And this has prevented
other services

00:14:44.360 --> 00:14:47.450
from actually challenging
these services to the extent

00:14:47.450 --> 00:14:50.880
that they're limiting
competition on the internet.

00:14:50.880 --> 00:14:53.210
Second, through
these services that

00:14:53.210 --> 00:14:58.160
are dominating the
internet, these companies

00:14:58.160 --> 00:14:59.690
are drawing up--

00:14:59.690 --> 00:15:02.390
hoovering up large
amounts of data

00:15:02.390 --> 00:15:04.850
of our personal information
through our engagement

00:15:04.850 --> 00:15:09.560
on the News Feed, as
well as through purchases

00:15:09.560 --> 00:15:13.150
of data from third parties,
as well as from third party

00:15:13.150 --> 00:15:17.510
websites to develop
behavioral profiles on us.

00:15:17.510 --> 00:15:19.940
And those first
two pillars move us

00:15:19.940 --> 00:15:23.930
to the third, which is that
these companies develop

00:15:23.930 --> 00:15:30.380
very precise and sophisticated
algorithms that do two things--

00:15:30.380 --> 00:15:34.910
curate content in our social
feeds and target ads at us

00:15:34.910 --> 00:15:38.210
based on our behavioral profile
and based on our extensive use

00:15:38.210 --> 00:15:39.870
of these platforms.

00:15:39.870 --> 00:15:43.780
And so what I'd suggest is that
this infrastructure has grown

00:15:43.780 --> 00:15:47.680
up in this way for
20 years, as we've

00:15:47.680 --> 00:15:52.900
seen Facebook and Google come to
the fore in the global economy,

00:15:52.900 --> 00:15:57.640
and really taken over the
internet commercially,

00:15:57.640 --> 00:16:04.600
to the extent that these impacts
against hate are not really--

00:16:04.600 --> 00:16:06.610
these companies are
not really challenged

00:16:06.610 --> 00:16:10.480
from a business perspective
to do anything about it

00:16:10.480 --> 00:16:12.730
until and unless the
public sentiment rises up

00:16:12.730 --> 00:16:17.510
so much that they have to
actually start to address it.

00:16:17.510 --> 00:16:20.560
What I'd suggest is that we
draw on the tremendous public

00:16:20.560 --> 00:16:25.060
sentiment against hate, against
the spread of disinformation,

00:16:25.060 --> 00:16:28.270
against the spread of
algorithmic discrimination

00:16:28.270 --> 00:16:32.830
and take this opportunity
in the next couple of years

00:16:32.830 --> 00:16:37.480
to push a regime, a regulatory
regime that addresses

00:16:37.480 --> 00:16:40.600
the harms of that business
model so that we have better

00:16:40.600 --> 00:16:43.690
competition on the internet, so
that we have better privacy, so

00:16:43.690 --> 00:16:46.810
that we can have
better transparency

00:16:46.810 --> 00:16:48.850
and to the ways that
these algorithms work.

00:16:48.850 --> 00:16:50.770
And what I'd
suggest is that that

00:16:50.770 --> 00:16:54.070
can start to address
in the long run

00:16:54.070 --> 00:16:59.170
this spread of hate issue, which
is really, really challenging

00:16:59.170 --> 00:17:01.600
us in really difficult ways.

00:17:01.600 --> 00:17:02.470
PHILLIP MARTIN: OK.

00:17:02.470 --> 00:17:09.130
The online content
that you, Oren--

00:17:09.130 --> 00:17:11.950
David Williams
have talked about--

00:17:11.950 --> 00:17:14.544
this stuff, of course, is
being consumed by everyone.

00:17:14.544 --> 00:17:15.460
It's across the board.

00:17:18.069 --> 00:17:20.680
But it's mainly being
consumed by young people

00:17:20.680 --> 00:17:23.319
when you talk about
online platforms.

00:17:23.319 --> 00:17:25.270
And we talk about
Teaching Tolerance,

00:17:25.270 --> 00:17:28.020
both the name of
the organization

00:17:28.020 --> 00:17:32.300
and an objective of
the organization.

00:17:32.300 --> 00:17:34.660
What are you finding
in terms, Maureen,

00:17:34.660 --> 00:17:38.229
in terms of young
people, the reception?

00:17:38.229 --> 00:17:39.895
And what are you
finding in the schools?

00:17:39.895 --> 00:17:42.490
How are the schools being
shaped by all of this?

00:17:42.490 --> 00:17:43.990
MAUREEN COSTELLO:
What we're finding

00:17:43.990 --> 00:17:48.520
is that schools are not
immune to the climate that

00:17:48.520 --> 00:17:54.190
is pervasive in the United
States Teaching Tolerance

00:17:54.190 --> 00:17:57.310
has always operated to
reduce prejudice in schools

00:17:57.310 --> 00:18:01.740
and to improve intergroup
relations among students.

00:18:01.740 --> 00:18:04.420
And we've always heard
about hate incidents.

00:18:04.420 --> 00:18:07.210
Starting in 2016,
we became aware

00:18:07.210 --> 00:18:09.010
that there were more
incidents happening

00:18:09.010 --> 00:18:12.160
as a result of the rhetoric
of the presidential campaign,

00:18:12.160 --> 00:18:13.390
and we surveyed teachers.

00:18:13.390 --> 00:18:16.990
And as a result, we came
out with two reports in 2016

00:18:16.990 --> 00:18:22.930
that showed three
alarming phenomena.

00:18:22.930 --> 00:18:25.630
The first, of course,
was that teachers

00:18:25.630 --> 00:18:29.050
were reporting that marginalized
students, whether they

00:18:29.050 --> 00:18:34.090
be immigrants, LGBT students,
students of color generally,

00:18:34.090 --> 00:18:38.500
religious minorities, were
feeling high levels of anxiety.

00:18:38.500 --> 00:18:42.040
And that has been supported
by subsequent studies,

00:18:42.040 --> 00:18:45.730
one out of UCLA last
year, that have just

00:18:45.730 --> 00:18:47.920
said this has continued.

00:18:47.920 --> 00:18:51.890
The second finding
was that bullying,

00:18:51.890 --> 00:18:55.960
which has been a long
standing concern of educators,

00:18:55.960 --> 00:18:58.720
had taken on a kind
of political tint,

00:18:58.720 --> 00:19:03.940
and that we might consider
that politics had weaponized

00:19:03.940 --> 00:19:08.200
bullying in a way, and that
the kind of rhetoric that

00:19:08.200 --> 00:19:10.240
was being mentioned
in political campaigns

00:19:10.240 --> 00:19:13.900
was now being used against
vulnerable students.

00:19:13.900 --> 00:19:15.910
And the third thing we
found was that teachers

00:19:15.910 --> 00:19:19.750
were really, really uncertain
about how to handle this--

00:19:19.750 --> 00:19:23.950
not only about how to support
marginalized children,

00:19:23.950 --> 00:19:28.630
how to contain the hate that
they were seeing emerging,

00:19:28.630 --> 00:19:32.380
and, finally, even how to talk
about politics and the election

00:19:32.380 --> 00:19:36.700
in a way that would
itself not seem partisan.

00:19:36.700 --> 00:19:38.866
We've been tracking hate
incidents at schools.

00:19:38.866 --> 00:19:40.240
And by hate
incidents at schools,

00:19:40.240 --> 00:19:42.790
we're not talking about
hate crimes necessarily.

00:19:42.790 --> 00:19:47.590
We're talking about harassment,
disparaging remarks,

00:19:47.590 --> 00:19:52.000
negative behaviors that
target a group of people

00:19:52.000 --> 00:19:54.549
based on their identity.

00:19:54.549 --> 00:19:56.590
It's very hard to say
whether they have increased

00:19:56.590 --> 00:19:59.620
because no one really was
tracking this very closely

00:19:59.620 --> 00:20:03.740
prior to the last
couple of years.

00:20:03.740 --> 00:20:08.710
But what we've seen is a
regular number of incidents.

00:20:08.710 --> 00:20:14.560
And as the FBI hate crime data
showed, 25% of the hate crimes

00:20:14.560 --> 00:20:18.160
happened in schools
from K through college.

00:20:18.160 --> 00:20:20.740
We're about to release a report
next month in which we've

00:20:20.740 --> 00:20:23.200
looked at both news
reports of hate incidents

00:20:23.200 --> 00:20:26.530
and also data gathered
from educators.

00:20:26.530 --> 00:20:30.970
And what we can tell you is that
anti-Semitism is on the rise.

00:20:30.970 --> 00:20:33.130
Racial harassment
is on the rise.

00:20:33.130 --> 00:20:38.590
Anti-immigrant, anti-LGBT,
anti-Muslim harassment are all

00:20:38.590 --> 00:20:41.950
happening at schools with a
terribly detrimental effect

00:20:41.950 --> 00:20:44.920
on students, most of whom--

00:20:44.920 --> 00:20:47.320
over 51%-- are
children of color,

00:20:47.320 --> 00:20:49.510
and they come from these
marginalized groups.

00:20:49.510 --> 00:20:53.260
So obviously, this is not only
a public health crisis of sorts,

00:20:53.260 --> 00:20:56.112
but also a problem
about making schools

00:20:56.112 --> 00:20:58.070
effective in doing what
they're supposed to do,

00:20:58.070 --> 00:20:59.680
which is educate.

00:20:59.680 --> 00:21:03.530
You cannot educate when
children don't feel safe.

00:21:03.530 --> 00:21:08.770
I would just leave by
saying that what we've also

00:21:08.770 --> 00:21:11.200
discovered-- and this
will be in our report--

00:21:11.200 --> 00:21:15.130
is that the vast majority of
incidents at schools never get

00:21:15.130 --> 00:21:19.120
reported in the news media,
that probably fewer than 5%

00:21:19.120 --> 00:21:21.070
are reported.

00:21:21.070 --> 00:21:22.990
And that for many,
many children,

00:21:22.990 --> 00:21:26.710
they're being exposed
to hateful language

00:21:26.710 --> 00:21:31.660
and to disparaging remarks
in a hostile environment

00:21:31.660 --> 00:21:35.080
in the very place where
they should feel safest.

00:21:35.080 --> 00:21:38.050
PHILLIP MARTIN: In the last two
years, as probably especially,

00:21:38.050 --> 00:21:43.270
you've seen a lot of kids
being emboldened, of chants,

00:21:43.270 --> 00:21:47.467
of all types of things that
perhaps you hadn't seen before.

00:21:47.467 --> 00:21:49.300
I don't think we could
divorce a lot of this

00:21:49.300 --> 00:21:51.140
from the political atmosphere.

00:21:51.140 --> 00:21:55.450
And to that end, I'd
like to turn to Jim,

00:21:55.450 --> 00:21:57.070
who we're talking to remotely.

00:21:57.070 --> 00:21:58.960
He's out in Wisconsin.

00:21:58.960 --> 00:22:02.410
Jim, as former attorney
general, as a former governor,

00:22:02.410 --> 00:22:05.500
what role are you
seeing in terms

00:22:05.500 --> 00:22:10.930
of the law, but, more
importantly, politics,

00:22:10.930 --> 00:22:14.050
in promulgating and
then propagating hate?

00:22:16.854 --> 00:22:18.270
JIM DOYLE: Well,
first on the law,

00:22:18.270 --> 00:22:22.640
let me say I argued before the
United States Supreme Court

00:22:22.640 --> 00:22:25.760
in the 1990s the
first case that went

00:22:25.760 --> 00:22:27.650
to them on hate
crimes in which they

00:22:27.650 --> 00:22:30.410
upheld the right of a state--

00:22:30.410 --> 00:22:34.640
the ability of a state to impose
harsher sanctions on people

00:22:34.640 --> 00:22:38.430
that commit crimes
motivated by racial,

00:22:38.430 --> 00:22:40.160
gender, other kinds of biases.

00:22:40.160 --> 00:22:42.600
And it was a big
win at the time.

00:22:42.600 --> 00:22:46.055
But I think we all have to
recognize that the law--

00:22:46.055 --> 00:22:48.950
and it's critical, I should
say, that those laws be enforced

00:22:48.950 --> 00:22:50.570
and people understand
that the law

00:22:50.570 --> 00:22:52.410
is on one side of this issue.

00:22:52.410 --> 00:22:56.030
It's not on the other side--
that the law is on the side

00:22:56.030 --> 00:22:58.707
that people ought to be able
to live freely in this country,

00:22:58.707 --> 00:23:00.290
and they ought to
live freely in a way

00:23:00.290 --> 00:23:05.850
that they are not harassed,
harmed because of their race,

00:23:05.850 --> 00:23:08.990
their religion, their sexual
orientation, whatever.

00:23:11.560 --> 00:23:14.660
But we ought to recognize
that the law is--

00:23:14.660 --> 00:23:16.670
for the issues that
have been talked about,

00:23:16.670 --> 00:23:20.990
the demeaning statements
in schools, the low-level,

00:23:20.990 --> 00:23:25.040
street-level interaction between
a police officer and somebody

00:23:25.040 --> 00:23:27.200
on the street that
can be so demeaning,

00:23:27.200 --> 00:23:32.360
that have the kind of harms
that Professor Williams has

00:23:32.360 --> 00:23:33.650
documented--

00:23:33.650 --> 00:23:36.410
that the law is a
pretty ham-fisted way

00:23:36.410 --> 00:23:37.460
to deal with those.

00:23:37.460 --> 00:23:39.680
It doesn't really
get to those kinds

00:23:39.680 --> 00:23:41.520
of very significant issues.

00:23:41.520 --> 00:23:43.520
That's where politics
becomes so important

00:23:43.520 --> 00:23:46.130
and where leadership
becomes so important.

00:23:46.130 --> 00:23:48.260
Much of this-- and I'd be
interested to see while

00:23:48.260 --> 00:23:49.850
it's accelerated--

00:23:49.850 --> 00:23:54.080
if I recall the statistics
from the Anti-Defamation

00:23:54.080 --> 00:23:56.570
and Southern Poverty Law
Institute and others--

00:23:56.570 --> 00:23:58.250
it really started
with the election

00:23:58.250 --> 00:24:02.480
of Barack Obama and
this virulent reaction

00:24:02.480 --> 00:24:04.700
by segments of our
society, the idea

00:24:04.700 --> 00:24:07.130
that an African-American
person could

00:24:07.130 --> 00:24:08.780
be president of
the United States,

00:24:08.780 --> 00:24:13.260
birthers, and all of that racist
stuff that came out of it.

00:24:13.260 --> 00:24:15.720
And now it's obviously
been accelerated,

00:24:15.720 --> 00:24:19.580
why political leadership
that talks in stereotypes,

00:24:19.580 --> 00:24:20.990
and that's what's so harmful.

00:24:20.990 --> 00:24:25.610
These horrible stereotypes that
that brown people coming across

00:24:25.610 --> 00:24:30.650
the borders are criminals,
that African-American people--

00:24:30.650 --> 00:24:33.120
the stereotypes they've
dealt with in my world

00:24:33.120 --> 00:24:34.250
of law enforcement.

00:24:34.250 --> 00:24:36.280
It's been so harmful
as there's somehow

00:24:36.280 --> 00:24:38.210
some kind of
inherent criminality

00:24:38.210 --> 00:24:40.710
that we all have to
be very scared about

00:24:40.710 --> 00:24:42.530
that is existing there.

00:24:42.530 --> 00:24:44.930
Those kinds of large
stereotypes that

00:24:44.930 --> 00:24:47.710
come from political
leadership from our president.

00:24:47.710 --> 00:24:48.470
Let's just say it.

00:24:48.470 --> 00:24:50.450
I'm mean, when you
have a president that

00:24:50.450 --> 00:24:53.600
talks in terms of people and
these kinds of groups instead

00:24:53.600 --> 00:24:56.270
of Americans as
American, citizens

00:24:56.270 --> 00:25:02.450
as immigrants as individual
people with hopes and dreams,

00:25:02.450 --> 00:25:05.300
most good, some bad.

00:25:05.300 --> 00:25:08.495
When we have some who lump--
who sees the world in these--

00:25:08.495 --> 00:25:10.740
and talks about it in that way.

00:25:10.740 --> 00:25:14.270
And then you have the
obvious, when politicians

00:25:14.270 --> 00:25:18.140
start drawing equivalencies
that what happened in Virginia

00:25:18.140 --> 00:25:20.930
was equal on both sides.

00:25:20.930 --> 00:25:25.850
That has a horrible effect
on our political atmosphere,

00:25:25.850 --> 00:25:28.610
but what concerns me is what
Professor Williams was talking

00:25:28.610 --> 00:25:31.070
about is the effect
on people that

00:25:31.070 --> 00:25:34.670
hear that coming at them,
an African-American student

00:25:34.670 --> 00:25:36.790
working hard in school,
trying to get ahead,

00:25:36.790 --> 00:25:40.850
who hears leadership
talking about the people

00:25:40.850 --> 00:25:43.130
of African-American
backgrounds with that kind

00:25:43.130 --> 00:25:46.230
of dismissive attitude.

00:25:46.230 --> 00:25:48.260
It has terrible,
harmful effects,

00:25:48.260 --> 00:25:52.160
and it does in the law, as well,
because these attitudes do then

00:25:52.160 --> 00:25:55.400
pervade the policing
on our streets and lead

00:25:55.400 --> 00:25:58.010
to some of the terrible
incidents we've seen.

00:25:58.010 --> 00:26:00.260
But even more the lower level--

00:26:00.260 --> 00:26:02.270
I shouldn't say more
but equally important

00:26:02.270 --> 00:26:06.350
that lower level, that kind
of routine stop on the street

00:26:06.350 --> 00:26:09.740
in which a young black person
is treated in a particularly

00:26:09.740 --> 00:26:12.680
aggressive way that a young
white person might not

00:26:12.680 --> 00:26:16.664
has very serious effects on
how people see the government,

00:26:16.664 --> 00:26:18.080
how they relate
to the government,

00:26:18.080 --> 00:26:19.756
how they see the
political process,

00:26:19.756 --> 00:26:21.380
whether they see it
as something that's

00:26:21.380 --> 00:26:23.240
out there to try to help them.

00:26:23.240 --> 00:26:27.860
So, obviously, my one question
I often ask is when I hear--

00:26:27.860 --> 00:26:30.440
I've heard the president
and others say,

00:26:30.440 --> 00:26:33.380
I'm tired of all this
political correctness.

00:26:33.380 --> 00:26:35.760
why can't we just say
what we want to say?

00:26:35.760 --> 00:26:38.360
And my question is, what is
it you really want to say?

00:26:38.360 --> 00:26:41.030
why don't you just tell us
what you really want to say?

00:26:41.030 --> 00:26:43.040
Let's get this out there.

00:26:43.040 --> 00:26:45.760
But that's really what we're
dealing with, with a lot of--

00:26:45.760 --> 00:26:47.510
and, politically, I
think, with the issues

00:26:47.510 --> 00:26:49.190
that we're talking here today.

00:26:49.190 --> 00:26:51.440
PHILLIP MARTIN: Well, one
of the things I'd like to do

00:26:51.440 --> 00:26:52.610
is even explore this--

00:26:52.610 --> 00:26:54.760
we might not have the time to
do it here-- but even explore

00:26:54.760 --> 00:26:56.060
the term political correctness.

00:26:56.060 --> 00:26:58.670
What does that mean
usually and often?

00:26:58.670 --> 00:27:01.380
And the other point
that you just made--

00:27:01.380 --> 00:27:05.120
and I want to address this to
our panel in a few minutes--

00:27:05.120 --> 00:27:09.540
is that about the
role of the president.

00:27:09.540 --> 00:27:12.800
It's the elephant in
the room, but it's one

00:27:12.800 --> 00:27:15.520
that I think we have to explore.

00:27:15.520 --> 00:27:19.510
Before we do that, I want to
turn to another clip, this one

00:27:19.510 --> 00:27:22.600
by Teaching Tolerance.

00:27:22.600 --> 00:27:26.260
This is a clip of a part
of a film they put together

00:27:26.260 --> 00:27:31.430
called Mix It Up at Lunch.

00:27:31.430 --> 00:27:34.580
GIRL: Mix it up day
was a very fun day.

00:27:34.580 --> 00:27:38.840
I got to see more people
mingling and getting along

00:27:38.840 --> 00:27:42.590
with people that they usually
don't necessarily talk to

00:27:42.590 --> 00:27:44.330
or even look at.

00:27:44.330 --> 00:27:46.347
And it was a very
fun thing to see

00:27:46.347 --> 00:27:48.180
that people were being
proactive and getting

00:27:48.180 --> 00:27:50.030
in there with other kids.

00:27:50.030 --> 00:27:54.360
WOMAN: I loved the whole day
even though it seemed chaotic.

00:27:54.360 --> 00:27:56.510
I loved everything
they did because it's

00:27:56.510 --> 00:28:01.350
our kids speaking to our
kids, which is what we need.

00:28:01.350 --> 00:28:03.470
We need interaction
between our students

00:28:03.470 --> 00:28:05.680
so that they understand.

00:28:05.680 --> 00:28:07.580
And, hopefully, even
though they took it

00:28:07.580 --> 00:28:12.080
as acting or just
singing, they'll

00:28:12.080 --> 00:28:13.730
go home tonight,
or this afternoon,

00:28:13.730 --> 00:28:16.490
or even think about it tomorrow,
you know, what they said

00:28:16.490 --> 00:28:19.840
really will make a
difference if I just stand up

00:28:19.840 --> 00:28:22.190
or if I learn that I'm
different from somebody else,

00:28:22.190 --> 00:28:23.870
and it's OK.

00:28:23.870 --> 00:28:25.670
And that's my
message that I hope

00:28:25.670 --> 00:28:28.400
they perceive from
today, whether it

00:28:28.400 --> 00:28:30.200
be today or tomorrow.

00:28:30.200 --> 00:28:34.880
But I think the whole thing
was a shining moment for me,

00:28:34.880 --> 00:28:38.630
just to have them involved
and work with each other.

00:28:38.630 --> 00:28:40.760
GIRL: My advice to
give to other students

00:28:40.760 --> 00:28:44.330
is to keep their eyes
open, don't shut off

00:28:44.330 --> 00:28:49.876
and jump to conclusions, to just
participate because I promise

00:28:49.876 --> 00:28:51.642
it's very rewarding in the end.

00:28:51.642 --> 00:28:54.052
[MUSIC PLAYING]

00:28:57.919 --> 00:28:59.627
PHILLIP MARTIN: Well,
that's a good idea.

00:28:59.627 --> 00:29:00.402
[LAUGHTER]

00:29:00.402 --> 00:29:02.110
PHILLIP MARTIN: Now
we're going to mix it

00:29:02.110 --> 00:29:07.910
up and right before my
second favorite meal lunch.

00:29:07.910 --> 00:29:12.080
And, Maureen, let's
start off with you.

00:29:12.080 --> 00:29:16.510
And this, obviously,
is part of an effort

00:29:16.510 --> 00:29:21.550
to counter the hate and the type
of intolerance we're seeing out

00:29:21.550 --> 00:29:23.030
there.

00:29:23.030 --> 00:29:27.670
Can you talk about other
aspects of Teaching Tolerance's

00:29:27.670 --> 00:29:30.310
programs and how you're
reaching or attempting

00:29:30.310 --> 00:29:34.210
to reach young people much
like the local program Facing

00:29:34.210 --> 00:29:36.660
History and Ourselves?

00:29:36.660 --> 00:29:38.410
MAUREEN COSTELLO: We
work through teachers

00:29:38.410 --> 00:29:42.910
across the United States
and millions of them

00:29:42.910 --> 00:29:45.220
turn to Teaching
Tolerance for advice,

00:29:45.220 --> 00:29:47.380
and guidance, and curriculum.

00:29:47.380 --> 00:29:51.100
And while we do sponsor
Mix It Up at Lunch Day,

00:29:51.100 --> 00:29:54.190
we're very cognizant of the
fact that this can't just

00:29:54.190 --> 00:29:58.090
be a moment in the school year,
that, in fact, schools are

00:29:58.090 --> 00:30:00.510
incredibly important places.

00:30:00.510 --> 00:30:03.580
They're crucibles for
building the society

00:30:03.580 --> 00:30:08.320
that we're all going to live
in in 10 years or 20 years,

00:30:08.320 --> 00:30:12.100
and they are also one of
the last common institutions

00:30:12.100 --> 00:30:13.120
standing.

00:30:13.120 --> 00:30:14.770
And so I think that
it's a time that

00:30:14.770 --> 00:30:18.430
calls for more investment in
making sure that schools are

00:30:18.430 --> 00:30:22.450
doing their jobs to counter hate
and to build that good society.

00:30:22.450 --> 00:30:25.150
What does that mean?

00:30:25.150 --> 00:30:26.890
All of our work is
guided by something

00:30:26.890 --> 00:30:28.990
we call the social
justice standards,

00:30:28.990 --> 00:30:32.470
and they're based on four
pillars, identity, diversity,

00:30:32.470 --> 00:30:34.540
justice, and action.

00:30:34.540 --> 00:30:36.220
The idea of identity
in our vision

00:30:36.220 --> 00:30:39.520
is that schools should have this
at the center of their vision

00:30:39.520 --> 00:30:42.160
so that, as they look
at students who come in,

00:30:42.160 --> 00:30:46.630
they want every student to find
in school a place where their

00:30:46.630 --> 00:30:49.090
own identities-- whatever
those identities are,

00:30:49.090 --> 00:30:52.970
be they religious, racial,
sexual orientation,

00:30:52.970 --> 00:30:58.030
whatever-- can be affirmed and
basically that they can have

00:30:58.030 --> 00:31:00.440
positive identity development.

00:31:00.440 --> 00:31:04.540
Secondly, that they
develop a curiosity--

00:31:04.540 --> 00:31:06.790
and obviously have
exposure-- to people

00:31:06.790 --> 00:31:08.860
with different
identities, but that it's

00:31:08.860 --> 00:31:13.210
a healthy open-minded
curiosity of, I know who I am.

00:31:13.210 --> 00:31:15.250
Tell me who you are.

00:31:15.250 --> 00:31:19.210
The third is we want children
to have a commitment to justice,

00:31:19.210 --> 00:31:21.272
and that shouldn't be
very controversial.

00:31:21.272 --> 00:31:23.230
I mean, it is part of
the Pledge of Allegiance,

00:31:23.230 --> 00:31:25.180
that we want justice for all.

00:31:25.180 --> 00:31:27.130
And we want children
to be able to think

00:31:27.130 --> 00:31:31.570
critically and recognize
injustice when they see it.

00:31:31.570 --> 00:31:33.970
And, finally, we feel that
the end of all education

00:31:33.970 --> 00:31:37.570
has to be a capacity
to take action,

00:31:37.570 --> 00:31:41.440
to work with others
to address injustices

00:31:41.440 --> 00:31:43.910
and to do the work that
we're all called upon to do,

00:31:43.910 --> 00:31:47.300
which is to make this world a
better place than we found it.

00:31:47.300 --> 00:31:52.940
And so for us, it's about
encouraging peer relationships.

00:31:52.940 --> 00:31:55.090
It's about encouraging
schools to have

00:31:55.090 --> 00:31:58.790
daily interactions between--

00:31:58.790 --> 00:32:03.280
among students and adults
that bring them together.

00:32:03.280 --> 00:32:07.180
It's curriculum based,
explicit curriculum

00:32:07.180 --> 00:32:09.880
about prejudice and
about stereotypes

00:32:09.880 --> 00:32:14.590
but also that builds community,
implicit curriculum that

00:32:14.590 --> 00:32:20.320
exposes students to the
lived experiences of others.

00:32:20.320 --> 00:32:22.750
This is incredibly
important, and there's

00:32:22.750 --> 00:32:27.070
lots of good research
that has shown that when

00:32:27.070 --> 00:32:30.370
students learn about the
history of discrimination, they,

00:32:30.370 --> 00:32:31.510
in fact--

00:32:31.510 --> 00:32:34.300
their attitudes, their
discriminatory attitudes

00:32:34.300 --> 00:32:36.110
decline and decrease.

00:32:36.110 --> 00:32:41.530
So we should be honest in
our curriculum about the warp

00:32:41.530 --> 00:32:44.532
and the flaws in our history.

00:32:44.532 --> 00:32:46.990
And, finally, I think the most
important thing-- and school

00:32:46.990 --> 00:32:49.480
leadership is incredibly
important here-- not

00:32:49.480 --> 00:32:54.040
every school in this country
is a cauldron of hatred.

00:32:54.040 --> 00:32:56.530
We hear from a lot of
teachers who talk about,

00:32:56.530 --> 00:32:58.630
how this doesn't
happen in my school,

00:32:58.630 --> 00:33:00.850
and they always point
to school leadership

00:33:00.850 --> 00:33:06.160
who really walk the walk as
well as talking the talk.

00:33:06.160 --> 00:33:09.680
Kids learn from adults, and
they learn from each other.

00:33:09.680 --> 00:33:12.040
And so what matters
is not only what

00:33:12.040 --> 00:33:14.110
we say but also what we do.

00:33:14.110 --> 00:33:16.870
And that means that we have
to greet every person who

00:33:16.870 --> 00:33:18.610
comes into that
school and treat them

00:33:18.610 --> 00:33:23.332
as a deserving human being
who deserves our respect.

00:33:23.332 --> 00:33:24.790
PHILLIP MARTIN:
It's funny but when

00:33:24.790 --> 00:33:26.665
I'm thinking about the
whole notion of trying

00:33:26.665 --> 00:33:29.027
to engender empathy--

00:33:29.027 --> 00:33:30.610
which is what your
program does, which

00:33:30.610 --> 00:33:33.040
is what the world of
difference program does--

00:33:33.040 --> 00:33:35.740
you're also dealing, however,
with a larger message that's

00:33:35.740 --> 00:33:38.020
coming across, for
example, and oftentimes,

00:33:38.020 --> 00:33:43.330
on Twitter of a huge megaphone
that certain individuals have

00:33:43.330 --> 00:33:46.160
in order to promulgate
a particular message.

00:33:46.160 --> 00:33:48.040
And one question I
would have for Oren,

00:33:48.040 --> 00:33:54.880
in the context of that, is how
do you basically get to a point

00:33:54.880 --> 00:33:58.000
where you think you're reaching
where you actually are making

00:33:58.000 --> 00:34:01.720
a world of difference,
when you have that bigger

00:34:01.720 --> 00:34:04.990
megaphone out there that might
be drowning out your message.

00:34:04.990 --> 00:34:08.110
OREN SEGAL: Yeah, I mean this
is a battle for hearts and minds

00:34:08.110 --> 00:34:09.370
at the end of the day.

00:34:09.370 --> 00:34:11.170
So in the Center
on Extremism, we

00:34:11.170 --> 00:34:13.330
believe sunlight is
the best disinfectant.

00:34:13.330 --> 00:34:14.980
You need to expose
the extremists.

00:34:14.980 --> 00:34:17.590
You need to expose the hate
so that people can frankly

00:34:17.590 --> 00:34:19.730
understand what
they're up against.

00:34:19.730 --> 00:34:21.400
But our work would
not be enough without

00:34:21.400 --> 00:34:23.360
our educational resources.

00:34:23.360 --> 00:34:25.210
So a World of
Difference, for example,

00:34:25.210 --> 00:34:29.170
it trains not only students
but teachers and their parents,

00:34:29.170 --> 00:34:31.270
not only how to identify bias--

00:34:31.270 --> 00:34:33.909
we're good at-- we're
pretty good at identifying

00:34:33.909 --> 00:34:36.100
what is racism,
anti-Semitism, what

00:34:36.100 --> 00:34:38.462
is Islamophobia, et cetera.

00:34:38.462 --> 00:34:39.920
But you know what's
more important?

00:34:39.920 --> 00:34:41.860
And this is a lesson
from the Holocaust

00:34:41.860 --> 00:34:43.690
is to have people
to say, I'm not just

00:34:43.690 --> 00:34:45.610
going to sit around and
do nothing about it.

00:34:45.610 --> 00:34:47.500
You're training
kids, and, often,

00:34:47.500 --> 00:34:50.290
this is peer-to-peer model,
as well, that we have at ADL.

00:34:50.290 --> 00:34:52.600
And that gives it a
little bit more legitimacy

00:34:52.600 --> 00:34:53.810
for younger people.

00:34:53.810 --> 00:34:55.750
But to say, you
have a stake here.

00:34:55.750 --> 00:34:56.939
You have a role.

00:34:56.939 --> 00:34:59.200
We will teach you
how to identify bias

00:34:59.200 --> 00:35:02.950
within institutions, within
others, and within yourself.

00:35:02.950 --> 00:35:04.810
And then we are going
to arm you and help

00:35:04.810 --> 00:35:09.292
you to speak out and challenge
that bias and that racism.

00:35:09.292 --> 00:35:10.750
That makes my job
as somebody who's

00:35:10.750 --> 00:35:12.700
tracking extremism
easier because I

00:35:12.700 --> 00:35:14.740
know there's an
army behind me who

00:35:14.740 --> 00:35:19.150
are going to be able to call it
out and do something about it.

00:35:19.150 --> 00:35:20.950
Deborah Lipstadt
said anti-Semitism

00:35:20.950 --> 00:35:23.410
starts with the Jews, but it
doesn't end with the Jews.

00:35:23.410 --> 00:35:25.930
The same is true for
all forms of hate,

00:35:25.930 --> 00:35:27.720
and look no further
than Pittsburgh,

00:35:27.720 --> 00:35:31.990
where here's an individual
who attacked 11 people largest

00:35:31.990 --> 00:35:35.020
massacre against Jews in
this country's history.

00:35:35.020 --> 00:35:38.140
And it was motivated by an
anti-immigrant sentiment,

00:35:38.140 --> 00:35:41.980
talking about the caravan,
talking about the social media

00:35:41.980 --> 00:35:45.940
hate of that day,
and he targeted Jews

00:35:45.940 --> 00:35:49.570
because in his conspiratorial
white supremacist worldview,

00:35:49.570 --> 00:35:52.160
the Jews are controlling
our immigration policy.

00:35:52.160 --> 00:35:55.060
So by teaching kids of
all different backgrounds,

00:35:55.060 --> 00:35:57.130
religious, races,
et cetera, that

00:35:57.130 --> 00:35:59.410
helps to the fight
against anti-Semitism.

00:35:59.410 --> 00:36:01.060
And the fight
against anti-Semitism

00:36:01.060 --> 00:36:03.460
helps fight all forms
of racism and bigotry.

00:36:03.460 --> 00:36:06.830
That's what we're trying to
teach our kids in our schools.

00:36:06.830 --> 00:36:10.710
PHILLIP MARTIN: Oren, and just
as a good segue way to David.

00:36:10.710 --> 00:36:13.270
You know how to talk
about this stuff.

00:36:13.270 --> 00:36:14.890
You know how to talk
about this stuff.

00:36:14.890 --> 00:36:17.680
All of you, you're
conversationalists.

00:36:17.680 --> 00:36:23.380
David, a lot of your research is
based on the conversations that

00:36:23.380 --> 00:36:27.370
take place between individuals
and the amelioration

00:36:27.370 --> 00:36:31.990
of discrimination as a
result of those discussions.

00:36:31.990 --> 00:36:35.920
Talk about that work
and what conclusions

00:36:35.920 --> 00:36:38.890
are you coming to in this
age of heightened anxiety

00:36:38.890 --> 00:36:40.664
and hate, if you will?

00:36:40.664 --> 00:36:42.580
DAVID WILLIAMS: I think
we need, as a society,

00:36:42.580 --> 00:36:45.910
to find safe places
where people can talk.

00:36:45.910 --> 00:36:48.070
When if someone, because
of their background

00:36:48.070 --> 00:36:49.570
and understanding,
said something

00:36:49.570 --> 00:36:53.680
that was inappropriate, they're
not castigated and excluded.

00:36:53.680 --> 00:36:57.310
So I think creating those
safe places generically

00:36:57.310 --> 00:37:01.480
is one thing that all of us
need to be because there's

00:37:01.480 --> 00:37:03.440
a sense in which--

00:37:03.440 --> 00:37:06.430
Kellogg Foundation had a
program called the Truth, Racial

00:37:06.430 --> 00:37:08.680
Healing &amp; Transformation.

00:37:08.680 --> 00:37:12.460
People need-- don't assume that
everyone has the same level

00:37:12.460 --> 00:37:14.060
of knowledge that you have.

00:37:14.060 --> 00:37:15.940
And there's a term in
their socialization

00:37:15.940 --> 00:37:18.970
that was used that didn't
think it was a problem,

00:37:18.970 --> 00:37:21.560
and they're now
learning it's a problem.

00:37:21.560 --> 00:37:23.890
So I think we need to be
patient with each other

00:37:23.890 --> 00:37:30.930
but be committed to firmly
but lovingly raise its use

00:37:30.930 --> 00:37:32.860
and provide truth.

00:37:32.860 --> 00:37:35.400
There is a study I
want to talk about it.

00:37:35.400 --> 00:37:37.540
It was published recently,
very elegant study

00:37:37.540 --> 00:37:40.510
published in Science, where
a group of researchers

00:37:40.510 --> 00:37:43.660
took political canvases
and sent them out

00:37:43.660 --> 00:37:47.620
to Democratic and
Republican voters

00:37:47.620 --> 00:37:52.690
and had these canvassers allow
the voter they were talking

00:37:52.690 --> 00:37:55.330
to to do most of the talking.

00:37:55.330 --> 00:38:01.510
And their job was to find
ways to link an experience

00:38:01.510 --> 00:38:05.020
in the life of the canvasser to
think of some experience they

00:38:05.020 --> 00:38:08.780
had had when they had
been treated negatively

00:38:08.780 --> 00:38:13.360
and then to link that to how
transgender individuals would

00:38:13.360 --> 00:38:16.510
feel when they are
treated negatively.

00:38:16.510 --> 00:38:19.990
And this showed that
this kind of conversation

00:38:19.990 --> 00:38:24.100
was effective in reducing
prejudice against transgender

00:38:24.100 --> 00:38:28.660
individuals, it was effective
in increasing levels of support

00:38:28.660 --> 00:38:35.330
for policies to ensure
anti-discrimination legislation

00:38:35.330 --> 00:38:37.120
towards transgender individuals.

00:38:37.120 --> 00:38:39.691
And what they would
say is the key

00:38:39.691 --> 00:38:44.770
is the specific language
and conversation that

00:38:44.770 --> 00:38:46.720
was used to engender this.

00:38:46.720 --> 00:38:48.790
And this organization out
in Southern California

00:38:48.790 --> 00:38:53.800
who does this work, they tested
13,000 conversations first

00:38:53.800 --> 00:38:56.590
to find the right
kind of conversation

00:38:56.590 --> 00:39:00.910
that nonetheless can open the
door to help to build empathy.

00:39:00.910 --> 00:39:04.870
And I think the building of
empathy of putting ourselves

00:39:04.870 --> 00:39:08.970
in the shoes of another
is one of the keys

00:39:08.970 --> 00:39:10.870
to build in the
kind of tolerance

00:39:10.870 --> 00:39:12.470
that we need in our society.

00:39:12.470 --> 00:39:15.040
PHILLIP MARTIN: One of
the cauldrons that we're

00:39:15.040 --> 00:39:19.330
seeing where empathy is
basically being challenged

00:39:19.330 --> 00:39:22.620
would contravene and it seems
to be taking place online.

00:39:22.620 --> 00:39:24.860
I think about
Reddit, for example,

00:39:24.860 --> 00:39:32.950
which has become a platform for
abuse and for a lot of haters.

00:39:32.950 --> 00:39:38.260
And, Dipayan, how are
you basically finding

00:39:38.260 --> 00:39:42.010
Reddit and other
social media platforms?

00:39:42.010 --> 00:39:45.880
Are the conversations
that David talked about

00:39:45.880 --> 00:39:47.980
and the efforts toward empathy--

00:39:47.980 --> 00:39:52.600
or can they possibly take
place on online platforms

00:39:52.600 --> 00:39:57.970
that seem to have been basically
subsumed by the haters?

00:39:57.970 --> 00:40:00.490
DIPAYAN GHOSH: Wow, it's
very difficult, I think.

00:40:00.490 --> 00:40:02.380
I think it's very
difficult. To cite

00:40:02.380 --> 00:40:05.920
another study, a
different study,

00:40:05.920 --> 00:40:09.370
MIT researchers showed
about just a few months ago

00:40:09.370 --> 00:40:10.720
in a paper--

00:40:10.720 --> 00:40:15.580
I believe in Science or
Nature-- that falsehoods travel

00:40:15.580 --> 00:40:17.830
20 times faster than the truth.

00:40:17.830 --> 00:40:21.280
And they travel faster,
they travel farther,

00:40:21.280 --> 00:40:24.850
and they reach deeper
into social networks,

00:40:24.850 --> 00:40:28.340
meaning to individual
people using Twitter,

00:40:28.340 --> 00:40:32.850
which was the social network
that these researchers

00:40:32.850 --> 00:40:34.990
analyzed.

00:40:34.990 --> 00:40:39.250
And I think what that suggests
is that, well, falsehood,

00:40:39.250 --> 00:40:42.400
and hate, and disinformation
are all linked together.

00:40:42.400 --> 00:40:46.570
And in the practice of
spreading hate online,

00:40:46.570 --> 00:40:48.340
on social networks,
or the practice

00:40:48.340 --> 00:40:51.160
of spreading
disinformation, we've

00:40:51.160 --> 00:40:54.190
seen over, and over,
and over, again,

00:40:54.190 --> 00:40:56.980
that propagators of
this kind of content

00:40:56.980 --> 00:40:59.500
are really linking
the two and trying

00:40:59.500 --> 00:41:04.660
to hit those thin cracks
in American society

00:41:04.660 --> 00:41:08.080
and pound them over, and over,
and over, again, and fracture

00:41:08.080 --> 00:41:14.650
society by targeting ads, or
targeting content, or pulling

00:41:14.650 --> 00:41:18.610
people into filter
bubbles and showering them

00:41:18.610 --> 00:41:22.720
with content about a
particular political issue that

00:41:22.720 --> 00:41:26.800
triggers hatred or triggers
discriminatory action going

00:41:26.800 --> 00:41:28.940
forward.

00:41:28.940 --> 00:41:33.190
And I think to really
resolve that kind of issue,

00:41:33.190 --> 00:41:34.910
we have to revisit
the business model.

00:41:34.910 --> 00:41:36.730
We have to think,
again, about how

00:41:36.730 --> 00:41:39.790
do Facebook, Twitter,
YouTube, how do they work?

00:41:39.790 --> 00:41:41.266
It's about these services.

00:41:41.266 --> 00:41:42.640
It's about the
collection of data

00:41:42.640 --> 00:41:45.520
to drive behavioral
profiling, and it's

00:41:45.520 --> 00:41:49.570
about these algorithms that
target ads and curate content.

00:41:49.570 --> 00:41:56.200
And to really address hate
speech in the long run,

00:41:56.200 --> 00:41:57.580
we're seeing two issues.

00:41:57.580 --> 00:42:01.960
We're seeing this problem
of filter bubbles which is--

00:42:01.960 --> 00:42:05.080
I believe-- is caused by this
business model of people trying

00:42:05.080 --> 00:42:08.650
to force people--

00:42:08.650 --> 00:42:11.440
hate speech propagators and
disinformation operators

00:42:11.440 --> 00:42:14.230
trying to force people
into these filter bubbles

00:42:14.230 --> 00:42:19.030
and increase engagement
over these platforms,

00:42:19.030 --> 00:42:21.160
as the internet
platforms themselves

00:42:21.160 --> 00:42:24.610
want to increase
their ad revenues.

00:42:24.610 --> 00:42:28.480
And we're seeing this
problem of pushing content

00:42:28.480 --> 00:42:32.320
against those thin cracks to
try to break people or break

00:42:32.320 --> 00:42:33.925
their will to be tolerant.

00:42:36.880 --> 00:42:40.150
I think the only place
to start, then, is

00:42:40.150 --> 00:42:42.580
to address that infrastructure.

00:42:42.580 --> 00:42:46.180
And that's going to require
a lot of political will,

00:42:46.180 --> 00:42:51.310
and right now if we-- we
talked earlier about politics,

00:42:51.310 --> 00:42:54.580
these issues even as they
pertain to social media

00:42:54.580 --> 00:42:58.180
are divided along
partisan lines.

00:42:58.180 --> 00:43:00.490
When I worked in the White
House during the Obama

00:43:00.490 --> 00:43:02.260
administration, we
saw this, and it

00:43:02.260 --> 00:43:05.590
was very difficult to
do anything about it.

00:43:05.590 --> 00:43:10.000
When I worked at Facebook in
Washington, we saw this again.

00:43:10.000 --> 00:43:14.860
And I think it's going
to be extraordinarily

00:43:14.860 --> 00:43:17.470
difficult to address it,
but we need political will.

00:43:17.470 --> 00:43:22.210
We need to build
sentiment and build up

00:43:22.210 --> 00:43:25.660
the public education on
these kinds of issues

00:43:25.660 --> 00:43:29.320
and start to address them
looking deep down at the way

00:43:29.320 --> 00:43:31.680
that the internet
is structured today.

00:43:31.680 --> 00:43:33.520
PHILLIP MARTIN: Let's
take it from online

00:43:33.520 --> 00:43:37.940
to the streets of the role, for
example, of law enforcement,

00:43:37.940 --> 00:43:43.630
in much of what many people see
as antithetical relationship

00:43:43.630 --> 00:43:49.575
with communities of color,
with many communities

00:43:49.575 --> 00:43:50.200
in the country.

00:43:50.200 --> 00:43:52.780
And then many seem
to be emboldened

00:43:52.780 --> 00:43:56.410
by the current administration,
the Justice Department,

00:43:56.410 --> 00:43:58.830
and by the White House.

00:43:58.830 --> 00:44:01.520
Jim, can you talk about--

00:44:01.520 --> 00:44:04.770
and then I'm going to
ask the panel pretty

00:44:04.770 --> 00:44:07.860
much the same-- the panel here,
pretty much the same question--

00:44:07.860 --> 00:44:12.000
can you talk about the role
that police officers are playing

00:44:12.000 --> 00:44:16.080
in terms of, if you will,
the receptivity to a lot

00:44:16.080 --> 00:44:22.860
of these negative messages
that are being, if you will,

00:44:22.860 --> 00:44:27.919
being seen as messages of hate.

00:44:27.919 --> 00:44:29.460
JIM DOYLE: Many more
people are going

00:44:29.460 --> 00:44:31.470
to have contact with
a police officer

00:44:31.470 --> 00:44:34.130
than they are with the
President of the United States.

00:44:34.130 --> 00:44:36.930
And many-- or a governor,
or an attorney general--

00:44:36.930 --> 00:44:41.880
and many are going to draw
their conclusions about how

00:44:41.880 --> 00:44:46.440
the government reacts to them
over their lifetimes based

00:44:46.440 --> 00:44:48.110
on that contact.

00:44:48.110 --> 00:44:50.770
We have been at this for a
long time of the training,

00:44:50.770 --> 00:44:52.500
and that's why I
really encourage

00:44:52.500 --> 00:44:54.090
all of people who
have spoken here

00:44:54.090 --> 00:44:57.450
about the training
they do to modernize

00:44:57.450 --> 00:44:59.760
police training on this issue.

00:44:59.760 --> 00:45:04.560
After Rodney King, we set
out on a large scale training

00:45:04.560 --> 00:45:06.750
in Wisconsin, as
many states did,

00:45:06.750 --> 00:45:10.680
on police understanding
racial divisions.

00:45:10.680 --> 00:45:12.590
Much of the training
when I look back at it--

00:45:12.590 --> 00:45:14.800
and I've seen the results--
was just plain wrong.

00:45:14.800 --> 00:45:20.220
I mean, a lot of it was
police officers going up

00:45:20.220 --> 00:45:23.370
to people to African-American
people and calling them,

00:45:23.370 --> 00:45:25.110
man, because they
thought that was

00:45:25.110 --> 00:45:27.030
how they were going
to relate more closely

00:45:27.030 --> 00:45:28.840
and be part of the community.

00:45:28.840 --> 00:45:30.870
Much of the training
and community policing

00:45:30.870 --> 00:45:32.880
was to teach white
police officers how

00:45:32.880 --> 00:45:35.520
to talk kind of
jive, street jive,

00:45:35.520 --> 00:45:41.160
thinking that somehow that was
making them culturally aware

00:45:41.160 --> 00:45:44.250
and with the people that
they were talking about.

00:45:44.250 --> 00:45:47.640
You saw it at Harvard with
the Professor Gates incident

00:45:47.640 --> 00:45:52.230
where a distinguished professor
is treated in that way.

00:45:52.230 --> 00:45:56.130
We probably all know, and I
know dear relatives and friends

00:45:56.130 --> 00:45:58.020
who have been treated--

00:45:58.020 --> 00:46:00.930
I've often asked-- I think the
training for police officers

00:46:00.930 --> 00:46:06.780
ought to start by having rich
African-American people come in

00:46:06.780 --> 00:46:09.060
so they know this
isn't about poverty

00:46:09.060 --> 00:46:12.090
and tell them about their worst
experience with the police.

00:46:12.090 --> 00:46:15.000
And white people are shocked
to hear these stories.

00:46:15.000 --> 00:46:16.740
I've done this a few times.

00:46:16.740 --> 00:46:19.290
To hear to hear their
friends talk about what

00:46:19.290 --> 00:46:22.080
happened to them when they went
in the hardware store and got

00:46:22.080 --> 00:46:24.480
followed around and then
stopped because maybe they

00:46:24.480 --> 00:46:25.740
were shoplifters.

00:46:25.740 --> 00:46:28.440
So it gets back to
this empathy issue.

00:46:28.440 --> 00:46:31.500
And much of the training
was good and, obviously,

00:46:31.500 --> 00:46:33.090
the major part of
law enforcement

00:46:33.090 --> 00:46:35.426
after training has to
be, this is the law,

00:46:35.426 --> 00:46:37.050
and you have to
enforce the law, and it

00:46:37.050 --> 00:46:41.100
doesn't matter who the person is
in front of you or their color.

00:46:41.100 --> 00:46:44.190
But when you get into these
deeper assumptions that people

00:46:44.190 --> 00:46:47.550
have-- and many police officers
are not immune to them--

00:46:47.550 --> 00:46:50.070
that for some genetic
reason black people

00:46:50.070 --> 00:46:53.140
have more criminal
disposition than white people,

00:46:53.140 --> 00:46:55.950
that's at the heart of much of
what we're talking about here.

00:46:55.950 --> 00:46:58.630
And that gets
reinforced politically.

00:46:58.630 --> 00:47:01.670
So I really encourage the kind
of training we've talked about

00:47:01.670 --> 00:47:03.790
and we've heard about in
the schools and others,

00:47:03.790 --> 00:47:06.120
particularly about
empathy, to really

00:47:06.120 --> 00:47:09.690
have that be the training that
moves into the law enforcement

00:47:09.690 --> 00:47:10.782
realm as well.

00:47:10.782 --> 00:47:12.240
I want to make one
other point if I

00:47:12.240 --> 00:47:14.190
can about politics quickly.

00:47:14.190 --> 00:47:17.070
We've come, unfortunately,
in this country now,

00:47:17.070 --> 00:47:22.320
to two parties that are-- one
is almost an all-white party

00:47:22.320 --> 00:47:24.450
and one is a party
that is made up

00:47:24.450 --> 00:47:30.690
of most racial minority, some
white males, and about half

00:47:30.690 --> 00:47:32.190
of the white females
in the country.

00:47:32.190 --> 00:47:34.080
If you just look at
this demographically,

00:47:34.080 --> 00:47:36.870
that's not a good place
for us to be politically

00:47:36.870 --> 00:47:40.500
because the white party has
to maximize its white vote

00:47:40.500 --> 00:47:41.880
in every election.

00:47:41.880 --> 00:47:44.190
And the way you do that
is to get white people,

00:47:44.190 --> 00:47:47.580
the majority people
voting as a block,

00:47:47.580 --> 00:47:51.000
and we saw this in the South
after the Civil Rights Act.

00:47:51.000 --> 00:47:54.390
Nobody thought in the South
that Republicans someday could

00:47:54.390 --> 00:47:57.207
get 75% of the white vote.

00:47:57.207 --> 00:47:58.540
They said, that'll never happen.

00:47:58.540 --> 00:47:59.820
Well, it's happened.

00:47:59.820 --> 00:48:03.190
And now you are seeing that same
thing happening politically.

00:48:03.190 --> 00:48:06.360
And the result is on this--
on the messaging issue that

00:48:06.360 --> 00:48:08.220
we've talked about--

00:48:08.220 --> 00:48:10.860
there are political
incentives now

00:48:10.860 --> 00:48:13.850
that are exploited, as was just
described, on the internet.

00:48:13.850 --> 00:48:16.180
What do we know about
what the Russians did?

00:48:16.180 --> 00:48:20.130
Some of that's come out is
they exploited racial divisions

00:48:20.130 --> 00:48:24.540
by putting all kinds of stuff
out on the internet that both--

00:48:24.540 --> 00:48:27.450
after police shootings--
that inflamed both sides.

00:48:27.450 --> 00:48:29.880
That's how they know
they can get to us.

00:48:29.880 --> 00:48:32.040
And our politics
is now driving us

00:48:32.040 --> 00:48:35.730
into that same position
where one side benefits

00:48:35.730 --> 00:48:37.830
from inflaming it,
and the other side

00:48:37.830 --> 00:48:39.990
benefits by trying
to make sure that all

00:48:39.990 --> 00:48:42.630
of the people of minority
background in the country

00:48:42.630 --> 00:48:43.920
are voting for their party.

00:48:43.920 --> 00:48:48.420
And we now are in a very, very
difficult political place.

00:48:48.420 --> 00:48:50.580
We've been here for a
long time, but it has now

00:48:50.580 --> 00:48:53.760
gotten that you just look at
the numbers, and the vote,

00:48:53.760 --> 00:48:58.580
and how it breaks down by
race, it is just stark.

00:48:58.580 --> 00:49:01.574
We have parties that are
divided on this issue.

00:49:01.574 --> 00:49:02.990
PHILLIP MARTIN: I
can tell you, we

00:49:02.990 --> 00:49:06.440
could talk about
this, folks, for days

00:49:06.440 --> 00:49:08.750
because it's so
much to talk about.

00:49:08.750 --> 00:49:11.990
We have time at this point
for just a few questions

00:49:11.990 --> 00:49:16.160
from our listeners
and our viewers.

00:49:16.160 --> 00:49:20.210
This question is from Michael
and he says, "Clearly, there

00:49:20.210 --> 00:49:22.940
are a host of levels
that impact racism,

00:49:22.940 --> 00:49:25.370
institutional,
cultural, et cetera,

00:49:25.370 --> 00:49:27.350
what is our best
chance of changing

00:49:27.350 --> 00:49:30.590
racism on an individual level?"

00:49:30.590 --> 00:49:34.340
I'm going to direct this one
right now to David and then

00:49:34.340 --> 00:49:37.460
to the rest of our panel.

00:49:37.460 --> 00:49:39.320
DAVID WILLIAMS: It's
a big challenge.

00:49:39.320 --> 00:49:43.280
I think I would say we need
to raise awareness levels.

00:49:43.280 --> 00:49:45.250
So what's happening
with teaching tolerance

00:49:45.250 --> 00:49:47.640
so that people
are knowledgeable.

00:49:47.640 --> 00:49:52.860
And I would say that the media
has a powerful role to play.

00:49:52.860 --> 00:49:55.465
We have seen-- now, I'm not
suggesting in any way, shape,

00:49:55.465 --> 00:49:57.090
or form that we've
solved the problem--

00:49:57.090 --> 00:49:59.510
but we have seen
striking declines

00:49:59.510 --> 00:50:02.960
in the levels of prejudice
against LGBT populations

00:50:02.960 --> 00:50:03.950
in the United States.

00:50:03.950 --> 00:50:05.750
That has not happened by chance.

00:50:05.750 --> 00:50:08.300
Much of that has been linked
in scientific research

00:50:08.300 --> 00:50:11.200
to explicit strategies
that were implemented

00:50:11.200 --> 00:50:14.610
in the media that has led
to reductions of prejudice.

00:50:14.610 --> 00:50:18.010
And I think we need
to change the culture.

00:50:18.010 --> 00:50:20.480
We need to change
what people think.

00:50:20.480 --> 00:50:24.470
And the media and other
larger cultural institutions,

00:50:24.470 --> 00:50:28.370
like religious institutions,
can also play an important role

00:50:28.370 --> 00:50:31.310
in changing the very
culture around these issues.

00:50:31.310 --> 00:50:32.760
PHILLIP MARTIN: Oren.

00:50:32.760 --> 00:50:35.570
OREN SEGAL: Yeah, I mean-- this
was the $64,000 or the million

00:50:35.570 --> 00:50:38.130
dollar question.

00:50:38.130 --> 00:50:41.420
I do think this, in tracking
extremism of all types

00:50:41.420 --> 00:50:43.550
and narratives,
especially, online,

00:50:43.550 --> 00:50:48.110
I am always disappointed by
the lack of content that tells

00:50:48.110 --> 00:50:49.890
the other side of the story.

00:50:49.890 --> 00:50:51.830
So if you go onto
YouTube, and you put 9/11.

00:50:51.830 --> 00:50:53.413
You want to learn
about what happened,

00:50:53.413 --> 00:50:55.850
and younger people are not
quite sure what happened

00:50:55.850 --> 00:50:57.080
because they weren't alive.

00:50:57.080 --> 00:50:59.660
It's only a couple
of suggestions

00:50:59.660 --> 00:51:03.260
away from conspiracy theories,
and these our more mainstream

00:51:03.260 --> 00:51:05.940
platforms that are being
exploited by those who are

00:51:05.940 --> 00:51:07.190
giving these false narratives.

00:51:07.190 --> 00:51:08.780
These hate hateful narratives.

00:51:08.780 --> 00:51:12.260
So I do think creating
an opportunity for people

00:51:12.260 --> 00:51:13.380
to create content--

00:51:13.380 --> 00:51:15.890
not just that counters
it, that's difficult--

00:51:15.890 --> 00:51:21.110
but to have a place to
develop your own narratives

00:51:21.110 --> 00:51:25.400
is part of that effort of
creating critical thinking.

00:51:25.400 --> 00:51:27.530
Not only think about
what you're seeing

00:51:27.530 --> 00:51:29.150
and what you're
bringing in, and think

00:51:29.150 --> 00:51:31.724
about who's trying to fool
you, and try to trick you,

00:51:31.724 --> 00:51:33.890
and brainwash you into
having these hateful beliefs,

00:51:33.890 --> 00:51:36.530
but you need to have an
opportunity to then leverage

00:51:36.530 --> 00:51:38.330
other types of content.

00:51:38.330 --> 00:51:41.240
I learn about new
technologies from two places,

00:51:41.240 --> 00:51:44.270
from extremists and from
my seven-year-old son.

00:51:44.270 --> 00:51:48.170
And so we need to be able to
arm our youth with the ability

00:51:48.170 --> 00:51:50.510
to tell their stories
in compelling ways that

00:51:50.510 --> 00:51:52.940
are as sexy, and cool,
and interesting as those

00:51:52.940 --> 00:51:55.250
who have those hateful ideas.

00:51:55.250 --> 00:51:58.320
PHILLIP MARTIN: One of the
questions I do want to ask--

00:51:58.320 --> 00:52:02.030
in the little time that we
have-- is, Maureen and Dipayan,

00:52:02.030 --> 00:52:03.800
are you saying--

00:52:03.800 --> 00:52:06.560
what is the impact
of what we're seeing

00:52:06.560 --> 00:52:08.600
outside of the United States?

00:52:08.600 --> 00:52:11.090
What impact is that
having on, if you will,

00:52:11.090 --> 00:52:14.660
thoughts about
tolerance intolerance

00:52:14.660 --> 00:52:16.100
here in the United States?

00:52:16.100 --> 00:52:18.650
Out of Hungary, for
example, where you see Roma

00:52:18.650 --> 00:52:21.800
under assault, where
you see immigrants

00:52:21.800 --> 00:52:24.200
immigration has
become an issue that

00:52:24.200 --> 00:52:26.990
has been defined as us
versus them like here

00:52:26.990 --> 00:52:29.660
in the United States.

00:52:29.660 --> 00:52:31.830
What's your view about
that, the impact?

00:52:31.830 --> 00:52:34.455
MAUREEN COSTELLO: I don't think
that we're seeing it explicitly

00:52:34.455 --> 00:52:37.500
on school children, for
instance, or even on educators,

00:52:37.500 --> 00:52:41.480
but I think that a lot of the
language and the ideas that

00:52:41.480 --> 00:52:46.100
are coming out of Europe are
being amplified in social media

00:52:46.100 --> 00:52:47.656
here in the United States.

00:52:47.656 --> 00:52:49.280
And nobody is paying
attention to where

00:52:49.280 --> 00:52:52.370
they're coming from, basically.

00:52:52.370 --> 00:52:55.490
So I think that the notion
that just because they're

00:52:55.490 --> 00:52:59.960
across the ocean, they don't
affect those is just untrue.

00:52:59.960 --> 00:53:02.390
DIPAYAN GHOSH: Just
to add to that,

00:53:02.390 --> 00:53:06.560
I completely agree with
Maureen, and we're definitely

00:53:06.560 --> 00:53:10.040
seeing some really nasty
themes come particularly

00:53:10.040 --> 00:53:12.860
from all over Europe,
Eastern Europe, Western

00:53:12.860 --> 00:53:17.930
Europe, and Northern
Europe, that are,

00:53:17.930 --> 00:53:23.420
as Oren and I have researched
a little bit and spoken about,

00:53:23.420 --> 00:53:26.090
driven from this idea
of identitarianism.

00:53:26.090 --> 00:53:30.620
And that is definitely
shaping a lot

00:53:30.620 --> 00:53:36.170
of cultural creation of this--

00:53:36.170 --> 00:53:42.420
or this ethos that we are
better and they're worse.

00:53:42.420 --> 00:53:46.220
And I think that people like
Richard Spencer and people

00:53:46.220 --> 00:53:48.830
like that in the United
States have certainly

00:53:48.830 --> 00:53:53.540
subsumed that message
and projected it here.

00:53:53.540 --> 00:53:58.364
And that's obviously dangerous.

00:53:58.364 --> 00:54:00.030
PHILLIP MARTIN: I'm
looking at our clock

00:54:00.030 --> 00:54:02.430
and so-- but I also
need all of you

00:54:02.430 --> 00:54:05.250
to just take a moment
to, if you will,

00:54:05.250 --> 00:54:08.130
summarize this discussion
and your thoughts

00:54:08.130 --> 00:54:13.290
on hatred and intolerance that's
sweeping across our nation,

00:54:13.290 --> 00:54:14.720
unfortunately.

00:54:14.720 --> 00:54:15.360
Oren.

00:54:15.360 --> 00:54:18.240
OREN SEGAL: Sure, to the degree
this is a final word for me,

00:54:18.240 --> 00:54:22.620
I would say, we have a heat map
at ADL where we track incidents

00:54:22.620 --> 00:54:26.730
of anti-Semitism, hate
crimes, extremist activity

00:54:26.730 --> 00:54:27.460
of all types.

00:54:27.460 --> 00:54:29.850
And I'm always reminded,
as we're trying to explain

00:54:29.850 --> 00:54:32.190
to the public, the
trends that we're seeing,

00:54:32.190 --> 00:54:35.430
that each one of those points
on a map over 5,000-- now,

00:54:35.430 --> 00:54:37.260
over the last two years--

00:54:37.260 --> 00:54:39.840
is a story of
community resilience.

00:54:39.840 --> 00:54:41.970
Is a story of people
coming together

00:54:41.970 --> 00:54:42.990
and rejecting that hate.

00:54:42.990 --> 00:54:44.610
And to your point
about the media,

00:54:44.610 --> 00:54:47.860
I think we need to start
telling more inspiring stories.

00:54:47.860 --> 00:54:51.300
And because when we hear
that, that also has an impact

00:54:51.300 --> 00:54:54.180
and that also maybe creates
courage amongst people

00:54:54.180 --> 00:54:57.430
to hold all those who are
purveyors of hate accountable,

00:54:57.430 --> 00:54:59.160
whether they're in
your local community

00:54:59.160 --> 00:55:01.220
or whether they're in
the highest office.

00:55:01.220 --> 00:55:02.880
We all have a voice.

00:55:02.880 --> 00:55:05.670
And we need to constantly
support all those

00:55:05.670 --> 00:55:07.980
in our communities to
show that hate really

00:55:07.980 --> 00:55:10.080
does have no place here.

00:55:10.080 --> 00:55:11.730
So you don't do this
work for 20 years

00:55:11.730 --> 00:55:14.320
without having some sort of hope
that things will get better.

00:55:14.320 --> 00:55:15.810
But I think the
data, and I think

00:55:15.810 --> 00:55:18.920
the training, whether it's for
law enforcement, students, et

00:55:18.920 --> 00:55:21.690
cetera, will help make
those dots a little bit

00:55:21.690 --> 00:55:23.460
more actionable.

00:55:23.460 --> 00:55:26.705
And you're turning basically
lemons into lemonade.

00:55:26.705 --> 00:55:28.080
DAVID WILLIAMS:
Two quick things.

00:55:28.080 --> 00:55:30.930
Governor Doyle talked
about interactions

00:55:30.930 --> 00:55:33.117
of African-Americans
with the police.

00:55:33.117 --> 00:55:35.200
I and other colleagues
published a paper last year

00:55:35.200 --> 00:55:39.750
in Lancet that shows
that when police killed

00:55:39.750 --> 00:55:42.240
an unarmed
African-American male,

00:55:42.240 --> 00:55:46.050
the mental health of the entire
African-American population

00:55:46.050 --> 00:55:49.230
in that state is adversely
affected for the next three

00:55:49.230 --> 00:55:50.200
months.

00:55:50.200 --> 00:55:52.950
So, again, it's
another example of this

00:55:52.950 --> 00:55:56.310
is affecting the quality of
life of individuals here.

00:55:56.310 --> 00:55:58.590
And, finally, my
other quick point

00:55:58.590 --> 00:56:02.440
is that this
environment of hate.

00:56:02.440 --> 00:56:04.860
It's not only about
individual interaction.

00:56:04.860 --> 00:56:08.400
It is driving social
policy, and we

00:56:08.400 --> 00:56:11.640
are leaning towards policies,
the policy proposals

00:56:11.640 --> 00:56:15.900
right now in Washington DC, that
will destroy the social safety

00:56:15.900 --> 00:56:18.100
net as we know it today.

00:56:18.100 --> 00:56:21.000
And we don't have to guess
about what would happen.

00:56:21.000 --> 00:56:23.940
In 1981, the omnibus
reconciliation bill,

00:56:23.940 --> 00:56:25.950
early in the Reagan
administration,

00:56:25.950 --> 00:56:29.550
let a million people lost
food stamps and 600,000 people

00:56:29.550 --> 00:56:31.620
would dropped from Medicare--

00:56:31.620 --> 00:56:32.370
Medicaid.

00:56:32.370 --> 00:56:36.510
And 250 community health centers
closed in the United States

00:56:36.510 --> 00:56:39.180
as a result to cuts
to social services,

00:56:39.180 --> 00:56:42.240
and there were pervasive
negative effects on children,

00:56:42.240 --> 00:56:45.840
on pregnant women, on the
elderly in the United States.

00:56:45.840 --> 00:56:48.480
So we have to look not
only at all interactions,

00:56:48.480 --> 00:56:51.300
we have to look at the
policies that we decide

00:56:51.300 --> 00:56:55.110
driven by false narratives.

00:56:55.110 --> 00:56:56.190
PHILLIP MARTIN: Dipayan.

00:56:56.190 --> 00:57:00.160
DIPAYAN GHOSH: I'll also
share just two quick thoughts.

00:57:00.160 --> 00:57:02.460
First, addressing
the online space.

00:57:02.460 --> 00:57:05.790
Again, I think these internet
platforms are designed

00:57:05.790 --> 00:57:10.830
the way they are because they're
designed to increase engagement

00:57:10.830 --> 00:57:12.990
and there's no
regulatory regime that

00:57:12.990 --> 00:57:16.500
sits above them right now
that that tells them, no, you

00:57:16.500 --> 00:57:18.600
can't do that. you
can't spread hate. you

00:57:18.600 --> 00:57:20.590
can't spread discrimination.

00:57:20.590 --> 00:57:23.460
One example, Latanya
Sweeney, a professor

00:57:23.460 --> 00:57:27.420
of computer science here at
Harvard, searched for her name

00:57:27.420 --> 00:57:29.580
on Google, and I
believe the story

00:57:29.580 --> 00:57:33.450
that she reported is that
she saw an ad for jails

00:57:33.450 --> 00:57:37.200
because Google inferred
that her name is associated

00:57:37.200 --> 00:57:42.360
with African-American heritage
and thought that, hey, we

00:57:42.360 --> 00:57:44.300
should show her this ad.

00:57:44.300 --> 00:57:47.270
In another example, Google--

00:57:47.270 --> 00:57:53.490
when kids were searching for
gorillas on Google Images--

00:57:53.490 --> 00:57:59.490
people saw images of minorities
in the United States.

00:57:59.490 --> 00:58:03.630
These systems are designed
to drive engagement

00:58:03.630 --> 00:58:06.595
because they want professor
Sweeney to click on that ad

00:58:06.595 --> 00:58:08.970
because they think that that's
going to drive engagement.

00:58:08.970 --> 00:58:11.770
They want people to--

00:58:11.770 --> 00:58:13.440
they design these
algorithms in ways

00:58:13.440 --> 00:58:17.670
that encourages clicks and
encourages bigger ad spend.

00:58:17.670 --> 00:58:22.230
So my first point is that we
need to address that system.

00:58:22.230 --> 00:58:24.540
We need our leadership
to understand

00:58:24.540 --> 00:58:29.190
how these systems work and start
to address them at their core.

00:58:29.190 --> 00:58:31.950
Second point is
just a broader one

00:58:31.950 --> 00:58:34.920
which is that I think
our leaders, in society,

00:58:34.920 --> 00:58:40.470
from politicians to
actors and actresses,

00:58:40.470 --> 00:58:46.590
thought leaders need to be more
honest and need to speak up.

00:58:46.590 --> 00:58:49.170
I think, just one example--

00:58:49.170 --> 00:58:52.140
this didn't come up yet--
but with Liam Neeson,

00:58:52.140 --> 00:58:56.076
we've seen over
the past few days.

00:58:56.076 --> 00:58:57.450
I want to highlight
not just what

00:58:57.450 --> 00:59:02.100
he said, which was honest to
the public, which I appreciate

00:59:02.100 --> 00:59:05.130
at some level, but also
what a soccer player,

00:59:05.130 --> 00:59:11.250
John Barnes of African origin
but who played in England,

00:59:11.250 --> 00:59:13.290
his reaction to Liam Neeson.

00:59:13.290 --> 00:59:16.110
And his reaction was one
of great appreciation

00:59:16.110 --> 00:59:21.930
for the honesty, instead of
just the traditional media

00:59:21.930 --> 00:59:24.060
reaction which was that,
wow, Liam Neeson is

00:59:24.060 --> 00:59:27.480
a horrible person, and
we need to vilify him.

00:59:27.480 --> 00:59:32.160
So I would appreciate
more honesty.

00:59:32.160 --> 00:59:34.140
We've seen that in
the US Congress,

00:59:34.140 --> 00:59:36.030
as well, over the past week.

00:59:36.030 --> 00:59:38.055
So just two quick
points, thank you.

00:59:38.055 --> 00:59:39.289
PHILLIP MARTIN: Maureen.

00:59:39.289 --> 00:59:40.830
MAUREEN COSTELLO:
I think we're going

00:59:40.830 --> 00:59:45.540
to win or lose this battle
in the schools, basically.

00:59:45.540 --> 00:59:51.000
If we do not orient our
schools towards the vision

00:59:51.000 --> 00:59:54.330
that they are, in fact,
building the society that we're

00:59:54.330 --> 00:59:57.960
going to live in in the
future, we have lost.

00:59:57.960 --> 01:00:01.920
We've been focusing a lot in
schools on college and career

01:00:01.920 --> 01:00:06.510
readiness, and we really have
to focus on social and emotional

01:00:06.510 --> 01:00:08.640
readiness, as well.

01:00:08.640 --> 01:00:12.390
My ideal-- the
ideal policy change

01:00:12.390 --> 01:00:18.060
is to really make integration
a focus of our school policy.

01:00:18.060 --> 01:00:20.220
But if we can't
go there, then we

01:00:20.220 --> 01:00:24.060
really need to make sure
that young people have

01:00:24.060 --> 01:00:26.580
digital intelligence so
that they know not only

01:00:26.580 --> 01:00:31.380
how to interpret material that
comes to them as consumers,

01:00:31.380 --> 01:00:34.900
but that they also learn how
to be productive consumers--

01:00:34.900 --> 01:00:38.370
producers for social media.

01:00:38.370 --> 01:00:42.870
I think at the end of the day,
I'm reminded of the phrase

01:00:42.870 --> 01:00:44.970
that Dr. King used in
his last book, which

01:00:44.970 --> 01:00:47.610
is that we have a choice
between chaos and community.

01:00:47.610 --> 01:00:51.000
And what we've been talking
about so much is isolation.

01:00:51.000 --> 01:00:53.220
Hate grows in isolation.

01:00:53.220 --> 01:00:55.830
And schools are
places of community.

01:00:55.830 --> 01:00:59.310
And so everything that we need
to do is deliberate talking--

01:00:59.310 --> 01:01:03.090
teaching about the kind of
discrimination that exists,

01:01:03.090 --> 01:01:09.780
not pretending it doesn't
exist, admitting that we've all

01:01:09.780 --> 01:01:12.509
been socialized to be racists.

01:01:12.509 --> 01:01:14.550
And I think it's particularly
an issue in schools

01:01:14.550 --> 01:01:19.250
because 80% of teachers
are white women.

01:01:19.250 --> 01:01:20.820
And I'm very fond
of white women.

01:01:20.820 --> 01:01:22.552
[LAUGHTER]

01:01:23.420 --> 01:01:28.440
But they carry
with them-- we all

01:01:28.440 --> 01:01:31.020
carry with us implicit biases.

01:01:31.020 --> 01:01:33.764
It's one of the issues that
a lot of good police training

01:01:33.764 --> 01:01:35.430
has tried to address,
and it's something

01:01:35.430 --> 01:01:39.030
we have to address
with teachers as well.

01:01:39.030 --> 01:01:40.920
But we really,
really need to decide

01:01:40.920 --> 01:01:43.830
that this next
generation has to be

01:01:43.830 --> 01:01:46.920
better than we are generally.

01:01:46.920 --> 01:01:49.950
PHILLIP MARTIN: And, Jim,
in Wisconsin, please,

01:01:49.950 --> 01:01:51.507
your last words.

01:01:51.507 --> 01:01:52.590
Well, not your last words.

01:01:52.590 --> 01:01:54.300
[LAUGHTER]

01:01:54.300 --> 01:01:55.260
Your words today.

01:01:55.260 --> 01:01:57.072
[LAUGHTER]

01:01:59.340 --> 01:02:01.020
JIM DOYLE: Well, I
agree with everything

01:02:01.020 --> 01:02:02.100
that's just been said.

01:02:02.100 --> 01:02:06.240
I had a friend, a European,
say to me recently,

01:02:06.240 --> 01:02:08.220
whatever happened
to your country?

01:02:08.220 --> 01:02:10.260
And he went off about
how bad everything was.

01:02:10.260 --> 01:02:12.000
I said, well, remember,
this is a country

01:02:12.000 --> 01:02:14.520
that, just a few
years ago, you were

01:02:14.520 --> 01:02:18.240
cheering because of the election
of Barack Obama as president.

01:02:18.240 --> 01:02:20.160
And it's not like
everybody just suddenly

01:02:20.160 --> 01:02:22.320
moved out of this country
and a whole new group,

01:02:22.320 --> 01:02:24.300
new people moved in.

01:02:24.300 --> 01:02:28.030
But what happened is we
are a complicated country.

01:02:28.030 --> 01:02:30.960
And while we've talked
about the problems here,

01:02:30.960 --> 01:02:34.050
we are a country
of great tolerance

01:02:34.050 --> 01:02:37.380
and of acceptance, and in our
schools, and churches, and much

01:02:37.380 --> 01:02:39.220
of the work that's
being done here.

01:02:39.220 --> 01:02:42.270
The police officers
all across the country

01:02:42.270 --> 01:02:45.100
are doing really, really
important things on this.

01:02:45.100 --> 01:02:47.350
So I'm going to end this if
I could just politically,

01:02:47.350 --> 01:02:49.890
which is in Wisconsin--
and we weren't unique--

01:02:49.890 --> 01:02:53.400
in this last
election, in November,

01:02:53.400 --> 01:02:57.540
we had more people vote
in an off-year election

01:02:57.540 --> 01:03:00.530
than at any time since
the Second World War

01:03:00.530 --> 01:03:04.230
and a massive turnout
at presidential levels.

01:03:04.230 --> 01:03:06.480
And it was a very,
very close vote,

01:03:06.480 --> 01:03:09.900
and, I will say the
person I wanted to win won

01:03:09.900 --> 01:03:11.380
but that's beside the point.

01:03:11.380 --> 01:03:16.590
The point is we do have a very,
very engaged political world

01:03:16.590 --> 01:03:17.820
right now.

01:03:17.820 --> 01:03:19.770
And all of the
education we've talked

01:03:19.770 --> 01:03:21.840
about, the issues
of implicit bias,

01:03:21.840 --> 01:03:24.810
of tolerance, the more we talk
about that in all the ways

01:03:24.810 --> 01:03:27.330
we've talked about here
today really affect

01:03:27.330 --> 01:03:30.680
how current voters and
younger people who are going

01:03:30.680 --> 01:03:32.790
to be coming voters will vote.

01:03:32.790 --> 01:03:35.060
And that's how in a democracy--

01:03:35.060 --> 01:03:38.250
back to Professor
Williams' comments

01:03:38.250 --> 01:03:42.600
about the policies that
make the difference-- that's

01:03:42.600 --> 01:03:44.430
how we try to make
sure the policies

01:03:44.430 --> 01:03:46.590
we want are effectuated.

01:03:46.590 --> 01:03:49.860
And so it is really critical
that we have a very, very

01:03:49.860 --> 01:03:54.840
engaged political body and
I'll give President Trump

01:03:54.840 --> 01:03:59.170
credit for this-- he has truly
engaged the American people.

01:03:59.170 --> 01:04:03.510
And we are seeing voting turnout
like we have never seen before,

01:04:03.510 --> 01:04:04.660
and that's a good thing.

01:04:04.660 --> 01:04:08.700
And that, politically, is the
way you address these issues.

01:04:08.700 --> 01:04:11.660
PHILLIP MARTIN: Jim, thank
you, and on that note.

01:04:11.660 --> 01:04:13.710
And on that note,
I want to thank

01:04:13.710 --> 01:04:18.930
our panel, Jim, Maureen,
Dipayan, you know David,

01:04:18.930 --> 01:04:19.524
and Oren.

01:04:19.524 --> 01:04:21.190
And I'd like to thank
you, our audience,

01:04:21.190 --> 01:04:23.640
for taking part in this forum.

01:04:23.640 --> 01:04:26.730
Let me mention also
something that's coming up

01:04:26.730 --> 01:04:29.565
on March 4, another event.

01:04:29.565 --> 01:04:30.690
And I'm going to read this.

01:04:30.690 --> 01:04:34.020
This is the Dr. Lawrence
and Roberta Cohn

01:04:34.020 --> 01:04:37.050
forum, it's deaths from
pregnancy and childbirth,

01:04:37.050 --> 01:04:40.530
why are more US mothers
dying and what can be done?

01:04:40.530 --> 01:04:46.410
This was also presented jointly
by PRI's The World and WGBH.

01:04:46.410 --> 01:04:49.560
I'd like to thank you, and
I'd like to thank our panel.

01:04:49.560 --> 01:04:51.970
Engaging discussion.

01:04:51.970 --> 01:04:55.020
There's a lot of work
to be done, obviously,

01:04:55.020 --> 01:04:58.650
but you've begun that work
and you are carrying out

01:04:58.650 --> 01:04:59.880
that work every day.

01:04:59.880 --> 01:05:01.020
Thank you very much.

01:05:01.020 --> 01:05:02.520
And I thank you.

01:05:02.520 --> 01:05:04.320
[APPLAUSE]

01:05:06.720 --> 01:05:09.770
[MUSIC PLAYING]

