Speaker 1:          00:00          Yeah.

Speaker 2:          00:01          In this video we'll see a second algorithm for updating the dictionary matrix in the spar schooling model. And a, this, a different algorithm is going to be based on blah, blah, coordinate descent optimization. So we're looking again at the problem of uh, optimizing the Dictionary d, uh, where we are going to assume that the spires representations for each input in our dataset are fixed and do not depend on the, and a, so if we do this, we've uh, seen before that it corresponds to minimizing with respect to our dictionary only the average reconstruction term because we're assuming that h of x is fixed. So we don't have this term here anymore. Okay. And we have the constraint that again, the columns of Matrix d, which are the atoms in our dictionary, a must be of unit norm.

Speaker 2:          00:57          So this alternative, a algorithm is going to be based on, uh, a, uh, an idea for a type of algorithm for optimization known as black corner. That corner dissent, uh, it's cornered descent because we are going to optimize with respect to each single variable while keeping the others fixed. And that's how we're going to update each variable, which respect to which each. So each parameter with respect to Itch, uh, we're optimizing and it's a block corner descent algorithm because we are going to do that for not just a single variable retrospect to all others, but with respect to a block of variables. In our case, the blocks are going to be each atom, each column, uh, Jay of, um, our Matrix d and uh, uh, the advantage is going to be that we'll get an update, we'll which does not require a learning rate. So what we'll actually do is that we'll consider updating only the JF column of our Dictionary Matrix deep and we actually find the best value of that column with respect to all other parameters.

Speaker 2:          02:08          So we'll actually find the, uh, uh, the optimum with respect to just this column, ignoring also the unit long constraint. And by doing this, then we don't have a learning. We just do a, a step that has us reach exactly, uh, the, the optimum retrospected, the jet column. And then what we do is that we just iterate and cycle over each column. So we'll do this for the first column, second column. So on once we reach the last column would go back to the first column. And we had to write like this until convergence. And we also have to introduce a projection step to make sure that the, uh, uh, columns of, uh, r Adams off unit norm. Okay. So let's see how we can do this. Uh, so let's say we are optimizing which respected the JF column and we keep all of the column, the other columns of Matrix d, uh, fixed.

Speaker 2:          02:57          So one way of finding the optimum, just find a fixed points that is take the gradient of the reconstruction error and set it to zero. So if we go back at our objective, if I want the Green Bridge suspected that g column of the reconstruction of the, sorry, of the Matrix d here at to take the, uh, they're there with respect to the reconstruction terms. So two would disappear and it would, so we have a two here which would cancel out, and then we'd have just parenthesis like this. And I have to multiply by, uh, the partial derivative of the reconstruction with respect to, uh, the elements in my JF column of Matrix Steam and a, so the Matrix, that gea column is going to be multiplied by Djf element of my representation. So all I have here is that I'll have a h of x, t j because that's the term that's being multiplied by Dgf, uh, the elements in the JF column of Matrix team. Okay. So that's what I have here actually.

Speaker 3:          04:01          Uh,

Speaker 2:          04:02          so this is exactly why that derived in the previous slide. So what I want to do is fine. What's the value for the GF column of d such that this is satisfied so that the, uh, gradient is equal to zero. And I'm going to assume all of that, all other columns are fixed. So, uh, first thing to notice is that this reconstruction is really just a, some of the columns, all columns of Matrix d multiplied by each associated element in, uh, the, uh, sparse coding representation. And so what I've done here is that I've separated this, some of overall columns into a sum over all columns different than j and then just a single term that, that that corresponds to. Uh, so the term associated with the JF column of Matrix team. Okay, I'm doing this because I want to separate out the column, the GF column here so I can put it back on the left and isolated and get an expression for it.

Speaker 2:          05:04          Okay. Now what I can do is that I can now distribute this, uh, into the parenthesis so I can just take this term and put it on the left. So what I would have is that if I multiply that side, I'd have a squared here and then I have h x t j right here. And then, uh, if I have this, then I can just take the sum over t of justice term, uh, which is a negative and put it on the left. So that's exactly what I've done here. I've got my summer over t of the Juke column of the Times h x t squared j. So that's this, this whole term here with the sun. And then I'm keeping whatever's left on the right of the right hand side of my equation. So I see that what I wrote here is exactly here.

Speaker 4:          06:01          Okay,

Speaker 2:          06:03          no, next, well I want to isolate with respect to this and now I notice that it does not depend on tea so I can actually put it outside of the sun and then I'd be left with the Jf Colin Times the sum of disarm, which does depend on teeth. And so I can then take that factor in divided on each side of the equation. So I'd, I'd be left with just the JF column. I have one over this, some over hxd squared, uh, j. So this is, uh, this is this thing here that, uh, is now on the right hand side. And then this here is just the same thing here. Okay. So now by doing this, I've been able to isolate for the JF column and a, and so that's a fixed point of my reconstruction error with respect only to the JF column of Matrix team.

Speaker 2:          06:57          And now the A, if we wanted to make sure that this is a minimum, we need to look at a second order conditions or make sure that the secondary Div at this point, we're actually at this vector. The action would need to be, uh, uh, say a positive, definite and a. So if we did that, then we'd actually find that it is indeed a minimum. So I won't do it here, but you can do it as an exercise. And so this gives me a analytical expression for obtaining a new value for my GF Adam. And a notice that it does not require learning rate like the projected grade in dissent algorithm that we've seen in the previous video. So that's one of the big advantage here. It's just more convenient not to have to specify a learning rate.

Speaker 2:          07:45          Now let's play a little bit with that expression that uh, so there's an analytical expression for the new value for the JF column a. So one thing that we can do is that we can, uh, take this term here, the scalar and put it inside the parenthesis. So now I have it here and also have it here. And uh, so it's been introduced here and multiplied here. And then the other thing I can do is that I can take the sum over tea and put it, uh, I guess distributed in the parenthesis. I have it right in front here, but also right in front here. So before this other, some over I instead. So I'm just switching the order in the sun for this term here. So if I, if I do this, then I get this term here and I get this other term here where now the sum is inside.

Speaker 2:          08:40          Now, uh, the advantage of doing this is that now we notice that I have a few terms which don't depend on matrix t. So first here, this term here, which is the sum of Dj value in my, uh, sparse codes for all the training examples. I may examples in my dataset that doesn't depend on d f h is a fixed, uh, here. Also, we don't see, uh, in the expression the value of d. It's just x multiplied by a DJF element of my, as far as codes. And the sum over all my dataset. And similarly here we have the cross product between the IMF element of the sparse code and the j element summed over all the data sets. So that also doesn't depend on the, so what I could do is precompute a matrix a which is just the, some of the other product between the sparks coding representations for all training examples and a, so the element Ij would just be, so the element igm matrix a would just be disturbed here.

Speaker 2:          09:44          And similarly the element JJ would be just the diagonal of a to the element on a diagonal matrix a which is be that term here as well as four B. If we did the other product of the input times the transpose a, so the other product of the input and it's far as representation. Well, if I just took the JFF column of that Matrix B, I would get disturbed. Okay. So now the advantage of doing this is that I can do this before I do any optimization of matrix t and keep that in memory and always use this computation. Uh, instead of actually computing it every time I update one Adam. Okay. So in terms of an algorithm that's going to be much more efficient because I can do just do it once a before I start optimizing with respect to the matrix. And so here the expression we have is just replacing with the different a and B terms.

Speaker 2:          10:45          And I'm doing another thing which is that, so, uh, if I want to write it, um, so this sum is over all eyes except Jay. Now if I take the column a Matrix, the column Djf column of Matrix a, I'm multiplying by Dee. I'm going to get, I'm going to get this sum here, but from one to a, I'm, I'm going to get the whole song which was like to ally. So I have to subtract from this term here, the case where i's equal to Jane. So, uh, because there's a negative here, I need to add this term. And so that's why I have this term here as well. Okay. So now I have an expression which is much simpler. Assuming I've computed all of these terms, which just corresponds to computing is some of our products between either, uh, the, uh, as far as coding representations with itself or the input with the sparkling representation.

Speaker 2:          11:41          And so now we're ready to look at, uh, what the code looks like. So, uh, here I would need to compute, uh, a and B as specified before with the, uh, you know, expression that we had on the previous slide and then one that once this is done, uh, as long as Dee is not converge for every iteration, what I do is that I cycle, I iterate over each column of the Matrix d so each Adam in my dictionary and then I perform my corner descent update. So that's the expression I had before. And then also have to project back on the, uh, in the space of, uh, uh, you did norm, uh, Adams. So I have to just take all my, my, my column, I just updated and we normalize it because I want my Adams in my dictionary to be normalized. And then I had to read like this, and this algorithm will actually convert to solution and notice that now this algorithm, unlike the projected grade in dissent, doesn't have a learning rate. So we don't have to specify it. So it's a more convenient algorithm to use. So like I said, this is known as a pluck cornered descent algorithm. It's a could say it's a projected Lockhorn dissent, a algorithm because of the projection we have here. Uh, and, uh, and its big advantage too to remember is that it does not require learning rate. So sometimes people would prefer using this, uh, for, for convenience. All right, so that's another alternative for updating the dictionary, uh, during a sparse coding.