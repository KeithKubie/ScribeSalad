WEBVTT
Kind: captions
Language: en

00:00:01.552 --> 00:00:02.760
BILLY RUTLEDGE: Hi, everyone.

00:00:02.760 --> 00:00:05.220
I'm Billy Rutledge from
the AOI team at Google.

00:00:05.220 --> 00:00:08.940
And we're here today at
CES in the NXP Pavilion

00:00:08.940 --> 00:00:11.880
to introduce our new
product-- the Edge TPU Dev

00:00:11.880 --> 00:00:18.250
Board that features our edge TPU
chip combined with the NXP iMX

00:00:18.250 --> 00:00:21.780
8 SoC as a kit for
developers to experiment

00:00:21.780 --> 00:00:24.920
with artificial intelligence
for the first time.

00:00:24.920 --> 00:00:27.810
So the board itself is
actually two pieces.

00:00:27.810 --> 00:00:30.930
It's the base board here,
which has all the connectors

00:00:30.930 --> 00:00:34.170
that most developers would
use to prototype a new product

00:00:34.170 --> 00:00:35.040
idea.

00:00:35.040 --> 00:00:38.080
And then the SoM module--

00:00:38.080 --> 00:00:42.510
SoM-- includes the
CPU, GPU, and TPU chip,

00:00:42.510 --> 00:00:45.000
as well as the memory
and Wi-Fi and Bluetooth.

00:00:45.000 --> 00:00:47.440
And it actually snaps
into the baseboard

00:00:47.440 --> 00:00:49.320
using high-density connectors.

00:00:49.320 --> 00:00:51.540
And so that allows
you to experiment

00:00:51.540 --> 00:00:55.990
with the actual hardware
in a development setting,

00:00:55.990 --> 00:00:59.430
but be able to buy the SoM
part for production line

00:00:59.430 --> 00:01:01.770
when you're ready to
take your smart speaker,

00:01:01.770 --> 00:01:07.780
smart dishwasher, smart TV to
a scalable production plan.

00:01:07.780 --> 00:01:10.320
So today we're showcasing
a few different demos

00:01:10.320 --> 00:01:14.030
of how you might experiment
with this type of technology,

00:01:14.030 --> 00:01:18.980
and hoping to inspire people to
explore using AI on the edge.

00:01:18.980 --> 00:01:20.730
PETER MALKIN: Hi, my
name is Peter Malkin.

00:01:20.730 --> 00:01:21.600
I work for Google.

00:01:21.600 --> 00:01:23.700
I'm a software tech
lead for AOI projects.

00:01:23.700 --> 00:01:26.520
And today we're showing you
a demo of facial detection

00:01:26.520 --> 00:01:29.430
that runs on Edge TPU.

00:01:29.430 --> 00:01:34.320
The key point about Edge TPU
is the privacy and security.

00:01:34.320 --> 00:01:36.330
From now on, your
pixels do not need

00:01:36.330 --> 00:01:37.800
to travel to a data center.

00:01:37.800 --> 00:01:40.500
You do not need to contribute
your data to any company.

00:01:40.500 --> 00:01:43.740
You can run all your machine
learning inference locally

00:01:43.740 --> 00:01:44.976
on the chipset.

00:01:44.976 --> 00:01:46.350
In this case in
particular, we've

00:01:46.350 --> 00:01:49.620
tried a network that can
recognize human face.

00:01:49.620 --> 00:01:52.080
And it's running
locally on device

00:01:52.080 --> 00:01:55.324
on a small embedded
system that runs Linux.

00:01:55.324 --> 00:01:57.240
JUNE TATE-GANS: Hi, my
name is June Tate-gans.

00:01:57.240 --> 00:01:59.430
I'm actually one of
the software engineers

00:01:59.430 --> 00:02:01.620
working on AOI projects.

00:02:01.620 --> 00:02:06.120
One of our demos here at CES
is actually a teachable machine

00:02:06.120 --> 00:02:09.419
where we actually
use local inference

00:02:09.419 --> 00:02:12.850
to train a model directly on
the device with no network

00:02:12.850 --> 00:02:13.874
connectivity.

00:02:13.874 --> 00:02:15.540
We call this our
teachable machine demo,

00:02:15.540 --> 00:02:17.670
and it's right here.

00:02:17.670 --> 00:02:20.352
And essentially it has a
camera pointing up at the sky.

00:02:20.352 --> 00:02:21.810
Now the first thing
I have to do is

00:02:21.810 --> 00:02:23.970
train it to teach it about
what the background is

00:02:23.970 --> 00:02:25.761
so it can differentiate
between the objects

00:02:25.761 --> 00:02:27.967
I'm about to show it and
what the background is.

00:02:27.967 --> 00:02:30.300
And the first thing I do is
I press one of these buttons

00:02:30.300 --> 00:02:32.650
to actually tell it
what it's looking at.

00:02:32.650 --> 00:02:35.130
So now it knows what
the background is.

00:02:35.130 --> 00:02:36.960
I can now train it on an object.

00:02:36.960 --> 00:02:39.450
In this particular case, I'm
gonna use this ice cream.

00:02:39.450 --> 00:02:42.920
So hold the ice cream
over, press the button.

00:02:42.920 --> 00:02:46.830
It now can differentiate between
background and ice cream.

00:02:46.830 --> 00:02:49.080
And you know it's machine
learning and doing inference

00:02:49.080 --> 00:02:51.140
because I can show
it a different color

00:02:51.140 --> 00:02:54.280
and get the same
result. And this

00:02:54.280 --> 00:02:59.057
can be extended to other objects
as well, such as this hot dog.

00:03:02.880 --> 00:03:07.777
So hot dog, ice
cream, hot dog again.

00:03:07.777 --> 00:03:09.110
And the same thing with a donut.

00:03:13.990 --> 00:03:19.172
So donut, hot dog,
and ice cream.

00:03:19.172 --> 00:03:20.130
LEONID LOBACHEV: Hello.

00:03:20.130 --> 00:03:20.930
My name is Leonid.

00:03:20.930 --> 00:03:23.640
I work for Google
for AOI project.

00:03:23.640 --> 00:03:26.580
And I will talk about our
[INAUDIBLE] demo here.

00:03:26.580 --> 00:03:30.280
So if you notice that under
each of the bigger demos,

00:03:30.280 --> 00:03:34.320
we have a small display
underneath with a depth camera.

00:03:34.320 --> 00:03:36.520
And it shows the time.

00:03:36.520 --> 00:03:38.220
And that's important
characteristic

00:03:38.220 --> 00:03:41.670
because it tracks how much
time people spend looking

00:03:41.670 --> 00:03:44.600
at the other bigger demos.

00:03:44.600 --> 00:03:48.147
And you can notice that we
display the bounding boxes

00:03:48.147 --> 00:03:49.230
around the people's faces.

00:03:49.230 --> 00:03:50.880
So here is my face.

00:03:50.880 --> 00:03:53.670
And there is a green
box showing that I'm

00:03:53.670 --> 00:03:56.480
looking toward the demo stand.

00:03:56.480 --> 00:03:58.410
And if I turn away,
like right now you

00:03:58.410 --> 00:04:01.810
can probably see the red box.

00:04:01.810 --> 00:04:05.460
I'm not sure myself, but
it should have been red.

00:04:05.460 --> 00:04:09.600
And this has been run totally
on the development board.

00:04:09.600 --> 00:04:13.010
You can notice that,
behind the display,

00:04:13.010 --> 00:04:16.649
we have the same words like
[INAUDIBLE] demos here.

00:04:16.649 --> 00:04:19.940
So there is no internet or
cloud connection required.

00:04:19.940 --> 00:04:23.660
And that's a good application
because we are ourselves

00:04:23.660 --> 00:04:29.310
interest at home many people are
looking at other bigger demos.

00:04:29.310 --> 00:04:31.770
And you can notice
like, in the middle,

00:04:31.770 --> 00:04:34.380
we have something like 4
and 1/2 hours right now.

00:04:34.380 --> 00:04:37.230
And in the corner stand there
is like slightly more than three

00:04:37.230 --> 00:04:38.020
hours.

00:04:38.020 --> 00:04:41.100
It's quite explainable,
but still interesting,

00:04:41.100 --> 00:04:42.790
statistics together.

00:04:42.790 --> 00:04:43.500
BILLY RUTLEDGE: So
we're just beginning

00:04:43.500 --> 00:04:45.416
to scratch the surface
of what's possible with

00:04:45.416 --> 00:04:47.310
artificial intelligence today.

00:04:47.310 --> 00:04:50.020
And we're excited to offer
the Google Edge TPU Dev

00:04:50.020 --> 00:04:54.270
Kit for the world to experiment
with on-device AI specifically

00:04:54.270 --> 00:04:58.800
to explore the capabilities
with high performance on device,

00:04:58.800 --> 00:05:01.740
security with having all the
data on the board itself,

00:05:01.740 --> 00:05:04.230
and performance by being able
to process everything locally

00:05:04.230 --> 00:05:05.459
on the machine.

00:05:05.459 --> 00:05:07.500
We think it will open up
a world of opportunities

00:05:07.500 --> 00:05:08.839
for new product development.

00:05:08.839 --> 00:05:11.130
We're excited to see what
you might build with it next.

00:05:11.130 --> 00:05:14.780
[MUSIC PLAYING]

