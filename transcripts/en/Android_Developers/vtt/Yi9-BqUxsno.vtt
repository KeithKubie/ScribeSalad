WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.460
[MUSIC PLAYING]

00:00:06.864 --> 00:00:08.280
MATHIEU CHARTIER:
Hello, everyone.

00:00:08.280 --> 00:00:11.520
I'm Mathieu and this
is my colleague Calin.

00:00:11.520 --> 00:00:12.960
And today, we're
going to be going

00:00:12.960 --> 00:00:16.761
over what's new with the Android
Runtime on Android, also known

00:00:16.761 --> 00:00:17.260
as ART.

00:00:20.360 --> 00:00:22.720
So what is ART?

00:00:22.720 --> 00:00:26.770
Well, ART is the software layer
in between the applications

00:00:26.770 --> 00:00:28.730
and the operating system.

00:00:28.730 --> 00:00:32.530
It provides mechanisms for
executing Java language

00:00:32.530 --> 00:00:35.410
and call in applications.

00:00:35.410 --> 00:00:39.200
To accomplish this,
ART does two things.

00:00:39.200 --> 00:00:42.700
It executes Dex files, the
intermediate representation

00:00:42.700 --> 00:00:46.390
of Android applications, through
a hybrid model consisting

00:00:46.390 --> 00:00:50.770
of an interpretation,
just-in-time compilation,

00:00:50.770 --> 00:00:55.570
and profile-based
ahead-of-time compilation.

00:00:55.570 --> 00:00:58.690
ART also does memory management
for Android applications

00:00:58.690 --> 00:01:05.500
through an automatic reclamation
through a garbage collector.

00:01:05.500 --> 00:01:07.900
This is a concurrent
compacting garbage collector,

00:01:07.900 --> 00:01:10.306
so that there's less jank
for your applications.

00:01:13.640 --> 00:01:15.430
Now let's look at
how ART has changed

00:01:15.430 --> 00:01:18.410
over the last few years.

00:01:18.410 --> 00:01:21.690
Over the years, there have
been many improvements to ART.

00:01:21.690 --> 00:01:25.000
In Nougat, we introduced a
profile-guided compilation

00:01:25.000 --> 00:01:29.020
to improve application startup
time, reduce memory usage,

00:01:29.020 --> 00:01:32.940
and reduce storage requirements.

00:01:32.940 --> 00:01:38.040
Also in Nougat, we added a JIT,
much like Dalvik used to have.

00:01:38.040 --> 00:01:40.980
This was done to remove the
need for optimizing apps.

00:01:40.980 --> 00:01:43.950
That was kind of a big problem
during Android system updates.

00:01:47.110 --> 00:01:50.200
And in Oreo, we added a new
concurrent compacting garbage

00:01:50.200 --> 00:01:54.610
collector to reduce RAM
requirements, have less jank,

00:01:54.610 --> 00:01:57.850
as well as accelerate
allocations.

00:01:57.850 --> 00:02:00.100
As you can see
here on the slide,

00:02:00.100 --> 00:02:03.460
this new garbage collector
enabled a new dump pointer

00:02:03.460 --> 00:02:06.940
allocator that is
17 times faster

00:02:06.940 --> 00:02:09.199
than the allocator in
Dalvik or in KitKat.

00:02:12.190 --> 00:02:14.700
Now we talked about what
happened in the past,

00:02:14.700 --> 00:02:17.740
but what's new in Android P?

00:02:17.740 --> 00:02:20.740
First of all, there are
new compiler optimizations

00:02:20.740 --> 00:02:23.680
that help accelerate the
performance of Kotlin code

00:02:23.680 --> 00:02:25.900
in Android.

00:02:25.900 --> 00:02:28.420
This is especially
important since Kotlin

00:02:28.420 --> 00:02:30.940
is a first-class
programming language

00:02:30.940 --> 00:02:34.090
for Android development.

00:02:34.090 --> 00:02:37.980
Next up, we have memory
and storage optimizations

00:02:37.980 --> 00:02:42.190
to help entry-level devices,
such as Android Go devices.

00:02:42.190 --> 00:02:44.290
This is important to help
improve the performance

00:02:44.290 --> 00:02:47.720
for the next billion users.

00:02:47.720 --> 00:02:51.460
And finally, we
have cloud profiles.

00:02:51.460 --> 00:02:54.550
Device collected profiles
from the just-in-time compiler

00:02:54.550 --> 00:02:57.760
are uploaded and
aggregated in the cloud

00:02:57.760 --> 00:03:01.030
to enable faster performance
directly after installation

00:03:01.030 --> 00:03:02.386
of applications.

00:03:05.790 --> 00:03:09.060
So let's start with Kotlin.

00:03:09.060 --> 00:03:10.770
Last year, we
announced Kotlin as

00:03:10.770 --> 00:03:12.480
a first-class
officially-supported

00:03:12.480 --> 00:03:14.500
programming language
for Android development.

00:03:14.500 --> 00:03:18.030
And then, we began to
investigate the performance.

00:03:18.030 --> 00:03:19.800
Why Kotlin, you might ask?

00:03:19.800 --> 00:03:22.960
Well, Kotlin is a safe,
expressive, concise,

00:03:22.960 --> 00:03:26.220
object-oriented language
that is designed

00:03:26.220 --> 00:03:28.300
to be interoperable
with Java language.

00:03:30.840 --> 00:03:34.440
The reason ART focuses
on optimizing Kotlin

00:03:34.440 --> 00:03:37.620
so that the developers can
leverage all of these language

00:03:37.620 --> 00:03:41.220
features while still having
fast and jank-free applications.

00:03:44.700 --> 00:03:46.860
Let's see how Kotlin
optimizations are normally

00:03:46.860 --> 00:03:50.250
performed inside of
the Android Runtime.

00:03:50.250 --> 00:03:52.050
Usually, optimizations
are performed

00:03:52.050 --> 00:03:53.754
in an investigative manner.

00:03:53.754 --> 00:03:55.170
And there's an
order of preference

00:03:55.170 --> 00:03:57.900
for fixing performance issues
so that the most amount

00:03:57.900 --> 00:04:00.150
of Kotlin applications
can actually

00:04:00.150 --> 00:04:03.330
benefit from the optimization.

00:04:03.330 --> 00:04:06.660
The preferred option is
fixing a performance issue

00:04:06.660 --> 00:04:08.250
inside of kotlinc.

00:04:08.250 --> 00:04:12.990
And kotlinc is the compiler
developed by JetBrains Google

00:04:12.990 --> 00:04:15.360
and JetBrains of course,
work closely together

00:04:15.360 --> 00:04:19.529
on all kinds of optimizations
and fixes for issues.

00:04:19.529 --> 00:04:21.360
If we fix a
performance issue here,

00:04:21.360 --> 00:04:23.310
it'll be able to be
deployed to the most

00:04:23.310 --> 00:04:26.340
amount of Kotlin applications.

00:04:26.340 --> 00:04:28.990
Alternatively, if
that doesn't work,

00:04:28.990 --> 00:04:30.930
then we consider fixing
the performance issue

00:04:30.930 --> 00:04:34.050
inside of bytecode converters.

00:04:34.050 --> 00:04:35.640
Fixing in the bytecode
converter will

00:04:35.640 --> 00:04:38.400
enable existing versions
of the Android platform

00:04:38.400 --> 00:04:42.090
to get the performance fix.

00:04:42.090 --> 00:04:45.140
And if that option doesn't
work, the last option

00:04:45.140 --> 00:04:47.960
is to fix the performance issue
in the Android Runtime, also

00:04:47.960 --> 00:04:50.040
known as ART.

00:04:50.040 --> 00:04:53.330
So the reason that we might not
want to fix in ART right away

00:04:53.330 --> 00:04:57.000
is because ART is updated as
part of the Android platform.

00:04:57.000 --> 00:05:00.987
So that means that not all
devices will get the fix.

00:05:00.987 --> 00:05:02.195
Now let's look at an example.

00:05:05.510 --> 00:05:07.250
One example of a
Kotlin optimization

00:05:07.250 --> 00:05:09.860
is the parameter null-check.

00:05:09.860 --> 00:05:12.530
As you can see here,
this is a simple method

00:05:12.530 --> 00:05:15.330
that just returns the
length of a string,

00:05:15.330 --> 00:05:18.830
but the string is nullable.

00:05:18.830 --> 00:05:22.370
So what this means is that the
compiler inserts a null-check

00:05:22.370 --> 00:05:26.150
into the function bytecode
to actually verify

00:05:26.150 --> 00:05:27.890
that the string is
not null, and throw

00:05:27.890 --> 00:05:30.650
the corresponding
exception if required.

00:05:30.650 --> 00:05:32.540
Implemented in the
bytecode, the first step

00:05:32.540 --> 00:05:34.970
is loading the name
of the parameter

00:05:34.970 --> 00:05:37.760
and then invoking
a separate function

00:05:37.760 --> 00:05:39.880
to do the actual null check.

00:05:39.880 --> 00:05:41.870
So there is some
extra overhead here

00:05:41.870 --> 00:05:45.110
as you might see, because the
invocation in the common case,

00:05:45.110 --> 00:05:47.330
you do the extra indication
that goes to the function

00:05:47.330 --> 00:05:48.810
to do the null check.

00:05:48.810 --> 00:05:51.040
And this function in
turns, if required,

00:05:51.040 --> 00:05:54.500
calls the third function to
throw the actual parameter is

00:05:54.500 --> 00:05:57.240
null exception.

00:05:57.240 --> 00:05:58.650
Checks such as
these are commonly

00:05:58.650 --> 00:06:01.000
required for Java
language and Kotlin

00:06:01.000 --> 00:06:03.510
interoperability because
Java language does not

00:06:03.510 --> 00:06:06.790
have a non-nullable property.

00:06:06.790 --> 00:06:10.080
Now let's see how we
can optimize this.

00:06:10.080 --> 00:06:12.660
If we look at the bytecode, one
of the first things we can do

00:06:12.660 --> 00:06:15.180
is actually inline the method
that does the null-check

00:06:15.180 --> 00:06:17.650
into the caller.

00:06:17.650 --> 00:06:20.880
After inlining, this
improves performance

00:06:20.880 --> 00:06:22.850
because there is
one less invocation.

00:06:22.850 --> 00:06:24.600
And from here, you can
see one other thing

00:06:24.600 --> 00:06:27.810
we can do is that the
name of the parameter

00:06:27.810 --> 00:06:31.530
is not actually required
unless the argument is null.

00:06:31.530 --> 00:06:34.110
So from here, we
can do code syncing

00:06:34.110 --> 00:06:39.070
to move loading of the parameter
name inside of the conditional.

00:06:39.070 --> 00:06:40.800
So overall, these
two optimizations

00:06:40.800 --> 00:06:43.410
help performance by removing
one invocation and one

00:06:43.410 --> 00:06:45.300
of the loadings of
a string literal.

00:06:49.220 --> 00:06:52.230
Apart from this optimization,
we also track Kotlin performance

00:06:52.230 --> 00:06:54.480
on various benchmarks.

00:06:54.480 --> 00:06:57.660
Other improvements here include
improved auto vectorization

00:06:57.660 --> 00:07:01.950
of loops, also intrinsic
methods that are specifically

00:07:01.950 --> 00:07:05.166
tailored for Kotlin code to
help improve performance there.

00:07:05.166 --> 00:07:06.540
So the ART team
is always working

00:07:06.540 --> 00:07:08.291
on improving this performance.

00:07:11.495 --> 00:07:13.870
Now that we've done Kotlin,
what about memory and storage

00:07:13.870 --> 00:07:16.120
improvements?

00:07:16.120 --> 00:07:18.880
So since ART is responsible
for Java language and Kotlin

00:07:18.880 --> 00:07:20.709
applications, it's
also pretty important

00:07:20.709 --> 00:07:22.750
to just kind of make sure
that the programs don't

00:07:22.750 --> 00:07:26.470
use too much memory and take
too much space on a device.

00:07:26.470 --> 00:07:28.570
There have been several
improvements focusing

00:07:28.570 --> 00:07:32.140
on this area, including reducing
the amount of space and memory

00:07:32.140 --> 00:07:33.580
usage required by Dex files.

00:07:37.284 --> 00:07:42.010
Now why are RAM and storage
optimizations important?

00:07:42.010 --> 00:07:44.010
Well, recall that last
year, we introduced

00:07:44.010 --> 00:07:46.230
a new initiative
called Android Go,

00:07:46.230 --> 00:07:48.480
aiming at running the
latest versions of Android

00:07:48.480 --> 00:07:51.720
on entry-level devices.

00:07:51.720 --> 00:07:54.840
Since these devices typically
have 1 gigabyte or less

00:07:54.840 --> 00:07:58.550
of RAM and 8 gigabytes
or less of storage,

00:07:58.550 --> 00:08:01.860
it's kind of important to
focus on optimizing these areas

00:08:01.860 --> 00:08:04.560
so that the users can
run enough applications

00:08:04.560 --> 00:08:06.540
and install as many
applications as they--

00:08:06.540 --> 00:08:11.010
or more applications than they
would otherwise be able to.

00:08:11.010 --> 00:08:13.310
Now this isn't just
for Android Go.

00:08:13.310 --> 00:08:16.220
Premium devices also benefit
from optimizations in these two

00:08:16.220 --> 00:08:18.966
areas, but since they
have more resources,

00:08:18.966 --> 00:08:20.340
normally it's to
a lesser degree.

00:08:23.480 --> 00:08:27.280
Anyways, before we talk about
RAM and storage optimizations,

00:08:27.280 --> 00:08:29.932
let's do a little bit of a
review about how applications

00:08:29.932 --> 00:08:31.140
work on your Android devices.

00:08:33.720 --> 00:08:36.870
An application normally comes
in an Application Package Kit,

00:08:36.870 --> 00:08:39.330
also known as an APK.

00:08:39.330 --> 00:08:40.890
Inside of the APK,
there are usually

00:08:40.890 --> 00:08:44.970
one or more Dalvik Executable
Files, also known as Dex files,

00:08:44.970 --> 00:08:48.000
that contain instructions that
ART uses to either interpret

00:08:48.000 --> 00:08:50.970
or compile your application.

00:08:50.970 --> 00:08:53.790
Since Dex files are required
to be quickly accessed

00:08:53.790 --> 00:08:57.570
during execution, they are
mapped directly into memory

00:08:57.570 --> 00:09:02.370
during application startup so
that ART can have quick access.

00:09:02.370 --> 00:09:04.950
This means that there is a
startup cost as well as a RAM

00:09:04.950 --> 00:09:09.240
cost proportional to the
size of the Dex file.

00:09:09.240 --> 00:09:12.872
Finally, Dex files are usually
stored twice on the device.

00:09:12.872 --> 00:09:14.580
The first place they
are stored is inside

00:09:14.580 --> 00:09:16.364
of the application package kit.

00:09:16.364 --> 00:09:17.780
And then the second
place they are

00:09:17.780 --> 00:09:20.910
stored is in an extracted form,
so that ART can have faster

00:09:20.910 --> 00:09:23.070
access during application
startup without needing

00:09:23.070 --> 00:09:26.980
to extract from the
ZIP file each time.

00:09:26.980 --> 00:09:32.460
Now let's take a closer look
at the contents of Dex files.

00:09:32.460 --> 00:09:34.410
Within a Dex file, there
are several sections

00:09:34.410 --> 00:09:35.850
containing different
types of data

00:09:35.850 --> 00:09:38.710
related to the applications.

00:09:38.710 --> 00:09:42.750
But where is the space
going in the Dex file?

00:09:42.750 --> 00:09:44.400
One way to do this
is you can kind of

00:09:44.400 --> 00:09:46.900
calculate where the space
is going for each Dex file

00:09:46.900 --> 00:09:48.990
and average out the results.

00:09:48.990 --> 00:09:52.290
This chart here is for the top
99 most downloaded applications

00:09:52.290 --> 00:09:53.580
in the Play Store.

00:09:53.580 --> 00:09:56.520
And you can see that the
largest section is the code item

00:09:56.520 --> 00:10:01.310
section containing the Dex
instructions used by ART.

00:10:01.310 --> 00:10:04.660
The next largest section
is the StringData section.

00:10:04.660 --> 00:10:06.550
And this section contains
the string literals

00:10:06.550 --> 00:10:12.220
loaded from code, method names,
class names, and field names.

00:10:12.220 --> 00:10:16.760
Combined, these two sections
are around 64% of the Dex file,

00:10:16.760 --> 00:10:20.524
so they're pretty important
areas to optimize.

00:10:20.524 --> 00:10:22.190
Let's see if there's
a way we can reduce

00:10:22.190 --> 00:10:23.314
the size of these sections.

00:10:25.550 --> 00:10:30.800
One new feature introduced in
Android P is called CompactDex.

00:10:30.800 --> 00:10:33.350
The goal of
CompactDex is simple--

00:10:33.350 --> 00:10:36.950
reduce the size of Dex files
to get memory and store savings

00:10:36.950 --> 00:10:39.820
on the device.

00:10:39.820 --> 00:10:42.220
From the previous slide,
we saw that some sections

00:10:42.220 --> 00:10:44.002
are larger than others.

00:10:44.002 --> 00:10:46.210
So it's important to just
focus on the large sections

00:10:46.210 --> 00:10:48.790
to get the most savings.

00:10:48.790 --> 00:10:52.450
For the code items, they
are more often deduplicated.

00:10:52.450 --> 00:10:54.350
And they also have
their headers shrunk

00:10:54.350 --> 00:10:56.830
to save a space for each
method, essentially, inside

00:10:56.830 --> 00:10:59.740
of the application.

00:10:59.740 --> 00:11:02.440
And another thing here worth
noting about the string data

00:11:02.440 --> 00:11:04.240
is that large
applications frequently

00:11:04.240 --> 00:11:07.750
shift multiple Dex files in
their APK because of Dex format

00:11:07.750 --> 00:11:09.280
limitations.

00:11:09.280 --> 00:11:11.740
Specifically, the
64k method limit

00:11:11.740 --> 00:11:14.650
means that you can only have
64,000 method references

00:11:14.650 --> 00:11:16.390
in a single Dex
file before needing

00:11:16.390 --> 00:11:19.880
to add another one
to your application.

00:11:19.880 --> 00:11:22.060
And every time you add
another Dex file, this causes

00:11:22.060 --> 00:11:24.790
duplication, specifically
of string data, that could

00:11:24.790 --> 00:11:27.490
otherwise be stored only once.

00:11:27.490 --> 00:11:30.880
CompactDex shrinks this
by providing deduplication

00:11:30.880 --> 00:11:34.090
across the Dex files in the APK.

00:11:34.090 --> 00:11:36.540
Now let's go to the
generation process.

00:11:39.960 --> 00:11:41.720
First, let's look
at how Dex files

00:11:41.720 --> 00:11:44.420
are processed on Android Oreo.

00:11:44.420 --> 00:11:48.160
The first step run by Dex AOT--
the Ahead-Of-Time compiler--

00:11:48.160 --> 00:11:51.020
is that the Dex files are
extracted from the APK

00:11:51.020 --> 00:11:53.700
and stored in a VDex container.

00:11:53.700 --> 00:11:55.950
The reason they are extracted,
as I mentioned earlier,

00:11:55.950 --> 00:11:57.770
is so that they can be
loaded more efficiently

00:11:57.770 --> 00:11:58.970
during application startup.

00:12:01.860 --> 00:12:04.620
One other thing here worth
noting is the profile.

00:12:04.620 --> 00:12:07.050
So the profile, as
introduced in Nougat,

00:12:07.050 --> 00:12:11.280
is essentially data about the
application execution including

00:12:11.280 --> 00:12:13.950
which methods are
executed during startup,

00:12:13.950 --> 00:12:16.820
what methods are hot-- so
compiled by the JIT compiler--

00:12:16.820 --> 00:12:19.470
and what classes are loaded.

00:12:19.470 --> 00:12:22.050
On Oreo, we are already
optimizing the Dex files

00:12:22.050 --> 00:12:26.850
stored in the VDex container by
applying layout optimizations.

00:12:26.850 --> 00:12:28.570
And also we were
deciding which methods

00:12:28.570 --> 00:12:31.480
to compile based on what
methods are hot in the Dex file.

00:12:35.580 --> 00:12:41.240
Now let's look at Dex processing
on Android P. In Android P,

00:12:41.240 --> 00:12:43.850
the ahead-of-time compiler
now converts the Dex files

00:12:43.850 --> 00:12:46.910
to a more efficient
CompactDex representation

00:12:46.910 --> 00:12:49.560
inside of the container.

00:12:49.560 --> 00:12:51.440
One new addition here
is the introduction

00:12:51.440 --> 00:12:53.270
of a shared data section.

00:12:53.270 --> 00:12:56.474
Specifically, data that is
present in multiple Dex files

00:12:56.474 --> 00:12:57.890
will be in the
shared data section

00:12:57.890 --> 00:13:00.530
only once, so it
kind of deduplicates

00:13:00.530 --> 00:13:02.260
data that's commonly shared.

00:13:02.260 --> 00:13:03.770
And one of the most
commonly shared

00:13:03.770 --> 00:13:05.850
things here is the StringData.

00:13:05.850 --> 00:13:07.850
So this is how we reduce
the large StringData

00:13:07.850 --> 00:13:10.320
section that we saw earlier.

00:13:10.320 --> 00:13:14.032
Finally, since the conversion
is automatically done on-device,

00:13:14.032 --> 00:13:15.740
this means that all
existing applications

00:13:15.740 --> 00:13:17.780
can get the benefits
of CompactDex

00:13:17.780 --> 00:13:19.835
without needing to
recompile their APKs.

00:13:23.590 --> 00:13:25.870
OK, so let's look at one
example of how we actually

00:13:25.870 --> 00:13:27.650
shrink the Dex code items.

00:13:27.650 --> 00:13:30.050
Apart from the
instructions, each code item

00:13:30.050 --> 00:13:32.202
has a 16-byte header.

00:13:32.202 --> 00:13:33.910
And then most of the
values in the header

00:13:33.910 --> 00:13:36.620
are usually small values.

00:13:36.620 --> 00:13:39.820
So what we do here is we
shrink the fields in the header

00:13:39.820 --> 00:13:41.226
to be 4 bytes each.

00:13:41.226 --> 00:13:42.850
And then we have an
optional pre-header

00:13:42.850 --> 00:13:43.974
to extend them as required.

00:13:47.100 --> 00:13:49.240
The pre-header is 0 bytes
in most of the cases,

00:13:49.240 --> 00:13:53.550
but can be up to 12
bytes in the worst case.

00:13:53.550 --> 00:13:55.710
So other than the
pre-header, we also

00:13:55.710 --> 00:13:57.930
shrink the instruction count.

00:13:57.930 --> 00:14:00.820
Since the average method is
not going to be that large,

00:14:00.820 --> 00:14:03.820
we shrink this down to 11
bytes instead of 32 bytes,

00:14:03.820 --> 00:14:06.060
and we use the 5
remaining bytes for flags

00:14:06.060 --> 00:14:08.700
that are ART specific.

00:14:08.700 --> 00:14:11.730
Finally, we moved
the debug information

00:14:11.730 --> 00:14:13.950
into a separate
space-efficient table

00:14:13.950 --> 00:14:16.410
to help enable more
deduplication of the code

00:14:16.410 --> 00:14:17.940
items.

00:14:17.940 --> 00:14:22.680
Overall, this optimization saves
around 12 bytes per code item

00:14:22.680 --> 00:14:23.670
in the CompactDex file.

00:14:27.680 --> 00:14:31.620
And here are the results for
the top 99 most downloaded APKs.

00:14:31.620 --> 00:14:35.250
So the average space required
by the Dex files on a device

00:14:35.250 --> 00:14:38.490
is around 11.6% smaller.

00:14:38.490 --> 00:14:40.350
And then other than
the storage savings,

00:14:40.350 --> 00:14:43.110
you also get memory savings
because the Dex files

00:14:43.110 --> 00:14:45.847
are resident in memory
during application usage--

00:14:45.847 --> 00:14:47.430
at least partially
resident in memory.

00:14:50.060 --> 00:14:51.360
And one more thing here.

00:14:51.360 --> 00:14:55.380
Let's go over the layout
optimizations a little bit.

00:14:55.380 --> 00:14:58.970
So even though we had introduced
the JIT profiles in Android N,

00:14:58.970 --> 00:15:02.060
we did not have any layout
optimizations back then.

00:15:02.060 --> 00:15:05.120
So what this means is the Dex
is kind of randomly ordered

00:15:05.120 --> 00:15:07.250
and not disregarding
the usage pattern.

00:15:09.810 --> 00:15:12.980
In Android O, we added this
type of layout optimization

00:15:12.980 --> 00:15:15.570
that groups the methods used
during application startup

00:15:15.570 --> 00:15:17.842
together and the
methods that are hot--

00:15:17.842 --> 00:15:19.800
so that means their code
is frequently accessed

00:15:19.800 --> 00:15:22.554
during execution-- together.

00:15:22.554 --> 00:15:23.970
This seems like a
pretty good ad--

00:15:23.970 --> 00:15:26.200
a pretty big win so far.

00:15:26.200 --> 00:15:31.170
Well, let's see what we did
for Android P. In Android P,

00:15:31.170 --> 00:15:33.720
we have more flexible
profile information,

00:15:33.720 --> 00:15:35.970
which enables us to put the
methods that are used only

00:15:35.970 --> 00:15:37.927
during startup together.

00:15:37.927 --> 00:15:39.510
This helps reduce
the amount of memory

00:15:39.510 --> 00:15:42.120
used because the application
or the operating system

00:15:42.120 --> 00:15:46.284
can remove those pages
from memory after startup.

00:15:46.284 --> 00:15:47.700
We also put the
hot code together,

00:15:47.700 --> 00:15:51.271
since it's frequently
accessed during execution.

00:15:51.271 --> 00:15:53.520
And finally, we put the code
that's just never touched

00:15:53.520 --> 00:15:55.170
at all during
execution at the end,

00:15:55.170 --> 00:15:59.070
so that it's not loaded
into memory unless required.

00:15:59.070 --> 00:16:01.920
And the reason that these layout
optimizations are important

00:16:01.920 --> 00:16:03.900
is because they improve
locality and reduce

00:16:03.900 --> 00:16:05.940
how many parts of the
Dex file are actually

00:16:05.940 --> 00:16:10.290
loaded into memory during
application usage and startup.

00:16:10.290 --> 00:16:11.910
So if you improve
the locality here,

00:16:11.910 --> 00:16:17.820
you can get startup benefits
and reduction in memory usage.

00:16:17.820 --> 00:16:20.005
And now to Calin
for cloud profiles.

00:16:23.288 --> 00:16:25.280
[APPLAUSE]

00:16:28.064 --> 00:16:29.480
CALIN JURAVLE:
Thank you, Mathieu.

00:16:29.480 --> 00:16:30.690
Hello, everyone.

00:16:30.690 --> 00:16:31.880
My name is Calin.

00:16:31.880 --> 00:16:33.710
And I'm here today
to present to you

00:16:33.710 --> 00:16:37.490
how we plan to improve and
scale up the Android Runtime

00:16:37.490 --> 00:16:40.200
profiling infrastructure.

00:16:40.200 --> 00:16:41.810
However, before we
start, profiling

00:16:41.810 --> 00:16:43.980
is a rather overloaded term.

00:16:43.980 --> 00:16:47.270
When we speak about profiling
in today's presentation,

00:16:47.270 --> 00:16:49.160
we're going to refer
to the metadata

00:16:49.160 --> 00:16:52.100
that Android Runtime captures
about the application

00:16:52.100 --> 00:16:54.190
execution, metadata
that is going

00:16:54.190 --> 00:16:58.860
to be feed into a profile-guided
optimization process.

00:16:58.860 --> 00:17:02.750
We're going to see how we extend
the on-device capabilities

00:17:02.750 --> 00:17:05.569
in order to drive performance
right at install time.

00:17:11.640 --> 00:17:15.760
Before we jump into what is
new and how actually it works,

00:17:15.760 --> 00:17:18.060
let me briefly remind
you how Android uses

00:17:18.060 --> 00:17:20.460
profile-guided optimizations.

00:17:20.460 --> 00:17:22.960
This is an efficient technique
that we introduced an Android

00:17:22.960 --> 00:17:26.619
Nougat as part of a
hybrid execution model.

00:17:26.619 --> 00:17:29.110
Hybrid means that the
code being executed

00:17:29.110 --> 00:17:31.230
can be in three
different optimization

00:17:31.230 --> 00:17:33.840
states at the same time.

00:17:33.840 --> 00:17:35.940
The primary goal
of this technique

00:17:35.940 --> 00:17:39.840
is to improve all key metrics
of the application performance.

00:17:39.840 --> 00:17:43.080
We're talking about faster
application startup time,

00:17:43.080 --> 00:17:46.890
reduced memory footprint,
a better user experience

00:17:46.890 --> 00:17:50.160
by providing less
jank during usage,

00:17:50.160 --> 00:17:53.340
less disk space used by
the compiler artifacts--

00:17:53.340 --> 00:17:56.190
which means more disk
space for our users--

00:17:56.190 --> 00:17:58.890
and nonetheless, an
increased battery life,

00:17:58.890 --> 00:18:00.870
because we do
heavy optimizations

00:18:00.870 --> 00:18:03.430
when the device is not used
rather than at the use time.

00:18:06.420 --> 00:18:08.220
How does this work?

00:18:08.220 --> 00:18:11.960
It all starts when the Play
Store installs the application.

00:18:11.960 --> 00:18:15.350
But first, we do very,
very light optimizations

00:18:15.350 --> 00:18:19.010
and we have the application
ready to go for the user.

00:18:19.010 --> 00:18:21.180
At first launch,
the application will

00:18:21.180 --> 00:18:24.470
start in what we call
an interpretation mode.

00:18:24.470 --> 00:18:26.840
As the runtime executes
the application code,

00:18:26.840 --> 00:18:29.240
it discovers the most
frequently used methods

00:18:29.240 --> 00:18:32.430
and the most important
methods to be optimized.

00:18:32.430 --> 00:18:35.120
That's when a JIT
system kicks in,

00:18:35.120 --> 00:18:38.040
and it'll optimize that code.

00:18:38.040 --> 00:18:41.000
During this time,
the JIT system also

00:18:41.000 --> 00:18:44.840
records what we call
profile information.

00:18:44.840 --> 00:18:47.590
This profile information
essentially encapsulates data

00:18:47.590 --> 00:18:50.090
about the methods that
are being executed

00:18:50.090 --> 00:18:53.580
and about the classes
that are being loaded.

00:18:53.580 --> 00:18:56.330
Every now and then, we
dump this profile to disk

00:18:56.330 --> 00:19:00.080
so that we can reuse it later.

00:19:00.080 --> 00:19:03.260
Once the device is put
aside and it's not in use--

00:19:03.260 --> 00:19:06.260
a state which we call
idle maintenance mode--

00:19:06.260 --> 00:19:08.690
we're going to use
that profile to drive

00:19:08.690 --> 00:19:11.870
profile-guided optimizations.

00:19:11.870 --> 00:19:15.860
The result is an optimized
app which will eventually

00:19:15.860 --> 00:19:18.780
replace the original state.

00:19:18.780 --> 00:19:21.620
Now when the user
relaunches the app,

00:19:21.620 --> 00:19:25.070
it will have a much snappier
startup time, a much better

00:19:25.070 --> 00:19:27.650
steady-state
performance execution,

00:19:27.650 --> 00:19:31.610
and overall, the
battery will drain less.

00:19:31.610 --> 00:19:34.190
In this state, the
application will

00:19:34.190 --> 00:19:38.550
be interpreted, just-in-time
compiled, or pre-optimized.

00:19:41.080 --> 00:19:44.710
Now just how efficient
is this technique?

00:19:44.710 --> 00:19:46.780
We gathered some
data from the field

00:19:46.780 --> 00:19:49.150
for Google Maps application.

00:19:49.150 --> 00:19:52.120
Here we can see two charts.

00:19:52.120 --> 00:19:55.990
The left one presents data
from a Marshmallow build.

00:19:55.990 --> 00:19:57.580
You can see that
the startup time

00:19:57.580 --> 00:19:59.500
is pretty constant over time.

00:19:59.500 --> 00:20:01.360
It does not fluctuate.

00:20:01.360 --> 00:20:02.980
And this is pretty
much expected.

00:20:02.980 --> 00:20:06.560
You don't want to
have deviations here.

00:20:06.560 --> 00:20:08.280
However on the
right-hand side, you

00:20:08.280 --> 00:20:12.710
can see that in Nougat, the
startup time drops over time.

00:20:12.710 --> 00:20:16.450
Eventually, it stabilizes
over being about 25%

00:20:16.450 --> 00:20:19.480
faster than it used
to be at install time.

00:20:19.480 --> 00:20:20.810
And this is great news.

00:20:20.810 --> 00:20:24.160
It means that the more
the user uses the app,

00:20:24.160 --> 00:20:25.600
the more we can optimize it.

00:20:25.600 --> 00:20:28.180
And over time, the performance
gets better and better.

00:20:32.820 --> 00:20:35.240
This is great, but
we can do better

00:20:35.240 --> 00:20:37.920
and we want to do better.

00:20:37.920 --> 00:20:41.730
We shouldn't need to wait
for optimal performance.

00:20:41.730 --> 00:20:44.400
And our goal with
cloud profiles is

00:20:44.400 --> 00:20:46.740
to deliver near optimal
performance for right

00:20:46.740 --> 00:20:49.350
after install time,
without having

00:20:49.350 --> 00:20:53.900
to wait for the
application to be profiled.

00:20:53.900 --> 00:20:57.990
So let's see how this
is going to work.

00:20:57.990 --> 00:21:01.640
Let me introduce you the
idea of cloud profiles.

00:21:01.640 --> 00:21:07.400
This is based on main
two key observations.

00:21:07.400 --> 00:21:11.980
First one is that usually apps
have many commonly used code

00:21:11.980 --> 00:21:17.770
paths that are shared between a
multitude of users and devices.

00:21:17.770 --> 00:21:21.310
Take, for example, classes
loading during startup time.

00:21:21.310 --> 00:21:24.260
Each device will have
its own specific set.

00:21:24.260 --> 00:21:27.690
However, globally, we can
extract the common intersection

00:21:27.690 --> 00:21:29.800
of all those classes.

00:21:29.800 --> 00:21:33.220
And that's valuable data
for us to optimize upon.

00:21:33.220 --> 00:21:37.690
Second, we know that most app
developers roll out their apps

00:21:37.690 --> 00:21:40.390
incrementally, starting
with alpha/beta

00:21:40.390 --> 00:21:45.370
channels or, for example,
1%, 2% of their user base.

00:21:45.370 --> 00:21:47.700
And the idea behind
cloud profiles

00:21:47.700 --> 00:21:51.790
is to use this initial set
of alpha/beta channel users

00:21:51.790 --> 00:21:55.987
to bootstrap performance
for the rest of the users.

00:21:55.987 --> 00:21:57.070
So how's it going to work?

00:22:00.860 --> 00:22:03.580
Once we have an
initial set of devices,

00:22:03.580 --> 00:22:05.170
we're going to
extract the profile

00:22:05.170 --> 00:22:08.860
information about your
APK from those devices.

00:22:08.860 --> 00:22:12.860
We're going to upload
that information to Play.

00:22:12.860 --> 00:22:16.510
And there, we're going
to combine everything.

00:22:16.510 --> 00:22:18.480
We're going to aggregate
whatever comes in

00:22:18.480 --> 00:22:21.520
and we're going to generate
what we call a core application

00:22:21.520 --> 00:22:23.650
profile.

00:22:23.650 --> 00:22:26.690
This core profile will
contain information

00:22:26.690 --> 00:22:28.450
that's relevant
across all device

00:22:28.450 --> 00:22:32.350
executions, and not
just a single one.

00:22:32.350 --> 00:22:35.200
When a new device requests
for that application

00:22:35.200 --> 00:22:39.970
to be installed, we're going
to deliver this core profile

00:22:39.970 --> 00:22:44.590
alongside the main
application APK to the device.

00:22:44.590 --> 00:22:47.470
Locally, the device
will be able to use

00:22:47.470 --> 00:22:51.190
that data to perform
profile-guided optimizations

00:22:51.190 --> 00:22:53.240
right at install time.

00:22:53.240 --> 00:22:55.750
That will deliver an
improved code startup

00:22:55.750 --> 00:22:59.110
performance and much better
steady-state performance

00:22:59.110 --> 00:22:59.620
over time.

00:23:03.050 --> 00:23:07.760
Now having profiles in the cloud
offers much more opportunities

00:23:07.760 --> 00:23:11.530
than directly influencing
the app performance

00:23:11.530 --> 00:23:14.620
with profile-guided
optimizations.

00:23:14.620 --> 00:23:18.620
The core profiles offers
valuable data, for example,

00:23:18.620 --> 00:23:21.890
for developers to act upon.

00:23:21.890 --> 00:23:23.840
And we believe there
is enough information

00:23:23.840 --> 00:23:27.910
there so that developers can
tune their own applications.

00:23:27.910 --> 00:23:30.510
We're going to explore how we
can share these data later.

00:23:34.090 --> 00:23:37.745
Now you can see in this workflow
that to deliver such a thing,

00:23:37.745 --> 00:23:41.801
we need support from Android
platform and Play alike.

00:23:41.801 --> 00:23:43.300
In today's presentation,
we're going

00:23:43.300 --> 00:23:45.080
to focus on the Android support.

00:23:49.960 --> 00:23:53.980
So what did we do in P to
support this lifecycle?

00:23:53.980 --> 00:23:56.050
We added new
interfaces that will

00:23:56.050 --> 00:23:59.850
allow us to extract the
profile and bootstrap

00:23:59.850 --> 00:24:02.710
the information from the cloud.

00:24:02.710 --> 00:24:07.210
The functionality is available
to all system-level apps which

00:24:07.210 --> 00:24:09.400
acquire the necessary
permissions.

00:24:09.400 --> 00:24:13.240
And in our case, Play
is just a consumer.

00:24:13.240 --> 00:24:17.080
The two APIs I'm talking
about are profile extractions.

00:24:17.080 --> 00:24:20.110
And these are exposed via
a new platform manager.

00:24:20.110 --> 00:24:23.290
We call it ART manager.

00:24:23.290 --> 00:24:27.040
The second API is
profile installation.

00:24:27.040 --> 00:24:30.040
And this is seemingly integrated
in the current installer

00:24:30.040 --> 00:24:31.570
session.

00:24:31.570 --> 00:24:35.390
What we did here is to add a new
kind of installation artifact

00:24:35.390 --> 00:24:39.550
that the platform understands.

00:24:39.550 --> 00:24:43.510
We called this Dex
Metadata files.

00:24:43.510 --> 00:24:46.730
Essentially, in a
similar way to the APKs,

00:24:46.730 --> 00:24:49.840
the Dex Metadata
files are archives

00:24:49.840 --> 00:24:53.210
which will contain information
in how the runtime can optimize

00:24:53.210 --> 00:24:55.330
the application.

00:24:55.330 --> 00:24:57.370
Initially, these
Dex Metadata files

00:24:57.370 --> 00:25:02.170
will contain the core profile
that I mentioned about earlier.

00:25:02.170 --> 00:25:05.720
At install time, they
will deliver these files,

00:25:05.720 --> 00:25:08.070
if they are available,
to the device,

00:25:08.070 --> 00:25:11.050
where they will be streamlined
into the Dex optimizer

00:25:11.050 --> 00:25:11.710
on device.

00:25:14.670 --> 00:25:18.800
It is worthwhile mentioning that
we'll offer support for Google

00:25:18.800 --> 00:25:21.110
Play dynamic delivery.

00:25:21.110 --> 00:25:23.390
So if you plan to
split the functionality

00:25:23.390 --> 00:25:26.990
of your application
in different APKs,

00:25:26.990 --> 00:25:29.930
all the APKs will have
their Dex Metadata files.

00:25:33.730 --> 00:25:35.820
So let's take a look how
everything fits together

00:25:35.820 --> 00:25:38.840
from the device perspective.

00:25:38.840 --> 00:25:42.420
You remember that I presented
this diagram in the beginning,

00:25:42.420 --> 00:25:47.740
showing how the
profiling works locally.

00:25:47.740 --> 00:25:49.770
Let's focus here
just on the profile

00:25:49.770 --> 00:25:51.654
file on the application.

00:25:54.330 --> 00:25:57.180
Once we manage to
capture a profile file,

00:25:57.180 --> 00:26:01.890
we're going upstream
this information to Play.

00:26:01.890 --> 00:26:04.210
On Play, as I mentioned,
we'll aggregate this data

00:26:04.210 --> 00:26:06.720
with many, many other profiles.

00:26:06.720 --> 00:26:08.640
And when we have
a core profiles,

00:26:08.640 --> 00:26:13.055
we're going to deliver it to
our new users as a core profile.

00:26:15.780 --> 00:26:18.410
The idea of the
core profile is not

00:26:18.410 --> 00:26:21.350
to replace on-device
profiling, it's

00:26:21.350 --> 00:26:25.950
only to bootstrap the
profile optimizations.

00:26:25.950 --> 00:26:27.380
So essentially
instead of starting

00:26:27.380 --> 00:26:30.320
with a completely blank
state about your application,

00:26:30.320 --> 00:26:32.780
we already know what are
the most commonly executed

00:26:32.780 --> 00:26:34.610
code paths, and we'll
be able to start

00:26:34.610 --> 00:26:38.520
the optimizations from there.

00:26:38.520 --> 00:26:42.510
So now essentially what was a
pure on-device profile feedback

00:26:42.510 --> 00:26:46.384
loop, it gets extended
with a cloud component.

00:26:51.820 --> 00:26:54.380
Now I keep talking
about this core profile,

00:26:54.380 --> 00:26:56.410
and I think it's
important to dedicate

00:26:56.410 --> 00:26:58.550
a bit more attention to it.

00:26:58.550 --> 00:27:03.040
So let's talk a bit how
we're going to build it.

00:27:03.040 --> 00:27:06.550
We already know that
on-device, from one execution

00:27:06.550 --> 00:27:10.500
to the other, the profiles
aggregate quite well.

00:27:10.500 --> 00:27:13.840
They reach a stable
point pretty fast.

00:27:13.840 --> 00:27:16.930
That means that it will not
reoptimize the application

00:27:16.930 --> 00:27:19.720
over and over and over again.

00:27:19.720 --> 00:27:23.770
After a few optimization
steps, it will stop.

00:27:23.770 --> 00:27:26.260
However, that's data
from one device.

00:27:26.260 --> 00:27:29.860
How well does this work when
you try to do it across devices?

00:27:29.860 --> 00:27:31.870
How many samples you
would need in order

00:27:31.870 --> 00:27:33.900
to get to a robust,
reliable profile?

00:27:37.290 --> 00:27:40.610
We looked at our own
Google applications

00:27:40.610 --> 00:27:43.180
and we tried to figure that out.

00:27:43.180 --> 00:27:46.890
Here we can see a
plot which represents

00:27:46.890 --> 00:27:49.560
the amount of information
in the core profiles

00:27:49.560 --> 00:27:52.280
relative to the total
number of aggregations.

00:27:55.360 --> 00:27:58.780
The y-axis represents the
amount of information.

00:27:58.780 --> 00:28:00.640
And the actual value--
numeric value--

00:28:00.640 --> 00:28:02.530
is not important there.

00:28:02.530 --> 00:28:04.690
What is important
from this graph

00:28:04.690 --> 00:28:08.920
is that actually
from 20, 40 kind

00:28:08.920 --> 00:28:13.240
of number of profile
aggregations, the information

00:28:13.240 --> 00:28:16.810
in the profile
reaches a plateau.

00:28:16.810 --> 00:28:18.670
And that's very important.

00:28:18.670 --> 00:28:21.110
It sends a very
important message.

00:28:21.110 --> 00:28:22.990
It means that the
alpha/beta channel

00:28:22.990 --> 00:28:26.370
users will provide
us with enough data

00:28:26.370 --> 00:28:28.510
to build a core profile.

00:28:28.510 --> 00:28:30.910
And it means that the
majority of the production

00:28:30.910 --> 00:28:33.910
users of your application
will always have

00:28:33.910 --> 00:28:35.690
the best possible experience.

00:28:40.830 --> 00:28:45.990
So how do we actually
aggregate the information?

00:28:45.990 --> 00:28:47.880
I mentioned before
that in the profile,

00:28:47.880 --> 00:28:52.200
you will find information
about classes and methods.

00:28:52.200 --> 00:28:55.020
On-device, this is
roughly how it looks like.

00:28:55.020 --> 00:28:57.630
We're going to take all the
executions that we have seen

00:28:57.630 --> 00:29:00.900
before, then we'll create
a union of everything

00:29:00.900 --> 00:29:02.760
that we've seen.

00:29:02.760 --> 00:29:04.530
In the aggregated
profile, you'll

00:29:04.530 --> 00:29:07.500
have information about classes,
methods, about everything

00:29:07.500 --> 00:29:10.435
that you've seen.

00:29:10.435 --> 00:29:14.750
On cloud however, we don't
really want everything.

00:29:14.750 --> 00:29:20.080
We only want the
commonly-executed code paths.

00:29:20.080 --> 00:29:22.720
And what we are doing
instead of having a union,

00:29:22.720 --> 00:29:25.900
we'll be having a
smart intersection.

00:29:25.900 --> 00:29:28.480
We'll be only keeping
the information

00:29:28.480 --> 00:29:31.330
relevant to all
executions, meaning

00:29:31.330 --> 00:29:35.170
we're going to filter
out all the outliers.

00:29:35.170 --> 00:29:38.800
The result is what we call the
core profile, which only keeps

00:29:38.800 --> 00:29:41.050
the most commonly-seen samples.

00:29:41.050 --> 00:29:43.620
And this is what's going to
get eventually to the device.

00:29:47.040 --> 00:29:50.000
How well does this work?

00:29:50.000 --> 00:29:53.550
Let's look again at data
captured from Google Apps.

00:29:53.550 --> 00:29:55.950
We tested this across a
variety of applications.

00:29:55.950 --> 00:29:59.340
And here are the results of
some representative ones.

00:29:59.340 --> 00:30:01.090
In this set, you
can find application

00:30:01.090 --> 00:30:04.260
which relies on native code--

00:30:04.260 --> 00:30:06.500
for example, Google Camera--

00:30:06.500 --> 00:30:11.780
or applications which are
much more Java oriented--

00:30:11.780 --> 00:30:15.030
say, Google Maps or Google Docs.

00:30:15.030 --> 00:30:16.520
For Google Camera,
for example, we

00:30:16.520 --> 00:30:20.400
get a startup time
improvement of about 12.6%.

00:30:20.400 --> 00:30:23.580
And that's excellent given that
the application itself doesn't

00:30:23.580 --> 00:30:25.770
have a lot of Java code.

00:30:25.770 --> 00:30:29.960
However, for Maps or Docs,
which are heavily Java-based,

00:30:29.960 --> 00:30:32.030
we can see that the
optimizations improves

00:30:32.030 --> 00:30:37.980
the startup time by
about 28% or 33%.

00:30:37.980 --> 00:30:41.220
Across the board, we can
see an average of about 20%

00:30:41.220 --> 00:30:42.550
improvement.

00:30:42.550 --> 00:30:44.070
And that obviously
depends with what

00:30:44.070 --> 00:30:47.370
the application is doing, how
my Java code is being used,

00:30:47.370 --> 00:30:48.486
and so on.

00:30:52.680 --> 00:30:56.550
Now I mentioned in the
beginning that besides improving

00:30:56.550 --> 00:30:58.240
the application
performance directly

00:30:58.240 --> 00:31:01.290
via profile-guided
optimizations,

00:31:01.290 --> 00:31:06.200
the profile also offer
much more opportunities.

00:31:06.200 --> 00:31:09.030
I'm going to present
a short use case study

00:31:09.030 --> 00:31:12.120
and walk you through
some important aspects

00:31:12.120 --> 00:31:15.870
that the profiles can reveal
about your application.

00:31:15.870 --> 00:31:17.520
During this use case
study, I'm going

00:31:17.520 --> 00:31:20.390
to focus on a single question--

00:31:20.390 --> 00:31:22.700
are you shipping unnecessary
code to the clients?

00:31:26.460 --> 00:31:26.960
Are you?

00:31:30.460 --> 00:31:33.150
Let's take a look at some data.

00:31:33.150 --> 00:31:38.470
Again, this case study reflects
the state of some Google Apps

00:31:38.470 --> 00:31:40.500
that we tested.

00:31:40.500 --> 00:31:44.640
We see that on average, we
profiled about 14% to 15%

00:31:44.640 --> 00:31:45.840
of the code.

00:31:45.840 --> 00:31:50.596
And about 85% of the
code remains unprofiled.

00:31:50.596 --> 00:31:51.970
When you spread
the distribution,

00:31:51.970 --> 00:31:54.960
we can see, for example,
that in some apps, 5% to 10%

00:31:54.960 --> 00:31:56.130
of the code gets profiled.

00:31:56.130 --> 00:31:58.770
In some other apps, even 50%
of the code gets profiled.

00:32:01.740 --> 00:32:07.800
And this is a rather intriguing
result. And the reason for that

00:32:07.800 --> 00:32:12.150
is that if the code is not
profiled, that most likely

00:32:12.150 --> 00:32:17.010
means that it might not
have ever been executed.

00:32:17.010 --> 00:32:20.080
Obviously, that's
for a good case.

00:32:20.080 --> 00:32:23.190
I mean, the code for example,
can be unexpected error code

00:32:23.190 --> 00:32:24.650
paths, right?

00:32:24.650 --> 00:32:28.100
We all want the applications
to be reliable and robust,

00:32:28.100 --> 00:32:30.160
and the error handling
must be there.

00:32:30.160 --> 00:32:32.600
Hopefully, it never
gets executed.

00:32:32.600 --> 00:32:34.970
You may have backwards
compatibility code,

00:32:34.970 --> 00:32:38.450
support for previous
API level and such.

00:32:38.450 --> 00:32:41.300
You may have features which
are not used on all devices.

00:32:41.300 --> 00:32:46.150
It may have features
very targeted.

00:32:46.150 --> 00:32:49.790
And you might also have a lot of
unnecessary code lying around,

00:32:49.790 --> 00:32:52.050
maybe including libraries
that you don't really use.

00:32:55.170 --> 00:32:57.550
Now it's a bit hard to
break down the percentage

00:32:57.550 --> 00:32:59.060
for these categories.

00:32:59.060 --> 00:33:02.620
And there can be other reasons
why we didn't profile the code.

00:33:02.620 --> 00:33:06.625
But the skewed distribution
here is a strong indication

00:33:06.625 --> 00:33:12.100
that there is a lot of room
of improvements for APK.

00:33:12.100 --> 00:33:15.130
The code can be
reorganized or trimmed down

00:33:15.130 --> 00:33:18.290
for better efficiency.

00:33:18.290 --> 00:33:22.270
For example, Google Play
introduced dynamic delivery

00:33:22.270 --> 00:33:24.280
schemes, which may
help you reduce

00:33:24.280 --> 00:33:26.890
the code that you share
by targeting features only

00:33:26.890 --> 00:33:27.742
to certain users.

00:33:27.742 --> 00:33:29.200
And that's something
that you might

00:33:29.200 --> 00:33:30.920
want to look at and
take advantage of.

00:33:35.160 --> 00:33:38.120
So we believe that there is
quite a bit of unnecessary code

00:33:38.120 --> 00:33:40.600
lying around, at
least in our own APKs.

00:33:43.110 --> 00:33:45.890
Now since we've focused on
the code that actually doesn't

00:33:45.890 --> 00:33:49.400
get profiled, is there anything
that we can extract out

00:33:49.400 --> 00:33:50.500
of the profile code?

00:33:55.060 --> 00:33:58.090
To understand this,
let me talk a bit

00:33:58.090 --> 00:34:00.290
about different categories
of profile code.

00:34:02.800 --> 00:34:04.660
When the application
code is being profiled,

00:34:04.660 --> 00:34:08.320
the runtime will try to label
it depending on its state.

00:34:08.320 --> 00:34:11.015
And you will have a label
for the startup category,

00:34:11.015 --> 00:34:14.530
for the post-startup category,
and for the hot category.

00:34:14.530 --> 00:34:17.409
Obviously, these are
pretty self-explanatory.

00:34:17.409 --> 00:34:19.360
The hot category of
the code is essentially

00:34:19.360 --> 00:34:22.540
what the runtime's seen to
be the most important part

00:34:22.540 --> 00:34:23.139
of your code.

00:34:25.969 --> 00:34:29.540
It's important to keep in mind
that these are not disjoint.

00:34:29.540 --> 00:34:32.870
Say for example, that the
method foo is being executed.

00:34:32.870 --> 00:34:34.460
This can be executing
during startup,

00:34:34.460 --> 00:34:37.159
it can be executing
during post-startup time,

00:34:37.159 --> 00:34:39.139
and it can also
be marked as hot--

00:34:39.139 --> 00:34:41.510
for example, if you have
a very heavy computation

00:34:41.510 --> 00:34:42.429
during that method.

00:34:45.050 --> 00:34:47.550
Now if you know the code which
is executing during startup

00:34:47.550 --> 00:34:49.239
time, if you focus
on that, you will

00:34:49.239 --> 00:34:52.870
be able to lower the startup
time of the application.

00:34:52.870 --> 00:34:55.000
As such, the first
impression that the users

00:34:55.000 --> 00:34:57.100
will have upon your
application will be very good.

00:35:00.280 --> 00:35:02.690
If you look at the
post-startup code,

00:35:02.690 --> 00:35:06.510
that will help you, for example,
lay out the application Dex

00:35:06.510 --> 00:35:07.230
bytecode.

00:35:07.230 --> 00:35:10.310
That will lead to
memory improvements

00:35:10.310 --> 00:35:12.270
and will be much smoother
on low-end devices.

00:35:17.110 --> 00:35:19.840
As for the hot code,
this is the code

00:35:19.840 --> 00:35:21.490
that should get
the most attention

00:35:21.490 --> 00:35:24.510
for your optimizations efforts.

00:35:24.510 --> 00:35:27.940
It's the code that is
most heavily optimized

00:35:27.940 --> 00:35:31.180
by the runtime, and it might
be so because the runtime

00:35:31.180 --> 00:35:34.690
identified that it would be
very beneficial to invest time

00:35:34.690 --> 00:35:36.710
there.

00:35:36.710 --> 00:35:39.382
And it's what if you,
for example, start with--

00:35:39.382 --> 00:35:41.840
try to improve the quality and
the performance of your app,

00:35:41.840 --> 00:35:43.775
this is where you should
spend your effort,

00:35:43.775 --> 00:35:46.730
or your initial effort.

00:35:46.730 --> 00:35:48.430
Now for this is
important, like how much

00:35:48.430 --> 00:35:51.370
code of your application is
actually being marked as hot?

00:35:51.370 --> 00:35:54.460
Because if everything
gets hot, then everything

00:35:54.460 --> 00:35:55.240
can be optimized.

00:35:55.240 --> 00:35:57.950
So that's not really useful.

00:36:00.540 --> 00:36:06.190
Let me show you the breakdown
of these three categories.

00:36:06.190 --> 00:36:10.330
In this graph, you can
see on the red columns,

00:36:10.330 --> 00:36:14.400
the percentages for the profile
code and the not-profile code.

00:36:14.400 --> 00:36:18.230
This will sum up to 100% and
it's what I showed you earlier.

00:36:18.230 --> 00:36:20.920
They are here just
for the reference.

00:36:20.920 --> 00:36:23.370
The blue boxes show
the percentages

00:36:23.370 --> 00:36:25.710
of the startup code,
the post-startup code,

00:36:25.710 --> 00:36:29.740
and the hot one relative
to the total Dex bytecode,

00:36:29.740 --> 00:36:33.220
so don't expect it to add 100%.

00:36:33.220 --> 00:36:36.700
Also, one piece of code can
be in different categories

00:36:36.700 --> 00:36:39.560
at the same time.

00:36:39.560 --> 00:36:42.200
But as you can see
here, the average--

00:36:42.200 --> 00:36:46.130
on average about 10% of the
application Dex bytecode

00:36:46.130 --> 00:36:48.620
is being marked as hot.

00:36:48.620 --> 00:36:51.840
And this indicates that
when you focus on your app

00:36:51.840 --> 00:36:55.570
optimizations, you can
dedicate the attention starting

00:36:55.570 --> 00:36:59.270
with just a small part of
your application code base.

00:36:59.270 --> 00:37:03.224
You obviously should spend time
on all the other parts as well,

00:37:03.224 --> 00:37:07.070
but probably this is where
you should start from.

00:37:07.070 --> 00:37:09.590
Let me go over a quick
review of what we presented

00:37:09.590 --> 00:37:13.910
today and the main benefits.

00:37:13.910 --> 00:37:16.750
We started with Kotlin,
and we described

00:37:16.750 --> 00:37:20.000
a few new compiler optimizations
that we added that focused

00:37:20.000 --> 00:37:22.730
on Kotlin performance.

00:37:22.730 --> 00:37:27.040
We described briefly how we
approach Kotlin optimizations,

00:37:27.040 --> 00:37:29.197
and that we first try
to seek improvements

00:37:29.197 --> 00:37:30.155
in the Kotlin compiler.

00:37:33.810 --> 00:37:36.690
We moved to memory and
storage optimizations,

00:37:36.690 --> 00:37:38.370
and my colleague
Mathieu introduced you

00:37:38.370 --> 00:37:41.550
to the concept of CompactDex.

00:37:41.550 --> 00:37:46.320
This is a new Dex format
available just on-device, which

00:37:46.320 --> 00:37:48.395
focuses on the memory savings.

00:37:51.360 --> 00:37:56.610
And finally, I presented you
the idea of cloud profiles.

00:37:56.610 --> 00:37:59.040
And we talk about
how we can bootstrap

00:37:59.040 --> 00:38:02.040
the profile-guided
optimizations using

00:38:02.040 --> 00:38:05.610
a small percentage of
alpha/beta channel users

00:38:05.610 --> 00:38:09.780
in order to lead important
performance improvement right

00:38:09.780 --> 00:38:12.240
after install time for the
majority of the production

00:38:12.240 --> 00:38:14.600
users.

00:38:14.600 --> 00:38:17.060
With this, I'd like to
thank you for your attention

00:38:17.060 --> 00:38:19.010
and for your presence.

00:38:19.010 --> 00:38:21.560
And I want to invite you all
to Android Runtime office

00:38:21.560 --> 00:38:24.370
hours tomorrow, where we can
answer any questions that you

00:38:24.370 --> 00:38:26.350
would have about
today's presentation

00:38:26.350 --> 00:38:28.420
or about the runtime in general.

00:38:28.420 --> 00:38:34.874
We're going to be at 5:30 in
section A. Thank you so much.

00:38:34.874 --> 00:38:37.309
[APPLAUSE]

00:38:37.309 --> 00:38:40.994
[MUSIC PLAYING]

