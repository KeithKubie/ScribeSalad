WEBVTT
Kind: captions
Language: en

00:00:01.976 --> 00:00:04.100
PRIYA GUPTA: Let's begin
with the obvious question.

00:00:04.100 --> 00:00:07.340
Why should one care about
distributed training?

00:00:07.340 --> 00:00:10.160
Training complex neural networks
with large amounts of data

00:00:10.160 --> 00:00:12.020
can often take a long time.

00:00:12.020 --> 00:00:13.970
In the graph here,
you can see training

00:00:13.970 --> 00:00:16.880
the resident 50 model on
a single but powerful GPU

00:00:16.880 --> 00:00:18.955
can take up to four days.

00:00:18.955 --> 00:00:21.080
If you have some experience
running complex machine

00:00:21.080 --> 00:00:24.530
learning models, this may
sound rather familiar to you.

00:00:24.530 --> 00:00:27.170
Bringing down your training
time from days to hours

00:00:27.170 --> 00:00:30.350
can have a significant
effect on your productivity

00:00:30.350 --> 00:00:34.415
because you can try
out new ideas faster.

00:00:34.415 --> 00:00:35.790
In this talk,
we're going to talk

00:00:35.790 --> 00:00:39.540
about distributed training, that
is running training in parallel

00:00:39.540 --> 00:00:43.590
on multiple devices such
as CPUs, GPUs, or TPUs

00:00:43.590 --> 00:00:46.080
to bring down your
training time.

00:00:46.080 --> 00:00:48.840
With the techniques that you--
we'll talk about in this talk,

00:00:48.840 --> 00:00:51.270
you can bring down your
training time from weeks or days

00:00:51.270 --> 00:00:54.240
to hours with just a few
lines of change of code

00:00:54.240 --> 00:00:57.940
and some powerful hardware.

00:00:57.940 --> 00:01:00.170
To achieve these goals,
we're pleased to introduce

00:01:00.170 --> 00:01:03.740
the new distribution
strategy API.

00:01:03.740 --> 00:01:05.980
This is an easy way to
distribute your TensorFlow

00:01:05.980 --> 00:01:09.780
training with very little
modification to your code.

00:01:09.780 --> 00:01:12.170
With distribution strategy
API, you no longer

00:01:12.170 --> 00:01:15.470
need to place ops or
parameters on specific devices,

00:01:15.470 --> 00:01:17.780
and you don't need to
restructure a model in a way

00:01:17.780 --> 00:01:20.810
that the losses and gradients
get aggregated correctly

00:01:20.810 --> 00:01:22.220
across the devices.

00:01:22.220 --> 00:01:26.509
Distribution strategy takes
care of all of that for you.

00:01:26.509 --> 00:01:28.800
So let's go with what are
the key goals of distribution

00:01:28.800 --> 00:01:30.340
strategy.

00:01:30.340 --> 00:01:32.130
The first one is ease of use.

00:01:32.130 --> 00:01:34.650
We want you to make minimal
code changes in order

00:01:34.650 --> 00:01:36.970
to distribute your training.

00:01:36.970 --> 00:01:40.320
The second is to give great
performance out of the box.

00:01:40.320 --> 00:01:43.350
Ideally, the user shouldn't
have to change any--

00:01:43.350 --> 00:01:46.230
change or configure any settings
to get the most performance out

00:01:46.230 --> 00:01:47.940
of their hardware.

00:01:47.940 --> 00:01:50.160
And third we want
distribution strategy

00:01:50.160 --> 00:01:52.431
to work in a variety of
different situations,

00:01:52.431 --> 00:01:54.180
so whether you want
to scale your training

00:01:54.180 --> 00:01:57.240
on different hardware
like GPUs or TPUs

00:01:57.240 --> 00:02:00.600
or you want to use different
APIs like Keras or estimator

00:02:00.600 --> 00:02:02.130
or if you want to
run distributed--

00:02:02.130 --> 00:02:03.930
different distribution
architectures

00:02:03.930 --> 00:02:06.030
like synchronous or
asynchronous training,

00:02:06.030 --> 00:02:08.610
we have one distribution
strategy to be useful for you

00:02:08.610 --> 00:02:09.830
in all these situations.

00:02:11.937 --> 00:02:14.020
So if you're just beginning
with machine learning,

00:02:14.020 --> 00:02:16.320
you might start your training
with a multi-core CPU

00:02:16.320 --> 00:02:17.410
on your desktop.

00:02:17.410 --> 00:02:20.370
TensorFlow takes care of
scaling onto a multi-core CPU

00:02:20.370 --> 00:02:23.170
automatically.

00:02:23.170 --> 00:02:25.020
Next, you may add a
GPU to your desktop

00:02:25.020 --> 00:02:26.670
to scale up your training.

00:02:26.670 --> 00:02:29.190
As long as you build your
program with the right CUDA

00:02:29.190 --> 00:02:31.890
libraries, TensorFlow
will automatically

00:02:31.890 --> 00:02:34.980
run your training on the GPU
and give you a nice performance

00:02:34.980 --> 00:02:37.190
boost.

00:02:37.190 --> 00:02:40.090
But what if you have multiple
GPUs on your machine,

00:02:40.090 --> 00:02:43.570
and you want to use all
of them for your training?

00:02:43.570 --> 00:02:46.480
This is where distribution
strategy comes in.

00:02:46.480 --> 00:02:48.170
In the next section,
we're going to talk

00:02:48.170 --> 00:02:50.840
about how you can use
distribution strategy to scale

00:02:50.840 --> 00:02:54.440
your training to multiple GPUs.

00:02:54.440 --> 00:02:56.720
First, we'll look at some
code to train the ResNet 50

00:02:56.720 --> 00:02:59.050
model without any distribution.

00:02:59.050 --> 00:03:01.670
We'll use a Keras API, which
is the recommended TensorFlow

00:03:01.670 --> 00:03:04.670
high level API.

00:03:04.670 --> 00:03:06.460
We begin by creating
some datasets

00:03:06.460 --> 00:03:11.310
for training and validation
using the TF data API.

00:03:11.310 --> 00:03:13.050
For the model,
we'll simply reuse

00:03:13.050 --> 00:03:18.540
the ResNet 50 that's prepackaged
with Keras and TensorFlow.

00:03:18.540 --> 00:03:23.010
Then we create an optimizer that
we'll be using in our training.

00:03:23.010 --> 00:03:26.730
Once we have these pieces, we
can compile the model providing

00:03:26.730 --> 00:03:29.220
the loss and optimizer and
maybe a few other things

00:03:29.220 --> 00:03:31.350
like metrics, which I've
omitted in the slide here.

00:03:34.060 --> 00:03:36.520
Once a model's compiled, you
can then begin your training

00:03:36.520 --> 00:03:38.980
by calling model dot fit,
providing the training

00:03:38.980 --> 00:03:41.950
dataset that you created
earlier, along with how many

00:03:41.950 --> 00:03:44.730
epochs you want to
run the training for.

00:03:44.730 --> 00:03:49.170
Fit will train your model and
update the models variables.

00:03:49.170 --> 00:03:51.810
Then you can call evaluate
with the validation dataset

00:03:51.810 --> 00:03:55.560
to see how well
your training did.

00:03:55.560 --> 00:03:57.650
So given this code
to run your training

00:03:57.650 --> 00:04:00.590
on a single machine
or a single GPU,

00:04:00.590 --> 00:04:02.600
let's see how we can use
distribution strategy

00:04:02.600 --> 00:04:05.620
to now run it on multiple GPUs.

00:04:05.620 --> 00:04:07.340
It's actually very simple.

00:04:07.340 --> 00:04:09.100
You need to make
only two changes.

00:04:09.100 --> 00:04:11.410
First, create an instance
of something called

00:04:11.410 --> 00:04:15.430
mirrored strategy and second
pass the strategy instance

00:04:15.430 --> 00:04:18.970
to the compile call with
the distribute argument.

00:04:18.970 --> 00:04:19.571
That's it.

00:04:19.571 --> 00:04:21.070
That's all the code
changes you need

00:04:21.070 --> 00:04:24.760
to now run this code on
multiple GPUs using distribution

00:04:24.760 --> 00:04:27.120
strategy.

00:04:27.120 --> 00:04:29.850
Mirror strategy is a type
of distribution strategy API

00:04:29.850 --> 00:04:33.330
that we introduced earlier.

00:04:33.330 --> 00:04:36.490
This API is available
intensive on point 11 release,

00:04:36.490 --> 00:04:38.059
which will be out very shortly.

00:04:38.059 --> 00:04:39.600
And in the bottom
of the slide, we've

00:04:39.600 --> 00:04:42.210
linked to a complete example
of training [INAUDIBLE]

00:04:42.210 --> 00:04:45.240
with Keras and multiple
GPUs that you can try out.

00:04:48.980 --> 00:04:50.480
With mirror strategy,
you don't need

00:04:50.480 --> 00:04:52.970
to make any changes to your
model code or your training

00:04:52.970 --> 00:04:55.860
loop, so it makes
it very easy to use.

00:04:55.860 --> 00:04:58.385
This is because we've changed
many underlying components

00:04:58.385 --> 00:05:00.830
of TensorFlow to be
distribution aware.

00:05:00.830 --> 00:05:04.820
So this includes the optimizer,
batch norm layers, metrics,

00:05:04.820 --> 00:05:08.075
and summaries are all
now distribution aware.

00:05:08.075 --> 00:05:10.450
You don't need to make any
changes to your input pipeline

00:05:10.450 --> 00:05:15.790
as well as long as you're using
the recommended TF data APIs.

00:05:15.790 --> 00:05:17.900
And finally saving
and checkpointing work

00:05:17.900 --> 00:05:18.810
seamlessly as well.

00:05:18.810 --> 00:05:21.900
So you can save with
no or one distribution

00:05:21.900 --> 00:05:24.637
strategy and a store
with another seamlessly.

00:05:27.449 --> 00:05:28.990
Now that you've seen
some code on how

00:05:28.990 --> 00:05:31.607
to use mirror strategy to
scale to multiple GPUs,

00:05:31.607 --> 00:05:33.190
let's look under the
hood a little bit

00:05:33.190 --> 00:05:35.500
and see what mirror
strategy does.

00:05:35.500 --> 00:05:38.740
In a nutshell, mirror strategy
implements data parallelism

00:05:38.740 --> 00:05:40.190
architecture.

00:05:40.190 --> 00:05:44.140
It mirrors the variables
on each device EGPU

00:05:44.140 --> 00:05:45.670
and hence the name
mirror strategy,

00:05:45.670 --> 00:05:49.872
and it uses AllReduce to
keep these variables in sync.

00:05:49.872 --> 00:05:51.580
And using these
techniques, it implements

00:05:51.580 --> 00:05:53.420
synchronous training.

00:05:53.420 --> 00:05:54.826
So that's a lot of terminology.

00:05:54.826 --> 00:05:56.200
Let's unpack each
of these a bit.

00:05:59.100 --> 00:06:02.040
What is data parallelism?

00:06:02.040 --> 00:06:04.770
Let's say you have end
workers or end devices.

00:06:04.770 --> 00:06:07.770
In data parallelism, each
device runs the same model

00:06:07.770 --> 00:06:10.080
and computation but for
the different subset

00:06:10.080 --> 00:06:12.000
of the input data.

00:06:12.000 --> 00:06:14.460
Each device computes
the loss and gradients

00:06:14.460 --> 00:06:17.310
based on the training
samples that it sees.

00:06:17.310 --> 00:06:18.900
And then we combine
these gradients

00:06:18.900 --> 00:06:21.480
and update the
models parameters.

00:06:21.480 --> 00:06:23.760
The updated model is then
used in the next round

00:06:23.760 --> 00:06:24.998
of computation.

00:06:27.750 --> 00:06:30.690
As I mentioned before, mirror
strategy mirrors the variables

00:06:30.690 --> 00:06:32.410
across the different devices.

00:06:32.410 --> 00:06:34.800
So let's say you have a
variable A your model.

00:06:34.800 --> 00:06:37.890
It'll be replicated
as A0, A1, A2, and A3

00:06:37.890 --> 00:06:39.990
across the four
different devices.

00:06:39.990 --> 00:06:42.630
And together these four
variables conceptually

00:06:42.630 --> 00:06:44.340
form a single
conceptual variable

00:06:44.340 --> 00:06:47.250
called a mirrored variable.

00:06:47.250 --> 00:06:50.325
These variables are kept in sync
by applying identical updates.

00:06:53.280 --> 00:06:55.330
A class of algorithms
called AllReduce

00:06:55.330 --> 00:06:58.030
can be used to keep
variables in sync

00:06:58.030 --> 00:07:01.810
by applying identical
gradient updates.

00:07:01.810 --> 00:07:04.660
AllReduce algorithms can be
used to aggregate the gradients

00:07:04.660 --> 00:07:07.360
across the different
devices, for example,

00:07:07.360 --> 00:07:12.080
by adding them up and making
them available on each device.

00:07:12.080 --> 00:07:14.650
It's a fused algorithm
that can be very efficient

00:07:14.650 --> 00:07:19.420
and reduce the overhead of
synchronization by quite a bit.

00:07:19.420 --> 00:07:21.600
There are many
versions of algorithm--

00:07:21.600 --> 00:07:24.030
AllReduce algorithms
available based

00:07:24.030 --> 00:07:28.350
on the communication available
between the different devices.

00:07:28.350 --> 00:07:31.990
One common algorithm is what
is known as ring all-reduce.

00:07:31.990 --> 00:07:35.180
In ring all-reduce, each device
sends a chunk of its gradients

00:07:35.180 --> 00:07:37.910
to its successor on the ring
and receives another chunk

00:07:37.910 --> 00:07:39.950
from its predecessor.

00:07:39.950 --> 00:07:42.260
There are a few more such
rounds of rate and exchanges,

00:07:42.260 --> 00:07:44.840
and at the end of
these exchanges,

00:07:44.840 --> 00:07:46.880
each device has
received a combined

00:07:46.880 --> 00:07:48.260
copy of all the gradients.

00:07:51.360 --> 00:07:54.340
Ring-all reduce also uses
network bandwidth optimally

00:07:54.340 --> 00:07:57.060
because it ensures that both the
upload and download bandwidth

00:07:57.060 --> 00:07:59.760
at each host is fully utilized.

00:07:59.760 --> 00:08:02.280
We have a team working on
fast implementations of all

00:08:02.280 --> 00:08:04.920
reduce for various
network topologies.

00:08:04.920 --> 00:08:07.320
Some hardware vendors
such as the Nvidia

00:08:07.320 --> 00:08:09.000
provide specialized
implementation

00:08:09.000 --> 00:08:11.540
of all-reduce for their
hardware, for example,

00:08:11.540 --> 00:08:13.580
Nvidia [INAUDIBLE].

00:08:13.580 --> 00:08:15.730
The bottom line is that
AllReduce can be fast

00:08:15.730 --> 00:08:18.610
when you have multiple
devices on a single machine

00:08:18.610 --> 00:08:23.970
or a small number of machines
with strong connectivity.

00:08:23.970 --> 00:08:25.620
Putting all these
pieces together,

00:08:25.620 --> 00:08:28.510
mirror strategy uses
mirrored variables and all

00:08:28.510 --> 00:08:30.900
reduce to implement
synchronous training.

00:08:30.900 --> 00:08:33.919
So let's see how that works.

00:08:33.919 --> 00:08:36.320
Let's say you have two
devices, device 0 and 1,

00:08:36.320 --> 00:08:39.429
and your model has two layers,
A and B. Each layer has

00:08:39.429 --> 00:08:40.456
a single variable.

00:08:40.456 --> 00:08:41.830
And as you can
see, the variables

00:08:41.830 --> 00:08:44.860
are replicated across
the two devices.

00:08:44.860 --> 00:08:47.800
Each device received one
subset of the input data,

00:08:47.800 --> 00:08:50.380
and it computes the forward
pass using its local copy

00:08:50.380 --> 00:08:53.200
of the variables.

00:08:53.200 --> 00:08:56.840
It then computes a backward
pass and computes the gradients.

00:08:56.840 --> 00:08:59.446
Once agreements are
computed on each device,

00:08:59.446 --> 00:09:01.070
the devices communicate
with each other

00:09:01.070 --> 00:09:04.250
using all reduce to
aggregate the gradients.

00:09:04.250 --> 00:09:06.050
And once the gradients
are aggregated,

00:09:06.050 --> 00:09:10.050
each device updates its
local copy of the variables.

00:09:10.050 --> 00:09:13.100
So in this way, the devices
are always kept in sync.

00:09:13.100 --> 00:09:14.810
The next forward
pass doesn't begin

00:09:14.810 --> 00:09:18.590
until each device has received
a copy of the combined gradients

00:09:18.590 --> 00:09:20.487
and updated its variables.

00:09:23.900 --> 00:09:26.310
All reduce can further
optimize things and bring down

00:09:26.310 --> 00:09:29.190
your training time by
overlapping computation

00:09:29.190 --> 00:09:32.010
of gradients at lower layers in
the network with transmission

00:09:32.010 --> 00:09:33.880
of gradients at
the higher layers.

00:09:33.880 --> 00:09:35.450
So in this case, you can see--

00:09:35.450 --> 00:09:38.040
you can compute the
gradients of layer A

00:09:38.040 --> 00:09:40.562
while you're transmitting
the gradients for layer B.

00:09:40.562 --> 00:09:42.520
And this can further
reduce your training time.

00:09:45.650 --> 00:09:47.650
So now that we've seen
how mirror strategy looks

00:09:47.650 --> 00:09:50.440
under the hood, let's look
at what type of performance

00:09:50.440 --> 00:09:52.240
and scaling you can
expect when using

00:09:52.240 --> 00:09:55.902
mirror strategy with
multi-- for multiple GPUs.

00:09:55.902 --> 00:09:59.030
We use a ResNet 50 model
with ImageNet dataset

00:09:59.030 --> 00:10:00.650
for our benchmarking.

00:10:00.650 --> 00:10:04.160
It's a very popular benchmark
for performance measurement.

00:10:04.160 --> 00:10:08.240
And we use Nvidia Teslas
V100 GPUs on Google Cloud.

00:10:08.240 --> 00:10:11.840
And we use a bat
size of 128 per GPU.

00:10:11.840 --> 00:10:14.660
On the x-axis here, you
can see the number of GPUs,

00:10:14.660 --> 00:10:17.810
and on the y-axis, you can
see images per second process

00:10:17.810 --> 00:10:20.090
during training.

00:10:20.090 --> 00:10:22.640
As you can see, as we
increase the number of GPUs

00:10:22.640 --> 00:10:24.770
from one to two
to four to eight,

00:10:24.770 --> 00:10:27.190
the images per
second processed is

00:10:27.190 --> 00:10:29.900
close to doubling every time.

00:10:29.900 --> 00:10:32.950
In fact, we're able to
achieve 90% to 95% scaling out

00:10:32.950 --> 00:10:33.580
of the box.

00:10:37.840 --> 00:10:41.220
Note that these numbers were
obtained by using the ResNet 50

00:10:41.220 --> 00:10:44.160
model that's available in our
official model garden depot,

00:10:44.160 --> 00:10:47.680
and currently it uses
the estimator API.

00:10:47.680 --> 00:10:50.077
We're working on Keras
performance actively.

00:10:53.210 --> 00:10:56.420
So far, we've talked a lot about
scaling onto multiple GPUs.

00:10:56.420 --> 00:10:58.490
What about cloud TPUs?

00:10:58.490 --> 00:11:01.250
TPU stands for a tensor
processing units.

00:11:01.250 --> 00:11:04.670
These are custom ASIC,
designed and built by Google

00:11:04.670 --> 00:11:08.352
especially for accelerating
machine learning workloads.

00:11:08.352 --> 00:11:10.685
In the picture here, you can
see the various generations

00:11:10.685 --> 00:11:11.570
of TPUs.

00:11:11.570 --> 00:11:14.210
On the top left,
you can see TPUE1.

00:11:14.210 --> 00:11:16.500
In the middle you
can see cloud TPUE2,

00:11:16.500 --> 00:11:18.910
which is now generally
available in Google Cloud.

00:11:18.910 --> 00:11:20.450
And on the right
side you can see

00:11:20.450 --> 00:11:24.020
TPUE3, which was just announced
in Google I/O a few months ago

00:11:24.020 --> 00:11:25.920
and is now available in alpha.

00:11:25.920 --> 00:11:27.520
And in the bottom
of the slide, you

00:11:27.520 --> 00:11:31.010
can see a TPU pod, which
is a number of cloud TPUs

00:11:31.010 --> 00:11:34.400
that are interconnected to each
other using a custom network.

00:11:34.400 --> 00:11:39.000
TPU pods are also now
available in alpha.

00:11:39.000 --> 00:11:41.660
So if you want to
learn more about TPUs,

00:11:41.660 --> 00:11:44.805
please attend Frank's talk
tomorrow on cloud TPUs.

00:11:44.805 --> 00:11:46.430
In this talk, we're
just going to focus

00:11:46.430 --> 00:11:49.160
on how you can use
distribution strategy to scale

00:11:49.160 --> 00:11:52.250
your training on TPUs.

00:11:52.250 --> 00:11:54.020
It's actually very
similar to what we just

00:11:54.020 --> 00:11:56.330
saw with mirror strategy,
but instead we'll

00:11:56.330 --> 00:11:58.880
use TPU strategy this time.

00:11:58.880 --> 00:12:03.050
So first you create an instance
of a TPU cluster resolver

00:12:03.050 --> 00:12:06.880
and give it the name of
your cloud TPU resource.

00:12:06.880 --> 00:12:09.820
Then you pass the clusters over
to the TPU strategy constructor

00:12:09.820 --> 00:12:12.250
along with another argument
called steps per run, which

00:12:12.250 --> 00:12:14.210
I'll come back to in a bit.

00:12:14.210 --> 00:12:14.770
That's it.

00:12:14.770 --> 00:12:16.310
Once you have the
strategy instance,

00:12:16.310 --> 00:12:18.930
you can pass it to your
compile call as before,

00:12:18.930 --> 00:12:21.000
and your training will
now run on cloud TPUs.

00:12:23.932 --> 00:12:25.640
So you can see, the
distribution strategy

00:12:25.640 --> 00:12:28.070
makes it really easy to
switch between different types

00:12:28.070 --> 00:12:28.640
of hardware.

00:12:31.170 --> 00:12:33.990
This API will be available
in the next TensorFlow

00:12:33.990 --> 00:12:36.740
release, which is 1.12.

00:12:36.740 --> 00:12:38.580
And in the bottom
of the slide, we've

00:12:38.580 --> 00:12:41.760
provided a link to
training ResNet 50

00:12:41.760 --> 00:12:43.710
with the estimator API
using TPU strategy.

00:12:46.850 --> 00:12:49.990
So let's talk a little bit
about what TPU strategy does.

00:12:49.990 --> 00:12:53.080
TPU strategy implements
the same architecture

00:12:53.080 --> 00:12:53.890
as mirror strategy.

00:12:53.890 --> 00:12:55.480
That is it implements
data parallelism

00:12:55.480 --> 00:12:58.240
with synchronous training.

00:12:58.240 --> 00:13:00.060
The cores on a TPU,
there are eight cores

00:13:00.060 --> 00:13:01.200
on a single cloud TPU.

00:13:01.200 --> 00:13:04.080
And these cores are connected
via fast interconnects.

00:13:04.080 --> 00:13:06.960
And this means that you can
do AllReduce really fast

00:13:06.960 --> 00:13:09.887
to aggregate the gradients.

00:13:09.887 --> 00:13:11.720
Coming back to those
steps per run parameter

00:13:11.720 --> 00:13:14.640
from the previous
slide, for most models

00:13:14.640 --> 00:13:16.640
the computation time
of a single step

00:13:16.640 --> 00:13:20.000
is small compared to the sum
of the communication overheads.

00:13:20.000 --> 00:13:22.100
So it makes sense to run
multiple steps at a time

00:13:22.100 --> 00:13:24.690
to amortize these overheads.

00:13:24.690 --> 00:13:27.150
So setting this number
to a high value like 100

00:13:27.150 --> 00:13:29.970
will give you the best
performance out of the TPUs.

00:13:29.970 --> 00:13:32.580
The TPU teams are working on
reducing these overhead so

00:13:32.580 --> 00:13:35.100
that in the future you
may not need to specify

00:13:35.100 --> 00:13:38.010
this argument anymore.

00:13:38.010 --> 00:13:40.090
And finally you can
also use TPU strategy

00:13:40.090 --> 00:13:43.080
to scale to cloud TPU pods,
which are, as I mentioned,

00:13:43.080 --> 00:13:44.670
in alpha release right now.

00:13:44.670 --> 00:13:47.850
TPU pods consist of many
clouds TPUs interconnected

00:13:47.850 --> 00:13:49.470
via fast network.

00:13:49.470 --> 00:13:52.320
And this means that AllReduce
across these different all TPU

00:13:52.320 --> 00:13:55.690
pods can be really fast as well.

00:13:55.690 --> 00:13:57.720
So that's all about cloud TPUs.

00:13:57.720 --> 00:14:01.050
I'll hand it off to my colleague
Magnus to talk about scaling

00:14:01.050 --> 00:14:03.701
onto multi-node with GPUs.

00:14:03.701 --> 00:14:06.250
MAGNUS HYTTSTEN: Thank you.

00:14:06.250 --> 00:14:09.390
So that was how we scale
on multiple GPU cores

00:14:09.390 --> 00:14:10.890
from the single node.

00:14:10.890 --> 00:14:14.564
What about multiple nodes the
way we have multiple computers?

00:14:14.564 --> 00:14:16.230
Because the fact is
that even though you

00:14:16.230 --> 00:14:18.940
can cram in a lot of
GPU cards, for example,

00:14:18.940 --> 00:14:21.300
on a single computer,
sooner or later, if you

00:14:21.300 --> 00:14:22.830
do massive amounts
of training, you

00:14:22.830 --> 00:14:25.080
will need to consider
an architecture where

00:14:25.080 --> 00:14:27.490
you can scale out the
multiple nodes as well.

00:14:27.490 --> 00:14:30.480
So this is an example where we
see four worker nodes with four

00:14:30.480 --> 00:14:32.100
GPU cards in each of them.

00:14:34.770 --> 00:14:37.440
In terms of support
for multi-GPU--

00:14:37.440 --> 00:14:39.360
multi-node support,
we have currently

00:14:39.360 --> 00:14:45.300
support for premade estimators
in terms of [INAUDIBLE] 1.11,

00:14:45.300 --> 00:14:47.660
which is subject to
be released shortly.

00:14:47.660 --> 00:14:49.800
And we are working
very, very hard

00:14:49.800 --> 00:14:52.950
with some awesome developers
to get this support into Keras

00:14:52.950 --> 00:14:53.730
as well.

00:14:53.730 --> 00:14:55.987
So you should be aware
that Keras support will

00:14:55.987 --> 00:14:57.195
be there as soon as possible.

00:15:01.190 --> 00:15:04.010
However, if you do
want to use Keras

00:15:04.010 --> 00:15:06.800
with a multi-node
distribution strategy,

00:15:06.800 --> 00:15:09.710
you can actually achieve that
using a little trick that's

00:15:09.710 --> 00:15:14.300
available in the Keras,
and that's called--

00:15:14.300 --> 00:15:17.120
it's a function called
the estimator 2 model.

00:15:17.120 --> 00:15:20.720
estimator 2 model--
the model 2 estimator.

00:15:20.720 --> 00:15:24.500
TF dot Keras estimator-- model
2 estimator that takes a Keras

00:15:24.500 --> 00:15:26.870
model as an argument
and then it actually

00:15:26.870 --> 00:15:28.550
returns an estimator
that you can

00:15:28.550 --> 00:15:29.990
use for multi-node training.

00:15:33.080 --> 00:15:36.500
So how do we set up a
multi-node training environment

00:15:36.500 --> 00:15:37.500
in the first place?

00:15:37.500 --> 00:15:39.620
This was a really,
really difficult problem

00:15:39.620 --> 00:15:42.890
up until the technology
that's open source now

00:15:42.890 --> 00:15:44.820
called Kubernetes was released.

00:15:44.820 --> 00:15:48.020
And so we-- even though you
can set up multi-node training

00:15:48.020 --> 00:15:50.721
with TensorFlow without
running Kubernetes,

00:15:50.721 --> 00:15:54.470
it will certainly help to use
Kubernetes as the orchestration

00:15:54.470 --> 00:15:56.660
platform to fire
up multiple modes.

00:15:56.660 --> 00:16:00.020
And Kubernetes this is
available in most clouds

00:16:00.020 --> 00:16:05.120
GCP and I think AWS
and others as well.

00:16:05.120 --> 00:16:06.140
So how does that work?

00:16:06.140 --> 00:16:10.460
Well, a Kubernetes cluster
contains a set of nodes.

00:16:10.460 --> 00:16:14.560
So in this particular picture,
you can see three nodes.

00:16:14.560 --> 00:16:18.170
In each of them
is a worker node.

00:16:18.170 --> 00:16:21.650
And what TensorFlow requires
in order for this to work

00:16:21.650 --> 00:16:25.260
is that each of these nodes have
an environment variable called

00:16:25.260 --> 00:16:28.160
TF underscore config defined.

00:16:28.160 --> 00:16:31.430
So every single node that
you're having your cluster

00:16:31.430 --> 00:16:33.340
needs to have this
variable defined.

00:16:33.340 --> 00:16:37.010
And in this TF config, you
have two parts, first of all,

00:16:37.010 --> 00:16:39.680
the cluster part,
which defines all

00:16:39.680 --> 00:16:43.640
of the hosts that participates
in the distributed training,

00:16:43.640 --> 00:16:45.500
all the nodes in your cluster.

00:16:45.500 --> 00:16:48.350
And the second one
is really to specify

00:16:48.350 --> 00:16:52.200
who am I. What is my
identity within this cluster?

00:16:52.200 --> 00:16:56.180
So you can see the
task here is 0.

00:16:56.180 --> 00:17:00.220
So this worker is host 1 port 1.

00:17:00.220 --> 00:17:01.060
It's 1.

00:17:01.060 --> 00:17:04.569
That's host 2 port, and
it's 2, meaning that it's

00:17:04.569 --> 00:17:07.329
host 3 and that-- at that port.

00:17:07.329 --> 00:17:10.119
So that's how you need to
configure your cluster in order

00:17:10.119 --> 00:17:13.099
to do this.

00:17:13.099 --> 00:17:16.180
So that is really cumbersome
to go around and round to all

00:17:16.180 --> 00:17:18.099
of the nodes and actually
provide the specific

00:17:18.099 --> 00:17:21.119
configuration and
Kubernetes provides--

00:17:21.119 --> 00:17:22.369
so how do you configure this--

00:17:22.369 --> 00:17:24.940
Kubernetes provides
an excellent way

00:17:24.940 --> 00:17:28.740
of doing that through its
deployment configuration,

00:17:28.740 --> 00:17:30.599
the yaml file, so
you can actually

00:17:30.599 --> 00:17:32.890
distribute the configuration,
the environment variables

00:17:32.890 --> 00:17:35.740
to set on the respective nodes.

00:17:35.740 --> 00:17:38.710
So how do we integrate
that with TensorFlow?

00:17:38.710 --> 00:17:40.650
Well, it's part of
the initial support.

00:17:40.650 --> 00:17:42.225
And this is just
one way of doing it.

00:17:42.225 --> 00:17:43.600
There are multiple
ways, but this

00:17:43.600 --> 00:17:45.880
is one way that we've tested.

00:17:45.880 --> 00:17:48.750
You can use a template
engine called Jinja.

00:17:48.750 --> 00:17:52.630
And you create a file
called a Jinja file,

00:17:52.630 --> 00:17:54.250
and there is
actually such a file

00:17:54.250 --> 00:17:57.850
available in the
TensorFlow slash ecosystem

00:17:57.850 --> 00:18:01.820
repository, observe not
the TensorFlow repository.

00:18:01.820 --> 00:18:03.250
This is the ecosystem.

00:18:03.250 --> 00:18:05.710
There will be a directory
under that repository called

00:18:05.710 --> 00:18:07.750
distribution underscore
strategy that

00:18:07.750 --> 00:18:11.620
contains useful functions to use
with distribution strategies.

00:18:11.620 --> 00:18:14.530
So you can use this
file as a template

00:18:14.530 --> 00:18:16.465
in order to
automatically generate

00:18:16.465 --> 00:18:20.710
the deployment dot yaml
for the Kubernetes cluster.

00:18:20.710 --> 00:18:24.160
So what would that look like
for a configuration like this

00:18:24.160 --> 00:18:26.140
where we have three nodes?

00:18:26.140 --> 00:18:27.530
Well, it's really,
really simple.

00:18:27.530 --> 00:18:30.570
The only thing you need
to do in this file--

00:18:30.570 --> 00:18:34.341
the Jinja file-- is the
highlighted configuration

00:18:34.341 --> 00:18:34.840
up here.

00:18:34.840 --> 00:18:37.630
You set the worker
replicas to three nodes.

00:18:37.630 --> 00:18:41.340
The rest is just code that you
keep for all of the executions

00:18:41.340 --> 00:18:42.550
you setup to do.

00:18:42.550 --> 00:18:43.340
Make sense?

00:18:43.340 --> 00:18:46.630
So this is actually a macro
that populates TF config based

00:18:46.630 --> 00:18:49.790
on this parameter up here.

00:18:49.790 --> 00:18:52.270
So that's very simple,
but what about the code?

00:18:52.270 --> 00:18:55.240
We've now configured
the Kubernetes cluster

00:18:55.240 --> 00:18:56.860
to be able to do
this distributed

00:18:56.860 --> 00:18:58.870
training with TensorFlow,
but there are also

00:18:58.870 --> 00:19:00.610
some stuff we need
to do with the code

00:19:00.610 --> 00:19:04.600
as we had for the
single node as well.

00:19:04.600 --> 00:19:08.730
So it's approximately the same
as for single node, the multi

00:19:08.730 --> 00:19:10.670
GPU configuration.

00:19:10.670 --> 00:19:12.570
So this is the estimator lingo.

00:19:12.570 --> 00:19:14.350
So I provide a config here.

00:19:14.350 --> 00:19:15.310
You see the run config?

00:19:15.310 --> 00:19:18.340
It's just a standard
estimator construct.

00:19:18.340 --> 00:19:21.790
And I set the train
distribute parameter to TF

00:19:21.790 --> 00:19:25.780
dot config distribute collective
AllReduce strategy, so not

00:19:25.780 --> 00:19:28.510
mirrored strategy for
multi-node configuration.

00:19:28.510 --> 00:19:30.860
It's collective
AllReduce strategy.

00:19:30.860 --> 00:19:33.190
And then I specify
the number of GPUs

00:19:33.190 --> 00:19:35.260
I have available for each
of these workers that I

00:19:35.260 --> 00:19:37.720
have my cluster.

00:19:37.720 --> 00:19:38.310
And that's it.

00:19:38.310 --> 00:19:40.020
Given that I have
that config object,

00:19:40.020 --> 00:19:44.400
I can just put that as part
of the config parameter

00:19:44.400 --> 00:19:48.410
when I do the conversion from
Keras over to an estimator.

00:19:48.410 --> 00:19:52.500
And I now have multi-GPU--

00:19:52.500 --> 00:19:56.250
multi-node, multi-GPU
in each of the nodes

00:19:56.250 --> 00:19:58.145
configured for TensorFlow.

00:20:01.260 --> 00:20:04.370
And so let's look at this
collective AllReduce strategy

00:20:04.370 --> 00:20:06.430
because that's something
different than what

00:20:06.430 --> 00:20:09.484
we talked about previously
with a mirrored strategy.

00:20:09.484 --> 00:20:10.400
So what is that thing?

00:20:10.400 --> 00:20:14.170
Well, it is specifically
designed for multiple worker

00:20:14.170 --> 00:20:15.070
nodes.

00:20:15.070 --> 00:20:17.620
And it's essentially based
on mirrored strategy,

00:20:17.620 --> 00:20:19.870
but it adds
functionality in order

00:20:19.870 --> 00:20:24.770
to deal with multi-host or
multi-workers in my cluster.

00:20:24.770 --> 00:20:27.280
And the good thing about
this is that it automatically

00:20:27.280 --> 00:20:32.020
selects the best algorithm
for doing reduce--

00:20:32.020 --> 00:20:35.020
the AllReduce function
across this cluster.

00:20:35.020 --> 00:20:36.470
So what does that mean?

00:20:36.470 --> 00:20:38.500
What kind of
algorithms do we have

00:20:38.500 --> 00:20:44.230
for doing AllReduce in a
multi-node configuration?

00:20:44.230 --> 00:20:45.970
Well, one of them
is very simple--

00:20:45.970 --> 00:20:49.720
very similar to what we have
for a single node, which

00:20:49.720 --> 00:20:53.410
is to ring-all reduce
in which case the GPUs,

00:20:53.410 --> 00:20:55.870
they just travel
across the nodes

00:20:55.870 --> 00:21:00.520
and they perform an overall ring
reduce across multiple hosts

00:21:00.520 --> 00:21:02.055
and GPUs.

00:21:02.055 --> 00:21:04.660
So essentially the same
as for single node.

00:21:04.660 --> 00:21:06.610
It's just that they
are traversing hosts

00:21:06.610 --> 00:21:09.640
with all of the penalties
associated of course

00:21:09.640 --> 00:21:11.470
of doing that depending
on the interconnect

00:21:11.470 --> 00:21:14.020
between these hosts.

00:21:14.020 --> 00:21:17.917
Another algorithm is
hierarchical all reduced.

00:21:17.917 --> 00:21:20.000
I think that this really
complicated English word.

00:21:23.400 --> 00:21:26.770
And what happens here
is that we essentially

00:21:26.770 --> 00:21:31.210
pass all of the variables
up to a single GPU card

00:21:31.210 --> 00:21:32.360
on the respective hosts.

00:21:32.360 --> 00:21:32.860
See that.

00:21:32.860 --> 00:21:35.260
We all send them missing
an error-- two errors over

00:21:35.260 --> 00:21:36.490
here-- with one arrow here.

00:21:36.490 --> 00:21:37.390
Never mind that.

00:21:37.390 --> 00:21:41.710
They're supposed to all send
this stuff to GPU 0, GPU 1.

00:21:41.710 --> 00:21:46.660
And then we do an AllReduce
across the nodes there.

00:21:46.660 --> 00:21:51.010
And the GPUs performing
that operation then

00:21:51.010 --> 00:21:53.230
propagates back to
the individual GPUs

00:21:53.230 --> 00:21:56.470
within its own node.

00:21:56.470 --> 00:21:58.750
So depending on network
and other characteristics

00:21:58.750 --> 00:22:02.110
of your setup and hardware,
one of these solutions

00:22:02.110 --> 00:22:03.340
would work very well.

00:22:03.340 --> 00:22:05.500
And the thing with
collective overdue strategy

00:22:05.500 --> 00:22:08.080
is they will automatically
detect the best algorithm

00:22:08.080 --> 00:22:10.210
to use in your
distributed cluster.

00:22:14.270 --> 00:22:19.460
So that was multi-node,
multi-accelerator cards

00:22:19.460 --> 00:22:20.960
within the nodes.

00:22:20.960 --> 00:22:25.180
There are also other ways
to scale to multiple nodes

00:22:25.180 --> 00:22:26.636
with TensorFlow.

00:22:26.636 --> 00:22:28.010
And one of them--
how many of you

00:22:28.010 --> 00:22:30.950
are familiar with
parameter server strategy?

00:22:30.950 --> 00:22:32.550
Parameter servers?

00:22:32.550 --> 00:22:38.530
This is the classical way of how
you do TensorFlow distributed

00:22:38.530 --> 00:22:39.650
training.

00:22:39.650 --> 00:22:42.020
And eventually this--
actually this way,

00:22:42.020 --> 00:22:46.940
the classical way, you should
not continue to do that.

00:22:46.940 --> 00:22:49.010
You should actually--
once we roll out

00:22:49.010 --> 00:22:52.050
distribution strategies,
that's the way to go.

00:22:52.050 --> 00:22:55.640
So what I'm describing here
is essentially the parameter

00:22:55.640 --> 00:22:57.830
server strategy, but
instead of describing it

00:22:57.830 --> 00:23:00.320
in the old classical
way of doing TensorFlow,

00:23:00.320 --> 00:23:02.660
I'm going to
describe how to do it

00:23:02.660 --> 00:23:04.490
with distribution strategies.

00:23:04.490 --> 00:23:05.730
Does that make sense?

00:23:05.730 --> 00:23:06.230
Yeah.

00:23:06.230 --> 00:23:08.840
If you didn't understand that
and you haven't used TO1,

00:23:08.840 --> 00:23:10.200
just don't worry about it.

00:23:10.200 --> 00:23:13.820
Just listen to what
I have to say here.

00:23:13.820 --> 00:23:18.140
To get a recap of what the
parameter service strategy is,

00:23:18.140 --> 00:23:22.770
it's essentially a strategy
where we have shared storage.

00:23:22.770 --> 00:23:24.620
We have a number
of worker nodes,

00:23:24.620 --> 00:23:27.930
and they're working on
batches of shared stories.

00:23:27.930 --> 00:23:30.000
They're working
completely independently.

00:23:30.000 --> 00:23:31.830
Well, not completely
we'll see shortly.

00:23:31.830 --> 00:23:33.290
But they are working
independently

00:23:33.290 --> 00:23:35.480
calculating gradients
based on batches.

00:23:35.480 --> 00:23:37.700
And then we have a number
of parameter servers.

00:23:37.700 --> 00:23:40.033
So these workers, when they
are finished with the batch,

00:23:40.033 --> 00:23:41.750
they send it up to
the parameter servers.

00:23:41.750 --> 00:23:44.060
The parameter servers,
they have the updates

00:23:44.060 --> 00:23:46.220
from the other workers,
so they calculate

00:23:46.220 --> 00:23:48.230
the average of the
gradients and then pass

00:23:48.230 --> 00:23:51.450
all of those variables
down to the workers.

00:23:51.450 --> 00:23:53.480
So it's not synchronous.

00:23:53.480 --> 00:23:55.280
These workers, they
will get updates

00:23:55.280 --> 00:23:57.920
on the variables in that
synchronous fashion, which

00:23:57.920 --> 00:23:59.660
has good sides and bad sides.

00:23:59.660 --> 00:24:01.940
The good side is one
worker can go out,

00:24:01.940 --> 00:24:06.807
and the other workers can
still execute as normal.

00:24:06.807 --> 00:24:07.890
That's the way this works.

00:24:07.890 --> 00:24:15.650
So how can we set this up in a
distributed strategy cluster?

00:24:15.650 --> 00:24:18.140
Well, it's real easy.

00:24:18.140 --> 00:24:20.850
Instead of just specifying
the worker replicas

00:24:20.850 --> 00:24:23.600
in their Jinja file,
we also specify

00:24:23.600 --> 00:24:25.590
the PS underscore replicas.

00:24:25.590 --> 00:24:28.220
So that's the number
of parameter servers

00:24:28.220 --> 00:24:31.950
that we have in our
Kubernetes cluster.

00:24:31.950 --> 00:24:33.970
So that is the Kubernetes setup.

00:24:33.970 --> 00:24:36.660
Now what about the code?

00:24:36.660 --> 00:24:38.880
So that's also really easy.

00:24:38.880 --> 00:24:41.860
You saw the run config--

00:24:41.860 --> 00:24:43.800
the config parameter previously.

00:24:43.800 --> 00:24:47.010
Instead of using the
collective AllReduce strategy--

00:24:47.010 --> 00:24:49.980
I got that right this time--
collective AllReduce strategy,

00:24:49.980 --> 00:24:52.200
you used the parameter
server strategy.

00:24:52.200 --> 00:24:53.460
See that?

00:24:53.460 --> 00:24:57.240
So it's just another type there.

00:24:57.240 --> 00:24:59.880
You still specified the
number of GPUs per worker,

00:24:59.880 --> 00:25:03.720
you specify the
config object to--

00:25:03.720 --> 00:25:05.400
Keras model to
estimator function

00:25:05.400 --> 00:25:07.710
call, and you're all done.

00:25:07.710 --> 00:25:10.500
So very, very few
lines of code needs

00:25:10.500 --> 00:25:13.200
changing even
though we're talking

00:25:13.200 --> 00:25:17.400
about massively different way of
doing distributed TensorFlow--

00:25:17.400 --> 00:25:20.180
TensorFlow training.

00:25:20.180 --> 00:25:23.660
There is one more configuration
that we are working on.

00:25:23.660 --> 00:25:26.610
I think we will have a release
of this in 1.11 at least

00:25:26.610 --> 00:25:28.900
we can try out.

00:25:28.900 --> 00:25:30.450
That is a really,
really cool setup

00:25:30.450 --> 00:25:33.480
where you actually run
distributed training

00:25:33.480 --> 00:25:34.440
from your laptop.

00:25:34.440 --> 00:25:37.350
And in this particular case, you
have all of your model training

00:25:37.350 --> 00:25:38.430
code here.

00:25:38.430 --> 00:25:40.380
And the only thing you--

00:25:40.380 --> 00:25:43.740
so forget about
parameter server.

00:25:43.740 --> 00:25:46.770
Now we're back to multiple
workers and AllReduce here.

00:25:46.770 --> 00:25:49.350
The only thing you fire
up on these workers

00:25:49.350 --> 00:25:53.610
is the TF underscore STD
underscore server dot pi

00:25:53.610 --> 00:25:55.620
or whatever variant
of that you want

00:25:55.620 --> 00:25:58.350
to use because this
code is available also

00:25:58.350 --> 00:26:00.720
in the TensorFlow
ecosystem repository.

00:26:00.720 --> 00:26:02.670
So you can go check
it out how we did it

00:26:02.670 --> 00:26:05.240
for this normal setup,
and you can change it

00:26:05.240 --> 00:26:06.540
to whatever way you want.

00:26:06.540 --> 00:26:09.320
The thing is that this
script and installation

00:26:09.320 --> 00:26:12.340
of the workers, they don't
have the model program at all.

00:26:12.340 --> 00:26:15.315
So when we fire up the model
training from our laptop

00:26:15.315 --> 00:26:19.630
or workstation here, it will
distribute that model over

00:26:19.630 --> 00:26:20.130
to those.

00:26:20.130 --> 00:26:22.320
So if you have any changes
to your model code,

00:26:22.320 --> 00:26:24.990
you can just make it locally,
and it will automatically

00:26:24.990 --> 00:26:28.050
distribute that out
to all of the workers.

00:26:28.050 --> 00:26:30.870
Now you may say, oh, that's
a hassle because now I've

00:26:30.870 --> 00:26:33.510
got to install this
script on all the workers.

00:26:33.510 --> 00:26:36.600
And you do not have to do that
because the only thing you do

00:26:36.600 --> 00:26:40.574
is just specify the script
parameter in the Jinja file

00:26:40.574 --> 00:26:42.240
that you've seen a
couple of times now--

00:26:42.240 --> 00:26:44.020
and we have the same
number of workers here--

00:26:44.020 --> 00:26:45.894
and that means that the
scripts will actually

00:26:45.894 --> 00:26:47.980
start on all of these nodes.

00:26:47.980 --> 00:26:50.610
So what we're talking about
here is the capability

00:26:50.610 --> 00:26:53.430
to fire up a Kubernetes
cluster with an arbitrary

00:26:53.430 --> 00:26:54.540
number of nodes.

00:26:54.540 --> 00:26:56.380
Without any
installation of code,

00:26:56.380 --> 00:26:58.770
you can use a local laptop,
and it will automatically

00:26:58.770 --> 00:27:00.311
distribute the model
and the training

00:27:00.311 --> 00:27:01.950
to all of these
worker nodes just

00:27:01.950 --> 00:27:05.350
by having these two lines here.

00:27:05.350 --> 00:27:07.780
What about the code?

00:27:07.780 --> 00:27:10.750
So again, we have the
wrong config here.

00:27:10.750 --> 00:27:13.330
And this time,
we're going to set

00:27:13.330 --> 00:27:17.290
a parameter called
experimental distribute

00:27:17.290 --> 00:27:20.100
to the distribute config.

00:27:20.100 --> 00:27:21.820
And as part of
distribute config,

00:27:21.820 --> 00:27:25.530
we are going to embed
a collective AllReduce

00:27:25.530 --> 00:27:29.470
strategy with, as we saw
before, the number of GPUs

00:27:29.470 --> 00:27:30.970
we have per worker.

00:27:30.970 --> 00:27:34.210
But the distributed config
requires one more parameter,

00:27:34.210 --> 00:27:36.880
and that is the remote cluster.

00:27:36.880 --> 00:27:40.450
The cluster-- the
master node here

00:27:40.450 --> 00:27:43.420
needs to know the cluster
to which it should

00:27:43.420 --> 00:27:46.810
send all the model code
for these demos that

00:27:46.810 --> 00:27:49.360
are waiting there for the
model code to be shared.

00:27:49.360 --> 00:27:52.290
Make sense?

00:27:52.290 --> 00:27:53.870
So you gotta specify
that parameters.

00:27:53.870 --> 00:27:55.940
Then you're finishing
up your config object

00:27:55.940 --> 00:27:58.490
in model testimony to
specify the config object.

00:27:58.490 --> 00:28:00.290
And as you've seen
before, it's just

00:28:00.290 --> 00:28:02.060
a couple of lines of
difference between

00:28:02.060 --> 00:28:06.110
these different configurations.

00:28:06.110 --> 00:28:11.960
That's really it for
TensorFlow multi-node training.

00:28:11.960 --> 00:28:15.450
So let's summarize what we've
talked about here today.

00:28:15.450 --> 00:28:18.890
First of all, we went through
the single node distribution

00:28:18.890 --> 00:28:21.140
strategy setup.

00:28:21.140 --> 00:28:24.710
We talked about the mirrored
strategy for multiple GPUs

00:28:24.710 --> 00:28:28.700
within a single node, and we
talked about the TPU strategy

00:28:28.700 --> 00:28:32.300
to distribute work to the TPUs.

00:28:32.300 --> 00:28:35.220
We also went through
the AllReduce algorithm,

00:28:35.220 --> 00:28:37.610
which is used by
distribution strategy

00:28:37.610 --> 00:28:40.490
to be able to do this
single load distribution.

00:28:40.490 --> 00:28:42.770
Then we talked about
multi-node distribution,

00:28:42.770 --> 00:28:46.940
talked about using Kubernetes to
distribute TensorFlow training

00:28:46.940 --> 00:28:51.440
using these Jinja files that
compiles or translates over

00:28:51.440 --> 00:28:53.630
to the yaml file for deployment.

00:28:53.630 --> 00:28:56.000
We talked about
the AllReduce using

00:28:56.000 --> 00:28:58.640
collective AllReduce strategy.

00:28:58.640 --> 00:29:01.490
We talked about the
parameter server setup

00:29:01.490 --> 00:29:03.740
with distribution strategy.

00:29:03.740 --> 00:29:07.430
And then finally we talked
about distributed training

00:29:07.430 --> 00:29:10.912
from a standalone client
distributing the model code

00:29:10.912 --> 00:29:11.745
over to the workers.

00:29:15.240 --> 00:29:17.670
Work in progress.

00:29:17.670 --> 00:29:21.570
So most of the stuff that we
talked about today, you'll

00:29:21.570 --> 00:29:25.230
find TensorFlow dot contrib
dot distribution strategy--

00:29:25.230 --> 00:29:27.450
you'll find that in the
TensorFlow repository.

00:29:27.450 --> 00:29:30.240
But as part of 1.11,
many of these things

00:29:30.240 --> 00:29:34.600
that we talked about you
will be able to start to use.

00:29:34.600 --> 00:29:37.095
If you really want try out,
you can also check out Nike

00:29:37.095 --> 00:29:38.220
and see how far we've gone.

00:29:38.220 --> 00:29:41.760
But 1.11 should be out shortly.

00:29:41.760 --> 00:29:43.980
We are working on
performance still.

00:29:43.980 --> 00:29:45.810
This is always going
to be something

00:29:45.810 --> 00:29:48.570
that we're going to work on
to match state of the art.

00:29:48.570 --> 00:29:51.310
As you saw with
single node multi-GPU,

00:29:51.310 --> 00:29:55.600
we've achieved 90%
to 95% for scaling

00:29:55.600 --> 00:29:57.720
a performance on the GPU cards.

00:29:57.720 --> 00:29:59.539
We're continuously
working on trying

00:29:59.539 --> 00:30:01.830
to improve this for all of
the different configurations

00:30:01.830 --> 00:30:03.760
we've talked about.

00:30:03.760 --> 00:30:09.450
TPU strategy will be available
as part of 1.12 with Keras.

00:30:09.450 --> 00:30:12.180
Right now it's only
available in estimator.

00:30:12.180 --> 00:30:14.980
But remember we have the
estimator trick, model

00:30:14.980 --> 00:30:17.340
to estimator within
Keras, so you can actually

00:30:17.340 --> 00:30:19.670
take a Keras model,
convert to an estimator,

00:30:19.670 --> 00:30:25.180
and still use GPUs strategy
that will be part of 1.11.

00:30:25.180 --> 00:30:28.570
Multi-worker GPU support
is something we're also

00:30:28.570 --> 00:30:30.310
working on as I said.

00:30:30.310 --> 00:30:33.400
So that means that in Keras--

00:30:33.400 --> 00:30:35.530
native Keras code,
we can actually

00:30:35.530 --> 00:30:39.580
specify multi-worker
and GPU support and also

00:30:39.580 --> 00:30:40.450
Eager execution.

00:30:40.450 --> 00:30:43.200
How many of you are familiar
with Eager execution?

00:30:43.200 --> 00:30:44.560
Got to check that out.

00:30:44.560 --> 00:30:48.190
That's a really important
feature of TensorFlow.

00:30:48.190 --> 00:30:50.140
So if you're not
using Eager, you

00:30:50.140 --> 00:30:52.210
should definitely stop
using anything else

00:30:52.210 --> 00:30:54.296
and start using Eager.

00:30:54.296 --> 00:30:56.420
The entire getting started
experience of TensorFlow

00:30:56.420 --> 00:30:58.780
is based on Eager
mode, and we will

00:30:58.780 --> 00:31:02.080
have great performance bridges
between Eager execution

00:31:02.080 --> 00:31:04.730
and graph mode execution and
all of this distribution.

00:31:04.730 --> 00:31:07.690
So the entire architecture
builds on this,

00:31:07.690 --> 00:31:09.160
so you should check it out.

00:31:09.160 --> 00:31:10.840
Eager execution
is also something

00:31:10.840 --> 00:31:13.720
we're working on so you can
directly in Eager execution

00:31:13.720 --> 00:31:17.272
mode utilize multiple GPU
cards in multiple nodes

00:31:17.272 --> 00:31:19.230
in the same way that we
discussed in the setup.

00:31:21.950 --> 00:31:24.920
And then when we have
multi-worker GPUs,

00:31:24.920 --> 00:31:26.660
obviously if one
fails and we talk

00:31:26.660 --> 00:31:32.270
about this AllReduce
synchronous gradient updates,

00:31:32.270 --> 00:31:34.130
we do have a discussion
of fault tolerance.

00:31:34.130 --> 00:31:35.810
So that's something
we're looking into

00:31:35.810 --> 00:31:38.268
to build into this, so we have
more resilience with respect

00:31:38.268 --> 00:31:39.860
to defaults.

00:31:39.860 --> 00:31:45.320
So another summary, what
did we talk about today?

00:31:45.320 --> 00:31:49.920
We talked about the distribution
API, which is very easy to use.

00:31:49.920 --> 00:31:53.060
It's the new way of doing
distributed TensorFlow

00:31:53.060 --> 00:31:54.710
training.

00:31:54.710 --> 00:31:56.780
Forget about anything
that you did before.

00:31:56.780 --> 00:32:00.500
Start to learn about
how to do this.

00:32:00.500 --> 00:32:02.390
We talked about
distribution strategies

00:32:02.390 --> 00:32:05.060
having great performance
right out of the box.

00:32:05.060 --> 00:32:08.510
We saw the scale in
between 1 and 8 GPUs

00:32:08.510 --> 00:32:10.850
on a Kubernetes cluster.

00:32:10.850 --> 00:32:16.190
And then we looked at how
it can scale across GPUs--

00:32:16.190 --> 00:32:21.560
different accelerators,
GPUs as well as TPUs,

00:32:21.560 --> 00:32:26.410
single node as well as
multi-node and TPU pod.

00:32:29.440 --> 00:32:30.440
And that's it.

00:32:30.440 --> 00:32:33.530
You should definitely take
a picture of this slide

00:32:33.530 --> 00:32:37.670
because this slide summarizes
all of the resources

00:32:37.670 --> 00:32:39.650
that we had.

00:32:39.650 --> 00:32:43.490
And with that, we are done.

00:32:43.490 --> 00:32:46.090
Thank you very much
for listening to this.

00:32:46.090 --> 00:32:48.710
If you have any questions--

