WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.270
[MUSIC PLAYING]

00:00:03.770 --> 00:00:04.760
LAURENCE MORONEY: So
the first question

00:00:04.760 --> 00:00:06.552
that comes out, of
course, is that whenever

00:00:06.552 --> 00:00:09.140
you see machine learning or you
hear about machine learning,

00:00:09.140 --> 00:00:11.180
it seems to be like
this magic wand.

00:00:11.180 --> 00:00:13.970
Your boss says, put machine
learning into your application.

00:00:13.970 --> 00:00:16.040
Or if you hear
about startups, they

00:00:16.040 --> 00:00:18.530
put machine learning into
their pitch somewhere.

00:00:18.530 --> 00:00:21.082
And then suddenly, they
become a viable company.

00:00:21.082 --> 00:00:22.290
But what is machine learning?

00:00:22.290 --> 00:00:23.630
What is it really all about?

00:00:23.630 --> 00:00:25.640
And particularly for
coders, what's machine

00:00:25.640 --> 00:00:26.600
learning all about?

00:00:26.600 --> 00:00:29.813
Actually, quick show of hands
if any of you are coders.

00:00:29.813 --> 00:00:31.730
Yeah, it's I/O. I guess
pretty much all of us,

00:00:31.730 --> 00:00:32.480
right, are coders.

00:00:32.480 --> 00:00:33.897
I do talks like
this all the time.

00:00:33.897 --> 00:00:36.022
And sometimes, I'll ask
how many people are coders,

00:00:36.022 --> 00:00:37.430
and three or four hands show up.

00:00:37.430 --> 00:00:41.227
So it's fun that we can geek out
and show a lot of code today.

00:00:41.227 --> 00:00:43.310
So I wanted to talk about
what machine learning is

00:00:43.310 --> 00:00:45.880
from a coding perspective
by picking a scenario.

00:00:45.880 --> 00:00:47.630
Can you imagine if you
were writing a game

00:00:47.630 --> 00:00:49.760
to play rock,
paper, and scissors?

00:00:49.760 --> 00:00:51.380
And you wanted to
write something

00:00:51.380 --> 00:00:54.305
so that you could move your
hand as a rock, a paper,

00:00:54.305 --> 00:00:55.263
or a scissors.

00:00:55.263 --> 00:00:56.930
The computer would
recognize that and be

00:00:56.930 --> 00:00:58.610
able to play that with you.

00:00:58.610 --> 00:01:01.440
Think about what that would
be like to actually write code

00:01:01.440 --> 00:01:02.390
for.

00:01:02.390 --> 00:01:04.940
You'd have to pull in
images from the camera,

00:01:04.940 --> 00:01:08.540
and you'd have to start looking
at the content of those images.

00:01:08.540 --> 00:01:10.130
And how would you
tell the difference

00:01:10.130 --> 00:01:11.810
between a rock and a scissors?

00:01:11.810 --> 00:01:13.310
Or how would you
tell the difference

00:01:13.310 --> 00:01:16.310
between a scissors and a paper?

00:01:16.310 --> 00:01:17.925
That would end up
being a lot of code

00:01:17.925 --> 00:01:19.550
that you would have
to write, and a lot

00:01:19.550 --> 00:01:21.105
of really complicated code.

00:01:21.105 --> 00:01:22.730
And not only the
difference in shapes--

00:01:22.730 --> 00:01:24.830
think about the
difference in skin tones,

00:01:24.830 --> 00:01:28.250
and male hands and female hands,
large hands and small hands,

00:01:28.250 --> 00:01:30.140
people with gnarly
knuckles like me,

00:01:30.140 --> 00:01:33.020
and people with nice
smooth hands like Karmel.

00:01:33.020 --> 00:01:34.460
So how is it that
you would end up

00:01:34.460 --> 00:01:36.293
being able to write all
the code to do this?

00:01:36.293 --> 00:01:38.630
It'd be really, really
complicated and, ultimately,

00:01:38.630 --> 00:01:40.820
not very feasible to write.

00:01:40.820 --> 00:01:42.658
And this is where
we start bringing

00:01:42.658 --> 00:01:43.700
machine learning into it.

00:01:43.700 --> 00:01:45.325
This is a very simple
scenario, but you

00:01:45.325 --> 00:01:47.450
can think about there are
many scenarios where it's

00:01:47.450 --> 00:01:49.550
really difficult to write
code to do something,

00:01:49.550 --> 00:01:51.340
and machine learning
may help in that.

00:01:51.340 --> 00:01:54.030
And I always like to think of
machine learning in this way.

00:01:54.030 --> 00:01:55.598
Think about traditional
programming.

00:01:55.598 --> 00:01:57.890
And in traditional programming,
something that has been

00:01:57.890 --> 00:01:59.432
our bread and butter
for many years--

00:01:59.432 --> 00:02:01.130
all of us here are coders--

00:02:01.130 --> 00:02:04.520
what it is is that we think
about expressing something

00:02:04.520 --> 00:02:06.770
and expressing rules in
a programming language,

00:02:06.770 --> 00:02:10.490
like Java, or Kotlin,
or Swift, or C++.

00:02:10.490 --> 00:02:12.698
And those rules
generally act on data.

00:02:12.698 --> 00:02:14.240
And then out of
that, we get answers.

00:02:14.240 --> 00:02:16.657
Like in rock, paper, scissors,
the data would be an image.

00:02:16.657 --> 00:02:18.260
And my rules would
be all my if-thens

00:02:18.260 --> 00:02:20.870
looking at the pixels in that
image to try and determine

00:02:20.870 --> 00:02:23.270
if something is a rock,
a paper, or a scissors.

00:02:23.270 --> 00:02:25.070
Machine learning then
turns this around.

00:02:25.070 --> 00:02:26.870
It flips the axes on this.

00:02:26.870 --> 00:02:29.240
And we say, hey, instead
of doing it this way

00:02:29.240 --> 00:02:31.840
where it's like we have to
think about all of these rules,

00:02:31.840 --> 00:02:34.740
and we have to write and express
all of these rules in code,

00:02:34.740 --> 00:02:36.590
what if we could provide
a lot of answers,

00:02:36.590 --> 00:02:40.160
and we could label those answers
and then have a machine infer

00:02:40.160 --> 00:02:42.097
the rules that maps
one to the other?

00:02:42.097 --> 00:02:44.180
So for example, in something
like the rock, paper,

00:02:44.180 --> 00:02:47.747
and scissors, we
could say, these

00:02:47.747 --> 00:02:48.830
are the pixels for a rock.

00:02:48.830 --> 00:02:50.430
And this is what
a rock looks like.

00:02:50.430 --> 00:02:52.760
And we could get hundreds
or thousands of images

00:02:52.760 --> 00:02:54.140
of people doing a rock--

00:02:54.140 --> 00:02:57.170
so we get diverse hands,
diverse skin tones,

00:02:57.170 --> 00:02:58.410
those kind of things.

00:02:58.410 --> 00:03:00.937
And we say, hey, this is
what a rock looks like.

00:03:00.937 --> 00:03:02.270
This is what a paper looks like.

00:03:02.270 --> 00:03:04.298
And this is what a
scissors looks like.

00:03:04.298 --> 00:03:05.840
And if a computer
can then figure out

00:03:05.840 --> 00:03:09.090
the patterns between
these and can be taught

00:03:09.090 --> 00:03:11.300
and it can learn what the
patterns is between these,

00:03:11.300 --> 00:03:13.292
now, we have machine learning.

00:03:13.292 --> 00:03:14.750
Now, we have an
application, and we

00:03:14.750 --> 00:03:18.760
have a computer that has
determined these things for us.

00:03:18.760 --> 00:03:22.600
So if we take a look
at this diagram again,

00:03:22.600 --> 00:03:24.630
and if we look at this
again and we replace

00:03:24.630 --> 00:03:27.958
what we've been talking about by
us creating rules, and we say,

00:03:27.958 --> 00:03:30.250
OK, this is machine learning,
we're to feed in answers,

00:03:30.250 --> 00:03:31.650
we're going to feed in
data, and the machine

00:03:31.650 --> 00:03:33.030
is going to infer the rules--

00:03:33.030 --> 00:03:34.780
what's that going to
look like at runtime?

00:03:34.780 --> 00:03:36.360
How can I then
run an application

00:03:36.360 --> 00:03:37.398
that looks like this?

00:03:37.398 --> 00:03:39.690
So this is what we're going
to call the training phase.

00:03:39.690 --> 00:03:42.180
We've trained what's going
to be called a model on this.

00:03:42.180 --> 00:03:44.055
And that model is
basically a neural network.

00:03:44.055 --> 00:03:46.347
And I'm going to be talking
a lot about neural networks

00:03:46.347 --> 00:03:47.522
in the next few minutes.

00:03:47.522 --> 00:03:49.980
But what that neural network
is-- we're going to wrap that.

00:03:49.980 --> 00:03:51.150
We're going to
call that a model.

00:03:51.150 --> 00:03:53.192
And then at runtime, we're
going to pass in data,

00:03:53.192 --> 00:03:55.660
and it's going to give us out
something called predictions.

00:03:55.660 --> 00:03:58.020
So for example, if I've
trained it on lots of rocks,

00:03:58.020 --> 00:04:00.180
lots of papers, and
lots of scissors,

00:04:00.180 --> 00:04:03.247
and then I'm going to hold
my fist up to a webcam,

00:04:03.247 --> 00:04:04.830
it's going to get
the data of my fist.

00:04:04.830 --> 00:04:06.120
And it's going to
give back what we

00:04:06.120 --> 00:04:07.560
like to call a
prediction that'll

00:04:07.560 --> 00:04:10.170
be something like, hey, there's
an 80% chance that's a rock.

00:04:10.170 --> 00:04:12.570
There's a 10% chance it's
a paper and 10% chance

00:04:12.570 --> 00:04:13.410
it's a scissor.

00:04:13.410 --> 00:04:14.512
Something like that.

00:04:14.512 --> 00:04:16.470
So a lot of the terminology
of machine learning

00:04:16.470 --> 00:04:18.787
is a little bit different
from traditional programming.

00:04:18.787 --> 00:04:20.370
We're calling it
training, rather than

00:04:20.370 --> 00:04:21.435
coding and compiling.

00:04:21.435 --> 00:04:23.310
We're calling it inference,
and we're getting

00:04:23.310 --> 00:04:24.518
predictions out of inference.

00:04:24.518 --> 00:04:26.340
So when you hear us
using terms like that,

00:04:26.340 --> 00:04:27.880
that's where it all comes from.

00:04:27.880 --> 00:04:29.880
It's pretty similar to stuff
that you've been doing already

00:04:29.880 --> 00:04:30.900
with traditional coding.

00:04:30.900 --> 00:04:33.010
It's just slightly
different terminology.

00:04:33.010 --> 00:04:34.620
So I'm going to
kick off a demo now

00:04:34.620 --> 00:04:37.650
where I'm going to train a model
for rock, paper, and scissors.

00:04:37.650 --> 00:04:39.340
The demo takes a few
minutes to train,

00:04:39.340 --> 00:04:42.610
so I'm just going to kick it
off before I get back to things.

00:04:42.610 --> 00:04:44.520
So I'm going to start it here.

00:04:44.520 --> 00:04:45.690
And it's starting.

00:04:45.690 --> 00:04:49.170
And as it starts to run, I
just want to show something

00:04:49.170 --> 00:04:49.980
as it goes through.

00:04:49.980 --> 00:04:51.553
So if you can
imagine a computer,

00:04:51.553 --> 00:04:53.970
I'm going to give it a whole
bunch of data of rock, paper,

00:04:53.970 --> 00:04:55.387
and scissors, and
I'm going to ask

00:04:55.387 --> 00:04:57.765
it to see if it can figure
out the rules for rock, paper,

00:04:57.765 --> 00:04:58.670
and scissors.

00:04:58.670 --> 00:05:00.540
So any one individual
item of data

00:05:00.540 --> 00:05:02.190
I give to it, there's
a one in three

00:05:02.190 --> 00:05:04.217
chance it gets it
right first time.

00:05:04.217 --> 00:05:06.300
If it was purely random,
and I said, what is this,

00:05:06.300 --> 00:05:09.330
there's a one in three chance it
would get it correct as a rock.

00:05:09.330 --> 00:05:10.950
So as I start
training, that's one

00:05:10.950 --> 00:05:12.450
of the things I
want you to see here

00:05:12.450 --> 00:05:15.540
is the accuracy that, the
first time through this,

00:05:15.540 --> 00:05:17.400
the accuracy was actually--

00:05:17.400 --> 00:05:19.590
it was exactly 0.333.

00:05:19.590 --> 00:05:22.380
Sometimes, when I run this
demo, it's a little bit more.

00:05:22.380 --> 00:05:24.210
But the idea is once
it started training,

00:05:24.210 --> 00:05:25.600
it's getting that random.

00:05:25.600 --> 00:05:27.930
It's like, OK, I'm just
throwing stuff at random.

00:05:27.930 --> 00:05:29.310
I'm making guesses of this.

00:05:29.310 --> 00:05:31.410
And it was, like,
one in three right.

00:05:31.410 --> 00:05:33.480
As we continue, we'll
see that it's actually

00:05:33.480 --> 00:05:34.890
getting more and more accurate.

00:05:34.890 --> 00:05:37.620
The second time around,
it's now 53% accurate.

00:05:37.620 --> 00:05:39.960
And as it continues, it will
get more and more accurate.

00:05:39.960 --> 00:05:41.340
But I'm going to switch
back to the slides

00:05:41.340 --> 00:05:43.257
and explain what it's
doing before we get back

00:05:43.257 --> 00:05:45.500
to see that finish.

00:05:45.500 --> 00:05:47.690
Can we go back to
the slides, please?

00:05:47.690 --> 00:05:48.650
OK.

00:05:48.650 --> 00:05:51.350
So the code to be able to
write something like this

00:05:51.350 --> 00:05:52.260
looks like this.

00:05:52.260 --> 00:05:53.840
This is a very
simple piece of code

00:05:53.840 --> 00:05:55.760
for creating a neural network.

00:05:55.760 --> 00:05:58.340
And what I want you to
focus on, first of all,

00:05:58.340 --> 00:06:01.050
are these things that I've
outlined in the red box.

00:06:01.050 --> 00:06:04.640
So these are the input to the
neural network and the output

00:06:04.640 --> 00:06:06.340
coming from the neural network.

00:06:06.340 --> 00:06:08.298
That's why I love talking
about neural networks

00:06:08.298 --> 00:06:09.930
at I/O, because
I/O, Input/Output.

00:06:09.930 --> 00:06:11.810
And you'll see I'll talk a
lot about inputs and outputs

00:06:11.810 --> 00:06:12.410
in this.

00:06:12.410 --> 00:06:15.423
So the input to this is
the size of the images.

00:06:15.423 --> 00:06:17.090
All of the images
that I'm going to feed

00:06:17.090 --> 00:06:19.257
to the neural network of
rocks, papers, and scissors

00:06:19.257 --> 00:06:22.550
are 150 square, and they're
a 3-byte color depth.

00:06:22.550 --> 00:06:25.327
And that's why you
see 150 by 150 by 3.

00:06:25.327 --> 00:06:26.910
And then the output
from this is going

00:06:26.910 --> 00:06:28.743
to be three things,
because we're

00:06:28.743 --> 00:06:30.410
classifying for three
different things--

00:06:30.410 --> 00:06:31.747
a rock, a paper, or a scissors.

00:06:31.747 --> 00:06:33.830
So always when you're
looking at a neural network,

00:06:33.830 --> 00:06:35.300
those are really the
first things to look at.

00:06:35.300 --> 00:06:36.080
What are my inputs?

00:06:36.080 --> 00:06:36.913
What are my outputs?

00:06:36.913 --> 00:06:38.367
What do they look like?

00:06:38.367 --> 00:06:39.950
But then there's
this mysterious thing

00:06:39.950 --> 00:06:41.690
in the middle
where we've created

00:06:41.690 --> 00:06:45.655
this tf.keras.layers.Dense,
and there's a 512 there.

00:06:45.655 --> 00:06:47.030
And a lot of people
wonder, well,

00:06:47.030 --> 00:06:49.190
what are those 512 things?

00:06:49.190 --> 00:06:51.260
Well, let me try and
explain that visually.

00:06:51.260 --> 00:06:55.670
So visually, what's going on
is what those 512 things are

00:06:55.670 --> 00:06:57.530
in the center of this diagram--

00:06:57.530 --> 00:07:00.350
consider them to
be 512 functions.

00:07:00.350 --> 00:07:03.410
And those functions all
have internal variables.

00:07:03.410 --> 00:07:05.540
And those internal
variables are just

00:07:05.540 --> 00:07:08.460
going to be initialized
with some random states.

00:07:08.460 --> 00:07:10.940
But what we want to
do is when we start

00:07:10.940 --> 00:07:14.210
passing the pixels from
the images into these,

00:07:14.210 --> 00:07:16.070
we want them to
try and figure out

00:07:16.070 --> 00:07:18.890
what kind of output,
based on those inputs,

00:07:18.890 --> 00:07:21.440
will give me the desired
output at the bottom?

00:07:21.440 --> 00:07:24.410
So function 0 is going
to grab all those pixels.

00:07:24.410 --> 00:07:26.780
Function 1 is going to
grab all those pixels.

00:07:26.780 --> 00:07:29.360
Function 2 is going to
grab all those pixels.

00:07:29.360 --> 00:07:31.550
And if those pixels are
the shape of a rock,

00:07:31.550 --> 00:07:33.440
then we want the output
of function 0, 1,

00:07:33.440 --> 00:07:36.560
and 2 all the way up
to 511 to be outputting

00:07:36.560 --> 00:07:39.200
to the box on the left at
the bottom-- to stick a 1

00:07:39.200 --> 00:07:40.370
in that box.

00:07:40.370 --> 00:07:42.060
And similarly for paper.

00:07:42.060 --> 00:07:44.550
If we say, OK, when the
pixels look like this,

00:07:44.550 --> 00:07:48.820
we want your outputs of F0,
F1, and F2 to go to this box.

00:07:48.820 --> 00:07:50.428
And that's the
process of learning.

00:07:50.428 --> 00:07:52.220
So all that's happening--
all that learning

00:07:52.220 --> 00:07:53.845
is when we talk about
machine learning,

00:07:53.845 --> 00:07:57.150
is setting those internal
variables in those functions

00:07:57.150 --> 00:07:59.192
so we get that desired output.

00:07:59.192 --> 00:08:00.650
Now, those internal
variables, just

00:08:00.650 --> 00:08:02.150
to confuse things
a little bit more,

00:08:02.150 --> 00:08:04.970
in machine learning parlance,
tend to be called parameters.

00:08:04.970 --> 00:08:08.468
And so for me, as a programmer,
it was hard at first

00:08:08.468 --> 00:08:09.260
to understand that.

00:08:09.260 --> 00:08:10.927
Because for me,
parameters are something

00:08:10.927 --> 00:08:12.390
I pass into a function.

00:08:12.390 --> 00:08:14.390
But in this case, when you hear
a machine learning person talk

00:08:14.390 --> 00:08:16.040
about parameters,
those are the values

00:08:16.040 --> 00:08:18.280
inside those functions
that are going to get set

00:08:18.280 --> 00:08:20.150
and going to get
changed as it tries

00:08:20.150 --> 00:08:24.512
to learn how I'm going to match
those inputs to those outputs.

00:08:24.512 --> 00:08:25.970
So if I go back to
the code and try

00:08:25.970 --> 00:08:27.740
to show this again in action--

00:08:27.740 --> 00:08:30.380
now, remember, my input shape
that I spoke about earlier on,

00:08:30.380 --> 00:08:32.951
the 150 by 150 by 3,
those are the pixels

00:08:32.951 --> 00:08:34.159
that I showed in the preview.

00:08:34.159 --> 00:08:36.447
I'm simulating them
here with gray boxes,

00:08:36.447 --> 00:08:38.030
but those are the
pixels that I showed

00:08:38.030 --> 00:08:39.830
in the previous diagrams.

00:08:39.830 --> 00:08:44.100
My functions, now, is that dense
layer in the middle, those 512.

00:08:44.100 --> 00:08:47.210
So that's 512 functions
randomly initialized

00:08:47.210 --> 00:08:49.790
or semi-randomly
initialized that I'm

00:08:49.790 --> 00:08:52.985
going to try to train to
match my inputs to my outputs.

00:08:52.985 --> 00:08:54.860
And then, of course,
the bottom-- those three

00:08:54.860 --> 00:08:57.680
are the three neurons that
are going to be my outputs.

00:08:57.680 --> 00:08:59.990
And I've just said the word
neuron for the first time.

00:08:59.990 --> 00:09:02.240
But ultimately, when
we talk about neurons

00:09:02.240 --> 00:09:05.540
and neural networks,
it's not really anything

00:09:05.540 --> 00:09:06.680
to do with the human brain.

00:09:06.680 --> 00:09:08.420
It's a very rough
simulation of how

00:09:08.420 --> 00:09:10.010
the human brain does things.

00:09:10.010 --> 00:09:12.080
And these internal functions
that try and figure

00:09:12.080 --> 00:09:14.140
out how to match the
inputs to the outputs,

00:09:14.140 --> 00:09:15.790
we call those neurons.

00:09:15.790 --> 00:09:17.995
And on my output, those
three at the bottom

00:09:17.995 --> 00:09:19.370
are also going to
be neurons too.

00:09:19.370 --> 00:09:22.010
And that's what lends the name
"neural networks" to this.

00:09:22.010 --> 00:09:24.620
It tends to sound a little
bit mysterious and special

00:09:24.620 --> 00:09:25.830
when we call it like that.

00:09:25.830 --> 00:09:27.372
But ultimately, just
think about them

00:09:27.372 --> 00:09:29.935
as functions with randomly
initialize variables

00:09:29.935 --> 00:09:31.310
that, over time,
are going to try

00:09:31.310 --> 00:09:33.350
to change the value
of those variables

00:09:33.350 --> 00:09:36.740
so that the inputs match
our desired outputs.

00:09:36.740 --> 00:09:39.693
So then there's this line,
the model.compile line.

00:09:39.693 --> 00:09:40.860
And what's that going to do?

00:09:40.860 --> 00:09:43.340
That's a kind of fancy term.

00:09:43.340 --> 00:09:45.020
It's not really
doing compilation

00:09:45.020 --> 00:09:47.300
where we're turning code
into bytecode as before.

00:09:47.300 --> 00:09:49.138
But think about the
two parameters to this.

00:09:49.138 --> 00:09:50.680
And these are the
most important part

00:09:50.680 --> 00:09:52.347
to learn in machine
learning-- and these

00:09:52.347 --> 00:09:54.320
are the loss and the optimizer.

00:09:54.320 --> 00:09:56.300
So the idea is the
job of these two

00:09:56.300 --> 00:09:57.830
is-- remember,
earlier on, I said

00:09:57.830 --> 00:10:00.780
it's going to randomly
initialize all those functions.

00:10:00.780 --> 00:10:02.250
And if they're
randomly initialized

00:10:02.250 --> 00:10:04.208
and I pass in something
that looks like a rock,

00:10:04.208 --> 00:10:05.780
there's a one in
three chance it's

00:10:05.780 --> 00:10:09.350
going to get it right as a
rock, or a paper, or scissors.

00:10:09.350 --> 00:10:10.760
So what the Loss
function does is

00:10:10.760 --> 00:10:13.580
it measures the results of
all the thousands of times

00:10:13.580 --> 00:10:14.510
I do that.

00:10:14.510 --> 00:10:16.910
It figures out how well
or how badly it did.

00:10:16.910 --> 00:10:19.610
And then based on that,
it passes that data

00:10:19.610 --> 00:10:22.430
to the other function, which is
called the Optimizer function.

00:10:22.430 --> 00:10:25.850
And the Optimizer function
then generates the next guess

00:10:25.850 --> 00:10:30.320
where the guess is set to be
the parameters of those 512

00:10:30.320 --> 00:10:32.880
little functions,
those 512 neurons.

00:10:32.880 --> 00:10:36.020
And if we keep repeating
this, we'll pass our data in.

00:10:36.020 --> 00:10:36.980
We'll take a look.

00:10:36.980 --> 00:10:37.910
We'll make a guess.

00:10:37.910 --> 00:10:39.642
We'll see how well
or how badly we did.

00:10:39.642 --> 00:10:41.100
Then based on that,
we'll optimize,

00:10:41.100 --> 00:10:42.380
and we'll make another guess.

00:10:42.380 --> 00:10:44.030
And we'll repeat,
repeat, repeat,

00:10:44.030 --> 00:10:46.510
until our guesses get better,
and better, and better.

00:10:46.510 --> 00:10:48.600
And that's what happens
in the model.fit.

00:10:48.600 --> 00:10:51.060
Here, you can see I have
model.fit where epochs--

00:10:51.060 --> 00:10:53.910
"ee-pocks," "epics," depending
on how you pronounce it--

00:10:53.910 --> 00:10:54.750
is 100.

00:10:54.750 --> 00:10:56.820
All it's doing is doing
that cycle 100 times.

00:10:56.820 --> 00:10:59.280
For every image, take a
look at the parameters.

00:10:59.280 --> 00:11:00.870
fit those parameters.

00:11:00.870 --> 00:11:01.560
Take a guess.

00:11:01.560 --> 00:11:03.420
Measure how good
or how bad you did,

00:11:03.420 --> 00:11:05.232
and then repeat and keep going.

00:11:05.232 --> 00:11:06.690
And the optimizer
then will make it

00:11:06.690 --> 00:11:08.040
better and better and better.

00:11:08.040 --> 00:11:09.300
So you can imagine the
first time through,

00:11:09.300 --> 00:11:11.550
you're going to get it right
roughly one in three times.

00:11:11.550 --> 00:11:13.008
Subsequent times,
it's going to get

00:11:13.008 --> 00:11:16.890
closer, and closer, and
closer, and better, and better.

00:11:16.890 --> 00:11:19.200
Now, those of us who know
a little bit about images

00:11:19.200 --> 00:11:21.540
and image processing
go, OK, that's nice,

00:11:21.540 --> 00:11:23.200
but it's a little naive.

00:11:23.200 --> 00:11:25.440
I'm just throwing all of
the pixels of the image--

00:11:25.440 --> 00:11:27.900
and maybe a lot of these
pixels aren't even set--

00:11:27.900 --> 00:11:29.790
into a neural
network and having it

00:11:29.790 --> 00:11:31.890
try to figure out from
those pixel values.

00:11:31.890 --> 00:11:34.010
Can I do it a little
bit smarter than that?

00:11:34.010 --> 00:11:36.030
And the answer to that is, yes.

00:11:36.030 --> 00:11:37.950
And one of the ways
that we can do it

00:11:37.950 --> 00:11:39.992
a little bit smarter than
that is using something

00:11:39.992 --> 00:11:41.220
called convolutions.

00:11:41.220 --> 00:11:43.290
Now, convolutions is
a convoluted term,

00:11:43.290 --> 00:11:44.880
if you'll excuse the pun.

00:11:44.880 --> 00:11:47.160
But the idea behind
convolutions is

00:11:47.160 --> 00:11:49.830
if you've ever done any
kind of image processing,

00:11:49.830 --> 00:11:52.680
the way you can sharpen
images or soften images

00:11:52.680 --> 00:11:55.697
with things like Photoshop,
it's exactly the same thing.

00:11:55.697 --> 00:11:57.780
So with a convolution, the
idea is you take a look

00:11:57.780 --> 00:11:59.710
at every pixel in the image.

00:11:59.710 --> 00:12:01.860
So for example, this
picture of a hand, and I'm

00:12:01.860 --> 00:12:05.470
just looking at one of the
pixels on the fingernail.

00:12:05.470 --> 00:12:09.898
And so that pixel is value 192
in the box on the left here.

00:12:09.898 --> 00:12:11.940
So if you take a look at
every pixel in the image

00:12:11.940 --> 00:12:14.100
and you look at its
immediate neighbors,

00:12:14.100 --> 00:12:16.590
and then you get something
called a filter, which

00:12:16.590 --> 00:12:18.090
is the gray box on the right.

00:12:18.090 --> 00:12:21.540
And you multiply out
the value of the pixel

00:12:21.540 --> 00:12:23.610
by the corresponding
value in the filter.

00:12:23.610 --> 00:12:25.980
And you do that for all
of the pixel's neighbors

00:12:25.980 --> 00:12:27.990
to get a new value
for the pixel.

00:12:27.990 --> 00:12:29.930
That's what a convolution is.

00:12:29.930 --> 00:12:31.680
Now, many of us, if
you've never done this

00:12:31.680 --> 00:12:34.097
before, you might be sitting
around thinking, why on earth

00:12:34.097 --> 00:12:35.050
would I do that?

00:12:35.050 --> 00:12:37.530
Well, the reason for that is
that when finding convolutions

00:12:37.530 --> 00:12:39.490
and finding filters,
it becomes really,

00:12:39.490 --> 00:12:42.700
really good at extracting
features in an image.

00:12:42.700 --> 00:12:44.230
So let me give an example.

00:12:44.230 --> 00:12:46.380
So if you look at the
image on the left here

00:12:46.380 --> 00:12:49.020
and I apply a filter
like this one,

00:12:49.020 --> 00:12:50.565
I will get the
image on the right.

00:12:50.565 --> 00:12:52.440
Now, what has happened
here is that the image

00:12:52.440 --> 00:12:55.650
on the left, I've thrown away a
lot of the noise in the image.

00:12:55.650 --> 00:12:58.450
And I've been able to
detect vertical lines.

00:12:58.450 --> 00:13:00.810
So just simply by applying
a filter like this,

00:13:00.810 --> 00:13:04.200
vertical lines are surviving
through the multiplication

00:13:04.200 --> 00:13:05.040
of the filter.

00:13:05.040 --> 00:13:07.290
And then similarly, if I
apply a filter like this one,

00:13:07.290 --> 00:13:09.105
horizontal lines survive.

00:13:09.105 --> 00:13:10.730
And there are lots
of filters out there

00:13:10.730 --> 00:13:12.438
that can be randomly
initialized and that

00:13:12.438 --> 00:13:15.420
can be learned that do
things like picking out

00:13:15.420 --> 00:13:19.260
items in an image, like
eyes, or ears, or fingers,

00:13:19.260 --> 00:13:21.430
or fingernails, and
things like that.

00:13:21.430 --> 00:13:23.993
So that's the idea
behind convolutions.

00:13:23.993 --> 00:13:25.410
Now, the next thing
is, OK, if I'm

00:13:25.410 --> 00:13:27.030
going to be doing
lots of processing

00:13:27.030 --> 00:13:29.520
on my image like this, and I'm
going to be doing training,

00:13:29.520 --> 00:13:31.740
and I'm going to have to
have hundreds of filters

00:13:31.740 --> 00:13:34.595
to try and pick out different
features in my image, that's

00:13:34.595 --> 00:13:36.720
going to be a lot of data
that I have to deal with.

00:13:36.720 --> 00:13:39.760
And wouldn't it be nice if
I could compress my images?

00:13:39.760 --> 00:13:41.670
So compression is
achieved through something

00:13:41.670 --> 00:13:43.080
called pooling.

00:13:43.080 --> 00:13:46.620
And it's a very,
very simple thing.

00:13:46.620 --> 00:13:48.330
Sometimes, it seems
a very complex term

00:13:48.330 --> 00:13:49.950
to describe something simple.

00:13:49.950 --> 00:13:51.810
But when we talk
about pooling, I'm

00:13:51.810 --> 00:13:54.802
going to apply, for example,
a 2 x 2 pool to an image.

00:13:54.802 --> 00:13:56.510
And what that's going
to do is it's going

00:13:56.510 --> 00:13:58.560
to take the pixels 2 x 2--

00:13:58.560 --> 00:14:00.090
like if you look
at my left here,

00:14:00.090 --> 00:14:02.370
if I've got 16
simulated pixels--

00:14:02.370 --> 00:14:05.400
I'm going to take the top four
in the top left-hand corner.

00:14:05.400 --> 00:14:07.830
And of those four, I'm going
to pick the biggest value.

00:14:07.830 --> 00:14:09.720
And then the next four on
the top right-hand corner,

00:14:09.720 --> 00:14:11.580
of those four, I'll
pick the biggest value,

00:14:11.580 --> 00:14:13.270
and so on, and so on.

00:14:13.270 --> 00:14:15.810
So what that's going to do
is effectively throw away

00:14:15.810 --> 00:14:20.040
75% of my pixels and just keep
the maximums in each of these 2

00:14:20.040 --> 00:14:22.710
x 2 little units.

00:14:22.710 --> 00:14:24.570
But the impact of that
is really interesting

00:14:24.570 --> 00:14:26.820
when we start combining
it with convolutions.

00:14:26.820 --> 00:14:28.980
So if you look at the image
that I created earlier

00:14:28.980 --> 00:14:32.010
on where I applied the filter
to that image of a person

00:14:32.010 --> 00:14:34.980
walking up the stairs,
and then I pool that,

00:14:34.980 --> 00:14:37.770
I get the image that's on
the right, which is 1/4

00:14:37.770 --> 00:14:39.900
the size of the original image.

00:14:39.900 --> 00:14:42.600
But not only is it not
losing any vital information,

00:14:42.600 --> 00:14:45.030
it's even enhancing some of
the vital information that

00:14:45.030 --> 00:14:46.060
came out of it.

00:14:46.060 --> 00:14:49.080
So pooling is your friend when
you start using convolutions

00:14:49.080 --> 00:14:50.997
because, if you have 128
filters, for example,

00:14:50.997 --> 00:14:52.497
that you apply to
your image, you're

00:14:52.497 --> 00:14:54.450
going to have 128
copies of your image.

00:14:54.450 --> 00:14:56.590
You're going to have
128 times the data.

00:14:56.590 --> 00:14:58.770
And when you're dealing
with thousands of images,

00:14:58.770 --> 00:15:01.567
that's going to slow down your
training time really fast.

00:15:01.567 --> 00:15:03.150
But pooling, then,
really speeds it up

00:15:03.150 --> 00:15:05.830
by shrinking the
size of your image.

00:15:05.830 --> 00:15:08.740
So now, when we want to start
learning with a neural network,

00:15:08.740 --> 00:15:11.520
now it's a case of, hey,
I've got my image at the top,

00:15:11.520 --> 00:15:13.560
I can start applying
convolutions to that.

00:15:13.560 --> 00:15:16.080
Like for example, my image
might be a smiley face.

00:15:16.080 --> 00:15:18.840
And one convolution will
keep it as a smiley face.

00:15:18.840 --> 00:15:21.450
Another one might keep the
circle outline of a head.

00:15:21.450 --> 00:15:23.280
Another one might kind
of change the shape

00:15:23.280 --> 00:15:24.960
of the head, things like that.

00:15:24.960 --> 00:15:27.600
And as I start applying more
and more convolutions to these

00:15:27.600 --> 00:15:30.810
and getting smaller and smaller
images, instead of me now

00:15:30.810 --> 00:15:33.703
having a big, fat image
that I'm trying to classify,

00:15:33.703 --> 00:15:36.120
that I'm trying to pick out
the features of to learn from,

00:15:36.120 --> 00:15:39.900
I can have lots of little images
highlighting features in that.

00:15:39.900 --> 00:15:42.000
So for example, in
rock, paper, scissors,

00:15:42.000 --> 00:15:45.690
my convolutions might show,
in some cases, five fingers,

00:15:45.690 --> 00:15:47.190
or four fingers and a thumb.

00:15:47.190 --> 00:15:49.650
And I know that that's
going to be a paper.

00:15:49.650 --> 00:15:52.530
Or it might show none, and I
know that's going to be a rock.

00:15:52.530 --> 00:15:55.320
And it then begins to make the
process of the machine learning

00:15:55.320 --> 00:15:57.858
these much, much simpler.

00:15:57.858 --> 00:15:58.900
So to show this quickly--

00:15:58.900 --> 00:16:01.390
I've been putting QR codes
on these slides, by the way.

00:16:01.390 --> 00:16:03.780
So I've open-sourced all the
code that I'm showing here

00:16:03.780 --> 00:16:05.020
and we're talking through.

00:16:05.020 --> 00:16:06.713
And this is a QR
code to a workbook

00:16:06.713 --> 00:16:08.130
where you can train
a rock, paper,

00:16:08.130 --> 00:16:10.050
scissors model for yourself.

00:16:10.050 --> 00:16:13.920
But once we do convolutions--
and earlier in the slide,

00:16:13.920 --> 00:16:15.930
you saw I had multiple
convolutions moving down.

00:16:15.930 --> 00:16:17.940
And this is what the code
for that would look like.

00:16:17.940 --> 00:16:20.232
I just have a convolution
layer, followed by a pooling.

00:16:20.232 --> 00:16:22.028
Another convolution,
followed by a pooling.

00:16:22.028 --> 00:16:23.820
Another convolution,
followed by a pooling,

00:16:23.820 --> 00:16:25.210
et cetera, et cetera.

00:16:25.210 --> 00:16:27.420
So the impact of that--
and remember, first of all,

00:16:27.420 --> 00:16:29.250
at the top, I have
my input shape,

00:16:29.250 --> 00:16:31.230
and I have my
output at the bottom

00:16:31.230 --> 00:16:32.670
where the Dense equals 3.

00:16:32.670 --> 00:16:35.010
So I'm going to switch
back to the demo

00:16:35.010 --> 00:16:39.860
now to see if it's
finished training.

00:16:39.860 --> 00:16:40.920
And we can see it.

00:16:40.920 --> 00:16:43.310
So we started off
with 33% accuracy.

00:16:43.310 --> 00:16:45.230
But as we went
through the epochs--

00:16:45.230 --> 00:16:47.780
I just did this one, I
think, for 15 epochs--

00:16:47.780 --> 00:16:51.270
it got steadily, and steadily,
and steadily more accurate.

00:16:51.270 --> 00:16:55.820
So after 15 loops of doing
this, it's now 96.83% accurate.

00:16:55.820 --> 00:16:58.040
So as a result, we can see,
using these techniques,

00:16:58.040 --> 00:17:00.080
using convolutions
like this, we've

00:17:00.080 --> 00:17:02.900
been actually able to train
something in just a few minutes

00:17:02.900 --> 00:17:06.319
to be roughly 97% accurate
at detecting rock, paper,

00:17:06.319 --> 00:17:07.339
and scissors.

00:17:07.339 --> 00:17:11.119
And if I just take a quick
plot here, we can see this

00:17:11.119 --> 00:17:14.210
is a plot of that accuracy-- the
red line showing the accuracy

00:17:14.210 --> 00:17:16.609
where we started at roughly 33%.

00:17:16.609 --> 00:17:18.530
And we're getting close to 100%.

00:17:18.530 --> 00:17:20.829
The blue line is I
have a separate data

00:17:20.829 --> 00:17:23.944
set of rock, paper, scissors
that I tested with, just

00:17:23.944 --> 00:17:25.069
to see how well it's doing.

00:17:25.069 --> 00:17:26.540
And it's pretty close.

00:17:26.540 --> 00:17:28.670
I need to do a little bit
of work in tweaking it.

00:17:28.670 --> 00:17:32.340
And I can actually try
an example to show you.

00:17:32.340 --> 00:17:34.180
So I'm going to upload a file.

00:17:34.180 --> 00:17:36.200
I'm going to choose a
file from my computer.

00:17:36.200 --> 00:17:38.620
I've nicely named
that file Paper,

00:17:38.620 --> 00:17:40.370
so you can guess it's a paper.

00:17:40.370 --> 00:17:42.737
And if I open that
and upload that,

00:17:42.737 --> 00:17:43.820
it's going to upload that.

00:17:43.820 --> 00:17:45.690
And then it's going
to give me an output.

00:17:45.690 --> 00:17:47.095
And the output is 1, 0, 0.

00:17:47.095 --> 00:17:48.470
So you think, ah,
I got it wrong.

00:17:48.470 --> 00:17:49.940
It detected it's a rock.

00:17:49.940 --> 00:17:52.460
But actually, my neurons
here, based on the labels,

00:17:52.460 --> 00:17:54.013
are in alphabetical order.

00:17:54.013 --> 00:17:56.180
So the alphabetical order
would be paper, then rock,

00:17:56.180 --> 00:17:57.140
then scissors.

00:17:57.140 --> 00:18:00.720
So it actually classified that
correctly by giving me a 1.

00:18:00.720 --> 00:18:01.790
So it's actually a paper.

00:18:01.790 --> 00:18:04.520
And we can try
another one at random.

00:18:04.520 --> 00:18:06.020
I'll choose a file
from my machine.

00:18:06.020 --> 00:18:10.120
I'll choose a scissors
and open that and run it.

00:18:10.120 --> 00:18:11.660
And again, paper,
rock, scissors,

00:18:11.660 --> 00:18:14.010
so we see it actually
classified that correctly.

00:18:14.010 --> 00:18:15.440
So this workbook
is online if you

00:18:15.440 --> 00:18:17.232
want to download it
and have a play with it

00:18:17.232 --> 00:18:19.070
to do classification
yourself and to see

00:18:19.070 --> 00:18:21.823
how easy it is for you to train
a neural network to do this.

00:18:21.823 --> 00:18:23.240
And then once you
have that model,

00:18:23.240 --> 00:18:25.910
you can implement that
model in your applications

00:18:25.910 --> 00:18:28.340
and then maybe play rock,
paper, scissors in your apps.

00:18:28.340 --> 00:18:30.048
Can we switch back to
the slides, please?

00:18:34.270 --> 00:18:38.500
So just to quickly show the
idea of how convolutions really

00:18:38.500 --> 00:18:41.440
help you with an image,
this is what that model

00:18:41.440 --> 00:18:43.330
looks like when I defined it.

00:18:43.330 --> 00:18:46.210
And at the top here, it might
look like a little bit of a bug

00:18:46.210 --> 00:18:48.293
at first, if you're
not used to doing this.

00:18:48.293 --> 00:18:50.710
But at the top here-- remember,
we said my image is coming

00:18:50.710 --> 00:18:52.670
in 150 x 150--

00:18:52.670 --> 00:18:54.670
it's actually saying,
hey, I'm going to pass out

00:18:54.670 --> 00:18:57.160
an image that's 148 x 148.

00:18:57.160 --> 00:18:59.710
Anybody know why?

00:18:59.710 --> 00:19:01.510
Is it a bug?

00:19:01.510 --> 00:19:02.350
No, it's not a bug.

00:19:02.350 --> 00:19:02.850
OK.

00:19:02.850 --> 00:19:05.080
So the reason why
is if my filter was

00:19:05.080 --> 00:19:10.210
3 x 3, for me to be able to look
at a pixel, I have to throw--

00:19:10.210 --> 00:19:12.100
for me to start on
the image, I have

00:19:12.100 --> 00:19:15.250
to start one pixel in and one
pixel down in order for it

00:19:15.250 --> 00:19:16.275
to have neighbors.

00:19:16.275 --> 00:19:18.400
So as a result, I have to
throw away all the pixels

00:19:18.400 --> 00:19:21.310
at the top, at the bottom,
and either side of my image.

00:19:21.310 --> 00:19:23.650
So I'm losing one
pixel on all sides.

00:19:23.650 --> 00:19:27.520
So my 150 x 150
becomes a 148 x 148.

00:19:27.520 --> 00:19:30.510
And then when I pool that,
I halved each of the axes.

00:19:30.510 --> 00:19:32.860
So it becomes 74 x 74.

00:19:32.860 --> 00:19:35.980
Then through the next iteration,
it becomes 36 x 36, then

00:19:35.980 --> 00:19:39.080
17 x 17, and then 7 x 7.

00:19:39.080 --> 00:19:41.620
So if you think about all
of these 150 squared images

00:19:41.620 --> 00:19:43.510
passing through all
of these convolutions

00:19:43.510 --> 00:19:46.690
are coming up with lots
of little 7 x 7 things.

00:19:46.690 --> 00:19:48.190
And those little 7
x 7 things should

00:19:48.190 --> 00:19:49.660
be highlighting a feature--

00:19:49.660 --> 00:19:50.990
it might be a fingernail.

00:19:50.990 --> 00:19:51.850
It might be a thumb.

00:19:51.850 --> 00:19:53.440
It might be a shape of a hand.

00:19:53.440 --> 00:19:56.590
And then those features that
come through the convolutions

00:19:56.590 --> 00:19:58.420
are then passed into
the neural network

00:19:58.420 --> 00:20:00.805
that we saw earlier on to
generate those parameters.

00:20:00.805 --> 00:20:02.680
And then from those
parameters, hopefully, it

00:20:02.680 --> 00:20:04.780
would make a guess, and
a really accurate guess,

00:20:04.780 --> 00:20:07.510
about something being a
rock, a paper, or scissors.

00:20:07.510 --> 00:20:10.750
So if you prefer an IDE
instead of using Collab,

00:20:10.750 --> 00:20:11.800
you can do that also.

00:20:11.800 --> 00:20:15.040
I tend to really like to use
PyCharm for my developments.

00:20:15.040 --> 00:20:17.150
Any PyCharm fans
here, out of interest?

00:20:17.150 --> 00:20:17.870
Yeah, nice.

00:20:17.870 --> 00:20:19.190
A lot of you.

00:20:19.190 --> 00:20:21.060
So here's a
screenshot of PyCharm

00:20:21.060 --> 00:20:23.560
when I was writing this rock,
paper, scissors thing before I

00:20:23.560 --> 00:20:27.730
pasted it over to Collab, where
you can run it from Collab.

00:20:27.730 --> 00:20:29.200
So PyCharm is
really, really nice.

00:20:29.200 --> 00:20:31.420
And you can do things like
step-by-step debugging.

00:20:31.420 --> 00:20:33.780
If we can switch to the
demo machine for a moment.

00:20:38.400 --> 00:20:40.430
Now, I'll do a quick
demo of PyCharm

00:20:40.430 --> 00:20:42.020
doing step-by-step debugging.

00:20:42.020 --> 00:20:44.630
So here, we can see we're
in rock, paper, scissors.

00:20:44.630 --> 00:20:46.940
And for example,
if I hit the Debug,

00:20:46.940 --> 00:20:48.480
I can even set breakpoints.

00:20:48.480 --> 00:20:50.490
So now, I have a
breakpoint on my code.

00:20:50.490 --> 00:20:52.490
So I can start taking a
look at what's happening

00:20:52.490 --> 00:20:53.810
in my neural network code.

00:20:53.810 --> 00:20:56.665
Here, this is where I'm
preloading the data into it.

00:20:56.665 --> 00:20:58.040
And I can step
through, and I can

00:20:58.040 --> 00:20:59.632
do a lot of debugging
to really make

00:20:59.632 --> 00:21:01.340
sure my neural network
is working the way

00:21:01.340 --> 00:21:02.683
that I want it to work.

00:21:02.683 --> 00:21:04.100
It's one of the
things that I hear

00:21:04.100 --> 00:21:05.642
a lot from developers
when they first

00:21:05.642 --> 00:21:07.238
get started with
machine learning

00:21:07.238 --> 00:21:08.780
is that, this seems
to be your models

00:21:08.780 --> 00:21:10.145
are very much a black box.

00:21:10.145 --> 00:21:12.270
You have all this Python
code for training a model,

00:21:12.270 --> 00:21:15.020
and then you have to do
some rough guesswork.

00:21:15.020 --> 00:21:17.090
With TensorFlow
being open-sourced,

00:21:17.090 --> 00:21:19.700
I can actually step
into the TensorFlow code

00:21:19.700 --> 00:21:22.400
in PyCharm, like I'm
doing here, to see

00:21:22.400 --> 00:21:25.593
how the training is going on,
to help me to debug my models.

00:21:25.593 --> 00:21:27.260
And Karmel, later,
is also going to show

00:21:27.260 --> 00:21:28.850
how something called
TensorBoard can

00:21:28.850 --> 00:21:30.410
be used for debugging models.

00:21:30.410 --> 00:21:32.118
Can we switch back to
the slides, please?

00:21:35.320 --> 00:21:41.010
So with that in mind, we've
gone from really just beginning

00:21:41.010 --> 00:21:42.760
to understand what
neural networks are all

00:21:42.760 --> 00:21:45.285
about and basic
"Hello, world!" code

00:21:45.285 --> 00:21:47.410
to taking a look at how we
can use something called

00:21:47.410 --> 00:21:48.093
convolutions.

00:21:48.093 --> 00:21:50.260
And they're something that
sounds really complicated

00:21:50.260 --> 00:21:52.070
and really difficult. But
once you start using them,

00:21:52.070 --> 00:21:54.195
you'll see they're actually
very, very easy to use,

00:21:54.195 --> 00:21:56.200
particularly for image
and text classification.

00:21:56.200 --> 00:21:58.750
And we saw then how,
in just a few minutes,

00:21:58.750 --> 00:22:00.670
we were able to train
a neural network to be

00:22:00.670 --> 00:22:04.030
able to recognize rock, paper,
and scissors with 97%, 98%

00:22:04.030 --> 00:22:04.910
accuracy.

00:22:04.910 --> 00:22:06.670
So that's just getting started.

00:22:06.670 --> 00:22:09.280
But now, to show us how to
actually stretch the framework,

00:22:09.280 --> 00:22:10.990
and to make it real,
and to do really

00:22:10.990 --> 00:22:12.850
cool and
production-quality stuff,

00:22:12.850 --> 00:22:14.660
Karmel is going
to share with us.

00:22:14.660 --> 00:22:15.160
Thank you.

00:22:15.160 --> 00:22:18.240
[APPLAUSE]

00:22:18.740 --> 00:22:19.888
KARMEL ALLISON: Hi.

00:22:19.888 --> 00:22:21.680
So quick show of hands
for, how many of you

00:22:21.680 --> 00:22:23.210
was that totally
new, and now you're

00:22:23.210 --> 00:22:27.010
paddling as fast as you can
to keep your head above water?

00:22:27.010 --> 00:22:28.485
All right, a fair number of you.

00:22:28.485 --> 00:22:30.860
I'm going to go over, now,
some of the tools and features

00:22:30.860 --> 00:22:33.410
that TensorFlow has to take
you from when you've actually

00:22:33.410 --> 00:22:36.320
got your model to all the
way through production.

00:22:36.320 --> 00:22:38.280
Don't worry, there is
no test at the end.

00:22:38.280 --> 00:22:41.090
So for those of you who are just
trying to keep up right now,

00:22:41.090 --> 00:22:42.590
track these words,
store somewhere

00:22:42.590 --> 00:22:44.630
in the back of your head
that this is all available.

00:22:44.630 --> 00:22:46.797
For the rest of you where
you've already got a model

00:22:46.797 --> 00:22:49.670
and you're looking for more
that you can do with it,

00:22:49.670 --> 00:22:51.020
pay attention now.

00:22:51.020 --> 00:22:52.430
All right.

00:22:52.430 --> 00:22:56.180
So Laurence went through an
image classification problem.

00:22:56.180 --> 00:22:58.730
In slides, we love image
classification problems,

00:22:58.730 --> 00:23:01.010
because they look
nice on slides.

00:23:01.010 --> 00:23:03.930
But maybe your data isn't an
image classification problem.

00:23:03.930 --> 00:23:07.940
What if you've got categorical
data or text-based data?

00:23:07.940 --> 00:23:09.770
TensorFlow provides
a number of tools

00:23:09.770 --> 00:23:11.690
that allow you to take
different data types

00:23:11.690 --> 00:23:13.430
and transform them
before loading them

00:23:13.430 --> 00:23:15.140
into a machine learning model.

00:23:15.140 --> 00:23:17.990
In particular, for
example, here, maybe we've

00:23:17.990 --> 00:23:19.640
got some user
clickstreams, right?

00:23:19.640 --> 00:23:21.170
And we've got a user ID.

00:23:21.170 --> 00:23:23.570
Now, if we fed that directly
into a deep learning model,

00:23:23.570 --> 00:23:26.510
our model would expect that
that is real valued and numeric.

00:23:26.510 --> 00:23:29.690
And it might think that user
number 125 has some relation

00:23:29.690 --> 00:23:33.557
to user 126, even though in
reality, that's not true.

00:23:33.557 --> 00:23:35.390
So we need to be able
to take data like this

00:23:35.390 --> 00:23:38.580
and transform it into data
that our model can understand.

00:23:38.580 --> 00:23:40.028
So how do we do that?

00:23:40.028 --> 00:23:41.570
Well, in TensorFlow,
one of the tools

00:23:41.570 --> 00:23:45.120
that we use extensively inside
of Google are feature columns.

00:23:45.120 --> 00:23:47.150
These are configurations
that allow

00:23:47.150 --> 00:23:51.480
you to configure transformations
on incoming data.

00:23:51.480 --> 00:23:54.140
So here, you can see we're
taking our categorical column,

00:23:54.140 --> 00:23:55.760
user ID, and we're
saying, hey, this

00:23:55.760 --> 00:23:58.820
is a categorical column
when we pass in data for it.

00:23:58.820 --> 00:24:01.672
And we don't want the model to
use it as a categorical column.

00:24:01.672 --> 00:24:04.130
We want to transform this, in
this case, into an embedding,

00:24:04.130 --> 00:24:04.340
right?

00:24:04.340 --> 00:24:06.140
So you could do a
one-hot representation.

00:24:06.140 --> 00:24:08.432
Here, we're going to do an
embedding that actually gets

00:24:08.432 --> 00:24:11.150
learned as we train our model.

00:24:11.150 --> 00:24:14.150
This embedding and other columns
that you have can then get

00:24:14.150 --> 00:24:16.470
directly fed into Keras layers.

00:24:16.470 --> 00:24:18.170
So here, we have a
Dense Features layer

00:24:18.170 --> 00:24:20.510
that's going to take all
these transformations

00:24:20.510 --> 00:24:23.060
and run them when we
pass our data through.

00:24:23.060 --> 00:24:26.060
And this feeds directly
downstream into our Keras model

00:24:26.060 --> 00:24:28.220
so that when we pass
input data through,

00:24:28.220 --> 00:24:30.260
the transformations
happen before we actually

00:24:30.260 --> 00:24:31.753
start learning from the data.

00:24:31.753 --> 00:24:33.170
And that ensures
that our model is

00:24:33.170 --> 00:24:34.850
learning what we want
it to learn, using

00:24:34.850 --> 00:24:38.560
real-value numerical data.

00:24:38.560 --> 00:24:40.160
And what do you
do with that layer

00:24:40.160 --> 00:24:41.720
once you've got
it in your model?

00:24:41.720 --> 00:24:44.630
Well, in Keras, we provide
quite a few layers.

00:24:44.630 --> 00:24:46.930
Laurence talked you through
convolutional layers,

00:24:46.930 --> 00:24:48.100
pooling layers.

00:24:48.100 --> 00:24:50.680
Those are some of the
popular ones in image models.

00:24:50.680 --> 00:24:52.520
But we've got a
whole host of layers

00:24:52.520 --> 00:24:53.958
depending on what
your needs are--

00:24:53.958 --> 00:24:56.500
so many that I couldn't fit them
in a single screenshot here.

00:24:56.500 --> 00:24:58.750
But there are RNNs,
drop out layers,

00:24:58.750 --> 00:25:01.960
batch norm, all sorts
of sampling layers.

00:25:01.960 --> 00:25:03.760
So no matter what
type of architecture

00:25:03.760 --> 00:25:05.260
you're building, whether
you're building something

00:25:05.260 --> 00:25:08.080
for your own small use case
and image classification model,

00:25:08.080 --> 00:25:10.858
whatever it is, or the latest
and greatest research model,

00:25:10.858 --> 00:25:12.400
there are a number
of built-in layers

00:25:12.400 --> 00:25:15.440
that are going to make
that a lot easier for you.

00:25:15.440 --> 00:25:19.030
And if you've got a custom
use case that's actually not

00:25:19.030 --> 00:25:20.680
represented in
one of the layers,

00:25:20.680 --> 00:25:22.720
and maybe you've got
custom algorithms or custom

00:25:22.720 --> 00:25:25.120
functionality, one of
the beauties of Keras

00:25:25.120 --> 00:25:27.400
is that it makes it
easy to subclass layers

00:25:27.400 --> 00:25:29.440
to build in your
own functionality.

00:25:29.440 --> 00:25:32.030
Here, we've got a Poincare
normalization layer.

00:25:32.030 --> 00:25:34.720
This represents a
Poincare embedding.

00:25:34.720 --> 00:25:37.210
This is not provided
out-of-the-box with TensorFlow,

00:25:37.210 --> 00:25:40.030
but a community member
has contributed this layer

00:25:40.030 --> 00:25:42.100
to the TensorFlow
add-ons repository,

00:25:42.100 --> 00:25:45.520
where we provide a number of
custom special use case layers.

00:25:45.520 --> 00:25:48.520
It's both useful, if you
need Poincare normalization,

00:25:48.520 --> 00:25:52.030
but also a very good example
of how you might write a custom

00:25:52.030 --> 00:25:54.440
layer to handle
all of your needs,

00:25:54.440 --> 00:25:56.893
if we don't have that
out-of-the-box for you.

00:25:56.893 --> 00:25:58.810
Here, you write the call
method, which handles

00:25:58.810 --> 00:26:01.730
the forward pass of this layer.

00:26:01.730 --> 00:26:04.000
So you can check out the
TensorFlow add-ons repository

00:26:04.000 --> 00:26:07.820
for more examples
of layers like this.

00:26:07.820 --> 00:26:10.655
In fact, everything in
Keras can be subclassed,

00:26:10.655 --> 00:26:11.530
or almost everything.

00:26:11.530 --> 00:26:14.440
You've got metrics,
losses, optimizers.

00:26:14.440 --> 00:26:17.410
If you need functionality that's
not provided out-of-the-box,

00:26:17.410 --> 00:26:20.320
we try to make it easy for you
to build on top of what Keras

00:26:20.320 --> 00:26:23.290
already provides, while still
taking advantage of the entire

00:26:23.290 --> 00:26:25.580
Keras and TensorFlow ecosystem.

00:26:25.580 --> 00:26:27.350
So here, I'm
subclassing a model.

00:26:27.350 --> 00:26:30.130
So if I need some custom
forward pass in my model,

00:26:30.130 --> 00:26:32.380
I'm able to do that
easily in the call method.

00:26:32.380 --> 00:26:36.010
And I can define custom training
loops within my custom model.

00:26:36.010 --> 00:26:38.680
This makes it easy to do-- in
this case, a trivial thing,

00:26:38.680 --> 00:26:41.050
like multiply by a magic number.

00:26:41.050 --> 00:26:42.760
But for a lot of
models where you

00:26:42.760 --> 00:26:45.430
need to do something that's
different than the standard fit

00:26:45.430 --> 00:26:47.800
loop, you're able to
customize in this way

00:26:47.800 --> 00:26:49.660
and still take advantage
of all the tooling

00:26:49.660 --> 00:26:52.820
that we provide for Keras.

00:26:52.820 --> 00:26:55.340
So one of the problems
with custom models

00:26:55.340 --> 00:26:56.840
and more complicated
models is it's

00:26:56.840 --> 00:26:58.340
hard to know whether
you're actually

00:26:58.340 --> 00:26:59.715
doing what you
think you're doing

00:26:59.715 --> 00:27:01.250
and whether your
model is training.

00:27:01.250 --> 00:27:03.950
One of the tools we provide
for Keras, and TensorFlow

00:27:03.950 --> 00:27:05.810
more broadly, is TensorBoard.

00:27:05.810 --> 00:27:07.310
This is a visualization tool.

00:27:07.310 --> 00:27:09.800
It's web based, and
it runs a server

00:27:09.800 --> 00:27:11.990
that will take in the
data as your model trains

00:27:11.990 --> 00:27:14.690
so that you can see real
time, epoch by epoch,

00:27:14.690 --> 00:27:17.480
or step by step, how
your model is doing.

00:27:17.480 --> 00:27:21.050
Here, you can see accuracy
and loss as the model trains

00:27:21.050 --> 00:27:22.130
and converges.

00:27:22.130 --> 00:27:24.530
And this allows you to track
your model as you train

00:27:24.530 --> 00:27:26.322
and ensure that you're
actually progressing

00:27:26.322 --> 00:27:27.862
towards convergence.

00:27:27.862 --> 00:27:29.570
And when you're using
Keras, you can also

00:27:29.570 --> 00:27:32.990
see that you get the full graph
of the layers that you've used.

00:27:32.990 --> 00:27:35.510
You can dig into those and
actually get the op-level graph

00:27:35.510 --> 00:27:36.872
in TensorFlow.

00:27:36.872 --> 00:27:39.080
And this is really helpful
in debugging, to make sure

00:27:39.080 --> 00:27:40.702
that you've correctly
wired your model

00:27:40.702 --> 00:27:42.410
and you're actually
building and training

00:27:42.410 --> 00:27:43.743
what you think you are training.

00:27:46.660 --> 00:27:48.210
In Keras, the way
you add this is

00:27:48.210 --> 00:27:49.830
as easy as a few lines of code.

00:27:49.830 --> 00:27:53.130
Here, we've got our TensorBoard
callback that we define.

00:27:53.130 --> 00:27:55.440
We add that to our
model during training.

00:27:55.440 --> 00:27:57.720
And that's going to
write out to the logs,

00:27:57.720 --> 00:27:59.430
to disk, a bunch of
different metrics

00:27:59.430 --> 00:28:04.340
that then get read in by
the TensorBoard web GUI.

00:28:04.340 --> 00:28:07.250
And as an added bonus, you
get built-in performance

00:28:07.250 --> 00:28:08.670
profiling with that.

00:28:08.670 --> 00:28:10.220
So one of the tabs
in TensorBoard

00:28:10.220 --> 00:28:12.020
is going to show you
where all of your ops

00:28:12.020 --> 00:28:15.470
are being placed, where you've
got performance bottlenecks.

00:28:15.470 --> 00:28:17.240
This is extremely
useful as you begin

00:28:17.240 --> 00:28:19.970
to build larger and
more models, because you

00:28:19.970 --> 00:28:22.190
will see that performance
during training

00:28:22.190 --> 00:28:24.710
can become one of the
bottlenecks in your process.

00:28:24.710 --> 00:28:28.940
And you really want
to make that faster.

00:28:28.940 --> 00:28:31.310
Speaking of performance,
this is a plot

00:28:31.310 --> 00:28:34.760
of how long it takes ResNet-50,
one of the most popular machine

00:28:34.760 --> 00:28:38.210
learning models for
image classification,

00:28:38.210 --> 00:28:40.010
to train using one GPU.

00:28:40.010 --> 00:28:42.790
Don't even ask how long
it takes with one CPU,

00:28:42.790 --> 00:28:44.540
because nobody likes
to sit there and wait

00:28:44.540 --> 00:28:45.410
until it finishes.

00:28:45.410 --> 00:28:46.993
But you can see that
it takes a better

00:28:46.993 --> 00:28:48.620
part of a week with one GPU.

00:28:48.620 --> 00:28:50.240
One of the beauties
of deep learning

00:28:50.240 --> 00:28:52.790
is that it is very
easily parallelizable.

00:28:52.790 --> 00:28:55.460
And so what we want to
provide as TensorFlow

00:28:55.460 --> 00:29:00.260
are ways to take this training
pipeline and parallelize it.

00:29:00.260 --> 00:29:02.630
The way we do that in
TensorFlow 2.0 is we're

00:29:02.630 --> 00:29:05.760
providing a series of
distribution strategies.

00:29:05.760 --> 00:29:07.940
These are going to make it
very easy for you to take

00:29:07.940 --> 00:29:09.110
your existing model code.

00:29:09.110 --> 00:29:10.670
Here, we've got a
Keras model that

00:29:10.670 --> 00:29:12.170
looks like many of
the others you've

00:29:12.170 --> 00:29:13.850
seen throughout this talk.

00:29:13.850 --> 00:29:18.410
And we're going to distribute
it over multiple GPUs.

00:29:18.410 --> 00:29:20.960
So here, we add the
mirrored strategy.

00:29:20.960 --> 00:29:22.850
With these few
lines of code, we're

00:29:22.850 --> 00:29:26.270
now able to distribute our
model across multiple GPUs.

00:29:26.270 --> 00:29:28.520
These strategies have been
designed from the ground up

00:29:28.520 --> 00:29:32.090
to be easy to use and to
scale with lots of different

00:29:32.090 --> 00:29:34.430
architectures and to give
you great out-of-the-box

00:29:34.430 --> 00:29:35.960
performance.

00:29:35.960 --> 00:29:37.970
So what this is actually doing--

00:29:37.970 --> 00:29:40.820
here, you can see that with
those few lines of code,

00:29:40.820 --> 00:29:44.480
by building our model under the
strategy scope, what we've done

00:29:44.480 --> 00:29:46.888
is we've taken the model,
we've copied it across all

00:29:46.888 --> 00:29:47.930
of our different devices.

00:29:47.930 --> 00:29:50.630
In this picture, let's
say we've got four GPUs.

00:29:50.630 --> 00:29:52.670
We copy our model
across those GPUs,

00:29:52.670 --> 00:29:54.298
and we shard the input data.

00:29:54.298 --> 00:29:55.840
That means that
you're actually going

00:29:55.840 --> 00:29:57.590
to be processing the
input in parallel

00:29:57.590 --> 00:29:59.840
across each of your
different devices.

00:29:59.840 --> 00:30:01.550
And in that way,
you're able to scale

00:30:01.550 --> 00:30:04.070
model training approximately
linearly with the number

00:30:04.070 --> 00:30:05.060
of devices you have.

00:30:05.060 --> 00:30:07.580
So if you've got four GPUs,
you can run approximately four

00:30:07.580 --> 00:30:09.890
times faster.

00:30:09.890 --> 00:30:11.720
What that ends up
looking like-- on ResNet,

00:30:11.720 --> 00:30:14.420
you can see that we
get great scaling.

00:30:14.420 --> 00:30:16.970
And just out-of-the-box, what
you're getting with that is

00:30:16.970 --> 00:30:20.930
that your variables are getting
mirrored and synced across all

00:30:20.930 --> 00:30:22.160
available devices.

00:30:22.160 --> 00:30:23.780
Batches are getting prefetched.

00:30:23.780 --> 00:30:25.890
All of this goes into
making your models much more

00:30:25.890 --> 00:30:29.900
performant during training
time, all without changing code

00:30:29.900 --> 00:30:32.390
when you're using Keras.

00:30:32.390 --> 00:30:33.140
All right.

00:30:33.140 --> 00:30:36.410
And mirrored strategy with multi
GPUs is just the beginning.

00:30:36.410 --> 00:30:38.913
As you scale models, as we
do at Google, for example,

00:30:38.913 --> 00:30:41.330
you might want to use multiple
nodes and multiple servers,

00:30:41.330 --> 00:30:43.730
each of which have
their own set of GPUs.

00:30:43.730 --> 00:30:45.890
You can use the multiworker
mirrored strategy

00:30:45.890 --> 00:30:47.960
for that, which is going
to take your model,

00:30:47.960 --> 00:30:49.910
replicate it across
multiple machines,

00:30:49.910 --> 00:30:53.180
all working synchronously
to train your model,

00:30:53.180 --> 00:30:55.950
mirroring variables
across all of them.

00:30:55.950 --> 00:30:59.480
This allows you to train your
model faster than ever before.

00:30:59.480 --> 00:31:02.602
And this API is
still experimental,

00:31:02.602 --> 00:31:03.560
as we're developing it.

00:31:03.560 --> 00:31:06.980
But in TensorFlow 2.0, you'll be
able to run this out-of-the-box

00:31:06.980 --> 00:31:09.740
and get that great performance
across large scale clusters.

00:31:12.500 --> 00:31:13.000
All right.

00:31:13.000 --> 00:31:15.490
So everything I've
talked about so far

00:31:15.490 --> 00:31:18.190
falls under the heading
of training models.

00:31:18.190 --> 00:31:20.350
And you will find that
a lot of model builders

00:31:20.350 --> 00:31:22.680
only ever think about
the training portion.

00:31:22.680 --> 00:31:24.430
But if you've got a
machine learning model

00:31:24.430 --> 00:31:26.180
that you're trying to
get into production,

00:31:26.180 --> 00:31:28.228
you know that's
only half the story.

00:31:28.228 --> 00:31:30.020
There's a whole other
half, which is, well,

00:31:30.020 --> 00:31:32.470
how do I take what I've
learned and actually serve

00:31:32.470 --> 00:31:36.640
that to customers or to
whoever the end user is, right?

00:31:36.640 --> 00:31:38.890
In TensorFlow, the way
we do that is you're

00:31:38.890 --> 00:31:41.800
going to have to serialize
your model into a saved model.

00:31:41.800 --> 00:31:44.493
This saved model becomes the
serialized format of your model

00:31:44.493 --> 00:31:46.660
that then integrates with
the rest of the TensorFlow

00:31:46.660 --> 00:31:47.590
ecosystem.

00:31:47.590 --> 00:31:50.990
That allows you to deploy
that model into production.

00:31:50.990 --> 00:31:54.130
So for example, we've got a
number of different libraries

00:31:54.130 --> 00:31:56.800
and utilities that can
take this saved model.

00:31:56.800 --> 00:31:58.350
For TensorFlow
Serving, we're going

00:31:58.350 --> 00:32:01.000
to be able to take
that model and do

00:32:01.000 --> 00:32:02.262
web-based serving requests.

00:32:02.262 --> 00:32:04.720
This is what we use at Google
for some of our largest scale

00:32:04.720 --> 00:32:05.970
systems.

00:32:05.970 --> 00:32:08.050
TensorFlow Lite is for
mobile development.

00:32:08.050 --> 00:32:10.180
TensorFlow.js is a
web-native solution

00:32:10.180 --> 00:32:11.780
for serving your models.

00:32:11.780 --> 00:32:13.360
I'm not going to
have time to go over

00:32:13.360 --> 00:32:15.030
all of these in the
next few minutes,

00:32:15.030 --> 00:32:17.530
but I will talk about TensorFlow
Serving and TensorFlow Lite

00:32:17.530 --> 00:32:18.730
a little bit more.

00:32:18.730 --> 00:32:22.130
But first, how do you
actually get to a saved model?

00:32:22.130 --> 00:32:24.460
Again, in TensorFlow 2.0,
this is going to be easy

00:32:24.460 --> 00:32:27.130
and out-of-the-box where you're
going to take your Keras model,

00:32:27.130 --> 00:32:27.905
you call .save.

00:32:27.905 --> 00:32:30.280
And this is going to write
out the TensorFlow saved model

00:32:30.280 --> 00:32:30.830
format.

00:32:30.830 --> 00:32:33.118
This is a serialized
version of your model.

00:32:33.118 --> 00:32:35.410
It includes the entire graph,
and all of the variables,

00:32:35.410 --> 00:32:37.410
and weights, and everything
that you've learned,

00:32:37.410 --> 00:32:40.160
and it writes that out to
disk so that you can take it,

00:32:40.160 --> 00:32:41.830
pass it to somebody
else, let's say.

00:32:41.830 --> 00:32:43.660
You can load it
back into Python.

00:32:43.660 --> 00:32:46.757
You're going to get all of
that Python object state back,

00:32:46.757 --> 00:32:47.590
as you can see here.

00:32:47.590 --> 00:32:49.900
And you could continue to
train, continue to use that.

00:32:49.900 --> 00:32:52.030
You could fine-tune
based on that.

00:32:52.030 --> 00:32:55.180
Or you could take that model
and load it into TF Serving.

00:32:55.180 --> 00:32:59.110
So TensorFlow Serving responds
to gRPC or REST requests.

00:32:59.110 --> 00:33:02.620
It acts as a front end
that takes the requests,

00:33:02.620 --> 00:33:04.573
it sends them to your
model for inference.

00:33:04.573 --> 00:33:05.990
It's going to get
the result back.

00:33:05.990 --> 00:33:07.750
So if you're building a web
app for our rock, paper,

00:33:07.750 --> 00:33:09.550
scissors game, you
could take a picture,

00:33:09.550 --> 00:33:11.350
send it to your server.

00:33:11.350 --> 00:33:13.960
The server is going to ask
the model, hey, what is this?

00:33:13.960 --> 00:33:17.770
Send back the answer, based
on what the model found.

00:33:17.770 --> 00:33:21.160
And in that way, you get
that full round trip.

00:33:21.160 --> 00:33:23.080
TensorFlow Serving is
what we use internally

00:33:23.080 --> 00:33:25.220
for many of our largest
machine learning models.

00:33:25.220 --> 00:33:27.580
So it's been optimized to
have low latency and high

00:33:27.580 --> 00:33:28.510
throughput.

00:33:28.510 --> 00:33:32.260
You can check it out
at TensorFlow.org.

00:33:32.260 --> 00:33:35.590
There's an entire suite
of production pipelining

00:33:35.590 --> 00:33:38.140
and processing
components that we call

00:33:38.140 --> 00:33:40.360
TensorFlow Extended, or TFX.

00:33:40.360 --> 00:33:43.360
You can learn more about
those at TensorFlow.org,

00:33:43.360 --> 00:33:45.820
using that handy dandy
QR code right there.

00:33:49.013 --> 00:33:51.430
And maybe you've got a model,
and you've got your web app.

00:33:51.430 --> 00:33:53.180
But really, you want
it on a phone, right?

00:33:53.180 --> 00:33:55.180
Because the future is mobile.

00:33:55.180 --> 00:33:57.070
You want to be able
to take this anywhere.

00:33:57.070 --> 00:33:58.990
So TensorFlow Lite
is the library

00:33:58.990 --> 00:34:01.720
that we provide for converting
your saved model into a very

00:34:01.720 --> 00:34:03.400
tiny, small footprint.

00:34:03.400 --> 00:34:05.210
So that can fit on
your mobile device.

00:34:05.210 --> 00:34:07.090
It can fit on embedded devices--

00:34:07.090 --> 00:34:09.080
Raspberry Pis, Edge TPUs.

00:34:09.080 --> 00:34:12.469
We now run these models across
a number of different devices.

00:34:12.469 --> 00:34:14.710
The way you do this is you
take that same saved model

00:34:14.710 --> 00:34:17.530
from that same model code
that you wrote originally.

00:34:17.530 --> 00:34:19.690
You use the TF Lite
converter, which shrinks

00:34:19.690 --> 00:34:21.380
the footprint of that model.

00:34:21.380 --> 00:34:24.699
And then it can be loaded
directly onto device.

00:34:24.699 --> 00:34:27.370
And this allows you to run
on-device, without internet,

00:34:27.370 --> 00:34:29.650
without a server in the
background, whatever

00:34:29.650 --> 00:34:31.409
your model is.

00:34:31.409 --> 00:34:33.040
And you can take
it, take TensorFlow,

00:34:33.040 --> 00:34:36.510
wherever you want to be.

00:34:36.510 --> 00:34:39.130
Now, we've run through,
really quickly,

00:34:39.130 --> 00:34:41.683
from some machine
learning fundamentals,

00:34:41.683 --> 00:34:43.600
through building your
first model, all the way

00:34:43.600 --> 00:34:45.310
through some of the
tools that TensorFlow

00:34:45.310 --> 00:34:48.590
provides for taking those and
deploying those to production.

00:34:48.590 --> 00:34:49.670
What do you do now?

00:34:49.670 --> 00:34:51.409
Well, there's a
lot more out there.

00:34:51.409 --> 00:34:52.659
You can go to google.dev.

00:34:52.659 --> 00:34:54.250
You can go to
TensorFlow.org where

00:34:54.250 --> 00:34:56.620
we've got a great
number of tutorials.

00:34:56.620 --> 00:34:57.550
You can go to GitHub.

00:34:57.550 --> 00:34:59.840
This is all open source.

00:34:59.840 --> 00:35:02.270
You can see the different
libraries there, ask questions,

00:35:02.270 --> 00:35:02.980
send PRs.

00:35:02.980 --> 00:35:04.420
We love PRs.

00:35:04.420 --> 00:35:07.895
And with that, I'd
like to say, thank you.

00:35:07.895 --> 00:35:09.520
LAURENCE MORONEY:
Thank you, very much.

00:35:09.520 --> 00:35:10.978
KARMEL ALLISON:
Laurence, back out.

00:35:10.978 --> 00:35:11.860
[APPLAUSE]

00:35:11.860 --> 00:35:15.210
[MUSIC PLAYING]

