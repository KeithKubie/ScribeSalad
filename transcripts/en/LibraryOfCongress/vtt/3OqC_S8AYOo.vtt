WEBVTT
Kind: captions
Language: en

00:00:00.790 --> 00:00:04.630
&gt;&gt; From the Library of
Congress in Washington, D.C.

00:00:13.040 --> 00:00:14.340
&gt;&gt; Carson Block: Right?

00:00:14.340 --> 00:00:15.640
All right.

00:00:15.640 --> 00:00:16.940
Thanks so much.

00:00:16.940 --> 00:00:18.240
Welcome. Good afternoon.

00:00:18.240 --> 00:00:19.540
I hope everyone had a great lunch.

00:00:19.540 --> 00:00:20.840
And, you know, it's so bright
up here and so dark out there,

00:00:20.840 --> 00:00:22.140
if you fell asleep
we wouldn't know it.

00:00:22.140 --> 00:00:24.590
But we're going to do our
best to make sure that nobody

00:00:24.590 --> 00:00:26.070
in here falls asleep
because this is going

00:00:26.070 --> 00:00:28.120
to be an excellent conversation.

00:00:28.120 --> 00:00:29.420
My name is Carson Block.

00:00:29.420 --> 00:00:33.680
I'm a Library Technology Consultant
on this Tech Trends Panel.

00:00:33.680 --> 00:00:35.280
&gt;&gt; John Resig: I'm John Resig.

00:00:35.280 --> 00:00:39.270
I work at Khan Academy but
in my spare time I do a lot

00:00:39.270 --> 00:00:40.670
of work with libraries.

00:00:40.670 --> 00:00:42.520
I'm a developer, my background.

00:00:42.520 --> 00:00:44.460
&gt;&gt; Alison Macrina: And
my name is Alison Macrina

00:00:44.460 --> 00:00:46.370
and I run the Library
Freedom Project

00:00:46.370 --> 00:00:49.190
which is a privacy
activism organization

00:00:49.190 --> 00:00:50.710 position:56%
for libraries and their communities.

00:00:50.710 --> 00:00:53.100 position:56%
And I also work with the Tor Project

00:00:53.100 --> 00:00:56.580
who builds privacy and
anonymity software.

00:00:56.580 --> 00:00:57.880
&gt;&gt; Carson Block: Outstanding.

00:00:57.880 --> 00:00:59.600
We've got a great set of panelists.

00:00:59.600 --> 00:01:00.900
We --

00:01:00.900 --> 00:01:02.200
&gt;&gt; Alison Macrina:
Shoutout to Jaimee.

00:01:02.200 --> 00:01:03.500
&gt;&gt; Carson Block: -- also
are missing somebody, yes.

00:01:03.500 --> 00:01:05.000
Jaimee Allay [assumed spelling]
was not able to make it.

00:01:05.000 --> 00:01:07.160
She was ill and couldn't
make this session.

00:01:07.160 --> 00:01:10.550
So we miss her greatly but we
believe she's on the Twitter

00:01:10.550 --> 00:01:13.120
and tweeting things
and including a link

00:01:13.120 --> 00:01:15.690
to a resource sheet
after this session.

00:01:15.690 --> 00:01:18.660
So please look at that as we go.

00:01:18.660 --> 00:01:20.650
Thinking about technology,

00:01:20.650 --> 00:01:23.050
we're going to have a very
fast-paced conversation

00:01:23.050 --> 00:01:24.930
up here even though we
only have 45 minutes.

00:01:24.930 --> 00:01:26.540
We decided we wanted
to cover everything.

00:01:26.540 --> 00:01:27.840
Is that okay?

00:01:27.840 --> 00:01:29.310
Everyone like everything
about technology?

00:01:29.310 --> 00:01:30.610
Great. Okay.

00:01:30.610 --> 00:01:32.360
So it's going to go kind of fast.

00:01:32.360 --> 00:01:35.310
One thing to remember though
is even as we talk about things

00:01:35.310 --> 00:01:39.140
that are granular down in
the field, tech changes --

00:01:39.140 --> 00:01:42.650
we have to view them whether
they're a fad or a trend, right?

00:01:42.650 --> 00:01:44.940
That's something that we're
always trying to sort out.

00:01:44.940 --> 00:01:48.690 position:56%
One of the things that Jaimee brings
to the panel that I want to talk

00:01:48.690 --> 00:01:53.150 position:56%
about is the discipline of strategic
foresight, and that's the idea

00:01:53.150 --> 00:01:56.450
of looking at how many
factors kind of join together

00:01:56.450 --> 00:01:58.300
to really help us make
some predictions

00:01:58.300 --> 00:02:00.030
or make some good choices
as we go forward.

00:02:00.030 --> 00:02:02.580
So we don't look at
technology in a silo,

00:02:02.580 --> 00:02:05.260
but we think about all these
things together and think

00:02:05.260 --> 00:02:09.260
about what might result including
social, technological, economic,

00:02:09.260 --> 00:02:10.950
environmental, and political.

00:02:10.950 --> 00:02:14.450
That's an acronym called
STEEP that's very handy

00:02:14.450 --> 00:02:16.730
when you're thinking
about the big picture.

00:02:16.730 --> 00:02:19.790
The other thing to remember is
there's lots of churn and --

00:02:19.790 --> 00:02:21.590
within the technology community.

00:02:21.590 --> 00:02:24.730
And one of the bad things in
public libraries specifically --

00:02:24.730 --> 00:02:27.040
not everywhere and certainly
not at this conference --

00:02:27.040 --> 00:02:29.740
but in public libraries a lot of
times we look at that churn in tech

00:02:29.740 --> 00:02:33.450
and we think, oh, gosh we're
losing something as things change.

00:02:33.450 --> 00:02:36.620
The rest of the world actually
sees that churn and they go, ah,

00:02:36.620 --> 00:02:38.400
we now have new opportunities.

00:02:38.400 --> 00:02:41.430
So that's why we want to encourage
you to do, is when you see churn

00:02:41.430 --> 00:02:43.100
and things changing that also means

00:02:43.100 --> 00:02:46.020
that you've got a brand
new opportunity to pursue.

00:02:46.020 --> 00:02:48.830
We have two themes today, and
we will have time for questions

00:02:48.830 --> 00:02:50.640
and interaction at the end.

00:02:50.640 --> 00:02:51.980
One is around data.

00:02:51.980 --> 00:02:53.280
We all love data, right?

00:02:53.280 --> 00:02:54.580
Data's big.

00:02:54.580 --> 00:02:55.880
It is huge.

00:02:55.880 --> 00:02:57.180
Data is important and data's scary.

00:02:57.180 --> 00:03:01.250
Also looking at cultural
institution, heritage institutions

00:03:01.250 --> 00:03:04.940
in particular, and some of the
tech trends affecting that.

00:03:04.940 --> 00:03:07.690
So we're going to start
right away with data.

00:03:07.690 --> 00:03:10.300
And as we all know -- we've worked

00:03:10.300 --> 00:03:12.480
in institutions that
are data creators.

00:03:12.480 --> 00:03:16.010
We've been creating data probably
our entire careers, right?

00:03:16.010 --> 00:03:19.420
Well, now, everyone is creating
data in our world, right?

00:03:19.420 --> 00:03:23.890
If you have a device on your
wrist to measure your biometrics,

00:03:23.890 --> 00:03:26.950
your movements around the
world, that's one sort of thing.

00:03:26.950 --> 00:03:29.600
There's that concept
of the quantified self.

00:03:29.600 --> 00:03:32.270
But we also have the
so-called Internet of Things

00:03:32.270 --> 00:03:34.590
which are the little -- lots of
little things in our environment,

00:03:34.590 --> 00:03:37.230
some of those with
sensors collecting lots

00:03:37.230 --> 00:03:41.390
of data and creating data pods.

00:03:41.390 --> 00:03:43.820
This is happening more
and more and more.

00:03:43.820 --> 00:03:48.290
And this is affecting
us because within a lot

00:03:48.290 --> 00:03:52.740
of this data collection we have
an expectation of two things

00:03:52.740 --> 00:03:55.650
that are kind of in conflict
that we're going to discuss.

00:03:55.650 --> 00:03:58.480
What is an expectation of
privacy, right, about our data?

00:03:58.480 --> 00:04:03.230
And having it not be exposed
or available to other folks.

00:04:03.230 --> 00:04:06.940
The other is an expectation of high
service that comes from analyzing

00:04:06.940 --> 00:04:09.950
that data and understanding
patterns, for instance,

00:04:09.950 --> 00:04:12.930
in our lives -- where
we're going and at a lot

00:04:12.930 --> 00:04:14.420
of degrees what we're doing.

00:04:14.420 --> 00:04:17.740
That kind of frames part
of the nuance of the issue.

00:04:17.740 --> 00:04:19.100
You're going to dive into that

00:04:19.100 --> 00:04:20.950
because we have both
things happening --

00:04:20.950 --> 00:04:24.250
expectation of privacy and an
expectation of higher levels

00:04:24.250 --> 00:04:26.850
of service using data
to serve customers.

00:04:26.850 --> 00:04:32.500
So let's talk a little bit
more about data mining, John.

00:04:32.500 --> 00:04:33.800
&gt;&gt; John Resig: Yeah.

00:04:33.800 --> 00:04:35.740
So, I mean, one of the
area that I, in general,

00:04:35.740 --> 00:04:40.950
that I'm really interested in is
ways in which we can use computers

00:04:40.950 --> 00:04:45.060
to automatically analyze large
amounts of data, the sort of things

00:04:45.060 --> 00:04:50.740 position:56%
that would be very, very challenging
to do for us mere mortals, I guess.

00:04:50.740 --> 00:04:54.660
But the -- there are a lot of
things that I think are interesting

00:04:54.660 --> 00:04:57.560
because like I personally,
I do a lot of work

00:04:57.560 --> 00:05:02.470
with different machine-learning
algorithms is finding ways --

00:05:02.470 --> 00:05:05.630
you know, ways in which these
computers can make determinations

00:05:05.630 --> 00:05:09.420
about things that, in aggregate,
that would be, you know,

00:05:09.420 --> 00:05:13.060 position:56%
looking at hundreds of thousands, if
not millions, of records of things

00:05:13.060 --> 00:05:16.570
that would take a human an
exceedingly long amount of time.

00:05:16.570 --> 00:05:22.300
And I think the point you were
touching on before about sort

00:05:22.300 --> 00:05:26.990
of tech trends and -- another
thing I would put in there would be

00:05:26.990 --> 00:05:29.460
like sort of a pragmatism and --

00:05:29.460 --> 00:05:32.600
because one of the things
that's tricky is that at least

00:05:32.600 --> 00:05:36.780
when you're using things like
machine learning algorithms,

00:05:36.780 --> 00:05:41.450
those things are extremely
technically complex.

00:05:41.450 --> 00:05:44.960
You usually have to have
someone with at least a degree

00:05:44.960 --> 00:05:49.230
in computer science who's capable
of working on these sort of things,

00:05:49.230 --> 00:05:52.680 position:56%
and it's not the sort of thing that
you can usually just kind of drop in

00:05:52.680 --> 00:05:55.340 position:56%
and things magically get sorted out.

00:05:55.340 --> 00:05:57.160
It requires a lot of, you know,

00:05:57.160 --> 00:05:59.920
very technical staff
and a lot of training.

00:05:59.920 --> 00:06:03.760
And one of the things that
I think is interesting is --

00:06:03.760 --> 00:06:07.420
one of the problems is actually
really hard to determine is

00:06:07.420 --> 00:06:12.540
when do you need a magical computer
system to solve all your problems?

00:06:12.540 --> 00:06:16.920
When do you just need one single
domain expert to kind of go in

00:06:16.920 --> 00:06:18.500
and just figure things out?

00:06:18.500 --> 00:06:21.580
And then like when do you
need maybe just like a lot

00:06:21.580 --> 00:06:23.590
of crowdsourcing to happen?

00:06:23.590 --> 00:06:26.300
And like these are all
like different steps.

00:06:26.300 --> 00:06:30.960 position:56%
And it's sometimes it's very hard as
an organization to kind of say, oh,

00:06:30.960 --> 00:06:34.500
you know -- because I feel like
people like to make the fast leap

00:06:34.500 --> 00:06:37.700
to the most technologically
cool thing they can.

00:06:37.700 --> 00:06:39.940
So it's like, oh, we can just
use this amazing algorithm

00:06:39.940 --> 00:06:41.280
and everything will be amazing.

00:06:41.280 --> 00:06:43.840
But then there's like, well,
what if we just hire, you know,

00:06:43.840 --> 00:06:46.320
this person over there
and they will just do it

00:06:46.320 --> 00:06:48.870
and because they know this
stuff really well, and they --

00:06:48.870 --> 00:06:51.710
we don't need them like to develop
a brand new algorithm to do it.

00:06:51.710 --> 00:06:54.560 position:56%
So, yeah, I guess this is one thing.

00:06:54.560 --> 00:06:59.340
Like I'm excited about but
I'm willing to hedge my bets

00:06:59.340 --> 00:07:02.310
and I'm willing to say
that, you know, like it's --

00:07:02.310 --> 00:07:05.560
we need to consider this
very, very carefully.

00:07:05.560 --> 00:07:06.860
&gt;&gt; Carson Block: Absolutely.

00:07:06.860 --> 00:07:09.180
It's like the right tool for the
right job, and I think raising

00:07:09.180 --> 00:07:10.830
that question is really important.

00:07:10.830 --> 00:07:14.190
And sometimes raising the
question of should we do it at all.

00:07:14.190 --> 00:07:15.490
&gt;&gt; Alison Macrina: I was
just about to ask that.

00:07:15.490 --> 00:07:19.380 position:56%
I mean, with a lot of these -- you
just said something about, you know,

00:07:19.380 --> 00:07:22.050
like it's sort of this
like exciting new frontier.

00:07:22.050 --> 00:07:23.910
And I think when we are
presented with those things,

00:07:23.910 --> 00:07:29.440
especially in libraries, we want to
provide a benefit to our patrons.

00:07:29.440 --> 00:07:33.350
And maybe sometimes we don't
always consider what the possible

00:07:33.350 --> 00:07:35.190
implications for exploitation
would be.

00:07:35.190 --> 00:07:39.560
You know, with data in that size, I
think about a few different things.

00:07:39.560 --> 00:07:42.060
The first thing is who owns it?

00:07:42.060 --> 00:07:45.010
Because most of the time when we're
talking about like big datasets

00:07:45.010 --> 00:07:46.480
like that, we're talking
about negotiating

00:07:46.480 --> 00:07:48.460
with giant private companies,

00:07:48.460 --> 00:07:50.570
sometimes the most
powerful in the world.

00:07:50.570 --> 00:07:56.470 position:56%
Now, is that who we want, especially
as public servants, you know?

00:07:56.470 --> 00:07:59.140
Is that good stewardship of data?

00:07:59.140 --> 00:08:00.760
I think that it's not.

00:08:00.760 --> 00:08:03.780
At the same time who else is
going to be able to manage it?

00:08:03.780 --> 00:08:06.970
Who else can provide us with
that level of information?

00:08:06.970 --> 00:08:12.580
I also think about, you know,
are we considering what kind

00:08:12.580 --> 00:08:18.040
of identifiable information is
in that data and how we got it

00:08:18.040 --> 00:08:19.340
and how it's being shared.

00:08:19.340 --> 00:08:20.960
Who has access to it?

00:08:20.960 --> 00:08:22.650
How it's stored.

00:08:22.650 --> 00:08:23.950
Is it secure?

00:08:23.950 --> 00:08:28.200
Because when you have information
at that level and you connect it

00:08:28.200 --> 00:08:32.260
to a network, the avenues for
exploitation are suddenly massive.

00:08:32.260 --> 00:08:35.770
It's not just people, you know,
possibly hacking it, but, you know,

00:08:35.770 --> 00:08:37.070
law enforcement requests.

00:08:37.070 --> 00:08:39.440
If you have information,
then it can be subpoenaed.

00:08:39.440 --> 00:08:42.470
I've seen some really
incredible projects

00:08:42.470 --> 00:08:44.900
with open data, for
example, city data.

00:08:44.900 --> 00:08:49.250 position:56%
Like I live in Massachusetts and the
Massachusetts ACLU has been working

00:08:49.250 --> 00:08:51.620
on this project called
Technology for Justice.

00:08:51.620 --> 00:08:53.840
A really amazing thing where
they've taken open city data

00:08:53.840 --> 00:08:58.040
and they're mapping policing in
Boston and showing how, you know,

00:08:58.040 --> 00:09:02.010
not surprising to activists, how it
maps along racially divided lines

00:09:02.010 --> 00:09:06.480
and it also maps along access
to social services and things

00:09:06.480 --> 00:09:08.910
like even train routes
and stuff like that.

00:09:08.910 --> 00:09:11.200
And you begin to see
like white neighborhoods

00:09:11.200 --> 00:09:13.560
where they don't even
let people of color

00:09:13.560 --> 00:09:16.210
in because the stop ratio
is just right outside.

00:09:16.210 --> 00:09:17.580
That kind of stuff is incredible.

00:09:17.580 --> 00:09:19.060 position:56%
But what are we getting within that?

00:09:19.060 --> 00:09:21.590
Are we getting the names and
addresses of all those people

00:09:21.590 --> 00:09:23.660
of color who have been
stopped by police?

00:09:23.660 --> 00:09:26.620
Is that information that --
is it in the public benefit?

00:09:26.620 --> 00:09:29.250
So I want to make sure that
we're asking all these questions

00:09:29.250 --> 00:09:31.790
as we're going into this
new exciting frontier.

00:09:31.790 --> 00:09:34.560 position:56%
&gt;&gt; Carson Block: And how many people
are actually asking those questions

00:09:34.560 --> 00:09:35.860
as we're --

00:09:35.860 --> 00:09:37.160
&gt;&gt; Alison Macrina: Not that many.

00:09:37.160 --> 00:09:38.460
&gt;&gt; Carson Block: -- going
forward [laughter], right?

00:09:38.460 --> 00:09:39.850
And I think that's
kind of an admin --

00:09:39.850 --> 00:09:44.310
something that we would like to
keep you with in terms of this idea

00:09:44.310 --> 00:09:46.050
that it's exciting and it's bad.

00:09:46.050 --> 00:09:47.350
&gt;&gt; Alison Macrina: Mm-hmm.

00:09:47.350 --> 00:09:48.650
&gt;&gt; Carson Block: It's actually up
to us to ask these questions and --

00:09:48.650 --> 00:09:49.950
&gt;&gt; Alison Macrina: Totally.

00:09:49.950 --> 00:09:51.250
&gt;&gt; Carson Block: -- actually
to understand that chain

00:09:51.250 --> 00:09:53.800
that all these things operate
in because you were just talking

00:09:53.800 --> 00:09:56.380
about many different pieces of
the technical infrastructure,

00:09:56.380 --> 00:09:59.420
the physical, the digital,
and how many --

00:09:59.420 --> 00:10:01.270
so to all the people
it's just a cloud.

00:10:01.270 --> 00:10:02.570
&gt;&gt; Alison Macrina:
Yeah, right [laughter].

00:10:02.570 --> 00:10:04.850
There's -- I love the
cloud as like one

00:10:04.850 --> 00:10:08.060
of the most effective
marketing gimmicks that exists

00:10:08.060 --> 00:10:10.040
because there is no cloud, right?

00:10:10.040 --> 00:10:14.000
There's no -- it's not like a
beautiful like floating zeros

00:10:14.000 --> 00:10:15.610
and ones up in the sky, you know?

00:10:15.610 --> 00:10:17.190
I mean it's a server,

00:10:17.190 --> 00:10:19.420
it's a physical server
that lives somewhere.

00:10:19.420 --> 00:10:20.990
You know, you have to start
thinking about the cloud

00:10:20.990 --> 00:10:22.910
as an actual physical place.

00:10:22.910 --> 00:10:26.710
It's a server somewhere that
somebody maintains, you know.

00:10:26.710 --> 00:10:29.710
And, you know, the cloud I think
there's too much trust that we put

00:10:29.710 --> 00:10:32.200
in these ideas of the
cloud, you know.

00:10:32.200 --> 00:10:33.690
And I think we need
to be more skeptical.

00:10:33.690 --> 00:10:34.990
&gt;&gt; Carson Block: Indeed.

00:10:34.990 --> 00:10:36.990
What do you think as
well in terms of --

00:10:36.990 --> 00:10:41.480
what are practical ways that folks
can go out of this room and go,

00:10:41.480 --> 00:10:43.910
I think I want to check
on dot, dot, dot, dot?

00:10:43.910 --> 00:10:46.370
Because I hope you're
all think that right now.

00:10:46.370 --> 00:10:47.670
&gt;&gt; John Resig: Yeah.

00:10:47.670 --> 00:10:49.490
I mean, yeah, I guess, yeah, one
of the tricky things that you kind

00:10:49.490 --> 00:10:53.950
of like need to ask the
question like who is benefitting

00:10:53.950 --> 00:10:55.340
from access to this information?

00:10:55.340 --> 00:11:00.990
And -- yeah, I guess when
I think about like --

00:11:00.990 --> 00:11:04.360
when I think of like a successful
project that uses lots of data

00:11:04.360 --> 00:11:08.120
in aggregate versus one
that's a lot more borderline,

00:11:08.120 --> 00:11:09.940 position:56%
and usually in every case of that --

00:11:09.940 --> 00:11:12.980
I can think of one
that's successful, it's --

00:11:12.980 --> 00:11:16.010
well, there's a phrase
in comedy, what --

00:11:16.010 --> 00:11:17.830
are you punching up or
are you punching down?

00:11:17.830 --> 00:11:19.130
&gt;&gt; Alison Macrina: Right, totally.

00:11:19.130 --> 00:11:20.430 position:56%
&gt;&gt; John Resig: Yeah, like are you --

00:11:20.430 --> 00:11:23.650
you know, are you impacting
people who, you know,

00:11:23.650 --> 00:11:27.960
are running the government, or
the police, or things, you know,

00:11:27.960 --> 00:11:31.260
the institutions that were,
you know, surrounding us?

00:11:31.260 --> 00:11:34.270
Or are we affecting people
who we should not yet --

00:11:34.270 --> 00:11:36.360
like there's no reasons
to be, you know,

00:11:36.360 --> 00:11:38.260
putting their addresses
online or something like that.

00:11:38.260 --> 00:11:39.560
&gt;&gt; Alison Macrina: Right.

00:11:39.560 --> 00:11:40.860
Public interest.

00:11:40.860 --> 00:11:42.160
&gt;&gt; John Resig: Yes.

00:11:42.160 --> 00:11:43.460
&gt;&gt; Alison Macrina: Yeah.

00:11:43.460 --> 00:11:44.760
&gt;&gt; John Resig: Absolutely.

00:11:44.760 --> 00:11:46.060
So, yeah. I feel like that is --
that's a really big aspect of it.

00:11:46.060 --> 00:11:47.360
And I think being very deliberate

00:11:47.360 --> 00:11:49.810
in realizing who's
going to be impacted.

00:11:49.810 --> 00:11:51.890
&gt;&gt; Alison Macrina: And I think
we care about these things,

00:11:51.890 --> 00:11:54.510
but we don't even necessarily
know the right questions to ask.

00:11:54.510 --> 00:11:57.120
I mean, a lot of this is
above our technical paygrade.

00:11:57.120 --> 00:11:59.220
Many of us working in
libraries, even those of us

00:11:59.220 --> 00:12:01.560
who are fairly technical,
like there's no possible way.

00:12:01.560 --> 00:12:03.880
The whole thing is too
big to know, you know.

00:12:03.880 --> 00:12:07.670
So even being able to go in and
say, look, I want to make sure

00:12:07.670 --> 00:12:10.400
that we do the most ethical
things that we can here.

00:12:10.400 --> 00:12:11.940
How do you start that?

00:12:11.940 --> 00:12:13.240
&gt;&gt; John Resig: Mm-hmm.

00:12:13.240 --> 00:12:14.540
&gt;&gt; Carson Block: How
do you start it?

00:12:14.540 --> 00:12:15.840
&gt;&gt; Alison Macrina:
Oh, man [laughter].

00:12:15.840 --> 00:12:17.140
Well, I mean --

00:12:17.140 --> 00:12:18.440
&gt;&gt; Carson Block: Because,
because you're very good

00:12:18.440 --> 00:12:19.740
at starting that conversation.

00:12:19.740 --> 00:12:21.040
&gt;&gt; Alison Macrina: Well, thank you.

00:12:21.040 --> 00:12:22.990
I mean, a few things that I
mentioned already, you know,

00:12:22.990 --> 00:12:27.860
like who -- you know, think about
data as like having, you know,

00:12:27.860 --> 00:12:30.110
a physical form effectively.

00:12:30.110 --> 00:12:31.830
I think that is very
helpful to think about it

00:12:31.830 --> 00:12:34.210
like the cloud is this like
nebulous sort of thing.

00:12:34.210 --> 00:12:36.860
Like it actually has
a server location

00:12:36.860 --> 00:12:39.270
so who owns that server, you know?

00:12:39.270 --> 00:12:40.650
And what are they doing with it?

00:12:40.650 --> 00:12:41.950
What's their interest in it?

00:12:41.950 --> 00:12:44.890
You know, I think about the biggest
cloud provider would probably be

00:12:44.890 --> 00:12:49.830
Google because of just the number,
the sheer number of services

00:12:49.830 --> 00:12:52.060
that we rely on from
Google all the time.

00:12:52.060 --> 00:12:56.440
And Google is the -- one of the
world's most powerful companies.

00:12:56.440 --> 00:12:58.090
They are an advertising company.

00:12:58.090 --> 00:12:59.800
They collect that information

00:12:59.800 --> 00:13:03.070
to generate advertising
revenue from it, you know?

00:13:03.070 --> 00:13:04.370
I know, shocker.

00:13:04.370 --> 00:13:05.670
&gt;&gt; Carson Block: No.

00:13:05.670 --> 00:13:06.970
&gt;&gt; Alison Macrina: Right.

00:13:06.970 --> 00:13:08.270
&gt;&gt; Carson Block: It is a shocker.

00:13:08.270 --> 00:13:09.570
&gt;&gt; Alison Macrina:
Ninety-something percent

00:13:09.570 --> 00:13:10.870
of Google's revenue is from ads.

00:13:10.870 --> 00:13:12.170
&gt;&gt; Carson Block: But a lot of
people don't think about that.

00:13:12.170 --> 00:13:13.470
&gt;&gt; Alison Macrina: They
don't think about it, right.

00:13:13.470 --> 00:13:14.770
And they don't think about
Google having actual servers

00:13:14.770 --> 00:13:16.800
where Google's engineers
have actual access to it,

00:13:16.800 --> 00:13:19.420
or Google has actual --
has a negotiation with it.

00:13:19.420 --> 00:13:22.360
Google does a great job at
server security, you know.

00:13:22.360 --> 00:13:24.260
That is one thing that
they've really prioritized,

00:13:24.260 --> 00:13:25.690
especially after Snowden.

00:13:25.690 --> 00:13:28.820
So that's the second thing,
like not just who owns it

00:13:28.820 --> 00:13:33.870
and who accesses it, but how is it
stored, because that's, you know,

00:13:33.870 --> 00:13:37.320
that physical server somebody
could exploit it, and then they --

00:13:37.320 --> 00:13:38.640
you have a whole other
set of problems.

00:13:38.640 --> 00:13:41.250
And then it's like, then within
the dataset itself, you know,

00:13:41.250 --> 00:13:44.540
how are you dealing with
personally identifying information?

00:13:44.540 --> 00:13:47.330
What constitutes personally
identifying information?

00:13:47.330 --> 00:13:50.020
Is there such a thing as
anonymized data in aggregate?

00:13:50.020 --> 00:13:52.210
No, there is not it
turns out [laughter].

00:13:52.210 --> 00:13:53.510
It takes --

00:13:53.510 --> 00:13:54.810
&gt;&gt; Carson Block: The
devil is in the details.

00:13:54.810 --> 00:13:56.600
&gt;&gt; Alison Macrina: -- three data
points to identify any human.

00:13:56.600 --> 00:13:58.530
We're all special snowflakes.

00:13:58.530 --> 00:14:01.380
So, you know, these
are -- those are some

00:14:01.380 --> 00:14:02.680
of the first questions I would ask.

00:14:02.680 --> 00:14:03.980
&gt;&gt; Carson Block: Very good.

00:14:03.980 --> 00:14:05.280
&gt;&gt; Alison Macrina: I don't know.

00:14:05.280 --> 00:14:06.580
What other questions?

00:14:06.580 --> 00:14:07.880
&gt;&gt; John Resig: Yeah.

00:14:07.880 --> 00:14:09.180
I mean there are a couple points
about -- are really fantastic.

00:14:09.180 --> 00:14:10.960
And it's like, yeah, when you're
using a service for free you have

00:14:10.960 --> 00:14:14.360
to ask yourself, well,
who is actually --

00:14:14.360 --> 00:14:15.860
like where's the money
coming -- yeah.

00:14:15.860 --> 00:14:17.160
&gt;&gt; Alison Macrina:
Versus -- right, totally.

00:14:17.160 --> 00:14:18.460
&gt;&gt; John Resig: Right.

00:14:18.460 --> 00:14:21.830
So if you're using -- an example
I'll bring up here because --

00:14:21.830 --> 00:14:23.330
I really like Flickr, for example.

00:14:23.330 --> 00:14:25.630
And I know a lot of institutions
have used that to put --

00:14:25.630 --> 00:14:26.930
upload the images to Flickr,

00:14:26.930 --> 00:14:29.650
provide nice little
annotations, community engagement.

00:14:29.650 --> 00:14:31.470
But like that's a great
example where if, you know,

00:14:31.470 --> 00:14:34.360 position:56%
using it for free you're like, well,

00:14:34.360 --> 00:14:36.080
at the end of the day,
who's the product?

00:14:36.080 --> 00:14:37.380
&gt;&gt; Alison Macrina: You.

00:14:37.380 --> 00:14:38.680
&gt;&gt; John Resig: Yeah,
exactly [laughter].

00:14:38.680 --> 00:14:39.980
The content you're putting
up there is the product.

00:14:39.980 --> 00:14:41.280
&gt;&gt; Alison Macrina: Right.

00:14:41.280 --> 00:14:42.720
&gt;&gt; John Resig: And it's
interesting because like, you know,

00:14:42.720 --> 00:14:46.020
just the other day like Yahoo,
like very quietly announced

00:14:46.020 --> 00:14:48.630 position:56%
that Flickr is now a legacy service.

00:14:48.630 --> 00:14:52.040
Read into that what you will, but
the way I would read into it is

00:14:52.040 --> 00:14:54.840
like I'm getting my stuff
off of there in [laughter] --

00:14:54.840 --> 00:14:56.350
but the thing is like this --

00:14:56.350 --> 00:14:58.460
so this is another
issue is like I would --

00:14:58.460 --> 00:15:03.590
I'm never going to put any
data on a service for free,

00:15:03.590 --> 00:15:04.890
or actually for pay
for that matter --

00:15:04.890 --> 00:15:06.190
&gt;&gt; Alison Macrina: Right.

00:15:06.190 --> 00:15:07.490
&gt;&gt; John Resig: -- that I
don't have a back-up for.

00:15:07.490 --> 00:15:08.790
&gt;&gt; Alison Macrina: sure.

00:15:08.790 --> 00:15:10.090
But this is the great
point that actually

00:15:10.090 --> 00:15:13.890
that Internet Archive has been
doing a project around, you know,

00:15:13.890 --> 00:15:15.760
a perpetual cloud storage.

00:15:15.760 --> 00:15:17.690
And that was exactly the
angle that they took.

00:15:17.690 --> 00:15:19.990
They were like, think of all
these services that you relied

00:15:19.990 --> 00:15:23.650
on that now no longer exist
and, ergo, all of your pictures

00:15:23.650 --> 00:15:25.220
and all of your memories.

00:15:25.220 --> 00:15:29.320
And so, yeah, not just who owns it
but what happens when they fail?

00:15:29.320 --> 00:15:33.900
There's the legacy part of it, you
know, maybe it just disappears.

00:15:33.900 --> 00:15:36.620
And then the other part of
it is that data is an asset.

00:15:36.620 --> 00:15:39.940
If a company goes bankrupt
your data can get sold

00:15:39.940 --> 00:15:41.940
when they get bought out, you know.

00:15:41.940 --> 00:15:43.400
So these are all different things.

00:15:43.400 --> 00:15:45.850
It's like thinking about
this ownership thing.

00:15:45.850 --> 00:15:49.250
What happens then, you know, when
that like little startup is gone

00:15:49.250 --> 00:15:50.550
or bought by somebody bigger?

00:15:50.550 --> 00:15:51.900
&gt;&gt; John Resig: Mm-hmm.

00:15:51.900 --> 00:15:53.280
Yeah. I guess -- I think one

00:15:53.280 --> 00:15:57.240
of the things that's extremely
challenging here though is

00:15:57.240 --> 00:16:00.890
that the nice part about free
service is that they're free.

00:16:00.890 --> 00:16:03.300
And that when you have not --

00:16:03.300 --> 00:16:06.090
when you don't have an
in-house technical staff

00:16:06.090 --> 00:16:08.950
and when you don't have the
ability to run your own servers

00:16:08.950 --> 00:16:12.580
and do all these sort of
things, free sounds really nice.

00:16:12.580 --> 00:16:16.010
And so this is the thing.

00:16:16.010 --> 00:16:20.490
I feel like I don't have a good
answer for because it's like well,

00:16:20.490 --> 00:16:23.580
okay, you just stop, don't
rely upon this free thing.

00:16:23.580 --> 00:16:26.260
But the things is that on the other
side is like well, you kind of have

00:16:26.260 --> 00:16:27.790 position:56%
to have like all these other things.

00:16:27.790 --> 00:16:29.370
&gt;&gt; Alison Macrina: But there are
different kinds of free, right?

00:16:29.370 --> 00:16:32.770
I think it's totally legit, like
we have to think about, you know,

00:16:32.770 --> 00:16:36.910
what kind of -- what kind of
like exchange is happening

00:16:36.910 --> 00:16:38.210
when we get something for free?

00:16:38.210 --> 00:16:40.840
Is it -- it's our data, you
know, that we're volunteering.

00:16:40.840 --> 00:16:44.940
I work a lot with free and
open source software and to me

00:16:44.940 --> 00:16:48.060
that is a good -- it's
not an ideal solution,

00:16:48.060 --> 00:16:52.100
but at least the reason why
you get the thing for free is

00:16:52.100 --> 00:16:55.200
that it is a community effort,
that all these different people all

00:16:55.200 --> 00:16:58.500
over the world are working
on it, usually as volunteers,

00:16:58.500 --> 00:17:01.570
and that is a thing that I like
to put my trust in a little more

00:17:01.570 --> 00:17:05.500
than like private company who maybe
makes something more sophisticated

00:17:05.500 --> 00:17:07.300
and nicer.

00:17:07.300 --> 00:17:08.740
You know, I think about,
you know, the difference

00:17:08.740 --> 00:17:11.610
between like Google Maps and
OpenStreetMaps, for example.

00:17:11.610 --> 00:17:13.990
OpenStreetMaps is a free
and open source project

00:17:13.990 --> 00:17:15.430
that doesn't work as well.

00:17:15.430 --> 00:17:18.860
Google works really well because
they got the little cars that drive

00:17:18.860 --> 00:17:20.820
around and take pictures, and
they're monitoring you when you're

00:17:20.820 --> 00:17:22.380
in traffic and all this stuff.

00:17:22.380 --> 00:17:24.980
But I think the first thing is
people have to know, you know,

00:17:24.980 --> 00:17:26.600 position:56%
what kind of exchange you're making.

00:17:26.600 --> 00:17:28.420
&gt;&gt; Carson Block: But I think
I have a magic bullet answer

00:17:28.420 --> 00:17:29.720
that no one's going to believe.

00:17:29.720 --> 00:17:31.020
&gt;&gt; Alison Macrina: Go on.

00:17:31.020 --> 00:17:33.430
&gt;&gt; Carson Block: We need to invest
more in our technology people

00:17:33.430 --> 00:17:36.780
in our cultural heritage
institutions and our libraries.

00:17:36.780 --> 00:17:39.940
Constantly what I've
been seeing again is,

00:17:39.940 --> 00:17:43.230
especially on the leadership level,

00:17:43.230 --> 00:17:46.200
they'll confuse free
with free, right?'

00:17:46.200 --> 00:17:47.500
&gt;&gt; Alison Macrina: Mm-hmm.

00:17:47.500 --> 00:17:48.940
&gt;&gt; Carson Block: So a free
open source software is free

00:17:48.940 --> 00:17:52.170 position:56%
as in freedom for you to pick up
the ball and to do something with it

00:17:52.170 --> 00:17:53.470
and it's [inaudible] community.

00:17:53.470 --> 00:17:54.770
&gt;&gt; Alison Macrina: It
takes time and -- mm-hmm.

00:17:54.770 --> 00:17:56.940
&gt;&gt; Carson Block: And that
investment actually has been lower.

00:17:56.940 --> 00:17:59.830
I would say that because of the
tipping point that we're at with --

00:17:59.830 --> 00:18:01.130
and we're just -- the tip

00:18:01.130 --> 00:18:03.400
of the iceberg is what we're
talking about in this issues.

00:18:03.400 --> 00:18:06.370
Because of that we need
more knowledgeable workers

00:18:06.370 --> 00:18:09.240
on the IT side, on
the technology side,

00:18:09.240 --> 00:18:13.390
that understand how the technology
impacts not just our services

00:18:13.390 --> 00:18:15.570
that we're giving people,
not just what they get,

00:18:15.570 --> 00:18:16.870
but also how we're doing it.

00:18:16.870 --> 00:18:21.340
Are we adhering to our own culture
of privacy or confidentiality?

00:18:21.340 --> 00:18:25.010
Are we doing the details
and due diligence in that?

00:18:25.010 --> 00:18:29.670
Right now we're not investing in
that in any way, shape, or form.

00:18:29.670 --> 00:18:32.120
And also I don't think we're
cultivating the right sort

00:18:32.120 --> 00:18:33.530
of tech leaders either.

00:18:33.530 --> 00:18:38.310
We've got lots of siloed folks who
are passionate and leading the ways

00:18:38.310 --> 00:18:41.190
in these little areas,
but not joined enough.

00:18:41.190 --> 00:18:45.270
And we also have some people that
are better at marketing themselves

00:18:45.270 --> 00:18:48.280
in terms of being wow, gee
whiz, instead of really looking

00:18:48.280 --> 00:18:51.960
at the people we serve, the
ideals that we're trying to serve

00:18:51.960 --> 00:18:53.770
in our communities in
carrying those forward.

00:18:53.770 --> 00:18:55.450
So I'll get off my soapbox on that.

00:18:55.450 --> 00:18:56.750
But that's a --

00:18:56.750 --> 00:18:58.050
&gt;&gt; Alison Macrina: That's a
very good soapbox [laughter].

00:18:58.050 --> 00:19:01.780
Hire them and pay them a lot
more than what we're paying them.

00:19:01.780 --> 00:19:03.080
&gt;&gt; Carson Block: Yes, yes.

00:19:03.080 --> 00:19:04.380 position:56%
&gt;&gt; Alison Macrina: That is dreadful.

00:19:04.380 --> 00:19:05.770
&gt;&gt; Do your best, everybody.

00:19:05.770 --> 00:19:07.070
&gt;&gt; Alison Macrina: Really.

00:19:07.070 --> 00:19:08.370
We should be embarrassed by that.

00:19:08.370 --> 00:19:09.670
&gt;&gt; Carson Block: Yeah, yeah.

00:19:09.670 --> 00:19:10.970
That's what I'm saying [laughter].

00:19:10.970 --> 00:19:12.270
Very nice [laughter].

00:19:12.270 --> 00:19:13.570
But don't feel bad, okay
because we've all --

00:19:13.570 --> 00:19:14.870
we know what the reality is.

00:19:14.870 --> 00:19:16.170
Every -- we do up here.

00:19:16.170 --> 00:19:18.850 position:56%
Let's move into some specific
things about cultural heritage data.

00:19:18.850 --> 00:19:22.160
We've got lots of content.

00:19:22.160 --> 00:19:24.450
We have lots of awesome,
awesome content.

00:19:24.450 --> 00:19:26.810
I can't believe some of
the great programs here.

00:19:26.810 --> 00:19:32.350 position:56%
I could not decide what to go to
next because of the awesome content.

00:19:32.350 --> 00:19:36.510
We also have this tension between
the aggregation and the hooks

00:19:36.510 --> 00:19:39.230
into it so that we can have
great content in one pot

00:19:39.230 --> 00:19:40.910
and share it in so many places.

00:19:40.910 --> 00:19:43.170
And then the hyper local
stuff because as we know,

00:19:43.170 --> 00:19:45.480
some of this content
that we're curating

00:19:45.480 --> 00:19:48.550
and making available is the
most important back home, right?

00:19:48.550 --> 00:19:51.030
So that's an interesting,
that's an interesting tension.

00:19:51.030 --> 00:19:54.670
And we don't want to leave it in a
silo, but it's really used there.

00:19:54.670 --> 00:19:59.020
The other thing that occurs to
me in terms of our uniqueness is

00:19:59.020 --> 00:20:01.660
that usually technology, we
use technology, hopefully,

00:20:01.660 --> 00:20:03.170
to get an economy of scale.

00:20:03.170 --> 00:20:06.130
That doesn't happen
very often in libraries

00:20:06.130 --> 00:20:08.970
because of the special
snowflake problem.

00:20:08.970 --> 00:20:12.100
Great libraries are hyper local
so that's our initial orientation.

00:20:12.100 --> 00:20:15.550
So it's very hard to get
technology that scales nationally

00:20:15.550 --> 00:20:17.570 position:56%
and internationally in the same way.

00:20:17.570 --> 00:20:20.750
That's really a problem of
cultural heritage institutions.

00:20:20.750 --> 00:20:22.420
Not a problem, but it's a challenge

00:20:22.420 --> 00:20:26.720
because even our processing
is really based

00:20:26.720 --> 00:20:28.330
on material type, right?

00:20:28.330 --> 00:20:32.040
I had one job where I was supposed
to go in and find more efficiencies

00:20:32.040 --> 00:20:34.990
between the different types
of material type processing,

00:20:34.990 --> 00:20:40.170
and not only could I not find
cross efficiencies that would work,

00:20:40.170 --> 00:20:42.910
I couldn't find any evidence that
anyone else has found that either.

00:20:42.910 --> 00:20:45.140
When you look at all the standards,
they're highly specialized

00:20:45.140 --> 00:20:46.440
to their material type, right?

00:20:46.440 --> 00:20:50.280
So a document, for instant, has
a different set of standards,

00:20:50.280 --> 00:20:53.530
requirements, and nuance
than a photograph, right,

00:20:53.530 --> 00:20:57.020
in terms of metadata,
descriptive, sharing it out.

00:20:57.020 --> 00:20:59.770
Many things that everyone
here knows about.

00:20:59.770 --> 00:21:03.800
There's also something really
interesting that doesn't get talked

00:21:03.800 --> 00:21:06.300
about nearly enough,
and that's rights.

00:21:06.300 --> 00:21:08.820
Who owns the stuff?

00:21:08.820 --> 00:21:10.120
Alison.

00:21:10.120 --> 00:21:11.420
&gt;&gt; Alison Macrina: Well,
it's certainly not us,

00:21:11.420 --> 00:21:13.080
and it should be [laughter].

00:21:13.080 --> 00:21:18.120
I mean, yeah, I mean what's
a longer answer than not us?

00:21:18.120 --> 00:21:19.420
Yeah. I --

00:21:19.420 --> 00:21:20.720
&gt;&gt; Carson Block: Honest
is good [laughter].

00:21:20.720 --> 00:21:22.020
&gt;&gt; Alison Macrina: Yeah.

00:21:22.020 --> 00:21:24.610
That's like an exchange that we've
been making over and over again.

00:21:24.610 --> 00:21:28.460 position:56%
Like thinking again about, you know,

00:21:28.460 --> 00:21:31.750
our inability to provide our own
services or like create the kind

00:21:31.750 --> 00:21:34.630
of tools that we need to
engage our patrons, you know,

00:21:34.630 --> 00:21:37.430
with digital collections and stuff.

00:21:37.430 --> 00:21:40.450
You know we've moved to
entirely third-party models

00:21:40.450 --> 00:21:42.070
and we've seen some of
the failures of this.

00:21:42.070 --> 00:21:47.000
I mean, the Adobe thing was a
scandal, as it should have been.

00:21:47.000 --> 00:21:50.330
That is just one of
these ways that --

00:21:50.330 --> 00:21:53.100
and the Adobe thing was
a big problem, obviously,

00:21:53.100 --> 00:21:54.860
because of the privacy breech.

00:21:54.860 --> 00:21:56.840
But it had been a problem
before that, I mean,

00:21:56.840 --> 00:22:00.630
because DRM is antithetical
to library values.

00:22:00.630 --> 00:22:03.790
The whole way that we've moved away

00:22:03.790 --> 00:22:06.210
from an ownership model
into a licensing model.

00:22:06.210 --> 00:22:08.770
I mean, we should have never
said yes to that, and we did.

00:22:08.770 --> 00:22:11.680
And I understand like
what we were thinking,

00:22:11.680 --> 00:22:14.280
but now it's very difficult to get
the toothpaste back in the tube.

00:22:14.280 --> 00:22:18.230
From a rights based standpoint,
you know, again thinking about,

00:22:18.230 --> 00:22:21.250
you know, who stores and
owns data and how that --

00:22:21.250 --> 00:22:23.400
what that means for access.

00:22:23.400 --> 00:22:28.070
If I am law enforcement and
I want to send, for example,

00:22:28.070 --> 00:22:31.840
a national security letter which
is a government subpoena that comes

00:22:31.840 --> 00:22:34.330
with an attached gag order
that says like you have to hand

00:22:34.330 --> 00:22:38.150
over information about your patrons
or your customers or whomever,

00:22:38.150 --> 00:22:41.460
and you can't tell
anyone that you've got it.

00:22:41.460 --> 00:22:44.280
Libraries in 2005 received
one of these.

00:22:44.280 --> 00:22:45.580
We don't have very good data

00:22:45.580 --> 00:22:47.530
about how many other
libraries may have gotten them

00:22:47.530 --> 00:22:50.170
because they have gag
orders attached to them.

00:22:50.170 --> 00:22:53.270
It's a very scary thing for
libraries to have to contend

00:22:53.270 --> 00:22:55.350
with this in a post-911 world.

00:22:55.350 --> 00:22:59.790
Even scarier is that if we rely
on all these third party vendors,

00:22:59.790 --> 00:23:01.760
we will never see that notice.

00:23:01.760 --> 00:23:03.060
We won't even be --

00:23:03.060 --> 00:23:04.360
&gt;&gt; Carson Block: Right.

00:23:04.360 --> 00:23:05.660
&gt;&gt; Alison Macrina: -- able to
talk to our attorneys or the ACLU

00:23:05.660 --> 00:23:10.100
or whomever because it
goes directly to Adobe

00:23:10.100 --> 00:23:13.530
or OverDrive or Elsevier
or whatever.

00:23:13.530 --> 00:23:16.110 position:56%
And so we've in some ways, you know,

00:23:16.110 --> 00:23:18.900
our charge of like
protecting privacy

00:23:18.900 --> 00:23:21.460
and protecting intellectual
freedom and all the amazing things

00:23:21.460 --> 00:23:23.110
that we've done in the
interest of that where we're

00:23:23.110 --> 00:23:26.620
like we purge records
and we, you know,

00:23:26.620 --> 00:23:28.940
we fight back against
unlawful information requests.

00:23:28.940 --> 00:23:30.850
It's been taken out of our hands.

00:23:30.850 --> 00:23:35.240
And that it is a --
that's a scary new world.

00:23:35.240 --> 00:23:37.890
&gt;&gt; Carson Block: It especially is
because I know that in the times

00:23:37.890 --> 00:23:39.190
when I was in -- working,

00:23:39.190 --> 00:23:40.490
when I actually had
gainful employment working

00:23:40.490 --> 00:23:42.580
in a library, I was
in a city library.

00:23:42.580 --> 00:23:45.500
I would get a call from --
usually it would be a casual call

00:23:45.500 --> 00:23:46.850
from the police department,

00:23:46.850 --> 00:23:49.360
a department that's part
of our stable, right?

00:23:49.360 --> 00:23:51.320
We are colleagues within
the organization.

00:23:51.320 --> 00:23:55.660 position:56%
It would be a casual call saying,
so, we want to know if so and so was

00:23:55.660 --> 00:23:57.690
at the library at a certain time.

00:23:57.690 --> 00:23:58.990
&gt;&gt; Alison Macrina: Yeah.

00:23:58.990 --> 00:24:00.290
&gt;&gt; Carson Block: And
I would say, so,

00:24:00.290 --> 00:24:01.590
you know that legally I cannot
give you that information.

00:24:01.590 --> 00:24:03.150
&gt;&gt; Alison Macrina: They
always do it so casually, too.

00:24:03.150 --> 00:24:04.450
They're like --

00:24:04.450 --> 00:24:05.750
&gt;&gt; Carson Block: Yeah.

00:24:05.750 --> 00:24:07.050 position:56%
&gt;&gt; Alison Macrina: -- we're friends.

00:24:07.050 --> 00:24:08.350
We're all in the same community.

00:24:08.350 --> 00:24:09.650
Don't worry about the
warrant provision.

00:24:09.650 --> 00:24:10.950
Like, you know, we're buddies.

00:24:10.950 --> 00:24:12.250
&gt;&gt; Carson Block: And at least at
that level they were asking, right?

00:24:12.250 --> 00:24:13.550
&gt;&gt; Alison Macrina: Yeah.

00:24:13.550 --> 00:24:14.850
&gt;&gt; Carson Block: They
would ask for that.

00:24:14.850 --> 00:24:16.150
And actually my -- like
with law enforcement,

00:24:16.150 --> 00:24:19.090
we always had great conversations
because what I would say is,

00:24:19.090 --> 00:24:21.980
you know, I can't give you
anything without a subpoena.

00:24:21.980 --> 00:24:24.360
However, we don't like
people breaking the law.

00:24:24.360 --> 00:24:25.880
So let's have a conversation
about this.

00:24:25.880 --> 00:24:28.550
What is actually prosecutable
evidence in this case?

00:24:28.550 --> 00:24:29.850
&gt;&gt; Alison Macrina: Yeah, yeah.

00:24:29.850 --> 00:24:31.610
&gt;&gt; Carson Block: And a lot of
times the officers I would talk to,

00:24:31.610 --> 00:24:32.910
they would say, well, you know,

00:24:32.910 --> 00:24:35.410
if we can see somebody doing
something that's better

00:24:35.410 --> 00:24:36.710
than anything.

00:24:36.710 --> 00:24:38.750
That's like, why don't you
just visit the library then

00:24:38.750 --> 00:24:40.050
if you think someone's
breaking the law.

00:24:40.050 --> 00:24:41.350
We'd love to see you.

00:24:41.350 --> 00:24:42.710
And we don't want lawbreaking
to happen within our building.

00:24:42.710 --> 00:24:44.420
We don't want people
to get hurt, you know.

00:24:44.420 --> 00:24:47.430
So that's a different sort
of conversation than, hey,

00:24:47.430 --> 00:24:50.340
we just got the honeypot
from Google.

00:24:50.340 --> 00:24:51.640
&gt;&gt; Alison Macrina: Right, yeah.

00:24:51.640 --> 00:24:52.940
&gt;&gt; Carson Block: We don't even
need to talk to you about this.

00:24:52.940 --> 00:24:54.240
&gt;&gt; Alison Macrina: Don't even
need to talk to you, yeah.

00:24:54.240 --> 00:24:55.540
Totally. Yeah.

00:24:55.540 --> 00:24:56.840
&gt;&gt; John Resig: Yeah.

00:24:56.840 --> 00:25:01.030
There's also the -- another tricky
issue in here which is the rights

00:25:01.030 --> 00:25:06.180
over the material itself by the
people who originally created it,

00:25:06.180 --> 00:25:08.530
or were involved in its creation.

00:25:08.530 --> 00:25:13.190
Like there was this blockbuster I
just read recently about a library

00:25:13.190 --> 00:25:18.650
that had digitized a lot of
really like old zines and stuff.

00:25:18.650 --> 00:25:21.040
So from pre-Internet era.

00:25:21.040 --> 00:25:22.340
&gt;&gt; Carson Block: How nice.

00:25:22.340 --> 00:25:25.670
&gt;&gt; John Resig: And so when they
were created there was never an

00:25:25.670 --> 00:25:28.430
expectation that they
would be digitized

00:25:28.430 --> 00:25:30.230
and distributed all over the world.

00:25:30.230 --> 00:25:31.530
You know, that they
were inherently --

00:25:31.530 --> 00:25:33.110
there were going to be just
these couple dozen copies

00:25:33.110 --> 00:25:34.610
and they would go to friends.

00:25:34.610 --> 00:25:37.140
And so the problem then
becomes is that some

00:25:37.140 --> 00:25:42.790
of these contained material that
mentioned very specific people,

00:25:42.790 --> 00:25:44.090
and if they were connected
back to those people

00:25:44.090 --> 00:25:46.560
who were still very much
alive, that it would --

00:25:46.560 --> 00:25:48.140
you know, it harmed their lives.

00:25:48.140 --> 00:25:49.440
&gt;&gt; Carson Block: Sure, yeah.

00:25:49.440 --> 00:25:51.560 position:56%
&gt;&gt; John Resig: So like, so this kind
of ties into the general like right

00:25:51.560 --> 00:25:55.770
to be forgotten where like do
those people have the right --

00:25:55.770 --> 00:25:57.810
then request through the library

00:25:57.810 --> 00:26:00.740
or whatever institution
had digitized it,

00:26:00.740 --> 00:26:04.480
that this material be
redacted or removed.

00:26:04.480 --> 00:26:09.250 position:56%
And this is something that I'm still
mentally trying to grapple with.

00:26:09.250 --> 00:26:13.550
I'm not sure I completely
understand it or can appreciate it.

00:26:13.550 --> 00:26:15.250
And I think one of the things
that's challenging though --

00:26:15.250 --> 00:26:18.990
but one of the points about this
I think I did understand was

00:26:18.990 --> 00:26:22.360
that libraries have that need
to have very clear policies

00:26:22.360 --> 00:26:24.640
around this, about like if you --

00:26:24.640 --> 00:26:27.670
if there is material that we have
digitized and have made accessible,

00:26:27.670 --> 00:26:30.580
that does involve you,
do you have the ability

00:26:30.580 --> 00:26:32.500
to request that they be removed?

00:26:32.500 --> 00:26:33.800
&gt;&gt; Alison Macrina: Yeah.

00:26:33.800 --> 00:26:35.550
&gt;&gt; John Resig: And if so, what is
the ways in which that can happen?

00:26:35.550 --> 00:26:36.850
&gt;&gt; Alison Macrina: It
goes back to the point

00:26:36.850 --> 00:26:38.980
that you were making
earlier about like, you know,

00:26:38.980 --> 00:26:41.140
what the power differential
there is, you know.

00:26:41.140 --> 00:26:44.570
Because I believe very strongly in
transparency for powerful people

00:26:44.570 --> 00:26:46.810
and entities like governments
and corporations.

00:26:46.810 --> 00:26:49.840
But privacy for individuals and the
right to be forgotten is so tricky

00:26:49.840 --> 00:26:54.770
because you don't want to open
up the opportunity for, you know,

00:26:54.770 --> 00:26:56.830 position:56%
powerful entities to be able to say,

00:26:56.830 --> 00:26:58.890
well now I want to
censor this stuff.

00:26:58.890 --> 00:27:00.600
But also some rights
are in conflict.

00:27:00.600 --> 00:27:02.730
I see the right to be
forgotten is a kind of extension

00:27:02.730 --> 00:27:04.700
of the Fifth Amendment
because if, you know,

00:27:04.700 --> 00:27:08.440
I want to remove information about
myself because, you know, for --

00:27:08.440 --> 00:27:09.780
I might not want to
solve, incriminate,

00:27:09.780 --> 00:27:14.220
or just because that should
belong to me in some way.

00:27:14.220 --> 00:27:16.920
But it's not an easy
thing to figure out.

00:27:16.920 --> 00:27:19.100
And then at an institutional
level, how do you,

00:27:19.100 --> 00:27:20.800
like, write a policy for that?

00:27:20.800 --> 00:27:22.250
You know, how do you even --

00:27:22.250 --> 00:27:23.550
&gt;&gt; Carson Block: How
do you understand it?

00:27:23.550 --> 00:27:24.850
&gt;&gt; Alison Macrina: Yeah.

00:27:24.850 --> 00:27:26.150
&gt;&gt; Carson Block: Right?

00:27:26.150 --> 00:27:27.450
&gt;&gt; Alison Macrina: Totally.

00:27:27.450 --> 00:27:28.750
&gt;&gt; Carson Block: And
on the other side

00:27:28.750 --> 00:27:30.050
of things we've got this
content that is great

00:27:30.050 --> 00:27:32.200
for mixing and mashing up.

00:27:32.200 --> 00:27:33.900
And I just experienced that.

00:27:33.900 --> 00:27:35.200
It was so cool.

00:27:35.200 --> 00:27:38.810
I'm working with Boulder Public
Library and we had a focus group

00:27:38.810 --> 00:27:42.030
for part of the Carnegie
Library for Local History.

00:27:42.030 --> 00:27:43.960
And this guy calls me up later.

00:27:43.960 --> 00:27:46.670
He says, "I want you to see
this website I created called

00:27:46.670 --> 00:27:51.250
hereminus100, so it's -- what he
did is he took all these different

00:27:51.250 --> 00:27:54.370
mashups of vintage
historical photographs.

00:27:54.370 --> 00:27:56.570
For instance, he had
a lot of mashups,

00:27:56.570 --> 00:27:57.870
but this one was really
kind of funny.

00:27:57.870 --> 00:28:00.660
He had a slider where he took
his own modern photograph

00:28:00.660 --> 00:28:05.410
from the same vantage point
and it was so incredibly cool.

00:28:05.410 --> 00:28:07.430
He didn't ask anybody
if this was okay.

00:28:07.430 --> 00:28:09.390
He just said, "Ah,
what do you think?

00:28:09.390 --> 00:28:11.130
Do you think they'll get mad?"

00:28:11.130 --> 00:28:12.430
That was his question to me.

00:28:12.430 --> 00:28:13.730
He goes, "Do you think
they'll get mad?"

00:28:13.730 --> 00:28:15.500
And I said, "Well, let's
look at the rights --

00:28:15.500 --> 00:28:16.800
&gt;&gt; Alison Macrina: Yeah.

00:28:16.800 --> 00:28:19.020 position:56%
&gt;&gt; -- "on the photograph number
one, because that's the bottom line.

00:28:19.020 --> 00:28:20.480
Mad's really got nothing
to do with it."

00:28:20.480 --> 00:28:21.780
&gt;&gt; Alison Macrina: Right.

00:28:21.780 --> 00:28:23.330
&gt;&gt; Carson Block: "Let's see if
you have the ability to use this.

00:28:23.330 --> 00:28:24.890
And I think it's awesome" --

00:28:24.890 --> 00:28:26.190
&gt;&gt; Alison Macrina:
Mm-hmm [laughter].

00:28:26.190 --> 00:28:27.490
&gt;&gt; Carson Block: --
"at the same time."

00:28:27.490 --> 00:28:30.550
But I keep thinking
that that's one type.

00:28:30.550 --> 00:28:33.070
We're also creating all
sorts of new digital content

00:28:33.070 --> 00:28:35.890
that I think will be ported
into all sorts of things

00:28:35.890 --> 00:28:40.700
like video game characters or
situations or motifs, for instance.

00:28:40.700 --> 00:28:44.090
Are you seeing anything that you
think is awesome when it comes

00:28:44.090 --> 00:28:46.150
to the sorts of things
that are being collected

00:28:46.150 --> 00:28:48.470
by cultural heritage
institutions now,

00:28:48.470 --> 00:28:52.200
and thinking that could
be used as dot, dot, dot?

00:28:52.200 --> 00:28:53.760
So do you think that
peaks your interest there?

00:28:53.760 --> 00:28:55.060
&gt;&gt; Alison Macrina: Hmm.

00:28:55.060 --> 00:28:57.390
That's a good question.

00:28:57.390 --> 00:28:58.690
&gt;&gt; Carson Block: Well,
we'll keep looking around --

00:28:58.690 --> 00:28:59.990
&gt;&gt; Alison Macrina: Yeah [laughter].

00:28:59.990 --> 00:29:01.290
&gt;&gt; Carson Block: --
while we're here.

00:29:01.290 --> 00:29:02.590
That was not on the script.

00:29:02.590 --> 00:29:03.890
&gt;&gt; Alison Macrina: I
mean it's making me think

00:29:03.890 --> 00:29:05.190
about copyright issues
more broadly I think.

00:29:05.190 --> 00:29:06.650
And it made my brain go
in a different direction.

00:29:06.650 --> 00:29:10.700
And just, you know, like
thinking about not just the way

00:29:10.700 --> 00:29:16.170
that our public is consuming
information and creating new things

00:29:16.170 --> 00:29:20.820
with it, and the implications of
that for copyright, plus, you know,

00:29:20.820 --> 00:29:25.570
new copyright issues, especially
with like trade agreements and all

00:29:25.570 --> 00:29:31.450
that sort of stuff that like would
create really draconian new rules

00:29:31.450 --> 00:29:36.000
around like DRM and other
licensing and stuff.

00:29:36.000 --> 00:29:39.030
And then also thinking
about like again, you know,

00:29:39.030 --> 00:29:42.300
in terms of like what the
public is kind of interested in

00:29:42.300 --> 00:29:44.960
and what they might want from
us as cultural institutions.

00:29:44.960 --> 00:29:50.370
Looking at the popularity
of Sci-Hub and LibGen,

00:29:50.370 --> 00:29:54.950
which are basically like, you
know, pirated academic papers

00:29:54.950 --> 00:29:59.380
for free online as they
should be, totally free.

00:29:59.380 --> 00:30:02.840
It's totally illegal
else we are suing them.

00:30:02.840 --> 00:30:07.220
But it's like in the Arron Swartz
like direct action kind of vein

00:30:07.220 --> 00:30:09.780
of like let's put this all
online because people need it

00:30:09.780 --> 00:30:12.200
and people are benefitting
from it, and they love it.

00:30:12.200 --> 00:30:15.280
And everybody who, you know --

00:30:15.280 --> 00:30:19.470
I mean, those of us in academic
institutions or anybody who has

00:30:19.470 --> 00:30:22.350
to deal with any kind of
like, you know, pay wall

00:30:22.350 --> 00:30:26.990
or per use subscription understands
why these things are so popular.

00:30:26.990 --> 00:30:30.880
So like what does that mean
for us in terms of, you know,

00:30:30.880 --> 00:30:36.460
the rights of the copyright
holder but what we need

00:30:36.460 --> 00:30:38.070
to provide as a public service?

00:30:38.070 --> 00:30:39.370
I don't really have an answer.

00:30:39.370 --> 00:30:40.670
I'm just sort of thinking about it.

00:30:40.670 --> 00:30:42.300
&gt;&gt; John Resig: I think one of
the things that's interesting is

00:30:42.300 --> 00:30:43.600
that there are two aspects of this.

00:30:43.600 --> 00:30:50.060
One is legally who has the right to
do activities with a certain thing?

00:30:50.060 --> 00:30:52.770
And that can be usually defined
with the help of a lawyer.

00:30:52.770 --> 00:30:55.520
And I will call out the, you know,

00:30:55.520 --> 00:30:57.700
the work that a near
public library did.

00:30:57.700 --> 00:31:00.190
And they're -- discussion
this afternoon on it as well,

00:31:00.190 --> 00:31:01.490
if I remember [inaudible].

00:31:01.490 --> 00:31:02.790
This afternoon or next afternoon.

00:31:02.790 --> 00:31:04.090
I don't remember.

00:31:04.090 --> 00:31:05.390
But -- this afternoon, okay.

00:31:05.390 --> 00:31:09.400
And -- but the work that they did
to methodically go through a lot

00:31:09.400 --> 00:31:12.930
of their materials and digitize
materials and figure out what was

00:31:12.930 --> 00:31:14.980
or was not in the public domain.

00:31:14.980 --> 00:31:20.120
And like that work is so, so hard
and so few institutions actually do

00:31:20.120 --> 00:31:23.700
that because it is, one,
it's time consuming,

00:31:23.700 --> 00:31:26.890
and that time is usually lawyer
time which is very expensive.

00:31:26.890 --> 00:31:29.060
And, so, like -- and --

00:31:29.060 --> 00:31:32.220
but the thing is, is that a lot
of institutions don't do this

00:31:32.220 --> 00:31:36.620
and instead they hedge their bets
with a lot of very vague language

00:31:36.620 --> 00:31:40.600
where they say, like, oh, this is
public domain for academic use.

00:31:40.600 --> 00:31:42.020
And you're like, what is
that even -- that doesn't --

00:31:42.020 --> 00:31:43.620
is not a thing that
exists, all right?

00:31:43.620 --> 00:31:46.680
You know, like either it's public
domain or it's not public domain.

00:31:46.680 --> 00:31:47.980
But the --

00:31:47.980 --> 00:31:49.280
&gt;&gt; Carson Block: Like a license.

00:31:49.280 --> 00:31:50.580
&gt;&gt; Alison Macrina:
Yeah [laughter], right.

00:31:50.580 --> 00:31:51.880
&gt;&gt; John Resig: So the things
is, is that there is --

00:31:51.880 --> 00:31:53.940
but there's a tricky part of
that here, and maybe this is sort

00:31:53.940 --> 00:31:59.110
of a thread of what we're going
through here is, you know,

00:31:59.110 --> 00:32:02.940
having the right to put something
online is not the same --

00:32:02.940 --> 00:32:05.360
like you could be legally
totally in the clear.

00:32:05.360 --> 00:32:09.250
However, you have to think
about who's going to get upset.

00:32:09.250 --> 00:32:10.820
Who could this impact?

00:32:10.820 --> 00:32:16.990
And so like upsetting Elsevier
is a completely different thing

00:32:16.990 --> 00:32:18.420
from upsetting, you know,

00:32:18.420 --> 00:32:20.840
someone whose personal
information you put on --

00:32:20.840 --> 00:32:22.140
&gt;&gt; Alison Macrina: Totally.

00:32:22.140 --> 00:32:23.440
&gt;&gt; John Resig: -- because
it was published back

00:32:23.440 --> 00:32:24.740 position:56%
in like 1970 in a zine or something.

00:32:24.740 --> 00:32:27.060
Like there's are things that kind
of have to be mentally separating.

00:32:27.060 --> 00:32:30.410
You have to think about who, you
know, who is being hurt by this

00:32:30.410 --> 00:32:32.660
or could be hurt or could be upset.

00:32:32.660 --> 00:32:33.960
Yeah.

00:32:33.960 --> 00:32:35.260
&gt;&gt; Carson Block: Absolutely.

00:32:35.260 --> 00:32:37.290
Well, we have about
10 more minutes left.

00:32:37.290 --> 00:32:38.590
&gt;&gt; Alison Macrina: Oh, wow.

00:32:38.590 --> 00:32:39.890
That went by so fast.

00:32:39.890 --> 00:32:41.190
&gt;&gt; Carson Block: And doesn't it
go -- it just goes by so quick.

00:32:41.190 --> 00:32:42.650
And we know that we
covered a lot of stuff.

00:32:42.650 --> 00:32:45.920
We kind of wanted to have a
smorgasbord of different topics

00:32:45.920 --> 00:32:48.240
that we thought were interesting
in tech trends in libraries

00:32:48.240 --> 00:32:49.650
and cultural heritage institutions.

00:32:49.650 --> 00:32:52.970
But we would love this to
have your voice in it, too.

00:32:52.970 --> 00:32:54.270 position:56%
Is there any comments that you have?

00:32:54.270 --> 00:32:55.900
Any questions that you have for us?

00:32:55.900 --> 00:32:59.350
And I think we've got a roving
microphone around here somewhere.

00:32:59.350 --> 00:33:00.880 position:56%
But we'd love to hear your comments.

00:33:00.880 --> 00:33:04.170
We'd especially love to hear
things that you're struggling with.

00:33:04.170 --> 00:33:06.010
It's good to hear.

00:33:06.010 --> 00:33:08.310
Any questions or comments?

00:33:10.300 --> 00:33:11.600
Lunch was good.

00:33:11.600 --> 00:33:14.290 position:56%
Oh, yes. We've got a gentleman here.

00:33:14.290 --> 00:33:15.590
Let's see.

00:33:15.590 --> 00:33:16.890
I think they're coming over
to you with the microphone.

00:33:16.890 --> 00:33:18.190
&gt;&gt; Okay.

00:33:18.190 --> 00:33:20.490
&gt;&gt; John Resig: Oh,
down the other side.

00:33:20.490 --> 00:33:23.970
&gt;&gt; Carson Block: Oh, here we go.

00:33:23.970 --> 00:33:25.270
Take your choice.

00:33:25.270 --> 00:33:26.570
You can be stereo [laughter].

00:33:26.570 --> 00:33:28.170
Speak in both of them I think.

00:33:28.170 --> 00:33:29.780
&gt;&gt; Alison Macrina: Use them both.

00:33:29.780 --> 00:33:33.060
&gt;&gt; Okay. Two things I
didn't think I heard.

00:33:33.060 --> 00:33:35.240
I'd like to get your opinions on.

00:33:35.240 --> 00:33:36.690
One of them is RDF and link data.

00:33:36.690 --> 00:33:37.990
And the other's deep learning.

00:33:37.990 --> 00:33:39.290
&gt;&gt; Alison Macrina: Wait.

00:33:39.290 --> 00:33:40.590
You --

00:33:40.590 --> 00:33:41.890
&gt;&gt; Carson Block: I'm sorry.

00:33:41.890 --> 00:33:43.190
Could you repeat --

00:33:43.190 --> 00:33:44.490
&gt;&gt; Alison Macrina: --
speak into the microphone.

00:33:44.490 --> 00:33:45.790
I --

00:33:45.790 --> 00:33:47.090
&gt;&gt; Carson Block: Yeah.

00:33:47.090 --> 00:33:48.390
Hold on. We'll --

00:33:48.390 --> 00:33:49.690
&gt;&gt; Hold it up.

00:33:49.690 --> 00:33:50.990
Sorry, sorry.

00:33:50.990 --> 00:33:52.290
&gt;&gt; Carson Block: Thank you.

00:33:52.290 --> 00:33:53.590
&gt;&gt; So two technologies I
didn't hear about which I'd

00:33:53.590 --> 00:33:54.890
like to get your take on -- RDF
and linked data and deep learning.

00:33:54.890 --> 00:33:58.250
The relevance of both of
those or not to the library.

00:33:58.250 --> 00:33:59.550
&gt;&gt; John Resig: Okay.

00:33:59.550 --> 00:34:01.270
Yeah. I guess I have
opinions on these.

00:34:01.270 --> 00:34:07.070
So I've been doing work
with RDF and linked data.

00:34:07.070 --> 00:34:09.410
So, sort of the promise
of this technology is

00:34:09.410 --> 00:34:14.220
that if everyone has their
information in a consistent format,

00:34:14.220 --> 00:34:16.380
talked about in the same way,

00:34:16.380 --> 00:34:20.290
that when you say this is a thing
called a book and it was written

00:34:20.290 --> 00:34:24.030
by a thing called an author,
and that author is described

00:34:24.030 --> 00:34:27.450
by these names and et cetera, et
cetera, that you will be able to,

00:34:27.450 --> 00:34:31.290
you know, show like for example
all the books that were written

00:34:31.290 --> 00:34:33.160
by a particular author
or were published

00:34:33.160 --> 00:34:35.060
by a certain publisher
or et cetera, et cetera.

00:34:35.060 --> 00:34:37.130
And all of this extends
beyond books.

00:34:37.130 --> 00:34:40.250
You can do this for art; you can
do this for all sorts of things.

00:34:40.250 --> 00:34:45.530
I feel like the -- one of the
things that's really tricky

00:34:45.530 --> 00:34:48.060
with the particular thing,
though, is that it is --

00:34:48.060 --> 00:34:52.720 position:56%
it's extremely idealistic in that --

00:34:52.720 --> 00:34:55.960
if you look at the
absolute best case scenario,

00:34:55.960 --> 00:34:59.490
where all the data is perfect and
everyone has put a lot of time

00:34:59.490 --> 00:35:03.210
and effort into making it perfect,
that best case scenario is amazing.

00:35:03.210 --> 00:35:06.800
Like you can do so much stuff and
explore so many relationships.

00:35:06.800 --> 00:35:11.250
However, the reality is that
many institutions do not have the

00:35:11.250 --> 00:35:16.070
ability, either technical or
logistical, to make it perfect.

00:35:16.070 --> 00:35:18.790
And so what you end up having are
a lot of institutions are somewhere

00:35:18.790 --> 00:35:22.280
down here who are just trying
to get basic information

00:35:22.280 --> 00:35:27.020
into their records, and so that
promise is not fully realized.

00:35:27.020 --> 00:35:30.330
So I think this is one thing that
is going to be a major struggle.

00:35:30.330 --> 00:35:33.400
Whereas like big institutions
like the British Museum,

00:35:33.400 --> 00:35:36.730
like they have a ton of like really
good linked data because that's

00:35:36.730 --> 00:35:38.650
because they're the British Museum.

00:35:38.650 --> 00:35:41.450
Whereas like a small,
you know, a small library

00:35:41.450 --> 00:35:43.200
or something is not going
to have that ability.

00:35:43.200 --> 00:35:45.240
They don't have the
staff to make it happen.

00:35:45.240 --> 00:35:47.200
So I think that's one
where you kind of --

00:35:47.200 --> 00:35:49.880
I feel like you have to
kind of take it with a,

00:35:49.880 --> 00:35:51.180
you know, a grain of salt.

00:35:51.180 --> 00:35:54.240
Where it's like, yes, in a perfect
world that could be amazing.

00:35:54.240 --> 00:35:58.200
But think about your specific
use case and think about, okay,

00:35:58.200 --> 00:36:00.690
what are the questions
we're trying to ask?

00:36:00.690 --> 00:36:01.990
How can we answer them?

00:36:01.990 --> 00:36:03.830
And then, how can we do
that with that sort of --

00:36:03.830 --> 00:36:06.680
in that limited little area?

00:36:06.680 --> 00:36:11.650
I guess the other question you
had was about deep learning.

00:36:11.650 --> 00:36:16.770
So, specifically, deep learning
is a type of machine learning,

00:36:16.770 --> 00:36:19.480
and it's been popularized
recently via Google

00:36:19.480 --> 00:36:23.500
and other organizations
doing deep learning for --

00:36:23.500 --> 00:36:25.150
like detecting objects in images,

00:36:25.150 --> 00:36:30.330 position:56%
using it for like their self-driving
cars, and stuff like that.

00:36:30.330 --> 00:36:31.990
And so one of the things --

00:36:31.990 --> 00:36:35.100
like I've been experimenting with
different deep learning algorithms

00:36:35.100 --> 00:36:39.060
to find and annotate images

00:36:39.060 --> 00:36:43.220
to improve their searchability,
for example.

00:36:43.220 --> 00:36:45.100
And this is interesting.

00:36:45.100 --> 00:36:51.660 position:56%
One of the tricky things is that you
have to be aware of what you're --

00:36:51.660 --> 00:36:55.760
how comfortable you are
with wrong answers, okay?

00:36:55.760 --> 00:36:59.310
So like -- because the
thing is that deep learning

00:36:59.310 --> 00:37:02.060
like really good algorithms
will be right.

00:37:02.060 --> 00:37:05.290
Let's say you have an image
with a -- there's a dog in it.

00:37:05.290 --> 00:37:09.990
Most algorithms will say,
in its top five guesses,

00:37:09.990 --> 00:37:13.180
it will be right 60%
of the time, okay?

00:37:13.180 --> 00:37:15.060
Or like just as an example.

00:37:15.060 --> 00:37:17.070
Whereas like that's
like the -- okay --

00:37:17.070 --> 00:37:18.370
&gt;&gt; Carson Block: Sixty percent.

00:37:18.370 --> 00:37:19.670
No, I was just -- I
was in that top five --

00:37:19.670 --> 00:37:20.970
&gt;&gt; John Resig: Yeah, yeah.

00:37:20.970 --> 00:37:22.340
So it will be like
cat, chicken, dog.

00:37:22.340 --> 00:37:24.770
And it's like, okay, well,
you got it mostly right.

00:37:24.770 --> 00:37:27.430
You know, there's a dog
in there [laughter].

00:37:27.430 --> 00:37:31.120
And -- but the, but the -- so
the thing is, is that that --

00:37:31.120 --> 00:37:32.600
&gt;&gt; Carson Block: I think we
went to a fortune teller.

00:37:32.600 --> 00:37:34.960
They had the same success rate.

00:37:34.960 --> 00:37:39.280 position:56%
&gt;&gt; John Resig: So like for example,
I used this for the dataset recently

00:37:39.280 --> 00:37:42.940
of artworks created
by artists, okay?

00:37:42.940 --> 00:37:46.400 position:56%
But the problem is, is that you need
a lot of training data to train it

00:37:46.400 --> 00:37:49.700
to kind of be like okay, here are
a ton of cats and here are a ton

00:37:49.700 --> 00:37:53.200
or dogs or -- but in the case of
paintings, you don't have that.

00:37:53.200 --> 00:37:56.120
You know, you only have a
very small subset of data.

00:37:56.120 --> 00:37:59.440
And I think the problem
is, is that yeah, it's --

00:37:59.440 --> 00:38:03.710
the theoretical future, again,
is going to be very cool.

00:38:03.710 --> 00:38:08.120
In the meantime, you have
to be very willing to have

00:38:08.120 --> 00:38:11.310
like wrong answers be prevalent.

00:38:11.310 --> 00:38:13.940
And you saw this just
recently like --

00:38:13.940 --> 00:38:16.010
again, bring up Flickr
again of all things.

00:38:16.010 --> 00:38:17.480 position:56%
Like they did this for their images.

00:38:17.480 --> 00:38:21.480 position:56%
Whenever you upload an image they
automatically annotate it with tags.

00:38:21.480 --> 00:38:24.150
And then you start having
really bad things happen

00:38:24.150 --> 00:38:26.180
when you have automatic
annotation happen.

00:38:26.180 --> 00:38:31.230
Like one of the things was like it
started to annotate people of color

00:38:31.230 --> 00:38:33.960
with like monkey and
stuff like that,

00:38:33.960 --> 00:38:36.330
which is like terrible, terrible.

00:38:36.330 --> 00:38:38.300
And it's like it should
not be happening.

00:38:38.300 --> 00:38:42.830
But that's because it -- how
they trained the algorithm was

00:38:42.830 --> 00:38:44.130
completely misguided.

00:38:44.130 --> 00:38:45.430
&gt;&gt; Alison Macrina: Right.

00:38:45.430 --> 00:38:46.730
The engineers were like
our algorithm is neutral.

00:38:46.730 --> 00:38:48.060
We showed it only white faces

00:38:48.060 --> 00:38:49.860
because that's what
we think of as people.

00:38:49.860 --> 00:38:53.600
And then, of course, you have
these outcomes that are awful

00:38:53.600 --> 00:38:56.540
and offensive and inaccurate.

00:38:56.540 --> 00:38:59.980 position:56%
And people are like no, but the data
like algorithms are all neutral.

00:38:59.980 --> 00:39:01.590
But people are not, you know.

00:39:01.590 --> 00:39:03.920
Math might be neutral,
but like people can,

00:39:03.920 --> 00:39:06.270
you know -- they screwed up.

00:39:06.270 --> 00:39:07.570
&gt;&gt; John Resig: So,
anyway [laughter].

00:39:07.570 --> 00:39:09.580
As long as we're saying that I feel

00:39:09.580 --> 00:39:12.430
like there's still a lot
of hurdles to overcome.

00:39:12.430 --> 00:39:13.730
&gt;&gt; Alison Macrina: Yeah.

00:39:13.730 --> 00:39:15.030
&gt;&gt; John Resig: And --

00:39:15.030 --> 00:39:16.330 position:56%
&gt;&gt; Alison Macrina: Well, look at
how fast the Microsoft AI went Nazi.

00:39:16.330 --> 00:39:17.630
&gt;&gt; Carson Block: Oh, yeah.

00:39:17.630 --> 00:39:18.930
&gt;&gt; Alison Macrina: She
went full Nazi 24 hours --

00:39:18.930 --> 00:39:20.230
&gt;&gt; Carson Block: Bam, bam.

00:39:20.230 --> 00:39:21.530
&gt;&gt; Alison Macrina: I mean,
if you can't design your bot

00:39:21.530 --> 00:39:23.150
to be Fortran-resistant in 2016 --

00:39:23.150 --> 00:39:25.280
[ Laughter ]

00:39:25.280 --> 00:39:26.580
-- what the hell are you doing?

00:39:26.580 --> 00:39:29.340 position:56%
They were just like repeat after me.

00:39:29.340 --> 00:39:30.640
And she did.

00:39:30.640 --> 00:39:31.940
I mean, of course, you know.

00:39:31.940 --> 00:39:35.250 position:56%
And so one thing I'm very interested
in is a burgeoning movement

00:39:35.250 --> 00:39:40.120
of ethics in AI and machine
learning and bots and all

00:39:40.120 --> 00:39:41.420
that fun stuff, you know?

00:39:41.420 --> 00:39:45.760
Like how do you train
a machine to be cool?

00:39:45.760 --> 00:39:47.060
&gt;&gt; Carson Block: Yeah.

00:39:47.060 --> 00:39:48.460
How do we raise our
digital children?

00:39:48.460 --> 00:39:50.860
&gt;&gt; Alison Macrina: Yeah,
exactly, exactly, exactly.

00:39:50.860 --> 00:39:52.160
&gt;&gt; Carson Block: Yeah, yeah.

00:39:52.160 --> 00:39:53.990
And I love the standards
question because standards I think

00:39:53.990 --> 00:39:57.430
at best are only referenced
by the world, right?

00:39:57.430 --> 00:40:00.080
Like in a closed system
we can maintain them.

00:40:00.080 --> 00:40:02.000
And I'm saying this because
I work on a little --

00:40:02.000 --> 00:40:03.300
a lot of building project.

00:40:03.300 --> 00:40:07.010
And let me tell you something about
how standards and specifications,

00:40:07.010 --> 00:40:10.700
which are hard in a building, are
just kind of referenced sometimes

00:40:10.700 --> 00:40:13.210
in that old construction process.

00:40:13.210 --> 00:40:15.300
So that is a super challenge there,

00:40:15.300 --> 00:40:17.460
especially with the churn
in technology, right?

00:40:17.460 --> 00:40:18.760
Things are changing all the time.

00:40:18.760 --> 00:40:22.160
We're realizing we haven't covered
everything, and how do we cover it?

00:40:22.160 --> 00:40:24.430
Do we have any other questions?

00:40:24.430 --> 00:40:27.910
Is food digesting well after lunch?

00:40:27.910 --> 00:40:29.210
&gt;&gt; John Resig: I know
this is the [inaudible].

00:40:29.210 --> 00:40:30.510 position:56%
&gt;&gt; Alison Macrina: There's one more.

00:40:30.510 --> 00:40:31.810
&gt;&gt; Carson Block: Excellent.

00:40:31.810 --> 00:40:33.110
Oh, good. Thank you.

00:40:33.110 --> 00:40:34.410
Thanks.

00:40:34.410 --> 00:40:36.610
&gt;&gt; I would just like to hear more
about what trends you're excited

00:40:36.610 --> 00:40:40.680
about or any projects you've
heard that are really interesting.

00:40:40.680 --> 00:40:41.980
&gt;&gt; Carson Block: Hmph.

00:40:41.980 --> 00:40:43.690
&gt;&gt; Alison Macrina: One trend
that I'm really excited

00:40:43.690 --> 00:40:49.180
about is something that like
generally I call secure defaults

00:40:49.180 --> 00:40:51.670
which are that we -- the Internet,

00:40:51.670 --> 00:40:53.810
generally speaking, is
a very hostile place.

00:40:53.810 --> 00:40:57.240
It is not secure or private
by default almost ever.

00:40:57.240 --> 00:41:00.080
The whole thing is
written on postcards.

00:41:00.080 --> 00:41:02.290
That means, you know, exactly
what you think it means,

00:41:02.290 --> 00:41:04.370
that anyone can see the
metadata and the content

00:41:04.370 --> 00:41:05.750
of what you're doing
depending on the kind

00:41:05.750 --> 00:41:07.050
of connections you're making.

00:41:07.050 --> 00:41:09.040
So what's been happening
now is there's a push

00:41:09.040 --> 00:41:11.660
to make things secure by default.

00:41:11.660 --> 00:41:16.530
And one project that I'm so excited
about, especially the implications

00:41:16.530 --> 00:41:19.400
that it has for libraries and
cultural heritage institutions,

00:41:19.400 --> 00:41:21.500
is something called Let's Encrypt.

00:41:21.500 --> 00:41:24.170
Let's Encrypt is a project with --

00:41:24.170 --> 00:41:26.510
it's being worked on by the
Electronic Frontier Foundation.

00:41:26.510 --> 00:41:28.630
If you don't know them,
they're sort of like the ACLU

00:41:28.630 --> 00:41:30.010
but just for the Internet.

00:41:30.010 --> 00:41:34.240
Electronic Frontier
Foundation, Mozilla, Akamai,

00:41:34.240 --> 00:41:37.660
a few other really big
Internet Service Institutes,

00:41:37.660 --> 00:41:38.960
you know, public and private.

00:41:38.960 --> 00:41:42.570
And basically what Let's Encrypt
is doing is making it really easy

00:41:42.570 --> 00:41:46.960
for anyone who runs a webserver,
that is to say a website,

00:41:46.960 --> 00:41:53.060 position:56%
to set up strong https encryption on
the whole site by default forever.

00:41:53.060 --> 00:41:58.990
So free automated renewals
every year.

00:41:58.990 --> 00:42:00.390
This is a really important thing

00:42:00.390 --> 00:42:04.100
for libraries especially
given our charge

00:42:04.100 --> 00:42:06.840
for stewardship of
our patrons' privacy.

00:42:06.840 --> 00:42:09.140
If our library websites
are not encrypted,

00:42:09.140 --> 00:42:13.010
if they use just regular
http to connect,

00:42:13.010 --> 00:42:16.060
that means that when your patrons
are going on your site and looking

00:42:16.060 --> 00:42:19.750
up books about divorce or
diabetes or gender identity

00:42:19.750 --> 00:42:23.690
or any other sensitive topics,
that is going out over the wire

00:42:23.690 --> 00:42:26.260
up to the rest of the
Internet in plain text.

00:42:26.260 --> 00:42:27.610
Anyone can see it.

00:42:27.610 --> 00:42:29.210
They can see the originating
location.

00:42:29.210 --> 00:42:30.920
They can see that it's
coming from your library.

00:42:30.920 --> 00:42:34.970
And if you've got an encrypted
login page, like if you encrypt

00:42:34.970 --> 00:42:38.740
where they enter their library
card, that is actually worthless.

00:42:38.740 --> 00:42:41.870
And even worse is that
it can create a cookie

00:42:41.870 --> 00:42:45.230
that then can be intercepted
by someone, so they can steal

00:42:45.230 --> 00:42:48.210 position:56%
that login information and then also
still see everything that's being

00:42:48.210 --> 00:42:50.050
done after the secure login.

00:42:50.050 --> 00:42:52.690
So Let's Encrypt is a project
to make it really simple

00:42:52.690 --> 00:42:54.960
for all library websites
to use encryption.

00:42:54.960 --> 00:42:59.400
I'm really happy that DPLA
has prioritized this actually.

00:42:59.400 --> 00:43:01.580
Mark has been working on this.

00:43:01.580 --> 00:43:05.150
They're not using Let's
Encrypt but they have --

00:43:05.150 --> 00:43:07.880
they're about to like fully
deploy https for their site,

00:43:07.880 --> 00:43:10.500
and they ran into some mixed
content issues and other things.

00:43:10.500 --> 00:43:13.060
But they ended up fixing it and
it took them just about a week,

00:43:13.060 --> 00:43:14.360
I think, to get it all together.

00:43:14.360 --> 00:43:16.660
If your library's interested
in Let's Encrypt,

00:43:16.660 --> 00:43:17.960
and you don't really
know what you're doing,

00:43:17.960 --> 00:43:19.640
you can talk too Library
Freedom Project.

00:43:19.640 --> 00:43:22.110
We have a privacy pledge
that we've gotten a bunch

00:43:22.110 --> 00:43:23.410
of people to sign onto.

00:43:23.410 --> 00:43:24.750
So that's probably
the biggest thing.

00:43:24.750 --> 00:43:27.000
And the sort of the most
important and like something

00:43:27.000 --> 00:43:29.300
that any library should do.

00:43:29.300 --> 00:43:32.990
And I'm encouraged to see
how many have done it so far.

00:43:32.990 --> 00:43:35.070
&gt;&gt; John Resig: I think
very tangentially,

00:43:35.070 --> 00:43:37.840
one of the technologies
that I'm most excited about,

00:43:37.840 --> 00:43:39.660
and I'll second Let's Encrypt.

00:43:39.660 --> 00:43:41.980
I've been using it myself
and I think it's fantastic.

00:43:41.980 --> 00:43:43.280
&gt;&gt; Alison Macrina: Awesome.

00:43:43.280 --> 00:43:44.580
&gt;&gt; John Resig: and it is --

00:43:44.580 --> 00:43:47.680
I've been using different
computer vision algorithms,

00:43:47.680 --> 00:43:53.330
but there's one particular type
of algorithm which is the kind

00:43:53.330 --> 00:43:56.490
that you see in like
Google Image Search or in --

00:43:56.490 --> 00:44:00.740
people familiar with TinEye,
where you can like upload an image

00:44:00.740 --> 00:44:03.680
and it will find images
that look like that image.

00:44:03.680 --> 00:44:06.970
Now I feel like this is a
particular problem that makes

00:44:06.970 --> 00:44:11.390
for a fantastic finding aid because
the thing is, is that when it comes

00:44:11.390 --> 00:44:16.230
to images if you don't have,
you know, amazing metadata to go

00:44:16.230 --> 00:44:19.570
with it, it can be
hard to locate them.

00:44:19.570 --> 00:44:22.920
However, if you have -- computers
are really good at, you know,

00:44:22.920 --> 00:44:26.800
seeing that these two images are
very, very similar to each other.

00:44:26.800 --> 00:44:29.050
So there are different
commercial algorithms out there.

00:44:29.050 --> 00:44:30.360
However, there's one open source,

00:44:30.360 --> 00:44:35.520
one I've been using just recently
called Pastec, P-A-S-T-E-C.

00:44:35.520 --> 00:44:37.580
And it's written by
this French developer.

00:44:37.580 --> 00:44:40.170
And I and some other people

00:44:40.170 --> 00:44:41.900
in the digital humanities
world have been contributing

00:44:41.900 --> 00:44:44.160
to this particular
algorithm because, one,

00:44:44.160 --> 00:44:47.210
it's the only open source one
that we know of, and, two,

00:44:47.210 --> 00:44:49.220
that it seems to actually
be pretty good.

00:44:49.220 --> 00:44:52.350 position:56%
So, like for example, like I've been
using it for some of my projects

00:44:52.350 --> 00:44:55.010
to help scholars locate
our artworks.

00:44:55.010 --> 00:44:59.390
But like the Brooklyn Museum,
for example, just last week,

00:44:59.390 --> 00:45:02.920
they took this and implemented a
piece of software so that people,

00:45:02.920 --> 00:45:06.600
when they're going around the
museum, can photograph artworks

00:45:06.600 --> 00:45:09.070
and it will find all the
information about the artwork,

00:45:09.070 --> 00:45:12.340
and just by looking at
the photo that they took.

00:45:12.340 --> 00:45:13.880
So like it's really
easy to implement.

00:45:13.880 --> 00:45:17.630
So this is one thing I feel like
can have an immediate direct impact

00:45:17.630 --> 00:45:20.730
on people who are like
trying to find information.

00:45:20.730 --> 00:45:23.460
And I think like that's like pretty
much like one of the definitions

00:45:23.460 --> 00:45:24.980
of what a library is
and what it provides.

00:45:24.980 --> 00:45:26.690
So I'm very excited about that.

00:45:26.690 --> 00:45:27.990
&gt;&gt; Carson Block: That's excellent.

00:45:27.990 --> 00:45:29.290
We're almost out --
we are over time,

00:45:29.290 --> 00:45:31.570
but I'll just mention the thing
that I'm like the most excited

00:45:31.570 --> 00:45:32.870
about because I'm a musician

00:45:32.870 --> 00:45:37.620
and that is the zero latency real
time collaboration technology

00:45:37.620 --> 00:45:41.050
which I forgot the name of because
I just learned about it last week.

00:45:41.050 --> 00:45:43.470
But I'll post it online.

00:45:43.470 --> 00:45:45.790
This is really important
for musicians

00:45:45.790 --> 00:45:48.290
who are geographically
dispersed to be able

00:45:48.290 --> 00:45:49.730
to play together in real time.

00:45:49.730 --> 00:45:53.080
There's this thing that happens
for musicians that's really weird,

00:45:53.080 --> 00:45:55.780
and that -- we hear each other
and we listen to each other,

00:45:55.780 --> 00:45:57.080
but when things are
really tight it's

00:45:57.080 --> 00:46:00.250
because we're anticipating what
the other person is going to do.

00:46:00.250 --> 00:46:03.760
And that's what makes an awesome,
awesome musical performance.

00:46:03.760 --> 00:46:05.750
And so I'm extremely
excited by that.

00:46:05.750 --> 00:46:09.660
Now you need a gig
connection for your session,

00:46:09.660 --> 00:46:12.080
not for your library,
but for your session.

00:46:12.080 --> 00:46:15.950
But I just go involved with that
group and I'm psyched about that

00:46:15.950 --> 00:46:17.720
because that fulfills this promise

00:46:17.720 --> 00:46:22.340
of actual interaction using
these wires and radio waves.

00:46:22.340 --> 00:46:23.750
&gt;&gt; Alison Macrina: That's really
all they're good for [laughter].

00:46:23.750 --> 00:46:26.110
&gt;&gt; Carson Block: That's
right, exactly.

00:46:26.110 --> 00:46:27.410
Amen, sister.

00:46:27.410 --> 00:46:30.530
Thank you very much
for your time today.

00:46:30.530 --> 00:46:31.830
Thanks for hanging
out with us today.

00:46:31.830 --> 00:46:33.130
Have a wonderful rest
of the conference.

00:46:33.130 --> 00:46:35.070
[ Applause ]

00:46:35.070 --> 00:46:37.510
&gt;&gt; Alison Macrina: Thanks, guys.

00:46:37.510 --> 00:46:44.210
[ Inaudible Conversations ]

00:46:44.210 --> 00:46:47.870
&gt;&gt; This has been a presentation
of the Library of Congress.

00:46:47.870 --> 00:46:50.220
Visit us at loc.gov.

