WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.271
[MUSIC PLAYING]

00:00:03.271 --> 00:00:05.270
MARTIN WICKE: We're
presenting this paper at KDD

00:00:05.270 --> 00:00:07.436
to share some of the lessons
we've learned designing

00:00:07.436 --> 00:00:08.810
the TensorFlow's APIs.

00:00:08.810 --> 00:00:11.480
We want to make machine
learning as easy as possible.

00:00:11.480 --> 00:00:13.900
That's all of machine learning,
including deep learning,

00:00:13.900 --> 00:00:15.608
including distributed
learning, including

00:00:15.608 --> 00:00:16.972
learning in huge data sets.

00:00:16.972 --> 00:00:18.680
The biggest challenge
to machine learning

00:00:18.680 --> 00:00:20.780
is that it's such a
quickly evolving field.

00:00:20.780 --> 00:00:22.850
If you're not careful,
APIs and frameworks

00:00:22.850 --> 00:00:24.500
can become outdated
very quickly.

00:00:24.500 --> 00:00:26.750
New developments can make
established methods obsolete

00:00:26.750 --> 00:00:28.550
basically overnight.

00:00:28.550 --> 00:00:30.780
Flexibility is therefore
a primary concern.

00:00:30.780 --> 00:00:32.846
APIs must allow custom
behavior and anticipate

00:00:32.846 --> 00:00:34.970
developments and research
results that haven't even

00:00:34.970 --> 00:00:35.780
happened yet.

00:00:35.780 --> 00:00:38.180
TensorFlow's main strengths
are that it's very flexible

00:00:38.180 --> 00:00:40.190
and that it scales to
distributed settings.

00:00:40.190 --> 00:00:43.244
We want to present an API that
preserves this flexibility

00:00:43.244 --> 00:00:44.660
while making the
most common usage

00:00:44.660 --> 00:00:47.000
patterns as simple as possible.

00:00:47.000 --> 00:00:49.560
Distributed machine learning
gets very complex very quickly.

00:00:49.560 --> 00:00:51.768
And we believe that we have
created a very simple way

00:00:51.768 --> 00:00:54.980
to get started, and then scale
up to any problem size or model

00:00:54.980 --> 00:00:55.775
complexity.

00:00:55.775 --> 00:00:58.010
Designing APIs means
that you're making

00:00:58.010 --> 00:00:59.420
a series of design decisions.

00:00:59.420 --> 00:01:01.520
And those represent
fundamental trade-offs.

00:01:01.520 --> 00:01:04.370
We have created tooling that
implements best practices.

00:01:04.370 --> 00:01:07.015
This means that following
best practices is very easy.

00:01:07.015 --> 00:01:08.690
While on the other
hand, if you want

00:01:08.690 --> 00:01:11.460
to do something very unusual,
you won't get as much help.

00:01:11.460 --> 00:01:13.550
That's why we have designed
the APIs in layers.

00:01:13.550 --> 00:01:16.400
Generally the highest level
API will get you there fastest.

00:01:16.400 --> 00:01:18.230
But if you need
something specialized,

00:01:18.230 --> 00:01:20.561
you can escape to the
layer below or below.

00:01:20.561 --> 00:01:23.060
The challenge for us was to
make sure that you can implement

00:01:23.060 --> 00:01:25.010
some custom behavior in
one part of your code,

00:01:25.010 --> 00:01:26.787
say, the [INAUDIBLE]
processing pipeline,

00:01:26.787 --> 00:01:29.120
while reusing all of the
library implementations for all

00:01:29.120 --> 00:01:30.566
the other parts of the code.

00:01:30.566 --> 00:01:31.940
Everything described
in the paper

00:01:31.940 --> 00:01:34.070
is now part of the
regular TensorFlow API.

00:01:34.070 --> 00:01:35.986
You can look at the
estimators and [INAUDIBLE]

00:01:35.986 --> 00:01:38.660
packages for the interfaces
that we describe in the paper.

00:01:38.660 --> 00:01:41.360
They are available
as of TensorFlow 1.2.

00:01:41.360 --> 00:01:44.144
If you're interested in learning
more, please do read our paper.

00:01:44.144 --> 00:01:45.560
If you're new to
machine learning,

00:01:45.560 --> 00:01:47.685
you can check out a talk
about effective TensorFlow

00:01:47.685 --> 00:01:50.090
for non-experts that
I gave at Google IO.

00:01:50.090 --> 00:01:54.040
[MUSIC PLAYING]

