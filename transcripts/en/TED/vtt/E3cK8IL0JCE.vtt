WEBVTT
Kind: captions
Language: en

00:00:13.131 --> 00:00:17.542
My relationship with the internet
reminds me of the setup

00:00:17.566 --> 00:00:19.399
to a clichÃ©d horror movie.

00:00:19.867 --> 00:00:24.253
You know, the blissfully happy family
moves in to their perfect new home,

00:00:24.277 --> 00:00:26.558
excited about their perfect future,

00:00:26.582 --> 00:00:30.103
and it's sunny outside
and the birds are chirping ...

00:00:30.857 --> 00:00:32.696
And then it gets dark.

00:00:32.720 --> 00:00:35.068
And there are noises from the attic.

00:00:35.092 --> 00:00:39.437
And we realize that that perfect
new house isn't so perfect.

00:00:40.485 --> 00:00:43.616
When I started working at Google in 2006,

00:00:43.640 --> 00:00:45.407
Facebook was just a two-year-old,

00:00:45.431 --> 00:00:47.443
and Twitter hadn't yet been born.

00:00:47.848 --> 00:00:52.258
And I was in absolute awe
of the internet and all of its promise

00:00:52.282 --> 00:00:53.719
to make us closer

00:00:53.743 --> 00:00:55.039
and smarter

00:00:55.063 --> 00:00:56.277
and more free.

00:00:57.265 --> 00:01:00.979
But as we were doing the inspiring work
of building search engines

00:01:01.003 --> 00:01:03.889
and video-sharing sites
and social networks,

00:01:04.907 --> 00:01:09.211
criminals, dictators and terrorists
were figuring out

00:01:09.235 --> 00:01:12.437
how to use those same
platforms against us.

00:01:13.417 --> 00:01:15.872
And we didn't have
the foresight to stop them.

00:01:16.746 --> 00:01:21.845
Over the last few years, geopolitical
forces have come online to wreak havoc.

00:01:21.869 --> 00:01:23.038
And in response,

00:01:23.062 --> 00:01:27.840
Google supported a few colleagues and me
to set up a new group called Jigsaw,

00:01:27.864 --> 00:01:32.460
with a mandate to make people safer
from threats like violent extremism,

00:01:32.484 --> 00:01:34.562
censorship, persecution --

00:01:35.186 --> 00:01:39.303
threats that feel very personal to me
because I was born in Iran,

00:01:39.327 --> 00:01:42.256
and I left in the aftermath
of a violent revolution.

00:01:43.525 --> 00:01:47.871
But I've come to realize
that even if we had all of the resources

00:01:47.895 --> 00:01:50.753
of all of the technology
companies in the world,

00:01:51.595 --> 00:01:52.825
we'd still fail

00:01:53.586 --> 00:01:56.534
if we overlooked one critical ingredient:

00:01:57.653 --> 00:02:03.442
the human experiences of the victims
and perpetrators of those threats.

00:02:04.935 --> 00:02:07.671
There are many challenges
I could talk to you about today.

00:02:07.695 --> 00:02:09.199
I'm going to focus on just two.

00:02:09.623 --> 00:02:11.702
The first is terrorism.

00:02:13.563 --> 00:02:16.120
So in order to understand
the radicalization process,

00:02:16.144 --> 00:02:20.431
we met with dozens of former members
of violent extremist groups.

00:02:21.590 --> 00:02:24.073
One was a British schoolgirl,

00:02:25.049 --> 00:02:28.748
who had been taken off of a plane
at London Heathrow

00:02:28.772 --> 00:02:33.464
as she was trying to make her way
to Syria to join ISIS.

00:02:34.281 --> 00:02:36.212
And she was 13 years old.

00:02:37.792 --> 00:02:42.417
So I sat down with her and her father,
and I said, "Why?"

00:02:42.441 --> 00:02:44.158
And she said,

00:02:44.182 --> 00:02:47.821
"I was looking at pictures
of what life is like in Syria,

00:02:47.845 --> 00:02:51.355
and I thought I was going to go
and live in the Islamic Disney World."

00:02:52.527 --> 00:02:54.611
That's what she saw in ISIS.

00:02:54.635 --> 00:02:58.127
She thought she'd meet and marry
a jihadi Brad Pitt

00:02:58.151 --> 00:03:01.209
and go shopping in the mall all day
and live happily ever after.

00:03:02.977 --> 00:03:05.801
ISIS understands what drives people,

00:03:05.825 --> 00:03:09.369
and they carefully craft a message
for each audience.

00:03:11.122 --> 00:03:12.633
Just look at how many languages

00:03:12.657 --> 00:03:14.930
they translate their
marketing material into.

00:03:15.677 --> 00:03:18.338
They make pamphlets,
radio shows and videos

00:03:18.362 --> 00:03:20.335
in not just English and Arabic,

00:03:20.359 --> 00:03:25.126
but German, Russian,
French, Turkish, Kurdish,

00:03:25.150 --> 00:03:26.822
Hebrew,

00:03:26.846 --> 00:03:28.587
Mandarin Chinese.

00:03:29.309 --> 00:03:33.501
I've even seen an ISIS-produced
video in sign language.

00:03:34.605 --> 00:03:36.489
Just think about that for a second:

00:03:36.513 --> 00:03:38.821
ISIS took the time and made the effort

00:03:38.845 --> 00:03:42.649
to ensure their message is reaching
the deaf and hard of hearing.

00:03:45.143 --> 00:03:47.287
It's actually not tech-savviness

00:03:47.311 --> 00:03:49.906
that is the reason why
ISIS wins hearts and minds.

00:03:49.930 --> 00:03:54.093
It's their insight into the prejudices,
the vulnerabilities, the desires

00:03:54.117 --> 00:03:55.891
of the people they're trying to reach

00:03:55.915 --> 00:03:57.076
that does that.

00:03:57.718 --> 00:03:59.147
That's why it's not enough

00:03:59.171 --> 00:04:03.410
for the online platforms
to focus on removing recruiting material.

00:04:04.518 --> 00:04:08.099
If we want to have a shot
at building meaningful technology

00:04:08.123 --> 00:04:09.997
that's going to counter radicalization,

00:04:10.021 --> 00:04:13.000
we have to start with the human
journey at its core.

00:04:13.884 --> 00:04:16.071
So we went to Iraq

00:04:16.095 --> 00:04:18.926
to speak to young men
who'd bought into ISIS's promise

00:04:18.950 --> 00:04:22.141
of heroism and righteousness,

00:04:22.165 --> 00:04:24.012
who'd taken up arms to fight for them

00:04:24.036 --> 00:04:25.374
and then who'd defected

00:04:25.398 --> 00:04:28.419
after they witnessed
the brutality of ISIS's rule.

00:04:28.880 --> 00:04:32.072
And I'm sitting there in this makeshift
prison in the north of Iraq

00:04:32.096 --> 00:04:36.646
with this 23-year-old who had actually
trained as a suicide bomber

00:04:36.670 --> 00:04:38.222
before defecting.

00:04:39.080 --> 00:04:40.238
And he says,

00:04:41.119 --> 00:04:44.339
"I arrived in Syria full of hope,

00:04:44.363 --> 00:04:48.728
and immediately, I had two
of my prized possessions confiscated:

00:04:48.752 --> 00:04:51.685
my passport and my mobile phone."

00:04:52.140 --> 00:04:54.546
The symbols of his physical
and digital liberty

00:04:54.570 --> 00:04:56.330
were taken away from him on arrival.

00:04:57.248 --> 00:05:00.758
And then this is the way he described
that moment of loss to me.

00:05:01.356 --> 00:05:02.942
He said,

00:05:02.966 --> 00:05:05.295
"You know in 'Tom and Jerry,'

00:05:06.192 --> 00:05:09.295
when Jerry wants to escape,
and then Tom locks the door

00:05:09.319 --> 00:05:10.475
and swallows the key

00:05:10.499 --> 00:05:14.050
and you see it bulging out
of his throat as it travels down?"

00:05:14.446 --> 00:05:17.599
And of course, I really could see
the image that he was describing,

00:05:17.623 --> 00:05:21.284
and I really did connect with the feeling
that he was trying to convey,

00:05:21.308 --> 00:05:23.329
which was one of doom,

00:05:23.353 --> 00:05:25.142
when you know there's no way out.

00:05:26.551 --> 00:05:27.840
And I was wondering:

00:05:28.644 --> 00:05:31.326
What, if anything,
could have changed his mind

00:05:31.350 --> 00:05:32.590
the day that he left home?

00:05:32.614 --> 00:05:33.864
So I asked,

00:05:33.888 --> 00:05:37.066
"If you knew everything that you know now

00:05:37.090 --> 00:05:40.141
about the suffering
and the corruption, the brutality --

00:05:40.165 --> 00:05:41.580
that day you left home,

00:05:41.604 --> 00:05:43.283
would you still have gone?"

00:05:43.786 --> 00:05:45.497
And he said, "Yes."

00:05:45.846 --> 00:05:48.128
And I thought, "Holy crap, he said 'Yes.'"

00:05:48.694 --> 00:05:49.913
And then he said,

00:05:49.937 --> 00:05:52.938
"At that point, I was so brainwashed,

00:05:52.962 --> 00:05:56.206
I wasn't taking in
any contradictory information.

00:05:56.744 --> 00:05:58.299
I couldn't have been swayed."

00:05:59.235 --> 00:06:01.762
"Well, what if you knew
everything that you know now

00:06:01.786 --> 00:06:03.884
six months before the day that you left?"

00:06:05.345 --> 00:06:08.476
"At that point, I think it probably
would have changed my mind."

00:06:10.138 --> 00:06:13.535
Radicalization isn't
this yes-or-no choice.

00:06:14.007 --> 00:06:16.984
It's a process, during which
people have questions --

00:06:17.008 --> 00:06:20.784
about ideology, religion,
the living conditions.

00:06:20.808 --> 00:06:23.574
And they're coming online for answers,

00:06:23.598 --> 00:06:25.515
which is an opportunity to reach them.

00:06:25.905 --> 00:06:28.919
And there are videos online
from people who have answers --

00:06:28.943 --> 00:06:31.819
defectors, for example,
telling the story of their journey

00:06:31.843 --> 00:06:33.426
into and out of violence;

00:06:33.450 --> 00:06:36.937
stories like the one from that man
I met in the Iraqi prison.

00:06:37.914 --> 00:06:40.504
There are locals who've uploaded
cell phone footage

00:06:40.528 --> 00:06:44.031
of what life is really like
in the caliphate under ISIS's rule.

00:06:44.055 --> 00:06:47.790
There are clerics who are sharing
peaceful interpretations of Islam.

00:06:48.830 --> 00:06:49.980
But you know what?

00:06:50.004 --> 00:06:53.024
These people don't generally have
the marketing prowess of ISIS.

00:06:54.049 --> 00:06:58.581
They risk their lives to speak up
and confront terrorist propaganda,

00:06:58.605 --> 00:07:00.816
and then they tragically
don't reach the people

00:07:00.840 --> 00:07:02.522
who most need to hear from them.

00:07:03.173 --> 00:07:05.785
And we wanted to see
if technology could change that.

00:07:06.205 --> 00:07:10.388
So in 2016, we partnered with Moonshot CVE

00:07:10.412 --> 00:07:13.592
to pilot a new approach
to countering radicalization

00:07:13.616 --> 00:07:15.396
called the "Redirect Method."

00:07:16.453 --> 00:07:19.465
It uses the power of online advertising

00:07:19.489 --> 00:07:24.003
to bridge the gap between
those susceptible to ISIS's messaging

00:07:24.027 --> 00:07:27.787
and those credible voices
that are debunking that messaging.

00:07:28.633 --> 00:07:29.783
And it works like this:

00:07:29.807 --> 00:07:31.768
someone looking for extremist material --

00:07:31.792 --> 00:07:34.782
say they search
for "How do I join ISIS?" --

00:07:34.806 --> 00:07:37.282
will see an ad appear

00:07:37.306 --> 00:07:42.188
that invites them to watch a YouTube video
of a cleric, of a defector --

00:07:42.212 --> 00:07:44.522
someone who has an authentic answer.

00:07:44.546 --> 00:07:48.169
And that targeting is based
not on a profile of who they are,

00:07:48.193 --> 00:07:51.246
but of determining something
that's directly relevant

00:07:51.270 --> 00:07:52.978
to their query or question.

00:07:54.122 --> 00:07:56.964
During our eight-week pilot
in English and Arabic,

00:07:56.988 --> 00:08:00.267
we reached over 300,000 people

00:08:00.291 --> 00:08:05.836
who had expressed an interest in
or sympathy towards a jihadi group.

00:08:06.626 --> 00:08:08.890
These people were now watching videos

00:08:08.914 --> 00:08:12.254
that could prevent them
from making devastating choices.

00:08:13.405 --> 00:08:17.132
And because violent extremism
isn't confined to any one language,

00:08:17.156 --> 00:08:18.960
religion or ideology,

00:08:18.984 --> 00:08:22.485
the Redirect Method is now
being deployed globally

00:08:22.509 --> 00:08:26.313
to protect people being courted online
by violent ideologues,

00:08:26.337 --> 00:08:28.933
whether they're Islamists,
white supremacists

00:08:28.957 --> 00:08:31.060
or other violent extremists,

00:08:31.084 --> 00:08:33.957
with the goal of giving them the chance
to hear from someone

00:08:33.981 --> 00:08:36.072
on the other side of that journey;

00:08:36.096 --> 00:08:38.935
to give them the chance to choose
a different path.

00:08:40.749 --> 00:08:46.729
It turns out that often the bad guys
are good at exploiting the internet,

00:08:46.753 --> 00:08:50.497
not because they're some kind
of technological geniuses,

00:08:50.521 --> 00:08:53.506
but because they understand
what makes people tick.

00:08:54.855 --> 00:08:57.224
I want to give you a second example:

00:08:58.019 --> 00:08:59.410
online harassment.

00:09:00.629 --> 00:09:03.992
Online harassers also work
to figure out what will resonate

00:09:04.016 --> 00:09:05.631
with another human being.

00:09:05.655 --> 00:09:08.765
But not to recruit them like ISIS does,

00:09:08.789 --> 00:09:10.064
but to cause them pain.

00:09:11.259 --> 00:09:12.601
Imagine this:

00:09:13.347 --> 00:09:15.006
you're a woman,

00:09:15.030 --> 00:09:16.443
you're married,

00:09:16.467 --> 00:09:17.621
you have a kid.

00:09:18.834 --> 00:09:20.618
You post something on social media,

00:09:20.642 --> 00:09:23.528
and in a reply,
you're told that you'll be raped,

00:09:24.577 --> 00:09:26.137
that your son will be watching,

00:09:26.825 --> 00:09:28.681
details of when and where.

00:09:29.148 --> 00:09:32.291
In fact, your home address
is put online for everyone to see.

00:09:33.580 --> 00:09:35.587
That feels like a pretty real threat.

00:09:37.113 --> 00:09:38.769
Do you think you'd go home?

00:09:39.999 --> 00:09:43.047
Do you think you'd continue doing
the thing that you were doing?

00:09:43.071 --> 00:09:46.291
Would you continue doing that thing
that's irritating your attacker?

00:09:48.016 --> 00:09:51.112
Online abuse has been this perverse art

00:09:51.136 --> 00:09:54.604
of figuring out what makes people angry,

00:09:54.628 --> 00:09:56.760
what makes people afraid,

00:09:56.784 --> 00:09:58.425
what makes people insecure,

00:09:58.449 --> 00:10:01.516
and then pushing those pressure points
until they're silenced.

00:10:02.333 --> 00:10:04.637
When online harassment goes unchecked,

00:10:04.661 --> 00:10:06.328
free speech is stifled.

00:10:07.196 --> 00:10:09.323
And even the people
hosting the conversation

00:10:09.347 --> 00:10:11.181
throw up their arms and call it quits,

00:10:11.205 --> 00:10:14.162
closing their comment sections
and their forums altogether.

00:10:14.186 --> 00:10:17.035
That means we're actually
losing spaces online

00:10:17.059 --> 00:10:19.046
to meet and exchange ideas.

00:10:19.939 --> 00:10:22.102
And where online spaces remain,

00:10:22.126 --> 00:10:26.596
we descend into echo chambers
with people who think just like us.

00:10:27.688 --> 00:10:30.187
But that enables
the spread of disinformation;

00:10:30.211 --> 00:10:32.395
that facilitates polarization.

00:10:34.508 --> 00:10:39.777
What if technology instead
could enable empathy at scale?

00:10:40.451 --> 00:10:42.937
This was the question
that motivated our partnership

00:10:42.961 --> 00:10:44.780
with Google's Counter Abuse team,

00:10:44.804 --> 00:10:45.982
Wikipedia

00:10:46.006 --> 00:10:47.940
and newspapers like the New York Times.

00:10:47.964 --> 00:10:50.840
We wanted to see if we could build
machine-learning models

00:10:50.864 --> 00:10:54.470
that could understand
the emotional impact of language.

00:10:55.062 --> 00:10:58.672
Could we predict which comments
were likely to make someone else leave

00:10:58.696 --> 00:11:00.070
the online conversation?

00:11:00.515 --> 00:11:04.402
And that's no mean feat.

00:11:04.426 --> 00:11:05.992
That's no trivial accomplishment

00:11:06.016 --> 00:11:08.579
for AI to be able to do
something like that.

00:11:08.603 --> 00:11:12.332
I mean, just consider
these two examples of messages

00:11:12.356 --> 00:11:14.580
that could have been sent to me last week.

00:11:15.517 --> 00:11:17.396
"Break a leg at TED!"

00:11:17.420 --> 00:11:18.584
... and

00:11:18.608 --> 00:11:20.734
"I'll break your legs at TED."

00:11:20.758 --> 00:11:22.004
(Laughter)

00:11:22.028 --> 00:11:23.541
You are human,

00:11:23.565 --> 00:11:25.775
that's why that's an obvious
difference to you,

00:11:25.799 --> 00:11:28.023
even though the words
are pretty much the same.

00:11:28.047 --> 00:11:31.126
But for AI, it takes some training
to teach the models

00:11:31.150 --> 00:11:32.721
to recognize that difference.

00:11:32.745 --> 00:11:35.990
The beauty of building AI
that can tell the difference

00:11:36.014 --> 00:11:41.064
is that AI can then scale to the size
of the online toxicity phenomenon,

00:11:41.088 --> 00:11:44.375
and that was our goal in building
our technology called Perspective.

00:11:45.056 --> 00:11:46.483
With the help of Perspective,

00:11:46.507 --> 00:11:48.090
the New York Times, for example,

00:11:48.114 --> 00:11:50.601
has increased spaces
online for conversation.

00:11:51.005 --> 00:11:52.315
Before our collaboration,

00:11:52.339 --> 00:11:56.644
they only had comments enabled
on just 10 percent of their articles.

00:11:57.495 --> 00:11:59.139
With the help of machine learning,

00:11:59.163 --> 00:12:01.060
they have that number up to 30 percent.

00:12:01.084 --> 00:12:02.240
So they've tripled it,

00:12:02.264 --> 00:12:04.181
and we're still just getting started.

00:12:04.872 --> 00:12:08.333
But this is about way more than just
making moderators more efficient.

00:12:10.076 --> 00:12:11.926
Right now I can see you,

00:12:11.950 --> 00:12:15.244
and I can gauge how what I'm saying
is landing with you.

00:12:16.370 --> 00:12:18.249
You don't have that opportunity online.

00:12:18.558 --> 00:12:22.193
Imagine if machine learning
could give commenters,

00:12:22.217 --> 00:12:23.379
as they're typing,

00:12:23.403 --> 00:12:26.750
real-time feedback about how
their words might land,

00:12:27.609 --> 00:12:30.633
just like facial expressions do
in a face-to-face conversation.

00:12:32.926 --> 00:12:34.768
Machine learning isn't perfect,

00:12:34.792 --> 00:12:37.186
and it still makes plenty of mistakes.

00:12:37.210 --> 00:12:38.767
But if we can build technology

00:12:38.791 --> 00:12:42.084
that understands the emotional
impact of language,

00:12:42.108 --> 00:12:43.568
we can build empathy.

00:12:43.592 --> 00:12:46.017
That means that we can have
dialogue between people

00:12:46.041 --> 00:12:47.857
with different politics,

00:12:47.881 --> 00:12:49.097
different worldviews,

00:12:49.121 --> 00:12:50.367
different values.

00:12:51.359 --> 00:12:56.134
And we can reinvigorate the spaces online
that most of us have given up on.

00:12:57.857 --> 00:13:01.642
When people use technology
to exploit and harm others,

00:13:01.666 --> 00:13:05.308
they're preying on our human fears
and vulnerabilities.

00:13:06.461 --> 00:13:09.969
If we ever thought
that we could build an internet

00:13:09.993 --> 00:13:12.571
insulated from the dark side of humanity,

00:13:12.595 --> 00:13:13.779
we were wrong.

00:13:14.361 --> 00:13:16.631
If we want today to build technology

00:13:16.655 --> 00:13:19.782
that can overcome
the challenges that we face,

00:13:19.806 --> 00:13:23.849
we have to throw our entire selves
into understanding the issues

00:13:23.873 --> 00:13:25.766
and into building solutions

00:13:25.790 --> 00:13:29.572
that are as human as the problems
they aim to solve.

00:13:30.071 --> 00:13:31.584
Let's make that happen.

00:13:31.924 --> 00:13:33.074
Thank you.

00:13:33.098 --> 00:13:36.375
(Applause)

