WEBVTT
Kind: captions
Language: en

00:00:06.050 --> 00:00:09.830
RJ MICHAEL: Hello
everyone I'm RJ Michael.

00:00:09.830 --> 00:00:14.050
I'm Director of
Games for Google.

00:00:14.050 --> 00:00:17.370
And is my great pleasure to
introduce you today one Ray

00:00:17.370 --> 00:00:19.750
Kurzweil.

00:00:19.750 --> 00:00:23.840
Ray Kurzweil is one of the
world's leading inventors,

00:00:23.840 --> 00:00:28.000
thinkers, and futurists,
with a 30-year track

00:00:28.000 --> 00:00:31.890
record of accurate predictions.

00:00:31.890 --> 00:00:34.110
He's called many things
by a number of sources.

00:00:34.110 --> 00:00:37.120
He's called "the restless
genius" by the "Wall Street

00:00:37.120 --> 00:00:42.140
Journal," "the ultimate thinking
machine" by "Forbes Magazine."

00:00:42.140 --> 00:00:45.750
PBS selected him as one of
their 16 revolutionaries

00:00:45.750 --> 00:00:48.100
who made America.

00:00:48.100 --> 00:00:50.720
Kurzweil was selected as
one of the top entrepreneurs

00:00:50.720 --> 00:00:53.750
by "Inc. Magazine,"
who described him

00:00:53.750 --> 00:00:59.750
as "the rightful heir
to Thomas Edison."

00:00:59.750 --> 00:01:01.650
Kurzweil was a
principal inventor

00:01:01.650 --> 00:01:06.550
of the first CCD flatbed
scanner, the first omnifont,

00:01:06.550 --> 00:01:09.080
optical character
recognition system.

00:01:09.080 --> 00:01:13.000
He did the first omnifont OCR.

00:01:13.000 --> 00:01:15.410
He did the first printed
speech reading machine

00:01:15.410 --> 00:01:18.450
for the blind, the first
text-to-speech synthesizer,

00:01:18.450 --> 00:01:21.170
the first music synthesizer
capable of recreating

00:01:21.170 --> 00:01:25.050
the grand piano and other
orchestral instruments,

00:01:25.050 --> 00:01:29.100
and the first commercially
marketed large vocabulary

00:01:29.100 --> 00:01:31.064
speech recognition system.

00:01:31.064 --> 00:01:33.230
This is the stuff that the
guy's done with his life,

00:01:33.230 --> 00:01:34.610
with his career so far.

00:01:34.610 --> 00:01:36.580
Among his many honors,
he's the recipient

00:01:36.580 --> 00:01:39.350
of the National
Medal of Technology.

00:01:39.350 --> 00:01:41.430
He was inducted into
the National Inventors

00:01:41.430 --> 00:01:42.760
Hall of Fame.

00:01:42.760 --> 00:01:46.990
He holds 20 honorary doctorates,
and he has received honors

00:01:46.990 --> 00:01:50.240
from three US Presidents.

00:01:50.240 --> 00:01:53.370
Ray has written five national
bestselling books, including

00:01:53.370 --> 00:01:58.010
the New York Times best seller,
"The Singularity is Near,"

00:01:58.010 --> 00:02:04.360
in 2005, and more recently
"How to Create a Mind" in 2012.

00:02:04.360 --> 00:02:06.980
He is Director of
Engineering at Google.

00:02:06.980 --> 00:02:11.210
He is now heading up a team
developing machine intelligence

00:02:11.210 --> 00:02:14.530
and natural language
understanding.

00:02:14.530 --> 00:02:19.460
He has also been a personal
hero of mine since I was young.

00:02:19.460 --> 00:02:25.700
And he has this drive to
make AI happen at Google.

00:02:25.700 --> 00:02:28.310
And I couldn't be more delighted
to announce to you guys

00:02:28.310 --> 00:02:30.770
that, as of today,
I am now going

00:02:30.770 --> 00:02:34.180
to start exploring the
entertainment and education

00:02:34.180 --> 00:02:38.580
space directly with Ray Kurzweil
in his new organization.

00:02:38.580 --> 00:02:40.350
And who knows where
this is going to go,

00:02:40.350 --> 00:02:42.000
but it's going to be awesome.

00:02:42.000 --> 00:02:45.270
Ladies and gentlemen, may
I introduce Ray Kurzweil.

00:02:45.270 --> 00:02:49.206
[APPLAUSE]

00:02:59.050 --> 00:03:01.130
RAY KURZWEIL: Thank
you, and thanks, RJ.

00:03:01.130 --> 00:03:04.130
And now that you're going
to be working with me,

00:03:04.130 --> 00:03:08.180
we have to work on your
enthusiasm a little bit.

00:03:08.180 --> 00:03:10.030
And it's great to be at Google.

00:03:10.030 --> 00:03:11.640
It's actually my first job.

00:03:11.640 --> 00:03:14.730
I'll talk a little bit
more about what I'm doing,

00:03:14.730 --> 00:03:19.170
but I'd like to share with
you some ideas about thinking.

00:03:19.170 --> 00:03:21.770
I've been thinking about
thinking for 50 years.

00:03:21.770 --> 00:03:23.380
First, I would like
to say a few words

00:03:23.380 --> 00:03:25.330
about the law of
accelerating returns.

00:03:25.330 --> 00:03:28.040
I won't say too much about
it, because many of you

00:03:28.040 --> 00:03:30.740
have heard me talk
about it before,

00:03:30.740 --> 00:03:34.040
but the law of accelerating
returns is alive and well.

00:03:34.040 --> 00:03:36.857
It's not just Moore's Law.

00:03:36.857 --> 00:03:38.690
I keep hearing people
say, well, Moore's Law

00:03:38.690 --> 00:03:40.200
is coming to an
end, as if that were

00:03:40.200 --> 00:03:43.970
synonymous with the exponential
growth of information

00:03:43.970 --> 00:03:44.510
technology.

00:03:44.510 --> 00:03:47.370
Moore's law was just
one paradigm among many.

00:03:47.370 --> 00:03:49.310
When I first studied
this in 1981,

00:03:49.310 --> 00:03:52.240
Moore's Law had only been
underway for a few years, which

00:03:52.240 --> 00:03:55.110
is shrinking components
on an integrated circuit.

00:03:55.110 --> 00:03:57.420
1950s, there were
shrinking vacuum tubes

00:03:57.420 --> 00:03:59.820
to keep this exponential
growth going.

00:04:02.610 --> 00:04:04.530
Gordon Moore originally
said Moore's Law

00:04:04.530 --> 00:04:06.330
would come to an end in 2002.

00:04:06.330 --> 00:04:09.900
Justin Rattner, the CTO
of Intel, now says 2022.

00:04:09.900 --> 00:04:12.280
But he'll also show
you, in their labs,

00:04:12.280 --> 00:04:14.300
the sixth paradigm--
three-dimensional,

00:04:14.300 --> 00:04:16.110
self-organizing
molecular circuits

00:04:16.110 --> 00:04:20.190
working, which will keep the law
of accelerating returns going

00:04:20.190 --> 00:04:23.040
well into late of this century.

00:04:23.040 --> 00:04:25.070
So what is the law of
accelerating returns?

00:04:25.070 --> 00:04:27.650
It's not that everything
grows exponentially, or even

00:04:27.650 --> 00:04:30.690
every aspect of
information technology.

00:04:30.690 --> 00:04:32.630
We're speeding up
transistors, making

00:04:32.630 --> 00:04:35.240
them smaller so the electrons
have less distance to travel,

00:04:35.240 --> 00:04:37.510
so transistors were faster.

00:04:37.510 --> 00:04:39.800
That was a sub-paradigm.

00:04:39.800 --> 00:04:41.690
I always felt we
were going too fast.

00:04:41.690 --> 00:04:43.890
The human brain is
actually very slow,

00:04:43.890 --> 00:04:47.180
computes about 100
calculations per second

00:04:47.180 --> 00:04:49.920
in the interneuronal
connections.

00:04:49.920 --> 00:04:51.700
But it's massively parallel.

00:04:51.700 --> 00:04:54.600
So there's actually a 100
trillion-fold parallelism.

00:04:54.600 --> 00:04:59.030
There's 100 billion neurons with
about 10,000 connections each,

00:04:59.030 --> 00:05:01.480
and that's where the
computations take place.

00:05:01.480 --> 00:05:03.910
So it's a very massively
parallel, but slow.

00:05:03.910 --> 00:05:06.300
So it uses very
little energy, and we

00:05:06.300 --> 00:05:08.520
needed to move more
in that direction,

00:05:08.520 --> 00:05:10.340
towards more parallelism
and less speed.

00:05:10.340 --> 00:05:15.200
So speeding up transistors
was just one sub-paradigm.

00:05:15.200 --> 00:05:19.330
And we already have taken
steps in the third dimension.

00:05:19.330 --> 00:05:22.800
Many memory circuits are
already three-dimensional.

00:05:22.800 --> 00:05:25.660
The law of accelerating
returns is not

00:05:25.660 --> 00:05:27.650
limited at all to Moore's Law.

00:05:27.650 --> 00:05:30.520
Moore's Law is one paradigm
among many in computation.

00:05:30.520 --> 00:05:33.120
Computation is one
example of many

00:05:33.120 --> 00:05:35.180
of the law of
accelerating returns.

00:05:35.180 --> 00:05:37.650
So what exactly
does it pertain to?

00:05:37.650 --> 00:05:41.520
It pertains to the price
performance and capacity

00:05:41.520 --> 00:05:45.610
of information
technology, and every form

00:05:45.610 --> 00:05:46.910
of information technology.

00:05:46.910 --> 00:05:50.900
So in computation, calculations
per second per constant dollar

00:05:50.900 --> 00:05:53.850
has been speeding up at an
exponential pace since the 1890

00:05:53.850 --> 00:05:54.750
census.

00:05:54.750 --> 00:05:56.550
But it's also true
of communications,

00:05:56.550 --> 00:06:02.090
biological technologies, brain
scanning, modeling the brain,

00:06:02.090 --> 00:06:05.300
being able to reprogram
biological data,

00:06:05.300 --> 00:06:08.672
printing-- the
spatial resolution

00:06:08.672 --> 00:06:11.130
of three-dimensional printing,
which is turning information

00:06:11.130 --> 00:06:12.830
into physical
products-- these are

00:06:12.830 --> 00:06:16.470
all examples of either price
performance or capacity

00:06:16.470 --> 00:06:18.800
of information technology.

00:06:18.800 --> 00:06:21.520
And I'll show you
this first example.

00:06:25.190 --> 00:06:28.110
I mean, this is the graph I
had in 1981 of computation.

00:06:28.110 --> 00:06:29.290
It's a logarithmic scale.

00:06:29.290 --> 00:06:31.740
Every label level is
100,000 times greater

00:06:31.740 --> 00:06:33.370
than the level below it.

00:06:33.370 --> 00:06:36.865
And so this represents
trillions-fold increase

00:06:36.865 --> 00:06:39.870
in calculations per second
for constant dollar.

00:06:39.870 --> 00:06:42.220
Moore's Law is the
fifth paradigm.

00:06:42.220 --> 00:06:46.047
But notice how smooth
a trajectory that is.

00:06:46.047 --> 00:06:47.380
It really has a mind of its own.

00:06:47.380 --> 00:06:49.790
It goes through thick and
thin through one piece.

00:06:49.790 --> 00:06:52.470
And exponential growth-- the
second point I want to make--

00:06:52.470 --> 00:06:54.332
is not intuitive.

00:06:54.332 --> 00:06:56.290
If you ever wonder, gee,
why do I have a brain?

00:06:56.290 --> 00:06:58.350
It's really to make
predictions about the future

00:06:58.350 --> 00:07:01.180
so we can anticipate the
consequences of our action,

00:07:01.180 --> 00:07:03.650
and the consequences
of inaction.

00:07:03.650 --> 00:07:06.377
But those built-in
predictors are linear.

00:07:06.377 --> 00:07:08.710
When we were walking through
the fields 1,000 years ago,

00:07:08.710 --> 00:07:11.170
we would make a prediction-- OK,
that animal is going that way.

00:07:11.170 --> 00:07:12.020
I don't want to meet him.

00:07:12.020 --> 00:07:13.586
I'm going to go a
different route.

00:07:13.586 --> 00:07:14.710
That was good for survival.

00:07:14.710 --> 00:07:16.720
That became hardwired
in our brains,

00:07:16.720 --> 00:07:18.940
but those predictors are linear.

00:07:18.940 --> 00:07:22.390
And people still use
their linear intuition

00:07:22.390 --> 00:07:23.200
about the future.

00:07:23.200 --> 00:07:26.916
That's the principal difference
between myself and my critics.

00:07:26.916 --> 00:07:28.540
We're both looking
at the same reality.

00:07:28.540 --> 00:07:30.590
We have similar
judgments about it.

00:07:30.590 --> 00:07:32.730
And if I thought
progress was linear,

00:07:32.730 --> 00:07:35.540
I'd be pessimistic also.

00:07:35.540 --> 00:07:37.440
And many things are linear.

00:07:37.440 --> 00:07:39.840
Biology-- health and
medicine was linear.

00:07:39.840 --> 00:07:41.110
That was still useful.

00:07:41.110 --> 00:07:43.620
The life expectancy
was 19 1,000 years ago.

00:07:43.620 --> 00:07:45.082
We've quadrupled it.

00:07:45.082 --> 00:07:47.290
I talked recently to some
junior high school students

00:07:47.290 --> 00:07:49.740
and pointed out they would
all be senior citizens

00:07:49.740 --> 00:07:51.720
if it hadn't been
for that progress.

00:07:51.720 --> 00:07:55.054
Life expectancy was
37 200 years ago.

00:07:55.054 --> 00:07:56.595
Schubert and Mozart
died in their 30s

00:07:56.595 --> 00:07:59.260
of bacterial infections,
and that was typical.

00:07:59.260 --> 00:08:01.450
That's all been from
linear progress.

00:08:01.450 --> 00:08:04.240
Exponential progress
is quite different.

00:08:04.240 --> 00:08:07.410
In a linear progression, that's
our intuition about the future

00:08:07.410 --> 00:08:09.100
goes one, two, three.

00:08:09.100 --> 00:08:10.600
And exponential
progression-- that's

00:08:10.600 --> 00:08:14.130
the reality, not of everything,
but of price performance

00:08:14.130 --> 00:08:16.170
capacity, of
information technology,

00:08:16.170 --> 00:08:18.075
goes one, two, four.

00:08:18.075 --> 00:08:20.200
It doesn't sound that
different, except by the time

00:08:20.200 --> 00:08:23.800
you get to 30, the linear
progression-- our intuition's

00:08:23.800 --> 00:08:24.810
at 30.

00:08:24.810 --> 00:08:29.110
The exponential progression
is at a billion.

00:08:29.110 --> 00:08:31.040
And that's not an
idle speculation.

00:08:31.040 --> 00:08:32.860
I mean, this is
several billion times

00:08:32.860 --> 00:08:35.700
more powerful per
dollar than the computer

00:08:35.700 --> 00:08:37.939
I used when I was an
undergraduate at MIT.

00:08:37.939 --> 00:08:40.630
It's several
trillion-fold since we

00:08:40.630 --> 00:08:43.580
started with the
1890 American census.

00:08:43.580 --> 00:08:45.820
But again, look at how
smooth a regression that is.

00:08:45.820 --> 00:08:48.430
Nothing has any impact on it.

00:08:48.430 --> 00:08:49.810
The third point
I want to make is

00:08:49.810 --> 00:08:51.880
that it's not just computation.

00:08:51.880 --> 00:08:54.370
It really affects every form
of information technology.

00:08:54.370 --> 00:08:56.050
And information
technology is ultimately

00:08:56.050 --> 00:08:59.440
going to transform
everything we care about,

00:08:59.440 --> 00:09:03.239
as a result of application
developers like yourself.

00:09:03.239 --> 00:09:04.780
I mean that's what
drives it forward,

00:09:04.780 --> 00:09:07.450
and I'll say more about that.

00:09:07.450 --> 00:09:10.317
We could buy one
transistor for $1 in 1968.

00:09:10.317 --> 00:09:12.025
I was pretty excited
about that because I

00:09:12.025 --> 00:09:14.159
was used to spending $50
for a telephone relay

00:09:14.159 --> 00:09:14.950
that could do that.

00:09:14.950 --> 00:09:17.200
We can now buy 10 billion.

00:09:17.200 --> 00:09:20.690
Cost of a transistor cycle has
come down by half every year.

00:09:20.690 --> 00:09:24.140
That's a 50%
deflation rate, so I

00:09:24.140 --> 00:09:27.180
can get the same computation,
or communication,

00:09:27.180 --> 00:09:29.030
or biological
technologies as I could

00:09:29.030 --> 00:09:31.190
a year ago for half the price.

00:09:31.190 --> 00:09:33.380
Economists actually worry
about that because we

00:09:33.380 --> 00:09:35.420
had massive deflation
during the Great

00:09:35.420 --> 00:09:37.400
Depression-- a different reason.

00:09:37.400 --> 00:09:39.540
There was a collapse
of consumer confidence.

00:09:39.540 --> 00:09:41.894
But the concern is, if I
can get the same stuff--

00:09:41.894 --> 00:09:43.810
and I'll talk about
three-dimensional printing

00:09:43.810 --> 00:09:48.340
in a moment to include physical
stuff-- for half the price,

00:09:48.340 --> 00:09:49.450
I'll buy more.

00:09:49.450 --> 00:09:51.850
I mean, that's economics
101, but I'm actually

00:09:51.850 --> 00:09:53.760
going to double my consumption.

00:09:53.760 --> 00:09:56.510
And if I don't, let's say
I increase my consumption

00:09:56.510 --> 00:09:58.790
in terms of bits
and bytes 50%, which

00:09:58.790 --> 00:10:02.750
is a lot-- the size of the
economy, not as measured

00:10:02.750 --> 00:10:05.510
in bits, bytes, and basepeds,
but as measured in currency--

00:10:05.510 --> 00:10:08.360
will shrink for a
variety of good reasons.

00:10:08.360 --> 00:10:10.220
That would be a bad thing.

00:10:10.220 --> 00:10:12.110
But that's not what happens.

00:10:12.110 --> 00:10:14.390
I've got 50 different
consumption charts like this.

00:10:14.390 --> 00:10:17.269
This is bits of memory consumed.

00:10:17.269 --> 00:10:19.060
We actually more than
double it every year.

00:10:19.060 --> 00:10:23.770
There's been 18% growth in
constant currency each year

00:10:23.770 --> 00:10:26.830
for the last 50 years in every
form of information technology,

00:10:26.830 --> 00:10:29.380
despite the fact that you
can get twice as much of it

00:10:29.380 --> 00:10:31.290
each year for the same price.

00:10:31.290 --> 00:10:35.700
And the reason for that
is application developers

00:10:35.700 --> 00:10:37.660
like all of you.

00:10:37.660 --> 00:10:40.500
I mean, why weren't there
social networks eight years ago?

00:10:40.500 --> 00:10:42.200
Was it because Mark
Zuckerberg was still

00:10:42.200 --> 00:10:43.600
in junior high school?

00:10:43.600 --> 00:10:45.726
No, there were
attempts to do it,

00:10:45.726 --> 00:10:47.350
and there are
arguments-- can we afford

00:10:47.350 --> 00:10:49.860
to allow users to
download a picture?

00:10:49.860 --> 00:10:51.860
The price performance
wasn't there.

00:10:51.860 --> 00:10:55.610
Why weren't there search
engines more than 15 years ago?

00:10:55.610 --> 00:10:58.040
I wrote in the early '80s,
when the ARPANET connected

00:10:58.040 --> 00:11:01.240
a few thousand scientists, that
this was growing exponentially,

00:11:01.240 --> 00:11:02.950
would necessarily do that.

00:11:02.950 --> 00:11:04.850
This would therefore
be a World Wide Web--

00:11:04.850 --> 00:11:07.730
I didn't use that term--
connecting hundreds of millions

00:11:07.730 --> 00:11:10.570
of people to each other, and
to vast knowledge resources

00:11:10.570 --> 00:11:11.795
by the late '90s.

00:11:11.795 --> 00:11:13.170
People thought
that was nuts when

00:11:13.170 --> 00:11:15.378
the entire American defense
budget could tie together

00:11:15.378 --> 00:11:18.310
a few thousand
scientists, but that's

00:11:18.310 --> 00:11:19.660
the power of exponential growth.

00:11:19.660 --> 00:11:21.110
That's what happened.

00:11:21.110 --> 00:11:23.547
And I wrote that there
would be so much information

00:11:23.547 --> 00:11:25.630
you couldn't find anything
without search engines,

00:11:25.630 --> 00:11:28.690
and the computational
communication resources

00:11:28.690 --> 00:11:31.710
needed for a search engine
would come into place.

00:11:31.710 --> 00:11:33.810
What you could not
predict is that it

00:11:33.810 --> 00:11:36.540
would be these couple of kids
in a Stanford dorm near here,

00:11:36.540 --> 00:11:39.550
who would take over the world
of search among the 50 projects

00:11:39.550 --> 00:11:40.820
that were seeking to do that.

00:11:40.820 --> 00:11:42.528
I'm not saying everything
is predictable,

00:11:42.528 --> 00:11:45.180
but you could predict
that search engines would

00:11:45.180 --> 00:11:47.590
be needed and feasible
in the late '90s.

00:11:47.590 --> 00:11:50.880
And Google was founded in '98.

00:11:50.880 --> 00:11:54.200
So "Time Magazine" wanted
a particular computer

00:11:54.200 --> 00:11:58.140
they'd covered as the last
point on this cover story

00:11:58.140 --> 00:11:59.730
about the law of
accelerating returns.

00:11:59.730 --> 00:12:00.438
It's right there.

00:12:00.438 --> 00:12:03.320
And this was a curve I'd
laid out 30 years earlier.

00:12:03.320 --> 00:12:06.530
So this is, in fact,
very predictable.

00:12:06.530 --> 00:12:08.280
And in terms of
all my predictions,

00:12:08.280 --> 00:12:12.010
when they're like this, in
terms of actual numbers-- price

00:12:12.010 --> 00:12:14.010
performance or capacity
of different information

00:12:14.010 --> 00:12:16.800
technologies-- they're
really right on the money.

00:12:16.800 --> 00:12:19.800
If you Google how my
predictions are faring,

00:12:19.800 --> 00:12:25.090
you'll get 150-page essay
looking at all the predictions

00:12:25.090 --> 00:12:27.730
that I've made as of that time,
which was a couple of years

00:12:27.730 --> 00:12:30.855
ago, including 147
that I made in the age

00:12:30.855 --> 00:12:33.410
of spiritual machines, which
I wrote in the late '90s,

00:12:33.410 --> 00:12:38.550
came out in '99, about
2009-- 78% were exactly

00:12:38.550 --> 00:12:40.560
correct to the year.

00:12:40.560 --> 00:12:45.170
And these were predictions
that were by decade.

00:12:45.170 --> 00:12:47.210
Another 8% were
off one year, so I

00:12:47.210 --> 00:12:48.980
called them essentially correct.

00:12:48.980 --> 00:12:52.090
So 86% were correct or
essentially correct.

00:12:52.090 --> 00:12:56.970
The ones that were
wrong, included things

00:12:56.970 --> 00:13:00.129
like regulatory and
cultural issues.

00:13:00.129 --> 00:13:01.670
Like, for example,
one that was wrong

00:13:01.670 --> 00:13:06.040
is that we'd have self-driving
cars, which, in fact, began

00:13:06.040 --> 00:13:08.760
a glimmer of working in 2009.

00:13:08.760 --> 00:13:12.090
If I had said 2014, it
would have been correct,

00:13:12.090 --> 00:13:16.420
because Google self-driving cars
have already gone 8,000 miles.

00:13:16.420 --> 00:13:20.240
And Google's going to launch a
fleet of experimental vehicles

00:13:20.240 --> 00:13:21.950
in Mountain View this year.

00:13:21.950 --> 00:13:26.417
But it wasn't correct for 2009.

00:13:26.417 --> 00:13:28.250
The number of bits we
move around wirelessly

00:13:28.250 --> 00:13:31.860
in the world over
the last century--

00:13:31.860 --> 00:13:35.350
there was Morse code, over
AM radio a century ago,

00:13:35.350 --> 00:13:38.800
today's 4G networks--
trillions-fold increase.

00:13:38.800 --> 00:13:42.010
But again, look at how
smooth of a trajectory is.

00:13:42.010 --> 00:13:44.510
Internet data traffic
doubling every year.

00:13:44.510 --> 00:13:48.880
Here's that graph I had of
the ARPANET in the early '80s,

00:13:48.880 --> 00:13:50.540
and projected that out.

00:13:50.540 --> 00:13:52.850
The graph on the right
is the same data,

00:13:52.850 --> 00:13:54.120
but on a linear scale.

00:13:54.120 --> 00:13:56.260
And that's how we experience it.

00:13:56.260 --> 00:13:58.560
We don't experience it in
the logarithmic domain.

00:13:58.560 --> 00:14:00.330
So to the casual
observer, it looked

00:14:00.330 --> 00:14:03.250
like, whoa, World Wide Web,
new thing, came out of nowhere.

00:14:03.250 --> 00:14:05.850
But you could see coming.

00:14:05.850 --> 00:14:10.300
And biology, this has been
a perfect exponential.

00:14:10.300 --> 00:14:12.820
The Genome Project
was announced in 1990,

00:14:12.820 --> 00:14:14.630
was not a mainstream project.

00:14:14.630 --> 00:14:16.650
Halfway through the
project, critics

00:14:16.650 --> 00:14:19.110
blasted it, saying,
look, here it's halfway

00:14:19.110 --> 00:14:20.320
through this 15-year project.

00:14:20.320 --> 00:14:22.980
You've only collected
1% of the genome.

00:14:22.980 --> 00:14:25.920
So seven years, 1%, it's
going to take 700 years,

00:14:25.920 --> 00:14:27.490
just like we said.

00:14:27.490 --> 00:14:29.840
My reaction is, we're at 1%.

00:14:29.840 --> 00:14:34.240
We're almost done, because it's
an exponential progression.

00:14:34.240 --> 00:14:35.120
[LAUGHTER]

00:14:35.120 --> 00:14:37.300
And indeed, it was
done seven years later,

00:14:37.300 --> 00:14:40.002
because 1% is seven
doublings from 100%.

00:14:40.002 --> 00:14:42.210
That's continued since the
end of the Genome Project.

00:14:42.210 --> 00:14:43.920
That first genome cost
a billion dollars.

00:14:43.920 --> 00:14:45.810
We're now down to a
few thousand dollars.

00:14:45.810 --> 00:14:47.070
But it's not just sequencing.

00:14:47.070 --> 00:14:50.800
We can now reprogram
this outdated data.

00:14:50.800 --> 00:14:53.240
And there are many
examples of that.

00:14:53.240 --> 00:14:54.740
The Johnson Diabetes
Center, they've

00:14:54.740 --> 00:14:56.630
turned off the fat
insulin receptor gene.

00:14:56.630 --> 00:14:58.530
We have technologies
like RNA interference

00:14:58.530 --> 00:14:59.960
that can turn genes off.

00:14:59.960 --> 00:15:02.320
These animals ate ravenously
and remained slim,

00:15:02.320 --> 00:15:04.660
and lived 20% longer.

00:15:04.660 --> 00:15:07.390
I've worked with a company
that adds a gene to patients

00:15:07.390 --> 00:15:10.170
with a terminal disease called
pulmonary hypertension, caused

00:15:10.170 --> 00:15:11.950
by one missing gene.

00:15:11.950 --> 00:15:14.660
They scrape out these cells
non-invasively from the throat,

00:15:14.660 --> 00:15:17.150
add the gene in vitro,
inspect it got done correctly,

00:15:17.150 --> 00:15:21.040
replicate it several million
fold-- another new technology.

00:15:21.040 --> 00:15:23.870
So they now have millions of
cells with that patient's DNA,

00:15:23.870 --> 00:15:25.830
but with the gene
they're missing.

00:15:25.830 --> 00:15:27.020
Inject it back in the body.

00:15:27.020 --> 00:15:29.270
The body recognizes
them as lung cells,

00:15:29.270 --> 00:15:34.630
and this is cured, this terminal
disease in human patients.

00:15:34.630 --> 00:15:36.540
There's an interesting
case combining

00:15:36.540 --> 00:15:38.650
a number of different
exponentially growing

00:15:38.650 --> 00:15:41.830
information technologies,
with this young girl who

00:15:41.830 --> 00:15:42.910
had a damaged windpipe.

00:15:42.910 --> 00:15:45.070
She was not going to be
able to survive with it.

00:15:45.070 --> 00:15:48.560
So they scanned her throat with
noninvasive imaging-- spatial

00:15:48.560 --> 00:15:50.230
resolution of
noninvasive imaging

00:15:50.230 --> 00:15:53.530
is doubling every year--
that's an important technology

00:15:53.530 --> 00:15:56.620
for reverse engineering
the brain-- designed

00:15:56.620 --> 00:16:00.690
her new windpipe in the
computer using computer design,

00:16:00.690 --> 00:16:04.630
printed it out with a 3D printer
using biodegradable materials,

00:16:04.630 --> 00:16:08.220
then populated this scaffolding
with her stem cells,

00:16:08.220 --> 00:16:12.714
using the same 3D printer,
grew out a new windpipe for her

00:16:12.714 --> 00:16:13.880
and installed it surgically.

00:16:13.880 --> 00:16:16.060
And it worked fine.

00:16:16.060 --> 00:16:20.390
You can now fix a broken
heart-- not yet from romance.

00:16:20.390 --> 00:16:22.160
That'll take us a
few more decades.

00:16:22.160 --> 00:16:23.190
[LAUGHTER]

00:16:23.190 --> 00:16:26.460
But half of all heart attack
survivors have a damaged heart.

00:16:26.460 --> 00:16:28.090
It's called low
ejection fraction.

00:16:28.090 --> 00:16:31.500
My father had that in the
'60s, and could hardly walk.

00:16:31.500 --> 00:16:34.290
Now you can fix that by
reprogramming adult stem cells,

00:16:34.290 --> 00:16:36.390
and rejuvenating the heart.

00:16:36.390 --> 00:16:38.910
You have to be a medical
tourist, because it's not

00:16:38.910 --> 00:16:41.100
yet approved here,
although it will be soon.

00:16:41.100 --> 00:16:42.380
There are many other examples.

00:16:42.380 --> 00:16:45.010
We could talk all
day about this,

00:16:45.010 --> 00:16:48.600
but our ability to reprogram
this outdated software

00:16:48.600 --> 00:16:50.070
is growing exponentially.

00:16:50.070 --> 00:16:52.840
These technologies are now
1,000 times more powerful

00:16:52.840 --> 00:16:54.600
than they were a
decade ago, when

00:16:54.600 --> 00:16:56.410
the Genome Project
was completed.

00:16:56.410 --> 00:16:58.840
They'll be another 1,000 times
more powerful in a decade,

00:16:58.840 --> 00:17:01.160
a million times more
powerful in 20 years.

00:17:01.160 --> 00:17:04.050
That's the implication of a
doubling in power every year.

00:17:04.050 --> 00:17:06.369
Somewhere between that
10 and 20 year mark,

00:17:06.369 --> 00:17:09.310
we'll see significant
differences in life

00:17:09.310 --> 00:17:11.869
expectancy-- not just
infant life expectancy,

00:17:11.869 --> 00:17:14.930
but your remaining
life expectancy.

00:17:14.930 --> 00:17:17.924
The models that are used
by life insurance companies

00:17:17.924 --> 00:17:19.930
sort of continue
the linear progress

00:17:19.930 --> 00:17:22.020
we've made before
health and medicine

00:17:22.020 --> 00:17:23.640
was in information
technology, which

00:17:23.640 --> 00:17:27.119
is based on, basically,
accidental findings.

00:17:27.119 --> 00:17:31.300
This is going to
go into high gear.

00:17:31.300 --> 00:17:34.700
Life expectancy is a
statistical phenomenon.

00:17:34.700 --> 00:17:37.960
You could still be hit by
the proverbial bus tomorrow.

00:17:37.960 --> 00:17:39.960
Of course, we're working
on that here at Google,

00:17:39.960 --> 00:17:41.425
also, with self-driving cars.

00:17:43.944 --> 00:17:45.360
And three-dimensional
printing-- I

00:17:45.360 --> 00:17:49.800
think we're in the
hype phase of this.

00:17:49.800 --> 00:17:52.350
I've written about the
life cycle of technologies.

00:17:52.350 --> 00:17:55.500
Usually early enthusiasts
who see the vision,

00:17:55.500 --> 00:17:59.490
but haven't really calculated
the timing correctly--

00:17:59.490 --> 00:18:01.000
and exponential
growth ultimately

00:18:01.000 --> 00:18:02.680
becomes transformative,
but it actually

00:18:02.680 --> 00:18:04.050
starts out very slowly.

00:18:04.050 --> 00:18:05.780
You're doubling
tiny, little numbers,

00:18:05.780 --> 00:18:11.170
like 1/10,000 of the genome
in 1990, 2/10,000 in 1991.

00:18:11.170 --> 00:18:13.850
So the progress doesn't
come on schedule.

00:18:13.850 --> 00:18:19.550
Disillusionment lets in, and
then you have basically a bust.

00:18:19.550 --> 00:18:21.110
And then it comes
back, as we really,

00:18:21.110 --> 00:18:23.490
truly understand
the true markers

00:18:23.490 --> 00:18:27.015
of what it will take
to be successful.

00:18:27.015 --> 00:18:28.390
This looks like
a young audience,

00:18:28.390 --> 00:18:32.600
but you may remember the
internet boom in the 1990s.

00:18:32.600 --> 00:18:36.796
If you had the URL dog.com,
you were a billionaire.

00:18:36.796 --> 00:18:38.170
Then around the
year 2000, people

00:18:38.170 --> 00:18:40.200
realized you can't
really make money

00:18:40.200 --> 00:18:41.450
with these internet companies.

00:18:41.450 --> 00:18:43.210
And there was the
internet bust, which

00:18:43.210 --> 00:18:45.010
almost took down the economy.

00:18:45.010 --> 00:18:46.890
But then it came
back, and today you

00:18:46.890 --> 00:18:51.110
have internet companies like
Google, and Apple, and Facebook

00:18:51.110 --> 00:18:55.930
that are worth hundreds
of billions of dollars.

00:18:55.930 --> 00:18:58.880
And we're kind of in
that hype phase now.

00:18:58.880 --> 00:19:03.390
I think we're still five or six
years away from the parameters

00:19:03.390 --> 00:19:04.790
we need to make this successful.

00:19:04.790 --> 00:19:07.420
We need sub-micron resolutions.

00:19:07.420 --> 00:19:09.980
The resolution
accuracy is improving

00:19:09.980 --> 00:19:12.510
at a rate of about 100
in 3D volume per decade,

00:19:12.510 --> 00:19:14.450
so it's exponential progress.

00:19:14.450 --> 00:19:16.970
But right now,
it's multi-micron.

00:19:16.970 --> 00:19:18.240
We can do interesting things.

00:19:18.240 --> 00:19:22.790
We had an opening a year ago
at Singularity University

00:19:22.790 --> 00:19:25.299
where the band-- all the
instruments that the band was

00:19:25.299 --> 00:19:27.590
playing were printed out on
three-dimensional printers.

00:19:27.590 --> 00:19:30.410
So there are interesting
niche applications.

00:19:30.410 --> 00:19:32.720
But by 2020, we'll be able
to print out clothing,

00:19:32.720 --> 00:19:33.930
for example.

00:19:33.930 --> 00:19:36.960
So people will go-- great,
there goes the fashion industry.

00:19:36.960 --> 00:19:38.190
But not so fast.

00:19:38.190 --> 00:19:42.050
I mean, look at other
industries that have already

00:19:42.050 --> 00:19:44.590
undergone the transformation
from physical products

00:19:44.590 --> 00:19:46.280
to digital products.

00:19:46.280 --> 00:19:48.790
A few years ago, if I wanted
to send you a book, or a movie,

00:19:48.790 --> 00:19:51.070
or a music album, I'd
send you a FedEx package.

00:19:51.070 --> 00:19:54.480
Today, I can send you
an email attachment.

00:19:54.480 --> 00:19:58.270
And there is indeed
an open source market

00:19:58.270 --> 00:20:02.620
with millions of free songs,
videos, movies, books,

00:20:02.620 --> 00:20:05.870
documents that you can
download legally for free.

00:20:05.870 --> 00:20:08.270
And you can have a very good
time with these free media

00:20:08.270 --> 00:20:09.350
products.

00:20:09.350 --> 00:20:11.720
But people still spend
money to read Harry Potter,

00:20:11.720 --> 00:20:14.134
or go to the latest
blockbuster, or get music

00:20:14.134 --> 00:20:15.300
from their favorite artists.

00:20:15.300 --> 00:20:18.580
And you have the coexistence of
the open source market, which

00:20:18.580 --> 00:20:21.330
is a great leveler, bringing
high-quality products

00:20:21.330 --> 00:20:24.280
to everyone at almost
no cost, or no cost,

00:20:24.280 --> 00:20:26.260
and a proprietary market.

00:20:26.260 --> 00:20:28.960
That'll be the nature of
the economy going forward.

00:20:28.960 --> 00:20:32.360
So in the 2020s, you'll be
able to download cool fashion

00:20:32.360 --> 00:20:34.740
designs, print them out
at pennies per pound,

00:20:34.740 --> 00:20:37.610
which is what it costs for
three-dimensional printing.

00:20:37.610 --> 00:20:41.050
But people will still
spend money for the latest

00:20:41.050 --> 00:20:42.840
hot designs from their
favorite designer.

00:20:45.980 --> 00:20:47.570
And I mentioned
I've been thinking

00:20:47.570 --> 00:20:48.930
about thinking for a long time.

00:20:48.930 --> 00:20:51.850
50 years ago, I
wrote a paper when

00:20:51.850 --> 00:20:54.570
I was 14, how I thought
the brain worked.

00:20:54.570 --> 00:20:56.590
There was actually
very little to go on.

00:20:56.590 --> 00:20:58.460
But I described it as
a series of modules,

00:20:58.460 --> 00:21:00.930
and each module could
recognize a pattern.

00:21:00.930 --> 00:21:03.550
And the essence of the human
brain was pattern recognition.

00:21:03.550 --> 00:21:06.400
We actually weren't very
good at logical thinking.

00:21:06.400 --> 00:21:09.120
Even then, I could see that
chess computers were based

00:21:09.120 --> 00:21:12.820
on logic, and being able to
look ahead, and calculate

00:21:12.820 --> 00:21:15.290
all the kind of move sequences.

00:21:15.290 --> 00:21:19.300
The human brain did it by deep
forms of pattern recognition.

00:21:19.300 --> 00:21:24.960
In '97, Kasparov was asked Deep
Blue analyzes 200,000 board

00:21:24.960 --> 00:21:25.920
positions a second.

00:21:25.920 --> 00:21:27.110
How many do you analyze?

00:21:27.110 --> 00:21:29.340
And he said, maybe
less than one.

00:21:29.340 --> 00:21:31.730
So how is it that he was
able to hold up at all?

00:21:31.730 --> 00:21:34.720
It's his deep powers
of pattern recognition.

00:21:34.720 --> 00:21:37.180
And I described it as a series
of modules, each of which

00:21:37.180 --> 00:21:40.790
could learn a pattern, remember
a pattern, implement a pattern,

00:21:40.790 --> 00:21:42.776
discover a pattern.

00:21:42.776 --> 00:21:44.650
And that these were
organized in hierarchies,

00:21:44.650 --> 00:21:47.500
and we created that hierarchy
with our own thinking.

00:21:47.500 --> 00:21:50.940
And that the whole neocortex
worked the same way.

00:21:50.940 --> 00:21:53.080
And that was actually
contrary to the common wisdom

00:21:53.080 --> 00:21:55.497
of the time, because
it was noticed

00:21:55.497 --> 00:21:57.080
that these different
regions, and they

00:21:57.080 --> 00:21:58.121
do very different things.

00:21:58.121 --> 00:22:01.420
So it was thought they must
be organized very differently.

00:22:01.420 --> 00:22:04.670
V1 in the back of the head
recognizes visual images,

00:22:04.670 --> 00:22:07.050
because that's where the
optic nerve spills into,

00:22:07.050 --> 00:22:09.540
and it can recognize the fact
that the edge of this table

00:22:09.540 --> 00:22:13.260
is flat, that there's
a horizontal crossbar

00:22:13.260 --> 00:22:16.730
in that capital A, and so on.

00:22:16.730 --> 00:22:19.440
The cruciform gyrus up
here recognizes faces.

00:22:19.440 --> 00:22:21.320
We know that because
if you conk somebody

00:22:21.320 --> 00:22:23.810
over the head in that
region, and knock it out,

00:22:23.810 --> 00:22:25.850
people can't recognize faces.

00:22:25.850 --> 00:22:29.100
The frontal cortex is famous for
language, and art, and science.

00:22:29.100 --> 00:22:31.430
They do very different things.

00:22:31.430 --> 00:22:34.050
They must be using
different algorithms.

00:22:34.050 --> 00:22:36.930
There was one neuroscientist
who actually did

00:22:36.930 --> 00:22:40.210
autopsies of the neocortex in
all of these different regions,

00:22:40.210 --> 00:22:42.325
and found they looked
exactly the same--

00:22:42.325 --> 00:22:44.790
the same neurons, the same
interconnection patterns.

00:22:44.790 --> 00:22:49.970
He said neocortex is
neocortex-- Vernon Mountcastle.

00:22:49.970 --> 00:22:51.070
And so I use that.

00:22:51.070 --> 00:22:55.050
And I also use observations
of human brains

00:22:55.050 --> 00:22:57.680
in action, which is still
our best laboratory,

00:22:57.680 --> 00:23:00.210
and described this basic method.

00:23:00.210 --> 00:23:05.090
This actually describes
the same algorithm,

00:23:05.090 --> 00:23:08.050
and actually describes
how each of these modules,

00:23:08.050 --> 00:23:10.760
which I count now
at 300 million,

00:23:10.760 --> 00:23:12.770
can recognize a pattern,
learn a pattern.

00:23:12.770 --> 00:23:16.380
It's basically
functionally similar

00:23:16.380 --> 00:23:19.250
to a hierarchical hidden
Markov model, which

00:23:19.250 --> 00:23:23.670
is a technology I worked on the
1990s, and speech recognition,

00:23:23.670 --> 00:23:26.989
and early forms of natural
language understanding.

00:23:26.989 --> 00:23:29.030
And it's a little bit
different than neural nets,

00:23:29.030 --> 00:23:32.950
because neural nets-- the basic
element is a neuron, which

00:23:32.950 --> 00:23:37.400
can kind of learn one state,
not really a whole pattern.

00:23:37.400 --> 00:23:42.200
And in my view, the basic module
is a pattern-recognition module

00:23:42.200 --> 00:23:44.430
that can learn a
more complex pattern.

00:23:44.430 --> 00:23:46.580
And a hierarchical
hidden Markov model

00:23:46.580 --> 00:23:49.700
is a hierarchy of Markov
models, each of which

00:23:49.700 --> 00:23:52.570
can learn a fairly
complicated pattern.

00:23:52.570 --> 00:23:55.210
I believe that is how
the human brain works.

00:23:55.210 --> 00:23:58.800
And that's what I'm
doing here at Google.

00:23:58.800 --> 00:24:00.530
I've given an early
version of this book

00:24:00.530 --> 00:24:03.060
to Larry Page a
couple years ago.

00:24:03.060 --> 00:24:04.160
He liked it.

00:24:04.160 --> 00:24:07.390
I met with him to ask him for an
investment in the company I was

00:24:07.390 --> 00:24:10.080
going to start to
develop these ideas.

00:24:10.080 --> 00:24:14.360
And he said, have you thought
of doing this here at Google?

00:24:14.360 --> 00:24:18.300
We have these terrific
resources in terms

00:24:18.300 --> 00:24:20.590
of data, and
computation, and talent.

00:24:20.590 --> 00:24:22.340
He actually said it
in a very low key way,

00:24:22.340 --> 00:24:25.410
but that was his message.

00:24:25.410 --> 00:24:30.040
So I met with
him-- Alan Eustace,

00:24:30.040 --> 00:24:32.630
who's here in the audience.

00:24:32.630 --> 00:24:34.710
They said I'd have the
kind of independence

00:24:34.710 --> 00:24:37.880
I'd have with my own company,
but these Google resources--

00:24:37.880 --> 00:24:42.450
and so I've been doing that
now for a year and a half.

00:24:42.450 --> 00:24:43.490
And it's been terrific.

00:24:43.490 --> 00:24:46.440
It's really the only place I
could do this project, which

00:24:46.440 --> 00:24:49.430
has been a 50-year endeavor.

00:24:49.430 --> 00:24:52.042
And the spatial resolution of
brain scanning, as I mentioned,

00:24:52.042 --> 00:24:53.000
is doubling every year.

00:24:53.000 --> 00:24:54.944
We can now see inside
a living brain,

00:24:54.944 --> 00:24:56.860
see it create your
thoughts, see your thoughts

00:24:56.860 --> 00:24:58.420
create your brain.

00:24:58.420 --> 00:25:01.540
And there's a few
significant milestones

00:25:01.540 --> 00:25:04.570
in the history of the biological
version of this thinking.

00:25:04.570 --> 00:25:06.950
You see it up there--
the neocortex.

00:25:06.950 --> 00:25:10.650
The neocortex emerged 200
million years ago with mammals.

00:25:10.650 --> 00:25:13.370
It was capable of a
different type of thinking,

00:25:13.370 --> 00:25:15.720
basically hierarchical thinking.

00:25:15.720 --> 00:25:17.230
Other animals
could learn things,

00:25:17.230 --> 00:25:19.759
but not with
elaborate hierarchies.

00:25:19.759 --> 00:25:21.300
They could have a
behavior that might

00:25:21.300 --> 00:25:24.100
have a hierarchical aspect
to it, but it was fixed.

00:25:24.100 --> 00:25:26.020
They couldn't learn
a new hierarchy--

00:25:26.020 --> 00:25:27.560
at least not in one lifetime.

00:25:27.560 --> 00:25:29.160
Maybe over 10,000
lifetimes, they

00:25:29.160 --> 00:25:32.220
could evolve a new behavior.

00:25:32.220 --> 00:25:34.530
Next significant thing that
happened-- 65 million years

00:25:34.530 --> 00:25:37.310
ago, there was a violent
change in the environment.

00:25:37.310 --> 00:25:39.166
We call it the Cretaceous
Extinction Event.

00:25:39.166 --> 00:25:40.790
That's when the
dinosaurs went extinct.

00:25:40.790 --> 00:25:44.620
That's when 75% of all the
animal and plant species

00:25:44.620 --> 00:25:46.280
went extinct.

00:25:46.280 --> 00:25:49.160
And that's when mammals
overtook their ecological niche,

00:25:49.160 --> 00:25:51.540
because they could adapt
their behavior quickly enough

00:25:51.540 --> 00:25:53.020
to cope with it.

00:25:53.020 --> 00:25:56.460
Next significant milestone
was 2 million years ago.

00:25:56.460 --> 00:25:58.070
We developed these
large foreheads,

00:25:58.070 --> 00:26:00.270
so we now had more neocortex.

00:26:00.270 --> 00:26:03.060
That neocortex has all
these folds and convolutions

00:26:03.060 --> 00:26:05.310
basically to increase
its surface area.

00:26:05.310 --> 00:26:06.260
It's a flat structure.

00:26:06.260 --> 00:26:09.360
It's about the size of a
table napkin and just as thin.

00:26:09.360 --> 00:26:12.300
It's one module thick, but
it has so many convolutions

00:26:12.300 --> 00:26:15.850
and ridges, it's
80% of your brain.

00:26:15.850 --> 00:26:18.260
And the frontal cortex--
it's often been thought

00:26:18.260 --> 00:26:20.230
it must be
qualitatively different,

00:26:20.230 --> 00:26:25.260
because it does these amazing
things, like art and poetry.

00:26:25.260 --> 00:26:31.210
But recent research projects
discovered, or examined,

00:26:31.210 --> 00:26:34.180
the issue, what happens
to V1, which I mentioned

00:26:34.180 --> 00:26:37.540
is in the back of the head, and
does these very simple things,

00:26:37.540 --> 00:26:40.300
like the fact that that's
a straight line-- what

00:26:40.300 --> 00:26:43.190
happens to it in a
congenitally blind person?

00:26:43.190 --> 00:26:46.060
They're not getting
any visual images.

00:26:46.060 --> 00:26:49.350
Well, the frontal cortex notices
hey, V1 isn't doing anything,

00:26:49.350 --> 00:26:51.630
and it actually harnesses
it to help it with language,

00:26:51.630 --> 00:26:55.470
and art, and science-- at
the opposite extreme end

00:26:55.470 --> 00:26:58.870
of the continuum of
complexity of features,

00:26:58.870 --> 00:27:00.960
showing that they're
both basically using

00:27:00.960 --> 00:27:03.340
the same algorithm.

00:27:03.340 --> 00:27:07.110
And so we are already doing
a very good job as primates

00:27:07.110 --> 00:27:08.990
without the frontal cortex.

00:27:08.990 --> 00:27:11.100
Now we had this
additional quantity,

00:27:11.100 --> 00:27:13.130
and so we could think
higher thoughts.

00:27:13.130 --> 00:27:16.440
Because the neocortex is
built on conceptual levels.

00:27:16.440 --> 00:27:19.170
Each level is more abstract
than the one below it.

00:27:19.170 --> 00:27:21.170
So the first thing
we invented was

00:27:21.170 --> 00:27:24.670
language, and that was about a
couple hundred thousand years

00:27:24.670 --> 00:27:25.450
ago.

00:27:25.450 --> 00:27:27.550
So I have an idea in my head.

00:27:27.550 --> 00:27:31.920
It's a hierarchy of
other ideas, and symbols,

00:27:31.920 --> 00:27:34.020
and other structures.

00:27:34.020 --> 00:27:37.300
And I want to actually
communicate that and basically

00:27:37.300 --> 00:27:41.190
transmit that hierarchical
structure to your neocortex.

00:27:41.190 --> 00:27:44.650
So we invented language
in order to do that.

00:27:44.650 --> 00:27:48.120
And it's a communication
medium that is hierarchical,

00:27:48.120 --> 00:27:50.450
so it could reflect the
hierarchical structures

00:27:50.450 --> 00:27:51.440
in the neocortex.

00:27:51.440 --> 00:27:54.050
The neocortex was
successful because the world

00:27:54.050 --> 00:27:55.159
is hierarchical.

00:27:55.159 --> 00:27:56.700
That's the best way
to understand it.

00:27:56.700 --> 00:27:58.050
Trees have limbs.

00:27:58.050 --> 00:28:00.270
Limbs have branches,
branches have other branches.

00:28:00.270 --> 00:28:01.810
Some branches have leaves.

00:28:01.810 --> 00:28:04.220
The world is organized in
a hierarchical fashion.

00:28:04.220 --> 00:28:07.400
And we could now represent
this in language.

00:28:07.400 --> 00:28:11.770
And if you ever want to see
some entertaining examples

00:28:11.770 --> 00:28:19.510
of the hierarchy in language,
read a Gabriel Garcia Marquez

00:28:19.510 --> 00:28:20.510
novel.

00:28:20.510 --> 00:28:22.900
He has one sentence
that's six pages long,

00:28:22.900 --> 00:28:24.665
and it's grammatically correct.

00:28:24.665 --> 00:28:28.860
And it has a fantastic array of
hierarchical structures showing

00:28:28.860 --> 00:28:31.890
the indefinite hierarchy we
can create with language,

00:28:31.890 --> 00:28:33.720
reflecting the
indefinite hierarchy we

00:28:33.720 --> 00:28:36.210
can have in our ideas.

00:28:36.210 --> 00:28:40.110
And there's then been a
continual acceleration

00:28:40.110 --> 00:28:40.900
of technology.

00:28:40.900 --> 00:28:42.900
Written language only
took a few thousand years.

00:28:42.900 --> 00:28:45.850
The first examples were
thousands of years ago.

00:28:45.850 --> 00:28:48.850
The printing press took 400
years to reach a mass audience.

00:28:48.850 --> 00:28:51.470
The telephone reached a quarter
of the American and European

00:28:51.470 --> 00:28:52.769
population in 50 years.

00:28:52.769 --> 00:28:54.060
The cellphone took seven years.

00:28:54.060 --> 00:28:57.250
Social networks, wikis,
and blogs took three years.

00:28:57.250 --> 00:29:00.770
We continually are
accelerating, basically,

00:29:00.770 --> 00:29:02.740
these information
technologies because

00:29:02.740 --> 00:29:05.130
of the law of
accelerating returns.

00:29:05.130 --> 00:29:09.010
And we can now simulate
aspects of the neocortex.

00:29:09.010 --> 00:29:12.780
And fundamentally what
my team-- and we're not

00:29:12.780 --> 00:29:14.960
the only team doing
this-- is trying

00:29:14.960 --> 00:29:17.340
to create a functional
simulation of the neocortex.

00:29:17.340 --> 00:29:19.560
And I'll tell you
the key problem

00:29:19.560 --> 00:29:21.700
is we can create hierarchies.

00:29:21.700 --> 00:29:25.900
So even in the 1990s, we had a
hierarchy of acoustic states,

00:29:25.900 --> 00:29:28.360
and then phonemes,
and then word models

00:29:28.360 --> 00:29:31.800
for speech recognition, and
then simple grammatical models,

00:29:31.800 --> 00:29:35.060
so that we could have a sentence
like, Move this paragraph to

00:29:35.060 --> 00:29:36.890
after the third paragraph
in the next page.

00:29:36.890 --> 00:29:40.590
And it would carry out
that simple command.

00:29:40.590 --> 00:29:43.690
But we couldn't actually
add a new layer ourselves.

00:29:43.690 --> 00:29:46.920
That's actually, if you
want to speak technically,

00:29:46.920 --> 00:29:50.960
the key technical challenge in
trying to create more and more

00:29:50.960 --> 00:29:55.070
flexible AI, is how do we
create the next conceptual level

00:29:55.070 --> 00:29:58.180
that's more abstract than the
ones we have, automatically

00:29:58.180 --> 00:30:02.030
from the data, rather than
reprogramming it ourselves

00:30:02.030 --> 00:30:03.605
using our human intelligence?

00:30:11.230 --> 00:30:16.700
I'll skip some of this to--
at the very high level,

00:30:16.700 --> 00:30:20.020
you have very abstract
ideas, like that's funny,

00:30:20.020 --> 00:30:22.900
that's ironic, she's pretty.

00:30:22.900 --> 00:30:25.170
This 16-year-old girl
is having brain surgery.

00:30:25.170 --> 00:30:27.750
And whenever they stimulated
these points showed in red,

00:30:27.750 --> 00:30:29.100
she would laugh.

00:30:29.100 --> 00:30:30.870
They wanted to be
able to talk to her.

00:30:30.870 --> 00:30:32.494
There's no pain
receptors in the brain,

00:30:32.494 --> 00:30:34.515
so you can do that
during brain surgery.

00:30:34.515 --> 00:30:36.890
And they thought they were
stimulating some kind of laugh

00:30:36.890 --> 00:30:38.850
reflex, but they quickly
realized that no, they

00:30:38.850 --> 00:30:42.010
were triggering the genuine
perception of humor.

00:30:42.010 --> 00:30:44.150
She just found
everything hilarious

00:30:44.150 --> 00:30:45.990
whenever they
simulated these points.

00:30:45.990 --> 00:30:47.910
You guys are so funny
just standing around,

00:30:47.910 --> 00:30:50.400
was a typical comment.

00:30:50.400 --> 00:30:52.065
And they weren't funny, so--

00:30:52.065 --> 00:30:54.390
[LAUGHTER]

00:30:54.390 --> 00:30:56.830
So an example from
another company

00:30:56.830 --> 00:31:00.240
that shows our beginning
ability to actually understand

00:31:00.240 --> 00:31:02.562
human language is WATSON.

00:31:02.562 --> 00:31:04.270
As you can see, WATSON
got a higher score

00:31:04.270 --> 00:31:07.360
than the best two human
players in Jeopardy combined.

00:31:07.360 --> 00:31:09.890
It got this query correct
in the rhyme category,

00:31:09.890 --> 00:31:12.300
"A long, tiresome
speech delivered

00:31:12.300 --> 00:31:14.960
by a frothy pie topping,"
and WATSON quickly

00:31:14.960 --> 00:31:17.060
said, "What is a
meringue harangue?"

00:31:17.060 --> 00:31:20.810
And WATSON got its knowledge by
reading 200 million documents

00:31:20.810 --> 00:31:23.030
of natural language,
including all

00:31:23.030 --> 00:31:25.010
of Wikipedia and
other encyclopedias.

00:31:25.010 --> 00:31:27.310
It doesn't understand each
page as well as you or I,

00:31:27.310 --> 00:31:31.780
but it makes up for that
by reading a lot of pages.

00:31:31.780 --> 00:31:34.590
And that's the kind of thing
we're trying to do here.

00:31:34.590 --> 00:31:37.210
We have a model that
I believe actually

00:31:37.210 --> 00:31:41.140
will solve this key
problem of being

00:31:41.140 --> 00:31:44.190
able to add to the
hierarchy automatically,

00:31:44.190 --> 00:31:47.380
so that we can handle,
ultimately, complex documents.

00:31:47.380 --> 00:31:49.010
So one application,
for example, would

00:31:49.010 --> 00:31:50.250
be in language translation.

00:31:50.250 --> 00:31:52.950
Right now, it does a very good
job through the power of data,

00:31:52.950 --> 00:31:56.680
and these Rosetta Stone
databases of translated text.

00:31:56.680 --> 00:31:58.110
By matching word
sequences, we're

00:31:58.110 --> 00:32:00.220
improving the way
that we match them.

00:32:00.220 --> 00:32:01.595
But we really
would like to do it

00:32:01.595 --> 00:32:04.027
the way a human does it,
which is to understand it.

00:32:04.027 --> 00:32:05.360
What does it mean to understand?

00:32:05.360 --> 00:32:06.940
It means to take the
language, and actually

00:32:06.940 --> 00:32:08.450
create this
hierarchical structure

00:32:08.450 --> 00:32:11.160
of the ideas in my head,
and then resynthesize it,

00:32:11.160 --> 00:32:13.226
re-articulate it in
the new language.

00:32:13.226 --> 00:32:14.850
That's the kind of
thing we hope to do.

00:32:14.850 --> 00:32:18.180
We'd like the search
engine to read for meeting.

00:32:18.180 --> 00:32:21.660
So if you put out a tweet,
"Everything Ray Kurzweil

00:32:21.660 --> 00:32:25.040
is saying at I/O is
nonsense," there's

00:32:25.040 --> 00:32:28.139
actually, in that simple
text, a hierarchy,

00:32:28.139 --> 00:32:30.180
which you need to understand
to really understand

00:32:30.180 --> 00:32:32.040
what that's trying to say.

00:32:32.040 --> 00:32:34.780
If you write a blog post,
you have something to say,

00:32:34.780 --> 00:32:36.770
and the search already
goes substantially

00:32:36.770 --> 00:32:39.320
beyond the base forms.

00:32:39.320 --> 00:32:42.100
It will understand the
syntactic structure.

00:32:42.100 --> 00:32:43.640
If you see the
word "he," it'll do

00:32:43.640 --> 00:32:45.550
that co-reference resolution.

00:32:45.550 --> 00:32:47.140
It'll understand synonyms.

00:32:47.140 --> 00:32:49.900
But it's not fully
modeling the ideas

00:32:49.900 --> 00:32:53.570
that you have to say when you
write an article or a blog

00:32:53.570 --> 00:32:54.320
post.

00:32:54.320 --> 00:32:57.980
And that's what we would
like to actually understand.

00:32:57.980 --> 00:32:59.630
And then you would
be able to dialogue

00:32:59.630 --> 00:33:02.750
with your search engine
to give it complex tasks,

00:33:02.750 --> 00:33:06.250
and interact with it the way you
would with a human assistant.

00:33:06.250 --> 00:33:07.780
And it would then go out.

00:33:07.780 --> 00:33:10.410
And maybe it wouldn't even
find the information that day,

00:33:10.410 --> 00:33:12.370
but a week later,
would pop up and say,

00:33:12.370 --> 00:33:14.440
you asked me this
question a week ago,

00:33:14.440 --> 00:33:18.810
and new research just came out
that answers that question,

00:33:18.810 --> 00:33:19.690
and so on.

00:33:24.160 --> 00:33:29.569
So let's stop here, and I notice
that time clock isn't working,

00:33:29.569 --> 00:33:30.860
so I have no idea where we are.

00:33:30.860 --> 00:33:36.580
But RJ, maybe you've
got some questions.

00:33:36.580 --> 00:33:38.790
RJ MICHAEL: Yes, indeed.

00:33:38.790 --> 00:33:40.720
So I've gathered
up some questions

00:33:40.720 --> 00:33:44.930
from people, fellow Googlers,
from some of our friends

00:33:44.930 --> 00:33:46.790
throughout the community.

00:33:46.790 --> 00:33:49.390
And we're going to
take this opportunity

00:33:49.390 --> 00:33:52.270
to ask some of these
questions, starting

00:33:52.270 --> 00:33:55.320
with a quote from
William Gibson,

00:33:55.320 --> 00:33:58.900
the famed author who said
that, the future is here.

00:33:58.900 --> 00:34:01.979
It's just unevenly distributed.

00:34:01.979 --> 00:34:03.770
Where do you think
things are running fast,

00:34:03.770 --> 00:34:05.970
and where they
lagging further behind

00:34:05.970 --> 00:34:08.721
than what you would
have expected?

00:34:08.721 --> 00:34:10.429
RAY KURZWEIL: Well,
I think it's actually

00:34:10.429 --> 00:34:12.110
very widely distributed.

00:34:12.110 --> 00:34:13.610
Companies like
Google-- and not just

00:34:13.610 --> 00:34:16.770
Google-- Apple,
Microsoft, Facebook--

00:34:16.770 --> 00:34:19.150
are not just using
these technologies

00:34:19.150 --> 00:34:21.929
with a few corporations
and government agencies.

00:34:21.929 --> 00:34:24.159
It's in billions of hands.

00:34:24.159 --> 00:34:28.440
Google search is used by
between 1 and 2 billion people,

00:34:28.440 --> 00:34:32.020
and we hope to expand that to
the next couple billion users

00:34:32.020 --> 00:34:34.150
and a couple billion after that.

00:34:34.150 --> 00:34:38.080
That's the business
model, and I believe

00:34:38.080 --> 00:34:40.370
that is actually how we
will use these technologies.

00:34:40.370 --> 00:34:42.650
They'll be very
widely distributed.

00:34:42.650 --> 00:34:45.409
And they're very democratizing.

00:34:45.409 --> 00:34:47.440
The technologies that
move very smoothly

00:34:47.440 --> 00:34:49.950
are the sort of pure application
of the law of accelerating

00:34:49.950 --> 00:34:50.449
returns.

00:34:50.449 --> 00:34:52.989
When you get into
regulatory issues,

00:34:52.989 --> 00:34:56.457
like we have with the
self-driving cars, maybe

00:34:56.457 --> 00:34:57.540
a little less predictable.

00:34:57.540 --> 00:35:00.270
There's a lot of
regulation in medicine.

00:35:00.270 --> 00:35:03.900
But I believe these
technologies ultimately

00:35:03.900 --> 00:35:07.480
will be so profoundly superior,
that they will actually

00:35:07.480 --> 00:35:10.000
accelerate these regulatory
processes as well.

00:35:12.660 --> 00:35:15.440
RJ MICHAEL: So you
outlined in your book, "How

00:35:15.440 --> 00:35:19.090
to Create a Mind,"
the idea of what it's

00:35:19.090 --> 00:35:20.940
going to take to
actually create a mind.

00:35:20.940 --> 00:35:25.100
And you've chosen to pursue
these ideas here at Google.

00:35:25.100 --> 00:35:28.470
Why Google?

00:35:28.470 --> 00:35:29.920
RAY KURZWEIL:
Well, it's actually

00:35:29.920 --> 00:35:32.240
the first time I've done that.

00:35:32.240 --> 00:35:37.380
But I've realized that you need
unique resources to do this.

00:35:37.380 --> 00:35:39.540
It's a very difficult problem.

00:35:39.540 --> 00:35:42.770
So for one thing you need a
tremendous amount of talent.

00:35:42.770 --> 00:35:48.120
That's, I think, the primary
resource that's unique--

00:35:48.120 --> 00:35:50.480
maybe not completely
unique, but it's certainly

00:35:50.480 --> 00:35:52.860
evident at Google.

00:35:52.860 --> 00:35:54.380
And then you want
to run something

00:35:54.380 --> 00:35:57.450
on a million computers, and
you want tremendous data

00:35:57.450 --> 00:35:59.110
that reflects language.

00:35:59.110 --> 00:36:01.980
And we have tens of
billions of pages--

00:36:01.980 --> 00:36:05.180
virtually all books
and web pages.

00:36:05.180 --> 00:36:08.470
And so this is not a project I
could do with my own company,

00:36:08.470 --> 00:36:13.790
even if I raised all the
money that I could hope for.

00:36:13.790 --> 00:36:18.310
And it's a bold company that
takes on major challenges,

00:36:18.310 --> 00:36:21.690
and tries to improve the
world with these applications,

00:36:21.690 --> 00:36:24.590
and make them widely available.

00:36:24.590 --> 00:36:28.270
So I like the philosophy
of the leadership.

00:36:28.270 --> 00:36:30.730
RJ MICHAEL: Me too.

00:36:30.730 --> 00:36:33.730
It's true.

00:36:33.730 --> 00:36:36.350
But I'm an engineer at
heart, and the engineer in me

00:36:36.350 --> 00:36:39.520
wants to know how you
intend to build this thing.

00:36:39.520 --> 00:36:44.240
Could you describe to us what
the engineered mind is like?

00:36:44.240 --> 00:36:46.760
What tech must we
implement versus

00:36:46.760 --> 00:36:49.690
what behavior do we
expect to emerge?

00:36:49.690 --> 00:36:52.110
What must be done in software?

00:36:52.110 --> 00:36:54.129
What must be done in hardware?

00:36:54.129 --> 00:36:56.170
RAY KURZWEIL: Well, on
the hardware requirements,

00:36:56.170 --> 00:36:58.470
I mean, to functionally
emulate the human brain--

00:36:58.470 --> 00:37:00.030
I've analyzed that.

00:37:00.030 --> 00:37:03.070
I've estimated it about 10
to the 14th calculations

00:37:03.070 --> 00:37:06.490
per second and
singularity is near.

00:37:06.490 --> 00:37:08.720
So I hedged that a bit and
said 10 to the 14th to 10

00:37:08.720 --> 00:37:10.260
to the 16th.

00:37:10.260 --> 00:37:12.590
I've reanalyzed it using
different methods in "How

00:37:12.590 --> 00:37:15.710
to Create a mind" that come
up again with 10 to the 14th.

00:37:15.710 --> 00:37:18.120
There've been a number of
independent analyses of that.

00:37:18.120 --> 00:37:20.170
They come with the same figure.

00:37:20.170 --> 00:37:24.850
We've already surpassed that
by three orders of magnitude

00:37:24.850 --> 00:37:27.630
in supercomputers.

00:37:27.630 --> 00:37:30.540
It'd be hard to provide 10
to the 14th calculations

00:37:30.540 --> 00:37:33.410
per second to all
of a billion users

00:37:33.410 --> 00:37:36.580
kind of using it more or
less at the same time.

00:37:36.580 --> 00:37:38.080
I've discussed this
with Larry Page,

00:37:38.080 --> 00:37:40.260
and he thinks no, actually
that could be possible.

00:37:40.260 --> 00:37:43.490
But the law of
accelerating returns

00:37:43.490 --> 00:37:46.180
will make that easy
by the early 2020.

00:37:46.180 --> 00:37:49.350
So it really comes down
to a software problem.

00:37:49.350 --> 00:37:52.660
I described, I think,
the key software problem.

00:37:52.660 --> 00:37:54.960
We can already create
these hierarchies.

00:37:54.960 --> 00:37:56.530
In Google, and in
other companies,

00:37:56.530 --> 00:37:59.550
there's debate between several
different learning methods,

00:37:59.550 --> 00:38:02.060
and they have pros and cons.

00:38:02.060 --> 00:38:05.680
We need one that can actually
represent hierarchies

00:38:05.680 --> 00:38:08.790
of complicated patterns,
where each pattern has

00:38:08.790 --> 00:38:12.800
its position in a complicated
hierarchy of patterns.

00:38:12.800 --> 00:38:15.540
And the key unsolved
problem is, how do we then

00:38:15.540 --> 00:38:19.800
add a conceptually more
abstract level to that?

00:38:19.800 --> 00:38:22.080
And I think we can
use machine learning

00:38:22.080 --> 00:38:25.290
to find the patterns
at that high level,

00:38:25.290 --> 00:38:27.910
but you need to be able
to model them correctly.

00:38:27.910 --> 00:38:29.840
And so that's what
we're exploring.

00:38:29.840 --> 00:38:31.790
And then applying
it to language.

00:38:31.790 --> 00:38:33.345
And ultimately,
Google will apply it

00:38:33.345 --> 00:38:35.670
to other types of input,
like videos and pictures.

00:38:35.670 --> 00:38:38.170
And we're already making
a lot of progress.

00:38:38.170 --> 00:38:41.090
Machine learning at Google
is already very powerful.

00:38:41.090 --> 00:38:43.460
Once we have a system
that's working,

00:38:43.460 --> 00:38:46.170
there will be little loops
that are very tight that

00:38:46.170 --> 00:38:48.040
are taking up the bulk
of the computation

00:38:48.040 --> 00:38:51.960
that we could put in
an ASIC, in a dedicated

00:38:51.960 --> 00:38:55.110
circuit, because you can get
1,000 fold increase in price

00:38:55.110 --> 00:38:59.530
performance by hard-wiring
repetitive algorithms

00:38:59.530 --> 00:39:00.990
in hardware.

00:39:00.990 --> 00:39:03.392
But and there are attempts,
of course, to do that.

00:39:03.392 --> 00:39:05.600
I think it's premature now,
because we haven't really

00:39:05.600 --> 00:39:08.240
settled on the right
type of machine learning.

00:39:08.240 --> 00:39:10.650
So we don't really know
what algorithm to speed up.

00:39:10.650 --> 00:39:13.820
But that'll be a straightforward
engineering trade-off,

00:39:13.820 --> 00:39:16.010
once we can actually
mess with the software.

00:39:16.010 --> 00:39:19.590
So it's a fundamentally
a software problem.

00:39:19.590 --> 00:39:21.590
RJ MICHAEL: So most of
the people in this room

00:39:21.590 --> 00:39:24.940
are engineers, or are heavily
involved in app development

00:39:24.940 --> 00:39:25.440
as well.

00:39:25.440 --> 00:39:28.200
And everything that you're
talking about here, these

00:39:28.200 --> 00:39:30.830
are exciting visions to
a lot of people here,

00:39:30.830 --> 00:39:32.700
and myself as well.

00:39:32.700 --> 00:39:35.280
But what can the
developers in this room

00:39:35.280 --> 00:39:38.280
do to turn these ideas of
yours into actual working

00:39:38.280 --> 00:39:40.590
products and systems?

00:39:40.590 --> 00:39:43.700
What role do these developers
play in pushing all of this

00:39:43.700 --> 00:39:45.004
forward for us?

00:39:45.004 --> 00:39:46.920
RAY KURZWEIL: Well it's
application developers

00:39:46.920 --> 00:39:48.450
that drive it forward.

00:39:48.450 --> 00:39:51.490
And I mean, there's
a debate in the AI

00:39:51.490 --> 00:39:54.170
field between traditional
artificial intelligence,

00:39:54.170 --> 00:39:56.390
and something called
AGI, Artificial General

00:39:56.390 --> 00:39:59.780
Intelligence, which is
implicitly a criticism that AI

00:39:59.780 --> 00:40:02.610
has not pursued
general intelligence,

00:40:02.610 --> 00:40:04.240
and it's gone often
to narrow things

00:40:04.240 --> 00:40:08.760
like OCRs, speech
recognition, or robotics.

00:40:08.760 --> 00:40:11.677
But I actually think we get
from here to there-- there

00:40:11.677 --> 00:40:16.360
being future AI, strong
AI-- one step at a time.

00:40:16.360 --> 00:40:18.230
And the steps are
applications, and we

00:40:18.230 --> 00:40:20.710
need to actually
optimize the technology

00:40:20.710 --> 00:40:21.747
for the applications.

00:40:21.747 --> 00:40:23.330
It's very hard to
develop a technology

00:40:23.330 --> 00:40:26.880
if you don't have something
to optimize it for.

00:40:26.880 --> 00:40:29.340
So I like the idea
of crossing the river

00:40:29.340 --> 00:40:31.746
kind of from one
stone to the next.

00:40:31.746 --> 00:40:33.620
People say, what about
that part of the river

00:40:33.620 --> 00:40:35.790
where it's too deep,
and there are no stones?

00:40:35.790 --> 00:40:38.860
So I'm not sure
the answer to that.

00:40:38.860 --> 00:40:42.770
But we do get from here to there
through one step at a time.

00:40:42.770 --> 00:40:44.350
And each step is sort of benign.

00:40:44.350 --> 00:40:47.380
It's exciting in the
application world,

00:40:47.380 --> 00:40:53.000
but it's not the
grand step to AI.

00:40:53.000 --> 00:40:55.420
But that's how we're
going to get there,

00:40:55.420 --> 00:40:58.020
and the application
developers push it forward,

00:40:58.020 --> 00:41:01.430
and make it practical, and
provide an economic business

00:41:01.430 --> 00:41:04.480
model for it.

00:41:04.480 --> 00:41:06.900
RJ MICHAEL: So this law
of accelerating return

00:41:06.900 --> 00:41:09.690
that you talk
about-- you've made

00:41:09.690 --> 00:41:14.140
it clear in natural space,
that it exists like this.

00:41:14.140 --> 00:41:17.360
And in a technology space,
it's definitely true.

00:41:17.360 --> 00:41:20.280
It all starts to feel
just like natural law,

00:41:20.280 --> 00:41:21.870
like natural progression.

00:41:21.870 --> 00:41:23.620
Why do we have to
work toward it?

00:41:23.620 --> 00:41:26.320
Why don't we just sit back
and let it happen for us?

00:41:26.320 --> 00:41:28.720
RAY KURZWEIL: Yeah, that
question comes up a lot--

00:41:28.720 --> 00:41:31.310
why don't we just sit
back and let it happen?

00:41:31.310 --> 00:41:33.020
Why are we working so hard?

00:41:33.020 --> 00:41:35.710
And if we did that,
it wouldn't happen.

00:41:35.710 --> 00:41:39.280
So what is actually predictable
is the human passion

00:41:39.280 --> 00:41:42.120
to make improvement, to
use the computers of 2014

00:41:42.120 --> 00:41:44.100
to create the computers of 2015.

00:41:44.100 --> 00:41:47.410
We couldn't do
that a decade ago.

00:41:47.410 --> 00:41:52.720
And we're able to improve
things in an exponential manner.

00:41:52.720 --> 00:41:53.740
Things are 1x.

00:41:53.740 --> 00:41:54.780
We try to make it 2x.

00:41:54.780 --> 00:41:58.140
If they are 1,000x, we don't
seek to make it 1,001x.

00:41:58.140 --> 00:42:00.900
We try to make it 2,000.

00:42:00.900 --> 00:42:03.090
And we have the
tools to do that.

00:42:03.090 --> 00:42:05.970
I have a mathematical treatment
in "Singularity is Near."

00:42:05.970 --> 00:42:09.032
The empirical data is
the strongest evidence

00:42:09.032 --> 00:42:10.490
for the law o
accelerating returns.

00:42:10.490 --> 00:42:15.610
But it is driven by application
developers and technology

00:42:15.610 --> 00:42:18.690
developers taking each step with
the current state of the art.

00:42:23.110 --> 00:42:25.310
RJ MICHAEL: And speaking
about the curves,

00:42:25.310 --> 00:42:34.210
and the rising curves-- sorry,
I lost my place on my page here.

00:42:34.210 --> 00:42:39.250
Well, so let's talk about
specialized smarts, then.

00:42:39.250 --> 00:42:41.410
There's a role for
specialized smarts,

00:42:41.410 --> 00:42:45.150
and the neocortex seems to have
a very general architecture--

00:42:45.150 --> 00:42:47.560
repeated architecture,
as you were mentioning--

00:42:47.560 --> 00:42:50.625
but there also seems to
be specific modules that

00:42:50.625 --> 00:42:51.884
have evolved.

00:42:51.884 --> 00:42:53.800
Do you think that there
are specific functions

00:42:53.800 --> 00:42:55.883
that we're going to need
to build for our learning

00:42:55.883 --> 00:42:56.610
machines?

00:42:56.610 --> 00:43:01.030
And how do you know when
to go really specific,

00:43:01.030 --> 00:43:04.720
and focus on one particular
thing, versus just allowing

00:43:04.720 --> 00:43:07.830
it to be handled by the
general architecture?

00:43:07.830 --> 00:43:09.830
RAY KURZWEIL: Well, we
still have the old brain,

00:43:09.830 --> 00:43:13.510
like the amygdala puts out an
ancient cascade of hormones,

00:43:13.510 --> 00:43:15.630
to prepare us for
a fight or flight.

00:43:15.630 --> 00:43:17.200
It's no longer
able to decide what

00:43:17.200 --> 00:43:19.910
to be afraid of, so your
boss walks in the room,

00:43:19.910 --> 00:43:22.210
and whether that causes
laughter or fear is really

00:43:22.210 --> 00:43:23.840
up to the neocortex.

00:43:23.840 --> 00:43:26.940
And the neocortex is a
general architecture.

00:43:26.940 --> 00:43:28.510
There are no
specialized regions.

00:43:28.510 --> 00:43:31.380
There's no music region.

00:43:31.380 --> 00:43:33.800
But the patterns
in music, or even

00:43:33.800 --> 00:43:36.370
particular types of
music-- whether it's

00:43:36.370 --> 00:43:41.920
Chopin waltzes, or hip-hop-- are
specialized types of knowledge.

00:43:41.920 --> 00:43:45.440
And we have a limited
capacity in neocortex,

00:43:45.440 --> 00:43:48.840
so you can really be a
world-class master of one

00:43:48.840 --> 00:43:49.820
thing.

00:43:49.820 --> 00:43:53.990
Einstein played the violin,
but he was no Jascha Heifetz.

00:43:53.990 --> 00:43:55.420
Heifetz was
interested in physics,

00:43:55.420 --> 00:43:58.040
but he was no Einstein.

00:43:58.040 --> 00:44:01.620
We really need to devote
the bulk of our neocortex

00:44:01.620 --> 00:44:04.530
that's not devoted
to every-day concerns

00:44:04.530 --> 00:44:09.100
to one type of knowledge, which
has its own type of patterns,

00:44:09.100 --> 00:44:11.100
and learn the patterns
that others have created,

00:44:11.100 --> 00:44:13.550
and then push it forward.

00:44:13.550 --> 00:44:19.141
But really, the architecture
is pretty much the same.

00:44:19.141 --> 00:44:20.640
RJ MICHAEL: I find
that fascinating.

00:44:20.640 --> 00:44:23.320
It's the same
algorithm repeated.

00:44:23.320 --> 00:44:26.620
So while it's
great for all of us

00:44:26.620 --> 00:44:28.100
that personal
technology has been

00:44:28.100 --> 00:44:31.310
freeing us up, and
empowering us all,

00:44:31.310 --> 00:44:33.210
giving us a lot more
free time, and giving us

00:44:33.210 --> 00:44:35.580
more capabilities
under our fingertips,

00:44:35.580 --> 00:44:37.560
do you think that
we are actually

00:44:37.560 --> 00:44:39.679
going to make good use of it?

00:44:39.679 --> 00:44:41.220
The thing that I
keep wondering about

00:44:41.220 --> 00:44:43.680
is are we essentially going
to use this free time, and all

00:44:43.680 --> 00:44:48.290
these extra capabilities that
computers give to us, to allow

00:44:48.290 --> 00:44:52.920
us to watch more television,
and consume more sugary snacks?

00:44:52.920 --> 00:44:54.500
Which is what I'm afraid of.

00:44:54.500 --> 00:44:57.630
So how are we going to use
this technology that you're

00:44:57.630 --> 00:45:01.419
developing, to ensure that
we actually will live better?

00:45:01.419 --> 00:45:03.460
RAY KURZWEIL: Well, there's
always pros and cons.

00:45:03.460 --> 00:45:07.220
We just happen to be on
the right slide here.

00:45:07.220 --> 00:45:09.470
And this is just
one perspective.

00:45:09.470 --> 00:45:11.310
People quickly lose perspective.

00:45:11.310 --> 00:45:14.230
We forget what things
were like eight years

00:45:14.230 --> 00:45:16.450
ago, before there were
social networks, 15 years ago

00:45:16.450 --> 00:45:18.380
before there were
search engines.

00:45:18.380 --> 00:45:20.140
Once these things
happen, we assume

00:45:20.140 --> 00:45:22.210
they have always been around.

00:45:22.210 --> 00:45:24.750
People certainly forget what
things were like 200 years ago,

00:45:24.750 --> 00:45:26.300
when Thomas Hobbes
described life

00:45:26.300 --> 00:45:29.300
as short, brutish,
disaster-prone, poverty-filled,

00:45:29.300 --> 00:45:30.890
disease-filled.

00:45:30.890 --> 00:45:32.719
Let's take a quick,
one-minute trip

00:45:32.719 --> 00:45:34.010
through the last two centuries.

00:45:34.010 --> 00:45:34.920
These are countries.

00:45:34.920 --> 00:45:36.230
The big, red circle is China.

00:45:36.230 --> 00:45:37.563
It does some interesting things.

00:45:37.563 --> 00:45:38.790
Keep an eye on that.

00:45:38.790 --> 00:45:42.010
The x-axis is income per
person in today's dollars,

00:45:42.010 --> 00:45:43.255
so you can understand it.

00:45:43.255 --> 00:45:45.380
So there were wealthy
countries and poor countries,

00:45:45.380 --> 00:45:46.700
but nobody was very wealthy.

00:45:46.700 --> 00:45:51.050
Income per person was hundreds
of dollars in today's dollars.

00:45:51.050 --> 00:45:53.490
On the y-axis is
life expectancy.

00:45:53.490 --> 00:45:57.860
In the '20s and '30s--
worldwide average, 37.

00:45:57.860 --> 00:46:00.960
So this was the beginning of the
Industrial Revolution, started

00:46:00.960 --> 00:46:03.550
in the textile industry
in England in 1800.

00:46:03.550 --> 00:46:07.160
A few countries are
making progress.

00:46:07.160 --> 00:46:10.030
But as you get to the
20th century, the 1900s,

00:46:10.030 --> 00:46:12.010
you'll see a wind
that carries all

00:46:12.010 --> 00:46:14.260
of these countries towards
the upper right-hand corner

00:46:14.260 --> 00:46:15.420
of the graph.

00:46:15.420 --> 00:46:19.650
The have, have-not
divide does not go away.

00:46:19.650 --> 00:46:21.650
There's still rich countries
and poor countries.

00:46:21.650 --> 00:46:24.233
But the countries that are worst
off at the end of the process

00:46:24.233 --> 00:46:26.300
are far better off
than the countries that

00:46:26.300 --> 00:46:28.640
were best off at the
beginning of the process.

00:46:28.640 --> 00:46:30.380
And I shouldn't say
end of the process,

00:46:30.380 --> 00:46:32.299
because the process
isn't ending.

00:46:32.299 --> 00:46:33.840
It's going to go
into high gear as we

00:46:33.840 --> 00:46:36.370
get to the more mature
phases of the biotechnology

00:46:36.370 --> 00:46:41.990
and three-dimensional printing
revolutions, AI, and so on.

00:46:41.990 --> 00:46:43.690
RJ MICHAEL: That's just awesome.

00:46:45.750 --> 00:46:47.125
RAY KURZWEIL: So
to be continued.

00:46:52.220 --> 00:46:56.950
RJ MICHAEL: So, and I think we
might have a few minutes left

00:46:56.950 --> 00:47:00.110
to take some questions from
the audience at this point.

00:47:00.110 --> 00:47:01.387
If--

00:47:01.387 --> 00:47:02.220
RAY KURZWEIL: Well--

00:47:02.220 --> 00:47:03.740
RJ MICHAEL: There's
two more left to ask,

00:47:03.740 --> 00:47:05.530
but I'm saving those
babies for the end.

00:47:05.530 --> 00:47:08.149
Unless you have something
you wanted to address.

00:47:08.149 --> 00:47:10.190
RAY KURZWEIL: I don't know
how much time we have,

00:47:10.190 --> 00:47:11.840
because the countdown
clock isn't working.

00:47:11.840 --> 00:47:13.080
RJ MICHAEL: Yeah, the
countdown clock stopped.

00:47:13.080 --> 00:47:14.720
We have two minutes left?

00:47:14.720 --> 00:47:15.450
Oh, two minutes.

00:47:15.450 --> 00:47:15.840
RAY KURZWEIL: OK.

00:47:15.840 --> 00:47:16.840
RJ MICHAEL: All
right, well, then, I'm

00:47:16.840 --> 00:47:18.590
going to stick to
my questions, then.

00:47:18.590 --> 00:47:20.875
Sorry, you guys.

00:47:20.875 --> 00:47:22.250
Because I've got
one that is just

00:47:22.250 --> 00:47:23.860
want of my favorite
interview questions,

00:47:23.860 --> 00:47:25.276
that have been
dying my whole life

00:47:25.276 --> 00:47:26.660
to ask Ray these questions.

00:47:26.660 --> 00:47:29.840
So if you would
tell us please, what

00:47:29.840 --> 00:47:33.130
are some of the more
humbling experiences

00:47:33.130 --> 00:47:35.300
you've had researching
and developing

00:47:35.300 --> 00:47:39.090
your concepts over the years?

00:47:39.090 --> 00:47:42.850
RAY KURZWEIL: Well, I think
it's this one unsolved research

00:47:42.850 --> 00:47:46.060
question, that the human
brain is able to do,

00:47:46.060 --> 00:47:48.600
and I think it's the key
to making further advances

00:47:48.600 --> 00:47:51.682
in artificial intelligence.

00:47:51.682 --> 00:47:54.850
The neocortex is
organized in these layers,

00:47:54.850 --> 00:47:58.060
and each layer is more
abstract than the one below it.

00:47:58.060 --> 00:48:00.710
And we're able to actually--
if we understand something,

00:48:00.710 --> 00:48:03.110
like we understood
speech recognition,

00:48:03.110 --> 00:48:05.740
we can actually identify
phonemes should be here,

00:48:05.740 --> 00:48:08.430
words should be here, and we
can create that hierarchy,

00:48:08.430 --> 00:48:12.100
and then use machine
learning to learn each level.

00:48:12.100 --> 00:48:16.910
But how do we then create a
more abstract level on its own?

00:48:16.910 --> 00:48:19.130
Because the neocortex does that.

00:48:19.130 --> 00:48:22.390
I've been watching my grandson
go through level after level.

00:48:22.390 --> 00:48:25.250
Now he's almost three, and
he's got quite a few levels

00:48:25.250 --> 00:48:27.410
under his belt.

00:48:27.410 --> 00:48:31.170
And that's done
with the neocortex,

00:48:31.170 --> 00:48:37.830
without really any external
input, other than his parents

00:48:37.830 --> 00:48:41.430
saying, that was good, Leo.

00:48:41.430 --> 00:48:44.180
So how do we do that?

00:48:44.180 --> 00:48:46.680
That's what we hope to solve.

00:48:46.680 --> 00:48:52.620
I think we can have a
stable set of hierarchies,

00:48:52.620 --> 00:48:55.870
and then find patterns at that
level using machine learning,

00:48:55.870 --> 00:48:58.610
and then automatically
add a new level.

00:48:58.610 --> 00:49:00.400
But that has never
been demonstrated,

00:49:00.400 --> 00:49:03.430
and if we could do
that, I think we'll

00:49:03.430 --> 00:49:05.860
make great strides in
artificial intelligence.

00:49:05.860 --> 00:49:09.904
But so far, that has
eluded the AI field.

00:49:09.904 --> 00:49:11.570
RJ MICHAEL: So then
I'll end with this--

00:49:11.570 --> 00:49:13.800
you mentioned your
grandchild, three-year-old.

00:49:13.800 --> 00:49:17.720
There's certain definitions
of the word consciousness

00:49:17.720 --> 00:49:20.100
that would suggest that a
three-year-old has not yet

00:49:20.100 --> 00:49:22.750
achieved consciousness--
awareness of self,

00:49:22.750 --> 00:49:25.080
awareness of what's in a
mirror, and things like that.

00:49:25.080 --> 00:49:26.020
RAY KURZWEIL: Well, he
would disagree with that.

00:49:26.020 --> 00:49:26.519
But--

00:49:28.942 --> 00:49:30.650
RJ MICHAEL: So I would
like to end, then,

00:49:30.650 --> 00:49:34.580
with three simple
questions, that I would

00:49:34.580 --> 00:49:36.710
ask you to take all
together for us.

00:49:36.710 --> 00:49:38.940
What is consciousness?

00:49:38.940 --> 00:49:41.390
What is free will?

00:49:41.390 --> 00:49:42.330
And what is soul?

00:49:46.170 --> 00:49:47.670
RAY KURZWEIL: Well,
I always thought

00:49:47.670 --> 00:49:52.720
you had a good sense
of humor, so one minute

00:49:52.720 --> 00:49:54.576
should be plenty for that.

00:49:54.576 --> 00:49:56.360
[LAUGHTER]

00:49:56.360 --> 00:49:58.450
We've debated that for
thousands of years,

00:49:58.450 --> 00:50:00.890
going back to the
Platonic dialogues.

00:50:00.890 --> 00:50:03.820
But to summarize,
consciousness--

00:50:03.820 --> 00:50:06.346
whether or not an entity--

00:50:06.346 --> 00:50:12.510
[LAUGHTER]

00:50:12.510 --> 00:50:14.270
Whether or not an
entity has consciousness

00:50:14.270 --> 00:50:16.430
is not a scientific
question, because there's

00:50:16.430 --> 00:50:23.320
no experiment you could run
that would really definitively--

00:50:23.320 --> 00:50:25.270
falsifiable experiment
that you could run,

00:50:25.270 --> 00:50:27.850
whether or not an entity
is conscious or not.

00:50:27.850 --> 00:50:30.060
We assume that each
other is conscious,

00:50:30.060 --> 00:50:32.635
that human agreement
falls apart when

00:50:32.635 --> 00:50:34.920
you go outside of
human experience.

00:50:34.920 --> 00:50:36.540
People disagree about animals.

00:50:36.540 --> 00:50:40.280
They will disagree
about future AIs.

00:50:40.280 --> 00:50:42.340
An AI could claim
it's conscious.

00:50:42.340 --> 00:50:45.730
Eugene Goostman claimed
that he was conscious,

00:50:45.730 --> 00:50:46.980
but it wasn't very convincing.

00:50:49.700 --> 00:50:52.989
And so you actually, but,
so some scientists say,

00:50:52.989 --> 00:50:54.530
well, it's not a
scientific question.

00:50:54.530 --> 00:50:55.321
It's not important.

00:50:55.321 --> 00:50:56.330
We should dismiss it.

00:50:56.330 --> 00:50:57.647
It's just an illusion.

00:50:57.647 --> 00:50:59.980
I think that's a mistake,
because our whole moral system

00:50:59.980 --> 00:51:02.930
is based on consciousness.

00:51:02.930 --> 00:51:04.190
So you need a leap of faith.

00:51:04.190 --> 00:51:07.750
My leap of faith is that if
an entity seems conscious,

00:51:07.750 --> 00:51:11.240
and seems to be having
the subjective experiences

00:51:11.240 --> 00:51:14.144
it claims to be having,
I'll believe it's conscious.

00:51:14.144 --> 00:51:15.810
I will also make an
objective prediction

00:51:15.810 --> 00:51:18.820
that most people will
accept the consciousness

00:51:18.820 --> 00:51:22.130
of these entities.

00:51:22.130 --> 00:51:27.450
And so a valid Turing Test--
I mean, I have a long, now,

00:51:27.450 --> 00:51:32.320
Turing Test bet with Mitch
Kapor that by 2029, a computer

00:51:32.320 --> 00:51:33.810
will pass the Turing Test.

00:51:33.810 --> 00:51:36.810
And we actually set a very
difficult set of rules.

00:51:36.810 --> 00:51:40.880
I think if an AI passes that,
people will really be convinced

00:51:40.880 --> 00:51:44.140
that it's really conscious,
and we will accept it

00:51:44.140 --> 00:51:46.820
as having those
subjective experiences.

00:51:46.820 --> 00:51:49.135
Identity is really a
continuation of a pattern.

00:51:49.135 --> 00:51:51.170
People say, what are
you talking about?

00:51:51.170 --> 00:51:53.630
Your identity is this,
you're physical stuff.

00:51:53.630 --> 00:51:56.690
It's this flesh and blood, but
actually this is very different

00:51:56.690 --> 00:51:58.410
than it was six months ago.

00:51:58.410 --> 00:52:01.060
All of our cells turn
over, some in hours,

00:52:01.060 --> 00:52:02.620
some in days, some in weeks.

00:52:02.620 --> 00:52:04.890
The different components
of the neurons,

00:52:04.890 --> 00:52:07.510
the tubules, the ion
channels, the filaments,

00:52:07.510 --> 00:52:10.120
change over in either
hours, or days, or weeks.

00:52:10.120 --> 00:52:13.010
I'm completely different stuff
than I was six months ago.

00:52:13.010 --> 00:52:17.640
So I make a comparison
to water in the stream.

00:52:17.640 --> 00:52:22.360
It may make that certain pattern
as it goes around a rock.

00:52:22.360 --> 00:52:27.050
That pattern can stay the
same for days, weeks, years.

00:52:27.050 --> 00:52:29.756
But the water actually
changes in milliseconds.

00:52:29.756 --> 00:52:32.162
So is that the same river?

00:52:32.162 --> 00:52:34.620
There's a Chinese proverb, you
can't walk in the same river

00:52:34.620 --> 00:52:35.830
twice.

00:52:35.830 --> 00:52:38.080
But it's actually a
continuation of a pattern.

00:52:38.080 --> 00:52:39.420
And that's what we are.

00:52:39.420 --> 00:52:44.290
The pattern changes slowly,
but continuation of pattern,

00:52:44.290 --> 00:52:46.640
even if we introduce
non-biological elements to it,

00:52:46.640 --> 00:52:50.070
we would be continuing
that pattern.

00:52:50.070 --> 00:52:55.050
And free will, that's
impossible to define.

00:52:55.050 --> 00:52:56.850
I'm not convinced
I have free will.

00:52:56.850 --> 00:52:59.730
Major decisions like
starting a project,

00:52:59.730 --> 00:53:03.340
coming to work at
Google, speaking at IO--

00:53:03.340 --> 00:53:05.247
did I really make that decision?

00:53:05.247 --> 00:53:06.080
What was I thinking?

00:53:08.680 --> 00:53:11.820
They just seem to
happen on their own.

00:53:11.820 --> 00:53:13.900
I do actually think
a lot, and I think

00:53:13.900 --> 00:53:16.100
I do have free will deciding
what to eat for lunch,

00:53:16.100 --> 00:53:18.890
so I think making
eating choices is maybe

00:53:18.890 --> 00:53:21.790
the heart of free will.

00:53:21.790 --> 00:53:23.200
So I'll leave it at that.

00:53:23.200 --> 00:53:24.610
RJ MICHAEL: OK,
that's very good.

00:53:24.610 --> 00:53:26.358
Thank you so much, Ray.

00:53:26.358 --> 00:53:28.302
You did just awesome.

00:53:28.302 --> 00:53:30.250
Thank you so much.

00:53:30.250 --> 00:53:31.934
Excellent.

