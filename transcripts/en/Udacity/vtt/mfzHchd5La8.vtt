WEBVTT
Kind: captions
Language: en

00:00:00.100 --> 00:00:04.100
I've mentioned overfitting before,
but I haven't yet defined it.

00:00:04.100 --> 00:00:05.580
Before we could define it, and

00:00:05.580 --> 00:00:09.800
I could give you an example,
we needed to have a definition of error.

00:00:09.800 --> 00:00:12.050
Let me now show you what I mean.

00:00:12.050 --> 00:00:17.480
Let's consider parameterized polynomial
models where we can, one at a time, add

00:00:17.480 --> 00:00:22.320
additional factors, like x, x squared,
x cubed, x to the fourth, and so on.

00:00:22.320 --> 00:00:26.120
Let's create a graph where we
have along the horizontal access

00:00:26.120 --> 00:00:31.470
degrees of freedom, or d,
the degree of our polynomial.

00:00:31.470 --> 00:00:35.110
And vertically here,
we'll have the error of our model.

00:00:35.110 --> 00:00:41.050
So let's measure error as we
increase d on our training set.

00:00:41.050 --> 00:00:46.510
So when d is smallest,
our error is greatest.

00:00:46.510 --> 00:00:51.890
And as we increase d,
our error drops and drops and drops.

00:00:51.890 --> 00:00:56.040
In other words, we're fitting
the data in sample better and better.

00:00:56.040 --> 00:01:01.680
When finally we get to N, where we
have as many parameters in our model

00:01:01.680 --> 00:01:06.210
as we do have items in our data set,
our error gets all the way down to zero.

00:01:06.210 --> 00:01:09.060
This is in sample error.

00:01:09.060 --> 00:01:12.460
Now, let's add a similar line for
out of sample error.

00:01:12.460 --> 00:01:15.410
Remember that we expect our
out of sample error to always

00:01:15.410 --> 00:01:18.460
be greater than or
equal to in sample error.

00:01:18.460 --> 00:01:20.740
The curve will look something like this.

00:01:20.740 --> 00:01:26.080
It'll start out at maximum error, about
the same as our in sample line, and

00:01:26.080 --> 00:01:30.190
as we go down,
we begin to diverge like this.

00:01:30.190 --> 00:01:33.710
Now in this region
both our in sample and

00:01:33.710 --> 00:01:37.390
out of sample errors
are still decreasing, but

00:01:37.390 --> 00:01:42.430
eventually we'll reach a point where
our out of sample begins to increase.

00:01:42.430 --> 00:01:45.770
In fact it may increase strongly.

00:01:45.770 --> 00:01:52.020
In this area, as we increase degrees
of freedom, our in sample error is

00:01:52.020 --> 00:01:57.330
decreasing, but
our out of sample error is increasing.

00:01:57.330 --> 00:01:59.700
And that's how we define overfitting.

00:01:59.700 --> 00:02:02.530
This is the region where
overfitting is occurring.

00:02:02.530 --> 00:02:04.430
So, let me state that again.

00:02:04.430 --> 00:02:09.949
In sample error is decreasing,
out of sample error is increasing.

00:02:09.949 --> 00:02:13.170
And we have those two together,
it's over fitting.

