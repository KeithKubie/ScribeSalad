WEBVTT
Kind: captions
Language: en

00:00:01.550 --> 00:00:06.430
&gt;&gt;Female presenter: So I wanted to introduce
all of you to Rebecca MacKinnon who is here

00:00:06.430 --> 00:00:11.670
to talk about her new book, Consent of the
Networked.

00:00:11.670 --> 00:00:17.109
Rebecca is currently a senior fellow at the
New America Foundation and also a co-founder

00:00:17.109 --> 00:00:23.849
of Global Voices which is a citizen journalism
network and she also had a stint at the Berkman

00:00:23.849 --> 00:00:25.179
Center at Harvard.

00:00:25.179 --> 00:00:33.840
But before that Rebecca was a foreign correspondent
for CNN in China and Japan and has an incredibly

00:00:33.840 --> 00:00:38.110
impressive career; she also speaks fluent
Mandarin.

00:00:38.110 --> 00:00:42.579
And Rebecca's also on the board of the Committee
to Protect Journalists and the Global Network

00:00:42.579 --> 00:00:47.200
Initiative of which Google is also a founding
member.

00:00:47.200 --> 00:00:51.350
So she's here to talk about her book which
I have to urge all of you to read because

00:00:51.350 --> 00:00:54.859
I think it's the, it's the book I wish I had
written.

00:00:54.859 --> 00:00:58.780
It's an incredibly thoughtful take on the
overall state of Internet freedom around the

00:00:58.780 --> 00:01:07.500
world and what roles that governments and
companies and citizens play in shaping that

00:01:07.500 --> 00:01:08.500
future.

00:01:08.500 --> 00:01:10.440
So please give a warm welcome to Rebecca.

00:01:10.440 --> 00:01:11.440
[applause]

00:01:11.440 --> 00:01:14.260
&gt;&gt;Rebecca MacKinnon: Thank you.

00:01:14.260 --> 00:01:15.260
[applause]

00:01:15.260 --> 00:01:20.260
Thank you very much for having me here today.

00:01:20.260 --> 00:01:24.400
It's funny when I started planning to write
the book and then started writing the book

00:01:24.400 --> 00:01:29.920
it was before the Arab Spring happened so
and then as the Arab Spring was happening

00:01:29.920 --> 00:01:35.400
in January and February, I was kind of realizing
I had to kind of reframe the book slightly

00:01:35.400 --> 00:01:36.940
based on what had happened.

00:01:36.940 --> 00:01:44.320
But it was also an incredible opportunity
to really show in very current impressing

00:01:44.320 --> 00:01:47.090
way why these issues matter.

00:01:47.090 --> 00:01:53.600
And I guess Wael Ghonim came here recently
or he's coming, he came two days ago.

00:01:53.600 --> 00:02:00.240
And in the beginning of the book I talk about
the Internet and social media obviously played

00:02:00.240 --> 00:02:05.880
some role in the Arab Spring, there's still
a lot of debate about how big a role, etcetera,

00:02:05.880 --> 00:02:09.069
etcetera and people are going to be arguing
about that for a long time.

00:02:09.069 --> 00:02:15.580
And Wael Ghonim famously said to CNN right
after Mubarak stepped down, "If you wanna

00:02:15.580 --> 00:02:19.000
liberate a society just give them the Internet."

00:02:19.000 --> 00:02:24.140
And it's turning out to be a little more complicated
than that especially in Egypt as they're really

00:02:24.140 --> 00:02:31.390
struggling to actually institute a democracy
despite having used the Internet to say no

00:02:31.390 --> 00:02:34.260
to a nasty regime.

00:02:34.260 --> 00:02:39.850
How do you then use the Internet to build
a robust and functional democracy is a much

00:02:39.850 --> 00:02:48.290
bigger question not only in Egypt but also
in Tunisia where now that they've got a democratically

00:02:48.290 --> 00:02:53.070
elected constituent assembly that's writing
a new constitution they're having arguments

00:02:53.070 --> 00:02:57.930
about what should the role of censorship and
surveillance be in a democratic society.

00:02:57.930 --> 00:03:05.940
And actually some censorship in Tunisia has
been reinstated for kind of moral, ethical

00:03:05.940 --> 00:03:11.430
reasons; people in a conservative Islamic
society don't want their children looking

00:03:11.430 --> 00:03:16.720
at porn and so how do you decide what the
mechanism should be and how do you prevent

00:03:16.720 --> 00:03:23.830
them from getting abused in a way that kind
of takes you at least some steps back to where

00:03:23.830 --> 00:03:31.270
you came from and away from where you're trying
to go as a democracy?

00:03:31.270 --> 00:03:34.850
And unlike a general audience you guys have
already, of course, think a lot about these

00:03:34.850 --> 00:03:41.830
issues but increasingly the citizens no matter
whether we're Tunisians or Americans or French

00:03:41.830 --> 00:03:47.849
people or whatever our relationship with government,
our political live are sort of remediated

00:03:47.849 --> 00:03:54.540
through the collection of technologies and
services and platforms that make up the Internet.

00:03:54.540 --> 00:03:59.360
And so what my book is really about is not
the question of does the Internet help the

00:03:59.360 --> 00:04:05.959
good guys more than the bad guys or is it
making us stupid or is it used more for good

00:04:05.959 --> 00:04:13.530
or more for evil, but how do we insure that
the Internet evolves in way that best serves

00:04:13.530 --> 00:04:16.310
the rights and interests of citizens?

00:04:16.310 --> 00:04:22.080
And how do we make sure that the Internet
moves in a manner, evolves in a manner that

00:04:22.080 --> 00:04:28.999
is compatible with democracy here in the United
States and compatible with the democratic

00:04:28.999 --> 00:04:35.770
aspirations of people around the world which
many people continue to risk their lives for

00:04:35.770 --> 00:04:37.099
everyday?

00:04:37.099 --> 00:04:43.560
And of course as company how does one support
that while still being a profitable business

00:04:43.560 --> 00:04:47.150
is of course a challenging question.

00:04:47.150 --> 00:04:53.139
And I deal in the book quite a bit about kind
of change in the nature of sovereignty and

00:04:53.139 --> 00:04:54.539
power.

00:04:54.539 --> 00:04:59.999
And so in the physical world of course we
have sovereign governments running countries

00:04:59.999 --> 00:05:04.660
and if you're lucky you're in a democracy
where you actually vote for your leaders and

00:05:04.660 --> 00:05:12.199
you can vote them out of office if they're
really awful or at least in theory, and you

00:05:12.199 --> 00:05:17.860
can, there might be some way to hold the government
accountable.

00:05:17.860 --> 00:05:23.800
But that also that the laws being passed by
the government are affecting the citizens

00:05:23.800 --> 00:05:25.449
in that country right?

00:05:25.449 --> 00:05:30.620
But now you have a situation, and we saw this
with SOPA, the Stop Online Piracy Act, and

00:05:30.620 --> 00:05:37.060
PIPA, the Senate version, where you have a
law passed by Congress which is elected by

00:05:37.060 --> 00:05:42.520
Americans which potentially, it fortunately
wasn't passed, but could potentially have

00:05:42.520 --> 00:05:47.330
affected the rights and freedoms of Internet
users all over the world who didn't vote for

00:05:47.330 --> 00:05:52.150
these people and have no way of holding them
accountable and who don't really necessarily

00:05:52.150 --> 00:05:57.150
care whether Indonesians are hurt by the law
they passed 'cause they really only care,

00:05:57.150 --> 00:05:59.369
frankly, about their own constituents.

00:05:59.369 --> 00:06:07.580
I mean there may be some who care, but ultimately
their jobs do not depend on whether or not

00:06:07.580 --> 00:06:12.680
they're affecting the rights of people in
Indonesia or Brazil or elsewhere.

00:06:12.680 --> 00:06:20.520
So that's kind of a sovereignty mismatch that's
going on that has been exacerbated by the

00:06:20.520 --> 00:06:27.240
Internet in terms of being able to hold state
power accountable against abuse of people

00:06:27.240 --> 00:06:29.389
who use the Internet.

00:06:29.389 --> 00:06:37.119
The mechanisms of nation-state democracy even
when they are working perfectly aren't, don't

00:06:37.119 --> 00:06:40.199
really solve, answer that question for us.

00:06:40.199 --> 00:06:45.240
And then, of course, you have the overlapping
private sovereignties of companies and I love

00:06:45.240 --> 00:06:49.629
this map this is the latest version of the
world map of social networks and everything

00:06:49.629 --> 00:06:55.699
blue on this map is countries where Facebook
is the most popular social network.

00:06:55.699 --> 00:07:01.180
And as we saw in the Arab Spring and with
the work of Wael Ghonim and others, that people

00:07:01.180 --> 00:07:07.349
used this globally, interconnected network
called Facebook, this platform, to challenge

00:07:07.349 --> 00:07:12.159
the sovereignty and legitimacy of their government
in a way that was very exciting.

00:07:12.159 --> 00:07:17.580
But you also have the companies making decisions
that may or may not always be compatible with

00:07:17.580 --> 00:07:19.110
the rights and interests of their users.

00:07:19.110 --> 00:07:25.619
So for instance, to continue on with the Facebook
example I talk about lots of companies including

00:07:25.619 --> 00:07:29.999
Google in the book, but to continue on with
the Facebook example, Facebook's privacy policies.

00:07:29.999 --> 00:07:36.900
They may work in certain contexts but if you
change the privacy policies suddenly and expose

00:07:36.900 --> 00:07:43.470
people's friends' networks what does that
mean for an Iranian activist who doesn't want

00:07:43.470 --> 00:07:46.649
their friends to go to jail for being associated
with them?

00:07:46.649 --> 00:07:51.759
And to what extent have the people at the
company thinking about the product and trying

00:07:51.759 --> 00:07:58.210
to make it profitable considered some of the
life and death consequences of their choices?

00:07:58.210 --> 00:08:00.830
And so it's kind of both negative and positive.

00:08:00.830 --> 00:08:08.649
And then of course the big red there is China
with Facebook being blocked and of course

00:08:08.649 --> 00:08:15.500
no need to tell the story of Google's experience
in China where Baidu is now the dominant search

00:08:15.500 --> 00:08:20.349
engine and kind of is solidified because of
what's happened recently.

00:08:20.349 --> 00:08:27.860
But also in Russia you have Russian social
networks are the predominant networks at the

00:08:27.860 --> 00:08:28.930
moment.

00:08:28.930 --> 00:08:33.740
And there have been some troubling things
going on despite kind of the protests and

00:08:33.740 --> 00:08:37.750
hopeful things happening, there is some real
questions about the relationship between the

00:08:37.750 --> 00:08:42.740
Russian government and the social networking
companies and the extent to which the Russian

00:08:42.740 --> 00:08:50.519
security services are accessing information
in a manner that is not helpful to the democracy

00:08:50.519 --> 00:08:53.870
movement there just to put it mildly.

00:08:53.870 --> 00:09:01.339
So you've got again this question of how do
you hold Internet companies accountable to

00:09:01.339 --> 00:09:05.600
users' interests and then how does the kind
of the private governance that's going on

00:09:05.600 --> 00:09:13.329
overlap with the governance that's happening,
that's being conducted by nation states and

00:09:13.329 --> 00:09:17.459
then how that plays out both within countries
and globally.

00:09:17.459 --> 00:09:24.710
So I use a phrase which Bob Boorstin doesn't
like very much but we still speak anyway,

00:09:24.710 --> 00:09:32.830
"the sovereigns of cyberspace," and how yes
companies, [phone rings] oops, I better silence

00:09:32.830 --> 00:09:34.259
my phone.

00:09:34.259 --> 00:09:41.209
Companies cannot tax people and they cannot
put them in jail so one can carry the analogy

00:09:41.209 --> 00:09:42.209
too far.

00:09:42.209 --> 00:09:48.519
But large global Internet companies and platforms
are playing a kind of governance function

00:09:48.519 --> 00:09:55.209
because they are having an impact on what
people can and cannot know, how they organize

00:09:55.209 --> 00:10:02.009
in real life, how their identities are manifested
in their communities which has a lot of real

00:10:02.009 --> 00:10:07.120
life implications and ultimately can have
a lot of political implications and implications

00:10:07.120 --> 00:10:09.399
for people's freedoms.

00:10:09.399 --> 00:10:16.560
And this whole question of to what extent
are these sort of benevolent kings or kings

00:10:16.560 --> 00:10:24.800
that claim to be benevolent, and is that enough,
is that acceptable enough in this world today

00:10:24.800 --> 00:10:31.060
when we rejected the divine right of kings
some time ago in real life?

00:10:31.060 --> 00:10:38.640
And just to kind of reinforce kind of the
sovereignty problems that I was talking about,

00:10:38.640 --> 00:10:45.980
again the Internet primarily comprised of
corporate platforms and services and so on,

00:10:45.980 --> 00:10:52.300
there's obviously some non-profit and decentralized
non-corporate things going on as well, but

00:10:52.300 --> 00:10:57.560
most citizens their relationship with their
government is mediated through these platforms

00:10:57.560 --> 00:11:01.350
and services that are owned and operated by
the private sector in different ways.

00:11:01.350 --> 00:11:08.769
And so in a kind of standard democratic kind
of setup within a democracy ideally what happens

00:11:08.769 --> 00:11:15.571
is that we vote for our elected representatives
and then if a company is doing something that

00:11:15.571 --> 00:11:24.240
hurts us, let's say the milk has chemicals
in it that's creating disease or a company

00:11:24.240 --> 00:11:28.740
is employing 10 year olds or something like
that, the government then goes and regulates

00:11:28.740 --> 00:11:35.029
the company and that kind of serves the interests
and kind of holds everybody accountable because

00:11:35.029 --> 00:11:38.720
accountability is going through the elected
democratic government.

00:11:38.720 --> 00:11:43.579
That's the ideal it never works out perfectly
but that's the ideal and it's kind of basically

00:11:43.579 --> 00:11:49.640
in the pre-Internet age kind of made sense
and sort of worked as much as it could as

00:11:49.640 --> 00:11:53.100
any human system ever works.

00:11:53.100 --> 00:11:57.529
But we have a number of problems: one is that
increasingly you have companies, with SOPA

00:11:57.529 --> 00:12:04.160
and PIPA that was a very clear thing, the
extent to which corporations influenced legislation

00:12:04.160 --> 00:12:09.670
and that may or may not take into account
the rights and interests of a lot of other

00:12:09.670 --> 00:12:11.790
stakeholders.

00:12:11.790 --> 00:12:18.500
And so in that case, and then also you have
a situation where companies particularly technology

00:12:18.500 --> 00:12:25.449
and information companies, Internet companies,
are also shaping kind of how we conduct our

00:12:25.449 --> 00:12:29.470
campaigns, how we conduct our activism, what
we know, how we know it, who we know, what

00:12:29.470 --> 00:12:31.110
people know about us, and so on.

00:12:31.110 --> 00:12:34.879
So they're kind of shaping our lives as well.

00:12:34.879 --> 00:12:39.070
And so one of the things I raise in my book
is do we need to have citizens kind of taking

00:12:39.070 --> 00:12:45.769
their concerns more directly to the companies
more often because one of the problems with

00:12:45.769 --> 00:12:48.819
regulation, I mean you could just be calling
for the government to regulate Google and

00:12:48.819 --> 00:12:53.360
everybody else more because Google's not doing
this, that, or the other properly, but there

00:12:53.360 --> 00:12:57.470
are a lot of problems with regulation.

00:12:57.470 --> 00:13:02.079
It tends to be ham-fisted, it tends to, it
may or may not solve the problem, it may be

00:13:02.079 --> 00:13:03.790
counterproductive.

00:13:03.790 --> 00:13:09.209
Are there ways to kind of, for companies to
engage with their constituents, with their

00:13:09.209 --> 00:13:17.959
stakeholders earlier in the process of innovation
and product development to anticipate problems

00:13:17.959 --> 00:13:23.250
that would end up enabling you to avoid people
calling for you to get regulated in the first

00:13:23.250 --> 00:13:25.670
place?

00:13:25.670 --> 00:13:30.509
If you kind of talk to people about what their
concerns are gonna be and the question about

00:13:30.509 --> 00:13:37.660
how you do that and we've had some conversations
more directly about we're in very early days

00:13:37.660 --> 00:13:40.660
about thinking about how a company might do
that.

00:13:40.660 --> 00:13:46.279
You also have the problem, like I said before,
of governments passing laws and doing things

00:13:46.279 --> 00:13:52.149
that influence people who have no control
over what they do and a company in one country

00:13:52.149 --> 00:13:57.589
is influencing people around the world whose
experiences they might not understand or really

00:13:57.589 --> 00:14:00.249
relate to in any way.

00:14:00.249 --> 00:14:06.430
And so then it almost becomes even more important
to the people around the world since they

00:14:06.430 --> 00:14:11.240
can't do anything about what that government's
doing but they might be able to engage with

00:14:11.240 --> 00:14:14.550
the company that interfaces with that government.

00:14:14.550 --> 00:14:20.610
Can that government, can that company sometimes
act as an advocate for users who are not from

00:14:20.610 --> 00:14:22.519
that country that is passing the laws?

00:14:22.519 --> 00:14:25.910
And that's also an interesting question to
think about.

00:14:25.910 --> 00:14:32.089
So after I've shown that really excessively
complicated graphic I'm going to show a funny

00:14:32.089 --> 00:14:38.399
video from Monty Python and The Holy Grail
which I think kind of outlines one of the

00:14:38.399 --> 00:14:40.290
core points of the book.

00:14:40.290 --> 00:14:41.290
So here we go.

00:14:41.290 --> 00:14:42.290
[video playing]

00:14:42.290 --> 00:14:43.290
&gt;&gt;Peasant: The two-thirds majority in the
case of all others

00:14:43.290 --> 00:14:44.290
&gt;&gt;King: Be quiet.

00:14:44.290 --> 00:14:45.290
I order you to be quiet.

00:14:45.290 --> 00:14:46.290
&gt;&gt;Peasant: Order? who does he think he is?
[chuckles]

00:14:46.290 --> 00:14:47.290
&gt;&gt;King: I'm your king.

00:14:47.290 --> 00:14:48.290
&gt;&gt;Peasant: Well, I didn't vote for you.

00:14:48.290 --> 00:14:49.290
&gt;&gt;King: You don't vote for kings.

00:14:49.290 --> 00:14:51.740
&gt;&gt;Peasant: Well, how'd you become king then.

00:14:51.740 --> 00:14:58.949
&gt;&gt;King: The Lady of the Lake, [ethereal music
plays] her arm clad in the purest shimmering

00:14:58.949 --> 00:15:03.420
samite held aloft Excalibur from the bosom
of the water signifying by divine providence

00:15:03.420 --> 00:15:04.420
that I, Arthur, was to carry Excalibur.

00:15:04.420 --> 00:15:05.420
That is why I am your king.

00:15:05.420 --> 00:15:07.990
&gt;&gt;Peasant: Listen, strange women lying in
ponds, distributing swords is no basis for

00:15:07.990 --> 00:15:08.990
a system of government.

00:15:08.990 --> 00:15:13.339
Supreme executive power derives from a mandate
from the masses not from some farcical, aquatic

00:15:13.339 --> 00:15:14.339
ceremony.

00:15:14.339 --> 00:15:15.339
&gt;&gt;King: Be quiet!

00:15:15.339 --> 00:15:19.779
&gt;&gt;Peasant: You can't expect to wield supreme
executive power just 'cause some watery tart

00:15:19.779 --> 00:15:22.519
threw a sword at you.

00:15:22.519 --> 00:15:24.899
&gt;&gt;King: Shut up!!

00:15:24.899 --> 00:15:25.899
[laughter]

00:15:25.899 --> 00:15:34.350
&gt;&gt;Rebecca MacKinnon: Well this is one of my
favorite Monty Python clips of all time and

00:15:34.350 --> 00:15:39.029
so I just love the fact that I can actually
use it to illustrate a point in my book.

00:15:39.029 --> 00:15:46.700
The point of it is that, one reason why it's
so funny, is that it juxtaposes two completely

00:15:46.700 --> 00:15:52.410
different ways of thinking about power and
sovereignty and the way in which governance

00:15:52.410 --> 00:15:53.910
happens.

00:15:53.910 --> 00:15:58.259
And the first way is the divine right of kings
and there was a point in time where everybody

00:15:58.259 --> 00:16:07.850
in the world was basically governed by some
king, sultan, whatever Khan, whatever it was

00:16:07.850 --> 00:16:18.120
and most of these rulers were claiming divine
authority, emperor, to rule their subjects.

00:16:18.120 --> 00:16:23.129
And it was assumed this was the only way to
govern anything and that's how power worked

00:16:23.129 --> 00:16:26.019
and that's how the world was.

00:16:26.019 --> 00:16:30.670
And that evolved, we had political innovation
or political evolution or whatever you wanna

00:16:30.670 --> 00:16:33.079
call it.

00:16:33.079 --> 00:16:39.809
In modern times it began in part with the
Magna Carta when the barons of England kind

00:16:39.809 --> 00:16:43.490
of decided that this divine right of kings
thing was not working for them very well and

00:16:43.490 --> 00:16:49.529
King John was a real jerk and he needed to
be bound by laws and otherwise they weren't

00:16:49.529 --> 00:16:51.160
going to be loyal to him.

00:16:51.160 --> 00:16:55.700
And that kind of evolved over time to the
concept several hundred years later with John

00:16:55.700 --> 00:17:01.790
Locke and kind of articulating the notion
of consent of the governed and that government

00:17:01.790 --> 00:17:08.060
does not derive its mandate from the masses
is illegitimate.

00:17:08.060 --> 00:17:13.530
And then it took a bit longer for a revolution
here in the United States, in America as it

00:17:13.530 --> 00:17:19.420
was called then, to happen and for the first
effort to actually implement consent of the

00:17:19.420 --> 00:17:22.130
governed in real life.

00:17:22.130 --> 00:17:30.810
And now kind of globally, even countries that
aren't democratic still claim that they are

00:17:30.810 --> 00:17:38.700
governing with the consent and sometimes come
up with really elaborate sham mechanisms to

00:17:38.700 --> 00:17:43.110
prove this; fake elections and all kinds of
different things.

00:17:43.110 --> 00:17:48.530
And so consent of the governed we're sort
of now, I argue in the book, we're kind of

00:17:48.530 --> 00:17:52.391
at a new Magna Carta moment where consent
of the governed based on the nation state

00:17:52.391 --> 00:17:59.190
is kind of the assumed way of how you organize
power and how you hold power accountable.

00:17:59.190 --> 00:18:04.270
But we're starting to recognize that that's
not working for us so well anymore for all

00:18:04.270 --> 00:18:09.880
the reasons I talked about earlier that one
nation state exercises power over technology

00:18:09.880 --> 00:18:13.870
and then it has effects on people elsewhere
and they're not held accountable.

00:18:13.870 --> 00:18:21.550
And you have private corporations that may
be well meaning but are making decisions that

00:18:21.550 --> 00:18:29.460
have all kinds of different effects and that
sometimes the constituent users find legitimate

00:18:29.460 --> 00:18:34.140
and good and other times find to be quite
illegitimate.

00:18:34.140 --> 00:18:43.530
And how do we build mechanisms that actually
constrain the use of power across a globally

00:18:43.530 --> 00:18:50.160
networked world, across this global digital
network, and how do you hold power accountable

00:18:50.160 --> 00:18:52.910
across a global digital network?

00:18:52.910 --> 00:18:56.800
And we don't have mechanisms to do that.

00:18:56.800 --> 00:19:01.610
The current sort of structure of politics
and geopolitics and governance and business

00:19:01.610 --> 00:19:07.040
practices and corporate structures is not
working for that and we don't have a lot of

00:19:07.040 --> 00:19:08.040
answers.

00:19:08.040 --> 00:19:11.890
I don't have sort of Rebecca MacKinnon's solution
for like how to build consent of the network

00:19:11.890 --> 00:19:18.100
and some kind of grand scheme, I'm calling
on people to figure it out.

00:19:18.100 --> 00:19:22.470
But the first step is recognizing that there's
been a change and we need to work for it.

00:19:22.470 --> 00:19:30.250
And just kind of quickly before I close there's
an analogy that I first heard from a professor

00:19:30.250 --> 00:19:38.660
at the University of Austin, Texas named Rosental
Alves who talks about how the pre-Internet

00:19:38.660 --> 00:19:41.790
age was an information desert.

00:19:41.790 --> 00:19:49.030
And so governance, accountability, sort of
our whole way of organizing our reality and

00:19:49.030 --> 00:19:59.690
our knowledge about reality was based on scarcity
and based on the management of scarce informational

00:19:59.690 --> 00:20:00.690
resources.

00:20:00.690 --> 00:20:05.770
And then suddenly the rain came with the Internet
and we're now living in a rain forest and

00:20:05.770 --> 00:20:12.920
the ways of organizing civilization that worked
well enough or made sense in the desert are

00:20:12.920 --> 00:20:16.110
not working in the rain forest so well.

00:20:16.110 --> 00:20:21.740
And we have all kinds of new organisms that
have built up and so how do we build a civilization

00:20:21.740 --> 00:20:30.670
in cyberspace that is respectful of the rights
of all of its members and that you can govern

00:20:30.670 --> 00:20:38.670
it but it's governed in a manner that's fair
for all of its members and in which the abuse

00:20:38.670 --> 00:20:44.590
of power and force is constrained to an adequate
degree?

00:20:44.590 --> 00:20:47.000
And we're not sure how to do that yet.

00:20:47.000 --> 00:20:50.940
Also in the book I talk about the importance
of the Digital Commons and this is something

00:20:50.940 --> 00:20:56.320
that Google has been very supportive of in
terms of open source software and open source

00:20:56.320 --> 00:21:01.890
communities and free content communities or
content sharing communities.

00:21:01.890 --> 00:21:09.600
And all the people who are producing both
standards as well as code as well as content

00:21:09.600 --> 00:21:16.750
on the Web who are doing so for reasons other
than financial motive, without which, without

00:21:16.750 --> 00:21:20.850
this robust community not only would we have
no Internet but it would be much harder for

00:21:20.850 --> 00:21:24.240
civil society to do what they do today on
the Internet.

00:21:24.240 --> 00:21:31.801
And I co-founded this organization Global
Voices which runs on WordPress and there are

00:21:31.801 --> 00:21:36.970
a lot of activist communities around the world
that would not be functioning online if it

00:21:36.970 --> 00:21:44.340
were not for open source, developers, and
communities that continue to make those products

00:21:44.340 --> 00:21:45.600
better.

00:21:45.600 --> 00:21:52.380
So the importance of the Digital Commons in
sort of keeping global, digital, civil society

00:21:52.380 --> 00:21:56.470
and dissent and discourse alive being really
important.

00:21:56.470 --> 00:22:02.000
And then just in closing there are various
experiments, there's ICANN is one experiment

00:22:02.000 --> 00:22:08.090
to deal with one specific set of issues in
governing specific Internet resources related

00:22:08.090 --> 00:22:13.370
to domain names and matching them with IP
addresses and coordinating that.

00:22:13.370 --> 00:22:21.030
And there are a lot of problems with ICANN,
but it's a multi-stakeholder model and is

00:22:21.030 --> 00:22:27.150
trying to kind of insist on remaining multi-stakeholder
despite a lot of pressure from a lot of governments

00:22:27.150 --> 00:22:31.000
to basically hand over its functions to the
United Nations.

00:22:31.000 --> 00:22:35.800
And the important of bringing in civil society
groups in addition to companies to kind of

00:22:35.800 --> 00:22:41.700
work out how the Internet should be coordinated
going forward being really important.

00:22:41.700 --> 00:22:45.640
And you're seeing more groups kind of coming
up with bills of rights and declaring their

00:22:45.640 --> 00:22:49.910
rights and saying, "Look the Universal Declaration
of Human Rights can actually apply to the

00:22:49.910 --> 00:22:54.990
Internet and should and here are guidelines
for how governments and companies can take

00:22:54.990 --> 00:23:01.720
Universal Rights and apply them to both their
lawmaking and their corporate operations."

00:23:01.720 --> 00:23:09.250
You have efforts like the Global Network Initiative,
that Google's a member of, to bring companies

00:23:09.250 --> 00:23:13.840
together, commit to basic standards on free
expression, privacy, and other aspects of

00:23:13.840 --> 00:23:19.920
human rights and work proactively with civil
society groups and socially responsible investors

00:23:19.920 --> 00:23:26.820
to try and anticipate problems and also kind
of work together to figure out how do you

00:23:26.820 --> 00:23:32.270
respond to really difficult challenges, and
how do you think through the human rights

00:23:32.270 --> 00:23:37.840
implications of your business in advance of
problems emerging, and how do you address

00:23:37.840 --> 00:23:41.130
them the problems when they emerge?

00:23:41.130 --> 00:23:46.160
Google's Transparency Report, I think that
all companies ought to be releasing something

00:23:46.160 --> 00:23:47.160
similar.

00:23:47.160 --> 00:23:53.870
Again it's really important that citizens
understand kind of how government power and

00:23:53.870 --> 00:23:57.530
corporate power over their information is
interfacing 'cause I think it's where that

00:23:57.530 --> 00:24:03.990
intersection happens where a lot of troubling
things can take place.

00:24:03.990 --> 00:24:08.450
And so the extent to which companies can be
transparent about what governments are demanding

00:24:08.450 --> 00:24:15.330
and how people's information is accessed and
how it's used I think is critically important.

00:24:15.330 --> 00:24:23.930
And another thing that Google's been doing
a fair amount of is with Google+ for instance

00:24:23.930 --> 00:24:27.510
there've been a number of executives who've
been working very hard to kind of talk to

00:24:27.510 --> 00:24:34.630
user communities and talk to critics and get
that feedback and try and improve on some

00:24:34.630 --> 00:24:37.070
of the problems that have come up.

00:24:37.070 --> 00:24:45.250
And we were having a conversation earlier
about how to systematize that more and really

00:24:45.250 --> 00:24:51.500
use conversations with stakeholders and communities
to improve the product, anticipate problems,

00:24:51.500 --> 00:24:52.500
build trust.

00:24:52.500 --> 00:24:57.250
You're also, of course, seeing a lot, more
and more company-targeted activism.

00:24:57.250 --> 00:25:04.050
I think it's a real trend that more and more
people, activist groups that have been, up

00:25:04.050 --> 00:25:11.000
until recently, primarily targeting their
activism to government are gonna be targeting

00:25:11.000 --> 00:25:17.340
Internet companies more and more and so that's
an interesting trend to watch and to figure

00:25:17.340 --> 00:25:20.370
out how to kind of interface with that.

00:25:20.370 --> 00:25:26.850
And, of course, I think finally with the SOPA/PIPA
thing publics, voting publics, are becoming

00:25:26.850 --> 00:25:31.000
more aware of how Internet related legislation
is affecting them.

00:25:31.000 --> 00:25:35.700
I think people haven't been paying too much
attention, I think they're starting to and

00:25:35.700 --> 00:25:40.150
it's going to be very interesting to see if
Internet rights become more of a mainstream

00:25:40.150 --> 00:25:45.660
political issue and how that affects our politics
and how that affects kind the role that companies

00:25:45.660 --> 00:25:51.830
play in that political discussion in this
country and around the democratic world.

00:25:51.830 --> 00:25:58.150
So with that I end with my favorite picture
from the Occupy Wall Street Movement with

00:25:58.150 --> 00:26:07.310
this guy who has a sign there, "Let's get
our s-h-i-t, let's figure this s-h-i-t out

00:26:07.310 --> 00:26:09.310
together."

00:26:09.310 --> 00:26:14.100
The point being is we're recognizing, I think,
we're at a point it's not just when it comes

00:26:14.100 --> 00:26:17.940
to the Internet but when it comes to all kinds
of issues in terms of the global economy,

00:26:17.940 --> 00:26:24.240
in terms of the way global governance works,
a lot of things aren't working so well for

00:26:24.240 --> 00:26:30.890
a lot of people on the planet and how do we
work out solutions?

00:26:30.890 --> 00:26:37.280
And we need to figure this out together; civil
society, companies, how do we create a planet

00:26:37.280 --> 00:26:44.890
that's more sustainable and that is the kind
of society that we want to live in not only

00:26:44.890 --> 00:26:52.910
sustainable in terms of environmentally or
our resources, but also how do companies contribute

00:26:52.910 --> 00:27:02.300
to a society that we want politically, the
kind of freedoms that we want, that we aspire

00:27:02.300 --> 00:27:03.300
to have?

00:27:03.300 --> 00:27:11.060
And how do we make sure that companies are
part of this ecosystem to insuring that democracy

00:27:11.060 --> 00:27:17.960
is as robust and healthy as possible and not
inadvertently contributing actually to the

00:27:17.960 --> 00:27:24.430
weakening of democracy in different ways and
the extent to which it's important that companies

00:27:24.430 --> 00:27:28.330
be thinking about that as much as they're
thinking about, "How am I contributing to

00:27:28.330 --> 00:27:35.130
a sustainable environment or healthy work
force?" or all these other things that companies

00:27:35.130 --> 00:27:39.540
have come to think about which they didn't
think about at all 100 years ago; it wasn't

00:27:39.540 --> 00:27:42.270
considered the business of business but now
is.

00:27:42.270 --> 00:27:48.130
So kind of privacy, human rights, freedom
of expression is now kind of entering into

00:27:48.130 --> 00:27:54.650
I think very early days of people realizing
that this is yet another part of being a sustainable

00:27:54.650 --> 00:27:56.310
business.

00:27:56.310 --> 00:27:57.630
So with that, thank you very much.

00:27:57.630 --> 00:28:02.930
I talked a little longer than I planned but
I hope it was interesting and I look forward

00:28:02.930 --> 00:28:09.930
to your questions or comments or objections,
whatever it is.

00:28:09.930 --> 00:28:11.160
[applause]

00:28:11.160 --> 00:28:14.850
&gt;&gt;male #1: Hi.

00:28:14.850 --> 00:28:20.410
So your rhetorical context seems to be that
democracy is in some sense the natural terminal

00:28:20.410 --> 00:28:26.080
point of political development and that American
corporations are sort of the natural terminal

00:28:26.080 --> 00:28:29.260
point of economic development.

00:28:29.260 --> 00:28:34.290
But then you undermine yourself by going on
to point out that the powers that are exerted

00:28:34.290 --> 00:28:36.910
by government are not well matched with the
--

00:28:36.910 --> 00:28:37.910
&gt;&gt;Rebecca MacKinnon: Right.

00:28:37.910 --> 00:28:39.090
&gt;&gt;male #1: people who actually vote for them.

00:28:39.090 --> 00:28:49.970
There also seems to be a contradiction in
the very basis that governments maybe it is

00:28:49.970 --> 00:28:54.140
good for them to be democratic but corporations
are meritocratic --

00:28:54.140 --> 00:28:55.140
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:28:55.140 --> 00:28:58.100
&gt;&gt;male #1: perhaps with some bizarre metrics.

00:28:58.100 --> 00:29:01.230
But how can one be right for government and
the other be right for business?

00:29:01.230 --> 00:29:04.150
I guess what I'm trying to ask is --

00:29:04.150 --> 00:29:05.340
&gt;&gt;Rebecca MacKinnon: Yeah.

00:29:05.340 --> 00:29:11.710
&gt;&gt;male #1: if I analyze what you're saying
as closely as I can from what I've heard today,

00:29:11.710 --> 00:29:17.160
why is the space of solutions so small, why
are we talking about governments and corporations

00:29:17.160 --> 00:29:18.160
--

00:29:18.160 --> 00:29:19.160
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:29:19.160 --> 00:29:26.560
&gt;&gt;male #1: when in fact it looks as if informed
decision making is really the problem that

00:29:26.560 --> 00:29:27.560
we're trying to solve --

00:29:27.560 --> 00:29:28.560
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:29:28.560 --> 00:29:32.380
&gt;&gt;male #1: and at this point I would like
to say something nice about China who at least

00:29:32.380 --> 00:29:36.190
seem to be trying to find that solution even
if they haven't got there yet and may not

00:29:36.190 --> 00:29:37.410
get there for a long time.

00:29:37.410 --> 00:29:38.760
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:29:38.760 --> 00:29:41.290
Yeah, well, thanks for that.

00:29:41.290 --> 00:29:49.780
I lived in China for nine years and so that's
definitely something one could talk about

00:29:49.780 --> 00:29:54.810
for a long time, about the intentions of the
government and to what extent.

00:29:54.810 --> 00:30:01.720
Obviously China's a much more complex place
then is kind of discussed, I think, in a lot

00:30:01.720 --> 00:30:04.660
of the media.

00:30:04.660 --> 00:30:11.890
But I guess this question about democracy,
when I talk about democracy as an ideal and

00:30:11.890 --> 00:30:17.220
I guess that's part of the problem of giving
a short talk, I mean sort of democracy with

00:30:17.220 --> 00:30:23.860
a small 'd' as opposed to democracy as the
United States currently practices it institutionally.

00:30:23.860 --> 00:30:33.530
And I'm not sure, maybe there's a better system
of holding power in check, I just don't know

00:30:33.530 --> 00:30:34.710
what it is.

00:30:34.710 --> 00:30:41.520
I started my academic career studying the
origins of the Chinese and Russian Revolutions

00:30:41.520 --> 00:30:46.350
and the origins of Marxism and sort of Communist
revolutions.

00:30:46.350 --> 00:30:54.180
And so therefore, I must admit, I'm deeply
suspicious of Utopian philosophies that kind

00:30:54.180 --> 00:31:01.770
of seek to kind of provide a total solution
or that are based on the idea that human nature

00:31:01.770 --> 00:31:03.340
can change.

00:31:03.340 --> 00:31:10.320
It's just everything I've studied and everything
I've experienced personally having lived in

00:31:10.320 --> 00:31:16.370
China for a very long time and also spent
a little time in Russian and having just studied,

00:31:16.370 --> 00:31:24.440
kind of, what went on; and my parents actually
wrote a book about a Communist journalist

00:31:24.440 --> 00:31:31.040
in the 30's and 40's who was involved with
several Communist movements at the time.

00:31:31.040 --> 00:31:37.190
And so I guess I've been exposed a lot to
how idealist the Socialist movements were

00:31:37.190 --> 00:31:42.930
at the turn of the last century and how badly
wrong they went and how horribly they were

00:31:42.930 --> 00:31:46.160
abused by evil people.

00:31:46.160 --> 00:31:54.840
And so, I guess, where I end up philosophically
is that democracy kind of as an ideal is a

00:31:54.840 --> 00:31:58.400
horribly, imperfect thing.

00:31:58.400 --> 00:32:05.340
But I haven't come across anything, I tend
to buy into the notion that we have to assume

00:32:05.340 --> 00:32:10.820
in our system of government, any system of
government, that people are gonna abuse power

00:32:10.820 --> 00:32:17.560
and that people are gonna act in their own
self interest and that you need to create

00:32:17.560 --> 00:32:23.390
a system that kind sets up checks and constraints,
and that as human beings that's the best we're

00:32:23.390 --> 00:32:28.400
ever gonna be able to hope for unless human
nature changes which I'm not counting on and

00:32:28.400 --> 00:32:35.370
which I think basing a system of political
movement on is dangerous.

00:32:35.370 --> 00:32:45.220
So I'd love to hear other ideas about other
political systems that might work better but

00:32:45.220 --> 00:32:54.510
I'm deeply suspicious of anything I've heard
so far that isn't small 'd' democratic and

00:32:54.510 --> 00:32:59.700
kind of small 'l' liberal and that's just
where I come from philosophically.

00:32:59.700 --> 00:33:06.560
And I guess then, again with corporations,
having spent time in the Communist world,

00:33:06.560 --> 00:33:13.650
I do think that capitalism needs to survive,
it needs to change, it needs to evolve, it's

00:33:13.650 --> 00:33:17.750
not currently in the way it's practiced in
many forms healthy.

00:33:17.750 --> 00:33:23.990
But again what are the alternatives?

00:33:23.990 --> 00:33:31.360
State run economies, I just know too many
people who have suffered under that kind of

00:33:31.360 --> 00:33:32.360
system.

00:33:32.360 --> 00:33:38.760
So I guess my hope is that we can find a way
to evolve private enterprise in a manner that's

00:33:38.760 --> 00:33:45.380
sustainable and evolve small 'd' democracy
in a manner that works in a globally networked

00:33:45.380 --> 00:33:46.700
world.

00:33:46.700 --> 00:33:54.550
And it is messy and I guess that's just sort
of the nature of where we are now that it

00:33:54.550 --> 00:34:04.940
does seem contradictory [laughs] because the
ideologies that have kind of been the norm

00:34:04.940 --> 00:34:08.430
aren't necessarily producing the best results.

00:34:08.430 --> 00:34:19.860
Yet all the ideological and kind of systemic
alternatives that I know of as they've been

00:34:19.860 --> 00:34:24.379
practiced to date have resulted in much greater
evil in my opinion.

00:34:24.379 --> 00:34:32.960
And China to the extent that it is gonna work
out I think is moving away from those ideologies

00:34:32.960 --> 00:34:37.930
and those systems rather than re-embracing
them.

00:34:37.930 --> 00:34:48.260
So yeah, but it's tough and I really appreciate
the points you made because had I been a really

00:34:48.260 --> 00:34:53.670
rigorous political scientist and theorist
rather than writing a more kind of populist,

00:34:53.670 --> 00:35:00.560
general book I think I would have quickly
gotten twisted around on the theory.

00:35:00.560 --> 00:35:04.680
[laughs] So you're absolutely right to point
that out.

00:35:04.680 --> 00:35:05.680
Um-hum.

00:35:05.680 --> 00:35:08.730
&gt;&gt;male #2: Hi, thanks again for coming.

00:35:08.730 --> 00:35:12.970
So my question is really about the consent
of the governed --

00:35:12.970 --> 00:35:13.970
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:35:13.970 --> 00:35:14.980
&gt;&gt;male #2: concept.

00:35:14.980 --> 00:35:20.350
So I think all of us here, especially people
involved in developing products, I wouldn't

00:35:20.350 --> 00:35:27.390
be surprised if people here feel that like
our users actually consent to use us --

00:35:27.390 --> 00:35:28.390
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:35:28.390 --> 00:35:29.850
&gt;&gt;male #2: way more than I, as a citizen,
consent to almost --

00:35:29.850 --> 00:35:30.850
&gt;&gt;Rebecca MacKinnon: Yeah.

00:35:30.850 --> 00:35:31.850
&gt;&gt;male #2: anything my government does.

00:35:31.850 --> 00:35:37.040
I mean on a daily basis a user's consenting
to use Google, for example.

00:35:37.040 --> 00:35:41.550
And so I guess I'm interested to tease out
a little bit more, I think that there's something

00:35:41.550 --> 00:35:43.400
to it I'm just interested to tease out --

00:35:43.400 --> 00:35:44.400
&gt;&gt;Rebecca MacKinnon: Yeah.

00:35:44.400 --> 00:35:47.190
&gt;&gt;male #2: a little bit more like what, what
is the thing that you're worried about that

00:35:47.190 --> 00:35:50.160
Google doesn't, needs more consent for somehow.

00:35:50.160 --> 00:35:51.280
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:35:51.280 --> 00:35:54.600
Oh that's a really good, that's a really good
question.

00:35:54.600 --> 00:36:02.060
Yeah and there's obviously a lot of problems
with democratic consent as practiced for exactly

00:36:02.060 --> 00:36:07.320
the reason you point out, that most people
are not exercising their ability to reinforce

00:36:07.320 --> 00:36:11.370
consent or withdraw consent they're just kind
of apathetic.

00:36:11.370 --> 00:36:15.510
And so to the extent that you're apathetic
is the extent to which you get the government

00:36:15.510 --> 00:36:21.270
you deserve I would argue.

00:36:21.270 --> 00:36:22.890
[laughs]

00:36:22.890 --> 00:36:27.220
And it's absolutely true people are clicking
through to terms of service and so on, but

00:36:27.220 --> 00:36:31.580
then there are a lot of things that I think
people are just kind of sticking with even

00:36:31.580 --> 00:36:36.470
though they're uncomfortable and unhappy with
them because they're on the service and it's

00:36:36.470 --> 00:36:43.560
such a pain to leave and I hear this from
people who use Facebook all the time.

00:36:43.560 --> 00:36:47.170
"Like, I need to be on Facebook because all
my friends are there," or "because I'm an

00:36:47.170 --> 00:36:51.960
environmental activist and running this campaign
I have to do it on Facebook, but this that

00:36:51.960 --> 00:36:55.270
and the other thing deeply troubles me and
I don't know what to do."

00:36:55.270 --> 00:36:56.830
You hear that a lot.

00:36:56.830 --> 00:37:01.990
And so with the Google+ thing, for instance,
there were a lot of people who said, "Who

00:37:01.990 --> 00:37:05.440
are you to say that?"

00:37:05.440 --> 00:37:13.360
So say there's an Iranian activist who has
a persistent online identity, Vahid Online,

00:37:13.360 --> 00:37:20.270
it's obviously not his real name and he got
kicked off of Google+ and the issue is to

00:37:20.270 --> 00:37:25.350
what extent this, and we've had conversations
about this in private, but to what extent

00:37:25.350 --> 00:37:26.350
does this--.

00:37:26.350 --> 00:37:35.120
Yeah I mean he probably did click on some
terms of service he didn't read and he doesn't

00:37:35.120 --> 00:37:41.760
have to use it, nobody's forcing him to, but
if he's also been kicked off of Facebook and

00:37:41.760 --> 00:37:42.760
everything else --

00:37:42.760 --> 00:37:46.830
[pause]

00:37:46.830 --> 00:37:52.140
would Google in the long run benefit from
developing a product in a way that has kind

00:37:52.140 --> 00:38:00.170
of more proactive buy in from its users so
that they feel that the policies that are

00:38:00.170 --> 00:38:04.060
being developed really kind of respect their
rights and interests?

00:38:04.060 --> 00:38:11.300
And how you do that, of course, is I think
we're still in really early days of figuring

00:38:11.300 --> 00:38:13.010
that out.

00:38:13.010 --> 00:38:19.480
I kind of feel and I certainly hope that the
companies that figure that out more than their

00:38:19.480 --> 00:38:23.990
competitors or faster than their competitors,
are going to have a real advantage in terms

00:38:23.990 --> 00:38:31.090
of the trust and buy in that they earn from
people who use their products.

00:38:31.090 --> 00:38:36.750
I'm very much hoping that that will be kind
of part of the value that companies create

00:38:36.750 --> 00:38:38.670
and a sort of innovation in a way.

00:38:38.670 --> 00:38:45.400
But yeah, 'cause at the same time taking a
product development decision to a vote of

00:38:45.400 --> 00:38:49.220
your users is, I mean, that would be ludicrous
right?

00:38:49.220 --> 00:38:51.900
[someone talking in background]

00:38:51.900 --> 00:38:58.820
But then the issue is, and this kinda gets
beyond the consent or at least the majority

00:38:58.820 --> 00:39:08.190
consent question because if you're, coming
back to sort of physical governance, if you're

00:39:08.190 --> 00:39:14.450
trying to run a democracy in which unpopular
minorities can also exist or people with really

00:39:14.450 --> 00:39:21.640
niche or we might say long tail concerns,
if you're trying to create a society in which

00:39:21.640 --> 00:39:28.360
those people can be heard and have the space
to exist and aren't squeezed out or living

00:39:28.360 --> 00:39:37.040
in fear or whatever, completely marginalized,
how do you structure that and how do you provide

00:39:37.040 --> 00:39:44.850
the mechanisms for those people's rights to
speak, to exist, to own property, to run a

00:39:44.850 --> 00:39:51.630
business, to publish, remain very protected
despite the fact that maybe the majority of

00:39:51.630 --> 00:39:59.190
people in that community might think that
they're completely illegitimate and should

00:39:59.190 --> 00:40:04.780
have no right to exist and don't like them
at all?

00:40:04.780 --> 00:40:05.780
[laughs]

00:40:05.780 --> 00:40:11.930
But the extent to which a society kinda protects
the right to exist and the right to thrive

00:40:11.930 --> 00:40:19.850
of its most kind of marginalized and vulnerable
members I think is important and as we, and

00:40:19.850 --> 00:40:29.770
then again sort of overlapping it, things
like Google on top of our attempt to continue

00:40:29.770 --> 00:40:36.890
to have the democracy when the public's fear
is kind of engaged primarily through these

00:40:36.890 --> 00:40:44.611
spaces and political campaigns and activism
and so on, how do you ensure that those people

00:40:44.611 --> 00:40:52.170
whose rights we want to be protected in real
life have a place to exist in these digital

00:40:52.170 --> 00:40:53.170
spaces?

00:40:53.170 --> 00:40:56.960
And then how do you kind of consult with them
and learn about their concerns, find a way

00:40:56.960 --> 00:41:03.230
to take their situation into account?

00:41:03.230 --> 00:41:12.490
And so that kind of goes beyond consent theory
to other stuff but I think as we're thinking

00:41:12.490 --> 00:41:19.880
about just how to be compatible with democracy,
which may or may not have anything to do with

00:41:19.880 --> 00:41:26.340
running a profitable business, [laughs] what
needs to be taken into consideration?

00:41:26.340 --> 00:41:34.310
&gt;&gt;female #1: So I wanted to ask you a question
that is a little bit off the core of your

00:41:34.310 --> 00:41:35.310
book --

00:41:35.310 --> 00:41:36.310
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:41:36.310 --> 00:41:40.900
&gt;&gt;female #1: which focuses on governments
and Internet software companies.

00:41:40.900 --> 00:41:47.460
I wanted to ask you about some of the developments
over the last year that we've learned about

00:41:47.460 --> 00:41:48.970
in regards to surveillance --

00:41:48.970 --> 00:41:49.970
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:41:49.970 --> 00:41:50.970
&gt;&gt;female #1: by governments.

00:41:50.970 --> 00:41:51.970
&gt;&gt;Rebecca MacKinnon: Yeah.

00:41:51.970 --> 00:41:52.970
&gt;&gt;female #1: Blue Coat --

00:41:52.970 --> 00:41:53.970
&gt;&gt;Rebecca MacKinnon: Um-hum..

00:41:53.970 --> 00:41:54.970
&gt;&gt;female #1: in Syria --

00:41:54.970 --> 00:41:55.970
&gt;&gt;Rebecca MacKinnon: Yeah.

00:41:55.970 --> 00:41:59.350
&gt;&gt;female #1: Cisco in China and that New York
Times' story from a couple of weeks ago about

00:41:59.350 --> 00:42:01.421
Apple and the iPhone which was a --

00:42:01.421 --> 00:42:02.490
&gt;&gt;Rebecca MacKinnon: Yeah.

00:42:02.490 --> 00:42:09.450
&gt;&gt;female #1: fascinating story, but to me
really captured a conversation and a debate

00:42:09.450 --> 00:42:12.100
that has been going on that predates the Internet.

00:42:12.100 --> 00:42:13.100
&gt;&gt;Rebecca MacKinnon: Yeah.

00:42:13.100 --> 00:42:15.450
&gt;&gt;female #1: It goes down to the whole question
of sweat shops --

00:42:15.450 --> 00:42:16.450
&gt;&gt;Rebecca MacKinnon: Yeah.

00:42:16.450 --> 00:42:17.450
&gt;&gt;female #1: and foreign workers -

00:42:17.450 --> 00:42:18.450
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:42:18.450 --> 00:42:22.190
&gt;&gt;female #1: and what the role of governments
are in Western democracies --

00:42:22.190 --> 00:42:23.190
&gt;&gt;Rebecca MacKinnon: Yeah.

00:42:23.190 --> 00:42:30.840
&gt;&gt;female #1: that, where companies are based
and yet use the labor of companies that are

00:42:30.840 --> 00:42:32.450
in regimes that are less open.

00:42:32.450 --> 00:42:33.450
&gt;&gt;Rebecca MacKinnon: Yeah.

00:42:33.450 --> 00:42:34.660
&gt;&gt;female #1: What's your take on that?

00:42:34.660 --> 00:42:41.920
&gt;&gt;Rebecca MacKinnon: Well, I mean the Apple
case just to kind of start from there is,

00:42:41.920 --> 00:42:48.920
Apple is reviled within the socially responsible
investment community [laughs] for exactly

00:42:48.920 --> 00:42:52.190
the sweat shop reason.

00:42:52.190 --> 00:42:58.700
And there is something called the Fair Labor
Association that a lot of manufacturers have

00:42:58.700 --> 00:43:01.990
joined and have been members of for a long
time.

00:43:01.990 --> 00:43:06.090
And some companies like Gap kind of took a
while to kind of turn around and recognize

00:43:06.090 --> 00:43:14.490
the benefit of being part of these types of
initiatives where they commit to basically

00:43:14.490 --> 00:43:19.940
be audited for their labor standards and to
kind of be certified that they're not abusing

00:43:19.940 --> 00:43:20.940
workers.

00:43:20.940 --> 00:43:28.369
And Apple's been, I think they announced recently
that they are gonna join the FLA, but they've

00:43:28.369 --> 00:43:32.540
been really resistant for the most part to
engage with the human rights community on

00:43:32.540 --> 00:43:38.040
labor standards and it's one of an extremely
long list of troubling things about Apple

00:43:38.040 --> 00:43:45.170
[laughs] that one might compile.

00:43:45.170 --> 00:43:49.540
And then this question of sales of surveillance
technology I actually do deal with that in

00:43:49.540 --> 00:43:56.290
the book as well although some of the major
news kind of came out after the book went

00:43:56.290 --> 00:43:58.450
into the publisher.

00:43:58.450 --> 00:44:04.960
But, yeah, absolutely, there are companies
that are profiting directly and knowingly

00:44:04.960 --> 00:44:13.470
from the, it's basically arms sales to repressive
regimes essentially, and this is one reason

00:44:13.470 --> 00:44:18.430
why this piece of legislation called The Global
Online Freedom Act has been revived kind of

00:44:18.430 --> 00:44:23.130
in a new form in the past couple months.

00:44:23.130 --> 00:44:28.420
And it's geared more than it was in the past
towards trying to constrain precisely this

00:44:28.420 --> 00:44:35.120
type of export and this type of business relationship.

00:44:35.120 --> 00:44:41.210
And the problem is, of course, it's hard because
a lot of this technology is also used for

00:44:41.210 --> 00:44:47.170
legitimate security purposes or legitimate
network management purposes and you can market

00:44:47.170 --> 00:44:52.820
it one way for a very repressive purpose and
then you can also market it in another way

00:44:52.820 --> 00:45:02.119
and so how exactly you regulate it 
is hard.

00:45:02.119 --> 00:45:10.820
I mean one actually interesting development
is that, what was it Websense recently joined

00:45:10.820 --> 00:45:19.270
the GNI and they're a filtering software company
and they say, "Look we're meant to help families

00:45:19.270 --> 00:45:27.200
keep their children off of certain websites
and companies keep their employees from playing

00:45:27.200 --> 00:45:29.730
on Facebook all day when they're supposed
to be working.

00:45:29.730 --> 00:45:34.720
We're not meant to be used as a tool of political
repression and so we want to kind of commit

00:45:34.720 --> 00:45:40.670
to, commit publicly to this notion that we
don't want our software to be used for political

00:45:40.670 --> 00:45:42.480
repressive purposes.

00:45:42.480 --> 00:45:47.119
And we're gonna work with investors and human
rights activists to kind of prevent that from

00:45:47.119 --> 00:45:48.119
happening."

00:45:48.119 --> 00:45:51.470
And they just joined so we're gonna kind have
to see how that evolves.

00:45:51.470 --> 00:45:57.250
But I think it would be important for these
companies who produce equipment and software

00:45:57.250 --> 00:46:06.460
that can be used, has this dual use function,
to make really robust commitments about what

00:46:06.460 --> 00:46:11.180
their technology is intended for and what
it's not, and to make commitments not to sell

00:46:11.180 --> 00:46:14.790
to regimes that obviously are gonna use it
in a particular way.

00:46:14.790 --> 00:46:20.080
And I think it would be, you may be able to
regulate that to some extent and I think the

00:46:20.080 --> 00:46:25.020
regulations actually moving towards SEC reporting
and this kind of thing so that investors will

00:46:25.020 --> 00:46:29.960
understand more whether this company is behaving
responsibly.

00:46:29.960 --> 00:46:36.760
But also the socially responsible investment
funds that are sort of gaining steam kind

00:46:36.760 --> 00:46:42.480
of across the board are starting to look more
at these issues too.

00:46:42.480 --> 00:46:50.010
And Boston Common, which is one of these funds,
they sold all their Cisco stock last year

00:46:50.010 --> 00:46:55.490
because they were so mad that Cisco wouldn't
engage in dialog with them about how their

00:46:55.490 --> 00:46:57.580
products were being used in China.

00:46:57.580 --> 00:47:04.540
And so I think we're gonna start seeing more
investor activism and more push to kind of

00:47:04.540 --> 00:47:09.110
require these types of companies to disclose
who they're selling to and to adopt policies,

00:47:09.110 --> 00:47:12.770
kind of know your customer and that you have
to kind of track where it's going and where

00:47:12.770 --> 00:47:17.340
it's being used and sell and market it responsibly.

00:47:17.340 --> 00:47:22.930
But it's a really, it's a tough and troubling
issue.

00:47:22.930 --> 00:47:24.320
Yep, you had a --

00:47:24.320 --> 00:47:25.770
&gt;&gt;male #1: Can I go round again?

00:47:25.770 --> 00:47:27.010
&gt;&gt;Rebecca MacKinnon: Yeah, sure, why not.

00:47:27.010 --> 00:47:31.720
&gt;&gt;male #1: So taking a step beyond consent
of the governed, I think we have to think

00:47:31.720 --> 00:47:33.020
about the informed consent --

00:47:33.020 --> 00:47:34.020
&gt;&gt;Rebecca MacKinnon: Yeah.

00:47:34.020 --> 00:47:35.020
&gt;&gt;male #1: of the governed.

00:47:35.020 --> 00:47:36.020
&gt;&gt;Rebecca MacKinnon: Absolutely, yeah.

00:47:36.020 --> 00:47:39.400
&gt;&gt;male #1: One of the things that, if I may
wax political, the Republicans and the Taliban

00:47:39.400 --> 00:47:40.400
have in common is --

00:47:40.400 --> 00:47:41.400
&gt;&gt;Rebecca MacKinnon: Yeah.

00:47:41.400 --> 00:47:42.400
&gt;&gt;male #1: this clever idea

00:47:42.400 --> 00:47:43.400
&gt;&gt;Rebecca MacKinnon: [laughs]

00:47:43.400 --> 00:47:44.400
&gt;&gt;male #1: that if you abolish science education
--

00:47:44.400 --> 00:47:45.400
&gt;&gt;Rebecca MacKinnon: Um-hum.

00:47:45.400 --> 00:47:47.400
&gt;&gt;male #1: and you abolish ethical education
then maybe people won't notice that we're

00:47:47.400 --> 00:47:48.730
saying something weird here.

00:47:48.730 --> 00:47:49.730
&gt;&gt;Rebecca MacKinnon: [laughs]

00:47:49.730 --> 00:47:50.730
&gt;&gt;male #1: And --

00:47:50.730 --> 00:47:51.960
&gt;&gt;Rebecca MacKinnon: Yeah, that is a problem.

00:47:51.960 --> 00:47:52.960
&gt;&gt;male #1: here --

00:47:52.960 --> 00:47:53.960
&gt;&gt;Rebecca MacKinnon: [laughs]

00:47:53.960 --> 00:48:00.190
&gt;&gt;male #1: on the corporate side here at Google
we have a rather peculiar position that it's

00:48:00.190 --> 00:48:04.310
all very well to talk about the free market
but the notion of marketing undermines the

00:48:04.310 --> 00:48:06.010
free market if you tell people what to buy
--

00:48:06.010 --> 00:48:07.010
&gt;&gt;Rebecca MacKinnon: Yeah.

00:48:07.010 --> 00:48:10.050
&gt;&gt;male #1: and you don't give them good opinions
then they'll act on the bad opinions --

00:48:10.050 --> 00:48:12.410
&gt;&gt;Rebecca MacKinnon: Yeah and you manipulate
them with kind of quasi --

00:48:12.410 --> 00:48:13.800
&gt;&gt;male #1: Right.

00:48:13.800 --> 00:48:16.960
So as we've stepped from a sort of informational
desert to more of an --

00:48:16.960 --> 00:48:17.960
&gt;&gt;Rebecca MacKinnon: Yeah.

00:48:17.960 --> 00:48:23.740
&gt;&gt;male #1: informational blizzard what do
you think we can do structurally --

00:48:23.740 --> 00:48:24.740
&gt;&gt;Rebecca MacKinnon: Yeah.

00:48:24.740 --> 00:48:25.740
&gt;&gt;male #1: in society to promote truth --

00:48:25.740 --> 00:48:26.859
&gt;&gt;Rebecca MacKinnon: Yeah.

00:48:26.859 --> 00:48:28.760
&gt;&gt;male #1: over political advantage?

00:48:28.760 --> 00:48:34.410
&gt;&gt;Rebecca MacKinnon: That's a really, that
is subject of some other books by some really

00:48:34.410 --> 00:48:40.170
smart people I know [laughs] and is a huge
matter of debate.

00:48:40.170 --> 00:48:46.650
And this whole question of to what extent
do people care about facts is very troubling.

00:48:46.650 --> 00:48:51.170
I completely agree.

00:48:51.170 --> 00:48:59.320
And as a former journalist, I'm often, one
of the reasons I left CNN was because my managers

00:48:59.320 --> 00:49:04.450
wanted me to do less to inform the public
than I was interested in doing and I was told

00:49:04.450 --> 00:49:08.681
I should cover my region more like a tourist
and that my expertise was getting in the way

00:49:08.681 --> 00:49:13.350
of kinds of stories they wanted on prime time
and so on.

00:49:13.350 --> 00:49:15.250
Dumb myself down basically.

00:49:15.250 --> 00:49:20.160
And so when the reason you go into journalism
is 'cause you think it's really important

00:49:20.160 --> 00:49:25.670
for the public to be informed and then your
bosses say, "Well, we just want you to kinda

00:49:25.670 --> 00:49:32.780
not really inform them that much," [laughs]
it's really depressing.

00:49:32.780 --> 00:49:37.590
But my dear friend and colleague Ethan Zuckerman
is working on a book that I think will come

00:49:37.590 --> 00:49:42.010
out in maybe a year or so.

00:49:42.010 --> 00:49:45.570
In dealing with what he calls the caring problem
which is a related problem which is how do

00:49:45.570 --> 00:49:52.290
you get people to pay attention to issues
that are important, and the importance of

00:49:52.290 --> 00:50:02.440
actually engineering for it and structuring
your systems around the recognition that people

00:50:02.440 --> 00:50:09.530
are probably gonna have a bias towards not
seeking out certain kinds of information or

00:50:09.530 --> 00:50:14.650
a bias towards being susceptible to manipulation
and so on.

00:50:14.650 --> 00:50:21.800
And so how do you engineer, and also just
kind of build online communities and platforms

00:50:21.800 --> 00:50:28.571
that try to kind of move the caring problem,
kind of address the caring problem in the

00:50:28.571 --> 00:50:29.571
other direction?

00:50:29.571 --> 00:50:33.080
And that's one of the things we try to do
with Global Voices is just kind of make is

00:50:33.080 --> 00:50:41.200
easier to access content from different countries
created by citizen journalists that's gonna

00:50:41.200 --> 00:50:44.680
give you a different point of view than what
you're getting from the New York Times or

00:50:44.680 --> 00:50:51.560
CNN, let alone Fox or whatever else, about
what's happening in whatever country it happens

00:50:51.560 --> 00:50:53.330
to be.

00:50:53.330 --> 00:50:58.850
But yeah I think it requires constant effort
because I think this is one of the problems,

00:50:58.850 --> 00:51:07.590
one of the many bugs in human nature that's
so troubling [laughs] which is that, yeah,

00:51:07.590 --> 00:51:12.490
we often times are more governed by emotions
and other things than we are by facts.

00:51:12.490 --> 00:51:18.740
And so how do we kind of say, "Okay, rather
than what people should be, let's accept it

00:51:18.740 --> 00:51:25.560
as a given all the bugs in the way people
are and try and compensate for that in the

00:51:25.560 --> 00:51:30.440
way we build information products, news products,
and so on."

00:51:30.440 --> 00:51:37.390
But it's really, really hard given, particularly
with for-profit news organizations being pushed

00:51:37.390 --> 00:51:42.109
to, if you're gonna expand circulation or
ratings kind of ends of driving a lot of companies

00:51:42.109 --> 00:51:44.960
in the opposite direction.

00:51:44.960 --> 00:51:50.430
And so the extent to which we can kind of
build information communities and the extent

00:51:50.430 --> 00:51:57.510
to which Google can kind of help bridge some
of these gaps, I think, is vitally important

00:51:57.510 --> 00:51:58.510
now.

00:51:58.510 --> 00:51:59.510
Anyway.

00:51:59.510 --> 00:52:04.650
&gt;&gt;male #3: One of the things here at Google
most of the people here benefit tremendously

00:52:04.650 --> 00:52:05.911
from a liberal democracy --

00:52:05.911 --> 00:52:06.911
&gt;&gt;Rebecca MacKinnon: Hum.

00:52:06.911 --> 00:52:12.740
&gt;&gt;male #3: for our lives and we tend to build
systems that we like and unfortunately some

00:52:12.740 --> 00:52:17.550
of those systems that we like we can deploy
them to the entire world instantaneously with

00:52:17.550 --> 00:52:18.920
sometimes devastating results.

00:52:18.920 --> 00:52:21.160
&gt;&gt;Rebecca MacKinnon: Um-hum, like with Buzz
or something.

00:52:21.160 --> 00:52:22.160
yeah.

00:52:22.160 --> 00:52:25.270
&gt;&gt;male #3: Yeah, I reread your article there.

00:52:25.270 --> 00:52:31.190
So how do we temperate that and what's the,
I mean it's a difficult problem because one

00:52:31.190 --> 00:52:32.980
person does have a tremendous amount of power
nowadays.

00:52:32.980 --> 00:52:33.980
&gt;&gt;Rebecca MacKinnon: Um-hum, yeah.

00:52:33.980 --> 00:52:35.410
&gt;&gt;male #3: And how do we check that?

00:52:35.410 --> 00:52:36.410
&gt;&gt;Rebecca MacKinnon: Yeah.

00:52:36.410 --> 00:52:42.850
Well I think part of it is in, I think the
first step is recognizing this problem and

00:52:42.850 --> 00:52:47.810
that something that's really cool and works
really well in the context of Mountain View

00:52:47.810 --> 00:52:51.350
may play out really differently in Syria.

00:52:51.350 --> 00:53:00.310
And, but there's actually people you can consult
with on that and you can talk to somebody

00:53:00.310 --> 00:53:07.890
who's sort of part of that community and you
can, I mean, this is one reason why with the

00:53:07.890 --> 00:53:12.570
Global Network Initiative why we advocate
that people run kind of human rights due diligence

00:53:12.570 --> 00:53:14.720
and kind of human rights assessments.

00:53:14.720 --> 00:53:19.680
When you're assessing a new product and how
it works you kind of game through, "Okay,

00:53:19.680 --> 00:53:28.320
this is really great for us here in Mountain
View, but what if I am a dissident in Syria?

00:53:28.320 --> 00:53:31.310
What are all the worse case scenarios that
might play out?

00:53:31.310 --> 00:53:36.310
Let's try and run those through and figure
out how that could play out and talk to people

00:53:36.310 --> 00:53:38.740
who've got experience using the Internet in
Syria."

00:53:38.740 --> 00:53:41.540
Or, "How is that gonna play out in China?"

00:53:41.540 --> 00:53:44.060
There are people you can talk to.

00:53:44.060 --> 00:53:49.050
Or how does that play out with people who
are fleeing their abusive spouses or people

00:53:49.050 --> 00:53:55.260
who don't want their sexual preference to
be known by their bosses, people who are maybe

00:53:55.260 --> 00:53:57.130
not quite so far from home.

00:53:57.130 --> 00:54:03.010
But I think the first step is just recognizing
that you can't know on your own, it's the

00:54:03.010 --> 00:54:08.150
humility, I can't know on my own exactly how
this product is gonna play out around the

00:54:08.150 --> 00:54:12.980
world amongst people whose experience is so
different from my own.

00:54:12.980 --> 00:54:18.520
But the good news is that there are people
out there who wanna help you think that through

00:54:18.520 --> 00:54:25.400
before you roll out the product and that you
can call upon them.

00:54:25.400 --> 00:54:32.850
And so organizations like the Global Network
Initiative which Google joined, have, that's

00:54:32.850 --> 00:54:39.720
kind of one interface where a company can
go to, to seek out the people you need to

00:54:39.720 --> 00:54:44.960
consult with to make sure that there aren't
unintended consequences.

00:54:44.960 --> 00:54:52.840
And so yeah I mean it was a really important
step with Google recognizing that Google has

00:54:52.840 --> 00:54:58.730
a lot of geniuses but you can't anticipate
everything –

00:54:58.730 --> 00:55:03.130
and just having that kind of humility is real
important.

00:55:03.130 --> 00:55:08.350
&gt;&gt;Female presenter: Just wanna thank you for
coming and talking with us.

00:55:08.350 --> 00:55:09.750
&gt;&gt;Rebecca MacKinnon: Thank you.

00:55:09.750 --> 00:55:10.750
[applause]

00:55:10.750 --> 00:55:12.120
Great conversation.

