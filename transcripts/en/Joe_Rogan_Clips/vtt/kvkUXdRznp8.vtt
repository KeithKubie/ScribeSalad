WEBVTT

1
00:00:01.020 --> 00:00:02.490
The Joe Rogan experience.

2
00:00:04.080 --> 00:00:09.080
<v 1>What if any research have you done on artificial intelligence and robotics and</v>

3
00:00:10.471 --> 00:00:14.670
autonomous weapons and the future of warfare,

4
00:00:14.671 --> 00:00:17.850
which a lot of people think it's going to be like what we're seeing now in Yemen

5
00:00:17.851 --> 00:00:21.600
with drones that we're going to be seeing that with robots on the ground and

6
00:00:21.601 --> 00:00:22.680
that this will be the future.

7
00:00:22.820 --> 00:00:26.600
<v 0>Huge amount for the book that I wrote called the Pentagon's brain.</v>

8
00:00:28.400 --> 00:00:33.080
Really impactful moment was going to Los Alamos when I went there to meet a

9
00:00:33.081 --> 00:00:37.580
DARPA scientist who was working on an artificial brain for DARPA.

10
00:00:38.060 --> 00:00:43.060
I mean this stuff is way brain trying to create a system,

11
00:00:45.320 --> 00:00:49.160
you know, a free thinking system and what his name was Garrett Kenyon.

12
00:00:49.170 --> 00:00:51.890
What he told me, it was just utterly fascinating because again,

13
00:00:51.891 --> 00:00:54.860
that human thing I'm always after. It's like, what are you doing? I mean,

14
00:00:54.861 --> 00:00:57.140
leave the science. You've had lots of guys on here.

15
00:00:57.141 --> 00:00:58.910
I'll talk to you about the high technology elements,

16
00:00:58.911 --> 00:01:03.020
but I'm interested in who's doing that, who's creating that science and why.

17
00:01:03.320 --> 00:01:06.620
And he said to me, this is like,

18
00:01:07.310 --> 00:01:10.490
like where artificial intelligence is right now with scientists who are really

19
00:01:10.491 --> 00:01:12.980
looking into this. It's like Magellan, you know,

20
00:01:13.370 --> 00:01:18.110
like who will discover the new world? But on the,

21
00:01:18.170 --> 00:01:21.290
on the idea of frightening artificial intelligence,

22
00:01:21.740 --> 00:01:26.120
he told me an interesting story about his daughter and he said, um,

23
00:01:26.180 --> 00:01:27.710
people seem to think like, you know,

24
00:01:27.711 --> 00:01:31.670
facial recognition software is like telling us that we're one step away from AI,

25
00:01:31.671 --> 00:01:35.630
true AI. And he said if, if he's, he was,

26
00:01:35.631 --> 00:01:36.860
he showed me on his iPhone,

27
00:01:36.890 --> 00:01:41.360
this was a couple of years ago and how much trouble the iPhone had recognizing

28
00:01:41.361 --> 00:01:46.130
him. Like if he put a hat on or if he made a funny face and he said,

29
00:01:46.430 --> 00:01:51.430
my daughter can recognize me from across a baseball field.

30
00:01:51.920 --> 00:01:55.730
You know, if I have a hat on, just by the way I walk. Right? And he said,

31
00:01:55.731 --> 00:01:59.120
if she, if she couldn't, there would be something really wrong with her.

32
00:01:59.180 --> 00:02:00.013
In other words,

33
00:02:00.140 --> 00:02:05.140
her human recognition abilities are truly intelligent and that is a system of

34
00:02:09.711 --> 00:02:13.970
systems, a biological system of systems that no scientist has, you know,

35
00:02:14.360 --> 00:02:17.900
the algorithm for which no one has ever been able to figure out yet.

36
00:02:18.410 --> 00:02:20.240
And he believes that we're far away from that.

37
00:02:20.450 --> 00:02:25.100
But the defense department on the other hand is moving us in that direction and

38
00:02:25.250 --> 00:02:28.970
absolutely wants autonomous weapons to be fighting Wars. Look,

39
00:02:28.971 --> 00:02:33.290
there was a program that said, um, I quote this in the book, it says the, um,

40
00:02:33.320 --> 00:02:36.470
the battle place is no place for humans.

41
00:02:39.440 --> 00:02:44.120
So drones are the way of the future, right?

42
00:02:44.121 --> 00:02:46.250
But they're used to kill people, which,

43
00:02:46.520 --> 00:02:50.600
which also means that the enemy is creating drone systems.

44
00:02:50.660 --> 00:02:54.680
And pretty soon that's going to be a big, a big issue. The big fear,

45
00:02:55.250 --> 00:02:57.890
the big fear is that they're going to be the first ones to implement it. I mean,

46
00:02:57.891 --> 00:02:58.880
what scares you about

47
00:02:59.760 --> 00:03:02.160
<v 1>everything? DARPA thinks AI could help troops,</v>

48
00:03:02.161 --> 00:03:05.130
telepathically control much machines. Of course they do.

49
00:03:05.640 --> 00:03:06.451
And then they probably can.

50
00:03:06.451 --> 00:03:09.120
I mean they've already got cursors that people can move around that are

51
00:03:09.121 --> 00:03:12.960
paraplegic, they can move them around with their mind in their eyes. Yeah,

52
00:03:12.990 --> 00:03:16.320
I think there's going to be a quite a few of those things.

53
00:03:16.530 --> 00:03:19.380
What does this [inaudible], and this is called the synapse. This is a,

54
00:03:20.310 --> 00:03:21.600
I'll look read this thing.

55
00:03:21.601 --> 00:03:26.601
That's DOE DARPA funded program develop electronic neuro morphic machine

56
00:03:27.541 --> 00:03:32.010
technology that scales to biological levels more simply stated,

57
00:03:32.280 --> 00:03:36.750
it is an attempt to build a new kind of computer with similar form and function

58
00:03:36.751 --> 00:03:38.580
to the mammalian brain.

59
00:03:39.000 --> 00:03:43.410
Such artificial brains would be used to build robots whose intelligence matches

60
00:03:43.411 --> 00:03:48.030
that of mice and cats. Jesus Christ, robot cats,

61
00:03:48.420 --> 00:03:49.950
robot cats coming to get us.

62
00:03:51.120 --> 00:03:55.680
<v 0>Well, they created something called the robo rat. That was the first,</v>

63
00:03:56.520 --> 00:04:01.500
um, bio hybrid, right? So a bio hybrid is when you mix a animal and a machine.

64
00:04:02.370 --> 00:04:06.060
And DARPA was doing that, um, right before nine 11.

65
00:04:06.120 --> 00:04:08.250
And people freaked out. They were like,

66
00:04:08.251 --> 00:04:13.251
you cannot put brain chips and rats and make them move through a maze by,

67
00:04:14.100 --> 00:04:16.470
uh, you know, remote control, which was what they were doing.

68
00:04:16.890 --> 00:04:20.490
And I interviewed the guys who were all working on this program before nine 11.

69
00:04:21.210 --> 00:04:24.630
And so the, the morality of the,

70
00:04:24.960 --> 00:04:26.730
of the citizenry was like, no.

71
00:04:27.270 --> 00:04:32.270
Then nine 11 happened and suddenly all this money got pumped into DARPA to do

72
00:04:35.580 --> 00:04:37.230
anything they wanted.

73
00:04:37.680 --> 00:04:41.400
The morality issue went out the window and they started creating all kinds of

74
00:04:41.401 --> 00:04:46.370
bio hybrids as I write in the Pentagon's brain. So they put, um, they,

75
00:04:46.371 --> 00:04:50.280
they now have pigeons that are mixed, you know,

76
00:04:50.700 --> 00:04:55.590
animal and machine. They created something called, there's a moth.

77
00:04:55.620 --> 00:04:58.620
So there's a Manduka sex to moth. That's what it's called, is a large moth.

78
00:04:59.100 --> 00:05:04.050
And sine DARPA, scientists put brain chips into the larva. Okay.

79
00:05:04.290 --> 00:05:09.290
So that when it cocooned and became a flying moth,

80
00:05:10.710 --> 00:05:12.210
it had the,

81
00:05:13.410 --> 00:05:15.750
the chip built into its system,

82
00:05:16.050 --> 00:05:21.050
making it easier to integrate and they could fly the moth around the lap.

83
00:05:21.540 --> 00:05:23.430
And that was a huge step.

84
00:05:23.431 --> 00:05:26.610
And this is now four years ago that I was interviewing these scientists.

85
00:05:27.390 --> 00:05:30.630
Did you see any of this stuff? I didn't see them off, but I, I told you,

86
00:05:30.631 --> 00:05:35.250
I saw that the limb regeneration lap was a trip. And this is all sprint.

87
00:05:36.120 --> 00:05:36.301
Well,

88
00:05:36.301 --> 00:05:39.540
they were just cutting limbs off of salamanders and watching the limbs grow

89
00:05:39.541 --> 00:05:44.340
back. Right. And examining that and saying, well, if a salamander can do this,

90
00:05:44.341 --> 00:05:47.880
so can we one day. And I said to them, but wait, that's impossible. You know?

91
00:05:47.881 --> 00:05:52.620
And they said, well it's not actually because humans have, they broke.

92
00:05:52.621 --> 00:05:56.130
I love scientists who break it down into terms I can understand.

93
00:05:56.131 --> 00:06:00.140
It's like what Elon did, you know? Right. Cause and they said to me,

94
00:06:00.920 --> 00:06:05.920
you were once a single cell in your mother's womb and then you were two and then

95
00:06:07.341 --> 00:06:10.490
you were, you know, right. So you can regenerate.

96
00:06:12.140 --> 00:06:13.160
And that's their premise.

97
00:06:13.190 --> 00:06:16.730
I mean these are the tops on the world's top scientist in regeneration.

98
00:06:16.910 --> 00:06:17.743
What is this Jamie?

99
00:06:18.400 --> 00:06:21.670
<v 1>The moth being stimulated by electro occurrence and its abdomen.</v>

100
00:06:22.660 --> 00:06:25.150
So the stimulation of the electro occurrence,

101
00:06:25.151 --> 00:06:28.240
they can cause it to go left or right. Is that it? Yeah,

102
00:06:28.241 --> 00:06:30.100
I was looking up these bio hybrid MOS.

103
00:06:30.250 --> 00:06:31.750
What was the thing that you threw your hands in your head?

104
00:06:32.140 --> 00:06:34.690
You were freaking out. Macaulay Culkin in home alone.

105
00:06:34.720 --> 00:06:37.380
This guy's reading something. He went, Oh yeah. I typed in,

106
00:06:37.630 --> 00:06:39.280
I started typing in bio hybrid stuff,

107
00:06:39.281 --> 00:06:42.790
and this is the first thing that popped up. Was this a shrimp article? Yeah.

108
00:06:42.791 --> 00:06:46.150
It says throw it. They're going to test them through Olympic themed events.

109
00:06:46.510 --> 00:06:47.380
Oh my God. Look at this.

110
00:06:47.500 --> 00:06:52.500
DARPA MTO seeks innovative proposals for the development of micro to Millie

111
00:06:52.690 --> 00:06:57.400
insect scale robotic technologies. Shrimp were developed. We'll develop.

112
00:06:57.430 --> 00:06:59.260
Okay, so shrimp is the

113
00:06:59.830 --> 00:07:02.860
<v 0>an acronym. Probably they, they love an acronym.</v>

114
00:07:04.310 --> 00:07:07.970
<v 1>We'll develop and demonstrate through a series of Olympic themed events,</v>

115
00:07:08.000 --> 00:07:12.800
multifunctional M M to C M scale robotic platforms,

116
00:07:13.010 --> 00:07:17.570
so I guess that's millimeter to centimeter scale robotic platforms with a focus

117
00:07:17.571 --> 00:07:22.520
on untethered mobility, maneuverability, and dexterity to achieve this goal.

118
00:07:22.521 --> 00:07:27.521
Shrimp will also provide foundational research in the area of micro actual

119
00:07:28.310 --> 00:07:33.310
actuator materials and endure energy efficient power systems for extremely swap

120
00:07:36.740 --> 00:07:41.420
capital. Letter S, copy level w, lowercase a,

121
00:07:41.480 --> 00:07:45.980
capital P constrained microbiotic systems. They expect as such,

122
00:07:45.981 --> 00:07:50.660
advances will be enabling for applications including search and rescue. Yeah,

123
00:07:50.661 --> 00:07:53.330
right. Search and rescue. Disaster relief. Yay.

124
00:07:53.331 --> 00:07:56.600
We're going to help people hazardous environment inspection or killing

125
00:07:56.601 --> 00:08:00.140
motherfuckers with a, with an evil nuclear B,

126
00:08:01.490 --> 00:08:05.570
that's all you need is B that goes in your mouth and blows up.

127
00:08:06.230 --> 00:08:08.390
Fuck, that's crazy.

128
00:08:08.880 --> 00:08:12.870
<v 0>I mean they do all kinds of planning for the future,</v>

129
00:08:13.170 --> 00:08:17.700
but the search and rescue thing is a, it's a great sort of, you know,

130
00:08:17.701 --> 00:08:21.240
way in which to present DARPA as doing all this great stuff.

131
00:08:21.241 --> 00:08:24.690
I interviewed DARPA scientists who said, look Andy, we've got,

132
00:08:24.691 --> 00:08:29.691
we're able to send robots into Fukushima to twist the right then.

133
00:08:30.870 --> 00:08:35.430
And yes, that is great, but that's far from the only thing that's been right.

134
00:08:35.670 --> 00:08:39.360
Yeah, well, yeah, but here's a trip you want to hear. I mean it gets,

135
00:08:39.660 --> 00:08:44.660
there are rabbit holes there because I sourced all these documents and also

136
00:08:45.751 --> 00:08:49.230
interviewed generals at the Pentagon who were like, we don't like a AI.

137
00:08:49.260 --> 00:08:54.030
We want like, we want this, we want our guys on the ground. You know, we,

138
00:08:54.090 --> 00:08:57.510
they believe in the, the warrior, that concept.

139
00:08:57.511 --> 00:08:59.480
And so the generals were very opposed with the,

140
00:08:59.850 --> 00:09:04.170
DARPA took a vote and it was like, no AI. We want humans in the mix.

141
00:09:04.620 --> 00:09:08.490
And so what a DARPA start doing, and they owe the generals. They said,

142
00:09:08.491 --> 00:09:13.170
why don't you, why can't we go more autonomous? And the answer was,

143
00:09:13.680 --> 00:09:16.470
we don't trust the machines. Okay?

144
00:09:16.980 --> 00:09:20.100
So right around that same time, what a DARPA start doing,

145
00:09:20.370 --> 00:09:25.370
it started looking into and hiring scientists who were working with how trust

146
00:09:26.131 --> 00:09:30.480
works in the brain, specifically with what is called the moral molecule.

147
00:09:30.510 --> 00:09:34.440
And it's this molecule in the brain that mothers emit when they're

148
00:09:34.441 --> 00:09:38.940
breastfeeding. Okay? Oxytocin. Yes. So think about that.

149
00:09:39.090 --> 00:09:42.000
And I mean, that's like the ultimate going way back biology.

150
00:09:42.001 --> 00:09:46.710
Like you have to have a mother, a trusting mother to breastfeed in,

151
00:09:46.740 --> 00:09:49.380
you know, pre history or otherwise you'd be eaten by, you'd be like,

152
00:09:49.381 --> 00:09:53.010
this is a bad idea. I'm stopping to do this, I'm going to die. Right?

153
00:09:53.400 --> 00:09:56.280
So they examined that molecule,

154
00:09:56.310 --> 00:09:58.590
the brain's moral molecule,

155
00:09:58.950 --> 00:10:02.670
and they began a program to work with that,

156
00:10:02.730 --> 00:10:06.570
to be able to give that to soldiers so that they're,

157
00:10:07.290 --> 00:10:09.420
that they trusted AI machines.

158
00:10:09.510 --> 00:10:13.170
And that's where I think you're getting into a really spooky,

159
00:10:13.200 --> 00:10:18.200
dark multi levels of manipulation about what humans want versus what the

160
00:10:19.381 --> 00:10:20.220
Pentagon wants.

161
00:10:20.530 --> 00:10:21.363
<v 1>Wow.</v>

162
00:10:22.830 --> 00:10:26.950
The worry about trusting the machines scares the shit out of me because that's

163
00:10:26.951 --> 00:10:29.110
what everyone's worried about when it comes to AI.

164
00:10:29.560 --> 00:10:32.320
Like that's what Elon Musk keeps warning people about,

165
00:10:32.530 --> 00:10:36.160
that these things are going to have superhuman capabilities and they're going to

166
00:10:36.161 --> 00:10:37.630
be sentient. And it's a matter of when.

167
00:10:37.840 --> 00:10:41.350
<v 0>Absolutely. So I, I, as the journalist said to myself, well wait a minute.</v>

168
00:10:41.380 --> 00:10:43.900
If the generals at the Pentagon, and I'm, you know, that's a,

169
00:10:43.960 --> 00:10:48.310
that's a euphemism, but the meaning that the actual opera, you know,

170
00:10:48.311 --> 00:10:51.070
the guys that are in charge here don't want that. Who does want this?

171
00:10:51.610 --> 00:10:55.060
And where my research took me to was the group that wants,

172
00:10:55.061 --> 00:10:57.310
that is what's called the defense science board.

173
00:10:57.790 --> 00:11:02.790
And those are the individuals who are counseling the Pentagon in the manner in

174
00:11:03.041 --> 00:11:04.030
which they should proceed.

175
00:11:04.330 --> 00:11:09.330
And now those individuals are all sitting on the boards of the defense

176
00:11:09.701 --> 00:11:10.570
contractors.

177
00:11:11.050 --> 00:11:15.310
So you can really see how money drives the rubric.

178
00:11:15.670 --> 00:11:19.150
The generals don't want it. The humans don't want it. But guess who does?

179
00:11:19.270 --> 00:11:23.980
The people who stand to make the money creating the autonomous systems.

180
00:11:24.430 --> 00:11:29.110
And that's exactly what Eisenhower warned us of in his farewell speech.

181
00:11:29.111 --> 00:11:33.220
You know, the military industrial complex. And the other part of that speech,

182
00:11:33.221 --> 00:11:38.020
which people don't, don't know as well, is that what he said, his antidote,

183
00:11:38.050 --> 00:11:38.980
Eisenhower said this,

184
00:11:38.981 --> 00:11:43.540
the antidote to the military industrial complex is an alert and knowledgeable

185
00:11:43.541 --> 00:11:45.970
citizenry. It's why I write my books,

186
00:11:46.420 --> 00:11:50.470
because an alert and knowledgeable citizenry has the ability to kind of push

187
00:11:50.471 --> 00:11:54.070
back and go, but we don't want that. Well, I think what we're worried

188
00:11:54.070 --> 00:11:58.690
<v 1>about is Pandora's box, right with when it comes to AI. And we're worried that,</v>

189
00:11:58.780 --> 00:12:02.230
first of all, if we're not the ones to open it, what if they open it? All right?

190
00:12:02.231 --> 00:12:06.460
What if the Chinese open it now and obviously their technology is super,

191
00:12:06.461 --> 00:12:09.700
super advanced. I mean their electronic technology particular,

192
00:12:09.701 --> 00:12:12.040
their cell phones are cutting edge. I mean,

193
00:12:12.280 --> 00:12:16.120
Apple and all these other companies are struggling to try to keep up with Walway

194
00:12:16.240 --> 00:12:20.860
in these one FC or a one, um, X. What is the fucking one?

195
00:12:20.861 --> 00:12:24.550
S T what does that one, what does that big company

196
00:12:26.620 --> 00:12:28.540
that uh, they just released some,

197
00:12:28.570 --> 00:12:33.570
they've just hired Robert Downey jr to give him millions of dollars to on one

198
00:12:33.821 --> 00:12:36.460
plus one plus seven. They have this,

199
00:12:36.461 --> 00:12:40.060
a new phone that doesn't have a front facing camera. You press a button,

200
00:12:40.061 --> 00:12:43.420
it slides out of the top. They figured out a way to make it the entire phone,

201
00:12:43.421 --> 00:12:48.040
all screen. And they're incredibly advanced in terms of their electronics. We,

202
00:12:48.041 --> 00:12:53.041
we deeply are concerned that they're going to be the ones that implement

203
00:12:53.680 --> 00:12:55.720
military autonomous,

204
00:12:55.810 --> 00:12:58.750
sentient robotics before we do.

205
00:12:59.500 --> 00:13:01.000
Because then you can essentially,

206
00:13:01.300 --> 00:13:05.350
you can launch them with no physical human cost on your side.

207
00:13:05.800 --> 00:13:07.020
And I mean they, they,

208
00:13:07.330 --> 00:13:11.560
they're literally weapons of mass destruction if you have robots that can go

209
00:13:11.561 --> 00:13:12.700
over there and just kill people.

210
00:13:13.230 --> 00:13:16.860
<v 0>And, and what they need for that is the world's fastest supercomputer. Right.</v>

211
00:13:16.890 --> 00:13:19.500
And what's interesting is that we just, we,

212
00:13:19.501 --> 00:13:22.890
America just overtook the Chinese in having, again,

213
00:13:22.891 --> 00:13:24.660
having the world's fastest super computer,

214
00:13:24.661 --> 00:13:27.990
but they had it for a couple of years and think about this. Okay.

215
00:13:28.170 --> 00:13:31.530
Cause you were saying hard to believe the Nazis were only, you know,

216
00:13:31.620 --> 00:13:32.520
not even like.

