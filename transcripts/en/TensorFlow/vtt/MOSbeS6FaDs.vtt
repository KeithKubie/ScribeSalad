WEBVTT
Kind: captions
Language: en

00:00:02.540 --> 00:00:05.360
NICK KREEGER: How's
it going, everybody?

00:00:05.360 --> 00:00:07.670
I'm here to talk about
TensorFlow and JavaScript

00:00:07.670 --> 00:00:08.430
today.

00:00:08.430 --> 00:00:11.600
My name is Nick, and this
is my colleague Ping.

00:00:11.600 --> 00:00:13.840
And we work on TensorFlow.js
here in Mountain View.

00:00:16.860 --> 00:00:19.490
So the traditional thinking
is machine learning only

00:00:19.490 --> 00:00:20.760
happens in Python, right?

00:00:20.760 --> 00:00:24.650
That's kind of what
everybody thinks about.

00:00:24.650 --> 00:00:28.430
But is that always the case?

00:00:28.430 --> 00:00:29.820
Has anybody seen this before?

00:00:29.820 --> 00:00:34.059
This is something we host on
our TensorFlow documentation.

00:00:34.059 --> 00:00:36.350
This is the machine learning
playground, the TensorFlow

00:00:36.350 --> 00:00:37.430
playground.

00:00:37.430 --> 00:00:41.420
And it was actually built by our
colleagues in the East Coast.

00:00:41.420 --> 00:00:45.140
And it was just a visual to put
into some of our ML classes.

00:00:45.140 --> 00:00:47.750
And it kind of
shows how data flows

00:00:47.750 --> 00:00:49.460
throughout a connected
neural network

00:00:49.460 --> 00:00:51.650
with different
activation functions.

00:00:51.650 --> 00:00:55.010
And this was a really
popular project we built,

00:00:55.010 --> 00:00:57.670
and it was a lot of fun to make.

00:00:57.670 --> 00:00:59.630
And we've gained a lot
of traction from it.

00:00:59.630 --> 00:01:01.670
So we started to think
maybe it makes sense

00:01:01.670 --> 00:01:04.459
to do ML in the browser.

00:01:04.459 --> 00:01:07.105
There's a lot of opportunities
for doing ML directly

00:01:07.105 --> 00:01:07.730
in the browser.

00:01:07.730 --> 00:01:09.570
We don't need any drivers.

00:01:09.570 --> 00:01:11.300
There's no CUDA
installation or anything.

00:01:11.300 --> 00:01:15.630
You could just run your code.

00:01:15.630 --> 00:01:17.820
The browser has a lot
of interactive features.

00:01:17.820 --> 00:01:21.280
Especially with over the last
several years of development,

00:01:21.280 --> 00:01:24.680
there is access to things
like sensors and cameras

00:01:24.680 --> 00:01:29.240
you can easily hook up to
that type of data stream.

00:01:29.240 --> 00:01:31.670
And the other great part
about doing ML directly

00:01:31.670 --> 00:01:34.190
in the browser is it's
a good privacy use case.

00:01:34.190 --> 00:01:37.070
You don't have to send any
user-facing data or any user

00:01:37.070 --> 00:01:39.860
data over the wire, over an RPC.

00:01:39.860 --> 00:01:43.150
To do inference behind the
scene in your infrastructure,

00:01:43.150 --> 00:01:47.250
you could actually just do
that directly on the client.

00:01:47.250 --> 00:01:50.210
So coming back to the
TensorFlow playground,

00:01:50.210 --> 00:01:52.805
this is about 400 lines
of JavaScript code,

00:01:52.805 --> 00:01:57.570
and it was very specifically
typed for this project.

00:01:57.570 --> 00:02:00.470
So our team kind of
took this prototype

00:02:00.470 --> 00:02:02.390
and started to build
a linear algebra

00:02:02.390 --> 00:02:05.730
library for the browser.

00:02:05.730 --> 00:02:08.250
This project was
initially started.

00:02:08.250 --> 00:02:10.860
It was all open source under--

00:02:10.860 --> 00:02:12.430
it was called deeplearn.js.

00:02:12.430 --> 00:02:14.310
And we took deeplearn.js
and aligned it

00:02:14.310 --> 00:02:16.350
with what we're doing
with TensorFlow internally

00:02:16.350 --> 00:02:20.400
with eager execution and that
type of alignment and launched

00:02:20.400 --> 00:02:24.970
TensorFlow.js last
April already.

00:02:24.970 --> 00:02:29.100
And once we launched it, we had
a lot of really great community

00:02:29.100 --> 00:02:30.930
and Google-built products.

00:02:30.930 --> 00:02:32.310
And I want to
highlight a couple.

00:02:32.310 --> 00:02:34.020
This is one that
we built at Google.

00:02:34.020 --> 00:02:35.800
It's called the
Teachable Machine.

00:02:35.800 --> 00:02:38.139
This is all done in the browser.

00:02:38.139 --> 00:02:39.930
There's like three
labels you can give what

00:02:39.930 --> 00:02:41.760
you're training in the webcam.

00:02:41.760 --> 00:02:44.460
There's like a green,
purple, and red.

00:02:44.460 --> 00:02:47.430
And it sort of highlights
how a basic image recognition

00:02:47.430 --> 00:02:49.380
model can run directly
in the browser.

00:02:49.380 --> 00:02:50.880
So this stuff all exists online.

00:02:50.880 --> 00:02:53.640
You can still find it.

00:02:53.640 --> 00:02:56.580
Another community built
a self-driving car

00:02:56.580 --> 00:02:58.282
all in the browser
called Metacar.

00:02:58.282 --> 00:02:58.990
And this is cool.

00:02:58.990 --> 00:03:00.948
You can watch it train,
and learn the inference

00:03:00.948 --> 00:03:04.470
in what the car is driving.

00:03:04.470 --> 00:03:05.540
People built games.

00:03:05.540 --> 00:03:08.640
So this is a web game
that somebody trained with

00:03:08.640 --> 00:03:11.320
TensorFlow.js to avoid--

00:03:11.320 --> 00:03:14.280
it's kind of a funny animation,
but there's a little dude

00:03:14.280 --> 00:03:15.660
running back and forth.

00:03:15.660 --> 00:03:17.610
And he's hiding from
those big balls.

00:03:17.610 --> 00:03:21.995
And the model is learning
to avoid the balls

00:03:21.995 --> 00:03:26.730
all through using TensorFlow.js
and continuing to play.

00:03:26.730 --> 00:03:27.730
This one is really cool.

00:03:27.730 --> 00:03:31.320
This is a Google project called
Magenta, which does a lot of ML

00:03:31.320 --> 00:03:32.700
with audio.

00:03:32.700 --> 00:03:35.640
We have a large library
called Magenta.js,

00:03:35.640 --> 00:03:40.260
which is built on TensorFlow.js
to do in-browser audio.

00:03:40.260 --> 00:03:42.280
This is a cool demo
somebody built.

00:03:42.280 --> 00:03:46.050
It's a digital synthesizer
that learns how to play music

00:03:46.050 --> 00:03:48.960
and can drive with it.

00:03:48.960 --> 00:03:51.840
Another cool example
that just came out is--

00:03:51.840 --> 00:03:53.980
this is all community-built
open source.

00:03:53.980 --> 00:03:56.190
It's called face-api.js.

00:03:56.190 --> 00:03:59.190
So it's a library that sits
on top of TensorFlow.js.

00:03:59.190 --> 00:04:02.220
It has a few different
type of image recognition.

00:04:02.220 --> 00:04:04.210
It can detect faces
and facial features,

00:04:04.210 --> 00:04:08.820
so even, like, toddlers
work pretty well.

00:04:08.820 --> 00:04:11.250
So I want to kind
of showcase how

00:04:11.250 --> 00:04:13.620
our library pieces together.

00:04:13.620 --> 00:04:16.300
There's sort of two main
components to TensorFlow.js.

00:04:16.300 --> 00:04:20.850
There's a Core API
and then a Layers API.

00:04:20.850 --> 00:04:23.520
And that is all powered
in the browser by WebGL.

00:04:23.520 --> 00:04:26.520
That's how we did the linear
algebra aspect for the browser

00:04:26.520 --> 00:04:31.680
is we bootstrap all the linear
algebra all through WebGL

00:04:31.680 --> 00:04:33.210
textures.

00:04:33.210 --> 00:04:36.090
And on the server side, we
actually ship our C code

00:04:36.090 --> 00:04:37.577
that we run Python--

00:04:37.577 --> 00:04:39.410
or I'm sorry, that
powers TensorFlow Python.

00:04:39.410 --> 00:04:42.460
So you get the
high-end CPU, GPU.

00:04:42.460 --> 00:04:45.000
And then eventually, we're
working on the TPU integration

00:04:45.000 --> 00:04:48.150
story for server side.

00:04:48.150 --> 00:04:50.790
And those who have used
Keras, the Layers API

00:04:50.790 --> 00:04:55.410
is almost the same as
Keras, very similar syntax.

00:04:55.410 --> 00:04:57.340
The Core APIs are OP level.

00:04:57.340 --> 00:05:00.870
And anyone who's worked
with TensorFlow SaveModels,

00:05:00.870 --> 00:05:04.360
that API will be pretty similar.

00:05:04.360 --> 00:05:08.850
OK, what can you do
today with TensorFlow.js?

00:05:08.850 --> 00:05:12.000
Well, you can actually just
author small models directly

00:05:12.000 --> 00:05:14.340
in the browser.

00:05:14.340 --> 00:05:16.720
There is a limited amount of
resources the browsers have,

00:05:16.720 --> 00:05:18.678
so we kind of get into
that a little bit later.

00:05:18.678 --> 00:05:23.340
But right now, you can do pure
model training in the browser.

00:05:23.340 --> 00:05:25.140
You can import
pretrained models,

00:05:25.140 --> 00:05:28.410
so this is a model that has been
trained somewhere else, usually

00:05:28.410 --> 00:05:31.170
in the cloud or on
some Python device.

00:05:31.170 --> 00:05:33.630
And we have a tool to
serialize the model

00:05:33.630 --> 00:05:38.790
and then run that inference
in Node or on the browser.

00:05:38.790 --> 00:05:41.670
And we have the ability
to retrain models, so very

00:05:41.670 --> 00:05:43.900
basic transfer learning.

00:05:43.900 --> 00:05:45.390
We can bring in a model.

00:05:45.390 --> 00:05:47.160
Anyone who's seen
TensorFlow for Poets,

00:05:47.160 --> 00:05:48.530
it's a very similar exercise.

00:05:51.120 --> 00:05:53.240
So to get started
with the Core API,

00:05:53.240 --> 00:05:57.930
I want to do just a very simple,
basic fitting a polynomial.

00:05:57.930 --> 00:06:00.780
So this is a scatter
of some data we have.

00:06:00.780 --> 00:06:03.750
And we're going to write
a really simple model

00:06:03.750 --> 00:06:08.400
to try to find the best
fit for this plot of data.

00:06:08.400 --> 00:06:14.200
It's the classic fx equals
ax squared plus bx plus c.

00:06:14.200 --> 00:06:14.700
Excuse me.

00:06:17.300 --> 00:06:20.120
So the first line-- this is
all ES6-style JavaScript,

00:06:20.120 --> 00:06:21.950
for those who are familiar.

00:06:21.950 --> 00:06:23.240
So we're going to import--

00:06:23.240 --> 00:06:26.970
@tensorflow/tfjs is the
name of our package.

00:06:26.970 --> 00:06:29.990
And we namespace it as tf.

00:06:29.990 --> 00:06:31.670
Now, our first
step is to include

00:06:31.670 --> 00:06:34.310
three different
variables-- a, b, and c.

00:06:34.310 --> 00:06:37.890
And we actually
initialize those as 0.1.

00:06:37.890 --> 00:06:41.300
This is going to be passed
into our training sequence.

00:06:41.300 --> 00:06:44.200
The next step to do is
declare our function.

00:06:44.200 --> 00:06:48.400
So this is all using the tfjs
APIs are doing that f of x

00:06:48.400 --> 00:06:54.190
equals ax squared plus b
to the power of x plus c.

00:06:54.190 --> 00:06:56.710
And we have some sugar to
make that a little bit more

00:06:56.710 --> 00:06:59.800
readable using chainable
APIs, which is a very

00:06:59.800 --> 00:07:03.790
common pattern in JavaScript.

00:07:03.790 --> 00:07:06.130
Next step is to declare
a loss function,

00:07:06.130 --> 00:07:09.420
just have a mean squared loss.

00:07:09.420 --> 00:07:12.310
And then we declare
the SGD optimizer

00:07:12.310 --> 00:07:14.710
with a default learning rate
we've declared somewhere

00:07:14.710 --> 00:07:16.159
in this code.

00:07:16.159 --> 00:07:18.200
And then finally we loop
it through our training.

00:07:18.200 --> 00:07:21.640
So EPOCHS we pass through,
and every step, we

00:07:21.640 --> 00:07:25.390
minimize our loss through
the SGD optimizer.

00:07:25.390 --> 00:07:28.030
This is very similar
to eager-style Python,

00:07:28.030 --> 00:07:32.326
for those who have done
that in the Python plane.

00:07:32.326 --> 00:07:33.700
Next thing I want
to highlight is

00:07:33.700 --> 00:07:38.436
the next step up-- that
layers, that Keras-style API.

00:07:38.436 --> 00:07:42.010
And to do so, we've been working
on doing audio recognition

00:07:42.010 --> 00:07:44.310
directly in the browser.

00:07:44.310 --> 00:07:46.850
And I want to highlight just
simply how that kind of works.

00:07:46.850 --> 00:07:50.570
So really simple spoken
commands like up,

00:07:50.570 --> 00:07:56.200
down, left, right can
be run through FFT

00:07:56.200 --> 00:07:57.790
to build a spectrogram.

00:07:57.790 --> 00:07:59.980
So we take audio
in, and we build

00:07:59.980 --> 00:08:04.800
a spectrogram as an image, and
we train our model on that.

00:08:04.800 --> 00:08:09.100
And we can actually build that
convolutional network pretty

00:08:09.100 --> 00:08:12.706
simply with our Layers API.

00:08:12.706 --> 00:08:15.850
And the first step is just the
same as our fitting polynomial.

00:08:15.850 --> 00:08:17.360
We'll include the package tfjs.

00:08:19.870 --> 00:08:21.970
And then we're going to
build a sequential model.

00:08:21.970 --> 00:08:25.420
This is very Keras-style.

00:08:25.420 --> 00:08:27.220
Excuse me.

00:08:27.220 --> 00:08:29.310
Our first step is
to do a conv2d,

00:08:29.310 --> 00:08:32.980
a couple of different
filters, and kernelSize.

00:08:32.980 --> 00:08:34.559
ReLU activation
functions-- again,

00:08:34.559 --> 00:08:39.340
this is very familiar for
those who have used Keras.

00:08:39.340 --> 00:08:41.645
Then we have a pooling layer.

00:08:41.645 --> 00:08:43.270
And then we're going
to go ahead and do

00:08:43.270 --> 00:08:47.960
some more conv2ds, and another
max-pooling level, and so on.

00:08:47.960 --> 00:08:50.960
We repeat as we work
our way down the funnel.

00:08:50.960 --> 00:08:56.260
And finally, we flatten out
our layers, add some dropout,

00:08:56.260 --> 00:09:00.940
add a large dense layer at
the very end, one more dropout

00:09:00.940 --> 00:09:09.620
layer, and then finally, our
softmax for audio labeling.

00:09:09.620 --> 00:09:11.230
And finally, let's
compile the model.

00:09:11.230 --> 00:09:13.450
So this is again,
very similar to Keras.

00:09:13.450 --> 00:09:15.580
We're going to compile
our model that we built.

00:09:15.580 --> 00:09:17.800
We'll know any errors
that we have as the model

00:09:17.800 --> 00:09:19.480
is constructed.

00:09:19.480 --> 00:09:22.180
Give it an optimizer,
and then we

00:09:22.180 --> 00:09:24.640
call model.fit to start
passing in our training data

00:09:24.640 --> 00:09:26.410
with our labels.

00:09:26.410 --> 00:09:30.307
And once the model has trained,
we can save it to disk.

00:09:30.307 --> 00:09:32.390
We have options for saving
directly in the browser

00:09:32.390 --> 00:09:35.250
and on a Node.js to file.

00:09:35.250 --> 00:09:38.540
And then finally, we can use
that model to do prediction.

00:09:38.540 --> 00:09:41.358
So we model.predict, and
we pass in our spectrogram.

00:09:43.990 --> 00:09:47.520
OK, so those were two quick
passes at some of the APIs we

00:09:47.520 --> 00:09:50.690
use-- the higher-level Core
and then the lower-level--

00:09:50.690 --> 00:09:53.360
I'm sorry, the higher-level
Layers API and the lower-level

00:09:53.360 --> 00:09:54.744
Core API.

00:09:54.744 --> 00:09:56.660
But one of the cool parts
of doing the browser

00:09:56.660 --> 00:09:59.810
is we can take in models that
have already been trained today

00:09:59.810 --> 00:10:02.990
and build interactive demos.

00:10:02.990 --> 00:10:06.450
And for that, I want to
showcase a small video.

00:10:06.450 --> 00:10:09.110
This is actually built
by a collaboration

00:10:09.110 --> 00:10:14.900
of the TensorFlow.js team and
a Google internal design firm.

00:10:14.900 --> 00:10:18.590
And we've built this
game for mobile devices.

00:10:18.590 --> 00:10:21.950
And it uses MobileNet to
do an emoji scavenger hunt.

00:10:21.950 --> 00:10:26.799
So in the game, the game
will suggest an emoji.

00:10:26.799 --> 00:10:28.340
And you have to run
around the office

00:10:28.340 --> 00:10:30.350
and find it with your
webcam on your phone.

00:10:30.350 --> 00:10:32.641
And this is all doing inference
that's powered directly

00:10:32.641 --> 00:10:34.190
with the browser.

00:10:34.190 --> 00:10:36.459
For this one, I'll
play a quick video,

00:10:36.459 --> 00:10:38.000
and it will kind of
give you a better

00:10:38.000 --> 00:10:38.730
highlight of what's going on.

00:10:38.730 --> 00:10:39.396
[VIDEO PLAYBACK]

00:10:39.396 --> 00:10:40.450
- This is an umbrella.

00:10:40.450 --> 00:10:41.990
So is this.

00:10:41.990 --> 00:10:44.070
This is a slice of pizza.

00:10:44.070 --> 00:10:45.710
So is this.

00:10:45.710 --> 00:10:48.200
Emojis have become a
language of their own.

00:10:48.200 --> 00:10:50.570
We use them everyday to
communicate in our texts

00:10:50.570 --> 00:10:53.780
and emails, so much so
that it's easy to forget

00:10:53.780 --> 00:10:56.390
about the real-world
objects they're based on,

00:10:56.390 --> 00:10:57.900
which got us thinking.

00:10:57.900 --> 00:11:00.320
Could we create a game
that challenges people

00:11:00.320 --> 00:11:04.070
to find the real-world versions
of the emojis we use everyday?

00:11:04.070 --> 00:11:07.220
Introducing "Emoji
Scavenger Hunt."

00:11:07.220 --> 00:11:10.660
"Emoji Scavenger Hunt"
uses TensorFlow.js.

00:11:10.660 --> 00:11:12.560
Open source meets
machine learning

00:11:12.560 --> 00:11:14.690
meets JavaScript meets fun.

00:11:14.690 --> 00:11:15.620
It works like this--

00:11:19.620 --> 00:11:21.170
we show you an emoji.

00:11:21.170 --> 00:11:24.480
Use your phone's camera to find
it before the clock runs out.

00:11:24.480 --> 00:11:25.700
- Hey, you found pen.

00:11:25.700 --> 00:11:27.980
- Find it in time and you
advance the next emoji.

00:11:32.480 --> 00:11:34.640
While you're searching,
you'll hear our machine

00:11:34.640 --> 00:11:36.500
learning system doing its thing.

00:11:36.500 --> 00:11:37.986
- Do I see a toilet tissue?

00:11:37.986 --> 00:11:39.110
- Hey, do you have a watch?

00:11:39.110 --> 00:11:41.670
- Was that a tub?

00:11:41.670 --> 00:11:43.120
Is that a glove?

00:11:43.120 --> 00:11:44.730
Hey, you found a watch.

00:11:44.730 --> 00:11:47.210
- See if you can find all
the emojis before the timer

00:11:47.210 --> 00:11:47.710
runs out.

00:11:47.710 --> 00:11:50.580
- Do I spy a broom?

00:11:50.580 --> 00:11:52.050
- "Emoji Scavenger Hunt"--

00:11:52.050 --> 00:11:53.850
powered by machine learning.

00:11:53.850 --> 00:11:58.950
Start your search at
g.co/EmojiScavengerHunt.

00:11:58.950 --> 00:11:59.730
- Do I see a URL?

00:12:04.456 --> 00:12:06.329
[END PLAYBACK]

00:12:06.329 --> 00:12:07.120
NICK KREEGER: Cool.

00:12:07.120 --> 00:12:09.275
So that sort of showcases
where someone has already

00:12:09.275 --> 00:12:10.900
done the hard work
of training a model.

00:12:10.900 --> 00:12:12.990
Now, we can build that
great interactive demo.

00:12:17.150 --> 00:12:18.900
OK, so I want to
highlight how we actually

00:12:18.900 --> 00:12:19.983
do that behind the scenes.

00:12:19.983 --> 00:12:23.310
So the first step is taking
that pretrained model.

00:12:23.310 --> 00:12:24.810
This is MobileNet.

00:12:24.810 --> 00:12:28.561
It's been trained under
Python TensorFlow.

00:12:28.561 --> 00:12:30.810
Internally in MobileNet,
those who have used MobileNet

00:12:30.810 --> 00:12:32.910
will know there's an object
detector that you can

00:12:32.910 --> 00:12:35.350
tune for a specific labeling.

00:12:35.350 --> 00:12:37.620
And then we import that
into our JavaScript app--

00:12:37.620 --> 00:12:40.830
the scavenger hunt.

00:12:40.830 --> 00:12:44.680
The first step once the model's
been trained, we save it.

00:12:44.680 --> 00:12:46.830
There is a few different
paths for doing this.

00:12:46.830 --> 00:12:50.800
There's the traditional
TensorFlow saved_model API.

00:12:50.800 --> 00:12:53.520
And we also support
Keras as well.

00:12:53.520 --> 00:12:58.820
So there's a sequential
MobileNet model for Keras.

00:12:58.820 --> 00:13:00.270
Then we have a conversion step.

00:13:00.270 --> 00:13:02.340
So this is a tool
that TensorFlow.js

00:13:02.340 --> 00:13:05.010
ships over Python.

00:13:05.010 --> 00:13:08.160
It's a pip install
tensorflowjs, and you can

00:13:08.160 --> 00:13:11.670
use the tensorflowjs_converter.

00:13:11.670 --> 00:13:13.164
For interacting
with a saved_model,

00:13:13.164 --> 00:13:14.580
there's a couple
different options

00:13:14.580 --> 00:13:18.170
for finding the output
of the inference graph

00:13:18.170 --> 00:13:21.420
and where we want to
serialize our artifacts.

00:13:21.420 --> 00:13:24.720
And then we also support
the Keras-style converter

00:13:24.720 --> 00:13:30.950
as well for HDF5 file format.

00:13:30.950 --> 00:13:33.200
And finally, we would like
to load those artifacts

00:13:33.200 --> 00:13:35.630
into the browser.

00:13:35.630 --> 00:13:38.410
So this is all JavaScript code.

00:13:38.410 --> 00:13:44.560
For that saved_model,
it's tf.loadSavedModel.

00:13:44.560 --> 00:13:46.200
And we have two
different artifacts

00:13:46.200 --> 00:13:47.200
that our script creates.

00:13:47.200 --> 00:13:53.650
There's a weights link and
then a link to the JSON file,

00:13:53.650 --> 00:13:57.650
which describes the
inference graph.

00:13:57.650 --> 00:14:00.200
And again, there's
the Keras-style one.

00:14:00.200 --> 00:14:02.530
Keras actually ships
all-in-one JSON file,

00:14:02.530 --> 00:14:06.880
which has one downside of it
avoids some of the caching

00:14:06.880 --> 00:14:10.840
that we provide
for saved_models.

00:14:10.840 --> 00:14:12.860
What happens in that
model conversion step?

00:14:12.860 --> 00:14:14.020
So the first thing we do--

00:14:14.020 --> 00:14:15.910
especially, like
saved_model has a lot

00:14:15.910 --> 00:14:18.290
of different paths
for the graph.

00:14:18.290 --> 00:14:21.160
There is an inference graph,
which is the one we want.

00:14:21.160 --> 00:14:22.900
There's steps for training.

00:14:22.900 --> 00:14:25.840
And a lot of time, if you're
using the tf.data pipeline,

00:14:25.840 --> 00:14:28.330
there's actually graphs
for all the data ops.

00:14:28.330 --> 00:14:30.600
So we actually pull out
the graph for inference,

00:14:30.600 --> 00:14:35.290
and then collapse ops as needed,
and run some optimization.

00:14:35.290 --> 00:14:37.900
And the one other great
thing we do for a saved_model

00:14:37.900 --> 00:14:41.740
is sharding of weights into 4
megabyte chunks, which cache

00:14:41.740 --> 00:14:43.220
nicely with modern browsers.

00:14:43.220 --> 00:14:47.430
So it's only a one-time fetch
for those larger models.

00:14:47.430 --> 00:14:51.580
And we support about 120 plus
of today's TensorFlow ops

00:14:51.580 --> 00:14:55.910
in that conversion step, and
we're always adding more.

00:14:55.910 --> 00:14:59.017
And again, the TF/Keras
Layers are supported

00:14:59.017 --> 00:15:00.100
with this conversion step.

00:15:04.380 --> 00:15:06.560
I also wanted to
showcase one more demo.

00:15:06.560 --> 00:15:10.500
This is a newer demo that
we've just shipped this summer.

00:15:10.500 --> 00:15:14.150
And it's using PoseNet, which
is a human estimation demo.

00:15:14.150 --> 00:15:15.600
And for this, I'm
going to hand it

00:15:15.600 --> 00:15:17.679
over the Ping, who is
going to highlight this.

00:15:17.679 --> 00:15:18.720
PING YU: All right, guys.

00:15:18.720 --> 00:15:24.150
So PoseNet is another
example of converting

00:15:24.150 --> 00:15:28.540
a Python-trained model and
loading to the browser.

00:15:28.540 --> 00:15:31.890
So on the right side, you
can see a lot of control

00:15:31.890 --> 00:15:34.330
that can fine-tune the model.

00:15:34.330 --> 00:15:37.750
And on the left side is
a live feed of a video.

00:15:37.750 --> 00:15:40.650
So in the video, you can see you
can detect my face features as

00:15:40.650 --> 00:15:45.930
well as my body parts.

00:15:45.930 --> 00:15:51.030
So this is a collaboration
between a Google research team

00:15:51.030 --> 00:15:54.330
as well as external contributor.

00:15:54.330 --> 00:15:58.710
So this model is in
our model repository.

00:15:58.710 --> 00:16:01.620
You can check them out.

00:16:01.620 --> 00:16:03.420
On the left side, you
can see actually it

00:16:03.420 --> 00:16:07.830
has about 15 FPS,
so Frame Per Second.

00:16:07.830 --> 00:16:10.170
You can build some
cool application,

00:16:10.170 --> 00:16:15.660
like build recognizing
motions for sports, et cetera.

00:16:15.660 --> 00:16:19.430
We also have other models
in that repository,

00:16:19.430 --> 00:16:23.850
so like audio command model
that Nick mentioned earlier.

00:16:23.850 --> 00:16:27.840
And also we're adding some
other, like object detection

00:16:27.840 --> 00:16:28.710
model.

00:16:28.710 --> 00:16:33.060
So all of that is available
for you to use in the browser.

00:16:33.060 --> 00:16:35.790
Just go ahead, check
them out, and let us know

00:16:35.790 --> 00:16:37.140
if you build any cool apps.

00:16:37.140 --> 00:16:37.640
Thanks.

00:16:42.350 --> 00:16:44.780
NICK KREEGER: And the great
part about that is it's

00:16:44.780 --> 00:16:48.004
feeding directly off of the
camera feed in real time.

00:16:48.004 --> 00:16:49.670
And we're doing about
15 frames a second

00:16:49.670 --> 00:16:56.030
and presenting over the
USB-C, so it does pretty well.

00:16:56.030 --> 00:16:58.640
OK, so I did mention earlier
about training directly

00:16:58.640 --> 00:16:59.310
in the browser.

00:16:59.310 --> 00:17:02.560
This is the retraining--
the transfer learning step.

00:17:02.560 --> 00:17:04.220
And for this, we have
another cool demo

00:17:04.220 --> 00:17:06.050
that we want to showcase.

00:17:06.050 --> 00:17:07.910
Again, we're using MobileNet.

00:17:07.910 --> 00:17:11.930
And Ping is going to pull this
demo up while I'm talking.

00:17:11.930 --> 00:17:15.619
So we built this demo where
we have a baseline MobileNet

00:17:15.619 --> 00:17:18.150
model that we've
loaded in the browser.

00:17:18.150 --> 00:17:22.579
And we're going to train
Ping's face to play Pac-Man.

00:17:22.579 --> 00:17:24.560
So he's going to start
collecting samples

00:17:24.560 --> 00:17:27.750
from the webcam of what
his up, down, left is.

00:17:27.750 --> 00:17:29.850
And so for this, he's
going to use his face.

00:17:29.850 --> 00:17:32.450
So as he's moving
his face around,

00:17:32.450 --> 00:17:33.860
he's collecting
different samples

00:17:33.860 --> 00:17:35.984
that we're going to pass
into that retraining step.

00:17:35.984 --> 00:17:39.030
So there's an up,
down, left, and right.

00:17:39.030 --> 00:17:41.120
And with this demo,
as you hold down,

00:17:41.120 --> 00:17:44.030
we're collecting
more and more frames.

00:17:44.030 --> 00:17:46.340
He's getting close, OK.

00:17:46.340 --> 00:17:48.680
And then now that he's
collected his frames,

00:17:48.680 --> 00:17:50.690
he's going to click
the Train Model button.

00:17:50.690 --> 00:17:53.390
And we'll watch our last
shoot straight down.

00:17:53.390 --> 00:17:55.435
It only takes a couple seconds.

00:17:55.435 --> 00:17:57.380
And now, he's ready
to play Pac-Man.

00:17:57.380 --> 00:17:59.165
So go ahead and hit
Play there, Ping.

00:17:59.165 --> 00:18:01.780
PING YU: All right, let's go.

00:18:01.780 --> 00:18:03.390
NICK KREEGER: All right.

00:18:03.390 --> 00:18:05.220
There you go.

00:18:05.220 --> 00:18:08.750
So the model is running
directly in the browser.

00:18:08.750 --> 00:18:11.540
We've retrained it to
those pictures of his face.

00:18:11.540 --> 00:18:14.540
And the controls are lighting
up left, down, right based

00:18:14.540 --> 00:18:15.740
on what the model is doing.

00:18:15.740 --> 00:18:16.615
PING YU: Aw, come on.

00:18:19.440 --> 00:18:22.410
NICK KREEGER: All right.

00:18:22.410 --> 00:18:24.780
So this is a great
use case of what

00:18:24.780 --> 00:18:28.130
you could do with
taking advantage

00:18:28.130 --> 00:18:31.320
of some of the stuff the browser
provides in doing accessibility

00:18:31.320 --> 00:18:34.530
for machine learning
and building cool apps.

00:18:34.530 --> 00:18:37.180
OK, Ping, we can
play Pac-Man all day.

00:18:37.180 --> 00:18:38.100
PING YU: I got this.

00:18:38.100 --> 00:18:39.475
NICK KREEGER:
These demos are all

00:18:39.475 --> 00:18:41.850
available on our site, which
we'll showcase at the end.

00:18:41.850 --> 00:18:43.950
You can actually
just run this today.

00:18:43.950 --> 00:18:46.660
No drivers, no
anything to install.

00:18:46.660 --> 00:18:47.160
OK.

00:18:47.160 --> 00:18:47.910
PING YU: Let's go.

00:18:47.910 --> 00:18:49.290
NICK KREEGER: Cool.

00:18:49.290 --> 00:18:52.590
All right, so I've showed
off a bunch of demos

00:18:52.590 --> 00:18:55.860
using our Core API,
using that Layers API,

00:18:55.860 --> 00:18:58.170
bringing in pretrained
models and doing

00:18:58.170 --> 00:19:01.180
some basic retraining.

00:19:01.180 --> 00:19:03.060
So where does performance
kind of step up--

00:19:03.060 --> 00:19:06.180
stand for TensorFlow.js
for our browser runtime,

00:19:06.180 --> 00:19:08.700
that WebGL-powered runtime?

00:19:08.700 --> 00:19:11.550
So this is some
benchmarks we've done

00:19:11.550 --> 00:19:14.320
using Python and MobileNet.

00:19:14.320 --> 00:19:17.790
So there's two computers that
we use for these benchmarks.

00:19:17.790 --> 00:19:23.190
The top one is a high-end
workstation with a 1080

00:19:23.190 --> 00:19:26.500
GTX, the high-end NVIDIA card.

00:19:26.500 --> 00:19:28.950
So it's super fast, a
little under 3 milliseconds

00:19:28.950 --> 00:19:30.810
for our inference time.

00:19:30.810 --> 00:19:36.630
And then we used a 13-inch
MacBook with a nonintegrated

00:19:36.630 --> 00:19:38.070
graphics card--

00:19:38.070 --> 00:19:41.070
or with a integrated graphics
card, not a standalone graphics

00:19:41.070 --> 00:19:42.150
card.

00:19:42.150 --> 00:19:45.090
And that was using the
CPU build so there's no--

00:19:45.090 --> 00:19:47.630
it's just the default
AVX instruction

00:19:47.630 --> 00:19:50.160
step we ship with TensorFlow.

00:19:50.160 --> 00:19:52.320
And we doing a little
under 60 milliseconds

00:19:52.320 --> 00:19:54.630
for inference time.

00:19:54.630 --> 00:19:59.250
So where does the TensorFlow.js
benchmark stand up?

00:19:59.250 --> 00:20:00.810
Well, it kind of depends.

00:20:00.810 --> 00:20:06.020
On that super beefy 1080
card, we're really close--

00:20:06.020 --> 00:20:09.660
about 11 milliseconds
per inference time.

00:20:09.660 --> 00:20:12.340
The CPUs they're running on this
laptop is a little bit slower.

00:20:12.340 --> 00:20:14.210
It's a little under
100 milliseconds

00:20:14.210 --> 00:20:16.440
per inference time, but that
was still giving us that

00:20:16.440 --> 00:20:19.650
15 to 20 frames per second,
which allows you to still build

00:20:19.650 --> 00:20:22.570
interactive demos.

00:20:22.570 --> 00:20:25.450
So this discussion leads
us to our next part,

00:20:25.450 --> 00:20:28.171
which is where does
TensorFlow.js on server-side

00:20:28.171 --> 00:20:28.670
come into?

00:20:31.456 --> 00:20:33.330
We think there's a lot
of great opportunities

00:20:33.330 --> 00:20:39.210
for going with JavaScript ML on
the server-side under Node.js.

00:20:39.210 --> 00:20:43.000
The ecosystem for Node
packages is really awesome.

00:20:43.000 --> 00:20:45.780
There's tons of
prebuilt libraries

00:20:45.780 --> 00:20:46.930
off the shelf for npm.

00:20:46.930 --> 00:20:49.010
You can build applications
really quickly

00:20:49.010 --> 00:20:53.190
and distribute them on all
these different cloud services.

00:20:53.190 --> 00:20:57.450
The default runtime for
Node.js V8 is super fast.

00:20:57.450 --> 00:20:59.310
It's had tons of
resources put into it

00:20:59.310 --> 00:21:01.360
by companies like Google.

00:21:01.360 --> 00:21:05.550
And we've seen benchmarks where
the JS interpreter in Node

00:21:05.550 --> 00:21:07.350
is 10 times faster
than the Python.

00:21:10.880 --> 00:21:14.030
By enabling TensorFlow
with Node.js,

00:21:14.030 --> 00:21:16.490
we actually get access to
that high-end hardware--

00:21:16.490 --> 00:21:21.030
so those Cloud TPUs,
the GPU, and so on.

00:21:21.030 --> 00:21:22.970
So those are all
exciting things.

00:21:22.970 --> 00:21:25.910
I wanted to showcase
one real simple use

00:21:25.910 --> 00:21:30.050
case of Node.js and TensorFlow.

00:21:30.050 --> 00:21:32.210
The code snippet I have
up here on the screen

00:21:32.210 --> 00:21:33.980
is actually a really
simple Express app.

00:21:33.980 --> 00:21:38.580
If anyone's used it, it's just
a request response handler.

00:21:38.580 --> 00:21:41.409
And we just handle
the endpoint /model,

00:21:41.409 --> 00:21:43.700
which has a request and a
response that we'll write out

00:21:43.700 --> 00:21:44.870
to.

00:21:44.870 --> 00:21:47.330
So this model,
right now actually,

00:21:47.330 --> 00:21:48.950
we have a model
that we've defined.

00:21:48.950 --> 00:21:51.158
And we're going to do some
prediction on input that's

00:21:51.158 --> 00:21:53.570
been passed into this endpoint.

00:21:53.570 --> 00:21:56.570
Now to turn on
TensorFlow.js with Node,

00:21:56.570 --> 00:21:58.430
it's one line of code.

00:21:58.430 --> 00:22:00.600
It's just importing the binding.

00:22:00.600 --> 00:22:02.720
So this is a binding
we ship over npm.

00:22:02.720 --> 00:22:04.880
And it gives you
the high-end power

00:22:04.880 --> 00:22:08.640
of TensorFlow C library, all
executed under the Node.js

00:22:08.640 --> 00:22:11.300
runtime.

00:22:11.300 --> 00:22:13.410
And what can you do
today with server-side?

00:22:13.410 --> 00:22:16.714
So all those demos
we showed of running

00:22:16.714 --> 00:22:18.380
the model in the
browser, those actually

00:22:18.380 --> 00:22:19.934
just run under Node as well.

00:22:19.934 --> 00:22:21.350
You can use our
conversion script.

00:22:23.960 --> 00:22:26.930
We ship the three
major platforms--

00:22:26.930 --> 00:22:29.720
MacOS, Linux, and Windows CPU.

00:22:29.720 --> 00:22:33.920
And we also have GPU and
CUDA for Linux and Windows.

00:22:33.920 --> 00:22:38.210
We just launched
Windows late last week.

00:22:38.210 --> 00:22:40.080
And all of the full
library support--

00:22:40.080 --> 00:22:43.334
so the Layers API and our
Core API all work today right

00:22:43.334 --> 00:22:44.500
out of the box with Node.js.

00:22:47.055 --> 00:22:51.040
And to kind of highlight how we
can bring all these components

00:22:51.040 --> 00:22:55.370
of npm and TensorFlow.js
and Node.js together,

00:22:55.370 --> 00:22:59.500
we built a little
interactive demo.

00:22:59.500 --> 00:23:03.400
So I know not everybody is
super familiar with baseball,

00:23:03.400 --> 00:23:05.590
but Major League
Baseball Advanced Media

00:23:05.590 --> 00:23:09.260
has this huge dataset where
they record using sensors

00:23:09.260 --> 00:23:12.520
that all the stadiums, the
different types of pitches

00:23:12.520 --> 00:23:14.680
that players throw at games.

00:23:14.680 --> 00:23:17.050
So there's pitches
that are really

00:23:17.050 --> 00:23:19.570
fast that have a high
velocity and low movement.

00:23:19.570 --> 00:23:21.880
And then there are pitches
who are a little slower that

00:23:21.880 --> 00:23:22.810
have more movement.

00:23:22.810 --> 00:23:27.190
So we curated this
dataset and built a model

00:23:27.190 --> 00:23:29.920
all in TensorFlow.js that
trains against this data

00:23:29.920 --> 00:23:31.420
and detects, I
think, seven or eight

00:23:31.420 --> 00:23:33.570
different types of pitches.

00:23:33.570 --> 00:23:37.510
And it renders it
through a socket.

00:23:37.510 --> 00:23:42.500
So don't get too hung up on
the intricacies of baseball.

00:23:42.500 --> 00:23:44.560
This is just really solving
a bread-and-butter ML

00:23:44.560 --> 00:23:47.730
problem of taking sensor
data and drawing up

00:23:47.730 --> 00:23:49.810
a classification.

00:23:49.810 --> 00:23:53.671
So for this, I'll have
Ping run through the demo.

00:23:53.671 --> 00:23:54.170
PING YU: OK.

00:24:00.480 --> 00:24:03.610
All right, so for
web developers,

00:24:03.610 --> 00:24:05.410
you could really
use TensorFlow.js

00:24:05.410 --> 00:24:09.800
to build a full-stack
kind of ML application.

00:24:09.800 --> 00:24:15.500
So on the left side is a
browser that I started a client

00:24:15.500 --> 00:24:17.440
on the browsing side
of the browser, which

00:24:17.440 --> 00:24:20.470
is trying to connect to the
server through socket.io.

00:24:20.470 --> 00:24:22.560
On the right side,
I have my console.

00:24:22.560 --> 00:24:26.470
I'm going to start
my Node.js server.

00:24:26.470 --> 00:24:31.540
Immediately, you see that it's
binded to our TensorFlow CPU

00:24:31.540 --> 00:24:32.560
runtime.

00:24:32.560 --> 00:24:36.640
And as it goes, the
model is getting trained.

00:24:36.640 --> 00:24:41.670
And the training stats are
fed back to the client-side.

00:24:41.670 --> 00:24:45.640
As the training progresses, you
can see the accuracy increase

00:24:45.640 --> 00:24:48.030
for all labels.

00:24:48.030 --> 00:24:52.740
The curve ball has about
90% accuracy right now.

00:24:52.740 --> 00:24:58.300
With server-side implementation,
it's easy to feed new data,

00:24:58.300 --> 00:24:59.560
not like inside a browser.

00:24:59.560 --> 00:25:00.880
It's much harder.

00:25:00.880 --> 00:25:02.800
Let me try and
click this button.

00:25:02.800 --> 00:25:07.660
What this will do is I
will load live MLB pitch

00:25:07.660 --> 00:25:09.880
data into this application.

00:25:09.880 --> 00:25:13.630
And we will try to
run inference on that.

00:25:13.630 --> 00:25:16.240
So let me click on that.

00:25:16.240 --> 00:25:19.600
So immediately, you
can see the orange bar

00:25:19.600 --> 00:25:24.040
is the prediction accuracy
for all of these labels.

00:25:24.040 --> 00:25:27.270
Some of them we actually did
better with the live data.

00:25:27.270 --> 00:25:29.770
It's 90% for changeup.

00:25:29.770 --> 00:25:32.680
Something we did a little
bit less accurate--

00:25:32.680 --> 00:25:35.500
fastball 2-seam is only 68%.

00:25:35.500 --> 00:25:38.140
So overall, I think
it should demonstrate

00:25:38.140 --> 00:25:40.210
that the model actually
generalized pretty

00:25:40.210 --> 00:25:43.000
well for the live data as well.

00:25:43.000 --> 00:25:44.903
So, yeah, back to you.

00:25:44.903 --> 00:25:46.301
Cool.

00:25:46.301 --> 00:25:47.300
NICK KREEGER: All right.

00:25:47.300 --> 00:25:51.080
I'm going to actually kill that
demo or my laptop will die.

00:25:51.080 --> 00:25:51.580
Great.

00:25:56.015 --> 00:25:58.930
So just highlighting exactly
what is going on there,

00:25:58.930 --> 00:26:02.440
there was the Node.js server
which was doing our training.

00:26:02.440 --> 00:26:06.100
There was a training
dataset and an eval dataset,

00:26:06.100 --> 00:26:08.740
which we were reporting
back over socket.io

00:26:08.740 --> 00:26:12.010
how good we are at each
class through our evaluation.

00:26:12.010 --> 00:26:14.710
And then we had the ability
to just easily reach out

00:26:14.710 --> 00:26:17.477
to MLB Advanced
Media through Node

00:26:17.477 --> 00:26:19.060
and parse through
their data, and then

00:26:19.060 --> 00:26:21.580
send in that to the model,
which was the orange prediction.

00:26:21.580 --> 00:26:25.762
So kind of a cool use case
of training my model, how

00:26:25.762 --> 00:26:27.220
does it stack up
to real-world data

00:26:27.220 --> 00:26:29.050
and doing like a
quick visualization.

00:26:29.050 --> 00:26:32.432
And that was all plain
JavaScript, plain HTML.

00:26:32.432 --> 00:26:34.390
And all the source code
we've shown you today--

00:26:34.390 --> 00:26:37.040
all the examples we've shown
you today are open source.

00:26:37.040 --> 00:26:41.080
And we'll link to
them at the end here.

00:26:41.080 --> 00:26:42.460
So performance--
so I highlighted

00:26:42.460 --> 00:26:44.950
where the WebGL runtime
kind of stacked up

00:26:44.950 --> 00:26:47.870
with that Python benchmark.

00:26:47.870 --> 00:26:50.620
So let's step in and
look at Python benchmark

00:26:50.620 --> 00:26:52.570
against the Node.js runtime.

00:26:52.570 --> 00:26:54.760
So again, these are
those initial benchmarks

00:26:54.760 --> 00:26:56.800
that I highlighted.

00:26:56.800 --> 00:27:00.670
The Node runtime itself is just
as fast as the Python runtime

00:27:00.670 --> 00:27:02.650
for inference of MobileNet.

00:27:02.650 --> 00:27:06.580
This is because we're using the
same library that Python uses,

00:27:06.580 --> 00:27:09.794
and there's not
much code to get to,

00:27:09.794 --> 00:27:11.710
and then we're running
all that high-end code.

00:27:14.430 --> 00:27:16.050
OK, I've highlighted
a lot of stuff

00:27:16.050 --> 00:27:20.820
we've built since basically
this year and launched in April.

00:27:20.820 --> 00:27:23.910
The Node stuff has been
out since the end of May.

00:27:23.910 --> 00:27:24.910
So what's next?

00:27:24.910 --> 00:27:26.670
What's the direction
that TensorFlow.js

00:27:26.670 --> 00:27:29.310
is looking to go in?

00:27:29.310 --> 00:27:31.480
We have some high-level
bets that we're doing.

00:27:31.480 --> 00:27:33.180
There's a project
that's going on

00:27:33.180 --> 00:27:35.730
that we're going to release
here very soon in the next month

00:27:35.730 --> 00:27:37.420
or so.

00:27:37.420 --> 00:27:39.060
It's our visualization library.

00:27:39.060 --> 00:27:41.850
So it's the ability to
pull in through the browser

00:27:41.850 --> 00:27:45.870
and do quick visualizations of
your model in the data we have.

00:27:45.870 --> 00:27:49.510
So look for that coming soon.

00:27:49.510 --> 00:27:54.460
We also have a full data API,
so very similar to the tf.data.

00:27:54.460 --> 00:27:56.520
It'll be browser
and node-specific,

00:27:56.520 --> 00:27:58.830
so there will be
convenience functions

00:27:58.830 --> 00:28:01.680
for I just want to read
data off of my webcam

00:28:01.680 --> 00:28:03.810
and not convert it to tensors.

00:28:03.810 --> 00:28:06.180
This API will
provide that for you.

00:28:06.180 --> 00:28:11.490
And on the server-side, it will
be giving highly-optimized data

00:28:11.490 --> 00:28:16.020
pipelines for doing
Node.js training.

00:28:16.020 --> 00:28:18.450
And so those are our
two high-level things.

00:28:18.450 --> 00:28:20.280
Those are the big
projects that kind of

00:28:20.280 --> 00:28:22.920
cross both of our runtimes.

00:28:22.920 --> 00:28:25.960
Looking forward for the browser,
we're working on performance.

00:28:25.960 --> 00:28:28.140
So those benchmarks that
I showed with WebGL,

00:28:28.140 --> 00:28:31.830
a lot of them are the
bottlenecks or limitations

00:28:31.830 --> 00:28:33.640
for WebGL.

00:28:33.640 --> 00:28:38.280
So we use 2D textures that
render the tensor data.

00:28:38.280 --> 00:28:41.220
There's some bottlenecks for
downloading those textures,

00:28:41.220 --> 00:28:42.860
reusing those textures.

00:28:42.860 --> 00:28:44.790
So we're working on
WebGL optimization.

00:28:47.460 --> 00:28:49.770
We're also adding
more and more ops.

00:28:49.770 --> 00:28:53.560
Lately, the focus has been
audio and text-based models.

00:28:53.560 --> 00:28:55.960
So we're adding a lot more
ops to help with that.

00:28:55.960 --> 00:29:00.420
We have a great stable library
of image recognition ops

00:29:00.420 --> 00:29:03.970
and the audio stuff is coming.

00:29:03.970 --> 00:29:05.620
And the other thing
we're looking at

00:29:05.620 --> 00:29:07.190
is helping push that spec.

00:29:07.190 --> 00:29:09.850
So the WebGL runtime
was really interesting,

00:29:09.850 --> 00:29:12.920
and it kind of helped
bootstrap ML in the browser.

00:29:12.920 --> 00:29:15.666
But WebGL isn't the
best use case for this.

00:29:15.666 --> 00:29:17.540
And we're looking at a
few different options.

00:29:17.540 --> 00:29:20.680
One is Compute Shaders,
which is much more

00:29:20.680 --> 00:29:22.720
similar to a
CUDA-like, where I can

00:29:22.720 --> 00:29:24.460
allocate the right
amount of GPU memory

00:29:24.460 --> 00:29:26.840
I need to use and do that.

00:29:26.840 --> 00:29:30.830
And we're also following
closely the WebGPU spec.

00:29:30.830 --> 00:29:34.300
So there's a bunch of different
offerings from Chrome,

00:29:34.300 --> 00:29:36.610
and Internet Explorer,
and the browser

00:29:36.610 --> 00:29:38.950
vendors for what we want to do.

00:29:38.950 --> 00:29:42.380
We're sort of helping
watch that space

00:29:42.380 --> 00:29:43.750
and provide guidance as needed.

00:29:46.700 --> 00:29:49.390
And on the Node.js
side, cloud integration

00:29:49.390 --> 00:29:51.040
is a thing we're looking at.

00:29:51.040 --> 00:29:55.600
This includes the
serverless-type integration

00:29:55.600 --> 00:30:00.640
points, integration with
our TPUs, and so on.

00:30:00.640 --> 00:30:03.790
We're actually
working on generating

00:30:03.790 --> 00:30:08.730
op code to provide all the
core TensorFlow ops in Node.js.

00:30:08.730 --> 00:30:12.220
The Python version of
TensorFlow, most of the code

00:30:12.220 --> 00:30:15.290
is actually generated from
our op registry internally,

00:30:15.290 --> 00:30:18.070
so we're writing that for
TypeScript for JavaScript users

00:30:18.070 --> 00:30:20.620
too.

00:30:20.620 --> 00:30:23.700
And we're providing a better
async support with libuv.

00:30:23.700 --> 00:30:26.470
So libuv is the
underpinning in Node.js

00:30:26.470 --> 00:30:28.960
for asynchronous programming.

00:30:28.960 --> 00:30:31.750
We're working better to make
that scheduler work much nicer,

00:30:31.750 --> 00:30:36.580
so we're not blocking as much
main application threads.

00:30:36.580 --> 00:30:38.800
OK, wrapping up-- we've
shown you a lot of stuff.

00:30:38.800 --> 00:30:43.080
I kind of want to step back and
highlight a couple of things.

00:30:43.080 --> 00:30:44.240
First one is our Core API.

00:30:44.240 --> 00:30:47.690
That's the bread and butter
of the TensorFlow.js suite.

00:30:47.690 --> 00:30:49.905
It's our op library.

00:30:49.905 --> 00:30:51.650
It allows you to
interact with tensors.

00:30:54.600 --> 00:30:56.390
And we also have our
Layers API, which

00:30:56.390 --> 00:30:59.600
is our Keras-style
API for training.

00:31:02.690 --> 00:31:05.410
And we also support SavedModel
and Keras model conversion

00:31:05.410 --> 00:31:06.910
today through our
converter script.

00:31:10.030 --> 00:31:13.780
And the newest runtime
we have is Node.js.

00:31:13.780 --> 00:31:16.910
We've just got done talking
about a bunch of that.

00:31:16.910 --> 00:31:20.530
And with that, I want to
thank you guys for attending.

00:31:20.530 --> 00:31:24.070
Everything I've shown you
is on js.tensorflow.org.

00:31:24.070 --> 00:31:26.470
We have quite a bit
of stuff up there.

00:31:26.470 --> 00:31:28.330
There's all those
demos that I showed.

00:31:28.330 --> 00:31:31.240
They are linked in that page,
so you can find them as well as

00:31:31.240 --> 00:31:32.620
the source code.

00:31:32.620 --> 00:31:35.440
We also have a variety
of GitHub repos.

00:31:35.440 --> 00:31:36.736
Everything we do is on GitHub.

00:31:39.580 --> 00:31:41.740
tensorflow/tfjs is our root one.

00:31:41.740 --> 00:31:43.180
That's our union package.

00:31:43.180 --> 00:31:46.700
We keep track of all of
our GitHub issues there.

00:31:46.700 --> 00:31:49.510
It also links out to a
variety of things we have now.

00:31:49.510 --> 00:31:52.090
We have an examples
repository, which

00:31:52.090 --> 00:31:56.350
has maybe 10 to 15
examples you can just run.

00:31:59.260 --> 00:32:01.790
There's also a link
to our models view.

00:32:01.790 --> 00:32:06.040
So this is models that we've
pretrained, packaged up

00:32:06.040 --> 00:32:09.430
for JavaScript use,
and published over npm.

00:32:09.430 --> 00:32:11.810
A lot of them actually
have wrapper APIs,

00:32:11.810 --> 00:32:14.200
where you don't even have
to take data and convert it

00:32:14.200 --> 00:32:17.240
to tensors and then pass
it in for inference.

00:32:17.240 --> 00:32:21.700
It just says here's
an image HTML canvas.

00:32:21.700 --> 00:32:23.320
Can you do a prediction?

00:32:23.320 --> 00:32:24.430
So those are really cool.

00:32:24.430 --> 00:32:27.190
All that stuff is
linked on tfjs.

00:32:27.190 --> 00:32:30.740
We also have a gallery too
of community-built stuff,

00:32:30.740 --> 00:32:33.989
and it's always growing.

00:32:33.989 --> 00:32:36.030
This is our community
mailing list you could also

00:32:36.030 --> 00:32:37.180
find on our website.

00:32:37.180 --> 00:32:40.500
There's a lot of good discussion
of for how do I do X, Y, and Z,

00:32:40.500 --> 00:32:41.550
or I need this feature.

00:32:41.550 --> 00:32:44.724
Can you please help?

00:32:44.724 --> 00:32:46.140
The gallery repo
I just mentioned,

00:32:46.140 --> 00:32:52.440
that's where all of our
community-built examples live

00:32:52.440 --> 00:32:53.910
and models repo.

00:32:53.910 --> 00:32:57.210
And that's all.

00:32:57.210 --> 00:32:59.960
[APPLAUSE]

