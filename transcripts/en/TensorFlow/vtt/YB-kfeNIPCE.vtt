WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.162
♪ (music) ♪

00:00:05.912 --> 00:00:08.194
(applause)

00:00:12.914 --> 00:00:15.686
Hi, everyone. Thanks for coming today.

00:00:15.686 --> 00:00:17.946
- My name is Daniel.
- My name is Nikhil.

00:00:17.946 --> 00:00:19.860
We're from the Google Brain team,

00:00:19.860 --> 00:00:23.518
and today we're delighted
to talk about JavaScript.

00:00:24.567 --> 00:00:27.266
So, Python has been
one of the mainstream languages

00:00:27.266 --> 00:00:29.457
for scientific computing,

00:00:29.457 --> 00:00:31.354
and it's been like that for a while.

00:00:31.354 --> 00:00:34.659
And there's a lot of tools
and libraries around Python.

00:00:35.749 --> 00:00:37.476
But is that where it ends?

00:00:38.026 --> 00:00:39.969
We're here today to talk--

00:00:40.719 --> 00:00:43.568
to convince you that JavaScript
and the browser

00:00:43.568 --> 00:00:45.456
have a lot to offer.

00:00:45.456 --> 00:00:48.676
And TensorFlow Playground
is a great example of that.

00:00:49.246 --> 00:00:53.032
I'm curious-- how many people
have seen TensorFlow Playground before?

00:00:54.142 --> 00:00:55.827
Oh, wow! Quite a few.

00:00:55.827 --> 00:00:57.259
I'm very glad.

00:00:57.259 --> 00:00:59.616
So, for those of you that haven't seen it,

00:00:59.616 --> 00:01:04.236
you can check it out after our talk
at <i>playground.tensorflow.org.</i>

00:01:04.236 --> 00:01:08.622
It is an in-browser visualization
of a small neural network,

00:01:08.622 --> 00:01:10.379
and it shows, in real time,

00:01:10.379 --> 00:01:13.126
all the internals
of the network as it's training.

00:01:13.766 --> 00:01:15.769
And this was a lot of fun to make.

00:01:15.769 --> 00:01:18.633
It had a huge, educational success.

00:01:18.633 --> 00:01:21.615
We've been getting emails
from high schools and universities

00:01:21.615 --> 00:01:24.995
that have been using this
to teach students about machine learning.

00:01:25.695 --> 00:01:27.616
After we launched Playground,

00:01:27.616 --> 00:01:30.664
we were wondering,
"Why was it so successful?"

00:01:30.664 --> 00:01:35.586
And we think one big reason
was because it was in the browser.

00:01:35.586 --> 00:01:40.276
And the browser is this unique platform
where the things you build,

00:01:40.276 --> 00:01:43.326
you can share
with anyone with just a link.

00:01:43.326 --> 00:01:47.348
And those people that open your app
don't have to install any drivers

00:01:47.348 --> 00:01:49.473
or any software,

00:01:49.473 --> 00:01:50.721
it just works.

00:01:50.721 --> 00:01:54.797
Another thing is the browser
is highly interactive,

00:01:54.797 --> 00:01:59.403
and so the user is going to be engaged
with whatever you're building.

00:01:59.873 --> 00:02:02.220
Another big thing is that browsers--

00:02:02.220 --> 00:02:04.212
we didn't take advantage
of this in the Playground,

00:02:04.212 --> 00:02:06.716
but browsers have access to sensors,

00:02:06.716 --> 00:02:10.788
like the microphone and the camera
and the accelerometer.

00:02:10.788 --> 00:02:14.575
All of these sensors
are behind standardized APIs

00:02:14.575 --> 00:02:16.354
that work on all browsers.

00:02:16.834 --> 00:02:19.356
And the last thing,
the most important thing,

00:02:19.356 --> 00:02:24.375
is the data that comes from these sensors
doesn't ever have to leave the client.

00:02:24.375 --> 00:02:28.434
You don't have to upload anything
to the server, which preserves privacy.

00:02:29.734 --> 00:02:34.949
Now, the playground that we built
is powered by a small neural network,

00:02:34.949 --> 00:02:37.387
300 lines of vanilla JavaScript

00:02:37.387 --> 00:02:39.745
that we wrote as a one-off library.

00:02:39.745 --> 00:02:43.313
It doesn't scale,
it's just simple for-loops,

00:02:43.313 --> 00:02:46.114
and it wasn't engineered to be reusable.

00:02:46.114 --> 00:02:52.557
But it was clear to us
that if we were to open the door

00:02:52.557 --> 00:02:55.764
for people to merge
machine learning and the browser,

00:02:55.764 --> 00:02:57.077
we had to build a library.

00:02:57.077 --> 00:02:58.101
And we did it!

00:02:58.101 --> 00:03:01.080
We released <i>deeplearn.js</i>,

00:03:01.080 --> 00:03:05.612
a JavaScript library
that is GPU-accelerated

00:03:05.612 --> 00:03:08.249
and it does that via WebGL,

00:03:08.249 --> 00:03:10.264
which is a standard in the browser,

00:03:10.264 --> 00:03:13.050
that allows you to render 3D graphics.

00:03:13.050 --> 00:03:15.811
We utilize it to do linear algebra for us.

00:03:15.811 --> 00:03:20.082
And <i>deeplearn.js</i> allows you to both
run inference in the browser

00:03:20.572 --> 00:03:22.915
and training entirely in the browser.

00:03:23.985 --> 00:03:27.221
When we released it,
we had an incredible momentum.

00:03:27.221 --> 00:03:29.433
The community took <i>deeplearn.js</i>

00:03:29.433 --> 00:03:31.815
and took existing models in Python,

00:03:31.815 --> 00:03:33.559
imported it into the browser

00:03:33.559 --> 00:03:35.901
and built interactive fun things with it.

00:03:35.901 --> 00:03:38.411
So, one example is the Style Transfer.

00:03:38.411 --> 00:03:41.373
Another person imported the character RNN,

00:03:41.373 --> 00:03:44.569
and then built a novel interface
that allows you to explore

00:03:44.569 --> 00:03:47.476
all the different
possible endings of a sentence,

00:03:47.476 --> 00:03:50.440
all generated by the model in real time.

00:03:50.440 --> 00:03:53.371
Another example
is a font-generative model--

00:03:53.371 --> 00:03:54.894
there was a post about this.

00:03:56.764 --> 00:04:00.203
The person that built it
allowed users to explore

00:04:00.203 --> 00:04:01.500
the hidden dimensions,

00:04:01.500 --> 00:04:04.094
the interesting dimensions
in the embedding space.

00:04:04.094 --> 00:04:09.451
And you can see how they relate
to boldness and slantedness of the font.

00:04:09.451 --> 00:04:12.845
And there were even educational examples,
like Teachable Machine

00:04:12.845 --> 00:04:15.063
that built this fun little game

00:04:15.063 --> 00:04:18.914
that taught people
how computer vision models work,

00:04:18.914 --> 00:04:22.863
so people could interact
directly with the webcam.

00:04:23.853 --> 00:04:26.348
Now, all the examples I showed you

00:04:26.818 --> 00:04:31.123
point to the incredible momentum
we have with <i>deeplearn.js</i>.

00:04:31.123 --> 00:04:34.993
And building on that momentum,
we're very excited today

00:04:34.993 --> 00:04:40.357
to announce that <i>deeplearn.js</i>
is joining the TensorFlow family.

00:04:40.357 --> 00:04:45.677
And with that, we are releasing
a new ecosystem of libraries and tools

00:04:45.677 --> 00:04:49.636
for machine learning with JavaScript,
called <i>TensorFlow.js</i>.

00:04:50.676 --> 00:04:53.643
Now, before we get into the details,

00:04:53.643 --> 00:04:58.373
I want to go over three main use cases
of how you can use <i>TensorFlow.js</i> today,

00:04:58.373 --> 00:05:00.796
with the tools and libraries
that we're releasing.

00:05:00.796 --> 00:05:04.537
So, one use case is you can write models
directly in the browser,

00:05:04.537 --> 00:05:07.391
and this has huge
educational implications--

00:05:08.041 --> 00:05:10.605
think of the playground
that I just showed.

00:05:10.605 --> 00:05:12.105
A second use case is--

00:05:12.105 --> 00:05:15.400
a major use case is you can take
a pre-existing model,

00:05:15.400 --> 00:05:17.514
pretrained model in Python,

00:05:17.514 --> 00:05:22.392
use a script, and you can import it
into the browser to do inference.

00:05:22.942 --> 00:05:28.315
And a related use case is the same model
that you take to do inference,

00:05:28.315 --> 00:05:31.344
you can retrain it,
potentially with private data

00:05:31.344 --> 00:05:35.425
that comes from those sensors
of the browser, in the browser itself.

00:05:36.095 --> 00:05:38.027
Now, to give you more of a schematic view,

00:05:38.027 --> 00:05:43.119
we have the browser that utilizes WebGL
to do fast, linear algebra.

00:05:43.119 --> 00:05:46.992
On top of it, <i>TensorFlow.js</i>
has two sets of APIs:

00:05:46.992 --> 00:05:50.522
the ops API,
which used to be <i>deeplearn.js</i>,

00:05:50.522 --> 00:05:54.743
and we worked hard to align
the API with TensorFlow Python.

00:05:54.743 --> 00:05:58.990
It is powered by an automatic
differentiation library

00:05:58.990 --> 00:06:01.953
that is built analogous to Eager mode.

00:06:01.953 --> 00:06:05.639
And on top of that,
we have a high-level API Layers API

00:06:05.639 --> 00:06:07.886
that allows you to use best practices

00:06:07.886 --> 00:06:10.957
and high-level building blocks
to write models.

00:06:11.747 --> 00:06:14.670
What I'm also very excited
today to announce

00:06:14.670 --> 00:06:19.445
is that we're releasing tools
that can take an existing Keras model

00:06:19.445 --> 00:06:21.047
or TensorFlow SavedModel,

00:06:21.047 --> 00:06:25.252
import it automatically
for execution in the browser.

00:06:26.642 --> 00:06:29.228
Now, to show you an example of our API,

00:06:29.718 --> 00:06:33.787
we're going to go over a small program

00:06:33.787 --> 00:06:36.918
that tries to learn the coefficients
of a quadratic function.

00:06:36.918 --> 00:06:40.322
So the coefficients we're trying to learn
are a, b, and c from data.

00:06:41.096 --> 00:06:44.003
So we have our <i>import tf</i>
from <i>TensorFlow.js</i>.

00:06:44.003 --> 00:06:45.325
For those of you that don't know,

00:06:45.325 --> 00:06:48.318
this is a standard
ES6 import in JavaScript--

00:06:48.318 --> 00:06:50.004
very common.

00:06:50.004 --> 00:06:52.307
We have our three tensors: a, b, and c.

00:06:52.307 --> 00:06:54.253
We mark them as variables,

00:06:54.253 --> 00:06:56.079
which means that they are mutable

00:06:56.079 --> 00:06:59.311
and the optimizer can change them.

00:06:59.311 --> 00:07:03.561
We have our <i>f(x)</i> function
that does the polynomial computation.

00:07:03.561 --> 00:07:07.309
You can see here familiar API,
like <i>tf.add</i> and <i>tf.square</i>,

00:07:07.309 --> 00:07:08.841
like TensorFlow.

00:07:08.841 --> 00:07:10.792
In addition to that API,

00:07:10.792 --> 00:07:12.937
we also have a Chaining API

00:07:12.937 --> 00:07:17.061
which allows you to call
these map operations on Tensors itself.

00:07:17.061 --> 00:07:19.760
And this leads to better readable code

00:07:19.760 --> 00:07:22.308
that is closer to how we write math.

00:07:22.308 --> 00:07:25.981
Chaining is very popular
in the JavaScript world.

00:07:25.981 --> 00:07:28.521
So that's the feed-forward
part of the model.

00:07:28.521 --> 00:07:31.140
Now, for the training part,
we need a loss function.

00:07:31.140 --> 00:07:34.564
So here is a loss function
that is just the mean-squared error

00:07:35.274 --> 00:07:37.644
between the prediction and the label.

00:07:37.644 --> 00:07:41.776
We have our optimizer, an <i>sgd</i> optimizer,

00:07:41.776 --> 00:07:45.921
and we train the model--
we call <i>optimizer.minimize</i>

00:07:45.921 --> 00:07:47.921
for some number of EPOCHS.

00:07:47.921 --> 00:07:49.367
And here I want to emphasize,

00:07:49.367 --> 00:07:53.952
for those of you
that have used <i>tf.figure</i> before

00:07:53.952 --> 00:07:56.189
or the talk before us, Alex's talk,

00:07:56.189 --> 00:08:01.724
the API in <i>TensorFlow.js</i>
is aligned with the Eager API in Python.

00:08:02.611 --> 00:08:08.341
Alright, so clearly that's not how
most people write machine learning,

00:08:08.341 --> 00:08:13.570
because those low-level linear
algebra ops can be quite verbose.

00:08:13.570 --> 00:08:16.158
And for that, we have our Layers API.

00:08:16.158 --> 00:08:18.125
And to show you an example of that,

00:08:18.125 --> 00:08:23.361
we're going to build
a recurrent neural network

00:08:23.361 --> 00:08:25.661
that learns to sum two numbers.

00:08:25.661 --> 00:08:29.092
But the complicated part
is that those numbers,

00:08:29.092 --> 00:08:35.048
like the number 90+10,
are being fed character by character.

00:08:35.048 --> 00:08:36.239
And then the neural network

00:08:36.239 --> 00:08:40.667
has to maintain an internal state
with an LSTM cell.

00:08:40.667 --> 00:08:43.711
That state then gets passed
into a decoder,

00:08:43.711 --> 00:08:47.559
and then the decoder has to output 100,
character by character.

00:08:47.559 --> 00:08:50.341
So it's a sequence-to-sequence model.

00:08:50.341 --> 00:08:52.292
This may sound a little complicated,

00:08:52.292 --> 00:08:56.501
but with the Layers API
is not that many lines of code.

00:08:56.501 --> 00:08:59.629
We have our <i>import tf</i> from <i>TensorFlow.js</i>.

00:09:00.909 --> 00:09:04.341
We have our sequential model,
which just means it's a stack of layers.

00:09:04.341 --> 00:09:08.418
For those of you that are familiar
with <i>tf.layers</i> in Python or Keras,

00:09:08.418 --> 00:09:10.431
this API looks very familiar.

00:09:11.291 --> 00:09:14.159
The first two layers are the encoder,

00:09:14.159 --> 00:09:16.310
the last three layers are the decoder.

00:09:16.920 --> 00:09:18.520
And that's our model.

00:09:19.260 --> 00:09:23.431
We then compile it
with a loss, an optimizer,

00:09:23.431 --> 00:09:25.260
and a metric we want to monitor,

00:09:25.260 --> 00:09:26.561
like <i>accuracy</i>,

00:09:26.561 --> 00:09:29.800
and we call <i>model.fit</i>, with our data.

00:09:29.800 --> 00:09:33.088
Now, what I want to point out here
is the <i>await</i> keyword.

00:09:33.088 --> 00:09:35.750
So, <i>model.fit</i> is an asynchronous call,

00:09:35.750 --> 00:09:37.110
which means--

00:09:37.110 --> 00:09:41.231
because in practice that can take
about 30 or 40 seconds in a browser.

00:09:41.231 --> 00:09:43.274
And in those 30 or 40 seconds,

00:09:43.274 --> 00:09:47.130
you don't want the main UI thread
of the browser to be locked.

00:09:47.130 --> 00:09:51.483
And this is why you get a callback
with a history object after that's done.

00:09:51.483 --> 00:09:54.763
And in between, the GPU
is going to do the work.

00:09:56.553 --> 00:09:58.599
Now, the code I showed you

00:09:58.599 --> 00:10:02.586
is when you are trying
to write models directly--

00:10:02.586 --> 00:10:05.450
when you want to write models
directly in the browser.

00:10:05.450 --> 00:10:10.230
But, as I said before, a major use case,
even with <i>deeplearn.js</i>

00:10:10.230 --> 00:10:12.919
was people importing models
that were pretrained,

00:10:12.919 --> 00:10:15.847
and they just want to do
inference in the browser.

00:10:15.847 --> 00:10:19.047
And before we jump
into the details of that,

00:10:19.047 --> 00:10:21.275
I want to show you a fun little game

00:10:21.275 --> 00:10:24.179
that our friends
at Google Brand Studio built

00:10:24.179 --> 00:10:27.479
that takes advantage
of an automatically pretrained model

00:10:27.479 --> 00:10:29.152
ported into the browser.

00:10:29.152 --> 00:10:31.861
And the game is called
Emoji Scavenger Hunt.

00:10:32.521 --> 00:10:34.294
And the way it works--

00:10:34.294 --> 00:10:37.193
I'm going to show you here
a real demo with the phone.

00:10:37.193 --> 00:10:38.583
It's in the browser.

00:10:40.803 --> 00:10:42.025
Let me see.

00:10:42.685 --> 00:10:43.924
Can I see here?

00:10:43.924 --> 00:10:46.800
So you can see I have a Chrome browser
opened up on a Pixel phone.

00:10:46.800 --> 00:10:48.902
You can see the URL on the top.

00:10:48.902 --> 00:10:53.012
And the game uses the webcam
and shows me an emoji,

00:10:53.012 --> 00:10:55.857
and then I have some time,
some number of seconds

00:10:55.857 --> 00:10:58.752
to find the real version
item of that emoji

00:10:58.752 --> 00:11:00.215
before the time runs out.

00:11:00.215 --> 00:11:01.409
So, before we play,

00:11:01.409 --> 00:11:05.668
Nikhil here is going to help me identify
the objects that this game asks.

00:11:05.668 --> 00:11:07.125
- You ready?
- (Nikhil) I'm ready.

00:11:07.125 --> 00:11:09.096
(Daniel) Alright, let's go.

00:11:09.750 --> 00:11:11.665
(countdown beeping)

00:11:13.275 --> 00:11:16.784
- Alright. Watch. Do you have a watch?
- (Nikhil) I have a watch.

00:11:16.784 --> 00:11:18.764
<i>(phone) Did I spot a velvet?</i>

00:11:18.764 --> 00:11:19.757
Go on.

00:11:19.757 --> 00:11:21.885
- (Daniel) Whoo! Yay! We got that!
- (Nikhil) Awesome.

00:11:21.885 --> 00:11:23.641
(Daniel) Let's see what our next item is.

00:11:23.641 --> 00:11:25.875
(countdown beeping)

00:11:25.875 --> 00:11:27.393
- (Daniel) Shoe.
- (Nikhil) Shoe.

00:11:27.393 --> 00:11:29.568
(Daniel) You got
to help me out here, buddy.

00:11:29.568 --> 00:11:30.977
- <i>(phone) Did I spot a--</i>
- (Daniel) Oh, yeah!

00:11:30.977 --> 00:11:32.979
- <i>(phone) You found shoe.</i>
- (Daniel) Whoo! We got the shoe!

00:11:32.979 --> 00:11:34.379
(Nikhil) Alright, what's next?

00:11:34.379 --> 00:11:36.749
(countdown beeping)

00:11:36.749 --> 00:11:38.228
(Daniel) Right, it wants a banana.

00:11:38.228 --> 00:11:41.705
(Nikhil) A banana? Does anyone have a--
this guy's got a banana!

00:11:41.705 --> 00:11:44.407
- (Daniel) Hold, what!
- (Nikhil) This guy's got a banana!

00:11:44.413 --> 00:11:46.328
- (Daniel) Come over here.
- <i>(phone) Am I seeing a wall?</i>

00:11:46.328 --> 00:11:48.158
- (Daniel) Yay! Alright!
- (Nikhil) Alright!

00:11:48.158 --> 00:11:49.768
- (Daniel) Look at us!
- (Nikhil) I'm ready.

00:11:49.768 --> 00:11:51.526
We're going to have a high score here.

00:11:52.323 --> 00:11:54.294
- (Daniel) Beer!
- <i>(phone) Could that be a hat?</i>

00:11:54.294 --> 00:11:55.784
(Nikhil) It's 10:30
in the morning, Daniel!

00:11:55.790 --> 00:11:58.070
- <i>(phone) Did I spot a velvet?</i>
- (Nikhil) Let's get back to the talk.

00:11:58.070 --> 00:12:00.630
- (Daniel) Alright. (chuckles)
- <i>(phone) I think I saw a milk can.</i>

00:12:00.630 --> 00:12:03.222
(Nikhil) Alright, so I'm going to jump
into the technical details

00:12:03.222 --> 00:12:05.459
of how we actually built that game.

00:12:05.459 --> 00:12:07.518
The clicker? Yep.

00:12:11.032 --> 00:12:14.148
So, what we did was we trained
the model in TensorFlow

00:12:14.148 --> 00:12:17.642
to be an object recognizer
for the Scavenger Hunt game.

00:12:17.642 --> 00:12:19.571
We chose about 400 different classes

00:12:19.571 --> 00:12:21.370
that would be reasonable
for a game like this,

00:12:21.370 --> 00:12:25.189
you know, watches and bananas and beer.

00:12:25.189 --> 00:12:28.443
So, what we did was we used
the TensorFlow For Poets codelab.

00:12:28.443 --> 00:12:30.174
And in that codelab,
what you essentially do

00:12:30.174 --> 00:12:32.873
is you take a pretrained MobileNet model.

00:12:32.873 --> 00:12:34.725
And if you don't know what MobileNet is,

00:12:34.725 --> 00:12:38.242
it's a state-of-the-art computer
vision model for edge devices.

00:12:38.242 --> 00:12:40.686
So what we effectively did
was we took that model

00:12:40.686 --> 00:12:42.913
and we retrained it for these classes.

00:12:43.533 --> 00:12:46.513
Now we have an object detector
in the Python world.

00:12:46.513 --> 00:12:48.591
How do we actually
now get this into the browser?

00:12:48.591 --> 00:12:52.402
Well, we provide a set of tools today
that help you actually do that.

00:12:52.402 --> 00:12:55.696
Once it's in, you skin the game
and you make the computer talk

00:12:55.696 --> 00:12:57.900
and all that kind of fun stuff.

00:12:57.900 --> 00:13:00.715
Let's jump into how
we actually can convert that model.

00:13:00.715 --> 00:13:05.344
So, as Daniel mentioned earlier today,
we actually support two types of models,

00:13:05.344 --> 00:13:08.473
so we support TensorFlow SavedModels--
we have a converter for that.

00:13:08.473 --> 00:13:12.044
And we also have a converter
for Keras SavedModels.

00:13:12.044 --> 00:13:14.596
So you define your model
and you save it with a Save model--

00:13:14.596 --> 00:13:16.429
this is a standard way to do that.

00:13:16.429 --> 00:13:19.743
Similarly, this is a code
that you would do that for Keras

00:13:19.743 --> 00:13:21.622
[inaudible].

00:13:21.622 --> 00:13:24.542
The next piece is that
we actually convert it to the Web.

00:13:25.192 --> 00:13:27.095
Today, we're releasing a pit package.

00:13:27.095 --> 00:13:29.596
It's <i>TensorFlow.js</i>,
you can install that there.

00:13:29.596 --> 00:13:30.904
There's a script in there

00:13:30.904 --> 00:13:33.638
that lets you point
to your TensorFlow SavedModel,

00:13:33.638 --> 00:13:35.611
and point to an output directory,

00:13:35.611 --> 00:13:36.754
and that output directory

00:13:36.754 --> 00:13:39.891
will be where those static
build artifacts will go for the web.

00:13:40.601 --> 00:13:42.382
Keras is the same exact flow.

00:13:42.382 --> 00:13:45.279
Point to you HDF5 input
and you have an output directory

00:13:45.279 --> 00:13:47.413
where those [build artifacts] will go.

00:13:47.413 --> 00:13:49.891
Now you statically host those
on your website somewhere,

00:13:49.891 --> 00:13:52.189
you know, just simple static hosting.

00:13:52.779 --> 00:13:56.991
And on the JavaScript side, we provide
an API that lets you load that model.

00:13:56.991 --> 00:13:59.091
So this is what it looks like
for a TensorFlow.

00:13:59.091 --> 00:14:01.773
In the TensorFlow SavedModel
you'll notice that it's a frozen model,

00:14:01.773 --> 00:14:05.385
we don't right now support
continuing training of this model.

00:14:05.763 --> 00:14:08.897
While in the Keras case,
we actually let you continue training,

00:14:08.897 --> 00:14:13.290
and we're working hard to keep
these APIs aligned in the future.

00:14:14.490 --> 00:14:17.110
Okay, so under the cover,
what are we actually doing?

00:14:17.110 --> 00:14:18.968
So we're doing some graph optimization,

00:14:18.968 --> 00:14:21.251
which essentially means
that we prune out nodes

00:14:21.251 --> 00:14:23.051
that you don't need
to make the prediction,

00:14:23.051 --> 00:14:24.874
you don't need that on the web.

00:14:25.834 --> 00:14:28.471
We optimize waits
for browser autocaching,

00:14:28.471 --> 00:14:31.258
so we pack in shard in chunks of 4MB

00:14:31.258 --> 00:14:34.999
which helps your browser be quick
the next time your page loads.

00:14:36.029 --> 00:14:39.449
Today, we support about 90
of the most commonly-used TensorFlow ops,

00:14:39.449 --> 00:14:43.439
and we're working very hard
to support more, like control flow ops.

00:14:43.439 --> 00:14:44.645
And we support 32

00:14:44.645 --> 00:14:47.083
of the most commonly-used
Keras layers today.

00:14:47.083 --> 00:14:51.249
And as I mentioned, we let
you continue training for Keras models,

00:14:51.249 --> 00:14:52.752
and we let you do evaluation,

00:14:52.752 --> 00:14:55.142
as well as make predictions
from that model.

00:14:56.682 --> 00:14:58.983
Okay, so obviously,
there's a lot you can do,

00:14:58.983 --> 00:15:02.580
with just porting your models
to the web for inference.

00:15:02.580 --> 00:15:06.351
But since the beginning of <i>deeplearn.js</i>,
we've made it a high priority

00:15:06.351 --> 00:15:08.871
to make sure that you can
train directly in the browser.

00:15:08.871 --> 00:15:11.177
This opens up the door for education

00:15:11.177 --> 00:15:13.928
and interactive tools
like we saw with the playground,

00:15:14.588 --> 00:15:18.167
as well as let you train with data
that never leaves your client.

00:15:18.167 --> 00:15:19.957
So this is huge for privacy.

00:15:20.747 --> 00:15:23.468
So you show off what you can do
with something like this,

00:15:23.468 --> 00:15:25.725
we've built another little game.

00:15:25.725 --> 00:15:30.234
So, the goal of the game
is to play Pac-Man with your webcam.

00:15:30.234 --> 00:15:31.920
Now, Daniel's going to be my helper here.

00:15:31.920 --> 00:15:35.804
He is much, much better at this game
than I am for some reason.

00:15:36.534 --> 00:15:37.692
Just say, "Hi!"

00:15:37.692 --> 00:15:40.290
So, there are three phases of the game.

00:15:40.290 --> 00:15:42.944
Phase 1-- we're going
to collect frames from the webcam

00:15:42.944 --> 00:15:44.331
and we're going to associate those

00:15:44.331 --> 00:15:46.735
with up, down, left, and right,
these classes.

00:15:46.735 --> 00:15:49.743
Now Daniel's going to move
his head up, down, left, and right,

00:15:49.743 --> 00:15:52.259
and he's just simply going
to play the game like that.

00:15:52.259 --> 00:15:54.552
And you'll notice,
as he's collecting frames,

00:15:54.552 --> 00:15:56.172
he's kind of moving around a little bit.

00:15:56.172 --> 00:15:59.635
This kind of helps the model
see different angles for that class

00:15:59.635 --> 00:16:01.542
and generalize a little bit better.

00:16:02.122 --> 00:16:05.039
So after he's done
collecting these frames,

00:16:05.039 --> 00:16:07.262
we're going to go and train our model.

00:16:07.832 --> 00:16:09.780
So we're not actually training
from scratch here

00:16:09.780 --> 00:16:11.345
when we hit that <i>Train</i> button.

00:16:11.345 --> 00:16:13.670
We're taking a pretrained MobileNet again,

00:16:13.670 --> 00:16:14.905
porting that to the web,

00:16:14.905 --> 00:16:17.741
and doing a retraining phase
with that data that's local,

00:16:17.741 --> 00:16:21.465
and we're using the Layers API
to do that in a browser here.

00:16:21.465 --> 00:16:23.659
Do you want to press that <i>Train</i> button?

00:16:25.474 --> 00:16:27.084
Alright. Our loss is going down--

00:16:27.084 --> 00:16:28.580
looks like we're learning something.

00:16:28.580 --> 00:16:29.922
That's great.

00:16:29.922 --> 00:16:32.252
So, as soon as we press that <i>Play</i> button,

00:16:32.252 --> 00:16:35.238
what's going to happen is we're going
to make predictions from the webcam--

00:16:35.238 --> 00:16:37.343
those are going to get plugged
into those controls

00:16:37.343 --> 00:16:40.389
and it's going to control
the Pac-Man game.

00:16:40.389 --> 00:16:41.801
Ready?

00:16:42.971 --> 00:16:44.803
Alright, so you can see
in the bottom right,

00:16:45.423 --> 00:16:47.738
it's highlighting the class
that it thinks it is.

00:16:47.738 --> 00:16:51.407
And Daniel, if he moves his head around,
you'll see it change [by] class.

00:16:51.857 --> 00:16:53.202
And he's off.

00:16:55.212 --> 00:16:57.342
(laughter)

00:16:58.492 --> 00:17:00.161
So... (chuckling)

00:17:02.121 --> 00:17:03.481
all of this code is online,

00:17:03.481 --> 00:17:05.816
and you can go fork it--
we invite you to do so.

00:17:05.816 --> 00:17:07.822
And obviously, this is just a game.

00:17:07.822 --> 00:17:10.916
But you can imagine other types
of applications of this,

00:17:10.916 --> 00:17:12.732
like make a browser extension

00:17:12.732 --> 00:17:15.856
that lets you control the page
for accessibility purposes.

00:17:15.856 --> 00:17:17.708
So again, all this code is online.

00:17:18.178 --> 00:17:21.150
Please go fork it, and play
and make something else with it.

00:17:22.700 --> 00:17:26.321
- Okay, Daniel. I know this is fun.
- (Daniel) I gotta? Alright.

00:17:26.321 --> 00:17:27.401
(laughter) (applause)

00:17:27.401 --> 00:17:29.611
You've got to get back to the talk.

00:17:33.066 --> 00:17:36.243
Okay, so let's talk a little bit
about performance.

00:17:36.243 --> 00:17:40.373
So what we're looking at here
is a benchmark of MobileNet 1.0

00:17:40.373 --> 00:17:41.621
running with TensorFlow--

00:17:41.621 --> 00:17:44.369
this is TensorFlow classic,
not with <i>TensorFlow.js</i>.

00:17:45.089 --> 00:17:47.365
I want to point out here,
we're using a batch size of 1.

00:17:47.365 --> 00:17:49.652
This is important
because we're thinking about this

00:17:49.652 --> 00:17:51.753
in the context
of an interactive application,

00:17:51.753 --> 00:17:55.341
so maybe this Pac-Man game
where you feed in webcam data,

00:17:55.341 --> 00:17:57.804
you want to know
what the prediction time is for 1.

00:17:57.804 --> 00:17:59.606
You can't really batch things.

00:17:59.606 --> 00:18:02.409
On the top row, we're looking
at TensorFlow CUDA

00:18:02.409 --> 00:18:05.005
running on a 1080GTX--

00:18:05.005 --> 00:18:06.589
this is a beefy machine.

00:18:06.589 --> 00:18:07.940
It's about three milliseconds.

00:18:07.940 --> 00:18:08.936
I want to point out--

00:18:08.936 --> 00:18:11.574
the shorter the bar,
the faster it is, clearly.

00:18:11.574 --> 00:18:13.691
On the second row we have TensorFlow CPU,

00:18:13.691 --> 00:18:15.823
and this is running with AVX-512,

00:18:15.823 --> 00:18:18.950
and this is on actually
one of these MacBook Pros here.

00:18:18.950 --> 00:18:21.462
It's about 60 milliseconds for that frame.

00:18:22.362 --> 00:18:24.799
Where does <i>TensorFlow.js</i>
fit into this picture?

00:18:25.181 --> 00:18:26.961
Well, it depends.

00:18:26.961 --> 00:18:29.643
If you're running on a beefy 1080GTX,

00:18:29.643 --> 00:18:31.751
we're actually getting
about 11 milliseconds

00:18:31.751 --> 00:18:33.961
for one pass through this MobileNet model,

00:18:33.961 --> 00:18:35.693
which is pretty good
if you think about this

00:18:35.693 --> 00:18:37.944
in the context of an interactive game.

00:18:37.944 --> 00:18:41.648
So, on the laptop that we just
showed the game with,

00:18:41.648 --> 00:18:43.048
we're getting about 100 milliseconds

00:18:43.048 --> 00:18:45.009
for that inference,
passed through MobileNet.

00:18:45.009 --> 00:18:46.492
That's still pretty good.

00:18:46.492 --> 00:18:48.147
You can build a whole interactive game

00:18:48.147 --> 00:18:50.622
with something that's running
at 100 milliseconds.

00:18:51.232 --> 00:18:53.741
So the web is only going to get
faster and faster.

00:18:53.741 --> 00:18:56.919
There's a whole new set
of standards coming, like WebGPU,

00:18:56.919 --> 00:19:00.264
that will really push the boundary
for these kinds of things.

00:19:00.264 --> 00:19:03.273
But the browser still has its limitations.

00:19:03.273 --> 00:19:05.961
You can only really get access

00:19:05.961 --> 00:19:10.391
to the GPU through WebGL on these APIs.

00:19:10.391 --> 00:19:12.239
So, how do we scale beyond that?

00:19:12.239 --> 00:19:16.198
How do we scale beyond the limitations
that we have in the browser?

00:19:16.908 --> 00:19:21.003
There's a whole ecosystem
of server-side JavaScript tools

00:19:21.003 --> 00:19:24.195
using <i>node.js</i> that we would love
to take advantage of.

00:19:25.305 --> 00:19:29.787
So today, I'm really happy to tell you
that we're working on <i>node.js</i> bindings

00:19:29.787 --> 00:19:32.148
to the TensorFlow C API.

00:19:32.918 --> 00:19:36.687
What that means is you'll be able to write
that same low-level ops API

00:19:36.687 --> 00:19:37.602
with the Eager mode

00:19:37.602 --> 00:19:39.339
we saw with the polynomial example,

00:19:39.339 --> 00:19:43.250
or the high-level Layers API
which we saw for the Pac-Man example,

00:19:43.250 --> 00:19:45.513
and bind to TensorFlow C,

00:19:45.513 --> 00:19:50.439
and run head lists in your TensorFlow
running with CUDA installed.

00:19:50.439 --> 00:19:53.718
Eventually, that also means we can run
with a TPU on a backend--

00:19:53.718 --> 00:19:55.716
that same JS code.

00:19:56.316 --> 00:19:58.755
So these bindings
are underactive developments,

00:19:58.755 --> 00:20:00.658
so stay tuned for more.

00:20:02.220 --> 00:20:04.790
Alright, so let's recap some of the things
that we launched today

00:20:04.790 --> 00:20:06.230
and that we talked about.

00:20:06.230 --> 00:20:08.888
So we talked about this low-level ops API

00:20:08.888 --> 00:20:12.769
which does hardware-
accelerated linear algebra

00:20:12.769 --> 00:20:16.291
as well as the Eager mode
differentiation for autograd.

00:20:16.291 --> 00:20:18.326
This was previously known as <i>deeplearn.js</i>.

00:20:18.326 --> 00:20:20.600
We're re-branding that today.

00:20:21.080 --> 00:20:24.237
We released the high-level Layers API.

00:20:24.237 --> 00:20:27.736
This is the Keras-inspired API
that mirrors TensorFlow Layers,

00:20:27.736 --> 00:20:30.376
and we saw an example
of that with the addition RNN,

00:20:30.376 --> 00:20:33.438
and we saw an example of that
with the Pac-Man demo.

00:20:34.448 --> 00:20:37.775
We also showed you how you can import
TensorFlow SavedModels,

00:20:37.775 --> 00:20:41.900
and Keras models for prediction
and retraining in the browser.

00:20:43.400 --> 00:20:46.531
We also have released a bunch of demos
and examples on Github.

00:20:46.531 --> 00:20:47.937
These are not the only two--

00:20:47.937 --> 00:20:50.026
there's a whole repository
of different examples

00:20:50.026 --> 00:20:51.498
that can get you started,

00:20:51.498 --> 00:20:55.274
and they'll have live links
so you can go and poke around and play.

00:20:55.274 --> 00:20:57.280
So I invite you to go do that.

00:20:58.880 --> 00:21:01.649
So we really want
to see you get involved in this project.

00:21:01.649 --> 00:21:03.081
We have a bunch of links here,

00:21:03.081 --> 00:21:06.023
so <i>js.tensorflow.org</i>
is our official website.

00:21:06.023 --> 00:21:08.707
All of these links,
everything we've talked about is there--

00:21:08.707 --> 00:21:12.141
there's tutorials,
there's documentation, etc.

00:21:12.141 --> 00:21:15.800
Our code is obviously open-sourced
under <i>tensorflow/tfjs</i>.

00:21:15.800 --> 00:21:17.876
So I invite you to go play there too.

00:21:18.556 --> 00:21:20.960
And we also started
a community mailing list today--

00:21:20.960 --> 00:21:22.613
that's the short link here.

00:21:22.613 --> 00:21:24.382
And the community
mailing list is for people

00:21:24.382 --> 00:21:27.814
to post demos and ask questions,
and that kind of thing.

00:21:27.814 --> 00:21:30.484
So this project
was not just Daniel and myself,

00:21:30.484 --> 00:21:32.243
this was a larger team effort

00:21:32.243 --> 00:21:34.849
between many of our amazing
colleagues at Google.

00:21:34.849 --> 00:21:36.421
So we want to thank them.

00:21:36.421 --> 00:21:37.497
We also want to thank

00:21:37.497 --> 00:21:40.941
all of the amazing open-source
contributors for <i>deeplearn.js</i>.

00:21:41.581 --> 00:21:44.074
And we're really excited
to build the next chapter

00:21:44.074 --> 00:21:46.806
of machine learning
and JavaScript with you.

00:21:46.806 --> 00:21:48.176
- Thank you
- (Daniel) Thank you.

00:21:48.176 --> 00:21:49.925
(applause)

00:21:49.925 --> 00:21:52.520
♪ (music) ♪

