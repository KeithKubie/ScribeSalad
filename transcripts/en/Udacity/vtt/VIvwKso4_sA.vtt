WEBVTT
Kind: captions
Language: en

00:00:01.010 --> 00:00:04.320
Hello everybody and welcome to Udacity's Explorers

00:00:04.320 --> 00:00:06.925
this week on Human-Robot Interaction and Robot Ethics.

00:00:06.924 --> 00:00:11.175
I'm here with the director of Silicon Valley Robotics. Andra Keay. Andra how are you?

00:00:11.175 --> 00:00:13.480
I'm great. Good to be here.

00:00:13.480 --> 00:00:17.219
Can you tell me a little bit about how you got started in robotics?

00:00:17.219 --> 00:00:20.609
I've always been a tech geek but I went through

00:00:20.609 --> 00:00:25.304
a few different pathways before coming back to robotics and I say coming back to it,

00:00:25.304 --> 00:00:28.800
I actually did film in television technologies and

00:00:28.800 --> 00:00:35.975
then computer and Internet technologies and I felt that I was being behind the curve.

00:00:35.975 --> 00:00:39.704
So I thought what really is the technology

00:00:39.704 --> 00:00:43.698
that is going to be the biggest influence on the 21st century?

00:00:43.698 --> 00:00:47.599
And how do I make sure that that's the technology that I'm involved with?

00:00:47.600 --> 00:00:50.579
So I went back to study robots and

00:00:50.579 --> 00:00:53.219
particularly the areas around how robots are

00:00:53.219 --> 00:00:56.280
going to- how are they going to affect society.

00:00:56.280 --> 00:01:02.239
So I call it not just human-robot interaction but human-robot culture.

00:01:02.238 --> 00:01:04.709
So the idea of like cobots and things like that as well just

00:01:04.709 --> 00:01:07.524
kind of everything working together in a nice.

00:01:07.525 --> 00:01:09.850
Definitely looking at how things work together.

00:01:09.849 --> 00:01:13.049
But going beyond how robots may work with us

00:01:13.049 --> 00:01:16.569
as individuals and starting to ask the questions like,

00:01:16.569 --> 00:01:21.209
"Will all taxi drivers be out of jobs if we have self-driving vehicles?

00:01:21.209 --> 00:01:22.784
Is that a fact,

00:01:22.784 --> 00:01:26.250
or will there be slower transitions?

00:01:26.250 --> 00:01:30.763
What does it mean if we make a change quickly versus as we make a change in stages?"

00:01:30.763 --> 00:01:34.185
And perhaps some of the things are, you know,

00:01:34.185 --> 00:01:38.609
where do we make big wrong assumptions about what a technology impact

00:01:38.609 --> 00:01:42.959
will be and how can we make better assumptions at

00:01:42.959 --> 00:01:47.909
predictions around what the impacts of technologies will be because I think most of

00:01:47.909 --> 00:01:53.149
the headlines that you hear or read are completely wrong.

00:01:53.150 --> 00:01:58.920
And in fact one of my favorite sayings is a Moore's Law which is that we greatly

00:01:58.920 --> 00:02:01.129
overestimate the short term impact of

00:02:01.129 --> 00:02:05.824
the technology but we vastly underestimate the long term impact.

00:02:05.825 --> 00:02:11.129
And I see robotics and the AI that's embodied in robotics as

00:02:11.129 --> 00:02:16.691
being fundamentally transformative technologies of the 21st century,

00:02:16.691 --> 00:02:21.844
but I think we get really confused over what are the short term and long term impacts.

00:02:21.844 --> 00:02:24.060
So, when you talk about human-robot interaction.

00:02:24.060 --> 00:02:26.400
It's not so much just a robot that has

00:02:26.400 --> 00:02:28.955
a face that kind of walks around and talks to people.

00:02:28.955 --> 00:02:30.594
It's much more than that right?

00:02:30.593 --> 00:02:34.104
I mean you're talking about taxicabs and driverless cars.

00:02:34.104 --> 00:02:39.634
It's really the idea of any type of interaction with a robot from a human perspective.

00:02:39.634 --> 00:02:45.329
Absolutely. And you raised one of those issues- robot systems and I think

00:02:45.330 --> 00:02:49.140
that a lot of HRI has focused very

00:02:49.139 --> 00:02:53.854
much on psychological studies of what happens between one robot and one person.

00:02:53.854 --> 00:02:58.289
But if you look at commercialization and my role with

00:02:58.289 --> 00:03:02.639
Silicon Valley robotics has been to help with the commercialization of

00:03:02.639 --> 00:03:06.209
a lot of robotics startups and of course you're trying

00:03:06.210 --> 00:03:11.450
to increase the spread of a technology.

00:03:11.449 --> 00:03:14.310
So you're not ever really in

00:03:14.310 --> 00:03:17.629
a situation of talking about one robot, one person interactions.

00:03:17.629 --> 00:03:21.629
You're talking about multi-interactions- multiple robots,

00:03:21.629 --> 00:03:27.629
multiple people and you're talking about things that need to include duration as well.

00:03:27.629 --> 00:03:32.805
So I felt that HRI is great as far as it goes.

00:03:32.805 --> 00:03:39.554
But for people to consider that is the only way to look at how

00:03:39.554 --> 00:03:42.944
are we going to interact with robots is really missing

00:03:42.944 --> 00:03:46.930
many of the other dimensions and durations.

00:03:46.931 --> 00:03:51.790
You know there's very little funding for things like a long term study.

00:03:51.789 --> 00:03:58.079
So there was one particular one that studied people's feelings about reimbursed

00:03:58.080 --> 00:04:05.800
in their life over a period of weeks rather than the one off kind of interaction.

00:04:05.800 --> 00:04:07.920
And when you did that it completely

00:04:07.919 --> 00:04:10.530
overturned a lot of the papers that were saying people

00:04:10.530 --> 00:04:15.664
respond this way to robots that do this and people respond that way to robots to do that.

00:04:15.664 --> 00:04:20.250
And I thought, to me that captured exactly where we're going wrong is we take

00:04:20.250 --> 00:04:25.889
these little snapshots and we think that it's giving us a full panorama and it's not.

00:04:25.889 --> 00:04:28.939
That's common in a lot of studies that you see actually right.

00:04:28.939 --> 00:04:32.685
There's always- you're digging in to try to find that one area

00:04:32.685 --> 00:04:37.084
but you're not really taking the whole perspective of actually what's going on.

00:04:37.084 --> 00:04:39.284
So you shoot in that area and you say,

00:04:39.285 --> 00:04:41.405
"Ah this is the exact result that I was looking for,

00:04:41.404 --> 00:04:43.074
I set up the study in such a way that I kind of

00:04:43.074 --> 00:04:45.889
forced my hand to be able to get something like that."

00:04:45.889 --> 00:04:48.659
But the reality is like in the grand scheme of things or

00:04:48.660 --> 00:04:52.310
a longer term study you'll realize that it's actually quite the opposite.

00:04:52.310 --> 00:04:57.014
Well, I think robotics is very flawed in not being a science.

00:04:57.014 --> 00:04:58.884
It's an engineering discipline.

00:04:58.884 --> 00:05:02.324
And most papers are not reproducible.

00:05:02.324 --> 00:05:04.529
And I'm talking well beyond HRI.

00:05:04.529 --> 00:05:07.559
It's really every area.

00:05:07.560 --> 00:05:10.379
This was one of the things that Willow Garage

00:05:10.379 --> 00:05:14.144
was very strongly about changing that paradigm

00:05:14.144 --> 00:05:21.264
by creating a common robotics platform and then a common robotics operating system.

00:05:21.264 --> 00:05:24.180
So you had a greater chance of being able to take

00:05:24.180 --> 00:05:30.689
somebody else's research and replicate it and ideally get the same results.

00:05:30.689 --> 00:05:34.679
And that's been pretty much lacking in the field of robotics.

00:05:34.678 --> 00:05:36.724
And for those people who don't know Willow Garage,

00:05:36.725 --> 00:05:39.990
he is also the creators of ROS which is why they use the platform for

00:05:39.990 --> 00:05:44.835
pretty much any robotics company that's starting within the last 10 years or so.

00:05:44.834 --> 00:05:49.979
Absolutely. It's now I think in the last five years,

00:05:49.980 --> 00:05:53.370
it's gone from being used by companies that knew

00:05:53.370 --> 00:05:59.514
Willow Garage to being used by pretty much every startup that I'm in contact with.

00:05:59.514 --> 00:06:02.444
And it may not be being used at

00:06:02.444 --> 00:06:05.189
their commercial release level but

00:06:05.189 --> 00:06:09.180
it's being used in the development stages at the very least.

00:06:09.180 --> 00:06:11.500
So kind of going back to the idea of HRI.

00:06:11.500 --> 00:06:14.144
What do you consider in your opinion,

00:06:14.144 --> 00:06:17.173
what makes a good robot that interacts with a human?

00:06:17.173 --> 00:06:25.557
I think that one of the key things is don't overspecialize,

00:06:25.557 --> 00:06:28.319
don't over adorn, don't make

00:06:28.319 --> 00:06:32.724
your robot really promise more than it's going to be able to deliver

00:06:32.725 --> 00:06:36.645
because I think we still have very very rudimentary abilities

00:06:36.644 --> 00:06:41.854
to behave appropriately in context.

00:06:41.855 --> 00:06:48.480
And as long as you can't really rely on a robot to know enough about a situation,

00:06:48.480 --> 00:06:53.700
to have anything approaching a human level of response,

00:06:53.699 --> 00:06:57.899
then creating something that gives us

00:06:57.899 --> 00:07:01.500
the expectation is going to- you know it's

00:07:01.500 --> 00:07:05.639
going to turn around and bite us because people will be disappointed in that robot.

00:07:05.639 --> 00:07:11.050
And we're at a weird point right now where there are a lot of claims.

00:07:11.050 --> 00:07:15.826
It's probably easier to sell a robot if you make a lot of promises about what it can do.

00:07:15.826 --> 00:07:19.560
And people love the idea of robots that they can

00:07:19.560 --> 00:07:23.576
have some sort of relationship in a very broad sense;

00:07:23.576 --> 00:07:29.780
companionship, it's cute, something like that but it makes it easier to sell the robot.

00:07:29.779 --> 00:07:34.769
But we haven't really had enough robots out there for people to start

00:07:34.769 --> 00:07:40.334
to see how well they remain engaged.

00:07:40.334 --> 00:07:43.604
And I think most good robotics companies

00:07:43.605 --> 00:07:46.439
are actually less concerned with selling the robot

00:07:46.439 --> 00:07:49.740
and more concerned with building something that is going to

00:07:49.740 --> 00:07:53.095
keep engagement once it has been sold,

00:07:53.095 --> 00:07:54.900
once it's in someone's house.

00:07:54.899 --> 00:07:57.884
How do you keep people interacting with it.

00:07:57.884 --> 00:08:01.709
So it's an interesting idea talking about the engagement level of a robot.

00:08:01.709 --> 00:08:03.689
Do you feel that there are certain types of robots

00:08:03.689 --> 00:08:05.954
that warrant a higher level of engagement?

00:08:05.954 --> 00:08:07.810
And like a stronger bond between people,

00:08:07.810 --> 00:08:09.478
so for instance, you know,

00:08:09.478 --> 00:08:11.729
government contractors I used to work in

00:08:11.730 --> 00:08:14.879
ground stations for UAVs and things like that but I read an article

00:08:14.879 --> 00:08:17.459
that one of the soldiers had a UAV that he

00:08:17.459 --> 00:08:21.710
flew on a bunch of different missions in Afghanistan or Iraq,

00:08:21.711 --> 00:08:27.535
and it was I think 15 missiles or something like that and it got shot down finally.

00:08:27.535 --> 00:08:28.817
And when it got shot down,

00:08:28.817 --> 00:08:35.430
the soldier demanded a burial for his UAV and they were like, "It's just a robot."

00:08:35.429 --> 00:08:38.969
And he was like this thing has saved my life so many times, you know,

00:08:38.970 --> 00:08:43.759
with all the landmines ideas and everything else out there he's like,

00:08:43.759 --> 00:08:46.158
"I want this thing buried, I want to have a proper sendoff for it."

00:08:46.158 --> 00:08:49.259
Do you think you're going to see that kind of reaction from more people kind of

00:08:49.259 --> 00:08:52.909
coming forward in the future with like social robots or home robots.

00:08:52.909 --> 00:08:56.639
Or do you think it's just going to be kind of one of those- it's kind of a cool thing?

00:08:56.639 --> 00:08:57.798
I interact with a little bit,

00:08:57.798 --> 00:09:00.720
but then after a while this new and shininess of it kind of wears

00:09:00.720 --> 00:09:04.134
off and we actually go back to treatment more like machine?

00:09:04.134 --> 00:09:07.944
I think it really depends on the depth of the interaction,

00:09:07.945 --> 00:09:11.370
and you've talked about what's really an extreme use case.

00:09:11.370 --> 00:09:14.924
It's been engaged in life in death situations.

00:09:14.924 --> 00:09:17.564
So you will have a very strong bond.

00:09:17.565 --> 00:09:21.375
If it's vacuuming your floors you won't.

00:09:21.375 --> 00:09:24.840
And one of the points that has been raised that I think was very

00:09:24.840 --> 00:09:29.490
interesting was to suggest that up until this moment in time,

00:09:29.490 --> 00:09:37.560
humans really had only a couple of ontological states that we could comprehend.

00:09:37.559 --> 00:09:41.429
One was alive and one was dead or inanimate.

00:09:41.429 --> 00:09:48.060
So you were either an object or you were an organism and it's saying,

00:09:48.061 --> 00:09:56.790
so do we then get into a kind of switch-like state of having things that behave or that

00:09:56.789 --> 00:10:01.159
we treat like organisms that then behave like objects and

00:10:01.159 --> 00:10:06.649
do we then treat them like objects or do we carry over that more emotional response?

00:10:06.649 --> 00:10:10.750
And what does that mean in terms of us developing perhaps empathy,

00:10:10.750 --> 00:10:14.049
is one of the biggest worries that people have.

00:10:14.049 --> 00:10:21.849
If we are engaging with something that looks like it's alive but we can treat it badly,

00:10:21.850 --> 00:10:27.460
is this going to create a lack of empathy in us?

00:10:27.460 --> 00:10:31.600
One of the interesting hypotheses is that we're going to

00:10:31.600 --> 00:10:36.430
evolve towards a third ontological awareness.

00:10:36.429 --> 00:10:43.149
That there are some things that are both alive and not alive that they can be emotionally

00:10:43.149 --> 00:10:50.164
engaging but they can be turned off and on at will without damage.

00:10:50.164 --> 00:10:54.534
So if we can really develop that third ontological awareness,

00:10:54.534 --> 00:10:57.504
then perhaps there is not an issue with us

00:10:57.504 --> 00:11:00.730
being trained into a lack of empathy because we

00:11:00.730 --> 00:11:07.914
all understand that it's a that thing as opposed to an it or a you.

00:11:07.914 --> 00:11:09.594
So it's an interesting idea.

00:11:09.595 --> 00:11:11.920
Right. When you started talking about,

00:11:11.919 --> 00:11:14.829
it can interact with a person and the person can relate

00:11:14.830 --> 00:11:18.379
to it but we can always turn off a switch.

00:11:18.379 --> 00:11:20.740
When you're dealing with something like

00:11:20.740 --> 00:11:23.919
that is there ever a point where you start looking at it

00:11:23.919 --> 00:11:26.334
from the other perspective of like "wow this thing

00:11:26.335 --> 00:11:28.873
actually has feelings and emotions right."

00:11:28.873 --> 00:11:31.655
So we've seen videos I'm sure you've seen them as well,

00:11:31.654 --> 00:11:35.740
Boston Dynamics is a big dog where people have, you know,

00:11:35.740 --> 00:11:40.110
the engineers have shown kicking this robot right. Having that scuffle to kind of...

00:11:40.110 --> 00:11:42.469
Generally most people go,

00:11:42.469 --> 00:11:43.840
"Oh oh!" When they see that.

00:11:43.840 --> 00:11:44.464
Exactly.

00:11:44.464 --> 00:11:46.215
It encourages that.

00:11:46.215 --> 00:11:50.560
However, one of the things I've also seen is that people that are more familiar with

00:11:50.559 --> 00:11:55.309
robots don't have quite that shocked pity feeling.

00:11:55.309 --> 00:11:58.000
So they've become much more used to what

00:11:58.000 --> 00:12:01.304
they're seeing and they will have a different level of response.

00:12:01.304 --> 00:12:03.849
And I think it comes back down to saying you

00:12:03.850 --> 00:12:08.710
know don't confuse the snapshot for the whole picture.

00:12:08.710 --> 00:12:12.940
I've got five what I call

00:12:12.940 --> 00:12:14.890
laws of robotics because we all love

00:12:14.889 --> 00:12:17.643
Asimov's laws of robotics even though they don't work.

00:12:17.643 --> 00:12:19.480
They're never going to work.

00:12:19.480 --> 00:12:25.017
And even though I think laws is really the wrong terminology to use,

00:12:25.017 --> 00:12:27.805
I think to be effective,

00:12:27.804 --> 00:12:30.154
laws kind of come later.

00:12:30.154 --> 00:12:33.414
What we really want to be talking about is design guidelines,

00:12:33.414 --> 00:12:37.779
suggestions that can educate and inform

00:12:37.779 --> 00:12:43.659
people towards better decisions at the earliest possible stages of building robots.

00:12:43.659 --> 00:12:47.860
But I call them five laws of robotics and they start really at

00:12:47.860 --> 00:12:52.959
the most critical level in terms of Maslow's Hierarchy of Needs which is,

00:12:52.958 --> 00:12:55.155
"Don't build killing machines."

00:12:55.155 --> 00:13:01.120
And while drones in warfare maybe they are currently

00:13:01.120 --> 00:13:08.049
accepted but they're a bit of a mix.

00:13:08.049 --> 00:13:09.549
They're informational.

00:13:09.549 --> 00:13:11.169
They can do other things.

00:13:11.169 --> 00:13:13.929
They're not necessarily weapons.

00:13:13.929 --> 00:13:18.939
I think the key concept there is if you build something that is nothing but

00:13:18.940 --> 00:13:24.275
a killing machine then it's very difficult to have some other purpose for it.

00:13:24.274 --> 00:13:29.169
So the key principle is always build a multipurpose robot.

00:13:29.169 --> 00:13:32.539
Not something that is purely evil.

00:13:32.539 --> 00:13:40.719
And then the next one down is build things that obey laws and it might seem obvious

00:13:40.720 --> 00:13:49.492
but there isn't really clear areas like does this robot breach privacy law?

00:13:49.491 --> 00:13:53.733
And are you aware of what privacy laws in different countries is?

00:13:53.734 --> 00:13:57.282
So when you're putting sensors onto something,

00:13:57.282 --> 00:14:01.465
are they going to be law abiding or not?

00:14:01.465 --> 00:14:05.050
And we do start to run into issues where you have a lot of

00:14:05.049 --> 00:14:09.094
senses required for autonomous vehicles for

00:14:09.095 --> 00:14:13.960
mobile robots and they're going to start to be traveling in areas

00:14:13.960 --> 00:14:19.378
where they can be unpredicted data breaches,

00:14:19.378 --> 00:14:22.240
privacy breaches, surveillance issues.

00:14:22.240 --> 00:14:24.594
So can you think through these things ahead of time?

00:14:24.594 --> 00:14:29.754
Those laws exist because there is a social standard that we've come to agree on.

00:14:29.754 --> 00:14:34.000
So you have to inform yourself as to why they exist and how what

00:14:34.000 --> 00:14:38.471
you build might end up putting you on the wrong side of those things.

00:14:38.471 --> 00:14:41.620
Ultimately, the other laws relate

00:14:41.620 --> 00:14:50.118
to building things that don't create the wrong expectation.

00:14:50.118 --> 00:14:55.300
So build a robot system that is as transparent as

00:14:55.299 --> 00:14:57.909
possible so that you understand as much as possible about

00:14:57.909 --> 00:15:01.038
what its intentions are and what its capabilities are.

00:15:01.038 --> 00:15:04.279
What it is able to do? And for me that is you know don't

00:15:04.279 --> 00:15:08.245
be deceptive and don't build deceptive robots.

00:15:08.245 --> 00:15:11.720
A robot that's humanoid is going to create

00:15:11.720 --> 00:15:17.899
the impression in us that it is capable of humanoid level behaviors.

00:15:17.899 --> 00:15:21.529
And the reality is that it's not you know but we're going to

00:15:21.529 --> 00:15:25.309
think that it can remember us from time to time.

00:15:25.309 --> 00:15:28.144
We're going to think that if it uses

00:15:28.144 --> 00:15:33.664
emotional hand gestures that it's going to be able to understand our emotional state,

00:15:33.664 --> 00:15:38.659
that it will understand things like proximity and body space.

00:15:38.659 --> 00:15:41.299
So you get into a lot of trouble because at

00:15:41.299 --> 00:15:45.834
the moment we don't really have that level of understanding or intelligence.

00:15:45.835 --> 00:15:49.940
So I think that there are reasons for building robots

00:15:49.940 --> 00:15:54.020
that can have a good social interaction but at

00:15:54.019 --> 00:15:58.519
the moment I think you need to keep them in a very narrow domain so

00:15:58.519 --> 00:16:03.620
that you don't start to imagine you've got a general purpose friend.

00:16:03.620 --> 00:16:13.230
Know what you have is a very helpful assistant driver or a very helpful medicine cabinet.

00:16:13.230 --> 00:16:17.225
You don't have a friend or something else

00:16:17.225 --> 00:16:22.392
and within that there are also other areas around transparency which relate to,

00:16:22.392 --> 00:16:26.840
is it possible for other people to understand how something is

00:16:26.840 --> 00:16:29.240
making its decisions and how it's operating which comes

00:16:29.240 --> 00:16:31.970
down to you know algorithmic transparency as well,

00:16:31.970 --> 00:16:35.560
where incorporating a lot of black boxes into what we do.

00:16:35.559 --> 00:16:40.334
And I know that's probably a problem that you've covered elsewhere.

00:16:40.335 --> 00:16:49.170
So ultimately from that is avoid black boxes and avoid overfeaturing a robot,

00:16:49.169 --> 00:16:51.486
you know, don't be humanoid if you don't need to be.

00:16:51.486 --> 00:16:53.915
Some of the best robots ones like say,

00:16:53.916 --> 00:16:58.160
Mayfield's robotics Kuri, it doesn't have a screen,

00:16:58.159 --> 00:17:02.074
it doesn't have a lot of features,

00:17:02.075 --> 00:17:07.596
it acts more like a toy than a humanoid.

00:17:07.596 --> 00:17:11.329
It makes sure that you can appreciate

00:17:11.329 --> 00:17:17.759
its limits much more and I think also Catalia Health's Mabu.

00:17:17.759 --> 00:17:22.644
And that's something that I see in the best robot designs.

00:17:22.644 --> 00:17:28.555
They focus on, can you do what you do well rather than trying to do everything.

00:17:28.555 --> 00:17:32.060
And ultimately there's one final guideline

00:17:32.059 --> 00:17:38.010
is can we tell who is responsible for robots? Who owns it?

00:17:38.010 --> 00:17:40.931
Do we have any kind of idea about registration,

00:17:40.931 --> 00:17:44.105
license plates for robots or something else like that

00:17:44.105 --> 00:17:49.995
because you are definitely already seeing robots in public places.

00:17:49.994 --> 00:17:52.564
In the Westfield shopping malls for example,

00:17:52.565 --> 00:17:56.557
in the streets of Redwood City delivering food,

00:17:56.557 --> 00:17:58.421
in San Francisco streets;

00:17:58.421 --> 00:18:01.630
San Jose, San Jose Airport.

00:18:01.630 --> 00:18:05.689
Now who is having that interaction with you?

00:18:05.689 --> 00:18:09.881
Right. Is it the robot or is it you know the person who's programmer of that?

00:18:09.881 --> 00:18:11.701
Well, the robots at the front of that.

00:18:11.701 --> 00:18:15.910
But is that an interaction on behalf of San Jose airport or is it

00:18:15.910 --> 00:18:21.080
an interaction on behalf of a third party who is using the space at San Jose Airport.

00:18:21.079 --> 00:18:25.934
Who do you contact if that robot runs over your toddler's toes.

00:18:25.934 --> 00:18:30.269
OK. All of these things but the physical potential harm and

00:18:30.269 --> 00:18:37.529
the economic and emotional potential harm that can happen in these situations.

00:18:37.529 --> 00:18:39.396
You need to be able to say,

00:18:39.396 --> 00:18:41.038
"Who's robot is that?"

00:18:41.038 --> 00:18:45.599
And I foresee a time coming up very soon where we have

00:18:45.599 --> 00:18:50.865
a lot of robots engaged in advertising and sales not just delivery.

00:18:50.865 --> 00:18:57.359
And as such it's not hard to create

00:18:57.359 --> 00:19:03.914
something that gives us the illusion of- that can capture our attention.

00:19:03.914 --> 00:19:06.389
OK. You know a billboard even if it's

00:19:06.390 --> 00:19:11.130
a moving billboard it's looking to capture our attention anywhere near as much

00:19:11.130 --> 00:19:16.815
as a humanoid object that can also use some movement and some other visual cues

00:19:16.815 --> 00:19:19.529
to capture our attention because ultimately we're still

00:19:19.529 --> 00:19:23.019
chimpanzees in terms of of our responses.

00:19:23.019 --> 00:19:26.369
There are certain things that will trigger our responses.

00:19:26.369 --> 00:19:29.304
Then what is the purpose?

00:19:29.305 --> 00:19:33.685
The robot is probably there to be conducting business of some kind.

00:19:33.684 --> 00:19:35.954
And are you going to be as

00:19:35.954 --> 00:19:39.809
informed and as in charge of that relationship as you would be if

00:19:39.809 --> 00:19:42.659
it was a person saying would you like to have a free sample of

00:19:42.660 --> 00:19:46.009
this and would you like to come into the shop and try this on.

00:19:46.009 --> 00:19:50.759
Or all of those other things that we're used to humans saying and we're used

00:19:50.759 --> 00:19:55.674
to ignoring or we're used to being out to assess who they are?

00:19:55.674 --> 00:19:57.954
What they're doing and why they're doing it?

00:19:57.954 --> 00:20:02.759
So that for me is the last area of real transparency where

00:20:02.759 --> 00:20:05.190
the license plates and where's the chain of

00:20:05.190 --> 00:20:07.860
responsibility and how do we have access to it.

00:20:07.859 --> 00:20:09.869
So talking about the interaction that you're having with some of

00:20:09.869 --> 00:20:12.909
these robots do you feel that the human biases of the people who

00:20:12.910 --> 00:20:15.900
are building design of these robots are actually

00:20:15.900 --> 00:20:20.646
impacting the kind of interaction we're having with it on the other end?

00:20:20.645 --> 00:20:28.019
Yeah, I think initially we had robots that were being built to the- how would you say,

00:20:28.019 --> 00:20:34.244
to the biases and desires and imagination of the robot builders.

00:20:34.244 --> 00:20:37.769
And also the limitations of

00:20:37.769 --> 00:20:43.710
the form factor around certain things so some robots came out looking

00:20:43.710 --> 00:20:47.924
female simply because you needed to have a wider hip base to

00:20:47.924 --> 00:20:50.200
incorporate whatever the mechanisms being used

00:20:50.200 --> 00:20:53.250
were and other robots came out looking something different.

00:20:53.250 --> 00:20:54.750
So we started off with

00:20:54.750 --> 00:21:01.140
purely physical mechanical limitations but then they became creatures of the imagination.

00:21:01.140 --> 00:21:10.110
And what's worrisome is that that starts to perpetuate biases and stereotypes.

00:21:10.109 --> 00:21:13.079
Now just as quickly as people start

00:21:13.079 --> 00:21:16.454
to become more informed and be looking at good design,

00:21:16.454 --> 00:21:24.419
I think we also have a negative pressure from businesses who can

00:21:24.420 --> 00:21:29.460
already quantify our responses to

00:21:29.460 --> 00:21:34.923
merchandising and start to use certain genders,

00:21:34.923 --> 00:21:38.670
voices and appearances based on what

00:21:38.670 --> 00:21:43.590
they've assessed is going to give us the most accurate and the...

00:21:43.589 --> 00:21:43.795
The best trigger if you will.

00:21:43.796 --> 00:21:45.390
Yeah the best trigger.

00:21:45.390 --> 00:21:50.009
Exactly. And one of the things that someone's brought up is you

00:21:50.009 --> 00:21:55.154
know where are the black robots.

00:21:55.154 --> 00:21:58.904
So you already have a lack of representation

00:21:58.904 --> 00:22:07.430
but I think the lack of representation is going to continue in really bad ways.

00:22:07.430 --> 00:22:11.769
Someone has expressed it really nicely and they've said

00:22:11.769 --> 00:22:17.750
that stereotypes are not a problem because they're false.

00:22:17.750 --> 00:22:20.805
They're a problem because they're only one of the truths.

00:22:20.805 --> 00:22:26.210
And whenever you use the stereotype you ignore all of the other truths.

00:22:26.210 --> 00:22:32.694
And so when we use those stereotype robots we're

00:22:32.694 --> 00:22:36.429
forcing a convergence on a very narrow set of

00:22:36.430 --> 00:22:40.233
representations that may be true and may be effective.

00:22:40.232 --> 00:22:46.348
But we're going to do away with having any of the other true representations.

00:22:46.348 --> 00:22:48.189
It'll be very off putting for a lot of people

00:22:48.190 --> 00:22:50.740
too if you're not in that target demographic.

00:22:50.740 --> 00:22:52.182
You're going to feel like I don't relate,

00:22:52.182 --> 00:22:54.879
I don't like the way that this thing is you know portraying itself to me.

00:22:54.880 --> 00:22:57.375
It's very off putting too. Right.

00:22:57.375 --> 00:22:59.890
We're going to see some very clear things around

00:22:59.890 --> 00:23:03.130
say one of the things that I've been tracking is the use of

00:23:03.130 --> 00:23:12.490
gendered stereotypes in work positions so kiosk robots tending to being female,

00:23:12.490 --> 00:23:18.265
assistant robots female, authoritative robots male.

00:23:18.265 --> 00:23:22.300
Now how do women break through the glass ceiling if we are

00:23:22.299 --> 00:23:28.359
creating a robotically amplified world in which women

00:23:28.359 --> 00:23:32.859
are the assistance and service providers rather than

00:23:32.859 --> 00:23:40.229
the executives or leaders or people with the voice of authority.

00:23:40.230 --> 00:23:44.964
This will be sending us a few steps backwards and we won't even see it coming.

00:23:44.964 --> 00:23:48.134
Yeah it's a really interesting concept right.

00:23:48.134 --> 00:23:50.509
When you hear people of genderising they're robots

00:23:50.509 --> 00:23:54.003
because you see it from time to time where someone will say,

00:23:54.003 --> 00:23:57.308
"Well, he's a good robot or she's a good robot."

00:23:57.308 --> 00:23:59.109
And I never quite understood why you

00:23:59.109 --> 00:24:00.766
would do something like that because really getting into the data,

00:24:00.767 --> 00:24:02.414
the robot is not a person.

00:24:02.414 --> 00:24:04.184
Like the robot has no feelings,

00:24:04.184 --> 00:24:06.759
the robot is just algorithms and

00:24:06.759 --> 00:24:11.643
sensors that are meant to work together to provide some sort of an experience,

00:24:11.643 --> 00:24:13.509
whatever that may be.

00:24:13.509 --> 00:24:20.009
So I prefer to call robot an it or that or they. What would you call it?

00:24:20.009 --> 00:24:23.525
Yeah absolutely. I try very hard to always call them it.

00:24:23.525 --> 00:24:27.670
I actually think that we should go ahead and develop a third set of

00:24:27.670 --> 00:24:34.750
pronouns similar to that third ontological awareness out of respect for what they are.

00:24:34.750 --> 00:24:38.634
And also to help us see them as

00:24:38.634 --> 00:24:45.369
the artificial embodied intelligences that a robot is.

00:24:45.369 --> 00:24:51.579
So I think we should first off this comes to overfeaturing a robot.

00:24:51.579 --> 00:24:57.814
Anything that adds trappings of gender should not be there.

00:24:57.815 --> 00:25:05.470
I really believe that we can create non-gendered asexual robots and we

00:25:05.470 --> 00:25:09.265
can utilize a new way of referring to them

00:25:09.265 --> 00:25:14.080
and that ultimately would be the safest thing for us as a society.

00:25:14.079 --> 00:25:17.259
Looking at some of the robots and the interactions that we have with them right now.

00:25:17.259 --> 00:25:21.525
Right. There's a lot of robots that you're starting to see move into factories right.

00:25:21.525 --> 00:25:24.970
There were the factories that we had of the past which were basically

00:25:24.970 --> 00:25:28.799
just gigantic robot arms and no humans were interacting.

00:25:28.799 --> 00:25:31.509
But now we're starting to see you know other companies like we think

00:25:31.509 --> 00:25:34.644
robotics is a great example right with their backs their robot

00:25:34.644 --> 00:25:37.960
where they're kind of showing hey this robot you know coexist in

00:25:37.960 --> 00:25:42.585
works alongside a human in a certain environment.

00:25:42.585 --> 00:25:45.640
It seems like that reaction is kind of mixed from a lot of

00:25:45.640 --> 00:25:49.895
people in these factories where it's like it's great that this robot is helping me out.

00:25:49.894 --> 00:25:54.674
But how long before this robot credit comes over and takes over my job.

00:25:54.674 --> 00:25:57.879
Do you think that we're going to get to a point where

00:25:57.880 --> 00:26:01.795
humans are going to be replaced by robots and factories altogether

00:26:01.795 --> 00:26:05.590
or do you think there's always going to be some level of existence of coexistence

00:26:05.589 --> 00:26:10.028
with a human and a robot in a factory working together?

00:26:10.028 --> 00:26:15.609
I believe that robots are always going to augment us rather than replace us.

00:26:15.609 --> 00:26:22.174
And I think at the moment the best thing to do is to say that robots take tasks not jobs.

00:26:22.174 --> 00:26:26.559
But I would like to see factories that don't have people in them.

00:26:26.559 --> 00:26:28.315
I think it's an inhuman job.

00:26:28.315 --> 00:26:31.815
It is the epitome of robotic labor.

00:26:31.815 --> 00:26:36.730
You know I don't believe that that is a job that feeds human need for

00:26:36.730 --> 00:26:42.519
social interaction or creativity or any of the things that we like.

00:26:42.519 --> 00:26:43.839
I think if people didn't need

00:26:43.839 --> 00:26:48.059
the money they wouldn't voluntarily go to work in a factory.

00:26:48.059 --> 00:26:51.669
And I think we have an opportunity to look at our world, our life,

00:26:51.670 --> 00:26:53.590
our economy, our society,

00:26:53.589 --> 00:26:57.349
and say we have an opportunity to remake everything.

00:26:57.349 --> 00:27:00.664
What are the things that people shouldn't be doing and they should not be doing

00:27:00.664 --> 00:27:04.349
a lot of the dull dirty dangerous jobs and it's one of

00:27:04.349 --> 00:27:08.054
the most striking arguments for autonomous vehicles

00:27:08.055 --> 00:27:13.890
is to reduce death on the roads you know

00:27:13.890 --> 00:27:16.440
automobile accidents are one of

00:27:16.440 --> 00:27:25.576
the greatest costs for- of human lives in in most societies these days.

00:27:25.576 --> 00:27:32.579
So that's definitely a reason to look at having

00:27:32.579 --> 00:27:36.074
autonomous vehicles which will have

00:27:36.075 --> 00:27:38.730
a much better safety record

00:27:38.730 --> 00:27:42.339
as long as we've got them to the point where they are definitely going to have.

00:27:42.339 --> 00:27:44.189
And I think you need to be at

00:27:44.190 --> 00:27:48.509
a much greater safety level than a human behind the wheel because

00:27:48.509 --> 00:27:55.769
we accept a certain amount of risk from human behavior.

00:27:55.769 --> 00:28:00.329
We will not accept that exact same amount of risk from robot behavior.

00:28:00.329 --> 00:28:04.154
Robots have to be significantly I think an order of magnitude

00:28:04.154 --> 00:28:09.049
better before we're going to want to wholesale adopt them into certain areas.

00:28:09.049 --> 00:28:12.634
But we shouldn't be mining underground directly.

00:28:12.634 --> 00:28:19.410
That's one of the most deadly jobs imaginable going into space is also deadly.

00:28:19.410 --> 00:28:22.080
I mean I definitely want to go to Mars but I

00:28:22.079 --> 00:28:24.869
don't want to spend unneeded amounts of time in space

00:28:24.869 --> 00:28:27.659
preparing habitats and building rockets and doing

00:28:27.660 --> 00:28:31.915
things that have huge exposure to radiation.

00:28:31.914 --> 00:28:34.769
So I think there's a whole class of jobs that we ought

00:28:34.769 --> 00:28:39.000
to automate completely as fast as possible.

00:28:39.000 --> 00:28:44.105
But every single robot that's out there needs to be built.

00:28:44.105 --> 00:28:47.390
It needs to be programmed and it needs to be managed.

00:28:47.390 --> 00:28:51.810
And I think we're going to have a huge class of employment that

00:28:51.809 --> 00:28:56.369
relates to looking after and training robots and it's

00:28:56.369 --> 00:28:58.859
going to range from the highly skilled you know

00:28:58.859 --> 00:29:04.469
more algorithmic jobs down to ones that are much more physical like oh it's

00:29:04.470 --> 00:29:08.414
missed plugging itself in down on aisle four

00:29:08.414 --> 00:29:13.279
let's go and walk along and make sure that they've all got good contacts to charge up.

00:29:13.279 --> 00:29:16.470
And while we're at it let's you know tidy that

00:29:16.470 --> 00:29:19.710
one up and we're slated to do routine maintenance.

00:29:19.710 --> 00:29:22.019
You know let's do these kind of things.

00:29:22.019 --> 00:29:25.825
You know I see robots and looking after robots.

00:29:25.825 --> 00:29:32.565
In many ways take the place that the automobile has had over the last century.

00:29:32.565 --> 00:29:38.519
So we've seen one transition where a huge chunk of

00:29:38.519 --> 00:29:41.039
human society used to be involved in looking after

00:29:41.039 --> 00:29:46.569
horses and then they transitioned to looking after automobiles.

00:29:46.569 --> 00:29:51.140
And I think there's going to be another transition to looking after the broader range.

00:29:51.141 --> 00:29:53.085
Let's say I call a car a robot,

00:29:53.085 --> 00:29:56.300
and it's just going to be many more and different.

00:29:56.300 --> 00:29:58.585
Do you think there just be more specialized in the sensitives,

00:29:58.585 --> 00:30:01.733
so let's say if you have a mechanic now you might have somebody who works on

00:30:01.733 --> 00:30:05.389
biped robots only or somebody who works on air vehicles only,

00:30:05.390 --> 00:30:07.860
do you think that's going to be kind of like the emergence of

00:30:07.859 --> 00:30:10.889
new jobs like well you're trained technically to

00:30:10.890 --> 00:30:13.170
do kind of all of them but you're really going to be focused on

00:30:13.170 --> 00:30:16.729
one area or one brand or one manufacturer?

00:30:16.729 --> 00:30:20.430
Oh, definitely you are going to have really infinite scope for

00:30:20.430 --> 00:30:24.900
specialization and ideally it won't be manufacturing

00:30:24.900 --> 00:30:28.200
specific because I think one of the things holding robotics

00:30:28.200 --> 00:30:30.539
back is a lack of some of

00:30:30.539 --> 00:30:33.359
the common standards and platforms that you're going to need to have.

00:30:33.359 --> 00:30:38.625
So you know we see this in very small scale with

00:30:38.625 --> 00:30:41.339
the importance of ROS and with things like the importance of

00:30:41.339 --> 00:30:44.299
a common standard for in defect on the end robot arms.

00:30:44.299 --> 00:30:47.644
So it doesn't really matter whether it's a Phantogram,

00:30:47.644 --> 00:30:53.299
a cougar arm or a universe robotics arm or a Sawyer from wreathing robotics.

00:30:53.299 --> 00:30:58.004
Those range of end effectors will all be able to be used on it.

00:30:58.005 --> 00:31:01.645
But where are some of the other things that we're still developing

00:31:01.644 --> 00:31:06.430
perhaps multi robot communication protocols and so forth.

00:31:06.430 --> 00:31:12.174
Ye, s maybe just using the Internet but what about in other situations.

00:31:12.174 --> 00:31:18.414
I think we're going to see more common standards emerge, we'll have to.

00:31:18.414 --> 00:31:21.000
But I think your specification will relate to form

00:31:21.000 --> 00:31:24.779
factor and will relate to industries so let's say

00:31:24.779 --> 00:31:29.069
a biped robot that does something on farms is going to be

00:31:29.069 --> 00:31:34.210
a vastly different biped robot to one that is doing something in clothing stores.

00:31:34.210 --> 00:31:34.559
Sure.

00:31:34.559 --> 00:31:42.398
So, I also think that there's a huge scope for creative employments.

00:31:42.398 --> 00:31:49.239
One of the jobs that you do with social robotics is write scripts and you know look at...

00:31:49.240 --> 00:31:51.039
Catalia Health as a great example of

00:31:51.039 --> 00:31:53.759
that half their staff as basically writers and psychologists.

00:31:53.759 --> 00:31:56.386
Yes. And companies like Anki,

00:31:56.386 --> 00:32:03.500
many of the others half of this stuff are game designers and visual illustrators.

00:32:03.500 --> 00:32:05.700
And I think also people who are

00:32:05.700 --> 00:32:11.455
dance and theater people because you're having to teach and create.

00:32:11.454 --> 00:32:15.519
The appropriate behaviors in whatever the narrow area

00:32:15.519 --> 00:32:19.779
is then you have to have domain expertise who understand you know what is

00:32:19.779 --> 00:32:24.430
important in a robot that has an interaction with you for the purposes of understanding

00:32:24.430 --> 00:32:27.430
your health needs and how different is that to

00:32:27.430 --> 00:32:31.595
a robot that is going to interact with you in a clothing store.

00:32:31.595 --> 00:32:34.120
And it's highly specialized but completely

00:32:34.119 --> 00:32:39.554
different modes of behavior and background knowledge required.

00:32:39.555 --> 00:32:43.450
But we underestimate the sheer physical.

00:32:43.450 --> 00:32:47.950
I mean robots if anyone can

00:32:47.950 --> 00:32:53.254
remember the very early days of the automobile and I personally can't,

00:32:53.253 --> 00:32:58.914
but they were all a little different and they all didn't work most of the time.

00:32:58.914 --> 00:33:00.110
More or less.

00:33:00.111 --> 00:33:03.820
So just physically looking after them.

00:33:03.819 --> 00:33:11.019
Sometimes the biggest thing we forget when we talk about robots entering the world is

00:33:11.019 --> 00:33:14.559
that they're going to be physical and it might be like I said

00:33:14.559 --> 00:33:18.549
as simple as putting it back on to charge properly.

00:33:18.549 --> 00:33:22.299
It might be moving something out of the way because it stopped working and

00:33:22.299 --> 00:33:26.845
maybe there is something blocking it so you can't tell he operated out of the way.

00:33:26.845 --> 00:33:28.990
There's going to be a heap of teleoperation

00:33:28.990 --> 00:33:32.890
and one of the big ways in which I think we need to

00:33:32.890 --> 00:33:36.400
interact with- where robots and humans will work

00:33:36.400 --> 00:33:40.590
together has to do with that remote operation.

00:33:40.589 --> 00:33:44.559
So I see a future where the robot can

00:33:44.559 --> 00:33:48.829
tell you where to find things on aisle four and offer to show you the way.

00:33:48.829 --> 00:33:53.349
But when you ask it if they think that this brand

00:33:53.349 --> 00:33:57.908
is better than that brand that immediately goes to the human operator...

00:33:57.909 --> 00:33:58.535
Right because with a robot when you use a product,

00:33:58.535 --> 00:33:58.536
let's say a shampoo or something like that.

00:33:58.536 --> 00:34:02.230
They can say, "Why would you want...?"

00:34:02.230 --> 00:34:08.320
Or necessarily have the ability to ask the right questions to understand who is it for?

00:34:08.320 --> 00:34:11.005
What is the this or

00:34:11.005 --> 00:34:16.599
perhaps just the trust level that we would have from an interaction with the person.

00:34:16.599 --> 00:34:19.750
DARPA has modified Dragon-Fly to put on

00:34:19.750 --> 00:34:23.860
a backpack onto this insect which basically controls

00:34:23.860 --> 00:34:27.970
its nervous systems so they can actually monitor and move the actual

00:34:27.969 --> 00:34:32.298
biological being in the way that they choose.

00:34:32.298 --> 00:34:35.581
Is this something that we should be kind of playing around with still?

00:34:35.581 --> 00:34:37.589
Or is this something that at this point in

00:34:37.590 --> 00:34:40.840
our technology that we feel like you know what we really don't need

00:34:40.840 --> 00:34:47.260
as many biologically created animals as possible we really should start

00:34:47.260 --> 00:34:49.735
focusing more on just purely robots

00:34:49.735 --> 00:34:54.394
or is this kind of like a necessary hurdle that we have to get over?

00:34:54.393 --> 00:34:57.604
I mean we see a lot with animal testing with makeup products and things like that

00:34:57.605 --> 00:35:01.860
but from a robot perspective it might be slightly concerning.

00:35:01.860 --> 00:35:07.450
Right because you're starting to say well we're just going to hijack this host and

00:35:07.449 --> 00:35:09.969
we're going to have this host do whatever we want it to do even though

00:35:09.969 --> 00:35:14.469
it's own physical being or entity of some form.

00:35:14.469 --> 00:35:19.494
There was a lot of outcry over the crowd from campaign for you know do it yourself

00:35:19.494 --> 00:35:25.344
mouse or rat brain hacking chips or cockroach.

00:35:25.344 --> 00:35:33.514
Cockroach hacking. There are several interesting ethical dilemmas and elements in there.

00:35:33.514 --> 00:35:38.199
When I see the fruits of this research being

00:35:38.199 --> 00:35:43.480
used to allow someone to use an embedded chip to

00:35:43.480 --> 00:35:51.144
move a prosthetic arm when they've had no movement then I think it's a good thing

00:35:51.144 --> 00:35:55.059
especially when I put it into the context of

00:35:55.059 --> 00:36:00.860
realizing that we still do incredible amount of research on animals.

00:36:00.860 --> 00:36:08.019
For cosmetics as you said for reasons that seem far less compelling but

00:36:08.019 --> 00:36:15.789
we kill a lot of animals and we torture a lot of animals for many reasons already.

00:36:15.789 --> 00:36:22.215
What's the real difference between putting chips in and remote controlling them.

00:36:22.215 --> 00:36:24.985
But it raises a couple of issues.

00:36:24.985 --> 00:36:28.414
One is what's the end goal of this?

00:36:28.414 --> 00:36:33.599
And I think the reality is we're starting to produce some very small flying robots.

00:36:33.599 --> 00:36:36.279
No where near as fuel efficient as

00:36:36.280 --> 00:36:39.970
your average insect bird but we're improving technologies

00:36:39.969 --> 00:36:42.864
all the time so I think you can see a future

00:36:42.864 --> 00:36:46.574
where we don't need to talk about it being a hijacked organism.

00:36:46.574 --> 00:36:50.974
It's purely a robotic miniature surveillance device.

00:36:50.974 --> 00:36:55.799
Is that OK for us to use something that most people might not be aware of.

00:36:55.800 --> 00:36:58.794
And why would we do that?

00:36:58.793 --> 00:37:02.079
And one of the reasons for doing a lot of

00:37:02.079 --> 00:37:08.275
biomorphic working robotics is this interesting two way street

00:37:08.275 --> 00:37:13.539
which we learn so much more about how biology works when we

00:37:13.539 --> 00:37:18.931
try to build artificial systems to imitate it and vice versa.

00:37:18.931 --> 00:37:22.599
We then learn so much more about how physical robotic,

00:37:22.599 --> 00:37:29.980
mechanical and electrical systems work when we try to replicate how the organisms work.

00:37:29.980 --> 00:37:33.340
So robotics feeds our understanding of

00:37:33.340 --> 00:37:36.970
biology which then feeds our understanding of robotics and

00:37:36.969 --> 00:37:44.589
this kind of feedback loop goes all the way back to Grey Walter's tortoises in the 50's.

00:37:44.590 --> 00:37:47.050
And he was a brain scientist and he built

00:37:47.050 --> 00:37:51.155
the first autonomous mobile robots analog and they were like

00:37:51.155 --> 00:37:53.580
flying robots and they displayed

00:37:53.579 --> 00:37:58.619
a very lifelike behavior simply through you know phototropic behaviors.

00:37:58.619 --> 00:38:01.539
And what inspired him was creating

00:38:01.539 --> 00:38:06.514
physical modeling of the workings of neurons in the brain.

00:38:06.514 --> 00:38:08.529
So there's one thing I do want to talk about

00:38:08.530 --> 00:38:13.380
because it's always the doom and gloom issue as you and I are both aware.

00:38:13.380 --> 00:38:16.720
Right. Is the idea that robots are taking all of our jobs.

00:38:16.719 --> 00:38:19.629
Everywhere you look it seems like lately there's another article that's popping

00:38:19.630 --> 00:38:23.250
up say robots are going to take all of our jobs by 2030.

00:38:23.250 --> 00:38:27.940
You know 80% of the jobs in the U.S. are going to be automated,

00:38:27.940 --> 00:38:31.500
farm workers are going to be out of their jobs in the next two years. Right.

00:38:31.500 --> 00:38:34.480
Like it's always to this level of like

00:38:34.480 --> 00:38:37.815
it's here and it's happening now and everything is going away and

00:38:37.815 --> 00:38:40.869
time to go grab you know your pitchforks

00:38:40.869 --> 00:38:44.900
and rise up against the robots before everything goes doom and gloom.

00:38:44.900 --> 00:38:46.420
What are your thoughts on kind of

00:38:46.420 --> 00:38:50.050
this portrayal in the media that you know robots are here to

00:38:50.050 --> 00:38:52.690
take everything away from people and people are going to

00:38:52.690 --> 00:38:55.450
be able to afford you know a life for their families.

00:38:55.449 --> 00:38:58.619
Then there's this concept of know taxation on top of these robots?

00:38:58.619 --> 00:39:01.703
The bonfire the robots issue.

00:39:01.704 --> 00:39:04.975
It is so completely wrong.

00:39:04.974 --> 00:39:08.815
Anybody who's developed robots has

00:39:08.815 --> 00:39:14.769
a much greater appreciation of just how far behind that state we are.

00:39:14.769 --> 00:39:19.780
I believe there are still fewer than two million industrial robots in the world.

00:39:19.780 --> 00:39:22.000
Global total today.

00:39:22.000 --> 00:39:25.809
And that's with China putting a huge amount of money into it

00:39:25.809 --> 00:39:29.500
so that they now have this here become I think

00:39:29.500 --> 00:39:31.989
the world's largest producer of robots as well as

00:39:31.989 --> 00:39:38.349
the world's largest consumer of robots pushing Germany out of the top spot.

00:39:38.349 --> 00:39:41.299
And yet the sum total two million.

00:39:41.300 --> 00:39:44.350
OK. It's going to take quite a while for us to

00:39:44.349 --> 00:39:47.815
spread sophisticated robotics technology around the globe.

00:39:47.815 --> 00:39:52.740
And then I think the other thing is to look at certain industries.

00:39:52.739 --> 00:39:56.469
And you mentioned farm workers.

00:39:56.469 --> 00:40:02.184
If you were to mention that in an agricultural area

00:40:02.184 --> 00:40:05.529
you would have everybody that owned a farm saying Can I have some

00:40:05.530 --> 00:40:08.850
of those because I can't get enough workers.

00:40:08.849 --> 00:40:11.860
And to look at this Labor Statistics,

00:40:11.860 --> 00:40:14.349
almost every country in the world has

00:40:14.349 --> 00:40:19.847
an aging population problem and it is worst in rural occupations.

00:40:19.847 --> 00:40:22.480
So you pretty much going to have every farmer in

00:40:22.480 --> 00:40:26.039
the world retiring in the next five to 10 years.

00:40:26.039 --> 00:40:29.110
And at the same time we're shutting down

00:40:29.110 --> 00:40:33.120
all of the cheap labor sources and seasonal labor sources that used to be.

00:40:33.119 --> 00:40:37.569
So how are we going to prevent the world from starving and yet keep

00:40:37.570 --> 00:40:42.410
food prices you know at the level that we expect them to be.

00:40:42.409 --> 00:40:46.269
And you know we really do have to look at doubling food production over

00:40:46.269 --> 00:40:50.942
the next generation not just keeping things as they are either.

00:40:50.943 --> 00:40:56.810
So I see some huge areas where we actually have big problems.

00:40:56.809 --> 00:41:00.489
We have global challenges that we need to do something about.

00:41:00.489 --> 00:41:05.070
So whenever people talk about robots as a problem I stop and I go,

00:41:05.070 --> 00:41:07.989
"Well what are the world's biggest problems?

00:41:07.989 --> 00:41:11.784
And will robotics have a role to play in solving,

00:41:11.784 --> 00:41:15.864
fixing or you know making those problems a little bit better.

00:41:15.864 --> 00:41:17.851
And I think absolutely yes."

00:41:17.851 --> 00:41:24.519
And if anyone is on the fence then focus on the ways that we need to solve

00:41:24.519 --> 00:41:26.800
these problems and think about

00:41:26.800 --> 00:41:32.880
the creative technologies that we have to put into play to fix global challenges.

00:41:32.880 --> 00:41:39.305
And I think that's the most productive thing you can do with your time rather than

00:41:39.304 --> 00:41:46.299
to spend it worried or in fear which first off I don't think is founded.

00:41:46.300 --> 00:41:51.519
But second off if you got that energy there is a lot of good things that you can do.

00:41:51.519 --> 00:41:53.465
So focus on those.

00:41:53.465 --> 00:41:59.434
And yeah and not on that fear.

00:41:59.434 --> 00:42:01.123
Well that's it for today,

00:42:01.123 --> 00:42:05.434
Udacity's Explorer Human-Robot Interaction and Robotic Ethics.

00:42:05.434 --> 00:42:08.329
Andra Keay. Thank you very much for your time, I appreciate it.

00:42:08.329 --> 00:42:10.110
Thanks so much Mike.

