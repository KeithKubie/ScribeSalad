WEBVTT
Kind: captions
Language: en

00:00:02.600 --> 00:00:04.480
 
welcome breasts I'm really glad you

00:00:04.480 --> 00:00:04.490
welcome breasts I'm really glad you
 

00:00:04.490 --> 00:00:07.030
welcome breasts I'm really glad you
could join us here today thank you thank

00:00:07.030 --> 00:00:07.040
could join us here today thank you thank
 

00:00:07.040 --> 00:00:09.789
could join us here today thank you thank
you Andrew so you know today you're the

00:00:09.789 --> 00:00:09.799
you Andrew so you know today you're the
 

00:00:09.799 --> 00:00:12.759
you Andrew so you know today you're the
director of research at Apple and you

00:00:12.759 --> 00:00:12.769
director of research at Apple and you
 

00:00:12.769 --> 00:00:14.799
director of research at Apple and you
also have a faculty and professor Rowan

00:00:14.799 --> 00:00:14.809
also have a faculty and professor Rowan
 

00:00:14.809 --> 00:00:17.440
also have a faculty and professor Rowan
County mother University so I'd love to

00:00:17.440 --> 00:00:17.450
County mother University so I'd love to
 

00:00:17.450 --> 00:00:20.139
County mother University so I'd love to
hear a bit about your personal story how

00:00:20.139 --> 00:00:20.149
hear a bit about your personal story how
 

00:00:20.149 --> 00:00:22.569
hear a bit about your personal story how
did you end up doing this you know deep

00:00:22.569 --> 00:00:22.579
did you end up doing this you know deep
 

00:00:22.579 --> 00:00:25.420
did you end up doing this you know deep
learning work that you do yeah it's it's

00:00:25.420 --> 00:00:25.430
learning work that you do yeah it's it's
 

00:00:25.430 --> 00:00:27.609
learning work that you do yeah it's it's
actually does some extent it was I

00:00:27.609 --> 00:00:27.619
actually does some extent it was I
 

00:00:27.619 --> 00:00:30.249
actually does some extent it was I
started in deploying to some extent by

00:00:30.249 --> 00:00:30.259
started in deploying to some extent by
 

00:00:30.259 --> 00:00:33.850
started in deploying to some extent by
luck I did my master's degree at Toronto

00:00:33.850 --> 00:00:33.860
luck I did my master's degree at Toronto
 

00:00:33.860 --> 00:00:35.770
luck I did my master's degree at Toronto
and then I took a year off I was

00:00:35.770 --> 00:00:35.780
and then I took a year off I was
 

00:00:35.780 --> 00:00:37.300
and then I took a year off I was
actually working in the financial sector

00:00:37.300 --> 00:00:37.310
actually working in the financial sector
 

00:00:37.310 --> 00:00:40.780
actually working in the financial sector
it's a bit surprising and at that time I

00:00:40.780 --> 00:00:40.790
it's a bit surprising and at that time I
 

00:00:40.790 --> 00:00:42.430
it's a bit surprising and at that time I
wasn't quite sure where they want to go

00:00:42.430 --> 00:00:42.440
wasn't quite sure where they want to go
 

00:00:42.440 --> 00:00:44.500
wasn't quite sure where they want to go
for my PhD or not and then something

00:00:44.500 --> 00:00:44.510
for my PhD or not and then something
 

00:00:44.510 --> 00:00:46.060
for my PhD or not and then something
happened something surprising happened I

00:00:46.060 --> 00:00:46.070
happened something surprising happened I
 

00:00:46.070 --> 00:00:48.819
happened something surprising happened I
was going to work one morning and I bump

00:00:48.819 --> 00:00:48.829
was going to work one morning and I bump
 

00:00:48.829 --> 00:00:52.690
was going to work one morning and I bump
into Jack Hinton and Jeff told me hey I

00:00:52.690 --> 00:00:52.700
into Jack Hinton and Jeff told me hey I
 

00:00:52.700 --> 00:00:55.390
into Jack Hinton and Jeff told me hey I
have this terrific idea come to my

00:00:55.390 --> 00:00:55.400
have this terrific idea come to my
 

00:00:55.400 --> 00:00:57.400
have this terrific idea come to my
office I'll show you and so we basically

00:00:57.400 --> 00:00:57.410
office I'll show you and so we basically
 

00:00:57.410 --> 00:00:59.560
office I'll show you and so we basically
work together and he started telling me

00:00:59.560 --> 00:00:59.570
work together and he started telling me
 

00:00:59.570 --> 00:01:01.990
work together and he started telling me
about you know these Boltzmann machines

00:01:01.990 --> 00:01:02.000
about you know these Boltzmann machines
 

00:01:02.000 --> 00:01:04.600
about you know these Boltzmann machines
and contrastive divergence and some of

00:01:04.600 --> 00:01:04.610
and contrastive divergence and some of
 

00:01:04.610 --> 00:01:06.760
and contrastive divergence and some of
some of the tricks which I didn't at

00:01:06.760 --> 00:01:06.770
some of the tricks which I didn't at
 

00:01:06.770 --> 00:01:08.469
some of the tricks which I didn't at
that time quite understand what he was

00:01:08.469 --> 00:01:08.479
that time quite understand what he was
 

00:01:08.479 --> 00:01:11.560
that time quite understand what he was
talking about but that really really

00:01:11.560 --> 00:01:11.570
talking about but that really really
 

00:01:11.570 --> 00:01:13.899
talking about but that really really
excited that was very exciting and

00:01:13.899 --> 00:01:13.909
excited that was very exciting and
 

00:01:13.909 --> 00:01:16.180
excited that was very exciting and
really excited me and then basically

00:01:16.180 --> 00:01:16.190
really excited me and then basically
 

00:01:16.190 --> 00:01:18.960
really excited me and then basically
within three months I started my PhD

00:01:18.960 --> 00:01:18.970
within three months I started my PhD
 

00:01:18.970 --> 00:01:23.170
within three months I started my PhD
with Jeff so so that was that was kind

00:01:23.170 --> 00:01:23.180
with Jeff so so that was that was kind
 

00:01:23.180 --> 00:01:25.570
with Jeff so so that was that was kind
of like the beginning because that was

00:01:25.570 --> 00:01:25.580
of like the beginning because that was
 

00:01:25.580 --> 00:01:29.469
of like the beginning because that was
back in 2005-2006 and this is where you

00:01:29.469 --> 00:01:29.479
back in 2005-2006 and this is where you
 

00:01:29.479 --> 00:01:31.840
back in 2005-2006 and this is where you
know some of the regional deploying

00:01:31.840 --> 00:01:31.850
know some of the regional deploying
 

00:01:31.850 --> 00:01:33.490
know some of the regional deploying
algorithms using restrictive Boltzmann

00:01:33.490 --> 00:01:33.500
algorithms using restrictive Boltzmann
 

00:01:33.500 --> 00:01:35.560
algorithms using restrictive Boltzmann
and supervised spec training were kind

00:01:35.560 --> 00:01:35.570
and supervised spec training were kind
 

00:01:35.570 --> 00:01:38.890
and supervised spec training were kind
of popping up and so you know that's

00:01:38.890 --> 00:01:38.900
of popping up and so you know that's
 

00:01:38.900 --> 00:01:41.140
of popping up and so you know that's
that's how I started it was really you

00:01:41.140 --> 00:01:41.150
that's how I started it was really you
 

00:01:41.150 --> 00:01:43.870
that's how I started it was really you
know that one particular morning when I

00:01:43.870 --> 00:01:43.880
know that one particular morning when I
 

00:01:43.880 --> 00:01:46.000
know that one particular morning when I
bumped into Jeff completely changed my

00:01:46.000 --> 00:01:46.010
bumped into Jeff completely changed my
 

00:01:46.010 --> 00:01:49.390
bumped into Jeff completely changed my
my future career moving forward and then

00:01:49.390 --> 00:01:49.400
my future career moving forward and then
 

00:01:49.400 --> 00:01:52.480
my future career moving forward and then
in fact you were a co-author on you know

00:01:52.480 --> 00:01:52.490
in fact you were a co-author on you know
 

00:01:52.490 --> 00:01:54.370
in fact you were a co-author on you know
one of the very early papers on

00:01:54.370 --> 00:01:54.380
one of the very early papers on
 

00:01:54.380 --> 00:01:56.440
one of the very early papers on
restricted Boltzmann machines there

00:01:56.440 --> 00:01:56.450
restricted Boltzmann machines there
 

00:01:56.450 --> 00:01:58.060
restricted Boltzmann machines there
really helped with this resurgence of

00:01:58.060 --> 00:01:58.070
really helped with this resurgence of
 

00:01:58.070 --> 00:02:00.520
really helped with this resurgence of
neural networks and deep learning tell

00:02:00.520 --> 00:02:00.530
neural networks and deep learning tell
 

00:02:00.530 --> 00:02:02.380
neural networks and deep learning tell
me a bit more what that was like you're

00:02:02.380 --> 00:02:02.390
me a bit more what that was like you're
 

00:02:02.390 --> 00:02:04.420
me a bit more what that was like you're
working on that seven oh yeah this was

00:02:04.420 --> 00:02:04.430
working on that seven oh yeah this was
 

00:02:04.430 --> 00:02:06.910
working on that seven oh yeah this was
this was actually a really this was

00:02:06.910 --> 00:02:06.920
this was actually a really this was
 

00:02:06.920 --> 00:02:08.469
this was actually a really this was
exciting year I was a first year it was

00:02:08.469 --> 00:02:08.479
exciting year I was a first year it was
 

00:02:08.479 --> 00:02:12.030
exciting year I was a first year it was
my first year as a PhD student and

00:02:12.030 --> 00:02:12.040
my first year as a PhD student and
 

00:02:12.040 --> 00:02:15.220
my first year as a PhD student and
Jeff and I we're trying to explore these

00:02:15.220 --> 00:02:15.230
Jeff and I we're trying to explore these
 

00:02:15.230 --> 00:02:17.380
Jeff and I we're trying to explore these
ideas of using restricted Boltzmann's

00:02:17.380 --> 00:02:17.390
ideas of using restricted Boltzmann's
 

00:02:17.390 --> 00:02:20.080
ideas of using restricted Boltzmann's
and and using pre-training tricks to

00:02:20.080 --> 00:02:20.090
and and using pre-training tricks to
 

00:02:20.090 --> 00:02:23.020
and and using pre-training tricks to
train multiple layers and specifically

00:02:23.020 --> 00:02:23.030
train multiple layers and specifically
 

00:02:23.030 --> 00:02:25.360
train multiple layers and specifically
we will try to focus on autoencoders you

00:02:25.360 --> 00:02:25.370
we will try to focus on autoencoders you
 

00:02:25.370 --> 00:02:26.980
we will try to focus on autoencoders you
know how do we do an only an extension

00:02:26.980 --> 00:02:26.990
know how do we do an only an extension
 

00:02:26.990 --> 00:02:30.670
know how do we do an only an extension
of PCA effectively and it was very

00:02:30.670 --> 00:02:30.680
of PCA effectively and it was very
 

00:02:30.680 --> 00:02:32.110
of PCA effectively and it was very
exciting because we've got these systems

00:02:32.110 --> 00:02:32.120
exciting because we've got these systems
 

00:02:32.120 --> 00:02:34.540
exciting because we've got these systems
to work on em these digits which was

00:02:34.540 --> 00:02:34.550
to work on em these digits which was
 

00:02:34.550 --> 00:02:37.030
to work on em these digits which was
exciting but then the next steps for us

00:02:37.030 --> 00:02:37.040
exciting but then the next steps for us
 

00:02:37.040 --> 00:02:39.790
exciting but then the next steps for us
were to really see whether we can extend

00:02:39.790 --> 00:02:39.800
were to really see whether we can extend
 

00:02:39.800 --> 00:02:42.010
were to really see whether we can extend
these models to dealing with phases so

00:02:42.010 --> 00:02:42.020
these models to dealing with phases so
 

00:02:42.020 --> 00:02:44.500
these models to dealing with phases so
remember we had this automated phases

00:02:44.500 --> 00:02:44.510
remember we had this automated phases
 

00:02:44.510 --> 00:02:46.420
remember we had this automated phases
data set and then we started looking at

00:02:46.420 --> 00:02:46.430
data set and then we started looking at
 

00:02:46.430 --> 00:02:48.070
data set and then we started looking at
can we do compression for document so we

00:02:48.070 --> 00:02:48.080
can we do compression for document so we
 

00:02:48.080 --> 00:02:49.810
can we do compression for document so we
started looking in all these different

00:02:49.810 --> 00:02:49.820
started looking in all these different
 

00:02:49.820 --> 00:02:53.890
started looking in all these different
data you know real-valued count binary

00:02:53.890 --> 00:02:53.900
data you know real-valued count binary
 

00:02:53.900 --> 00:02:58.090
data you know real-valued count binary
and throughout you know a year it was I

00:02:58.090 --> 00:02:58.100
and throughout you know a year it was I
 

00:02:58.100 --> 00:03:00.190
and throughout you know a year it was I
was a first-year PhD students it was a

00:03:00.190 --> 00:03:00.200
was a first-year PhD students it was a
 

00:03:00.200 --> 00:03:04.560
was a first-year PhD students it was a
big learning experience for me but and

00:03:04.560 --> 00:03:04.570
big learning experience for me but and
 

00:03:04.570 --> 00:03:07.480
big learning experience for me but and
really within six or seven months we

00:03:07.480 --> 00:03:07.490
really within six or seven months we
 

00:03:07.490 --> 00:03:09.700
really within six or seven months we
were able to get really interesting

00:03:09.700 --> 00:03:09.710
were able to get really interesting
 

00:03:09.710 --> 00:03:10.990
were able to get really interesting
results and really good results

00:03:10.990 --> 00:03:11.000
results and really good results
 

00:03:11.000 --> 00:03:12.670
results and really good results
something that we you know we were able

00:03:12.670 --> 00:03:12.680
something that we you know we were able
 

00:03:12.680 --> 00:03:14.710
something that we you know we were able
to train these very deep autoencoders

00:03:14.710 --> 00:03:14.720
to train these very deep autoencoders
 

00:03:14.720 --> 00:03:16.030
to train these very deep autoencoders
this is something that you couldn't do

00:03:16.030 --> 00:03:16.040
this is something that you couldn't do
 

00:03:16.040 --> 00:03:19.479
this is something that you couldn't do
at that time using sort of traditional

00:03:19.479 --> 00:03:19.489
at that time using sort of traditional
 

00:03:19.489 --> 00:03:22.540
at that time using sort of traditional
optimization techniques and then it's

00:03:22.540 --> 00:03:22.550
optimization techniques and then it's
 

00:03:22.550 --> 00:03:24.660
optimization techniques and then it's
you know it turns out it's a really

00:03:24.660 --> 00:03:24.670
you know it turns out it's a really
 

00:03:24.670 --> 00:03:28.410
you know it turns out it's a really
really exciting paper for us that was

00:03:28.410 --> 00:03:28.420
really exciting paper for us that was
 

00:03:28.420 --> 00:03:31.750
really exciting paper for us that was
that was super exciting year because it

00:03:31.750 --> 00:03:31.760
that was super exciting year because it
 

00:03:31.760 --> 00:03:34.150
that was super exciting year because it
was a lot of learning for me but at the

00:03:34.150 --> 00:03:34.160
was a lot of learning for me but at the
 

00:03:34.160 --> 00:03:37.330
was a lot of learning for me but at the
same time the results turn out to be you

00:03:37.330 --> 00:03:37.340
same time the results turn out to be you
 

00:03:37.340 --> 00:03:39.820
same time the results turn out to be you
know really really impressive for what

00:03:39.820 --> 00:03:39.830
know really really impressive for what
 

00:03:39.830 --> 00:03:43.000
know really really impressive for what
we were trying to do so in the early

00:03:43.000 --> 00:03:43.010
we were trying to do so in the early
 

00:03:43.010 --> 00:03:44.740
we were trying to do so in the early
days of this resurgence of deep learning

00:03:44.740 --> 00:03:44.750
days of this resurgence of deep learning
 

00:03:44.750 --> 00:03:47.550
days of this resurgence of deep learning
or a lot of the activity was centered on

00:03:47.550 --> 00:03:47.560
or a lot of the activity was centered on
 

00:03:47.560 --> 00:03:49.630
or a lot of the activity was centered on
restricted Boltzmann machines and then

00:03:49.630 --> 00:03:49.640
restricted Boltzmann machines and then
 

00:03:49.640 --> 00:03:53.290
restricted Boltzmann machines and then
people see machines as a there's still a

00:03:53.290 --> 00:03:53.300
people see machines as a there's still a
 

00:03:53.300 --> 00:03:54.759
people see machines as a there's still a
lot of exciting research they're being

00:03:54.759 --> 00:03:54.769
lot of exciting research they're being
 

00:03:54.769 --> 00:03:56.830
lot of exciting research they're being
done including some in your group but

00:03:56.830 --> 00:03:56.840
done including some in your group but
 

00:03:56.840 --> 00:03:59.710
done including some in your group but
what's happening with both machines yeah

00:03:59.710 --> 00:03:59.720
what's happening with both machines yeah
 

00:03:59.720 --> 00:04:00.880
what's happening with both machines yeah
that's it that's a very good question I

00:04:00.880 --> 00:04:00.890
that's it that's a very good question I
 

00:04:00.890 --> 00:04:04.540
that's it that's a very good question I
think that in the early days the way

00:04:04.540 --> 00:04:04.550
think that in the early days the way
 

00:04:04.550 --> 00:04:06.160
think that in the early days the way
that we were using restricted Boltzmann

00:04:06.160 --> 00:04:06.170
that we were using restricted Boltzmann
 

00:04:06.170 --> 00:04:08.440
that we were using restricted Boltzmann
machines is you sort of can imagine

00:04:08.440 --> 00:04:08.450
machines is you sort of can imagine
 

00:04:08.450 --> 00:04:10.090
machines is you sort of can imagine
training a stack of these restricted

00:04:10.090 --> 00:04:10.100
training a stack of these restricted
 

00:04:10.100 --> 00:04:12.040
training a stack of these restricted
both machines that would allow you to

00:04:12.040 --> 00:04:12.050
both machines that would allow you to
 

00:04:12.050 --> 00:04:14.500
both machines that would allow you to
learn effectively one layer at a time

00:04:14.500 --> 00:04:14.510
learn effectively one layer at a time
 

00:04:14.510 --> 00:04:16.570
learn effectively one layer at a time
and there's a good theory behind you

00:04:16.570 --> 00:04:16.580
and there's a good theory behind you
 

00:04:16.580 --> 00:04:18.009
and there's a good theory behind you
know when you add a particular layer it

00:04:18.009 --> 00:04:18.019
know when you add a particular layer it
 

00:04:18.019 --> 00:04:19.479
know when you add a particular layer it
improves the variation bound and so

00:04:19.479 --> 00:04:19.489
improves the variation bound and so
 

00:04:19.489 --> 00:04:21.520
improves the variation bound and so
forth under certain conditions so there

00:04:21.520 --> 00:04:21.530
forth under certain conditions so there
 

00:04:21.530 --> 00:04:23.469
forth under certain conditions so there
was a theoretical justification and

00:04:23.469 --> 00:04:23.479
was a theoretical justification and
 

00:04:23.479 --> 00:04:24.610
was a theoretical justification and
these models were working

00:04:24.610 --> 00:04:24.620
these models were working
 

00:04:24.620 --> 00:04:26.830
these models were working
well in terms of being able to

00:04:26.830 --> 00:04:26.840
well in terms of being able to
 

00:04:26.840 --> 00:04:29.050
well in terms of being able to
pre-trained these systems and then

00:04:29.050 --> 00:04:29.060
pre-trained these systems and then
 

00:04:29.060 --> 00:04:32.950
pre-trained these systems and then
around 2009/2010 once the computer

00:04:32.950 --> 00:04:32.960
around 2009/2010 once the computer
 

00:04:32.960 --> 00:04:37.030
around 2009/2010 once the computer
started showing up you know GPUs then a

00:04:37.030 --> 00:04:37.040
started showing up you know GPUs then a
 

00:04:37.040 --> 00:04:38.980
started showing up you know GPUs then a
lot of us started realizing that

00:04:38.980 --> 00:04:38.990
lot of us started realizing that
 

00:04:38.990 --> 00:04:41.590
lot of us started realizing that
actually directly optimizing these deep

00:04:41.590 --> 00:04:41.600
actually directly optimizing these deep
 

00:04:41.600 --> 00:04:45.060
actually directly optimizing these deep
neural networks was you know was giving

00:04:45.060 --> 00:04:45.070
neural networks was you know was giving
 

00:04:45.070 --> 00:04:47.350
neural networks was you know was giving
similar results or even better results

00:04:47.350 --> 00:04:47.360
similar results or even better results
 

00:04:47.360 --> 00:04:49.450
similar results or even better results
so just standard back problems out the

00:04:49.450 --> 00:04:49.460
so just standard back problems out the
 

00:04:49.460 --> 00:04:51.460
so just standard back problems out the
pre-training or restricted Boltzmann

00:04:51.460 --> 00:04:51.470
pre-training or restricted Boltzmann
 

00:04:51.470 --> 00:04:52.540
pre-training or restricted Boltzmann
machine that's right that's right

00:04:52.540 --> 00:04:52.550
machine that's right that's right
 

00:04:52.550 --> 00:04:54.460
machine that's right that's right
and that's sort of over you know three

00:04:54.460 --> 00:04:54.470
and that's sort of over you know three
 

00:04:54.470 --> 00:04:56.200
and that's sort of over you know three
or four years and it was exciting to the

00:04:56.200 --> 00:04:56.210
or four years and it was exciting to the
 

00:04:56.210 --> 00:04:57.490
or four years and it was exciting to the
whole community because people thought

00:04:57.490 --> 00:04:57.500
whole community because people thought
 

00:04:57.500 --> 00:04:58.840
whole community because people thought
that wow you can actually train these

00:04:58.840 --> 00:04:58.850
that wow you can actually train these
 

00:04:58.850 --> 00:05:01.000
that wow you can actually train these
deep models using these pre training

00:05:01.000 --> 00:05:01.010
deep models using these pre training
 

00:05:01.010 --> 00:05:04.659
deep models using these pre training
mechanisms and then you know with more

00:05:04.659 --> 00:05:04.669
mechanisms and then you know with more
 

00:05:04.669 --> 00:05:06.279
mechanisms and then you know with more
compute people start realizing that you

00:05:06.279 --> 00:05:06.289
compute people start realizing that you
 

00:05:06.289 --> 00:05:07.779
compute people start realizing that you
can just basically do standard back

00:05:07.779 --> 00:05:07.789
can just basically do standard back
 

00:05:07.789 --> 00:05:09.070
can just basically do standard back
propagation something that we couldn't

00:05:09.070 --> 00:05:09.080
propagation something that we couldn't
 

00:05:09.080 --> 00:05:12.820
propagation something that we couldn't
do back in 2005 or you know 2004 because

00:05:12.820 --> 00:05:12.830
do back in 2005 or you know 2004 because
 

00:05:12.830 --> 00:05:16.300
do back in 2005 or you know 2004 because
it would take us months to do it on CPUs

00:05:16.300 --> 00:05:16.310
it would take us months to do it on CPUs
 

00:05:16.310 --> 00:05:19.920
it would take us months to do it on CPUs
and so that was that was a big change

00:05:19.920 --> 00:05:19.930
and so that was that was a big change
 

00:05:19.930 --> 00:05:22.420
and so that was that was a big change
the other thing that I think that we

00:05:22.420 --> 00:05:22.430
the other thing that I think that we
 

00:05:22.430 --> 00:05:24.520
the other thing that I think that we
haven't really figured out what to do

00:05:24.520 --> 00:05:24.530
haven't really figured out what to do
 

00:05:24.530 --> 00:05:26.650
haven't really figured out what to do
with you know both machines and deep

00:05:26.650 --> 00:05:26.660
with you know both machines and deep
 

00:05:26.660 --> 00:05:28.510
with you know both machines and deep
Boltzmann machines I believe they're

00:05:28.510 --> 00:05:28.520
Boltzmann machines I believe they're
 

00:05:28.520 --> 00:05:30.010
Boltzmann machines I believe they're
very powerful models because you can

00:05:30.010 --> 00:05:30.020
very powerful models because you can
 

00:05:30.020 --> 00:05:32.080
very powerful models because you can
think of them as generative models you

00:05:32.080 --> 00:05:32.090
think of them as generative models you
 

00:05:32.090 --> 00:05:33.120
think of them as generative models you
know they try to model complex

00:05:33.120 --> 00:05:33.130
know they try to model complex
 

00:05:33.130 --> 00:05:36.339
know they try to model complex
distributions in the data but when we

00:05:36.339 --> 00:05:36.349
distributions in the data but when we
 

00:05:36.349 --> 00:05:38.339
distributions in the data but when we
start looking at learning algorithms

00:05:38.339 --> 00:05:38.349
start looking at learning algorithms
 

00:05:38.349 --> 00:05:40.480
start looking at learning algorithms
learning algorithms right now they

00:05:40.480 --> 00:05:40.490
learning algorithms right now they
 

00:05:40.490 --> 00:05:42.129
learning algorithms right now they
require using you know Markov chain

00:05:42.129 --> 00:05:42.139
require using you know Markov chain
 

00:05:42.139 --> 00:05:44.200
require using you know Markov chain
Monte Carlo in variational learning and

00:05:44.200 --> 00:05:44.210
Monte Carlo in variational learning and
 

00:05:44.210 --> 00:05:47.560
Monte Carlo in variational learning and
such which is not a scalable as back

00:05:47.560 --> 00:05:47.570
such which is not a scalable as back
 

00:05:47.570 --> 00:05:51.370
such which is not a scalable as back
propagation algorithm so so we get have

00:05:51.370 --> 00:05:51.380
propagation algorithm so so we get have
 

00:05:51.380 --> 00:05:54.510
propagation algorithm so so we get have
to figure out more efficient ways of

00:05:54.510 --> 00:05:54.520
to figure out more efficient ways of
 

00:05:54.520 --> 00:05:56.439
to figure out more efficient ways of
training these models and also the use

00:05:56.439 --> 00:05:56.449
training these models and also the use
 

00:05:56.449 --> 00:05:58.589
training these models and also the use
of convolution it's something that's

00:05:58.589 --> 00:05:58.599
of convolution it's something that's
 

00:05:58.599 --> 00:06:02.469
of convolution it's something that's
fairly difficult to integrate into these

00:06:02.469 --> 00:06:02.479
fairly difficult to integrate into these
 

00:06:02.479 --> 00:06:04.450
fairly difficult to integrate into these
models I remember some of your work on

00:06:04.450 --> 00:06:04.460
models I remember some of your work on
 

00:06:04.460 --> 00:06:07.600
models I remember some of your work on
on using provost ik max pooling for sort

00:06:07.600 --> 00:06:07.610
on using provost ik max pooling for sort
 

00:06:07.610 --> 00:06:09.990
on using provost ik max pooling for sort
of building these generative models of

00:06:09.990 --> 00:06:10.000
of building these generative models of
 

00:06:10.000 --> 00:06:13.960
of building these generative models of
different objects and using these ideas

00:06:13.960 --> 00:06:13.970
different objects and using these ideas
 

00:06:13.970 --> 00:06:16.300
different objects and using these ideas
of convolution was also very very

00:06:16.300 --> 00:06:16.310
of convolution was also very very
 

00:06:16.310 --> 00:06:17.740
of convolution was also very very
exciting but at the same time it's still

00:06:17.740 --> 00:06:17.750
exciting but at the same time it's still
 

00:06:17.750 --> 00:06:20.050
exciting but at the same time it's still
extremely hard to train these models so

00:06:20.050 --> 00:06:20.060
extremely hard to train these models so
 

00:06:20.060 --> 00:06:22.060
extremely hard to train these models so
it's unlikely Israel yes how much these

00:06:22.060 --> 00:06:22.070
it's unlikely Israel yes how much these
 

00:06:22.070 --> 00:06:23.980
it's unlikely Israel yes how much these
work right and so we still have to

00:06:23.980 --> 00:06:23.990
work right and so we still have to
 

00:06:23.990 --> 00:06:28.180
work right and so we still have to
figure out water I on the on the other

00:06:28.180 --> 00:06:28.190
figure out water I on the on the other
 

00:06:28.190 --> 00:06:29.890
figure out water I on the on the other
side some of the recent work using

00:06:29.890 --> 00:06:29.900
side some of the recent work using
 

00:06:29.900 --> 00:06:31.659
side some of the recent work using
variational encoders for example which

00:06:31.659 --> 00:06:31.669
variational encoders for example which
 

00:06:31.669 --> 00:06:33.430
variational encoders for example which
could be viewed as directed versions of

00:06:33.430 --> 00:06:33.440
could be viewed as directed versions of
 

00:06:33.440 --> 00:06:36.100
could be viewed as directed versions of
Boltzmann machines we have figured out a

00:06:36.100 --> 00:06:36.110
Boltzmann machines we have figured out a
 

00:06:36.110 --> 00:06:38.409
Boltzmann machines we have figured out a
ways of of training these models was a

00:06:38.409 --> 00:06:38.419
ways of of training these models was a
 

00:06:38.419 --> 00:06:40.630
ways of of training these models was a
work by Maxwell and in there there

00:06:40.630 --> 00:06:40.640
work by Maxwell and in there there
 

00:06:40.640 --> 00:06:43.330
work by Maxwell and in there there
Kingma on using you know we pair with

00:06:43.330 --> 00:06:43.340
Kingma on using you know we pair with
 

00:06:43.340 --> 00:06:45.969
Kingma on using you know we pair with
relation tricks and now we can use back

00:06:45.969 --> 00:06:45.979
relation tricks and now we can use back
 

00:06:45.979 --> 00:06:49.149
relation tricks and now we can use back
propagation algorithm within the

00:06:49.149 --> 00:06:49.159
propagation algorithm within the
 

00:06:49.159 --> 00:06:51.040
propagation algorithm within the
stochastic system which is which is

00:06:51.040 --> 00:06:51.050
stochastic system which is which is
 

00:06:51.050 --> 00:06:53.649
stochastic system which is which is
driving a lot of progress right now but

00:06:53.649 --> 00:06:53.659
driving a lot of progress right now but
 

00:06:53.659 --> 00:06:55.330
driving a lot of progress right now but
we haven't quite figured out how to do

00:06:55.330 --> 00:06:55.340
we haven't quite figured out how to do
 

00:06:55.340 --> 00:06:58.120
we haven't quite figured out how to do
that in in the case of Boltzmann machine

00:06:58.120 --> 00:06:58.130
that in in the case of Boltzmann machine
 

00:06:58.130 --> 00:07:00.490
that in in the case of Boltzmann machine
so so that's a very interesting

00:07:00.490 --> 00:07:00.500
so so that's a very interesting
 

00:07:00.500 --> 00:07:02.620
so so that's a very interesting
perspective I actually wasn't aware of

00:07:02.620 --> 00:07:02.630
perspective I actually wasn't aware of
 

00:07:02.630 --> 00:07:04.540
perspective I actually wasn't aware of
which was in an earlier era where

00:07:04.540 --> 00:07:04.550
which was in an earlier era where
 

00:07:04.550 --> 00:07:08.200
which was in an earlier era where
computers were slower that the RPM you

00:07:08.200 --> 00:07:08.210
computers were slower that the RPM you
 

00:07:08.210 --> 00:07:09.969
computers were slower that the RPM you
know the pre-training was really

00:07:09.969 --> 00:07:09.979
know the pre-training was really
 

00:07:09.979 --> 00:07:12.430
know the pre-training was really
important as only fast the computation

00:07:12.430 --> 00:07:12.440
important as only fast the computation
 

00:07:12.440 --> 00:07:15.610
important as only fast the computation
that that drove switching to standing

00:07:15.610 --> 00:07:15.620
that that drove switching to standing
 

00:07:15.620 --> 00:07:18.490
that that drove switching to standing
back from you know in terms of the

00:07:18.490 --> 00:07:18.500
back from you know in terms of the
 

00:07:18.500 --> 00:07:19.839
back from you know in terms of the
evolution of the community is thinking

00:07:19.839 --> 00:07:19.849
evolution of the community is thinking
 

00:07:19.849 --> 00:07:22.060
evolution of the community is thinking
in deep learning another topic I know

00:07:22.060 --> 00:07:22.070
in deep learning another topic I know
 

00:07:22.070 --> 00:07:23.200
in deep learning another topic I know
you spent a lot of time thinking about

00:07:23.200 --> 00:07:23.210
you spent a lot of time thinking about
 

00:07:23.210 --> 00:07:27.100
you spent a lot of time thinking about
this the generative unsupervised versus

00:07:27.100 --> 00:07:27.110
this the generative unsupervised versus
 

00:07:27.110 --> 00:07:29.500
this the generative unsupervised versus
supervised approaches do share bit about

00:07:29.500 --> 00:07:29.510
supervised approaches do share bit about
 

00:07:29.510 --> 00:07:31.540
supervised approaches do share bit about
how you're thinking about that has

00:07:31.540 --> 00:07:31.550
how you're thinking about that has
 

00:07:31.550 --> 00:07:33.310
how you're thinking about that has
evolved over time yeah I think that's a

00:07:33.310 --> 00:07:33.320
evolved over time yeah I think that's a
 

00:07:33.320 --> 00:07:35.890
evolved over time yeah I think that's a
that's a really I feel like it's a very

00:07:35.890 --> 00:07:35.900
that's a really I feel like it's a very
 

00:07:35.900 --> 00:07:38.440
that's a really I feel like it's a very
important topic particularly if we think

00:07:38.440 --> 00:07:38.450
important topic particularly if we think
 

00:07:38.450 --> 00:07:40.890
important topic particularly if we think
about unsupervised or semi-supervised or

00:07:40.890 --> 00:07:40.900
about unsupervised or semi-supervised or
 

00:07:40.900 --> 00:07:44.709
about unsupervised or semi-supervised or
generative generative models because to

00:07:44.709 --> 00:07:44.719
generative generative models because to
 

00:07:44.719 --> 00:07:46.180
generative generative models because to
some extent a lot of successes that

00:07:46.180 --> 00:07:46.190
some extent a lot of successes that
 

00:07:46.190 --> 00:07:48.459
some extent a lot of successes that
we've seen there recently is due to

00:07:48.459 --> 00:07:48.469
we've seen there recently is due to
 

00:07:48.469 --> 00:07:50.500
we've seen there recently is due to
supervised learning and back in the

00:07:50.500 --> 00:07:50.510
supervised learning and back in the
 

00:07:50.510 --> 00:07:53.740
supervised learning and back in the
early days unsupervised learning was was

00:07:53.740 --> 00:07:53.750
early days unsupervised learning was was
 

00:07:53.750 --> 00:07:55.540
early days unsupervised learning was was
primarily viewed as unsupervised pre

00:07:55.540 --> 00:07:55.550
primarily viewed as unsupervised pre
 

00:07:55.550 --> 00:07:57.100
primarily viewed as unsupervised pre
training because we didn't know how to

00:07:57.100 --> 00:07:57.110
training because we didn't know how to
 

00:07:57.110 --> 00:08:00.100
training because we didn't know how to
train these multi-layer systems and even

00:08:00.100 --> 00:08:00.110
train these multi-layer systems and even
 

00:08:00.110 --> 00:08:03.339
train these multi-layer systems and even
today if you're working in a settings

00:08:03.339 --> 00:08:03.349
today if you're working in a settings
 

00:08:03.349 --> 00:08:04.600
today if you're working in a settings
where you have lots and lots of

00:08:04.600 --> 00:08:04.610
where you have lots and lots of
 

00:08:04.610 --> 00:08:06.940
where you have lots and lots of
unlabeled data and a small fraction of

00:08:06.940 --> 00:08:06.950
unlabeled data and a small fraction of
 

00:08:06.950 --> 00:08:08.409
unlabeled data and a small fraction of
labeled examples you know these

00:08:08.409 --> 00:08:08.419
labeled examples you know these
 

00:08:08.419 --> 00:08:10.060
labeled examples you know these
unsupervised pre training models so

00:08:10.060 --> 00:08:10.070
unsupervised pre training models so
 

00:08:10.070 --> 00:08:11.649
unsupervised pre training models so
building these generative models can

00:08:11.649 --> 00:08:11.659
building these generative models can
 

00:08:11.659 --> 00:08:15.820
building these generative models can
help you know for for supervised die so

00:08:15.820 --> 00:08:15.830
help you know for for supervised die so
 

00:08:15.830 --> 00:08:17.830
help you know for for supervised die so
I think that a lot of us in the

00:08:17.830 --> 00:08:17.840
I think that a lot of us in the
 

00:08:17.840 --> 00:08:20.200
I think that a lot of us in the
community you know it's kind of less it

00:08:20.200 --> 00:08:20.210
community you know it's kind of less it
 

00:08:20.210 --> 00:08:21.550
community you know it's kind of less it
was the belief when I started doing my

00:08:21.550 --> 00:08:21.560
was the belief when I started doing my
 

00:08:21.560 --> 00:08:24.250
was the belief when I started doing my
PhD was all about generative models and

00:08:24.250 --> 00:08:24.260
PhD was all about generative models and
 

00:08:24.260 --> 00:08:26.950
PhD was all about generative models and
try to learn these stacks of ball

00:08:26.950 --> 00:08:26.960
try to learn these stacks of ball
 

00:08:26.960 --> 00:08:28.480
try to learn these stacks of ball
because that was the only way for us to

00:08:28.480 --> 00:08:28.490
because that was the only way for us to
 

00:08:28.490 --> 00:08:33.100
because that was the only way for us to
train these systems today there is a lot

00:08:33.100 --> 00:08:33.110
train these systems today there is a lot
 

00:08:33.110 --> 00:08:35.560
train these systems today there is a lot
of work right now on generative modeling

00:08:35.560 --> 00:08:35.570
of work right now on generative modeling
 

00:08:35.570 --> 00:08:36.940
of work right now on generative modeling
you know if you look at generative

00:08:36.940 --> 00:08:36.950
you know if you look at generative
 

00:08:36.950 --> 00:08:38.140
you know if you look at generative
adversarial Network

00:08:38.140 --> 00:08:38.150
adversarial Network
 

00:08:38.150 --> 00:08:39.730
adversarial Network
if you look at variation within quarters

00:08:39.730 --> 00:08:39.740
if you look at variation within quarters
 

00:08:39.740 --> 00:08:41.829
if you look at variation within quarters
the energy models is something that my

00:08:41.829 --> 00:08:41.839
the energy models is something that my
 

00:08:41.839 --> 00:08:45.370
the energy models is something that my
lab is working on right now as well I

00:08:45.370 --> 00:08:45.380
lab is working on right now as well I
 

00:08:45.380 --> 00:08:47.530
lab is working on right now as well I
think it's it's very exciting research

00:08:47.530 --> 00:08:47.540
think it's it's very exciting research
 

00:08:47.540 --> 00:08:49.870
think it's it's very exciting research
but we haven't perhaps we haven't quite

00:08:49.870 --> 00:08:49.880
but we haven't perhaps we haven't quite
 

00:08:49.880 --> 00:08:52.390
but we haven't perhaps we haven't quite
figured it out again for many of you who

00:08:52.390 --> 00:08:52.400
figured it out again for many of you who
 

00:08:52.400 --> 00:08:53.740
figured it out again for many of you who
are thinking about getting in the

00:08:53.740 --> 00:08:53.750
are thinking about getting in the
 

00:08:53.750 --> 00:08:56.290
are thinking about getting in the
deploying field this is one area that's

00:08:56.290 --> 00:08:56.300
deploying field this is one area that's
 

00:08:56.300 --> 00:08:58.870
deploying field this is one area that's
I think we you know will make a lot of

00:08:58.870 --> 00:08:58.880
I think we you know will make a lot of
 

00:08:58.880 --> 00:09:00.430
I think we you know will make a lot of
progress and hopefully in the near

00:09:00.430 --> 00:09:00.440
progress and hopefully in the near
 

00:09:00.440 --> 00:09:02.170
progress and hopefully in the near
future so unsupervised early

00:09:02.170 --> 00:09:02.180
future so unsupervised early
 

00:09:02.180 --> 00:09:03.820
future so unsupervised early
unsupervised learning right head laying

00:09:03.820 --> 00:09:03.830
unsupervised learning right head laying
 

00:09:03.830 --> 00:09:04.810
unsupervised learning right head laying
oh maybe you can think of it as

00:09:04.810 --> 00:09:04.820
oh maybe you can think of it as
 

00:09:04.820 --> 00:09:06.550
oh maybe you can think of it as
unsupervised learning or semi-supervised

00:09:06.550 --> 00:09:06.560
unsupervised learning or semi-supervised
 

00:09:06.560 --> 00:09:09.160
unsupervised learning or semi-supervised
learning where you have I give you some

00:09:09.160 --> 00:09:09.170
learning where you have I give you some
 

00:09:09.170 --> 00:09:13.570
learning where you have I give you some
hints or some examples of what what

00:09:13.570 --> 00:09:13.580
hints or some examples of what what
 

00:09:13.580 --> 00:09:16.120
hints or some examples of what what
different things mean and I throw you

00:09:16.120 --> 00:09:16.130
different things mean and I throw you
 

00:09:16.130 --> 00:09:18.519
different things mean and I throw you
lots and lots of unlabeled data so you

00:09:18.519 --> 00:09:18.529
lots and lots of unlabeled data so you
 

00:09:18.529 --> 00:09:20.110
lots and lots of unlabeled data so you
know thank you very important insight

00:09:20.110 --> 00:09:20.120
know thank you very important insight
 

00:09:20.120 --> 00:09:22.120
know thank you very important insight
that in an earlier era of deep learning

00:09:22.120 --> 00:09:22.130
that in an earlier era of deep learning
 

00:09:22.130 --> 00:09:24.120
that in an earlier era of deep learning
where computers just slower the

00:09:24.120 --> 00:09:24.130
where computers just slower the
 

00:09:24.130 --> 00:09:26.079
where computers just slower the
restricted Boltzmann machine and deep

00:09:26.079 --> 00:09:26.089
restricted Boltzmann machine and deep
 

00:09:26.089 --> 00:09:27.480
restricted Boltzmann machine and deep
Boltzmann stream that was needed for

00:09:27.480 --> 00:09:27.490
Boltzmann stream that was needed for
 

00:09:27.490 --> 00:09:29.710
Boltzmann stream that was needed for
initializing the neural network weights

00:09:29.710 --> 00:09:29.720
initializing the neural network weights
 

00:09:29.720 --> 00:09:31.990
initializing the neural network weights
but as computers got faster straight

00:09:31.990 --> 00:09:32.000
but as computers got faster straight
 

00:09:32.000 --> 00:09:33.810
but as computers got faster straight
backprop then start to work much better

00:09:33.810 --> 00:09:33.820
backprop then start to work much better
 

00:09:33.820 --> 00:09:36.550
backprop then start to work much better
so you know one of the topic that I know

00:09:36.550 --> 00:09:36.560
so you know one of the topic that I know
 

00:09:36.560 --> 00:09:37.690
so you know one of the topic that I know
you've spent a lot of time thinking

00:09:37.690 --> 00:09:37.700
you've spent a lot of time thinking
 

00:09:37.700 --> 00:09:41.880
you've spent a lot of time thinking
about is the supervised learning versus

00:09:41.880 --> 00:09:41.890
about is the supervised learning versus
 

00:09:41.890 --> 00:09:43.870
about is the supervised learning versus
generative models unsupervised learning

00:09:43.870 --> 00:09:43.880
generative models unsupervised learning
 

00:09:43.880 --> 00:09:48.130
generative models unsupervised learning
approaches so how has your tell me a bit

00:09:48.130 --> 00:09:48.140
approaches so how has your tell me a bit
 

00:09:48.140 --> 00:09:50.050
approaches so how has your tell me a bit
about how you're thinking on that debate

00:09:50.050 --> 00:09:50.060
about how you're thinking on that debate
 

00:09:50.060 --> 00:09:52.180
about how you're thinking on that debate
has evolved over time I think that we

00:09:52.180 --> 00:09:52.190
has evolved over time I think that we
 

00:09:52.190 --> 00:09:55.870
has evolved over time I think that we
all believe that we should be able to to

00:09:55.870 --> 00:09:55.880
all believe that we should be able to to
 

00:09:55.880 --> 00:09:57.160
all believe that we should be able to to
make progress there it's just it's just

00:09:57.160 --> 00:09:57.170
make progress there it's just it's just
 

00:09:57.170 --> 00:10:00.370
make progress there it's just it's just
you know you know all the work on

00:10:00.370 --> 00:10:00.380
you know you know all the work on
 

00:10:00.380 --> 00:10:02.380
you know you know all the work on
Boltzmann machines variational t

00:10:02.380 --> 00:10:02.390
Boltzmann machines variational t
 

00:10:02.390 --> 00:10:04.750
Boltzmann machines variational t
encoders yes you can think a lot of

00:10:04.750 --> 00:10:04.760
encoders yes you can think a lot of
 

00:10:04.760 --> 00:10:07.540
encoders yes you can think a lot of
these models as generative models but we

00:10:07.540 --> 00:10:07.550
these models as generative models but we
 

00:10:07.550 --> 00:10:10.360
these models as generative models but we
haven't quite figured out you know how

00:10:10.360 --> 00:10:10.370
haven't quite figured out you know how
 

00:10:10.370 --> 00:10:14.500
haven't quite figured out you know how
to you know really make them work and

00:10:14.500 --> 00:10:14.510
to you know really make them work and
 

00:10:14.510 --> 00:10:16.720
to you know really make them work and
how can you make use of logic almost and

00:10:16.720 --> 00:10:16.730
how can you make use of logic almost and
 

00:10:16.730 --> 00:10:19.720
how can you make use of logic almost and
even if even for I see a lot of an IT

00:10:19.720 --> 00:10:19.730
even if even for I see a lot of an IT
 

00:10:19.730 --> 00:10:23.860
even if even for I see a lot of an IT
sector you know companies have lots and

00:10:23.860 --> 00:10:23.870
sector you know companies have lots and
 

00:10:23.870 --> 00:10:24.760
sector you know companies have lots and
lots of data

00:10:24.760 --> 00:10:24.770
lots of data
 

00:10:24.770 --> 00:10:26.620
lots of data
lots of unlabeled data there's a lots of

00:10:26.620 --> 00:10:26.630
lots of unlabeled data there's a lots of
 

00:10:26.630 --> 00:10:29.100
lots of unlabeled data there's a lots of
efforts for going through annotations

00:10:29.100 --> 00:10:29.110
efforts for going through annotations
 

00:10:29.110 --> 00:10:32.350
efforts for going through annotations
because that's the only way for us to to

00:10:32.350 --> 00:10:32.360
because that's the only way for us to to
 

00:10:32.360 --> 00:10:33.699
because that's the only way for us to to
make progress right now and it seems

00:10:33.699 --> 00:10:33.709
make progress right now and it seems
 

00:10:33.709 --> 00:10:36.460
make progress right now and it seems
like you know we should be able to make

00:10:36.460 --> 00:10:36.470
like you know we should be able to make
 

00:10:36.470 --> 00:10:38.290
like you know we should be able to make
use of unlabeled data because it's you

00:10:38.290 --> 00:10:38.300
use of unlabeled data because it's you
 

00:10:38.300 --> 00:10:40.210
use of unlabeled data because it's you
know it's just abundance of it and and

00:10:40.210 --> 00:10:40.220
know it's just abundance of it and and
 

00:10:40.220 --> 00:10:41.820
know it's just abundance of it and and
we haven't quite figured out how to do

00:10:41.820 --> 00:10:41.830
we haven't quite figured out how to do
 

00:10:41.830 --> 00:10:45.810
we haven't quite figured out how to do
yet so you mentioned for people wanting

00:10:45.810 --> 00:10:45.820
yet so you mentioned for people wanting
 

00:10:45.820 --> 00:10:48.090
yet so you mentioned for people wanting
to enter deep learning research you know

00:10:48.090 --> 00:10:48.100
to enter deep learning research you know
 

00:10:48.100 --> 00:10:49.940
to enter deep learning research you know
unsupervised learning the exciting area

00:10:49.940 --> 00:10:49.950
unsupervised learning the exciting area
 

00:10:49.950 --> 00:10:52.470
unsupervised learning the exciting area
today there are a lot of people wanting

00:10:52.470 --> 00:10:52.480
today there are a lot of people wanting
 

00:10:52.480 --> 00:10:54.509
today there are a lot of people wanting
to enter a deep learning either research

00:10:54.509 --> 00:10:54.519
to enter a deep learning either research
 

00:10:54.519 --> 00:10:57.090
to enter a deep learning either research
or applied work so for this global

00:10:57.090 --> 00:10:57.100
or applied work so for this global
 

00:10:57.100 --> 00:10:59.490
or applied work so for this global
community either researcher of my work

00:10:59.490 --> 00:10:59.500
community either researcher of my work
 

00:10:59.500 --> 00:11:01.769
community either researcher of my work
what advice would you have yes I think

00:11:01.769 --> 00:11:01.779
what advice would you have yes I think
 

00:11:01.779 --> 00:11:04.889
what advice would you have yes I think
that one of one of the key advisors I

00:11:04.889 --> 00:11:04.899
that one of one of the key advisors I
 

00:11:04.899 --> 00:11:09.960
that one of one of the key advisors I
think I should give is people entering

00:11:09.960 --> 00:11:09.970
think I should give is people entering
 

00:11:09.970 --> 00:11:11.490
think I should give is people entering
that field I would encourage them to

00:11:11.490 --> 00:11:11.500
that field I would encourage them to
 

00:11:11.500 --> 00:11:14.610
that field I would encourage them to
just try different things and not be

00:11:14.610 --> 00:11:14.620
just try different things and not be
 

00:11:14.620 --> 00:11:16.590
just try different things and not be
afraid to try new things and not be

00:11:16.590 --> 00:11:16.600
afraid to try new things and not be
 

00:11:16.600 --> 00:11:18.600
afraid to try new things and not be
afraid to try to innovate I can give you

00:11:18.600 --> 00:11:18.610
afraid to try to innovate I can give you
 

00:11:18.610 --> 00:11:21.180
afraid to try to innovate I can give you
one example which is when I was a

00:11:21.180 --> 00:11:21.190
one example which is when I was a
 

00:11:21.190 --> 00:11:23.490
one example which is when I was a
graduate student you know we were

00:11:23.490 --> 00:11:23.500
graduate student you know we were
 

00:11:23.500 --> 00:11:25.590
graduate student you know we were
looking at neural nets and he's a highly

00:11:25.590 --> 00:11:25.600
looking at neural nets and he's a highly
 

00:11:25.600 --> 00:11:28.050
looking at neural nets and he's a highly
non convex systems that are hard to

00:11:28.050 --> 00:11:28.060
non convex systems that are hard to
 

00:11:28.060 --> 00:11:30.600
non convex systems that are hard to
optimize and I remember talking to my

00:11:30.600 --> 00:11:30.610
optimize and I remember talking to my
 

00:11:30.610 --> 00:11:32.220
optimize and I remember talking to my
friends with in the optimization

00:11:32.220 --> 00:11:32.230
friends with in the optimization
 

00:11:32.230 --> 00:11:35.400
friends with in the optimization
community and the feedback was always

00:11:35.400 --> 00:11:35.410
community and the feedback was always
 

00:11:35.410 --> 00:11:37.920
community and the feedback was always
that well there is no way you can solve

00:11:37.920 --> 00:11:37.930
that well there is no way you can solve
 

00:11:37.930 --> 00:11:39.690
that well there is no way you can solve
these problems because these are non

00:11:39.690 --> 00:11:39.700
these problems because these are non
 

00:11:39.700 --> 00:11:41.490
these problems because these are non
convex we don't understand optimization

00:11:41.490 --> 00:11:41.500
convex we don't understand optimization
 

00:11:41.500 --> 00:11:44.730
convex we don't understand optimization
how could you ever even do that you know

00:11:44.730 --> 00:11:44.740
how could you ever even do that you know
 

00:11:44.740 --> 00:11:46.319
how could you ever even do that you know
compared to doing comics optimization

00:11:46.319 --> 00:11:46.329
compared to doing comics optimization
 

00:11:46.329 --> 00:11:49.760
compared to doing comics optimization
and it was surprising because in our lab

00:11:49.760 --> 00:11:49.770
and it was surprising because in our lab
 

00:11:49.770 --> 00:11:52.530
and it was surprising because in our lab
you know we never really cared that much

00:11:52.530 --> 00:11:52.540
you know we never really cared that much
 

00:11:52.540 --> 00:11:55.019
you know we never really cared that much
about those specific problems we just

00:11:55.019 --> 00:11:55.029
about those specific problems we just
 

00:11:55.029 --> 00:11:57.480
about those specific problems we just
were thinking about how can we optimize

00:11:57.480 --> 00:11:57.490
were thinking about how can we optimize
 

00:11:57.490 --> 00:11:58.860
were thinking about how can we optimize
and whether we can get interesting

00:11:58.860 --> 00:11:58.870
and whether we can get interesting
 

00:11:58.870 --> 00:12:01.710
and whether we can get interesting
results and that effectively was driving

00:12:01.710 --> 00:12:01.720
results and that effectively was driving
 

00:12:01.720 --> 00:12:04.350
results and that effectively was driving
the community so we're not were you know

00:12:04.350 --> 00:12:04.360
the community so we're not were you know
 

00:12:04.360 --> 00:12:07.500
the community so we're not were you know
we were we were not scared maybe to some

00:12:07.500 --> 00:12:07.510
we were we were not scared maybe to some
 

00:12:07.510 --> 00:12:09.060
we were we were not scared maybe to some
extent because we didn't maybe because

00:12:09.060 --> 00:12:09.070
extent because we didn't maybe because
 

00:12:09.070 --> 00:12:11.069
extent because we didn't maybe because
we were lacking actually the theory

00:12:11.069 --> 00:12:11.079
we were lacking actually the theory
 

00:12:11.079 --> 00:12:13.949
we were lacking actually the theory
behind optimization but but I would

00:12:13.949 --> 00:12:13.959
behind optimization but but I would
 

00:12:13.959 --> 00:12:16.290
behind optimization but but I would
encourage people to just try and not be

00:12:16.290 --> 00:12:16.300
encourage people to just try and not be
 

00:12:16.300 --> 00:12:18.690
encourage people to just try and not be
afraid to try to tackle hard problems

00:12:18.690 --> 00:12:18.700
afraid to try to tackle hard problems
 

00:12:18.700 --> 00:12:21.000
afraid to try to tackle hard problems
yeah and I remember you once said don't

00:12:21.000 --> 00:12:21.010
yeah and I remember you once said don't
 

00:12:21.010 --> 00:12:23.100
yeah and I remember you once said don't
learn to code just into high level you

00:12:23.100 --> 00:12:23.110
learn to code just into high level you
 

00:12:23.110 --> 00:12:24.300
learn to code just into high level you
know deep learning frameworks but

00:12:24.300 --> 00:12:24.310
know deep learning frameworks but
 

00:12:24.310 --> 00:12:26.310
know deep learning frameworks but
actually understand yes that's right I

00:12:26.310 --> 00:12:26.320
actually understand yes that's right I
 

00:12:26.320 --> 00:12:27.930
actually understand yes that's right I
think that bolon it's one of the things

00:12:27.930 --> 00:12:27.940
think that bolon it's one of the things
 

00:12:27.940 --> 00:12:29.400
think that bolon it's one of the things
that I try to do it when I teach you

00:12:29.400 --> 00:12:29.410
that I try to do it when I teach you
 

00:12:29.410 --> 00:12:32.310
that I try to do it when I teach you
deep learning class is is is one of the

00:12:32.310 --> 00:12:32.320
deep learning class is is is one of the
 

00:12:32.320 --> 00:12:34.230
deep learning class is is is one of the
for one of the homeworks I'm asking

00:12:34.230 --> 00:12:34.240
for one of the homeworks I'm asking
 

00:12:34.240 --> 00:12:36.600
for one of the homeworks I'm asking
people to actually code backpropagation

00:12:36.600 --> 00:12:36.610
people to actually code backpropagation
 

00:12:36.610 --> 00:12:37.949
people to actually code backpropagation
algorithm for convolutional neural

00:12:37.949 --> 00:12:37.959
algorithm for convolutional neural
 

00:12:37.959 --> 00:12:41.329
algorithm for convolutional neural
networks and it's you know it's painful

00:12:41.329 --> 00:12:41.339
networks and it's you know it's painful
 

00:12:41.339 --> 00:12:43.949
networks and it's you know it's painful
but but at the same time if you do it

00:12:43.949 --> 00:12:43.959
but but at the same time if you do it
 

00:12:43.959 --> 00:12:46.319
but but at the same time if you do it
once you really understand how these

00:12:46.319 --> 00:12:46.329
once you really understand how these
 

00:12:46.329 --> 00:12:49.860
once you really understand how these
systems operate and how they work and

00:12:49.860 --> 00:12:49.870
systems operate and how they work and
 

00:12:49.870 --> 00:12:51.630
systems operate and how they work and
how you can efficiently implement them

00:12:51.630 --> 00:12:51.640
how you can efficiently implement them
 

00:12:51.640 --> 00:12:53.730
how you can efficiently implement them
on on on GPU and I think it's it's

00:12:53.730 --> 00:12:53.740
on on on GPU and I think it's it's
 

00:12:53.740 --> 00:12:54.889
on on on GPU and I think it's it's
important for

00:12:54.889 --> 00:12:54.899
important for
 

00:12:54.899 --> 00:12:58.009
important for
you too when you go into research or

00:12:58.009 --> 00:12:58.019
you too when you go into research or
 

00:12:58.019 --> 00:12:59.720
you too when you go into research or
industry you have a really good

00:12:59.720 --> 00:12:59.730
industry you have a really good
 

00:12:59.730 --> 00:13:01.519
industry you have a really good
understanding of what these systems are

00:13:01.519 --> 00:13:01.529
understanding of what these systems are
 

00:13:01.529 --> 00:13:04.850
understanding of what these systems are
doing so it's it's important I think you

00:13:04.850 --> 00:13:04.860
doing so it's it's important I think you
 

00:13:04.860 --> 00:13:07.009
doing so it's it's important I think you
know since you have both academic

00:13:07.009 --> 00:13:07.019
know since you have both academic
 

00:13:07.019 --> 00:13:09.049
know since you have both academic
experience that's professor and

00:13:09.049 --> 00:13:09.059
experience that's professor and
 

00:13:09.059 --> 00:13:11.449
experience that's professor and
corporate experience I'm curious if

00:13:11.449 --> 00:13:11.459
corporate experience I'm curious if
 

00:13:11.459 --> 00:13:13.819
corporate experience I'm curious if
someone's sensitive learning what are

00:13:13.819 --> 00:13:13.829
someone's sensitive learning what are
 

00:13:13.829 --> 00:13:15.889
someone's sensitive learning what are
their pros and cons of doing a PhD

00:13:15.889 --> 00:13:15.899
their pros and cons of doing a PhD
 

00:13:15.899 --> 00:13:18.530
their pros and cons of doing a PhD
versus joining a company yeah I think

00:13:18.530 --> 00:13:18.540
versus joining a company yeah I think
 

00:13:18.540 --> 00:13:20.509
versus joining a company yeah I think
that's that's actually a very good

00:13:20.509 --> 00:13:20.519
that's that's actually a very good
 

00:13:20.519 --> 00:13:24.139
that's that's actually a very good
question in my particular lab I have a

00:13:24.139 --> 00:13:24.149
question in my particular lab I have a
 

00:13:24.149 --> 00:13:26.660
question in my particular lab I have a
mix of students some students want to go

00:13:26.660 --> 00:13:26.670
mix of students some students want to go
 

00:13:26.670 --> 00:13:29.419
mix of students some students want to go
and take an academic route some students

00:13:29.419 --> 00:13:29.429
and take an academic route some students
 

00:13:29.429 --> 00:13:31.579
and take an academic route some students
want to go and take an industry route

00:13:31.579 --> 00:13:31.589
want to go and take an industry route
 

00:13:31.589 --> 00:13:34.549
want to go and take an industry route
and it's it's becoming very challenging

00:13:34.549 --> 00:13:34.559
and it's it's becoming very challenging
 

00:13:34.559 --> 00:13:37.970
and it's it's becoming very challenging
because you can do amazing research in

00:13:37.970 --> 00:13:37.980
because you can do amazing research in
 

00:13:37.980 --> 00:13:41.119
because you can do amazing research in
industry and and you can also do amazing

00:13:41.119 --> 00:13:41.129
industry and and you can also do amazing
 

00:13:41.129 --> 00:13:42.350
industry and and you can also do amazing
research in academia but in terms of

00:13:42.350 --> 00:13:42.360
research in academia but in terms of
 

00:13:42.360 --> 00:13:46.999
research in academia but in terms of
pros and cons in academia I feel like

00:13:46.999 --> 00:13:47.009
pros and cons in academia I feel like
 

00:13:47.009 --> 00:13:50.480
pros and cons in academia I feel like
you have more freedom to work on long

00:13:50.480 --> 00:13:50.490
you have more freedom to work on long
 

00:13:50.490 --> 00:13:53.600
you have more freedom to work on long
term problems or if you think about some

00:13:53.600 --> 00:13:53.610
term problems or if you think about some
 

00:13:53.610 --> 00:13:56.629
term problems or if you think about some
crazy problem you can work on it so you

00:13:56.629 --> 00:13:56.639
crazy problem you can work on it so you
 

00:13:56.639 --> 00:13:59.030
crazy problem you can work on it so you
have a little bit more more freedom at

00:13:59.030 --> 00:13:59.040
have a little bit more more freedom at
 

00:13:59.040 --> 00:14:01.669
have a little bit more more freedom at
the same time the research that you're

00:14:01.669 --> 00:14:01.679
the same time the research that you're
 

00:14:01.679 --> 00:14:03.559
the same time the research that you're
doing industry is also very exciting

00:14:03.559 --> 00:14:03.569
doing industry is also very exciting
 

00:14:03.569 --> 00:14:07.400
doing industry is also very exciting
because in many cases you can with your

00:14:07.400 --> 00:14:07.410
because in many cases you can with your
 

00:14:07.410 --> 00:14:09.499
because in many cases you can with your
research you can impact millions of

00:14:09.499 --> 00:14:09.509
research you can impact millions of
 

00:14:09.509 --> 00:14:13.189
research you can impact millions of
users if you develop you know a core AI

00:14:13.189 --> 00:14:13.199
users if you develop you know a core AI
 

00:14:13.199 --> 00:14:16.429
users if you develop you know a core AI
technology and and obviously within the

00:14:16.429 --> 00:14:16.439
technology and and obviously within the
 

00:14:16.439 --> 00:14:18.889
technology and and obviously within the
industry you have much more resources in

00:14:18.889 --> 00:14:18.899
industry you have much more resources in
 

00:14:18.899 --> 00:14:22.119
industry you have much more resources in
terms of compute and be able to you know

00:14:22.119 --> 00:14:22.129
terms of compute and be able to you know
 

00:14:22.129 --> 00:14:26.540
terms of compute and be able to you know
do really amazing things so there are

00:14:26.540 --> 00:14:26.550
do really amazing things so there are
 

00:14:26.550 --> 00:14:27.919
do really amazing things so there are
pluses and minuses that it really

00:14:27.919 --> 00:14:27.929
pluses and minuses that it really
 

00:14:27.929 --> 00:14:30.499
pluses and minuses that it really
depends on on what you want to do and

00:14:30.499 --> 00:14:30.509
depends on on what you want to do and
 

00:14:30.509 --> 00:14:32.689
depends on on what you want to do and
right now it's interesting very

00:14:32.689 --> 00:14:32.699
right now it's interesting very
 

00:14:32.699 --> 00:14:35.299
right now it's interesting very
interesting environment where academics

00:14:35.299 --> 00:14:35.309
interesting environment where academics
 

00:14:35.309 --> 00:14:37.519
interesting environment where academics
move to industry and then you know focus

00:14:37.519 --> 00:14:37.529
move to industry and then you know focus
 

00:14:37.529 --> 00:14:39.949
move to industry and then you know focus
on industry move to academia but not as

00:14:39.949 --> 00:14:39.959
on industry move to academia but not as
 

00:14:39.959 --> 00:14:43.160
on industry move to academia but not as
much and so it's it's you know it's it's

00:14:43.160 --> 00:14:43.170
much and so it's it's you know it's it's
 

00:14:43.170 --> 00:14:45.949
much and so it's it's you know it's it's
it's a very it's very exciting times it

00:14:45.949 --> 00:14:45.959
it's a very it's very exciting times it
 

00:14:45.959 --> 00:14:47.660
it's a very it's very exciting times it
sounds like your academic machine

00:14:47.660 --> 00:14:47.670
sounds like your academic machine
 

00:14:47.670 --> 00:14:49.220
sounds like your academic machine
learning is great and corporate machine

00:14:49.220 --> 00:14:49.230
learning is great and corporate machine
 

00:14:49.230 --> 00:14:51.199
learning is great and corporate machine
learning is great and the most important

00:14:51.199 --> 00:14:51.209
learning is great and the most important
 

00:14:51.209 --> 00:14:53.090
learning is great and the most important
thing is just jumping right either one

00:14:53.090 --> 00:14:53.100
thing is just jumping right either one
 

00:14:53.100 --> 00:14:54.919
thing is just jumping right either one
just jump in so it really depends on

00:14:54.919 --> 00:14:54.929
just jump in so it really depends on
 

00:14:54.929 --> 00:14:56.539
just jump in so it really depends on
your own your preferences because you

00:14:56.539 --> 00:14:56.549
your own your preferences because you
 

00:14:56.549 --> 00:14:59.150
your own your preferences because you
can do amazing research in either place

00:14:59.150 --> 00:14:59.160
can do amazing research in either place
 

00:14:59.160 --> 00:15:00.780
can do amazing research in either place
so you've mentioned

00:15:00.780 --> 00:15:00.790
so you've mentioned
 

00:15:00.790 --> 00:15:02.490
so you've mentioned
supervised learning as one exciting

00:15:02.490 --> 00:15:02.500
supervised learning as one exciting
 

00:15:02.500 --> 00:15:04.920
supervised learning as one exciting
frontier for research are there other

00:15:04.920 --> 00:15:04.930
frontier for research are there other
 

00:15:04.930 --> 00:15:07.080
frontier for research are there other
areas that you consider exciting

00:15:07.080 --> 00:15:07.090
areas that you consider exciting
 

00:15:07.090 --> 00:15:09.390
areas that you consider exciting
frontiers for research yeah absolutely I

00:15:09.390 --> 00:15:09.400
frontiers for research yeah absolutely I
 

00:15:09.400 --> 00:15:12.000
frontiers for research yeah absolutely I
think that what I see now community

00:15:12.000 --> 00:15:12.010
think that what I see now community
 

00:15:12.010 --> 00:15:13.140
think that what I see now community
right now in particularly deep learning

00:15:13.140 --> 00:15:13.150
right now in particularly deep learning
 

00:15:13.150 --> 00:15:17.460
right now in particularly deep learning
community is there are a few trends one

00:15:17.460 --> 00:15:17.470
community is there are a few trends one
 

00:15:17.470 --> 00:15:19.320
community is there are a few trends one
particular area that I think is really

00:15:19.320 --> 00:15:19.330
particular area that I think is really
 

00:15:19.330 --> 00:15:22.140
particular area that I think is really
exciting is the area of deep

00:15:22.140 --> 00:15:22.150
exciting is the area of deep
 

00:15:22.150 --> 00:15:24.480
exciting is the area of deep
reinforcement learning because we were

00:15:24.480 --> 00:15:24.490
reinforcement learning because we were
 

00:15:24.490 --> 00:15:27.240
reinforcement learning because we were
able to figure out how we can train Ages

00:15:27.240 --> 00:15:27.250
able to figure out how we can train Ages
 

00:15:27.250 --> 00:15:29.520
able to figure out how we can train Ages
in virtual worlds and this is something

00:15:29.520 --> 00:15:29.530
in virtual worlds and this is something
 

00:15:29.530 --> 00:15:32.130
in virtual worlds and this is something
that's in just the last couple of years

00:15:32.130 --> 00:15:32.140
that's in just the last couple of years
 

00:15:32.140 --> 00:15:34.110
that's in just the last couple of years
you see a lot a lot of progress of how

00:15:34.110 --> 00:15:34.120
you see a lot a lot of progress of how
 

00:15:34.120 --> 00:15:36.270
you see a lot a lot of progress of how
can we scale these systems how can we

00:15:36.270 --> 00:15:36.280
can we scale these systems how can we
 

00:15:36.280 --> 00:15:37.860
can we scale these systems how can we
develop new algorithms how can we get

00:15:37.860 --> 00:15:37.870
develop new algorithms how can we get
 

00:15:37.870 --> 00:15:40.440
develop new algorithms how can we get
ages to communicate to each other with

00:15:40.440 --> 00:15:40.450
ages to communicate to each other with
 

00:15:40.450 --> 00:15:43.080
ages to communicate to each other with
each other and and it's I think that

00:15:43.080 --> 00:15:43.090
each other and and it's I think that
 

00:15:43.090 --> 00:15:45.840
each other and and it's I think that
that area is and generally the the

00:15:45.840 --> 00:15:45.850
that area is and generally the the
 

00:15:45.850 --> 00:15:47.460
that area is and generally the the
settings where you're interacting with

00:15:47.460 --> 00:15:47.470
settings where you're interacting with
 

00:15:47.470 --> 00:15:52.110
settings where you're interacting with
the environment is super exciting the

00:15:52.110 --> 00:15:52.120
the environment is super exciting the
 

00:15:52.120 --> 00:15:55.260
the environment is super exciting the
other area that I think is really

00:15:55.260 --> 00:15:55.270
other area that I think is really
 

00:15:55.270 --> 00:15:57.720
other area that I think is really
exciting as well is the area of

00:15:57.720 --> 00:15:57.730
exciting as well is the area of
 

00:15:57.730 --> 00:15:58.950
exciting as well is the area of
reasoning and natural language

00:15:58.950 --> 00:15:58.960
reasoning and natural language
 

00:15:58.960 --> 00:16:02.760
reasoning and natural language
understanding so can we build dialogue

00:16:02.760 --> 00:16:02.770
understanding so can we build dialogue
 

00:16:02.770 --> 00:16:05.190
understanding so can we build dialogue
based systems can we build systems that

00:16:05.190 --> 00:16:05.200
based systems can we build systems that
 

00:16:05.200 --> 00:16:09.120
based systems can we build systems that
can reason that can read text and be

00:16:09.120 --> 00:16:09.130
can reason that can read text and be
 

00:16:09.130 --> 00:16:11.220
can reason that can read text and be
able to you know answer questions

00:16:11.220 --> 00:16:11.230
able to you know answer questions
 

00:16:11.230 --> 00:16:13.320
able to you know answer questions
intelligently I think this is something

00:16:13.320 --> 00:16:13.330
intelligently I think this is something
 

00:16:13.330 --> 00:16:16.080
intelligently I think this is something
that a lot of research is is focusing on

00:16:16.080 --> 00:16:16.090
that a lot of research is is focusing on
 

00:16:16.090 --> 00:16:20.250
that a lot of research is is focusing on
right now and then there's not a sort of

00:16:20.250 --> 00:16:20.260
right now and then there's not a sort of
 

00:16:20.260 --> 00:16:23.160
right now and then there's not a sort of
sub-aerial so is this area of being able

00:16:23.160 --> 00:16:23.170
sub-aerial so is this area of being able
 

00:16:23.170 --> 00:16:26.400
sub-aerial so is this area of being able
to learn from fewer examples so

00:16:26.400 --> 00:16:26.410
to learn from fewer examples so
 

00:16:26.410 --> 00:16:27.810
to learn from fewer examples so
typically you know people think of it as

00:16:27.810 --> 00:16:27.820
typically you know people think of it as
 

00:16:27.820 --> 00:16:30.270
typically you know people think of it as
one short learning or transfer learning

00:16:30.270 --> 00:16:30.280
one short learning or transfer learning
 

00:16:30.280 --> 00:16:35.520
one short learning or transfer learning
a setting where you know you you learn

00:16:35.520 --> 00:16:35.530
a setting where you know you you learn
 

00:16:35.530 --> 00:16:37.470
a setting where you know you you learn
something about the world and I throw

00:16:37.470 --> 00:16:37.480
something about the world and I throw
 

00:16:37.480 --> 00:16:39.600
something about the world and I throw
you a new task at you and you can solve

00:16:39.600 --> 00:16:39.610
you a new task at you and you can solve
 

00:16:39.610 --> 00:16:42.120
you a new task at you and you can solve
this task very quickly much like humans

00:16:42.120 --> 00:16:42.130
this task very quickly much like humans
 

00:16:42.130 --> 00:16:44.130
this task very quickly much like humans
do without requiring lots and lots of

00:16:44.130 --> 00:16:44.140
do without requiring lots and lots of
 

00:16:44.140 --> 00:16:47.280
do without requiring lots and lots of
labeled examples and so this is

00:16:47.280 --> 00:16:47.290
labeled examples and so this is
 

00:16:47.290 --> 00:16:49.230
labeled examples and so this is
something that's a lot of us in the

00:16:49.230 --> 00:16:49.240
something that's a lot of us in the
 

00:16:49.240 --> 00:16:51.480
something that's a lot of us in the
community are trying to figure out how

00:16:51.480 --> 00:16:51.490
community are trying to figure out how
 

00:16:51.490 --> 00:16:53.640
community are trying to figure out how
we can how we can do that and how can we

00:16:53.640 --> 00:16:53.650
we can how we can do that and how can we
 

00:16:53.650 --> 00:16:55.430
we can how we can do that and how can we
have come closer to human-like

00:16:55.430 --> 00:16:55.440
have come closer to human-like
 

00:16:55.440 --> 00:16:58.260
have come closer to human-like
human-like learning abilities Thank You

00:16:58.260 --> 00:16:58.270
human-like learning abilities Thank You
 

00:16:58.270 --> 00:17:00.030
human-like learning abilities Thank You
Russ for sharing all the comments and

00:17:00.030 --> 00:17:00.040
Russ for sharing all the comments and
 

00:17:00.040 --> 00:17:01.800
Russ for sharing all the comments and
inside so there's especially if you say

00:17:01.800 --> 00:17:01.810
inside so there's especially if you say
 

00:17:01.810 --> 00:17:03.990
inside so there's especially if you say
hearing the story of your early days

00:17:03.990 --> 00:17:04.000
hearing the story of your early days
 

00:17:04.000 --> 00:17:07.260
hearing the story of your early days
dude thanks Andrea yeah thanks for

00:17:07.260 --> 00:17:07.270
dude thanks Andrea yeah thanks for
 

00:17:07.270 --> 00:17:09.720
dude thanks Andrea yeah thanks for
having me

