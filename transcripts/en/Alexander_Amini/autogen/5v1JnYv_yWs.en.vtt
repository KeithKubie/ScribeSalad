WEBVTT
Kind: captions
Language: en

00:00:02.800 --> 00:00:05.320
good afternoon everyone thank you all

00:00:05.320 --> 00:00:05.330
good afternoon everyone thank you all
 

00:00:05.330 --> 00:00:07.150
good afternoon everyone thank you all
for joining us my name is Alexandra

00:00:07.150 --> 00:00:07.160
for joining us my name is Alexandra
 

00:00:07.160 --> 00:00:09.100
for joining us my name is Alexandra
Meany and one of the course organizers

00:00:09.100 --> 00:00:09.110
Meany and one of the course organizers
 

00:00:09.110 --> 00:00:12.430
Meany and one of the course organizers
for six s-191 this is mi t--'s official

00:00:12.430 --> 00:00:12.440
for six s-191 this is mi t--'s official
 

00:00:12.440 --> 00:00:14.230
for six s-191 this is mi t--'s official
course on introduction to deep learning

00:00:14.230 --> 00:00:14.240
course on introduction to deep learning
 

00:00:14.240 --> 00:00:16.929
course on introduction to deep learning
and this is actually the third year that

00:00:16.929 --> 00:00:16.939
and this is actually the third year that
 

00:00:16.939 --> 00:00:19.359
and this is actually the third year that
we're offering this course and we've got

00:00:19.359 --> 00:00:19.369
we're offering this course and we've got
 

00:00:19.369 --> 00:00:20.650
we're offering this course and we've got
a really good one in store for you this

00:00:20.650 --> 00:00:20.660
a really good one in store for you this
 

00:00:20.660 --> 00:00:22.900
a really good one in store for you this
year with a lot of awesome updates so I

00:00:22.900 --> 00:00:22.910
year with a lot of awesome updates so I
 

00:00:22.910 --> 00:00:24.780
year with a lot of awesome updates so I
really hope that you enjoy it

00:00:24.780 --> 00:00:24.790
really hope that you enjoy it
 

00:00:24.790 --> 00:00:28.389
really hope that you enjoy it
so what is this course all about this is

00:00:28.389 --> 00:00:28.399
so what is this course all about this is
 

00:00:28.399 --> 00:00:30.670
so what is this course all about this is
a one-week intensive boot camp on

00:00:30.670 --> 00:00:30.680
a one-week intensive boot camp on
 

00:00:30.680 --> 00:00:31.929
a one-week intensive boot camp on
everything deep learning

00:00:31.929 --> 00:00:31.939
everything deep learning
 

00:00:31.939 --> 00:00:33.970
everything deep learning
you'll get up close and personal with

00:00:33.970 --> 00:00:33.980
you'll get up close and personal with
 

00:00:33.980 --> 00:00:35.320
you'll get up close and personal with
some of the foundations of the

00:00:35.320 --> 00:00:35.330
some of the foundations of the
 

00:00:35.330 --> 00:00:37.360
some of the foundations of the
algorithms driving this remarkable field

00:00:37.360 --> 00:00:37.370
algorithms driving this remarkable field
 

00:00:37.370 --> 00:00:39.730
algorithms driving this remarkable field
and you'll actually learn how to build

00:00:39.730 --> 00:00:39.740
and you'll actually learn how to build
 

00:00:39.740 --> 00:00:42.220
and you'll actually learn how to build
some intelligent algorithms capable of

00:00:42.220 --> 00:00:42.230
some intelligent algorithms capable of
 

00:00:42.230 --> 00:00:46.720
some intelligent algorithms capable of
solving incredibly complex problems so

00:00:46.720 --> 00:00:46.730
solving incredibly complex problems so
 

00:00:46.730 --> 00:00:48.610
solving incredibly complex problems so
over the past couple years deep learning

00:00:48.610 --> 00:00:48.620
over the past couple years deep learning
 

00:00:48.620 --> 00:00:51.120
over the past couple years deep learning
has revolutionized many aspects of

00:00:51.120 --> 00:00:51.130
has revolutionized many aspects of
 

00:00:51.130 --> 00:00:53.860
has revolutionized many aspects of
research and industry including things

00:00:53.860 --> 00:00:53.870
research and industry including things
 

00:00:53.870 --> 00:00:56.440
research and industry including things
like autonomous vehicles medicine and

00:00:56.440 --> 00:00:56.450
like autonomous vehicles medicine and
 

00:00:56.450 --> 00:00:58.830
like autonomous vehicles medicine and
healthcare reinforcement learning

00:00:58.830 --> 00:00:58.840
healthcare reinforcement learning
 

00:00:58.840 --> 00:01:02.410
healthcare reinforcement learning
generative modeling robotics and a whole

00:01:02.410 --> 00:01:02.420
generative modeling robotics and a whole
 

00:01:02.420 --> 00:01:05.020
generative modeling robotics and a whole
host of other applications like natural

00:01:05.020 --> 00:01:05.030
host of other applications like natural
 

00:01:05.030 --> 00:01:07.530
host of other applications like natural
language processing finance and security

00:01:07.530 --> 00:01:07.540
language processing finance and security
 

00:01:07.540 --> 00:01:09.969
language processing finance and security
but before we talk about that I think we

00:01:09.969 --> 00:01:09.979
but before we talk about that I think we
 

00:01:09.979 --> 00:01:11.469
but before we talk about that I think we
should start by taking a step back and

00:01:11.469 --> 00:01:11.479
should start by taking a step back and
 

00:01:11.479 --> 00:01:13.810
should start by taking a step back and
talking about something at the core of

00:01:13.810 --> 00:01:13.820
talking about something at the core of
 

00:01:13.820 --> 00:01:16.240
talking about something at the core of
this class which is intelligence what is

00:01:16.240 --> 00:01:16.250
this class which is intelligence what is
 

00:01:16.250 --> 00:01:19.090
this class which is intelligence what is
intelligence well I like to define

00:01:19.090 --> 00:01:19.100
intelligence well I like to define
 

00:01:19.100 --> 00:01:21.430
intelligence well I like to define
intelligence as the ability to process

00:01:21.430 --> 00:01:21.440
intelligence as the ability to process
 

00:01:21.440 --> 00:01:24.780
intelligence as the ability to process
information to inform future decisions

00:01:24.780 --> 00:01:24.790
information to inform future decisions
 

00:01:24.790 --> 00:01:27.490
information to inform future decisions
the field of artificial intelligence is

00:01:27.490 --> 00:01:27.500
the field of artificial intelligence is
 

00:01:27.500 --> 00:01:29.860
the field of artificial intelligence is
actually building algorithms artificial

00:01:29.860 --> 00:01:29.870
actually building algorithms artificial
 

00:01:29.870 --> 00:01:31.410
actually building algorithms artificial
algorithms to do exactly that

00:01:31.410 --> 00:01:31.420
algorithms to do exactly that
 

00:01:31.420 --> 00:01:34.480
algorithms to do exactly that
bit process information to inform future

00:01:34.480 --> 00:01:34.490
bit process information to inform future
 

00:01:34.490 --> 00:01:37.539
bit process information to inform future
predictions now machine learning is

00:01:37.539 --> 00:01:37.549
predictions now machine learning is
 

00:01:37.549 --> 00:01:39.340
predictions now machine learning is
simply a subset of artificial

00:01:39.340 --> 00:01:39.350
simply a subset of artificial
 

00:01:39.350 --> 00:01:41.710
simply a subset of artificial
intelligence or AI that actually focuses

00:01:41.710 --> 00:01:41.720
intelligence or AI that actually focuses
 

00:01:41.720 --> 00:01:45.280
intelligence or AI that actually focuses
on teaching an algorithm how to take

00:01:45.280 --> 00:01:45.290
on teaching an algorithm how to take
 

00:01:45.290 --> 00:01:48.039
on teaching an algorithm how to take
information and do this without

00:01:48.039 --> 00:01:48.049
information and do this without
 

00:01:48.049 --> 00:01:50.020
information and do this without
explicitly being told the sequence of

00:01:50.020 --> 00:01:50.030
explicitly being told the sequence of
 

00:01:50.030 --> 00:01:52.800
explicitly being told the sequence of
rules but instead learn the sequence of

00:01:52.800 --> 00:01:52.810
rules but instead learn the sequence of
 

00:01:52.810 --> 00:01:56.740
rules but instead learn the sequence of
patterns from the data itself deep

00:01:56.740 --> 00:01:56.750
patterns from the data itself deep
 

00:01:56.750 --> 00:01:58.359
patterns from the data itself deep
learning is simply a subset of machine

00:01:58.359 --> 00:01:58.369
learning is simply a subset of machine
 

00:01:58.369 --> 00:02:00.010
learning is simply a subset of machine
learning which takes this idea one step

00:02:00.010 --> 00:02:00.020
learning which takes this idea one step
 

00:02:00.020 --> 00:02:02.320
learning which takes this idea one step
further and actually tries to extract

00:02:02.320 --> 00:02:02.330
further and actually tries to extract
 

00:02:02.330 --> 00:02:04.510
further and actually tries to extract
these patterns automatically from raw

00:02:04.510 --> 00:02:04.520
these patterns automatically from raw
 

00:02:04.520 --> 00:02:07.510
these patterns automatically from raw
data without being needed without the

00:02:07.510 --> 00:02:07.520
data without being needed without the
 

00:02:07.520 --> 00:02:10.839
data without being needed without the
need to for the human to actually come

00:02:10.839 --> 00:02:10.849
need to for the human to actually come
 

00:02:10.849 --> 00:02:13.330
need to for the human to actually come
in and annotate these rules that the

00:02:13.330 --> 00:02:13.340
in and annotate these rules that the
 

00:02:13.340 --> 00:02:14.809
in and annotate these rules that the
system needs to learn

00:02:14.809 --> 00:02:14.819
system needs to learn
 

00:02:14.819 --> 00:02:18.030
system needs to learn
and that's what this class is all about

00:02:18.030 --> 00:02:18.040
and that's what this class is all about
 

00:02:18.040 --> 00:02:20.309
and that's what this class is all about
teaching algorithms how to learn a task

00:02:20.309 --> 00:02:20.319
teaching algorithms how to learn a task
 

00:02:20.319 --> 00:02:23.250
teaching algorithms how to learn a task
from raw data we want to provide you

00:02:23.250 --> 00:02:23.260
from raw data we want to provide you
 

00:02:23.260 --> 00:02:25.920
from raw data we want to provide you
with a solid foundation that so that you

00:02:25.920 --> 00:02:25.930
with a solid foundation that so that you
 

00:02:25.930 --> 00:02:27.240
with a solid foundation that so that you
can learn how these algorithms work

00:02:27.240 --> 00:02:27.250
can learn how these algorithms work
 

00:02:27.250 --> 00:02:29.820
can learn how these algorithms work
under the hood and with the practical

00:02:29.820 --> 00:02:29.830
under the hood and with the practical
 

00:02:29.830 --> 00:02:30.809
under the hood and with the practical
skills so that you can actually

00:02:30.809 --> 00:02:30.819
skills so that you can actually
 

00:02:30.819 --> 00:02:32.900
skills so that you can actually
implement these algorithms from scratch

00:02:32.900 --> 00:02:32.910
implement these algorithms from scratch
 

00:02:32.910 --> 00:02:35.220
implement these algorithms from scratch
using deep learning frameworks like

00:02:35.220 --> 00:02:35.230
using deep learning frameworks like
 

00:02:35.230 --> 00:02:37.259
using deep learning frameworks like
tensor flow which is the current most

00:02:37.259 --> 00:02:37.269
tensor flow which is the current most
 

00:02:37.269 --> 00:02:39.059
tensor flow which is the current most
popular deep learning framework that you

00:02:39.059 --> 00:02:39.069
popular deep learning framework that you
 

00:02:39.069 --> 00:02:42.440
popular deep learning framework that you
can code some of neural networks and

00:02:42.440 --> 00:02:42.450
can code some of neural networks and
 

00:02:42.450 --> 00:02:44.580
can code some of neural networks and
deep learning model and other deep

00:02:44.580 --> 00:02:44.590
deep learning model and other deep
 

00:02:44.590 --> 00:02:48.000
deep learning model and other deep
learning models we have an amazing set

00:02:48.000 --> 00:02:48.010
learning models we have an amazing set
 

00:02:48.010 --> 00:02:49.830
learning models we have an amazing set
of lectures lined up for you this week

00:02:49.830 --> 00:02:49.840
of lectures lined up for you this week
 

00:02:49.840 --> 00:02:51.750
of lectures lined up for you this week
including today which will kick off an

00:02:51.750 --> 00:02:51.760
including today which will kick off an
 

00:02:51.760 --> 00:02:54.289
including today which will kick off an
introduction on neural networks and

00:02:54.289 --> 00:02:54.299
introduction on neural networks and
 

00:02:54.299 --> 00:02:56.340
introduction on neural networks and
sequence based modeling which you'll

00:02:56.340 --> 00:02:56.350
sequence based modeling which you'll
 

00:02:56.350 --> 00:02:57.539
sequence based modeling which you'll
hear about in the second part of the

00:02:57.539 --> 00:02:57.549
hear about in the second part of the
 

00:02:57.549 --> 00:03:00.570
hear about in the second part of the
class tomorrow we'll cover some about

00:03:00.570 --> 00:03:00.580
class tomorrow we'll cover some about
 

00:03:00.580 --> 00:03:03.050
class tomorrow we'll cover some about
some stuff about computer vision and

00:03:03.050 --> 00:03:03.060
some stuff about computer vision and
 

00:03:03.060 --> 00:03:08.190
some stuff about computer vision and
deep generative modeling and the day

00:03:08.190 --> 00:03:08.200
deep generative modeling and the day
 

00:03:08.200 --> 00:03:09.470
deep generative modeling and the day
after that we'll talk even about

00:03:09.470 --> 00:03:09.480
after that we'll talk even about
 

00:03:09.480 --> 00:03:12.300
after that we'll talk even about
reinforcement learning and end on some

00:03:12.300 --> 00:03:12.310
reinforcement learning and end on some
 

00:03:12.310 --> 00:03:15.059
reinforcement learning and end on some
of the challenges and limitations of the

00:03:15.059 --> 00:03:15.069
of the challenges and limitations of the
 

00:03:15.069 --> 00:03:17.460
of the challenges and limitations of the
current deep learning approaches and and

00:03:17.460 --> 00:03:17.470
current deep learning approaches and and
 

00:03:17.470 --> 00:03:19.349
current deep learning approaches and and
kind of touch on how we can move forward

00:03:19.349 --> 00:03:19.359
kind of touch on how we can move forward
 

00:03:19.359 --> 00:03:22.800
kind of touch on how we can move forward
as a field past these challenges we'll

00:03:22.800 --> 00:03:22.810
as a field past these challenges we'll
 

00:03:22.810 --> 00:03:24.509
as a field past these challenges we'll
also spend the final two days hearing

00:03:24.509 --> 00:03:24.519
also spend the final two days hearing
 

00:03:24.519 --> 00:03:27.569
also spend the final two days hearing
from some guest lectures from top a AI

00:03:27.569 --> 00:03:27.579
from some guest lectures from top a AI
 

00:03:27.579 --> 00:03:31.229
from some guest lectures from top a AI
researchers these are bound to be

00:03:31.229 --> 00:03:31.239
researchers these are bound to be
 

00:03:31.239 --> 00:03:34.380
researchers these are bound to be
extremely interesting though we have

00:03:34.380 --> 00:03:34.390
extremely interesting though we have
 

00:03:34.390 --> 00:03:37.530
extremely interesting though we have
speakers from Nvidia IBM Google coming

00:03:37.530 --> 00:03:37.540
speakers from Nvidia IBM Google coming
 

00:03:37.540 --> 00:03:39.030
speakers from Nvidia IBM Google coming
to give talks so I highly recommend

00:03:39.030 --> 00:03:39.040
to give talks so I highly recommend
 

00:03:39.040 --> 00:03:42.240
to give talks so I highly recommend
attending these as well and finally the

00:03:42.240 --> 00:03:42.250
attending these as well and finally the
 

00:03:42.250 --> 00:03:43.979
attending these as well and finally the
class will conclude with some final

00:03:43.979 --> 00:03:43.989
class will conclude with some final
 

00:03:43.989 --> 00:03:45.990
class will conclude with some final
project presentations from students like

00:03:45.990 --> 00:03:46.000
project presentations from students like
 

00:03:46.000 --> 00:03:48.059
project presentations from students like
you and the audience will where you'll

00:03:48.059 --> 00:03:48.069
you and the audience will where you'll
 

00:03:48.069 --> 00:03:50.129
you and the audience will where you'll
present some final projects for this

00:03:50.129 --> 00:03:50.139
present some final projects for this
 

00:03:50.139 --> 00:03:52.080
present some final projects for this
class and then we'll end on an award

00:03:52.080 --> 00:03:52.090
class and then we'll end on an award
 

00:03:52.090 --> 00:03:56.610
class and then we'll end on an award
ceremony to celebrate so as you might

00:03:56.610 --> 00:03:56.620
ceremony to celebrate so as you might
 

00:03:56.620 --> 00:03:58.770
ceremony to celebrate so as you might
have seen or heard already this class is

00:03:58.770 --> 00:03:58.780
have seen or heard already this class is
 

00:03:58.780 --> 00:04:00.210
have seen or heard already this class is
offered for credit you can take this

00:04:00.210 --> 00:04:00.220
offered for credit you can take this
 

00:04:00.220 --> 00:04:02.430
offered for credit you can take this
class for grade and if you're taking

00:04:02.430 --> 00:04:02.440
class for grade and if you're taking
 

00:04:02.440 --> 00:04:03.690
class for grade and if you're taking
this class for grade you have two

00:04:03.690 --> 00:04:03.700
this class for grade you have two
 

00:04:03.700 --> 00:04:04.949
this class for grade you have two
options to fulfill your grade

00:04:04.949 --> 00:04:04.959
options to fulfill your grade
 

00:04:04.959 --> 00:04:07.979
options to fulfill your grade
requirement first option is that you can

00:04:07.979 --> 00:04:07.989
requirement first option is that you can
 

00:04:07.989 --> 00:04:10.080
requirement first option is that you can
actually do a project proposal where you

00:04:10.080 --> 00:04:10.090
actually do a project proposal where you
 

00:04:10.090 --> 00:04:12.360
actually do a project proposal where you
will present your project on the final

00:04:12.360 --> 00:04:12.370
will present your project on the final
 

00:04:12.370 --> 00:04:13.530
will present your project on the final
day of class that's what I was saying

00:04:13.530 --> 00:04:13.540
day of class that's what I was saying
 

00:04:13.540 --> 00:04:15.360
day of class that's what I was saying
before on Friday you can present your

00:04:15.360 --> 00:04:15.370
before on Friday you can present your
 

00:04:15.370 --> 00:04:17.610
before on Friday you can present your
project and this is just a three minute

00:04:17.610 --> 00:04:17.620
project and this is just a three minute
 

00:04:17.620 --> 00:04:19.440
project and this is just a three minute
presentation we'll be very strict on the

00:04:19.440 --> 00:04:19.450
presentation we'll be very strict on the
 

00:04:19.450 --> 00:04:22.379
presentation we'll be very strict on the
time here and we realized that one week

00:04:22.379 --> 00:04:22.389
time here and we realized that one week
 

00:04:22.389 --> 00:04:24.089
time here and we realized that one week
is a super short time to actually come

00:04:24.089 --> 00:04:24.099
is a super short time to actually come
 

00:04:24.099 --> 00:04:25.649
is a super short time to actually come
up with a deep learning project so we're

00:04:25.649 --> 00:04:25.659
up with a deep learning project so we're
 

00:04:25.659 --> 00:04:27.360
up with a deep learning project so we're
not going to actually be judging you on

00:04:27.360 --> 00:04:27.370
not going to actually be judging you on
 

00:04:27.370 --> 00:04:28.320
not going to actually be judging you on
the results

00:04:28.320 --> 00:04:28.330
the results
 

00:04:28.330 --> 00:04:30.689
the results
you create during this week instead what

00:04:30.689 --> 00:04:30.699
you create during this week instead what
 

00:04:30.699 --> 00:04:32.550
you create during this week instead what
we're looking for is the novelty of the

00:04:32.550 --> 00:04:32.560
we're looking for is the novelty of the
 

00:04:32.560 --> 00:04:34.409
we're looking for is the novelty of the
ideas and how well you can present it

00:04:34.409 --> 00:04:34.419
ideas and how well you can present it
 

00:04:34.419 --> 00:04:36.180
ideas and how well you can present it
given such a short amount of time in

00:04:36.180 --> 00:04:36.190
given such a short amount of time in
 

00:04:36.190 --> 00:04:40.110
given such a short amount of time in
three minutes and we kind of think it's

00:04:40.110 --> 00:04:40.120
three minutes and we kind of think it's
 

00:04:40.120 --> 00:04:41.939
three minutes and we kind of think it's
like an art to being able to present

00:04:41.939 --> 00:04:41.949
like an art to being able to present
 

00:04:41.949 --> 00:04:44.730
like an art to being able to present
something in just three minutes so we

00:04:44.730 --> 00:04:44.740
something in just three minutes so we
 

00:04:44.740 --> 00:04:46.200
something in just three minutes so we
kind of want to hold you to that tight

00:04:46.200 --> 00:04:46.210
kind of want to hold you to that tight
 

00:04:46.210 --> 00:04:48.899
kind of want to hold you to that tight
time schedule and kind of enforce it

00:04:48.899 --> 00:04:48.909
time schedule and kind of enforce it
 

00:04:48.909 --> 00:04:50.850
time schedule and kind of enforce it
very tightly just so that you're forced

00:04:50.850 --> 00:04:50.860
very tightly just so that you're forced
 

00:04:50.860 --> 00:04:52.860
very tightly just so that you're forced
to really think about what is the core

00:04:52.860 --> 00:04:52.870
to really think about what is the core
 

00:04:52.870 --> 00:04:54.540
to really think about what is the core
idea that you want to present to us on

00:04:54.540 --> 00:04:54.550
idea that you want to present to us on
 

00:04:54.550 --> 00:04:58.830
idea that you want to present to us on
Friday your projects your presentations

00:04:58.830 --> 00:04:58.840
Friday your projects your presentations
 

00:04:58.840 --> 00:05:00.990
Friday your projects your presentations
will be judged by a panel of judges and

00:05:00.990 --> 00:05:01.000
will be judged by a panel of judges and
 

00:05:01.000 --> 00:05:05.219
will be judged by a panel of judges and
will be awarding GPUs and some home

00:05:05.219 --> 00:05:05.229
will be awarding GPUs and some home
 

00:05:05.229 --> 00:05:07.950
will be awarding GPUs and some home
Google home AI assistants this year

00:05:07.950 --> 00:05:07.960
Google home AI assistants this year
 

00:05:07.960 --> 00:05:10.740
Google home AI assistants this year
we're offering three NVIDIA GPUs each

00:05:10.740 --> 00:05:10.750
we're offering three NVIDIA GPUs each
 

00:05:10.750 --> 00:05:13.320
we're offering three NVIDIA GPUs each
one worth over $1,000 some of you know

00:05:13.320 --> 00:05:13.330
one worth over $1,000 some of you know
 

00:05:13.330 --> 00:05:16.409
one worth over $1,000 some of you know
these GPUs are the backbone of doing

00:05:16.409 --> 00:05:16.419
these GPUs are the backbone of doing
 

00:05:16.419 --> 00:05:18.540
these GPUs are the backbone of doing
cutting-edge deep learning research and

00:05:18.540 --> 00:05:18.550
cutting-edge deep learning research and
 

00:05:18.550 --> 00:05:21.659
cutting-edge deep learning research and
it's really foundational or essential if

00:05:21.659 --> 00:05:21.669
it's really foundational or essential if
 

00:05:21.669 --> 00:05:22.950
it's really foundational or essential if
you want to be doing this kind of

00:05:22.950 --> 00:05:22.960
you want to be doing this kind of
 

00:05:22.960 --> 00:05:24.689
you want to be doing this kind of
research so we're really happy that we

00:05:24.689 --> 00:05:24.699
research so we're really happy that we
 

00:05:24.699 --> 00:05:26.700
research so we're really happy that we
can offer you these types this type of

00:05:26.700 --> 00:05:26.710
can offer you these types this type of
 

00:05:26.710 --> 00:05:30.390
can offer you these types this type of
hardware the second option if you don't

00:05:30.390 --> 00:05:30.400
hardware the second option if you don't
 

00:05:30.400 --> 00:05:32.309
hardware the second option if you don't
want to do the project presentation but

00:05:32.309 --> 00:05:32.319
want to do the project presentation but
 

00:05:32.319 --> 00:05:33.809
want to do the project presentation but
you still want to receive credit for

00:05:33.809 --> 00:05:33.819
you still want to receive credit for
 

00:05:33.819 --> 00:05:35.700
you still want to receive credit for
this class you can do the second option

00:05:35.700 --> 00:05:35.710
this class you can do the second option
 

00:05:35.710 --> 00:05:37.950
this class you can do the second option
which is a little more boring in my

00:05:37.950 --> 00:05:37.960
which is a little more boring in my
 

00:05:37.960 --> 00:05:40.230
which is a little more boring in my
opinion but you can write a one-page

00:05:40.230 --> 00:05:40.240
opinion but you can write a one-page
 

00:05:40.240 --> 00:05:43.709
opinion but you can write a one-page
review of a deep learning paper and this

00:05:43.709 --> 00:05:43.719
review of a deep learning paper and this
 

00:05:43.719 --> 00:05:45.330
review of a deep learning paper and this
will be doing the last day of class and

00:05:45.330 --> 00:05:45.340
will be doing the last day of class and
 

00:05:45.340 --> 00:05:47.370
will be doing the last day of class and
this is for people I don't want to do

00:05:47.370 --> 00:05:47.380
this is for people I don't want to do
 

00:05:47.380 --> 00:05:48.719
this is for people I don't want to do
the project presentation but you still

00:05:48.719 --> 00:05:48.729
the project presentation but you still
 

00:05:48.729 --> 00:05:53.580
the project presentation but you still
want to get credit for this class please

00:05:53.580 --> 00:05:53.590
want to get credit for this class please
 

00:05:53.590 --> 00:05:55.140
want to get credit for this class please
post to Piazza if you have questions

00:05:55.140 --> 00:05:55.150
post to Piazza if you have questions
 

00:05:55.150 --> 00:05:57.180
post to Piazza if you have questions
about the labs that we'll be doing today

00:05:57.180 --> 00:05:57.190
about the labs that we'll be doing today
 

00:05:57.190 --> 00:05:59.399
about the labs that we'll be doing today
or any of the future days if you have

00:05:59.399 --> 00:05:59.409
or any of the future days if you have
 

00:05:59.409 --> 00:06:00.570
or any of the future days if you have
questions about the course in general

00:06:00.570 --> 00:06:00.580
questions about the course in general
 

00:06:00.580 --> 00:06:02.670
questions about the course in general
there's course information on the

00:06:02.670 --> 00:06:02.680
there's course information on the
 

00:06:02.680 --> 00:06:05.550
there's course information on the
website enter deep learning com

00:06:05.550 --> 00:06:05.560
website enter deep learning com
 

00:06:05.560 --> 00:06:07.260
website enter deep learning com
along with announcements digital

00:06:07.260 --> 00:06:07.270
along with announcements digital
 

00:06:07.270 --> 00:06:09.570
along with announcements digital
recordings as well as slides for these

00:06:09.570 --> 00:06:09.580
recordings as well as slides for these
 

00:06:09.580 --> 00:06:11.670
recordings as well as slides for these
classes today's slides are already

00:06:11.670 --> 00:06:11.680
classes today's slides are already
 

00:06:11.680 --> 00:06:13.350
classes today's slides are already
released so you can find everything

00:06:13.350 --> 00:06:13.360
released so you can find everything
 

00:06:13.360 --> 00:06:15.450
released so you can find everything
online and of course if you have any

00:06:15.450 --> 00:06:15.460
online and of course if you have any
 

00:06:15.460 --> 00:06:17.399
online and of course if you have any
questions you can email us at intro to

00:06:17.399 --> 00:06:17.409
questions you can email us at intro to
 

00:06:17.409 --> 00:06:20.550
questions you can email us at intro to
deep learning - staff at MIT edu this

00:06:20.550 --> 00:06:20.560
deep learning - staff at MIT edu this
 

00:06:20.560 --> 00:06:22.140
deep learning - staff at MIT edu this
course has an incredible team that you

00:06:22.140 --> 00:06:22.150
course has an incredible team that you
 

00:06:22.150 --> 00:06:23.490
course has an incredible team that you
can reach out to in case you have any

00:06:23.490 --> 00:06:23.500
can reach out to in case you have any
 

00:06:23.500 --> 00:06:26.459
can reach out to in case you have any
questions or issues about anything so

00:06:26.459 --> 00:06:26.469
questions or issues about anything so
 

00:06:26.469 --> 00:06:28.890
questions or issues about anything so
please don't hesitate to reach out and

00:06:28.890 --> 00:06:28.900
please don't hesitate to reach out and
 

00:06:28.900 --> 00:06:30.719
please don't hesitate to reach out and
finally we want to give a huge thanks to

00:06:30.719 --> 00:06:30.729
finally we want to give a huge thanks to
 

00:06:30.729 --> 00:06:32.820
finally we want to give a huge thanks to
all of the sponsors that made this

00:06:32.820 --> 00:06:32.830
all of the sponsors that made this
 

00:06:32.830 --> 00:06:36.659
all of the sponsors that made this
course possible so now let's start with

00:06:36.659 --> 00:06:36.669
course possible so now let's start with
 

00:06:36.669 --> 00:06:39.180
course possible so now let's start with
the fun stuff and actually let's start

00:06:39.180 --> 00:06:39.190
the fun stuff and actually let's start
 

00:06:39.190 --> 00:06:41.460
the fun stuff and actually let's start
by asking ourselves a question

00:06:41.460 --> 00:06:41.470
by asking ourselves a question
 

00:06:41.470 --> 00:06:44.340
by asking ourselves a question
why do we even care about this class why

00:06:44.340 --> 00:06:44.350
why do we even care about this class why
 

00:06:44.350 --> 00:06:46.470
why do we even care about this class why
did you all come here today what is why

00:06:46.470 --> 00:06:46.480
did you all come here today what is why
 

00:06:46.480 --> 00:06:47.960
did you all come here today what is why
do we care about deep learning well

00:06:47.960 --> 00:06:47.970
do we care about deep learning well
 

00:06:47.970 --> 00:06:50.330
do we care about deep learning well
traditional machine learning algorithms

00:06:50.330 --> 00:06:50.340
traditional machine learning algorithms
 

00:06:50.340 --> 00:06:53.160
traditional machine learning algorithms
typically define sets of rules or

00:06:53.160 --> 00:06:53.170
typically define sets of rules or
 

00:06:53.170 --> 00:06:57.300
typically define sets of rules or
features that you want to extract from

00:06:57.300 --> 00:06:57.310
features that you want to extract from
 

00:06:57.310 --> 00:06:59.340
features that you want to extract from
the data usually these are hand

00:06:59.340 --> 00:06:59.350
the data usually these are hand
 

00:06:59.350 --> 00:07:00.960
the data usually these are hand
engineered features and they tend to be

00:07:00.960 --> 00:07:00.970
engineered features and they tend to be
 

00:07:00.970 --> 00:07:03.060
engineered features and they tend to be
extremely brittle in practice

00:07:03.060 --> 00:07:03.070
extremely brittle in practice
 

00:07:03.070 --> 00:07:05.640
extremely brittle in practice
now the key idea is a key insight of

00:07:05.640 --> 00:07:05.650
now the key idea is a key insight of
 

00:07:05.650 --> 00:07:08.490
now the key idea is a key insight of
deep learning is that let's not hand

00:07:08.490 --> 00:07:08.500
deep learning is that let's not hand
 

00:07:08.500 --> 00:07:10.410
deep learning is that let's not hand
engineer these features instead let's

00:07:10.410 --> 00:07:10.420
engineer these features instead let's
 

00:07:10.420 --> 00:07:13.020
engineer these features instead let's
learn them directly from raw data that

00:07:13.020 --> 00:07:13.030
learn them directly from raw data that
 

00:07:13.030 --> 00:07:16.500
learn them directly from raw data that
is can we learn in order to detect the

00:07:16.500 --> 00:07:16.510
is can we learn in order to detect the
 

00:07:16.510 --> 00:07:19.230
is can we learn in order to detect the
face we can first detect the edges in

00:07:19.230 --> 00:07:19.240
face we can first detect the edges in
 

00:07:19.240 --> 00:07:21.750
face we can first detect the edges in
the picture compose these edges together

00:07:21.750 --> 00:07:21.760
the picture compose these edges together
 

00:07:21.760 --> 00:07:24.570
the picture compose these edges together
to start detecting things like eyes

00:07:24.570 --> 00:07:24.580
to start detecting things like eyes
 

00:07:24.580 --> 00:07:27.210
to start detecting things like eyes
mouth and nose and then composing these

00:07:27.210 --> 00:07:27.220
mouth and nose and then composing these
 

00:07:27.220 --> 00:07:30.570
mouth and nose and then composing these
features together to detect higher-level

00:07:30.570 --> 00:07:30.580
features together to detect higher-level
 

00:07:30.580 --> 00:07:34.080
features together to detect higher-level
structures in the face and and this is

00:07:34.080 --> 00:07:34.090
structures in the face and and this is
 

00:07:34.090 --> 00:07:35.430
structures in the face and and this is
all performed in a hierarchical manner

00:07:35.430 --> 00:07:35.440
all performed in a hierarchical manner
 

00:07:35.440 --> 00:07:37.200
all performed in a hierarchical manner
so the question of deep learning is how

00:07:37.200 --> 00:07:37.210
so the question of deep learning is how
 

00:07:37.210 --> 00:07:40.560
so the question of deep learning is how
can we go from raw image pixels or raw

00:07:40.560 --> 00:07:40.570
can we go from raw image pixels or raw
 

00:07:40.570 --> 00:07:44.520
can we go from raw image pixels or raw
data in general to a more complex and

00:07:44.520 --> 00:07:44.530
data in general to a more complex and
 

00:07:44.530 --> 00:07:46.530
data in general to a more complex and
complex representation as the data flows

00:07:46.530 --> 00:07:46.540
complex representation as the data flows
 

00:07:46.540 --> 00:07:50.580
complex representation as the data flows
through the model and actually the

00:07:50.580 --> 00:07:50.590
through the model and actually the
 

00:07:50.590 --> 00:07:52.620
through the model and actually the
fundamental fundamental building blocks

00:07:52.620 --> 00:07:52.630
fundamental fundamental building blocks
 

00:07:52.630 --> 00:07:54.390
fundamental fundamental building blocks
of deep learning have existed for

00:07:54.390 --> 00:07:54.400
of deep learning have existed for
 

00:07:54.400 --> 00:07:57.240
of deep learning have existed for
decades and their underlying algorithms

00:07:57.240 --> 00:07:57.250
decades and their underlying algorithms
 

00:07:57.250 --> 00:08:00.390
decades and their underlying algorithms
have been studied for many years even

00:08:00.390 --> 00:08:00.400
have been studied for many years even
 

00:08:00.400 --> 00:08:02.310
have been studied for many years even
before that so why are we studying this

00:08:02.310 --> 00:08:02.320
before that so why are we studying this
 

00:08:02.320 --> 00:08:06.720
before that so why are we studying this
now well for one data has become so

00:08:06.720 --> 00:08:06.730
now well for one data has become so
 

00:08:06.730 --> 00:08:08.340
now well for one data has become so
prevalent in today's society we're

00:08:08.340 --> 00:08:08.350
prevalent in today's society we're
 

00:08:08.350 --> 00:08:11.250
prevalent in today's society we're
living in the age of big data where we

00:08:11.250 --> 00:08:11.260
living in the age of big data where we
 

00:08:11.260 --> 00:08:12.900
living in the age of big data where we
have more access to data than ever

00:08:12.900 --> 00:08:12.910
have more access to data than ever
 

00:08:12.910 --> 00:08:14.909
have more access to data than ever
before and these models are hungry for

00:08:14.909 --> 00:08:14.919
before and these models are hungry for
 

00:08:14.919 --> 00:08:17.580
before and these models are hungry for
data so we need to feed them with all

00:08:17.580 --> 00:08:17.590
data so we need to feed them with all
 

00:08:17.590 --> 00:08:19.290
data so we need to feed them with all
the data and a lot of this datasets that

00:08:19.290 --> 00:08:19.300
the data and a lot of this datasets that
 

00:08:19.300 --> 00:08:21.210
the data and a lot of this datasets that
we have available like computer vision

00:08:21.210 --> 00:08:21.220
we have available like computer vision
 

00:08:21.220 --> 00:08:22.500
we have available like computer vision
datasets natural language processing

00:08:22.500 --> 00:08:22.510
datasets natural language processing
 

00:08:22.510 --> 00:08:25.320
datasets natural language processing
datasets this raw amount of data was

00:08:25.320 --> 00:08:25.330
datasets this raw amount of data was
 

00:08:25.330 --> 00:08:27.570
datasets this raw amount of data was
just not available when these algorithms

00:08:27.570 --> 00:08:27.580
just not available when these algorithms
 

00:08:27.580 --> 00:08:30.810
just not available when these algorithms
were created second these algorithms

00:08:30.810 --> 00:08:30.820
were created second these algorithms
 

00:08:30.820 --> 00:08:33.870
were created second these algorithms
require or these albums are massively

00:08:33.870 --> 00:08:33.880
require or these albums are massively
 

00:08:33.880 --> 00:08:36.029
require or these albums are massively
parallel lies about their core at their

00:08:36.029 --> 00:08:36.039
parallel lies about their core at their
 

00:08:36.039 --> 00:08:37.469
parallel lies about their core at their
most fundamental building blocks that

00:08:37.469 --> 00:08:37.479
most fundamental building blocks that
 

00:08:37.479 --> 00:08:39.060
most fundamental building blocks that
you'll learn today they're massively

00:08:39.060 --> 00:08:39.070
you'll learn today they're massively
 

00:08:39.070 --> 00:08:41.219
you'll learn today they're massively
paralyzed Abul and this means that they

00:08:41.219 --> 00:08:41.229
paralyzed Abul and this means that they
 

00:08:41.229 --> 00:08:43.230
paralyzed Abul and this means that they
can benefit tremendously from very

00:08:43.230 --> 00:08:43.240
can benefit tremendously from very
 

00:08:43.240 --> 00:08:46.280
can benefit tremendously from very
specialized hardware such as GPUs and

00:08:46.280 --> 00:08:46.290
specialized hardware such as GPUs and
 

00:08:46.290 --> 00:08:50.100
specialized hardware such as GPUs and
again technology like these GPUs simply

00:08:50.100 --> 00:08:50.110
again technology like these GPUs simply
 

00:08:50.110 --> 00:08:52.680
again technology like these GPUs simply
did not exist in the decades that deep

00:08:52.680 --> 00:08:52.690
did not exist in the decades that deep
 

00:08:52.690 --> 00:08:54.450
did not exist in the decades that deep
learning or the foundations of deep

00:08:54.450 --> 00:08:54.460
learning or the foundations of deep
 

00:08:54.460 --> 00:08:55.290
learning or the foundations of deep
learning were devil

00:08:55.290 --> 00:08:55.300
learning were devil
 

00:08:55.300 --> 00:08:57.990
learning were devil
and finally due to open-source tool

00:08:57.990 --> 00:08:58.000
and finally due to open-source tool
 

00:08:58.000 --> 00:08:59.670
and finally due to open-source tool
boxes like tensorflow which will you

00:08:59.670 --> 00:08:59.680
boxes like tensorflow which will you
 

00:08:59.680 --> 00:09:01.980
boxes like tensorflow which will you
learn to use in this class building and

00:09:01.980 --> 00:09:01.990
learn to use in this class building and
 

00:09:01.990 --> 00:09:04.260
learn to use in this class building and
deploying these models has become more

00:09:04.260 --> 00:09:04.270
deploying these models has become more
 

00:09:04.270 --> 00:09:05.880
deploying these models has become more
streamlined than ever before it is

00:09:05.880 --> 00:09:05.890
streamlined than ever before it is
 

00:09:05.890 --> 00:09:07.650
streamlined than ever before it is
becoming increasingly and increasingly

00:09:07.650 --> 00:09:07.660
becoming increasingly and increasingly
 

00:09:07.660 --> 00:09:11.340
becoming increasingly and increasingly
easy to abstract away all of the details

00:09:11.340 --> 00:09:11.350
easy to abstract away all of the details
 

00:09:11.350 --> 00:09:13.590
easy to abstract away all of the details
and build a neural network and train a

00:09:13.590 --> 00:09:13.600
and build a neural network and train a
 

00:09:13.600 --> 00:09:14.850
and build a neural network and train a
neural network and then deploy that

00:09:14.850 --> 00:09:14.860
neural network and then deploy that
 

00:09:14.860 --> 00:09:17.550
neural network and then deploy that
neural network in practice to solve a

00:09:17.550 --> 00:09:17.560
neural network in practice to solve a
 

00:09:17.560 --> 00:09:19.860
neural network in practice to solve a
very complex problem in just tens of

00:09:19.860 --> 00:09:19.870
very complex problem in just tens of
 

00:09:19.870 --> 00:09:21.390
very complex problem in just tens of
lines of code you can solve you can

00:09:21.390 --> 00:09:21.400
lines of code you can solve you can
 

00:09:21.400 --> 00:09:23.910
lines of code you can solve you can
create a facial classifier that's

00:09:23.910 --> 00:09:23.920
create a facial classifier that's
 

00:09:23.920 --> 00:09:26.220
create a facial classifier that's
capable of recognizing very complex

00:09:26.220 --> 00:09:26.230
capable of recognizing very complex
 

00:09:26.230 --> 00:09:30.930
capable of recognizing very complex
faces from the environment so let's

00:09:30.930 --> 00:09:30.940
faces from the environment so let's
 

00:09:30.940 --> 00:09:32.760
faces from the environment so let's
start with the most fundamental building

00:09:32.760 --> 00:09:32.770
start with the most fundamental building
 

00:09:32.770 --> 00:09:35.400
start with the most fundamental building
block of deep learning and that's the

00:09:35.400 --> 00:09:35.410
block of deep learning and that's the
 

00:09:35.410 --> 00:09:36.870
block of deep learning and that's the
fundamental building block that makes up

00:09:36.870 --> 00:09:36.880
fundamental building block that makes up
 

00:09:36.880 --> 00:09:39.030
fundamental building block that makes up
a neural network and that is a neuron so

00:09:39.030 --> 00:09:39.040
a neural network and that is a neuron so
 

00:09:39.040 --> 00:09:41.520
a neural network and that is a neuron so
what is the neuron in deep learning we

00:09:41.520 --> 00:09:41.530
what is the neuron in deep learning we
 

00:09:41.530 --> 00:09:44.280
what is the neuron in deep learning we
call it a perceptron and how does it

00:09:44.280 --> 00:09:44.290
call it a perceptron and how does it
 

00:09:44.290 --> 00:09:48.270
call it a perceptron and how does it
work so the idea of a perceptron or a

00:09:48.270 --> 00:09:48.280
work so the idea of a perceptron or a
 

00:09:48.280 --> 00:09:51.450
work so the idea of a perceptron or a
single neuron is very simple let's start

00:09:51.450 --> 00:09:51.460
single neuron is very simple let's start
 

00:09:51.460 --> 00:09:53.370
single neuron is very simple let's start
by talking about and describing the

00:09:53.370 --> 00:09:53.380
by talking about and describing the
 

00:09:53.380 --> 00:09:56.850
by talking about and describing the
feed-forward information of information

00:09:56.850 --> 00:09:56.860
feed-forward information of information
 

00:09:56.860 --> 00:09:59.970
feed-forward information of information
through that model we define a set of

00:09:59.970 --> 00:09:59.980
through that model we define a set of
 

00:09:59.980 --> 00:10:02.880
through that model we define a set of
inputs x1 through XM which you can see

00:10:02.880 --> 00:10:02.890
inputs x1 through XM which you can see
 

00:10:02.890 --> 00:10:05.820
inputs x1 through XM which you can see
on the left hand side and each of these

00:10:05.820 --> 00:10:05.830
on the left hand side and each of these
 

00:10:05.830 --> 00:10:07.440
on the left hand side and each of these
inputs are actually multiplied by a

00:10:07.440 --> 00:10:07.450
inputs are actually multiplied by a
 

00:10:07.450 --> 00:10:13.650
inputs are actually multiplied by a
corresponding weight w1 through WM so

00:10:13.650 --> 00:10:13.660
corresponding weight w1 through WM so
 

00:10:13.660 --> 00:10:16.920
corresponding weight w1 through WM so
you can imagine if you have x1 you x w1

00:10:16.920 --> 00:10:16.930
you can imagine if you have x1 you x w1
 

00:10:16.930 --> 00:10:20.100
you can imagine if you have x1 you x w1
you have x2 you x w2 and so on you take

00:10:20.100 --> 00:10:20.110
you have x2 you x w2 and so on you take
 

00:10:20.110 --> 00:10:22.320
you have x2 you x w2 and so on you take
all of those multiplications and you add

00:10:22.320 --> 00:10:22.330
all of those multiplications and you add
 

00:10:22.330 --> 00:10:24.390
all of those multiplications and you add
them up so these come together in a

00:10:24.390 --> 00:10:24.400
them up so these come together in a
 

00:10:24.400 --> 00:10:26.640
them up so these come together in a
summation and then you pass this

00:10:26.640 --> 00:10:26.650
summation and then you pass this
 

00:10:26.650 --> 00:10:29.250
summation and then you pass this
weighted sum through a nonlinear

00:10:29.250 --> 00:10:29.260
weighted sum through a nonlinear
 

00:10:29.260 --> 00:10:31.230
weighted sum through a nonlinear
activation function to produce a final

00:10:31.230 --> 00:10:31.240
activation function to produce a final
 

00:10:31.240 --> 00:10:36.180
activation function to produce a final
output which we'll call Y so that's

00:10:36.180 --> 00:10:36.190
output which we'll call Y so that's
 

00:10:36.190 --> 00:10:40.230
output which we'll call Y so that's
really simple let's I actually left out

00:10:40.230 --> 00:10:40.240
really simple let's I actually left out
 

00:10:40.240 --> 00:10:41.820
really simple let's I actually left out
one detail in that previous slide so

00:10:41.820 --> 00:10:41.830
one detail in that previous slide so
 

00:10:41.830 --> 00:10:44.280
one detail in that previous slide so
I'll add it here now we also have this

00:10:44.280 --> 00:10:44.290
I'll add it here now we also have this
 

00:10:44.290 --> 00:10:46.830
I'll add it here now we also have this
other turn term this green term which is

00:10:46.830 --> 00:10:46.840
other turn term this green term which is
 

00:10:46.840 --> 00:10:48.960
other turn term this green term which is
a bias term which allows you to shift

00:10:48.960 --> 00:10:48.970
a bias term which allows you to shift
 

00:10:48.970 --> 00:10:51.030
a bias term which allows you to shift
your activation function left and right

00:10:51.030 --> 00:10:51.040
your activation function left and right
 

00:10:51.040 --> 00:10:54.330
your activation function left and right
and now on the right side you can kind

00:10:54.330 --> 00:10:54.340
and now on the right side you can kind
 

00:10:54.340 --> 00:10:56.490
and now on the right side you can kind
of see this diagram illustrated as a

00:10:56.490 --> 00:10:56.500
of see this diagram illustrated as a
 

00:10:56.500 --> 00:10:59.070
of see this diagram illustrated as a
mathematical formula as a single

00:10:59.070 --> 00:10:59.080
mathematical formula as a single
 

00:10:59.080 --> 00:11:01.110
mathematical formula as a single
equation we can actually rewrite this

00:11:01.110 --> 00:11:01.120
equation we can actually rewrite this
 

00:11:01.120 --> 00:11:03.750
equation we can actually rewrite this
now using linear algebra using vectors

00:11:03.750 --> 00:11:03.760
now using linear algebra using vectors
 

00:11:03.760 --> 00:11:06.660
now using linear algebra using vectors
dot products and matrices so let's do

00:11:06.660 --> 00:11:06.670
dot products and matrices so let's do
 

00:11:06.670 --> 00:11:09.240
dot products and matrices so let's do
that so now

00:11:09.240 --> 00:11:09.250
that so now
 

00:11:09.250 --> 00:11:12.450
that so now
is a vector of our inputs x1 through M

00:11:12.450 --> 00:11:12.460
is a vector of our inputs x1 through M
 

00:11:12.460 --> 00:11:14.790
is a vector of our inputs x1 through M
so instead of now a single number X

00:11:14.790 --> 00:11:14.800
so instead of now a single number X
 

00:11:14.800 --> 00:11:17.160
so instead of now a single number X
capital X is a vector of all of the

00:11:17.160 --> 00:11:17.170
capital X is a vector of all of the
 

00:11:17.170 --> 00:11:20.160
capital X is a vector of all of the
inputs capital W is a vector of all of

00:11:20.160 --> 00:11:20.170
inputs capital W is a vector of all of
 

00:11:20.170 --> 00:11:22.860
inputs capital W is a vector of all of
the weights 1 through m and we can

00:11:22.860 --> 00:11:22.870
the weights 1 through m and we can
 

00:11:22.870 --> 00:11:24.870
the weights 1 through m and we can
simply take their weighted sum by taking

00:11:24.870 --> 00:11:24.880
simply take their weighted sum by taking
 

00:11:24.880 --> 00:11:26.340
simply take their weighted sum by taking
the dot product between these two

00:11:26.340 --> 00:11:26.350
the dot product between these two
 

00:11:26.350 --> 00:11:30.840
the dot product between these two
vectors then we add our bias like I said

00:11:30.840 --> 00:11:30.850
vectors then we add our bias like I said
 

00:11:30.850 --> 00:11:33.060
vectors then we add our bias like I said
before or biased now is a single number

00:11:33.060 --> 00:11:33.070
before or biased now is a single number
 

00:11:33.070 --> 00:11:36.570
before or biased now is a single number
W not and applying that non linear term

00:11:36.570 --> 00:11:36.580
W not and applying that non linear term
 

00:11:36.580 --> 00:11:38.910
W not and applying that non linear term
so the nonlinear term transfers that

00:11:38.910 --> 00:11:38.920
so the nonlinear term transfers that
 

00:11:38.920 --> 00:11:41.520
so the nonlinear term transfers that
transforms that scalar input to another

00:11:41.520 --> 00:11:41.530
transforms that scalar input to another
 

00:11:41.530 --> 00:11:47.400
transforms that scalar input to another
scalar output Y so you might now be

00:11:47.400 --> 00:11:47.410
scalar output Y so you might now be
 

00:11:47.410 --> 00:11:49.380
scalar output Y so you might now be
wondering what is this thing that I've

00:11:49.380 --> 00:11:49.390
wondering what is this thing that I've
 

00:11:49.390 --> 00:11:50.880
wondering what is this thing that I've
been referring to as an activation

00:11:50.880 --> 00:11:50.890
been referring to as an activation
 

00:11:50.890 --> 00:11:52.110
been referring to as an activation
function I've mentioned it a couple

00:11:52.110 --> 00:11:52.120
function I've mentioned it a couple
 

00:11:52.120 --> 00:11:53.970
function I've mentioned it a couple
times I called it by a couple different

00:11:53.970 --> 00:11:53.980
times I called it by a couple different
 

00:11:53.980 --> 00:11:55.890
times I called it by a couple different
names first was a nonlinear function

00:11:55.890 --> 00:11:55.900
names first was a nonlinear function
 

00:11:55.900 --> 00:11:58.410
names first was a nonlinear function
then was an activation function what is

00:11:58.410 --> 00:11:58.420
then was an activation function what is
 

00:11:58.420 --> 00:11:58.680
then was an activation function what is
it

00:11:58.680 --> 00:11:58.690
it
 

00:11:58.690 --> 00:12:01.170
it
so one common example of a nonlinear

00:12:01.170 --> 00:12:01.180
so one common example of a nonlinear
 

00:12:01.180 --> 00:12:02.850
so one common example of a nonlinear
activation function is called the

00:12:02.850 --> 00:12:02.860
activation function is called the
 

00:12:02.860 --> 00:12:04.950
activation function is called the
sigmoid function and you can see one

00:12:04.950 --> 00:12:04.960
sigmoid function and you can see one
 

00:12:04.960 --> 00:12:08.010
sigmoid function and you can see one
here defined on the bottom right this is

00:12:08.010 --> 00:12:08.020
here defined on the bottom right this is
 

00:12:08.020 --> 00:12:09.990
here defined on the bottom right this is
a function that takes as input any real

00:12:09.990 --> 00:12:10.000
a function that takes as input any real
 

00:12:10.000 --> 00:12:13.680
a function that takes as input any real
number and outputs a new number between

00:12:13.680 --> 00:12:13.690
number and outputs a new number between
 

00:12:13.690 --> 00:12:16.260
number and outputs a new number between
0 and 1 so you can see it's essentially

00:12:16.260 --> 00:12:16.270
0 and 1 so you can see it's essentially
 

00:12:16.270 --> 00:12:18.300
0 and 1 so you can see it's essentially
collapsing your input between this range

00:12:18.300 --> 00:12:18.310
collapsing your input between this range
 

00:12:18.310 --> 00:12:21.240
collapsing your input between this range
of 0 and 1 this is just one example of

00:12:21.240 --> 00:12:21.250
of 0 and 1 this is just one example of
 

00:12:21.250 --> 00:12:22.500
of 0 and 1 this is just one example of
an activation function but there are

00:12:22.500 --> 00:12:22.510
an activation function but there are
 

00:12:22.510 --> 00:12:24.510
an activation function but there are
many many many activation functions used

00:12:24.510 --> 00:12:24.520
many many many activation functions used
 

00:12:24.520 --> 00:12:25.350
many many many activation functions used
in neural networks

00:12:25.350 --> 00:12:25.360
in neural networks
 

00:12:25.360 --> 00:12:28.020
in neural networks
here are some common ones and throughout

00:12:28.020 --> 00:12:28.030
here are some common ones and throughout
 

00:12:28.030 --> 00:12:29.370
here are some common ones and throughout
this presentation you'll see these

00:12:29.370 --> 00:12:29.380
this presentation you'll see these
 

00:12:29.380 --> 00:12:31.970
this presentation you'll see these
tensorflow code blocks on the bottom

00:12:31.970 --> 00:12:31.980
tensorflow code blocks on the bottom
 

00:12:31.980 --> 00:12:35.040
tensorflow code blocks on the bottom
like like this for example and I'll just

00:12:35.040 --> 00:12:35.050
like like this for example and I'll just
 

00:12:35.050 --> 00:12:37.380
like like this for example and I'll just
be using these as a as a way to kind of

00:12:37.380 --> 00:12:37.390
be using these as a as a way to kind of
 

00:12:37.390 --> 00:12:39.810
be using these as a as a way to kind of
bridge the gap between the theories that

00:12:39.810 --> 00:12:39.820
bridge the gap between the theories that
 

00:12:39.820 --> 00:12:41.430
bridge the gap between the theories that
you'll learn in this class with some of

00:12:41.430 --> 00:12:41.440
you'll learn in this class with some of
 

00:12:41.440 --> 00:12:42.870
you'll learn in this class with some of
the tensor flow that you'll be

00:12:42.870 --> 00:12:42.880
the tensor flow that you'll be
 

00:12:42.880 --> 00:12:45.330
the tensor flow that you'll be
practicing in the labs later today and

00:12:45.330 --> 00:12:45.340
practicing in the labs later today and
 

00:12:45.340 --> 00:12:49.170
practicing in the labs later today and
through the week so the sigmoid function

00:12:49.170 --> 00:12:49.180
through the week so the sigmoid function
 

00:12:49.180 --> 00:12:50.280
through the week so the sigmoid function
like I mentioned before which you can

00:12:50.280 --> 00:12:50.290
like I mentioned before which you can
 

00:12:50.290 --> 00:12:52.470
like I mentioned before which you can
see on the left-hand side is useful for

00:12:52.470 --> 00:12:52.480
see on the left-hand side is useful for
 

00:12:52.480 --> 00:12:54.390
see on the left-hand side is useful for
modeling probabilities because like I

00:12:54.390 --> 00:12:54.400
modeling probabilities because like I
 

00:12:54.400 --> 00:12:58.470
modeling probabilities because like I
said it collapses your your input to

00:12:58.470 --> 00:12:58.480
said it collapses your your input to
 

00:12:58.480 --> 00:12:59.520
said it collapses your your input to
between 0 &amp; 1

00:12:59.520 --> 00:12:59.530
between 0 &amp; 1
 

00:12:59.530 --> 00:13:01.350
between 0 &amp; 1
since probabilities are modeled between

00:13:01.350 --> 00:13:01.360
since probabilities are modeled between
 

00:13:01.360 --> 00:13:03.990
since probabilities are modeled between
0 &amp; 1 this is actually the perfect

00:13:03.990 --> 00:13:04.000
0 &amp; 1 this is actually the perfect
 

00:13:04.000 --> 00:13:05.760
0 &amp; 1 this is actually the perfect
activation function for the end of your

00:13:05.760 --> 00:13:05.770
activation function for the end of your
 

00:13:05.770 --> 00:13:07.230
activation function for the end of your
neural network if you want to predict

00:13:07.230 --> 00:13:07.240
neural network if you want to predict
 

00:13:07.240 --> 00:13:08.940
neural network if you want to predict
probability distributions at the end

00:13:08.940 --> 00:13:08.950
probability distributions at the end
 

00:13:08.950 --> 00:13:11.640
probability distributions at the end
another popular option is the r lu

00:13:11.640 --> 00:13:11.650
another popular option is the r lu
 

00:13:11.650 --> 00:13:13.079
another popular option is the r lu
function which you can see on the far

00:13:13.079 --> 00:13:13.089
function which you can see on the far
 

00:13:13.089 --> 00:13:15.870
function which you can see on the far
right-hand side this function is an

00:13:15.870 --> 00:13:15.880
right-hand side this function is an
 

00:13:15.880 --> 00:13:17.579
right-hand side this function is an
extremely simple one to compute it's

00:13:17.579 --> 00:13:17.589
extremely simple one to compute it's
 

00:13:17.589 --> 00:13:20.640
extremely simple one to compute it's
piecewise linear and it's very popular

00:13:20.640 --> 00:13:20.650
piecewise linear and it's very popular
 

00:13:20.650 --> 00:13:22.889
piecewise linear and it's very popular
because it's so easy to compute but

00:13:22.889 --> 00:13:22.899
because it's so easy to compute but
 

00:13:22.899 --> 00:13:25.799
because it's so easy to compute but
has this non-linearity at Z equals zero

00:13:25.799 --> 00:13:25.809
has this non-linearity at Z equals zero
 

00:13:25.809 --> 00:13:29.609
has this non-linearity at Z equals zero
so at Z less than 0 this function equals

00:13:29.609 --> 00:13:29.619
so at Z less than 0 this function equals
 

00:13:29.619 --> 00:13:31.799
so at Z less than 0 this function equals
0 and at Z greater than 0 it just equals

00:13:31.799 --> 00:13:31.809
0 and at Z greater than 0 it just equals
 

00:13:31.809 --> 00:13:33.540
0 and at Z greater than 0 it just equals
the input and because of this

00:13:33.540 --> 00:13:33.550
the input and because of this
 

00:13:33.550 --> 00:13:35.369
the input and because of this
non-linearity it's still able to capture

00:13:35.369 --> 00:13:35.379
non-linearity it's still able to capture
 

00:13:35.379 --> 00:13:36.809
non-linearity it's still able to capture
all of the great properties of

00:13:36.809 --> 00:13:36.819
all of the great properties of
 

00:13:36.819 --> 00:13:38.730
all of the great properties of
activation functions while still being

00:13:38.730 --> 00:13:38.740
activation functions while still being
 

00:13:38.740 --> 00:13:42.869
activation functions while still being
extremely simple to compute and now I

00:13:42.869 --> 00:13:42.879
extremely simple to compute and now I
 

00:13:42.879 --> 00:13:45.179
extremely simple to compute and now I
want to talk a little bit about why do

00:13:45.179 --> 00:13:45.189
want to talk a little bit about why do
 

00:13:45.189 --> 00:13:47.669
want to talk a little bit about why do
we use activation functions at all I

00:13:47.669 --> 00:13:47.679
we use activation functions at all I
 

00:13:47.679 --> 00:13:49.230
we use activation functions at all I
think a great part of this class is to

00:13:49.230 --> 00:13:49.240
think a great part of this class is to
 

00:13:49.240 --> 00:13:51.299
think a great part of this class is to
actually ask questions and not take

00:13:51.299 --> 00:13:51.309
actually ask questions and not take
 

00:13:51.309 --> 00:13:53.249
actually ask questions and not take
anything for granted so if I tell you we

00:13:53.249 --> 00:13:53.259
anything for granted so if I tell you we
 

00:13:53.259 --> 00:13:55.109
anything for granted so if I tell you we
need an activation function the first

00:13:55.109 --> 00:13:55.119
need an activation function the first
 

00:13:55.119 --> 00:13:56.429
need an activation function the first
thing that should come to your mind is

00:13:56.429 --> 00:13:56.439
thing that should come to your mind is
 

00:13:56.439 --> 00:13:57.960
thing that should come to your mind is
well why do we need that activation

00:13:57.960 --> 00:13:57.970
well why do we need that activation
 

00:13:57.970 --> 00:14:01.439
well why do we need that activation
function so activation functions the

00:14:01.439 --> 00:14:01.449
function so activation functions the
 

00:14:01.449 --> 00:14:03.480
function so activation functions the
purpose of activation functions is to

00:14:03.480 --> 00:14:03.490
purpose of activation functions is to
 

00:14:03.490 --> 00:14:05.249
purpose of activation functions is to
introduce nonlinearities into the

00:14:05.249 --> 00:14:05.259
introduce nonlinearities into the
 

00:14:05.259 --> 00:14:08.040
introduce nonlinearities into the
network this is extremely important in

00:14:08.040 --> 00:14:08.050
network this is extremely important in
 

00:14:08.050 --> 00:14:09.809
network this is extremely important in
deep learning or in machine learning in

00:14:09.809 --> 00:14:09.819
deep learning or in machine learning in
 

00:14:09.819 --> 00:14:12.389
deep learning or in machine learning in
general because in real life data is

00:14:12.389 --> 00:14:12.399
general because in real life data is
 

00:14:12.399 --> 00:14:15.869
general because in real life data is
almost always very nonlinear imagine I

00:14:15.869 --> 00:14:15.879
almost always very nonlinear imagine I
 

00:14:15.879 --> 00:14:17.999
almost always very nonlinear imagine I
told you to separate here the green from

00:14:17.999 --> 00:14:18.009
told you to separate here the green from
 

00:14:18.009 --> 00:14:20.009
told you to separate here the green from
the red points you might think that's

00:14:20.009 --> 00:14:20.019
the red points you might think that's
 

00:14:20.019 --> 00:14:21.540
the red points you might think that's
easy but then what if I told you you had

00:14:21.540 --> 00:14:21.550
easy but then what if I told you you had
 

00:14:21.550 --> 00:14:24.389
easy but then what if I told you you had
to only use a single line to do it

00:14:24.389 --> 00:14:24.399
to only use a single line to do it
 

00:14:24.399 --> 00:14:27.299
to only use a single line to do it
well now it's impossible that actually

00:14:27.299 --> 00:14:27.309
well now it's impossible that actually
 

00:14:27.309 --> 00:14:28.919
well now it's impossible that actually
makes the problem not only really hard

00:14:28.919 --> 00:14:28.929
makes the problem not only really hard
 

00:14:28.929 --> 00:14:30.480
makes the problem not only really hard
like I said it makes it impossible in

00:14:30.480 --> 00:14:30.490
like I said it makes it impossible in
 

00:14:30.490 --> 00:14:31.949
like I said it makes it impossible in
fact if you use linear activation

00:14:31.949 --> 00:14:31.959
fact if you use linear activation
 

00:14:31.959 --> 00:14:34.169
fact if you use linear activation
functions in a neural network no matter

00:14:34.169 --> 00:14:34.179
functions in a neural network no matter
 

00:14:34.179 --> 00:14:37.009
functions in a neural network no matter
how deep or wide your neural network is

00:14:37.009 --> 00:14:37.019
how deep or wide your neural network is
 

00:14:37.019 --> 00:14:39.900
how deep or wide your neural network is
no matter how many neurons it has this

00:14:39.900 --> 00:14:39.910
no matter how many neurons it has this
 

00:14:39.910 --> 00:14:41.460
no matter how many neurons it has this
is the best that I will be able to do

00:14:41.460 --> 00:14:41.470
is the best that I will be able to do
 

00:14:41.470 --> 00:14:43.230
is the best that I will be able to do
produce a linear decision boundary

00:14:43.230 --> 00:14:43.240
produce a linear decision boundary
 

00:14:43.240 --> 00:14:45.150
produce a linear decision boundary
between the red and the green points and

00:14:45.150 --> 00:14:45.160
between the red and the green points and
 

00:14:45.160 --> 00:14:46.590
between the red and the green points and
that's because it's using linear

00:14:46.590 --> 00:14:46.600
that's because it's using linear
 

00:14:46.600 --> 00:14:49.079
that's because it's using linear
activation functions when we introduce a

00:14:49.079 --> 00:14:49.089
activation functions when we introduce a
 

00:14:49.089 --> 00:14:51.329
activation functions when we introduce a
nonlinear activation function that

00:14:51.329 --> 00:14:51.339
nonlinear activation function that
 

00:14:51.339 --> 00:14:53.549
nonlinear activation function that
allows us to approximate arbitrarily

00:14:53.549 --> 00:14:53.559
allows us to approximate arbitrarily
 

00:14:53.559 --> 00:14:56.100
allows us to approximate arbitrarily
complex functions and draw arbitrarily

00:14:56.100 --> 00:14:56.110
complex functions and draw arbitrarily
 

00:14:56.110 --> 00:14:58.139
complex functions and draw arbitrarily
complex decision boundaries in this

00:14:58.139 --> 00:14:58.149
complex decision boundaries in this
 

00:14:58.149 --> 00:15:00.869
complex decision boundaries in this
feature space and that's exactly what

00:15:00.869 --> 00:15:00.879
feature space and that's exactly what
 

00:15:00.879 --> 00:15:02.429
feature space and that's exactly what
makes neural networks so powerful in

00:15:02.429 --> 00:15:02.439
makes neural networks so powerful in
 

00:15:02.439 --> 00:15:05.699
makes neural networks so powerful in
practice so let's understand this with a

00:15:05.699 --> 00:15:05.709
practice so let's understand this with a
 

00:15:05.709 --> 00:15:08.189
practice so let's understand this with a
simple example imagine I give you a

00:15:08.189 --> 00:15:08.199
simple example imagine I give you a
 

00:15:08.199 --> 00:15:11.489
simple example imagine I give you a
trains Network with weights W on the top

00:15:11.489 --> 00:15:11.499
trains Network with weights W on the top
 

00:15:11.499 --> 00:15:17.100
trains Network with weights W on the top
here so W 0 is 1 and let's say W 0 is 1

00:15:17.100 --> 00:15:17.110
here so W 0 is 1 and let's say W 0 is 1
 

00:15:17.110 --> 00:15:20.789
here so W 0 is 1 and let's say W 0 is 1
the W vector is 3 negative 2 so this is

00:15:20.789 --> 00:15:20.799
the W vector is 3 negative 2 so this is
 

00:15:20.799 --> 00:15:23.910
the W vector is 3 negative 2 so this is
a trained neural network and I want to

00:15:23.910 --> 00:15:23.920
a trained neural network and I want to
 

00:15:23.920 --> 00:15:26.910
a trained neural network and I want to
feed in a new input to this network well

00:15:26.910 --> 00:15:26.920
feed in a new input to this network well
 

00:15:26.920 --> 00:15:28.919
feed in a new input to this network well
how do we compute the output remember

00:15:28.919 --> 00:15:28.929
how do we compute the output remember
 

00:15:28.929 --> 00:15:31.499
how do we compute the output remember
from before it's the dot product we add

00:15:31.499 --> 00:15:31.509
from before it's the dot product we add
 

00:15:31.509 --> 00:15:33.660
from before it's the dot product we add
our bias and we compute a non-linearity

00:15:33.660 --> 00:15:33.670
our bias and we compute a non-linearity
 

00:15:33.670 --> 00:15:35.670
our bias and we compute a non-linearity
there's three steps

00:15:35.670 --> 00:15:35.680
there's three steps
 

00:15:35.680 --> 00:15:38.220
there's three steps
so let's take a look at what's going on

00:15:38.220 --> 00:15:38.230
so let's take a look at what's going on
 

00:15:38.230 --> 00:15:40.710
so let's take a look at what's going on
here what's inside of this nonlinear

00:15:40.710 --> 00:15:40.720
here what's inside of this nonlinear
 

00:15:40.720 --> 00:15:42.840
here what's inside of this nonlinear
function the input to the nonlinear

00:15:42.840 --> 00:15:42.850
function the input to the nonlinear
 

00:15:42.850 --> 00:15:45.799
function the input to the nonlinear
function well this is just a 2d line in

00:15:45.799 --> 00:15:45.809
function well this is just a 2d line in
 

00:15:45.809 --> 00:15:48.689
function well this is just a 2d line in
fact we can actually plot this 2d line

00:15:48.689 --> 00:15:48.699
fact we can actually plot this 2d line
 

00:15:48.699 --> 00:15:51.480
fact we can actually plot this 2d line
in what we call the feature space so on

00:15:51.480 --> 00:15:51.490
in what we call the feature space so on
 

00:15:51.490 --> 00:15:54.449
in what we call the feature space so on
the x axis you can see X 1 which is the

00:15:54.449 --> 00:15:54.459
the x axis you can see X 1 which is the
 

00:15:54.459 --> 00:15:57.600
the x axis you can see X 1 which is the
first input and on the y axis you can

00:15:57.600 --> 00:15:57.610
first input and on the y axis you can
 

00:15:57.610 --> 00:15:59.910
first input and on the y axis you can
see X 2 which is the second input this

00:15:59.910 --> 00:15:59.920
see X 2 which is the second input this
 

00:15:59.920 --> 00:16:02.910
see X 2 which is the second input this
neural network has two inputs we can

00:16:02.910 --> 00:16:02.920
neural network has two inputs we can
 

00:16:02.920 --> 00:16:04.650
neural network has two inputs we can
plot the line when it is equal to zero

00:16:04.650 --> 00:16:04.660
plot the line when it is equal to zero
 

00:16:04.660 --> 00:16:06.359
plot the line when it is equal to zero
and you can actually see it in the

00:16:06.359 --> 00:16:06.369
and you can actually see it in the
 

00:16:06.369 --> 00:16:09.329
and you can actually see it in the
feature space here if I give you a new

00:16:09.329 --> 00:16:09.339
feature space here if I give you a new
 

00:16:09.339 --> 00:16:11.400
feature space here if I give you a new
point a new input to this neural network

00:16:11.400 --> 00:16:11.410
point a new input to this neural network
 

00:16:11.410 --> 00:16:13.889
point a new input to this neural network
you can also plot this new point in the

00:16:13.889 --> 00:16:13.899
you can also plot this new point in the
 

00:16:13.899 --> 00:16:16.230
you can also plot this new point in the
same feature space so here's the point

00:16:16.230 --> 00:16:16.240
same feature space so here's the point
 

00:16:16.240 --> 00:16:19.439
same feature space so here's the point
negative 1 2 you can plot it like this

00:16:19.439 --> 00:16:19.449
negative 1 2 you can plot it like this
 

00:16:19.449 --> 00:16:21.660
negative 1 2 you can plot it like this
and actually you can compute the output

00:16:21.660 --> 00:16:21.670
and actually you can compute the output
 

00:16:21.670 --> 00:16:24.059
and actually you can compute the output
by plugging it into this equation that

00:16:24.059 --> 00:16:24.069
by plugging it into this equation that
 

00:16:24.069 --> 00:16:26.699
by plugging it into this equation that
we created before this line if we plug

00:16:26.699 --> 00:16:26.709
we created before this line if we plug
 

00:16:26.709 --> 00:16:30.329
we created before this line if we plug
it in we get 1 minus 3 minus 4 right

00:16:30.329 --> 00:16:30.339
it in we get 1 minus 3 minus 4 right
 

00:16:30.339 --> 00:16:32.369
it in we get 1 minus 3 minus 4 right
which equals minus 6 that's the input to

00:16:32.369 --> 00:16:32.379
which equals minus 6 that's the input to
 

00:16:32.379 --> 00:16:34.859
which equals minus 6 that's the input to
our activation function and then when we

00:16:34.859 --> 00:16:34.869
our activation function and then when we
 

00:16:34.869 --> 00:16:36.509
our activation function and then when we
feed it through our activation function

00:16:36.509 --> 00:16:36.519
feed it through our activation function
 

00:16:36.519 --> 00:16:38.660
feed it through our activation function
here I'm using sigmoid again for example

00:16:38.660 --> 00:16:38.670
here I'm using sigmoid again for example
 

00:16:38.670 --> 00:16:41.910
here I'm using sigmoid again for example
our final output is zero point zero zero

00:16:41.910 --> 00:16:41.920
our final output is zero point zero zero
 

00:16:41.920 --> 00:16:45.030
our final output is zero point zero zero
two ok what does that number mean let's

00:16:45.030 --> 00:16:45.040
two ok what does that number mean let's
 

00:16:45.040 --> 00:16:46.590
two ok what does that number mean let's
go back to this illustration of the

00:16:46.590 --> 00:16:46.600
go back to this illustration of the
 

00:16:46.600 --> 00:16:49.079
go back to this illustration of the
feature space again what this feature

00:16:49.079 --> 00:16:49.089
feature space again what this feature
 

00:16:49.089 --> 00:16:50.970
feature space again what this feature
space is doing is essentially dividing

00:16:50.970 --> 00:16:50.980
space is doing is essentially dividing
 

00:16:50.980 --> 00:16:54.269
space is doing is essentially dividing
the space into two hyperplanes remember

00:16:54.269 --> 00:16:54.279
the space into two hyperplanes remember
 

00:16:54.279 --> 00:16:57.059
the space into two hyperplanes remember
that the sigmoid function outputs values

00:16:57.059 --> 00:16:57.069
that the sigmoid function outputs values
 

00:16:57.069 --> 00:17:01.889
that the sigmoid function outputs values
between 0 and 1 and at z equals 0 when

00:17:01.889 --> 00:17:01.899
between 0 and 1 and at z equals 0 when
 

00:17:01.899 --> 00:17:04.380
between 0 and 1 and at z equals 0 when
the input to the sigmoid is 0 the output

00:17:04.380 --> 00:17:04.390
the input to the sigmoid is 0 the output
 

00:17:04.390 --> 00:17:06.809
the input to the sigmoid is 0 the output
of the sigmoid is 0.5 so essentially

00:17:06.809 --> 00:17:06.819
of the sigmoid is 0.5 so essentially
 

00:17:06.819 --> 00:17:08.819
of the sigmoid is 0.5 so essentially
you're splitting your space into two

00:17:08.819 --> 00:17:08.829
you're splitting your space into two
 

00:17:08.829 --> 00:17:10.649
you're splitting your space into two
planes one where Z is greater than zero

00:17:10.649 --> 00:17:10.659
planes one where Z is greater than zero
 

00:17:10.659 --> 00:17:13.289
planes one where Z is greater than zero
and one more Z is less than zero and one

00:17:13.289 --> 00:17:13.299
and one more Z is less than zero and one
 

00:17:13.299 --> 00:17:15.449
and one more Z is less than zero and one
where Y is greater than 0.5 and one

00:17:15.449 --> 00:17:15.459
where Y is greater than 0.5 and one
 

00:17:15.459 --> 00:17:17.340
where Y is greater than 0.5 and one
where Y is less than 0.5 the two are

00:17:17.340 --> 00:17:17.350
where Y is less than 0.5 the two are
 

00:17:17.350 --> 00:17:21.090
where Y is less than 0.5 the two are
synonymous but when we're dealing with

00:17:21.090 --> 00:17:21.100
synonymous but when we're dealing with
 

00:17:21.100 --> 00:17:23.429
synonymous but when we're dealing with
small dimensional input data like here

00:17:23.429 --> 00:17:23.439
small dimensional input data like here
 

00:17:23.439 --> 00:17:24.899
small dimensional input data like here
we're dealing with only two dimensions

00:17:24.899 --> 00:17:24.909
we're dealing with only two dimensions
 

00:17:24.909 --> 00:17:27.529
we're dealing with only two dimensions
we can make these beautiful plots and

00:17:27.529 --> 00:17:27.539
we can make these beautiful plots and
 

00:17:27.539 --> 00:17:29.760
we can make these beautiful plots and
these are very valuable and actually

00:17:29.760 --> 00:17:29.770
these are very valuable and actually
 

00:17:29.770 --> 00:17:31.200
these are very valuable and actually
visualizing the learning algorithm

00:17:31.200 --> 00:17:31.210
visualizing the learning algorithm
 

00:17:31.210 --> 00:17:33.419
visualizing the learning algorithm
visualizing how our output is relating

00:17:33.419 --> 00:17:33.429
visualizing how our output is relating
 

00:17:33.429 --> 00:17:35.730
visualizing how our output is relating
to our input we're gonna find very soon

00:17:35.730 --> 00:17:35.740
to our input we're gonna find very soon
 

00:17:35.740 --> 00:17:38.340
to our input we're gonna find very soon
that we can't really do this for all

00:17:38.340 --> 00:17:38.350
that we can't really do this for all
 

00:17:38.350 --> 00:17:41.310
that we can't really do this for all
problems because while here we're

00:17:41.310 --> 00:17:41.320
problems because while here we're
 

00:17:41.320 --> 00:17:42.720
problems because while here we're
dealing with only two inputs in

00:17:42.720 --> 00:17:42.730
dealing with only two inputs in
 

00:17:42.730 --> 00:17:44.549
dealing with only two inputs in
practical applications and deep neural

00:17:44.549 --> 00:17:44.559
practical applications and deep neural
 

00:17:44.559 --> 00:17:46.080
practical applications and deep neural
networks we're gonna be dealing with

00:17:46.080 --> 00:17:46.090
networks we're gonna be dealing with
 

00:17:46.090 --> 00:17:47.879
networks we're gonna be dealing with
hundreds thousands or even millions of

00:17:47.879 --> 00:17:47.889
hundreds thousands or even millions of
 

00:17:47.889 --> 00:17:48.870
hundreds thousands or even millions of
inputs to the network

00:17:48.870 --> 00:17:48.880
inputs to the network
 

00:17:48.880 --> 00:17:51.960
inputs to the network
at any given time and then drawing one

00:17:51.960 --> 00:17:51.970
at any given time and then drawing one
 

00:17:51.970 --> 00:17:53.730
at any given time and then drawing one
of these plots in thousand dimensional

00:17:53.730 --> 00:17:53.740
of these plots in thousand dimensional
 

00:17:53.740 --> 00:17:58.170
of these plots in thousand dimensional
space is going to become pretty tough so

00:17:58.170 --> 00:17:58.180
space is going to become pretty tough so
 

00:17:58.180 --> 00:17:59.940
space is going to become pretty tough so
now that we have an idea of the

00:17:59.940 --> 00:17:59.950
now that we have an idea of the
 

00:17:59.950 --> 00:18:02.820
now that we have an idea of the
perceptron a single neuron let's start

00:18:02.820 --> 00:18:02.830
perceptron a single neuron let's start
 

00:18:02.830 --> 00:18:04.560
perceptron a single neuron let's start
by building neural networks from the

00:18:04.560 --> 00:18:04.570
by building neural networks from the
 

00:18:04.570 --> 00:18:06.990
by building neural networks from the
ground up using one neuron and seeing

00:18:06.990 --> 00:18:07.000
ground up using one neuron and seeing
 

00:18:07.000 --> 00:18:10.080
ground up using one neuron and seeing
how this all comes together let's

00:18:10.080 --> 00:18:10.090
how this all comes together let's
 

00:18:10.090 --> 00:18:12.690
how this all comes together let's
revisit our diagram of the perceptron if

00:18:12.690 --> 00:18:12.700
revisit our diagram of the perceptron if
 

00:18:12.700 --> 00:18:14.640
revisit our diagram of the perceptron if
there's a few things that you remember

00:18:14.640 --> 00:18:14.650
there's a few things that you remember
 

00:18:14.650 --> 00:18:16.050
there's a few things that you remember
from this class I want to remember this

00:18:16.050 --> 00:18:16.060
from this class I want to remember this
 

00:18:16.060 --> 00:18:17.790
from this class I want to remember this
so there's three steps to computing the

00:18:17.790 --> 00:18:17.800
so there's three steps to computing the
 

00:18:17.800 --> 00:18:20.790
so there's three steps to computing the
output of a perceptron dot product add a

00:18:20.790 --> 00:18:20.800
output of a perceptron dot product add a
 

00:18:20.800 --> 00:18:24.140
output of a perceptron dot product add a
bias taking non-linearity three steps

00:18:24.140 --> 00:18:24.150
bias taking non-linearity three steps
 

00:18:24.150 --> 00:18:26.430
bias taking non-linearity three steps
let's simplify the diagram a little bit

00:18:26.430 --> 00:18:26.440
let's simplify the diagram a little bit
 

00:18:26.440 --> 00:18:27.750
let's simplify the diagram a little bit
I just got rid of the bias

00:18:27.750 --> 00:18:27.760
I just got rid of the bias
 

00:18:27.760 --> 00:18:29.310
I just got rid of the bias
I removed the weights just for

00:18:29.310 --> 00:18:29.320
I removed the weights just for
 

00:18:29.320 --> 00:18:31.700
I removed the weights just for
simplicity to keep things simple and

00:18:31.700 --> 00:18:31.710
simplicity to keep things simple and
 

00:18:31.710 --> 00:18:35.220
simplicity to keep things simple and
just note here that I'm writing Z as the

00:18:35.220 --> 00:18:35.230
just note here that I'm writing Z as the
 

00:18:35.230 --> 00:18:38.520
just note here that I'm writing Z as the
input to the to the activation function

00:18:38.520 --> 00:18:38.530
input to the to the activation function
 

00:18:38.530 --> 00:18:41.040
input to the to the activation function
so this is the weighted combination

00:18:41.040 --> 00:18:41.050
so this is the weighted combination
 

00:18:41.050 --> 00:18:45.210
so this is the weighted combination
essentially of your inputs Y is then

00:18:45.210 --> 00:18:45.220
essentially of your inputs Y is then
 

00:18:45.220 --> 00:18:47.880
essentially of your inputs Y is then
taking the activation function with

00:18:47.880 --> 00:18:47.890
taking the activation function with
 

00:18:47.890 --> 00:18:52.290
taking the activation function with
input Z so the final output like I said

00:18:52.290 --> 00:18:52.300
input Z so the final output like I said
 

00:18:52.300 --> 00:18:55.530
input Z so the final output like I said
Y is is on the right-hand side here and

00:18:55.530 --> 00:18:55.540
Y is is on the right-hand side here and
 

00:18:55.540 --> 00:18:58.590
Y is is on the right-hand side here and
it's the activation function applied to

00:18:58.590 --> 00:18:58.600
it's the activation function applied to
 

00:18:58.600 --> 00:19:01.440
it's the activation function applied to
this weighted sum if we want to define a

00:19:01.440 --> 00:19:01.450
this weighted sum if we want to define a
 

00:19:01.450 --> 00:19:03.630
this weighted sum if we want to define a
multi output neural network now all we

00:19:03.630 --> 00:19:03.640
multi output neural network now all we
 

00:19:03.640 --> 00:19:05.790
multi output neural network now all we
have to do is add another perceptron to

00:19:05.790 --> 00:19:05.800
have to do is add another perceptron to
 

00:19:05.800 --> 00:19:08.820
have to do is add another perceptron to
this picture now we have two outputs

00:19:08.820 --> 00:19:08.830
this picture now we have two outputs
 

00:19:08.830 --> 00:19:10.800
this picture now we have two outputs
each one is a normal perceptron like we

00:19:10.800 --> 00:19:10.810
each one is a normal perceptron like we
 

00:19:10.810 --> 00:19:13.710
each one is a normal perceptron like we
defined before no nothing extra and each

00:19:13.710 --> 00:19:13.720
defined before no nothing extra and each
 

00:19:13.720 --> 00:19:15.420
defined before no nothing extra and each
one is taking all the inputs from the

00:19:15.420 --> 00:19:15.430
one is taking all the inputs from the
 

00:19:15.430 --> 00:19:17.520
one is taking all the inputs from the
left-hand side computing this weighted

00:19:17.520 --> 00:19:17.530
left-hand side computing this weighted
 

00:19:17.530 --> 00:19:21.570
left-hand side computing this weighted
sum adding a bias and passing it through

00:19:21.570 --> 00:19:21.580
sum adding a bias and passing it through
 

00:19:21.580 --> 00:19:24.510
sum adding a bias and passing it through
an activation function let's keep going

00:19:24.510 --> 00:19:24.520
an activation function let's keep going
 

00:19:24.520 --> 00:19:26.340
an activation function let's keep going
now let's take a look at a single

00:19:26.340 --> 00:19:26.350
now let's take a look at a single
 

00:19:26.350 --> 00:19:28.620
now let's take a look at a single
layered neural network this is one where

00:19:28.620 --> 00:19:28.630
layered neural network this is one where
 

00:19:28.630 --> 00:19:30.300
layered neural network this is one where
we have a single hidden layer between

00:19:30.300 --> 00:19:30.310
we have a single hidden layer between
 

00:19:30.310 --> 00:19:32.370
we have a single hidden layer between
our inputs and our outputs we call it a

00:19:32.370 --> 00:19:32.380
our inputs and our outputs we call it a
 

00:19:32.380 --> 00:19:34.740
our inputs and our outputs we call it a
hidden layer because unlike the input

00:19:34.740 --> 00:19:34.750
hidden layer because unlike the input
 

00:19:34.750 --> 00:19:36.540
hidden layer because unlike the input
and the output which are strictly

00:19:36.540 --> 00:19:36.550
and the output which are strictly
 

00:19:36.550 --> 00:19:39.450
and the output which are strictly
observable or hidden layers learned so

00:19:39.450 --> 00:19:39.460
observable or hidden layers learned so
 

00:19:39.460 --> 00:19:42.240
observable or hidden layers learned so
we don't explicitly enforce any behavior

00:19:42.240 --> 00:19:42.250
we don't explicitly enforce any behavior
 

00:19:42.250 --> 00:19:43.770
we don't explicitly enforce any behavior
on the hidden layer and that's why we

00:19:43.770 --> 00:19:43.780
on the hidden layer and that's why we
 

00:19:43.780 --> 00:19:46.500
on the hidden layer and that's why we
call it hidden in that sense since we

00:19:46.500 --> 00:19:46.510
call it hidden in that sense since we
 

00:19:46.510 --> 00:19:48.000
call it hidden in that sense since we
now have a transformation from the

00:19:48.000 --> 00:19:48.010
now have a transformation from the
 

00:19:48.010 --> 00:19:50.190
now have a transformation from the
inputs to the hidden layer and hidden

00:19:50.190 --> 00:19:50.200
inputs to the hidden layer and hidden
 

00:19:50.200 --> 00:19:53.040
inputs to the hidden layer and hidden
layer to the outputs we're going to need

00:19:53.040 --> 00:19:53.050
layer to the outputs we're going to need
 

00:19:53.050 --> 00:19:55.950
layer to the outputs we're going to need
two weight matrices so we're going to

00:19:55.950 --> 00:19:55.960
two weight matrices so we're going to
 

00:19:55.960 --> 00:19:58.650
two weight matrices so we're going to
call it W one to go from input to hidden

00:19:58.650 --> 00:19:58.660
call it W one to go from input to hidden
 

00:19:58.660 --> 00:20:02.280
call it W one to go from input to hidden
layer and W two to go from hidden layer

00:20:02.280 --> 00:20:02.290
layer and W two to go from hidden layer
 

00:20:02.290 --> 00:20:02.730
layer and W two to go from hidden layer
to

00:20:02.730 --> 00:20:02.740
to
 

00:20:02.740 --> 00:20:05.430
to
output but again the story here's the

00:20:05.430 --> 00:20:05.440
output but again the story here's the
 

00:20:05.440 --> 00:20:08.970
output but again the story here's the
same dot product add a bias for each of

00:20:08.970 --> 00:20:08.980
same dot product add a bias for each of
 

00:20:08.980 --> 00:20:10.200
same dot product add a bias for each of
the neurons and then compute an

00:20:10.200 --> 00:20:10.210
the neurons and then compute an
 

00:20:10.210 --> 00:20:12.990
the neurons and then compute an
activation function let's zoom in now to

00:20:12.990 --> 00:20:13.000
activation function let's zoom in now to
 

00:20:13.000 --> 00:20:15.570
activation function let's zoom in now to
a single hidden hidden unit in this

00:20:15.570 --> 00:20:15.580
a single hidden hidden unit in this
 

00:20:15.580 --> 00:20:17.820
a single hidden hidden unit in this
hidden layer if we look at the single

00:20:17.820 --> 00:20:17.830
hidden layer if we look at the single
 

00:20:17.830 --> 00:20:20.790
hidden layer if we look at the single
unit take z2 for example it is just the

00:20:20.790 --> 00:20:20.800
unit take z2 for example it is just the
 

00:20:20.800 --> 00:20:23.010
unit take z2 for example it is just the
same perceptron that we saw before I'm

00:20:23.010 --> 00:20:23.020
same perceptron that we saw before I'm
 

00:20:23.020 --> 00:20:24.390
same perceptron that we saw before I'm
going to keep repeating myself

00:20:24.390 --> 00:20:24.400
going to keep repeating myself
 

00:20:24.400 --> 00:20:26.669
going to keep repeating myself
we took a dot product with the inputs we

00:20:26.669 --> 00:20:26.679
we took a dot product with the inputs we
 

00:20:26.679 --> 00:20:29.669
we took a dot product with the inputs we
applied a bias and then actually so

00:20:29.669 --> 00:20:29.679
applied a bias and then actually so
 

00:20:29.679 --> 00:20:31.020
applied a bias and then actually so
since it's Z we had not applied our

00:20:31.020 --> 00:20:31.030
since it's Z we had not applied our
 

00:20:31.030 --> 00:20:33.540
since it's Z we had not applied our
activation function yet so it's just a

00:20:33.540 --> 00:20:33.550
activation function yet so it's just a
 

00:20:33.550 --> 00:20:36.690
activation function yet so it's just a
dot product plus a bias so far if we

00:20:36.690 --> 00:20:36.700
dot product plus a bias so far if we
 

00:20:36.700 --> 00:20:38.540
dot product plus a bias so far if we
took it and took a look at a different

00:20:38.540 --> 00:20:38.550
took it and took a look at a different
 

00:20:38.550 --> 00:20:42.750
took it and took a look at a different
neuron let's say z3 or z4 the idea here

00:20:42.750 --> 00:20:42.760
neuron let's say z3 or z4 the idea here
 

00:20:42.760 --> 00:20:43.950
neuron let's say z3 or z4 the idea here
is gonna be the same but we're probably

00:20:43.950 --> 00:20:43.960
is gonna be the same but we're probably
 

00:20:43.960 --> 00:20:46.020
is gonna be the same but we're probably
going to end up with a different value

00:20:46.020 --> 00:20:46.030
going to end up with a different value
 

00:20:46.030 --> 00:20:48.330
going to end up with a different value
for Z 3 and C 4 just because the weights

00:20:48.330 --> 00:20:48.340
for Z 3 and C 4 just because the weights
 

00:20:48.340 --> 00:20:51.210
for Z 3 and C 4 just because the weights
leading from Z 3 to the inputs are going

00:20:51.210 --> 00:20:51.220
leading from Z 3 to the inputs are going
 

00:20:51.220 --> 00:20:52.590
leading from Z 3 to the inputs are going
to be different for each of those

00:20:52.590 --> 00:20:52.600
to be different for each of those
 

00:20:52.600 --> 00:20:55.380
to be different for each of those
neurons so this picture looks a little

00:20:55.380 --> 00:20:55.390
neurons so this picture looks a little
 

00:20:55.390 --> 00:20:56.850
neurons so this picture looks a little
bit messy so let's clean things up a

00:20:56.850 --> 00:20:56.860
bit messy so let's clean things up a
 

00:20:56.860 --> 00:20:58.350
bit messy so let's clean things up a
little bit more and just replace all of

00:20:58.350 --> 00:20:58.360
little bit more and just replace all of
 

00:20:58.360 --> 00:21:00.030
little bit more and just replace all of
these hidden layers all these lines

00:21:00.030 --> 00:21:00.040
these hidden layers all these lines
 

00:21:00.040 --> 00:21:02.640
these hidden layers all these lines
between the hidden layers with these

00:21:02.640 --> 00:21:02.650
between the hidden layers with these
 

00:21:02.650 --> 00:21:04.380
between the hidden layers with these
symbols these symbols denote fully

00:21:04.380 --> 00:21:04.390
symbols these symbols denote fully
 

00:21:04.390 --> 00:21:06.930
symbols these symbols denote fully
connected layers where each input to the

00:21:06.930 --> 00:21:06.940
connected layers where each input to the
 

00:21:06.940 --> 00:21:09.210
connected layers where each input to the
layer is connected to each output of the

00:21:09.210 --> 00:21:09.220
layer is connected to each output of the
 

00:21:09.220 --> 00:21:11.280
layer is connected to each output of the
layer another common name for these is

00:21:11.280 --> 00:21:11.290
layer another common name for these is
 

00:21:11.290 --> 00:21:15.470
layer another common name for these is
called dense layers and you can actually

00:21:15.470 --> 00:21:15.480
called dense layers and you can actually
 

00:21:15.480 --> 00:21:18.840
called dense layers and you can actually
write this in tensor flow using just

00:21:18.840 --> 00:21:18.850
write this in tensor flow using just
 

00:21:18.850 --> 00:21:20.549
write this in tensor flow using just
four lines of code so this neural

00:21:20.549 --> 00:21:20.559
four lines of code so this neural
 

00:21:20.559 --> 00:21:22.260
four lines of code so this neural
network which is a single layered multi

00:21:22.260 --> 00:21:22.270
network which is a single layered multi
 

00:21:22.270 --> 00:21:25.280
network which is a single layered multi
output neural network can be called by

00:21:25.280 --> 00:21:25.290
output neural network can be called by
 

00:21:25.290 --> 00:21:27.990
output neural network can be called by
instantiating your inputs feeding those

00:21:27.990 --> 00:21:28.000
instantiating your inputs feeding those
 

00:21:28.000 --> 00:21:30.120
instantiating your inputs feeding those
inputs into a hidden layer like I'm

00:21:30.120 --> 00:21:30.130
inputs into a hidden layer like I'm
 

00:21:30.130 --> 00:21:31.740
inputs into a hidden layer like I'm
doing here which is just defined as a

00:21:31.740 --> 00:21:31.750
doing here which is just defined as a
 

00:21:31.750 --> 00:21:35.240
doing here which is just defined as a
single dense layer and then taking those

00:21:35.240 --> 00:21:35.250
single dense layer and then taking those
 

00:21:35.250 --> 00:21:38.130
single dense layer and then taking those
hidden outputs feeding that into another

00:21:38.130 --> 00:21:38.140
hidden outputs feeding that into another
 

00:21:38.140 --> 00:21:42.780
hidden outputs feeding that into another
dense layer to produce your outputs the

00:21:42.780 --> 00:21:42.790
dense layer to produce your outputs the
 

00:21:42.790 --> 00:21:44.070
dense layer to produce your outputs the
final model is to find it end to end

00:21:44.070 --> 00:21:44.080
final model is to find it end to end
 

00:21:44.080 --> 00:21:46.140
final model is to find it end to end
with that single line at the end model

00:21:46.140 --> 00:21:46.150
with that single line at the end model
 

00:21:46.150 --> 00:21:47.820
with that single line at the end model
of inputs and outputs and that just

00:21:47.820 --> 00:21:47.830
of inputs and outputs and that just
 

00:21:47.830 --> 00:21:49.799
of inputs and outputs and that just
essentially connects the graph and to

00:21:49.799 --> 00:21:49.809
essentially connects the graph and to
 

00:21:49.809 --> 00:21:53.940
essentially connects the graph and to
end so now let's keep building on this

00:21:53.940 --> 00:21:53.950
end so now let's keep building on this
 

00:21:53.950 --> 00:21:55.710
end so now let's keep building on this
idea now we want to build a deep neural

00:21:55.710 --> 00:21:55.720
idea now we want to build a deep neural
 

00:21:55.720 --> 00:21:57.150
idea now we want to build a deep neural
network what is the deep neural network

00:21:57.150 --> 00:21:57.160
network what is the deep neural network
 

00:21:57.160 --> 00:21:58.410
network what is the deep neural network
well it's just one where we keep

00:21:58.410 --> 00:21:58.420
well it's just one where we keep
 

00:21:58.420 --> 00:22:00.360
well it's just one where we keep
stacking these hidden layers back to

00:22:00.360 --> 00:22:00.370
stacking these hidden layers back to
 

00:22:00.370 --> 00:22:01.680
stacking these hidden layers back to
back to back to back to create

00:22:01.680 --> 00:22:01.690
back to back to back to create
 

00:22:01.690 --> 00:22:04.880
back to back to back to create
increasingly deeper and deeper models

00:22:04.880 --> 00:22:04.890
increasingly deeper and deeper models
 

00:22:04.890 --> 00:22:07.200
increasingly deeper and deeper models
one where the output is computed by

00:22:07.200 --> 00:22:07.210
one where the output is computed by
 

00:22:07.210 --> 00:22:08.970
one where the output is computed by
going deeper into the network and

00:22:08.970 --> 00:22:08.980
going deeper into the network and
 

00:22:08.980 --> 00:22:11.159
going deeper into the network and
computing these weighted sums over and

00:22:11.159 --> 00:22:11.169
computing these weighted sums over and
 

00:22:11.169 --> 00:22:12.360
computing these weighted sums over and
over and over again with these

00:22:12.360 --> 00:22:12.370
over and over again with these
 

00:22:12.370 --> 00:22:16.030
over and over again with these
activation functions repeatedly applied

00:22:16.030 --> 00:22:16.040
activation functions repeatedly applied
 

00:22:16.040 --> 00:22:17.980
activation functions repeatedly applied
so this is awesome now we have an idea

00:22:17.980 --> 00:22:17.990
so this is awesome now we have an idea
 

00:22:17.990 --> 00:22:19.570
so this is awesome now we have an idea
on how to actually build a neural

00:22:19.570 --> 00:22:19.580
on how to actually build a neural
 

00:22:19.580 --> 00:22:21.220
on how to actually build a neural
network from scratch going all the way

00:22:21.220 --> 00:22:21.230
network from scratch going all the way
 

00:22:21.230 --> 00:22:24.460
network from scratch going all the way
from a single perceptron and we know how

00:22:24.460 --> 00:22:24.470
from a single perceptron and we know how
 

00:22:24.470 --> 00:22:25.870
from a single perceptron and we know how
to compose them to create very complex

00:22:25.870 --> 00:22:25.880
to compose them to create very complex
 

00:22:25.880 --> 00:22:28.810
to compose them to create very complex
deep neural networks as well let's take

00:22:28.810 --> 00:22:28.820
deep neural networks as well let's take
 

00:22:28.820 --> 00:22:30.730
deep neural networks as well let's take
a look at how we can apply this to a

00:22:30.730 --> 00:22:30.740
a look at how we can apply this to a
 

00:22:30.740 --> 00:22:33.130
a look at how we can apply this to a
very real problem that I know a lot of

00:22:33.130 --> 00:22:33.140
very real problem that I know a lot of
 

00:22:33.140 --> 00:22:34.510
very real problem that I know a lot of
you probably care about so I was

00:22:34.510 --> 00:22:34.520
you probably care about so I was
 

00:22:34.520 --> 00:22:37.050
you probably care about so I was
thinking of a problem potential that

00:22:37.050 --> 00:22:37.060
thinking of a problem potential that
 

00:22:37.060 --> 00:22:39.640
thinking of a problem potential that
some of you might care about it took me

00:22:39.640 --> 00:22:39.650
some of you might care about it took me
 

00:22:39.650 --> 00:22:41.440
some of you might care about it took me
a while but I think this might be one so

00:22:41.440 --> 00:22:41.450
a while but I think this might be one so
 

00:22:41.450 --> 00:22:44.440
a while but I think this might be one so
at MIT we care a lot about passing our

00:22:44.440 --> 00:22:44.450
at MIT we care a lot about passing our
 

00:22:44.450 --> 00:22:46.450
at MIT we care a lot about passing our
classes so I think a very good example

00:22:46.450 --> 00:22:46.460
classes so I think a very good example
 

00:22:46.460 --> 00:22:48.310
classes so I think a very good example
is let's train a neural network to

00:22:48.310 --> 00:22:48.320
is let's train a neural network to
 

00:22:48.320 --> 00:22:49.750
is let's train a neural network to
determine if you're gonna pass your

00:22:49.750 --> 00:22:49.760
determine if you're gonna pass your
 

00:22:49.760 --> 00:22:53.320
determine if you're gonna pass your
class so to do this let's start with a

00:22:53.320 --> 00:22:53.330
class so to do this let's start with a
 

00:22:53.330 --> 00:22:55.780
class so to do this let's start with a
simple two input feature model one

00:22:55.780 --> 00:22:55.790
simple two input feature model one
 

00:22:55.790 --> 00:22:57.280
simple two input feature model one
feature is the number of lectures that

00:22:57.280 --> 00:22:57.290
feature is the number of lectures that
 

00:22:57.290 --> 00:22:59.500
feature is the number of lectures that
you attend the other feature is the

00:22:59.500 --> 00:22:59.510
you attend the other feature is the
 

00:22:59.510 --> 00:23:01.120
you attend the other feature is the
number of hours that you spend on the

00:23:01.120 --> 00:23:01.130
number of hours that you spend on the
 

00:23:01.130 --> 00:23:05.380
number of hours that you spend on the
final project again since we have two

00:23:05.380 --> 00:23:05.390
final project again since we have two
 

00:23:05.390 --> 00:23:06.790
final project again since we have two
inputs we can plot this data on a

00:23:06.790 --> 00:23:06.800
inputs we can plot this data on a
 

00:23:06.800 --> 00:23:09.340
inputs we can plot this data on a
feature map like we did before green

00:23:09.340 --> 00:23:09.350
feature map like we did before green
 

00:23:09.350 --> 00:23:11.230
feature map like we did before green
points here represent previous students

00:23:11.230 --> 00:23:11.240
points here represent previous students
 

00:23:11.240 --> 00:23:13.690
points here represent previous students
from previous years that pass the class

00:23:13.690 --> 00:23:13.700
from previous years that pass the class
 

00:23:13.700 --> 00:23:15.460
from previous years that pass the class
red points represent students that

00:23:15.460 --> 00:23:15.470
red points represent students that
 

00:23:15.470 --> 00:23:17.920
red points represent students that
failed the class now if you want to find

00:23:17.920 --> 00:23:17.930
failed the class now if you want to find
 

00:23:17.930 --> 00:23:19.090
failed the class now if you want to find
out if you're gonna pass or fail to

00:23:19.090 --> 00:23:19.100
out if you're gonna pass or fail to
 

00:23:19.100 --> 00:23:20.800
out if you're gonna pass or fail to
class you can also apply yourself on

00:23:20.800 --> 00:23:20.810
class you can also apply yourself on
 

00:23:20.810 --> 00:23:23.590
class you can also apply yourself on
this map you spent you came to four

00:23:23.590 --> 00:23:23.600
this map you spent you came to four
 

00:23:23.600 --> 00:23:25.330
this map you spent you came to four
lectures spend five hours on your final

00:23:25.330 --> 00:23:25.340
lectures spend five hours on your final
 

00:23:25.340 --> 00:23:27.850
lectures spend five hours on your final
project and you want to know if you're

00:23:27.850 --> 00:23:27.860
project and you want to know if you're
 

00:23:27.860 --> 00:23:30.700
project and you want to know if you're
going to pass or fail and you want to

00:23:30.700 --> 00:23:30.710
going to pass or fail and you want to
 

00:23:30.710 --> 00:23:31.960
going to pass or fail and you want to
actually build a neural networks that's

00:23:31.960 --> 00:23:31.970
actually build a neural networks that's
 

00:23:31.970 --> 00:23:34.390
actually build a neural networks that's
going to learn this look at the old the

00:23:34.390 --> 00:23:34.400
going to learn this look at the old the
 

00:23:34.400 --> 00:23:36.460
going to learn this look at the old the
the previous people that took the scores

00:23:36.460 --> 00:23:36.470
the previous people that took the scores
 

00:23:36.470 --> 00:23:38.680
the previous people that took the scores
and determine if you all pass or fail as

00:23:38.680 --> 00:23:38.690
and determine if you all pass or fail as
 

00:23:38.690 --> 00:23:41.860
and determine if you all pass or fail as
well so let's do it

00:23:41.860 --> 00:23:41.870
well so let's do it
 

00:23:41.870 --> 00:23:44.290
well so let's do it
we have two inputs one is four one is

00:23:44.290 --> 00:23:44.300
we have two inputs one is four one is
 

00:23:44.300 --> 00:23:46.720
we have two inputs one is four one is
five these are fed into a single layered

00:23:46.720 --> 00:23:46.730
five these are fed into a single layered
 

00:23:46.730 --> 00:23:48.370
five these are fed into a single layered
neural network with three hidden units

00:23:48.370 --> 00:23:48.380
neural network with three hidden units
 

00:23:48.380 --> 00:23:50.620
neural network with three hidden units
and we see that the final output

00:23:50.620 --> 00:23:50.630
and we see that the final output
 

00:23:50.630 --> 00:23:52.870
and we see that the final output
probability that you will pass this

00:23:52.870 --> 00:23:52.880
probability that you will pass this
 

00:23:52.880 --> 00:23:58.960
probability that you will pass this
class is 0.1 or 10% not very good that's

00:23:58.960 --> 00:23:58.970
class is 0.1 or 10% not very good that's
 

00:23:58.970 --> 00:24:00.550
class is 0.1 or 10% not very good that's
actually really bad news can anyone

00:24:00.550 --> 00:24:00.560
actually really bad news can anyone
 

00:24:00.560 --> 00:24:03.760
actually really bad news can anyone
guess why this person who actually was

00:24:03.760 --> 00:24:03.770
guess why this person who actually was
 

00:24:03.770 --> 00:24:05.920
guess why this person who actually was
in the part of the feature space it

00:24:05.920 --> 00:24:05.930
in the part of the feature space it
 

00:24:05.930 --> 00:24:07.120
in the part of the feature space it
looked like they were actually in a good

00:24:07.120 --> 00:24:07.130
looked like they were actually in a good
 

00:24:07.130 --> 00:24:08.380
looked like they were actually in a good
part of this feature space looked like

00:24:08.380 --> 00:24:08.390
part of this feature space looked like
 

00:24:08.390 --> 00:24:10.240
part of this feature space looked like
they were gonna pass the class why did

00:24:10.240 --> 00:24:10.250
they were gonna pass the class why did
 

00:24:10.250 --> 00:24:11.920
they were gonna pass the class why did
this neural network give me such a bad

00:24:11.920 --> 00:24:11.930
this neural network give me such a bad
 

00:24:11.930 --> 00:24:16.780
this neural network give me such a bad
prediction here yeah exactly so the

00:24:16.780 --> 00:24:16.790
prediction here yeah exactly so the
 

00:24:16.790 --> 00:24:19.090
prediction here yeah exactly so the
network was not trained essentially this

00:24:19.090 --> 00:24:19.100
network was not trained essentially this
 

00:24:19.100 --> 00:24:20.350
network was not trained essentially this
network is like a baby that was just

00:24:20.350 --> 00:24:20.360
network is like a baby that was just
 

00:24:20.360 --> 00:24:22.420
network is like a baby that was just
born it has no idea of what lectures are

00:24:22.420 --> 00:24:22.430
born it has no idea of what lectures are
 

00:24:22.430 --> 00:24:24.100
born it has no idea of what lectures are
it doesn't know where final labs are it

00:24:24.100 --> 00:24:24.110
it doesn't know where final labs are it
 

00:24:24.110 --> 00:24:25.720
it doesn't know where final labs are it
doesn't know anything about this world

00:24:25.720 --> 00:24:25.730
doesn't know anything about this world
 

00:24:25.730 --> 00:24:28.180
doesn't know anything about this world
it's these are just numbers to it it's

00:24:28.180 --> 00:24:28.190
it's these are just numbers to it it's
 

00:24:28.190 --> 00:24:29.800
it's these are just numbers to it it's
been randomly initialized it has

00:24:29.800 --> 00:24:29.810
been randomly initialized it has
 

00:24:29.810 --> 00:24:31.570
been randomly initialized it has
no idea about the problem so we have to

00:24:31.570 --> 00:24:31.580
no idea about the problem so we have to
 

00:24:31.580 --> 00:24:32.860
no idea about the problem so we have to
actually train it we have to teach it

00:24:32.860 --> 00:24:32.870
actually train it we have to teach it
 

00:24:32.870 --> 00:24:36.160
actually train it we have to teach it
how to get the right answer so the first

00:24:36.160 --> 00:24:36.170
how to get the right answer so the first
 

00:24:36.170 --> 00:24:37.360
how to get the right answer so the first
thing that we have to do is tell the

00:24:37.360 --> 00:24:37.370
thing that we have to do is tell the
 

00:24:37.370 --> 00:24:39.370
thing that we have to do is tell the
network when it makes a mistake so that

00:24:39.370 --> 00:24:39.380
network when it makes a mistake so that
 

00:24:39.380 --> 00:24:41.380
network when it makes a mistake so that
we can correct it in the future now how

00:24:41.380 --> 00:24:41.390
we can correct it in the future now how
 

00:24:41.390 --> 00:24:43.780
we can correct it in the future now how
do we do this in neural networks the

00:24:43.780 --> 00:24:43.790
do we do this in neural networks the
 

00:24:43.790 --> 00:24:45.430
do we do this in neural networks the
loss of a network is actually what

00:24:45.430 --> 00:24:45.440
loss of a network is actually what
 

00:24:45.440 --> 00:24:47.860
loss of a network is actually what
defines when the network makes the wrong

00:24:47.860 --> 00:24:47.870
defines when the network makes the wrong
 

00:24:47.870 --> 00:24:50.350
defines when the network makes the wrong
prediction it takes the input and the

00:24:50.350 --> 00:24:50.360
prediction it takes the input and the
 

00:24:50.360 --> 00:24:53.200
prediction it takes the input and the
predicted output sorry it takes as input

00:24:53.200 --> 00:24:53.210
predicted output sorry it takes as input
 

00:24:53.210 --> 00:24:55.810
predicted output sorry it takes as input
the predicted output and the ground

00:24:55.810 --> 00:24:55.820
the predicted output and the ground
 

00:24:55.820 --> 00:24:58.060
the predicted output and the ground
truth actual output if your predicted

00:24:58.060 --> 00:24:58.070
truth actual output if your predicted
 

00:24:58.070 --> 00:24:59.590
truth actual output if your predicted
output and your ground truth output are

00:24:59.590 --> 00:24:59.600
output and your ground truth output are
 

00:24:59.600 --> 00:25:01.240
output and your ground truth output are
very close to each other then that

00:25:01.240 --> 00:25:01.250
very close to each other then that
 

00:25:01.250 --> 00:25:03.070
very close to each other then that
essentially means that your loss is

00:25:03.070 --> 00:25:03.080
essentially means that your loss is
 

00:25:03.080 --> 00:25:04.390
essentially means that your loss is
going to be very low you didn't make a

00:25:04.390 --> 00:25:04.400
going to be very low you didn't make a
 

00:25:04.400 --> 00:25:06.940
going to be very low you didn't make a
mistake but if your ground truth output

00:25:06.940 --> 00:25:06.950
mistake but if your ground truth output
 

00:25:06.950 --> 00:25:08.350
mistake but if your ground truth output
is very far away from your predicted

00:25:08.350 --> 00:25:08.360
is very far away from your predicted
 

00:25:08.360 --> 00:25:10.000
is very far away from your predicted
output that means that you should have a

00:25:10.000 --> 00:25:10.010
output that means that you should have a
 

00:25:10.010 --> 00:25:12.010
output that means that you should have a
very high loss you just have a lot of

00:25:12.010 --> 00:25:12.020
very high loss you just have a lot of
 

00:25:12.020 --> 00:25:13.960
very high loss you just have a lot of
error and your network should correct

00:25:13.960 --> 00:25:13.970
error and your network should correct
 

00:25:13.970 --> 00:25:18.520
error and your network should correct
that so let's assume that we have data

00:25:18.520 --> 00:25:18.530
that so let's assume that we have data
 

00:25:18.530 --> 00:25:21.130
that so let's assume that we have data
not just from one student now but we

00:25:21.130 --> 00:25:21.140
not just from one student now but we
 

00:25:21.140 --> 00:25:22.540
not just from one student now but we
have data from many many different

00:25:22.540 --> 00:25:22.550
have data from many many different
 

00:25:22.550 --> 00:25:25.770
have data from many many different
students passing and failing the class

00:25:25.770 --> 00:25:25.780
students passing and failing the class
 

00:25:25.780 --> 00:25:28.360
students passing and failing the class
we now care about how this model does

00:25:28.360 --> 00:25:28.370
we now care about how this model does
 

00:25:28.370 --> 00:25:30.370
we now care about how this model does
not just on that one student but across

00:25:30.370 --> 00:25:30.380
not just on that one student but across
 

00:25:30.380 --> 00:25:33.790
not just on that one student but across
the entire population of students and we

00:25:33.790 --> 00:25:33.800
the entire population of students and we
 

00:25:33.800 --> 00:25:35.800
the entire population of students and we
call this the empirical loss and that's

00:25:35.800 --> 00:25:35.810
call this the empirical loss and that's
 

00:25:35.810 --> 00:25:37.540
call this the empirical loss and that's
just the mean of all of the losses for

00:25:37.540 --> 00:25:37.550
just the mean of all of the losses for
 

00:25:37.550 --> 00:25:39.820
just the mean of all of the losses for
the individual students we can do it by

00:25:39.820 --> 00:25:39.830
the individual students we can do it by
 

00:25:39.830 --> 00:25:42.340
the individual students we can do it by
literally just computing the mean sorry

00:25:42.340 --> 00:25:42.350
literally just computing the mean sorry
 

00:25:42.350 --> 00:25:43.630
literally just computing the mean sorry
just computing the loss for each of

00:25:43.630 --> 00:25:43.640
just computing the loss for each of
 

00:25:43.640 --> 00:25:45.390
just computing the loss for each of
these students and taking their mean

00:25:45.390 --> 00:25:45.400
these students and taking their mean
 

00:25:45.400 --> 00:25:48.010
these students and taking their mean
when training a network what we really

00:25:48.010 --> 00:25:48.020
when training a network what we really
 

00:25:48.020 --> 00:25:50.290
when training a network what we really
want to do is not minimize the loss for

00:25:50.290 --> 00:25:50.300
want to do is not minimize the loss for
 

00:25:50.300 --> 00:25:51.970
want to do is not minimize the loss for
any particular student but we want to

00:25:51.970 --> 00:25:51.980
any particular student but we want to
 

00:25:51.980 --> 00:25:53.950
any particular student but we want to
minimize the loss across the entire

00:25:53.950 --> 00:25:53.960
minimize the loss across the entire
 

00:25:53.960 --> 00:25:58.840
minimize the loss across the entire
training set so if we go back to our

00:25:58.840 --> 00:25:58.850
training set so if we go back to our
 

00:25:58.850 --> 00:26:01.600
training set so if we go back to our
problem on path predicting if you'll

00:26:01.600 --> 00:26:01.610
problem on path predicting if you'll
 

00:26:01.610 --> 00:26:03.610
problem on path predicting if you'll
pass or fail to class this is a problem

00:26:03.610 --> 00:26:03.620
pass or fail to class this is a problem
 

00:26:03.620 --> 00:26:05.620
pass or fail to class this is a problem
of binary classification your output is

00:26:05.620 --> 00:26:05.630
of binary classification your output is
 

00:26:05.630 --> 00:26:08.230
of binary classification your output is
0 or 1 we already learned that when

00:26:08.230 --> 00:26:08.240
0 or 1 we already learned that when
 

00:26:08.240 --> 00:26:09.850
0 or 1 we already learned that when
outputs are 0 or 1 you're probably going

00:26:09.850 --> 00:26:09.860
outputs are 0 or 1 you're probably going
 

00:26:09.860 --> 00:26:13.360
outputs are 0 or 1 you're probably going
to want to use a soft max output for

00:26:13.360 --> 00:26:13.370
to want to use a soft max output for
 

00:26:13.370 --> 00:26:15.190
to want to use a soft max output for
those of you who aren't familiar with

00:26:15.190 --> 00:26:15.200
those of you who aren't familiar with
 

00:26:15.200 --> 00:26:17.050
those of you who aren't familiar with
cross entropy this was an idea

00:26:17.050 --> 00:26:17.060
cross entropy this was an idea
 

00:26:17.060 --> 00:26:19.060
cross entropy this was an idea
introduced actually at MIT and a

00:26:19.060 --> 00:26:19.070
introduced actually at MIT and a
 

00:26:19.070 --> 00:26:21.010
introduced actually at MIT and a
master's thesis here over 50 years ago

00:26:21.010 --> 00:26:21.020
master's thesis here over 50 years ago
 

00:26:21.020 --> 00:26:22.900
master's thesis here over 50 years ago
it's widely used in different areas like

00:26:22.900 --> 00:26:22.910
it's widely used in different areas like
 

00:26:22.910 --> 00:26:24.580
it's widely used in different areas like
thermodynamics and we use it here in

00:26:24.580 --> 00:26:24.590
thermodynamics and we use it here in
 

00:26:24.590 --> 00:26:26.110
thermodynamics and we use it here in
machine learning as well it's used all

00:26:26.110 --> 00:26:26.120
machine learning as well it's used all
 

00:26:26.120 --> 00:26:29.680
machine learning as well it's used all
over information theory and what this is

00:26:29.680 --> 00:26:29.690
over information theory and what this is
 

00:26:29.690 --> 00:26:31.330
over information theory and what this is
doing here is essentially computing the

00:26:31.330 --> 00:26:31.340
doing here is essentially computing the
 

00:26:31.340 --> 00:26:34.890
doing here is essentially computing the
loss between this zero one output and

00:26:34.890 --> 00:26:34.900
loss between this zero one output and
 

00:26:34.900 --> 00:26:37.720
loss between this zero one output and
the true output that the student either

00:26:37.720 --> 00:26:37.730
the true output that the student either
 

00:26:37.730 --> 00:26:40.360
the true output that the student either
passed or failed to class let's suppose

00:26:40.360 --> 00:26:40.370
passed or failed to class let's suppose
 

00:26:40.370 --> 00:26:42.910
passed or failed to class let's suppose
instead of computing a zero one output

00:26:42.910 --> 00:26:42.920
instead of computing a zero one output
 

00:26:42.920 --> 00:26:43.480
instead of computing a zero one output
now we want to

00:26:43.480 --> 00:26:43.490
now we want to
 

00:26:43.490 --> 00:26:45.010
now we want to
compute the actual grade that you will

00:26:45.010 --> 00:26:45.020
compute the actual grade that you will
 

00:26:45.020 --> 00:26:47.380
compute the actual grade that you will
get on the class so now it's not 0-1 but

00:26:47.380 --> 00:26:47.390
get on the class so now it's not 0-1 but
 

00:26:47.390 --> 00:26:49.419
get on the class so now it's not 0-1 but
it's actually a grade it could be any

00:26:49.419 --> 00:26:49.429
it's actually a grade it could be any
 

00:26:49.429 --> 00:26:52.480
it's actually a grade it could be any
number actually right now we want to use

00:26:52.480 --> 00:26:52.490
number actually right now we want to use
 

00:26:52.490 --> 00:26:54.190
number actually right now we want to use
a different loss because the output of

00:26:54.190 --> 00:26:54.200
a different loss because the output of
 

00:26:54.200 --> 00:26:56.020
a different loss because the output of
our net of our neural network is

00:26:56.020 --> 00:26:56.030
our net of our neural network is
 

00:26:56.030 --> 00:26:58.570
our net of our neural network is
different and defining losses is

00:26:58.570 --> 00:26:58.580
different and defining losses is
 

00:26:58.580 --> 00:27:00.610
different and defining losses is
actually kind of one of the arts in deep

00:27:00.610 --> 00:27:00.620
actually kind of one of the arts in deep
 

00:27:00.620 --> 00:27:01.750
actually kind of one of the arts in deep
learning so you have to define the

00:27:01.750 --> 00:27:01.760
learning so you have to define the
 

00:27:01.760 --> 00:27:03.790
learning so you have to define the
questions that you're asking so you can

00:27:03.790 --> 00:27:03.800
questions that you're asking so you can
 

00:27:03.800 --> 00:27:05.770
questions that you're asking so you can
define the loss that you need to

00:27:05.770 --> 00:27:05.780
define the loss that you need to
 

00:27:05.780 --> 00:27:08.799
define the loss that you need to
optimize over so here in this example

00:27:08.799 --> 00:27:08.809
optimize over so here in this example
 

00:27:08.809 --> 00:27:10.690
optimize over so here in this example
since we're not optimizing over zero one

00:27:10.690 --> 00:27:10.700
since we're not optimizing over zero one
 

00:27:10.700 --> 00:27:12.760
since we're not optimizing over zero one
loss we're optimizing over any real

00:27:12.760 --> 00:27:12.770
loss we're optimizing over any real
 

00:27:12.770 --> 00:27:14.830
loss we're optimizing over any real
number we're gonna use a mean squared

00:27:14.830 --> 00:27:14.840
number we're gonna use a mean squared
 

00:27:14.840 --> 00:27:17.160
number we're gonna use a mean squared
error loss and that's just computing the

00:27:17.160 --> 00:27:17.170
error loss and that's just computing the
 

00:27:17.170 --> 00:27:19.419
error loss and that's just computing the
squared error so you take the difference

00:27:19.419 --> 00:27:19.429
squared error so you take the difference
 

00:27:19.429 --> 00:27:21.130
squared error so you take the difference
between what you expect the output to be

00:27:21.130 --> 00:27:21.140
between what you expect the output to be
 

00:27:21.140 --> 00:27:23.919
between what you expect the output to be
and what you're actually output was you

00:27:23.919 --> 00:27:23.929
and what you're actually output was you
 

00:27:23.929 --> 00:27:25.780
and what you're actually output was you
take that difference you square it and

00:27:25.780 --> 00:27:25.790
take that difference you square it and
 

00:27:25.790 --> 00:27:27.010
take that difference you square it and
you compute the mean over your entire

00:27:27.010 --> 00:27:27.020
you compute the mean over your entire
 

00:27:27.020 --> 00:27:30.730
you compute the mean over your entire
population okay great

00:27:30.730 --> 00:27:30.740
population okay great
 

00:27:30.740 --> 00:27:32.620
population okay great
so now let's put some of this

00:27:32.620 --> 00:27:32.630
so now let's put some of this
 

00:27:32.630 --> 00:27:33.970
so now let's put some of this
information together we've learned how

00:27:33.970 --> 00:27:33.980
information together we've learned how
 

00:27:33.980 --> 00:27:35.860
information together we've learned how
to build neural networks we've learned

00:27:35.860 --> 00:27:35.870
to build neural networks we've learned
 

00:27:35.870 --> 00:27:38.230
to build neural networks we've learned
how to quantify their loss now we can

00:27:38.230 --> 00:27:38.240
how to quantify their loss now we can
 

00:27:38.240 --> 00:27:40.650
how to quantify their loss now we can
learn how to actually use that loss to

00:27:40.650 --> 00:27:40.660
learn how to actually use that loss to
 

00:27:40.660 --> 00:27:43.060
learn how to actually use that loss to
iteratively update and train the neural

00:27:43.060 --> 00:27:43.070
iteratively update and train the neural
 

00:27:43.070 --> 00:27:46.950
iteratively update and train the neural
network over time given some data and

00:27:46.950 --> 00:27:46.960
network over time given some data and
 

00:27:46.960 --> 00:27:49.060
network over time given some data and
essentially what this amounts to what

00:27:49.060 --> 00:27:49.070
essentially what this amounts to what
 

00:27:49.070 --> 00:27:51.400
essentially what this amounts to what
this boils down to is that we want to

00:27:51.400 --> 00:27:51.410
this boils down to is that we want to
 

00:27:51.410 --> 00:27:54.430
this boils down to is that we want to
find the weights of the neural network W

00:27:54.430 --> 00:27:54.440
find the weights of the neural network W
 

00:27:54.440 --> 00:27:58.750
find the weights of the neural network W
that minimize this empirical loss so

00:27:58.750 --> 00:27:58.760
that minimize this empirical loss so
 

00:27:58.760 --> 00:28:01.540
that minimize this empirical loss so
remember again the empirical loss is the

00:28:01.540 --> 00:28:01.550
remember again the empirical loss is the
 

00:28:01.550 --> 00:28:03.040
remember again the empirical loss is the
loss over the entire training set it's

00:28:03.040 --> 00:28:03.050
loss over the entire training set it's
 

00:28:03.050 --> 00:28:04.720
loss over the entire training set it's
the mean loss of all of the popular of

00:28:04.720 --> 00:28:04.730
the mean loss of all of the popular of
 

00:28:04.730 --> 00:28:06.160
the mean loss of all of the popular of
all of the individuals in the training

00:28:06.160 --> 00:28:06.170
all of the individuals in the training
 

00:28:06.170 --> 00:28:08.799
all of the individuals in the training
set and we want to minimize that loss

00:28:08.799 --> 00:28:08.809
set and we want to minimize that loss
 

00:28:08.809 --> 00:28:10.930
set and we want to minimize that loss
and that essentially means we want to

00:28:10.930 --> 00:28:10.940
and that essentially means we want to
 

00:28:10.940 --> 00:28:13.870
and that essentially means we want to
find the weights the parameterization of

00:28:13.870 --> 00:28:13.880
find the weights the parameterization of
 

00:28:13.880 --> 00:28:16.450
find the weights the parameterization of
the network that results in the minimum

00:28:16.450 --> 00:28:16.460
the network that results in the minimum
 

00:28:16.460 --> 00:28:20.799
the network that results in the minimum
loss remember again that W here is just

00:28:20.799 --> 00:28:20.809
loss remember again that W here is just
 

00:28:20.809 --> 00:28:22.600
loss remember again that W here is just
a collection it's just a set of all of

00:28:22.600 --> 00:28:22.610
a collection it's just a set of all of
 

00:28:22.610 --> 00:28:24.520
a collection it's just a set of all of
the weights in the network so before I

00:28:24.520 --> 00:28:24.530
the weights in the network so before I
 

00:28:24.530 --> 00:28:27.490
the weights in the network so before I
define W as W 0 W 1 which is the weights

00:28:27.490 --> 00:28:27.500
define W as W 0 W 1 which is the weights
 

00:28:27.500 --> 00:28:29.560
define W as W 0 W 1 which is the weights
for the first layer second layer third

00:28:29.560 --> 00:28:29.570
for the first layer second layer third
 

00:28:29.570 --> 00:28:33.430
for the first layer second layer third
layer etc and you keep stacking all of

00:28:33.430 --> 00:28:33.440
layer etc and you keep stacking all of
 

00:28:33.440 --> 00:28:34.720
layer etc and you keep stacking all of
these weights together you combine them

00:28:34.720 --> 00:28:34.730
these weights together you combine them
 

00:28:34.730 --> 00:28:36.220
these weights together you combine them
and you want to compute this

00:28:36.220 --> 00:28:36.230
and you want to compute this
 

00:28:36.230 --> 00:28:38.320
and you want to compute this
optimization problem over all of these

00:28:38.320 --> 00:28:38.330
optimization problem over all of these
 

00:28:38.330 --> 00:28:43.930
optimization problem over all of these
weights so again remember our loss

00:28:43.930 --> 00:28:43.940
weights so again remember our loss
 

00:28:43.940 --> 00:28:45.669
weights so again remember our loss
function what does our loss function

00:28:45.669 --> 00:28:45.679
function what does our loss function
 

00:28:45.679 --> 00:28:47.710
function what does our loss function
look like it's just a simple function

00:28:47.710 --> 00:28:47.720
look like it's just a simple function
 

00:28:47.720 --> 00:28:51.940
look like it's just a simple function
that takes as inputs our weights and if

00:28:51.940 --> 00:28:51.950
that takes as inputs our weights and if
 

00:28:51.950 --> 00:28:53.650
that takes as inputs our weights and if
we have two weights we can actually

00:28:53.650 --> 00:28:53.660
we have two weights we can actually
 

00:28:53.660 --> 00:28:55.630
we have two weights we can actually
visualize it again we can see on the

00:28:55.630 --> 00:28:55.640
visualize it again we can see on the
 

00:28:55.640 --> 00:28:57.060
visualize it again we can see on the
x-axis

00:28:57.060 --> 00:28:57.070
x-axis
 

00:28:57.070 --> 00:28:59.590
x-axis
one way so this is one scaler that we

00:28:59.590 --> 00:28:59.600
one way so this is one scaler that we
 

00:28:59.600 --> 00:29:01.299
one way so this is one scaler that we
can change and another way on the y axis

00:29:01.299 --> 00:29:01.309
can change and another way on the y axis
 

00:29:01.309 --> 00:29:04.060
can change and another way on the y axis
and on the z axis this is our actual

00:29:04.060 --> 00:29:04.070
and on the z axis this is our actual
 

00:29:04.070 --> 00:29:08.649
and on the z axis this is our actual
loss if we want to find the lowest point

00:29:08.649 --> 00:29:08.659
loss if we want to find the lowest point
 

00:29:08.659 --> 00:29:10.480
loss if we want to find the lowest point
in this landscape that corresponds to

00:29:10.480 --> 00:29:10.490
in this landscape that corresponds to
 

00:29:10.490 --> 00:29:12.909
in this landscape that corresponds to
the minimum loss and we want to find

00:29:12.909 --> 00:29:12.919
the minimum loss and we want to find
 

00:29:12.919 --> 00:29:14.200
the minimum loss and we want to find
that point so that we can find the

00:29:14.200 --> 00:29:14.210
that point so that we can find the
 

00:29:14.210 --> 00:29:16.659
that point so that we can find the
corresponding weights that were set to

00:29:16.659 --> 00:29:16.669
corresponding weights that were set to
 

00:29:16.669 --> 00:29:20.680
corresponding weights that were set to
achieve that minimum loss so how do we

00:29:20.680 --> 00:29:20.690
achieve that minimum loss so how do we
 

00:29:20.690 --> 00:29:23.440
achieve that minimum loss so how do we
do it we use this technique called loss

00:29:23.440 --> 00:29:23.450
do it we use this technique called loss
 

00:29:23.450 --> 00:29:26.889
do it we use this technique called loss
optimization through gradient descent we

00:29:26.889 --> 00:29:26.899
optimization through gradient descent we
 

00:29:26.899 --> 00:29:29.260
optimization through gradient descent we
start by picking an initial point on

00:29:29.260 --> 00:29:29.270
start by picking an initial point on
 

00:29:29.270 --> 00:29:32.169
start by picking an initial point on
this landscape an initial w0 w1 so

00:29:32.169 --> 00:29:32.179
this landscape an initial w0 w1 so
 

00:29:32.179 --> 00:29:35.139
this landscape an initial w0 w1 so
here's this point this black cross we

00:29:35.139 --> 00:29:35.149
here's this point this black cross we
 

00:29:35.149 --> 00:29:38.230
here's this point this black cross we
start at this point we compute the

00:29:38.230 --> 00:29:38.240
start at this point we compute the
 

00:29:38.240 --> 00:29:41.409
start at this point we compute the
gradient at this local point and in this

00:29:41.409 --> 00:29:41.419
gradient at this local point and in this
 

00:29:41.419 --> 00:29:43.000
gradient at this local point and in this
landscape we can see that the gradient

00:29:43.000 --> 00:29:43.010
landscape we can see that the gradient
 

00:29:43.010 --> 00:29:47.039
landscape we can see that the gradient
tells us the direction of maximal ascent

00:29:47.039 --> 00:29:47.049
tells us the direction of maximal ascent
 

00:29:47.049 --> 00:29:49.269
tells us the direction of maximal ascent
now that we know the direction of the

00:29:49.269 --> 00:29:49.279
now that we know the direction of the
 

00:29:49.279 --> 00:29:50.950
now that we know the direction of the
maximal ascent we can reverse that

00:29:50.950 --> 00:29:50.960
maximal ascent we can reverse that
 

00:29:50.960 --> 00:29:53.260
maximal ascent we can reverse that
gradient and actually take a small step

00:29:53.260 --> 00:29:53.270
gradient and actually take a small step
 

00:29:53.270 --> 00:29:56.649
gradient and actually take a small step
in the opposite direction that moves us

00:29:56.649 --> 00:29:56.659
in the opposite direction that moves us
 

00:29:56.659 --> 00:29:58.510
in the opposite direction that moves us
closer towards the lowest point because

00:29:58.510 --> 00:29:58.520
closer towards the lowest point because
 

00:29:58.520 --> 00:30:00.279
closer towards the lowest point because
we're taking a greedy approach to move

00:30:00.279 --> 00:30:00.289
we're taking a greedy approach to move
 

00:30:00.289 --> 00:30:01.870
we're taking a greedy approach to move
in the opposite direction of the

00:30:01.870 --> 00:30:01.880
in the opposite direction of the
 

00:30:01.880 --> 00:30:04.750
in the opposite direction of the
gradient we can iteratively repeat this

00:30:04.750 --> 00:30:04.760
gradient we can iteratively repeat this
 

00:30:04.760 --> 00:30:07.029
gradient we can iteratively repeat this
process over and over and over again we

00:30:07.029 --> 00:30:07.039
process over and over and over again we
 

00:30:07.039 --> 00:30:08.860
process over and over and over again we
computing the gradient at each time and

00:30:08.860 --> 00:30:08.870
computing the gradient at each time and
 

00:30:08.870 --> 00:30:11.560
computing the gradient at each time and
keep moving moving closer towards that

00:30:11.560 --> 00:30:11.570
keep moving moving closer towards that
 

00:30:11.570 --> 00:30:15.310
keep moving moving closer towards that
lowest minimum we can summarize this

00:30:15.310 --> 00:30:15.320
lowest minimum we can summarize this
 

00:30:15.320 --> 00:30:17.200
lowest minimum we can summarize this
algorithm known as gradient descent in

00:30:17.200 --> 00:30:17.210
algorithm known as gradient descent in
 

00:30:17.210 --> 00:30:20.919
algorithm known as gradient descent in
pseudocode by this the pseudocode on the

00:30:20.919 --> 00:30:20.929
pseudocode by this the pseudocode on the
 

00:30:20.929 --> 00:30:23.230
pseudocode by this the pseudocode on the
left-hand side we start by initializing

00:30:23.230 --> 00:30:23.240
left-hand side we start by initializing
 

00:30:23.240 --> 00:30:25.419
left-hand side we start by initializing
our weights randomly computing this

00:30:25.419 --> 00:30:25.429
our weights randomly computing this
 

00:30:25.429 --> 00:30:30.340
our weights randomly computing this
gradient DJ DW then updating our weights

00:30:30.340 --> 00:30:30.350
gradient DJ DW then updating our weights
 

00:30:30.350 --> 00:30:31.960
gradient DJ DW then updating our weights
in the opposite direction of that

00:30:31.960 --> 00:30:31.970
in the opposite direction of that
 

00:30:31.970 --> 00:30:37.289
in the opposite direction of that
gradient we used this small amount ADA

00:30:37.289 --> 00:30:37.299
gradient we used this small amount ADA
 

00:30:37.299 --> 00:30:40.750
gradient we used this small amount ADA
which you can see here and this is

00:30:40.750 --> 00:30:40.760
which you can see here and this is
 

00:30:40.760 --> 00:30:42.580
which you can see here and this is
essentially what we call the learning

00:30:42.580 --> 00:30:42.590
essentially what we call the learning
 

00:30:42.590 --> 00:30:44.440
essentially what we call the learning
rate this is determining how much of a

00:30:44.440 --> 00:30:44.450
rate this is determining how much of a
 

00:30:44.450 --> 00:30:46.480
rate this is determining how much of a
step we take and how much we trust that

00:30:46.480 --> 00:30:46.490
step we take and how much we trust that
 

00:30:46.490 --> 00:30:48.430
step we take and how much we trust that
with that gradient update that we

00:30:48.430 --> 00:30:48.440
with that gradient update that we
 

00:30:48.440 --> 00:30:50.380
with that gradient update that we
computed we'll talk more about this

00:30:50.380 --> 00:30:50.390
computed we'll talk more about this
 

00:30:50.390 --> 00:30:53.409
computed we'll talk more about this
later but for now let's take a look at

00:30:53.409 --> 00:30:53.419
later but for now let's take a look at
 

00:30:53.419 --> 00:30:56.950
later but for now let's take a look at
this term here this gradient DJ DW is

00:30:56.950 --> 00:30:56.960
this term here this gradient DJ DW is
 

00:30:56.960 --> 00:30:59.889
this term here this gradient DJ DW is
actually explaining how the lost changes

00:30:59.889 --> 00:30:59.899
actually explaining how the lost changes
 

00:30:59.899 --> 00:31:03.370
actually explaining how the lost changes
with respect to each of the weights but

00:31:03.370 --> 00:31:03.380
with respect to each of the weights but
 

00:31:03.380 --> 00:31:05.139
with respect to each of the weights but
I never actually told you how to compute

00:31:05.139 --> 00:31:05.149
I never actually told you how to compute
 

00:31:05.149 --> 00:31:07.510
I never actually told you how to compute
this term this is actually a crucial

00:31:07.510 --> 00:31:07.520
this term this is actually a crucial
 

00:31:07.520 --> 00:31:09.310
this term this is actually a crucial
part of deep learning and neural

00:31:09.310 --> 00:31:09.320
part of deep learning and neural
 

00:31:09.320 --> 00:31:10.720
part of deep learning and neural
networks in general

00:31:10.720 --> 00:31:10.730
networks in general
 

00:31:10.730 --> 00:31:12.460
networks in general
computing this term is essentially all

00:31:12.460 --> 00:31:12.470
computing this term is essentially all
 

00:31:12.470 --> 00:31:15.010
computing this term is essentially all
that matters when you try and optimize

00:31:15.010 --> 00:31:15.020
that matters when you try and optimize
 

00:31:15.020 --> 00:31:17.140
that matters when you try and optimize
your network is the most computational

00:31:17.140 --> 00:31:17.150
your network is the most computational
 

00:31:17.150 --> 00:31:19.570
your network is the most computational
part of training as well and it's known

00:31:19.570 --> 00:31:19.580
part of training as well and it's known
 

00:31:19.580 --> 00:31:22.660
part of training as well and it's known
as back propagation we'll start with a

00:31:22.660 --> 00:31:22.670
as back propagation we'll start with a
 

00:31:22.670 --> 00:31:24.850
as back propagation we'll start with a
very simple network with only one hidden

00:31:24.850 --> 00:31:24.860
very simple network with only one hidden
 

00:31:24.860 --> 00:31:26.890
very simple network with only one hidden
input sorry with one input one hidden

00:31:26.890 --> 00:31:26.900
input sorry with one input one hidden
 

00:31:26.900 --> 00:31:30.450
input sorry with one input one hidden
layer one handed and unit and one output

00:31:30.450 --> 00:31:30.460
layer one handed and unit and one output
 

00:31:30.460 --> 00:31:33.250
layer one handed and unit and one output
computing the gradient of our loss with

00:31:33.250 --> 00:31:33.260
computing the gradient of our loss with
 

00:31:33.260 --> 00:31:37.090
computing the gradient of our loss with
respect to W to corresponds to telling

00:31:37.090 --> 00:31:37.100
respect to W to corresponds to telling
 

00:31:37.100 --> 00:31:40.000
respect to W to corresponds to telling
us how much a small change in our and W

00:31:40.000 --> 00:31:40.010
us how much a small change in our and W
 

00:31:40.010 --> 00:31:45.910
us how much a small change in our and W
two affects our output or loss so if we

00:31:45.910 --> 00:31:45.920
two affects our output or loss so if we
 

00:31:45.920 --> 00:31:48.490
two affects our output or loss so if we
write this as a derivative we can start

00:31:48.490 --> 00:31:48.500
write this as a derivative we can start
 

00:31:48.500 --> 00:31:50.410
write this as a derivative we can start
by computing this by simply expanding

00:31:50.410 --> 00:31:50.420
by computing this by simply expanding
 

00:31:50.420 --> 00:31:52.270
by computing this by simply expanding
this derivative into a chain by using

00:31:52.270 --> 00:31:52.280
this derivative into a chain by using
 

00:31:52.280 --> 00:31:55.030
this derivative into a chain by using
the chain rule backwards from the loss

00:31:55.030 --> 00:31:55.040
the chain rule backwards from the loss
 

00:31:55.040 --> 00:31:58.030
the chain rule backwards from the loss
through the output and that looks like

00:31:58.030 --> 00:31:58.040
through the output and that looks like
 

00:31:58.040 --> 00:32:06.460
through the output and that looks like
this so DJ DW 2 becomes DJ dy dy DW 2 ok

00:32:06.460 --> 00:32:06.470
this so DJ DW 2 becomes DJ dy dy DW 2 ok
 

00:32:06.470 --> 00:32:07.930
this so DJ DW 2 becomes DJ dy dy DW 2 ok
and that's just a simple application of

00:32:07.930 --> 00:32:07.940
and that's just a simple application of
 

00:32:07.940 --> 00:32:10.990
and that's just a simple application of
the chain rule now let's suppose instead

00:32:10.990 --> 00:32:11.000
the chain rule now let's suppose instead
 

00:32:11.000 --> 00:32:13.180
the chain rule now let's suppose instead
of computing DJ DW 2 we want to compute

00:32:13.180 --> 00:32:13.190
of computing DJ DW 2 we want to compute
 

00:32:13.190 --> 00:32:17.320
of computing DJ DW 2 we want to compute
DJ DW 1 so I've changed the W 1 the W 2

00:32:17.320 --> 00:32:17.330
DJ DW 1 so I've changed the W 1 the W 2
 

00:32:17.330 --> 00:32:19.150
DJ DW 1 so I've changed the W 1 the W 2
to a W 1 on the left hand side and now

00:32:19.150 --> 00:32:19.160
to a W 1 on the left hand side and now
 

00:32:19.160 --> 00:32:21.580
to a W 1 on the left hand side and now
we want to compute this well we can

00:32:21.580 --> 00:32:21.590
we want to compute this well we can
 

00:32:21.590 --> 00:32:24.760
we want to compute this well we can
simply apply the chain rule again we can

00:32:24.760 --> 00:32:24.770
simply apply the chain rule again we can
 

00:32:24.770 --> 00:32:27.130
simply apply the chain rule again we can
take that middle term now expand it out

00:32:27.130 --> 00:32:27.140
take that middle term now expand it out
 

00:32:27.140 --> 00:32:29.080
take that middle term now expand it out
again using the same chain rule and back

00:32:29.080 --> 00:32:29.090
again using the same chain rule and back
 

00:32:29.090 --> 00:32:30.910
again using the same chain rule and back
propagate those gradients even further

00:32:30.910 --> 00:32:30.920
propagate those gradients even further
 

00:32:30.920 --> 00:32:35.710
propagate those gradients even further
back in in the network and essentially

00:32:35.710 --> 00:32:35.720
back in in the network and essentially
 

00:32:35.720 --> 00:32:37.240
back in in the network and essentially
we keep repeating this for every weight

00:32:37.240 --> 00:32:37.250
we keep repeating this for every weight
 

00:32:37.250 --> 00:32:39.220
we keep repeating this for every weight
in the network using the gradients for

00:32:39.220 --> 00:32:39.230
in the network using the gradients for
 

00:32:39.230 --> 00:32:41.140
in the network using the gradients for
later layers to back propagate those

00:32:41.140 --> 00:32:41.150
later layers to back propagate those
 

00:32:41.150 --> 00:32:43.540
later layers to back propagate those
errors back into the original input we

00:32:43.540 --> 00:32:43.550
errors back into the original input we
 

00:32:43.550 --> 00:32:45.580
errors back into the original input we
do this for all of the weights and and

00:32:45.580 --> 00:32:45.590
do this for all of the weights and and
 

00:32:45.590 --> 00:32:46.810
do this for all of the weights and and
that gives us our gradient for each

00:32:46.810 --> 00:32:46.820
that gives us our gradient for each
 

00:32:46.820 --> 00:32:58.599
that gives us our gradient for each
weight

00:32:58.599 --> 00:32:58.609
 

00:32:58.609 --> 00:33:01.279
yeah you're completely right so the

00:33:01.279 --> 00:33:01.289
yeah you're completely right so the
 

00:33:01.289 --> 00:33:02.840
yeah you're completely right so the
question is how do you ensure that this

00:33:02.840 --> 00:33:02.850
question is how do you ensure that this
 

00:33:02.850 --> 00:33:04.759
question is how do you ensure that this
gives you a global minimum instead of a

00:33:04.759 --> 00:33:04.769
gives you a global minimum instead of a
 

00:33:04.769 --> 00:33:08.810
gives you a global minimum instead of a
local minimum right so you don't we have

00:33:08.810 --> 00:33:08.820
local minimum right so you don't we have
 

00:33:08.820 --> 00:33:10.519
local minimum right so you don't we have
no guarantees on that this is not a

00:33:10.519 --> 00:33:10.529
no guarantees on that this is not a
 

00:33:10.529 --> 00:33:14.269
no guarantees on that this is not a
global minimum the whole training of

00:33:14.269 --> 00:33:14.279
global minimum the whole training of
 

00:33:14.279 --> 00:33:15.769
global minimum the whole training of
stochastic gradient sent is a greedy

00:33:15.769 --> 00:33:15.779
stochastic gradient sent is a greedy
 

00:33:15.779 --> 00:33:17.269
stochastic gradient sent is a greedy
optimization algorithm so you're only

00:33:17.269 --> 00:33:17.279
optimization algorithm so you're only
 

00:33:17.279 --> 00:33:18.649
optimization algorithm so you're only
taking this greedy approach and

00:33:18.649 --> 00:33:18.659
taking this greedy approach and
 

00:33:18.659 --> 00:33:21.289
taking this greedy approach and
optimizing only a local minimum there

00:33:21.289 --> 00:33:21.299
optimizing only a local minimum there
 

00:33:21.299 --> 00:33:23.570
optimizing only a local minimum there
are different ways extensions of

00:33:23.570 --> 00:33:23.580
are different ways extensions of
 

00:33:23.580 --> 00:33:25.129
are different ways extensions of
stochastic gradient descent that don't

00:33:25.129 --> 00:33:25.139
stochastic gradient descent that don't
 

00:33:25.139 --> 00:33:27.560
stochastic gradient descent that don't
take a greedy approach they take an

00:33:27.560 --> 00:33:27.570
take a greedy approach they take an
 

00:33:27.570 --> 00:33:29.090
take a greedy approach they take an
adaptive approach they look around a

00:33:29.090 --> 00:33:29.100
adaptive approach they look around a
 

00:33:29.100 --> 00:33:30.830
adaptive approach they look around a
little bit these are typically more

00:33:30.830 --> 00:33:30.840
little bit these are typically more
 

00:33:30.840 --> 00:33:33.259
little bit these are typically more
expensive to compute stochastic gradient

00:33:33.259 --> 00:33:33.269
expensive to compute stochastic gradient
 

00:33:33.269 --> 00:33:35.210
expensive to compute stochastic gradient
side is extremely cheap to compute in

00:33:35.210 --> 00:33:35.220
side is extremely cheap to compute in
 

00:33:35.220 --> 00:33:37.549
side is extremely cheap to compute in
practice and that's one of the reasons

00:33:37.549 --> 00:33:37.559
practice and that's one of the reasons
 

00:33:37.559 --> 00:33:39.710
practice and that's one of the reasons
it's used the second reason is that in

00:33:39.710 --> 00:33:39.720
it's used the second reason is that in
 

00:33:39.720 --> 00:33:42.489
it's used the second reason is that in
practice local minimum tend to be

00:33:42.489 --> 00:33:42.499
practice local minimum tend to be
 

00:33:42.499 --> 00:33:47.960
practice local minimum tend to be
sufficient so that's the back

00:33:47.960 --> 00:33:47.970
sufficient so that's the back
 

00:33:47.970 --> 00:33:51.440
sufficient so that's the back
propagation algorithm in theory it

00:33:51.440 --> 00:33:51.450
propagation algorithm in theory it
 

00:33:51.450 --> 00:33:52.999
propagation algorithm in theory it
sounds very simple it's just an

00:33:52.999 --> 00:33:53.009
sounds very simple it's just an
 

00:33:53.009 --> 00:33:55.339
sounds very simple it's just an
application of the chain rule but now

00:33:55.339 --> 00:33:55.349
application of the chain rule but now
 

00:33:55.349 --> 00:33:58.159
application of the chain rule but now
let's touch on some insights on training

00:33:58.159 --> 00:33:58.169
let's touch on some insights on training
 

00:33:58.169 --> 00:33:59.779
let's touch on some insights on training
these neural networks in practice that

00:33:59.779 --> 00:33:59.789
these neural networks in practice that
 

00:33:59.789 --> 00:34:01.339
these neural networks in practice that
makes it incredibly complex and this

00:34:01.339 --> 00:34:01.349
makes it incredibly complex and this
 

00:34:01.349 --> 00:34:02.899
makes it incredibly complex and this
gets back to that that previous point

00:34:02.899 --> 00:34:02.909
gets back to that that previous point
 

00:34:02.909 --> 00:34:04.549
gets back to that that previous point
that previous question that was raised

00:34:04.549 --> 00:34:04.559
that previous question that was raised
 

00:34:04.559 --> 00:34:07.129
that previous question that was raised
in practice training neural networks is

00:34:07.129 --> 00:34:07.139
in practice training neural networks is
 

00:34:07.139 --> 00:34:08.869
in practice training neural networks is
incredibly difficult this is a

00:34:08.869 --> 00:34:08.879
incredibly difficult this is a
 

00:34:08.879 --> 00:34:11.089
incredibly difficult this is a
visualization of the lost landscape of a

00:34:11.089 --> 00:34:11.099
visualization of the lost landscape of a
 

00:34:11.099 --> 00:34:14.389
visualization of the lost landscape of a
neural network in practice this is a

00:34:14.389 --> 00:34:14.399
neural network in practice this is a
 

00:34:14.399 --> 00:34:16.250
neural network in practice this is a
paper from about a year ago and the

00:34:16.250 --> 00:34:16.260
paper from about a year ago and the
 

00:34:16.260 --> 00:34:17.720
paper from about a year ago and the
authors visualize what a deep neural

00:34:17.720 --> 00:34:17.730
authors visualize what a deep neural
 

00:34:17.730 --> 00:34:19.819
authors visualize what a deep neural
network lost landscape really looks like

00:34:19.819 --> 00:34:19.829
network lost landscape really looks like
 

00:34:19.829 --> 00:34:22.399
network lost landscape really looks like
you can see many many many local minimum

00:34:22.399 --> 00:34:22.409
you can see many many many local minimum
 

00:34:22.409 --> 00:34:24.829
you can see many many many local minimum
here lot minimizing this loss and

00:34:24.829 --> 00:34:24.839
here lot minimizing this loss and
 

00:34:24.839 --> 00:34:26.960
here lot minimizing this loss and
finally the optimal true minimum is

00:34:26.960 --> 00:34:26.970
finally the optimal true minimum is
 

00:34:26.970 --> 00:34:29.829
finally the optimal true minimum is
extremely difficult

00:34:29.829 --> 00:34:29.839
extremely difficult
 

00:34:29.839 --> 00:34:32.539
extremely difficult
now recall the update equation that we

00:34:32.539 --> 00:34:32.549
now recall the update equation that we
 

00:34:32.549 --> 00:34:34.250
now recall the update equation that we
fought defined for a gradient descent

00:34:34.250 --> 00:34:34.260
fought defined for a gradient descent
 

00:34:34.260 --> 00:34:36.829
fought defined for a gradient descent
previously we take our weights and we

00:34:36.829 --> 00:34:36.839
previously we take our weights and we
 

00:34:36.839 --> 00:34:40.280
previously we take our weights and we
subtract we move towards the negative

00:34:40.280 --> 00:34:40.290
subtract we move towards the negative
 

00:34:40.290 --> 00:34:41.899
subtract we move towards the negative
gradient and we update our weights in

00:34:41.899 --> 00:34:41.909
gradient and we update our weights in
 

00:34:41.909 --> 00:34:46.490
gradient and we update our weights in
that direction I didn't talk too much

00:34:46.490 --> 00:34:46.500
that direction I didn't talk too much
 

00:34:46.500 --> 00:34:48.980
that direction I didn't talk too much
about this parameter heydo this is what

00:34:48.980 --> 00:34:48.990
about this parameter heydo this is what
 

00:34:48.990 --> 00:34:50.359
about this parameter heydo this is what
we called the learning rate I briefly

00:34:50.359 --> 00:34:50.369
we called the learning rate I briefly
 

00:34:50.369 --> 00:34:52.339
we called the learning rate I briefly
touched on it and this is essentially

00:34:52.339 --> 00:34:52.349
touched on it and this is essentially
 

00:34:52.349 --> 00:34:54.770
touched on it and this is essentially
determining how large of a step we take

00:34:54.770 --> 00:34:54.780
determining how large of a step we take
 

00:34:54.780 --> 00:34:57.890
determining how large of a step we take
at each iteration in practice setting

00:34:57.890 --> 00:34:57.900
at each iteration in practice setting
 

00:34:57.900 --> 00:34:59.120
at each iteration in practice setting
the learning rate can be extremely

00:34:59.120 --> 00:34:59.130
the learning rate can be extremely
 

00:34:59.130 --> 00:35:01.339
the learning rate can be extremely
difficult and actually very important

00:35:01.339 --> 00:35:01.349
difficult and actually very important
 

00:35:01.349 --> 00:35:03.380
difficult and actually very important
for making sure that you avoid local

00:35:03.380 --> 00:35:03.390
for making sure that you avoid local
 

00:35:03.390 --> 00:35:06.650
for making sure that you avoid local
minima again so if we set the learning

00:35:06.650 --> 00:35:06.660
minima again so if we set the learning
 

00:35:06.660 --> 00:35:08.480
minima again so if we set the learning
rate to slow then the model may get

00:35:08.480 --> 00:35:08.490
rate to slow then the model may get
 

00:35:08.490 --> 00:35:10.050
rate to slow then the model may get
stuck in local minimum like

00:35:10.050 --> 00:35:10.060
stuck in local minimum like
 

00:35:10.060 --> 00:35:12.270
stuck in local minimum like
this it could also converge very slowly

00:35:12.270 --> 00:35:12.280
this it could also converge very slowly
 

00:35:12.280 --> 00:35:13.680
this it could also converge very slowly
even in the case that it gets to a

00:35:13.680 --> 00:35:13.690
even in the case that it gets to a
 

00:35:13.690 --> 00:35:16.320
even in the case that it gets to a
global minimum if we set the learning

00:35:16.320 --> 00:35:16.330
global minimum if we set the learning
 

00:35:16.330 --> 00:35:19.260
global minimum if we set the learning
rate too large the gradients essentially

00:35:19.260 --> 00:35:19.270
rate too large the gradients essentially
 

00:35:19.270 --> 00:35:21.510
rate too large the gradients essentially
explodes and we diverge from the loss

00:35:21.510 --> 00:35:21.520
explodes and we diverge from the loss
 

00:35:21.520 --> 00:35:24.750
explodes and we diverge from the loss
itself and it's also been setting the

00:35:24.750 --> 00:35:24.760
itself and it's also been setting the
 

00:35:24.760 --> 00:35:26.760
itself and it's also been setting the
learning rate to the correct amount can

00:35:26.760 --> 00:35:26.770
learning rate to the correct amount can
 

00:35:26.770 --> 00:35:28.620
learning rate to the correct amount can
be extremely tedious in practice such

00:35:28.620 --> 00:35:28.630
be extremely tedious in practice such
 

00:35:28.630 --> 00:35:30.180
be extremely tedious in practice such
that we overshoot some of the local

00:35:30.180 --> 00:35:30.190
that we overshoot some of the local
 

00:35:30.190 --> 00:35:32.820
that we overshoot some of the local
minima get ourselves into a reasonable

00:35:32.820 --> 00:35:32.830
minima get ourselves into a reasonable
 

00:35:32.830 --> 00:35:35.190
minima get ourselves into a reasonable
local global minima and then converge in

00:35:35.190 --> 00:35:35.200
local global minima and then converge in
 

00:35:35.200 --> 00:35:40.260
local global minima and then converge in
within that global minima how can we do

00:35:40.260 --> 00:35:40.270
within that global minima how can we do
 

00:35:40.270 --> 00:35:43.050
within that global minima how can we do
this in a clever way so one option is

00:35:43.050 --> 00:35:43.060
this in a clever way so one option is
 

00:35:43.060 --> 00:35:45.150
this in a clever way so one option is
that we can try a lot of different

00:35:45.150 --> 00:35:45.160
that we can try a lot of different
 

00:35:45.160 --> 00:35:47.340
that we can try a lot of different
possible learning rates see what works

00:35:47.340 --> 00:35:47.350
possible learning rates see what works
 

00:35:47.350 --> 00:35:51.390
possible learning rates see what works
best in practice and in practice this is

00:35:51.390 --> 00:35:51.400
best in practice and in practice this is
 

00:35:51.400 --> 00:35:53.100
best in practice and in practice this is
actually a very common technique so a

00:35:53.100 --> 00:35:53.110
actually a very common technique so a
 

00:35:53.110 --> 00:35:55.200
actually a very common technique so a
lot of people just try a lot of learning

00:35:55.200 --> 00:35:55.210
lot of people just try a lot of learning
 

00:35:55.210 --> 00:35:57.540
lot of people just try a lot of learning
rates and see what works best let's see

00:35:57.540 --> 00:35:57.550
rates and see what works best let's see
 

00:35:57.550 --> 00:35:59.010
rates and see what works best let's see
if we can do something a bit smarter

00:35:59.010 --> 00:35:59.020
if we can do something a bit smarter
 

00:35:59.020 --> 00:36:01.500
if we can do something a bit smarter
than that as well how about we design an

00:36:01.500 --> 00:36:01.510
than that as well how about we design an
 

00:36:01.510 --> 00:36:05.160
than that as well how about we design an
adaptive algorithm that learnt that you

00:36:05.160 --> 00:36:05.170
adaptive algorithm that learnt that you
 

00:36:05.170 --> 00:36:07.890
adaptive algorithm that learnt that you
that adapts its learning rate according

00:36:07.890 --> 00:36:07.900
that adapts its learning rate according
 

00:36:07.900 --> 00:36:10.590
that adapts its learning rate according
to the lost landscape so this can take

00:36:10.590 --> 00:36:10.600
to the lost landscape so this can take
 

00:36:10.600 --> 00:36:13.230
to the lost landscape so this can take
into account the gradient at other

00:36:13.230 --> 00:36:13.240
into account the gradient at other
 

00:36:13.240 --> 00:36:16.080
into account the gradient at other
locations and loss it can take into

00:36:16.080 --> 00:36:16.090
locations and loss it can take into
 

00:36:16.090 --> 00:36:18.810
locations and loss it can take into
account how fast we're learning how how

00:36:18.810 --> 00:36:18.820
account how fast we're learning how how
 

00:36:18.820 --> 00:36:20.550
account how fast we're learning how how
large the gradient is at that location

00:36:20.550 --> 00:36:20.560
large the gradient is at that location
 

00:36:20.560 --> 00:36:23.460
large the gradient is at that location
or many other options but now since our

00:36:23.460 --> 00:36:23.470
or many other options but now since our
 

00:36:23.470 --> 00:36:25.290
or many other options but now since our
learning rate is not fixed for all of

00:36:25.290 --> 00:36:25.300
learning rate is not fixed for all of
 

00:36:25.300 --> 00:36:28.140
learning rate is not fixed for all of
the iterations of gradient descent we

00:36:28.140 --> 00:36:28.150
the iterations of gradient descent we
 

00:36:28.150 --> 00:36:29.670
the iterations of gradient descent we
have a bit more flexibility now in

00:36:29.670 --> 00:36:29.680
have a bit more flexibility now in
 

00:36:29.680 --> 00:36:33.090
have a bit more flexibility now in
learning in fact this has been widely

00:36:33.090 --> 00:36:33.100
learning in fact this has been widely
 

00:36:33.100 --> 00:36:34.800
learning in fact this has been widely
studied as well there are many many

00:36:34.800 --> 00:36:34.810
studied as well there are many many
 

00:36:34.810 --> 00:36:36.630
studied as well there are many many
different options for optimization

00:36:36.630 --> 00:36:36.640
different options for optimization
 

00:36:36.640 --> 00:36:38.700
different options for optimization
schemes that are present in tensorflow

00:36:38.700 --> 00:36:38.710
schemes that are present in tensorflow
 

00:36:38.710 --> 00:36:41.220
schemes that are present in tensorflow
and here are examples of some of them

00:36:41.220 --> 00:36:41.230
and here are examples of some of them
 

00:36:41.230 --> 00:36:43.200
and here are examples of some of them
during your labs I encourage you to try

00:36:43.200 --> 00:36:43.210
during your labs I encourage you to try
 

00:36:43.210 --> 00:36:45.330
during your labs I encourage you to try
out different of these different ones of

00:36:45.330 --> 00:36:45.340
out different of these different ones of
 

00:36:45.340 --> 00:36:47.850
out different of these different ones of
these optimizers and see how they're

00:36:47.850 --> 00:36:47.860
these optimizers and see how they're
 

00:36:47.860 --> 00:36:49.920
these optimizers and see how they're
different which works best which doesn't

00:36:49.920 --> 00:36:49.930
different which works best which doesn't
 

00:36:49.930 --> 00:36:51.720
different which works best which doesn't
work so well for your particular problem

00:36:51.720 --> 00:36:51.730
work so well for your particular problem
 

00:36:51.730 --> 00:36:57.420
work so well for your particular problem
and they're all adaptive in nature so

00:36:57.420 --> 00:36:57.430
and they're all adaptive in nature so
 

00:36:57.430 --> 00:36:58.800
and they're all adaptive in nature so
now I want to continue talking about

00:36:58.800 --> 00:36:58.810
now I want to continue talking about
 

00:36:58.810 --> 00:37:00.990
now I want to continue talking about
tips for training these networks in

00:37:00.990 --> 00:37:01.000
tips for training these networks in
 

00:37:01.000 --> 00:37:03.660
tips for training these networks in
practice and focus on the very powerful

00:37:03.660 --> 00:37:03.670
practice and focus on the very powerful
 

00:37:03.670 --> 00:37:06.410
practice and focus on the very powerful
idea of batching gradient descent and

00:37:06.410 --> 00:37:06.420
idea of batching gradient descent and
 

00:37:06.420 --> 00:37:10.170
idea of batching gradient descent and
batching your data in general so to do

00:37:10.170 --> 00:37:10.180
batching your data in general so to do
 

00:37:10.180 --> 00:37:12.870
batching your data in general so to do
this let's revisit this idea of gradient

00:37:12.870 --> 00:37:12.880
this let's revisit this idea of gradient
 

00:37:12.880 --> 00:37:16.140
this let's revisit this idea of gradient
descent very quickly so the gradient is

00:37:16.140 --> 00:37:16.150
descent very quickly so the gradient is
 

00:37:16.150 --> 00:37:18.270
descent very quickly so the gradient is
actually very computational to compute

00:37:18.270 --> 00:37:18.280
actually very computational to compute
 

00:37:18.280 --> 00:37:20.190
actually very computational to compute
this back propagation algorithm if you

00:37:20.190 --> 00:37:20.200
this back propagation algorithm if you
 

00:37:20.200 --> 00:37:21.600
this back propagation algorithm if you
want to compute it for all of the data

00:37:21.600 --> 00:37:21.610
want to compute it for all of the data
 

00:37:21.610 --> 00:37:23.340
want to compute it for all of the data
samples in your training data set which

00:37:23.340 --> 00:37:23.350
samples in your training data set which
 

00:37:23.350 --> 00:37:23.910
samples in your training data set which
may be

00:37:23.910 --> 00:37:23.920
may be
 

00:37:23.920 --> 00:37:26.609
may be
massive in modern data sets it's

00:37:26.609 --> 00:37:26.619
massive in modern data sets it's
 

00:37:26.619 --> 00:37:28.710
massive in modern data sets it's
essentially amounting to a summation

00:37:28.710 --> 00:37:28.720
essentially amounting to a summation
 

00:37:28.720 --> 00:37:32.010
essentially amounting to a summation
over all of these data points in most

00:37:32.010 --> 00:37:32.020
over all of these data points in most
 

00:37:32.020 --> 00:37:34.109
over all of these data points in most
real life problems this is extremely

00:37:34.109 --> 00:37:34.119
real life problems this is extremely
 

00:37:34.119 --> 00:37:35.760
real life problems this is extremely
computational and not feasible to

00:37:35.760 --> 00:37:35.770
computational and not feasible to
 

00:37:35.770 --> 00:37:38.630
computational and not feasible to
compute on every iteration so instead

00:37:38.630 --> 00:37:38.640
compute on every iteration so instead
 

00:37:38.640 --> 00:37:41.069
compute on every iteration so instead
people have come up with this idea of

00:37:41.069 --> 00:37:41.079
people have come up with this idea of
 

00:37:41.079 --> 00:37:42.839
people have come up with this idea of
stochastic gradient descent and that

00:37:42.839 --> 00:37:42.849
stochastic gradient descent and that
 

00:37:42.849 --> 00:37:44.970
stochastic gradient descent and that
involves picking a single point in your

00:37:44.970 --> 00:37:44.980
involves picking a single point in your
 

00:37:44.980 --> 00:37:47.190
involves picking a single point in your
data set computing the gradient with

00:37:47.190 --> 00:37:47.200
data set computing the gradient with
 

00:37:47.200 --> 00:37:49.799
data set computing the gradient with
respect to that point and then using

00:37:49.799 --> 00:37:49.809
respect to that point and then using
 

00:37:49.809 --> 00:37:51.690
respect to that point and then using
that to update your grade to update your

00:37:51.690 --> 00:37:51.700
that to update your grade to update your
 

00:37:51.700 --> 00:37:55.109
that to update your grade to update your
your weights so this is great because

00:37:55.109 --> 00:37:55.119
your weights so this is great because
 

00:37:55.119 --> 00:37:57.180
your weights so this is great because
now computing a gradient of a single

00:37:57.180 --> 00:37:57.190
now computing a gradient of a single
 

00:37:57.190 --> 00:37:58.589
now computing a gradient of a single
point is much easier than computing the

00:37:58.589 --> 00:37:58.599
point is much easier than computing the
 

00:37:58.599 --> 00:38:01.799
point is much easier than computing the
gradient over many points but at the

00:38:01.799 --> 00:38:01.809
gradient over many points but at the
 

00:38:01.809 --> 00:38:03.180
gradient over many points but at the
same time since we're only looking at

00:38:03.180 --> 00:38:03.190
same time since we're only looking at
 

00:38:03.190 --> 00:38:05.329
same time since we're only looking at
one point this can be extremely noisy

00:38:05.329 --> 00:38:05.339
one point this can be extremely noisy
 

00:38:05.339 --> 00:38:07.740
one point this can be extremely noisy
sure we take a different point each time

00:38:07.740 --> 00:38:07.750
sure we take a different point each time
 

00:38:07.750 --> 00:38:10.410
sure we take a different point each time
but still when we move and we take a

00:38:10.410 --> 00:38:10.420
but still when we move and we take a
 

00:38:10.420 --> 00:38:12.930
but still when we move and we take a
step in that direction of that point we

00:38:12.930 --> 00:38:12.940
step in that direction of that point we
 

00:38:12.940 --> 00:38:14.849
step in that direction of that point we
may be going in in a step that's not

00:38:14.849 --> 00:38:14.859
may be going in in a step that's not
 

00:38:14.859 --> 00:38:16.650
may be going in in a step that's not
necessarily representative of the entire

00:38:16.650 --> 00:38:16.660
necessarily representative of the entire
 

00:38:16.660 --> 00:38:21.900
necessarily representative of the entire
data set so is there a middle ground

00:38:21.900 --> 00:38:21.910
data set so is there a middle ground
 

00:38:21.910 --> 00:38:23.640
data set so is there a middle ground
such that we don't have to have a

00:38:23.640 --> 00:38:23.650
such that we don't have to have a
 

00:38:23.650 --> 00:38:27.839
such that we don't have to have a
stochastic a stochastic gradient but we

00:38:27.839 --> 00:38:27.849
stochastic a stochastic gradient but we
 

00:38:27.849 --> 00:38:29.400
stochastic a stochastic gradient but we
can still be kind of computationally

00:38:29.400 --> 00:38:29.410
can still be kind of computationally
 

00:38:29.410 --> 00:38:32.940
can still be kind of computationally
efficient in the sense so instead of

00:38:32.940 --> 00:38:32.950
efficient in the sense so instead of
 

00:38:32.950 --> 00:38:34.980
efficient in the sense so instead of
computing a noisy gradient of a single

00:38:34.980 --> 00:38:34.990
computing a noisy gradient of a single
 

00:38:34.990 --> 00:38:36.900
computing a noisy gradient of a single
point let's get a better estimate by

00:38:36.900 --> 00:38:36.910
point let's get a better estimate by
 

00:38:36.910 --> 00:38:40.410
point let's get a better estimate by
batching our data into mini batches of B

00:38:40.410 --> 00:38:40.420
batching our data into mini batches of B
 

00:38:40.420 --> 00:38:43.170
batching our data into mini batches of B
data points capital B data points so now

00:38:43.170 --> 00:38:43.180
data points capital B data points so now
 

00:38:43.180 --> 00:38:45.030
data points capital B data points so now
this gives us an estimate of the true

00:38:45.030 --> 00:38:45.040
this gives us an estimate of the true
 

00:38:45.040 --> 00:38:47.520
this gives us an estimate of the true
gradient by just averaging the gradient

00:38:47.520 --> 00:38:47.530
gradient by just averaging the gradient
 

00:38:47.530 --> 00:38:50.910
gradient by just averaging the gradient
from each of these points this is great

00:38:50.910 --> 00:38:50.920
from each of these points this is great
 

00:38:50.920 --> 00:38:53.309
from each of these points this is great
because now it's much easier to compute

00:38:53.309 --> 00:38:53.319
because now it's much easier to compute
 

00:38:53.319 --> 00:38:56.640
because now it's much easier to compute
than full gradient descent it's a lot

00:38:56.640 --> 00:38:56.650
than full gradient descent it's a lot
 

00:38:56.650 --> 00:38:58.440
than full gradient descent it's a lot
less points typically B is on the order

00:38:58.440 --> 00:38:58.450
less points typically B is on the order
 

00:38:58.450 --> 00:39:01.559
less points typically B is on the order
of less than 100 or approximately in

00:39:01.559 --> 00:39:01.569
of less than 100 or approximately in
 

00:39:01.569 --> 00:39:04.980
of less than 100 or approximately in
that range and it's a lot more accurate

00:39:04.980 --> 00:39:04.990
that range and it's a lot more accurate
 

00:39:04.990 --> 00:39:06.690
that range and it's a lot more accurate
than stochastic gradient descent because

00:39:06.690 --> 00:39:06.700
than stochastic gradient descent because
 

00:39:06.700 --> 00:39:08.069
than stochastic gradient descent because
you're considering a larger population

00:39:08.069 --> 00:39:08.079
you're considering a larger population
 

00:39:08.079 --> 00:39:12.329
you're considering a larger population
as well this increase in gradient

00:39:12.329 --> 00:39:12.339
as well this increase in gradient
 

00:39:12.339 --> 00:39:14.670
as well this increase in gradient
accuracy estimation actually allows us

00:39:14.670 --> 00:39:14.680
accuracy estimation actually allows us
 

00:39:14.680 --> 00:39:16.500
accuracy estimation actually allows us
to converge much quicker as well because

00:39:16.500 --> 00:39:16.510
to converge much quicker as well because
 

00:39:16.510 --> 00:39:17.609
to converge much quicker as well because
it means that we can increase our

00:39:17.609 --> 00:39:17.619
it means that we can increase our
 

00:39:17.619 --> 00:39:18.980
it means that we can increase our
learning rate and trust our gradient

00:39:18.980 --> 00:39:18.990
learning rate and trust our gradient
 

00:39:18.990 --> 00:39:22.380
learning rate and trust our gradient
more with each step which ultimately

00:39:22.380 --> 00:39:22.390
more with each step which ultimately
 

00:39:22.390 --> 00:39:25.470
more with each step which ultimately
means that we can train faster this

00:39:25.470 --> 00:39:25.480
means that we can train faster this
 

00:39:25.480 --> 00:39:27.210
means that we can train faster this
allows for massively parallel lyza

00:39:27.210 --> 00:39:27.220
allows for massively parallel lyza
 

00:39:27.220 --> 00:39:28.980
allows for massively parallel lyza
become potations because we can split up

00:39:28.980 --> 00:39:28.990
become potations because we can split up
 

00:39:28.990 --> 00:39:32.160
become potations because we can split up
batches across the GPU send batches all

00:39:32.160 --> 00:39:32.170
batches across the GPU send batches all
 

00:39:32.170 --> 00:39:33.990
batches across the GPU send batches all
over the GPU compute their gradients

00:39:33.990 --> 00:39:34.000
over the GPU compute their gradients
 

00:39:34.000 --> 00:39:35.880
over the GPU compute their gradients
simultaneously and then aggregate them

00:39:35.880 --> 00:39:35.890
simultaneously and then aggregate them
 

00:39:35.890 --> 00:39:36.550
simultaneously and then aggregate them
back

00:39:36.550 --> 00:39:36.560
back
 

00:39:36.560 --> 00:39:40.570
back
to even speed up even further now the

00:39:40.570 --> 00:39:40.580
to even speed up even further now the
 

00:39:40.580 --> 00:39:42.340
to even speed up even further now the
last topic I want to address before

00:39:42.340 --> 00:39:42.350
last topic I want to address before
 

00:39:42.350 --> 00:39:45.640
last topic I want to address before
ending is this idea of overfitting this

00:39:45.640 --> 00:39:45.650
ending is this idea of overfitting this
 

00:39:45.650 --> 00:39:49.030
ending is this idea of overfitting this
is one of the most fundamental problems

00:39:49.030 --> 00:39:49.040
is one of the most fundamental problems
 

00:39:49.040 --> 00:39:51.460
is one of the most fundamental problems
in machine learning as a whole not just

00:39:51.460 --> 00:39:51.470
in machine learning as a whole not just
 

00:39:51.470 --> 00:39:55.390
in machine learning as a whole not just
deep learning and at its core it

00:39:55.390 --> 00:39:55.400
deep learning and at its core it
 

00:39:55.400 --> 00:39:58.990
deep learning and at its core it
involves understanding the complexity of

00:39:58.990 --> 00:39:59.000
involves understanding the complexity of
 

00:39:59.000 --> 00:40:00.790
involves understanding the complexity of
your model so you want to build a model

00:40:00.790 --> 00:40:00.800
your model so you want to build a model
 

00:40:00.800 --> 00:40:02.800
your model so you want to build a model
that performs well and generalized as

00:40:02.800 --> 00:40:02.810
that performs well and generalized as
 

00:40:02.810 --> 00:40:04.780
that performs well and generalized as
well not just to your training set but

00:40:04.780 --> 00:40:04.790
well not just to your training set but
 

00:40:04.790 --> 00:40:09.010
well not just to your training set but
to your test set as well assume that you

00:40:09.010 --> 00:40:09.020
to your test set as well assume that you
 

00:40:09.020 --> 00:40:10.180
to your test set as well assume that you
want to build a model that describes

00:40:10.180 --> 00:40:10.190
want to build a model that describes
 

00:40:10.190 --> 00:40:13.120
want to build a model that describes
these points you can go on the left-hand

00:40:13.120 --> 00:40:13.130
these points you can go on the left-hand
 

00:40:13.130 --> 00:40:15.220
these points you can go on the left-hand
side which is just a line fitting a line

00:40:15.220 --> 00:40:15.230
side which is just a line fitting a line
 

00:40:15.230 --> 00:40:16.870
side which is just a line fitting a line
through these points this is under

00:40:16.870 --> 00:40:16.880
through these points this is under
 

00:40:16.880 --> 00:40:19.420
through these points this is under
fitting the complexity of your model is

00:40:19.420 --> 00:40:19.430
fitting the complexity of your model is
 

00:40:19.430 --> 00:40:21.610
fitting the complexity of your model is
not large enough to really learn the

00:40:21.610 --> 00:40:21.620
not large enough to really learn the
 

00:40:21.620 --> 00:40:24.160
not large enough to really learn the
full complexity of the data or you can

00:40:24.160 --> 00:40:24.170
full complexity of the data or you can
 

00:40:24.170 --> 00:40:25.510
full complexity of the data or you can
go on the right-hand side which is

00:40:25.510 --> 00:40:25.520
go on the right-hand side which is
 

00:40:25.520 --> 00:40:27.520
go on the right-hand side which is
overfitting where you're essentially

00:40:27.520 --> 00:40:27.530
overfitting where you're essentially
 

00:40:27.530 --> 00:40:30.130
overfitting where you're essentially
building a very complex model to

00:40:30.130 --> 00:40:30.140
building a very complex model to
 

00:40:30.140 --> 00:40:32.110
building a very complex model to
essentially memorize the data and this

00:40:32.110 --> 00:40:32.120
essentially memorize the data and this
 

00:40:32.120 --> 00:40:33.370
essentially memorize the data and this
is not useful either because when you

00:40:33.370 --> 00:40:33.380
is not useful either because when you
 

00:40:33.380 --> 00:40:36.040
is not useful either because when you
show a new data it's not going to sense

00:40:36.040 --> 00:40:36.050
show a new data it's not going to sense
 

00:40:36.050 --> 00:40:38.410
show a new data it's not going to sense
it's not going to perfectly match on the

00:40:38.410 --> 00:40:38.420
it's not going to perfectly match on the
 

00:40:38.420 --> 00:40:39.910
it's not going to perfectly match on the
training data and it means that you're

00:40:39.910 --> 00:40:39.920
training data and it means that you're
 

00:40:39.920 --> 00:40:42.420
training data and it means that you're
going to have high generalization error

00:40:42.420 --> 00:40:42.430
going to have high generalization error
 

00:40:42.430 --> 00:40:44.800
going to have high generalization error
ideally we want to end up with a model

00:40:44.800 --> 00:40:44.810
ideally we want to end up with a model
 

00:40:44.810 --> 00:40:46.960
ideally we want to end up with a model
in the middle that is not too complex to

00:40:46.960 --> 00:40:46.970
in the middle that is not too complex to
 

00:40:46.970 --> 00:40:48.520
in the middle that is not too complex to
memorize all of our training data but

00:40:48.520 --> 00:40:48.530
memorize all of our training data but
 

00:40:48.530 --> 00:40:51.130
memorize all of our training data but
still able to generalize and perform

00:40:51.130 --> 00:40:51.140
still able to generalize and perform
 

00:40:51.140 --> 00:40:53.350
still able to generalize and perform
well even we have when we have brand new

00:40:53.350 --> 00:40:53.360
well even we have when we have brand new
 

00:40:53.360 --> 00:40:57.100
well even we have when we have brand new
training and testing inputs so to

00:40:57.100 --> 00:40:57.110
training and testing inputs so to
 

00:40:57.110 --> 00:40:58.960
training and testing inputs so to
address this problem let's talk about

00:40:58.960 --> 00:40:58.970
address this problem let's talk about
 

00:40:58.970 --> 00:41:01.030
address this problem let's talk about
regularization for deep neural networks

00:41:01.030 --> 00:41:01.040
regularization for deep neural networks
 

00:41:01.040 --> 00:41:03.160
regularization for deep neural networks
deep neural regularization is a

00:41:03.160 --> 00:41:03.170
deep neural regularization is a
 

00:41:03.170 --> 00:41:05.560
deep neural regularization is a
technique that you can introduce to your

00:41:05.560 --> 00:41:05.570
technique that you can introduce to your
 

00:41:05.570 --> 00:41:08.170
technique that you can introduce to your
networks that will discourage complex

00:41:08.170 --> 00:41:08.180
networks that will discourage complex
 

00:41:08.180 --> 00:41:12.880
networks that will discourage complex
models from being learned and as before

00:41:12.880 --> 00:41:12.890
models from being learned and as before
 

00:41:12.890 --> 00:41:14.980
models from being learned and as before
we've seen that it's crucial for our

00:41:14.980 --> 00:41:14.990
we've seen that it's crucial for our
 

00:41:14.990 --> 00:41:18.490
we've seen that it's crucial for our
models to be able to generalize to data

00:41:18.490 --> 00:41:18.500
models to be able to generalize to data
 

00:41:18.500 --> 00:41:20.500
models to be able to generalize to data
beyond our training set but also to

00:41:20.500 --> 00:41:20.510
beyond our training set but also to
 

00:41:20.510 --> 00:41:22.990
beyond our training set but also to
generate generalize to data in our

00:41:22.990 --> 00:41:23.000
generate generalize to data in our
 

00:41:23.000 --> 00:41:26.620
generate generalize to data in our
testing set as well the most popular

00:41:26.620 --> 00:41:26.630
testing set as well the most popular
 

00:41:26.630 --> 00:41:28.330
testing set as well the most popular
regularization technique in deep

00:41:28.330 --> 00:41:28.340
regularization technique in deep
 

00:41:28.340 --> 00:41:30.160
regularization technique in deep
learning is a very simple idea called

00:41:30.160 --> 00:41:30.170
learning is a very simple idea called
 

00:41:30.170 --> 00:41:32.800
learning is a very simple idea called
dropout let's revisit this and a picture

00:41:32.800 --> 00:41:32.810
dropout let's revisit this and a picture
 

00:41:32.810 --> 00:41:35.740
dropout let's revisit this and a picture
of a deep neural network again and drop

00:41:35.740 --> 00:41:35.750
of a deep neural network again and drop
 

00:41:35.750 --> 00:41:38.170
of a deep neural network again and drop
out during training we randomly set some

00:41:38.170 --> 00:41:38.180
out during training we randomly set some
 

00:41:38.180 --> 00:41:40.090
out during training we randomly set some
of our activations of the hidden neurons

00:41:40.090 --> 00:41:40.100
of our activations of the hidden neurons
 

00:41:40.100 --> 00:41:43.360
of our activations of the hidden neurons
to 0 with some probability that's why we

00:41:43.360 --> 00:41:43.370
to 0 with some probability that's why we
 

00:41:43.370 --> 00:41:44.350
to 0 with some probability that's why we
call it dropping out because we're

00:41:44.350 --> 00:41:44.360
call it dropping out because we're
 

00:41:44.360 --> 00:41:46.810
call it dropping out because we're
essentially killing off those neurons so

00:41:46.810 --> 00:41:46.820
essentially killing off those neurons so
 

00:41:46.820 --> 00:41:48.130
essentially killing off those neurons so
let's do that so we kill off these

00:41:48.130 --> 00:41:48.140
let's do that so we kill off these
 

00:41:48.140 --> 00:41:49.930
let's do that so we kill off these
random sample of neurons

00:41:49.930 --> 00:41:49.940
random sample of neurons
 

00:41:49.940 --> 00:41:51.190
random sample of neurons
and now we've created a different

00:41:51.190 --> 00:41:51.200
and now we've created a different
 

00:41:51.200 --> 00:41:53.680
and now we've created a different
pathway through the network let's say

00:41:53.680 --> 00:41:53.690
pathway through the network let's say
 

00:41:53.690 --> 00:41:55.029
pathway through the network let's say
that you dropped 50 percent of the

00:41:55.029 --> 00:41:55.039
that you dropped 50 percent of the
 

00:41:55.039 --> 00:41:57.010
that you dropped 50 percent of the
neurons this means that those

00:41:57.010 --> 00:41:57.020
neurons this means that those
 

00:41:57.020 --> 00:41:59.620
neurons this means that those
activations are set to zero and the

00:41:59.620 --> 00:41:59.630
activations are set to zero and the
 

00:41:59.630 --> 00:42:01.539
activations are set to zero and the
network is not going to rely too heavily

00:42:01.539 --> 00:42:01.549
network is not going to rely too heavily
 

00:42:01.549 --> 00:42:03.640
network is not going to rely too heavily
on any particular path through the

00:42:03.640 --> 00:42:03.650
on any particular path through the
 

00:42:03.650 --> 00:42:05.380
on any particular path through the
network but it's instead going to find a

00:42:05.380 --> 00:42:05.390
network but it's instead going to find a
 

00:42:05.390 --> 00:42:07.120
network but it's instead going to find a
whole ensemble of different paths

00:42:07.120 --> 00:42:07.130
whole ensemble of different paths
 

00:42:07.130 --> 00:42:08.799
whole ensemble of different paths
because it doesn't know which path is

00:42:08.799 --> 00:42:08.809
because it doesn't know which path is
 

00:42:08.809 --> 00:42:10.120
because it doesn't know which path is
going to be dropped out at any given

00:42:10.120 --> 00:42:10.130
going to be dropped out at any given
 

00:42:10.130 --> 00:42:13.930
going to be dropped out at any given
time we repeat this process on every

00:42:13.930 --> 00:42:13.940
time we repeat this process on every
 

00:42:13.940 --> 00:42:15.430
time we repeat this process on every
training iteration now dropping out a

00:42:15.430 --> 00:42:15.440
training iteration now dropping out a
 

00:42:15.440 --> 00:42:20.400
training iteration now dropping out a
new set of 50 50 % of the neurons and

00:42:20.400 --> 00:42:20.410
new set of 50 50 % of the neurons and
 

00:42:20.410 --> 00:42:22.750
new set of 50 50 % of the neurons and
the result of this is essentially a

00:42:22.750 --> 00:42:22.760
the result of this is essentially a
 

00:42:22.760 --> 00:42:25.359
the result of this is essentially a
model that like I said creates an

00:42:25.359 --> 00:42:25.369
model that like I said creates an
 

00:42:25.369 --> 00:42:27.849
model that like I said creates an
ensemble of multiple models through the

00:42:27.849 --> 00:42:27.859
ensemble of multiple models through the
 

00:42:27.859 --> 00:42:29.559
ensemble of multiple models through the
paths of the network and is able to

00:42:29.559 --> 00:42:29.569
paths of the network and is able to
 

00:42:29.569 --> 00:42:35.740
paths of the network and is able to
generalize better to unseen test data so

00:42:35.740 --> 00:42:35.750
generalize better to unseen test data so
 

00:42:35.750 --> 00:42:36.849
generalize better to unseen test data so
the second technique for a

00:42:36.849 --> 00:42:36.859
the second technique for a
 

00:42:36.859 --> 00:42:41.260
the second technique for a
regularization is this notion that we'll

00:42:41.260 --> 00:42:41.270
regularization is this notion that we'll
 

00:42:41.270 --> 00:42:44.160
regularization is this notion that we'll
talk about which is early stopping and

00:42:44.160 --> 00:42:44.170
talk about which is early stopping and
 

00:42:44.170 --> 00:42:46.990
talk about which is early stopping and
the idea here is also extremely simple

00:42:46.990 --> 00:42:47.000
the idea here is also extremely simple
 

00:42:47.000 --> 00:42:48.460
the idea here is also extremely simple
let's train our neural network like

00:42:48.460 --> 00:42:48.470
let's train our neural network like
 

00:42:48.470 --> 00:42:51.099
let's train our neural network like
before no dropout but let's just stop

00:42:51.099 --> 00:42:51.109
before no dropout but let's just stop
 

00:42:51.109 --> 00:42:52.630
before no dropout but let's just stop
training before we have a chance to

00:42:52.630 --> 00:42:52.640
training before we have a chance to
 

00:42:52.640 --> 00:42:56.319
training before we have a chance to
overfit so we start training and the

00:42:56.319 --> 00:42:56.329
overfit so we start training and the
 

00:42:56.329 --> 00:42:57.730
overfit so we start training and the
definition of overfitting is just when

00:42:57.730 --> 00:42:57.740
definition of overfitting is just when
 

00:42:57.740 --> 00:42:59.769
definition of overfitting is just when
our model starts to perform worse on the

00:42:59.769 --> 00:42:59.779
our model starts to perform worse on the
 

00:42:59.779 --> 00:43:04.269
our model starts to perform worse on the
test set then on the training set so we

00:43:04.269 --> 00:43:04.279
test set then on the training set so we
 

00:43:04.279 --> 00:43:05.829
test set then on the training set so we
can start off and we can plot how our

00:43:05.829 --> 00:43:05.839
can start off and we can plot how our
 

00:43:05.839 --> 00:43:07.450
can start off and we can plot how our
loss is going for both the training and

00:43:07.450 --> 00:43:07.460
loss is going for both the training and
 

00:43:07.460 --> 00:43:09.010
loss is going for both the training and
test set we can see that both are

00:43:09.010 --> 00:43:09.020
test set we can see that both are
 

00:43:09.020 --> 00:43:11.140
test set we can see that both are
decreasing so we keep training now we

00:43:11.140 --> 00:43:11.150
decreasing so we keep training now we
 

00:43:11.150 --> 00:43:13.750
decreasing so we keep training now we
can see that the training the validation

00:43:13.750 --> 00:43:13.760
can see that the training the validation
 

00:43:13.760 --> 00:43:14.890
can see that the training the validation
both losses are kind of starting to

00:43:14.890 --> 00:43:14.900
both losses are kind of starting to
 

00:43:14.900 --> 00:43:17.349
both losses are kind of starting to
plateau here we can keep going the

00:43:17.349 --> 00:43:17.359
plateau here we can keep going the
 

00:43:17.359 --> 00:43:19.210
plateau here we can keep going the
training loss is always going to decay

00:43:19.210 --> 00:43:19.220
training loss is always going to decay
 

00:43:19.220 --> 00:43:20.440
training loss is always going to decay
it's always going to keep decreasing

00:43:20.440 --> 00:43:20.450
it's always going to keep decreasing
 

00:43:20.450 --> 00:43:22.660
it's always going to keep decreasing
because especially if you have a network

00:43:22.660 --> 00:43:22.670
because especially if you have a network
 

00:43:22.670 --> 00:43:26.049
because especially if you have a network
that is having such a large capacity to

00:43:26.049 --> 00:43:26.059
that is having such a large capacity to
 

00:43:26.059 --> 00:43:28.450
that is having such a large capacity to
essentially memorize your data you can

00:43:28.450 --> 00:43:28.460
essentially memorize your data you can
 

00:43:28.460 --> 00:43:30.700
essentially memorize your data you can
always perfectly get a training accuracy

00:43:30.700 --> 00:43:30.710
always perfectly get a training accuracy
 

00:43:30.710 --> 00:43:33.130
always perfectly get a training accuracy
of 0 that's not always the case but in a

00:43:33.130 --> 00:43:33.140
of 0 that's not always the case but in a
 

00:43:33.140 --> 00:43:34.359
of 0 that's not always the case but in a
lot of times with deep neural networks

00:43:34.359 --> 00:43:34.369
lot of times with deep neural networks
 

00:43:34.369 --> 00:43:37.269
lot of times with deep neural networks
since they're so expressive and have so

00:43:37.269 --> 00:43:37.279
since they're so expressive and have so
 

00:43:37.279 --> 00:43:39.039
since they're so expressive and have so
many weights they're able to actually

00:43:39.039 --> 00:43:39.049
many weights they're able to actually
 

00:43:39.049 --> 00:43:41.049
many weights they're able to actually
memorize the data if you let them train

00:43:41.049 --> 00:43:41.059
memorize the data if you let them train
 

00:43:41.059 --> 00:43:44.049
memorize the data if you let them train
for too long if we keep training like

00:43:44.049 --> 00:43:44.059
for too long if we keep training like
 

00:43:44.059 --> 00:43:46.630
for too long if we keep training like
you see the training set continues to

00:43:46.630 --> 00:43:46.640
you see the training set continues to
 

00:43:46.640 --> 00:43:48.789
you see the training set continues to
decrease now the validation set starts

00:43:48.789 --> 00:43:48.799
decrease now the validation set starts
 

00:43:48.799 --> 00:43:53.109
decrease now the validation set starts
to increase and if we keep doing this

00:43:53.109 --> 00:43:53.119
to increase and if we keep doing this
 

00:43:53.119 --> 00:43:56.349
to increase and if we keep doing this
the trend continues the idea of early

00:43:56.349 --> 00:43:56.359
the trend continues the idea of early
 

00:43:56.359 --> 00:43:57.970
the trend continues the idea of early
stopping is essentially that we want to

00:43:57.970 --> 00:43:57.980
stopping is essentially that we want to
 

00:43:57.980 --> 00:44:00.250
stopping is essentially that we want to
focus on this point here and stop

00:44:00.250 --> 00:44:00.260
focus on this point here and stop
 

00:44:00.260 --> 00:44:03.220
focus on this point here and stop
training when we reach this point so we

00:44:03.220 --> 00:44:03.230
training when we reach this point so we
 

00:44:03.230 --> 00:44:04.020
training when we reach this point so we
can key

00:44:04.020 --> 00:44:04.030
can key
 

00:44:04.030 --> 00:44:06.270
can key
basically records of the model during

00:44:06.270 --> 00:44:06.280
basically records of the model during
 

00:44:06.280 --> 00:44:09.150
basically records of the model during
training and once we start to detect

00:44:09.150 --> 00:44:09.160
training and once we start to detect
 

00:44:09.160 --> 00:44:11.339
training and once we start to detect
overfitting we can just stop and take

00:44:11.339 --> 00:44:11.349
overfitting we can just stop and take
 

00:44:11.349 --> 00:44:14.520
overfitting we can just stop and take
that last model that was still occurring

00:44:14.520 --> 00:44:14.530
that last model that was still occurring
 

00:44:14.530 --> 00:44:18.390
that last model that was still occurring
before the overfitting happened right so

00:44:18.390 --> 00:44:18.400
before the overfitting happened right so
 

00:44:18.400 --> 00:44:20.460
before the overfitting happened right so
on the left hand side you can see the

00:44:20.460 --> 00:44:20.470
on the left hand side you can see the
 

00:44:20.470 --> 00:44:22.349
on the left hand side you can see the
under fitting you don't want to stop too

00:44:22.349 --> 00:44:22.359
under fitting you don't want to stop too
 

00:44:22.359 --> 00:44:24.930
under fitting you don't want to stop too
early you want to let the model get the

00:44:24.930 --> 00:44:24.940
early you want to let the model get the
 

00:44:24.940 --> 00:44:27.359
early you want to let the model get the
minimum validation set accuracy but also

00:44:27.359 --> 00:44:27.369
minimum validation set accuracy but also
 

00:44:27.369 --> 00:44:29.849
minimum validation set accuracy but also
you don't want to keep training such

00:44:29.849 --> 00:44:29.859
you don't want to keep training such
 

00:44:29.859 --> 00:44:31.440
you don't want to keep training such
that the validation accuracy starts the

00:44:31.440 --> 00:44:31.450
that the validation accuracy starts the
 

00:44:31.450 --> 00:44:34.500
that the validation accuracy starts the
increase on the other end as well so

00:44:34.500 --> 00:44:34.510
increase on the other end as well so
 

00:44:34.510 --> 00:44:36.030
increase on the other end as well so
I'll conclude this first lecture by

00:44:36.030 --> 00:44:36.040
I'll conclude this first lecture by
 

00:44:36.040 --> 00:44:37.589
I'll conclude this first lecture by
summarizing three key points that we

00:44:37.589 --> 00:44:37.599
summarizing three key points that we
 

00:44:37.599 --> 00:44:40.200
summarizing three key points that we
have covered so far first we learned

00:44:40.200 --> 00:44:40.210
have covered so far first we learned
 

00:44:40.210 --> 00:44:42.150
have covered so far first we learned
about the fundamental building blocks of

00:44:42.150 --> 00:44:42.160
about the fundamental building blocks of
 

00:44:42.160 --> 00:44:43.890
about the fundamental building blocks of
deep learning which is just a single

00:44:43.890 --> 00:44:43.900
deep learning which is just a single
 

00:44:43.900 --> 00:44:46.560
deep learning which is just a single
neuron or called the perceptron we

00:44:46.560 --> 00:44:46.570
neuron or called the perceptron we
 

00:44:46.570 --> 00:44:48.480
neuron or called the perceptron we
learned about back propagation how to

00:44:48.480 --> 00:44:48.490
learned about back propagation how to
 

00:44:48.490 --> 00:44:51.420
learned about back propagation how to
stack these neurons into complex deep

00:44:51.420 --> 00:44:51.430
stack these neurons into complex deep
 

00:44:51.430 --> 00:44:53.190
stack these neurons into complex deep
neural networks how to back propagate

00:44:53.190 --> 00:44:53.200
neural networks how to back propagate
 

00:44:53.200 --> 00:44:55.920
neural networks how to back propagate
and errors through them and learn

00:44:55.920 --> 00:44:55.930
and errors through them and learn
 

00:44:55.930 --> 00:45:00.510
and errors through them and learn
complex loss functions and finally we

00:45:00.510 --> 00:45:00.520
complex loss functions and finally we
 

00:45:00.520 --> 00:45:02.400
complex loss functions and finally we
discussed some of the practical details

00:45:02.400 --> 00:45:02.410
discussed some of the practical details
 

00:45:02.410 --> 00:45:04.890
discussed some of the practical details
and tricks to training neural networks

00:45:04.890 --> 00:45:04.900
and tricks to training neural networks
 

00:45:04.900 --> 00:45:07.920
and tricks to training neural networks
that are really crucial today if you

00:45:07.920 --> 00:45:07.930
that are really crucial today if you
 

00:45:07.930 --> 00:45:10.079
that are really crucial today if you
want to work in this field such as

00:45:10.079 --> 00:45:10.089
want to work in this field such as
 

00:45:10.089 --> 00:45:13.520
want to work in this field such as
batching regularization and and others

00:45:13.520 --> 00:45:13.530
batching regularization and and others
 

00:45:13.530 --> 00:45:16.950
batching regularization and and others
so now I'll take any questions or if

00:45:16.950 --> 00:45:16.960
so now I'll take any questions or if
 

00:45:16.960 --> 00:45:19.079
so now I'll take any questions or if
there are no questions and I'm gonna

00:45:19.079 --> 00:45:19.089
there are no questions and I'm gonna
 

00:45:19.089 --> 00:45:20.910
there are no questions and I'm gonna
hand the mic over to ovah who will talk

00:45:20.910 --> 00:45:20.920
hand the mic over to ovah who will talk
 

00:45:20.920 --> 00:45:25.020
hand the mic over to ovah who will talk
about sequence modeling thank you

00:45:25.020 --> 00:45:25.030
about sequence modeling thank you
 

00:45:25.030 --> 00:45:29.559
about sequence modeling thank you
[Applause]

