WEBVTT
Kind: captions
Language: en

00:00:00.160 --> 00:00:00.868
TOM SIMONITE: Hi.

00:00:10.200 --> 00:00:11.870
Good morning.

00:00:11.870 --> 00:00:14.050
Welcome to day
three of Google I/O,

00:00:14.050 --> 00:00:16.850
and what should be a fun
conversation about machine

00:00:16.850 --> 00:00:18.850
learning and artificial
intelligence.

00:00:18.850 --> 00:00:19.850
My name is Tom Simonite.

00:00:19.850 --> 00:00:23.650
I'm San Francisco bureau chief
for MIT Technology Review.

00:00:23.650 --> 00:00:26.100
And like all of you, I've
been hearing a lot recently

00:00:26.100 --> 00:00:28.520
about the growing power
of machine learning.

00:00:28.520 --> 00:00:30.520
We've seen some striking
results come out

00:00:30.520 --> 00:00:33.340
of academic and
industrial research labs,

00:00:33.340 --> 00:00:36.510
and they've moved very quickly
into the hands of developers,

00:00:36.510 --> 00:00:39.380
who have been using them to
make new products and services

00:00:39.380 --> 00:00:40.700
and companies.

00:00:40.700 --> 00:00:42.600
I'm joined by three
people this morning

00:00:42.600 --> 00:00:45.520
who can tell us about
how this new technology

00:00:45.520 --> 00:00:48.880
and the capabilities it brings
are coming out into the world.

00:00:48.880 --> 00:00:51.110
They are Aparna
Chennapragada, who

00:00:51.110 --> 00:00:53.050
is the director of
product management

00:00:53.050 --> 00:00:56.470
and worked on the Google
Now mobile assistant,

00:00:56.470 --> 00:01:00.220
Jeff Dean, who leads the Google
Brain research group here

00:01:00.220 --> 00:01:02.870
in Mountain View,
and John Giannandrea,

00:01:02.870 --> 00:01:06.280
who is head of search and
machine intelligence at Google.

00:01:06.280 --> 00:01:08.560
Thanks for joining
me, all of you.

00:01:08.560 --> 00:01:10.840
We're going to talk
for about 30 minutes,

00:01:10.840 --> 00:01:15.220
and then there will be time
for questions from the floor.

00:01:15.220 --> 00:01:16.640
John, why don't
we start with you?

00:01:16.640 --> 00:01:19.580
You could set the scene for us.

00:01:19.580 --> 00:01:21.920
Artificial intelligence
and machine learning

00:01:21.920 --> 00:01:23.320
are not brand new concepts.

00:01:23.320 --> 00:01:24.920
They've been around
for a long time,

00:01:24.920 --> 00:01:27.060
but we're suddenly hearing
a lot more about them.

00:01:27.060 --> 00:01:28.537
Large companies
and small companies

00:01:28.537 --> 00:01:30.120
are investing more
in this technology,

00:01:30.120 --> 00:01:31.540
and there's a lot of excitement.

00:01:31.540 --> 00:01:33.540
You can even get a
large number of people

00:01:33.540 --> 00:01:37.040
to come to a talk about this
thing early in the morning.

00:01:37.040 --> 00:01:39.730
So what's going on?

00:01:39.730 --> 00:01:41.360
Tell these people
why they're here.

00:01:41.360 --> 00:01:41.990
JOHN GIANNANDREA:
What's going on?

00:01:41.990 --> 00:01:42.867
Yeah, thanks, Tom.

00:01:42.867 --> 00:01:44.450
I mean, I think in
the last few years,

00:01:44.450 --> 00:01:48.580
we've seen extraordinary results
in fields that hadn't really

00:01:48.580 --> 00:01:51.170
moved the needle for many
years, like speech recognition

00:01:51.170 --> 00:01:52.400
and image understanding.

00:01:52.400 --> 00:01:55.220
The error rates are just
falling dramatically,

00:01:55.220 --> 00:01:58.040
mostly because of advances
in deep neural networks,

00:01:58.040 --> 00:02:00.520
so-called deep learning.

00:02:00.520 --> 00:02:03.280
I think these
techniques are not new.

00:02:03.280 --> 00:02:06.220
People have been using neural
networks for many, many years.

00:02:06.220 --> 00:02:09.400
But a combination of events
over the last few years

00:02:09.400 --> 00:02:11.210
has made them much
more effective,

00:02:11.210 --> 00:02:14.035
and caused us to invest
a lot in getting them

00:02:14.035 --> 00:02:17.336
into the hands of developers.

00:02:17.336 --> 00:02:19.825
People talk about it
in terms of AI winters,

00:02:19.825 --> 00:02:20.700
and things like this.

00:02:20.700 --> 00:02:23.680
I think we're kind of
an AI spring right now.

00:02:23.680 --> 00:02:25.440
We're just seeing
remarkable progress

00:02:25.440 --> 00:02:26.852
across a huge number of fields.

00:02:26.852 --> 00:02:27.560
TOM SIMONITE: OK.

00:02:27.560 --> 00:02:28.934
And now, how long
have you worked

00:02:28.934 --> 00:02:30.760
in artificial
intelligence, John?

00:02:30.760 --> 00:02:31.550
JOHN GIANNANDREA:
Well, we started

00:02:31.550 --> 00:02:33.950
investing heavily in this at
Google about four years ago.

00:02:33.950 --> 00:02:35.741
I mean, we've been
working in these fields,

00:02:35.741 --> 00:02:38.450
like speech recognition,
for over a decade.

00:02:38.450 --> 00:02:40.920
But we kind of got serious
about our investments

00:02:40.920 --> 00:02:44.290
about four years ago,
and getting organized

00:02:44.290 --> 00:02:46.222
to do things that
ultimately resulted

00:02:46.222 --> 00:02:48.180
in the release of things
like TensorFlow, which

00:02:48.180 --> 00:02:49.282
Jeff's team's worked on.

00:02:49.282 --> 00:02:49.990
TOM SIMONITE: OK.

00:02:49.990 --> 00:02:52.085
And we'll talk more about
that later, I'm sure.

00:02:52.085 --> 00:02:56.607
Aparna, give us a perspective
from the view of someone

00:02:56.607 --> 00:02:57.440
who builds products.

00:02:57.440 --> 00:02:59.770
So John says this
technology has suddenly

00:02:59.770 --> 00:03:03.240
become more powerful
and accurate and useful.

00:03:03.240 --> 00:03:05.024
Does that open up
new horizons for you,

00:03:05.024 --> 00:03:06.940
when you're thinking
about what you can build?

00:03:06.940 --> 00:03:08.565
APARNA CHENNAPRAGADA:
Yeah, absolutely.

00:03:08.565 --> 00:03:12.080
I think for me, these are
great as a technology.

00:03:12.080 --> 00:03:13.470
But as a means to
an end, they're

00:03:13.470 --> 00:03:17.640
powerful tool kits to help
solve real problems, right?

00:03:17.640 --> 00:03:20.340
And for us, as building
products, and for you guys,

00:03:20.340 --> 00:03:22.990
too, there's two ways
that machine learning

00:03:22.990 --> 00:03:24.110
changes the game.

00:03:24.110 --> 00:03:27.440
One is that it can turbo charge
existing use cases-- that

00:03:27.440 --> 00:03:30.380
is, existing problems
like speech recognition--

00:03:30.380 --> 00:03:33.270
by dramatically changing
some technical components

00:03:33.270 --> 00:03:34.540
that power the product.

00:03:34.540 --> 00:03:37.480
If you're building a voice
enabled assistant, the word

00:03:37.480 --> 00:03:40.280
error rate that John was talking
about, as soon as it dropped,

00:03:40.280 --> 00:03:42.270
we actually saw the usage go up.

00:03:42.270 --> 00:03:46.010
So the product gets more usable
as machine learning improves

00:03:46.010 --> 00:03:47.260
the underlying engine.

00:03:47.260 --> 00:03:48.870
Same thing with translation.

00:03:48.870 --> 00:03:51.780
As translation gets
better, Google Translate,

00:03:51.780 --> 00:03:54.330
it scales to 100-plus languages.

00:03:54.330 --> 00:03:55.695
And photos is a great example.

00:03:55.695 --> 00:03:57.320
You've heard Sundar
talk about it, too,

00:03:57.320 --> 00:04:00.110
that as soon as you have
better image understanding,

00:04:00.110 --> 00:04:02.350
the photo labeling gets
better, and therefore, I

00:04:02.350 --> 00:04:03.450
can organize my photos.

00:04:03.450 --> 00:04:04.930
So it's a means to an end.

00:04:04.930 --> 00:04:06.969
That's one way, certainly,
that we have seen.

00:04:06.969 --> 00:04:09.260
But I think the second way
that's, personally, far more

00:04:09.260 --> 00:04:14.134
exciting to see is where it can
unlock new product use cases.

00:04:14.134 --> 00:04:17.170
So turbocharging existing
use cases is one thing,

00:04:17.170 --> 00:04:19.290
but where can you
kind of see problems

00:04:19.290 --> 00:04:22.960
that really weren't thought
of as AI or data problems?

00:04:22.960 --> 00:04:26.730
And thanks to mobile, here--
3 billion phones-- a lot

00:04:26.730 --> 00:04:29.195
of the real world problems
are turning into AI problems,

00:04:29.195 --> 00:04:29.720
right?

00:04:29.720 --> 00:04:31.570
Transportation,
health, and so on.

00:04:31.570 --> 00:04:32.822
That's pretty exciting, too.

00:04:32.822 --> 00:04:33.530
TOM SIMONITE: OK.

00:04:33.530 --> 00:04:35.790
And so is one
consequence of this

00:04:35.790 --> 00:04:38.390
that we can make computers
less annoying, do you think?

00:04:38.390 --> 00:04:40.050
I mean, that would be nice.

00:04:40.050 --> 00:04:41.700
We'd all had these
experiences where

00:04:41.700 --> 00:04:44.616
you have a very clear idea of
what it is you're trying to do,

00:04:44.616 --> 00:04:46.240
but it feels like
the software is doing

00:04:46.240 --> 00:04:47.910
everything it can to stop you.

00:04:47.910 --> 00:04:50.076
Maybe that's a form of
artificial intelligence, too.

00:04:50.076 --> 00:04:50.640
I don't know.

00:04:50.640 --> 00:04:53.070
But can you make more
seamless experiences

00:04:53.070 --> 00:04:55.435
that just make life easier?

00:04:55.435 --> 00:04:56.560
APARNA CHENNAPRAGADA: Yeah.

00:04:56.560 --> 00:04:59.120
And I think in this case,
again, one of the things

00:04:59.120 --> 00:05:01.670
to think about is, how do
you make sure-- especially

00:05:01.670 --> 00:05:03.170
as you build
products-- how do you

00:05:03.170 --> 00:05:06.590
make sure your interface
scales with the intelligence?

00:05:06.590 --> 00:05:09.100
The UI needs to be
proportional to AI.

00:05:09.100 --> 00:05:12.105
I cannot believe I said some
pseudo formula in front of Jeff

00:05:12.105 --> 00:05:14.050
Dean.

00:05:14.050 --> 00:05:15.820
But I think that's
really important,

00:05:15.820 --> 00:05:18.230
to make sure that the
UI scales with the AI.

00:05:18.230 --> 00:05:19.160
TOM SIMONITE: OK.

00:05:19.160 --> 00:05:23.910
And Jeff, for
people like Aparna,

00:05:23.910 --> 00:05:26.110
building products,
to do that, we

00:05:26.110 --> 00:05:27.590
need this kind of
translation step

00:05:27.590 --> 00:05:29.156
which your group is working on.

00:05:29.156 --> 00:05:30.730
So Google Brain is
a research group.

00:05:30.730 --> 00:05:33.240
Works in some very fundamental
questions in its field.

00:05:33.240 --> 00:05:36.450
But you also build
this infrastructure,

00:05:36.450 --> 00:05:38.790
which you're kind of inventing
from scratch, that makes

00:05:38.790 --> 00:05:41.534
it possible to use this stuff.

00:05:41.534 --> 00:05:42.200
JEFF DEAN: Yeah.

00:05:42.200 --> 00:05:44.520
I mean, I think,
obviously, in order

00:05:44.520 --> 00:05:46.810
to make progress on
these kinds of problems,

00:05:46.810 --> 00:05:50.780
it's really important to be
able to try lots of experiments

00:05:50.780 --> 00:05:52.760
and do that as
quickly as you can.

00:05:52.760 --> 00:05:55.230
There's a very
fundamental difference

00:05:55.230 --> 00:05:58.157
between having an
experiment take a few hours,

00:05:58.157 --> 00:05:59.740
versus something
that takes six weeks.

00:05:59.740 --> 00:06:03.110
It's just a very different
model of doing science.

00:06:03.110 --> 00:06:06.200
And so, one of the
things we work on

00:06:06.200 --> 00:06:10.380
is trying to build really
scalable systems that are also

00:06:10.380 --> 00:06:13.690
flexible and easy to express
new kinds of machine learning

00:06:13.690 --> 00:06:14.640
ideas.

00:06:14.640 --> 00:06:16.550
So that's how
TensorFlow came about.

00:06:16.550 --> 00:06:19.290
It's sort of our internal
research vehicle,

00:06:19.290 --> 00:06:23.377
but also robust enough to take
something you've done and done

00:06:23.377 --> 00:06:25.710
lots of experiments on, and
then, when you get something

00:06:25.710 --> 00:06:28.240
that works well, to take that
and move it into a production

00:06:28.240 --> 00:06:31.250
environment, run things
on phones or in data

00:06:31.250 --> 00:06:36.940
centers, on RTPUs, that we
announced a couple days ago.

00:06:36.940 --> 00:06:39.800
And that seamless
transition from research

00:06:39.800 --> 00:06:41.980
to putting things
into real products

00:06:41.980 --> 00:06:43.372
is what we're all about.

00:06:43.372 --> 00:06:44.080
TOM SIMONITE: OK.

00:06:44.080 --> 00:06:48.010
And so, TensorFlow is this
very flexible package.

00:06:48.010 --> 00:06:49.420
It's very valuable to Google.

00:06:49.420 --> 00:06:51.294
You're building a lot
of things on top of it.

00:06:51.294 --> 00:06:52.900
But you're giving
it away for free.

00:06:52.900 --> 00:06:54.149
Have you thought this through?

00:06:54.149 --> 00:06:56.850
Isn't this something you
should be keeping closely held?

00:06:56.850 --> 00:06:57.800
JEFF DEAN: Yeah.

00:06:57.800 --> 00:07:00.008
There was actually a little
bit of debate internally.

00:07:00.008 --> 00:07:02.840
But I think we decided
to open source it,

00:07:02.840 --> 00:07:05.509
and it's got a nice Apache
2.0 license which basically

00:07:05.509 --> 00:07:07.550
means you can take it and
do pretty much whatever

00:07:07.550 --> 00:07:09.590
you want with it.

00:07:09.590 --> 00:07:14.130
And the reason we did
that is several fold.

00:07:14.130 --> 00:07:18.720
One is, we think it's a really
good way of making research

00:07:18.720 --> 00:07:20.860
ideas and machine learning
propagate more quickly

00:07:20.860 --> 00:07:22.560
throughout the community.

00:07:22.560 --> 00:07:26.130
People can publish
something they've done,

00:07:26.130 --> 00:07:27.930
and people can
pick up that thing

00:07:27.930 --> 00:07:30.850
and reproduce those people's
results or build on them.

00:07:30.850 --> 00:07:33.160
And if you look
on GitHub, there's

00:07:33.160 --> 00:07:36.110
like 1,500 repositories,
now, that mention TensorFlow,

00:07:36.110 --> 00:07:38.360
and only five of
them are from Google.

00:07:38.360 --> 00:07:41.452
And so, it's people doing all
kinds of stuff with TensorFlow.

00:07:41.452 --> 00:07:43.785
And I think that free exchange
of ideas and accelerating

00:07:43.785 --> 00:07:47.010
of that is one of the
main reasons we did that.

00:07:47.010 --> 00:07:47.810
TOM SIMONITE: OK.

00:07:47.810 --> 00:07:49.790
And where is this going?

00:07:49.790 --> 00:07:52.730
So I imagine, right now,
that TensorFlow is mostly

00:07:52.730 --> 00:07:55.230
used by people who are quite
familiar with machine learning.

00:07:55.230 --> 00:07:59.456
But ultimately, the
way I hear people

00:07:59.456 --> 00:08:00.830
talk about machine
learning, it's

00:08:00.830 --> 00:08:03.000
just going to be used
by everyone everywhere.

00:08:03.000 --> 00:08:05.650
So can developers
who don't have much

00:08:05.650 --> 00:08:07.530
of a background in this
stuff pick it up yet?

00:08:07.530 --> 00:08:08.964
Is that possible?

00:08:08.964 --> 00:08:09.630
JEFF DEAN: Yeah.

00:08:09.630 --> 00:08:12.525
So I think, actually,
there's a whole set

00:08:12.525 --> 00:08:15.150
of ways in which people can take
advantage of machine learning.

00:08:15.150 --> 00:08:18.012
One is, as a fundamental
machine learning researcher,

00:08:18.012 --> 00:08:19.470
you want to develop
new algorithms.

00:08:19.470 --> 00:08:21.730
And that's going to be a
relatively small fraction

00:08:21.730 --> 00:08:23.095
of people in the world.

00:08:23.095 --> 00:08:26.370
But as new algorithms
and models are developed

00:08:26.370 --> 00:08:29.220
to solve particular
problems, those models

00:08:29.220 --> 00:08:31.690
can be applied in lots of
different kinds of things.

00:08:31.690 --> 00:08:35.400
If you look at the use
of machine learning

00:08:35.400 --> 00:08:36.909
in the diabetic
retinopathy stuff

00:08:36.909 --> 00:08:39.020
that Sundar mentioned
a couple days ago,

00:08:39.020 --> 00:08:41.440
that's a very similar problem
to a lot of other problems

00:08:41.440 --> 00:08:43.064
where you're trying
to look at an image

00:08:43.064 --> 00:08:45.670
and detect some part
of it that's unusual.

00:08:45.670 --> 00:08:48.870
We have a similar
problem of finding text

00:08:48.870 --> 00:08:51.600
in Street View images so
that we can read the text.

00:08:51.600 --> 00:08:54.100
And that looks pretty
similar to a model

00:08:54.100 --> 00:08:58.070
to detect diseased parts of an
eye, just different training

00:08:58.070 --> 00:08:59.330
data, but the same model.

00:08:59.330 --> 00:09:02.630
So I think the
broader set of models

00:09:02.630 --> 00:09:05.440
will be accessible to
more and more people.

00:09:05.440 --> 00:09:07.070
And then there's
even an easier way,

00:09:07.070 --> 00:09:09.700
where you don't really need
much machine learning knowledge

00:09:09.700 --> 00:09:12.540
at all, and that is to
use pre-trained APIs.

00:09:12.540 --> 00:09:15.320
Essentially, you can
use our Cloud Vision API

00:09:15.320 --> 00:09:17.682
or our Speech APIs very simply.

00:09:17.682 --> 00:09:20.140
You just give us an image, and
we give you back good stuff.

00:09:20.140 --> 00:09:22.320
And as part of the
TensorFlow flow open source,

00:09:22.320 --> 00:09:26.560
we also released, for example,
an inception model that

00:09:26.560 --> 00:09:29.850
does image classification that's
the same model as underlies

00:09:29.850 --> 00:09:30.592
Google Photos.

00:09:30.592 --> 00:09:31.300
TOM SIMONITE: OK.

00:09:31.300 --> 00:09:33.490
So will it be possible for
someone-- maybe they're

00:09:33.490 --> 00:09:37.380
an experienced builder of
apps, but don't know much about

00:09:37.380 --> 00:09:39.180
machine learning--
they could just

00:09:39.180 --> 00:09:42.120
have an idea and kind of use
these building blocks to put it

00:09:42.120 --> 00:09:42.620
together?

00:09:42.620 --> 00:09:42.830
JEFF DEAN: Yeah.

00:09:42.830 --> 00:09:45.250
Actually, I think one of the
reasons TensorFlow has taken

00:09:45.250 --> 00:09:47.400
off, is the tutorials in
TensorFlow are actually

00:09:47.400 --> 00:09:53.110
quite good at illustrating
six or seven important kinds

00:09:53.110 --> 00:09:55.320
of models in machine
learning, and showing people

00:09:55.320 --> 00:09:58.665
how they work, stepping through
both the machine learning

00:09:58.665 --> 00:10:01.040
that's going on underneath,
and also how you express them

00:10:01.040 --> 00:10:01.920
in TensorFlow.

00:10:01.920 --> 00:10:03.340
That's been pretty
well received.

00:10:03.340 --> 00:10:04.048
TOM SIMONITE: OK.

00:10:04.048 --> 00:10:06.070
And Aparna, I think
we've seen in the past

00:10:06.070 --> 00:10:10.930
that when a new platform of mode
of interaction comes forward,

00:10:10.930 --> 00:10:13.230
we have to experiment
with it for some time

00:10:13.230 --> 00:10:16.356
before we figure out
what works, right?

00:10:16.356 --> 00:10:17.730
And sometimes,
when we look back,

00:10:17.730 --> 00:10:19.920
we might think, oh,
those first generation

00:10:19.920 --> 00:10:23.814
mobile apps were kind of
clunky, and maybe not so smart.

00:10:23.814 --> 00:10:25.230
How are we going
with that process

00:10:25.230 --> 00:10:28.840
here, where we're starting
to have to understand

00:10:28.840 --> 00:10:30.950
what types of interaction work?

00:10:30.950 --> 00:10:32.075
APARNA CHENNAPRAGADA: Yeah.

00:10:32.075 --> 00:10:34.790
And I think it's one of the
things that's not intuitive

00:10:34.790 --> 00:10:36.870
when you start out, you
rush out into a new area,

00:10:36.870 --> 00:10:38.340
like we've all done.

00:10:38.340 --> 00:10:39.870
So one experience,
for example, when

00:10:39.870 --> 00:10:42.570
we started working on Google
Now, one thing we realized

00:10:42.570 --> 00:10:44.920
is, it's really
important to make sure

00:10:44.920 --> 00:10:49.280
that, depending on the product
domain, some of these black box

00:10:49.280 --> 00:10:51.120
systems, you need
to pay attention

00:10:51.120 --> 00:10:55.080
to what we call internally
as the wow to WTH ratio.

00:10:55.080 --> 00:10:57.130
That is, as soon
as you kind of say,

00:10:57.130 --> 00:11:00.220
hey, there are some delightful
magical moments, right?

00:11:00.220 --> 00:11:02.880
But then, if you
kind of get it wrong,

00:11:02.880 --> 00:11:04.620
there's a high cost to the user.

00:11:04.620 --> 00:11:06.490
So to give you an
example, in Google Search,

00:11:06.490 --> 00:11:09.150
let's say you search for, I
don't know, Justin Timberlake,

00:11:09.150 --> 00:11:12.670
and we got a slightly
less relevant answer.

00:11:12.670 --> 00:11:13.990
Not a big deal, right?

00:11:13.990 --> 00:11:16.690
But then, if the assistant
told you to sit in the car,

00:11:16.690 --> 00:11:18.960
go drive to the
airport, and you missed

00:11:18.960 --> 00:11:21.330
your flight, what the hell?

00:11:21.330 --> 00:11:23.790
So I think it's really important
to get that ratio right,

00:11:23.790 --> 00:11:27.460
especially in the early
stages of this new platform.

00:11:27.460 --> 00:11:29.490
The other thing
we noticed also is

00:11:29.490 --> 00:11:33.330
that explainability or
interpretability really builds

00:11:33.330 --> 00:11:35.630
trust in many of these cases.

00:11:35.630 --> 00:11:37.710
So you want to be
careful about looking

00:11:37.710 --> 00:11:41.130
at which parts of the problem
you use machine learning

00:11:41.130 --> 00:11:43.080
and you drop this into.

00:11:43.080 --> 00:11:46.250
You want to look at problems
that are easy for machines

00:11:46.250 --> 00:11:48.589
and hard for humans,
the repetitive things,

00:11:48.589 --> 00:11:50.880
and then make sure that those
are the problems that you

00:11:50.880 --> 00:11:52.440
throw machine learning against.

00:11:52.440 --> 00:11:56.520
But you don't want to be
unpredictable and inscrutable.

00:11:56.520 --> 00:11:59.490
TOM SIMONITE: And one mode of
interaction that everyone seems

00:11:59.490 --> 00:12:01.570
to be very excited
about, now, is this idea

00:12:01.570 --> 00:12:02.830
of conversational interface.

00:12:02.830 --> 00:12:06.510
So we saw the introduction on
Wednesday of Google Assistant,

00:12:06.510 --> 00:12:10.885
but lots of other companies
are building these things, too.

00:12:10.885 --> 00:12:13.290
Do we know that
definitely works?

00:12:13.290 --> 00:12:15.250
What do we know
about how you design

00:12:15.250 --> 00:12:17.780
a conversational interface,
or what the limitations

00:12:17.780 --> 00:12:19.299
and strengths are?

00:12:19.299 --> 00:12:21.590
APARNA CHENNAPRAGADA: I think,
again, at a broad level,

00:12:21.590 --> 00:12:24.580
you want to make sure that
you can have this trust.

00:12:24.580 --> 00:12:26.720
So [INAUDIBLE]
domains make it easy.

00:12:26.720 --> 00:12:29.560
So it's very hard to make
a very horizontal system

00:12:29.560 --> 00:12:31.620
work that works for anything.

00:12:31.620 --> 00:12:33.620
But I'm actually pretty
excited at the progress.

00:12:33.620 --> 00:12:36.540
We just launched-- open
sourced-- the sentence parser,

00:12:36.540 --> 00:12:37.990
Parsey Mcparseface.

00:12:37.990 --> 00:12:41.082
I just wanted to say that name.

00:12:41.082 --> 00:12:43.040
But it's really exciting,
because then you say,

00:12:43.040 --> 00:12:46.450
OK, you're starting to see the
beginning of conversational,

00:12:46.450 --> 00:12:49.744
or at least a natural language
sentence understanding,

00:12:49.744 --> 00:12:52.160
and then you have building
blocks that build on top of it.

00:12:52.160 --> 00:12:52.955
TOM SIMONITE: OK.

00:12:52.955 --> 00:12:56.400
And John, with your search
hat on for a second,

00:12:56.400 --> 00:13:01.010
we heard on Wednesday that,
I think, 20% of US searches

00:13:01.010 --> 00:13:02.240
are now done by voice.

00:13:02.240 --> 00:13:04.895
So people have clearly
got comfortable with this,

00:13:04.895 --> 00:13:06.520
and you've managed
to provide something

00:13:06.520 --> 00:13:09.040
that they want to use.

00:13:09.040 --> 00:13:12.450
Is the Assistant
interface to search

00:13:12.450 --> 00:13:14.680
going to grow in a
similar way, do you think?

00:13:14.680 --> 00:13:17.660
Is it going to take over a
big chunk of people's search

00:13:17.660 --> 00:13:18.160
queries?

00:13:18.160 --> 00:13:19.118
JOHN GIANNANDREA: Yeah.

00:13:19.118 --> 00:13:22.050
We think of the Assistant as a
fundamentally different product

00:13:22.050 --> 00:13:24.020
than search, and
I think it's going

00:13:24.020 --> 00:13:25.270
to be used in a different way.

00:13:25.270 --> 00:13:26.800
But we've been
working on what we

00:13:26.800 --> 00:13:28.930
call voice search
for many, many years,

00:13:28.930 --> 00:13:30.840
and we have this
evidence that people

00:13:30.840 --> 00:13:32.310
like it and are using it.

00:13:32.310 --> 00:13:36.790
And I would say our key
differentiator, there, is just

00:13:36.790 --> 00:13:38.810
the depth of search, and
the number of questions

00:13:38.810 --> 00:13:40.980
we can answer, and the
kinds of complexities

00:13:40.980 --> 00:13:43.380
that we can deal with.

00:13:43.380 --> 00:13:46.540
I think language and dialogue
is the big unsolved problem

00:13:46.540 --> 00:13:48.470
in computer science.

00:13:48.470 --> 00:13:50.632
So imagine you're
reading an article

00:13:50.632 --> 00:13:52.340
and then writing a
shorter version of it.

00:13:52.340 --> 00:13:54.380
That's currently beyond
the state of the art.

00:13:54.380 --> 00:13:56.755
I think the important thing
about the open source release

00:13:56.755 --> 00:14:02.090
we did of the parser is it's
using TensorFlow as well.

00:14:02.090 --> 00:14:03.830
So in the same way
as Jeff explained,

00:14:03.830 --> 00:14:06.280
the functionality of this
in Google Photos for finding

00:14:06.280 --> 00:14:08.260
your photos is actually
available open source,

00:14:08.260 --> 00:14:09.760
and people can
actually play with it

00:14:09.760 --> 00:14:11.124
and run a cloud version of it.

00:14:11.124 --> 00:14:13.540
We feel the same way about
natural language understanding,

00:14:13.540 --> 00:14:15.920
and we have many more
years of investment

00:14:15.920 --> 00:14:19.360
to make in getting to really
natural dialogue systems,

00:14:19.360 --> 00:14:20.860
where you can say
anything you want,

00:14:20.860 --> 00:14:23.060
and we have a good shot
of understanding it.

00:14:23.060 --> 00:14:25.620
So for us, this is a journey.

00:14:25.620 --> 00:14:29.420
Clearly, we have a fairly usable
product in voice search today.

00:14:29.420 --> 00:14:31.220
And the Assistant, we
hope, when we launch

00:14:31.220 --> 00:14:33.800
later this year,
people will similarly

00:14:33.800 --> 00:14:36.092
like to use it and
find it useful.

00:14:36.092 --> 00:14:36.800
TOM SIMONITE: OK.

00:14:36.800 --> 00:14:39.200
Do you need a different
monetization model

00:14:39.200 --> 00:14:40.550
for the Assistant dialogue?

00:14:40.550 --> 00:14:42.030
Is that something--

00:14:42.030 --> 00:14:42.970
JOHN GIANNANDREA: We're
really focused, right now,

00:14:42.970 --> 00:14:45.080
on building something
that users like to use.

00:14:45.080 --> 00:14:46.810
I think Google
has a long history

00:14:46.810 --> 00:14:49.735
of trying to build things
that people find useful.

00:14:49.735 --> 00:14:52.110
And if they find them useful,
and they use them at scale,

00:14:52.110 --> 00:14:54.940
then we'll figure out a way
to actually have a business

00:14:54.940 --> 00:14:56.352
to support that.

00:14:56.352 --> 00:14:57.060
TOM SIMONITE: OK.

00:14:57.060 --> 00:14:58.310
So you mentioned
that there are still

00:14:58.310 --> 00:14:59.940
a lot of open research
questions here,

00:14:59.940 --> 00:15:03.460
so maybe we could talk
about that a little bit.

00:15:03.460 --> 00:15:05.200
As you described,
there have been

00:15:05.200 --> 00:15:08.020
some very striking improvements
in machine learning recently,

00:15:08.020 --> 00:15:09.561
but there's a lot
that can't be done.

00:15:09.561 --> 00:15:11.450
I mean, if I go to my
daughter's preschool,

00:15:11.450 --> 00:15:14.410
I would see young children
learning and using

00:15:14.410 --> 00:15:17.600
language in ways that your
software can't match right now.

00:15:17.600 --> 00:15:21.840
So can you give us a summary
of the territory that's

00:15:21.840 --> 00:15:22.722
still to be explored?

00:15:22.722 --> 00:15:23.680
JOHN GIANNANDREA: Yeah.

00:15:23.680 --> 00:15:25.460
There's a lot still to be done.

00:15:25.460 --> 00:15:28.940
I think there's
a couple of areas

00:15:28.940 --> 00:15:30.410
which researchers
around the world

00:15:30.410 --> 00:15:32.590
are furiously trying to attack.

00:15:32.590 --> 00:15:35.340
So one is learning from
smaller numbers of examples.

00:15:35.340 --> 00:15:37.470
Today, the learning
systems that we have,

00:15:37.470 --> 00:15:39.890
including deep neural
networks, typically

00:15:39.890 --> 00:15:41.759
require really large
numbers of examples.

00:15:41.759 --> 00:15:43.300
Which is why, as
Jeff was describing,

00:15:43.300 --> 00:15:44.758
they can take a
long time to train,

00:15:44.758 --> 00:15:48.340
and the experiment
time can be slow.

00:15:48.340 --> 00:15:51.316
So it's great that
we can give systems

00:15:51.316 --> 00:15:53.900
hundreds of thousands or
millions of labeled examples,

00:15:53.900 --> 00:15:56.085
but clearly, small children
don't need to do that.

00:15:56.085 --> 00:15:58.210
They can learn from very
small numbers of examples.

00:15:58.210 --> 00:16:00.324
So that's an open problem.

00:16:00.324 --> 00:16:02.740
I think another very important
problem in machine learning

00:16:02.740 --> 00:16:05.300
is what the researchers call
transfer learning, which

00:16:05.300 --> 00:16:07.710
is learning something
in one domain,

00:16:07.710 --> 00:16:09.869
and then being able to
apply it in another.

00:16:09.869 --> 00:16:11.410
Right now, you have
to build a system

00:16:11.410 --> 00:16:13.592
to learn one particular
task, and then that's not

00:16:13.592 --> 00:16:14.800
transferable to another task.

00:16:14.800 --> 00:16:17.980
So for example, the
AlphaGo system that

00:16:17.980 --> 00:16:20.990
won the Go
Championship in Korea,

00:16:20.990 --> 00:16:24.440
that system can't, a priori,
play chess or tic tac toe.

00:16:24.440 --> 00:16:26.225
So that's a big,
big open problem

00:16:26.225 --> 00:16:28.802
in machine learning that lots
of people are interested in.

00:16:28.802 --> 00:16:29.510
TOM SIMONITE: OK.

00:16:29.510 --> 00:16:33.561
And Jeff, this is kind of on
your group, to some extent,

00:16:33.561 --> 00:16:34.060
isn't it?

00:16:34.060 --> 00:16:35.240
You need to figure this out.

00:16:35.240 --> 00:16:38.680
Are there particular
avenues or recent results

00:16:38.680 --> 00:16:41.544
that you would highlight
that seem to be promising?

00:16:41.544 --> 00:16:42.210
JEFF DEAN: Yeah.

00:16:42.210 --> 00:16:46.470
I think we're making, actually,
pretty significant progress

00:16:46.470 --> 00:16:48.470
in doing a better job of
language understanding.

00:16:48.470 --> 00:16:53.030
I think, if you look at where
computer vision was three

00:16:53.030 --> 00:16:54.700
or four or five
years ago, it was

00:16:54.700 --> 00:16:57.110
kind of just starting
to show signs of life,

00:16:57.110 --> 00:16:58.580
in terms of really
making progress.

00:16:58.580 --> 00:17:02.930
And I think we're starting to
see the same thing in language

00:17:02.930 --> 00:17:06.810
understanding kinds of models,
translation, parsing, question

00:17:06.810 --> 00:17:08.740
answering kinds of things.

00:17:08.740 --> 00:17:12.230
In terms of open problems,
I think unsupervised

00:17:12.230 --> 00:17:14.210
learning, being able to
learn from observations

00:17:14.210 --> 00:17:15.950
of the world that
are not labeled,

00:17:15.950 --> 00:17:18.710
and then occasionally getting
a few labeled examples that

00:17:18.710 --> 00:17:21.109
tell you, these are important
things about the world

00:17:21.109 --> 00:17:23.410
to pay attention
to, that's really

00:17:23.410 --> 00:17:27.599
one of the key open challenges
in machine learning.

00:17:27.599 --> 00:17:31.190
And one more, I would
add, is, right now,

00:17:31.190 --> 00:17:34.130
what you need a lot of
machine learning expertise for

00:17:34.130 --> 00:17:36.740
is to kind of device the
right model structure

00:17:36.740 --> 00:17:38.870
for a particular
kind of problem.

00:17:38.870 --> 00:17:41.800
For an image problem, I should
use convolutional neural nets,

00:17:41.800 --> 00:17:44.670
or for language problems, I
should use this particular kind

00:17:44.670 --> 00:17:46.680
of recurrent neural net.

00:17:46.680 --> 00:17:48.720
And I think one
of the things that

00:17:48.720 --> 00:17:50.730
would be really
powerful and amazing

00:17:50.730 --> 00:17:54.860
is if the system itself could
device the right structure

00:17:54.860 --> 00:17:57.240
for the data it's observing.

00:17:57.240 --> 00:17:59.900
So learning model
structure concurrently

00:17:59.900 --> 00:18:02.590
with trying to solve some
set of tasks, I think,

00:18:02.590 --> 00:18:05.172
would be a really great
open research problem.

00:18:05.172 --> 00:18:05.880
TOM SIMONITE: OK.

00:18:05.880 --> 00:18:08.600
So instead of you having
to design the system

00:18:08.600 --> 00:18:11.569
and then setting
it loose to learn,

00:18:11.569 --> 00:18:13.860
the learning system would
build itself, to some extent?

00:18:13.860 --> 00:18:14.568
JEFF DEAN: Right.

00:18:14.568 --> 00:18:17.432
Right now, you kind of define
the scaffolding of the model,

00:18:17.432 --> 00:18:18.890
and then you fiddle
with parameters

00:18:18.890 --> 00:18:21.200
as part of the learning
process, but you don't sort of

00:18:21.200 --> 00:18:22.954
introduce new kinds
of connections

00:18:22.954 --> 00:18:24.203
in the model structure itself.

00:18:24.203 --> 00:18:25.036
TOM SIMONITE: Right.

00:18:25.036 --> 00:18:25.880
OK.

00:18:25.880 --> 00:18:29.520
And unsupervised learning,
just giving it that label,

00:18:29.520 --> 00:18:31.730
it makes it sound like
one unitary problem, which

00:18:31.730 --> 00:18:32.430
may not be true.

00:18:32.430 --> 00:18:36.770
But will big
progress on that come

00:18:36.770 --> 00:18:41.340
from one flash of insight
and a new algorithm,

00:18:41.340 --> 00:18:46.784
or will it be-- I don't
know-- a longer slog?

00:18:46.784 --> 00:18:47.450
JEFF DEAN: Yeah.

00:18:47.450 --> 00:18:50.710
If I knew, that
would be [INAUDIBLE].

00:18:50.710 --> 00:18:53.230
I have a feeling that it's
not going to be, like,

00:18:53.230 --> 00:18:54.260
100 different things.

00:18:54.260 --> 00:18:57.170
I feel like there's
a few key insights

00:18:57.170 --> 00:19:00.240
that new kinds of
learning algorithms

00:19:00.240 --> 00:19:03.420
could pick up on
as to what aspects

00:19:03.420 --> 00:19:06.730
of the world the model is
observing are important.

00:19:06.730 --> 00:19:08.660
And knowing which
things are important

00:19:08.660 --> 00:19:11.100
is one of the key things
about unsupervised learning.

00:19:11.100 --> 00:19:12.790
TOM SIMONITE: OK.

00:19:12.790 --> 00:19:18.110
Aparna, so what Jeff's team
kind of works out, eventually,

00:19:18.110 --> 00:19:19.610
should come through
into your hands,

00:19:19.610 --> 00:19:21.370
and you could build
stuff with it.

00:19:21.370 --> 00:19:23.140
Is there something
that you would really

00:19:23.140 --> 00:19:26.180
like him to invent tomorrow,
so you can start building

00:19:26.180 --> 00:19:28.249
stuff with it the day after?

00:19:28.249 --> 00:19:30.040
APARNA CHENNAPRAGADA:
Auto generate emails.

00:19:30.040 --> 00:19:32.560
No, I'm kidding.

00:19:32.560 --> 00:19:35.162
I do think, actually, what's
interesting is, you've heard

00:19:35.162 --> 00:19:36.370
these building blocks, right?

00:19:36.370 --> 00:19:40.230
So machine perception, computer
vision, wasn't a thing,

00:19:40.230 --> 00:19:41.940
and now it's actually reliable.

00:19:41.940 --> 00:19:44.680
Language understanding,
it's getting there.

00:19:44.680 --> 00:19:45.950
Translation is getting there.

00:19:45.950 --> 00:19:49.340
To me, the next other building
block you can make machines do

00:19:49.340 --> 00:19:51.170
is hand-eye coordination.

00:19:51.170 --> 00:19:53.280
So you've seen the
robot arms video

00:19:53.280 --> 00:19:56.390
that Sundar talked about
and showed at the keynote,

00:19:56.390 --> 00:20:00.720
but imagine if you could kind
of have these rote tasks that

00:20:00.720 --> 00:20:03.780
are harder, tedious
for humans, but if you

00:20:03.780 --> 00:20:07.520
had reliable hand-eye
coordination built in, that's

00:20:07.520 --> 00:20:09.790
in a learned system versus
a controlled system code

00:20:09.790 --> 00:20:11.750
that you usually write,
and it's very brittle,

00:20:11.750 --> 00:20:13.800
suddenly, it opens up a
lot more opportunities.

00:20:13.800 --> 00:20:16.270
Just off the top of my
head, why isn't there

00:20:16.270 --> 00:20:18.190
anything for,
like, elderly care?

00:20:18.190 --> 00:20:21.630
Like, you are an 80-year-old
woman with a bad back,

00:20:21.630 --> 00:20:23.080
and you're picking up things.

00:20:23.080 --> 00:20:24.800
Why isn't there something there?

00:20:24.800 --> 00:20:27.185
Or even something as mundane
with natural language

00:20:27.185 --> 00:20:28.060
understanding, right?

00:20:28.060 --> 00:20:29.340
I have a seven-year-old.

00:20:29.340 --> 00:20:31.050
I'm a mom of a 7-year-old.

00:20:31.050 --> 00:20:33.230
Why isn't there something
for, I don't know,

00:20:33.230 --> 00:20:36.250
math homework, with natural
language understanding?

00:20:36.250 --> 00:20:38.000
JOHN GIANNANDREA: So
I think one of things

00:20:38.000 --> 00:20:39.520
we've learned in
the last few years

00:20:39.520 --> 00:20:42.370
is that things that
are hard for people

00:20:42.370 --> 00:20:44.060
to do, we can teach
computers to do,

00:20:44.060 --> 00:20:45.800
and things that are
easy for us to do

00:20:45.800 --> 00:20:47.587
are still the hard
problems for computers.

00:20:47.587 --> 00:20:48.420
TOM SIMONITE: Right.

00:20:48.420 --> 00:20:49.520
OK.

00:20:49.520 --> 00:20:56.330
And does that mean we're still
missing some big new field

00:20:56.330 --> 00:20:57.080
we need to invent?

00:20:57.080 --> 00:20:59.496
Because most of the things
we've been talking about so far

00:20:59.496 --> 00:21:01.400
have been built on top
of this deep learning

00:21:01.400 --> 00:21:02.449
and neural network.

00:21:02.449 --> 00:21:04.740
JOHN GIANNANDREA: I think
robotics work is interesting,

00:21:04.740 --> 00:21:08.460
because it gives the
computer system an embodiment

00:21:08.460 --> 00:21:10.280
in the world, right?

00:21:10.280 --> 00:21:13.130
So learning from
tactile environments

00:21:13.130 --> 00:21:16.210
is a new kind of learning,
as opposed to just looking

00:21:16.210 --> 00:21:17.830
at unsupervised or supervised.

00:21:17.830 --> 00:21:21.080
Just reading text is a
particular environment.

00:21:21.080 --> 00:21:23.936
Perception, looking at
images, looking at audio,

00:21:23.936 --> 00:21:25.560
trying to understand
what this song is,

00:21:25.560 --> 00:21:27.400
that's another kind of problem.

00:21:27.400 --> 00:21:29.070
I think interacting
with the real world

00:21:29.070 --> 00:21:30.660
is a whole other
kind of problem.

00:21:30.660 --> 00:21:30.910
TOM SIMONITE: Right.

00:21:30.910 --> 00:21:31.690
OK.

00:21:31.690 --> 00:21:33.145
That's interesting.

00:21:33.145 --> 00:21:35.270
Maybe this is a good time
to talk a little bit more

00:21:35.270 --> 00:21:35.960
about DeepMind.

00:21:35.960 --> 00:21:38.600
I know that they are very
interested in this idea

00:21:38.600 --> 00:21:43.360
of embodiment, the idea you
have to submerge this learning

00:21:43.360 --> 00:21:45.230
agent in a world that
it can learn from.

00:21:45.230 --> 00:21:47.710
Can you explain how
they're approaching this?

00:21:47.710 --> 00:21:48.090
JOHN GIANNANDREA: Yeah, sure.

00:21:48.090 --> 00:21:49.902
I mean, DeepMind is
another research group

00:21:49.902 --> 00:21:52.110
that we have at Google, and
we work closely with them

00:21:52.110 --> 00:21:53.500
all the time.

00:21:53.500 --> 00:21:57.070
They are particularly interested
in learning from simulations.

00:21:57.070 --> 00:21:59.320
So they've done a lot
of work with video games

00:21:59.320 --> 00:22:01.630
and simulations of
physical environments,

00:22:01.630 --> 00:22:04.280
and that's one of the research
directions that they have.

00:22:04.280 --> 00:22:06.680
It's been very productive.

00:22:06.680 --> 00:22:08.800
TOM SIMONITE: OK.

00:22:08.800 --> 00:22:09.620
Is it just games?

00:22:09.620 --> 00:22:12.090
Are they moving into
different types of simulation?

00:22:12.090 --> 00:22:14.090
JOHN GIANNANDREA: Well,
there's a very fine line

00:22:14.090 --> 00:22:17.260
between a video game-- a
three-dimensional video game--

00:22:17.260 --> 00:22:20.600
and a physics simulation
already environment, right?

00:22:20.600 --> 00:22:22.210
I mean, some video
games are, in fact,

00:22:22.210 --> 00:22:26.140
full simulations of worlds,
so there's not really

00:22:26.140 --> 00:22:27.002
a bright line there.

00:22:27.002 --> 00:22:27.710
TOM SIMONITE: OK.

00:22:27.710 --> 00:22:29.310
And do DeepMind
work on robotics?

00:22:29.310 --> 00:22:30.532
They don't, I didn't think.

00:22:30.532 --> 00:22:32.490
JOHN GIANNANDREA: They're
doing a bunch of work

00:22:32.490 --> 00:22:33.960
in a bunch of different
fields, some of which

00:22:33.960 --> 00:22:35.610
gets published, some
of which is not.

00:22:35.610 --> 00:22:36.580
TOM SIMONITE: OK.

00:22:36.580 --> 00:22:40.320
And the robot arms that we saw
in the keynote on Wednesday,

00:22:40.320 --> 00:22:41.800
are they within
your group, Jeff?

00:22:41.800 --> 00:22:42.260
JEFF DEAN: Yes.

00:22:42.260 --> 00:22:42.610
TOM SIMONITE: OK.

00:22:42.610 --> 00:22:44.254
So can you tell us
about that project?

00:22:44.254 --> 00:22:44.920
JEFF DEAN: Sure.

00:22:44.920 --> 00:22:46.890
So that was a collaboration
between our group

00:22:46.890 --> 00:22:52.520
and the robotics teams in Google
X. Actually, what happened was,

00:22:52.520 --> 00:22:53.980
one of our
researchers discovered

00:22:53.980 --> 00:22:55.840
that the robotics
team, actually,

00:22:55.840 --> 00:22:59.095
had 20 unused arms sitting
in a closet somewhere.

00:22:59.095 --> 00:23:01.220
They were a model that was
going to be discontinued

00:23:01.220 --> 00:23:02.312
and not actually used.

00:23:02.312 --> 00:23:06.550
So we're like, hey, we should
set these up in a room.

00:23:06.550 --> 00:23:10.110
And basically, just
the idea of having

00:23:10.110 --> 00:23:12.774
a little bit larger scale
robotics test environment

00:23:12.774 --> 00:23:14.690
than just one arm, which
is what you typically

00:23:14.690 --> 00:23:18.560
have in a physical
robotics lab, would

00:23:18.560 --> 00:23:22.604
make it possible to do a bit
more exploratory research.

00:23:22.604 --> 00:23:24.770
So one of the first things
we did with that was just

00:23:24.770 --> 00:23:27.964
have the robots learn
to pick up objects.

00:23:27.964 --> 00:23:29.630
And one of the nice
properties that has,

00:23:29.630 --> 00:23:32.600
it's a completely
supervised problem.

00:23:32.600 --> 00:23:34.100
The robot can try
to grab something,

00:23:34.100 --> 00:23:36.510
and if it closes its griper
all the way, it failed.

00:23:36.510 --> 00:23:38.093
And if it didn't
close it all the way,

00:23:38.093 --> 00:23:40.020
and it picked something
up, it succeeded.

00:23:40.020 --> 00:23:44.070
And so it's learning from
raw camera pixel inputs

00:23:44.070 --> 00:23:45.732
directly to torque
motor controls.

00:23:45.732 --> 00:23:47.190
And there's just
a neural net there

00:23:47.190 --> 00:23:51.750
that's trained to pick things up
based on the observations it's

00:23:51.750 --> 00:23:55.280
making of things as it
approaches a particular object.

00:23:55.280 --> 00:23:57.475
TOM SIMONITE: And is that
quite a slow process?

00:23:57.475 --> 00:23:59.600
I mean, that fact that you
have multiple arms going

00:23:59.600 --> 00:24:02.120
at once made me think
that, maybe, you

00:24:02.120 --> 00:24:04.600
were trying to maximize your
throughput, or something.

00:24:04.600 --> 00:24:05.308
JEFF DEAN: Right.

00:24:05.308 --> 00:24:08.810
So if you have 20 arms, you get
20 times as much experience.

00:24:08.810 --> 00:24:11.990
And if you think about how small
kids learn to pick stuff up,

00:24:11.990 --> 00:24:13.870
it takes them maybe
a year, or something,

00:24:13.870 --> 00:24:17.180
to go from being able to
move their arm to really be

00:24:17.180 --> 00:24:19.270
able to grasp simple objects.

00:24:19.270 --> 00:24:22.420
And by parallelizing
this across more arms,

00:24:22.420 --> 00:24:24.797
you can pool the experience
of the robotic arms a bit.

00:24:24.797 --> 00:24:25.630
TOM SIMONITE: I see.

00:24:25.630 --> 00:24:27.780
OK.

00:24:27.780 --> 00:24:29.862
JEFF DEAN: And they
need less sleep.

00:24:29.862 --> 00:24:31.544
TOM SIMONITE: Right.

00:24:31.544 --> 00:24:32.960
John, at the start
of the session,

00:24:32.960 --> 00:24:35.560
you referred to this
concept of AI winter,

00:24:35.560 --> 00:24:39.280
and you said you
thought it was spring.

00:24:39.280 --> 00:24:43.300
When do we know
that it's summer?

00:24:43.300 --> 00:24:45.260
JOHN GIANNANDREA:
Summer follows spring.

00:24:45.260 --> 00:24:47.330
I mean, there's still a
lot of unsolved problems.

00:24:47.330 --> 00:24:49.204
I think problems around
dialogue and language

00:24:49.204 --> 00:24:52.670
are the ones that I'm
particularly interested in.

00:24:52.670 --> 00:24:56.235
And so, until we can teach
a computer to really read,

00:24:56.235 --> 00:24:59.970
I don't think we can
declare that it's summer.

00:24:59.970 --> 00:25:02.710
I mean, if you can imagine
a computer's really reading

00:25:02.710 --> 00:25:04.080
and internalizing a document.

00:25:04.080 --> 00:25:05.010
So it's interesting.

00:25:05.010 --> 00:25:08.970
So translation is reading
a paragraph in one language

00:25:08.970 --> 00:25:10.810
and writing it in
another language.

00:25:10.810 --> 00:25:12.510
In order to do that
really, really well,

00:25:12.510 --> 00:25:13.195
you have to be
able to paraphrase.

00:25:13.195 --> 00:25:15.444
You have to be able to reorder
words, and so on and so

00:25:15.444 --> 00:25:17.210
forth So imagine
translating something

00:25:17.210 --> 00:25:18.649
from English to English.

00:25:18.649 --> 00:25:21.190
So you read a paragraph, and
you write a different paragraph.

00:25:21.190 --> 00:25:25.350
If we could do that, I think
I would declare summer.

00:25:25.350 --> 00:25:26.510
TOM SIMONITE: OK.

00:25:26.510 --> 00:25:30.400
Reading is-- well, there are
different levels of reading,

00:25:30.400 --> 00:25:31.120
aren't there?

00:25:31.120 --> 00:25:33.264
Do you know--

00:25:33.264 --> 00:25:35.680
JOHN GIANNANDREA: If you can
paraphrase, then you really--

00:25:35.680 --> 00:25:36.375
TOM SIMONITE: Then you
think that-- if you

00:25:36.375 --> 00:25:37.140
could reach that level.

00:25:37.140 --> 00:25:37.780
JOHN GIANNANDREA: And
actually understood--

00:25:37.780 --> 00:25:39.160
TOM SIMONITE: Then
you've got some argument.

00:25:39.160 --> 00:25:40.909
JOHN GIANNANDREA: And
to a certain extent,

00:25:40.909 --> 00:25:42.500
today, our translation
systems, which

00:25:42.500 --> 00:25:45.760
are not perfect by any
means, are getting better.

00:25:45.760 --> 00:25:46.830
They do do some of that.

00:25:46.830 --> 00:25:48.100
They do do some paraphrasing.

00:25:48.100 --> 00:25:49.490
They do do some re-ordering.

00:25:49.490 --> 00:25:52.130
They do do a remarkable amount
of language understanding.

00:25:52.130 --> 00:25:54.750
So I'm hopeful researchers
around the world

00:25:54.750 --> 00:25:55.659
will get there.

00:25:55.659 --> 00:25:57.950
And it's very important to
us that our natural language

00:25:57.950 --> 00:25:59.730
APIs become part of
our cloud platform,

00:25:59.730 --> 00:26:02.931
and that people can
experiment with it, and help.

00:26:02.931 --> 00:26:04.430
JEFF DEAN: One thing
I would say is,

00:26:04.430 --> 00:26:05.804
I don't think
there's going to be

00:26:05.804 --> 00:26:08.830
this abrupt line between
spring and summer, right?

00:26:08.830 --> 00:26:11.710
There's going to be developments
that push the state of the art

00:26:11.710 --> 00:26:13.510
forward in lots of
different areas in kind

00:26:13.510 --> 00:26:16.380
of this smooth gradient
of capabilities.

00:26:16.380 --> 00:26:18.840
And at some point,
something becomes

00:26:18.840 --> 00:26:21.150
possible that didn't
used to be possible,

00:26:21.150 --> 00:26:23.020
and people kind of
move the goalposts

00:26:23.020 --> 00:26:28.804
of what they think of as
really, truly hard problems.

00:26:28.804 --> 00:26:30.720
APARNA CHENNAPRAGADA:
The classic joke, right?

00:26:30.720 --> 00:26:32.859
It's only AI until
it starts working,

00:26:32.859 --> 00:26:34.150
and then it's computer science.

00:26:34.150 --> 00:26:36.270
JEFF DEAN: Like, if you'd
asked me four years ago,

00:26:36.270 --> 00:26:38.390
could a computer
write a sentence

00:26:38.390 --> 00:26:40.450
given an image as input?

00:26:40.450 --> 00:26:42.220
And I would have said,
I don't think they

00:26:42.220 --> 00:26:43.190
can do that for a little while.

00:26:43.190 --> 00:26:44.764
And they can actually
do that today,

00:26:44.764 --> 00:26:46.680
and that's kind of a
good example of something

00:26:46.680 --> 00:26:48.980
that has made a lot of
progress in the last few years.

00:26:48.980 --> 00:26:51.320
And now you sort of say,
OK, that's in our tool

00:26:51.320 --> 00:26:53.152
chest of capabilities.

00:26:53.152 --> 00:26:53.860
TOM SIMONITE: OK.

00:26:53.860 --> 00:26:56.320
But if we're not that
great at predicting

00:26:56.320 --> 00:27:00.020
how the progress goes, does
that mean we can't see winter,

00:27:00.020 --> 00:27:00.975
if it comes back?

00:27:04.300 --> 00:27:06.685
JOHN GIANNANDREA: If we
stop seeing progress,

00:27:06.685 --> 00:27:09.899
then I think we could question
what the future's going

00:27:09.899 --> 00:27:10.440
to look like.

00:27:10.440 --> 00:27:14.419
But today, the rate of-- I
think researchers in the field

00:27:14.419 --> 00:27:16.210
are excited about this,
and maybe the field

00:27:16.210 --> 00:27:18.830
is a little bit over-hyped
because of the rate of progress

00:27:18.830 --> 00:27:19.860
we're seeing.

00:27:19.860 --> 00:27:21.610
Because something like
speech recognition,

00:27:21.610 --> 00:27:23.484
which didn't work for
my wife five years ago,

00:27:23.484 --> 00:27:29.570
and now works flawlessly,
because image identification

00:27:29.570 --> 00:27:32.460
is now working better than
human raters for many fields.

00:27:32.460 --> 00:27:36.186
So there's these narrow fields
for which algorithms are not

00:27:36.186 --> 00:27:37.560
superhuman in
their capabilities.

00:27:37.560 --> 00:27:39.240
So we're seeing
tremendous progress.

00:27:39.240 --> 00:27:42.662
And so it's very exciting for
people working in this field.

00:27:42.662 --> 00:27:43.370
TOM SIMONITE: OK.

00:27:43.370 --> 00:27:44.110
Great.

00:27:44.110 --> 00:27:46.130
I should just note that,
in a couple of minutes,

00:27:46.130 --> 00:27:48.870
we will open up the
floor for questions.

00:27:48.870 --> 00:27:52.380
There are microphones here and
here in the main seating area,

00:27:52.380 --> 00:27:55.346
and there's one microphone
up in the press area, which

00:27:55.346 --> 00:27:57.012
I can't see right
now, but hopefully you

00:27:57.012 --> 00:27:58.136
can figure out where it is.

00:28:01.930 --> 00:28:04.815
Sundar Pichai, CEO of Google,
has spoken a lot recently

00:28:04.815 --> 00:28:06.940
about how he thinks we're
moving from a world which

00:28:06.940 --> 00:28:09.030
is mobile-first to AI-first.

00:28:11.570 --> 00:28:13.800
I'm interested to hear
what you think that means.

00:28:13.800 --> 00:28:16.625
Maybe, Aparna, you
could speak to that.

00:28:16.625 --> 00:28:18.000
APARNA CHENNAPRAGADA:
I interpret

00:28:18.000 --> 00:28:19.340
it a couple different ways.

00:28:19.340 --> 00:28:21.740
One is, if you look at
how mobile's changed,

00:28:21.740 --> 00:28:25.380
how you experience
computing, it's

00:28:25.380 --> 00:28:28.146
not happened at one level
of the stack, right?

00:28:28.146 --> 00:28:29.520
It's at the
interface level, it's

00:28:29.520 --> 00:28:31.544
at the information level,
and infrastructure.

00:28:31.544 --> 00:28:33.210
And I think that's
the same thing that's

00:28:33.210 --> 00:28:36.190
going to happen with AI and
any of these machine learning

00:28:36.190 --> 00:28:39.160
techniques, which is, you'll
have infrastructure layer

00:28:39.160 --> 00:28:39.910
improvements.

00:28:39.910 --> 00:28:41.930
You saw the
announcement about TPU.

00:28:41.930 --> 00:28:44.510
You'll have a bunch of
algorithms and models

00:28:44.510 --> 00:28:47.150
improvements at the intelligence
and information layer,

00:28:47.150 --> 00:28:48.650
and there will be
interface changes.

00:28:48.650 --> 00:28:51.016
So the best UI is
probably no UI.

00:28:51.016 --> 00:28:52.110
TOM SIMONITE: Right.

00:28:52.110 --> 00:28:53.190
OK.

00:28:53.190 --> 00:28:57.074
John, what does
AI-first mean to you?

00:28:57.074 --> 00:28:58.490
JOHN GIANNANDREA:
I think it means

00:28:58.490 --> 00:29:01.920
that this assistant kind of
layer is available to you

00:29:01.920 --> 00:29:02.990
wherever you are.

00:29:02.990 --> 00:29:05.250
Whether you're in your
car, or whether it's

00:29:05.250 --> 00:29:07.230
ambient in your house,
or whether you're

00:29:07.230 --> 00:29:10.530
using your mobile
device or laptop,

00:29:10.530 --> 00:29:12.430
that there is this
smart assistance

00:29:12.430 --> 00:29:17.345
that you find very quietly
useful to you all the time.

00:29:17.345 --> 00:29:19.470
Kind of how Google search
is for most people today.

00:29:19.470 --> 00:29:23.250
I think most people would not
want search engines taken away

00:29:23.250 --> 00:29:24.500
from them, right?

00:29:24.500 --> 00:29:26.490
So I think that being
that useful to people,

00:29:26.490 --> 00:29:27.948
so that people take
it for granted,

00:29:27.948 --> 00:29:29.970
and then it's ambient
across all your devices,

00:29:29.970 --> 00:29:31.689
is what AI-first means to me.

00:29:31.689 --> 00:29:33.855
TOM SIMONITE: And we're in
the early stages of this,

00:29:33.855 --> 00:29:34.397
do you think?

00:29:34.397 --> 00:29:35.355
JOHN GIANNANDREA: Yeah.

00:29:35.355 --> 00:29:36.440
It's a journey, I think.

00:29:36.440 --> 00:29:37.762
It's a multi-year journey

00:29:37.762 --> 00:29:38.470
TOM SIMONITE: OK.

00:29:38.470 --> 00:29:39.110
Great.

00:29:39.110 --> 00:29:41.980
So thanks for a
fascinating conversation.

00:29:41.980 --> 00:29:44.690
Now, we'll let someone else ask
the questions for a little bit.

00:29:44.690 --> 00:29:49.450
I will alternate between
the press mic and the mics

00:29:49.450 --> 00:29:51.470
down here at the front.

00:29:51.470 --> 00:29:53.132
Please keep your
questions short,

00:29:53.132 --> 00:29:54.590
so we can get
through more of them,

00:29:54.590 --> 00:29:58.110
and make sure they're
questions, not statements.

00:29:58.110 --> 00:30:03.190
We will start with the
press mic, wherever it is.

00:30:13.457 --> 00:30:14.915
MALE SPEAKER:
There's nobody there.

00:30:14.915 --> 00:30:18.262
TOM SIMONITE: I really doubt
the press has no questions.

00:30:18.262 --> 00:30:18.970
What's happening?

00:30:18.970 --> 00:30:20.795
Why don't we start
with the developer mic

00:30:20.795 --> 00:30:23.830
right here on the right?

00:30:23.830 --> 00:30:28.280
AUDIENCE: I have a philosophical
question about prejudice.

00:30:28.280 --> 00:30:31.230
People tend to have prejudice.

00:30:31.230 --> 00:30:33.540
Do you think this
is a step stone

00:30:33.540 --> 00:30:36.260
that we need to take in
artificial intelligence,

00:30:36.260 --> 00:30:40.995
and how would
society accept that?

00:30:40.995 --> 00:30:43.370
JOHN GIANNANDREA: I'm not sure
I understand the question.

00:30:43.370 --> 00:30:46.320
Some people have prejudice, and?

00:30:46.320 --> 00:30:49.380
AUDIENCE: Some people
have the tendency

00:30:49.380 --> 00:30:53.830
to have prejudice, which
might lead to behaviors

00:30:53.830 --> 00:30:56.040
such as discrimination.

00:30:56.040 --> 00:30:57.530
TOM SIMONITE: So
the question is,

00:30:57.530 --> 00:31:00.004
will the systems that the
people build have biases?

00:31:00.004 --> 00:31:01.170
JOHN GIANNANDREA: Oh, I see.

00:31:01.170 --> 00:31:02.150
I see.

00:31:02.150 --> 00:31:05.380
Will people's prejudices creep
into machine learning systems?

00:31:05.380 --> 00:31:07.140
I think that is a risk.

00:31:07.140 --> 00:31:10.010
I think it all depends on the
training data that we choose.

00:31:10.010 --> 00:31:13.070
We've already seen some issues
with this kind of problem.

00:31:13.070 --> 00:31:14.779
So I think it all
depends on carefully

00:31:14.779 --> 00:31:16.320
selecting training
data, particularly

00:31:16.320 --> 00:31:17.278
for supervised systems.

00:31:19.880 --> 00:31:21.440
TOM SIMONITE: OK.

00:31:21.440 --> 00:31:23.841
Is the press mic
working, at this point?

00:31:23.841 --> 00:31:24.632
SEAN HOLLISTER: Hi.

00:31:24.632 --> 00:31:26.887
I'm Sean Hollister, up
here in the press mic.

00:31:26.887 --> 00:31:27.720
TOM SIMONITE: Great.

00:31:27.720 --> 00:31:28.320
Go for it.

00:31:28.320 --> 00:31:29.403
SEAN HOLLISTER: Hi, there.

00:31:29.403 --> 00:31:33.770
I wanted to ask about the role
of privacy in machine learning.

00:31:33.770 --> 00:31:38.530
You need a lot of data to
make these observations

00:31:38.530 --> 00:31:41.310
and to help people
with machine learning.

00:31:41.310 --> 00:31:44.100
I give all my photos
to Google Photos,

00:31:44.100 --> 00:31:47.100
and I wonder what happens
to them afterwards.

00:31:47.100 --> 00:31:49.610
What allows Google
to see what they

00:31:49.610 --> 00:31:53.680
are, and is that ever shared
in any way with anyone else?

00:31:53.680 --> 00:31:55.852
Personally, I don't care
very much about that.

00:31:55.852 --> 00:31:57.310
I'm not worried my
photos are going

00:31:57.310 --> 00:32:00.070
to get out to other folks,
but where do they go?

00:32:00.070 --> 00:32:01.990
What do you do with them?

00:32:01.990 --> 00:32:04.586
And to what degree
are they protected?

00:32:04.586 --> 00:32:06.253
JEFF DEAN: Do you
want to take that one?

00:32:06.253 --> 00:32:07.669
APARNA CHENNAPRAGADA:
I think this

00:32:07.669 --> 00:32:09.710
is one of the most
important things

00:32:09.710 --> 00:32:12.370
that we look at across products.

00:32:12.370 --> 00:32:14.430
So even with photos,
or Google Now,

00:32:14.430 --> 00:32:16.150
or voice, and all
of these things.

00:32:16.150 --> 00:32:20.090
There's actually two principles
we codify into building this.

00:32:20.090 --> 00:32:22.340
One is, there's
a very explicit--

00:32:22.340 --> 00:32:25.650
it's a very transparent
contract between the user

00:32:25.650 --> 00:32:29.240
and the product that is, you
basically know what benefits

00:32:29.240 --> 00:32:31.370
you're getting with
the data, and the data

00:32:31.370 --> 00:32:32.810
is there to help you.

00:32:32.810 --> 00:32:34.390
That's one principle.

00:32:34.390 --> 00:32:39.300
But the second is, by default,
it's an opt-in experience.

00:32:39.300 --> 00:32:40.620
You're in the driver's seat.

00:32:40.620 --> 00:32:42.750
In some sense, let's
say, you're saying,

00:32:42.750 --> 00:32:45.280
hey, I do want to get
traffic information when

00:32:45.280 --> 00:32:48.270
I'm on Shoreline, because
it's clogged up to Shoreline

00:32:48.270 --> 00:32:50.589
Amphitheater, you, of
course, need the system

00:32:50.589 --> 00:32:51.880
to know where your location is.

00:32:51.880 --> 00:32:55.450
Because you don't want to know
how the traffic is in Napa.

00:32:55.450 --> 00:32:58.720
So having that contract
be transparent, but also

00:32:58.720 --> 00:33:04.520
an opt-in, I think it really
addresses the equation.

00:33:04.520 --> 00:33:06.630
But I think the other
thing to add in here

00:33:06.630 --> 00:33:11.030
is also that, by definition,
all of these are for your eyes

00:33:11.030 --> 00:33:12.690
only, right?

00:33:12.690 --> 00:33:16.466
In terms of, like, all your data
is yours, and that's an axiom.

00:33:16.466 --> 00:33:18.340
JOHN GIANNANDREA: And
to answer his question,

00:33:18.340 --> 00:33:19.820
we would never share his photos.

00:33:19.820 --> 00:33:24.110
We train models based on other
photos that are not yours,

00:33:24.110 --> 00:33:26.016
and then the machine
looks at your photos,

00:33:26.016 --> 00:33:27.640
and it can label it,
but we would never

00:33:27.640 --> 00:33:29.000
share your private photo there.

00:33:29.000 --> 00:33:31.100
SEAN HOLLISTER: To what
degree is advertising

00:33:31.100 --> 00:33:34.330
anonymously-targeted
at folks like me,

00:33:34.330 --> 00:33:37.170
based on the contents
of things I upload,

00:33:37.170 --> 00:33:40.410
little inferences you
make in the meta data?

00:33:40.410 --> 00:33:44.400
Is any of that going to
advertisers in any way,

00:33:44.400 --> 00:33:47.300
even in aggregate, hey,
this is a person who

00:33:47.300 --> 00:33:48.560
seems to like dogs?

00:33:50.414 --> 00:33:51.830
JOHN GIANNANDREA:
For your photos?

00:33:51.830 --> 00:33:52.070
No.

00:33:52.070 --> 00:33:52.695
Absolutely not.

00:33:52.695 --> 00:33:53.110
APARNA CHENNAPRAGADA: No.

00:33:53.110 --> 00:33:53.818
TOM SIMONITE: OK.

00:33:53.818 --> 00:33:55.776
Let's go to this mic right here.

00:33:55.776 --> 00:33:58.740
AUDIENCE: My questions
is for Aparna, about,

00:33:58.740 --> 00:34:02.140
what is the thought process
behind creating a new product?

00:34:02.140 --> 00:34:05.460
Because there are so many things
that these guys are creating.

00:34:05.460 --> 00:34:08.350
So how do you go from-- because
it's kind of obvious right

00:34:08.350 --> 00:34:10.685
now to see if you
have my emails,

00:34:10.685 --> 00:34:14.120
and you know that I'm
traveling tomorrow to New York,

00:34:14.120 --> 00:34:16.485
it's kind of simple to
do that on my calendar

00:34:16.485 --> 00:34:17.820
and create an event.

00:34:17.820 --> 00:34:21.760
How do you go from
robotic arms, trying

00:34:21.760 --> 00:34:25.150
to understand how to get
things, to an actual product?

00:34:25.150 --> 00:34:27.739
The question is, what is the
thought process behind it?

00:34:27.739 --> 00:34:27.985
APARNA CHENNAPRAGADA: Yeah.

00:34:27.985 --> 00:34:29.568
I'll give you the
short version of it.

00:34:29.568 --> 00:34:32.100
And, obviously, there's
a longer version of it.

00:34:32.100 --> 00:34:33.500
Wait for the medium post.

00:34:33.500 --> 00:34:35.440
But I think the
short version of it

00:34:35.440 --> 00:34:38.730
is, to echo one
thing JG said, you

00:34:38.730 --> 00:34:41.199
want to pick problems
that are easy for machines

00:34:41.199 --> 00:34:42.659
and hard for humans.

00:34:42.659 --> 00:34:45.120
So AI plus machine
learning is not

00:34:45.120 --> 00:34:47.719
going to turn a non-problem
into a real problem

00:34:47.719 --> 00:34:49.840
that people need solving.

00:34:49.840 --> 00:34:53.449
It's like, you can take
Christopher Nolan and Ben

00:34:53.449 --> 00:34:56.697
Affleck, and you can still end
up with Batman Versus Superman.

00:34:56.697 --> 00:34:59.030
So you want to make sure that
the problem you're solving

00:34:59.030 --> 00:35:00.400
is a real one.

00:35:00.400 --> 00:35:02.760
Many of our failures,
even internally

00:35:02.760 --> 00:35:06.130
and external, like frenzy
around bots and AI,

00:35:06.130 --> 00:35:09.310
is when you kid yourself that
the problem needs solving.

00:35:09.310 --> 00:35:12.400
And the second one, the
second quick insight there,

00:35:12.400 --> 00:35:15.809
is that you also want to
build an iterative model.

00:35:15.809 --> 00:35:18.100
That is, you want to kind of
start small, and say, hey,

00:35:18.100 --> 00:35:19.860
travel needs some assistance.

00:35:19.860 --> 00:35:22.740
What are the top five things
that people need help with?

00:35:22.740 --> 00:35:25.142
And see which of these
things can scale.

00:35:25.142 --> 00:35:26.850
JEFF DEAN: I would
add one thing to that,

00:35:26.850 --> 00:35:29.160
which is, often,
we're doing research

00:35:29.160 --> 00:35:31.250
on a particular kind of problem.

00:35:31.250 --> 00:35:34.700
And then, when we have
something we think is useful,

00:35:34.700 --> 00:35:37.760
we'll share that internally,
as presentations or whatever,

00:35:37.760 --> 00:35:39.740
and maybe highlight
a few places where

00:35:39.740 --> 00:35:42.180
we think this kind of
technology could be used.

00:35:42.180 --> 00:35:45.800
And that's sort of a good way
to inform the product designers

00:35:45.800 --> 00:35:49.420
about what kinds of things
are now possible that

00:35:49.420 --> 00:35:50.720
didn't used to be possible.

00:35:50.720 --> 00:35:51.428
TOM SIMONITE: OK.

00:35:51.428 --> 00:35:54.250
Let's have another question
from the press section up there.

00:35:54.250 --> 00:35:54.990
AUDIENCE: Yeah.

00:35:54.990 --> 00:35:59.640
There's a lot of talk, lately,
about sort of a fear of AI.

00:35:59.640 --> 00:36:04.680
Elon Musk likened it
to summoning the demon.

00:36:04.680 --> 00:36:07.580
Whether that's overblown
or not, whether it's

00:36:07.580 --> 00:36:10.360
perception versus
reality, there seems

00:36:10.360 --> 00:36:13.990
to be a lot of mistrust
or fear of going

00:36:13.990 --> 00:36:15.140
too far in this direction.

00:36:15.140 --> 00:36:18.100
How much stock
you put into that?

00:36:18.100 --> 00:36:22.190
And how do you win the
trust of the public, when

00:36:22.190 --> 00:36:24.712
you show experiments
like the robot arm thing

00:36:24.712 --> 00:36:26.670
on the keynote, which
was really cool, but sort

00:36:26.670 --> 00:36:29.392
of simultaneously
creepy at the same time?

00:36:29.392 --> 00:36:31.350
JOHN GIANNANDREA: So I
get this question a lot.

00:36:31.350 --> 00:36:34.544
I think there's
this notion that's

00:36:34.544 --> 00:36:36.460
been in the press for
the last couple of years

00:36:36.460 --> 00:36:38.040
about so-called
super intelligence,

00:36:38.040 --> 00:36:40.570
that somehow AI
will beget more AI,

00:36:40.570 --> 00:36:42.780
and then it will be exponential.

00:36:42.780 --> 00:36:46.392
I think researchers in the field
don't put much stock in that.

00:36:46.392 --> 00:36:48.350
I don't think we think
it's a real concern yet.

00:36:48.350 --> 00:36:49.933
In fact, I think
we're a long way away

00:36:49.933 --> 00:36:51.040
from it being a concern.

00:36:51.040 --> 00:36:53.630
There are some
researchers who actually

00:36:53.630 --> 00:36:55.110
think about these
ethical problems,

00:36:55.110 --> 00:36:56.595
and think about
AI safety, and we

00:36:56.595 --> 00:36:57.845
think that's really important.

00:36:57.845 --> 00:37:00.450
And we work on this
stuff with them,

00:37:00.450 --> 00:37:01.980
and we support
that kind of work.

00:37:01.980 --> 00:37:06.460
But I think it's a concern that
is decades and decades away.

00:37:06.460 --> 00:37:08.200
It's also conflated
with the fact

00:37:08.200 --> 00:37:10.420
that people look at things
like robots learning

00:37:10.420 --> 00:37:12.420
to pick things up,
and that's somehow

00:37:12.420 --> 00:37:14.940
inherently scary to people.

00:37:14.940 --> 00:37:16.800
I think it's our job,
when we bring products

00:37:16.800 --> 00:37:19.050
to market, to do it
in a thoughtful way

00:37:19.050 --> 00:37:21.430
that people find
genuinely useful.

00:37:21.430 --> 00:37:26.195
So a good example I would give
you is, in Google products,

00:37:26.195 --> 00:37:28.320
when you're looking for a
place, like a coffee shop

00:37:28.320 --> 00:37:30.740
or something, we'll
show you when it's busy.

00:37:30.740 --> 00:37:34.200
And that's the product of
fairly advanced machine learning

00:37:34.200 --> 00:37:36.640
that takes aggregate signals
in a privacy-preserving way

00:37:36.640 --> 00:37:38.390
and says, yeah, this
coffee shop is really

00:37:38.390 --> 00:37:39.680
busy on a Saturday morning.

00:37:39.680 --> 00:37:41.660
That doesn't seem
scary to me, right?

00:37:41.660 --> 00:37:46.270
That doesn't seem
anything like a bad thing

00:37:46.270 --> 00:37:47.750
to bring into the world.

00:37:47.750 --> 00:37:50.410
So I think there's a bit of a
disconnect between the somewhat

00:37:50.410 --> 00:37:52.990
extended hype, and the
actual use of this technology

00:37:52.990 --> 00:37:54.022
in everyday products.

00:37:54.022 --> 00:37:54.730
TOM SIMONITE: OK.

00:37:54.730 --> 00:37:55.376
Next question.

00:37:55.376 --> 00:37:56.290
AUDIENCE: Thank you.

00:37:56.290 --> 00:37:58.750
So given Google's
source of revenue

00:37:58.750 --> 00:38:02.140
and the high use of
ad blockers, is there

00:38:02.140 --> 00:38:04.520
any possibility of
using machine learning

00:38:04.520 --> 00:38:07.900
to maybe ensure that the
appropriate ads are served?

00:38:07.900 --> 00:38:10.040
Or if there's multiple
versions of the same ad,

00:38:10.040 --> 00:38:12.450
that the ad that
would apply most to me

00:38:12.450 --> 00:38:14.665
would be served to me,
and to a different user,

00:38:14.665 --> 00:38:16.682
a different version,
and things like that?

00:38:16.682 --> 00:38:17.894
Is that on the roadmap?

00:38:17.894 --> 00:38:18.560
JEFF DEAN: Yeah.

00:38:18.560 --> 00:38:20.221
I think, in general,
there's a lot

00:38:20.221 --> 00:38:21.720
of potential
applications of machine

00:38:21.720 --> 00:38:24.110
learning to advertising.

00:38:24.110 --> 00:38:25.710
Google has actually
been using machine

00:38:25.710 --> 00:38:29.580
learning in our advertising
system for more than a decade.

00:38:29.580 --> 00:38:34.080
And I think one of the
things about deciding

00:38:34.080 --> 00:38:35.660
what ads to show
to users is, you

00:38:35.660 --> 00:38:38.250
want them to be relevant
and useful to that user.

00:38:38.250 --> 00:38:40.770
And it's better to
not show an ad at all,

00:38:40.770 --> 00:38:44.999
if you don't have something
that seems plausibly relevant.

00:38:44.999 --> 00:38:47.290
And that's always been Google's
advertising philosophy.

00:38:47.290 --> 00:38:51.270
And other websites on the
web don't necessarily quite

00:38:51.270 --> 00:38:53.917
have the same balance,
in that respect.

00:38:53.917 --> 00:38:56.250
But I do think there's plenty
of opportunity to continue

00:38:56.250 --> 00:38:59.440
to improve advertising
systems and make them better,

00:38:59.440 --> 00:39:03.140
so that you see less ads, but
they're actually more useful.

00:39:03.140 --> 00:39:03.870
TOM SIMONITE: OK.

00:39:03.870 --> 00:39:05.626
Next question from at the top.

00:39:05.626 --> 00:39:08.250
JACK CLARK: Jack Clark
with Bloomberg News.

00:39:08.250 --> 00:39:12.470
So how do you
differentiate to the user

00:39:12.470 --> 00:39:17.810
between a sponsored advert, and
one that is provided by your AI

00:39:17.810 --> 00:39:18.770
naturally?

00:39:18.770 --> 00:39:21.890
How do I know that the burger
joint you're suggesting

00:39:21.890 --> 00:39:26.410
is like a paid-for link,
or is it a genuine link?

00:39:26.410 --> 00:39:27.970
JEFF DEAN: So in
our user interfaces,

00:39:27.970 --> 00:39:30.820
we always clearly
delimit advertisements.

00:39:30.820 --> 00:39:33.640
And in general, all
ads that we show

00:39:33.640 --> 00:39:36.240
are selected algorithmically
by our systems.

00:39:36.240 --> 00:39:38.730
They're not like, you
can just give us an ad,

00:39:38.730 --> 00:39:40.710
and we will always
show it to someone.

00:39:40.710 --> 00:39:43.320
We always decide what
is the likelihood

00:39:43.320 --> 00:39:45.250
that this ad is going
to be useful to someone,

00:39:45.250 --> 00:39:48.070
before we decide to show
that advertiser's ad.

00:39:48.070 --> 00:39:51.126
JACK CLARK: Does this extend to
stuff like Google Home, where

00:39:51.126 --> 00:39:53.840
it will say, this is
a sponsored restaurant

00:39:53.840 --> 00:39:57.260
we're going to send you to.

00:39:57.260 --> 00:39:58.802
JEFF DEAN: I don't
know that product.

00:39:58.802 --> 00:40:00.301
JOHN GIANNANDREA:
I mean, we haven't

00:40:00.301 --> 00:40:01.420
launched Google Home yet.

00:40:01.420 --> 00:40:05.590
So a lot of these product
decisions are still to be made.

00:40:05.590 --> 00:40:08.410
I think we do, as
a general rule,

00:40:08.410 --> 00:40:10.820
clearly identify when
something is sponsored

00:40:10.820 --> 00:40:13.250
versus when it's organic.

00:40:13.250 --> 00:40:13.990
TOM SIMONITE: OK.

00:40:13.990 --> 00:40:15.190
Next question here.

00:40:15.190 --> 00:40:15.920
AUDIENCE: Hi.

00:40:15.920 --> 00:40:19.360
This is a question
for Jeff Dean.

00:40:19.360 --> 00:40:22.101
I'm very much intrigued by
the Google Brain project

00:40:22.101 --> 00:40:22.850
that you're doing.

00:40:22.850 --> 00:40:25.880
Very cool t-shirt.

00:40:25.880 --> 00:40:28.420
The question is, what
is the road map of that,

00:40:28.420 --> 00:40:32.880
and how does it relate to
the point of singularity?

00:40:32.880 --> 00:40:34.860
JEFF DEAN: Aha.

00:40:34.860 --> 00:40:41.110
So the road map of-- this is
sort of the project code name

00:40:41.110 --> 00:40:43.060
for the team that I work on.

00:40:43.060 --> 00:40:45.470
Basically, the
team was developed

00:40:45.470 --> 00:40:49.900
to investigate the use
of advanced methods

00:40:49.900 --> 00:40:54.000
in machine learning to solve
difficult problems in AI.

00:40:54.000 --> 00:40:57.470
And we're continuing to
work on pushing the state

00:40:57.470 --> 00:40:59.220
of the art in that area.

00:40:59.220 --> 00:41:01.720
And I think that means working
in lots of different areas,

00:41:01.720 --> 00:41:04.750
building the right kinds
of hardware with TPUs,

00:41:04.750 --> 00:41:07.620
building the right systems
infrastructure with things

00:41:07.620 --> 00:41:08.710
like TensorFlow.

00:41:08.710 --> 00:41:10.660
Solving the right
research problems

00:41:10.660 --> 00:41:14.950
that are not
connected to products,

00:41:14.950 --> 00:41:17.810
and then figuring out ways
in which machine learning can

00:41:17.810 --> 00:41:22.580
be used to advance
different kinds of fields,

00:41:22.580 --> 00:41:25.220
as we solve different
problems along the road.

00:41:25.220 --> 00:41:27.780
I'm not a big believer
in the singularity.

00:41:27.780 --> 00:41:30.620
I think all exponentials
look like exponentials

00:41:30.620 --> 00:41:34.802
at the beginning, but then
they run out of stuff.

00:41:34.802 --> 00:41:35.510
TOM SIMONITE: OK.

00:41:35.510 --> 00:41:36.550
Thanks for the question.

00:41:36.550 --> 00:41:38.039
Back to the pressbox.

00:41:38.039 --> 00:41:39.080
STEVEN MAX PATTERSON: Hi.

00:41:39.080 --> 00:41:41.550
Steven Max Patterson, IDG.

00:41:41.550 --> 00:41:45.460
I was looking at Google
Home and Google Assistant,

00:41:45.460 --> 00:41:50.480
and it looks like it's
really a platform.

00:41:50.480 --> 00:41:53.430
And it's a composite
of other platforms,

00:41:53.430 --> 00:41:58.130
like the Knowledge Graph, Google
Cloud Speech, Google machine

00:41:58.130 --> 00:42:00.262
learning, the Awareness API.

00:42:00.262 --> 00:42:06.460
Is this a feature that other
consumer device manufacturers

00:42:06.460 --> 00:42:09.960
could include, and is that the
intent and direction of Google,

00:42:09.960 --> 00:42:11.712
is to make this a platform?

00:42:13.415 --> 00:42:14.790
JOHN GIANNANDREA:
It's definitely

00:42:14.790 --> 00:42:18.350
the case that most of
our machine learning APIs

00:42:18.350 --> 00:42:21.590
are migrating to the cloud
platform, which enables people

00:42:21.590 --> 00:42:25.290
to use, for example, our speech
capabilities in other products.

00:42:25.290 --> 00:42:27.710
I think the Google Assistant
is intended to be, actually,

00:42:27.710 --> 00:42:29.660
a holistic product
delivered from Google.

00:42:29.660 --> 00:42:30.380
That makes sense.

00:42:30.380 --> 00:42:32.390
But it may make
sense to syndicate

00:42:32.390 --> 00:42:34.140
that to other manufacturers
at some point.

00:42:34.140 --> 00:42:36.080
We don't have any
plans to do that today.

00:42:36.080 --> 00:42:37.496
But in general,
we're trying to be

00:42:37.496 --> 00:42:39.990
as open as we can with
the component pieces

00:42:39.990 --> 00:42:41.390
that you just
mentioned, and make

00:42:41.390 --> 00:42:43.960
them available as Cloud
APIs, and in many cases,

00:42:43.960 --> 00:42:45.722
as open source
solutions as well.

00:42:45.722 --> 00:42:46.430
JEFF DEAN: Right.

00:42:46.430 --> 00:42:47.930
I think one of the
things about that

00:42:47.930 --> 00:42:49.850
is, making those
individual pieces available

00:42:49.850 --> 00:42:53.320
enables everyone in the world
to take advantage of some

00:42:53.320 --> 00:42:55.200
of the machine learning
research we've done,

00:42:55.200 --> 00:42:57.550
and be able to do things
like label images,

00:42:57.550 --> 00:42:59.434
or do speech
recognition really well.

00:42:59.434 --> 00:43:00.850
And then they can
go off and build

00:43:00.850 --> 00:43:03.630
really cool, amazing things
that aren't necessarily

00:43:03.630 --> 00:43:05.800
the kinds of things
we're working on.

00:43:05.800 --> 00:43:07.430
JOHN GIANNANDREA: Yeah, and many
companies are doing this today.

00:43:07.430 --> 00:43:08.870
They're using our
translate APIs.

00:43:08.870 --> 00:43:12.417
They're using our Cloud
Speech APIs today.

00:43:12.417 --> 00:43:13.250
TOM SIMONITE: Right.

00:43:13.250 --> 00:43:15.889
We have time for one last quick
question from this mic here.

00:43:15.889 --> 00:43:16.430
AUDIENCE: Hi.

00:43:16.430 --> 00:43:18.420
I'm [INAUDIBLE].

00:43:18.420 --> 00:43:22.290
John, you said that you
would declare summer

00:43:22.290 --> 00:43:25.550
if, in language
understanding, it

00:43:25.550 --> 00:43:30.200
would be able to translate
from one paragraph in English

00:43:30.200 --> 00:43:32.150
to another paragraph in English.

00:43:32.150 --> 00:43:35.510
Don't you think that making
that possible requires

00:43:35.510 --> 00:43:40.210
really complete understanding
of the world, and everything

00:43:40.210 --> 00:43:44.100
that's going on, just to
catch the emotional level that

00:43:44.100 --> 00:43:47.820
is in the paragraph, or even
the physical understanding

00:43:47.820 --> 00:43:50.150
of the world around us?

00:43:50.150 --> 00:43:52.380
JOHN GIANNANDREA: Yeah, I do.

00:43:52.380 --> 00:43:55.489
I use that example because
it is really, really hard.

00:43:55.489 --> 00:43:58.030
So I don't think we're going to
be done for many, many years.

00:43:58.030 --> 00:44:00.070
I think there's a
lot of work to do.

00:44:00.070 --> 00:44:02.117
We built the Google
Knowledge Graph, in part,

00:44:02.117 --> 00:44:03.950
to answer that question,
so that we actually

00:44:03.950 --> 00:44:05.540
had some semantic
understanding of at least

00:44:05.540 --> 00:44:07.690
the things in the world, and
some of the relationships

00:44:07.690 --> 00:44:08.231
between them.

00:44:08.231 --> 00:44:09.710
But yeah, it's a
very hard problem.

00:44:09.710 --> 00:44:11.550
And I used that
example because it's

00:44:11.550 --> 00:44:13.500
pretty clear we won't
be done for a long time.

00:44:13.500 --> 00:44:14.839
TOM SIMONITE: OK.

00:44:14.839 --> 00:44:16.630
Sorry, there's no time
for other questions.

00:44:16.630 --> 00:44:17.900
Thanks for the question.

00:44:17.900 --> 00:44:20.660
A good forward-looking
note to end on.

00:44:20.660 --> 00:44:23.050
We'll see how it works
out over the coming years.

00:44:23.050 --> 00:44:25.250
Thank you for joining
me, all of you on stage,

00:44:25.250 --> 00:44:29.730
and thanks for the questions
and coming for the session.

00:44:29.730 --> 00:44:32.780
[MUSIC PLAYING]

