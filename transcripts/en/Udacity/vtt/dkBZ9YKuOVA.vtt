WEBVTT
Kind: captions
Language: en

00:00:00.280 --> 00:00:02.480
Speaking of solutions, this is the last little bit of thing

00:00:02.480 --> 00:00:05.630
that you need to know. And that is. This defines a

00:00:05.630 --> 00:00:08.480
problem. But, what we also want to have, whenever we have

00:00:08.480 --> 00:00:12.900
a problem. Is a solution. So, the solution to the Markov

00:00:12.900 --> 00:00:16.650
Decision Process, is something called a policy. And, what a policy

00:00:16.650 --> 00:00:20.140
does. Is, it's a function, that takes in a state. And

00:00:20.140 --> 00:00:23.290
returns an action, in other words, for any given state that

00:00:23.290 --> 00:00:25.470
you're in, it tells you the action that you should take.

00:00:25.470 --> 00:00:26.720
&gt;&gt; Like as a hint?

00:00:26.720 --> 00:00:28.840
&gt;&gt; No, it just tells you, this is the at.

00:00:28.840 --> 00:00:30.890
Well, I mean, I suppose you don't have to do

00:00:30.890 --> 00:00:33.330
it, but the way we think about Markov Decision Processes,

00:00:33.330 --> 00:00:35.140
is that this is the action that will be taken.

00:00:35.140 --> 00:00:37.450
&gt;&gt; I see, so it's more of an order.

00:00:37.450 --> 00:00:39.440
&gt;&gt; Yes, it's a command. Okay.

00:00:39.440 --> 00:00:41.770
&gt;&gt; So that's all a policy is. A policy is

00:00:41.770 --> 00:00:44.270
solution to a Markov Decision Process. And there is a

00:00:44.270 --> 00:00:47.380
special policy, which I'm writing here as policy star, or

00:00:47.380 --> 00:00:51.080
the optimal policy, and that is the policy that maximizes

00:00:51.080 --> 00:00:55.530
your long-term expected reward. So if all the policies you

00:00:55.530 --> 00:00:57.870
could take, of all the decisions you might take, this

00:00:57.870 --> 00:01:00.850
is the policy that optimizes the amount of reward that

00:01:00.850 --> 00:01:04.480
you're going to receive or expect to receive over your lifetime.

00:01:04.480 --> 00:01:06.690
&gt;&gt; So, like, at the end?

00:01:06.690 --> 00:01:08.240
&gt;&gt; Well, at yeah, at the end, or at

00:01:08.240 --> 00:01:10.840
any given point in time, how much reward you're receiving.

00:01:10.840 --> 00:01:12.640
&gt;From the Markov Decision Process point of view, there doesn't

00:01:12.640 --> 00:01:16.530
have to be an end. Okay. Though in this example,

00:01:16.530 --> 00:01:18.750
you don't get anything, and then at the end, you get paid off.

00:01:18.750 --> 00:01:21.120
&gt;&gt; Right, or unpaid off.

00:01:21.120 --> 00:01:21.780
&gt;&gt; Right.

00:01:21.780 --> 00:01:23.630
&gt;&gt; If you fall into the red square. So

00:01:23.630 --> 00:01:26.910
actually, your question points out something very important here. I

00:01:26.910 --> 00:01:29.210
mentioned earlier when I talked about the three kinds of

00:01:29.210 --> 00:01:31.720
learning that there, supervised learning and reinforced learning were sort

00:01:31.720 --> 00:01:34.520
of. Similar, except that instead of getting Ys and

00:01:34.520 --> 00:01:37.590
Xs we were given Ys and, Xs and Zs. And

00:01:37.590 --> 00:01:40.250
this is exactly what's happening here. Here what we would

00:01:40.250 --> 00:01:42.290
like to have if we wanted to learn a policy

00:01:42.290 --> 00:01:45.940
is a bunch of sa pairs as training examples. Well

00:01:45.940 --> 00:01:48.210
here's the state and the action you should've took, taken, here's

00:01:48.210 --> 00:01:50.110
another state and the action you should've taken, so on and

00:01:50.110 --> 00:01:53.170
so forth. And then we would learn a function, the policy,

00:01:53.170 --> 00:01:57.150
that maps states to actions. But what we actually see

00:01:57.150 --> 00:02:00.250
in the reinforcement learning world, in the Markov Decision Process world,

00:02:00.250 --> 00:02:04.100
is we see states, actions, and then the rewards that we

00:02:04.100 --> 00:02:07.390
received. And so in fact, this problem of seeing a sequence

00:02:07.390 --> 00:02:11.250
of states, actions, and rewards. It's very different

00:02:11.250 --> 00:02:13.340
from the problem of being told. This is the

00:02:13.340 --> 00:02:15.700
correct action to take to maximize a function.

00:02:15.700 --> 00:02:17.165
Or find a function that maps from state to

00:02:17.165 --> 00:02:19.710
action. Instead, we say well, if you're in

00:02:19.710 --> 00:02:21.860
this state, and you take this action, this is

00:02:21.860 --> 00:02:23.540
the reward that you would see. And then

00:02:23.540 --> 00:02:26.060
from that, we need to find the optimal action.

00:02:26.060 --> 00:02:29.500
&gt;&gt; So Pi star is being the F from that previous slide?

00:02:29.500 --> 00:02:29.920
&gt;&gt; Right.

00:02:29.920 --> 00:02:32.610
&gt;&gt; And R is being Z?

00:02:32.610 --> 00:02:35.680
&gt;&gt; Yes. And y is being a.

00:02:35.680 --> 00:02:39.040
&gt;&gt; And s is being x or x is being s

00:02:39.040 --> 00:02:40.470
&gt;&gt; Got you.

00:02:40.470 --> 00:02:40.930
&gt;&gt; Right.

00:02:40.930 --> 00:02:42.900
&gt;&gt; So but, I'm, okay I'm a little confused

00:02:42.900 --> 00:02:45.880
about this notion of a policy. So we have

00:02:45.880 --> 00:02:47.720
the, the, the thing we tried to do to

00:02:47.720 --> 00:02:50.820
get the goals was up, up, right, right, right. Yes.

00:02:50.820 --> 00:02:54.140
&gt;&gt; I don't see how to capture that as a policy.

00:02:54.140 --> 00:02:57.720
&gt;&gt; It's actually fairly straightforward. What a policy would say is:

00:02:57.720 --> 00:02:59.180
What state are you in? Tell me what action you

00:02:59.180 --> 00:03:02.420
should take. So, the policy, basically is this: When you're in

00:03:02.420 --> 00:03:06.430
the state, start, the start state, the action you should take

00:03:06.430 --> 00:03:10.540
is up. And it would have a mapping. For every state

00:03:10.540 --> 00:03:13.200
that you might see, whether it's this state, this state,

00:03:13.200 --> 00:03:17.030
this state, this state, this state, this state, this state, or

00:03:17.030 --> 00:03:19.890
even these two states, and it will tell you what action

00:03:19.890 --> 00:03:22.910
you should take. And that's what a policy is. A policy,

00:03:22.910 --> 00:03:26.220
very simply, is nothing more than a function that tells you what

00:03:26.220 --> 00:03:29.550
action to take at every, in any state you happen to come across.

00:03:29.550 --> 00:03:31.750
&gt;&gt; Okay, but the, but the. The question that

00:03:31.750 --> 00:03:34.510
you asked before was about up, up, right, right, right.

00:03:34.510 --> 00:03:34.990
&gt;&gt; Mm hm.

00:03:34.990 --> 00:03:38.470
&gt;&gt; And, it seems like, because of

00:03:38.470 --> 00:03:40.850
the stochastic transitions. You might not be in

00:03:40.850 --> 00:03:43.340
the same state. Like, you don't know what

00:03:43.340 --> 00:03:45.370
state you're in, when you take those actions.

00:03:45.370 --> 00:03:47.910
&gt;&gt; No, so, one of the things for what we're talking about

00:03:47.910 --> 00:03:49.620
here, for the Markov Decision Process.

00:03:49.620 --> 00:03:52.780
Is, there're states, there're actions, there're rewards.

00:03:52.780 --> 00:03:56.540
You always know what state you're in, and you know what reward you receive.

00:03:56.540 --> 00:04:00.420
&gt;&gt; So does that mean you can't do up, up right right right?

00:04:00.420 --> 00:04:02.630
&gt;&gt; Well, the way it would work in a Markov Decision Process,

00:04:02.630 --> 00:04:06.260
so what you're describing is is what's often called a plan. You

00:04:06.260 --> 00:04:09.780
know, it's, tell me what sequence of actions I should take from

00:04:09.780 --> 00:04:13.020
here. What Markov Decision Process does and what a pr, a policy

00:04:13.020 --> 00:04:16.410
does is it doesn't tell you what sequence of actions to take from a particular

00:04:16.410 --> 00:04:20.589
state. It tells you what action to take in a particular state. You will then

00:04:20.589 --> 00:04:23.560
end up in another state because of

00:04:23.560 --> 00:04:25.820
the transition model, the transition function. And then

00:04:25.820 --> 00:04:27.080
when you're in that state you ask the

00:04:27.080 --> 00:04:30.330
policy what actions should I take now? Okay.

00:04:30.330 --> 00:04:33.440
&gt;&gt; Right, so this is actually a key point.

00:04:34.890 --> 00:04:38.490
Although we talked about it in the language of planning,

00:04:38.490 --> 00:04:41.190
which is very common for the people who di, for example

00:04:41.190 --> 00:04:43.500
take any ag course, the thing about this in terms of planning,

00:04:43.500 --> 00:04:45.510
what are the things that I can do to accomplish my

00:04:45.510 --> 00:04:49.140
goals? The Markov Decision Process way of thinking about it, the reinforcement

00:04:49.140 --> 00:04:51.980
way of thinking about it, or the typical reinforcement learning way

00:04:51.980 --> 00:04:55.180
of thinking about it, really doesn't talk about plans directly. But instead,

00:04:55.180 --> 00:04:59.660
talks about policies. Which from which you can infer a plan, but

00:04:59.660 --> 00:05:03.540
this has the advantage that it tells you what to do everywhere.

00:05:03.540 --> 00:05:07.390
And it's robust to the underlying stochastic of the word. World.

00:05:07.390 --> 00:05:11.090
&gt;&gt; So, is it clear that's all you need to be able to behave well.

00:05:11.090 --> 00:05:15.060
&gt;&gt; Well, it's certainly the case, that if you have a policy and that policy

00:05:15.060 --> 00:05:18.370
is optimal, it does tell you what to do, no matter what situation you're in.

00:05:18.370 --> 00:05:19.215
&gt;&gt; 'Kay.

00:05:19.215 --> 00:05:21.630
&gt;&gt; And so, if you have that, then that's definitely

00:05:21.630 --> 00:05:23.780
all you need to behave well. But I mean could it

00:05:23.780 --> 00:05:25.540
be that you wanted to do something like up, up,

00:05:25.540 --> 00:05:28.280
right, right, right which you cant write down as a policy?

00:05:28.280 --> 00:05:28.650
&gt;&gt; And why cant

00:05:28.650 --> 00:05:29.910
you write that down as a policy?

00:05:29.910 --> 00:05:33.070
&gt;&gt; Because the policies are only telling you what action to do as a function

00:05:33.070 --> 00:05:37.580
of the state not sort of like how far along you are in the sequence.

00:05:37.580 --> 00:05:40.880
&gt;&gt; Right unless, of course, you fold that into your state some how. But thats

00:05:40.880 --> 00:05:45.350
exactly right, the way to think about this is. The idea of coming up with

00:05:45.350 --> 00:05:49.870
a concrete plan of what to do for the next 20 time steps is different

00:05:49.870 --> 00:05:53.800
from the problem of whatever step I happen to be in, whatever state I happen

00:05:53.800 --> 00:05:55.350
to be in, what's the next best thing

00:05:55.350 --> 00:05:58.590
I can do? And just always asking that question.

00:05:58.590 --> 00:05:59.340
&gt;&gt; Hm.

00:05:59.340 --> 00:06:01.110
&gt;&gt; If you always ask that question,

00:06:01.110 --> 00:06:03.230
that will induce a sequence, but that sequence

00:06:03.230 --> 00:06:05.580
is actually dependent upon the set of states

00:06:05.580 --> 00:06:07.960
that you see. Whereas in the other case

00:06:07.960 --> 00:06:10.270
where we wrote down a particular policy, you'll

00:06:10.270 --> 00:06:12.250
notice that was only dependent upon the state

00:06:12.250 --> 00:06:16.030
you started in and it had to ignore the states that you saw along the way.

00:06:17.033 --> 00:06:18.800
&gt;&gt; And the only way to fix that would be

00:06:18.800 --> 00:06:21.220
to say, well, after I've taken an action, let me look at the state

00:06:21.220 --> 00:06:24.430
I'm in and see if I should do something different with it. But if

00:06:24.430 --> 00:06:28.310
you're going to do that, then why are you trying to compute the complete set

00:06:28.310 --> 00:06:32.290
of states? Or I'm sorry, the complete set of actions that you might take.

00:06:32.290 --> 00:06:33.350
&gt;&gt; Okay.

00:06:33.350 --> 00:06:35.390
&gt;&gt; Okay, so there you go. Now, a lot

00:06:35.390 --> 00:06:38.380
of what we're going to be talking about next Michael, is,

00:06:38.380 --> 00:06:40.930
given that we have MDP, we have this Markov Decision

00:06:40.930 --> 00:06:44.080
Process defined like this. How do we go from this

00:06:44.080 --> 00:06:47.080
problem definition to finding a good policy, and

00:06:47.080 --> 00:06:50.090
in particular, finding the optimal policy? That makes sense.

00:06:50.090 --> 00:06:51.680
&gt;&gt; Good. And there you go.

