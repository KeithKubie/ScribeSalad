WEBVTT
Kind: captions
Language: en

00:00:01.089 --> 00:00:02.630
RACHAD ALAO: My name
is Rachad, and I

00:00:02.630 --> 00:00:05.560
lead Android's media framework.

00:00:05.560 --> 00:00:08.020
Media is an integral
part of our life,

00:00:08.020 --> 00:00:11.230
bringing us together,
enabling us to share emotions

00:00:11.230 --> 00:00:14.440
through music,
pictures and video.

00:00:14.440 --> 00:00:17.320
Today we'll discuss how to
create these experiences

00:00:17.320 --> 00:00:20.480
and share them with Android.

00:00:20.480 --> 00:00:24.660
First, Glenn and Andy
from the media team

00:00:24.660 --> 00:00:27.110
will start with a
little story on how

00:00:27.110 --> 00:00:30.990
to build great audio
experiences with Android.

00:00:30.990 --> 00:00:34.200
Then Lajos and
Ollie will continue

00:00:34.200 --> 00:00:35.960
with great video experiences.

00:00:35.960 --> 00:00:39.910
And finally, Eddy
will introduce us

00:00:39.910 --> 00:00:46.230
to a very compelling way of
building camera application.

00:00:46.230 --> 00:00:49.590
Now, please welcome
Glenn and Andy for audio.

00:00:49.590 --> 00:00:55.700
[APPLAUSE]

00:00:55.700 --> 00:00:58.990
GLENN KASTEN: Hi, all you
audiophiles, videophiles,

00:00:58.990 --> 00:01:00.170
and cameraphiles.

00:01:00.170 --> 00:01:02.970
I'm Glenn Kasten
from Android audio,

00:01:02.970 --> 00:01:05.390
and my focus is on performance.

00:01:05.390 --> 00:01:10.580
So given performance, I
love integers in audio.

00:01:10.580 --> 00:01:13.870
Integers are simple,
fast, and reliable.

00:01:13.870 --> 00:01:16.260
You just can't go
wrong with integers.

00:01:16.260 --> 00:01:18.830
In fact, some of my best
friends are integers.

00:01:18.830 --> 00:01:23.890
I'd like you to meet two of my
favorite buddies-- Min and Max.

00:01:27.100 --> 00:01:29.530
When it comes to audio,
integers are ideal.

00:01:29.530 --> 00:01:30.910
Fixed-point, too.

00:01:30.910 --> 00:01:35.570
16-bit PCM is where it's at.

00:01:35.570 --> 00:01:36.850
I just wish it didn't clip.

00:01:39.750 --> 00:01:42.320
That's why in Android
the entire data pipeline

00:01:42.320 --> 00:01:46.590
is almost all 16-bit PCM.

00:01:46.590 --> 00:01:48.160
ANDY HUNG: Uh, wait
a minute Glenn.

00:01:48.160 --> 00:01:49.730
That's so 1980s!

00:01:49.730 --> 00:01:52.760
16-bit PCM was fine
for the days of CDs.

00:01:52.760 --> 00:01:53.770
But come on!

00:01:53.770 --> 00:01:56.290
It's 2014!

00:01:56.290 --> 00:01:58.480
In L Developer Preview
we're introducing

00:01:58.480 --> 00:02:01.950
floating-point audio, starting
with the audio track data.

00:02:01.950 --> 00:02:04.620
We've already updated
part of the pipeline,

00:02:04.620 --> 00:02:06.399
and now are planning
out the rest.

00:02:06.399 --> 00:02:07.940
There'll be significant
benefits when

00:02:07.940 --> 00:02:11.030
the entire pipeline
is converted.

00:02:11.030 --> 00:02:12.820
GLENN KASTEN: But
why floating point?

00:02:12.820 --> 00:02:14.475
That's so inefficient!

00:02:14.475 --> 00:02:16.350
ANDY HUNG: Floating-point
can be just as fast

00:02:16.350 --> 00:02:18.390
as integers on
modern processors--

00:02:18.390 --> 00:02:21.540
especially with [? Cindy ?]
and auto-order execution.

00:02:21.540 --> 00:02:23.680
Floating-point improves
quality and helps

00:02:23.680 --> 00:02:24.890
with the clipping, too.

00:02:24.890 --> 00:02:29.040
Check out our I/O Bytes "Will
it Float?" to learn more

00:02:29.040 --> 00:02:30.230
GLENN KASTEN: Will it float?

00:02:30.230 --> 00:02:32.412
Hmm.

00:02:32.412 --> 00:02:33.370
ANDY HUNG: Don't worry.

00:02:33.370 --> 00:02:36.322
The audio endpoints
are still integer.

00:02:36.322 --> 00:02:37.280
GLENN KASTEN: Oh, whew!

00:02:37.280 --> 00:02:41.321
And the endpoints can
have more than 16 bits.

00:02:41.321 --> 00:02:41.820
Woo-hoo!

00:02:44.880 --> 00:02:48.160
Now, about that resampler box.

00:02:48.160 --> 00:02:50.470
Andy, you're not
changing that, I hope.

00:02:50.470 --> 00:02:53.370
Don't mess with my linear
interpolator, the fastest

00:02:53.370 --> 00:02:56.096
resampler in the West.

00:02:56.096 --> 00:02:57.720
ANDY HUNG: Glenn,
that's so old school.

00:02:57.720 --> 00:02:59.760
In L Developer
Preview we've worked

00:02:59.760 --> 00:03:01.830
to improve our resamplers.

00:03:01.830 --> 00:03:05.830
Better quality through
reduced aliasing.

00:03:05.830 --> 00:03:08.500
Check out this spectrogram
of a sine sweep.

00:03:08.500 --> 00:03:12.400
Before, on the top, shows
the aliasing and harmonics;

00:03:12.400 --> 00:03:14.100
and after, on the
bottom, is clear.

00:03:14.100 --> 00:03:16.860
We've also improved
at twice the speed.

00:03:16.860 --> 00:03:21.080
So we also dynamically
compute our resampling filters

00:03:21.080 --> 00:03:22.960
to adjust for your
sampling ratios.

00:03:30.935 --> 00:03:32.060
GLENN KASTEN: This clicker.

00:03:32.060 --> 00:03:32.560
There we go!

00:03:32.560 --> 00:03:33.251
OK.

00:03:33.251 --> 00:03:35.250
ANDY HUNG: This is all
built into the framework.

00:03:35.250 --> 00:03:39.010
So you can mix, play, and
record at different sample rates

00:03:39.010 --> 00:03:40.340
with great quality.

00:03:40.340 --> 00:03:42.750
But don't use resamplers
if you don't need to,

00:03:42.750 --> 00:03:45.344
because it consumes CPU
and adds to latency.

00:03:45.344 --> 00:03:46.260
GLENN KASTEN: Latency?

00:03:46.260 --> 00:03:48.400
Did somebody say latency?

00:03:48.400 --> 00:03:50.680
That is my favorite topic!

00:03:50.680 --> 00:03:52.750
I love latency!

00:03:52.750 --> 00:03:57.180
In fact, here comes one
of my favorite developers.

00:03:57.180 --> 00:04:00.400
CHRISTIAN HOWES: What do
you mean, you love latency?

00:04:00.400 --> 00:04:02.860
GLENN KASTEN: Oh, I mean
I love reducing latency.

00:04:02.860 --> 00:04:04.090
Sorry about that.

00:04:04.090 --> 00:04:06.710
Anyway, I'd like you to meet
Christian Howes from StarMaker

00:04:06.710 --> 00:04:08.100
Interactive.

00:04:08.100 --> 00:04:11.100
And I'm just going to
let him tell his story.

00:04:11.100 --> 00:04:14.820
CHRISTIAN HOWES: So StarMaker
is a global talent discovery

00:04:14.820 --> 00:04:16.160
and enablement network.

00:04:16.160 --> 00:04:19.329
We make singing games for your
mobile device and a platform

00:04:19.329 --> 00:04:22.040
for you to share your
talent with the world.

00:04:22.040 --> 00:04:25.390
Our goal is to make singing
fun again for everyone,

00:04:25.390 --> 00:04:28.590
and finding great talent
amongst our user base.

00:04:28.590 --> 00:04:30.610
But for me it's also personal.

00:04:30.610 --> 00:04:32.910
I'm a percussionist, a
professionally trained

00:04:32.910 --> 00:04:34.560
musician, and an educator.

00:04:34.560 --> 00:04:37.020
And I want to bring
the on-stage experience

00:04:37.020 --> 00:04:40.611
to people who wouldn't
normally have that opportunity.

00:04:40.611 --> 00:04:42.110
GLENN KASTEN: When
we first met, you

00:04:42.110 --> 00:04:43.818
didn't seem too happy
with Android audio.

00:04:43.818 --> 00:04:45.480
What was the problem, Christian?

00:04:45.480 --> 00:04:47.355
CHRISTIAN HOWES: Well,
part of the experience

00:04:47.355 --> 00:04:49.760
of being on stage is to
hear yourself singing,

00:04:49.760 --> 00:04:53.680
to be a part of the
music, inside that music.

00:04:53.680 --> 00:04:57.210
Anything that takes away from
that just ruins the experience.

00:04:57.210 --> 00:04:59.600
And I couldn't get that
feeling on Android.

00:04:59.600 --> 00:05:01.900
GLENN KASTEN: Can you
show us what you mean?

00:05:01.900 --> 00:05:03.650
CHRISTIAN HOWES: (VOICE
ECHOING) Um, here.

00:05:03.650 --> 00:05:03.910
Um, here.

00:05:03.910 --> 00:05:04.530
Let me see.

00:05:04.530 --> 00:05:05.300
Let me see.

00:05:05.300 --> 00:05:06.640
Can hear me now?

00:05:06.640 --> 00:05:07.990
Can you hear me now?

00:05:07.990 --> 00:05:09.110
GLENN KASTEN: What?

00:05:09.110 --> 00:05:11.772
Is that delay from Android?

00:05:11.772 --> 00:05:13.480
CHRISTIAN HOWES: Yes,
that's exactly what

00:05:13.480 --> 00:05:14.600
we're talking about.

00:05:14.600 --> 00:05:18.220
It's like singing to yourself
across the Grand Canyon.

00:05:18.220 --> 00:05:20.940
Hearing a delay while
you're singing just

00:05:20.940 --> 00:05:22.500
takes you out of the experience.

00:05:22.500 --> 00:05:23.470
GLENN KASTEN: But
come on, Christian,

00:05:23.470 --> 00:05:25.240
I heard a couple of
seconds of latency.

00:05:25.240 --> 00:05:26.310
That's ridiculous.

00:05:26.310 --> 00:05:26.945
That's crazy.

00:05:30.364 --> 00:05:33.860
[LAUGHTER]

00:05:33.860 --> 00:05:36.500
If you do a Google
search for Android audio,

00:05:36.500 --> 00:05:40.950
this is number five,
I believe-- or four.

00:05:40.950 --> 00:05:42.900
There have been some
wild rumors out there.

00:05:42.900 --> 00:05:47.894
Yes, the latency has been
high, but not this high.

00:05:47.894 --> 00:05:48.810
CHRISTIAN HOWES: Yeah.

00:05:48.810 --> 00:05:51.450
So folks, that demo was
a bit of an exaggeration,

00:05:51.450 --> 00:05:53.300
just for the explanation.

00:05:53.300 --> 00:05:54.370
But you get the point.

00:05:54.370 --> 00:05:55.760
The latency is annoying.

00:05:59.860 --> 00:06:03.170
So I'm going to do a quick demo
of what our app is actually

00:06:03.170 --> 00:06:04.890
like right now.

00:06:04.890 --> 00:06:06.270
I'm going to shut off my mic.

00:06:09.290 --> 00:06:11.479
[MUSIC PLAYING]

00:06:21.510 --> 00:06:23.260
GLENN KASTEN: So Christian, cut.

00:06:23.260 --> 00:06:24.180
Cut, cut, cut.

00:06:24.180 --> 00:06:25.240
Hold on a second.

00:06:25.240 --> 00:06:26.310
OK.

00:06:26.310 --> 00:06:28.440
The instrumentals were
great, but I'm sorry.

00:06:28.440 --> 00:06:31.260
I didn't even hear you at all.

00:06:31.260 --> 00:06:32.320
What's up?

00:06:32.320 --> 00:06:34.140
CHRISTIAN HOWES: Well,
that's the point.

00:06:34.140 --> 00:06:37.090
We had to disable the
real time feedback because

00:06:37.090 --> 00:06:38.290
of the latency.

00:06:38.290 --> 00:06:40.850
Users complained because
our most important feature

00:06:40.850 --> 00:06:43.010
is missing from the app.

00:06:43.010 --> 00:06:45.390
GLENN KASTEN: Well, it seems
like kind of a nice app.

00:06:45.390 --> 00:06:48.544
I mean it's almost fun.

00:06:48.544 --> 00:06:50.210
CHRISTIAN HOWES: Yeah,
but we're missing

00:06:50.210 --> 00:06:52.372
the "you" in the performance.

00:06:52.372 --> 00:06:53.330
GLENN KASTEN: I'll say.

00:06:53.330 --> 00:06:55.660
So how bad was the round
trip latency-- or rather,

00:06:55.660 --> 00:06:56.940
is the round trip latency?

00:06:56.940 --> 00:06:58.856
CHRISTIAN HOWES: Well,
it's not the 12 seconds

00:06:58.856 --> 00:06:59.710
that we saw earlier.

00:06:59.710 --> 00:07:00.770
It's not two seconds.

00:07:00.770 --> 00:07:02.290
But it is noticeable.

00:07:02.290 --> 00:07:05.580
We've measured 200
milliseconds or more.

00:07:05.580 --> 00:07:09.220
Enough that we ignored Android
completely until our users

00:07:09.220 --> 00:07:12.120
clamored loudly enough.

00:07:12.120 --> 00:07:14.170
When we finally
implemented the app,

00:07:14.170 --> 00:07:17.610
we had to disable this real time
monitoring of the user vocals.

00:07:17.610 --> 00:07:19.230
We've tried to account
for the latency

00:07:19.230 --> 00:07:21.480
and still make a good quality
recording for the user

00:07:21.480 --> 00:07:24.380
to share, but we still
had trouble with that too.

00:07:24.380 --> 00:07:28.300
GLENN KASTEN: Hey, Christian,
we can figure this out.

00:07:28.300 --> 00:07:29.515
Show us how it works inside.

00:07:29.515 --> 00:07:30.640
CHRISTIAN HOWES: All right.

00:07:30.640 --> 00:07:35.440
So our current app is still
using the Java-based solution.

00:07:35.440 --> 00:07:38.440
We have two MP3 audio
files-- the instrumental

00:07:38.440 --> 00:07:40.260
and the guide vocal tracks.

00:07:40.260 --> 00:07:44.810
We decode and mix those files
to play on the headphones.

00:07:44.810 --> 00:07:47.250
Then we record the microphone
input of the user singing,

00:07:47.250 --> 00:07:49.530
pass it through an
audio processing library

00:07:49.530 --> 00:07:52.820
for pitch detection
and pitch correction.

00:07:52.820 --> 00:07:55.580
As the user is singing, we
also mix the instrumental track

00:07:55.580 --> 00:07:57.720
with the processed
microphone stream

00:07:57.720 --> 00:08:01.200
and encode that into an
AAC file for the user

00:08:01.200 --> 00:08:02.980
to listen to and
then later share.

00:08:07.550 --> 00:08:10.410
What we really want
to do is have this all

00:08:10.410 --> 00:08:13.360
happen fast enough that
that processed microphone

00:08:13.360 --> 00:08:16.350
input could be mixed in
with the headphones for you

00:08:16.350 --> 00:08:18.060
to hear it in real time.

00:08:18.060 --> 00:08:20.590
That's also going to improve
our scoring algorithm

00:08:20.590 --> 00:08:23.950
and have a more accurate
timing of the pitch data.

00:08:23.950 --> 00:08:25.990
GLENN KASTEN: So at
last year's-- I'm sorry,

00:08:25.990 --> 00:08:27.698
hold on a second, let
me go back to that.

00:08:32.880 --> 00:08:35.650
Oh well.

00:08:35.650 --> 00:08:37.510
This clicker is wild.

00:08:37.510 --> 00:08:40.990
At last year's I/O, [? Rafe ?]
[? Lavine ?] and I presented

00:08:40.990 --> 00:08:42.900
reduced audio output latency.

00:08:42.900 --> 00:08:44.660
Did you even try
those techniques?

00:08:44.660 --> 00:08:46.330
CHRISTIAN HOWES: Well, no.

00:08:46.330 --> 00:08:47.670
We never did.

00:08:47.670 --> 00:08:53.250
We read the blog post and saw
it probably wasn't ready yet.

00:08:53.250 --> 00:08:54.970
The other thing is
last year's talk

00:08:54.970 --> 00:08:57.980
was all about output latency,
and what was important

00:08:57.980 --> 00:09:01.770
for our app is
the input latency.

00:09:01.770 --> 00:09:04.030
We need to get that audio
moving through the pipeline

00:09:04.030 --> 00:09:05.230
just so much faster.

00:09:08.245 --> 00:09:09.870
GLENN KASTEN: So I
hear you, Christian.

00:09:09.870 --> 00:09:12.890
And I hear all of
you developers too.

00:09:12.890 --> 00:09:16.050
No, I don't love mic latency.

00:09:16.050 --> 00:09:17.580
I've actually been
working on this.

00:09:17.580 --> 00:09:19.912
And we're going to share
our results with you today.

00:09:19.912 --> 00:09:20.870
CHRISTIAN HOWES: Right.

00:09:20.870 --> 00:09:22.950
So Glenn has shared
his progress with me,

00:09:22.950 --> 00:09:25.520
and I want to show you what
we've been doing with it.

00:09:25.520 --> 00:09:28.960
We've tried out the new reduced
latency APIs in the L Developer

00:09:28.960 --> 00:09:29.920
Preview.

00:09:29.920 --> 00:09:31.705
I'm going to shut off
my lapel mic again,

00:09:31.705 --> 00:09:34.755
and this time you'll hear me
actually sing through the app.

00:09:39.980 --> 00:09:42.130
[MUSIC PLAYING]

00:09:53.024 --> 00:09:54.232
GLENN KASTEN: Hey, Christian!

00:09:57.110 --> 00:09:58.581
I can hear you!

00:09:58.581 --> 00:10:01.230
[APPLAUSE]

00:10:01.230 --> 00:10:02.600
Sounds great.

00:10:02.600 --> 00:10:04.397
And how's the latency?

00:10:04.397 --> 00:10:06.230
CHRISTIAN HOWES: It's
dramatically improved.

00:10:06.230 --> 00:10:08.590
It's finally good enough
that, with the changes

00:10:08.590 --> 00:10:11.500
we've done together, I can
turn on that real time audio

00:10:11.500 --> 00:10:12.439
monitoring.

00:10:12.439 --> 00:10:14.480
GLENN KASTEN: And did you
have to change anything

00:10:14.480 --> 00:10:15.714
in your app to do this?

00:10:15.714 --> 00:10:17.630
CHRISTIAN HOWES: Well,
it's never that simple.

00:10:17.630 --> 00:10:20.310
Of course, we had to
make a few changes.

00:10:20.310 --> 00:10:22.330
We followed last
year's recommendations,

00:10:22.330 --> 00:10:24.910
and re-implemented
our entire pipeline

00:10:24.910 --> 00:10:28.550
in the open SL ES APIs.

00:10:28.550 --> 00:10:31.610
So a full C re-implementation.

00:10:31.610 --> 00:10:35.800
But it actually simplified our
audio pipeline significantly.

00:10:35.800 --> 00:10:37.920
There's fewer steps
now, and less places

00:10:37.920 --> 00:10:39.577
for the synchronization
to get off,

00:10:39.577 --> 00:10:41.160
so our recording's
going to be better,

00:10:41.160 --> 00:10:43.160
and you're going to hear
the voice in real time.

00:10:43.160 --> 00:10:45.118
GLENN KASTEN: And did
you run into any problems

00:10:45.118 --> 00:10:46.490
when you re-implemented?

00:10:46.490 --> 00:10:47.865
CHRISTIAN HOWES:
Well, there were

00:10:47.865 --> 00:10:49.810
a few challenges along the way.

00:10:49.810 --> 00:10:52.530
I'm going to tell you
about a couple of them.

00:10:52.530 --> 00:10:55.860
We do a significant amount
of digital signal processing

00:10:55.860 --> 00:10:58.270
with our pitch detection
and pitch correction.

00:10:58.270 --> 00:11:00.590
The CPU load of this
can be kind of bursty,

00:11:00.590 --> 00:11:03.860
which is really awkward with
the smaller buffer sizes.

00:11:03.860 --> 00:11:06.240
Also we had to
implement a resampler,

00:11:06.240 --> 00:11:08.800
because the different
hardware devices expect

00:11:08.800 --> 00:11:09.612
different things.

00:11:09.612 --> 00:11:11.070
I really wish we
had that resampler

00:11:11.070 --> 00:11:13.640
that Andy was just
talking about.

00:11:13.640 --> 00:11:16.800
And we don't actually yet
have this new pipeline

00:11:16.800 --> 00:11:19.350
in our app proper.

00:11:19.350 --> 00:11:20.100
GLENN KASTEN: Wow.

00:11:20.100 --> 00:11:22.460
Those are pretty hard problems.

00:11:22.460 --> 00:11:24.960
CHRISTIAN HOWES: Yes, but
they were all solvable.

00:11:24.960 --> 00:11:26.070
And you know what?

00:11:26.070 --> 00:11:28.484
The user doesn't really care.

00:11:28.484 --> 00:11:29.900
Everything that
we're trying to do

00:11:29.900 --> 00:11:32.600
is get the user inside
the performance.

00:11:32.600 --> 00:11:34.870
Our users just want to
connect with the artists

00:11:34.870 --> 00:11:37.800
and to be a part of the song.

00:11:37.800 --> 00:11:39.280
GLENN KASTEN: Thanks, Christian.

00:11:39.280 --> 00:11:42.110
Professional musicians
and recording engineers

00:11:42.110 --> 00:11:46.380
expect latency less
than 10 milliseconds.

00:11:46.380 --> 00:11:48.240
We're not there yet.

00:11:48.240 --> 00:11:50.890
But we have made
progress in the platform,

00:11:50.890 --> 00:11:52.680
and it will enable
new applications

00:11:52.680 --> 00:11:54.700
that are currently not possible.

00:11:54.700 --> 00:11:57.260
There are a lot of difficult
hardware and system integration

00:11:57.260 --> 00:11:58.720
issues that we need
to work through

00:11:58.720 --> 00:12:03.380
so that we can scale this
up to the entire ecosystem.

00:12:03.380 --> 00:12:05.180
I want to thank
all of our partners

00:12:05.180 --> 00:12:08.550
who have contributed-- in
particular, Christian Howes

00:12:08.550 --> 00:12:10.150
from StarMaker Interactive.

00:12:10.150 --> 00:12:11.746
CHRISTIAN HOWES:
Thank you, Glenn

00:12:11.746 --> 00:12:13.165
[APPLAUSE]

00:12:16.436 --> 00:12:18.560
GLENN KASTEN: And I'm hoping
she's in the audience.

00:12:18.560 --> 00:12:21.040
Amanda [? Chowdhury ?]
from Smule

00:12:21.040 --> 00:12:25.060
has also given us very
valuable feedback.

00:12:25.060 --> 00:12:26.920
[APPLAUSE]

00:12:29.710 --> 00:12:32.290
So we've just seen several
improvements in Android audio.

00:12:32.290 --> 00:12:35.690
A higher quality
floating-point pipeline,

00:12:35.690 --> 00:12:39.360
new resamplers, and my favorite,
reduced round trip latency.

00:12:39.360 --> 00:12:42.040
So the big question is,
what do we do with all this?

00:12:42.040 --> 00:12:45.610
We've seen one example of what
we can do in terms of singing.

00:12:45.610 --> 00:12:48.740
I personally am really
excited about real time voice

00:12:48.740 --> 00:12:51.740
processing, including
ordinary speech processing.

00:12:51.740 --> 00:12:56.420
And another pet favorite of
mine is augmented reality.

00:12:56.420 --> 00:13:00.380
But the big question is,
what do you want, Andy?

00:13:00.380 --> 00:13:02.450
ANDY HUNG: I'm into
audio signal processing,

00:13:02.450 --> 00:13:06.225
and I really want the
improved audio quality.

00:13:06.225 --> 00:13:07.850
GLENN KASTEN: And
how about all of you?

00:13:07.850 --> 00:13:11.170
What do you want to do?

00:13:11.170 --> 00:13:13.650
So thank you all for coming.

00:13:13.650 --> 00:13:16.780
We're going to be available
after this session for Q&amp;A--

00:13:16.780 --> 00:13:20.664
Christian, myself, and Andy--
to talk about any questions

00:13:20.664 --> 00:13:23.080
you have about Android audio,
and particularly the problem

00:13:23.080 --> 00:13:25.660
areas that came up
during Christian's demo.

00:13:25.660 --> 00:13:27.440
Please check out
the links above,

00:13:27.440 --> 00:13:29.365
and be sure to watch
our I/O Bytes videos.

00:13:32.170 --> 00:13:36.930
ANDY HUNG: And now we hand off
to Lajos from Android video.

00:13:36.930 --> 00:13:37.930
GLENN KASTEN: Thank you.

00:13:42.910 --> 00:13:43.910
LAJOS MOLNAR: All right.

00:13:46.808 --> 00:13:47.780
All right.

00:13:47.780 --> 00:13:50.280
So let's talk about video.

00:13:50.280 --> 00:13:52.910
First, I want you to
ponder this question.

00:13:52.910 --> 00:13:58.610
If a picture is worth a thousand
words, what is a video worth?

00:13:58.610 --> 00:14:00.180
Hopefully, more than a thousand.

00:14:00.180 --> 00:14:03.730
Maybe 24,000 a second.

00:14:03.730 --> 00:14:06.050
So why is video so hot?

00:14:06.050 --> 00:14:08.330
Perhaps this clip for
our Nexus 5 campaign

00:14:08.330 --> 00:14:10.918
can shed light to that.

00:14:10.918 --> 00:14:11.906
[VIDEO PLAYBACK]

00:14:11.906 --> 00:14:13.882
[MUSIC PLAYING]

00:14:23.762 --> 00:14:25.740
[END VIDEO PLAYBACK]

00:14:25.740 --> 00:14:27.730
So video is hot, because
it's the next best

00:14:27.730 --> 00:14:30.600
thing to being in the moment, to
being with the people that you

00:14:30.600 --> 00:14:33.770
love, and to satisfy
our desire to see.

00:14:33.770 --> 00:14:37.760
People have been hungry for
rich, multi-sensory, multimedia

00:14:37.760 --> 00:14:39.840
experiences from the beginning.

00:14:39.840 --> 00:14:41.940
They want to capture
and share memories.

00:14:41.940 --> 00:14:45.190
They want to create, and
they want to communicate.

00:14:45.190 --> 00:14:48.570
In fact, video has been
around for nearly 140 years.

00:14:48.570 --> 00:14:50.670
A Huffington Post
editorial actually

00:14:50.670 --> 00:14:53.310
asked a very similar question
about the birth of video,

00:14:53.310 --> 00:14:56.180
and concluded that in today's
world of social media,

00:14:56.180 --> 00:14:59.250
video may be worth
more than ever before.

00:14:59.250 --> 00:15:01.240
And Android is
positioned perfectly

00:15:01.240 --> 00:15:03.680
to take advantage of that.

00:15:03.680 --> 00:15:06.660
Android has both high-
and low-level interfaces

00:15:06.660 --> 00:15:10.470
that enable you to create
great multimedia experiences.

00:15:10.470 --> 00:15:13.600
The high-level interfaces like
MediaPlayer, MediaRecorder

00:15:13.600 --> 00:15:15.160
work by themselves.

00:15:15.160 --> 00:15:17.980
And the lower-level ones,
MediaCodec and his friends,

00:15:17.980 --> 00:15:20.040
they work as a team.

00:15:20.040 --> 00:15:22.160
For some applications, the
higher level interfaces

00:15:22.160 --> 00:15:23.550
are enough.

00:15:23.550 --> 00:15:26.880
Plus, they are supporting
the widest range of devices.

00:15:26.880 --> 00:15:28.780
For others,
applications that want

00:15:28.780 --> 00:15:31.290
to have the best
multimedia experience,

00:15:31.290 --> 00:15:32.660
they use both interfaces.

00:15:32.660 --> 00:15:34.880
They take advantage
of the low-level APIs

00:15:34.880 --> 00:15:39.050
on newer devices to provide a
polished, fine-tuned behavior.

00:15:39.050 --> 00:15:41.070
And they provide
fallback behavior

00:15:41.070 --> 00:15:44.380
on older devices using
the high-level interfaces.

00:15:44.380 --> 00:15:46.644
And then there are
apps that require

00:15:46.644 --> 00:15:48.560
the low-level interfaces
for their operations,

00:15:48.560 --> 00:15:52.780
such as media editing, or some
kind of creative applications.

00:15:52.780 --> 00:15:54.940
And these are not enabled
by the high-level API.

00:15:54.940 --> 00:16:00.320
So there are really no wrong
answers of which APIs to use.

00:16:00.320 --> 00:16:03.230
Today, video is everywhere.

00:16:03.230 --> 00:16:05.700
In the developed
world, basically you

00:16:05.700 --> 00:16:07.890
can take video for granted.

00:16:07.890 --> 00:16:11.010
What increases is
the quality bar.

00:16:11.010 --> 00:16:13.820
So I want you to see how-- we're
going to go through a few use

00:16:13.820 --> 00:16:17.030
cases to see how Android
can help you aim higher.

00:16:17.030 --> 00:16:18.680
So one of my
favorite pastimes is

00:16:18.680 --> 00:16:21.370
watching movies-- and
probably for many others.

00:16:21.370 --> 00:16:23.900
Video playback has been a
standard feature of Android

00:16:23.900 --> 00:16:28.540
since the beginning, and we
continue to make improvements.

00:16:28.540 --> 00:16:31.480
Bandwidth remains a constant
battle on mobile devices.

00:16:31.480 --> 00:16:33.180
So with the L
Developer Preview we

00:16:33.180 --> 00:16:36.830
are supporting the best
codecs, such as VP9 and HEVC.

00:16:36.830 --> 00:16:38.580
And these give you
higher resolutions

00:16:38.580 --> 00:16:40.480
for the same bandwidth.

00:16:40.480 --> 00:16:43.900
In order to take advantage
of the available bandwidth,

00:16:43.900 --> 00:16:46.340
KitKat introduced support
for adaptive playback,

00:16:46.340 --> 00:16:48.570
where you can seamlessly
change the video resolution

00:16:48.570 --> 00:16:50.150
during playback.

00:16:50.150 --> 00:16:51.610
And in the L
Developer Preview, we

00:16:51.610 --> 00:16:54.650
have dramatically
improved the scheduling

00:16:54.650 --> 00:16:58.380
of video playback, which results
in less jitters, smoother pans,

00:16:58.380 --> 00:17:02.474
and less interference
from system events.

00:17:02.474 --> 00:17:04.890
So here's Ollie to tell you
about YouTube and Play Movies,

00:17:04.890 --> 00:17:07.270
and how they are taking
advantage of the media APIs.

00:17:10.120 --> 00:17:11.036
OLIVER WOODMAN: Hello.

00:17:11.036 --> 00:17:12.260
So my name's Oliver Woodman.

00:17:12.260 --> 00:17:15.195
I work on the Play Movies and
YouTube application teams.

00:17:15.195 --> 00:17:18.436
And I'm specifically
focused on video playback.

00:17:18.436 --> 00:17:20.310
So it can be a bit of
a challenge on Android.

00:17:20.310 --> 00:17:23.119
So I want to just tell you
a little bit about how we're

00:17:23.119 --> 00:17:26.335
using these APIs internally at
Google in our own applications,

00:17:26.335 --> 00:17:28.710
and about some of the results
that we're able to achieve.

00:17:31.240 --> 00:17:34.250
So when we talk about
high-level and low-level APIs,

00:17:34.250 --> 00:17:39.180
as Lajos mentioned, when we're
talking about high-level APIs,

00:17:39.180 --> 00:17:42.420
for video playback we're really
interested in MediaPlayer.

00:17:42.420 --> 00:17:43.810
So it's kind of self-contained.

00:17:43.810 --> 00:17:45.390
It does everything for you.

00:17:45.390 --> 00:17:47.980
And both Play Movies and
YouTube use MediaPlayer

00:17:47.980 --> 00:17:51.340
on devices that don't have
the newer, low-level APIs.

00:17:51.340 --> 00:17:54.540
So we effectively just
have a kind of thin shim

00:17:54.540 --> 00:17:57.700
which wraps MediaPlayer,
and effectively MediaPlayer

00:17:57.700 --> 00:18:00.040
does everything for us.

00:18:00.040 --> 00:18:03.700
But on newer devices, we
have these low-level APIs.

00:18:03.700 --> 00:18:05.800
So we're really interested
in MediaExtractor,

00:18:05.800 --> 00:18:07.760
which does networking
and buffering and sample

00:18:07.760 --> 00:18:10.430
extraction of media.

00:18:10.430 --> 00:18:13.450
MediaCodec, which will take
these samples and decode them.

00:18:13.450 --> 00:18:17.280
AudioTrack, which will play
decoded audio out of speakers.

00:18:17.280 --> 00:18:19.450
And then MediaDrm
as well, which will

00:18:19.450 --> 00:18:21.920
handle encrypted playbacks.

00:18:21.920 --> 00:18:23.630
So while these
APIs are available,

00:18:23.630 --> 00:18:26.097
we've actually decided to
take a different approach.

00:18:26.097 --> 00:18:28.305
And we've developed an
application-level media player

00:18:28.305 --> 00:18:31.520
called ExoPlayer, which is
predominantly written in Java

00:18:31.520 --> 00:18:34.580
and calls through to
these low-level APIs.

00:18:34.580 --> 00:18:37.560
So we pull up the player logic,
and actually for Play Movies

00:18:37.560 --> 00:18:39.870
and for YouTube we
actually also pull up

00:18:39.870 --> 00:18:42.690
the networking, buffering,
and extraction components

00:18:42.690 --> 00:18:45.735
and re-implement them on
that application layer.

00:18:45.735 --> 00:18:48.110
So there are a number of
reasons why this is a good idea,

00:18:48.110 --> 00:18:50.274
and you should think
about doing this, too.

00:18:50.274 --> 00:18:52.190
Firstly, it just gives
you a lot more control.

00:18:52.190 --> 00:18:54.790
So previously, all this logic
which is part of the framework

00:18:54.790 --> 00:18:56.600
is now part of your app.

00:18:56.600 --> 00:18:57.820
You can easily update it.

00:18:57.820 --> 00:19:01.520
You can easily customize it
to suit your exact use case.

00:19:01.520 --> 00:19:03.620
And you can use the
implement features

00:19:03.620 --> 00:19:06.784
that MediaPlayer
just doesn't support.

00:19:06.784 --> 00:19:08.200
And the main thing
that we've done

00:19:08.200 --> 00:19:09.650
in terms of
implementing features

00:19:09.650 --> 00:19:11.900
is we've added support for
Play Movies and YouTube

00:19:11.900 --> 00:19:15.420
to play DASH streams, which is
a type of adaptive streaming

00:19:15.420 --> 00:19:17.450
technology.

00:19:17.450 --> 00:19:20.210
So it's similar to other
similar streaming technologies.

00:19:20.210 --> 00:19:21.890
It's nothing particularly new.

00:19:21.890 --> 00:19:24.140
It allows you to encode
your video streams

00:19:24.140 --> 00:19:27.530
in multiple resolutions
and bit rates.

00:19:27.530 --> 00:19:30.600
And then, during playback,
you can adapt the quality

00:19:30.600 --> 00:19:32.270
that you're
delivering to the user

00:19:32.270 --> 00:19:34.490
by requesting small chunks
from different streams

00:19:34.490 --> 00:19:36.650
and, depending on the
network conditions,

00:19:36.650 --> 00:19:39.040
you can then vary the
quality that you're actually

00:19:39.040 --> 00:19:42.920
requesting in order to give the
best resolution without giving

00:19:42.920 --> 00:19:45.500
the user rebuffers.

00:19:45.500 --> 00:19:48.557
So in terms of why you
should want to do this,

00:19:48.557 --> 00:19:50.640
we've been doing this on
newer devices for a while

00:19:50.640 --> 00:19:52.280
now in Play Movies,
and we've been

00:19:52.280 --> 00:19:53.613
seeing some pretty good results.

00:19:53.613 --> 00:19:55.860
So here are a few.

00:19:55.860 --> 00:19:58.730
So compared with the old way of
doing things using MediaPlayer,

00:19:58.730 --> 00:20:01.340
we find that with
ExoPlayer DASH playbacks,

00:20:01.340 --> 00:20:04.180
we can shave 65% off
start-up latency--

00:20:04.180 --> 00:20:06.850
so the initial buffering
spinner before the video

00:20:06.850 --> 00:20:09.150
starts playing.

00:20:09.150 --> 00:20:12.220
We're able to reduce
rebuffering rate significantly.

00:20:12.220 --> 00:20:15.127
So this is the number of
playbacks-- so the fraction

00:20:15.127 --> 00:20:17.585
of playbacks that the user will
see a rebuffer interrupting

00:20:17.585 --> 00:20:19.660
their playback at some point.

00:20:19.660 --> 00:20:22.071
We have reduced that by 40%.

00:20:22.071 --> 00:20:24.570
And we've done all this while
simultaneously giving the user

00:20:24.570 --> 00:20:25.820
higher quality content.

00:20:25.820 --> 00:20:28.060
So the video resolution
in terms of height

00:20:28.060 --> 00:20:31.494
is improved by about
11% on average.

00:20:31.494 --> 00:20:32.910
So these are really
great results.

00:20:32.910 --> 00:20:34.910
We're really happy
about what we're doing

00:20:34.910 --> 00:20:37.090
and how we're improving
the experience.

00:20:37.090 --> 00:20:39.640
And we really think that
people should investigate

00:20:39.640 --> 00:20:42.760
this kind of approach
as well themselves.

00:20:42.760 --> 00:20:45.491
But ExoPlayer, our
application-level media player,

00:20:45.491 --> 00:20:46.615
it took some time to build.

00:20:46.615 --> 00:20:48.740
It's about 16,000 lines of code.

00:20:48.740 --> 00:20:50.440
And the code is
relatively complex.

00:20:50.440 --> 00:20:54.340
So delivering-- building on
top of these low-level APIs

00:20:54.340 --> 00:20:58.000
can be a bit of a challenge
just by their nature.

00:20:58.000 --> 00:21:00.686
And for that reason we're
giving you ExoPlayer.

00:21:00.686 --> 00:21:02.250
We're open-sourcing it.

00:21:02.250 --> 00:21:04.757
It's on GitHub already.

00:21:04.757 --> 00:21:05.590
Go and check it out.

00:21:05.590 --> 00:21:08.800
And this is pretty much
the exact same core player

00:21:08.800 --> 00:21:11.310
technology that we're using
in our Play Movies and YouTube

00:21:11.310 --> 00:21:12.697
applications.

00:21:12.697 --> 00:21:13.688
[APPLAUSE]

00:21:13.688 --> 00:21:14.188
Thank you!

00:21:19.160 --> 00:21:21.760
So it supports DASH,
although we're still

00:21:21.760 --> 00:21:24.530
working on widening the range
of manifests that we support.

00:21:24.530 --> 00:21:26.030
But pretty much
everything is there,

00:21:26.030 --> 00:21:29.790
and we're continuing to work
on improving coverage there.

00:21:29.790 --> 00:21:31.210
It also supports
SmoothStreaming,

00:21:31.210 --> 00:21:33.190
which is a similar
adaptive technology.

00:21:33.190 --> 00:21:35.752
And we've deliberately
designed it to be extensible.

00:21:35.752 --> 00:21:38.210
Obviously, you can just take
the source code and change it.

00:21:38.210 --> 00:21:40.680
But equally, we've made
it easy to inject things

00:21:40.680 --> 00:21:42.770
into the library,
so you can do things

00:21:42.770 --> 00:21:45.502
like inject your own custom
network stack if you want to,

00:21:45.502 --> 00:21:46.585
inject a persistent cache.

00:21:46.585 --> 00:21:51.410
And really it's just a much more
powerful model for development.

00:21:51.410 --> 00:21:54.260
So that's what we've been doing
in YouTube and Play Movies.

00:21:54.260 --> 00:21:56.420
Before I finish,
I just want to say

00:21:56.420 --> 00:21:58.940
that we learned quite a lot
from developing this new media

00:21:58.940 --> 00:22:02.570
player ourselves, and we've been
feeding what we've learned back

00:22:02.570 --> 00:22:05.510
into making our low-level
APIs in the Android framework

00:22:05.510 --> 00:22:09.610
itself even better
than they are already.

00:22:09.610 --> 00:22:11.090
So this started with Kitkat.

00:22:11.090 --> 00:22:13.000
We realized it was
quite hard to achieve

00:22:13.000 --> 00:22:15.477
very accurate audio-video sync.

00:22:15.477 --> 00:22:18.060
You could get pretty close, but
there was some unknown latency

00:22:18.060 --> 00:22:21.757
in the audio track, though
there was no way to query.

00:22:21.757 --> 00:22:23.090
So we fixed that back in KitKat.

00:22:23.090 --> 00:22:27.720
We added a new getTimestamp API
that lets you figure this out.

00:22:27.720 --> 00:22:30.990
And for the devices running
older versions of the Android

00:22:30.990 --> 00:22:33.200
operating system, ExoPlayer
also has a workaround

00:22:33.200 --> 00:22:37.780
which fixes the
audio-video sync problem.

00:22:37.780 --> 00:22:39.710
There was also-- it's
just another niggle,

00:22:39.710 --> 00:22:42.330
but it's pretty annoying
if you're a developer.

00:22:42.330 --> 00:22:44.643
AudioTrack's write methods
required you to give it

00:22:44.643 --> 00:22:46.540
a byte array, but
actually, when you

00:22:46.540 --> 00:22:49.050
decode audio using
a MediaCodec API,

00:22:49.050 --> 00:22:50.520
you actually get a byte buffer.

00:22:50.520 --> 00:22:52.510
So you had to do
this pointless copy

00:22:52.510 --> 00:22:55.800
just to get data from
one place to another.

00:22:55.800 --> 00:22:57.340
Now L Developer
Preview fixes that.

00:22:57.340 --> 00:22:59.836
We've added a write method
that takes a byte buffer.

00:22:59.836 --> 00:23:01.210
And whilst we were
at it, we also

00:23:01.210 --> 00:23:04.100
added a non-blocking
variant in order

00:23:04.100 --> 00:23:06.910
to do non-blocking writes.

00:23:06.910 --> 00:23:09.580
And finally, but probably
most importantly,

00:23:09.580 --> 00:23:11.970
is the problem of
smooth video playback.

00:23:11.970 --> 00:23:15.840
So until now, you've
had to time your calls

00:23:15.840 --> 00:23:18.500
to MediacCodec release
output buffer yourself.

00:23:18.500 --> 00:23:22.095
And you have to evenly space
these so that the actual frames

00:23:22.095 --> 00:23:24.830
are dispatched in
an even manner.

00:23:24.830 --> 00:23:28.990
And if you're running the
old VM for Android-- so

00:23:28.990 --> 00:23:31.650
Dalvik-- this could be a problem
if your garbage collector came

00:23:31.650 --> 00:23:33.490
and paused your thread
at the exact time

00:23:33.490 --> 00:23:35.857
you were supposed to
release the frame.

00:23:35.857 --> 00:23:36.940
You could get pretty good.

00:23:36.940 --> 00:23:38.523
You had to be pretty
careful about not

00:23:38.523 --> 00:23:41.000
generating too much garbage.

00:23:41.000 --> 00:23:43.040
If you went to our
tech talk earlier,

00:23:43.040 --> 00:23:44.860
you will know that
things are much better,

00:23:44.860 --> 00:23:47.324
and have largely made
this issue go away.

00:23:47.324 --> 00:23:48.740
We've actually
gone a bit further,

00:23:48.740 --> 00:23:50.280
and we've added a
new method which

00:23:50.280 --> 00:23:53.110
allows you to release
a frame ahead of time

00:23:53.110 --> 00:23:55.440
specifying a time, a short
period in the future,

00:23:55.440 --> 00:23:58.880
where you would actually like
that frame to hit the screen.

00:23:58.880 --> 00:24:01.060
And we've seen that this
really helps improve

00:24:01.060 --> 00:24:04.000
the smoothness of video
playback by letting the really

00:24:04.000 --> 00:24:06.150
low-level parts of Android
time the frame release,

00:24:06.150 --> 00:24:09.060
and we've been investing
some effort there in order

00:24:09.060 --> 00:24:12.637
to make sure it does a
really good job of that.

00:24:12.637 --> 00:24:14.095
So with that, I'd
like to hand back

00:24:14.095 --> 00:24:15.660
to Lajos, who's
going to tell you

00:24:15.660 --> 00:24:19.500
some more experiences,
building experiences,

00:24:19.500 --> 00:24:20.970
on the Android APIs.

00:24:20.970 --> 00:24:22.005
Thank you.

00:24:22.005 --> 00:24:23.370
[APPLAUSE]

00:24:23.370 --> 00:24:25.190
LAJOS MOLNAR: All right.

00:24:25.190 --> 00:24:26.434
Thanks, Ollie.

00:24:26.434 --> 00:24:28.960
So let's look at
another example.

00:24:28.960 --> 00:24:32.820
We saw how Android can help
you accomplish the real time

00:24:32.820 --> 00:24:34.430
demands of video playback.

00:24:34.430 --> 00:24:37.030
But Android's
low-level media APIs

00:24:37.030 --> 00:24:39.030
are also perfect for
media processing.

00:24:39.030 --> 00:24:39.890
[VIDEO PLAYBACK]

00:24:39.890 --> 00:24:40.515
[MUSIC PLAYING]

00:24:49.920 --> 00:24:50.837
[END VIDEO PLAYBACK]

00:24:50.837 --> 00:24:52.670
So that was Auto Awesome
Movie from Google+,

00:24:52.670 --> 00:24:56.540
and it is a great example of
how to use the lower level media

00:24:56.540 --> 00:24:58.850
APIs to create content.

00:24:58.850 --> 00:25:01.180
You can use these interfaces
as building blocks.

00:25:01.180 --> 00:25:04.760
And they beautifully integrate
with GL textures and shaders.

00:25:04.760 --> 00:25:08.090
For example, you can output
decoded video onto a texture--

00:25:08.090 --> 00:25:10.110
even multiple videos.

00:25:10.110 --> 00:25:13.034
And then you can blend them
together using a GL shader.

00:25:13.034 --> 00:25:14.450
And then you can
capture the video

00:25:14.450 --> 00:25:16.890
from the resulting
texture and encode it.

00:25:16.890 --> 00:25:19.120
So that's video editing.

00:25:19.120 --> 00:25:22.460
Or you can apply filters through
the audio or video buffers

00:25:22.460 --> 00:25:24.980
to extract the
information from them.

00:25:24.980 --> 00:25:28.550
For example you can use a GLES
shader to detect scene changes

00:25:28.550 --> 00:25:31.450
or changes in color balance.

00:25:31.450 --> 00:25:33.260
So then, the question
is, what can you

00:25:33.260 --> 00:25:35.470
do with these low
level building blocks?

00:25:35.470 --> 00:25:38.960
So I assert that it
really depends on you.

00:25:38.960 --> 00:25:41.530
What can you do?

00:25:41.530 --> 00:25:45.214
So consider all the connectivity
and additional sensors

00:25:45.214 --> 00:25:46.880
that are on a mobile
device, and there's

00:25:46.880 --> 00:25:50.530
something amazing
waiting to be created.

00:25:50.530 --> 00:25:52.210
So let's look at
another example that

00:25:52.210 --> 00:25:55.341
uses video, the camera, and
connectivity to help you

00:25:55.341 --> 00:25:57.840
connect to the people you love,
even when they are far away.

00:25:57.840 --> 00:25:58.095
[VIDEO PLAYBACK]

00:25:58.095 --> 00:25:58.850
WOMAN (ON VIDEO):
It's a chance that he

00:25:58.850 --> 00:26:00.751
has to interact
with his children,

00:26:00.751 --> 00:26:02.500
because he misses out
on so many memories.

00:26:02.500 --> 00:26:05.470
He's making a memory by
telling these stories,

00:26:05.470 --> 00:26:06.640
and interacting with them.

00:26:06.640 --> 00:26:07.598
That's just phenomenal.

00:26:07.598 --> 00:26:10.577
You can't even begin to imagine.

00:26:10.577 --> 00:26:11.410
[END VIDEO PLAYBACK]

00:26:11.410 --> 00:26:12.826
LAJOS MOLNAR: So
that was Hangout,

00:26:12.826 --> 00:26:15.130
which also uses the
low-level media APIs

00:26:15.130 --> 00:26:17.460
to do real time communication.

00:26:17.460 --> 00:26:20.240
So let's start from a
basic camera preview.

00:26:20.240 --> 00:26:23.890
I mean you can capture
the surface-- the camera

00:26:23.890 --> 00:26:26.190
preview buffers, and
apply a GL shader-- maybe

00:26:26.190 --> 00:26:27.420
add a little hat.

00:26:27.420 --> 00:26:29.860
And then compress it.

00:26:29.860 --> 00:26:32.490
You can also compress
a different stream

00:26:32.490 --> 00:26:33.810
from the same source.

00:26:33.810 --> 00:26:36.250
Maybe a different hat, or maybe
a lower resolution content

00:26:36.250 --> 00:26:37.560
for lower bandwidth.

00:26:37.560 --> 00:26:40.306
Then you send the packets
across the network.

00:26:40.306 --> 00:26:41.680
And then, on the
receiver's side,

00:26:41.680 --> 00:26:44.910
you use the adaptive playback
features to seamlessly switch

00:26:44.910 --> 00:26:47.010
between the different streams.

00:26:47.010 --> 00:26:51.490
So this is basically the
basic operation of Hangouts.

00:26:51.490 --> 00:26:53.800
So we are adding a couple
of new features that

00:26:53.800 --> 00:26:56.530
will be fully exposed
in the next release,

00:26:56.530 --> 00:27:01.380
such as dynamic encoded bitrate
control and temporal layering.

00:27:01.380 --> 00:27:04.320
In temporal layering, the
individual video frames

00:27:04.320 --> 00:27:05.710
are divided into layers.

00:27:05.710 --> 00:27:07.220
So the idea is
that you should be

00:27:07.220 --> 00:27:09.710
able to decode all
the frames in a layer

00:27:09.710 --> 00:27:12.097
without any frames
in a higher layer.

00:27:12.097 --> 00:27:14.430
So this allows more radical
bitrate control, because you

00:27:14.430 --> 00:27:16.880
can discard-- not
even transmit--

00:27:16.880 --> 00:27:19.730
whole sets of buffers.

00:27:19.730 --> 00:27:21.520
For example, when
you use three layers,

00:27:21.520 --> 00:27:24.180
Layer 0 contains
every fourth frame.

00:27:24.180 --> 00:27:25.890
So it's at a quarter frame rate.

00:27:25.890 --> 00:27:29.330
Layer 1 also contains
every fourth frame.

00:27:29.330 --> 00:27:32.460
But together with Layer 0,
now the video is half rate.

00:27:32.460 --> 00:27:35.209
And then Layer 2 contains
the remaining frames.

00:27:35.209 --> 00:27:37.125
But here, notice that
each frame is droppable.

00:27:40.890 --> 00:27:43.910
So another thing that people
like to do is share stuff.

00:27:43.910 --> 00:27:47.640
So if you're an Android mobile
user with this tiny screen,

00:27:47.640 --> 00:27:50.440
and then you come across a
large, beautiful display,

00:27:50.440 --> 00:27:52.210
you would want to
take advantage of it.

00:27:52.210 --> 00:27:53.930
Wouldn't you?

00:27:53.930 --> 00:27:57.300
So Android, by default,
mirrors your activity

00:27:57.300 --> 00:27:59.040
onto external displays.

00:27:59.040 --> 00:28:00.210
And that works fine.

00:28:00.210 --> 00:28:02.170
But why not enhance
your application

00:28:02.170 --> 00:28:04.770
by providing a differentiated
specific content

00:28:04.770 --> 00:28:05.730
on the second screen?

00:28:09.180 --> 00:28:11.970
You can use MediaRouter
and DisplayManager

00:28:11.970 --> 00:28:13.460
to detect the
additional display,

00:28:13.460 --> 00:28:16.300
and then you create a
presentation to actually show

00:28:16.300 --> 00:28:18.119
the specific content on it.

00:28:18.119 --> 00:28:19.910
So there are two sample
apps in the Android

00:28:19.910 --> 00:28:23.180
and the Android SDK
that demonstrate

00:28:23.180 --> 00:28:24.280
to you how to do this.

00:28:27.560 --> 00:28:30.390
Additionally, you can take
advantage of nearby Google Cast

00:28:30.390 --> 00:28:35.250
devices, such as Chromecast or
an Android TV device and fling

00:28:35.250 --> 00:28:36.220
content onto them.

00:28:36.220 --> 00:28:39.810
So this, you use a
Google Cast SDK for that,

00:28:39.810 --> 00:28:43.280
and it's supported for all
Android 2.3 or later devices.

00:28:43.280 --> 00:28:48.010
Or, after KitKat MR1, you
can also present or mirror

00:28:48.010 --> 00:28:50.920
onto these devices.

00:28:50.920 --> 00:28:53.520
And now, the icing on the cake.

00:28:53.520 --> 00:28:56.420
We have not forgotten about
you, native developers,

00:28:56.420 --> 00:28:58.060
if you're in the room.

00:28:58.060 --> 00:29:02.240
So the low-level media APIs
are now available in the NDK.

00:29:02.240 --> 00:29:05.142
And thes APIs mimic
the Java ones.

00:29:05.142 --> 00:29:07.850
[APPLAUSE]

00:29:07.850 --> 00:29:11.210
And you can use them on
32-bit or 64-bit binaries.

00:29:11.210 --> 00:29:13.630
And hopefully, you visited
the NDK booth already.

00:29:13.630 --> 00:29:14.890
But if not, check it out.

00:29:14.890 --> 00:29:20.120
There's still some specific
content going on there.

00:29:20.120 --> 00:29:22.450
So I want you to think about
how you can take advantage

00:29:22.450 --> 00:29:24.290
of these Android
media building blocks,

00:29:24.290 --> 00:29:27.570
and where with
media or video add

00:29:27.570 --> 00:29:29.815
to the delight of your users.

00:29:29.815 --> 00:29:31.940
And what is unique about
the [INAUDIBLE] experience

00:29:31.940 --> 00:29:33.790
that you could take
advantage of to create

00:29:33.790 --> 00:29:37.620
that extra magic for
your application?

00:29:37.620 --> 00:29:39.960
For more in-depth coverage
of our media APIs,

00:29:39.960 --> 00:29:42.400
or of ExoPlayer, check
out our I/O Bytes.

00:29:42.400 --> 00:29:44.140
Maybe you already
have seen them.

00:29:44.140 --> 00:29:45.950
And check out ExoPlayer source.

00:29:45.950 --> 00:29:47.450
Now, Eddy will tell
you about what's

00:29:47.450 --> 00:29:48.616
new in the camera framework.

00:29:55.510 --> 00:29:56.800
EDDY TALVALA: Thanks, Lajos.

00:29:56.800 --> 00:29:57.759
So I'm Eddy Talvala.

00:29:57.759 --> 00:29:59.800
I'm the Tech Lead for
Android's Camera framework.

00:29:59.800 --> 00:30:01.250
And I'm here on
behalf of my team

00:30:01.250 --> 00:30:03.880
to talk about what we've been
doing for the last few years

00:30:03.880 --> 00:30:07.260
in Android Camera.

00:30:07.260 --> 00:30:08.729
Cameras are great.

00:30:08.729 --> 00:30:10.520
You can use them to do
all sorts of things.

00:30:10.520 --> 00:30:12.740
You can use them to
record your memories.

00:30:12.740 --> 00:30:15.280
You can use them to share
experiences you've had.

00:30:15.280 --> 00:30:17.100
You can use them to
document the world.

00:30:17.100 --> 00:30:18.590
And if you're actually
good at photography,

00:30:18.590 --> 00:30:21.006
which I'd like to be one day,
you can actually create art.

00:30:23.950 --> 00:30:25.720
Like this.

00:30:25.720 --> 00:30:27.479
And the mobile devices
have been getting

00:30:27.479 --> 00:30:29.770
better and better every year
in terms of their cameras.

00:30:29.770 --> 00:30:31.260
The hardware is getting better.

00:30:31.260 --> 00:30:32.570
They're getting faster.

00:30:32.570 --> 00:30:34.070
And really I think
mobile devices

00:30:34.070 --> 00:30:36.390
can become better cameras
than anything else

00:30:36.390 --> 00:30:40.890
out there, because they have so
much more than just the camera.

00:30:40.890 --> 00:30:44.302
They have a lot of
additional sensors.

00:30:44.302 --> 00:30:46.010
They have a whole lot
of computing power,

00:30:46.010 --> 00:30:48.680
both the CPU and the GPU.

00:30:48.680 --> 00:30:51.360
They're connected
to the network,

00:30:51.360 --> 00:30:53.490
so they can connect
to the world and know

00:30:53.490 --> 00:30:56.380
about the context of where you
are, what you've been doing,

00:30:56.380 --> 00:30:59.970
and connect to additional
resources from the cloud.

00:30:59.970 --> 00:31:03.052
But right now, if you want to
build an advanced camera API,

00:31:03.052 --> 00:31:05.010
it turns out the current
camera APIs on Android

00:31:05.010 --> 00:31:06.600
really are holding us back.

00:31:06.600 --> 00:31:09.540
Advanced camera applications
want more control

00:31:09.540 --> 00:31:11.650
of capture, more control
of post-processing,

00:31:11.650 --> 00:31:14.070
and they want to do
everything really fast.

00:31:14.070 --> 00:31:16.400
And right now the
Android camera API

00:31:16.400 --> 00:31:18.230
has limited capture controls.

00:31:18.230 --> 00:31:19.739
It's very hard-wired
operating mode.

00:31:19.739 --> 00:31:21.530
You're either taking
a picture or recording

00:31:21.530 --> 00:31:23.374
a video-- you can't do both.

00:31:23.374 --> 00:31:25.040
Very minimal feedback
on what's actually

00:31:25.040 --> 00:31:26.810
going on in the
whole camera system.

00:31:26.810 --> 00:31:29.900
And they are either very
slow at high resolution

00:31:29.900 --> 00:31:33.970
or, if you want to go fast,
you're limited to video rates.

00:31:33.970 --> 00:31:35.990
And they're completely
unsynchronized.

00:31:35.990 --> 00:31:38.020
So it's pretty much a
point and shoot camera.

00:31:38.020 --> 00:31:40.360
If you could just press
the buttons by software,

00:31:40.360 --> 00:31:42.080
is what the API is.

00:31:42.080 --> 00:31:43.760
So what I'm going
to talk about today

00:31:43.760 --> 00:31:46.010
is our new camera API,
which we built totally

00:31:46.010 --> 00:31:49.310
from the ground up to address
these challenging new camera

00:31:49.310 --> 00:31:51.330
applications.

00:31:51.330 --> 00:31:55.990
It has a pipeline
model and operates--

00:31:55.990 --> 00:31:58.300
allows you to get full
manual control of capture

00:31:58.300 --> 00:31:59.133
and post-processing.

00:32:02.680 --> 00:32:05.960
The pipeline model is
fully synchronized,

00:32:05.960 --> 00:32:08.942
and you get feedback for
every capture you take.

00:32:08.942 --> 00:32:11.400
And the data coming out of this
you can have multiple image

00:32:11.400 --> 00:32:14.340
streams coming out at full
resolution, full frame rate.

00:32:14.340 --> 00:32:16.210
And it's all synchronized.

00:32:16.210 --> 00:32:19.240
So with the old API, if you want
a full resolution 8 megapixel

00:32:19.240 --> 00:32:22.990
picture from your
Nexus 5, this is

00:32:22.990 --> 00:32:25.830
what you get-- one
to three frames

00:32:25.830 --> 00:32:28.590
per second at 8 megapixels.

00:32:28.590 --> 00:32:31.050
With the exact same hardware
on the L Developer Preview

00:32:31.050 --> 00:32:34.090
in our new API, you can do this.

00:32:34.090 --> 00:32:37.200
30 frames per second, 8
megapixels, uncompressed YUV.

00:32:37.200 --> 00:32:38.713
[APPLAUSE]

00:32:44.520 --> 00:32:46.550
Or even this.

00:32:46.550 --> 00:32:49.480
Again, 30 frames per second,
variable exposure time,

00:32:49.480 --> 00:32:52.130
full resolution.

00:32:52.130 --> 00:32:54.170
So what are we
doing differently?

00:32:54.170 --> 00:32:55.970
What are we doing
to enable this?

00:32:55.970 --> 00:32:58.511
Because you know, it turns out
that if you went back in time,

00:32:58.511 --> 00:33:00.740
the Galaxy Nexus could
actually, at the low levels,

00:33:00.740 --> 00:33:02.860
capture 30 frames a
second at 5 megapixels--

00:33:02.860 --> 00:33:05.092
its maximum resolution.

00:33:05.092 --> 00:33:07.520
But this wasn't
available in the API.

00:33:07.520 --> 00:33:09.220
So how come?

00:33:09.220 --> 00:33:13.500
So this is a very rough
diagram of a real camera.

00:33:13.500 --> 00:33:14.344
There is a sensor.

00:33:14.344 --> 00:33:15.510
There is an image processor.

00:33:15.510 --> 00:33:17.140
And both of them
have multiple stages

00:33:17.140 --> 00:33:18.992
that the image goes through.

00:33:18.992 --> 00:33:20.450
And of course, each
of these stages

00:33:20.450 --> 00:33:23.320
has some settings, like the
exposure time, or the gamma

00:33:23.320 --> 00:33:24.290
curve, or whatever.

00:33:24.290 --> 00:33:26.700
So I've marked the
settings for each stage

00:33:26.700 --> 00:33:28.830
here with a little yellow S.

00:33:28.830 --> 00:33:30.880
When you're streaming
data through for video

00:33:30.880 --> 00:33:33.580
or for high resolution--
high rate of capture-- each

00:33:33.580 --> 00:33:36.660
of these pipelines is busy
processing a different image.

00:33:36.660 --> 00:33:38.160
So here you can see
image number one

00:33:38.160 --> 00:33:40.980
has finished processing, has
been sent the application,

00:33:40.980 --> 00:33:43.270
has gotten all the
yellow settings applied.

00:33:43.270 --> 00:33:47.390
So with the old API, if you
now change the parameters using

00:33:47.390 --> 00:33:50.160
the set parameters call,
you do a global change.

00:33:50.160 --> 00:33:52.927
All the parameters change at
once to the red parameters.

00:33:52.927 --> 00:33:55.385
And then, if you look at what
happens to the images flowing

00:33:55.385 --> 00:33:57.650
through the pipeline,
some of them

00:33:57.650 --> 00:33:59.900
have already been processed
through the yellow stages,

00:33:59.900 --> 00:34:02.440
and now they'll get
red settings instead.

00:34:02.440 --> 00:34:05.180
So if you look at the output,
the settings are inconsistent.

00:34:05.180 --> 00:34:06.970
Your first image is
all yellow settings.

00:34:06.970 --> 00:34:08.300
The last image is all red.

00:34:08.300 --> 00:34:10.050
But everything in
between is inconsistent.

00:34:10.050 --> 00:34:12.639
You don't actually know if you
have the exposure you wanted,

00:34:12.639 --> 00:34:13.889
or the gamma curve you wanted.

00:34:13.889 --> 00:34:15.912
You have some mix.

00:34:15.912 --> 00:34:18.120
And of course, since every
Android device potentially

00:34:18.120 --> 00:34:20.107
has a different set
of pipeline stages,

00:34:20.107 --> 00:34:21.690
you really have no
idea how long it'll

00:34:21.690 --> 00:34:23.357
take for any given
setting to propagate.

00:34:23.357 --> 00:34:25.148
So if you want to do
something really fast,

00:34:25.148 --> 00:34:26.739
you want to take
a burst of images,

00:34:26.739 --> 00:34:28.980
it's very difficult to do,
because you don't really

00:34:28.980 --> 00:34:32.624
know when it will happen
and when you'll be done.

00:34:32.624 --> 00:34:33.790
So you could just slow down.

00:34:33.790 --> 00:34:35.331
And this is what
usually happens when

00:34:35.331 --> 00:34:37.489
you take high-resolution
pictures.

00:34:37.489 --> 00:34:39.409
So you just send one
image through at a time.

00:34:39.409 --> 00:34:41.580
So, settings are
consistent, but you're now

00:34:41.580 --> 00:34:44.179
going 1/6 the speed as before.

00:34:44.179 --> 00:34:49.290
So this isn't very suitable for
advanced applications as well.

00:34:49.290 --> 00:34:52.000
So instead, what we do now is
that we look at the problem

00:34:52.000 --> 00:34:53.199
differently.

00:34:53.199 --> 00:34:55.500
Each of the images that
go through the system

00:34:55.500 --> 00:34:57.740
have the settings
attached to them.

00:34:57.740 --> 00:35:00.720
So the settings flow through
the pipeline with the images

00:35:00.720 --> 00:35:02.690
instead of being
these global things.

00:35:02.690 --> 00:35:05.950
So instead of having a black
box that sends you data,

00:35:05.950 --> 00:35:07.730
you send in requests
for images, and they

00:35:07.730 --> 00:35:10.320
get filled along as they
flow through the pipeline.

00:35:10.320 --> 00:35:12.670
So here I have some yellow,
blue, and red settings.

00:35:12.670 --> 00:35:14.220
And as they flow
through the system

00:35:14.220 --> 00:35:16.120
the settings go along
with image buffers,

00:35:16.120 --> 00:35:18.580
and everything is consistent
at consistent settings

00:35:18.580 --> 00:35:21.020
and high frame rate.

00:35:21.020 --> 00:35:22.690
And there aren't any modes here.

00:35:22.690 --> 00:35:25.200
The system is simply a
streaming flowing API.

00:35:25.200 --> 00:35:26.655
You provide a
request and now it's

00:35:26.655 --> 00:35:29.660
common data that's
synchronized to what you want.

00:35:29.660 --> 00:35:33.360
So let's take a quick look at
the building blocks of the API

00:35:33.360 --> 00:35:36.800
that implements this
pipeline architecture.

00:35:36.800 --> 00:35:38.800
To start off with, you
just get a camera manager

00:35:38.800 --> 00:35:41.090
from the getSystemService call.

00:35:41.090 --> 00:35:43.840
The camera manager lets you
enumerate camera devices.

00:35:43.840 --> 00:35:45.840
Query their characteristics,
like which way

00:35:45.840 --> 00:35:47.060
they're pointing.

00:35:47.060 --> 00:35:48.587
And then you can open them.

00:35:48.587 --> 00:35:50.420
So I get the information
first, and then you

00:35:50.420 --> 00:35:51.753
decide it's the camera you want.

00:35:51.753 --> 00:35:52.680
You open it.

00:35:52.680 --> 00:35:55.960
The camera device lets you
do two important things.

00:35:55.960 --> 00:35:58.580
First, it lets you create an
individual capture session.

00:35:58.580 --> 00:36:00.250
You take a set of
outputs that you

00:36:00.250 --> 00:36:01.960
want to target for
this session and use

00:36:01.960 --> 00:36:03.530
it to create a CaptureSession.

00:36:03.530 --> 00:36:07.330
And the outputs are
the Android surfaces.

00:36:07.330 --> 00:36:09.230
And then you can
create Capture Request.

00:36:09.230 --> 00:36:11.480
Each Capture Request again
defines all the settings

00:36:11.480 --> 00:36:14.090
for a single image that's going
to come out of the system.

00:36:14.090 --> 00:36:16.231
And then you start sending
them to the session.

00:36:16.231 --> 00:36:17.980
Session captures the
image asynchronously,

00:36:17.980 --> 00:36:20.420
and when it's done it
sends you back the metadata

00:36:20.420 --> 00:36:22.460
about the result.

00:36:22.460 --> 00:36:24.140
So the characteristics
include things

00:36:24.140 --> 00:36:27.640
like the available manual
exposure times, the resolutions

00:36:27.640 --> 00:36:29.870
and formats supported
by this device,

00:36:29.870 --> 00:36:31.960
possible available
autofocus modes,

00:36:31.960 --> 00:36:35.630
and the general capability
flags of this device.

00:36:35.630 --> 00:36:38.490
The requests are all of the
settings for a single capture.

00:36:38.490 --> 00:36:40.450
Here we're asking for
one millisecond exposure

00:36:40.450 --> 00:36:43.870
time, color correction
gains for post processing,

00:36:43.870 --> 00:36:46.020
and a continuous autofocus mode.

00:36:46.020 --> 00:36:48.407
And the request also
include which of the targets

00:36:48.407 --> 00:36:50.240
you configured for this
session you actually

00:36:50.240 --> 00:36:51.656
want to use for this request.

00:36:51.656 --> 00:36:53.530
So, for example, if you
configure the session

00:36:53.530 --> 00:36:57.550
with a preview output and a
high resolution shaping output,

00:36:57.550 --> 00:36:59.050
you might want some
of your requests

00:36:59.050 --> 00:37:02.090
to test target preview
for a steady viewfinder,

00:37:02.090 --> 00:37:05.230
and some of them that
are user-triggered

00:37:05.230 --> 00:37:06.950
provide the shaping data.

00:37:06.950 --> 00:37:09.140
So any subset of the
configure services

00:37:09.140 --> 00:37:11.140
can be used in each request.

00:37:11.140 --> 00:37:13.205
And then, on the output,
you get the result.

00:37:13.205 --> 00:37:15.850
It contains time
stamps, the state

00:37:15.850 --> 00:37:18.811
of devices like the flash--
here the flash is fired.

00:37:18.811 --> 00:37:20.310
The autofocus is
successfully locked

00:37:20.310 --> 00:37:22.640
and it says there's a face
in this particular image.

00:37:22.640 --> 00:37:25.970
And you know that it was
exactly capture request-- which

00:37:25.970 --> 00:37:27.636
capture request that
you sent in that

00:37:27.636 --> 00:37:29.260
was captured that
has these statistics.

00:37:29.260 --> 00:37:30.884
So you can correlate
this to image data

00:37:30.884 --> 00:37:34.090
and analyze it and know
exactly what you got.

00:37:34.090 --> 00:37:35.752
So putting this
all together, you

00:37:35.752 --> 00:37:37.710
start out with a set of
outputs you care about.

00:37:37.710 --> 00:37:40.370
Here, some examples
are SurfaceView,

00:37:40.370 --> 00:37:44.540
RenderScript allocation,
an ImageReader for a JPEG,

00:37:44.540 --> 00:37:46.920
a MediaCodec for video
encoding, or SurfaceTexture

00:37:46.920 --> 00:37:49.980
to send to open GL.

00:37:49.980 --> 00:37:52.110
You get the surfaces from them.

00:37:52.110 --> 00:37:54.300
You create a capture session.

00:37:54.300 --> 00:37:57.570
And creating the session also
defines your maximum frame

00:37:57.570 --> 00:37:58.220
rate.

00:37:58.220 --> 00:38:00.290
It looks at the surfaces
you've provided.

00:38:00.290 --> 00:38:02.350
It looks at the
resolutions you want.

00:38:02.350 --> 00:38:04.760
And the maximum resolution
defines how fast

00:38:04.760 --> 00:38:06.480
you can actually run the system.

00:38:06.480 --> 00:38:09.380
On the Nexus 5 we can run at
30 frames per second for 8

00:38:09.380 --> 00:38:13.619
megapixel maximum output,
but this may vary per device.

00:38:13.619 --> 00:38:15.410
And then, once you've
configured a session,

00:38:15.410 --> 00:38:17.910
you start creating capture
requests and sending them.

00:38:17.910 --> 00:38:19.160
So everything is asynchronous.

00:38:19.160 --> 00:38:21.700
The requests go
into a queue, get

00:38:21.700 --> 00:38:24.310
processed by the
hardware in the pipeline.

00:38:24.310 --> 00:38:28.490
As the requests are
processed, filled buffers

00:38:28.490 --> 00:38:30.774
are sent to your outputs
with time stamps,

00:38:30.774 --> 00:38:33.065
and then results with time
stamps are sent to the queue

00:38:33.065 --> 00:38:35.080
once the captures are complete.

00:38:35.080 --> 00:38:37.022
And this is the entire
flow of the API.

00:38:37.022 --> 00:38:38.230
There aren't any other modes.

00:38:38.230 --> 00:38:39.210
There's no video mode.

00:38:39.210 --> 00:38:40.890
There's no still-capture mode.

00:38:40.890 --> 00:38:42.750
You simply decide what
requests you want.

00:38:42.750 --> 00:38:44.550
You send them in at
the rate you want,

00:38:44.550 --> 00:38:47.200
and they come out
when they're done.

00:38:47.200 --> 00:38:49.600
At high rates, of course,
so you can have as many

00:38:49.600 --> 00:38:53.320
of them in the
queue as you'd like.

00:38:53.320 --> 00:38:55.842
So I already talked about
the different outputs.

00:38:55.842 --> 00:38:56.800
Different destinations.

00:38:56.800 --> 00:38:59.640
You can get output in
uncompressed YUV, compressed

00:38:59.640 --> 00:39:01.760
JPEG, or RAW_SENSOR data.

00:39:01.760 --> 00:39:03.260
And there's a whole
lot of controls,

00:39:03.260 --> 00:39:05.093
and I'm not going to
talk about all of them.

00:39:05.093 --> 00:39:06.718
You control a bunch
of manual controls,

00:39:06.718 --> 00:39:08.259
and there's still
a bunch of controls

00:39:08.259 --> 00:39:10.340
for the automatic side--
exposure, autofocus,

00:39:10.340 --> 00:39:13.260
and auto white balance.

00:39:13.260 --> 00:39:17.090
We have a large number of camera
characteristics fields to read.

00:39:17.090 --> 00:39:19.819
All the settings, possible
ranges from manual control.

00:39:19.819 --> 00:39:21.610
A lot of information
about the sensor color

00:39:21.610 --> 00:39:23.990
spaces for raw
sensor processing,

00:39:23.990 --> 00:39:25.740
and the information
about the hardware,

00:39:25.740 --> 00:39:27.650
the lensing capabilities.

00:39:27.650 --> 00:39:30.370
And on the result side,
we have information

00:39:30.370 --> 00:39:32.340
about the state of all
the automatic algorithms

00:39:32.340 --> 00:39:35.090
if you're running them, and
the state of the hardware--

00:39:35.090 --> 00:39:36.990
like if the flash
fired and so on.

00:39:36.990 --> 00:39:40.940
We also provide some statistics
like the neutral color point

00:39:40.940 --> 00:39:42.930
of the image, whether
there are faces,

00:39:42.930 --> 00:39:45.580
and what the lens shading
compensation map is.

00:39:45.580 --> 00:39:48.310
And we'll have a number
of Android DevBytes videos

00:39:48.310 --> 00:39:49.270
about this.

00:39:49.270 --> 00:39:50.890
The first one should
be going up soon.

00:39:50.890 --> 00:39:53.720
I'm getting the
very basics going.

00:39:53.720 --> 00:39:56.010
One important note
that for the L Preview,

00:39:56.010 --> 00:39:57.835
this only works on the Nexus 5.

00:39:57.835 --> 00:40:01.590
The Nexus 7 or the Emulator
Images don't run the new API.

00:40:01.590 --> 00:40:05.620
If you try, it will crash
in some horrible way.

00:40:05.620 --> 00:40:10.426
So that's a very brief
tour of what the API is.

00:40:10.426 --> 00:40:12.800
The question is, what can
you actually do with it?

00:40:12.800 --> 00:40:16.650
So our first serious
shipping application is HDR+

00:40:16.650 --> 00:40:18.930
on the Nexus 5.

00:40:18.930 --> 00:40:21.310
It's the Nexus 5's high dynamic
range and low-light mode.

00:40:21.310 --> 00:40:23.985
It was shipped using a
pre-release version of our API

00:40:23.985 --> 00:40:26.050
in Android KitKat.

00:40:26.050 --> 00:40:27.995
So here's a sample image.

00:40:27.995 --> 00:40:30.030
A very hard problem.

00:40:30.030 --> 00:40:32.120
This is a dimly
lit party, and this

00:40:32.120 --> 00:40:35.160
is a picture of a Nexus 5
standard picture capture

00:40:35.160 --> 00:40:36.910
cropped a bit.

00:40:36.910 --> 00:40:38.090
So it's very dark.

00:40:38.090 --> 00:40:40.390
So the denoising algorithms
had to do a lot of work,

00:40:40.390 --> 00:40:43.250
and the picture, as a result,
is very blurry and still kind of

00:40:43.250 --> 00:40:45.210
splotchy.

00:40:45.210 --> 00:40:48.310
So let's try HDR+.

00:40:48.310 --> 00:40:54.290
HDR+ in 0.9 seconds captures
115 megabytes of data,

00:40:54.290 --> 00:40:57.050
analyzes the scene, and
produces an output result

00:40:57.050 --> 00:40:59.171
that's on the right.

00:40:59.171 --> 00:41:03.980
As you can see, it's a lot
sharper, it's a lot less noisy,

00:41:03.980 --> 00:41:05.800
and the dynamic range
is actually improved.

00:41:05.800 --> 00:41:07.820
You can see the television
in the background.

00:41:07.820 --> 00:41:10.110
If I zoom up close
to the eye here,

00:41:10.110 --> 00:41:12.690
you can see on the left that
there's barely any detail

00:41:12.690 --> 00:41:16.320
in the regular capture, where
there's quite a bit of detail

00:41:16.320 --> 00:41:20.660
in the HDR+ and you can see
the improved dynamic range.

00:41:20.660 --> 00:41:22.780
So this has been shipping.

00:41:22.780 --> 00:41:25.050
This uses the advanced
capabilities of the API

00:41:25.050 --> 00:41:28.830
for manual capture
in high-rate bursts.

00:41:28.830 --> 00:41:32.040
Another possibility you
could do with the API

00:41:32.040 --> 00:41:35.480
is taking a stack of images
with a bunch of different depths

00:41:35.480 --> 00:41:36.740
very quickly.

00:41:36.740 --> 00:41:41.890
So here we take--
move the Nexus 5 lens

00:41:41.890 --> 00:41:46.740
from far to near focus
in about 0.8 seconds

00:41:46.740 --> 00:41:49.369
capturing a set of 8
megapixel images, as we go.

00:41:49.369 --> 00:41:50.910
You can now take
this burst of images

00:41:50.910 --> 00:41:52.720
and analyze it
for, perhaps, make

00:41:52.720 --> 00:41:54.320
an image that's in
focus everywhere,

00:41:54.320 --> 00:41:56.260
or create some
sort of depth map.

00:41:56.260 --> 00:41:59.890
There's a number
of applications.

00:41:59.890 --> 00:42:03.800
We're also enabling support
for Adobe DNG raw formats

00:42:03.800 --> 00:42:06.030
in the Android frameworks.

00:42:06.030 --> 00:42:09.465
So we will provide unsupported
devices, including the Nexus 5.

00:42:09.465 --> 00:42:11.416
They will provide
raw sensor data

00:42:11.416 --> 00:42:13.040
that can either be
analyzed by the app,

00:42:13.040 --> 00:42:15.310
or saved into a DNG
file that can then

00:42:15.310 --> 00:42:18.120
be opened in professional
post-processing applications

00:42:18.120 --> 00:42:19.514
like Lightroom, Aperture.

00:42:19.514 --> 00:42:21.180
Or you can even upload
to Google Photos,

00:42:21.180 --> 00:42:24.930
and they'll actually
happily process your DNGs.

00:42:24.930 --> 00:42:27.540
And finally, I'm going
to try a demo application

00:42:27.540 --> 00:42:29.205
of another interesting
application.

00:42:32.010 --> 00:42:34.170
Let's see if this works.

00:42:34.170 --> 00:42:36.740
Let's see this work-- camera.

00:42:36.740 --> 00:42:37.240
All right.

00:42:37.240 --> 00:42:39.170
So Rachad is going
to be a model, here.

00:42:39.170 --> 00:42:40.870
Stand right about there.

00:42:40.870 --> 00:42:43.420
So.

00:42:43.420 --> 00:42:44.380
All right.

00:42:44.380 --> 00:42:47.670
So as you can see here,
this is a regular viewfinder

00:42:47.670 --> 00:42:48.490
on the Nexus 5.

00:42:48.490 --> 00:42:50.540
You can see Rachad just
fine, but the audience

00:42:50.540 --> 00:42:53.070
is a little dark.

00:42:53.070 --> 00:42:55.430
So we have-- I said we have
manual exposure control.

00:42:55.430 --> 00:42:56.995
So let's see if
we can do better.

00:42:56.995 --> 00:42:59.504
Let's switch to manual exposure.

00:42:59.504 --> 00:43:01.670
And the default exposure's
a little bright on Rachad

00:43:01.670 --> 00:43:03.770
and still too dark
on the audience.

00:43:03.770 --> 00:43:05.130
And to make this thing a
little more interesting,

00:43:05.130 --> 00:43:06.879
this is not actually
just manual exposure.

00:43:06.879 --> 00:43:10.560
This is actually a
split-screen manual exposure.

00:43:10.560 --> 00:43:12.720
So I'm going to brighten
up the audience a bit.

00:43:12.720 --> 00:43:16.260
And we get some really
bad glare from the lights

00:43:16.260 --> 00:43:17.600
there, unfortunately.

00:43:17.600 --> 00:43:19.200
But you can see
that on the right

00:43:19.200 --> 00:43:20.951
I can see Rachad just fine.

00:43:20.951 --> 00:43:22.450
On the left, I can
see the audience.

00:43:22.450 --> 00:43:24.290
And if I pan over the
audience, it's black.

00:43:24.290 --> 00:43:27.070
And if I pan the other way,
Rachad is a little overexposed.

00:43:27.070 --> 00:43:28.570
So now I can see
both sides, but I'd

00:43:28.570 --> 00:43:31.220
like to see it at the
same time, of course.

00:43:31.220 --> 00:43:32.730
So let's use RenderScript.

00:43:32.730 --> 00:43:34.690
And we have a direct
efficient copy

00:43:34.690 --> 00:43:37.870
in our data processing
path into RenderScript.

00:43:37.870 --> 00:43:41.410
And blend this into a single
high-dynamic range viewfinder.

00:43:41.410 --> 00:43:44.630
So now you can see both Rachad
on the right and the audience

00:43:44.630 --> 00:43:47.351
on the left, and the viewfinder
is still rather smooth.

00:43:47.351 --> 00:43:48.850
Actually could be
a little smoother.

00:43:48.850 --> 00:43:51.100
Notice that on the right,
I'm getting feedback

00:43:51.100 --> 00:43:52.239
about the exposure time.

00:43:52.239 --> 00:43:53.655
And the exposure
time had actually

00:43:53.655 --> 00:43:55.100
gotten long enough
that we were starting

00:43:55.100 --> 00:43:56.235
to drop the frame rate.

00:43:56.235 --> 00:43:58.170
So let me drop it back
it down to demonstrate

00:43:58.170 --> 00:44:00.330
that this is actually
quite smooth.

00:44:00.330 --> 00:44:04.110
And you can compare
regular viewfinder.

00:44:04.110 --> 00:44:06.410
Rachad is still dark, the
audience is completely gone.

00:44:06.410 --> 00:44:09.510
Split screen, and then the blend
where you can see everything.

00:44:09.510 --> 00:44:11.835
And this all runs at
30 frames per second.

00:44:11.835 --> 00:44:14.460
And if you want to know what the
secret is-- how do we actually

00:44:14.460 --> 00:44:18.052
do a split screen-- we don't
actually have a fancy sensor.

00:44:18.052 --> 00:44:20.510
We're actually just alternating
exposures every other frame

00:44:20.510 --> 00:44:22.190
in a continuous burst.

00:44:22.190 --> 00:44:25.160
So the sensor's not--
every other frame is long,

00:44:25.160 --> 00:44:27.060
every other frame is
short, and we simply

00:44:27.060 --> 00:44:29.521
blend the last
images we get out.

00:44:29.521 --> 00:44:31.020
So this is the kind
of thing you can

00:44:31.020 --> 00:44:34.740
do when you're not limited
to modes like burst.

00:44:34.740 --> 00:44:37.170
The API has one
mode of operation.

00:44:37.170 --> 00:44:39.245
And it can do bursts, it
can do streaming bursts,

00:44:39.245 --> 00:44:40.710
it can do single images.

00:44:40.710 --> 00:44:41.894
Whatever you want.

00:44:41.894 --> 00:44:42.650
All right.

00:44:42.650 --> 00:44:43.970
Thank you, Rachad.

00:44:43.970 --> 00:44:45.930
[APPLAUSE]

00:44:51.320 --> 00:44:55.680
So in summary, we made a
new Camera API for Android.

00:44:55.680 --> 00:44:58.020
We think it's pretty awesome.

00:44:58.020 --> 00:45:01.620
We really hope to get a lot of
feedback during the L Preview

00:45:01.620 --> 00:45:03.740
to help us guide
what to do next,

00:45:03.740 --> 00:45:06.070
and to get feedback on
bugs, because there's still

00:45:06.070 --> 00:45:08.210
some rough edges we know about.

00:45:08.210 --> 00:45:10.150
So please take it for
a spin on the Nexus 5.

00:45:10.150 --> 00:45:12.810
Let us know what you think.

00:45:12.810 --> 00:45:16.370
And I'd like to get actually
Rachad back to wrap up.

00:45:16.370 --> 00:45:17.958
Thank you very much.

00:45:17.958 --> 00:45:21.149
[APPLAUSE]

00:45:21.149 --> 00:45:21.940
RACHAD ALAO: Hello.

00:45:21.940 --> 00:45:22.910
Thank you very much.

00:45:22.910 --> 00:45:24.880
I hope you enjoyed the session.

00:45:24.880 --> 00:45:26.806
We'll all be available
in the corner.

00:45:26.806 --> 00:45:28.180
If you have
additional questions,

00:45:28.180 --> 00:45:29.640
we'll be happy to answer them.

00:45:29.640 --> 00:45:31.420
Thank you!

