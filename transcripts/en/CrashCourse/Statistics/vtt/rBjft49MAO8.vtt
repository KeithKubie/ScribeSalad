WEBVTT
Kind: captions
Language: en

00:00:02.700 --> 00:00:06.360
Hi, I’m Adriene Hill, and Welcome back to Crash Course Statistics.

00:00:06.360 --> 00:00:11.160
This is the episode you’ve been waiting for. The episode we designed this shelf for. The episode that

00:00:11.160 --> 00:00:24.020
you have heard a lot about. (NORMAL DIST MONTAGE)

00:00:24.080 --> 00:00:28.740
Well, today, we’ll get to see why we talk SO MUCH about the normal distribution.

00:00:28.740 --> 00:00:38.000
INTRO

00:00:38.000 --> 00:00:41.780
Things like height, IQ, standardized test
scores, and a lot of mechanically generated

00:00:41.789 --> 00:00:46.900
things like the weight of cereal boxes are
normally distributed, but many other interesting

00:00:46.900 --> 00:00:50.890
things from blood pressure, to debt, to fuel efficiency just aren’t.

00:00:50.890 --> 00:00:58.340
One reason we talk so much about normal distributions
is because distributions of means are normally

00:00:58.359 --> 00:01:00.460
distributed, even if populations aren’t.

00:01:00.460 --> 00:01:05.210
The normal distribution is symmetric, which means its mean, median and mode are all the

00:01:05.210 --> 00:01:10.160
same value. And it’s most popular values
are in the middle, with skinny tails to either side.

00:01:10.160 --> 00:01:14.900
In general, when we ask scientific questions, we’re not comparing individual scores or

00:01:14.910 --> 00:01:19.880
values like the weight of one blue jay, or
the number of kills from one League of Legends game,

00:01:19.890 --> 00:01:24.390
we’re comparing groups--or samples--of
them. So we’re often concerned with the

00:01:24.390 --> 00:01:27.970
distributions of the means, not the population.

00:01:27.970 --> 00:01:31.990
In order to meaningfully compare whether two means are different, we need to know something

00:01:31.990 --> 00:01:37.760
about their distribution: the sampling distribution of sample means. Also called the sampling

00:01:37.760 --> 00:01:39.300
distribution for short.

00:01:39.300 --> 00:01:43.060
And before we go any further, I want to say
that the distribution of sample means is not

00:01:43.060 --> 00:01:46.800
something we create, we don’t actually draw an infinite number of samples to plot and

00:01:46.800 --> 00:01:53.180
observe their means. This distribution, like
most distributions, is a description of a process.

00:01:53.180 --> 00:01:58.180
Take income. Income is skewed….so we might think the distribution of all possible mean

00:01:58.180 --> 00:02:02.860
incomes would also be skewed. But they’re actually normally distributed.

00:02:02.860 --> 00:02:06.940
In the real population there are people that make a huge amount of money. Think Oprah,

00:02:06.940 --> 00:02:12.510
Jeff Bezos, and Bill Gates. But when we take the mean of a group of three randomly selected

00:02:12.510 --> 00:02:18.060
people, it becomes much less likely to see
extreme mean incomes because in order to have

00:02:18.060 --> 00:02:22.560
an income that’s as high as Oprah’s, you’d need to randomly select 3 people with pretty

00:02:22.560 --> 00:02:24.340
high incomes, instead of just one.

00:02:24.340 --> 00:02:28.380
Since scientific questions usually ask us to compare groups rather than individuals,

00:02:28.380 --> 00:02:32.630
this makes our lives a lot easier, because instead of an infinite amount of different

00:02:32.630 --> 00:02:37.140
distributions to keep track of, we can just
keep track of one: the normal distribution.

00:02:37.150 --> 00:02:41.189
The reason that sampling distributions are
almost always normal is laid out in the Central

00:02:41.189 --> 00:02:45.749
Limit Theorem. The Central Limit Theorem states that the distribution of sample means for

00:02:45.749 --> 00:02:51.560
an independent, random variable, will get
closer and closer to a normal distribution

00:02:51.560 --> 00:02:56.189
as the size of the sample gets bigger and
bigger, even if the original population distribution

00:02:56.189 --> 00:02:57.879
isn’t normal itself.

00:02:57.879 --> 00:03:02.000
As we get further into inferential statistics
and making models to describe our data, this

00:03:02.000 --> 00:03:06.999
will become more useful. Many inferential
techniques in statistics rely on the assumption

00:03:06.999 --> 00:03:11.889
that the distribution of sample means is normal, and the Central Limit Theorem allows us to

00:03:11.889 --> 00:03:13.470
claim that they usually are.

00:03:13.470 --> 00:03:16.530
Let’s look at a simulation of the Central
Limit Theorem in action.

00:03:16.530 --> 00:03:22.109
For our first example, imagine a discrete,
uniform distribution. Like dice rolls. The

00:03:22.109 --> 00:03:24.870
distribution of values for a single dice roll
looks like this:

00:03:24.870 --> 00:03:29.459
With a sample size of 1--the regular distribution of dice values--there’s one way to get a

00:03:29.459 --> 00:03:32.189
1, one way to get a 2, one way to get a 3….and so on.

00:03:32.189 --> 00:03:37.249
But we want to look at the mean of say...2
dice rolls, meaning our sample size is 2.

00:03:37.249 --> 00:03:40.580
With two dice. Let’s first look at all the
sums of the dice rolls we can get:

00:03:40.580 --> 00:03:44.720
2,3,4,5,6,7,8,9,10,11,12

00:03:44.720 --> 00:03:50.060
There’s only one way to get 2 and 12, either
two ones, or two 6’s, but there’s 6 ways

00:03:50.060 --> 00:03:57.349
to get 7, [1,6],[2,5], [3,4] or [6,1],[5,2],
and [4,3]...which lends significance to the

00:03:57.349 --> 00:04:00.469
number 7 - which is the number you’ll roll
most often.

00:04:00.469 --> 00:04:04.819
But back to means, we have the possible sums, but we want the mean, so we’ll divide each

00:04:04.820 --> 00:04:08.239
total value by two, giving us this distribution:

00:04:08.240 --> 00:04:12.389
Even though our population distribution is uniform, The distribution of sample means

00:04:12.389 --> 00:04:17.420
is looking more normal, even with a sample
size of 2. As our sample size gets bigger

00:04:17.430 --> 00:04:23.020
and bigger, the middle values get more common, and the tail values are less and less common.

00:04:23.020 --> 00:04:27.590
We can use the multiplication rule from probability to see why that happens.

00:04:27.590 --> 00:04:33.050
If you roll a die one time, the probability
of getting a 1--the lowest value--is ⅙.

00:04:33.050 --> 00:04:36.840
When you increase the number of rolls to two, the probability of getting a mean of 1, is

00:04:36.840 --> 00:04:44.300
now 1/36, or ⅙ times ⅙, since you have
to get two 1’s to have a mean of 1.

00:04:44.300 --> 00:04:48.020
Getting a mean value of 2 is a little bit easier since you can have a mean roll of 2

00:04:48.020 --> 00:04:54.020
both by rolling two 2’s, but also by rolling
a 3 and a 1, or a 1 and a 3. So the probability

00:04:54.020 --> 00:04:56.380
is 3(1/36).

00:04:56.380 --> 00:05:00.700
If we had the patience to roll a die 20 times, the probability of getting a mean roll value

00:05:00.710 --> 00:05:06.960
of 1 would be (⅙)^20 since the only way
to get a mean of 1 on 20 dice rolls is to

00:05:06.980 --> 00:05:13.599
roll a one. Every. Single. Time. So you can
see that even with a sample size of only 20,

00:05:13.600 --> 00:05:17.270
the means of our dice rolls will look pretty close to normal.

00:05:17.270 --> 00:05:22.200
The mean of the distribution of sample means is 3.5, the same as the mean of our original

00:05:22.210 --> 00:05:27.810
uniform distribution of dice rolls, and this
is always true about sampling distributions:

00:05:27.810 --> 00:05:32.569
Their mean is always the same as the population they’re derived from. So with large samples,

00:05:32.569 --> 00:05:36.820
the sample means will be a pretty good estimate of the true population mean.

00:05:36.820 --> 00:05:39.100
There are two separate distributions we’re
talking about.

00:05:39.100 --> 00:05:43.840
There is the original population distribution that’s generating each individual die roll,

00:05:43.840 --> 00:05:48.540
and there is a distribution of sample means that tells you the frequency of all the possible

00:05:48.540 --> 00:05:54.180
sample means you could get by drawing a sample of a certain size--here 20--from that original

00:05:54.180 --> 00:06:00.860
population distribution. Again, population distribution. And sampling distribution of sample means.

00:06:00.940 --> 00:06:02.740
But while the mean of the distribution of sample

00:06:02.740 --> 00:06:08.030
means is the same as the population’s, it’s
standard deviation is not, which might be

00:06:08.030 --> 00:06:13.419
intuitive since we saw how larger sample sizes render extreme values--like a mean roll value

00:06:13.419 --> 00:06:19.759
of 1 or 6--very unlikely, while making values
close to the mean more and more likely.

00:06:19.759 --> 00:06:24.500
And it doesn’t just work for uniform population distributions. Normal population distributions

00:06:24.500 --> 00:06:30.600
also give normal distributions of sample means, as do skewed distributions, and this weird looking guy:

00:06:30.600 --> 00:06:35.860
In fact, with a large sample, any distribution
with finite variance will have a distribution

00:06:35.860 --> 00:06:39.040
of sample means that is approximately normal.

00:06:39.040 --> 00:06:43.500
This is incredibly useful. We can use the nice, symmetric and mathematically pleasant

00:06:43.500 --> 00:06:48.430
normal distribution to calculate things like
percentiles, as well as how weird or rare

00:06:48.430 --> 00:06:52.590
a difference between two sample means actually is.

00:06:52.590 --> 00:06:57.060
The standard deviation of a distribution of sample means is still related to the original

00:06:57.060 --> 00:07:01.800
standard deviation. But as we saw, the bigger the sample size, the closer your sample means

00:07:01.800 --> 00:07:07.659
are to the true population mean, so we need
to adjust the original population standard

00:07:07.660 --> 00:07:11.200
deviation somehow to reflect this. The way we do it mathematically is to divide

00:07:11.200 --> 00:07:13.650
by the square root of n--our sample size.

00:07:13.650 --> 00:07:18.479
Since we divide by the square root of n, as
n gets big, the standard deviation--or sigma--gets

00:07:18.479 --> 00:07:23.789
smaller.. which we can see in these simulations of sampling distributions of size 20, 50,

00:07:23.789 --> 00:07:28.750
and 100. The larger the sample size, the skinnier the distribution of sample means.

00:07:28.750 --> 00:07:32.960
For example, say you grab 5 boxes of strawberries at your local grocery store--you’re making

00:07:32.960 --> 00:07:36.980
the pies for a pie eating contest--and weigh them when you get home. The mean weight of

00:07:36.990 --> 00:07:39.240
a box of strawberries from your grocery store is 15oz.

00:07:39.250 --> 00:07:42.830
But that means that you don’t have quite enough strawberries. You thought that the

00:07:42.830 --> 00:07:47.490
boxes were about 16oz, and you wonder if the grocery store got a new supplier that gives

00:07:47.490 --> 00:07:48.680
you a little less.

00:07:48.680 --> 00:07:53.270
You do a quick Google search and find a small farming company’s blog. They package boxes

00:07:53.270 --> 00:07:58.499
of strawberries for a local grocery store,
they list the mean weight of their boxes--16oz--and

00:07:58.500 --> 00:08:01.440
the standard deviation--1.25 oz.

00:08:01.440 --> 00:08:06.320
That’s all the information we need to calculate the distribution of sample means for a sample

00:08:06.320 --> 00:08:11.650
of 5 boxes. Part of the mathematical pleasantness of the normal distribution is that if you

00:08:11.650 --> 00:08:16.590
know the mean and standard deviation, you
know the exact shape of the distribution.

00:08:16.590 --> 00:08:20.270
So you grab your computer and pull up a stats program to plot the distribution of sample

00:08:20.270 --> 00:08:27.260
means with a mean of 16oz and a standard deviation of 1.25 divided by the square root of 5--the

00:08:27.260 --> 00:08:28.260
sample size.

00:08:28.260 --> 00:08:32.430
We call The standard deviation of a sampling distribution the standard error so that we

00:08:32.430 --> 00:08:37.260
don’t get it confused with the population
standard deviation, it’s still a standard

00:08:37.260 --> 00:08:39.260
deviation, just of a different distribution.

00:08:39.260 --> 00:08:44.110
Our distribution of sample means for a sample of 5 boxes looks like this.

00:08:44.110 --> 00:08:48.029
And now that we know what it looks like, we can see how different the mean strawberry

00:08:48.029 --> 00:08:50.740
box weights of 15oz really is.

00:08:50.740 --> 00:08:54.339
When we graph it over the distribution of
sample means, we can see that it’s not too

00:08:54.339 --> 00:08:59.250
close to the mean of 16oz, but it’s not
too far either...We need a more concrete way

00:08:59.250 --> 00:09:04.460
to decide whether the 15oz is really that
far away from the mean of 16oz.

00:09:04.460 --> 00:09:09.080
It might help if we had a measure of how different we expect one sample mean to be from the true

00:09:09.080 --> 00:09:14.240
mean, luckily we do: the standard error which tells us the average distance between a sample

00:09:14.240 --> 00:09:16.370
mean and the true mean of 16oz.

00:09:16.370 --> 00:09:19.770
This is where personal judgement comes in. We could decide for example, that if a sample

00:09:19.770 --> 00:09:24.000
mean was more than 2 standard errors away from the mean, we’d be suspicious. If that

00:09:24.000 --> 00:09:28.670
was the case then maybe there was some systematic reduction in strawberries, because it’s

00:09:28.670 --> 00:09:33.360
unlikely our sample mean was randomly that different from the true mean.

00:09:33.360 --> 00:09:38.940
In this case our standard error would be 0.56. If we decided 2 standard errors was too far

00:09:38.959 --> 00:09:44.060
away, we wouldn’t have much to be suspicious about. Maybe we should hold off leaving a

00:09:44.060 --> 00:09:47.080
nasty comment on the strawberry farmers blog.

00:09:47.080 --> 00:09:50.540
Looking at the distribution of sample means helped us compare two means, but we can also

00:09:50.540 --> 00:09:55.620
use sampling distributions to compare other parameters like proportions, Regression Coefficients,

00:09:55.620 --> 00:09:59.550
or standard deviations, which also follow the Central Limit Theorem.

00:09:59.550 --> 00:10:04.040
The CLT allows us to use the same tools, like a distributions, with all different kinds

00:10:04.040 --> 00:10:08.860
of questions. You may be interested in whether your favorite baseball team has better batting

00:10:08.860 --> 00:10:14.329
averages, and your friend may care about whether Tylenol cures her headache faster than ibuprofen.

00:10:14.329 --> 00:10:18.520
Thanks to the CLT you can both use the same tools to find your answers.

00:10:18.520 --> 00:10:22.280
But when you look at things on a group level instead of the individual level, all these

00:10:22.280 --> 00:10:28.210
diverse shapes and the populations that make them converge to one common distribution:

00:10:28.210 --> 00:10:31.380
the normal distribution.

00:10:31.380 --> 00:10:35.899
And the simplicity of the normal distribution allows us to make meaningful comparisons between

00:10:35.900 --> 00:10:40.800
groups like whether hiring managers hire fewer single mothers, or whether male chefs make

00:10:40.800 --> 00:10:45.480
more money. These comparisons help us know where things fit in the world.

00:10:45.480 --> 00:10:47.680
Thanks for watching. I'll see you next time.

