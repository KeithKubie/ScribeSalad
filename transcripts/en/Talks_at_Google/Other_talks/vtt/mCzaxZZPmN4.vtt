WEBVTT
Kind: captions
Language: en

00:00:02.946 --> 00:00:04.420
SPEAKER: Tim.

00:00:04.420 --> 00:00:05.670
Welcome to Google.

00:00:05.670 --> 00:00:05.920
TIM URBAN: Thank you.

00:00:05.920 --> 00:00:06.419
[LAUGH]

00:00:06.419 --> 00:00:10.570
[APPLAUSE]

00:00:11.729 --> 00:00:13.770
SPEAKER: On behalf of
Google and all the Googlers

00:00:13.770 --> 00:00:16.602
here and those streaming from
around the world, welcome.

00:00:16.602 --> 00:00:17.560
It's great to have you.

00:00:17.560 --> 00:00:18.851
TIM URBAN: Thank you very much.

00:00:18.851 --> 00:00:20.620
This is great.

00:00:20.620 --> 00:00:22.920
I've wanted to do a Google
Talk for a long time,

00:00:22.920 --> 00:00:24.525
so this is very exciting.

00:00:24.525 --> 00:00:25.150
SPEAKER: Great.

00:00:25.150 --> 00:00:26.925
Because knowing
that you were coming

00:00:26.925 --> 00:00:30.430
and that you tend to get
immersed in a talk, in a topic,

00:00:30.430 --> 00:00:33.400
we wanted to get you a small
token of our appreciation.

00:00:33.400 --> 00:00:36.150
It's a book called
"How Google Works."

00:00:36.150 --> 00:00:38.640
[LAUGHTER]

00:00:38.640 --> 00:00:40.760
And I think it's about
as long as your posts

00:00:40.760 --> 00:00:41.710
on superintelligence.

00:00:41.710 --> 00:00:42.210
[LAUGHTER]

00:00:42.210 --> 00:00:43.710
TIM URBAN: Oh, there's pictures.

00:00:43.710 --> 00:00:44.770
SPEAKER: Oh, there are pictures.

00:00:44.770 --> 00:00:45.270
All right.

00:00:45.270 --> 00:00:47.520
I didn't think there was
stick figures in there.

00:00:47.520 --> 00:00:49.360
TIM URBAN: This is
Eric Schmidt dancing.

00:00:49.360 --> 00:00:51.120
[LAUGHTER]

00:00:51.810 --> 00:00:52.310
Good.

00:00:52.310 --> 00:00:53.018
This is exciting.

00:00:53.018 --> 00:00:53.545
Thank you.

00:00:53.545 --> 00:00:55.670
SPEAKER: So this talk was
hosted for a group called

00:00:55.670 --> 00:00:56.669
the Singularity Network.

00:00:56.669 --> 00:00:58.150
And for a bit of
background, it's

00:00:58.150 --> 00:01:01.540
about a 2,000 Googler mailing
list, which frequently

00:01:01.540 --> 00:01:04.741
goes on 600 post tangents
about what really means

00:01:04.741 --> 00:01:07.240
strong, artificial intelligence,
or whether your computer is

00:01:07.240 --> 00:01:08.470
going to take over the world.

00:01:08.470 --> 00:01:11.460
And I'm pretty sure thousands
of hours of paid Google time

00:01:11.460 --> 00:01:16.090
has been wasted on the page,
looking for circularity

00:01:16.090 --> 00:01:19.590
in the simulation argument
or accurately grading

00:01:19.590 --> 00:01:21.280
all of Ray Kurzweil's
predictions,

00:01:21.280 --> 00:01:24.200
or trying to see if
the Chinese room really

00:01:24.200 --> 00:01:25.260
could exist in China.

00:01:25.260 --> 00:01:26.094
It's sort of a hub--

00:01:26.094 --> 00:01:27.885
TIM URBAN: It's an
anxious group of people.

00:01:27.885 --> 00:01:28.470
[LAUGH]

00:01:28.470 --> 00:01:31.100
SPEAKER: It is a hub for
procrastination about topics

00:01:31.100 --> 00:01:33.160
that I've heard that you're
pretty interested in.

00:01:33.160 --> 00:01:35.170
So we're really, really
excited to have you.

00:01:35.170 --> 00:01:35.795
TIM URBAN: Yes.

00:01:35.795 --> 00:01:36.440
Well, great.

00:01:36.440 --> 00:01:38.340
SPEAKER: So today, for
today's presentation,

00:01:38.340 --> 00:01:41.081
I want to talk about artificial
intelligence and, you know,

00:01:41.081 --> 00:01:42.330
how you came to understand it.

00:01:42.330 --> 00:01:44.199
I mean, to start,
I've love to hear

00:01:44.199 --> 00:01:45.740
how you were introduced
to the topic,

00:01:45.740 --> 00:01:47.964
and why you really
started digging it.

00:01:47.964 --> 00:01:48.630
TIM URBAN: Yeah.

00:01:48.630 --> 00:01:49.920
Well, it was kind of
one of those things

00:01:49.920 --> 00:01:51.130
that I kept hearing about.

00:01:51.130 --> 00:01:54.420
And I kept hearing,
like, smart people

00:01:54.420 --> 00:01:56.741
I otherwise respected
talking a lot about it.

00:01:56.741 --> 00:01:58.240
It's like Burning
Man, a little bit.

00:01:58.240 --> 00:01:59.920
[LAUGHTER]

00:01:59.920 --> 00:02:02.300
And I'd be like, really?

00:02:02.300 --> 00:02:03.930
You're really talking about AI?

00:02:03.930 --> 00:02:04.980
This was a few years ago.

00:02:04.980 --> 00:02:06.100
Now, of course, I
think more people

00:02:06.100 --> 00:02:07.850
are aware that this
is an important topic.

00:02:07.850 --> 00:02:10.889
But I finally said, OK.

00:02:10.889 --> 00:02:15.710
I need to figure
out what's going on,

00:02:15.710 --> 00:02:20.090
and whether this is exciting
or scary or both or neither.

00:02:20.090 --> 00:02:24.780
And so I started reading
various kind of books out there.

00:02:24.780 --> 00:02:27.150
I read Nick Bostrom's
"Superintelligence" book, which

00:02:27.150 --> 00:02:32.180
is kind of one of the only books
that's, like, simultaneously

00:02:32.180 --> 00:02:34.705
riveting and, like,
mind numbingly boring.

00:02:34.705 --> 00:02:36.040
[LAUGHTER]

00:02:36.040 --> 00:02:38.650
Like, it's an incredible
feat that he's

00:02:38.650 --> 00:02:41.400
accomplished with that book.

00:02:41.400 --> 00:02:44.407
And I read a bunch
of Kurzweil stuff

00:02:44.407 --> 00:02:46.740
and, you know, I think some
other kind of excited people

00:02:46.740 --> 00:02:49.030
like Peter Diamandis.

00:02:49.030 --> 00:02:50.760
And just, there's
a crowd of people

00:02:50.760 --> 00:02:53.130
that are very optimistic.

00:02:53.130 --> 00:02:57.170
And then I read a bunch
of other things, articles,

00:02:57.170 --> 00:03:00.530
and watched nothing that fancy.

00:03:00.530 --> 00:03:02.800
I read a bunch of PDFs that
come out of universities.

00:03:02.800 --> 00:03:04.620
I watched a bunch
of YouTube videos.

00:03:04.620 --> 00:03:09.930
And the way I do
research is instead

00:03:09.930 --> 00:03:13.210
of worrying about finding a
professional source that's

00:03:13.210 --> 00:03:17.110
definitely going to be the most
legitimate, accurate source,

00:03:17.110 --> 00:03:19.900
I just read and watch
and take in, like,

00:03:19.900 --> 00:03:22.960
a ton of stuff, most of which
alone should not probably

00:03:22.960 --> 00:03:23.940
be trusted.

00:03:23.940 --> 00:03:26.310
But eventually, by the time
you kind of get to the end,

00:03:26.310 --> 00:03:26.976
you're like, OK.

00:03:26.976 --> 00:03:29.290
I at least have a sense
of what people are saying.

00:03:29.290 --> 00:03:33.010
And with AI, there's not really
any definitive source anyway.

00:03:33.010 --> 00:03:36.720
And so I just did that
for about a month.

00:03:36.720 --> 00:03:37.480
Nothing too crazy.

00:03:37.480 --> 00:03:41.840
And then I wrote kind of a big
summary of what I had learned.

00:03:41.840 --> 00:03:45.340
That was the beginning
of me working

00:03:45.340 --> 00:03:46.882
on trying to understand AI.

00:03:46.882 --> 00:03:49.340
SPEAKER: Was there anyone you
read that you were like, wow,

00:03:49.340 --> 00:03:52.687
this guy is crazy?

00:03:52.687 --> 00:03:54.020
TIM URBAN: Kurzweil's a little--

00:03:54.020 --> 00:03:55.930
[LAUGHTER]

00:03:55.930 --> 00:03:59.750
I mean, I'm hoping he's right.

00:03:59.750 --> 00:04:03.450
I mean, according to him, I'm
going to be able to switch

00:04:03.450 --> 00:04:08.750
my senses on and off and to
be in a true virtual-immersed

00:04:08.750 --> 00:04:13.090
second reality, and we're going
to replace my bloodstream with

00:04:13.090 --> 00:04:14.940
something different, and--

00:04:14.940 --> 00:04:15.714
SPEAKER: Nanobots.

00:04:15.714 --> 00:04:16.380
TIM URBAN: What?

00:04:16.380 --> 00:04:16.839
SPEAKER: Nanobots.

00:04:16.839 --> 00:04:17.505
TIM URBAN: Yeah.

00:04:17.505 --> 00:04:19.480
Oh, the nanobots, of course.

00:04:19.480 --> 00:04:22.910
He lives with a lot of nanobots.

00:04:22.910 --> 00:04:25.065
And we are going to be the AI.

00:04:25.065 --> 00:04:26.440
We're going to
merge with the AI.

00:04:26.440 --> 00:04:30.534
We are going to be
superintelligent ourselves.

00:04:30.534 --> 00:04:31.450
And it's not just him.

00:04:31.450 --> 00:04:33.241
I've heard a lot of
people talk about that.

00:04:33.241 --> 00:04:34.830
It sounds insane.

00:04:34.830 --> 00:04:36.410
But then the people
that are kind

00:04:36.410 --> 00:04:42.800
of more the doomsday-type
people, they also sound insane.

00:04:42.800 --> 00:04:44.550
And I kept looking for
someone who's like,

00:04:44.550 --> 00:04:46.770
don't listen to any
of these people.

00:04:46.770 --> 00:04:48.450
It's just software.

00:04:48.450 --> 00:04:50.570
Nothing crazy is
going to happen.

00:04:50.570 --> 00:04:52.040
It's going to get
very intelligent.

00:04:52.040 --> 00:04:54.590
Nothing that big or that life
changing's going to happen.

00:04:54.590 --> 00:04:56.410
And I didn't find
anyone saying that.

00:04:56.410 --> 00:04:57.180
So I said, OK.

00:04:57.180 --> 00:04:59.070
So at this point,
now I trust everyone.

00:04:59.070 --> 00:05:03.290
Because there's just--
everyone sounds insane.

00:05:03.290 --> 00:05:05.180
Clearly something
weird's going to happen.

00:05:07.730 --> 00:05:08.230
So, yeah.

00:05:08.230 --> 00:05:09.030
I mean, as I said.

00:05:09.030 --> 00:05:10.670
It's almost hard
to find anyone who

00:05:10.670 --> 00:05:13.870
really writes about AI
that doesn't sound insane,

00:05:13.870 --> 00:05:14.855
to be honest.

00:05:14.855 --> 00:05:15.426
[LAUGH]

00:05:15.426 --> 00:05:17.550
SPEAKER: So at Google, we
have a few different ways

00:05:17.550 --> 00:05:19.008
of developing
machine intelligence.

00:05:19.008 --> 00:05:21.540
I think neural nets is one
of the most common here.

00:05:21.540 --> 00:05:23.040
And when digging
through your posts,

00:05:23.040 --> 00:05:27.020
you mentioned three ways that we
could potentially get AGI, ASI.

00:05:27.020 --> 00:05:30.320
You talk about whole-brain
emulation, genetic algorithms,

00:05:30.320 --> 00:05:31.685
neural net inspired.

00:05:31.685 --> 00:05:33.060
From your research,
was there any

00:05:33.060 --> 00:05:35.020
that you thought would
be most likely to reach

00:05:35.020 --> 00:05:37.397
artificial intelligence?

00:05:37.397 --> 00:05:37.980
TIM URBAN: No.

00:05:37.980 --> 00:05:39.850
I have no idea what
I'm-- I have absolute--

00:05:39.850 --> 00:05:42.266
I am not a good person-- and
most people in this room have

00:05:42.266 --> 00:05:43.280
a better idea.

00:05:43.280 --> 00:05:45.780
Honestly my job is to figure
out what people are saying.

00:05:45.780 --> 00:05:47.940
And the thing is,
for other topics

00:05:47.940 --> 00:05:50.999
I've written about, I've
written about Elon's companies

00:05:50.999 --> 00:05:52.540
and cryonics, and
other things, and I

00:05:52.540 --> 00:05:54.800
feel more confident
there, because there

00:05:54.800 --> 00:05:56.150
was a lot more consensus.

00:05:56.150 --> 00:05:58.880
And so I can basically
read enough consensus,

00:05:58.880 --> 00:06:01.350
and now I can take
the conviction

00:06:01.350 --> 00:06:03.069
that the thinkers
have, and I can say,

00:06:03.069 --> 00:06:04.610
now I can speak with
that conviction.

00:06:04.610 --> 00:06:07.000
So I can steal their conviction.

00:06:07.000 --> 00:06:09.350
But when you read
a bunch of things,

00:06:09.350 --> 00:06:11.740
and experts are saying
opposite things,

00:06:11.740 --> 00:06:13.770
then I have no
self-confidence now.

00:06:13.770 --> 00:06:14.820
[LAUGHTER]

00:06:14.820 --> 00:06:15.440
Right?

00:06:15.440 --> 00:06:18.079
So it's like, if I
at one point thought

00:06:18.079 --> 00:06:20.370
that neural nets were an
absolutely great way to do it,

00:06:20.370 --> 00:06:22.286
I've now talked to three
people who have said,

00:06:22.286 --> 00:06:24.510
that is just absolute nonsense.

00:06:24.510 --> 00:06:26.310
So now I don't want
to say anything.

00:06:26.310 --> 00:06:29.700
But in the last
month, I've talked

00:06:29.700 --> 00:06:34.740
to the head of a pretty
respected AI company,

00:06:34.740 --> 00:06:37.310
Vicarious.

00:06:37.310 --> 00:06:40.625
And three or four other people,
like, of the same-- MIRI.

00:06:40.625 --> 00:06:42.542
I had a chance to talk
to Luke Muehlhauser.

00:06:42.542 --> 00:06:43.500
Muehlhauser, something.

00:06:43.500 --> 00:06:44.820
Whatever.

00:06:44.820 --> 00:06:46.490
Like, the smartest dude ever.

00:06:46.490 --> 00:06:49.270
And someone who's
working in open AI.

00:06:49.270 --> 00:06:54.390
And you know, again, it's
really, like, dramatically

00:06:54.390 --> 00:06:56.470
differing opinions.

00:06:56.470 --> 00:06:59.912
So then when I'm confused
about something and I say,

00:06:59.912 --> 00:07:01.370
I'm doing talks on
AI-- and I don't

00:07:01.370 --> 00:07:03.370
know what to say about
this, because I now

00:07:03.370 --> 00:07:07.010
have lost my conviction about
whether or not that's actually,

00:07:07.010 --> 00:07:08.010
like, objectively true.

00:07:08.010 --> 00:07:10.840
And I ask them about it, and
I think I get a clear answer,

00:07:10.840 --> 00:07:13.340
and then I ask someone else,
and it's something different.

00:07:13.340 --> 00:07:17.030
So again, at this
point, I'm excited.

00:07:17.030 --> 00:07:19.460
Because I always think,
like, if it's good,

00:07:19.460 --> 00:07:21.940
if the good story comes true,
of course that's exciting,

00:07:21.940 --> 00:07:22.100
you know?

00:07:22.100 --> 00:07:23.266
Everyone's happy about that.

00:07:23.266 --> 00:07:25.510
We get to, like,
slide down rainbows

00:07:25.510 --> 00:07:28.160
and we get to-- we
don't have to die,

00:07:28.160 --> 00:07:29.950
and all the problems are solved.

00:07:29.950 --> 00:07:32.265
And the bad story,
I'm a little like,

00:07:32.265 --> 00:07:34.265
it's kind of cool to be
here for the apocalypse.

00:07:34.265 --> 00:07:35.905
[LAUGHTER]

00:07:35.905 --> 00:07:37.340
Right?

00:07:37.340 --> 00:07:40.220
At least it's like, it's kind
of awesome to see it happen.

00:07:40.220 --> 00:07:41.220
So I'm kind of like, OK.

00:07:41.220 --> 00:07:46.170
If AI kills all of us, then
at least that's interesting.

00:07:46.170 --> 00:07:49.039
And selfishly, I'm already
kind of like, then,

00:07:49.039 --> 00:07:50.330
I'll probably be kind of older.

00:07:50.330 --> 00:07:52.330
Like, life isn't that fun
anymore at that point.

00:07:52.330 --> 00:07:52.922
[LAUGHTER]

00:07:52.922 --> 00:07:56.644
And for people who are babies
now, that's a little less fun.

00:07:56.644 --> 00:07:58.060
SPEAKER: So you're
sort of hinting

00:07:58.060 --> 00:07:59.900
at the idea of an
intelligence explosion.

00:07:59.900 --> 00:08:00.190
TIM URBAN: Yes.

00:08:00.190 --> 00:08:01.900
SPEAKER: And this
is something I think

00:08:01.900 --> 00:08:04.720
Kurzweil and Bostrom
both relatively think

00:08:04.720 --> 00:08:06.660
is very possible.

00:08:06.660 --> 00:08:10.660
You talked about death
and living forever.

00:08:10.660 --> 00:08:15.910
Why does this intelligence
explosion lead to that?

00:08:15.910 --> 00:08:20.480
TIM URBAN: Because--
so I always like to,

00:08:20.480 --> 00:08:26.020
as just an illustrative tool,
draw intelligence staircase.

00:08:26.020 --> 00:08:27.560
Which is not, of
course, scientific,

00:08:27.560 --> 00:08:32.059
but there is a scale
of intelligence.

00:08:32.059 --> 00:08:35.190
And so an ant has
less cognitive ability

00:08:35.190 --> 00:08:39.490
than a chicken who has less
than an ape or a chimp, who

00:08:39.490 --> 00:08:41.110
has less than we do.

00:08:41.110 --> 00:08:44.990
And so the thing that
blows my mind about that

00:08:44.990 --> 00:08:46.740
is that we're not at
the end of evolution.

00:08:46.740 --> 00:08:48.340
We didn't, like,
the finish button.

00:08:48.340 --> 00:08:50.030
Like, we're in the
middle of evolution.

00:08:50.030 --> 00:08:52.094
And you know, whether
evolution itself

00:08:52.094 --> 00:08:54.760
is no longer the thing that will
create higher intelligence-- it

00:08:54.760 --> 00:08:57.050
could have been that
for 3.8 billion years,

00:08:57.050 --> 00:08:58.680
evolution was the thing.

00:08:58.680 --> 00:09:02.090
And literally in our
lifetimes, that changes.

00:09:02.090 --> 00:09:06.990
And now biology building
artificial intelligence

00:09:06.990 --> 00:09:08.330
becomes the new thing.

00:09:08.330 --> 00:09:10.230
And self-improving AI
becomes the new way

00:09:10.230 --> 00:09:13.060
that the smartest thing on
the planet gets smarter.

00:09:13.060 --> 00:09:20.690
Regardless whatever the mode
is, the fact that the concept

00:09:20.690 --> 00:09:23.810
of something that is as
smarter than we are--

00:09:23.810 --> 00:09:27.210
as smarter than we are,
as we are than chimps--

00:09:27.210 --> 00:09:27.760
[LAUGH]

00:09:27.760 --> 00:09:29.320
--not well articulated.

00:09:29.320 --> 00:09:31.980
You know what I'm
saying though-- is just

00:09:31.980 --> 00:09:32.920
so crazy to me.

00:09:32.920 --> 00:09:36.210
Like, a chimp is
really intelligent.

00:09:36.210 --> 00:09:38.110
We're, like, almost
identical DNA.

00:09:38.110 --> 00:09:40.424
I mean, that is so
incredibly close.

00:09:40.424 --> 00:09:42.590
If there was an alien from
some other dimension that

00:09:42.590 --> 00:09:45.173
was assessing life on Earth, and
they saw a chimp and a human,

00:09:45.173 --> 00:09:48.040
they would say, these are
essentially the same species,

00:09:48.040 --> 00:09:49.542
as far as we're concerned.

00:09:49.542 --> 00:09:51.000
And yet, not only
could a chimp not

00:09:51.000 --> 00:09:54.600
build a skyscraper
or an airplane.

00:09:54.600 --> 00:09:56.370
When a chimp sees
something like that,

00:09:56.370 --> 00:09:57.870
it just assumes
it's part of nature.

00:09:57.870 --> 00:09:59.827
It can't even understand
that we built that.

00:09:59.827 --> 00:10:02.410
You couldn't even explain to the
chimp that that's not a bird.

00:10:02.410 --> 00:10:04.705
That's something we
made, that airplane.

00:10:04.705 --> 00:10:07.080
So then I think something
that's equally smarter than us,

00:10:07.080 --> 00:10:10.680
and that's just a little
bit, not only could we

00:10:10.680 --> 00:10:11.597
not do what it can do.

00:10:11.597 --> 00:10:13.846
Not only can we not understand
what it can understand.

00:10:13.846 --> 00:10:15.755
We can't even get
that it built that.

00:10:15.755 --> 00:10:18.680
Like, it can try to explain
it to us all at once,

00:10:18.680 --> 00:10:20.920
and we won't be
able to grasp what

00:10:20.920 --> 00:10:23.510
it's trying to explain to us.

00:10:23.510 --> 00:10:24.740
That to me is so crazy.

00:10:24.740 --> 00:10:26.240
And so that's a
little bit above us.

00:10:26.240 --> 00:10:27.960
Of course, how
smart is that thing?

00:10:27.960 --> 00:10:34.727
If you talk about a human
level AGI that's coding itself,

00:10:34.727 --> 00:10:37.060
it's a computer scientist,
and it's about as intelligent

00:10:37.060 --> 00:10:37.370
as we are.

00:10:37.370 --> 00:10:39.536
But you know, it's a really
good computer scientist,

00:10:39.536 --> 00:10:40.330
like many humans.

00:10:40.330 --> 00:10:41.630
Then it gets to
that level where it

00:10:41.630 --> 00:10:43.750
can do stuff we can't even
understand that it did.

00:10:43.750 --> 00:10:45.970
How good a computer
scientist is that?

00:10:45.970 --> 00:10:49.120
I mean, it's going to be able
to build things and prove itself

00:10:49.120 --> 00:10:52.140
so dramatically quickly in a
way that we literally won't even

00:10:52.140 --> 00:10:53.980
be able to understand.

00:10:53.980 --> 00:10:56.230
Now that itself--
so that, of course,

00:10:56.230 --> 00:10:59.495
leads to an insane
intelligence explosion.

00:10:59.495 --> 00:11:03.670
I always talk about how
we have words for stupid.

00:11:03.670 --> 00:11:07.995
We say 85 IQ, we
would call stupid.

00:11:07.995 --> 00:11:10.980
And 140 IQ, we
call really smart.

00:11:10.980 --> 00:11:13.300
But what if something
has a 14,000 IQ?

00:11:13.300 --> 00:11:15.170
We can't even
begin to understand

00:11:15.170 --> 00:11:17.540
what that means on Earth.

00:11:17.540 --> 00:11:20.330
Species that are more
intelligent have power.

00:11:20.330 --> 00:11:22.610
Our higher intelligence
gives us power

00:11:22.610 --> 00:11:25.680
over every single species that's
less intelligent than we are.

00:11:25.680 --> 00:11:27.960
Even something as
smart as chimps,

00:11:27.960 --> 00:11:29.746
we can just put them in a cage.

00:11:29.746 --> 00:11:30.787
What are you going to do?

00:11:30.787 --> 00:11:32.250
[LAUGHTER]

00:11:32.250 --> 00:11:33.210
We have tasers.

00:11:33.210 --> 00:11:35.290
Good luck with that, chimp.

00:11:35.290 --> 00:11:37.930
We can poison its food.

00:11:37.930 --> 00:11:40.610
We have endless,
endless tranquilizers.

00:11:40.610 --> 00:11:43.200
We have endless ways to
just completely own them

00:11:43.200 --> 00:11:45.720
in every way that a
species can be owned.

00:11:45.720 --> 00:11:48.710
And so if you just continue
to extrapolate that,

00:11:48.710 --> 00:11:51.430
well, intelligence equals power.

00:11:51.430 --> 00:11:54.190
When something is so
dramatically more intelligent,

00:11:54.190 --> 00:11:55.940
then we have to assume
it's more powerful.

00:11:55.940 --> 00:11:59.610
In which case, everything we
attribute to God, God's power--

00:11:59.610 --> 00:12:01.720
you know, we look at
books like the Bible,

00:12:01.720 --> 00:12:06.280
I mean, well, that kind of power
is actually potentially real

00:12:06.280 --> 00:12:09.980
when something gets
more intelligent.

00:12:09.980 --> 00:12:11.460
And then this is
where, of course,

00:12:11.460 --> 00:12:14.380
I have to drop in that,
well, but I've also

00:12:14.380 --> 00:12:18.480
heard this thing, which
an example here is,

00:12:18.480 --> 00:12:20.690
this is something I've seen
people on both sides of,

00:12:20.690 --> 00:12:23.100
and I'm constantly going
back and forth myself.

00:12:23.100 --> 00:12:27.837
But what I just
said about something

00:12:27.837 --> 00:12:29.420
a little more
intelligent than we are,

00:12:29.420 --> 00:12:31.000
not only can we not
do what it can do.

00:12:31.000 --> 00:12:32.550
We can't even understand
that it did it even if it

00:12:32.550 --> 00:12:33.690
tried to explain it to us.

00:12:33.690 --> 00:12:37.830
I've heard people argue
that that's not true.

00:12:37.830 --> 00:12:41.980
I think there's a book by
David Deutsch, I've heard.

00:12:41.980 --> 00:12:44.780
And Elon Musk thinks this,
and a few other people

00:12:44.780 --> 00:12:48.210
that-- actually, that's not
true that we are, at this point,

00:12:48.210 --> 00:12:48.940
computers.

00:12:48.940 --> 00:12:50.780
We're weak computers,
but we're computers.

00:12:50.780 --> 00:12:52.510
And once you're a
computer, nothing

00:12:52.510 --> 00:12:53.840
is not explainable to us.

00:12:53.840 --> 00:12:55.380
There's no such
thing as something

00:12:55.380 --> 00:12:56.660
that we couldn't understand.

00:12:56.660 --> 00:12:58.260
It's not an analogy.

00:12:58.260 --> 00:13:01.010
We've crossed some magical
black and white line

00:13:01.010 --> 00:13:02.900
that we are now
on this other side

00:13:02.900 --> 00:13:04.420
where that'll never happen.

00:13:04.420 --> 00:13:08.080
And that we are conscious,
and that things-- once we're

00:13:08.080 --> 00:13:10.240
conscious, there's
nothing that is

00:13:10.240 --> 00:13:13.102
an equivalent of us to animals.

00:13:13.102 --> 00:13:15.310
Again, these are super smart
people who believe this.

00:13:15.310 --> 00:13:16.270
And I've heard a
lot of people who

00:13:16.270 --> 00:13:17.520
do believe this kind of thing.

00:13:17.520 --> 00:13:19.854
To me, that sounds insane
just because biology

00:13:19.854 --> 00:13:20.770
isn't black and white.

00:13:20.770 --> 00:13:23.590
We're in the middle
of evolution.

00:13:23.590 --> 00:13:25.340
To me, what we
call consciousness

00:13:25.340 --> 00:13:27.925
is just what it feels like to
be human-level intelligent.

00:13:27.925 --> 00:13:29.550
And if something were
more intelligent,

00:13:29.550 --> 00:13:30.640
they would have a
consciousness that's

00:13:30.640 --> 00:13:33.390
a different nature, that we
don't even begin to understand,

00:13:33.390 --> 00:13:34.550
that we can't grasp.

00:13:34.550 --> 00:13:35.450
That's how I see it.

00:13:35.450 --> 00:13:38.065
But again, there's
20 of these where

00:13:38.065 --> 00:13:41.210
it's like a fundamental
disagreement in the tech

00:13:41.210 --> 00:13:43.919
community and in the
philosopher communities

00:13:43.919 --> 00:13:44.960
about this kind of thing.

00:13:44.960 --> 00:13:45.890
So it's hard to say.

00:13:45.890 --> 00:13:47.806
It's hard to even imagine
what that thing more

00:13:47.806 --> 00:13:49.642
intelligent than
us would be like.

00:13:49.642 --> 00:13:52.100
SPEAKER: It's really hard to
imagine what it would be like,

00:13:52.100 --> 00:13:54.840
and it's even more hard to
imagine what it's going to do.

00:13:54.840 --> 00:13:56.820
And so I think most
of us are probably

00:13:56.820 --> 00:13:59.740
familiar with the paperclip
argument and the idea

00:13:59.740 --> 00:14:02.282
of anthropomorphization.

00:14:02.282 --> 00:14:05.140
Could you talk a little
about AI motivation and sort

00:14:05.140 --> 00:14:06.947
of what you wrote about that?

00:14:06.947 --> 00:14:07.613
TIM URBAN: Yeah.

00:14:07.613 --> 00:14:10.610
Well, a lot of what
I wrote about it

00:14:10.610 --> 00:14:12.990
came from Nick
Bostrom's thinking,

00:14:12.990 --> 00:14:15.890
because it just
made a lot of sense

00:14:15.890 --> 00:14:19.940
that anthropomorphizing is a
real amateur error when you're

00:14:19.940 --> 00:14:21.170
thinking about this stuff.

00:14:21.170 --> 00:14:29.650
And if you made a
spider superintelligent,

00:14:29.650 --> 00:14:31.880
bad, bad times for everyone.

00:14:31.880 --> 00:14:34.150
[LAUGHTER]

00:14:34.150 --> 00:14:38.650
But it's not going to
suddenly be Mr. Rogers,

00:14:38.650 --> 00:14:41.350
and want to be kind and
thoughtful and empathetic,

00:14:41.350 --> 00:14:43.320
because it's still in
its nature, in its DNA.

00:14:43.320 --> 00:14:44.207
It's still a spider.

00:14:44.207 --> 00:14:46.290
And so it's still going
to want the kind of-- it's

00:14:46.290 --> 00:14:48.120
going to have that core
drive still in many ways.

00:14:48.120 --> 00:14:49.080
It'll get more complex.

00:14:49.080 --> 00:14:51.580
Maybe it'll develop some version
of its own kind of empathy.

00:14:51.580 --> 00:14:55.460
But to expect that it's going
to have something similar to us,

00:14:55.460 --> 00:14:58.590
our version of empathy and
our values of valuing humans

00:14:58.590 --> 00:15:02.780
in particular is very
specific to our brains, which

00:15:02.780 --> 00:15:04.640
evolved over a
long period of time

00:15:04.640 --> 00:15:06.320
to specifically be
this way so we'd

00:15:06.320 --> 00:15:10.020
be good tribe members
in a human tribe

00:15:10.020 --> 00:15:13.200
on the Earth in this
period of time-- I mean,

00:15:13.200 --> 00:15:15.290
it's really specific.

00:15:15.290 --> 00:15:17.760
And to expect that even a
spider-- which is biology,

00:15:17.760 --> 00:15:21.720
so that still has a lot
more in common with us.

00:15:21.720 --> 00:15:24.960
To expect that even a
different kind of biology

00:15:24.960 --> 00:15:26.560
would end up there,
end up anywhere

00:15:26.560 --> 00:15:29.650
near the kind of
specific values we

00:15:29.650 --> 00:15:32.340
needed to have and the
specific form of empathy

00:15:32.340 --> 00:15:35.109
and understanding of life that
we needed to have, is very low.

00:15:35.109 --> 00:15:36.650
So then when you
talk about something

00:15:36.650 --> 00:15:38.549
that's not a human
at all, I just

00:15:38.549 --> 00:15:40.090
think it's crazy to
assume it's going

00:15:40.090 --> 00:15:42.548
to be empathetic and altruistic
once it gets to that level,

00:15:42.548 --> 00:15:45.830
because those are human
brain hormonal things.

00:15:45.830 --> 00:15:48.800
Those are not just automatic,
when something gets powerful,

00:15:48.800 --> 00:15:51.660
something gets intelligent,
it's not automatic.

00:15:51.660 --> 00:15:53.710
And so therefore, then
you have the problem

00:15:53.710 --> 00:15:55.280
of trying to program values.

00:15:55.280 --> 00:15:57.280
And you have so many
different problems of this.

00:15:57.280 --> 00:16:02.550
One is if you had a good
person, really nice,

00:16:02.550 --> 00:16:08.190
good human from the year 900 try
to program a superintelligent

00:16:08.190 --> 00:16:11.440
being with knowing what's right
and wrong, the thing would be,

00:16:11.440 --> 00:16:15.150
like, tearing the
skin off of infidels

00:16:15.150 --> 00:16:17.300
of some kind that don't
look like it, or whatever.

00:16:17.300 --> 00:16:18.680
And they'd be thinking,
I'm doing good.

00:16:18.680 --> 00:16:19.630
I'm doing good for God.

00:16:19.630 --> 00:16:20.790
I mean, it's really different.

00:16:20.790 --> 00:16:21.998
We now would say, nope, nope.

00:16:21.998 --> 00:16:23.842
Don't put that guy in
charge of anything.

00:16:23.842 --> 00:16:25.770
[LAUGHTER]

00:16:25.770 --> 00:16:26.840
We have learned a lot.

00:16:26.840 --> 00:16:31.020
We have a much more nuanced
truth about humanity now,

00:16:31.020 --> 00:16:33.090
and we are going to
do it with our values.

00:16:33.090 --> 00:16:37.857
But again, thinking just along
the same kind of analogy,

00:16:37.857 --> 00:16:40.440
there's a future that will look
at us and just think, oh, man.

00:16:40.440 --> 00:16:41.140
They are tribal.

00:16:41.140 --> 00:16:42.500
They are animal.

00:16:42.500 --> 00:16:44.840
These were really
primitive people.

00:16:44.840 --> 00:16:50.126
So it's hard for us to program
anything in a permanent sense

00:16:50.126 --> 00:16:51.500
right now, and
assume that that's

00:16:51.500 --> 00:16:52.541
going to be a good thing.

00:16:52.541 --> 00:16:54.660
Secondly, if you ask 10
people on Earth right now,

00:16:54.660 --> 00:16:56.085
OK, list what's right and wrong.

00:16:56.085 --> 00:16:57.460
How should we
program this thing?

00:16:57.460 --> 00:16:59.168
You're going to get
10 different answers.

00:16:59.168 --> 00:17:01.583
ISIS has a really good
idea of what they think

00:17:01.583 --> 00:17:02.881
the AI should be programmed as.

00:17:02.881 --> 00:17:04.839
Not because-- you know,
you can call them evil.

00:17:04.839 --> 00:17:06.420
But really, they just
think that they're

00:17:06.420 --> 00:17:08.670
doing-- they think they know
a right and wrong that we

00:17:08.670 --> 00:17:09.569
don't understand.

00:17:09.569 --> 00:17:11.069
So you have problems there.

00:17:11.069 --> 00:17:14.124
Even if we all agreed,
even if we were eternal

00:17:14.124 --> 00:17:19.690
in our understanding,
so nothing changed,

00:17:19.690 --> 00:17:20.690
you have major problems.

00:17:20.690 --> 00:17:22.481
Because you can't just
program something to

00:17:22.481 --> 00:17:25.829
do good, because we don't
want the thing to do good.

00:17:25.829 --> 00:17:29.310
We want the thing to
do good for all things,

00:17:29.310 --> 00:17:31.810
but humans for sure.

00:17:31.810 --> 00:17:33.749
We don't want it
to say, well, OK.

00:17:33.749 --> 00:17:34.790
I'm going to do good now.

00:17:34.790 --> 00:17:36.930
And look at all this
beautiful life on the planet.

00:17:36.930 --> 00:17:38.330
And, well, there's
one species that's

00:17:38.330 --> 00:17:39.320
killing a lot of the others.

00:17:39.320 --> 00:17:40.236
Let's get rid of them.

00:17:40.236 --> 00:17:41.130
[LAUGHTER]

00:17:41.130 --> 00:17:42.350
We don't want that.

00:17:42.350 --> 00:17:44.840
So we want it to
do good kind of.

00:17:44.840 --> 00:17:47.320
We wanted to also
make sure it's, like,

00:17:47.320 --> 00:17:49.080
being a selfish
dick when it comes

00:17:49.080 --> 00:17:53.304
to this species in relation to
all the others, for example.

00:17:53.304 --> 00:17:54.970
And then, of course,
you know, again, we

00:17:54.970 --> 00:17:56.136
have disagreements on Earth.

00:17:56.136 --> 00:17:58.580
So it is just such a
nightmare to even try

00:17:58.580 --> 00:18:01.484
to figure out what we
would want it to do

00:18:01.484 --> 00:18:02.650
in a way we could all agree.

00:18:02.650 --> 00:18:04.910
Then you have the
problem of, you

00:18:04.910 --> 00:18:07.370
can try to program
something a certain way

00:18:07.370 --> 00:18:10.230
and try to program
something extremely nuanced

00:18:10.230 --> 00:18:12.920
like human empathy,
and hope that then

00:18:12.920 --> 00:18:16.016
when it becomes
dramatically powerful,

00:18:16.016 --> 00:18:19.010
that what we think
we did is actually

00:18:19.010 --> 00:18:23.690
what will carry out into
this now all-powerful being.

00:18:23.690 --> 00:18:27.490
I think Bostrom and some others
believe that the initial coding

00:18:27.490 --> 00:18:28.840
will stick.

00:18:28.840 --> 00:18:32.275
He says, it doesn't matter
how smart a human gets.

00:18:32.275 --> 00:18:34.560
A human still cares
about self-preservation

00:18:34.560 --> 00:18:36.330
and reproduction, and
basic human things.

00:18:36.330 --> 00:18:37.455
That's not going to change.

00:18:37.455 --> 00:18:39.600
That's at our core, and
everything that happens

00:18:39.600 --> 00:18:40.447
comes down to that.

00:18:40.447 --> 00:18:42.030
And so a lot of these
thinkers believe

00:18:42.030 --> 00:18:44.113
that even when something
becomes superintelligent,

00:18:44.113 --> 00:18:47.650
that those core goals
and that core motivation

00:18:47.650 --> 00:18:48.950
would still be what it is.

00:18:48.950 --> 00:18:50.580
And we have to hope
we get it right.

00:18:50.580 --> 00:18:52.163
But then I've heard
others that think,

00:18:52.163 --> 00:18:53.420
that's not necessarily true.

00:18:53.420 --> 00:18:55.253
We should not assume
that something far more

00:18:55.253 --> 00:18:56.880
intelligent than we
are wouldn't begin

00:18:56.880 --> 00:18:59.300
to change its own core makeup.

00:18:59.300 --> 00:19:02.480
So again, this is when
I kind of just go,

00:19:02.480 --> 00:19:04.460
I'm happy this isn't my problem.

00:19:04.460 --> 00:19:08.100
I'm happy that I am
not one of the-- I

00:19:08.100 --> 00:19:09.480
don't have to figure this out.

00:19:09.480 --> 00:19:12.230
I get to sit back and watch
the show, and kind of root

00:19:12.230 --> 00:19:14.470
for one way.

00:19:14.470 --> 00:19:16.580
Because it's just so
incredibly daunting.

00:19:16.580 --> 00:19:18.880
Not that people shouldn't
be going for it.

00:19:18.880 --> 00:19:22.290
I'm happy that other people
are doing stuff like OpenAI.

00:19:22.290 --> 00:19:24.169
Which, even that,
I've heard people say,

00:19:24.169 --> 00:19:25.960
that's the best thing
that's ever happened.

00:19:25.960 --> 00:19:29.040
And some other people say,
that is the worst thing that

00:19:29.040 --> 00:19:31.580
could possibly happen,
that Elon and those others

00:19:31.580 --> 00:19:34.270
are totally off base with that.

00:19:34.270 --> 00:19:36.090
So it's hard to know anything.

00:19:36.090 --> 00:19:38.510
But what I do know is that a
lot more money and a lot more

00:19:38.510 --> 00:19:40.420
resources and a
lot more thinking

00:19:40.420 --> 00:19:46.970
is currently going into AI
development and then AI safety.

00:19:46.970 --> 00:19:49.606
Because AI development's
a lot more fun, exciting,

00:19:49.606 --> 00:19:50.230
and profitable.

00:19:52.930 --> 00:19:55.740
You're going to win glory
with AI development,

00:19:55.740 --> 00:20:00.184
not with AI safety.

00:20:00.184 --> 00:20:07.700
And that's one thing when it's
the American or other kind

00:20:07.700 --> 00:20:11.120
of developed country
companies working on it.

00:20:11.120 --> 00:20:14.080
But what happens when we
begin to get into an arms race

00:20:14.080 --> 00:20:15.140
with it?

00:20:15.140 --> 00:20:17.700
Even if we have good
intentions, immediately, safety

00:20:17.700 --> 00:20:19.380
goes out the door, and we
start to think we have to win.

00:20:19.380 --> 00:20:20.520
Because if we don't win,
they're going to win.

00:20:20.520 --> 00:20:22.850
And we'll take our chances
with whatever we develop.

00:20:22.850 --> 00:20:25.433
So an arms race is probably the
worst thing that could happen.

00:20:25.433 --> 00:20:26.646
And so, yeah.

00:20:26.646 --> 00:20:28.520
Now I'm just like a
crazy old man just, like,

00:20:28.520 --> 00:20:30.260
going off on endless tangents.

00:20:30.260 --> 00:20:31.560
Because this is-- but, yeah.

00:20:31.560 --> 00:20:33.447
[LAUGHTER]

00:20:33.447 --> 00:20:36.030
SPEAKER: I want to talk a little
about the reactions to those.

00:20:36.030 --> 00:20:38.200
Because I knew the community
around "Wait But Why"

00:20:38.200 --> 00:20:39.900
is a really big thing.

00:20:39.900 --> 00:20:42.460
I can see that your post
was first on our LISTSERV

00:20:42.460 --> 00:20:45.160
on February 11, 2015,
which was about three weeks

00:20:45.160 --> 00:20:47.350
after the first came out.

00:20:47.350 --> 00:20:49.144
I'll show you the
reactions later.

00:20:49.144 --> 00:20:51.810
But how have people responded to
your work on superintelligence?

00:20:54.590 --> 00:20:56.580
TIM URBAN: You
know, it's a range.

00:20:56.580 --> 00:20:59.570
The people who don't know
stuff like me, the people who

00:20:59.570 --> 00:21:01.420
were like me before I
started researching,

00:21:01.420 --> 00:21:03.020
we're all just like, whoa.

00:21:03.020 --> 00:21:05.075
I need to forward
this to everyone.

00:21:05.075 --> 00:21:05.950
I can't believe this.

00:21:05.950 --> 00:21:08.449
The same exact reaction I had
when I was reading this stuff.

00:21:08.449 --> 00:21:11.990
And then among experts,
I've gotten a few reactions.

00:21:11.990 --> 00:21:13.707
There's someone being like, yes.

00:21:13.707 --> 00:21:15.040
Finally, someone is saying this.

00:21:15.040 --> 00:21:16.880
Well, everyone should read this.

00:21:16.880 --> 00:21:19.382
And to the other side of
the spectrum being like,

00:21:19.382 --> 00:21:20.800
no one should read this.

00:21:20.800 --> 00:21:22.870
This is so misguided
and full of errors.

00:21:22.870 --> 00:21:24.828
And then there's somewhere
in the middle, which

00:21:24.828 --> 00:21:29.930
is the patting me on the head
and being like, you know,

00:21:29.930 --> 00:21:33.555
I wouldn't put my
name on it, but yes.

00:21:33.555 --> 00:21:35.370
This is fine for--

00:21:35.370 --> 00:21:36.800
[LAUGHTER]

00:21:36.800 --> 00:21:37.820
--the layman to read.

00:21:37.820 --> 00:21:39.720
This will do the job fine.

00:21:39.720 --> 00:21:41.980
Again, this is not to
be taken that seriously.

00:21:41.980 --> 00:21:42.720
But it ranges.

00:21:42.720 --> 00:21:45.410
And I've gotten, again, I've
gotten some people that are

00:21:45.410 --> 00:21:47.830
experts that actually really
think that it's a good thing

00:21:47.830 --> 00:21:49.490
for a layman to read,
and others that don't.

00:21:49.490 --> 00:21:50.280
So I don't know.

00:21:50.280 --> 00:21:51.092
I don't know.

00:21:51.092 --> 00:21:52.800
But again, those people
I'm talking about

00:21:52.800 --> 00:21:53.810
are the tiny minority.

00:21:53.810 --> 00:21:55.310
Most people were
people like me that

00:21:55.310 --> 00:21:57.070
were just totally introduced
to this for the first time.

00:21:57.070 --> 00:21:58.700
When they think of AI, they
thought of "The Terminator."

00:21:58.700 --> 00:22:00.185
I mean, it was
really simple stuff.

00:22:00.185 --> 00:22:01.560
Because what do
we know about AI?

00:22:01.560 --> 00:22:02.727
We know what movies tell us.

00:22:02.727 --> 00:22:04.476
There's a million
fiction movies about AI,

00:22:04.476 --> 00:22:06.230
and that's what people
base their info on.

00:22:06.230 --> 00:22:08.550
And there's some stuff in
those movies that is accurate

00:22:08.550 --> 00:22:10.247
or that is at least
based in science,

00:22:10.247 --> 00:22:11.705
and other stuff
that's totally not.

00:22:11.705 --> 00:22:15.100
And so I think for
that reason alone,

00:22:15.100 --> 00:22:17.710
just to introduce
this to people,

00:22:17.710 --> 00:22:20.600
to correct some
simple misconceptions,

00:22:20.600 --> 00:22:24.080
like the concept that, you
know, AI is anthropomorphic.

00:22:24.080 --> 00:22:28.080
And to understand that the
danger isn't that the AI turns

00:22:28.080 --> 00:22:30.850
on us because it wants
power, because that's

00:22:30.850 --> 00:22:35.561
anthropomorphizing, but that it
builds a new beautiful house,

00:22:35.561 --> 00:22:37.060
because that's what
it's working on.

00:22:37.060 --> 00:22:39.101
And oh, there's an ant
hill underneath the house.

00:22:39.101 --> 00:22:41.500
Not my problem, and that's
where the ant hill is, so.

00:22:41.500 --> 00:22:43.750
SPEAKER: So one of the people
who's a fan of your work

00:22:43.750 --> 00:22:45.060
is Elon Musk, like you mention.

00:22:45.060 --> 00:22:46.560
Can you talk about
working with Elon

00:22:46.560 --> 00:22:51.000
and doing the intense
four-page interview,

00:22:51.000 --> 00:22:53.440
and are you ready to
join his colony on Mars?

00:22:53.440 --> 00:22:54.880
[LAUGHTER]

00:22:55.840 --> 00:22:57.810
The metaphorical
"Mayflower," I guess.

00:22:57.810 --> 00:23:02.338
TIM URBAN: I would say there's
a 70% chance that I step

00:23:02.338 --> 00:23:03.587
on Mars at some point in life.

00:23:03.587 --> 00:23:04.087
[LAUGHTER]

00:23:04.087 --> 00:23:04.699
SPEAKER: Wow.

00:23:04.699 --> 00:23:05.990
TIM URBAN: I think that's fair.

00:23:05.990 --> 00:23:08.340
I think that's reasonable,
the more you kind of learn

00:23:08.340 --> 00:23:10.610
about where things are headed.

00:23:10.610 --> 00:23:13.824
So that's exciting.

00:23:13.824 --> 00:23:15.490
And I think there's
even a higher chance

00:23:15.490 --> 00:23:16.590
that I and most
people in this room

00:23:16.590 --> 00:23:17.839
end up in space at some point.

00:23:17.839 --> 00:23:19.820
I think it's going to be
very normal for humans

00:23:19.820 --> 00:23:22.260
to go to space, to take their
9-year-old for his birthday

00:23:22.260 --> 00:23:27.380
to space on these little
space tourism things.

00:23:27.380 --> 00:23:29.490
That's going to, I think,
become pretty common,

00:23:29.490 --> 00:23:30.540
and it's going to
be really cool.

00:23:30.540 --> 00:23:32.060
I saw something
this morning tweeted

00:23:32.060 --> 00:23:36.150
by one of the twin
astronauts, one of those two.

00:23:36.150 --> 00:23:39.500
I actually met one
of them in person.

00:23:39.500 --> 00:23:40.250
And I--

00:23:40.250 --> 00:23:41.000
AUDIENCE: You did?

00:23:41.000 --> 00:23:41.590
Which one?

00:23:41.590 --> 00:23:43.110
TIM URBAN: I met Mark.

00:23:43.110 --> 00:23:48.510
And I was like, I want
to say that I was reading

00:23:48.510 --> 00:23:52.060
all your tweets, and thank you
for being out there for a year.

00:23:52.060 --> 00:23:57.259
But maybe I should be talking to
you about your wife's heroism,

00:23:57.259 --> 00:23:58.300
because you're other one.

00:23:58.300 --> 00:24:00.873
And so I just said, thank
you for your service.

00:24:00.873 --> 00:24:02.871
[LAUGHTER]

00:24:02.871 --> 00:24:05.120
Anyway, so he tweeted some
thing about these balloons,

00:24:05.120 --> 00:24:06.990
this company that has these
huge balloons bringing things

00:24:06.990 --> 00:24:07.690
into space.

00:24:07.690 --> 00:24:08.860
It's going to be cool.

00:24:08.860 --> 00:24:10.414
Again, very off topic.

00:24:10.414 --> 00:24:11.080
So back to Elon.

00:24:14.020 --> 00:24:15.484
Yeah.

00:24:15.484 --> 00:24:19.830
Elon read the AI post, and
I think a couple others,

00:24:19.830 --> 00:24:22.660
and he tweeted a post or two.

00:24:22.660 --> 00:24:23.351
And then, yeah.

00:24:23.351 --> 00:24:25.850
And then his person got in touch
and kind of said, you know,

00:24:25.850 --> 00:24:30.340
he'd like to talk to you about
maybe doing some writing stuff.

00:24:30.340 --> 00:24:32.370
So I had, like, the
most stressful phone

00:24:32.370 --> 00:24:34.581
call of my life.

00:24:34.581 --> 00:24:38.220
And I'm in my apartment, in
my pajamas, on my headphones,

00:24:38.220 --> 00:24:39.943
and just pacing
around being like--

00:24:39.943 --> 00:24:41.360
[LAUGHTER]

00:24:41.360 --> 00:24:48.400
But we had a nice convo after
the-- kind of say that we,

00:24:48.400 --> 00:24:50.870
like-- he's super awkward.

00:24:50.870 --> 00:24:51.810
I'm kind of awkward.

00:24:51.810 --> 00:24:54.500
And so we had an awkward-off
for the first five minutes.

00:24:54.500 --> 00:24:55.000
[LAUGHTER]

00:24:55.000 --> 00:24:56.330
An old Western awkward-off.

00:24:59.376 --> 00:25:02.420
And I actually had a chance
to talk to him many times.

00:25:02.420 --> 00:25:04.920
And each time, we would begin
with a five-minute old Western

00:25:04.920 --> 00:25:05.420
awkward-off.

00:25:05.420 --> 00:25:06.325
[LAUGHTER]

00:25:06.325 --> 00:25:08.680
It takes some time to settle in.

00:25:08.680 --> 00:25:10.190
And then I read
Ashlee Vance's book,

00:25:10.190 --> 00:25:12.050
and he said that, yes,
for the first-- like,

00:25:12.050 --> 00:25:15.090
after five minutes, he
finally settles down.

00:25:15.090 --> 00:25:15.930
And I was like, yes.

00:25:15.930 --> 00:25:16.430
OK.

00:25:16.430 --> 00:25:17.263
So it's not just me.

00:25:17.263 --> 00:25:19.680
This is, like, his thing.

00:25:19.680 --> 00:25:20.625
But, no.

00:25:20.625 --> 00:25:26.610
It was really awesome
to get to just

00:25:26.610 --> 00:25:28.060
have a chance to talk to him.

00:25:28.060 --> 00:25:32.090
And he's so not like he should
be, given that he's Elon.

00:25:32.090 --> 00:25:33.830
Like, he should
be more reserved,

00:25:33.830 --> 00:25:38.430
and he should just have a lot
of-- certain kind of gravitas.

00:25:38.430 --> 00:25:39.540
But he's not.

00:25:39.540 --> 00:25:41.490
He just seems like he's
someone who's never

00:25:41.490 --> 00:25:43.330
changed from who he always was.

00:25:46.454 --> 00:25:48.120
He'll talk to me on
the phone like he'll

00:25:48.120 --> 00:25:50.619
talk in an interview, like he'll
talk to a press conference,

00:25:50.619 --> 00:25:54.610
like he'll talk to
some friends at SpaceX.

00:25:54.610 --> 00:25:57.070
He just kind of is
always who he is.

00:25:57.070 --> 00:26:01.460
And right away, was just
talking about all the things he

00:26:01.460 --> 00:26:03.890
wishes people knew more about.

00:26:03.890 --> 00:26:06.320
Why lowering the cost
of space travel's

00:26:06.320 --> 00:26:08.770
important and becoming a
space-faring civilization.

00:26:08.770 --> 00:26:12.220
Why ultimately Mars is not
just a cool thing to do,

00:26:12.220 --> 00:26:13.890
but really important.

00:26:13.890 --> 00:26:16.800
Why electric vehicles
and accelerating

00:26:16.800 --> 00:26:21.400
the advent of sustainable
energy is critically important.

00:26:21.400 --> 00:26:26.000
And so it was the biggest
no-brainer in history

00:26:26.000 --> 00:26:27.455
to dedicate time to this.

00:26:27.455 --> 00:26:29.690
It's a hero of mine
who also is taking

00:26:29.690 --> 00:26:32.480
on the most important issues
with the raddest possible

00:26:32.480 --> 00:26:33.690
companies.

00:26:33.690 --> 00:26:38.866
Very obvious person to want
to dedicate a lot of my time

00:26:38.866 --> 00:26:40.980
to helping and working
on it in any way I could.

00:26:40.980 --> 00:26:42.550
So, yeah.

00:26:42.550 --> 00:26:45.490
Then I had a chance to
go out to the factories

00:26:45.490 --> 00:26:47.450
and meet some of the
different executives,

00:26:47.450 --> 00:26:48.510
and really talk to them.

00:26:48.510 --> 00:26:50.310
And there was a lot of trust.

00:26:50.310 --> 00:26:52.780
We kind of had this agreement.

00:26:52.780 --> 00:26:55.770
Because this is not, like, an
old organization who just says,

00:26:55.770 --> 00:26:58.390
well, this is how
we do press here.

00:26:58.390 --> 00:26:59.770
They kind of said, yeah.

00:26:59.770 --> 00:27:00.490
Talk to people.

00:27:00.490 --> 00:27:02.760
And they're not
all press-approved,

00:27:02.760 --> 00:27:06.840
so just let us know stuff
they say, and make sure.

00:27:06.840 --> 00:27:08.120
And it was great.

00:27:08.120 --> 00:27:10.330
They basically told everyone,
like, trust this guy.

00:27:10.330 --> 00:27:11.080
Tell him whatever.

00:27:11.080 --> 00:27:12.371
I don't know why, but they did.

00:27:12.371 --> 00:27:13.670
[LAUGHTER]

00:27:13.670 --> 00:27:17.680
And I honored that by-- I kept
my journalistic integrity,

00:27:17.680 --> 00:27:20.210
because I said I was publishing
this on "Wait But Why,"

00:27:20.210 --> 00:27:22.450
and I was going to be able
to do it the way I wanted.

00:27:22.450 --> 00:27:24.740
But I said, for quotes,
I am happy to send them

00:27:24.740 --> 00:27:27.790
to you, to let you see if
you want them all in there.

00:27:27.790 --> 00:27:29.860
Like, this is not some
weird journal rules,

00:27:29.860 --> 00:27:32.359
like, journalism rules
that I just don't even

00:27:32.359 --> 00:27:33.150
know what they are.

00:27:33.150 --> 00:27:34.250
And I don't care.

00:27:34.250 --> 00:27:36.060
Why have everything
be so adversarial?

00:27:36.060 --> 00:27:36.980
It's just like, sure.

00:27:36.980 --> 00:27:37.480
Check them.

00:27:37.480 --> 00:27:39.710
If you want to change your
quote, like, you said.

00:27:39.710 --> 00:27:42.168
If it's not what you want to
say, like, say something else.

00:27:42.168 --> 00:27:43.050
It's not like, ha!

00:27:43.050 --> 00:27:44.620
That was the honest thing,
and now you're lying.

00:27:44.620 --> 00:27:46.167
You're still you saying stuff.

00:27:46.167 --> 00:27:47.450
[LAUGHTER]

00:27:47.450 --> 00:27:50.080
And the truth is, they
changed almost nothing at all.

00:27:50.080 --> 00:27:52.430
There was tiny little
things like the bandwidth

00:27:52.430 --> 00:27:53.990
of their satellite internet.

00:27:53.990 --> 00:27:55.130
They didn't want their
competitors-- I mean,

00:27:55.130 --> 00:27:56.020
the tiniest thing.

00:27:56.020 --> 00:27:56.800
So it was great.

00:27:56.800 --> 00:27:58.880
It could not have been
a more fun project.

00:27:58.880 --> 00:28:00.800
It got totally out
of hand, lengthwise.

00:28:00.800 --> 00:28:08.811
It was a four-post,
95,000-word thing.

00:28:08.811 --> 00:28:09.310
But yeah.

00:28:09.310 --> 00:28:10.643
It could not have been more fun.

00:28:10.643 --> 00:28:13.880
And I learned so much
about all those industries,

00:28:13.880 --> 00:28:18.700
about entrepreneurship, about
how technology progresses

00:28:18.700 --> 00:28:20.400
and moves in general.

00:28:20.400 --> 00:28:21.960
I had to study the past a lot.

00:28:21.960 --> 00:28:23.590
I had to study
electrical, you know,

00:28:23.590 --> 00:28:26.890
revolution in the
1880s and Henry Ford,

00:28:26.890 --> 00:28:27.900
and everything he did.

00:28:27.900 --> 00:28:31.160
And I understand how airplanes
came about, and all that.

00:28:31.160 --> 00:28:32.740
So it was just super fun.

00:28:32.740 --> 00:28:35.220
And also, probably
most importantly,

00:28:35.220 --> 00:28:38.140
I got to examine
how Elon thinks,

00:28:38.140 --> 00:28:39.730
which is totally the secret.

00:28:39.730 --> 00:28:43.630
I mean, I'm so convinced that
he's smarter than everyone here

00:28:43.630 --> 00:28:46.676
and richer and more
ambitious and more insane,

00:28:46.676 --> 00:28:48.050
harder working,
all those things.

00:28:48.050 --> 00:28:51.030
But honestly, if that was it,
there would be more Elons.

00:28:51.030 --> 00:28:54.400
I mean, it's his
incredible ability

00:28:54.400 --> 00:28:59.720
to really genuinely reason
from first principles in almost

00:28:59.720 --> 00:29:02.330
everything he does
that's important.

00:29:02.330 --> 00:29:06.610
That is a theme with all those
icons in history of all kinds,

00:29:06.610 --> 00:29:10.540
art and entrepreneurship and
politics and whatever, who

00:29:10.540 --> 00:29:11.790
end up really changing things.

00:29:11.790 --> 00:29:15.510
And so just to see that
up close and to examine it

00:29:15.510 --> 00:29:18.770
with actual quotes--
he has this quote.

00:29:18.770 --> 00:29:21.964
When he was, like,
seven or something--

00:29:21.964 --> 00:29:23.380
this is from the
Ashlee Vance book

00:29:23.380 --> 00:29:28.604
where he says something
like he would go to a girl

00:29:28.604 --> 00:29:30.770
during recess who would
say, I'm scared of the dark.

00:29:30.770 --> 00:29:32.686
And he'd say, I used to
be scared of the dark,

00:29:32.686 --> 00:29:34.490
and then I realized
that dark is just

00:29:34.490 --> 00:29:38.200
the absence of photons in the
visible light wavelengths,

00:29:38.200 --> 00:29:40.575
and 500 to 700 nanometers.

00:29:40.575 --> 00:29:41.997
[LAUGHTER]

00:29:41.997 --> 00:29:44.330
And so when we see that, we
hear that, we say, oh, look.

00:29:44.330 --> 00:29:47.220
You know, like, of course,
he's being a little weird,

00:29:47.220 --> 00:29:49.200
but he's of course
being rational.

00:29:49.200 --> 00:29:50.870
And the cute kid is
scared of the dark.

00:29:50.870 --> 00:29:52.810
We all know the dark
isn't actually scary.

00:29:52.810 --> 00:29:56.060
And good for Elon
for, like, adorably

00:29:56.060 --> 00:29:57.060
seeing it like an adult.

00:29:57.060 --> 00:30:03.040
And yet he says a quote 30, 40
years later in some interviewer

00:30:03.040 --> 00:30:06.311
that I watched on YouTube
where he says, you know,

00:30:06.311 --> 00:30:08.810
I don't know why people are so
scared of starting a company.

00:30:08.810 --> 00:30:10.330
Really, what's the worse
that's going to happen?

00:30:10.330 --> 00:30:11.330
You're not going to die.

00:30:11.330 --> 00:30:12.580
You're not going to starve.

00:30:12.580 --> 00:30:13.835
Like, what's the worst
that's going to happen

00:30:13.835 --> 00:30:14.900
if more people start a company?

00:30:14.900 --> 00:30:16.710
And I'm thinking,
that's the same quote.

00:30:16.710 --> 00:30:18.560
That's the visible
photon absence

00:30:18.560 --> 00:30:20.340
of-- that is the same quote.

00:30:20.340 --> 00:30:22.670
And the only difference
is he's the only one being

00:30:22.670 --> 00:30:25.290
the adult there, and we're all
the little girl who's saying,

00:30:25.290 --> 00:30:26.526
I'm scared of the dark.

00:30:26.526 --> 00:30:28.150
And I had so many
epiphanies like that,

00:30:28.150 --> 00:30:30.660
looking at simple
things about the way he

00:30:30.660 --> 00:30:31.740
talks and things he does.

00:30:31.740 --> 00:30:34.419
I say, that is the
whole key right there.

00:30:34.419 --> 00:30:35.210
That is the answer.

00:30:35.210 --> 00:30:37.200
So that ended up
being the fourth post

00:30:37.200 --> 00:30:39.100
of the series, just
about how he thinks,

00:30:39.100 --> 00:30:42.160
which has totally changed
the way I at least attempt

00:30:42.160 --> 00:30:43.840
to grow in my thinking.

00:30:43.840 --> 00:30:45.340
SPEAKER: One of the
things that Elon

00:30:45.340 --> 00:30:47.760
won't touch on publicly,
and actually Google either,

00:30:47.760 --> 00:30:48.817
is cryonics.

00:30:48.817 --> 00:30:50.650
But you've recently
written a post about it.

00:30:50.650 --> 00:30:53.450
And in fact, you said, you've
quit cryo-procrastinating,

00:30:53.450 --> 00:30:55.647
and that you registered
yourself, right?

00:30:55.647 --> 00:30:56.480
TIM URBAN: Well, OK.

00:30:56.480 --> 00:30:57.400
So what happened is--

00:30:57.400 --> 00:30:58.360
[LAUGHTER]

00:30:58.360 --> 00:31:00.120
No, I really-- so I
made an appointment.

00:31:00.120 --> 00:31:01.510
[LAUGHTER]

00:31:01.510 --> 00:31:05.410
And I talked to Alcor, and I've
talked to a life insurance guy.

00:31:05.410 --> 00:31:08.880
And I have the plan
worked out, and I

00:31:08.880 --> 00:31:11.777
am going to be-- I will
pledge to be signed up fully

00:31:11.777 --> 00:31:12.360
in six months.

00:31:12.360 --> 00:31:13.693
I'm good to die in six months.

00:31:13.693 --> 00:31:14.580
[LAUGHTER]

00:31:14.580 --> 00:31:19.270
But then what happened was I
was close, and he was like, OK.

00:31:19.270 --> 00:31:21.460
So here is like 17 attachments.

00:31:21.460 --> 00:31:23.380
Just print those out
and fill those out.

00:31:23.380 --> 00:31:25.590
And you need, like, weird
scans of other things.

00:31:25.590 --> 00:31:28.089
And you need to just make sure
you have this information,

00:31:28.089 --> 00:31:29.880
then fill those out,
things with the boxes.

00:31:29.880 --> 00:31:31.505
And I just suddenly
was like, yeah, OK.

00:31:31.505 --> 00:31:33.010
I'll do that in the fall.

00:31:33.010 --> 00:31:36.420
So I am
cryo-procrastinating now.

00:31:36.420 --> 00:31:39.420
But the real daunting
thing is like, I don't even

00:31:39.420 --> 00:31:40.600
know how to begin.

00:31:40.600 --> 00:31:41.570
Do I have to call?

00:31:41.570 --> 00:31:42.942
How do I trust this company?

00:31:42.942 --> 00:31:44.150
That work, I've done already.

00:31:44.150 --> 00:31:45.970
So that's why I have
faith that I will do it,

00:31:45.970 --> 00:31:47.969
because I've done all of
the really icky things.

00:31:47.969 --> 00:31:50.440
And I just need to
finish the deal there

00:31:50.440 --> 00:31:54.359
with a pretty cheap monthly
premium life insurance policy,

00:31:54.359 --> 00:31:55.150
and I'm like, good.

00:31:55.150 --> 00:31:55.650
Covered.

00:31:55.650 --> 00:32:00.585
Like, everyone should do that if
living a long time sounds fun.

00:32:00.585 --> 00:32:01.960
Not that it'll
definitely happen,

00:32:01.960 --> 00:32:04.190
but it definitely will not
happen if you don't do it.

00:32:04.190 --> 00:32:05.084
[LAUGHTER]

00:32:05.085 --> 00:32:07.210
SPEAKER: In the post, you
talk about the difference

00:32:07.210 --> 00:32:09.790
between being dead
and mostly dead.

00:32:09.790 --> 00:32:11.480
Could you talk a
little bit about why

00:32:11.480 --> 00:32:12.600
that matters for cryonics?

00:32:12.600 --> 00:32:13.266
TIM URBAN: Yeah.

00:32:13.266 --> 00:32:18.470
So dead is not nearly
as easily defined a word

00:32:18.470 --> 00:32:21.490
as we all think it is.

00:32:21.490 --> 00:32:23.967
50 years ago, someone
collapses on the street.

00:32:23.967 --> 00:32:25.050
Their heart stops beating.

00:32:25.050 --> 00:32:26.250
They're not breathing.

00:32:26.250 --> 00:32:26.950
They're dead.

00:32:26.950 --> 00:32:31.510
They're declared dead, and
that's the end of them.

00:32:31.510 --> 00:32:34.760
Today if that happens, they're
rushed to the hospital.

00:32:34.760 --> 00:32:37.180
We have resuscitators
of all different kinds.

00:32:37.180 --> 00:32:41.872
And we can usually often
get them going again,

00:32:41.872 --> 00:32:43.330
and then figure
out what was wrong.

00:32:43.330 --> 00:32:47.340
And very often, those people
live another bunch of decades.

00:32:47.340 --> 00:32:49.130
So they weren't dead.

00:32:49.130 --> 00:32:54.500
They were unable to be saved
with technology 50 years ago.

00:32:54.500 --> 00:32:57.190
So again, the analogy
applies today.

00:32:57.190 --> 00:33:02.110
There's clinical death, which
is when your heart stops beating

00:33:02.110 --> 00:33:03.165
and you stop breathing.

00:33:03.165 --> 00:33:05.070
Again, it's been a little
bit since I wrote the post.

00:33:05.070 --> 00:33:06.690
I think that's the
exact definition.

00:33:06.690 --> 00:33:09.002
And that's clinical
death, meaning--

00:33:09.002 --> 00:33:11.210
the difference between
clinical death and legal death

00:33:11.210 --> 00:33:14.000
is that clinical
death, there usually

00:33:14.000 --> 00:33:16.960
could be procedures that
could try-- we know that you

00:33:16.960 --> 00:33:18.390
are potentially saveable.

00:33:18.390 --> 00:33:22.830
But if you have a
do-not-resuscitate contract

00:33:22.830 --> 00:33:25.430
that you've signed,
you know, if they

00:33:25.430 --> 00:33:26.930
know they don't
have much time left,

00:33:26.930 --> 00:33:29.221
they have a terminally ill
thing and they're suffering,

00:33:29.221 --> 00:33:32.980
and once that stops, that's
kind of a way for the hospital

00:33:32.980 --> 00:33:35.630
to legally say,
let's let this go.

00:33:35.630 --> 00:33:37.450
And the person would
like to let it go.

00:33:37.450 --> 00:33:40.120
That's clinical death.

00:33:40.120 --> 00:33:44.750
Legal death happens once
the heart has stopped

00:33:44.750 --> 00:33:47.067
and breathing has stopped
for, I think, five minutes.

00:33:47.067 --> 00:33:48.150
Maybe five or six minutes.

00:33:48.150 --> 00:33:49.830
I forget what the
exact number is.

00:33:49.830 --> 00:33:52.550
And that's when
you are, no matter

00:33:52.550 --> 00:33:56.440
what they-- at that point, no
matter what they tried to do,

00:33:56.440 --> 00:33:59.740
you can't salvage that, because
you have now become brain dead.

00:33:59.740 --> 00:34:02.960
And brain death is the
definition of legal death.

00:34:02.960 --> 00:34:04.640
So again, people
consider-- today,

00:34:04.640 --> 00:34:07.130
there's clinical death, which
we can treat like death.

00:34:07.130 --> 00:34:08.239
It's not assisted suicide.

00:34:08.239 --> 00:34:09.506
We can treat death legally.

00:34:09.506 --> 00:34:11.880
And then there's legal death
when you're officially done.

00:34:11.880 --> 00:34:12.963
There's nothing we can do.

00:34:12.963 --> 00:34:15.330
Now what cryonicists say--
and these people are--

00:34:15.330 --> 00:34:17.570
I was super suspicious
going into this.

00:34:17.570 --> 00:34:19.520
Are these, like,
Scientologist-type people?

00:34:19.520 --> 00:34:22.226
Are these going to be
people that are-- you know.

00:34:22.226 --> 00:34:24.650
These are hardcore scientists.

00:34:24.650 --> 00:34:28.580
These are completely, utterly
rational, reasonable people

00:34:28.580 --> 00:34:30.289
who don't attack the
people who disagree.

00:34:30.289 --> 00:34:32.496
They say, I understand why
you think that, and here's

00:34:32.496 --> 00:34:33.130
our thinking.

00:34:33.130 --> 00:34:33.750
They're great.

00:34:33.750 --> 00:34:37.810
The Alcor website FAQ is
just such a smart place.

00:34:37.810 --> 00:34:42.889
And what they say is,
today, after five minutes,

00:34:42.889 --> 00:34:46.880
you are unable to
be saved in 2016.

00:34:46.880 --> 00:34:49.840
But you very well might be
able to be saved and restored,

00:34:49.840 --> 00:34:53.040
and that illness you have, that
even if you could be saved,

00:34:53.040 --> 00:34:54.675
maybe you only have
a few weeks anyway,

00:34:54.675 --> 00:34:56.800
it's going to be suffering,
well maybe that illness

00:34:56.800 --> 00:34:59.930
also in a hospital
in 2040, no problem.

00:34:59.930 --> 00:35:00.910
We can fix that.

00:35:00.910 --> 00:35:03.974
We can fix that, and we can
also easily resuscitate you

00:35:03.974 --> 00:35:04.640
and restore you.

00:35:04.640 --> 00:35:07.110
Even if you're brain
dead today, they

00:35:07.110 --> 00:35:09.000
explain that brain
death is not actually

00:35:09.000 --> 00:35:10.630
that your brain
is beyond repair.

00:35:10.630 --> 00:35:13.760
It's that the state
of your body is such

00:35:13.760 --> 00:35:16.050
that when blood
gets going again,

00:35:16.050 --> 00:35:18.790
it will damage the
cells in your brain.

00:35:18.790 --> 00:35:20.630
So there's no way
to bring you back

00:35:20.630 --> 00:35:23.870
without then
irreparably killing you.

00:35:23.870 --> 00:35:26.010
Again, 2040, 2060.

00:35:26.010 --> 00:35:28.640
Those hospitals, they might
say, this is no problem for us.

00:35:28.640 --> 00:35:30.660
We easily have technology
to help this person.

00:35:30.660 --> 00:35:34.320
So what cryonics wants to
do, because they do consider

00:35:34.320 --> 00:35:38.980
you still very much alive
and just beyond hope today,

00:35:38.980 --> 00:35:41.740
they want to put you
on biological pause

00:35:41.740 --> 00:35:43.080
by vitrifying you.

00:35:43.080 --> 00:35:45.080
Not freezing you, because
then the ice crystals

00:35:45.080 --> 00:35:46.510
would destroy your cells.

00:35:46.510 --> 00:35:49.500
Vitrifying, which is putting
you in a glass-like state where

00:35:49.500 --> 00:35:53.420
essentially your cells,
the activity and the atoms

00:35:53.420 --> 00:35:57.180
in your body have just slowed
so much that they can't move.

00:35:57.180 --> 00:35:59.310
But they put antifreeze
solution, actually,

00:35:59.310 --> 00:36:02.870
in your veins so that it will
not be able to congeal into it

00:36:02.870 --> 00:36:04.850
or to crystallize into ice.

00:36:04.850 --> 00:36:07.900
So you end up vitrified
in this biological pause.

00:36:07.900 --> 00:36:11.040
And they're incredibly
honest and fair about saying,

00:36:11.040 --> 00:36:14.019
there's a good chance that
the way we're doing this today

00:36:14.019 --> 00:36:16.060
will have messed something
up, and in the future,

00:36:16.060 --> 00:36:18.740
even if they had the technology,
they won't be able to save you.

00:36:18.740 --> 00:36:20.890
Or that that technology
will never happen,

00:36:20.890 --> 00:36:26.930
or that you'll be there, and
reanimating vitrified people

00:36:26.930 --> 00:36:29.570
is not a top priority of theirs.

00:36:29.570 --> 00:36:31.525
We don't know.

00:36:31.525 --> 00:36:33.650
But they believe that it
will be, because they say,

00:36:33.650 --> 00:36:36.850
is it a priority to help someone
who's in a coma get out of it?

00:36:36.850 --> 00:36:37.350
Of course.

00:36:37.350 --> 00:36:39.520
And not because the doctor cares
about you or is a good person.

00:36:39.520 --> 00:36:40.710
Because it's their job.

00:36:40.710 --> 00:36:43.335
And they think that once people
start being able to be revived,

00:36:43.335 --> 00:36:45.420
a vitrified person is
just as much a living

00:36:45.420 --> 00:36:46.850
patient as anyone else.

00:36:46.850 --> 00:36:49.030
So anyway, they say,
this might not work.

00:36:49.030 --> 00:36:50.960
But at least it gives a shot.

00:36:50.960 --> 00:36:53.290
And it's putting a bet
on the future technology.

00:36:53.290 --> 00:36:55.060
And you know, if
you look at the past

00:36:55.060 --> 00:36:58.250
and how much we would have blown
the people's minds in centuries

00:36:58.250 --> 00:37:00.875
past had we brought
them to today,

00:37:00.875 --> 00:37:02.500
I don't want to bet
against the future.

00:37:02.500 --> 00:37:03.749
The future is going to be rad.

00:37:03.749 --> 00:37:05.080
It's going to shock us.

00:37:05.080 --> 00:37:06.291
So I'll take my chances.

00:37:06.291 --> 00:37:06.790
Sure.

00:37:06.790 --> 00:37:08.267
Like, here's my vitrified body.

00:37:08.267 --> 00:37:09.100
See what you can do.

00:37:09.100 --> 00:37:13.020
It's better than the
alternative, which is really

00:37:13.020 --> 00:37:14.020
shitty, the alternative.

00:37:14.020 --> 00:37:14.675
[LAUGHTER]

00:37:14.675 --> 00:37:16.050
SPEAKER: Many
people in this room

00:37:16.050 --> 00:37:18.570
are aware of the potential
impact of superintelligence.

00:37:18.570 --> 00:37:20.057
We talked about it.

00:37:20.057 --> 00:37:21.890
In fact, many of us are
on the bleeding edge

00:37:21.890 --> 00:37:23.010
of its development.

00:37:23.010 --> 00:37:24.530
What message would
you pass along

00:37:24.530 --> 00:37:26.946
to people who are developing
artificial intelligence right

00:37:26.946 --> 00:37:28.540
now?

00:37:28.540 --> 00:37:30.283
TIM URBAN: Honestly,
I want to be like,

00:37:30.283 --> 00:37:32.070
oh, don't forget about safety.

00:37:32.070 --> 00:37:38.350
But I think those people are
aware of the safety problem

00:37:38.350 --> 00:37:39.840
already.

00:37:39.840 --> 00:37:41.360
It's not like
they're not-- these

00:37:41.360 --> 00:37:43.539
are really intelligent
people, and I

00:37:43.539 --> 00:37:44.830
think that they're aware of it.

00:37:44.830 --> 00:37:48.909
And I think that they
are trying to do it

00:37:48.909 --> 00:37:49.950
in the best way possible.

00:37:49.950 --> 00:37:52.329
I think they know that
this is going to happen.

00:37:52.329 --> 00:37:53.370
Someone's going to do it.

00:37:53.370 --> 00:37:54.953
The human species
is going to do this.

00:37:54.953 --> 00:37:58.270
You can't stop the species as a
whole from building technology

00:37:58.270 --> 00:37:59.950
that it wants to build.

00:37:59.950 --> 00:38:02.420
So I would just say,
like, good luck.

00:38:02.420 --> 00:38:03.093
Keep going.

00:38:03.093 --> 00:38:03.832
[LAUGHTER]

00:38:03.832 --> 00:38:05.790
Seriously, I think they're
doing awesome stuff,

00:38:05.790 --> 00:38:07.010
and I just hope
it works out well.

00:38:07.010 --> 00:38:08.218
And someone's going to do it.

00:38:08.218 --> 00:38:09.495
And yeah.

00:38:09.495 --> 00:38:10.370
SPEAKER: Thanks, Tim.

00:38:10.370 --> 00:38:11.036
TIM URBAN: Yeah.

00:38:11.036 --> 00:38:15.140
[APPLAUSE]

00:38:19.580 --> 00:38:21.080
SPEAKER: Now if you
don't mind, we'd

00:38:21.080 --> 00:38:22.280
like to move into questions
from the audience.

00:38:22.280 --> 00:38:23.090
TIM URBAN: Yes.

00:38:23.090 --> 00:38:26.129
AUDIENCE: Who should we
invite to speak next?

00:38:26.129 --> 00:38:28.170
TIM URBAN: I mean, you're
asking at a weird time,

00:38:28.170 --> 00:38:30.320
because I'm working
on a post that's

00:38:30.320 --> 00:38:34.510
I'm about to do a big VR post,
very much, like, in this world.

00:38:34.510 --> 00:38:36.860
But at the moment, I'm
working on a post that

00:38:36.860 --> 00:38:40.670
is completely unrelated
to technology,

00:38:40.670 --> 00:38:44.570
and it has to do with,
like, our culture

00:38:44.570 --> 00:38:46.730
kind of censoring freedom
of speech a little bit,

00:38:46.730 --> 00:38:48.760
and me being scared
as a blogger to write

00:38:48.760 --> 00:38:52.060
about all kinds of
important social issues

00:38:52.060 --> 00:38:54.900
that I shouldn't be
scared to write about.

00:38:54.900 --> 00:38:58.860
And as, like, a liberal guy,
I'm scared of the liberals here.

00:38:58.860 --> 00:39:05.227
And so I would say someone
like Sam Harris or, you know--

00:39:05.227 --> 00:39:07.060
AUDIENCE: Who's also
[INAUDIBLE] this topic.

00:39:07.060 --> 00:39:07.727
TIM URBAN: Yeah.

00:39:07.727 --> 00:39:08.226
Right.

00:39:08.226 --> 00:39:10.110
And Sam Harris also is
awesome when he talks

00:39:10.110 --> 00:39:13.930
about AI or any of this stuff.

00:39:13.930 --> 00:39:15.456
He's just one of
these people that's

00:39:15.456 --> 00:39:16.830
interested in
interesting things.

00:39:16.830 --> 00:39:18.800
But he's my intellectual hero.

00:39:18.800 --> 00:39:23.150
He really is, like, just
super logical and super brave.

00:39:23.150 --> 00:39:25.442
SPEAKER: The question
of from submitted.

00:39:25.442 --> 00:39:27.900
Could you talk about learning
new subject areas well enough

00:39:27.900 --> 00:39:29.020
to write about them?

00:39:29.020 --> 00:39:30.550
Are there signs
you personally use

00:39:30.550 --> 00:39:32.690
for gauging the soundness
of your own understanding

00:39:32.690 --> 00:39:33.780
of the new area?

00:39:33.780 --> 00:39:34.934
What are they?

00:39:34.934 --> 00:39:35.600
TIM URBAN: Yeah.

00:39:35.600 --> 00:39:39.330
So I very much am
clear about what

00:39:39.330 --> 00:39:40.646
I am, which is not an expert.

00:39:40.646 --> 00:39:42.020
I will never try
to be an expert.

00:39:42.020 --> 00:39:45.800
I'm not going to try to
teach experts anything.

00:39:45.800 --> 00:39:50.592
What I'm trying to do is
the equivalent of enough

00:39:50.592 --> 00:39:52.800
where I could sit down at,
like, a table with friends

00:39:52.800 --> 00:39:54.230
and just be like, OK.

00:39:54.230 --> 00:39:56.890
Listen to this thing I've been
reading about for 40 hours.

00:39:56.890 --> 00:40:00.500
And by the time it's done,
they're like, oh, OK.

00:40:00.500 --> 00:40:01.170
I'm with you.

00:40:01.170 --> 00:40:02.725
Like, I want to go
from here to here.

00:40:02.725 --> 00:40:05.350
If the expert's at the ceiling,
I want to go from here to here,

00:40:05.350 --> 00:40:08.110
and then get a lot of people
that are curious like me here

00:40:08.110 --> 00:40:09.140
up there with me.

00:40:09.140 --> 00:40:12.290
And so for me, that
is often simply

00:40:12.290 --> 00:40:15.180
I'm reading smart
people saying stuff,

00:40:15.180 --> 00:40:18.650
and then I'm trying to--
and the thing that I'm

00:40:18.650 --> 00:40:21.540
good at that sometimes they're
not is taking what they're

00:40:21.540 --> 00:40:24.090
saying, especially if
they're in multiple views,

00:40:24.090 --> 00:40:26.729
and building some kind
of memorable framework

00:40:26.729 --> 00:40:28.270
to put them all
together in something

00:40:28.270 --> 00:40:29.670
where someone can read that.

00:40:29.670 --> 00:40:32.592
And it's one thing to read
it and understand it then,

00:40:32.592 --> 00:40:34.300
but maybe six months
later, still kind of

00:40:34.300 --> 00:40:38.980
have that in your head because
the framework or the terms,

00:40:38.980 --> 00:40:41.880
or something about the
way it was articulated

00:40:41.880 --> 00:40:45.110
helps you build that permanent
framework in your head.

00:40:45.110 --> 00:40:46.480
So that's my job.

00:40:46.480 --> 00:40:48.440
And sometimes it's easier.

00:40:51.020 --> 00:40:54.260
I found cryonics to be easier,
because the cryonicists are all

00:40:54.260 --> 00:40:55.600
saying the same thing.

00:40:55.600 --> 00:40:58.870
And then there's a lot of
people who don't understand it

00:40:58.870 --> 00:41:01.760
who are not to be
taken seriously.

00:41:01.760 --> 00:41:04.049
Ironically, the
cryogenecists who

00:41:04.049 --> 00:41:06.340
are the ones who work with
cold temperatures for things

00:41:06.340 --> 00:41:08.320
like preserving organs
and other things

00:41:08.320 --> 00:41:09.960
like that, they hate cryonics.

00:41:09.960 --> 00:41:12.340
It's like the nerd
war of the cold.

00:41:12.340 --> 00:41:13.900
It's like this odd situation.

00:41:13.900 --> 00:41:16.420
But just again and again, I
read quotes from them that

00:41:16.420 --> 00:41:18.253
would say, oh-- it was
just immediately say,

00:41:18.253 --> 00:41:21.600
oh, you're being like
a lying politician.

00:41:21.600 --> 00:41:22.840
You're just slandering them.

00:41:22.840 --> 00:41:25.820
So I would just rule them out
as interesting intellectuals.

00:41:25.820 --> 00:41:30.690
You know, and I would find--
I look for real dissent.

00:41:30.690 --> 00:41:33.210
Because the thing I don't want
to do is read a viewpoint,

00:41:33.210 --> 00:41:35.377
and there's a big very
valid dissent viewpoint,

00:41:35.377 --> 00:41:35.960
and I miss it.

00:41:35.960 --> 00:41:38.300
And then I present
this very strongly.

00:41:38.300 --> 00:41:39.840
If there is a valid
dissent point,

00:41:39.840 --> 00:41:42.150
I want to read both
and then present both.

00:41:42.150 --> 00:41:45.540
So for me, it's just
making sure that I've

00:41:45.540 --> 00:41:50.700
identified what the various
relevant viewpoints are.

00:41:50.700 --> 00:41:54.690
And then once I feel like I've
read a bunch of all of them

00:41:54.690 --> 00:41:58.570
and I'm not being
propaganded by one of them,

00:41:58.570 --> 00:42:00.140
then I'm ready to go.

00:42:00.140 --> 00:42:03.220
I don't need to go
much further than that.

00:42:03.220 --> 00:42:05.130
AUDIENCE: So there
are some concepts

00:42:05.130 --> 00:42:07.790
that make it hard
to think accurately.

00:42:07.790 --> 00:42:11.730
Superstition, for example, makes
it hard to do science right.

00:42:11.730 --> 00:42:13.660
And I'm wondering, I
was struck by how often

00:42:13.660 --> 00:42:17.810
you use the word insane in
talking about AI researchers.

00:42:17.810 --> 00:42:20.900
I'm wondering if it's possible
that there's some concept that

00:42:20.900 --> 00:42:25.330
is contagious and sticky
and pervasive in AI

00:42:25.330 --> 00:42:30.220
thinking-- perhaps
consciousness-- that makes

00:42:30.220 --> 00:42:33.070
it very hard to think sanely.

00:42:33.070 --> 00:42:36.570
And so when we ask
AI researchers today,

00:42:36.570 --> 00:42:41.180
it's like asking a medieval
theologian about science.

00:42:41.180 --> 00:42:42.530
TIM URBAN: Yeah.

00:42:42.530 --> 00:42:45.844
I was having a discussion
the other day about, what

00:42:45.844 --> 00:42:48.510
are the things that are going to
be really offensive in 50 years

00:42:48.510 --> 00:42:52.192
that we're just going for it
with now without realizing it?

00:42:52.192 --> 00:42:53.650
One of the things
someone mentioned

00:42:53.650 --> 00:42:55.210
was, like, the word
crazy and insane

00:42:55.210 --> 00:42:58.160
might be, like, highly
offensive in 50 years.

00:42:58.160 --> 00:43:00.910
Anyway, just since I
throw it out there, like,

00:43:00.910 --> 00:43:04.780
20 times in this conversation,
I'll mention that.

00:43:04.780 --> 00:43:07.750
I think that to me, the
thing that just strikes me

00:43:07.750 --> 00:43:12.110
as, oh, there could be
a total delusion here

00:43:12.110 --> 00:43:17.420
or they could be a smarter
future people or smarter

00:43:17.420 --> 00:43:19.870
future something
might look at us today

00:43:19.870 --> 00:43:23.070
and say, oh, they were totally
missing something critical.

00:43:23.070 --> 00:43:24.760
I think that's
what you're asking.

00:43:24.760 --> 00:43:28.760
AUDIENCE: Something has to be
disbelieved in, like miracles,

00:43:28.760 --> 00:43:31.130
in order to think
accurately about the way

00:43:31.130 --> 00:43:33.180
the scientific world works.

00:43:33.180 --> 00:43:35.830
TIM URBAN: It may have
to do with consciousness.

00:43:35.830 --> 00:43:37.820
It might have to
do with-- there's

00:43:37.820 --> 00:43:40.084
a big distinction--
again, I have trouble

00:43:40.084 --> 00:43:41.500
with the concept
of consciousness.

00:43:41.500 --> 00:43:45.840
But a distinction between if
we do believe in consciousness,

00:43:45.840 --> 00:43:50.200
and then, if AI can become
conscious-- if whatever

00:43:50.200 --> 00:43:52.750
codes that in our brain
isn't some magical thing,

00:43:52.750 --> 00:43:56.040
it's just neurons firing
in a certain pattern.

00:43:56.040 --> 00:44:00.200
So if we can replicate
that, we actually

00:44:00.200 --> 00:44:02.670
might view it as not
even necessarily bad

00:44:02.670 --> 00:44:04.530
if it wins and,
like, we end up out.

00:44:04.530 --> 00:44:07.520
Because it's more important
than we are in that case.

00:44:07.520 --> 00:44:10.220
We're more
important-- if someone

00:44:10.220 --> 00:44:12.830
kills a dog, that's
bad, but way less bad

00:44:12.830 --> 00:44:14.380
than if they kill a human.

00:44:14.380 --> 00:44:19.070
And because humans are smarter,
they have a higher capacity

00:44:19.070 --> 00:44:22.540
for suffering and for joy.

00:44:22.540 --> 00:44:24.770
And so we consider
ourselves more important.

00:44:24.770 --> 00:44:25.745
[LAUGH]

00:44:25.745 --> 00:44:27.260
He's looking at
me like I'm crazy.

00:44:27.260 --> 00:44:27.770
But if--

00:44:27.770 --> 00:44:28.640
[LAUGHTER]

00:44:28.640 --> 00:44:30.980
--if we get to a
superintelligent AI that

00:44:30.980 --> 00:44:33.170
is conscious, I
think in the future

00:44:33.170 --> 00:44:34.610
we could look at
it as very myopic

00:44:34.610 --> 00:44:36.609
that we thought that we
were this critical piece

00:44:36.609 --> 00:44:39.379
of the puzzle that
we needed to last.

00:44:39.379 --> 00:44:41.670
We may look and say, the
thing that is much smarter now

00:44:41.670 --> 00:44:42.420
is more important.

00:44:42.420 --> 00:44:44.125
We were an important
link in that chain.

00:44:44.125 --> 00:44:46.250
Right now, it's very hard
for anyone to think that.

00:44:46.250 --> 00:44:47.710
We're such humans.

00:44:47.710 --> 00:44:51.486
So that's one thing.

00:44:51.486 --> 00:44:53.110
And then if it's not
conscious, then it

00:44:53.110 --> 00:44:55.527
becomes this really upsetting
thing, that it's smarter

00:44:55.527 --> 00:44:56.610
and then it got rid of us.

00:44:56.610 --> 00:44:59.560
Because we were actually the
highest real consciousness.

00:44:59.560 --> 00:45:01.840
And the thing that's
higher than us is now

00:45:01.840 --> 00:45:05.810
just kind of a mindless thing,
and that becomes a huge shame.

00:45:05.810 --> 00:45:07.330
Still looking at
me like I'm crazy.

00:45:07.330 --> 00:45:12.592
But I would say that probably
we're full of those things

00:45:12.592 --> 00:45:15.050
because we're talking about a
higher level of intelligence.

00:45:15.050 --> 00:45:16.966
That's not something we
even have the capacity

00:45:16.966 --> 00:45:17.817
to understand.

00:45:17.817 --> 00:45:19.650
SPEAKER: On your blog
"Wait But Why," you've

00:45:19.650 --> 00:45:22.990
written about a variety of
high-impact, futurist subjects,

00:45:22.990 --> 00:45:26.700
ranging from AGI to cryonics
to the future of energy.

00:45:26.700 --> 00:45:28.970
In the context of powerful
emergent technology,

00:45:28.970 --> 00:45:31.230
how do you think about
the future of society?

00:45:31.230 --> 00:45:32.910
What areas are you
most optimistic

00:45:32.910 --> 00:45:35.196
or pessimistic about?

00:45:35.196 --> 00:45:37.320
TIM URBAN: We're always in
the battle as a society,

00:45:37.320 --> 00:45:40.999
just like each of us is in an
internal battle of just-- we're

00:45:40.999 --> 00:45:42.165
like a transitional species.

00:45:45.480 --> 00:45:51.120
If you looked at really
tribal, almost subhumans,

00:45:51.120 --> 00:45:53.650
before we were fully
humans, to wherever

00:45:53.650 --> 00:45:57.750
we would be going if AI
hadn't come here and disrupted

00:45:57.750 --> 00:45:59.380
evolution, wherever
we would end up

00:45:59.380 --> 00:46:03.410
as a species of total
prefrontal cortex,

00:46:03.410 --> 00:46:08.360
rational decision maker, just
total rational beings that

00:46:08.360 --> 00:46:13.067
are very kind of-- that
are just totally in charge

00:46:13.067 --> 00:46:15.400
of their thoughts, and come
from a total rational place,

00:46:15.400 --> 00:46:16.233
we're in the middle.

00:46:16.233 --> 00:46:19.180
We're battling with that new
prefrontal cortex we've got,

00:46:19.180 --> 00:46:21.640
and this new rational
cognitive ability

00:46:21.640 --> 00:46:23.660
we have that other
animals don't have.

00:46:23.660 --> 00:46:25.290
And then there's
this complete animal

00:46:25.290 --> 00:46:26.440
that it's stuck inside of.

00:46:29.100 --> 00:46:32.090
And so society as a
whole is somewhere

00:46:32.090 --> 00:46:40.730
between humans as a tribal
and warring and just

00:46:40.730 --> 00:46:44.410
very small-minded
animal species,

00:46:44.410 --> 00:46:51.440
and a species of the
future that has just

00:46:51.440 --> 00:46:52.480
gotten way beyond that.

00:46:52.480 --> 00:46:56.000
And so I think as
technology emerges,

00:46:56.000 --> 00:46:58.970
the thing is, you don't have to
be fully rational as a species

00:46:58.970 --> 00:47:01.282
to develop amazing,
powerful technology.

00:47:01.282 --> 00:47:02.740
And that's kind of
the scary thing.

00:47:02.740 --> 00:47:04.573
Like, I wish we could
get there as a species

00:47:04.573 --> 00:47:08.575
first before we started
building a lot of these things.

00:47:08.575 --> 00:47:11.220
It's dangerous for
an animal species

00:47:11.220 --> 00:47:13.060
to have that much
power, because we

00:47:13.060 --> 00:47:17.830
have very short-sighted, very
weird, selfish motivations.

00:47:17.830 --> 00:47:24.610
So I think that it is worrisome
as we create more technology,

00:47:24.610 --> 00:47:26.780
that the technology's still
controlled by an animal

00:47:26.780 --> 00:47:27.280
species.

00:47:27.280 --> 00:47:28.879
But I would say a
lot of it relates

00:47:28.879 --> 00:47:29.920
to what I just mentioned.

00:47:29.920 --> 00:47:34.590
I think that we need to continue
to be a marketplace of ideas

00:47:34.590 --> 00:47:37.660
as a society
through all of this,

00:47:37.660 --> 00:47:42.490
and resist the urge to
get too much on a team

00:47:42.490 --> 00:47:43.880
and really become hate-filled.

00:47:43.880 --> 00:47:46.609
Which, again, my
thinking these days

00:47:46.609 --> 00:47:48.400
is that it's not just
a thing of the right,

00:47:48.400 --> 00:47:50.160
which a lot of leftists
like to believe,

00:47:50.160 --> 00:47:52.280
that it's really on both sides.

00:47:52.280 --> 00:47:55.100
It's become highly tribal.

00:47:55.100 --> 00:47:57.020
And I think that that
makes it very hard

00:47:57.020 --> 00:48:01.580
for us to move forward in
a really productive way.

00:48:01.580 --> 00:48:04.700
AUDIENCE: Earlier you
made some brief allusions

00:48:04.700 --> 00:48:08.350
to the emerging field
of artificial morality.

00:48:08.350 --> 00:48:10.820
Now some of the
more recent findings

00:48:10.820 --> 00:48:13.480
in the psychology
of morality say

00:48:13.480 --> 00:48:17.380
that the reason we can have
such different morals today

00:48:17.380 --> 00:48:20.590
than our ancestors did 2,000
years ago, despite having

00:48:20.590 --> 00:48:23.690
essentially the same genes,
is that our morals are

00:48:23.690 --> 00:48:27.180
a function of primary wants and
desires and the environments

00:48:27.180 --> 00:48:28.930
in which we find ourselves.

00:48:28.930 --> 00:48:32.460
If that turns out a good way
to do it with AI as well,

00:48:32.460 --> 00:48:37.290
what do you think, as opposed
to companionship, ambition,

00:48:37.290 --> 00:48:42.830
curiosity that drives
humans, what should AI want?

00:48:42.830 --> 00:48:46.110
TIM URBAN: It's interesting,
our environment plus our nature

00:48:46.110 --> 00:48:49.170
is what determines how our
morality will shake out.

00:48:52.270 --> 00:48:53.772
It's weird, because
AI, again, it's

00:48:53.772 --> 00:48:56.105
tempting in that question to
anthropomorphize and think,

00:48:56.105 --> 00:48:58.102
well, what would it
want in its environment?

00:48:58.102 --> 00:48:58.810
AUDIENCE: Oh, no.

00:48:58.810 --> 00:49:02.384
What I'm talking about is, what
should we program it to want?

00:49:02.384 --> 00:49:03.050
TIM URBAN: Yeah.

00:49:03.050 --> 00:49:06.370
Well, I think we would
want to think-- it's like

00:49:06.370 --> 00:49:08.240
the [INAUDIBLE] thing.

00:49:08.240 --> 00:49:11.020
What would we want
were we better?

00:49:11.020 --> 00:49:13.780
And I think you have to get to
philosophers about something

00:49:13.780 --> 00:49:14.280
like this.

00:49:14.280 --> 00:49:17.500
It goes beyond scientists
and engineers in many ways

00:49:17.500 --> 00:49:21.250
to try to figure out what we
would want if we were better,

00:49:21.250 --> 00:49:25.070
but even better to figure
out-- to kind of let something

00:49:25.070 --> 00:49:27.780
in more intelligent than us.

00:49:27.780 --> 00:49:31.190
Figure out what we would
want if we were better.

00:49:31.190 --> 00:49:33.190
Because we're not better,
so we don't want that.

00:49:33.190 --> 00:49:34.100
[LAUGH]

00:49:35.010 --> 00:49:37.679
And to continue to
understand humans

00:49:37.679 --> 00:49:39.220
better than humans
understand humans,

00:49:39.220 --> 00:49:40.680
and to understand
what humans would

00:49:40.680 --> 00:49:43.860
be like in a perfect utopia
of plentiful resources

00:49:43.860 --> 00:49:47.000
when everyone is acting
totally rational.

00:49:47.000 --> 00:49:48.540
What would we want then?

00:49:48.540 --> 00:49:52.640
And basically, let
it figure it out.

00:49:52.640 --> 00:49:54.972
SPEAKER: Sometimes blog posts
can only achieve so much.

00:49:54.972 --> 00:49:56.430
Have you considered
forming a group

00:49:56.430 --> 00:49:57.880
or taking on a job
that would have

00:49:57.880 --> 00:49:59.750
more direct impact
on ethical issues

00:49:59.750 --> 00:50:02.967
like the development
of superintelligent AI?

00:50:02.967 --> 00:50:04.800
TIM URBAN: I've thought
about different ways

00:50:04.800 --> 00:50:08.290
to kind of scale "Wait But Why."

00:50:08.290 --> 00:50:15.540
And the jury's still
out for what I think

00:50:15.540 --> 00:50:17.040
the best way to do that is.

00:50:17.040 --> 00:50:23.040
Right now, I kind of
think that I'd rather

00:50:23.040 --> 00:50:27.970
write a post that inspires a
bunch of 18-year-olds at MIT

00:50:27.970 --> 00:50:30.562
who are smarter than I am
to go into those things,

00:50:30.562 --> 00:50:32.020
and then switch to
a different post

00:50:32.020 --> 00:50:34.200
that-- I think I can have
actually a lot of impact

00:50:34.200 --> 00:50:41.120
that way, by kind of inspiring
or educating or getting excited

00:50:41.120 --> 00:50:46.580
a bunch of people who are more
capable of actually figuring

00:50:46.580 --> 00:50:50.950
out these answers and putting
their brain into it than I am.

00:50:50.950 --> 00:50:54.350
So I think right now, that is
the way that it's scalable,

00:50:54.350 --> 00:50:59.000
is that it can scale by
basically indirectly directing

00:50:59.000 --> 00:51:02.560
more energy towards
these things.

00:51:02.560 --> 00:51:05.627
And I think I always fear that
if I hired people and start

00:51:05.627 --> 00:51:07.960
bringing in more of a team,
I'll spend my time managing,

00:51:07.960 --> 00:51:10.470
and we might lose a little bit
of what right now is making

00:51:10.470 --> 00:51:12.090
an impact on "Wait But Why."

00:51:12.090 --> 00:51:14.850
So I'm still figuring
it out, but I

00:51:14.850 --> 00:51:17.319
hope to be able to scale
this in certain ways

00:51:17.319 --> 00:51:18.110
through the future.

00:51:18.110 --> 00:51:18.812
Yeah.

00:51:18.812 --> 00:51:20.020
SPEAKER: Tim, thanks so much.

00:51:20.020 --> 00:51:22.270
It's been an absolute honor
to be here with you today.

00:51:22.270 --> 00:51:26.000
[APPLAUSE]

