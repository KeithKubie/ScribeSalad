WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.347
[MUSIC PLAYING]

00:00:08.700 --> 00:00:11.130
SARA ROBINSON: Welcome
to machine learning APIs

00:00:11.130 --> 00:00:12.370
by example.

00:00:12.370 --> 00:00:15.120
I'm going to teach you how
you can use pre-trained APIs

00:00:15.120 --> 00:00:19.080
to access pre-trained models
with a single REST API request.

00:00:19.080 --> 00:00:20.340
My name is Sara Robinson.

00:00:20.340 --> 00:00:22.820
You can find me on
Twitter at srobtweets,

00:00:22.820 --> 00:00:26.000
and I'm a developer advocate on
the Google Cloud Platform team.

00:00:26.000 --> 00:00:28.274
I focus on big data
and machine learning.

00:00:28.274 --> 00:00:29.190
I'm based in New York.

00:00:29.190 --> 00:00:30.540
This is my first time in Poland.

00:00:30.540 --> 00:00:33.579
I'm super excited to be here.

00:00:33.579 --> 00:00:34.620
So before I get started--

00:00:34.620 --> 00:00:35.110
[APPLAUSE]

00:00:35.110 --> 00:00:35.609
Thank you.

00:00:39.204 --> 00:00:41.120
I want to talk about
what machine learning is.

00:00:41.120 --> 00:00:43.110
So at a high level,
machine learning

00:00:43.110 --> 00:00:45.540
is teaching computers
to recognize patterns

00:00:45.540 --> 00:00:47.430
in the same way
that our brains do.

00:00:47.430 --> 00:00:49.110
So over time, as
machine learning

00:00:49.110 --> 00:00:51.720
models are given more
examples and experience

00:00:51.720 --> 00:00:52.500
they can improve.

00:00:52.500 --> 00:00:55.570
And with more data they're able
to generate better predictions.

00:00:55.570 --> 00:00:57.570
So it's really easy for
a child to differentiate

00:00:57.570 --> 00:01:00.360
between a picture
of a cat or a dog.

00:01:00.360 --> 00:01:02.640
But it's much, much more
difficult to teach a computer

00:01:02.640 --> 00:01:04.450
to do the same thing.

00:01:04.450 --> 00:01:06.450
So let's pretend for a
minute that we don't have

00:01:06.450 --> 00:01:07.660
any machine learning models.

00:01:07.660 --> 00:01:09.410
We don't have any
deep neural networks.

00:01:09.410 --> 00:01:13.435
And let's try some human
powered image detection.

00:01:13.435 --> 00:01:15.810
So if we take these two images
of an apple and an orange,

00:01:15.810 --> 00:01:17.185
and we were to
write an algorithm

00:01:17.185 --> 00:01:18.955
to differentiate
between these two,

00:01:18.955 --> 00:01:20.580
what are some things
we might look for?

00:01:20.580 --> 00:01:22.520
You can shout it out.

00:01:22.520 --> 00:01:24.484
Color, I heard color,
that's a good one.

00:01:24.484 --> 00:01:26.150
So if we looked for
color, we could say,

00:01:26.150 --> 00:01:28.520
are the majority of the
pixels in the image red?

00:01:28.520 --> 00:01:29.750
If so, it's an apple.

00:01:29.750 --> 00:01:31.300
Otherwise it's an orange.

00:01:31.300 --> 00:01:34.010
And that would work pretty
well in this example.

00:01:34.010 --> 00:01:37.232
But what if we had
grayscale images?

00:01:37.232 --> 00:01:38.690
Then we'd have to
start over again.

00:01:38.690 --> 00:01:41.180
What are some things
you might look for now?

00:01:41.180 --> 00:01:43.250
Stem, texture.

00:01:43.250 --> 00:01:44.250
Those are all good ones.

00:01:44.250 --> 00:01:46.450
We could look at those
different qualities.

00:01:46.450 --> 00:01:49.760
And that would take into
account the grayscale images.

00:01:49.760 --> 00:01:53.750
But then what if we got crazy
and added a third fruit?

00:01:53.750 --> 00:01:56.030
What if we added a
mango to the equation?

00:01:56.030 --> 00:01:57.720
Then we have to
start all over again.

00:01:57.720 --> 00:01:59.220
So you get the idea.

00:01:59.220 --> 00:02:01.230
But these images are
all pretty similar.

00:02:01.230 --> 00:02:02.510
They're all images of fruit.

00:02:02.510 --> 00:02:04.484
They're relatively circular.

00:02:04.484 --> 00:02:05.900
So the image
classification should

00:02:05.900 --> 00:02:08.881
be a lot easier if we have two
images that look nothing alike,

00:02:08.881 --> 00:02:09.380
right?

00:02:09.380 --> 00:02:13.920
So what if we have a
picture of a dog and a mop?

00:02:13.920 --> 00:02:15.870
They have pretty much
nothing in common.

00:02:15.870 --> 00:02:17.460
The mop is not
living or breathing,

00:02:17.460 --> 00:02:20.580
it has no eyes, nose, or ears.

00:02:20.580 --> 00:02:24.810
It's actually kind of
difficult. So here we

00:02:24.810 --> 00:02:28.400
have pictures of sheep dogs
and mops, four of each.

00:02:28.400 --> 00:02:30.510
It's actually kind of hard
even for the human eye

00:02:30.510 --> 00:02:33.390
to be able to distinguish
between the two.

00:02:33.390 --> 00:02:34.890
So the point I'm
trying to make here

00:02:34.890 --> 00:02:36.598
is, what if we have
photos of everything?

00:02:36.598 --> 00:02:38.880
We don't know exactly
what photos our users

00:02:38.880 --> 00:02:40.650
and our application
might upload.

00:02:40.650 --> 00:02:43.170
Might not just be photos
of fruit or animals.

00:02:43.170 --> 00:02:46.455
We're probably going to have
photos of all sorts of things.

00:02:46.455 --> 00:02:48.580
And in addition to photos
we might have other types

00:02:48.580 --> 00:02:49.570
of unstructured data.

00:02:49.570 --> 00:02:53.800
So you might have
video, audio, or text.

00:02:53.800 --> 00:02:55.540
And on Google Cloud
Platform we have

00:02:55.540 --> 00:02:58.940
two ways to help you make sense
of this unstructured data.

00:02:58.940 --> 00:03:00.670
So on the left hand
side, if you want

00:03:00.670 --> 00:03:03.520
to use custom data to build and
train your own machine learning

00:03:03.520 --> 00:03:05.950
models from scratch
we have TensorFlow,

00:03:05.950 --> 00:03:08.567
which is an open source
library to help you do that.

00:03:08.567 --> 00:03:10.900
And if you want to run your
TensorFlow models on managed

00:03:10.900 --> 00:03:13.330
Google infrastructure you can
use Cloud Machine Learning

00:03:13.330 --> 00:03:14.690
Engine.

00:03:14.690 --> 00:03:17.360
What I want to focus on
today is the right hand side.

00:03:17.360 --> 00:03:19.970
And this is what I like to
call friendly machine learning.

00:03:19.970 --> 00:03:23.830
So these are APIs that give you
access to pre-trained models

00:03:23.830 --> 00:03:25.935
with a single REST API request.

00:03:25.935 --> 00:03:27.730
So I'm going to cover
all of these APIs

00:03:27.730 --> 00:03:30.580
in this presentation,
and I'll dive right

00:03:30.580 --> 00:03:32.560
in with the Vision API.

00:03:32.560 --> 00:03:35.440
The Vision API lets you
do complex image detection

00:03:35.440 --> 00:03:38.234
with a simple REST API request.

00:03:38.234 --> 00:03:39.900
And before I get
started, I want to talk

00:03:39.900 --> 00:03:41.580
about some companies
that are using

00:03:41.580 --> 00:03:43.630
the Vision API in production.

00:03:43.630 --> 00:03:46.860
So the first one, the example
on the left is Disney.

00:03:46.860 --> 00:03:49.890
And they use the Vision API
for a game that promoted

00:03:49.890 --> 00:03:51.480
their movie "Pete's Dragon."

00:03:51.480 --> 00:03:54.960
And what the game did was it
was a scavenger hunt type game

00:03:54.960 --> 00:03:56.550
that sent users on
a quest where they

00:03:56.550 --> 00:03:58.466
had to take a picture
of a certain item, maybe

00:03:58.466 --> 00:04:00.600
a couch or a computer.

00:04:00.600 --> 00:04:03.370
And if they took a
picture of that item,

00:04:03.370 --> 00:04:06.180
the app would superimpose an
image of Elliott the dragon

00:04:06.180 --> 00:04:07.110
on that photo.

00:04:07.110 --> 00:04:10.020
So they needed a way to
verify, did the user's image

00:04:10.020 --> 00:04:12.150
match the clue
that it was given?

00:04:12.150 --> 00:04:14.750
And the Vision API was
a great fit for that.

00:04:14.750 --> 00:04:18.329
Realtor.com is a real
estate listing service.

00:04:18.329 --> 00:04:21.149
And they use the Vision API's
optical character recognition

00:04:21.149 --> 00:04:23.790
feature to extract
text from an image.

00:04:23.790 --> 00:04:26.100
So when somebody is
looking for houses

00:04:26.100 --> 00:04:28.320
and they use the
realtor.com app,

00:04:28.320 --> 00:04:30.240
they can take a picture
of the for sale sign

00:04:30.240 --> 00:04:32.790
and then be immediately
directed to the correct listing

00:04:32.790 --> 00:04:33.960
in the app.

00:04:33.960 --> 00:04:35.940
And realtor.com is
using the Vision API

00:04:35.940 --> 00:04:38.220
to extract the text
from that picture

00:04:38.220 --> 00:04:41.830
and direct them to
the relevant listing.

00:04:41.830 --> 00:04:44.410
So these are the core
features of the Vision API.

00:04:44.410 --> 00:04:47.740
Label detection will tell you
what is this a picture of?

00:04:47.740 --> 00:04:51.310
Face detection will tell you are
there faces in the image, where

00:04:51.310 --> 00:04:54.040
the face is located, and what
are the different emotions

00:04:54.040 --> 00:04:54.820
in those faces?

00:04:54.820 --> 00:04:58.940
Are they happy, are they
sad, angry, or surprised?

00:04:58.940 --> 00:05:01.932
OCR was the use case I mentioned
before with realtor.com.

00:05:01.932 --> 00:05:04.330
This can extract
text from an image.

00:05:04.330 --> 00:05:05.780
It can tell you
where the text is

00:05:05.780 --> 00:05:08.430
and what language
that text is in.

00:05:08.430 --> 00:05:10.260
And then explicit
content detection,

00:05:10.260 --> 00:05:12.300
pretty self-explanatory,
will tell you,

00:05:12.300 --> 00:05:14.470
is this image
appropriate or not?

00:05:14.470 --> 00:05:16.740
And this is really useful
for pretty much any site

00:05:16.740 --> 00:05:19.249
that has a lot of user
generated content.

00:05:19.249 --> 00:05:20.790
So you can instead
of having somebody

00:05:20.790 --> 00:05:23.040
manually review the content,
is it appropriate or not?

00:05:23.040 --> 00:05:24.780
You can send it to the API.

00:05:24.780 --> 00:05:28.520
And then you only have to
review a subset of those images.

00:05:28.520 --> 00:05:31.130
Landmark detection will
identify common landmarks

00:05:31.130 --> 00:05:35.010
and logo detection can find
the company logos in an image.

00:05:35.010 --> 00:05:36.864
So to show you some
of the JSON responses

00:05:36.864 --> 00:05:38.280
you get back from
these endpoints,

00:05:38.280 --> 00:05:39.970
I'm going to look
at face detection.

00:05:39.970 --> 00:05:41.970
This is a selfie I took
with two of my teammates

00:05:41.970 --> 00:05:43.760
who are actually here
at this conference.

00:05:43.760 --> 00:05:46.280
We took this last year
in Petra in Jordan.

00:05:46.280 --> 00:05:48.410
And face detection
returns an object

00:05:48.410 --> 00:05:50.600
for each face it
finds the image.

00:05:50.600 --> 00:05:53.030
The JSON response here we
see is just for my face.

00:05:53.030 --> 00:05:55.550
So we can see that headwear
likelihood returned

00:05:55.550 --> 00:05:56.810
very unlikely, which is true.

00:05:56.810 --> 00:05:58.670
I'm not wearing a
hat in the picture,

00:05:58.670 --> 00:06:00.500
although both of
my teammates are.

00:06:00.500 --> 00:06:02.570
And then joy likelihood
returned very likely.

00:06:02.570 --> 00:06:03.160
I was excited.

00:06:03.160 --> 00:06:05.710
I was on vacation
taking selfies.

00:06:05.710 --> 00:06:08.600
And it also returns data on
where different features are

00:06:08.600 --> 00:06:11.540
in your face.

00:06:11.540 --> 00:06:13.290
And then if we look
at landmark detection,

00:06:13.290 --> 00:06:16.150
we can take this picture of
a pretty common landmark.

00:06:16.150 --> 00:06:18.420
This is the Eiffel Tower, right?

00:06:18.420 --> 00:06:20.270
It's actually not
the Eiffel Tower.

00:06:20.270 --> 00:06:23.376
It is the Paris Hotel
and Casino in Las Vegas.

00:06:23.376 --> 00:06:24.750
And I wanted to
see if the Vision

00:06:24.750 --> 00:06:26.490
API could spot the difference.

00:06:26.490 --> 00:06:29.560
And the answer
was that it could.

00:06:29.560 --> 00:06:33.400
So it correctly identified this
as the Paris Hotel and Casino

00:06:33.400 --> 00:06:36.060
and you'll notice that
MID and the response.

00:06:36.060 --> 00:06:39.570
The MID is an ID that maps
to Google's Knowledge Graph.

00:06:39.570 --> 00:06:42.180
So if you want to get more
information on this entity

00:06:42.180 --> 00:06:45.030
you can send the ID to
the Knowledge Graph API

00:06:45.030 --> 00:06:48.030
to get more data on the
Paris Hotel and Casino.

00:06:48.030 --> 00:06:51.234
We get a bounding box of where
this is found in the image,

00:06:51.234 --> 00:06:52.650
and then we also
get the latitude,

00:06:52.650 --> 00:06:57.120
longitude coordinates
for this landmark.

00:06:57.120 --> 00:06:59.790
We've got some additional
features in the Vision API.

00:06:59.790 --> 00:07:02.190
One is crop hints, which
will give you suggested crop

00:07:02.190 --> 00:07:04.230
dimensions for your photo.

00:07:04.230 --> 00:07:07.110
Web annotations will search
the web for additional details

00:07:07.110 --> 00:07:07.740
on your image.

00:07:07.740 --> 00:07:10.860
And I'm going to get into
that more on the next slide.

00:07:10.860 --> 00:07:13.230
And then document
text annotations

00:07:13.230 --> 00:07:16.890
improves the OCR
model for the API.

00:07:16.890 --> 00:07:20.730
So it's much easier
for it to identify text

00:07:20.730 --> 00:07:22.050
in large blocks of text.

00:07:22.050 --> 00:07:25.170
So if you have an image of
a menu or a business card,

00:07:25.170 --> 00:07:29.440
it can break your text down into
paragraphs, words, and symbols.

00:07:29.440 --> 00:07:32.130
So I want to focus a little
more on the web annotations

00:07:32.130 --> 00:07:33.970
endpoint.

00:07:33.970 --> 00:07:36.270
And to do that I have
this image of a car.

00:07:36.270 --> 00:07:38.730
It's not just any car, it's
a car from the "Harry Potter"

00:07:38.730 --> 00:07:39.230
movies.

00:07:39.230 --> 00:07:40.890
I'm a big "Harry Potter" fan.

00:07:40.890 --> 00:07:42.305
And it's on display in a museum.

00:07:42.305 --> 00:07:43.930
So this is a car from
the second movie.

00:07:43.930 --> 00:07:47.430
And I wanted to see what
entities the web annotations

00:07:47.430 --> 00:07:49.900
endpoint was able to find.

00:07:49.900 --> 00:07:51.880
So the first one I
found was that this

00:07:51.880 --> 00:07:53.950
was a Ford Anglia,
which is, indeed,

00:07:53.950 --> 00:07:57.380
the correct model of the car.

00:07:57.380 --> 00:07:59.840
It also returned
art science museum,

00:07:59.840 --> 00:08:02.330
which is a museum in Singapore
where this car is currently

00:08:02.330 --> 00:08:04.350
on display.

00:08:04.350 --> 00:08:06.930
And then finally it was
able to tell me that this

00:08:06.930 --> 00:08:08.280
is from "Harry Potter."

00:08:08.280 --> 00:08:10.320
So the way that
it does this is it

00:08:10.320 --> 00:08:12.750
finds similar images
from across the web.

00:08:12.750 --> 00:08:14.700
And based on the context
of the pages where

00:08:14.700 --> 00:08:16.680
those other images
are found it's

00:08:16.680 --> 00:08:18.075
able to extract these entities.

00:08:20.822 --> 00:08:22.530
We also get, in addition
to the entities,

00:08:22.530 --> 00:08:24.800
the web annotations endpoint
will give us additional details

00:08:24.800 --> 00:08:25.410
on the image.

00:08:25.410 --> 00:08:28.097
So we'll get all of
URLs where this image

00:08:28.097 --> 00:08:29.180
has been found on the web.

00:08:29.180 --> 00:08:30.680
So this is really
useful if you have

00:08:30.680 --> 00:08:32.720
an app where users
are uploading images

00:08:32.720 --> 00:08:35.000
and you want to make sure
that this image is theirs,

00:08:35.000 --> 00:08:36.950
it's unique, it hasn't
been seen before.

00:08:36.950 --> 00:08:40.549
You can use full matching
images to guarantee that.

00:08:40.549 --> 00:08:42.590
Partial matching images,
exactly what it sounds.

00:08:42.590 --> 00:08:46.320
It'll give you URLs of
visually similar images.

00:08:46.320 --> 00:08:49.230
So this is kind of like
a reverse image search.

00:08:49.230 --> 00:08:52.802
So we might see images of
this car in another context.

00:08:52.802 --> 00:08:54.260
And then pages with
matching images

00:08:54.260 --> 00:08:57.980
will just show you the URL where
all the exact image matches

00:08:57.980 --> 00:08:58.510
were found.

00:09:00.595 --> 00:09:02.970
You can all try this in the
browser with your own images.

00:09:02.970 --> 00:09:05.894
If you go to
cloud.google.com/vision,

00:09:05.894 --> 00:09:08.310
you can try it out, see if
it's right for your application

00:09:08.310 --> 00:09:10.840
before you start writing code.

00:09:10.840 --> 00:09:13.370
And in case you were
wondering about our sheepdog

00:09:13.370 --> 00:09:15.780
versus mop example
from before, this

00:09:15.780 --> 00:09:17.970
is what the API returned
for the bottom right

00:09:17.970 --> 00:09:19.230
picture of the sheepdog.

00:09:19.230 --> 00:09:24.300
So it's 99% sure that it's
a dog, and it's even 77%

00:09:24.300 --> 00:09:27.570
sure that it's a komondore,
which is the exact breed of dog

00:09:27.570 --> 00:09:30.180
that this is.

00:09:30.180 --> 00:09:32.640
It was able to identify
this as a broom or a tool.

00:09:32.640 --> 00:09:34.860
And I won't to show you the
response for all of them.

00:09:34.860 --> 00:09:36.610
But the API was able
to identify three out

00:09:36.610 --> 00:09:38.940
of four for each of
these correctly, which

00:09:38.940 --> 00:09:41.910
is pretty good considering
that this model wasn't trained

00:09:41.910 --> 00:09:46.110
specifically to identify
pictures of sheepdogs and mops.

00:09:46.110 --> 00:09:51.010
And it was able to identify
almost all of them correctly.

00:09:51.010 --> 00:09:52.672
So that was the Vision API.

00:09:52.672 --> 00:09:54.130
Next I want to move
into the Speech

00:09:54.130 --> 00:09:57.640
API, which exposes the
functionality of OK Google

00:09:57.640 --> 00:09:58.685
to developers.

00:09:58.685 --> 00:10:01.390
So it lets you implement
speech to text transcription

00:10:01.390 --> 00:10:06.510
in your applications
in over 110 languages.

00:10:06.510 --> 00:10:08.640
One company using
this API is Azar,

00:10:08.640 --> 00:10:10.350
and this is a chat application.

00:10:10.350 --> 00:10:13.590
They've connected over 15
billion matches so far.

00:10:13.590 --> 00:10:15.380
And they're using
the Speech API.

00:10:15.380 --> 00:10:17.640
Any time an audio
snippet is sent

00:10:17.640 --> 00:10:19.140
between people
they'll transcribe it

00:10:19.140 --> 00:10:20.940
with a Speech API.

00:10:20.940 --> 00:10:23.830
And these APIs are really easy
to combine with each other.

00:10:23.830 --> 00:10:26.370
So they're also using
the translation API.

00:10:26.370 --> 00:10:29.295
Any time two of the matches
don't speak the same language,

00:10:29.295 --> 00:10:31.290
they'll transcribe it
with the Speech API

00:10:31.290 --> 00:10:34.740
and then use pipe it
through the translate API

00:10:34.740 --> 00:10:36.480
to translate it into
the host's language.

00:10:39.019 --> 00:10:40.435
We recently launched
a new feature

00:10:40.435 --> 00:10:44.260
on Speech API just a couple of
weeks ago, speech time stamps.

00:10:44.260 --> 00:10:47.140
So if you specify this
in your API request,

00:10:47.140 --> 00:10:49.420
instead of just getting
the transcription,

00:10:49.420 --> 00:10:52.420
you'll also get the start
and end time for each word

00:10:52.420 --> 00:10:55.000
in your transcription, which
makes it really easy to search

00:10:55.000 --> 00:10:56.220
for text within your audio.

00:10:58.820 --> 00:11:01.710
So I'm going to go into a
demo shortly of the API.

00:11:01.710 --> 00:11:03.460
Briefly I'm going to
explain how it works.

00:11:03.460 --> 00:11:06.860
So I wrote a bash script that's
going to call the Speech API.

00:11:06.860 --> 00:11:08.530
First it'll make
a recording using

00:11:08.530 --> 00:11:12.960
SoX, which is just a command
line utility for audio.

00:11:12.960 --> 00:11:15.610
Then we'll create an API
request in a JSON file.

00:11:15.610 --> 00:11:17.430
We'll send it to the
Speech API and then

00:11:17.430 --> 00:11:20.310
we'll receive the response back.

00:11:20.310 --> 00:11:23.930
So let's go to the demo.

00:11:23.930 --> 00:11:25.550
Cool.

00:11:25.550 --> 00:11:27.950
So the way that I'm
going to call my script

00:11:27.950 --> 00:11:31.156
is just with bash request.sh.

00:11:31.156 --> 00:11:33.530
And it's going to ask me to
record five seconds of audio.

00:11:33.530 --> 00:11:35.030
So I'm going to
record something.

00:11:35.030 --> 00:11:37.170
Here I go.

00:11:37.170 --> 00:11:39.920
I'm super excited to be in
Krakow, Poland for a Google

00:11:39.920 --> 00:11:42.920
Developer Days Europe.

00:11:42.920 --> 00:11:44.780
And I'm going to
send out to the API.

00:11:44.780 --> 00:11:49.190
While I do that, this is what
my request file looks like.

00:11:49.190 --> 00:11:50.890
It looks like it did
a pretty good job.

00:11:50.890 --> 00:11:52.820
This is the
transcription that I got,

00:11:52.820 --> 00:11:57.326
and it's 93% confident that
it transcribed that correctly.

00:11:57.326 --> 00:11:58.700
| want to briefly
talk about what

00:11:58.700 --> 00:12:00.860
the request file looks like.

00:12:00.860 --> 00:12:02.990
So here in our
config we just need

00:12:02.990 --> 00:12:04.820
to tell the API the encoding.

00:12:04.820 --> 00:12:07.160
Type of our audio,
we're using FLAC here.

00:12:07.160 --> 00:12:09.350
The sample rate in hertz.

00:12:09.350 --> 00:12:11.390
The language code, which
if you leave this out

00:12:11.390 --> 00:12:12.820
will default to English.

00:12:12.820 --> 00:12:14.570
If you're transcribing
in another language

00:12:14.570 --> 00:12:16.320
you just need to specify
the language code

00:12:16.320 --> 00:12:17.950
that you're using.

00:12:17.950 --> 00:12:20.080
You can optionally
pass it some phrases.

00:12:20.080 --> 00:12:22.090
So if you've got some
proper nouns that

00:12:22.090 --> 00:12:24.940
are specific to your
application that the API may not

00:12:24.940 --> 00:12:29.310
recognize normally you can give
it hints of things to look for.

00:12:29.310 --> 00:12:32.630
We can also give it a parameter
called max alternatives.

00:12:32.630 --> 00:12:36.480
So you notice here we got three
possible transcriptions back.

00:12:36.480 --> 00:12:39.420
So you can use max
alternatives to specify that.

00:12:39.420 --> 00:12:41.810
And then you can pass your
content either as a base 64

00:12:41.810 --> 00:12:45.710
encoded string or as a
URL of a audio file stored

00:12:45.710 --> 00:12:47.480
in Google Cloud Storage.

00:12:47.480 --> 00:12:49.880
So that is how the
Speech API works.

00:12:49.880 --> 00:12:51.530
And I've got one
more Speech API demo

00:12:51.530 --> 00:12:55.460
I want to show you to highlight
the time stamps feature.

00:12:55.460 --> 00:12:57.620
So here I've got a video
of [? Orz ?] talking

00:12:57.620 --> 00:13:01.280
about GCP's Google Cloud
Platforms pricing philosophy.

00:13:01.280 --> 00:13:03.890
And below the video I
have the transcription

00:13:03.890 --> 00:13:06.290
that the Speech API gave back.

00:13:06.290 --> 00:13:08.330
Now what I can do
with time stamps

00:13:08.330 --> 00:13:11.780
is I built a demo that we can
search through the transcript

00:13:11.780 --> 00:13:12.330
essentially.

00:13:12.330 --> 00:13:13.340
So if I click on this--

00:13:13.340 --> 00:13:13.730
[VIDEO PLAYBACK]

00:13:13.730 --> 00:13:14.870
- Google Cloud Platform del--

00:13:14.870 --> 00:13:15.170
[END PLAYBACK]

00:13:15.170 --> 00:13:17.180
SARA ROBINSON: We can skip to
the exact point in our audio

00:13:17.180 --> 00:13:18.170
where this occurs.

00:13:18.170 --> 00:13:19.154
So if I go to another--

00:13:19.154 --> 00:13:19.820
[VIDEO PLAYBACK]

00:13:19.820 --> 00:13:20.585
- --70%--

00:13:20.585 --> 00:13:20.840
[END PLAYBACK]

00:13:20.840 --> 00:13:22.530
SARA ROBINSON: --line,
you get the idea.

00:13:22.530 --> 00:13:24.950
So the time stamps feature
I'm really excited about,

00:13:24.950 --> 00:13:26.870
because it makes
it easy to search

00:13:26.870 --> 00:13:30.390
a large library of video content
just using the transcription.

00:13:30.390 --> 00:13:33.080
So the way that I did this
is I extracted the audio

00:13:33.080 --> 00:13:36.660
from the video and then sent
that audio to the Speech API.

00:13:36.660 --> 00:13:38.210
And I have more
than one video here.

00:13:38.210 --> 00:13:42.410
So I've got a bunch of
videos about Cloud, Firebase.

00:13:42.410 --> 00:13:45.020
So let's say I want to
search for all my videos

00:13:45.020 --> 00:13:47.150
that mention Firebase.

00:13:47.150 --> 00:13:49.160
I can do that here
and I can actually

00:13:49.160 --> 00:13:52.490
jump to the exact moment in
the video where I see Firebase.

00:13:52.490 --> 00:13:54.440
This video is a
screencast tutorial

00:13:54.440 --> 00:13:56.390
about Firebase hosting.

00:13:56.390 --> 00:13:57.705
And we can see all the points--

00:13:57.705 --> 00:13:57.910
[VIDEO PLAYBACK]

00:13:57.910 --> 00:13:58.850
- Firebase hosting--

00:13:58.850 --> 00:13:59.120
[END PLAYBACK]

00:13:59.120 --> 00:14:01.578
SARA ROBINSON: That mention
Firebase, which is pretty cool.

00:14:01.578 --> 00:14:04.460
In this video we've only
got one mention of Firebase.

00:14:04.460 --> 00:14:06.530
So if our job was to
look through the video

00:14:06.530 --> 00:14:08.300
and find all the
mentions of Firebase,

00:14:08.300 --> 00:14:10.340
chances are we might miss it.

00:14:10.340 --> 00:14:12.350
But with the Speech
API's time stamp feature,

00:14:12.350 --> 00:14:14.360
we're able to jump right
to that point in the video.

00:14:14.360 --> 00:14:15.026
[VIDEO PLAYBACK]

00:14:15.026 --> 00:14:16.457
- Firebase real time da--

00:14:16.457 --> 00:14:17.040
[END PLAYBACK]

00:14:17.040 --> 00:14:19.290
SARA ROBINSON: So that's an
example of the time stamps

00:14:19.290 --> 00:14:20.900
feature of the Speech API.

00:14:20.900 --> 00:14:23.850
So with the Speech API
you can transcribe audio.

00:14:23.850 --> 00:14:25.730
Once you've got that
text transcription,

00:14:25.730 --> 00:14:27.320
the next thing you
might want to do

00:14:27.320 --> 00:14:28.970
is extract more data from it.

00:14:28.970 --> 00:14:31.590
And that's where the Natural
Language API comes into play.

00:14:31.590 --> 00:14:33.560
So the Natural
Language API, you can

00:14:33.560 --> 00:14:39.470
extract entities, sentiment,
and syntax from your text.

00:14:39.470 --> 00:14:41.570
One company that's
using it is Wootric,

00:14:41.570 --> 00:14:43.970
and Wootric is a customer
feedback platform.

00:14:43.970 --> 00:14:46.160
So what they enable
their customers to do

00:14:46.160 --> 00:14:48.540
is if you look at that
box on the top right,

00:14:48.540 --> 00:14:50.720
their customers place
these feedback forms

00:14:50.720 --> 00:14:53.120
in various places throughout
their application.

00:14:53.120 --> 00:14:55.040
And they ask them,
how is your experience

00:14:55.040 --> 00:14:57.537
on this specific
page from zero to 10?

00:14:57.537 --> 00:14:59.120
So they rate their
experience and then

00:14:59.120 --> 00:15:01.249
they have this open
ended feedback.

00:15:01.249 --> 00:15:03.290
And Wootric's job is to
help their customers make

00:15:03.290 --> 00:15:05.150
sense of this feedback.

00:15:05.150 --> 00:15:06.830
So it's pretty easy
for them to make

00:15:06.830 --> 00:15:08.734
sense of the numbered feedback.

00:15:08.734 --> 00:15:10.400
But what's much more
difficult is making

00:15:10.400 --> 00:15:12.322
sense of that open ended text.

00:15:12.322 --> 00:15:14.780
And they're using all three
methods of the Natural Language

00:15:14.780 --> 00:15:16.100
API to do this.

00:15:16.100 --> 00:15:19.130
So they're using entity
and syntax annotation

00:15:19.130 --> 00:15:21.950
to extract the subjects
and see how people are

00:15:21.950 --> 00:15:23.904
talking about different topics.

00:15:23.904 --> 00:15:25.820
And then they're using
sentiment analysis just

00:15:25.820 --> 00:15:28.900
to gauge, did the numbered
square the person line up--

00:15:28.900 --> 00:15:31.929
line up with the open ended
feedback that they gave?

00:15:31.929 --> 00:15:33.470
And so this enables
them to do things

00:15:33.470 --> 00:15:36.290
like say they have a high
priority customer that's

00:15:36.290 --> 00:15:37.779
angry about usability.

00:15:37.779 --> 00:15:39.320
So they can route
that request really

00:15:39.320 --> 00:15:40.910
fast to the right person.

00:15:40.910 --> 00:15:42.290
And with the
Natural Language API

00:15:42.290 --> 00:15:43.664
they're able to
route and respond

00:15:43.664 --> 00:15:45.830
to customer feedback
in near real time

00:15:45.830 --> 00:15:48.620
instead of having people
manually go through each open

00:15:48.620 --> 00:15:50.922
ended response.

00:15:50.922 --> 00:15:53.130
So now I want to talk about
each of the three methods

00:15:53.130 --> 00:15:55.090
of the Natural Language API.

00:15:55.090 --> 00:15:57.240
The first one is
extracting entities.

00:15:57.240 --> 00:15:59.700
Obviously I have a "Harry
Potter" example again.

00:15:59.700 --> 00:16:01.800
So I just took a
sentence about JK Rowling

00:16:01.800 --> 00:16:03.480
from her Wikipedia
page, and I wanted

00:16:03.480 --> 00:16:07.950
to see what entities the
API found in this example.

00:16:07.950 --> 00:16:10.030
We got these five entities back.

00:16:10.030 --> 00:16:12.530
And then I wanted to look
closely at the JSON response

00:16:12.530 --> 00:16:15.070
it gave for each one.

00:16:15.070 --> 00:16:17.590
So notice that these
are three different ways

00:16:17.590 --> 00:16:18.814
of referring to JK Rowling.

00:16:18.814 --> 00:16:20.230
If you're wondering,
the third one

00:16:20.230 --> 00:16:22.780
is a pen name she used for
a different book series.

00:16:22.780 --> 00:16:24.790
That's a topic for another talk.

00:16:24.790 --> 00:16:26.980
But all of these map
to the same entity.

00:16:26.980 --> 00:16:29.170
So they all point to
her Wikipedia page.

00:16:29.170 --> 00:16:31.510
So it's able to normalize
these different mentions

00:16:31.510 --> 00:16:33.290
of the same thing.

00:16:33.290 --> 00:16:36.560
So in our JSON response
we get back her name,

00:16:36.560 --> 00:16:38.322
the type of entity--
she's a person--

00:16:38.322 --> 00:16:40.030
and then we get some
additional metadata.

00:16:40.030 --> 00:16:44.860
We get that MID, which maps
to her Knowledge Graph entity,

00:16:44.860 --> 00:16:49.010
and then we get the
Wikipedia URL for her.

00:16:49.010 --> 00:16:51.532
Similarly for British, it
maps to the Wikipedia page

00:16:51.532 --> 00:16:52.490
for the United Kingdom.

00:16:52.490 --> 00:16:55.390
So if instead this had said
United Kingdom born novelist,

00:16:55.390 --> 00:16:59.400
it would have mapped
to the same entity ID.

00:16:59.400 --> 00:17:01.999
And we got a similar
one for "Harry Potter."

00:17:01.999 --> 00:17:03.540
And if you have
entities in your text

00:17:03.540 --> 00:17:06.329
that don't have a Wikipedia
page associated with them,

00:17:06.329 --> 00:17:08.790
the API will also be
able to identify those.

00:17:08.790 --> 00:17:11.099
It just won't return
anything for metadata.

00:17:11.099 --> 00:17:13.330
So if this instead
had said my name

00:17:13.330 --> 00:17:15.329
the metadata would be
empty because I don't have

00:17:15.329 --> 00:17:19.520
a Wikipedia page yet, at least.

00:17:19.520 --> 00:17:23.329
The second thing we can
do is analyze sentiment.

00:17:23.329 --> 00:17:26.210
This might be something I might
find on a restaurant review.

00:17:26.210 --> 00:17:29.367
The food was terrible, I
will not be going back.

00:17:29.367 --> 00:17:30.950
If I was a manager
of this restaurant,

00:17:30.950 --> 00:17:32.780
this is probably
something I'd want to flag

00:17:32.780 --> 00:17:33.680
and maybe respond to.

00:17:33.680 --> 00:17:36.096
But chances are I don't want
to go through all the reviews

00:17:36.096 --> 00:17:36.860
to do that.

00:17:36.860 --> 00:17:39.380
So I can use the sentiment
analysis endpoint

00:17:39.380 --> 00:17:41.570
to get some data on the
sentiment of this text.

00:17:41.570 --> 00:17:43.290
And it returns two values.

00:17:43.290 --> 00:17:46.190
The first is score,
and score will tell us

00:17:46.190 --> 00:17:49.790
on a scale of negative 1 to
1 how negative or positive

00:17:49.790 --> 00:17:51.590
is this text?

00:17:51.590 --> 00:17:53.780
And then magnitude
tells us regardless

00:17:53.780 --> 00:17:55.760
of being positive or
negative, how strong

00:17:55.760 --> 00:17:57.170
is the sentiment in this text?

00:17:57.170 --> 00:17:59.389
And this is a range
from 0 to infinity

00:17:59.389 --> 00:18:00.930
normalized to the
length of the text.

00:18:00.930 --> 00:18:02.580
So since this is
pretty short we've

00:18:02.580 --> 00:18:05.920
got a small number of 0.9.

00:18:05.920 --> 00:18:10.040
And then the last method of
the API is analyzing syntax.

00:18:10.040 --> 00:18:12.460
So this gives us more
linguistic details

00:18:12.460 --> 00:18:14.900
on the contents of our text.

00:18:14.900 --> 00:18:17.590
So here, if we the sentence,
the Natural Language API

00:18:17.590 --> 00:18:20.350
helps us understand text,
the first thing we get

00:18:20.350 --> 00:18:22.030
is a dependency graph.

00:18:22.030 --> 00:18:24.730
So this is called a
dependency parse tree,

00:18:24.730 --> 00:18:26.170
which tells us
essentially how do

00:18:26.170 --> 00:18:29.360
the different words in the
sentence relate to each other?

00:18:29.360 --> 00:18:32.050
This is one piece
of data we get back.

00:18:32.050 --> 00:18:34.210
We also get the
parse label which

00:18:34.210 --> 00:18:37.040
tells us what's the role
of each word in a sentence?

00:18:37.040 --> 00:18:39.760
So in this example,
helps is the root verb,

00:18:39.760 --> 00:18:42.070
API is the nominal subject.

00:18:42.070 --> 00:18:46.260
So it tell us the
role of each word.

00:18:46.260 --> 00:18:47.560
We also get part of speech.

00:18:47.560 --> 00:18:52.034
So tells us is it an
adjective, a verb, a pronoun?

00:18:52.034 --> 00:18:53.450
And then we get
[? lemma, ?] which

00:18:53.450 --> 00:18:55.615
is the canonical
form of the verb.

00:18:55.615 --> 00:18:56.990
So in this case,
we just have one

00:18:56.990 --> 00:18:59.612
for helps the
canonical form is help

00:18:59.612 --> 00:19:01.070
and this is really
useful if you're

00:19:01.070 --> 00:19:03.530
counting how many times
a certain word appears

00:19:03.530 --> 00:19:05.067
in the context of your app.

00:19:05.067 --> 00:19:06.650
You probably don't
want to count helps

00:19:06.650 --> 00:19:08.780
and help as two
separate mentions.

00:19:08.780 --> 00:19:10.170
They're related
to the same word.

00:19:10.170 --> 00:19:14.860
So you would use the canonical
form here to do that.

00:19:14.860 --> 00:19:17.970
And finally, we get additional
morphology details on our text.

00:19:17.970 --> 00:19:20.511
This is going to be a little
bit different depending on which

00:19:20.511 --> 00:19:22.440
language you send to the API.

00:19:22.440 --> 00:19:24.450
So this is just a
visualization of all the JSON

00:19:24.450 --> 00:19:25.585
on you get back.

00:19:25.585 --> 00:19:27.210
You can actually
create your own if you

00:19:27.210 --> 00:19:29.760
go to the natural
language product page.

00:19:29.760 --> 00:19:31.690
There is a little try
it demo in the browser.

00:19:31.690 --> 00:19:32.940
So you can enter your own text--

00:19:32.940 --> 00:19:34.190
I'll share a link at the end--

00:19:34.190 --> 00:19:36.690
you can enter your own text and
generate these visualizations

00:19:36.690 --> 00:19:37.190
as well.

00:19:39.574 --> 00:19:41.740
So I'm going to show you a
demo specifically focused

00:19:41.740 --> 00:19:44.380
on the syntax annotation
endpoint of how

00:19:44.380 --> 00:19:47.030
you could actually use
that in an application.

00:19:47.030 --> 00:19:49.270
So I've been running some
natural language processing

00:19:49.270 --> 00:19:52.390
on tweets using the
hashtag GDDEurope.

00:19:52.390 --> 00:19:55.330
And what I did is I wrote a
little node script that calls

00:19:55.330 --> 00:19:57.340
the Twitter streaming API.

00:19:57.340 --> 00:20:00.040
And I'm just looking for
tweets with that hashtag.

00:20:00.040 --> 00:20:02.560
So the streaming API won't
give me all the tweets,

00:20:02.560 --> 00:20:06.340
but it'll give me a subset of
tweets with GDDEurope in it.

00:20:06.340 --> 00:20:08.925
So I'm taking those tweets,
and I take the text of them,

00:20:08.925 --> 00:20:11.050
and I send that through to
the Natural Language API

00:20:11.050 --> 00:20:12.500
for processing.

00:20:12.500 --> 00:20:14.590
And then I store
that in BigQuery.

00:20:14.590 --> 00:20:16.930
And BigQuery is Google
Cloud Platform's big data

00:20:16.930 --> 00:20:18.260
as a service tool.

00:20:18.260 --> 00:20:21.460
So it lets you analyze really
large data sets super fast.

00:20:24.060 --> 00:20:25.250
So let's switch to a demo.

00:20:28.700 --> 00:20:29.890
There we go.

00:20:29.890 --> 00:20:32.210
OK, so this is the
BigQuery web UI.

00:20:32.210 --> 00:20:35.090
It lets me visualize my data
stored in my BigQuery table

00:20:35.090 --> 00:20:36.750
directly in the browser.

00:20:36.750 --> 00:20:40.280
And here I have the schema, let
me make this a little bigger.

00:20:40.280 --> 00:20:42.300
I've got the schema
for my table.

00:20:42.300 --> 00:20:45.380
So I'm storing the ID of each
tweet, the text of each tweet,

00:20:45.380 --> 00:20:48.650
when it was created, how
many followers the user has,

00:20:48.650 --> 00:20:51.350
the hashtag as a JSON string,
which I get from the Twitter

00:20:51.350 --> 00:20:55.130
API, the tokens, which is a
giant JSON string returned

00:20:55.130 --> 00:20:57.710
from the Natural Language
API, and then the score

00:20:57.710 --> 00:20:59.430
and magnitude.

00:20:59.430 --> 00:21:00.860
So we can preview this.

00:21:00.860 --> 00:21:03.770
Looks like we've collected
about 840 tweets so far, which

00:21:03.770 --> 00:21:05.720
is really, really
small for BigQuery.

00:21:05.720 --> 00:21:07.400
BigQuery's meant
for lots of data.

00:21:07.400 --> 00:21:09.390
But this is just a demo.

00:21:09.390 --> 00:21:11.480
So we've got the
text of the tweet,

00:21:11.480 --> 00:21:14.200
and we've got the hashtags.

00:21:14.200 --> 00:21:16.870
And we have this giant JSON
string of the Natural Language

00:21:16.870 --> 00:21:18.580
API response.

00:21:18.580 --> 00:21:21.100
And we write all our
BigQuery queries in SQL.

00:21:21.100 --> 00:21:22.630
So you may be
wondering, how am I

00:21:22.630 --> 00:21:25.660
going to parse this giant
JSON string with SQL?

00:21:25.660 --> 00:21:28.210
And the answer is that
BigQuery has a feature called

00:21:28.210 --> 00:21:30.640
user defined functions,
which lets you write

00:21:30.640 --> 00:21:34.030
custom JavaScript functions
to parse different columns

00:21:34.030 --> 00:21:35.510
in your table.

00:21:35.510 --> 00:21:37.660
So here I want to find
what adjectives people are

00:21:37.660 --> 00:21:40.010
using when they talk
about GDD Europe.

00:21:40.010 --> 00:21:43.540
And I've written a function
just to look for each token.

00:21:43.540 --> 00:21:46.990
And if part of speech
tag is adjective,

00:21:46.990 --> 00:21:48.790
I'm going to do a running count.

00:21:48.790 --> 00:21:51.030
So I'm running this query.

00:21:51.030 --> 00:21:54.079
And let's see the results.

00:21:54.079 --> 00:21:55.870
So it looks like pretty
positive adjectives

00:21:55.870 --> 00:21:58.540
used about GDD
Europe, which is good.

00:21:58.540 --> 00:22:02.380
So these are the most
commonly used right now.

00:22:02.380 --> 00:22:04.840
And again, what's really
cool about this is this

00:22:04.840 --> 00:22:07.450
ran this on all
840 of our tweets

00:22:07.450 --> 00:22:10.510
and it ran really
fast, 3.1 seconds.

00:22:10.510 --> 00:22:12.550
It ran this custom function
on all of the tweets

00:22:12.550 --> 00:22:14.950
on our table which
is pretty cool.

00:22:14.950 --> 00:22:17.230
I'll do one more query for you.

00:22:17.230 --> 00:22:18.910
I'm a big fan of
emojis, so I wanted

00:22:18.910 --> 00:22:22.420
to see which emojis are people
using most frequently when

00:22:22.420 --> 00:22:24.080
they talk about GDD Europe.

00:22:24.080 --> 00:22:26.080
So I'm looking at
the token again.

00:22:26.080 --> 00:22:28.420
It looks like the laughing
crying is a popular one.

00:22:28.420 --> 00:22:30.280
Clapping, Firebase.

00:22:30.280 --> 00:22:31.700
Overall not too many, though.

00:22:31.700 --> 00:22:35.830
So maybe people should use
more emojis in their tweets.

00:22:35.830 --> 00:22:38.070
But the goal of this
demo is just to show you

00:22:38.070 --> 00:22:39.820
what you can do with
the syntax annotation

00:22:39.820 --> 00:22:41.780
endpoint of the
Natural Language API

00:22:41.780 --> 00:22:43.570
to do things like
see how sentiment

00:22:43.570 --> 00:22:46.469
is around a particular topic.

00:22:46.469 --> 00:22:47.260
So that's the demo.

00:22:47.260 --> 00:22:49.540
We can go back to
the slides now.

00:22:52.900 --> 00:22:54.970
So another thing you might
want to do with text,

00:22:54.970 --> 00:22:57.400
in addition to analyze
it or transcribe it,

00:22:57.400 --> 00:23:00.010
is translate it
into many languages

00:23:00.010 --> 00:23:02.335
to support all the users
in your application.

00:23:02.335 --> 00:23:05.110
The Translation API
lets you translate text

00:23:05.110 --> 00:23:07.387
in over 100 languages.

00:23:07.387 --> 00:23:09.470
Before we get into it, I
want to look a little bit

00:23:09.470 --> 00:23:10.639
at Google Translate.

00:23:10.639 --> 00:23:12.180
So I'm a big fan of
Google Translate.

00:23:12.180 --> 00:23:14.740
Anyone else here use it?

00:23:14.740 --> 00:23:16.554
Looks like most people.

00:23:16.554 --> 00:23:18.970
So I use, especially when I
travel somewhere where English

00:23:18.970 --> 00:23:20.011
isn't the first language.

00:23:20.011 --> 00:23:22.010
I was on a trip
to Japan last year

00:23:22.010 --> 00:23:23.830
and I really wanted
to order octopus.

00:23:23.830 --> 00:23:26.611
So I translated it, found
out that the word was tako.

00:23:26.611 --> 00:23:27.860
Which I was a little confused.

00:23:27.860 --> 00:23:30.020
But I was like, I'm
going to go with this.

00:23:30.020 --> 00:23:33.130
So I showed it to the
waiter at the restaurant

00:23:33.130 --> 00:23:35.170
and I did get octopus back.

00:23:35.170 --> 00:23:37.150
So Google Translate success.

00:23:37.150 --> 00:23:39.400
But you probably want to do
more with Google Translate

00:23:39.400 --> 00:23:42.010
than just translate
the word for octopus.

00:23:42.010 --> 00:23:43.510
And the translation
API essentially

00:23:43.510 --> 00:23:48.860
exposes all the functionality of
Google Translate to developers.

00:23:48.860 --> 00:23:51.980
One company that's using the
translation API as Airbnb.

00:23:51.980 --> 00:23:54.320
And they actually have
60% of the bookings

00:23:54.320 --> 00:23:58.100
are connecting people that use
the app in different languages.

00:23:58.100 --> 00:24:00.020
And they're using the
translation API not only

00:24:00.020 --> 00:24:04.100
to translate listings, but
also reviews and conversations.

00:24:04.100 --> 00:24:06.770
And they found that
using the translation API

00:24:06.770 --> 00:24:09.680
significantly improves a
guest's likelihood to book.

00:24:12.450 --> 00:24:15.180
Here's a Python example of how
you would call the translation

00:24:15.180 --> 00:24:19.620
API using the Google
Cloud module for Python.

00:24:19.620 --> 00:24:21.450
And notice here
that we just pass

00:24:21.450 --> 00:24:24.230
in the string of the text
we'd like to translate

00:24:24.230 --> 00:24:25.935
and the target text language.

00:24:25.935 --> 00:24:27.810
We don't need to tell
it the source language,

00:24:27.810 --> 00:24:30.100
the API can detect that for us.

00:24:30.100 --> 00:24:32.190
And if you also just want
to use the API to detect

00:24:32.190 --> 00:24:35.160
the language of text, so if
you've got users inputting text

00:24:35.160 --> 00:24:36.960
in a variety of
languages, you can just

00:24:36.960 --> 00:24:39.240
use the detect language
endpoint to do that.

00:24:39.240 --> 00:24:44.180
So we get our translation
result, pretty straightforward.

00:24:44.180 --> 00:24:45.920
One improvement
we made to the API

00:24:45.920 --> 00:24:47.810
a few months ago
was an improvement

00:24:47.810 --> 00:24:51.090
to the model called neural
machine translation.

00:24:51.090 --> 00:24:53.870
The way that the model worked
before with first generation

00:24:53.870 --> 00:24:57.260
translation is that it would
take each word in your sentence

00:24:57.260 --> 00:24:59.100
and translate it word for word.

00:24:59.100 --> 00:25:01.797
So this is similar to what you
might do if you were traveling,

00:25:01.797 --> 00:25:03.380
you had a dictionary
with you, and you

00:25:03.380 --> 00:25:05.030
had a sentence you
wanted translate,

00:25:05.030 --> 00:25:07.125
and you did a
lookup of each word.

00:25:07.125 --> 00:25:08.750
Now, the accuracy
would be pretty good.

00:25:08.750 --> 00:25:11.180
You'd probably get
your idea across,

00:25:11.180 --> 00:25:12.680
but it wouldn't be perfect.

00:25:12.680 --> 00:25:14.600
And what neural machine
translation does

00:25:14.600 --> 00:25:17.240
is it takes the context
of each of the surrounding

00:25:17.240 --> 00:25:19.100
words around each
word in a sentence

00:25:19.100 --> 00:25:22.582
and it's able to produce much
higher quality translations.

00:25:22.582 --> 00:25:24.540
There's a great New York
Times article about it

00:25:24.540 --> 00:25:25.623
if you want to learn more.

00:25:25.623 --> 00:25:31.440
It's available at this link,
bit.ly/nyt-ai-awakening.

00:25:31.440 --> 00:25:34.080
And just to highlight the
differences between the two

00:25:34.080 --> 00:25:37.340
models, I've got a "Harry
Potter" example again.

00:25:37.340 --> 00:25:38.590
What I have here on the left--

00:25:38.590 --> 00:25:39.825
I know there's a lot
of text on this slide,

00:25:39.825 --> 00:25:41.160
but I'll explain it--

00:25:41.160 --> 00:25:43.620
is the original
Spanish translation.

00:25:43.620 --> 00:25:46.860
So "Harry Potter" was translated
into over 70 languages,

00:25:46.860 --> 00:25:49.056
and they didn't use the
translation API to do that.

00:25:49.056 --> 00:25:50.930
That probably wouldn't
have worked very well.

00:25:50.930 --> 00:25:53.610
They had somebody translating
the text for each one.

00:25:53.610 --> 00:25:56.850
So this is a paragraph from the
original Spanish translation.

00:25:56.850 --> 00:25:59.339
And I've translated
it into English

00:25:59.339 --> 00:26:01.380
using first generation
translation in the middle,

00:26:01.380 --> 00:26:03.600
and then the improved
neural machine

00:26:03.600 --> 00:26:05.306
translation on the right.

00:26:05.306 --> 00:26:07.180
And I've just bolded
some of the differences.

00:26:07.180 --> 00:26:09.013
So you can see these
are small improvements,

00:26:09.013 --> 00:26:12.210
but they make a big difference
in terms of the overall quality

00:26:12.210 --> 00:26:14.260
of the translation.

00:26:14.260 --> 00:26:17.910
So that is the translation API.

00:26:17.910 --> 00:26:20.190
And the last API I want
to share with all of you

00:26:20.190 --> 00:26:21.642
is the Video Intelligence API.

00:26:21.642 --> 00:26:23.100
This is our newest
machine learning

00:26:23.100 --> 00:26:25.734
API on Google Cloud Platform.

00:26:25.734 --> 00:26:27.150
And what it lets
you do is it lets

00:26:27.150 --> 00:26:30.660
you understand your video's
entities at shot, frame,

00:26:30.660 --> 00:26:33.490
or video level.

00:26:33.490 --> 00:26:35.850
One company that's
using it is Cantemo.

00:26:35.850 --> 00:26:38.180
They're a media asset
management company.

00:26:38.180 --> 00:26:40.877
And so all of their users have
tons and tons of video content

00:26:40.877 --> 00:26:42.460
that they're uploading
into the system

00:26:42.460 --> 00:26:45.310
and they're using the Video
API to help their users better

00:26:45.310 --> 00:26:48.055
be able to search large
libraries of video.

00:26:51.030 --> 00:26:53.460
The best way to see
how the Video API works

00:26:53.460 --> 00:26:54.990
is through a demo.

00:26:54.990 --> 00:26:58.980
And so the demo I'm going to
show you here, what I've got

00:26:58.980 --> 00:27:02.850
is the video of a Super Bowl
commercial for Google Home.

00:27:02.850 --> 00:27:06.070
And I'm going to play the video.

00:27:06.070 --> 00:27:09.430
And you'll notice that it
starts with a mountain pass,

00:27:09.430 --> 00:27:12.009
goes through a
house, a city street.

00:27:12.009 --> 00:27:13.550
Lots of scenes
changing in the video.

00:27:13.550 --> 00:27:14.800
Now we see a dog.

00:27:14.800 --> 00:27:17.120
I won't play the whole
thing, but you get the idea.

00:27:17.120 --> 00:27:19.370
There's lots of scene
changes in the video.

00:27:19.370 --> 00:27:22.211
And if you were to manually
transcribe what was happening,

00:27:22.211 --> 00:27:23.710
you'd have to watch
the whole thing,

00:27:23.710 --> 00:27:25.840
write down what was
happening in each scene,

00:27:25.840 --> 00:27:29.120
and then maybe store those
tags in a database somewhere.

00:27:29.120 --> 00:27:31.030
And with the Video
API, it essentially

00:27:31.030 --> 00:27:32.540
does all that for you.

00:27:32.540 --> 00:27:34.700
So the JSON response
tells you at a high level

00:27:34.700 --> 00:27:37.580
what's happening in this
video, what is the video about,

00:27:37.580 --> 00:27:39.730
and then at a granular
level, what's happening

00:27:39.730 --> 00:27:41.960
in each scene of the video?

00:27:41.960 --> 00:27:44.190
So if we look below this,
we can see all the labels

00:27:44.190 --> 00:27:46.450
that the Video API returned.

00:27:46.450 --> 00:27:48.730
So for example, it knew
there was a dog in the video,

00:27:48.730 --> 00:27:51.700
and it knew exactly
when that dog appeared.

00:27:51.700 --> 00:27:55.300
It knew that it ended
with birthday cake.

00:27:55.300 --> 00:27:57.520
And if we scroll
down, we can see

00:27:57.520 --> 00:28:00.697
it identified even the type
of dog that we had there.

00:28:00.697 --> 00:28:02.530
And it's also able to
identify that mountain

00:28:02.530 --> 00:28:05.704
pass from the beginning scene.

00:28:05.704 --> 00:28:06.620
So that's pretty cool.

00:28:06.620 --> 00:28:08.110
And this is just one video.

00:28:08.110 --> 00:28:10.750
So chances are if you want to
analyze videos you probably

00:28:10.750 --> 00:28:14.230
got a lot of videos sitting
around in storage buckets.

00:28:14.230 --> 00:28:17.200
And the Video API, with the
JSON response you get back,

00:28:17.200 --> 00:28:18.580
it makes it really
easy to search

00:28:18.580 --> 00:28:21.210
a large library of videos.

00:28:21.210 --> 00:28:23.310
So if you look here
I've got lots of videos.

00:28:23.310 --> 00:28:26.090
And let's say I'm a
sports media company.

00:28:26.090 --> 00:28:27.710
And I've got lots
of sports footage,

00:28:27.710 --> 00:28:30.440
and I want to create a highlight
reel just of one specific type

00:28:30.440 --> 00:28:31.132
of content.

00:28:31.132 --> 00:28:32.840
So let's say I want
to create a highlight

00:28:32.840 --> 00:28:36.500
reel of all my baseball
clips in my videos.

00:28:36.500 --> 00:28:38.120
So without the
Video API, I'd have

00:28:38.120 --> 00:28:40.820
to have somebody manually
watching all the videos to find

00:28:40.820 --> 00:28:42.010
baseball.

00:28:42.010 --> 00:28:44.670
With the Video Intelligence
API this is pretty easy.

00:28:44.670 --> 00:28:47.000
So if I search
for baseball here,

00:28:47.000 --> 00:28:50.870
we see that I get back not only
which videos have baseball,

00:28:50.870 --> 00:28:53.990
but we can see all the scenes
that have baseball in them.

00:28:53.990 --> 00:28:56.707
So this video is almost
entirely about baseball.

00:28:56.707 --> 00:28:58.040
I really like this example here.

00:28:58.040 --> 00:29:00.770
Because we've got a
long video and there's

00:29:00.770 --> 00:29:03.810
only one tiny baseball
clip in the video.

00:29:03.810 --> 00:29:05.720
So here we have the
year end search video

00:29:05.720 --> 00:29:08.300
which Google publishes at the
end of every year highlighting

00:29:08.300 --> 00:29:09.590
top searches.

00:29:09.590 --> 00:29:13.402
And if we skip here we can
see that baseball clip.

00:29:13.402 --> 00:29:15.860
It's actually when the Cubs
won the World Series last year.

00:29:15.860 --> 00:29:18.930
From Chicago, so I was
pretty excited about that.

00:29:18.930 --> 00:29:21.360
But there's only one small
baseball clip in this video.

00:29:21.360 --> 00:29:24.979
So if somebody's job was
to scan it for that clip,

00:29:24.979 --> 00:29:26.270
chances are they might miss it.

00:29:26.270 --> 00:29:28.940
It's only one or two
seconds of the video.

00:29:28.940 --> 00:29:31.070
But with the Video
API it's really easy

00:29:31.070 --> 00:29:33.710
to make our library searchable.

00:29:33.710 --> 00:29:35.450
So I'll do one more search.

00:29:35.450 --> 00:29:38.632
I'm going to search
for beach, because it

00:29:38.632 --> 00:29:40.340
might be nice to be
on a beach right now.

00:29:40.340 --> 00:29:42.590
It's a little cold today,
it was a little rainy.

00:29:42.590 --> 00:29:44.874
So now can't go to a
beach, but we can search

00:29:44.874 --> 00:29:46.040
for all of our beach videos.

00:29:46.040 --> 00:29:48.050
It's the next best thing.

00:29:48.050 --> 00:29:52.880
So here we can find the beach
clips in all of our videos,

00:29:52.880 --> 00:29:54.100
which is pretty cool.

00:29:54.100 --> 00:29:56.440
So something that used
to take hours, now

00:29:56.440 --> 00:29:58.340
with the Video
Intelligence API you

00:29:58.340 --> 00:30:01.094
can do in seconds or minutes.

00:30:01.094 --> 00:30:02.260
Let's go back to the slides.

00:30:04.916 --> 00:30:07.040
So just briefly I want to
share how the demo works,

00:30:07.040 --> 00:30:09.680
because it uses a couple of
other Google Cloud Platform

00:30:09.680 --> 00:30:11.210
products as well.

00:30:11.210 --> 00:30:14.690
All of the videos are stored in
a Google Cloud storage bucket.

00:30:14.690 --> 00:30:17.852
And I've got a Cloud function
listening on that bucket.

00:30:17.852 --> 00:30:19.310
So that Cloud
function is triggered

00:30:19.310 --> 00:30:21.081
anytime a new video is added.

00:30:21.081 --> 00:30:22.830
It's going to make
sure it's a video file.

00:30:22.830 --> 00:30:25.790
And then if it is, we will send
it to the Video Intelligence

00:30:25.790 --> 00:30:27.290
API for processing.

00:30:27.290 --> 00:30:29.390
And the Video API
lets you pass it

00:30:29.390 --> 00:30:32.180
in the request parameters
the URL of a different cloud

00:30:32.180 --> 00:30:35.120
storage bucket where you'd like
it to write the annotations to.

00:30:35.120 --> 00:30:36.874
So once it's done
annotating the video,

00:30:36.874 --> 00:30:39.290
it's going to write all my
annotations to a separate cloud

00:30:39.290 --> 00:30:40.950
storage bucket.

00:30:40.950 --> 00:30:43.027
My front end is a
Node.js app that's

00:30:43.027 --> 00:30:44.610
running on Google
App Engine, which is

00:30:44.610 --> 00:30:46.454
our platform as a service tool.

00:30:46.454 --> 00:30:48.870
And so the front end isn't
calling the Video API directly.

00:30:48.870 --> 00:30:51.030
All it's doing is
getting the video

00:30:51.030 --> 00:30:53.160
from one cloud storage
bucket and then

00:30:53.160 --> 00:30:55.220
the associated
metadata from another.

00:30:58.160 --> 00:31:00.800
This is what the response
looks like for label detection.

00:31:00.800 --> 00:31:04.190
So here we have a video that
has a scene of bird's eye view.

00:31:04.190 --> 00:31:05.900
And we get this location data.

00:31:05.900 --> 00:31:08.120
So it's going to tell
us the start and end

00:31:08.120 --> 00:31:11.960
time in microseconds that this
label occurs in our video.

00:31:11.960 --> 00:31:14.000
And if it occurred
more than once,

00:31:14.000 --> 00:31:16.945
I'd have more than one object in
my segment part of my response.

00:31:16.945 --> 00:31:18.560
And we also got a
confidence score.

00:31:18.560 --> 00:31:20.720
How confident is it
that it correctly

00:31:20.720 --> 00:31:24.730
labeled the scene of our video?

00:31:24.730 --> 00:31:26.215
So that's all I
got for the APIs.

00:31:26.215 --> 00:31:29.620
I encourage you to try them
all out in the browser.

00:31:29.620 --> 00:31:31.179
So as I mentioned
for the Vision API,

00:31:31.179 --> 00:31:33.220
I showed you how you can
try that in the browser.

00:31:33.220 --> 00:31:36.880
You can actually do that for all
of our machine learning APIs.

00:31:36.880 --> 00:31:39.340
You can use your own
audio, text, video,

00:31:39.340 --> 00:31:41.990
try it out, see
the API response.

00:31:41.990 --> 00:31:44.920
And then if you want to find
the code for most of the demos

00:31:44.920 --> 00:31:47.710
that I showed you today,
it's available in the two

00:31:47.710 --> 00:31:49.150
repos listed there.

00:31:49.150 --> 00:31:51.811
So I'll let you all get
a picture of that slide

00:31:51.811 --> 00:31:52.310
real quick.

00:31:55.590 --> 00:31:57.596
And thank you.

00:31:57.596 --> 00:31:58.470
Thanks for having me.

00:31:58.470 --> 00:32:01.520
[APPLAUSE]

