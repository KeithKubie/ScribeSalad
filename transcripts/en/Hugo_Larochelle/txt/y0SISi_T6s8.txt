Speaker 1:          00:01          In this video, we'll briefly discuss a convolutional variant of the restricted Boltzmann machine. So we saw a, to take a, a normal t four neural network and adapted to, uh, images by making it convolutional, uh, now what about, uh, designing convolutional neural networks that are unsupervised. So for instance, we've seen the restrictive bolsa machine as a type of neural network, but that can be unsupervised. They can learn about the distribution of the data to be able to extract features without actually providing labels. So can we design in convolutional variant of the RBM? Uh, well, yes. Uh, and one way of doing this really is just to apply these similar ideas we've discussed in the convolutional neural net in the feet forward supervised case and adapt them for the unsupervised case that is, have hidden units that are locally connected, that share parameters and that are divided into feature maps and introduce the notion of pulling units.

Speaker 1:          01:02          So that's exactly what unlikely. And his colleagues and his colleagues in the 2009 I've done. And so I'm going to briefly describe what they've done are what they've done. So, um, uh, what they do is that they again, define and energy function in the RBM where, where you have hidden units that interact with some visible units in a visible layer and the interact through some connections that are weighted. We also have some biases for the visible and hidden units. But now, uh, and I won't go into details of this, a notation here, uh, but now essentially a hidden unit in their feature map came at a position igene. That feature map is only going to be connected to an neighborhood. And in this notation here, the neighborhood is indexed by this, uh, r and s variable here. So, uh, essentially we have, um, our, which is right here, uh, which is, uh, as a value from one to n, w and the Nwa is essentially the size of the receptive field of that, of the hidden units in the

Speaker 2:          02:16          uh, and, uh, filter maps.

Speaker 1:          02:19          If we write it into a more,

Speaker 2:          02:21          uh, uh,

Speaker 1:          02:23          confusional notation, then essentially all the filter maps, which because it's an RBM now filter map actually correspond to a filter map of binary random variables is going to be, uh, connected. It's going to be multiplied by the result of performing convolution of some, uh, wait filter with the original input. V V is actually here. It's an Italy, but in the annotation here, they're, they're using V for a vector or matrices. So it should really be involved here. So we're taking image involving it with some filter map, a parameter matrix, and we multiplied the result of this element wise with the stochastic binary hidden units in the cave filter map. And then we add those up. And these terms are now involved now defined the energy function in the convulsion or Rpm.

Speaker 1:          03:22          So that takes care of introducing a dis idea of local connectivity and a parameter sharing across hidden units and the segmenting, the hidden institute feature maps. Now what about probably, uh, what about pooling? Now how do we introduce this idea into to the RPM? So a Leah doll introduced the notion of a probabilistic pooling type of unit. And a, the idea is as follows, so we'll have a, so we have a unit say here, which is, uh, in some future map layer and it's as we see locally connected with and just a receptive field within the physical layer. And that unit, it will be a belonging, will belong into a local neighborhood where there will be other hidden units within the filter map. Um, and now we'll add another layer right on top of it. Uh, these pulling units p and uh, they will be equal to one if only if at least one of the hidden unit in its current funding neighborhood in the filter map right below is equal to one.

Speaker 1:          04:30          So if there's at least one year binary unit here, which is equal to one, the pooling unit will be equal to one. So this is as if, uh, an otherwise it's equal to zero. So really just a pulling unit is equal to the Max of the hidden units and the filter maps associated with filter maps, uh, right below. And another thing that they propose is that within the pooling, their boyhood, this a neighborhood where we are taking the maximum over the binary hidden units, but also impose that there can be only one hidden unit which is equal to one. And so this will introduce the kind of competition that we see in uh, local, uh, uh, contrast normalization in a regular feed forward convolutional network. So it means that within the neighborhood there's actually only one feature that will be, uh, stochastically active in the German model of the restricted Boltzmann machine.

Speaker 1:          05:26          So in other words, if we write as this with this notation here, the uh, computer chase computation that corresponds to computing the pre activation of the, uh, filter map where here they're just assuming there's a single input channel. So we just have one compulsion to do. And they also introduced some bias. So this I a notation here, if we use it, the probability of a hidden units being called to one is going be the exponential of the pre activation divided by one plus the sum over all the other units in the neighborhood to which the hidden unit, Hki j belongs to of the exponential of their p activation. So this is just to take into account the fact that the other hidden units also have a probability of being caught. Two one and the one here is for the case where none of the hidden units in the neighborhood are equal to one, they all equal to zero, in which case, if we look at the energy function, we'd get a zero term that's multiplying, uh, all the, uh, pre activations, uh, of all the hidden units in the, in the neighborhood.

Speaker 1:          06:40          Okay. So, uh, we essentially have a sort of soft max over these hidden units in that neighborhood, but with an extra possibility, which is none of the units are equal to two one and they're all equal to zero. So it's kind of this extended softmax kind of activation here. And then the pooling unit, uh, while it will be equal to zero with probability one over one plus the normalization. So that's essentially the case where all, uh, so if the cooling unit is zero, it means that also all the hidden units are equal to zero. So that's the extra term to make sure that all of these probabilities of each being equal to one. And uh, the, uh, uh, and the case where they're all equal to zero sums to, uh, this whole thing sums to one. So now when we going to sample, when you will save performing gib sampling or assembling the value of the hidden units age and the pulling units, what we have to do is for each neighborhood, we have to sample from that distribution here. So we determine whether which of the hill union is active or whether none of them are active. And we determined this using this sort of extended soft max kind of a distribution.

Speaker 1:          07:57          And then if we sample and we're performing give sampling, we sampled all the value of the hidden units than sampling the visible units, uh, is, uh, if we're assuming the physical units are binary and again, we could similarly extend to other types of visible units are using the same ideas. We would just stay the hidden layer, uh, feature maps and can involve them with their corresponding, uh, a matrix of parameters, their filters and summing over all the feature maps. And then, uh, that convolution at position Ij plus some bias, see pass through the sigmoid gives us the probability that this, uh, particular visible unit at position Ij is equal to one a and one minus. That would be the palliative it being equal to zero. So essentially, uh, the difference between a regular RBM is that when we're performing, give, give sampling, instead of doing a matrix multiplication, sigmoid sampling matrix, pontification sigmoid sampling, uh, we're doing convolution a nun in the irony, which is this, a sort of extended soft Max ober over the local neighborhood sampling and then convolution, a sigmoid sampling and then alternating like this to perform gib sampling. In this convolutional no variant of their restricted Boltzmann machine.

Speaker 1:          09:26          So using these conditionals, Eh, we can perform contrast of divergence because we have a, we now have a sampling, these negative samples we need for updating the a, the model. I will need the gradients of the energy function to derive our update for the uh, uh, for the parameters of the filter maps and a weekend compute them similarly to how we compute the back propagated gradients and irregular feed forward neural network using convolutions. We can stack these rbms one on top of the other. So we would actually use the pool units as the values on which to train another conclusion rpm. And then we could get multiple layers of a convolutional rbms like this. And this in fact is we can think of it as giving us a pre training procedure that's a fork emotional neural network that does not require just extracting patches. And we'd actually be a globe, a train globally on the images. And so, uh, to get a, so I've been going fairly quickly over this to get more details about this. I encourage you to read unlikelies and his colleagues papers, uh, which was then published in 2009. And it's, uh, available on the website for this course.