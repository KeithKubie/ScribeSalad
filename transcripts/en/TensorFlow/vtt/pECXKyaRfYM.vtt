WEBVTT
Kind: captions
Language: en

00:00:00.263 --> 00:00:03.078
♪ (music) ♪

00:00:04.857 --> 00:00:05.857
Welcome back.

00:00:05.857 --> 00:00:08.447
- I'm Paige Bailey, and this is...
- Laurence Moroney,

00:00:08.447 --> 00:00:12.217
and we are here to answer
all of your <i>Ask TensorFlow</i> questions

00:00:12.217 --> 00:00:13.638
at the TensorFlow Dev Summit.

00:00:13.638 --> 00:00:17.006
So, if you have any questions,
please submit them to social media

00:00:17.006 --> 00:00:19.572
with the hashtag #AskTensorFlow.

00:00:19.572 --> 00:00:21.407
And we'll answer
as many of them as we can,

00:00:21.407 --> 00:00:24.164
but any that we can't get to today
we'll try to reach out to you later

00:00:24.164 --> 00:00:25.614
- to answer them.
- Absolutely.

00:00:25.617 --> 00:00:28.116
So let's get started
with the first question.

00:00:28.116 --> 00:00:30.612
- Okay, so shall I try this one?
- Yeah, absolutely.

00:00:30.612 --> 00:00:33.297
So this one, I think, came on Twitter,
from @yuehgoh, and it said:

00:00:33.297 --> 00:00:36.386
"Once I installed tensorflow-gpu,
import did not work for me.

00:00:36.386 --> 00:00:38.806
I have been trying to use it,"
but has been unable to do so.

00:00:38.806 --> 00:00:41.088
"It fails to load
native tensorflow runtime."

00:00:41.088 --> 00:00:42.765
I remember seeing this one.

00:00:42.765 --> 00:00:44.126
Actually, it was on YouTube,

00:00:44.126 --> 00:00:45.807
and it was in response to one of my videos

00:00:45.807 --> 00:00:49.385
about pip-installed
TensorFlow GPU in Colab,

00:00:49.385 --> 00:00:51.425
because once upon a time in Colab,

00:00:51.425 --> 00:00:54.306
you had to pip install TensorFlow GPU
to be able to use it,

00:00:54.306 --> 00:00:57.027
and now if you try to do that,
you end up having some issues.

00:00:57.027 --> 00:00:59.546
The reason for that is actually
really good, and it's good news,

00:00:59.546 --> 00:01:01.468
and it's because you don't need
to do it anymore.

00:01:01.468 --> 00:01:02.466
Excellent!

00:01:02.466 --> 00:01:05.187
So, actually, if we switch to Colab
for a second on my laptop,

00:01:05.187 --> 00:01:06.187
I can show you.

00:01:06.187 --> 00:01:08.297
This was the notebook
that I was showing earlier on.

00:01:08.297 --> 00:01:11.317
And all you have to do,
if you want to use GPU in Colab,

00:01:11.317 --> 00:01:13.319
is just change your runtime type,

00:01:13.319 --> 00:01:15.376
pick GPU as the hardware accelerator,

00:01:15.376 --> 00:01:18.247
and now you don't need
to pip install TensorFlow GPU;

00:01:18.247 --> 00:01:21.009
it actually does it for you
under the hood, behind the scenes.

00:01:21.009 --> 00:01:22.276
It's really, really cool,

00:01:22.276 --> 00:01:24.706
and that's why earlier
I was able to train this so quickly,

00:01:24.706 --> 00:01:26.415
because I was actually using the GPU.

00:01:26.415 --> 00:01:28.844
And, as you can see,
there's no pip install GPU on here.

00:01:28.844 --> 00:01:29.837
(Paige) Excellent.

00:01:29.837 --> 00:01:32.437
Whenever we were initially testing
TensorFlow 2.0,

00:01:32.437 --> 00:01:36.777
we had a kind of similar issue,
as well, with the GPU install,

00:01:36.777 --> 00:01:40.027
in that you needed specific CUDA drivers.

00:01:40.027 --> 00:01:43.806
But now, CUDA 10
is supported in Colab, as well.

00:01:43.806 --> 00:01:47.565
So Colab is a great experience
if you're using a GPU

00:01:47.565 --> 00:01:49.867
or if you're using
any other accelerated hardware.

00:01:49.867 --> 00:01:52.006
(Laurence) Yeah, and a pro tip
going forward, as well,

00:01:52.006 --> 00:01:53.548
if you want to do GPU stuff,

00:01:53.548 --> 00:01:55.910
because this was something
that I ran into a number of times

00:01:55.910 --> 00:01:57.486
when trying to use the GPU,

00:01:57.486 --> 00:01:59.596
was that you always
have to carefully take a look

00:01:59.596 --> 00:02:03.516
at the version of CUDA
and cuDNN that you're using.

00:02:03.516 --> 00:02:05.124
Because I made the mistake

00:02:05.124 --> 00:02:07.186
that I just went to the vendor's website,

00:02:07.186 --> 00:02:10.016
I downloaded the latest versions,
I installed them,

00:02:10.016 --> 00:02:11.942
and then I saw TensorFlow
was actually supporting

00:02:11.942 --> 00:02:13.728
a slightly earlier version.

00:02:13.728 --> 00:02:16.296
So if you do get an error
when you're trying to use GPU,

00:02:16.296 --> 00:02:17.764
just take a look at the version

00:02:17.764 --> 00:02:19.449
of the driver
that it's looking to support,

00:02:19.449 --> 00:02:20.896
and then, from the vendor's website,

00:02:20.896 --> 00:02:22.506
download that specific version.

00:02:22.506 --> 00:02:25.077
- (Paige) Yeah, driver issues...
- Driver issues.

00:02:25.077 --> 00:02:27.058
(Paige) They're always a treat, right?

00:02:27.058 --> 00:02:28.435
Exactly!

00:02:28.435 --> 00:02:31.257
It's one of the things
that makes our job interesting.

00:02:31.257 --> 00:02:32.876
Absolutely.

00:02:32.876 --> 00:02:34.797
Alright, so shall we take a look
at the next one?

00:02:34.797 --> 00:02:37.278
Yeah, let's go-- Oh! @adkumar!

00:02:37.278 --> 00:02:42.461
So @adkumar had at least eight
excellent questions on Twitter.

00:02:44.281 --> 00:02:45.806
Maybe he wins the volume award.

00:02:45.806 --> 00:02:47.229
He absolutely does!

00:02:47.229 --> 00:02:49.946
And we'll get to all of them,

00:02:49.946 --> 00:02:53.384
not in this sort of
<i>Ask TensorFlow</i> segment,

00:02:53.384 --> 00:02:55.436
but let's focus on just one for today

00:02:55.436 --> 00:02:57.027
and then answer the rest offline.

00:02:57.027 --> 00:02:59.467
Yeah, and I think a lot of them
were really about file formats

00:02:59.467 --> 00:03:01.076
and how do I use different file formats,

00:03:01.076 --> 00:03:02.532
so shall we drill into that?

00:03:02.532 --> 00:03:04.636
Absolutely. So the question is:

00:03:04.636 --> 00:03:08.707
"Which is the preferred format
for saving the model going forward,"

00:03:08.707 --> 00:03:11.394
<i>saved_model</i> or something else?

00:03:11.394 --> 00:03:14.836
And, if we look at the laptop,
we can take a gander

00:03:14.836 --> 00:03:17.796
at one of the slides
from the keynote this morning,

00:03:17.796 --> 00:03:20.600
really showing that Keras
is a first-class citizen

00:03:20.600 --> 00:03:21.854
in TensorFlow 2.0

00:03:21.854 --> 00:03:26.516
and <i>SavedModel</i> is at the heart
of every deployment.

00:03:26.516 --> 00:03:29.886
So here you can see <i>SavedModel</i>
being used for TensorFlow Serving,

00:03:29.886 --> 00:03:32.118
for TensorFlow Lite, for TensorFlow.js,

00:03:32.118 --> 00:03:34.969
and lots of other language bindings,

00:03:34.969 --> 00:03:37.825
so really, we're pushing
for the <i>SavedModel</i>.

00:03:37.825 --> 00:03:40.238
(Laurence) And if you focus
on <i>SavedModel</i> you can't go wrong.

00:03:40.238 --> 00:03:41.276
(Paige) Yes, absolutely.

00:03:41.276 --> 00:03:43.058
It's a lot easier to use

00:03:43.058 --> 00:03:46.917
than some of the other deployment options
that we'd seen before.

00:03:46.917 --> 00:03:49.436
Yeah, so I think the guidance
would be in the recommendation,

00:03:49.436 --> 00:03:51.067
not just for AD, but for everybody else.

00:03:51.067 --> 00:03:53.337
And, when you're thinking
about saving out your models,

00:03:53.337 --> 00:03:55.728
take a look at <i>SavedModel,</i>
consider using <i>SavedModel,</i>

00:03:55.728 --> 00:03:58.996
and, as a result, it's not only
is the advantage of the file format,

00:03:58.996 --> 00:04:02.005
but just how it's supported
across all of these things.

00:04:02.005 --> 00:04:04.356
And an excellent point of TensorFlow 2.0--

00:04:04.356 --> 00:04:07.257
I'm just going to keep selling it--
is that we have a number

00:04:07.257 --> 00:04:10.127
of code samples and tutorials
available today

00:04:10.127 --> 00:04:12.968
about how you can deploy
your models with <i>SavedModel.</i>

00:04:12.968 --> 00:04:14.856
Yeah, and I've personally found,

00:04:14.856 --> 00:04:18.327
from playing with some
of the TensorFlow Lite stuff in 2.0,

00:04:18.327 --> 00:04:20.726
saving as a <i>SavedModel</i>
and then going through the conversion

00:04:20.726 --> 00:04:23.137
to the TF Lite process,
it was a lot easier for me

00:04:23.137 --> 00:04:24.747
than in previous iterations

00:04:24.747 --> 00:04:27.108
where I had to use <i>TocoConverter</i>
and all that kind of stuff.

00:04:27.108 --> 00:04:29.936
So it's really being refined.
We're really iterating on that.

00:04:29.936 --> 00:04:32.887
- And I think it looks really cool!
- Excellent!

00:04:32.887 --> 00:04:36.249
So thanks for all of those questions, AD.
There's some great stuff in there.

00:04:36.249 --> 00:04:38.006
We'll try to answer
some of the rest of them,

00:04:38.006 --> 00:04:39.969
but understood that
most of them are focused

00:04:39.969 --> 00:04:43.266
around the file format
and hopefully <i>SavedModel</i> will help you.

00:04:43.266 --> 00:04:45.887
- Alright.
- Perfect, so let's go to the next one.

00:04:45.887 --> 00:04:49.725
So this next one comes from Elie Gakuba,

00:04:49.725 --> 00:04:53.117
asking, "Is it possible
to run tensorboard on colabs?"

00:04:53.117 --> 00:04:55.446
- And I know this made Paige really happy!
- Ah, yes!!

00:04:55.446 --> 00:04:59.057
Oh, dude! You are going
to be so delighted!

00:04:59.057 --> 00:05:02.448
Because before TensorBoard
was running on Colabs,

00:05:02.448 --> 00:05:05.056
we were talking about it:
"We really want it on Colabs!"

00:05:05.056 --> 00:05:06.586
It was <i>so</i> painful.

00:05:06.586 --> 00:05:08.816
And if you wanted to get it working

00:05:08.816 --> 00:05:10.687
in a Colab notebook or in Jupyter,

00:05:10.687 --> 00:05:12.936
you ended up using a tool like Ngrok,

00:05:12.936 --> 00:05:18.206
and that was kind of not approved
by our bosses, or in general.

00:05:20.063 --> 00:05:24.496
But yes, the good news is that
you can run TensorBoard in Colabs.

00:05:24.496 --> 00:05:27.057
(Laurence) And when it was
first announced internally in Google,

00:05:27.057 --> 00:05:28.496
before it was publically announced,

00:05:28.496 --> 00:05:30.396
we all got this email from Paige,

00:05:30.396 --> 00:05:32.806
and it was full of all these
smiley emojis and hearts.

00:05:32.806 --> 00:05:34.457
(Paige laughs)

00:05:34.457 --> 00:05:36.227
So, Elie, thank you for the question,

00:05:36.227 --> 00:05:37.825
because I think you've made Paige's day.

00:05:37.825 --> 00:05:40.181
(Paige) Excellent!
And so you can see here in the Colab,

00:05:40.181 --> 00:05:42.737
I'm running through
and downloading some files

00:05:42.737 --> 00:05:45.136
in the hope that we could
play with it a little bit,

00:05:45.136 --> 00:05:48.026
but here you can see it actually working.

00:05:48.026 --> 00:05:51.751
You should be able to do
different operations like smoothing,

00:05:53.565 --> 00:05:55.517
changing some of the values,

00:05:55.517 --> 00:05:59.736
and then also using
the embedding visualizer

00:05:59.736 --> 00:06:02.008
directly from your Colab notebook

00:06:02.008 --> 00:06:04.256
in order to understand your accuracies

00:06:04.256 --> 00:06:08.136
and to be able to do
model performance debugging.

00:06:08.136 --> 00:06:11.238
Another nice thing that the team
has been working very, very hard on

00:06:11.238 --> 00:06:13.797
is that you don't have to specify ports.

00:06:13.797 --> 00:06:16.597
So you don't have to remember

00:06:16.597 --> 00:06:20.042
if you wanted to have
multiple TensorBoard instances running,

00:06:20.042 --> 00:06:25.757
that you were using, what is it,
6006, or whatever, for another.

00:06:25.757 --> 00:06:29.166
It just automatically selects one
that would be a good candidate

00:06:29.166 --> 00:06:30.987
and creates it for you.

00:06:30.987 --> 00:06:33.607
So the team is phenomenal.

00:06:33.607 --> 00:06:37.156
If you have any interest whatsoever
in TensorBoard at all,

00:06:37.156 --> 00:06:41.045
I suggest stalking their PRs, like I do,

00:06:41.045 --> 00:06:43.296
because that's how I found out

00:06:43.296 --> 00:06:49.047
that TensorBoard got added
to Jupyter notebooks and also to Colab.

00:06:49.047 --> 00:06:51.536
But, yeah, so excited.

00:06:51.536 --> 00:06:54.497
And we'll have this link
in the documentation for the video,

00:06:54.497 --> 00:06:57.677
as well, the little notes underneath,
for you to go and play with.

00:06:57.677 --> 00:07:00.897
And I do have to say, it's so great
to have a PR stalker in our group.

00:07:00.907 --> 00:07:02.316
(laughter)

00:07:02.316 --> 00:07:06.018
I get push notifications
to my phone-- It's a problem.

00:07:06.018 --> 00:07:09.306
But yeah, they've been doing
such great work.

00:07:09.306 --> 00:07:13.218
(Laurence) So the question is yes,
TensorBoard is working in Colab.

00:07:13.218 --> 00:07:15.797
- And also Project Jupyter notebooks!
- Nice.

00:07:15.797 --> 00:07:19.377
Use it wherever!
TensorBoard everywhere!

00:07:19.377 --> 00:07:21.265
TensorBoard everywhere;
we all love TensorBoard.

00:07:21.265 --> 00:07:22.951
I haven't really played with it that much,

00:07:22.951 --> 00:07:26.006
but do a lot of the plugins also work?

00:07:26.006 --> 00:07:30.104
So TensorBoard really is a collection

00:07:30.104 --> 00:07:31.663
of these different visualizations,

00:07:31.663 --> 00:07:33.866
so you can see scalers,
like your accuracy;

00:07:33.866 --> 00:07:35.686
you can see histograms;

00:07:35.686 --> 00:07:37.525
you can see embedding visualizers,

00:07:37.525 --> 00:07:39.219
which allows you to do clustering,

00:07:39.219 --> 00:07:40.688
like that great MNIST example

00:07:40.688 --> 00:07:42.367
from the Dev Summit a couple years ago.

00:07:42.367 --> 00:07:44.007
- With all that stuff.
- Moving it around.

00:07:44.007 --> 00:07:45.765
- I play with that all day.
- It's beautiful.

00:07:45.765 --> 00:07:49.669
And then also Beholder,
which was created by a community member.

00:07:49.669 --> 00:07:51.962
The plugins...
It's so funny you mention it.

00:07:51.962 --> 00:07:54.896
A couple of our GSOC,
our Google Summer of Code projects

00:07:54.896 --> 00:07:57.079
this summer are focused on getting

00:07:57.079 --> 00:08:00.999
additional visualization plugins
added to TensorBoard.

00:08:00.999 --> 00:08:02.258
(Laurence) Cool! Nice!

00:08:02.258 --> 00:08:05.458
So in addition to the What-If Tool,
in addition to Beholder,

00:08:05.458 --> 00:08:08.020
you could make
your own TensorBoard plugin.

00:08:08.020 --> 00:08:10.696
While I could geek out about this all day,
I think we should move on

00:08:10.696 --> 00:08:12.671
to some other questions
that some folks have had.

00:08:12.671 --> 00:08:14.156
Can't we kick out about this all day?

00:08:14.156 --> 00:08:16.574
Can we extend the stream
for three or four hours?

00:08:16.574 --> 00:08:19.778
So the next question that came in,
from Amirhosein Herandy,

00:08:19.778 --> 00:08:22.446
"How would you use
feature_columns with Keras?"

00:08:22.446 --> 00:08:24.208
I know you know
the answer to this question.

00:08:24.208 --> 00:08:26.166
But I know that it's
a particular passion of yours

00:08:26.166 --> 00:08:28.018
that you're working
with Estimators in Keras.

00:08:28.018 --> 00:08:30.016
So, for the folks who are watching,

00:08:30.016 --> 00:08:31.966
feature columns
are really part of Estimators.

00:08:31.966 --> 00:08:33.627
They're a way of really getting your data

00:08:33.627 --> 00:08:35.927
efficiently into Estimators.

00:08:35.927 --> 00:08:38.247
And with people saying,
"Hey, it's all there in Estimators.

00:08:38.247 --> 00:08:39.765
What about Keras?"

00:08:39.765 --> 00:08:42.092
You've been working on
some great stuff around that, right?

00:08:42.092 --> 00:08:46.168
I have, but I know
that your YouTube channel

00:08:46.168 --> 00:08:50.119
has a series from Karmel,
who spoke earlier today.

00:08:50.119 --> 00:08:53.067
Yeah, so Karmel
is our engineering director

00:08:53.067 --> 00:08:56.106
for high-level APIs,
and she has this great series

00:08:56.106 --> 00:08:58.906
around high-level APIs for TensorFlow,

00:08:58.906 --> 00:09:01.227
really, really teaching you
how to use the high-level APIs.

00:09:01.227 --> 00:09:04.396
And Karmel is working actively,
and her team are working actively,

00:09:04.396 --> 00:09:07.726
for parity of things
like feature columns in TensorFlow 2.0.

00:09:07.726 --> 00:09:09.696
I'm not sure if it's fully
there yet in the Alpha.

00:09:09.696 --> 00:09:11.278
I haven't checked into it yet.

00:09:11.278 --> 00:09:14.676
But, yeah, it's on the way,
if it's not there already.

00:09:14.676 --> 00:09:16.506
So you should be able
to use them in Keras.

00:09:16.506 --> 00:09:19.566
(Paige) Yes, and we're also
in the process of building out--

00:09:19.566 --> 00:09:22.387
If you wanted to migrate your models
from using Estimators

00:09:22.387 --> 00:09:25.563
to being more
of a TensorFlow 2.0 format

00:09:25.563 --> 00:09:27.069
with Keras,

00:09:27.069 --> 00:09:30.128
I am currently in the process
of building a migration guide.

00:09:30.128 --> 00:09:33.206
So if you have any interest around that,

00:09:33.206 --> 00:09:34.406
please feel free to reach out.

00:09:34.406 --> 00:09:37.096
And we're excited
to get that released pretty soon.

00:09:37.096 --> 00:09:38.776
I'm really excited
to see that, personally,

00:09:38.776 --> 00:09:40.736
because the very first stuff
I did in TensorFlow

00:09:40.736 --> 00:09:43.210
was with Estimators,
before I learned Keras,

00:09:43.210 --> 00:09:46.616
and I really want to go back
and start changing them to use Keras

00:09:46.616 --> 00:09:47.806
without rewriting them.

00:09:47.806 --> 00:09:48.789
Absolutely.

00:09:48.789 --> 00:09:51.615
Keras is just friendlier
to work with, my feel.

00:09:51.615 --> 00:09:53.161
Yeah, so they're both great.

00:09:53.161 --> 00:09:56.442
Estimators really gives you the power.
Keras, great for beginners.

00:09:56.442 --> 00:09:59.612
So hopefully we'll get
the best of both worlds.

00:09:59.612 --> 00:10:01.343
Shall we take a look at the next question?

00:10:01.343 --> 00:10:02.342
Yes!

00:10:02.342 --> 00:10:05.774
Jeff Thomas asks,
"looking for some simple data sets

00:10:05.774 --> 00:10:08.232
for testing and comparing
different training methods."

00:10:08.232 --> 00:10:11.370
Aha! Different!
"Looking for new data sets."

00:10:11.370 --> 00:10:13.283
MNIST is great. Fashion-MNIST is great.

00:10:13.283 --> 00:10:14.313
But, after a while,

00:10:14.313 --> 00:10:16.084
people want something
new and fresh, right?

00:10:16.084 --> 00:10:17.083
Yes.

00:10:17.083 --> 00:10:19.292
What do you think we can say to Jeff?

00:10:19.292 --> 00:10:22.212
We could tell them
about TensorFlow Datasets!

00:10:22.212 --> 00:10:24.172
And there's a great blog post
right here about it.

00:10:24.172 --> 00:10:25.173
Yes, it is.

00:10:25.173 --> 00:10:28.482
And TensorFlow Datasets
is really about creating

00:10:28.482 --> 00:10:31.743
those data-ingestion pipelines

00:10:31.743 --> 00:10:35.193
for you to be able to easily use
a variety of datasets

00:10:35.193 --> 00:10:37.784
with all of your deep learning
and machine learning models

00:10:37.784 --> 00:10:39.952
with just a few lines of code.

00:10:39.952 --> 00:10:41.903
So, if you're familiar with Scikit-learn

00:10:41.903 --> 00:10:45.982
and all of its nifty
data-ingestion practices,

00:10:45.982 --> 00:10:47.802
this feels very similar.

00:10:47.802 --> 00:10:50.092
It's very easy to do training
and testing splits

00:10:50.092 --> 00:10:52.363
and verification [splits],

00:10:52.363 --> 00:10:56.052
and we have a lot of datasets
readily available right now

00:10:56.052 --> 00:10:58.382
for you to go and explore.

00:10:58.382 --> 00:11:02.262
Another thing that I would
especially like to call out

00:11:02.262 --> 00:11:05.613
is a member of our community--

00:11:05.613 --> 00:11:07.918
So anybody can make your data famous.

00:11:07.918 --> 00:11:11.803
If you have a dataset that you're using
in your research lab,

00:11:11.803 --> 00:11:14.603
if you have a bright and shiny CSV

00:11:14.603 --> 00:11:18.206
that you think would be a cool add
to the structured section--

00:11:18.206 --> 00:11:20.336
I've never heard a CSV
called bright and shiny before.

00:11:20.336 --> 00:11:24.002
Well, you know... Everybody uses them.

00:11:26.052 --> 00:11:28.150
One of our community members,
Andrew Kondrich,

00:11:28.150 --> 00:11:30.132
who's an undergrad researcher at Stanford,

00:11:30.132 --> 00:11:32.943
he added this CheXpert Dataset
from his lab--

00:11:32.943 --> 00:11:36.872
200,000 chest radiograms,
from the Stanford ML group.

00:11:36.872 --> 00:11:40.082
And he was able to do it
in less than a day.

00:11:40.082 --> 00:11:44.943
It really is just as simple as--
take the template format

00:11:44.943 --> 00:11:47.353
for images or for audio
or whatever you're using,

00:11:47.353 --> 00:11:50.363
add some additional metadata
for the dataset,

00:11:51.003 --> 00:11:53.162
change, potentially, the types

00:11:53.162 --> 00:11:55.631
for some of the features
that you're using with it,

00:11:55.631 --> 00:11:57.839
- and, voila! there you go.
- You're off to the races!

00:11:57.839 --> 00:12:00.051
Absolutely, and you can
make your data famous!

00:12:00.051 --> 00:12:01.933
And one of the really
important things about it,

00:12:01.933 --> 00:12:04.811
particularly if you're getting started
or if you're learning,

00:12:04.811 --> 00:12:07.012
is that if you take a look
at a lot of the samples

00:12:07.012 --> 00:12:09.233
pre-TensorFlow Datasets.

00:12:09.233 --> 00:12:10.831
There tends to be lots and lots of code

00:12:10.831 --> 00:12:14.852
about download your data,
unzip it here, label it like this,

00:12:14.852 --> 00:12:16.683
put these files in these folders,

00:12:16.683 --> 00:12:19.002
or take your CSV and make these features,

00:12:19.002 --> 00:12:21.426
but when it's in TFDS,
it's like one or two lines of code,

00:12:21.426 --> 00:12:23.394
and the data will just get sorted

00:12:23.394 --> 00:12:26.113
into training and test sets for you,
that type of thing.

00:12:26.113 --> 00:12:27.972
So I think for learners, in particular,

00:12:27.972 --> 00:12:29.712
I found it really, really exciting

00:12:29.712 --> 00:12:32.022
because I didn't have
to go through 100 lines of code

00:12:32.022 --> 00:12:33.743
before I got to the [neural] network.

00:12:33.743 --> 00:12:35.553
And also data science-y people.

00:12:35.553 --> 00:12:38.147
So much of creating and training a model

00:12:38.147 --> 00:12:40.284
is understanding the shape and the format

00:12:40.284 --> 00:12:44.738
and the statistical distributions,
and this is really helpful.

00:12:44.738 --> 00:12:46.402
(Laurence) Yeah. So thanks very much.

00:12:46.402 --> 00:12:49.071
♪ (music) ♪

