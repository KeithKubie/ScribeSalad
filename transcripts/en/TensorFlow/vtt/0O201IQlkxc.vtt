WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.976
[MUSIC PLAYING]

00:00:07.337 --> 00:00:08.170
CHRISTINA GREER: Hi.

00:00:08.170 --> 00:00:10.720
My name is Christina and
I'm a software engineer

00:00:10.720 --> 00:00:12.730
on the Google Brain team.

00:00:12.730 --> 00:00:14.950
I'm here today to tell
you about some tools

00:00:14.950 --> 00:00:17.200
that my team and I
have built to help

00:00:17.200 --> 00:00:20.320
make the end-to-end lifecycle
of the machine learning pipeline

00:00:20.320 --> 00:00:22.740
easier.

00:00:22.740 --> 00:00:25.650
I'm going to start by
talking about model analysis

00:00:25.650 --> 00:00:27.340
and validation.

00:00:27.340 --> 00:00:31.140
These are two different
components in TFX,

00:00:31.140 --> 00:00:35.640
but they are very similar in
how they're actually executed.

00:00:35.640 --> 00:00:40.320
The main difference is how you
as an end user will use them.

00:00:40.320 --> 00:00:42.570
I'm going to start by
talking about the evaluator.

00:00:45.980 --> 00:00:48.640
So why is model
evaluation important?

00:00:48.640 --> 00:00:52.120
Well, for one thing,
we have gathered data.

00:00:52.120 --> 00:00:54.110
We've cleaned that data.

00:00:54.110 --> 00:00:56.150
We've trained a model.

00:00:56.150 --> 00:00:58.670
But we really want to make
sure that model works.

00:00:58.670 --> 00:01:00.830
And so, model
evaluation can help

00:01:00.830 --> 00:01:03.280
you assess the overall
quality of your model.

00:01:06.330 --> 00:01:10.190
You also may want to analyze
how your model is performing

00:01:10.190 --> 00:01:13.310
on specific slices of the data.

00:01:13.310 --> 00:01:15.850
So in this case, with
the Chicago taxi example

00:01:15.850 --> 00:01:17.980
that Clemens started
this off with,

00:01:17.980 --> 00:01:21.060
why are my tip predictions
sometimes wrong?

00:01:21.060 --> 00:01:24.630
Slicing the data and looking
at where you're doing poorly

00:01:24.630 --> 00:01:28.110
can be a real benefit, because
it identifies some low hanging

00:01:28.110 --> 00:01:33.510
fruit where you can get gains
in accuracy by adding more data

00:01:33.510 --> 00:01:36.310
or making some other changes
to make some of these segments

00:01:36.310 --> 00:01:36.810
improve.

00:01:39.890 --> 00:01:42.410
You also want to track
your performance over time.

00:01:42.410 --> 00:01:45.560
You're going to be continuously
training models and updating

00:01:45.560 --> 00:01:49.160
them with fresh data, so that
your models don't get stale.

00:01:49.160 --> 00:01:52.400
And you want to make sure that
your metrics are improving

00:01:52.400 --> 00:01:54.050
over time and not regressing.

00:01:54.050 --> 00:01:58.210
And model evaluation can
help you with all of this.

00:01:58.210 --> 00:02:00.300
The component of TFX
that supports this

00:02:00.300 --> 00:02:02.070
is called the evaluator.

00:02:02.070 --> 00:02:05.490
And it is based on a library
called TensorFlow Model

00:02:05.490 --> 00:02:07.420
Analysis.

00:02:07.420 --> 00:02:09.850
From the pipeline
perspective, you

00:02:09.850 --> 00:02:11.830
have inputs, which
is your eval set that

00:02:11.830 --> 00:02:14.890
was generated by
your ExampleGen.

00:02:14.890 --> 00:02:18.920
You have the trainer
outputting a saved model.

00:02:18.920 --> 00:02:21.700
You also need to specify
the splits in your data

00:02:21.700 --> 00:02:24.730
that you find most interesting,
so that the evaluator can

00:02:24.730 --> 00:02:26.890
precompute metrics for
these slices of data.

00:02:30.740 --> 00:02:33.310
Your data then goes
into the evaluator.

00:02:33.310 --> 00:02:36.880
And a process is run
to generate metrics

00:02:36.880 --> 00:02:42.140
for the overall slice and the
slices that you have specified.

00:02:42.140 --> 00:02:46.800
The output of the evaluator
is evaluation metrics.

00:02:46.800 --> 00:02:49.470
This is a structured data
format that has your data,

00:02:49.470 --> 00:02:52.680
the splits you specified, and
the metrics that correspond

00:02:52.680 --> 00:02:54.640
to each one of these splits.

00:02:54.640 --> 00:02:56.820
The TensorFlow Model
Analysis library

00:02:56.820 --> 00:02:58.950
also has a
visualization tool that

00:02:58.950 --> 00:03:02.010
allows you to load
up these metrics

00:03:02.010 --> 00:03:05.840
and dig around in your data
in a user friendly way.

00:03:09.980 --> 00:03:13.130
So going back to our
Chicago taxi example,

00:03:13.130 --> 00:03:17.140
you can see how the model
evaluator can help you look

00:03:17.140 --> 00:03:19.340
at your top line objective.

00:03:19.340 --> 00:03:22.980
How well can you predict trips
that result in large tips?

00:03:22.980 --> 00:03:27.230
The TFMA visualization shows
the overall slice of data here.

00:03:27.230 --> 00:03:32.510
The numbers are probably
small, but accuracy is 94.7%.

00:03:32.510 --> 00:03:33.320
That's pretty good.

00:03:33.320 --> 00:03:35.000
You'd get an A for that.

00:03:35.000 --> 00:03:37.750
But maybe you want to say 95%.

00:03:37.750 --> 00:03:41.840
95% accuracy is a lot
better number than 94, 94.7.

00:03:41.840 --> 00:03:45.022
So maybe you want to
bump that up a bit.

00:03:45.022 --> 00:03:47.230
So then you can dig into
why your tip predictions are

00:03:47.230 --> 00:03:49.780
sometimes wrong.

00:03:49.780 --> 00:03:52.450
We have sliced the data
here by the hour of day

00:03:52.450 --> 00:03:54.540
that the trip starts on.

00:03:54.540 --> 00:03:57.750
And we've sorted by
poor performance.

00:03:57.750 --> 00:03:59.500
When I look at this
data, I see that trips

00:03:59.500 --> 00:04:04.720
that start, like, 2:00, 3:00
AM were performing quite poorly

00:04:04.720 --> 00:04:07.860
in these times.

00:04:07.860 --> 00:04:10.560
Because of the statistics
generation tool

00:04:10.560 --> 00:04:12.840
that Clemens talked
about, I do know

00:04:12.840 --> 00:04:14.370
that the data is sparse here.

00:04:14.370 --> 00:04:17.490
But if I didn't know that,
perhaps I would think,

00:04:17.490 --> 00:04:19.620
maybe there's
something that people

00:04:19.620 --> 00:04:21.630
that get taxis at 2:00
or 3:00 in the morning

00:04:21.630 --> 00:04:25.773
might have in common that
causes erratic tipping behavior.

00:04:25.773 --> 00:04:28.440
Someone smarter than me is going
to have to figure that one out.

00:04:31.078 --> 00:04:32.870
You also want to know
if you can get better

00:04:32.870 --> 00:04:35.400
at predicting trips over time.

00:04:35.400 --> 00:04:38.090
So you are continuously training
these models for new data,

00:04:38.090 --> 00:04:40.110
and you're hoping
that you get better.

00:04:40.110 --> 00:04:42.350
So the TensorFlow
Model Analysis tool

00:04:42.350 --> 00:04:45.320
that powers the
evaluator and TFX

00:04:45.320 --> 00:04:48.210
can show you the trends
of your metrics over time.

00:04:48.210 --> 00:04:50.390
And so here you see
three different models

00:04:50.390 --> 00:04:53.580
and the performance over
each with accuracy in AUC.

00:04:56.722 --> 00:04:58.180
Now I'm going to
move on to talking

00:04:58.180 --> 00:05:01.360
about the ModelValidator
component.

00:05:01.360 --> 00:05:07.810
With the evaluator, you
were an active user.

00:05:07.810 --> 00:05:09.730
You generated the metrics.

00:05:09.730 --> 00:05:11.410
You loaded them up in the UI.

00:05:11.410 --> 00:05:12.730
You dug around in your data.

00:05:12.730 --> 00:05:14.380
You looked for
issues that you could

00:05:14.380 --> 00:05:16.300
fix to improve your model.

00:05:16.300 --> 00:05:18.640
But eventually, you're
going to iterate.

00:05:18.640 --> 00:05:20.080
Your data is going
to get better.

00:05:20.080 --> 00:05:21.340
Your model's going to improve.

00:05:21.340 --> 00:05:23.528
And you're going to
be ready to launch.

00:05:23.528 --> 00:05:25.570
You're also going to have
a pipeline continuously

00:05:25.570 --> 00:05:29.430
feeding new data
into this model.

00:05:29.430 --> 00:05:32.980
And every time you generate
a new model with new data,

00:05:32.980 --> 00:05:35.490
you don't want to have to do
a manual process of pushing

00:05:35.490 --> 00:05:37.890
this to a server somewhere.

00:05:37.890 --> 00:05:40.360
The ModelValidator
component of TFX

00:05:40.360 --> 00:05:42.900
acts as a gate that
keeps you from pushing

00:05:42.900 --> 00:05:45.690
bad versions of your model,
while allowing you to automate

00:05:45.690 --> 00:05:47.295
pushing of quality models.

00:05:50.190 --> 00:05:52.690
So why model validation
is important--

00:05:52.690 --> 00:05:56.100
we really want to avoid pushing
models with degraded quality,

00:05:56.100 --> 00:05:58.650
specifically in an
automated fashion.

00:05:58.650 --> 00:06:01.500
If you train a
model with new data

00:06:01.500 --> 00:06:03.420
and the performance
drops, but say

00:06:03.420 --> 00:06:05.700
it increases in certain
segments of the data

00:06:05.700 --> 00:06:07.440
that you really care
about, maybe you

00:06:07.440 --> 00:06:10.720
make the judgment call that
this is an improvement overall.

00:06:10.720 --> 00:06:12.890
So we'll launch it.

00:06:12.890 --> 00:06:15.900
But you don't want to
do this automatically.

00:06:15.900 --> 00:06:18.510
You want to have some
say before you do this.

00:06:18.510 --> 00:06:21.890
So this acts as your gatekeeper.

00:06:21.890 --> 00:06:24.830
You also want to avoid
breaking downstream components.

00:06:24.830 --> 00:06:27.140
If your model suddenly
started outputting something

00:06:27.140 --> 00:06:28.925
that your server
binary couldn't handle,

00:06:28.925 --> 00:06:30.800
you'd want to know that
also before you push.

00:06:33.920 --> 00:06:35.750
The TFX component
that supports this

00:06:35.750 --> 00:06:38.060
is called the ModelValidator.

00:06:38.060 --> 00:06:42.350
It takes very similar inputs and
outputs to the model evaluator.

00:06:42.350 --> 00:06:44.120
And the libraries that
compute the metrics

00:06:44.120 --> 00:06:46.400
are pretty much the same
underneath the hood.

00:06:46.400 --> 00:06:49.760
However, instead of one
model, you provide two--

00:06:49.760 --> 00:06:52.850
the new model that you're trying
to evaluate and the last good

00:06:52.850 --> 00:06:55.160
evaluated model.

00:06:55.160 --> 00:06:59.220
It then runs on your
if eval split data

00:06:59.220 --> 00:07:02.480
and compares the metrics on
the same data between the two

00:07:02.480 --> 00:07:04.300
models.

00:07:04.300 --> 00:07:08.050
If your metrics have stayed
the same or improved,

00:07:08.050 --> 00:07:11.450
then you go ahead
and bless the model.

00:07:11.450 --> 00:07:14.570
If the metrics that you
care about have degraded,

00:07:14.570 --> 00:07:17.130
you will not bless the model.

00:07:17.130 --> 00:07:19.130
Get some information about
which metrics failed,

00:07:19.130 --> 00:07:22.120
so that you can do
some further analysis.

00:07:22.120 --> 00:07:25.120
The outcome of this is
a validation outcome.

00:07:25.120 --> 00:07:27.070
It just says blessed if
everything went right.

00:07:31.060 --> 00:07:33.310
Another thing to note
about the ModelValidator

00:07:33.310 --> 00:07:37.060
is that it allows you to do
next day eval of your previously

00:07:37.060 --> 00:07:38.360
pushed model.

00:07:38.360 --> 00:07:41.440
So maybe the last
model that you blessed,

00:07:41.440 --> 00:07:44.380
it was trained with old data.

00:07:44.380 --> 00:07:47.680
With the ModelValidator, it
evaluates it on the new data.

00:07:51.240 --> 00:07:54.050
And finally, I'm going
to talk about the pusher.

00:07:54.050 --> 00:07:56.120
The pusher is probably
the simplest component

00:07:56.120 --> 00:07:58.840
in the entire TFX pipeline.

00:07:58.840 --> 00:08:02.870
But it does serve
quite a useful purpose.

00:08:02.870 --> 00:08:05.080
It has one input, which
is that blessing that you

00:08:05.080 --> 00:08:06.762
got from the ModelValidator.

00:08:09.930 --> 00:08:14.540
And then the output is if
you passed your validation,

00:08:14.540 --> 00:08:20.210
then the pusher will copy your
saved model into a file system

00:08:20.210 --> 00:08:21.845
destination that
you've specified.

00:08:24.460 --> 00:08:26.260
And now you're ready
to serve your model

00:08:26.260 --> 00:08:29.923
and make it useful to
the world at large.

00:08:29.923 --> 00:08:31.840
I'm going to talk about
model deployment next.

00:08:34.510 --> 00:08:36.380
So this is where we are.

00:08:36.380 --> 00:08:38.650
We have a trained SavedModel.

00:08:38.650 --> 00:08:42.789
A SavedModel is a universal
serialization format

00:08:42.789 --> 00:08:44.950
for TensorFlow models.

00:08:44.950 --> 00:08:49.660
It contains your graph, your
learned variable weights,

00:08:49.660 --> 00:08:53.410
your assets like
embeddings and vocabs.

00:08:53.410 --> 00:08:56.440
But to you, this is just
an implementation detail.

00:08:56.440 --> 00:09:00.550
Where you really want to
be is you have an API.

00:09:00.550 --> 00:09:02.380
You have a server
that you can query

00:09:02.380 --> 00:09:04.378
to get answers in
real time or provide

00:09:04.378 --> 00:09:05.545
those answers to your users.

00:09:09.610 --> 00:09:11.540
We provide several
deployment options.

00:09:11.540 --> 00:09:13.290
And many of them are
going to be discussed

00:09:13.290 --> 00:09:15.210
at other talks in the session.

00:09:15.210 --> 00:09:18.240
TensorFlow.js is optimized
for serving in the browser

00:09:18.240 --> 00:09:20.130
or on Node.js.

00:09:20.130 --> 00:09:22.680
TensorFlow Lite is optimized
for mobile devices.

00:09:22.680 --> 00:09:25.260
We already heard a talk about
how Google Assistant is using

00:09:25.260 --> 00:09:31.890
TensorFlow Lite to support model
inference on their Google Home

00:09:31.890 --> 00:09:33.350
devices.

00:09:33.350 --> 00:09:35.400
TensorFlow Hub is something new.

00:09:35.400 --> 00:09:38.670
And Andre is going to come
on in about five minutes

00:09:38.670 --> 00:09:41.880
and tell you about that, so I'm
not going to step on his toes.

00:09:41.880 --> 00:09:44.220
I'm going to talk about
TensorFlow Serving.

00:09:44.220 --> 00:09:47.490
So if you want to put
up a REST API that

00:09:47.490 --> 00:09:49.260
serves answers for
your model, you

00:09:49.260 --> 00:09:50.802
would want to use
TensorFlow Serving.

00:09:53.260 --> 00:09:55.560
And why would you
want to use this?

00:09:55.560 --> 00:09:57.330
For one thing,
TensorFlow Serving

00:09:57.330 --> 00:10:00.280
has a lot of flexibility.

00:10:00.280 --> 00:10:02.100
It supports multi-tenancy.

00:10:02.100 --> 00:10:06.970
You can run multiple models
on a single server instance.

00:10:06.970 --> 00:10:09.588
You can also run multiple
versions of the same model.

00:10:09.588 --> 00:10:11.130
This can be really
useful when you're

00:10:11.130 --> 00:10:13.470
trying to canary a new model.

00:10:13.470 --> 00:10:17.490
Say you have a tried and
tested version of your model.

00:10:17.490 --> 00:10:18.910
You've created a new one.

00:10:18.910 --> 00:10:20.160
It's passed your evaluator.

00:10:20.160 --> 00:10:21.810
It's passed your validation.

00:10:21.810 --> 00:10:24.870
But you still want to do some
A/B testing with real users

00:10:24.870 --> 00:10:27.665
before you completely
switch over.

00:10:27.665 --> 00:10:29.040
TensorFlow Serving
supports this.

00:10:32.270 --> 00:10:35.400
We also support optimization
with GPU and TensorRT.

00:10:38.710 --> 00:10:41.500
And you can expose a
gRPC or a REST API.

00:10:44.010 --> 00:10:47.830
TensorFlow Serving is also
optimized for high performance.

00:10:47.830 --> 00:10:52.040
It provides low latency,
request batching--

00:10:52.040 --> 00:10:53.820
so that you can
optimize your throughput

00:10:53.820 --> 00:10:57.000
while still respecting
latency requirements--

00:10:57.000 --> 00:10:58.540
and traffic isolation.

00:10:58.540 --> 00:11:01.395
So if you are serving multiple
models on a single server,

00:11:01.395 --> 00:11:03.780
a traffic spike in
one of those models

00:11:03.780 --> 00:11:06.580
won't affect the
serving of the other.

00:11:06.580 --> 00:11:11.290
And finally, TensorFlow
Serving is production-ready.

00:11:11.290 --> 00:11:12.790
This is what we
used to serve many

00:11:12.790 --> 00:11:15.310
of our models inside of Google.

00:11:15.310 --> 00:11:19.090
We've served millions
of QPS with it.

00:11:19.090 --> 00:11:20.890
You can scale in
minutes, particularly

00:11:20.890 --> 00:11:25.740
if you use the Docker image
and scale up on Kubernetes.

00:11:25.740 --> 00:11:28.180
And we support dynamic
version refresh.

00:11:28.180 --> 00:11:30.460
So you can specify a
version refresh policy

00:11:30.460 --> 00:11:32.830
to either take the latest
version of your model,

00:11:32.830 --> 00:11:35.440
or you can pin to
a specific version.

00:11:35.440 --> 00:11:37.300
This can be really
useful for rollbacks

00:11:37.300 --> 00:11:40.450
if you find a problem
with the latest version

00:11:40.450 --> 00:11:41.920
after you've already pushed.

00:11:44.810 --> 00:11:46.850
I'm going to go into a
little bit more detail

00:11:46.850 --> 00:11:50.780
about how you might deploy
a REST API for your model.

00:11:50.780 --> 00:11:54.260
We have two different options
for doing this presented here.

00:11:54.260 --> 00:11:57.460
The first, the top
command is using Docker,

00:11:57.460 --> 00:11:59.830
which we really recommend.

00:11:59.830 --> 00:12:02.750
It requires a little bit of
ramp up at the beginning,

00:12:02.750 --> 00:12:04.810
but you will really save
time in the long run

00:12:04.810 --> 00:12:07.420
by not having to
manage your environment

00:12:07.420 --> 00:12:10.760
and not having to manage
your own dependencies.

00:12:10.760 --> 00:12:12.620
You can also run locally
on your own host,

00:12:12.620 --> 00:12:16.930
but then you do have to do
all of that stuff long term.

00:12:16.930 --> 00:12:19.390
I'm going to go into a little
bit more detail on the Docker

00:12:19.390 --> 00:12:20.660
run command.

00:12:20.660 --> 00:12:23.070
So you start with Docker run.

00:12:23.070 --> 00:12:26.827
You choose a port that you
want to bind your API to.

00:12:26.827 --> 00:12:28.660
You provide the path
to the saved model that

00:12:28.660 --> 00:12:30.130
was generated by your trainer.

00:12:30.130 --> 00:12:34.250
Hopefully, it was
pushed by the pusher.

00:12:34.250 --> 00:12:35.930
You provide the model name.

00:12:35.930 --> 00:12:38.330
And you tell Docker to run
the TensorFlow Serving binary.

00:12:41.960 --> 00:12:44.090
Another advantage
of using Docker

00:12:44.090 --> 00:12:47.390
is that you can easily
enable hardware acceleration.

00:12:47.390 --> 00:12:49.790
If you're running
on a host with a GPU

00:12:49.790 --> 00:12:53.480
and the Nvidia Docker
image installed,

00:12:53.480 --> 00:12:57.650
you can modify this command
line by a few tokens,

00:12:57.650 --> 00:12:59.810
and then be running on
accelerated hardware.

00:13:03.100 --> 00:13:05.350
If you need even
further optimization,

00:13:05.350 --> 00:13:07.420
we now support
optimizing your model

00:13:07.420 --> 00:13:10.140
for serving using TensorRT.

00:13:10.140 --> 00:13:12.290
TensorRT is a
platform for Nvidia

00:13:12.290 --> 00:13:16.750
for optimized deep
learning inference.

00:13:16.750 --> 00:13:19.660
Your Chicago taxi example
that we've been using here

00:13:19.660 --> 00:13:21.820
probably wouldn't
benefit from this.

00:13:21.820 --> 00:13:25.213
But if you had, say, an image
recognition model, a ResNet,

00:13:25.213 --> 00:13:27.130
you could really get
some performance boosting

00:13:27.130 --> 00:13:30.650
and cost savings
by using TensorRT.

00:13:30.650 --> 00:13:33.140
We provide a command
line that allows

00:13:33.140 --> 00:13:36.770
you to convert the saved
model into a TensorRT

00:13:36.770 --> 00:13:38.240
optimized model.

00:13:38.240 --> 00:13:40.880
So then again, a very simple
change to that original command

00:13:40.880 --> 00:13:41.870
line.

00:13:41.870 --> 00:13:44.570
And you're running on
accelerated GPU hardware

00:13:44.570 --> 00:13:46.573
with TensorRT optimization.

00:13:49.680 --> 00:13:53.040
So to put it all
together again, we

00:13:53.040 --> 00:13:56.310
introduced TensorFlow
Extended or TFX.

00:13:56.310 --> 00:14:00.000
We showed you how the different
components that TFX consists of

00:14:00.000 --> 00:14:01.890
can work together
to help you manage

00:14:01.890 --> 00:14:05.830
the end-to-end lifecycle of
your machine learning pipeline.

00:14:05.830 --> 00:14:07.630
First, you have your data.

00:14:07.630 --> 00:14:09.630
And we have tools to help
you make sense of that

00:14:09.630 --> 00:14:13.360
and process it and
prepare it for training.

00:14:13.360 --> 00:14:16.240
We then support
training your model.

00:14:16.240 --> 00:14:17.750
And after you train
your model, we

00:14:17.750 --> 00:14:19.500
provide tools that
allow you to make sense

00:14:19.500 --> 00:14:21.750
of what you're seeing, of
what your model's doing,

00:14:21.750 --> 00:14:23.340
and to make improvements.

00:14:23.340 --> 00:14:27.120
Also, to make sure
that you don't regress.

00:14:27.120 --> 00:14:28.860
Then we have the
pusher that allows

00:14:28.860 --> 00:14:31.530
you to push to various
deployment options

00:14:31.530 --> 00:14:34.680
and make your model available to
serve users in the real world.

00:14:37.400 --> 00:14:39.200
To get started with
TensorFlow Extended,

00:14:39.200 --> 00:14:40.970
please visit us on GitHub.

00:14:40.970 --> 00:14:45.260
There is also more documentation
at TensorFlow.org/tfx.

00:14:45.260 --> 00:14:47.690
And some of my teammates are
running a workshop tomorrow.

00:14:47.690 --> 00:14:49.260
And they'd love
to see you there.

00:14:49.260 --> 00:14:50.880
You don't need to
bring a laptop.

00:14:50.880 --> 00:14:53.580
We have machines that are
set up and ready to go.

00:14:53.580 --> 00:14:55.847
And you can get some
hands-on experience

00:14:55.847 --> 00:14:56.930
using TensorFlow Extended.

00:14:56.930 --> 00:15:00.280
[MUSIC PLAYING]

