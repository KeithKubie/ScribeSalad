WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.419
 
hi and welcome back you've seen by now

00:00:02.419 --> 00:00:02.429
hi and welcome back you've seen by now
 

00:00:02.429 --> 00:00:04.460
hi and welcome back you've seen by now
that changin your net can involve

00:00:04.460 --> 00:00:04.470
that changin your net can involve
 

00:00:04.470 --> 00:00:06.410
that changin your net can involve
setting a lot of different hyper

00:00:06.410 --> 00:00:06.420
setting a lot of different hyper
 

00:00:06.420 --> 00:00:08.480
setting a lot of different hyper
parameters now how do you go about

00:00:08.480 --> 00:00:08.490
parameters now how do you go about
 

00:00:08.490 --> 00:00:10.190
parameters now how do you go about
finding a good setting for these hyper

00:00:10.190 --> 00:00:10.200
finding a good setting for these hyper
 

00:00:10.200 --> 00:00:12.350
finding a good setting for these hyper
parameters in this video I want to share

00:00:12.350 --> 00:00:12.360
parameters in this video I want to share
 

00:00:12.360 --> 00:00:14.780
parameters in this video I want to share
with you some guidelines some tips how

00:00:14.780 --> 00:00:14.790
with you some guidelines some tips how
 

00:00:14.790 --> 00:00:16.790
with you some guidelines some tips how
to systematically organize your hyper

00:00:16.790 --> 00:00:16.800
to systematically organize your hyper
 

00:00:16.800 --> 00:00:18.830
to systematically organize your hyper
parameter tuning process which hopefully

00:00:18.830 --> 00:00:18.840
parameter tuning process which hopefully
 

00:00:18.840 --> 00:00:20.540
parameter tuning process which hopefully
will make it more efficient for you to

00:00:20.540 --> 00:00:20.550
will make it more efficient for you to
 

00:00:20.550 --> 00:00:22.400
will make it more efficient for you to
converge on a good setting of the hyper

00:00:22.400 --> 00:00:22.410
converge on a good setting of the hyper
 

00:00:22.410 --> 00:00:24.650
converge on a good setting of the hyper
parameters one of the painful things

00:00:24.650 --> 00:00:24.660
parameters one of the painful things
 

00:00:24.660 --> 00:00:26.929
parameters one of the painful things
about training deep nets is the sheer

00:00:26.929 --> 00:00:26.939
about training deep nets is the sheer
 

00:00:26.939 --> 00:00:28.519
about training deep nets is the sheer
number of hyper parameters you have to

00:00:28.519 --> 00:00:28.529
number of hyper parameters you have to
 

00:00:28.529 --> 00:00:30.620
number of hyper parameters you have to
deal with ranging from the learning rate

00:00:30.620 --> 00:00:30.630
deal with ranging from the learning rate
 

00:00:30.630 --> 00:00:34.639
deal with ranging from the learning rate
alpha to the momentum term beta the

00:00:34.639 --> 00:00:34.649
alpha to the momentum term beta the
 

00:00:34.649 --> 00:00:37.790
alpha to the momentum term beta the
using momentum or the hyper parameters

00:00:37.790 --> 00:00:37.800
using momentum or the hyper parameters
 

00:00:37.800 --> 00:00:40.040
using momentum or the hyper parameters
for the atom optimization algorithm

00:00:40.040 --> 00:00:40.050
for the atom optimization algorithm
 

00:00:40.050 --> 00:00:43.630
for the atom optimization algorithm
which were beta 1 beta 2 and epsilon

00:00:43.630 --> 00:00:43.640
which were beta 1 beta 2 and epsilon
 

00:00:43.640 --> 00:00:46.420
which were beta 1 beta 2 and epsilon
maybe have to pick the number of layers

00:00:46.420 --> 00:00:46.430
maybe have to pick the number of layers
 

00:00:46.430 --> 00:00:48.619
maybe have to pick the number of layers
maybe have to pick the number of hidden

00:00:48.619 --> 00:00:48.629
maybe have to pick the number of hidden
 

00:00:48.629 --> 00:00:52.729
maybe have to pick the number of hidden
units for the different layers and maybe

00:00:52.729 --> 00:00:52.739
units for the different layers and maybe
 

00:00:52.739 --> 00:00:55.160
units for the different layers and maybe
you want to use learning rate decay so

00:00:55.160 --> 00:00:55.170
you want to use learning rate decay so
 

00:00:55.170 --> 00:00:58.910
you want to use learning rate decay so
you don't just use a single learning

00:00:58.910 --> 00:00:58.920
you don't just use a single learning
 

00:00:58.920 --> 00:01:01.369
you don't just use a single learning
rate alpha and then of course you might

00:01:01.369 --> 00:01:01.379
rate alpha and then of course you might
 

00:01:01.379 --> 00:01:06.679
rate alpha and then of course you might
need to choose the mini batch size so it

00:01:06.679 --> 00:01:06.689
need to choose the mini batch size so it
 

00:01:06.689 --> 00:01:08.450
need to choose the mini batch size so it
turns out some of these parameters are

00:01:08.450 --> 00:01:08.460
turns out some of these parameters are
 

00:01:08.460 --> 00:01:10.250
turns out some of these parameters are
more important than others for most

00:01:10.250 --> 00:01:10.260
more important than others for most
 

00:01:10.260 --> 00:01:12.590
more important than others for most
learning applications I would say alpha

00:01:12.590 --> 00:01:12.600
learning applications I would say alpha
 

00:01:12.600 --> 00:01:14.630
learning applications I would say alpha
the learning rate is the most important

00:01:14.630 --> 00:01:14.640
the learning rate is the most important
 

00:01:14.640 --> 00:01:17.120
the learning rate is the most important
hyper parameter to tune other than alpha

00:01:17.120 --> 00:01:17.130
hyper parameter to tune other than alpha
 

00:01:17.130 --> 00:01:19.330
hyper parameter to tune other than alpha
a few other high performances I tend to

00:01:19.330 --> 00:01:19.340
a few other high performances I tend to
 

00:01:19.340 --> 00:01:23.899
a few other high performances I tend to
but maybe to Nick's would be maybe the

00:01:23.899 --> 00:01:23.909
but maybe to Nick's would be maybe the
 

00:01:23.909 --> 00:01:26.810
but maybe to Nick's would be maybe the
momentum term I say is 0.9 is a good

00:01:26.810 --> 00:01:26.820
momentum term I say is 0.9 is a good
 

00:01:26.820 --> 00:01:30.020
momentum term I say is 0.9 is a good
default and also tune the mini batch

00:01:30.020 --> 00:01:30.030
default and also tune the mini batch
 

00:01:30.030 --> 00:01:32.330
default and also tune the mini batch
size to make sure that the optimization

00:01:32.330 --> 00:01:32.340
size to make sure that the optimization
 

00:01:32.340 --> 00:01:34.789
size to make sure that the optimization
algorithm is running efficiently often

00:01:34.789 --> 00:01:34.799
algorithm is running efficiently often
 

00:01:34.799 --> 00:01:37.460
algorithm is running efficiently often
also fit around the hidden units of the

00:01:37.460 --> 00:01:37.470
also fit around the hidden units of the
 

00:01:37.470 --> 00:01:39.710
also fit around the hidden units of the
ones I've circled in orange these are

00:01:39.710 --> 00:01:39.720
ones I've circled in orange these are
 

00:01:39.720 --> 00:01:41.740
ones I've circled in orange these are
really the three that would consider

00:01:41.740 --> 00:01:41.750
really the three that would consider
 

00:01:41.750 --> 00:01:44.060
really the three that would consider
second in importance to the learning

00:01:44.060 --> 00:01:44.070
second in importance to the learning
 

00:01:44.070 --> 00:01:46.550
second in importance to the learning
rate alpha and then third in importance

00:01:46.550 --> 00:01:46.560
rate alpha and then third in importance
 

00:01:46.560 --> 00:01:48.740
rate alpha and then third in importance
you know after sitting around the others

00:01:48.740 --> 00:01:48.750
you know after sitting around the others
 

00:01:48.750 --> 00:01:50.749
you know after sitting around the others
the number of layers can sometimes make

00:01:50.749 --> 00:01:50.759
the number of layers can sometimes make
 

00:01:50.759 --> 00:01:53.899
the number of layers can sometimes make
a huge difference and so can learning

00:01:53.899 --> 00:01:53.909
a huge difference and so can learning
 

00:01:53.909 --> 00:01:56.300
a huge difference and so can learning
rate decay and then when using the atom

00:01:56.300 --> 00:01:56.310
rate decay and then when using the atom
 

00:01:56.310 --> 00:01:58.190
rate decay and then when using the atom
algorithm I actually pretty much never

00:01:58.190 --> 00:01:58.200
algorithm I actually pretty much never
 

00:01:58.200 --> 00:02:00.590
algorithm I actually pretty much never
tune beta-1 beta-2 an epsilon pretty

00:02:00.590 --> 00:02:00.600
tune beta-1 beta-2 an epsilon pretty
 

00:02:00.600 --> 00:02:02.870
tune beta-1 beta-2 an epsilon pretty
much always used point nine point nine

00:02:02.870 --> 00:02:02.880
much always used point nine point nine
 

00:02:02.880 --> 00:02:05.080
much always used point nine point nine
nine nine and ten to the minus eight

00:02:05.080 --> 00:02:05.090
nine nine and ten to the minus eight
 

00:02:05.090 --> 00:02:07.429
nine nine and ten to the minus eight
although you can try tuning those as

00:02:07.429 --> 00:02:07.439
although you can try tuning those as
 

00:02:07.439 --> 00:02:09.559
although you can try tuning those as
well if you wish but hopefully does give

00:02:09.559 --> 00:02:09.569
well if you wish but hopefully does give
 

00:02:09.569 --> 00:02:11.990
well if you wish but hopefully does give
you some rough sense of what type of

00:02:11.990 --> 00:02:12.000
you some rough sense of what type of
 

00:02:12.000 --> 00:02:13.620
you some rough sense of what type of
parameters might be

00:02:13.620 --> 00:02:13.630
parameters might be
 

00:02:13.630 --> 00:02:17.010
parameters might be
more important than others alpha most

00:02:17.010 --> 00:02:17.020
more important than others alpha most
 

00:02:17.020 --> 00:02:18.800
more important than others alpha most
important for sure

00:02:18.800 --> 00:02:18.810
important for sure
 

00:02:18.810 --> 00:02:21.720
important for sure
follow maybe by the ones I've circled in

00:02:21.720 --> 00:02:21.730
follow maybe by the ones I've circled in
 

00:02:21.730 --> 00:02:23.730
follow maybe by the ones I've circled in
orange follow maybe by the ones I

00:02:23.730 --> 00:02:23.740
orange follow maybe by the ones I
 

00:02:23.740 --> 00:02:26.520
orange follow maybe by the ones I
circled in purple but this isn't a hard

00:02:26.520 --> 00:02:26.530
circled in purple but this isn't a hard
 

00:02:26.530 --> 00:02:28.290
circled in purple but this isn't a hard
and fast rule and I think other deep

00:02:28.290 --> 00:02:28.300
and fast rule and I think other deep
 

00:02:28.300 --> 00:02:30.810
and fast rule and I think other deep
learning practitioners may well disagree

00:02:30.810 --> 00:02:30.820
learning practitioners may well disagree
 

00:02:30.820 --> 00:02:32.640
learning practitioners may well disagree
with you all have different intuitions

00:02:32.640 --> 00:02:32.650
with you all have different intuitions
 

00:02:32.650 --> 00:02:35.250
with you all have different intuitions
on these now if you're trying to tune

00:02:35.250 --> 00:02:35.260
on these now if you're trying to tune
 

00:02:35.260 --> 00:02:37.710
on these now if you're trying to tune
some set of high preferences how do you

00:02:37.710 --> 00:02:37.720
some set of high preferences how do you
 

00:02:37.720 --> 00:02:40.170
some set of high preferences how do you
select the set of values to explore in

00:02:40.170 --> 00:02:40.180
select the set of values to explore in
 

00:02:40.180 --> 00:02:41.970
select the set of values to explore in
earlier generations of machine learning

00:02:41.970 --> 00:02:41.980
earlier generations of machine learning
 

00:02:41.980 --> 00:02:43.590
earlier generations of machine learning
algorithms if you had to hyper

00:02:43.590 --> 00:02:43.600
algorithms if you had to hyper
 

00:02:43.600 --> 00:02:45.780
algorithms if you had to hyper
parameters which I'm calling how to

00:02:45.780 --> 00:02:45.790
parameters which I'm calling how to
 

00:02:45.790 --> 00:02:47.190
parameters which I'm calling how to
prime to one and have a ground to to

00:02:47.190 --> 00:02:47.200
prime to one and have a ground to to
 

00:02:47.200 --> 00:02:50.190
prime to one and have a ground to to
here it was common practice to sample

00:02:50.190 --> 00:02:50.200
here it was common practice to sample
 

00:02:50.200 --> 00:02:53.550
here it was common practice to sample
the points you know in a grid like so

00:02:53.550 --> 00:02:53.560
the points you know in a grid like so
 

00:02:53.560 --> 00:02:59.130
the points you know in a grid like so
and systematically explore these values

00:02:59.130 --> 00:02:59.140
and systematically explore these values
 

00:02:59.140 --> 00:03:00.930
and systematically explore these values
here I'm placing down a five by five

00:03:00.930 --> 00:03:00.940
here I'm placing down a five by five
 

00:03:00.940 --> 00:03:03.600
here I'm placing down a five by five
grid in practice it could be more or

00:03:03.600 --> 00:03:03.610
grid in practice it could be more or
 

00:03:03.610 --> 00:03:05.820
grid in practice it could be more or
less than five five grid but you try out

00:03:05.820 --> 00:03:05.830
less than five five grid but you try out
 

00:03:05.830 --> 00:03:07.980
less than five five grid but you try out
in this example or twenty five points

00:03:07.980 --> 00:03:07.990
in this example or twenty five points
 

00:03:07.990 --> 00:03:10.560
in this example or twenty five points
and then you know pick whichever hyper

00:03:10.560 --> 00:03:10.570
and then you know pick whichever hyper
 

00:03:10.570 --> 00:03:13.380
and then you know pick whichever hyper
parameter works best and this practice

00:03:13.380 --> 00:03:13.390
parameter works best and this practice
 

00:03:13.390 --> 00:03:15.240
parameter works best and this practice
works okay when the number of

00:03:15.240 --> 00:03:15.250
works okay when the number of
 

00:03:15.250 --> 00:03:17.330
works okay when the number of
hyperparameters was relatively small

00:03:17.330 --> 00:03:17.340
hyperparameters was relatively small
 

00:03:17.340 --> 00:03:19.890
hyperparameters was relatively small
indeed learning what we tend to do and

00:03:19.890 --> 00:03:19.900
indeed learning what we tend to do and
 

00:03:19.900 --> 00:03:21.300
indeed learning what we tend to do and
what I recommend you do instead is

00:03:21.300 --> 00:03:21.310
what I recommend you do instead is
 

00:03:21.310 --> 00:03:24.540
what I recommend you do instead is
choose the points at random so go ahead

00:03:24.540 --> 00:03:24.550
choose the points at random so go ahead
 

00:03:24.550 --> 00:03:27.240
choose the points at random so go ahead
and you know choose maybe your same

00:03:27.240 --> 00:03:27.250
and you know choose maybe your same
 

00:03:27.250 --> 00:03:29.160
and you know choose maybe your same
number of points all right 25 points and

00:03:29.160 --> 00:03:29.170
number of points all right 25 points and
 

00:03:29.170 --> 00:03:31.710
number of points all right 25 points and
then try out the hyper parameters on

00:03:31.710 --> 00:03:31.720
then try out the hyper parameters on
 

00:03:31.720 --> 00:03:34.110
then try out the hyper parameters on
this randomly chosen set of points and

00:03:34.110 --> 00:03:34.120
this randomly chosen set of points and
 

00:03:34.120 --> 00:03:37.290
this randomly chosen set of points and
the reason you do that is that it's

00:03:37.290 --> 00:03:37.300
the reason you do that is that it's
 

00:03:37.300 --> 00:03:39.720
the reason you do that is that it's
difficult to know in advance which hyper

00:03:39.720 --> 00:03:39.730
difficult to know in advance which hyper
 

00:03:39.730 --> 00:03:41.460
difficult to know in advance which hyper
parameters are going to be the most

00:03:41.460 --> 00:03:41.470
parameters are going to be the most
 

00:03:41.470 --> 00:03:43.530
parameters are going to be the most
important for your problem and as you

00:03:43.530 --> 00:03:43.540
important for your problem and as you
 

00:03:43.540 --> 00:03:44.880
important for your problem and as you
saw in the previous slide some hyper

00:03:44.880 --> 00:03:44.890
saw in the previous slide some hyper
 

00:03:44.890 --> 00:03:46.199
saw in the previous slide some hyper
parameters are actually much more

00:03:46.199 --> 00:03:46.209
parameters are actually much more
 

00:03:46.209 --> 00:03:48.690
parameters are actually much more
important than others so to take an

00:03:48.690 --> 00:03:48.700
important than others so to take an
 

00:03:48.700 --> 00:03:50.400
important than others so to take an
example let's say hyper parameter one

00:03:50.400 --> 00:03:50.410
example let's say hyper parameter one
 

00:03:50.410 --> 00:03:52.800
example let's say hyper parameter one
turns out to be alpha the learning rate

00:03:52.800 --> 00:03:52.810
turns out to be alpha the learning rate
 

00:03:52.810 --> 00:03:55.530
turns out to be alpha the learning rate
and to take an extreme example let's say

00:03:55.530 --> 00:03:55.540
and to take an extreme example let's say
 

00:03:55.540 --> 00:03:58.590
and to take an extreme example let's say
that hyper parameter two was that value

00:03:58.590 --> 00:03:58.600
that hyper parameter two was that value
 

00:03:58.600 --> 00:04:00.210
that hyper parameter two was that value
epsilon that you have in the denominator

00:04:00.210 --> 00:04:00.220
epsilon that you have in the denominator
 

00:04:00.220 --> 00:04:03.449
epsilon that you have in the denominator
of the atom algorithm so your choice of

00:04:03.449 --> 00:04:03.459
of the atom algorithm so your choice of
 

00:04:03.459 --> 00:04:05.280
of the atom algorithm so your choice of
alpha matters a lot in your choice of

00:04:05.280 --> 00:04:05.290
alpha matters a lot in your choice of
 

00:04:05.290 --> 00:04:09.060
alpha matters a lot in your choice of
epsilon hardly matters so if you sample

00:04:09.060 --> 00:04:09.070
epsilon hardly matters so if you sample
 

00:04:09.070 --> 00:04:12.270
epsilon hardly matters so if you sample
in a grid then you've really tried out

00:04:12.270 --> 00:04:12.280
in a grid then you've really tried out
 

00:04:12.280 --> 00:04:17.039
in a grid then you've really tried out
five values of alpha and you might find

00:04:17.039 --> 00:04:17.049
five values of alpha and you might find
 

00:04:17.049 --> 00:04:18.510
five values of alpha and you might find
that all of the different values of

00:04:18.510 --> 00:04:18.520
that all of the different values of
 

00:04:18.520 --> 00:04:20.190
that all of the different values of
epsilon gives you essentially the same

00:04:20.190 --> 00:04:20.200
epsilon gives you essentially the same
 

00:04:20.200 --> 00:04:20.780
epsilon gives you essentially the same
answer

00:04:20.780 --> 00:04:20.790
answer
 

00:04:20.790 --> 00:04:24.290
answer
so you've now trained 25 models and only

00:04:24.290 --> 00:04:24.300
so you've now trained 25 models and only
 

00:04:24.300 --> 00:04:26.960
so you've now trained 25 models and only
got them to try out five values for the

00:04:26.960 --> 00:04:26.970
got them to try out five values for the
 

00:04:26.970 --> 00:04:28.400
got them to try out five values for the
learning rate alpha which is the thing

00:04:28.400 --> 00:04:28.410
learning rate alpha which is the thing
 

00:04:28.410 --> 00:04:30.140
learning rate alpha which is the thing
that's really important whereas in

00:04:30.140 --> 00:04:30.150
that's really important whereas in
 

00:04:30.150 --> 00:04:32.840
that's really important whereas in
contrast if you were to sample a random

00:04:32.840 --> 00:04:32.850
contrast if you were to sample a random
 

00:04:32.850 --> 00:04:35.710
contrast if you were to sample a random
then you know you all have tried out

00:04:35.710 --> 00:04:35.720
then you know you all have tried out
 

00:04:35.720 --> 00:04:38.060
then you know you all have tried out
twenty-five distinct values of the

00:04:38.060 --> 00:04:38.070
twenty-five distinct values of the
 

00:04:38.070 --> 00:04:40.070
twenty-five distinct values of the
learning rate alpha and therefore you'd

00:04:40.070 --> 00:04:40.080
learning rate alpha and therefore you'd
 

00:04:40.080 --> 00:04:42.320
learning rate alpha and therefore you'd
be more likely to find a value that

00:04:42.320 --> 00:04:42.330
be more likely to find a value that
 

00:04:42.330 --> 00:04:44.390
be more likely to find a value that
works really well I've explained this

00:04:44.390 --> 00:04:44.400
works really well I've explained this
 

00:04:44.400 --> 00:04:46.610
works really well I've explained this
example using just two hyper parameters

00:04:46.610 --> 00:04:46.620
example using just two hyper parameters
 

00:04:46.620 --> 00:04:48.680
example using just two hyper parameters
in practice you might be searching over

00:04:48.680 --> 00:04:48.690
in practice you might be searching over
 

00:04:48.690 --> 00:04:50.990
in practice you might be searching over
many more hyper parameters than this so

00:04:50.990 --> 00:04:51.000
many more hyper parameters than this so
 

00:04:51.000 --> 00:04:53.000
many more hyper parameters than this so
if you have safety hyper parameters I

00:04:53.000 --> 00:04:53.010
if you have safety hyper parameters I
 

00:04:53.010 --> 00:04:55.040
if you have safety hyper parameters I
guess instead searching over a square

00:04:55.040 --> 00:04:55.050
guess instead searching over a square
 

00:04:55.050 --> 00:04:57.020
guess instead searching over a square
you're searching over a cube where this

00:04:57.020 --> 00:04:57.030
you're searching over a cube where this
 

00:04:57.030 --> 00:05:00.050
you're searching over a cube where this
third dimension is hyper parameter three

00:05:00.050 --> 00:05:00.060
third dimension is hyper parameter three
 

00:05:00.060 --> 00:05:03.500
third dimension is hyper parameter three
and then by sampling within this you

00:05:03.500 --> 00:05:03.510
and then by sampling within this you
 

00:05:03.510 --> 00:05:05.330
and then by sampling within this you
know three dimensional tube you get to

00:05:05.330 --> 00:05:05.340
know three dimensional tube you get to
 

00:05:05.340 --> 00:05:07.250
know three dimensional tube you get to
try out a lot more values of each of

00:05:07.250 --> 00:05:07.260
try out a lot more values of each of
 

00:05:07.260 --> 00:05:09.530
try out a lot more values of each of
your three high parameters and in

00:05:09.530 --> 00:05:09.540
your three high parameters and in
 

00:05:09.540 --> 00:05:11.540
your three high parameters and in
practice you might be searching over

00:05:11.540 --> 00:05:11.550
practice you might be searching over
 

00:05:11.550 --> 00:05:13.430
practice you might be searching over
even more hyper parameters than three

00:05:13.430 --> 00:05:13.440
even more hyper parameters than three
 

00:05:13.440 --> 00:05:15.260
even more hyper parameters than three
and sometimes it's just hard to know in

00:05:15.260 --> 00:05:15.270
and sometimes it's just hard to know in
 

00:05:15.270 --> 00:05:17.240
and sometimes it's just hard to know in
advance which ones turn out to be the

00:05:17.240 --> 00:05:17.250
advance which ones turn out to be the
 

00:05:17.250 --> 00:05:19.070
advance which ones turn out to be the
really important high parameters for

00:05:19.070 --> 00:05:19.080
really important high parameters for
 

00:05:19.080 --> 00:05:21.680
really important high parameters for
your application and something random

00:05:21.680 --> 00:05:21.690
your application and something random
 

00:05:21.690 --> 00:05:23.570
your application and something random
rather than in a grid and it shows that

00:05:23.570 --> 00:05:23.580
rather than in a grid and it shows that
 

00:05:23.580 --> 00:05:27.110
rather than in a grid and it shows that
you're more richly exploring the pot set

00:05:27.110 --> 00:05:27.120
you're more richly exploring the pot set
 

00:05:27.120 --> 00:05:28.490
you're more richly exploring the pot set
of possible values for the most

00:05:28.490 --> 00:05:28.500
of possible values for the most
 

00:05:28.500 --> 00:05:30.200
of possible values for the most
important hyper parameters whether they

00:05:30.200 --> 00:05:30.210
important hyper parameters whether they
 

00:05:30.210 --> 00:05:32.150
important hyper parameters whether they
turn out to be when you stand for hyper

00:05:32.150 --> 00:05:32.160
turn out to be when you stand for hyper
 

00:05:32.160 --> 00:05:34.850
turn out to be when you stand for hyper
parameters another common practice is to

00:05:34.850 --> 00:05:34.860
parameters another common practice is to
 

00:05:34.860 --> 00:05:38.060
parameters another common practice is to
use a course to find something scheme so

00:05:38.060 --> 00:05:38.070
use a course to find something scheme so
 

00:05:38.070 --> 00:05:39.290
use a course to find something scheme so
let's say in this two-dimensional

00:05:39.290 --> 00:05:39.300
let's say in this two-dimensional
 

00:05:39.300 --> 00:05:41.990
let's say in this two-dimensional
example that you've sampled these points

00:05:41.990 --> 00:05:42.000
example that you've sampled these points
 

00:05:42.000 --> 00:05:44.960
example that you've sampled these points
and maybe you found that this point work

00:05:44.960 --> 00:05:44.970
and maybe you found that this point work
 

00:05:44.970 --> 00:05:46.580
and maybe you found that this point work
the best it may be a few other points

00:05:46.580 --> 00:05:46.590
the best it may be a few other points
 

00:05:46.590 --> 00:05:48.440
the best it may be a few other points
around it tended to work really well

00:05:48.440 --> 00:05:48.450
around it tended to work really well
 

00:05:48.450 --> 00:05:51.440
around it tended to work really well
then in the course defined scheme what

00:05:51.440 --> 00:05:51.450
then in the course defined scheme what
 

00:05:51.450 --> 00:05:53.930
then in the course defined scheme what
you might do is zoom in to a smaller

00:05:53.930 --> 00:05:53.940
you might do is zoom in to a smaller
 

00:05:53.940 --> 00:05:56.720
you might do is zoom in to a smaller
region of the hyper parameters and then

00:05:56.720 --> 00:05:56.730
region of the hyper parameters and then
 

00:05:56.730 --> 00:06:00.740
region of the hyper parameters and then
sample more densely within this space or

00:06:00.740 --> 00:06:00.750
sample more densely within this space or
 

00:06:00.750 --> 00:06:03.980
sample more densely within this space or
maybe a gain at random but to then focus

00:06:03.980 --> 00:06:03.990
maybe a gain at random but to then focus
 

00:06:03.990 --> 00:06:07.210
maybe a gain at random but to then focus
more resources on searching within this

00:06:07.210 --> 00:06:07.220
more resources on searching within this
 

00:06:07.220 --> 00:06:09.830
more resources on searching within this
blue square if you're suspecting that

00:06:09.830 --> 00:06:09.840
blue square if you're suspecting that
 

00:06:09.840 --> 00:06:12.110
blue square if you're suspecting that
the best setting of the hyper parameters

00:06:12.110 --> 00:06:12.120
the best setting of the hyper parameters
 

00:06:12.120 --> 00:06:14.570
the best setting of the hyper parameters
may be in this region so after doing a

00:06:14.570 --> 00:06:14.580
may be in this region so after doing a
 

00:06:14.580 --> 00:06:18.680
may be in this region so after doing a
core sample of this entire square that

00:06:18.680 --> 00:06:18.690
core sample of this entire square that
 

00:06:18.690 --> 00:06:21.080
core sample of this entire square that
tells you to then focus on on a smaller

00:06:21.080 --> 00:06:21.090
tells you to then focus on on a smaller
 

00:06:21.090 --> 00:06:24.050
tells you to then focus on on a smaller
square you can then stop pull more

00:06:24.050 --> 00:06:24.060
square you can then stop pull more
 

00:06:24.060 --> 00:06:26.420
square you can then stop pull more
densely in this smallest square so this

00:06:26.420 --> 00:06:26.430
densely in this smallest square so this
 

00:06:26.430 --> 00:06:28.610
densely in this smallest square so this
type of a course to find search is also

00:06:28.610 --> 00:06:28.620
type of a course to find search is also
 

00:06:28.620 --> 00:06:30.830
type of a course to find search is also
frequently use and by trying out these

00:06:30.830 --> 00:06:30.840
frequently use and by trying out these
 

00:06:30.840 --> 00:06:32.750
frequently use and by trying out these
different values of the high parameters

00:06:32.750 --> 00:06:32.760
different values of the high parameters
 

00:06:32.760 --> 00:06:33.480
different values of the high parameters
you can then

00:06:33.480 --> 00:06:33.490
you can then
 

00:06:33.490 --> 00:06:36.120
you can then
pick whatever value allows you to do

00:06:36.120 --> 00:06:36.130
pick whatever value allows you to do
 

00:06:36.130 --> 00:06:38.520
pick whatever value allows you to do
best on your training set objective or

00:06:38.520 --> 00:06:38.530
best on your training set objective or
 

00:06:38.530 --> 00:06:41.100
best on your training set objective or
does best on your development sets or

00:06:41.100 --> 00:06:41.110
does best on your development sets or
 

00:06:41.110 --> 00:06:44.460
does best on your development sets or
whatever you're trying to optimize in

00:06:44.460 --> 00:06:44.470
whatever you're trying to optimize in
 

00:06:44.470 --> 00:06:46.800
whatever you're trying to optimize in
your hyper parameter search process so

00:06:46.800 --> 00:06:46.810
your hyper parameter search process so
 

00:06:46.810 --> 00:06:48.510
your hyper parameter search process so
hope this gives you a way to more

00:06:48.510 --> 00:06:48.520
hope this gives you a way to more
 

00:06:48.520 --> 00:06:50.159
hope this gives you a way to more
systematically organize your hyper

00:06:50.159 --> 00:06:50.169
systematically organize your hyper
 

00:06:50.169 --> 00:06:52.170
systematically organize your hyper
parameter search process the two key

00:06:52.170 --> 00:06:52.180
parameter search process the two key
 

00:06:52.180 --> 00:06:54.659
parameter search process the two key
takeaways are used random something not

00:06:54.659 --> 00:06:54.669
takeaways are used random something not
 

00:06:54.669 --> 00:06:57.629
takeaways are used random something not
a grid search and consider optionally

00:06:57.629 --> 00:06:57.639
a grid search and consider optionally
 

00:06:57.639 --> 00:06:59.760
a grid search and consider optionally
but consider implementing a course

00:06:59.760 --> 00:06:59.770
but consider implementing a course
 

00:06:59.770 --> 00:07:02.249
but consider implementing a course
defined search process but there's even

00:07:02.249 --> 00:07:02.259
defined search process but there's even
 

00:07:02.259 --> 00:07:04.350
defined search process but there's even
more to hyper parameter search than this

00:07:04.350 --> 00:07:04.360
more to hyper parameter search than this
 

00:07:04.360 --> 00:07:06.300
more to hyper parameter search than this
let's talk more in the next video about

00:07:06.300 --> 00:07:06.310
let's talk more in the next video about
 

00:07:06.310 --> 00:07:08.310
let's talk more in the next video about
how to choose the right scale on which

00:07:08.310 --> 00:07:08.320
how to choose the right scale on which
 

00:07:08.320 --> 00:07:11.759
how to choose the right scale on which
to sample your hyper parameters

