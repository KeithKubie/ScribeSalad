WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.626
[MUSIC PLAYING]

00:00:03.626 --> 00:00:04.875
CHRIS LEARY: Hello, everybody.

00:00:04.875 --> 00:00:06.160
I'm Chris Leary.

00:00:06.160 --> 00:00:08.200
I'm a compiler
engineer at Google.

00:00:08.200 --> 00:00:11.860
And I'm here today to tell you
that we can compile TensorFlow,

00:00:11.860 --> 00:00:13.390
which is super exciting.

00:00:13.390 --> 00:00:17.320
And we have documentation about
how you can compile TensorFlow.

00:00:17.320 --> 00:00:19.450
And we have open
source documentation

00:00:19.450 --> 00:00:21.820
about the open source
compiler that you can compile

00:00:21.820 --> 00:00:23.440
open source TensorFlow with.

00:00:23.440 --> 00:00:25.960
So as the ancient
proverb says, "It

00:00:25.960 --> 00:00:28.084
takes a village to
raise a compiler."

00:00:28.084 --> 00:00:29.500
And many people
have been involved

00:00:29.500 --> 00:00:31.120
in the development of XLA.

00:00:31.120 --> 00:00:33.130
I'm up here speaking about XLA.

00:00:33.130 --> 00:00:34.859
But we've had a lot
of contributions.

00:00:34.859 --> 00:00:37.150
And we look forward to a lot
more community involvement

00:00:37.150 --> 00:00:40.964
and community contributions now
that it's in the open source.

00:00:40.964 --> 00:00:42.880
So I wanted to start off
by talking about some

00:00:42.880 --> 00:00:45.370
of the key strengths
of TensorFlow.

00:00:45.370 --> 00:00:48.674
It's flexible, it's expressive,
and it's extensible.

00:00:48.674 --> 00:00:50.590
And we've heard about
some of those attributes

00:00:50.590 --> 00:00:52.760
in the keynote.

00:00:52.760 --> 00:00:54.890
So how is this
accomplished in TensorFlow?

00:00:54.890 --> 00:00:58.460
The flexibility is due in part
to its interpreted nature.

00:00:58.460 --> 00:00:59.720
It's in the name, TensorFlow.

00:00:59.720 --> 00:01:02.300
It uses this data flow
programming model.

00:01:02.300 --> 00:01:05.540
So what the program does,
you hand it a graph,

00:01:05.540 --> 00:01:07.810
it finds a node that's ready
to execute, it grabs it,

00:01:07.810 --> 00:01:10.700
it runs it, and then a bunch
more nodes become ready.

00:01:10.700 --> 00:01:13.340
And then it goes and grabs
those nodes and executes them.

00:01:13.340 --> 00:01:16.340
So that's the interpreter loop
at the heart of TensorFlow.

00:01:16.340 --> 00:01:17.150
It's dynamic.

00:01:17.150 --> 00:01:20.390
And this is owed in part
to its Pythonic heritage.

00:01:20.390 --> 00:01:22.296
You get to use your
full expressiveness

00:01:22.296 --> 00:01:23.420
when you define your graph.

00:01:23.420 --> 00:01:25.970
And you're not constrained by
some external entity that tells

00:01:25.970 --> 00:01:28.370
you what you can
and cannot express.

00:01:28.370 --> 00:01:29.120
And it's stateful.

00:01:29.120 --> 00:01:30.770
So you get to program
using variables

00:01:30.770 --> 00:01:33.980
like you're used to programming
in other environments.

00:01:33.980 --> 00:01:35.960
And it's "Black-Box" modular.

00:01:35.960 --> 00:01:39.350
So you can come up with
a new dataflow operation

00:01:39.350 --> 00:01:41.990
and plug it into TensorFlow
that the original TensorFlow

00:01:41.990 --> 00:01:44.890
designers had never
even thought of.

00:01:44.890 --> 00:01:47.150
So the question is how, do
we take all those strengths

00:01:47.150 --> 00:01:48.870
and add more speed?

00:01:48.870 --> 00:01:50.600
So this is a tank
with jet engines

00:01:50.600 --> 00:01:52.045
bolted on the top of it.

00:01:52.045 --> 00:01:53.420
I thought that
really exemplified

00:01:53.420 --> 00:01:57.140
what we're going for with
this kind of approach.

00:01:57.140 --> 00:01:58.670
So the way that
we accomplish this

00:01:58.670 --> 00:02:01.490
is with just-in-time
compilation via XLA,

00:02:01.490 --> 00:02:03.692
which stands for
Accelerated Linear Algebra.

00:02:03.692 --> 00:02:05.150
And this is compiler
infrastructure

00:02:05.150 --> 00:02:07.550
that we've developed, so that
you can take a TensorFlow

00:02:07.550 --> 00:02:11.330
graph in and spit out optimized
and specialized assembly

00:02:11.330 --> 00:02:12.890
for that graph.

00:02:12.890 --> 00:02:14.930
And to show you how
this works, we're

00:02:14.930 --> 00:02:18.970
going to start with a
shell, as programmers love.

00:02:18.970 --> 00:02:22.400
So if we start
this video, you'll

00:02:22.400 --> 00:02:24.980
see the TensorFlow shell,
as you can build and run

00:02:24.980 --> 00:02:27.420
in open source.

00:02:27.420 --> 00:02:29.630
And we're passing it a flag
to say, spit out the XLA

00:02:29.630 --> 00:02:31.230
assembly that's produced.

00:02:31.230 --> 00:02:33.556
So we're piecing in a
very simple expression.

00:02:33.556 --> 00:02:35.930
So we're actually just taking
four floating point numbers

00:02:35.930 --> 00:02:37.280
and multiplying them.

00:02:37.280 --> 00:02:39.710
And we're putting
this explicitly

00:02:39.710 --> 00:02:41.450
onto the XLA CPU device.

00:02:41.450 --> 00:02:44.270
So the compiler is exposed
as a device in one mode

00:02:44.270 --> 00:02:45.680
inside of TensorFlow.

00:02:45.680 --> 00:02:51.860
So when we press Enter on
this expression and pause,

00:02:51.860 --> 00:02:54.620
then we'll actually see just a
couple of assembly instructions

00:02:54.620 --> 00:02:55.490
get emitted.

00:02:55.490 --> 00:02:58.025
And some people in the audience
are like, oh, of course,

00:02:58.025 --> 00:02:59.540
I know that assembly.

00:02:59.540 --> 00:03:01.760
But the point of
this assembly is

00:03:01.760 --> 00:03:03.650
there are no loops,
because we knew

00:03:03.650 --> 00:03:05.810
that there were exactly
four elements that you're

00:03:05.810 --> 00:03:07.490
going to multiply in the graph.

00:03:07.490 --> 00:03:09.500
So it actually emitted
the one instruction

00:03:09.500 --> 00:03:12.680
that you need in this
Vector Extended Architecture

00:03:12.680 --> 00:03:15.590
to implement that expression
on that CPU device.

00:03:15.590 --> 00:03:18.050
So optimized and
specialized for the graph

00:03:18.050 --> 00:03:20.630
that you fed in with your
TensorFlow expression.

00:03:20.630 --> 00:03:22.790
And if we continue
on, we'll also

00:03:22.790 --> 00:03:26.540
see that we can place that same
expression onto a GPU device

00:03:26.540 --> 00:03:32.420
and compile some GPU assembly
code on the XLA_GPU device that

00:03:32.420 --> 00:03:35.210
corresponds to what we
generated on the CPU side.

00:03:35.210 --> 00:03:39.420
So as we scroll up,
the GPU assembly

00:03:39.420 --> 00:03:40.890
has a little bit
more boilerplate,

00:03:40.890 --> 00:03:42.556
because it's launching
multiple threads,

00:03:42.556 --> 00:03:44.380
and threads have to
figure out what to do.

00:03:44.380 --> 00:03:46.170
But this number
4 here is saying,

00:03:46.170 --> 00:03:48.180
I am launching four GPU threads.

00:03:48.180 --> 00:03:49.680
And I am specialized
for the fact

00:03:49.680 --> 00:03:51.840
that that is the
exact expression

00:03:51.840 --> 00:03:53.580
that you typed into TensorFlow.

00:03:53.580 --> 00:03:56.250
So this shows that we understand
what expression you give us

00:03:56.250 --> 00:03:59.070
at runtime in the
TensorFlow graph

00:03:59.070 --> 00:04:02.222
and then compile based
on it for maximum speed.

00:04:02.222 --> 00:04:03.930
So if we switch back
to the presentation.

00:04:08.650 --> 00:04:09.560
Thank you.

00:04:09.560 --> 00:04:13.010
So we see that XLA
for CPU and GPU

00:04:13.010 --> 00:04:15.260
works in standard
TensorFlow shell,

00:04:15.260 --> 00:04:19.120
as you would use it
in a day to day basis.

00:04:19.120 --> 00:04:21.909
So the missing ingredient
to all this is you.

00:04:21.909 --> 00:04:23.890
We need you to turn it on.

00:04:23.890 --> 00:04:27.490
So as you go and
build TensorFlow,

00:04:27.490 --> 00:04:29.350
we really encourage
you to turn this

00:04:29.350 --> 00:04:33.130
on, see how it performs for your
use case, file GitHub issues,

00:04:33.130 --> 00:04:34.840
and tell us how you
think compilation

00:04:34.840 --> 00:04:38.007
could benefit the way
that you use TensorFlow.

00:04:38.007 --> 00:04:40.090
We really need more
involvement from the community

00:04:40.090 --> 00:04:42.160
to understand where
to prioritize efforts,

00:04:42.160 --> 00:04:44.743
because there's a lot of things
you could do with compilation.

00:04:44.743 --> 00:04:47.080
There's a lot of potential
avenues you can go down.

00:04:47.080 --> 00:04:48.970
So I've said
just-in-time compiler.

00:04:48.970 --> 00:04:51.700
So what's this just-in-time
stuff all about?

00:04:51.700 --> 00:04:54.970
The essence of just-in-time is
that the program to be compiled

00:04:54.970 --> 00:04:56.750
is built at runtime.

00:04:56.750 --> 00:04:59.875
So when I type in a TensorFlow
expression and press Enter,

00:04:59.875 --> 00:05:02.500
I don't want to sit there at the
command line for a few minutes

00:05:02.500 --> 00:05:04.180
while you figure out
how to compile it.

00:05:04.180 --> 00:05:06.940
So we need to have low
overhead compilation.

00:05:06.940 --> 00:05:10.600
In the blink of an eye, the
assembly has to be generated.

00:05:10.600 --> 00:05:13.615
And another aspect of it
is that in ML research,

00:05:13.615 --> 00:05:15.490
you don't want to say
what your batch size is

00:05:15.490 --> 00:05:17.020
at the very beginning of time.

00:05:17.020 --> 00:05:19.030
You want to be able to
say the batch size right

00:05:19.030 --> 00:05:20.890
when you figure out
what you want it to be.

00:05:20.890 --> 00:05:23.840
So variables can bind
very late in the process.

00:05:23.840 --> 00:05:26.200
You could say your batch
size at the very last second.

00:05:26.200 --> 00:05:29.500
And because it's a JIT compiler,
it will compile it at the point

00:05:29.500 --> 00:05:31.360
that it's defined.

00:05:31.360 --> 00:05:33.910
So you get to prototype with
the normal freedom of TensorFlow

00:05:33.910 --> 00:05:38.410
development is the gist of
this just-in-time compilation.

00:05:38.410 --> 00:05:40.900
So if we look at the
TensorFlow level block diagram,

00:05:40.900 --> 00:05:43.300
we have the existing
TensorFlow core.

00:05:43.300 --> 00:05:45.580
And Todd is going to be
talking about this in more

00:05:45.580 --> 00:05:47.470
detail in his next talk.

00:05:47.470 --> 00:05:50.080
But XLA slots into the
TensorFlow ecosystem

00:05:50.080 --> 00:05:53.320
here on the bottom right,
where we have some auto JIT

00:05:53.320 --> 00:05:55.450
capabilities, where we'll
find clusters to JIT

00:05:55.450 --> 00:05:59.260
for you or you can place things
explicitly on devices, as we

00:05:59.260 --> 00:06:02.460
showed in that demo video.

00:06:02.460 --> 00:06:04.620
So what we think we've
developed with XLA

00:06:04.620 --> 00:06:07.150
is a set of
complementary attributes.

00:06:07.150 --> 00:06:10.380
So the flexibility that we
get through interpretation,

00:06:10.380 --> 00:06:12.690
we get the optimization
benefits of compilation

00:06:12.690 --> 00:06:13.920
on the flip side.

00:06:13.920 --> 00:06:17.310
The dynamism that lets you
write very expressive programs,

00:06:17.310 --> 00:06:20.430
because we find out all of these
dimensions at the last second

00:06:20.430 --> 00:06:22.950
and then specialize for
them, we get the benefits

00:06:22.950 --> 00:06:25.320
of having the static
information about all the bounds

00:06:25.320 --> 00:06:27.720
of the tensors in your graph.

00:06:27.720 --> 00:06:30.900
We use pure semantics to
make it easier to compile.

00:06:30.900 --> 00:06:32.640
So TensorFlow
actually handles all

00:06:32.640 --> 00:06:35.410
of the external
variable rebindings

00:06:35.410 --> 00:06:39.100
after the compiled
executable is run,

00:06:39.100 --> 00:06:41.400
which makes a smaller
surface area for compilation

00:06:41.400 --> 00:06:44.640
and makes it easier to target,
say, new exotic devices.

00:06:44.640 --> 00:06:47.700
And you're able to build up
many of the TensorFlow core

00:06:47.700 --> 00:06:50.760
operations from these primitives
that we know how to compile.

00:06:50.760 --> 00:06:52.290
So you get the
optimization benefits

00:06:52.290 --> 00:06:53.960
of all of the things
on the right-hand side,

00:06:53.960 --> 00:06:56.460
and you get to think and write
in the traditional way that's

00:06:56.460 --> 00:06:58.660
reflected on the left-hand side.

00:06:58.660 --> 00:07:00.870
So what has us excited
about this whole approach?

00:07:00.870 --> 00:07:02.730
Server-side speed ups, for one.

00:07:02.730 --> 00:07:05.100
So through this JIT
compilation and specialization

00:07:05.100 --> 00:07:08.010
that we've talked about, we
see model-shaped benchmark wins

00:07:08.010 --> 00:07:09.030
of up to 60%.

00:07:09.030 --> 00:07:10.710
So those are models
as you would run

00:07:10.710 --> 00:07:13.990
in the data center and
model-looking microbenchmarks

00:07:13.990 --> 00:07:16.770
roughly in that speed up range.

00:07:16.770 --> 00:07:18.780
And for one extreme
in-house example,

00:07:18.780 --> 00:07:21.750
we saw SyntaxNet get
latency reductions

00:07:21.750 --> 00:07:24.870
from around 200 microseconds
to 5 microseconds.

00:07:24.870 --> 00:07:26.640
And the reason for
this was SyntaxNet

00:07:26.640 --> 00:07:29.040
had a lot of little
operations in the graph,

00:07:29.040 --> 00:07:32.300
so the interpreter has to go
grab each little operation.

00:07:32.300 --> 00:07:35.430
And that process of going and
grabbing the little operations

00:07:35.430 --> 00:07:38.255
and running them incurs
some latency overhead.

00:07:38.255 --> 00:07:39.630
And by compiling,
you're actually

00:07:39.630 --> 00:07:43.745
able to eliminate all that
latency overhead away.

00:07:43.745 --> 00:07:45.120
Another thing that
has us excited

00:07:45.120 --> 00:07:47.280
are mobile footprint reductions.

00:07:47.280 --> 00:07:49.620
So with our ahead-of-time
compilation--

00:07:49.620 --> 00:07:51.770
we talked about
just-in-time compilation,

00:07:51.770 --> 00:07:54.480
but you can also go
through a build process

00:07:54.480 --> 00:07:58.500
to turn models into executables
if you want to do that upfront.

00:07:58.500 --> 00:08:00.180
So this is just an
executable like you

00:08:00.180 --> 00:08:02.130
would run on the
command line, and it's

00:08:02.130 --> 00:08:05.370
able to eliminate much
of the TensorFlow runtime

00:08:05.370 --> 00:08:07.500
and really reduce
the binary size.

00:08:07.500 --> 00:08:10.170
And this lets you also
cross-compile these models

00:08:10.170 --> 00:08:14.140
for ARM, PowerPC, and
x86 CPU platforms.

00:08:14.140 --> 00:08:16.030
So one example that
we have in-house was

00:08:16.030 --> 00:08:19.930
an LSTM model for mobile was
able to go from 2.6 megabytes

00:08:19.930 --> 00:08:21.370
to less than 600
kilobytes, which

00:08:21.370 --> 00:08:25.850
was a 4x reduction in
the deployment footprint.

00:08:25.850 --> 00:08:28.310
The general thing that has us
excited about this approach

00:08:28.310 --> 00:08:31.550
is that analyzing a whole
graph, a whole program

00:08:31.550 --> 00:08:34.030
is made easy by this
compiler infrastructure.

00:08:34.030 --> 00:08:36.530
So we have this thing called
the High-Level Optimizer that's

00:08:36.530 --> 00:08:39.740
able to look at a linear
algebra level graph

00:08:39.740 --> 00:08:42.950
and create a reusable toolkit
of global optimizations

00:08:42.950 --> 00:08:44.058
to work on it.

00:08:44.058 --> 00:08:47.960
So even though you compile for
different platforms, CPU, GPU,

00:08:47.960 --> 00:08:50.750
and other exotic devices,
we've parameterized

00:08:50.750 --> 00:08:52.640
out the things that
are specific to a given

00:08:52.640 --> 00:08:57.320
platform at this high-level
optimization toolkit level.

00:08:57.320 --> 00:08:59.030
And so you're able
to mix and match

00:08:59.030 --> 00:09:02.150
the platform-agnostic and
the target-specific pieces,

00:09:02.150 --> 00:09:04.160
so that if there's some
optimization that really

00:09:04.160 --> 00:09:08.780
benefits, say, GPUs, you're able
to use that for targeting GPUs,

00:09:08.780 --> 00:09:12.140
but you're able to use the
general set of optimizations

00:09:12.140 --> 00:09:16.017
in the toolkit for targeting
all the other platforms.

00:09:16.017 --> 00:09:17.100
So there are caveats here.

00:09:17.100 --> 00:09:17.850
It's experimental.

00:09:17.850 --> 00:09:19.470
It's still early days.

00:09:19.470 --> 00:09:21.990
Not all TensorFlow
operations compile.

00:09:21.990 --> 00:09:25.200
And some of this is because very
dynamic operations, that it's

00:09:25.200 --> 00:09:27.900
very difficult to
describe the type for,

00:09:27.900 --> 00:09:29.070
won't compile by design.

00:09:29.070 --> 00:09:32.190
So for example, dynamic stitch
has a very difficult type

00:09:32.190 --> 00:09:33.900
to define for its operations.

00:09:33.900 --> 00:09:36.930
So that is not something that
XLA supports and it may never

00:09:36.930 --> 00:09:38.370
support it.

00:09:38.370 --> 00:09:40.620
The performance is
improving on a daily basis.

00:09:40.620 --> 00:09:42.360
So not everything is faster.

00:09:42.360 --> 00:09:44.630
And we haven't been able
to devote equal time to all

00:09:44.630 --> 00:09:45.880
the platforms that we support.

00:09:45.880 --> 00:09:48.700
But with the community's
involvement and prioritization,

00:09:48.700 --> 00:09:50.887
we believe that we
could do much more.

00:09:50.887 --> 00:09:51.720
So it's open source.

00:09:51.720 --> 00:09:54.410
So really, we want you
to try it out, file bugs,

00:09:54.410 --> 00:09:56.160
and let us know how
this compilation could

00:09:56.160 --> 00:09:59.280
benefit your workload.

00:09:59.280 --> 00:10:02.880
So to recap, to go over the
benefits of compilation,

00:10:02.880 --> 00:10:06.480
we're able to specialize
the code for the computation

00:10:06.480 --> 00:10:08.370
that you handed to the compiler.

00:10:08.370 --> 00:10:12.330
So we're able to eliminate
operation dispatch overhead.

00:10:12.330 --> 00:10:14.700
We're able to fuse
operations together.

00:10:14.700 --> 00:10:17.540
So normally, if you take
one operation at a time,

00:10:17.540 --> 00:10:20.700
you'll pull in all the data
from memory, operate on it,

00:10:20.700 --> 00:10:23.820
push it back out to memory, and
the next operation will come,

00:10:23.820 --> 00:10:25.890
pull it back in from
memory, operate on it,

00:10:25.890 --> 00:10:27.510
and spit it back out.

00:10:27.510 --> 00:10:29.670
But when you have a
compiler in the mix,

00:10:29.670 --> 00:10:32.280
you're actually able to pull
all the memory in one time,

00:10:32.280 --> 00:10:34.200
operate on it with
all the operations

00:10:34.200 --> 00:10:36.270
that you want to do at
a time, and then flush

00:10:36.270 --> 00:10:37.440
it back out to memory.

00:10:37.440 --> 00:10:38.640
So this is called fusion.

00:10:38.640 --> 00:10:40.860
So we're able to avoid
these expensive round

00:10:40.860 --> 00:10:42.630
trips to memory, which
can benefit power

00:10:42.630 --> 00:10:45.470
on mobile platforms as well.

00:10:45.470 --> 00:10:48.570
We're able to perform precise
and global buffer analysis.

00:10:48.570 --> 00:10:51.520
So really, this means, because
we can look at the whole graph

00:10:51.520 --> 00:10:54.010
and how all of the tensors
are flowing between all

00:10:54.010 --> 00:10:57.670
the operations, we're able to
make compile time trade-offs

00:10:57.670 --> 00:10:59.890
between memory usage and speed.

00:10:59.890 --> 00:11:02.440
So you might have additional
concurrency that comes,

00:11:02.440 --> 00:11:04.570
but it might be at the
cost of additional memory.

00:11:04.570 --> 00:11:05.986
So if you were
memory constrained,

00:11:05.986 --> 00:11:08.800
you may want to reduce
the peak memory usage.

00:11:08.800 --> 00:11:11.230
And we're able to, in
the code generator,

00:11:11.230 --> 00:11:14.320
unroll and vectorize, which
are some key optimizations

00:11:14.320 --> 00:11:17.390
for today's modern platforms,
via these known dimensions

00:11:17.390 --> 00:11:17.890
that we get.

00:11:17.890 --> 00:11:20.410
So we know that the tensor
bound for batch size

00:11:20.410 --> 00:11:22.210
is a particular value.

00:11:22.210 --> 00:11:23.590
And by specializing
for it, we're

00:11:23.590 --> 00:11:26.560
able to unlock a whole
category of optimizations

00:11:26.560 --> 00:11:29.410
that modern CPUs and
GPUs and other platforms

00:11:29.410 --> 00:11:32.410
are designed to run
most effectively.

00:11:32.410 --> 00:11:35.545
And as we noted, you get
much smaller executables

00:11:35.545 --> 00:11:38.080
out the back, because you
generate exactly what you

00:11:38.080 --> 00:11:41.600
need for this specific graph.

00:11:41.600 --> 00:11:43.700
So how does this
work under the hood?

00:11:43.700 --> 00:11:46.340
Fundamentally, an
XLA program is a set

00:11:46.340 --> 00:11:49.550
of static, decomposed
TensorFlow operations.

00:11:49.550 --> 00:11:52.340
So they're math-looking
primitive operations

00:11:52.340 --> 00:11:55.430
that you compose to
make macro operations.

00:11:55.430 --> 00:11:58.400
So this is able to support
many neural net definitions

00:11:58.400 --> 00:12:00.350
that we've seen
in the wild today.

00:12:00.350 --> 00:12:02.370
So this is a classic
TensorFlow example.

00:12:02.370 --> 00:12:04.070
You may have seen
this graph before.

00:12:04.070 --> 00:12:07.420
So MatMul and Add, I
know those from math.

00:12:07.420 --> 00:12:09.320
Relu is just a
fancy way of saying

00:12:09.320 --> 00:12:11.990
max of zero and some value.

00:12:11.990 --> 00:12:14.450
But then softmax, that's
kind of interesting one.

00:12:14.450 --> 00:12:16.520
That one's not like
the other ones.

00:12:16.520 --> 00:12:20.990
So softmax is implemented,
hand-coded in C++ in vanilla

00:12:20.990 --> 00:12:22.760
TensorFlow today.

00:12:22.760 --> 00:12:25.820
And the question is, why
can't we compose softmax out

00:12:25.820 --> 00:12:27.380
of the other
TensorFlow operations

00:12:27.380 --> 00:12:29.260
if they support the definition?

00:12:29.260 --> 00:12:31.120
And one of the
answers that we hear

00:12:31.120 --> 00:12:33.290
is because people don't
want to pay a performance

00:12:33.290 --> 00:12:34.970
penalty for doing that.

00:12:34.970 --> 00:12:38.540
But what if operation
composition had the performance

00:12:38.540 --> 00:12:41.030
of the hand-coded C++?

00:12:41.030 --> 00:12:44.120
So if you look at the kind of
stuff that softmax has inside,

00:12:44.120 --> 00:12:47.900
it has dots and adds
and reduces and EXPs.

00:12:47.900 --> 00:12:50.210
These are all
math-like operations.

00:12:50.210 --> 00:12:52.460
And actually, what
the TensorFlow to XLA

00:12:52.460 --> 00:12:55.640
bridge, that Todd
will talk about, does

00:12:55.640 --> 00:12:57.860
is this built in
operation decomposition,

00:12:57.860 --> 00:13:00.080
taking the macro
operation and emitting

00:13:00.080 --> 00:13:04.580
several micro operations that we
know how to compile out of it.

00:13:04.580 --> 00:13:07.090
So with the primitive
operation composition,

00:13:07.090 --> 00:13:10.150
we're still able to generate
the fused and optimized

00:13:10.150 --> 00:13:12.600
composite kernel, like you
would have written by hand.

00:13:16.260 --> 00:13:18.890
So note that this is all
expressible in TensorFlow.

00:13:18.890 --> 00:13:21.330
All those operations that we
showed in the softmax slide

00:13:21.330 --> 00:13:24.080
are all primitive
TensorFlow operations,

00:13:24.080 --> 00:13:26.660
but XLA removes the
performance concern

00:13:26.660 --> 00:13:29.420
of composing these TensorFlow
operations together.

00:13:29.420 --> 00:13:33.260
So we're also able to avoid
combinatorial explosions

00:13:33.260 --> 00:13:35.150
of custom operation fusions.

00:13:35.150 --> 00:13:37.130
For example, we see this
a lot in ML research

00:13:37.130 --> 00:13:40.580
with LSTM cells, where
you would technically

00:13:40.580 --> 00:13:44.240
have to specialize for every
macro operation, for every set

00:13:44.240 --> 00:13:46.700
of primitives inside
the macro operation,

00:13:46.700 --> 00:13:49.320
for every dimension
size, for every backend,

00:13:49.320 --> 00:13:52.190
CPU, GPU, different
platforms, for every device

00:13:52.190 --> 00:13:53.570
within that platform.

00:13:53.570 --> 00:13:56.720
But the JIT is able
to look at what you've

00:13:56.720 --> 00:14:00.597
provided to it in the graph,
fuse it in a dynamic way,

00:14:00.597 --> 00:14:02.930
and then produce the optimized
assembly for the platform

00:14:02.930 --> 00:14:04.850
that you're running on.

00:14:04.850 --> 00:14:07.760
We've also tried to make
it a minimal surface area,

00:14:07.760 --> 00:14:11.030
so that exotic devices
can slot into this XLA

00:14:11.030 --> 00:14:12.020
set of primitives.

00:14:12.020 --> 00:14:16.730
So if you have a LLVM
pipeline and a runtime plug-in

00:14:16.730 --> 00:14:18.890
that we call
StreamExecutor, then you

00:14:18.890 --> 00:14:22.690
can actually slot into XLA
today as a potential option,

00:14:22.690 --> 00:14:24.500
because the CPU
and GPU back ends

00:14:24.500 --> 00:14:28.700
are effectively using
these two throughout

00:14:28.700 --> 00:14:32.270
So we think that XLA is useful
throughout the development

00:14:32.270 --> 00:14:33.350
lifecycle.

00:14:33.350 --> 00:14:34.780
So you get to use
JIT compilation

00:14:34.780 --> 00:14:36.220
when you're prototyping.

00:14:36.220 --> 00:14:38.590
You get to use compilation
caching as you scale.

00:14:38.590 --> 00:14:40.647
So when people request
things from the server,

00:14:40.647 --> 00:14:41.980
you're not compiling every time.

00:14:41.980 --> 00:14:44.920
You're hitting in a compilation
cache that says, ah, here's

00:14:44.920 --> 00:14:47.710
the specialized and
optimized assembly code.

00:14:47.710 --> 00:14:50.170
And you're able to use
ahead-of-time compilation

00:14:50.170 --> 00:14:53.710
for mobile and embedded devices,
and for latency minimization

00:14:53.710 --> 00:14:55.310
on those platforms.

00:14:55.310 --> 00:14:57.250
So you get to
control and observe

00:14:57.250 --> 00:14:59.110
static properties
of the program.

00:14:59.110 --> 00:15:00.910
Especially on mobile
and embedded devices,

00:15:00.910 --> 00:15:02.590
this can be important,
for example,

00:15:02.590 --> 00:15:04.871
for reducing peak memory usage.

00:15:04.871 --> 00:15:06.370
And so now I'm going
to turn it over

00:15:06.370 --> 00:15:10.480
to Todd, who works on XLA as
a member of the Google Brain

00:15:10.480 --> 00:15:14.400
team, to deep dive on XLA
in the TensorFlow ecosystem.

00:15:14.400 --> 00:15:14.950
So Todd?

00:15:20.710 --> 00:15:23.290
TODD WANG: Thanks, Chris.

00:15:23.290 --> 00:15:23.830
Let's see.

00:15:23.830 --> 00:15:26.230
Oh, so maybe-- well,
I'm an engineer, right?

00:15:26.230 --> 00:15:30.141
So like when Zach had
mentioned to me that, oh,

00:15:30.141 --> 00:15:32.140
would you be willing to
present some of the work

00:15:32.140 --> 00:15:35.380
that you've been working on
with a bunch of other people,

00:15:35.380 --> 00:15:36.400
I said, yeah, sure.

00:15:36.400 --> 00:15:38.650
But I always felt like
in the back of my mind

00:15:38.650 --> 00:15:41.590
whether there would
be some fear of his

00:15:41.590 --> 00:15:43.970
that maybe I'd come up here
to show this one slide.

00:15:43.970 --> 00:15:46.360
This is the
documentation for how

00:15:46.360 --> 00:15:48.294
TensorFlow integrates with XLA.

00:15:48.294 --> 00:15:49.960
It'd be like, well,
just read the slide.

00:15:49.960 --> 00:15:51.930
Like, we'll go along
with it together.

00:15:51.930 --> 00:15:53.692
But no, no, I'm not
going to do that.

00:15:53.692 --> 00:15:55.150
But I want to take
this opportunity

00:15:55.150 --> 00:15:58.000
to introduce this
symbol here, right?

00:15:58.000 --> 00:16:00.022
So what is this symbol?

00:16:00.022 --> 00:16:01.480
So everyone from
the '90s remembers

00:16:01.480 --> 00:16:03.910
that this symbol meant
my website isn't really

00:16:03.910 --> 00:16:04.930
working, right?

00:16:04.930 --> 00:16:06.670
And so what's the
significance here?

00:16:06.670 --> 00:16:09.520
Well, there's a reason why we
sort of call it experimental.

00:16:09.520 --> 00:16:11.470
And on some of
the documentation,

00:16:11.470 --> 00:16:13.185
we sort of mentioned
it like this, right?

00:16:13.185 --> 00:16:15.310
And it's not because we
don't really believe in it.

00:16:15.310 --> 00:16:17.370
In fact, it's the opposite.

00:16:17.370 --> 00:16:19.820
We think that the approach
is actually really strong.

00:16:19.820 --> 00:16:22.360
And we think that the
challenges that we face

00:16:22.360 --> 00:16:23.402
are sort of surmountable.

00:16:23.402 --> 00:16:25.485
The results that we're
getting, the early results,

00:16:25.485 --> 00:16:27.830
are really good, but we're
definitely not done yet.

00:16:27.830 --> 00:16:31.000
And so that's why I wanted to
present it to you guys now.

00:16:31.000 --> 00:16:35.140
And so I'm going
to phrase this talk

00:16:35.140 --> 00:16:36.610
in terms of the
actual open source

00:16:36.610 --> 00:16:37.912
directories for the code.

00:16:37.912 --> 00:16:39.370
There's four
directories down here.

00:16:39.370 --> 00:16:40.869
And I'm going to
go through each one

00:16:40.869 --> 00:16:43.100
to describe what's
actually inside of them.

00:16:43.100 --> 00:16:45.160
And so the first one is XLA.

00:16:45.160 --> 00:16:47.830
And so unsurprisingly, this
is sort of the compiler

00:16:47.830 --> 00:16:50.410
that Chris was
just talking about.

00:16:50.410 --> 00:16:52.810
As a refresher or maybe
sort of to describe it

00:16:52.810 --> 00:16:55.360
at another level of
detail, right, like,

00:16:55.360 --> 00:16:58.010
here's XLA in one picture, OK?

00:16:58.010 --> 00:17:00.196
And so this is generally
how compilers work, right?

00:17:00.196 --> 00:17:01.570
When you talk to
compiler people,

00:17:01.570 --> 00:17:04.450
it's like, well, you've got some
representations for the program

00:17:04.450 --> 00:17:05.230
that you want.

00:17:05.230 --> 00:17:07.510
And in the end, you want
to generate machine code.

00:17:07.510 --> 00:17:08.973
That's what the thing does.

00:17:08.973 --> 00:17:10.348
And in the middle,
it's like, you

00:17:10.348 --> 00:17:13.000
go through these phases of,
like, rewriting the graphs

00:17:13.000 --> 00:17:15.640
or, like, lowering the graphs,
different representations.

00:17:15.640 --> 00:17:17.618
But at the end of the
day, that's kind of it.

00:17:17.618 --> 00:17:20.470
And so on the left here,
we have the XLA graph.

00:17:20.470 --> 00:17:22.480
And what this is
composed of, the nodes

00:17:22.480 --> 00:17:25.868
are linear algebra
operations and the edges

00:17:25.868 --> 00:17:28.480
are sort of the flow
of data, like tensors,

00:17:28.480 --> 00:17:29.930
through these operations.

00:17:29.930 --> 00:17:32.150
So that's pretty
straightforward.

00:17:32.150 --> 00:17:35.590
We lower that to LLVM
IR in the middle here.

00:17:35.590 --> 00:17:38.740
Now, LLVM is an open
source compiler toolkit

00:17:38.740 --> 00:17:41.180
sort of framework
that's pretty popular.

00:17:41.180 --> 00:17:43.750
IR's just an abbreviation
for its intermediate

00:17:43.750 --> 00:17:44.785
representation.

00:17:44.785 --> 00:17:46.660
So we've basically
lowered the representation

00:17:46.660 --> 00:17:48.852
from sort of linear
algebra type stuff

00:17:48.852 --> 00:17:50.560
to kind of assembly
type stuff, you know,

00:17:50.560 --> 00:17:52.120
general assembly stuff.

00:17:52.120 --> 00:17:54.460
And so LLVM then
takes that and does

00:17:54.460 --> 00:17:56.950
co-generation, where we
co-generate for actual machine

00:17:56.950 --> 00:17:58.270
platforms.

00:17:58.270 --> 00:18:00.760
The examples I've given
here, there is x86,

00:18:00.760 --> 00:18:03.760
so that supports, like,
servers or your workstations,

00:18:03.760 --> 00:18:05.800
ARM, right, for
your mobile phones,

00:18:05.800 --> 00:18:08.447
or even, like, PTX,
so this is for GPUs.

00:18:08.447 --> 00:18:10.030
So we have a sort
of pretty wide range

00:18:10.030 --> 00:18:12.030
of support for the
actual platforms

00:18:12.030 --> 00:18:13.480
that we can run this code on.

00:18:13.480 --> 00:18:15.274
And so the takeaway,
the thing you

00:18:15.274 --> 00:18:16.690
want to remember
from this picture

00:18:16.690 --> 00:18:18.299
is that, like, it's
just a compiler,

00:18:18.299 --> 00:18:20.590
it's sort of like taking
things from one representation

00:18:20.590 --> 00:18:22.420
and putting it down to another.

00:18:22.420 --> 00:18:24.160
In our context,
the important thing

00:18:24.160 --> 00:18:26.920
is that as long as we can
get things into an XLA graph,

00:18:26.920 --> 00:18:30.600
then out pops
machine code for us.

00:18:30.600 --> 00:18:33.560
So the next directory
under this compiler

00:18:33.560 --> 00:18:36.300
subdirectory is TF to XLA.

00:18:36.300 --> 00:18:37.760
And these, literally,
by the way,

00:18:37.760 --> 00:18:39.260
are the real directory
names, right?

00:18:39.260 --> 00:18:40.980
We're not trying to
hide anything here.

00:18:40.980 --> 00:18:41.480
And

00:18:41.480 --> 00:18:43.830
So TF to XLA, sort
of unsurprisingly,

00:18:43.830 --> 00:18:47.940
it's the stuff that turns
TensorFlow into XLA.

00:18:47.940 --> 00:18:50.640
And so similar to that
previous picture for XLA,

00:18:50.640 --> 00:18:54.240
here's one picture that sort of
describes what TensorFlow is.

00:18:54.240 --> 00:18:57.480
And so on the left, we've got
all the different programming

00:18:57.480 --> 00:19:01.560
languages that people can use to
build their TensorFlow program,

00:19:01.560 --> 00:19:02.190
right?

00:19:02.190 --> 00:19:05.357
And Python is sort of
like the most widely

00:19:05.357 --> 00:19:07.440
used, probably, right now,
but there's many others

00:19:07.440 --> 00:19:08.371
that we do support.

00:19:08.371 --> 00:19:09.870
And what you're
doing when you build

00:19:09.870 --> 00:19:13.320
that program is that you're
constructing this graph,

00:19:13.320 --> 00:19:14.427
the TensorFlow graph.

00:19:14.427 --> 00:19:16.260
And this is the graph
of all your operations

00:19:16.260 --> 00:19:19.820
and all of the tensors
flowing through them, OK?

00:19:19.820 --> 00:19:22.470
And then we take that graph,
and as Rajat mentioned,

00:19:22.470 --> 00:19:26.180
we have a C++ runtime that
actually executes this.

00:19:26.180 --> 00:19:28.200
And so there's two
components here.

00:19:28.200 --> 00:19:30.870
One is called the executor
and the other is the kernels.

00:19:30.870 --> 00:19:34.040
So the job of the executor
is to actually walk

00:19:34.040 --> 00:19:36.680
through this graph, and
when it hits each node,

00:19:36.680 --> 00:19:39.890
it looks up in the kernels and
finds the appropriate kernel

00:19:39.890 --> 00:19:40.970
and actually runs it.

00:19:40.970 --> 00:19:43.490
And so as an example, I've
labeled two of the graph nodes

00:19:43.490 --> 00:19:45.740
here, like, add and softmax.

00:19:45.740 --> 00:19:47.420
Add is just adding two things.

00:19:47.420 --> 00:19:50.700
And softmax, well, Chris
just described that.

00:19:50.700 --> 00:19:52.346
And so for add,
you think about it,

00:19:52.346 --> 00:19:53.720
like, the executor's
going to hit

00:19:53.720 --> 00:19:57.470
this add node and need to add
the two inputs that come in.

00:19:57.470 --> 00:19:59.810
And so it does that by
looking up inside the kernels,

00:19:59.810 --> 00:20:02.285
and finding the C++ fast
implementation for it

00:20:02.285 --> 00:20:04.770
for the device that you're
on, and then actually performs

00:20:04.770 --> 00:20:05.870
the addition.

00:20:05.870 --> 00:20:07.970
Similar for softmax.

00:20:07.970 --> 00:20:09.410
We hit the softmax node.

00:20:09.410 --> 00:20:10.490
We look up in the kernel.

00:20:10.490 --> 00:20:12.698
This one happens to be a
little bit more complicated.

00:20:12.698 --> 00:20:15.740
It's not just a simple one
arithmetic operation type

00:20:15.740 --> 00:20:16.340
thing.

00:20:16.340 --> 00:20:18.530
But still, it looks
it up, runs it.

00:20:18.530 --> 00:20:21.350
And so that's basically how
TensorFlow kind of works today.

00:20:21.350 --> 00:20:24.494
This is without XLA.

00:20:24.494 --> 00:20:25.910
So the next slide
is going to show

00:20:25.910 --> 00:20:28.340
sort of how we take
TensorFlow and turn this

00:20:28.340 --> 00:20:30.022
into an XLA program.

00:20:30.022 --> 00:20:32.480
And this one may be-- it's kind
of a lot of stuff going on,

00:20:32.480 --> 00:20:34.220
so I'll walk it through.

00:20:34.220 --> 00:20:36.467
On the left, we've got
the TensorFlow graph.

00:20:36.467 --> 00:20:38.050
This is that same
thing I just showed.

00:20:38.050 --> 00:20:42.000
And it has the add and
softmax labels, OK?

00:20:42.000 --> 00:20:43.800
On the right, what
we're trying to generate

00:20:43.800 --> 00:20:46.380
is we're trying to
generate that XLA graph.

00:20:46.380 --> 00:20:48.114
Remember from that
first slide about,

00:20:48.114 --> 00:20:49.530
well, as long as
we can get things

00:20:49.530 --> 00:20:51.222
into this representation,
then XLA's

00:20:51.222 --> 00:20:52.680
going to take care
of all the rest.

00:20:52.680 --> 00:20:56.260
It's going to compile the
thing down to machine code.

00:20:56.260 --> 00:20:59.262
And so the way we do that, we
have a local executor, which

00:20:59.262 --> 00:21:00.720
is basically the
same thing that we

00:21:00.720 --> 00:21:03.180
had for the normal
TensorFlow runtime,

00:21:03.180 --> 00:21:05.410
but we swapped out the kernels.

00:21:05.410 --> 00:21:07.680
So remember, in the
previous picture,

00:21:07.680 --> 00:21:09.120
this was an orange box.

00:21:09.120 --> 00:21:11.850
Oh, by the way, so orange
means sort of TensorFlow

00:21:11.850 --> 00:21:14.610
to keep with the theme and
everything, and blue means

00:21:14.610 --> 00:21:16.940
XLA in all my slides.

00:21:16.940 --> 00:21:18.740
So we swapped out
that orange box,

00:21:18.740 --> 00:21:20.740
right, with the kernels,
the TensorFlow kernels,

00:21:20.740 --> 00:21:23.280
and we swapped in the XLA ones.

00:21:23.280 --> 00:21:26.070
And the job of these kernels,
instead of actually executing

00:21:26.070 --> 00:21:28.940
the operation, it's going to
be to create this graph, right?

00:21:28.940 --> 00:21:32.340
To take the TensorFlow graph
and turn it into the XLA graph.

00:21:32.340 --> 00:21:36.014
And so an example here for
add, well, we hit the add node

00:21:36.014 --> 00:21:37.680
and we say, OK, we
need to do something.

00:21:37.680 --> 00:21:39.096
We look up the
kernel and it says,

00:21:39.096 --> 00:21:41.960
great, I know how to transform
the TensorFlow add into the XLA

00:21:41.960 --> 00:21:42.564
add.

00:21:42.564 --> 00:21:44.730
In this particular example,
it's sort of one to one.

00:21:44.730 --> 00:21:47.540
This was a really simple case.

00:21:47.540 --> 00:21:50.240
So Chris mentioned softmax,
how it's sort of this more

00:21:50.240 --> 00:21:52.105
composite operation, right?

00:21:52.105 --> 00:21:53.480
It's built up of
a bunch of stuff

00:21:53.480 --> 00:21:56.210
inside, like dot
and exponentiation

00:21:56.210 --> 00:21:57.540
and stuff like that.

00:21:57.540 --> 00:21:59.450
And so this graph
here is showing

00:21:59.450 --> 00:22:02.560
that softmax and the TensorFlow
graph is just one op.

00:22:02.560 --> 00:22:06.110
And we look up the
translation kernel

00:22:06.110 --> 00:22:09.162
and we turn that into
multiple ops in the XLA graph.

00:22:09.162 --> 00:22:11.120
And that's basically all
there is to it, right?

00:22:11.120 --> 00:22:15.320
This is how we transform
TensorFlow graphs into XLA.

00:22:15.320 --> 00:22:17.476
We have a link
here that describes

00:22:17.476 --> 00:22:19.100
where all the kernels
are that actually

00:22:19.100 --> 00:22:20.607
perform this translation.

00:22:20.607 --> 00:22:22.190
And here's the first
point where we're

00:22:22.190 --> 00:22:24.452
going to see this
awesome men at work.

00:22:24.452 --> 00:22:26.410
I kind of wish it was a
animated GIF, you know?

00:22:26.410 --> 00:22:27.910
Because that'd be
more to the point.

00:22:27.910 --> 00:22:32.390
But this men at work, basically,
the kernels that we have,

00:22:32.390 --> 00:22:33.830
they're not all there, right?

00:22:33.830 --> 00:22:35.990
Like, we haven't implemented
every TensorFlow op.

00:22:35.990 --> 00:22:38.336
And this is a place
where for some of them,

00:22:38.336 --> 00:22:39.710
they're actually
quite difficult.

00:22:39.710 --> 00:22:41.570
And that's the reason we
haven't implemented them.

00:22:41.570 --> 00:22:43.028
And maybe, as Chris
was mentioning,

00:22:43.028 --> 00:22:45.320
some operations really just
don't translate that well,

00:22:45.320 --> 00:22:47.300
because of their
very dynamic nature.

00:22:47.300 --> 00:22:50.169
For other ones, we just haven't
gotten around to it, really.

00:22:50.169 --> 00:22:52.460
And so this is a place where
actually community support

00:22:52.460 --> 00:22:55.560
would be really useful.

00:22:55.560 --> 00:22:57.430
OK, so we've sort of
covered what XLA is

00:22:57.430 --> 00:23:01.177
and we've covered how we
translate TensorFlow into XLA.

00:23:01.177 --> 00:23:02.760
So the next two
points are going to be

00:23:02.760 --> 00:23:06.570
about how we actually
use this cool compiler

00:23:06.570 --> 00:23:07.320
inside TensorFlow.

00:23:07.320 --> 00:23:09.194
And the first thing
we're going to talk about

00:23:09.194 --> 00:23:11.770
is JIT, which is, like, the
just-in-time compilation.

00:23:16.560 --> 00:23:20.610
And so in the theme of, like,
sort of graph transformations,

00:23:20.610 --> 00:23:23.430
I'm going to describe JIT
as sort of this graph, how

00:23:23.430 --> 00:23:24.870
the graph changes.

00:23:24.870 --> 00:23:28.980
And so on the left, we have
a normal TensorFlow graph.

00:23:28.980 --> 00:23:33.426
And in this red trapezoid,
we've identified three nodes.

00:23:33.426 --> 00:23:35.175
And these three nodes
are going to be sort

00:23:35.175 --> 00:23:38.250
of able to be compiled by XLA.

00:23:38.250 --> 00:23:41.340
The other nodes in the graph are
not, for the purposes of this,

00:23:41.340 --> 00:23:42.534
going to be compiled by XLA.

00:23:42.534 --> 00:23:43.950
And there's many
reasons for that.

00:23:43.950 --> 00:23:47.610
We said, sometimes operations
just aren't implemented.

00:23:47.610 --> 00:23:50.370
Or other times we've chosen
the clustering that just

00:23:50.370 --> 00:23:52.140
happens not to cluster them in.

00:23:52.140 --> 00:23:54.223
And so this example, we've
chosen these particular

00:23:54.223 --> 00:23:56.630
or these three nodes.

00:23:56.630 --> 00:23:58.880
On the right, we see that
we've taken the graph

00:23:58.880 --> 00:24:02.360
and we've rewritten it, so that
the same nodes outside of it

00:24:02.360 --> 00:24:04.110
are still there,
but on the inside,

00:24:04.110 --> 00:24:06.080
we've taken that
cluster or that subgraph

00:24:06.080 --> 00:24:08.720
and put it inside this
sort of pink bubble, OK?

00:24:08.720 --> 00:24:10.850
And now when you think
about how TensorFlow,

00:24:10.850 --> 00:24:14.074
like, the runtime works, it's
working exactly the same.

00:24:14.074 --> 00:24:15.740
It's going off of
this right-hand graph,

00:24:15.740 --> 00:24:17.960
where it's going
to visit each node,

00:24:17.960 --> 00:24:20.510
and it's going to look up
the kernel for the operation

00:24:20.510 --> 00:24:23.530
that it represents, and
it's going execute it.

00:24:23.530 --> 00:24:25.212
When it hits this
pink bubble, that's

00:24:25.212 --> 00:24:27.170
where some of the special
stuff happens, right?

00:24:27.170 --> 00:24:30.100
We have inside of it the
notion that this subgraph still

00:24:30.100 --> 00:24:30.817
exists.

00:24:30.817 --> 00:24:32.150
And we're going to compile that.

00:24:32.150 --> 00:24:34.580
We're going to transform that
graph into the XLA graph,

00:24:34.580 --> 00:24:37.460
and transform that into
actual machine code,

00:24:37.460 --> 00:24:39.500
and then execute that.

00:24:39.500 --> 00:24:41.660
This is also where that
caching occurs, right,

00:24:41.660 --> 00:24:45.200
where if you happen to hit
that same subgraph again,

00:24:45.200 --> 00:24:47.210
so if you run the same
graph multiple times,

00:24:47.210 --> 00:24:50.240
or within your program even, if
you have the same subgraph that

00:24:50.240 --> 00:24:51.740
shows up multiple
times, we're not

00:24:51.740 --> 00:24:54.164
going to pay the cost of
compilation multiple times.

00:24:54.164 --> 00:24:56.330
We're going to just compile
that once and execute it

00:24:56.330 --> 00:24:57.163
over and over again.

00:25:00.325 --> 00:25:01.950
So in this picture,
we're going to show

00:25:01.950 --> 00:25:06.186
some of the more
standard sort of ways

00:25:06.186 --> 00:25:07.560
that you show up
in these graphs.

00:25:07.560 --> 00:25:09.435
Like, the previous one
is kind of simplified.

00:25:09.435 --> 00:25:12.380
Now, this one's obviously
also very simplified,

00:25:12.380 --> 00:25:14.760
but the main point here
that we're trying to show

00:25:14.760 --> 00:25:17.340
is that a single
TensorFlow graph

00:25:17.340 --> 00:25:20.550
can have multiple such
clusters show up, right?

00:25:20.550 --> 00:25:23.130
And in this example, we've
got two of them on the left.

00:25:23.130 --> 00:25:24.870
We've got this blue
rectangle and we've

00:25:24.870 --> 00:25:27.870
got that same sort
of red trapezoid.

00:25:27.870 --> 00:25:30.480
So the clustering phase is
going to go and identify

00:25:30.480 --> 00:25:33.120
which clusters are
going to be compiled.

00:25:33.120 --> 00:25:35.594
And on the right side, we've
done the exact same thing.

00:25:35.594 --> 00:25:38.010
We've turned those two things
into their respective little

00:25:38.010 --> 00:25:38.730
bubbles.

00:25:38.730 --> 00:25:41.850
And when the TensorFlow
executor gets to these nodes,

00:25:41.850 --> 00:25:44.220
we're going to compile and
run them with the caching

00:25:44.220 --> 00:25:45.316
and everything.

00:25:47.777 --> 00:25:49.860
So in this slide, we're
going to talk a little bit

00:25:49.860 --> 00:25:53.620
about one of the sort of
challenges that shows up.

00:25:53.620 --> 00:25:57.036
You might think, well, that was
sort of pretty straightforward.

00:25:57.036 --> 00:25:58.410
So here's a
particular challenge.

00:25:58.410 --> 00:26:00.576
I mean, it's not, like,
super difficult or anything,

00:26:00.576 --> 00:26:02.869
but it's kind of
non-trivial at scale.

00:26:02.869 --> 00:26:04.410
So this particular
one, we're showing

00:26:04.410 --> 00:26:07.980
a graph where we picked
a clustering that

00:26:07.980 --> 00:26:13.451
happens to be pretty bad Yeah,
in fact, it's bad clustering.

00:26:13.451 --> 00:26:15.700
Now, you might want to think
a little bit about, well,

00:26:15.700 --> 00:26:18.730
what exactly about
this clustering is bad?

00:26:18.730 --> 00:26:20.830
If we happened to pick
this clustering, like,

00:26:20.830 --> 00:26:22.649
what would actually go wrong?

00:26:22.649 --> 00:26:24.190
And so the problem
is evident sort of

00:26:24.190 --> 00:26:26.080
on the right-hand
side, where we've

00:26:26.080 --> 00:26:31.840
created a cycle between the
non-JITed nodes and the JITed

00:26:31.840 --> 00:26:32.770
nodes.

00:26:32.770 --> 00:26:35.166
And the issue here is that
just the execution engine,

00:26:35.166 --> 00:26:37.290
it's going to have no way
to determine, like, well,

00:26:37.290 --> 00:26:38.480
which one do I run first?

00:26:38.480 --> 00:26:40.864
How do I actually
execute this graph?

00:26:40.864 --> 00:26:42.280
And so this is
obviously something

00:26:42.280 --> 00:26:44.800
that we need to avoid while
we're doing the JIT process.

00:26:44.800 --> 00:26:48.670
This particular example, it's
not, like, super difficult,

00:26:48.670 --> 00:26:50.350
but I mean, as we
get to larger graphs,

00:26:50.350 --> 00:26:51.580
this actually becomes
a little bit more

00:26:51.580 --> 00:26:52.990
challenging and non-trivial.

00:26:52.990 --> 00:26:55.510
So this is an example, a
flavor of the concrete things

00:26:55.510 --> 00:26:59.570
that the JIT level is
actually performing for you.

00:26:59.570 --> 00:27:02.210
So in order to turn on
the JIT compilation,

00:27:02.210 --> 00:27:04.434
we've got two main
ways right now.

00:27:04.434 --> 00:27:06.350
And so I'll walkthrough
through the first one.

00:27:06.350 --> 00:27:07.940
That's in this blue
box on the top.

00:27:07.940 --> 00:27:09.770
It's called Whole
session, right?

00:27:09.770 --> 00:27:12.290
And we've really tried
to make it super simple.

00:27:12.290 --> 00:27:14.620
And so what this is, ha,
I say, like, we really

00:27:14.620 --> 00:27:16.620
try to make it super
simple and then you've got,

00:27:16.620 --> 00:27:19.910
like, this super long.

00:27:19.910 --> 00:27:22.400
OK but I mean, the concept
is actually super simple.

00:27:22.400 --> 00:27:25.304
Never mind the long,
like, label, right?

00:27:25.304 --> 00:27:26.720
Conceptually, what
you're doing is

00:27:26.720 --> 00:27:28.190
you've got this
configuration that you're

00:27:28.190 --> 00:27:30.200
going to pass into to
run your session, right,

00:27:30.200 --> 00:27:32.120
to create your session.

00:27:32.120 --> 00:27:34.310
And all you need to do is
set this one flag on it

00:27:34.310 --> 00:27:36.830
to say, like, turn on the JIT.

00:27:36.830 --> 00:27:38.229
And there you go.

00:27:38.229 --> 00:27:39.770
What that's going
to do is it's going

00:27:39.770 --> 00:27:41.311
to take your entire
graph, it's going

00:27:41.311 --> 00:27:43.400
to determine which
clusters of the graph

00:27:43.400 --> 00:27:45.170
can actually be
compiled, and it's

00:27:45.170 --> 00:27:47.822
going to go and rewrite
the graph into that.

00:27:47.822 --> 00:27:49.280
And then when you
actually execute,

00:27:49.280 --> 00:27:51.290
it's going to, like, every
time we hit that node, it's

00:27:51.290 --> 00:27:53.060
going to perform that
JIT process for you

00:27:53.060 --> 00:27:55.240
and actually compile
down to machine code.

00:27:55.240 --> 00:27:58.747
And so the concept here is
that you flip this one flag on

00:27:58.747 --> 00:28:00.580
and you get, like,
automatic benefit, right?

00:28:00.580 --> 00:28:02.746
Things just get
magically faster for you.

00:28:02.746 --> 00:28:06.950
So the second way that we
can actually turn on JIT

00:28:06.950 --> 00:28:07.880
compilation.

00:28:07.880 --> 00:28:09.470
This one's a little
bit more explicit.

00:28:09.470 --> 00:28:11.390
We call it, like,
manually scoped, right?

00:28:11.390 --> 00:28:14.630
And the idea here is that
here you take your program,

00:28:14.630 --> 00:28:16.880
and instead of just
turning on this one flag,

00:28:16.880 --> 00:28:20.420
you explicitly put scopes inside
of your program where you can

00:28:20.420 --> 00:28:24.350
manually control exactly what
gets compiled and what doesn't.

00:28:24.350 --> 00:28:26.870
And one of the ideas
here is that, well, you

00:28:26.870 --> 00:28:30.200
might know due to the particular
model or graph that you have,

00:28:30.200 --> 00:28:32.150
that certain areas,
it's going to be

00:28:32.150 --> 00:28:36.620
very valuable to compute or to
compile this subgraph, but not

00:28:36.620 --> 00:28:37.430
this other one.

00:28:37.430 --> 00:28:38.730
And so this is how you
can accomplish that.

00:28:38.730 --> 00:28:40.190
You get, like, complete control.

00:28:40.190 --> 00:28:41.565
In this particular
example, we've

00:28:41.565 --> 00:28:45.320
got, like, this one operation
down here, which is add,

00:28:45.320 --> 00:28:49.520
and that's the only thing that's
going to be compiled by XLA.

00:28:49.520 --> 00:28:52.020
So I said we wanted to make
this sort of super simple,

00:28:52.020 --> 00:28:54.904
but it's not as simple
as we would like, right?

00:28:54.904 --> 00:28:57.320
Like, I kind of said, well,
you just turn on this one flag

00:28:57.320 --> 00:28:58.460
and it sort of
does all the rest.

00:28:58.460 --> 00:28:59.795
Well, ideally, it's
like, why do you have

00:28:59.795 --> 00:29:00.800
to turn the flag on at all?

00:29:00.800 --> 00:29:03.270
Like, shouldn't it just be,
like, doing this automatically?

00:29:03.270 --> 00:29:05.120
And this is the point where
there's a little bit of work

00:29:05.120 --> 00:29:05.690
in progress.

00:29:05.690 --> 00:29:07.700
Like, we have a lot
of ideas here, right,

00:29:07.700 --> 00:29:09.890
like about different
heuristics that you

00:29:09.890 --> 00:29:13.190
might want to use in terms
of when to turn the thing on,

00:29:13.190 --> 00:29:14.960
what kinds of
clusters to create,

00:29:14.960 --> 00:29:15.960
and things of that sort.

00:29:15.960 --> 00:29:17.376
And so that's
something that we're

00:29:17.376 --> 00:29:18.895
sort of working on right now.

00:29:18.895 --> 00:29:21.020
In the interim, there's
sort of like these two ways

00:29:21.020 --> 00:29:23.061
that you can sort of try
things out for yourself.

00:29:25.850 --> 00:29:28.740
OK, we're, like, zooming
right along here.

00:29:28.740 --> 00:29:31.910
OK, so so far, we've
talked about XLA,

00:29:31.910 --> 00:29:35.139
how we transformed
TensorFlow into XLA,

00:29:35.139 --> 00:29:37.430
and how we do the just-in-time
time compilation, right?

00:29:37.430 --> 00:29:39.770
You sort of like flip this
flag and automatically things

00:29:39.770 --> 00:29:42.200
should get faster or
subportions of your graph

00:29:42.200 --> 00:29:43.232
should get faster.

00:29:43.232 --> 00:29:44.690
This last part is
going to be about

00:29:44.690 --> 00:29:46.670
ahead-of-time compilation.

00:29:46.670 --> 00:29:49.391
And keeping in the
sort of graph motif,

00:29:49.391 --> 00:29:51.140
I'm going to describe--
so the one picture

00:29:51.140 --> 00:29:53.540
sense of, what does
ahead-of-time compilation

00:29:53.540 --> 00:29:55.550
really mean, right?

00:29:55.550 --> 00:29:58.130
And so on the left, we start
with that normal TensorFlow

00:29:58.130 --> 00:30:00.350
graph.

00:30:00.350 --> 00:30:04.610
And unlike the JIT, we
take that entire graph now

00:30:04.610 --> 00:30:09.230
and we transform that
into the XLA graph.

00:30:09.230 --> 00:30:11.570
Like, remember, for the
JIT, we were taking, like,

00:30:11.570 --> 00:30:14.345
subportions of the graph,
where as you're running, like,

00:30:14.345 --> 00:30:16.670
if particular operations
couldn't be compiled,

00:30:16.670 --> 00:30:17.450
that's fine.

00:30:17.450 --> 00:30:21.800
We'll just leave those out,
we'll execute them as normal,

00:30:21.800 --> 00:30:25.220
but then we'll compile the stuff
that actually can be compiled.

00:30:25.220 --> 00:30:28.704
In this mode, the entire
graph needs to be compilable.

00:30:28.704 --> 00:30:30.120
But you get a
benefit out of this.

00:30:30.120 --> 00:30:33.170
Because once you transform
the entire thing into XLA,

00:30:33.170 --> 00:30:36.542
then at the end, out spits
this binary, like x86 or ARM.

00:30:36.542 --> 00:30:38.375
I mean, you can just
run this thing directly

00:30:38.375 --> 00:30:42.030
on your mobile phone
or your server.

00:30:42.030 --> 00:30:44.411
And so this is a pretty
powerful technique.

00:30:44.411 --> 00:30:47.420
The way that we do this
is through something

00:30:47.420 --> 00:30:49.040
called tfcompile.

00:30:49.040 --> 00:30:51.320
This is a tool that's
written in C++,

00:30:51.320 --> 00:30:53.180
which is really sort
of like a wrapper tool,

00:30:53.180 --> 00:30:56.960
the same way that, like, a GCC
or, like, a Clang or something.

00:30:56.960 --> 00:30:59.664
It kicks off, like, the
entire compiler, all right?

00:30:59.664 --> 00:31:01.580
And it's pretty simple,
the way that it works.

00:31:01.580 --> 00:31:03.620
The concept is that
on the left side,

00:31:03.620 --> 00:31:06.050
you start with your
normal TensorFlow graph

00:31:06.050 --> 00:31:08.330
and you give it a configuration
file that tells it

00:31:08.330 --> 00:31:10.100
what the feeds and fetches are.

00:31:10.100 --> 00:31:12.080
So feeds and fetches
are just like TensorFlow

00:31:12.080 --> 00:31:16.910
terminology for what the inputs
are and what the outputs are.

00:31:16.910 --> 00:31:19.640
You feed that into this
compiler, this tool,

00:31:19.640 --> 00:31:22.779
and out at the other end
pops, like, your binary, like,

00:31:22.779 --> 00:31:25.070
machine code for whatever
platform that you want to run

00:31:25.070 --> 00:31:27.574
on and a C++ header.

00:31:27.574 --> 00:31:29.240
To give you a little
bit more of an idea

00:31:29.240 --> 00:31:31.460
of what exactly are
we doing, right?

00:31:31.460 --> 00:31:35.240
That C++ header is conceptually
like a function, right?

00:31:35.240 --> 00:31:37.010
We've turned that
graph and we've

00:31:37.010 --> 00:31:38.840
identified what the
inputs are. identified

00:31:38.840 --> 00:31:39.880
with the outputs are.

00:31:39.880 --> 00:31:43.100
So we turn that entire
graph into a single function

00:31:43.100 --> 00:31:45.170
where the feeds are
the input arguments

00:31:45.170 --> 00:31:47.300
and the fetches are
the output arguments.

00:31:47.300 --> 00:31:49.184
And so that's pretty
simple, right?

00:31:49.184 --> 00:31:51.350
I'm actually pretty excited
about this, because this

00:31:51.350 --> 00:31:53.600
is a pretty cool way to
think about things in terms

00:31:53.600 --> 00:31:55.790
of not just reducing
binary sizes,

00:31:55.790 --> 00:31:58.320
but also, like, different
ways of deployment, right?

00:31:58.320 --> 00:32:02.300
We've got this, like, fancy
way to describe machine

00:32:02.300 --> 00:32:05.150
learning computations
in TensorFlow, right?

00:32:05.150 --> 00:32:07.190
And you can build these
extraordinarily complex

00:32:07.190 --> 00:32:09.620
and, like, intricate models.

00:32:09.620 --> 00:32:12.590
All of that turns into
this graph for TensorFlow.

00:32:12.590 --> 00:32:15.306
And you can feed that entire
graph into this tfcompile.

00:32:15.306 --> 00:32:16.829
And out at the other
end pops, like,

00:32:16.829 --> 00:32:18.620
a single function that
you can call, right,

00:32:18.620 --> 00:32:20.090
with your inputs
and your outputs.

00:32:20.090 --> 00:32:23.410
And that's actually pretty cool.

00:32:23.410 --> 00:32:27.380
Some of the remaining pieces of
work here, right, that we have.

00:32:27.380 --> 00:32:31.300
One of them is that the only
binaries that we currently

00:32:31.300 --> 00:32:34.480
support, the only
back end, is for CPU.

00:32:34.480 --> 00:32:39.850
And so both for workstations,
like x86, or for mobile,

00:32:39.850 --> 00:32:42.760
like ARM but not GPU yet.

00:32:42.760 --> 00:32:45.190
And there's no technical reason
why we haven't done this.

00:32:45.190 --> 00:32:46.807
It's just we have limited time.

00:32:46.807 --> 00:32:49.390
And so we haven't done it yet,
but that's definitely something

00:32:49.390 --> 00:32:52.190
that we want to
do moving forward.

00:32:52.190 --> 00:32:54.850
Another cool future direction
actually is I've mentioned,

00:32:54.850 --> 00:32:57.910
well, the only sort of language
binding that we support right

00:32:57.910 --> 00:32:59.440
now is C++.

00:32:59.440 --> 00:33:01.397
Or actually, technically,
C as well, I guess.

00:33:01.397 --> 00:33:02.980
Either way, you can
call this function

00:33:02.980 --> 00:33:04.355
through this header file.

00:33:04.355 --> 00:33:05.980
There's no reason
why, conceptually, we

00:33:05.980 --> 00:33:08.680
couldn't add a thin
wrapper that allows

00:33:08.680 --> 00:33:11.200
you to call this function
now through your programming

00:33:11.200 --> 00:33:12.400
language of choice.

00:33:12.400 --> 00:33:15.430
Like, basically, everything
supports binding to C

00:33:15.430 --> 00:33:16.109
to some extent.

00:33:16.109 --> 00:33:17.900
And so you could use,
like, Python and SWIG

00:33:17.900 --> 00:33:20.320
or you could use, like, well,
in this example, like, Java

00:33:20.320 --> 00:33:21.140
and JNI.

00:33:21.140 --> 00:33:23.620
Or if you like Go, right,
you could use Go and cgo.

00:33:23.620 --> 00:33:25.780
Like, there's many different
ways where, like, you

00:33:25.780 --> 00:33:27.820
could actually simplify
your deployment

00:33:27.820 --> 00:33:29.451
story using a compiler.

00:33:32.040 --> 00:33:34.020
So the next few slides
are going to talk

00:33:34.020 --> 00:33:37.320
a little bit about the
details through exactly

00:33:37.320 --> 00:33:39.037
how this tfcompile thing works.

00:33:39.037 --> 00:33:40.620
The first part I'm
going to talk about

00:33:40.620 --> 00:33:42.600
is, you know, I had a
little configuration

00:33:42.600 --> 00:33:45.600
file where we said, what are the
feeds and what are the fetches?

00:33:45.600 --> 00:33:49.950
And so this is expressed
in sort of Google protobuf.

00:33:49.950 --> 00:33:52.260
So if you're not familiar
with this, the file

00:33:52.260 --> 00:33:54.810
itself right now is the text
version of the protobuf.

00:33:54.810 --> 00:33:56.610
It looks kind of like JSON.

00:33:56.610 --> 00:33:58.740
It's not very
complicated, actually.

00:33:58.740 --> 00:34:01.050
So this top part
has, like, two feeds.

00:34:01.050 --> 00:34:04.240
And remember, we're identifying
the inputs into the graph,

00:34:04.240 --> 00:34:04.740
right?

00:34:04.740 --> 00:34:06.570
So in this particular
simple example,

00:34:06.570 --> 00:34:10.050
we've got, like, x and y happen
to be our inputs in this graph.

00:34:10.050 --> 00:34:14.219
And we tell the compiler
exactly what the shapes are.

00:34:14.219 --> 00:34:16.409
This is a good point
to reemphasize, right?

00:34:16.409 --> 00:34:19.005
Like, Chris had mentioned
sort of TensorFlow

00:34:19.005 --> 00:34:20.880
has this extreme
flexibility, where it really

00:34:20.880 --> 00:34:23.800
allows you to do stuff
very easily and quickly.

00:34:23.800 --> 00:34:26.539
But for compilers, it's much
better if you can specialize,

00:34:26.539 --> 00:34:27.072
right?

00:34:27.072 --> 00:34:28.530
That's one of the
benefits that you

00:34:28.530 --> 00:34:30.929
get, if you can specialize
based on the exact sizes

00:34:30.929 --> 00:34:32.110
that you have.

00:34:32.110 --> 00:34:34.139
And so here for
any of the inputs

00:34:34.139 --> 00:34:36.540
that come into your graph,
you tell it exactly,

00:34:36.540 --> 00:34:40.389
like, here's the exact sizes
that we're going to use.

00:34:40.389 --> 00:34:43.300
Similarly, for as the feeds,
we've got the fetch here.

00:34:43.300 --> 00:34:46.360
And for the fetch, you
basically just identify

00:34:46.360 --> 00:34:49.750
which node or nodes you want
as outputs of the function

00:34:49.750 --> 00:34:51.340
that you're going to generate.

00:34:51.340 --> 00:34:54.110
In this case, you don't have
to specify the shape anymore.

00:34:54.110 --> 00:34:58.570
In fact, you can't, just
because given the input shapes,

00:34:58.570 --> 00:35:02.020
we can sort of infer what
the shape is at the output

00:35:02.020 --> 00:35:03.640
all the way through the graph.

00:35:03.640 --> 00:35:05.879
And so this is
basically the only sort

00:35:05.879 --> 00:35:07.420
of piece of
configuration that you're

00:35:07.420 --> 00:35:11.469
going to need to actually
run this compiler.

00:35:11.469 --> 00:35:13.260
The next step is going
to talk about, well,

00:35:13.260 --> 00:35:15.200
how do you actually
invoke the thing, right?

00:35:15.200 --> 00:35:16.920
So I've got this
configuration file.

00:35:16.920 --> 00:35:19.900
I've got my graph.

00:35:19.900 --> 00:35:21.660
How do I kick it off?

00:35:21.660 --> 00:35:23.640
And so the example I'm
going to present here

00:35:23.640 --> 00:35:25.494
is when you use Bazel.

00:35:25.494 --> 00:35:26.910
So basically,
through Bazel, we've

00:35:26.910 --> 00:35:29.010
created a Bazel
build macro, which

00:35:29.010 --> 00:35:32.160
is just like syntactic sugar
to kick off the whole thing.

00:35:32.160 --> 00:35:35.460
We've introduced this one
notion called tf_library

00:35:35.460 --> 00:35:36.787
and it's pretty simple.

00:35:36.787 --> 00:35:39.120
You're basically just giving
it, like, three main pieces

00:35:39.120 --> 00:35:41.536
of information and it's the
ones we've just already talked

00:35:41.536 --> 00:35:42.584
about.

00:35:42.584 --> 00:35:44.000
The first one is
the graph, right?

00:35:44.000 --> 00:35:47.070
You sort of give it the
graph in protobuf form.

00:35:47.070 --> 00:35:49.050
The second one is the
feeds and fetches.

00:35:49.050 --> 00:35:50.310
This is a configuration file.

00:35:50.310 --> 00:35:51.960
That was that thing
that I just showed.

00:35:51.960 --> 00:35:54.501
You know, simple, like, here's
the feeds, here's the fetches,

00:35:54.501 --> 00:35:55.920
here's the size of the feeds.

00:35:55.920 --> 00:35:58.980
And the third thing here is
the C++ class name, right?

00:35:58.980 --> 00:36:01.920
And so this is the class name
that will be generated for you

00:36:01.920 --> 00:36:04.800
in that C++ header, which
will allow you to invoke

00:36:04.800 --> 00:36:06.560
the computation.

00:36:06.560 --> 00:36:10.620
Now, one thing to note, I know,
like, in the open source world,

00:36:10.620 --> 00:36:12.630
I mean, I experience
this, too, right?

00:36:12.630 --> 00:36:15.750
Like, not that many
people use Bazel.

00:36:15.750 --> 00:36:20.510
And so if you don't actually use
Bazel, all is not lost, right?

00:36:20.510 --> 00:36:22.740
Like the tfcompile
tool is just something

00:36:22.740 --> 00:36:24.090
you can run the command line.

00:36:24.090 --> 00:36:27.480
And basically, like, these
configuration options

00:36:27.480 --> 00:36:29.310
that I just described
are the flags

00:36:29.310 --> 00:36:30.800
that you pass into the tool.

00:36:30.800 --> 00:36:32.490
And in addition, you
can tell it where

00:36:32.490 --> 00:36:34.320
do I want my output
object file and where

00:36:34.320 --> 00:36:36.360
do I want my output file to go.

00:36:36.360 --> 00:36:37.550
And that's about it.

00:36:37.550 --> 00:36:41.295
So we tried to make this
sort of easily integratable

00:36:41.295 --> 00:36:43.440
into whatever build
system you have.

00:36:43.440 --> 00:36:45.060
Just a note on that,
it's like, this

00:36:45.060 --> 00:36:47.760
is another place where
lots of community help

00:36:47.760 --> 00:36:50.280
would be great, because
there's, like, an astounding

00:36:50.280 --> 00:36:52.890
number of different build
environments that people have,

00:36:52.890 --> 00:36:54.416
like Make, CMake.

00:36:54.416 --> 00:36:56.290
There's, like, a whole
bunch of these things.

00:36:56.290 --> 00:36:58.331
So it should be relatively
simple to actually add

00:36:58.331 --> 00:37:01.160
in that integration.

00:37:01.160 --> 00:37:03.940
OK, so here it's sort of like
there's a whole bunch of code.

00:37:03.940 --> 00:37:05.930
And I know, like, people's
eyes are sort of glazing over

00:37:05.930 --> 00:37:06.430
and stuff.

00:37:06.430 --> 00:37:09.200
So I'm going to, like, just
try to describe this in sort

00:37:09.200 --> 00:37:11.242
of like a straightforward way.

00:37:11.242 --> 00:37:12.950
And so what we're
trying to describe here

00:37:12.950 --> 00:37:14.866
is, like, the simple way
that you can actually

00:37:14.866 --> 00:37:17.510
invoke the computation
that we've just described.

00:37:17.510 --> 00:37:21.620
This is C++ code, because that's
sort of the language that we

00:37:21.620 --> 00:37:22.941
currently support.

00:37:22.941 --> 00:37:24.440
And at the top, you
sort of include,

00:37:24.440 --> 00:37:25.834
well, your generated header.

00:37:25.834 --> 00:37:28.250
This is the thing that was
automatically generated for you

00:37:28.250 --> 00:37:31.040
to invoke the computation.

00:37:31.040 --> 00:37:36.890
And on this line, we instantiate
this class, this TestMatMul.

00:37:36.890 --> 00:37:40.670
That was the class that's
described, defined inside

00:37:40.670 --> 00:37:42.080
of that generated header.

00:37:42.080 --> 00:37:44.090
And note, we've got this
MatMul variable now.

00:37:44.090 --> 00:37:45.650
So if you look
through the slide,

00:37:45.650 --> 00:37:47.660
basically, matmul,
that instance is, like,

00:37:47.660 --> 00:37:49.650
what we're going
to use throughout.

00:37:49.650 --> 00:37:52.170
The next step is a to set up
the arguments to actually run

00:37:52.170 --> 00:37:52.919
this computation.

00:37:52.919 --> 00:37:54.960
This particular computation
we're going to assume

00:37:54.960 --> 00:37:56.820
is, like, a matrix
multiplication.

00:37:56.820 --> 00:37:59.550
And so all of this code is
really just a fancy-- you know,

00:37:59.550 --> 00:38:03.270
this is how in C++, you say
that my first argument is going

00:38:03.270 --> 00:38:05.917
to be the numbers 1 to 6 and my
second argument's going to be

00:38:05.917 --> 00:38:06.750
the numbers 7 to 12.

00:38:09.280 --> 00:38:12.340
And then you call this one
thing called matmul.Run.

00:38:12.340 --> 00:38:15.010
And basically, that's
invoking whatever computation

00:38:15.010 --> 00:38:16.990
you had in your graph.

00:38:16.990 --> 00:38:19.390
And at the end, you can
grab the results out

00:38:19.390 --> 00:38:21.119
of the matmul object.

00:38:21.119 --> 00:38:23.410
And so hopefully this gives
you a sense for, like, kind

00:38:23.410 --> 00:38:24.910
of how simple it really is.

00:38:24.910 --> 00:38:28.690
You can take whatever
arbitrarily complex model

00:38:28.690 --> 00:38:30.850
that you have and boil
it down to a function

00:38:30.850 --> 00:38:33.440
that you can just call.

00:38:33.440 --> 00:38:36.790
And so to recap, we've
covered sort of the compiler

00:38:36.790 --> 00:38:39.640
subdirectory under TensorFlow,
which has XLA, right,

00:38:39.640 --> 00:38:42.160
this is the compiler,
c to XLA, which

00:38:42.160 --> 00:38:44.840
does the TensorFlow
to XLA transformation,

00:38:44.840 --> 00:38:46.390
the JIT, which is
the just-in-time,

00:38:46.390 --> 00:38:50.220
and AOT, which is ahead-of-time.

00:38:50.220 --> 00:38:53.950
And so now I'm going to
present some results.

00:38:53.950 --> 00:38:56.650
So one thing to note
here is that when

00:38:56.650 --> 00:38:58.480
Megan was talking
about some of the cool,

00:38:58.480 --> 00:39:00.430
you know, the awesome
performance results

00:39:00.430 --> 00:39:05.430
that we've had, none of that was
actually including XLA, right?

00:39:05.430 --> 00:39:08.010
So the first slide I'm
going to show is on JIT.

00:39:08.010 --> 00:39:11.460
And when you use JIT for
different models running

00:39:11.460 --> 00:39:14.550
on GPU, what is the speed up?

00:39:14.550 --> 00:39:18.510
And so the way to read
this is that each row is

00:39:18.510 --> 00:39:21.090
one particular model
that we've run.

00:39:21.090 --> 00:39:25.080
And the numbers on the chart
are describing, well, what's

00:39:25.080 --> 00:39:26.550
the speed up or slow down?

00:39:26.550 --> 00:39:28.710
So in essence, for
each row, we've

00:39:28.710 --> 00:39:32.610
run this particular
model two ways.

00:39:32.610 --> 00:39:35.370
We've run it once using
just normal TensorFlow.

00:39:35.370 --> 00:39:37.890
And we've run it another
time with XLA enabled,

00:39:37.890 --> 00:39:39.489
with that JIT flag enabled.

00:39:39.489 --> 00:39:41.280
And the JIT flag is
going to compile things

00:39:41.280 --> 00:39:43.800
down and hopefully
make things faster.

00:39:43.800 --> 00:39:45.750
And so what you're
seeing here is

00:39:45.750 --> 00:39:49.110
that basically all of the green
stuff is positive numbers.

00:39:49.110 --> 00:39:52.560
And so kind of the whole point
in this slide is, well, faster

00:39:52.560 --> 00:39:54.450
is better.

00:39:54.450 --> 00:39:57.540
And the collection of models
that we actually have here

00:39:57.540 --> 00:39:59.730
are sort of things that
we actually do care about.

00:39:59.730 --> 00:40:01.750
It's not just arbitrary things.

00:40:01.750 --> 00:40:04.890
The first couple of
examples are for GMT.

00:40:04.890 --> 00:40:07.060
That's the neural
machine translation.

00:40:07.060 --> 00:40:10.500
So it's one of these
stacked LSTM-type models.

00:40:10.500 --> 00:40:13.200
Next down the list, we've got
Inception, AlexNet, like, some

00:40:13.200 --> 00:40:16.472
of the image processing models.

00:40:16.472 --> 00:40:19.140
One of the reasons
why we see speed ups

00:40:19.140 --> 00:40:21.360
here are due to the
things that Chris actually

00:40:21.360 --> 00:40:22.260
mentioned, right?

00:40:22.260 --> 00:40:23.886
We get this fusion
of operations.

00:40:23.886 --> 00:40:25.260
Like, element and
wise operations

00:40:25.260 --> 00:40:27.000
get automatically
fused together.

00:40:27.000 --> 00:40:32.679
And so if the long pole
basically in your computation

00:40:32.679 --> 00:40:34.220
is streaming through
all of the data,

00:40:34.220 --> 00:40:35.580
well, that's going
to help a lot, right?

00:40:35.580 --> 00:40:36.960
You've got that these
tiny operations,

00:40:36.960 --> 00:40:39.540
like, you want to add things,
you want to multiply things,

00:40:39.540 --> 00:40:41.029
you can fuse them all together.

00:40:41.029 --> 00:40:43.320
Also, there's sort of different
paralyzation and buffer

00:40:43.320 --> 00:40:45.630
allocation strategies that
we have that can actually

00:40:45.630 --> 00:40:47.756
provide additional speed up.

00:40:47.756 --> 00:40:49.380
Another thing we want
to point out here

00:40:49.380 --> 00:40:51.600
is that it's not
just for inference

00:40:51.600 --> 00:40:52.860
or not just for training.

00:40:52.860 --> 00:40:54.360
It's actually for both.

00:40:54.360 --> 00:40:57.540
And so here the pink
boxes are highlighting

00:40:57.540 --> 00:41:00.150
the training examples
or the models

00:41:00.150 --> 00:41:01.710
where we actually ran training.

00:41:01.710 --> 00:41:05.045
So both the forward pass
and back [INAUDIBLE].

00:41:05.045 --> 00:41:06.420
And this is actually
pretty cool.

00:41:06.420 --> 00:41:08.730
One observation, though,
here is that, well, it

00:41:08.730 --> 00:41:11.400
seems, at least on this picture,
that lots of the training ones

00:41:11.400 --> 00:41:13.150
are sort of lower down
on the list, right?

00:41:13.150 --> 00:41:15.600
They're not getting as much
of a performance improvement

00:41:15.600 --> 00:41:18.160
or maybe, even in some cases,
maybe a negative performance

00:41:18.160 --> 00:41:18.660
improvement.

00:41:18.660 --> 00:41:20.940
So negative here means
that we actually are worse.

00:41:20.940 --> 00:41:22.960
We're not as good.

00:41:22.960 --> 00:41:25.560
And so on the training
side, one reason for this,

00:41:25.560 --> 00:41:27.874
it's basically, you know,
training graphs, in general,

00:41:27.874 --> 00:41:29.290
are a little bit
more complicated.

00:41:29.290 --> 00:41:30.620
You can think about,
like, inference graphs

00:41:30.620 --> 00:41:32.190
as only having the forward pass.

00:41:32.190 --> 00:41:33.180
But once you add
training, you're

00:41:33.180 --> 00:41:34.320
going to have, like, backprop.

00:41:34.320 --> 00:41:35.850
You're going to have to
compute the gradients.

00:41:35.850 --> 00:41:37.266
And so that makes
the entire graph

00:41:37.266 --> 00:41:38.525
larger and more complicated.

00:41:38.525 --> 00:41:39.900
And that's one of
the reasons why

00:41:39.900 --> 00:41:41.910
sort of like some of the
numbers are a little bit lower

00:41:41.910 --> 00:41:43.650
for the training side, but
it's definitely something

00:41:43.650 --> 00:41:44.649
that we want to work on.

00:41:44.649 --> 00:41:48.930
And so trying to improve all of
these numbers as much as we can

00:41:48.930 --> 00:41:53.210
is something that we're
actively working on right now.

00:41:53.210 --> 00:41:55.180
So the next slide.

00:41:55.180 --> 00:41:56.890
So actually, with
that previous slide,

00:41:56.890 --> 00:41:58.973
if you saw that in an
academic paper or something,

00:41:58.973 --> 00:42:00.580
you'd be like,
yeah, whatever, man.

00:42:00.580 --> 00:42:01.913
I don't believe this guy, right?

00:42:01.913 --> 00:42:04.450
Because it's like, it just
looks like too positive.

00:42:04.450 --> 00:42:06.970
And so here I've taken a
collection of microbenchmarks.

00:42:06.970 --> 00:42:09.459
Oh, another thing to note,
actually, these numbers

00:42:09.459 --> 00:42:10.000
are for real.

00:42:10.000 --> 00:42:11.530
These numbers are all actually--

00:42:11.530 --> 00:42:14.330
I grabbed them, like, yesterday
off of the actual benchmarks

00:42:14.330 --> 00:42:16.417
that we've been running,
like, constantly.

00:42:16.417 --> 00:42:18.250
And so I just grabbed
a snapshot of whatever

00:42:18.250 --> 00:42:21.280
yesterday was and, like,
plopped them onto this chart.

00:42:21.280 --> 00:42:24.845
And so here, again, it's
the same graph layout.

00:42:24.845 --> 00:42:27.220
The green stuff on the right,
you know, positive numbers.

00:42:27.220 --> 00:42:28.053
Those are speed ups.

00:42:28.053 --> 00:42:28.690
That's good.

00:42:28.690 --> 00:42:30.940
And the red stuff on the
left, negative numbers.

00:42:30.940 --> 00:42:32.350
That's kind of bad.

00:42:32.350 --> 00:42:34.630
And so we see some
pretty good speed ups.

00:42:34.630 --> 00:42:37.090
There's a big, hefty chunk
here where things get up

00:42:37.090 --> 00:42:39.650
to as much as 50%.

00:42:39.650 --> 00:42:41.894
And down here there's,
like, room to improve.

00:42:41.894 --> 00:42:43.810
I talked a little bit
about some of the things

00:42:43.810 --> 00:42:45.460
that we might want
to do to improve.

00:42:45.460 --> 00:42:47.860
Another thing that we've
noticed down here is that--

00:42:47.860 --> 00:42:49.401
you can't read the
font, because it's

00:42:49.401 --> 00:42:51.639
tiny-- but then lots
of the models down here

00:42:51.639 --> 00:42:53.680
or lots of the microbenchmarks
that we're running

00:42:53.680 --> 00:42:56.390
are for sort of
like prefused cells.

00:42:56.390 --> 00:42:59.380
So these are things
where we take advantage

00:42:59.380 --> 00:43:03.100
of existing compilers or
existing implementations

00:43:03.100 --> 00:43:05.320
of fast-fused kernels.

00:43:05.320 --> 00:43:10.090
And so when the TensorFlow
op comes in that does that,

00:43:10.090 --> 00:43:12.240
TensorFlow by itself
does a really good job,

00:43:12.240 --> 00:43:13.990
because it's kicking
off this fast kernel,

00:43:13.990 --> 00:43:18.010
but the XLA side doesn't really
do a good job of that right

00:43:18.010 --> 00:43:18.760
now.

00:43:18.760 --> 00:43:21.760
And so just to be, like, sort
of honest about where we are,

00:43:21.760 --> 00:43:22.750
that's kind of where
we are, but we're

00:43:22.750 --> 00:43:23.916
trying to improve the thing.

00:43:27.181 --> 00:43:29.180
So this slide really
proves to you that I'm not,

00:43:29.180 --> 00:43:30.950
like, in marketing or
something like that.

00:43:33.730 --> 00:43:37.970
Yeah, I actually got
a C-plus in marketing.

00:43:37.970 --> 00:43:39.770
So the previous two
slides were GPU, right?

00:43:39.770 --> 00:43:41.660
And so this slide's, like, CPU.

00:43:41.660 --> 00:43:44.040
Again, on the JIT, right?

00:43:44.040 --> 00:43:46.580
And again, speed ups and
green are supposed to be good.

00:43:46.580 --> 00:43:48.830
And slowdowns and red
are supposed to be bad.

00:43:48.830 --> 00:43:50.870
And so this is, like, not good.

00:43:50.870 --> 00:43:53.450
And so the simple
way to describe this

00:43:53.450 --> 00:43:55.935
is that, yeah, we've got the
men at work thing, you know?

00:43:55.935 --> 00:43:57.560
I can give you a
little bit more detail

00:43:57.560 --> 00:43:58.684
behind that, though, right?

00:43:58.684 --> 00:44:00.560
Like, one of the
simple reasons is

00:44:00.560 --> 00:44:04.880
because we don't have
unlimited people working on it

00:44:04.880 --> 00:44:06.950
and so we have to prioritize.

00:44:06.950 --> 00:44:09.200
And we're kind of like
practical people, right?

00:44:09.200 --> 00:44:10.700
We kind of thought,
well, I mean,

00:44:10.700 --> 00:44:13.280
if you're using TensorFlow and
you want to use this compiler,

00:44:13.280 --> 00:44:15.380
then you probably want
to make things faster.

00:44:15.380 --> 00:44:17.210
And if you want to make
things faster, then

00:44:17.210 --> 00:44:19.670
probably you're using GPUs
use right now already.

00:44:19.670 --> 00:44:21.586
And so the first thing
we're going to focus on

00:44:21.586 --> 00:44:23.300
is, like, improving
GPU performance,

00:44:23.300 --> 00:44:26.390
because you can get, like,
the most benefit out of that.

00:44:26.390 --> 00:44:29.180
One of the concrete reasons
why this particular-- many

00:44:29.180 --> 00:44:31.310
of these benchmarks
are actually worse

00:44:31.310 --> 00:44:34.230
is that currently,
TensorFlow already paralyzes

00:44:34.230 --> 00:44:35.480
much of the operations, right?

00:44:35.480 --> 00:44:38.600
If you have multiple operations,
they can run in parallel.

00:44:38.600 --> 00:44:41.750
This is on top of the within
single op parallelism,

00:44:41.750 --> 00:44:43.640
where, let's say,
you have a big matmul

00:44:43.640 --> 00:44:46.730
and we do fancy tiling and
stuff to parallelize that as

00:44:46.730 --> 00:44:48.940
much as possible.

00:44:48.940 --> 00:44:52.100
On the XLA side, we do
have separate back ends

00:44:52.100 --> 00:44:54.380
for sequential and
parallel, but we simply

00:44:54.380 --> 00:44:56.030
haven't put very
much time into it.

00:44:56.030 --> 00:44:57.738
And so there's a lot
of low-hanging fruit

00:44:57.738 --> 00:45:00.530
here, actually, where I think we
can improve these numbers quite

00:45:00.530 --> 00:45:02.930
easily and quite dramatically.

00:45:02.930 --> 00:45:04.430
So this would also
be an area where,

00:45:04.430 --> 00:45:06.950
if you're interested, then,
like, there's definitely

00:45:06.950 --> 00:45:08.420
in terms of community
involvement,

00:45:08.420 --> 00:45:09.545
there's lots of stuff here.

00:45:09.545 --> 00:45:12.250
And it's all in open source.

00:45:12.250 --> 00:45:14.860
So that would be kind
of a lame sort of slide

00:45:14.860 --> 00:45:16.447
to end on in terms
of performance.

00:45:16.447 --> 00:45:18.280
But one thing we haven't
really talked about

00:45:18.280 --> 00:45:19.856
is smaller binaries, right?

00:45:19.856 --> 00:45:21.730
I mentioned, like,
there's many ways that you

00:45:21.730 --> 00:45:24.340
can use the compilation.

00:45:24.340 --> 00:45:26.340
You can do, like, a
JIT or ahead-of-time.

00:45:26.340 --> 00:45:27.970
This is for
ahead-of-time, right?

00:45:27.970 --> 00:45:30.053
And the specific thing
we're trying to target here

00:45:30.053 --> 00:45:31.810
is that on mobile,
like, binary size

00:45:31.810 --> 00:45:32.830
actually makes a big difference.

00:45:32.830 --> 00:45:34.600
Like, your users don't want
to sit around and wait for,

00:45:34.600 --> 00:45:36.880
like, many, many
megabytes to be downloaded

00:45:36.880 --> 00:45:38.040
for your application.

00:45:38.040 --> 00:45:39.790
And so using the
compiler, we can actually

00:45:39.790 --> 00:45:43.990
get significant sort of
binary size reductions.

00:45:43.990 --> 00:45:47.304
The particular example we have
here is for a stacked LSTM.

00:45:47.304 --> 00:45:48.970
This particular one
happens to be, like,

00:45:48.970 --> 00:45:50.155
three deep and 60 wide.

00:45:50.155 --> 00:45:53.500
So you've got LSTM cells sort
of like 60 wide in sequence

00:45:53.500 --> 00:45:55.240
and then three deep.

00:45:55.240 --> 00:45:56.950
And it's compiled
for Android ARM.

00:45:56.950 --> 00:45:59.380
And this is, like,
a real application.

00:45:59.380 --> 00:46:01.930
The original binary
size was 2.6 megabytes.

00:46:01.930 --> 00:46:05.140
And that breaks down to a 1
megabyte for the TensorFlow

00:46:05.140 --> 00:46:08.740
runtime, with only the
ops that we actually need,

00:46:08.740 --> 00:46:11.090
and then 1.6 megabytes
for the graph.

00:46:11.090 --> 00:46:14.021
And when we compile that down,
we end up with 600 kilobytes,

00:46:14.021 --> 00:46:14.520
right?

00:46:14.520 --> 00:46:16.103
And so that was what
Chris was talking

00:46:16.103 --> 00:46:18.430
about in terms of, like,
a four times reduction.

00:46:18.430 --> 00:46:22.570
272 kilobytes of that is code
and 330 kilobytes is weights.

00:46:22.570 --> 00:46:24.610
So I'm going to
focus in on that 272.

00:46:24.610 --> 00:46:27.580
Because even four times,
that's sort of not enough.

00:46:27.580 --> 00:46:29.110
I want to make it bigger, right?

00:46:29.110 --> 00:46:31.150
So I want to point out,
this picture down here,

00:46:31.150 --> 00:46:33.608
which is sort of like you can't
really read anything on it,

00:46:33.608 --> 00:46:37.390
it's basically a tree map
of all of the binary size

00:46:37.390 --> 00:46:40.990
usage broken down by
what's actually using it.

00:46:40.990 --> 00:46:44.440
And so on this box here, this
is the actual generated code

00:46:44.440 --> 00:46:45.440
that we created.

00:46:45.440 --> 00:46:47.660
It's only 88 kilobytes.

00:46:47.660 --> 00:46:50.890
So the next box that we
have is Eigen. And so this

00:46:50.890 --> 00:46:52.480
is 76 kilobytes.

00:46:52.480 --> 00:46:54.670
This is a good point
to also mention that,

00:46:54.670 --> 00:46:56.152
so you might
wonder, well, what's

00:46:56.152 --> 00:46:57.610
Eigen doing in this
picture, right?

00:46:57.610 --> 00:47:01.360
Like, Eigen, if you don't
know, is a C++ templated sort

00:47:01.360 --> 00:47:05.850
of specialized linear
algebra sort of library.

00:47:05.850 --> 00:47:08.470
So it let's you do, like,
matmuls really fast.

00:47:08.470 --> 00:47:12.010
Does that sort of tiling and
stuff depending on your sizes.

00:47:12.010 --> 00:47:15.542
In XLA, we have the opportunity
to not just implement

00:47:15.542 --> 00:47:17.000
all these things
on our own, right?

00:47:17.000 --> 00:47:18.458
But if we have good
implementations

00:47:18.458 --> 00:47:20.720
that already exist, we can
just take advantage of it.

00:47:20.720 --> 00:47:22.261
And so in this
particular case, we've

00:47:22.261 --> 00:47:24.145
taken advantage for
single-threaded Eigen

00:47:24.145 --> 00:47:26.050
for the matrix
multiplications that

00:47:26.050 --> 00:47:27.970
happen inside the LCM cells.

00:47:27.970 --> 00:47:30.340
So that takes up 76 kilobytes.

00:47:30.340 --> 00:47:32.590
And actually, on the
right, the majority

00:47:32.590 --> 00:47:34.840
is taken up by standard
Android libraries.

00:47:34.840 --> 00:47:36.574
This is, like, 108 kilobytes.

00:47:36.574 --> 00:47:37.990
So this is just a
snapshot of sort

00:47:37.990 --> 00:47:40.090
of like the we believe
we can actually

00:47:40.090 --> 00:47:43.060
make things much smaller
even if we really wanted to.

00:47:43.060 --> 00:47:45.769
But 272 or 600 kilobytes
even is a pretty good start.

00:47:45.769 --> 00:47:47.560
It's, like, a four
times reduction already.

00:47:50.940 --> 00:47:52.470
And so to summarize,
we've talked

00:47:52.470 --> 00:47:56.400
about how so TensorFlow
graphs are compiled via XLA.

00:47:56.400 --> 00:47:59.670
We have both just-in-time and
ahead-of-time compilation.

00:47:59.670 --> 00:48:03.180
And so they're both pretty cool
and serve different use cases.

00:48:03.180 --> 00:48:05.070
And some models,
some real models,

00:48:05.070 --> 00:48:06.930
are actually already
faster or smaller.

00:48:06.930 --> 00:48:08.490
So it's all really neat.

00:48:08.490 --> 00:48:11.359
And oh, yeah, the last thing is
that work in progress, right?

00:48:11.359 --> 00:48:13.650
Like, we're continually trying
to improve these things.

00:48:13.650 --> 00:48:15.066
Like, all those
benchmark numbers,

00:48:15.066 --> 00:48:17.270
we're working on that
on sort of a daily basis

00:48:17.270 --> 00:48:20.390
to try to make things better.

00:48:20.390 --> 00:48:23.020
And so with that,
that's the talk.

00:48:23.020 --> 00:48:26.670
[MUSIC PLAYING]

