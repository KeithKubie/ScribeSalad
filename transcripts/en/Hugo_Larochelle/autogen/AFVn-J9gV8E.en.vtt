WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.690
 in this capsule we see some 

00:00:01.650 --> 00:00:08.010
 properties of the tax collector's gallery 

00:00:06.690 --> 00:00:11.639
 firstly the iperceptions jaws 

00:00:08.010 --> 00:00:14.130
 it's an algorithm that will try to 

00:00:11.639 --> 00:00:16.859
 look for a linear separator between 

00:00:14.130 --> 00:00:20.250
 classes be distinguished as 

00:00:16.859 --> 00:00:23.070
 specified so if we are in a space 

00:00:20.250 --> 00:00:25.289
 where the vectors are of size n will say 

00:00:23.070 --> 00:00:29.160
 we are going to look for a hyper plan that will 

00:00:25.289 --> 00:00:30.929
 separate examples are of a class d 

00:00:29.160 --> 00:00:33.000
 other examples are from the other class 

00:00:30.929 --> 00:00:35.399
 so it's easier to visualize 

00:00:33.000 --> 00:00:37.050
 dimensions 2 so imagine we are 

00:00:35.399 --> 00:00:39.300
 two classes that are the points will 

00:00:37.050 --> 00:00:41.040
 the black dots so his industry 

00:00:39.300 --> 00:00:44.730
 associated with class 1 and the other class 

00:00:41.040 --> 00:00:48.690
 zero what a father 70 years of doing is 

00:00:44.730 --> 00:00:50.430
 to find a right that will separate the 

00:00:48.690 --> 00:00:52.010
 two classes so having on one side the 

00:00:50.430 --> 00:00:54.870
 white dots on one side of the black dots 

00:00:52.010 --> 00:00:57.480
 and in fact we can demonstrate that the 

00:00:54.870 --> 00:01:00.180
 parameters vector w in the 

00:00:57.480 --> 00:01:04.170
 collector it's actually to be 

00:01:00.180 --> 00:01:08.310
 the vector perpendicular to the right 

00:01:04.170 --> 00:01:10.830
 who separate who will separate our our two 

00:01:08.310 --> 00:01:14.640
 classes like that and when are we going to talk 

00:01:10.830 --> 00:01:16.229
 of decision area for the father 

00:01:14.640 --> 00:01:19.500
 it's round for a general classification 

00:01:16.229 --> 00:01:21.840
 to talk about the surface that separates 

00:01:19.500 --> 00:01:24.330
 two regions in two classes 

00:01:21.840 --> 00:01:27.360
 different and specifically does that 

00:01:24.330 --> 00:01:31.770
 is going to be the surface that matches point 

00:01:27.360 --> 00:01:34.290
 where we do we are at the limit between the 

00:01:31.770 --> 00:01:35.729
 class 1 and zero class so that's it 

00:01:34.290 --> 00:01:38.970
 we want to tell them by surface 

00:01:35.729 --> 00:01:40.710
 of editing is so a tax collector we're going 

00:01:38.970 --> 00:01:42.479
 see the property that the surface of 

00:01:40.710 --> 00:01:44.939
 decisions associated with the tax collector 

00:01:42.479 --> 00:01:48.290
 it's going to be a surface that's linear 

00:01:44.939 --> 00:01:48.290
 which corresponds to a hyper plus 

00:01:49.850 --> 00:01:54.000
 now what about the algorithm 

00:01:52.140 --> 00:01:57.990
 to train a tax collector so have 

00:01:54.000 --> 00:02:00.899
 to get a model that fits in 

00:01:57.990 --> 00:02:03.270
 observing gives us so is this 

00:02:00.899 --> 00:02:04.469
 algorithm the sino example 

00:02:03.270 --> 00:02:06.240
 are linear 

00:02:04.469 --> 00:02:08.970
 separable so it's possible to do 

00:02:06.240 --> 00:02:10.550
 go in two dimensions a straight that 

00:02:08.970 --> 00:02:12.440
 separate the two classes 

00:02:10.550 --> 00:02:14.930
 this algorithm of the tax collector one is 

00:02:12.440 --> 00:02:17.060
 guaranteed to converge to a solution with 

00:02:14.930 --> 00:02:19.550
 an error we therefore actually a 

00:02:17.060 --> 00:02:22.580
 right that separate our two well 

00:02:19.550 --> 00:02:25.700
 classes on the training set 

00:02:22.580 --> 00:02:28.610
 whatever the choice of the rate 

00:02:25.700 --> 00:02:30.830
 learning alpha ok so in this 

00:02:28.610 --> 00:02:32.270
 that if in fact the choice of both the 

00:02:30.830 --> 00:02:34.480
 more or less important in the sense 

00:02:32.270 --> 00:02:38.090
 are your necessarily finding a 

00:02:34.480 --> 00:02:40.730
 talkie forgiveness a great plan that goes well 

00:02:38.090 --> 00:02:42.410
 separate our classes and the father is found 

00:02:40.730 --> 00:02:44.900
 have a zero error on our 

00:02:42.410 --> 00:02:46.550
 training grounds by cons if we 

00:02:44.900 --> 00:02:49.130
 has data that is not worthy to me 

00:02:46.550 --> 00:02:50.780
 separates mca that no right that allows 

00:02:49.130 --> 00:02:52.870
 or losers that helps separate a 

00:02:50.780 --> 00:02:55.310
 class in 2 so we have an example here where 

00:02:52.870 --> 00:02:56.690
 we would not be able to find 

00:02:55.310 --> 00:02:59.570
 to pass a right that goes well 

00:02:56.690 --> 00:03:02.450
 separate the two classes then in this 

00:02:59.570 --> 00:03:03.730
 case also to obtain a guarantee of 

00:03:02.450 --> 00:03:05.570
 convergence 

00:03:03.730 --> 00:03:06.260
 we will have to make a small 

00:03:05.570 --> 00:03:09.380
 change 

00:03:06.260 --> 00:03:10.910
 so we have a little modification on 

00:03:09.380 --> 00:03:12.890
 will still be able to get a 

00:03:10.910 --> 00:03:15.290
 convergence towards the solution that is the 

00:03:12.890 --> 00:03:17.989
 smallest mistake possible on our 

00:03:15.290 --> 00:03:19.459
 training center and for that in fact 

00:03:17.989 --> 00:03:21.560
 you have to have a learning rate 

00:03:19.459 --> 00:03:22.790
 which vote depends on the number of 

00:03:21.560 --> 00:03:24.739
 day when we made up 

00:03:22.790 --> 00:03:27.470
 now so in fact need 

00:03:24.739 --> 00:03:29.870
 a learning rate that goes down 

00:03:27.470 --> 00:03:32.510
 an iteration then an update 

00:03:29.870 --> 00:03:34.340
 to another is a way to get 

00:03:32.510 --> 00:03:35.840
 relies of having this guarantee is 

00:03:34.340 --> 00:03:37.940
 tired of taking a learning tour 

00:03:35.840 --> 00:03:39.260
 and divide it so that everything 

00:03:37.940 --> 00:03:40.850
 constant learning but we're going 

00:03:39.260 --> 00:03:43.940
 divide by one more 

00:03:40.850 --> 00:03:46.040
 another constant that will determine to 

00:03:43.940 --> 00:03:49.370
 how fast the self learning is going 

00:03:46.040 --> 00:03:51.680
 decrease times khaki the number here that the 

00:03:49.370 --> 00:03:55.310
 number of updates we made 

00:03:51.680 --> 00:03:56.720
 so far starting at m so the 

00:03:55.310 --> 00:03:58.160
 his made his small modifications the 

00:03:56.720 --> 00:04:00.200
 beautiful we will see convergence anyway 

00:03:58.160 --> 00:04:02.480
 towards the best possible solution on 

00:04:00.200 --> 00:04:03.920
 the intention of the training even if 

00:04:02.480 --> 00:04:09.709
 our data are pauline and gives back to his 

00:04:03.920 --> 00:04:12.769
 parents a council that is helpful 

00:04:09.709 --> 00:04:14.030
 to see a little bit of the progression of 

00:04:12.769 --> 00:04:16.850
 learning is what is called 

00:04:14.030 --> 00:04:18.799
 the learning curve simply a 

00:04:16.850 --> 00:04:21.470
 curve that plot either the error rate 

00:04:18.799 --> 00:04:23.470
 or success rate according to preference 

00:04:21.470 --> 00:04:25.840
 so if it's the error rate in section 

00:04:23.470 --> 00:04:27.340
 sap val are there the sooner the 

00:04:25.840 --> 00:04:30.730
 success rate have a curve that is going 

00:04:27.340 --> 00:04:32.410
 increase or close so as 

00:04:30.730 --> 00:04:34.540
 learning a curve that traces the 

00:04:32.410 --> 00:04:36.490
 any success where the error rate in 

00:04:34.540 --> 00:04:38.500
 according to the number of updates of 

00:04:36.490 --> 00:04:40.150
 parameters that we did up to m so 

00:04:38.500 --> 00:04:42.100
 so useful for visualizing progress 

00:04:40.150 --> 00:04:44.050
 of learning and we have three 

00:04:42.100 --> 00:04:45.400
 examples here at first that where we have a 

00:04:44.050 --> 00:04:47.500
 training set that is 

00:04:45.400 --> 00:04:49.300
 linearly separable when we see 

00:04:47.500 --> 00:04:51.670
 is that as and when a certain 

00:04:49.300 --> 00:04:53.940
 oscillations in because if that's the 

00:04:51.670 --> 00:04:56.080
 success rate so their percentage of 

00:04:53.940 --> 00:04:57.370
 well-classified training example 

00:04:56.080 --> 00:04:59.110
 because we see that as we go 

00:04:57.370 --> 00:05:02.170
 measure it improves and eventually 

00:04:59.110 --> 00:05:03.790
 we actually reach 100% of 

00:05:02.170 --> 00:05:05.470
 classification on the intention to 

00:05:03.790 --> 00:05:07.780
 training 

00:05:05.470 --> 00:05:09.550
 then we have a case here where the 

00:05:07.780 --> 00:05:11.770
 data are not linear separate me 

00:05:09.550 --> 00:05:13.300
 and here we do what we observe is that 

00:05:11.770 --> 00:05:14.770
 with a learning rate that 

00:05:13.300 --> 00:05:17.110
 constant what is the case here 

00:05:14.770 --> 00:05:19.090
 so that's the learning cateau 

00:05:17.110 --> 00:05:21.970
 constant we do not really 

00:05:19.090 --> 00:05:26.880
 improvement in the sense that the rate 

00:05:21.970 --> 00:05:29.410
 of error or sorry the success rate does not 

00:05:26.880 --> 00:05:32.710
 do not converge towards a certain value 

00:05:29.410 --> 00:05:35.380
 we always have oscillations but its 

00:05:32.710 --> 00:05:38.020
 teen learning that is decreasing 

00:05:35.380 --> 00:05:40.780
 for example with the formula that I have 

00:05:38.020 --> 00:05:43.090
 shown sometimes where we have a formula where we 

00:05:40.780 --> 00:05:45.729
 divided by a term that depends on the number 

00:05:43.090 --> 00:05:46.120
 updates that we made up 

00:05:45.729 --> 00:05:48.220
 now 

00:05:46.120 --> 00:05:50.140
 but when we look at it we have 

00:05:48.220 --> 00:05:52.810
 a more stable ski-to-ski curve 

00:05:50.140 --> 00:05:54.130
 which quietly offered measure one 

00:05:52.810 --> 00:05:57.400
 made updates will converge 

00:05:54.130 --> 00:06:01.479
 to a straight line to a fixed value of 

00:05:57.400 --> 00:06:02.590
 a correct classification rate and 

00:06:01.479 --> 00:06:04.270
 in this case because it's not dignified 

00:06:02.590 --> 00:06:05.710
 advanced by ramon is not a rate that is 

00:06:04.270 --> 00:06:08.310
 to 1 but that is a little bit below 

00:06:05.710 --> 00:06:08.310
 an Oscar 

00:06:10.840 --> 00:06:17.169
 so we saw the algorithm of the tax collector 

00:06:14.680 --> 00:06:19.540
 have enough to see a little hastily 

00:06:17.169 --> 00:06:22.449
 why what the rule of update 

00:06:19.540 --> 00:06:23.470
 parameters had blood it was a 

00:06:22.449 --> 00:06:25.540
 good update rule for 

00:06:23.470 --> 00:06:27.310
 improve the performance of the tax collector 

00:06:25.540 --> 00:06:30.940
 have been able to better classify our 

00:06:27.310 --> 00:06:32.680
 training data same state is this 

00:06:30.940 --> 00:06:34.660
 what was commenting on what we could 

00:06:32.680 --> 00:06:36.490
 analyze a little better it is she that the 

00:06:34.660 --> 00:06:37.380
 club that actually sees why this is a 

00:06:36.490 --> 00:06:40.090
 good procedure 

00:06:37.380 --> 00:06:41.830
 so there is actually a more 

00:06:40.090 --> 00:06:43.630
 general to develop aghribs 

00:06:41.830 --> 00:06:45.940
 learning still we'll call 

00:06:43.630 --> 00:06:47.530
 the case of minimizing a loss 

00:06:45.940 --> 00:06:49.030
 so actually several of the algorithm 

00:06:47.530 --> 00:06:50.560
 learning will be seen as a 

00:06:49.030 --> 00:06:54.160
 algorithm that tries to minimize a 

00:06:50.560 --> 00:06:55.870
 loss in this case also by fart this 

00:06:54.160 --> 00:06:58.990
 that we mean without a son a kind of 

00:06:55.870 --> 00:07:02.169
 measure of distance between what we have 

00:06:58.990 --> 00:07:04.960
 predicted is what we would like to have 

00:07:02.169 --> 00:07:08.080
 so we see the notte the bone for so in 

00:07:04.960 --> 00:07:10.870
 english losses between yt who is the target 

00:07:08.080 --> 00:07:12.370
 are we lost and in favor of 

00:07:10.870 --> 00:07:15.880
 learning is essentially that 

00:07:12.370 --> 00:07:17.530
 it is that we have some certain costs 

00:07:15.880 --> 00:07:19.590
 are seen or some measure of 

00:07:17.530 --> 00:07:23.169
 performance that is going to be another loss 

00:07:19.590 --> 00:07:24.940
 associated with a predictor h a model 

00:07:23.169 --> 00:07:26.919
 given are you already in fact 

00:07:24.940 --> 00:07:29.169
 is that this model minimizes the most 

00:07:26.919 --> 00:07:31.450
 possible this loss associate it with doing 

00:07:29.169 --> 00:07:36.669
 a prediction for another and x 

00:07:31.450 --> 00:07:41.260
 when the target associated and there is we are going 

00:07:36.669 --> 00:07:43.090
 see made case presenting this problem 

00:07:41.260 --> 00:07:45.130
 there in this general framework there he goes 

00:07:43.090 --> 00:07:48.370
 exist a certain definition of the 

00:07:45.130 --> 00:07:50.380
 loss that will allow us to derivatives 

00:07:48.370 --> 00:07:52.120
 the tax collector's grid just like 

00:07:50.380 --> 00:07:54.250
 algorithms that try to use it 

00:07:52.120 --> 00:07:56.530
 the average loss on our data 

00:07:54.250 --> 00:07:58.780
 training the railing we will have 

00:07:56.530 --> 00:08:00.669
 some reminders of some notions in 

00:07:58.780 --> 00:08:02.710
 differential calculus that will be useful 

00:08:00.669 --> 00:08:05.729
 to approach this problem 

00:08:02.710 --> 00:08:05.729
 optimization there 

