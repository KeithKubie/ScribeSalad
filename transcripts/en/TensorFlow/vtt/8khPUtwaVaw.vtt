WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.444
[MUSIC PLAYING]

00:00:08.770 --> 00:00:10.270
ELENA NIEDDU: I'm
excited to be here

00:00:10.270 --> 00:00:14.380
and to talk to you about the
In Codice Ratio project--

00:00:14.380 --> 00:00:17.470
that is a project going on
at Roma Tre University--

00:00:17.470 --> 00:00:21.220
and to talk to you
about TensorFlow

00:00:21.220 --> 00:00:23.800
help us build a module
that is able to transcribe

00:00:23.800 --> 00:00:27.360
ancient manuscripts in the
Vatican Secret Archive.

00:00:27.360 --> 00:00:30.750
So some introduction first.

00:00:30.750 --> 00:00:32.009
This is our team.

00:00:32.009 --> 00:00:36.480
On the right, you can see
paleographers and archivists.

00:00:36.480 --> 00:00:39.900
And on the left, there is
us, a data science team.

00:00:39.900 --> 00:00:43.440
And that's why I think the
name we chose, In Codice Ratio,

00:00:43.440 --> 00:00:45.420
reflects us very well.

00:00:45.420 --> 00:00:48.390
Because it's a word
play between the Italian

00:00:48.390 --> 00:00:50.670
and the Latin meaning
of the word "codice."

00:00:50.670 --> 00:00:54.270
Now, in Latin, "in codice
ratio" would mean a knowledge

00:00:54.270 --> 00:00:55.470
through manuscripts.

00:00:55.470 --> 00:00:58.800
But the word "codice" in Italian
also means software code,

00:00:58.800 --> 00:01:01.320
so it's also knowledge
through software,

00:01:01.320 --> 00:01:04.230
which is exactly what
we're planning to do.

00:01:04.230 --> 00:01:07.430
And so you might
ask yourselves, what

00:01:07.430 --> 00:01:10.630
brings paleographers and
archivists and data scientists

00:01:10.630 --> 00:01:11.550
together?

00:01:11.550 --> 00:01:14.110
Well, they have one
problem in common.

00:01:14.110 --> 00:01:17.970
They both want to discover
knowledge from big data.

00:01:17.970 --> 00:01:20.580
We are used to think of
big data as something

00:01:20.580 --> 00:01:22.470
that happens in the web.

00:01:22.470 --> 00:01:24.810
But actually,
historical archives

00:01:24.810 --> 00:01:27.090
are endless source of
historical information,

00:01:27.090 --> 00:01:29.760
of important information,
of cultural information.

00:01:29.760 --> 00:01:34.140
And just to give you a scale of
how large this information can

00:01:34.140 --> 00:01:37.020
be, let's just
compare for a second

00:01:37.020 --> 00:01:39.030
the size of the
Vatican Secret Archive

00:01:39.030 --> 00:01:40.890
to the height of Mount Everest.

00:01:40.890 --> 00:01:44.070
Now, if you were to take each
shelving of a Vatican Secret

00:01:44.070 --> 00:01:47.050
Archive and stack it
one top of the other,

00:01:47.050 --> 00:01:50.670
you would get to
85 kilometers tall.

00:01:50.670 --> 00:01:54.300
That is about 10 times
the size of Mount Everest.

00:01:54.300 --> 00:01:58.020
And the content spans the
centuries and the continents.

00:01:58.020 --> 00:02:00.090
For example, there,
you have examples

00:02:00.090 --> 00:02:04.080
of letters coming from China,
from Europe, from Africa,

00:02:04.080 --> 00:02:05.610
and, of course,
from the Americas.

00:02:08.530 --> 00:02:09.979
So what is our goal?

00:02:09.979 --> 00:02:12.400
Our goal is to build
tool and technology that

00:02:12.400 --> 00:02:15.010
enable historians,
archivists, and scholars

00:02:15.010 --> 00:02:18.850
of the humanities in general
to perform large-scale analysis

00:02:18.850 --> 00:02:21.070
on historical archives.

00:02:21.070 --> 00:02:23.320
Because right now, the
process, let me tell you,

00:02:23.320 --> 00:02:24.430
is entirely manual.

00:02:24.430 --> 00:02:27.550
You still have to go there,
consult the documents manually,

00:02:27.550 --> 00:02:31.360
and be able to read that
very challenging handwriting.

00:02:31.360 --> 00:02:33.820
And then, if you
find information

00:02:33.820 --> 00:02:35.530
that may be linked to
another collection,

00:02:35.530 --> 00:02:37.195
then you have to do
it all by yourself.

00:02:41.490 --> 00:02:45.320
But first, we have to face
the very first challenge that

00:02:45.320 --> 00:02:48.490
is when you are dealing with
web content-- for example,

00:02:48.490 --> 00:02:50.990
if you want to extract data
from the internet-- well, that's

00:02:50.990 --> 00:02:52.240
already text.

00:02:52.240 --> 00:02:55.160
And when we said we're dealing
with the historical documents,

00:02:55.160 --> 00:02:57.130
that's often scans.

00:02:57.130 --> 00:03:02.060
And traditional OCR is
fine for printed text.

00:03:02.060 --> 00:03:03.995
But then you get to this.

00:03:03.995 --> 00:03:06.950
This is medieval handwriting.

00:03:06.950 --> 00:03:09.690
It's Latin, a language
nobody uses anymore.

00:03:09.690 --> 00:03:14.000
It's a handwriting nobody is
able to write or read anymore,

00:03:14.000 --> 00:03:14.840
for that matter.

00:03:14.840 --> 00:03:16.490
It's heavily abbreviated.

00:03:16.490 --> 00:03:19.440
And still, you want to
get texts out of it.

00:03:19.440 --> 00:03:22.190
So you might want to train
a machine learning module.

00:03:22.190 --> 00:03:24.860
Of course, you want.

00:03:24.860 --> 00:03:27.290
But then, we come to
the second challenge.

00:03:27.290 --> 00:03:30.920
And that is scalability in the
data set collection process.

00:03:30.920 --> 00:03:34.470
Now, the graph you see there
is a logarithmic scale.

00:03:34.470 --> 00:03:36.470
And it might show you
something that you already

00:03:36.470 --> 00:03:39.560
know that is known as the
zip flow that tells you

00:03:39.560 --> 00:03:45.740
that there is very few words
occurring humongous times.

00:03:45.740 --> 00:03:48.950
And then, most of the words,
they do not occur that often.

00:03:48.950 --> 00:03:50.180
What does that mean for us?

00:03:50.180 --> 00:03:55.170
That if we want to collect data,
for example, at word level,

00:03:55.170 --> 00:03:57.320
at vocabulary level,
this means that we

00:03:57.320 --> 00:04:01.940
have to annotate thousands
of lines of text, which

00:04:01.940 --> 00:04:05.420
means hundreds of pages, OK?

00:04:05.420 --> 00:04:08.590
And similar systems do exist.

00:04:08.590 --> 00:04:10.340
They are state of
the art systems.

00:04:10.340 --> 00:04:12.740
But most of the
paleographers, even when

00:04:12.740 --> 00:04:16.550
they know of these tools,
get discouraged in using them

00:04:16.550 --> 00:04:20.390
because they say, well, it's
not cost-effective for me.

00:04:20.390 --> 00:04:24.530
Because it can take up to
months, or even years, of work

00:04:24.530 --> 00:04:28.130
on these documents just to get
a transcription system that they

00:04:28.130 --> 00:04:29.870
will maybe use once or twice--

00:04:29.870 --> 00:04:33.790
I don't know-- whereas they
would like to do it faster.

00:04:33.790 --> 00:04:37.480
So we asked ourself, how
can we scale on this task?

00:04:37.480 --> 00:04:42.150
And so we decided to go by
easier step, simpler step.

00:04:42.150 --> 00:04:43.560
The very first
things that we did

00:04:43.560 --> 00:04:46.710
was to collect data
for single characters.

00:04:46.710 --> 00:04:49.610
And these enabled
us not to involve

00:04:49.610 --> 00:04:52.410
paleographers but people
with very less experience.

00:04:52.410 --> 00:04:55.960
We built a custom
crowdsourcing platform

00:04:55.960 --> 00:04:58.900
that worked pretty much
like CAPTCHA solving.

00:04:58.900 --> 00:05:01.410
What you see there is an actual
screen from the platform.

00:05:01.410 --> 00:05:04.950
So the workers were
presented with an image

00:05:04.950 --> 00:05:06.810
and with a target.

00:05:06.810 --> 00:05:10.350
And they had to match
the target and select

00:05:10.350 --> 00:05:12.150
the areas inside of the image.

00:05:12.150 --> 00:05:17.215
And in this way, we were
able to involve more than 500

00:05:17.215 --> 00:05:18.090
high school students.

00:05:18.090 --> 00:05:20.070
And in about two
weeks' work, we made

00:05:20.070 --> 00:05:23.290
more than 40,000 annotations.

00:05:23.290 --> 00:05:27.490
So now we had the data, we
wanted to build a model.

00:05:27.490 --> 00:05:31.120
When I started working
at the project,

00:05:31.120 --> 00:05:33.310
I was pretty much a beginner
in machine learning.

00:05:33.310 --> 00:05:38.140
And so TensorFlow helped
me put in practice

00:05:38.140 --> 00:05:39.670
what I was studying in theory.

00:05:39.670 --> 00:05:42.460
And so it was a
great help that I

00:05:42.460 --> 00:05:45.850
could rely on tutorials
and on the community

00:05:45.850 --> 00:05:50.650
and, where everything else
failed, even the source code.

00:05:50.650 --> 00:05:54.770
So we started
experimenting, and we

00:05:54.770 --> 00:05:56.750
decided to start small first.

00:05:56.750 --> 00:05:58.380
We didn't want to overkill.

00:05:58.380 --> 00:06:01.260
We wanted the model to
fit, exactly, our data.

00:06:01.260 --> 00:06:03.500
So we started small and
proceeded incrementally

00:06:03.500 --> 00:06:06.050
and, in this phase,
in a constant cycle

00:06:06.050 --> 00:06:09.300
of tuning hyperparameters
and model tuning

00:06:09.300 --> 00:06:12.470
and choosing the best optimizer,
the best thing initializers,

00:06:12.470 --> 00:06:14.810
the number of layers
and the type of layers,

00:06:14.810 --> 00:06:17.120
and then evaluating
and training again.

00:06:17.120 --> 00:06:18.350
Then we used Keras.

00:06:18.350 --> 00:06:20.600
It was good for us because
it allowed us to keep

00:06:20.600 --> 00:06:22.970
the code small and readable.

00:06:22.970 --> 00:06:25.760
And then, this is
what we settled for.

00:06:25.760 --> 00:06:28.520
It might look trivial.

00:06:28.520 --> 00:06:36.110
But it allowed us to get up
to a 94% average accuracy

00:06:36.110 --> 00:06:38.870
on our test characters.

00:06:38.870 --> 00:06:41.030
So where does this fit
in the whole scheme

00:06:41.030 --> 00:06:43.580
of the transcription system?

00:06:43.580 --> 00:06:45.050
It's there in the middle.

00:06:45.050 --> 00:06:48.010
And it's actually, so far,
the only [INAUDIBLE] part,

00:06:48.010 --> 00:06:49.630
but we are planning to expand.

00:06:49.630 --> 00:06:53.060
And you will see how later--
we will see how later.

00:06:53.060 --> 00:06:54.830
And so we have the input image.

00:06:54.830 --> 00:06:57.440
So far, we're relying
on an oversegmentation

00:06:57.440 --> 00:06:58.970
that is old-school.

00:06:58.970 --> 00:07:00.500
It's a bit old-school,
but it allows

00:07:00.500 --> 00:07:04.790
us to feed single characters
or combinations of characters

00:07:04.790 --> 00:07:07.480
inside of the classifier,
which then produces

00:07:07.480 --> 00:07:11.150
a different transcription
who are ranked according

00:07:11.150 --> 00:07:14.780
to a Latin language model, which
we also build from publicly

00:07:14.780 --> 00:07:17.800
available sources.

00:07:17.800 --> 00:07:19.520
How good do we get?

00:07:19.520 --> 00:07:23.290
We get about 65%
exact transcription.

00:07:23.290 --> 00:07:28.240
And we can get up to 80% if we
consider minor spelling errors

00:07:28.240 --> 00:07:30.470
or if the segmentation
is perfect.

00:07:30.470 --> 00:07:34.300
If we had perfect segmentation,
we could get up to 80%.

00:07:34.300 --> 00:07:37.220
We will see that this
can be more challenging.

00:07:37.220 --> 00:07:37.720
OK.

00:07:37.720 --> 00:07:40.930
So what are our
plans for a future?

00:07:40.930 --> 00:07:43.630
We're very excited
about the integration

00:07:43.630 --> 00:07:44.920
of TensorFlow and Keras.

00:07:44.920 --> 00:07:48.520
Because I described the
process as being fully Keras.

00:07:48.520 --> 00:07:52.750
What we actually found out was
that sometimes some feature

00:07:52.750 --> 00:07:54.550
were lagging behind,
and sometimes we

00:07:54.550 --> 00:08:00.070
wanted to get one part of
the features from Keras

00:08:00.070 --> 00:08:01.120
or from TensorFlow.

00:08:01.120 --> 00:08:02.898
And so we found
ourselves doing lots of--

00:08:02.898 --> 00:08:04.940
I don't know if that's
your experience, as well--

00:08:04.940 --> 00:08:07.065
but we found ourselves
doing lots of back and forth

00:08:07.065 --> 00:08:08.568
between TensorFlow and Keras.

00:08:08.568 --> 00:08:10.360
And now, we get the
best of the two worlds,

00:08:10.360 --> 00:08:13.580
so we're very
excited about that.

00:08:13.580 --> 00:08:17.080
And so how do we plan to expand
our machine learning system?

00:08:17.080 --> 00:08:20.470
First thing first,
we are trying U-Nets

00:08:20.470 --> 00:08:21.830
for semantic segmentation.

00:08:21.830 --> 00:08:24.160
These are the same Nets that
achieved very good results

00:08:24.160 --> 00:08:25.960
on medical imaging.

00:08:25.960 --> 00:08:28.660
And we're planning to
use them to get rid

00:08:28.660 --> 00:08:32.980
of these tricky computer
vision, old-school segmentation.

00:08:32.980 --> 00:08:37.669
And that would also achieve the
result of having classification

00:08:37.669 --> 00:08:38.169
together.

00:08:38.169 --> 00:08:40.840
Because this is semantic
segmentation we're talking of.

00:08:40.840 --> 00:08:43.750
These are some
preliminary examples

00:08:43.750 --> 00:08:45.040
that work particularly well.

00:08:45.040 --> 00:08:47.530
Of course, there is work
still that we have to do.

00:08:47.530 --> 00:08:50.200
And then, of course, since
there could still be ambiguity,

00:08:50.200 --> 00:08:53.140
we could do error correction
and then transcription.

00:08:53.140 --> 00:08:55.090
But I think this
would be, in itself,

00:08:55.090 --> 00:08:57.750
a significant improvement.

00:08:57.750 --> 00:09:00.100
And another thing we're
experimenting with

00:09:00.100 --> 00:09:01.500
is enlarging our data set.

00:09:01.500 --> 00:09:04.410
Because we don't want
to stick to characters.

00:09:04.410 --> 00:09:05.260
We want to evolve.

00:09:05.260 --> 00:09:07.680
We want to move to
word level, and even

00:09:07.680 --> 00:09:10.290
sentence level,
annotated characters.

00:09:10.290 --> 00:09:12.415
But still, our
focus is scalability

00:09:12.415 --> 00:09:13.540
in the data set collection.

00:09:13.540 --> 00:09:14.998
So we want to
involve paleographers

00:09:14.998 --> 00:09:18.250
as little as possible.

00:09:18.250 --> 00:09:22.980
So for example, this is our
generated inputs from GAN.

00:09:22.980 --> 00:09:24.780
But we are also
planning on using,

00:09:24.780 --> 00:09:26.540
for example, a
variational autoencoder

00:09:26.540 --> 00:09:28.560
so that we can
evolve our data set

00:09:28.560 --> 00:09:31.380
with little human interaction--

00:09:31.380 --> 00:09:32.100
the less we can.

00:09:35.100 --> 00:09:38.460
And in the end, this would
bring us to actually use

00:09:38.460 --> 00:09:43.230
sequence model that could take
full advantage of the sentence

00:09:43.230 --> 00:09:46.590
level context, for
example, and could even

00:09:46.590 --> 00:09:54.300
be able to solve things that
we couldn't be able to solve

00:09:54.300 --> 00:09:56.520
with single character
classification-- for example,

00:09:56.520 --> 00:09:57.210
abbreviation.

00:09:57.210 --> 00:10:01.380
In this kind of text, many words
occur abbreviated, for example,

00:10:01.380 --> 00:10:03.110
just like you would text.

00:10:03.110 --> 00:10:04.740
In some texts, you
would say me too

00:10:04.740 --> 00:10:08.230
and use two, the number, or 4U.

00:10:08.230 --> 00:10:10.590
And that's the same with
this kind of manuscript.

00:10:10.590 --> 00:10:13.710
And that's one of the
application you could have.

00:10:13.710 --> 00:10:15.990
Also, we are planning
to use sequence models

00:10:15.990 --> 00:10:18.180
to get to a neural language
model because so far,

00:10:18.180 --> 00:10:20.850
we only have experimented
with statistics.

00:10:20.850 --> 00:10:25.600
And one last thing
before I let you go.

00:10:25.600 --> 00:10:27.240
I mentioned the
people in the team,

00:10:27.240 --> 00:10:30.180
but there is so many people
I would like to thank

00:10:30.180 --> 00:10:31.740
that were not in that slides.

00:10:31.740 --> 00:10:36.240
And first of all Simone,
who should have been here,

00:10:36.240 --> 00:10:37.270
but he couldn't make it.

00:10:37.270 --> 00:10:42.630
And he was my machine
learning Jedi Master.

00:10:42.630 --> 00:10:47.670
And then Pi School of AI and
Sébastien Bratiéres and Lukasz

00:10:47.670 --> 00:10:50.940
Kaiser for their
amazing mentoring.

00:10:50.940 --> 00:10:53.850
And Marica Ascione, who is
the high school teacher that

00:10:53.850 --> 00:10:56.700
actually allowed us to
involve those students that

00:10:56.700 --> 00:10:58.770
were part of the platform.

00:10:58.770 --> 00:11:00.930
And, of course,
all of the graduate

00:11:00.930 --> 00:11:05.040
and undergraduate students
that worked with us and help us

00:11:05.040 --> 00:11:07.050
achieve what we have
achieved and what we

00:11:07.050 --> 00:11:08.680
plan to achieve in the future.

00:11:08.680 --> 00:11:11.010
And of course, thank
you for your attention.

00:11:11.010 --> 00:11:12.210
[APPLAUSE]

00:11:12.210 --> 00:11:15.260
[MUSIC PLAYING]

