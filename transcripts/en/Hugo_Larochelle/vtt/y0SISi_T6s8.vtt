WEBVTT

1
00:00:01.150 --> 00:00:01.721
In this video,

2
00:00:01.721 --> 00:00:05.470
we'll briefly discuss a convolutional variant of the restricted Boltzmann

3
00:00:05.471 --> 00:00:09.790
machine.
So we saw a,

4
00:00:09.820 --> 00:00:14.220
to take a,
a normal t four neural network and adapted to,
uh,

5
00:00:14.250 --> 00:00:17.680
images by making it convolutional,
uh,

6
00:00:17.770 --> 00:00:19.730
now what about,
uh,

7
00:00:19.731 --> 00:00:23.920
designing convolutional neural networks that are unsupervised.
So for instance,

8
00:00:23.921 --> 00:00:27.610
we've seen the restrictive bolsa machine as a type of neural network,

9
00:00:27.640 --> 00:00:29.060
but that can be unsupervised.

10
00:00:29.061 --> 00:00:32.980
They can learn about the distribution of the data to be able to extract features

11
00:00:33.190 --> 00:00:35.350
without actually providing labels.

12
00:00:36.150 --> 00:00:40.780
So can we design in convolutional variant of the RBM?
Uh,

13
00:00:40.810 --> 00:00:42.070
well,
yes.
Uh,

14
00:00:42.090 --> 00:00:46.150
and one way of doing this really is just to apply these similar ideas we've

15
00:00:46.151 --> 00:00:50.440
discussed in the convolutional neural net in the feet forward supervised case

16
00:00:50.650 --> 00:00:53.110
and adapt them for the unsupervised case that is,

17
00:00:53.170 --> 00:00:55.390
have hidden units that are locally connected,

18
00:00:55.660 --> 00:01:00.070
that share parameters and that are divided into feature maps and introduce the

19
00:01:00.071 --> 00:01:04.150
notion of pulling units.
So that's exactly what unlikely.

20
00:01:04.151 --> 00:01:07.870
And his colleagues and his colleagues in the 2009 I've done.

21
00:01:08.140 --> 00:01:12.250
And so I'm going to briefly describe what they've done are what they've done.

22
00:01:13.540 --> 00:01:17.860
So,
um,
uh,
what they do is that they again,

23
00:01:17.861 --> 00:01:20.740
define and energy function in the RBM where,

24
00:01:20.741 --> 00:01:25.741
where you have hidden units that interact with some visible units in a visible

25
00:01:26.291 --> 00:01:30.580
layer and the interact through some connections that are weighted.

26
00:01:30.820 --> 00:01:35.620
We also have some biases for the visible and hidden units.
But now,

27
00:01:35.650 --> 00:01:39.620
uh,
and I won't go into details of this,
a notation here,
uh,

28
00:01:39.670 --> 00:01:44.670
but now essentially a hidden unit in their feature map came at a position igene.

29
00:01:45.221 --> 00:01:48.880
That feature map is only going to be connected to an neighborhood.

30
00:01:48.970 --> 00:01:53.530
And in this notation here,
the neighborhood is indexed by this,
uh,

31
00:01:53.920 --> 00:01:57.990
r and s variable here.
So,

32
00:01:58.410 --> 00:02:02.880
uh,
essentially we have,
um,
our,

33
00:02:03.150 --> 00:02:07.410
which is right here,
uh,
which is,
uh,
as a value from one to n,

34
00:02:07.411 --> 00:02:12.411
w and the Nwa is essentially the size of the receptive field of that,

35
00:02:14.430 --> 00:02:15.860
of the hidden units in the

36
00:02:16.380 --> 00:02:18.420
<v 1>uh,
and,
uh,
filter maps.</v>

37
00:02:19.250 --> 00:02:21.470
<v 0>If we write it into a more,</v>

38
00:02:21.570 --> 00:02:23.460
<v 1>uh,
uh,</v>

39
00:02:23.590 --> 00:02:25.090
<v 0>confusional notation,</v>

40
00:02:25.390 --> 00:02:29.320
then essentially all the filter maps,

41
00:02:29.380 --> 00:02:33.460
which because it's an RBM now filter map actually correspond to a filter map of

42
00:02:33.461 --> 00:02:37.810
binary random variables is going to be,
uh,

43
00:02:37.900 --> 00:02:38.590
connected.

44
00:02:38.590 --> 00:02:43.590
It's going to be multiplied by the result of performing convolution of some,

45
00:02:44.810 --> 00:02:49.090
uh,
wait filter with the original input.

46
00:02:49.091 --> 00:02:53.060
V V is actually here.
It's an Italy,
but in the annotation here,
they're,

47
00:02:53.070 --> 00:02:57.850
they're using V for a vector or matrices.
So it should really be involved here.

48
00:02:58.330 --> 00:03:01.960
So we're taking image involving it with some filter map,

49
00:03:01.990 --> 00:03:03.490
a parameter matrix,

50
00:03:03.940 --> 00:03:08.940
and we multiplied the result of this element wise with the stochastic binary

51
00:03:09.491 --> 00:03:14.020
hidden units in the cave filter map.
And then we add those up.

52
00:03:14.021 --> 00:03:18.550
And these terms are now involved now defined the energy function in the

53
00:03:18.551 --> 00:03:19.870
convulsion or Rpm.

54
00:03:22.870 --> 00:03:27.870
So that takes care of introducing a dis idea of local connectivity and a

55
00:03:28.400 --> 00:03:31.900
parameter sharing across hidden units and the segmenting,

56
00:03:31.901 --> 00:03:36.460
the hidden institute feature maps.
Now what about probably,
uh,
what about pooling?

57
00:03:36.461 --> 00:03:38.770
Now how do we introduce this idea into to the RPM?

58
00:03:39.340 --> 00:03:44.340
So a Leah doll introduced the notion of a probabilistic pooling type of unit.

59
00:03:45.220 --> 00:03:48.940
And a,
the idea is as follows,
so we'll have a,

60
00:03:48.941 --> 00:03:52.900
so we have a unit say here,
which is,
uh,

61
00:03:52.930 --> 00:03:57.930
in some future map layer and it's as we see locally connected with and just a

62
00:03:58.571 --> 00:04:02.410
receptive field within the physical layer.
And that unit,

63
00:04:02.440 --> 00:04:04.150
it will be a belonging,

64
00:04:04.440 --> 00:04:09.430
will belong into a local neighborhood where there will be other hidden units

65
00:04:09.431 --> 00:04:12.500
within the filter map.
Um,

66
00:04:12.790 --> 00:04:16.300
and now we'll add another layer right on top of it.
Uh,

67
00:04:16.301 --> 00:04:19.420
these pulling units p and uh,

68
00:04:19.810 --> 00:04:24.810
they will be equal to one if only if at least one of the hidden unit in its

69
00:04:25.020 --> 00:04:30.010
current funding neighborhood in the filter map right below is equal to one.

70
00:04:30.010 --> 00:04:33.430
So if there's at least one year binary unit here,
which is equal to one,

71
00:04:33.760 --> 00:04:37.390
the pooling unit will be equal to one.
So this is as if,
uh,

72
00:04:37.400 --> 00:04:38.890
an otherwise it's equal to zero.

73
00:04:39.340 --> 00:04:44.340
So really just a pulling unit is equal to the Max of the hidden units and the

74
00:04:44.591 --> 00:04:49.060
filter maps associated with filter maps,
uh,
right below.

75
00:04:49.930 --> 00:04:54.250
And another thing that they propose is that within the pooling,
their boyhood,

76
00:04:54.910 --> 00:04:58.900
this a neighborhood where we are taking the maximum over the binary hidden
units,

77
00:04:59.100 --> 00:05:03.700
but also impose that there can be only one hidden unit which is equal to one.

78
00:05:04.450 --> 00:05:09.340
And so this will introduce the kind of competition that we see in uh,
local,

79
00:05:09.400 --> 00:05:10.540
uh,
uh,

80
00:05:10.600 --> 00:05:14.680
contrast normalization in a regular feed forward convolutional network.

81
00:05:15.010 --> 00:05:18.970
So it means that within the neighborhood there's actually only one feature that

82
00:05:18.971 --> 00:05:19.920
will be,
uh,

83
00:05:20.200 --> 00:05:25.060
stochastically active in the German model of the restricted Boltzmann machine.

84
00:05:26.080 --> 00:05:27.670
So in other words,

85
00:05:27.910 --> 00:05:32.050
if we write as this with this notation here,

86
00:05:32.140 --> 00:05:33.460
the uh,

87
00:05:33.500 --> 00:05:38.260
computer chase computation that corresponds to computing the pre activation of

88
00:05:38.410 --> 00:05:39.243
the,
uh,

89
00:05:39.250 --> 00:05:44.250
filter map where here they're just assuming there's a single input channel.

90
00:05:44.440 --> 00:05:47.830
So we just have one compulsion to do.
And they also introduced some bias.

91
00:05:48.220 --> 00:05:52.060
So this I a notation here,
if we use it,

92
00:05:52.360 --> 00:05:57.360
the probability of a hidden units being called to one is going be the

93
00:05:57.621 --> 00:06:02.621
exponential of the pre activation divided by one plus the sum over all the other

94
00:06:05.841 --> 00:06:09.860
units in the neighborhood to which the hidden unit,

95
00:06:09.920 --> 00:06:13.880
Hki j belongs to of the exponential of their p activation.

96
00:06:14.960 --> 00:06:19.160
So this is just to take into account the fact that the other hidden units also

97
00:06:19.161 --> 00:06:20.540
have a probability of being caught.

98
00:06:20.541 --> 00:06:25.541
Two one and the one here is for the case where none of the hidden units in the

99
00:06:26.001 --> 00:06:29.330
neighborhood are equal to one,
they all equal to zero,
in which case,

100
00:06:29.480 --> 00:06:30.800
if we look at the energy function,

101
00:06:30.801 --> 00:06:35.630
we'd get a zero term that's multiplying,
uh,
all the,

102
00:06:35.820 --> 00:06:39.980
uh,
pre activations,
uh,
of all the hidden units in the,
in the neighborhood.

103
00:06:40.580 --> 00:06:42.200
Okay.
So,
uh,

104
00:06:42.201 --> 00:06:46.490
we essentially have a sort of soft max over these hidden units in that

105
00:06:46.491 --> 00:06:49.370
neighborhood,
but with an extra possibility,

106
00:06:49.371 --> 00:06:53.930
which is none of the units are equal to two one and they're all equal to zero.

107
00:06:54.170 --> 00:06:57.530
So it's kind of this extended softmax kind of activation here.

108
00:06:58.400 --> 00:07:00.360
And then the pooling unit,
uh,

109
00:07:00.380 --> 00:07:05.380
while it will be equal to zero with probability one over one plus the

110
00:07:05.741 --> 00:07:09.350
normalization.
So that's essentially the case where all,
uh,

111
00:07:09.380 --> 00:07:10.820
so if the cooling unit is zero,

112
00:07:10.821 --> 00:07:14.210
it means that also all the hidden units are equal to zero.

113
00:07:14.330 --> 00:07:18.710
So that's the extra term to make sure that all of these probabilities of each

114
00:07:18.711 --> 00:07:22.000
being equal to one.
And uh,
the,
uh,
uh,

115
00:07:22.010 --> 00:07:25.560
and the case where they're all equal to zero sums to,
uh,

116
00:07:25.910 --> 00:07:30.500
this whole thing sums to one.
So now when we going to sample,

117
00:07:30.770 --> 00:07:35.330
when you will save performing gib sampling or assembling the value of the hidden

118
00:07:35.331 --> 00:07:39.560
units age and the pulling units,
what we have to do is for each neighborhood,

119
00:07:39.590 --> 00:07:42.170
we have to sample from that distribution here.

120
00:07:42.500 --> 00:07:47.120
So we determine whether which of the hill union is active or whether none of

121
00:07:47.121 --> 00:07:47.990
them are active.

122
00:07:48.290 --> 00:07:52.940
And we determined this using this sort of extended soft max kind of a

123
00:07:52.970 --> 00:07:53.803
distribution.

124
00:07:57.800 --> 00:08:01.400
And then if we sample and we're performing give sampling,

125
00:08:01.460 --> 00:08:05.750
we sampled all the value of the hidden units than sampling the visible units,

126
00:08:06.080 --> 00:08:10.280
uh,
is,
uh,
if we're assuming the physical units are binary and again,

127
00:08:10.281 --> 00:08:14.570
we could similarly extend to other types of visible units are using the same

128
00:08:14.571 --> 00:08:18.680
ideas.
We would just stay the hidden layer,
uh,

129
00:08:18.710 --> 00:08:23.320
feature maps and can involve them with their corresponding,
uh,

130
00:08:23.690 --> 00:08:25.640
a matrix of parameters,

131
00:08:25.670 --> 00:08:30.500
their filters and summing over all the feature maps.
And then,

132
00:08:30.750 --> 00:08:34.330
uh,
that convolution at position Ij plus some bias,

133
00:08:34.331 --> 00:08:39.260
see pass through the sigmoid gives us the probability that this,

134
00:08:39.510 --> 00:08:39.830
uh,

135
00:08:39.830 --> 00:08:44.830
particular visible unit at position Ij is equal to one a and one minus.

136
00:08:45.171 --> 00:08:49.970
That would be the palliative it being equal to zero.
So essentially,

137
00:08:50.420 --> 00:08:53.960
uh,
the difference between a regular RBM is that when we're performing,
give,

138
00:08:53.961 --> 00:08:57.990
give sampling,
instead of doing a matrix multiplication,

139
00:08:57.991 --> 00:09:02.250
sigmoid sampling matrix,
pontification sigmoid sampling,
uh,

140
00:09:02.310 --> 00:09:06.660
we're doing convolution a nun in the irony,
which is this,

141
00:09:06.800 --> 00:09:11.800
a sort of extended soft Max ober over the local neighborhood sampling and then

142
00:09:13.021 --> 00:09:14.060
convolution,

143
00:09:14.510 --> 00:09:19.110
a sigmoid sampling and then alternating like this to perform gib sampling.

144
00:09:19.170 --> 00:09:22.020
In this convolutional no variant of their restricted Boltzmann machine.

145
00:09:26.710 --> 00:09:29.050
So using these conditionals,
Eh,

146
00:09:29.170 --> 00:09:33.430
we can perform contrast of divergence because we have a,
we now have a sampling,

147
00:09:33.431 --> 00:09:37.300
these negative samples we need for updating the a,
the model.

148
00:09:37.900 --> 00:09:42.900
I will need the gradients of the energy function to derive our update for the

149
00:09:44.340 --> 00:09:45.450
uh,
uh,

150
00:09:45.610 --> 00:09:50.610
for the parameters of the filter maps and a weekend compute them similarly to

151
00:09:51.101 --> 00:09:55.180
how we compute the back propagated gradients and irregular feed forward neural

152
00:09:55.181 --> 00:09:57.430
network using convolutions.

153
00:09:58.300 --> 00:10:01.360
We can stack these rbms one on top of the other.

154
00:10:01.361 --> 00:10:06.361
So we would actually use the pool units as the values on which to train another

155
00:10:07.421 --> 00:10:08.400
conclusion rpm.

156
00:10:08.410 --> 00:10:12.100
And then we could get multiple layers of a convolutional rbms like this.

157
00:10:12.970 --> 00:10:17.950
And this in fact is we can think of it as giving us a pre training procedure

158
00:10:18.130 --> 00:10:21.880
that's a fork emotional neural network that does not require just extracting

159
00:10:21.881 --> 00:10:26.080
patches.
And we'd actually be a globe,
a train globally on the images.

160
00:10:26.890 --> 00:10:28.900
And so,
uh,
to get a,

161
00:10:28.901 --> 00:10:33.130
so I've been going fairly quickly over this to get more details about this.

162
00:10:33.131 --> 00:10:36.850
I encourage you to read unlikelies and his colleagues papers,

163
00:10:37.270 --> 00:10:41.360
uh,
which was then published in 2009.
And it's,
uh,

164
00:10:41.680 --> 00:10:44.320
available on the website for this course.

