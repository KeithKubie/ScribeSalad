WEBVTT
Kind: captions
Language: en

00:00:00.220 --> 00:00:01.819
So as I know from the last slide, right?

00:00:01.819 --> 00:00:07.150
Once we start thinking about goal
abstraction and sort of all of these

00:00:07.150 --> 00:00:11.800
functions or modules or
procedures running at the same time,

00:00:11.800 --> 00:00:15.520
what you really have done is turn
this into a problem of arbitration.

00:00:15.520 --> 00:00:17.170
By the way this does have a name.

00:00:17.170 --> 00:00:20.540
I really like the name, it's called
modular reinforcement learning.

00:00:20.540 --> 00:00:24.420
And that just sort of captures the idea
that we divided the world into a bunch

00:00:24.420 --> 00:00:27.790
of modules or
what we might call options or

00:00:27.790 --> 00:00:30.390
what we might call
temporally extended actions.

00:00:30.390 --> 00:00:32.560
And these things always have
associated with them goals.

00:00:32.560 --> 00:00:36.090
They're always trying to do something or
to accomplish something.

00:00:36.090 --> 00:00:38.740
They're different models and
with operating system, and

00:00:38.740 --> 00:00:42.160
we need to decide which program
we're going to let run next.

00:00:42.160 --> 00:00:44.490
And there are lots of ways
you might imagine doing this.

00:00:44.490 --> 00:00:49.180
Here are three of the more popular
ones each of which has strings and

00:00:49.180 --> 00:00:52.890
drawbacks and they're kind of the things
that you would come up with if you were

00:00:52.890 --> 00:00:55.230
given five minutes on a test
to come up with something.

00:00:55.230 --> 00:00:57.320
So here are the various
arbitration techniques.

00:00:57.320 --> 00:01:01.080
So the simplest one is greatest
mass Q learning which well,

00:01:01.080 --> 00:01:02.860
Michael what do you think that means?

00:01:02.860 --> 00:01:06.350
&gt;&gt; It means Greatest Mass Q learning.

00:01:07.600 --> 00:01:11.270
&gt;&gt; So we have Q functions maybe for
each of the individual subtasks and

00:01:11.270 --> 00:01:15.680
we say well whoever's got the highest
Q value gets to take the next action.

00:01:15.680 --> 00:01:16.540
&gt;&gt; That's close.

00:01:16.540 --> 00:01:18.260
So the first thing you said is right.

00:01:18.260 --> 00:01:21.130
Since each of these things are their own
little goals running around they're like

00:01:21.130 --> 00:01:25.830
their own little RL bots or agents,
then each of them has a Q function.

00:01:25.830 --> 00:01:31.030
And what greatest mass Q function does
is it adds up all of the Q functions.

00:01:31.030 --> 00:01:34.450
So for the state that we happen
to be in, add up the Q value for

00:01:34.450 --> 00:01:35.570
every state action pair.

00:01:35.570 --> 00:01:39.039
And whichever one is the largest
that's the one that we execute.

00:01:40.750 --> 00:01:45.770
So I was saying whichever Q function
has the largest value you're actually

00:01:45.770 --> 00:01:49.360
saying which action after combining
the Q functions has the largest rate.

00:01:49.360 --> 00:01:50.080
&gt;&gt; Right.

00:01:50.080 --> 00:01:52.260
&gt;&gt; So it's sort of like they're voting.

00:01:52.260 --> 00:01:56.910
&gt;&gt; That's exactly right it's exactly
like they're voting so I say that

00:01:56.910 --> 00:02:01.010
the Q value of taking a particular
action in any state is actually the sum

00:02:02.410 --> 00:02:07.130
of all the Q values for
each of the agents indexed here by i.

00:02:07.130 --> 00:02:09.169
&gt;&gt; And then we take, we go greedy.

00:02:09.169 --> 00:02:14.320
&gt;&gt; I picked the action that has
the most mass across all the agents.

00:02:14.320 --> 00:02:15.760
&gt;&gt; Okay, so then why is it Q then?

00:02:15.760 --> 00:02:19.130
Why isn't it just greatest
action mass or something?.

00:02:19.130 --> 00:02:21.890
&gt;&gt; Well the reason you have it as
Q-learning is because you assume there

00:02:21.890 --> 00:02:26.090
was Q-learning going on and you're using
the Q-function to determine how to vote.

00:02:26.090 --> 00:02:27.799
So that's exactly why
it's called Q-learning

00:02:27.799 --> 00:02:29.850
because there's a Q-function involved.

00:02:29.850 --> 00:02:31.540
So this is adding everything up and

00:02:31.540 --> 00:02:34.240
it does sort of exactly
what you would expect.

00:02:34.240 --> 00:02:38.910
Basically everyone votes and whichever
wins has the biggest vote wins.

00:02:38.910 --> 00:02:41.940
&gt;&gt; So I want to say that
the the modules are voting but

00:02:41.940 --> 00:02:43.170
they're voting on different actions.

00:02:43.170 --> 00:02:46.280
And whichever action has
the largest number of votes wins.

00:02:46.280 --> 00:02:47.580
&gt;&gt; Right, I'm sorry,
that's what I meant to say.

00:02:47.580 --> 00:02:48.310
What did I say?

00:02:48.310 --> 00:02:48.940
&gt;&gt; Okay.

00:02:48.940 --> 00:02:53.885
&gt;&gt; Well it sounded like,
it was interpretable as the whichever

00:02:53.885 --> 00:02:58.840
Q-module gets the most
votes gets to decide.

00:02:58.840 --> 00:02:59.490
&gt;&gt; I'm sorry.

00:02:59.490 --> 00:03:03.110
Yeah, so that's actually closer
to what you were describing.

00:03:03.110 --> 00:03:03.910
&gt;&gt; Yeah I know.
&gt;&gt; But yeah so

00:03:03.910 --> 00:03:06.090
here all you're doing is
you're voting for each action.

00:03:06.090 --> 00:03:09.190
That's the greatest mass Q-learning and
you just go from there.

00:03:09.190 --> 00:03:13.920
And that's neat and
kind of makes sense if you assume

00:03:13.920 --> 00:03:18.060
sort of they're all care about
some ultimate goal together.

00:03:18.060 --> 00:03:23.840
But people have observed that that could
be bad because it might not be good for

00:03:23.840 --> 00:03:26.490
any particular agent, right?

00:03:26.490 --> 00:03:31.020
So imagine that I have a couple of,
I have ten agents, they're ten actions,

00:03:31.020 --> 00:03:35.190
and each one strongly believes
in a subset of those actions.

00:03:35.190 --> 00:03:36.250
But they're different ones.

00:03:36.250 --> 00:03:38.320
So each one being the different action.

00:03:38.320 --> 00:03:41.890
And so, or better yet, there are five,

00:03:41.890 --> 00:03:44.370
there are ten actions and
there's five agents.

00:03:44.370 --> 00:03:48.510
And there's an action that is the fifth
choice of every one of those agents.

00:03:48.510 --> 00:03:51.800
But because the fifth choice of every
one of those agents it's the one

00:03:51.800 --> 00:03:54.450
that ends up getting the most mass and
so

00:03:54.450 --> 00:03:56.860
you haven't really satisfied
any of the subgoals.

00:03:56.860 --> 00:04:00.110
&gt;&gt; Yeah I mean I could see that as being
a bad thing or I could see that as being

00:04:00.110 --> 00:04:03.210
good thing right because it is choosing
that action not just because it's

00:04:03.210 --> 00:04:08.050
bad it's just that actually because
it makes everybody a little happy.

00:04:08.050 --> 00:04:10.440
Instead of making some
people really unhappy.

00:04:10.440 --> 00:04:13.790
Right but you can construct examples
where sort of everyone's least favorite

00:04:13.790 --> 00:04:16.810
choice will still end up being the one
with the most votes if nobody agrees

00:04:16.810 --> 00:04:18.000
on anything else.

00:04:18.000 --> 00:04:20.459
But that's okay I mean it's still as you
say it's a reasonable thing to do under

00:04:20.459 --> 00:04:24.130
lots of circumstances or just happen to
be cases where doesn't always work which

00:04:24.130 --> 00:04:27.180
would there's no free lunch so it's
not surprising that that's the case.

