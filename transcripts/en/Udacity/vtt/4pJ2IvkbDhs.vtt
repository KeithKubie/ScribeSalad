WEBVTT
Kind: captions
Language: en

00:00:00.270 --> 00:00:05.750
So, what I'm claiming in step two here
is that, if we end up choosing a policy

00:00:05.750 --> 00:00:09.530
that is not based on knowing all
of the transitions that there

00:00:09.530 --> 00:00:12.620
are still some unknown transitions out
there, but we end up taking a policy

00:00:12.620 --> 00:00:16.180
that puts us into a loop where
we never visit any of those,

00:00:16.180 --> 00:00:19.970
then that still has to be an optimal
loop, we're not making any mistakes.

00:00:19.970 --> 00:00:23.570
It could be that we could do as well
by uncovering those other values but

00:00:23.570 --> 00:00:28.170
we can't do any better because
we're assuming in step two

00:00:28.170 --> 00:00:32.020
that anything that we don't know
has the largest possible value and

00:00:32.020 --> 00:00:33.910
so, if it has the largest
possible value and

00:00:33.910 --> 00:00:37.730
we still don't want to go there, then
we're doing at least as well as optimal.

00:00:37.730 --> 00:00:38.510
&gt;&gt; Yeah, that's kind of cute.

00:00:38.510 --> 00:00:40.350
&gt;&gt; All right, but now,
we need step three.

00:00:40.350 --> 00:00:44.980
So now, step three says,
okay, we solve the MDP,

00:00:44.980 --> 00:00:50.250
this sort of imagined MDP that has
these optimistic estimates in it,

00:00:50.250 --> 00:00:54.140
and let's say that that
actually takes us to a state

00:00:54.140 --> 00:00:57.150
action pair that we haven't been
to before, and we learn its value.

00:00:57.150 --> 00:00:57.710
Mm-hm.

00:00:57.710 --> 00:01:00.530
&gt;&gt; How many mistakes might
we make on route to that?

00:01:00.530 --> 00:01:02.620
So, when we actually get to that state,
and

00:01:02.620 --> 00:01:06.580
traverse an edge that we didn't know
about, that could have been a mistake.

00:01:06.580 --> 00:01:07.310
&gt;&gt; Right.

00:01:07.310 --> 00:01:09.340
&gt;&gt; So, that's definitely, one.

00:01:09.340 --> 00:01:12.890
And how many mistakes might we make
seeking out the state that actually

00:01:12.890 --> 00:01:15.000
turned out to be a really bad state?

00:01:15.000 --> 00:01:16.070
&gt;&gt; From a particular state?

00:01:16.070 --> 00:01:18.170
I guess, the number of states.

00:01:18.170 --> 00:01:19.260
&gt;&gt; Yes, right.

00:01:19.260 --> 00:01:22.830
So, the number of transitions that we
might take just to find out that we were

00:01:22.830 --> 00:01:26.340
wrong, could be as big as N.

00:01:26.340 --> 00:01:28.906
&gt;&gt; N minus one or
something like that but yeah.

00:01:28.906 --> 00:01:31.853
&gt;&gt; Yeah, so,
this isn't a deterministic M.B.P.

00:01:31.853 --> 00:01:35.484
under to any state our path is
going to be either bounded by N, or

00:01:35.484 --> 00:01:39.666
if it's bigger than N, it's infinity
because we're actually looping

00:01:39.666 --> 00:01:42.750
it without actually getting
to that other place.

00:01:42.750 --> 00:01:45.140
&gt;&gt; Right.
&gt;&gt; So the most it can be is in N right?

00:01:45.140 --> 00:01:45.930
&gt;&gt; Mm hm.
&gt;&gt; All right.

00:01:45.930 --> 00:01:51.650
So, given that, we might make N mistakes
to find out something new, to learn

00:01:51.650 --> 00:01:54.710
something new, to visit a state action
pair that we've never seen before.

00:01:54.710 --> 00:01:56.480
How many times can that happen?

00:01:56.480 --> 00:02:00.500
How many times can it be that
there's a policy that brings us to

00:02:00.500 --> 00:02:02.510
a state action pair that
we haven't been to before?

00:02:03.640 --> 00:02:05.520
&gt;&gt; How many times can that be?

00:02:06.570 --> 00:02:08.220
&gt;&gt; Yeah, so each time.

00:02:08.220 --> 00:02:09.810
This is sort of step three.

00:02:09.810 --> 00:02:11.150
Maybe I should write in English.

00:02:11.150 --> 00:02:13.300
All right, so what this step three is,

00:02:13.300 --> 00:02:17.380
is that what our our max
algorithm tells us to do is to

00:02:17.380 --> 00:02:20.430
execute a path that actually brings
us to an unknown state action pair.

00:02:20.430 --> 00:02:23.420
Then, we go traverse that edge
find out what it really is and

00:02:23.420 --> 00:02:25.090
generate a new policy.

00:02:25.090 --> 00:02:27.970
So, the number of steps that it might
take to get to that new state action

00:02:27.970 --> 00:02:29.140
pair is N.

00:02:29.140 --> 00:02:30.780
&gt;&gt; Right.
&gt;&gt; And the number of times that we might

00:02:30.780 --> 00:02:33.340
discover a new state
action pair is what?

00:02:33.340 --> 00:02:34.710
&gt;&gt; N times the number of actions.

00:02:34.710 --> 00:02:37.330
n times the number of actions because

00:02:37.330 --> 00:02:41.080
that's the total number of unknown
things in the graph to begin with.

00:02:41.080 --> 00:02:42.940
&gt;&gt; Right.
&gt;&gt; Right, so we're exploring the MDP and

00:02:42.940 --> 00:02:45.800
each time we visit state and
take an action we

00:02:45.800 --> 00:02:48.610
haven't taken before that's
going to reduce one from the count.

00:02:48.610 --> 00:02:52.670
So, the largest that count is
in times K and so, the total

00:02:52.670 --> 00:02:57.290
number of mistakes that this algorithm
can make is bounded by N squared K.

00:02:57.290 --> 00:02:58.610
&gt;&gt; Hey, that's polynomial.

00:02:58.610 --> 00:02:59.320
N, N, and K.

00:02:59.320 --> 00:03:01.360
&gt;&gt; It is and
it also doesn't depend on Epsilon, and

00:03:01.360 --> 00:03:02.750
it also doesn't depend on Delta.

00:03:02.750 --> 00:03:04.050
&gt;&gt; That's even better.

00:03:04.050 --> 00:03:05.720
So, wait,
if it doesn't depend on Epsilon, and

00:03:05.720 --> 00:03:09.500
it doesn't depend on Delta, then it
means it's polynomial in those things.

00:03:09.500 --> 00:03:10.020
&gt;&gt; Sure.
Yes,

00:03:10.020 --> 00:03:11.750
it's a very simple
polynomial in those things.

00:03:11.750 --> 00:03:13.570
&gt;&gt; So,
then we're polynomial in everything.

00:03:13.570 --> 00:03:14.110
Yeah.
Yes, so,

00:03:14.110 --> 00:03:15.900
this is an efficient
exploration algorithm for

00:03:15.900 --> 00:03:17.340
deterministic MDP's, right.

00:03:17.340 --> 00:03:19.400
All it's saying is,
whenever we don't know something,

00:03:19.400 --> 00:03:22.120
assume it's awesome and
behave accordingly.

00:03:22.120 --> 00:03:23.090
If we learn something new.

00:03:23.090 --> 00:03:24.030
Make note of it.

00:03:24.030 --> 00:03:27.190
And so, the number of times that we take
a step that might not be the best thing

00:03:27.190 --> 00:03:30.240
to do for that state,
it can't be any bigger than N squared K.

00:03:30.240 --> 00:03:31.360
&gt;&gt; That's beautiful.

00:03:31.360 --> 00:03:35.270
&gt;&gt; Yeah, it's pretty cool, this kind
of analysis comes from a paper that

00:03:35.270 --> 00:03:39.380
Koenig wrote with Reed Simmons where
they actually analyzed Q learning in

00:03:39.380 --> 00:03:43.360
this kind of setting and they got
a worse bound like N cubed times K.

00:03:43.360 --> 00:03:48.170
But this algorithm is sort of pretty
simple, and gives us a nice bound and

00:03:48.170 --> 00:03:49.640
it sort of does what you want it to do.

00:03:51.070 --> 00:03:54.720
&gt;&gt; It's cool, I will point out that
in cube case, also a polynomial.

00:03:54.720 --> 00:03:57.540
&gt;&gt; Yes, yes, but it's a slightly worse
polynomial, you're absolutely right.

00:03:57.540 --> 00:04:00.150
I mean, he showed that Q learning has
a polynomial bound in these kinds of

00:04:00.150 --> 00:04:01.090
environments.

00:04:01.090 --> 00:04:02.910
If you set up the rewards correctly, and

00:04:02.910 --> 00:04:04.890
if you set up the initial
value functions correctly.

00:04:04.890 --> 00:04:10.079
All right, so, that gets us through
stochastic MDPs that have no state, and

00:04:10.079 --> 00:04:16.209
state MDPs that have no stochastic, and
the last thing we want to dive into is,

00:04:16.209 --> 00:04:19.790
what happens if we do something like
our max except for on stochastic MDP's.

00:04:19.790 --> 00:04:22.500
&gt;&gt; Well, we still have another case,
we have to worry about the case where

00:04:22.500 --> 00:04:24.460
we have no state and
we have no stochastic's.

00:04:24.460 --> 00:04:25.000
&gt;&gt; Ahh.

00:04:26.500 --> 00:04:28.510
So, okay, let's do that one really fast.

00:04:28.510 --> 00:04:29.480
&gt;&gt; All right.

00:04:29.480 --> 00:04:32.050
So, you're talking about
an environment has one state and

00:04:32.050 --> 00:04:33.330
all the actions are deterministic.

00:04:33.330 --> 00:04:36.450
They give you a deterministic reward and
return you back to that state.

00:04:36.450 --> 00:04:36.990
&gt;&gt; Right.
&gt;&gt; So,

00:04:36.990 --> 00:04:39.350
this is essentially
deterministic bandit problem.

00:04:39.350 --> 00:04:40.830
&gt;&gt; Right.
&gt;&gt; So, how many mistakes might,

00:04:40.830 --> 00:04:44.730
like what would be an algorithm for this
and how many mistakes would it make?

00:04:44.730 --> 00:04:48.000
&gt;&gt; The dumbest one, or the simplest one
would just be taking every action once,

00:04:48.000 --> 00:04:49.130
and then you know everything.

00:04:49.130 --> 00:04:52.390
&gt;&gt; Right, and each of those, so K minus
1 of those are mistakes possibly.

00:04:53.460 --> 00:04:54.030
And then,

00:04:54.030 --> 00:04:58.060
we only know that we just teach using
the optimal action at their after.

00:04:58.060 --> 00:05:00.680
&gt;&gt; Yeah, you could do a little bit
better view assume that everything is

00:05:00.680 --> 00:05:04.750
our max because as you come
across our RMX, you ca stop.

00:05:04.750 --> 00:05:09.100
&gt;&gt; Very good, yes, right, if you end up
finding along the way, an action that

00:05:09.100 --> 00:05:11.940
actually gives you the full value,
the full RMX value, then we don't

00:05:11.940 --> 00:05:14.690
have to explore the rest of them because
they're not going to be any better.

00:05:14.690 --> 00:05:15.820
Yeah, okay, so that gives us.

00:05:15.820 --> 00:05:18.150
It doesn't give us a better bound, but

00:05:18.150 --> 00:05:20.950
it does probably give us a better
performance if we keep track of that.

00:05:20.950 --> 00:05:22.835
So, yes, so this is a silly case.

00:05:22.835 --> 00:05:23.640
[LAUGH]
&gt;&gt; Well, but

00:05:23.640 --> 00:05:26.440
it does allow us to say we
solved three of the four.

00:05:26.440 --> 00:05:27.370
&gt;&gt; Great.
All right, so

00:05:27.370 --> 00:05:29.140
let's dive into number four.

