WEBVTT
Kind: captions
Language: en

00:00:00.030 --> 00:00:04.970
&gt;&gt; From the Library of
Congress in Washington, DC.

00:00:16.930 --> 00:00:18.460
&gt;&gt; Abbey Potter: Hello.

00:00:18.460 --> 00:00:21.760
I am Abbey Potter.

00:00:21.760 --> 00:00:24.780
I'm with the Library of Congress
labs team and we're really happy

00:00:24.780 --> 00:00:30.400
to host the HTRC and their
text mining workshop today.

00:00:30.400 --> 00:00:32.870
This is a topic we're
very interested in.

00:00:32.870 --> 00:00:35.350
We-- The library also has--

00:00:35.350 --> 00:00:37.610
like I'm sure many of your
institutions has loads

00:00:37.610 --> 00:00:39.990
of copyright or restricted
materials

00:00:39.990 --> 00:00:45.040
and we're always interested
to find ways to use that

00:00:45.040 --> 00:00:48.070
and make it useful to our users.

00:00:48.070 --> 00:00:51.600
So, we're really looking
forward to this workshop.

00:00:51.600 --> 00:00:54.170
So I will turn it
over to Harriett

00:00:54.170 --> 00:00:56.200
and she will get us started.

00:00:56.200 --> 00:00:57.500
Is that right?

00:00:57.500 --> 00:00:59.800
OK.

00:01:06.060 --> 00:01:07.360
&gt;&gt; Harriett Green: Yes.

00:01:07.360 --> 00:01:09.760
So we love to-- we really like
to thank Abbey and Jaime Mears

00:01:09.760 --> 00:01:12.020
for inviting us here at
the Library of Congress.

00:01:12.020 --> 00:01:15.540
So we're really excited to talk
with you today about HathiTrust

00:01:15.540 --> 00:01:18.940
and text mining and how
US librarians can start

00:01:18.940 --> 00:01:20.240
to engage in this research.

00:01:20.240 --> 00:01:24.590
So, just as we-- just to
start up with a quick--

00:01:24.590 --> 00:01:26.220
we're going to do
introductions that again

00:01:26.220 --> 00:01:28.070
as Abbey said won't be filmed.

00:01:28.070 --> 00:01:30.080
My name is Harriett Green.

00:01:30.080 --> 00:01:32.070
I'm the PI for this project.

00:01:32.070 --> 00:01:34.830
And I'll be accompanied
by, well,

00:01:34.830 --> 00:01:37.140
[inaudible] with
Eleanor Dickson,

00:01:37.140 --> 00:01:39.290
also from the University of
Illinois, and Amanda Henley

00:01:39.290 --> 00:01:41.600
from the University
of North Carolina

00:01:41.600 --> 00:01:42.990
and she'll be circulating.

00:01:42.990 --> 00:01:46.480
So definitely keep an eye
out for Amanda as you go

00:01:46.480 --> 00:01:48.570
through the process
and you need help.

00:01:48.570 --> 00:01:51.550
So just to give a
quick introduction

00:01:51.550 --> 00:01:54.180
about the project itself
and kind of the context

00:01:54.180 --> 00:01:58.570
that we're doing this work
today, so we received a grant

00:01:58.570 --> 00:02:02.610
from the IMLS about--
in 2015 to carry

00:02:02.610 --> 00:02:06.060
out this project that's really
aimed at empowering librarians

00:02:06.060 --> 00:02:07.930
to engage with digital
scholarship

00:02:07.930 --> 00:02:10.900
and giving you the basic
foundational tools and skills

00:02:10.900 --> 00:02:14.360
to start talking with
faculty, working with faculty

00:02:14.360 --> 00:02:17.470
in any context that you work in,
so that we're not just thinking

00:02:17.470 --> 00:02:20.050
about digital scholarship
librarians but subject liaisons,

00:02:20.050 --> 00:02:23.410
technical services, librarians,
archivists, all contexts

00:02:23.410 --> 00:02:26.080
that librarians are starting to
engage with digital scholarship.

00:02:26.080 --> 00:02:28.930
So you can see, all of you
should have visited our website

00:02:28.930 --> 00:02:33.040
by this point,
teach.htrc.illinois.edu.

00:02:33.040 --> 00:02:35.400
And these are the
goals, the formal goals

00:02:35.400 --> 00:02:37.020
that we stated for the project.

00:02:37.020 --> 00:02:39.010
Again, this workshop
is aiming to arm you

00:02:39.010 --> 00:02:41.580
with some instructional
content as well as tool skills.

00:02:41.580 --> 00:02:43.470
So everything that
you downloaded today,

00:02:43.470 --> 00:02:45.260
all the slides, all
the materials,

00:02:45.260 --> 00:02:47.090
they're here for you to use.

00:02:47.090 --> 00:02:49.360
So we really love to hear
your stories afterward

00:02:49.360 --> 00:02:52.190
of how you're actually using
this content as well to teach

00:02:52.190 --> 00:02:54.790
and work with students
and faculty.

00:02:54.790 --> 00:02:57.820
And then kind of the larger
goals, empowering librarians

00:02:57.820 --> 00:03:00.250
to become research
partners on digital projects

00:03:00.250 --> 00:03:01.970
and we'll do some
discussions throughout the day

00:03:01.970 --> 00:03:03.270
to think about that.

00:03:03.270 --> 00:03:05.420
And then thinking about as
digital scholarship centers

00:03:05.420 --> 00:03:07.860
and services are emerging
on your campuses, you know,

00:03:07.860 --> 00:03:12.090
enabling you to start engaging
with those centers and all

00:03:12.090 --> 00:03:13.560
of the different
initiatives that are happening

00:03:13.560 --> 00:03:16.460
on a lot of our campuses.

00:03:16.460 --> 00:03:19.090
So, first and foremost,
I want to thank the team

00:03:19.090 --> 00:03:20.540
that put this workshop together,

00:03:20.540 --> 00:03:22.040
that put all the
materials together,

00:03:22.040 --> 00:03:24.450
certainly it isn't just
me-- and it's not just me,

00:03:24.450 --> 00:03:28.370
not just Eleanor and Amanda,
but a whole cohort of people.

00:03:28.370 --> 00:03:32.650
So, in addition, the co-PIs are
Angela Courtney from Indiana,

00:03:32.650 --> 00:03:35.400
Stephen Downie from the
School of Information Sciences

00:03:35.400 --> 00:03:38.920
at Illinois, Teresa Heidenwolf
from Lafayette College,

00:03:38.920 --> 00:03:42.360
Geoff Morse from Northwestern,
and Amanda from North Carolina.

00:03:42.360 --> 00:03:45.020
And that also highlights
that we're not just coming

00:03:45.020 --> 00:03:46.820
from a research library
perspective,

00:03:46.820 --> 00:03:49.280
there's a nice array of types
of libraries that we're coming

00:03:49.280 --> 00:03:51.860
from as we created this
workshop curriculum.

00:03:51.860 --> 00:03:54.250
So we're trying to account
for all the different types

00:03:54.250 --> 00:03:56.490
of library context that
you'll be working in.

00:03:56.490 --> 00:03:59.190
And then the project members
are really the backbone

00:03:59.190 --> 00:04:00.490
of this work.

00:04:00.490 --> 00:04:02.310
In particular, I want
to highlight Eleanor,

00:04:02.310 --> 00:04:05.930
who along with Leanne
Nay and Ruohua Han,

00:04:05.930 --> 00:04:07.350
because they wrote
this curriculum.

00:04:07.350 --> 00:04:09.380
So they did a lot
of the hard work

00:04:09.380 --> 00:04:11.790
to put these materials
together, so I really want

00:04:11.790 --> 00:04:14.750
to thank them as well.

00:04:14.750 --> 00:04:16.580
So, this is our first
module that's going

00:04:16.580 --> 00:04:20.010
to really give you an
introduction to text analysis

00:04:20.010 --> 00:04:22.430
and thinking about what
it is, giving you kind

00:04:22.430 --> 00:04:23.730
of a larger framework.

00:04:23.730 --> 00:04:25.030
And then we're also
going to talk

00:04:25.030 --> 00:04:28.130
about the HathiTrust
Research Center

00:04:28.130 --> 00:04:31.050
that we'll be working
with today as well.

00:04:31.050 --> 00:04:34.290
So again, this module, we'll
talk about text analysis,

00:04:34.290 --> 00:04:37.950
what it means to do
workflows with text analysis.

00:04:37.950 --> 00:04:40.090
We'll then talk a little
bit about the HathiTrust

00:04:40.090 --> 00:04:42.550
and the HathiTrust Research
Center and the context

00:04:42.550 --> 00:04:44.280
that we'll be using
to start thinking

00:04:44.280 --> 00:04:46.370
about text analysis
with this resource.

00:04:46.370 --> 00:04:48.710
And then we'll also
introduce the hands-on example

00:04:48.710 --> 00:04:51.220
and case study that we'll
be using throughout the day

00:04:51.220 --> 00:04:53.650
that will again give you a
framework and reference point

00:04:53.650 --> 00:04:58.080
for how text analysis works and
how US librarians can also start

00:04:58.080 --> 00:05:00.720
to work with text analysis.

00:05:00.720 --> 00:05:03.600
So, first of all,
what is text analysis?

00:05:03.600 --> 00:05:05.700
So let's start with
just the basics.

00:05:05.700 --> 00:05:09.660
So what it is and the definition
that we'll be using is

00:05:09.660 --> 00:05:12.370
that text analysis is a subset

00:05:12.370 --> 00:05:14.530
of data mining and
data analysis.

00:05:14.530 --> 00:05:16.480
And broadly speaking,
it's the process

00:05:16.480 --> 00:05:18.090
by which computers are used

00:05:18.090 --> 00:05:21.190
to reveal information
in and about text.

00:05:21.190 --> 00:05:24.620
And we took this definition
from Marti Hearst 2003 article

00:05:24.620 --> 00:05:27.530
that you can see in our provided
bibliography both at the end

00:05:27.530 --> 00:05:29.450
of the slides and
on our website.

00:05:29.450 --> 00:05:32.250
So, what we-- with
text analysis,

00:05:32.250 --> 00:05:35.040
computer algorithms
can discern patterns in

00:05:35.040 --> 00:05:37.050
and about the text often

00:05:37.050 --> 00:05:39.650
when it's bodies of
unstructured text.

00:05:39.650 --> 00:05:41.670
And unstructured means
that little is known

00:05:41.670 --> 00:05:43.610
about the semantic
meaning of the text data

00:05:43.610 --> 00:05:47.280
and it doesn't fit a defined
data model or database.

00:05:47.280 --> 00:05:51.340
So an algorithm, and we'll
revisit this again later today,

00:05:51.340 --> 00:05:53.290
is simply a process by which--

00:05:53.290 --> 00:05:56.380
a computational process
by which output is created

00:05:56.380 --> 00:05:57.730
from certain input.

00:05:57.730 --> 00:06:00.110
So in text analysis,
the input is the text

00:06:00.110 --> 00:06:02.260
and then the algorithm
works on the text

00:06:02.260 --> 00:06:04.200
and the output would
be indicators

00:06:04.200 --> 00:06:06.980
to help you discern
patterns about that text.

00:06:06.980 --> 00:06:08.960
And text analysis is
more about search.

00:06:08.960 --> 00:06:11.310
It's not just search and
discovery, but it's also

00:06:11.310 --> 00:06:13.350
that something is
present in that text.

00:06:13.350 --> 00:06:15.810
So exploring what does
it mean for that pattern,

00:06:15.810 --> 00:06:17.260
for that thing to be there.

00:06:17.260 --> 00:06:20.760
So for example, knowing that
creativity appears x number

00:06:20.760 --> 00:06:23.260
of times in a text is
only the first step.

00:06:23.260 --> 00:06:25.410
We then want to know
what the patterns

00:06:25.410 --> 00:06:27.410
about its appearance
tell us about culture,

00:06:27.410 --> 00:06:29.420
tell us about history,
et cetera.

00:06:29.420 --> 00:06:32.580
And text analysis can be
used in a variety of purposes

00:06:32.580 --> 00:06:35.730
and actually each of you in your
everyday lives have probably

00:06:35.730 --> 00:06:38.380
encountered text analysis
in many different contexts.

00:06:38.380 --> 00:06:40.790
So in a scholarly
realm, it's been used

00:06:40.790 --> 00:06:45.390
to analyze scientific
literature and to pick up trends

00:06:45.390 --> 00:06:47.690
in medical research to discern
things that might have happened

00:06:47.690 --> 00:06:50.950
that otherwise you don't see
in individual articles or just

00:06:50.950 --> 00:06:53.260
in a year's span of
medical literature.

00:06:53.260 --> 00:06:55.390
But it can also be used
in developing tools.

00:06:55.390 --> 00:06:58.780
So in your email, when you-- the
spam that your email picks up

00:06:58.780 --> 00:07:01.810
and tosses into another folder,
that's using text analysis

00:07:01.810 --> 00:07:04.350
because it's discerning
patterns and emails to say,

00:07:04.350 --> 00:07:06.490
this is a rogue email,
it's not useful,

00:07:06.490 --> 00:07:08.210
I'm going to put it
in the spam folder.

00:07:08.210 --> 00:07:10.210
So we're each-- we're
all interacting

00:07:10.210 --> 00:07:13.370
with text analysis
actually every day.

00:07:13.370 --> 00:07:15.820
So, how does text
analysis work in general?

00:07:15.820 --> 00:07:20.200
So text analysis usually
follows these basic steps.

00:07:20.200 --> 00:07:23.080
So first, the text needs to
be transformed into a form

00:07:23.080 --> 00:07:25.710
that human readers are
familiar with into something

00:07:25.710 --> 00:07:28.080
that the computer can "read".

00:07:28.080 --> 00:07:31.170
So this means that we often
need to break apart the text

00:07:31.170 --> 00:07:33.470
into smaller pieces
and reduce it

00:07:33.470 --> 00:07:35.860
in what we call abstracting
it into something

00:07:35.860 --> 00:07:40.010
that the computer can read
and crunch through algorithms.

00:07:40.010 --> 00:07:42.090
And then counting is
what happens next.

00:07:42.090 --> 00:07:46.150
So often, the computer will
count words, phrases of speech,

00:07:46.150 --> 00:07:50.970
parts of speech, different--
whatever you define it to be

00:07:50.970 --> 00:07:54.480
so that it can start
processing the text.

00:07:54.480 --> 00:07:55.780
And these counts are used

00:07:55.780 --> 00:07:57.680
to identify characteristics
of the text.

00:07:57.680 --> 00:08:00.810
And then the researches
apply the algorithms,

00:08:00.810 --> 00:08:05.190
often computational statistics
to develop hypotheses based

00:08:05.190 --> 00:08:11.570
on the patterns they're seeing
from those counts in the text.

00:08:11.570 --> 00:08:12.880
So how does this
impact research?

00:08:12.880 --> 00:08:14.700
So again, let's think
about the broader context

00:08:14.700 --> 00:08:18.270
of what does this mean for text
analysis to be used in research.

00:08:18.270 --> 00:08:22.680
So in general, text analysis
enables a shift in perspective

00:08:22.680 --> 00:08:24.450
of the researcher
which leads to a shift

00:08:24.450 --> 00:08:25.890
in the research questions.

00:08:25.890 --> 00:08:28.500
And so, this is often
called distant reading

00:08:28.500 --> 00:08:30.690
and this is a phrase
coined by Franco Moretti,

00:08:30.690 --> 00:08:32.950
a Stanford professor, meaning

00:08:32.950 --> 00:08:34.670
that when you do
distant reading,

00:08:34.670 --> 00:08:36.100
you're not just reading
literature

00:08:36.100 --> 00:08:38.800
by studying particular
text, which is traditional

00:08:38.800 --> 00:08:41.080
for close reading
but by aggregating

00:08:41.080 --> 00:08:43.780
and analyzing massive
amounts of text

00:08:43.780 --> 00:08:46.040
and "reading them
from a distance".

00:08:46.040 --> 00:08:48.510
So the scaling up in
this distancing can bring

00:08:48.510 --> 00:08:52.110
out more insights but from a
very different vantage point.

00:08:52.110 --> 00:08:53.620
And it's also worth mentioning

00:08:53.620 --> 00:08:56.110
that text analysis is
not the end all be all.

00:08:56.110 --> 00:08:58.390
It's really one step in
the research process.

00:08:58.390 --> 00:09:00.920
And it can be combined
often with close reading

00:09:00.920 --> 00:09:03.800
and this is what we might
call intermediate reading

00:09:03.800 --> 00:09:05.710
or distant close reading.

00:09:05.710 --> 00:09:08.130
And the shift in research
perspective then allows

00:09:08.130 --> 00:09:09.880
for shifts in research
questions,

00:09:09.880 --> 00:09:12.480
so that old questions
can sometimes be answered

00:09:12.480 --> 00:09:13.780
in new ways.

00:09:13.780 --> 00:09:16.340
So some of the possibilities
that text analysis open

00:09:16.340 --> 00:09:18.480
up is questions that
you can't prove

00:09:18.480 --> 00:09:20.130
by doing human reading alone.

00:09:20.130 --> 00:09:23.390
A human can't read a
million books in one sitting

00:09:23.390 --> 00:09:25.110
or even years of sitting.

00:09:25.110 --> 00:09:28.530
So what-- how do we enable
computers to help us with that?

00:09:28.530 --> 00:09:32.010
And by that token, it allows
larger corpora for analysis

00:09:32.010 --> 00:09:34.950
by aggregating and analyzing
mass amounts of text.

00:09:34.950 --> 00:09:38.150
And then it allows
for textual studies

00:09:38.150 --> 00:09:41.710
that cover longer time spans
as well, again, by aggregating

00:09:41.710 --> 00:09:45.810
and analyzing those
larger corpora of text.

00:09:45.810 --> 00:09:49.580
Text analysis research
explores a wide range of topics

00:09:49.580 --> 00:09:53.790
and we heard from some of these
examples address certain issues,

00:09:53.790 --> 00:09:56.720
so from biomedical
discovery to literate history

00:09:56.720 --> 00:10:00.130
to religious history,
and what really comes

00:10:00.130 --> 00:10:02.380
down to is the research
question itself.

00:10:02.380 --> 00:10:05.610
And the research questions
that are particularly conducive

00:10:05.610 --> 00:10:09.670
to text analysis methods often
involve these characteristics.

00:10:09.670 --> 00:10:11.630
So one is change over time.

00:10:11.630 --> 00:10:13.810
So are we seeing changes?

00:10:13.810 --> 00:10:17.040
Pattern recognition and
identifying patterns

00:10:17.040 --> 00:10:20.110
that might be useful to know,
and then comparative analysis

00:10:20.110 --> 00:10:22.150
as well, so how our works--

00:10:22.150 --> 00:10:25.710
you know, identifying
different changes in works

00:10:25.710 --> 00:10:30.260
or interesting things and
comparing these works.

00:10:30.260 --> 00:10:32.610
So, let's look at some
real-world examples

00:10:32.610 --> 00:10:36.210
that show these characteristics
as well as demonstrate impacts

00:10:36.210 --> 00:10:38.880
on the research that
we discussed earlier.

00:10:38.880 --> 00:10:43.210
So, let's take about
three, four minutes.

00:10:43.210 --> 00:10:45.810
And in your tables or
you can turn around,

00:10:45.810 --> 00:10:50.980
you can go to this URL,
go.illinois.edu and then look

00:10:50.980 --> 00:10:52.410
at the research examples.

00:10:52.410 --> 00:10:54.770
We have three research--

00:10:54.770 --> 00:10:58.910
types of research questions
with text analysis and discuss,

00:10:58.910 --> 00:11:01.110
you know, how-- what do
these projects involve change

00:11:01.110 --> 00:11:04.750
over time, pattern recognition,
and/or comparative analysis,

00:11:04.750 --> 00:11:06.370
what kind of text
data do they use,

00:11:06.370 --> 00:11:07.670
and what are their findings?

00:11:07.670 --> 00:11:10.160
So let's take a minute to go
through these and just kind

00:11:10.160 --> 00:11:14.180
of discuss a bit more and maybe
verify what you guys thought

00:11:14.180 --> 00:11:17.690
about which-- what are the
answers to these questions.

00:11:17.690 --> 00:11:21.150
So the first example
that we looked

00:11:21.150 --> 00:11:23.060
at was a well-known example.

00:11:23.060 --> 00:11:26.170
And it's using a method
that we call stylometry.

00:11:26.170 --> 00:11:29.300
And so involved when
"Cuckoo's Calling"

00:11:29.300 --> 00:11:32.000
by Robert Galbraith
was published in 2013,

00:11:32.000 --> 00:11:33.900
people wondered if the
book was actually written

00:11:33.900 --> 00:11:35.270
by J.K. Rowling.

00:11:35.270 --> 00:11:38.980
So the research question is,
did J.K. Rowling write the

00:11:38.980 --> 00:11:42.120
"Cuckoo's Calling" under the
pen name Robert Galbraith?

00:11:42.120 --> 00:11:45.550
And that's kind of impossible to
prove just by reading the book.

00:11:45.550 --> 00:11:47.700
And so as we heard earlier
with another question,

00:11:47.700 --> 00:11:49.970
you get the sense that,
maybe something is here

00:11:49.970 --> 00:11:51.880
but you're not entirely certain.

00:11:51.880 --> 00:11:57.820
So it involves comparative
analysis, didn't make it that,

00:11:57.820 --> 00:12:00.680
and comparing "Cuckoo's
Calling" with other books

00:12:00.680 --> 00:12:03.330
by J.K. Rowling, and
then recognizing patterns

00:12:03.330 --> 00:12:05.050
between her writing
and the writing that's

00:12:05.050 --> 00:12:06.350
in "Cuckoo's Calling".

00:12:06.350 --> 00:12:09.260
And you see at the citation at
the bottom is the full study

00:12:09.260 --> 00:12:11.960
by Patrick Juola
and he was a scholar

00:12:11.960 --> 00:12:13.730
who explored this question.

00:12:13.730 --> 00:12:17.410
So what he did is that
human reading led to a hunch

00:12:17.410 --> 00:12:19.180
about the authorship question,

00:12:19.180 --> 00:12:23.040
then Patrick Juola conducted
a stylometry analysis

00:12:23.040 --> 00:12:25.860
to complete a computational
comparison of diction

00:12:25.860 --> 00:12:28.510
between the book and
others written by Rowling,

00:12:28.510 --> 00:12:29.890
the "Harry Potter" series.

00:12:29.890 --> 00:12:32.530
So by comparing a set
of linguistic variables

00:12:32.530 --> 00:12:35.430
between the text of "Cuckoo's
Calling" for their text,

00:12:35.430 --> 00:12:37.900
one by Rowling and three
others by other authors,

00:12:37.900 --> 00:12:40.940
so as to extract her text,
the results suggested

00:12:40.940 --> 00:12:45.150
that J.K. Rowling's style was
the closest to Cuckoo's Calling.

00:12:45.150 --> 00:12:48.090
And so this provided a
ton of statistical proof

00:12:48.090 --> 00:12:51.650
of an authorial fingerprint and
Rowling then admitted that yes,

00:12:51.650 --> 00:12:55.050
she actually did write the book.

00:12:55.050 --> 00:12:57.970
Any questions about that one?

00:12:57.970 --> 00:13:00.600
The next one is a
question that may come

00:13:00.600 --> 00:13:03.420
up actually pretty often
when you're working

00:13:03.420 --> 00:13:06.820
with researchers is, you may
want to explore the question,

00:13:06.820 --> 00:13:09.630
what are themes common in
19th century literature?

00:13:09.630 --> 00:13:11.810
Again, this question is
impossible because you need

00:13:11.810 --> 00:13:13.260
to read a very large corpus

00:13:13.260 --> 00:13:15.430
of 19th century literature
that's impossible

00:13:15.430 --> 00:13:16.840
to do with human reading.

00:13:16.840 --> 00:13:19.760
So Matt Jockers and David
Mimno explored this question

00:13:19.760 --> 00:13:22.590
in their paper in
2012 and this is a--

00:13:22.590 --> 00:13:26.090
and so it involves
recognition of thematic patterns

00:13:26.090 --> 00:13:29.060
in the writing and then
comparative analysis.

00:13:29.060 --> 00:13:34.350
So what they did, this
is their approach, they--

00:13:34.350 --> 00:13:38.150
to explore the themes, they
ran large quantities of text

00:13:38.150 --> 00:13:41.340
through a statistical algorithm
to discover what we call topics

00:13:41.340 --> 00:13:43.510
and we're going to talk about
this a little later today.

00:13:43.510 --> 00:13:46.460
But topic modeling, just to
talk about in this context,

00:13:46.460 --> 00:13:50.160
is based on the idea that words
that co-occur are more likely

00:13:50.160 --> 00:13:53.630
to be related about the same
thing so that co-occurring words

00:13:53.630 --> 00:13:57.470
that the computer picks out
are represented as topics.

00:13:57.470 --> 00:14:01.830
And so here's a visualization
of one of their results.

00:14:01.830 --> 00:14:05.950
So it's a word cloud, like a
Wordle, that consists of a theme

00:14:05.950 --> 00:14:07.940
that they identified
algorithmically.

00:14:07.940 --> 00:14:10.520
So, this is one that's
labeled Female Fashion.

00:14:10.520 --> 00:14:14.690
So we see gown, silk,
dress, lace, ribbon.

00:14:14.690 --> 00:14:17.390
So these are words that tended
to co-occur across the corpus

00:14:17.390 --> 00:14:18.740
of text that they
were analyzing.

00:14:18.740 --> 00:14:21.890
And so through these results,
Jockers and Mimno were able

00:14:21.890 --> 00:14:25.170
to argue that authors from
this time period wrote a lot

00:14:25.170 --> 00:14:28.400
about what women wore
among other things.

00:14:28.400 --> 00:14:33.490
So any questions about that?

00:14:33.490 --> 00:14:37.710
And then the last
example that we looked

00:14:37.710 --> 00:14:40.400
at was what textual
characteristics constitute

00:14:40.400 --> 00:14:41.700
literary language?

00:14:41.700 --> 00:14:44.770
And this was a good one for
looking at with text analysis

00:14:44.770 --> 00:14:47.340
because it covers a very
large time span at looking

00:14:47.340 --> 00:14:48.920
at a breadth of literature.

00:14:48.920 --> 00:14:53.450
So it explores both change
over time as well as patterns.

00:14:53.450 --> 00:14:58.040
And so Ted Underwood and Jordan
Sellers used classification

00:14:58.040 --> 00:15:01.950
of algorithms to study what
characteristics constitute

00:15:01.950 --> 00:15:03.250
literary language.

00:15:03.250 --> 00:15:04.870
So what they did is
their approach is

00:15:04.870 --> 00:15:08.070
to show the difference in
words used in poetry, drama,

00:15:08.070 --> 00:15:10.960
and fiction with those
used nonfiction prose

00:15:10.960 --> 00:15:15.250
to demonstrate how literary
language developed over time.

00:15:15.250 --> 00:15:17.150
So they trained a
computational model

00:15:17.150 --> 00:15:20.050
to recognize literary
genres, and then they compared

00:15:20.050 --> 00:15:22.120
which words were most
frequently used over time

00:15:22.120 --> 00:15:25.050
in nonfiction prose
versus literary genres.

00:15:25.050 --> 00:15:28.800
And so these results that they
found demonstrated for poetry,

00:15:28.800 --> 00:15:32.780
drama, and fiction a tendency to
start using older English words.

00:15:32.780 --> 00:15:34.690
So again, here's
a visualization,

00:15:34.690 --> 00:15:38.540
one of the visualizations
they have in their paper,

00:15:38.540 --> 00:15:41.560
and it shows one way that
literary diction differs

00:15:41.560 --> 00:15:45.610
from nonfiction writing between
1700 all the way to 1900.

00:15:45.610 --> 00:15:49.490
So the y axis shows the ratio of
old words to new English words

00:15:49.490 --> 00:15:51.760
and the x axis shows years.

00:15:51.760 --> 00:15:55.040
And so the graph shows age
over time of the words.

00:15:55.040 --> 00:15:57.600
And so the words in poetry,
drama, and fiction are

00:15:57.600 --> 00:16:00.790
at the top in the purple line

00:16:00.790 --> 00:16:03.480
and the nonfiction prose
is the lower line in gray.

00:16:03.480 --> 00:16:06.710
And so the graph shows a
gradual increase in the use

00:16:06.710 --> 00:16:10.100
of new words, both
categories until about 1775,

00:16:10.100 --> 00:16:13.640
and then we see a divergence
with older words beginning

00:16:13.640 --> 00:16:16.330
to be more prevalent in
poetry, drama, and fiction.

00:16:16.330 --> 00:16:18.760
And so this analysis starts
to allow them to point

00:16:18.760 --> 00:16:21.230
to certain patterns
in literary language.

00:16:21.230 --> 00:16:24.640
So, next, we're going to talk a
little bit about the HathiTrust

00:16:24.640 --> 00:16:26.860
and the HathiTrust Digital
Library and how they work

00:16:26.860 --> 00:16:31.520
to support text analysis, and
again, talking a little bit

00:16:31.520 --> 00:16:33.230
about the context we'll
be working in today.

00:16:33.230 --> 00:16:36.070
So this is a diagram that shows
how the HathiTrust Digital

00:16:36.070 --> 00:16:38.920
Library and the HathiTrust
Research Center are

00:16:38.920 --> 00:16:41.240
in a basic text analysis
workflow.

00:16:41.240 --> 00:16:42.940
So know-- and we'll talk
about this a bit more.

00:16:42.940 --> 00:16:45.480
This is not a naturally
occurring workflow

00:16:45.480 --> 00:16:49.260
but an optional approach that
a researcher can initiate.

00:16:49.260 --> 00:16:50.560
And so we'll talk
a little bit more

00:16:50.560 --> 00:16:52.750
about how text analysis is
actually quite nonlinear

00:16:52.750 --> 00:16:54.050
and like this.

00:16:54.050 --> 00:16:56.770
So, in a basic text
analysis workflow,

00:16:56.770 --> 00:17:00.420
the researcher gathers digitized
text that's been scanned

00:17:00.420 --> 00:17:01.720
and OCR-ed.

00:17:01.720 --> 00:17:04.500
And OCR is optical
character recognition.

00:17:04.500 --> 00:17:06.670
And then they apply
computational methods

00:17:06.670 --> 00:17:07.970
to the text.

00:17:07.970 --> 00:17:09.270
So as we saw in those examples,

00:17:09.270 --> 00:17:11.570
they could do comparative
analysis, word counts,

00:17:11.570 --> 00:17:13.980
all sorts of things to
start discerning patterns

00:17:13.980 --> 00:17:15.900
in the work-- in the text.

00:17:15.900 --> 00:17:19.150
And then they analyze the
results with those algorithms.

00:17:19.150 --> 00:17:21.860
And where the HTRC, as we call
the HathiTrust Research Center,

00:17:21.860 --> 00:17:24.730
enters in is it allows
researchers to gather text

00:17:24.730 --> 00:17:27.480
at scale from the
HathiTrust Digital Library

00:17:27.480 --> 00:17:31.100
and then it also-- the HTRC
also provides tools and services

00:17:31.100 --> 00:17:33.120
that we'll work with
today that allow

00:17:33.120 --> 00:17:37.390
to apply computational methods
and tools to those texts

00:17:37.390 --> 00:17:39.160
from the HathiTrust
Digital Library.

00:17:39.160 --> 00:17:41.970
And then again, the research
though at the bottom still has

00:17:41.970 --> 00:17:43.390
to do their own analysis.

00:17:43.390 --> 00:17:46.420
So, as much as we work
with the researchers

00:17:46.420 --> 00:17:48.450
as US librarians work
with the researchers,

00:17:48.450 --> 00:17:51.850
the researchers then has
to do their own analysis.

00:17:51.850 --> 00:17:54.000
So, how many of you have heard

00:17:54.000 --> 00:17:55.800
of the HathiTrust,
have worked with it?

00:17:55.800 --> 00:17:57.100
Yes, everybody.

00:17:57.100 --> 00:17:58.830
So, I'm not going to
belabor this point,

00:17:58.830 --> 00:18:02.750
but just a very basic overview,
HathiTrust was founded in 2008

00:18:02.750 --> 00:18:04.630
and it's a community
research libraries committed

00:18:04.630 --> 00:18:06.590
to the long-term
curation and availability

00:18:06.590 --> 00:18:08.540
of the cultural record
and it's an offshoot

00:18:08.540 --> 00:18:10.230
of the Google Book
project and most

00:18:10.230 --> 00:18:11.570
of the digitization has happened

00:18:11.570 --> 00:18:13.730
at academic libraries
such as Uris.

00:18:13.730 --> 00:18:16.880
And so currently, it's over
120 partner institutions

00:18:16.880 --> 00:18:20.310
who are continuing to
contribute digitized material.

00:18:20.310 --> 00:18:23.850
So, the HathiTrust Digital
Library is concerned

00:18:23.850 --> 00:18:26.530
with the collecting,
preserving, and providing access

00:18:26.530 --> 00:18:28.990
to this content from the
partner institutions.

00:18:28.990 --> 00:18:32.970
It currently has over 60 million
volumes, about half in English,

00:18:32.970 --> 00:18:35.440
and material going back
to the 15th century,

00:18:35.440 --> 00:18:37.830
though the majority is
from the 20th century.

00:18:37.830 --> 00:18:40.120
And so, because of
the concentration--

00:18:40.120 --> 00:18:42.310
more of a concentration
in 20th century,

00:18:42.310 --> 00:18:45.770
about 63% of the material is
in copyright or restricted

00:18:45.770 --> 00:18:48.220
because right status
is known-- unknown.

00:18:48.220 --> 00:18:51.670
And then the remaining 37% is
what you can search and read

00:18:51.670 --> 00:18:54.100
on that-- through
that search interface.

00:18:54.100 --> 00:18:56.600
But there's currently a
copyright review process

00:18:56.600 --> 00:18:58.620
that a number of librarians,
some of your colleagues,

00:18:58.620 --> 00:19:00.990
may be involved in to
determine right status

00:19:00.990 --> 00:19:03.170
and then books are taken
or put into public domain

00:19:03.170 --> 00:19:05.310
as they determine that.

00:19:05.310 --> 00:19:07.460
And so-- And then
soon, very soon,

00:19:07.460 --> 00:19:08.850
the HathiTrust Digital Library--

00:19:08.850 --> 00:19:12.430
HathiTrust Research Center is
getting set up to provide access

00:19:12.430 --> 00:19:15.130
to that in-copyright
text through a process

00:19:15.130 --> 00:19:17.570
that we'll talk about
in a second,

00:19:17.570 --> 00:19:19.870
non-consumptive research.

00:19:20.940 --> 00:19:22.980
So, HathiTrust Research
Center is allow--

00:19:22.980 --> 00:19:25.440
is on the other end
of HathiTrust in terms

00:19:25.440 --> 00:19:27.750
of allowing researchers
to gather and analyze

00:19:27.750 --> 00:19:29.720
and produce new knowledge
from the text

00:19:29.720 --> 00:19:31.130
in the HathiTrust
Digital Library.

00:19:31.130 --> 00:19:34.960
So, researchers can do large
scale computational research

00:19:34.960 --> 00:19:37.260
through-- with HathiTrust text.

00:19:37.260 --> 00:19:40.260
The center is co-located both--

00:19:40.260 --> 00:19:42.580
and co-led by Indiana
University Bloomington

00:19:42.580 --> 00:19:44.200
and the University of Illinois.

00:19:44.200 --> 00:19:47.930
And both institutions do
a number of R&amp;D projects

00:19:47.930 --> 00:19:51.410
and user engagement
to enable researchers

00:19:51.410 --> 00:19:53.830
to do large scale
computational research.

00:19:53.830 --> 00:19:56.510
And so as I mentioned,

00:19:56.510 --> 00:19:58.910
the HathiTrust Research
Center is getting ready

00:19:58.910 --> 00:20:01.290
to provide access to
in-copyright text.

00:20:01.290 --> 00:20:03.450
Both with the in-copyright
text as well

00:20:03.450 --> 00:20:05.550
as the public domain text

00:20:05.550 --> 00:20:08.550
that researchers do large
scale computational research,

00:20:08.550 --> 00:20:10.920
we operate within
a non-consumptive

00:20:10.920 --> 00:20:12.220
research paradigm.

00:20:12.220 --> 00:20:15.440
So this is the formal definition
right here but basically,

00:20:15.440 --> 00:20:20.030
it allows a person to run tools
or algorithms against the text

00:20:20.030 --> 00:20:22.900
without actually
letting them read that.

00:20:22.900 --> 00:20:25.380
And this is because it
complies with copyright law

00:20:25.380 --> 00:20:28.570
because the researcher is
engaging with the ideas

00:20:28.570 --> 00:20:31.470
of the works rather than
their specific expressions.

00:20:31.470 --> 00:20:34.600
And so it's sometimes called
non-expressive use because of

00:20:34.600 --> 00:20:37.770
that particular type
of engagement.

00:20:37.770 --> 00:20:41.300
And so the non-consumptive
research protocol--

00:20:41.300 --> 00:20:44.860
not protocol, but paradigm is
the foundational underlying

00:20:44.860 --> 00:20:48.400
structure of how the HTRC
provides large scale access

00:20:48.400 --> 00:20:50.060
to the HathiTrust works.

00:20:50.060 --> 00:20:53.410
And so Eleanor actually
co-wrote a policy that goes

00:20:53.410 --> 00:20:55.740
into further detail about
our non-consumptive research

00:20:55.740 --> 00:20:59.700
approach and that's on the
HathiTrust website as well.

00:20:59.700 --> 00:21:07.090
So, this is our outline for
the remaining part of the day.

00:21:07.090 --> 00:21:09.350
And so, the outline of
the workshop is going to,

00:21:09.350 --> 00:21:12.310
in a very basic sense,
follow the research process

00:21:12.310 --> 00:21:15.690
of how a researcher might
conduct text analysis.

00:21:15.690 --> 00:21:18.320
So, after this, the next two
modules that I'll take you

00:21:18.320 --> 00:21:21.270
through will be about
gathering textual data

00:21:21.270 --> 00:21:24.130
from both the HathiTrust
and then more generally.

00:21:24.130 --> 00:21:27.790
And then we'll talk about
working with textual data

00:21:27.790 --> 00:21:30.510
and doing data cleaning.

00:21:30.510 --> 00:21:33.130
And then later, when
Eleanor takes over,

00:21:33.130 --> 00:21:35.530
we'll talk about
analyzing textual data,

00:21:35.530 --> 00:21:38.970
both again from the HathiTrust
and doing-- using Python.

00:21:38.970 --> 00:21:42.130
And then we'll end the day
with visualizing textual data

00:21:42.130 --> 00:21:43.830
and doing some data
visualization.

00:21:43.830 --> 00:21:46.960
So in each module, we're going
to do hands-on activities

00:21:46.960 --> 00:21:49.650
to think about how you would
approach a research question

00:21:49.650 --> 00:21:52.500
from each of these stages
and we're, in particular,

00:21:52.500 --> 00:21:56.050
going to use an example research
question and a case study

00:21:56.050 --> 00:21:58.150
at each step of the
research process.

00:21:58.150 --> 00:22:01.300
And again, we'll be using both
HathiTrust Research Center tools

00:22:01.300 --> 00:22:04.700
and non-HTRC tools as well so
that you're getting a sense

00:22:04.700 --> 00:22:07.990
of both HathiTrust, but also
more transferable skills

00:22:07.990 --> 00:22:09.290
as well.

00:22:09.290 --> 00:22:13.100
And so what our aim is, is
to really talk about the--

00:22:13.100 --> 00:22:15.610
give you a sense of the
processes, the approaches,

00:22:15.610 --> 00:22:18.320
and the tools that you can use
for engaging in text analysis.

00:22:18.320 --> 00:22:20.760
So you'll hopefully leave
today with a better sense

00:22:20.760 --> 00:22:23.300
of what is text analysis,
what's involved,

00:22:23.300 --> 00:22:25.650
and how might it be used by
researchers on your campus.

00:22:25.650 --> 00:22:28.160
So we're going to cover
both programming concepts

00:22:28.160 --> 00:22:29.860
and computational methods
but you're not going

00:22:29.860 --> 00:22:31.770
to be an expert coder
when you leave here.

00:22:31.770 --> 00:22:33.570
You'll have some
tools in your toolbox.

00:22:33.570 --> 00:22:36.680
And-- But this will provide
you kind of a jumping off point

00:22:36.680 --> 00:22:39.530
that you can then go to other
workshops, other tutorials.

00:22:39.530 --> 00:22:40.930
There's a wealth of resources

00:22:40.930 --> 00:22:44.200
out there that'll
give you more nuances

00:22:44.200 --> 00:22:47.130
that we won't be able cover
here in today's workshop.

00:22:47.130 --> 00:22:49.970
And so we'll just give you
an introduction into things

00:22:49.970 --> 00:22:53.040
to think about high-end
text analysis research.

00:22:53.040 --> 00:22:55.820
So throughout the workshop,
we're going to return

00:22:55.820 --> 00:22:58.790
to this sample reference
question for each stage.

00:22:58.790 --> 00:23:00.320
So imagine that a
student comes to you

00:23:00.320 --> 00:23:02.530
with this potential
research topic.

00:23:02.530 --> 00:23:03.830
I'm a student in
history who would

00:23:03.830 --> 00:23:06.990
like to incorporate digital
methods into my research.

00:23:06.990 --> 00:23:08.630
I study American politics.

00:23:08.630 --> 00:23:11.430
And in particular, I'd like
to examine how concepts

00:23:11.430 --> 00:23:13.870
such as liberty change
over time.

00:23:13.870 --> 00:23:16.240
So as we work in each
model throughout the day,

00:23:16.240 --> 00:23:18.120
we're going to practice
different approaches

00:23:18.120 --> 00:23:20.090
to answering this
reference question

00:23:20.090 --> 00:23:22.170
with these different
types of tools.

00:23:22.170 --> 00:23:24.320
And so this question hopefully
will also give you a better

00:23:24.320 --> 00:23:27.560
sense of how the role
of a librarian can play

00:23:27.560 --> 00:23:30.370
in helping the text
analysis research as well.

00:23:30.370 --> 00:23:34.060
And then we're also
going to look

00:23:34.060 --> 00:23:37.070
at throughout the workshop
a real-life case study.

00:23:37.070 --> 00:23:39.830
And this is a real research
project that we worked

00:23:39.830 --> 00:23:42.350
with in the HathiTrust
Research Center.

00:23:42.350 --> 00:23:45.380
It is called the Creativity
Boom and it was conducted

00:23:45.380 --> 00:23:48.520
by researcher Sam Franklin
who was at Brown at the time

00:23:48.520 --> 00:23:50.280
as a doctoral student.

00:23:50.280 --> 00:23:52.030
He's currently at
Stanford, I believe.

00:23:52.030 --> 00:23:55.180
And he worked with the HTRC
to explore the use and meaning

00:23:55.180 --> 00:23:56.590
of the words creative

00:23:56.590 --> 00:23:59.460
and creativity throughout
the 20th century.

00:23:59.460 --> 00:24:00.760
So throughout the
workshop today,

00:24:00.760 --> 00:24:03.140
we're going to discuss how
this researcher approached this

00:24:03.140 --> 00:24:05.230
question at each
stage of the process

00:24:05.230 --> 00:24:07.330
and that can give us
a more concrete sense

00:24:07.330 --> 00:24:10.370
of what somebody actually
did and considered

00:24:10.370 --> 00:24:14.100
in doing their research project
in the text-- throughout--

00:24:14.100 --> 00:24:15.970
for this text analysis project.

00:24:15.970 --> 00:24:19.420
And if you want to see the full
abstract in much more detail

00:24:19.420 --> 00:24:22.180
about the project, you can
follow that link on the Wiki--

00:24:22.180 --> 00:24:24.210
to the Wiki-- HTRC Wiki.

00:24:24.210 --> 00:24:29.660
And so, before we move on,
we also want to emphasize

00:24:29.660 --> 00:24:33.200
and to start off the day
that our workshop module

00:24:33.200 --> 00:24:35.730
and our outline may
just appear to suggest

00:24:35.730 --> 00:24:37.030
that the research workflow

00:24:37.030 --> 00:24:41.200
for our text analysis project is
linear and follows a very clean,

00:24:41.200 --> 00:24:43.670
predetermined process
like this diagram

00:24:43.670 --> 00:24:46.330
that somebody finds
text, prepares text,

00:24:46.330 --> 00:24:49.210
applies the algorithms,
and then analyzes

00:24:49.210 --> 00:24:51.130
and visualizes the results.

00:24:51.130 --> 00:24:54.370
So, while we'll talk
about it in that context,

00:24:54.370 --> 00:24:57.560
the reality is much
more like this.

00:24:57.560 --> 00:25:01.680
So, an actual text analysis
workflow is quite complicated,

00:25:01.680 --> 00:25:02.980
can be more complicated,

00:25:02.980 --> 00:25:06.290
and it's rarely a linear
sequential process.

00:25:06.290 --> 00:25:08.210
It's often more like
this diagram.

00:25:08.210 --> 00:25:10.670
So depending on what the
researcher is trying to do

00:25:10.670 --> 00:25:13.240
and the type of project, they
may search for some text,

00:25:13.240 --> 00:25:16.670
they may find it, they may clean
it, but then they figure out,

00:25:16.670 --> 00:25:17.970
oh, I need to get more.

00:25:17.970 --> 00:25:20.420
They may do exploratory
visualization and that says, oh,

00:25:20.420 --> 00:25:21.750
I need to get more text.

00:25:21.750 --> 00:25:23.310
And they may go all the
way to preparing the text

00:25:23.310 --> 00:25:25.960
for the final algorithm,
and still have to go back.

00:25:25.960 --> 00:25:28.030
So there's a lot
of small cycles,

00:25:28.030 --> 00:25:29.680
a lot of iterative steps.

00:25:29.680 --> 00:25:32.810
So, again, while we're using
these modules as we're going

00:25:32.810 --> 00:25:36.050
to talk about the text analysis
research process is a fairly

00:25:36.050 --> 00:25:39.760
sequential process, things are
a lot messier in real life.

00:25:39.760 --> 00:25:41.180
And then some of
you have experience

00:25:41.180 --> 00:25:43.840
in those kinds of
projects as well.

00:25:48.860 --> 00:25:50.970
OK. Welcome back, everyone.

00:25:50.970 --> 00:25:55.130
It sounds like we had some great
discussions over the break.

00:25:55.130 --> 00:25:57.450
OK. So we're going to get
started again and we're now--

00:25:57.450 --> 00:25:59.340
the next two modules
are going to be talking

00:25:59.340 --> 00:26:01.410
about accessing textual data.

00:26:01.410 --> 00:26:04.160
And some of your comments
earlier highlighted some

00:26:04.160 --> 00:26:05.460
of the challenges that
we're going to talk

00:26:05.460 --> 00:26:07.540
about in a little
more depth right now.

00:26:07.540 --> 00:26:11.460
So in addition to discussing the
variety of text data providers,

00:26:11.460 --> 00:26:13.100
we'll talk about
building a corpus

00:26:13.100 --> 00:26:15.030
from the HathiTrust
Digital Library

00:26:15.030 --> 00:26:16.810
and then how do we
import it into HTRC

00:26:16.810 --> 00:26:18.960
and that will be our
hands-on activity.

00:26:18.960 --> 00:26:21.180
So, again, here's an outline

00:26:21.180 --> 00:26:23.340
on what we'll be covering
in this next module.

00:26:23.340 --> 00:26:27.970
We'll explore the options for
finding data for text analysis

00:26:27.970 --> 00:26:30.280
and we'll also consider
the concept

00:26:30.280 --> 00:26:33.030
of dataset for text analysis.

00:26:33.030 --> 00:26:34.330
And then for our
hands-on activity,

00:26:34.330 --> 00:26:37.030
we're going to build a workset,
what we call text dataset

00:26:37.030 --> 00:26:39.470
as a workset from
the HathiTrust.

00:26:39.470 --> 00:26:42.010
And we'll talk about
text as data as corpora,

00:26:42.010 --> 00:26:43.500
which we'll talk about
a little bit more.

00:26:43.500 --> 00:26:46.370
And then we'll talk
about the case study

00:26:46.370 --> 00:26:49.070
and the reference
question and talk

00:26:49.070 --> 00:26:52.370
about in the case study how
Sam built his Creativity Corpus

00:26:52.370 --> 00:26:55.370
from HathiTrust volumes and
understand a little bit more

00:26:55.370 --> 00:26:59.710
about real-world strategies
for doing data collection.

00:26:59.710 --> 00:27:01.420
So by the end of this module,
where we're going to end

00:27:01.420 --> 00:27:04.090
up is you will have created
a collection of volumes

00:27:04.090 --> 00:27:06.780
from the HathiTrust Digital
Library and prepared it

00:27:06.780 --> 00:27:11.590
for analytics in the
HTRC Analytics portal.

00:27:13.730 --> 00:27:18.450
OK. So first, when we think
about collecting text as data,

00:27:18.450 --> 00:27:21.200
this is a great quote that
we pulled from an article

00:27:21.200 --> 00:27:22.500
by Thomas Padilla,

00:27:22.500 --> 00:27:24.930
who's currently a visiting
digital research services

00:27:24.930 --> 00:27:26.230
librarian at UNLV.

00:27:26.230 --> 00:27:30.630
And he wrote about three common
challenges to getting text.

00:27:30.630 --> 00:27:33.300
So first, the data of
interest has to be found

00:27:33.300 --> 00:27:35.380
as we heard some
of our comments.

00:27:35.380 --> 00:27:36.730
That can be a challenge.

00:27:36.730 --> 00:27:38.950
Second, the data
must be gettable.

00:27:38.950 --> 00:27:40.300
So again, how do we actually get

00:27:40.300 --> 00:27:42.460
to that text data
if it's impossible?

00:27:42.460 --> 00:27:44.110
And third, if it's
not already formed

00:27:44.110 --> 00:27:47.810
to the perfect wildest
dreams type of format,

00:27:47.810 --> 00:27:50.540
ways must be known of
getting that data into a state

00:27:50.540 --> 00:27:53.820
that is really usable for the
desired computational methods

00:27:53.820 --> 00:27:55.120
and tools.

00:27:55.120 --> 00:27:57.830
So before any text analysis
can actually be conducted,

00:27:57.830 --> 00:28:00.800
researchers need to find
that text data they need.

00:28:00.800 --> 00:28:03.970
And the process is not always
as easy as it may seem.

00:28:03.970 --> 00:28:07.170
And so today, in this next
module, in the next few minutes,

00:28:07.170 --> 00:28:10.960
we're going to explore
ways of finding text.

00:28:10.960 --> 00:28:13.490
So, there's plenty of
sources, as we've heard,

00:28:13.490 --> 00:28:16.840
for getting text data for
analysis but suitable text

00:28:16.840 --> 00:28:19.400
for computational analysis
is not always easy to get.

00:28:19.400 --> 00:28:22.370
And so these are some of
the challenges that you

00:28:22.370 --> 00:28:25.550
and researchers will encounter
in trying to get text data,

00:28:25.550 --> 00:28:29.170
so one, copyright and
licensing restrictions.

00:28:29.170 --> 00:28:32.330
So may that-- copyright
restrictions may prohibit users

00:28:32.330 --> 00:28:36.300
from accessing or publishing
with that text data or storing

00:28:36.300 --> 00:28:37.790
out on their own computers.

00:28:37.790 --> 00:28:41.180
Another common issue, our texts
are often provided in formats

00:28:41.180 --> 00:28:44.280
that require additional
processing before computational

00:28:44.280 --> 00:28:46.010
analysis can even be conducted.

00:28:46.010 --> 00:28:49.240
So, some documents are just
scanned images as we heard.

00:28:49.240 --> 00:28:52.080
And they need to be OCR-ed
and made machine-readable

00:28:52.080 --> 00:28:54.460
and cleaned before
text analysis.

00:28:54.460 --> 00:28:57.090
And then another issue is
the systems themselves.

00:28:57.090 --> 00:28:59.880
So the system may be hard for
the researchers to navigate

00:28:59.880 --> 00:29:01.180
to get at that text data

00:29:01.180 --> 00:29:04.270
and they might need higher-level
programming skills in order

00:29:04.270 --> 00:29:07.920
to be able to grapple with those
database systems and get the--

00:29:07.920 --> 00:29:10.310
pull the text out
of that system.

00:29:10.310 --> 00:29:13.620
And additionally, some
issues may be easier to deal

00:29:13.620 --> 00:29:15.250
with when you have just
a small amount of text,

00:29:15.250 --> 00:29:16.980
if you're just trying
to get 10 volumes.

00:29:16.980 --> 00:29:20.960
But when you think about larger
and larger corpora and working

00:29:20.960 --> 00:29:23.710
up the scale, but when
researchers try to work

00:29:23.710 --> 00:29:26.490
at that large scale, the
issues are magnified.

00:29:26.490 --> 00:29:28.920
So, some things may not
be apparent until you try

00:29:28.920 --> 00:29:31.590
to get that larger corpora.

00:29:31.590 --> 00:29:34.190
But things are being under the
works to hopefully make some

00:29:34.190 --> 00:29:35.500
of these challenges easier.

00:29:35.500 --> 00:29:38.360
So, now we're going to talk
about some of the common types

00:29:38.360 --> 00:29:40.460
of sources where users
can start looking

00:29:40.460 --> 00:29:42.300
for textual data that they want.

00:29:42.300 --> 00:29:45.890
So one source that's quite
common especially what

00:29:45.890 --> 00:29:49.250
researchers see right
away are vendor databases.

00:29:49.250 --> 00:29:52.610
So the databases that
we purchase and provide

00:29:52.610 --> 00:29:55.680
to our researchers,
those are often a source

00:29:55.680 --> 00:29:57.530
that people immediately point to

00:29:57.530 --> 00:29:59.470
but often there are
licensing restrictions

00:29:59.470 --> 00:30:01.810
because even though the
data that's provided is

00:30:01.810 --> 00:30:04.610
of higher quality, agreements--

00:30:04.610 --> 00:30:07.070
but the vendors prohibit
us from getting that data

00:30:07.070 --> 00:30:09.200
to actually use for
text analysis rather

00:30:09.200 --> 00:30:10.500
than just reading.

00:30:10.500 --> 00:30:12.840
So there's often
prohibitions on storing

00:30:12.840 --> 00:30:14.140
and publishing that data.

00:30:14.140 --> 00:30:17.830
So there's a few strategies that
we suggest that can make that--

00:30:17.830 --> 00:30:20.360
working with the
vendors easier, first,

00:30:20.360 --> 00:30:22.280
the library can negotiate
addendums

00:30:22.280 --> 00:30:25.600
to their licensing contracts
that allows researchers

00:30:25.600 --> 00:30:29.650
to get access to the text
data and permit text analysis.

00:30:29.650 --> 00:30:33.040
Some vendors are providing
services for text analysis.

00:30:33.040 --> 00:30:34.970
So they're building
tools and services

00:30:34.970 --> 00:30:36.650
for text mining their
collections.

00:30:36.650 --> 00:30:41.720
Gale is building a tool, JSTOR
Data for Research, LexisNexis.

00:30:41.720 --> 00:30:44.720
So some vendors do have
these services out there

00:30:44.720 --> 00:30:47.570
but they can be expensive and
sometimes hard to work with.

00:30:47.570 --> 00:30:51.160
And then in some cases, research
can ask for special permissions

00:30:51.160 --> 00:30:54.660
to mine a specific portion
of the vendor-provided data

00:30:54.660 --> 00:30:57.980
but sometimes that requires
fees as well for getting

00:30:57.980 --> 00:31:00.360
that special permission
and getting the access

00:31:00.360 --> 00:31:02.460
to store and publish the data.

00:31:02.460 --> 00:31:05.800
And so one example is
JSTOR Data for Research

00:31:05.800 --> 00:31:07.560
and that one is open
and available.

00:31:07.560 --> 00:31:10.870
It provides limited
types of corpora

00:31:10.870 --> 00:31:13.930
from the JSTOR collection.

00:31:13.930 --> 00:31:16.360
So one is vendor.

00:31:16.360 --> 00:31:17.910
And then digital collection.

00:31:17.910 --> 00:31:20.330
So we've heard some of the
biggest digital collections,

00:31:20.330 --> 00:31:23.790
the Biodiversity Heritage
Library, digitized newspapers

00:31:23.790 --> 00:31:25.890
from Chronicling America.

00:31:25.890 --> 00:31:28.140
So, those collections and
many, many more that we have

00:31:28.140 --> 00:31:31.140
in our libraries have a wealth
of text data as we've heard.

00:31:31.140 --> 00:31:35.680
But these data from libraries is
often siloed and not formulated

00:31:35.680 --> 00:31:37.160
for research at scale.

00:31:37.160 --> 00:31:39.920
And so when finding data
from digital collections,

00:31:39.920 --> 00:31:43.080
there are two key things
that we suggest that you look

00:31:43.080 --> 00:31:44.760
for that can make things easier.

00:31:44.760 --> 00:31:46.060
First, make sure that it's

00:31:46.060 --> 00:31:49.650
in accessible format,
often plain text.

00:31:49.650 --> 00:31:52.190
Second, look for digital
collection content

00:31:52.190 --> 00:31:54.770
that can allow for
bulk download as well.

00:31:54.770 --> 00:31:58.420
And the University of North
Carolina's DocSouth Data is an

00:31:58.420 --> 00:32:01.340
example of a digital collection
that's really great to be used

00:32:01.340 --> 00:32:04.040
for text analysis because
they've made it in plain text

00:32:04.040 --> 00:32:06.520
and they've made it
accessible for bulk download.

00:32:06.520 --> 00:32:08.650
And there's many others
out there as well.

00:32:08.650 --> 00:32:12.930
And then social media.

00:32:12.930 --> 00:32:15.910
So social media platforms are an
additional source for text data

00:32:15.910 --> 00:32:18.850
and research based on analysis

00:32:18.850 --> 00:32:22.020
of social media data is getting
increasingly popular and useful.

00:32:22.020 --> 00:32:25.540
So some social media
platforms do have systems

00:32:25.540 --> 00:32:27.570
that allow access
to some of the data.

00:32:27.570 --> 00:32:29.700
Twitter, in particular,
has an API

00:32:29.700 --> 00:32:33.260
and there are some third
party tools available as well

00:32:33.260 --> 00:32:35.820
that allow researchers
to plug in and get data.

00:32:35.820 --> 00:32:38.150
So we-- at Illinois,
we had Crimson Hexagon

00:32:38.150 --> 00:32:39.450
as one of those tools.

00:32:39.450 --> 00:32:41.690
So there's many out there
that might be accessible.

00:32:41.690 --> 00:32:44.120
So we're going to
talk a little bit more

00:32:44.120 --> 00:32:46.920
about Twitter APIs
later but this is some--

00:32:46.920 --> 00:32:52.100
the ability to get at social
media data is constantly

00:32:52.100 --> 00:32:55.140
evolving as well.

00:32:55.140 --> 00:33:02.720
So, let's talk a bit with
those qualifications in mind,

00:33:02.720 --> 00:33:04.490
we'd like you to
consider, you know,

00:33:04.490 --> 00:33:08.410
what would be the challenges
and weaknesses of trying

00:33:08.410 --> 00:33:11.440
to use those different
sources if the student wanted

00:33:11.440 --> 00:33:13.260
to a build a corpus
for political history?

00:33:13.260 --> 00:33:15.050
So what will be the strengths
and weaknesses for each

00:33:15.050 --> 00:33:18.400
of the sources of data for that
particular research question?

00:33:18.400 --> 00:33:20.410
These are some of the
things that you want to talk

00:33:20.410 --> 00:33:22.880
about with the researcher
as they're trying to figure

00:33:22.880 --> 00:33:25.270
out what text data
they want to use.

00:33:25.270 --> 00:33:27.730
So, do they have a
data source in mind?

00:33:27.730 --> 00:33:29.030
It is digitized?

00:33:29.030 --> 00:33:31.680
What's the period, place,
or person of interest?

00:33:31.680 --> 00:33:33.390
So where is the framing?

00:33:33.390 --> 00:33:35.670
And then how much
flexibility is needed

00:33:35.670 --> 00:33:37.280
for working with that data?

00:33:37.280 --> 00:33:40.890
Are there copyrighting and
licensing restrictions?

00:33:40.890 --> 00:33:43.110
How experienced is
the researcher?

00:33:43.110 --> 00:33:44.800
So they may need
programming skills

00:33:44.800 --> 00:33:46.290
to get at some of that data?

00:33:46.290 --> 00:33:47.860
Does the researcher
have funding?

00:33:47.860 --> 00:33:51.880
So, as was mentioned,
ProQuest or Wiley may charge

00:33:51.880 --> 00:33:53.180
for them to access that data.

00:33:53.180 --> 00:33:56.070
And then what format does the
researcher expect it to be in?

00:33:56.070 --> 00:33:57.990
So somebody raised
the issue of media.

00:33:57.990 --> 00:34:00.850
So depending on what kind of
media formats they want to use,

00:34:00.850 --> 00:34:02.460
do they want to just use text?

00:34:02.460 --> 00:34:06.170
Or is it going to have to
be converted in a format?

00:34:06.170 --> 00:34:07.640
Yeah. So these are just
some of the questions

00:34:07.640 --> 00:34:09.690
that you might think
about as you're talking

00:34:09.690 --> 00:34:12.230
with the researcher about
what type of data they need,

00:34:12.230 --> 00:34:15.100
when they need it, et cetera.

00:34:15.100 --> 00:34:20.710
So when you build a corpora,
there are several things--

00:34:20.710 --> 00:34:24.320
the process that you do
is to build that corpora.

00:34:24.320 --> 00:34:27.730
And so, as I've-- as we
mentioned, corpora are bodies

00:34:27.730 --> 00:34:29.700
of text and that's just the
term that we use to talk

00:34:29.700 --> 00:34:33.120
about the unstructured bodies of
text that you gather together.

00:34:33.120 --> 00:34:36.010
So, the researcher starts
building their corpora first

00:34:36.010 --> 00:34:38.430
by identifying text
and that's often done

00:34:38.430 --> 00:34:41.190
through a full text search,
so finding the key term,

00:34:41.190 --> 00:34:43.900
like creativity or key
phrase that's going

00:34:43.900 --> 00:34:45.710
to start identifying
the types of text

00:34:45.710 --> 00:34:47.010
that you'd be interested in.

00:34:47.010 --> 00:34:50.840
And then depending on those
specific research questions

00:34:50.840 --> 00:34:55.470
that somebody is using,
exploring, one can identify text

00:34:55.470 --> 00:34:58.240
with metadata, so looking
for certain authors,

00:34:58.240 --> 00:35:02.020
looking for a certain date
range, or even a specific genre.

00:35:02.020 --> 00:35:03.330
And that gets into thinking

00:35:03.330 --> 00:35:04.990
about how accurate
is the metadata

00:35:04.990 --> 00:35:06.850
in identifying these texts.

00:35:06.850 --> 00:35:09.620
And then often, it's a
combination of these two things

00:35:09.620 --> 00:35:12.840
in terms of identifying the text
first in full text searching

00:35:12.840 --> 00:35:15.050
and then looking through the
metadata to pick up things

00:35:15.050 --> 00:35:17.460
that may have not had
immediate identifiers

00:35:17.460 --> 00:35:19.610
through full text search.

00:35:19.610 --> 00:35:22.700
So once the texts
have been identified,

00:35:22.700 --> 00:35:24.960
the next step is
often deduplication,

00:35:24.960 --> 00:35:27.560
which means getting rid
of multiples of volumes.

00:35:27.560 --> 00:35:31.120
And so what to keep and what
to discard is really dependent

00:35:31.120 --> 00:35:33.470
on the researcher as we'll
see in some examples.

00:35:33.470 --> 00:35:37.940
So, some ways that a researcher
might do some deduplication

00:35:37.940 --> 00:35:40.250
might be based on
the OCR quality.

00:35:40.250 --> 00:35:44.510
So OCR quality can vary highly
from fairly dirty as we'll talk

00:35:44.510 --> 00:35:48.930
about to highly, highly curated,
choosing the earliest edition,

00:35:48.930 --> 00:35:51.240
editions without
forwords and afterwords.

00:35:51.240 --> 00:35:52.870
Again, there's-- and we'll see

00:35:52.870 --> 00:35:56.070
in Sam's project a little later
kind of all the different ways

00:35:56.070 --> 00:35:58.690
that you can choose
to deduplicate.

00:35:58.690 --> 00:36:01.880
And so now, we're going
to look-- we're going to--

00:36:01.880 --> 00:36:04.030
in this module, we're
going to focus a bit more

00:36:04.030 --> 00:36:08.310
on how the HTRC can allow
you to build text datasets

00:36:08.310 --> 00:36:10.110
through the-- from the
HathiTrust Digital Library

00:36:10.110 --> 00:36:13.010
collections and how you can
think about assisting users

00:36:13.010 --> 00:36:16.130
in building up that text
corpora for analysis.

00:36:16.130 --> 00:36:21.700
So the HTRC worksets are one way
that the HathiTrust allows users

00:36:21.700 --> 00:36:24.040
to create text corpora
to analyze.

00:36:24.040 --> 00:36:27.020
So what we call a workset
is basically a user-created

00:36:27.020 --> 00:36:30.080
collection of text from the
HathiTrust Digital Library.

00:36:30.080 --> 00:36:32.570
And so you can think of
them as textual datasets.

00:36:32.570 --> 00:36:34.370
And so the idea behind
the workset is

00:36:34.370 --> 00:36:36.710
that researchers can build
their own collections

00:36:36.710 --> 00:36:39.200
which is something they're used
to doing in the analog world

00:36:39.200 --> 00:36:43.060
through other means, through
long practices in humanities,

00:36:43.060 --> 00:36:46.530
and then these worksets can be
cited and used by other users,

00:36:46.530 --> 00:36:48.990
which helps ensure
reproducibility when we think

00:36:48.990 --> 00:36:50.530
about research reproducibility,

00:36:50.530 --> 00:36:54.020
and then people can test your
results using that same analysis

00:36:54.020 --> 00:36:55.650
with that same collection.

00:36:55.650 --> 00:36:58.250
And worksets are
often also suited

00:36:58.250 --> 00:37:00.460
for non-consumptive
access which means

00:37:00.460 --> 00:37:03.380
that users don't directly
consume the text themselves

00:37:03.380 --> 00:37:07.350
but they run tools or algorithms
against that collection of text

00:37:07.350 --> 00:37:09.650
for research analysis.

00:37:11.450 --> 00:37:15.000
So when you view a
workset in HTRC Analytics,

00:37:15.000 --> 00:37:17.250
you aren't seeing the text,
but you're seeing metadata

00:37:17.250 --> 00:37:20.160
about the volumes that you've
gathered in that workset.

00:37:20.160 --> 00:37:24.010
And so you can't read the text
in that interface on the left.

00:37:24.010 --> 00:37:26.050
And then, at the
right, you see the--

00:37:26.050 --> 00:37:28.760
what the workset is at
its core is a manifest.

00:37:28.760 --> 00:37:32.240
So this is a manifest of the
HathiTrust Digital Library IDs

00:37:32.240 --> 00:37:33.550
for each of those volumes.

00:37:33.550 --> 00:37:37.140
So how are we going
to build worksets?

00:37:37.140 --> 00:37:41.640
So, worksets are stored in
the HTRC Analytics system

00:37:41.640 --> 00:37:43.250
and you'll need to create
an account which all

00:37:43.250 --> 00:37:44.800
of you should have done tied

00:37:44.800 --> 00:37:47.120
to your institutional
email address.

00:37:47.120 --> 00:37:49.320
So you have to have a
user institutional address

00:37:49.320 --> 00:37:53.560
and not Gmail or Yahoo
to soar your worksets.

00:37:53.560 --> 00:37:55.650
And so there's two ways
to build your workset.

00:37:55.650 --> 00:37:57.260
So you can create collections

00:37:57.260 --> 00:38:00.320
with the HathiTrust Collection
Builder which we'll be using

00:38:00.320 --> 00:38:02.410
and import from the
Collection Builder

00:38:02.410 --> 00:38:05.960
into the analytics
portal or another way is

00:38:05.960 --> 00:38:08.910
to compile the volume
IDs from that manifest,

00:38:08.910 --> 00:38:12.770
compile them elsewhere, and
then upload it to Analytics

00:38:12.770 --> 00:38:17.620
and it pools the workset based
on that upload of volume IDs.

00:38:17.620 --> 00:38:19.840
We're going to do the
first method of importing

00:38:19.840 --> 00:38:21.540
from the Collection Builder.

00:38:21.540 --> 00:38:25.710
So, this-- again, here's our
sample reference question.

00:38:25.710 --> 00:38:28.970
And what we want to do is
create a textual dataset

00:38:28.970 --> 00:38:33.560
of volumes called-- that
the student might use

00:38:33.560 --> 00:38:36.740
for their research
question and then import it

00:38:36.740 --> 00:38:40.040
into HTRC Analytics
to do some analysis.

00:38:40.040 --> 00:38:45.340
Now, so what I'm going to do is
I'll first demo this hands-on

00:38:45.340 --> 00:38:47.970
activity and then we'll
allow you some time to kind

00:38:47.970 --> 00:38:51.000
of just go dive in and do
it based on the handout.

00:38:51.000 --> 00:38:53.270
So on page 5 and 6
are the directions

00:38:53.270 --> 00:38:54.950
for this next activity
we'll be doing.

00:38:54.950 --> 00:38:56.720
So what we're going to do
is we're going to login

00:38:56.720 --> 00:39:00.130
to the digital library, create
a collection of volumes.

00:39:00.130 --> 00:39:02.530
We're going to take a collection
from the public papers

00:39:02.530 --> 00:39:05.580
of the president of the United
States because, let's say,

00:39:05.580 --> 00:39:08.390
the student heard about that
collection and they're like,

00:39:08.390 --> 00:39:10.190
I think these volumes
might be useful

00:39:10.190 --> 00:39:11.490
for what I'd like to analyze.

00:39:11.490 --> 00:39:14.800
So, how do we gather
a collection for them

00:39:14.800 --> 00:39:17.800
of the public papers to the
president of the United States?

00:39:17.800 --> 00:39:21.220
So, I'm going to demo that now

00:39:21.220 --> 00:39:28.610
and then we'll have
time to then go in, so.

00:39:28.610 --> 00:39:33.180
We'll switch to the
digital library.

00:39:33.180 --> 00:39:35.080
And so what we'll do, we'll go

00:39:35.080 --> 00:39:37.920
to the digital library
and I'll log in.

00:39:37.920 --> 00:39:40.690
And if you're a HathiTrust
member, you should see your--

00:39:40.690 --> 00:39:43.750
you should be able to
find your institution.

00:39:43.750 --> 00:39:48.070
If not, you would
log in as a guest.

00:39:48.070 --> 00:39:50.370
So I'll login.

00:39:56.700 --> 00:39:59.620
OK. So we're going to go to
advanced full text search.

00:39:59.620 --> 00:40:08.410
And we're going to use this
exact phrase, public papers

00:40:08.410 --> 00:40:12.670
in the title and then this
exact phrase, United States.

00:40:12.670 --> 00:40:16.980
And then search.

00:40:16.980 --> 00:40:29.040
OK. So, we have-- So when
we build the workset,

00:40:29.040 --> 00:40:33.090
the first thing we do is we
identify the volumes we want.

00:40:33.090 --> 00:40:35.560
So, we click check, check box.

00:40:35.560 --> 00:40:39.670
You can do individual
ones or you can go

00:40:39.670 --> 00:40:41.620
up to the top and
just do select.

00:40:41.620 --> 00:40:45.400
Let's say I want 100 per page

00:40:45.400 --> 00:40:48.980
and then I can select
all on that page.

00:40:48.980 --> 00:40:51.100
And then, I'm going to
create a new collection.

00:40:51.100 --> 00:40:53.180
So I-- you can see I
have several collections,

00:40:53.180 --> 00:40:59.990
but I'll create a new one and
I'll call this collection.

00:40:59.990 --> 00:41:04.260
And then, you can add a
description if you want.

00:41:04.260 --> 00:41:06.450
And then, you can make
it private or public.

00:41:06.450 --> 00:41:08.910
Public if you have a very
highly curated workset.

00:41:08.910 --> 00:41:14.470
But we'll leave it as private
so that only I can use it.

00:41:14.470 --> 00:41:17.100
And then, I have a
collection created.

00:41:17.100 --> 00:41:20.160
And then, I go to
my collections.

00:41:20.160 --> 00:41:25.900
And I'm demoing this and
then we'll take some time

00:41:25.900 --> 00:41:28.430
to go through it.

00:41:28.430 --> 00:41:31.280
So, go to my collections.

00:41:31.280 --> 00:41:33.730
You find the collection
that you just created.

00:41:33.730 --> 00:41:35.220
So, I'll scroll down.

00:41:35.220 --> 00:41:38.860
Here it is.

00:41:38.860 --> 00:41:43.870
And then, you'll
download the TSV of the--

00:41:43.870 --> 00:41:47.620
the metadata as a TSV format.

00:41:47.620 --> 00:41:48.920
So, you download it.

00:41:48.920 --> 00:41:51.590
Save it in a place
where you can find it,

00:41:51.590 --> 00:41:54.070
some lines in my
downloads folder.

00:41:57.870 --> 00:42:01.600
And then, you upload
it into analytics.

00:42:01.600 --> 00:42:03.610
&gt;&gt; I know everything is on the--

00:42:03.610 --> 00:42:04.910
&gt;&gt; Harriett Green: Yeah.

00:42:04.910 --> 00:42:06.210
[ Inaudible ]

00:42:06.210 --> 00:42:07.510
Oh, I'm going to--
I'm showing it

00:42:07.510 --> 00:42:08.810
and then we'll have some
time for you to do it.

00:42:08.810 --> 00:42:10.110
&gt;&gt; OK.

00:42:10.110 --> 00:42:11.410
&gt;&gt; Harriett Green: Yeah.

00:42:11.410 --> 00:42:12.710
So, I'm just going to-- just
showing to see how you do it.

00:42:12.710 --> 00:42:15.920
And so once you do the
works-- Once you get into--

00:42:15.920 --> 00:42:17.870
Once you download it,
you'll go into the worksets

00:42:17.870 --> 00:42:20.510
and then you create a workset.

00:42:20.510 --> 00:42:26.340
So, let's first-- And so then--

00:42:26.340 --> 00:42:30.020
Well, and then I
upload it through here.

00:42:30.020 --> 00:42:34.680
So, I will-- I'm just showing
kind of like the basic steps.

00:42:37.970 --> 00:42:40.700
And then create.

00:42:40.700 --> 00:42:47.710
OK. So, let's-- And then
I create the workset

00:42:47.710 --> 00:42:49.010
and I have a workset.

00:42:49.010 --> 00:42:51.750
OK. So, we're just kind of
show you how to do the process.

00:42:51.750 --> 00:42:53.050
So now, we're going
to take a few minutes.

00:42:53.050 --> 00:42:54.350
And if everybody wants to take--

00:42:54.350 --> 00:42:56.150
go through the steps
and we'll circulate

00:42:56.150 --> 00:42:58.610
and help you build your workset.

00:42:58.610 --> 00:43:02.820
Now, we have the screenshots
actually for this as well both

00:43:02.820 --> 00:43:05.980
as a backup and also when
you take these materials

00:43:05.980 --> 00:43:07.280
and try to use them.

00:43:07.280 --> 00:43:09.550
So, these are available
for doing that.

00:43:09.550 --> 00:43:12.230
But just to review the steps
again for building the workset,

00:43:12.230 --> 00:43:15.510
we went into the digital
library interface.

00:43:15.510 --> 00:43:18.920
The core search, we logged in.

00:43:18.920 --> 00:43:22.010
So, everybody should log in
with their institutional address

00:43:22.010 --> 00:43:27.060
or their Google guest.

00:43:27.060 --> 00:43:30.010
Then we all search for volumes.

00:43:30.010 --> 00:43:33.000
And so, we use the advanced
full text search to search

00:43:33.000 --> 00:43:34.730
for United States
and public papers.

00:43:34.730 --> 00:43:36.720
So, that really narrowed
our workset

00:43:36.720 --> 00:43:39.170
because as you may have searched
in HathiTrust Digital Library,

00:43:39.170 --> 00:43:42.360
you can get thousands
of results very fast.

00:43:42.360 --> 00:43:47.940
And then, we filtered the
volumes to see one thing

00:43:47.940 --> 00:43:51.210
that was brought up in the
exercise that was good to note.

00:43:51.210 --> 00:43:53.980
You need to select volumes
that are full text view.

00:43:53.980 --> 00:43:56.890
So, that's the only way that the
algorithm currently will read

00:43:56.890 --> 00:43:59.000
the workset as if you have
full text view volume.

00:43:59.000 --> 00:44:00.570
So, that's something
to keep in mind

00:44:00.570 --> 00:44:02.850
as you're filtering as well.

00:44:02.850 --> 00:44:04.750
And then, we added
them the collection

00:44:04.750 --> 00:44:07.210
by creating a new collection.

00:44:07.210 --> 00:44:10.550
One thing to note about when
you create your collection name,

00:44:10.550 --> 00:44:15.620
it has to have the characters
only A through Z, 0 through 9.

00:44:15.620 --> 00:44:18.470
So you can't add
different characters.

00:44:18.470 --> 00:44:19.770
So, keep that in mind as well.

00:44:19.770 --> 00:44:23.960
So if somebody is creating a
collection and they have to--

00:44:23.960 --> 00:44:25.820
they went into problems,
that's why.

00:44:25.820 --> 00:44:29.070
And then, go to my
collections to view it.

00:44:29.070 --> 00:44:33.200
And then, we clicked in
to view the collection.

00:44:33.200 --> 00:44:36.290
And then, you want to
download the TSV file.

00:44:36.290 --> 00:44:40.640
And then, we went into
analytics and logged in to that.

00:44:41.820 --> 00:44:46.920
And then, we clicked on the
worksets from of the menu.

00:44:46.920 --> 00:44:51.180
And then you should have
chosen Create a Workset.

00:44:52.530 --> 00:44:56.160
And then you wanted to-- you
should've uploaded the TSV file

00:44:56.160 --> 00:44:59.570
so that it could
read your workset.

00:44:59.570 --> 00:45:02.260
And, again, keeping in mind
that your workset has to have A

00:45:02.260 --> 00:45:06.000
through Z, 0 through 9 and
only selected characters,

00:45:06.000 --> 00:45:08.220
and then you have
the created workset.

00:45:08.220 --> 00:45:11.180
So, let's-- in our
closing minutes,

00:45:11.180 --> 00:45:13.960
let's take a look
at what Sam did.

00:45:13.960 --> 00:45:15.740
So, he searched across
the full text

00:45:15.740 --> 00:45:18.070
of HathiTrust Digital
Library for creative,

00:45:18.070 --> 00:45:20.040
that's the root of it.

00:45:20.040 --> 00:45:22.050
And he wanted results

00:45:22.050 --> 00:45:24.240
that either contained
creative of creativity.

00:45:24.240 --> 00:45:25.630
So that's how he searched.

00:45:25.630 --> 00:45:29.370
And then after the search, he
had an initial list of over 2--

00:45:29.370 --> 00:45:31.440
of 2.7 million volumes.

00:45:31.440 --> 00:45:33.590
But there were some
duplicate materials.

00:45:33.590 --> 00:45:38.550
So, he decided the duplicates
he wanted to get rid off was--

00:45:38.550 --> 00:45:41.320
have these different
editions of the same work,

00:45:41.320 --> 00:45:43.830
but discard multiple
copies of the same edition.

00:45:43.830 --> 00:45:46.790
So in that case as
Eleanor was mentioning,

00:45:46.790 --> 00:45:49.510
three libraries may have
digitized the same edition

00:45:49.510 --> 00:45:51.050
and it will add all three

00:45:51.050 --> 00:45:53.650
if you don't identify
those same copies.

00:45:53.650 --> 00:45:56.800
And so, again, different
rules for different projects.

00:45:56.800 --> 00:45:58.750
So, Sam decided he
wanted different editions

00:45:58.750 --> 00:46:00.210
but not the same edition.

00:46:00.210 --> 00:46:01.630
Somebody else may want--

00:46:01.630 --> 00:46:04.750
have a different criteria
for their project.

00:46:04.750 --> 00:46:07.690
And the deduplication
was not done by hand

00:46:07.690 --> 00:46:11.260
with over a million volumes, but
he used the metadata to filter

00:46:11.260 --> 00:46:14.750
through and then filter out
those volumes he didn't want.

00:46:14.750 --> 00:46:17.280
And now, we're going to go
straight into the other version,

00:46:17.280 --> 00:46:22.350
other ways of gathering text
data for doing bulk retrieval.

00:46:22.350 --> 00:46:26.450
So as some of the final
questions asked, what happens

00:46:26.450 --> 00:46:28.600
when you want more than
a thousand volumes?

00:46:28.600 --> 00:46:32.320
What happens when you need to
do larger scale text retrieval?

00:46:32.320 --> 00:46:34.970
So now, we're going to
talk about some strategies

00:46:34.970 --> 00:46:42.140
for doing web-- gathering text
from web in bulk using APIs

00:46:42.140 --> 00:46:46.180
and doing file transfers
together text data.

00:46:46.180 --> 00:46:48.670
So, this here is an
outline of what we'll cover

00:46:48.670 --> 00:46:50.160
in the next 45 minutes.

00:46:50.160 --> 00:46:53.060
We'll take a look at how
researchers can access data

00:46:53.060 --> 00:46:56.410
in the automated way that allows
them to download text in bulk.

00:46:56.410 --> 00:46:59.730
So, we're going to explore
like basic APR protocols.

00:46:59.730 --> 00:47:01.990
So if you heard API and
you're a little scared of it,

00:47:01.990 --> 00:47:03.670
don't worry, we're going
to break that down for you.

00:47:03.670 --> 00:47:06.410
And then, we're going to do some
hands-on with the command line

00:47:06.410 --> 00:47:11.190
and use wget which is a
fairly basic script program

00:47:11.190 --> 00:47:12.930
to do web scraping.

00:47:12.930 --> 00:47:15.920
And then at the end,
we'll conclude

00:47:15.920 --> 00:47:20.570
by seeing how Sam gathered his
data for his Creativity Corpus.

00:47:20.570 --> 00:47:23.570
So, where we're going to end up
is we're going to run command

00:47:23.570 --> 00:47:25.460
from the command line
interface and we're going

00:47:25.460 --> 00:47:27.470
to scrape text from a webpage.

00:47:27.470 --> 00:47:34.890
OK. So for text analysis
projects, more--

00:47:34.890 --> 00:47:37.450
most researchers will
need more than 10 texts.

00:47:37.450 --> 00:47:39.850
They're going to be working
with hundreds, thousands

00:47:39.850 --> 00:47:41.520
or even millions of texts.

00:47:41.520 --> 00:47:44.060
And getting all this data
can be very time-intensive.

00:47:44.060 --> 00:47:47.300
And so as people have pointed
out already just pointing

00:47:47.300 --> 00:47:48.840
and clicking is pretty
inefficient.

00:47:48.840 --> 00:47:50.370
So, it's really necessary

00:47:50.370 --> 00:47:53.170
to automate data
gathering when possible.

00:47:53.170 --> 00:47:55.300
So, we're going to walk about
in the next few minutes what are

00:47:55.300 --> 00:47:59.120
some options for doing automated
data retrieval and we're going

00:47:59.120 --> 00:48:01.200
to then practice some
basic web scraping.

00:48:01.200 --> 00:48:04.130
So, one, there's a few--

00:48:04.130 --> 00:48:07.010
three different ways in
particular we want to highlight

00:48:07.010 --> 00:48:09.170
for how you automate
the retrieval of text.

00:48:09.170 --> 00:48:11.430
And this is, again, especially
important when you're thinking

00:48:11.430 --> 00:48:13.600
about large scale text analysis.

00:48:13.600 --> 00:48:16.910
So, first, providers can
make their files available

00:48:16.910 --> 00:48:18.850
through file transfer
mechanisms.

00:48:18.850 --> 00:48:22.660
So, file transfer
protocol or FTP or SFTP,

00:48:22.660 --> 00:48:24.940
if it's a secure file
transfer protocol,

00:48:24.940 --> 00:48:28.300
are ways for moving especially
large files from one place

00:48:28.300 --> 00:48:30.260
to another on the internet.

00:48:30.260 --> 00:48:33.360
And then another way of
transferring files is rsync,

00:48:33.360 --> 00:48:35.130
which is what the
HathiTrust uses.

00:48:35.130 --> 00:48:37.640
And this is efficient because
it only sends the differences

00:48:37.640 --> 00:48:41.230
between files from the source
location to the destination.

00:48:41.230 --> 00:48:43.870
So, you run rsync
from the command line,

00:48:43.870 --> 00:48:47.780
and this is what is used to
download extracted features data

00:48:47.780 --> 00:48:53.170
which we'll work with
a bit later on today.

00:48:53.170 --> 00:48:57.220
Another way to bulk retrieval
of text data content is

00:48:57.220 --> 00:48:59.810
through web scraping,
which means as some

00:48:59.810 --> 00:49:03.550
of you may have heard about,
grabbing text from the web.

00:49:03.550 --> 00:49:05.280
So, there's lots of text
out there on the web

00:49:05.280 --> 00:49:07.460
that could be used
for text analysis.

00:49:07.460 --> 00:49:10.080
And web scraping helps
you avoid just copying

00:49:10.080 --> 00:49:11.880
and pasting every
single web page.

00:49:11.880 --> 00:49:15.210
So, there's a couple ways
to scrape text from the web.

00:49:15.210 --> 00:49:17.760
You can run commands on the
command line which we're going

00:49:17.760 --> 00:49:19.490
to talk about a little later.

00:49:19.490 --> 00:49:20.980
And then you can write a script.

00:49:20.980 --> 00:49:24.230
And a script is basically a
file that contains commands,

00:49:24.230 --> 00:49:27.430
programming statements that
the computer will then follow

00:49:27.430 --> 00:49:29.500
to then carry out those actions

00:49:29.500 --> 00:49:32.000
and scrape the web
from the website.

00:49:32.000 --> 00:49:34.740
And so, we're going to be using
scripts a little later on today

00:49:34.740 --> 00:49:36.870
and some of our hands-on
activities,

00:49:36.870 --> 00:49:38.450
but just to introduce
that concept.

00:49:38.450 --> 00:49:40.680
And then, there's a
web scraping software.

00:49:40.680 --> 00:49:42.590
So if you don't want to
write scripts, there's things

00:49:42.590 --> 00:49:47.390
such as Beautiful Soup,
Web Scraper IO or Kimono

00:49:47.390 --> 00:49:49.660
that do the work for you.

00:49:49.660 --> 00:49:52.140
So, we're going to
come back to that.

00:49:52.140 --> 00:49:53.700
But a few things before--

00:49:53.700 --> 00:49:55.560
We're going to come back to
web scraping a little later.

00:49:55.560 --> 00:49:57.750
But we wanted to
highlight a few things

00:49:57.750 --> 00:50:01.130
that are just good citizen
practices when you decide

00:50:01.130 --> 00:50:04.480
to do web scraping because
web scraping could put a large

00:50:04.480 --> 00:50:08.010
workload on a server, and this
could upset the data holder,

00:50:08.010 --> 00:50:09.960
if they're like why is somebody
trying to take down my website,

00:50:09.960 --> 00:50:11.390
and you may be doing
that unintentionally.

00:50:11.390 --> 00:50:14.580
So, you know, a lot of data
providers are more than willing

00:50:14.580 --> 00:50:17.640
to share, but it's good to
just ask them, you know,

00:50:17.640 --> 00:50:21.570
notify them that-- ask them
for access and check for an API

00:50:21.570 --> 00:50:24.930
which is less intensive on
their website, and that's a way

00:50:24.930 --> 00:50:27.060
that they provide
text data to you.

00:50:27.060 --> 00:50:31.940
And if you still have to use web
scraping, if there is an API,

00:50:31.940 --> 00:50:34.010
we suggest that you
time your requests.

00:50:34.010 --> 00:50:36.240
And so, there's a delay
between the hits on the server.

00:50:36.240 --> 00:50:39.090
So, this is not only
polite, but it also signifies

00:50:39.090 --> 00:50:40.430
that you're a good citizen

00:50:40.430 --> 00:50:42.230
and that you're doing
this not maliciously.

00:50:42.230 --> 00:50:46.340
So, just kind of timing
those requests and make sure

00:50:46.340 --> 00:50:47.640
that you're not taking
down the site.

00:50:47.640 --> 00:50:52.500
Now, APIs, we're going to talk
a couple few minutes bout APIs.

00:50:52.500 --> 00:50:55.290
So, this is another way to
automate the retrieval of text.

00:50:55.290 --> 00:50:59.010
So, API stands for application
programming interfaces.

00:50:59.010 --> 00:51:01.930
And so, APIs are basically
instructions written in code

00:51:01.930 --> 00:51:04.770
for accessing systems
or collections

00:51:04.770 --> 00:51:07.950
and they provide digital
pathways to and from content.

00:51:07.950 --> 00:51:10.060
And so basically, for example

00:51:10.060 --> 00:51:13.730
when you use the Amazon's API
you can display information

00:51:13.730 --> 00:51:17.990
about the items or for sale
on Amazon or gather or items,

00:51:17.990 --> 00:51:22.330
data about sites and items
not available for Amazon just

00:51:22.330 --> 00:51:24.300
by using their API and
grabbing that data.

00:51:24.300 --> 00:51:27.250
So, you might think of an
API like a mailbox and a key.

00:51:27.250 --> 00:51:30.670
So, you open the door with the
key and then that allows you

00:51:30.670 --> 00:51:33.790
to retrieve or deposit
items via that API.

00:51:33.790 --> 00:51:35.530
So usually, you have
to write code

00:51:35.530 --> 00:51:37.520
to retrieve content via APIs.

00:51:37.520 --> 00:51:40.390
But some of them do have
graphical user interfaces what

00:51:40.390 --> 00:51:43.320
we call GUIs that make that
process a little easier

00:51:43.320 --> 00:51:45.020
and more consistent
and convenient.

00:51:45.020 --> 00:51:48.300
So again, a number of
digital providers have APIs.

00:51:48.300 --> 00:51:52.360
So, Amazon as I mentioned,
YouTube, a number of places,

00:51:52.360 --> 00:51:55.010
a number of sites make
those APIs available,

00:51:55.010 --> 00:51:57.310
Twitter which we've just--
we've mentioned earlier.

00:51:57.310 --> 00:51:59.740
So, you can grab data, you
can grab a certain amount

00:51:59.740 --> 00:52:02.300
of metadata and some
of them are limited.

00:52:02.300 --> 00:52:04.940
So, Twitter can limit
the number of tweets

00:52:04.940 --> 00:52:07.690
that you can then display
on a non-Twitter website.

00:52:07.690 --> 00:52:08.990
But then, there's also things

00:52:08.990 --> 00:52:10.640
like the Chronicling
America API.

00:52:10.640 --> 00:52:15.110
So, our hosts do have
API that allow people

00:52:15.110 --> 00:52:17.880
to gather the digitized
newspaper data.

00:52:17.880 --> 00:52:20.340
So, there's a variety
of examples out there,

00:52:20.340 --> 00:52:24.760
of ways that we can gather data
from both digital collections

00:52:24.760 --> 00:52:27.420
as well as commercial
providers as well.

00:52:27.420 --> 00:52:33.040
So, the HathiTrust does
have a bibliographic API.

00:52:33.040 --> 00:52:36.790
And so access to some
APIs is available

00:52:36.790 --> 00:52:40.130
through a publicly available URL
which is what HathiTrust does

00:52:40.130 --> 00:52:42.000
for its bibliographic API.

00:52:42.000 --> 00:52:44.490
And so, that's the kind we're
going to experiment today.

00:52:44.490 --> 00:52:46.070
So, this API is primary

00:52:46.070 --> 00:52:49.680
for retrieving bibliographic
metadata and volume data.

00:52:49.680 --> 00:52:52.520
And so, there's other APIs for
gathering other types of data,

00:52:52.520 --> 00:52:55.590
but this is one of the more
straightforward ones and--

00:52:55.590 --> 00:52:56.980
to practice with.

00:52:56.980 --> 00:52:59.740
And so, we use this
bibliographic API.

00:52:59.740 --> 00:53:02.820
We're going to gather the
metadata using a specially

00:53:02.820 --> 00:53:06.270
formatted URL plus
the volume ID number.

00:53:06.270 --> 00:53:07.820
And so, we're going to talk
about, we're going to do

00:53:07.820 --> 00:53:10.480
in this next activity,
a short activity

00:53:10.480 --> 00:53:16.430
to gather a bibliographic
data through the bib API.

00:53:16.430 --> 00:53:19.580
So, we're going to use--

00:53:19.580 --> 00:53:20.890
So the hands-on activity
we're going to do.

00:53:20.890 --> 00:53:23.890
And then, the instructions
are on page 7.

00:53:23.890 --> 00:53:25.910
And this-- And I really
recommend you look

00:53:25.910 --> 00:53:28.520
at the instructions because it
really lays out how to do this.

00:53:28.520 --> 00:53:30.590
So, what we're going
to do is we're going

00:53:30.590 --> 00:53:33.830
to search the HathiTrust for
a volume and then you're going

00:53:33.830 --> 00:53:38.650
to click on the full or limited
search volume and then you want

00:53:38.650 --> 00:53:40.950
to find the volume ID.

00:53:40.950 --> 00:53:44.410
So, what we're going to
do, I'll demonstrate it.

00:53:44.410 --> 00:53:48.160
And then, you're going to
use the URL in the format.

00:53:48.160 --> 00:53:51.190
You should see a little
diagram, a little worksheet

00:53:51.190 --> 00:53:52.510
that has the different spaces.

00:53:52.510 --> 00:53:55.810
And so, you're going to
use that formatted URL,

00:53:55.810 --> 00:53:58.340
and you should see
this on your handout.

00:53:58.340 --> 00:54:00.330
You're going to use
this formatted URL

00:54:00.330 --> 00:54:04.410
to retrieve the bibliographic
data using the HathiTrust

00:54:04.410 --> 00:54:05.710
bib ID.

00:54:05.710 --> 00:54:09.330
So, let me demonstrate that and
we'll walk through it together.

00:54:12.530 --> 00:54:14.400
And then, we'll give
a couple minutes

00:54:14.400 --> 00:54:16.560
for you to try on your own.

00:54:16.560 --> 00:54:22.900
OK. So, what you want to do, so
let's say that we have public--

00:54:25.710 --> 00:54:30.700
so, let's say I want to
retrieve the bibliographic data

00:54:30.700 --> 00:54:34.910
for one of these volumes.

00:54:34.910 --> 00:54:40.850
So, I click into the full
view, so I found this book.

00:54:40.850 --> 00:54:45.760
And when you find the
book, everything from the--

00:54:45.760 --> 00:54:50.950
you go to the URL, everything
from-- it says ID equals,

00:54:50.950 --> 00:54:52.580
and then you see a code.

00:54:52.580 --> 00:54:55.450
And the first few
letters are the code

00:54:55.450 --> 00:54:57.630
for that digitizing institution.

00:54:57.630 --> 00:55:02.890
And if you highlight everything
from that code to the semicolon,

00:55:02.890 --> 00:55:05.290
that's the volume ID you want

00:55:05.290 --> 00:55:07.220
for the HathiTrust
Digital Library.

00:55:07.220 --> 00:55:11.580
So then, I'll go back.

00:55:11.580 --> 00:55:13.490
So then if I go back-- And so,

00:55:13.490 --> 00:55:17.050
each of you should see
this worksheet here

00:55:17.050 --> 00:55:19.010
and you can write it.

00:55:19.010 --> 00:55:25.030
So, I just want to
copy this format.

00:55:25.030 --> 00:55:26.330
So, what you're going
to type in--

00:55:26.330 --> 00:55:29.640
or you can copy this from the
materials you've downloaded.

00:55:29.640 --> 00:55:34.660
You have catalog.hathitrust.org,
api/volumes.

00:55:34.660 --> 00:55:36.630
The next step is
brief versus full,

00:55:36.630 --> 00:55:39.530
so that will just say
how long the record is.

00:55:39.530 --> 00:55:42.540
And then, we're going to put
here the ID type that I found

00:55:42.540 --> 00:55:46.810
in that URL, and then .json.

00:55:48.160 --> 00:55:52.730
So, I copy that from
here and open a new tab.

00:55:52.730 --> 00:55:56.300
So, I'll delete that.

00:55:56.300 --> 00:56:00.360
And so, here is the
first part, volumes.

00:56:00.360 --> 00:56:04.820
And then, let's say
I want to a full.

00:56:04.820 --> 00:56:08.220
And then, the ID
type would be htid.

00:56:08.220 --> 00:56:17.120
And as you can see,
there's several types of ID.

00:56:17.120 --> 00:56:18.920
We're just going
to use the htid,

00:56:18.920 --> 00:56:21.940
but you could also use the
OCLC, the record number,

00:56:21.940 --> 00:56:25.190
the library congress
number, catalog number.

00:56:25.190 --> 00:56:28.870
So, there's several types of
volume ID that you could use.

00:56:28.870 --> 00:56:36.460
So if I copy the URL and I
say full and then I use htid

00:56:36.460 --> 00:56:39.520
and then I go back to
the book where I found it

00:56:39.520 --> 00:56:47.270
and I'll copy this ID number
right here and then I paste

00:56:47.270 --> 00:56:56.230
that in and then .json and I hit
enter, and then that is the API

00:56:56.230 --> 00:56:58.050
for the bibliographic data.

00:56:58.050 --> 00:56:59.870
Does that make sense?

00:56:59.870 --> 00:57:02.400
So, let's take a couple minutes.

00:57:02.400 --> 00:57:04.490
We'll circulate if
you want to try it.

00:57:04.490 --> 00:57:07.930
And again, you might use that
worksheet to kind of fill in,

00:57:07.930 --> 00:57:12.530
if you want to write down each
piece and then paste it all in.

00:57:12.530 --> 00:57:15.390
So, we're going to move on to--
Again, this was the introduction

00:57:15.390 --> 00:57:19.350
and there's much more out there
both on the HathiTrust API

00:57:19.350 --> 00:57:21.800
that you're welcome
to read and delve into

00:57:21.800 --> 00:57:23.660
or ask some questions.

00:57:23.660 --> 00:57:26.430
But we're going to start and
dive into the command line

00:57:26.430 --> 00:57:28.730
and do some web scraping now.

00:57:28.730 --> 00:57:32.620
And so, this is Bash shell
in terminal on the Mac.

00:57:32.620 --> 00:57:36.780
And so, we're going talk
about this a bit more,

00:57:36.780 --> 00:57:38.790
but this is kind
of what it looks

00:57:38.790 --> 00:57:40.940
like if you're on your computer.

00:57:40.940 --> 00:57:42.930
So, what is the command line?

00:57:42.930 --> 00:57:46.270
So, command line is a text-based
interface that takes in commands

00:57:46.270 --> 00:57:49.000
and passes them to the
computer's operating system

00:57:49.000 --> 00:57:52.160
to run and accomplish
a wide range of tasks.

00:57:52.160 --> 00:57:55.850
So, the interfaces usually
implement what we call a shell.

00:57:55.850 --> 00:57:58.250
So on a Mac, that shell
is called terminal,

00:57:58.250 --> 00:57:59.810
and that is a Bash shell.

00:57:59.810 --> 00:58:01.720
On Windows, it's command prompt.

00:58:01.720 --> 00:58:03.020
It's not the same thing.

00:58:03.020 --> 00:58:04.690
They do not speak
the same language.

00:58:04.690 --> 00:58:06.540
So when we talk about a
Bash shell, we're talking

00:58:06.540 --> 00:58:08.630
about a Unix-based shell.

00:58:08.630 --> 00:58:10.140
So, Mac is default.

00:58:10.140 --> 00:58:11.570
It's a default Unix system.

00:58:11.570 --> 00:58:14.490
And so when you open terminal,
you already have a Bash shell.

00:58:14.490 --> 00:58:18.810
For Windows, you want to
install GitBash or Cygwin.

00:58:18.810 --> 00:58:22.380
Windows 10 does come
installed with a Bash shell.

00:58:22.380 --> 00:58:24.270
But anything earlier than
that, you're going to have

00:58:24.270 --> 00:58:26.550
to install another program.

00:58:26.550 --> 00:58:31.920
And so avoiding-- In order to
avoid all these system issues,

00:58:31.920 --> 00:58:34.230
because we see a variety
of computers out here,

00:58:34.230 --> 00:58:36.280
we're using PythonAnywhere.

00:58:36.280 --> 00:58:39.790
So, both to avoid the operating
system issues now and then also

00:58:39.790 --> 00:58:43.730
for teaching in the future
PythonAnywhere works well

00:58:43.730 --> 00:58:45.810
when you're trying to
teach workshops based

00:58:45.810 --> 00:58:47.760
on our experience.

00:58:47.760 --> 00:58:50.830
So, PythonAnywhere
is a browser-based

00:58:50.830 --> 00:58:52.130
programming environment.

00:58:52.130 --> 00:58:55.670
So, you can write code,
you can save the files

00:58:55.670 --> 00:58:58.530
and also run it from-- and then
it all runs in your browser.

00:58:58.530 --> 00:58:59.830
So, it comes with a Bash--

00:58:59.830 --> 00:59:01.650
built-in Bash console that
we're going to be using

00:59:01.650 --> 00:59:03.870
to run the commands,
and it doesn't interact

00:59:03.870 --> 00:59:05.450
with your local file system.

00:59:05.450 --> 00:59:08.460
So as we're going to see in
a few minutes and as you saw

00:59:08.460 --> 00:59:11.910
in the setup, we have to upload
the files in order to be able

00:59:11.910 --> 00:59:13.210
to run them and interact

00:59:13.210 --> 00:59:17.060
with them via PythonAnywhere's
Bash shell.

00:59:17.060 --> 00:59:21.520
So, just a couple basic tips
for working in the shell.

00:59:21.520 --> 00:59:23.220
So first of all, a directory,

00:59:23.220 --> 00:59:25.410
what it calls a directory
is essentially a folder.

00:59:25.410 --> 00:59:29.810
So what you see on your folder
is actually the directory.

00:59:29.810 --> 00:59:32.860
So when you-- And so when change
directories, it's like going

00:59:32.860 --> 00:59:35.980
from the desktop and double
clicking on downloads.

00:59:35.980 --> 00:59:38.310
Cases, spaces and
punctuation matter.

00:59:38.310 --> 00:59:40.580
So if you get errors,
a lot of times,

00:59:40.580 --> 00:59:43.040
it's because you've
forgotten a punctuation,

00:59:43.040 --> 00:59:45.030
you've added a space
that shouldn't be there.

00:59:45.030 --> 00:59:46.860
So, that's immediately the
first thing I always look

00:59:46.860 --> 00:59:49.020
at if something is not working.

00:59:49.020 --> 00:59:50.770
And then hitting tab on--

00:59:50.770 --> 00:59:53.400
when you're in the command
line will autocomplete.

00:59:53.400 --> 00:59:55.880
So if you start typing the
name of a file and you hit tab,

00:59:55.880 --> 00:59:58.270
it will say, oh yeah, that
file is in the directory,

00:59:58.270 --> 00:59:59.750
I'll just autocomplete it.

00:59:59.750 --> 01:00:02.040
So, that's pretty handy.

01:00:02.040 --> 01:00:04.180
And also when you have long
filenames, as you're going

01:00:04.180 --> 01:00:05.840
to see, there are some
files that we're using

01:00:05.840 --> 01:00:07.430
that has super long numbers

01:00:07.430 --> 01:00:10.350
and you can just start
and just tab through.

01:00:10.350 --> 01:00:12.560
And then hitting in
the up arrow will cycle

01:00:12.560 --> 01:00:14.030
through the last
command you entered.

01:00:14.030 --> 01:00:16.570
So again, if you get an error
message, instead of having

01:00:16.570 --> 01:00:19.230
to type in the entire script
all over again, just hit the

01:00:19.230 --> 01:00:23.610
up arrow and then you can go
back and correct that command.

01:00:23.610 --> 01:00:26.240
And then when you view
files in the shell,

01:00:26.240 --> 01:00:28.940
you have to quit
before viewing command.

01:00:28.940 --> 01:00:33.150
And so, use quit, q, to
quit viewing the file.

01:00:33.150 --> 01:00:35.100
So, we have a video created

01:00:35.100 --> 01:00:36.810
by our graduate assistant
Roja [assumed spelling]

01:00:36.810 --> 01:00:38.590
that provides a nice
introduction

01:00:38.590 --> 01:00:40.400
to the command line.

01:00:40.400 --> 01:00:47.660
And this is also linked from
our project website as well.

01:01:02.590 --> 01:01:04.890
Why there's no volume?

01:01:11.340 --> 01:01:13.640
OK. Well, we may--

01:01:20.190 --> 01:01:23.330
I think we forgot to
set up the speakers.

01:01:23.330 --> 01:01:25.980
So in the interest
of time, we'll--

01:01:25.980 --> 01:01:29.770
we have the backup slides that
kind of go through the steps

01:01:29.770 --> 01:01:32.080
that she takes us through.

01:01:32.080 --> 01:01:37.930
So, the Roja's video takes you

01:01:37.930 --> 01:01:41.990
through the basic steps,
but we have it here.

01:01:41.990 --> 01:01:47.560
So if you look on your
handout on page 8,

01:01:47.560 --> 01:01:49.700
you'll see the commands
that she takes us

01:01:49.700 --> 01:01:53.450
through at the bottom
of the page.

01:01:53.450 --> 01:01:59.500
So, pwd allows you to see
your current directory.

01:01:59.500 --> 01:02:03.800
And then, mkdir allows you
to create a new directory

01:02:03.800 --> 01:02:05.800
or create a new folder.

01:02:05.800 --> 01:02:09.310
Cd is to change the
directory, so to go up and down

01:02:09.310 --> 01:02:11.210
into different folders.

01:02:11.210 --> 01:02:14.710
Ls allows you to list the
files in your directory.

01:02:14.710 --> 01:02:17.870
And then, mv allows
you to rename the file,

01:02:17.870 --> 01:02:22.120
you type in less then
allow you to view the file.

01:02:22.120 --> 01:02:25.550
And then-- Let's see.

01:02:25.550 --> 01:02:28.490
And then, the other to touch
tells you to create a new file.

01:02:28.490 --> 01:02:32.810
So, these are some of the
immediate-- these are--

01:02:32.810 --> 01:02:34.110
this gives you kind
of a chi-chi.

01:02:34.110 --> 01:02:36.670
But, let's-- In that case,
let's just go to PythonAnywhere

01:02:36.670 --> 01:02:39.020
and start and we'll do some--

01:02:39.020 --> 01:02:40.700
and go through some
of these basics.

01:02:40.700 --> 01:02:44.240
So, if everybody wants
to go into PythonAnywhere

01:02:44.240 --> 01:02:47.410
and we'll walk through this.

01:02:47.410 --> 01:02:53.660
And then everybody should be
logged in for the free account.

01:03:01.100 --> 01:03:06.670
OK. So, the consoles is where
you start your Bash file.

01:03:06.670 --> 01:03:09.600
So, let's click on consoles.

01:03:09.600 --> 01:03:12.440
And you may see this.

01:03:12.440 --> 01:03:15.260
It says you have no consoles.

01:03:15.260 --> 01:03:19.840
So, we want to click on Bash
to start a new console in Bash.

01:03:26.530 --> 01:03:29.250
So, has everybody
opened the Bash shell?

01:03:29.250 --> 01:03:31.360
All right.

01:03:31.360 --> 01:03:38.550
So, you type in p-- So
following the steps on page 8,

01:03:38.550 --> 01:03:41.200
we type in pwd, that
tells us where we're at.

01:03:41.200 --> 01:03:43.010
So, that's-- If you start
getting lost, you're not sure

01:03:43.010 --> 01:03:47.170
where you're at, pwd, so this
shows in my home directory.

01:03:47.170 --> 01:03:52.410
And then if you type ls,
you see all the files.

01:03:53.680 --> 01:03:58.600
So, you may not see
anything at the moment.

01:04:00.030 --> 01:04:03.070
Well, you should see the
activity files dot zip,

01:04:03.070 --> 01:04:04.640
does everybody see that?

01:04:04.640 --> 01:04:05.940
Yeah. OK.

01:04:05.940 --> 01:04:07.240
[ Inaudible ]

01:04:07.240 --> 01:04:09.610
Yes. So then you see everything.

01:04:09.610 --> 01:04:14.010
OK. So, the first thing
we're going to do is

01:04:14.010 --> 01:04:16.390
that activity underscore
files dot zip,

01:04:16.390 --> 01:04:19.650
we're going to unzip that.

01:04:19.650 --> 01:04:20.950
I don't have it here.

01:04:20.950 --> 01:04:27.640
So what-- if you haven't
already, when you go to files,

01:04:27.640 --> 01:04:30.740
you should see a list of files

01:04:30.740 --> 01:04:35.010
and you see the activity
files dot zip uploaded here.

01:04:35.010 --> 01:04:37.310
So, I'm going to upload it.

01:04:41.700 --> 01:04:46.460
So once you make sure that you
have the activity files dot zip,

01:04:46.460 --> 01:04:53.590
we'll go back to the
console and then just type

01:04:53.590 --> 01:04:59.470
in unzip activity files dot zip.

01:05:08.270 --> 01:05:13.310
And then the next step, if you
see step 3 is mv activity files.

01:05:13.310 --> 01:05:15.900
So, you want to move-- In order
to do the activities we're going

01:05:15.900 --> 01:05:22.610
to be doing, you
want to move them.

01:05:22.610 --> 01:05:35.980
So, I'll do mv activity files
and then space home hegreen.

01:05:38.610 --> 01:05:40.770
So, is everybody able
to complete this,

01:05:40.770 --> 01:05:44.110
to unzip the files
and move them?

01:05:44.110 --> 01:05:46.780
Great. And as I showed,
punctuation matters.

01:05:46.780 --> 01:05:50.580
So, make sure the
punctuation is correct.

01:05:50.580 --> 01:05:54.740
And then if you do ls, you
should see these files.

01:05:54.740 --> 01:05:59.620
Does everybody see these files?

01:05:59.620 --> 01:06:03.450
So after you've done
the previous steps,

01:06:03.450 --> 01:06:06.120
when you type ls, you
should now see those files

01:06:06.120 --> 01:06:08.070
in your directory.

01:06:08.070 --> 01:06:09.980
Are we good?

01:06:09.980 --> 01:06:12.500
OK. So, web scraping.

01:06:12.500 --> 01:06:14.950
So, we're going to use
the command line to--

01:06:14.950 --> 01:06:18.330
tool wget, and this
helps you transfer files

01:06:18.330 --> 01:06:20.420
from the web server by doing--

01:06:20.420 --> 01:06:22.080
which is otherwise
known as web scraping.

01:06:22.080 --> 01:06:26.450
So, it also can follow links on
the page and follow content too.

01:06:26.450 --> 01:06:29.340
So, how you-- depending on
how you write the wget script,

01:06:29.340 --> 01:06:31.980
it can follow just one level of
links or it can go three levels

01:06:31.980 --> 01:06:35.260
down and scrape all the pages
underneath those links as well.

01:06:35.260 --> 01:06:40.380
So, you do-- you specify that
kind of option and other options

01:06:40.380 --> 01:06:44.430
through the flagged letters
that you put in the wget.

01:06:44.430 --> 01:06:49.300
And so, options for a wget
include -l, that tells how far

01:06:49.300 --> 01:06:51.390
in the hierarchy of
links you want to go.

01:06:51.390 --> 01:06:54.330
And then limit rate sets
that transfer speed.

01:06:54.330 --> 01:06:58.240
So as I mentioned earlier, being
a good citizen and not hitting

01:06:58.240 --> 01:06:59.800
that web server constantly,

01:06:59.800 --> 01:07:03.350
the limit rate dictates how
fast you hit the web server.

01:07:03.350 --> 01:07:06.850
And so the script, the
script that we use--

01:07:06.850 --> 01:07:09.140
we're going to use today
will have all those options,

01:07:09.140 --> 01:07:11.270
and we'll see in a
second how that works.

01:07:11.270 --> 01:07:14.420
So, we're going to return to
our sample reference question.

01:07:14.420 --> 01:07:17.890
And so, the students wind
to analyze how concepts

01:07:17.890 --> 01:07:19.940
such as liberty change
over time.

01:07:19.940 --> 01:07:22.480
And so, one possible approach
that the student might use

01:07:22.480 --> 01:07:25.570
to gathering data in addition
to the HathiTrust is maybe

01:07:25.570 --> 01:07:28.370
to scrape some political
speeches from an open source.

01:07:28.370 --> 01:07:30.060
So, we're going to
use WikiSource

01:07:30.060 --> 01:07:33.780
to start building their corpus
and scrape some text from there.

01:07:33.780 --> 01:07:37.740
So in this activity
we're going to do now,

01:07:37.740 --> 01:07:41.010
we're going to run a wget
command to scrape text

01:07:41.010 --> 01:07:44.230
from the open WikiSource version

01:07:44.230 --> 01:07:47.410
of George Washington's Fourth
State of the Union Address.

01:07:47.410 --> 01:07:50.070
So, we have the website
source here.

01:07:50.070 --> 01:07:52.820
If you have the slides
or the handout open,

01:07:52.820 --> 01:07:56.000
this is where you can-- you
can copy the website address.

01:07:56.000 --> 01:07:58.240
We highly recommend
that you copy it.

01:07:58.240 --> 01:08:01.020
But let's take a look at it.

01:08:02.490 --> 01:08:06.230
So, here's the WikiSource page
that we're going to scrape.

01:08:06.230 --> 01:08:07.880
So, you can see there's
lots of text, there's lots

01:08:07.880 --> 01:08:10.820
of different links, et cetera.

01:08:15.330 --> 01:08:16.820
So, what you're going
to need, you're going

01:08:16.820 --> 01:08:18.290
to have your PythonAnywhere
open again.

01:08:18.290 --> 01:08:19.590
We're going to use
the Bash shell

01:08:19.590 --> 01:08:22.090
that we were just working
in, the website URL.

01:08:22.090 --> 01:08:27.530
Again, I recommend you
copy it from the handout

01:08:27.530 --> 01:08:29.410
or from the command itself.

01:08:29.410 --> 01:08:31.720
And then, we're going to
use this wget command.

01:08:31.720 --> 01:08:34.330
And again, this is
something you can copy

01:08:34.330 --> 01:08:37.200
from the handout or slides.

01:08:37.200 --> 01:08:42.620
So, let's look at
that w-- that command.

01:08:42.620 --> 01:08:44.840
So, here's the source.

01:08:44.840 --> 01:08:48.230
And so, we're going to use
this command, and I recommend

01:08:48.230 --> 01:08:51.170
that you copy this or you
can carefully type it.

01:08:51.170 --> 01:08:52.950
So, here's the command.

01:08:52.950 --> 01:08:59.180
So, we have wget
and then -l space 1.

01:08:59.180 --> 01:09:01.700
And so, that means that it's
only going to go one level.

01:09:01.700 --> 01:09:04.760
It's not going to follow
links too far down.

01:09:04.760 --> 01:09:10.370
And then -- limit rate means
that we're doing a speed of 20k,

01:09:10.370 --> 01:09:13.120
and 20k is about a thoughtful--
it's pretty thoughtful speed.

01:09:13.120 --> 01:09:15.030
We're not hitting
the server too much.

01:09:15.030 --> 01:09:17.090
And then, we give the URL
for what we're scraping.

01:09:17.090 --> 01:09:20.700
So as Amanda and Eleanor just
said, actually it's better

01:09:20.700 --> 01:09:24.420
to follow the link or Google
the Fourth State of the Union

01:09:24.420 --> 01:09:26.450
and copy the web
address from there.

01:09:26.450 --> 01:09:30.070
And then output, dash-- so --

01:09:30.070 --> 01:09:34.490
output document washington
underscore 4 dot txt,

01:09:34.490 --> 01:09:37.730
that means we want it to write
what it grabs from the website.

01:09:37.730 --> 01:09:41.270
We want it to put in a new
file called Washington 4.

01:09:41.270 --> 01:09:42.850
So otherwise, the
command is going

01:09:42.850 --> 01:09:45.680
to name it something that's
going to be based on that URL.

01:09:45.680 --> 01:09:48.740
So, we're just going to keep
things concise and tell them--

01:09:48.740 --> 01:09:51.890
tell the command put it in
this Washington 4 document.

01:09:51.890 --> 01:10:02.860
OK. So, we go to
the Bash command.

01:10:02.860 --> 01:10:17.850
So, we're going to wget
-l 1 -- limit rate=20k.

01:10:17.850 --> 01:10:23.380
And then, I'll go to the
website, copy the URL,

01:10:23.380 --> 01:10:32.800
paste it in, and then --
output-document=washington.

01:10:32.800 --> 01:10:37.280
And actually, I think I
already have a Washington 4.

01:10:37.280 --> 01:10:41.690
So, I'm just going to
do Washington speech.

01:10:41.690 --> 01:10:47.350
And then I hit enter.

01:10:47.350 --> 01:10:53.030
OK. So, let me actually
copy the URL that works.

01:11:10.980 --> 01:11:16.430
OK. So everybody, when
you scrape the text,

01:11:16.430 --> 01:11:19.240
you should see this
kind of screen.

01:11:19.240 --> 01:11:22.500
It connects to the server.

01:11:22.500 --> 01:11:28.000
So once you have the
text saved to your files,

01:11:28.000 --> 01:11:31.250
so there's two ways that
you can view that file.

01:11:31.250 --> 01:11:34.560
You can either go
back into your files.

01:11:37.570 --> 01:11:41.130
And you have this
super zoomed in.

01:11:41.130 --> 01:11:43.830
Go back into your files.

01:11:43.830 --> 01:11:47.600
Find the washington underscore
4 and see-- if you see that,

01:11:47.600 --> 01:11:48.960
that means it's scraped.

01:11:48.960 --> 01:11:51.060
So, there it is.

01:11:51.060 --> 01:11:54.300
And this is what's been
scraped from the web page.

01:11:54.300 --> 01:11:58.770
And you can see,
there's a lot in there

01:11:58.770 --> 01:12:00.650
that we may or may not want.

01:12:00.650 --> 01:12:06.850
And then, I'll turn the
other way, is to go back

01:12:06.850 --> 01:12:09.510
into our Bash console.

01:12:09.510 --> 01:12:15.210
[ Inaudible ]

01:12:15.210 --> 01:12:18.610
The-- There's a menu up here.

01:12:18.610 --> 01:12:19.910
&gt;&gt; Oh.

01:12:19.910 --> 01:12:21.210
&gt;&gt; Harriett Green: Yeah.

01:12:21.210 --> 01:12:22.510
So, the PythonAnywhere menu

01:12:22.510 --> 01:12:24.810
up here lets you
move back and forth.

01:12:29.070 --> 01:12:32.660
So, I'd hit control l to
clear my screen again.

01:12:32.660 --> 01:12:37.570
And then I have to
do less washington--

01:12:37.570 --> 01:12:43.810
what was it, do less
washington 4.txt.

01:12:43.810 --> 01:12:47.600
I can view the file in
the Bash console itself.

01:12:47.600 --> 01:12:52.700
And then, I hit quit to
get out of the viewing.

01:12:52.700 --> 01:12:56.510
So, I hit q and it takes me
back to the Bash console.

01:12:56.510 --> 01:13:01.650
[ Inaudible ]

01:13:01.650 --> 01:13:07.710
So, you're looking-- what
you're looking at is the--

01:13:07.710 --> 01:13:10.260
all the text plus the HTML code.

01:13:10.260 --> 01:13:11.580
&gt;&gt; All the tags?

01:13:11.580 --> 01:13:13.250
&gt;&gt; Harriett Green:
Yeah, with the HTML tags

01:13:13.250 --> 01:13:15.010
as well as the text itself.

01:13:15.010 --> 01:13:16.670
[ Inaudible ]

01:13:16.670 --> 01:13:19.060
Right. Right.

01:13:19.060 --> 01:13:22.680
But this time, we scraped
it onto our own computer

01:13:22.680 --> 01:13:24.510
for ourselves, exactly.

01:13:24.510 --> 01:13:29.230
[ Inaudible ]

01:13:29.230 --> 01:13:30.920
Right. Yeah.

01:13:30.920 --> 01:13:32.680
Yeah. This is something
that you would--

01:13:32.680 --> 01:13:37.430
especially if you're doing
a multiple files at once.

01:13:37.430 --> 01:13:42.390
So, let's talk about--
So, what happened?

01:13:42.390 --> 01:13:45.000
So as we-- as somebody
just asked,

01:13:45.000 --> 01:13:49.110
we executed the wget
command and it scraped

01:13:49.110 --> 01:13:52.460
from that web page using the
URL that we pointed it to

01:13:52.460 --> 01:13:54.520
and it grabbed everything
on that page.

01:13:54.520 --> 01:13:56.540
So, not just the text that
we were reading, but under--

01:13:56.540 --> 01:13:59.660
everything underlying it
underneath because we told it

01:13:59.660 --> 01:14:02.500
to go to the server
and just grab it.

01:14:02.500 --> 01:14:06.830
So, what do you think is going
to be removed from this file

01:14:06.830 --> 01:14:08.160
to prepare it for data analysis?

01:14:08.160 --> 01:14:11.040
Because that's the next step,
is preparing this for analysis.

01:14:11.040 --> 01:14:14.710
There's a lot of excess headers
and footers that we might want

01:14:14.710 --> 01:14:17.010
to get rid off before
we actually make it.

01:14:17.010 --> 01:14:18.310
[ Inaudible ]

01:14:18.310 --> 01:14:19.810
Yes. Everything--
That's a good point

01:14:19.810 --> 01:14:23.640
for what we're going to do next.

01:14:23.640 --> 01:14:25.730
Great. That's the key.

01:14:25.730 --> 01:14:28.370
Any other thoughts?

01:14:28.370 --> 01:14:30.000
So, we're almost at lunch.

01:14:30.000 --> 01:14:34.300
So just to wrap up.

01:14:34.300 --> 01:14:35.680
So, we did this.

01:14:35.680 --> 01:14:38.360
So, there's other options
that you could use.

01:14:38.360 --> 01:14:41.390
So, this was a very-- wget
is a fairly basic command

01:14:41.390 --> 01:14:44.190
that we can run, but
there's many other options.

01:14:44.190 --> 01:14:48.050
So Beautiful Soup, it's a
Python-based web scraping tool.

01:14:48.050 --> 01:14:49.350
It's very powerful.

01:14:49.350 --> 01:14:52.030
A lot of researchers use it.

01:14:52.030 --> 01:14:54.180
I've had researchers
come up to me and say,

01:14:54.180 --> 01:14:55.480
I want to use Beautiful Soup.

01:14:55.480 --> 01:14:58.210
And then I'm like teaching
myself Beautiful Soup right now.

01:14:58.210 --> 01:15:00.360
So, there's-- that's one option.

01:15:00.360 --> 01:15:03.840
Also, you can write web
scraping commands into a script.

01:15:03.840 --> 01:15:05.690
So, a script is a
list of directions

01:15:05.690 --> 01:15:07.160
for your computer to follow.

01:15:07.160 --> 01:15:08.700
And so, we're going
to-- And after lunch,

01:15:08.700 --> 01:15:10.450
we're going to use
some Python scripts

01:15:10.450 --> 01:15:12.760
to do some text analysis.

01:15:12.760 --> 01:15:15.060
But a script lets you iterate

01:15:15.060 --> 01:15:16.730
through more content
and do it in bulk.

01:15:16.730 --> 01:15:19.440
So, we just ran this
for one specific URL.

01:15:19.440 --> 01:15:23.090
But if you have 100 web pages
that you need to scrape,

01:15:23.090 --> 01:15:25.750
you don't want to write wget
for every single one of those.

01:15:25.750 --> 01:15:28.100
So, a script will let
you do that in bulk.

01:15:28.100 --> 01:15:30.830
And then again, make sure
when you do web scraping

01:15:30.830 --> 01:15:33.430
to time your requests so that
you aren't hitting somebody's

01:15:33.430 --> 01:15:35.760
web server heavily.

01:15:35.760 --> 01:15:39.600
And so-- So, you're
being polite.

01:15:39.600 --> 01:15:42.980
So, creating work sets is just
one way working with text data

01:15:42.980 --> 01:15:44.600
in the HathiTrust
which we talked

01:15:44.600 --> 01:15:46.200
about earlier this morning.

01:15:46.200 --> 01:15:50.360
So, you could also download data
from the HathiTrust and HTRC

01:15:50.360 --> 01:15:52.250
to work on your own machines.

01:15:52.250 --> 01:15:54.960
And this chart gives
you overview

01:15:54.960 --> 01:15:56.260
of all the different ways

01:15:56.260 --> 01:15:58.830
that you can work
with HathiTrust data.

01:15:58.830 --> 01:16:03.910
So, you can do a custom data
request and get bulk access

01:16:03.910 --> 01:16:06.670
to the page images
and OCR of the--

01:16:06.670 --> 01:16:09.330
texts that are in public domain.

01:16:09.330 --> 01:16:12.300
And the researcher can
use abstracted text,

01:16:12.300 --> 01:16:14.160
which we'll talk about
after lunch as well.

01:16:14.160 --> 01:16:16.020
You can use the extracted
features

01:16:16.020 --> 01:16:19.200
which are per volume
files, select metadata

01:16:19.200 --> 01:16:23.420
and data contents and you get
JSON files of the selected data

01:16:23.420 --> 01:16:26.150
such as word counts and
metadata about those volumes.

01:16:26.150 --> 01:16:29.220
And then very advanced
researchers can use the public

01:16:29.220 --> 01:16:34.330
domain-- access public domain
HTRC data through the data API,

01:16:34.330 --> 01:16:35.950
which is used through
a special environment

01:16:35.950 --> 01:16:37.670
that we call the data capsule.

01:16:37.670 --> 01:16:39.810
So, the data capsule is
for advanced researchers,

01:16:39.810 --> 01:16:41.780
and it's not great for
our workshop environment.

01:16:41.780 --> 01:16:43.500
It's more for individual
projects.

01:16:43.500 --> 01:16:45.500
So, we won't be mentioning
it in this workshop.

01:16:45.500 --> 01:16:46.800
We're going through it,

01:16:46.800 --> 01:16:48.670
but we want to mention
that as an advance.

01:16:48.670 --> 01:16:50.920
And so both the custom
data request

01:16:50.920 --> 01:16:52.600
and extractive features
are retrieved

01:16:52.600 --> 01:16:54.400
through those file
transfer methods.

01:16:54.400 --> 01:16:56.180
So, there's no user interface.

01:16:56.180 --> 01:16:59.070
So, use FTP, rsync
to retrieve those.

01:16:59.070 --> 01:17:04.830
OK. So to wrap up, let's
look at what Sam did.

01:17:04.830 --> 01:17:07.920
So after creating a
final list of volumes

01:17:07.920 --> 01:17:10.380
from the digital
library, he used rsync

01:17:10.380 --> 01:17:14.590
to retrieve an extracted
features dataset from the HTRC.

01:17:14.590 --> 01:17:17.720
And again, rsync is a
command line utility

01:17:17.720 --> 01:17:22.380
that transfer the files between
his computers and the others.

01:17:22.380 --> 01:17:24.620
And so really quickly, just

01:17:24.620 --> 01:17:27.370
to say what is this
extracted features dataset

01:17:27.370 --> 01:17:28.750
that we've been mentioning.

01:17:28.750 --> 01:17:32.030
So again, it's JSON files for
each volume from the HathiTrust

01:17:32.030 --> 01:17:35.270
that researchers can download,
and it has both metadata,

01:17:35.270 --> 01:17:38.680
bibliographic metadata as
well as inferred metadata

01:17:38.680 --> 01:17:44.180
such as page numbering, page
counts, word counts and more

01:17:44.180 --> 01:17:49.290
that we'll talk about in
a bit more detail later.

01:17:49.290 --> 01:17:53.200
So, this is something
that we encourage--

01:17:53.200 --> 01:17:56.080
we don't quite have the
time now to go through it.

01:17:56.080 --> 01:17:58.620
But some of you have
mentioned how do we think

01:17:58.620 --> 01:18:01.490
about our digital
collections as text data?

01:18:01.490 --> 01:18:04.460
So, the Santa Barbara Statement
on Collections as Data came

01:18:04.460 --> 01:18:06.750
out of the Collections
as Data National Forum

01:18:06.750 --> 01:18:09.210
which was an IMLS-funded
forum last year.

01:18:09.210 --> 01:18:11.370
So at that link, you can
read their statement.

01:18:11.370 --> 01:18:13.620
And it provides a set
of high level principles

01:18:13.620 --> 01:18:15.650
to guide collections
as data work.

01:18:15.650 --> 01:18:17.330
And they just received
another phase of that

01:18:17.330 --> 01:18:21.390
from the Mellon Foundation to
do some more work in this area.

01:18:21.390 --> 01:18:25.170
So, this is something that
we encourage you to reflect

01:18:25.170 --> 01:18:29.560
on as we think about making our
own digital collections ready,

01:18:29.560 --> 01:18:32.180
available for researchers
as text data.

01:18:32.180 --> 01:18:34.810
And so, these are just some
of the highlighted principles

01:18:34.810 --> 01:18:36.320
that you might consider.

01:18:36.320 --> 01:18:40.010
And principle 2 of
the statement says,

01:18:40.010 --> 01:18:41.820
collections as data
developments aims

01:18:41.820 --> 01:18:44.140
to encourage computational
use of digitized

01:18:44.140 --> 01:18:45.750
and born digital collections.

01:18:45.750 --> 01:18:47.620
So, we think about what
are the textual documents,

01:18:47.620 --> 01:18:50.200
what are the types of works
in our digital collections,

01:18:50.200 --> 01:18:53.830
you know, what can we do to
encourage computational use.

01:18:53.830 --> 01:18:55.410
Our host right here, the Library

01:18:55.410 --> 01:18:58.370
of Congress is doing
immense work with the LC Lab.

01:18:58.370 --> 01:19:01.250
So, I encourage all of you to
really look at all the resources

01:19:01.250 --> 01:19:03.590
that they've made available
for accessing their collections

01:19:03.590 --> 01:19:06.410
and actually some great
IPython notebooks even how

01:19:06.410 --> 01:19:07.710
to work with the data.

01:19:07.710 --> 01:19:09.010
It's really great.

01:19:09.010 --> 01:19:10.310
&gt;&gt; Eleanor Dickson: So
through the afternoon,

01:19:10.310 --> 01:19:13.150
we're going to be talking
about working with text data,

01:19:13.150 --> 01:19:15.620
then we'll finally get to the
analyzing text data piece.

01:19:15.620 --> 01:19:17.760
So, this is your text
analysis workshop, right?

01:19:17.760 --> 01:19:22.060
And then at the very end, we'll
sort of expand or contract based

01:19:22.060 --> 01:19:24.950
on time module on
visualizing text data.

01:19:24.950 --> 01:19:28.780
And if we don't quite hit that
last module, we've recorded it

01:19:28.780 --> 01:19:30.700
and you could always watch it
later if you're interested.

01:19:30.700 --> 01:19:32.950
But I'm hopeful today
about our timing.

01:19:32.950 --> 01:19:34.250
All right.

01:19:34.250 --> 01:19:37.260
So in this module,
we're in Module 3

01:19:37.260 --> 01:19:38.610
of the curriculum we're using.

01:19:38.610 --> 01:19:41.980
We're going to be thinking about
what happens when text is data.

01:19:41.980 --> 01:19:45.070
We'll talk about common
steps for cleaning

01:19:45.070 --> 01:19:46.760
and preparing text data,

01:19:46.760 --> 01:19:48.700
thinking about how you
might make recommendations

01:19:48.700 --> 01:19:50.000
for researchers about how

01:19:50.000 --> 01:19:52.540
to get their text data
ready for analysis.

01:19:52.540 --> 01:19:55.330
We'll practice a bit of data
cleaning on the speeches

01:19:55.330 --> 01:19:56.630
that you scraped earlier.

01:19:56.630 --> 01:19:59.270
And then, we'll come back
to Sam and see what he did

01:19:59.270 --> 01:20:02.640
with his Creativity Corpus
to get it ready for analysis.

01:20:02.640 --> 01:20:05.360
So, this is where we'll end up.

01:20:05.360 --> 01:20:07.270
I know it sort of
looks like both look

01:20:07.270 --> 01:20:08.940
like blank text on white boxes.

01:20:08.940 --> 01:20:11.910
But in one, if you
squint, you can maybe see

01:20:11.910 --> 01:20:15.360
that in the top left corner,
there are still the HTML tags.

01:20:15.360 --> 01:20:16.820
In the bottom right,
we've taken them out.

01:20:16.820 --> 01:20:19.110
So, we're going to practice
running a Python script

01:20:19.110 --> 01:20:22.390
to get that data cleaned up.

01:20:22.390 --> 01:20:23.690
All right.

01:20:23.690 --> 01:20:26.580
But first, let's talk
about humanities data.

01:20:26.580 --> 01:20:28.660
So, I work at the
University of Illinois.

01:20:28.660 --> 01:20:30.090
Illinois likes to
think about data

01:20:30.090 --> 01:20:31.960
as material that's generated

01:20:31.960 --> 01:20:33.790
or collected while
conducting research.

01:20:33.790 --> 01:20:35.450
So, that's an expansive
definition.

01:20:35.450 --> 01:20:37.770
And when we're thinking
about humanities data,

01:20:37.770 --> 01:20:40.320
we might be thinking about
things like citations,

01:20:40.320 --> 01:20:44.010
code or algorithms, databases,
geospatial coordinates.

01:20:44.010 --> 01:20:46.360
Can anybody think of
any other kinds of data

01:20:46.360 --> 01:20:49.830
that might be humanities data?

01:20:49.830 --> 01:20:51.130
Yeah?

01:20:51.130 --> 01:20:52.430
&gt;&gt; Text.

01:20:52.430 --> 01:20:53.730
&gt;&gt; Eleanor Dickson: Text.

01:20:53.730 --> 01:20:55.540
Yeah. You know, it's amazing
how many people don't offer

01:20:55.540 --> 01:20:57.010
that as an answer
in this workshop.

01:20:57.010 --> 01:20:58.310
So yeah, text data, right?

01:20:58.310 --> 01:20:59.610
That's what we're here for.

01:20:59.610 --> 01:21:00.910
What else?

01:21:00.910 --> 01:21:02.210
Yeah. Go ahead.

01:21:02.210 --> 01:21:03.750
[ Inaudible ]

01:21:03.750 --> 01:21:05.050
Yeah, images.

01:21:05.050 --> 01:21:06.350
Great.

01:21:06.350 --> 01:21:07.650
[ Inaudible ]

01:21:07.650 --> 01:21:11.130
Yeah, recordings, oral
histories in particular.

01:21:11.130 --> 01:21:13.570
Anything else?

01:21:13.570 --> 01:21:14.870
&gt;&gt; Transcriptions.

01:21:14.870 --> 01:21:16.410
&gt;&gt; Eleanor Dickson:
Transcriptions, yup.

01:21:16.410 --> 01:21:18.530
That's all great.

01:21:18.530 --> 01:21:22.610
So when we start to think about
text as data, we often have

01:21:22.610 --> 01:21:26.120
to shift the way that
we are approaching text

01:21:26.120 --> 01:21:28.220
as we commonly think about it.

01:21:28.220 --> 01:21:31.280
So, you might not think about
data quality necessarily

01:21:31.280 --> 01:21:34.980
when you're reading say a
novel unless you spill coffee

01:21:34.980 --> 01:21:36.340
on your library book, right?

01:21:36.340 --> 01:21:38.880
But in this case, we want
to think about the quality

01:21:38.880 --> 01:21:41.830
of the data that we're
going to use for analysis.

01:21:41.830 --> 01:21:45.210
So, OCR can be both
clean or dirty.

01:21:45.210 --> 01:21:47.130
Are you familiar
with those terms?

01:21:47.130 --> 01:21:50.260
So yeah, this is a basic term,
clean OCR, someone has gone

01:21:50.260 --> 01:21:54.270
through and then corrections,
dirty OCR has had no corrections

01:21:54.270 --> 01:21:55.820
on or minimal corrections.

01:21:55.820 --> 01:21:58.350
So, how to you address OCR
is dirty, it's uncorrected.

01:21:58.350 --> 01:22:01.240
And part of that has to do
with how large the corpus is.

01:22:01.240 --> 01:22:05.700
It would take a lot to clean
up over 16 million volumes.

01:22:05.700 --> 01:22:09.050
And also the active cleaning
which I hope that you'll see

01:22:09.050 --> 01:22:12.720
as we go through this module
is individually driven.

01:22:12.720 --> 01:22:15.120
Every researcher, every
person has an idea

01:22:15.120 --> 01:22:17.960
of what it would mean
to remediate their data.

01:22:17.960 --> 01:22:21.120
And so, we don't want to
make any choices to the OCR

01:22:21.120 --> 01:22:23.660
that would then make it
more difficult for somebody

01:22:23.660 --> 01:22:26.700
to actually get out the
information they were hoping

01:22:26.700 --> 01:22:28.000
to get.

01:22:28.000 --> 01:22:30.070
When we think about text as
data, we also start to think

01:22:30.070 --> 01:22:32.260
about the corpus or the corpora.

01:22:32.260 --> 01:22:34.580
So, a text corpus is
a digital collection

01:22:34.580 --> 01:22:37.170
or an individual's text dataset.

01:22:37.170 --> 01:22:39.450
And then, corpora
are bodies of text.

01:22:39.450 --> 01:22:41.570
So, that's just the plural.

01:22:41.570 --> 01:22:44.050
And another nice way that
you can think about text

01:22:44.050 --> 01:22:46.530
as data is thinking about
the way that the text

01:22:46.530 --> 01:22:49.210
when you're getting ready
to analyze it might go

01:22:49.210 --> 01:22:52.240
through the process of
decomposition or recomposition.

01:22:52.240 --> 01:22:55.430
And these are terms that we
borrow from Jeffrey Rockwell.

01:22:55.430 --> 01:22:58.090
So, cleaning can
involve discarding data

01:22:58.090 --> 01:23:00.900
which can be like decomposition.

01:23:00.900 --> 01:23:03.220
So, maybe you're getting
rid of elements that were

01:23:03.220 --> 01:23:04.970
on the page or certain words.

01:23:04.970 --> 01:23:07.880
And you're getting past what
was human readable to something

01:23:07.880 --> 01:23:10.150
that a computer can
do something with.

01:23:10.150 --> 01:23:14.460
And so oftentimes, the
prepared text might be illegible

01:23:14.460 --> 01:23:16.830
and that you-- you know,
you could sit and read say

01:23:16.830 --> 01:23:20.330
that JSON file that we looked
at earlier with the metadata.

01:23:20.330 --> 01:23:23.240
But it's not legible
in terms of something

01:23:23.240 --> 01:23:25.180
that you would enjoy
sitting down to read

01:23:25.180 --> 01:23:27.020
on the weekend for fun.

01:23:27.020 --> 01:23:31.550
So, a researcher goes about
the process of preparing data.

01:23:31.550 --> 01:23:34.440
They'll do things like
correct OCR errors.

01:23:34.440 --> 01:23:37.630
They may remove text
from the page object

01:23:37.630 --> 01:23:39.090
like titles or page headers.

01:23:39.090 --> 01:23:41.990
Can anyone think why
that would be important

01:23:41.990 --> 01:23:44.260
to remove the page header?

01:23:44.260 --> 01:23:46.060
Yeah?

01:23:46.060 --> 01:23:50.230
[ Inaudible ]

01:23:50.230 --> 01:23:51.530
Yeah. Yeah.

01:23:51.530 --> 01:23:54.040
It doesn't tell us much to
know that Moby-Dick is--

01:23:54.040 --> 01:23:55.850
those are the two most
frequently occurring words

01:23:55.850 --> 01:23:57.150
in Moby-Dick, right?

01:23:57.150 --> 01:23:59.110
We aren't learning anything
more about what the--

01:23:59.110 --> 01:24:01.520
about what Melville was saying.

01:24:01.520 --> 01:24:03.430
Removing tags, that's the
one we're going to practice.

01:24:03.430 --> 01:24:06.820
You might split or combine
files, remove certain words

01:24:06.820 --> 01:24:09.150
or punctuation marks, lowercase
text or tokenize words.

01:24:09.150 --> 01:24:10.600
And we're going to
look at some of these

01:24:10.600 --> 01:24:13.490
in more detail in a second.

01:24:13.490 --> 01:24:14.790
All right.

01:24:14.790 --> 01:24:17.240
So, just really drive
home these key concepts.

01:24:17.240 --> 01:24:19.620
Oftentimes when we're
thinking about how we're going

01:24:19.620 --> 01:24:22.960
to prepare text, we can
think about it in terms

01:24:22.960 --> 01:24:24.570
of chunking and then
recombining.

01:24:24.570 --> 01:24:27.280
So first, thinking
about chunking the text,

01:24:27.280 --> 01:24:29.210
which is the idea that
you would split the text

01:24:29.210 --> 01:24:30.980
into smaller pieces.

01:24:30.980 --> 01:24:35.630
And so often, it's going
beyond the book or the volume

01:24:35.630 --> 01:24:38.740
as an object and breaking
it into smaller pieces

01:24:38.740 --> 01:24:42.670
that have some importance
to the research question.

01:24:42.670 --> 01:24:45.960
So, let's say somebody wanted to
do a comparative analysis of all

01:24:45.960 --> 01:24:47.390
of the speeches of
Abraham Lincoln

01:24:47.390 --> 01:24:50.590
and maybe there was one book
that was published of all

01:24:50.590 --> 01:24:53.830
of those speeches and then
the researcher is going to go

01:24:53.830 --> 01:24:57.910
through and break that text file
that represents this book object

01:24:57.910 --> 01:25:00.900
into smaller pieces one
file for every speech.

01:25:00.900 --> 01:25:02.200
So, they're chunking it up.

01:25:02.200 --> 01:25:04.310
So, they can do that
comparative work.

01:25:04.310 --> 01:25:07.020
Other times, we're thinking
about grouping text.

01:25:07.020 --> 01:25:08.940
Maybe you're going
out to WikiSource

01:25:08.940 --> 01:25:12.960
and you're grabbing all of
the George Washington speeches

01:25:12.960 --> 01:25:16.410
that you can find there and
you end up with these files

01:25:16.410 --> 01:25:18.080
that you scraped from
the web and you're going

01:25:18.080 --> 01:25:19.870
to concatenate them together

01:25:19.870 --> 01:25:22.250
to make one big file
that's all George Washington

01:25:22.250 --> 01:25:26.190
so you could compare
that to Abraham Lincoln.

01:25:26.190 --> 01:25:27.960
The important thing here
is that you really have

01:25:27.960 --> 01:25:32.210
to go beyond thinking about the
volume, the book, the periodical

01:25:32.210 --> 01:25:36.510
as a discrete object
that is based--

01:25:36.510 --> 01:25:38.410
the most important
meaning, right?

01:25:38.410 --> 01:25:40.180
You might want to break
it up into pieces.

01:25:40.180 --> 01:25:41.960
You might want to smash it
together with something else.

01:25:41.960 --> 01:25:46.010
So, you're sort of
taking that next step.

01:25:46.010 --> 01:25:47.700
Another key concept
that we want to talk

01:25:47.700 --> 01:25:49.580
about here is tokenization.

01:25:49.580 --> 01:25:53.690
So, tokenization is the idea
that you're going to break text

01:25:53.690 --> 01:25:55.950
into pieces that
are called tokens.

01:25:55.950 --> 01:26:00.850
And the process of doing this
will discard punctuation marks.

01:26:00.850 --> 01:26:04.290
I mean, often there are some
normalization that happens also

01:26:04.290 --> 01:26:07.200
where maybe the text
becomes lowercase.

01:26:07.200 --> 01:26:11.050
So here, we have the beginning
of the Gettysburg Address

01:26:11.050 --> 01:26:13.500
and you can see that--
you can read it.

01:26:13.500 --> 01:26:16.380
It's human readable, but it's
not human readable in a sense

01:26:16.380 --> 01:26:18.840
that this isn't how you would
sit down to read the speech.

01:26:18.840 --> 01:26:21.530
And every one of the
words has been broken

01:26:21.530 --> 01:26:24.800
out into an individual
chunk where, you know,

01:26:24.800 --> 01:26:27.320
we're seeing the notation
with the brackets.

01:26:27.320 --> 01:26:31.270
And then now, because
this process has happened,

01:26:31.270 --> 01:26:33.930
this doesn't-- this isn't
a human editing process,

01:26:33.930 --> 01:26:37.720
this is a computer-generated
process, tokenize the text.

01:26:37.720 --> 01:26:40.980
We have these chunks that
you can start to count up.

01:26:40.980 --> 01:26:43.290
So, this is that--
a bit about reducing

01:26:43.290 --> 01:26:45.670
and abstracting the text.

01:26:47.290 --> 01:26:48.590
All right.

01:26:48.590 --> 01:26:49.890
So, it's important
also to keep in mind

01:26:49.890 --> 01:26:51.190
that data preparation is going

01:26:51.190 --> 01:26:53.330
to affect the results
of analysis.

01:26:53.330 --> 01:26:54.810
So the amount of text, the size

01:26:54.810 --> 01:26:57.260
of the chunks will
impact what you get

01:26:57.260 --> 01:26:59.450
out of a text analysis algorithm

01:26:59.450 --> 01:27:02.650
which stop words are removed
will also have an impact,

01:27:02.650 --> 01:27:05.840
which characters are
included or taken out,

01:27:05.840 --> 01:27:07.410
whether the text is lower--

01:27:07.410 --> 01:27:10.210
put into lowercase or
otherwise normalized.

01:27:10.210 --> 01:27:12.500
I mean-- And this
process takes time.

01:27:12.500 --> 01:27:14.250
This is often considered
to be one

01:27:14.250 --> 01:27:16.100
of the most time-intensive
pieces

01:27:16.100 --> 01:27:19.000
of doing data analysis
is this sort of--

01:27:19.000 --> 01:27:21.030
I don't like this term, but sort

01:27:21.030 --> 01:27:23.100
of data janitorial work,
right, data munging.

01:27:23.100 --> 01:27:26.100
This takes a lot of effort to
get everything ready for you

01:27:26.100 --> 01:27:28.620
to put it in for your analysis.

01:27:28.620 --> 01:27:31.310
And this is where
running some kind

01:27:31.310 --> 01:27:33.570
of scripting process
becomes useful.

01:27:33.570 --> 01:27:37.620
So, you don't want this to
turn into an editing process.

01:27:37.620 --> 01:27:39.980
And I think for some,
there is the temptation

01:27:39.980 --> 01:27:42.540
to think I'm going
to get my text ready.

01:27:42.540 --> 01:27:45.470
And there is a point at which
I have gone through in red,

01:27:45.470 --> 01:27:48.660
you know, all of my volumes
and I've lowercased everything

01:27:48.660 --> 01:27:50.560
and everything is
spelled perfectly.

01:27:50.560 --> 01:27:54.590
And while that temptation is
there, it's up to the researcher

01:27:54.590 --> 01:27:58.850
to decide how much noise they're
willing to accept in their data.

01:27:58.850 --> 01:28:02.630
And some level of noise is
generally accepted to be OK.

01:28:02.630 --> 01:28:05.000
But it's up to the researcher to
figure out what that level is.

01:28:05.000 --> 01:28:06.300
Yeah?

01:28:06.300 --> 01:28:07.600
[ Inaudible ]

01:28:07.600 --> 01:28:10.770
Yeah. So normalizing words,

01:28:10.770 --> 01:28:14.850
think about if you were doing a
study of 19th century literature

01:28:14.850 --> 01:28:17.480
and you had some works that
were written by American authors

01:28:17.480 --> 01:28:18.780
and some by British authors

01:28:18.780 --> 01:28:21.930
and they used two different
spellings for words like color.

01:28:21.930 --> 01:28:23.230
You have to make the choice,

01:28:23.230 --> 01:28:26.030
do I want to leave the spelling
difference there or do I want

01:28:26.030 --> 01:28:30.360
to say that every time the text
has color, you normalize it

01:28:30.360 --> 01:28:32.830
to either O-R or O-U-R.

01:28:32.830 --> 01:28:34.720
Yeah. Any other questions?

01:28:34.720 --> 01:28:39.610
That's a great one.

01:28:39.610 --> 01:28:44.920
OK. So in your small groups, you
are going to read definitions

01:28:44.920 --> 01:28:48.740
for some of these data
preparation terms.

01:28:48.740 --> 01:28:50.970
So, there are seven of them.

01:28:50.970 --> 01:28:53.240
And so, divide them
amongst yourselves.

01:28:53.240 --> 01:28:54.940
You'll each read two or three.

01:28:54.940 --> 01:28:56.520
And then when you finish
reading, you're going

01:28:56.520 --> 01:29:00.220
to explain what you read to your
group mates, to your partners.

01:29:00.220 --> 01:29:03.130
These definitions are taken
from a paper that was written

01:29:03.130 --> 01:29:05.900
by Denny and Spirling in 2017.

01:29:05.900 --> 01:29:08.090
And I quite like
this paper actually.

01:29:08.090 --> 01:29:09.810
I say this in every
workshop that we do,

01:29:09.810 --> 01:29:11.220
but I think this is a fun read.

01:29:11.220 --> 01:29:14.140
So if you're interested
in learning more,

01:29:14.140 --> 01:29:15.440
I would recommend going back

01:29:15.440 --> 01:29:16.970
and reading the paper
that they wrote.

01:29:16.970 --> 01:29:20.930
But we'll give you about five
minutes to take the time to read

01:29:20.930 --> 01:29:23.100
through these and then
discuss them in small groups.

01:29:23.100 --> 01:29:24.620
All right.

01:29:24.620 --> 01:29:25.920
How is it going?

01:29:25.920 --> 01:29:28.040
Did you all get through
all of them?

01:29:28.040 --> 01:29:31.970
OK. Did you have any questions
about the terms that you read?

01:29:31.970 --> 01:29:36.510
Any thoughts about these-- about
the terms that you went through?

01:29:36.510 --> 01:29:39.510
Yeah. Go ahead.

01:29:39.510 --> 01:29:56.550
[ Inaudible ]

01:29:56.550 --> 01:29:58.510
You mean if you--

01:29:58.510 --> 01:30:03.670
[ Inaudible ]

01:30:03.670 --> 01:30:08.590
Yeah. So, you could choose not
to lowercase all of your text

01:30:08.590 --> 01:30:11.130
if you think, oh, this
is chockfull of names

01:30:11.130 --> 01:30:14.030
that I think would create
disambiguation problems

01:30:14.030 --> 01:30:17.410
if I did lowercase
normalization.

01:30:17.410 --> 01:30:19.070
But you can also do a part

01:30:19.070 --> 01:30:23.620
of speech tagging before
you start your analysis.

01:30:23.620 --> 01:30:24.920
So then, we're going to look

01:30:24.920 --> 01:30:27.140
at a dataset example later
this afternoon where all

01:30:27.140 --> 01:30:30.550
of the tokens have been put
through part of speech tagging.

01:30:30.550 --> 01:30:32.110
So, that's another way to figure

01:30:32.110 --> 01:30:36.420
out when you mean a rose
is a rose is a rose.

01:30:36.420 --> 01:30:40.140
Yeah. But it-- In some of these
instances, you would choose not

01:30:40.140 --> 01:30:44.030
to do the thing that you think
would cause an issue for you say

01:30:44.030 --> 01:30:49.650
for the-- one about the numbers
with the sections of US code.

01:30:49.650 --> 01:30:52.420
In that example, those
researchers might choose

01:30:52.420 --> 01:30:54.920
to leave numbers in whereas
other researchers might take

01:30:54.920 --> 01:30:56.410
them out.

01:30:56.410 --> 01:30:57.850
Yeah. Yeah.

01:30:57.850 --> 01:31:00.510
Yeah. I saw a hand in the back.

01:31:00.510 --> 01:31:17.610
[ Inaudible ]

01:31:17.610 --> 01:31:19.700
Yeah. I think that's sort

01:31:19.700 --> 01:31:23.800
of like a subject
expertise question.

01:31:23.800 --> 01:31:25.210
So, that's know your data,

01:31:25.210 --> 01:31:27.170
know your research
question kind of thing.

01:31:27.170 --> 01:31:30.420
If you think it would be really
important in your dataset

01:31:30.420 --> 01:31:32.460
because of the nature, the
materials you're looking

01:31:32.460 --> 01:31:34.160
at to keep United
States together,

01:31:34.160 --> 01:31:37.110
then the researcher would
set that at the outset.

01:31:37.110 --> 01:31:40.340
But I've never seen an
engram list like that.

01:31:40.340 --> 01:31:43.510
Yeah. Do you have a question?

01:31:43.510 --> 01:32:02.220
[ Inaudible ]

01:32:02.220 --> 01:32:06.800
Well, so for some algorithms
like-- and frequently terms,

01:32:06.800 --> 01:32:13.810
what you're trying to study
has more to do with the terms

01:32:13.810 --> 01:32:15.460
that were used frequently.

01:32:15.460 --> 01:32:20.520
And so, you just don't really
need to have to them there

01:32:20.520 --> 01:32:21.870
in order to do the work.

01:32:21.870 --> 01:32:24.930
And so, taking them out can make
sure that you're looking at data

01:32:24.930 --> 01:32:27.380
that you think is
meaningful in some way.

01:32:27.380 --> 01:32:31.650
So, size is one piece,
but also this is one step

01:32:31.650 --> 01:32:34.150
of setting the scope
of your research.

01:32:34.150 --> 01:32:36.080
And when we look at Sam
later, we're going to see some

01:32:36.080 --> 01:32:38.440
of the tokens that
he got rid of.

01:32:38.440 --> 01:32:41.710
And this is sort of what
goes in, what comes--

01:32:41.710 --> 01:32:43.510
what goes in is what will
come out sort of thing

01:32:43.510 --> 01:32:46.580
because you could-- especially
with things like stop words,

01:32:46.580 --> 01:32:49.400
you could get really sort of
red penny with your stop words

01:32:49.400 --> 01:32:51.370
and go through and just
start taking things out.

01:32:51.370 --> 01:32:53.610
And suddenly, you're
going to have topic models

01:32:53.610 --> 01:32:56.850
that have only certain
words displayed.

01:32:56.850 --> 01:33:00.300
So, it's a balancing act
about trying to make sure

01:33:00.300 --> 01:33:03.870
that the results are-- you
feel are somehow meaningful

01:33:03.870 --> 01:33:07.730
and representative, but also
that you have not pushed

01:33:07.730 --> 01:33:09.290
in such a way that
you're just getting

01:33:09.290 --> 01:33:13.300
out what you wanted
to get out of it.

01:33:13.300 --> 01:33:15.660
It's a balancing act.

01:33:15.660 --> 01:33:18.080
&gt;&gt; So [inaudible]
elsewhere is to try

01:33:18.080 --> 01:33:22.640
to always have a baseline
text that stays the same?

01:33:22.640 --> 01:33:23.940
&gt;&gt; Eleanor Dickson: Yes.

01:33:23.940 --> 01:33:25.240
Yeah.

01:33:25.240 --> 01:33:26.540
&gt;&gt; [Inaudible] to
talk about that,

01:33:26.540 --> 01:33:27.840
but any of these
changes it can make,

01:33:27.840 --> 01:33:29.140
that would be on top like--

01:33:29.140 --> 01:33:30.440
&gt;&gt; Eleanor Dickson: Yes.

01:33:30.440 --> 01:33:31.740
Yeah.

01:33:31.740 --> 01:33:33.040
&gt;&gt; -- baseline text figured out

01:33:33.040 --> 01:33:34.340
and then you would remove
things or add things--

01:33:34.340 --> 01:33:35.640
&gt;&gt; Eleanor Dickson: Yes.

01:33:35.640 --> 01:33:36.940
Yeah.

01:33:36.940 --> 01:33:38.240
&gt;&gt; -- on top in a
different version.

01:33:38.240 --> 01:33:39.540
&gt;&gt; Eleanor Dickson: Yes.

01:33:39.540 --> 01:33:40.840
Yeah. We-- You know, we don't
talk about that, but we should

01:33:40.840 --> 01:33:42.140
because it comes up regularly.

01:33:42.140 --> 01:33:43.440
And sometimes, we have people
who are confused like, oh,

01:33:43.440 --> 01:33:45.070
if you'd get rid of all words,
then you've made, you know,

01:33:45.070 --> 01:33:48.220
these changes that you can't
go back, but-- you're right.

01:33:48.220 --> 01:33:50.460
You want to do this
in separate versions.

01:33:50.460 --> 01:33:53.450
So then if you do run an
analysis where you say you take

01:33:53.450 --> 01:33:56.860
out certain kinds of stop words
and it doesn't look quite right,

01:33:56.860 --> 01:33:59.050
then you just go back
to the earlier version

01:33:59.050 --> 01:34:02.230
and you make some other
modifications and you try again.

01:34:02.230 --> 01:34:04.730
Yeah. So, this-- it's not like
these are permanent changes.

01:34:04.730 --> 01:34:06.780
This is sort of building on top

01:34:06.780 --> 01:34:09.890
and you can always go
back and try again.

01:34:09.890 --> 01:34:12.080
It's definitely an
iterative process.

01:34:12.080 --> 01:34:16.020
Yeah. Any other thoughts
or questions?

01:34:16.020 --> 01:34:17.320
All right.

01:34:17.320 --> 01:34:20.930
How many of you have experience
with Python programming

01:34:20.930 --> 01:34:23.070
or programming generally?

01:34:23.070 --> 01:34:24.370
Awesome. OK.

01:34:24.370 --> 01:34:25.670
Well, this actually is--

01:34:25.670 --> 01:34:30.100
you guys are an experienced
crowd, so-- which is great.

01:34:30.100 --> 01:34:38.000
So, just as a reminder, yeah, we
don't intend the Python portion

01:34:38.000 --> 01:34:43.220
of the program to prepare you to
go out and write your own code

01:34:43.220 --> 01:34:46.410
or to be an expert
developer, but we just want you

01:34:46.410 --> 01:34:47.710
to feel more comfortable within.

01:34:47.710 --> 01:34:49.960
If you already feel
really comfortable,

01:34:49.960 --> 01:34:52.310
then this might be a good
time for you to lean over

01:34:52.310 --> 01:34:54.000
and help your neighbor if they
need a little bit more help

01:34:54.000 --> 01:34:55.750
getting through the activities.

01:34:55.750 --> 01:34:58.540
OK. So, we start with an
introduction to Python.

01:34:58.540 --> 01:34:59.840
So, what is Python?

01:34:59.840 --> 01:35:02.180
So, Python is a scripting
language,

01:35:02.180 --> 01:35:04.180
and it's also an
interpreted language.

01:35:04.180 --> 01:35:07.020
And if you try to look up on
the internet what is Python,

01:35:07.020 --> 01:35:09.760
often the definition you find is
it is an interpreted language.

01:35:09.760 --> 01:35:11.060
And I don't know about you,

01:35:11.060 --> 01:35:13.050
but that means literarily
nothing to me.

01:35:13.050 --> 01:35:14.880
But what interpreted means is

01:35:14.880 --> 01:35:16.890
that it follows step-by-step
directions.

01:35:16.890 --> 01:35:19.930
So, what we're going to be
doing is running a Python script

01:35:19.930 --> 01:35:22.040
which is a series of
step-by-step instructions

01:35:22.040 --> 01:35:24.080
that your computer
is going to follow

01:35:24.080 --> 01:35:26.340
in order to complete a task.

01:35:26.340 --> 01:35:30.080
And so, the task today is going
to be things like we're going

01:35:30.080 --> 01:35:33.300
to take out the tags, we're
going to remove stop words,

01:35:33.300 --> 01:35:35.640
but the script is just
a list of directions

01:35:35.640 --> 01:35:37.580
that your computer follows.

01:35:37.580 --> 01:35:40.030
And Python has a relatively
straightforward syntax.

01:35:40.030 --> 01:35:42.340
This is something that
the Python community feels

01:35:42.340 --> 01:35:43.640
good about.

01:35:43.640 --> 01:35:44.940
They call-- You know,
we're Pythonic.

01:35:44.940 --> 01:35:47.500
So, I don't know if any of you
have used SQL where you're used

01:35:47.500 --> 01:35:49.540
to having the semicolon
at the end of the line.

01:35:49.540 --> 01:35:51.470
That's not a thing
they do in Python.

01:35:51.470 --> 01:35:54.040
But what that means is if
you pick up a Python script

01:35:54.040 --> 01:35:56.640
and you have some idea
of what you're looking at

01:35:56.640 --> 01:35:59.490
and also the person has
well documented their code,

01:35:59.490 --> 01:36:02.450
it's not too hard to get a
feel for what's happening.

01:36:02.450 --> 01:36:05.160
Whether you could write it on
your own is another question,

01:36:05.160 --> 01:36:08.150
but it's sort of like, I
see they have this thing

01:36:08.150 --> 01:36:09.450
and it's going to do that thing,

01:36:09.450 --> 01:36:10.790
I sort of understand
what's happening.

01:36:10.790 --> 01:36:13.610
So, that's one of the benefits
of talking about Python today.

01:36:13.610 --> 01:36:15.190
All right.

01:36:15.190 --> 01:36:16.640
So, there are a few
different ways

01:36:16.640 --> 01:36:19.290
that one would interact
with Python.

01:36:19.290 --> 01:36:22.600
I'm-- We're going to mention two
and then I'll throw out a third,

01:36:22.600 --> 01:36:25.060
but we're going to
use just one of those.

01:36:25.060 --> 01:36:27.940
So, we mention this
first one because one

01:36:27.940 --> 01:36:32.050
of you will invariably open up
the Bash shell, type in Python

01:36:32.050 --> 01:36:34.730
and hit enter and then you're
going to see these three carets

01:36:34.730 --> 01:36:37.880
and you're going to go, huh,
what am I supposed to do now?

01:36:37.880 --> 01:36:41.130
That means when you type in
Python and then hit enter,

01:36:41.130 --> 01:36:44.690
you are running what's called
the Python interpreter.

01:36:44.690 --> 01:36:46.960
So, you're doing
interactive programming,

01:36:46.960 --> 01:36:50.560
which means that the computer or
these three carets are like, OK,

01:36:50.560 --> 01:36:53.730
give me a Python command to
enter, give me a little bit

01:36:53.730 --> 01:36:55.170
of Python for me to do.

01:36:55.170 --> 01:36:58.190
And so, you do it line
by line or step by step.

01:36:58.190 --> 01:37:00.280
Remember, it's just
step-by-step directions.

01:37:00.280 --> 01:37:03.010
So, this is one way to get
experience with Python is

01:37:03.010 --> 01:37:04.580
to practice in the interpreter.

01:37:04.580 --> 01:37:08.130
But you can see where this
would become not useful

01:37:08.130 --> 01:37:11.950
if you say wanted a nice file
that you could reuse over

01:37:11.950 --> 01:37:14.510
and over again and you don't
want to type out the directions.

01:37:14.510 --> 01:37:15.960
So, this-- we're not
doing this today.

01:37:15.960 --> 01:37:19.550
But if you do type in Python
and then you see the carets --

01:37:19.550 --> 01:37:22.190
yeah, there are three,
sorry, just--

01:37:22.190 --> 01:37:23.550
well, maybe there's actually
four, but there's three.

01:37:23.550 --> 01:37:25.910
OK. You're going to type in quit

01:37:25.910 --> 01:37:29.160
and then open parens
close parens.

01:37:29.160 --> 01:37:30.460
That's how you get out of there.

01:37:30.460 --> 01:37:31.760
All right.

01:37:31.760 --> 01:37:33.060
But you're not going to
because we did the warning.

01:37:33.060 --> 01:37:36.550
OK. So, the way that we are
going to use Python today is

01:37:36.550 --> 01:37:38.900
by running scripts
that have been written.

01:37:38.900 --> 01:37:41.270
So, scripts again are
just lists of directions

01:37:41.270 --> 01:37:43.160
that your computer
is going to follow.

01:37:43.160 --> 01:37:48.420
You save the file where the
script is written in a format

01:37:48.420 --> 01:37:50.430
that has .py at the end.

01:37:50.430 --> 01:37:53.420
So, this is a Python script that
you can execute, you can run,

01:37:53.420 --> 01:37:57.230
and then it will open the file
and follow the directions.

01:37:57.230 --> 01:37:58.530
And then, we're going
to be running these

01:37:58.530 --> 01:38:00.130
from the command line.

01:38:00.130 --> 01:38:04.360
So on the command line,
you'll enter in Python

01:38:04.360 --> 01:38:05.700
and then the name of the file

01:38:05.700 --> 01:38:07.360
and maybe some additional
directions

01:38:07.360 --> 01:38:09.050
and then you'll run it that way.

01:38:09.050 --> 01:38:11.390
Another way that we could
teach this workshop is

01:38:11.390 --> 01:38:13.440
by using Jupyter Notebooks.

01:38:13.440 --> 01:38:16.560
And I know-- or I think that
there are nice Jupyter Notebooks

01:38:16.560 --> 01:38:20.820
for some of the LC Labs work
that they have been doing.

01:38:20.820 --> 01:38:23.370
And if you're just
starting out with Python,

01:38:23.370 --> 01:38:27.650
and you're like totally blank
mind in terms of where to go

01:38:27.650 --> 01:38:29.320
from here, you might
actually want to start

01:38:29.320 --> 01:38:33.310
with Jupyter Notebooks because
what it does is it takes the

01:38:33.310 --> 01:38:36.190
script, so each of those
step-by-step directions.

01:38:36.190 --> 01:38:37.780
And it's a way of rendering it

01:38:37.780 --> 01:38:41.050
so that you can have
narrative before and after

01:38:41.050 --> 01:38:43.050
and it's embedding
it in a viewer

01:38:43.050 --> 01:38:46.400
that I think is much more
friendly for the beginner

01:38:46.400 --> 01:38:47.930
to understand what's happening.

01:38:47.930 --> 01:38:50.950
It's a little-- It's like the
interactive programming meets

01:38:50.950 --> 01:38:52.790
the running of the
file in this way.

01:38:52.790 --> 01:38:55.880
So, it's a nice combination
of the two.

01:38:55.880 --> 01:38:57.180
All right.

01:38:57.180 --> 01:38:58.990
So when you want to run a Python
script from the command line,

01:38:58.990 --> 01:39:01.750
the first thing that you
have to type in is Python.

01:39:01.750 --> 01:39:03.720
So, you have to tell
the computer,

01:39:03.720 --> 01:39:05.850
I'm about to give
you directions,

01:39:05.850 --> 01:39:08.540
they're in a certain language
and this language is Python.

01:39:08.540 --> 01:39:09.850
And then you give
the script name.

01:39:09.850 --> 01:39:13.000
And then at the end,
you include arguments.

01:39:13.000 --> 01:39:15.900
So, these arguments are
some additional directions

01:39:15.900 --> 01:39:17.710
that the script needs to run.

01:39:17.710 --> 01:39:20.410
And in the script itself,
it will say whether

01:39:20.410 --> 01:39:22.410
or not it requires arguments

01:39:22.410 --> 01:39:25.010
and it will number
how many there are

01:39:25.010 --> 01:39:26.700
and give some idea
for what those are.

01:39:26.700 --> 01:39:28.810
And we're going to look
in a second about--

01:39:28.810 --> 01:39:30.860
to see what that looks
like in practice.

01:39:30.860 --> 01:39:32.410
So, tell the computer,
get ready,

01:39:32.410 --> 01:39:35.030
I'm going to give you some
Python, direct it to which file

01:39:35.030 --> 01:39:37.520
to open and run and
then there may be this

01:39:37.520 --> 01:39:40.810
additional information.

01:39:40.810 --> 01:39:42.110
All right.

01:39:42.110 --> 01:39:44.850
So, let's return to our
sample research question.

01:39:44.850 --> 01:39:48.440
And we are going to use a Python
script to prepare the text data

01:39:48.440 --> 01:39:51.110
that we scraped from WikiSource.

01:39:51.110 --> 01:39:52.410
All right.

01:39:52.410 --> 01:39:55.720
So, we're going to remove
the tags, blah, blah, blah.

01:39:55.720 --> 01:39:57.040
You need the Bash console.

01:39:57.040 --> 01:39:59.040
So, you might want to start
pulling up PythonAnywhere

01:39:59.040 --> 01:40:00.340
and get your console ready.

01:40:00.340 --> 01:40:02.170
We're going to be running it

01:40:02.170 --> 01:40:04.390
over the washington
underscore 4 dot txt.

01:40:04.390 --> 01:40:06.360
And the name of the
script we're going

01:40:06.360 --> 01:40:09.280
to be using is remove
tag dot py.

01:40:11.060 --> 01:40:12.360
All right.

01:40:12.360 --> 01:40:16.820
So, I'm going to move
into PythonAnywhere.

01:40:19.890 --> 01:40:22.830
Did we talk about control l?

01:40:22.830 --> 01:40:25.830
If your shell looks
really ugly, do control l

01:40:25.830 --> 01:40:29.010
and it will remove-- sort
of like start you afresh.

01:40:29.010 --> 01:40:33.030
But you can still arrow
through your preview commands.

01:40:33.030 --> 01:40:34.330
They didn't disappear.

01:40:34.330 --> 01:40:35.630
They just are gone
from your view.

01:40:35.630 --> 01:40:37.750
All right.

01:40:37.750 --> 01:40:46.380
So, let's take a look at the
file that we're going to run.

01:40:50.130 --> 01:40:53.890
So, we're doing a
remove tag dot py.

01:40:53.890 --> 01:40:56.510
So, you'll see at the
top that we're doing--

01:40:56.510 --> 01:40:58.050
we have these import statements.

01:40:58.050 --> 01:40:59.900
We'll talk more about
that later.

01:40:59.900 --> 01:41:02.210
And then you'll see
that there's a line.

01:41:02.210 --> 01:41:05.690
So we have this line in
green, that's a comment.

01:41:05.690 --> 01:41:07.120
The green is a comment.

01:41:07.120 --> 01:41:08.970
The computer doesn't read that.

01:41:08.970 --> 01:41:10.270
It skips over.

01:41:10.270 --> 01:41:13.190
But it's for you, the human,
to understand what's happening.

01:41:13.190 --> 01:41:15.360
And it says requires
an input file name

01:41:15.360 --> 01:41:16.660
when running this script.

01:41:16.660 --> 01:41:19.930
So we have, yeah, done an OK
job of documenting the process

01:41:19.930 --> 01:41:21.230
of what's happening here.

01:41:21.230 --> 01:41:26.460
And you'll see that
there's sys.argv and then 1.

01:41:26.460 --> 01:41:30.400
So, this script requires one
argument in order to run.

01:41:30.400 --> 01:41:31.970
And the point of
the argument here is

01:41:31.970 --> 01:41:35.390
that they want the user
who's running the command

01:41:35.390 --> 01:41:37.590
to supply some additional
information

01:41:37.590 --> 01:41:40.430
so that you could pop
in different files,

01:41:40.430 --> 01:41:42.960
different text files
and run the same script

01:41:42.960 --> 01:41:45.150
over a different text file.

01:41:45.150 --> 01:41:47.940
OK. So if you have this open,
you're going to just want

01:41:47.940 --> 01:41:50.110
to close it and go back
to your Bash console.

01:41:50.110 --> 01:41:51.470
All right.

01:41:51.470 --> 01:41:55.540
And so, we're just going to do
the first step all together.

01:41:55.540 --> 01:41:59.940
So, what is the first thing
that we need to type in?

01:41:59.940 --> 01:42:01.240
&gt;&gt; Python.

01:42:01.240 --> 01:42:02.540
&gt;&gt; Python.

01:42:02.540 --> 01:42:03.840
&gt;&gt; But don't have return.

01:42:03.840 --> 01:42:05.140
&gt;&gt; But don't have return.

01:42:05.140 --> 01:42:07.680
And then, what's the next
thing that we need to type in?

01:42:07.680 --> 01:42:08.980
&gt;&gt; Remove tags.

01:42:08.980 --> 01:42:10.660
&gt;&gt; Remove tags, yup.

01:42:10.660 --> 01:42:13.190
And I'm exceptionally lazy.

01:42:13.190 --> 01:42:14.490
So, I'm using my tab.

01:42:14.490 --> 01:42:16.430
So I have to type as
little as possible.

01:42:16.430 --> 01:42:19.010
And then, what's the last
thing I need to type in?

01:42:19.010 --> 01:42:20.390
[ Inaudible ]

01:42:20.390 --> 01:42:21.690
The name of the script.

01:42:21.690 --> 01:42:25.820
Yup. And I have several of them.

01:42:25.820 --> 01:42:27.790
And I didn't get an error.

01:42:27.790 --> 01:42:34.580
So now, I'm going to
view the file using less.

01:42:34.580 --> 01:42:37.790
Oh, sorry.

01:42:37.790 --> 01:42:39.090
We rename the file.

01:42:39.090 --> 01:42:41.230
Look. I didn't read
the directions well.

01:42:41.230 --> 01:42:45.100
So we see here-- This
is probably too small

01:42:45.100 --> 01:42:47.780
for you, isn't it?

01:42:47.780 --> 01:42:50.970
Is that better for viewing?

01:42:50.970 --> 01:42:55.240
We see here that we're opening a
file called tagless file dot txt

01:42:55.240 --> 01:42:56.540
and we're writing to it.

01:42:56.540 --> 01:43:01.310
So, can you see how this is--
I mean, it's not like easy,

01:43:01.310 --> 01:43:02.830
but it's also like, OK,

01:43:02.830 --> 01:43:04.990
I can understand
something is getting opened

01:43:04.990 --> 01:43:06.620
and something is
getting written, right?

01:43:06.620 --> 01:43:08.430
So, we're opening a file
and we're writing to it.

01:43:08.430 --> 01:43:11.140
So when I try to do the less
command here, I did it wrong

01:43:11.140 --> 01:43:13.300
because I was trying
to look at Washington.

01:43:13.300 --> 01:43:16.180
But we didn't actually-- As we
discussed a few minutes ago,

01:43:16.180 --> 01:43:18.170
we didn't overwrite
anything on Washington 4.

01:43:18.170 --> 01:43:21.480
We created a new file that
doesn't have any tags.

01:43:21.480 --> 01:43:25.020
So, I want to do
tagless file dot txt.

01:43:25.020 --> 01:43:27.060
So, you should all
have this file now.

01:43:27.060 --> 01:43:28.990
Maybe you should be able to see

01:43:28.990 --> 01:43:31.740
that the tags have
been stripped out.

01:43:38.320 --> 01:43:41.820
So remember, you can put
[inaudible] up if you get stuck.

01:43:41.820 --> 01:43:45.450
If you have gone through
this, try to do the activity

01:43:45.450 --> 01:43:47.410
at the bottom of the page 13.

01:43:47.410 --> 01:43:51.180
So, can you remove
the stop words?

01:43:51.180 --> 01:43:52.840
All right.

01:43:52.840 --> 01:43:56.660
So if you're an experienced
Python person,

01:43:56.660 --> 01:43:59.640
you probably don't feel too
proud of yourself right now.

01:43:59.640 --> 01:44:02.080
But if that's the first time you
have ever run a Python script,

01:44:02.080 --> 01:44:03.890
congratulations,
it was less scary

01:44:03.890 --> 01:44:05.640
that you might have thought
it was going to be, right?

01:44:05.640 --> 01:44:07.240
It's just like entering
the commands

01:44:07.240 --> 01:44:08.660
that we did earlier except

01:44:08.660 --> 01:44:12.210
that we're running the
commands over these files.

01:44:12.210 --> 01:44:15.700
OK. So-- And we had
someone point

01:44:15.700 --> 01:44:17.270
out that we think
there's something wrong

01:44:17.270 --> 01:44:20.260
because we didn't remove the
tag at the end, right, which,

01:44:20.260 --> 01:44:23.380
it points out a pitfall of
taking somebody else's code

01:44:23.380 --> 01:44:25.780
and trying it on
your own materials.

01:44:25.780 --> 01:44:28.600
You want to-- You know, there's
going to be a process of trying

01:44:28.600 --> 01:44:30.700
to make it fit for
what you want to do

01:44:30.700 --> 01:44:32.850
or correcting a mistake
that they made.

01:44:32.850 --> 01:44:37.390
OK. We skip this,
blah, blah, blah.

01:44:37.390 --> 01:44:44.450
OK. So, you executed a command
to get most of the URL--

01:44:44.450 --> 01:44:47.960
or sorry, most of the HTML tags.

01:44:47.960 --> 01:44:53.240
And whoa, we're in a really
weird place in these slides.

01:44:53.240 --> 01:44:54.540
I'm sorry.

01:44:54.540 --> 01:44:55.840
It's like this is
not making sense.

01:44:55.840 --> 01:44:58.140
Let's move ahead.

01:44:59.810 --> 01:45:03.530
Yeah. This is where
we want to be.

01:45:03.530 --> 01:45:04.830
All right.

01:45:04.830 --> 01:45:07.440
So, look for those carets to
denote the HTML tags or most

01:45:07.440 --> 01:45:09.610
of them and then use
the regular expression

01:45:09.610 --> 01:45:13.720
and then Python's replace
to get rid of what was

01:45:13.720 --> 01:45:16.860
in between those tags.

01:45:16.860 --> 01:45:19.890
So, it's simple and
also slightly broken.

01:45:19.890 --> 01:45:23.290
And this is also not the most
robust way to remove the tags.

01:45:23.290 --> 01:45:26.260
So, this is just
more for playing.

01:45:26.260 --> 01:45:28.850
But if somebody were
working with lots of files,

01:45:28.850 --> 01:45:30.330
they would want to
make modification

01:45:30.330 --> 01:45:33.030
so that they could feed in
an entire directory of files

01:45:33.030 --> 01:45:34.640
and not just one at a time.

01:45:34.640 --> 01:45:36.520
Or if somebody had
used something

01:45:36.520 --> 01:45:39.150
like Harriett mentioned
Beautiful Soup earlier.

01:45:39.150 --> 01:45:40.450
The joy of Beautiful Soup is

01:45:40.450 --> 01:45:43.170
that you can direct
your web scraping

01:45:43.170 --> 01:45:45.370
to only grab what's
between certain tags.

01:45:45.370 --> 01:45:48.260
And then you just skip
this whole mess altogether.

01:45:48.260 --> 01:45:50.240
And we did that.

01:45:50.240 --> 01:45:51.540
All right.

01:45:51.540 --> 01:45:52.840
So, what did Sam do?

01:45:52.840 --> 01:45:57.180
So when Sam was doing
his data preparation,

01:45:57.180 --> 01:46:00.420
so remember when we last left
Sam, he had first generated

01:46:00.420 --> 01:46:04.600
that very large list and then he
had started to lead down within

01:46:04.600 --> 01:46:06.910
that list to hone
in on the volumes

01:46:06.910 --> 01:46:08.630
that he really wanted
for his analysis.

01:46:08.630 --> 01:46:13.050
And then he went a step further
to restrict the materials

01:46:13.050 --> 01:46:14.810
that he was looking
at even more.

01:46:14.810 --> 01:46:18.920
So the first thing that he did
was he only kept the pages,

01:46:18.920 --> 01:46:23.620
the data that he was looking at,
retained page level information

01:46:23.620 --> 01:46:26.170
so he could see this was
page 1, this was page 2.

01:46:26.170 --> 01:46:28.340
And he said, I only
want the pages

01:46:28.340 --> 01:46:31.990
that included my search term, so
that creativ with the asterisk.

01:46:31.990 --> 01:46:33.540
And he got rid of
everything else

01:46:33.540 --> 01:46:35.850
because he decided he
was most interested

01:46:35.850 --> 01:46:40.040
in studying the discourse around
words like creativity or creativ

01:46:40.040 --> 01:46:42.540
and he didn't care if the word
occurred at the very beginning

01:46:42.540 --> 01:46:46.720
of the text and then at the
end never again, he didn't care

01:46:46.720 --> 01:46:48.580
of what was happening
at the end of the work.

01:46:48.580 --> 01:46:50.070
He just wanted to know
what was happening right

01:46:50.070 --> 01:46:51.370
around that word.

01:46:51.370 --> 01:46:54.000
But then proximity,
using pages is one way

01:46:54.000 --> 01:46:57.030
to model that proximity.

01:46:57.030 --> 01:47:00.930
And then, he discarded all
tokens such so as pronouns

01:47:00.930 --> 01:47:03.770
and conjunctions that he
didn't think carried meaning.

01:47:03.770 --> 01:47:05.340
So, he was keeping
what he considered

01:47:05.340 --> 01:47:08.570
to be the most meaningful terms.

01:47:08.570 --> 01:47:09.870
All right.

01:47:09.870 --> 01:47:12.470
So, how many of you have read
this piece "Against Cleaning"

01:47:12.470 --> 01:47:14.430
by Katie Rawson and
Trevor Munoz?

01:47:14.430 --> 01:47:15.730
It's really nice.

01:47:15.730 --> 01:47:17.040
We're not going to have
you read the whole thing.

01:47:17.040 --> 01:47:19.080
But I'd recommend that you
do if you're interested

01:47:19.080 --> 01:47:22.580
in exploring more of the
things that they're talking

01:47:22.580 --> 01:47:24.710
about in the passage that
we're going to look at.

01:47:24.710 --> 01:47:28.620
So, the project that they're
describing just for a little bit

01:47:28.620 --> 01:47:32.520
of context was working with data
from the "What's on the Menu?"

01:47:32.520 --> 01:47:35.460
project. And they were
talking about curetting menus,

01:47:35.460 --> 01:47:36.760
are you familiar with this?

01:47:36.760 --> 01:47:38.680
So, it's a big crowd
sourcing project

01:47:38.680 --> 01:47:43.240
from the New York Public Library
where they transcribe menus

01:47:43.240 --> 01:47:45.400
from restaurants in New York.

01:47:45.400 --> 01:47:48.570
And then dealing with this
massive trove of data,

01:47:48.570 --> 01:47:51.580
they had to decide, is
chocolate cake the same thing

01:47:51.580 --> 01:47:54.600
as chocolate torte for
the purposes of our data,

01:47:54.600 --> 01:47:57.830
is molten chocolate cake the
same thing as chocolate torte.

01:47:57.830 --> 01:47:59.990
And so, they were thinking
through this process

01:47:59.990 --> 01:48:04.830
of preparing and cleaning
their data and what these sort

01:48:04.830 --> 01:48:06.130
of choices that they
were making,

01:48:06.130 --> 01:48:08.600
how it would impact
research, their own research

01:48:08.600 --> 01:48:10.500
and other's research
on the corpus.

01:48:10.500 --> 01:48:11.800
All right.

01:48:11.800 --> 01:48:13.100
So, here's the paragraph.

01:48:13.100 --> 01:48:14.540
It's also on page
14 of your handout.

01:48:14.540 --> 01:48:17.990
I'll give you just a
second to read through it.

01:48:17.990 --> 01:48:21.490
OK. So, you made it to 151.

01:48:21.490 --> 01:48:23.880
We finally talk about
analyzing text data.

01:48:23.880 --> 01:48:25.290
You made it.

01:48:25.290 --> 01:48:27.330
You didn't know you're
getting into, did you?

01:48:27.330 --> 01:48:28.630
All right.

01:48:28.630 --> 01:48:30.080
So, we divide this topic up just

01:48:30.080 --> 01:48:32.160
like we did the gathering
text data piece

01:48:32.160 --> 01:48:35.810
into two separate sections.

01:48:35.810 --> 01:48:38.050
So, we're going to talk about
off-the-shelf-tools first

01:48:38.050 --> 01:48:39.350
and then we're going to talk

01:48:39.350 --> 01:48:41.970
about more advanced
do-it-yourself approaches

01:48:41.970 --> 01:48:43.270
to text analysis.

01:48:43.270 --> 01:48:45.090
So in this first half
that we're talking

01:48:45.090 --> 01:48:47.840
about off-the-shelf tools, we're
going to discuss the benefits

01:48:47.840 --> 01:48:50.080
and drawbacks of
using a pre-built tool

01:48:50.080 --> 01:48:51.380
for text analysis.

01:48:51.380 --> 01:48:54.950
We're going to learn about a
web-based topic modeling tool

01:48:54.950 --> 01:48:56.490
from the HTRC.

01:48:56.490 --> 01:49:00.110
And then, we're going to talk
about an attempt that Sam had

01:49:00.110 --> 01:49:03.180
to use an off-the-shelf tool
for his analysis and talk

01:49:03.180 --> 01:49:05.090
about his outcomes there.

01:49:05.090 --> 01:49:06.390
All right.

01:49:06.390 --> 01:49:08.700
So, where we'll end up,
we'll have some word clouds

01:49:08.700 --> 01:49:14.330
of some topics related to
public papers from the 1970s.

01:49:14.330 --> 01:49:16.070
All right.

01:49:16.070 --> 01:49:19.970
So, there are a handful of
pre-built text analysis tools

01:49:19.970 --> 01:49:21.660
on the market, so to speak.

01:49:21.660 --> 01:49:23.790
The benefit of these tools is

01:49:23.790 --> 01:49:26.750
that for the most part,
they're easy to use.

01:49:26.750 --> 01:49:29.870
So, they give the
impression of plug and play.

01:49:29.870 --> 01:49:31.170
You put data in.

01:49:31.170 --> 01:49:32.470
Your results come out.

01:49:32.470 --> 01:49:36.850
And because they don't
require programming or coding,

01:49:36.850 --> 01:49:38.150
they are good for teaching

01:49:38.150 --> 01:49:40.220
and for getting people
up and running.

01:49:40.220 --> 01:49:43.620
But you-- Once you start working
with scholars in this field

01:49:43.620 --> 01:49:46.730
for a little while, you find
that people have a threshold

01:49:46.730 --> 01:49:49.420
in which they say like this
doesn't do what I want it

01:49:49.420 --> 01:49:50.980
to do anymore, just
give me the data.

01:49:50.980 --> 01:49:53.700
That's something that we
hear a lot from researchers,

01:49:53.700 --> 01:49:55.000
just give me the data.

01:49:55.000 --> 01:49:58.870
So, the drawbacks of using these
pre-built off-the-shelf tools

01:49:58.870 --> 01:50:00.650
are that there's less control.

01:50:00.650 --> 01:50:02.790
And then because of the
way that they're built,

01:50:02.790 --> 01:50:05.160
there are limited capabilities
because you're constricted

01:50:05.160 --> 01:50:07.760
by the system as it exists.

01:50:07.760 --> 01:50:09.590
That said, it can
be very powerful

01:50:09.590 --> 01:50:12.700
for getting people just started
with text analysis for teaching,

01:50:12.700 --> 01:50:15.190
for quick examples where
you don't really need

01:50:15.190 --> 01:50:16.800
to do a deep dive.

01:50:16.800 --> 01:50:19.890
So, some examples here
are Voyant and Lexus

01:50:19.890 --> 01:50:23.010
and then also the
HTRC algorithms.

01:50:23.010 --> 01:50:26.430
So, we're going to be using
the topic modeling algorithm.

01:50:27.860 --> 01:50:29.160
All right.

01:50:29.160 --> 01:50:32.140
So then in contrast to
the off-the-shelf tools,

01:50:32.140 --> 01:50:34.230
there are these do-it-yourself
tools.

01:50:34.230 --> 01:50:37.740
So, these are an alternative
to these pre-built tools,

01:50:37.740 --> 01:50:40.560
and they generally involve
some kind of programming.

01:50:40.560 --> 01:50:43.440
So, you're going to have
to know how to code,

01:50:43.440 --> 01:50:45.880
how to install things from
the command line usually.

01:50:45.880 --> 01:50:48.540
You're going to have to have
sort of a deeper knowledge

01:50:48.540 --> 01:50:51.590
of the files that you're
working with, file types,

01:50:51.590 --> 01:50:55.790
and then also how to install
and get these programs going.

01:50:55.790 --> 01:50:59.650
But the benefits are that you
are running them on your own.

01:50:59.650 --> 01:51:02.140
So, it's more flexibility
for the researcher.

01:51:02.140 --> 01:51:05.340
Generally, they allow for more
parameterization and control

01:51:05.340 --> 01:51:06.750
over the research process.

01:51:06.750 --> 01:51:08.520
But again, they require
that technical knowledge

01:51:08.520 --> 01:51:12.650
that not everyone has and quite
honestly not everyone is willing

01:51:12.650 --> 01:51:15.890
to pick up or wants to
pick up, and that's OK too.

01:51:15.890 --> 01:51:19.580
But we're going to
return to these later.

01:51:19.580 --> 01:51:21.380
Yeah?

01:51:21.380 --> 01:51:33.810
[ Inaudible ]

01:51:33.810 --> 01:51:35.110
Yes. Yeah.

01:51:35.110 --> 01:51:37.970
So, you want to encourage
people to think critically

01:51:37.970 --> 01:51:40.370
about the tool that they're
using, report which tool

01:51:40.370 --> 01:51:43.280
that they use so that others
understand how they came

01:51:43.280 --> 01:51:46.130
to their conclusions.

01:51:46.130 --> 01:51:47.430
All right.

01:51:47.430 --> 01:51:49.780
So when you go about the process
of picking a pre-built tool

01:51:49.780 --> 01:51:52.550
or recommending a pre-built
tool, it really depends

01:51:52.550 --> 01:51:54.140
on the goal of the
researcher which is sort

01:51:54.140 --> 01:51:56.840
of a flippant thing to say here,
like well it depends, right?

01:51:56.840 --> 01:51:58.140
But it really does.

01:51:58.140 --> 01:52:01.670
So, the tools have different
strengths and weaknesses.

01:52:01.670 --> 01:52:04.840
So if you want to do a quick
analysis, get a visualization up

01:52:04.840 --> 01:52:06.140
and running really quickly.

01:52:06.140 --> 01:52:08.110
Voyant and Lexus are fantastic.

01:52:08.110 --> 01:52:09.610
They're easy to use.

01:52:09.610 --> 01:52:13.340
You can run and-- You can
install and run Voyant locally,

01:52:13.340 --> 01:52:17.380
but there is a web-based version
and Lexus is also web-based.

01:52:17.380 --> 01:52:19.960
If you have a researcher who
wants to do concordances,

01:52:19.960 --> 01:52:23.470
then they probably want to be
using a tool called AntConc

01:52:23.470 --> 01:52:25.130
which I don't have
much experience with,

01:52:25.130 --> 01:52:27.490
but my colleagues tell
me it's easy to use.

01:52:27.490 --> 01:52:31.380
Voyant also has a nice
concordance function.

01:52:31.380 --> 01:52:32.780
And then if someone
is interested

01:52:32.780 --> 01:52:34.450
in doing machine learning,

01:52:34.450 --> 01:52:38.330
there is a pre-built tool called
a WEKA, the WEKA workbench,

01:52:38.330 --> 01:52:40.600
and that's another
one that you install.

01:52:40.600 --> 01:52:43.040
You run it locally,
but it has an interface

01:52:43.040 --> 01:52:45.890
that the researcher walks
themselves through in order

01:52:45.890 --> 01:52:47.790
to carry out the
machine learning process.

01:52:47.790 --> 01:52:50.840
So, they don't have to
program a tool themselves.

01:52:54.490 --> 01:52:55.790
I hear typing.

01:52:55.790 --> 01:52:58.090
So, I'm just going to pause.

01:53:04.710 --> 01:53:06.010
All right.

01:53:06.010 --> 01:53:07.310
Are we good?

01:53:07.310 --> 01:53:08.610
OK. All right.

01:53:08.610 --> 01:53:09.910
So, the tools that we're--

01:53:09.910 --> 01:53:11.340
off-the-shelf tools
that we're looking

01:53:11.340 --> 01:53:13.920
at in particular today are--

01:53:13.920 --> 01:53:16.450
we're going to look at a topic
modeling tool that's part

01:53:16.450 --> 01:53:18.160
of the suite of algorithms
provided

01:53:18.160 --> 01:53:19.860
by the HathiTrust
Research Center.

01:53:19.860 --> 01:53:23.920
So, the HTRC algorithms are plug
and play text analysis tools.

01:53:23.920 --> 01:53:26.930
They're built into that
HTRC analytics interface,

01:53:26.930 --> 01:53:29.900
and that means that
they're mostly as is.

01:53:29.900 --> 01:53:32.140
So, you can do some
parameterization.

01:53:32.140 --> 01:53:36.020
You have some choices
about how to run your job

01:53:36.020 --> 01:53:37.530
when you do your analysis.

01:53:37.530 --> 01:53:40.070
But you're limited in
what you're allowed to set

01:53:40.070 --> 01:53:45.600
as a parameter and what is built
into the tool to ask the user.

01:53:45.600 --> 01:53:49.840
And they're built specifically
to analyze HTRC worksets.

01:53:49.840 --> 01:53:53.480
So the data that you moved
over earlier, that's the kind

01:53:53.480 --> 01:53:54.780
of things that you can analyze.

01:53:54.780 --> 01:53:56.610
In there, you can't
bring in outside data.

01:53:56.610 --> 01:53:57.910
Because remember,
we're just looking

01:53:57.910 --> 01:54:00.490
at these manifests
of volume IDs.

01:54:00.490 --> 01:54:01.790
So, they're good
when you want to work

01:54:01.790 --> 01:54:03.730
with HT texts specifically.

01:54:03.730 --> 01:54:05.690
All right.

01:54:05.690 --> 01:54:07.350
Some are task-oriented.

01:54:07.350 --> 01:54:09.040
Others are more analytic.

01:54:09.040 --> 01:54:11.850
We're going to be using the
topic modeling algorithm

01:54:11.850 --> 01:54:14.660
which is more of an
analytic type algorithm

01:54:14.660 --> 01:54:16.380
that helps the researcher
to get a feel

01:54:16.380 --> 01:54:20.640
for the content of
their data set.

01:54:22.220 --> 01:54:23.520
All right.

01:54:23.520 --> 01:54:24.820
How many of you are familiar

01:54:24.820 --> 01:54:26.380
with this idea of
a bag-of-words?

01:54:26.380 --> 01:54:27.680
All right.

01:54:27.680 --> 01:54:29.240
So, we're going to be
talking about topic modeling

01:54:29.240 --> 01:54:31.810
in this algorithm, and
topic modeling is a kind

01:54:31.810 --> 01:54:34.210
of text analysis process

01:54:34.210 --> 01:54:38.950
that uses a bag-of-words
set of paradigm.

01:54:38.950 --> 01:54:42.630
And when you're working
in this bag-of-words idea,

01:54:42.630 --> 01:54:48.420
approach to text analysis, you
stop caring about the word order

01:54:48.420 --> 01:54:50.120
and how it appeared on the page

01:54:50.120 --> 01:54:52.470
and you just care
that it was there.

01:54:52.470 --> 01:54:55.920
So, there are some algorithmic
processes that just want

01:54:55.920 --> 01:54:58.280
to know what was in the text.

01:54:58.280 --> 01:55:01.440
So, I sometimes imagine
this like I pick up the page

01:55:01.440 --> 01:55:04.200
and I shake it off and
the words come down.

01:55:04.200 --> 01:55:06.310
And now I have my bag
and I shake up my bag

01:55:06.310 --> 01:55:08.350
and the order is lost.

01:55:08.350 --> 01:55:10.630
So that means that
you're discarding things

01:55:10.630 --> 01:55:13.130
like grammar as well.

01:55:13.130 --> 01:55:16.430
So you're missing some of that
information but for the purposes

01:55:16.430 --> 01:55:17.950
of these algorithms, the way

01:55:17.950 --> 01:55:19.760
that they have been
conceived off by the folks

01:55:19.760 --> 01:55:23.120
who built them is that that
is OK, that doesn't matter.

01:55:23.120 --> 01:55:25.140
So here we see the
Gettysburg Address again

01:55:25.140 --> 01:55:26.950
but we see the words
all jumbled up.

01:55:26.950 --> 01:55:28.250
So this is our bag.

01:55:28.250 --> 01:55:31.730
We have our bag of words
at the bottom, all right.

01:55:31.730 --> 01:55:33.930
So we are talking
about topic modeling.

01:55:33.930 --> 01:55:35.910
So here we see some
of our key terms

01:55:35.910 --> 01:55:37.950
in the context of
topic modeling.

01:55:37.950 --> 01:55:40.610
So when you go about
topic modeling,

01:55:40.610 --> 01:55:43.710
when you use a topic modeling
algorithm, this is what happens.

01:55:43.710 --> 01:55:47.620
You chunk the text, you
hack it up into documents,

01:55:47.620 --> 01:55:49.200
and the documents
are just chunks.

01:55:49.200 --> 01:55:54.650
They're pieces of text and those
documents are bags of words.

01:55:54.650 --> 01:55:56.180
So the word order doesn't matter

01:55:56.180 --> 01:55:59.100
within these chunks
of documents.

01:55:59.100 --> 01:56:02.240
Generally, you remove stop
words or you remove some words.

01:56:02.240 --> 01:56:05.640
So the tool we're using
today remove stop words.

01:56:05.640 --> 01:56:09.330
Some topic modeling experts
are moving towards removing the

01:56:09.330 --> 01:56:11.680
infrequently used terms
instead of the stop words.

01:56:11.680 --> 01:56:14.730
But the one that we're going
to use today is the stop words

01:56:14.730 --> 01:56:17.150
and that's sort of traditionally
how it's been done.

01:56:17.150 --> 01:56:20.180
And then each word in
the document is compared

01:56:20.180 --> 01:56:23.160
and it's sort of like the
computer picks up a word

01:56:23.160 --> 01:56:27.590
and it goes, huh,
topic, what other bags,

01:56:27.590 --> 01:56:29.650
what other documents
does this word appear in

01:56:29.650 --> 01:56:32.640
and what other words
are around it.

01:56:32.640 --> 01:56:34.450
Check, check, check,
check, check, OK.

01:56:34.450 --> 01:56:35.930
Pick up another word, OK.

01:56:35.930 --> 01:56:38.970
You know here is chunk, which
other bags is it in, OK.

01:56:38.970 --> 01:56:41.400
And then it starts to
build a model, that's a--

01:56:41.400 --> 01:56:44.370
it's a guess, it's
probabilistic to say I think

01:56:44.370 --> 01:56:48.620
that if I find the word
Washington in this text,

01:56:48.620 --> 01:56:53.360
it is likely to be in the
context of these other words.

01:56:53.360 --> 01:56:56.300
And that is suggesting that
there's some kind of meaning

01:56:56.300 --> 01:57:00.670
in the way that we choose to
talk about certain topics.

01:57:00.670 --> 01:57:04.150
So if I'm talking about chicken
noodle soup, you're likely

01:57:04.150 --> 01:57:07.720
to find words like bowl and
spoon and chicken and winter

01:57:07.720 --> 01:57:10.810
and you're unlikely to find
things like sand, right?

01:57:10.810 --> 01:57:14.090
So there-- it just follow some
sort of logical progression.

01:57:14.090 --> 01:57:16.470
So the topic is just
a prediction

01:57:16.470 --> 01:57:18.020
of word co-occurrence.

01:57:18.020 --> 01:57:19.940
But don't think about
it as co-occurrence

01:57:19.940 --> 01:57:22.030
as like the words
appeared side by side.

01:57:22.030 --> 01:57:25.320
It's co-occurring in the same
bag, they're together in the bag

01:57:25.320 --> 01:57:29.550
so they are conceptually
likely about the same thing.

01:57:29.550 --> 01:57:31.970
All right, so here are some
tips for topic modeling.

01:57:31.970 --> 01:57:34.480
A treat topic modeling is
one step in the analysis.

01:57:34.480 --> 01:57:36.660
You don't just like throw
it in, beep boop boop

01:57:36.660 --> 01:57:38.210
and then you get something
out and you're like, oh,

01:57:38.210 --> 01:57:41.060
my text is about
whatever these topics say.

01:57:41.060 --> 01:57:43.830
Again, input, just like
we talked about before,

01:57:43.830 --> 01:57:45.330
it affects your output.

01:57:45.330 --> 01:57:48.570
So you can start to [inaudible]
your analysis in certain ways,

01:57:48.570 --> 01:57:50.710
such that maybe your
topics start to look

01:57:50.710 --> 01:57:53.620
like what you expected by
getting rid of lots and lots

01:57:53.620 --> 01:57:57.340
of words in your stop
word list and say

01:57:57.340 --> 01:58:00.760
by doing topic modeling
over one novel, OK.

01:58:00.760 --> 01:58:02.220
Well, I can read a novel, right.

01:58:02.220 --> 01:58:04.810
I can tell you what
topics are in it.

01:58:04.810 --> 01:58:06.840
The strength of topic
modeling generally has to do

01:58:06.840 --> 01:58:08.380
with like lots of text, right?

01:58:08.380 --> 01:58:11.130
So it tends to work better when
you're looking at more text.

01:58:11.130 --> 01:58:13.790
You want to be somewhat
familiar with your input data

01:58:13.790 --> 01:58:16.160
because there are some human
interpretation that happens

01:58:16.160 --> 01:58:18.630
with your results, so you
need to be able to say like,

01:58:18.630 --> 01:58:21.320
oh, no that's not good.

01:58:21.320 --> 01:58:23.020
What made this result not good?

01:58:23.020 --> 01:58:26.280
Is it my input data, is it
the way I set the parameters,

01:58:26.280 --> 01:58:29.900
what do I need to do in order
to get results that are good?

01:58:29.900 --> 01:58:31.480
And then you also need

01:58:31.480 --> 01:58:33.100
to understand the tool
that you're using.

01:58:33.100 --> 01:58:36.310
Depending on the tool, you'll
have different outcomes.

01:58:36.310 --> 01:58:38.200
And so, you need to
know what's happening.

01:58:38.200 --> 01:58:39.910
Just like Rick was saying,
if you want to go back

01:58:39.910 --> 01:58:42.790
and defend your work, you need
to know actually what happened

01:58:42.790 --> 01:58:44.090
with the tool that
you were using.

01:58:44.090 --> 01:58:46.020
And if you can't speak
to what was happening

01:58:46.020 --> 01:58:49.780
with the tool then you have less
firm ground to stand on in terms

01:58:49.780 --> 01:58:52.150
of defending your results.

01:58:52.150 --> 01:58:54.760
All right, so this
is the description

01:58:54.760 --> 01:58:57.420
of the HTRC's topic
modeling algorithm.

01:58:57.420 --> 01:59:01.850
The slide is probably a little
bit too small for you to see

01:59:01.850 --> 01:59:06.510
but that's OK because you
all are going to read it.

01:59:06.510 --> 01:59:08.190
So in-- well, actually,

01:59:08.190 --> 01:59:11.060
let's see how we're
doing in terms of time.

01:59:11.060 --> 01:59:15.090
No, you're not going to read

01:59:15.090 --> 01:59:17.220
because it's going
to take too long, OK.

01:59:17.220 --> 01:59:18.520
All right.

01:59:18.520 --> 01:59:22.420
So what the HTRC topic
modeling algorithm does is

01:59:22.420 --> 01:59:24.840
that it loads each page.

01:59:24.840 --> 01:59:28.430
So in HathiTrust every
page is a separate file,

01:59:28.430 --> 01:59:30.420
that's just an artifact
of the system.

01:59:30.420 --> 01:59:35.180
So this algorithm treats
every page as a document

01:59:35.180 --> 01:59:37.240
for the purposes of
the topic modeling.

01:59:37.240 --> 01:59:39.230
It removes the first
and the last line.

01:59:39.230 --> 01:59:40.830
So, it's doing some
preprocessing.

01:59:40.830 --> 01:59:44.370
It joins hyphenated words that
occur at the end of the line.

01:59:44.370 --> 01:59:46.420
It removes all tokens
that don't consist

01:59:46.420 --> 01:59:47.930
of alphanumeric characters
and we're going

01:59:47.930 --> 01:59:51.740
to see how there are some things
that squeak through anyway.

01:59:51.740 --> 01:59:55.840
I remove stop words based on a
filter that is set by the tool.

01:59:55.840 --> 01:59:58.310
So it's, again, a know
your tool sort of thing.

01:59:58.310 --> 02:00:00.070
And then it uses MALLET.

02:00:00.070 --> 02:00:02.240
How many of you have
heard of MALLET?

02:00:02.240 --> 02:00:04.890
Great. So MALLET is a
pretty commonly used topic

02:00:04.890 --> 02:00:06.190
modeling tool.

02:00:06.190 --> 02:00:09.090
It uses MALLET on the back-end
to build out these topics.

02:00:09.090 --> 02:00:10.680
All right.

02:00:10.680 --> 02:00:13.340
So we're back with our
sample reference question.

02:00:13.340 --> 02:00:16.780
And we are going to run
this algorithm all together

02:00:16.780 --> 02:00:20.330
on the same work set so that
we all have the same results

02:00:20.330 --> 02:00:23.210
to view together, because we're
going to go through the process

02:00:23.210 --> 02:00:24.810
of thinking about the results.

02:00:24.810 --> 02:00:26.710
All right.

02:00:26.710 --> 02:00:29.460
So, bring out the
HTRC analytic site.

02:00:29.460 --> 02:00:31.650
Make sure that you're
still logged in.

02:00:31.650 --> 02:00:35.140
I might have booted you
because you've been inactive.

02:00:37.760 --> 02:00:40.060
Yeah, sign in.

02:00:51.440 --> 02:00:53.740
OK.

02:01:05.770 --> 02:01:07.470
That should work.

02:01:07.470 --> 02:01:11.460
That's how long every password
at Indiana University has to be.

02:01:13.900 --> 02:01:17.560
So, I've-- We're going
to use just this one.

02:01:17.560 --> 02:01:19.900
Yeah, I always tell people
[inaudible] it makes you feel a

02:01:19.900 --> 02:01:21.200
little better for
having to do it.

02:01:21.200 --> 02:01:22.500
All right.

02:01:22.500 --> 02:01:24.580
So then once you're on
analytics.hathitrust.org,

02:01:24.580 --> 02:01:28.190
you're signed in, you're
going to click algorithms

02:01:28.190 --> 02:01:32.780
and you're going to see that
there are suite of tools here.

02:01:32.780 --> 02:01:34.950
You're taking this
workshop at a--

02:01:34.950 --> 02:01:39.140
not in opportune time but sort
of a weird moment because we're

02:01:39.140 --> 02:01:40.580
in the process of
refreshing these.

02:01:40.580 --> 02:01:42.190
So if you logged
back in next week,

02:01:42.190 --> 02:01:44.390
the topic modeling algorithm
will be slightly different.

02:01:44.390 --> 02:01:46.280
It will be slightly more robust.

02:01:46.280 --> 02:01:47.900
Individualization
will be cooler.

02:01:47.900 --> 02:01:50.710
So-- But this is just
a good time for us

02:01:50.710 --> 02:01:52.960
to practice what it's
like to read the results

02:01:52.960 --> 02:01:54.920
of a topic modeling algorithm.

02:01:54.920 --> 02:01:56.220
All right.

02:01:56.220 --> 02:01:57.520
So we have our description
here again

02:01:57.520 --> 02:02:02.530
about what's happening
and you-- Let's see.

02:02:04.290 --> 02:02:08.550
We are on page 16
of the handout.

02:02:08.550 --> 02:02:10.910
So this is actually
not hard to do

02:02:10.910 --> 02:02:13.520
and so I'm just going
to let you do it.

02:02:13.520 --> 02:02:14.900
So follow the directions.

02:02:14.900 --> 02:02:17.070
Run the topic modeling
algorithm.

02:02:17.070 --> 02:02:19.870
The key thing here before you
get too far is that you want

02:02:19.870 --> 02:02:21.520
to make sure you're
including public work sets

02:02:21.520 --> 02:02:24.430
and then you want to make
sure that you're getting

02:02:24.430 --> 02:02:26.610
to the same one that
we're all using.

02:02:26.610 --> 02:02:34.200
So type in EF and then
hit the down arrow.

02:02:34.200 --> 02:02:35.500
Yeah, you won't see all these.

02:02:35.500 --> 02:02:36.800
These are mine.

02:02:36.800 --> 02:02:38.120
Until you get-- These
are private.

02:02:38.120 --> 02:02:39.540
So, it'll work for you.

02:02:39.540 --> 02:02:41.510
Type EF and then hit
the down arrow once

02:02:41.510 --> 02:02:43.770
and you'll find poli
underscore science

02:02:43.770 --> 02:02:46.740
underscore ddrf@eleanordickson.

02:02:46.740 --> 02:02:49.240
They're alphabetical
after the @ by username.

02:02:49.240 --> 02:02:51.540
So--

02:02:54.600 --> 02:02:57.610
We'll all get the same results
if we use the same parameters

02:02:57.610 --> 02:03:00.070
because of the way the
results are being cached.

02:03:00.070 --> 02:03:05.380
So, check steps 5 and 6 to
do the parameterization.

02:03:05.380 --> 02:03:14.080
: So I'll mention two right now
that your jobs are going to go

02:03:14.080 --> 02:03:16.350
through like look
at the split that's

02:03:16.350 --> 02:03:18.760
because we're all doing the
same thing at the same time.

02:03:18.760 --> 02:03:20.380
That's a job that's
was already run,

02:03:20.380 --> 02:03:22.090
the exact the same parameters.

02:03:22.090 --> 02:03:25.280
So if you're teaching
a workshop and you want

02:03:25.280 --> 02:03:29.070
to make the time go faster,
run the job and advance

02:03:29.070 --> 02:03:30.820
that you expect the
students to do.

02:03:30.820 --> 02:03:33.870
The way that this is working
is that the job is being sent

02:03:33.870 --> 02:03:37.440
to the computers at Indiana
University and then you get put

02:03:37.440 --> 02:03:40.550
in a line with other jobs
on their compute cluster.

02:03:40.550 --> 02:03:43.210
And so, if you have a really
big job, you might have to wait

02:03:43.210 --> 02:03:45.760
in line longer, which isn't
how we're used to thinking

02:03:45.760 --> 02:03:48.950
about computers very often right
now like, OK, I want results

02:03:48.950 --> 02:03:51.840
but sometimes there-- it will be
sitting in that queuing and then

02:03:51.840 --> 02:03:54.070
on the running if it's
really big for a while.

02:03:54.070 --> 02:03:56.640
If it takes a really long
time, some things gone wrong

02:03:56.640 --> 02:03:59.770
and you should cancel and
try again or contact someone.

02:03:59.770 --> 02:04:02.480
But you can sometimes
have to wait a few minutes

02:04:02.480 --> 02:04:06.210
to get results back, especially
if it's an intensive process.

02:04:06.210 --> 02:04:08.030
All right, so here
are my results here.

02:04:08.030 --> 02:04:11.080
It looks like most of
you have gotten them.

02:04:11.080 --> 02:04:14.580
So we see the word clouds here.

02:04:14.580 --> 02:04:16.620
We see this XML file.

02:04:16.620 --> 02:04:19.820
This is part of what I was
saying would get more robust.

02:04:19.820 --> 02:04:22.430
The result files are much
nicer and the update.

02:04:22.430 --> 02:04:25.210
But you see the way that it's
weighted in the word cloud.

02:04:25.210 --> 02:04:28.930
Then you can also look at the--

02:04:28.930 --> 02:04:31.100
the words that appear
in each topic.

02:04:31.100 --> 02:04:32.760
But I'm going to move
back to the slides

02:04:32.760 --> 02:04:34.200
because it's actually
a little bigger there

02:04:34.200 --> 02:04:36.970
and it will be easier
for us to look at.

02:04:41.880 --> 02:04:43.180
All right.

02:04:43.180 --> 02:04:45.020
So these are some of the
topics that were generated.

02:04:45.020 --> 02:04:46.320
I guess these are all the topics

02:04:46.320 --> 02:04:49.650
that were generated
when we run it.

02:04:49.650 --> 02:04:51.300
The other thing to know
about topic modeling

02:04:51.300 --> 02:04:54.080
because it's probabilistic,
means that outside

02:04:54.080 --> 02:04:57.900
of this HTRC context where
the results are cached,

02:04:57.900 --> 02:05:00.080
if you run a topic model
and then run it again

02:05:00.080 --> 02:05:01.790
and then run it again,
you're going

02:05:01.790 --> 02:05:05.760
to get a different results every
time because it's just a guess.

02:05:05.760 --> 02:05:08.590
And so that's something
to expect to have happened

02:05:08.590 --> 02:05:11.130
if you start getting
into topic modeling.

02:05:11.130 --> 02:05:12.430
All right.

02:05:12.430 --> 02:05:14.200
So look-- So you'll see that
they're just numbered zero

02:05:14.200 --> 02:05:16.770
through four, so we
have five topics.

02:05:16.770 --> 02:05:19.190
And they're not named, right?

02:05:19.190 --> 02:05:22.130
The computer is not like,
oh yeah, this is, you know,

02:05:22.130 --> 02:05:24.890
public parks or whatever.

02:05:24.890 --> 02:05:27.550
So, what would you
name these topics?

02:05:27.550 --> 02:05:32.210
Can you think of any
that seem like, OK,

02:05:32.210 --> 02:05:36.650
I have an idea what
this topic is?

02:05:36.650 --> 02:05:39.130
&gt;&gt; Well, number three
looks like foreign affairs.

02:05:39.130 --> 02:05:40.430
&gt;&gt; Eleanor Dickson: OK.

02:05:40.430 --> 02:05:42.790
Yeah foreign affairs.

02:05:42.790 --> 02:05:46.010
Anything else?

02:05:46.010 --> 02:05:48.880
&gt;&gt; Number one looks
like communications.

02:05:48.880 --> 02:05:50.180
&gt;&gt; Eleanor Dickson: Yup.

02:05:50.180 --> 02:05:53.830
I definitely see
that, communication.

02:05:53.830 --> 02:05:56.390
Anything else?

02:05:56.390 --> 02:06:03.340
&gt;&gt; My favorite one is relating
to this propaganda flux.

02:06:03.340 --> 02:06:04.640
&gt;&gt; Eleanor Dickson: Yeah, yeah.

02:06:04.640 --> 02:06:07.970
I like that one too,
propaganda flux, yeah.

02:06:11.900 --> 02:06:14.200
Anything else?

02:06:15.340 --> 02:06:19.410
&gt;&gt; Four. It's like economy
or resource infrastructure.

02:06:19.410 --> 02:06:20.710
&gt;&gt; Eleanor Dickson: Yeah.

02:06:20.710 --> 02:06:23.940
So you know I didn't do which
I missed the key piece here,

02:06:23.940 --> 02:06:27.500
which is what were the text
that we even looked at, right?

02:06:27.500 --> 02:06:30.430
So, know your input date and
know your results, right?

02:06:30.430 --> 02:06:32.950
So these are-- Remember
before we were looking

02:06:32.950 --> 02:06:35.520
at those public papers when
we were doing the searching,

02:06:35.520 --> 02:06:37.580
these are all of
the public papers

02:06:37.580 --> 02:06:40.280
from presidents in the 1970s.

02:06:40.280 --> 02:06:42.700
So these are topics
from the 1970s,

02:06:42.700 --> 02:06:46.690
speeches across a couple
different presidents.

02:06:46.690 --> 02:06:48.410
Any other thoughts here?

02:06:48.410 --> 02:06:50.310
You'll see that we're getting
some weird tokens, right.

02:06:50.310 --> 02:06:51.610
&gt;&gt; That was weird.

02:06:51.610 --> 02:06:52.910
&gt;&gt; Eleanor Dickson: Yeah.

02:06:52.910 --> 02:06:54.210
We're seeing some artifacts
there, something is not--

02:06:54.210 --> 02:06:56.930
something is a little funky
with the tokenization.

02:06:56.930 --> 02:06:58.680
So that's a know your
tool sort of thing, right?

02:06:58.680 --> 02:06:59.980
Yeah.

02:06:59.980 --> 02:07:03.430
&gt;&gt; If you're thinking
about it in a context

02:07:03.430 --> 02:07:06.660
of a time it's talking
about, you know, question

02:07:06.660 --> 02:07:14.650
or it's trying to formulate
the topics before them.

02:07:14.650 --> 02:07:19.220
So if it looks disconnected
but if you think about it

02:07:19.220 --> 02:07:22.000
as to what might be going
on, you know, there's oil,

02:07:22.000 --> 02:07:25.270
there's management
question, American,

02:07:25.270 --> 02:07:28.030
good, is this trying to--

02:07:28.030 --> 02:07:29.330
&gt;&gt; Eleanor Dickson: Yeah.

02:07:29.330 --> 02:07:31.600
&gt;&gt; -- create an order
where there's chaos.

02:07:31.600 --> 02:07:32.900
&gt;&gt; Eleanor Dickson: Yeah.

02:07:32.900 --> 02:07:34.200
Go ahead.

02:07:34.200 --> 02:07:37.270
&gt;&gt; A topic one looks to me
somehow like community building

02:07:37.270 --> 02:07:38.960
or a consensus building.

02:07:38.960 --> 02:07:41.020
A lot of words in there
are like the things

02:07:41.020 --> 02:07:42.520
that someone would use
it and now you told

02:07:42.520 --> 02:07:43.820
that these are speeches.

02:07:43.820 --> 02:07:45.120
&gt;&gt; Eleanor Dickson: Yeah.

02:07:45.120 --> 02:07:46.780
&gt;&gt; So the kind of things that a
president might tell American--

02:07:46.780 --> 02:07:48.080
&gt;&gt; Eleanor Dickson: Yes, yeah.

02:07:48.080 --> 02:07:51.170
&gt;&gt; -- while working together
here in our government back

02:07:51.170 --> 02:07:54.720
on track or that kind of sort
of that seems to be, what,

02:07:54.720 --> 02:07:57.930
the purpose of it, bringing
the country together.

02:07:57.930 --> 02:07:59.230
&gt;&gt; Eleanor Dickson: Yeah.

02:07:59.230 --> 02:08:02.550
Yeah. Do you feel like looking
at these you get a general idea

02:08:02.550 --> 02:08:04.370
of what kinds of things
they might have been talking

02:08:04.370 --> 02:08:05.670
about during that time period?

02:08:05.670 --> 02:08:07.510
So someone might--
the topic by the way.

02:08:07.510 --> 02:08:22.730
[ Inaudible ]

02:08:22.730 --> 02:08:24.030
So it might be that for you,

02:08:24.030 --> 02:08:26.390
you feel like we didn't
have enough data that went

02:08:26.390 --> 02:08:28.900
in to get interesting
discernible topics

02:08:28.900 --> 02:08:30.200
from one another?

02:08:30.200 --> 02:08:32.000
Yeah.

02:08:32.000 --> 02:08:41.180
[ Inaudible ]

02:08:41.180 --> 02:08:44.480
Yeah. You got a feel, OK,
they're talking about energy

02:08:44.480 --> 02:08:47.220
and oil and these are the
kinds of words they're using

02:08:47.220 --> 02:08:49.680
in the context of that topic.

02:08:49.680 --> 02:08:51.130
Anything else?

02:08:51.130 --> 02:08:52.560
Any other questions
about topic modeling?

02:08:52.560 --> 02:08:54.950
I know it's sort of as-- can
feel like a strange concept.

02:08:54.950 --> 02:08:56.250
Yeah.

02:08:56.250 --> 02:08:59.330
&gt;&gt; Well I know-- I've
attended some other workshops,

02:08:59.330 --> 02:09:01.400
I haven't done it myself

02:09:01.400 --> 02:09:05.310
but approximately how
many pages was this?

02:09:05.310 --> 02:09:06.800
&gt;&gt; Eleanor Dickson: Let's see.

02:09:06.800 --> 02:09:12.210
So it was 19-- Somewhere
around 19 volumes, 16 to 19.

02:09:12.210 --> 02:09:17.090
So, it's something like 2,000
or 3,000 pages probably.

02:09:17.090 --> 02:09:20.050
&gt;&gt; OK. So that's a
huge number of chance

02:09:20.050 --> 02:09:22.580
and a small number of topics.

02:09:22.580 --> 02:09:23.880
&gt;&gt; Eleanor Dickson: Yeah.

02:09:23.880 --> 02:09:27.810
&gt;&gt; And so-- And this
looking at--

02:09:27.810 --> 02:09:32.580
so it's looking across each
of those thousands of pages

02:09:32.580 --> 02:09:36.030
and then trying to
find things that things

02:09:36.030 --> 02:09:37.400
that are [inaudible] across--

02:09:37.400 --> 02:09:38.700
&gt;&gt; Eleanor Dickson: Yup.

02:09:38.700 --> 02:09:40.000
&gt;&gt; -- all the stuff I think.

02:09:40.000 --> 02:09:41.300
&gt;&gt; Eleanor Dickson: Yup,
that's a great point.

02:09:41.300 --> 02:09:42.890
So if we had run this
with more topics,

02:09:42.890 --> 02:09:46.210
our results might have
felt more granular

02:09:46.210 --> 02:09:47.610
or they would have
been different.

02:09:47.610 --> 02:09:48.910
&gt;&gt; Like [inaudible]
might be able

02:09:48.910 --> 02:09:52.630
to see more different prompts
or different kinds of concerns

02:09:52.630 --> 02:09:56.160
like oil might broken
out from inflation.

02:09:56.160 --> 02:09:57.460
&gt;&gt; Eleanor Dickson:
Yeah, absolutely.

02:09:57.460 --> 02:09:58.760
That's a great point.

02:09:58.760 --> 02:10:00.060
Yup, because we have quite a bit
of text and just a few topics.

02:10:00.060 --> 02:10:02.750
&gt;&gt; And so that would be the
thing you might consider

02:10:02.750 --> 02:10:05.200
adjusting to see
what [inaudible].

02:10:05.200 --> 02:10:06.500
&gt;&gt; Eleanor Dickson: Yes.

02:10:06.500 --> 02:10:07.800
Yeah. This is the kind
of thing where you--

02:10:07.800 --> 02:10:09.100
I understand your point.

02:10:09.100 --> 02:10:11.110
You might go, OK, these
all seem really similar.

02:10:11.110 --> 02:10:14.020
What happens if I bump
it up to 10 or 20?

02:10:14.020 --> 02:10:16.020
Yeah. And then the new
algorithm that's coming out,

02:10:16.020 --> 02:10:18.620
you can train a whole
bunch, 20, 40, 60,

02:10:18.620 --> 02:10:20.130
80 and so you can get a feel
for what that looks like.

02:10:20.130 --> 02:10:22.270
&gt;&gt; But you won't be able to
chunk larger than [inaudible]?

02:10:22.270 --> 02:10:23.570
&gt;&gt; Eleanor Dickson:
I-- it's three--

02:10:23.570 --> 02:10:26.110
I think it treats
every volume as--

02:10:26.110 --> 02:10:27.560
I forget how it does
the chunking actually.

02:10:27.560 --> 02:10:28.860
I shouldn't say.

02:10:28.860 --> 02:10:30.160
Yeah.

02:10:30.160 --> 02:10:31.460
&gt;&gt; [Inaudible] using the page.

02:10:31.460 --> 02:10:32.760
&gt;&gt; Eleanor Dickson:
In here it's a page.

02:10:32.760 --> 02:10:34.060
Yeah. Yeah.

02:10:34.060 --> 02:10:35.360
Yeah. Any other thoughts?

02:10:35.360 --> 02:10:36.660
Yeah?

02:10:36.660 --> 02:10:37.960
&gt;&gt; So once you've got to the
topic modeling, if you wanted

02:10:37.960 --> 02:10:40.010
to go back then to
find out which pages,

02:10:40.010 --> 02:10:45.790
what is the impression of
those-- of your topics?

02:10:45.790 --> 02:10:48.820
How do you get back
to [inaudible]?

02:10:48.820 --> 02:10:50.120
&gt;&gt; Eleanor Dickson: Yeah.

02:10:50.120 --> 02:10:51.420
That's a great question.

02:10:51.420 --> 02:10:52.720
So that's to know
your tool thing.

02:10:52.720 --> 02:10:54.710
With this tool we're not seeing
what volume these topics were

02:10:54.710 --> 02:10:56.010
most prevalent in.

02:10:56.010 --> 02:10:59.420
Some other topic modeling
tools allow you to say, oh, OK,

02:10:59.420 --> 02:11:01.910
this is coming mostly
from this volume

02:11:01.910 --> 02:11:03.750
or I can see the way
the volumes relate

02:11:03.750 --> 02:11:05.400
to one another with the page.

02:11:05.400 --> 02:11:08.650
And I know Harry has worked
on projects, topic modeling

02:11:08.650 --> 02:11:11.540
where as a group because
she uses this nice example.

02:11:11.540 --> 02:11:14.400
They did topic modeling and
then they divided it up.

02:11:14.400 --> 02:11:15.700
You can correct me if I'm wrong.

02:11:15.700 --> 02:11:17.550
But they would go back
to the original text

02:11:17.550 --> 02:11:20.000
and think critically
about the topics

02:11:20.000 --> 02:11:22.640
and then the pages they were
looking at in order to try

02:11:22.640 --> 02:11:26.080
to hone in on what they thought
were the most relevant topics

02:11:26.080 --> 02:11:27.380
and things like that.

02:11:27.380 --> 02:11:28.680
Is that right?

02:11:28.680 --> 02:11:29.980
OK. Yeah. So that
sort of going back

02:11:29.980 --> 02:11:31.280
and forth is definitely a one--

02:11:31.280 --> 02:11:33.190
that's why we talk about
topic modeling as one step

02:11:33.190 --> 02:11:36.290
in the research process because
oftentimes researchers are going

02:11:36.290 --> 02:11:37.590
back and forth.

02:11:37.590 --> 02:11:40.470
Yeah. Did I answer the question?

02:11:40.470 --> 02:11:44.060
&gt;&gt; So there are-- to a
lot of ways to do it.

02:11:44.060 --> 02:11:45.360
&gt;&gt; Eleanor Dickson:
So with this tool, no.

02:11:45.360 --> 02:11:46.660
That's a know-- Yeah.

02:11:46.660 --> 02:11:47.960
That's a know your tool thing.

02:11:47.960 --> 02:11:49.260
&gt;&gt; You have to have
a different tool.

02:11:49.260 --> 02:11:50.560
&gt;&gt; Eleanor Dickson: Yeah.

02:11:50.560 --> 02:11:51.890
Yeah. So may-- so in this
case it's sort of like, OK,

02:11:51.890 --> 02:11:54.460
I have a feel of the
contour of my data.

02:11:54.460 --> 02:11:57.090
Maybe for the preliminary
researcher, that's enough

02:11:57.090 --> 02:11:59.350
and we're going to see
how somebody used this

02:11:59.350 --> 02:12:01.260
for preliminary research
or for our student, right?

02:12:01.260 --> 02:12:02.690
But if you feel like
this is enough

02:12:02.690 --> 02:12:04.860
for whatever they're
doing their coursework.

02:12:04.860 --> 02:12:08.230
But for a published paper, if
you really wanted to go deep,

02:12:08.230 --> 02:12:10.610
you might want to run
a more robust tool.

02:12:10.610 --> 02:12:12.620
Yeah. Go ahead.

02:12:12.620 --> 02:12:15.370
&gt;&gt; So, I mean, this is quite
interesting because it says

02:12:15.370 --> 02:12:18.590
like nuclear definitely being
tied here at the military,

02:12:18.590 --> 02:12:20.720
whereas the oil is
tied into like labors.

02:12:20.720 --> 02:12:22.020
That's interesting.

02:12:22.020 --> 02:12:24.510
But my question is like,
how confident can I be

02:12:24.510 --> 02:12:27.450
that this is-- right, where
is the data behind that

02:12:27.450 --> 02:12:29.240
or the numbers behind
like be confident

02:12:29.240 --> 02:12:32.300
of the validity of
this distinction?

02:12:32.300 --> 02:12:33.600
&gt;&gt; Eleanor Dickson: Yeah.

02:12:33.600 --> 02:12:34.900
That's a good question.

02:12:34.900 --> 02:12:36.200
Yeah.

02:12:36.200 --> 02:12:37.500
&gt;&gt; Is the online data
or anything there?

02:12:37.500 --> 02:12:39.210
I've got more background
in social science research.

02:12:39.210 --> 02:12:40.510
&gt;&gt; Eleanor Dickson: Yes.

02:12:40.510 --> 02:12:43.030
So, in this case you
could go back and look

02:12:43.030 --> 02:12:45.440
at the public domain
volumes and--

02:12:45.440 --> 02:12:47.850
because we were looking at
these presidential speeches,

02:12:47.850 --> 02:12:51.550
and then you could get a feel
too for what you're seeing.

02:12:51.550 --> 02:12:52.890
Oftentimes the results

02:12:52.890 --> 02:12:55.240
of the topic modeling
algorithm will give you the

02:12:55.240 --> 02:12:57.440
probability rankings.

02:12:57.440 --> 02:12:58.740
But it's-- that's sort

02:12:58.740 --> 02:13:00.040
of how you're building
your certainties.

02:13:00.040 --> 02:13:03.120
It's also sort of like know what
you're putting in to get a feel

02:13:03.120 --> 02:13:04.810
for what you're getting
out make sense.

02:13:04.810 --> 02:13:06.110
Yeah. Go ahead.

02:13:06.110 --> 02:13:09.230
&gt;&gt; So it's only a
different take on this.

02:13:09.230 --> 02:13:11.850
Given the way this
is set up, this--

02:13:11.850 --> 02:13:18.410
you've seen the volumes going
by [inaudible] where it seemed

02:13:18.410 --> 02:13:25.130
to be more important because of
the format, the genre to do this

02:13:25.130 --> 02:13:26.920
by paragraph, where you're going

02:13:26.920 --> 02:13:29.490
by significant theme
in each paragraph.

02:13:29.490 --> 02:13:30.880
&gt;&gt; Eleanor Dickson:
Or by speech, right?

02:13:30.880 --> 02:13:36.940
So there are different ways
that a researcher might.

02:13:36.940 --> 02:13:41.440
They might say, page
doesn't make sense for me.

02:13:41.440 --> 02:13:46.240
Maybe I wanted it
to be a speech.

02:13:46.240 --> 02:13:48.540
But do you think
that that would--

02:13:48.540 --> 02:13:52.690
&gt;&gt; Oh, I-- It was a
couple of years ago

02:13:52.690 --> 02:13:55.340
that I took the workshop,

02:13:55.340 --> 02:14:03.500
so it's possible things have
evolved since then but--

02:14:03.500 --> 02:14:07.440
and I can actually
[inaudible] for example.

02:14:07.440 --> 02:14:11.540
A lot of people are arguing if--

02:14:11.540 --> 02:14:15.880
that your chunks have

02:14:15.880 --> 02:14:19.740
to be exactly the
same number of word--

02:14:19.740 --> 02:14:21.040
&gt;&gt; Eleanor Dickson: Oh, yes.

02:14:21.040 --> 02:14:22.340
Yeah.

02:14:22.340 --> 02:14:23.640
&gt;&gt; -- and then--

02:14:23.640 --> 02:14:24.940
&gt;&gt; Eleanor Dickson:
That's a good point.

02:14:24.940 --> 02:14:26.240
Yeah.

02:14:26.240 --> 02:14:28.230
&gt;&gt; -- you have to be
build there [inaudible]--

02:14:28.230 --> 02:14:29.530
&gt;&gt; Eleanor Dickson: No.

02:14:29.530 --> 02:14:31.300
You're right about that.

02:14:31.300 --> 02:14:34.580
&gt;&gt; -- on the organization of the
actual content because you're--

02:14:34.580 --> 02:14:35.970
of the [inaudible] of the
conscious organization

02:14:35.970 --> 02:14:37.650
because what you're
trying to get

02:14:37.650 --> 02:14:39.390
at is the unconscious
or assumptions--

02:14:39.390 --> 02:14:40.690
&gt;&gt; Eleanor Dickson: Yup.

02:14:40.690 --> 02:14:42.010
&gt;&gt; -- that are embedded
in the text.

02:14:42.010 --> 02:14:43.310
&gt;&gt; Eleanor Dickson: Yeah.

02:14:43.310 --> 02:14:44.610
No. That's a great point.

02:14:44.610 --> 02:14:45.910
Yeah.

02:14:45.910 --> 02:14:48.830
&gt;&gt; And so-- I mean if
you really want to get

02:14:48.830 --> 02:14:52.940
into this [inaudible], there's
a ton of high level stuff--

02:14:52.940 --> 02:14:54.240
&gt;&gt; Eleanor Dickson: Yeah.

02:14:54.240 --> 02:14:55.540
&gt;&gt; -- [inaudible]
and sometimes--

02:14:55.540 --> 02:14:56.840
I'm not going to pronounce
it properly, I'm going to--

02:14:56.840 --> 02:15:00.560
there's like a three name kind--

02:15:00.560 --> 02:15:02.510
&gt;&gt; Eleanor Dickson: Oh,
latent Dirichlet allocation.

02:15:02.510 --> 02:15:03.810
Yeah.

02:15:03.810 --> 02:15:06.940
&gt;&gt; So you can read about
what the algorithms are.

02:15:06.940 --> 02:15:09.180
OK. And the [inaudible]
interested and there's a lot

02:15:09.180 --> 02:15:14.220
of great stuff out there that's
kind of complicating [inaudible]

02:15:14.220 --> 02:15:16.810
that you may have
the benefit for it.

02:15:16.810 --> 02:15:18.110
&gt;&gt; Eleanor Dickson: Yeah.

02:15:18.110 --> 02:15:19.410
And topic modeling I
think it was something

02:15:19.410 --> 02:15:21.050
that people are really
excited about for a while

02:15:21.050 --> 02:15:23.260
and then it had started to
wane in popularity with--

02:15:23.260 --> 02:15:24.560
amongst certain researchers.

02:15:24.560 --> 02:15:25.860
But it's, I think, really great
for conversation and workshop.

02:15:25.860 --> 02:15:27.160
Yeah. OK. Yeah?

02:15:27.160 --> 02:15:28.460
&gt;&gt; Just briefly.

02:15:28.460 --> 02:15:29.760
This tool only work
with data [inaudible].

02:15:29.760 --> 02:15:31.060
&gt;&gt; Eleanor Dickson: Yes.

02:15:31.060 --> 02:15:32.360
Yup. Yeah, these official
ones that are like that.

02:15:32.360 --> 02:15:33.660
Yeah.

02:15:33.660 --> 02:15:34.960
&gt;&gt; But like going on
you can put your--

02:15:34.960 --> 02:15:36.260
&gt;&gt; Eleanor Dickson: Yeah.

02:15:36.260 --> 02:15:37.560
&gt;&gt; -- own [inaudible].

02:15:37.560 --> 02:15:38.860
&gt;&gt; Eleanor Dickson: Yeah.

02:15:38.860 --> 02:15:40.160
Yeah.

02:15:40.160 --> 02:15:41.460
&gt;&gt; So [inaudible] that
box that can do this.

02:15:41.460 --> 02:15:42.760
&gt;&gt; Eleanor Dickson: Yeah.

02:15:42.760 --> 02:15:44.060
Yes. Yeah.

02:15:44.060 --> 02:15:45.360
Well, Voyant doesn't do
topic modeling but, yeah.

02:15:45.360 --> 02:15:46.660
&gt;&gt; All right.

02:15:46.660 --> 02:15:47.960
OK. Yeah. But some of them.

02:15:47.960 --> 02:15:49.260
Sorry. Yeah.

02:15:49.260 --> 02:15:50.560
&gt;&gt; Eleanor Dickson: All right.

02:15:50.560 --> 02:15:51.860
So we did this.

02:15:51.860 --> 02:15:53.160
All right.

02:15:53.160 --> 02:15:54.460
So let's talk about
Sam really quickly.

02:15:54.460 --> 02:15:55.760
So we've been tracking Sam on
the more successful track he was

02:15:55.760 --> 02:15:57.060
on in his research project.

02:15:57.060 --> 02:15:58.360
Remember at the very beginning
we talked about sort of the fits

02:15:58.360 --> 02:15:59.660
and starts of doing
this kind of research.

02:15:59.660 --> 02:16:00.960
So before Sam started building
out his true Creativity Corpus,

02:16:00.960 --> 02:16:02.260
he built a corpus of text
from the public domain--

02:16:02.260 --> 02:16:03.560
from 1950 on that included the
word creativity in HathiTrust.

02:16:03.560 --> 02:16:04.860
And then he used the HTRC
topic modeling algorithm

02:16:04.860 --> 02:16:06.160
to get a feel for
what was there.

02:16:06.160 --> 02:16:07.460
And these are some of
the topics that he got.

02:16:07.460 --> 02:16:08.760
They might be pretty
small for you.

02:16:08.760 --> 02:16:10.060
Hopefully you can see them OK.

02:16:10.060 --> 02:16:11.360
Thanks. Well, actually,
let me do this.

02:16:11.360 --> 02:16:12.660
So I'm just going to zoom in.

02:16:12.660 --> 02:16:13.960
I don't know.

02:16:13.960 --> 02:16:15.260
I can do that.

02:16:15.260 --> 02:16:16.560
All right.

02:16:16.560 --> 02:16:17.860
Does that help you
see it all though?

02:16:17.860 --> 02:16:19.160
OK. Well, I might
have just ruined it.

02:16:19.160 --> 02:16:20.460
No.

02:16:20.460 --> 02:16:21.760
Yeah. This is what
I get for closing

02:16:21.760 --> 02:16:23.060
with the mouse, isn't it?

02:16:23.060 --> 02:16:25.010
OK. Well, I'm not going to
try to make it bigger for you.

02:16:25.010 --> 02:16:26.310
That was a failure.

02:16:26.310 --> 02:16:27.610
All right.

02:16:27.610 --> 02:16:28.910
Here they are again.

02:16:28.910 --> 02:16:30.210
OK. So these are some of
the topics that he got.

02:16:30.210 --> 02:16:32.510
What do you think
about these topics?

02:16:35.130 --> 02:16:41.850
So, he has things like system
planning, state, local, public,

02:16:41.850 --> 02:16:45.300
programs, children time, school,
children education, science,

02:16:45.300 --> 02:16:48.470
life, human, man, world, nature,
body, forms, here's order, form.

02:16:48.470 --> 02:16:51.860
What do you think
about these topics?

02:16:51.860 --> 02:16:53.160
Yeah?

02:16:53.160 --> 02:16:55.610
&gt;&gt; Can I have kind of
a procedural question?

02:16:55.610 --> 02:17:00.090
Well, just putting them in
order, I've noticed for instance

02:17:00.090 --> 02:17:03.500
when we did this test
that the first group

02:17:03.500 --> 02:17:04.890
of the letters were smaller.

02:17:04.890 --> 02:17:08.840
Does that mean that, I mean,
is there any rhyme or reason

02:17:08.840 --> 02:17:12.270
to how they are presented
in this order?

02:17:12.270 --> 02:17:13.570
&gt;&gt; Eleanor Dickson: Yeah.

02:17:13.570 --> 02:17:15.280
So the color doesn't mean
much except that it seems

02:17:15.280 --> 02:17:17.510
like there's some kind of
ranking that happens in terms

02:17:17.510 --> 02:17:19.790
of like this big
one is dark blue.

02:17:19.790 --> 02:17:22.150
But the size has to
do with the prevalence

02:17:22.150 --> 02:17:24.320
of that word within the topic.

02:17:24.320 --> 02:17:27.770
&gt;&gt; So, for instance on our
first result of what we did,

02:17:27.770 --> 02:17:29.070
nothing was a standout.

02:17:29.070 --> 02:17:31.440
And that means that all
of those words occurred

02:17:31.440 --> 02:17:33.000
about the same time.

02:17:33.000 --> 02:17:34.630
&gt;&gt; Eleanor Dickson: Within the
context of those other words.

02:17:34.630 --> 02:17:36.830
Yeah. But sometimes
there's one that's like sort

02:17:36.830 --> 02:17:40.510
of more salient terms, it's more
characteristic of the topic.

02:17:40.510 --> 02:17:44.500
&gt;&gt; So is there a reason
why one comes back with one

02:17:44.500 --> 02:17:47.580
and the next one is two
and the next one is three?

02:17:47.580 --> 02:17:48.880
&gt;&gt; Eleanor Dickson: No.

02:17:48.880 --> 02:17:50.180
I don't think that that order--

02:17:50.180 --> 02:17:51.480
I don't think that
order has meaning.

02:17:51.480 --> 02:17:52.780
&gt;&gt; So that's just random?

02:17:52.780 --> 02:17:54.080
&gt;&gt; Eleanor : Yeah.

02:17:54.080 --> 02:17:56.260
I'm pretty sure that that
is not a meaningful order.

02:17:56.260 --> 02:17:58.260
&gt;&gt; I think it has to do
with the order in terms

02:17:58.260 --> 02:18:00.330
of the words in the test.

02:18:00.330 --> 02:18:01.630
&gt;&gt; Eleanor Dickson: Yeah.

02:18:01.630 --> 02:18:02.930
&gt;&gt; As knowledge go through.

02:18:02.930 --> 02:18:04.230
&gt;&gt; Eleanor Dickson: But
I don't think it means

02:18:04.230 --> 02:18:05.530
like this one is the most
important topic or something.

02:18:05.530 --> 02:18:08.110
But I'm-- I would want to
check myself before I say

02:18:08.110 --> 02:18:09.410
that with certainty.

02:18:09.410 --> 02:18:10.710
Yeah. All right.

02:18:10.710 --> 02:18:12.190
Any thoughts about
Sam's topics here?

02:18:12.190 --> 02:18:13.490
Yeah? Go ahead.

02:18:13.490 --> 02:18:16.720
&gt;&gt; So I think it's interesting
just the different facets

02:18:16.720 --> 02:18:19.370
of creativity that
it's demonstrating.

02:18:19.370 --> 02:18:22.260
Some of them I would have
expected [inaudible] like art

02:18:22.260 --> 02:18:23.560
and colors and children,
but the [inaudible],

02:18:23.560 --> 02:18:24.860
I have like state
and local like--

02:18:24.860 --> 02:18:26.160
&gt;&gt; Eleanor Dickson: Yeah.

02:18:26.160 --> 02:18:27.460
&gt;&gt; -- many.

02:18:27.460 --> 02:18:28.760
If we think about like
there's creativity in planning

02:18:28.760 --> 02:18:30.060
that happens, would that
wouldn't have been right?

02:18:30.060 --> 02:18:31.360
&gt;&gt; Eleanor Dickson: Yeah.

02:18:31.360 --> 02:18:32.660
&gt;&gt; So, it was just kind
of surprising thing,

02:18:32.660 --> 02:18:34.040
but I can see how it fits.

02:18:34.040 --> 02:18:35.760
&gt;&gt; Eleanor Dickson: Can anyone
think of why you're seeing a lot

02:18:35.760 --> 02:18:38.130
about state and local
government?

02:18:38.130 --> 02:18:39.430
&gt;&gt; Yeah.

02:18:39.430 --> 02:18:40.810
&gt;&gt; So, I was thinking
about that corpus

02:18:40.810 --> 02:18:42.910
and then it's 1950
[inaudible] literature

02:18:42.910 --> 02:18:44.490
which are mostly [inaudible].

02:18:44.490 --> 02:18:45.790
&gt;&gt; Eleanor Dickson: Yeah.

02:18:45.790 --> 02:18:47.090
Yeah. So--

02:18:47.090 --> 02:18:48.390
&gt;&gt; That's an easy--

02:18:48.390 --> 02:18:49.690
&gt;&gt; Eleanor Dickson: Yeah.

02:18:49.690 --> 02:18:50.990
So, if you're interested

02:18:50.990 --> 02:18:52.290
in tracking how the term
creativity changes over time,

02:18:52.290 --> 02:18:53.590
do you think that only
looking at public domain

02:18:53.590 --> 02:18:56.030
after 1950 is the best way
to build your dissertation?

02:18:56.030 --> 02:18:57.430
Probably not.

02:18:57.430 --> 02:18:59.880
Yeah. So this goes back-- yeah?

02:18:59.880 --> 02:19:01.180
&gt;&gt; [Inaudible] if you go back.

02:19:01.180 --> 02:19:02.480
&gt;&gt; Eleanor Dickson: Yeah.

02:19:02.480 --> 02:19:03.780
&gt;&gt; You see man and man--

02:19:03.780 --> 02:19:05.080
&gt;&gt; Eleanor Dickson: Yeah.

02:19:05.080 --> 02:19:06.380
Uh-huh.

02:19:06.380 --> 02:19:07.680
&gt;&gt; -- that's the one.

02:19:07.680 --> 02:19:08.980
And if you were combining
[inaudible]

02:19:08.980 --> 02:19:10.280
and man together it
would be twice as big.

02:19:10.280 --> 02:19:12.960
And so, yeah, there's been
some kind of [inaudible] about,

02:19:12.960 --> 02:19:17.210
you know, the benefit to
man if people but other--

02:19:17.210 --> 02:19:20.550
on the right people
is the biggest word

02:19:20.550 --> 02:19:22.200
in a very small topic
over there.

02:19:22.200 --> 02:19:24.320
So the [inaudible] would
explore that more [inaudible].

02:19:24.320 --> 02:19:25.620
&gt;&gt; Eleanor Dickson: Yeah.

02:19:25.620 --> 02:19:26.920
Yeah. That's really interesting.

02:19:26.920 --> 02:19:28.220
&gt;&gt; You could see
one in [inaudible].

02:19:28.220 --> 02:19:29.520
&gt;&gt; Eleanor Dickson: Yeah.

02:19:29.520 --> 02:19:32.050
Yeah. So this is an
example of being familiar

02:19:32.050 --> 02:19:35.490
with your input text and then
also examining your results

02:19:35.490 --> 02:19:36.790
to see if they make sense.

02:19:36.790 --> 02:19:39.180
So you could see a less
discerning researcher getting

02:19:39.180 --> 02:19:42.120
this out and writing a
chapter of their dissertation

02:19:42.120 --> 02:19:44.200
with some thoughts about the way

02:19:44.200 --> 02:19:49.290
that the term creativity has
changed to be really important

02:19:49.290 --> 02:19:51.940
in governmental documents
or government speak.

02:19:51.940 --> 02:19:53.360
But Sam thought, I don't know.

02:19:53.360 --> 02:19:54.660
This isn't quite right.

02:19:54.660 --> 02:19:58.100
So what he ended up doing was
taking a different approach.

02:19:58.100 --> 02:19:59.900
So this is the sort
of start and the stop

02:19:59.900 --> 02:20:01.670
of this kind of research.

02:20:01.670 --> 02:20:05.050
So that's when he started using
those extracted features files.

02:20:05.050 --> 02:20:08.500
So he was able to get a
data that was in copyright

02:20:08.500 --> 02:20:09.940
and not just in the
public domain.

02:20:09.940 --> 02:20:11.410
So he could make the argument

02:20:11.410 --> 02:20:13.250
that he wanted to
make in his work.

02:20:13.250 --> 02:20:14.790
All right.

02:20:14.790 --> 02:20:16.370
So this again we just throw it.

02:20:16.370 --> 02:20:18.430
So if you missed when I was
showing the slide earlier,

02:20:18.430 --> 02:20:20.390
all of the examples of the
tools, this is a good time

02:20:20.390 --> 02:20:22.390
to start scribbling
them down again.

02:20:22.390 --> 02:20:25.450
So, again, these are some
other pre-built tools.

02:20:25.450 --> 02:20:27.880
Which one you would want to
recommend depends on the goal

02:20:27.880 --> 02:20:30.380
for analysis, something
like Voyant

02:20:30.380 --> 02:20:33.550
and Lexos are good
web-based tools.

02:20:33.550 --> 02:20:36.950
They make really pretty
nice clean visualizations.

02:20:36.950 --> 02:20:39.370
Lexos has a text
cleaning function built-in

02:20:39.370 --> 02:20:40.920
which I find quite nice.

02:20:40.920 --> 02:20:43.050
AntConc for concordances,
the WEKA again

02:20:43.050 --> 02:20:45.850
and then the HTRC
algorithms if you want

02:20:45.850 --> 02:20:48.980
to us HT text specifically.

02:20:48.980 --> 02:20:50.280
All right.

02:20:50.280 --> 02:20:52.380
I want to see how
we're doing on time.

02:20:52.380 --> 02:20:53.810
Anyway, we're not too far back.

02:20:53.810 --> 02:20:56.300
OK. So can you think
about the kinds

02:20:56.300 --> 02:20:58.680
of researchers you might
recommend these off the shelf

02:20:58.680 --> 02:20:59.980
tools to?

02:20:59.980 --> 02:21:03.310
And if you have done this
before this recommending,

02:21:03.310 --> 02:21:05.730
what techniques have you used

02:21:05.730 --> 02:21:09.450
for introducing them
to researchers?

02:21:09.450 --> 02:21:11.350
Yeah? Yeah.

02:21:11.350 --> 02:21:12.650
Go ahead.

02:21:12.650 --> 02:21:17.430
&gt;&gt; I'm going to be talking
to some graduate students

02:21:17.430 --> 02:21:19.010
with childhood studies
coming up and they're going

02:21:19.010 --> 02:21:20.310
to be doing more
social science research.

02:21:20.310 --> 02:21:22.770
And I'm thinking this is
something they might really be

02:21:22.770 --> 02:21:24.660
interested in because
there would be a lot

02:21:24.660 --> 02:21:26.830
of [inaudible] documents and
all that to check and see,

02:21:26.830 --> 02:21:28.540
the children's bureaus
and their mother's bureau.

02:21:28.540 --> 02:21:31.670
But if they could narrow
beyond just that document set,

02:21:31.670 --> 02:21:34.390
maybe I can put that
together for them in advance.

02:21:34.390 --> 02:21:35.690
&gt;&gt; Eleanor Dickson: Yeah.

02:21:35.690 --> 02:21:39.210
&gt;&gt; Then they could use that
as a way to practice looking

02:21:39.210 --> 02:21:44.100
for terms or just using
that as a dataset.

02:21:44.100 --> 02:21:45.400
&gt;&gt; Eleanor Dickson: Yeah.

02:21:45.400 --> 02:21:46.700
That's great.

02:21:46.700 --> 02:21:48.280
&gt;&gt; I did have one question
because I was wondering

02:21:48.280 --> 02:21:50.880
if there's any way, so
I've worked with someone

02:21:50.880 --> 02:21:57.180
who is doing a lot of research
on comparing the number of miles

02:21:57.180 --> 02:22:00.250
and roads in specific
countries and we've used Hathi

02:22:00.250 --> 02:22:02.450
and it will come up with charts.

02:22:02.450 --> 02:22:03.750
&gt;&gt; Eleanor Dickson: Yeah.

02:22:03.750 --> 02:22:05.050
The charts.

02:22:05.050 --> 02:22:06.350
Yeah.

02:22:06.350 --> 02:22:08.200
&gt;&gt; Is there any way to pull
out the data from charts?

02:22:08.200 --> 02:22:10.050
&gt;&gt; Eleanor Dickson:
Not without going back

02:22:10.050 --> 02:22:15.620
to the original document
and redoing the OCR process.

02:22:15.620 --> 02:22:17.040
&gt;&gt; So, no?

02:22:17.040 --> 02:22:18.340
&gt;&gt; Eleanor Dickson: Yeah.

02:22:18.340 --> 02:22:19.640
It's really complicated.

02:22:19.640 --> 02:22:20.940
Yeah. Yeah.

02:22:20.940 --> 02:22:22.240
I wish it were easier.

02:22:22.240 --> 02:22:26.540
Yeah. Any other questions
or thoughts?

02:22:26.540 --> 02:22:27.840
Yeah?

02:22:27.840 --> 02:22:29.140
&gt;&gt; Well, this again is

02:22:29.140 --> 02:22:31.300
in researcher's first
say or for the purpose.

02:22:31.300 --> 02:22:34.190
I guess what I'm
trying to say is some

02:22:34.190 --> 02:22:38.070
of these tools are really
good way to convince people

02:22:38.070 --> 02:22:42.020
that there might be a benefit
to exploring digital research

02:22:42.020 --> 02:22:44.990
in either the humanities
and the social sciences

02:22:44.990 --> 02:22:46.640
that haven't been
already doing it.

02:22:46.640 --> 02:22:47.940
&gt;&gt; Eleanor Dickson: Right.

02:22:47.940 --> 02:22:50.300
&gt;&gt; Because it is-- especially
we find ways these things would

02:22:50.300 --> 02:22:52.310
seem relevant to them--

02:22:52.310 --> 02:22:53.610
&gt;&gt; Eleanor Dickson: Right.

02:22:53.610 --> 02:22:55.160
&gt;&gt; -- and [inaudible] an example
that teaches them a little bit

02:22:55.160 --> 02:22:59.230
of something because there's
a lot of skepticism now

02:22:59.230 --> 02:23:02.170
about why should we be investing
in this infrastructure.

02:23:02.170 --> 02:23:05.530
This is just me speaking.

02:23:05.530 --> 02:23:07.740
We're here at the library.

02:23:07.740 --> 02:23:11.170
There's an infrastructure
of providing the content

02:23:11.170 --> 02:23:13.500
to researchers and then there's
the additional infrastructure

02:23:13.500 --> 02:23:15.240
of training our own staff

02:23:15.240 --> 02:23:17.420
to understand what the
researcher is doing with it

02:23:17.420 --> 02:23:21.280
and both of those
are requiring some--

02:23:21.280 --> 02:23:22.580
&gt;&gt; Eleanor: Yeah.

02:23:22.580 --> 02:23:23.880
&gt;&gt; -- the report.

02:23:23.880 --> 02:23:25.180
&gt;&gt; Eleanor Dickson: Yeah.

02:23:25.180 --> 02:23:26.480
Yeah.

02:23:26.480 --> 02:23:27.780
&gt;&gt; So, actually having
people see the results can be

02:23:27.780 --> 02:23:29.080
very helpful.

02:23:29.080 --> 02:23:30.380
&gt;&gt; Eleanor Dickson: Yeah.

02:23:30.380 --> 02:23:31.680
And a good way to say, OK,
here are some possibilities

02:23:31.680 --> 02:23:32.980
and you don't have to learn
how to code right now.

02:23:32.980 --> 02:23:34.280
That doesn't have
to be step one,

02:23:34.280 --> 02:23:35.580
I need to become a programmer.

02:23:35.580 --> 02:23:36.880
Yeah. Just what's possible.

02:23:36.880 --> 02:23:38.430
&gt;&gt; Yeah. And just, you
know, it shows some of these

02:23:38.430 --> 02:23:40.050
to not [inaudible] they do,

02:23:40.050 --> 02:23:42.790
but they help people understand
the work that's being presented

02:23:42.790 --> 02:23:44.090
to them.

02:23:44.090 --> 02:23:45.390
&gt;&gt; Eleanor Dickson: Yup.

02:23:45.390 --> 02:23:46.690
Yeah.

02:23:46.690 --> 02:23:47.990
&gt;&gt; So they can understand
the concepts too,

02:23:47.990 --> 02:23:49.290
concepts [inaudible].

02:23:49.290 --> 02:23:50.590
&gt;&gt; Eleanor Dickson: Yeah.

02:23:50.590 --> 02:23:51.890
So they can evaluate
other's work better.

02:23:51.890 --> 02:23:53.190
Yeah. Did you have a--

02:23:53.190 --> 02:23:54.490
&gt;&gt; Yeah. I was just going to
[inaudible] off the shelf tools

02:23:54.490 --> 02:23:55.790
or anything that's
an assignment.

02:23:55.790 --> 02:23:57.090
&gt;&gt; Eleanor Dickson: Yeah.

02:23:57.090 --> 02:23:58.560
&gt;&gt; If they-- I mean at the
most they have a semester tour

02:23:58.560 --> 02:23:59.860
about that.

02:23:59.860 --> 02:24:02.540
Your average [inaudible] tough
time to learn how to code.

02:24:02.540 --> 02:24:04.340
&gt;&gt; Eleanor Dickson: Yeah.

02:24:04.340 --> 02:24:07.720
[ Inaudible ]

02:24:07.720 --> 02:24:09.080
Yeah. That's great.

02:24:09.080 --> 02:24:10.380
It's like what are you--

02:24:10.380 --> 02:24:11.820
what's the skill you're
trying to teach them?

02:24:11.820 --> 02:24:14.320
Is the skill the
interpreting the results?

02:24:14.320 --> 02:24:15.680
Is the skill the using the tool

02:24:15.680 --> 02:24:16.980
or is it learning
how to program?

02:24:16.980 --> 02:24:18.940
And a sort of a weighing--

02:24:18.940 --> 02:24:21.150
oftentimes you can
get bogged down.

02:24:21.150 --> 02:24:23.450
Yeah. Any other thoughts?

02:24:24.670 --> 02:24:26.470
Yeah?

02:24:26.470 --> 02:24:32.550
[ Inaudible ]

02:24:32.550 --> 02:24:34.320
Yes. Yeah.

02:24:34.320 --> 02:24:36.780
[ Inaudible ]

02:24:36.780 --> 02:24:40.910
Yeah. So there is-- we
are leaving into a break.

02:24:40.910 --> 02:24:42.540
So I'm going to go
out of the slides.

02:24:42.540 --> 02:24:45.330
And I'll show you where
you can find the list.

02:24:45.330 --> 02:24:47.650
[ Inaudible ]

02:24:47.650 --> 02:24:54.580
Yeah. So, the unfortunate
thing about the way--

02:24:54.580 --> 02:24:57.780
So, it's really hard to
tokenize certain languages.

02:24:57.780 --> 02:24:59.800
So, split it into
chunks because of the way

02:24:59.800 --> 02:25:01.990
that the characters
are rendered.

02:25:01.990 --> 02:25:04.800
Especially CJK characters,
the Chinese, Japanese

02:25:04.800 --> 02:25:08.030
and Korean characters, the
tokenization is just bad

02:25:08.030 --> 02:25:11.700
and the tools don't
work as well with them.

02:25:11.700 --> 02:25:14.650
The topic modeling tool that
we have right works best

02:25:14.650 --> 02:25:17.290
on English language text
and it does OK I think

02:25:17.290 --> 02:25:18.630
with some non-English text.

02:25:18.630 --> 02:25:19.990
But if it's non-Roman
characters,

02:25:19.990 --> 02:25:21.470
then it gets really wonky.

02:25:21.470 --> 02:25:26.330
But I will show you here
just the list of languages.

02:25:27.620 --> 02:25:29.460
So you have an idea of
what some of those--

02:25:29.460 --> 02:25:31.160
some of the breakdown is like.

02:25:31.160 --> 02:25:33.950
You can see it's quite
a bit of English.

02:25:33.950 --> 02:25:36.140
And then there's quite a bit
of German and French and sort

02:25:36.140 --> 02:25:37.850
of tails off from there.

02:25:37.850 --> 02:25:40.850
Yeah. OK. So we're going
to come back to talking

02:25:40.850 --> 02:25:45.140
about Python a little bit more
thinking again at how you pick

02:25:45.140 --> 02:25:48.070
up and reuse somebody else's
code that they've written.

02:25:48.070 --> 02:25:49.370
And also we're going to talk

02:25:49.370 --> 02:25:51.850
about installing
libraries for Python.

02:25:51.850 --> 02:25:55.110
They mix it so that you can
do research more easily,

02:25:55.110 --> 02:25:56.630
do work more easily.

02:25:56.630 --> 02:25:59.310
So we're going to review
some text analysis strategies

02:25:59.310 --> 02:26:01.520
for advanced researchers
to think about when

02:26:01.520 --> 02:26:04.020
to make skill-appropriate
recommendations.

02:26:04.020 --> 02:26:06.450
We'll explore text analysis
methods in more depth.

02:26:06.450 --> 02:26:08.220
We'll use an HTRC dataset

02:26:08.220 --> 02:26:10.560
to conduct exploratory
data analysis.

02:26:10.560 --> 02:26:14.140
And then of course
we'll see what's handed.

02:26:14.140 --> 02:26:15.440
All right.

02:26:15.440 --> 02:26:16.740
So this is where
we're going to end up.

02:26:16.740 --> 02:26:21.320
We're going to create a list of
the top adjectives in a volume

02:26:21.320 --> 02:26:24.470
and then-- or a directory of
volumes, and then we're going

02:26:24.470 --> 02:26:28.770
to look at the word count
by page in a volume.

02:26:28.770 --> 02:26:30.970
All right.

02:26:30.970 --> 02:26:35.900
So, what I want us to do
is look at these three--

02:26:35.900 --> 02:26:41.050
No, if two, two broad areas,
key approaches to text analysis

02:26:41.050 --> 02:26:44.680
and then you're going to do a
little activity in small groups.

02:26:44.680 --> 02:26:45.980
All right.

02:26:45.980 --> 02:26:47.580
So, sometimes when you're
thinking about text analysis,

02:26:47.580 --> 02:26:49.980
you hear people talking about
natural language processing.

02:26:49.980 --> 02:26:52.220
Is that a term that you
all are familiar with?

02:26:52.220 --> 02:26:55.680
OK. So, natural language
processing is using computers

02:26:55.680 --> 02:26:58.170
to understand the meaning,
relationships and semantics

02:26:58.170 --> 02:27:00.390
within human-language text.

02:27:00.390 --> 02:27:02.080
So some specific
methods are things

02:27:02.080 --> 02:27:04.080
like named entity extraction.

02:27:04.080 --> 02:27:08.010
So what are the names of the
people, places, organizations,

02:27:08.010 --> 02:27:13.060
the dates that can be-- a
certain kind of entity extracted

02:27:13.060 --> 02:27:15.100
that appear in a text.

02:27:15.100 --> 02:27:18.780
Sentiment analysis is another
kind of an LP processing.

02:27:18.780 --> 02:27:21.360
So, what does this text--

02:27:21.360 --> 02:27:23.340
what is the mood or the
feeling of this text?

02:27:23.340 --> 02:27:25.460
Is it positive, is it negative?

02:27:25.460 --> 02:27:27.580
And then there's
also a stylometry.

02:27:27.580 --> 02:27:30.930
So, what can we learn from
measuring features of style

02:27:30.930 --> 02:27:33.560
in this text that tell
us something about work.

02:27:33.560 --> 02:27:36.170
And then in this other
bucket, we can think

02:27:36.170 --> 02:27:37.470
about machine learning

02:27:37.470 --> 02:27:39.540
as another approach
to text analysis.

02:27:39.540 --> 02:27:42.940
So machine learning is training
computers to recognize patterns.

02:27:42.940 --> 02:27:46.170
So remember text analysis,
relevant patterns, right?

02:27:46.170 --> 02:27:49.790
So two specific methods that
we mention are topic modeling

02:27:49.790 --> 02:27:51.940
and naive Bayes classification.

02:27:51.940 --> 02:27:56.460
So topic modeling is what
we just looked at obviously.

02:27:56.460 --> 02:27:59.830
And topic modeling doesn't
take much human input.

02:27:59.830 --> 02:28:01.460
It's unsupervised.

02:28:01.460 --> 02:28:03.630
So you throw some
data at the computer,

02:28:03.630 --> 02:28:06.450
you set some parameters
and you get something out

02:28:06.450 --> 02:28:08.980
and then you think
about your results.

02:28:08.980 --> 02:28:12.970
But that training piece, the
computer is doing it on its own

02:28:12.970 --> 02:28:16.300
without much information
from you, guidance from you

02:28:16.300 --> 02:28:18.780
about what to do
or how to do it.

02:28:18.780 --> 02:28:20.970
I mean, it's just
running a process.

02:28:20.970 --> 02:28:22.270
That's specified.

02:28:22.270 --> 02:28:25.680
We can contrast that with
naive Bayes classification

02:28:25.680 --> 02:28:27.960
which is supervised
machine learning.

02:28:27.960 --> 02:28:29.770
So in supervised
machine learning,

02:28:29.770 --> 02:28:33.240
the human tells the
computer some information.

02:28:33.240 --> 02:28:35.830
It keys it up, it precedes
it with some information

02:28:35.830 --> 02:28:37.940
and then it asked to learn based

02:28:37.940 --> 02:28:40.000
on the information
that it was provided.

02:28:40.000 --> 02:28:41.930
So that's supervised
machine learning.

02:28:41.930 --> 02:28:44.200
And naive Bayes classification
is an example

02:28:44.200 --> 02:28:46.090
of supervised machine learning.

02:28:46.090 --> 02:28:48.940
So, a kind of research
question you might ask are

02:28:48.940 --> 02:28:50.270
which of the categories

02:28:50.270 --> 02:28:53.860
that I have named does
this text belong to?

02:28:53.860 --> 02:28:57.730
So you could say, hey, computer,
here are some fiction written

02:28:57.730 --> 02:29:00.740
by women in Britain
in the 1900s.

02:29:00.740 --> 02:29:03.200
And here are some
fiction written by women

02:29:03.200 --> 02:29:05.440
in the US in the 2000s.

02:29:05.440 --> 02:29:10.470
Here is a blob of text,
a whole mass of text,

02:29:10.470 --> 02:29:13.260
you go through them and tell
me if it was written by a woman

02:29:13.260 --> 02:29:19.450
in 19th-- yeah, 1900s,
in 19th century Britain.

02:29:19.450 --> 02:29:21.420
I'm just messing
with my examples now.

02:29:21.420 --> 02:29:24.370
Or in the 2000s in the US,
that was our two groups, right?

02:29:24.370 --> 02:29:26.900
So you tell me based on
what I told you was true

02:29:26.900 --> 02:29:31.580
for this smaller subset what
is true about the larger group.

02:29:31.580 --> 02:29:33.690
So that's naive Bayes
classification.

02:29:33.690 --> 02:29:35.350
All right.

02:29:35.350 --> 02:29:37.600
So you're going to look
back at those examples

02:29:37.600 --> 02:29:40.370
that we read way
back this morning,

02:29:40.370 --> 02:29:43.170
are those research examples
that you read through.

02:29:43.170 --> 02:29:46.290
And in pairs or by
yourself if you want to,

02:29:46.290 --> 02:29:49.500
but we prefer you work in
small groups if you don't mind.

02:29:49.500 --> 02:29:51.690
Go through and identify
the broad area

02:29:51.690 --> 02:29:53.250
and then the specific method.

02:29:53.250 --> 02:29:57.200
And again these broad areas
are NLP and machine learning.

02:29:57.200 --> 02:30:01.600
And then the specific methods
are named entity extraction,

02:30:01.600 --> 02:30:04.830
sentiment analysis and
stylometry and topic modeling

02:30:04.830 --> 02:30:06.130
and naive Bayes classification.

02:30:06.130 --> 02:30:08.780
And if you need to
go back to look

02:30:08.780 --> 02:30:11.090
at the research examples
again to remember,

02:30:11.090 --> 02:30:14.040
there's a link here
that's up on the screen.

02:30:14.040 --> 02:30:22.060
What's the broad area for the
Rowling and Galbraith example?

02:30:22.060 --> 02:30:26.180
&gt;&gt; Stylometry, because I just
found it in Scientific American.

02:30:26.180 --> 02:30:27.480
&gt;&gt; Eleanor Dickson: OK.

02:30:27.480 --> 02:30:28.780
So that's our specific
method, that's great.

02:30:28.780 --> 02:30:30.080
And what's the broad area?

02:30:30.080 --> 02:30:31.380
&gt;&gt; Natural language processing.

02:30:31.380 --> 02:30:32.680
&gt;&gt; Eleanor Dickson:
Natural language processing.

02:30:32.680 --> 02:30:36.710
So to do NLP, we need the
words in order on the page

02:30:36.710 --> 02:30:40.600
and we often leave in
for stylometry at least--

02:30:40.600 --> 02:30:42.820
We don't take these [inaudible]
words out, they're kept in.

02:30:42.820 --> 02:30:45.570
Because those tell us something
about the writing style

02:30:45.570 --> 02:30:49.440
which can help us figure
out who wrote something.

02:30:49.440 --> 02:30:51.660
All right, what about
significant themes

02:30:51.660 --> 02:30:53.070
in 19th century literature?

02:30:53.070 --> 02:30:54.890
What's the broad area?

02:30:54.890 --> 02:30:56.190
&gt;&gt; Machine learning.

02:30:56.190 --> 02:30:57.490
&gt;&gt; Eleanor Dickson:
Machine learning, good.

02:30:57.490 --> 02:30:58.790
And a specific method?

02:30:58.790 --> 02:31:00.090
[Inaudible] Great.

02:31:00.090 --> 02:31:01.390
All right.

02:31:01.390 --> 02:31:02.690
And then the emergence
of literary diction,

02:31:02.690 --> 02:31:03.990
what's the broad area?

02:31:03.990 --> 02:31:05.290
&gt;&gt; Machine learning.

02:31:05.290 --> 02:31:06.590
&gt;&gt; Eleanor Dickson: Good.

02:31:06.590 --> 02:31:07.890
And the specific method?

02:31:07.890 --> 02:31:09.190
&gt;&gt; Naive Bayes.

02:31:09.190 --> 02:31:10.490
&gt;&gt; Eleanor Dickson: Yeah, naive
Bayes classification, yeah.

02:31:10.490 --> 02:31:12.840
So they said here are some
fiction, here are some poetry,

02:31:12.840 --> 02:31:15.320
here are some prose,
now you tell us

02:31:15.320 --> 02:31:18.080
which category the
text belongs to.

02:31:18.080 --> 02:31:22.900
And then they were able to
chart out this change over time.

02:31:22.900 --> 02:31:24.770
All right.

02:31:24.770 --> 02:31:28.190
So, throughout the
day we've come back

02:31:28.190 --> 02:31:30.020
to a few different
ways of looking

02:31:30.020 --> 02:31:32.530
at a text analysis workflow.

02:31:32.530 --> 02:31:34.510
So, this is another
way that you can think

02:31:34.510 --> 02:31:38.400
about how the research
process progresses just sort

02:31:38.400 --> 02:31:40.250
of flipped in a new way.

02:31:40.250 --> 02:31:42.330
So you can think about the
workflows going like this.

02:31:42.330 --> 02:31:45.790
You have raw text and then
you do some preparation,

02:31:45.790 --> 02:31:49.800
so you deduplicate, you get
rid of some of the pages,

02:31:49.800 --> 02:31:53.030
you get your corpus ready to go.

02:31:53.030 --> 02:31:55.150
You do the normalization,
the cleaning.

02:31:55.150 --> 02:31:58.440
And then we translate
the text into features

02:31:58.440 --> 02:32:00.120
and that's where we are now.

02:32:00.120 --> 02:32:01.650
So remember we're
getting the text

02:32:01.650 --> 02:32:03.510
into something that
can be counted.

02:32:03.510 --> 02:32:04.900
It's all about counting.

02:32:04.900 --> 02:32:07.060
So we need something
the computer can count.

02:32:07.060 --> 02:32:08.800
So those are the features
and we're going to talk

02:32:08.800 --> 02:32:11.070
about a dataset that's
all features and what

02:32:11.070 --> 02:32:12.370
that means in a second.

02:32:12.370 --> 02:32:15.430
And then you can plug that
in for algorithmic use.

02:32:16.760 --> 02:32:21.350
So, within HTRC, we can think
about features as they relate

02:32:21.350 --> 02:32:24.190
to a dataset that's called the
extracted features dataset.

02:32:24.190 --> 02:32:26.550
And Harriet mentioned this
really briefly early on.

02:32:26.550 --> 02:32:28.100
We were talking about
Sam like, oh hey,

02:32:28.100 --> 02:32:30.140
Sam downloaded extracted
features.

02:32:30.140 --> 02:32:32.300
So now we're circling
back around to that.

02:32:32.300 --> 02:32:35.390
So the extracted features
dataset is a downloadable

02:32:35.390 --> 02:32:39.800
dataset of 15.7 million
volumes from HathiTrust.

02:32:39.800 --> 02:32:42.440
So including both the
public domain and the--

02:32:42.440 --> 02:32:45.360
in copyright data and they
are meant for the researcher

02:32:45.360 --> 02:32:48.930
who wants to work on their
own machine or outside

02:32:48.930 --> 02:32:53.280
of our systems, who
is interested

02:32:53.280 --> 02:32:56.900
in having a little bit of
the processing of the text

02:32:56.900 --> 02:32:59.830
on their behalf, and I'll say
more about that in a second.

02:32:59.830 --> 02:33:04.070
And who is also able and willing
to work with bags of words.

02:33:04.070 --> 02:33:05.870
So you can't do NLP with these

02:33:05.870 --> 02:33:08.870
because the word order
is not there anymore,

02:33:08.870 --> 02:33:11.470
but you can do any analysis
that requires bag of words.

02:33:11.470 --> 02:33:12.890
So this slide is out of date.

02:33:12.890 --> 02:33:14.430
It should be 15.7.

02:33:14.430 --> 02:33:19.540
And I forget what the billion
of page this is but it's a lot.

02:33:19.540 --> 02:33:21.980
All right, so sometimes you
call EF, extracted features.

02:33:21.980 --> 02:33:24.620
So these features are
selected data and metadata

02:33:24.620 --> 02:33:27.950
from every page in every volume

02:33:27.950 --> 02:33:32.620
within that 15.7 million
volume snapshot of the corpus.

02:33:32.620 --> 02:33:34.430
And they're extracted
from the raw text

02:33:34.430 --> 02:33:36.190
and the metadata
for those volumes.

02:33:36.190 --> 02:33:38.060
And they're meant to
position the researcher

02:33:38.060 --> 02:33:39.510
to begin their analysis.

02:33:39.510 --> 02:33:42.990
So, a lot of text
analysis starts or proceeds

02:33:42.990 --> 02:33:45.100
with this translation
into features.

02:33:45.100 --> 02:33:49.930
And so, by providing the text
as features, HTRC is one,

02:33:49.930 --> 02:33:54.080
poising the researcher to
begin their study and also two,

02:33:54.080 --> 02:33:55.890
making it so that the release

02:33:55.890 --> 02:33:59.920
of this massive dataset
is non-consumptive.

02:33:59.920 --> 02:34:02.820
So the text is not recreatable.

02:34:02.820 --> 02:34:07.160
So that's one solution
for providing access,

02:34:07.160 --> 02:34:10.670
a derived dataset
in copyright text.

02:34:10.670 --> 02:34:11.970
All right.

02:34:11.970 --> 02:34:14.280
So, do you know what
the features look like?

02:34:14.280 --> 02:34:17.370
So, there is-- They're in JSON
format which you saw earlier

02:34:17.370 --> 02:34:21.420
when we used the bibliographic
API, what JSON looks like.

02:34:21.420 --> 02:34:23.730
How many of you are
familiar with JSON?

02:34:23.730 --> 02:34:27.130
Great. So if you can read
XML, you can read JSON.

02:34:27.130 --> 02:34:29.900
JSON, the structure,
it's nested like XML is.

02:34:29.900 --> 02:34:31.210
It's just less verbose.

02:34:31.210 --> 02:34:33.740
There's no opening
and closing tags.

02:34:33.740 --> 02:34:37.190
You just see the blue
text before the colon.

02:34:37.190 --> 02:34:39.710
It's-- I think it's
nicer to read.

02:34:39.710 --> 02:34:41.640
OK, so it's JSON format

02:34:41.640 --> 02:34:45.380
and there is one
JSON file per volume

02:34:45.380 --> 02:34:48.730
in that 15.7 million
volume snapshot.

02:34:48.730 --> 02:34:51.690
And then within every
one of those JSON files,

02:34:51.690 --> 02:34:53.570
there's a hierarchy
in a structure

02:34:53.570 --> 02:34:55.240
and that's what we're
running through now.

02:34:55.240 --> 02:34:59.940
So at the top of the file,
there are per volume features.

02:34:59.940 --> 02:35:03.590
So, bibliographic
metadata about the volume.

02:35:03.590 --> 02:35:05.870
And then there is information

02:35:05.870 --> 02:35:09.400
like what language
is this volume

02:35:09.400 --> 02:35:11.590
that the cataloger
said that it was.

02:35:11.590 --> 02:35:16.420
Some identifiers, the title,
things like that, some dates.

02:35:16.420 --> 02:35:20.680
And then nested under that,
for every page in the volume,

02:35:20.680 --> 02:35:23.060
there is more metadata.

02:35:23.060 --> 02:35:25.600
So there are per page features.

02:35:25.600 --> 02:35:27.530
So there's the page sequence.

02:35:27.530 --> 02:35:33.300
So this doesn't correspond to
the page number at the bottom

02:35:33.300 --> 02:35:36.860
of the page but it
corresponds to the scan order.

02:35:36.860 --> 02:35:39.650
So if they scanned all of
the, you know, opening pages

02:35:39.650 --> 02:35:44.530
which they often do, then
it would start with 000001,

02:35:44.530 --> 02:35:45.830
and that might be the cover,

02:35:45.830 --> 02:35:47.540
it might be the inside
page, et cetera.

02:35:47.540 --> 02:35:50.450
OK, so the sequence, and then
there's computationally-inferred

02:35:50.450 --> 02:35:51.750
metadata.

02:35:51.750 --> 02:35:53.050
So this isn't the
bibliographic metadata.

02:35:53.050 --> 02:35:55.340
This is metadata that was
inferred in the process

02:35:55.340 --> 02:35:57.270
of creating these
features files.

02:35:57.270 --> 02:35:59.840
So there are things
like how many words,

02:35:59.840 --> 02:36:03.100
lines and sentences
appeared on that page

02:36:03.100 --> 02:36:05.770
and how many of them were empty.

02:36:05.770 --> 02:36:09.340
So then you could say like, huh,
look, here are all of the places

02:36:09.340 --> 02:36:12.940
in this file where the
page is mostly empty.

02:36:12.940 --> 02:36:15.130
Maybe it has an image on
it, maybe it's the beginning

02:36:15.130 --> 02:36:16.440
or the end of a chapter.

02:36:16.440 --> 02:36:17.890
And you could start
to use these features

02:36:17.890 --> 02:36:19.930
to understand the
contours of your data.

02:36:19.930 --> 02:36:24.420
And then there's also
computationally-inferred meta--

02:36:24.420 --> 02:36:27.340
language, sorry,
at the page level.

02:36:27.340 --> 02:36:30.200
So the computer will say this
page is in English, this page is

02:36:30.200 --> 02:36:33.160
in French which is more
nuances than you might get

02:36:33.160 --> 02:36:36.410
from just a larger
catalog record.

02:36:36.410 --> 02:36:41.530
OK. So then we are like moving
our way down the hierarchy.

02:36:41.530 --> 02:36:45.060
And so then within each page
section, we break the page

02:36:45.060 --> 02:36:48.570
up into three pieces,
header, body and footer.

02:36:48.570 --> 02:36:51.440
So you can see the way that
we have this highlighted here

02:36:51.440 --> 02:36:53.620
yellow across the
top for the header

02:36:53.620 --> 02:36:55.740
and then the body
is outlined in red.

02:36:55.740 --> 02:36:58.870
And then you'll see
some of these things

02:36:58.870 --> 02:37:00.790
that might get picked up.

02:37:00.790 --> 02:37:04.610
OK. So, for each of these
pieces, for each page

02:37:04.610 --> 02:37:07.870
in the volume, the header,
body and footer, there is line,

02:37:07.870 --> 02:37:09.490
empty line, and sentence count.

02:37:09.490 --> 02:37:11.060
So you have it for the whole
page and then you have it

02:37:11.060 --> 02:37:13.870
for the section, then you
have the counts of beginning

02:37:13.870 --> 02:37:17.320
and end-line characters and
then you have token counts.

02:37:17.320 --> 02:37:20.040
And what are tokens?

02:37:20.040 --> 02:37:21.340
&gt;&gt; Words.

02:37:21.340 --> 02:37:22.640
&gt;&gt; Words, OK.

02:37:22.640 --> 02:37:23.940
So then we have words
and word counts.

02:37:23.940 --> 02:37:27.940
And so, you'll see here that we
have things, OK, token count,

02:37:27.940 --> 02:37:30.480
line count, empty line count.

02:37:30.480 --> 02:37:31.780
We're moving down.

02:37:31.780 --> 02:37:35.710
And then we have all of
the tokens that start.

02:37:35.710 --> 02:37:39.380
And you'll see that they're
tagged with a part of speech.

02:37:39.380 --> 02:37:42.580
This is so that the part of
speech information isn't lost

02:37:42.580 --> 02:37:44.630
when the bag of words
is created.

02:37:44.630 --> 02:37:47.190
So we're keeping some
of that meaning there.

02:37:47.190 --> 02:37:52.060
And you'll see that they
are marked with that code,

02:37:52.060 --> 02:37:54.570
that two-letter code which
is from the Penn Tree Bank.

02:37:54.570 --> 02:37:56.810
Are any of you familiar
with the Penn Tree Bank?

02:37:56.810 --> 02:38:01.790
OK. So, this is a standard
linguistics coding system

02:38:01.790 --> 02:38:04.390
for languages, or I'm
sorry for parts of speech.

02:38:04.390 --> 02:38:07.950
So JJ is adjective,
NNP is proper noun.

02:38:07.950 --> 02:38:09.860
And so this provides
some additional context

02:38:09.860 --> 02:38:11.160
about the information.

02:38:11.160 --> 02:38:12.970
But look, we have all of
the words that appeared

02:38:12.970 --> 02:38:16.240
on this page, just not the
order in which they appeared.

02:38:16.240 --> 02:38:19.390
OK. So you could identify
the parts of the book,

02:38:19.390 --> 02:38:23.820
you could do topic modeling,
you could classify the volume,

02:38:23.820 --> 02:38:25.330
sort of like that example we saw

02:38:25.330 --> 02:38:27.110
where they were saying
this is fiction,

02:38:27.110 --> 02:38:28.560
this is nonfiction prose.

02:38:28.560 --> 02:38:31.820
All right, any questions
about that?

02:38:31.820 --> 02:38:33.120
We're sort of like [inaudible].

02:38:33.120 --> 02:38:34.420
We're going to come
back to it in a second.

02:38:34.420 --> 02:38:35.720
Yeah.

02:38:35.720 --> 02:38:37.850
&gt;&gt; Can you just go back just--

02:38:37.850 --> 02:38:41.900
You said that there
is one JSON file per--

02:38:41.900 --> 02:38:44.090
&gt;&gt; Eleanor Dickson: Volume.

02:38:44.090 --> 02:38:45.390
&gt;&gt; -- volume.

02:38:45.390 --> 02:38:46.690
&gt;&gt; Eleanor Dickson: Yes.

02:38:46.690 --> 02:38:47.990
&gt;&gt; OK.

02:38:47.990 --> 02:38:49.290
&gt;&gt; Eleanor Dickson: Yeah.

02:38:49.290 --> 02:38:50.590
&gt;&gt; All right.

02:38:50.590 --> 02:38:51.890
&gt;&gt; Eleanor Dickson: Yeah.

02:38:51.890 --> 02:38:53.190
So, they take the--
all of the plain text

02:38:53.190 --> 02:38:55.220
that constitutes a volume
and then they process it

02:38:55.220 --> 02:38:58.610
and they spit out a JSON file
that corresponds to the volume.

02:38:58.610 --> 02:39:00.210
And then within that JSON file,

02:39:00.210 --> 02:39:02.120
that file is broken
up into the sections.

02:39:02.120 --> 02:39:03.850
&gt;&gt; So you're-- with this
dataset you're working

02:39:03.850 --> 02:39:07.140
with 15.7 million files?

02:39:07.140 --> 02:39:08.440
&gt;&gt; Eleanor Dickson: Yes.

02:39:08.440 --> 02:39:09.740
But you don't have to
get all 15.7 million.

02:39:09.740 --> 02:39:11.040
&gt;&gt; Sure.

02:39:11.040 --> 02:39:12.340
&gt;&gt; Eleanor Dickson: Yeah.

02:39:12.340 --> 02:39:13.640
Yeah.

02:39:13.640 --> 02:39:14.940
&gt;&gt; All right.

02:39:14.940 --> 02:39:16.240
&gt;&gt; Eleanor Dickson: I
say that as a warning

02:39:16.240 --> 02:39:17.540
because if you did
it would like, yeah,

02:39:17.540 --> 02:39:18.840
you could be the-- Yeah.

02:39:18.840 --> 02:39:20.140
&gt;&gt; Yeah.

02:39:20.140 --> 02:39:21.440
&gt;&gt; Eleanor Dickson: OK.

02:39:21.440 --> 02:39:22.740
&gt;&gt; So the number after the--

02:39:22.740 --> 02:39:24.040
of each tag, is that
the difference?

02:39:24.040 --> 02:39:25.340
&gt;&gt; Eleanor Dickson: Yeah, it's
the number of times it occurred

02:39:25.340 --> 02:39:26.640
in that section of the page.

02:39:26.640 --> 02:39:27.940
&gt;&gt; That have style [inaudible].

02:39:27.940 --> 02:39:29.240
&gt;&gt; Eleanor Dickson: Yeah, yeah.

02:39:29.240 --> 02:39:30.540
And we had a question earlier.

02:39:30.540 --> 02:39:31.840
I can't remember who asked it.

02:39:31.840 --> 02:39:33.140
I think that you
did in the front

02:39:33.140 --> 02:39:34.440
about stripping away the
headers and the footers and what

02:39:34.440 --> 02:39:35.740
if you didn't actually
want it gone?

02:39:35.740 --> 02:39:37.040
So it was you, right?

02:39:37.040 --> 02:39:39.120
So in this dataset, the idea is,
well, we prepped the researcher

02:39:39.120 --> 02:39:41.690
so that if they want to
take it out, they can.

02:39:41.690 --> 02:39:42.990
But if, for some reason,

02:39:42.990 --> 02:39:44.950
you think that your data is
less likely to have a header

02:39:44.950 --> 02:39:47.420
and a footer line and you want
to keep it in, or you think

02:39:47.420 --> 02:39:49.490
that it's important for your
work for whatever reason,

02:39:49.490 --> 02:39:50.840
we haven't taken it all away.

02:39:50.840 --> 02:39:54.320
It's still there so you could
knit the pieces back together.

02:39:54.320 --> 02:39:55.620
Yeah, go ahead.

02:39:55.620 --> 02:39:56.920
&gt;&gt; So I'm still [inaudible]

02:39:56.920 --> 02:39:58.220
to understand what's
in [inaudible].

02:39:58.220 --> 02:39:59.520
So it's one JSON file--

02:39:59.520 --> 02:40:00.820
&gt;&gt; Eleanor Dickson: Yeah.

02:40:00.820 --> 02:40:02.120
&gt;&gt; -- and it has information
about the whole book at the top.

02:40:02.120 --> 02:40:03.420
&gt;&gt; Eleanor Dickson: Yes.

02:40:03.420 --> 02:40:04.720
Yeah.

02:40:04.720 --> 02:40:06.020
&gt;&gt; And then page by page.

02:40:06.020 --> 02:40:07.320
&gt;&gt; Eleanor Dickson: Yeah.

02:40:07.320 --> 02:40:08.620
It says here's page one, here's
some information about it.

02:40:08.620 --> 02:40:10.450
And page one, the
header has this,

02:40:10.450 --> 02:40:12.090
the body has this,
the footer has this.

02:40:12.090 --> 02:40:13.390
&gt;&gt; And all these words--

02:40:13.390 --> 02:40:14.690
&gt;&gt; Eleanor Dickson: Yes.

02:40:14.690 --> 02:40:15.990
&gt;&gt; -- are there?

02:40:15.990 --> 02:40:17.290
&gt;&gt; Eleanor Dickson: Yup.

02:40:17.290 --> 02:40:18.590
And here's page two and it
has a header, body, footer.

02:40:18.590 --> 02:40:20.040
&gt;&gt; It doesn't have
the [inaudible] text.

02:40:20.040 --> 02:40:22.250
&gt;&gt; Eleanor Dickson: Yeah, it
has the words and the number

02:40:22.250 --> 02:40:25.720
of times that they occurred,
part of speech tagged, yeah.

02:40:25.720 --> 02:40:27.230
Any other questions?

02:40:27.230 --> 02:40:33.440
OK. So we're going to sort of--
we're doing like a zigzag here.

02:40:33.440 --> 02:40:35.240
So we're going to
talk a little bit more

02:40:35.240 --> 02:40:37.350
about do-it-yourself
text analysis.

02:40:37.350 --> 02:40:42.330
So some researchers will not
want to use off-the-shelf tools.

02:40:42.330 --> 02:40:43.630
They are just not
going to want to do it.

02:40:43.630 --> 02:40:45.140
They're great for
getting their feet wet

02:40:45.140 --> 02:40:47.720
but they are just not
going to be satisfied

02:40:47.720 --> 02:40:49.020
and they're going to want more.

02:40:49.020 --> 02:40:50.360
And those are the
just give me the data

02:40:50.360 --> 02:40:52.560
and let me do my
work kinds of people.

02:40:52.560 --> 02:40:55.280
And we are going to talk about
what kinds of tools those kinds

02:40:55.280 --> 02:40:57.450
of researchers use
for their analysis.

02:40:57.450 --> 02:40:59.850
So that kind of researcher
wants more control

02:40:59.850 --> 02:41:02.170
over the processes
of their analysis.

02:41:02.170 --> 02:41:04.900
They want to know the intimate
details of what's happening

02:41:04.900 --> 02:41:08.490
and they really want to
be able to parameterize

02:41:08.490 --> 02:41:11.770
and set the specifications for
the work that they're doing.

02:41:11.770 --> 02:41:14.470
And so this do-it-yourself
approach is often a

02:41:14.470 --> 02:41:19.230
mix-and-match sort of
combination of various tools

02:41:19.230 --> 02:41:22.100
like programming languages
you can think of as a tool

02:41:22.100 --> 02:41:24.180
that a researcher is
going to put together

02:41:24.180 --> 02:41:26.190
and they might use one tool
at one point in the research

02:41:26.190 --> 02:41:27.810
and another tool
at another time.

02:41:27.810 --> 02:41:31.550
All right, so this toolkit
is researcher-dependent.

02:41:31.550 --> 02:41:34.790
There are wars fought
over Python versus R

02:41:34.790 --> 02:41:36.660
and people have strong
feelings about it.

02:41:36.660 --> 02:41:38.840
And so it's really-- it's about
what the researcher wants.

02:41:38.840 --> 02:41:40.660
There's no right
tool necessarily,

02:41:40.660 --> 02:41:42.640
it's really researcher-driven.

02:41:42.640 --> 02:41:45.250
And generally, to be able
to do this kind of work,

02:41:45.250 --> 02:41:47.560
you need some understanding
of statistics

02:41:47.560 --> 02:41:49.200
because remember
it's all counting.

02:41:49.200 --> 02:41:52.480
So you need to understand
what your results are.

02:41:52.480 --> 02:41:55.580
And so this leads me to the
point that it often draws

02:41:55.580 --> 02:41:57.230
on expert collaborators.

02:41:57.230 --> 02:42:00.400
So we sometimes see
in digital humanities

02:42:00.400 --> 02:42:03.860
and text analysis research teams
where you have people pairing

02:42:03.860 --> 02:42:06.040
up in collaborations
or partnerships

02:42:06.040 --> 02:42:09.670
where someone is crunching
data or analyzing data

02:42:09.670 --> 02:42:11.140
in combination with
somebody else.

02:42:11.140 --> 02:42:13.960
And the toolkit consists
of command line tools

02:42:13.960 --> 02:42:15.710
and programming languages.

02:42:15.710 --> 02:42:18.530
All right, so what
are those tools?

02:42:18.530 --> 02:42:22.650
So some general command-line
tools that are commonly used--

02:42:22.650 --> 02:42:26.600
and by command-line tool,
I mean, you install it

02:42:26.600 --> 02:42:29.500
on your computer and you
run it from the command-line

02:42:29.500 --> 02:42:32.000
but you're not doing
computer programming.

02:42:32.000 --> 02:42:34.920
So MALLET is one tool
we talked about earlier.

02:42:34.920 --> 02:42:36.760
That's a command-line tool.

02:42:36.760 --> 02:42:40.830
And there are set sort of
instructions that you follow

02:42:40.830 --> 02:42:43.490
that are MALLET and you run
it from the command-line

02:42:43.490 --> 02:42:46.160
so you say like, MALLET
do this, MALLET do that

02:42:46.160 --> 02:42:48.650
and you feed data in,
you set parameters

02:42:48.650 --> 02:42:51.130
and you get an output.

02:42:51.130 --> 02:42:52.710
And MALLET is good
for topic modeling

02:42:52.710 --> 02:42:55.510
and it can also do some
kinds of classification.

02:42:55.510 --> 02:42:57.290
Some researchers used this--

02:42:57.290 --> 02:42:59.330
they use Stanford
suite of NLP tools

02:42:59.330 --> 02:43:01.800
for natural language processing.

02:43:01.800 --> 02:43:05.250
MALLET and Stanford's NLP are
both Java-based and I think both

02:43:05.250 --> 02:43:09.620
of them have a graphical-user
interface that you could use

02:43:09.620 --> 02:43:11.570
if you didn't want to use
it on the command-line.

02:43:11.570 --> 02:43:12.870
But oftentimes these sorts

02:43:12.870 --> 02:43:17.600
of researchers are using
them on the command-line.

02:43:17.600 --> 02:43:19.790
OK, and then there
are code libraries.

02:43:19.790 --> 02:43:21.630
So someone asked earlier
about code libraries.

02:43:21.630 --> 02:43:24.800
So this is the time when
we're talking about that.

02:43:24.800 --> 02:43:26.680
All right, so part

02:43:26.680 --> 02:43:29.540
of programming is
re-using code, all right.

02:43:29.540 --> 02:43:34.000
So no one is starting from
zero when they're programming.

02:43:34.000 --> 02:43:37.050
You've got a computer, it
has an operating system

02:43:37.050 --> 02:43:38.350
like something has been done

02:43:38.350 --> 02:43:40.380
for you generally before
you get started, right?

02:43:40.380 --> 02:43:45.300
So just think about the library
as a crutch, a support tool, a--

02:43:45.300 --> 02:43:49.790
an implement that allows
you to perform certain tasks

02:43:49.790 --> 02:43:53.460
that somebody else wrote and
described and you just sort

02:43:53.460 --> 02:43:58.130
of like slide it in
to your work, OK?

02:43:58.130 --> 02:44:01.780
So often you share
these like chunks

02:44:01.780 --> 02:44:03.930
of existing code as a library.

02:44:03.930 --> 02:44:07.990
So you'll say, oh, I'm
going to use this library.

02:44:07.990 --> 02:44:10.020
You install it via a package.

02:44:10.020 --> 02:44:11.420
And I mentioned this
because we're going

02:44:11.420 --> 02:44:13.370
to use a package manager.

02:44:13.370 --> 02:44:15.720
We're installing the
library via package.

02:44:15.720 --> 02:44:19.170
And then the packages are
collections of bits of code

02:44:19.170 --> 02:44:21.290
that are made up into modules.

02:44:21.290 --> 02:44:23.060
And this is all-- I will
show you how this looks

02:44:23.060 --> 02:44:24.360
like in a second.

02:44:24.360 --> 02:44:25.660
And then these modules
facilitate the

02:44:25.660 --> 02:44:26.960
programming tasks.

02:44:26.960 --> 02:44:28.260
So I'm a visual person.

02:44:28.260 --> 02:44:29.560
You can think about
it a little bit

02:44:29.560 --> 02:44:31.700
like a screwdriver kit, right?

02:44:31.700 --> 02:44:34.190
So here is my library
which is the big kit

02:44:34.190 --> 02:44:37.600
and then I have a head
for the screw driver

02:44:37.600 --> 02:44:39.200
that does different tasks.

02:44:39.200 --> 02:44:44.020
Here is my one that's
going to, oh god, open,

02:44:44.020 --> 02:44:46.290
flat-head screw, right?

02:44:46.290 --> 02:44:47.790
So, my flat-head
screwdriver, right?

02:44:47.790 --> 02:44:53.560
OK, well that's like
tokenizing the text, right?

02:44:53.560 --> 02:44:55.010
So they correspond to a task.

02:44:55.010 --> 02:44:57.140
So they all-- they
have-- each play a part.

02:44:57.140 --> 02:45:00.410
All right, so I like this tweet.

02:45:00.410 --> 02:45:02.980
We don't have the notes up
here so I'm unfortunately going

02:45:02.980 --> 02:45:05.860
to remember this person's
like non-Twitter name.

02:45:05.860 --> 02:45:09.530
But they said, "Me on Python,
spend two hours working at how

02:45:09.530 --> 02:45:12.710
to do X, discover a library
design to do X better.

02:45:12.710 --> 02:45:14.300
Spend three hours
wading into the library.

02:45:14.300 --> 02:45:16.700
Repeat." And this is
sort of a common workflow

02:45:16.700 --> 02:45:18.110
for the beginner
programmer where you think,

02:45:18.110 --> 02:45:19.570
I'm going to do this
thing and I'm going

02:45:19.570 --> 02:45:20.870
to make it all by myself.

02:45:20.870 --> 02:45:22.950
And then you're like, oh,
somebody already did that,

02:45:22.950 --> 02:45:25.520
and there's a tool, there's a
library I can use and it's like,

02:45:25.520 --> 02:45:27.770
oh, crap, I got to figure
out how to use the library.

02:45:27.770 --> 02:45:30.410
So this is sort of
a common cycle.

02:45:30.410 --> 02:45:33.780
OK. So to install a
package for Python,

02:45:33.780 --> 02:45:35.390
you use a package manager.

02:45:35.390 --> 02:45:38.220
And we're going to use one
today that's called pip

02:45:38.220 --> 02:45:42.820
and that comes preinstall-- like
it's preinstalled with Python.

02:45:42.820 --> 02:45:45.070
So, if you're running
Python on your computer

02:45:45.070 --> 02:45:47.840
or you installed Python,
it's going to include pip,

02:45:47.840 --> 02:45:49.580
so you could pip install things.

02:45:49.580 --> 02:45:51.420
And the general syntax
to pip install,

02:45:51.420 --> 02:45:53.550
use a pip install, the thing.

02:45:53.550 --> 02:45:56.130
And then pip goes out and
gets the thing for you

02:45:56.130 --> 02:46:00.200
from wherever it lives say
on Github or another website.

02:46:00.200 --> 02:46:02.630
There are other installers
that people use,

02:46:02.630 --> 02:46:04.980
other package managers
like Homebrew.

02:46:04.980 --> 02:46:06.410
Another one is Conda.

02:46:06.410 --> 02:46:09.280
So if you are running
a machine right now

02:46:09.280 --> 02:46:11.450
where you haven't done a lot of
Python programming but you want

02:46:11.450 --> 02:46:13.950
to get started, I would
recommend downloading

02:46:13.950 --> 02:46:16.350
and installing Anaconda.

02:46:16.350 --> 02:46:20.910
And Anaconda, their package
manager is called Conda.

02:46:20.910 --> 02:46:22.210
But Anaconda is robust.

02:46:22.210 --> 02:46:24.540
It includes lots of libraries so
you don't have to install them.

02:46:24.540 --> 02:46:28.550
OK. All right, so if you want
to use a library's module

02:46:28.550 --> 02:46:30.210
and a script, remember
the modules are

02:46:30.210 --> 02:46:31.670
like the heads of
the screwdriver.

02:46:31.670 --> 02:46:36.810
You have to import-- Do
these import statements

02:46:36.810 --> 02:46:38.110
at the beginning.

02:46:38.110 --> 02:46:40.260
So when you just start
programming or wanting

02:46:40.260 --> 02:46:43.160
to reuse somebody
else's code, open it up

02:46:43.160 --> 02:46:45.630
and see what they
imported at the beginning.

02:46:45.630 --> 02:46:50.790
Because if you just like
download somebody's file and try

02:46:50.790 --> 02:46:53.810
to run it using your Python run
the script and then you're going

02:46:53.810 --> 02:46:56.420
to get an error like,
what's Pandas?

02:46:56.420 --> 02:46:59.070
I don't know, computer
doesn't know, right?

02:46:59.070 --> 02:47:01.010
So look at the import
statements at the beginning

02:47:01.010 --> 02:47:02.350
and see what libraries
they're using

02:47:02.350 --> 02:47:04.230
because you better have those
installed on your computer

02:47:04.230 --> 02:47:05.960
so that you can reuse them.

02:47:05.960 --> 02:47:07.780
So that's where you
go to look to see, OK,

02:47:07.780 --> 02:47:10.300
these are the dependencies
that I need to have installed

02:47:10.300 --> 02:47:12.980
and ready to go in order
to reuse the script.

02:47:12.980 --> 02:47:17.070
So this will get declared at the
top and then they get reused.

02:47:17.070 --> 02:47:20.640
So, I'm going to step away
from the mic, which is not good

02:47:20.640 --> 02:47:22.320
for the recording, but we
see import Pandas as pd.

02:47:22.320 --> 02:47:26.050
And then look, if
we look down here,

02:47:26.050 --> 02:47:28.460
we see pd is being
referenced again.

02:47:28.460 --> 02:47:30.370
That's because we're
using a little bit

02:47:30.370 --> 02:47:33.020
of the Pandas to the spit.

02:47:33.020 --> 02:47:34.320
Does that make sense?

02:47:34.320 --> 02:47:37.070
So you bring it in and then
you write it into your code

02:47:37.070 --> 02:47:40.230
and the Pandas help do
the process for you.

02:47:40.230 --> 02:47:44.030
OK. So, some common Python
text analysis libraries.

02:47:44.030 --> 02:47:47.550
SciKit learn is a really popular
one for machine learning.

02:47:47.550 --> 02:47:50.930
Pandas is a common one
for doing data science.

02:47:50.930 --> 02:47:52.400
We're going to-- The tool--

02:47:52.400 --> 02:47:55.790
The library we're going to use
relies partially on Pandas.

02:47:55.790 --> 02:47:58.190
NLTK, if you're like,
well, I would really

02:47:58.190 --> 02:48:00.690
like to learn Python in the
context of text analysis,

02:48:00.690 --> 02:48:02.300
I recommend starting with NLTK

02:48:02.300 --> 02:48:05.100
because they have the most
beautiful book that's online

02:48:05.100 --> 02:48:07.650
and they have activities and
you can sort of teach yourself

02:48:07.650 --> 02:48:10.070
in the context of NLTK.

02:48:10.070 --> 02:48:14.040
All right, so can you use
your imaginations and think

02:48:14.040 --> 02:48:19.930
about what nltk dot words
underscore tokenize would do

02:48:19.930 --> 02:48:23.500
for you as part of
the NLTK library?

02:48:25.160 --> 02:48:26.460
Yeah.

02:48:26.460 --> 02:48:27.760
&gt;&gt; It would [inaudible]
words in a day.

02:48:27.760 --> 02:48:29.060
&gt;&gt; Eleanor Dickson:
Yeah, we would--

02:48:29.060 --> 02:48:30.620
well, it would break
our text into tokens.

02:48:30.620 --> 02:48:31.920
So we're creating the tokens.

02:48:31.920 --> 02:48:33.780
Remember I said no
and is like doing--

02:48:33.780 --> 02:48:36.510
They're not like
human doing something.

02:48:36.510 --> 02:48:37.960
Here, we've made tokens now.

02:48:37.960 --> 02:48:41.060
We're getting ready for
the text to be processed.

02:48:41.060 --> 02:48:44.110
OK. All right, so the
library we're going

02:48:44.110 --> 02:48:46.990
to use today is the Feature
Reader Python library,

02:48:46.990 --> 02:48:49.230
and it was developed
by some of the folks

02:48:49.230 --> 02:48:51.280
who are affiliated with HTRC.

02:48:51.280 --> 02:48:53.990
And it's meant to interact with
the extractive feature file,

02:48:53.990 --> 02:48:58.170
so it makes processing and
analyzing those files easier.

02:48:58.170 --> 02:49:00.100
So it's meant to parse the JSON.

02:49:00.100 --> 02:49:02.040
Sometimes people
ask where it lives.

02:49:02.040 --> 02:49:03.340
I love this question.

02:49:03.340 --> 02:49:04.640
It lives on Github
and we're going

02:49:04.640 --> 02:49:07.160
to go get it from
Github with pip.

02:49:07.160 --> 02:49:11.490
And if you want to use it, you
have to have Pandas installed.

02:49:11.490 --> 02:49:14.600
And lucky for us Pandas
is installed already

02:49:14.600 --> 02:49:17.690
in Python anywhere, so we
don't have to do any of that.

02:49:17.690 --> 02:49:19.450
All right.

02:49:19.450 --> 02:49:24.800
So, oh no, let's go back to
our sample research question.

02:49:27.290 --> 02:49:30.800
And I'll show you this because
I think it can be helpful.

02:49:30.800 --> 02:49:32.860
So we are going to pull out--

02:49:32.860 --> 02:49:36.180
first we're going to
install the Feature Reader

02:49:36.180 --> 02:49:40.950
and then we're going to pull out
the top adjectives in our file

02:49:40.950 --> 02:49:44.330
and we're going to compare
two directories of files.

02:49:44.330 --> 02:49:47.240
One directory of extracted
features files relates

02:49:47.240 --> 02:49:48.720
to that work set that
you used earlier,

02:49:48.720 --> 02:49:51.730
so it's all of the same
volumes but they're rendered

02:49:51.730 --> 02:49:53.030
in extracted features.

02:49:53.030 --> 02:49:54.330
And then we're going to look

02:49:54.330 --> 02:49:57.970
at the presidential
papers from the 1930s.

02:49:57.970 --> 02:50:02.040
So we're just going to see
how we can compare the two.

02:50:02.040 --> 02:50:04.190
Is there anything else here?

02:50:04.190 --> 02:50:09.070
No. OK. So let's do
our pip installation.

02:50:09.070 --> 02:50:12.400
All right, so we need to--

02:50:12.400 --> 02:50:14.940
make sure I'm not going
too far off script.

02:50:14.940 --> 02:50:18.410
OK, so we want to pip install
the HTRC Feature Reader.

02:50:18.410 --> 02:50:23.050
We're on page 19 of
the handout, step 3.

02:50:24.220 --> 02:50:27.250
So when you're pip
installing in Python anywhere,

02:50:27.250 --> 02:50:34.010
you'll see that we have to
use this dash, dash user tag

02:50:34.010 --> 02:50:37.420
and that dash, dash user tells
Python anywhere that you want

02:50:37.420 --> 02:50:39.800
to install it in
your user account.

02:50:39.800 --> 02:50:41.670
Otherwise the pip
install doesn't work.

02:50:41.670 --> 02:50:42.970
You don't always
have to do that.

02:50:42.970 --> 02:50:44.600
This is sort of a
Python anywhere thing.

02:50:44.600 --> 02:50:47.270
OK. Or an option
in Python anywhere.

02:50:47.270 --> 02:50:51.330
OK, so you do pip
install dash, dash user

02:50:51.330 --> 02:50:56.760
and then you do HTRC
Feature Reader.

02:50:56.760 --> 02:51:00.010
And then through the magic
of the package installer,

02:51:00.010 --> 02:51:02.280
the package handler, this is
why I'm using one that's nice.

02:51:02.280 --> 02:51:05.380
It goes out, it finds the code
on Github and then it stalls.

02:51:05.380 --> 02:51:08.220
I have it already installed
so it says it's satisfied.

02:51:08.220 --> 02:51:09.520
It doesn't have to do anything.

02:51:09.520 --> 02:51:12.210
So did you all get it installed?

02:51:12.210 --> 02:51:13.510
It's thinking?

02:51:13.510 --> 02:51:15.040
[ Inaudible Discussion ]

02:51:15.040 --> 02:51:16.340
OK.

02:51:16.340 --> 02:51:19.090
&gt;&gt; So the final in
Github is the [inaudible]

02:51:19.090 --> 02:51:22.390
for anyone to [inaudible].

02:51:22.390 --> 02:51:26.940
It's not a user [inaudible]?

02:51:26.940 --> 02:51:28.240
&gt;&gt; Eleanor Dickson: So yeah.

02:51:28.240 --> 02:51:37.350
So let's-- I'll show you
what it looks like here.

02:51:37.350 --> 02:51:40.170
[Inaudible] So yeah, to open
and then if you want it,

02:51:40.170 --> 02:51:42.870
there's documentation
here and the date--

02:51:42.870 --> 02:51:49.560
the actual library is in--
which one would it be in?

02:51:49.560 --> 02:51:50.870
Scripts, I think.

02:51:50.870 --> 02:51:52.170
I don't know.

02:51:52.170 --> 02:51:53.470
Actually I'm not going
to say which one it's in

02:51:53.470 --> 02:51:54.770
but there's documentation here.

02:51:54.770 --> 02:51:56.070
It comes and it gets
it from this.

02:51:56.070 --> 02:51:58.510
And the library is--

02:51:58.510 --> 02:52:08.810
[ Inaudible ]

02:52:08.810 --> 02:52:11.310
I'm trying to figure out which
is a good link that I want.

02:52:11.310 --> 02:52:13.600
Yeah, OK. So the
library is things

02:52:13.600 --> 02:52:15.960
like defining functions for you.

02:52:15.960 --> 02:52:21.300
So it's like bits-- like
little bits of Python code

02:52:21.300 --> 02:52:24.920
that are prewritten and
yeah-- And so this--

02:52:24.920 --> 02:52:28.200
the files where this
lives on Github.

02:52:28.200 --> 02:52:31.500
And then the package manager
goes and gets it for you.

02:52:31.500 --> 02:52:32.800
[Inaudible] Yes, yeah.

02:52:32.800 --> 02:52:34.100
[Inaudible] Yes.

02:52:34.100 --> 02:52:37.730
Yeah, yeah, yeah.

02:52:37.730 --> 02:52:40.250
And it's just like a
slightly different interaction

02:52:40.250 --> 02:52:44.520
than if you were to-- I'm
really bad at Github, like fork

02:52:44.520 --> 02:52:46.120
or download something
from Github.

02:52:46.120 --> 02:52:49.390
It's just the package manager
handles the installation for you

02:52:49.390 --> 02:52:53.260
and it like tries to do nice
things like manage the versions

02:52:53.260 --> 02:52:54.850
of software that you're using.

02:52:54.850 --> 02:52:57.210
Yeah. All right.

02:52:57.210 --> 02:52:58.710
How's it going?

02:52:58.710 --> 02:53:00.110
Got it?

02:53:00.110 --> 02:53:01.410
&gt;&gt; Yes.

02:53:01.410 --> 02:53:02.710
&gt;&gt; Eleanor Dickson: All
right, easy enough, right?

02:53:02.710 --> 02:53:04.010
OK, so now we're going to do--

02:53:04.010 --> 02:53:05.590
We're going to look
at the top adjectives.

02:53:05.590 --> 02:53:07.940
So we're on step 5.

02:53:07.940 --> 02:53:10.510
So enter in your command to
look at the top adjectives.

02:53:10.510 --> 02:53:15.480
So remember we start with
Python, top adjectives,

02:53:15.480 --> 02:53:19.760
that's the name of the
script we're running, dot py.

02:53:19.760 --> 02:53:23.100
And then we have to give
it a directory name,

02:53:23.100 --> 02:53:28.830
because the directory name
is expected as an argument

02:53:28.830 --> 02:53:31.130
for the script to run.

02:53:37.430 --> 02:53:41.150
So you'll see we have a
variable called decade

02:53:41.150 --> 02:53:43.530
and it's asking for
one argument.

02:53:43.530 --> 02:53:46.040
And so that argument
is our directory.

02:53:46.040 --> 02:53:49.360
Assumes extracted features are
stored in directories labeled

02:53:49.360 --> 02:53:54.400
by decade and requires that
the directory name be supplied

02:53:54.400 --> 02:53:55.910
in running the script.

02:53:55.910 --> 02:53:57.890
So that's-- We have
that commented here.

02:53:57.890 --> 02:54:01.350
And then you'll see at the top
of the script we're importing.

02:54:01.350 --> 02:54:03.340
So we're importing
feature reader

02:54:03.340 --> 02:54:05.110
from the HTRC Feature Reader.

02:54:05.110 --> 02:54:08.360
We're importing this module--
yeah, module called glob,

02:54:08.360 --> 02:54:10.780
which is my favorite,
because you get to write.

02:54:10.780 --> 02:54:13.160
Wait for it.

02:54:14.370 --> 02:54:15.760
Oh man, I ruined the joke.

02:54:15.760 --> 02:54:20.030
Where did it go?

02:54:20.030 --> 02:54:21.330
Yeah, oh here we go.

02:54:21.330 --> 02:54:22.630
Yeah, you got it right.

02:54:22.630 --> 02:54:24.510
Glob, glob, which is the best
part of this thing, right?

02:54:24.510 --> 02:54:26.000
And that's basically
just setting

02:54:26.000 --> 02:54:27.780
up your directory path for you.

02:54:27.780 --> 02:54:30.170
And then, what else
do we have here?

02:54:30.170 --> 02:54:32.460
We have Pandas, remember
that's installed as pd.

02:54:32.460 --> 02:54:33.760
And then sys again,

02:54:33.760 --> 02:54:35.550
sys is what's doing this whole
thing about the argument.

02:54:35.550 --> 02:54:37.550
All right, so then if we run it,

02:54:37.550 --> 02:54:39.530
it's going to take a little
while because it's going

02:54:39.530 --> 02:54:43.940
through all of those files
that are pretty structured,

02:54:43.940 --> 02:54:46.410
the data is pretty
structured and it's pulling

02:54:46.410 --> 02:54:50.810
out every time it
sees the JJ code.

02:54:50.810 --> 02:54:57.510
Every time it's saying the
adjectives or every time I find

02:54:57.510 --> 02:55:02.130
in the JSON, one of the terms
that's called JJ, tagged as JJ,

02:55:02.130 --> 02:55:06.190
and then it's appending
them to a list and counting.

02:55:06.190 --> 02:55:08.870
So we're going to get overall of
those volumes the total number

02:55:08.870 --> 02:55:10.170
of times they occurred.

02:55:10.170 --> 02:55:11.470
Oh, and we're done.

02:55:11.470 --> 02:55:15.490
&gt;&gt; The directories,
are those pre--

02:55:15.490 --> 02:55:19.230
would you if you found that
it-- the featured [inaudible].

02:55:19.230 --> 02:55:20.530
&gt;&gt; Eleanor Dickson: Yeah.

02:55:20.530 --> 02:55:22.360
&gt;&gt; You would have to create
your own reference for--

02:55:22.360 --> 02:55:23.660
&gt;&gt; Eleanor Dickson: Yes.

02:55:23.660 --> 02:55:24.960
Yeah, yeah, yeah.

02:55:24.960 --> 02:55:26.260
And so you could look at--

02:55:26.260 --> 02:55:29.020
I mean in this case you wouldn't
have to do it by decade.

02:55:29.020 --> 02:55:32.470
In this example we are but
you could do it by author,

02:55:32.470 --> 02:55:34.550
whatever you wanted, and
you could feed in a set.

02:55:34.550 --> 02:55:39.070
Yeah. All right, so once
you get your 1970s list,

02:55:39.070 --> 02:55:42.060
do it again with the 1930s
directory and then talk

02:55:42.060 --> 02:55:45.040
to your neighbor
about what you see.

02:55:45.040 --> 02:55:47.980
If you are like, this is boring,
I need something else to do

02:55:47.980 --> 02:55:50.390
or you finished discussing
with your neighbor earlier,

02:55:50.390 --> 02:55:52.250
see if you can edit
the script to look

02:55:52.250 --> 02:55:54.980
for a different part of speech.

02:55:54.980 --> 02:55:57.390
And there are some
hints at the bottom

02:55:57.390 --> 02:55:59.510
of page 19, about
how to do that.

02:55:59.510 --> 02:56:12.110
[ Inaudible ]

02:56:12.110 --> 02:56:15.070
They're the extracted
features files

02:56:15.070 --> 02:56:19.510
for the presidential papers
to that corresponding decade.

02:56:19.510 --> 02:56:22.140
[ Inaudible ]

02:56:22.140 --> 02:56:24.930
So the one nice thing
about the feature reader is

02:56:24.930 --> 02:56:27.670
that you don't actually
have to unzip the files.

02:56:27.670 --> 02:56:30.480
But if you wanted to--
just like we used unzip

02:56:30.480 --> 02:56:34.620
with our activities files
for the bz2 you use bunzip2

02:56:34.620 --> 02:56:38.680
and then you could look at
the file if you wanted to.

02:56:38.680 --> 02:56:40.480
Yeah.

02:56:40.480 --> 02:56:43.670
[ Inaudible ]

02:56:43.670 --> 02:56:46.450
Yes, yeah, the presidential
papers in that decade.

02:56:46.450 --> 02:56:48.250
Yeah.

02:56:48.250 --> 02:57:18.720
[ Inaudible ]

02:57:18.720 --> 02:57:21.540
Or there's fewer volumes,
right, I think the-- yeah.

02:57:21.540 --> 02:57:24.530
But that's the sort of
thing you would want

02:57:24.530 --> 02:57:27.790
to not use raw counts
for probably, right?

02:57:27.790 --> 02:57:29.090
&gt;&gt; Sure.

02:57:29.090 --> 02:57:30.390
&gt;&gt; Eleanor Dickson: Yeah.

02:57:30.390 --> 02:57:32.780
Do some percentage.

02:57:32.780 --> 02:57:35.570
And so this is what you're
seeing in your console

02:57:35.570 --> 02:57:37.550
where it sort of looks
like a spreadsheet.

02:57:37.550 --> 02:57:39.600
That's a Pandas data frame.

02:57:39.600 --> 02:57:41.510
Just in case for the curious.

02:57:41.510 --> 02:57:43.060
Yeah. Go ahead.

02:57:43.060 --> 02:57:45.310
[ Inaudible ]

02:57:45.310 --> 02:57:47.360
Yeah, it is other.

02:57:47.360 --> 02:57:49.160
Yeah.

02:57:49.160 --> 02:57:54.090
[ Inaudible ]

02:57:54.090 --> 02:57:56.510
Yeah, it is actually
the word other.

02:57:56.510 --> 02:58:02.110
[ Inaudible ]

02:58:02.110 --> 02:58:05.760
Yeah. So there's no
normalization that happens here,

02:58:05.760 --> 02:58:07.310
and that's interesting.

02:58:07.310 --> 02:58:09.350
That's the sort of thing where
you'd want to look at that

02:58:09.350 --> 02:58:14.410
and go, huh, let me open up
these files or let me search

02:58:14.410 --> 02:58:16.620
through them, use grep
or something and pull

02:58:16.620 --> 02:58:19.520
out the capital,
right, and try to figure

02:58:19.520 --> 02:58:20.950
out why that's the case.

02:58:20.950 --> 02:58:23.470
And it could be a weird
thing where they're thinking

02:58:23.470 --> 02:58:26.020
like you're saying, oh, you
know, president so and so,

02:58:26.020 --> 02:58:30.380
maybe they think it's describing
that so and so, right,

02:58:30.380 --> 02:58:32.440
some sort of tokenization
error probably.

02:58:32.440 --> 02:58:33.840
Yeah, go ahead.

02:58:33.840 --> 02:58:45.180
&gt;&gt; On 1970s, the one, two,
three, four, fifth one down,

02:58:45.180 --> 02:58:50.190
and then again in-- now,
just the 1970, it lists--

02:58:50.190 --> 02:58:51.490
it's just like a hyphen or a--

02:58:51.490 --> 02:58:52.790
&gt;&gt; It is [inaudible].

02:58:52.790 --> 02:58:54.090
&gt;&gt; Eleanor Dickson: Yeah,

02:58:54.090 --> 02:58:55.390
so that's another
tokenization error, right?

02:58:55.390 --> 02:58:58.650
So, this is the sort of
thing where you're playing

02:58:58.650 --> 02:59:00.720
around with your data
and you'd want to think

02:59:00.720 --> 02:59:03.300
like what is this
underscore, right?

02:59:03.300 --> 02:59:05.570
And that's the importance of
the human here to go, well,

02:59:05.570 --> 02:59:09.110
I'm not going to shape an
academic argument that says

02:59:09.110 --> 02:59:11.740
like the underscore
was quite important

02:59:11.740 --> 02:59:16.170
to president speak--
right, you know.

02:59:16.170 --> 02:59:18.920
So that's some like
error here and the way

02:59:18.920 --> 02:59:20.220
that the text is tokenized.

02:59:20.220 --> 02:59:22.800
&gt;&gt; Is there a change
since you did it?

02:59:22.800 --> 02:59:24.550
It's not getting
different results.

02:59:24.550 --> 02:59:26.740
&gt;&gt; Eleanor Dickson: So somebody
asked abut that recently

02:59:26.740 --> 02:59:28.800
and I don't think that
we had rerun them,

02:59:28.800 --> 02:59:31.250
but maybe the version
that we are using

02:59:31.250 --> 02:59:34.230
on the instructor's guide, like
maybe it's slightly different.

02:59:34.230 --> 02:59:35.680
It shouldn't be different but
we have that earlier this week.

02:59:35.680 --> 02:59:37.510
So it could be that there
are slightly different files.

02:59:37.510 --> 02:59:45.180
[ Inaudible ]

02:59:45.180 --> 02:59:46.480
Interesting.

02:59:46.480 --> 02:59:49.900
[Inaudible] Yeah, it could
be in the process of some

02:59:49.900 --> 02:59:52.910
of our going back and forth
between versions that we're--

02:59:52.910 --> 02:59:56.050
I'm using an older
version of files or one say

02:59:56.050 --> 02:59:59.730
that has one fewer extracted
features file, right,

02:59:59.730 --> 03:00:01.170
which would account for less.

03:00:01.170 --> 03:00:05.910
Yeah. OK. So, any other thoughts

03:00:05.910 --> 03:00:11.390
about the adjectives
that you're seeing?

03:00:11.390 --> 03:00:13.190
OK.

03:00:13.190 --> 03:00:17.960
[ Inaudible ]

03:00:17.960 --> 03:00:19.260
Yeah.

03:00:19.260 --> 03:00:22.310
&gt;&gt; There's a lot more
specificity in the '70s.

03:00:23.640 --> 03:00:29.570
Like environmental, military and
private, a lot of the adjectives

03:00:29.570 --> 03:00:34.080
in the 30s are more-- I guess
there's economic there too

03:00:35.250 --> 03:00:39.570
but small, large,
serious, bold, you know,

03:00:39.570 --> 03:00:40.870
they're just generic adjectives.

03:00:40.870 --> 03:00:42.170
&gt;&gt; Eleanor Dickson: In the '30s?

03:00:42.170 --> 03:00:43.470
&gt;&gt; In the '30s.

03:00:43.470 --> 03:00:46.200
&gt;&gt; Eleanor Dickson:
Yeah, that's interesting.

03:00:46.200 --> 03:00:49.470
So we can think about the kind
of analysis that you just did

03:00:49.470 --> 03:00:50.770
and then what we're going

03:00:50.770 --> 03:00:53.040
to do while we make
this quick visualization

03:00:53.040 --> 03:00:55.720
as exploratory data analysis.

03:00:55.720 --> 03:00:59.480
So this sort of hits
back on that theme

03:00:59.480 --> 03:01:02.500
of the research process
as iterative

03:01:02.500 --> 03:01:06.570
or not progressing
in a linear fashion.

03:01:06.570 --> 03:01:09.410
So oftentimes, the researcher
especially, you know,

03:01:09.410 --> 03:01:11.280
you have these JSON files.

03:01:11.280 --> 03:01:12.650
They're zipped up.

03:01:12.650 --> 03:01:15.280
It sort of it feels
a little black boxy,

03:01:15.280 --> 03:01:18.060
but what the researcher is
going to do is do something

03:01:18.060 --> 03:01:21.350
like create that data
frame and get a feel

03:01:21.350 --> 03:01:24.080
for what are these
underscores, right?

03:01:24.080 --> 03:01:25.380
This doesn't make sense.

03:01:25.380 --> 03:01:26.780
And then they need to think OK,

03:01:26.780 --> 03:01:28.560
is there something
weird about my data?

03:01:28.560 --> 03:01:31.040
Is there something weird about
the process that I'm using?

03:01:31.040 --> 03:01:34.550
And that's sort of this
going through the process

03:01:34.550 --> 03:01:37.050
like starting a little bit,
looking at what you have

03:01:37.050 --> 03:01:39.580
because it would be
basically impossible to open

03:01:39.580 --> 03:01:44.810
up these 19 JSON files and read
through them with the human eye

03:01:44.810 --> 03:01:46.360
and notice those trends.

03:01:46.360 --> 03:01:49.040
So that's what exploratory
data analysis allows us to do.

03:01:49.040 --> 03:01:51.330
Get familiar with data,
recognize patterns

03:01:51.330 --> 03:01:53.280
and potential problems.

03:01:53.280 --> 03:01:55.380
It's really hard to see
those trends if you're say

03:01:55.380 --> 03:01:58.880
like opening a massive CSV file
to get a feel for what's there.

03:01:58.880 --> 03:02:01.810
So this exploratory approach
helps us understand the contours

03:02:01.810 --> 03:02:03.110
of the data.

03:02:03.110 --> 03:02:05.920
So some basic exploratory
data analysis strategies are

03:02:05.920 --> 03:02:08.540
to plot raw data,
plot simple statistics

03:02:08.540 --> 03:02:10.820
and then maybe do
some comparison

03:02:10.820 --> 03:02:12.420
between different
plots to get an idea

03:02:12.420 --> 03:02:15.600
of what's the same
and what's different.

03:02:15.600 --> 03:02:17.790
OK. So we have this sort of
like, oh, we're going to talk

03:02:17.790 --> 03:02:20.410
about visualization for a couple
of minutes and then we're going

03:02:20.410 --> 03:02:25.820
to talk about it again more
later as much later as we have.

03:02:25.820 --> 03:02:28.960
OK. So in-- we just want to
mention while we're still though

03:02:28.960 --> 03:02:31.570
on the track of thinking
about Python,

03:02:31.570 --> 03:02:33.540
that in Python there
are libraries

03:02:33.540 --> 03:02:35.360
for doing data visualization.

03:02:35.360 --> 03:02:37.680
So matplotlib, there's pyplot.

03:02:37.680 --> 03:02:40.430
That's a function that
allows for visualization.

03:02:40.430 --> 03:02:43.640
And then there's also a
library that's called ggplot.

03:02:43.640 --> 03:02:47.900
And then in R, there's a
library called ggplot2.

03:02:47.900 --> 03:02:50.230
And then you might hear
people talk about D3.js.

03:02:50.230 --> 03:02:54.190
That's a JavaScript library
for doing visualizations.

03:02:54.190 --> 03:02:57.420
So these are the kinds
of visualization tools,

03:02:57.420 --> 03:03:00.960
libraries that the more advanced
researcher might use in order

03:03:00.960 --> 03:03:05.050
to create those kinds of bar
graphs and things like that

03:03:05.050 --> 03:03:07.350
that we're going to
play within a second.

03:03:16.300 --> 03:03:20.440
OK. I hear typing, so pausing.

03:03:20.440 --> 03:03:27.140
So we are going to look at
the word count by page in one

03:03:27.140 --> 03:03:31.420
of the volumes from the
1970s presidential speeches.

03:03:31.420 --> 03:03:33.850
So it's presidential
papers of the presidents

03:03:33.850 --> 03:03:36.500
of the United States, one
of the Gerald Ford volumes.

03:03:36.500 --> 03:03:37.800
I can't remember if
he has more than one,

03:03:37.800 --> 03:03:40.040
but it is a Gerald Ford volume.

03:03:40.040 --> 03:03:41.340
All right.

03:03:41.340 --> 03:03:42.640
So we're going to run a script
called word count dot py

03:03:42.640 --> 03:03:45.390
and it's going to create a
nice visualization for you

03:03:45.390 --> 03:03:47.750
and then we're going to think
about what that looks like.

03:03:47.750 --> 03:03:49.050
All right.

03:03:49.050 --> 03:03:51.990
So you see here that the script
I'm just going to show it to you

03:03:51.990 --> 03:03:54.780
on the slide, we're
importing the modules here,

03:03:54.780 --> 03:04:01.510
which includes matplotlib's
pyplot as PLT.

03:04:01.510 --> 03:04:04.660
The PLT is just laziness so that
when you want to use it again,

03:04:04.660 --> 03:04:07.180
you just have to write
PLT and not matplotlib.

03:04:07.180 --> 03:04:13.070
And then we are going to create
a file that's called words.png.

03:04:13.070 --> 03:04:15.040
So when you run the
script, it's going to look

03:04:15.040 --> 03:04:17.350
like nothing happened
but then go back and look

03:04:17.350 --> 03:04:18.980
at your file viewer list

03:04:18.980 --> 03:04:20.460
and you'll see a new
one called words.png.

03:04:20.460 --> 03:04:25.980
So why don't you go ahead
and run this command.

03:04:25.980 --> 03:04:30.970
This is one that doesn't
require any arguments.

03:04:30.970 --> 03:04:33.770
We'll notice that there's
not a dot, sys.argv.

03:04:33.770 --> 03:04:36.160
That's not a requirement
for a Python script.

03:04:36.160 --> 03:04:39.430
This one, it's hard
coded into the script,

03:04:39.430 --> 03:04:42.660
the file that we're going
to open up and plot.

03:04:55.370 --> 03:04:59.030
&gt;&gt; Can you go back to the
filename we're supposed to--

03:04:59.030 --> 03:05:00.330
&gt;&gt; Eleanor Dickson: Yeah.

03:05:00.330 --> 03:05:04.850
So the filename that we're
running is word count--

03:05:04.850 --> 03:05:07.490
word underscore count dot py.

03:05:07.490 --> 03:05:14.100
And then you'll see that if it
goes OK, I don't get an error.

03:05:14.100 --> 03:05:18.810
And then I can go back and
look at my list of files.

03:05:26.830 --> 03:05:30.030
And I'll see that I now have a
file that's called words.png.

03:05:30.030 --> 03:05:33.920
If I clicked to download it,
it just opens it in the window.

03:05:41.220 --> 03:05:43.720
So right away, I noticed
some interesting things here.

03:05:43.720 --> 03:05:47.350
Do you have your file up?

03:05:47.350 --> 03:05:49.650
What are you seeing?

03:05:56.310 --> 03:05:59.990
Yeah. So we see at the-- up
to probably about page 100,

03:05:59.990 --> 03:06:02.010
there are fewer words.

03:06:02.010 --> 03:06:06.090
So maybe that's in table
of contents or in index

03:06:06.090 --> 03:06:07.450
or something like that.

03:06:07.450 --> 03:06:08.940
But then what do
we see from there?

03:06:08.940 --> 03:06:11.360
It's pretty steady, right?

03:06:11.360 --> 03:06:14.940
And then there's these
two huge spikes and I want

03:06:14.940 --> 03:06:16.280
to know what those mean, right?

03:06:16.280 --> 03:06:19.830
So if I were doing this as part
of exploratory work, I would go,

03:06:19.830 --> 03:06:23.980
huh, did I write the code wrong
or is there really one page here

03:06:23.980 --> 03:06:27.060
that has over a thousand
words on it?

03:06:27.060 --> 03:06:29.640
Or maybe it got tokenized
in an interesting way?

03:06:29.640 --> 03:06:32.880
Maybe it's a table with, I don't
know, a thousand numbers on it

03:06:32.880 --> 03:06:34.200
and those got tokenized
separately.

03:06:34.200 --> 03:06:35.500
I don't know.

03:06:35.500 --> 03:06:37.350
So this is the sort of
thing that a researcher--

03:06:37.350 --> 03:06:39.900
you would create the sort of
visualization and then you'd go,

03:06:39.900 --> 03:06:43.130
OK, I need to dig back into
the data and get a feel

03:06:43.130 --> 03:06:44.590
for what that means for me.

03:06:44.590 --> 03:06:49.880
&gt;&gt; And we could break over that
JSON file and go to that page.

03:06:49.880 --> 03:06:51.180
&gt;&gt; Eleanor Dickson: Yeah, yeah.

03:06:51.180 --> 03:06:52.480
So you could open
up to that JSON file

03:06:52.480 --> 03:06:54.510
and go down to that page.

03:06:54.510 --> 03:06:57.350
[ Inaudible ]

03:06:57.350 --> 03:06:59.500
Yup, yup, yup.

03:06:59.500 --> 03:07:02.220
And there are some
researchers who do things

03:07:02.220 --> 03:07:06.130
like when the work is not in
the public domain will go check

03:07:06.130 --> 03:07:09.320
in an actual lib like
physical lib-- actual library.

03:07:09.320 --> 03:07:11.020
Yeah?

03:07:11.020 --> 03:07:12.680
&gt;&gt; So what are we
actually seeing here?

03:07:12.680 --> 03:07:14.150
&gt;&gt; Eleanor Dickson:
So we're seeing words

03:07:14.150 --> 03:07:17.240
by page in one volume.

03:07:17.240 --> 03:07:20.420
So on the bottom, the
x-axis, we're seeing page

03:07:20.420 --> 03:07:23.090
and then along the y-axis,
we're seeing words per page.

03:07:23.090 --> 03:07:25.710
&gt;&gt; And is it in order?

03:07:25.710 --> 03:07:27.760
&gt;&gt; Eleanor Dickson:
Page number order.

03:07:27.760 --> 03:07:30.520
&gt;&gt; Right. So it's
page 800 something?

03:07:30.520 --> 03:07:31.820
&gt;&gt; Eleanor Dickson: Yes, yes.

03:07:31.820 --> 03:07:33.120
&gt;&gt; OK. So then you can go
to the physical volume--

03:07:33.120 --> 03:07:34.420
&gt;&gt; Eleanor Dickson: Yes.

03:07:34.420 --> 03:07:35.720
&gt;&gt; -- and start--

03:07:35.720 --> 03:07:37.020
&gt;&gt; Eleanor Dickson: Yup, yup.

03:07:37.020 --> 03:07:38.320
And say, huh, did I do it wrong?

03:07:38.320 --> 03:07:39.620
Is the tokenization wrong?

03:07:39.620 --> 03:07:40.920
What is happening here?

03:07:40.920 --> 03:07:42.220
Maybe it's a really interesting
thing I want to look at.

03:07:42.220 --> 03:07:44.250
Or maybe it's not interesting at
all but at least you've looked.

03:07:44.250 --> 03:07:45.550
Yeah, go ahead.

03:07:45.550 --> 03:07:47.240
&gt;&gt; So if you wanted to
change the x and y axis

03:07:47.240 --> 03:07:52.860
for whatever reason or to
further granulize the chart,

03:07:52.860 --> 03:07:57.010
you want to go into
the word count dot py--

03:07:57.010 --> 03:07:58.310
&gt;&gt; Eleanor Dickson: Yes, yeah.

03:07:58.310 --> 03:07:59.610
&gt;&gt; -- make those
changes yourself.

03:07:59.610 --> 03:08:00.910
&gt;&gt; Eleanor Dickson: Yes, yeah.

03:08:00.910 --> 03:08:02.290
You would have to pull
out something different

03:08:02.290 --> 03:08:05.520
for what you wanted to look at,
specify what you wanted to look

03:08:05.520 --> 03:08:09.070
at and then pyplot, the
tool we're using here

03:08:09.070 --> 03:08:12.410
to do the visualization, you
can set different parameters

03:08:12.410 --> 03:08:15.140
for how the x and y
axis are displayed,

03:08:15.140 --> 03:08:18.010
like there are some
customization that you can do.

03:08:18.010 --> 03:08:19.840
&gt;&gt; So you would do that in the--

03:08:19.840 --> 03:08:21.490
&gt;&gt; Eleanor Dickson: In
the script still, yeah.

03:08:21.490 --> 03:08:23.940
&gt;&gt; In the scripts
or in the matplot?

03:08:23.940 --> 03:08:25.240
&gt;&gt; Eleanor Dickson:
In the script, yes.

03:08:25.240 --> 03:08:30.140
Yup. And you would just call
out to matplotlib, that library,

03:08:30.140 --> 03:08:33.940
say, you know, I'm using you
but I want you when you run

03:08:33.940 --> 03:08:38.240
to make this picture to be green
or show this on the x-axis.

03:08:38.240 --> 03:08:39.540
Yeah.

03:08:39.540 --> 03:08:40.840
&gt;&gt; OK.

03:08:40.840 --> 03:08:42.140
&gt;&gt; Eleanor Dickson:
Any other questions?

03:08:42.140 --> 03:08:45.180
Yeah?

03:08:45.180 --> 03:08:54.170
&gt;&gt; Just to curve what you said,
even though it's extracting

03:08:54.170 --> 03:09:05.050
on the copy written material
and it's running the whole,

03:09:05.050 --> 03:09:07.990
so it's running it, it will say
this is in volume 15 page 5.

03:09:07.990 --> 03:09:10.610
So giving that specificity,
but then you have to have

03:09:10.610 --> 03:09:12.540
that file separately because
it's a partial view over.

03:09:12.540 --> 03:09:13.840
&gt;&gt; Eleanor Dickson:
Well, I guess it depends

03:09:13.840 --> 03:09:17.180
on what you think
of as partial view.

03:09:17.180 --> 03:09:19.130
So if it's enough
to do the checking

03:09:19.130 --> 03:09:21.920
to have the word counts,
then you have them there

03:09:21.920 --> 03:09:24.330
in the extracted features file.

03:09:24.330 --> 03:09:28.710
&gt;&gt; If you wanted to do
further analysis and you wanted

03:09:28.710 --> 03:09:30.010
to do a closed reading--

03:09:30.010 --> 03:09:31.310
&gt;&gt; Eleanor Dickson: Yeah.

03:09:31.310 --> 03:09:40.550
You would have to go back
to the original file.

03:09:40.550 --> 03:09:41.850
Yeah, yeah.

03:09:41.850 --> 03:09:43.150
Yes, yeah.

03:09:43.150 --> 03:09:44.450
Yeah. Yup?

03:09:44.450 --> 03:09:45.750
&gt;&gt; The actual JSON files,

03:09:45.750 --> 03:09:47.050
you can't actually
view that and then--

03:09:47.050 --> 03:09:48.350
&gt;&gt; Eleanor Dickson:
Yeah, you can.

03:09:48.350 --> 03:09:49.650
Let's look at one
really quickly.

03:09:49.650 --> 03:09:50.950
So if you want to open it,
remember we used unzip before.

03:09:50.950 --> 03:09:52.250
We use-- it's called bunzip2.

03:09:52.250 --> 03:09:53.550
All right.

03:09:53.550 --> 03:09:54.850
And then you're going to
put in the name of the file.

03:09:54.850 --> 03:09:56.150
Actually, let me just
see what's there.

03:09:56.150 --> 03:09:57.450
I'm going to copy and paste.

03:09:57.450 --> 03:09:58.750
So I'm going to unzip
one of these.

03:09:58.750 --> 03:10:01.050
OK.

03:10:06.880 --> 03:10:09.870
OK. Any other questions
while I'm doing this?

03:10:13.180 --> 03:10:14.480
And then there's the JSON.

03:10:14.480 --> 03:10:16.490
It's not pretty printed
but there's all

03:10:16.490 --> 03:10:17.790
of the information there.

03:10:17.790 --> 03:10:19.810
The nice thing about that
feature reader library is

03:10:19.810 --> 03:10:26.040
that you don't have to unzip the
file which is-- and for-- sorry.

03:10:26.040 --> 03:10:28.690
For getting your
feet wet with it,

03:10:28.690 --> 03:10:32.000
it feels like there's
a barrier there, right,

03:10:32.000 --> 03:10:34.310
like there's some disconnect
between you and the file.

03:10:34.310 --> 03:10:36.940
But if you're working with
loads of data, then having it

03:10:36.940 --> 03:10:38.980
in a compressed format is
better because you're taking

03:10:38.980 --> 03:10:40.780
up less space on your machine.

03:10:40.780 --> 03:10:46.120
[ Inaudible ]

03:10:46.120 --> 03:10:47.420
Yes, yeah.

03:10:47.420 --> 03:10:48.740
That it doesn't know
how to read the JSON.

03:10:48.740 --> 03:10:54.000
Yeah. OK. So we are doing great

03:10:54.000 --> 03:10:57.770
and we are going
to-- we did this.

03:10:57.770 --> 03:11:01.430
I'm actually doing
really well on time.

03:11:01.430 --> 03:11:05.320
I'll say briefly that when they
did this example that you looked

03:11:05.320 --> 03:11:08.710
at earlier, that
machine learning project

03:11:08.710 --> 03:11:11.150
that we have talked
about a couple times now,

03:11:11.150 --> 03:11:13.610
they use extracted
features data for that work.

03:11:13.610 --> 03:11:16.220
So that was all using
extracted features.

03:11:16.220 --> 03:11:17.520
OK. All right.

03:11:17.520 --> 03:11:18.820
Let's talk about Sam again.

03:11:18.820 --> 03:11:20.120
OK. So now we're going to--

03:11:20.120 --> 03:11:22.080
almost the end of
our time with Sam.

03:11:22.080 --> 03:11:25.560
So Sam now that he
has his data prepared.

03:11:25.560 --> 03:11:28.060
It's reduced down to just
the text he wants to look at,

03:11:28.060 --> 03:11:31.410
then Sam did topic
modeling only on those pages

03:11:31.410 --> 03:11:33.630
where his search term appeared.

03:11:33.630 --> 03:11:36.580
And he ends up with the
topics that show the changes

03:11:36.580 --> 03:11:38.740
and the use of the
word creativity

03:11:38.740 --> 03:11:41.410
over the 20th century.

03:11:41.410 --> 03:11:45.780
So now he's graphing the
use of the topics over time.

03:11:45.780 --> 03:11:49.940
So it's small but at the top
we see the words that are

03:11:49.940 --> 03:11:53.840
in that topic and then
you see the probability

03:11:53.840 --> 03:11:56.180
that it would occur
over time going down.

03:11:56.180 --> 03:11:59.340
So it's just a different
way of viewing the topics.

03:11:59.340 --> 03:12:03.700
But we see that over time
creativity becomes used less

03:12:03.700 --> 03:12:06.210
in topics that include words
like, God, Christ, Jesus,

03:12:06.210 --> 03:12:09.100
creation, word, species,
animals, natural, plants, soil,

03:12:09.100 --> 03:12:10.900
invention, power
creative, own, ideas.

03:12:10.900 --> 03:12:15.200
And we see it going up in
topics that include words

03:12:15.200 --> 03:12:18.770
like advertising,
media, marketing, poetry,

03:12:18.770 --> 03:12:22.210
language, poets, poems.

03:12:22.210 --> 03:12:26.650
So based on these two ways
of looking at that data

03:12:26.650 --> 03:12:32.050
and those changes, Sam was
able to make the argument

03:12:32.050 --> 03:12:34.050
in this chapter of
his dissertation

03:12:34.050 --> 03:12:38.760
that he used this work
in to say that the usage

03:12:38.760 --> 03:12:41.280
of the word creativity
changed over the 20th century

03:12:41.280 --> 03:12:46.040
to move away from thinking about
creation as a generative thing

03:12:46.040 --> 03:12:49.960
like this was created into
talking about it in terms

03:12:49.960 --> 03:12:55.530
of artistic or a productive
creativity in that sort of space

03:12:55.530 --> 03:12:57.780
where we think about in terms
of art and things like that.

03:12:57.780 --> 03:12:59.080
Yeah?

03:12:59.080 --> 03:13:01.930
&gt;&gt; Well, [inaudible] to go read
the dissertation [inaudible]

03:13:01.930 --> 03:13:03.560
but I want to make sure
I understand what--

03:13:03.560 --> 03:13:07.200
I think he said-- so
using just the pages

03:13:07.200 --> 03:13:10.040
that had variations
on the word created.

03:13:10.040 --> 03:13:11.360
&gt;&gt; Eleanor Dickson: Yes.

03:13:11.360 --> 03:13:16.860
&gt;&gt; So are those each
a separate chunk?

03:13:16.860 --> 03:13:19.190
&gt;&gt; Eleanor Dickson: You
know, I don't know how he fed

03:13:19.190 --> 03:13:20.490
that into the topic
model [inaudible].

03:13:20.490 --> 03:13:21.880
That's a good question.

03:13:21.880 --> 03:13:24.660
&gt;&gt; But like-- so somehow
he told them he's in JSON

03:13:24.660 --> 03:13:27.180
that identified things by page?

03:13:27.180 --> 03:13:28.480
&gt;&gt; Eleanor Dickson: Yeah, yeah.

03:13:28.480 --> 03:13:29.890
So he was using the
extracted features files.

03:13:29.890 --> 03:13:32.530
So he just used those
words-- the word counts.

03:13:32.530 --> 03:13:34.990
&gt;&gt; So, where they appear on the
page and he pulled that out?

03:13:34.990 --> 03:13:36.420
&gt;&gt; Eleanor Dickson: He pulled--
Yeah, all the other words.

03:13:36.420 --> 03:13:37.720
Yeah.

03:13:37.720 --> 03:13:39.240
&gt;&gt; OK. So somehow
[inaudible] chunk them off.

03:13:39.240 --> 03:13:40.540
&gt;&gt; Eleanor Dickson: Yeah.

03:13:40.540 --> 03:13:41.840
It might be at the
page level still.

03:13:41.840 --> 03:13:43.140
Yeah.

03:13:43.140 --> 03:13:45.830
&gt;&gt; But then having to
get a graph over time.

03:13:45.830 --> 03:13:47.130
&gt;&gt; Eleanor Dickson: Yeah.

03:13:47.130 --> 03:13:49.370
So I think that they
took those probabilities

03:13:49.370 --> 03:13:51.810
so they would have said--

03:13:51.810 --> 03:13:53.530
try to think about how
they would have done this.

03:13:53.530 --> 03:13:59.050
They said-- I saw this topic
advertising, media, marketing,

03:13:59.050 --> 03:14:02.640
sales, television occur
not very many times

03:14:02.640 --> 03:14:05.120
when I did the topic
modeling by year.

03:14:05.120 --> 03:14:07.650
I think they did it for every
year, modeled every year,

03:14:07.650 --> 03:14:09.820
but I did see it a lot in 2000

03:14:09.820 --> 03:14:11.880
and then they've
made that chart.

03:14:11.880 --> 03:14:13.180
Does that make sense?

03:14:13.180 --> 03:14:15.660
&gt;&gt; No. Because I thought
the topics were derived

03:14:15.660 --> 03:14:19.620
from the text so you couldn't
go and look for the same topic

03:14:19.620 --> 03:14:22.340
and it-- like say
you had the 18--

03:14:22.340 --> 03:14:24.890
the 1970 data and
the 1930 data--

03:14:24.890 --> 03:14:26.190
&gt;&gt; Eleanor Dickson: Yeah.

03:14:26.190 --> 03:14:27.680
&gt;&gt; -- you wouldn't expect
to find the exact same topic

03:14:27.680 --> 03:14:30.710
in each one because the topics
are derived from the data.

03:14:30.710 --> 03:14:34.320
You don't impose the topic and
look for it in both places.

03:14:34.320 --> 03:14:35.620
&gt;&gt; Eleanor Dickson: Yeah.

03:14:35.620 --> 03:14:37.830
So I think that-- go ahead.

03:14:37.830 --> 03:14:39.280
&gt;&gt; My understanding with that--

03:14:39.280 --> 03:14:42.180
so those are like
example words, right?

03:14:42.180 --> 03:14:44.080
&gt;&gt; Eleanor Dickson: Uh-huh,
that would occur in that topic.

03:14:44.080 --> 03:14:45.380
Yeah.

03:14:45.380 --> 03:14:48.010
&gt;&gt; So that you wouldn't
search for like advertising

03:14:48.010 --> 03:14:51.400
in the same-- on the same
page as that would be

03:14:51.400 --> 03:14:54.190
like for all the
things and then--

03:14:54.190 --> 03:14:57.890
and then like the
one in the 1950s.

03:14:57.890 --> 03:15:01.910
&gt;&gt; So, you think those were
occurrences of those words?

03:15:01.910 --> 03:15:03.210
&gt;&gt; Yeah, exactly
like [inaudible].

03:15:03.210 --> 03:15:04.510
&gt;&gt; Oh, see, I--

03:15:04.510 --> 03:15:05.810
&gt;&gt; Eleanor Dickson: I don't--

03:15:05.810 --> 03:15:07.110
&gt;&gt; [Inaudible] this
because I'm not sure I--

03:15:07.110 --> 03:15:08.410
&gt;&gt; Eleanor Dickson: I
don't know that that's true

03:15:08.410 --> 03:15:09.710
but how they got from
the topics or the topics

03:15:09.710 --> 03:15:12.510
over time, I'm not quite sure--

03:15:12.510 --> 03:15:15.860
[ Inaudible ]

03:15:15.860 --> 03:15:21.130
Yeah. I don't know if they broke
the text out into like pieces

03:15:21.130 --> 03:15:22.860
but it has something to
do with the likelihood

03:15:22.860 --> 03:15:25.980
that that topic was going
to occur in that year.

03:15:25.980 --> 03:15:27.280
Yeah. Go ahead.

03:15:27.280 --> 03:15:29.060
&gt;&gt; So I mean do you think that
they-- one way would then--

03:15:29.060 --> 03:15:31.860
is to do what had overall kind
of these are the words that come

03:15:31.860 --> 03:15:34.120
up generally and then--

03:15:34.120 --> 03:15:37.710
but in year by year
so you do the overall

03:15:37.710 --> 03:15:39.510
on the entire purpose
or [inaudible]

03:15:39.510 --> 03:15:41.660
that list then back in--

03:15:41.660 --> 03:15:42.960
&gt;&gt; Eleanor Dickson:
I don't think so.

03:15:42.960 --> 03:15:45.270
I think it's more like
he either took snippets

03:15:45.270 --> 03:15:46.810
that would have been
year by year

03:15:46.810 --> 03:15:52.360
or he did one big topic
model and then the output

03:15:52.360 --> 03:15:55.720
of however he did it said
that it was occurring--

03:15:55.720 --> 03:15:58.810
the topic had a high
prevalence in these volumes

03:15:58.810 --> 03:16:00.640
that were published
in these years,

03:16:00.640 --> 03:16:03.640
maybe that's how
they did the-- yeah.

03:16:03.640 --> 03:16:06.100
&gt;&gt; [Inaudible] exploratory and
then refining your [inaudible].

03:16:06.100 --> 03:16:08.790
&gt;&gt; Eleanor Dickson: Yeah, yeah.

03:16:08.790 --> 03:16:10.360
OK. Any other questions?

03:16:10.360 --> 03:16:11.660
That's a good question.

03:16:11.660 --> 03:16:13.460
Yeah?

03:16:13.460 --> 03:16:38.390
[ Inaudible ]

03:16:38.390 --> 03:16:39.690
Yeah, that's a good point.

03:16:39.690 --> 03:16:42.190
&gt;&gt; It's just interesting
how he chose to depict it

03:16:42.190 --> 03:16:45.900
because it makes it look like
they're all about the same kind

03:16:45.900 --> 03:16:50.760
of terms [inaudible]
normalized y-axis.

03:16:50.760 --> 03:16:52.060
&gt;&gt; Eleanor Dickson: Yeah,
that's a great point.

03:16:52.060 --> 03:16:55.880
So for the purposes of wherever
I grabbed these visualizations

03:16:55.880 --> 03:16:59.980
from whatever report
he had done, yeah,

03:16:59.980 --> 03:17:02.320
they tried to make the steepness

03:17:02.320 --> 03:17:04.110
of the line look
the same, right?

03:17:04.110 --> 03:17:06.860
But you're pointing
out that the scale

03:17:06.860 --> 03:17:10.800
on the y-axis are different
for the different charts.

03:17:10.800 --> 03:17:14.980
So we get the impact here
of the trend going down

03:17:14.980 --> 03:17:16.780
but it might look more drastic

03:17:16.780 --> 03:17:20.660
than it would otherwise
feel potentially.

03:17:20.660 --> 03:17:26.040
Yeah. Any other thoughts here?

03:17:26.040 --> 03:17:33.650
OK. So it was always the
awkward spot we end up in.

03:17:33.650 --> 03:17:37.780
Do you want us to have
a nice discussion,

03:17:37.780 --> 03:17:41.180
do the wrap up to our
survey or do you want

03:17:41.180 --> 03:17:46.510
to do 10-minute play time
with a visualization tool?

03:17:46.510 --> 03:17:47.810
Let's use the tool.

03:17:47.810 --> 03:17:50.500
You all look really
[laughter] full of energy.

03:17:50.500 --> 03:17:51.800
All right.

03:17:51.800 --> 03:17:54.340
So if you wanted to go
and go through the rest

03:17:54.340 --> 03:17:59.850
of this module 5 about data
visualization, then you're--

03:17:59.850 --> 03:18:02.460
I would recommend watching
the recording that we did

03:18:02.460 --> 03:18:05.120
and I'll show you where
that is at the very end.

03:18:05.120 --> 03:18:09.610
But I'm going to skip down
to where there's a link here

03:18:09.610 --> 03:18:12.010
and we're going to
play with Bookworm.

03:18:12.010 --> 03:18:14.850
All right.

03:18:14.850 --> 03:18:19.280
So before we open it
up, we talked earlier

03:18:19.280 --> 03:18:21.460
about N-gram inclusion.

03:18:21.460 --> 03:18:24.310
So N-gram-- just as a reminder,

03:18:24.310 --> 03:18:27.370
an N-gram is a contiguous
chain of n item.

03:18:27.370 --> 03:18:30.810
So where n is the number
from a sequence of text.

03:18:30.810 --> 03:18:33.310
So you could have a unit gram,
that's one word, a bigram,

03:18:33.310 --> 03:18:35.450
that's two, trigram,
that's three.

03:18:35.450 --> 03:18:38.050
And oftentimes when you're
creating bigrams like--

03:18:38.050 --> 03:18:41.890
or creating N-grams like
these bigrams in the box here,

03:18:41.890 --> 03:18:45.130
we're seeing that it's sort of
a sliding scale over the text.

03:18:45.130 --> 03:18:47.630
So one bigram is four score

03:18:47.630 --> 03:18:52.140
and the next must score
n and then n7, 7 years.

03:18:52.140 --> 03:18:54.360
So, sort of moving along.

03:18:54.360 --> 03:18:57.350
And so that's the text that
we're going to be playing with,

03:18:57.350 --> 03:18:59.880
the visualization tool,
the text that it's running

03:18:59.880 --> 03:19:04.460
on is the N-grams or unigrams.

03:19:04.460 --> 03:19:06.300
So I always think like,
oh, wouldn't it be fun

03:19:06.300 --> 03:19:07.800
if I just searched city names?

03:19:07.800 --> 03:19:09.740
I'm from California
and then you say like,

03:19:09.740 --> 03:19:11.330
I can't do Los Angeles.

03:19:11.330 --> 03:19:13.190
So you can't put in a
bigram in the search bar.

03:19:13.190 --> 03:19:14.850
We're going to do unigrams only.

03:19:14.850 --> 03:19:16.310
All right.

03:19:16.310 --> 03:19:18.660
So we're playing with
a tool called Bookworm.

03:19:18.660 --> 03:19:20.680
And Bookworm is a-- how many

03:19:20.680 --> 03:19:25.100
of you have played
with Google N-grams?

03:19:25.100 --> 03:19:29.230
Yeah. So Bookworm is like ht or
sys version of Google N-grams.

03:19:29.230 --> 03:19:30.860
So, some of the same
people who worked

03:19:30.860 --> 03:19:34.870
on the original Google
N-gram viewer worked

03:19:34.870 --> 03:19:36.410
on this project also.

03:19:36.410 --> 03:19:41.000
But the benefit we see here,
we lose out on phrase searching

03:19:41.000 --> 03:19:43.730
but we get the benefit
of a library metadata.

03:19:43.730 --> 03:19:48.340
So if you have the analytics
site open and you click

03:19:48.340 --> 03:19:52.530
on explore up at the very top,
you don't have to be logged in.

03:19:52.530 --> 03:19:54.760
And then the first link

03:19:54.760 --> 03:19:57.160
under explore is
HathiTrust plus Bookworm.

03:19:57.160 --> 03:19:59.280
Click try it now.

03:19:59.280 --> 03:20:03.510
And then you'll see that it's
pretty populated with polka.

03:20:03.510 --> 03:20:05.680
Just an interesting choice.

03:20:05.680 --> 03:20:09.060
And then you can
search for your n terms.

03:20:09.060 --> 03:20:14.870
So if you add words
to search here and--

03:20:17.390 --> 03:20:21.760
yeah, you end up with
two different bars.

03:20:21.760 --> 03:20:24.670
So every word is its
own bar on the graph

03:20:24.670 --> 03:20:26.880
so you're not graphing
them on one line.

03:20:26.880 --> 03:20:29.100
There will be two
separate lines.

03:20:29.100 --> 03:20:33.480
And then if you click on this
thing that looks like a funnel,

03:20:33.480 --> 03:20:35.700
you can facet your text.

03:20:35.700 --> 03:20:40.580
So limit it based on the
bibliographic metadata.

03:20:40.580 --> 03:20:44.010
So you can narrow it
down based on language,

03:20:44.010 --> 03:20:49.940
publication country, publication
state, the classification terms.

03:20:49.940 --> 03:20:54.960
&gt;&gt; Can you get down to the
specific journal title?

03:20:54.960 --> 03:20:56.950
&gt;&gt; Eleanor Dickson: So you--

03:20:56.950 --> 03:21:01.410
The thing that you can
do is click on the line.

03:21:01.410 --> 03:21:06.280
Sometimes it takes a
second especially in Chrome.

03:21:06.280 --> 03:21:07.580
There we go.

03:21:07.580 --> 03:21:10.330
And it will pull up every
place where the word occurred.

03:21:10.330 --> 03:21:13.380
&gt;&gt; Can you search
only on the specific--

03:21:13.380 --> 03:21:16.470
&gt;&gt; Eleanor Dickson: No, yeah.

03:21:16.470 --> 03:21:17.840
That would be cool, huh.

03:21:17.840 --> 03:21:20.470
And then I recommend control
clicking or right clicking

03:21:20.470 --> 03:21:23.840
to open because it
opens in the same tab

03:21:23.840 --> 03:21:29.180
and if you go back your
visualization is lost.

03:21:29.180 --> 03:21:33.110
And then the other thing
that you can do here is play

03:21:33.110 --> 03:21:35.770
around with either the
dates, the metrics.

03:21:35.770 --> 03:21:39.910
So I was doing words per million
to try to normalize for the fact

03:21:39.910 --> 03:21:42.690
that there was more
text produced over time

03:21:42.690 --> 03:21:43.990
so you don't want
to do raw counts

03:21:43.990 --> 03:21:45.290
because then you'd always think

03:21:45.290 --> 03:21:48.080
that things are more prevalent
later in time than earlier.

03:21:48.080 --> 03:21:51.070
So there's some balancing
the choice you have

03:21:51.070 --> 03:21:52.680
by doing words per million.

03:21:52.680 --> 03:21:54.520
And then you can also
do case sensitive

03:21:54.520 --> 03:21:57.450
versus insensitive depending on
the word that you're searching.

03:21:57.450 --> 03:21:59.730
So I'll give you a few minutes
to play here, just like two

03:21:59.730 --> 03:22:03.890
or three minutes, see if you
find anything interesting.

03:22:03.890 --> 03:22:07.130
I think that puts pressure on
people, so also not interesting,

03:22:07.130 --> 03:22:09.600
but it's always fun to
see what people search.

03:22:09.600 --> 03:22:13.690
I went down the path one time
to learn about the history

03:22:13.690 --> 03:22:15.560
of vaccines because
I thought, oh,

03:22:15.560 --> 03:22:17.210
wouldn't vaccine be interesting?

03:22:17.210 --> 03:22:20.840
And then you get these blips
that are OCR errors that have

03:22:20.840 --> 03:22:25.090
to do with the Latin for
cow and then I learned

03:22:25.090 --> 03:22:27.200
about how the first
vaccines were for cowpox.

03:22:27.200 --> 03:22:29.220
So it was like oh look, the
tool taught me something

03:22:29.220 --> 03:22:30.780
that I wasn't expecting to know.

03:22:30.780 --> 03:22:32.290
So it can be useful.

03:22:32.290 --> 03:22:33.590
All right.

03:22:33.590 --> 03:22:34.890
Anything else someone
wants to share?

03:22:34.890 --> 03:22:36.190
We have about 10 minutes

03:22:36.190 --> 03:22:37.770
so we can probably
move into our wrap up.

03:22:37.770 --> 03:22:42.620
If you are interested in--
let me get this webpage up,

03:22:42.620 --> 03:22:45.290
if you're interested in learning
more about visualization

03:22:45.290 --> 03:22:48.490
at least the way that
we are talking about it

03:22:48.490 --> 03:22:50.610
in this project, well,

03:22:50.610 --> 03:22:55.220
you go look through the
curriculum materials or--

03:22:55.220 --> 03:22:59.090
We did a series of webinars
leading up to this week.

03:22:59.090 --> 03:23:02.460
We haven't put the last one on
the web yet but we will soon.

03:23:02.460 --> 03:23:04.110
So they started in
May and they went

03:23:04.110 --> 03:23:05.500
through August 1 in a month.

03:23:05.500 --> 03:23:08.100
And the first one was
me doing the state

03:23:08.100 --> 03:23:11.500
of visualization module
because I don't know

03:23:11.500 --> 03:23:13.690
that we have ever gotten
to it in a workshop,

03:23:13.690 --> 03:23:18.010
so you are not alone in the
fact that you did not get

03:23:18.010 --> 03:23:19.480
through the whole thing.

03:23:19.480 --> 03:23:22.140
So yeah, we put it up there
so that people could enjoy

03:23:22.140 --> 03:23:23.960
if they wanted to or watch it.

03:23:23.960 --> 03:23:25.260
I don't know if they enjoyed it.

03:23:25.260 --> 03:23:26.920
And then we have data management

03:23:26.920 --> 03:23:28.910
and then we have this
mining newspaper data.

03:23:28.910 --> 03:23:30.230
Harriett mentioned that earlier.

03:23:30.230 --> 03:23:35.580
That's all about using R to mine
the Library of Congress, the--

03:23:35.580 --> 03:23:38.590
chronicling America
newspapers dataset.

03:23:38.590 --> 03:23:40.430
And then the last one

03:23:40.430 --> 03:23:44.090
that happened this week was a
librarian from the University

03:23:44.090 --> 03:23:47.820
of Iowa talking about a research
collaboration she's been

03:23:47.820 --> 03:23:51.000
involved in where it was-- she
was part of a research team

03:23:51.000 --> 03:23:54.380
at Iowa that partnered with
HTRC on a research project.

03:23:54.380 --> 03:23:56.930
And so what it was like for
her to provide that sort

03:23:56.930 --> 03:24:01.830
of library oriented
text analysis support

03:24:01.830 --> 03:24:04.230
within that project team.

03:24:04.230 --> 03:24:07.660
So again, all these materials
are there for you to use,

03:24:07.660 --> 03:24:09.960
reuse if you would like.

03:24:24.690 --> 03:24:25.990
All right.

03:24:25.990 --> 03:24:28.820
With our last few minutes, I'll
just put the slide up again

03:24:28.820 --> 03:24:30.410
that shows the names of
all the folks who've worked

03:24:30.410 --> 03:24:31.960
on this project.

03:24:31.960 --> 03:24:34.390
There have been many.

03:24:34.390 --> 03:24:36.490
And let you know that
if you have questions

03:24:36.490 --> 03:24:40.550
after the workshop, you are
more than welcome to contact us

03:24:40.550 --> 03:24:42.800
at the email address
that's on the screen.

03:24:42.800 --> 03:24:46.290
So it's HTRC underscore
workshop@library.illinois.edu.

03:24:46.290 --> 03:24:49.930
If you have specific
questions about the contents,

03:24:49.930 --> 03:24:52.690
reusing the content
on what we covered

03:24:52.690 --> 03:24:55.540
and then you can also refer
back to the project website.

03:24:55.540 --> 03:25:01.890
We've been piloting a software
platform called comments

03:25:01.890 --> 03:25:03.190
in a box.

03:25:03.190 --> 03:25:05.980
Are any of you familiar with
comments in a box, for all folks

03:25:05.980 --> 03:25:09.430
who have attended the workshops
since we started doing them,

03:25:09.430 --> 03:25:12.160
so you will likely see an
invitation to the comments

03:25:12.160 --> 03:25:14.210
in a box instance
for this project.

03:25:14.210 --> 03:25:16.460
And there's some conversation
that happens on that forum

03:25:16.460 --> 03:25:19.880
about ways people have
reused the curriculum,

03:25:19.880 --> 03:25:22.340
examples they've seen of
text analysis projects,

03:25:22.340 --> 03:25:24.160
help getting started,
that sort of thing.

03:25:24.160 --> 03:25:25.460
So if you see the invitation,

03:25:25.460 --> 03:25:27.430
you're more than obviously
welcome to joint that group

03:25:27.430 --> 03:25:29.670
and join in the conversation.

03:25:29.670 --> 03:25:31.750
But other than that,
we are done.

03:25:31.750 --> 03:25:35.430
&gt;&gt; This has been a presentation
of the Library of Congress.

03:25:35.430 --> 03:25:37.940
Visit us at loc.gov.

