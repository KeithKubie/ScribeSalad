WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:02.717
&gt;&gt; Alright, let's do another quiz. That previous example that

00:00:02.717 --> 00:00:06.198
we looked at of intervals, was nice and pedagogical, and reasonable

00:00:06.198 --> 00:00:09.384
to think about, but we actually hadn't really talked about any

00:00:09.384 --> 00:00:14.210
learning algorithms that used intervals. On the other hand, linear separators

00:00:14.210 --> 00:00:16.720
are a very big deal in machine learning. So, it's,

00:00:16.720 --> 00:00:19.370
it's very worthwhile, and it turns out to be not too

00:00:19.370 --> 00:00:22.210
bad to work out what the vc dimension is for linear

00:00:22.210 --> 00:00:26.350
separators. So, let's say that we're in two dimensional space, and

00:00:26.350 --> 00:00:29.150
so our hypotheses have the form that you've got

00:00:29.150 --> 00:00:32.860
a parameter, a weight parameter, w. And were going to just

00:00:32.860 --> 00:00:35.760
take that weight parameter, take the dot product with whatever

00:00:35.760 --> 00:00:37.710
the input is, and see whether its greater than or

00:00:37.710 --> 00:00:41.326
equal to some value, theta. And if it is,

00:00:41.326 --> 00:00:43.885
then we say that's a positive example, and if not

00:00:43.885 --> 00:00:46.562
it's a negative example, and geometrically that just means that

00:00:46.562 --> 00:00:51.410
we've, we end up specifying a line, and everything on

00:00:51.410 --> 00:00:53.380
one side of the line is going to be positive, and

00:00:53.380 --> 00:00:55.070
everything on the other side of the line's going to be negative.

00:00:55.070 --> 00:00:57.950
&gt;&gt; Got it. That makes sense. So what's the vc

00:00:57.950 --> 00:01:01.115
dimension? Oh, they're going to have to tell us. I like that.

00:01:01.115 --> 00:01:02.034
&gt;&gt; Alright.

00:01:02.034 --> 00:01:03.014
&gt;&gt; Go.

