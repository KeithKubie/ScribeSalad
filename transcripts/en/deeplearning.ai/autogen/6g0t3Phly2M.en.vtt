WEBVTT
Kind: captions
Language: en

00:00:00.060 --> 00:00:02.210
 
if you suspect in your networkers

00:00:02.210 --> 00:00:02.220
if you suspect in your networkers
 

00:00:02.220 --> 00:00:04.130
if you suspect in your networkers
overfitting your data that is if you

00:00:04.130 --> 00:00:04.140
overfitting your data that is if you
 

00:00:04.140 --> 00:00:06.050
overfitting your data that is if you
have a high variance problem one of the

00:00:06.050 --> 00:00:06.060
have a high variance problem one of the
 

00:00:06.060 --> 00:00:07.610
have a high variance problem one of the
first things is to try this pervy

00:00:07.610 --> 00:00:07.620
first things is to try this pervy
 

00:00:07.620 --> 00:00:10.190
first things is to try this pervy
regularization the other way to address

00:00:10.190 --> 00:00:10.200
regularization the other way to address
 

00:00:10.200 --> 00:00:11.930
regularization the other way to address
high variance is to get more training

00:00:11.930 --> 00:00:11.940
high variance is to get more training
 

00:00:11.940 --> 00:00:14.089
high variance is to get more training
data there's also quite reliable we

00:00:14.089 --> 00:00:14.099
data there's also quite reliable we
 

00:00:14.099 --> 00:00:16.039
data there's also quite reliable we
can't always get more training data to

00:00:16.039 --> 00:00:16.049
can't always get more training data to
 

00:00:16.049 --> 00:00:18.439
can't always get more training data to
be expensive get more data but adding

00:00:18.439 --> 00:00:18.449
be expensive get more data but adding
 

00:00:18.449 --> 00:00:20.480
be expensive get more data but adding
regularization will often help to

00:00:20.480 --> 00:00:20.490
regularization will often help to
 

00:00:20.490 --> 00:00:22.070
regularization will often help to
prevent overfitting or to reduce

00:00:22.070 --> 00:00:22.080
prevent overfitting or to reduce
 

00:00:22.080 --> 00:00:24.290
prevent overfitting or to reduce
variance during your network so let's

00:00:24.290 --> 00:00:24.300
variance during your network so let's
 

00:00:24.300 --> 00:00:26.089
variance during your network so let's
see how regularization works let's

00:00:26.089 --> 00:00:26.099
see how regularization works let's
 

00:00:26.099 --> 00:00:27.890
see how regularization works let's
develop these ideas using logistic

00:00:27.890 --> 00:00:27.900
develop these ideas using logistic
 

00:00:27.900 --> 00:00:29.779
develop these ideas using logistic
regression recall that the logistic

00:00:29.779 --> 00:00:29.789
regression recall that the logistic
 

00:00:29.789 --> 00:00:31.640
regression recall that the logistic
regression you try to minimize the cost

00:00:31.640 --> 00:00:31.650
regression you try to minimize the cost
 

00:00:31.650 --> 00:00:35.510
regression you try to minimize the cost
function J which is defined as this cost

00:00:35.510 --> 00:00:35.520
function J which is defined as this cost
 

00:00:35.520 --> 00:00:38.270
function J which is defined as this cost
function some of your training examples

00:00:38.270 --> 00:00:38.280
function some of your training examples
 

00:00:38.280 --> 00:00:40.520
function some of your training examples
of the losses of the individual

00:00:40.520 --> 00:00:40.530
of the losses of the individual
 

00:00:40.530 --> 00:00:43.190
of the losses of the individual
predictions and different examples where

00:00:43.190 --> 00:00:43.200
predictions and different examples where
 

00:00:43.200 --> 00:00:46.459
predictions and different examples where
you recall that wmd logistic regression

00:00:46.459 --> 00:00:46.469
you recall that wmd logistic regression
 

00:00:46.469 --> 00:00:49.970
you recall that wmd logistic regression
are their parameters so W is a an X

00:00:49.970 --> 00:00:49.980
are their parameters so W is a an X
 

00:00:49.980 --> 00:00:52.729
are their parameters so W is a an X
dimensional parameter vector and B is a

00:00:52.729 --> 00:00:52.739
dimensional parameter vector and B is a
 

00:00:52.739 --> 00:00:55.850
dimensional parameter vector and B is a
real number and so to add regularization

00:00:55.850 --> 00:00:55.860
real number and so to add regularization
 

00:00:55.860 --> 00:00:57.920
real number and so to add regularization
to rich it's regression what you do is

00:00:57.920 --> 00:00:57.930
to rich it's regression what you do is
 

00:00:57.930 --> 00:01:00.830
to rich it's regression what you do is
add to it this thing lambda which is

00:01:00.830 --> 00:01:00.840
add to it this thing lambda which is
 

00:01:00.840 --> 00:01:03.349
add to it this thing lambda which is
called the regularization parameter say

00:01:03.349 --> 00:01:03.359
called the regularization parameter say
 

00:01:03.359 --> 00:01:05.000
called the regularization parameter say
more about them in a second but lambda

00:01:05.000 --> 00:01:05.010
more about them in a second but lambda
 

00:01:05.010 --> 00:01:10.070
more about them in a second but lambda
over 2m times the norm of W squared so

00:01:10.070 --> 00:01:10.080
over 2m times the norm of W squared so
 

00:01:10.080 --> 00:01:14.899
over 2m times the norm of W squared so
here the norm of W squared is just equal

00:01:14.899 --> 00:01:14.909
here the norm of W squared is just equal
 

00:01:14.909 --> 00:01:19.910
here the norm of W squared is just equal
to sum from J equals 1 to NX of WJ

00:01:19.910 --> 00:01:19.920
to sum from J equals 1 to NX of WJ
 

00:01:19.920 --> 00:01:22.789
to sum from J equals 1 to NX of WJ
squared or this can also be written W

00:01:22.789 --> 00:01:22.799
squared or this can also be written W
 

00:01:22.799 --> 00:01:24.980
squared or this can also be written W
transpose W is just a squared Euclidean

00:01:24.980 --> 00:01:24.990
transpose W is just a squared Euclidean
 

00:01:24.990 --> 00:01:27.890
transpose W is just a squared Euclidean
norm of the parameter vector W and this

00:01:27.890 --> 00:01:27.900
norm of the parameter vector W and this
 

00:01:27.900 --> 00:01:33.920
norm of the parameter vector W and this
is called um l2 regularization because

00:01:33.920 --> 00:01:33.930
is called um l2 regularization because
 

00:01:33.930 --> 00:01:35.810
is called um l2 regularization because
here you're using the Euclidean norm all

00:01:35.810 --> 00:01:35.820
here you're using the Euclidean norm all
 

00:01:35.820 --> 00:01:37.819
here you're using the Euclidean norm all
those how the l2 norm with the parameter

00:01:37.819 --> 00:01:37.829
those how the l2 norm with the parameter
 

00:01:37.829 --> 00:01:39.830
those how the l2 norm with the parameter
vector W now why do you regular eyes

00:01:39.830 --> 00:01:39.840
vector W now why do you regular eyes
 

00:01:39.840 --> 00:01:42.410
vector W now why do you regular eyes
just a parameter W why don't we add

00:01:42.410 --> 00:01:42.420
just a parameter W why don't we add
 

00:01:42.420 --> 00:01:46.219
just a parameter W why don't we add
something here you know about a B as

00:01:46.219 --> 00:01:46.229
something here you know about a B as
 

00:01:46.229 --> 00:01:49.190
something here you know about a B as
well in practice you could do this but I

00:01:49.190 --> 00:01:49.200
well in practice you could do this but I
 

00:01:49.200 --> 00:01:52.130
well in practice you could do this but I
usually just omit this because if a

00:01:52.130 --> 00:01:52.140
usually just omit this because if a
 

00:01:52.140 --> 00:01:54.859
usually just omit this because if a
nuclear parameters W is usually a pretty

00:01:54.859 --> 00:01:54.869
nuclear parameters W is usually a pretty
 

00:01:54.869 --> 00:01:56.990
nuclear parameters W is usually a pretty
high dimensional parameter vector

00:01:56.990 --> 00:01:57.000
high dimensional parameter vector
 

00:01:57.000 --> 00:01:59.289
high dimensional parameter vector
especially the high variance problem

00:01:59.289 --> 00:01:59.299
especially the high variance problem
 

00:01:59.299 --> 00:02:02.179
especially the high variance problem
maybe W just as a lot of parameters so

00:02:02.179 --> 00:02:02.189
maybe W just as a lot of parameters so
 

00:02:02.189 --> 00:02:03.440
maybe W just as a lot of parameters so
you aren't sitting all the crown's as

00:02:03.440 --> 00:02:03.450
you aren't sitting all the crown's as
 

00:02:03.450 --> 00:02:05.649
you aren't sitting all the crown's as
well whereas B is just a single number

00:02:05.649 --> 00:02:05.659
well whereas B is just a single number
 

00:02:05.659 --> 00:02:08.690
well whereas B is just a single number
so almost all the parameters are in W

00:02:08.690 --> 00:02:08.700
so almost all the parameters are in W
 

00:02:08.700 --> 00:02:11.330
so almost all the parameters are in W
rather than B and if you add this last

00:02:11.330 --> 00:02:11.340
rather than B and if you add this last
 

00:02:11.340 --> 00:02:12.410
rather than B and if you add this last
term in

00:02:12.410 --> 00:02:12.420
term in
 

00:02:12.420 --> 00:02:13.910
term in
actors won't make much of the difference

00:02:13.910 --> 00:02:13.920
actors won't make much of the difference
 

00:02:13.920 --> 00:02:16.100
actors won't make much of the difference
because B is just one parameter out of a

00:02:16.100 --> 00:02:16.110
because B is just one parameter out of a
 

00:02:16.110 --> 00:02:17.600
because B is just one parameter out of a
very large number of parameters in

00:02:17.600 --> 00:02:17.610
very large number of parameters in
 

00:02:17.610 --> 00:02:20.120
very large number of parameters in
practice I usually just don't bother to

00:02:20.120 --> 00:02:20.130
practice I usually just don't bother to
 

00:02:20.130 --> 00:02:22.610
practice I usually just don't bother to
include it but you can if you want

00:02:22.610 --> 00:02:22.620
include it but you can if you want
 

00:02:22.620 --> 00:02:25.250
include it but you can if you want
so l2 regularization is the most common

00:02:25.250 --> 00:02:25.260
so l2 regularization is the most common
 

00:02:25.260 --> 00:02:27.979
so l2 regularization is the most common
type of regularization you might have

00:02:27.979 --> 00:02:27.989
type of regularization you might have
 

00:02:27.989 --> 00:02:29.900
type of regularization you might have
also heard of something to talk about a

00:02:29.900 --> 00:02:29.910
also heard of something to talk about a
 

00:02:29.910 --> 00:02:34.100
also heard of something to talk about a
1 regularization and that's when you had

00:02:34.100 --> 00:02:34.110
1 regularization and that's when you had
 

00:02:34.110 --> 00:02:36.949
1 regularization and that's when you had
instead of this l 2-norm you instead as

00:02:36.949 --> 00:02:36.959
instead of this l 2-norm you instead as
 

00:02:36.959 --> 00:02:43.970
instead of this l 2-norm you instead as
a term that is lambda over m of sum over

00:02:43.970 --> 00:02:43.980
a term that is lambda over m of sum over
 

00:02:43.980 --> 00:02:47.180
a term that is lambda over m of sum over
of this and this is also called the l1

00:02:47.180 --> 00:02:47.190
of this and this is also called the l1
 

00:02:47.190 --> 00:02:50.539
of this and this is also called the l1
norm of the parameter vector W so it

00:02:50.539 --> 00:02:50.549
norm of the parameter vector W so it
 

00:02:50.549 --> 00:02:52.850
norm of the parameter vector W so it
will subscript one down there and then I

00:02:52.850 --> 00:02:52.860
will subscript one down there and then I
 

00:02:52.860 --> 00:02:55.039
will subscript one down there and then I
guess whether you put M or two m in the

00:02:55.039 --> 00:02:55.049
guess whether you put M or two m in the
 

00:02:55.049 --> 00:02:57.140
guess whether you put M or two m in the
denominator is just a scaling constant

00:02:57.140 --> 00:02:57.150
denominator is just a scaling constant
 

00:02:57.150 --> 00:03:00.860
denominator is just a scaling constant
if you use l1 regularization then W will

00:03:00.860 --> 00:03:00.870
if you use l1 regularization then W will
 

00:03:00.870 --> 00:03:03.559
if you use l1 regularization then W will
end up being sparse and what that means

00:03:03.559 --> 00:03:03.569
end up being sparse and what that means
 

00:03:03.569 --> 00:03:06.199
end up being sparse and what that means
is that the W vector will have a lot of

00:03:06.199 --> 00:03:06.209
is that the W vector will have a lot of
 

00:03:06.209 --> 00:03:08.900
is that the W vector will have a lot of
zeros in it and some people say that

00:03:08.900 --> 00:03:08.910
zeros in it and some people say that
 

00:03:08.910 --> 00:03:11.210
zeros in it and some people say that
this can help with compressing the model

00:03:11.210 --> 00:03:11.220
this can help with compressing the model
 

00:03:11.220 --> 00:03:13.130
this can help with compressing the model
because the certain parameters are 0

00:03:13.130 --> 00:03:13.140
because the certain parameters are 0
 

00:03:13.140 --> 00:03:15.530
because the certain parameters are 0
then you need less memory to store the

00:03:15.530 --> 00:03:15.540
then you need less memory to store the
 

00:03:15.540 --> 00:03:17.120
then you need less memory to store the
model although I find in practice

00:03:17.120 --> 00:03:17.130
model although I find in practice
 

00:03:17.130 --> 00:03:19.220
model although I find in practice
l1 regularization to make your model

00:03:19.220 --> 00:03:19.230
l1 regularization to make your model
 

00:03:19.230 --> 00:03:21.140
l1 regularization to make your model
small ourselves only a little bit so I

00:03:21.140 --> 00:03:21.150
small ourselves only a little bit so I
 

00:03:21.150 --> 00:03:23.030
small ourselves only a little bit so I
don't think they've used that much at

00:03:23.030 --> 00:03:23.040
don't think they've used that much at
 

00:03:23.040 --> 00:03:24.979
don't think they've used that much at
least not for the purpose of compressing

00:03:24.979 --> 00:03:24.989
least not for the purpose of compressing
 

00:03:24.989 --> 00:03:27.289
least not for the purpose of compressing
your model and when people train your

00:03:27.289 --> 00:03:27.299
your model and when people train your
 

00:03:27.299 --> 00:03:29.509
your model and when people train your
networks l2 regularization is just use

00:03:29.509 --> 00:03:29.519
networks l2 regularization is just use
 

00:03:29.519 --> 00:03:32.360
networks l2 regularization is just use
much much more often so sticking up so

00:03:32.360 --> 00:03:32.370
much much more often so sticking up so
 

00:03:32.370 --> 00:03:35.410
much much more often so sticking up so
the notation here so one not detail

00:03:35.410 --> 00:03:35.420
the notation here so one not detail
 

00:03:35.420 --> 00:03:41.170
the notation here so one not detail
lambda here is called the regularization

00:03:41.170 --> 00:03:41.180
 
 

00:03:41.180 --> 00:03:47.060
 
parameter and usually you set this using

00:03:47.060 --> 00:03:47.070
parameter and usually you set this using
 

00:03:47.070 --> 00:03:48.920
parameter and usually you set this using
your development set on using hold out

00:03:48.920 --> 00:03:48.930
your development set on using hold out
 

00:03:48.930 --> 00:03:50.630
your development set on using hold out
cross validation when you try to write

00:03:50.630 --> 00:03:50.640
cross validation when you try to write
 

00:03:50.640 --> 00:03:53.120
cross validation when you try to write
your values and see what does the best

00:03:53.120 --> 00:03:53.130
your values and see what does the best
 

00:03:53.130 --> 00:03:54.830
your values and see what does the best
in terms of trading off between doing

00:03:54.830 --> 00:03:54.840
in terms of trading off between doing
 

00:03:54.840 --> 00:03:57.680
in terms of trading off between doing
wrong your training set versus also

00:03:57.680 --> 00:03:57.690
wrong your training set versus also
 

00:03:57.690 --> 00:04:00.080
wrong your training set versus also
setting the two norm of your parameters

00:04:00.080 --> 00:04:00.090
setting the two norm of your parameters
 

00:04:00.090 --> 00:04:01.880
setting the two norm of your parameters
to be small which helps prevent

00:04:01.880 --> 00:04:01.890
to be small which helps prevent
 

00:04:01.890 --> 00:04:04.160
to be small which helps prevent
overfitting so long there is another

00:04:04.160 --> 00:04:04.170
overfitting so long there is another
 

00:04:04.170 --> 00:04:06.140
overfitting so long there is another
hyper parameter that you might have to

00:04:06.140 --> 00:04:06.150
hyper parameter that you might have to
 

00:04:06.150 --> 00:04:08.090
hyper parameter that you might have to
chew and by the way for the program

00:04:08.090 --> 00:04:08.100
chew and by the way for the program
 

00:04:08.100 --> 00:04:11.330
chew and by the way for the program
exercises lambda is a reserved keyword

00:04:11.330 --> 00:04:11.340
exercises lambda is a reserved keyword
 

00:04:11.340 --> 00:04:14.809
exercises lambda is a reserved keyword
in the Python programming language so in

00:04:14.809 --> 00:04:14.819
in the Python programming language so in
 

00:04:14.819 --> 00:04:18.860
in the Python programming language so in
the program exercise you will have OMB D

00:04:18.860 --> 00:04:18.870
the program exercise you will have OMB D
 

00:04:18.870 --> 00:04:21.380
the program exercise you will have OMB D
without the a so it's not to clash with

00:04:21.380 --> 00:04:21.390
without the a so it's not to clash with
 

00:04:21.390 --> 00:04:23.930
without the a so it's not to clash with
the reserved keyword in Python so we use

00:04:23.930 --> 00:04:23.940
the reserved keyword in Python so we use
 

00:04:23.940 --> 00:04:24.950
the reserved keyword in Python so we use
our MD

00:04:24.950 --> 00:04:24.960
our MD
 

00:04:24.960 --> 00:04:26.870
our MD
to represent the lambda regularization

00:04:26.870 --> 00:04:26.880
to represent the lambda regularization
 

00:04:26.880 --> 00:04:30.170
to represent the lambda regularization
parameter so this is how you implement

00:04:30.170 --> 00:04:30.180
parameter so this is how you implement
 

00:04:30.180 --> 00:04:32.210
parameter so this is how you implement
l2 regularization for logistic

00:04:32.210 --> 00:04:32.220
l2 regularization for logistic
 

00:04:32.220 --> 00:04:34.370
l2 regularization for logistic
regression how about the neural network

00:04:34.370 --> 00:04:34.380
regression how about the neural network
 

00:04:34.380 --> 00:04:37.249
regression how about the neural network
in a neural network you have a cost

00:04:37.249 --> 00:04:37.259
in a neural network you have a cost
 

00:04:37.259 --> 00:04:39.710
in a neural network you have a cost
function there's a function of all of

00:04:39.710 --> 00:04:39.720
function there's a function of all of
 

00:04:39.720 --> 00:04:44.510
function there's a function of all of
your parameters W 1 V 1 2 W capital L B

00:04:44.510 --> 00:04:44.520
your parameters W 1 V 1 2 W capital L B
 

00:04:44.520 --> 00:04:46.309
your parameters W 1 V 1 2 W capital L B
capital L with capital L is the number

00:04:46.309 --> 00:04:46.319
capital L with capital L is the number
 

00:04:46.319 --> 00:04:49.460
capital L with capital L is the number
of layers in your network and so the

00:04:49.460 --> 00:04:49.470
of layers in your network and so the
 

00:04:49.470 --> 00:04:53.260
of layers in your network and so the
cost function is this some of the losses

00:04:53.260 --> 00:04:53.270
cost function is this some of the losses
 

00:04:53.270 --> 00:04:57.490
cost function is this some of the losses
summed over your M training examples and

00:04:57.490 --> 00:04:57.500
summed over your M training examples and
 

00:04:57.500 --> 00:05:00.499
summed over your M training examples and
so it's a dragon ization you add lambda

00:05:00.499 --> 00:05:00.509
so it's a dragon ization you add lambda
 

00:05:00.509 --> 00:05:06.400
so it's a dragon ization you add lambda
over 2m of sum over all of your

00:05:06.400 --> 00:05:06.410
over 2m of sum over all of your
 

00:05:06.410 --> 00:05:10.159
over 2m of sum over all of your
parameter W your parameter matrix of W

00:05:10.159 --> 00:05:10.169
parameter W your parameter matrix of W
 

00:05:10.169 --> 00:05:14.080
parameter W your parameter matrix of W
of there let's call the squared norm

00:05:14.080 --> 00:05:14.090
of there let's call the squared norm
 

00:05:14.090 --> 00:05:19.249
of there let's call the squared norm
where this norm of a matrix really the

00:05:19.249 --> 00:05:19.259
where this norm of a matrix really the
 

00:05:19.259 --> 00:05:21.610
where this norm of a matrix really the
squared norm is defined as the sum of I

00:05:21.610 --> 00:05:21.620
squared norm is defined as the sum of I
 

00:05:21.620 --> 00:05:24.980
squared norm is defined as the sum of I
sum of a J of you know each of the

00:05:24.980 --> 00:05:24.990
sum of a J of you know each of the
 

00:05:24.990 --> 00:05:29.390
sum of a J of you know each of the
elements of that matrix squared and if

00:05:29.390 --> 00:05:29.400
elements of that matrix squared and if
 

00:05:29.400 --> 00:05:30.980
elements of that matrix squared and if
you want the indices of the summation

00:05:30.980 --> 00:05:30.990
you want the indices of the summation
 

00:05:30.990 --> 00:05:34.040
you want the indices of the summation
this is sum from I equals 1 to n L minus

00:05:34.040 --> 00:05:34.050
this is sum from I equals 1 to n L minus
 

00:05:34.050 --> 00:05:36.920
this is sum from I equals 1 to n L minus
1 sum from J equals 1 through n L

00:05:36.920 --> 00:05:36.930
1 sum from J equals 1 through n L
 

00:05:36.930 --> 00:05:43.459
1 sum from J equals 1 through n L
because W is a n L minus 1 by n L

00:05:43.459 --> 00:05:43.469
because W is a n L minus 1 by n L
 

00:05:43.469 --> 00:05:45.649
because W is a n L minus 1 by n L
dimensional matrix where these are the

00:05:45.649 --> 00:05:45.659
dimensional matrix where these are the
 

00:05:45.659 --> 00:05:47.689
dimensional matrix where these are the
number of hidden units or number of

00:05:47.689 --> 00:05:47.699
number of hidden units or number of
 

00:05:47.699 --> 00:05:49.760
number of hidden units or number of
units and layers L minus 1 and they are

00:05:49.760 --> 00:05:49.770
units and layers L minus 1 and they are
 

00:05:49.770 --> 00:05:54.320
units and layers L minus 1 and they are
L so this matrix norm it turns out is

00:05:54.320 --> 00:05:54.330
L so this matrix norm it turns out is
 

00:05:54.330 --> 00:05:59.170
L so this matrix norm it turns out is
called the Frobenius norm of a matrix

00:05:59.170 --> 00:05:59.180
called the Frobenius norm of a matrix
 

00:05:59.180 --> 00:06:03.980
called the Frobenius norm of a matrix
denoted with f in the subscript so for

00:06:03.980 --> 00:06:03.990
denoted with f in the subscript so for
 

00:06:03.990 --> 00:06:06.920
denoted with f in the subscript so for
arcane linear algebra technical reasons

00:06:06.920 --> 00:06:06.930
arcane linear algebra technical reasons
 

00:06:06.930 --> 00:06:09.649
arcane linear algebra technical reasons
this is not called the you know l2 norm

00:06:09.649 --> 00:06:09.659
this is not called the you know l2 norm
 

00:06:09.659 --> 00:06:11.810
this is not called the you know l2 norm
of a matrix instead is called the

00:06:11.810 --> 00:06:11.820
of a matrix instead is called the
 

00:06:11.820 --> 00:06:14.779
of a matrix instead is called the
Frobenius norm of a matrix i know it

00:06:14.779 --> 00:06:14.789
Frobenius norm of a matrix i know it
 

00:06:14.789 --> 00:06:15.950
Frobenius norm of a matrix i know it
sounds like giving one edge which is

00:06:15.950 --> 00:06:15.960
sounds like giving one edge which is
 

00:06:15.960 --> 00:06:18.770
sounds like giving one edge which is
called l2 normal matrix but for really

00:06:18.770 --> 00:06:18.780
called l2 normal matrix but for really
 

00:06:18.780 --> 00:06:20.779
called l2 normal matrix but for really
are ten reasons that you don't need to

00:06:20.779 --> 00:06:20.789
are ten reasons that you don't need to
 

00:06:20.789 --> 00:06:23.060
are ten reasons that you don't need to
know by convention this is called the

00:06:23.060 --> 00:06:23.070
know by convention this is called the
 

00:06:23.070 --> 00:06:24.709
know by convention this is called the
Frobenius long it just means the sum of

00:06:24.709 --> 00:06:24.719
Frobenius long it just means the sum of
 

00:06:24.719 --> 00:06:28.159
Frobenius long it just means the sum of
squared of elements of a matrix so how

00:06:28.159 --> 00:06:28.169
squared of elements of a matrix so how
 

00:06:28.169 --> 00:06:29.810
squared of elements of a matrix so how
do you in turn gradient descent of this

00:06:29.810 --> 00:06:29.820
do you in turn gradient descent of this
 

00:06:29.820 --> 00:06:33.620
do you in turn gradient descent of this
previously we would compute G W you know

00:06:33.620 --> 00:06:33.630
previously we would compute G W you know
 

00:06:33.630 --> 00:06:36.799
previously we would compute G W you know
using back drop

00:06:36.799 --> 00:06:36.809
 
 

00:06:36.809 --> 00:06:40.260
 
we're back properly give us the partial

00:06:40.260 --> 00:06:40.270
we're back properly give us the partial
 

00:06:40.270 --> 00:06:43.350
we're back properly give us the partial
derivative of J with respect to W will

00:06:43.350 --> 00:06:43.360
derivative of J with respect to W will
 

00:06:43.360 --> 00:06:46.889
derivative of J with respect to W will
really W for any given L and then you

00:06:46.889 --> 00:06:46.899
really W for any given L and then you
 

00:06:46.899 --> 00:06:51.480
really W for any given L and then you
update WL as WL minus the learning rate

00:06:51.480 --> 00:06:51.490
update WL as WL minus the learning rate
 

00:06:51.490 --> 00:06:54.510
update WL as WL minus the learning rate
times D so this was before we added this

00:06:54.510 --> 00:06:54.520
times D so this was before we added this
 

00:06:54.520 --> 00:06:56.820
times D so this was before we added this
extra regularization term to the

00:06:56.820 --> 00:06:56.830
extra regularization term to the
 

00:06:56.830 --> 00:06:58.950
extra regularization term to the
objective now there is added this

00:06:58.950 --> 00:06:58.960
objective now there is added this
 

00:06:58.960 --> 00:07:00.959
objective now there is added this
regularization term the objective what

00:07:00.959 --> 00:07:00.969
regularization term the objective what
 

00:07:00.969 --> 00:07:03.480
regularization term the objective what
you do is you take DW and you add to it

00:07:03.480 --> 00:07:03.490
you do is you take DW and you add to it
 

00:07:03.490 --> 00:07:08.070
you do is you take DW and you add to it
lambda over m times w and then you just

00:07:08.070 --> 00:07:08.080
lambda over m times w and then you just
 

00:07:08.080 --> 00:07:10.469
lambda over m times w and then you just
compute this update same as before and

00:07:10.469 --> 00:07:10.479
compute this update same as before and
 

00:07:10.479 --> 00:07:12.359
compute this update same as before and
it turns out that with this new

00:07:12.359 --> 00:07:12.369
it turns out that with this new
 

00:07:12.369 --> 00:07:16.350
it turns out that with this new
definition of DW l this is still um you

00:07:16.350 --> 00:07:16.360
definition of DW l this is still um you
 

00:07:16.360 --> 00:07:20.699
definition of DW l this is still um you
know d this new DW l is still a correct

00:07:20.699 --> 00:07:20.709
know d this new DW l is still a correct
 

00:07:20.709 --> 00:07:22.260
know d this new DW l is still a correct
definition of the derivative of your

00:07:22.260 --> 00:07:22.270
definition of the derivative of your
 

00:07:22.270 --> 00:07:24.089
definition of the derivative of your
cost function with respect to your

00:07:24.089 --> 00:07:24.099
cost function with respect to your
 

00:07:24.099 --> 00:07:26.399
cost function with respect to your
parameters now they've added the extra

00:07:26.399 --> 00:07:26.409
parameters now they've added the extra
 

00:07:26.409 --> 00:07:29.549
parameters now they've added the extra
regularization term at the end and it's

00:07:29.549 --> 00:07:29.559
regularization term at the end and it's
 

00:07:29.559 --> 00:07:31.559
regularization term at the end and it's
for this reason that l2 regularization

00:07:31.559 --> 00:07:31.569
for this reason that l2 regularization
 

00:07:31.569 --> 00:07:36.749
for this reason that l2 regularization
is sometimes also called weight decay so

00:07:36.749 --> 00:07:36.759
is sometimes also called weight decay so
 

00:07:36.759 --> 00:07:41.100
is sometimes also called weight decay so
if i take this definition of yo DWO and

00:07:41.100 --> 00:07:41.110
if i take this definition of yo DWO and
 

00:07:41.110 --> 00:07:43.619
if i take this definition of yo DWO and
just plug it in here then you see that

00:07:43.619 --> 00:07:43.629
just plug it in here then you see that
 

00:07:43.629 --> 00:07:46.860
just plug it in here then you see that
the update is WL gives updated as WL

00:07:46.860 --> 00:07:46.870
the update is WL gives updated as WL
 

00:07:46.870 --> 00:07:49.199
the update is WL gives updated as WL
times the learning rate alpha at times

00:07:49.199 --> 00:07:49.209
times the learning rate alpha at times
 

00:07:49.209 --> 00:07:56.179
times the learning rate alpha at times
you know the thing from backdrop um plus

00:07:56.179 --> 00:07:56.189
you know the thing from backdrop um plus
 

00:07:56.189 --> 00:08:03.420
you know the thing from backdrop um plus
lambda over m times WL this with a minus

00:08:03.420 --> 00:08:03.430
lambda over m times WL this with a minus
 

00:08:03.430 --> 00:08:08.239
lambda over m times WL this with a minus
sign here and so this is equal to WL

00:08:08.239 --> 00:08:08.249
sign here and so this is equal to WL
 

00:08:08.249 --> 00:08:13.679
sign here and so this is equal to WL
minus alpha lambda over m times WL minus

00:08:13.679 --> 00:08:13.689
minus alpha lambda over m times WL minus
 

00:08:13.689 --> 00:08:16.019
minus alpha lambda over m times WL minus
alpha at times the only thing you got

00:08:16.019 --> 00:08:16.029
alpha at times the only thing you got
 

00:08:16.029 --> 00:08:20.429
alpha at times the only thing you got
from backdrop and so this term shows

00:08:20.429 --> 00:08:20.439
from backdrop and so this term shows
 

00:08:20.439 --> 00:08:23.040
from backdrop and so this term shows
that whatever the matrix WL is you're

00:08:23.040 --> 00:08:23.050
that whatever the matrix WL is you're
 

00:08:23.050 --> 00:08:24.809
that whatever the matrix WL is you're
going to make it a little bit smaller

00:08:24.809 --> 00:08:24.819
going to make it a little bit smaller
 

00:08:24.819 --> 00:08:27.179
going to make it a little bit smaller
right this is actually as if you're

00:08:27.179 --> 00:08:27.189
right this is actually as if you're
 

00:08:27.189 --> 00:08:28.949
right this is actually as if you're
taking the matrix W and you're

00:08:28.949 --> 00:08:28.959
taking the matrix W and you're
 

00:08:28.959 --> 00:08:31.589
taking the matrix W and you're
multiplying it by 1 minus alpha lambda

00:08:31.589 --> 00:08:31.599
multiplying it by 1 minus alpha lambda
 

00:08:31.599 --> 00:08:34.469
multiplying it by 1 minus alpha lambda
over m you're really taking the matrix W

00:08:34.469 --> 00:08:34.479
over m you're really taking the matrix W
 

00:08:34.479 --> 00:08:37.350
over m you're really taking the matrix W
and subtracting alpha lambda over m

00:08:37.350 --> 00:08:37.360
and subtracting alpha lambda over m
 

00:08:37.360 --> 00:08:38.730
and subtracting alpha lambda over m
times it seems like you're multiplying

00:08:38.730 --> 00:08:38.740
times it seems like you're multiplying
 

00:08:38.740 --> 00:08:41.279
times it seems like you're multiplying
to make the matrix W by this number

00:08:41.279 --> 00:08:41.289
to make the matrix W by this number
 

00:08:41.289 --> 00:08:42.329
to make the matrix W by this number
which is going to be a little bit less

00:08:42.329 --> 00:08:42.339
which is going to be a little bit less
 

00:08:42.339 --> 00:08:45.260
which is going to be a little bit less
than 1 so this is why

00:08:45.260 --> 00:08:45.270
than 1 so this is why
 

00:08:45.270 --> 00:08:47.780
than 1 so this is why
Oh - norm regularization is also called

00:08:47.780 --> 00:08:47.790
Oh - norm regularization is also called
 

00:08:47.790 --> 00:08:51.139
Oh - norm regularization is also called
weight decay because it's just like the

00:08:51.139 --> 00:08:51.149
weight decay because it's just like the
 

00:08:51.149 --> 00:08:53.060
weight decay because it's just like the
ordinary gradient descent where you

00:08:53.060 --> 00:08:53.070
ordinary gradient descent where you
 

00:08:53.070 --> 00:08:56.720
ordinary gradient descent where you
update W by subtracting alpha times the

00:08:56.720 --> 00:08:56.730
update W by subtracting alpha times the
 

00:08:56.730 --> 00:08:59.210
update W by subtracting alpha times the
original gradient gone from backdrop but

00:08:59.210 --> 00:08:59.220
original gradient gone from backdrop but
 

00:08:59.220 --> 00:09:04.100
original gradient gone from backdrop but
now you're also you know multiplying W

00:09:04.100 --> 00:09:04.110
now you're also you know multiplying W
 

00:09:04.110 --> 00:09:06.470
now you're also you know multiplying W
binds in some by this thing which is

00:09:06.470 --> 00:09:06.480
binds in some by this thing which is
 

00:09:06.480 --> 00:09:08.600
binds in some by this thing which is
more than less than one so the

00:09:08.600 --> 00:09:08.610
more than less than one so the
 

00:09:08.610 --> 00:09:10.100
more than less than one so the
alternative name for l2 regularization

00:09:10.100 --> 00:09:10.110
alternative name for l2 regularization
 

00:09:10.110 --> 00:09:12.500
alternative name for l2 regularization
is waiting K I'm not really going to use

00:09:12.500 --> 00:09:12.510
is waiting K I'm not really going to use
 

00:09:12.510 --> 00:09:16.040
is waiting K I'm not really going to use
that name but the intuition for why it's

00:09:16.040 --> 00:09:16.050
that name but the intuition for why it's
 

00:09:16.050 --> 00:09:18.290
that name but the intuition for why it's
called weight decay is that this first

00:09:18.290 --> 00:09:18.300
called weight decay is that this first
 

00:09:18.300 --> 00:09:21.260
called weight decay is that this first
term here is equal to this so you're

00:09:21.260 --> 00:09:21.270
term here is equal to this so you're
 

00:09:21.270 --> 00:09:23.300
term here is equal to this so you're
just multiplying the weight matrix by a

00:09:23.300 --> 00:09:23.310
just multiplying the weight matrix by a
 

00:09:23.310 --> 00:09:25.910
just multiplying the weight matrix by a
number slightly less than one so that's

00:09:25.910 --> 00:09:25.920
number slightly less than one so that's
 

00:09:25.920 --> 00:09:28.100
number slightly less than one so that's
how you influence l2 regularization in

00:09:28.100 --> 00:09:28.110
how you influence l2 regularization in
 

00:09:28.110 --> 00:09:30.199
how you influence l2 regularization in
your network now one question that

00:09:30.199 --> 00:09:30.209
your network now one question that
 

00:09:30.209 --> 00:09:32.240
your network now one question that
Pierce doctors asked me is you know hey

00:09:32.240 --> 00:09:32.250
Pierce doctors asked me is you know hey
 

00:09:32.250 --> 00:09:32.690
Pierce doctors asked me is you know hey
Andrew

00:09:32.690 --> 00:09:32.700
Andrew
 

00:09:32.700 --> 00:09:34.610
Andrew
why does regularization prevent

00:09:34.610 --> 00:09:34.620
why does regularization prevent
 

00:09:34.620 --> 00:09:36.500
why does regularization prevent
overfitting let's take a quick look at

00:09:36.500 --> 00:09:36.510
overfitting let's take a quick look at
 

00:09:36.510 --> 00:09:38.210
overfitting let's take a quick look at
the next video or maintain some

00:09:38.210 --> 00:09:38.220
the next video or maintain some
 

00:09:38.220 --> 00:09:40.060
the next video or maintain some
intuition for how regularization

00:09:40.060 --> 00:09:40.070
intuition for how regularization
 

00:09:40.070 --> 00:09:43.610
intuition for how regularization
prevents overfitting

