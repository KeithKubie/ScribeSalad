WEBVTT
Kind: captions
Language: en

00:00:00.399 --> 00:00:01.399
Hi everyone!

00:00:01.399 --> 00:00:03.630
Welcome to the new Seeker Elements set.

00:00:03.630 --> 00:00:07.759
We’re still going to be covering all of
the mind bogglingly awesome discoveries in

00:00:07.759 --> 00:00:11.669
the science and tech world, we’ve just got
a fun new way to do it.

00:00:11.669 --> 00:00:15.539
And what better way to kick things off then
with an update from the field of machine learning

00:00:15.539 --> 00:00:16.960
and robotics.

00:00:16.960 --> 00:00:21.789
If you’ve ever seen a baby kick and squirm
involuntarily or watched a small child clumsily

00:00:21.789 --> 00:00:26.429
try to do a complex physical task, you know
that it takes humans years to develop fine

00:00:26.429 --> 00:00:27.900
motor skills.

00:00:27.900 --> 00:00:33.840
So it makes sense that intuitive, fluid motions
are hard to get robots to do on their own...up

00:00:33.840 --> 00:00:35.100
until now.

00:00:35.100 --> 00:00:38.500
And get this--the robots are teaching themselves.

00:00:38.500 --> 00:00:42.850
A new robotic demonstration from a company
called Open AI uses something called machine

00:00:42.850 --> 00:00:48.080
learning, specifically a neural network, to
allow this robotic hand to perform a complicated

00:00:48.080 --> 00:00:51.290
series of independent object manipulations.

00:00:51.290 --> 00:00:53.420
That means the motions you’re seeing here?

00:00:53.420 --> 00:00:58.611
The robot is doing that by itself, without
any input from or control by a human, and

00:00:58.611 --> 00:01:02.080
without any DIRECT programming to perform
each action.

00:01:02.080 --> 00:01:05.030
But first--what is machine learning?

00:01:05.030 --> 00:01:07.340
Machine learning is a subset of artificial
intelligence.

00:01:07.340 --> 00:01:12.010
It’s getting computers to perform tasks
without being explicitly programmed to do

00:01:12.010 --> 00:01:13.010
them.

00:01:13.010 --> 00:01:17.440
Take one of the most advanced robots we have
today, a robot that helps us perform surgeries.

00:01:17.440 --> 00:01:21.440
This is a traditionally programmed robot,
that has to be explicitly told what to do

00:01:21.440 --> 00:01:22.750
every time.

00:01:22.750 --> 00:01:27.160
The programmer has to write: “if this happens,
the machine will do that”, for every step

00:01:27.160 --> 00:01:28.810
of that robot’s action.

00:01:28.810 --> 00:01:32.990
For tasks where that would be prohibitively
time-intensive, machine learning algorithms

00:01:32.990 --> 00:01:35.000
can be used instead.

00:01:35.000 --> 00:01:39.390
These are algorithms that you can expose to
vast quantities of data, from which they can

00:01:39.390 --> 00:01:42.560
‘learn’ certain criteria and identify
patterns.

00:01:42.560 --> 00:01:46.880
So how is this applied in something like the
robotic hand from Open AI?

00:01:46.880 --> 00:01:50.980
In this situation the main data sets are all
the different positions of the hand and the

00:01:50.980 --> 00:01:51.980
block.

00:01:51.980 --> 00:01:56.160
But the combination of all of these possibilities
gives us way too many options for the robot

00:01:56.160 --> 00:01:59.720
to practice in real life so that it can ‘learn’
each one.

00:01:59.720 --> 00:02:04.860
So instead, the researchers used a massive
amount of computing power to simulate training

00:02:04.860 --> 00:02:10.099
the hand --they designed a virtual space in
which the robot could experience myriad hand

00:02:10.099 --> 00:02:14.700
and block positions at an accelerated pace
inside a computer model.

00:02:14.700 --> 00:02:19.180
The team estimates they exposed the robotic
hand to about 100 years of trial and error

00:02:19.180 --> 00:02:24.969
experience in just 50 hours of simulation
. In addition to letting the hand ‘practice’,

00:02:24.969 --> 00:02:29.440
the researchers also randomized some aspects
of the simulation, variables like the size

00:02:29.440 --> 00:02:32.840
and slipperiness of the block and even the
force of gravity.

00:02:32.840 --> 00:02:37.150
While the simulation couldn’t reproduce
everything the robot would encounter when

00:02:37.150 --> 00:02:41.529
handling the cube in the ‘real world’,
these variables made it more likely that the

00:02:41.529 --> 00:02:44.939
simulation practice would be useful in real-world
conditions.

00:02:44.939 --> 00:02:49.310
Because of all the variables involved, the
research team used a kind of machine learning

00:02:49.310 --> 00:02:54.620
algorithm with a ‘memory’ --based loosely
on the way a human memory would work--making

00:02:54.620 --> 00:02:59.459
this particular algorithm a neural network:
a kind of machine learning loosely based on

00:02:59.459 --> 00:03:01.609
the human brain and its logic structures.

00:03:01.609 --> 00:03:07.019
They then transferred all of this ‘learned’
information to the real-life robotic hand,

00:03:07.019 --> 00:03:11.840
which is equipped with a set of cameras that
can estimate the object’s position and orientation.

00:03:11.840 --> 00:03:13.230
The end result?

00:03:13.230 --> 00:03:18.340
Simply ask the hand to manipulate the object
in a certain way--say, to reorient the block

00:03:18.340 --> 00:03:21.249
with its purple side up--and it’ll do it.

00:03:21.249 --> 00:03:26.110
You ask for an outcome, and the robot can
provide that with no further input from you

00:03:26.110 --> 00:03:29.519
because it taught itself the series of motions
it needed to get there.

00:03:29.519 --> 00:03:34.870
As you can see*, this robot developed motions
you may recognize from your own hands, just

00:03:34.870 --> 00:03:39.019
by learning what motions were most efficient
and effective at moving the block without

00:03:39.019 --> 00:03:40.430
dropping it just with input from visual stimuli
and joint sensors on each hand.

00:03:40.430 --> 00:03:44.230
Other experts in the field of robotics and
machine learning state that while this example

00:03:44.230 --> 00:03:48.280
is exciting and comparatively elegant, it’s
not necessarily new.

00:03:48.280 --> 00:03:53.120
It’s also still quite limited to an object
of convenient size in a hand that’s facing

00:03:53.120 --> 00:03:57.409
up, so it’s not dealing with as many challenges
as a machine-learning robot being asked to

00:03:57.409 --> 00:04:01.290
complete a task that would be useful in say,
an assembly line.

00:04:01.290 --> 00:04:06.569
So while robotics is still catching up to
human capabilities, it is making strides.

00:04:06.569 --> 00:04:10.909
This work from Open AI shows us that developing
machine learning algorithms and neural networks

00:04:10.909 --> 00:04:15.290
could help us make more precise and dexterous
robots, that not only help us with the things

00:04:15.290 --> 00:04:20.349
we don’t want to do or can’t do--but teach
themselves how to do it.

00:04:20.349 --> 00:04:24.780
For more on AI, subscribe to Seeker, and check
out this video here about how AI is being

00:04:24.780 --> 00:04:27.680
used in the real world to monitor your data.

00:04:27.680 --> 00:04:32.370
And fun fact, some machine learning algorithms
are also teaching themselves as they go.

00:04:32.370 --> 00:04:36.340
As they complete the task they taught themselves
to do, they’re learning from their mistakes

00:04:36.340 --> 00:04:40.840
and evolving their own algorithm to make it
more accurate--with some small guidance from

00:04:40.840 --> 00:04:41.840
human programmers.

00:04:41.840 --> 00:04:42.980
Thanks for watching Seeker.

