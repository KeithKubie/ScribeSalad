WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.810
 this capsule we see a last type 

00:00:02.129 --> 00:00:06.359
 of learning algorithms by 

00:00:03.810 --> 00:00:10.080
 active reinforcement either the search for 

00:00:06.359 --> 00:00:12.389
 plans then all the methods we have 

00:00:10.080 --> 00:00:15.660
 seen until now will deliver 

00:00:12.389 --> 00:00:17.550
 their plans or policies that 

00:00:15.660 --> 00:00:20.189
 want to run in the environment 

00:00:17.550 --> 00:00:21.630
 which is an estimate of the 

00:00:20.189 --> 00:00:23.160
 optimal policy they always go the 

00:00:21.630 --> 00:00:25.470
 drift and from a function of 

00:00:23.160 --> 00:00:29.130
 values ​​or an action function worth 

00:00:25.470 --> 00:00:33.149
 so always at some point then of 

00:00:29.130 --> 00:00:37.710
 l it's equal to something like 

00:00:33.149 --> 00:00:43.680
 max argue of a function that will 

00:00:37.710 --> 00:00:49.020
 depend on either v2 s or q2 is 

00:00:43.680 --> 00:00:51.120
 this for example and so in fact all 

00:00:49.020 --> 00:00:54.090
 these methods the good all use this 

00:00:51.120 --> 00:00:57.270
 kind of how to choose what is the 

00:00:54.090 --> 00:00:58.739
 decision to make and are really going 

00:00:57.270 --> 00:01:00.870
 just deferred to comment on what they 

00:00:58.739 --> 00:01:02.629
 will estimate final either the function of 

00:01:00.870 --> 00:01:06.119
 the time when the value action function 

00:01:02.629 --> 00:01:08.159
 no other idea it would be to optimize 

00:01:06.119 --> 00:01:12.510
 directly in fact the full so of 

00:01:08.159 --> 00:01:14.369
 keep in mind a current plan then 

00:01:12.510 --> 00:01:17.729
 to make changes for 

00:01:14.369 --> 00:01:20.970
 get a new bonus plan followed 

00:01:17.729 --> 00:01:23.700
 of a new plan worse premiums premium a 

00:01:20.970 --> 00:01:25.020
 so actually locates having dug up on 

00:01:23.700 --> 00:01:29.490
 the possible values ​​of a plan in 

00:01:25.020 --> 00:01:32.430
 making some modifications here this 

00:01:29.490 --> 00:01:35.970
 we will be looking for plans 

00:01:32.430 --> 00:01:37.979
 so be an initial state s 0 which 

00:01:35.970 --> 00:01:41.909
 we are interested in actually finding the 

00:01:37.979 --> 00:01:44.369
 political py that maximizes the sum of 

00:01:41.909 --> 00:01:47.610
 rewards future hope when I 

00:01:44.369 --> 00:01:50.520
 start the simulation in 0 for example 

00:01:47.610 --> 00:01:52.380
 if I want to optimize a strategy in 

00:01:50.520 --> 00:01:55.770
 the frame of a game rock Tunisian ma 

00:01:52.380 --> 00:01:58.140
 strategy in order to have the best 

00:01:55.770 --> 00:02:00.570
 are future rewards expected at 

00:01:58.140 --> 00:02:01.560
 from the beginning of the part ok so 

00:02:00.570 --> 00:02:04.979
 it's true what interests me 

00:02:01.560 --> 00:02:06.439
 optimize now so what I 

00:02:04.979 --> 00:02:09.030
 could actually do it's just 

00:02:06.439 --> 00:02:11.730
 directly optimized this policy 

00:02:09.030 --> 00:02:12.040
 pilat by making changes then 

00:02:11.730 --> 00:02:13.989
 1 

00:02:12.040 --> 00:02:16.870
 it's about improving what's the value 

00:02:13.989 --> 00:02:18.609
 associated this policy here again a 

00:02:16.870 --> 00:02:20.019
 strong footprint a learning not 

00:02:18.609 --> 00:02:21.120
 strengthening so we do not know our 

00:02:20.019 --> 00:02:23.859
 transition model 

00:02:21.120 --> 00:02:25.900
 so we can not calculate directly 

00:02:23.859 --> 00:02:27.640
 view of the mind but by solving the 

00:02:25.900 --> 00:02:30.010
 system of equations for different 

00:02:27.640 --> 00:02:30.549
 besides since then optimizing the 

00:02:30.010 --> 00:02:32.769
 policy 

00:02:30.549 --> 00:02:34.629
 a bit like we do it in iteration by 

00:02:32.769 --> 00:02:38.889
 politics so 

00:02:34.629 --> 00:02:42.760
 but for each test we go 

00:02:38.889 --> 00:02:45.480
 have a stochastic estimate of 

00:02:42.760 --> 00:02:47.590
 calais a possible value of 

00:02:45.480 --> 00:02:52.959
 rewards the sum of rewards 

00:02:47.590 --> 00:02:54.609
 if I'm a politician who gave and 

00:02:52.959 --> 00:02:59.500
 that I'm doing a situation I'm doing a 

00:02:54.609 --> 00:03:01.349
 try a search by policy 

00:02:59.500 --> 00:03:04.299
 what we are going to do is directly 

00:03:01.349 --> 00:03:06.629
 optimized from against against 

00:03:04.299 --> 00:03:09.010
 results of stochastic simulations 

00:03:06.629 --> 00:03:12.189
 politics then so that's a little 

00:03:09.010 --> 00:03:15.180
 as an alternative a variant 

00:03:12.189 --> 00:03:17.290
 learning not reinforcement of 

00:03:15.180 --> 00:03:19.359
 the political iteration we saw 

00:03:17.290 --> 00:03:22.959
 in the decision process mark 

00:03:19.359 --> 00:03:23.650
 well in the case where we know the model 

00:03:22.959 --> 00:03:25.569
 of transition 

00:03:23.650 --> 00:03:27.669
 so in this case also it is the variant 

00:03:25.569 --> 00:03:28.389
 finally the nation by politics but 

00:03:27.669 --> 00:03:30.430
 in case 

00:03:28.389 --> 00:03:32.799
 in fact we do not know in the end 

00:03:30.430 --> 00:03:34.629
 our transition model and it's this 

00:03:32.799 --> 00:03:38.639
 we are going to call the search for plans 

00:03:34.629 --> 00:03:38.639
 or political or English not so sure 

00:03:39.900 --> 00:03:45.609
 for example if we return an example of 

00:03:43.540 --> 00:03:49.000
 of environment of a grid 3 by 

00:03:45.609 --> 00:03:51.250
 four ben if we do a simulation 

00:03:49.000 --> 00:03:56.280
 for a policy that gives and we will 

00:03:51.250 --> 00:03:58.989
 observe a reinforcement sequence 

00:03:56.280 --> 00:04:04.180
 can that good blowjob is that vida 

00:03:58.989 --> 00:04:07.269
 es 0 it's going to be its value is going to be 

00:04:04.180 --> 00:04:08.859
 relatively close to 0.72 so the 

00:04:07.269 --> 00:04:09.939
 sum of all these reinforcements to see there 

00:04:08.859 --> 00:04:12.579
 some variation 

00:04:09.939 --> 00:04:14.949
 but since that's one of the results 

00:04:12.579 --> 00:04:17.039
 possible v20 is going to be anyway 

00:04:14.949 --> 00:04:19.750
 relatively close to this value 

00:04:17.039 --> 00:04:21.459
 then a search approach of 

00:04:19.750 --> 00:04:23.050
 politics we will see here that the sum 

00:04:21.459 --> 00:04:24.930
 we will see who is very simple if you 

00:04:23.050 --> 00:04:28.580
 make an approach by el campo 

00:04:24.930 --> 00:04:29.880
 we will actually use research 

00:04:28.580 --> 00:04:32.070
 local 

00:04:29.880 --> 00:04:33.539
 we will convert the problem of doing 

00:04:32.070 --> 00:04:36.300
 learning by inevitably as a 

00:04:33.539 --> 00:04:37.289
 local search problem is this 

00:04:36.300 --> 00:04:39.990
 that we could do is for a 

00:04:37.289 --> 00:04:42.419
 number of iterations we're going 

00:04:39.990 --> 00:04:44.729
 take our current policy picon 

00:04:42.419 --> 00:04:47.580
 will initialize in a certain way so 

00:04:44.729 --> 00:04:53.550
 here would possibly have the first 

00:04:47.580 --> 00:04:56.310
 handle initialize civilized then 

00:04:53.550 --> 00:04:57.780
 maybe randomly is there at 

00:04:56.310 --> 00:04:58.800
 each iteration will take our 

00:04:57.780 --> 00:05:00.840
 current policy 

00:04:58.800 --> 00:05:03.090
 we will do a test so a simulation 

00:05:00.840 --> 00:05:07.199
 in the environment and then it goes us 

00:05:03.090 --> 00:05:09.690
 give an estimate of the value for 

00:05:07.199 --> 00:05:11.160
 this policy there in zero because I 

00:05:09.690 --> 00:05:12.210
 do and I'm just going to cumulate to 

00:05:11.160 --> 00:05:18.509
 from the beginning 

00:05:12.210 --> 00:05:20.759
 all reinforcements until the end and 

00:05:18.509 --> 00:05:23.220
 then fir trees for each policy 

00:05:20.759 --> 00:05:25.110
 neighboring of a successor policy of 

00:05:23.220 --> 00:05:27.900
 my current policy which so my little 

00:05:25.110 --> 00:05:28.889
 neighbor I will call him who premium but 

00:05:27.900 --> 00:05:30.990
 I'm going to do a simulation with 

00:05:28.889 --> 00:05:33.060
 each of these new policies there 

00:05:30.990 --> 00:05:35.430
 these policies change that will give me 

00:05:33.060 --> 00:05:36.930
 a value of the sum of the rewards 

00:05:35.430 --> 00:05:39.900
 from the beginning to the end 

00:05:36.930 --> 00:05:42.300
 I will compare that with my values 

00:05:39.900 --> 00:05:44.070
 current of the 2 0 so my new 

00:05:42.300 --> 00:05:46.470
 value life I compare it with it 

00:05:44.070 --> 00:05:48.180
 makes 2 0 here it's better if it's a 

00:05:46.470 --> 00:05:52.139
 politics gives me a better money 

00:05:48.180 --> 00:05:54.180
 rewards until the end but I 

00:05:52.139 --> 00:05:55.860
 just want to changed my policy for 

00:05:54.180 --> 00:05:58.680
 my new policy that takes precedence over 

00:05:55.860 --> 00:06:00.479
 neighboring policy then the value of 1st 

00:05:58.680 --> 00:06:03.180
 s 0 this policy this is going to be the 

00:06:00.479 --> 00:06:05.009
 value life I do that for everything but 

00:06:03.180 --> 00:06:06.900
 neighboring policies then I start again 

00:06:05.009 --> 00:06:09.389
 like that so take my new 

00:06:06.900 --> 00:06:11.909
 current policy generating these 

00:06:09.389 --> 00:06:14.909
 neighboring politics successor policy 

00:06:11.909 --> 00:06:16.919
 to her and to look if I can not 

00:06:14.909 --> 00:06:17.820
 improved this policy there he continued 

00:06:16.919 --> 00:06:23.580
 like that for a number 

00:06:17.820 --> 00:06:26.070
 of iterations then we need a 

00:06:23.580 --> 00:06:28.789
 way to generate successors to a 

00:06:26.070 --> 00:06:31.259
 politics that gave and ben 

00:06:28.789 --> 00:06:34.740
 the alternative a possibility that would be 

00:06:31.259 --> 00:06:36.409
 if I'm two years old, vary the action 

00:06:34.740 --> 00:06:38.689
 for a 

00:06:36.409 --> 00:06:39.499
 only state and do it for all 

00:06:38.689 --> 00:06:41.899
 possible states 

00:06:39.499 --> 00:06:44.209
 so if i had politics in pid es 0 

00:06:41.899 --> 00:06:46.550
 chf the action to the ppp of the only 

00:06:44.209 --> 00:06:48.110
 threesome but i could say that 

00:06:46.550 --> 00:06:50.059
 I will take the same action as a 

00:06:48.110 --> 00:06:53.209
 party animal correction to May 3 instead I 

00:06:50.059 --> 00:06:54.830
 will do in 0 the action to 2 or l 

00:06:53.209 --> 00:06:58.580
 reverse i will keep 1-0 action 1 

00:06:54.830 --> 00:07:00.559
 but in the state is this a I'm rather 

00:06:58.580 --> 00:07:02.419
 take the action together and so that 

00:07:00.559 --> 00:07:04.189
 would be put two neighboring policies that 

00:07:02.419 --> 00:07:06.619
 I would have to test in my algorithms 

00:07:04.189 --> 00:07:08.119
 and see also there is not one of the two 

00:07:06.619 --> 00:07:10.909
 which improves in relation to my policy 

00:07:08.119 --> 00:07:12.379
 running this being said it's a 

00:07:10.909 --> 00:07:14.749
 approach that brand me worked well 

00:07:12.379 --> 00:07:15.459
 I if the state space and not too much 

00:07:14.749 --> 00:07:18.649
 serious 

00:07:15.459 --> 00:07:19.819
 then if tired of actions and small 

00:07:18.649 --> 00:07:22.189
 when if I had two actions 

00:07:19.819 --> 00:07:24.319
 possible for each of the states if 

00:07:22.189 --> 00:07:26.089
 I had three possible actions here 

00:07:24.319 --> 00:07:27.709
 I should consider the case where at the 

00:07:26.089 --> 00:07:30.409
 place of l2 it would be the third 

00:07:27.709 --> 00:07:32.929
 same action here so we can see 

00:07:30.409 --> 00:07:34.369
 that many successor policies for 

00:07:32.929 --> 00:07:35.749
 easily explode if I have a lot 

00:07:34.369 --> 00:07:36.259
 of steps that a lot of actions 

00:07:35.749 --> 00:07:38.389
 possible 

00:07:36.259 --> 00:07:40.669
 so that's one of the problems and we 

00:07:38.389 --> 00:07:43.490
 could always have imagined doing a 

00:07:40.669 --> 00:07:45.079
 so random sampling choose 

00:07:43.490 --> 00:07:47.899
 randomly some of the stuffs 

00:07:45.079 --> 00:07:51.229
 parise some reactions randomly 

00:07:47.899 --> 00:07:54.439
 so I will not discuss how to do 

00:07:51.229 --> 00:07:55.490
 optimization this process there and I 

00:07:54.439 --> 00:07:57.529
 want to do it earlier to literature 

00:07:55.490 --> 00:08:01.189
 to see ways to attack this 

00:07:57.529 --> 00:08:02.599
 problem there but the problem that is 

00:08:01.189 --> 00:08:05.239
 especially important is the one 

00:08:02.599 --> 00:08:07.309
 linked to the fact that we made a lot of 

00:08:05.239 --> 00:08:08.599
 stochastic variations in values 

00:08:07.309 --> 00:08:09.409
 we will observe because a 

00:08:08.599 --> 00:08:11.029
 simulation 

00:08:09.409 --> 00:08:13.529
 there is a stochastic aspect because 

00:08:11.029 --> 00:08:14.729
 my environment and possibly 

00:08:13.529 --> 00:08:16.919
 stochastic and not necessarily 

00:08:14.729 --> 00:08:19.409
 deterministic it means that for the 

00:08:16.919 --> 00:08:20.909
 same policy given in doing I'm going 

00:08:19.409 --> 00:08:22.979
 maybe observe if I do two 

00:08:20.909 --> 00:08:24.959
 simulations independent of values ​​well 

00:08:22.979 --> 00:08:27.479
 different and that makes me go 

00:08:24.959 --> 00:08:29.189
 maybe did change my policy 

00:08:27.479 --> 00:08:32.159
 just because by chance my 

00:08:29.189 --> 00:08:34.139
 new policy successor and more 

00:08:32.159 --> 00:08:37.199
 was better than the other 

00:08:34.139 --> 00:08:40.229
 so for example if I have some 

00:08:37.199 --> 00:08:42.029
 strategy I'm in poker which a 

00:08:40.229 --> 00:08:44.459
 politics that I am to play at 

00:08:42.029 --> 00:08:46.589
 poker but necessarily the fact that I 

00:08:44.459 --> 00:08:48.569
 give us not your depend on maps 

00:08:46.589 --> 00:08:50.430
 that I'm going to get it and that's not something 

00:08:48.569 --> 00:08:53.209
 thing that I control and it can 

00:08:50.430 --> 00:08:55.680
 than a player a better strategy 

00:08:53.209 --> 00:08:57.540
 lost this tilting opponent players 

00:08:55.680 --> 00:08:59.850
 for a hand given one of the best qu 

00:08:57.540 --> 00:09:06.600
 so we would like that before 

00:08:59.850 --> 00:09:09.660
 a way to control this factor there he 

00:09:06.600 --> 00:09:11.220
 is a way of doing that way not bad 

00:09:09.660 --> 00:09:12.569
 interesting so the idea is we want 

00:09:11.220 --> 00:09:15.569
 control the variations of a 

00:09:12.569 --> 00:09:17.189
 simulation to another and if we have access to 

00:09:15.569 --> 00:09:19.559
 a generator so it's actually done 

00:09:17.189 --> 00:09:21.420
 simulations on computer and that 

00:09:19.559 --> 00:09:24.660
 it's the environment in which our 

00:09:21.420 --> 00:09:26.519
 money made take actions betty 

00:09:24.660 --> 00:09:30.050
 that we can do is that at the beginning of 

00:09:26.519 --> 00:09:32.189
 each simulation done we will initialize 

00:09:30.050 --> 00:09:34.730
 our random number generator 

00:09:32.189 --> 00:09:39.660
 simulator 

00:09:34.730 --> 00:09:42.449
 allamumma metals for me but my 

00:09:39.660 --> 00:09:43.949
 my initial policy which if I have a 

00:09:42.449 --> 00:09:46.980
 political then current 

00:09:43.949 --> 00:09:50.939
 I'm doing a simulation that will give me 

00:09:46.980 --> 00:09:53.550
 a value v2 0 and the cij considers a 

00:09:50.939 --> 00:09:55.050
 neighborly politics that premium 

00:09:53.550 --> 00:10:01.620
 the simulation I'm going to do I'm going 

00:09:55.050 --> 00:10:06.689
 do it with the same state of the generator 

00:10:01.620 --> 00:10:08.100
 of random numbers so in the case 

00:10:06.689 --> 00:10:10.709
 of a card game actually it would like 

00:10:08.100 --> 00:10:13.019
 say that leave the same cards that 

00:10:10.709 --> 00:10:15.300
 are going to be digging in the same way so that 

00:10:13.019 --> 00:10:18.059
 will allow to control for these 

00:10:15.300 --> 00:10:19.529
 variations there that could have a 

00:10:18.059 --> 00:10:22.079
 greater influence on value 

00:10:19.529 --> 00:10:25.170
 of a simulation that the policy that has 

00:10:22.079 --> 00:10:26.950
 been followed so this general idea the 

00:10:25.170 --> 00:10:30.100
 cct laws that is followed by 

00:10:26.950 --> 00:10:32.290
 great pegasus who the algorithm that is 

00:10:30.100 --> 00:10:34.360
 used for simulations so 

00:10:32.290 --> 00:10:36.550
 acrobatic with a cape town such a 

00:10:34.360 --> 00:10:38.560
 idea that we saw at the very beginning of this 

00:10:36.550 --> 00:10:40.750
 section 6 of the co I give you the link 

00:10:38.560 --> 00:10:44.530
 here if you want to see said if 

00:10:40.750 --> 00:10:46.840
 other simulations the 2 of this 

00:10:44.530 --> 00:10:48.460
 algorithm that took them controlled 

00:10:46.840 --> 00:10:51.960
 nil buddies then you will also have 

00:10:48.460 --> 00:10:54.040
 references to articles that 

00:10:51.960 --> 00:10:57.910
 describe a little better the comment this 

00:10:54.040 --> 00:11:00.370
 that this system controls it a 

00:10:57.910 --> 00:11:04.330
 sensors were unlocked so that's it 

00:11:00.370 --> 00:11:05.800
 it is very fast fast flyby of 

00:11:04.330 --> 00:11:07.480
 comment on what we can do 

00:11:05.800 --> 00:11:09.940
 looking for plans to do 

00:11:07.480 --> 00:11:12.700
 active learning and what are the 

00:11:09.940 --> 00:11:14.770
 issues consider it in the way 

00:11:12.700 --> 00:11:17.440
 what did the plan searches did to 

00:11:14.770 --> 00:11:20.170
 comment on what we sign that we generate 

00:11:17.440 --> 00:11:21.840
 neighboring new policies having 

00:11:20.170 --> 00:11:25.840
 also realized that it takes 

00:11:21.840 --> 00:11:27.220
 control the variations related or to the 

00:11:25.840 --> 00:11:31.660
 stochastic city of our environment 

00:11:27.220 --> 00:11:34.270
 for example by controlling the choice 

00:11:31.660 --> 00:11:38.940
 from the normal generation to the right 

00:11:34.270 --> 00:11:38.940
 between different simulations 

