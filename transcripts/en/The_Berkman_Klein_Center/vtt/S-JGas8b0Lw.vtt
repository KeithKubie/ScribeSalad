WEBVTT
Kind: captions
Language: en

00:00:05.260 --> 00:00:07.220
One of the challenges of building new

00:00:07.230 --> 00:00:09.330
technologies is that we often want them

00:00:09.330 --> 00:00:11.130
to solve things that have been very

00:00:11.130 --> 00:00:13.530
socially difficult to solve; things that

00:00:13.530 --> 00:00:15.360
we don't have answers to; problems that

00:00:15.360 --> 00:00:17.580
we don't know how we would best go about

00:00:17.580 --> 00:00:19.109
it in a socially responsible way.

00:00:19.109 --> 00:00:21.720
Technology cannot fill that gap for us.

00:00:21.720 --> 00:00:24.119
In fact technology is more likely than

00:00:24.119 --> 00:00:25.650
not, and artificial intelligence in

00:00:25.650 --> 00:00:27.750
particular, to exacerbate existing

00:00:27.750 --> 00:00:30.420
challenges. So if we look at different

00:00:30.420 --> 00:00:32.189
issues where we have major social

00:00:32.189 --> 00:00:34.559
challenges ahead of us, whether that is

00:00:34.559 --> 00:00:36.239
in the business realm, in criminal

00:00:36.239 --> 00:00:39.450
justice, in medicine, in education, we

00:00:39.450 --> 00:00:41.280
need to think hard and deep about how we

00:00:41.280 --> 00:00:43.260
want to marry technology and artificial

00:00:43.260 --> 00:00:45.420
intelligence into the broader social

00:00:45.420 --> 00:00:47.309
challenges that we're seeing with those

00:00:47.309 --> 00:00:49.680
systems. Artificial intelligence in many

00:00:49.680 --> 00:00:51.000
ways right now means different things to

00:00:51.000 --> 00:00:52.739
different people. For the technical

00:00:52.739 --> 00:00:54.690
community it's a very particular and

00:00:54.690 --> 00:00:57.449
narrow set of technologies, very much

00:00:57.449 --> 00:01:00.000
focused on neural networks, or

00:01:00.000 --> 00:01:01.050
advanced machine learning techniques, or

00:01:01.050 --> 00:01:04.140
robotics of a particular ilk. And these

00:01:04.140 --> 00:01:05.430
kinds of techniques have been in

00:01:05.430 --> 00:01:07.110
development for an extended period of

00:01:07.110 --> 00:01:09.360
time, so most technical folks are

00:01:09.360 --> 00:01:10.710
thinking about the iterations, the

00:01:10.710 --> 00:01:13.200
evolutions, the histories of that. For the

00:01:13.200 --> 00:01:14.490
business community, artificial

00:01:14.490 --> 00:01:16.710
intelligence has become the new buzzword,

00:01:16.710 --> 00:01:19.200
the idea of being able to do magical

00:01:19.200 --> 00:01:20.790
things with large amounts of data to

00:01:20.790 --> 00:01:22.830
solve problems that have in many ways

00:01:22.830 --> 00:01:25.020
become socially intractable that we hope that

00:01:25.020 --> 00:01:26.939
we can solve through technical means.

00:01:26.939 --> 00:01:27.960
For the public,

00:01:27.960 --> 00:01:30.479
AI really refers to the imagination

00:01:30.479 --> 00:01:33.210
that computers can do crazy things, and

00:01:33.210 --> 00:01:34.619
those crazy things can be both positive

00:01:34.619 --> 00:01:37.979
solving the world's problems, you know

00:01:37.979 --> 00:01:39.630
coming down and computers appearing to

00:01:39.630 --> 00:01:40.560
be smart.

00:01:40.560 --> 00:01:42.030
They can also be absolutely terrifying

00:01:42.030 --> 00:01:43.950
and usually there we refer to Hollywood

00:01:43.950 --> 00:01:46.350
concepts to really come back to it.

00:01:46.350 --> 00:01:47.850
One of the most important things to do when

00:01:47.850 --> 00:01:49.200
we start to study artificial

00:01:49.200 --> 00:01:51.180
intelligence is to actually bring together

00:01:51.180 --> 00:01:53.759
different frameworks of thinking. We need

00:01:53.759 --> 00:01:55.200
to think about it both technically and

00:01:55.200 --> 00:01:57.570
socially. And the main reason is because

00:01:57.570 --> 00:01:59.700
the biggest problems ahead of us are not

00:01:59.700 --> 00:02:02.130
simply technical or simply social. In

00:02:02.130 --> 00:02:03.780
fact it's the marriage between the two

00:02:03.780 --> 00:02:05.610
that becomes the most important. For this

00:02:05.610 --> 00:02:07.350
reason we have to take different kinds

00:02:07.350 --> 00:02:09.060
of social issues very seriously.

00:02:09.060 --> 00:02:09.390
We have

00:02:09.390 --> 00:02:11.610
to really understand what's at stake, the

00:02:11.610 --> 00:02:15.450
biases of the data that are involved in making artificial intelligence

00:02:15.450 --> 00:02:18.300
function, the interpretation layers, the

00:02:18.300 --> 00:02:19.920
ways in which these systems can get

00:02:19.920 --> 00:02:22.319
manipulated. All of these social issues

00:02:22.319 --> 00:02:24.180
become critical to making certain that

00:02:24.180 --> 00:02:26.700
the technologies are done right. The key

00:02:26.700 --> 00:02:28.560
and challenge of figuring out how to think

00:02:28.560 --> 00:02:31.260
ethically is to actually think about how

00:02:31.260 --> 00:02:33.330
we want to marry different kinds of

00:02:33.330 --> 00:02:34.920
social mindsets and different kinds of

00:02:34.920 --> 00:02:37.380
technical mindsets. How do we get the

00:02:37.380 --> 00:02:39.180
technical folks to start articulating

00:02:39.180 --> 00:02:41.340
the realm of possibilities that are

00:02:41.340 --> 00:02:43.020
available to us? And what are the

00:02:43.020 --> 00:02:44.610
governance structures that we want to

00:02:44.610 --> 00:02:46.350
see in place to make certain that we can

00:02:46.350 --> 00:02:48.150
choose responsibly?

00:02:48.150 --> 00:02:49.739
We're entering a realm where we're paying a lot

00:02:49.739 --> 00:02:51.420
of attention to cybersecurity, where we're

00:02:51.420 --> 00:02:53.430
realizing that the security of our

00:02:53.430 --> 00:02:55.470
infrastructure can put us at risk in

00:02:55.470 --> 00:02:57.900
tremendous ways. The same will be true of

00:02:57.900 --> 00:02:59.910
artificial intelligence, but the risks

00:02:59.910 --> 00:03:01.410
that we have faced aren't necessarily

00:03:01.410 --> 00:03:03.420
about traditional hacking. They're about

00:03:03.420 --> 00:03:05.430
the manipulation of data, about data

00:03:05.430 --> 00:03:07.410
being misinterpreted in different ways,

00:03:07.410 --> 00:03:09.300
about the ways of cleaning and

00:03:09.300 --> 00:03:11.880
processing that data to do analysis, not

00:03:11.880 --> 00:03:13.320
taking into account certain social

00:03:13.320 --> 00:03:15.690
issues. And so this means we need to

00:03:15.690 --> 00:03:17.850
really think about the whole process in

00:03:17.850 --> 00:03:19.920
which we produce artificial intelligence

00:03:19.920 --> 00:03:32.330
systems.

