WEBVTT
Kind: captions
Language: en

00:00:00.012 --> 00:00:02.180
&gt;&gt; All right. So we should take a moment to define

00:00:02.180 --> 00:00:04.530
inductive learning. So inductive learning is

00:00:04.530 --> 00:00:05.920
learning from examples. It's the kind

00:00:05.920 --> 00:00:09.410
of learning problem that we've been looking at the whole time, But

00:00:09.410 --> 00:00:11.932
we haven't been very precise about all the various quantities that we

00:00:11.932 --> 00:00:14.940
want to be able to talk about. So the number of properties

00:00:14.940 --> 00:00:17.710
that we actually need to be thinking about when we go and

00:00:17.710 --> 00:00:21.910
define what an inductive learning problem is, and measure what an inductive

00:00:21.910 --> 00:00:25.020
learning algorithm does. So one of them is just a simple thing

00:00:25.020 --> 00:00:27.950
like, what's the probability that the training is actually is going

00:00:27.950 --> 00:00:29.938
work. You know, whatever it is that it's trying to produce,

00:00:29.938 --> 00:00:33.610
it may be that in some cases, because the data is

00:00:33.610 --> 00:00:35.740
really noisy or just you know, got a, got a bad set

00:00:35.740 --> 00:00:39.230
of data, it might not actually work. So, we generally talk

00:00:39.230 --> 00:00:44.550
about a quantity like 1 minus delta as the probability of success.

00:00:44.550 --> 00:00:46.600
Delta here obviously is a probability of failure and this is

00:00:46.600 --> 00:00:50.670
just 1 minus that. There's also issues like the number of examples

00:00:50.670 --> 00:00:54.292
to train on. How many, how much data does the does the algorithm

00:00:54.292 --> 00:00:56.760
get to see? . Is there a letter you like for that, Charles?

00:00:58.030 --> 00:00:58.290
&gt;&gt; no.

00:00:58.290 --> 00:00:58.880
&gt;&gt; Okay.

00:00:58.880 --> 00:01:00.180
&gt;&gt; Is there a letter you like?

00:01:00.180 --> 00:01:02.700
&gt;&gt; I don't know, sometimes I like M for number of samples. But I thought

00:01:02.700 --> 00:01:05.860
maybe you went, you would want N because, you know, things tend to grow with N.

00:01:05.860 --> 00:01:07.600
&gt;&gt; Yeah. I did want N, but then I thought well,

00:01:07.600 --> 00:01:09.488
we can't use N because we use N for everything else.

00:01:09.488 --> 00:01:13.290
&gt;&gt; [LAUGH] Fair enough. There's also something that we really

00:01:13.290 --> 00:01:16.220
haven't talked about yet, but you could imagine that the complexity

00:01:16.220 --> 00:01:18.140
of the hypothesis class might matter. Why, why

00:01:18.140 --> 00:01:20.430
do you think that could come into play, Charles?

00:01:20.430 --> 00:01:22.560
&gt;&gt; Well, if you don't have a very

00:01:22.560 --> 00:01:26.280
complex hypothesis class, then there's some things, well,

00:01:26.280 --> 00:01:27.490
do you mean the complexity of the class

00:01:27.490 --> 00:01:29.690
or the complexity of the hypotheses in the class?

00:01:29.690 --> 00:01:31.880
&gt;&gt; That's a good question. Do we mean the complexity of the

00:01:31.880 --> 00:01:34.880
class or the complexity of the hypotheses in the class? Well it

00:01:34.880 --> 00:01:38.120
depends on how we measure complexity. But the complexity of the class

00:01:38.120 --> 00:01:41.880
could be like the sum of the complexities of all the hypotheses

00:01:41.880 --> 00:01:43.270
in the class, so it could be both.

00:01:43.270 --> 00:01:47.550
&gt;&gt; Hm. Well if, if you mean, you know, a hypothesis class is complex if

00:01:47.550 --> 00:01:51.640
it has very complex hypotheses, then you can say, well, if you have a hypothesis

00:01:51.640 --> 00:01:54.470
class that can't represent much, then it

00:01:54.470 --> 00:01:56.870
will be hard for you to, well, represent

00:01:56.870 --> 00:01:57.500
much. It will be hard for you

00:01:57.500 --> 00:02:00.320
to learn anything complicated. So that could matter.

00:02:00.320 --> 00:02:03.360
&gt;&gt; Sure. That's right. Now, could you see a downside to having

00:02:03.360 --> 00:02:07.050
a complexity class, I'm sorry, a hypothesis class that is very, very

00:02:07.050 --> 00:02:07.530
complex?

00:02:07.530 --> 00:02:10.560
&gt;&gt; You mean like my daughter? sure. I think

00:02:10.560 --> 00:02:12.360
you could, it would be much easier to overfit.

00:02:12.360 --> 00:02:14.690
&gt;&gt; Right. So getting something that actually works well might

00:02:14.690 --> 00:02:16.750
be challenging. You might need a lot more data to

00:02:16.750 --> 00:02:18.600
kind of nail down what your're, what you're really talking

00:02:18.600 --> 00:02:20.162
about. So it's a bit of a double edged sword.

00:02:20.162 --> 00:02:22.680
All right. So then, there, another issue is, well, you

00:02:22.680 --> 00:02:24.430
know, it may be easy to learn if you don't

00:02:24.430 --> 00:02:28.270
have to learn very well. [LAUGH] So the, the accuracy

00:02:28.270 --> 00:02:32.650
to which the target concept is approximated, often written as epsilon,

00:02:32.650 --> 00:02:34.840
is another thing that's going to be important in understanding

00:02:34.840 --> 00:02:38.330
the complexity of a learning algorithm. And and so those, those

00:02:38.330 --> 00:02:41.600
are kind of the main complexity related quantities that I,

00:02:41.600 --> 00:02:45.140
that I wanted to talk about. But there's also some choices

00:02:45.140 --> 00:02:48.060
as to how the learning problem is actually framed. There's

00:02:48.060 --> 00:02:50.900
the manner in which training examples are presented. And there's the

00:02:50.900 --> 00:02:54.460
manner in which training examples are selected for presentation. And we're

00:02:54.460 --> 00:02:58.160
going to talk about both of these. Let me just first

00:02:58.160 --> 00:02:59.520
say that when I talk about the

00:02:59.520 --> 00:03:02.790
manner in which training examples are presented, there's,

00:03:02.790 --> 00:03:07.260
there's two that I think are really, really important to look at. One is batch

00:03:07.260 --> 00:03:08.400
and that's mostly what we've looked at

00:03:08.400 --> 00:03:11.190
so far, that there's a training set that's

00:03:11.190 --> 00:03:14.780
fixed and handed over to the algorithm in a big bolus, right. A big group.

00:03:14.780 --> 00:03:15.690
&gt;&gt; A big batch.

00:03:15.690 --> 00:03:19.848
&gt;&gt; A big batch, exactly. Or it could also be presented on line, so

00:03:19.848 --> 00:03:23.209
one at a time. So we say to the training algorithm, or the learning algorithm,

00:03:23.209 --> 00:03:25.459
here's an example. And then it has to predict what

00:03:25.459 --> 00:03:28.530
the label is. And then the algorithm can say, oh

00:03:28.530 --> 00:03:30.570
here's what the right label is. Let's try again. Here's

00:03:30.570 --> 00:03:32.660
another example. And it can go back and forth like that.

00:03:32.660 --> 00:03:33.225
&gt;&gt; Mm-hm.

00:03:33.225 --> 00:03:35.210
&gt;&gt; We haven't really talked about algorithms that work that way, but

00:03:35.210 --> 00:03:38.500
it is useful in the context of computational learning theory to have

00:03:38.500 --> 00:03:40.990
both kinds of algorithms. They have different sorts of behavior. All right.

00:03:40.990 --> 00:03:44.150
So let's talk about the manner in which training examples are selected.

