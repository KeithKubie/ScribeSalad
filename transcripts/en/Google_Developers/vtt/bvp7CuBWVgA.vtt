WEBVTT
Kind: captions
Language: en

00:00:05.380 --> 00:00:08.240
GUIDO VAN ROSSUM: I
think I can start.

00:00:08.240 --> 00:00:09.178
Sounds like everyone
can hear me.

00:00:09.178 --> 00:00:11.318
Welcome to the first
session of the morning.

00:00:11.318 --> 00:00:14.798
I'm glad you all chose to
learn more about App Engine.

00:00:15.438 --> 00:00:17.198
In particular, Appstats,
which is a tool for

00:00:17.198 --> 00:00:18.980
App Engine developers.

00:00:18.980 --> 00:00:22.260
Either Python or Java to learn
more about the performance

00:00:22.260 --> 00:00:24.818
of their application.

00:00:24.818 --> 00:00:27.898
So quick overview of the talk.

00:00:27.898 --> 00:00:30.300
Note the wave URL and the
Twitter tag at the bottom.

00:00:30.300 --> 00:00:33.120
Please use those for
any interaction.

00:00:33.120 --> 00:00:36.680
I'm going to give a little bit
of an introduction, a demo

00:00:36.680 --> 00:00:39.080
of the tool of course.

00:00:39.400 --> 00:00:42.580
Some background, what you would
do to write your own tool like

00:00:42.580 --> 00:00:45.960
this because there's actually
not really any magic involved.

00:00:46.820 --> 00:00:50.560
How to configure
it, how to use it.

00:00:50.560 --> 00:00:53.780
Hopefully I'll have plenty of
time to talk about the number

00:00:53.780 --> 00:00:57.360
of specific performance issues
that we found in various

00:00:57.360 --> 00:01:01.580
applications that I wrote or
colleagues wrote or early users

00:01:01.580 --> 00:01:07.380
wrote where Appstats turned out
to be very helpful for

00:01:07.380 --> 00:01:10.580
performance analysis.

00:01:10.580 --> 00:01:15.498
So the high order bits of
Appstats is it's a tool library

00:01:15.498 --> 00:01:18.060
that you link in with your
App Engine application.

00:01:18.060 --> 00:01:22.320
It is specific for App Engine
and it analyzes your remote

00:01:22.320 --> 00:01:23.520
procedure call performance.

00:01:23.820 --> 00:01:26.920
It's available for
Python since February.

00:01:26.920 --> 00:01:29.700
It's available for
Java since March.

00:01:29.700 --> 00:01:33.040
The tools work the same, they
have the same user interface.

00:01:33.040 --> 00:01:36.150
The actual implementation that
is linked into your application

00:01:36.150 --> 00:01:39.860
is somewhat different.

00:01:39.860 --> 00:01:43.800
This is a very quick graphical
overview of the main timeline

00:01:43.800 --> 00:01:46.598
view that gives you most of the
insights in your application.

00:01:46.598 --> 00:01:50.660
From here on you can also drill
down to specific things like

00:01:50.660 --> 00:01:53.878
stack trace corresponding
to each call basically.

00:01:53.878 --> 00:01:58.018
It shows you what the total
time was taken up by your

00:01:58.018 --> 00:02:03.738
request, what RPC calls it made
and where those RPC calls were

00:02:03.738 --> 00:02:06.540
relative to the timeline and
how long they took of course.

00:02:06.540 --> 00:02:08.858
And then the red bars show you
how much you were charged

00:02:08.858 --> 00:02:10.338
for each of those RPC calls.

00:02:11.458 --> 00:02:15.680
Some basics as I
mentioned, Appstats is

00:02:15.680 --> 00:02:16.918
a user-land library.

00:02:16.918 --> 00:02:18.460
It's just linked with
your application.

00:02:18.460 --> 00:02:23.540
There is no special system
calls, no support in the

00:02:23.540 --> 00:02:27.378
App Engine server for
this tool specifically.

00:02:28.138 --> 00:02:30.360
It has a little bit of
run-time overhead.

00:02:30.360 --> 00:02:32.918
I'll get into that more later.

00:02:32.918 --> 00:02:37.020
I think the run-time
overhead is pretty minimal.

00:02:37.020 --> 00:02:40.138
The APIs it uses to do all its
instrumentation magic are

00:02:40.138 --> 00:02:44.528
existing APIs that had
been added to the App

00:02:44.528 --> 00:02:46.058
Engine APIs long ago.

00:02:46.058 --> 00:02:49.838
Some of the instrumentation
APIs it uses are actually

00:02:49.838 --> 00:02:50.378
standard Python APIs.

00:02:50.538 --> 00:02:55.918
Others, for like measuring
things specific to RPCs

00:02:55.918 --> 00:02:58.218
are App Engine specific.

00:02:58.218 --> 00:03:00.180
There's no need to
download anything.

00:03:00.180 --> 00:03:03.600
This is part of the standard
software development kit that

00:03:03.600 --> 00:03:07.340
you already have when you start
developing with App Engine.

00:03:07.340 --> 00:03:07.800
I mean, that's your
only download.

00:03:08.220 --> 00:03:10.400
There's no extra download
just for this tool.

00:03:11.058 --> 00:03:13.798
You do have to do a little
bit of configuration.

00:03:13.798 --> 00:03:16.780
If you're using Java it's
that about 10 lines of XML.

00:03:16.780 --> 00:03:19.460
If you're using Python it's
about four lines of Python

00:03:19.460 --> 00:03:21.698
and two lines of YML.

00:03:21.698 --> 00:03:25.200
And finally, the user interface
for this tool and that is

00:03:25.200 --> 00:03:29.578
somewhat unusual perhaps, but
suitable for something that is

00:03:29.578 --> 00:03:32.958
self-contained in all user-land
code, user interface is

00:03:32.958 --> 00:03:36.718
actually part of the library.

00:03:36.978 --> 00:03:39.338
What is the main thing you
can do with Appstats?

00:03:40.378 --> 00:03:44.618
I would say probably the most
important thing you can do is

00:03:44.618 --> 00:03:48.498
find which parts of your
application, which specific

00:03:48.498 --> 00:03:54.438
request handlers are expensive
and which RPC calls, for

00:03:54.438 --> 00:03:57.558
example, a particular database
query or a call to send e-mail

00:03:57.558 --> 00:04:01.898
or fetch an external URL are
the hottest parts of

00:04:01.898 --> 00:04:05.458
your application.

00:04:05.458 --> 00:04:12.460
So if you're taking Don Knuth's
paradigm too heart, which says,

00:04:12.460 --> 00:04:16.338
to avoid premature optimization
you write a correct application

00:04:16.338 --> 00:04:19.060
first then you tackle
optimization and these tools

00:04:19.060 --> 00:04:22.000
show you where to start doing
that optimization so you only

00:04:22.000 --> 00:04:24.980
have to touch a small
amount of your code.

00:04:24.980 --> 00:04:29.140
Now Appstats records everything
about every single remote

00:04:29.140 --> 00:04:32.498
procedure call made
by your application.

00:04:32.498 --> 00:04:34.920
In App Engine pretty much
everything is a remote

00:04:34.920 --> 00:04:38.058
procedure call whether it's
sending email or fetching an

00:04:38.058 --> 00:04:41.538
external URL or making a
call to the datastore

00:04:41.538 --> 00:04:42.838
or even memcache.

00:04:42.838 --> 00:04:43.778
All those things are
considered RPCs.

00:04:46.380 --> 00:04:49.458
The tool shows you, as I
already showed briefly,

00:04:49.458 --> 00:04:51.600
shows you a timeline of
individual requests.

00:04:51.600 --> 00:04:56.700
It also shows some overview of
sort of global statistics.

00:04:56.700 --> 00:04:59.138
How many requests of
each type did you get?

00:04:59.938 --> 00:05:04.378
How many RPCs of each type
did you make and sort of a

00:05:04.378 --> 00:05:08.278
combination of which requests
caused most of the RPCs

00:05:08.278 --> 00:05:10.818
of each particular type.

00:05:10.938 --> 00:05:14.938
The timeline in most cases
is the most important tool

00:05:14.938 --> 00:05:17.540
to find out where the
time actually goes.

00:05:17.540 --> 00:05:21.278
And the Python version at least
lets you drill down to the

00:05:21.278 --> 00:05:24.378
content of stack frames so you
can view local variables, which

00:05:24.378 --> 00:05:26.800
is often very helpful in
finding out why a particular

00:05:26.800 --> 00:05:29.738
call is being made.

00:05:29.738 --> 00:05:31.198
So now it's time to go
demo, do a little demo.

00:05:33.518 --> 00:05:38.740
I'll start actually-- always
a little tricky to find the

00:05:38.740 --> 00:05:41.058
right window-- OK, this
is an application.

00:05:41.058 --> 00:05:42.438
It's called Rietveld.

00:05:42.138 --> 00:05:43.938
It's also called Code Review.

00:05:43.718 --> 00:05:45.018
It runs on Apps.

00:05:45.138 --> 00:05:47.638
I wrote it about two
years ago actually as

00:05:47.638 --> 00:05:49.818
a demo for App Engine.

00:05:52.220 --> 00:05:54.100
It has a number of
interesting features.

00:05:54.100 --> 00:05:56.860
It allows you to lets
see how fast it is.

00:05:56.860 --> 00:06:01.660
Yes, it allows you to view
patches, comment on them.

00:06:01.660 --> 00:06:03.180
I'm not going to go into
much detail there.

00:06:03.300 --> 00:06:11.238
The important thing is that
we have an Appstats window

00:06:11.238 --> 00:06:12.578
connected to this application.

00:06:13.698 --> 00:06:17.778
Actually months ago I linked
Appstats into Rietveld and all

00:06:17.778 --> 00:06:21.260
I have to do now is go to the
stats URL and see what the

00:06:21.260 --> 00:06:24.058
most recent statistics were.

00:06:24.058 --> 00:06:26.958
And this is a pretty
overwhelmingly

00:06:26.958 --> 00:06:27.180
window I'm afraid.

00:06:27.180 --> 00:06:29.518
Let's see if I can find a
smaller version of this.

00:06:29.720 --> 00:06:36.220
This is an earlier snapshot
that I took this morning

00:06:36.220 --> 00:06:39.418
after resetting the data
collection briefly.

00:06:39.758 --> 00:06:42.220
So we have three sections
of the overview UI.

00:06:42.860 --> 00:06:46.340
On the left we have everything
ordered by RPC calls.

00:06:46.340 --> 00:06:50.918
So datastore next, datastore
get, datastore run query.

00:06:50.918 --> 00:06:55.540
On the right we have everything
grouped by particular calls

00:06:55.540 --> 00:06:56.818
into the applications.

00:06:56.818 --> 00:06:59.338
So slash is a popular request.

00:06:59.338 --> 00:07:01.718
Repos is another one.

00:07:01.718 --> 00:07:06.540
And then various
diff-related requests.

00:07:07.958 --> 00:07:12.378
At the bottom we see the entry
point to the next important

00:07:12.378 --> 00:07:14.828
part of the application,
which this is just a

00:07:14.828 --> 00:07:16.378
log of recent requests.

00:07:16.378 --> 00:07:21.958
So actually this snapshot was
taken a few seconds after I

00:07:21.958 --> 00:07:24.958
reset the data gathering
so the log only has

00:07:24.958 --> 00:07:25.638
seven entries or so.

00:07:26.898 --> 00:07:29.878
Let's go back to
the current one.

00:07:29.878 --> 00:07:30.158
So we scroll down.

00:07:30.158 --> 00:07:34.218
We see that there have
been a lot more calls

00:07:34.218 --> 00:07:38.838
since this morning.

00:07:38.278 --> 00:07:42.618
Here we have some of the
more recent requests.

00:07:42.618 --> 00:07:44.258
I think this is probably
going to be a nice one.

00:07:44.258 --> 00:07:47.120
So here we have timeline of
this particular request.

00:07:47.120 --> 00:07:51.458
At the top we see the request
that causes this particular

00:07:51.458 --> 00:07:53.158
sequence of events to happen.

00:07:53.158 --> 00:07:57.720
On the left we see exactly
which calls it made.

00:07:57.720 --> 00:07:59.860
It made a memcache set call.

00:07:59.180 --> 00:08:01.018
It had a datastore get call.

00:08:01.018 --> 00:08:04.238
It ran four different queries
and somehow the second query

00:08:04.238 --> 00:08:07.598
always comes out by far
the most expensive.

00:08:07.598 --> 00:08:09.878
So it will be interesting
to drill down on that.

00:08:09.878 --> 00:08:11.918
So let's click on
that second query.

00:08:11.918 --> 00:08:15.950
We can click right there and
now it opens the stack find.

00:08:15.950 --> 00:08:18.178
This is actually still the same
window, I just scrolled down to

00:08:18.178 --> 00:08:20.478
the corresponding sequence.

00:08:20.478 --> 00:08:24.220
So here is another view
off all those RPCs.

00:08:24.220 --> 00:08:26.698
Again, the memcache set,
datastore get, but here

00:08:26.698 --> 00:08:27.578
you see more detail.

00:08:27.578 --> 00:08:31.420
You see the contents of the
request buffer, the contents of

00:08:31.420 --> 00:08:35.000
the response buffer and then
the various stack frames.

00:08:35.900 --> 00:08:40.040
So while data stored with py is
some internal library, db init

00:08:40.040 --> 00:08:41.640
is in another internal library.

00:08:41.140 --> 00:08:44.460
This is actually library
that you use to interact

00:08:44.460 --> 00:08:46.440
with a data store.

00:08:46.440 --> 00:08:47.500
Typically a data store
that probably has a

00:08:47.500 --> 00:08:47.820
lower level library.

00:08:47.820 --> 00:08:51.440
Then here views the
application code.

00:08:51.440 --> 00:08:52.260
Let's see what was
on the stack.

00:08:52.260 --> 00:08:53.860
So we see some local variables.

00:08:53.860 --> 00:08:57.140
Drafts, issue, batchset, id.

00:08:57.140 --> 00:09:01.120
If we want to know sort of
where this lives in source code

00:09:01.120 --> 00:09:06.580
we can open the source code in
the new window and there is our

00:09:06.580 --> 00:09:08.340
small little primitive
source code browser.

00:09:08.340 --> 00:09:13.380
So we can see that all those
variables, like issue comes

00:09:13.380 --> 00:09:17.220
directly into the query.

00:09:17.220 --> 00:09:19.880
So to go back to the top there
are a few other things that

00:09:19.880 --> 00:09:21.140
you can see about this
request right away.

00:09:21.460 --> 00:09:25.620
If you were to click on this
link it would actually send

00:09:25.620 --> 00:09:28.880
another request like
that to the server.

00:09:28.880 --> 00:09:30.440
So this is that
particular view.

00:09:30.440 --> 00:09:31.760
There it is.

00:09:31.760 --> 00:09:35.360
I don't know who looked at
this thing, it wasn't me.

00:09:35.360 --> 00:09:36.800
So there are a couple tabs.

00:09:36.800 --> 00:09:39.848
The RPC stats shows
you how many calls of

00:09:39.848 --> 00:09:41.000
each type it made.

00:09:41.320 --> 00:09:43.540
How much real time was spent in
each of those calls and how

00:09:43.540 --> 00:09:46.760
much time you were charged
for each of those calls.

00:09:46.760 --> 00:09:52.300
Then there's the CGI
environment which shows you

00:09:52.300 --> 00:09:54.980
basically the header and
some other information.

00:09:54.980 --> 00:09:58.340
If you're using Python you'll
be familiar with this.

00:09:58.340 --> 00:10:02.440
From this you can
reconstruct the request.

00:10:02.440 --> 00:10:04.558
Like the host header
had this value.

00:10:04.018 --> 00:10:08.300
This was the refer header, the
user agent header and so on.

00:10:08.300 --> 00:10:12.560
And finally, in some cases it's
kind of useful to know exactly

00:10:12.560 --> 00:10:16.900
what the search path for Python
modules was and so you

00:10:16.900 --> 00:10:18.620
can see that here.

00:10:18.620 --> 00:10:22.120
Back to the timeline and
now to the presentation.

00:10:22.120 --> 00:10:27.380
I should mention that the
Java tool has a very

00:10:27.380 --> 00:10:30.640
similar user interface.

00:10:30.640 --> 00:10:32.860
You'll see that the
CGI environment and

00:10:32.860 --> 00:10:34.280
the [? system ?]

00:10:34.280 --> 00:10:38.900
path part are not present
in the user interface.

00:10:38.900 --> 00:10:42.540
I'm not giving a live demo of
the Java version, however you

00:10:42.540 --> 00:10:47.140
can hear more about this tool
in the Java context in tomorrow

00:10:47.140 --> 00:10:49.940
morning's talk by Don
Schwartz and Toby Reyelts.

00:10:50.200 --> 00:10:54.860
And by the way I heard in the
keynote this morning that the

00:10:54.860 --> 00:10:57.920
Java version of Appstats has
also been integrated in Speed

00:10:57.920 --> 00:11:02.220
Tracer which is in general a
browser side performance

00:11:02.220 --> 00:11:05.100
analysis tool that has a
similar graphical user

00:11:05.100 --> 00:11:07.760
interface as Appstats.

00:11:08.220 --> 00:11:11.060
Speed Tracer has an optional
integration with Java

00:11:11.060 --> 00:11:13.140
Appstats in the background.

00:11:13.140 --> 00:11:16.260
And there's another talk going
on, which unfortunately you've

00:11:16.260 --> 00:11:18.720
already missed because it's
going on simultaneously with

00:11:18.720 --> 00:11:19.100
this one about Speed Tracer.

00:11:19.100 --> 00:11:23.400
But I'll also show that
off a little bit.

00:11:23.400 --> 00:11:26.760
So little bit about the
background of Appstats.

00:11:28.760 --> 00:11:32.060
It's important to understand
exactly what a remote

00:11:32.060 --> 00:11:33.840
procedure call is.

00:11:33.840 --> 00:11:36.020
In the App Engine environment,
in the context of App Engine

00:11:36.020 --> 00:11:41.020
it's a very, clearly defined
thing where it's a round trip

00:11:41.020 --> 00:11:45.580
between your application and
something in the back end.

00:11:45.740 --> 00:11:48.340
Now that something in the back
end could be Google's mail

00:11:48.340 --> 00:11:52.820
service infrastructure.

00:11:52.820 --> 00:11:54.700
It could be the App
Engine data store.

00:11:54.520 --> 00:11:58.600
It could be the memcache server
dedicated to your application.

00:11:59.020 --> 00:12:01.220
It could be any number
of those things.

00:12:01.220 --> 00:12:04.800
It could also be an external
website that you've accessing

00:12:04.800 --> 00:12:06.420
through URLfetch interface.

00:12:07.580 --> 00:12:10.160
All those things are RPCs and
all those things are recorded

00:12:10.160 --> 00:12:14.840
by Appstats so it's not
limited to database calls.

00:12:14.840 --> 00:12:17.800
I should mention that in the
case of database calls and I

00:12:17.800 --> 00:12:20.420
think we actually saw
an example of that.

00:12:20.420 --> 00:12:23.600
If I can find that
window quickly.

00:12:23.600 --> 00:12:28.000
Yeah, here these data store
next calls, they're actually

00:12:28.000 --> 00:12:32.640
in a sense continuation
calls for large queries.

00:12:32.640 --> 00:12:37.460
If a query returns under 20
results and under a megabyte of

00:12:37.460 --> 00:12:41.300
data all you see in your RPC
stats is a single

00:12:41.300 --> 00:12:42.760
run query call.

00:12:42.760 --> 00:12:45.960
If your query is larger than
either of those numbers, more

00:12:45.960 --> 00:12:52.660
than 20 results of more than a
megabytes of data returned

00:12:52.660 --> 00:12:58.460
because of limitations in how
many results you can fetch at

00:12:58.460 --> 00:13:02.440
one time, in one was response,
it's actually broken up

00:13:02.440 --> 00:13:03.580
into multiple calls.

00:13:03.580 --> 00:13:06.120
The first one of which will be
called run query and all the

00:13:06.120 --> 00:13:08.920
subsequent ones will
be called next.

00:13:08.920 --> 00:13:12.280
In general, if you see a lot of
next calls you're probably

00:13:12.280 --> 00:13:15.740
doing inefficient queries
because it's unlikely that

00:13:15.740 --> 00:13:18.840
one single request in your
application is interested in

00:13:18.840 --> 00:13:21.600
hundreds of query results.

00:13:21.600 --> 00:13:23.000
You're probably doing some
kind of computation.

00:13:23.000 --> 00:13:27.000
It's good to off-line
more easily.

00:13:27.000 --> 00:13:34.380
Anyway, why did I decide to
write a tool whose only purpose

00:13:34.380 --> 00:13:39.560
it is to instrument through
remote procedure calls?

00:13:39.560 --> 00:13:42.880
Well, looking at the timing, if
you look at the typical web

00:13:42.880 --> 00:13:52.620
application running on App
Engine you'll just notice that

00:13:52.620 --> 00:13:56.120
almost all your time-- and
again, I'll just show one of

00:13:56.120 --> 00:14:03.640
our Appstats windows-- almost
all your real time goes to

00:14:03.640 --> 00:14:07.920
waiting for queries or other
remote procedure calls.

00:14:07.920 --> 00:14:10.020
The only place in this graph
where there is significant time

00:14:10.020 --> 00:14:14.860
that is not spent waiting for
an RPC is somewhere here.

00:14:14.860 --> 00:14:17.260
And I imagine that this is
where it's actually doing some

00:14:17.260 --> 00:14:20.760
template expansion, but that's
a very small fraction of the

00:14:20.760 --> 00:14:23.880
total response time of this
application because this took

00:14:23.880 --> 00:14:27.920
over 800 milliseconds to
compute and almost 700 hundred

00:14:27.920 --> 00:14:33.720
milliseconds of that
was pure RPC time.

00:14:33.720 --> 00:14:37.160
So in a sense I have to admit
that Rietveld even after two

00:14:37.160 --> 00:14:40.700
years is not all that
optimized, which makes it a

00:14:40.700 --> 00:14:45.820
great topic for this
presentation of course.

00:14:45.820 --> 00:14:49.100
So some statistics just to give
you a feel of how fast App

00:14:49.100 --> 00:14:52.400
Engine is and you can use
Appstats to measure these

00:14:52.400 --> 00:14:54.920
numbers for your
own application.

00:14:54.920 --> 00:14:57.860
Writing a single entry
to the data store is 50

00:14:57.860 --> 00:14:58.040
or 100 milliseconds.

00:14:58.040 --> 00:15:02.120
And actually the size of the
entry is barely important.

00:15:02.120 --> 00:15:05.720
What is much more important in
determining how long it

00:15:05.720 --> 00:15:09.560
actually takes and also how
much you're charged for it is

00:15:09.560 --> 00:15:13.180
actually the number of
properties in the data that

00:15:13.180 --> 00:15:16.500
require indexing because
the index updates are

00:15:16.500 --> 00:15:17.340
part of the write.

00:15:17.340 --> 00:15:20.700
Getting an object, of course,
if you know the key is much

00:15:20.700 --> 00:15:23.340
faster, but it still takes
10 or 20 milliseconds.

00:15:24.920 --> 00:15:26.400
The fastest queries I've
measured were about

00:15:26.400 --> 00:15:27.820
20 milliseconds.

00:15:27.180 --> 00:15:30.860
I've seen them vary between
20 and 100 milliseconds.

00:15:31.040 --> 00:15:34.540
If you're doing complex queries
by comparing a number of

00:15:34.540 --> 00:15:39.480
different properties using
multiple indexes, even a query

00:15:39.480 --> 00:15:41.540
that only returns a few
results can take half

00:15:41.540 --> 00:15:43.720
a second sometimes.

00:15:43.720 --> 00:15:46.800
Memcache is probably the
fastest API we have.

00:15:46.800 --> 00:15:48.460
The fastest RPC.

00:15:48.460 --> 00:15:52.940
Even a memcache call, one
round trip, can be 5

00:15:52.940 --> 00:15:53.680
or 10 milliseconds.

00:15:53.680 --> 00:15:55.320
The fastest I've seen is
about 4 milliseconds.

00:15:57.260 --> 00:16:02.340
And realize that memcache is
just copying a number of bytes

00:16:02.340 --> 00:16:04.440
in a single hash table look up.

00:16:04.440 --> 00:16:06.580
There's no IO that goes on.

00:16:06.580 --> 00:16:10.340
So all the time, all that 5 or
10 milliseconds that you spend

00:16:10.340 --> 00:16:13.640
waiting for memcache is
purely network traffic.

00:16:15.780 --> 00:16:17.738
You're application
makes the call.

00:16:17.738 --> 00:16:19.378
The data is sent to
the memcache server.

00:16:20.660 --> 00:16:23.320
Once the memcache server has
received the last byte of your

00:16:23.320 --> 00:16:25.400
request it does its work.

00:16:25.400 --> 00:16:27.460
Copies the response in
the outfield buffer

00:16:27.460 --> 00:16:28.160
and then sends back.

00:16:28.160 --> 00:16:30.140
And all the time
you're waiting.

00:16:30.300 --> 00:16:33.580
I haven't measured it, but I
imagine that the actual times

00:16:33.580 --> 00:16:39.420
spent in the memcache server
doing real work is probably

00:16:39.420 --> 00:16:42.558
something like 10 or 100
microseconds and everything

00:16:42.558 --> 00:16:46.000
else is just waiting for the
data to get from here

00:16:46.000 --> 00:16:47.840
or to there and back.

00:16:47.840 --> 00:16:51.400
Even though this is all of
course within one Google data

00:16:51.400 --> 00:16:53.080
center using the fastest
network and hardware

00:16:53.080 --> 00:16:56.480
available in the universe.

00:16:56.480 --> 00:17:02.380
So anyway, because App Engine
applications are very networked

00:17:02.380 --> 00:17:06.760
every operation you do, whether
it's memcache or datastore or

00:17:06.760 --> 00:17:09.720
something else goes out to a
server that does the

00:17:09.720 --> 00:17:12.940
actual work for you.

00:17:12.940 --> 00:17:16.780
The typical application is
slowed down by how many remote

00:17:16.780 --> 00:17:20.340
procedure calls it makes more
than by any other factor.

00:17:20.340 --> 00:17:22.820
Meaning if you're thinking
about sort of traditional

00:17:22.820 --> 00:17:25.580
optimization of your
application you might think of

00:17:25.580 --> 00:17:29.240
things like oh, I'm using an n
square algorithm where I really

00:17:29.240 --> 00:17:30.580
should be using a
log n algorithm.

00:17:31.840 --> 00:17:35.260
That is not the important thing
in optimizing your web apps.

00:17:35.260 --> 00:17:37.960
Well yes, it could be,
but it's unlikely.

00:17:37.960 --> 00:17:41.960
It's much more likely that
you're making 10 RPC calls to

00:17:41.960 --> 00:17:44.920
get the same object over and
over because you have some bug

00:17:44.920 --> 00:17:49.580
in your application or you're
making 10 RPC calls to fetch 10

00:17:49.580 --> 00:17:51.990
different objects that you
could just as well fetch in

00:17:51.990 --> 00:17:55.520
a single batching RPC call.

00:17:55.520 --> 00:17:59.780
So this is why ROC
instrumentation is so important

00:17:59.780 --> 00:18:01.474
and why I recommended that
you all try this out.

00:18:03.760 --> 00:18:07.820
So what does Appstats
use under the hood?

00:18:07.820 --> 00:18:10.380
How is it implemented?

00:18:10.380 --> 00:18:13.980
I have way more information
about this, but I decided

00:18:13.980 --> 00:18:17.520
that there wasn't
enough time in my talk.

00:18:17.520 --> 00:18:22.040
I don't want to go 30 minutes
over like the keynote.

00:18:23.260 --> 00:18:27.100
So the one thing I want to
mention, there's a general

00:18:27.100 --> 00:18:31.738
feature in App Engine, general
API, called call hooks where

00:18:31.738 --> 00:18:35.578
any part of user code can
register a user function to be

00:18:35.578 --> 00:18:39.880
called right at the
start of an RPC.

00:18:39.020 --> 00:18:43.880
The function is called with a
number of arguments that tell

00:18:43.880 --> 00:18:47.558
it exactly which kind of RPC
is being made and with

00:18:47.558 --> 00:18:49.380
which request buffer.

00:18:49.380 --> 00:18:51.738
And there's another function
you can register that gets

00:18:51.738 --> 00:18:54.740
called when the RPC returns.

00:18:54.740 --> 00:18:57.200
And that function also has the
response buffer of course and

00:18:57.200 --> 00:19:00.920
it has information about
whether the request actually

00:19:00.920 --> 00:19:02.800
succeeded or it failed.

00:19:02.800 --> 00:19:08.380
This is most of the magic to
writing Appstats because all

00:19:08.380 --> 00:19:12.380
we do is in the precall
hook as we call it.

00:19:12.380 --> 00:19:16.000
We record the content of the
request buffer and we crawl

00:19:16.000 --> 00:19:18.580
down the stack to record all
the stack frames and

00:19:18.580 --> 00:19:20.460
local variables.

00:19:20.460 --> 00:19:23.400
And then in the post call hook
we match that up with the

00:19:23.400 --> 00:19:25.480
response and the error status
and then we write

00:19:25.480 --> 00:19:26.480
that to memory.

00:19:26.480 --> 00:19:29.880
This is just written to main
memory and in a typical

00:19:29.880 --> 00:19:34.320
situation over the duration
of single request maybe

00:19:34.320 --> 00:19:36.860
you collect a few
megabytes of data.

00:19:36.860 --> 00:19:40.700
In modern processors that's
absolutely not a problem.

00:19:40.700 --> 00:19:43.300
At the end of the request we
gather all that data that we've

00:19:43.300 --> 00:19:48.200
collected, serialize it and
write it to memcachel and so of

00:19:48.200 --> 00:19:49.678
course, we practice
what we preach.

00:19:49.678 --> 00:19:52.140
We write two different pieces
of information to memcache.

00:19:52.140 --> 00:19:55.838
We write a short summary
record that is used

00:19:55.838 --> 00:19:57.078
by the summary page.

00:19:57.078 --> 00:19:59.180
Let me see, what is
the summary page?

00:19:59.180 --> 00:20:00.900
This is the summary page.

00:20:00.900 --> 00:20:03.480
So it has information
about a whole bunch of

00:20:03.480 --> 00:20:04.538
different requests.

00:20:04.538 --> 00:20:09.220
So the way it gets this is by
crawling memcache looking for

00:20:09.220 --> 00:20:10.740
every single summary record.

00:20:11.800 --> 00:20:14.920
So the summary records have to
be small because memcache

00:20:14.920 --> 00:20:17.618
doesn't have to give you a way
of say, only return the first

00:20:17.618 --> 00:20:19.360
100 bytes of each record.

00:20:19.360 --> 00:20:22.540
On the other hand, the long
details record of course is

00:20:22.540 --> 00:20:26.980
used to reconstruct what you
see in this page, which

00:20:26.980 --> 00:20:27.720
has much more information.

00:20:27.720 --> 00:20:31.880
Look at all those stack traces.

00:20:31.880 --> 00:20:34.358
There's a lot of data there.

00:20:34.358 --> 00:20:37.600
Anyway, this data is
written to memcache.

00:20:39.260 --> 00:20:43.260
That's the end of the
information gathering stage.

00:20:43.260 --> 00:20:47.560
Then later at your leisure-- as
long as it hasn't expired out

00:20:47.560 --> 00:20:51.180
of memcache of course-- you
point your browser at the UI,

00:20:51.180 --> 00:20:55.200
which I mentioned before and
the UI just gets the

00:20:55.200 --> 00:20:58.180
information from memcache
and displays it.

00:20:58.180 --> 00:21:01.540
It's the little Django
template based application.

00:21:01.540 --> 00:21:04.640
It's actually very simple.

00:21:04.640 --> 00:21:07.020
Now everyone's question and I
know this was the first

00:21:07.020 --> 00:21:11.540
question that showed up on the
wave a few days ago actually,

00:21:11.540 --> 00:21:13.720
what is that overhead
of Appstats?

00:21:16.500 --> 00:21:19.118
Well, I was interested in this
myself so I made sure that

00:21:19.118 --> 00:21:21.400
I could figure it out.

00:21:21.400 --> 00:21:24.800
First of all, the callback
functions that are used to

00:21:24.800 --> 00:21:29.680
record the information actually
sort of start a separate clock

00:21:29.680 --> 00:21:33.618
ticking when the function is
called and when the recording

00:21:33.618 --> 00:21:37.180
function itself is finished
recording and so we keep track

00:21:37.180 --> 00:21:42.020
of how much time we spend
recording and we log that

00:21:42.020 --> 00:21:43.420
at the end of the request.

00:21:43.420 --> 00:21:49.560
We also log how much time
actually to write the complete

00:21:49.560 --> 00:21:51.180
set of data to memcache.

00:21:52.560 --> 00:21:56.820
So I highlighted every
request that actually

00:21:56.820 --> 00:21:58.900
has Appstats activated.

00:21:59.440 --> 00:22:03.480
Logs one line in the
application logs.

00:22:03.480 --> 00:22:06.240
You can see this very easily in
the administration console, but

00:22:06.240 --> 00:22:13.740
just by going to logs and
clicking on show all records

00:22:13.740 --> 00:22:21.560
with level info or better.

00:22:21.560 --> 00:22:24.780
And so you'll see these
messages that say, saved.

00:22:24.780 --> 00:22:27.480
That just means something was
saved by Appstats to memcache.

00:22:27.480 --> 00:22:30.680
Actually for debugging
purpose it mostly shows

00:22:30.680 --> 00:22:32.320
the memcache key.

00:22:32.320 --> 00:22:36.120
Then it shows the size of
the small record it wrote.

00:22:36.120 --> 00:22:38.420
The summary is typically
maybe 100 bytes, 150.

00:22:39.540 --> 00:22:41.740
Then it shows the size
of the full record, 100

00:22:41.740 --> 00:22:42.780
K is pretty typical.

00:22:42.780 --> 00:22:44.020
Then it shows the overhead.

00:22:44.020 --> 00:22:48.480
So in this case we only spent
four milliseconds recording the

00:22:48.480 --> 00:22:52.560
data, but then we spent
another, almost 50 milliseconds

00:22:52.560 --> 00:22:54.660
writing that data to memcache
because here analyzing

00:22:54.660 --> 00:22:57.520
100K takes a little time.

00:22:57.520 --> 00:23:02.960
It also actually logs a link
back into the Appstats UI

00:23:02.960 --> 00:23:05.660
if you've done it right.

00:23:05.660 --> 00:23:07.500
Now let's do a
little calculation.

00:23:07.500 --> 00:23:10.940
How much data is going
to put in memcache?

00:23:10.940 --> 00:23:13.800
Now it's important to
understand that Appstats

00:23:13.800 --> 00:23:15.660
doesn't sort of record
an infinite amount of

00:23:15.660 --> 00:23:16.180
data in the memcache.

00:23:16.180 --> 00:23:19.560
That would be bad because we'd
sort of be pushing all your

00:23:19.560 --> 00:23:24.540
applications data that is
useful for actually making your

00:23:24.540 --> 00:23:26.440
application faster, we'd be
pushing that out of memcache

00:23:26.440 --> 00:23:27.580
if we filled it up.

00:23:27.580 --> 00:23:33.000
So we actually only record the
most recent 1,000 requests.

00:23:33.000 --> 00:23:37.200
The way we do that is actually
by cleverly hashing the time

00:23:37.200 --> 00:23:41.860
stamp of the request to a
memcache key, so all the

00:23:41.860 --> 00:23:45.580
requests at some point those
keys start rolling over

00:23:45.580 --> 00:23:46.660
and you'll just be
overwriting old logs.

00:23:50.660 --> 00:23:53.060
And of course, the actual time
stamp is also contained in

00:23:53.060 --> 00:24:02.180
the records so the UI knows
what the actual time was.

00:24:02.180 --> 00:24:07.620
It doesn't only have
the hash of the time.

00:24:07.620 --> 00:24:10.900
So if we store 1,000 keys and
a typical key stores about

00:24:10.900 --> 00:24:15.900
100 kilobytes of data that
would be 100 megabytes.

00:24:16.618 --> 00:24:21.620
In the grand scheme of memcache
I wouldn't call that a drop

00:24:21.620 --> 00:24:24.020
in the bucket, but it's
certainly not very much.

00:24:24.500 --> 00:24:28.080
The typical amount of memcache
data that you're allowed to

00:24:28.080 --> 00:24:30.520
have is much larger than that.

00:24:30.700 --> 00:24:34.840
So we'll still have the
questions, so well, is it fast

00:24:34.840 --> 00:24:39.198
enough if my application is
super, extremely popular?

00:24:39.198 --> 00:24:41.818
Now Rietveld is not super,
extremely popular.

00:24:41.818 --> 00:24:46.318
It has maybe one or a few
QPS in the best of cases.

00:24:46.318 --> 00:24:49.338
For Rietveld we've turned it
on, I think about half a year

00:24:49.338 --> 00:24:51.960
ago and we've never gone back.

00:24:51.960 --> 00:24:56.820
We've just kept it on all the
time and it works great.

00:24:56.940 --> 00:24:59.440
Actually, I looked recently
at the logs and the

00:24:59.440 --> 00:25:03.098
typical request about 20
milliseconds at the most is

00:25:03.098 --> 00:25:04.358
attributable to overhead.

00:25:04.358 --> 00:25:08.680
So since the typical request
in Rietveld takes about

00:25:08.680 --> 00:25:09.560
half a second to run.

00:25:10.300 --> 00:25:14.740
That is not the
important at all.

00:25:14.740 --> 00:25:19.280
Now if you have a very popular
application, if you're like 100

00:25:19.280 --> 00:25:25.360
or 1,000 QPS you might not
want to store data about

00:25:25.360 --> 00:25:26.598
every single request.

00:25:27.300 --> 00:25:29.260
First of all, you've probably
already done some optimization

00:25:29.260 --> 00:25:36.320
to get to that level of QPS,
but moreover if you're only

00:25:36.320 --> 00:25:40.818
saving the 1,000 most recent
requests you'll end up having a

00:25:40.818 --> 00:25:42.660
fairly random sampling
of requests anyway.

00:25:43.040 --> 00:25:47.000
So what you can do there is
actually use a specific

00:25:47.000 --> 00:25:51.380
configuration parameter for
Appstats that lets you say,

00:25:51.380 --> 00:25:57.020
only record randomly say 1% or
5% of all requests or 0.1%.

00:25:58.180 --> 00:26:03.078
And by using that you'll sort
of completely reduce the amount

00:26:03.078 --> 00:26:09.618
of overhead that Appstats
applies to your application

00:26:09.618 --> 00:26:13.180
because it's like the overhead
for one request times like

00:26:13.180 --> 00:26:15.980
1%, which is almost nothing.

00:26:15.980 --> 00:26:20.440
Now if you want to know
how much this costs, 20

00:26:20.440 --> 00:26:26.540
milliseconds every second for
example, I think would be

00:26:26.540 --> 00:26:28.560
under $1 of CPU time or so.

00:26:30.118 --> 00:26:36.000
If you turn this on-- and I'm
not sure I'm even doing this

00:26:36.000 --> 00:26:40.140
calculation right, but I did
look at how much you would

00:26:40.140 --> 00:26:44.960
actually pay in dollar costs
for this and it's only a very

00:26:44.960 --> 00:26:47.640
small amount of money each day
even if you turn it on full

00:26:47.640 --> 00:26:50.180
speed, so I wouldn't
worry too much.

00:26:50.980 --> 00:26:55.598
At the same time, you might
try it out first in SDK.

00:26:57.040 --> 00:26:59.780
When I first wrote the tool I
actually learned more from

00:26:59.780 --> 00:27:03.720
pointing it at my application
when running in the SDK because

00:27:03.720 --> 00:27:06.960
even a single request, it was
so obvious which things I was

00:27:06.960 --> 00:27:11.480
doing wrong that I didn't even
have to run it in production

00:27:11.480 --> 00:27:14.118
for a long time.

00:27:14.118 --> 00:27:19.400
If you want to configure
Appstats in Python it's four

00:27:19.400 --> 00:27:22.520
lines of Python code that you
can literally just cut and

00:27:22.520 --> 00:27:24.300
paste from the documentation.

00:27:24.300 --> 00:27:28.480
There's no additional tailoring
for your application required.

00:27:28.480 --> 00:27:32.420
You put these four lines into a
file named appengine_config.py

00:27:32.420 --> 00:27:35.600
in the top level directory
of your application.

00:27:35.600 --> 00:27:39.298
And deploy your application
and now that application

00:27:39.298 --> 00:27:39.960
is recording.

00:27:39.960 --> 00:27:43.940
You can check the logs
in admin console.

00:27:43.940 --> 00:27:46.340
Make sure to look for
info level logs and you

00:27:46.340 --> 00:27:49.320
can check that it's
actually saving stuff.

00:27:49.560 --> 00:27:51.990
To be able to view the user
interface you have to

00:27:51.990 --> 00:27:52.480
add these two lines.

00:27:52.480 --> 00:27:55.780
Again, literally these two
lines do your app load YML

00:27:55.780 --> 00:27:59.540
file and them point your
browser to slash stats.

00:27:59.758 --> 00:28:04.140
If somehow slash stats is
already a URL in your

00:28:04.140 --> 00:28:06.400
application you can actually
map it to something else as

00:28:06.400 --> 00:28:10.360
long as in the end
it ends in stats.

00:28:11.140 --> 00:28:16.280
The UI script is smart enough
to only let administrators

00:28:16.280 --> 00:28:19.318
access this data, so you don't
have to worry that your users

00:28:19.318 --> 00:28:21.978
are going to look at the
statistics of your app

00:28:21.978 --> 00:28:25.840
and figure out a DOS
attack or something.

00:28:25.840 --> 00:28:28.880
In Java the mechanism
is different.

00:28:29.260 --> 00:28:32.270
It's adding 10 or
20 lines of XML.

00:28:32.270 --> 00:28:33.200
The basic ideas is the same.

00:28:33.200 --> 00:28:35.720
Very simple cut and paste,
redeploy and you're

00:28:35.720 --> 00:28:36.700
in business.

00:28:36.700 --> 00:28:39.100
And for details see the
Java documentation.

00:28:39.100 --> 00:28:45.400
Just type Java App Enginge,
Appstats in your browser

00:28:45.400 --> 00:28:47.860
and you'll find it.

00:28:47.860 --> 00:28:49.780
For Python users
there's one caveat.

00:28:49.780 --> 00:28:53.460
When we first released two
years ago all our documentation

00:28:53.460 --> 00:28:57.960
had as an example, sort of in
their hello world example code

00:28:57.960 --> 00:29:03.340
something called wsgiref
handler CGI handler as the way

00:29:03.340 --> 00:29:06.680
to sort of start your
application assuming your

00:29:06.680 --> 00:29:08.980
application is using the WSGI
framework, which all Python

00:29:08.980 --> 00:29:09.820
apps pretty much do.

00:29:09.820 --> 00:29:10.920
All Python web frameworks.

00:29:10.920 --> 00:29:14.080
That doesn't actually
work right.

00:29:14.520 --> 00:29:17.340
There's some other reasons why
WSGI ref dressed handlers is

00:29:17.340 --> 00:29:21.200
not a good thing to use so
there's a web app specific

00:29:21.200 --> 00:29:25.920
funtion, run_wsgi_app that you
have to use amd even though you

00:29:25.920 --> 00:29:31.000
import this from web app, which
is App Engines own sort of

00:29:31.000 --> 00:29:33.920
miniature web framework, it
doesn't mean that you have to

00:29:33.920 --> 00:29:37.540
use that web framework for the
rest of your application.

00:29:37.660 --> 00:29:41.040
You can still use Django
or whatever you like.

00:29:41.040 --> 00:29:45.078
So there's more you can do in
the configuration and I'm

00:29:45.078 --> 00:29:48.260
actually not going to say that
much about this except that you

00:29:48.260 --> 00:29:54.600
download the SDK, search for
the Appstats and then in that

00:29:54.600 --> 00:29:56.300
Appstats directory find
a file named

00:29:56.300 --> 00:30:02.778
sample_appengine_config.py and
it has commented out blocks of

00:30:02.778 --> 00:30:06.880
configuration variables with
explanations of exactly why

00:30:06.880 --> 00:30:07.240
they're there.

00:30:07.240 --> 00:30:10.280
And using this you can sort
of change things like

00:30:10.280 --> 00:30:11.480
the key rotation scheme.

00:30:11.480 --> 00:30:16.980
If you want to spend more time,
more memcache space to say for

00:30:16.980 --> 00:30:20.400
example 10,000 keys you can
change to key rotation scheme

00:30:20.400 --> 00:30:24.840
If you want to reduce the space
used up for stack frames.

00:30:24.840 --> 00:30:27.540
You can sort of change the
maximum stack depth that is

00:30:27.540 --> 00:30:32.038
recorded reported or how many
variables per frame or how

00:30:32.038 --> 00:30:37.020
many bytes to store for
each variable maximally.

00:30:37.160 --> 00:30:41.220
You can tell it to skip certain
frames or you can tell it to

00:30:41.220 --> 00:30:44.880
select only certain events and
the randomized fraction of

00:30:44.880 --> 00:30:46.940
all events is actually
a special case of that.

00:30:46.940 --> 00:30:50.680
You can select on things like,
what user was logged in or what

00:30:50.680 --> 00:30:55.800
IP address did it come from or
the presence of a certain query

00:30:55.800 --> 00:30:59.440
parameter or anything else
you can gather out of the

00:30:59.440 --> 00:31:00.940
request headers actually.

00:31:00.940 --> 00:31:04.020
And then there's the path
normalization, which is

00:31:04.020 --> 00:31:07.480
probably best shown
by looking at this.

00:31:07.480 --> 00:31:10.940
So these x's I put in by
a little function that

00:31:10.940 --> 00:31:12.600
does path normalization.

00:31:12.600 --> 00:31:14.800
Because in Rietveld every
issue has its own URL.

00:31:15.300 --> 00:31:18.560
Because we wanted sort of the
same code is responsible

00:31:18.560 --> 00:31:19.240
for those of course.

00:31:19.240 --> 00:31:24.000
So we want to map all these to
the same key and so slash 1, 2,

00:31:24.000 --> 00:31:28.998
3, 4, 5 slash div slash 1 slash
2 is mapped to slash x slash

00:31:28.998 --> 00:31:33.240
div slash x slash x and let's
look at the other guy.

00:31:35.240 --> 00:31:36.408
Lost it.

00:31:36.408 --> 00:31:37.576
Doesn't matter.

00:31:38.660 --> 00:31:43.260
OK, so that's the stuff you can
do with advanced configuration

00:31:43.260 --> 00:31:48.078
and I see we have almost half
an hour left so say 10-15

00:31:48.078 --> 00:31:48.460
minutes for questions.

00:31:48.460 --> 00:31:52.920
That gives me 10-15
minutes for some specific

00:31:52.920 --> 00:31:54.098
performance issues.

00:31:54.098 --> 00:32:00.038
And first I sort of want to get
on my soapbox and say you, if

00:32:00.038 --> 00:32:04.140
you ever wrote an App Engine
app there is most likely a

00:32:04.140 --> 00:32:07.358
performance issue in your app
that you're not familiar with.

00:32:07.358 --> 00:32:13.080
And the reason is that when
you're testing your app you're

00:32:13.080 --> 00:32:16.580
thinking much more about
correctness than about

00:32:16.580 --> 00:32:17.360
performance in most cases.

00:32:17.360 --> 00:32:20.960
Now of course, after
correctness comes performance,

00:32:20.960 --> 00:32:24.500
but I hope we've all learned
that thinking about performance

00:32:24.500 --> 00:32:28.380
before you've even reached
correctness is ludicrous.

00:32:28.380 --> 00:32:30.780
So things like unit testing
frameworks make it very

00:32:30.780 --> 00:32:35.700
easy to check for correct
working of her application.

00:32:35.700 --> 00:32:40.980
Not so easy to check for sort
of maintained performance

00:32:40.980 --> 00:32:42.160
characteristics.

00:32:42.160 --> 00:32:45.000
Typical testing procedures,
whether it's manual testing or

00:32:45.000 --> 00:32:48.880
automated testing doesn't
really look for performance

00:32:48.880 --> 00:32:51.640
issues specifically.

00:32:51.640 --> 00:32:55.480
Also, often the performance
issues are sort of a little

00:32:55.480 --> 00:32:57.460
bit here, a little bit there.

00:32:58.720 --> 00:33:01.100
Two gets you could
combine into one here.

00:33:01.100 --> 00:33:04.000
A get over there that you
don't need at all or only

00:33:04.000 --> 00:33:06.860
in certain cases there.

00:33:06.860 --> 00:33:13.078
And it's hard to sort of tell
the difference between my app

00:33:13.078 --> 00:33:15.880
is slow because well yes, I'm
using it on the internet.

00:33:15.880 --> 00:33:18.420
Of course the internet,
sometimes it's slow.

00:33:18.420 --> 00:33:20.660
It could be the wireless
in the building.

00:33:20.660 --> 00:33:24.600
It could be some piece of
server infrastructure that you

00:33:24.600 --> 00:33:29.460
have no control over or maybe
your application is actually

00:33:29.460 --> 00:33:32.058
doing something very
inefficient with remote
606
00:33:32,058 --&gt; 00:33:32,038
procedure calls.

00:33:32.038 --> 00:33:35.558
All those things sort of
blur together so you don't

00:33:35.558 --> 00:33:37.860
necessarily look at that.

00:33:37.860 --> 00:33:41.300
So here is, in a before and
after pictures quickly,

00:33:41.300 --> 00:33:43.018
why it matters.

00:33:43.018 --> 00:33:44.900
And this is actually from
a blog by Jens Scheffler.

00:33:47.720 --> 00:33:51.720
This is a very simple demo app
and the first version takes

00:33:51.720 --> 00:33:55.740
about almost 400 milliseconds
real time to execute.

00:33:55.740 --> 00:33:59.100
And what it does is it fetches
20 objects from the datastore.

00:33:59.720 --> 00:34:00.280
One at the time.

00:34:00.280 --> 00:34:02.700
So it's a four loop and
in the four loop is a

00:34:02.700 --> 00:34:04.960
datastore get call.

00:34:04.960 --> 00:34:07.880
And the second version, which
performs exactly the same

00:34:07.880 --> 00:34:11.440
amount of work, it also gets
those very same 20 objects, but

00:34:11.440 --> 00:34:16.340
it uses a single batch RPC to
pass 20 keys to the datastore

00:34:16.340 --> 00:34:18.440
at once and now it only takes
200 milliseconds to execute.

00:34:18.440 --> 00:34:23.798
It's sort of an indirect proof
that this is actually doing the

00:34:23.798 --> 00:34:25.458
same thing as the red bars.

00:34:25.458 --> 00:34:28.078
The red bars represent how much
you're being charged for all

00:34:28.078 --> 00:34:32.318
those datastore operations
and to some extent for the

00:34:32.318 --> 00:34:34.678
CPU in your application.

00:34:34.678 --> 00:34:38.978
So the red bar here is about
500 milliseconds long meaning

00:34:38.978 --> 00:34:42.858
that you're charged 500 virtual
milliseconds for those

00:34:42.858 --> 00:34:45.738
operations and here it's
actually 440 I think within the

00:34:45.738 --> 00:34:53.138
natural variability of
the cost of requests.

00:34:53.138 --> 00:34:55.158
Because the datastore is
actually fetching the same 20

00:34:55.158 --> 00:34:58.578
objects for you it's natural
that you're being charged

00:34:58.578 --> 00:35:00.718
the same amount.

00:35:00.718 --> 00:35:03.800
However it's doing it twice as
fast at only 200 milliseconds

00:35:03.800 --> 00:35:04.400
rather than 400 hundred.

00:35:04.400 --> 00:35:08.740
So your users see a huge
improvement and so this

00:35:08.740 --> 00:35:11.238
is always important.

00:35:12.158 --> 00:35:16.118
So issue number one and this is
sort of the most basic one.

00:35:16.118 --> 00:35:19.280
This is where you start off
when you're first looking at

00:35:19.280 --> 00:35:22.598
performance for an app that
you haven't looked at

00:35:22.598 --> 00:35:23.460
performance at all.

00:35:23.460 --> 00:35:26.738
The problem is you don't
actually know what to cache.

00:35:26.738 --> 00:35:27.960
So what do you do?

00:35:27.960 --> 00:35:32.300
Well you could start thinking
about it, sort of analyze, what

00:35:32.300 --> 00:35:35.518
requests are the users making?

00:35:35.518 --> 00:35:39.598
And try to follow the code path
through your application.

00:35:39.598 --> 00:35:42.218
And very quickly you get lost
because you don't actually know

00:35:42.218 --> 00:35:45.558
if it's taking this or that
branch of a particular if.

00:35:45.558 --> 00:35:49.098
What you do instead is you link
your application with Appstats,

00:35:49.098 --> 00:35:52.578
configure it, run it and now
you'll see immediately which

00:35:52.578 --> 00:35:56.860
parts of your application are
used, are making the most

00:35:56.860 --> 00:35:58.638
RPC calls and now you know
where to start optimizing.

00:35:59.860 --> 00:36:03.440
And often you didn't even know
that every single request was

00:36:03.440 --> 00:36:04.900
touching a certain part
of the datastore.

00:36:05.060 --> 00:36:09.118
That's one of the things that
always surprises me, how many

00:36:09.118 --> 00:36:14.780
sort of hidden requests are in
some generic piece of code that

00:36:14.780 --> 00:36:17.238
you sort of forget about
because you wrote it on the

00:36:17.238 --> 00:36:20.638
first day of development.

00:36:20.638 --> 00:36:23.818
Now you start adding some
caching code and the second

00:36:23.818 --> 00:36:27.778
bug-- and this is actually a
state that I have to admit that

00:36:27.778 --> 00:36:30.800
shockingly when I started
looking at RPC performance,

00:36:30.800 --> 00:36:33.100
Rietveld had actually been in
this state for about

00:36:33.100 --> 00:36:34.238
three months.

00:36:34.238 --> 00:36:39.058
We had sort of done some
analysis of where is it slow

00:36:39.058 --> 00:36:43.458
without Appstats and we had put
some caching in and we had

00:36:43.458 --> 00:36:46.360
actually made the right guess
about where to put the caching

00:36:46.360 --> 00:36:50.558
in and the caching was
completely working except after

00:36:50.558 --> 00:36:53.058
we successfully retrieved an
object from the cache we were

00:36:53.058 --> 00:36:56.658
also still retrieving the same
object from the datastore

00:36:56.658 --> 00:37:01.598
through some little bug in a
boolean variable that was not

00:37:01.598 --> 00:37:04.508
set correctly or a function
that wasn't returning the

00:37:04.508 --> 00:37:08.478
right true or false result.

00:37:08.478 --> 00:37:11.738
We had not thought of this
possibility in the code.

00:37:11.738 --> 00:37:14.260
We had actually checked, we
thought we'd check that the

00:37:14.260 --> 00:37:16.478
code was working, caching
correctly by putting

00:37:16.478 --> 00:37:17.860
various log messages in.

00:37:17.860 --> 00:37:19.620
So the logs showed us
yes it's retrieving the

00:37:19.620 --> 00:37:21.640
value from the cache.

00:37:21.640 --> 00:37:22.280
It's working.

00:37:22.280 --> 00:37:26.078
We didn't actually go to the
point of measuring was it also

00:37:26.078 --> 00:37:32.098
faster and Appstats immediately
pointed out that it wasn't.

00:37:32.098 --> 00:37:36.280
So a very specific case of
that, this came up a few weeks

00:37:36.280 --> 00:37:40.078
ago in a simple internal
app that a colleague wrote.

00:37:42.800 --> 00:37:44.760
This is typical caching code.

00:37:44.760 --> 00:37:45.918
You try to get something
from memcache.

00:37:47.158 --> 00:37:50.178
If it's not in memcache then
you try to get it from the

00:37:50.178 --> 00:37:52.880
datastore, so in this
case what we're getting

00:37:52.880 --> 00:37:53.760
is actually a query.

00:37:53.760 --> 00:37:57.058
We're asking for all
state objects with

00:37:57.058 --> 00:37:58.800
a certain property.

00:37:58.800 --> 00:38:02.868
And to sort of protect the
innocent I'm not showing what

00:38:02.868 --> 00:38:03.158
the name of the property was.

00:38:04.478 --> 00:38:08.300
Then we put the result in the
datastore and then we started

00:38:08.300 --> 00:38:11.260
iterating over the results.

00:38:11.260 --> 00:38:15.478
Well, what you forget except
you can see it if you look at

00:38:15.478 --> 00:38:19.400
the bottom of the slide is
that this is not actually

00:38:19.400 --> 00:38:20.998
storing the query results.

00:38:20.998 --> 00:38:23.758
This store is caching
a query object.

00:38:23.758 --> 00:38:28.600
A query object is just a little
piece of memory that stores

00:38:28.600 --> 00:38:30.320
what things to query for.

00:38:30.320 --> 00:38:33.760
It doesn't actually interact
with the datastore directory.

00:38:33.760 --> 00:38:38.098
So this call probably took
about 100 microseconds to

00:38:38.098 --> 00:38:38.678
create that query object.

00:38:39.218 --> 00:38:44.158
Then we cache that query object
and then the query object

00:38:44.158 --> 00:38:47.958
either retrieved from the cache
or recently constructed is

00:38:47.958 --> 00:38:48.498
used to execute the query.

00:38:48.498 --> 00:38:50.158
This is where all
the time goes.

00:38:50.158 --> 00:38:54.150
The solution of course is
to know that this sort of

00:38:54.150 --> 00:38:55.118
little pitfall exists.

00:38:57.098 --> 00:39:01.338
By other analysis we know and
by sort of bug reports about

00:39:01.338 --> 00:39:04.118
seemingly unrelated things we
know that this is a very common

00:39:04.118 --> 00:39:06.920
bug in our users applications.

00:39:06.920 --> 00:39:10.078
So think about whether you're
actually caching query results

00:39:10.078 --> 00:39:10.838
and not query objects.

00:39:12.398 --> 00:39:15.278
The solution of course is to
actually fetch the query

00:39:15.278 --> 00:39:20.138
results and then up cache those
results because that is what

00:39:20.138 --> 00:39:22.138
the author of the code
clearly intended.

00:39:22.138 --> 00:39:26.100
Caching a query object is
completely useless because

00:39:26.100 --> 00:39:28.620
computing a query object never
takes more than a fraction

00:39:28.620 --> 00:39:31.218
of a millisecond.

00:39:31.218 --> 00:39:36.958
OK so now we're getting into
very different territory.

00:39:36.958 --> 00:39:38.918
Everybody loves abstraction.

00:39:38.918 --> 00:39:42.780
Everybody loves to write sort
of little sub packages that

00:39:42.780 --> 00:39:45.898
take care of a certain problem
and other sub packages that

00:39:45.898 --> 00:39:48.498
take care of a different
problem and they don't talk to

00:39:48.498 --> 00:39:51.700
each other because that way you
reduce coupling between your

00:39:51.700 --> 00:39:55.260
code and then you layer other
things on top of these and now

00:39:55.260 --> 00:40:01.318
suddenly your application
is very slow.

00:40:01.318 --> 00:40:05.858
And I love abstraction myself
but I also think it's very

00:40:05.858 --> 00:40:09.880
important to be able to see
through abstraction and sort

00:40:09.880 --> 00:40:15.860
of look through different
layers and find out what is

00:40:15.860 --> 00:40:16.958
happening to performance.

00:40:16.958 --> 00:40:19.818
And so a typical problem that
can happen is that two

00:40:19.818 --> 00:40:22.238
different parts of an
application are independently

00:40:22.238 --> 00:40:24.538
deciding that they need a
certain object from the data

00:40:24.538 --> 00:40:29.340
store and they fetch it, but
through completely different

00:40:29.340 --> 00:40:31.898
means they end up fetching
the same object.

00:40:32.218 --> 00:40:34.898
Which means two RPCs where
you only needed one if you

00:40:34.898 --> 00:40:36.598
had the proper caching.

00:40:36.598 --> 00:40:40.258
I'll leave it up to you to
decide exactly how to solve

00:40:40.258 --> 00:40:41.638
the caching problem.

00:40:42.280 --> 00:40:44.640
You could change the
abstraction so that those

00:40:44.640 --> 00:40:47.840
results are passed back and
forth by the higher level logic

00:40:47.840 --> 00:40:52.178
or you could put in some kind
of low-level caching, write

00:40:52.178 --> 00:40:54.620
your own little in memory
caching solution.

00:40:54.620 --> 00:40:58.860
The one thing I want to point
out is that the solution for

00:40:58.860 --> 00:41:01.538
this particular problem is
never to use memcache because

00:41:01.538 --> 00:41:04.238
now you're just replacing
redundant datastore calls with

00:41:04.238 --> 00:41:08.800
redundant memcache calls and
remember those memcache calls

00:41:08.800 --> 00:41:11.638
still take 5 or 10 milliseconds
where you probably could

00:41:11.638 --> 00:41:14.260
just find a pointer to the
object in 1 microsecond.

00:41:17.258 --> 00:41:20.658
OK, issue number five.

00:41:21.198 --> 00:41:25.328
This is the staircase pattern
that I showed at the start of

00:41:25.328 --> 00:41:27.738
this section of the talk.

00:41:27.738 --> 00:41:31.098
Just looping over datastore
keys and fetching items one at

00:41:31.098 --> 00:41:35.460
a time where you could actually
make use of a batch call.

00:41:35.460 --> 00:41:39.220
And this is a problem both when
using the datastore, it's also

00:41:39.220 --> 00:41:41.120
a problem when using memcache.

00:41:42.698 --> 00:41:45.118
Again, with memcache and
actually in fact with memcache

00:41:45.118 --> 00:41:49.318
in particular because most of
the time you spend waiting for

00:41:49.318 --> 00:41:53.198
memcache is actually just
network IO, fetching a hundred

00:41:53.198 --> 00:41:56.658
items from memcache is just as
fast as fetching one item

00:41:56.658 --> 00:41:59.358
assuming that the result
fits in a megabyte.

00:42:00.778 --> 00:42:04.138
And so use those batching APIs,
know they exist and for the

00:42:04.138 --> 00:42:08.738
database the typical batching
API just passes a list of

00:42:08.738 --> 00:42:09.898
keys instead of a key.

00:42:09.898 --> 00:42:14.240
From memcache there are
separate multicalls with APIs

00:42:14.240 --> 00:42:17.620
with the word multi in their
name that take a list of keys

00:42:17.620 --> 00:42:21.718
and then return a dictionary
of mapping keys to values.

00:42:21.718 --> 00:42:25.178
Jens Scheffler wrote very
entertainingly about this in

00:42:25.178 --> 00:42:29.218
his patterns of doom blog.

00:42:29.218 --> 00:42:35.058
OK, I think I have two
more specific batching

00:42:35.058 --> 00:42:37.118
external URL requests.

00:42:37.478 --> 00:42:41.178
There's not really a very
simple fetch multiple URLs

00:42:41.178 --> 00:42:46.598
call, but there is an
asynchronous API for URL fetch.

00:42:46.598 --> 00:42:50.878
And here again is a little
before and after thing.

00:42:51.238 --> 00:42:55.078
The before we were making I
think, about five URL fetch

00:42:55.078 --> 00:42:58.358
calls, one at a time and you
see that the total time spent

00:42:58.358 --> 00:43:01.598
there is over 1,300
milliseconds because

00:43:01.598 --> 00:43:03.578
each of those calls are
200-300 milliseconds.

00:43:04.318 --> 00:43:08.218
Here we use the asynchronous
URL fetch API.

00:43:08.218 --> 00:43:12.038
Just look that up in the
documentation or look at Jens'

00:43:12.038 --> 00:43:14.538
blog for a sample code.

00:43:14.538 --> 00:43:18.158
And all those URL fetches just
execute in peril, you shoot

00:43:18.158 --> 00:43:24.198
five of them at once and five
different servers execute them

00:43:24.198 --> 00:43:28.540
completely oblivious of each
other and return five results

00:43:28.540 --> 00:43:33.020
and you have the same set of
results in 1/5 of the

00:43:33.020 --> 00:43:35.918
time effectively.

00:43:36.138 --> 00:43:40.918
And while you're waiting
for those URL fetch calls

00:43:40.918 --> 00:43:43.840
to complete you could
even do other stuff.

00:43:45.898 --> 00:43:48.378
So if sort of the structure of
your application is I have to

00:43:48.378 --> 00:43:51.900
fetch a bunch of external URLs
and I also have to do a bunch

00:43:51.900 --> 00:43:55.980
of datastore and memcache
interaction you can do your

00:43:55.980 --> 00:43:57.900
datastore and memcache
interaction while you're

00:43:57.900 --> 00:44:00.620
waiting for the external
fetches to complete because

00:44:00.620 --> 00:44:03.960
these external fetches are
often like an order of

00:44:03.960 --> 00:44:07.438
magnitude slower than
datastore interactions.

00:44:07.438 --> 00:44:11.978
So the final one and actually I
want to mention this, but I

00:44:11.978 --> 00:44:14.058
really think you should just go
look it up in Nick

00:44:14.058 --> 00:44:16.158
Johnson's blog.

00:44:16.158 --> 00:44:19.580
Nick just disappeared,
otherwise I'd point him

00:44:19.580 --> 00:44:21.878
out and he was wearing
a yellow t-shirt.

00:44:21.878 --> 00:44:27.818
So this is another version
of the staircase of gets,

00:44:27.818 --> 00:44:31.600
caused in particular by
reference properties.

00:44:31.600 --> 00:44:35.378
A reference property is a very
nice thing in the db module

00:44:35.378 --> 00:44:41.338
that lets you sort of abstract
away the existence of keys in a

00:44:41.338 --> 00:44:45.000
datastore and use a more object
oriented model where for

00:44:45.000 --> 00:44:49.118
example, a blog post object has
an attribute that is a blog

00:44:49.118 --> 00:44:52.918
author object and this all
works out very nicely and you

00:44:52.918 --> 00:44:57.120
can sort of do a query, get 10
blog post results and then pass

00:44:57.120 --> 00:45:02.718
those blog post results to a
template and the template will

00:45:02.718 --> 00:45:06.100
fetch the blog author object
for each blog post and render

00:45:06.100 --> 00:45:10.718
the authors name, which
is retrieved from there.

00:45:11.018 --> 00:45:14.038
This is a very nice way of
normalizing your database.

00:45:14.178 --> 00:45:19.778
The only problem is that every
object where there is an

00:45:19.778 --> 00:45:24.258
attribute fetch of an author
object that is actually

00:45:24.258 --> 00:45:29.438
translated into a single
datastore get call because the

00:45:29.438 --> 00:45:33.900
reference property code is not
smart enough to realize that

00:45:33.900 --> 00:45:36.498
you're actually going to make a
similar call with a different

00:45:36.498 --> 00:45:41.718
key or perhaps the same key
from a different post object to

00:45:41.718 --> 00:45:44.558
retrieve a different or maybe
the same author object.

00:45:45.918 --> 00:45:48.898
You have an efficient query
and then you still have a

00:45:48.898 --> 00:45:50.878
staircase of gets after that.

00:45:50.878 --> 00:45:54.838
And Nick figured out that the
right solution is to use

00:45:54.838 --> 00:45:59.760
reference property prefectching
and this is actually the

00:45:59.760 --> 00:46:01.820
simplest version of his code.

00:46:01.820 --> 00:46:05.120
In his blog he presents
two more general

00:46:05.120 --> 00:46:07.698
versions of this code.

00:46:07.698 --> 00:46:13.118
It basically requires using
class properties and a

00:46:13.118 --> 00:46:15.078
special method named get
value for datastore.

00:46:16.658 --> 00:46:18.118
Look it up.

00:46:18.398 --> 00:46:20.138
I think if you Google for
reference property prefetching

00:46:20.138 --> 00:46:23.118
and app enginge you'll
certainly find his among the

00:46:23.118 --> 00:46:26.478
first two or three hits.

00:46:26.478 --> 00:46:28.598
It is a huge improvement
in your applications

00:46:28.598 --> 00:46:30.378
in many cases.

00:46:30.378 --> 00:46:31.198
So now we have
time for Q and A.

00:46:32.218 --> 00:46:36.218
The clock over there says
we have about 12 minutes.

00:46:36.280 --> 00:46:39.238
I have documentation links, but
I'm sure a search engine will

00:46:39.238 --> 00:46:41.778
find those much more quickly.

00:46:41.778 --> 00:46:44.458
I can bring up the wave
and I'm not sure what I'm

00:46:44.458 --> 00:46:49.158
supposed to do with it.

00:46:49.158 --> 00:46:55.038
So let's see what happens
if we refresh that.

00:46:55.038 --> 00:46:57.618
But in the meantime-- it looks
like the refresh it taking

00:46:57.618 --> 00:46:59.760
awhile-- if there's someone
in the audience who has a

00:46:59.760 --> 00:47:01.464
question please speak up.

00:47:10.554 --> 00:47:12.580
AUDIENCE: I think you
mentioned [INAUDIBLE].

00:47:23.938 --> 00:47:25.548
GUIDO VAN ROSSUM: Is there
any way to paralyze what?

00:47:25.548 --> 00:47:26.408
AUDIENCE: [INAUDIBLE].

00:47:28.198 --> 00:47:30.858
GUIDO VAN ROSSUM:
Funny you should ask.

00:47:30.858 --> 00:47:35.558
It isn't yet a standard app
engine feature, but there is a

00:47:35.558 --> 00:47:39.698
third party company that has
made available and

00:47:39.698 --> 00:47:42.558
unfortunately I've sort of
mislaid the name of the

00:47:42.558 --> 00:47:46.340
company, but I'm sure that
searching for asynchronous

00:47:46.340 --> 00:47:49.860
datastore requests will
turn something up.

00:47:49.860 --> 00:47:53.838
There is an open source library
that lets you do asynchronous

00:47:53.838 --> 00:47:55.898
datastore requests and
I believe queries

00:47:55.898 --> 00:47:58.378
are part of that.

00:47:59.258 --> 00:48:04.058
It's possible that we'll
eventually open up the API and

00:48:04.058 --> 00:48:05.078
build that into app engine.

00:48:05.078 --> 00:48:09.098
Let me scroll down.

00:48:15.458 --> 00:48:18.958
OK, well the first question,
will the presentation Guido

00:48:18.958 --> 00:48:20.918
gave be available for download?

00:48:20.918 --> 00:48:23.498
I'm particularly interested
in some parts about

00:48:23.498 --> 00:48:25.678
optimizing apps.

00:48:25.718 --> 00:48:28.998
I'd be happy to
make it available.

00:48:29.098 --> 00:48:30.258
It'll probably be a few days.

00:48:30.258 --> 00:48:34.618
I know that they're also
recording all the sessions on

00:48:34.618 --> 00:48:36.998
YouTube so you should be able
to find it there in

00:48:36.998 --> 00:48:38.838
a few days as well.

00:48:38.838 --> 00:48:44.698
I promise if there's any way to
sort of upload slides from

00:48:44.698 --> 00:48:48.018
presentations to the IO 2010
website I'll do that and

00:48:48.018 --> 00:48:53.820
otherwise I'll put it in some
of the app engine specific file

00:48:53.820 --> 00:49:01.158
sections on code at Google.com
and post a link to the group.

00:49:01.158 --> 00:49:01.958
OK, number two.

00:49:01.958 --> 00:49:05.858
Caching is hard to get right,
do you see App Engine adding

00:49:05.858 --> 00:49:06.498
support in their
API for caching?

00:49:07.758 --> 00:49:10.618
For example, by adding a
prefetch for the query

00:49:10.618 --> 00:49:17.638
to make all sum field
data be prefetched?

00:49:17.638 --> 00:49:18.238
That's a good idea.

00:49:18.238 --> 00:49:23.218
We have been thinking on and
off about providing a higher

00:49:23.218 --> 00:49:28.238
level layer of caching, but
because there are different

00:49:28.238 --> 00:49:31.138
levels of caching and there are
different sort of requirements

00:49:31.138 --> 00:49:36.498
it's really hard to write a
generic, automatic caching

00:49:36.498 --> 00:49:39.258
layer that works in all
circumstances and for all

00:49:39.258 --> 00:49:41.398
types of applications.

00:49:41.678 --> 00:49:43.538
We're still working
on that problem.

00:49:43.260 --> 00:49:47.758
But in the meantime it's
probably better to look at some

00:49:47.758 --> 00:49:50.800
point solutions like Nick
Johnson's reference property

00:49:50.800 --> 00:49:55.918
prefetch, The URL for the
Appstats presentation, that's

00:49:55.918 --> 00:49:57.760
really the same as the first
one, what happened there?

00:50:02.098 --> 00:50:11.298
AUDIENCE: [INAUDIBLE]

00:50:11.298 --> 00:50:15.360
GUIDO VAN ROSSUM: Yeah,
let's see what happens.

00:50:15.360 --> 00:50:16.158
It's coming.

00:50:16.338 --> 00:50:25.138
I think there was-- well
that's a great one.

00:50:25.698 --> 00:50:29.998
What advice do you have for
users of Appstats on how to

00:50:29.998 --> 00:50:32.638
interpret the data gathered
by it and how to optimize

00:50:32.638 --> 00:50:36.118
their code using the data?

00:50:36.118 --> 00:50:41.840
That's a very generic question
that I actually think the best

00:50:41.840 --> 00:50:47.678
answer is watch the video
of this presentation.

00:50:47.678 --> 00:50:51.298
I don't think I have like
a one-sentence addition

00:50:51.298 --> 00:50:54.198
that suddenly turns you
into an Appstats or

00:50:54.198 --> 00:50:58.278
optimization expert.

00:50:58.278 --> 00:51:02.460
Appstats puts some overhead on
the profiled application, but I

00:51:02.460 --> 00:51:07.198
am wondering how much money
will it cost me if I use

00:51:07.198 --> 00:51:14.578
Appstats all the time?

00:51:14.578 --> 00:51:16.520
So I tried to mention
that before.

00:51:16.520 --> 00:51:19.060
I think it's under a few
dollars a day, even for

00:51:19.060 --> 00:51:21.400
a popular application.

00:51:21.400 --> 00:51:27.598
But by all means turn it on for
a little while and sort of

00:51:27.598 --> 00:51:32.478
check those log messages for
overhand and do the calculation

00:51:32.478 --> 00:51:36.098
yourself or maybe if your
application is really popular,

00:51:36.098 --> 00:51:41.058
if you're in the 50 QPS club or
something you may be able to

00:51:41.058 --> 00:51:44.718
actually see a bump in your
request rates and a bump in

00:51:44.718 --> 00:51:46.696
your daily usage reports.

00:51:46.696 --> 00:51:47.348
AUDIENCE: [INAUDIBLE].

00:51:53.400 --> 00:51:55.178
GUIDO VAN ROSSUM: Yes, so
that's a good suggestion.

00:51:55.178 --> 00:51:58.218
He suggested that some
APIs could be made free.

00:51:58.218 --> 00:51:59.718
Actually memcache is free.

00:51:59.718 --> 00:52:03.098
Sending e-mail is free,
URL fetch is free.

00:52:03.098 --> 00:52:06.498
The only APIs that we
charge for is pretty

00:52:06.498 --> 00:52:07.938
much the datastore.

00:52:07.938 --> 00:52:12.118
But since Appstats itself runs
as a user-land library there's

00:52:12.118 --> 00:52:20.418
no direct way for discovering
whether the CPU time attributed

00:52:20.418 --> 00:52:24.658
to your application is actually
spent in Appstats or in any

00:52:24.658 --> 00:52:25.878
other part of your
applications.

00:52:25.878 --> 00:52:29.998
So that makes that
a little bit iffy.

00:52:30.920 --> 00:52:33.060
If we had a way to separate
that out clearly I

00:52:33.060 --> 00:52:36.498
would like to do that.

00:52:36.498 --> 00:52:38.718
OK, this one we got.

00:52:38.718 --> 00:52:41.598
How bringing a call JVM
online can impact Java

00:52:41.598 --> 00:52:42.558
App Engine performance.

00:52:43.078 --> 00:52:45.308
Imagine there are similar
concerns for the

00:52:45.308 --> 00:52:46.900
pipe and runtime.

00:52:46.900 --> 00:52:50.560
Can Appstats provide insight
into the performance f due to

00:52:50.560 --> 00:52:54.538
factors related to the
hosting environments?

00:52:55.558 --> 00:52:57.718
This is funny that you
should mention this.

00:53:00.860 --> 00:53:06.318
I noticed that some Appstats
requests had a very large

00:53:06.318 --> 00:53:11.380
charge for CPU time that was
not sort of seemingly matched

00:53:11.380 --> 00:53:18.058
by the real time spent
in the application.

00:53:18.058 --> 00:53:24.718
Like if the CPU time charged is
two seconds more, two virtual

00:53:24.718 --> 00:53:30.358
CPU seconds more than the real
time and most of the real time

00:53:30.358 --> 00:53:34.180
is accounted for by RPCs, where
exactly are you spending

00:53:34.180 --> 00:53:35.080
that CPU time?

00:53:35.080 --> 00:53:40.100
And I just looked in the admin
console logs for that

00:53:40.100 --> 00:53:44.438
particular request and realized
that that was a loading request

00:53:44.438 --> 00:53:47.098
and sort of, I forget when we
introduced this, but a few

00:53:47.098 --> 00:53:52.760
releases ago we started logging
a special message for every

00:53:52.760 --> 00:53:55.598
request that is identified
as a loading request.

00:53:55.598 --> 00:53:58.118
And sure enough the loading
requests were the ones

00:53:58.118 --> 00:54:00.280
that have extra CPU time.

00:54:00.280 --> 00:54:04.698
So a simple enhancement that I
want to apply to Appstats is

00:54:04.698 --> 00:54:12.338
actually figure out how much
CPU time we've already spent

00:54:12.338 --> 00:54:15.718
when Appstats is first invoked
and then we know that that time

00:54:15.718 --> 00:54:20.918
is all spent importing the
library and modules

00:54:20.918 --> 00:54:21.578
of the application.

00:54:21.738 --> 00:54:25.798
So we'll be able to represent
that graphically in a separate

00:54:25.798 --> 00:54:33.178
way so you can say a little bit
more about the cost of loading.

00:54:33.178 --> 00:54:42.398
We also have some plans for
making the cost of loading an

00:54:42.398 --> 00:54:47.578
application less dramatic, but
I don't want this to turn into

00:54:47.578 --> 00:54:49.358
sort of an improvised
product announcement.

00:54:49.658 --> 00:54:52.658
I don't want to go into
anymore detail there.

00:54:53.718 --> 00:54:56.778
Let's see if there are
anymore questions.

00:54:56.778 --> 00:55:00.118
Many performance problems I see
are deadline exceeded errors,

00:55:00.118 --> 00:55:04.780
anyway to diagnose those?

00:55:04.780 --> 00:55:05.738
That's a tricky one.

00:55:05.738 --> 00:55:11.298
I mean, if you got a deadline
exceeded there are sort

00:55:11.298 --> 00:55:13.300
of two possibilities.

00:55:13.300 --> 00:55:17.118
Either your application is just
trying to do way too much work

00:55:17.118 --> 00:55:21.058
for a single request and then
the best thing you can do is

00:55:21.058 --> 00:55:24.880
using Appstats look at those
requests that don't quite

00:55:24.880 --> 00:55:28.780
exceed the deadline, but are
very close because I think when

00:55:28.780 --> 00:55:33.018
you actually hit a deadline
exceeded error Appstats will

00:55:33.018 --> 00:55:36.218
not have time to necessarily
save all the data.

00:55:36.218 --> 00:55:40.040
So that's not very reliable,
but if you're exceeding the

00:55:40.040 --> 00:55:44.098
deadline you're also probably,
likely getting close to it in

00:55:44.098 --> 00:55:49.320
other requests and so analyzing
the request that make it to the

00:55:49.320 --> 00:55:54.638
deadline that like take 29
seconds or even 15 seconds

00:55:54.638 --> 00:55:57.518
will probably be very
helpful in that case.

00:55:58.278 --> 00:56:04.838
Another unfortunate reality is
that sometimes if we have

00:56:04.838 --> 00:56:09.378
certain performance problems in
our datastore cluster it's

00:56:09.378 --> 00:56:13.688
possible that applications sort
of on the average will see

00:56:13.688 --> 00:56:17.858
more deadline errors than
during happier times.

00:56:18.118 --> 00:56:22.958
If those things happen to more
than one or two applications

00:56:22.958 --> 00:56:29.960
our developer relations team
usually post to the App Engine

00:56:29.960 --> 00:56:33.118
group so you will be notified
of why you were exceeding

00:56:33.118 --> 00:56:40.218
deadline errors in
that particular time.

00:56:40.218 --> 00:56:43.998
Can I instrument my own code
and have profiling appear in

00:56:43.998 --> 00:56:44.458
the Appstats visualizer?

00:56:45.800 --> 00:56:51.658
There is actually an API where
you can create a point event

00:56:51.658 --> 00:56:56.198
that shows up with a stack
trays, we don't yet have a way

00:56:56.198 --> 00:57:04.238
to from the application sort of
indicate a start and end time

00:57:04.238 --> 00:57:06.420
for an event, but that would
actually be a fairly simple

00:57:06.420 --> 00:57:08.718
thing to add to the API.

00:57:09.198 --> 00:57:13.038
If someone wants to help me by
sort of proposing a patch to

00:57:13.038 --> 00:57:15.750
do that it'll happen
faster, otherwise it'll

00:57:15.750 --> 00:57:16.648
happen eventually.

00:57:20.478 --> 00:57:27.238
I think we're through with the
all the moderator questions.

00:57:27.238 --> 00:57:29.578
We have a minute left,
anyone in the audience

00:57:29.578 --> 00:57:30.184
want to add something?

00:57:31.156 --> 00:57:32.130
AUDIENCE:

00:57:32.130 --> 00:57:37.980
I have a question about
what if we are just doing

00:57:37.980 --> 00:57:39.940
a data pull, right?

00:57:39.940 --> 00:57:40.900
Not doing a lot.

00:57:42.860 --> 00:57:49.520
[INAUDIBLE]

00:57:49.520 --> 00:57:51.380
GUIDO VAN ROSSUM: Contact
developer relationships because

00:57:51.380 --> 00:57:52.900
that's not supposed to happen.

00:57:52.900 --> 00:58:03.738
If a single datastore write
fails and it's not because you

00:58:03.738 --> 00:58:05.678
did a whole bunch of them, but
it's just like you ran a

00:58:05.678 --> 00:58:09.918
little, very limited amount of
time them you make one write

00:58:09.918 --> 00:58:16.438
call and it fails, if that is
not due to some kind of

00:58:16.438 --> 00:58:19.268
unanticipated back end
error I'd be surprised.
1020
00:58:19,914 --&gt; 00:58:19,238
AUDIENCE:

00:58:19.238 --> 00:58:20.208
[INAUDIBLE].

00:58:32.338 --> 00:58:34.158
GUIDO VAN ROSSUM: No two
cases are actually the same.

00:58:36.898 --> 00:58:40.718
Usually our own monitoring
picks up those things because

00:58:40.718 --> 00:58:43.678
we monitor things like failure
rate over all applications and

00:58:43.678 --> 00:58:48.860
over specific applications, so
spikes in failure rates and

00:58:48.860 --> 00:58:54.658
spikes in response times often
immediately trigger pagers and

00:58:54.658 --> 00:58:58.220
engineers will go scurry off
and find out what's going on

00:58:58.220 --> 00:59:01.458
and fix things if necessary.

00:59:01.458 --> 00:59:06.438
So it's not necessary to
do something about it.

00:59:06.438 --> 00:59:08.320
You don't need to report
it in general because we

00:59:08.320 --> 00:59:11.078
already figured it out.

00:59:11.078 --> 00:59:15.458
And unfortunately we're out of
time now, but I'll be available

00:59:15.458 --> 00:59:17.138
throughout the conference.

00:59:17.138 --> 00:59:22.138
Thanks everyone.

