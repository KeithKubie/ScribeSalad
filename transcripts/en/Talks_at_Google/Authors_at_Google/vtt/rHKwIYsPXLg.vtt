WEBVTT
Kind: captions
Language: en

00:00:01.802 --> 00:00:03.510
JOHN BRACAGLIA: My
name is John Bacaglia.

00:00:03.510 --> 00:00:05.710
And I'm a Googler working
in YouTube operations.

00:00:05.710 --> 00:00:09.190
I also lead a group called
the Singularity Network,

00:00:09.190 --> 00:00:11.770
an internal organization
focused on discussions

00:00:11.770 --> 00:00:14.460
and rationality in
artificial intelligence.

00:00:14.460 --> 00:00:19.250
I'm pleased to be here
today with Mr. John Searle.

00:00:19.250 --> 00:00:21.500
As a brief introduction,
John Searle

00:00:21.500 --> 00:00:23.910
is the Slusser Professor of
Philosophy at the University

00:00:23.910 --> 00:00:25.900
of California-Berkeley.

00:00:25.900 --> 00:00:27.760
He is widely noted
for his contributions

00:00:27.760 --> 00:00:30.450
to the philosophy of
language, philosophy of mind,

00:00:30.450 --> 00:00:32.310
and social philosophy.

00:00:32.310 --> 00:00:35.620
John has received the Jean Nicod
Prize, the National Humanities

00:00:35.620 --> 00:00:38.650
Medal in the Mind and
Brain prize for his work.

00:00:38.650 --> 00:00:42.050
Among his noble concepts is
the Chinese room argument

00:00:42.050 --> 00:00:45.220
against strong
artificial intelligence.

00:00:45.220 --> 00:00:46.681
John Searle, everyone.

00:00:46.681 --> 00:00:47.181
[APPLAUSE]

00:00:47.181 --> 00:00:50.127
JOHN SEARLE: Thank
you Thank you.

00:00:53.564 --> 00:00:55.050
Many thanks.

00:00:55.050 --> 00:00:58.260
It's great to be back at Google.

00:00:58.260 --> 00:01:00.699
It is a university
outside of a university.

00:01:00.699 --> 00:01:02.740
And sometimes, I think,
this is what a university

00:01:02.740 --> 00:01:04.379
ought really to look like.

00:01:04.379 --> 00:01:06.180
Anyway, it's just
terrific to be here.

00:01:06.180 --> 00:01:08.785
And I'm going to talk
about some-- well,

00:01:08.785 --> 00:01:10.660
I'm going to talk about
a whole lot of stuff.

00:01:10.660 --> 00:01:12.510
But, basically, I want
to start with talking

00:01:12.510 --> 00:01:16.930
about the significance of
technological advances.

00:01:16.930 --> 00:01:19.960
And America, especially,
but everybody, really,

00:01:19.960 --> 00:01:23.030
is inclined to just
celebrate the advances.

00:01:23.030 --> 00:01:25.790
If they got a self-driving
car who the hell

00:01:25.790 --> 00:01:28.320
cares about whether
or not it's conscious.

00:01:28.320 --> 00:01:32.660
But I'm going to say there
are a lot of things that

00:01:32.660 --> 00:01:35.570
matter for certain purposes
about the understanding

00:01:35.570 --> 00:01:36.750
of the technology.

00:01:36.750 --> 00:01:38.740
And that's really what
I'm going to talk about.

00:01:38.740 --> 00:01:41.010
Now to begin with, I have
to make a couple rather

00:01:41.010 --> 00:01:43.610
boring distinctions
because you won't really

00:01:43.610 --> 00:01:46.820
understand contemporary
intellectual life if you don't

00:01:46.820 --> 00:01:49.140
understand these distinctions.

00:01:49.140 --> 00:01:52.960
In our culture, there's a
big deal about objectivity

00:01:52.960 --> 00:01:54.160
and subjectivity.

00:01:54.160 --> 00:01:57.220
We strive for an
objective science.

00:01:57.220 --> 00:02:00.130
The problem is that these
notions are systematically

00:02:00.130 --> 00:02:04.400
ambiguous in a way that produces
intellectual catastrophes.

00:02:04.400 --> 00:02:10.210
They're ambiguous between a
sense, which is epistemic,

00:02:10.210 --> 00:02:13.940
where epistemic means having
to do with knowledge--

00:02:13.940 --> 00:02:19.030
epistemic-- and a sense,
which is ontological,

00:02:19.030 --> 00:02:23.470
where ontological means
having to do with existence.

00:02:23.470 --> 00:02:25.946
I hate using a lot of
fancy polysyllabic words.

00:02:25.946 --> 00:02:27.570
And I'll try to keep
them to a minimum.

00:02:27.570 --> 00:02:31.090
But I need these two,
epistemic and ontological.

00:02:31.090 --> 00:02:34.770
Now the problem with
objectivity and subjectivity

00:02:34.770 --> 00:02:42.090
is that they're
systematically ambiguous--

00:02:42.090 --> 00:02:44.570
I'll just abbreviate
subjectivity--

00:02:44.570 --> 00:02:48.680
between an epistemic sense
and an ontological sense.

00:02:48.680 --> 00:02:52.620
Epistemically, the
distinction is between types

00:02:52.620 --> 00:02:54.200
of knowledge claims.

00:02:54.200 --> 00:02:58.570
If I say, Rembrandt
died in 1606, well-- no,

00:02:58.570 --> 00:02:59.370
he didn't die then.

00:02:59.370 --> 00:03:00.290
He was born then.

00:03:00.290 --> 00:03:03.220
I'd say Rembrandt
was born in 1606.

00:03:03.220 --> 00:03:06.020
That is to say, it's a
matter of objective fact.

00:03:06.020 --> 00:03:09.020
That's epistemically objective.

00:03:09.020 --> 00:03:11.140
But if I say Rembrandt
is the greatest

00:03:11.140 --> 00:03:14.910
painter that ever lived, well,
that's a matter of opinion.

00:03:14.910 --> 00:03:18.490
That is epistemically subject.

00:03:18.490 --> 00:03:22.130
So we have epistemic
objectivity and subjectivity.

00:03:22.130 --> 00:03:26.420
Underlying that is a distinction
in modes of existence.

00:03:26.420 --> 00:03:30.650
Lots of things exist regardless
of what anybody thinks.

00:03:30.650 --> 00:03:33.480
Mountains, molecules,
and tectonic plates

00:03:33.480 --> 00:03:38.440
have a mode of existence that
is ontologically objective.

00:03:38.440 --> 00:03:40.670
But pains and
pickles and itches,

00:03:40.670 --> 00:03:45.290
they only exist insofar as they
are experienced by a subject.

00:03:45.290 --> 00:03:48.370
They are ontologically
subjective.

00:03:48.370 --> 00:03:50.400
So I want everybody to
get that distinction

00:03:50.400 --> 00:03:53.160
because it's very
important because-- well,

00:03:53.160 --> 00:03:57.490
for a lot of reasons, but
one is lots of phenomena that

00:03:57.490 --> 00:04:02.640
are ontologically subjective
admit of an account which

00:04:02.640 --> 00:04:05.280
is epistemically objective.

00:04:05.280 --> 00:04:07.620
I first got interested
in this kind of stuff.

00:04:07.620 --> 00:04:10.770
I thought, well, why don't
these brain guys solve

00:04:10.770 --> 00:04:12.170
the problem of consciousness.

00:04:12.170 --> 00:04:16.279
And I went over UCSF to
their neurobiology gang

00:04:16.279 --> 00:04:17.700
and told them,
why the hell don't

00:04:17.700 --> 00:04:20.420
you guys figure out how the
brain causes consciousness?

00:04:20.420 --> 00:04:22.010
What am I paying you to do?

00:04:22.010 --> 00:04:26.530
And their reaction was,
look, we're doing science.

00:04:26.530 --> 00:04:29.570
Science is objective.

00:04:29.570 --> 00:04:33.120
And you, yourself, admit that
consciousness is subjective.

00:04:33.120 --> 00:04:36.090
So there can't be a
science of consciousness.

00:04:36.090 --> 00:04:39.610
Now you'll all recognize
that's a fallacy of ambiguity.

00:04:39.610 --> 00:04:44.240
Science is indeed
epistemically objective

00:04:44.240 --> 00:04:47.370
because we strive
for claims that

00:04:47.370 --> 00:04:49.080
can be established
as true or false,

00:04:49.080 --> 00:04:51.540
independent of the
attitudes of the makers

00:04:51.540 --> 00:04:53.670
and interpreters of the claim.

00:04:53.670 --> 00:04:59.710
But epistemic
objectivity of the theory

00:04:59.710 --> 00:05:04.450
does not preclude an
epistemically objective account

00:05:04.450 --> 00:05:07.009
of a domain that's
ontologically subjective.

00:05:07.009 --> 00:05:09.050
I promised you I wouldn't
use too many big words,

00:05:09.050 --> 00:05:10.580
but anyway there are a few.

00:05:10.580 --> 00:05:11.650
The point is this.

00:05:11.650 --> 00:05:14.660
You can have an epistemically
objective science

00:05:14.660 --> 00:05:18.040
of consciousness, even
though consciousness

00:05:18.040 --> 00:05:20.760
is ontologically subjective.

00:05:20.760 --> 00:05:22.850
Now that's going
to be important.

00:05:22.850 --> 00:05:25.300
And there's another distinction.

00:05:25.300 --> 00:05:26.830
Since not everybody
can see this,

00:05:26.830 --> 00:05:29.410
I'm going to erase
as I go along.

00:05:29.410 --> 00:05:32.120
There's another distinction
which is crucial.

00:05:32.120 --> 00:05:37.045
And that's between phenomena
that are observer-independent.

00:05:40.170 --> 00:05:44.140
And there I'm thinking of
mountains and molecules

00:05:44.140 --> 00:05:47.960
and tectonic plates, how
they exist regardless

00:05:47.960 --> 00:05:49.840
of what anybody thinks.

00:05:49.840 --> 00:05:51.360
But the world is
full of stuff that

00:05:51.360 --> 00:05:56.640
matters to us that
is observer-relative.

00:05:56.640 --> 00:06:01.080
It only exists relative
to observers and users.

00:06:01.080 --> 00:06:08.490
So, for example, the piece of
paper in my wallet is money.

00:06:08.490 --> 00:06:11.790
But the fact that makes it money
is not a fact of its chemistry.

00:06:11.790 --> 00:06:15.230
It's a fact about the attitudes
that we have toward it.

00:06:15.230 --> 00:06:19.280
So money is observer-relative.

00:06:19.280 --> 00:06:22.830
Money, property, government,
marriage, universities, Google,

00:06:22.830 --> 00:06:25.340
cocktail parties,
and summer vacations

00:06:25.340 --> 00:06:27.040
are all observer-relative.

00:06:30.920 --> 00:06:34.760
And that has to be distinguished
from observer-independent.

00:06:34.760 --> 00:06:38.420
And notice now, all
observer-relative phenomenon

00:06:38.420 --> 00:06:41.910
are created by
human consciousness.

00:06:41.910 --> 00:06:47.280
Hence, they contain an element
of ontological subjectivity.

00:06:47.280 --> 00:06:49.880
But you already know
that you can have,

00:06:49.880 --> 00:06:53.960
in some cases, an
epistemically objective

00:06:53.960 --> 00:06:57.830
science of a domain that
is observer-relative.

00:06:57.830 --> 00:07:01.590
That's why you can have an
objective science of economics

00:07:01.590 --> 00:07:04.290
even though the phenomena
studied by economics

00:07:04.290 --> 00:07:06.890
is, in general,
observer-relative,

00:07:06.890 --> 00:07:11.010
and hence contains an element
of ontological subjectivity.

00:07:11.010 --> 00:07:12.470
Economists tend to forget that.

00:07:12.470 --> 00:07:14.150
They tend to think
that economics

00:07:14.150 --> 00:07:16.730
is kind of like physics,
only it's harder.

00:07:19.850 --> 00:07:22.140
When I studied economics,
I was appalled.

00:07:22.140 --> 00:07:25.790
We learned that marginal
cost equals marginal revenue

00:07:25.790 --> 00:07:28.610
in the same tone of
voice that in physics we

00:07:28.610 --> 00:07:30.890
learned that force equals
mass times acceleration.

00:07:30.890 --> 00:07:35.930
They're totally different
because the stuff in economics

00:07:35.930 --> 00:07:39.160
is all observer-relative
and contains an element

00:07:39.160 --> 00:07:41.300
of ontological subjectivity.

00:07:41.300 --> 00:07:44.087
And when the subjectivity
changes-- ffft--

00:07:44.087 --> 00:07:45.170
the whole thing collapses.

00:07:45.170 --> 00:07:48.750
That was discovered in 2008.

00:07:48.750 --> 00:07:50.510
This is not a lecture
about economics.

00:07:50.510 --> 00:07:52.990
I want you to keep
all that in mind.

00:07:52.990 --> 00:07:57.620
Now that's important because
a lot of the phenomena that

00:07:57.620 --> 00:08:00.190
are studied in
cognitive science,

00:08:00.190 --> 00:08:03.740
particularly phenomena of
intelligence, cognition,

00:08:03.740 --> 00:08:06.350
memory, thought, perception,
and all the rest of it

00:08:06.350 --> 00:08:08.660
have two different senses.

00:08:08.660 --> 00:08:14.020
They have one sense, which
is observer-independent,

00:08:14.020 --> 00:08:17.950
and another sense, which
is observer-relative.

00:08:17.950 --> 00:08:21.930
And, consequently, we
have to be very careful

00:08:21.930 --> 00:08:25.010
that we don't confuse
those senses because many

00:08:25.010 --> 00:08:28.010
of the crucial concepts
in cognitive science

00:08:28.010 --> 00:08:32.400
have as their
reference phenomena

00:08:32.400 --> 00:08:37.890
that are observer-relative
and not observer-independent.

00:08:37.890 --> 00:08:39.450
I'm going to get to that.

00:08:39.450 --> 00:08:41.120
OK, everybody up with us so far?

00:08:41.120 --> 00:08:43.310
I want everything to
sound so obvious you

00:08:43.310 --> 00:08:45.810
think, why does this guy bore
us with these platitudes?

00:08:45.810 --> 00:08:47.601
Why doesn't he say
something controversial?

00:08:50.150 --> 00:08:52.570
Now I'm going to
go and talk about

00:08:52.570 --> 00:08:54.910
some intellectual history.

00:08:54.910 --> 00:08:57.780
Many years ago, before
any of you were born,

00:08:57.780 --> 00:09:00.260
a new discipline was born.

00:09:00.260 --> 00:09:03.502
It was called cognitive science.

00:09:03.502 --> 00:09:05.210
And it was founded by
a whole bunch of us

00:09:05.210 --> 00:09:08.040
who got sick of behaviorism
in psychology, effectively.

00:09:08.040 --> 00:09:09.920
That was the reason for it.

00:09:09.920 --> 00:09:16.660
And the Sloan Foundation used
to fly us around to lecture,

00:09:16.660 --> 00:09:17.810
mostly to each other.

00:09:17.810 --> 00:09:19.018
But anyway, that's all right.

00:09:19.018 --> 00:09:20.630
We were called Sloan Rangers.

00:09:20.630 --> 00:09:25.520
And I was invited to lecture to
the Artificial Intelligence Lab

00:09:25.520 --> 00:09:26.687
at Yale.

00:09:26.687 --> 00:09:28.770
And I thought, well, christ,
I don't know anything

00:09:28.770 --> 00:09:30.750
about artificial intelligence.

00:09:30.750 --> 00:09:34.620
So I went out and bought a book
written by the guys at Yale.

00:09:34.620 --> 00:09:41.350
And I remember thinking,
$16.95 plus tax-- money wasted.

00:09:41.350 --> 00:09:45.060
But it turned out I was wrong.

00:09:45.060 --> 00:09:49.490
They had in there a theory about
how computers could understand.

00:09:49.490 --> 00:09:53.780
And the idea was that you
give the computer a story.

00:09:53.780 --> 00:09:57.520
And then you ask the computer
questions about the story.

00:09:57.520 --> 00:10:00.390
And the computer would give the
correct answer to the questions

00:10:00.390 --> 00:10:04.070
even though the answer was
not contained in the story.

00:10:04.070 --> 00:10:05.610
A typical story.

00:10:05.610 --> 00:10:07.865
A guy goes into a restaurant
and orders a hamburger.

00:10:07.865 --> 00:10:09.365
When they brought
him the hamburger,

00:10:09.365 --> 00:10:11.180
it was burned to a crisp.

00:10:11.180 --> 00:10:13.100
The guy stormed out
of the restaurant

00:10:13.100 --> 00:10:14.890
and didn't even pay his bill.

00:10:14.890 --> 00:10:17.860
Question, did the guy
eat the hamburger?

00:10:17.860 --> 00:10:19.990
Well, all of you computers
know the answer to that.

00:10:19.990 --> 00:10:21.750
No, the guy didn't
eat the hamburger.

00:10:21.750 --> 00:10:24.940
And I won't tell you the
story where the answer is yes.

00:10:24.940 --> 00:10:28.112
It's equally boring.

00:10:28.112 --> 00:10:33.560
Now, the point was this proves
that the computer really

00:10:33.560 --> 00:10:35.600
understands the story.

00:10:35.600 --> 00:10:39.900
So there I was on my way to
New Haven on United Airlines

00:10:39.900 --> 00:10:40.746
at 30,000 feet.

00:10:40.746 --> 00:10:42.620
And I thought, well,
hell, they could give me

00:10:42.620 --> 00:10:44.450
these stories in Chinese.

00:10:44.450 --> 00:10:47.540
And I could follow the computer
program for answering stories.

00:10:47.540 --> 00:10:50.350
And I don't understand
a word of the story.

00:10:50.350 --> 00:10:53.010
And I thought, well,
that's an objection

00:10:53.010 --> 00:10:55.080
they must have thought of.

00:10:55.080 --> 00:10:57.750
And besides that
won't keep me going

00:10:57.750 --> 00:11:00.350
for a whole week in New Haven.

00:11:00.350 --> 00:11:02.630
Well, it turned out they
hadn't thought of it.

00:11:02.630 --> 00:11:06.200
And everybody was
convinced I was wrong.

00:11:06.200 --> 00:11:08.640
But interestingly they
all had different reasons

00:11:08.640 --> 00:11:10.830
for thinking I was wrong.

00:11:10.830 --> 00:11:13.770
And the argument has gone
on longer than a week.

00:11:13.770 --> 00:11:15.859
It's gone on for 35 years.

00:11:15.859 --> 00:11:17.900
I mean, how often do I
have to refute these guys?

00:11:17.900 --> 00:11:22.070
But anyway, let's go through it.

00:11:22.070 --> 00:11:24.300
The way the argument goes
in its simplest version

00:11:24.300 --> 00:11:29.610
is I am locked in a room
full of Chinese-- well,

00:11:29.610 --> 00:11:32.440
they're boxes full of Chinese
symbols and a rule book

00:11:32.440 --> 00:11:34.820
in English for
manipulating the symbols.

00:11:34.820 --> 00:11:38.200
Unknown to me, the boxes
are called a database,

00:11:38.200 --> 00:11:40.720
and the rule book
is called a program.

00:11:40.720 --> 00:11:44.910
In coming in the room,
I get Chinese symbols.

00:11:44.910 --> 00:11:46.590
Unknown to me,
those are questions.

00:11:46.590 --> 00:11:48.370
I look up what I'm
supposed to do.

00:11:48.370 --> 00:11:49.940
And after I shuffle
a lot of symbols,

00:11:49.940 --> 00:11:51.180
I give back other symbols.

00:11:51.180 --> 00:11:54.130
And those are answers
to the questions.

00:11:54.130 --> 00:11:56.551
Now we will suppose--
I hope your bored

00:11:56.551 --> 00:11:57.550
with this, because I am.

00:11:57.550 --> 00:12:01.380
I mean, I've told
this story many times.

00:12:01.380 --> 00:12:04.090
We will suppose that they get
so good at writing the program,

00:12:04.090 --> 00:12:06.280
I get so good at
shuffling the symbols,

00:12:06.280 --> 00:12:08.650
that my answers are
indistinguishable

00:12:08.650 --> 00:12:10.620
from a native Chinese speaker.

00:12:10.620 --> 00:12:13.570
I pass the Turing test
for understanding Chinese.

00:12:13.570 --> 00:12:16.700
All the same, I don't
understand a word of Chinese.

00:12:16.700 --> 00:12:18.920
And there's no way
in the Chinese room

00:12:18.920 --> 00:12:23.050
that I could come to understand
Chinese because all I am

00:12:23.050 --> 00:12:24.930
is a computer system.

00:12:24.930 --> 00:12:28.350
And the rules I operate
are a computer program.

00:12:28.350 --> 00:12:30.490
And-- and this is
the important point--

00:12:30.490 --> 00:12:33.600
the program is
purely syntactical.

00:12:33.600 --> 00:12:37.530
It is defined entirely
as a set of operations

00:12:37.530 --> 00:12:39.110
over syntactical elements.

00:12:39.110 --> 00:12:41.230
To put it slightly
more technically,

00:12:41.230 --> 00:12:43.250
the notion same
implemented program

00:12:43.250 --> 00:12:45.370
defines an
equivalence class that

00:12:45.370 --> 00:12:48.840
is specified completely
independently of any physics

00:12:48.840 --> 00:12:51.510
and, in particular,
independent of the physics

00:12:51.510 --> 00:12:53.300
of its realization.

00:12:53.300 --> 00:12:56.860
The bottom line is
if I don't understand

00:12:56.860 --> 00:13:00.100
the questions and the answers
on the basis of implementing

00:13:00.100 --> 00:13:03.840
the program, then neither does
any other digital computer

00:13:03.840 --> 00:13:06.350
on that basis
because no computer

00:13:06.350 --> 00:13:09.060
has anything that I don't have.

00:13:09.060 --> 00:13:13.040
Computers are purely
syntactical devices.

00:13:13.040 --> 00:13:15.880
Their operations are
defined syntactically.

00:13:15.880 --> 00:13:19.170
And human intelligence
requires more than syntax.

00:13:19.170 --> 00:13:20.930
It requires a semantics.

00:13:20.930 --> 00:13:25.380
It requires an understanding
of what's going on.

00:13:25.380 --> 00:13:30.180
You can see this if you
contrast my behavior in English

00:13:30.180 --> 00:13:31.940
with my behavior in Chinese.

00:13:31.940 --> 00:13:35.500
They ask me
questions in English.

00:13:35.500 --> 00:13:37.467
And I give answers in English.

00:13:37.467 --> 00:13:39.800
They say, what's the longest
river in the United States?

00:13:39.800 --> 00:13:41.680
And I say, well,
it's the Mississippi,

00:13:41.680 --> 00:13:44.720
or the Mississippi-Missouri,
depending

00:13:44.720 --> 00:13:46.630
on if you count
that as one river.

00:13:46.630 --> 00:13:50.556
They ask me in Chinese, what's
the longest river in China?

00:13:50.556 --> 00:13:52.680
I don't know what the
question is or what it means.

00:13:52.680 --> 00:13:54.420
All I got are Chinese symbols.

00:13:54.420 --> 00:13:57.640
But I look up what I'm supposed
to do with that symbol,

00:13:57.640 --> 00:14:00.660
and I give back an answer,
which is the right answer.

00:14:00.660 --> 00:14:02.050
It says, it's the Yangtze.

00:14:02.050 --> 00:14:03.479
That's the longest
river in China.

00:14:03.479 --> 00:14:04.520
I don't know any of that.

00:14:04.520 --> 00:14:06.560
I'm just a computer.

00:14:06.560 --> 00:14:12.530
So the bottom line is that the
implemented computer program

00:14:12.530 --> 00:14:15.130
by itself is never
going to be sufficient

00:14:15.130 --> 00:14:18.770
for human understanding
because human understanding has

00:14:18.770 --> 00:14:20.190
more than syntax.

00:14:20.190 --> 00:14:22.280
It has a semantics.

00:14:22.280 --> 00:14:24.710
There are two
fundamental principles

00:14:24.710 --> 00:14:27.680
that underlie the
Chinese room argument.

00:14:27.680 --> 00:14:30.100
And both of them seem
to me obviously true.

00:14:30.100 --> 00:14:32.310
You can state each
in four words.

00:14:32.310 --> 00:14:35.390
Syntax is not semantics.

00:14:35.390 --> 00:14:40.320
And simulation is
not duplication.

00:14:40.320 --> 00:14:42.000
You can simulate--
you're going to have

00:14:42.000 --> 00:14:43.620
plenty of time for questions.

00:14:43.620 --> 00:14:45.180
How much time we
got, by the way?

00:14:45.180 --> 00:14:45.830
I want to--

00:14:45.830 --> 00:14:48.247
JOHN BRACAGLIA: We'll leave
time for questions at the end.

00:14:48.247 --> 00:14:49.663
JOHN SEARLE: I
want everybody that

00:14:49.663 --> 00:14:51.940
has a question to have a
chance to ask the question.

00:14:51.940 --> 00:14:55.130
Anyway, that's the famous
Chinese room argument.

00:14:55.130 --> 00:14:57.770
And it takes about five
minutes to explain it.

00:14:57.770 --> 00:15:03.380
Now you'd be amazed at
the responses I got.

00:15:03.380 --> 00:15:05.840
They were absolutely
breathtaking

00:15:05.840 --> 00:15:08.900
in their preposterousness.

00:15:08.900 --> 00:15:11.510
Now let me give
you some answers.

00:15:11.510 --> 00:15:14.260
A favorite answer was this.

00:15:14.260 --> 00:15:16.130
You were there in a room.

00:15:16.130 --> 00:15:18.280
You had all those symbols.

00:15:18.280 --> 00:15:19.240
You had a box.

00:15:19.240 --> 00:15:22.910
You probably had scratch
paper on which to work.

00:15:22.910 --> 00:15:26.150
Now, it wasn't you
that understood.

00:15:26.150 --> 00:15:30.340
You're just a CPU, they would
say with contempt, the Central

00:15:30.340 --> 00:15:31.950
Processing Unit.

00:15:31.950 --> 00:15:34.870
I didn't know what any of these
words meant in those days.

00:15:34.870 --> 00:15:41.090
CPU, it's the system
that understands.

00:15:41.090 --> 00:15:43.210
And when I first
heard this, I mean,

00:15:43.210 --> 00:15:45.990
the room understands
Chinese, I said to the guy.

00:15:45.990 --> 00:15:49.510
And he said, yes, the
room understands Chinese.

00:15:49.510 --> 00:15:51.770
Well, it's a desperate answer.

00:15:51.770 --> 00:15:53.810
And I admire courage.

00:15:53.810 --> 00:15:55.150
But it's got a problem.

00:15:55.150 --> 00:15:57.850
And that is the reason
I don't understand

00:15:57.850 --> 00:16:00.380
is I can't get from the
syntax to the semantics.

00:16:00.380 --> 00:16:01.500
But the room can't either.

00:16:01.500 --> 00:16:05.630
How does the room get from
the syntax of the computer

00:16:05.630 --> 00:16:09.400
program of the input
symbols to the semantics

00:16:09.400 --> 00:16:11.270
of the understanding
of the symbols?

00:16:11.270 --> 00:16:13.270
There's no way the
room can get there

00:16:13.270 --> 00:16:15.800
because that would
require some consciousness

00:16:15.800 --> 00:16:18.470
in the room in addition
to my consciousness.

00:16:18.470 --> 00:16:20.710
And there is no
such consciousness.

00:16:20.710 --> 00:16:22.310
Anyway, that was
one of many answers.

00:16:22.310 --> 00:16:24.530
One of my favorites was this.

00:16:24.530 --> 00:16:25.950
This was in a public debate.

00:16:25.950 --> 00:16:30.190
A guy said to me, but
suppose we ask you,

00:16:30.190 --> 00:16:33.370
do you understand Chinese?

00:16:33.370 --> 00:16:36.920
And suppose you say, yes,
I understand Chinese.

00:16:36.920 --> 00:16:37.940
Well?

00:16:37.940 --> 00:16:42.270
Well, OK, let's try that
and see how far we get.

00:16:42.270 --> 00:16:45.780
I get a question
that looks like this.

00:16:48.960 --> 00:16:52.000
Now, this will be in a
dialect of Chinese some of you

00:16:52.000 --> 00:16:55.000
won't recognize.

00:16:55.000 --> 00:16:58.400
Unknown to me,
that symbol means,

00:16:58.400 --> 00:17:00.030
do you understand Chinese?

00:17:00.030 --> 00:17:02.120
I look up what I'm
supposed to do.

00:17:02.120 --> 00:17:03.840
And I give them
back a symbol that's

00:17:03.840 --> 00:17:06.650
in the same dialect of Chinese.

00:17:06.650 --> 00:17:08.040
And it looks like that.

00:17:08.040 --> 00:17:12.099
And that says, why do you guys
ask me such dumb questions?

00:17:12.099 --> 00:17:15.215
Can't you see that I
understand Chinese?

00:17:18.510 --> 00:17:23.430
I could go on with the other
responses and objections,

00:17:23.430 --> 00:17:25.770
but I think they're
all equally feeble.

00:17:25.770 --> 00:17:29.010
The bottom line is
there's a logical truth.

00:17:29.010 --> 00:17:32.790
And that is that the
implemented computer program

00:17:32.790 --> 00:17:34.370
is defined syntactically.

00:17:34.370 --> 00:17:36.190
And that's not a weakness.

00:17:36.190 --> 00:17:37.790
That's the power.

00:17:37.790 --> 00:17:42.830
The power of the syntactical
definition of computation

00:17:42.830 --> 00:17:47.060
is you can implement it on
electronic machines that

00:17:47.060 --> 00:17:51.770
can perform literally
millions of computations

00:17:51.770 --> 00:17:53.521
in a very small amount of time.

00:17:53.521 --> 00:17:55.270
I'm not sure I believe
this, but it always

00:17:55.270 --> 00:17:58.320
says it in the
textbooks, that Deep Blue

00:17:58.320 --> 00:18:02.300
can do 250 million
computations in a second.

00:18:02.300 --> 00:18:04.750
OK, I take their word for it.

00:18:04.750 --> 00:18:07.630
So it's not a
weakness of computers.

00:18:07.630 --> 00:18:09.900
Now, another argument
I sometimes got

00:18:09.900 --> 00:18:12.730
was, well, in
programs, we often have

00:18:12.730 --> 00:18:15.850
a section called the semantics
of natural understanding

00:18:15.850 --> 00:18:16.620
programs.

00:18:16.620 --> 00:18:17.330
And that's right.

00:18:17.330 --> 00:18:20.230
But, of course, what they do
is they put in more computer

00:18:20.230 --> 00:18:21.110
implementation.

00:18:21.110 --> 00:18:23.870
They put in more syntax.

00:18:23.870 --> 00:18:25.990
Now, so far, so good.

00:18:25.990 --> 00:18:28.500
And I think if that's
all there was to say,

00:18:28.500 --> 00:18:30.030
I've said all of that before.

00:18:30.030 --> 00:18:32.350
But now I want to go on
to something much more

00:18:32.350 --> 00:18:33.690
interesting.

00:18:33.690 --> 00:18:35.862
And here goes with that.

00:18:35.862 --> 00:18:36.570
Now how we doing?

00:18:36.570 --> 00:18:39.009
I'm not-- everybody
seems to understand

00:18:39.009 --> 00:18:41.050
there's going to be plenty
of time for questions.

00:18:41.050 --> 00:18:42.845
I insist on a good
question period.

00:18:42.845 --> 00:18:44.470
So let me take a
drink of water, and we

00:18:44.470 --> 00:18:46.950
go to the next step, which
I think is more important.

00:18:51.800 --> 00:18:54.250
A lot of people
thought, well, look,

00:18:54.250 --> 00:18:59.880
maybe the computer doesn't
understand Chinese,

00:18:59.880 --> 00:19:05.840
but all the same, it does
information processing.

00:19:05.840 --> 00:19:09.330
And it does, after
all, do computation.

00:19:09.330 --> 00:19:12.980
That's what we define
the machine to do.

00:19:12.980 --> 00:19:15.940
And I had to review a
couple of books recently.

00:19:15.940 --> 00:19:19.860
One book said that
we live in a new age,

00:19:19.860 --> 00:19:22.350
the age of information.

00:19:22.350 --> 00:19:25.390
And in a wonderful
outburst, the author

00:19:25.390 --> 00:19:29.100
said everything is information.

00:19:29.100 --> 00:19:33.260
Now that ought to worry us
if everything is information.

00:19:33.260 --> 00:19:34.690
And I read another book.

00:19:34.690 --> 00:19:37.270
This was an optimistic book.

00:19:37.270 --> 00:19:39.980
I reviewed-- this for "The
New York Review of Books"--

00:19:39.980 --> 00:19:44.620
a less optimistic book by a
guy who said computers are now

00:19:44.620 --> 00:19:48.650
so smart they're almost
as smart as we are.

00:19:48.650 --> 00:19:52.340
And pretty soon, they'll
be just as smart as we are.

00:19:52.340 --> 00:19:55.340
And then I don't have to tell
this audience the next step.

00:19:55.340 --> 00:19:58.470
They'll be much
smarter than we are.

00:19:58.470 --> 00:20:02.090
And then look out
because they might get

00:20:02.090 --> 00:20:05.390
sick of being oppressed by us.

00:20:05.390 --> 00:20:10.080
And they might simply rise
up and overthrow us all.

00:20:10.080 --> 00:20:11.971
And this, the author
said modestly--

00:20:11.971 --> 00:20:13.470
I guess this is how
you sell books--

00:20:13.470 --> 00:20:17.120
he said this may be the
greatest challenge that humanity

00:20:17.120 --> 00:20:22.820
has ever faced,
the upcoming revolt

00:20:22.820 --> 00:20:25.790
of super-smart computers.

00:20:25.790 --> 00:20:29.560
Now, I want to say both
of these claims are silly.

00:20:29.560 --> 00:20:31.570
I mean, I'm speaking
shorthand here.

00:20:31.570 --> 00:20:34.180
There'll be plenty of
chance to answer me.

00:20:34.180 --> 00:20:36.740
And I want to say briefly why.

00:20:36.740 --> 00:20:42.020
The notion of intelligence
has two different senses.

00:20:42.020 --> 00:20:46.410
It has an
observer-independent sense

00:20:46.410 --> 00:20:50.610
where it identifies something
that is psychologically real.

00:20:50.610 --> 00:20:56.680
So I am more intelligent
than my dog Tarski.

00:20:56.680 --> 00:20:58.880
Now, Tarski's pretty
smart, I agree.

00:20:58.880 --> 00:21:02.306
But overall, I'm
smarter than Tarski.

00:21:02.306 --> 00:21:03.680
I've had four
dogs, by they way--

00:21:03.680 --> 00:21:05.138
Frege, Russell,
Ludwig, and Tarski.

00:21:08.190 --> 00:21:10.622
And Tarski, he's a
Bernese mountain dog.

00:21:10.622 --> 00:21:12.080
I'm sorry I didn't
bring him along,

00:21:12.080 --> 00:21:15.540
but he's too big for the car.

00:21:15.540 --> 00:21:17.460
Now, he's very smart.

00:21:17.460 --> 00:21:21.680
But he does have intelligence
in the same sense that I do.

00:21:21.680 --> 00:21:24.150
Only he happens to have
somewhat less than I do.

00:21:24.150 --> 00:21:28.520
Now, my computer is
also intelligent.

00:21:28.520 --> 00:21:31.110
And it also processes
information.

00:21:31.110 --> 00:21:36.020
But-- and this is the key
point-- it's observer-relative.

00:21:36.020 --> 00:21:41.180
The only sense in which the
computer has intelligence

00:21:41.180 --> 00:21:46.030
is not in an intrinsic, but it's
in an observer-relative sense.

00:21:46.030 --> 00:21:49.460
We can interpret its
operations in such a way

00:21:49.460 --> 00:21:52.820
that we can make-- now, watch
this terminology-- we can make

00:21:52.820 --> 00:21:57.100
epistemically objective
claims of intelligence

00:21:57.100 --> 00:21:59.450
even though the
intelligence in question

00:21:59.450 --> 00:22:03.510
is entirely in the
eye of the beholder.

00:22:03.510 --> 00:22:06.380
This was brought
home forcefully to me

00:22:06.380 --> 00:22:10.500
when I read in the newspapers
that IBM had designed

00:22:10.500 --> 00:22:15.390
a computer program which could
beat the world's leading chess

00:22:15.390 --> 00:22:16.870
player.

00:22:16.870 --> 00:22:22.000
And in the same sense in
which Kasparov beat Karpov

00:22:22.000 --> 00:22:24.930
so we were told Deep
Blue beat Kasparov.

00:22:24.930 --> 00:22:28.420
Now that ought to worry us
because for Karpov and Kasparov

00:22:28.420 --> 00:22:31.530
to play chess, they both
have to be conscious

00:22:31.530 --> 00:22:33.420
that they're playing chess.

00:22:33.420 --> 00:22:35.080
They both have to
know such things

00:22:35.080 --> 00:22:38.280
as I opened with
pawn to king four,

00:22:38.280 --> 00:22:41.040
and my queen is threatened
on the left-hand side

00:22:41.040 --> 00:22:42.460
of the board.

00:22:42.460 --> 00:22:46.020
But now notice, Deep
Blue knows none on that

00:22:46.020 --> 00:22:48.710
because it doesn't
know anything.

00:22:48.710 --> 00:22:52.340
You can make epistemically
objective claims

00:22:52.340 --> 00:22:53.190
about Deep Blue.

00:22:53.190 --> 00:22:55.470
It made such and such a move.

00:22:55.470 --> 00:22:58.360
But the attributions of
intelligent chess playing,

00:22:58.360 --> 00:23:01.940
this move or that move,
it's all observer-relative.

00:23:01.940 --> 00:23:04.220
None of it is intrinsic.

00:23:04.220 --> 00:23:08.910
In the intrinsic sense in
which I have more intelligence

00:23:08.910 --> 00:23:14.170
than my dog, my computer has
zero intelligence-- absolutely

00:23:14.170 --> 00:23:15.900
none at all.

00:23:15.900 --> 00:23:18.690
It's a very complex
electronic circuit

00:23:18.690 --> 00:23:22.470
that we have designed to
behave as if it were thinking,

00:23:22.470 --> 00:23:24.220
as if it were intelligent.

00:23:24.220 --> 00:23:28.170
But in the strict sense, in
the observer-independent sense

00:23:28.170 --> 00:23:30.530
in which you and I
have intelligence,

00:23:30.530 --> 00:23:34.170
there is zero intelligence
in the computer.

00:23:34.170 --> 00:23:36.790
It's all observer-relative.

00:23:36.790 --> 00:23:38.820
And what goes for
intelligence goes

00:23:38.820 --> 00:23:42.150
for all of the key notions
in cognitive science.

00:23:42.150 --> 00:23:45.930
The notions of intelligence,
memory, perception,

00:23:45.930 --> 00:23:48.860
decision-making,
rationality-- all those

00:23:48.860 --> 00:23:51.730
have two different
senses, a sense

00:23:51.730 --> 00:23:55.290
where they identify
psychologically real phenomena

00:23:55.290 --> 00:23:58.880
of the sort that goes on
in you and me and the sort

00:23:58.880 --> 00:24:02.170
where they identify
observer-relative phenomena.

00:24:02.170 --> 00:24:03.930
But in the intrinsic
sense in which

00:24:03.930 --> 00:24:07.900
you and I have intelligence, the
machinery we're talking about

00:24:07.900 --> 00:24:09.700
has zero intelligence.

00:24:09.700 --> 00:24:12.390
It's no question of its
having more or less.

00:24:12.390 --> 00:24:14.580
It's not in the same
line of business.

00:24:14.580 --> 00:24:17.960
All of the intelligence is
in the eye of the beholder.

00:24:17.960 --> 00:24:21.190
It's all observer-relative.

00:24:21.190 --> 00:24:24.590
Now, you might say-- and I would
say-- so, for most purposes,

00:24:24.590 --> 00:24:26.020
it makes no difference at all.

00:24:26.020 --> 00:24:29.870
I mean, if you can design a
car that can drive itself,

00:24:29.870 --> 00:24:32.380
who cares if it's
conscious or not?

00:24:32.380 --> 00:24:35.025
Who cares if it literally
has any intelligence?

00:24:35.025 --> 00:24:35.980
And I agree.

00:24:35.980 --> 00:24:38.070
For most purposes,
it doesn't matter.

00:24:38.070 --> 00:24:41.940
For practical purposes, it
doesn't matter whether or not

00:24:41.940 --> 00:24:43.940
you have the
observer-independent

00:24:43.940 --> 00:24:46.200
or the observer-relative sense.

00:24:46.200 --> 00:24:49.010
The only point where
it matters, if you

00:24:49.010 --> 00:24:54.900
think there's some psychological
significance to the attribution

00:24:54.900 --> 00:24:56.980
of intelligence
to machinery which

00:24:56.980 --> 00:24:58.990
has no intrinsic intelligence.

00:24:58.990 --> 00:25:04.480
Now, notice the
intelligence by which we--

00:25:04.480 --> 00:25:08.630
the mental processes by which
we attribute intelligence

00:25:08.630 --> 00:25:12.020
to the computer
require consciousness.

00:25:12.020 --> 00:25:14.640
So the attribution of
observer-relativity

00:25:14.640 --> 00:25:16.420
is done by conscious agents.

00:25:16.420 --> 00:25:21.550
But the consciousness is not
itself observer-relative.

00:25:21.550 --> 00:25:25.240
The consciousness that creates
the observer-relative phenomena

00:25:25.240 --> 00:25:28.620
is not itself observer-relative.

00:25:28.620 --> 00:25:30.760
But now let's get to
the crunch line then.

00:25:30.760 --> 00:25:35.490
If information is
systematically ambiguous

00:25:35.490 --> 00:25:38.760
between an intrinsic sense,
in which you and I have

00:25:38.760 --> 00:25:41.790
information, and an
observer-relative sense,

00:25:41.790 --> 00:25:44.760
in which the computer
has information,

00:25:44.760 --> 00:25:47.160
what about computation?

00:25:47.160 --> 00:25:50.880
After all, computation,
that must surely

00:25:50.880 --> 00:25:53.210
be intrinsic to the computer.

00:25:53.210 --> 00:25:56.430
That's what we designed and
built the damn things to do,

00:25:56.430 --> 00:25:58.180
was computation.

00:25:58.180 --> 00:26:01.680
But, of course, the same
distinction applies.

00:26:01.680 --> 00:26:03.450
And I want to take
a drink of water

00:26:03.450 --> 00:26:05.680
and think about
history for a moment.

00:26:05.680 --> 00:26:13.130
When I first read
Alan Turing's article,

00:26:13.130 --> 00:26:18.470
it was called "Computing
Machinery and Intelligence."

00:26:18.470 --> 00:26:21.470
Now why didn't he call it
"Computers and Intelligence"?

00:26:21.470 --> 00:26:22.720
Well, you all know the answer.

00:26:22.720 --> 00:26:27.010
In those days, "computer"
meant "person who computes."

00:26:27.010 --> 00:26:31.520
A computer is like a
runner or a piano player.

00:26:31.520 --> 00:26:33.810
It's some human who
does the operation.

00:26:33.810 --> 00:26:36.220
Nowadays nobody would
think that because the word

00:26:36.220 --> 00:26:37.820
has changed its meaning.

00:26:37.820 --> 00:26:41.300
Or, rather, it's acquired
the systematic ambiguity

00:26:41.300 --> 00:26:46.510
between the
observer-relative sense

00:26:46.510 --> 00:26:49.020
and the
observer-independent sense.

00:26:49.020 --> 00:26:51.800
Now we think that
a computer names

00:26:51.800 --> 00:26:55.420
a type of machinery, not a human
being who actually carries out

00:26:55.420 --> 00:26:56.820
computation.

00:26:56.820 --> 00:26:59.540
But the same distinction
that we've been applying,

00:26:59.540 --> 00:27:01.380
the same distinction
that we discovered

00:27:01.380 --> 00:27:04.370
in all these other
cases, that applies

00:27:04.370 --> 00:27:09.070
to computation in the literal
or observer-independent

00:27:09.070 --> 00:27:14.150
sense in which I will now
do a simple computation.

00:27:14.150 --> 00:27:21.200
I will do a computation
using the addition function.

00:27:21.200 --> 00:27:22.340
And here's how it goes.

00:27:22.340 --> 00:27:24.470
It's not a very big deal.

00:27:24.470 --> 00:27:26.870
One plus one equals two.

00:27:26.870 --> 00:27:30.250
Now, the sense in which I
carried out a computation

00:27:30.250 --> 00:27:33.530
is absolutely intrinsic
and observer-independent.

00:27:33.530 --> 00:27:35.410
I don't care what
anybody says about me.

00:27:35.410 --> 00:27:38.220
If the experts say, well,
you weren't really computing.

00:27:38.220 --> 00:27:39.300
No, I was.

00:27:39.300 --> 00:27:41.970
I consciously did a computation.

00:27:41.970 --> 00:27:46.120
When my pocket calculator
does the same operation,

00:27:46.120 --> 00:27:49.170
the operation is entirely
observer-relative.

00:27:49.170 --> 00:27:53.840
Intrinsically all that goes on
is a set of electronic state

00:27:53.840 --> 00:27:56.630
transitions that we
have designed so that we

00:27:56.630 --> 00:27:58.640
can interpret computationally.

00:27:58.640 --> 00:28:02.720
And, again, to repeat, for most
purposes, it doesn't matter.

00:28:02.720 --> 00:28:05.110
When it matters is
when people say,

00:28:05.110 --> 00:28:12.480
well, we've created this race
of mechanical intelligences.

00:28:12.480 --> 00:28:14.540
And they might rise
up and overthrow us.

00:28:14.540 --> 00:28:18.890
Or they attribute some
other equally implausible

00:28:18.890 --> 00:28:21.840
psychological interpretation
to the machinery.

00:28:21.840 --> 00:28:24.740
In commercial computers,
the computation

00:28:24.740 --> 00:28:26.810
is observer-relative.

00:28:26.810 --> 00:28:29.010
Now notice, you all
know that doesn't mean

00:28:29.010 --> 00:28:30.980
it's epistemically subjective.

00:28:30.980 --> 00:28:33.610
And I pay a lot of
money so that Apple

00:28:33.610 --> 00:28:37.240
will make a piece of machinery
that will implement programs

00:28:37.240 --> 00:28:40.280
that my earlier computers
were not intelligent enough

00:28:40.280 --> 00:28:41.050
to implement.

00:28:41.050 --> 00:28:43.510
Notice the observer-relative
attribution

00:28:43.510 --> 00:28:45.060
of intelligence here.

00:28:45.060 --> 00:28:47.390
So it's absolutely
harmless unless you

00:28:47.390 --> 00:28:50.360
think there's some
psychological significance.

00:28:50.360 --> 00:28:54.420
Now what is lacking, of
course, in the machinery, which

00:28:54.420 --> 00:28:56.110
we have in human
beings which makes

00:28:56.110 --> 00:28:59.030
the difference
between the observer

00:28:59.030 --> 00:29:04.830
relativity of the computation
in the commercial computer

00:29:04.830 --> 00:29:08.120
and the intrinsic or observer
independent computation

00:29:08.120 --> 00:29:11.120
that I have just performed on
the blackboard, what's lacking

00:29:11.120 --> 00:29:13.500
is consciousness.

00:29:13.500 --> 00:29:17.040
All observer-relative
phenomena are

00:29:17.040 --> 00:29:20.100
created by human and
animal consciousness.

00:29:20.100 --> 00:29:22.090
But the human and
animal consciousness

00:29:22.090 --> 00:29:26.140
that creates them is not
itself observer-relative.

00:29:26.140 --> 00:29:30.490
So there's an intrinsic mental
phenomena, the consciousness

00:29:30.490 --> 00:29:35.140
of the agent, which creates the
observer-relative phenomena,

00:29:35.140 --> 00:29:38.680
or interprets the mechanical
system in an observer

00:29:38.680 --> 00:29:40.360
relative fashion.

00:29:40.360 --> 00:29:43.480
But the consciousness that
creates observer relativity

00:29:43.480 --> 00:29:45.940
is not itself observer-relative.

00:29:45.940 --> 00:29:47.980
It's intrinsic.

00:29:47.980 --> 00:29:51.480
Now, I wanted to save plenty
of time for discussion.

00:29:51.480 --> 00:29:54.390
So let me catch my
breath and then give

00:29:54.390 --> 00:29:57.600
a kind of summary of
the main thrust of what

00:29:57.600 --> 00:29:59.132
I've been arguing.

00:29:59.132 --> 00:30:00.715
And one of things I
haven't emphasized

00:30:00.715 --> 00:30:02.810
but I want to
emphasize now, and that

00:30:02.810 --> 00:30:07.010
is most of the apparatus,
the conceptual apparatus,

00:30:07.010 --> 00:30:12.190
we have for discussing these
issues is totally obsolete.

00:30:12.190 --> 00:30:15.010
The difference between the
mental and the physical,

00:30:15.010 --> 00:30:20.900
the difference between the
social and the individual,

00:30:20.900 --> 00:30:27.780
and the distinction between
those features which

00:30:27.780 --> 00:30:33.800
can be identified in an
observer-relative fashion,

00:30:33.800 --> 00:30:36.950
such as computation,
and those which

00:30:36.950 --> 00:30:39.510
can be identified in an
observer-independent fashion,

00:30:39.510 --> 00:30:40.940
such as computation.

00:30:40.940 --> 00:30:44.840
We're confused by the
vocabulary which doesn't make

00:30:44.840 --> 00:30:46.910
the matters sufficiently clear.

00:30:46.910 --> 00:30:48.600
And I'm going to
end this discussion

00:30:48.600 --> 00:30:51.585
by going through some of the
elements of the vocabulary.

00:30:51.585 --> 00:30:53.890
Now, let me have a drink of
water and catch my breath.

00:31:00.500 --> 00:31:03.000
Let's start with
that old question,

00:31:03.000 --> 00:31:05.860
could a machine think?

00:31:05.860 --> 00:31:09.540
Well, I said the
vocabulary was obsolete.

00:31:09.540 --> 00:31:11.380
And the vocabulary of
humans and machines

00:31:11.380 --> 00:31:16.260
is already obsolete
because if by machine is

00:31:16.260 --> 00:31:19.480
meant a physical system
capable of performing

00:31:19.480 --> 00:31:22.305
certain functions, then
we're all machines.

00:31:22.305 --> 00:31:23.990
I'm a machine.

00:31:23.990 --> 00:31:25.120
You're a machine.

00:31:25.120 --> 00:31:29.230
And my guess is only
machines could think.

00:31:29.230 --> 00:31:30.180
Why?

00:31:30.180 --> 00:31:32.000
Well that's the next step.

00:31:32.000 --> 00:31:37.650
Thinking is a biological
process created in the brain

00:31:37.650 --> 00:31:43.660
by certain quite complex,
but insufficiently understood

00:31:43.660 --> 00:31:45.750
neurobiological processes.

00:31:45.750 --> 00:31:49.060
So in order to think,
you've got to have a brain,

00:31:49.060 --> 00:31:52.630
or you've got to have something
with equivalent causal powers

00:31:52.630 --> 00:31:53.377
to the brain.

00:31:53.377 --> 00:31:55.710
We might figure out a way to
do it in some other medium.

00:31:55.710 --> 00:31:58.240
We don't know enough about
how the brain does it.

00:31:58.240 --> 00:32:01.500
So we don't know how to
create it artificially.

00:32:01.500 --> 00:32:04.090
So could a machine think?

00:32:04.090 --> 00:32:06.270
Human beings are machines.

00:32:06.270 --> 00:32:08.680
Yes, but could you make
an artificial machine

00:32:08.680 --> 00:32:09.520
that could think?

00:32:09.520 --> 00:32:10.160
Why not?

00:32:10.160 --> 00:32:12.120
It's like an artificial heart.

00:32:12.120 --> 00:32:14.925
The question, can you build
an artificial brain that

00:32:14.925 --> 00:32:16.780
can think, is like
the question, can you

00:32:16.780 --> 00:32:19.440
build an artificial
heart that pumps blood.

00:32:19.440 --> 00:32:22.880
We know how the
heart does it, so we

00:32:22.880 --> 00:32:24.360
know how to do it artificially.

00:32:24.360 --> 00:32:27.140
We don't know how the brain
does it, so we have no idea.

00:32:27.140 --> 00:32:27.960
Let me repeat this.

00:32:27.960 --> 00:32:31.200
We have no idea how
to create a thinking

00:32:31.200 --> 00:32:34.040
machine because we don't
know how the brain does it.

00:32:34.040 --> 00:32:36.860
All we can do is
a simulation using

00:32:36.860 --> 00:32:38.330
some sort of formal system.

00:32:38.330 --> 00:32:39.870
But that's not the real thing.

00:32:39.870 --> 00:32:41.870
You don't create
thinking that way,

00:32:41.870 --> 00:32:44.900
whereas the artificial heart
really does pump blood.

00:32:44.900 --> 00:32:46.410
So we had two questions.

00:32:46.410 --> 00:32:48.690
Could a machine think?

00:32:48.690 --> 00:32:52.050
And could an artificially-made
machine think?

00:32:52.050 --> 00:32:54.700
Answer the question
one is obviously yes.

00:32:54.700 --> 00:32:57.720
Answer to question two
is, we don't know yet,

00:32:57.720 --> 00:33:00.030
but there's no
obstacle in principle.

00:33:00.030 --> 00:33:01.910
Does everybody see that?

00:33:01.910 --> 00:33:04.510
Building an artificial
brain is like building

00:33:04.510 --> 00:33:05.730
an artificial heart.

00:33:05.730 --> 00:33:09.440
The only thing is no
one has begun to try it.

00:33:09.440 --> 00:33:12.490
They haven't begun to try it
because they have no idea how

00:33:12.490 --> 00:33:13.940
the actual brain does it.

00:33:13.940 --> 00:33:16.930
So they don't know how
to imitate actual brains.

00:33:16.930 --> 00:33:21.380
Well, OK, but could you build
an artificial brain that

00:33:21.380 --> 00:33:24.386
could think out of some
completely different materials,

00:33:24.386 --> 00:33:25.760
out of something
that had nothing

00:33:25.760 --> 00:33:28.520
to do with nucleo-proteins,
had nothing to

00:33:28.520 --> 00:33:30.410
with neurons and
neurotransmitters

00:33:30.410 --> 00:33:32.000
and all the rest of it.

00:33:32.000 --> 00:33:34.650
And the answer is,
again, we don't know.

00:33:34.650 --> 00:33:37.150
That seems to me
an open question.

00:33:37.150 --> 00:33:39.780
If we knew how the
brain did it, we

00:33:39.780 --> 00:33:42.040
might be able to
define-- I mean,

00:33:42.040 --> 00:33:45.940
be able to design machines
that could do it using

00:33:45.940 --> 00:33:49.190
some completely different
biochemistry in a way

00:33:49.190 --> 00:33:52.860
that the artificial
heart doesn't use

00:33:52.860 --> 00:33:54.540
muscle tissue to pump blood.

00:33:54.540 --> 00:33:57.200
You don't need muscle
tissue to pump blood.

00:33:57.200 --> 00:34:00.320
And maybe you don't need brain
tissue to create consciousness.

00:34:00.320 --> 00:34:01.410
We just are ignorant.

00:34:01.410 --> 00:34:04.520
But notice there's no
obstacle in principle.

00:34:04.520 --> 00:34:08.290
The problem is no one has begun
to think about how you would

00:34:08.290 --> 00:34:13.130
build a thinking machine,
how you'd build a thinking

00:34:13.130 --> 00:34:16.400
machine out of some
material other than neurons

00:34:16.400 --> 00:34:21.409
because they haven't begun
to think about how we might

00:34:21.409 --> 00:34:23.489
duplicate and not
merely simulate

00:34:23.489 --> 00:34:26.350
what the brain actually does.

00:34:26.350 --> 00:34:28.719
So the question,
could a machine think,

00:34:28.719 --> 00:34:30.639
could an artificial
machine think,

00:34:30.639 --> 00:34:32.750
could an artificial
machine made out

00:34:32.750 --> 00:34:34.920
of some completely
different materials,

00:34:34.920 --> 00:34:37.420
could those machines think?

00:34:37.420 --> 00:34:39.489
And now the next question
is the obvious one.

00:34:39.489 --> 00:34:41.550
Well, how about a computer?

00:34:41.550 --> 00:34:43.090
Could a computer think?

00:34:43.090 --> 00:34:45.350
Now, you have to
be careful here.

00:34:45.350 --> 00:34:49.860
Because if a computer is defined
as anything that can carry out

00:34:49.860 --> 00:34:53.380
computations, well, I just did.

00:34:53.380 --> 00:34:55.610
This is a computation.

00:34:55.610 --> 00:34:57.320
So I'm a computer.

00:34:57.320 --> 00:34:59.250
And so are all of you.

00:34:59.250 --> 00:35:02.780
Any conscious agent
capable of carrying out

00:35:02.780 --> 00:35:07.740
that simple computation is
capable, is both a computer

00:35:07.740 --> 00:35:09.920
and capable of thinking.

00:35:09.920 --> 00:35:11.870
So my guess is-- and
I didn't have a chance

00:35:11.870 --> 00:35:15.820
to develop this idea-- is
that not only can computers

00:35:15.820 --> 00:35:18.050
think-- you and
me-- but my guess

00:35:18.050 --> 00:35:20.380
is that anything
capable of thinking

00:35:20.380 --> 00:35:22.640
would have to be
capable of carrying out

00:35:22.640 --> 00:35:24.930
simple computations.

00:35:24.930 --> 00:35:28.290
But now what is the
status of computation?

00:35:28.290 --> 00:35:33.080
Well, the key element here is
the one I've already mentioned.

00:35:33.080 --> 00:35:36.540
Computation has two senses,
an observer-independent sense

00:35:36.540 --> 00:35:39.660
and an observer-relative sense.

00:35:39.660 --> 00:35:43.090
In the observer-relative
sense, anything

00:35:43.090 --> 00:35:47.361
is a computer if you can ascribe
a computational interpretation.

00:35:47.361 --> 00:35:47.860
Watch.

00:35:47.860 --> 00:35:50.230
I'll show you a very
simple computer.

00:35:50.230 --> 00:35:54.470
That computer just computed
a well-known function.

00:35:54.470 --> 00:35:57.470
s equals one-half gt squared.

00:35:57.470 --> 00:35:59.530
And if you had a
good-enough watch,

00:35:59.530 --> 00:36:02.500
you could actually
time and figure out

00:36:02.500 --> 00:36:05.150
how far the damn thing fell.

00:36:05.150 --> 00:36:05.790
Everybody sees.

00:36:05.790 --> 00:36:07.880
It's elementary mathematics.

00:36:07.880 --> 00:36:10.920
So if this is a
computer, then anything

00:36:10.920 --> 00:36:13.680
is a computer because
being a computer

00:36:13.680 --> 00:36:16.460
in the observer-relative
sense is not

00:36:16.460 --> 00:36:18.320
an intrinsic feature
of an object,

00:36:18.320 --> 00:36:21.230
but a feature of our
interpretation of the physics

00:36:21.230 --> 00:36:22.374
of the phenomenon.

00:36:22.374 --> 00:36:23.790
In the old Chinese
room days, when

00:36:23.790 --> 00:36:27.210
I had to debate these guys,
at one point, I'd take my pen,

00:36:27.210 --> 00:36:32.780
slam it on a table, and say
that is a digital computer.

00:36:32.780 --> 00:36:36.120
It just happens to have a
boring computer program.

00:36:36.120 --> 00:36:38.951
The program says stay there.

00:36:38.951 --> 00:36:42.742
The point is nobody
ever called me on this

00:36:42.742 --> 00:36:43.950
because it's obviously right.

00:36:43.950 --> 00:36:46.825
It satisfies a
textbook definition.

00:36:46.825 --> 00:36:48.200
You know, in the
early days, they

00:36:48.200 --> 00:36:51.490
tried to snow me with a whole
lot of technical razzmatazz.

00:36:51.490 --> 00:36:54.790
"You've left out the distinction
between the virtual machine

00:36:54.790 --> 00:36:57.240
and the non-virtual
machine" or "you've

00:36:57.240 --> 00:36:58.631
left out the transducers."

00:36:58.631 --> 00:37:00.880
You see, I didn't know what
the hell a transducer was,

00:37:00.880 --> 00:37:01.810
a virtual machine.

00:37:01.810 --> 00:37:04.370
But it takes about five
minutes to learn those things.

00:37:04.370 --> 00:37:07.830
Anyway, so now we get to the
crucial question in this.

00:37:07.830 --> 00:37:10.540
If computers can think,
man-made computers

00:37:10.540 --> 00:37:14.640
can think, machines can
think, what about computation?

00:37:14.640 --> 00:37:20.440
Does computation name a
machine, a thinking process?

00:37:20.440 --> 00:37:24.210
That is, is computation,
as defined by Alan Turing,

00:37:24.210 --> 00:37:26.870
is that itself
sufficient for thinking?

00:37:26.870 --> 00:37:28.790
And you now know
the answer to that.

00:37:28.790 --> 00:37:32.430
In the observer-relative
sense, the answer is no.

00:37:32.430 --> 00:37:35.170
Computation is not
a fact of nature.

00:37:35.170 --> 00:37:37.990
It's a fact of our
interpretation.

00:37:37.990 --> 00:37:42.180
And insofar as we can create
artificial machines that

00:37:42.180 --> 00:37:46.230
carry out computations,
the computation by itself

00:37:46.230 --> 00:37:48.570
is never going to be
sufficient for thinking

00:37:48.570 --> 00:37:52.500
or any other cognitive process
because the computation is

00:37:52.500 --> 00:37:56.220
defined purely formally
or syntactically.

00:37:56.220 --> 00:37:59.940
Turing machines are not
to be found in nature.

00:37:59.940 --> 00:38:03.930
They're to be found in our
interpretations of nature.

00:38:03.930 --> 00:38:05.862
Now, let me add, a lot
of people think, ah,

00:38:05.862 --> 00:38:07.820
this debate has something
to do with technology

00:38:07.820 --> 00:38:10.310
or there'll be
advances in technology.

00:38:10.310 --> 00:38:12.070
I think that
technology's wonderful.

00:38:12.070 --> 00:38:13.250
And I welcome it.

00:38:13.250 --> 00:38:17.740
And I see no limits to the
possibilities of technology.

00:38:17.740 --> 00:38:20.640
My aim is this talk is
simply to get across,

00:38:20.640 --> 00:38:24.720
you shouldn't misunderstand the
philosophical, psychological,

00:38:24.720 --> 00:38:28.890
and, indeed, scientific
implication of the technology.

00:38:28.890 --> 00:38:29.850
Thank you very much.

00:38:29.850 --> 00:38:31.320
[APPLAUSE]

00:38:41.267 --> 00:38:42.600
JOHN BRACAGLIA: Thank you, John.

00:38:42.600 --> 00:38:44.183
JOHN SEARLE: I'm
sorry I talk so fast,

00:38:44.183 --> 00:38:46.360
but I want to leave plenty
of time for questions.

00:38:46.360 --> 00:38:49.120
JOHN BRACAGLIA: We'll start
with one question from Mr. Ray

00:38:49.120 --> 00:38:49.620
Kurzweil.

00:38:49.620 --> 00:38:50.676
RAY KURZWEIL: Is this on?

00:38:50.676 --> 00:38:51.509
[INTERPOSING VOICES]

00:38:55.373 --> 00:38:58.030
RAY KURZWEIL:
Well, thanks, John.

00:38:58.030 --> 00:39:02.040
I'm one of those guys you've
been debating this issue for 18

00:39:02.040 --> 00:39:02.830
years, I think.

00:39:05.350 --> 00:39:10.100
And I would praise the
Chinese room for its longevity

00:39:10.100 --> 00:39:14.420
because it does really get
at the apparent absurdity

00:39:14.420 --> 00:39:17.620
that some deterministic
process like computation

00:39:17.620 --> 00:39:20.898
could possibly be responsible
for something like thinking.

00:39:20.898 --> 00:39:22.273
And you point out
the distinction

00:39:22.273 --> 00:39:29.420
of thinking between its effects
and the subjective states,

00:39:29.420 --> 00:39:32.450
which is a synonym
for consciousness.

00:39:32.450 --> 00:39:41.270
So I quoted you here in my
book "Singularity is Near,"

00:39:41.270 --> 00:39:46.900
at the equivalence of neurons
and even brains with machines.

00:39:46.900 --> 00:39:51.320
So then I took your argument
why a machine and a computer

00:39:51.320 --> 00:39:56.600
could not truly understand
what it's doing and simply

00:39:56.600 --> 00:39:59.720
substituted human
brain for computers,

00:39:59.720 --> 00:40:01.790
since you said they
were equivalent,

00:40:01.790 --> 00:40:04.490
and neurotransmitter
concentrations and related

00:40:04.490 --> 00:40:09.090
mechanisms for formal
symbols, since basically

00:40:09.090 --> 00:40:10.770
neurotransmitter
concentrations, it's

00:40:10.770 --> 00:40:14.120
just a mechanistic concept.

00:40:14.120 --> 00:40:17.240
And so you wrote, with
those substitutions,

00:40:17.240 --> 00:40:20.110
the human brain
succeeds by manipulating

00:40:20.110 --> 00:40:24.640
neurotransmitter concentrations
and other related mechanisms.

00:40:24.640 --> 00:40:26.290
The neurotransmitter
concentrations

00:40:26.290 --> 00:40:28.260
and related
mechanisms themselves

00:40:28.260 --> 00:40:30.000
are quite meaningless.

00:40:30.000 --> 00:40:32.660
They only have the meaning
we have attached to them.

00:40:32.660 --> 00:40:34.490
The human brain knows
nothing of this.

00:40:34.490 --> 00:40:37.720
It just shuffles the
neurotransmitter concentrations

00:40:37.720 --> 00:40:39.670
and related mechanisms.

00:40:39.670 --> 00:40:43.836
Therefore, the human brain
cannot have true understanding.

00:40:43.836 --> 00:40:44.335
So--

00:40:44.335 --> 00:40:46.300
[LAUGHTER]

00:40:46.300 --> 00:40:49.500
JOHN SEARLE: There's something
interesting variations, again,

00:40:49.500 --> 00:40:50.490
on my original.

00:40:50.490 --> 00:40:54.170
RAY KURZWEIL: But the
point I'd like to make,

00:40:54.170 --> 00:40:57.010
and that I'd be interested
in your addressing,

00:40:57.010 --> 00:41:00.490
is the nature of
consciousness because, I mean,

00:41:00.490 --> 00:41:04.870
you said today, and you
wrote, the essential thing

00:41:04.870 --> 00:41:06.820
is to recognize
that consciousness

00:41:06.820 --> 00:41:10.230
is a biological processes
like digestion, lactation,

00:41:10.230 --> 00:41:12.930
photosynthesis, or mitosis.

00:41:12.930 --> 00:41:14.960
We know that brains
cause consciousness

00:41:14.960 --> 00:41:18.050
with specific
biological mechanisms.

00:41:18.050 --> 00:41:21.072
But how do we know that
a brain is conscious?

00:41:21.072 --> 00:41:22.530
How do you know
that I'm conscious?

00:41:22.530 --> 00:41:24.721
And how do you--

00:41:24.721 --> 00:41:25.720
JOHN SEARLE: [INAUDIBLE]

00:41:25.720 --> 00:41:27.720
RAY KURZWEIL: And how do we know
if a computer was conscious?

00:41:27.720 --> 00:41:29.440
We don't have a
computer today that

00:41:29.440 --> 00:41:32.990
seems conscious, that's
convincing in its responses.

00:41:32.990 --> 00:41:35.280
But my prediction is we will.

00:41:35.280 --> 00:41:37.800
We can argue about
the time frame.

00:41:37.800 --> 00:41:40.680
And when we do, how do we
know if it's conscious of

00:41:40.680 --> 00:41:43.290
it just seems conscious?

00:41:43.290 --> 00:41:45.670
How do we measure that?

00:41:45.670 --> 00:41:47.670
JOHN SEARLE: Well, there
are two questions here.

00:41:47.670 --> 00:41:52.370
One is, if you do a substitution
of words that I didn't use

00:41:52.370 --> 00:41:55.820
and the words I did use, can
you get these observed results?

00:41:55.820 --> 00:41:57.480
And, of course, you can do that.

00:41:57.480 --> 00:42:01.310
That's a well-known
technique of politicians.

00:42:01.310 --> 00:42:02.910
But that wasn't the claim.

00:42:02.910 --> 00:42:06.170
What is the difference between
the computer and the brain?

00:42:06.170 --> 00:42:10.440
In one sentence, the brain
is a causal mechanism

00:42:10.440 --> 00:42:16.060
that produces consciousness
by a certain rather complex

00:42:16.060 --> 00:42:19.750
and still imperfectly understood
neurobiological processes.

00:42:19.750 --> 00:42:23.480
But those are quite specific
to a certain electrochemistry.

00:42:23.480 --> 00:42:24.820
We just don't know the details.

00:42:24.820 --> 00:42:29.270
But we don't know if you mess
around in the synaptic cleft,

00:42:29.270 --> 00:42:31.250
you're going to
get weird effects.

00:42:31.250 --> 00:42:33.370
How does cocaine work?

00:42:33.370 --> 00:42:36.470
Well, it isn't because it's
got a peculiar computational

00:42:36.470 --> 00:42:37.640
capacity.

00:42:37.640 --> 00:42:39.570
Because it messes
with the capacity

00:42:39.570 --> 00:42:43.180
of the postsynaptic receptors
to reabsorb quite specific

00:42:43.180 --> 00:42:45.892
neurotransmitters,
norepinephrine--

00:42:45.892 --> 00:42:46.850
what are the other two?

00:42:46.850 --> 00:42:49.210
God, I'm flunking the exam here.

00:42:49.210 --> 00:42:50.540
Dopamine.

00:42:50.540 --> 00:42:51.460
Gaba is the third.

00:42:51.460 --> 00:42:57.760
Anyway, the brain, like the
stomach or any other organ,

00:42:57.760 --> 00:43:01.000
is a specific causal mechanism.

00:43:01.000 --> 00:43:04.490
And it functions on specific
biochemical principles.

00:43:04.490 --> 00:43:06.860
The problem of
the computer is it

00:43:06.860 --> 00:43:09.750
has nothing to do
with the specifics

00:43:09.750 --> 00:43:11.850
of the implementation.

00:43:11.850 --> 00:43:13.770
Any implementation
will do provided

00:43:13.770 --> 00:43:16.620
it's sufficient to carry out
the steps in the program.

00:43:16.620 --> 00:43:19.530
Programs are purely
formal or syntactical.

00:43:19.530 --> 00:43:21.120
The brain is not.

00:43:21.120 --> 00:43:23.960
The brain is a specific
biological organ

00:43:23.960 --> 00:43:26.120
that operates on
specific principles.

00:43:26.120 --> 00:43:28.340
And to create a
conscious machine,

00:43:28.340 --> 00:43:30.920
we've got to know how to
duplicate the causal powers

00:43:30.920 --> 00:43:32.730
of those principles.

00:43:32.730 --> 00:43:36.140
Now, the computer
doesn't in that way

00:43:36.140 --> 00:43:40.670
work as a causal mechanism
producing higher level

00:43:40.670 --> 00:43:41.610
features.

00:43:41.610 --> 00:43:46.740
Rather, computation names an
abstract mathematical process

00:43:46.740 --> 00:43:50.570
that we have found ways to
implement in specific hardware.

00:43:50.570 --> 00:43:54.040
But the hardware is not
essential to the computation.

00:43:54.040 --> 00:43:57.510
Any system that can
carry out the computation

00:43:57.510 --> 00:43:59.344
will be equivalent.

00:43:59.344 --> 00:44:01.010
Now, the second
question is about how do

00:44:01.010 --> 00:44:02.570
you know about consciousness.

00:44:02.570 --> 00:44:04.630
Well, think about real life.

00:44:04.630 --> 00:44:08.120
How do I know my dog
Tarski is conscious

00:44:08.120 --> 00:44:13.370
and this thing here, my
smartphone, is not conscious?

00:44:13.370 --> 00:44:15.820
I don't have any doubts
about either one.

00:44:15.820 --> 00:44:19.120
I can tell that Tarski
is conscious not

00:44:19.120 --> 00:44:20.420
on behavioristic grounds.

00:44:20.420 --> 00:44:22.961
People say, well, it's because
he behaves like a human being.

00:44:22.961 --> 00:44:23.840
He doesn't.

00:44:23.840 --> 00:44:25.430
See, human beings
I know when they

00:44:25.430 --> 00:44:29.500
see me don't rush up and lick
my hands and wag their tails.

00:44:29.500 --> 00:44:30.950
They just don't.

00:44:30.950 --> 00:44:32.190
My friends don't do that.

00:44:32.190 --> 00:44:33.480
But Tarski does.

00:44:33.480 --> 00:44:35.980
I can see that
Tarski is conscious

00:44:35.980 --> 00:44:37.860
because he's got
a machinery that's

00:44:37.860 --> 00:44:40.130
relatively similar to my own.

00:44:40.130 --> 00:44:41.199
Those are his eyes.

00:44:41.199 --> 00:44:41.990
These are his ears.

00:44:41.990 --> 00:44:43.320
This is his skin.

00:44:43.320 --> 00:44:50.310
He has mechanisms that mediate
the input stimuli to the output

00:44:50.310 --> 00:44:53.630
behavior that are relatively
similar to human mechanisms.

00:44:53.630 --> 00:44:56.350
This is why I'm completely
confident that Tarski's

00:44:56.350 --> 00:44:57.610
conscious.

00:44:57.610 --> 00:45:01.220
I don't know anything
about fleas and termites.

00:45:01.220 --> 00:45:04.070
You know, your typical
termite's got 100,000 neurons.

00:45:04.070 --> 00:45:05.340
Is that enough?

00:45:05.340 --> 00:45:08.755
Well, I lose 100,000
on a big weekend.

00:45:08.755 --> 00:45:11.590
So I don't know if that's
enough for consciousness.

00:45:11.590 --> 00:45:12.860
But that's a factual question.

00:45:12.860 --> 00:45:14.580
I'll leave that to the experts.

00:45:14.580 --> 00:45:17.030
But as far as human
beings are concerned

00:45:17.030 --> 00:45:19.609
there isn't any question
that everybody in this room

00:45:19.609 --> 00:45:20.150
is conscious.

00:45:20.150 --> 00:45:22.316
I mean, maybe that guy over
there is falling asleep,

00:45:22.316 --> 00:45:27.700
but there's no question about
what the general-- it's not

00:45:27.700 --> 00:45:29.030
even a theory that I hold.

00:45:29.030 --> 00:45:30.830
It's a background
presupposition.

00:45:30.830 --> 00:45:32.720
The way I assume that
the floor is solid,

00:45:32.720 --> 00:45:35.230
I simply take it for granted
that everybody's conscious.

00:45:35.230 --> 00:45:37.290
If forced to
justify it, I could.

00:45:37.290 --> 00:45:41.110
Now, there's always a
problem about the details

00:45:41.110 --> 00:45:42.690
of other minds.

00:45:42.690 --> 00:45:45.930
Of course, I know
you're conscious.

00:45:45.930 --> 00:45:49.060
But are you suffering the
angst of post-industrial man

00:45:49.060 --> 00:45:50.660
under late capitalism?

00:45:50.660 --> 00:45:53.390
Well, I have a lot of
friends who claim they do.

00:45:53.390 --> 00:45:55.840
And they think I'm
philistine because I don't.

00:45:55.840 --> 00:45:56.710
But that's tougher.

00:45:56.710 --> 00:45:58.740
We'd have to have a
conversation about that.

00:45:58.740 --> 00:46:01.160
But for consciousness,
it's not a real problem

00:46:01.160 --> 00:46:03.560
in a real-life case.

00:46:03.560 --> 00:46:05.390
AUDIENCE: So you've
said that we haven't

00:46:05.390 --> 00:46:08.360
begun to understand how
brains work or build

00:46:08.360 --> 00:46:09.320
comparable machines.

00:46:09.320 --> 00:46:12.020
But imagine in the future we do.

00:46:12.020 --> 00:46:16.440
So we can run a simulation,
as you put it, of a brain.

00:46:16.440 --> 00:46:18.580
And then we interface
it with reality

00:46:18.580 --> 00:46:21.590
through motor output,
sensory input.

00:46:21.590 --> 00:46:23.150
What's the difference
between that

00:46:23.150 --> 00:46:25.205
and a brain, which
you say you know

00:46:25.205 --> 00:46:26.330
is producing consciousness?

00:46:26.330 --> 00:46:27.050
In

00:46:27.050 --> 00:46:30.180
JOHN SEARLE: In some cases,
there's no difference at all.

00:46:30.180 --> 00:46:32.830
And the difference
doesn't matter.

00:46:32.830 --> 00:46:34.512
If you've got a
machine-- I hope you

00:46:34.512 --> 00:46:36.720
guys are, in fact, building
it because the newspapers

00:46:36.720 --> 00:46:39.300
say you are.

00:46:39.300 --> 00:46:41.840
If you've got a
program that'll drive

00:46:41.840 --> 00:46:45.170
my car without a conscious
driver, that's great.

00:46:45.170 --> 00:46:46.610
I think that's wonderful.

00:46:46.610 --> 00:46:49.710
The question is not, what
can the technology do?

00:46:49.710 --> 00:46:52.250
My daddy was an electrical
engineer for AT&amp;T.

00:46:52.250 --> 00:46:53.690
And his biggest
disappointment was

00:46:53.690 --> 00:46:56.660
I decided to be a
philosopher, for God's sake,

00:46:56.660 --> 00:47:02.560
instead of going to Bell
Labs and MIT as he had hoped.

00:47:02.560 --> 00:47:06.090
So I have no problem with the
success of the technology.

00:47:06.090 --> 00:47:08.520
The question is,
what does it mean?

00:47:08.520 --> 00:47:10.100
Of course, if
you've got a machine

00:47:10.100 --> 00:47:13.485
that can drive a
car as well as I,

00:47:13.485 --> 00:47:16.610
or probably better than
I can, then so much

00:47:16.610 --> 00:47:17.820
the better for the machinery.

00:47:17.820 --> 00:47:21.690
The question is, what is the
philosophical psychological

00:47:21.690 --> 00:47:23.650
scientific significance of that?

00:47:23.650 --> 00:47:25.700
And if you think, well,
that means you've created

00:47:25.700 --> 00:47:27.550
consciousness, you have not.

00:47:27.550 --> 00:47:29.670
You have to have more
to create consciousness.

00:47:29.670 --> 00:47:31.890
And for a whole lot of
things, consciousness

00:47:31.890 --> 00:47:33.840
matters desperately.

00:47:33.840 --> 00:47:35.780
In this case of this
book that I reviewed,

00:47:35.780 --> 00:47:37.950
where the guy said,
well, they got machines

00:47:37.950 --> 00:47:40.770
that are going to rise
up and overthrow us all,

00:47:40.770 --> 00:47:43.170
it's not a serious possibility
because the machines

00:47:43.170 --> 00:47:44.570
have no consciousness.

00:47:44.570 --> 00:47:47.580
They have no conscious
psychological state.

00:47:47.580 --> 00:47:51.155
It's about like saying the shoes
might get up out of the closet

00:47:51.155 --> 00:47:52.914
and walk all over us.

00:47:52.914 --> 00:47:55.080
After all, we've been walking
on them for centuries,

00:47:55.080 --> 00:47:56.204
why don't they strike back?

00:47:56.204 --> 00:47:58.820
It is not a real-life worry.

00:47:58.820 --> 00:47:59.540
Yeah?

00:47:59.540 --> 00:48:01.890
AUDIENCE: The difference that
I'm interested in-- sorry,

00:48:01.890 --> 00:48:04.100
the similarity I'm interested
in is not necessarily

00:48:04.100 --> 00:48:06.790
the output or the
outcome of the system,

00:48:06.790 --> 00:48:11.850
but rather, that is, it has
the internal causal similarity

00:48:11.850 --> 00:48:13.970
to the brain that you mentioned.

00:48:13.970 --> 00:48:16.220
JOHN SEARLE: Yeah, that's
a factual question.

00:48:16.220 --> 00:48:20.420
The question is, to what
extent are the processes that

00:48:20.420 --> 00:48:24.300
go on in the computer
isomorphic to processes

00:48:24.300 --> 00:48:25.740
that go on in the brain?

00:48:25.740 --> 00:48:30.170
As far as we know,
not very much.

00:48:30.170 --> 00:48:32.060
I mean, the
chess-playing programs

00:48:32.060 --> 00:48:33.600
were a good example of this.

00:48:33.600 --> 00:48:36.700
In the early days
of AI, they tried

00:48:36.700 --> 00:48:40.200
to interview great chess players
and find out what their thought

00:48:40.200 --> 00:48:42.560
processes were and
get them to try

00:48:42.560 --> 00:48:44.320
to duplicate that on computers.

00:48:44.320 --> 00:48:47.140
Well, we now know
how Deep Blue worked.

00:48:47.140 --> 00:48:50.910
Deep Blue can calculate
250 million chess positions

00:48:50.910 --> 00:48:52.560
in one second.

00:48:52.560 --> 00:48:55.740
See, chess is a trivial game
from a games theoretical point

00:48:55.740 --> 00:48:58.130
of view because you have
perfect information.

00:48:58.130 --> 00:49:00.240
And you have a finite
number of possibilities.

00:49:00.240 --> 00:49:01.960
So there are x number
of possibilities

00:49:01.960 --> 00:49:03.470
of responding to a
move and x number

00:49:03.470 --> 00:49:05.050
of possibilities for that move.

00:49:05.050 --> 00:49:09.880
It's interesting to us because
of the exponential problem.

00:49:09.880 --> 00:49:13.430
And it's very hard to
program computers that

00:49:13.430 --> 00:49:18.960
can go very many steps in
the exponents, but IBM did.

00:49:18.960 --> 00:49:20.570
It's of no
psychological interest.

00:49:20.570 --> 00:49:22.310
And to their credit,
the people in AI

00:49:22.310 --> 00:49:25.210
did not claim it as a great
victory for-- at least the ones

00:49:25.210 --> 00:49:27.309
I know didn't claim
it as a victory for AI

00:49:27.309 --> 00:49:28.850
because they could
see it had nothing

00:49:28.850 --> 00:49:30.900
to do with human cognition.

00:49:30.900 --> 00:49:34.560
So my guess is it's an
interesting philosophical

00:49:34.560 --> 00:49:38.520
question-- or psychological
question-- to what extent

00:49:38.520 --> 00:49:41.190
the actual processes
in the brain

00:49:41.190 --> 00:49:43.930
mirror a computational
simulation.

00:49:43.930 --> 00:49:45.930
And, of course, to
some respect, they do.

00:49:45.930 --> 00:49:47.560
That's why computational
simulations

00:49:47.560 --> 00:49:49.520
are interesting in
all sorts of fields

00:49:49.520 --> 00:49:51.550
and not just in
psychology, because you

00:49:51.550 --> 00:49:54.610
can simulate all sorts of
processes that are going on.

00:49:54.610 --> 00:49:56.830
But that's not strong AI.

00:49:56.830 --> 00:50:00.060
Strong AI says the simulation
isn't just a simulation.

00:50:00.060 --> 00:50:01.440
It's a duplication.

00:50:01.440 --> 00:50:03.740
And that we can refute.

00:50:03.740 --> 00:50:06.730
AUDIENCE: Could you prove to
me that you understand English?

00:50:06.730 --> 00:50:09.594
JOHN SEARLE: Yeah,
I wouldn't bother.

00:50:09.594 --> 00:50:11.760
(SPEAKING WITH BRITISH
ACCENT) When I was in Oxford,

00:50:11.760 --> 00:50:13.776
many people doubted that I did.

00:50:17.200 --> 00:50:20.290
I happened to be in a rather
snobbish college called

00:50:20.290 --> 00:50:22.360
Christ Church.

00:50:22.360 --> 00:50:24.440
And, of course, I
don't speak English.

00:50:24.440 --> 00:50:25.800
I never pretended to.

00:50:25.800 --> 00:50:29.740
I speak a dialect of American,
which makes many English people

00:50:29.740 --> 00:50:31.270
shudder at the thought.

00:50:36.980 --> 00:50:39.980
AUDIENCE: So you've said
you understand English,

00:50:39.980 --> 00:50:42.820
but how do I know you're
not just a computer program?

00:50:42.820 --> 00:50:45.950
JOHN SEARLE: Well, it's
the same question as Ray's.

00:50:45.950 --> 00:50:48.950
And the answer is
all sorts of ways.

00:50:48.950 --> 00:50:53.520
You know, if it got to a
crunch, you might ask me.

00:50:53.520 --> 00:50:55.490
Now I might give a
dishonest answer.

00:50:55.490 --> 00:50:56.970
Or I might give
an honest answer.

00:50:56.970 --> 00:50:59.390
But there's one route
that you don't want to go.

00:50:59.390 --> 00:51:01.620
And that's the epistemic route.

00:51:01.620 --> 00:51:04.760
The epistemic route
says, well, you

00:51:04.760 --> 00:51:07.880
have as much evidence that
the computer is conscious

00:51:07.880 --> 00:51:10.050
as that we have that
you are conscious.

00:51:10.050 --> 00:51:11.050
No, not really.

00:51:11.050 --> 00:51:13.340
I mean, I could go
into some detail

00:51:13.340 --> 00:51:18.560
about what it is about people's
physical structure that

00:51:18.560 --> 00:51:20.620
make them capable of
producing consciousness.

00:51:20.620 --> 00:51:22.710
You don't have to
have a fancy theory.

00:51:22.710 --> 00:51:25.260
I don't need a fancy
theory of neurobiology

00:51:25.260 --> 00:51:26.552
to say those are your eyes.

00:51:26.552 --> 00:51:27.760
You spoke through your mouth.

00:51:30.500 --> 00:51:34.370
The question was an expression
of a conscious intention

00:51:34.370 --> 00:51:35.670
to ask a question.

00:51:35.670 --> 00:51:40.660
Believe me, if you are a
locally produced machine,

00:51:40.660 --> 00:51:44.610
Google is further
along than I thought.

00:51:44.610 --> 00:51:45.820
But clearly, you're not.

00:51:49.700 --> 00:51:54.190
JOHN BRACAGLIA: We're going to
take a question from the Dory.

00:51:54.190 --> 00:51:55.900
JOHN SEARLE: Is he next?

00:51:55.900 --> 00:51:57.400
JOHN BRACAGLIA: We
had some people--

00:51:57.400 --> 00:51:58.369
AUDIENCE: Almost.

00:51:58.369 --> 00:51:59.785
JOHN BRACAGLIA:
We had some people

00:51:59.785 --> 00:52:01.760
submit questions ahead of time.

00:52:01.760 --> 00:52:02.682
JOHN SEARLE: OK.

00:52:02.682 --> 00:52:06.174
JOHN BRACAGLIA: So we're
going to read those as well.

00:52:06.174 --> 00:52:06.840
JOHN SEARLE: OK.

00:52:06.840 --> 00:52:07.400
All right.

00:52:07.400 --> 00:52:08.210
Right.

00:52:08.210 --> 00:52:10.400
JOHN BRACAGLIA: So the
first question from the Dory

00:52:10.400 --> 00:52:12.860
is, what is the definition of
consciousness you've been using

00:52:12.860 --> 00:52:14.040
for the duration of this talk?

00:52:14.040 --> 00:52:14.386
JOHN SEARLE: OK.

00:52:14.386 --> 00:52:14.732
Here goes.

00:52:14.732 --> 00:52:15.910
JOHN BRACAGLIA: Please be
as specific as possible.

00:52:15.910 --> 00:52:18.250
JOHN SEARLE: It is typically
said that consciousness

00:52:18.250 --> 00:52:19.750
is hard to define.

00:52:19.750 --> 00:52:22.350
I think it's rather
easy to define.

00:52:22.350 --> 00:52:24.240
We don't have a
scientific definition

00:52:24.240 --> 00:52:26.460
because we don't have
a scientific theory.

00:52:26.460 --> 00:52:29.190
The commonsense
definition of any term

00:52:29.190 --> 00:52:32.210
will identify the target
of the investigation.

00:52:32.210 --> 00:52:35.020
Water is a clear,
colorless, tasteless liquid.

00:52:35.020 --> 00:52:36.900
And it comes in
bottles like this.

00:52:36.900 --> 00:52:38.610
That's the commonsense
definition.

00:52:38.610 --> 00:52:41.344
You do science and
you discover it's H2O.

00:52:41.344 --> 00:52:42.760
Well, with
consciousness, we're in

00:52:42.760 --> 00:52:46.250
the clear, colorless,
liquid, tasteless sense.

00:52:46.250 --> 00:52:47.520
But here it is.

00:52:47.520 --> 00:52:50.070
Consciousness consists
of all those states

00:52:50.070 --> 00:52:53.460
of feeling or
sentience or awareness

00:52:53.460 --> 00:52:55.700
that begin in the
morning when you awake

00:52:55.700 --> 00:52:57.570
from a dreamless sleep.

00:52:57.570 --> 00:53:00.280
And they go on all day
long until you fall asleep

00:53:00.280 --> 00:53:04.315
again or otherwise become, as
they would say, unconscious.

00:53:04.315 --> 00:53:08.350
On this definition, dreams
are a form of consciousness.

00:53:08.350 --> 00:53:11.050
The secret, the essence,
of consciousness

00:53:11.050 --> 00:53:13.180
is that for any
conscious state, there's

00:53:13.180 --> 00:53:17.000
something it feels like to
be in that conscious state.

00:53:17.000 --> 00:53:18.750
Now, for that reason,
consciousness always

00:53:18.750 --> 00:53:21.450
has a subjective ontology.

00:53:21.450 --> 00:53:25.740
Remember, I gave you that
subjective-objective bit.

00:53:25.740 --> 00:53:27.840
It always has a
subjective ontology.

00:53:27.840 --> 00:53:30.174
That's the working
definition of consciousness.

00:53:30.174 --> 00:53:31.590
And that's the one
that's actually

00:53:31.590 --> 00:53:35.500
used by neurobiological
investigators trying to figure

00:53:35.500 --> 00:53:36.735
out how the brain does it.

00:53:36.735 --> 00:53:38.401
That's what you're
trying to figure out.

00:53:38.401 --> 00:53:39.830
How does the brain produce that?

00:53:39.830 --> 00:53:41.440
How does it exist in the brain?

00:53:41.440 --> 00:53:43.460
How does it function?

00:53:43.460 --> 00:53:45.960
AUDIENCE: I'd like
to propose a stronger

00:53:45.960 --> 00:53:48.980
bound on your observation
that we do not

00:53:48.980 --> 00:53:51.860
know how to build a
thinking machine today.

00:53:51.860 --> 00:53:54.130
Even if we knew how to
build it, because, I mean,

00:53:54.130 --> 00:53:57.685
our thinking machine was built
by the process of evolution,

00:53:57.685 --> 00:53:59.310
I'd like to propose--
well, what do you

00:53:59.310 --> 00:54:02.510
think about stating
that, actually, we

00:54:02.510 --> 00:54:04.612
may not have the time?

00:54:04.612 --> 00:54:06.620
And that it actually
may not matter.

00:54:06.620 --> 00:54:10.210
The reason we may not have the
time is the probabilities that

00:54:10.210 --> 00:54:12.710
need to happen, like the
asteroid falling and wiping

00:54:12.710 --> 00:54:15.840
the dinosaurs and whatnot,
may not happen in the universe

00:54:15.840 --> 00:54:16.920
that we live been.

00:54:16.920 --> 00:54:20.690
But if you subscribe to the
parallel universes theory,

00:54:20.690 --> 00:54:23.380
then there is some artificial
consciousness somewhere else.

00:54:23.380 --> 00:54:24.130
JOHN SEARLE: Yeah.

00:54:24.130 --> 00:54:30.160
OK, about we may not have the
time, well, I'm in a hurry.

00:54:30.160 --> 00:54:32.500
But I think we ought to
try as hard as we can.

00:54:32.500 --> 00:54:33.000
It's true.

00:54:33.000 --> 00:54:35.120
Maybe some things are
beyond our capacity

00:54:35.120 --> 00:54:38.130
to solve in the life of
human beings on Earth.

00:54:38.130 --> 00:54:40.940
But let's get busy and try.

00:54:40.940 --> 00:54:43.190
There was a period when
people said, well, we'll never

00:54:43.190 --> 00:54:45.000
really understand life.

00:54:45.000 --> 00:54:47.100
And while we don't
fully understand it,

00:54:47.100 --> 00:54:48.846
but we're pretty far along.

00:54:48.846 --> 00:54:50.720
I mean, the old debate
between the mechanists

00:54:50.720 --> 00:54:53.400
and the vitalists, that doesn't
make any sense to us anymore.

00:54:53.400 --> 00:54:55.530
So we made a lot of progress.

00:54:55.530 --> 00:54:57.450
There was another
half to your question.

00:54:57.450 --> 00:54:59.065
AUDIENCE: It may not matter
because all universe--

00:54:59.065 --> 00:54:59.390
JOHN SEARLE: Oh, yeah.

00:54:59.390 --> 00:55:00.930
Maybe conscious doesn't matter.

00:55:00.930 --> 00:55:02.085
Well, it's where I live.

00:55:02.085 --> 00:55:04.982
It matters to me.

00:55:04.982 --> 00:55:06.440
AUDIENCE:
Philosophically speaking.

00:55:06.440 --> 00:55:07.898
JOHN SEARLE: Yeah,
but the point is

00:55:07.898 --> 00:55:10.060
there are a lot of things
that may or may not

00:55:10.060 --> 00:55:12.410
matter which are desperately
important to us--

00:55:12.410 --> 00:55:15.750
democracy and sex and
literature and good food

00:55:15.750 --> 00:55:17.020
and all that kind of stuff.

00:55:17.020 --> 00:55:18.519
Maybe it doesn't
matter to somebody,

00:55:18.519 --> 00:55:22.162
but all those things matter
to me in varying degrees.

00:55:22.162 --> 00:55:24.620
AUDIENCE: Your artificial heart
analogy that you mentioned.

00:55:24.620 --> 00:55:26.880
I think you included the
idea that it's possible,

00:55:26.880 --> 00:55:28.380
just like with the
artificial heart,

00:55:28.380 --> 00:55:30.770
that we use different materials
and different approaches

00:55:30.770 --> 00:55:34.080
to simulate a heart
and, in some ways,

00:55:34.080 --> 00:55:37.920
go beyond just-- come
closer to duplication,

00:55:37.920 --> 00:55:40.180
that we might, in theory,
be able to do the same thing

00:55:40.180 --> 00:55:41.784
with an artificial brain.

00:55:41.784 --> 00:55:43.450
I'm wondering if you
think it's possible

00:55:43.450 --> 00:55:46.170
that going down the
path just trying

00:55:46.170 --> 00:55:50.460
to do a simulation of a
brain accidentally creates

00:55:50.460 --> 00:55:52.970
a consciousness or accidentally
creates duplication,

00:55:52.970 --> 00:55:55.260
even if we don't intend to
do it with exact same means

00:55:55.260 --> 00:55:57.241
as a brain is made.

00:55:57.241 --> 00:55:59.240
JOHN SEARLE: I would say
to believe in that, you

00:55:59.240 --> 00:56:00.960
have to believe in miracles.

00:56:00.960 --> 00:56:03.000
You have to-- now
think about it.

00:56:03.000 --> 00:56:07.180
We can do computer simulations
of just about anything

00:56:07.180 --> 00:56:08.980
you can describe precisely.

00:56:08.980 --> 00:56:12.470
You do a computer
simulation of digestion.

00:56:12.470 --> 00:56:14.960
And you could get
a computer model

00:56:14.960 --> 00:56:17.810
that does a perfect
model of digesting pizza.

00:56:17.810 --> 00:56:20.650
For all I know, maybe somebody
in this building has done it.

00:56:20.650 --> 00:56:23.730
But once you've done that, you
don't rush out and buy a pizza

00:56:23.730 --> 00:56:25.510
and stuff it in the
computer because it

00:56:25.510 --> 00:56:26.760
isn't going to digest a pizza.

00:56:26.760 --> 00:56:29.330
What it gives you is
a picture or a model

00:56:29.330 --> 00:56:31.580
or a mathematical diagram.

00:56:31.580 --> 00:56:33.490
And I have no objection to that.

00:56:33.490 --> 00:56:36.750
But if my life depended
on figuring out

00:56:36.750 --> 00:56:38.940
how the brain produces
consciousness,

00:56:38.940 --> 00:56:40.663
I would use the
computer the way you

00:56:40.663 --> 00:56:42.770
use a computer in any
branch of biology.

00:56:42.770 --> 00:56:45.080
It's very useful
for figuring out

00:56:45.080 --> 00:56:47.800
the implications of your
axioms, for figuring out

00:56:47.800 --> 00:56:50.410
the possible experiments
that you could design.

00:56:50.410 --> 00:56:52.920
But somehow or
other that the idea

00:56:52.920 --> 00:56:56.560
that the computer simulation
of cognitive behavior

00:56:56.560 --> 00:56:59.100
might provide the key
to the biochemistry,

00:56:59.100 --> 00:57:01.110
well, it's not out
of the question,

00:57:01.110 --> 00:57:02.110
it's just not plausible.

00:57:02.110 --> 00:57:04.401
JOHN BRACAGLIA: Humans are
easily fooled and frequently

00:57:04.401 --> 00:57:06.630
overestimate the
intelligence of machines.

00:57:06.630 --> 00:57:09.280
Can you propose a better
test of general intelligence

00:57:09.280 --> 00:57:12.630
than the Turing test, one
that is less likely to relate

00:57:12.630 --> 00:57:14.372
false positives?

00:57:14.372 --> 00:57:16.080
JOHN SEARLE: Well,
you all know my answer

00:57:16.080 --> 00:57:18.860
to that is the first
step is to distinguish

00:57:18.860 --> 00:57:21.700
between genuine intrinsic
observer-independent

00:57:21.700 --> 00:57:26.010
intelligence and
observer-relative intelligence.

00:57:26.010 --> 00:57:27.910
And observer-relative
intelligence

00:57:27.910 --> 00:57:30.310
is always in the
eye of the beholder.

00:57:30.310 --> 00:57:32.890
And anything will
have the intelligence

00:57:32.890 --> 00:57:34.900
that you're able
to attribute to it.

00:57:34.900 --> 00:57:38.980
I just attributed a great deal
of intelligence to this object

00:57:38.980 --> 00:57:40.510
because it can
compute a function,

00:57:40.510 --> 00:57:42.630
s equals one-half squared.

00:57:42.630 --> 00:57:45.780
Now this object has
prodigious intelligence

00:57:45.780 --> 00:57:48.110
because it discriminates
one hair from-- I

00:57:48.110 --> 00:57:50.500
won't demonstrate
it, but in any-- take

00:57:50.500 --> 00:57:53.830
my word for it that it
does, even in a head that's

00:57:53.830 --> 00:57:54.880
sparse with hair.

00:57:54.880 --> 00:57:57.720
So because intelligence
is observer-relative,

00:57:57.720 --> 00:58:00.540
you have to tell me the
criteria by which we're

00:58:00.540 --> 00:58:01.490
going to judge it.

00:58:01.490 --> 00:58:03.710
And the problem with
the Turing test--

00:58:03.710 --> 00:58:06.780
well, it's got all
sorts of problems,

00:58:06.780 --> 00:58:12.130
but the basic problem is that
both the input and the output

00:58:12.130 --> 00:58:15.440
are what they are only
relative to our interpretation.

00:58:15.440 --> 00:58:17.820
You have to interpret
this as a question.

00:58:17.820 --> 00:58:20.020
And you have to interpret
that as an answer.

00:58:20.020 --> 00:58:23.110
One bottom line of
my whole discussion

00:58:23.110 --> 00:58:24.910
today is that the
Turing test fails.

00:58:24.910 --> 00:58:27.060
It doesn't give you a
test of intelligence.

00:58:27.060 --> 00:58:30.290
AUDIENCE: So you seem to take
it as an article of faith

00:58:30.290 --> 00:58:33.450
that we are conscious,
that your dog is conscious,

00:58:33.450 --> 00:58:35.300
and that that
consciousness comes

00:58:35.300 --> 00:58:39.650
from biological material, the
likes of which we can't really

00:58:39.650 --> 00:58:40.850
understand.

00:58:40.850 --> 00:58:42.350
But forgive me for
saying this, that

00:58:42.350 --> 00:58:45.580
makes you sound like an
intelligent design theorist who

00:58:45.580 --> 00:58:50.270
says that because
evolution and everything

00:58:50.270 --> 00:58:53.420
in this creative
universe that exists

00:58:53.420 --> 00:58:55.860
is so complex, that
it couldn't have

00:58:55.860 --> 00:58:58.080
evolved from inert material.

00:58:58.080 --> 00:59:00.580
So somewhere between
an amoeba and your dog,

00:59:00.580 --> 00:59:02.510
there must not be consciousness.

00:59:02.510 --> 00:59:05.060
And I'm not sure where
you would draw that line.

00:59:05.060 --> 00:59:08.060
And so if consciousness
in human beings

00:59:08.060 --> 00:59:10.970
is emergent, or even in
your dog at some point

00:59:10.970 --> 00:59:13.390
in the evolutionary
scale, why couldn't it

00:59:13.390 --> 00:59:16.240
emerge from a
computation system that's

00:59:16.240 --> 00:59:19.450
sufficiently distributed,
networked, and has the ability

00:59:19.450 --> 00:59:23.010
to perform many calculations
and maybe is even hooked

00:59:23.010 --> 00:59:25.460
into biologic systems?

00:59:25.460 --> 00:59:28.860
JOHN SEARLE: Well, about could
it emerge, miracles are always

00:59:28.860 --> 00:59:30.430
possible.

00:59:30.430 --> 00:59:32.840
How do you know
that you don't have

00:59:32.840 --> 00:59:35.050
chemical processes
that will turn this

00:59:35.050 --> 00:59:37.500
into a conscious comb?

00:59:37.500 --> 00:59:38.610
How do I know that?

00:59:38.610 --> 00:59:40.660
Well, it's not a
serious possibility.

00:59:40.660 --> 00:59:43.080
I mean, the mechanisms
by which consciousness

00:59:43.080 --> 00:59:46.260
is created in the brain
are quite specific.

00:59:46.260 --> 00:59:48.410
And remember, this
is the key point.

00:59:48.410 --> 00:59:51.330
Any system that
creates consciousness

00:59:51.330 --> 00:59:53.700
has to duplicate
those causal powers.

00:59:53.700 --> 00:59:57.390
That's like saying, you don't
have to have feathers in order

00:59:57.390 --> 01:00:00.270
to have a flying machine,
but you have to duplicate

01:00:00.270 --> 01:00:04.070
and not merely simulate the
causal power of the bird

01:00:04.070 --> 01:00:07.010
to overcome the force of gravity
in the Earth's atmosphere.

01:00:07.010 --> 01:00:08.340
And that's what airplanes do.

01:00:08.340 --> 01:00:10.455
They duplicate causal powers.

01:00:10.455 --> 01:00:12.580
They use the same principle,
Bernoulli's principle,

01:00:12.580 --> 01:00:15.300
to overcome the
force of gravity.

01:00:15.300 --> 01:00:18.250
But the idea that somehow
or other you might do it

01:00:18.250 --> 01:00:22.070
just by doing a simulation
of certain formal structures

01:00:22.070 --> 01:00:26.460
of input-output mechanisms,
of input-output functions,

01:00:26.460 --> 01:00:28.576
well, miracles are
always possible.

01:00:28.576 --> 01:00:29.700
But it doesn't seem likely.

01:00:29.700 --> 01:00:31.434
That's not the way
evolution works.

01:00:31.434 --> 01:00:33.350
AUDIENCE: But machines
can improve themselves.

01:00:33.350 --> 01:00:35.960
And you're making the case
for why an amoeba could never

01:00:35.960 --> 01:00:38.734
develop into your dog over
a sufficiently long period

01:00:38.734 --> 01:00:40.025
of time and have consciousness.

01:00:40.025 --> 01:00:40.710
JOHN SEARLE: No, I
didn't make that case.

01:00:40.710 --> 01:00:41.740
No, I didn't make that case.

01:00:41.740 --> 01:00:42.573
[INTERPOSING VOICES]

01:00:44.550 --> 01:00:46.000
JOHN SEARLE: Amoeba
don't have it.

01:00:46.000 --> 01:00:48.140
AUDIENCE: You're refuting
that consciousness

01:00:48.140 --> 01:00:53.690
could emerge from a sufficiently
complex computation system.

01:00:53.690 --> 01:00:57.130
JOHN SEARLE: Complexity is
always observer-relative.

01:00:57.130 --> 01:00:58.830
If you talk about
complexity, you

01:00:58.830 --> 01:01:02.010
have to talk about the metric.

01:01:02.010 --> 01:01:04.800
What is the metric by which
you calculate complexity?

01:01:04.800 --> 01:01:06.555
I think complexity is
probably irrelevant.

01:01:06.555 --> 01:01:10.340
It might turn out that
the mechanism is simple.

01:01:10.340 --> 01:01:12.540
There's nothing
in my account that

01:01:12.540 --> 01:01:16.440
says a computer could
never become conscious.

01:01:16.440 --> 01:01:19.120
Of course, we're all conscious
computers, as I said.

01:01:19.120 --> 01:01:21.200
And the point about
the amoeba is not

01:01:21.200 --> 01:01:25.400
that amoebas can't evolve into
much more complex organisms.

01:01:25.400 --> 01:01:27.000
Maybe that's what happened.

01:01:27.000 --> 01:01:30.930
But the amoeba as it stands--
a single-celled organism--

01:01:30.930 --> 01:01:33.800
that doesn't have enough
machinery to duplicate

01:01:33.800 --> 01:01:35.540
the causal powers of the brain.

01:01:35.540 --> 01:01:41.410
I am not doing a science
fiction project to say, well,

01:01:41.410 --> 01:01:43.630
there can never be an
artificially created

01:01:43.630 --> 01:01:47.670
consciousness by people busy
designing computer programs.

01:01:47.670 --> 01:01:50.240
Of course, I'm not saying
that's logically impossible.

01:01:50.240 --> 01:01:53.170
I'm just saying it's not
an intelligent project.

01:01:53.170 --> 01:01:55.050
If you're thinking
about your life depends

01:01:55.050 --> 01:01:57.520
on building a machine that
creates consciousness,

01:01:57.520 --> 01:02:00.790
you don't sit down your console
and start programming things

01:02:00.790 --> 01:02:02.820
in some programming language.

01:02:02.820 --> 01:02:05.790
It's the wrong way
to go about it.

01:02:05.790 --> 01:02:09.420
AUDIENCE: If we gave you a
disassembly of Google Translate

01:02:09.420 --> 01:02:12.370
and had you implement the
Chinese room experiment,

01:02:12.370 --> 01:02:14.960
either it would
take you thousands

01:02:14.960 --> 01:02:19.240
of years to run all the assembly
instructions on pen and paper,

01:02:19.240 --> 01:02:22.490
or else you'd end up
decompiling it into English

01:02:22.490 --> 01:02:26.300
and heavily optimizing
it in that form.

01:02:26.300 --> 01:02:28.499
And in the process,
you'd come to learn a lot

01:02:28.499 --> 01:02:30.790
about the relationships
between the different variables

01:02:30.790 --> 01:02:31.840
and subroutines.

01:02:31.840 --> 01:02:35.580
So who's to say that an
understanding of Chinese

01:02:35.580 --> 01:02:37.200
wouldn't emerge from that?

01:02:37.200 --> 01:02:40.305
JOHN SEARLE: Well, OK, I
love this kind of question.

01:02:40.305 --> 01:02:40.840
All right.

01:02:40.840 --> 01:02:43.480
Now, let me say, of course,
when I did the original thought

01:02:43.480 --> 01:02:47.000
experiment, anybody will point
out to you if you actually

01:02:47.000 --> 01:02:49.640
were carrying out the
steps in a program

01:02:49.640 --> 01:02:51.770
for answering
questions in Chinese,

01:02:51.770 --> 01:02:54.380
well, we'd be around for
several million years.

01:02:54.380 --> 01:02:56.614
OK, I take their word for it.

01:02:56.614 --> 01:02:58.030
I'm not a programmer,
but I assume

01:02:58.030 --> 01:03:00.770
it would take an
enormous amount of time.

01:03:00.770 --> 01:03:08.780
But the point of the
argument is not the example.

01:03:08.780 --> 01:03:11.180
The example is
designed to illustrate

01:03:11.180 --> 01:03:12.450
the point of the argument.

01:03:12.450 --> 01:03:14.290
The point of the
argument can be given

01:03:14.290 --> 01:03:16.420
in the following derivation.

01:03:16.420 --> 01:03:19.780
Programs are formal
or syntactical.

01:03:19.780 --> 01:03:22.640
That's axiom number one.

01:03:22.640 --> 01:03:24.830
That's all there
is to the program.

01:03:24.830 --> 01:03:26.670
To put it slightly
more pretentiously,

01:03:26.670 --> 01:03:29.540
the notion same
implemented program

01:03:29.540 --> 01:03:32.520
defines an equivalence
class specified entirely

01:03:32.520 --> 01:03:34.640
formally or syntactically.

01:03:34.640 --> 01:03:37.970
But minds have a
semantics, and-- and this

01:03:37.970 --> 01:03:41.280
is the whole point of the
example-- the syntax by itself

01:03:41.280 --> 01:03:43.060
is not sufficient
for the semantics.

01:03:43.060 --> 01:03:44.780
That's the point of the example.

01:03:44.780 --> 01:03:49.960
The Chinese room is designed
to illustrate axiom three, that

01:03:49.960 --> 01:03:54.570
just having the steps in
the program is not by itself

01:03:54.570 --> 01:03:57.220
sufficient for a semantics.

01:03:57.220 --> 01:03:58.400
And minds have a semantics.

01:03:58.400 --> 01:04:02.930
Now, it follows from those
that if the computer is defined

01:04:02.930 --> 01:04:05.150
in terms of its
program operations,

01:04:05.150 --> 01:04:08.210
syntactical operations,
then the program operations,

01:04:08.210 --> 01:04:09.920
the computer operations
by themselves

01:04:09.920 --> 01:04:11.760
are never sufficient
for understanding

01:04:11.760 --> 01:04:13.890
because they lack a semantics.

01:04:13.890 --> 01:04:16.810
But, of course, I'm
not saying, well, you

01:04:16.810 --> 01:04:19.690
could not build a machine
that was both a computer

01:04:19.690 --> 01:04:20.720
and had semantics.

01:04:20.720 --> 01:04:21.667
We are such machines.

01:04:21.667 --> 01:04:23.500
AUDIENCE: You couldn't
verify experimentally

01:04:23.500 --> 01:04:25.710
what the difference might
be between semantics

01:04:25.710 --> 01:04:28.570
and what would
emerge from thousands

01:04:28.570 --> 01:04:33.224
of years of experience with
a given syntactical program.

01:04:33.224 --> 01:04:35.390
JOHN SEARLE: I think you
can-- I don't inherit this.

01:04:35.390 --> 01:04:36.840
He does.

01:04:36.840 --> 01:04:39.930
I think you don't want to
go the epistemic route.

01:04:39.930 --> 01:04:41.410
You don't want to
say, well, look

01:04:41.410 --> 01:04:45.460
you can't tell the difference
between the thinking machine

01:04:45.460 --> 01:04:48.020
and the non-thinking machine.

01:04:48.020 --> 01:04:50.130
The reason that's
the wrong route to go

01:04:50.130 --> 01:04:54.185
is we now have
overwhelming evidence

01:04:54.185 --> 01:04:57.310
of what sorts of
mechanisms produce what

01:04:57.310 --> 01:04:58.282
sorts of cognition.

01:04:58.282 --> 01:04:59.990
When I first got
interested in the brain,

01:04:59.990 --> 01:05:01.600
I went out and bought
all the textbooks.

01:05:01.600 --> 01:05:03.020
By the way, if you want
to learn a subject,

01:05:03.020 --> 01:05:04.120
that's the way to do it.

01:05:04.120 --> 01:05:05.690
Go buy all the
freshman textbooks

01:05:05.690 --> 01:05:08.060
because they're
easy to understand.

01:05:08.060 --> 01:05:13.520
One of these textbooks, it
said cats have different color

01:05:13.520 --> 01:05:15.150
vision from ours.

01:05:15.150 --> 01:05:18.090
Their visual experiences
are different from ours.

01:05:18.090 --> 01:05:21.170
And I thought, christ,
have these guys been cats?

01:05:21.170 --> 01:05:23.870
Have the other
cats mind problem?

01:05:23.870 --> 01:05:26.840
Do they know what
it's like to be a cat?

01:05:26.840 --> 01:05:29.439
And the answer is, of
course, they know completely

01:05:29.439 --> 01:05:31.480
what's the cat's color
vision is because they can

01:05:31.480 --> 01:05:32.730
look at the color receptors.

01:05:32.730 --> 01:05:35.380
And cats do have different
color vision from ours

01:05:35.380 --> 01:05:37.301
because they have
different color receptors.

01:05:37.301 --> 01:05:38.300
I forget the difference.

01:05:38.300 --> 01:05:40.210
You can look them
up in any textbook.

01:05:40.210 --> 01:05:42.110
But if in real life
we're completely

01:05:42.110 --> 01:05:47.120
confident that my dog can hear
parts of the auditory spectrum

01:05:47.120 --> 01:05:47.950
that I can't hear.

01:05:47.950 --> 01:05:51.150
He can hear the higher
frequencies that I can't hear.

01:05:51.150 --> 01:05:54.360
And cats have a different
color vision from mine

01:05:54.360 --> 01:05:57.420
because we can see
what the apparatus is.

01:05:57.420 --> 01:05:58.440
We got another question?

01:05:58.440 --> 01:05:58.940
You're on.

01:05:58.940 --> 01:06:00.360
JOHN BRACAGLIA: This will
be our final question.

01:06:00.360 --> 01:06:01.470
JOHN SEARLE: OK.

01:06:01.470 --> 01:06:02.845
I'm prepared to
go all afternoon.

01:06:02.845 --> 01:06:04.550
I love this kind of crap.

01:06:04.550 --> 01:06:06.440
AUDIENCE: So at the
beginning of your talk,

01:06:06.440 --> 01:06:08.850
you mentioned an anecdote
about neuroscientists

01:06:08.850 --> 01:06:10.936
not being interested
in consciousness.

01:06:10.936 --> 01:06:13.310
And, of course, by this time,
a number of neuroscientists

01:06:13.310 --> 01:06:13.976
have studied it.

01:06:13.976 --> 01:06:16.190
And so they'll
present stimuli that

01:06:16.190 --> 01:06:18.090
are near the threshold
of perceptibility

01:06:18.090 --> 01:06:21.562
and measure the brain responses
when it's above or below.

01:06:21.562 --> 01:06:22.770
What do you think about that?

01:06:22.770 --> 01:06:23.640
Is that on the right track?

01:06:23.640 --> 01:06:24.890
What would you do differently?

01:06:24.890 --> 01:06:27.320
JOHN SEARLE: No, I think one
of the best things that's

01:06:27.320 --> 01:06:29.370
happened in my lifetime--
it's getting a rather

01:06:29.370 --> 01:06:31.655
long lifetime-- is
that there is now

01:06:31.655 --> 01:06:36.920
a thriving industry of
neuroscientific investigations

01:06:36.920 --> 01:06:37.820
of consciousness.

01:06:37.820 --> 01:06:40.750
That's how we will
get the answer.

01:06:40.750 --> 01:06:42.250
When I first got
interested in this,

01:06:42.250 --> 01:06:46.484
I told you I went over to UCSF
and told those guys get busy.

01:06:46.484 --> 01:06:47.900
The last thing
they wanted to hear

01:06:47.900 --> 01:06:50.890
was being nagged by some
philosopher, I can tell you.

01:06:50.890 --> 01:06:54.290
But one guy said to me--
famous neuroscientist

01:06:54.290 --> 01:06:58.290
said-- in my discipline,
it's OK to be

01:06:58.290 --> 01:07:02.607
interested in consciousness,
but get tenure first.

01:07:02.607 --> 01:07:05.230
Get tenure first.

01:07:05.230 --> 01:07:07.370
Now, there has been a change.

01:07:07.370 --> 01:07:09.000
I don't take credit
for the change,

01:07:09.000 --> 01:07:10.540
but I've certainly
been urging it.

01:07:10.540 --> 01:07:13.665
You can now get tenure by
working on consciousness.

01:07:13.665 --> 01:07:16.350
Now, neuroscience has
changed, that now there's

01:07:16.350 --> 01:07:18.652
a thriving industry
in neuroscience

01:07:18.652 --> 01:07:20.610
of people who are actually
trying to figure out

01:07:20.610 --> 01:07:22.400
how the brain does it.

01:07:22.400 --> 01:07:25.920
And when they figure that out--
and I don't see any obstacle

01:07:25.920 --> 01:07:27.860
to figuring that
out-- it will be

01:07:27.860 --> 01:07:29.810
an enormous intellectual
breakthrough,

01:07:29.810 --> 01:07:31.830
when we figure out how
exactly does the brain

01:07:31.830 --> 01:07:33.075
create consciousness.

01:07:33.075 --> 01:07:34.450
AUDIENCE: But in
particular, that

01:07:34.450 --> 01:07:37.392
approach they're using now-- I
use the example of presenting

01:07:37.392 --> 01:07:39.600
stimuli that are near the
threshold of perceptibility

01:07:39.600 --> 01:07:41.662
and looking for
neural correlates,

01:07:41.662 --> 01:07:43.370
do you think that's
going to be fruitful?

01:07:43.370 --> 01:07:46.701
What particular questions
would you ask to find out?

01:07:46.701 --> 01:07:48.950
JOHN SEARLE: I happened to
be interested in this crap.

01:07:48.950 --> 01:07:50.620
And if you're
interested in my views,

01:07:50.620 --> 01:07:54.540
I published an article in the
"Annual Review of Neuroscience"

01:07:54.540 --> 01:07:56.030
with a title "Consciousness."

01:07:56.030 --> 01:07:57.260
It's easy to remember.

01:07:57.260 --> 01:07:58.850
You can find it on the web.

01:07:58.850 --> 01:08:01.650
And what I said is,
there are two main lines

01:08:01.650 --> 01:08:03.810
of research going on today.

01:08:03.810 --> 01:08:06.120
There are guys who take what
I call the building block

01:08:06.120 --> 01:08:06.740
approach.

01:08:06.740 --> 01:08:09.560
And they try to find
the neuronal correlate

01:08:09.560 --> 01:08:10.950
of particular experiences.

01:08:10.950 --> 01:08:12.370
You see a red object.

01:08:12.370 --> 01:08:14.350
Or you hear the
sound of middle C.

01:08:14.350 --> 01:08:15.960
What's the correlate
in the brain?

01:08:15.960 --> 01:08:18.560
And the idea they have
is if you can figure out

01:08:18.560 --> 01:08:21.210
how the brain creates
the experience of red,

01:08:21.210 --> 01:08:23.100
you've cracked
the whole problem.

01:08:23.100 --> 01:08:24.649
Because it's like DNA.

01:08:24.649 --> 01:08:27.399
You don't have to figure
out how every phenotype is

01:08:27.399 --> 01:08:28.370
caused by DNA.

01:08:28.370 --> 01:08:31.420
If you get the general
principles, that's enough.

01:08:31.420 --> 01:08:34.939
Now, the problem is they're
not making much progress

01:08:34.939 --> 01:08:36.950
on this what I call the
building block approach.

01:08:36.950 --> 01:08:38.970
It seems to me a much
more fruitful approach

01:08:38.970 --> 01:08:42.660
is likely to be think of
consciousness as coming

01:08:42.660 --> 01:08:44.370
in a unified field.

01:08:44.370 --> 01:08:47.279
Think of perception not
as creating consciousness,

01:08:47.279 --> 01:08:49.630
but as modifying
the conscious field.

01:08:49.630 --> 01:08:52.080
So when I see the red
in this guy's shirt,

01:08:52.080 --> 01:08:54.180
it modifies my conscience field.

01:08:54.180 --> 01:08:57.300
I now have an experience of
red I didn't have before.

01:08:57.300 --> 01:09:00.250
Most people-- and the
history of science

01:09:00.250 --> 01:09:03.279
supports them-- use the
building block approach

01:09:03.279 --> 01:09:05.979
because most of the
history of science

01:09:05.979 --> 01:09:08.729
has proceeded atomistically.

01:09:08.729 --> 01:09:10.399
You figure out how
little things work,

01:09:10.399 --> 01:09:12.100
and then you go to big things.

01:09:12.100 --> 01:09:15.069
They're not making much
progress with consciousness.

01:09:15.069 --> 01:09:17.830
And I think the reason
is you need to figure out

01:09:17.830 --> 01:09:20.649
how the brain creates
the conscious field

01:09:20.649 --> 01:09:23.670
in the first place because
particular experiences,

01:09:23.670 --> 01:09:26.430
like the perception of red
or the sound of middle C,

01:09:26.430 --> 01:09:29.319
those modify that
conscious field.

01:09:29.319 --> 01:09:31.750
They don't create a
conscious field from nothing.

01:09:31.750 --> 01:09:34.359
They modify an existing
conscious field.

01:09:34.359 --> 01:09:37.279
Now, it's much harder
to do that because you

01:09:37.279 --> 01:09:40.319
have to figure out how
large chunks of the brain

01:09:40.319 --> 01:09:41.740
create consciousness.

01:09:41.740 --> 01:09:42.750
And we don't know that.

01:09:42.750 --> 01:09:47.810
The problem is in an MRI, that
conscious brain looks a lot

01:09:47.810 --> 01:09:50.080
like the unconscious brain.

01:09:50.080 --> 01:09:52.050
And there must be some
differences there.

01:09:52.050 --> 01:09:54.644
But at this point-- and I
haven't been working on it.

01:09:54.644 --> 01:09:56.060
I've been working
on other things.

01:09:56.060 --> 01:09:59.710
But I want somebody to
tell me exactly what's

01:09:59.710 --> 01:10:02.110
the difference between
the conscious brain

01:10:02.110 --> 01:10:04.920
and the unconscious brain that
accounts for consciousness.

01:10:04.920 --> 01:10:06.030
We're not there yet.

01:10:06.030 --> 01:10:10.150
However, what I'm doing here
is neurobiological speculation.

01:10:10.150 --> 01:10:12.420
I mean, I'm going
to be answered not

01:10:12.420 --> 01:10:14.640
by a philosophical
argument, but by somebody

01:10:14.640 --> 01:10:18.060
who does the hard research of
figuring out exactly what are

01:10:18.060 --> 01:10:21.210
the mechanisms in the brain
the produce consciousness

01:10:21.210 --> 01:10:23.065
and exactly how do they work.

01:10:23.065 --> 01:10:25.440
JOHN BRACAGLIA: John, it's
been an immense, immense honor

01:10:25.440 --> 01:10:26.524
to be here with you today.

01:10:26.524 --> 01:10:27.856
Thank you so much for your time.

01:10:27.856 --> 01:10:29.410
And thank you for
talking to Google.

01:10:29.410 --> 01:10:31.201
JOHN SEARLE: Well,
thank you for having me.

01:10:31.201 --> 01:10:32.160
[APPLAUSE]

