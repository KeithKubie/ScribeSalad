WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:03.630
Ah, the joys of the
modern digital age.

00:00:03.630 --> 00:00:05.760
Now as far as ages
go, we've clearly

00:00:05.760 --> 00:00:09.050
entered into the Cambrian
explosion of digital content.

00:00:09.050 --> 00:00:11.400
You see, we have
plethoras of data

00:00:11.400 --> 00:00:14.530
out there on the internet--
books, music, videos,

00:00:14.530 --> 00:00:17.670
even your favorite TV shows
are all available at the touch

00:00:17.670 --> 00:00:19.770
of a button or a
finger to any device

00:00:19.770 --> 00:00:22.030
you want, anywhere in the world.

00:00:22.030 --> 00:00:23.600
Now think about
this for a minute.

00:00:23.600 --> 00:00:26.690
I mean, we're talking about
yottabytes upon exabytes

00:00:26.690 --> 00:00:28.790
upon googles of information.

00:00:28.790 --> 00:00:30.380
All flowing through
the internet,

00:00:30.380 --> 00:00:33.120
everywhere in the world,
every second of the day.

00:00:33.120 --> 00:00:35.340
Now since the late '70s,
information theorists

00:00:35.340 --> 00:00:38.410
and computer scientists have
been utilizing compression

00:00:38.410 --> 00:00:41.650
algorithms to effectively
reduce the size of this content,

00:00:41.650 --> 00:00:44.250
to keep the internet from
coming to a standstill.

00:00:44.250 --> 00:00:46.540
And this is why today we
start hearing more and more

00:00:46.540 --> 00:00:50.900
about the connection between
conversion rates and data sizes

00:00:50.900 --> 00:00:52.400
for applications and websites.

00:00:52.400 --> 00:00:55.051
So for example, a user may
browse away from a website

00:00:55.051 --> 00:00:56.550
if it hasn't loaded
in three seconds

00:00:56.550 --> 00:00:59.430
or may not download an
application because it

00:00:59.430 --> 00:01:02.530
can incur extra charges
by their carrier.

00:01:02.530 --> 00:01:05.500
Now savvy developers need to
take control of this fact.

00:01:05.500 --> 00:01:07.910
And that means bucking
the trend of just using

00:01:07.910 --> 00:01:09.950
standard compression
algorithms out there.

00:01:09.950 --> 00:01:12.120
Instead, you need to
start digging in deep,

00:01:12.120 --> 00:01:14.690
understanding your data,
and utilizing compression

00:01:14.690 --> 00:01:17.850
in new and interesting ways
to make your users happy.

00:01:17.850 --> 00:01:20.100
But fear not, young developer.

00:01:20.100 --> 00:01:21.810
I'm here to help
with that process.

00:01:21.810 --> 00:01:24.667
My name is Colt McAnlis,
and this is Compressor Head.

00:01:30.750 --> 00:01:36.680
In 1836, an American inventor
named Samuel F. Morse

00:01:36.680 --> 00:01:39.880
concocted a way to
send electrical pulses

00:01:39.880 --> 00:01:41.190
through a wire.

00:01:41.190 --> 00:01:43.290
Now, at the end of
that wire was actually

00:01:43.290 --> 00:01:45.980
attached an electromagnet.

00:01:45.980 --> 00:01:47.740
Himself along with
other physicists,

00:01:47.740 --> 00:01:49.860
realized that if they
could separate out

00:01:49.860 --> 00:01:52.020
the pulses of
electricity on the wire

00:01:52.020 --> 00:01:53.870
to control the
electric magnet, they

00:01:53.870 --> 00:01:56.550
could ring a bell every
time a pulse came through.

00:01:56.550 --> 00:02:00.280
And thus was born the
electrical telegraph system.

00:02:00.280 --> 00:02:02.570
You see, the entire
intent of this operation

00:02:02.570 --> 00:02:04.870
was to find a way to
send natural language

00:02:04.870 --> 00:02:08.890
code over great distances,
using a pretty robust medium.

00:02:08.890 --> 00:02:10.639
Now of course, they
needed to figure a way

00:02:10.639 --> 00:02:12.720
to devise a situation
where they could

00:02:12.720 --> 00:02:15.245
take an English
character-- a, b, or c--

00:02:15.245 --> 00:02:17.120
and actually turn it
into electrical pulses--

00:02:17.120 --> 00:02:19.250
dot dash dot dot dash.

00:02:19.250 --> 00:02:20.960
And thus was born Morse Code.

00:02:25.860 --> 00:02:27.940
The way they were able
to assign this code

00:02:27.940 --> 00:02:30.340
was actually based on
the inverse frequency

00:02:30.340 --> 00:02:32.450
of a symbol occurring
in the English language.

00:02:32.450 --> 00:02:35.990
So for example, e, the most
common occurring letter

00:02:35.990 --> 00:02:39.276
in the dictionary, actually
was assigned the smallest code.

00:02:39.276 --> 00:02:41.150
The reason for this was
you have to remember,

00:02:41.150 --> 00:02:42.816
they have some guy
sitting there banging

00:02:42.816 --> 00:02:45.546
on the electrical magnet,
sending pulses through a wire.

00:02:45.546 --> 00:02:47.670
And they wanted to minimize
the amount of work that

00:02:47.670 --> 00:02:50.550
needed to be done to
communicate a message.

00:02:50.550 --> 00:02:52.600
And of course, all of
the other symbols on here

00:02:52.600 --> 00:02:54.340
are based upon
their probability.

00:02:54.340 --> 00:02:57.300
Getting more complex, the less
probable they are to occur.

00:03:02.910 --> 00:03:04.540
Now in modern
computing, we actually

00:03:04.540 --> 00:03:06.020
use a different sort of form.

00:03:06.020 --> 00:03:07.960
And that's actually
called binary code.

00:03:07.960 --> 00:03:11.144
Now binary code is not a
variable length symbol.

00:03:11.144 --> 00:03:12.560
Instead what we
say is that you're

00:03:12.560 --> 00:03:14.620
going to use a set
number of bits,

00:03:14.620 --> 00:03:17.360
and then we're able to
assign a symbol to a given

00:03:17.360 --> 00:03:20.180
number of bits using
that static amount.

00:03:20.180 --> 00:03:22.950
So for example, ASCII
assigns eight bits

00:03:22.950 --> 00:03:23.980
to a single character.

00:03:23.980 --> 00:03:26.460
So when we get a stream
of these bits on the wire,

00:03:26.460 --> 00:03:28.580
we know we can read in
eight bits at a time

00:03:28.580 --> 00:03:31.000
and determine what symbol
we're actually using.

00:03:31.000 --> 00:03:34.410
Now however, if we decided
to move the other direction

00:03:34.410 --> 00:03:37.110
and implement a frequency
change, much like Morse

00:03:37.110 --> 00:03:39.390
did-- sorry.

00:03:39.390 --> 00:03:42.730
You see that we can actually
get to something a little bit

00:03:42.730 --> 00:03:43.340
better.

00:03:43.340 --> 00:03:45.600
Now what happens here is
if we take into account

00:03:45.600 --> 00:03:48.360
the probability or frequency
occurrence of the character

00:03:48.360 --> 00:03:51.300
and use that to assign
a variable length bit

00:03:51.300 --> 00:03:53.570
pattern to it, we
can actually start

00:03:53.570 --> 00:03:54.910
getting some interesting things.

00:03:54.910 --> 00:03:57.920
So for example, c occurring
in a given symbol stream

00:03:57.920 --> 00:04:01.200
80% of the time, gets assigned
the shortest character,

00:04:01.200 --> 00:04:02.610
a single zero.

00:04:02.610 --> 00:04:05.330
Now what this means is
instead of assigning

00:04:05.330 --> 00:04:07.820
eight bits per character
for our entire stream,

00:04:07.820 --> 00:04:10.630
we can actually assign a
variable length of characters

00:04:10.630 --> 00:04:14.180
and actually end up in this
example getting 81% savings.

00:04:14.180 --> 00:04:17.301
This is the definition
of variable length codes.

00:04:17.301 --> 00:04:19.550
And it's what we're going
to talk about more of today.

00:04:22.650 --> 00:04:25.050
Now it would seem that you
can't really have a discussion

00:04:25.050 --> 00:04:28.730
about compression without
actually going back and talking

00:04:28.730 --> 00:04:31.040
about information theory itself.

00:04:31.040 --> 00:04:33.179
And when you're talking
about information theory,

00:04:33.179 --> 00:04:34.970
you can't talk about
it without introducing

00:04:34.970 --> 00:04:37.060
this guy, Cloud Shannon.

00:04:37.060 --> 00:04:40.680
Now Cloud, who besides being
awesome at parties-- really

00:04:40.680 --> 00:04:42.420
good with keg
stands-- pretty much

00:04:42.420 --> 00:04:45.380
invented what we call
modern information theory.

00:04:45.380 --> 00:04:47.225
Now what Cloud did
effectively was

00:04:47.225 --> 00:04:50.220
he found that there was a
correlation between information

00:04:50.220 --> 00:04:53.380
content and the probability
of that symbol occurring

00:04:53.380 --> 00:04:54.310
in the stream.

00:04:54.310 --> 00:04:56.880
Now he was able to
actually measure

00:04:56.880 --> 00:04:59.990
this correlation with
the logarithm function

00:04:59.990 --> 00:05:02.166
and actually get a result
in the number bits.

00:05:02.166 --> 00:05:03.540
So effectively,
he could find out

00:05:03.540 --> 00:05:08.070
what the information content
was in terms of binary data.

00:05:08.070 --> 00:05:10.490
Now this itself was
a pretty bold move

00:05:10.490 --> 00:05:12.690
that actually defined how
information theory works.

00:05:12.690 --> 00:05:14.980
But for our needs, what
we're more interested

00:05:14.980 --> 00:05:17.090
with is his
definition of entropy.

00:05:17.090 --> 00:05:19.520
See, when you take all of
the probability content

00:05:19.520 --> 00:05:22.490
and information content of a
stream and sum it together,

00:05:22.490 --> 00:05:25.130
you end up with the
algorithm for entropy.

00:05:25.130 --> 00:05:27.270
Now besides being
a little gnarly

00:05:27.270 --> 00:05:30.850
here, what we really find out
is that entropy is actually

00:05:30.850 --> 00:05:34.680
a matching of an estimate for
the minimum number of bits

00:05:34.680 --> 00:05:38.250
that each symbol has
to have on average

00:05:38.250 --> 00:05:40.300
to represent the stream.

00:05:40.300 --> 00:05:42.180
Don't worry, we'll take
a look at an example

00:05:42.180 --> 00:05:43.430
of that real quick.

00:05:43.430 --> 00:05:45.294
So let's take a look
at this in practice.

00:05:45.294 --> 00:05:46.710
So let's say we
have a stream that

00:05:46.710 --> 00:05:49.220
only has two symbols in
it, whose probabilities are

00:05:49.220 --> 00:05:51.670
represented by P1 and P2.

00:05:51.670 --> 00:05:55.050
So you can see here that while
the probability is 0.5 and 0.5

00:05:55.050 --> 00:05:57.940
between these two symbols,
the entropy value actually

00:05:57.940 --> 00:05:58.880
gives us 1.

00:05:58.880 --> 00:06:01.800
Basically saying that
we need 1 bit per symbol

00:06:01.800 --> 00:06:04.050
to represent all symbols
in this stream, right?

00:06:04.050 --> 00:06:06.060
So two symbols, we have 0 or 1.

00:06:06.060 --> 00:06:06.950
That's pretty good.

00:06:06.950 --> 00:06:08.890
Now as we start skewing
the probability,

00:06:08.890 --> 00:06:10.870
effectively one
symbol starts becoming

00:06:10.870 --> 00:06:13.154
more dominant in the stream
than the other symbol.

00:06:13.154 --> 00:06:14.570
So we start seeing
the probability

00:06:14.570 --> 00:06:18.620
of P1 start rising
and P2 start falling.

00:06:18.620 --> 00:06:22.940
We can see that the entropy
value ends up at about 0.88.

00:06:22.940 --> 00:06:27.490
Roughly meaning that on average,
we need 0.88 bits per symbol

00:06:27.490 --> 00:06:29.530
to represent all the
symbols in the stream.

00:06:29.530 --> 00:06:30.550
And this is good, right?

00:06:30.550 --> 00:06:33.190
This is where we come back
to variable length codes.

00:06:33.190 --> 00:06:35.120
Some codes are bigger,
some codes are shorter.

00:06:35.120 --> 00:06:36.536
To further this
point, you can see

00:06:36.536 --> 00:06:40.760
that we're really skewed
down here in 0.99 and 0.01.

00:06:40.760 --> 00:06:42.525
The entropy is as
close to 0 as it

00:06:42.525 --> 00:06:43.900
can get, meaning
that we're going

00:06:43.900 --> 00:06:47.660
to see a lot of bias for
P0 and a large code for P1.

00:06:47.660 --> 00:06:49.250
Let's look at this a bit more.

00:06:49.250 --> 00:06:52.560
So let's say we have something
very simplistic here.

00:06:52.560 --> 00:06:54.140
We've got four symbols, right?

00:06:54.140 --> 00:06:57.330
And each one has an equal
probability of 0.25.

00:06:57.330 --> 00:07:01.400
Now in this environment,
the entropy is actually 2.

00:07:01.400 --> 00:07:04.220
So we have to have
minimum 2 bits per symbol

00:07:04.220 --> 00:07:06.470
to represent all the
symbols in this stream.

00:07:06.470 --> 00:07:08.800
Which actually boils down
to something that looks like

00:07:08.800 --> 00:07:12.780
this-- 00, 01, 10, and 11.

00:07:12.780 --> 00:07:15.070
This is pretty much the
basis of how binary works.

00:07:15.070 --> 00:07:16.820
If you know the number
of bits per symbol,

00:07:16.820 --> 00:07:18.030
you can read it pretty easily.

00:07:18.030 --> 00:07:19.655
Now to look at entropy
a little deeper.

00:07:19.655 --> 00:07:23.440
If we actually start changing
the probability of the symbols

00:07:23.440 --> 00:07:25.460
in the stream, we get
some different results.

00:07:25.460 --> 00:07:27.050
So let's say we
have a set-up here,

00:07:27.050 --> 00:07:29.060
where we've got four
symbols, and each one

00:07:29.060 --> 00:07:33.770
has a different probability,
0.49, 0.25, 0.25, and 0.01.

00:07:33.770 --> 00:07:36.580
When you actually run this
through the entropy equation,

00:07:36.580 --> 00:07:40.220
you end up with about
0.157 bits per symbol.

00:07:40.220 --> 00:07:42.440
Again, this is on
average, right.

00:07:42.440 --> 00:07:46.290
Now this means that we can
start assigning variable length

00:07:46.290 --> 00:07:49.530
codes to the stream to
start achieving compression.

00:07:49.530 --> 00:07:51.800
Because we really can
only get compression once

00:07:51.800 --> 00:07:54.010
we have differing probabilities.

00:07:54.010 --> 00:07:56.570
So in this case, A, being
the most dominant symbol,

00:07:56.570 --> 00:08:00.050
actually gets the smallest
code word of 1 bit per symbol.

00:08:00.050 --> 00:08:02.390
And over time, we're going
to end up with compression.

00:08:02.390 --> 00:08:04.560
Now remember that
entropy actually

00:08:04.560 --> 00:08:08.490
represents the best estimate
of the minimum number of bits

00:08:08.490 --> 00:08:11.000
required to
represent the stream.

00:08:11.000 --> 00:08:13.410
And of course, that also
brings us into the next point.

00:08:13.410 --> 00:08:15.990
If you're going to be
using variable length codes

00:08:15.990 --> 00:08:18.154
and get compression
out of them, you

00:08:18.154 --> 00:08:20.070
need to make sure that
you assign the shortest

00:08:20.070 --> 00:08:23.740
codes to the most frequent
symbols in your stream.

00:08:23.740 --> 00:08:25.430
If you don't follow
this principle,

00:08:25.430 --> 00:08:26.190
you're not going
to get compression,

00:08:26.190 --> 00:08:28.189
and you're really not
going to have a good time.

00:08:31.530 --> 00:08:33.880
So now let's take a
look at how you actually

00:08:33.880 --> 00:08:36.580
use these variable length
codes in your software

00:08:36.580 --> 00:08:39.260
to start encoding and
decoding your data stream.

00:08:39.260 --> 00:08:42.730
So let's say we start with
our symbol stream A, B, C.

00:08:42.730 --> 00:08:44.580
And of course, we also
have a symbol table.

00:08:44.580 --> 00:08:48.510
Now a symbol table is going to
map your given symbols-- A, B,

00:08:48.510 --> 00:08:52.660
and C-- to your given variable
length code words, VLCs.

00:08:52.660 --> 00:08:56.770
So A goes to 0, B goes to 10,
and C of course goes to 11.

00:08:56.770 --> 00:08:59.890
Now encoding is actually
extremely straightforward.

00:08:59.890 --> 00:09:02.140
All we have to do is read
a symbol off the stream,

00:09:02.140 --> 00:09:04.596
figure out what code word
it goes to, and then output

00:09:04.596 --> 00:09:05.970
that code word to
the bit stream.

00:09:05.970 --> 00:09:07.160
So let's see what
that looks like.

00:09:07.160 --> 00:09:08.410
We take an A off the stream.

00:09:08.410 --> 00:09:11.430
And of course we find that
A just goes to a single 0.

00:09:11.430 --> 00:09:15.360
So we can simply output a
single 0 to the bit stream.

00:09:15.360 --> 00:09:18.130
Now of course we take B
off the symbol stream.

00:09:18.130 --> 00:09:19.810
Look through our
symbol table and find

00:09:19.810 --> 00:09:23.370
that B corresponds to 1 and 0,
which is pretty easy for us.

00:09:23.370 --> 00:09:27.160
We again put up a 1 and
then put up a 0 as well.

00:09:27.160 --> 00:09:30.110
And of course C, the final
one here, goes to 1 1,

00:09:30.110 --> 00:09:33.470
which again is
pretty easy to do.

00:09:33.470 --> 00:09:36.630
Now the encoding process here
is actually not that difficult,

00:09:36.630 --> 00:09:37.310
as I said.

00:09:37.310 --> 00:09:41.050
We start with symbols, we moved
to code words, and it's done.

00:09:41.050 --> 00:09:43.992
Now decoding is a little
bit trickier though.

00:09:43.992 --> 00:09:45.450
Because in decoding,
we effectively

00:09:45.450 --> 00:09:47.200
have to start with
our bit stream,

00:09:47.200 --> 00:09:49.070
and then read
through and determine

00:09:49.070 --> 00:09:51.020
what symbols we're
working towards.

00:09:51.020 --> 00:09:52.610
Now here's the tricky part.

00:09:52.610 --> 00:09:55.680
Because our code words are
actually variable length,

00:09:55.680 --> 00:09:57.730
we don't know how
many bits to read.

00:09:57.730 --> 00:10:01.560
Let's take a look
at what that says.

00:10:01.560 --> 00:10:05.290
So we start with our
input stream, 01, 01, 1.

00:10:05.290 --> 00:10:08.600
Now we read a single
bit off of the stream.

00:10:08.600 --> 00:10:10.360
We check this against
our symbol table.

00:10:10.360 --> 00:10:13.390
And we realize that the first
symbol is actually an A, right?

00:10:13.390 --> 00:10:16.050
A is a single 0, we got a
single 0 off the stream,

00:10:16.050 --> 00:10:19.530
so we can actually put
A in the output stream.

00:10:19.530 --> 00:10:21.980
Now we read the next
bit off the stream.

00:10:21.980 --> 00:10:22.730
And it's a 1.

00:10:22.730 --> 00:10:24.720
Here is where things get tricky.

00:10:24.720 --> 00:10:27.800
We don't know if this is going
to be a B or C, because you

00:10:27.800 --> 00:10:31.460
could see that both of their
code words start with a 1.

00:10:31.460 --> 00:10:33.780
This means we actually have
to read another bit off

00:10:33.780 --> 00:10:35.500
of the stream before
we can actually

00:10:35.500 --> 00:10:38.380
make a decision about
which code this is.

00:10:38.380 --> 00:10:40.780
So we read a 1 and a 0.

00:10:40.780 --> 00:10:43.660
And we can see that it actually
corresponds to the letter B.

00:10:43.660 --> 00:10:47.187
That means we read those off,
we output a B to the stream.

00:10:47.187 --> 00:10:48.770
Now we do this again
for the next one.

00:10:48.770 --> 00:10:50.779
We read one bit
again, because we

00:10:50.779 --> 00:10:53.070
have to always start with
the least common denominator.

00:10:53.070 --> 00:10:54.736
And we realize that
that could either be

00:10:54.736 --> 00:10:57.610
B or C. We read in a secondary
bit and use that pair.

00:10:57.610 --> 00:11:00.650
And we realize that we've
just read the letter C off

00:11:00.650 --> 00:11:01.880
of the stream.

00:11:01.880 --> 00:11:04.150
Now this is all
fine and dandy when

00:11:04.150 --> 00:11:05.960
things are working correctly.

00:11:05.960 --> 00:11:08.370
But the reality is
that it's really hard

00:11:08.370 --> 00:11:12.330
to design and create the
proper variable length codes.

00:11:12.330 --> 00:11:14.450
In fact, a lot can
go wrong in how

00:11:14.450 --> 00:11:18.340
you're choosing what code words
are assigned to what symbols.

00:11:18.340 --> 00:11:21.170
And this is where things can
actually get really wrong.

00:11:21.170 --> 00:11:23.440
So let's say we end up here.

00:11:23.440 --> 00:11:26.670
We end up with the same input
bit stream that we saw before.

00:11:26.670 --> 00:11:29.040
But notice that the code
word for the symbol C

00:11:29.040 --> 00:11:30.060
has actually changed.

00:11:30.060 --> 00:11:32.300
It's now 1, 0, 1
instead of 1, 1.

00:11:32.300 --> 00:11:34.600
So let's start decoding
and see what happens.

00:11:34.600 --> 00:11:36.690
We read the 0 off
of the bit stream,

00:11:36.690 --> 00:11:38.500
and we notice that
that's an A. Fantastic.

00:11:38.500 --> 00:11:41.380
An A goes out to the stream,
and all unicorns are happy.

00:11:41.380 --> 00:11:44.070
However when get to the
next one, we go-- read a 1,

00:11:44.070 --> 00:11:46.846
and we realize that that could
be either B or C. Fantastic.

00:11:46.846 --> 00:11:48.470
Still things that
we're doing normally.

00:11:48.470 --> 00:11:50.640
However, when we
read the next 0,

00:11:50.640 --> 00:11:54.514
we're still not clear if this
is a B or C. We can't actually

00:11:54.514 --> 00:11:55.180
make a decision.

00:11:55.180 --> 00:11:57.770
It looks like a 1,
0 should go to a B.

00:11:57.770 --> 00:12:00.190
But this might actually
be a C. Because if we read

00:12:00.190 --> 00:12:02.815
the third bit, we could
actually make that decision.

00:12:02.815 --> 00:12:04.940
So what we're looking at
here is because of the way

00:12:04.940 --> 00:12:09.220
we've encoded our VLCs, we
don't know if this 1 0 1 0 bit

00:12:09.220 --> 00:12:11.900
pattern is actually
two Bs or a C

00:12:11.900 --> 00:12:15.020
and an A. This
actually gives rise

00:12:15.020 --> 00:12:17.320
to something you need
to follow when designing

00:12:17.320 --> 00:12:18.570
your variable length codes.

00:12:18.570 --> 00:12:21.340
And that is known as
the prefix property.

00:12:21.340 --> 00:12:23.330
Or more specifically,
once a code

00:12:23.330 --> 00:12:26.010
has been assigned
to a given symbol,

00:12:26.010 --> 00:12:30.060
no other code can start
with that bit pattern.

00:12:30.060 --> 00:12:32.570
So if we have 1 0
with B, we should not

00:12:32.570 --> 00:12:35.400
be able to start the
letter C with 1 0.

00:12:35.400 --> 00:12:37.460
That's going to make us
have a really bad time.

00:12:41.185 --> 00:12:42.560
Of course, at this
point you have

00:12:42.560 --> 00:12:44.830
to ask yourself where do
variable length codes come

00:12:44.830 --> 00:12:45.330
from?

00:12:45.330 --> 00:12:46.996
I mean they don't
just drop from the sky

00:12:46.996 --> 00:12:49.450
into your computer for
your encoding purposes.

00:12:49.450 --> 00:12:51.170
For this, we actually
have to introduce

00:12:51.170 --> 00:12:52.780
someone named Peter Elias.

00:12:52.780 --> 00:12:57.610
Now besides being a renowned
tap dance fusion instructor,

00:12:57.610 --> 00:12:59.380
he also invented beta codes.

00:12:59.380 --> 00:13:04.400
Or what we call today
modern binary codes.

00:13:04.400 --> 00:13:06.660
Now as we talked about,
binary codes actually

00:13:06.660 --> 00:13:08.795
aren't variable length
codes, because they

00:13:08.795 --> 00:13:10.370
don't obey the prefix property.

00:13:10.370 --> 00:13:12.800
But Dr. Elias didn't stop there.

00:13:12.800 --> 00:13:17.040
You see, he actually constructed
an entire fleet of compression

00:13:17.040 --> 00:13:19.570
codes that he used
for various patterns.

00:13:19.570 --> 00:13:22.180
Now, in the early days
of creating these codes,

00:13:22.180 --> 00:13:24.890
you could actually take an
integer value, such as 0

00:13:24.890 --> 00:13:27.730
through 5, and actually
use that to denote

00:13:27.730 --> 00:13:30.170
what type of generation on
the code you're going to have.

00:13:30.170 --> 00:13:32.300
So for instance, this is
known as the unary code.

00:13:32.300 --> 00:13:34.870
Probably the most simplistic
variable length code

00:13:34.870 --> 00:13:37.440
that obeys the prefix
property that we have.

00:13:37.440 --> 00:13:40.670
The way that unary codes are
constructed is given a specific

00:13:40.670 --> 00:13:45.130
index, you actually append
that many 1's-- minus 1--

00:13:45.130 --> 00:13:46.500
to the front of it before a 0.

00:13:46.500 --> 00:13:47.920
So for example
down here at 5, you

00:13:47.920 --> 00:13:50.510
can see we've appended
four 1's and a 0.

00:13:50.510 --> 00:13:52.150
4 gives us three 1's and a 0.

00:13:52.150 --> 00:13:53.497
3 gives us two 1's and so on.

00:13:53.497 --> 00:13:55.580
Of course, because of the
way this is constructed,

00:13:55.580 --> 00:13:58.900
0 actually doesn't have a
value representation here.

00:13:58.900 --> 00:14:00.410
Now what's important
about the way

00:14:00.410 --> 00:14:03.324
that Dr. Elias actually
constructed his codes,

00:14:03.324 --> 00:14:05.740
was that they all were built
around a specific probability

00:14:05.740 --> 00:14:06.550
distribution.

00:14:06.550 --> 00:14:08.930
So unary codes are
actually optimal when

00:14:08.930 --> 00:14:11.820
all of your symbols follow
probability set forth by P

00:14:11.820 --> 00:14:14.760
to the n is 2 the negative n.

00:14:14.760 --> 00:14:16.300
Now if your symbols
don't actually

00:14:16.300 --> 00:14:17.890
follow this probability,
you're going

00:14:17.890 --> 00:14:20.160
to get a little bit
of bloat in your codes

00:14:20.160 --> 00:14:21.146
later on in the stream.

00:14:21.146 --> 00:14:23.020
And you're actually not
going to hit entropy.

00:14:23.020 --> 00:14:25.530
But unary codes aren't
the only thing he created.

00:14:25.530 --> 00:14:26.500
Let's see here.

00:14:26.500 --> 00:14:30.070
We've also got gamma
codes, which of course you

00:14:30.070 --> 00:14:33.030
see have a completely
different construction process.

00:14:33.030 --> 00:14:35.380
That's mainly because they
have a different probability

00:14:35.380 --> 00:14:37.720
distribution down
here in the bottom.

00:14:37.720 --> 00:14:41.990
As well as delta codes,
or a variant of that.

00:14:41.990 --> 00:14:44.790
And themselves are actually
constructed quite differently,

00:14:44.790 --> 00:14:47.550
and have this monster of a
probability in the bottom.

00:14:47.550 --> 00:14:50.720
And of course we have
the fabled omega code,

00:14:50.720 --> 00:14:53.840
which is known to many far and
wide for having one of the more

00:14:53.840 --> 00:14:56.110
complex probability
distributions

00:14:56.110 --> 00:14:57.660
that you can't actually list it.

00:14:57.660 --> 00:15:00.620
It's kind of like saying
Voldemort out loud.

00:15:00.620 --> 00:15:01.122
Anyhow.

00:15:01.122 --> 00:15:02.496
Back in the day,
the way that you

00:15:02.496 --> 00:15:04.871
would choose what variable
length code to actually encode

00:15:04.871 --> 00:15:08.600
with is you would actually look
at the probability of your set

00:15:08.600 --> 00:15:11.001
and sort of determine how
many symbols you would have,

00:15:11.001 --> 00:15:13.000
against which code is
actually going to give you

00:15:13.000 --> 00:15:15.250
the optimal encoding
for that set.

00:15:15.250 --> 00:15:18.440
So for example, if you have
somewhere between 64 and 88

00:15:18.440 --> 00:15:20.439
unique symbols that
needed code word,

00:15:20.439 --> 00:15:22.730
you could actually find that
if the probability matched

00:15:22.730 --> 00:15:24.500
properly, you could
use omega codes

00:15:24.500 --> 00:15:26.355
and get the best bit
pattern you needed.

00:15:26.355 --> 00:15:27.980
Of course, if you
were lower than that,

00:15:27.980 --> 00:15:29.710
you may need to choose gammas.

00:15:29.710 --> 00:15:32.220
Or somewhere in the middle,
you may need deltas.

00:15:32.220 --> 00:15:34.510
Of course this brings us
to an interesting question.

00:15:34.510 --> 00:15:36.910
If you have a set of
symbols that don't actually

00:15:36.910 --> 00:15:39.770
follow any of these known
probability distribution

00:15:39.770 --> 00:15:41.320
functions, what do you do?

00:15:41.320 --> 00:15:44.020
Well of course the issue
with that-- let's see here--

00:15:44.020 --> 00:15:47.750
is that there are hundreds
of variable length codes

00:15:47.750 --> 00:15:48.350
out there.

00:15:48.350 --> 00:15:50.680
You see, we've got Schalkwijk
codes, Phased-In codes,

00:15:50.680 --> 00:15:53.160
Punctured codes, Stout
codes, Fibonacci codes,

00:15:53.160 --> 00:15:55.080
and really there's a
lot to choose from.

00:15:55.080 --> 00:15:57.190
Or you could do something
a little bit different

00:15:57.190 --> 00:15:59.720
and actually construct
your VLCs based

00:15:59.720 --> 00:16:01.165
upon your probability directly.

00:16:01.165 --> 00:16:02.248
Let's take a look at that.

00:16:06.540 --> 00:16:11.850
In 1951, while David Huffman
was still a student at MIT,

00:16:11.850 --> 00:16:14.420
he was taking an electrical
engineering class

00:16:14.420 --> 00:16:17.134
where his professor
gave him an opportunity.

00:16:17.134 --> 00:16:18.550
He could either
write a term paper

00:16:18.550 --> 00:16:19.924
solving a very
difficult problem,

00:16:19.924 --> 00:16:21.770
or he could study for his final.

00:16:21.770 --> 00:16:24.947
Of course David, being a little
bit optimized at the time,

00:16:24.947 --> 00:16:26.780
decided that he would
rather solve a problem

00:16:26.780 --> 00:16:28.360
than study for his exams.

00:16:28.360 --> 00:16:31.900
Now the problem that the
professor gave him was

00:16:31.900 --> 00:16:34.540
try to find the
most efficient way

00:16:34.540 --> 00:16:37.810
to represent letters,
numbers and other symbols

00:16:37.810 --> 00:16:39.390
using binary code.

00:16:39.390 --> 00:16:42.730
Of course, David decided to
chomp into this full bore.

00:16:42.730 --> 00:16:46.450
And after months and months
of chewing on this problem

00:16:46.450 --> 00:16:47.770
was about to give up.

00:16:47.770 --> 00:16:49.810
Effectively, he had
given up and was

00:16:49.810 --> 00:16:52.200
to the point of
throwing away all

00:16:52.200 --> 00:16:54.490
of his notes, when it hit him.

00:16:54.490 --> 00:16:56.620
He would later say that
it was like lightning

00:16:56.620 --> 00:16:59.640
striking his brain as soon as
the papers hit the trash can.

00:16:59.640 --> 00:17:01.020
So what he developed
at that time

00:17:01.020 --> 00:17:04.599
would later be recognized by
Donald Knuth as one of the most

00:17:04.599 --> 00:17:08.760
fundamental methods of computer
science and data transmission

00:17:08.760 --> 00:17:10.420
that we'd ever discovered.

00:17:10.420 --> 00:17:12.720
See, what happened
was David Huffman

00:17:12.720 --> 00:17:16.670
found a way to represent and
create variable length codes

00:17:16.670 --> 00:17:21.109
for any given probability in
a very simplistic and compact

00:17:21.109 --> 00:17:22.670
binary tree form.

00:17:22.670 --> 00:17:24.760
Now later he went on,
of course, to figure out

00:17:24.760 --> 00:17:26.380
how to levitate
things with his mind.

00:17:26.380 --> 00:17:29.000
But until then, he
just used this code,

00:17:29.000 --> 00:17:31.064
which we are going to
take a look at now.

00:17:34.780 --> 00:17:36.990
Now the brilliance of
what Huffman discovered

00:17:36.990 --> 00:17:40.470
was actually a connection
between assigning the symbol

00:17:40.470 --> 00:17:42.060
to the code word
using the table.

00:17:42.060 --> 00:17:43.690
Now what he was
able to do here was

00:17:43.690 --> 00:17:46.270
find that if he
used a binary tree,

00:17:46.270 --> 00:17:49.430
he could effectively use the
probabilities from the symbol

00:17:49.430 --> 00:17:52.600
table alongside the
branches of the binary tree

00:17:52.600 --> 00:17:55.170
to create an
optimal binary code.

00:17:55.170 --> 00:17:57.745
Well let's build a Huffman tree.

00:17:57.745 --> 00:17:59.120
So typically what
you're given is

00:17:59.120 --> 00:18:00.915
a set of symbols with
their probabilities.

00:18:00.915 --> 00:18:02.290
And the first step
in the process

00:18:02.290 --> 00:18:04.590
is to actually sort them such
that the highest probability

00:18:04.590 --> 00:18:06.048
symbol's at the
front of the array,

00:18:06.048 --> 00:18:08.530
and the least probable symbol
is at the back of the array.

00:18:08.530 --> 00:18:10.280
From here, what we're
actually going to do

00:18:10.280 --> 00:18:13.950
is pop the two least probable
symbols off the array.

00:18:13.950 --> 00:18:17.010
And actually make them
childs of a new node

00:18:17.010 --> 00:18:20.550
that's pushed in there.

00:18:20.550 --> 00:18:22.540
We actually continue
this process all the way

00:18:22.540 --> 00:18:23.350
down the line.

00:18:23.350 --> 00:18:26.110
Comparing the two last
elements of the array

00:18:26.110 --> 00:18:29.650
together and making them
children of a new value,

00:18:29.650 --> 00:18:31.460
that's then pushed
onto the array.

00:18:31.460 --> 00:18:34.620
Now at this point we have a
fully created binary tree.

00:18:34.620 --> 00:18:37.530
And to move from this tree to
the generation of Huffman codes

00:18:37.530 --> 00:18:38.950
is pretty simplistic.

00:18:38.950 --> 00:18:42.170
All we have to do is
assign a 0 and a 1

00:18:42.170 --> 00:18:46.100
to the left and right
branches accordingly.

00:18:46.100 --> 00:18:49.950
To actually generate the
VLC for a given symbol,

00:18:49.950 --> 00:18:53.210
all we have to do is walk
from that node up to the root

00:18:53.210 --> 00:18:55.690
and record all the 0's
and 1's that we trace.

00:18:55.690 --> 00:18:58.820
So for example, A moving
from its note to the root

00:18:58.820 --> 00:19:00.320
gives us 0.

00:19:00.320 --> 00:19:05.170
B moving from the node to
the root gives us 0 and 1.

00:19:05.170 --> 00:19:08.840
And moving from C to
the root gives us 1 1.

00:19:08.840 --> 00:19:11.100
This effectively
is the generation

00:19:11.100 --> 00:19:13.870
of our variable length codes
using the Huffman generation

00:19:13.870 --> 00:19:15.530
algorithm.

00:19:15.530 --> 00:19:19.630
And now that is a Huffman tree.

00:19:27.940 --> 00:19:30.490
It's pretty easy to see that
a little bit of compression

00:19:30.490 --> 00:19:32.410
can actually get you a long way.

00:19:32.410 --> 00:19:35.610
And in something as simple
as statistical encoding,

00:19:35.610 --> 00:19:37.170
you can use variable
length codes

00:19:37.170 --> 00:19:38.880
to actually shrink
your data set.

00:19:38.880 --> 00:19:41.220
Of course, the next thing
you really want to talk about

00:19:41.220 --> 00:19:42.750
is what other
compression algorithms

00:19:42.750 --> 00:19:44.916
are out there that you can
apply to your data stream

00:19:44.916 --> 00:19:48.650
before applying these variable
length codes to actually get

00:19:48.650 --> 00:19:50.440
you below entropy itself.

00:19:50.440 --> 00:19:52.840
Of course, that's a
topic for different show.

00:19:52.840 --> 00:19:53.870
My name is Colt McAnlis.

00:19:53.870 --> 00:19:55.510
Thanks for watching.

00:19:55.510 --> 00:20:00.060
[MUSIC PLAYING]

