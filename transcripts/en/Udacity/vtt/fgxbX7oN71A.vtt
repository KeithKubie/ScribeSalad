WEBVTT
Kind: captions
Language: en

00:00:00.200 --> 00:00:01.839
Okay Charles, what have we learned? And I mean

00:00:01.839 --> 00:00:05.150
specifically in the context of this game theory two lesson.

00:00:05.150 --> 00:00:08.670
&gt;&gt; That's a that's a good question. We learned

00:00:08.670 --> 00:00:12.580
about Iterated Prisoners Dilemma. Which turns out to be cool,

00:00:12.580 --> 00:00:14.770
and it solves the problem, and we learned about

00:00:14.770 --> 00:00:21.200
how we can connect iterated prison's dilemma to reinforcement learning.

00:00:21.200 --> 00:00:22.590
&gt;&gt; What do you mean?

00:00:22.590 --> 00:00:23.220
&gt;&gt; Through the discount.

00:00:23.220 --> 00:00:25.230
&gt;&gt; Yeah, so I think of that as being the

00:00:25.230 --> 00:00:26.660
idea of repeated games.

00:00:26.660 --> 00:00:29.990
&gt;&gt; Right. Let's see, what else have we learned? So we learned about

00:00:29.990 --> 00:00:33.410
iterated prison's dilemma, which allowed us to get past this really scary thing

00:00:33.410 --> 00:00:35.370
with repeated games, connected it with

00:00:35.370 --> 00:00:38.290
reinforcement learning. The discounting. And then we

00:00:38.290 --> 00:00:40.880
learned other things like for example, I

00:00:40.880 --> 00:00:43.080
don't remember. What, what did we learn?

00:00:43.080 --> 00:00:45.920
&gt;&gt; Well so the, the connection between iterated prisoner's dilemma and

00:00:45.920 --> 00:00:51.320
repeated games was the idea that we can actually encourage cooperation.

00:00:51.320 --> 00:00:54.506
And in fact, there's a whole bunch of new Nash equilibria that appear

00:00:54.506 --> 00:00:57.640
when you work in repeated games. That was the concept of the Folk Theorem.

00:00:57.640 --> 00:01:00.180
&gt;&gt; Right. The Folk Theorem. So, the Folk Theorem

00:01:00.180 --> 00:01:02.740
is really cool. And this whole notion of repeated

00:01:02.740 --> 00:01:05.650
games really seems like a clever way of getting

00:01:05.650 --> 00:01:09.460
out of what appear to be limitations in game theory.

00:01:09.460 --> 00:01:12.740
&gt;&gt; Right. Yeah. And in particular by using things like threats.

00:01:12.740 --> 00:01:15.820
&gt;&gt; Right. But only plausible threats.

00:01:15.820 --> 00:01:16.330
&gt;&gt; Right, so that

00:01:16.330 --> 00:01:18.270
was the next thing we talked about. The idea

00:01:18.270 --> 00:01:20.980
that an equilibrium could be subgame perfect or not,

00:01:20.980 --> 00:01:23.145
and if it wasn't then the threats could be

00:01:23.145 --> 00:01:26.970
implausible. But in the subgame perfect setting, they're more plausible.

00:01:26.970 --> 00:01:30.360
&gt;&gt; Right let's see and then we learned about Min-max Q.

00:01:30.360 --> 00:01:31.510
&gt;&gt; Well there was one last thing we did

00:01:31.510 --> 00:01:34.950
on the repeated games, which was the Computational Folk theorem.

00:01:34.950 --> 00:01:37.130
&gt;&gt; Yes you're right. So basically what we learned

00:01:37.130 --> 00:01:40.610
is that Michael Littman does cool stuff in game theory.

00:01:40.610 --> 00:01:41.400
&gt;&gt; Or at least he does

00:01:41.400 --> 00:01:43.230
stuff that he's willing to talk about in a MOOC.

00:01:43.230 --> 00:01:46.100
&gt;&gt; Yes, so that's, that's there's actually a

00:01:46.100 --> 00:01:49.340
technical term for that right? MOOC acceptable research?

00:01:49.340 --> 00:01:50.260
&gt;&gt; Oh, I didn't know that.

00:01:50.260 --> 00:01:50.565
&gt;&gt; Mm-hm.

00:01:50.565 --> 00:01:54.220
&gt;&gt; So all these things are by virtue of the

00:01:54.220 --> 00:01:56.690
fact that they showed up in this class look acceptable.

00:01:56.690 --> 00:01:57.260
&gt;&gt; Exactly.

00:01:57.260 --> 00:02:00.330
&gt;&gt; Alright, you're right, but then we switch to stochastic games.

00:02:00.330 --> 00:02:01.083
&gt;&gt; Mm-hm.

00:02:01.083 --> 00:02:04.480
&gt;&gt; And they generalize MDP's and repeated games.

00:02:05.990 --> 00:02:06.198
&gt;&gt; Mm-hm.

00:02:06.198 --> 00:02:07.130
&gt;&gt; Anything else?

00:02:07.130 --> 00:02:10.400
&gt;&gt; Well, that particularly got us to min-max

00:02:10.400 --> 00:02:12.940
Q and then eventually to Nash Q. But

00:02:12.940 --> 00:02:16.260
despite the fact that Nash Q doesn't work, we ended up in a place of hope.

00:02:16.260 --> 00:02:19.550
&gt;&gt; [LAUGH] We end with some hopefulness.

00:02:19.550 --> 00:02:20.830
&gt;&gt; Yeah, and you know, I think that

00:02:20.830 --> 00:02:23.205
that's actually a lesson for the entire course.

00:02:23.205 --> 00:02:25.110
That at the end of the day, sometimes

00:02:25.110 --> 00:02:27.005
it doesn't always work, but there is always hope.

00:02:27.005 --> 00:02:32.620
&gt;&gt; [LAUGH] We don't give up and that's, that's, that's how research works.

00:02:32.620 --> 00:02:35.908
Even when we have impossibility results for things like clustering, or

00:02:35.908 --> 00:02:38.350
multi-agent multi-agent learning and decision

00:02:38.350 --> 00:02:41.990
making, we still keep struggling forward.

00:02:41.990 --> 00:02:45.610
&gt;&gt; And keep learning, and isn't that what's really important. I think so.

00:02:45.610 --> 00:02:47.900
&gt;&gt; Its important for us, and its important for machines.

00:02:47.900 --> 00:02:50.990
&gt;&gt; Yes, that is beautiful. I feel like we've made

00:02:50.990 --> 00:02:52.835
it to a good place, Michael. Perhaps we should stop.

00:02:52.835 --> 00:02:56.056
&gt;&gt; [LAUGH] Well it has been, it has been

00:02:56.056 --> 00:02:57.750
delightful getting to talk to everyone, and it has been

00:02:57.750 --> 00:02:59.870
very fun getting to talk with you, Charles.

00:02:59.870 --> 00:03:03.080
And thanks to everybody for making this happen.

00:03:03.080 --> 00:03:06.980
&gt;&gt; I agree. And we have one more chance to talk with one another as we

00:03:06.980 --> 00:03:10.380
wrap up the class. And I look forward to that. So, I will see you then, Michael.

00:03:10.380 --> 00:03:12.380
&gt;&gt; Awesome. Do we get to see each other in person for that?

00:03:12.380 --> 00:03:14.890
&gt;&gt; We get to see each other in person for that. That will be fun.

00:03:14.890 --> 00:03:15.230
&gt;&gt; Yay!

00:03:15.230 --> 00:03:17.630
&gt;&gt; Okay, well, bye, Michael. I'll see you next time.

00:03:17.630 --> 00:03:19.290
&gt;&gt; Bye. See yeah.

00:03:19.290 --> 00:03:19.620
&gt;&gt; Bye, bye.

