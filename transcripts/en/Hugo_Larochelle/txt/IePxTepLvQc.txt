Speaker 1:          00:00          Yeah.

Speaker 2:          00:01          In this video we'll see an online variant of the dictionary learning spar scolding algorithm, uh, which is going to be able to scale to very large datasets.

Speaker 1:          00:11          Okay.

Speaker 2:          00:11          We've seen there the previous video, uh, that we could learn a dictionary Matrix d for some giving Dataset by alternating between inferring the sparse codes for all of our training data and then updating the dictionary by, uh, assuming that the part's codes are fixed and then finding the best dictionary for, uh, these sparse codes, uh, through, uh, the computation of this, uh, a these a and B matrices. And then running block corner, that corner that dissent, uh, to get a new improved dictionary matrix and then performing alternating between these two updates until the dictionary matrix has not changed. Now one problem with this algorithm is that if tea is very large capital t, so the number of training inputs in our data set is very large. We'll be spending most of our time computing amb and we'll be performing only a single a dictionary update, uh, after having visited all of the training data.

Speaker 1:          01:16          Yeah.

Speaker 2:          01:16          So these types of algorithms that perform a single, a new update of our parameters after a whole pass over the whole training set is known as a batch, as a batch algorithm. Uh, so it's batch as opposed to online where an online algorithm would be able to take a single new observation and update itself to adapt itself to the new observation. And one big advantage of an algorithm that's like this is that if I have a large Dataset, uh, it will be able to make many updates of its parameters, uh, before it, uh, before reaching the end of the Dataset. And it can also be applied in the context where we are constantly obtaining new data, uh, in an online fashion. So for instance, if it were downloading images from the web constantly, and, uh, so that's, uh, that's one big problem. I want to say that I say it's a single update after each fast.

Speaker 2:          02:16          Of course when we run corner the sense, uh, d cornered the scent Algorithm Inter, uh, inside, it does many updates of the dictionary. But, so when I see single update, I mean that, uh, after calling once the block corner, the call and until the, once it has converged and give us a given us a new d, uh, we get this new improved value of t, uh, after, uh, only once after having passed over the whole training sets, I have income computed, the A and B matrix. That's it. So that's what I mean by a single update here. And so, so this is not going to scale very well to very large datasets and a, so what we'd like is that we'd to get an argument, I can perform an update given a new observation just based on the new observation at that. It's, it's a dictionary a immediately.

Speaker 2:          03:08          And so, uh, to get this, we'll do the following if, ah, two together and online parent of our algorithm. So given the new eight x team and so, and we'll be iterating over each [inaudible] we can get from our dataset picking one, uh, sequentially, we'll first performed the inference of its parts codes for that given the current training example. And then, um, what we'll do is that will updates are a and B matrix, uh, using a running average. So what I mean by that is that we'll have an update of matrix B of the form. Well, I'll take B, uh, better times B, and then I'll add that one minus better the new outer product, either x with age or age with itself. Uh, so I'll incorporate that into my a running average. So here instead of t plus one, it should be small team. It's the same t as, as this t here.

Speaker 2:          04:12          And uh, so this means that EMV will be what is known as a running average. So it's an average that we keep updating every time we see a new example. And, uh, it's such that the latest, uh, example we've incorporated into the running average have a bigger weight than examples that I've been incorporated, uh, uh, preview much more previously in the, uh, while the algorithm was running. And so a Beta is a number between zero and one, and it's sort of, uh, uh, represents how much we want to, uh, uh, put importance into previously visited examples. Bet As close to one that it means that we'll put a lot of weight on the previous value Beta and less weight on the new value data. So unfortunately he will have a hyper parameter that we have to tune and it's a akin to the learning rates say to training, uh, Natto in quarter or uh, a an RBM with some stochastic gradient descent procedure.

Speaker 2:          05:11          So in that sense it's not such a big deal, but now we have to specify this better. And so we'll, uh, so to sum up then for the given like c and its latent sparse representation, we'll update our running average over AMD and then we'll use these matrices to update our current value of the dictionary matrix deep. And to update it. We'll run the block corner descent algorithm, but we'll warm started or will start from the previous value of t. And now since based on just a new example, XD, a single new example, we don't expect d to change too much then actually performing these block corn. The step shouldn't take too much time cause we already fairly close to a new solution, uh, to, to a solution that incorporates the information from XD. And, uh, so, uh, this, uh, part should converge very fairly equipment.

Speaker 2:          06:10          So let's put everything into a single algorithm. And so the advantage of this algorithm, uh, which is probably the algorithm you, you really should implement if you're on the apply it to any type of data, even if your training data is small. Uh, so this and my head is going to have the advantage of being able to scaling up to any size of training data. Uh, so, so, you know, I recommend implementing disparent instead as opposed to the previous one that alternates in that is batch. So the first step is to initialize deep. So we had to do this also before, I just didn't talk about it yet. Um, notice you should in initialize d two zero for one thing, it doesn't respect the, uh, unit norm constraint for the Adams. And also if these equal to zero, then it's impossible to reconstruct the, the, uh, uh, the input.

Speaker 2:          06:56          So you can get sort of in trouble. So can try to think about why the equals zero would be a problem. What I would recommend is that to initialize the can use a just random a matrix that you then normalize, uh, uh, each of the columns or you can take a, if you have a say 10 items, you decided that you wanted there late and representation of Sitel which is very smart and normally you use bigger, but say at 10, you could just take the first 10 example XD, uh, normalize the examples to you that norm and use each example as a initial value for the columns of tea. That's something that people sometimes do and tends to work well in practice. So initialized d in some way and then until Diaz converged. So we'll assume that uh, we're not in a, we're not using a stream of data when we have a data set and we uh, we are cycling over the example x team.

Speaker 2:          07:53          Then for each XD we'll enforce for a will infer it's parsed code. We'll update the running averages and then we'll perform the block corner, the scent algorithm using the value, the new value of Matrix and the, and so while the as not converge, we'll uh, go over the columns, the Adams in matrix a d and then we'll perform the updates were familiar with that. We've seen in the description of the block corner, the set. Okay. So now we have an algorithm that can learn for some given data, set a good dictionary matrix that will allow us to extract some sparse representations that can maintain some useful information about the input, but it is also spars. And now this algorithm disparative. This algorithm can also scale to very large datasets. Well, I should say that this algorithm was taken, uh, in, is highly inspired from this paper by [inaudible] back pounds and, uh, Shapiro in, uh, 2000, uh, nine, uh, you can go look it up. It's a very interesting paper.