WEBVTT
Kind: captions
Language: en

00:00:03.060 --> 00:00:06.560
Hi, I’m Adriene Hill, and welcome back to
Crash Course Statistics.

00:00:06.569 --> 00:00:08.930
In the last episode, we talked about the value
of Big Data.

00:00:08.930 --> 00:00:14.179
But, as Big Data (and the statistics we do
with it) permeate more areas of our lives,

00:00:14.179 --> 00:00:16.480
there are also new problems that come up.

00:00:16.480 --> 00:00:20.721
How can we learn from useful data while still
keeping it safe and private?

00:00:20.721 --> 00:00:25.780
When there’s SO much data that we have to
rely on algorithms to manage it can we trust

00:00:25.780 --> 00:00:26.900
those algorithms?

00:00:26.900 --> 00:00:31.000
Today, we’re going to have a discussion
about the potential downsides of Big Data

00:00:31.000 --> 00:00:33.600
in our lives and some possible solutions.

00:00:33.600 --> 00:00:42.720
INTRO

00:00:42.720 --> 00:00:44.600
Let’s start with a Thought Bubble.

00:00:44.600 --> 00:00:48.350
This story comes out of a collaboration between
the University of Washington and the University

00:00:48.350 --> 00:00:50.190
of California Irvine.

00:00:50.190 --> 00:00:53.960
The team wanted to create an algorithm that
could take an image and determine whether

00:00:53.960 --> 00:00:55.769
it was of a husky or a wolf.

00:00:55.769 --> 00:00:59.010
To do that, they trained the algorithm with
a bunch of pictures.

00:00:59.010 --> 00:01:04.059
These images are a great example of “BIG
DATA” that we CAN wrap our heads around.

00:01:04.059 --> 00:01:06.490
Pictures are generally made up of millions
of tiny pixels.

00:01:06.490 --> 00:01:10.240
And each pixel is made up of three colors,
red, green, and blue.

00:01:10.240 --> 00:01:13.189
So three values per pixel is a lot of data.

00:01:13.189 --> 00:01:15.100
The algorithm ended up doing pretty well.

00:01:15.100 --> 00:01:19.270
The team may have anticipated that it would
recognize the animals’ different body types,

00:01:19.270 --> 00:01:21.420
facial features, or body placement.

00:01:21.420 --> 00:01:25.390
But it turned out that it wasn’t focusing
on the animals’ appearances at all.

00:01:25.390 --> 00:01:28.060
It was mainly looking at snow.

00:01:28.060 --> 00:01:32.789
In the data used to create the algorithm,
the research team inadvertently included many

00:01:32.789 --> 00:01:35.540
photos of wolves in the snow.

00:01:35.540 --> 00:01:37.820
Huskies were often pictured without any snow
around.

00:01:37.820 --> 00:01:42.159
The algorithm picked up on that and glommed
onto it as an easy way to tell if something

00:01:42.159 --> 00:01:44.000
was a husky or wolf.

00:01:44.000 --> 00:01:48.369
Once the researchers learned this, they did
an experiment, feeding the algorithm new images

00:01:48.369 --> 00:01:50.430
that had been digitally altered.

00:01:50.430 --> 00:01:54.940
According to one of them, Dr. Sameer Singh,
“When we hid the wolf in the image and sent

00:01:54.940 --> 00:01:59.899
it across, the network would still predict
that it was a wolf, but when we hid the snow,

00:01:59.899 --> 00:02:02.719
it would not be able to predict that it was
a wolf anymore."

00:02:02.719 --> 00:02:05.340
The algorithm just learned from what it was
given.

00:02:05.340 --> 00:02:10.179
And based on the data it was trained with,
snow meant the image was way more likely to

00:02:10.179 --> 00:02:11.590
be a wolf.

00:02:11.590 --> 00:02:13.220
Even more than adorable wolf-i-ness.

00:02:13.220 --> 00:02:14.340
Thanks, Thought Bubble.

00:02:14.340 --> 00:02:18.430
That algorithm brings us to our first concern
with Big Data: bias.

00:02:18.430 --> 00:02:21.600
The defining characteristic of “Big Data”
is that it’s big.

00:02:21.600 --> 00:02:24.510
It’s too big for a lot of the usual programs
we use to look at data.

00:02:24.510 --> 00:02:27.990
In fact it’s sometimes even too big for
us to comprehend.

00:02:27.990 --> 00:02:32.870
And when huge amounts of data are used to
create algorithms, we can inadvertently introduce

00:02:32.870 --> 00:02:33.870
bias.

00:02:33.870 --> 00:02:35.110
Like the wolf and snow problem

00:02:35.110 --> 00:02:38.900
Other algorithms could do similar things,
but with higher stakes.

00:02:38.900 --> 00:02:43.590
Ones used to determine mortgage and insurance
rates, or assess the risk someone will do

00:02:43.590 --> 00:02:49.550
something illegal in the future, might pick
up on things like race or other minority statuses.

00:02:49.550 --> 00:02:50.550
And this is real.

00:02:50.550 --> 00:02:55.239
Judges in the U.S. use risk assessment programs
while making sentencing decisions.

00:02:55.239 --> 00:02:59.459
A commonly-used one is COMPAS, which was created
by the company Equivant.

00:02:59.459 --> 00:03:05.060
It basically gives a score of how likely a
person is to commit another crime within two

00:03:05.060 --> 00:03:06.060
years.

00:03:06.060 --> 00:03:08.840
In 2016, ProPublica published an investigation
of COMPAS.

00:03:08.840 --> 00:03:13.239
They looked at the scores of 7,000 people
who had been arrested Broward County, Florida.

00:03:13.239 --> 00:03:17.640
The scores were compared with whether those
people actually ended up committing crimes

00:03:17.640 --> 00:03:19.340
again within two years.

00:03:19.340 --> 00:03:24.970
In addition to other concerning revelations,
ProPublica found, “The formula was particularly

00:03:24.970 --> 00:03:30.519
likely to falsely flag black defendants as
future criminals, wrongly labeling them this

00:03:30.519 --> 00:03:34.010
way at almost twice the rate as white defendants.

00:03:34.010 --> 00:03:39.430
[And] white defendants were mislabeled as
low risk more often than black defendants.”

00:03:39.430 --> 00:03:41.980
Equivant, then called Northpointe, did disagree
with these findings.

00:03:41.980 --> 00:03:45.720
But, the company’s founder, Tim Brennan,
also claimed that in order to make scores

00:03:45.720 --> 00:03:51.560
as accurate as possible, certain factors had
to be included that could correlate with race.

00:03:51.560 --> 00:03:56.790
ProPublica cited the examples “poverty,
joblessness and social marginalization.”

00:03:56.790 --> 00:04:01.650
So we can’t consider ourselves “safe”
just because we think that our data is neutral.

00:04:01.650 --> 00:04:05.540
We should also look for ways to make sure
that the data we use to create our algorithm

00:04:05.540 --> 00:04:08.269
is as representative as possible.

00:04:08.269 --> 00:04:13.590
If we wanted to build an algorithm that predicted
the success of CEOs, and we ONLY gave it examples

00:04:13.590 --> 00:04:19.290
of males who succeeded and females who failed
then our algorithm will have a bias.

00:04:19.290 --> 00:04:22.340
We have to supply it with good, unbiased data.

00:04:22.340 --> 00:04:25.810
Males who succeeded and failed, and females
that succeeded and failed.

00:04:25.810 --> 00:04:29.930
In the tech world, you’ll often hear the
phrase “Garbage In, Garbage Out” which

00:04:29.930 --> 00:04:32.270
means that bad input will lead to bad output.

00:04:32.270 --> 00:04:38.110
You can’t put biased data into an algorithm,
and expect an unbiased output.

00:04:38.110 --> 00:04:42.000
It can be hard to determine what kind of data
will lead to biased decision making especially

00:04:42.000 --> 00:04:45.139
considering most of these algorithms are proprietary.

00:04:45.139 --> 00:04:48.930
In the Equivant example I mentioned earlier,
the company wouldn’t reveal the details

00:04:48.930 --> 00:04:53.389
of the algorithm used for COMPAS to ProPublica
for that exact reason.

00:04:53.389 --> 00:04:57.460
It’s also hard to figure out exactly what
an algorithm is doing from the time we give

00:04:57.460 --> 00:05:00.970
it raw data, to the time it gives us an output,
or decision.

00:05:00.970 --> 00:05:04.840
With the methods we’ve talked about in this
series, like regression, it’s easy to see

00:05:04.840 --> 00:05:07.410
which variables they consider important.

00:05:07.410 --> 00:05:13.050
But other Big Data methods, like neural networks,
are often way less forthcoming with the “reasoning”

00:05:13.050 --> 00:05:14.390
behind their outputs.

00:05:14.390 --> 00:05:18.780
While we can’t always tell what algorithms
are doing, some researchers have made other

00:05:18.780 --> 00:05:24.240
algorithms that can act as a sort of translator
to turn the complex calculations of another

00:05:24.240 --> 00:05:26.870
algorithm into something humans can understand.

00:05:26.870 --> 00:05:31.699
The more humans can understand what an algorithm
is doing, the more opportunities we have to

00:05:31.699 --> 00:05:35.080
recognize biased data and the resulting decisions.

00:05:35.080 --> 00:05:38.400
Some believe that search and social media
websites that use algorithms to affect your

00:05:38.400 --> 00:05:44.420
experience based on your data should be required
to release more information about that algorithm

00:05:44.420 --> 00:05:46.350
-- how it works and what it’s doing.

00:05:46.350 --> 00:05:49.020
That’s called algorithmic transparency.

00:05:49.020 --> 00:05:51.350
Privacy is another big concern in the Age
of Big Data.

00:05:51.350 --> 00:05:55.920
There’s all kinds of personal data about
you that you might not want people to know.

00:05:55.920 --> 00:05:59.340
There are your entertainment choices, like
what you’re reading and how many times you’ve

00:05:59.340 --> 00:06:02.710
watched Fuller House or listened to “I Like
It”.

00:06:02.710 --> 00:06:06.860
And your school or work--emails, cloud services,
web browsing.

00:06:06.860 --> 00:06:11.400
Even your basic information like your location,
your step count, or heart rate are tracked

00:06:11.400 --> 00:06:13.400
by your various smart devices.

00:06:13.400 --> 00:06:17.949
Companies like 23andMe or Ancestry.com might
even have your genetic code.

00:06:17.949 --> 00:06:20.750
Maybe you spend a lot of time at a place with
security cameras.

00:06:20.750 --> 00:06:25.120
There are a lot of questions when thinking
about privacy: Who has access to all that

00:06:25.120 --> 00:06:26.120
information?

00:06:26.120 --> 00:06:27.120
What are they doing with it?

00:06:27.120 --> 00:06:28.410
Who are they sharing it with?

00:06:28.410 --> 00:06:31.800
And what assumptions are they making about
us with the data they have?

00:06:31.800 --> 00:06:36.870
In 2018, The European Union implemented a
law--the General Data Protection Regulation,

00:06:36.870 --> 00:06:41.891
or GDPR for short--that addresses a lot of
the privacy concerns people have with the

00:06:41.891 --> 00:06:43.349
use of Big Data.

00:06:43.349 --> 00:06:48.110
It requires companies that deal with Big Data
to be more transparent about what they’re

00:06:48.110 --> 00:06:49.510
collecting and who can see it.

00:06:49.510 --> 00:06:54.259
And it might be one of the reasons you got
a LOT of emails about updated Privacy Policies…back

00:06:54.259 --> 00:06:55.680
in May of 2018.

00:06:55.680 --> 00:06:59.750
The U.S. has The Children’s Online Privacy
Protection Act, which went into effect in

00:06:59.750 --> 00:07:00.750
2000.

00:07:00.750 --> 00:07:05.000
It’s intended to protect the privacy of
children under the age of thirteen.

00:07:05.000 --> 00:07:10.419
The Act basically requires websites and apps
to get parental approval for the personal

00:07:10.419 --> 00:07:12.319
information it might collect from kids.

00:07:12.319 --> 00:07:15.930
And using that information for targeted ads
is not allowed.

00:07:15.930 --> 00:07:20.960
In 2018, a study of about 6,000 children’s
apps was published in the journal Proceedings

00:07:20.960 --> 00:07:23.450
on Privacy Enhancing Technologies.

00:07:23.450 --> 00:07:28.259
It found that about 57% of them were “potentially
violating COPPA.”

00:07:28.259 --> 00:07:32.699
Examples of violations included “sharing
of personal information without applying reasonable

00:07:32.699 --> 00:07:37.270
security measures,” “potential sharing
[of] persistent identifiers with third parties

00:07:37.270 --> 00:07:42.900
for prohibited purposes,” and “[sharing]
location or contact information without consent.”

00:07:42.900 --> 00:07:47.120
Later that year, the attorney general of New
Mexico filed a lawsuit against an app maker

00:07:47.120 --> 00:07:48.790
for violating COPPA.

00:07:48.790 --> 00:07:51.500
Privacy laws have been around for a long time
all over the world.

00:07:51.500 --> 00:07:54.780
But as they pertain to Big Data, a lot of
this stuff is new, we’re still figuring

00:07:54.780 --> 00:07:55.780
it out.

00:07:55.780 --> 00:07:59.970
At the same time, when universities, hospitals,
and other organizations share data, we learn

00:07:59.970 --> 00:08:00.970
a lot.

00:08:00.970 --> 00:08:02.780
It can be useful.

00:08:02.780 --> 00:08:07.879
A health organization’s survey on risky
behaviors, like drug use, could have incredibly

00:08:07.879 --> 00:08:10.500
valuable results to researchers and policy
makers.

00:08:10.500 --> 00:08:15.620
So, we can try to make it so that data can’t
be easily connected to the specific person

00:08:15.620 --> 00:08:16.729
it came from.

00:08:16.729 --> 00:08:21.880
The obvious first step is to not include people’s
names, or other unique, identifying personal

00:08:21.880 --> 00:08:22.880
information.

00:08:22.880 --> 00:08:23.880
But that may not be enough.

00:08:23.880 --> 00:08:27.659
If someone has a rare disease, simply knowing
the city where they live might be enough to

00:08:27.659 --> 00:08:29.080
figure out who they are.

00:08:29.080 --> 00:08:33.200
One option to combat this issue is to make
sure that there are at least 2 or more subjects

00:08:33.200 --> 00:08:35.500
that have the same characteristics.

00:08:35.500 --> 00:08:37.340
This is called k-anonymity.

00:08:37.340 --> 00:08:41.300
K is the number of subjects who share the
exact same characteristics.

00:08:41.300 --> 00:08:45.080
If there are two people with that disease
from that city, we have 2-anonymity because

00:08:45.080 --> 00:08:47.250
there were 2 subjects with the same characteristics.

00:08:47.250 --> 00:08:52.470
In our dataset, these two subjects are indistinguishable
from each other, which helps keep the data

00:08:52.470 --> 00:08:53.470
private.

00:08:53.470 --> 00:08:55.800
And the larger k is, on average, the better.

00:08:55.800 --> 00:08:59.550
Outside of medical research, there are debates
about what companies should be expected to

00:08:59.550 --> 00:09:00.550
keep private.

00:09:00.550 --> 00:09:02.210
DNA companies, for example.

00:09:02.210 --> 00:09:08.720
In 2018, Joseph James DeAngelo was arrested
as the suspected Golden State Killer.

00:09:08.720 --> 00:09:13.830
Investigators found DeAngelo because they
had DNA from a crime scene, which they uploaded

00:09:13.830 --> 00:09:18.860
to a public, online genealogy database called
GEDmatch.

00:09:18.860 --> 00:09:22.020
The database doesn’t collect DNA, but lets
people upload profiles.

00:09:22.020 --> 00:09:27.770
So, the investigators were able to connect
DeAngelo’s DNA with other relatives and

00:09:27.770 --> 00:09:30.210
figure out who he was from there.

00:09:30.210 --> 00:09:34.720
Even though he hadn’t uploaded anything
to the site personally, the information his

00:09:34.720 --> 00:09:37.570
relatives had submitted was enough.

00:09:37.570 --> 00:09:42.040
GEDmatch does have rules about whose DNA you
can upload to the site, like you can upload

00:09:42.040 --> 00:09:44.760
your own or someone else’s with permission.

00:09:44.760 --> 00:09:51.160
Their site policy also currently states that
DNA can be uploaded if it was “obtained

00:09:51.160 --> 00:09:56.850
and authorized by law enforcement to either:
(1) identify a perpetrator of a violent crime

00:09:56.850 --> 00:10:02.160
against another individual; or (2) identify
remains of a deceased individual.”

00:10:02.160 --> 00:10:07.080
And the revelation that cases could get solved
this way has led to questions of how private

00:10:07.080 --> 00:10:10.850
companies with DNA databases should be keeping
their data.

00:10:10.850 --> 00:10:13.970
Currently, we don’t know how often cases
get solved like this.

00:10:13.970 --> 00:10:18.730
Although a spokesperson for 23andMe told the
New York Times that they’ve received “a

00:10:18.730 --> 00:10:23.340
handful of inquiries over the course of 11
years” from law enforcement.

00:10:23.340 --> 00:10:25.630
He claimed that data was never handed over.

00:10:25.630 --> 00:10:30.770
Criminal investigations aside, it’s regular
practice for both 23andMe and AncestryDNA

00:10:30.770 --> 00:10:33.250
to share data with medical researchers.

00:10:33.250 --> 00:10:35.760
Though participants can opt in or out.

00:10:35.760 --> 00:10:42.380
And in 2018, it was announced that the pharmaceutical
company GlaxoSmithKline invested $300 million

00:10:42.380 --> 00:10:46.470
in 23andMe for drug development with the company’s
resources.

00:10:46.470 --> 00:10:50.530
We have privacy laws in the U.S., but a lot
of this is still ambiguous.

00:10:50.530 --> 00:10:54.280
When it comes to your personal privacy, the
best thing you can do is try to be as informed

00:10:54.280 --> 00:10:58.490
as possible about what’s happening to your
data when you put it out there.

00:10:58.490 --> 00:11:03.450
And all the data out there means there’s
a lot of information that can be stolen.

00:11:03.450 --> 00:11:08.010
And yes, better technology DOES allow for
more protections like encryption but it also

00:11:08.010 --> 00:11:11.260
exposes our data to wider scale breeches.

00:11:11.260 --> 00:11:14.760
Hackers are after your personal information---
information that can be used to set up lines

00:11:14.760 --> 00:11:18.000
of credit-- like when Equifax was hacked in
2017.

00:11:18.000 --> 00:11:21.270
They also want your photos (remember the iCloud
in hack in 2014)

00:11:21.270 --> 00:11:22.270
Your indiscretions…

00:11:22.270 --> 00:11:24.030
(Ashley Madison)

00:11:24.030 --> 00:11:25.130
Your email addresses…

00:11:25.130 --> 00:11:27.700
(Yahoo was hacked back in 2013 and 2014)

00:11:27.700 --> 00:11:32.360
Your business files… (remember the Sony
studios hack after “The Interview”)

00:11:32.360 --> 00:11:36.880
Hackers have no qualms about cutting off your
access to play FIFA--like when the Playstation

00:11:36.880 --> 00:11:40.180
network shut down after an attack in 2011.

00:11:40.180 --> 00:11:44.280
Companies and institutions like these that
collect our data have responsibility to protect

00:11:44.280 --> 00:11:45.280
it.

00:11:45.280 --> 00:11:48.520
But just how much responsibility and what
happens when they don’t.

00:11:48.520 --> 00:11:49.860
We’re still figuring that out.

00:11:49.860 --> 00:11:53.380
We don’t want to let our excitement about
Big Data to outpace our caution.

00:11:53.380 --> 00:11:58.220
We don’t want to be like the scientists
in Jurassic Park: so preoccupied with whether

00:11:58.220 --> 00:12:02.000
we could, and not stopping to think about
whether we should.

00:12:02.000 --> 00:12:07.790
As a society we need to think about and implement
solutions to the problems big data creates.

00:12:07.790 --> 00:12:10.380
We want to use for good not not good.

00:12:10.380 --> 00:12:12.880
Thanks for watching. I'll see you next time.

