WEBVTT
Kind: captions
Language: en

00:00:04.920 --> 00:00:07.560
BRADY FORREST: Hey,
Adam, next slide.

00:00:07.560 --> 00:00:10.000
All right, welcome to Ignite.

00:00:10.000 --> 00:00:13.276
[APPLAUSE]

00:00:14.680 --> 00:00:16.700
We are super happy
to be back here

00:00:16.700 --> 00:00:19.830
for the sixth straight
Ignite at Google I/O.

00:00:19.830 --> 00:00:23.360
And we've got eight
amazing speakers for today,

00:00:23.360 --> 00:00:26.100
and now we're going
to get started.

00:00:26.100 --> 00:00:32.369
So Ignite started a decade ago,
and it's on the simple premise

00:00:32.369 --> 00:00:36.360
that everybody has a story,
that everybody deserves a chance

00:00:36.360 --> 00:00:38.120
to be a rock star.

00:00:38.120 --> 00:00:40.110
But when it first
started out, it

00:00:40.110 --> 00:00:44.740
was really just about
giving people a chance

00:00:44.740 --> 00:00:46.990
to share ideas over beer.

00:00:46.990 --> 00:00:48.640
We set up in a bar.

00:00:48.640 --> 00:00:50.710
We invited friends over.

00:00:50.710 --> 00:00:53.490
And 200 people showed up.

00:00:53.490 --> 00:00:56.570
AUDIENCE: Where's the beer?

00:00:56.570 --> 00:00:59.490
BRADY FORREST: Ask Google.

00:00:59.490 --> 00:01:01.960
And all of the speakers
got five minutes

00:01:01.960 --> 00:01:05.940
on stage, 15 seconds
a slide, 20 slides.

00:01:05.940 --> 00:01:10.040
And it turns out that's the
perfect amount of time to share

00:01:10.040 --> 00:01:13.130
one idea, to share one story.

00:01:13.130 --> 00:01:17.690
And it helps new speakers
really distill their thoughts.

00:01:17.690 --> 00:01:20.970
Over time, Ignite started
to spread around the world.

00:01:20.970 --> 00:01:24.220
It's now in over 300 cities
on all six continents.

00:01:26.920 --> 00:01:28.930
And Ignite is changing.

00:01:28.930 --> 00:01:32.030
So very recently,
Tim, our first speaker

00:01:32.030 --> 00:01:33.960
allowed me to re-acquire Ignite.

00:01:33.960 --> 00:01:38.200
And we are now Ignite Talks PBC,
a public benefit corporation

00:01:38.200 --> 00:01:40.530
devoted to giving
people a chance

00:01:40.530 --> 00:01:43.600
to experience public
speaking for the first time

00:01:43.600 --> 00:01:44.850
around the world.

00:01:44.850 --> 00:01:47.530
So thank you very much
for having us here.

00:01:47.530 --> 00:01:50.610
And now I'd like to welcome up
my friend, the founder and CEO

00:01:50.610 --> 00:01:52.590
of O'Reilly Media, Tim O'Reilly.

00:01:52.590 --> 00:01:54.200
[APPLAUSE]

00:01:54.200 --> 00:01:56.110
TIM O'REILLY: Hey,
thanks, Brady.

00:02:01.270 --> 00:02:03.540
So I'm working on
a new event that I

00:02:03.540 --> 00:02:06.000
call the Next Economy
Summit about technology

00:02:06.000 --> 00:02:07.206
and the future of work.

00:02:07.206 --> 00:02:09.039
I've been out there,
talking to smart people

00:02:09.039 --> 00:02:12.460
like Hal Varian who said to me
something that stuck with me.

00:02:12.460 --> 00:02:17.130
He said, my grandfather wouldn't
recognize what I do as work.

00:02:17.130 --> 00:02:18.036
But was he kidding?

00:02:18.036 --> 00:02:20.410
The more things change, the
more they stay the same here.

00:02:20.410 --> 00:02:22.470
Here's a bunch of
programmers at Pivotal,

00:02:22.470 --> 00:02:25.410
and then a Victorian sweatshop.

00:02:25.410 --> 00:02:31.020
But actually, something
is really different.

00:02:31.020 --> 00:02:35.100
Because the programmers are
not actually the workers.

00:02:35.100 --> 00:02:37.500
Low wage employers like
McDonald's and Walmart

00:02:37.500 --> 00:02:38.550
are the new sweatshop.

00:02:38.550 --> 00:02:40.258
Something different
is happening in tech.

00:02:40.258 --> 00:02:45.360
McDonald's has 440,000
employees delivering 68 services

00:02:45.360 --> 00:02:48.520
to 68 million monthly
users, Snapchat 300

00:02:48.520 --> 00:02:49.800
to do 100 million users.

00:02:49.800 --> 00:02:50.330
Why?

00:02:50.330 --> 00:02:52.940
Because this is what
the worker looks like.

00:02:52.940 --> 00:02:55.960
That's the GitHub repo for
one of the workers at Google,

00:02:55.960 --> 00:02:59.000
TensorFlow, does a whole
lot of work for millions

00:02:59.000 --> 00:03:02.380
of users with very few people.

00:03:02.380 --> 00:03:04.930
The programmers, all of
you who are programmers,

00:03:04.930 --> 00:03:06.810
are actually managers.

00:03:06.810 --> 00:03:09.920
Every day you're inspecting the
performance of your workers,

00:03:09.920 --> 00:03:12.370
giving them instruction
in the form of code,

00:03:12.370 --> 00:03:13.900
about how to do a better job.

00:03:13.900 --> 00:03:16.370
That's what the
build, measure, learn

00:03:16.370 --> 00:03:18.690
cycle is, it's your
management practices.

00:03:18.690 --> 00:03:20.520
But there are other
companies where

00:03:20.520 --> 00:03:22.130
the programs are the managers.

00:03:22.130 --> 00:03:23.630
Think about Uber or Lyft.

00:03:23.630 --> 00:03:26.660
There are programs
telling humans what to do.

00:03:26.660 --> 00:03:29.660
They're telling them where to
show up, how much to get paid,

00:03:29.660 --> 00:03:33.100
and, of course,
organizing ratings.

00:03:33.100 --> 00:03:35.270
But the people who've
written the algorithms

00:03:35.270 --> 00:03:38.440
are not necessarily doing a
good job by their human workers.

00:03:38.440 --> 00:03:41.510
Because Uber ends up paying
people less than minimum wage

00:03:41.510 --> 00:03:43.930
in some cities because
they've written the algorithm

00:03:43.930 --> 00:03:46.860
to satisfy only user
needs without thinking

00:03:46.860 --> 00:03:48.540
about the needs of the workers.

00:03:48.540 --> 00:03:50.560
And this seems to
me to be like what

00:03:50.560 --> 00:03:52.080
happened in the
early days of search

00:03:52.080 --> 00:03:56.350
with companies like Altavista,
here retrieved from 1996.

00:03:56.350 --> 00:03:58.822
You know, it was a pretty
shitty search experience.

00:03:58.822 --> 00:03:59.970
[LAUGHTER]

00:03:59.970 --> 00:04:02.130
You know, and along came Google.

00:04:02.130 --> 00:04:05.780
And they figured out that
by tuning the algorithm,

00:04:05.780 --> 00:04:08.070
by figuring out lots and
lots of different factors,

00:04:08.070 --> 00:04:09.910
they created a better
experience for users.

00:04:09.910 --> 00:04:11.451
They could also
figure out how to get

00:04:11.451 --> 00:04:12.550
paid with the ad auction.

00:04:12.550 --> 00:04:14.960
There's a lot of
work ahead of us

00:04:14.960 --> 00:04:17.720
to figure out how to make
these algorithms that

00:04:17.720 --> 00:04:20.640
manage humans work
better for the workers

00:04:20.640 --> 00:04:22.060
as well as for the users.

00:04:22.060 --> 00:04:25.260
I predicted that this would be
the year in which labor issues

00:04:25.260 --> 00:04:27.840
became really critical,
competitive advantage

00:04:27.840 --> 00:04:29.440
between Uber and Lyft.

00:04:29.440 --> 00:04:31.620
And sure enough,
Uber just made a deal

00:04:31.620 --> 00:04:33.140
with the Mechanics Union.

00:04:33.140 --> 00:04:36.820
Because as my friend
David Rolf of the SEIU who

00:04:36.820 --> 00:04:39.050
spoke at last year's
Next Economy Summit said,

00:04:39.050 --> 00:04:42.130
"God did not make being an
auto worker a good job."

00:04:42.130 --> 00:04:42.630
Right?

00:04:42.630 --> 00:04:44.650
It took a lot of work
by a lot of people

00:04:44.650 --> 00:04:48.360
to figure out how to make
work good for humans.

00:04:48.360 --> 00:04:50.840
And when you look at a
company like Walmart, which

00:04:50.840 --> 00:04:55.250
has really crappy jobs for
humans, managed by algorithms,

00:04:55.250 --> 00:04:58.210
the scheduling algorithms that
tell people when to show up,

00:04:58.210 --> 00:05:00.480
have no affordances for workers.

00:05:00.480 --> 00:05:02.740
When you go to a
McDonald's, you might

00:05:02.740 --> 00:05:06.055
be assigned what's called
a "clopen," a shift where

00:05:06.055 --> 00:05:08.680
you have to close the store and
open it the next morning, maybe

00:05:08.680 --> 00:05:09.700
five hours apart.

00:05:09.700 --> 00:05:13.100
And the fact that you live
an hour away doesn't matter.

00:05:13.100 --> 00:05:17.680
You don't do it, you get fired--
those terrible algorithms

00:05:17.680 --> 00:05:18.180
there.

00:05:18.180 --> 00:05:21.690
And that's why the algorithms
that Uber and Lyft are

00:05:21.690 --> 00:05:24.800
using actually give
people more affordances.

00:05:24.800 --> 00:05:27.300
They haven't yet figured out
how to get people paid enough,

00:05:27.300 --> 00:05:29.010
but they are giving
people control

00:05:29.010 --> 00:05:31.620
over their lives in a way
that the scheduling algorithms

00:05:31.620 --> 00:05:35.490
that made the new
sweatshop do not.

00:05:35.490 --> 00:05:38.630
But back to Hal, there
are also other kinds

00:05:38.630 --> 00:05:40.870
of jobs he wouldn't
recognize as work.

00:05:40.870 --> 00:05:43.040
How about all the
people who are creating

00:05:43.040 --> 00:05:46.470
content on social media,
on Facebook and YouTube?

00:05:46.470 --> 00:05:51.490
Some people are starting to make
a living at it, but not enough.

00:05:51.490 --> 00:05:53.400
I lost a slide here.

00:05:53.400 --> 00:05:55.120
I had one in here,
but anyway, we'll

00:05:55.120 --> 00:05:56.390
just go forward with this.

00:05:56.390 --> 00:05:59.480
This is what led
Jack Conte to start

00:05:59.480 --> 00:06:03.650
Patreon, which is a
crowdfunding site for artists.

00:06:03.650 --> 00:06:06.330
He was a member of a
musical duo, or still is,

00:06:06.330 --> 00:06:08.070
called Pomplamoose
with his wife.

00:06:08.070 --> 00:06:12.530
They did 17 million views of
one of their videos on YouTube,

00:06:12.530 --> 00:06:14.870
and it turned out
to give them $3,500.

00:06:14.870 --> 00:06:16.740
He said, there's got
to be a better way.

00:06:16.740 --> 00:06:20.680
And so he basically is now
raising millions of dollars.

00:06:20.680 --> 00:06:22.870
Brandon Stanton, Humans
of New York, this guy

00:06:22.870 --> 00:06:24.840
has raised millions of
dollars for charity.

00:06:24.840 --> 00:06:28.590
But he gets his living
from doing speaking gigs,

00:06:28.590 --> 00:06:32.180
amazing social media genius.

00:06:32.180 --> 00:06:35.060
So all of this is
a way of saying

00:06:35.060 --> 00:06:38.540
we have to think in the same
way that Norman Mailer asked

00:06:38.540 --> 00:06:40.420
the question, why
are we in Vietnam

00:06:40.420 --> 00:06:42.540
and gave a poem
about American macho,

00:06:42.540 --> 00:06:44.520
we have to ask
ourselves, why are we

00:06:44.520 --> 00:06:48.770
seeing this political uprising
about income inequality?

00:06:48.770 --> 00:06:50.510
And we have a role
in this, because it's

00:06:50.510 --> 00:06:54.400
time to build technology and
companies as if people matter.

00:06:54.400 --> 00:06:57.410
And because we are actually
not just serving users.

00:06:57.410 --> 00:06:59.840
We are building systems of work.

00:06:59.840 --> 00:07:02.760
We have to think hard
about the future of work.

00:07:02.760 --> 00:07:04.035
Thank you.

00:07:04.035 --> 00:07:06.046
[APPLAUSE]

00:07:12.540 --> 00:07:13.790
BRADY FORREST: Thank you, Tim.

00:07:13.790 --> 00:07:16.810
And thanks again for
the support with Ignite.

00:07:16.810 --> 00:07:19.450
Now we're going straight
to the future with robots.

00:07:19.450 --> 00:07:23.170
Please welcome up the
executive director of Long Now

00:07:23.170 --> 00:07:26.585
and the star of
"Battlebots," Zander Rose.

00:07:26.585 --> 00:07:28.790
[APPLAUSE]

00:07:29.630 --> 00:07:32.070
ZANDER ROSE: Thank you.

00:07:32.070 --> 00:07:34.700
Yeah, so in my spare
time, one of the things

00:07:34.700 --> 00:07:36.340
that I got into
over 20 years ago

00:07:36.340 --> 00:07:40.310
was combat robotics, which got
started around here in the Bay

00:07:40.310 --> 00:07:42.980
Area about 20 years ago.

00:07:42.980 --> 00:07:46.060
And it started as
an event called

00:07:46.060 --> 00:07:48.520
Robot Wars in Fort Mason.

00:07:48.520 --> 00:07:51.780
And pretty early on,
one of the things that

00:07:51.780 --> 00:07:56.422
started shaking out where the
types of designs of robots that

00:07:56.422 --> 00:07:58.880
could be allowed, because there
were a lot of rules, mainly

00:07:58.880 --> 00:08:00.870
a weight limit, that
you're working up against.

00:08:00.870 --> 00:08:04.650
But the real thing that
started to come about

00:08:04.650 --> 00:08:06.230
is like are you
building a spinner,

00:08:06.230 --> 00:08:09.910
a lifter, a flipper, a
puncher, something that cuts?

00:08:09.910 --> 00:08:14.530
And so the robot that we first
built back almost 20 years ago

00:08:14.530 --> 00:08:17.420
is a robot called Rhino
where we developed

00:08:17.420 --> 00:08:20.780
a very powerful pneumatic
kind of punching system.

00:08:20.780 --> 00:08:22.670
And it did pretty well.

00:08:22.670 --> 00:08:24.420
We were able to get
kind of third place.

00:08:24.420 --> 00:08:27.200
And then there was a TV show
that was on Comedy Central

00:08:27.200 --> 00:08:30.670
back then, now
about 12 years ago.

00:08:30.670 --> 00:08:33.030
But when we took that same
pneumatic system, instead

00:08:33.030 --> 00:08:35.210
of trying to leverage
off our own robot,

00:08:35.210 --> 00:08:37.414
to turn that system
vertically, we

00:08:37.414 --> 00:08:39.830
could actually start launching
other robots up in the air.

00:08:39.830 --> 00:08:41.705
And so this was a much
more successful robot.

00:08:41.705 --> 00:08:45.400
We won several championships
with this robot named Toro back

00:08:45.400 --> 00:08:46.560
in that TV show.

00:08:46.560 --> 00:08:48.190
And the TV show
went off the air.

00:08:48.190 --> 00:08:52.014
And then just this
last year, ABC

00:08:52.014 --> 00:08:54.180
called up and said they
wanted to do the show again.

00:08:54.180 --> 00:08:56.424
And so we started building
from scratch once again.

00:08:56.424 --> 00:08:57.840
And one of our
main things that we

00:08:57.840 --> 00:09:01.320
started learning
about new components--

00:09:01.320 --> 00:09:04.410
how we're going to design now
is with really high performance

00:09:04.410 --> 00:09:07.440
components that have come out
of the automotive industry,

00:09:07.440 --> 00:09:09.240
the electric bike industry.

00:09:09.240 --> 00:09:11.190
Now there's actual
robot controllers

00:09:11.190 --> 00:09:14.660
like these ones that
have a processor on board

00:09:14.660 --> 00:09:20.930
so that you can do some
of the programming onboard

00:09:20.930 --> 00:09:23.459
the robot as much as the
controls that you're doing.

00:09:23.459 --> 00:09:25.250
Some things that have
really changed things

00:09:25.250 --> 00:09:27.740
are things like battery, these
lithium polymer batteries.

00:09:27.740 --> 00:09:31.750
The old robots we built had 40
pounds of lead acid batteries

00:09:31.750 --> 00:09:32.667
or NiCad batteries.

00:09:32.667 --> 00:09:34.250
Now they have six
pounds of batteries,

00:09:34.250 --> 00:09:36.690
and they have even
better performance.

00:09:36.690 --> 00:09:39.270
And some components we had
to fabricate ourselves.

00:09:39.270 --> 00:09:42.300
So we actually had to design
and build our own gearboxes

00:09:42.300 --> 00:09:44.190
in order to get the
kind of performance

00:09:44.190 --> 00:09:48.110
to weight ratio in kind
of titanium, steel,

00:09:48.110 --> 00:09:50.770
and aluminum gear boxes.

00:09:50.770 --> 00:09:52.680
And the other constraint
is that you find out

00:09:52.680 --> 00:09:54.721
that you're building this
robot, because it's TV,

00:09:54.721 --> 00:09:56.070
two months before the TV show.

00:09:56.070 --> 00:09:59.670
And so we went straight to
CAD working with a bunch

00:09:59.670 --> 00:10:01.730
of the cool new Autodesk tools.

00:10:01.730 --> 00:10:04.590
But we had to design in a
way that once we had a design

00:10:04.590 --> 00:10:06.020
we could fabricate very fast.

00:10:06.020 --> 00:10:08.100
So we kind of used
this flat pack

00:10:08.100 --> 00:10:10.580
design of notching
everything together,

00:10:10.580 --> 00:10:12.620
went to laser cut wood.

00:10:12.620 --> 00:10:14.600
We could just hot glue
everything together,

00:10:14.600 --> 00:10:17.430
put all our components inside
the robot, see if it works.

00:10:17.430 --> 00:10:20.450
We probably built six or
seven versions in wood

00:10:20.450 --> 00:10:21.990
before we went to metal.

00:10:21.990 --> 00:10:24.530
But those exact
same cutting files

00:10:24.530 --> 00:10:26.460
that we used in
wood we could then

00:10:26.460 --> 00:10:27.930
put into a water jet machine.

00:10:27.930 --> 00:10:29.460
And what we were
cutting in wood,

00:10:29.460 --> 00:10:33.050
we're now cutting in aluminum,
titanium, very high strength

00:10:33.050 --> 00:10:35.122
steel like AR500 steel.

00:10:35.122 --> 00:10:36.580
And the main
constraint that you're

00:10:36.580 --> 00:10:39.430
always up against in trying to
build a Battlebot is weight.

00:10:39.430 --> 00:10:42.040
It's a 250-pound
weight class right now.

00:10:42.040 --> 00:10:43.830
And so at every
stage, you're trying

00:10:43.830 --> 00:10:48.170
to trade weight on the inside
of the robot for performance

00:10:48.170 --> 00:10:51.800
in the weapons system or armor.

00:10:51.800 --> 00:10:54.920
And so once you do make
it out of metal, you know,

00:10:54.920 --> 00:10:58.990
the hot glue of the metal
world is MIG welding.

00:10:58.990 --> 00:11:02.030
Once you now are working in
this kind of format of water jet

00:11:02.030 --> 00:11:04.340
to MIG welding, we were
able to build bot chassis

00:11:04.340 --> 00:11:06.340
in literally one day.

00:11:06.340 --> 00:11:08.240
And then the much
slower part comes

00:11:08.240 --> 00:11:11.760
when you have to cram all the
actual robotic stuff in there

00:11:11.760 --> 00:11:14.470
and make it so that it
could perform reasonably.

00:11:14.470 --> 00:11:16.210
Last year, the
robot that we built

00:11:16.210 --> 00:11:19.320
was a launcher-- again,
probably three times

00:11:19.320 --> 00:11:22.860
as powerful as any one we'd ever
built before-- called Bronco.

00:11:22.860 --> 00:11:25.920
And it has 12,000
pounds of lifting force

00:11:25.920 --> 00:11:28.790
in a 250-pound
robot that can exert

00:11:28.790 --> 00:11:31.080
that force in a little less
than 1/100 of a second.

00:11:31.080 --> 00:11:35.120
So it's a very hard robot
on itself, in a sense.

00:11:35.120 --> 00:11:39.960
This is us fighting a
robot called Stinger.

00:11:39.960 --> 00:11:43.702
And this was the quarter
finals of last season.

00:11:43.702 --> 00:11:46.182
[LAUGHTER]

00:11:46.182 --> 00:11:49.158
[APPLAUSE]

00:11:50.150 --> 00:11:52.940
And then in the semifinals,
we got taken out

00:11:52.940 --> 00:11:55.410
by a robot called Tombstone
with a spinning weapon.

00:11:55.410 --> 00:11:57.118
To give you an idea
of the kind of forces

00:11:57.118 --> 00:11:58.969
that are involved here,
that's 3/8 aluminum

00:11:58.969 --> 00:12:00.510
that looks like it's
torn like paper,

00:12:00.510 --> 00:12:02.980
and armor grade 38
tie, just getting

00:12:02.980 --> 00:12:05.220
shredded across this robot.

00:12:05.220 --> 00:12:08.270
So being able to contain the
forces and take those forces

00:12:08.270 --> 00:12:09.520
is really difficult.

00:12:09.520 --> 00:12:11.294
We just finished
filming season 2.

00:12:11.294 --> 00:12:12.710
This is the new
version of Bronco.

00:12:12.710 --> 00:12:14.470
It has six wheels.

00:12:14.470 --> 00:12:16.780
And basically
wheels get torn off,

00:12:16.780 --> 00:12:18.840
so we wanted to have
a robot that that

00:12:18.840 --> 00:12:21.130
after some of the
wheels got torn off

00:12:21.130 --> 00:12:22.780
could still keep running.

00:12:22.780 --> 00:12:27.570
And the new season starts
I think Thursday, June 23.

00:12:27.570 --> 00:12:30.720
And you can see the
evolution of robot combat

00:12:30.720 --> 00:12:32.680
as it comes together now.

00:12:32.680 --> 00:12:33.952
Thank you very much.

00:12:33.952 --> 00:12:37.048
[APPLAUSE]

00:12:40.129 --> 00:12:41.920
BRADY FORREST: Thank
you very much, Zander.

00:12:41.920 --> 00:12:45.460
And now we're going to space
with the author of a new book

00:12:45.460 --> 00:12:46.750
in cooperation with NASA.

00:12:46.750 --> 00:12:48.540
Please welcome up Ariel Waldman.

00:12:48.540 --> 00:12:50.500
[APPLAUSE]

00:12:50.500 --> 00:12:53.440
ARIEL WALDMAN: Thanks.

00:12:53.440 --> 00:12:54.430
Hello.

00:12:54.430 --> 00:12:58.610
So this is an image that's from
my favorite spacecraft, Voyager

00:12:58.610 --> 00:13:01.046
I. And if you can see that
little pale blue speck

00:13:01.046 --> 00:13:02.420
of a pixel there,
that's actually

00:13:02.420 --> 00:13:04.710
earth as seen from about
four billion miles away.

00:13:04.710 --> 00:13:06.720
So that's where you can
find me if you need me.

00:13:06.720 --> 00:13:09.740
But the reason why I love
images like these so much

00:13:09.740 --> 00:13:12.410
is because it really shows us
how space exploration often

00:13:12.410 --> 00:13:15.370
changes our view of ourselves
and our place in the universe.

00:13:15.370 --> 00:13:17.080
But similarly, I
think we should change

00:13:17.080 --> 00:13:18.890
how we view space exploration.

00:13:18.890 --> 00:13:20.860
Most of us are just
observing astronauts

00:13:20.860 --> 00:13:22.430
exploring on behalf of us.

00:13:22.430 --> 00:13:24.752
But we ourselves aren't
doing much exploring.

00:13:24.752 --> 00:13:26.460
And this relates to
my own personal story

00:13:26.460 --> 00:13:28.380
because I was watching
a documentary called

00:13:28.380 --> 00:13:30.799
"When We Left Earth" about
during the early days.

00:13:30.799 --> 00:13:33.090
And I became so incredibly
inspired by this documentary

00:13:33.090 --> 00:13:35.896
that I decided to
send someone at NASA

00:13:35.896 --> 00:13:38.270
an email saying I was a huge
fan of what they were doing.

00:13:38.270 --> 00:13:40.030
And if they ever
needed someone like me,

00:13:40.030 --> 00:13:42.071
someone completely without
a science background--

00:13:42.071 --> 00:13:43.190
I went to art school.

00:13:43.190 --> 00:13:44.260
I was here.

00:13:44.260 --> 00:13:46.900
Serendipitously, I got a
job at NASA from that email,

00:13:46.900 --> 00:13:49.096
and it was a total
fan girl moment.

00:13:49.096 --> 00:13:51.733
[APPLAUSE]

00:13:52.620 --> 00:13:54.390
And so I wanted to
share a quick clip

00:13:54.390 --> 00:13:56.743
from this documentary about
why I found it so inspiring.

00:13:56.743 --> 00:13:57.870
[VIDEO PLAYBACK]

00:13:57.870 --> 00:13:59.210
-Knew nothing about spacecraft.

00:13:59.210 --> 00:14:00.480
We knew nothing about orbits.

00:14:00.480 --> 00:14:03.980
-And I saw a lot of
rockets launched.

00:14:03.980 --> 00:14:09.170
I'd say somewhere between
30% and 40% of them failed.

00:14:09.170 --> 00:14:10.505
[EXPLOSION]

00:14:10.505 --> 00:14:11.400
[END VIDEO PLAYBACK]

00:14:11.400 --> 00:14:12.920
ARIEL WALDMAN: And that's
what I love so much

00:14:12.920 --> 00:14:15.280
about this documentary,
because as they were saying,

00:14:15.280 --> 00:14:17.090
they don't know anything
about space crafts or rockets.

00:14:17.090 --> 00:14:17.980
They were figuring it out.

00:14:17.980 --> 00:14:19.680
And I thought to myself,
I don't know anything

00:14:19.680 --> 00:14:20.720
about spacecraft or rockets.

00:14:20.720 --> 00:14:21.678
I want to work at NASA.

00:14:21.678 --> 00:14:23.800
That sounds amazing.

00:14:23.800 --> 00:14:26.270
So I got that chance, and it
completely changed my life.

00:14:26.270 --> 00:14:28.460
And I got to learn about
all sorts of things--

00:14:28.460 --> 00:14:31.900
dark matter and robots and what
our galaxy really looks like.

00:14:31.900 --> 00:14:33.950
It looks like something
like this, actually.

00:14:33.950 --> 00:14:36.510
And one of the coolest
things I got to learn about

00:14:36.510 --> 00:14:38.120
were black holes.

00:14:38.120 --> 00:14:40.820
And black holes are really
interesting because they're

00:14:40.820 --> 00:14:44.450
these incredibly uncharted
territories of space.

00:14:44.450 --> 00:14:46.200
They're something
that has been science

00:14:46.200 --> 00:14:48.960
fiction for a long time, but
now they're actually science.

00:14:48.960 --> 00:14:51.430
And they're just
fascinating, but they also

00:14:51.430 --> 00:14:53.480
get a bit of a bad rap.

00:14:53.480 --> 00:14:55.730
People put all their
wonders and hopes

00:14:55.730 --> 00:14:57.350
and dreams into black holes.

00:14:57.350 --> 00:14:59.770
They wonder all sorts of
things about, you know,

00:14:59.770 --> 00:15:01.750
what if we live inside
of a black hole?

00:15:01.750 --> 00:15:03.310
Or what are white holes?

00:15:03.310 --> 00:15:06.970
Or they wonder what if we were
to fall through a black hole?

00:15:06.970 --> 00:15:10.760
Would we actually have to spend
time with Matthew McConaughey?

00:15:10.760 --> 00:15:12.822
I really, really hope not.

00:15:12.822 --> 00:15:14.160
[LAUGHS]

00:15:14.160 --> 00:15:15.889
But black holes are
incredibly evil.

00:15:15.889 --> 00:15:17.680
They're seen as evil
things because they're

00:15:17.680 --> 00:15:20.220
known for gobbling up planets
and stars and maybe even

00:15:20.220 --> 00:15:21.210
entire galaxies.

00:15:21.210 --> 00:15:22.280
And do they ever stop?

00:15:22.280 --> 00:15:24.400
Do they ever get tired
of eating things?

00:15:24.400 --> 00:15:26.420
Well, it turns
out, they sometimes

00:15:26.420 --> 00:15:27.860
do get tired of eating things.

00:15:27.860 --> 00:15:29.910
When they bite off more
than they can chew,

00:15:29.910 --> 00:15:32.300
they actually vomit it
back out into the universe

00:15:32.300 --> 00:15:35.310
violently in the form of
mixed up space dust and space

00:15:35.310 --> 00:15:36.560
gas and space energy.

00:15:36.560 --> 00:15:38.060
And that mixed up
stuff can actually

00:15:38.060 --> 00:15:41.260
go on to create future
stars, future planets.

00:15:41.260 --> 00:15:44.220
So in a sense, they're kind of
the creators of the universe.

00:15:44.220 --> 00:15:46.620
And we can't image
a black hole, but we

00:15:46.620 --> 00:15:50.730
can image a black hole's vomit,
which you can see right here.

00:15:50.730 --> 00:15:53.670
And imaging black
vomit is really great

00:15:53.670 --> 00:15:55.790
because it allows us to
find new black holes.

00:15:55.790 --> 00:15:58.810
And there's actually a project
called Galaxy Zoo Radio

00:15:58.810 --> 00:16:01.170
on spacehack.org, which
is a directory of ways

00:16:01.170 --> 00:16:03.990
to participate in space
exploration that I had created.

00:16:03.990 --> 00:16:05.800
And you can actually
find black holes

00:16:05.800 --> 00:16:07.970
by combing through their vomit.

00:16:07.970 --> 00:16:10.890
This is something that should
be science fiction, anyone being

00:16:10.890 --> 00:16:12.120
able to discover black holes.

00:16:12.120 --> 00:16:13.700
But it is actually science.

00:16:13.700 --> 00:16:15.640
And turning science
fiction into science

00:16:15.640 --> 00:16:18.945
is at the heart of NASA
innovative advanced concepts.

00:16:18.945 --> 00:16:20.320
It's a program
that I help advise

00:16:20.320 --> 00:16:23.750
at NASA that funds the more
sci-fi, futuristic, out there,

00:16:23.750 --> 00:16:27.740
weird stuff that could
transform future space missions.

00:16:27.740 --> 00:16:29.690
So the types of
stuff that they fund

00:16:29.690 --> 00:16:32.180
are things like the
Comet Hitchhiker.

00:16:32.180 --> 00:16:34.130
And the Comet
Hitchhiker is literally

00:16:34.130 --> 00:16:37.690
a project where a probe can
go to a comet, tether into it,

00:16:37.690 --> 00:16:40.090
and harvest its kinetic
energy to explore

00:16:40.090 --> 00:16:44.150
the solar system twice as fast
as any of our current probes.

00:16:44.150 --> 00:16:46.910
There's also projects like
the Solar Lunar Rover, which

00:16:46.910 --> 00:16:50.060
essentially uses computer
vision to route better

00:16:50.060 --> 00:16:52.330
routes on the moon for
rovers so that they

00:16:52.330 --> 00:16:55.300
can stay in continuous
sunlight and actually perform

00:16:55.300 --> 00:16:59.030
better science, and they don't
have to turn off overnight.

00:16:59.030 --> 00:17:01.450
There's also projects like
the Titan Submarine, which

00:17:01.450 --> 00:17:04.660
is literally a concept
to build a submarine,

00:17:04.660 --> 00:17:06.220
send it to a moon
of Saturn, and have

00:17:06.220 --> 00:17:09.260
it explore the lakes of
methane and ethane there and do

00:17:09.260 --> 00:17:10.980
a bunch of cool stuff
while it's there.

00:17:10.980 --> 00:17:14.940
Having a submarine on a moon
around Saturn is amazing to me.

00:17:14.940 --> 00:17:17.210
And then there's projects
that are actually

00:17:17.210 --> 00:17:20.750
about using technology from
MIT Media Lab that can actually

00:17:20.750 --> 00:17:23.650
see behind corners, and
strapping it to a space probe,

00:17:23.650 --> 00:17:26.500
and allowing it to actually be
able to see the inside lunar

00:17:26.500 --> 00:17:29.370
caves, which is
really, really cool.

00:17:29.370 --> 00:17:32.230
So the coolest thing
about NIAC is the fact

00:17:32.230 --> 00:17:35.010
that they accept
proposals from anyone.

00:17:35.010 --> 00:17:39.160
Anyone can apply every year
and actually submit a proposal,

00:17:39.160 --> 00:17:42.620
get it funded, by NASA,
and actually perform

00:17:42.620 --> 00:17:43.800
science fiction.

00:17:43.800 --> 00:17:46.820
So to me, when it comes to space
exploration and black holes

00:17:46.820 --> 00:17:49.440
and all of this stuff, the
thing that's really cool

00:17:49.440 --> 00:17:51.620
is about spitting
out, vomiting out,

00:17:51.620 --> 00:17:53.990
awesome stuff into
the universe to go on

00:17:53.990 --> 00:17:55.530
to transform the future.

00:17:55.530 --> 00:17:56.261
Thank you.

00:17:56.261 --> 00:17:58.907
[APPLAUSE]

00:18:04.950 --> 00:18:06.470
BRADY FORREST: Thank you, Ariel.

00:18:06.470 --> 00:18:09.120
And her book is entitled
"What's It Like in Space?"

00:18:09.120 --> 00:18:10.850
It's pretty awesome.

00:18:10.850 --> 00:18:13.260
And now to learn about
users, please welcome up

00:18:13.260 --> 00:18:15.690
US researcher
Elizabeth Churchill.

00:18:15.690 --> 00:18:18.012
[APPLAUSE]

00:18:20.352 --> 00:18:23.560
ELIZABETH CHURCHILL: Hi, so I've
been studying human computer

00:18:23.560 --> 00:18:25.310
interaction for a long time.

00:18:25.310 --> 00:18:27.000
And more recently,
I'm turning my eye

00:18:27.000 --> 00:18:28.750
to human data interaction.

00:18:28.750 --> 00:18:31.530
And when I say data, I mean
all of the data about you

00:18:31.530 --> 00:18:35.140
as a consumer that is collected
from devices and services, all

00:18:35.140 --> 00:18:38.860
the data that you're collecting
as developers from people.

00:18:38.860 --> 00:18:40.900
And if you take one thing
away from this talk,

00:18:40.900 --> 00:18:43.680
I want you to just
ask yourselves, why?

00:18:43.680 --> 00:18:45.680
Why is the data being collected?

00:18:45.680 --> 00:18:47.360
Why are you collecting the data?

00:18:47.360 --> 00:18:49.105
For what purpose?

00:18:49.105 --> 00:18:50.480
Because I think
one of the things

00:18:50.480 --> 00:18:53.910
we need as data scientists is
to be clear about the data we're

00:18:53.910 --> 00:18:54.660
collecting.

00:18:54.660 --> 00:18:57.740
Now, I've worked with data
scientists and designers

00:18:57.740 --> 00:19:01.890
to build recommendation
algorithms for merchandising,

00:19:01.890 --> 00:19:03.010
quite positive.

00:19:03.010 --> 00:19:06.340
I'm now working with the
privacy and safety experts

00:19:06.340 --> 00:19:09.310
to worry about some of
the concerns people have.

00:19:09.310 --> 00:19:12.030
And the piece down the
middle is visibility.

00:19:12.030 --> 00:19:14.970
And so even in corporations,
we don't always

00:19:14.970 --> 00:19:17.720
have visibility into the
reasons why we collect data.

00:19:17.720 --> 00:19:22.260
And consumers certainly don't,
and that causes anxiety.

00:19:22.260 --> 00:19:24.920
And when you talk to people
about data collection,

00:19:24.920 --> 00:19:27.360
they often think
about surveillance.

00:19:27.360 --> 00:19:30.070
Now, surveillance
evokes the idea

00:19:30.070 --> 00:19:33.470
of the collection of data
for a particular purpose.

00:19:33.470 --> 00:19:37.100
And as we move into this new
world, though, of the internet

00:19:37.100 --> 00:19:39.980
of things and smart
cities and smart homes,

00:19:39.980 --> 00:19:42.400
a lot of sensor data
is being collected

00:19:42.400 --> 00:19:44.950
with no particular
purpose in mind.

00:19:44.950 --> 00:19:46.430
Back to the question, why?

00:19:46.430 --> 00:19:48.550
Because we can,
just because we can,

00:19:48.550 --> 00:19:51.190
because it might be useful
perhaps, because we haven't

00:19:51.190 --> 00:19:53.430
thought about not doing it.

00:19:53.430 --> 00:19:56.630
Now, data science
is in its infancy,

00:19:56.630 --> 00:19:58.060
and it's a fetish object.

00:19:58.060 --> 00:19:59.230
It's a fetish topic.

00:19:59.230 --> 00:20:01.420
Everyone loves data science.

00:20:01.420 --> 00:20:03.880
But we're at the
beginning of understanding

00:20:03.880 --> 00:20:06.620
what we should collect and
what we're not collecting,

00:20:06.620 --> 00:20:08.730
as well as what
we are collecting.

00:20:08.730 --> 00:20:12.210
So it's not surprising that as
consumers and those concerned

00:20:12.210 --> 00:20:16.290
about data, sousveillance,
or the watched

00:20:16.290 --> 00:20:19.780
watching the watchers, is
becoming increasingly something

00:20:19.780 --> 00:20:20.980
we're interested in.

00:20:20.980 --> 00:20:23.699
We're asking
questions about why.

00:20:23.699 --> 00:20:25.240
Now, I've done a
series of interviews

00:20:25.240 --> 00:20:28.340
with people about their
concerns about data.

00:20:28.340 --> 00:20:31.190
And it isn't black or
white, unlike my slides.

00:20:31.190 --> 00:20:34.240
It's about wanting a
contract and a conversation.

00:20:34.240 --> 00:20:36.380
It's about wanting
to participate,

00:20:36.380 --> 00:20:38.400
wanting visibility.

00:20:38.400 --> 00:20:42.110
Now, a friend of mine, when
she found out she was pregnant,

00:20:42.110 --> 00:20:45.840
decided that she did not want to
be tracked about her pregnancy.

00:20:45.840 --> 00:20:48.110
So she used incognito
mode for searches.

00:20:48.110 --> 00:20:49.530
She bought nothing online.

00:20:49.530 --> 00:20:52.810
She didn't talk on social
media about her pregnancy.

00:20:52.810 --> 00:20:55.070
She took efforts to
opt out, including

00:20:55.070 --> 00:21:00.330
paying for large objects like
prams with cash and gift cards

00:21:00.330 --> 00:21:02.180
rather than credit cards.

00:21:02.180 --> 00:21:05.180
Now, nothing wrong
with that, but what

00:21:05.180 --> 00:21:08.090
was wrong with the
model in the data model?

00:21:08.090 --> 00:21:10.920
It flagged her as a
potential criminal.

00:21:10.920 --> 00:21:13.880
It flagged her as
fraudulent, potentially.

00:21:13.880 --> 00:21:16.080
So she had to unravel that.

00:21:16.080 --> 00:21:19.750
Data science is in its infancy,
and our predictive models

00:21:19.750 --> 00:21:24.460
about what people's intent or
actions are not always right.

00:21:24.460 --> 00:21:26.300
So what we're doing
if we're not careful

00:21:26.300 --> 00:21:29.250
is creating a climate
of lack of trust.

00:21:29.250 --> 00:21:31.220
And what that means is
people may potentially

00:21:31.220 --> 00:21:34.440
walk away from our
services and devices

00:21:34.440 --> 00:21:38.050
because we're not including them
in thinking hard about this.

00:21:38.050 --> 00:21:41.130
And so again, ask why
you're collecting data.

00:21:41.130 --> 00:21:43.050
Ask what models you're building.

00:21:43.050 --> 00:21:45.120
Ask what data you're
not collecting in order

00:21:45.120 --> 00:21:47.650
to build a better model,
a more rounded model.

00:21:47.650 --> 00:21:50.530
And think about why
not participation.

00:21:50.530 --> 00:21:53.000
Now, I mentioned to
my friend, and one

00:21:53.000 --> 00:21:55.660
of the big issues I think
we need to think about

00:21:55.660 --> 00:21:58.760
is meaningful opt
in and opt out.

00:21:58.760 --> 00:22:00.960
What does it mean
to let people choose

00:22:00.960 --> 00:22:04.320
to opt out of data collection?

00:22:04.320 --> 00:22:07.220
In the EU, this is becoming
increasingly a big issue,

00:22:07.220 --> 00:22:11.210
and a regulation is about to
pass, also about deletion,

00:22:11.210 --> 00:22:13.040
the right to be forgotten.

00:22:13.040 --> 00:22:14.910
Data science is in its infancy.

00:22:14.910 --> 00:22:18.630
We don't yet understand how
the statistical models we buy

00:22:18.630 --> 00:22:21.960
will cope with the
removal of data.

00:22:21.960 --> 00:22:25.460
So deletion is a really
interesting technical problem.

00:22:25.460 --> 00:22:27.570
I've mentioned forecasting.

00:22:27.570 --> 00:22:29.650
Intent modeling and
forecast modeling

00:22:29.650 --> 00:22:33.640
is clearly beneficial in
many, many, many instances.

00:22:33.640 --> 00:22:37.140
But we have to not just
take the forecasts as given.

00:22:37.140 --> 00:22:39.730
We have to ask why
and what, and we

00:22:39.730 --> 00:22:44.450
have to come with the primary
premise as building trust

00:22:44.450 --> 00:22:48.930
and collaboration with people
of whose data we're collecting.

00:22:48.930 --> 00:22:50.820
We should start
with models of trust

00:22:50.820 --> 00:22:54.530
first, and then think about
where we're going forward.

00:22:54.530 --> 00:22:56.260
So I just want to
sum up the things

00:22:56.260 --> 00:23:00.220
I've been thinking about--
asking why; meaningful opt in,

00:23:00.220 --> 00:23:01.140
opt out.

00:23:01.140 --> 00:23:02.650
What does it mean to delete?

00:23:02.650 --> 00:23:04.660
What are the forecasts
models of the future?

00:23:04.660 --> 00:23:06.620
And how can we forecast
what data we're

00:23:06.620 --> 00:23:08.560
collecting that's artefactual?

00:23:08.560 --> 00:23:11.410
How will it be used
and build on trust?

00:23:11.410 --> 00:23:13.960
So I want to think
about you talking

00:23:13.960 --> 00:23:15.870
to me about human
data interaction

00:23:15.870 --> 00:23:17.230
from your perspective.

00:23:17.230 --> 00:23:18.140
Thank you.

00:23:18.140 --> 00:23:21.000
[APPLAUSE]

00:23:21.500 --> 00:23:22.541
BRADY FORREST: Well done.

00:23:24.870 --> 00:23:26.180
Thank you, Elizabeth.

00:23:26.180 --> 00:23:27.680
And now we're going
to find out what

00:23:27.680 --> 00:23:30.330
happens when a nerd decides
to learn about sports.

00:23:30.330 --> 00:23:31.854
For his walking up Toby Segaran.

00:23:34.460 --> 00:23:37.010
TOBY SEGARAN: Hi, I'm Toby.

00:23:37.010 --> 00:23:38.615
I really, really like gambling.

00:23:42.984 --> 00:23:44.400
And fan fantasy
sports is actually

00:23:44.400 --> 00:23:47.220
one of the only legal
forms of gambling

00:23:47.220 --> 00:23:48.840
that you can do
online in the US.

00:23:48.840 --> 00:23:53.340
And this is my story
of the 2016 NFL season.

00:23:53.340 --> 00:23:56.820
I started noticing these
articles just appearing

00:23:56.820 --> 00:24:00.300
on feeds and stuff like that
about how these sites were

00:24:00.300 --> 00:24:03.630
ripping off sports fans because
there are too many sharks.

00:24:03.630 --> 00:24:05.710
And I thought to
myself, well, maybe

00:24:05.710 --> 00:24:12.080
I'm a shark This involves
two things I know a lot

00:24:12.080 --> 00:24:15.850
about, gambling and algorithms,
and one thing I knew nothing

00:24:15.850 --> 00:24:17.462
about, which was sports.

00:24:17.462 --> 00:24:19.170
But I figured two out
of three isn't bad,

00:24:19.170 --> 00:24:21.200
and maybe I should
give this a shot.

00:24:21.200 --> 00:24:24.300
So I'll talk you through
how fantasy football works.

00:24:24.300 --> 00:24:26.202
You can see you
come to these sites,

00:24:26.202 --> 00:24:27.660
and there are lots
of competitions.

00:24:27.660 --> 00:24:32.030
There are different buy-ins,
different prizes you can get.

00:24:32.030 --> 00:24:34.990
And then you can click through
to one of the actual contests

00:24:34.990 --> 00:24:41.150
and you can see that there
are different payouts.

00:24:41.150 --> 00:24:46.380
Most notably is that
500 people can enter,

00:24:46.380 --> 00:24:47.825
but only 100 people win here.

00:24:47.825 --> 00:24:49.950
And some of the top prizes
are actually quite high.

00:24:49.950 --> 00:24:54.000
You're only paying $27, and
you could win over $1,000.

00:24:54.000 --> 00:24:55.770
And to actually
set up the teams,

00:24:55.770 --> 00:24:59.160
you buy a bunch of real life
players, and you have a budget.

00:24:59.160 --> 00:25:01.050
So you can't just pick
all the best ones.

00:25:01.050 --> 00:25:02.790
And you get points
based on how well

00:25:02.790 --> 00:25:05.020
they play in the Sunday games.

00:25:05.020 --> 00:25:08.560
So this is how you
set up the teams.

00:25:08.560 --> 00:25:10.670
And this is everything
I knew about football

00:25:10.670 --> 00:25:12.820
before I started.

00:25:12.820 --> 00:25:18.560
[LAUGHS] Matt Saracen
and Moxon, they

00:25:18.560 --> 00:25:21.590
were the main guy, the ones
that make the movies about it.

00:25:21.590 --> 00:25:22.950
So there's a quarterback.

00:25:22.950 --> 00:25:25.400
And it turns out there
are other players too.

00:25:25.400 --> 00:25:27.480
This is Antonio Brown.

00:25:27.480 --> 00:25:28.770
He's a wide receiver.

00:25:28.770 --> 00:25:30.520
This is the guy the
quarterback throws to.

00:25:30.520 --> 00:25:33.570
And this will become
important later.

00:25:33.570 --> 00:25:36.230
They can get you a lot of
fantasy points as well.

00:25:36.230 --> 00:25:39.700
It also turns out there's
an entire other team called

00:25:39.700 --> 00:25:40.960
the defense.

00:25:40.960 --> 00:25:44.727
And the defense plays
when the quarterbacks

00:25:44.727 --> 00:25:45.810
playing on the other team.

00:25:45.810 --> 00:25:47.370
So the quarterbacks
never even played

00:25:47.370 --> 00:25:50.700
each other, which is, I guess,
why the movies are always

00:25:50.700 --> 00:25:55.170
plotted around one quarterback
and not so much a rivalry.

00:25:55.170 --> 00:25:57.077
OK, so here's why
this is difficult.

00:25:57.077 --> 00:25:59.660
When you construct a team, there
are always different choices.

00:25:59.660 --> 00:26:03.256
And basically there are three
quadrillion possible teams

00:26:03.256 --> 00:26:05.380
once you figure out all
the combinations of players

00:26:05.380 --> 00:26:06.088
you could put in.

00:26:06.088 --> 00:26:07.590
So that's far too
many for a human

00:26:07.590 --> 00:26:10.820
to consider and even far too
many for a machine to consider.

00:26:10.820 --> 00:26:14.970
The strategy also is that
you have to basically play

00:26:14.970 --> 00:26:15.910
really risky teams.

00:26:15.910 --> 00:26:18.500
Because being just slightly
above average doesn't get you

00:26:18.500 --> 00:26:20.790
any prizes in these
tournament contests.

00:26:20.790 --> 00:26:22.750
They have different ones,
but that's less fun.

00:26:22.750 --> 00:26:25.890
And I figured this would
be the way to play.

00:26:25.890 --> 00:26:27.360
So what I figured
out was I needed

00:26:27.360 --> 00:26:29.710
to enter many teams
per week, figuring out

00:26:29.710 --> 00:26:32.480
the best players, what
the scores would likely

00:26:32.480 --> 00:26:35.830
to be, and construct
a team out of those,

00:26:35.830 --> 00:26:37.850
and then actually construct
a portfolio of teams

00:26:37.850 --> 00:26:41.220
that have a chance of
being right near the top.

00:26:41.220 --> 00:26:44.037
So I did a bunch of
modeling, but it turns out

00:26:44.037 --> 00:26:45.620
that if you just
average the projected

00:26:45.620 --> 00:26:47.646
scores that the
sports sites give you,

00:26:47.646 --> 00:26:49.770
those are actually a fairly
good proxy for how well

00:26:49.770 --> 00:26:51.650
the players are going to do.

00:26:51.650 --> 00:26:54.012
But what's interesting
is that even though this

00:26:54.012 --> 00:26:55.720
might be the average
score that they get,

00:26:55.720 --> 00:26:58.230
the distributions of the
scores are quite different.

00:26:58.230 --> 00:27:00.220
The quarterback is
much more consistent

00:27:00.220 --> 00:27:01.970
than the wide receiver.

00:27:01.970 --> 00:27:04.920
So in order to have these high
variance teams, these really

00:27:04.920 --> 00:27:07.790
risky teams, you want to
think about the distributions

00:27:07.790 --> 00:27:09.702
that you're playing with.

00:27:09.702 --> 00:27:11.910
The other thing is that the
players' scores correlate

00:27:11.910 --> 00:27:12.576
with each other.

00:27:12.576 --> 00:27:15.031
Because the quarterback
passes to the wide receiver,

00:27:15.031 --> 00:27:16.530
they actually have
high correlations

00:27:16.530 --> 00:27:17.800
between their scores.

00:27:17.800 --> 00:27:19.640
Essentially, if the
quarterback does well,

00:27:19.640 --> 00:27:21.250
the wide receiver
tends to do well.

00:27:21.250 --> 00:27:25.530
So having two on the same
team-- these two players

00:27:25.530 --> 00:27:29.270
from the same real life team--
helps increase your variance.

00:27:29.270 --> 00:27:31.310
So the idea would be to
create a set of teams

00:27:31.310 --> 00:27:33.840
with really high variance
who are going to score really

00:27:33.840 --> 00:27:36.560
well or really poorly and
that have low correlation

00:27:36.560 --> 00:27:38.120
with each other.

00:27:38.120 --> 00:27:41.240
And of course you still
want them to score well.

00:27:41.240 --> 00:27:45.030
And so the way I did this
was I created a set of teams

00:27:45.030 --> 00:27:48.900
at random and then
sort of predict

00:27:48.900 --> 00:27:52.800
the outcomes of this portfolio
based on probabilities of what

00:27:52.800 --> 00:27:54.360
those score, given
the distributions,

00:27:54.360 --> 00:27:57.450
given their correlations
with each other.

00:27:57.450 --> 00:28:00.870
So you can see here that I
can calculate the probability

00:28:00.870 --> 00:28:05.200
that one of my teams will score
above 140, the probability

00:28:05.200 --> 00:28:07.350
that one will score
above 160-- 160

00:28:07.350 --> 00:28:11.280
was a good metric for being
in that top green spot--

00:28:11.280 --> 00:28:14.140
and the correlations
between them.

00:28:14.140 --> 00:28:17.640
And then we optimize by
changing out the players.

00:28:17.640 --> 00:28:20.000
And maybe you can increase
the expected values

00:28:20.000 --> 00:28:23.120
and the variance of
each team while also

00:28:23.120 --> 00:28:24.586
decreasing the correlation.

00:28:24.586 --> 00:28:25.960
We just keep doing
this randomly,

00:28:25.960 --> 00:28:28.600
and we can see the probability's
getting higher and higher.

00:28:28.600 --> 00:28:30.640
So here are my final results.

00:28:30.640 --> 00:28:35.270
For the 2015 season,
expectation is negative 10%

00:28:35.270 --> 00:28:36.430
because they take a cut.

00:28:36.430 --> 00:28:41.670
My rate of return was 296%,
but only eight of my teams

00:28:41.670 --> 00:28:42.760
actually won.

00:28:42.760 --> 00:28:43.608
Yep.

00:28:43.608 --> 00:28:44.604
[LAUGHS]

00:28:44.604 --> 00:28:47.512
[APPLAUSE]

00:28:51.295 --> 00:28:52.670
BRADY FORREST: So
how many people

00:28:52.670 --> 00:28:56.600
here have read "Programming
Collective Intelligence"?

00:28:56.600 --> 00:28:57.120
Really?

00:28:57.120 --> 00:28:58.080
Nobody?

00:28:58.080 --> 00:28:59.590
Best selling data science book?

00:28:59.590 --> 00:29:02.120
Anyway, that's what happens
when the author decides

00:29:02.120 --> 00:29:04.990
to learn how to play football.

00:29:04.990 --> 00:29:08.100
Now we have a Double E star.

00:29:08.100 --> 00:29:10.170
Please welcome up Star Simpson.

00:29:10.170 --> 00:29:12.816
[APPLAUSE]

00:29:17.780 --> 00:29:20.510
STAR SIMPSON: So I got into
electronics almost completely

00:29:20.510 --> 00:29:21.380
by accident.

00:29:21.380 --> 00:29:22.350
And thank you, Brady.

00:29:22.350 --> 00:29:24.680
My name's Star Simpson, and
I'm a hardware designer.

00:29:28.050 --> 00:29:31.010
I got into electronics when
I was 14 years old because I

00:29:31.010 --> 00:29:32.640
found this book.

00:29:32.640 --> 00:29:34.690
And I found this book
in this old computer lab

00:29:34.690 --> 00:29:36.470
at my high school that
had been abandoned.

00:29:36.470 --> 00:29:37.890
It was locked most of the time.

00:29:37.890 --> 00:29:40.500
For some reason I was in
it, and I found a book.

00:29:40.500 --> 00:29:43.090
And it was so important to me
I knew I needed to have it.

00:29:43.090 --> 00:29:45.710
So I promised I was going to
take it home for the summer

00:29:45.710 --> 00:29:48.550
without asking.

00:29:48.550 --> 00:29:50.736
I took it home, and I
looked at all the parts

00:29:50.736 --> 00:29:53.110
I needed to build every project
in it and went to my mom.

00:29:53.110 --> 00:29:55.193
And I was like, Mom, I
need all these electronics.

00:29:55.193 --> 00:29:57.780
And it was a lot of
money, but she did it.

00:29:57.780 --> 00:30:00.290
And she ended up getting
me all the parts.

00:30:00.290 --> 00:30:03.330
I spent the whole summer
building every project in it.

00:30:03.330 --> 00:30:06.830
And I realize now she got a
bargain, honestly, entertaining

00:30:06.830 --> 00:30:09.645
me for three whole months.

00:30:09.645 --> 00:30:12.760
And I went, and I finished
every project in that book,

00:30:12.760 --> 00:30:13.640
cover to cover.

00:30:13.640 --> 00:30:18.620
I left no project unbuilt,
one after another.

00:30:18.620 --> 00:30:20.270
It was amazing.

00:30:20.270 --> 00:30:21.314
I learned so much.

00:30:21.314 --> 00:30:22.480
I built all of these things.

00:30:22.480 --> 00:30:25.130
I built a circuit that had
a game where you touch it,

00:30:25.130 --> 00:30:27.110
and like you won or lost.

00:30:27.110 --> 00:30:29.220
I built something that
lets you count numbers

00:30:29.220 --> 00:30:30.400
on a little digital display.

00:30:30.400 --> 00:30:31.942
I mean, I thought
it was really cool.

00:30:31.942 --> 00:30:33.316
I built something
that remembered

00:30:33.316 --> 00:30:34.630
which state it should be in.

00:30:34.630 --> 00:30:36.410
And I felt like I
gained this super power.

00:30:36.410 --> 00:30:39.300
It was awesome.

00:30:39.300 --> 00:30:40.870
That's me gaining a super power.

00:30:40.870 --> 00:30:43.481
I had shorter hair then.

00:30:43.481 --> 00:30:44.980
The thing that I
thought was so cool

00:30:44.980 --> 00:30:49.850
about it was that you learned
to connect things that are,

00:30:49.850 --> 00:30:52.160
you know, math, to
how it's described,

00:30:52.160 --> 00:30:54.440
to how it exists in
the physical world.

00:30:54.440 --> 00:30:57.800
And the thing that's
great to me especially

00:30:57.800 --> 00:30:59.987
is I'm not the only one to
have had that experience.

00:30:59.987 --> 00:31:02.570
Millions of people have actually
used this same book to learn,

00:31:02.570 --> 00:31:04.070
and I know engineers
today who still

00:31:04.070 --> 00:31:06.900
think in terms of the pictures
that were in the book.

00:31:06.900 --> 00:31:10.210
I ended up going back to
school, my high school.

00:31:10.210 --> 00:31:11.750
I joined the robotics team.

00:31:11.750 --> 00:31:13.380
I ended up becoming
president of it.

00:31:13.380 --> 00:31:14.249
I got into MIT.

00:31:14.249 --> 00:31:16.040
I ended up studying
electrical engineering,

00:31:16.040 --> 00:31:17.706
getting a job as an
electrical engineer,

00:31:17.706 --> 00:31:21.390
and I think it's amazing
what a little nudge can do.

00:31:21.390 --> 00:31:23.830
But I think today's world
is different than the one I

00:31:23.830 --> 00:31:24.370
learned in.

00:31:24.370 --> 00:31:27.310
So this is all of the Radio
Shacks that are now closed.

00:31:27.310 --> 00:31:30.870
It's basically a map
of the United States.

00:31:30.870 --> 00:31:32.830
Fortunately, we
have the internet.

00:31:32.830 --> 00:31:35.420
That's fantastic, right?

00:31:35.420 --> 00:31:37.872
But I think there's something
to the physical experience

00:31:37.872 --> 00:31:39.580
of building electronics
that's different.

00:31:39.580 --> 00:31:40.600
It's tangible.

00:31:40.600 --> 00:31:42.950
When you poke at something,
and you can debug it,

00:31:42.950 --> 00:31:45.660
and you can see where
your mistake is,

00:31:45.660 --> 00:31:49.590
that's so, so
important, so tangible.

00:31:49.590 --> 00:31:51.490
So I found that book again.

00:31:51.490 --> 00:31:53.780
It's called "Getting
Started in Electronics."

00:31:53.780 --> 00:31:57.810
And, you know, the whole
thing is drawn by hand.

00:31:57.810 --> 00:32:00.330
The idea is that you
found someone's notebook,

00:32:00.330 --> 00:32:05.279
and it just happens to be
about learning electronics.

00:32:05.279 --> 00:32:07.570
I forgot about it for years,
and I just found it again.

00:32:07.570 --> 00:32:11.500
I put it back, by the way,
at the end of the summer.

00:32:11.500 --> 00:32:14.620
And it talks you through how
to understand electronics

00:32:14.620 --> 00:32:16.560
as a language.

00:32:16.560 --> 00:32:19.240
So let me talk a little
bit about how that works.

00:32:19.240 --> 00:32:22.520
Electronics are described
in terms of these pictures.

00:32:22.520 --> 00:32:26.940
So this is sort of like the
floor plan of a circuit.

00:32:26.940 --> 00:32:28.944
So each of those is
a meaningful symbol.

00:32:28.944 --> 00:32:29.610
It's part of it.

00:32:33.170 --> 00:32:35.660
So once you have
that floor plan,

00:32:35.660 --> 00:32:38.000
you can understand
how it works, and you

00:32:38.000 --> 00:32:40.850
can figure out how to build
the physical thing from it.

00:32:40.850 --> 00:32:45.450
So in that sense to me, it's a
lot like learning how to read

00:32:45.450 --> 00:32:47.580
and learning how the physical
arrangement of things

00:32:47.580 --> 00:32:49.871
is meaningful.

00:32:49.871 --> 00:32:52.370
So once you can understand a
schematic, which the book works

00:32:52.370 --> 00:32:55.840
you through, you can go on to
build the physical hardware

00:32:55.840 --> 00:32:56.990
based on it.

00:32:56.990 --> 00:32:58.410
Or you can even go in reverse.

00:32:58.410 --> 00:33:00.430
And this was a major
insight for me,

00:33:00.430 --> 00:33:02.900
was learning you could
look at a piece of hardware

00:33:02.900 --> 00:33:04.900
and work out how
it goes together.

00:33:04.900 --> 00:33:07.920
In fact, this is a picture
to me of an insight that

00:33:07.920 --> 00:33:09.960
took a lot of time to build up.

00:33:09.960 --> 00:33:11.680
This was a thing
you do, is you could

00:33:11.680 --> 00:33:13.870
hold the piece of
hardware up to a light.

00:33:13.870 --> 00:33:16.310
And you could see how
everything was connected.

00:33:16.310 --> 00:33:18.000
And if you're
really patient, you

00:33:18.000 --> 00:33:20.390
can write down all
of the connections

00:33:20.390 --> 00:33:22.620
and understand how it works.

00:33:22.620 --> 00:33:25.550
But so, you know, schematics
are usually these symbols.

00:33:25.550 --> 00:33:27.420
They're usually the
same sort of components.

00:33:27.420 --> 00:33:28.720
You see them everywhere.

00:33:28.720 --> 00:33:31.680
And once you can figure out
how they all go together,

00:33:31.680 --> 00:33:34.130
you can decide what
you want them to do.

00:33:34.130 --> 00:33:36.360
And I started thinking
about how to make

00:33:36.360 --> 00:33:40.470
the piece of hardware that
included all of those insights.

00:33:40.470 --> 00:33:43.030
So you can see here, instead
of holding it up to a light,

00:33:43.030 --> 00:33:44.990
I've designed a circuit
board that shows

00:33:44.990 --> 00:33:46.600
how everything's connected.

00:33:46.600 --> 00:33:48.110
It has the schematic.

00:33:48.110 --> 00:33:50.720
It includes the language
that describes itself.

00:33:50.720 --> 00:33:52.460
This is the backside
of the same board.

00:33:52.460 --> 00:33:54.559
You can see a
description telling you

00:33:54.559 --> 00:33:56.850
how it's supposed to work
and what it's supposed to do.

00:33:56.850 --> 00:33:59.370
Because I want everyone to be
able to have the experience

00:33:59.370 --> 00:34:03.090
I did when I was first
learning electronics,

00:34:03.090 --> 00:34:05.030
to fill the gap left
behind by Radio Shack.

00:34:05.030 --> 00:34:06.770
They are called
Circuit Classics.

00:34:06.770 --> 00:34:07.912
Thank you.

00:34:07.912 --> 00:34:10.227
[APPLAUSE]

00:34:15.815 --> 00:34:17.190
BRADY FORREST:
And as it happens,

00:34:17.190 --> 00:34:20.356
you can back her on
Kickstarter right now.

00:34:20.356 --> 00:34:22.469
But Google asked
for at least one

00:34:22.469 --> 00:34:30.880
practical talk, so we're
going to-- [LAUGHS]

00:34:30.880 --> 00:34:33.850
So we're going to learn
how various haircuts apply

00:34:33.850 --> 00:34:34.860
to product design.

00:34:34.860 --> 00:34:36.686
Please welcome up Nadav.

00:34:36.686 --> 00:34:38.500
[APPLAUSE]

00:34:40.960 --> 00:34:44.350
NADAV AHARONY: So whenever
you design a new product,

00:34:44.350 --> 00:34:46.010
especially if it's
disruptive, you're

00:34:46.010 --> 00:34:47.870
going to run into a
lot of limitations--

00:34:47.870 --> 00:34:50.320
technology, problems,
bugs, delays,

00:34:50.320 --> 00:34:52.210
missing data, or missing users.

00:34:52.210 --> 00:34:55.239
Now, the more value you have,
the more problems your users

00:34:55.239 --> 00:34:56.520
will let you get away with.

00:34:56.520 --> 00:34:58.560
The issue is that if
you have design problems

00:34:58.560 --> 00:35:00.726
in their experience, they'll
abandon you before they

00:35:00.726 --> 00:35:02.060
figure out your value.

00:35:02.060 --> 00:35:03.684
So that's where the
mullet comes in.

00:35:03.684 --> 00:35:05.850
For those of you who haven't
been around in the 80s,

00:35:05.850 --> 00:35:07.420
or certain parts
of the US today,

00:35:07.420 --> 00:35:10.270
a mullet is a haircut that
offers business up front

00:35:10.270 --> 00:35:12.360
and a party in the back.

00:35:12.360 --> 00:35:14.320
The mullet approach
to product design

00:35:14.320 --> 00:35:16.560
offers you delighting
the users up front

00:35:16.560 --> 00:35:19.980
while keeping all of your
limitations in the back.

00:35:19.980 --> 00:35:22.060
We'll talk about a
few examples and ways

00:35:22.060 --> 00:35:24.770
to try to approach this.

00:35:24.770 --> 00:35:27.000
So one of the most basic
things you start with

00:35:27.000 --> 00:35:30.620
is use what you have, for those
of you who remember MacGyver.

00:35:30.620 --> 00:35:32.800
And basically a very
common set of problems

00:35:32.800 --> 00:35:35.280
are related to delays and
network connectivity issues.

00:35:35.280 --> 00:35:37.310
So by anticipating them,
and being proactive,

00:35:37.310 --> 00:35:39.050
you can actually cache
and preload things

00:35:39.050 --> 00:35:40.810
so that when your user
gets to the point,

00:35:40.810 --> 00:35:42.940
they see an
instantaneous content

00:35:42.940 --> 00:35:44.550
and feel like it's very fast.

00:35:44.550 --> 00:35:46.157
That's one of the basic things.

00:35:46.157 --> 00:35:47.990
Instagram is great at
these kinds of tricks.

00:35:47.990 --> 00:35:50.850
And another trick they have
is an optimistic UI, where

00:35:50.850 --> 00:35:53.090
when the user presses an
action, the UI tells them

00:35:53.090 --> 00:35:55.550
it's also already successful
even before they synced it

00:35:55.550 --> 00:35:58.296
to the server, being
optimistic about it.

00:35:58.296 --> 00:36:00.670
Another totally different way
to look at your limitations

00:36:00.670 --> 00:36:03.190
is to just treat them as part
of your product requirements.

00:36:03.190 --> 00:36:05.530
I was working a few years
ago on a fitness experience

00:36:05.530 --> 00:36:07.240
when people didn't
have data plans yet.

00:36:07.240 --> 00:36:10.380
So we actually
designed an experience

00:36:10.380 --> 00:36:12.010
the showed the UI
every three days

00:36:12.010 --> 00:36:14.510
and assumed that within three
days, they'll connect to Wi-Fi

00:36:14.510 --> 00:36:16.191
and will upload stuff
in the background.

00:36:16.191 --> 00:36:18.190
We also were trying to
do accurate retroactivity

00:36:18.190 --> 00:36:19.550
recognition, which
didn't work well

00:36:19.550 --> 00:36:21.258
with those phones of
those days with lots

00:36:21.258 --> 00:36:22.640
of bugs, battery issues.

00:36:22.640 --> 00:36:24.230
It was hard to get
the users to hold

00:36:24.230 --> 00:36:27.190
the phone in the right way
for us to accurately get it.

00:36:27.190 --> 00:36:29.892
So what we ended up doing
is a high level score

00:36:29.892 --> 00:36:31.600
of your activity level
that was very easy

00:36:31.600 --> 00:36:35.899
to score, very easy to collect,
and not killing the battery.

00:36:35.899 --> 00:36:37.940
And the users could have
the phone anywhere, even

00:36:37.940 --> 00:36:38.850
in their backpack.

00:36:38.850 --> 00:36:40.690
We could still tell if you're
walking more than taking

00:36:40.690 --> 00:36:42.315
the elevator or things
like this, which

00:36:42.315 --> 00:36:43.750
was great for our experience.

00:36:43.750 --> 00:36:45.990
Still with fitness,
another type of limitation

00:36:45.990 --> 00:36:47.939
can actually come
from the outside world

00:36:47.939 --> 00:36:50.230
and the environment where
your product lives, you know,

00:36:50.230 --> 00:36:51.140
like winter.

00:36:51.140 --> 00:36:55.570
So winter offers you limitations
in weather, shorter days,

00:36:55.570 --> 00:36:57.210
and lots of heavy family meals.

00:36:57.210 --> 00:36:59.293
So when you go out running
after that heavy family

00:36:59.293 --> 00:37:01.330
meal in January, you're
going to be very far

00:37:01.330 --> 00:37:03.360
away from your record
in the summer, which

00:37:03.360 --> 00:37:04.540
will be demotivating.

00:37:04.540 --> 00:37:07.430
Strava caught onto the fact
that the Gregorian calendar

00:37:07.430 --> 00:37:10.860
is really nicely aligned
with that dip in fitness.

00:37:10.860 --> 00:37:13.210
So by resetting your
personal annual score,

00:37:13.210 --> 00:37:14.700
your first run is
already a record,

00:37:14.700 --> 00:37:16.980
and your second run is a better
improvement on that record.

00:37:16.980 --> 00:37:19.063
So they're using the
limitations and the solutions

00:37:19.063 --> 00:37:20.217
from the environment.

00:37:20.217 --> 00:37:22.550
Have you ever thought about
why Google Cardboard doesn't

00:37:22.550 --> 00:37:24.360
have a headband?

00:37:24.360 --> 00:37:26.320
So Cardboard tries to
run on as many phones

00:37:26.320 --> 00:37:28.980
as possible with lots of
processing power and graphics

00:37:28.980 --> 00:37:29.760
abilities.

00:37:29.760 --> 00:37:33.120
You should also know that with
VR, if things are jittery,

00:37:33.120 --> 00:37:34.730
people get motion sickness.

00:37:34.730 --> 00:37:36.750
By not having a strap,
they're forcing the user

00:37:36.750 --> 00:37:38.654
to hold it with both
hands and move slower

00:37:38.654 --> 00:37:40.820
in the speed of their torso
or their shoulders which

00:37:40.820 --> 00:37:43.300
is much slower than the
speed of your head moving.

00:37:43.300 --> 00:37:46.350
So by using one limitation,
you solve a worst one.

00:37:46.350 --> 00:37:48.820
Form informs the function.

00:37:48.820 --> 00:37:50.390
Now, a totally
different approach

00:37:50.390 --> 00:37:52.070
is to get your users to help.

00:37:52.070 --> 00:37:54.647
If you design your
experience carefully enough,

00:37:54.647 --> 00:37:56.230
you can have your
users do your chores

00:37:56.230 --> 00:37:58.900
and enjoy them and solve
all your limitations

00:37:58.900 --> 00:38:00.480
as you progress.

00:38:00.480 --> 00:38:02.590
A common problem is
getting to critical mass.

00:38:02.590 --> 00:38:05.510
And a common solution is
finding value for a single user

00:38:05.510 --> 00:38:07.120
like Delicious did
where users started

00:38:07.120 --> 00:38:09.990
by going into saving their
own bookmarks in Delicious.

00:38:09.990 --> 00:38:13.230
And over time, the value grew
and the network effect kicked.

00:38:13.230 --> 00:38:16.570
So like hair, your
product grows with time.

00:38:16.570 --> 00:38:17.877
Remember this.

00:38:17.877 --> 00:38:19.460
You can take this
approach and even do

00:38:19.460 --> 00:38:21.640
a totally different
experience for early adopters.

00:38:21.640 --> 00:38:23.790
Waze had a data problem,
a data limitation,

00:38:23.790 --> 00:38:25.998
where they didn't have enough
roads in the beginning.

00:38:25.998 --> 00:38:28.207
So they had a
game-like experience

00:38:28.207 --> 00:38:30.290
where their first users
were driving through roads

00:38:30.290 --> 00:38:33.710
like a Pac-Man and eating up
power ups on new roads, which

00:38:33.710 --> 00:38:35.320
motivated them to
help map the system.

00:38:35.320 --> 00:38:38.000
Once that was done, they
threw away that experience.

00:38:38.000 --> 00:38:40.284
You can even take that
solution and productize it.

00:38:40.284 --> 00:38:41.950
Luis Von Ahn has been
great about taking

00:38:41.950 --> 00:38:45.060
these very hard problems and
making fun user experiences.

00:38:45.060 --> 00:38:47.317
While they learn a new
language or play a game,

00:38:47.317 --> 00:38:48.900
they're actually
solving hard problems

00:38:48.900 --> 00:38:52.880
like translating the internet
or tagging photos and images.

00:38:52.880 --> 00:38:54.860
As you see, the mullet
is a very powerful tool.

00:38:54.860 --> 00:38:56.880
And we have to treat
it respectfully.

00:38:56.880 --> 00:38:58.670
We have to remember
that our goal is

00:38:58.670 --> 00:39:00.820
to delight users
not to scam them

00:39:00.820 --> 00:39:04.410
and not to make
ourselves look better.

00:39:04.410 --> 00:39:06.640
And some companies
tend to forget that.

00:39:06.640 --> 00:39:08.630
A few examples, you
know, Ashley Madison

00:39:08.630 --> 00:39:11.470
had a female number problem,
and so they used bots and fake

00:39:11.470 --> 00:39:13.520
accounts to try to solve that.

00:39:13.520 --> 00:39:15.340
Volkswagen had a
technology problem,

00:39:15.340 --> 00:39:18.224
and their workaround was to find
a detector for being tested.

00:39:18.224 --> 00:39:19.890
And they are still
screwing their users.

00:39:19.890 --> 00:39:22.710
So don't screw your users.

00:39:22.710 --> 00:39:24.960
Some other takeaways
are that you really have

00:39:24.960 --> 00:39:27.000
to understand your limitations.

00:39:27.000 --> 00:39:29.310
You have to understand your
users and your motivation.

00:39:29.310 --> 00:39:30.990
And you have to understand
the milieu, the environment

00:39:30.990 --> 00:39:32.570
in which your product
lives in, in order

00:39:32.570 --> 00:39:34.403
to design the best
product experience, which

00:39:34.403 --> 00:39:37.290
is not necessarily always the
best engineering experience.

00:39:37.290 --> 00:39:39.220
Now, as you see, this
is a very powerful tool.

00:39:39.220 --> 00:39:40.720
It could be applied
to other domains

00:39:40.720 --> 00:39:41.980
as well like giving talks.

00:39:41.980 --> 00:39:44.180
When you need to give a talk
about a topic you have no idea

00:39:44.180 --> 00:39:46.805
about, pick the right experience
that you'll still get applause

00:39:46.805 --> 00:39:50.090
at the end like 20 slides,
5 minutes, and no audience

00:39:50.090 --> 00:39:51.500
questions at the end.

00:39:51.500 --> 00:39:52.800
Thank you very much.

00:39:52.800 --> 00:39:55.800
[APPLAUSE]

00:40:01.005 --> 00:40:02.380
BRADY FORREST: I
never thought it

00:40:02.380 --> 00:40:04.230
would be awesome to be
compared to a mullet.

00:40:04.230 --> 00:40:06.100
Thank you, Nadav.

00:40:06.100 --> 00:40:08.244
And now our final
talk is just going

00:40:08.244 --> 00:40:10.160
to give you some food
for thought for when you

00:40:10.160 --> 00:40:11.700
do design those applications.

00:40:11.700 --> 00:40:14.190
Please welcome up Renee DiResta.

00:40:14.190 --> 00:40:16.615
[APPLAUSE]

00:40:21.950 --> 00:40:23.940
RENEE DIRESTA: So
conspiracy theorists

00:40:23.940 --> 00:40:26.270
think that the Brazilian
government, the CDC,

00:40:26.270 --> 00:40:27.730
and the World
Health Organization

00:40:27.730 --> 00:40:30.380
are lying to us
about the Zika Virus.

00:40:30.380 --> 00:40:34.030
But they're not sure whether
to blame vaccines, GMOs,

00:40:34.030 --> 00:40:36.830
chemtrails, or Monsanto.

00:40:36.830 --> 00:40:39.850
And residents in Portland,
Oregon, a few years back,

00:40:39.850 --> 00:40:42.590
voted to eliminate fluoride
from their water supply.

00:40:42.590 --> 00:40:44.240
And depending on
which side you asked,

00:40:44.240 --> 00:40:45.720
it was either a
toxic chemical that

00:40:45.720 --> 00:40:48.310
caused cancer or a
toxic mind controlled

00:40:48.310 --> 00:40:52.430
substance used by fascist
regimes-- and also dentists.

00:40:52.430 --> 00:40:54.300
And then there was
the time last year

00:40:54.300 --> 00:40:56.930
when the federal
government invaded Texas.

00:40:56.930 --> 00:40:58.530
Now, that actually
never happened,

00:40:58.530 --> 00:41:00.660
but local residents
were so convinced

00:41:00.660 --> 00:41:04.380
that routine military practice
exercise Jade Helm was an Obama

00:41:04.380 --> 00:41:07.777
attempt to impose martial law
that the governor of the state

00:41:07.777 --> 00:41:10.110
said that he would have the
state national guard monitor

00:41:10.110 --> 00:41:12.274
the exercise.

00:41:12.274 --> 00:41:13.690
So, we laugh at
this stuff, and we

00:41:13.690 --> 00:41:15.520
say these conspiracy
theorists, they're

00:41:15.520 --> 00:41:17.230
so unlike the rest of us.

00:41:17.230 --> 00:41:19.920
But this is actually happening
with increasing frequency.

00:41:19.920 --> 00:41:22.327
And so we need to
understand why.

00:41:22.327 --> 00:41:24.160
Because the thing that
these stories have in

00:41:24.160 --> 00:41:26.360
common is that all of these
myths and conspiracies

00:41:26.360 --> 00:41:28.690
propagated on social networks.

00:41:28.690 --> 00:41:31.660
Algorithms are
influencing policy and not

00:41:31.660 --> 00:41:33.376
necessarily for the better.

00:41:33.376 --> 00:41:35.750
I'm sure everyone in here has
heard of the filter bubble.

00:41:35.750 --> 00:41:37.829
It's the idea that
platforms, they

00:41:37.829 --> 00:41:39.370
want to show you
what you want to see

00:41:39.370 --> 00:41:42.480
because it keeps you engaged
and targetable for longer.

00:41:42.480 --> 00:41:45.180
And the filter bubble was
about partisan politics,

00:41:45.180 --> 00:41:47.790
but what we're seeing now
is recommendation engines.

00:41:47.790 --> 00:41:50.470
And it's a little bit
more like radicalization.

00:41:50.470 --> 00:41:53.590
Because once you join one
conspiracy theory group

00:41:53.590 --> 00:41:56.000
on Facebook,
anti-vax, for example,

00:41:56.000 --> 00:41:58.930
it starts to serve you content
not only about that conspiracy

00:41:58.930 --> 00:42:00.710
but about related conspiracies.

00:42:00.710 --> 00:42:04.140
You probably want to know
about chemtrails, about GMOs,

00:42:04.140 --> 00:42:05.980
about how the earth is flat.

00:42:05.980 --> 00:42:07.860
And yes, that's real.

00:42:07.860 --> 00:42:09.720
And once you join
those groups, people

00:42:09.720 --> 00:42:11.860
will tell you-- they
will welcome you.

00:42:11.860 --> 00:42:13.430
And they'll reinforce
your beliefs.

00:42:13.430 --> 00:42:16.090
And they'll congratulate
you on being awake.

00:42:16.090 --> 00:42:18.510
And the experience is very
much like being recruited

00:42:18.510 --> 00:42:21.370
into a cult. Because social
networking has disrupted

00:42:21.370 --> 00:42:23.520
the market for cult leaders.

00:42:23.520 --> 00:42:25.300
And the reason this
matters is that we're

00:42:25.300 --> 00:42:27.360
at a point where
over 50% of adults

00:42:27.360 --> 00:42:30.290
say that they get their news
primarily from social channels

00:42:30.290 --> 00:42:31.520
like Facebook.

00:42:31.520 --> 00:42:34.234
And it matters what you see,
because if you see things

00:42:34.234 --> 00:42:36.150
from people you know or
people in your groups,

00:42:36.150 --> 00:42:38.230
you're more likely to trust it.

00:42:38.230 --> 00:42:39.770
And the other kind
of corollary here

00:42:39.770 --> 00:42:43.410
is that social networking also
made us all content creators.

00:42:43.410 --> 00:42:46.050
But it really deprecated
the editorial function.

00:42:46.050 --> 00:42:48.420
So now we have a point where
anyone can write anything

00:42:48.420 --> 00:42:51.230
anywhere, and as long as they
can achieve enough uptake,

00:42:51.230 --> 00:42:54.529
it's likely to rise in search
rankings on social networks.

00:42:54.529 --> 00:42:56.070
So we've reached a
point where things

00:42:56.070 --> 00:42:57.927
that are popular and
emotionally resonant

00:42:57.927 --> 00:43:00.010
are much more likely to
be seen by you than things

00:43:00.010 --> 00:43:00.910
that are true.

00:43:00.910 --> 00:43:03.850
Facts really just don't matter.

00:43:03.850 --> 00:43:08.290
So the power to influence our
opinions, the power to shape

00:43:08.290 --> 00:43:10.680
believes, increasingly
lies with those

00:43:10.680 --> 00:43:13.150
who most effectively
disseminate their message,

00:43:13.150 --> 00:43:15.410
whether that message
is true or not.

00:43:15.410 --> 00:43:18.834
And what happens online
and what happens offline,

00:43:18.834 --> 00:43:20.000
there's no difference there.

00:43:20.000 --> 00:43:22.220
There's no gap there.

00:43:22.220 --> 00:43:24.540
This is, in some
ways, fantastic.

00:43:24.540 --> 00:43:26.840
Social networks are a
great tool for organizing.

00:43:26.840 --> 00:43:30.200
But the problem that we have is
that in conspiracy theory land,

00:43:30.200 --> 00:43:32.180
there's an asymmetry of passion.

00:43:32.180 --> 00:43:34.220
It's actually a very
small number of people,

00:43:34.220 --> 00:43:37.930
but they're producing far
more content than what

00:43:37.930 --> 00:43:39.810
you would see from
the people who aren't

00:43:39.810 --> 00:43:41.390
really prone to those beliefs.

00:43:41.390 --> 00:43:43.350
And we saw this play out
in an interesting way

00:43:43.350 --> 00:43:46.310
in California last year
during the heated debate

00:43:46.310 --> 00:43:49.480
about the vaccine law, which
was that the conspiracy

00:43:49.480 --> 00:43:51.250
that vaccines cause
Autism eventually

00:43:51.250 --> 00:43:53.750
lead us to a measles outbreak
because school vaccination

00:43:53.750 --> 00:43:54.830
rates were so low.

00:43:54.830 --> 00:43:56.610
And so legislators
stepped up and tried

00:43:56.610 --> 00:43:58.540
to remedy the problem
legislatively.

00:43:58.540 --> 00:44:00.690
And all of a sudden,
the asymmetry of passion

00:44:00.690 --> 00:44:01.780
was in full view.

00:44:01.780 --> 00:44:03.670
Because the vast
majority of content

00:44:03.670 --> 00:44:06.330
across all social channels--
Pinterest, Facebook, Twitter,

00:44:06.330 --> 00:44:08.740
you name-- was anti-vax.

00:44:08.740 --> 00:44:11.230
But when legislators actually
reached out and started

00:44:11.230 --> 00:44:13.450
polling their constituents
and reaching out

00:44:13.450 --> 00:44:16.580
to people directly,
they found that over 67%

00:44:16.580 --> 00:44:21.010
were in favor of the law, and
over 85% believed that vaccines

00:44:21.010 --> 00:44:24.330
were safe and didn't buy into
the conspiracy theory nonsense.

00:44:24.330 --> 00:44:26.430
But those people aren't
getting up in the morning

00:44:26.430 --> 00:44:27.626
to tweet about this.

00:44:27.626 --> 00:44:29.250
People aren't getting
up in the morning

00:44:29.250 --> 00:44:31.440
to tweet about the
earth being round.

00:44:31.440 --> 00:44:35.310
And so we've reached
the point where

00:44:35.310 --> 00:44:37.810
the kind of
legislative interaction

00:44:37.810 --> 00:44:40.100
spawned by conspiracy
theory groups waste time,

00:44:40.100 --> 00:44:41.440
and they waste money.

00:44:41.440 --> 00:44:43.306
And so now it's
up to us in tech,

00:44:43.306 --> 00:44:44.930
which is why I'm
giving this talk here,

00:44:44.930 --> 00:44:47.480
to start thinking about the
ethics of platform design

00:44:47.480 --> 00:44:49.150
and how our
recommendation engines

00:44:49.150 --> 00:44:51.510
and sort of the
unintentional side effects

00:44:51.510 --> 00:44:54.410
are actually shaping the future.

00:44:54.410 --> 00:44:56.510
So Facebook has been in
the news about this a lot

00:44:56.510 --> 00:44:57.680
over the last week.

00:44:57.680 --> 00:45:00.770
And they are using
humans in addition

00:45:00.770 --> 00:45:04.339
to algorithms to kind of amplify
certain editorial stories.

00:45:04.339 --> 00:45:05.880
They are starting
to note things that

00:45:05.880 --> 00:45:07.235
are hoaxes on the platform.

00:45:07.235 --> 00:45:08.610
You know, and here
we are Google,

00:45:08.610 --> 00:45:11.650
and Google was talking about
maybe having a truth ranking.

00:45:11.650 --> 00:45:14.020
But I'm going to close with
a paraphrase from the filter

00:45:14.020 --> 00:45:14.690
bubble.

00:45:14.690 --> 00:45:17.984
We need these algorithms to have
a sense of civic responsibility

00:45:17.984 --> 00:45:20.150
in order for the web to be
that thing that we've all

00:45:20.150 --> 00:45:21.640
dreamed of it being.

00:45:21.640 --> 00:45:22.430
Thank you.

00:45:22.430 --> 00:45:26.503
[APPLAUSE]

00:45:31.414 --> 00:45:33.330
BRADY FORREST: All right,
thank you very much.

00:45:33.330 --> 00:45:35.284
We'll be back next year.

00:45:35.284 --> 00:45:36.700
And the videos
will be up shortly.

00:45:39.380 --> 00:45:43.330
[MUSIC PLAYING]

