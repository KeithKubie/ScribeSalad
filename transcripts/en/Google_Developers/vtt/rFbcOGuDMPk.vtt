WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.416
[MUSIC PLAYING]

00:00:08.559 --> 00:00:09.600
TOM SLATER: Hi, everyone.

00:00:09.600 --> 00:00:10.450
I'm Tom.

00:00:10.450 --> 00:00:13.320
I'm one of the technical
leads over on our VR and AR

00:00:13.320 --> 00:00:14.970
efforts at Google.

00:00:14.970 --> 00:00:19.320
My team specializes in
developer productivity tools.

00:00:19.320 --> 00:00:21.360
I'm really excited,
just one week

00:00:21.360 --> 00:00:24.130
after announcing AR Core,
to stand here today and talk

00:00:24.130 --> 00:00:28.100
to you all about the
AR Core SDK preview

00:00:28.100 --> 00:00:30.870
and how you can really
easily add AR capabilities

00:00:30.870 --> 00:00:32.612
to your Android apps.

00:00:32.612 --> 00:00:34.320
So just a quick run
through of what we're

00:00:34.320 --> 00:00:36.070
going to talk about today.

00:00:36.070 --> 00:00:39.060
So we're going to talk about
AR and its really closely

00:00:39.060 --> 00:00:40.240
related cousin VR.

00:00:40.240 --> 00:00:44.340
We're going to talk about the
concepts that underpin AR Core.

00:00:44.340 --> 00:00:46.680
We're going to have a
walk through the API.

00:00:46.680 --> 00:00:48.990
We're going to talk
through all the options

00:00:48.990 --> 00:00:51.344
that we have to
build our AR content.

00:00:51.344 --> 00:00:53.760
And we're going to talk about
all the options for building

00:00:53.760 --> 00:00:54.520
your code as well.

00:00:57.260 --> 00:01:00.250
So before we get into
the technical details,

00:01:00.250 --> 00:01:04.750
I really want to start with
what immersive computing means,

00:01:04.750 --> 00:01:08.690
where we've come from, and
what can it do for you.

00:01:08.690 --> 00:01:11.890
So I'm always asked
why AR and VR?

00:01:11.890 --> 00:01:14.440
We have these mobile
devices, and it's all great.

00:01:14.440 --> 00:01:16.700
Why should we go and
do these other things?

00:01:16.700 --> 00:01:20.290
So with Cardboard,
Daydream, and Tango,

00:01:20.290 --> 00:01:22.960
we've been investing in this
space for quite some time.

00:01:22.960 --> 00:01:24.610
And we really believe
that interacting

00:01:24.610 --> 00:01:27.650
with your data in a more
natural way is the future.

00:01:27.650 --> 00:01:30.430
Immersive computing removes
a lot of those abstractions

00:01:30.430 --> 00:01:32.530
between you and your computing.

00:01:32.530 --> 00:01:35.770
So I want to take a minute just
discussing these technologies

00:01:35.770 --> 00:01:39.500
before we jump into some code.

00:01:39.500 --> 00:01:41.810
So we think of AR
and VR as being

00:01:41.810 --> 00:01:44.480
points on a spectrum
of immersive computing.

00:01:44.480 --> 00:01:48.230
On the far left, you have
reality, which is the world

00:01:48.230 --> 00:01:50.570
as we know it, and
we're sitting in today.

00:01:50.570 --> 00:01:54.510
If we start to add digital
content into our reality,

00:01:54.510 --> 00:01:57.560
then we start to augment
it, and that is, hence,

00:01:57.560 --> 00:01:59.390
augmented reality.

00:01:59.390 --> 00:02:03.440
And if we completely replace
that reality with the virtual,

00:02:03.440 --> 00:02:07.030
we have virtual reality.

00:02:07.030 --> 00:02:11.250
And so AR can bring any
of your digital content

00:02:11.250 --> 00:02:17.400
to you and in your world, such
as this guy posing with a dog.

00:02:17.400 --> 00:02:19.350
So some of the benefits of AR--

00:02:19.350 --> 00:02:23.580
you can see objects at real
size and scale in your world.

00:02:23.580 --> 00:02:26.040
Imagine being able
to buy furniture

00:02:26.040 --> 00:02:28.470
and being able to see if it
actually fits in your house

00:02:28.470 --> 00:02:30.240
before you buy it.

00:02:30.240 --> 00:02:32.310
You can also see
things in context.

00:02:32.310 --> 00:02:34.680
Again, imagine buying that
furniture and making sure

00:02:34.680 --> 00:02:37.920
that it matches with all of
your other decor in your house.

00:02:37.920 --> 00:02:39.450
And being able
to-- imagine being

00:02:39.450 --> 00:02:42.270
able to annotate the
real world with post-it

00:02:42.270 --> 00:02:45.990
notes without actually
causing a real physical mess.

00:02:45.990 --> 00:02:51.662
And it also adds the ability
for natural input for 3D scenes.

00:02:51.662 --> 00:02:54.120
So if ever you've used digital
content creation or modeling

00:02:54.120 --> 00:02:56.040
tools, it's actually
really hard to control

00:02:56.040 --> 00:02:58.350
that camera in your scene.

00:02:58.350 --> 00:03:01.110
With augmented reality,
you just hold your phone,

00:03:01.110 --> 00:03:05.900
and you just look at it,
and anyone can do it.

00:03:05.900 --> 00:03:07.970
And then if you replace
everything in your world

00:03:07.970 --> 00:03:10.520
with the virtual,
with digital content,

00:03:10.520 --> 00:03:12.210
you've got virtual reality.

00:03:12.210 --> 00:03:17.510
It allows you to go to places
and visit worlds in an instant.

00:03:17.510 --> 00:03:21.080
And some benefits of this are
you get complete immersion.

00:03:21.080 --> 00:03:24.440
You really feel like you've been
transported to another place.

00:03:24.440 --> 00:03:28.580
And there's some really, really
huge innovations in input

00:03:28.580 --> 00:03:32.150
that really lets you work
naturally in your world.

00:03:32.150 --> 00:03:35.090
Both VR and AR enable us
to experience computing

00:03:35.090 --> 00:03:37.370
more like we experience
the real world,

00:03:37.370 --> 00:03:41.910
and both take advantage of a
lot of the same technologies.

00:03:41.910 --> 00:03:45.240
So back in 2014, we
started with the idea

00:03:45.240 --> 00:03:47.520
that your devices should
be able to understand

00:03:47.520 --> 00:03:49.260
more of the world.

00:03:49.260 --> 00:03:51.270
So with dedicated
hardware, Tango

00:03:51.270 --> 00:03:52.890
allowed us to
understand the depth

00:03:52.890 --> 00:03:55.500
and allowed us to build some
really great applications,

00:03:55.500 --> 00:03:59.550
from being able to measure
your world, being able to map

00:03:59.550 --> 00:04:02.370
and share your house,
and to be able to play

00:04:02.370 --> 00:04:05.490
some games on your
tabletop with your friends.

00:04:05.490 --> 00:04:08.700
We built two consumer
devices with Asus and Lenovo,

00:04:08.700 --> 00:04:12.120
and developers create more than
100 applications for Tango.

00:04:12.120 --> 00:04:16.110
We learned a lot, and now,
we can do more with software

00:04:16.110 --> 00:04:17.829
than we could three years ago.

00:04:17.829 --> 00:04:21.480
And we don't have to rely on
custom sensors, which leads us

00:04:21.480 --> 00:04:24.555
today and last week's
announcement with AR Core.

00:04:27.650 --> 00:04:30.230
So last week, we
announced AR Core.

00:04:30.230 --> 00:04:33.020
It's a preview SDK that
allows you to get up

00:04:33.020 --> 00:04:36.690
and running with building
AR applications right now.

00:04:36.690 --> 00:04:39.280
AR Core takes everything
we learned from Tango,

00:04:39.280 --> 00:04:41.990
and it makes AR available on
the phone you have today--

00:04:41.990 --> 00:04:46.010
no depth cameras or custom
special sensors required.

00:04:46.010 --> 00:04:48.080
AR Core is currently
in preview, and we're

00:04:48.080 --> 00:04:50.870
looking to developers like
you to give us feedback on how

00:04:50.870 --> 00:04:54.356
to make this SDK work for you.

00:04:54.356 --> 00:04:56.820
And with two billion
devices out there,

00:04:56.820 --> 00:05:00.130
we have a huge potential
audience for this technology.

00:05:00.130 --> 00:05:03.480
With AR Core so far running
on the Pixel, Pixel XL,

00:05:03.480 --> 00:05:06.510
and the Galaxy S8, we currently
run on millions of devices

00:05:06.510 --> 00:05:07.370
already.

00:05:07.370 --> 00:05:09.000
And we're working
with manufacturers

00:05:09.000 --> 00:05:12.624
like Samsung, Huawei,
LG, Asus, and others

00:05:12.624 --> 00:05:14.040
so that at the end
of the preview,

00:05:14.040 --> 00:05:18.405
we anticipate that we're going
to run on 100 million devices.

00:05:18.405 --> 00:05:20.280
And we're working with
these hardware vendors

00:05:20.280 --> 00:05:23.970
to make this possible with
a really consistent bar

00:05:23.970 --> 00:05:26.370
for quality and high
performance, in the same way

00:05:26.370 --> 00:05:29.680
that we did with Daydream.

00:05:29.680 --> 00:05:31.552
And so our before we
get into the code,

00:05:31.552 --> 00:05:33.010
I think it's really,
really helpful

00:05:33.010 --> 00:05:36.550
to understand some of the
fundamental concepts behind AR,

00:05:36.550 --> 00:05:39.850
just so you know how AR Core
is working under the hood.

00:05:39.850 --> 00:05:43.340
So there are three main
concepts to think about.

00:05:43.340 --> 00:05:48.310
One is motion tracking, two is
environmental understanding,

00:05:48.310 --> 00:05:50.360
and three is light estimation.

00:05:50.360 --> 00:05:54.690
So let's go through them in
a little bit of detail now.

00:05:54.690 --> 00:05:59.490
So to render AR content, you
need a virtual camera that

00:05:59.490 --> 00:06:01.590
matches your physical camera.

00:06:01.590 --> 00:06:05.760
You render the virtual scene,
you composite with your camera,

00:06:05.760 --> 00:06:07.290
and you're done.

00:06:07.290 --> 00:06:08.730
This sounds simple.

00:06:08.730 --> 00:06:10.800
It's actually
really, really hard.

00:06:10.800 --> 00:06:13.020
Whilst your phone gyroscope
is really, really great

00:06:13.020 --> 00:06:15.517
for rotation, it
can drift over time.

00:06:15.517 --> 00:06:17.100
And whilst your
accelerometer is great

00:06:17.100 --> 00:06:19.740
for those instantaneous
inputs, it's

00:06:19.740 --> 00:06:22.899
not so great to figure
out an actual position.

00:06:22.899 --> 00:06:24.690
So the really hard part
of getting AR right

00:06:24.690 --> 00:06:27.180
is to figure out this
translation and rotation

00:06:27.180 --> 00:06:29.580
of your device in
real time so you

00:06:29.580 --> 00:06:32.760
can render digital objects
with the same virtual camera

00:06:32.760 --> 00:06:35.350
as your physical camera.

00:06:35.350 --> 00:06:37.690
If you get this wrong,
objects in the world

00:06:37.690 --> 00:06:40.270
will be misaligned with
their virtual equivalents.

00:06:40.270 --> 00:06:43.570
They will swim and jump,
and they won't appear

00:06:43.570 --> 00:06:46.420
properly rooted in the world.

00:06:46.420 --> 00:06:48.250
And you can see
how effective we've

00:06:48.250 --> 00:06:50.980
done this with AR Core
because of this scarecrow

00:06:50.980 --> 00:06:55.560
who looks just like everyone
else queuing for tacos.

00:06:55.560 --> 00:06:59.000
So to do this, AR Core
uses the device's camera

00:06:59.000 --> 00:07:01.790
and inertial measuring
unit to track

00:07:01.790 --> 00:07:04.250
exactly where your device it
in the world using a process

00:07:04.250 --> 00:07:08.610
called Concurrent Odometry and
Mapping, also known as COM.

00:07:08.610 --> 00:07:10.400
It looks for visually
distinct features

00:07:10.400 --> 00:07:12.380
that can track over
successive frames

00:07:12.380 --> 00:07:16.160
and builds a point cloud so it
can localize against that point

00:07:16.160 --> 00:07:17.630
cloud.

00:07:17.630 --> 00:07:19.760
This, combined with the
high frequency IMU data,

00:07:19.760 --> 00:07:21.710
gives you rotation
and translation

00:07:21.710 --> 00:07:24.620
in the world so you can render
virtual content in exactly

00:07:24.620 --> 00:07:26.150
the right place.

00:07:26.150 --> 00:07:28.940
This is over and above other AR
experiences you may have tried,

00:07:28.940 --> 00:07:33.770
which only uses the gyroscope
to get a rotation, which

00:07:33.770 --> 00:07:36.890
has the problem of content
sliding around your world,

00:07:36.890 --> 00:07:39.890
and you can't move in
closer for a better look.

00:07:39.890 --> 00:07:42.440
This is really, really key for
anchoring your digital content

00:07:42.440 --> 00:07:44.190
over the real world.

00:07:44.190 --> 00:07:45.740
So this illustration
is an example

00:07:45.740 --> 00:07:48.220
of the device tracking
feature points in your world

00:07:48.220 --> 00:07:51.140
and creating a point cloud,
but when it does it for real,

00:07:51.140 --> 00:07:55.230
has actually a lot more
points than just four.

00:07:55.230 --> 00:07:58.560
So on top of motion tracking,
which is really important,

00:07:58.560 --> 00:08:01.480
environment understanding
is also super important.

00:08:01.480 --> 00:08:05.200
Rendering content isn't actually
that interesting by itself.

00:08:05.200 --> 00:08:08.120
You need to be able to interact
with your world as well.

00:08:08.120 --> 00:08:11.050
So AR Core is looking for
clusters of those feature

00:08:11.050 --> 00:08:15.150
points that appear to lie on
common horizontal surfaces,

00:08:15.150 --> 00:08:17.680
and it makes these surfaces
available to your app

00:08:17.680 --> 00:08:20.020
as planes.

00:08:20.020 --> 00:08:22.720
Since planes are
mathematically infinite,

00:08:22.720 --> 00:08:26.050
AR Core also provides the bounds
of these surfaces as a polygon.

00:08:26.050 --> 00:08:28.120
And you can use this
information to place objects

00:08:28.120 --> 00:08:32.600
in your world like
this Android guy here.

00:08:32.600 --> 00:08:35.570
So planes are detected
on horizontal surfaces,

00:08:35.570 --> 00:08:39.140
such as the floor, tables,
kitchen countertops, benches,

00:08:39.140 --> 00:08:41.299
chairs, you name it.

00:08:41.299 --> 00:08:44.360
However, because AR
Core uses feature points

00:08:44.360 --> 00:08:47.030
to detect these
surfaces, flat surfaces

00:08:47.030 --> 00:08:50.300
without any texture or
highly reflective surfaces

00:08:50.300 --> 00:08:54.050
might not be detected properly.

00:08:54.050 --> 00:08:56.250
And then finally,
light estimation.

00:08:56.250 --> 00:08:58.250
So AR Core is able
to detect information

00:08:58.250 --> 00:09:00.380
about the current
environment's lighting

00:09:00.380 --> 00:09:02.120
and gives you a
value representing

00:09:02.120 --> 00:09:05.270
the average intensity
of a given camera image.

00:09:05.270 --> 00:09:07.327
This information lets
you light your scene

00:09:07.327 --> 00:09:09.410
and your virtual objects
under the same conditions

00:09:09.410 --> 00:09:11.400
as the environment,
which increases

00:09:11.400 --> 00:09:13.340
that sense of realism.

00:09:13.340 --> 00:09:15.470
If you don't do this,
your digital objects

00:09:15.470 --> 00:09:17.120
will stand out and
not appear to be

00:09:17.120 --> 00:09:19.190
a part of that world,
which is really, really

00:09:19.190 --> 00:09:20.489
key to realistic rendering.

00:09:20.489 --> 00:09:22.280
And it lets you do some
really fun effects,

00:09:22.280 --> 00:09:24.110
like have this lion
who gets scared

00:09:24.110 --> 00:09:27.370
when you turn out the lights.

00:09:27.370 --> 00:09:30.850
And it might seem frivolous, but
it's actually really important.

00:09:30.850 --> 00:09:33.220
If you've ever taken your
camera and just looked

00:09:33.220 --> 00:09:35.530
at a light source or
pointed it out the window,

00:09:35.530 --> 00:09:37.330
the auto exposure of
your camera actually

00:09:37.330 --> 00:09:38.980
changes that range
pretty hugely,

00:09:38.980 --> 00:09:42.880
so it can be really not so
great if you don't take this

00:09:42.880 --> 00:09:45.580
into account.

00:09:45.580 --> 00:09:48.420
So now, we've gone through
the main concepts of AR Core.

00:09:48.420 --> 00:09:50.310
Let's jump into
some code and see

00:09:50.310 --> 00:09:54.870
what it takes to build an
AR application in Java.

00:09:54.870 --> 00:09:57.840
So this is how I
think of the API.

00:09:57.840 --> 00:10:00.810
You create a session, which
represents the AR session

00:10:00.810 --> 00:10:02.130
that you're running.

00:10:02.130 --> 00:10:04.770
Once you've got this,
you update the session,

00:10:04.770 --> 00:10:06.180
and it gives you a frame.

00:10:06.180 --> 00:10:08.970
Once you have a frame,
that represents your camera

00:10:08.970 --> 00:10:14.940
and all the metadata that goes
alongside that, you get planes.

00:10:14.940 --> 00:10:17.700
Once you have planes, you're
able to create anchors.

00:10:17.700 --> 00:10:19.560
With anchors, you're
able to place content

00:10:19.560 --> 00:10:21.660
in your real world.

00:10:21.660 --> 00:10:26.730
And our Hello AR app exercises
all of those parts on the API,

00:10:26.730 --> 00:10:30.580
and it is on GitHub,
and is great to get up

00:10:30.580 --> 00:10:33.490
and running with this.

00:10:33.490 --> 00:10:36.280
So going through the code
it's really, really easy

00:10:36.280 --> 00:10:38.590
to start an AR Core session.

00:10:38.590 --> 00:10:40.840
You just start a session.

00:10:40.840 --> 00:10:43.580
We provide a default
configuration file,

00:10:43.580 --> 00:10:46.690
which basically turns on
every part of the API--

00:10:46.690 --> 00:10:50.470
motion tracking, plane
finding, light estimation.

00:10:50.470 --> 00:10:52.840
Simple way to check if
your device does not

00:10:52.840 --> 00:10:53.750
support AR Core.

00:10:57.420 --> 00:11:01.980
And when we come to render our
application, first thing we do

00:11:01.980 --> 00:11:08.250
is clear the frame, and then
we simply call session.update.

00:11:08.250 --> 00:11:10.140
One of the key concepts
to understand here

00:11:10.140 --> 00:11:12.270
is, when you create
your session,

00:11:12.270 --> 00:11:15.810
one of the things you might want
to change within your config

00:11:15.810 --> 00:11:20.520
is whether it's using blocking
or latest camera image.

00:11:20.520 --> 00:11:23.610
If it uses blocking,
it basically gives you

00:11:23.610 --> 00:11:26.760
a frame at the rate at
which your camera runs,

00:11:26.760 --> 00:11:28.590
which kind of makes sense.

00:11:28.590 --> 00:11:31.440
You don't want to render
any faster than you have to.

00:11:31.440 --> 00:11:34.470
You want to make sure that every
pose aligns with every frame.

00:11:34.470 --> 00:11:36.000
But for some of
your applications,

00:11:36.000 --> 00:11:39.060
where you want a really smooth
update, you have animations,

00:11:39.060 --> 00:11:41.340
you might want to just render
as fast as you possibly

00:11:41.340 --> 00:11:44.680
can and at the expense
of power and performance.

00:11:47.260 --> 00:11:50.160
So once you have a frame, we
have this helper function,

00:11:50.160 --> 00:11:54.510
Hit Test, which helps us to
cast a ray into the world based

00:11:54.510 --> 00:11:56.580
on a touch location
and see if you have

00:11:56.580 --> 00:11:59.480
touched one of these planes.

00:11:59.480 --> 00:12:02.490
If it collided, and the
tap was within the bounds

00:12:02.490 --> 00:12:05.710
of that detected plane,
then we create an anchor.

00:12:05.710 --> 00:12:09.430
And we'll go into anchors in a
little bit more detail later.

00:12:09.430 --> 00:12:12.870
So we also have to see if the
frame was actually tracking.

00:12:12.870 --> 00:12:15.030
If you have put your hand
in front of the camera

00:12:15.030 --> 00:12:16.613
or something, and
you're not tracking,

00:12:16.613 --> 00:12:19.650
then you want to make sure that
your intersections are correct.

00:12:22.270 --> 00:12:24.860
And then you query the
frame for all the data

00:12:24.860 --> 00:12:26.900
that you need to
render your objects.

00:12:26.900 --> 00:12:28.820
And don't worry if
you haven't used

00:12:28.820 --> 00:12:32.160
projection matrices before, and
you don't know a view matrix

00:12:32.160 --> 00:12:32.660
is.

00:12:32.660 --> 00:12:34.400
That's OK.

00:12:34.400 --> 00:12:36.770
As mentioned earlier, to
render this AR content,

00:12:36.770 --> 00:12:39.680
you have to match the field
of view of your virtual camera

00:12:39.680 --> 00:12:42.080
with the field of view
of the real camera.

00:12:42.080 --> 00:12:44.870
The projection matrix contains
all of those properties

00:12:44.870 --> 00:12:47.420
that you need,
and you just query

00:12:47.420 --> 00:12:51.110
AR Core to-- the AR Core
session to gain access to those.

00:12:51.110 --> 00:12:54.140
And in this example, we
set the range of objects

00:12:54.140 --> 00:12:58.370
that you will render from 10
centimeters to 100 meters.

00:12:58.370 --> 00:13:00.620
And then the view
matrix is what contains

00:13:00.620 --> 00:13:03.350
all of the information
for motion tracking.

00:13:03.350 --> 00:13:05.120
That actually contains
the pose where

00:13:05.120 --> 00:13:07.890
the camera is in the world.

00:13:07.890 --> 00:13:09.920
And then finally,
just getting access

00:13:09.920 --> 00:13:11.690
to the lighting estimation--

00:13:11.690 --> 00:13:13.490
it's just a simple
accessor call.

00:13:13.490 --> 00:13:15.380
Once you've got that
value, you can either

00:13:15.380 --> 00:13:17.600
use it for some
logic, or you can

00:13:17.600 --> 00:13:21.090
use it to affect your
rendering or your lighting.

00:13:21.090 --> 00:13:23.540
And then finally, we loop
over all of our anchors

00:13:23.540 --> 00:13:25.280
that we've placed in the world.

00:13:25.280 --> 00:13:30.480
And if they're being tracked,
we render those objects.

00:13:30.480 --> 00:13:32.870
So if the projection
matrix contained the camera

00:13:32.870 --> 00:13:36.530
properties, and the view match
matrix contained the camera

00:13:36.530 --> 00:13:39.710
location, the model matrix
contains the location

00:13:39.710 --> 00:13:42.260
of that anchor within the world.

00:13:42.260 --> 00:13:44.912
With that combined model
view projection matrix,

00:13:44.912 --> 00:13:46.370
you have everything
you need to put

00:13:46.370 --> 00:13:48.380
the pixels on the right
place on the screen,

00:13:48.380 --> 00:13:53.450
properly overlaid on
top of your camera feed.

00:13:53.450 --> 00:13:57.710
So talking about anchors,
why do we need anchors?

00:13:57.710 --> 00:13:59.810
What is this concept
of an anchor?

00:13:59.810 --> 00:14:04.017
So you might think, well,
it's three dimensional space.

00:14:04.017 --> 00:14:05.600
Why don't we just
call where you start

00:14:05.600 --> 00:14:08.990
the application the origin
and place your objects

00:14:08.990 --> 00:14:11.100
relative to that?

00:14:11.100 --> 00:14:14.600
So it turns out there's actually
some error in the poses you

00:14:14.600 --> 00:14:17.060
get back from motion tracking.

00:14:17.060 --> 00:14:19.880
Motion tracking is
constantly updating

00:14:19.880 --> 00:14:21.500
its understanding of the world.

00:14:21.500 --> 00:14:24.740
And if you use anchors, as
this understanding of the world

00:14:24.740 --> 00:14:29.100
updates, the pose of your
models will update as well.

00:14:29.100 --> 00:14:31.160
A good example where
you want to use anchors

00:14:31.160 --> 00:14:33.620
is imagine walking around
a building in a loop.

00:14:33.620 --> 00:14:35.450
When you get back to
where you started,

00:14:35.450 --> 00:14:37.662
that drift will have
accumulated over time,

00:14:37.662 --> 00:14:39.620
and you really want to
use anchors to make sure

00:14:39.620 --> 00:14:43.690
that that all stays correct.

00:14:43.690 --> 00:14:46.104
And so if you place an
anchor in the world,

00:14:46.104 --> 00:14:47.520
you should also
make sure that you

00:14:47.520 --> 00:14:51.670
place the digital object on
top of that anchor as well.

00:14:51.670 --> 00:14:54.750
So any time there is
error in that rotation,

00:14:54.750 --> 00:14:57.660
then the further away that your
object is from that anchor,

00:14:57.660 --> 00:15:00.560
you'll end up with this lever
arm effect, where you're

00:15:00.560 --> 00:15:03.060
rotating around a pivot that
is in the center of the object,

00:15:03.060 --> 00:15:06.630
and it translates off
from where it wants to be.

00:15:06.630 --> 00:15:10.775
And then really think about
these anchors are there

00:15:10.775 --> 00:15:13.380
to root your digital objects,
your physical objects

00:15:13.380 --> 00:15:14.520
in the world.

00:15:14.520 --> 00:15:18.150
If I was going to place
an object on a chair,

00:15:18.150 --> 00:15:20.040
I want to create one
anchor for the chair

00:15:20.040 --> 00:15:21.780
and place the
object on the chair.

00:15:21.780 --> 00:15:24.810
If I want to create-- put
10 objects on my desk,

00:15:24.810 --> 00:15:26.790
I don't need to
create 10 anchors.

00:15:26.790 --> 00:15:30.150
I should just create one and
place them all relative to it.

00:15:30.150 --> 00:15:33.630
And then just again, avoid
using those global coordinate

00:15:33.630 --> 00:15:34.320
systems.

00:15:34.320 --> 00:15:39.820
You'll have a bad
time if you use those.

00:15:39.820 --> 00:15:43.180
So we kind of skipped over this.

00:15:43.180 --> 00:15:45.820
Apart from cleaning the
screen, we didn't really

00:15:45.820 --> 00:15:48.100
talk about rendering 3D.

00:15:48.100 --> 00:15:50.800
Rendering 3D is actually
a really big topic

00:15:50.800 --> 00:15:55.270
that I couldn't possibly cover
in a 30-minute talk here today.

00:15:55.270 --> 00:15:58.060
But to give you
something to look at,

00:15:58.060 --> 00:15:59.410
there are a few options.

00:15:59.410 --> 00:16:02.495
You can use Open GLES
directly, and we actually

00:16:02.495 --> 00:16:04.870
have some great tutorials on
how to get started with this

00:16:04.870 --> 00:16:07.810
on our developer.android.com.

00:16:07.810 --> 00:16:10.270
Or you can use frameworks
such as [INAUDIBLE],, which

00:16:10.270 --> 00:16:13.180
does a lot of that
heavy lifting for you.

00:16:13.180 --> 00:16:15.530
Or you can take a look
our Hello AR sample,

00:16:15.530 --> 00:16:18.772
which actually contains some
model loading code and model

00:16:18.772 --> 00:16:20.230
rendering code as
well, if you just

00:16:20.230 --> 00:16:23.390
want to place some
objects in your world.

00:16:23.390 --> 00:16:26.920
So not only does AR
Core work with Java,

00:16:26.920 --> 00:16:29.590
it also works with
Unity and Unreal,

00:16:29.590 --> 00:16:32.230
and we've done a lot
of work to make sure

00:16:32.230 --> 00:16:34.270
that it integrates really well.

00:16:34.270 --> 00:16:36.280
Common game engines
like these, they

00:16:36.280 --> 00:16:38.680
remove a lot of the
complexity from managing

00:16:38.680 --> 00:16:41.770
a complex 3D scene, so that
you can focus on actually

00:16:41.770 --> 00:16:44.790
building your application.

00:16:44.790 --> 00:16:47.760
So we've ported
Hello AR into Unity.

00:16:47.760 --> 00:16:48.800
You can see it there.

00:16:48.800 --> 00:16:51.990
It exercises all
of those same APIs,

00:16:51.990 --> 00:16:54.510
and we've designed
it so you can easily

00:16:54.510 --> 00:16:57.150
get up to speed, use
those scripts and prefabs

00:16:57.150 --> 00:16:58.410
in your own application.

00:16:58.410 --> 00:17:02.910
And it comes as part of the
SDK when you download it,

00:17:02.910 --> 00:17:05.730
and similarly, we have the
same for Unreal as well.

00:17:05.730 --> 00:17:07.829
We've done everything
to make building your AR

00:17:07.829 --> 00:17:11.430
applications really
easy using the tools

00:17:11.430 --> 00:17:14.880
that you're most familiar with.

00:17:14.880 --> 00:17:17.300
So another option is WebXR.

00:17:17.300 --> 00:17:21.589
So real web standards
for AR don't exist yet,

00:17:21.589 --> 00:17:23.540
but these prototypes
allow web developers

00:17:23.540 --> 00:17:26.960
to start building augmented
reality web experiences today.

00:17:26.960 --> 00:17:29.210
Their experiments will teach
us all what AR on the web

00:17:29.210 --> 00:17:31.640
could look like,
which will hopefully

00:17:31.640 --> 00:17:33.500
make the real web
standards arrive faster

00:17:33.500 --> 00:17:35.330
and be better designed.

00:17:35.330 --> 00:17:37.670
These capabilities are
built on top of WebVR,

00:17:37.670 --> 00:17:39.320
so if you're familiar
with that, it's

00:17:39.320 --> 00:17:42.530
really easy to get up
and running with WebXR.

00:17:42.530 --> 00:17:44.840
And this demo that
you can see here also

00:17:44.840 --> 00:17:49.730
works in experimental
versions of Chrome for AR kit

00:17:49.730 --> 00:17:53.410
as well on iOS.

00:17:53.410 --> 00:17:57.820
And so we've talked a lot
about building AR applications,

00:17:57.820 --> 00:18:02.440
but the really hard part
is how do I get content?

00:18:02.440 --> 00:18:05.800
We all know how to
create content for 2D.

00:18:05.800 --> 00:18:07.540
We know how to get images.

00:18:07.540 --> 00:18:10.534
We know how to get
text, fonts, videos.

00:18:10.534 --> 00:18:11.950
We know how to
create them, and we

00:18:11.950 --> 00:18:14.230
know how to use them
in our applications.

00:18:14.230 --> 00:18:18.190
So Google have been working in
AR and VR for quite some time,

00:18:18.190 --> 00:18:20.570
and we've made creating
this content really,

00:18:20.570 --> 00:18:24.810
really easy, which is
why we created Blocks.

00:18:24.810 --> 00:18:29.620
So Blocks lets you build 3D
content in VR really quickly.

00:18:29.620 --> 00:18:31.630
And even if you don't
have a VR headset,

00:18:31.630 --> 00:18:33.490
take a look at our library.

00:18:33.490 --> 00:18:37.640
Any content tagged as remixable
is available for download.

00:18:37.640 --> 00:18:39.860
So let's take a quick look
at some of the content

00:18:39.860 --> 00:18:43.180
that our creators
have built for blocks.

00:18:43.180 --> 00:18:47.550
So if you want this guy, a
ramen chef, in your kitchen,

00:18:47.550 --> 00:18:51.400
that power is now open to you.

00:18:51.400 --> 00:18:53.240
And if you want this
guy to watch over you

00:18:53.240 --> 00:18:57.530
at night on your bedside
table, you can live that dream.

00:18:57.530 --> 00:18:59.300
It's really, really
impressive, what

00:18:59.300 --> 00:19:01.910
people have made using Blocks,
and it's a super easy way

00:19:01.910 --> 00:19:05.730
to get 3D content
into your application.

00:19:05.730 --> 00:19:09.080
So I encourage you all to
head to our Blocks website,

00:19:09.080 --> 00:19:11.060
download it, and give it a play.

00:19:11.060 --> 00:19:14.810
It's really, really fun
building this content in VR.

00:19:14.810 --> 00:19:18.410
So before we finish,
I'd love to give you

00:19:18.410 --> 00:19:22.190
a taste of some of the things
that we've built in AR Core.

00:19:22.190 --> 00:19:25.676
[MUSIC PLAYING]

00:20:34.900 --> 00:20:38.480
So this has been a whirlwind
tour of the capabilities of AR

00:20:38.480 --> 00:20:41.760
Core, and all the information
you need to get started

00:20:41.760 --> 00:20:44.610
is there today on our website.

00:20:44.610 --> 00:20:46.957
So we've talked about
the fundamentals of AR.

00:20:46.957 --> 00:20:49.290
We've talked about the options
you have to build your AR

00:20:49.290 --> 00:20:52.350
applications, and we've talked
about how to build content

00:20:52.350 --> 00:20:53.890
for those applications.

00:20:53.890 --> 00:20:57.270
We're actively seeking
feedback from developers

00:20:57.270 --> 00:20:59.340
such as yourselves through
this preview phase,

00:20:59.340 --> 00:21:01.770
and we'll be monitoring
our GitHub issues

00:21:01.770 --> 00:21:03.480
and other channels
closely so that we're

00:21:03.480 --> 00:21:06.670
building a platform that works
for what you want to build.

00:21:06.670 --> 00:21:10.140
So you can see that AR marks
the next big shift in what's

00:21:10.140 --> 00:21:12.120
possible with mobile devices.

00:21:12.120 --> 00:21:14.259
Get started with the
AR Core preview today.

00:21:14.259 --> 00:21:16.050
We really can't wait
to see what you build.

00:21:16.050 --> 00:21:17.250
[APPLAUSE]

00:21:17.250 --> 00:21:20.600
[MUSIC PLAYING]

