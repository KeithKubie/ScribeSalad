WEBVTT
Kind: captions
Language: en

00:00:01.980 --> 00:00:03.290
HAL VARIAN: Hi, I'm Hal Varian.

00:00:03.290 --> 00:00:06.930
I'm the chief economist
here at Google.

00:00:06.930 --> 00:00:09.480
It's a great
pleasure to introduce

00:00:09.480 --> 00:00:12.215
Erik Brynjolfsson
and Andrew McAfee.

00:00:12.215 --> 00:00:16.379
I've known Erik for so
long, I can spell his name.

00:00:16.379 --> 00:00:19.061
[LAUGHTER]

00:00:19.061 --> 00:00:20.610
It's really quite
an achievement.

00:00:20.610 --> 00:00:23.350
I haven't known Andy that long.

00:00:23.350 --> 00:00:24.179
One F or two Fs?

00:00:24.179 --> 00:00:24.970
ANDREW MCAFEE: See?

00:00:24.970 --> 00:00:25.650
We'll get there.

00:00:25.650 --> 00:00:27.066
HAL VARIAN: Well,
we'll get there.

00:00:27.066 --> 00:00:29.270
Someday I'll learn it,
but I read their book,

00:00:29.270 --> 00:00:31.720
and now I understand
all this effort

00:00:31.720 --> 00:00:36.740
that I put into spelling-- it's
not worth anything anymore.

00:00:36.740 --> 00:00:41.540
Two Ss, one F, all this stuff--
I just type a couple letters,

00:00:41.540 --> 00:00:45.310
and then some automatic
machine completes the spelling.

00:00:45.310 --> 00:00:49.000
So this is just the first
and simplest instance

00:00:49.000 --> 00:00:51.010
of what we're all going
to encounter, according

00:00:51.010 --> 00:00:54.760
to the authors, so they're
going to spend the next 45 or 50

00:00:54.760 --> 00:00:59.660
minutes telling us about what
other jobs computers are going

00:00:59.660 --> 00:01:00.470
to take.

00:01:00.470 --> 00:01:01.270
Thank you.

00:01:01.270 --> 00:01:02.330
ERIK BRYNJOLFSSON:
Thank you, Hal.

00:01:02.330 --> 00:01:04.454
It's a pleasure, and
according to our calculations,

00:01:04.454 --> 00:01:07.859
you will be the last person
to be replaced by a robot.

00:01:07.859 --> 00:01:09.900
Oh, you did make one
mistake in that intro, which

00:01:09.900 --> 00:01:12.455
is, we're not going to
speak for 40 or 50 minutes.

00:01:12.455 --> 00:01:13.830
The reason we're
here is, we want

00:01:13.830 --> 00:01:16.590
to hear a bit from you about all
the amazing things that you're

00:01:16.590 --> 00:01:19.100
doing, and the
questions that you have,

00:01:19.100 --> 00:01:21.100
so Andy and I going
to try and speak

00:01:21.100 --> 00:01:23.960
for maybe for 15 or 20
minutes, and then do Q&amp;A,

00:01:23.960 --> 00:01:25.960
and we'll answer
whatever questions you

00:01:25.960 --> 00:01:28.977
have about the research
we've been doing at MIT,

00:01:28.977 --> 00:01:30.560
and the other companies
we've visited,

00:01:30.560 --> 00:01:32.480
and the people we've
interacted with, and will

00:01:32.480 --> 00:01:34.950
hopefully will be a very
lively dialogue that way.

00:01:34.950 --> 00:01:38.640
We realize that the best way
to learn about these issues

00:01:38.640 --> 00:01:40.570
is not to retreat to
our offices at MIT,

00:01:40.570 --> 00:01:43.830
and close the doors and windows,
but to get out and interact,

00:01:43.830 --> 00:01:46.920
and hopefully it'll be something
that'll be beneficial to you,

00:01:46.920 --> 00:01:48.100
as well.

00:01:48.100 --> 00:01:49.690
Now, the reason
we wrote this book

00:01:49.690 --> 00:01:52.080
was, in part, out
of our own confusion

00:01:52.080 --> 00:01:54.442
about a number of trends
that were going on,

00:01:54.442 --> 00:01:55.900
and they came from
different groups

00:01:55.900 --> 00:01:57.024
that we've been talking to.

00:01:57.024 --> 00:02:01.130
We visited a lot of companies,
like Google and technology

00:02:01.130 --> 00:02:02.130
companies in the Valley.

00:02:02.130 --> 00:02:04.755
We also spent a lot of time with
technologists at the Media Lab

00:02:04.755 --> 00:02:07.160
and CSAIL, the computer
science and the iLab

00:02:07.160 --> 00:02:09.840
at MIT and elsewhere, and
heard about the wondrous things

00:02:09.840 --> 00:02:11.440
that technology could do.

00:02:11.440 --> 00:02:13.990
We had a better experience
riding in the driverless car

00:02:13.990 --> 00:02:16.110
than Hal did.

00:02:16.110 --> 00:02:18.640
And we came away
from that with a lot

00:02:18.640 --> 00:02:21.760
of the infectious
enthusiasm and optimism

00:02:21.760 --> 00:02:24.530
that a lot of technologists
have about being

00:02:24.530 --> 00:02:27.627
able to solve all sorts of
problems with technology.

00:02:27.627 --> 00:02:29.710
But we also spent a lot
of time with other groups,

00:02:29.710 --> 00:02:32.280
in particular economists.

00:02:32.280 --> 00:02:35.060
I was just at the American
Economic Association meetings

00:02:35.060 --> 00:02:38.910
in Philadelphia, and it reminded
me why the economists are often

00:02:38.910 --> 00:02:42.040
called the dismal science,
because they weren't nearly as

00:02:42.040 --> 00:02:45.080
optimistic as our
technology friends.

00:02:45.080 --> 00:02:47.660
They pointed out some stats
there weren't so encouraging.

00:02:47.660 --> 00:02:52.830
Median income has basically
stagnated since the 1990s.

00:02:52.830 --> 00:02:56.120
The employment population
ratio has fallen dramatically,

00:02:56.120 --> 00:02:58.640
and hasn't improved, although,
as you may have heard,

00:02:58.640 --> 00:03:00.760
unemployment has
gotten somewhat better.

00:03:00.760 --> 00:03:04.000
That's mainly because more
people have dropped out

00:03:04.000 --> 00:03:07.084
of the labor force, not so
much because people are hiring.

00:03:07.084 --> 00:03:08.875
And there's a whole
set of other statistics

00:03:08.875 --> 00:03:10.700
that aren't as
encouraging, and so you

00:03:10.700 --> 00:03:14.436
contrast not just the
rapid innovation, but also

00:03:14.436 --> 00:03:16.060
some of the encouraging
economic stats,

00:03:16.060 --> 00:03:19.800
like the fact that we have hit,
last month, the record wealth

00:03:19.800 --> 00:03:23.980
in the United States-- $77
trillion record GDP, record

00:03:23.980 --> 00:03:27.400
productivity, but yet these
numbers on median income

00:03:27.400 --> 00:03:30.460
and employment, and
it struck Andy and I

00:03:30.460 --> 00:03:36.190
as kind of a paradox, arguably
the great paradox of our era.

00:03:36.190 --> 00:03:38.170
Tracing the numbers
back a bit, we

00:03:38.170 --> 00:03:41.830
found that these numbers
weren't always so decoupled--

00:03:41.830 --> 00:03:44.550
that through most
of the 20th century,

00:03:44.550 --> 00:03:46.950
and from what we can tell,
the 19th century as well,

00:03:46.950 --> 00:03:48.580
although the data
aren't quite as good,

00:03:48.580 --> 00:03:52.070
these numbers all rose in
rose in tandem together.

00:03:52.070 --> 00:03:55.100
Productivity, employment,
median income--

00:03:55.100 --> 00:03:57.835
they were relatively
tightly coupled, but then

00:03:57.835 --> 00:04:01.590
starting about 15 years
ago, they became decoupled,

00:04:01.590 --> 00:04:05.400
with productivity and wealth
and aggregate continuing

00:04:05.400 --> 00:04:09.060
to grow significantly,
but the other numbers

00:04:09.060 --> 00:04:12.240
tailing off, or stagnating,
or even falling some.

00:04:12.240 --> 00:04:15.530
And this accounted for
these different perspectives

00:04:15.530 --> 00:04:18.290
that the technologist
or the techno-optimist

00:04:18.290 --> 00:04:20.394
had when they look at
one part of the story,

00:04:20.394 --> 00:04:21.810
versus some of the
economists, who

00:04:21.810 --> 00:04:24.040
were looking at another
part of the story.

00:04:24.040 --> 00:04:25.690
And we wanted to
try and reconcile

00:04:25.690 --> 00:04:26.730
how that could be true.

00:04:26.730 --> 00:04:28.380
I mean, was one
group right or wrong?

00:04:28.380 --> 00:04:30.390
And, in fact, they
were both right,

00:04:30.390 --> 00:04:32.580
but they were looking at
different parts of it,

00:04:32.580 --> 00:04:34.890
and furthermore, most
interestingly, we

00:04:34.890 --> 00:04:37.160
think that both
aspects of this-- both

00:04:37.160 --> 00:04:39.300
the record productivity
and wealth,

00:04:39.300 --> 00:04:42.160
and the stagnating
median income--

00:04:42.160 --> 00:04:44.680
actually have a
common cause, and that

00:04:44.680 --> 00:04:48.110
is the fact that technology
has been racing ahead,

00:04:48.110 --> 00:04:51.000
but many people have not
been keeping up with that.

00:04:51.000 --> 00:04:53.110
There's kind of a dirty
secret in economics

00:04:53.110 --> 00:04:54.960
that not many people
recognize, because it

00:04:54.960 --> 00:04:57.150
hasn't been so important
before, and that

00:04:57.150 --> 00:05:00.550
is that while technology
can grow the economic pie,

00:05:00.550 --> 00:05:03.300
and really is what some
economists jokingly say

00:05:03.300 --> 00:05:06.580
is the only free lunch
that we believe in,

00:05:06.580 --> 00:05:10.010
there is no economic law
that says that everyone's

00:05:10.010 --> 00:05:13.410
going to benefit from
those technological gains.

00:05:13.410 --> 00:05:16.540
It's entirely possible that some
people will be made worse off.

00:05:16.540 --> 00:05:19.320
You've heard of the buggy
whip manufacturers that

00:05:19.320 --> 00:05:21.650
were put out of business
when Henry Ford introduced

00:05:21.650 --> 00:05:23.020
the automobile.

00:05:23.020 --> 00:05:24.570
Well, sure, there
are some people

00:05:24.570 --> 00:05:26.220
who may be made worse off.

00:05:26.220 --> 00:05:28.190
It turns out that the
size of that group

00:05:28.190 --> 00:05:30.590
can be almost arbitrarily large.

00:05:30.590 --> 00:05:34.010
It could be 50%
or more of people

00:05:34.010 --> 00:05:36.450
who don't share
in those benefits,

00:05:36.450 --> 00:05:38.790
and the data suggests
that in the past 10,

00:05:38.790 --> 00:05:42.430
15 years, that's been the
pattern that's been emerging

00:05:42.430 --> 00:05:45.890
is that technology has
been making the pie bigger,

00:05:45.890 --> 00:05:48.510
but most of those
benefits have been

00:05:48.510 --> 00:05:51.740
accruing to a
relatively small group.

00:05:51.740 --> 00:05:54.440
Maybe 60 to 70% of them have
been accruing to about 1%

00:05:54.440 --> 00:05:57.490
of the people in the
past decade or so,

00:05:57.490 --> 00:05:59.840
and a lot of that, while
there are many forces at work,

00:05:59.840 --> 00:06:03.770
a lot of it has to do with
the nature of technology.

00:06:03.770 --> 00:06:08.310
To give you an example,
consider a simple application

00:06:08.310 --> 00:06:10.280
like tax preparation.

00:06:10.280 --> 00:06:12.500
So TurboTax can do your taxes.

00:06:12.500 --> 00:06:14.720
Many of you probably
use TurboTax.

00:06:14.720 --> 00:06:15.220
I do.

00:06:15.220 --> 00:06:19.620
It's a simple piece of software
that has codified that process,

00:06:19.620 --> 00:06:23.030
and reduced it to a basic
set of steps-- digitized it--

00:06:23.030 --> 00:06:25.100
and once it's become
digitized, guess what?

00:06:25.100 --> 00:06:26.360
You can make a copy of it.

00:06:26.360 --> 00:06:27.810
You could make 10 copies of it.

00:06:27.810 --> 00:06:29.970
You could make 100
million copies of it,

00:06:29.970 --> 00:06:32.980
and the marginal cost of
that is very, very low.

00:06:32.980 --> 00:06:37.850
Anything that's digital can be
reproduced at nearly zero cost.

00:06:37.850 --> 00:06:39.470
What's more, each
copy is virtually

00:06:39.470 --> 00:06:41.940
identical to the
original, and it

00:06:41.940 --> 00:06:44.470
could be transmitted
almost instantaneously

00:06:44.470 --> 00:06:46.030
anywhere on the planet.

00:06:46.030 --> 00:06:48.510
As Hal writes in his book,
"Information Rules," which

00:06:48.510 --> 00:06:50.640
you should all read if
you haven't already,

00:06:50.640 --> 00:06:55.640
these characteristics of
free, perfect, and instant

00:06:55.640 --> 00:06:58.280
are not common for most
other goods and services,

00:06:58.280 --> 00:07:00.550
to say the least, and they
lead to sometimes some

00:07:00.550 --> 00:07:03.480
unusual economics.

00:07:03.480 --> 00:07:05.340
Some of those economics
are what we call,

00:07:05.340 --> 00:07:08.495
winner take all markets,
because if someone has made

00:07:08.495 --> 00:07:10.953
a tax-preparation program, you
don't necessarily want to go

00:07:10.953 --> 00:07:13.670
out and buy the hundredth-best
or the thousandth-best

00:07:13.670 --> 00:07:15.486
or the 10,000th-best program.

00:07:15.486 --> 00:07:17.610
You want to buy probably
the best one you can find.

00:07:17.610 --> 00:07:19.660
Maybe there's some
room for a small amount

00:07:19.660 --> 00:07:22.290
of differentiation, if people
have slightly different kinds

00:07:22.290 --> 00:07:26.360
of needs, and so a very
small group-- maybe one--

00:07:26.360 --> 00:07:29.780
will win the dominant
share of the market.

00:07:29.780 --> 00:07:31.540
Sticking with tax
preparation, that

00:07:31.540 --> 00:07:36.330
means that the people at Intuit
have done very, very well.

00:07:36.330 --> 00:07:39.050
There are some
billionaires there now.

00:07:39.050 --> 00:07:41.950
All of us, as consumers, have
done well, because for $39

00:07:41.950 --> 00:07:43.910
we can get tax
preparation, and we

00:07:43.910 --> 00:07:46.750
have a lot of consumer surplus
and benefits-- ease of use--

00:07:46.750 --> 00:07:48.370
that we wouldn't
have had otherwise.

00:07:48.370 --> 00:07:50.670
But there are a bunch of
people in the tax-preparation

00:07:50.670 --> 00:07:53.350
business, at H&amp;R
Block and elsewhere,

00:07:53.350 --> 00:07:55.279
that may have had a
college education,

00:07:55.279 --> 00:07:56.820
may have had some
skills that they've

00:07:56.820 --> 00:08:00.010
invested a long time in,
and now they're not really

00:08:00.010 --> 00:08:01.380
adding a lot of value anymore.

00:08:01.380 --> 00:08:04.520
What they do isn't
really all that useful

00:08:04.520 --> 00:08:07.720
to most of the rest of
us, and as a result,

00:08:07.720 --> 00:08:10.900
the ranks of people who say
their job is tax preparation

00:08:10.900 --> 00:08:14.610
has fallen dramatically over
the past five or six years.

00:08:14.610 --> 00:08:16.360
And it's, of course,
not because taxes

00:08:16.360 --> 00:08:17.720
have gotten so much easier.

00:08:17.720 --> 00:08:21.340
It's because technology
can now do that job.

00:08:21.340 --> 00:08:23.880
Now, that's just one example,
but it's, in many ways,

00:08:23.880 --> 00:08:28.310
a microcosm of what's happening
in many other industries, where

00:08:28.310 --> 00:08:30.610
you can codify more
and more processes,

00:08:30.610 --> 00:08:34.710
and reduce them into a digital
form, and then replicate them.

00:08:34.710 --> 00:08:37.039
Marc Andreessen
has famously said

00:08:37.039 --> 00:08:39.400
that software is
eating the world,

00:08:39.400 --> 00:08:41.890
and he's got a point
that this digitization is

00:08:41.890 --> 00:08:44.350
at the core of more
and more industries--

00:08:44.350 --> 00:08:47.490
not just the traditional
software industries,

00:08:47.490 --> 00:08:54.060
but, of course, music and
media, manufacturing, finance,

00:08:54.060 --> 00:08:54.829
retailing.

00:08:54.829 --> 00:08:56.870
In fact, it's hard to
think of an industry that's

00:08:56.870 --> 00:08:59.460
not affected, with the possible
exception of our industry,

00:08:59.460 --> 00:09:02.270
education, which I
think is next in line

00:09:02.270 --> 00:09:04.810
for that kind of
revolution, and that

00:09:04.810 --> 00:09:06.740
means that the same
kind of not just

00:09:06.740 --> 00:09:09.330
technology, but the
same kind of economics

00:09:09.330 --> 00:09:12.370
is coming to more
and more industries.

00:09:12.370 --> 00:09:14.960
A number of economists,
including our colleague, David

00:09:14.960 --> 00:09:17.500
Autor, and others
at MIT have a look

00:09:17.500 --> 00:09:20.830
at what's happened to what they
call the skill content of jobs.

00:09:20.830 --> 00:09:23.470
You take the overall
occupations in the economy,

00:09:23.470 --> 00:09:26.480
and look at the underlying
skills required to do them,

00:09:26.480 --> 00:09:28.520
what you find is
that those jobs that

00:09:28.520 --> 00:09:32.060
involve routine
information-processing tasks,

00:09:32.060 --> 00:09:35.520
like, for instance, tax
preparation, but also travel

00:09:35.520 --> 00:09:39.480
agents, bookkeepers,
secretaries-- those jobs have

00:09:39.480 --> 00:09:44.010
been the ones that have shrunk
the most over the past decade,

00:09:44.010 --> 00:09:48.660
because automation has
affected those tasks the most.

00:09:48.660 --> 00:09:52.250
Now, in doing the
research for this book,

00:09:52.250 --> 00:09:55.320
Andy and I spent time visiting
companies like Google,

00:09:55.320 --> 00:09:56.927
and we came to the
conclusion that it

00:09:56.927 --> 00:09:58.510
wasn't just that
machines were getting

00:09:58.510 --> 00:10:01.210
good at routine
information-processing tasks,

00:10:01.210 --> 00:10:03.901
but they're getting good at
a broader and broader set

00:10:03.901 --> 00:10:04.400
of tasks.

00:10:04.400 --> 00:10:05.983
I think Andy may be
talking about some

00:10:05.983 --> 00:10:08.840
of those in a little while,
but there's a whole wider

00:10:08.840 --> 00:10:12.740
set of applications, and really,
in some ways, amazing things

00:10:12.740 --> 00:10:14.640
that machines can do
now, many of which

00:10:14.640 --> 00:10:16.780
I think people in this
room are probably inventing

00:10:16.780 --> 00:10:19.190
and discovering, and
that means that as

00:10:19.190 --> 00:10:22.280
disruptive as the past 10
years were-- and it was

00:10:22.280 --> 00:10:25.500
a very disruptive decade-- we
think that the next decade is

00:10:25.500 --> 00:10:28.060
going to be even more
disruptive, as it's going

00:10:28.060 --> 00:10:32.440
to create even more wealth, even
more productivity, even more

00:10:32.440 --> 00:10:34.080
possibilities, but
also, it's going

00:10:34.080 --> 00:10:37.080
to change the skill
content of more jobs.

00:10:37.080 --> 00:10:39.220
It's going to mean
that more people will

00:10:39.220 --> 00:10:41.290
find that the existing
tasks they're doing

00:10:41.290 --> 00:10:44.789
aren't adding value to the
economy in the same way

00:10:44.789 --> 00:10:47.330
that they were before, because
they find themselves competing

00:10:47.330 --> 00:10:49.180
with machines,
and you don't want

00:10:49.180 --> 00:10:53.240
to be competing with a $39 piece
of software if you're human.

00:10:53.240 --> 00:10:56.660
Not many people want to work
for those kinds of wages.

00:10:56.660 --> 00:11:01.670
So in the end, do we come
away with this analysis

00:11:01.670 --> 00:11:05.490
as optimists, or
are we pessimists?

00:11:05.490 --> 00:11:09.100
Andy and I don't put ourselves
in either the utopian category

00:11:09.100 --> 00:11:11.040
or the stagnationist category.

00:11:11.040 --> 00:11:14.020
We call ourselves
mindful optimists,

00:11:14.020 --> 00:11:17.840
because we believe
that we can address

00:11:17.840 --> 00:11:19.720
the challenges and the
opportunities created

00:11:19.720 --> 00:11:22.440
by these new technologies,
but that there's

00:11:22.440 --> 00:11:24.660
no technological
determinism that's

00:11:24.660 --> 00:11:30.066
going to automatically lead
to one future or another.

00:11:30.066 --> 00:11:32.210
The book is called "The
Second Machine Age,"

00:11:32.210 --> 00:11:35.180
but we learn a lot by looking
at the first machine age, which

00:11:35.180 --> 00:11:40.170
was the Industrial Revolution,
and in the Industrial

00:11:40.170 --> 00:11:42.880
Revolution, there were
some new technologies,

00:11:42.880 --> 00:11:45.700
some wondrous new technologies
that came along that bent

00:11:45.700 --> 00:11:49.250
the curve of human history,
that took us from a relatively

00:11:49.250 --> 00:11:51.690
stagnant previous
several millennia,

00:11:51.690 --> 00:11:53.540
in terms of living
standards, population,

00:11:53.540 --> 00:11:58.010
and many other metrics, towards
an explosion of productivity

00:11:58.010 --> 00:12:00.220
and increased living standards.

00:12:00.220 --> 00:12:06.750
And those technologies primarily
automated physical tasks--

00:12:06.750 --> 00:12:09.520
automated our muscles,
like the steam engine,

00:12:09.520 --> 00:12:13.300
and later, technologies like
the internal combustion engine.

00:12:13.300 --> 00:12:15.660
The second machine
age is much more

00:12:15.660 --> 00:12:19.400
about automating cognitive
tasks, mental work,

00:12:19.400 --> 00:12:23.000
and we think it's going to have
equally profound implications.

00:12:23.000 --> 00:12:25.380
While, when you automate
make muscle tasks,

00:12:25.380 --> 00:12:28.630
it actually increases the
demand for control systems,

00:12:28.630 --> 00:12:31.190
or cognitive tasks,
and in that sense,

00:12:31.190 --> 00:12:34.510
humans tend to be complements
for the automated power

00:12:34.510 --> 00:12:35.570
systems.

00:12:35.570 --> 00:12:38.650
It's not so clear that
automating cognitive tasks

00:12:38.650 --> 00:12:42.460
and augmenting cognitive tasks
is going to be complementary,

00:12:42.460 --> 00:12:45.940
versus a substitute,
for human labor.

00:12:45.940 --> 00:12:48.460
And as we work through
the implications

00:12:48.460 --> 00:12:50.150
of the second
machine age, we need

00:12:50.150 --> 00:12:53.020
to think about how we can
make the kinds of adjustments

00:12:53.020 --> 00:12:54.600
we made in the
first machine age,

00:12:54.600 --> 00:12:57.130
so that we don't just
get the increased

00:12:57.130 --> 00:13:00.580
wealth and productivity,
but also the kind of shared

00:13:00.580 --> 00:13:04.620
prosperity that we had through
most of the past 200 years.

00:13:04.620 --> 00:13:06.620
So with that, let me turn
it over to Andy, who's

00:13:06.620 --> 00:13:08.590
going to flesh some of
that out, and then we

00:13:08.590 --> 00:13:11.670
want to turn to all of you for
some questions and comments,

00:13:11.670 --> 00:13:13.337
and have a robust
discussion about this.

00:13:13.337 --> 00:13:15.045
ANDREW MCAFEE: Yeah,
and thank you, Erik.

00:13:15.045 --> 00:13:17.670
As Erik says, the main thing
we want to do is talk with you

00:13:17.670 --> 00:13:20.360
all, so I don't want to
take up too much time.

00:13:20.360 --> 00:13:21.570
I want to do two things.

00:13:21.570 --> 00:13:23.850
I want to talk about
where this book came from,

00:13:23.850 --> 00:13:26.270
what its origin
was, and then what

00:13:26.270 --> 00:13:28.467
its goals became, as
we were writing it,

00:13:28.467 --> 00:13:30.550
because if anyone tells
you they know what they're

00:13:30.550 --> 00:13:32.440
trying to accomplish
exactly with the book

00:13:32.440 --> 00:13:35.230
when they start writing it,
they have just lied to you.

00:13:35.230 --> 00:13:38.900
So I want to talk about what our
goals became, as we fleshed out

00:13:38.900 --> 00:13:41.330
this book that became
"The Second Machine Age."

00:13:41.330 --> 00:13:42.830
I think if honestly
if there were

00:13:42.830 --> 00:13:47.780
one day where I had
this eureka moment,

00:13:47.780 --> 00:13:51.390
and where I realized that we
needed to go, first of all,

00:13:51.390 --> 00:13:53.750
get a lot smarter
ourselves, and then start

00:13:53.750 --> 00:13:55.590
writing and publicizing
it to the world,

00:13:55.590 --> 00:13:59.670
it would be in the fall of 2010,
when I'm reading the "New York

00:13:59.670 --> 00:14:02.890
Times" online, and there
is a story about the fact

00:14:02.890 --> 00:14:06.520
that this company developed a
car that was driving itselves

00:14:06.520 --> 00:14:10.370
on American roads in traffic,
with no human intervention

00:14:10.370 --> 00:14:11.490
whatsoever.

00:14:11.490 --> 00:14:15.110
And I spit my coffee at that
point for a very specific

00:14:15.110 --> 00:14:18.459
reason-- this was not
supposed to happen.

00:14:18.459 --> 00:14:20.000
You all were not
supposed to do this.

00:14:20.000 --> 00:14:21.110
Shame on you.

00:14:21.110 --> 00:14:23.870
But but more broadly,
the self-driving car

00:14:23.870 --> 00:14:26.930
was supposed to remain firmly
in the realm of science fiction,

00:14:26.930 --> 00:14:29.030
and not over in the
realm of reality.

00:14:29.030 --> 00:14:31.320
We knew this because a
couple of colleagues of ours

00:14:31.320 --> 00:14:34.110
had written a fantastic
book six years earlier,

00:14:34.110 --> 00:14:38.870
in 2004, explaining, to
our complete satisfaction,

00:14:38.870 --> 00:14:41.960
why this driving a car
was going to remain over

00:14:41.960 --> 00:14:43.780
on the human side of
the balance sheet,

00:14:43.780 --> 00:14:45.350
and not over on
the digital side.

00:14:45.350 --> 00:14:47.450
And they made a very
convincing argument

00:14:47.450 --> 00:14:49.570
that the pattern-matching
work that

00:14:49.570 --> 00:14:51.650
had to go on-- high-stakes
pattern-matching

00:14:51.650 --> 00:14:52.415
to drive a car.

00:14:52.415 --> 00:14:54.920
If you get it wrong, you're
going to hurt somebody.

00:14:54.920 --> 00:14:56.810
The amount of
information you had

00:14:56.810 --> 00:14:59.190
to take in, the different
sensory channels you had

00:14:59.190 --> 00:15:01.930
to incorporate, the formal
and the informal rules

00:15:01.930 --> 00:15:04.110
of the road that you had
to be aware of-- this

00:15:04.110 --> 00:15:05.790
was a daunting task.

00:15:05.790 --> 00:15:07.500
We humans are kind
of good at it.

00:15:07.500 --> 00:15:09.652
Technology was lousy
at it, and that

00:15:09.652 --> 00:15:11.110
was the way it was
going to remain,

00:15:11.110 --> 00:15:12.526
and Erik and I
both read the book.

00:15:12.526 --> 00:15:14.720
We taught it in our
courses, we believed it,

00:15:14.720 --> 00:15:16.420
and then you all
screwed up the equation

00:15:16.420 --> 00:15:19.630
by basically building a car
that did this kind of stuff.

00:15:19.630 --> 00:15:23.220
And then over and over
again, we saw these examples

00:15:23.220 --> 00:15:25.430
of science fiction
becoming reality,

00:15:25.430 --> 00:15:28.410
and a quote from Hemingway
kept echoing in my head.

00:15:28.410 --> 00:15:31.180
He has a wonderful quote
about how a man goes broke.

00:15:31.180 --> 00:15:34.690
He says it's gradually
and then suddenly.

00:15:34.690 --> 00:15:37.490
And we kept on seeing these
examples of really incremental,

00:15:37.490 --> 00:15:40.090
really kind of boring progress,
turning into holy cow,

00:15:40.090 --> 00:15:43.632
we're done, eureka, this
mission is accomplished.

00:15:43.632 --> 00:15:45.340
How many people have
heard of the problem

00:15:45.340 --> 00:15:46.800
in robotics of SLAM?

00:15:46.800 --> 00:15:49.190
I just want to see how
geeky this audience is.

00:15:49.190 --> 00:15:50.020
Oh, fantastic.

00:15:50.020 --> 00:15:52.330
I know something more
than a group at Google.

00:15:52.330 --> 00:15:54.020
I'm going to go
home happy today.

00:15:54.020 --> 00:15:57.620
SLAM is another thing that
we humans do really easily.

00:15:57.620 --> 00:16:01.230
Everyone, point
to the staircase.

00:16:01.230 --> 00:16:03.160
Duh.

00:16:03.160 --> 00:16:05.640
All of us could locate
where we are in the room

00:16:05.640 --> 00:16:07.360
right now, because
we engage in what's

00:16:07.360 --> 00:16:10.720
called Simultaneous
Localization and Mapping.

00:16:10.720 --> 00:16:13.210
All of us know where the
staircase is in the room.

00:16:13.210 --> 00:16:15.430
All of us know where the
doors and the hallways are.

00:16:15.430 --> 00:16:17.780
We know where we
are in that room.

00:16:17.780 --> 00:16:20.780
It turns out it's fiendishly
difficult, astonishingly

00:16:20.780 --> 00:16:24.420
difficult, to give a robot that
kind of capability, as well.

00:16:24.420 --> 00:16:26.620
There is a review of
the literature on SLAM

00:16:26.620 --> 00:16:29.450
that was published, I
believe, in 2008 that said,

00:16:29.450 --> 00:16:31.280
we've made essentially
no headway,

00:16:31.280 --> 00:16:33.530
and we can't see what's going
to change going forward.

00:16:33.530 --> 00:16:36.710
This is one of the holy grails
of the robotics discipline.

00:16:36.710 --> 00:16:39.560
A colleague of ours named
John Leonard, back at MIT,

00:16:39.560 --> 00:16:43.020
last year, or no more
than a couple years ago,

00:16:43.020 --> 00:16:46.350
essentially solved SLAM for
a medium-sized room-- I'm not

00:16:46.350 --> 00:16:50.330
making this up-- by
waving a Kinect around.

00:16:50.330 --> 00:16:52.850
So he took $150 of
consumer equipment,

00:16:52.850 --> 00:16:55.220
and basically solved this
long-standing challenge

00:16:55.220 --> 00:16:56.150
in robotics.

00:16:56.150 --> 00:16:58.340
So over and over again,
we kept on seeing

00:16:58.340 --> 00:17:03.590
these really nasty, thorny
problems just being blown away

00:17:03.590 --> 00:17:07.720
by smart geeks working with
incredibly powerful computers

00:17:07.720 --> 00:17:10.170
and oceans of data, and that
was really the motivation

00:17:10.170 --> 00:17:12.575
to write the book-- for,
first of all, for me and Erik

00:17:12.575 --> 00:17:15.339
to understand this world,
and then to communicate it.

00:17:15.339 --> 00:17:17.776
And then two other
goals-- the second thing

00:17:17.776 --> 00:17:19.359
I want to talk about
is the goals that

00:17:19.359 --> 00:17:22.220
appeared as we're writing the
book-- two other goals became

00:17:22.220 --> 00:17:22.960
apparent.

00:17:22.960 --> 00:17:24.440
The first was that
we felt like we

00:17:24.440 --> 00:17:26.770
had to do battle with
two forces out there,

00:17:26.770 --> 00:17:28.200
and Erik has talked about them.

00:17:28.200 --> 00:17:31.450
One is the force of,
there's nothing new here.

00:17:31.450 --> 00:17:32.720
There's nothing to see here.

00:17:32.720 --> 00:17:33.570
Move on.

00:17:33.570 --> 00:17:35.560
Technology has the same
role in the economy

00:17:35.560 --> 00:17:37.389
that it's always
had in the economy.

00:17:37.389 --> 00:17:39.930
It's not going to lead to any
bigger or different disruptions

00:17:39.930 --> 00:17:41.440
than has gone on before.

00:17:41.440 --> 00:17:43.260
I just don't believe
that, and I felt

00:17:43.260 --> 00:17:45.360
was important to try
to stake out that turf,

00:17:45.360 --> 00:17:47.990
and defend the claim
that honestly, this

00:17:47.990 --> 00:17:49.410
is some kind of new regime.

00:17:49.410 --> 00:17:51.820
This as a second
machine age here.

00:17:51.820 --> 00:17:54.470
The second one is kind of
the opposite of the nothing

00:17:54.470 --> 00:17:55.130
to see here.

00:17:55.130 --> 00:17:58.130
It's the nothing to worry
about here, because everything

00:17:58.130 --> 00:18:00.070
is going to be
hunky-dory because

00:18:00.070 --> 00:18:01.630
of technological progress.

00:18:01.630 --> 00:18:05.220
We need some utopians out there
who say, oh, you and your silly

00:18:05.220 --> 00:18:07.512
worries about jobs and wages
and workers and all that.

00:18:07.512 --> 00:18:08.970
We're creating a
world that's going

00:18:08.970 --> 00:18:10.980
to be so munificent that
essentially, everything's

00:18:10.980 --> 00:18:11.688
going to be free.

00:18:11.688 --> 00:18:12.960
No one's going to have a job.

00:18:12.960 --> 00:18:15.220
We can all just do Maker
Faire all the time,

00:18:15.220 --> 00:18:17.080
and things are going
to be fantastic.

00:18:17.080 --> 00:18:20.240
So what are you boring,
Northeast, academic people

00:18:20.240 --> 00:18:21.290
worried about?

00:18:21.290 --> 00:18:23.370
Just come hang out at
Food Camp for long enough,

00:18:23.370 --> 00:18:25.662
and you're going to walk
away just totally optimistic

00:18:25.662 --> 00:18:26.870
about the state of the world.

00:18:26.870 --> 00:18:28.320
We don't buy that, either.

00:18:28.320 --> 00:18:30.720
There are some real
challenges coming up

00:18:30.720 --> 00:18:33.710
because of this technological
progress, and to ignore them

00:18:33.710 --> 00:18:35.730
is a pretty bad idea.

00:18:35.730 --> 00:18:38.050
As Erik said, right
now it's pretty clear

00:18:38.050 --> 00:18:41.280
that there's some polarization
and some hollowing out going on

00:18:41.280 --> 00:18:45.400
in the middle class of America,
because the stereotypical,

00:18:45.400 --> 00:18:48.630
median, American worker does
not have a college education.

00:18:48.630 --> 00:18:52.880
Fewer than 40% of us do, and
is not doing manual labor.

00:18:52.880 --> 00:18:56.050
They're doing low-level,
fairly routine knowledge work.

00:18:56.050 --> 00:18:58.500
As with most of us
know, that is squarely

00:18:58.500 --> 00:19:01.740
in the sights of this wave of
progress that we're seeing,

00:19:01.740 --> 00:19:03.860
but that wave
doesn't stop there.

00:19:03.860 --> 00:19:08.570
Most of us would not call
a doctor a median worker.

00:19:08.570 --> 00:19:10.860
Most of us would not
call medical diagnosis

00:19:10.860 --> 00:19:13.910
a routine
information-processing task,

00:19:13.910 --> 00:19:16.540
but what Watson is doing--
since it's won "Jeopardy,"

00:19:16.540 --> 00:19:19.500
it needs a new job-- it
has gone to medical school,

00:19:19.500 --> 00:19:21.790
and I'm pretty convinced
that if it is not today

00:19:21.790 --> 00:19:25.250
the world's best diagnostician,
it will be fairly quickly.

00:19:25.250 --> 00:19:28.160
So these disruptions
in the labor force

00:19:28.160 --> 00:19:32.350
are not going to be confined
to that fairly low-wage, fairly

00:19:32.350 --> 00:19:33.310
average worker.

00:19:33.310 --> 00:19:36.460
They're going to go both
up-- up in the age, the wage,

00:19:36.460 --> 00:19:38.630
and the skill, and the
education distribution--

00:19:38.630 --> 00:19:41.630
and then downward, as well,
because we're doing things

00:19:41.630 --> 00:19:45.400
like learning how to pick fruit
automatically, which is not

00:19:45.400 --> 00:19:49.230
a prestigious job, but
certainly a human one now, too.

00:19:49.230 --> 00:19:51.580
And then our last
goal was to try

00:19:51.580 --> 00:19:53.900
to offer some prescriptions
about what to do,

00:19:53.900 --> 00:19:55.470
and I want to be pretty clear.

00:19:55.470 --> 00:19:57.310
Our prescriptions
are not out there

00:19:57.310 --> 00:20:01.530
in the singularity future
kind of time frame.

00:20:01.530 --> 00:20:05.410
Ray Kurzweil has the 2045
turf staked out pretty well.

00:20:05.410 --> 00:20:06.952
We don't want to go
encroach on that.

00:20:06.952 --> 00:20:08.410
We don't know if
we're going to get

00:20:08.410 --> 00:20:09.680
to his version of the future.

00:20:09.680 --> 00:20:13.640
One of the funniest things that
ever happened to me, as we were

00:20:13.640 --> 00:20:15.760
writing this book-- I
was on stage with a very

00:20:15.760 --> 00:20:18.460
prominent roboticist, who
I don't think I'll name,

00:20:18.460 --> 00:20:20.740
and he got asked a question
about the singularity.

00:20:20.740 --> 00:20:23.020
And he gave a response
that I thought

00:20:23.020 --> 00:20:25.182
was incredibly rude,
until I thought

00:20:25.182 --> 00:20:26.390
that was incredibly accurate.

00:20:26.390 --> 00:20:29.730
He said, look, Ray
Kurzweil is going to die.

00:20:29.730 --> 00:20:33.240
[LAUGHTER]

00:20:33.240 --> 00:20:33.860
All right.

00:20:33.860 --> 00:20:36.680
So we don't want to be out there
in the 2045 time framework.

00:20:36.680 --> 00:20:38.720
What Erik and I are
talking about in the book,

00:20:38.720 --> 00:20:41.950
and our recommendations,
focus on time frames a lot,

00:20:41.950 --> 00:20:45.060
a lot closer in than that,
and I can bucket them

00:20:45.060 --> 00:20:48.480
into two time phases-- things
we could and should do tomorrow,

00:20:48.480 --> 00:20:52.409
the immediate stuff, and then
the slightly longer-term stuff.

00:20:52.409 --> 00:20:53.950
The immediate stuff
is what you would

00:20:53.950 --> 00:20:57.380
learn if you pick any Econ
101 textbook off the shelf.

00:20:57.380 --> 00:20:59.730
I don't care if it was
written by Greg Mankiw, who

00:20:59.730 --> 00:21:02.560
advises Republican candidates,
or Paul Krugman, who's

00:21:02.560 --> 00:21:04.790
a pretty well-known
liberal economist.

00:21:04.790 --> 00:21:07.560
Their Econ 101 textbook
would tell a government

00:21:07.560 --> 00:21:11.180
to do a small set of
things, and to do them very,

00:21:11.180 --> 00:21:15.185
very well-- to do education,
to foster entrepreneurship

00:21:15.185 --> 00:21:17.600
in an environment, to
double down and make

00:21:17.600 --> 00:21:19.700
sure you've got
world-class infrastructure,

00:21:19.700 --> 00:21:22.410
and to have a fairly
welcoming immigration policy,

00:21:22.410 --> 00:21:24.220
and to invest in
original research.

00:21:24.220 --> 00:21:26.820
This is uncontroversial
among economists.

00:21:26.820 --> 00:21:29.980
We think it's exactly the right
the immediate-term playbook

00:21:29.980 --> 00:21:33.350
to restart economic growth, and
therefore, restart job and wage

00:21:33.350 --> 00:21:34.420
growth, as well.

00:21:34.420 --> 00:21:36.130
Looking out slightly
longer term,

00:21:36.130 --> 00:21:39.460
the crazy, radical ideas
that we have in the book

00:21:39.460 --> 00:21:43.230
are things like-- how many of us
have heard of the Earned Income

00:21:43.230 --> 00:21:44.860
Tax Credit?

00:21:44.860 --> 00:21:45.360
Good.

00:21:45.360 --> 00:21:47.860
This is basically a negative
income tax for people

00:21:47.860 --> 00:21:50.830
lower down on the wage
and income distribution.

00:21:50.830 --> 00:21:53.600
It rewards a dollar of
work with more than $1

00:21:53.600 --> 00:21:55.620
via the income-tax system.

00:21:55.620 --> 00:21:56.610
Great idea.

00:21:56.610 --> 00:21:59.360
Let's subsidize
labor, especially

00:21:59.360 --> 00:22:01.500
at the entry points
of the workforce,

00:22:01.500 --> 00:22:02.890
as heavily as we can.

00:22:02.890 --> 00:22:05.920
Milton Friedman, that
notorious socialist,

00:22:05.920 --> 00:22:09.910
advocated for a negative
income tax a lot.

00:22:09.910 --> 00:22:11.540
We think that's
a fantastic idea,

00:22:11.540 --> 00:22:14.460
so we believe there are more
than tweaks-- interventions

00:22:14.460 --> 00:22:17.260
you can do in things
like the tax code that

00:22:17.260 --> 00:22:20.890
will go a long way toward
putting people back to work.

00:22:20.890 --> 00:22:23.490
Farther out, again, we
are not singulartarians.

00:22:23.490 --> 00:22:26.350
We're not talking about the
economy of 50 years from now.

00:22:26.350 --> 00:22:29.910
For what it's worth, I think
it's a science-fiction economy.

00:22:29.910 --> 00:22:31.410
I think it's, we're
at the holodeck.

00:22:31.410 --> 00:22:33.055
I think we're in
some crazy place,

00:22:33.055 --> 00:22:34.430
because you all
are going to keep

00:22:34.430 --> 00:22:37.100
doing what you do for
the next 50 years.

00:22:37.100 --> 00:22:39.559
It's going to take us into a
world that I can't envision.

00:22:39.559 --> 00:22:41.350
What I do know, though,
is it could take us

00:22:41.350 --> 00:22:44.320
into a more utopian, kind
of a cool, "Star Trek,"

00:22:44.320 --> 00:22:45.820
explore new worlds.

00:22:45.820 --> 00:22:48.130
The phrase we use when
we're not talking to geeks

00:22:48.130 --> 00:22:51.460
is a digital Athens for that
world, where the citizens have

00:22:51.460 --> 00:22:53.400
this enlightened
discourse all the time,

00:22:53.400 --> 00:22:55.830
and instead of being
supported by human slaves,

00:22:55.830 --> 00:22:58.470
they're supported by
a big army of machines

00:22:58.470 --> 00:23:01.710
that do the hard, dirty,
heavy lifting in an economy.

00:23:01.710 --> 00:23:04.260
There are dystopian
strains, as well,

00:23:04.260 --> 00:23:06.495
and if we continue to
polarize the economy,

00:23:06.495 --> 00:23:09.380
if we provide no opportunities,
if mobility continues

00:23:09.380 --> 00:23:12.010
to go down-- those are
outcomes that I think push us

00:23:12.010 --> 00:23:14.477
in more of a
dystopian direction.

00:23:14.477 --> 00:23:17.060
The broadest point I think that
we make when we write the book

00:23:17.060 --> 00:23:20.292
is that the choices that
we all make, starting now,

00:23:20.292 --> 00:23:22.250
are going to determine
which of those two paths

00:23:22.250 --> 00:23:24.860
we create when we head
down, going forward.

00:23:24.860 --> 00:23:26.909
I think that's probably
a good place to shut up.

00:23:26.909 --> 00:23:29.450
Let's turn this open, and see
what people want to talk about.

00:23:29.450 --> 00:23:31.527
You've had your hand
up for a little while.

00:23:31.527 --> 00:23:33.110
AUDIENCE: Yeah, I
was just wondering--

00:23:33.110 --> 00:23:35.560
you were talking about
negative income taxes,

00:23:35.560 --> 00:23:37.110
and one thing that
comes to my mind

00:23:37.110 --> 00:23:40.880
is the patent-protection
system, where it essentially

00:23:40.880 --> 00:23:44.080
grants government-sanctioned
monopoly to the person doing

00:23:44.080 --> 00:23:50.150
the R&amp;D. If technology
improves, would you advocate

00:23:50.150 --> 00:23:52.870
some sort of reduction
in the patent window,

00:23:52.870 --> 00:23:55.660
because that would increase
competition, and then share?

00:23:55.660 --> 00:23:59.120
ANDREW MCAFEE: Yeah, this is
an amazingly controversial area

00:23:59.120 --> 00:24:03.130
of work among our colleagues--
IP log and IP regimes,

00:24:03.130 --> 00:24:03.661
in general.

00:24:03.661 --> 00:24:05.410
The stuff that makes
the most sense to me,

00:24:05.410 --> 00:24:06.826
and I want to hear
what Erik says,

00:24:06.826 --> 00:24:08.960
are that we've
probably gone too far

00:24:08.960 --> 00:24:11.570
with some categories
of IP protection.

00:24:11.570 --> 00:24:13.320
It's not clear to me
that software patents

00:24:13.320 --> 00:24:14.750
are doing more good than harm.

00:24:14.750 --> 00:24:17.060
We might want to revisit that.

00:24:17.060 --> 00:24:19.720
Companies like yours buy
these companies mainly

00:24:19.720 --> 00:24:21.460
as a war chest for
patents, and then you

00:24:21.460 --> 00:24:23.830
go rattle the saber
against some other company,

00:24:23.830 --> 00:24:25.660
and then you have some
kind of agreement.

00:24:25.660 --> 00:24:27.750
That's not value-added activity.

00:24:27.750 --> 00:24:29.930
Amazingly enough,
there's a cartoon

00:24:29.930 --> 00:24:32.140
made by Disney called
"Steamboat Willie"

00:24:32.140 --> 00:24:34.680
that is still under copyright.

00:24:34.680 --> 00:24:37.220
I think it was
made in the 1920s.

00:24:37.220 --> 00:24:39.250
It's the precursor to
Mickey Mouse, which

00:24:39.250 --> 00:24:42.470
Disney wants to hold
onto tenaciously.

00:24:42.470 --> 00:24:44.840
I'm not sure what purpose
is served by that.

00:24:44.840 --> 00:24:48.410
So I'm a huge fan of
property, in general,

00:24:48.410 --> 00:24:50.160
but I think our property
protections might

00:24:50.160 --> 00:24:52.520
have gone a bit too
far in the IP world.

00:24:52.520 --> 00:24:53.540
Does that make sense?

00:24:53.540 --> 00:24:55.956
ERIK BRYNJOLFSSON: Yeah, I
think you got it exactly right,

00:24:55.956 --> 00:24:58.520
and I would just
elaborate by saying

00:24:58.520 --> 00:25:00.912
that the purpose of
intellectual property, according

00:25:00.912 --> 00:25:02.620
to the Constitution,
and for that matter,

00:25:02.620 --> 00:25:05.203
according to economists, which,
in this case, happen to agree,

00:25:05.203 --> 00:25:10.010
is to foster the creative
arts, to foster R&amp;D,

00:25:10.010 --> 00:25:12.350
and I think, in
many cases, we've

00:25:12.350 --> 00:25:14.750
lost sight of that
with a regime.

00:25:14.750 --> 00:25:16.420
It's been taken
over by lobbyists.

00:25:16.420 --> 00:25:19.010
The copyright of Mickey
Mouse, and the fact

00:25:19.010 --> 00:25:21.120
it's been extended
and re-extended--

00:25:21.120 --> 00:25:24.240
there's the Sonny Bono
Copyright Extension Act--

00:25:24.240 --> 00:25:27.630
that basically retroactively
extended copyrights.

00:25:27.630 --> 00:25:30.240
It's hard to come up
with any rationale

00:25:30.240 --> 00:25:33.550
how that would increase the
incentive of people in 1920

00:25:33.550 --> 00:25:35.980
to make more inventions
by giving more

00:25:35.980 --> 00:25:38.882
wealth to the people who
own those copyrights today.

00:25:38.882 --> 00:25:40.840
So that's a perfect--
and then software patents

00:25:40.840 --> 00:25:45.280
is another one where, when you
build on previous innovations,

00:25:45.280 --> 00:25:48.390
then patents can
discourage innovation,

00:25:48.390 --> 00:25:49.870
rather than
providing incentives,

00:25:49.870 --> 00:25:53.510
because they basically take away
the returns from the people who

00:25:53.510 --> 00:25:55.960
are building the new
stuff, and giving it back

00:25:55.960 --> 00:25:58.040
to people who have built
the pre-existing stuff.

00:25:58.040 --> 00:25:59.414
So for all those
reasons, there's

00:25:59.414 --> 00:26:01.900
some serious re-thinking
that needs to be done,

00:26:01.900 --> 00:26:04.010
or actually, returning
to the old thinking that

00:26:04.010 --> 00:26:05.500
needs to be done.

00:26:05.500 --> 00:26:07.900
AUDIENCE: What do you think
are the greatest risks

00:26:07.900 --> 00:26:12.710
to your predictions about
rising unemployment and falling

00:26:12.710 --> 00:26:16.770
median income and hollowing
out of the middle class

00:26:16.770 --> 00:26:17.350
and so forth?

00:26:17.350 --> 00:26:19.100
ERIK BRYNJOLFSSON:
Well, just to be clear,

00:26:19.100 --> 00:26:21.120
we're not necessarily
predicting that.

00:26:21.120 --> 00:26:24.590
We see that as something that's
happened in the past few years,

00:26:24.590 --> 00:26:26.850
and as Andy said
at the end, and I

00:26:26.850 --> 00:26:29.970
tried to say at the end, as
well, the outcomes are going

00:26:29.970 --> 00:26:35.404
to be very much dependent on
our choices, and when you say,

00:26:35.404 --> 00:26:37.070
what are the risks,
do you mean in terms

00:26:37.070 --> 00:26:39.947
of risks of us getting it wrong,
or risks that if that happens,

00:26:39.947 --> 00:26:41.030
what will be the downside?

00:26:41.030 --> 00:26:42.680
There's two interpretations
of the word, risk.

00:26:42.680 --> 00:26:44.179
Which one do you
like us to address?

00:26:44.179 --> 00:26:45.650
AUDIENCE: What are
the factors most

00:26:45.650 --> 00:26:48.650
likely to prevent
those things happening?

00:26:48.650 --> 00:26:51.510
ANDREW MCAFEE: What can we do if
we don't like the trajectories?

00:26:51.510 --> 00:26:52.810
What can we do?

00:26:52.810 --> 00:26:56.600
AUDIENCE: Well, it's partly that
kind of proactive what can we

00:26:56.600 --> 00:26:59.910
do, but it might also
be, trends play out

00:26:59.910 --> 00:27:01.010
in a way very different--

00:27:01.010 --> 00:27:01.463
ERIK BRYNJOLFSSON:
Yeah, yeah, yeah--

00:27:01.463 --> 00:27:02.370
AUDIENCE: --than
you would assume.

00:27:02.370 --> 00:27:03.280
ERIK BRYNJOLFSSON: --and we
would like to see that happen.

00:27:03.280 --> 00:27:06.570
I mean, we can learn a little
bit from the first machine age,

00:27:06.570 --> 00:27:09.075
because that was also a
time of great disruption.

00:27:09.075 --> 00:27:12.560
It was almost exactly 200
years ago this year-- actually,

00:27:12.560 --> 00:27:15.270
it was-- that the
Luddites started

00:27:15.270 --> 00:27:18.060
smashing the looms in
the machines in England

00:27:18.060 --> 00:27:20.730
because they saw them
as taking away jobs.

00:27:20.730 --> 00:27:22.230
And they had a point
that there were

00:27:22.230 --> 00:27:25.630
a lot of people-- weavers--
who were losing their jobs,

00:27:25.630 --> 00:27:29.130
but at the end, new jobs
were created by the economy,

00:27:29.130 --> 00:27:31.950
and you could argue, and it'd
be a fact, that technology

00:27:31.950 --> 00:27:34.710
has always been destroying
jobs, and it's always

00:27:34.710 --> 00:27:38.110
been creating jobs, and over
time, historically we've

00:27:38.110 --> 00:27:41.620
created as many new jobs
as we've eliminated.

00:27:41.620 --> 00:27:45.890
Now, those trends have become
decoupled in the past 15 years.

00:27:45.890 --> 00:27:47.760
If you look historically,
one of the reasons

00:27:47.760 --> 00:27:53.400
that we did create new jobs
was because we, as individuals,

00:27:53.400 --> 00:27:56.010
as entrepreneurs, and
as governments, we

00:27:56.010 --> 00:27:58.350
made a whole series
of choices, which

00:27:58.350 --> 00:28:01.410
helped maintain
shared prosperity.

00:28:01.410 --> 00:28:05.250
Just to give you a
couple of examples--

00:28:05.250 --> 00:28:08.270
it's been called
America's best idea--

00:28:08.270 --> 00:28:11.420
is universal, mandatory
education, and that

00:28:11.420 --> 00:28:13.840
helped bring a
bunch of people who

00:28:13.840 --> 00:28:17.260
used to be farmers, and give
them a set of skills that made

00:28:17.260 --> 00:28:20.260
them suitable for a whole
set of other types of jobs

00:28:20.260 --> 00:28:22.540
and occupations, and
America led the world

00:28:22.540 --> 00:28:24.660
in pushing that, and
not coincidentally,

00:28:24.660 --> 00:28:27.840
led the world in shared
prosperity and living

00:28:27.840 --> 00:28:28.510
standards.

00:28:28.510 --> 00:28:30.290
But it wasn't just
some government action.

00:28:30.290 --> 00:28:31.790
It was also a bunch
of entrepreneurs

00:28:31.790 --> 00:28:34.840
that helped invent and
discover the new industries.

00:28:34.840 --> 00:28:37.560
90% of Americans used
to work in agriculture.

00:28:37.560 --> 00:28:39.350
Now it's less than 2%.

00:28:39.350 --> 00:28:42.180
That difference didn't join
the ranks of unemployed,

00:28:42.180 --> 00:28:43.017
thankfully.

00:28:43.017 --> 00:28:44.850
They were employed in
totally new industries

00:28:44.850 --> 00:28:47.790
that people like Henry Ford
and Steve Jobs and Bill Gates

00:28:47.790 --> 00:28:51.580
and Larry Page and Sergey
Brin invented that people back

00:28:51.580 --> 00:28:54.220
then probably could never
have even conceived of,

00:28:54.220 --> 00:28:56.070
so it's very much the
role of entrepreneurs

00:28:56.070 --> 00:28:59.310
to help identify what
those new industries are.

00:28:59.310 --> 00:29:01.820
The data say that right now,
we aren't doing it as fast

00:29:01.820 --> 00:29:03.040
as we used to.

00:29:03.040 --> 00:29:05.130
We'd like to see
more entrepreneurship

00:29:05.130 --> 00:29:07.800
to help speed up the creative
destruction, especially

00:29:07.800 --> 00:29:09.702
the creative side.

00:29:09.702 --> 00:29:11.410
Surprisingly, when we
looked at the data,

00:29:11.410 --> 00:29:13.610
we found that in
the 2000s, there

00:29:13.610 --> 00:29:16.710
were fewer new enterprises
and job creation

00:29:16.710 --> 00:29:18.560
than there were in
the '90s or the '80s,

00:29:18.560 --> 00:29:20.893
so we think of this as being
a very entrepreneurial era,

00:29:20.893 --> 00:29:24.030
but actually, the data suggests
that in some ways, it's not.

00:29:24.030 --> 00:29:26.530
If we could speed that up,
if we could, in some sense,

00:29:26.530 --> 00:29:28.290
crowdsource this
question of where will

00:29:28.290 --> 00:29:30.410
the jobs come from,
which is historically

00:29:30.410 --> 00:29:32.320
how we've solved the
problem, we think

00:29:32.320 --> 00:29:35.720
we'd be hopeful that we could
improve on that situation.

00:29:35.720 --> 00:29:39.460
But we have to say that in
fairness, we are not completely

00:29:39.460 --> 00:29:41.810
sure that the future
is going to play out

00:29:41.810 --> 00:29:44.220
just the way the past did.

00:29:44.220 --> 00:29:47.520
It's been said-- I heard
this from Hal Varian--

00:29:47.520 --> 00:29:50.945
that history doesn't
repeat itself,

00:29:50.945 --> 00:29:51.945
but sometimes it rhymes.

00:29:51.945 --> 00:29:54.495
I'm not sure where you
got that from, but--

00:29:54.495 --> 00:29:57.120
ANDREW MCAFEE: Hal Varian looks
a lot like Mark Twain, I think.

00:29:57.120 --> 00:29:57.930
ERIK BRYNJOLFSSON: Yeah,
yeah, I think maybe

00:29:57.930 --> 00:29:59.310
that's where he got it from.

00:29:59.310 --> 00:30:01.154
So maybe we will be
able to replicate

00:30:01.154 --> 00:30:02.820
some of the successes,
but I'm sure it's

00:30:02.820 --> 00:30:05.070
going to require some
different-- we can't just

00:30:05.070 --> 00:30:07.729
double down on doing the exact
same thing we did before.

00:30:07.729 --> 00:30:10.270
We're going to have to have some
different kinds of policies,

00:30:10.270 --> 00:30:12.561
and that's why we wanted to
start this conversation was

00:30:12.561 --> 00:30:14.550
to help identify what
those new changes are

00:30:14.550 --> 00:30:15.480
going to need to be.

00:30:15.480 --> 00:30:17.229
ANDREW MCAFEE: I like
your question a lot.

00:30:17.229 --> 00:30:21.310
I think the main thing that
might make the future play out

00:30:21.310 --> 00:30:23.510
differently than I am
envisioning right now

00:30:23.510 --> 00:30:26.600
is if Erik and I made the
classic geek mistake, when

00:30:26.600 --> 00:30:28.510
we were writing the
book, of becoming too

00:30:28.510 --> 00:30:30.730
enamored of the technologies
we were looking at,

00:30:30.730 --> 00:30:33.000
and projecting them
too far in the future,

00:30:33.000 --> 00:30:34.750
and thinking that their
impact is actually

00:30:34.750 --> 00:30:36.124
going to be bigger
than they are.

00:30:36.124 --> 00:30:39.580
I don't think we've made
that mistake, but if we have,

00:30:39.580 --> 00:30:42.640
then economic growth will
have some really different

00:30:42.640 --> 00:30:47.740
consequences, and the
employment impact of technology

00:30:47.740 --> 00:30:50.430
will be less than I am
currently thinking it is.

00:30:50.430 --> 00:30:52.200
That'd be kind of a
good mistake to make.

00:30:52.200 --> 00:30:53.991
AUDIENCE: So a little
more than a year ago,

00:30:53.991 --> 00:30:56.060
Paul Krugman wrote
an op-ed in which

00:30:56.060 --> 00:30:58.529
he observed that a lot
of the new technologies

00:30:58.529 --> 00:31:01.070
are going to start putting more
wealth in the hands of people

00:31:01.070 --> 00:31:03.590
that have capital start
with-- people that can afford

00:31:03.590 --> 00:31:07.190
the robot army that then
takes away manufacturing jobs.

00:31:07.190 --> 00:31:09.850
So I'm wondering what kind
of economic restructuring

00:31:09.850 --> 00:31:13.280
do you need to go through
to be able to deal

00:31:13.280 --> 00:31:14.687
with that new paradigm?

00:31:14.687 --> 00:31:16.270
It almost happened
before, like you've

00:31:16.270 --> 00:31:17.450
mentioned, in the
first machine age,

00:31:17.450 --> 00:31:19.783
but it seems like it's going
to be on a lot larger scale

00:31:19.783 --> 00:31:20.982
this time.

00:31:20.982 --> 00:31:23.190
ANDREW MCAFEE: The great
phrase that Krugman used was

00:31:23.190 --> 00:31:25.020
"robots and robber
barons," right,

00:31:25.020 --> 00:31:27.220
are going to get all the
benefits of the world

00:31:27.220 --> 00:31:28.950
that we're creating,
and he thinks

00:31:28.950 --> 00:31:30.340
that's a pretty bad outcome.

00:31:30.340 --> 00:31:31.570
We do, too.

00:31:31.570 --> 00:31:33.930
What can we do to make
sure that doesn't happen?

00:31:33.930 --> 00:31:37.010
Again, we're going to sound a
little bit like broken records.

00:31:37.010 --> 00:31:39.950
What we don't want to do, what
we're really, really careful--

00:31:39.950 --> 00:31:41.720
what we're sure we
don't want to do

00:31:41.720 --> 00:31:43.880
is storm the barricades
of capitalism,

00:31:43.880 --> 00:31:47.010
and fundamentally rethink the
economic contract right now,

00:31:47.010 --> 00:31:48.790
in 2014.

00:31:48.790 --> 00:31:50.770
Most of it's
working pretty well.

00:31:50.770 --> 00:31:52.990
The biggest problem
that you identify,

00:31:52.990 --> 00:31:56.270
and that we spend time on, is
it might get harder and harder

00:31:56.270 --> 00:31:58.750
to make a living
from your labor,

00:31:58.750 --> 00:32:00.430
as opposed to from your capital.

00:32:00.430 --> 00:32:03.670
If that's really what's going
on, let's remove the barriers.

00:32:03.670 --> 00:32:05.030
Let's try to upscale our labor.

00:32:05.030 --> 00:32:08.880
Let's remove any tax or whatever
distortions to doing that,

00:32:08.880 --> 00:32:12.340
and let's see how that goes
before we start thinking

00:32:12.340 --> 00:32:14.680
about getting rid of
capitalism or demonizing

00:32:14.680 --> 00:32:16.740
capitalists or things like that.

00:32:16.740 --> 00:32:18.830
AUDIENCE: So if you
assume that there's

00:32:18.830 --> 00:32:22.300
something called structural
unemployment, which,

00:32:22.300 --> 00:32:26.450
for my purpose of my
question, means someone whose

00:32:26.450 --> 00:32:30.120
potential benefits
of being employed

00:32:30.120 --> 00:32:33.770
are less than the costs
of employing them.

00:32:33.770 --> 00:32:36.410
I guess my question is,
do you think that exists?

00:32:36.410 --> 00:32:38.130
Do you think there's
a population that

00:32:38.130 --> 00:32:39.930
fits that category?

00:32:39.930 --> 00:32:41.404
How large do you
think it is today,

00:32:41.404 --> 00:32:43.070
and what do you think
is going to happen

00:32:43.070 --> 00:32:46.314
to that number over the
next coming decades?

00:32:46.314 --> 00:32:47.105
ANDREW MCAFEE: Sir?

00:32:47.105 --> 00:32:47.770
ERIK BRYNJOLFSSON: Sure.

00:32:47.770 --> 00:32:49.130
There are a number
of ways that you

00:32:49.130 --> 00:32:50.546
can get-- the
answer is yes, I can

00:32:50.546 --> 00:32:52.330
see there is some
structural unemployment,

00:32:52.330 --> 00:32:54.840
and there are a number of
ways you can get to that,

00:32:54.840 --> 00:32:58.010
and some of them are
very discouraging.

00:32:58.010 --> 00:33:02.740
If you look to-- in the 1930s,
when much of agriculture-- when

00:33:02.740 --> 00:33:04.500
tractors were
introduced, mechanizing

00:33:04.500 --> 00:33:06.950
a lot of agriculture, Joe
Stiglitz and others have

00:33:06.950 --> 00:33:09.370
pointed out that
took tens of millions

00:33:09.370 --> 00:33:11.180
of people who used
to work on farms,

00:33:11.180 --> 00:33:12.680
and threw them into
the labor force,

00:33:12.680 --> 00:33:16.040
and it caused a
macroeconomic, or helped

00:33:16.040 --> 00:33:18.820
contribute to a macroeconomic
recession, or the Great

00:33:18.820 --> 00:33:22.810
Depression, and it took a
decade or more for many of them

00:33:22.810 --> 00:33:24.840
to be reskilled, and
for new industries

00:33:24.840 --> 00:33:26.350
to crop up to hire them.

00:33:26.350 --> 00:33:29.240
And you can easily see
that that kind of mechanism

00:33:29.240 --> 00:33:32.790
could happen today, not with
farmers, because there aren't

00:33:32.790 --> 00:33:34.840
any of them left,
hardly-- there's only 2%

00:33:34.840 --> 00:33:38.070
of the population
and falling-- but

00:33:38.070 --> 00:33:41.010
in, say, routine
information-processing tasks.

00:33:41.010 --> 00:33:44.580
And one of the things that
concerns me is that by the time

00:33:44.580 --> 00:33:48.480
you find new skills and new
jobs for those people to do,

00:33:48.480 --> 00:33:50.611
what if the technology
has changed again?

00:33:50.611 --> 00:33:52.110
What if you have
another disruption,

00:33:52.110 --> 00:33:56.540
and then you're in this constant
pattern of trying to catch up

00:33:56.540 --> 00:33:58.100
with what's going on before?

00:33:58.100 --> 00:34:00.300
And if the pace of
technology speeds up,

00:34:00.300 --> 00:34:02.050
then you could see
that not only would you

00:34:02.050 --> 00:34:03.550
have structural
unemployment, but it

00:34:03.550 --> 00:34:06.930
could get more and
more severe over time.

00:34:06.930 --> 00:34:08.770
One of the ways this
manifests itself is not

00:34:08.770 --> 00:34:12.159
just through unemployment,
but also through inequality.

00:34:12.159 --> 00:34:13.812
If you're a true
free-market economist,

00:34:13.812 --> 00:34:15.520
you say, well, markets
will always clear.

00:34:15.520 --> 00:34:18.330
I mean, you just have let
them clear, and at some level,

00:34:18.330 --> 00:34:19.429
that's what the math says.

00:34:19.429 --> 00:34:21.860
That's true, but the
wage at which they clear,

00:34:21.860 --> 00:34:25.300
the price at which they clear,
could be arbitrarily low.

00:34:25.300 --> 00:34:26.920
It could be $5 an hour.

00:34:26.920 --> 00:34:28.219
It could be $0.05 an hour.

00:34:28.219 --> 00:34:31.030
It could be five-hundreth
of a cent an hour.

00:34:31.030 --> 00:34:33.589
And I think most of us would
agree that at least two

00:34:33.589 --> 00:34:35.630
of those numbers, and
probably all three of them,

00:34:35.630 --> 00:34:37.730
aren't ones we feel very
comfortable as seeing

00:34:37.730 --> 00:34:40.108
as the clearing wage for labor.

00:34:40.108 --> 00:34:41.399
AUDIENCE: It could be negative.

00:34:41.399 --> 00:34:42.982
ERIK BRYNJOLFSSON:
Well, if you figure

00:34:42.982 --> 00:34:46.600
the cost of hiring somebody--
like right now, if you look

00:34:46.600 --> 00:34:49.500
at-- somebody gave me
a horse, and said here,

00:34:49.500 --> 00:34:52.232
you can have a horse, and you
can use it to ride to work

00:34:52.232 --> 00:34:53.940
and whatever else you
want to do with it,

00:34:53.940 --> 00:34:55.870
I'd say, thanks, but no thanks.

00:34:55.870 --> 00:35:00.954
I really, even if I got for
free, it wouldn't be worth it.

00:35:00.954 --> 00:35:03.370
And so you're certainly right
that an input of production,

00:35:03.370 --> 00:35:06.210
once you cover the other
costs associated with it,

00:35:06.210 --> 00:35:08.310
even 0 might be
too high a price.

00:35:08.310 --> 00:35:10.851
ANDREW MCAFEE: But the question
that Erik and I have wrestled

00:35:10.851 --> 00:35:13.730
with the hardest, as we wrote
this book-- we've had not just

00:35:13.730 --> 00:35:16.580
bantering, but honest arguments
and disagreements about it--

00:35:16.580 --> 00:35:20.170
is, are we heading
into an area where

00:35:20.170 --> 00:35:22.010
the average worker is a horse?

00:35:22.010 --> 00:35:24.990
In other words, I couldn't walk
up to a factory or a warehouse

00:35:24.990 --> 00:35:28.202
or an office park with
a horse and say, hey,

00:35:28.202 --> 00:35:30.410
you can have this horse for
a tenth of a penny a day.

00:35:30.410 --> 00:35:31.080
Put it to work.

00:35:31.080 --> 00:35:32.490
They'd say, get lost.

00:35:32.490 --> 00:35:35.420
I have no use for that input
in my system of production

00:35:35.420 --> 00:35:36.040
anymore.

00:35:36.040 --> 00:35:38.585
We're not headed there
tomorrow or next year

00:35:38.585 --> 00:35:41.860
or next five years, even with
this ridiculous technological

00:35:41.860 --> 00:35:42.380
progress.

00:35:42.380 --> 00:35:44.856
The question I honestly
don't know the answer to

00:35:44.856 --> 00:35:49.600
is, are some of us heading into
horse territory in the economy

00:35:49.600 --> 00:35:50.590
that we're creating?

00:35:50.590 --> 00:35:52.220
Most of the really
smart economists

00:35:52.220 --> 00:35:55.440
that we talk to find that a
fairly ridiculous proposition.

00:35:55.440 --> 00:35:57.610
I was talking earlier
this week with a guy who's

00:35:57.610 --> 00:36:00.500
got a Nobel Prize, and I
said, does your intuition

00:36:00.500 --> 00:36:02.440
tell you that if we
play our cards right--

00:36:02.440 --> 00:36:04.350
and that's not a
given, obviously,

00:36:04.350 --> 00:36:06.040
with the polarization
in Washington

00:36:06.040 --> 00:36:08.110
and everything-- if we
play our cards right,

00:36:08.110 --> 00:36:10.630
though, that we can be back
to a full-employment economy

00:36:10.630 --> 00:36:13.760
after this nasty transition
maybe lasting 20 years?

00:36:13.760 --> 00:36:16.420
And he said very confidently,
oh, yeah, absolutely.

00:36:16.420 --> 00:36:18.570
We can get back to a
full-employment economy.

00:36:18.570 --> 00:36:21.610
I just find myself less
confident than that.

00:36:21.610 --> 00:36:23.725
And again, I want to
stress, it's your fault.

00:36:23.725 --> 00:36:25.774
[LAUGHTER]

00:36:25.774 --> 00:36:28.190
AUDIENCE: There's no doubt
that technology is contributing

00:36:28.190 --> 00:36:30.420
to the hollowing out
of the middle class,

00:36:30.420 --> 00:36:33.730
but there's another effect,
which is technologically

00:36:33.730 --> 00:36:35.260
related, and I'm
not sure if you're

00:36:35.260 --> 00:36:36.926
talking about it
directly or indirectly,

00:36:36.926 --> 00:36:39.170
but just the fact that
communication barriers have

00:36:39.170 --> 00:36:42.270
come down, and so now a
lot of knowledge work that

00:36:42.270 --> 00:36:44.820
used to be done in America
can now be done elsewhere,

00:36:44.820 --> 00:36:46.650
which is good for
other countries,

00:36:46.650 --> 00:36:48.190
but just not so
great for this one.

00:36:48.190 --> 00:36:52.440
So how do those two
variables interact,

00:36:52.440 --> 00:36:55.827
and what will the trend be?

00:36:55.827 --> 00:36:57.410
ERIK BRYNJOLFSSON:
Well, you're right.

00:36:57.410 --> 00:36:59.690
I think that there are
two big trends that

00:36:59.690 --> 00:37:01.460
are affecting the
economy today--

00:37:01.460 --> 00:37:02.890
globalization and technology.

00:37:02.890 --> 00:37:04.765
I mean, there are some
other, smaller trends,

00:37:04.765 --> 00:37:06.590
in terms of cultural
changes, unionization,

00:37:06.590 --> 00:37:08.620
but just focusing on
those two big ones,

00:37:08.620 --> 00:37:10.850
I agree entirely that
globalization is,

00:37:10.850 --> 00:37:13.666
in a large part, enabled by
some advances in technology,

00:37:13.666 --> 00:37:15.290
especially communications
technologies,

00:37:15.290 --> 00:37:18.140
also some transportation
technologies.

00:37:18.140 --> 00:37:21.320
But of the two, we think
that the technology one is

00:37:21.320 --> 00:37:24.220
the one with more legs, and
the one that's accelerating,

00:37:24.220 --> 00:37:26.560
and if you look
at a lot of kinds

00:37:26.560 --> 00:37:30.470
of jobs that have been offshored
historically, now many of them

00:37:30.470 --> 00:37:32.180
are coming back to
the United States.

00:37:32.180 --> 00:37:34.106
Which low-wage
country do you think

00:37:34.106 --> 00:37:35.480
Apple, on their
30th anniversary,

00:37:35.480 --> 00:37:38.620
is making their
Mac Pro computers?

00:37:38.620 --> 00:37:39.370
MALE SPEAKER: USA.

00:37:39.370 --> 00:37:40.030
ERIK BRYNJOLFSSON: USA.

00:37:40.030 --> 00:37:41.571
They're making them
in Austin, Texas,

00:37:41.571 --> 00:37:44.165
not because the wages
are so low in Texas--

00:37:44.165 --> 00:37:46.070
maybe we should say,
not just because they're

00:37:46.070 --> 00:37:50.480
so low in Texas-- but not
because of the wages there,

00:37:50.480 --> 00:37:53.190
but because there's hardly
any labor required anymore.

00:37:53.190 --> 00:37:55.030
They're bringing them
back here, and that's

00:37:55.030 --> 00:37:57.760
happening for a lot of
knowledge work, as well.

00:37:57.760 --> 00:37:59.640
Larry Summers pointed
out to us once

00:37:59.640 --> 00:38:01.830
when we were talking
about this that if you

00:38:01.830 --> 00:38:04.570
look at manufacturing
employment in China,

00:38:04.570 --> 00:38:07.030
how has that changed in
the past 15, 20 years,

00:38:07.030 --> 00:38:09.410
as manufacturing
has grown in China?

00:38:09.410 --> 00:38:10.650
It's actually fallen.

00:38:10.650 --> 00:38:12.890
There are 20
million fewer people

00:38:12.890 --> 00:38:17.030
working in manufacturing now
than there were 20 years ago,

00:38:17.030 --> 00:38:20.290
so manufacturing employment is
leaving both the United States

00:38:20.290 --> 00:38:26.430
and China, not going to
Vietnam, but going to robots.

00:38:26.430 --> 00:38:31.840
And so, in many ways, our
view is that if anything,

00:38:31.840 --> 00:38:33.450
offshoring is just
a weigh station

00:38:33.450 --> 00:38:35.390
on the road to
automation, and some

00:38:35.390 --> 00:38:38.080
of these low-wage
countries are even more

00:38:38.080 --> 00:38:41.330
in the bulls-eye of automation
in this second machine

00:38:41.330 --> 00:38:45.690
age than American or Western
European or Japanese workers,

00:38:45.690 --> 00:38:48.950
because the more routine the
tasks are that you're doing,

00:38:48.950 --> 00:38:52.210
the easier it is for someone
to write a piece of software,

00:38:52.210 --> 00:38:56.140
or a robot or a
program to do that.

00:38:56.140 --> 00:38:58.400
And if your competitive
advantage is, hey,

00:38:58.400 --> 00:39:00.270
we'll do it for
cheaper, well, I don't

00:39:00.270 --> 00:39:03.000
think that's a very
strong barrier to entry,

00:39:03.000 --> 00:39:05.110
compared to what's
happening with the pace

00:39:05.110 --> 00:39:06.680
of technological change.

00:39:06.680 --> 00:39:07.750
ANDREW MCAFEE: And
you asked specifically

00:39:07.750 --> 00:39:08.670
about knowledge work.

00:39:08.670 --> 00:39:11.110
I came across this really
interesting series of stories

00:39:11.110 --> 00:39:12.740
in the press awhile
back, because

00:39:12.740 --> 00:39:14.240
some different
reporters noticed it,

00:39:14.240 --> 00:39:17.100
and then they were calling
up their insurance company

00:39:17.100 --> 00:39:19.100
or their credit card
company or their bank.

00:39:19.100 --> 00:39:22.660
They seem to be interacting
with this perfectly accented

00:39:22.660 --> 00:39:25.750
American voice that was
giving them somewhat canned

00:39:25.750 --> 00:39:28.930
responses, with the same
sentences or sentence fragments

00:39:28.930 --> 00:39:30.100
over and over again.

00:39:30.100 --> 00:39:31.600
And so they pushed
on that, and they

00:39:31.600 --> 00:39:35.220
noticed that they think there
are two different strains going

00:39:35.220 --> 00:39:36.320
on right now.

00:39:36.320 --> 00:39:39.000
One is where there's a Filipino
listening to what they're

00:39:39.000 --> 00:39:41.870
saying, because English
skills are pretty good there,

00:39:41.870 --> 00:39:43.550
and then pressing
a button for which

00:39:43.550 --> 00:39:45.560
American-accented
response they're

00:39:45.560 --> 00:39:47.565
supposed to spit
back on the fly.

00:39:47.565 --> 00:39:49.660
The slightly more
advanced version of that

00:39:49.660 --> 00:39:52.190
is there's an algorithm
listening to what the person is

00:39:52.190 --> 00:39:54.310
saying, and the
algorithm hits the button

00:39:54.310 --> 00:39:56.950
for which pre-recorded human
voice should be given back

00:39:56.950 --> 00:39:57.950
to the person.

00:39:57.950 --> 00:40:00.560
Step three of that
progression, I believe, is--

00:40:00.560 --> 00:40:02.350
we have all seen
the movie "Her"--

00:40:02.350 --> 00:40:04.810
is Scarlett Johansson's
synthetic voice is just going

00:40:04.810 --> 00:40:06.010
to give us a response back.

00:40:06.010 --> 00:40:06.960
ERIK BRYNJOLFSSON: That's the
future we're all hoping for.

00:40:06.960 --> 00:40:09.400
ANDREW MCAFEE: Yeah, exactly,
can't get here soon enough.

00:40:09.400 --> 00:40:09.900
[LAUGHTER]

00:40:09.900 --> 00:40:13.140
ANDREW MCAFEE: But it's not
science fiction stretched

00:40:13.140 --> 00:40:16.250
anymore to imagine a completely
synthetic interaction

00:40:16.250 --> 00:40:16.990
like that.

00:40:16.990 --> 00:40:19.130
Again, for knowledge
work, we think

00:40:19.130 --> 00:40:21.180
that offshoring
is a weigh station

00:40:21.180 --> 00:40:23.450
on the way to automation.

00:40:23.450 --> 00:40:25.050
AUDIENCE: How much
do you think this

00:40:25.050 --> 00:40:28.469
is a US issue, versus
a global issue?

00:40:28.469 --> 00:40:30.760
ERIK BRYNJOLFSSON: Well, it's
certainly a global issue.

00:40:30.760 --> 00:40:32.370
I mean, if you look
at the-- I mean,

00:40:32.370 --> 00:40:34.640
a lot of the trends we gave
were using US numbers, frankly,

00:40:34.640 --> 00:40:36.070
because we are the most
familiar with them,

00:40:36.070 --> 00:40:38.000
and some of the best
data is available here,

00:40:38.000 --> 00:40:41.810
but if you look at the OECD,
the Organization of Economic

00:40:41.810 --> 00:40:45.220
Cooperation and Development,
the most developed countries,

00:40:45.220 --> 00:40:46.710
you see a very similar pattern.

00:40:46.710 --> 00:40:50.157
For instance, the hollowing
out of routine jobs

00:40:50.157 --> 00:40:52.490
has happened in virtually
every one of those industries,

00:40:52.490 --> 00:40:54.290
according to research by
my colleague, David Autor,

00:40:54.290 --> 00:40:54.990
and others.

00:40:54.990 --> 00:40:57.540
The inequality has grown
in all those countries.

00:40:57.540 --> 00:41:01.252
It has grown in Sweden,
Germany, France, Japan.

00:41:01.252 --> 00:41:02.710
There's only two
or three countries

00:41:02.710 --> 00:41:05.180
where it hasn't grown-- Greece
and, I think, one other one,

00:41:05.180 --> 00:41:06.596
and there's just
some weird things

00:41:06.596 --> 00:41:09.840
going on in those countries
that probably aren't typical.

00:41:09.840 --> 00:41:12.390
And as we just said, when you
look at the developing nations,

00:41:12.390 --> 00:41:14.394
you see, I think
arguably, that they

00:41:14.394 --> 00:41:15.810
may be even more
in the bulls-eye,

00:41:15.810 --> 00:41:19.040
and I think this could be a real
concern, because historically,

00:41:19.040 --> 00:41:22.640
having low wages and taking
some of those kinds of jobs

00:41:22.640 --> 00:41:25.510
in textiles and other areas
has been a path to development

00:41:25.510 --> 00:41:28.720
to get out of that
underdeveloped state.

00:41:28.720 --> 00:41:31.305
You for awhile do those
really low-wage jobs,

00:41:31.305 --> 00:41:33.180
and then, as you work
your way up the ladder,

00:41:33.180 --> 00:41:36.180
you start joining
the broader economy.

00:41:36.180 --> 00:41:39.990
If that path is cut off,
it could be very troubling.

00:41:39.990 --> 00:41:42.160
Those countries that
haven't made the transition

00:41:42.160 --> 00:41:44.949
may be stuck for a long time on
the other side of the ledger.

00:41:44.949 --> 00:41:46.490
ANDREW MCAFEE: There
was a nice piece

00:41:46.490 --> 00:41:49.100
of work done by a couple
of University of Chicago

00:41:49.100 --> 00:41:52.170
economists just recently
that looked at the returns

00:41:52.170 --> 00:41:55.710
to capital versus labor in many
countries around the world,

00:41:55.710 --> 00:41:58.210
and they found the pattern
was really similar, almost

00:41:58.210 --> 00:41:59.390
without exception.

00:41:59.390 --> 00:42:02.270
The share of total GDP that
was getting paid out in labor

00:42:02.270 --> 00:42:05.250
was heading south country
after country after country

00:42:05.250 --> 00:42:06.010
around the world.

00:42:06.010 --> 00:42:07.910
We can think of an
explanation for that

00:42:07.910 --> 00:42:09.200
doesn't involve technology.

00:42:09.200 --> 00:42:12.452
And in fact, they said the
explanation is explicitly

00:42:12.452 --> 00:42:13.410
information technology.

00:42:13.410 --> 00:42:14.120
ERIK BRYNJOLFSSON:
Right, and I think

00:42:14.120 --> 00:42:15.970
that that's a very important
point to emphasize,

00:42:15.970 --> 00:42:17.890
is because there are
people who say well, it's

00:42:17.890 --> 00:42:19.660
because of what
their Congress did,

00:42:19.660 --> 00:42:21.370
or what the guys in Wall
Street did, or whatever,

00:42:21.370 --> 00:42:23.550
and I'm sure there have been
a lot of bad things done

00:42:23.550 --> 00:42:25.383
in Washington and Wall
Street and elsewhere,

00:42:25.383 --> 00:42:29.840
but the fact that this pattern
is so pervasive and so global

00:42:29.840 --> 00:42:32.410
suggests to us, or is
additional evidence to us,

00:42:32.410 --> 00:42:34.710
that there are some more
fundamental economic forces

00:42:34.710 --> 00:42:36.570
and technological
forces at work.

00:42:36.570 --> 00:42:40.680
AUDIENCE: I think once you
have big problems like health,

00:42:40.680 --> 00:42:47.160
and legal system politics
become automated,

00:42:47.160 --> 00:42:48.900
like having a differential
diagnosis done

00:42:48.900 --> 00:42:50.422
by a machine or a
phone app, and it

00:42:50.422 --> 00:42:51.880
tells you exactly
what's happening,

00:42:51.880 --> 00:42:54.470
you probably have
more trust of a better

00:42:54.470 --> 00:42:57.010
diagnosis than the
best doctor out there,

00:42:57.010 --> 00:43:02.830
and for this layer of very
high-paid cognitive type

00:43:02.830 --> 00:43:07.410
of skills, there is a lot
of infrastructure built

00:43:07.410 --> 00:43:11.810
to protect this system, and a
lot of politics and lobbyists

00:43:11.810 --> 00:43:15.450
that want to make
sure that it stays.

00:43:15.450 --> 00:43:19.980
And can we create
a baseline for,

00:43:19.980 --> 00:43:23.280
let's say, before we
go that route, there

00:43:23.280 --> 00:43:27.810
needs to be a unanimous approval
on what can go-- not to serve

00:43:27.810 --> 00:43:31.150
a particular interest of a
certain group, but rather,

00:43:31.150 --> 00:43:32.380
humanity at large?

00:43:32.380 --> 00:43:34.477
ANDREW MCAFEE: So no is
the short answer to that.

00:43:34.477 --> 00:43:36.810
ERIK BRYNJOLFSSON: After that
polite way of [INAUDIBLE].

00:43:36.810 --> 00:43:36.840
[INTERPOSING VOICES]

00:43:36.840 --> 00:43:37.700
ANDREW MCAFEE: Yeah, it
was a lovely question.

00:43:37.700 --> 00:43:39.240
I'm going to dismissive answer.

00:43:39.240 --> 00:43:41.630
No, we certainly can't change
our political structure

00:43:41.630 --> 00:43:43.980
that deeply, so we have
to have a plebiscite

00:43:43.980 --> 00:43:46.020
before every instance
of technological change

00:43:46.020 --> 00:43:48.360
and lobbying protect-- we're
not going to get there.

00:43:48.360 --> 00:43:50.410
But you bring up a
really important point,

00:43:50.410 --> 00:43:52.950
which is that-- I
won't speak for Erik--

00:43:52.950 --> 00:43:55.600
I think that the main impediment
to a lot of the stuff,

00:43:55.600 --> 00:43:57.470
the great stuff, that
we're going to see

00:43:57.470 --> 00:43:59.740
is exactly the barriers
that you identify--

00:43:59.740 --> 00:44:03.730
our regulatory barriers that
are there to protect incumbents,

00:44:03.730 --> 00:44:06.600
as opposed to protecting
consumers-- they primarily

00:44:06.600 --> 00:44:09.420
have that effect-- are
lobbying groups or all

00:44:09.420 --> 00:44:12.570
the forces of inertia
that are trying

00:44:12.570 --> 00:44:13.840
to keep stuff from happening.

00:44:13.840 --> 00:44:16.260
When you look at the-- some
of Uber's recent troubles

00:44:16.260 --> 00:44:17.290
are self-inflicted.

00:44:17.290 --> 00:44:18.904
Let's be really
clear about that.

00:44:18.904 --> 00:44:20.570
Some of the other
ones have been brought

00:44:20.570 --> 00:44:22.700
on by exactly the
forces you identify.

00:44:22.700 --> 00:44:25.130
Did you all hear
that France is making

00:44:25.130 --> 00:44:29.430
Uber wait a minimum
of 15 minutes

00:44:29.430 --> 00:44:33.070
before the car can show up
purely to protect their taxis.

00:44:33.070 --> 00:44:36.050
This is insane, but we're
going to see all kinds of stuff

00:44:36.050 --> 00:44:37.524
like that go on.

00:44:37.524 --> 00:44:38.940
Stupid question
time-- how many of

00:44:38.940 --> 00:44:41.190
us have heard of Tim O'Reilly?

00:44:41.190 --> 00:44:41.690
Heh heh.

00:44:41.690 --> 00:44:42.700
Dumb, right?

00:44:42.700 --> 00:44:44.460
Tim has a beautiful
way to phrase it,

00:44:44.460 --> 00:44:46.084
and it's something
that I carry around.

00:44:46.084 --> 00:44:50.010
He says, look, with our policy
and our regulatory decisions,

00:44:50.010 --> 00:44:51.170
we have two choices.

00:44:51.170 --> 00:44:53.260
We can protect the
past from the future,

00:44:53.260 --> 00:44:55.870
or we can protect the
future from the past.

00:44:55.870 --> 00:44:58.950
For heaven's sake, let's protect
the future from the past.

00:44:58.950 --> 00:44:59.470
ERIK BRYNJOLFSSON:
Right, and that's

00:44:59.470 --> 00:45:02.080
not to say that there aren't
policies that we should adopt,

00:45:02.080 --> 00:45:06.250
but I 100% agree with Andy
that there's a natural reaction

00:45:06.250 --> 00:45:09.510
to either smash the machines,
the Luddite approach,

00:45:09.510 --> 00:45:14.690
or to freeze the existing
occupations and jobs the way

00:45:14.690 --> 00:45:17.560
they are through a whole set
of occupational licensing

00:45:17.560 --> 00:45:20.290
and another incumbent
protection acts.

00:45:20.290 --> 00:45:22.790
And while those seem
like natural ways

00:45:22.790 --> 00:45:24.580
to prevent some of
the bad outcomes,

00:45:24.580 --> 00:45:25.955
ultimately we
think they're going

00:45:25.955 --> 00:45:27.920
to be more destructive
than beneficial.

00:45:27.920 --> 00:45:29.920
The way that the economy
has thrived in the past

00:45:29.920 --> 00:45:31.760
has been what does
Joseph Schumpeter called

00:45:31.760 --> 00:45:34.400
creative destruction,
and the answer

00:45:34.400 --> 00:45:37.070
is not to try to slow
down the new ideas

00:45:37.070 --> 00:45:38.450
and the new technologies.

00:45:38.450 --> 00:45:41.280
It's to speed up our
adaptation, and the government

00:45:41.280 --> 00:45:44.270
can help in many ways by, for
instance, I mentioned earlier,

00:45:44.270 --> 00:45:48.050
universal public education was
one idea, and raising the skill

00:45:48.050 --> 00:45:48.910
levels.

00:45:48.910 --> 00:45:51.540
Encouraging, boosting, and
speeding entrepreneurship--

00:45:51.540 --> 00:45:53.020
not slowing it down.

00:45:53.020 --> 00:45:54.980
Doing things with tax
policy, doing things

00:45:54.980 --> 00:45:56.521
with R&amp;D. So there
are lots of things

00:45:56.521 --> 00:45:58.250
you can do to help
with the preconditions

00:45:58.250 --> 00:46:00.640
to smooth and speed
the transition,

00:46:00.640 --> 00:46:03.900
but freezing in the
historical structure

00:46:03.900 --> 00:46:08.419
is certainly going to
do more harm than good.

00:46:08.419 --> 00:46:09.960
AUDIENCE: Hi, so
companies like Apple

00:46:09.960 --> 00:46:12.560
make money by building fancy
gadgets to sell to consumers,

00:46:12.560 --> 00:46:15.190
and companies like Google make
money by allowing advertisers

00:46:15.190 --> 00:46:17.460
to advertise their fancy gadgets
to consumers, and all of this

00:46:17.460 --> 00:46:19.660
requires that the consumers
are there, and they have jobs,

00:46:19.660 --> 00:46:19.780
and they have--

00:46:19.780 --> 00:46:20.696
MALE SPEAKER 2: Money.

00:46:20.696 --> 00:46:21.840
AUDIENCE: --money.

00:46:21.840 --> 00:46:23.826
So in a sense, we
have an interest

00:46:23.826 --> 00:46:25.700
in making sure that this
situation you talked

00:46:25.700 --> 00:46:29.160
about where workers turn into
horses doesn't actually happen.

00:46:29.160 --> 00:46:31.327
Are there any economic
models which talk about this,

00:46:31.327 --> 00:46:33.326
and say that there might
be some kind of tipping

00:46:33.326 --> 00:46:35.890
point where it all gets into
a horrible, negative feedback

00:46:35.890 --> 00:46:36.680
loop, and?

00:46:36.680 --> 00:46:39.220
ANDREW MCAFEE: We can we
can summarize the situation

00:46:39.220 --> 00:46:42.740
you're describing, in
an apocryphal story

00:46:42.740 --> 00:46:44.790
about Henry Ford II
and Walter Reuther, who

00:46:44.790 --> 00:46:46.549
was the head of the
auto workers' union.

00:46:46.549 --> 00:46:48.090
There's a story
about the two of them

00:46:48.090 --> 00:46:50.814
going through a very highly
automated, brand new plant

00:46:50.814 --> 00:46:52.230
that Ford had
built, and they were

00:46:52.230 --> 00:46:53.630
kind of joshing with each other.

00:46:53.630 --> 00:46:55.440
I'm sure they hated
each other deeply,

00:46:55.440 --> 00:46:57.400
but they were kind of
joshing with each other,

00:46:57.400 --> 00:46:59.899
and Henry Ford points to one
of his new pieces of equipment,

00:46:59.899 --> 00:47:01.530
and he says, hey,
Walter, how are you

00:47:01.530 --> 00:47:03.980
going to get those
robots to pay union dues?

00:47:03.980 --> 00:47:05.780
And Reuther says, hey,
Henry, how are you

00:47:05.780 --> 00:47:07.950
going to get them to buy cars?

00:47:07.950 --> 00:47:11.370
So this is exactly the problem
but that you're identifying.

00:47:11.370 --> 00:47:14.450
If people are out of work, if
their wages are going down,

00:47:14.450 --> 00:47:16.610
aggregate demand in
the economy goes down,

00:47:16.610 --> 00:47:19.900
we know that's a really
bad thing to have happen,

00:47:19.900 --> 00:47:24.380
and what I don't know is
how soon is this coming?

00:47:24.380 --> 00:47:25.700
How bad will it be?

00:47:25.700 --> 00:47:29.006
But any reluctance
to spend is going

00:47:29.006 --> 00:47:30.630
to cause the economy
and overall demand

00:47:30.630 --> 00:47:33.290
to head in the wrong direction,
and that reluctance to spend

00:47:33.290 --> 00:47:34.860
can be based in
reality, or it can

00:47:34.860 --> 00:47:37.562
be based on Keynes'
famous animal spirits.

00:47:37.562 --> 00:47:39.270
ERIK BRYNJOLFSSON:
Right, and the example

00:47:39.270 --> 00:47:41.228
I gave earlier about what
happened in the Great

00:47:41.228 --> 00:47:43.615
Depression-- Joe
Stiglitz described that

00:47:43.615 --> 00:47:47.270
as being one of the contributing
factors to that fall in demand,

00:47:47.270 --> 00:47:50.880
and so it's going to potentially
have macroeconomic effects, as

00:47:50.880 --> 00:47:52.485
well as these
distributional effects.

00:47:52.485 --> 00:47:54.110
AUDIENCE: I wanted
to ask you something

00:47:54.110 --> 00:47:55.891
personal and specific.

00:47:55.891 --> 00:47:58.140
ANDREW MCAFEE: And I see
we're just about out of time.

00:47:58.140 --> 00:47:59.750
[LAUGHTER]

00:47:59.750 --> 00:48:01.760
AUDIENCE: Last week there
was a lengthy article

00:48:01.760 --> 00:48:03.593
in the "Economist" about
the future of jobs,

00:48:03.593 --> 00:48:05.597
and it liberally
quoted from you--

00:48:05.597 --> 00:48:07.930
ANDREW MCAFEE: Not liberally
enough, I'll tell you that.

00:48:07.930 --> 00:48:10.300
AUDIENCE: --in your
upcoming book, and so

00:48:10.300 --> 00:48:16.930
I wanted to ask you if you are a
parent, or want to be a parent,

00:48:16.930 --> 00:48:19.240
what would you
advise your children

00:48:19.240 --> 00:48:21.990
to do who are going to enter
the workforce in the next two

00:48:21.990 --> 00:48:22.680
decades?

00:48:22.680 --> 00:48:25.260
What careers and what kind
of education opportunities

00:48:25.260 --> 00:48:28.460
should they pursue, because
the classic types of things

00:48:28.460 --> 00:48:31.970
that maybe we have told them in
the past-- go become a lawyer,

00:48:31.970 --> 00:48:35.690
go become an accountant,
go become a doctor.

00:48:35.690 --> 00:48:38.860
This is a ticket to
professionalism, upper middle

00:48:38.860 --> 00:48:42.790
class lifestyle, and the odds
of you becoming unemployed

00:48:42.790 --> 00:48:44.900
are very low, and I
don't feel that that

00:48:44.900 --> 00:48:47.410
is going to be the case
going forward anymore,

00:48:47.410 --> 00:48:49.150
and so I want to
put that on you,

00:48:49.150 --> 00:48:50.707
and if you had to
advise your own son

00:48:50.707 --> 00:48:52.040
or daughter, what would you say?

00:48:52.040 --> 00:48:53.330
ANDREW MCAFEE: Let's have
the parent answer first.

00:48:53.330 --> 00:48:56.160
ERIK BRYNJOLFSSON: It's a good
question, and it's a tough one,

00:48:56.160 --> 00:48:57.630
but there are still
some categories

00:48:57.630 --> 00:48:59.410
that machines are
not very good at,

00:48:59.410 --> 00:49:02.830
and we are hesitate to--
we almost never say never

00:49:02.830 --> 00:49:04.980
because of what we've seen
in terms of advances--

00:49:04.980 --> 00:49:07.100
but things that involve
significant amounts

00:49:07.100 --> 00:49:10.140
of creativity, invention,
entrepreneurship.

00:49:10.140 --> 00:49:11.927
Those have not been
affected at all.

00:49:11.927 --> 00:49:13.260
In fact, they've been augmented.

00:49:13.260 --> 00:49:15.340
They've become more valuable.

00:49:15.340 --> 00:49:18.695
Interpersonal interactions
and skills-- machines so far

00:49:18.695 --> 00:49:20.320
are still not very
good, although we've

00:49:20.320 --> 00:49:22.100
got friends in the Media Lab
and elsewhere that are there

00:49:22.100 --> 00:49:24.910
pushing the frontiers on that,
but whether that's motivating

00:49:24.910 --> 00:49:27.610
people, or even just
caring for a sick child,

00:49:27.610 --> 00:49:29.260
comforting somebody--
that's something

00:49:29.260 --> 00:49:31.220
that machines don't
seem to be very good at.

00:49:31.220 --> 00:49:33.880
I would encourage my
kids to be flexible,

00:49:33.880 --> 00:49:38.580
because whatever types of tasks
that are or aren't automated

00:49:38.580 --> 00:49:40.659
now-- it's likely to
change, and people

00:49:40.659 --> 00:49:42.200
are going to have
to change and adapt

00:49:42.200 --> 00:49:44.620
much more quickly
than in the past.

00:49:44.620 --> 00:49:47.170
So we don't know for
sure exactly which

00:49:47.170 --> 00:49:49.100
categories are going
to be affected.

00:49:49.100 --> 00:49:51.560
Those are some of them, but
maintaining that flexibility

00:49:51.560 --> 00:49:54.405
is probably going to be one of
the key things you can do you.

00:49:54.405 --> 00:49:56.280
ANDREW MCAFEE: You asked
a personal question.

00:49:56.280 --> 00:49:58.113
I'll give a little bit
of a personal answer.

00:49:58.113 --> 00:50:00.564
I'm not a parent, but
the most common question

00:50:00.564 --> 00:50:01.980
that we get asked
about this stuff

00:50:01.980 --> 00:50:04.460
comes from parents
of kids of all ages

00:50:04.460 --> 00:50:05.900
saying, what advice do I give?

00:50:05.900 --> 00:50:06.980
What do I do?

00:50:06.980 --> 00:50:09.300
So my advice for
parents of young kids

00:50:09.300 --> 00:50:10.740
is actually a lot easier for me.

00:50:10.740 --> 00:50:12.810
I say, just send
them to Montessori.

00:50:12.810 --> 00:50:14.210
And not that that's a panacea.

00:50:14.210 --> 00:50:16.660
Are there Montessori
kids in the room?

00:50:16.660 --> 00:50:18.230
Really?

00:50:18.230 --> 00:50:19.250
That's amazing.

00:50:19.250 --> 00:50:22.190
If one or both your
founders were in the room,

00:50:22.190 --> 00:50:23.570
hands would be going up.

00:50:23.570 --> 00:50:26.860
There's a Montessori mafia
among the technology elite,

00:50:26.860 --> 00:50:28.766
and I was a Montessori kid, too.

00:50:28.766 --> 00:50:30.390
And what it taught
me is that the world

00:50:30.390 --> 00:50:32.181
is a really interesting
place, and your job

00:50:32.181 --> 00:50:33.500
is to go poke at it.

00:50:33.500 --> 00:50:34.890
I think that's great.

00:50:34.890 --> 00:50:36.961
My school only went up
through the third grade,

00:50:36.961 --> 00:50:39.460
so in the fourth grade, I went
into the public school system

00:50:39.460 --> 00:50:41.780
in the Indiana town
where I'm from,

00:50:41.780 --> 00:50:44.120
and I felt like I had
been sent to the Gulag.

00:50:44.120 --> 00:50:47.400
I'm like, you want me
to sit here all day,

00:50:47.400 --> 00:50:50.050
and a series of things
is going to be inflicted

00:50:50.050 --> 00:50:51.910
on me at 45-minute intervals?

00:50:51.910 --> 00:50:53.220
What are you people, sadists?

00:50:53.220 --> 00:50:55.640
It just made no sense
to me, whatsoever.

00:50:55.640 --> 00:50:58.540
So early on, go teach
your kids that the world

00:50:58.540 --> 00:50:59.760
is an interesting place.

00:50:59.760 --> 00:51:03.160
At the college level, my
silly bumper sticker advice

00:51:03.160 --> 00:51:04.940
is put down the
beer pong paddle,

00:51:04.940 --> 00:51:06.480
and just go do a double major.

00:51:06.480 --> 00:51:09.290
Go hang out with the theater
geeks at one side of campus,

00:51:09.290 --> 00:51:11.530
and the math geeks at
the other side of campus.

00:51:11.530 --> 00:51:13.205
Fill up your toolkit
as much as you can.

00:51:13.205 --> 00:51:15.830
ERIK BRYNJOLFSSON: And the only
other thing I would add to that

00:51:15.830 --> 00:51:19.226
is that if we do tend to more
of a winner take all market,

00:51:19.226 --> 00:51:21.600
there's even more of a premium
than there was in the past

00:51:21.600 --> 00:51:23.780
on being the very
best in something,

00:51:23.780 --> 00:51:25.770
so it becomes important
to pursue things

00:51:25.770 --> 00:51:26.930
that you're really
passionate about,

00:51:26.930 --> 00:51:28.390
that you're interested
in, because we all

00:51:28.390 --> 00:51:30.500
know that you're not likely
to be the best at anything

00:51:30.500 --> 00:51:31.950
that you're just doing
because you have to

00:51:31.950 --> 00:51:33.250
or because you're supposed to.

00:51:33.250 --> 00:51:35.640
But there are going to
be niches-- large niches

00:51:35.640 --> 00:51:38.510
and small niches and tiny
ones-- where somebody

00:51:38.510 --> 00:51:40.677
has the potential to be
among the best in the world,

00:51:40.677 --> 00:51:42.218
and those are the
ones that are still

00:51:42.218 --> 00:51:44.720
going to be in demand, whereas
if you just another person

00:51:44.720 --> 00:51:46.835
doing a job because
you have to, it's

00:51:46.835 --> 00:51:49.030
going to be increasingly
hard for people

00:51:49.030 --> 00:51:51.700
to find a way to make
a living doing that.

00:51:51.700 --> 00:51:54.070
AUDIENCE: We seem to
take it for granted

00:51:54.070 --> 00:51:57.500
that investing education
is a wise thing

00:51:57.500 --> 00:52:01.480
to do, at least on a
macroeconomic scale,

00:52:01.480 --> 00:52:05.400
but on a personal
scale, given the rising

00:52:05.400 --> 00:52:11.020
popularity and prosperity,
if I'm not rich,

00:52:11.020 --> 00:52:15.550
and my prospect of becoming
well-off are very, very slim,

00:52:15.550 --> 00:52:22.450
why should I be motivated to
invest in educating myself

00:52:22.450 --> 00:52:26.435
if the chances of that having
me better off are so low?

00:52:26.435 --> 00:52:27.810
ERIK BRYNJOLFSSON:
Well, the data

00:52:27.810 --> 00:52:31.800
suggests that even today,
the difference in earnings

00:52:31.800 --> 00:52:34.220
between people with more
education and less education

00:52:34.220 --> 00:52:36.080
is enormous, and
it's been growing,

00:52:36.080 --> 00:52:38.870
so from a purely selfish,
personal point of view,

00:52:38.870 --> 00:52:41.130
it does pay to get
more education.

00:52:41.130 --> 00:52:43.810
But it's important
to understand,

00:52:43.810 --> 00:52:45.760
when we talk about
education, first off,

00:52:45.760 --> 00:52:48.840
we don't just mean college
or post-graduate education.

00:52:48.840 --> 00:52:53.600
We mean all levels, whether
it's K through 12, vocational,

00:52:53.600 --> 00:52:57.840
lifelong learning-- all of them
building more human capital

00:52:57.840 --> 00:53:00.790
in any of those dimensions
is likely to add value.

00:53:00.790 --> 00:53:03.640
But we also think that education
has to be reinvented exactly

00:53:03.640 --> 00:53:06.060
along the lines that Andy
was describing earlier.

00:53:06.060 --> 00:53:09.310
The traditional pattern of
having people sit in rows,

00:53:09.310 --> 00:53:12.090
and learn to follow
instructions carefully

00:53:12.090 --> 00:53:15.450
from the person at the
front of the room-- well,

00:53:15.450 --> 00:53:17.650
following instructions is
something that machines

00:53:17.650 --> 00:53:21.330
are really pretty good at doing,
so education of that model,

00:53:21.330 --> 00:53:23.880
and imparting facts
and numbers, isn't

00:53:23.880 --> 00:53:25.570
likely to be the
kind of thing that we

00:53:25.570 --> 00:53:27.220
need more of the future.

00:53:27.220 --> 00:53:29.280
Education that
sparks creativity,

00:53:29.280 --> 00:53:31.880
that allows you to think about
new ways of putting things

00:53:31.880 --> 00:53:34.390
together, and create
things-- that's much more

00:53:34.390 --> 00:53:36.500
likely to be something
that adds value.

00:53:36.500 --> 00:53:38.070
ANDREW MCAFEE: Here, here.

00:53:38.070 --> 00:53:40.190
AUDIENCE: Thanks for
a wonderful talk.

00:53:40.190 --> 00:53:42.160
So coming back to
this policy question,

00:53:42.160 --> 00:53:44.635
how aware would you
say the average policy

00:53:44.635 --> 00:53:47.340
maker or politician in
US is about these trends,

00:53:47.340 --> 00:53:50.290
and how could we actually
get more of our politicians

00:53:50.290 --> 00:53:53.210
to understand the nuance of
what you're talking about?

00:53:53.210 --> 00:53:55.652
ANDREW MCAFEE: Someone needs
to write a book about it.

00:53:55.652 --> 00:53:57.620
[LAUGHTER]

00:53:57.620 --> 00:54:01.140
ANDREW MCAFEE: You can either be
really pessimistic or somewhat

00:54:01.140 --> 00:54:02.200
optimistic about it.

00:54:02.200 --> 00:54:06.920
I'd say the awareness is early,
early days, but growing kind

00:54:06.920 --> 00:54:09.550
of quickly, and everyone
that we talk to,

00:54:09.550 --> 00:54:11.550
these really well-informed
people, bemoan

00:54:11.550 --> 00:54:14.620
the polarization in Washington,
and this environment we have

00:54:14.620 --> 00:54:16.700
where, if you say
black, by definition,

00:54:16.700 --> 00:54:18.870
I'm going to say white.

00:54:18.870 --> 00:54:20.740
But the green shoots
that we see are

00:54:20.740 --> 00:54:24.560
around some pretty important
areas, like immigration policy.

00:54:24.560 --> 00:54:27.410
There is a pretty broad
bipartisan agreement

00:54:27.410 --> 00:54:29.890
that we need to
change some things.

00:54:29.890 --> 00:54:32.510
There's a lot of talk about how
we need to reinvent education.

00:54:32.510 --> 00:54:36.750
No one's exactly sure
how we need to do that.

00:54:36.750 --> 00:54:40.000
So I would say the awareness
is, especially about this tech

00:54:40.000 --> 00:54:43.010
stuff, because Washington's
not a technophilic place,

00:54:43.010 --> 00:54:44.740
the awareness is low.

00:54:44.740 --> 00:54:47.250
It's getting better
kind of quickly,

00:54:47.250 --> 00:54:49.390
and maybe it'll get
OK at some point.

00:54:49.390 --> 00:54:50.380
ERIK BRYNJOLFSSON:
Yeah, I mean, that's

00:54:50.380 --> 00:54:52.129
certainly one of the
more depressing parts

00:54:52.129 --> 00:54:54.310
of our analysis is
just the way things are

00:54:54.310 --> 00:54:58.330
so stagnant in Washington and
in public policy, in general.

00:54:58.330 --> 00:55:00.660
There is a growing
amount of anger,

00:55:00.660 --> 00:55:03.100
whether it's the Tea Party
or Occupy Wall Street,

00:55:03.100 --> 00:55:05.870
or other groups, but our sense
is that, by and large, they

00:55:05.870 --> 00:55:08.611
mostly have misdiagnosed
the nature of the problem,

00:55:08.611 --> 00:55:10.360
and if you don't have
the right diagnosis,

00:55:10.360 --> 00:55:12.980
you're probably not going to
have the right prescriptions.

00:55:12.980 --> 00:55:15.950
And so a big part of the reason
we wrote this book was to help

00:55:15.950 --> 00:55:17.590
change the conversation.

00:55:17.590 --> 00:55:20.000
Ultimately we've
been told by people

00:55:20.000 --> 00:55:23.830
who know better-- Washington
doesn't lead the country.

00:55:23.830 --> 00:55:25.580
It's people who
lead it, and they

00:55:25.580 --> 00:55:28.340
will respond to what
people are demanding.

00:55:28.340 --> 00:55:31.580
So phase one is
spreading the awareness,

00:55:31.580 --> 00:55:34.260
as I think you just
brought up, getting people

00:55:34.260 --> 00:55:37.097
to understand some of the
issues that are important,

00:55:37.097 --> 00:55:38.930
and once that conversation
has been changed,

00:55:38.930 --> 00:55:41.590
we're going to have much
better hope of changing

00:55:41.590 --> 00:55:43.372
the kinds of policies.

00:55:43.372 --> 00:55:44.830
AUDIENCE: So you
talked quite a bit

00:55:44.830 --> 00:55:47.915
about what regulators
can do to counteract

00:55:47.915 --> 00:55:49.310
some of these effects.

00:55:49.310 --> 00:55:52.150
Is anything we can do,
given that at Google we

00:55:52.150 --> 00:55:55.260
are pushing technology forward
and are often techno-optimists?

00:55:55.260 --> 00:55:56.550
ANDREW MCAFEE: Yes, stop, OK?

00:55:56.550 --> 00:55:57.800
ERIK BRYNJOLFSSON: No, no, no.

00:55:57.800 --> 00:55:58.510
ANDREW MCAFEE: No, don't.

00:55:58.510 --> 00:55:59.460
That's the first thing.

00:55:59.460 --> 00:56:00.876
This is actually
really important.

00:56:00.876 --> 00:56:03.230
We come across a lot of
really well-meaning people

00:56:03.230 --> 00:56:05.870
who kind of want you guys
to stop what you're doing,

00:56:05.870 --> 00:56:08.195
and want the tech sector
to slow down and stop

00:56:08.195 --> 00:56:09.320
putting people out of work.

00:56:09.320 --> 00:56:12.860
We think that's the single-worst
idea that we come across,

00:56:12.860 --> 00:56:16.450
so by all means, carry on
taking us into the future.

00:56:16.450 --> 00:56:18.820
It's incredibly important work.

00:56:18.820 --> 00:56:22.430
As individuals, advocate,
support enlightened policies,

00:56:22.430 --> 00:56:25.700
vote for and support people
who are taking our policies

00:56:25.700 --> 00:56:28.940
in the right direction, and just
participate in the democracy.

00:56:28.940 --> 00:56:31.830
Our friend Larry Lessig
talks about how we just

00:56:31.830 --> 00:56:34.340
feel so disengaged
from the process,

00:56:34.340 --> 00:56:36.380
and we've become increasingly
cynical about it.

00:56:36.380 --> 00:56:38.330
Just don't do that, please.

00:56:38.330 --> 00:56:39.610
ERIK BRYNJOLFSSON: You
know, technology-- go ahead.

00:56:39.610 --> 00:56:41.170
AUDIENCE: I think I
meant in between, so--

00:56:41.170 --> 00:56:42.510
ERIK BRYNJOLFSSON: Yeah, yeah.

00:56:42.510 --> 00:56:43.490
Let me try and go
to that, because I

00:56:43.490 --> 00:56:45.890
think that the technologists
and engineers have been

00:56:45.890 --> 00:56:48.710
great at solving
problems, and we talked

00:56:48.710 --> 00:56:51.130
to some of the folks that
responded to the first DARPA

00:56:51.130 --> 00:56:53.610
Grand Challenge, which
was, as you know probably,

00:56:53.610 --> 00:56:56.342
to build a driverless car,
which many people thought

00:56:56.342 --> 00:56:57.800
was impossible,
and they responded.

00:56:57.800 --> 00:57:02.710
The new DARPA Urban Challenge
is a robot-- six-foot,

00:57:02.710 --> 00:57:07.190
Atlas robot-- that weighs about
300 pounds that actually will

00:57:07.190 --> 00:57:09.320
be able to walk up to a
vehicle, open the door,

00:57:09.320 --> 00:57:14.430
get inside, drive it, get
out, walk across rubble, up

00:57:14.430 --> 00:57:17.380
some stairs, attach a hose,
a whole set of other tasks.

00:57:17.380 --> 00:57:19.500
And again, at first it
seemed like science-fiction

00:57:19.500 --> 00:57:22.520
impossibility, but our friends
at the MIT Computer Science

00:57:22.520 --> 00:57:25.970
and AI Lab and elsewhere seem
to be making headway on that.

00:57:25.970 --> 00:57:28.910
It may be that we need
to think about a new kind

00:57:28.910 --> 00:57:33.010
of grand challenge that isn't to
make an amazing new technology,

00:57:33.010 --> 00:57:35.600
but is just to think
about how we can reinvent

00:57:35.600 --> 00:57:38.570
an economic, social,
organizational system--

00:57:38.570 --> 00:57:41.860
one that creates as many
jobs as it's automating.

00:57:41.860 --> 00:57:44.540
Many of the previous
engineering tasks have either

00:57:44.540 --> 00:57:47.044
implicitly or quite
explicitly been designed

00:57:47.044 --> 00:57:48.460
to answer the
question, how can we

00:57:48.460 --> 00:57:50.190
take labor out of this process?

00:57:50.190 --> 00:57:52.370
How can we make this
more labor-saving,

00:57:52.370 --> 00:57:55.250
and have a machine do what a
person used to be able to do?

00:57:55.250 --> 00:57:56.880
But technology
doesn't have to just

00:57:56.880 --> 00:57:58.810
be a substitute for humans.

00:57:58.810 --> 00:58:01.290
Many technologies can be
complements for humans,

00:58:01.290 --> 00:58:03.010
and can enhance the
value of humans,

00:58:03.010 --> 00:58:05.350
and maybe if we put
more focused effort

00:58:05.350 --> 00:58:09.660
into a grand challenge for
creating technologies that

00:58:09.660 --> 00:58:12.440
boost the amounts of labor,
that are complements to labor,

00:58:12.440 --> 00:58:15.020
and allow people to do things
they couldn't have done before,

00:58:15.020 --> 00:58:17.810
then we are likely to get that
kind of shared-prosperity world

00:58:17.810 --> 00:58:18.890
that we were hoping for.

00:58:18.890 --> 00:58:20.670
And so that would be the
kind of grand challenge.

00:58:20.670 --> 00:58:23.045
We haven't quite figured out
how to formulate that right,

00:58:23.045 --> 00:58:24.550
but maybe if we
could do that, it

00:58:24.550 --> 00:58:26.550
would inspire many
of you and people

00:58:26.550 --> 00:58:28.440
elsewhere in the
country and the world

00:58:28.440 --> 00:58:30.420
to try to solve that
problem, as opposed

00:58:30.420 --> 00:58:32.404
to waiting for, say,
Washington to take action.

00:58:32.404 --> 00:58:34.820
ANDREW MCAFEE: I think we have
time for one more question,

00:58:34.820 --> 00:58:37.510
but it has to be a really
bang-up final question.

00:58:37.510 --> 00:58:39.020
Are you ready?

00:58:39.020 --> 00:58:39.941
AUDIENCE: Sure.

00:58:39.941 --> 00:58:40.440
Maybe.

00:58:40.440 --> 00:58:42.523
So actually, that was a
really interesting answer,

00:58:42.523 --> 00:58:46.120
if under-specified so far,
that you started to give,

00:58:46.120 --> 00:58:49.210
because before you
started to give that,

00:58:49.210 --> 00:58:51.240
I would've started to
wonder if you had had

00:58:51.240 --> 00:58:53.573
the reaction-- other people
had the reaction to the book

00:58:53.573 --> 00:58:56.560
that your proposed
prescriptions don't seem

00:58:56.560 --> 00:59:01.260
of the same magnitude as the
problem that you're describing.

00:59:01.260 --> 00:59:03.970
For the most part, you seem
to just be describing tweaks,

00:59:03.970 --> 00:59:05.470
or maybe if you
have one big idea,

00:59:05.470 --> 00:59:07.620
it's education, which everybody
has talked about for years,

00:59:07.620 --> 00:59:09.780
and nobody knows how to fix,
and if you could fix it,

00:59:09.780 --> 00:59:11.670
it wouldn't pay off for
decades down the line anyway,

00:59:11.670 --> 00:59:13.336
and you have new
problems by that point.

00:59:13.336 --> 00:59:19.360
So what do you think is the
biggest policy prescription

00:59:19.360 --> 00:59:21.890
or advice for the
future that you

00:59:21.890 --> 00:59:26.390
have that is proportionate to
the magnitude of the problem?

00:59:26.390 --> 00:59:29.930
ANDREW MCAFEE: And the nastiest
and maybe most accurate

00:59:29.930 --> 00:59:31.560
comment we've gotten
about our work

00:59:31.560 --> 00:59:33.810
is, you guys are
proposing linear solutions

00:59:33.810 --> 00:59:36.190
to an exponential situation.

00:59:36.190 --> 00:59:39.980
Ow, right?

00:59:39.980 --> 00:59:42.460
So we're mindful of it.

00:59:42.460 --> 00:59:44.230
What we don't want
to do is say that we

00:59:44.230 --> 00:59:47.220
know what the trajectory of
technology is going to be,

00:59:47.220 --> 00:59:50.010
and therefore what kind
of moonshotty intervention

00:59:50.010 --> 00:59:51.360
is appropriate to get there.

00:59:51.360 --> 00:59:54.650
What we're saying instead is
essentially, let's iterate.

00:59:54.650 --> 00:59:55.410
Let's tinker.

00:59:55.410 --> 00:59:57.200
Let's mess with the
things that we know

00:59:57.200 --> 00:59:59.420
will move the dial in
the right direction,

00:59:59.420 --> 01:00:02.260
and let's make sure that we're
tracking the problem correctly,

01:00:02.260 --> 01:00:04.540
as opposed to trying to
anticipate where this thing is

01:00:04.540 --> 01:00:07.810
going to be in some crazy
time frame down the road.

01:00:07.810 --> 01:00:12.310
I take a lot of insight from
your chairman's repeated

01:00:12.310 --> 01:00:14.720
assertion that the crazy,
long-frame planning

01:00:14.720 --> 01:00:18.000
horizon for Google
is five years.

01:00:18.000 --> 01:00:20.860
Overshooting that by a lot
with our policy recommendations

01:00:20.860 --> 01:00:24.180
seems like a recipe for
failure or disaster.

01:00:24.180 --> 01:00:25.590
ERIK BRYNJOLFSSON:
I mean, you've

01:00:25.590 --> 01:00:27.090
raised a very
important point, which

01:00:27.090 --> 01:00:31.060
is that we don't have a
silver-bullet solution to this.

01:00:31.060 --> 01:00:32.874
I wish we did,
but we can't point

01:00:32.874 --> 01:00:34.790
to one silver-bullet
solution, and that partly

01:00:34.790 --> 01:00:38.300
reflects how we went
about writing this book.

01:00:38.300 --> 01:00:41.344
Many people who write this kind
of book start with a policy,

01:00:41.344 --> 01:00:42.760
and then they go
and write a bunch

01:00:42.760 --> 01:00:45.089
of reasons why this policy
needs to be put in place.

01:00:45.089 --> 01:00:46.880
ANDREW MCAFEE: Because
they're policy guys.

01:00:46.880 --> 01:00:47.921
ERIK BRYNJOLFSSON: Right.

01:00:47.921 --> 01:00:49.840
We are not policy
guys, and we focused

01:00:49.840 --> 01:00:51.880
on diagnosing what's going on.

01:00:51.880 --> 01:00:53.470
We want to lay those issues out.

01:00:53.470 --> 01:00:56.690
We have three chapters' worth
of potential solutions that we

01:00:56.690 --> 01:00:58.340
think will help
with the problem,

01:00:58.340 --> 01:01:02.050
but ultimately we think that
by start this conversation,

01:01:02.050 --> 01:01:05.940
maybe you and others in this
room and elsewhere will help

01:01:05.940 --> 01:01:06.900
develop the solutions.

01:01:06.900 --> 01:01:08.850
We lay out some of
the problems there,

01:01:08.850 --> 01:01:11.880
and this is the kind of thing
that, just as in the past,

01:01:11.880 --> 01:01:14.340
we hope to crowdsource
it, in essence.

01:01:14.340 --> 01:01:16.860
And we have tried to walk
the walk a little bit,

01:01:16.860 --> 01:01:18.840
in terms of starting
a new initiative

01:01:18.840 --> 01:01:21.470
on the digital economy at MIT,
where we were bringing together

01:01:21.470 --> 01:01:24.489
people to grapple
with these questions,

01:01:24.489 --> 01:01:26.280
and we're hoping we
can make some progress.

01:01:26.280 --> 01:01:29.865
But I have to confess,
I can't say here,

01:01:29.865 --> 01:01:31.740
if you just implemented
the Earned Income Tax

01:01:31.740 --> 01:01:33.440
Credit, the problem
will be solved,

01:01:33.440 --> 01:01:35.065
because it's not
going to be that easy.

01:01:35.065 --> 01:01:36.990
AUDIENCE: A very
brief follow-up--

01:01:36.990 --> 01:01:38.570
when you're in a
ditch, stop digging.

01:01:38.570 --> 01:01:41.150
What's the single-most
thing that we shouldn't

01:01:41.150 --> 01:01:43.480
do to make the problem worse
than maybe we are already

01:01:43.480 --> 01:01:44.286
doing?

01:01:44.286 --> 01:01:45.660
ANDREW MCAFEE:
Turning out people

01:01:45.660 --> 01:01:49.910
who do not have the skills to be
valuable workers in the economy

01:01:49.910 --> 01:01:51.730
of today, let alone tomorrow.

01:01:51.730 --> 01:01:55.410
The business leaders that we
talk to are so frustrated.

01:01:55.410 --> 01:01:56.940
Up and down the
skill ladder, they

01:01:56.940 --> 01:01:59.270
can't find the people
that they need, even

01:01:59.270 --> 01:02:02.590
for the positions they have
open, even here in America.

01:02:02.590 --> 01:02:04.620
That just doesn't
make any sense.

01:02:04.620 --> 01:02:05.350
ERIK BRYNJOLFSSON:
I would second that.

01:02:05.350 --> 01:02:06.766
I would also say
that there's been

01:02:06.766 --> 01:02:09.440
a problem of siloing
technical people,

01:02:09.440 --> 01:02:11.930
from the economists
and the policy makers.

01:02:11.930 --> 01:02:15.580
We find that these groups that
we've tried to bridge just

01:02:15.580 --> 01:02:16.830
don't speak the same language.

01:02:16.830 --> 01:02:18.663
They don't even recognize
the same problems,

01:02:18.663 --> 01:02:20.760
and if we could get people
coordinating on those,

01:02:20.760 --> 01:02:22.344
I think that would
be half the battle.

01:02:22.344 --> 01:02:24.010
ANDREW MCAFEE: Thank
you all for coming.

01:02:24.010 --> 01:02:24.850
This is fantastic.

01:02:24.850 --> 01:02:29.952
[APPLAUSE]

