WEBVTT
Kind: captions
Language: en

00:00:03.260 --> 00:00:06.780
Hi, I’m Adriene Hill, and welcome back to
Crash Course Statistics.

00:00:06.780 --> 00:00:09.340
Lies. Damn lies. And statistics

00:00:09.340 --> 00:00:11.180
Stats gets a bad rap.

00:00:11.180 --> 00:00:12.840
And sometimes it makes sense why.

00:00:12.840 --> 00:00:17.940
We’ve talked a lot about how p-values let
us know something significant in our data--but

00:00:17.949 --> 00:00:21.780
those p-values and the data behind them can be manipulated.

00:00:21.780 --> 00:00:23.050
Hacked.

00:00:23.050 --> 00:00:24.800
P hacked.

00:00:24.800 --> 00:00:30.520
P-hacking is manipulating data or analyses
to artificially get significant p-values.

00:00:30.529 --> 00:00:34.620
Today we’re going to take a break from learning new statistical models, and instead look at

00:00:34.620 --> 00:00:36.970
some statistics gone wrong.

00:00:36.970 --> 00:00:39.200
And maybe also some props gone wrong.

00:00:39.200 --> 00:00:48.500
INTRO

00:00:48.500 --> 00:00:52.879
To recap to calculate a p-value, we look at
the Null Hypothesis--which is the idea that

00:00:52.879 --> 00:00:54.260
there’s no effect.

00:00:54.260 --> 00:00:58.000
This can be no effect of shoe color on the
number of steps you walked today, or no effect

00:00:58.000 --> 00:01:00.800
of grams of fat in your diet on energy levels.

00:01:00.800 --> 00:01:04.760
Whatever it is, we set this hypothesis up
just so that we can try to shoot it down.

00:01:04.760 --> 00:01:10.570
In the NHST framework we either reject, or
fail to reject the null.

00:01:10.570 --> 00:01:14.550
This binary decision process leads us to 4
possible scenarios:

00:01:14.550 --> 00:01:17.660
The null is true and we correctly fail to
reject it

00:01:17.660 --> 00:01:20.940
The null is true but we incorrectly reject
it.

00:01:20.940 --> 00:01:24.010
The null is false and we correctly reject
it.

00:01:24.010 --> 00:01:27.490
The null is false and we incorrectly fail
to reject it.

00:01:27.490 --> 00:01:31.250
Out of these four options, scientists who
expect to see a relationship are usually hoping

00:01:31.250 --> 00:01:32.600
for this one.

00:01:32.600 --> 00:01:39.630
In NHST, failing to reject the null is a lack
of any evidence, not evidence that nothing

00:01:39.630 --> 00:01:40.200
happened.

00:01:40.200 --> 00:01:44.780
So scientists and researchers are incentivised to find something significant.

00:01:44.780 --> 00:01:49.760
Academic journals don’t want to publish
a result saying: “We don’t have convincing

00:01:49.760 --> 00:01:55.540
evidence that chocolate cures cancer but also we don’t have convincing evidence that it doesn't".

00:01:55.540 --> 00:01:57.440
Popular websites don’t want that either.

00:01:57.440 --> 00:01:59.360
That’s like anti-clickbait.

00:01:59.360 --> 00:02:03.800
In science, being able to publish your results is your ticket to job stability, a higher

00:02:03.810 --> 00:02:05.800
salary, and prestige.

00:02:05.800 --> 00:02:09.660
In this quest to achieve positive results,
sometimes things can go wrong.

00:02:09.660 --> 00:02:15.500
P-hacking is when analyses are being chosen based on what makes the p-value significant,

00:02:15.500 --> 00:02:17.680
not what’s the best analysis plan.

00:02:17.680 --> 00:02:22.020
Statistical tests that look normal on the
surface may have been p-hacked.

00:02:22.030 --> 00:02:26.640
And we should be careful when consuming or doing research so that we’re not misled

00:02:26.650 --> 00:02:27.970
by p-hacked analyses.

00:02:27.970 --> 00:02:29.540
“P-hacking” isn’t always malicious.

00:02:29.540 --> 00:02:33.770
It could come from a gap in a researcher’s
statistical knowledge, a well-intentioned

00:02:33.770 --> 00:02:38.269
belief in a specific scientific theory, or
just an honest mistake.

00:02:38.269 --> 00:02:41.319
Regardless of what’s behind p-hacking, it’s
a problem.

00:02:41.319 --> 00:02:43.900
Much of scientific theory is based on p-values.

00:02:43.900 --> 00:02:48.340
Ideally, we should choose which analyses we’re going to do before we see the data.

00:02:48.340 --> 00:02:52.660
And even then, we accept that sometimes we’ll get a significant result even if there’s

00:02:52.660 --> 00:02:54.439
no real effect, just by chance.

00:02:54.439 --> 00:02:58.129
It’s a risk we take when we use Null Hypothesis Significance Testing.

00:02:58.129 --> 00:03:02.220
But we don’t want researchers to intentionally create effects that look significant, even

00:03:02.220 --> 00:03:03.220
when they’re not.

00:03:03.220 --> 00:03:07.440
When scientists p-hack, they’re often putting out research results that just aren’t real.

00:03:07.440 --> 00:03:11.640
And the ramifications of these incorrect studies can be small--like convincing people that

00:03:11.640 --> 00:03:15.810
eating chocolate will cause weight loss--to
very, very serious--like contributing to a

00:03:15.810 --> 00:03:19.540
study that convinced many people to stop vaccinating their kids.

00:03:19.540 --> 00:03:21.300
Analyses can be complicated.

00:03:21.310 --> 00:03:26.260
For example, x-k-c-d had a comic associating jelly beans and acne.

00:03:26.260 --> 00:03:29.060
So you grab a box of jelly beans and get experimenting.

00:03:29.060 --> 00:03:33.249
It turns out that you get a p-value that’s
greater than 0.05.

00:03:33.249 --> 00:03:38.599
Since your alpha cutoff is 0.05, you fail
to reject the null that jelly beans are not

00:03:38.599 --> 00:03:39.900
associated with breakouts.

00:03:39.900 --> 00:03:43.560
But the comic goes on there are different
COLORS of jelly beans.

00:03:43.560 --> 00:03:46.569
Maybe it’s only one color that’s linked
with acne!

00:03:46.569 --> 00:03:49.540
So you go off to the lab to test the twenty
different colors.

00:03:49.540 --> 00:03:53.489
And the green ones produce a significant p-value!

00:03:53.489 --> 00:03:57.950
But before you run off to the newspapers to
tell everyone to stop eating green jelly beans,

00:03:57.950 --> 00:03:59.299
let’s think about what happened.

00:03:59.299 --> 00:04:05.209
We know that there’s a 5% chance of getting a p-value less than 0.05, even if no color

00:04:05.209 --> 00:04:07.430
of jelly bean is actually linked to acne.

00:04:07.430 --> 00:04:09.109
That’s a 1 in 20 chance.

00:04:09.109 --> 00:04:11.420
And we just did 20 separate tests.

00:04:11.420 --> 00:04:15.440
So what’s the likelihood here that we’d
incorrectly reject the null?

00:04:15.440 --> 00:04:19.410
Turns out with 20 tests--it’s way higher
than 5%.

00:04:19.410 --> 00:04:24.680
If jelly beans are not linked with acne, then
each individual test has a 5% chance of being

00:04:24.680 --> 00:04:28.620
significant, and a 95% chance of not being
significant.

00:04:28.620 --> 00:04:35.920
So the probability of having NONE of our 20
tests come up significant is 0.95 to the twentieth

00:04:35.920 --> 00:04:38.210
power, or about 36%.

00:04:38.210 --> 00:04:43.440
That means that about 64% of the time, 1 or more of these test will be significant, just

00:04:43.440 --> 00:04:47.830
by chance, even though jelly beans have no
effect on acne.

00:04:47.830 --> 00:04:52.370
And 64% is a lot higher than the 5% chance
you may have been expecting.

00:04:52.370 --> 00:04:56.960
This inflated Type I error rate is called
the Family Wise Error rate.

00:04:56.960 --> 00:05:01.650
When doing multiple related tests, or even
multiple follow up comparisons on a significant

00:05:01.650 --> 00:05:05.830
ANOVA test, Family Wise Error rates can go
up quite a lot.

00:05:05.830 --> 00:05:10.190
Which means that if the null is true, we’re
going to get a LOT more significant results

00:05:10.190 --> 00:05:14.500
than our prescribed Type I error rate of 5%
implies.

00:05:14.500 --> 00:05:18.570
If you’re a researcher who put a lot of
heart, time, and effort into doing a study

00:05:18.570 --> 00:05:22.640
similar to our jelly bean one, and you found
a non-significant overall effect, that’s

00:05:22.640 --> 00:05:23.940
pretty rough. Dissapointing.

00:05:23.950 --> 00:05:26.880
No one is likely to publish your non-results.

00:05:26.890 --> 00:05:31.410
But we don’t want to just keep running tests until we find something significant.

00:05:31.410 --> 00:05:35.700
A Cornell food science lab was studying the effects of the price of a buffet on the amount

00:05:35.700 --> 00:05:37.360
people ate at that buffet.

00:05:37.360 --> 00:05:40.700
They set up a buffet and charged half the
people full price, and gave the other half

00:05:40.700 --> 00:05:42.570
a 50% discount.

00:05:42.570 --> 00:05:47.070
The experiment tracked what people ate, how much they ate, and who they ate it with, and

00:05:47.070 --> 00:05:48.750
had them fill out a long questionnaire.

00:05:48.750 --> 00:05:53.000
The original hypothesis was that there is
an effect of buffet price on the amount that

00:05:53.000 --> 00:05:54.080
people eat.

00:05:54.080 --> 00:05:57.920
But after running their planned analysis,
it turned out that there wasn’t a statistically

00:05:57.920 --> 00:05:59.230
significant difference.

00:05:59.230 --> 00:06:03.760
So, according to emails published by Buzzfeed, the head of the lab encouraged another lab

00:06:03.760 --> 00:06:07.780
member to do some digging and look at all
sorts of smaller groups.

00:06:07.780 --> 00:06:12.010
“males, females, lunch goers, dinner goers,
people sitting alone, people eating in groups

00:06:12.010 --> 00:06:17.000
of 2, people eating in groups of 2+, people
who order alcohol, people who order soft drinks,

00:06:17.000 --> 00:06:20.470
people who sit close to buffet, people who
sit far away…”

00:06:20.470 --> 00:06:24.740
According to those same emails, they also
tested these groups on several different variables

00:06:24.740 --> 00:06:29.370
like “[number of] pieces of pizza, [number
of] trips, fill level of plate, did they get

00:06:29.370 --> 00:06:31.040
dessert, did they order a drink...”

00:06:31.040 --> 00:06:33.780
Results from this study were eventually published in 4 different papers.

00:06:33.790 --> 00:06:35.160
And got media attention.

00:06:35.160 --> 00:06:40.280
But one was later retracted and 3 of the papers had corrections issued because of accusations

00:06:40.280 --> 00:06:43.130
of p-hacking and other unethical data practices.

00:06:43.130 --> 00:06:47.760
The fact that there were a few, out of many,
statistical tests conducted by this team that

00:06:47.760 --> 00:06:51.450
were statistically significant is no surprise.

00:06:51.450 --> 00:06:53.830
Many researchers have criticized these results.

00:06:53.830 --> 00:06:58.310
Just like in our fake jelly bean experiment,
they created a huge number of possible tests.

00:06:58.310 --> 00:07:03.050
And even if buffet price had no effect on
the eating habits of buffet goers, we know

00:07:03.050 --> 00:07:08.460
that some, if not many, of these tests were
likely to be significant just by chance.

00:07:08.460 --> 00:07:12.640
And the more analyses that were conducted, the more likely finding those fluke results becomes.

00:07:12.640 --> 00:07:18.060
By the time you do 14 separate tests, it’s
more likely than not that you’ll get at

00:07:18.070 --> 00:07:22.250
LEAST one statistically significant result,
even if there’s nothing there.

00:07:22.250 --> 00:07:26.410
The main problem arises when those few significant results are reported without the context of

00:07:26.410 --> 00:07:27.820
all the non-significant ones.

00:07:27.820 --> 00:07:29.470
Let’s pretend that you make firecrackers.

00:07:29.470 --> 00:07:33.960
And you’re new to making fire crackers. You're not great at it. And sometimes make mistakes that cause

00:07:33.960 --> 00:07:36.510
the crackers to fizzle when they should go
“BOOM”.

00:07:36.510 --> 00:07:40.050
You make one batch of 100 firecrackers and only 5 of them work.

00:07:40.050 --> 00:07:43.440
You take those 5 exploded firecrackers (with video proof that they really went off) to

00:07:43.440 --> 00:07:47.210
a business meeting to try to convince some Venture Capitalists to give you some money

00:07:47.210 --> 00:07:48.210
to grow your business.

00:07:48.210 --> 00:07:52.200
Conveniently, they don’t ask whether you
made any other failed firecrackers.

00:07:52.200 --> 00:07:53.990
They think you’re showing them everything
you made.

00:07:53.990 --> 00:07:57.400
And you start to feel a little bad about taking their million dollars.

00:07:57.400 --> 00:08:00.710
Instead, you do the right thing, and tell
them that you actually made 100 firecrackers

00:08:00.710 --> 00:08:03.620
and these are just the ones that turned out
okay.

00:08:03.620 --> 00:08:08.220
Once they know that 95 of the firecrackers that you made failed, they’re not

00:08:08.220 --> 00:08:09.000
going to give you money.

00:08:09.000 --> 00:08:11.980
Multiple statistical tests on the same data
are similar.

00:08:11.980 --> 00:08:16.240
Significant results usually indicate to us
that something interesting could be happening.

00:08:16.240 --> 00:08:18.280
That’s why we use significance tests.

00:08:18.280 --> 00:08:22.770
But if you see only 5 out of 100 tests are
significant you’re probably going to be

00:08:22.770 --> 00:08:27.800
a bit more suspicious that those significant
results are false positives.

00:08:27.800 --> 00:08:30.720
Those 5 good firecrackers may have just been good luck.

00:08:30.720 --> 00:08:34.680
When researchers conduct many statistical
tests, but only report the significant ones,

00:08:34.680 --> 00:08:36.220
it’s misleading.

00:08:36.220 --> 00:08:40.630
Depending on how transparent they are, it
can even seem like they only ran 5 tests,

00:08:40.630 --> 00:08:42.479
of which 5 were significant.

00:08:42.479 --> 00:08:44.740
There is a way to account for Family Wise
Errors.

00:08:44.740 --> 00:08:48.330
The world is complex, and sometimes so are the experiments that we use to explore it.

00:08:48.330 --> 00:08:52.720
While it’s important for people doing research to define the hypotheses they’re going to

00:08:52.720 --> 00:08:57.390
test before they look at any data, it’s
understandable that during the course of the

00:08:57.390 --> 00:08:59.639
experiment they may get new ideas.

00:08:59.639 --> 00:09:04.360
One simple way around this is to correct for the inflation in your Family Wise Error rate.

00:09:04.360 --> 00:09:08.199
If you want the overall Type I error rate
for all your tests to be 5%, then you can

00:09:08.199 --> 00:09:10.779
adjust your p-values accordingly.

00:09:10.780 --> 00:09:14.540
One very simple way to do this is to apply
a Bonferroni correction.

00:09:14.540 --> 00:09:20.080
Instead of setting a usual threshold--like
0.05--to decide when a p-value is significant

00:09:20.089 --> 00:09:25.520
or non-significant, you take the usual threshold and divide it by the number of tests you’re doing.

00:09:25.520 --> 00:09:29.880
If we wanted to test the effect of 5 different
health measures on risk of stroke, we would

00:09:29.880 --> 00:09:34.060
take our original threshold--0.05--and divide
by 5.

00:09:34.079 --> 00:09:36.790
That leaves us with a new cutoff of 0.01.

00:09:36.790 --> 00:09:40.920
So in order to determine if the effect of
hours of exercise--or any of our other 4 measures--has

00:09:40.920 --> 00:09:47.580
a significant effect on your risk of stroke,
you would need to have a p-value of below 0.01

00:09:47.589 --> 00:09:48.800
instead of 0.05.

00:09:48.810 --> 00:09:52.839
This may seem like a lot of hoopla over a
few extra statistical tests, but making sure

00:09:52.839 --> 00:09:57.230
that we limit the likelihood of putting out
false research is really important.

00:09:57.230 --> 00:10:00.870
We always want to put out good research, and as much as possible, we want the results we

00:10:00.870 --> 00:10:02.560
publish to be correct.

00:10:02.560 --> 00:10:06.559
If you don’t do research yourself, these
problems can seem far removed from your everyday

00:10:06.559 --> 00:10:08.510
life, but they still affect you.

00:10:08.510 --> 00:10:12.999
These results might affect the amount of chemicals that are allowed in your food and water, or

00:10:12.999 --> 00:10:14.899
laws that politicians are writing.

00:10:14.899 --> 00:10:19.300
And spotting questionable science means you not have to avoid those green jelly beans.

00:10:19.300 --> 00:10:21.769
Cause green jelly beans are clearly the best.

00:10:21.769 --> 00:10:24.100
Thanks for watching, I’ll see you next time.

