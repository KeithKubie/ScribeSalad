WEBVTT
Kind: captions
Language: en

00:00:00.230 --> 00:00:03.730
There's a work out of Stanford and
UC Berkeley

00:00:03.730 --> 00:00:08.780
on using reinforcement learning to
actually fly model helicopter tricks.

00:00:08.780 --> 00:00:09.800
&gt;&gt; Like what?

00:00:09.800 --> 00:00:12.310
&gt;&gt; So it turns out that if you,
you're flying a model helicopter,

00:00:12.310 --> 00:00:13.870
I've tried to do this.

00:00:13.870 --> 00:00:15.230
Turns out to be really hard to do.

00:00:15.230 --> 00:00:19.920
Part of it is you have to coordinate
the blades in the blades in the Blades.

00:00:19.920 --> 00:00:25.360
&gt;&gt; Yeah, and part of it is that you
can do crazy things with helicopters,

00:00:25.360 --> 00:00:26.130
if you know how to do it.

00:00:26.130 --> 00:00:28.680
You can make them fly upside down,
you can make them swing back and

00:00:28.680 --> 00:00:32.549
forth, you can make them kind
of roll over and over and over.

00:00:32.549 --> 00:00:34.420
You can do all sorts of really,
well I don't know,

00:00:34.420 --> 00:00:36.390
you can do all sorts of amazing things,
but

00:00:36.390 --> 00:00:40.030
professional stunt flying helicopter
people can do amazing things.

00:00:40.030 --> 00:00:43.760
What the reinforcement learning work was
about is trying to get a system to learn

00:00:43.760 --> 00:00:45.980
to do those same kinds of tricks.

00:00:45.980 --> 00:00:48.170
&gt;&gt; And they were able to do
this with real helicopters or

00:00:48.170 --> 00:00:49.340
real model helicopters?

00:00:49.340 --> 00:00:51.800
&gt;&gt; A real model helicopter,
not a simulator, but

00:00:51.800 --> 00:00:54.380
an actual Mini helicopter.

00:00:54.380 --> 00:00:55.050
&gt;&gt; Okay, so that's cool.

00:00:55.050 --> 00:00:59.450
So you sort of convinced me that burlap
and repository, that's sort of a good.

00:00:59.450 --> 00:01:00.158
Okay, I buy that.

00:01:00.158 --> 00:01:01.767
But I do want to point out
something that you said,

00:01:01.767 --> 00:01:02.876
that I thought was interesting.

00:01:02.876 --> 00:01:04.608
You talked about simulators and

00:01:04.608 --> 00:01:08.140
that maybe what we really want to
do is work in the real world.

00:01:08.140 --> 00:01:11.370
But it occurs to me that at
least one of those examples,

00:01:11.370 --> 00:01:12.980
the simulator is the real world.

00:01:12.980 --> 00:01:14.540
So, games, right?

00:01:14.540 --> 00:01:16.460
So if you have games like Backgammon,

00:01:16.460 --> 00:01:19.920
if you have simulations that
people actually interact with

00:01:21.020 --> 00:01:24.330
then that is actually the real problem
itself, that is the real world.

00:01:24.330 --> 00:01:27.129
But if I'm playing Pac-Man and
I've got a simulator for Pac-Man,

00:01:27.129 --> 00:01:28.950
that's the same thing as having Pac-Man.

00:01:28.950 --> 00:01:31.740
And if I can learn to play Pac-Man,
I've learn to play Pac-Man.

00:01:31.740 --> 00:01:33.200
There's not really a simulator involved.

00:01:33.200 --> 00:01:36.517
&gt;&gt; I would make a distinction between
grid world type simulators where

00:01:36.517 --> 00:01:38.380
they were developed specifically for

00:01:38.380 --> 00:01:40.948
reinforcement learning
researchers to work with.

00:01:40.948 --> 00:01:44.134
Versus, as you say, the kinds of
simulators that people actually interact

00:01:44.134 --> 00:01:46.638
with when they're doing things
like playing video games.

00:01:46.638 --> 00:01:50.421
&gt;&gt; And so, video games actually is
a really great intermediate step between

00:01:50.421 --> 00:01:54.480
real physical world stuff and
things that can be run in simulation.

00:01:54.480 --> 00:01:57.320
And actually there's been some
terrific work in that space.

00:01:57.320 --> 00:01:58.190
&gt;&gt; Really, like what?

00:01:58.190 --> 00:02:00.790
&gt;&gt; Well, one of the things that
I'm very excited about recently is

00:02:00.790 --> 00:02:05.180
a group called Deep Mind, which
they're very good at naming things.

00:02:05.180 --> 00:02:06.770
Have a system for

00:02:06.770 --> 00:02:10.020
actually doing what they call deep
reinforcement learning, deep q learning.

00:02:10.020 --> 00:02:14.920
Where they are actually trying to
learn behavior in Atari video games.

00:02:14.920 --> 00:02:19.110
Video games from the 80s, where they
actually are using Atari emulators,

00:02:19.110 --> 00:02:21.870
the actual games as people
actually play them.

00:02:21.870 --> 00:02:25.540
Taking the information that's
available on the screen as input and

00:02:25.540 --> 00:02:27.390
sending joystick commands as output, and

00:02:27.390 --> 00:02:30.270
learning to play actually at
the level of human players.

00:02:30.270 --> 00:02:32.950
&gt;&gt; Wait,
they're as good as human players?

00:02:32.950 --> 00:02:35.740
&gt;&gt; They are as good on
average as human players.

00:02:35.740 --> 00:02:37.980
In some of the games, they're
actually much better than people.

00:02:37.980 --> 00:02:41.710
Games like Breakout where you
have to bounce a ball all around.

00:02:41.710 --> 00:02:42.340
&gt;&gt; Okay.
&gt;&gt; And

00:02:42.340 --> 00:02:45.480
then there's other games where they're
actually much much worse than people.

00:02:45.480 --> 00:02:49.120
Games like Frost Bite where you have
to jump around on a bunch of ice floes.

00:02:49.120 --> 00:02:50.390
It seems very difficult for

00:02:50.390 --> 00:02:52.780
the system to actually learn
what to do in that game.

00:02:52.780 --> 00:02:57.070
&gt;&gt; Okay, so their heads in an oven,
their feet is in ice water.

00:02:57.070 --> 00:02:58.248
But on average, they're comfortable.

00:02:58.248 --> 00:02:59.408
&gt;&gt; I see what you're saying.

00:02:59.408 --> 00:03:02.698
So, your point is that it's not
playing just like a person.

00:03:02.698 --> 00:03:05.197
It's playing better and
worse than people, so

00:03:05.197 --> 00:03:08.456
on average you can say that it's
kind of playing like people.

00:03:08.456 --> 00:03:10.200
&gt;&gt; Mm-hm, but still that's kind of cool.

00:03:10.200 --> 00:03:11.220
&gt;&gt; I think it's really amazing and

00:03:11.220 --> 00:03:15.980
I think it's that particular data set
is itself really interesting, because

00:03:15.980 --> 00:03:20.220
there's 40 some odd Atari video games
that are all built in the same system.

00:03:20.220 --> 00:03:22.850
You can use the same inputs, and
the same outputs, and the same learning

00:03:22.850 --> 00:03:25.310
algorithm, but you can put in different
games to see what it will do.

00:03:25.310 --> 00:03:28.656
&gt;&gt; Okay, so it is just like Weka and
it's just like UCI, except here,

00:03:28.656 --> 00:03:32.590
Weka is the burlap algorithms or
whatever, reinforcement algorithms.

00:03:32.590 --> 00:03:36.850
And UCI, instead of being input output
pairs are sort of the real world.

00:03:36.850 --> 00:03:40.090
&gt;&gt; Yeah, and there's some work that's
being done in collecting data from

00:03:40.090 --> 00:03:40.590
the real world.

00:03:40.590 --> 00:03:44.778
For example, medical diagnosis or
online education.

00:03:44.778 --> 00:03:45.998
&gt;&gt; I like that,
we should do something like that.

00:03:45.998 --> 00:03:46.614
&gt;&gt; [LAUGH] For

00:03:46.614 --> 00:03:51.630
trying to figure out what the right way
of interacting with people online is.

00:03:51.630 --> 00:03:53.830
People have collected some of that data
and there's reinforcement learning

00:03:53.830 --> 00:03:55.440
algorithms that are being
run at that data.

00:03:55.440 --> 00:03:59.370
It's not the full reinforcement learning
problem, because you don't actually,

00:03:59.370 --> 00:04:02.830
the learning system doesn't actually
have to decide how to collect the data,

00:04:02.830 --> 00:04:04.260
it's already been collected for it.

00:04:04.260 --> 00:04:06.950
But it is really using real data and
making decisions based on that and

00:04:06.950 --> 00:04:09.340
I think that's a really
promising direction for

00:04:09.340 --> 00:04:10.970
reinforcement learning
to go in the future.

00:04:10.970 --> 00:04:12.926
&gt;&gt; Okay, I like it.
So I got two things out of that.

00:04:12.926 --> 00:04:16.048
The first thing is reinforcement
learning really can be used in the real

00:04:16.048 --> 00:04:17.504
world, it has been, it can be and

00:04:17.504 --> 00:04:19.757
there's a nice bright future for
it that's good.

00:04:19.757 --> 00:04:21.168
And the second thing is I was right,

00:04:21.168 --> 00:04:23.220
there was a bunch of stuff
we hadn't talked about.

00:04:23.220 --> 00:04:25.855
&gt;&gt; Yeah, okay, all right,
that's a valid point.

00:04:25.855 --> 00:04:27.955
&gt;&gt; Okay, was there anything else
you want to wrap up with anything?

00:04:27.955 --> 00:04:31.735
&gt;&gt; Well, I feel like we should say maybe
a little bit about the current direction

00:04:31.735 --> 00:04:33.105
in the future of reinforcement learning.

00:04:33.105 --> 00:04:36.365
So, one of the things I think is really
neat is that it's having an impact

00:04:36.365 --> 00:04:38.675
not just on engineering and
computer science, but

00:04:38.675 --> 00:04:42.935
also on the behavioral sciences and
the neurosciences.

00:04:42.935 --> 00:04:43.710
&gt;&gt; How so?

00:04:43.710 --> 00:04:46.730
&gt;&gt; Well so, the sorts of things that we
think about reinforcement learning being

00:04:46.730 --> 00:04:50.070
good for, are an agent it's
interacting with some environment,

00:04:50.070 --> 00:04:51.250
it's making decisions.

00:04:51.250 --> 00:04:54.030
It's trying to figure out what to do,
it's trying to be happy.

00:04:54.030 --> 00:04:57.610
And so you could argue that animals and
people are trying to do exactly those

00:04:57.610 --> 00:05:00.980
sorts of things,
that we are reinforcement learners.

00:05:00.980 --> 00:05:02.980
&gt;&gt; Sure animals do it, people do it.

00:05:02.980 --> 00:05:03.748
&gt;&gt; Yeah.
&gt;&gt; Birds do it.

00:05:03.748 --> 00:05:06.350
&gt;&gt; Educated fleas have been known
to do reinforcement learning.

00:05:06.350 --> 00:05:06.850
&gt;&gt; Right, okay.

00:05:06.850 --> 00:05:10.240
&gt;&gt; And so, the interesting thing about
that is the Nora scientists when they're

00:05:10.240 --> 00:05:11.060
trying to think about, okay.

00:05:11.060 --> 00:05:13.929
Well, how can we understand the
algorithms that essentially that people

00:05:13.929 --> 00:05:14.750
are running.

00:05:14.750 --> 00:05:17.380
They're turning to the reinforcement
learning field as a source of

00:05:17.380 --> 00:05:21.100
inspiration for the structure of
the algorithm, how they can be set up.

00:05:21.100 --> 00:05:23.390
What sorts of things seem to work and

00:05:23.390 --> 00:05:27.334
not work and they're actually looking
for evidence of these in real brain.

00:05:27.334 --> 00:05:28.796
&gt;&gt; I like that.
I like that a lot.

00:05:28.796 --> 00:05:30.245
And you know what I
really like about that?

00:05:30.245 --> 00:05:32.550
That it takes us back to the beginning
of the machine learning class.

00:05:32.550 --> 00:05:34.960
It takes us back to the beginning of
the reinforcement learning class,

00:05:34.960 --> 00:05:37.980
where we tried to emphasize the point
that reinforcement learning

00:05:37.980 --> 00:05:40.730
is not just a set of algorithms,
it's not just an approach.

00:05:40.730 --> 00:05:43.340
It's actually a way of
thinking about problems.

00:05:43.340 --> 00:05:46.588
And so really this whole class
has been about solving problems.

00:05:46.588 --> 00:05:48.535
And I feel like we have
solved some problems.

00:05:48.535 --> 00:05:51.150
&gt;&gt; Great, I think that's
a wonderful note to end things on,

00:05:51.150 --> 00:05:52.110
what do you think?

00:05:52.110 --> 00:05:54.140
&gt;&gt; No, I think this is really valuable,
thanks so much.

00:05:54.140 --> 00:05:57.020
&gt;&gt; All right well,
thank you very much, Michael.

00:05:57.020 --> 00:05:59.400
I appreciate it and
I will see you in the next class.

00:06:01.610 --> 00:06:02.110
Bye.

