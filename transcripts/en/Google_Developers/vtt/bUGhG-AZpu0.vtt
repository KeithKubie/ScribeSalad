WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.332
[MUSIC PLAYING]

00:00:06.670 --> 00:00:09.170
CHRIS KELLEY: Thank you
so much for joining us.

00:00:09.170 --> 00:00:10.420
My name is Chris.

00:00:10.420 --> 00:00:12.190
I'm a designer and
prototyper working

00:00:12.190 --> 00:00:15.150
on immersive
prototyping at Google,

00:00:15.150 --> 00:00:17.170
and I'm joined by
Ellie and Luca.

00:00:17.170 --> 00:00:19.960
And today, we're going to talk
about exploring AR interaction.

00:00:19.960 --> 00:00:21.251
It's really awesome to be here.

00:00:23.380 --> 00:00:26.560
We explore immersive computing
through rapid prototyping

00:00:26.560 --> 00:00:29.350
of AR and VR experiments.

00:00:29.350 --> 00:00:34.210
Often, that's focused on use
case exploration or app ideas.

00:00:34.210 --> 00:00:37.710
We work fast, which
means we fail fast,

00:00:37.710 --> 00:00:41.050
but that means
that we learn fast.

00:00:41.050 --> 00:00:43.930
We spend a week or two on
each prototyping sprint,

00:00:43.930 --> 00:00:45.551
and at the end of
the sprint, we end

00:00:45.551 --> 00:00:47.050
with a functional
prototype starting

00:00:47.050 --> 00:00:49.600
from a tightly scoped question.

00:00:49.600 --> 00:00:51.760
And then we put that
prototype in people's hands

00:00:51.760 --> 00:00:52.968
and we see what we can learn.

00:00:56.140 --> 00:00:58.420
So this talk is going to
be about takeaways we have

00:00:58.420 --> 00:01:01.150
from those AR explorations.

00:01:01.150 --> 00:01:03.400
But first, I want to set
the table a little bit

00:01:03.400 --> 00:01:05.850
and talk about what we mean
when we say augmented reality.

00:01:08.770 --> 00:01:10.621
When a lot of people
think about AR,

00:01:10.621 --> 00:01:13.120
the first thing they think about
is bringing virtual objects

00:01:13.120 --> 00:01:14.685
to users in the world.

00:01:14.685 --> 00:01:15.310
And it is that.

00:01:15.310 --> 00:01:16.390
That's part of it.

00:01:16.390 --> 00:01:17.980
We call this the out of AR.

00:01:20.540 --> 00:01:22.880
But AR also means
more than that.

00:01:22.880 --> 00:01:25.100
It means being able to
understand the world visually

00:01:25.100 --> 00:01:28.175
to bring information to users,
and we call this understanding

00:01:28.175 --> 00:01:30.969
the in of AR.

00:01:30.969 --> 00:01:32.510
Many of the tools
and techniques that

00:01:32.510 --> 00:01:35.750
were created for computer vision
and machine learning perfectly

00:01:35.750 --> 00:01:38.735
complement tools like
ARCore, which is Google's AR

00:01:38.735 --> 00:01:39.610
development platform.

00:01:42.500 --> 00:01:45.070
So when we explore AR,
we build experiences

00:01:45.070 --> 00:01:47.220
that include one of
these approaches or both.

00:01:49.950 --> 00:01:52.820
So this talk is going to
be about three magic powers

00:01:52.820 --> 00:01:55.130
that we've found for AR.

00:01:55.130 --> 00:01:58.130
We think that these magic powers
can help you build better AR

00:01:58.130 --> 00:02:00.039
experiences for your users.

00:02:00.039 --> 00:02:02.330
So we're going to talk about
some prototypes that we've

00:02:02.330 --> 00:02:04.820
built and share our
learnings with you

00:02:04.820 --> 00:02:07.490
during each of these three magic
power areas during the talk.

00:02:10.300 --> 00:02:13.860
First, I'll talk to you about
context-driven superpowers.

00:02:13.860 --> 00:02:16.180
That's about how we can
combine visual and physical

00:02:16.180 --> 00:02:21.220
understanding of the world to
make magical AR experiences.

00:02:21.220 --> 00:02:25.172
Then Ellie will talk to you
about shared augmentations.

00:02:25.172 --> 00:02:27.130
And this is really all
about the different ways

00:02:27.130 --> 00:02:29.514
that we can connect
people together in AR,

00:02:29.514 --> 00:02:31.930
and how we can empower them
just by putting them together.

00:02:37.210 --> 00:02:40.270
And then Luca will
cover expressive inputs.

00:02:40.270 --> 00:02:42.160
This is about how
AR can help unlock

00:02:42.160 --> 00:02:44.860
authentic and natural
understanding for our users.

00:02:48.830 --> 00:02:52.030
So let's start about
context-driven superpowers.

00:02:52.030 --> 00:02:54.810
What this really means
is using AR technologies

00:02:54.810 --> 00:02:57.640
to deeply understand
the context of a device,

00:02:57.640 --> 00:03:02.200
and then build experiences that
directly leverage that context.

00:03:02.200 --> 00:03:04.670
And there's two parts
to an AR context.

00:03:04.670 --> 00:03:06.670
One is visual
understanding, and the other

00:03:06.670 --> 00:03:09.220
is physical understanding.

00:03:09.220 --> 00:03:11.110
With ARCore, this
gives your phone

00:03:11.110 --> 00:03:12.730
the ability to
understand and sense

00:03:12.730 --> 00:03:15.339
its environment physically.

00:03:15.339 --> 00:03:17.380
But through computer vision
and machine learning,

00:03:17.380 --> 00:03:19.954
we can make sense of
the world visually.

00:03:19.954 --> 00:03:21.370
And by combining
these results, we

00:03:21.370 --> 00:03:23.890
get an authentic
understanding of the scene,

00:03:23.890 --> 00:03:27.210
which is a natural building
block of magical AR.

00:03:30.010 --> 00:03:32.910
So let's start with
visual understanding.

00:03:32.910 --> 00:03:35.410
The prototyping community has
done some awesome explorations

00:03:35.410 --> 00:03:36.910
here, and we've done
a few of our own

00:03:36.910 --> 00:03:38.076
that we're excited to share.

00:03:43.060 --> 00:03:45.400
To start, we
wondered if we could

00:03:45.400 --> 00:03:50.230
trigger custom experiences from
visual signals in the world.

00:03:50.230 --> 00:03:52.660
Traditional apps today
leverage all kinds of device

00:03:52.660 --> 00:03:54.390
signals to trigger experiences.

00:03:54.390 --> 00:03:57.130
GPS, the IMU, et cetera.

00:03:57.130 --> 00:04:01.180
So could we use visual
input as a signal as well?

00:04:01.180 --> 00:04:03.760
We built a really basic
implementation of this concept.

00:04:03.760 --> 00:04:05.620
This uses ARCore and
the Google Cloud Vision

00:04:05.620 --> 00:04:08.410
API that detects
any kind of snowman

00:04:08.410 --> 00:04:10.720
in the scene, which triggers
a particle system that

00:04:10.720 --> 00:04:12.820
starts to snow.

00:04:12.820 --> 00:04:14.620
So through visual
understanding, we

00:04:14.620 --> 00:04:17.140
were able to tailor an
experience to specific cues

00:04:17.140 --> 00:04:19.510
in the environment for users.

00:04:19.510 --> 00:04:23.960
This enables adaptable and
context aware applications.

00:04:23.960 --> 00:04:27.110
So even though this
example is a simple one,

00:04:27.110 --> 00:04:29.720
the concept can be
extended so much further.

00:04:29.720 --> 00:04:32.710
For example, yesterday we
announced the augmented images

00:04:32.710 --> 00:04:34.360
API for ARCore.

00:04:34.360 --> 00:04:36.190
So if you use this,
you can make something

00:04:36.190 --> 00:04:41.650
like an experience that reacts
relative to device movement

00:04:41.650 --> 00:04:43.510
around an image in
the scene, or even

00:04:43.510 --> 00:04:46.812
from a known distance to
an object in the world.

00:04:46.812 --> 00:04:48.520
If you think this
concept is interesting,

00:04:48.520 --> 00:04:51.784
I highly recommend checking
out the AR VR demo tent.

00:04:51.784 --> 00:04:53.950
They have some amazing
augmented images demos there.

00:04:58.240 --> 00:05:00.190
The next thing we
wanted to know is

00:05:00.190 --> 00:05:03.571
if we could bridge the gap
between digital and physical,

00:05:03.571 --> 00:05:06.070
and, for example, bring some
of the most delightful features

00:05:06.070 --> 00:05:09.490
of e-readers to physical books.

00:05:09.490 --> 00:05:11.860
The digital age has brought
all kinds of improvements

00:05:11.860 --> 00:05:14.890
to some traditional
human behaviors,

00:05:14.890 --> 00:05:17.830
and e-readers have brought lots
of cool new things to reading.

00:05:17.830 --> 00:05:19.538
But if you're like
me, sometimes you just

00:05:19.538 --> 00:05:24.529
missed the tactility in holding
a great book in your hands.

00:05:24.529 --> 00:05:26.570
So we wanted to know if
we could bridge that gap.

00:05:26.570 --> 00:05:29.590
In this prototype, users
highlight a passage or word

00:05:29.590 --> 00:05:31.390
with their finger
and they instantly

00:05:31.390 --> 00:05:33.860
get back a definition.

00:05:33.860 --> 00:05:37.120
This is a great example of a
short-form-focused interaction

00:05:37.120 --> 00:05:39.160
that required no
setup for users.

00:05:39.160 --> 00:05:41.080
It was an easy win
only made possible

00:05:41.080 --> 00:05:43.810
by visual understanding.

00:05:43.810 --> 00:05:45.642
But as soon as we
tried this prototype,

00:05:45.642 --> 00:05:47.350
there were two downfalls
that we noticed,

00:05:47.350 --> 00:05:50.380
and they became immediately
apparent when we used it.

00:05:50.380 --> 00:05:53.530
The first is that it was really
difficult to aim your finger

00:05:53.530 --> 00:05:55.810
at a small moving
target on a phone,

00:05:55.810 --> 00:05:57.670
and maybe the page
is moving as well,

00:05:57.670 --> 00:05:59.545
and you're trying to
target this little word.

00:05:59.545 --> 00:06:00.820
That was really hard.

00:06:00.820 --> 00:06:03.246
And the second was that when
you're highlighting a word,

00:06:03.246 --> 00:06:04.870
your finger is blocking
the exact thing

00:06:04.870 --> 00:06:07.240
that you're trying to see.

00:06:07.240 --> 00:06:10.690
Now, these are easily solvable
with a follow-up UX iteration,

00:06:10.690 --> 00:06:13.490
but they illustrate
a larger lesson.

00:06:13.490 --> 00:06:15.960
And that's that with any
kind of immersive computing,

00:06:15.960 --> 00:06:20.190
you really have to try it
before you can judge it.

00:06:20.190 --> 00:06:22.540
An interaction might sound
great when you talk about it

00:06:22.540 --> 00:06:24.814
and it might even look
good in a visual mock,

00:06:24.814 --> 00:06:26.230
but until you have
it in your hand

00:06:26.230 --> 00:06:28.180
and you can feel it
and try it, you're

00:06:28.180 --> 00:06:30.424
not going to know
if it works or not.

00:06:30.424 --> 00:06:32.090
You really have to
put it in a prototype

00:06:32.090 --> 00:06:33.465
so you can create
your own facts.

00:06:38.290 --> 00:06:40.000
Another thing we
think about a lot

00:06:40.000 --> 00:06:42.670
is, can we help people
learn more effectively?

00:06:42.670 --> 00:06:45.910
Could we use AR to
make learning better?

00:06:45.910 --> 00:06:47.775
There's many styles of
learning, and if you

00:06:47.775 --> 00:06:49.150
combine these
styles of learning,

00:06:49.150 --> 00:06:52.980
it often results in faster
and higher-quality learning.

00:06:52.980 --> 00:06:56.500
In this prototype, we
combined visual, oral, verbal,

00:06:56.500 --> 00:06:58.810
and kinesthetic learning
to teach people how

00:06:58.810 --> 00:07:01.810
to make the perfect espresso.

00:07:01.810 --> 00:07:02.900
The videos explain--

00:07:02.900 --> 00:07:03.400
I'm sorry.

00:07:03.400 --> 00:07:06.250
We placed videos around
the espresso machine

00:07:06.250 --> 00:07:08.962
in the physical locations
where that step occurs.

00:07:08.962 --> 00:07:10.920
So if you were learning
how to use the grinder,

00:07:10.920 --> 00:07:14.030
the video for the grinder
is right next to it.

00:07:14.030 --> 00:07:16.276
Now, for users to
trigger that video,

00:07:16.276 --> 00:07:17.650
they move their
phone to the area

00:07:17.650 --> 00:07:19.640
and then they can
watch the lesson.

00:07:19.640 --> 00:07:22.690
That added physical component
of the physical proximity

00:07:22.690 --> 00:07:25.020
of the video and
the actual device

00:07:25.020 --> 00:07:27.850
made a huge difference
in general understanding.

00:07:27.850 --> 00:07:31.240
In our studies, users who had
never used an espresso machine

00:07:31.240 --> 00:07:35.890
before easily made an espresso
after using this prototype.

00:07:35.890 --> 00:07:37.420
So for some kinds
of learning, this

00:07:37.420 --> 00:07:39.760
can be really
beneficial for users.

00:07:39.760 --> 00:07:41.410
Now, unfortunately
for our prototype,

00:07:41.410 --> 00:07:43.180
one thing that we
learned here was

00:07:43.180 --> 00:07:45.640
that it's actually really
hard to hold your phone

00:07:45.640 --> 00:07:48.270
and make an espresso
at the same time.

00:07:48.270 --> 00:07:51.274
So you need to be really
mindful of the fact

00:07:51.274 --> 00:07:52.690
that your users
might be splitting

00:07:52.690 --> 00:07:56.030
their physical resources
between the phone and the world.

00:07:56.030 --> 00:07:57.930
And so as it applies
to your use case,

00:07:57.930 --> 00:08:00.100
try building experiences
that are really

00:08:00.100 --> 00:08:02.004
snackable and hands-free.

00:08:06.450 --> 00:08:09.889
Speaking of combining learning
and superpowers together,

00:08:09.889 --> 00:08:11.430
we wondered if AR
could help us learn

00:08:11.430 --> 00:08:13.554
from hidden information
that's layered in the world

00:08:13.554 --> 00:08:15.640
all around us.

00:08:15.640 --> 00:08:17.650
This is a prototype
that we built

00:08:17.650 --> 00:08:20.490
that's an immersive
language learning app.

00:08:20.490 --> 00:08:23.490
We showed translations roughly
next to objects of interest

00:08:23.490 --> 00:08:26.280
and positioned these labels
by taking a point cloud

00:08:26.280 --> 00:08:29.040
sample from around the object
and putting the label sort

00:08:29.040 --> 00:08:31.758
of in the middle of the points.

00:08:31.758 --> 00:08:34.049
Users found this kind of
immersive learning really fun,

00:08:34.049 --> 00:08:36.120
and we saw users
freely exploring

00:08:36.120 --> 00:08:39.510
the world looking for other
things to learn about.

00:08:39.510 --> 00:08:41.250
So we found that
if you give people

00:08:41.250 --> 00:08:44.105
the freedom to roam and tools
that are simple and flexible,

00:08:44.105 --> 00:08:45.730
the experiences that
you build for them

00:08:45.730 --> 00:08:46.860
can create immense value.

00:08:51.260 --> 00:08:53.930
We now have physical
understanding.

00:08:53.930 --> 00:08:56.240
This is AR's ability
to extract and infer

00:08:56.240 --> 00:08:59.720
information and meaning
from the world around you.

00:08:59.720 --> 00:09:02.210
When a device knows exactly
where it is, not only in space,

00:09:02.210 --> 00:09:04.779
but also relative
to other devices,

00:09:04.779 --> 00:09:06.320
we can start to do
things that really

00:09:06.320 --> 00:09:10.550
feel like you have superpowers.

00:09:10.550 --> 00:09:12.710
For example, we
can start to make

00:09:12.710 --> 00:09:14.990
interactions that are
extremely physical, natural,

00:09:14.990 --> 00:09:16.275
and delightful.

00:09:16.275 --> 00:09:17.900
Humans have been
physically interacting

00:09:17.900 --> 00:09:19.980
with each other for
a really long time,

00:09:19.980 --> 00:09:22.571
but digital life has abstracted
some of those interactions.

00:09:22.571 --> 00:09:24.320
We wondered if we could
swing the pendulum

00:09:24.320 --> 00:09:28.580
back the other direction
a little bit using AR.

00:09:28.580 --> 00:09:32.570
So in this prototype, much like
a carnival milk bottle game,

00:09:32.570 --> 00:09:34.880
you fling a baseball out
of the top of your phone

00:09:34.880 --> 00:09:38.930
and it hits milk bottles that
are shown on other devices.

00:09:38.930 --> 00:09:42.676
You just point the ball where
you want to go, and it goes.

00:09:42.676 --> 00:09:44.300
We did this by putting
multiple devices

00:09:44.300 --> 00:09:46.010
in a shared coordinate
system, which

00:09:46.010 --> 00:09:49.550
you could do using the new
Google Cloud Anchors API

00:09:49.550 --> 00:09:52.670
that we announced
for ARCore yesterday.

00:09:52.670 --> 00:09:54.380
And one thing you'll
notice here is

00:09:54.380 --> 00:09:57.192
that we aren't even showing
users past their camera.

00:09:57.192 --> 00:09:59.150
Now, we did that deliberately
because we really

00:09:59.150 --> 00:10:00.950
wanted to stretch
and see how far we

00:10:00.950 --> 00:10:04.430
could take this concept
of physical interaction.

00:10:04.430 --> 00:10:06.860
And one thing we learned
was that once people learned

00:10:06.860 --> 00:10:08.690
to do it, they found
it really natural

00:10:08.690 --> 00:10:10.970
and actually had a
lot of fun with it.

00:10:10.970 --> 00:10:14.360
But almost every user that
tried it had to be not only

00:10:14.360 --> 00:10:17.540
told how to do it, but
shown how to do it.

00:10:17.540 --> 00:10:19.470
People actually had to
flip this mental switch

00:10:19.470 --> 00:10:22.700
of the expectations they
have for how a 2D smartphone

00:10:22.700 --> 00:10:24.352
interaction works.

00:10:24.352 --> 00:10:26.810
So you really need to be mindful
of the context that people

00:10:26.810 --> 00:10:28.768
are bringing in and the
mental models they have

00:10:28.768 --> 00:10:30.304
for 2D smartphone interactions.

00:10:35.410 --> 00:10:38.680
We also wanted to know if we
could help someone visualize

00:10:38.680 --> 00:10:43.000
the future in a way that would
let them make better decisions.

00:10:43.000 --> 00:10:45.742
Humans pay attention to the
things that matter to us.

00:10:45.742 --> 00:10:47.200
And in a literal
sense, the imagery

00:10:47.200 --> 00:10:48.760
that appears in our
peripheral vision

00:10:48.760 --> 00:10:50.759
takes a lower cognitive
priority than the things

00:10:50.759 --> 00:10:52.890
we're focused on.

00:10:52.890 --> 00:10:55.750
Would smartphone AR
be any different?

00:10:55.750 --> 00:10:59.080
In this experiment, we
overlaid the architectural mesh

00:10:59.080 --> 00:11:01.660
of the homeowner's remodel on
top of the active construction

00:11:01.660 --> 00:11:03.280
project.

00:11:03.280 --> 00:11:05.980
The homeowner could
visualize in context

00:11:05.980 --> 00:11:09.470
what the changes to their
home was going to look like.

00:11:09.470 --> 00:11:11.930
Now, at the time that this
prototype was created,

00:11:11.930 --> 00:11:14.290
we had to do actual manual
alignment of this model

00:11:14.290 --> 00:11:15.294
on top of the house.

00:11:15.294 --> 00:11:16.210
You could do it today.

00:11:16.210 --> 00:11:19.240
If I rebuilt it, I would
use the augmented images API

00:11:19.240 --> 00:11:20.570
that we announced yesterday.

00:11:20.570 --> 00:11:22.153
It would be much
easier to put a fixed

00:11:22.153 --> 00:11:25.150
image in a location, the
house, and sync them together.

00:11:25.150 --> 00:11:27.910
But even with that initial
friction for the UX,

00:11:27.910 --> 00:11:30.220
the homeowner got tremendous
value out of this.

00:11:30.220 --> 00:11:34.074
In fact, they went back to their
architect after seeing this

00:11:34.074 --> 00:11:35.740
and changed the design
of their new home

00:11:35.740 --> 00:11:37.600
because they found out
that they weren't going

00:11:37.600 --> 00:11:39.974
to have enough space in the
upstairs bathroom-- something

00:11:39.974 --> 00:11:42.490
they hadn't noticed
in the plans before.

00:11:42.490 --> 00:11:45.430
So the lesson is that if you
provide people high-quality,

00:11:45.430 --> 00:11:51.430
personally relevant
content, you can create ways

00:11:51.430 --> 00:11:53.950
that people will find really
valuable and attention grabbing

00:11:53.950 --> 00:11:55.067
experiences.

00:11:58.847 --> 00:12:00.680
But when does modifying
the real environment

00:12:00.680 --> 00:12:01.940
start to break down?

00:12:01.940 --> 00:12:04.585
You may be familiar
with the uncanny valley.

00:12:04.585 --> 00:12:05.960
It's a concept
that suggests when

00:12:05.960 --> 00:12:08.120
things that are really
familiar to humans

00:12:08.120 --> 00:12:10.420
are almost right but
just a little bit off,

00:12:10.420 --> 00:12:12.500
it makes us feel uneasy.

00:12:12.500 --> 00:12:14.810
Subtle manipulations of
the real environment in AR

00:12:14.810 --> 00:12:16.820
can sometimes feel similar.

00:12:16.820 --> 00:12:19.380
It can be difficult
to get right.

00:12:19.380 --> 00:12:20.990
In this specific
example, we tried

00:12:20.990 --> 00:12:22.550
removing things from the world.

00:12:22.550 --> 00:12:26.922
We created this AR invisibility
cloak for the plant.

00:12:26.922 --> 00:12:29.380
What we did was we created a
point cloud around the object,

00:12:29.380 --> 00:12:31.880
attached little cubes
to the point cloud,

00:12:31.880 --> 00:12:33.710
applied a material
to those points,

00:12:33.710 --> 00:12:36.840
and extracted the texture from
the surrounding environment.

00:12:36.840 --> 00:12:38.840
That worked pretty well
in uniform environments,

00:12:38.840 --> 00:12:41.360
but unfortunately, the world
doesn't have too many of those.

00:12:41.360 --> 00:12:44.630
It's made up of dynamic
lighting and subtle patterns,

00:12:44.630 --> 00:12:47.294
so this always ended up
looking a little bit weird.

00:12:47.294 --> 00:12:49.460
Remember to be thoughtful
about the way that you add

00:12:49.460 --> 00:12:51.440
or remove things
from the environment.

00:12:51.440 --> 00:12:53.420
People are really
perceptive, and so you

00:12:53.420 --> 00:12:55.250
need to strive to
build experiences

00:12:55.250 --> 00:12:56.900
that align with
their expectations,

00:12:56.900 --> 00:12:58.483
or at the very least,
don't defy them.

00:13:01.080 --> 00:13:04.129
But is physical understanding
always critical?

00:13:04.129 --> 00:13:05.920
All points in the
section have their place,

00:13:05.920 --> 00:13:08.419
but, ultimately, you have to
be guided by your critical user

00:13:08.419 --> 00:13:09.700
journeys.

00:13:09.700 --> 00:13:11.440
In this example,
we wanted to build

00:13:11.440 --> 00:13:15.430
a viewer for this amazing 3D
model by Damon [INAUDIBLE]..

00:13:15.430 --> 00:13:17.770
It was important that people
could see the model in 3D

00:13:17.770 --> 00:13:19.975
and move around to
discover the object.

00:13:19.975 --> 00:13:21.850
A challenge, though,
was that the camera feed

00:13:21.850 --> 00:13:24.649
was creating a lot of visual
noise and distraction.

00:13:24.649 --> 00:13:26.440
People were having a
hard time appreciating

00:13:26.440 --> 00:13:28.540
the nuances of the model.

00:13:28.540 --> 00:13:31.240
We adopted concepts from
filmmaking and guided users

00:13:31.240 --> 00:13:34.030
by using focus and
depth of field,

00:13:34.030 --> 00:13:36.940
all which were controlled
by the user's motion.

00:13:36.940 --> 00:13:39.271
This resulted in people
feeling encouraged to explore,

00:13:39.271 --> 00:13:41.020
and they really stopped
getting distracted

00:13:41.020 --> 00:13:44.000
by the physical environment.

00:13:44.000 --> 00:13:46.520
So humans are already
great at so many things.

00:13:46.520 --> 00:13:49.540
AR really allows us to leverage
those existing capabilities

00:13:49.540 --> 00:13:52.750
to make interactions
feel invisible.

00:13:52.750 --> 00:13:56.110
If we leverage visual and
physical understanding

00:13:56.110 --> 00:13:58.420
together, we can
build experiences that

00:13:58.420 --> 00:14:00.840
really give people superpowers.

00:14:00.840 --> 00:14:02.560
With that, Ellie is
going to talk to you

00:14:02.560 --> 00:14:05.619
about special opportunities we
have in shared augmentations.

00:14:05.619 --> 00:14:06.910
ELLIE NATTINGER: Thanks, Chris.

00:14:10.720 --> 00:14:12.580
So I'm Ellie Nattinger.

00:14:12.580 --> 00:14:14.530
I'm a software
engineer and prototyper

00:14:14.530 --> 00:14:17.020
on Google's VR and AR team.

00:14:17.020 --> 00:14:19.510
Chris has talked about
the kinds of experiences

00:14:19.510 --> 00:14:22.180
you start to have when
your devices can understand

00:14:22.180 --> 00:14:24.010
the world around
you, and I'm going

00:14:24.010 --> 00:14:27.460
to talk about what happens when
you can share those experiences

00:14:27.460 --> 00:14:30.170
with the people around you.

00:14:30.170 --> 00:14:34.330
We're interested not only
in adding AR augmentations

00:14:34.330 --> 00:14:38.980
to your own reality, but also
in sharing those augmentations.

00:14:38.980 --> 00:14:42.010
If you listened to the
developer keynote yesterday,

00:14:42.010 --> 00:14:44.040
you know that shared
AR experiences

00:14:44.040 --> 00:14:48.130
is a really big topic
for us these days.

00:14:48.130 --> 00:14:51.250
For one thing, a
shared reality lets

00:14:51.250 --> 00:14:55.070
people be immersed in
the same experience.

00:14:55.070 --> 00:14:56.590
Think about a movie theater.

00:14:56.590 --> 00:14:58.520
Why do movie theaters exist?

00:14:58.520 --> 00:15:01.270
Everybody's watching a movie
that they could probably

00:15:01.270 --> 00:15:04.600
watch at home on their
television or their computer

00:15:04.600 --> 00:15:06.550
by themselves much
more comfortably not

00:15:06.550 --> 00:15:10.660
having to go anywhere,
but it feels qualitatively

00:15:10.660 --> 00:15:15.220
different to be in a space
with other people sharing

00:15:15.220 --> 00:15:17.670
that experience.

00:15:17.670 --> 00:15:20.710
And beyond those kinds of
shared passive experiences,

00:15:20.710 --> 00:15:25.000
having a shared reality lets
you collaborate, lets you learn,

00:15:25.000 --> 00:15:27.610
lets you build
and play together.

00:15:27.610 --> 00:15:30.280
We think you should be able to
share your augmented realities

00:15:30.280 --> 00:15:33.070
with your friends, and your
families, and your colleagues,

00:15:33.070 --> 00:15:35.140
so we've done a
variety of explorations

00:15:35.140 --> 00:15:38.230
about how do you build
those kinds of shared

00:15:38.230 --> 00:15:40.430
realities in AR.

00:15:40.430 --> 00:15:42.970
First, there's kind of
a technical question.

00:15:42.970 --> 00:15:47.830
How do you get people
aligned in a shared AR space?

00:15:47.830 --> 00:15:49.960
There's a number of
ways we've tried.

00:15:49.960 --> 00:15:52.000
If you don't need
a lot of accuracy,

00:15:52.000 --> 00:15:55.210
you could just start your
apps with all the devices

00:15:55.210 --> 00:15:57.970
in approximately
the same location.

00:15:57.970 --> 00:16:00.550
You could use markers
or augmented images

00:16:00.550 --> 00:16:04.570
so multiple users can all point
their devices at one picture

00:16:04.570 --> 00:16:07.200
and get a common
point of reference--

00:16:07.200 --> 00:16:11.440
cures the zero, zero,
zero of my virtual world.

00:16:11.440 --> 00:16:15.130
And you can even use the
new ARCore Cloud Anchors API

00:16:15.130 --> 00:16:17.800
that we just announced
yesterday to localize

00:16:17.800 --> 00:16:20.830
multiple devices against
the visual features

00:16:20.830 --> 00:16:23.860
of a particular space.

00:16:23.860 --> 00:16:26.140
In addition to the
technical considerations,

00:16:26.140 --> 00:16:28.630
we've found three
axes of experience

00:16:28.630 --> 00:16:30.520
that we think are really
useful to consider

00:16:30.520 --> 00:16:32.770
when you're designing
these kinds of shared

00:16:32.770 --> 00:16:34.780
augmented experiences.

00:16:34.780 --> 00:16:38.670
First of those is
co-located versus remote.

00:16:38.670 --> 00:16:41.500
Are your users in the
same physical space

00:16:41.500 --> 00:16:44.490
or different physical spaces?

00:16:44.490 --> 00:16:50.460
Second is, how much precision
is required, or is it optional?

00:16:50.460 --> 00:16:54.090
Do you have to have everybody
see the virtual bunny

00:16:54.090 --> 00:16:56.670
at exactly the same
point in the world,

00:16:56.670 --> 00:16:59.920
or do you have a little bit
of flexibility about that?

00:16:59.920 --> 00:17:02.160
And the third is
whether your experience

00:17:02.160 --> 00:17:04.020
is synchronous or asynchronous.

00:17:04.020 --> 00:17:07.230
Is everybody participating
in this augmented experience

00:17:07.230 --> 00:17:11.310
at exactly the same time, or
at slightly different times?

00:17:11.310 --> 00:17:14.910
And we see these not as
necessarily binary axes,

00:17:14.910 --> 00:17:17.099
but more of a
continuum that you can

00:17:17.099 --> 00:17:20.430
consider when you're designing
these multi-person AR

00:17:20.430 --> 00:17:22.229
experiences.

00:17:22.229 --> 00:17:24.270
So let's talk about some
prototypes and apps that

00:17:24.270 --> 00:17:26.603
fall on different points of
the spectrum and the lessons

00:17:26.603 --> 00:17:28.740
we've learned from them.

00:17:28.740 --> 00:17:30.540
To start with, we've
found that when

00:17:30.540 --> 00:17:32.640
you've got a group
that's interacting

00:17:32.640 --> 00:17:35.230
with the same content
in the same space,

00:17:35.230 --> 00:17:39.900
you really need shared,
precise, spatial registration.

00:17:39.900 --> 00:17:42.870
For example, let's say
you're in a classroom.

00:17:42.870 --> 00:17:44.550
Imagine if a group
of students who

00:17:44.550 --> 00:17:47.960
are doing a unit on the solar
system could all look at

00:17:47.960 --> 00:17:51.660
and walk around the globe,
or an asteroid field,

00:17:51.660 --> 00:17:53.490
or look at the sun.

00:17:53.490 --> 00:17:56.970
In Expeditions AR, one
of Google's initial AR

00:17:56.970 --> 00:18:00.570
experiences, all the students
can point their devices

00:18:00.570 --> 00:18:02.640
to a marker, they
calibrate themselves

00:18:02.640 --> 00:18:05.250
against a shared location,
they see the object

00:18:05.250 --> 00:18:09.300
in the same place, and
then what this allows

00:18:09.300 --> 00:18:11.310
is for a teacher to
be able to point out

00:18:11.310 --> 00:18:13.420
particular parts of the object.

00:18:13.420 --> 00:18:17.160
Oh, if you all come over and
look at this side of the sun,

00:18:17.160 --> 00:18:19.440
you see a cut-out into its core.

00:18:19.440 --> 00:18:22.530
Over here on the Earth,
you can see a hurricane.

00:18:22.530 --> 00:18:25.620
Everybody starts get a
spatial understanding

00:18:25.620 --> 00:18:29.320
of the parts of the object and
where they are in the world.

00:18:29.320 --> 00:18:31.230
So when does it matter
that your shared

00:18:31.230 --> 00:18:33.090
space has a lot of precision?

00:18:33.090 --> 00:18:34.720
When you have
multiple people who

00:18:34.720 --> 00:18:37.470
are all in the
same physical space

00:18:37.470 --> 00:18:40.230
interacting with or
looking at the exact same

00:18:40.230 --> 00:18:42.225
augmented objects
at the same time.

00:18:45.410 --> 00:18:46.910
We were also curious--

00:18:46.910 --> 00:18:50.120
how much can we take advantage
of people's existing spatial

00:18:50.120 --> 00:18:55.160
awareness when you're working
in high-precision shared spaces?

00:18:55.160 --> 00:18:58.460
We experimented with this in
this multi-person construction

00:18:58.460 --> 00:19:01.010
application, where you've
got multiple people who

00:19:01.010 --> 00:19:05.900
are all building onto a shared
AR object in the same space.

00:19:05.900 --> 00:19:08.690
Adding blocks to each
other, everybody's

00:19:08.690 --> 00:19:10.430
being able to coordinate.

00:19:10.430 --> 00:19:13.340
And you want to be able to tell
what part of the object someone

00:19:13.340 --> 00:19:14.390
is working on.

00:19:14.390 --> 00:19:17.810
Have your physical movement
support that collaboration.

00:19:17.810 --> 00:19:19.910
Like, if Chris is
over here and he's

00:19:19.910 --> 00:19:22.800
placing some green
blocks in the real world,

00:19:22.800 --> 00:19:24.410
I'm not going to
step in front of him

00:19:24.410 --> 00:19:27.020
and start putting yellow
blocks there instead.

00:19:27.020 --> 00:19:32.360
We've got a natural sense of how
to collaborate, how to arrange,

00:19:32.360 --> 00:19:35.060
how to coordinate
ourselves in space.

00:19:35.060 --> 00:19:36.680
People already have that sense.

00:19:36.680 --> 00:19:39.650
So we can keep
that in a shared AR

00:19:39.650 --> 00:19:44.540
if we've got our virtual objects
precisely lined up enough.

00:19:44.540 --> 00:19:46.220
We also found it
helpful to notice

00:19:46.220 --> 00:19:50.180
that because you can see both
the digital object but also

00:19:50.180 --> 00:19:52.910
the other people through
the pass-through camera,

00:19:52.910 --> 00:19:55.160
you are able to get a pretty
good sense of what people

00:19:55.160 --> 00:19:58.130
were looking at as well as what
they were interacting with.

00:20:01.320 --> 00:20:03.090
We've also wondered
what would it

00:20:03.090 --> 00:20:07.380
feel like to have a shared AR
experience for multiple people

00:20:07.380 --> 00:20:10.740
in the same space, but who
aren't necessarily interacting

00:20:10.740 --> 00:20:12.520
with the same things?

00:20:12.520 --> 00:20:17.040
So think of this more
like an AR LAN party.

00:20:17.040 --> 00:20:18.810
Where we're all
in the same space,

00:20:18.810 --> 00:20:20.610
or maybe could be
different spaces,

00:20:20.610 --> 00:20:23.070
we're seeing connected
things, and we're

00:20:23.070 --> 00:20:24.730
having a shared experience.

00:20:24.730 --> 00:20:28.500
So this prototype's a
competitive quiz guessing game

00:20:28.500 --> 00:20:30.450
where you look at
the map and you

00:20:30.450 --> 00:20:33.600
have to figure out where on the
globe you think is represented

00:20:33.600 --> 00:20:35.760
and stick your
pushpin in, get points

00:20:35.760 --> 00:20:38.040
depending on how close you are.

00:20:38.040 --> 00:20:40.770
We've got the state synced,
so we know who's winning.

00:20:40.770 --> 00:20:43.530
But the location of
where that globe is

00:20:43.530 --> 00:20:45.990
doesn't actually need
to be synchronized.

00:20:45.990 --> 00:20:48.930
And maybe you don't want it to
be synchronized because I don't

00:20:48.930 --> 00:20:51.780
want anybody to get a clue
based on where I'm sticking

00:20:51.780 --> 00:20:53.310
my pushpin into the globe.

00:20:53.310 --> 00:20:55.800
It's fun to be together,
even when we're not looking

00:20:55.800 --> 00:20:59.910
at exactly the same AR things.

00:20:59.910 --> 00:21:03.840
And do we always need our
spaces to align exactly?

00:21:03.840 --> 00:21:06.610
Sometimes it's enough just
to be in the same room.

00:21:06.610 --> 00:21:09.930
This prototype example's
of an AR boat race.

00:21:09.930 --> 00:21:12.420
You blow on the
microphone of your phone,

00:21:12.420 --> 00:21:15.540
and it creates the wind
that propels your boat

00:21:15.540 --> 00:21:18.630
down the little AR track.

00:21:18.630 --> 00:21:21.640
By us being next to each
other when we start the app

00:21:21.640 --> 00:21:24.540
and spawn the track, we get
a shared physical experience

00:21:24.540 --> 00:21:27.660
even though our AR worlds
might not perfectly align.

00:21:27.660 --> 00:21:31.080
We get to keep all the elements
of the social game play--

00:21:31.080 --> 00:21:34.044
talking to each other,
our physical presence--

00:21:34.044 --> 00:21:36.210
but we're not necessarily
touching the same objects.

00:21:39.040 --> 00:21:42.000
Another super interesting
area we've been playing with

00:21:42.000 --> 00:21:45.510
is how audio can
be a way to include

00:21:45.510 --> 00:21:49.560
multiple people in a single
device AR experience.

00:21:49.560 --> 00:21:53.150
If you think of the standard
Magic Window device AR,

00:21:53.150 --> 00:21:54.720
it's a pretty
personal experience.

00:21:54.720 --> 00:21:57.930
I'm looking at this
thing through my phone.

00:21:57.930 --> 00:22:02.010
But now, imagine you can
leave a sound in AR that

00:22:02.010 --> 00:22:06.660
has a 3D position like
any other virtual thing,

00:22:06.660 --> 00:22:08.400
and now you start to
be able to hear it,

00:22:08.400 --> 00:22:10.860
even if you're not
necessarily looking at it.

00:22:10.860 --> 00:22:12.780
And other people
can hear the sound

00:22:12.780 --> 00:22:14.890
from your device
at the same time.

00:22:14.890 --> 00:22:18.090
So for an example, let's say
you could leave little notes

00:22:18.090 --> 00:22:18.990
all over your space.

00:22:18.990 --> 00:22:20.350
Might look something like this.

00:22:25.320 --> 00:22:26.972
I'm a plant.

00:22:26.972 --> 00:22:28.460
I'm a plant.

00:22:28.460 --> 00:22:31.932
I'm a plant.

00:22:31.932 --> 00:22:33.420
I'm elephant.

00:22:33.420 --> 00:22:34.908
I'm elephant.

00:22:34.908 --> 00:22:37.390
I'm elephant.

00:22:37.390 --> 00:22:38.635
This is a chair.

00:22:38.635 --> 00:22:39.746
This is a chair.

00:22:39.746 --> 00:22:41.174
This is a chair.

00:22:41.174 --> 00:22:42.602
I'm a plant.

00:22:42.602 --> 00:22:43.554
I'm a plant.

00:22:43.554 --> 00:22:44.506
I'm elephant.

00:22:44.506 --> 00:22:45.268
I'm elephant.

00:22:45.268 --> 00:22:45.934
This is a chair.

00:22:45.934 --> 00:22:49.440
This is a chair.

00:22:49.440 --> 00:22:51.940
So notice, you don't have
to be the one with a phone

00:22:51.940 --> 00:22:54.400
to get a sense of where
these audio annotations start

00:22:54.400 --> 00:22:55.825
to live in physical space.

00:22:59.400 --> 00:23:01.080
Another question we've asked--

00:23:01.080 --> 00:23:04.770
if you have a synchronous AR
experience with multiple people

00:23:04.770 --> 00:23:08.640
who are in different places,
what kind of representation

00:23:08.640 --> 00:23:10.600
do you need of the other person?

00:23:10.600 --> 00:23:13.650
So let's imagine you have
maybe a shared AR photos app

00:23:13.650 --> 00:23:15.900
where multiple people
can look at photos

00:23:15.900 --> 00:23:17.670
that are arranged in space.

00:23:17.670 --> 00:23:20.220
So I'm taking pictures
in one location,

00:23:20.220 --> 00:23:22.890
I'm viewing them
arranged around me in AR,

00:23:22.890 --> 00:23:24.870
and then I want to
share my AR experience

00:23:24.870 --> 00:23:26.430
with Luca, who comes
in and joins me

00:23:26.430 --> 00:23:28.470
from a remote location.

00:23:28.470 --> 00:23:31.380
What we found-- we needed
a couple of things to make

00:23:31.380 --> 00:23:34.740
us feel like we were connected
and sharing the same AR

00:23:34.740 --> 00:23:37.440
experience, even though we
were in different places.

00:23:37.440 --> 00:23:40.050
We needed to have a voice
connection so we could actually

00:23:40.050 --> 00:23:42.180
talk about the
pictures, and we needed

00:23:42.180 --> 00:23:44.339
to know where the other
person was looking.

00:23:44.339 --> 00:23:46.380
See which picture you're
paying attention to when

00:23:46.380 --> 00:23:47.700
you're talking about it.

00:23:47.700 --> 00:23:50.040
But what was interesting
is we didn't actually

00:23:50.040 --> 00:23:53.610
need to know where the other
person was, as long as we had

00:23:53.610 --> 00:23:55.140
that shared frame of reference.

00:23:55.140 --> 00:23:57.320
We're all here, here's
what I'm looking at,

00:23:57.320 --> 00:24:00.570
here's what Luca's looking at.

00:24:00.570 --> 00:24:04.920
We've also been curious
about asymmetric experiences.

00:24:04.920 --> 00:24:08.250
What happens when users
share the same space

00:24:08.250 --> 00:24:10.980
and the same augmentations,
but they've got different roles

00:24:10.980 --> 00:24:12.520
in the experience?

00:24:12.520 --> 00:24:14.640
So for instance,
in this prototype,

00:24:14.640 --> 00:24:18.240
Chris is using his phone as a
controller to draw in space,

00:24:18.240 --> 00:24:21.600
but he's not actually seeing
the AR annotations he's drawing.

00:24:21.600 --> 00:24:23.700
The other person sees
the same AR content

00:24:23.700 --> 00:24:25.799
and uses their phone
to take a video.

00:24:25.799 --> 00:24:28.090
They're playing different
roles in the same experience.

00:24:28.090 --> 00:24:30.999
Kind of artist versus
cinematographer.

00:24:30.999 --> 00:24:32.790
And we found there
could be some challenges

00:24:32.790 --> 00:24:35.100
to asymmetric
experiences if there's

00:24:35.100 --> 00:24:37.740
a lack of information about
what the other person is

00:24:37.740 --> 00:24:38.850
experiencing.

00:24:38.850 --> 00:24:41.820
For instance, Chris can't
tell what Luca's filming

00:24:41.820 --> 00:24:47.450
or see how his drawing
looks from far away.

00:24:47.450 --> 00:24:49.970
So as we mentioned
previously, these kinds

00:24:49.970 --> 00:24:51.920
of different
combinations of space,

00:24:51.920 --> 00:24:55.970
and time, and precision are
relevant for multi-person AR

00:24:55.970 --> 00:24:58.520
experiences, and they have
different technical and

00:24:58.520 --> 00:25:00.360
experiential needs.

00:25:00.360 --> 00:25:02.810
If you have multiple
people in the same space

00:25:02.810 --> 00:25:06.210
with the same augmentations
at the same time,

00:25:06.210 --> 00:25:07.550
then you need a way of sharing.

00:25:07.550 --> 00:25:09.950
You need a way of
common localization.

00:25:09.950 --> 00:25:13.730
That's why we created the
new Cloud Anchors API.

00:25:13.730 --> 00:25:16.340
If you've got multiple
people in the same space

00:25:16.340 --> 00:25:18.830
with different augmentations
at the same time,

00:25:18.830 --> 00:25:21.230
the kind of AR LAN
party model, you

00:25:21.230 --> 00:25:23.330
need some way to share data.

00:25:23.330 --> 00:25:24.770
And if you've got
multiple people

00:25:24.770 --> 00:25:28.310
in different spaces interacting
with the same augmentations

00:25:28.310 --> 00:25:30.110
at the same time,
you need sharing

00:25:30.110 --> 00:25:34.520
in some kind of representation
of that interaction.

00:25:34.520 --> 00:25:37.100
So shared AR experiences
is a big area.

00:25:37.100 --> 00:25:38.990
We've explored some
parts of the space.

00:25:38.990 --> 00:25:41.630
We'd love to see what
you all come up with.

00:25:41.630 --> 00:25:43.752
So Chris has talked
about examples

00:25:43.752 --> 00:25:45.710
where your device
understands your surroundings

00:25:45.710 --> 00:25:48.291
and gives you special powers,
I talked about examples

00:25:48.291 --> 00:25:49.790
where you've got
multiple people who

00:25:49.790 --> 00:25:51.340
can collaborate and interact.

00:25:51.340 --> 00:25:53.840
Now Luca will talk about what
happens when your devices have

00:25:53.840 --> 00:25:55.760
a better understanding
of you and allow

00:25:55.760 --> 00:25:57.717
for more expressive inputs.

00:25:57.717 --> 00:25:58.216
Luca?

00:26:00.910 --> 00:26:03.630
LUCA PRASSO: Thank you, Ellie.

00:26:03.630 --> 00:26:06.230
My name is Luca Prasso,
and I'm a prototyper

00:26:06.230 --> 00:26:10.870
and a technical artist working
in the Google AR and VR team.

00:26:10.870 --> 00:26:12.690
So let's talk about
the device that you

00:26:12.690 --> 00:26:16.020
carry with you every day and the
ones that are all around you,

00:26:16.020 --> 00:26:18.450
and how they can provide
the meaningful and authentic

00:26:18.450 --> 00:26:23.140
signals that we can use in
our augmented experiences.

00:26:23.140 --> 00:26:25.890
So ARCore tracks
the device motion

00:26:25.890 --> 00:26:29.910
as we move to the real world
and provides some understanding

00:26:29.910 --> 00:26:31.570
of the environment.

00:26:31.570 --> 00:26:35.940
And these signals can be used to
create powerful, and creative,

00:26:35.940 --> 00:26:39.210
and expressive tools,
and offer new ways for us

00:26:39.210 --> 00:26:42.510
to interact with
digital content.

00:26:42.510 --> 00:26:45.600
So the data represents
who we are, what we know,

00:26:45.600 --> 00:26:47.680
and what we have.

00:26:47.680 --> 00:26:49.500
And we were interested
in understanding

00:26:49.500 --> 00:26:53.220
if the user can connect more
deeply if the data is displayed

00:26:53.220 --> 00:26:58.140
around them in 3D, and through
AR and physical aspirations,

00:26:58.140 --> 00:27:00.430
they can look at this data.

00:27:00.430 --> 00:27:03.390
So we took a database of
several thousand world cities,

00:27:03.390 --> 00:27:04.890
and we mapped it
in an area that's

00:27:04.890 --> 00:27:06.930
wide as a football field.

00:27:06.930 --> 00:27:11.520
We assign a dot to every city
and we scale the dot based

00:27:11.520 --> 00:27:14.220
on the population of the city.

00:27:14.220 --> 00:27:16.200
And each country has
a different color.

00:27:16.200 --> 00:27:21.870
So now you can walk
to this data field.

00:27:21.870 --> 00:27:26.100
And as ARCore tracks
the motion of the user,

00:27:26.100 --> 00:27:28.080
we play footsteps in sync.

00:27:28.080 --> 00:27:31.570
You take a step and
you hear a step.

00:27:31.570 --> 00:27:34.530
And [INAUDIBLE] sound
fields surrounds the user

00:27:34.530 --> 00:27:37.770
and enhances the experience
and the sense of exploration

00:27:37.770 --> 00:27:39.380
of this data forest.

00:27:39.380 --> 00:27:43.080
And flight paths are
displayed up in the sky.

00:27:43.080 --> 00:27:46.230
And the pass-through
camera is heavily tinted

00:27:46.230 --> 00:27:49.690
so that we can allow the
user to focus on the data

00:27:49.690 --> 00:27:52.680
and then still give
a sense of presence.

00:27:52.680 --> 00:27:56.130
And what happens is the user, as
he walks to the physical space,

00:27:56.130 --> 00:27:58.770
he starts mapping, and
pairing, and creating

00:27:58.770 --> 00:28:02.950
this mental map between the
data and the physical location.

00:28:02.950 --> 00:28:06.160
And starts understanding
better, in this particular case,

00:28:06.160 --> 00:28:09.330
the relative distance
between the places.

00:28:09.330 --> 00:28:11.910
And what we discover is
also that the gestures that

00:28:11.910 --> 00:28:16.330
are a part of our digital life
every day, a pinch to zoom,

00:28:16.330 --> 00:28:20.230
it's now in AR something
more traditional.

00:28:20.230 --> 00:28:23.370
It's actually moving closer
to the digital object

00:28:23.370 --> 00:28:26.310
and inspecting it like
we do with a real object.

00:28:26.310 --> 00:28:29.280
And pan and drag means
taking a couple of steps

00:28:29.280 --> 00:28:33.870
to the right to look
at the information.

00:28:33.870 --> 00:28:37.120
So physical exploration like
this is very fascinating,

00:28:37.120 --> 00:28:39.690
but we need to take into
account all the different users

00:28:39.690 --> 00:28:42.990
and provide the alternative
move and affordances.

00:28:42.990 --> 00:28:45.270
So in AR, a user
can move everywhere,

00:28:45.270 --> 00:28:49.020
but what if he cannot or
he doesn't want to move?

00:28:49.020 --> 00:28:50.220
What if he's sitting?

00:28:50.220 --> 00:28:53.770
So in this particular case, we
allow the user to simply point

00:28:53.770 --> 00:28:56.670
the phone everywhere they
want to go, tap on the screen

00:28:56.670 --> 00:29:01.050
anywhere, and the application
will move the point of view

00:29:01.050 --> 00:29:02.660
in that direction.

00:29:02.660 --> 00:29:06.660
At the same time, we still
have to provide audio, haptics,

00:29:06.660 --> 00:29:11.980
and color effects to enhance
the sense of physical space

00:29:11.980 --> 00:29:14.850
the user has to have
while traveling.

00:29:14.850 --> 00:29:17.970
And so we found that this
is a powerful mechanism

00:29:17.970 --> 00:29:21.060
to explore a certain type of
data that makes sense in the 3D

00:29:21.060 --> 00:29:26.220
space and to allow the user
to discover hidden patterns.

00:29:26.220 --> 00:29:28.980
But can we go beyond
the pixels that you

00:29:28.980 --> 00:29:30.820
can find on your screen?

00:29:30.820 --> 00:29:35.370
We're fascinated by the
spatial audio and a way

00:29:35.370 --> 00:29:39.000
to incorporate audio
into an AR experience.

00:29:39.000 --> 00:29:45.060
So we combine ARCore and
the Google Resonance SDK.

00:29:45.060 --> 00:29:48.700
And Resonance is this very
powerful spatial audio engine

00:29:48.700 --> 00:29:50.610
that recently
Google open-sourced.

00:29:50.610 --> 00:29:53.290
And you should check it
out because it's great.

00:29:53.290 --> 00:29:55.860
And so now I can
take audio sources

00:29:55.860 --> 00:29:59.100
and place them into
the 3D locations,

00:29:59.100 --> 00:30:02.580
and animate them, and describe
the properties of the walls,

00:30:02.580 --> 00:30:06.390
and the ceilings, and the
floor, and all the obstacles.

00:30:06.390 --> 00:30:09.410
And now as the ARCore
moves the point of view,

00:30:09.410 --> 00:30:11.610
it carries with it
the digital ears,

00:30:11.610 --> 00:30:14.640
the Resonance used
to render accurately

00:30:14.640 --> 00:30:17.820
the sounds in the scene.

00:30:17.820 --> 00:30:19.780
So what can we do with this?

00:30:19.780 --> 00:30:23.910
So we imagine, what if I
can sit next to a performer

00:30:23.910 --> 00:30:27.390
during an acoustic concert, or
a classical concert, or a jazz

00:30:27.390 --> 00:30:28.520
performance?

00:30:28.520 --> 00:30:31.170
What if I can be
onstage with actors,

00:30:31.170 --> 00:30:35.310
and listen to their
play, and be there?

00:30:35.310 --> 00:30:39.270
So we took two amazing
actors, Chris and Ellie,

00:30:39.270 --> 00:30:42.240
and we asked them
to record separately

00:30:42.240 --> 00:30:44.050
lines from Shakespeare.

00:30:44.050 --> 00:30:48.510
And we placed these audio
sources a few feet apart

00:30:48.510 --> 00:30:51.240
and we surrounded
the environment

00:30:51.240 --> 00:30:56.220
with an ambisonic sound field of
a rain forest, of the raining.

00:30:56.220 --> 00:31:01.060
And then later on, we switched
to a room with a lot of reverb

00:31:01.060 --> 00:31:02.040
into the walls.

00:31:06.367 --> 00:31:08.700
CHRIS KELLEY: Thou told'st
me they were stolen unto this

00:31:08.700 --> 00:31:11.360
wood, and here I am, and
wode within this wood,

00:31:11.360 --> 00:31:12.941
because I cannot meet my Hermia.

00:31:12.941 --> 00:31:16.354
Hence, get thee gone,
and follow me no more.

00:31:16.354 --> 00:31:18.645
ELLIE NATTINGER: You draw
me, you hard-hearted adamant,

00:31:18.645 --> 00:31:23.085
but yet you draw not iron,
for my heart is true as steel.

00:31:23.085 --> 00:31:26.250
Leave you your power
to draw, and I shall

00:31:26.250 --> 00:31:28.000
have no power to follow you.

00:31:28.000 --> 00:31:29.630
CHRIS KELLEY: Do I entice you?

00:31:29.630 --> 00:31:30.746
Do I speak to you fair?

00:31:30.746 --> 00:31:34.730
Or rather, do I not in plainest
truth tell you, I do not,

00:31:34.730 --> 00:31:37.730
nor I cannot love you?

00:31:37.730 --> 00:31:39.860
LUCA PRASSO: So now the
user can walk around,

00:31:39.860 --> 00:31:43.010
maybe with his eyes closed,
a nice pair of headphones,

00:31:43.010 --> 00:31:46.680
and it's like being on
stage with these actors.

00:31:46.680 --> 00:31:49.730
So we took this example
and we extended it.

00:31:49.730 --> 00:31:54.110
We observed that we can build
in real-time a 2D map of where

00:31:54.110 --> 00:31:56.820
the user has been so
far with his phone

00:31:56.820 --> 00:31:58.940
as he's walking around.

00:31:58.940 --> 00:32:02.150
And so at any given time
when the user hits a button,

00:32:02.150 --> 00:32:06.710
we can programmatically place
audio recording in space

00:32:06.710 --> 00:32:09.410
where we know that the user
can reach with the phone

00:32:09.410 --> 00:32:11.747
and with their ears.

00:32:11.747 --> 00:32:15.170
[MUSIC PLAYING]

00:32:30.330 --> 00:32:34.350
And suddenly, the user
becomes the human mixer

00:32:34.350 --> 00:32:35.910
of this experience.

00:32:35.910 --> 00:32:38.460
And different
instruments can populate

00:32:38.460 --> 00:32:42.300
your squares, and your
rooms, and your schools.

00:32:42.300 --> 00:32:44.730
And this opens the door
to an amazing amount

00:32:44.730 --> 00:32:49.840
of opportunities with AR
audio-first experiments.

00:32:49.840 --> 00:32:51.570
So let's go back to
visual understanding.

00:32:51.570 --> 00:32:54.120
Chris mentioned that the
computer vision and machine

00:32:54.120 --> 00:32:57.240
learning can interpret the
things that are around us,

00:32:57.240 --> 00:33:01.460
and this is also important to
understand the body in turning

00:33:01.460 --> 00:33:03.870
into an expressive controller.

00:33:03.870 --> 00:33:05.970
So in real life,
we are surrounded

00:33:05.970 --> 00:33:08.970
by a lot of sound sources
for all of the places.

00:33:08.970 --> 00:33:11.400
And naturally, our
body and our head

00:33:11.400 --> 00:33:15.570
moves to mix and
focus on what we like

00:33:15.570 --> 00:33:17.310
and what we want to listen to.

00:33:17.310 --> 00:33:23.130
So can we take this intuition
into the way we watch movies

00:33:23.130 --> 00:33:25.990
or play video games
on a mobile device?

00:33:25.990 --> 00:33:30.480
So what we did, we took
the phone camera signal,

00:33:30.480 --> 00:33:33.510
fed it to Google Mobile Vision.

00:33:33.510 --> 00:33:37.150
That gave us a head position
and head orientation.

00:33:37.150 --> 00:33:41.170
And we fed it to
Google Resonance SDK.

00:33:41.170 --> 00:33:44.910
And we said, OK, you're
watching a scene in which actors

00:33:44.910 --> 00:33:48.600
are in a forest, and they're all
around you, and it's raining.

00:33:48.600 --> 00:33:52.860
So now as I leave my phone
far away from my head,

00:33:52.860 --> 00:33:53.920
I hear the forest.

00:33:53.920 --> 00:33:56.580
As I'm taking the phone
closer to my face,

00:33:56.580 --> 00:33:58.140
I start hearing
the actors playing.

00:34:00.810 --> 00:34:02.870
I warn you, this is
an Oscar performance.

00:34:05.810 --> 00:34:09.240
[THUNDER RUMBLES]

00:34:10.134 --> 00:34:11.550
ELLIE NATTINGER:
Our company here.

00:34:14.345 --> 00:34:16.310
CHRIS KELLEY: My man,
according to the script.

00:34:16.310 --> 00:34:18.601
ELLIE NATTINGER: Here is the
scroll of every man's name

00:34:18.601 --> 00:34:21.860
which is thought fit through all
Athens to play in our interlude

00:34:21.860 --> 00:34:24.550
before the duke and the
duchess on his [INAUDIBLE]

00:34:24.550 --> 00:34:26.300
LUCA PRASSO: So now
what is interesting is

00:34:26.300 --> 00:34:28.738
that the tiny little
motions that we

00:34:28.738 --> 00:34:31.279
can do when we're watching and
we're playing this experience,

00:34:31.279 --> 00:34:34.550
it can be turned into
subtle changes in the user

00:34:34.550 --> 00:34:38.260
experience that we can control.

00:34:38.260 --> 00:34:40.929
So we talk about how
the changes in poses

00:34:40.929 --> 00:34:44.260
can become a trigger
to drive interaction.

00:34:44.260 --> 00:34:47.162
In this Google Research
app called [INAUDIBLE],,

00:34:47.162 --> 00:34:49.000
we actually exploit
the opposite--

00:34:49.000 --> 00:34:50.770
the absence of motion.

00:34:50.770 --> 00:34:52.239
And when the user--

00:34:52.239 --> 00:34:53.350
in this case, my kids--

00:34:53.350 --> 00:34:57.800
stop posing, the
app takes a picture.

00:34:57.800 --> 00:35:02.180
And so the simple mechanism that
is triggered by computer vision

00:35:02.180 --> 00:35:04.870
creates the incredible,
delightful opportunities

00:35:04.870 --> 00:35:09.120
that, apparently, my kids love.

00:35:09.120 --> 00:35:12.030
And Research is doing
incredible progress

00:35:12.030 --> 00:35:15.720
in looking at an RGB image
and understanding where

00:35:15.720 --> 00:35:18.720
the body pose and skeleton is.

00:35:18.720 --> 00:35:22.460
And you should check out the
Google Research blog post

00:35:22.460 --> 00:35:26.640
because their post estimation
research is amazing.

00:35:26.640 --> 00:35:31.320
So we took Ellie's video and we
fed it to the machine computer

00:35:31.320 --> 00:35:32.130
algorithm.

00:35:32.130 --> 00:35:35.580
And we got back, a bunch of
3D poses and segmentation

00:35:35.580 --> 00:35:37.470
masks of Ellie.

00:35:37.470 --> 00:35:39.900
And this opens the door
to a lot of variety

00:35:39.900 --> 00:35:43.500
of experiments with
creative filters

00:35:43.500 --> 00:35:45.090
that we can apply to this.

00:35:45.090 --> 00:35:46.950
But what's more
interesting for us

00:35:46.950 --> 00:35:49.590
is that it also allows
us to understand better

00:35:49.590 --> 00:35:54.270
the intent and the
context of the user.

00:35:54.270 --> 00:35:58.760
So we took this pose
estimation technology

00:35:58.760 --> 00:36:01.040
and we added a
digital character.

00:36:01.040 --> 00:36:05.300
Now it tries to mimic what
the human character is doing.

00:36:05.300 --> 00:36:08.570
And this allows [INAUDIBLE]
now to bring your family

00:36:08.570 --> 00:36:09.440
and friends--

00:36:09.440 --> 00:36:11.270
in this case, my son, Noah--

00:36:11.270 --> 00:36:16.520
into the scene so that he can
act and create a nice video.

00:36:16.520 --> 00:36:22.700
But this also, like
Ellie mentioned before,

00:36:22.700 --> 00:36:25.070
we should consider
the situation,

00:36:25.070 --> 00:36:28.190
because this is an
asymmetric experience.

00:36:28.190 --> 00:36:30.500
What you don't see
here is how frustrated

00:36:30.500 --> 00:36:33.050
my son was after a
few minutes because he

00:36:33.050 --> 00:36:34.910
couldn't see what was going on.

00:36:34.910 --> 00:36:38.570
I was the one having fun
taking picture and video him,

00:36:38.570 --> 00:36:39.980
and he didn't see much.

00:36:39.980 --> 00:36:42.140
He could only hear
the lion roaring.

00:36:42.140 --> 00:36:44.540
So we need to be
extremely mindful

00:36:44.540 --> 00:36:47.840
as the developer about
this unbalance of delight.

00:36:47.840 --> 00:36:52.130
And so maybe I should have
passed the image of the phone

00:36:52.130 --> 00:36:57.050
to a nearby TV so I can make
my son first-class citizen

00:36:57.050 --> 00:37:00.650
in this experience.

00:37:00.650 --> 00:37:03.580
So all this AR technology
and the physical

00:37:03.580 --> 00:37:05.910
and the visual understanding
are ingredients

00:37:05.910 --> 00:37:09.820
that allow us to unlock all
kinds of new expressive input

00:37:09.820 --> 00:37:10.826
mechanisms.

00:37:10.826 --> 00:37:11.950
And we are still exploring.

00:37:11.950 --> 00:37:14.480
We're just at the
beginning of this journey.

00:37:14.480 --> 00:37:16.630
But we are excited to
hear what you think

00:37:16.630 --> 00:37:18.910
and what you want
to come up with.

00:37:18.910 --> 00:37:21.610
So to summarize,
we shared a bunch

00:37:21.610 --> 00:37:25.960
of ways in which we think about
AR and various aspirations

00:37:25.960 --> 00:37:28.550
that we have done.

00:37:28.550 --> 00:37:32.200
We talked about expanding
our definition of AR.

00:37:32.200 --> 00:37:35.770
Putting content into the world,
but also pulling information

00:37:35.770 --> 00:37:37.979
from the world.

00:37:37.979 --> 00:37:39.520
And these are all
ingredients that we

00:37:39.520 --> 00:37:43.960
use to create these magical
AR superpowers to enhance

00:37:43.960 --> 00:37:47.500
the social interactions
and to express yourself

00:37:47.500 --> 00:37:49.780
in this new digital medium.

00:37:49.780 --> 00:37:53.170
So we combined
ARCore capabilities

00:37:53.170 --> 00:37:56.170
with different
Google technologies,

00:37:56.170 --> 00:37:57.790
and this gives us
the opportunity

00:37:57.790 --> 00:38:00.850
to explore all these
new interaction models.

00:38:00.850 --> 00:38:02.770
And we encourage
you, developers,

00:38:02.770 --> 00:38:06.020
to stretch your
definition of AR.

00:38:06.020 --> 00:38:08.850
But we want to do this together.

00:38:08.850 --> 00:38:10.660
We're going to keep
exploring, but what

00:38:10.660 --> 00:38:12.880
we want to hear
what tickled you,

00:38:12.880 --> 00:38:15.550
what tickled your curiosity.

00:38:15.550 --> 00:38:17.799
So we can wait to see
what you build next.

00:38:17.799 --> 00:38:19.090
Thank you very much for coming.

00:38:19.090 --> 00:38:23.777
[MUSIC PLAYING]

