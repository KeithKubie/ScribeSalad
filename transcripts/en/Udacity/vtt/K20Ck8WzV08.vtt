WEBVTT
Kind: captions
Language: en

00:00:00.150 --> 00:00:03.740
So last time we were talking about
how contraction mappings make it so

00:00:03.740 --> 00:00:05.670
that value iteration converges you know,

00:00:05.670 --> 00:00:09.120
in the limit that we end up
solving the Bellman equations.

00:00:09.120 --> 00:00:11.550
I'm not going to want to say too
much more about value iteration,

00:00:11.550 --> 00:00:15.080
but on one slide I want to at least
summarize a couple interesting things

00:00:15.080 --> 00:00:16.360
that are worth knowing.

00:00:16.360 --> 00:00:17.470
All right, so here's the first one.

00:00:17.470 --> 00:00:20.704
If you think about QT star,
so that's the Q function

00:00:20.704 --> 00:00:24.610
that we get if we run value
iteration for T star iterations.

00:00:24.610 --> 00:00:25.110
&gt;&gt; Okay.

00:00:25.110 --> 00:00:26.960
&gt;&gt; We know that it
converges in the limit.

00:00:26.960 --> 00:00:30.510
We know that QT eventually
goes to Q star.

00:00:30.510 --> 00:00:35.220
But here's kind of an interesting thing
that we know about the finite horizon.

00:00:35.220 --> 00:00:40.680
So there is some T star,
less than infinity, that's polynomial in

00:00:40.680 --> 00:00:44.640
the number of states, the number of
actions, the magnitude of the rewards in

00:00:44.640 --> 00:00:48.600
the reward function, the number of bits
of precision that are used to specify

00:00:48.600 --> 00:00:52.460
the transition probabilities,
and one over one minus gamma.

00:00:52.460 --> 00:00:57.007
So that, if we run valuation for
that many steps,

00:00:57.007 --> 00:01:01.470
the Q function that we get
out is Q sub T star of SA.

00:01:01.470 --> 00:01:03.870
If we define a policy, pi SA,

00:01:03.870 --> 00:01:07.170
which is just the greedy policy
with respect to that Q function.

00:01:07.170 --> 00:01:09.400
That policy is optimal, okay?

00:01:09.400 --> 00:01:11.710
So what we're saying here is
that we know that in the limit,

00:01:11.710 --> 00:01:13.540
if we run value iteration for

00:01:13.540 --> 00:01:17.920
an infinite number of steps, then the Q
function that we get out at that point,

00:01:17.920 --> 00:01:21.800
the greedy policy with respect
to that Q function, is optimal.

00:01:21.800 --> 00:01:24.880
But what this is saying is that
there is some time before infinity

00:01:24.880 --> 00:01:28.930
where we get a Q function
that's close enough, so

00:01:28.930 --> 00:01:32.600
that if you do the greedy policy with
respect to it, it really is optimal.

00:01:32.600 --> 00:01:34.630
Like all the way 100% optimal.

00:01:34.630 --> 00:01:37.870
&gt;&gt; So what that really
means it's polynomial and

00:01:37.870 --> 00:01:40.270
the things you would expect to matter.

00:01:40.270 --> 00:01:42.800
And so that's saying that you
can actually solve this in

00:01:42.800 --> 00:01:44.250
a reasonable amount of time.

00:01:44.250 --> 00:01:44.950
&gt;&gt; Yeah.

00:01:44.950 --> 00:01:47.220
&gt;&gt; I mean any time I see the word
polynomial, I'm like okay,

00:01:47.220 --> 00:01:49.300
what you're saying is,
this is reasonable.

00:01:49.300 --> 00:01:51.320
&gt;&gt; Yeah,
now the trick in this particular think.

00:01:51.320 --> 00:01:55.220
The reason it's not just polynomial
is this one over one minus gamma.

00:01:55.220 --> 00:01:58.920
So as gamma gets close to one,
this blows up and that's not

00:01:58.920 --> 00:02:03.440
polynomial bounded in say the number of
bits that it takes to write down gamma.

00:02:03.440 --> 00:02:07.590
So it really is exponential in terms
of the number of bits it takes to write

00:02:07.590 --> 00:02:08.475
down the whole problem.

00:02:08.475 --> 00:02:09.258
&gt;&gt; Mm.

00:02:09.258 --> 00:02:13.030
&gt;&gt; But even so, if you take
gamma to be some fixed constant.

00:02:13.030 --> 00:02:16.570
Then one over one minus gamma might be
really big, but it's some fixed constant

00:02:16.570 --> 00:02:18.750
and we're polynomial in all
the rest of this stuff.

00:02:18.750 --> 00:02:23.680
So, what this really all boils down to
is the idea that once you fix an MDP and

00:02:23.680 --> 00:02:26.230
the number of bits of precision
that your using to write it down,

00:02:26.230 --> 00:02:28.390
then there's going to be
some optimal action and

00:02:28.390 --> 00:02:31.590
the might be other actions that are tied
for optimal in any given state.

00:02:31.590 --> 00:02:34.760
But the second best
action is going to be

00:02:34.760 --> 00:02:38.280
some actual bounded
amount away from optimal.

00:02:38.280 --> 00:02:40.530
So, it can't get arbitrarily
close to optimal.

00:02:40.530 --> 00:02:42.330
It's going to be some distance away.

00:02:42.330 --> 00:02:45.400
And what that gives us is that when
we run value iteration enough,

00:02:45.400 --> 00:02:47.730
eventually the distance
between the best action and

00:02:47.730 --> 00:02:50.520
the second best action
gets bigger than that gap.

00:02:51.560 --> 00:02:55.470
And once it's bigger than that gap, then
the greedy policy's going to be optimal.

00:02:55.470 --> 00:02:59.140
It's going to choose the actual
optimal action in all states.

00:02:59.140 --> 00:03:02.150
&gt;&gt; So VI converges and it converges
in a reasonable amount of time.

00:03:02.150 --> 00:03:04.660
&gt;&gt; It's not that it converges
in a reasonable amount of time.

00:03:04.660 --> 00:03:07.350
The greedy policy with
respect to value iteration

00:03:07.350 --> 00:03:09.600
converges in kind of
a reasonable amount of time.

00:03:09.600 --> 00:03:10.820
&gt;&gt; Okay.
That's a much better way of saying it.

00:03:10.820 --> 00:03:11.330
Okay.

00:03:11.330 --> 00:03:12.120
I buy that.

00:03:12.120 --> 00:03:14.170
&gt;&gt; This turns about to be
a consequence of Cramer's rule,

00:03:14.170 --> 00:03:15.270
which we're not going to talk about.

00:03:15.270 --> 00:03:19.460
But this is why, once we write down
everything with polynomial precision,

00:03:19.460 --> 00:03:23.370
that we are guaranteed to get some
fixed size gap, between the best and

00:03:23.370 --> 00:03:24.050
the second best.

00:03:24.050 --> 00:03:26.916
&gt;&gt; It always amazes me how you manage
to get Seinfeld into our conversations.

00:03:26.916 --> 00:03:29.022
&gt;&gt; [LAUGH] Kramer rules.

00:03:29.022 --> 00:03:29.962
&gt;&gt; [LAUGH]

