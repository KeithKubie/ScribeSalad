WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.315
[MUSIC PLAYING]

00:00:06.020 --> 00:00:10.670
AVI GOLDFARB: This
project was trying

00:00:10.670 --> 00:00:15.980
to get a simple view and an
easily understood view of what

00:00:15.980 --> 00:00:19.220
the current excitement
around AI means.

00:00:19.220 --> 00:00:23.823
And we came to this
because of where we are

00:00:23.823 --> 00:00:26.340
at the University of Toronto.

00:00:26.340 --> 00:00:30.429
And a few blocks down from
us in the Business School

00:00:30.429 --> 00:00:31.970
at the University
of Toronto, there's

00:00:31.970 --> 00:00:33.375
this Computer
Science Department.

00:00:33.375 --> 00:00:35.000
And in the Computer
Science Department,

00:00:35.000 --> 00:00:40.800
a lot of the key innovations
around machine learning and AI

00:00:40.800 --> 00:00:42.290
were developed.

00:00:42.290 --> 00:00:45.740
And we together-- me,
Ajay, and Joshua--

00:00:45.740 --> 00:00:48.790
run an organization called
the Creative Destruction Lab.

00:00:48.790 --> 00:00:50.540
And what the Creative
Destruction Lab does

00:00:50.540 --> 00:00:54.190
is it helps early stage
science-based start-ups scale.

00:00:54.190 --> 00:00:56.720
We started it in 2012.

00:00:56.720 --> 00:00:58.435
And as we started,
in our first year,

00:00:58.435 --> 00:00:59.810
we had a couple
of companies that

00:00:59.810 --> 00:01:01.309
were calling
themselves AI companies

00:01:01.309 --> 00:01:03.530
before anyone was really
thinking about AI companies,

00:01:03.530 --> 00:01:08.330
because they were driven by PhD
students out of the Computer

00:01:08.330 --> 00:01:09.620
Science Department--

00:01:09.620 --> 00:01:11.009
mostly Hinton's PhD students.

00:01:11.009 --> 00:01:13.050
And in the second year,
there were a couple more.

00:01:13.050 --> 00:01:15.020
And in 2014, there
were a few more.

00:01:15.020 --> 00:01:16.640
And by 2015, there
was this flood

00:01:16.640 --> 00:01:20.204
of companies calling
themselves AI companies.

00:01:20.204 --> 00:01:21.620
And we realized
this was something

00:01:21.620 --> 00:01:23.870
we should get our heads around.

00:01:23.870 --> 00:01:28.430
Luckily at the same time Ajay
and I were on sabbatical.

00:01:28.430 --> 00:01:30.110
So we had some time,
we could think,

00:01:30.110 --> 00:01:32.360
and we were actually on
sabbatical just down the road.

00:01:32.360 --> 00:01:33.890
We were at Stanford.

00:01:33.890 --> 00:01:38.090
And we got to see both the
flood of companies coming

00:01:38.090 --> 00:01:41.240
into our lab in Toronto
and the excitement that

00:01:41.240 --> 00:01:45.620
was starting to percolate
here in the Bay Area.

00:01:45.620 --> 00:01:48.556
And we realized that
we had a bit of a lead,

00:01:48.556 --> 00:01:50.180
as we saw these
companies first, and it

00:01:50.180 --> 00:01:54.040
was time to really get our heads
around what this all meant.

00:01:54.040 --> 00:01:57.780
And so the insights
in the book and what

00:01:57.780 --> 00:02:00.670
we're going to talk about
for the next 40 minutes or so

00:02:00.670 --> 00:02:04.540
are explicitly
based on one, seeing

00:02:04.540 --> 00:02:07.450
all these now hundreds
of science-based AI

00:02:07.450 --> 00:02:10.286
start-ups coming
through the lab,

00:02:10.286 --> 00:02:15.700
and two, our experience of
being right here in essentially

00:02:15.700 --> 00:02:17.950
the center of it all
on the innovation

00:02:17.950 --> 00:02:21.370
side, the commercialization
side, to figure out, well,

00:02:21.370 --> 00:02:23.300
what's really going on.

00:02:23.300 --> 00:02:29.230
So in this crowd we need to have
no shyness about that there's

00:02:29.230 --> 00:02:31.870
a lot of hype around AI.

00:02:31.870 --> 00:02:33.940
There's some sense that
artificial intelligence

00:02:33.940 --> 00:02:35.200
is going to change everything.

00:02:35.200 --> 00:02:37.409
We see these headlines
almost every day.

00:02:37.409 --> 00:02:38.950
And we also see
these other headlines

00:02:38.950 --> 00:02:40.158
with a little bit of anxiety.

00:02:40.158 --> 00:02:44.121
Wait, if the machine is
intelligent, what about me?

00:02:44.121 --> 00:02:45.370
I thought that was what I did.

00:02:45.370 --> 00:02:46.286
That's what humans do.

00:02:46.286 --> 00:02:48.790
If the machines are
intelligent, what's left for us?

00:02:48.790 --> 00:02:50.860
And underlying this
is a lot of confusion

00:02:50.860 --> 00:02:54.830
really about what is
artificial intelligence.

00:02:54.830 --> 00:02:57.070
So if you read the press,
you have some sense

00:02:57.070 --> 00:03:00.220
that artificial intelligence
is either on the happy side,

00:03:00.220 --> 00:03:05.410
something like the C-3PO, which
is a robot who essentially does

00:03:05.410 --> 00:03:07.960
everything that a
human can do, but is

00:03:07.960 --> 00:03:09.970
friendly and nice and helpful.

00:03:09.970 --> 00:03:12.160
Or perhaps, we have Skynet
from "The Terminator,"

00:03:12.160 --> 00:03:15.150
where intelligent machines are
going to take over the world.

00:03:15.150 --> 00:03:18.080
Now, you may or may
not think that's crazy,

00:03:18.080 --> 00:03:20.200
but it's important to
recognize the reason we're

00:03:20.200 --> 00:03:23.500
talking about AI in 2018, and
we weren't talking about it

00:03:23.500 --> 00:03:27.920
in 2008 or 1998, is not
because of this technology.

00:03:27.920 --> 00:03:30.120
It's because of advances
in machine learning.

00:03:30.120 --> 00:03:33.550
And so when we think about what
these advances are, why are we

00:03:33.550 --> 00:03:35.620
talking about AI today?

00:03:35.620 --> 00:03:37.990
It's because of
prediction technology.

00:03:37.990 --> 00:03:41.980
And so we should think about
the recent advances in AI

00:03:41.980 --> 00:03:44.680
as advances in prediction--

00:03:44.680 --> 00:03:47.650
better, faster,
cheaper prediction.

00:03:47.650 --> 00:03:50.050
To try to get your heads
around why that might matter

00:03:50.050 --> 00:03:51.850
and why that might
be fundamental,

00:03:51.850 --> 00:03:55.600
it's useful to go back a
technology and remember 1995.

00:03:55.600 --> 00:03:57.730
So looking around
the room, there

00:03:57.730 --> 00:04:00.850
might be six of you
who remember 1995.

00:04:00.850 --> 00:04:03.220
I remember 1995.

00:04:03.220 --> 00:04:05.500
It was a really exciting
year in technology.

00:04:05.500 --> 00:04:09.250
So why was 1995 such an
exciting year in technology?

00:04:09.250 --> 00:04:15.910
Well, it was the last vestiges
of the public internet, NSFNET,

00:04:15.910 --> 00:04:17.709
were privatized.

00:04:17.709 --> 00:04:21.279
Netscape had their IPO where
they valued at billions

00:04:21.279 --> 00:04:22.570
of dollars with zero profit.

00:04:22.570 --> 00:04:25.270
At the time that
was really crazy.

00:04:25.270 --> 00:04:28.060
And Bill Gates wrote his
internet tidal wave email

00:04:28.060 --> 00:04:30.340
saying, this is the
technology that we

00:04:30.340 --> 00:04:32.710
need to focus our attention on.

00:04:32.710 --> 00:04:35.380
So Microsoft, perhaps
they missed it,

00:04:35.380 --> 00:04:36.970
perhaps it had been
the background.

00:04:36.970 --> 00:04:40.780
In 1995, he realized this
was the future of computing

00:04:40.780 --> 00:04:42.430
and the whole company
started to change

00:04:42.430 --> 00:04:44.950
their direction toward AI--

00:04:44.950 --> 00:04:47.260
toward the internet.

00:04:47.260 --> 00:04:51.126
And so everything
seemed to be changing.

00:04:51.126 --> 00:04:53.000
And people stopped
talking about the internet

00:04:53.000 --> 00:04:55.610
as an exciting new
technology and they started

00:04:55.610 --> 00:04:58.130
talking about as a new economy.

00:04:58.130 --> 00:05:00.920
That the old rules
didn't seem to apply

00:05:00.920 --> 00:05:03.950
and it was a whole new set of
rules that were going to apply.

00:05:03.950 --> 00:05:07.160
We didn't need our
economics textbooks.

00:05:07.160 --> 00:05:08.720
We had to write new ones.

00:05:08.720 --> 00:05:11.450
Now, there was one set
of people who said,

00:05:11.450 --> 00:05:13.740
it's not a new economy.

00:05:13.740 --> 00:05:16.160
It's the same old
economy, we just

00:05:16.160 --> 00:05:20.690
need to understand that
the costs of certain things

00:05:20.690 --> 00:05:21.680
have fallen.

00:05:21.680 --> 00:05:23.720
The costs of search have fallen.

00:05:23.720 --> 00:05:25.790
The costs of
reproduction have fallen.

00:05:25.790 --> 00:05:28.940
The costs of
communication have fallen.

00:05:28.940 --> 00:05:32.590
And once we understand
which costs have fallen,

00:05:32.590 --> 00:05:35.750
we can apply the
same old economics.

00:05:35.750 --> 00:05:38.914
And maybe the dominant academic
economic textbook writer

00:05:38.914 --> 00:05:40.580
at the time who's
sitting in this room--

00:05:40.580 --> 00:05:42.440
your chief economist
Hal Varian--

00:05:42.440 --> 00:05:46.760
was perhaps the leader in
really thinking that through.

00:05:46.760 --> 00:05:48.560
And he and Carl Shapiro
wrote this book,

00:05:48.560 --> 00:05:52.400
"Information Rules," which
laid out explicitly that idea.

00:05:52.400 --> 00:05:55.070
That the old
economics still apply.

00:05:55.070 --> 00:05:58.520
You just need to think through
what's changed, what's cheaper,

00:05:58.520 --> 00:06:01.940
and then we can draw on
decades, if not centuries,

00:06:01.940 --> 00:06:05.910
of economic ideas to
understand the consequences.

00:06:05.910 --> 00:06:09.380
Now, let's jump back another
technology generation to think

00:06:09.380 --> 00:06:11.250
this through a little bit more.

00:06:11.250 --> 00:06:13.080
So this is a semiconductor.

00:06:13.080 --> 00:06:19.052
This is the technology that's
underlying your computer.

00:06:19.052 --> 00:06:20.510
And when we talk
about Moore's Law,

00:06:20.510 --> 00:06:23.240
we think about,
well, it's doubling

00:06:23.240 --> 00:06:25.790
the number of transistors
in a semiconductor every so

00:06:25.790 --> 00:06:27.240
many months.

00:06:27.240 --> 00:06:29.090
How do we really
think that through?

00:06:29.090 --> 00:06:31.840
What does your
computer really do?

00:06:31.840 --> 00:06:34.960
Well, as an economist,
I think of it this way.

00:06:34.960 --> 00:06:37.540
We think of it as OK, it
drops the cost in something.

00:06:37.540 --> 00:06:39.702
It used to be
expensive-- something--

00:06:39.702 --> 00:06:42.160
and then semiconductors came
along and computers came along

00:06:42.160 --> 00:06:44.490
and that thing became cheap.

00:06:44.490 --> 00:06:47.940
And so what does your
computer really do?

00:06:47.940 --> 00:06:50.010
It actually only does one thing.

00:06:50.010 --> 00:06:51.810
Your computer does arithmetic.

00:06:51.810 --> 00:06:53.480
That's it.

00:06:53.480 --> 00:06:56.650
All your computer
does is arithmetic.

00:06:56.650 --> 00:06:59.380
But once arithmetic
is cheap enough,

00:06:59.380 --> 00:07:02.560
we find all sorts of
opportunities for arithmetic

00:07:02.560 --> 00:07:04.510
that we might not have
thought of before.

00:07:04.510 --> 00:07:05.710
This is economics 101.

00:07:05.710 --> 00:07:08.320
This is the idea that demand
curves slope downward.

00:07:08.320 --> 00:07:10.780
When something is
cheap, we do more of it

00:07:10.780 --> 00:07:12.760
and we buy more of it.

00:07:12.760 --> 00:07:16.690
But because arithmetic
became so much cheaper,

00:07:16.690 --> 00:07:19.510
there were all sorts of crazy
applications for arithmetic

00:07:19.510 --> 00:07:21.890
that most of us might
not have thought of.

00:07:21.890 --> 00:07:24.480
So the first applications
for machinery arithmetic,

00:07:24.480 --> 00:07:26.550
for machine computing,
were the same

00:07:26.550 --> 00:07:28.500
as the applications
for human computing.

00:07:28.500 --> 00:07:31.140
So we had artillery tables.

00:07:31.140 --> 00:07:33.051
So we had cannons,
they shot cannonballs.

00:07:33.051 --> 00:07:34.800
It's a pretty difficult
arithmetic problem

00:07:34.800 --> 00:07:37.500
to figure out where those
cannonballs are going to land.

00:07:37.500 --> 00:07:40.734
We used to have teams of
humans whose job was computers.

00:07:40.734 --> 00:07:42.900
And you might have seen the
movie, "Hidden Figures."

00:07:42.900 --> 00:07:44.100
That's what they were doing.

00:07:44.100 --> 00:07:46.560
These were humans
doing arithmetic

00:07:46.560 --> 00:07:49.230
to figure out classic
arithmetic problems that

00:07:49.230 --> 00:07:50.940
were of first-order
importance to space

00:07:50.940 --> 00:07:53.270
exploration and the military.

00:07:53.270 --> 00:07:56.360
Then a handful of other
human arithmetic problems

00:07:56.360 --> 00:07:59.330
started to be replaced
by machine arithmetic.

00:07:59.330 --> 00:08:03.710
Accounting-- accountants used
to spend their time adding.

00:08:03.710 --> 00:08:06.830
You look at what the accounting
curriculum in the 1940s

00:08:06.830 --> 00:08:10.510
and 1950s was, classic
homework problem

00:08:10.510 --> 00:08:13.510
was to open the white
pages of the phone-book

00:08:13.510 --> 00:08:17.966
and literally add up all the
phone numbers on the page.

00:08:17.966 --> 00:08:19.090
Why was that your homework?

00:08:19.090 --> 00:08:21.190
Because that's what you
would spend your time

00:08:21.190 --> 00:08:23.050
doing after graduation.

00:08:23.050 --> 00:08:26.020
Your life as an accountant
was spent adding.

00:08:26.020 --> 00:08:28.280
Accountants don't add anymore.

00:08:28.280 --> 00:08:30.460
This is just not what they do.

00:08:30.460 --> 00:08:31.930
On the positive
side, there's still

00:08:31.930 --> 00:08:34.570
lots of jobs for accountants
and there's lots of accountants,

00:08:34.570 --> 00:08:37.780
because it turned out the
people who are best positioned

00:08:37.780 --> 00:08:40.600
to do the arithmetic
were also best positioned

00:08:40.600 --> 00:08:42.880
to understand what to
do when the machine did

00:08:42.880 --> 00:08:45.040
the arithmetic for them.

00:08:45.040 --> 00:08:48.660
But as arithmetic became
cheaper and cheaper and cheaper,

00:08:48.660 --> 00:08:51.330
we found all sorts of new
applications for arithmetic

00:08:51.330 --> 00:08:53.130
that we might not have
thought of before.

00:08:53.130 --> 00:08:55.590
It turns out that when
arithmetic is cheap,

00:08:55.590 --> 00:08:58.280
games are an arithmetic problem.

00:08:58.280 --> 00:09:00.390
Mail is arithmetic.

00:09:00.390 --> 00:09:01.770
Music is arithmetic.

00:09:01.770 --> 00:09:03.780
Pictures are arithmetic.

00:09:03.780 --> 00:09:05.550
And once arithmetic
became cheap,

00:09:05.550 --> 00:09:08.172
we found all these new
applications for arithmetic

00:09:08.172 --> 00:09:09.880
that we might not have
thought of before.

00:09:09.880 --> 00:09:12.840
Your computer does arithmetic,
but because it does it

00:09:12.840 --> 00:09:17.850
so cheaply, we end up finding
arithmetic problems everywhere.

00:09:17.850 --> 00:09:21.709
And so that gets us to the
current excitement around AI.

00:09:21.709 --> 00:09:24.000
Here is one of the foundational
technologies behind it.

00:09:24.000 --> 00:09:27.547
A representation of a
convolutional neural net.

00:09:27.547 --> 00:09:28.880
What should we think about here?

00:09:28.880 --> 00:09:30.770
Well, same graph.

00:09:30.770 --> 00:09:32.954
It drops the cost of something.

00:09:32.954 --> 00:09:34.370
But in this context,
we think it's

00:09:34.370 --> 00:09:38.630
useful to think of it as a
drop in the cost of prediction.

00:09:38.630 --> 00:09:43.760
Prediction is using information
you have to fill in information

00:09:43.760 --> 00:09:44.870
you don't have.

00:09:44.870 --> 00:09:47.450
It could be about the
future, but it could also

00:09:47.450 --> 00:09:49.320
be about the
present or the past.

00:09:49.320 --> 00:09:53.450
It's the process of filling
in missing information.

00:09:53.450 --> 00:09:58.580
And what we've seen is that
as prediction gets cheaper,

00:09:58.580 --> 00:10:02.510
we're finding more and more
applications for prediction,

00:10:02.510 --> 00:10:04.500
just like with arithmetic.

00:10:04.500 --> 00:10:07.190
So the first applications
for machine prediction

00:10:07.190 --> 00:10:10.220
are exactly the same
as the applications

00:10:10.220 --> 00:10:13.400
that we were doing prediction
before we had these new tools.

00:10:13.400 --> 00:10:16.779
Loan defaults-- you walk into
a bank, you want to get a loan,

00:10:16.779 --> 00:10:18.320
the bank has to
decide whether you're

00:10:18.320 --> 00:10:19.790
going to pay them back or not.

00:10:19.790 --> 00:10:21.192
That's a prediction problem.

00:10:21.192 --> 00:10:23.150
And increasingly, we're
using machine learning,

00:10:23.150 --> 00:10:27.500
we're using AI tools,
to make that prediction.

00:10:27.500 --> 00:10:30.410
The insurance industry
loves these tools.

00:10:30.410 --> 00:10:33.860
The insurance industry
is based on prediction.

00:10:33.860 --> 00:10:35.690
Are you going to
make a claim or not,

00:10:35.690 --> 00:10:37.640
and how big is that
claim going to be?

00:10:37.640 --> 00:10:39.380
That's a prediction problem.

00:10:39.380 --> 00:10:43.970
And so over time, we've
seen increasing use

00:10:43.970 --> 00:10:49.180
of machine prediction
replacing other older tools.

00:10:49.180 --> 00:10:51.610
Now, as prediction's
gotten cheaper,

00:10:51.610 --> 00:10:53.650
we found a whole bunch
of new applications

00:10:53.650 --> 00:10:55.650
for prediction, new
ways of thinking

00:10:55.650 --> 00:10:58.150
about prediction that we might
not have thought of before.

00:10:58.150 --> 00:11:00.940
Medical diagnosis is
a prediction problem.

00:11:00.940 --> 00:11:02.920
What does your doctor do?

00:11:02.920 --> 00:11:05.800
They take information
about your symptoms

00:11:05.800 --> 00:11:08.320
and fill in the missing
information of the cause.

00:11:08.320 --> 00:11:10.090
That's prediction.

00:11:10.090 --> 00:11:11.830
If you asked a doctor
20 or 30 years ago

00:11:11.830 --> 00:11:13.288
if they were doing
prediction, they

00:11:13.288 --> 00:11:14.650
might not have realized it.

00:11:14.650 --> 00:11:17.080
But now it's pretty
clear that diagnosis

00:11:17.080 --> 00:11:19.500
is a prediction problem.

00:11:19.500 --> 00:11:22.050
Object classification
is a prediction problem.

00:11:22.050 --> 00:11:25.620
Your eyes take in
light signals and fill

00:11:25.620 --> 00:11:29.070
in the missing information
of what that object is

00:11:29.070 --> 00:11:31.320
in some context for it.

00:11:31.320 --> 00:11:33.967
Autonomous driving is
a prediction problem.

00:11:33.967 --> 00:11:35.550
There's the obvious
prediction problem

00:11:35.550 --> 00:11:39.380
of predicting what those
other crazy drivers are doing.

00:11:39.380 --> 00:11:44.540
But actually the key insight
in the recent advances

00:11:44.540 --> 00:11:47.692
in autonomous driving is
much more about, well,

00:11:47.692 --> 00:11:50.150
all we have to do is predict
what a good human driver would

00:11:50.150 --> 00:11:51.330
do.

00:11:51.330 --> 00:11:54.960
Once we can predict what a
good human driver would do,

00:11:54.960 --> 00:12:00.540
then we can create vehicles that
drive like good human drivers.

00:12:00.540 --> 00:12:04.650
So it's a reframing of
this prediction problem.

00:12:04.650 --> 00:12:08.130
And that's a key
element of the art

00:12:08.130 --> 00:12:11.204
of understanding and
identifying new opportunities

00:12:11.204 --> 00:12:12.120
from cheap prediction.

00:12:12.120 --> 00:12:15.000
How do we reframe old
problems, whether it's

00:12:15.000 --> 00:12:18.930
medical diagnosis, object
classification, or driving,

00:12:18.930 --> 00:12:22.170
as prediction problems, as
processes focused on filling

00:12:22.170 --> 00:12:23.464
in missing information?

00:12:26.370 --> 00:12:29.160
That's all well
and good and we've

00:12:29.160 --> 00:12:31.950
thought about, OK, well, we're
going to do more prediction.

00:12:31.950 --> 00:12:34.230
But other things change
in value as well.

00:12:34.230 --> 00:12:36.240
That's where the
anxiety comes from.

00:12:36.240 --> 00:12:37.860
The anxiety is, well,
if the machine's

00:12:37.860 --> 00:12:41.400
doing the prediction, what's
the human going to do?

00:12:41.400 --> 00:12:44.940
And so this is the
other econ 101 concept

00:12:44.940 --> 00:12:48.510
that still applies in the
context of cheap prediction.

00:12:48.510 --> 00:12:52.517
So when the price of coffee
falls, we buy more coffee.

00:12:52.517 --> 00:12:53.850
That we've talked about already.

00:12:53.850 --> 00:12:55.560
Demand curves slope down.

00:12:55.560 --> 00:12:58.800
The second thing to note is
when the price of coffee falls,

00:12:58.800 --> 00:13:01.230
we buy less tea.

00:13:01.230 --> 00:13:03.030
So when coffee is
cheap, we're going

00:13:03.030 --> 00:13:04.680
to buy coffee instead of tea.

00:13:04.680 --> 00:13:06.900
When machine
prediction is cheap,

00:13:06.900 --> 00:13:10.440
we're going to have machines do
the prediction and not humans.

00:13:10.440 --> 00:13:12.000
But the important
thing to remember

00:13:12.000 --> 00:13:15.730
is there are complements
to prediction.

00:13:15.730 --> 00:13:19.380
So just like when coffee
becomes cheap, we buy more cream

00:13:19.380 --> 00:13:21.070
and we buy more sugar.

00:13:21.070 --> 00:13:24.210
So when coffee is cheap, cream
and sugar becomes valuable.

00:13:24.210 --> 00:13:28.080
The key question that you
need to ask both yourself

00:13:28.080 --> 00:13:31.440
and your organization is,
what are the core complements

00:13:31.440 --> 00:13:32.460
to prediction?

00:13:32.460 --> 00:13:34.590
What are the cream
and sugar that become

00:13:34.590 --> 00:13:37.680
more valuable as
prediction becomes cheap?

00:13:37.680 --> 00:13:39.270
And the way to
think that through

00:13:39.270 --> 00:13:42.390
is to recognize that
prediction is valuable

00:13:42.390 --> 00:13:45.340
because it's an input
into decision-making.

00:13:45.340 --> 00:13:47.200
That's why prediction is useful.

00:13:47.200 --> 00:13:49.080
That's why this is a
transformative drop

00:13:49.080 --> 00:13:51.810
in the cost, as opposed
to an incremental one.

00:13:51.810 --> 00:13:54.270
And decision-making
is everywhere.

00:13:54.270 --> 00:13:56.340
You make big decisions.

00:13:56.340 --> 00:13:58.710
You make decisions on
what job should I take?

00:13:58.710 --> 00:13:59.880
Who should I marry?

00:13:59.880 --> 00:14:02.160
Should I marry?

00:14:02.160 --> 00:14:03.212
When should I retire?

00:14:03.212 --> 00:14:04.420
And you make small decisions.

00:14:04.420 --> 00:14:05.461
Should I write that down?

00:14:05.461 --> 00:14:06.990
Should I scratch my face?

00:14:06.990 --> 00:14:08.910
Should I watch that bit again?

00:14:08.910 --> 00:14:12.555
These decisions are everywhere
and because prediction

00:14:12.555 --> 00:14:15.320
is an input into
decision-making,

00:14:15.320 --> 00:14:18.250
prediction ends up
being foundation.

00:14:18.250 --> 00:14:21.520
The important thing to
remember, though, as well

00:14:21.520 --> 00:14:23.510
is that prediction is
not decision-making,

00:14:23.510 --> 00:14:25.450
it's a component
of decision-making.

00:14:25.450 --> 00:14:28.570
And we're trying to identify
the cream and sugar, the things

00:14:28.570 --> 00:14:31.720
that become more valuable
as prediction becomes cheap,

00:14:31.720 --> 00:14:35.900
we need to think through the
other elements of a decision.

00:14:35.900 --> 00:14:39.030
And so in thinking
through decision-making,

00:14:39.030 --> 00:14:43.730
what we found it useful to
do is put some structure

00:14:43.730 --> 00:14:46.250
around the components
of a decision.

00:14:46.250 --> 00:14:48.590
And broadly speaking-- we
put prediction at the center,

00:14:48.590 --> 00:14:50.690
because that's what's changed--

00:14:50.690 --> 00:14:52.640
but all sorts of
other things are

00:14:52.640 --> 00:14:55.860
inputs into a final
decision and an outcome.

00:14:55.860 --> 00:14:58.540
So your data is key.

00:14:58.540 --> 00:15:01.340
That's not news to
most people here.

00:15:01.340 --> 00:15:06.650
Data is increasingly valuable
because prediction is cheap

00:15:06.650 --> 00:15:10.050
and it's an input
into prediction.

00:15:10.050 --> 00:15:14.010
Actions are in many
ways more valuable,

00:15:14.010 --> 00:15:16.260
because there's no point
in making a decision if you

00:15:16.260 --> 00:15:18.220
can't do anything about it.

00:15:18.220 --> 00:15:20.010
And so being able
to do something

00:15:20.010 --> 00:15:24.630
with your cheap prediction
is increasingly important.

00:15:24.630 --> 00:15:27.570
Then the one of these that
I want to talk about today

00:15:27.570 --> 00:15:29.220
is judgment.

00:15:29.220 --> 00:15:30.960
And judgment,
broadly speaking, is

00:15:30.960 --> 00:15:32.940
knowing which
predictions to make

00:15:32.940 --> 00:15:35.850
and what to do with those
predictions once you have them.

00:15:35.850 --> 00:15:39.330
You can't make a decision
unless you know what

00:15:39.330 --> 00:15:41.940
to do with your predictions.

00:15:41.940 --> 00:15:44.880
And I don't know if you guys
have seen the movie "I, Robot."

00:15:44.880 --> 00:15:47.170
Some people have,
some people haven't.

00:15:47.170 --> 00:15:49.110
But in this movie,
there's one scene

00:15:49.110 --> 00:15:52.620
that makes it very clear
what this distinction

00:15:52.620 --> 00:15:55.470
between prediction
and judgment is.

00:15:55.470 --> 00:15:58.436
So Will Smith is the
star of the movie

00:15:58.436 --> 00:16:00.810
and he has a flashback scene
where he's in a car accident

00:16:00.810 --> 00:16:03.240
with a 12-year-old girl.

00:16:03.240 --> 00:16:05.580
And they're drowning
and then a robot

00:16:05.580 --> 00:16:10.150
arrives, somehow miraculously,
and can save one of them.

00:16:10.150 --> 00:16:12.900
And the robot apparently
makes this calculation

00:16:12.900 --> 00:16:16.320
that Will Smith has a
45% chance of survival

00:16:16.320 --> 00:16:18.900
and the girl only
had an 11% chance.

00:16:18.900 --> 00:16:21.910
And therefore, the
robot saves Will Smith.

00:16:21.910 --> 00:16:24.750
And Will Smith
concludes that the robot

00:16:24.750 --> 00:16:25.950
made the wrong decision.

00:16:25.950 --> 00:16:28.080
11% was more than enough.

00:16:28.080 --> 00:16:30.301
A human being would
have known that.

00:16:30.301 --> 00:16:31.800
So that's all well
and good and he's

00:16:31.800 --> 00:16:34.260
assuming that the
robot values his life

00:16:34.260 --> 00:16:36.440
and the girl's life the same.

00:16:36.440 --> 00:16:39.000
But in order for the
robot to make a decision,

00:16:39.000 --> 00:16:41.570
it needs the
prediction on survival

00:16:41.570 --> 00:16:44.460
and a statement about
how much more valuable

00:16:44.460 --> 00:16:47.720
the girl's life has to
be than Will Smith's life

00:16:47.720 --> 00:16:49.710
in order to choose.

00:16:49.710 --> 00:16:52.640
So this decision that
we've seen, all it says

00:16:52.640 --> 00:16:57.410
is Will Smith's life is worth
at least a quarter of the girl's

00:16:57.410 --> 00:16:58.680
life.

00:16:58.680 --> 00:17:02.700
That valuation decision
matters, because at some point

00:17:02.700 --> 00:17:05.339
even Will Smith would
disagree with this.

00:17:05.339 --> 00:17:09.660
At some point, if her chance
of survival was 1%, or 0.1%,

00:17:09.660 --> 00:17:14.000
or 0.01%, that
decision would flip.

00:17:14.000 --> 00:17:15.440
That's judgment.

00:17:15.440 --> 00:17:18.020
That's knowing what to
do with the prediction

00:17:18.020 --> 00:17:20.510
once you have one.

00:17:20.510 --> 00:17:23.460
And so judgment is the
process of determining

00:17:23.460 --> 00:17:25.950
what the reward is to
a particular action

00:17:25.950 --> 00:17:28.580
in a particular environment.

00:17:28.580 --> 00:17:30.500
And to understand
the consequences

00:17:30.500 --> 00:17:33.815
of cheap prediction and its
importance in decision-making,

00:17:33.815 --> 00:17:37.580
I'm going to turn it over
to Ajay to talk about tools.

00:17:37.580 --> 00:17:38.570
AJAY AGRAWAL: Great.

00:17:38.570 --> 00:17:39.070
OK.

00:17:39.070 --> 00:17:41.020
Thanks, Avi.

00:17:41.020 --> 00:17:44.594
And thanks Hal and your
colleagues for inviting us

00:17:44.594 --> 00:17:45.760
here to talk about our book.

00:17:48.790 --> 00:17:54.050
So this triangle pyramid is
the representation of the five

00:17:54.050 --> 00:17:54.986
sections of the book.

00:17:54.986 --> 00:17:56.360
And we start off
with prediction,

00:17:56.360 --> 00:17:59.210
which goes over the bits
that Avi just covered.

00:17:59.210 --> 00:18:01.760
And in essence, there
are parts that you'll

00:18:01.760 --> 00:18:05.690
be very familiar with, in terms
of just the technical parts

00:18:05.690 --> 00:18:06.800
of prediction.

00:18:06.800 --> 00:18:09.256
Also, why machine
prediction is--

00:18:09.256 --> 00:18:10.880
in what ways is it
similar or different

00:18:10.880 --> 00:18:13.970
than our traditional
prediction tools.

00:18:13.970 --> 00:18:15.400
And the economics
of predictions.

00:18:15.400 --> 00:18:19.080
And so the essence of
obvious point that there

00:18:19.080 --> 00:18:21.110
are three key insights.

00:18:21.110 --> 00:18:23.570
Insight number one is that
when prediction becomes cheap

00:18:23.570 --> 00:18:24.880
we use more of it.

00:18:24.880 --> 00:18:27.470
Insight two is when
prediction becomes cheap

00:18:27.470 --> 00:18:30.472
that it lowers the
value of the substitute

00:18:30.472 --> 00:18:32.180
to machine prediction,
which is it lowers

00:18:32.180 --> 00:18:33.950
the value of human prediction.

00:18:33.950 --> 00:18:37.250
And implication number three,
that as machine prediction

00:18:37.250 --> 00:18:40.940
becomes cheap, it increases
the value of complements

00:18:40.940 --> 00:18:44.400
to prediction, like input data.

00:18:44.400 --> 00:18:46.940
So if data is the new oil,
why is it the new oil?

00:18:46.940 --> 00:18:47.709
It's new.

00:18:47.709 --> 00:18:48.500
We always had data.

00:18:48.500 --> 00:18:52.190
But it's now oil when before
it wasn't so much oil,

00:18:52.190 --> 00:18:53.780
because predictions
become cheaper.

00:18:53.780 --> 00:18:57.380
So that data we had before is
more valuable as a complement.

00:18:57.380 --> 00:19:00.509
And our human judgment
becomes more valuable

00:19:00.509 --> 00:19:02.300
as prediction becomes
cheaper, because it's

00:19:02.300 --> 00:19:05.330
a complement to prediction
and decision-making.

00:19:05.330 --> 00:19:07.100
And actions become
more valuable,

00:19:07.100 --> 00:19:10.430
because we can apply our actions
to higher fidelity predictions.

00:19:10.430 --> 00:19:12.170
So that's section one.

00:19:12.170 --> 00:19:14.540
Section two is on
decision-making,

00:19:14.540 --> 00:19:16.580
which are how these
components come together.

00:19:16.580 --> 00:19:18.038
So effectively,
what we're doing is

00:19:18.038 --> 00:19:20.900
we're taking on the one hand,
these recent advances in AI

00:19:20.900 --> 00:19:22.700
are a new technology
for prediction,

00:19:22.700 --> 00:19:25.280
but we're applying them to
50 years of decision theory.

00:19:25.280 --> 00:19:27.140
So we've got a
well-established theory

00:19:27.140 --> 00:19:31.940
and we're just dropping this
super power prediction tool

00:19:31.940 --> 00:19:34.970
inside a well-established
theory to understand

00:19:34.970 --> 00:19:36.922
what are the implications
for decision-making.

00:19:36.922 --> 00:19:39.110
The section three
that's on tools

00:19:39.110 --> 00:19:44.830
is perhaps the most
practical bit of the book.

00:19:44.830 --> 00:19:50.720
I'm just going to describe a
couple of the highlight bits.

00:19:50.720 --> 00:19:53.390
When we are building
these tools--

00:19:53.390 --> 00:19:58.550
effectively, every AI that
we build right now is a tool.

00:19:58.550 --> 00:20:00.560
It performs a particular
prediction task.

00:20:00.560 --> 00:20:03.150
And the way we think of
this is we divide up--

00:20:03.150 --> 00:20:05.355
within an organization
like this,

00:20:05.355 --> 00:20:06.730
there'll be a
bunch of workflows.

00:20:06.730 --> 00:20:09.260
Workflow is anything that
turns an input into an output.

00:20:09.260 --> 00:20:12.630
So it can be a product
would be a workflow.

00:20:12.630 --> 00:20:14.740
We can divide the
workflows into tasks

00:20:14.740 --> 00:20:17.480
and each task is predicated
on one or a few decisions.

00:20:17.480 --> 00:20:19.630
And AIs work at the task level.

00:20:19.630 --> 00:20:23.420
So AIs don't do
workflows, they do tasks.

00:20:23.420 --> 00:20:25.442
And so just to give an
example-- a lot of you

00:20:25.442 --> 00:20:26.900
will probably have
seen this-- this

00:20:26.900 --> 00:20:29.750
was an interview with
the CFO of Goldman Sachs.

00:20:29.750 --> 00:20:33.890
It starts off with this very
dramatic opening sentence

00:20:33.890 --> 00:20:36.980
that at its height back in 2000,
the US Cash Equities Trading

00:20:36.980 --> 00:20:39.080
Desk at Goldman
employed 600 traders,

00:20:39.080 --> 00:20:41.030
and now there are just two left.

00:20:41.030 --> 00:20:43.640
But then it goes on to, I
think the more important bit--

00:20:43.640 --> 00:20:48.560
it talks about the AIs
moving to more automation,

00:20:48.560 --> 00:20:51.170
moving to more complex
problems, like trading

00:20:51.170 --> 00:20:52.340
currencies and credit.

00:20:52.340 --> 00:20:54.140
They emulate as closely
as possible what

00:20:54.140 --> 00:20:55.280
a human trader would do.

00:20:55.280 --> 00:20:57.260
You can change, emulate,
as closely as possible

00:20:57.260 --> 00:20:58.220
to predict--

00:20:58.220 --> 00:21:00.860
effectively, they predict
what a human trader would do.

00:21:00.860 --> 00:21:02.990
But then most interestingly,
down further on,

00:21:02.990 --> 00:21:07.580
they break up the IPO
process into tasks.

00:21:07.580 --> 00:21:10.880
So Goldman has already
mapped 146 distinct steps

00:21:10.880 --> 00:21:12.200
taken in any IPO.

00:21:12.200 --> 00:21:14.664
So what we do when we're
working on these problems,

00:21:14.664 --> 00:21:16.580
we take the workflow,
we divide it into tasks.

00:21:16.580 --> 00:21:19.460
In this case, the Goldman
Sachs IPO process workflow

00:21:19.460 --> 00:21:21.570
can be broken into 146 tasks.

00:21:21.570 --> 00:21:24.140
And then we effectively
just estimate

00:21:24.140 --> 00:21:28.730
what's the ROI for building
an AI to do that task.

00:21:28.730 --> 00:21:31.670
And then we rank
order the tasks.

00:21:31.670 --> 00:21:33.500
Putting the ones with
the highest ROI--

00:21:33.500 --> 00:21:35.870
the Return On Investment--
at the top of the list.

00:21:35.870 --> 00:21:37.121
And then we work our way down.

00:21:37.121 --> 00:21:39.578
And so in terms of just when
organizations show up and say,

00:21:39.578 --> 00:21:40.670
where do I even start?

00:21:40.670 --> 00:21:45.590
This is just a course
description of how we start.

00:21:45.590 --> 00:21:49.880
And obviously, there are
many AI projects here.

00:21:49.880 --> 00:21:50.570
And this is old.

00:21:50.570 --> 00:21:52.611
I suspect it's at least
double or probably triple

00:21:52.611 --> 00:21:53.530
by this stage--

00:21:53.530 --> 00:21:56.470
the number of AI tools here.

00:21:56.470 --> 00:21:59.030
We have large
companies that show up

00:21:59.030 --> 00:22:01.790
to our Creative Destruction
Lab in Toronto and they'll say,

00:22:01.790 --> 00:22:06.500
hey, we've got three AI pilot
projects in our company,

00:22:06.500 --> 00:22:09.680
or four or five, are we
at the frontier of AI?

00:22:09.680 --> 00:22:12.410
And once we start breaking
these things down--

00:22:12.410 --> 00:22:14.660
workflows into tasks and
figuring out where we can get

00:22:14.660 --> 00:22:17.330
a lift from building
a prediction machine--

00:22:17.330 --> 00:22:22.370
we see that there are often
hundreds if not thousands

00:22:22.370 --> 00:22:23.540
of opportunities to do that.

00:22:23.540 --> 00:22:28.350
And of course, this organization
is at the frontier of that.

00:22:28.350 --> 00:22:33.050
So one of the tools that
we have found very helpful

00:22:33.050 --> 00:22:35.660
for companies that
are just starting

00:22:35.660 --> 00:22:38.270
to wade into applications
of AI is this thing

00:22:38.270 --> 00:22:40.520
that we call the AI Canvas.

00:22:40.520 --> 00:22:42.650
But effectively, it is just
taking those components

00:22:42.650 --> 00:22:50.550
that Avi described and writing
down the elements in English.

00:22:50.550 --> 00:22:54.000
So first of all, what is the
key prediction of this task?

00:22:54.000 --> 00:22:59.360
And so you'd be surprised,
people that do a task every day

00:22:59.360 --> 00:23:00.920
struggle at first
trying to identify

00:23:00.920 --> 00:23:03.360
what is the prediction
that underlies this task.

00:23:03.360 --> 00:23:05.630
In other words, what
we do is we look

00:23:05.630 --> 00:23:09.120
for elements of uncertainty.

00:23:09.120 --> 00:23:10.620
And prediction
doesn't add any value

00:23:10.620 --> 00:23:12.040
when there's no uncertainty.

00:23:12.040 --> 00:23:14.310
So with our first
clue of where we

00:23:14.310 --> 00:23:15.720
go to look for,
where we're going

00:23:15.720 --> 00:23:17.430
to get some action
for deploying an AI,

00:23:17.430 --> 00:23:20.610
is where are we operating in
conditions of uncertainty?

00:23:20.610 --> 00:23:23.040
And then what is
the key prediction?

00:23:23.040 --> 00:23:25.500
And then once an AI
delivers that prediction,

00:23:25.500 --> 00:23:28.980
what's the human judgment that's
applied to the prediction?

00:23:28.980 --> 00:23:30.660
And what's the
action that we take

00:23:30.660 --> 00:23:33.300
as a function of having the
prediction and the judgment

00:23:33.300 --> 00:23:35.370
in the type of outcome?

00:23:35.370 --> 00:23:37.200
And then three types of data.

00:23:37.200 --> 00:23:39.480
The data we use to train
the AI, the data which

00:23:39.480 --> 00:23:41.490
we use to run the
AI, and the data we

00:23:41.490 --> 00:23:45.730
use to enhance the
AI as it operates.

00:23:45.730 --> 00:23:50.301
But the key point here is that
there are senior level people,

00:23:50.301 --> 00:23:52.050
whether it's a bank,
or an insurance firm,

00:23:52.050 --> 00:23:53.910
or manufacturing
firm, drug discovery,

00:23:53.910 --> 00:23:56.250
who have never written
a line of code,

00:23:56.250 --> 00:24:00.230
can sit down and start
filling these things out.

00:24:00.230 --> 00:24:02.910
And within a day, a
senior management team

00:24:02.910 --> 00:24:04.560
can have a dozen or
a couple of dozen

00:24:04.560 --> 00:24:07.830
of these and all of a sudden
feel a level of comfort around,

00:24:07.830 --> 00:24:10.270
OK, I get the basic idea.

00:24:10.270 --> 00:24:12.270
Of course, they
can't build in AI,

00:24:12.270 --> 00:24:14.400
but they now have
got a framework

00:24:14.400 --> 00:24:16.890
that they can hand to
people who can build it.

00:24:16.890 --> 00:24:20.250
And their thinking
largely center around

00:24:20.250 --> 00:24:23.340
what is the core
prediction of this task?

00:24:23.340 --> 00:24:27.540
So we found this thing to be
just a useful way to get people

00:24:27.540 --> 00:24:30.210
started.

00:24:30.210 --> 00:24:37.980
So most of the AI tools that we
build are like any other tools.

00:24:37.980 --> 00:24:41.790
We build them and we use them
in the service of executing

00:24:41.790 --> 00:24:42.990
against a given strategy.

00:24:42.990 --> 00:24:45.060
So the organization has
some kind of strategy

00:24:45.060 --> 00:24:47.640
and whether it's a
tool, just like a word

00:24:47.640 --> 00:24:50.250
processor or a spreadsheet,
we build an AI tool that

00:24:50.250 --> 00:24:51.930
just makes us more productive.

00:24:51.930 --> 00:24:56.040
So tools are generally there
to enhance productivity,

00:24:56.040 --> 00:25:01.500
to enable us to better execute
against the given strategy.

00:25:01.500 --> 00:25:04.620
But occasionally,
these AI tools so

00:25:04.620 --> 00:25:07.800
fundamentally change
the underlying economics

00:25:07.800 --> 00:25:11.640
of the business, that they
change the strategy itself.

00:25:11.640 --> 00:25:15.720
And so I want to just spend a
couple of minutes on AI tools

00:25:15.720 --> 00:25:17.970
that impact strategy.

00:25:17.970 --> 00:25:21.030
And when they do that,
a common vernacular

00:25:21.030 --> 00:25:24.330
for this type of phenomenon
is what some people

00:25:24.330 --> 00:25:27.730
refer to as disruption.

00:25:27.730 --> 00:25:29.290
So again, this is
not an audience

00:25:29.290 --> 00:25:33.640
where this is any surprise,
but if we were giving this talk

00:25:33.640 --> 00:25:36.190
two or three years ago, this
would have been largely talked

00:25:36.190 --> 00:25:37.240
about if.

00:25:37.240 --> 00:25:40.864
It would have been if we
can achieve this in AI,

00:25:40.864 --> 00:25:42.280
then wouldn't this
be interesting.

00:25:42.280 --> 00:25:44.800
Or this would be possible
if we could achieve that.

00:25:44.800 --> 00:25:47.590
Now over the last 24,
36 months, the number

00:25:47.590 --> 00:25:50.410
of proof of concepts that we've
had, whether it's in vision,

00:25:50.410 --> 00:25:54.700
or it's in natural language,
or it's in motion control,

00:25:54.700 --> 00:25:56.950
I don't think any more
most of these are ifs.

00:25:56.950 --> 00:25:59.620
We know now that
they are plausible

00:25:59.620 --> 00:26:01.690
and so now it's just
turning the crank

00:26:01.690 --> 00:26:04.780
and moving the predictions
up to commercial grade.

00:26:04.780 --> 00:26:08.380
So this is now all a
conversation about when,

00:26:08.380 --> 00:26:10.030
not if.

00:26:10.030 --> 00:26:12.760
So here's a thought experiment
that we use for a strategy.

00:26:12.760 --> 00:26:17.260
The basic idea is that we
call it science fictioning.

00:26:17.260 --> 00:26:20.500
And the thing here,
though, is that it's

00:26:20.500 --> 00:26:22.580
very constrained
science fictioning,

00:26:22.580 --> 00:26:26.110
which is that the science
fictioning is predicated

00:26:26.110 --> 00:26:28.990
on a single parameter
that can move.

00:26:28.990 --> 00:26:32.380
And so the thought experiment
is imagine a radio knob,

00:26:32.380 --> 00:26:35.200
but instead of turning up
the volume when you turn up

00:26:35.200 --> 00:26:38.972
the knob, you are turning up the
prediction accuracy of your AI.

00:26:38.972 --> 00:26:41.500
So that's the only thing that
you're allowed to manipulate.

00:26:41.500 --> 00:26:44.770
And so you do that
and then you just

00:26:44.770 --> 00:26:47.360
think through what are the
consequences of doing that.

00:26:47.360 --> 00:26:50.980
So a useful thought experiment
is to take this idea

00:26:50.980 --> 00:26:53.620
and apply to an AI that
everyone's familiar with, which

00:26:53.620 --> 00:26:55.242
is the recommendation engine.

00:26:55.242 --> 00:26:56.950
So for example, the
recommendation engine

00:26:56.950 --> 00:26:58.650
on Amazon.

00:26:58.650 --> 00:27:05.170
And so what's interesting about
this is that it's a useful way

00:27:05.170 --> 00:27:07.240
to think about how
this could have

00:27:07.240 --> 00:27:10.704
an effect that is non-linear.

00:27:10.704 --> 00:27:11.620
So we go on to Amazon.

00:27:11.620 --> 00:27:13.940
Everybody here knows,
has shopped on Amazon,

00:27:13.940 --> 00:27:16.840
and has a feeling for how this
recommendation engine works.

00:27:16.840 --> 00:27:19.330
You're shopping around, it
recommends you some stuff.

00:27:19.330 --> 00:27:22.330
And for Avi, Joshua,
and I, it is on average

00:27:22.330 --> 00:27:25.090
about 5% accurate,
meaning out of every 20

00:27:25.090 --> 00:27:29.080
things it recommends to
us, we buy one of them.

00:27:29.080 --> 00:27:31.180
And given the fact
that it's pulling

00:27:31.180 --> 00:27:34.000
from a catalog of millions
of possibilities, the fact

00:27:34.000 --> 00:27:38.227
that it serves us up 20 and
we choose one, is not too bad.

00:27:38.227 --> 00:27:40.810
And so the process, of course,
is that we go on their website,

00:27:40.810 --> 00:27:42.310
we browse around, we
see things we like,

00:27:42.310 --> 00:27:43.980
we put them in our
basket, we pay for them.

00:27:43.980 --> 00:27:45.940
An order shows up at an
Amazon fulfillment center,

00:27:45.940 --> 00:27:47.710
and some human gets
that on their tablet,

00:27:47.710 --> 00:27:50.500
and the Kiva robots dance
around the fulfillment center.

00:27:50.500 --> 00:27:52.459
They bring up the
stuff to the human.

00:27:52.459 --> 00:27:54.250
Human picks them out,
puts them in the box,

00:27:54.250 --> 00:27:56.291
put the label in the box,
ships it to your house.

00:27:56.291 --> 00:27:58.359
It arrives in the
back of a truck.

00:27:58.359 --> 00:27:59.900
And then someone
knocks on your door.

00:27:59.900 --> 00:28:00.520
They ring the doorbell.

00:28:00.520 --> 00:28:01.360
They put the thing
in your porch.

00:28:01.360 --> 00:28:01.970
You open the door.

00:28:01.970 --> 00:28:02.890
You bring in the box.

00:28:02.890 --> 00:28:03.640
You open the box.

00:28:03.640 --> 00:28:05.431
And then you've got
your thing from Amazon.

00:28:05.431 --> 00:28:09.176
We can generalize that by saying
that this is a business model

00:28:09.176 --> 00:28:10.300
of shopping, then shipping.

00:28:10.300 --> 00:28:14.170
So we shop for the stuff
and then Amazon ships it.

00:28:14.170 --> 00:28:17.530
And so the thought experiment is
now imagine the recommendation

00:28:17.530 --> 00:28:21.100
engine and everyday the people
in machine learning team

00:28:21.100 --> 00:28:23.980
at Amazon are working
away at turning that knob.

00:28:23.980 --> 00:28:26.230
And so maybe now
it's at 2 out of 10.

00:28:26.230 --> 00:28:29.230
And they enhance the algorithms,
they collect more data.

00:28:29.230 --> 00:28:30.280
3 out of 10.

00:28:30.280 --> 00:28:32.290
They acquire data
set, like Whole Foods.

00:28:32.290 --> 00:28:35.050
They learn more about our
purchasing behavior offline

00:28:35.050 --> 00:28:37.570
and they get up to a 4
out of 10 or 5 out of 10.

00:28:37.570 --> 00:28:40.870
And there is some
number, and it doesn't

00:28:40.870 --> 00:28:44.230
have to be a spinal tap
level of prediction accuracy.

00:28:44.230 --> 00:28:46.310
But there's some-- maybe
it's a 6 out of 10,

00:28:46.310 --> 00:28:47.726
maybe it's a 7 or
10-- but there's

00:28:47.726 --> 00:28:51.490
some number where when they
get to that level of prediction

00:28:51.490 --> 00:28:54.340
accuracy, somebody
at Amazon says,

00:28:54.340 --> 00:28:57.700
we're good enough at predicting
what they want, why are we

00:28:57.700 --> 00:28:59.620
waiting for them to order it?

00:28:59.620 --> 00:29:02.050
Let's just ship it.

00:29:02.050 --> 00:29:04.030
And so why would they
do that even when

00:29:04.030 --> 00:29:08.350
they know that they're not at
a 10 out of 10 or 11 out of 10?

00:29:08.350 --> 00:29:11.920
Because let's say that they
ship us a box of 10 things.

00:29:11.920 --> 00:29:14.080
And we open the door,
we open the box,

00:29:14.080 --> 00:29:15.550
and we like six of the things.

00:29:15.550 --> 00:29:20.770
We keep them and we put four
of them back in the box.

00:29:20.770 --> 00:29:23.650
In the absence of them having
preemptively shipped it to us,

00:29:23.650 --> 00:29:26.350
we might have only ordered two
of those things from Amazon.

00:29:26.350 --> 00:29:28.990
And now we are
taking six of them

00:29:28.990 --> 00:29:30.880
and preempting four
things that we might have

00:29:30.880 --> 00:29:32.680
bought from their competitors.

00:29:32.680 --> 00:29:36.790
And the benefit of selling
us that extra stuff

00:29:36.790 --> 00:29:40.570
may outweigh the cost of
dealing with the extra returns.

00:29:40.570 --> 00:29:42.070
But now that they
have those returns

00:29:42.070 --> 00:29:43.810
to lower the cost of
dealing with the returns,

00:29:43.810 --> 00:29:45.370
maybe they invest
in a fleet of trucks

00:29:45.370 --> 00:29:46.953
that drive down our
street once a week

00:29:46.953 --> 00:29:49.630
and pick up all the things
from you and your neighbors

00:29:49.630 --> 00:29:53.140
that they dropped off that it
turned out you didn't want.

00:29:53.140 --> 00:29:54.990
So why is this interesting?

00:29:54.990 --> 00:29:58.080
It's interesting because as you
think about that recommendation

00:29:58.080 --> 00:30:00.420
engine-- we've all seen
it, we've been using it--

00:30:00.420 --> 00:30:02.169
and it's been getting
a little bit better,

00:30:02.169 --> 00:30:04.840
a little bit better, a
little bit better over time.

00:30:04.840 --> 00:30:06.420
But it's not dramatic.

00:30:06.420 --> 00:30:08.250
It doesn't change a strategy.

00:30:08.250 --> 00:30:11.232
It's just a slightly better
recommendation engine.

00:30:11.232 --> 00:30:13.440
But the thought experiment
here is the non-linearity.

00:30:13.440 --> 00:30:15.210
In other words, it gets
better, better, better.

00:30:15.210 --> 00:30:17.251
And we just feel it getting
incrementally better.

00:30:17.251 --> 00:30:19.660
And some of us don't even
feel it getting better.

00:30:19.660 --> 00:30:24.970
But when it crosses a line
that doesn't mean perfect,

00:30:24.970 --> 00:30:27.170
there's a potentially
step function change

00:30:27.170 --> 00:30:29.300
in the effect on the business.

00:30:29.300 --> 00:30:32.690
And all of a sudden,
they start shipping--

00:30:32.690 --> 00:30:35.480
so they change the model
from shopping then shipping,

00:30:35.480 --> 00:30:36.740
to shipping then shopping.

00:30:36.740 --> 00:30:40.810
They ship to us and then
we shop on our doorstep.

00:30:40.810 --> 00:30:45.040
And so we find that to be a
very useful thought experiment.

00:30:45.040 --> 00:30:47.529
We go literally AI by AI by AI.

00:30:47.529 --> 00:30:49.070
We go through each
AI and we say, OK,

00:30:49.070 --> 00:30:51.610
what happens is
we turn the knob?

00:30:51.610 --> 00:30:55.330
And is this just a tool
that incrementally enhances

00:30:55.330 --> 00:30:58.630
some part of the process
or is this something

00:30:58.630 --> 00:31:01.240
that when the knob gets
far enough along that it

00:31:01.240 --> 00:31:03.730
will have a transformational
effect on the business?

00:31:03.730 --> 00:31:08.684
And in the case of Amazon, who
knows whether they'll do it,

00:31:08.684 --> 00:31:10.600
but it's not like they've
never thought of it.

00:31:10.600 --> 00:31:13.150
They have this and a couple
of patents on what they

00:31:13.150 --> 00:31:15.010
call anticipatory shipping.

00:31:15.010 --> 00:31:20.960
And they're piloting versions of
this already in narrow markets.

00:31:20.960 --> 00:31:23.580
But whether or not they
do it is not the point.

00:31:23.580 --> 00:31:28.360
It is that you can imagine
how this type of thinking

00:31:28.360 --> 00:31:33.660
about the process can have
non-linear effects on strategy.

00:31:33.660 --> 00:31:37.070
So from our perspective, when
we see at this organization

00:31:37.070 --> 00:31:39.540
an announcement that you're
moving from mobile first

00:31:39.540 --> 00:31:43.209
to AI first as a strategy,
from an economics lens

00:31:43.209 --> 00:31:45.000
our question is, well,
what does that mean?

00:31:45.000 --> 00:31:47.250
In other words, to what
extent is this just pixie dust

00:31:47.250 --> 00:31:50.074
that everyone in the Bay
talks about being AI first,

00:31:50.074 --> 00:31:51.740
because if you sprinkle
AI on something,

00:31:51.740 --> 00:31:54.510
its valuation doubles.

00:31:54.510 --> 00:32:00.000
And so our thesis is there's
no, there's really an underlying

00:32:00.000 --> 00:32:02.190
strategy here, and what is it?

00:32:02.190 --> 00:32:05.490
So an outsider's
perception of what does it

00:32:05.490 --> 00:32:08.940
mean at Google to have
an AI first strategy--

00:32:08.940 --> 00:32:12.630
what it means to us
is that at Google you

00:32:12.630 --> 00:32:16.710
have put the knob at the
very top of your strategy

00:32:16.710 --> 00:32:18.250
priorities.

00:32:18.250 --> 00:32:20.850
And so in other words, from
an outsider's perspective,

00:32:20.850 --> 00:32:23.970
when someone says AI first--

00:32:23.970 --> 00:32:27.904
so first of all, the strategy
here before was mobile first.

00:32:27.904 --> 00:32:29.070
So what does that even mean?

00:32:29.070 --> 00:32:30.529
What does mobile first mean?

00:32:30.529 --> 00:32:32.070
So from an economist's
point of view,

00:32:32.070 --> 00:32:33.480
mobile first means
not just that you're

00:32:33.480 --> 00:32:35.730
going to be good at mobile,
because no company will put up

00:32:35.730 --> 00:32:38.070
their hand and say, well, we
want to be mediocre at mobile.

00:32:38.070 --> 00:32:39.720
Everybody wants to
be good at mobile.

00:32:39.720 --> 00:32:41.200
But what mobile
first means to us

00:32:41.200 --> 00:32:45.900
is that the company is
prioritizing performance

00:32:45.900 --> 00:32:48.560
on mobile, even at the
expense of other things.

00:32:48.560 --> 00:32:50.400
So that when there's a
trade-off to be made,

00:32:50.400 --> 00:32:54.900
the trade will be made in favor
of performing well in mobile.

00:32:54.900 --> 00:32:56.700
So what does it
mean to be AI first?

00:32:56.700 --> 00:32:59.770
When Google announced
AI first, Peter Norvig

00:32:59.770 --> 00:33:02.460
answered this on Quora.

00:33:02.460 --> 00:33:06.362
And so his description
was effectively,

00:33:06.362 --> 00:33:08.570
"With information retrieval,
anything over 80% recall

00:33:08.570 --> 00:33:09.540
and precision is pretty good.

00:33:09.540 --> 00:33:11.164
Not every suggestion
has to be perfect,

00:33:11.164 --> 00:33:13.290
since the user can ignore
the bad suggestions.

00:33:13.290 --> 00:33:15.397
With assistance, there
is much higher barrier.

00:33:15.397 --> 00:33:16.980
You wouldn't use a
service that booked

00:33:16.980 --> 00:33:20.490
the wrong reservation 20% of the
time, or even 2% of the time.

00:33:20.490 --> 00:33:22.830
So an assistant needs to be
much more accurate, and thus

00:33:22.830 --> 00:33:24.746
more intelligent, more
aware of the situation.

00:33:24.746 --> 00:33:26.910
That's what we call AI first."

00:33:26.910 --> 00:33:30.600
And what we would add
on to his definition

00:33:30.600 --> 00:33:34.950
is even if it means at the
expense of other things.

00:33:34.950 --> 00:33:37.980
So even if it means at
the expense of user's

00:33:37.980 --> 00:33:41.310
experience in the
short-term, or revenues

00:33:41.310 --> 00:33:42.960
in the short-term,
or potentially

00:33:42.960 --> 00:33:45.120
privacy in the short-term.

00:33:45.120 --> 00:33:47.760
In other words, things that
help us crank the dial.

00:33:47.760 --> 00:33:50.700
What helps us crank
the dial, because we

00:33:50.700 --> 00:33:53.227
can transform our
capabilities in terms

00:33:53.227 --> 00:33:55.560
of what we can do if we get
our prediction accuracy high

00:33:55.560 --> 00:33:56.200
enough.

00:33:56.200 --> 00:33:58.770
So that's why we're
making prediction accuracy

00:33:58.770 --> 00:34:01.050
such a priority.

00:34:01.050 --> 00:34:04.410
Other forms of trade-offs--
so when other CEOs heard

00:34:04.410 --> 00:34:06.870
your announcement, and then we
started getting calls, well,

00:34:06.870 --> 00:34:08.489
what does it mean that
Google is going AI first

00:34:08.489 --> 00:34:09.830
and should we be AI first, too?

00:34:13.330 --> 00:34:16.290
This is another allocation
of scarce resources.

00:34:16.290 --> 00:34:19.480
Putting AI as a priority.

00:34:19.480 --> 00:34:23.190
And so when we saw this
about moving the Google Brain

00:34:23.190 --> 00:34:27.920
team into the office
right next to the CEO--

00:34:27.920 --> 00:34:30.579
a year ago the Google Brain
team of mathematicians, coders,

00:34:30.579 --> 00:34:32.620
hardware engineers sat in
a small office building

00:34:32.620 --> 00:34:34.380
at the other side of
the company's campus.

00:34:34.380 --> 00:34:36.338
Over the past few months,
it switched buildings

00:34:36.338 --> 00:34:37.980
and now works right
beside the area

00:34:37.980 --> 00:34:40.424
where the CEO and other
top executives work.

00:34:40.424 --> 00:34:42.090
When this story came
out in the "Times,"

00:34:42.090 --> 00:34:44.460
the part that we
felt was missing

00:34:44.460 --> 00:34:47.219
was they never
covered who got moved.

00:34:47.219 --> 00:34:48.817
Who became second?

00:34:48.817 --> 00:34:50.400
In other words, when
AI becomes first,

00:34:50.400 --> 00:34:52.500
something has to become second.

00:34:52.500 --> 00:34:56.100
And that's what makes
something a strategy.

00:34:56.100 --> 00:34:59.010
Is that it means an allocation
of scarce resources.

00:34:59.010 --> 00:35:01.250
In this case, it
with scarce resources

00:35:01.250 --> 00:35:03.990
of space next to the CEO.

00:35:03.990 --> 00:35:08.070
And so this just
as a strategy when

00:35:08.070 --> 00:35:11.490
people put turning the knob at
the top of their priority list,

00:35:11.490 --> 00:35:14.040
it means that they're
doing that potentially

00:35:14.040 --> 00:35:16.740
at the expense of other things.

00:35:16.740 --> 00:35:19.840
So I'll just conclude
with this point here.

00:35:19.840 --> 00:35:23.590
What we've been feeling is
some level of dissonance.

00:35:23.590 --> 00:35:25.900
Which is on the one hand,
people coming in and saying,

00:35:25.900 --> 00:35:26.733
hey, look, I get it.

00:35:26.733 --> 00:35:28.680
I see the AIs, like the
recommendation engines

00:35:28.680 --> 00:35:32.280
of Amazon and other sites, and
things like Siri, and so on.

00:35:32.280 --> 00:35:34.320
All these different AI tools.

00:35:34.320 --> 00:35:36.060
And they're neat.

00:35:36.060 --> 00:35:38.050
And they're impressive.

00:35:38.050 --> 00:35:39.930
But they're not
transformational.

00:35:39.930 --> 00:35:43.507
They're not
transforming industries.

00:35:43.507 --> 00:35:45.090
And so on the one
hand, they see this.

00:35:45.090 --> 00:35:46.500
Things that are neat,
but not transformational.

00:35:46.500 --> 00:35:47.916
On the other hand,
this is a graph

00:35:47.916 --> 00:35:50.220
of venture capital
going into AI.

00:35:50.220 --> 00:35:53.320
Then there is the
various countries,

00:35:53.320 --> 00:35:57.900
whether it's France, or
England, or US having policies

00:35:57.900 --> 00:36:01.230
making significant bets on AI.

00:36:01.230 --> 00:36:03.720
Google and then a series of
other companies announcing they

00:36:03.720 --> 00:36:05.400
were going to be AI first.

00:36:05.400 --> 00:36:07.470
Then governments
like China having

00:36:07.470 --> 00:36:09.720
a very aggressive
strategy on AI,

00:36:09.720 --> 00:36:13.350
with a fair amount of capital
to support that and goals

00:36:13.350 --> 00:36:17.370
of being the leader in AI
in some fields in 2020,

00:36:17.370 --> 00:36:20.340
and more fields by 2025, and
dominant across all fields

00:36:20.340 --> 00:36:21.160
by 2030.

00:36:21.160 --> 00:36:23.840
And now potentially
accelerating that.

00:36:23.840 --> 00:36:29.940
The president of Russia making
remarks like the leader of AI

00:36:29.940 --> 00:36:32.517
will be the country
that rules the world.

00:36:32.517 --> 00:36:34.100
And then a conference
that Hal was at,

00:36:34.100 --> 00:36:36.490
that we hosted in Toronto,
where a number of people

00:36:36.490 --> 00:36:39.790
spoke including
Danny Kahneman, who's

00:36:39.790 --> 00:36:43.300
the author of the popular book,
"Thinking, Fast and Slow,"

00:36:43.300 --> 00:36:45.460
that many of you may have read.

00:36:45.460 --> 00:36:47.530
He made the following remarks.

00:36:47.530 --> 00:36:51.430
So in other words, we expected
him, partly because of his age

00:36:51.430 --> 00:36:53.170
and partly because
of the fact that he's

00:36:53.170 --> 00:36:56.590
been thinking about human
thinking for so long, that he

00:36:56.590 --> 00:36:59.860
would be a defender
of all the things

00:36:59.860 --> 00:37:02.920
that make us human and
distinct from machines.

00:37:02.920 --> 00:37:05.972
So we expected him to be
the conservative wise view

00:37:05.972 --> 00:37:07.180
at the end of the conference.

00:37:07.180 --> 00:37:10.000
And instead, he closed our
conference with the following.

00:37:10.000 --> 00:37:11.680
He said, "I want
to end on a story.

00:37:11.680 --> 00:37:13.930
A well-known novelist
wrote me some time ago

00:37:13.930 --> 00:37:15.064
that he's planning a novel.

00:37:15.064 --> 00:37:16.480
The novel is about
a love triangle

00:37:16.480 --> 00:37:19.120
between two humans and a robot,
and what he wanted to know

00:37:19.120 --> 00:37:21.930
is how would the robot be
different from the people.

00:37:21.930 --> 00:37:23.676
I proposed three
main differences.

00:37:23.676 --> 00:37:24.550
The first is obvious.

00:37:24.550 --> 00:37:27.940
The robot will be better
at statistical reasoning.

00:37:27.940 --> 00:37:30.190
The second is that the
robot would have much higher

00:37:30.190 --> 00:37:31.850
emotional intelligence."

00:37:31.850 --> 00:37:34.270
And so here he had earlier
made reference to the fact

00:37:34.270 --> 00:37:37.450
that robots are able
to envision systems,

00:37:37.450 --> 00:37:40.180
are able to detect
changes in emotion

00:37:40.180 --> 00:37:42.700
from happy to sad, or
to jealous or to angry,

00:37:42.700 --> 00:37:45.430
with a much higher
accuracy level than humans.

00:37:45.430 --> 00:37:47.670
And not just a visual
signal, also audio signal.

00:37:47.670 --> 00:37:49.420
So with very short
amount of audio signal,

00:37:49.420 --> 00:37:51.310
able to detect
changes in emotion

00:37:51.310 --> 00:37:53.230
much faster than humans.

00:37:53.230 --> 00:37:55.990
"The third is that the
robot would be wiser.

00:37:55.990 --> 00:37:57.010
Wisdom is breadth.

00:37:57.010 --> 00:37:58.970
Wisdom is not having
too narrow a view.

00:37:58.970 --> 00:38:00.220
That is the essence of wisdom.

00:38:00.220 --> 00:38:01.480
It's broad framing.

00:38:01.480 --> 00:38:03.730
A robot will be endowed
with broad framing.

00:38:03.730 --> 00:38:06.460
When it has learned enough, it
will be wiser than we people

00:38:06.460 --> 00:38:08.560
because we do not
have a broad frame.

00:38:08.560 --> 00:38:10.480
We are narrow thinkers,
we are noisy thinkers,

00:38:10.480 --> 00:38:12.617
And it is very easy
to improve upon us.

00:38:12.617 --> 00:38:14.200
I do not think that
there is very much

00:38:14.200 --> 00:38:16.480
that we can do that
computers will not eventually

00:38:16.480 --> 00:38:18.460
learn to do."

00:38:18.460 --> 00:38:21.440
So on the one hand,
we have people saying,

00:38:21.440 --> 00:38:24.384
well, wait a minute,
these AI tools are neat

00:38:24.384 --> 00:38:26.800
and they are impressive, but
they're not transformational.

00:38:26.800 --> 00:38:30.310
On the other hand, we
have so many people

00:38:30.310 --> 00:38:32.980
of power and
influence say, you're

00:38:32.980 --> 00:38:34.990
making very big bets on AI.

00:38:34.990 --> 00:38:38.840
How do we reconcile
these two things?

00:38:38.840 --> 00:38:40.540
On the one hand,
nothing transformative.

00:38:40.540 --> 00:38:45.010
On the other hand, such big
investments and speculation.

00:38:45.010 --> 00:38:48.580
And, of course, in our view,
the way to reconcile this

00:38:48.580 --> 00:38:51.290
is having a thesis on time.

00:38:51.290 --> 00:38:53.980
Which is if you think
the knob will turn,

00:38:53.980 --> 00:38:56.310
and you think that knob will
take 10 years or 20 years

00:38:56.310 --> 00:38:59.539
to turn, then you'll make
a set of investments today

00:38:59.539 --> 00:39:02.080
that are very different than if
you think that knob will turn

00:39:02.080 --> 00:39:06.749
in three years, or two years,
or something much shorter term.

00:39:06.749 --> 00:39:09.040
And, of course, that knob
will turn at different rates,

00:39:09.040 --> 00:39:11.206
in different applications,
and with different access

00:39:11.206 --> 00:39:12.530
to different types of data.

00:39:12.530 --> 00:39:15.740
But in our view, from
a strategy perspective,

00:39:15.740 --> 00:39:17.890
one of the most
important starting points

00:39:17.890 --> 00:39:20.160
is having a thesis on time.

00:39:20.160 --> 00:39:22.670
So two people in
the same industry

00:39:22.670 --> 00:39:25.510
may make very different
bets based on their thesis

00:39:25.510 --> 00:39:28.450
on how fast the dial will turn.

00:39:28.450 --> 00:39:33.040
And so this was another
reasonably recent article

00:39:33.040 --> 00:39:36.110
in the "Times,"
and they're quoting

00:39:36.110 --> 00:39:38.480
Robert Work, former Deputy
Secretary of Defense.

00:39:38.480 --> 00:39:42.520
And in this article
he refers to--

00:39:42.520 --> 00:39:46.600
talking about US versus
China-- and he refers to this--

00:39:46.600 --> 00:39:48.970
he says, this is
a Sputnik moment.

00:39:48.970 --> 00:39:52.600
And this really speaks to the
point about people's thesis

00:39:52.600 --> 00:39:53.830
on time.

00:39:53.830 --> 00:39:59.900
I don't think there's any
company in this country

00:39:59.900 --> 00:40:05.680
and maybe in the
world that has treated

00:40:05.680 --> 00:40:11.110
this technology with such a
level of urgency as Google has.

00:40:11.110 --> 00:40:13.690
In other words, you were
early out of the gates.

00:40:13.690 --> 00:40:15.930
As Avi was saying in the
beginning of the talk,

00:40:15.930 --> 00:40:18.670
some of the
foundational innovations

00:40:18.670 --> 00:40:20.350
in this field of
machine learning

00:40:20.350 --> 00:40:23.200
came out of our backyard in
the University of Toronto.

00:40:23.200 --> 00:40:27.220
But this is the organization
that capitalized on it first.

00:40:27.220 --> 00:40:32.170
And so I think and we think
that there are organizations now

00:40:32.170 --> 00:40:34.180
across industries
who are just starting

00:40:34.180 --> 00:40:37.360
to come to the realization
that you came to three or four

00:40:37.360 --> 00:40:41.560
years ago, and starting to
make bets in this domain.

00:40:41.560 --> 00:40:44.680
And they're realizing that
this is their Sputnik moment.

00:40:44.680 --> 00:40:47.110
That in other words,
these don't come around.

00:40:47.110 --> 00:40:49.780
If you are a manager or
a leader in some part

00:40:49.780 --> 00:40:51.910
of your organization,
these don't

00:40:51.910 --> 00:40:54.310
come around once a
quarter, once a year,

00:40:54.310 --> 00:40:56.270
even once every few years.

00:40:56.270 --> 00:40:58.270
This is the type of thing
that comes around once

00:40:58.270 --> 00:40:59.650
in a generation.

00:40:59.650 --> 00:41:03.100
And so from an individual's
point perspective,

00:41:03.100 --> 00:41:05.749
people are betting their
careers on-- some fraction

00:41:05.749 --> 00:41:07.540
of people are betting
their careers on what

00:41:07.540 --> 00:41:08.530
this is going to do.

00:41:08.530 --> 00:41:11.170
And same with some companies.

00:41:11.170 --> 00:41:13.476
And one of the reasons
this is a privilege for Avi

00:41:13.476 --> 00:41:16.360
and I to come and talk
about our book here

00:41:16.360 --> 00:41:20.260
is that some are also
doing it-- in our view,

00:41:20.260 --> 00:41:24.910
they're so far ahead, that
they are making decisions that

00:41:24.910 --> 00:41:27.640
have a humanity level impact.

00:41:27.640 --> 00:41:29.669
And there is no company
more so than this one.

00:41:29.669 --> 00:41:31.210
So it's a pleasure
for us to be here.

00:41:31.210 --> 00:41:31.709
That's it.

00:41:31.709 --> 00:41:32.351
Thanks.

00:41:32.351 --> 00:41:35.237
[APPLAUSE]

00:41:39.580 --> 00:41:41.790
AUDIENCE: So a very
interesting talk.

00:41:41.790 --> 00:41:43.570
It occurred to me
that one thing that

00:41:43.570 --> 00:41:46.270
might be missing from the
picture that you described

00:41:46.270 --> 00:41:51.280
is that there's a back
reaction from society that

00:41:51.280 --> 00:41:54.340
occurs when you deploy
some of these AI machine

00:41:54.340 --> 00:41:55.692
learning technologies.

00:41:55.692 --> 00:41:57.400
And I'm thinking in
particular, one thing

00:41:57.400 --> 00:42:01.780
you mentioned that if Google,
for instance, is putting AI--

00:42:01.780 --> 00:42:03.445
turning up this knob
on AI at the top

00:42:03.445 --> 00:42:05.520
of their priority list--

00:42:05.520 --> 00:42:08.080
does that mean that
they are putting things

00:42:08.080 --> 00:42:09.534
like data privacy second?

00:42:09.534 --> 00:42:11.200
And I don't think
anyone at this company

00:42:11.200 --> 00:42:15.400
would agree that we would
sacrifice user data privacy

00:42:15.400 --> 00:42:18.089
at the expense of promoting AI.

00:42:18.089 --> 00:42:19.630
And another example
is, for instance,

00:42:19.630 --> 00:42:23.140
with the recent Facebook
developments with respect

00:42:23.140 --> 00:42:24.490
to Cambridge Analytica.

00:42:24.490 --> 00:42:28.720
I mean, they've turned up
their AI knob so that users

00:42:28.720 --> 00:42:31.600
are spending as much time as
possible on that platform,

00:42:31.600 --> 00:42:34.980
even if it's in a
kind of echo chamber.

00:42:34.980 --> 00:42:37.780
And what they've seen is that
there's a back reaction because

00:42:37.780 --> 00:42:39.920
of its possible
political effects

00:42:39.920 --> 00:42:44.320
on the outcomes of
elections, that users

00:42:44.320 --> 00:42:46.390
aren't happy with that.

00:42:46.390 --> 00:42:49.000
And in that case,
they might have

00:42:49.000 --> 00:42:51.820
to turn that knob
back down, or at least

00:42:51.820 --> 00:42:54.140
point it in a completely
different direction from where

00:42:54.140 --> 00:42:54.910
they were going.

00:42:54.910 --> 00:42:57.204
So I wondered if you had
any comment on that angle?

00:42:57.204 --> 00:42:59.370
AVI GOLDFARB: So privacy
is tricky for a whole bunch

00:42:59.370 --> 00:43:00.100
of reasons.

00:43:00.100 --> 00:43:03.400
And the way to think
about privacy strategy

00:43:03.400 --> 00:43:05.710
is, as I think we like
to think about it,

00:43:05.710 --> 00:43:08.290
is it's also a trade-off.

00:43:08.290 --> 00:43:12.190
But in the sense that
you have to have both

00:43:12.190 --> 00:43:16.211
as a nation and a company
enough freedom to use user data

00:43:16.211 --> 00:43:17.710
so that you can do
something with it

00:43:17.710 --> 00:43:21.190
and train your AIs, but
enough restriction so that

00:43:21.190 --> 00:43:22.990
your customers trust you.

00:43:22.990 --> 00:43:28.870
And that latter point is of
first order of importance.

00:43:28.870 --> 00:43:33.460
So if any company is
seen to be abusing

00:43:33.460 --> 00:43:36.130
their users and
their users' privacy,

00:43:36.130 --> 00:43:39.850
that is almost surely
going to be a bad strategy

00:43:39.850 --> 00:43:42.100
and not going to improve--

00:43:42.100 --> 00:43:44.030
and they're not
going get any data

00:43:44.030 --> 00:43:46.870
and so that's actually going
to backfire on their AI point.

00:43:46.870 --> 00:43:52.840
So I think to reinforce your
point, it's exactly as you say,

00:43:52.840 --> 00:43:55.200
which you have to be
respectful enough.

00:43:55.200 --> 00:43:57.160
You have to be respectful
of user privacy

00:43:57.160 --> 00:43:59.350
and you have to
respect your users,

00:43:59.350 --> 00:44:01.900
or else they won't let you
be AI first, because you

00:44:01.900 --> 00:44:03.420
won't get the data to do it.

00:44:03.420 --> 00:44:06.565
AUDIENCE: I think I wanted
to push back on the--

00:44:06.565 --> 00:44:09.970
it seemed almost like the
Amazon recommendation algorithm

00:44:09.970 --> 00:44:12.460
was being discounted
a little bit there,

00:44:12.460 --> 00:44:15.520
because I've actually discovered
some great books through that.

00:44:15.520 --> 00:44:19.330
As well as on YouTube, the
recommendation for videos based

00:44:19.330 --> 00:44:22.090
on oftentimes it's talks
that I'm watching on YouTube,

00:44:22.090 --> 00:44:23.830
it will recommend
someone who I've never

00:44:23.830 --> 00:44:28.630
heard of who I actually later
become really interested in

00:44:28.630 --> 00:44:29.950
and learn a lot from.

00:44:29.950 --> 00:44:31.720
So I'm really interested
in this question

00:44:31.720 --> 00:44:33.760
of how those
algorithms can get even

00:44:33.760 --> 00:44:38.750
better at showing people things
that they didn't know existed.

00:44:38.750 --> 00:44:41.470
And sometimes, I think with
both Amazon and YouTube,

00:44:41.470 --> 00:44:43.090
I'm not sure exactly
how it works,

00:44:43.090 --> 00:44:45.542
but I'm sure it's optimizing
for some sort of proxy,

00:44:45.542 --> 00:44:47.500
like whether they're
buying, or whether they're

00:44:47.500 --> 00:44:49.060
clicking, or watching.

00:44:49.060 --> 00:44:51.790
But I think with things
like books and videos where

00:44:51.790 --> 00:44:55.570
they're complex products,
there's an opportunity to get

00:44:55.570 --> 00:44:59.020
feedback from the user, like
a qualitative feedback--

00:44:59.020 --> 00:45:01.960
like what I liked about this
book or these types of things--

00:45:01.960 --> 00:45:06.620
and have that be a function to
better inform the algorithm,

00:45:06.620 --> 00:45:08.090
rather than just some proxy.

00:45:08.090 --> 00:45:10.960
My question, I guess, is do
you know of any work being

00:45:10.960 --> 00:45:12.645
done like that?

00:45:12.645 --> 00:45:14.020
I guess it would
probably be more

00:45:14.020 --> 00:45:17.050
in the domain of a university
or something than maybe

00:45:17.050 --> 00:45:18.080
in the private sector.

00:45:18.080 --> 00:45:22.330
But I guess the idea of using a
qualitative feedback mechanism

00:45:22.330 --> 00:45:23.870
to better inform the AI.

00:45:23.870 --> 00:45:25.870
AVI GOLDFARB: So there
were two questions there.

00:45:25.870 --> 00:45:28.690
I'm going to actually answer
your first question, which

00:45:28.690 --> 00:45:34.750
is what do we mean by the
current recommendation system

00:45:34.750 --> 00:45:35.829
not being transformative.

00:45:35.829 --> 00:45:37.370
I think that's
underlining the point.

00:45:37.370 --> 00:45:40.480
What we mean by that is,
Amazon's business model

00:45:40.480 --> 00:45:42.670
is the same business
model in many ways

00:45:42.670 --> 00:45:45.130
as the Sears catalog
was 100 years ago.

00:45:45.130 --> 00:45:47.350
And so how did the
Sears catalog work?

00:45:47.350 --> 00:45:49.090
Well, you got a
catalog in the mail,

00:45:49.090 --> 00:45:50.800
and you looked through
it, and you told

00:45:50.800 --> 00:45:52.480
Sears what you wanted to buy.

00:45:52.480 --> 00:45:55.030
And then they sent it
that recommendation

00:45:55.030 --> 00:45:56.842
to the warehouse--

00:45:56.842 --> 00:45:59.300
that request to their warehouse
and they shipped it to you.

00:45:59.300 --> 00:46:03.135
And as Sears improved the
development of their catalogs,

00:46:03.135 --> 00:46:04.510
they started to
figure out things

00:46:04.510 --> 00:46:06.580
like different
kinds of customers

00:46:06.580 --> 00:46:08.440
want different things
in their catalog.

00:46:08.440 --> 00:46:11.230
And so in some sense, their
recommendations got better.

00:46:11.230 --> 00:46:14.140
And Amazon's recommendations
are, don't get me wrong,

00:46:14.140 --> 00:46:16.420
way better than the
Sears catalogs were.

00:46:16.420 --> 00:46:21.040
But at the end of the day,
it's the same business model

00:46:21.040 --> 00:46:22.390
just done better.

00:46:22.390 --> 00:46:23.890
A lot better, but done better.

00:46:23.890 --> 00:46:25.816
Where it becomes
transformational

00:46:25.816 --> 00:46:27.190
is when those
recommendations get

00:46:27.190 --> 00:46:30.156
so good that they no longer
have to have that business model

00:46:30.156 --> 00:46:32.030
and they can have a
different business model.

00:46:32.030 --> 00:46:34.910
That's what we meant by that.

00:46:34.910 --> 00:46:37.980
AUDIENCE: Probably a prediction
that machines cannot do yet,

00:46:37.980 --> 00:46:40.640
I'd like to ask humans.

00:46:40.640 --> 00:46:44.030
From where you see
the world, do you

00:46:44.030 --> 00:46:49.070
have a short list of areas where
this transformative threshold

00:46:49.070 --> 00:46:52.480
will be crossed across,
like beyond Silicon Valley?

00:46:55.600 --> 00:47:02.530
AJAY AGRAWAL: So my view is
that just as a thematic change,

00:47:02.530 --> 00:47:05.320
many more things
will be personalized.

00:47:05.320 --> 00:47:10.600
So in other words, we do so much
delivery of goods and services

00:47:10.600 --> 00:47:12.222
based on averages.

00:47:12.222 --> 00:47:13.930
So the one that
everybody's familiar with

00:47:13.930 --> 00:47:15.080
is medical services.

00:47:15.080 --> 00:47:19.030
In other words, if given
your age and some very basic

00:47:19.030 --> 00:47:21.422
characteristics about
you, you get treated--

00:47:21.422 --> 00:47:23.380
you and I would probably
get the same treatment

00:47:23.380 --> 00:47:26.410
for if we had some
kind of ailment,

00:47:26.410 --> 00:47:29.200
because we're both males
of roughly the same age.

00:47:29.200 --> 00:47:32.500
And so, so many things--

00:47:32.500 --> 00:47:37.360
in other words, the fidelity
of the predictions in our view

00:47:37.360 --> 00:47:41.092
will lead to
personalization of--

00:47:41.092 --> 00:47:42.550
when we talk about
shopping, that's

00:47:42.550 --> 00:47:44.620
just another form
of personalization--

00:47:44.620 --> 00:47:46.540
of personalization
of so many things

00:47:46.540 --> 00:47:50.085
that as a thematic change we'll
move from mass to personalized.

00:47:50.085 --> 00:47:52.460
And which people have been
talking about for a long time,

00:47:52.460 --> 00:47:54.490
is personalization will
just become a thing.

00:47:54.490 --> 00:48:00.260
But now we're starting to
actually see it in action.

00:48:00.260 --> 00:48:02.640
AUDIENCE: So I had a question
about the judgment aspect

00:48:02.640 --> 00:48:04.750
in the model that you
guys were mentioning.

00:48:04.750 --> 00:48:06.500
And it's a two-part question.

00:48:06.500 --> 00:48:08.770
So the first one was,
with the improvements

00:48:08.770 --> 00:48:12.450
in the lowering
cost of prediction,

00:48:12.450 --> 00:48:15.190
will judgments become
more polarized?

00:48:15.190 --> 00:48:20.440
And what I mean by that
is, as the model turns up

00:48:20.440 --> 00:48:23.230
and AI becomes
smarter and providing

00:48:23.230 --> 00:48:28.180
more accurate judgment, I think
human judgment will be pushed

00:48:28.180 --> 00:48:32.020
into a corner of yes or no.

00:48:32.020 --> 00:48:34.060
Because, for example,
the AI might come up

00:48:34.060 --> 00:48:37.180
with a prediction,
like oh, 96% 97%

00:48:37.180 --> 00:48:40.150
says you should choose
option A over option B.

00:48:40.150 --> 00:48:42.910
And so the human
judgment aspect for that

00:48:42.910 --> 00:48:48.280
is, well, if it's only I have a
3% gap, I'm going to go with A.

00:48:48.280 --> 00:48:49.970
So that was the other thing.

00:48:49.970 --> 00:48:52.060
And the second
thing is, if someone

00:48:52.060 --> 00:48:58.030
were to reject the highly
favored option out of the list

00:48:58.030 --> 00:49:00.640
and go with option B,
wouldn't their judgment

00:49:00.640 --> 00:49:05.080
be more scrutinized and maybe
even held more responsible

00:49:05.080 --> 00:49:10.452
since they deferred
human judgment over AI?

00:49:10.452 --> 00:49:12.910
AVI GOLDFARB: So we think about
judgment that can be before

00:49:12.910 --> 00:49:14.243
or after you get the prediction.

00:49:14.243 --> 00:49:16.569
But here's where it
gets really complicated,

00:49:16.569 --> 00:49:17.860
within the legal system or not.

00:49:17.860 --> 00:49:22.060
Which is that you actually have
to say explicitly how much you

00:49:22.060 --> 00:49:23.810
value different things.

00:49:23.810 --> 00:49:28.330
So in a car accident
context, the machine--

00:49:28.330 --> 00:49:32.260
you have to pre-specify
what you think

00:49:32.260 --> 00:49:36.250
a life is worth relative
to other types of damage

00:49:36.250 --> 00:49:37.750
and other lives.

00:49:37.750 --> 00:49:41.380
And that opens up-- in
the health care context,

00:49:41.380 --> 00:49:44.299
we have all sorts
of similar things.

00:49:44.299 --> 00:49:46.090
Once you have a good
prediction on survival

00:49:46.090 --> 00:49:48.160
under different
treatments, for example,

00:49:48.160 --> 00:49:54.760
you need to explicitly say,
this is the threshold where it's

00:49:54.760 --> 00:49:57.790
worth it to save this person.

00:49:57.790 --> 00:50:01.150
And so that becomes just
a first order issue.

00:50:01.150 --> 00:50:05.500
Because you've specified
it, it can be audited,

00:50:05.500 --> 00:50:08.345
and that can be a legal
challenge and a liability

00:50:08.345 --> 00:50:10.345
challenge, and something
that you just need to--

00:50:14.290 --> 00:50:17.830
you cannot use the prediction
without actually just

00:50:17.830 --> 00:50:22.300
explicitly saying what you
value and that opens up risk.

00:50:22.300 --> 00:50:24.970
AJAY AGRAWAL: Let me just
add to that one, which is--

00:50:24.970 --> 00:50:29.062
so first of all, you mentioned
AI judgment and human judgment.

00:50:29.062 --> 00:50:31.020
In our view of the world,
AIs have no judgment.

00:50:31.020 --> 00:50:32.830
AIs never have judgment.

00:50:32.830 --> 00:50:35.170
All they do is prediction.

00:50:35.170 --> 00:50:36.670
Humans do judgment.

00:50:36.670 --> 00:50:38.110
AIs don't.

00:50:38.110 --> 00:50:41.094
Now, that doesn't mean that
sometimes AIs can't look

00:50:41.094 --> 00:50:42.260
like they're doing judgment.

00:50:42.260 --> 00:50:44.590
Because if they get enough
examples of our judgment,

00:50:44.590 --> 00:50:47.080
they can learn to
predict the judgment.

00:50:47.080 --> 00:50:48.790
But they don't have judgment.

00:50:48.790 --> 00:50:51.080
They are simply
making predictions.

00:50:51.080 --> 00:50:53.170
So that's the first bit.

00:50:53.170 --> 00:50:55.990
So then what you said-- and
this is a thing that I think is

00:50:55.990 --> 00:51:03.030
really a first order issue for
us to get our heads around--

00:51:03.030 --> 00:51:06.480
is you said, well, as they're
doing more and more of this,

00:51:06.480 --> 00:51:07.050
is this--

00:51:07.050 --> 00:51:09.341
I think you used the words
like push us into a corner--

00:51:09.341 --> 00:51:11.641
where the AIs are
doing 98% of the work,

00:51:11.641 --> 00:51:13.140
they're doing all
these predictions,

00:51:13.140 --> 00:51:14.765
and then they're just
tossing them over

00:51:14.765 --> 00:51:16.360
for us to make the
final judgment,

00:51:16.360 --> 00:51:19.890
and we're doing
less and less stuff.

00:51:19.890 --> 00:51:25.379
And I think that I also
end up often having

00:51:25.379 --> 00:51:26.670
a thought like that in my head.

00:51:26.670 --> 00:51:32.550
And that's because I think what
we're really good at as humans

00:51:32.550 --> 00:51:36.290
is we're good at extrapolating.

00:51:36.290 --> 00:51:37.832
Doing this knob
exercise, and saying,

00:51:37.832 --> 00:51:40.206
OK, if that prediction gets
better and better and better,

00:51:40.206 --> 00:51:41.900
then our bit gets
smaller and smaller,

00:51:41.900 --> 00:51:44.910
and the AIs do more work
and we're doing less.

00:51:44.910 --> 00:51:47.400
I think what we're
very poor at--

00:51:47.400 --> 00:51:49.320
what humans are very poor at--

00:51:49.320 --> 00:51:54.450
is imagining what other things
we will now apply judgment

00:51:54.450 --> 00:51:57.630
to because we have these low
cost, high fidelity predictions

00:51:57.630 --> 00:51:59.140
that we've never had before.

00:51:59.140 --> 00:52:03.297
So the thought experiment that
I offer the room is imagine--

00:52:03.297 --> 00:52:05.880
I just saw Henry Winkler being
interviewed on Stephen Colbert,

00:52:05.880 --> 00:52:07.260
so he's on my mind--

00:52:07.260 --> 00:52:12.060
so imagine walking up to
the cast of "Happy Days,"

00:52:12.060 --> 00:52:15.930
and saying, imagine having
a handheld device that's got

00:52:15.930 --> 00:52:17.970
super good, super
fast arithmetic.

00:52:17.970 --> 00:52:19.710
What would you do with it?

00:52:19.710 --> 00:52:22.476
And chances are, in
that era, people are not

00:52:22.476 --> 00:52:23.850
going to imagine
any of the stuff

00:52:23.850 --> 00:52:26.340
that we're currently doing.

00:52:26.340 --> 00:52:31.310
And so I think our barrier is
just imagining all the things

00:52:31.310 --> 00:52:33.380
that we're going
to do and that we

00:52:33.380 --> 00:52:35.630
will apply our judgment
to because now we

00:52:35.630 --> 00:52:38.720
get to apply that judgment
to much higher fidelity

00:52:38.720 --> 00:52:40.650
cheaper predictions
than we currently have.

00:52:40.650 --> 00:52:44.000
So an example to think about
is, imagine accountants.

00:52:44.000 --> 00:52:46.772
Accountants used to
effectively have two tasks.

00:52:46.772 --> 00:52:48.230
One is the one Avi
described, which

00:52:48.230 --> 00:52:50.370
is where they would add
up a bunch of numbers.

00:52:50.370 --> 00:52:53.340
So they would type
them in and add them.

00:52:53.340 --> 00:52:56.570
And then the second one
was after they added it up,

00:52:56.570 --> 00:52:58.480
then they ask questions
of their data.

00:52:58.480 --> 00:53:00.813
They say, well, what would
happen if interest rates went

00:53:00.813 --> 00:53:01.370
up by 1%?

00:53:01.370 --> 00:53:03.020
Or let's say they're
calculating net present value

00:53:03.020 --> 00:53:03.811
of some investment.

00:53:03.811 --> 00:53:06.450
They'll say, what would happen
if our sales were 3% higher

00:53:06.450 --> 00:53:08.405
in the fourth quarter?

00:53:08.405 --> 00:53:10.280
And then they would type
it all back in again

00:53:10.280 --> 00:53:14.162
with the variable changed and
come up with a new answer.

00:53:14.162 --> 00:53:15.620
So they had two
parts to their job.

00:53:15.620 --> 00:53:17.453
One was the typing
adding part and the other

00:53:17.453 --> 00:53:19.460
was the asking
questions of their data.

00:53:19.460 --> 00:53:22.860
Now spreadsheets roll into town.

00:53:22.860 --> 00:53:26.040
And if Avi was a faster
typer adder than me--

00:53:26.040 --> 00:53:29.250
so that was a valued
skill he had--

00:53:29.250 --> 00:53:32.610
that when spreadsheets arrived,
now he and I are the same.

00:53:32.610 --> 00:53:35.490
Now there's a much higher
return to the accountant who's

00:53:35.490 --> 00:53:38.190
good at asking questions,
because the adding typing

00:53:38.190 --> 00:53:39.990
part is super fast and cheap.

00:53:39.990 --> 00:53:43.340
And so the person who ask
good questions of their data,

00:53:43.340 --> 00:53:46.320
there's a higher return to
that part of the skill set.

00:53:46.320 --> 00:53:50.580
And so here our thesis is that
they'll be in higher returns

00:53:50.580 --> 00:53:55.830
to judgment, because I don't--
when people say, oh, you guys,

00:53:55.830 --> 00:53:58.204
do you think machines
are so great?

00:53:58.204 --> 00:54:00.370
That they're going to be
all these wonderful things.

00:54:00.370 --> 00:54:03.810
Do you think machines are
going to be so spectacular?

00:54:03.810 --> 00:54:05.400
The answer is not really.

00:54:05.400 --> 00:54:07.650
It's just that we think
that humans are not quite as

00:54:07.650 --> 00:54:08.691
great as we think we are.

00:54:08.691 --> 00:54:11.220
We're very poor predictors
and the machines

00:54:11.220 --> 00:54:14.841
are going to just become much
better predictors than we are.

00:54:14.841 --> 00:54:15.840
AVI GOLDFARB: Thank you.

00:54:15.840 --> 00:54:18.890
[APPLAUSE]

