Speaker 1:          00:00          You are listening to radio lab radio from W N Y. S. T. E. I'm Jad Abumrad. I'm Robert Krulwich. And you know what this is, is radio that, yeah. Okay. So we're going to play you a little bit of tape first just to set up the, what we're going to do today. About a month ago we were doing the thing about the fake news. Yeah. We're very worried about a lot of fake news. A lot of people are. But in the middle of doing that reporting, we, we were talking with a fellow from Vanity Fair. My name is Nick Dalton. I'm a special correspondent for Vanity Fair. And in the course of our conversation, nick and this had nothing to do what we were talking about, by the way, nick just got into a sort of a, well he went into a kind of a nervous reverie, I'd say. Yeah. He was like, you know, you guys want to talk about fake news, but that's not actually,

Speaker 2:          00:47          it's eating at me. The thing that I've been pretty obsessed with lately is actually not fake news, but it's automation and artificial intelligence. And uhm, and driverless cars because it's going to have a larger effect on society than any technology that I think has ever been created in the history of mankind. I know that's kind of a bold statement, but, uh, but you've got to imagine that you know, that there will be in the next 10 years, 20 to 50 million jobs that will just vanish, uh, to automation. Um, you've got, you know, million truckers that will lose their jobs. Um, uh, the, but it's not, we think about like automation and driverless cars and we think about the fact that, um, they are going to, uh, the people that just drive the cars, like the taxi drivers and the truckers are going to lose their jobs.

Speaker 2:          01:36          But what we don't realize is that there, there are entire industries that, that are built around just cars. So for example, if you're not driving the car, why do you need insurance? There's no parking tickets because you're driving this car knows where it can and cannot park and goes and finds a spot and moves and so on. Um, if there are truckers that are no longer using rest stops because driverless cars don't have to stop and pee or take a nap, then all of those little rest stops all across America are effected. People aren't stopping to use the restrooms, they're not buying burgers, they're not staying in these hotels and so on and so forth. And, and then you look at driverless cars to a next level. The whole concept of what a car is, is going to change. So for example, right now a car has five seats in a wheel, but if I'm not driving, well what's the point of having five seats in a wheel?

Speaker 2:          02:23          You could imagine that you take different cars. And maybe when I was on my way here to this interview, I wanted to work out, so I called a driverless gym card or I have a meeting out in Santa Monica after this and it's an hour. So I call a movie car to watch a movie on the way out there or office car and I pick up someone else and we have a meeting on the way and all of these things are going to happen, not in a vacuum, but simultaneously. This, you know, pizza delivery drivers are going to replace by robots that will actually cook your pizza on the way to your house in a little box and then deliver it. And so, kind of a little bit of a along with an answer, but I truly do think that, um, that it's gonna have a massive effect on society.

Speaker 3:          03:01          Am I stressing you guys out? Are you, are you having heart palpitations over there? My blood pressure has gone. Doesn't that? So that's a fairly compelling description of, uh, of a, um, of a very dangerous future. Yes. But you know what? It's funny, uh, one of the things that, I mean, we, we couldn't use that tape initially at least, right? But we kept thinking about it because it actually weirdly points us back to a story we did about a decade ago. The story of a moral problem that's about to get totally re-imagined. It may be that what nick is worried about and what we were worried about 10 years ago have now come dangerously close together. So what we thought we'd do is we are, we're going to play you this story as we did it then sort of the full segment and then we're going to amend it on the back end.

Speaker 3:          03:47          And by way of just disclaiming, this was at a moment in our development where there's just like way too many sound effects. Just gratuitous. You don't have to apologize. I'm going to apologize cause it was just too much, just too much. And also like we talk about the FMSI machine, like it's this like amazing thing when was, it's sort of commonplace now. Anyhow, it doesn't matter. We're going to play it for you. Uh, and then talk about it on the back end. This, we start with a description of something called the trolley problem. You Ready? Yup. Alright. You're near some train tracks.

Speaker 4:          04:21          Go there and your mind. Okay. There are five workers on the tracks working. You've got their backs turned to the trolley, which is coming in the distance.

Speaker 3:          04:30          I mean they were repairing the track, they were repairing the descent. Unbeknownst to them, the trolley is approaching. They don't see it. You can't shout to them. Okay. And if you do nothing, here's what will happen. I think workers will die. God, I was a horrible experience. I don't want that to happen. But you didn't. But you have a choice. You can do a nothing or B. It's so happens next to you is a lever pull the lever trolley will jump onto some sidetracks where there is only one person working. So if the send the trolley goes on the second track, it will kill the one guy. Yeah. So there's your choice. Do you kill one man by pulling a lever or do you kill five men by doing nothing? Well, I'm going to pull the lever naturally. Alright, here's part two. You stand in near some train tracks.

Speaker 3:          05:25          Five guys are in the tracks just as before and there is the trolley company. Same five guys working and five guys back to the train. They can't see. Yeah, exactly. However, and I made a couple changes. Now you're standing on a foot bridge. The past is over the tracks. You're looking down onto the tracks. There's no lever anywhere to be seen except next to you. There is a guy. What do you mean as a guy, a large guy, large individuals standing next to you on the bridge looking down with you over the tracks and you realize, wait, I can save those five workers if I push this man, give him a little tap. No, no hills land on the tracks.

Speaker 5:          06:03          Hey, stop the train. Oh man,

Speaker 6:          06:05          got to do that. I'm not going to do that. But surely you realize the math is the same. You mean I'll say four people this way? Yeah. Yeah, but this time I'm pushing the guy. Are you insane?

Speaker 3:          06:16          All right. Here's the thing. If you ask people these questions, and we did starting with the first, is it okay to kill one man to save five using a lever? Nine out of 10 people will say yes, yes, yes. Yeah. But if you ask them, is it okay to kill one man to say five by pushing the guy, nine out of 10 people will say no, no, no, no. It is practically universal. And the thing is if you ask people why is it okay to murder? Cause that's what it is. Murder a man with a lever and not okay to do it with your hands.

Speaker 7:          06:50          People don't really know. Pulling the library to save the five, no, that feels better than pushing the one to save the five. But I don't really know why. So that's a good, there's a good moral quandary, Maria. Huh?

Speaker 6:          07:09          And if having a moral sense is a unique and special human quality than maybe we, we us to humans anyway, you and me should at least inquire as to why this happens. And I happen to have met somebody

Speaker 5:          07:22          who has a hunch. He's a young guy at Princeton University, wild, uh, curly here, but the mischief in his eye, his name is Josh Goon. Alrighty. And he spent the last few years trying to figure out where this inconsistency comes from. How do people make this judgment, forget whether or not these judgements are right or wrong, just what's going on in the brain that makes people distinguish so naturally and intuitively between these two cases, which from an actuarial point of view are very, very, very similar, if not identical.

Speaker 6:          07:55          Josh is by the way a philosopher and in neuroscientists. So this gives him special powers. He doesn't sort of sit back in a chair, smoke a pipe and think, now why do you have these differences? He's just, no, I would like to look inside people's heads because in our heads we may find clues as to where these feelings of revulsion or acceptance come from

Speaker 5:          08:17          in our brains. Hi. So we're here in the control room where you basically just see and it just so happens that in the basement of Princeton, different things, one, there was this, um, yeah, yeah. Wow. Big circular thing. Yeah. It looks kind of like an airplane and [inaudible]

Speaker 6:          08:30          180,000 pound brains.

Speaker 5:          08:35          I'll tell you a funny story. You can't have any metal in there because the magnet, so we have this long list of questions that we ask people to make sure they can go in. Do you have a pacemaker or have you ever worked with metal, blah, blah, blah blah. Have you ever with metal? Yeah. Cause you could have little flecks of metal in your eyes that you would never even know are there from having done metal working. And one of the questions is whether or not you wear wig or anything like that because they often have metal wires in with that. And there was this very nice woman who does brain research here, who's Italian and she's asking her subjects over the phone, all, all these screening questions. And so I have this person over to dinner. She said, yeah, you know, I ended up, uh, doing this study, but it's asked you the weirdest questions. This woman's like, do you have a head of, and, and I'm like, what does it have to do with if I have herpes or not? And I want to say it anyway. And she said, you know, she asked you, she said, do you have a hairpiece? But she, uh, so now she asks people if you wear a wig or whatever.

Speaker 6:          09:26          Anyhow, what Josh does is he invites people into this room, has them lie down on what is essentially a cot on rollers and he rules them into the machine. Their heads are braced. So they're sort of stuck in there. They were done this, oh yeah, Yup. Several times and many tells them stories. He tells him the same too, you know, trolley tales that you told before. And then at the very instant that they're deciding whether I should push the lever or whether I should push the man. That instant the scanner snaps pictures of their brains. And what he found in those pictures was a frankly a little startling. Uh, he showed us some, I, I'll, I'll, I'll show you some, some, some stuff. Okay. Let me think. The picture that I'm looking at is this sort of, uh, it's, it's a brain looked, I guess from the top down, top down and sort of sliced, you know, like, like a, like a Deli slicer.

Speaker 6:          10:16          And the first slide that he showed me was a human brain being asked the question, would you pull the lever? And the answer, in most cases was yes, but pull the level when the brain saying yes, you'd see little pint of peanut shaped spots of yellow. This little guy right here and these two guys right there, the green was being active in these places and oddly enough when ever people said yes, yes, yes to the lever question, the very same pattern lit up. Then he showed me another slide. This was a side of brain saying, no, no, I would not push the man. I will not push the large man. And in this picture, this one we're looking at here, this, it was a totally different constellation of regions that lit up. This is the, no, no, no crowd. I think this is

Speaker 5:          10:59          part of the no, no, no crowd. So when people answer yes to the level of question, there are, there are places in their brain which glow, but when they answer, no, I will not push the man, then you get a completely different part of the brain lighting up. Even though the questions are basically the same, what does that mean and what does Josh make this? Oh, he has a theory about this. A theory, not proven, but I think that this is what I think the evidence suggests. Suggest that the human brain doesn't hum along like one big unified system. Instead he says, maybe in your brain, in every brain you'll find little warring tribes, little subgroups, one that is sort of doing a logical sort of counting kind of thing. You've got one part of the brain that says, Huh, five lives versus one life.

Speaker 5:          11:43          Wouldn't it be better to say five versus one? And that's the part that would glow when you answer yes, I'd pull the lever. Yeah, pull the lever, but there's this other part of the brain. Would you really, really like personally killing another human being and gets very upset that the fat man ks and shouts in effect? Oh No, you don't understand it on that level and says, no, no, no, Dad, don't do never do everything. A couple of books now instead of having sort of one system that just sort of churns out the answer and bing, we have multiple systems, they give different answers and they Duke it out and hopefully out of that competition comes morality.

Speaker 6:          12:25          This is not a trivial discovery that you struggle to find right and wrong depending upon what part of your brain is shouting the loudest. This is, it's like bleachers, morality. Do you buy this? Hmm. Ah, you know, I, I just don't know. I always kind of suspected that a sense of right and wrong is mostly stuff that you get from your mom and your dad and from experience that it's culturally learned. For the most part, Josh is kind of a radical in this respect. He thinks it's biological. I mean deeply biological that somehow we inherit from the deep past, a sense of right and wrong. That's already in our brains from the get go before mom and dad

Speaker 5:          13:08          are our primate ancestors, before we were full blown, humans had intensely social lives. They have social mechanisms that prevent them from doing all the nasty things that they might otherwise be interested in doing. And so deepen our brain. We have what you might call basic primate morality and basic primate morality doesn't understand things like tax evasion, but it does understand things like pushing your buddy off of a cliff. Ah, so you're thinking then that the man on the bridge that I'm on the bridge next to the large man and then I have hundreds of thousands of years of training in my brain that says don't murder the large man. Right? Whereas even if I'm thinking if I murdered the large man, I'm going to save five lives and only kill the woman, but there's something deeper down that says, don't murder the large man right now, that case, I think it's a pretty easy case even though it's five versus one.

Speaker 5:          14:00          In that case, people just go with what we might call the inner chimp, but there are other, but their unfortunate way of describing an act of, of deep goodness. That's interesting. 10 Commandments forgot inner chimp, right? Well, what's interesting is that we think of of basic human morality as being handed down from on high, and it's probably better to say that it was handed up from below, that our most basic core moral values are not the things that we humans have invented, but the things that we've actually inherited from other people, the stuff that we humans have invented or the things that seem more peripheral and variable, but something as basic as thou shalt not kill, which many people think was handed down in tablet form from a mountaintop, from God directly to humans. No chimps involved, right? You're suggesting that hundreds of thousands of years of on the ground training have gotten our brains to think, don't kill your kin, don't kill your these, you know, that should be your default response. I mean, certainly chimpanzees are extremely violent and they do kill each other, but they don't do it. As a matter of, of course, they, so to speak, have to have some context sensitive reason for doing so. So, hey, how are we getting to the rubber that you think that profound moral positions may be somehow embedded in brain chemistry? Um, yeah,

Speaker 3:          15:27          and Josh thinks, uh, there are times when these different moral positions that we have embedded inside of us in our brains when they can come into conflict. And in the original episode we went into one more story. This one you might call the crying baby dilemma.

Speaker 5:          15:44          The situation, uh, is, is somewhat similar to the last episode of Mash for people who are familiar with that. But the way we told the story, it goes like this. It's wartime [inaudible] patrol coming down the road. You are hiding in the basement with some of your fellow villagers. Let's kill those lights. And the enemy soldiers are outside. They have orders to kill anyone that they find

Speaker 8:          16:05          quiet. Please nobody make a sound until the pastors. So

Speaker 9:          16:10          you are, you're huddled in the basement all around your enemy troops and you're holding your baby in your arms, your baby with a coat, a bit of a sniffle. And you know, the good baby could cough at any moment. [inaudible]

Speaker 5:          16:27          they hear your baby. They're going to find you and the baby and everyone else, and they're going to kill everybody. And the only way you can stop this from happening is covered the baby's mouth. But if you do that, the baby's going to smother and die. If you don't cover the baby's mouth, soldiers are going to find everybody and everybody's going to be killed, including you, including your baby, and you have the choice. Would you smother your own baby to save the village or would you like your baby cough? Knowing the consequences, and this is a very tough question. People take a long time to think about it and some people say yes and some people say no

Speaker 8:          17:06          and children are a blessing and a gift from God. And we do not do that to children. Yes, I think I would heal my baby to save everyone else and myself. No, I would not kill the baby. I feel because it's my baby. I have the right to terminate the life. I'd like to say that I would kill the baby, but I don't know if I'd have the inner strength. Know if it comes down to killing my own child, my own daughter or my own son. Then I choose that. Yeah, if you have to do, because it was done in World War Two when the Germans were coming around, there was a mother that had a baby that was crying and rather than be found, she actually suffocated the baby, but the other people live. Sounds like an old mash. They know you do not kill your baby. Okay.

Speaker 5:          17:46          In the final mash episode,

Speaker 6:          17:50          the Korean woman who is a character in this piece, she later, sure baby. She killed it. She killed it.

Speaker 10:         18:00          Okay.

Speaker 11:         18:01          Oh my God. Oh my God. I didn't mean for it to kill it. I just wanted that to be quiet. It was a baby. Great kids mothered around bay rate

Speaker 10:         18:29          [inaudible].

Speaker 6:          18:30          What Josh did is he asked people the question, would you murder your own child while they were in the brain scanner and adjust the moment when they were trying to decide what they would do. He took pictures of their brains and what he saw, the contest we described before was global in the brain. It was like a world war. That gang of accountants, that part of the brain was busy [inaudible] calculating, cut a whole village could die of old village, could die. But the older and deeper reflects also was lit up shouting, don't kill the baby. No, don't kill the baby. Go inside. The brain was literally divided. Do the calculations. Don't kill two different tribes in the brain. Literally trying to just shout each other out. And Jen, this was a different kind of contest than the ones we talked about before. Remember before when people were pushing a man off of a bridge, overwhelmingly their brains yelled, no, no, don't push the man. And when people were pulling the lever overwhelmingly, yeah, yeah, pull the lever right there. It was distinct here. I don't think really anybody wins. Well who breaks the time and they had to answer something, right? Yeah. Yeah. That's a good question. And now is there a, do you, what happens, is it just two cries that fight each other out or

Speaker 5:          19:54          is there a judge? Well that's an interesting question and that's one of the things that we're looking at

Speaker 6:          19:59          when you are in this moment with parts of your brain contesting, there are two brain regions, these two areas here towards the front, right behind your eyebrows, left and right that light up

Speaker 10:         20:12          [inaudible]

Speaker 6:          20:12          and this is particular to us. He showed me a slide that's

Speaker 5:          20:15          those uh, sort of areas that are very highly developed in humans as compared to other species. So when we have a problem that we need to deliberate over the light, the front of the brain, this is above my eyebrow sort of. Yeah. Yeah, right about there. And there's two of them. One on the left and one on the right, bilateral. And they are the things that monkeys don't have as much of that. We have, certainly these parts of the brain are more highly developed in humans. Oh, looking at these two flashes of like at the front of a human brain, you could say, we are looking at what makes us special.

Speaker 12:         20:49          No.

Speaker 3:          20:49          So that's a fair statement. They human being wrestling with a problem. That's what that is. Yeah. Where it's both emotional, but there's also a sort of irrational attempt to sort of sort through those emotions. Those are the cases that are showing more activity in that area. So in those cases, when these dots above our eyebrows become active, what are they doing?

Speaker 6:          21:07          Well, he doesn't know for sure, but what he found is in these close contests, whenever those nodes are very, very active, it appears that the calculating section of the brain gets a bit of a boost. And the visceral inner chimp section of the brain is kind of muffled. Oh Hey

Speaker 12:         21:29          [inaudible]

Speaker 6:          21:29          people who chose to kill their children, who made what is essentially a logical decision over and over those subjects had brighter glows in these two areas and longer glows in these two areas. So there is a definite association between these two dots above the eyebrow and the power of the logical brain over the inner chamber or the visceral

Speaker 3:          21:53          right. Well, you know, that's the hypothesis. So it's going to take a lot of more research to sort of tease apart what these different parts of the brain are doing. Or if some of these are just sort of activated in incidental kind of way. I mean we really don't know. This is all, all very new

Speaker 13:         22:14          [inaudible].

Speaker 3:          22:14          Okay. So that was the story we put together many, many, many years ago, about a decade ago. And at that point the whole idea of thinking of morality is kind of purely a brain thing. It's relatively new and certainly the idea of philosophers working with MRI machines, that was super new. But now here we are 10 years later and some updates, uh, first of all, Josh Green. So in the, in the long, long [inaudible] the theme of times, I assume now you have a three giraffes, two Bob's rats and children. Yeah. So two kids and we're, we're close to adding a cat. We talk to them again. He has started a family. He's switched labs from Princeton to Harvard. But that whole time, that interim decade, he has still been thinking and working on the trolley problem. Did you ever write the story differently? Absolutely. So for years he's been trying out different permutations of the scenario on people. Like okay,

Speaker 6:          23:03          by firing freshman in college,

Speaker 3:          23:06          instead of pushing the guy off the bridge with your hands, what if you did it but not with your hands. So in one version we asked people about hitting a switch that opens a trap door on the foot bridge and drops the person in one version of that. The switches right next to the person in another version of the switch is far away and in yet another version, you're right next to the person and you don't push them off with your hands, but you push them with a pole. Oh, and to cut to the chase, uh, what Josh has found is it the basic results that we talked about that's roughly held up still the case that people would like to save the most number of lives, but not if it means pushing somebody with their own hands or with a Paul that matter. Now here's something kind of interesting.

Speaker 3:          23:47          Uh, he and others have found that there are two groups that are more willing to push the guy off the bridge. They are Buddhist monks and psychopaths. I mean some people just don't care very much about hurting other people. They don't have that kind of an emotional response. That would be the cycle bass, where's the Buddhist monks presumably are really good at shushing their inner champs. He called it and just saying to themselves, you know, I'm aware that this is that killing somebody is a terrible thing to do and I feel that, but I recognize that this has done for a noble reason and therefore it's, it's okay, so there's all kinds of interesting things you can say about the trolley problem as a thought experiment, but at the end of the day it's just that it's a thought experiment. What got us interested in revisiting it is that it seems like the thought experiment is about to get real.

Speaker 4:          24:45          That's coming up right after the break.

Speaker 14:         24:58          This is Amanda Darby calling from Rockville, Maryland. Radiolab is supported in part by the Alfred p Sloan Foundation enhancing public understanding of science and technology in the modern world. More information aboutSloan@wwwdotsloan.org

Speaker 15:         25:17          I'm forming you as attorney Prepara. My new podcast is called stay tuned with pre, it's about how power works in American. I talked to justice department officials to federal judges, even to a former defense secretary. And I tell the story of what happened when Donald Trump fired me. It's a collaboration between Wny c studios, cafe and Pineapple Street media. Get it wherever you get your pocket.

Speaker 11:         25:40          Yes,

Speaker 3:          25:50          Chad, Robert Radiolab. Okay, so where we left it is it the trolley problems about to get real. Here's how Josh Green put it. You know now as we're entering the age of self driving cars, oh this is, this is like the trolley problem. Now finally come to life

Speaker 8:          26:03          owner's card. Oh, the future of the automobile is here on us cars, autonomous vehicles. It's here. Build this part first. Self-Driving Volvo will be offered to customers in 2021 ah oh. Where's it going with this legislation or the first of its kind focused on the car of the future. That is more of a supercomputer on wheels. Okay, so self driving cars, unless you've been living under a muffler, they are coming. It's going to be a little bit of an adjustment for some of us. We hit the brakes. But what Josh meant

Speaker 3:          26:41          when he said it's the trolley problem come to life is basically this.

Speaker 16:         26:45          Imagine this scenario. Self-Driving card now is, uh, headed towards a bunch of pedestrians in the road. The only way to save them as to swerve out of the way, but that will run the car into a concrete wall and it will kill the passenger in the car.

Speaker 3:          26:59          Ah,

Speaker 16:         26:59          what should the car do? Should the car go straight and run over, say those five people or should it swerve and, and, and, and, and kill the one person

Speaker 3:          27:06          that suddenly is a real world question.

Speaker 17:         27:14          [inaudible]

Speaker 8:          27:15          if you ask people in the abstract like what theoretically should a car in this situation do, they're much more likely to say, I think you should sacrifice one for the good of the many. They should just try to do the most good or avoid the most harm. So if it's between one driver and five pedestrians, logically it would be the driver driver. Be Selfless. I think it should kill the driver.

Speaker 3:          27:36          But when you ask people, forget the theory, would you want to drive in a car that would

Speaker 16:         27:41          potentially sacrifice you to save the lives of more people in order to minimize the total amount of harm?

Speaker 8:          27:46          No, I wouldn't buy it. No. Absolutely not. That would kill me in it. No. So I'm not going, I'm not going to buy a car that's going to purposely kill me. Hell No. I wouldn't buy it for sure. No, I'll sell it, but I wouldn't buy it.

Speaker 3:          28:05          So there's your problem. People would sell a car and an idea of moral reasoning that they themselves wouldn't buy. And last fall and exec at Mercedes-Benz

Speaker 8:          28:18          face planted right into the middle of this contradiction.

Speaker 18:         28:26          Welcome to Paris, one of the most beautiful cities in the world. And Welcome to the 2016 Paris motor show home to some of the most beautiful cars in the world.

Speaker 3:          28:32          Okay. October, 2016, uh, the Paris motor show, you had something like a million people coming in over the course of a few days. All the major car makers

Speaker 18:         28:41          here is Ferrari. You can see the LaFerrari Abeta and of course the new gt four seat Luso t

Speaker 3:          28:48          everybody was debuting their new cars. Endo. One of the big presenters in this whole affair was this guy.

Speaker 18:         28:53          In the future you'll have cars where you don't even have to have your hands on the steering wheel anymore, but, but maybe you watch a movie on the head up display or maybe you want to do your emails. That's really what we're striving for.

Speaker 3:          29:04          This is Christoph on Hugo, a senior safety manager at Mercedes Benz. He was at the show sort of demonstrating a prototype of a car that could sort of self-drive its way through traffic and this e-class today, for example, he's got a maximum of comfort and support systems. You'll actually look forward to being stuck in traffic jams when you, of course, of course, he's doing dozens and dozens of interviews through the show and in one of those interviews, unfortunately this one we don't have on tape. He was asked, what would your driverless car do in a trolley problem type dilemma or maybe you have to choose between one or many and he answered quote, know you can save

Speaker 19:         29:42          one person at least say that one.

Speaker 3:          29:45          If you know you can save one person, save that one person.

Speaker 19:         29:48          I use the one in the car.

Speaker 3:          29:50          This is Michael Taylor corresponder for car and driver magazine. He was the one that Christoph on. Hugo said that too.

Speaker 19:         29:56          If you know for sure that one thing one day is can be prevented and that's your first priority.

Speaker 3:          30:04          Now, when he said this to you [inaudible], did it seem controversial at all in the moment?

Speaker 19:         30:09          In the moment it seems incredibly logical.

Speaker 3:          30:12          I mean all he's really doing is saying what's on people's minds, which is that, no, I wouldn't buy it. Who's going to buy a car? The chooses somebody else over them. Anyhow, he makes that comment. Michael Prince it and

Speaker 20:         30:29          a kerfuffle ensues. Say the one in the car. That's Christoph von Hugo from Mercedes. But then when you look at the questions, you sound like a bit of a heel because you want to save yourself as opposed to the pedestrian. Doesn't it ring though of like just privilege? It does. Yeah, it does. What would you do? It's you or a pedestrian and it's just, you know, I don't know anything about this pedestrian. It's just you were a pedestrian, just a regular guy walking down the street, screw everyone who's not in a Mercedes.

Speaker 16:         30:56          And there was this kind of uproar about that, uh, how dare you drive these selfish, you know, make these selfish cars. Uh, and then he walked it back and he said, no, no. What I mean is that, uh, just that we, that we have a better chance of protecting the people in the car, so we're going to protect them because they're easier to protect. But of course, yeah, there's always going to be trade offs. Yeah.

Speaker 3:          31:16          And those tradeoffs could get really, really tricky and subtle because obviously these cars have sensors, sensors, like cameras, radars, lasers, and ultrasound sensors. This is Raj Rajkumar. He's a professor at Carnegie Mellon. I'm the, uh, code director of the GM, CMU connected and autonomous driving collaborative research lab. He is one of the guys that is writing the code that will go inside gms a driverless car. And he says, yeah, the sensors at the moment on these cars still evolving. Pretty basic. We are very happy if today can actually detect a pedestrian, can detect a bicyclist or motorcyclist, different vehicles of different shapes, sizes and colors. But he says it won't be long before you can actually know a lot more about who these people are. Eventually they will be able to detect people of different sizes, shapes, and colors. Like, oh, that's a skinny person. That's a small person, tall person, black person, white person.

Speaker 3:          32:14          That's a little boy. That's a little girl. So forget the basic moral math. Like what does a car do if it has to decide, oh, do I save this boy or this girl? What about two girls versus one boy and an adult, but a cat versus a dog, a 75 year old guy in a suit versus that person over there who might be homeless. You can see where this is going and it's conceivable the cars will know our medical records. And back at the car show we've also had that term car to car. Uh, also one of the enabling technologies in highly automated driving. Mercedes guy basically said in a couple of years the cars will be network, there'll be talking to each other. So just imagine a scenario where like cars are about to get into accidents and right at the decision point they're like conferring. Well who do you have in your car? Me, I've got a 70 year old Wall Street guy makes eight figures. How about you? Well, I'm a bus full of kids. Kids have more years left. You need to move. We'll hold up. I see that your kids come from a poor neighborhood and have asthma. So I don't know.

Speaker 21:         33:06          So you can basically tie yourself up and knots a wrap yourself around an axle. We do not think that any programmer should be given this major a burden of deciding who survives and who gets killed. I think these are a very fundamental, deep, uh, issues that society has to decide at large. I don't think a programmer eating pizza and sipping coke should be making the call.

Speaker 3:          33:33          How does society decide? I mean, help me imagine that. I think it clearly has to be an evolutionary process. I believe Raj told us two things basically need to happen. First. We need to get these Robo cars on the road, get more experienced with how they interact with us human drivers and how we interact with them and to their need to be like industry wide summit.

Speaker 1:          33:53          No one company is going to solve that.

Speaker 3:          33:55          This is Bill Ford Jr of the Ford Company, uh, giving a speech in October of 2016 at the Economic Club of DC.

Speaker 1:          34:02          And we have to have, because he could you imagine if we had one algorithm and Toyota had another and general motors had another, I mean it would be amiable obviously couldn't do that.

Speaker 3:          34:11          What if the Tibetan cards make one decision in the American cars make another.

Speaker 1:          34:15          So we need to have a national discussion on ethics I think of, because we've never had to think of these things before, but the cars will have the time and the ability to do that

Speaker 21:         34:26          live a colleague,

Speaker 3:          34:28          boys this pasta so far, Germany is the only country that we know of that has tackled this head on. One of the most significant points the ethics commission made is that autonomous and connected driving is an ethical imperative. The government has released a code of ethics that says among other things that self driving cars are forbidden to discriminate between humans and almost any way not on race, not on gender, not on age, nothing shouldn't be programmed into the cars. One can imagine a few classes being added, uh, in the Geneva Convention, if you will, of what these automated vehicles should do. A globally accepted standard, if you will. How we get there to that globally accepted standard is anyone's guess and what it will look like. Whether it'll be like a coherent set of rules or like rife with the kind of contradictions we see in our own brain. That also remains to be seen, but one thing is clear. Oh, there are cars coming. Build this with their questions. Let me back from

Speaker 4:          35:38          me controlling it. Oh dear Jesus. I could never, oh, where's it going? Oh my God.

Speaker 3:          35:50          Okay. We do need to caveat all this by saying that the moral dilemma we're talking about in the case of these driverless cars is going to be super rare. Mostly what will probably happen is that like the plane loads full of people that die every day from car accidents. Well that's just going to hit the floor and so you have to balance the few cases where a car might make a decision you don't like against the massive number of lives saved.

Speaker 1:          36:17          I think I actually have a a different thing I was thinking, even though you dramatically bring down the, the, the number of bad things that happen on road, you dramatically bring down the collisions. You dramatically bring down the mortality. You dramatically lower the number of people who are drunk coming home from a party and just ram someone sideways and killing three of them and injuring two of them for the rest of their lives. Those kinds of things go way down, but they're the ones that remain, are engineered like they are calculated, uh, almost with foresight. [inaudible] so here's the difference and this is just an interesting difference. Like Ah Damn, that's so sad that happened that that guy got drunk and kind of that maybe he should go to jail. But you mean that this society engineered this in. That is a big difference. One is operatic and seems like the forces of destiny and the other seems mechanical and pre fought through. He meditated and there's something dark about a premeditated expected death and I don't know what you do about that. Everybody's on the hook for that. The killers

Speaker 3:          37:27          in the particular as it feels dark, it's a little bit like when you know, should you kill your own baby? Did you save the villas? Like in the particular instance of that one child, it's dark, but against the backdrop of the life saved. It's just a tiny bit

Speaker 1:          37:38          pinprick of darkness. That's all. Yeah, but you know how humans are. If you argue back that yes, a bunch of smarty pants is concocted a mathematical formula, which meant that some people had to die in here they are. There are many fewer than before a human being. Just like Josh would tell you, we'd have a roar of feeling and of anger and saying, how dare you engineer this in? No, no, no, no, no.

Speaker 3:          38:02          And that human being needs to meditate like the monks to silence that feeling because the feeling in that case, it's just getting in the way.

Speaker 1:          38:10          Yes and no, and that may be impossible unless you're a monk for God's sake. See, we're right back where we started down. Yeah. All right. We should go. Chad, you have to thank some people. No,

Speaker 3:          38:23          yes. A this piece was produced by Amanda Arran check with help from Bethel Hob Tay special thanks to e odd Rawan, Edmond Awad and Sidney Levine from the moral machine group at MIT. Also thanks to Suretec Karaman Chin Chang

Speaker 1:          38:36          and robo race for all their help, and I guess we should go now. Yeah, I'll, um, I'm Jad Abumrad not coming into your car. You don't mind? Just take my own, I'm going to rig up an autonomous vehicle to the bottom of your bed, so you're going to go to bed and suddenly find yourself on the highway driving you wherever I want anyhow. Okay. We should go. Yeah, I'm [inaudible] I'm Robert Krulwich. Thanks for listening.