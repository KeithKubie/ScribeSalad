WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.315
[MUSIC PLAYING]

00:00:04.835 --> 00:00:06.960
JEFF DEAN: I'm excited to
be here today to tell you

00:00:06.960 --> 00:00:11.190
about how I see deep
learning and how

00:00:11.190 --> 00:00:14.250
it can be used to solve some of
the really challenging problems

00:00:14.250 --> 00:00:16.170
that the world is facing.

00:00:16.170 --> 00:00:18.357
And I should point out
that I'm presenting

00:00:18.357 --> 00:00:20.440
the work of many, many
different people at Google.

00:00:20.440 --> 00:00:22.842
So this is a broad perspective
of a lot of the research

00:00:22.842 --> 00:00:23.550
that we're doing.

00:00:23.550 --> 00:00:27.040
It's not purely my work.

00:00:27.040 --> 00:00:29.880
So first, I'm sure you
may have all noticed,

00:00:29.880 --> 00:00:32.980
but machine learning is
growing in importance.

00:00:32.980 --> 00:00:37.140
There's a lot more emphasis
on machine learning research.

00:00:37.140 --> 00:00:39.270
There's a lot more uses
of machine learning.

00:00:39.270 --> 00:00:42.420
This is a graph showing
how many Arxiv papers--

00:00:42.420 --> 00:00:44.220
Arxiv is a preprint
hosting service

00:00:44.220 --> 00:00:46.170
for all kinds of
different research.

00:00:46.170 --> 00:00:47.970
And this is the
subcategories of it

00:00:47.970 --> 00:00:49.780
that are related to
machine learning.

00:00:49.780 --> 00:00:53.460
And what you see is that,
since 2009, we've actually

00:00:53.460 --> 00:00:56.820
been growing the number of
papers posted at a really

00:00:56.820 --> 00:01:00.810
fast exponential rate, actually
faster than the Moore's Law

00:01:00.810 --> 00:01:03.840
growth rate of computational
power that we got so nice

00:01:03.840 --> 00:01:06.500
and used to for 40 years
but it's now slowed down.

00:01:06.500 --> 00:01:10.050
So we've replaced the nice
growth in computing performance

00:01:10.050 --> 00:01:15.180
with growth in people
generating ideas, which is nice.

00:01:15.180 --> 00:01:18.120
And deep learning is
this particular form

00:01:18.120 --> 00:01:19.780
of machine learning.

00:01:19.780 --> 00:01:21.750
It's actually a
rebranding in some sense

00:01:21.750 --> 00:01:25.470
of a very old set of
ideas around creating

00:01:25.470 --> 00:01:27.060
artificial neural networks.

00:01:27.060 --> 00:01:30.000
These are these collections of
simple trainable mathematical

00:01:30.000 --> 00:01:34.500
units organized in layers where
the higher layers typically

00:01:34.500 --> 00:01:36.240
build higher levels
of abstraction

00:01:36.240 --> 00:01:39.390
based on things that the
lower layers are learning.

00:01:39.390 --> 00:01:43.200
And you can train these
things end to end.

00:01:43.200 --> 00:01:45.720
And the algorithms that
underlie a lot of the work

00:01:45.720 --> 00:01:47.220
that we're doing
today actually were

00:01:47.220 --> 00:01:49.170
developed 35, 40 years ago.

00:01:49.170 --> 00:01:51.480
In fact, my colleague
Geoff Hinton

00:01:51.480 --> 00:01:56.250
just won the Turing Award this
year along with Yann LeCun

00:01:56.250 --> 00:01:58.710
and Yoshua Bengio
for a lot of the work

00:01:58.710 --> 00:02:03.010
that they did over the
past 30 or 40 years.

00:02:03.010 --> 00:02:05.970
And really the
ideas are not new.

00:02:05.970 --> 00:02:10.470
But what's changed is we got
amazing results 30 or 40 years

00:02:10.470 --> 00:02:14.130
ago on toyish
problems but didn't

00:02:14.130 --> 00:02:16.890
have the computational resources
to make these approaches work

00:02:16.890 --> 00:02:18.600
on real large scale problems.

00:02:18.600 --> 00:02:21.353
But starting about
eight or nine years ago,

00:02:21.353 --> 00:02:23.520
we started to have enough
computation to really make

00:02:23.520 --> 00:02:26.500
these approaches work well.

00:02:26.500 --> 00:02:30.360
And so what are things-- think
of a neural net as something

00:02:30.360 --> 00:02:33.420
that can learn really
complicated functions that

00:02:33.420 --> 00:02:35.525
map from input to output.

00:02:35.525 --> 00:02:36.900
Now that sounds
kind of abstract.

00:02:36.900 --> 00:02:39.850
You think of functions as like
y equals x squared or something.

00:02:39.850 --> 00:02:43.110
But really these functions
can be very complicated

00:02:43.110 --> 00:02:45.150
and can learn from
very raw forms of data.

00:02:45.150 --> 00:02:47.310
So you can take the
pixels of an image

00:02:47.310 --> 00:02:49.020
and train a neural
net to predict

00:02:49.020 --> 00:02:51.900
what is in the image as a
categorical label like that's

00:02:51.900 --> 00:02:52.890
a leopard.

00:02:52.890 --> 00:02:56.250
That's one of my
vacation photos.

00:02:56.250 --> 00:02:59.220
From audio wave
forms, you can learn

00:02:59.220 --> 00:03:01.650
to predict a transcript
of what is being said.

00:03:01.650 --> 00:03:03.750
How cold is it outside?

00:03:03.750 --> 00:03:07.050
You can learn to take input
in one language-- hello,

00:03:07.050 --> 00:03:08.220
how are you--

00:03:08.220 --> 00:03:12.900
and predict the output being
that sentence translated

00:03:12.900 --> 00:03:13.960
into another language.

00:03:13.960 --> 00:03:16.265
[SPEAKING FRENCH]

00:03:16.265 --> 00:03:17.890
You can even do more
complicated things

00:03:17.890 --> 00:03:23.320
like take the pixels of an
image and create a caption that

00:03:23.320 --> 00:03:24.290
describes the image.

00:03:24.290 --> 00:03:25.290
It's not just category.

00:03:25.290 --> 00:03:26.830
It's like a simple sentence.

00:03:26.830 --> 00:03:30.230
A cheetah lying on top of a
car, which is kind of unusual

00:03:30.230 --> 00:03:30.887
anyway.

00:03:30.887 --> 00:03:32.720
Your priority for that
should be pretty low.

00:03:35.270 --> 00:03:36.830
And in the field
of computer vision,

00:03:36.830 --> 00:03:39.800
we've made great strides
thanks to neural nets.

00:03:39.800 --> 00:03:42.500
In 2011, the Stanford
ImageNet contest,

00:03:42.500 --> 00:03:44.840
which is a contest
held every year,

00:03:44.840 --> 00:03:46.590
the winning entry did
not use neural nets.

00:03:46.590 --> 00:03:47.548
That was the last year.

00:03:47.548 --> 00:03:49.300
The winning entry did
not use neural nets.

00:03:49.300 --> 00:03:50.880
They got 26% error.

00:03:50.880 --> 00:03:52.760
And that won the contest.

00:03:52.760 --> 00:03:56.010
We know this task--

00:03:56.010 --> 00:03:57.510
it's not a trivial task.

00:03:57.510 --> 00:03:59.690
So humans themselves
have about 5%

00:03:59.690 --> 00:04:01.830
error, because you
have to distinguish

00:04:01.830 --> 00:04:03.680
among 1,000 different
categories of things

00:04:03.680 --> 00:04:06.290
including like a picture of
a dog, you have to say which

00:04:06.290 --> 00:04:07.700
of 40 breeds of dog is it.

00:04:07.700 --> 00:04:11.180
So it's not a completely
trivial thing.

00:04:11.180 --> 00:04:16.600
And in 2016, for example, the
winning entry got 3% error.

00:04:16.600 --> 00:04:18.660
So this is just a
huge fundamental leap

00:04:18.660 --> 00:04:19.748
in computer vision.

00:04:19.748 --> 00:04:21.540
You know, computers
went from basically not

00:04:21.540 --> 00:04:25.800
being able to see in 2011 to
now we can see pretty darn well.

00:04:25.800 --> 00:04:28.620
And that has huge ramifications
for all kinds of things

00:04:28.620 --> 00:04:30.553
in the world not
just computer science

00:04:30.553 --> 00:04:32.970
but like the application of
machine learning and computing

00:04:32.970 --> 00:04:35.940
to perceiving the
world around us.

00:04:35.940 --> 00:04:37.283
OK.

00:04:37.283 --> 00:04:38.700
So the rest of
this talk I'm going

00:04:38.700 --> 00:04:44.970
to frame in a way of-- but
in 2008, the US National

00:04:44.970 --> 00:04:48.540
Academy of Engineering
published this list of 14

00:04:48.540 --> 00:04:51.450
grand engineering challenges
for the 21st century.

00:04:51.450 --> 00:04:53.280
And they got together
a bunch of experts

00:04:53.280 --> 00:04:55.110
across lots of
different domains.

00:04:55.110 --> 00:04:56.910
And they all
collectively came up

00:04:56.910 --> 00:04:58.980
with this list of
14 things, which

00:04:58.980 --> 00:05:01.080
I think you can agree
these are actually

00:05:01.080 --> 00:05:02.380
pretty challenging problems.

00:05:02.380 --> 00:05:04.230
And if we made progress
on all of them,

00:05:04.230 --> 00:05:06.490
the world would be
a healthier place.

00:05:06.490 --> 00:05:10.150
We'd have a safer place.

00:05:10.150 --> 00:05:12.360
We'd have more
scientific discovery.

00:05:12.360 --> 00:05:16.270
All these things are
important problems.

00:05:16.270 --> 00:05:19.380
And so given the limited
time, what I'm going to do

00:05:19.380 --> 00:05:22.080
is talk about the
ones in boldface.

00:05:22.080 --> 00:05:27.540
And we have projects in Google
Research that are focused

00:05:27.540 --> 00:05:29.970
on all the ones listed in red.

00:05:29.970 --> 00:05:33.910
But I'm not going to talk
about the other ones.

00:05:33.910 --> 00:05:36.420
And so that's kind of the
tour of the rest of the talk.

00:05:36.420 --> 00:05:39.070
We're just going to
dive in and off we go.

00:05:39.070 --> 00:05:41.310
I think we start with
restoring and improving

00:05:41.310 --> 00:05:42.700
urban infrastructure.

00:05:42.700 --> 00:05:43.200
Right.

00:05:43.200 --> 00:05:47.130
We know cities were designed--
the basic structure of cities

00:05:47.130 --> 00:05:49.580
has been designed
quite some time ago.

00:05:49.580 --> 00:05:52.350
But there's some
changes that we're

00:05:52.350 --> 00:05:55.110
on the cusp of that are going to
really dramatically change how

00:05:55.110 --> 00:05:57.660
we might want to design cities.

00:05:57.660 --> 00:05:59.910
And, in particular,
autonomous vehicles

00:05:59.910 --> 00:06:03.810
are on the verge of
commercial practicality.

00:06:03.810 --> 00:06:07.770
This is from our Waymo
colleagues, part of Alphabet.

00:06:07.770 --> 00:06:12.660
They've been doing work in
this space for almost a decade.

00:06:12.660 --> 00:06:16.470
And the basic problem
of an autonomous vehicle

00:06:16.470 --> 00:06:18.360
is you have to perceive
the world around you

00:06:18.360 --> 00:06:20.840
from raw sensory inputs,
things like light [INAUDIBLE],,

00:06:20.840 --> 00:06:24.553
and cameras, and radar,
and other kinds of things.

00:06:24.553 --> 00:06:26.970
And you want to build a model
of the world and the objects

00:06:26.970 --> 00:06:29.012
around you and understand
what those objects are.

00:06:29.012 --> 00:06:31.320
Is that a pedestrian
or a light pole?

00:06:31.320 --> 00:06:32.580
Is it a car that's moving?

00:06:32.580 --> 00:06:33.870
What is it?

00:06:33.870 --> 00:06:38.340
And then also be able to predict
both a short time from now,

00:06:38.340 --> 00:06:41.370
like where is that car
going to be in one second,

00:06:41.370 --> 00:06:44.250
and then make a set of
decisions about what actions

00:06:44.250 --> 00:06:46.080
you want to take to
accomplish the goals,

00:06:46.080 --> 00:06:51.420
get from A to B without
having any trouble.

00:06:51.420 --> 00:06:54.210
And it's really thanks
to deep learning vision

00:06:54.210 --> 00:06:57.027
based algorithms and fusing
of all the sensor data

00:06:57.027 --> 00:06:58.860
that we can actually
build maps of the world

00:06:58.860 --> 00:07:01.500
like this that
are understandings

00:07:01.500 --> 00:07:03.210
of the environment
around us and actually

00:07:03.210 --> 00:07:07.680
have these things operate
in the real world.

00:07:07.680 --> 00:07:10.590
This is not some
distant far off dream.

00:07:10.590 --> 00:07:15.300
Waymo is actually
operating about 100 cars

00:07:15.300 --> 00:07:17.400
with passengers in the
back seat and no safety

00:07:17.400 --> 00:07:20.800
drivers in the front seat in
the Phoenix, Arizona area.

00:07:20.800 --> 00:07:23.550
And so this is a
pretty strong sense

00:07:23.550 --> 00:07:25.290
that this is pretty
close to reality.

00:07:25.290 --> 00:07:29.310
Now Arizona is one of the easier
self-driving car environments.

00:07:29.310 --> 00:07:32.730
It's like it never rains.

00:07:32.730 --> 00:07:36.120
It's too hot so there aren't
that many pedestrians.

00:07:36.120 --> 00:07:38.042
The streets are very wide.

00:07:38.042 --> 00:07:39.375
The other drivers are very slow.

00:07:42.300 --> 00:07:43.800
Downtown San
Francisco is harder,

00:07:43.800 --> 00:07:48.750
but this is a sign that
it's not that far off.

00:07:48.750 --> 00:07:51.300
Obviously, a vision
works, it's easier

00:07:51.300 --> 00:07:53.570
to build robots that can
do things in the world.

00:07:53.570 --> 00:07:55.570
If you can't see, it's
really hard to do things.

00:07:55.570 --> 00:07:57.690
But if you can start to
see, you can actually

00:07:57.690 --> 00:08:00.540
have practical
robotics things that

00:08:00.540 --> 00:08:02.972
use computer vision to then
make decisions about how

00:08:02.972 --> 00:08:04.180
they should act in the world.

00:08:04.180 --> 00:08:07.650
So this is a video of a
bunch of robots practicing

00:08:07.650 --> 00:08:10.600
picking things up, and then
dropping them and picking

00:08:10.600 --> 00:08:14.380
more things up, and essentially
trying to grasp things.

00:08:14.380 --> 00:08:18.240
And it turns out that one
nice thing about robots

00:08:18.240 --> 00:08:20.520
is you can actually
collect the sensor data

00:08:20.520 --> 00:08:24.000
and pool the experience
of many robots,

00:08:24.000 --> 00:08:27.780
and then collectively train on
their collective experience,

00:08:27.780 --> 00:08:32.250
and then get a
better model of how

00:08:32.250 --> 00:08:35.417
to actually grasp things,
and then push that out

00:08:35.417 --> 00:08:36.000
to the robots.

00:08:36.000 --> 00:08:37.417
And then the next
day they can all

00:08:37.417 --> 00:08:39.580
practice with a slightly
better grasping model,

00:08:39.580 --> 00:08:41.640
because unlike
humans that you plop

00:08:41.640 --> 00:08:45.960
on the carpet in
your living room,

00:08:45.960 --> 00:08:48.990
they don't get to
pool their experience.

00:08:48.990 --> 00:08:49.530
OK.

00:08:49.530 --> 00:08:55.200
So in 2015, the success rate
on a particular grasping task

00:08:55.200 --> 00:08:58.500
of grasping objects that a
robot has never seen before

00:08:58.500 --> 00:09:00.960
was about 65%.

00:09:00.960 --> 00:09:04.375
When we use this
kind of arm farm--

00:09:04.375 --> 00:09:05.750
that's what that
thing is called.

00:09:05.750 --> 00:09:09.930
I wanted to call it the
armpit, but I was overruled.

00:09:09.930 --> 00:09:13.458
Basically, by collecting
a lot of experience,

00:09:13.458 --> 00:09:15.750
we were actually able to get
a pretty significant boost

00:09:15.750 --> 00:09:19.020
in grasp success
rate, up to 78%.

00:09:19.020 --> 00:09:24.630
And then with further work on
algorithms and more refinement

00:09:24.630 --> 00:09:28.150
of the approach, we're now
able to get a 96% grasp success

00:09:28.150 --> 00:09:28.650
right.

00:09:28.650 --> 00:09:32.310
So this is pretty good
progress in three years.

00:09:32.310 --> 00:09:35.417
We've gone from a third of the
time you fail to pick something

00:09:35.417 --> 00:09:37.500
up, which is very hard to
actually string together

00:09:37.500 --> 00:09:41.040
a whole sequence of things and
actually have robots actually

00:09:41.040 --> 00:09:45.930
do things in the real world, to
grasping almost working quite

00:09:45.930 --> 00:09:47.550
reliably.

00:09:47.550 --> 00:09:49.362
So that's exciting.

00:09:49.362 --> 00:09:50.820
We've also been
doing a lot of work

00:09:50.820 --> 00:09:55.297
on how do we get robots
to do things more easily.

00:09:55.297 --> 00:09:57.130
Rather than having them
practice themselves,

00:09:57.130 --> 00:09:59.380
maybe we can demonstrate
things to them.

00:09:59.380 --> 00:10:03.960
So this is one of our
AI residents doing work.

00:10:03.960 --> 00:10:06.270
They also do fantastic
machine learning research,

00:10:06.270 --> 00:10:10.110
but they also film demonstration
videos for these robots.

00:10:10.110 --> 00:10:12.150
And what you see here
is a simulated robot

00:10:12.150 --> 00:10:16.260
trying to emulate from the
raw pixels of the video what

00:10:16.260 --> 00:10:18.190
it's seeing.

00:10:18.190 --> 00:10:22.710
And on the right, you see a
few demonstrations of pouring

00:10:22.710 --> 00:10:25.950
and the robot using
those video clips,

00:10:25.950 --> 00:10:28.830
five or 10 seconds of
someone pouring something,

00:10:28.830 --> 00:10:31.770
and some reinforcement learning
based trials to attempt

00:10:31.770 --> 00:10:35.340
to learn to pour on its own.

00:10:35.340 --> 00:10:38.370
After 15 trials and about
15 minutes of training,

00:10:38.370 --> 00:10:41.040
it's able to pour
that well, I would

00:10:41.040 --> 00:10:43.770
say like at the level
of a four-year-old

00:10:43.770 --> 00:10:45.960
not an eight-year-old.

00:10:45.960 --> 00:10:47.370
But that's actually much--

00:10:47.370 --> 00:10:50.880
in 15 minutes of
effort, it's able to get

00:10:50.880 --> 00:10:54.928
to that level of success,
which is a pretty big deal.

00:10:54.928 --> 00:10:57.190
OK.

00:10:57.190 --> 00:10:59.610
One of the other areas that
was in the grand challenges

00:10:59.610 --> 00:11:01.080
was advanced health informatics.

00:11:01.080 --> 00:11:02.747
I think you saw in
the keynote yesterday

00:11:02.747 --> 00:11:05.022
the work on lung cancer.

00:11:05.022 --> 00:11:06.480
We've also been
doing a lot of work

00:11:06.480 --> 00:11:10.560
on an eye disease called
diabetic retinopathy, which

00:11:10.560 --> 00:11:13.510
is the fastest growing cause
of blindness in the world.

00:11:13.510 --> 00:11:16.020
There's 115 million people
in the world with diabetes.

00:11:16.020 --> 00:11:18.660
And each of them ideally
would be screened every year

00:11:18.660 --> 00:11:21.030
to see if they have
diabetic retinopathy, which

00:11:21.030 --> 00:11:24.120
is a degenerative eye disease
that if you catch in time

00:11:24.120 --> 00:11:25.163
it's very treatable.

00:11:25.163 --> 00:11:26.580
But if you don't
catch it in time,

00:11:26.580 --> 00:11:29.680
you can suffer full or
partial vision loss.

00:11:29.680 --> 00:11:31.350
And so it's really
important that we

00:11:31.350 --> 00:11:34.790
be able to screen everyone
that is at risk for this.

00:11:34.790 --> 00:11:36.600
And yeah.

00:11:36.600 --> 00:11:37.350
Regular screening.

00:11:37.350 --> 00:11:38.820
And that's the
image that you get

00:11:38.820 --> 00:11:42.570
to see as an ophthalmologist.

00:11:42.570 --> 00:11:44.850
And in India, for
example, there's

00:11:44.850 --> 00:11:47.010
a shortage of more than
100,000 eye doctors

00:11:47.010 --> 00:11:50.800
to do the necessary amount
of screening of this disease.

00:11:50.800 --> 00:11:53.520
And so 45% of patients
suffer vision loss

00:11:53.520 --> 00:11:55.937
before they're diagnosed,
which is tragic,

00:11:55.937 --> 00:11:58.020
because it's a completely
preemptible thing if you

00:11:58.020 --> 00:12:00.760
catch it in time.

00:12:00.760 --> 00:12:03.313
And basically, the way an
ophthalmologist looks at this

00:12:03.313 --> 00:12:05.230
is they look at these
images and they grade it

00:12:05.230 --> 00:12:08.200
on a five point scale, one,
two, three, four, or five,

00:12:08.200 --> 00:12:10.270
looking for things like
these little hemorrhages

00:12:10.270 --> 00:12:13.240
that you see on the
right hand side.

00:12:13.240 --> 00:12:16.103
And it's a little subjective.

00:12:16.103 --> 00:12:17.520
So if you ask two
ophthalmologists

00:12:17.520 --> 00:12:22.370
to grade the same image, they
agree on the score, one, two,

00:12:22.370 --> 00:12:25.335
three, four, or five,
60% of the time.

00:12:25.335 --> 00:12:26.960
And if you ask the
same ophthalmologist

00:12:26.960 --> 00:12:29.570
to grade the same image
a few hours later,

00:12:29.570 --> 00:12:33.540
they agree with themselves
65% of the time.

00:12:33.540 --> 00:12:37.700
And this is why second opinions
are useful in medicine,

00:12:37.700 --> 00:12:41.425
because some of these things
are actually quite subjective.

00:12:41.425 --> 00:12:43.550
And it's actually a big
deal because the difference

00:12:43.550 --> 00:12:47.630
between a two and a three
is actually go away and come

00:12:47.630 --> 00:12:49.310
back in a year versus
we better get you

00:12:49.310 --> 00:12:50.660
into the clinic next week.

00:12:53.210 --> 00:12:57.410
Nonetheless, this is actually
a computer vision problem.

00:12:57.410 --> 00:13:01.180
And so instead of having a
classification of a thousand

00:13:01.180 --> 00:13:04.150
general categories
of dogs and leopards,

00:13:04.150 --> 00:13:07.030
you can actually just have
five categories of the five

00:13:07.030 --> 00:13:09.670
levels of diabetic
retinopathy and train

00:13:09.670 --> 00:13:13.390
the model on eye images
and an assessment

00:13:13.390 --> 00:13:15.610
of what the score should be.

00:13:15.610 --> 00:13:17.950
And if you do that,
you can actually

00:13:17.950 --> 00:13:21.160
get the images labeled by
several ophthalmologists, six

00:13:21.160 --> 00:13:24.370
or seven, so that you reduce
the variance that you already

00:13:24.370 --> 00:13:28.380
see between ophthalmologists
assessing the same image.

00:13:28.380 --> 00:13:30.162
Five of them say it's two.

00:13:30.162 --> 00:13:32.620
Two of them say it's a three,
it's probably more like a two

00:13:32.620 --> 00:13:34.680
than a three.

00:13:34.680 --> 00:13:37.380
And if you do that,
then you can essentially

00:13:37.380 --> 00:13:41.070
get a model that is on
par or slightly better

00:13:41.070 --> 00:13:44.380
than the average board
certified ophthalmologist that's

00:13:44.380 --> 00:13:48.170
set at doing this
task, which is great.

00:13:48.170 --> 00:13:52.030
This is work published
at the end of 2016

00:13:52.030 --> 00:13:59.412
by my colleagues in "JAMA,"
which is a top medical journal.

00:13:59.412 --> 00:14:00.870
We wanted to do
even better though.

00:14:00.870 --> 00:14:04.560
So it turns out you can
actually, instead of--

00:14:04.560 --> 00:14:08.240
you can get the images labeled
by retinal specialists who

00:14:08.240 --> 00:14:12.420
have more training in
retinal eye disease.

00:14:12.420 --> 00:14:15.000
And instead of getting
independent assessments,

00:14:15.000 --> 00:14:16.440
you get three
retinal specialists

00:14:16.440 --> 00:14:18.000
in a room for each image.

00:14:18.000 --> 00:14:19.800
And you essentially
say, OK, you all

00:14:19.800 --> 00:14:23.020
have to come up with
an adjudicated number.

00:14:23.020 --> 00:14:25.930
What number do you
agree on for each image?

00:14:25.930 --> 00:14:27.600
And if you do that,
then you can train

00:14:27.600 --> 00:14:30.995
on the output of this consensus
of three retinal specialists.

00:14:30.995 --> 00:14:32.370
And you actually
now have a model

00:14:32.370 --> 00:14:34.710
that is on par with
retinal specialists, which

00:14:34.710 --> 00:14:38.310
is the gold standard
of care in this area,

00:14:38.310 --> 00:14:40.620
rather than the
not as good model

00:14:40.620 --> 00:14:43.032
trained on an
ophthalmologist's opinion.

00:14:43.032 --> 00:14:44.490
And so this is
something that we've

00:14:44.490 --> 00:14:47.670
seen born out where you have
really good high quality

00:14:47.670 --> 00:14:49.110
training data and
you can actually

00:14:49.110 --> 00:14:50.880
then train a model
on that and get

00:14:50.880 --> 00:14:53.190
the effects of retinal
specialists into the model.

00:14:55.558 --> 00:14:57.600
But the other neat thing
is you can actually have

00:14:57.600 --> 00:14:59.470
completely new discoveries.

00:14:59.470 --> 00:15:04.350
So someone new joined the
ophthalmology research team

00:15:04.350 --> 00:15:07.440
as a warm up exercise
to understand

00:15:07.440 --> 00:15:09.240
how our tools worked.

00:15:09.240 --> 00:15:12.588
Lily Peng, who is on
the stage yesterday,

00:15:12.588 --> 00:15:14.130
said, oh, why don't
you go see if you

00:15:14.130 --> 00:15:19.110
can predict age and gender
from the retinal image

00:15:19.110 --> 00:15:21.480
just to see if the machine
learning pipeline--

00:15:21.480 --> 00:15:24.090
a person could get that machine
learning pipeline going?

00:15:24.090 --> 00:15:27.270
And ophthalmologists
can't predict gender

00:15:27.270 --> 00:15:29.310
from an eye image.

00:15:29.310 --> 00:15:30.820
They don't know how to do that.

00:15:30.820 --> 00:15:33.540
And so Lilly thought the
average that you see on this

00:15:33.540 --> 00:15:36.090
should be no better
than flipping a coin.

00:15:36.090 --> 00:15:37.085
You see a 0.5.

00:15:37.085 --> 00:15:38.460
And the person
went away and they

00:15:38.460 --> 00:15:40.190
said, OK, I've got it done.

00:15:40.190 --> 00:15:41.988
My AUC is 0.7.

00:15:41.988 --> 00:15:43.530
And Lilly is like,
hmm, that's weird.

00:15:43.530 --> 00:15:45.270
Go check everything
and come back.

00:15:45.270 --> 00:15:46.770
And so they came
back and they said,

00:15:46.770 --> 00:15:48.145
OK, I've made a
few improvements.

00:15:48.145 --> 00:15:51.090
It's now 0.8.

00:15:51.090 --> 00:15:53.670
That got people excited
because all of a sudden

00:15:53.670 --> 00:15:55.170
we realized you can
actually predict

00:15:55.170 --> 00:15:58.260
a whole bunch of interesting
things from a retinal image.

00:15:58.260 --> 00:16:00.390
In particular, you
can actually detect

00:16:00.390 --> 00:16:02.398
someone's self-reported sex.

00:16:02.398 --> 00:16:04.440
And you can predict a
whole bunch of other things

00:16:04.440 --> 00:16:08.220
like their age, things about
their systolic and diastolic

00:16:08.220 --> 00:16:11.400
blood pressure, their
hemoglobin level.

00:16:11.400 --> 00:16:13.680
And it turns out you combine
those things together

00:16:13.680 --> 00:16:18.690
and you can get a prediction of
someone's cardiovascular risk

00:16:18.690 --> 00:16:21.300
at the same level of accuracy
that normally a much more

00:16:21.300 --> 00:16:24.210
invasive blood test where you
have to draw blood, send it off

00:16:24.210 --> 00:16:28.023
to the lab, wait 24 hours,
get the lab test back.

00:16:28.023 --> 00:16:29.940
Now you can just do that
with a retinal image.

00:16:29.940 --> 00:16:33.120
So there's real hope that
this could be a new thing

00:16:33.120 --> 00:16:35.020
that if you go to
the doctor you'll get

00:16:35.020 --> 00:16:36.388
a picture of your eye taken.

00:16:36.388 --> 00:16:38.430
And we'll have a longitudinal
history of your eye

00:16:38.430 --> 00:16:41.030
and be able to learn
new things from it.

00:16:41.030 --> 00:16:44.350
So we're pretty
excited about that.

00:16:44.350 --> 00:16:48.190
A lot of the grand challenges
were around understanding

00:16:48.190 --> 00:16:51.790
molecules and chemistry better.

00:16:51.790 --> 00:16:53.303
One is engineer
better medicines.

00:16:53.303 --> 00:16:54.970
But this work that
I'm going to show you

00:16:54.970 --> 00:16:58.130
might apply to some
of these other things.

00:16:58.130 --> 00:17:01.480
So one of the things quantum
chemists want to be able to do

00:17:01.480 --> 00:17:03.580
is predict properties
of molecules.

00:17:03.580 --> 00:17:06.730
You know, will this thing
bind to this other thing?

00:17:06.730 --> 00:17:08.060
Is it toxic?

00:17:08.060 --> 00:17:09.940
What are its quantum properties?

00:17:09.940 --> 00:17:11.890
And the normal way
they do this is they

00:17:11.890 --> 00:17:14.290
have a really computationally
expensive simulator.

00:17:14.290 --> 00:17:17.290
And you plug in this
molecule configuration.

00:17:17.290 --> 00:17:18.730
You wait about an hour.

00:17:18.730 --> 00:17:21.530
And at the end of that you get
the output, which says, OK,

00:17:21.530 --> 00:17:25.000
here are the things
the simulator told you.

00:17:25.000 --> 00:17:28.569
So it turns out-- and
it's a slow process.

00:17:28.569 --> 00:17:31.750
You can't consider that
many different molecules

00:17:31.750 --> 00:17:33.520
like you might like to.

00:17:33.520 --> 00:17:35.380
It turns out you can
use the simulator

00:17:35.380 --> 00:17:38.140
as a teacher for a neural net.

00:17:38.140 --> 00:17:39.700
So you can do that.

00:17:39.700 --> 00:17:41.890
And then all of a sudden
you have a neural net

00:17:41.890 --> 00:17:45.250
that can basically learn to
do what the simulator can

00:17:45.250 --> 00:17:47.740
do but way faster.

00:17:47.740 --> 00:17:49.780
And so now you
have something that

00:17:49.780 --> 00:17:51.520
is about 300,000 times faster.

00:17:51.520 --> 00:17:54.340
And you can't
distinguish the accuracy

00:17:54.340 --> 00:17:57.640
of the output of the neural
net versus the simulator.

00:17:57.640 --> 00:18:00.403
And so that's a completely
game changing thing

00:18:00.403 --> 00:18:01.570
if you're a quantum chemist.

00:18:01.570 --> 00:18:05.320
All of a sudden your tool
is sped up by 300,000 times.

00:18:05.320 --> 00:18:07.103
And all of a sudden
that means you

00:18:07.103 --> 00:18:08.770
can do a very different
kind of science.

00:18:08.770 --> 00:18:10.720
You can say, oh, while
I'm going to lunch

00:18:10.720 --> 00:18:12.950
I should probably screen
100 million molecules.

00:18:12.950 --> 00:18:15.010
And when I come
back, I'll have 1,000

00:18:15.010 --> 00:18:17.860
that might be interesting.

00:18:17.860 --> 00:18:20.615
So that's a pretty
interesting trend.

00:18:20.615 --> 00:18:22.240
And I think it's one
that will play out

00:18:22.240 --> 00:18:24.550
in lots and lots of
different scientific fields

00:18:24.550 --> 00:18:26.860
or engineering fields
where you have this really

00:18:26.860 --> 00:18:28.900
expensive simulator
but you can actually

00:18:28.900 --> 00:18:32.320
learn to approximate it with
a much cheaper neural net

00:18:32.320 --> 00:18:34.810
or machine learning
based model and get

00:18:34.810 --> 00:18:38.320
a simulator that's much faster.

00:18:38.320 --> 00:18:39.190
OK.

00:18:39.190 --> 00:18:41.750
Engineer the tools of
scientific discovery.

00:18:41.750 --> 00:18:43.810
I have a feeling this
14th one was just

00:18:43.810 --> 00:18:48.430
kind of a vague catch all thing
that the panel of experts that

00:18:48.430 --> 00:18:52.168
was convened decided should do.

00:18:52.168 --> 00:18:54.460
But it's pretty clear that
if machine learning is going

00:18:54.460 --> 00:18:59.440
to be a big part of scientific
discovery and engineering,

00:18:59.440 --> 00:19:02.680
we want good tools to express
machine learning algorithms.

00:19:02.680 --> 00:19:05.260
And so that's the
motivation for why

00:19:05.260 --> 00:19:07.660
we created TensorFlow is we
wanted to be to have tools

00:19:07.660 --> 00:19:10.570
that we could use to express
our own machine learning ideas

00:19:10.570 --> 00:19:12.670
and share them with
the rest of the world,

00:19:12.670 --> 00:19:16.930
and have other researchers
exchange machine learning ideas

00:19:16.930 --> 00:19:22.180
and put machine learning models
into practice in products

00:19:22.180 --> 00:19:24.050
and other environments.

00:19:24.050 --> 00:19:27.880
And so we released
this at the end of 2015

00:19:27.880 --> 00:19:30.100
with this Apache 2.0 license.

00:19:30.100 --> 00:19:33.760
And basically it has this
graph based computational model

00:19:33.760 --> 00:19:38.650
that you can then optimize with
a bunch of traditional compiler

00:19:38.650 --> 00:19:42.400
optimizations and it
then can be mapped

00:19:42.400 --> 00:19:43.912
onto a variety of
different devices.

00:19:43.912 --> 00:19:45.370
So you can run the
same computation

00:19:45.370 --> 00:19:49.150
on CPUs or GPUs or our TPUs
that I'll tell you about in

00:19:49.150 --> 00:19:50.170
a minute.

00:19:50.170 --> 00:19:53.750
Eager Mode makes this graph
implicit rather than explicit,

00:19:53.750 --> 00:19:57.790
which is coming
in TensorFlow 2.0.

00:19:57.790 --> 00:20:00.250
And the community
seems to have adopted

00:20:00.250 --> 00:20:01.820
TensorFlow reasonably well.

00:20:01.820 --> 00:20:04.750
And we've been excited by
all the different things

00:20:04.750 --> 00:20:07.780
that we've seen other
people do, both in terms

00:20:07.780 --> 00:20:10.510
of contributing to the
core TensorFlow system

00:20:10.510 --> 00:20:14.480
but also making use of it
to do interesting things.

00:20:14.480 --> 00:20:20.120
And so it's got some pretty
good engagement kinds of stats.

00:20:20.120 --> 00:20:23.200
50 million downloads for a
fairly obscure programming

00:20:23.200 --> 00:20:26.500
packages is a fair
number that seems

00:20:26.500 --> 00:20:29.170
like a good mark of traction.

00:20:31.457 --> 00:20:32.790
And we've seen people do things.

00:20:32.790 --> 00:20:34.170
So I mentioned this in
the keynote yesterday.

00:20:34.170 --> 00:20:34.860
I like this one.

00:20:34.860 --> 00:20:37.650
It's basically a company
building fitness center

00:20:37.650 --> 00:20:40.770
for cows so you can tell
which of your 100 dairy cows

00:20:40.770 --> 00:20:43.570
is behaving a little
strangely today.

00:20:43.570 --> 00:20:47.190
There is a research team at
Penn State and the International

00:20:47.190 --> 00:20:49.800
Institute of Tropical
Agriculture in Tanzania

00:20:49.800 --> 00:20:52.500
that is building a machine
learning model that

00:20:52.500 --> 00:20:56.160
can run on device on a phone in
the middle of a cassava field

00:20:56.160 --> 00:20:58.770
without any network
connection to actually detect

00:20:58.770 --> 00:21:00.600
does this cassava
plant have disease

00:21:00.600 --> 00:21:02.280
and how should I treat it.

00:21:02.280 --> 00:21:04.260
I think this is a
good example of how

00:21:04.260 --> 00:21:07.808
we want machine
learning to run in lots

00:21:07.808 --> 00:21:08.850
and lots of environments.

00:21:08.850 --> 00:21:10.753
Lots of places in
the world sometimes

00:21:10.753 --> 00:21:11.670
you have connectivity.

00:21:11.670 --> 00:21:12.503
Sometimes you don't.

00:21:12.503 --> 00:21:15.210
A lot of cases you want
it to run on device.

00:21:15.210 --> 00:21:18.263
And it's really going
to be the future.

00:21:18.263 --> 00:21:20.430
You're going to have machine
learning models running

00:21:20.430 --> 00:21:25.130
on tiny microcontrollers, all
kinds of things like this.

00:21:25.130 --> 00:21:25.960
OK.

00:21:25.960 --> 00:21:29.230
I'm going to use the remaining
time to take you on a tour

00:21:29.230 --> 00:21:32.620
through some researchy projects
and then sketch how they might

00:21:32.620 --> 00:21:35.060
fit together in the future.

00:21:35.060 --> 00:21:39.490
So I believe what we want is
we want bigger machine learning

00:21:39.490 --> 00:21:40.960
models than we have today.

00:21:40.960 --> 00:21:42.460
But in order to
make that practical,

00:21:42.460 --> 00:21:45.040
we want models that
are sparsely activated.

00:21:45.040 --> 00:21:48.680
So think of a giant model, maybe
with 1,000 different pieces.

00:21:48.680 --> 00:21:52.390
But you activate 20 or 30 of
those pieces for any given

00:21:52.390 --> 00:21:56.320
example, rather than the
entire set of 1,000 pieces.

00:21:56.320 --> 00:21:59.830
We know this is a property
that real organisms have

00:21:59.830 --> 00:22:04.180
in their neural systems is
most of their neural capacity

00:22:04.180 --> 00:22:06.700
is not active at
any given point.

00:22:06.700 --> 00:22:11.350
That's partly how they're
so power efficient.

00:22:11.350 --> 00:22:13.440
Right.

00:22:13.440 --> 00:22:16.570
So some work we did a couple
of years ago at this point

00:22:16.570 --> 00:22:21.040
is what we call a sparsely
gated mixture of experts layer.

00:22:21.040 --> 00:22:26.890
And the essential idea is
these pink rectangles here

00:22:26.890 --> 00:22:28.810
are normal neural net layers.

00:22:28.810 --> 00:22:30.770
But between a couple
of neural net layers,

00:22:30.770 --> 00:22:33.220
we're going to insert
another collection

00:22:33.220 --> 00:22:36.992
of tiny little neural
nets that we call experts.

00:22:36.992 --> 00:22:38.950
And we're going to have
a gating network that's

00:22:38.950 --> 00:22:42.220
going to learn to activate
just a few of those.

00:22:42.220 --> 00:22:44.050
It's going to learn
which of those experts

00:22:44.050 --> 00:22:48.040
is most effective for a
particular kind of example.

00:22:48.040 --> 00:22:50.450
And the expert might
have a lot of parameters.

00:22:50.450 --> 00:22:54.650
It might be pretty large
matrix of parameters.

00:22:54.650 --> 00:22:56.710
And we're going to
have a lot of them.

00:22:56.710 --> 00:23:00.230
So we have in total eight
billion-ish parameters.

00:23:00.230 --> 00:23:02.650
But we're going to activate
just a couple of the experts

00:23:02.650 --> 00:23:05.500
on any given example.

00:23:05.500 --> 00:23:08.070
And you can see that when
you learn to route things,

00:23:08.070 --> 00:23:11.280
you try to learn to
use the expert that

00:23:11.280 --> 00:23:13.832
is most effective at
this particular example.

00:23:13.832 --> 00:23:15.540
And when you send it
to multiple experts,

00:23:15.540 --> 00:23:18.150
that gives you a signal to
train the routing network,

00:23:18.150 --> 00:23:21.390
the gating network so that it
can learn that this expert is

00:23:21.390 --> 00:23:24.300
really good when you're
talking about language that

00:23:24.300 --> 00:23:26.790
is about innovation
and researchy things

00:23:26.790 --> 00:23:28.560
like you see on
the left hand side.

00:23:28.560 --> 00:23:31.800
And this center expert
is really good at talking

00:23:31.800 --> 00:23:34.192
about playing a leading
role and central role.

00:23:34.192 --> 00:23:36.150
And the one on the right
is really good at kind

00:23:36.150 --> 00:23:39.498
of quicky adverby things.

00:23:39.498 --> 00:23:42.600
And so they actually do
develop very different kinds

00:23:42.600 --> 00:23:44.580
of expertise.

00:23:44.580 --> 00:23:47.910
And the nice thing
about this is if you

00:23:47.910 --> 00:23:52.540
compare this in a translation
task with the bottom row,

00:23:52.540 --> 00:23:56.070
you can essentially get
a significant improvement

00:23:56.070 --> 00:23:57.210
in translation accuracy.

00:23:57.210 --> 00:23:58.770
That's the blue score there.

00:23:58.770 --> 00:24:02.060
So one blue point improvement
is a pretty significant thing.

00:24:02.060 --> 00:24:05.940
We really look like one
blue point improvements.

00:24:05.940 --> 00:24:09.000
And because it has all
this extra capacity,

00:24:09.000 --> 00:24:12.210
we can actually make the
sizes of the pink layers

00:24:12.210 --> 00:24:15.103
smaller than they were
in the original model.

00:24:15.103 --> 00:24:16.770
And so we can actually
shrink the amount

00:24:16.770 --> 00:24:20.370
of computation used per word
by about a factor of two,

00:24:20.370 --> 00:24:22.350
so 50% cheaper inference.

00:24:22.350 --> 00:24:26.962
And the training time goes
way down because we just

00:24:26.962 --> 00:24:28.170
have all this extra capacity.

00:24:28.170 --> 00:24:31.380
And it's easier to train a
model with a lot of parameters.

00:24:31.380 --> 00:24:33.300
And so we have about
1/10 the training cost

00:24:33.300 --> 00:24:34.560
in terms of GPU days.

00:24:37.800 --> 00:24:38.680
OK.

00:24:38.680 --> 00:24:40.138
We've also been
doing a lot of work

00:24:40.138 --> 00:24:43.272
on AutoML, which is this
idea behind automating some

00:24:43.272 --> 00:24:45.480
of the machine learning
tasks that a machine learning

00:24:45.480 --> 00:24:47.520
researcher or engineer does.

00:24:47.520 --> 00:24:50.190
And the idea behind
AutoML is currently

00:24:50.190 --> 00:24:52.740
you think about solving a
machine learning problem

00:24:52.740 --> 00:24:55.290
where you have some data.

00:24:55.290 --> 00:24:56.610
You have some computation.

00:24:56.610 --> 00:24:58.200
And you have an ML
expert sit down.

00:24:58.200 --> 00:24:59.340
And they do a bunch
of experiments.

00:24:59.340 --> 00:25:00.882
And they kind of
stir it all together

00:25:00.882 --> 00:25:05.190
and run lots of GPU
days worth of effort.

00:25:05.190 --> 00:25:07.030
And you hopefully
get a solution.

00:25:07.030 --> 00:25:09.390
So what if we could
turn this into using

00:25:09.390 --> 00:25:12.240
more computation to replace
some of the experimentation

00:25:12.240 --> 00:25:13.318
that a machine learning--

00:25:13.318 --> 00:25:15.360
someone with a lot of
machine learning experience

00:25:15.360 --> 00:25:18.360
would actually do?

00:25:18.360 --> 00:25:21.060
And one of the decisions that
a machine learning expert makes

00:25:21.060 --> 00:25:24.450
is what architecture, what
neural network structure

00:25:24.450 --> 00:25:25.950
makes sense for this problem.

00:25:25.950 --> 00:25:28.770
You know, should I use a 13
layer model or a nine layer

00:25:28.770 --> 00:25:29.490
model?

00:25:29.490 --> 00:25:31.920
Should it have three by three
or five by five filters?

00:25:31.920 --> 00:25:35.890
Should it have skip
connections or not?

00:25:35.890 --> 00:25:39.450
And so if you're willing to
say let's try to take this

00:25:39.450 --> 00:25:42.450
up a level and do
some meta learning,

00:25:42.450 --> 00:25:46.080
then we can basically have a
model that generates models

00:25:46.080 --> 00:25:49.390
and then try those models on the
problem we actually care about.

00:25:49.390 --> 00:25:53.800
So the basic iteration
of meta learning here

00:25:53.800 --> 00:25:55.800
is we're going to have a
model generating model.

00:25:55.800 --> 00:25:57.390
We're going to
generate 10 models.

00:25:57.390 --> 00:25:59.220
We're going to train
each of those models.

00:25:59.220 --> 00:26:01.137
And we're going to see
how well they each work

00:26:01.137 --> 00:26:02.490
on the problem we care about.

00:26:02.490 --> 00:26:07.560
And we're going to use the loss
or the accuracy of those models

00:26:07.560 --> 00:26:10.110
as a reinforcement learning
signal for the model generating

00:26:10.110 --> 00:26:14.190
model so that we can steer away
from models that didn't seem

00:26:14.190 --> 00:26:16.140
to work very well
and towards models

00:26:16.140 --> 00:26:17.320
that seem to work better.

00:26:17.320 --> 00:26:20.750
And then we just repeat a lot.

00:26:20.750 --> 00:26:24.140
And when we repeat a
lot, we essentially

00:26:24.140 --> 00:26:27.700
get more and more
accurate models over time.

00:26:27.700 --> 00:26:29.590
And it works.

00:26:29.590 --> 00:26:32.200
And it produces models that
are a little strange looking.

00:26:32.200 --> 00:26:34.627
Like they're a little
more unstructured

00:26:34.627 --> 00:26:36.460
than you might think
of a model that a human

00:26:36.460 --> 00:26:37.480
might have designed.

00:26:37.480 --> 00:26:40.990
So here we have all these
crazy skip connections.

00:26:40.990 --> 00:26:43.717
But they're analogous
to some of the ideas

00:26:43.717 --> 00:26:45.550
that machine learning
researchers themselves

00:26:45.550 --> 00:26:46.577
have come up with in.

00:26:46.577 --> 00:26:48.160
For example, the
resonant architecture

00:26:48.160 --> 00:26:51.520
has a more structured
style of skip connection.

00:26:51.520 --> 00:26:53.740
But the basic idea is
you want information

00:26:53.740 --> 00:26:57.670
to be able to flow more directly
from the input to the output

00:26:57.670 --> 00:27:00.400
without going through as many
intermediate computational

00:27:00.400 --> 00:27:01.310
layers.

00:27:01.310 --> 00:27:03.760
And the system seems
to have developed

00:27:03.760 --> 00:27:07.280
that intuition itself.

00:27:07.280 --> 00:27:11.120
And the nice thing is
these models actually

00:27:11.120 --> 00:27:11.850
work pretty well.

00:27:11.850 --> 00:27:15.350
So if you look at
this graph, accuracy

00:27:15.350 --> 00:27:17.960
is on the y-axis for
the ImageNet problem.

00:27:17.960 --> 00:27:22.350
And computational
cost of the models,

00:27:22.350 --> 00:27:25.617
which are represented by
dots here, is on the x-axis.

00:27:25.617 --> 00:27:27.200
So generally, you
see this trend where

00:27:27.200 --> 00:27:29.033
if you have a more
computationally expensive

00:27:29.033 --> 00:27:33.020
model, you generally
get higher accuracy.

00:27:33.020 --> 00:27:34.400
And each of these
black dots here

00:27:34.400 --> 00:27:38.030
is something that was a
significant amount of effort

00:27:38.030 --> 00:27:41.672
by a bunch of top computer
vision researchers or machine

00:27:41.672 --> 00:27:43.130
learning researchers
that then they

00:27:43.130 --> 00:27:46.610
published and advanced the
state of the art at the time.

00:27:46.610 --> 00:27:52.120
And so if you apply AutoML
to this problem, what you see

00:27:52.120 --> 00:27:56.050
is that you actually exceed
the frontier of the hand

00:27:56.050 --> 00:28:00.550
generated models that the
community has come up with.

00:28:00.550 --> 00:28:02.660
And you do this both
at the high end,

00:28:02.660 --> 00:28:05.320
where you care
most about accuracy

00:28:05.320 --> 00:28:08.223
and don't care as much
about computational costs

00:28:08.223 --> 00:28:10.390
so you can get a model
that's slightly more accurate

00:28:10.390 --> 00:28:12.335
with less computational cost.

00:28:12.335 --> 00:28:13.960
And at the low end,
you can get a model

00:28:13.960 --> 00:28:16.120
that's significantly more
accurate for a very small

00:28:16.120 --> 00:28:19.690
amount of computational cost.

00:28:19.690 --> 00:28:22.000
And that, I think, is a
pretty interesting result.

00:28:22.000 --> 00:28:25.990
It says that we should really
let computers and machine

00:28:25.990 --> 00:28:27.460
learning researchers
work together

00:28:27.460 --> 00:28:34.438
to develop the best models
for these kinds of problems.

00:28:34.438 --> 00:28:35.980
And we've turned
this into a product.

00:28:35.980 --> 00:28:39.370
So we have Cloud AutoML
as a Cloud product.

00:28:39.370 --> 00:28:41.650
And you can try that
on your own problem.

00:28:41.650 --> 00:28:44.740
So if you were
maybe a company that

00:28:44.740 --> 00:28:46.900
doesn't have a lot of
machine learning researchers,

00:28:46.900 --> 00:28:48.608
or machine learning
engineers yourselves,

00:28:48.608 --> 00:28:50.775
you can actually just
take a bunch of images in

00:28:50.775 --> 00:28:52.900
and categories of things
you want to do-- maybe you

00:28:52.900 --> 00:28:55.060
have pictures from
your assembly line.

00:28:55.060 --> 00:28:58.720
You want to predict what
part is this image of.

00:28:58.720 --> 00:29:01.640
You can actually get a high
quality model for that.

00:29:01.640 --> 00:29:04.330
And we've extended this to
things more than just vision.

00:29:04.330 --> 00:29:07.300
So you can do videos, and
language, and translation.

00:29:07.300 --> 00:29:10.780
And more recently we've
introduced something

00:29:10.780 --> 00:29:13.030
that allows you to
predict relational data

00:29:13.030 --> 00:29:15.370
from other relational data.

00:29:15.370 --> 00:29:17.980
You want to predict will this
customer buy something given

00:29:17.980 --> 00:29:22.160
their past orders or something.

00:29:22.160 --> 00:29:25.495
We've also obviously continued
research in the AutoML field.

00:29:25.495 --> 00:29:28.790
So we've got some work looking
at the use of evolution

00:29:28.790 --> 00:29:31.490
rather than reinforcement
learning for the search,

00:29:31.490 --> 00:29:33.410
learning the
optimization update rule,

00:29:33.410 --> 00:29:35.720
learning the nonlinearity
function rather than just

00:29:35.720 --> 00:29:38.030
assuming we should
use [INAUDIBLE]

00:29:38.030 --> 00:29:42.320
or some other kind of
activation function.

00:29:42.320 --> 00:29:44.510
We've actually got some
work on incorporating

00:29:44.510 --> 00:29:47.420
both inference latency
and the accuracy.

00:29:47.420 --> 00:29:50.300
Let's say you want a
really good model that has

00:29:50.300 --> 00:29:51.780
to run in seven milliseconds.

00:29:51.780 --> 00:29:54.050
We can find the
most accurate model

00:29:54.050 --> 00:29:58.370
that will run in your time
budget allowed by using a more

00:29:58.370 --> 00:30:00.350
complicated reward function.

00:30:00.350 --> 00:30:04.280
We can learn how to augment
data so that you can stretch

00:30:04.280 --> 00:30:07.100
the amount of label data
you have in interesting ways

00:30:07.100 --> 00:30:09.813
more effectively than
handwritten data augmentation.

00:30:09.813 --> 00:30:11.480
And we can explore
lots of architectures

00:30:11.480 --> 00:30:16.160
to make this whole search
process a bit more efficient.

00:30:16.160 --> 00:30:16.690
OK.

00:30:16.690 --> 00:30:18.520
But it's clear if we're going
to try these approaches,

00:30:18.520 --> 00:30:20.650
we're going to need more
computational power.

00:30:20.650 --> 00:30:23.740
And I think one of the
truisms of machine learning

00:30:23.740 --> 00:30:27.010
over the last decade or
so is more computational

00:30:27.010 --> 00:30:29.650
power tends to
get better results

00:30:29.650 --> 00:30:31.270
when you have enough data.

00:30:31.270 --> 00:30:33.700
And so it's really
nice that deep learning

00:30:33.700 --> 00:30:36.340
is this really
broadly useful tool

00:30:36.340 --> 00:30:38.500
across so many different
problem domains,

00:30:38.500 --> 00:30:41.140
because that means you can start
to think about specializing

00:30:41.140 --> 00:30:43.740
hardware for deep
learning but have

00:30:43.740 --> 00:30:46.160
it apply to many, many things.

00:30:46.160 --> 00:30:49.120
And so there are two properties
that deep learning algorithms

00:30:49.120 --> 00:30:49.780
tend to have.

00:30:49.780 --> 00:30:53.720
One is they're very tolerant
of reduced precision.

00:30:53.720 --> 00:30:58.450
So if you do calculations to
one decimal digit of precision,

00:30:58.450 --> 00:31:00.790
that's perfectly fine with
most of these algorithms.

00:31:00.790 --> 00:31:03.820
You don't need six or
seven digits of precision.

00:31:03.820 --> 00:31:06.640
And the other thing
is that they are all--

00:31:06.640 --> 00:31:08.680
all these algorithms I've
shown you are made up

00:31:08.680 --> 00:31:11.530
of a handful of specific
operations, things like matrix

00:31:11.530 --> 00:31:14.860
multiplies, vector dot
products, essentially

00:31:14.860 --> 00:31:16.430
dense linear algebra.

00:31:16.430 --> 00:31:19.600
So if you can build
machines, computers,

00:31:19.600 --> 00:31:24.130
that are really good at reduced
precision dense linear algebra,

00:31:24.130 --> 00:31:26.410
then you can accelerate lots
of these machine learning

00:31:26.410 --> 00:31:29.560
algorithms quite a lot compared
to more general purpose

00:31:29.560 --> 00:31:32.110
computers that have
general purpose CPUs that

00:31:32.110 --> 00:31:34.030
can run all kinds
of things or even

00:31:34.030 --> 00:31:37.780
GPUs which tend to be somewhat
good at this but tend to have,

00:31:37.780 --> 00:31:39.970
for example, higher precision
than you might want.

00:31:43.230 --> 00:31:45.290
So we started to
think about building

00:31:45.290 --> 00:31:47.510
specialized hardware when
I did this kind of thought

00:31:47.510 --> 00:31:48.890
exercise in 2012.

00:31:48.890 --> 00:31:51.170
We were starting to
see the initial success

00:31:51.170 --> 00:31:53.420
of deep neural nets
for speech recognition

00:31:53.420 --> 00:31:55.640
and for image
recognition and starting

00:31:55.640 --> 00:31:57.230
to think about how
would we deploy

00:31:57.230 --> 00:31:58.940
these in some of our products.

00:31:58.940 --> 00:32:00.710
And so there was
this scary moment

00:32:00.710 --> 00:32:04.352
where we realized that if speech
started to work really well,

00:32:04.352 --> 00:32:05.810
and at that time
we couldn't run it

00:32:05.810 --> 00:32:07.310
on device because
the devices didn't

00:32:07.310 --> 00:32:08.900
have enough
computational power, what

00:32:08.900 --> 00:32:11.300
if 100 million users started
talking to their phones

00:32:11.300 --> 00:32:14.660
for three minutes a day, which
is not implausible if speech

00:32:14.660 --> 00:32:16.788
starts to work a lot better.

00:32:16.788 --> 00:32:18.830
And if we were running
the speech models on CPUs,

00:32:18.830 --> 00:32:21.163
we need to double the number
of computers in Google data

00:32:21.163 --> 00:32:24.730
centers, which is slightly
terrifying to launch

00:32:24.730 --> 00:32:28.070
one feature in one product.

00:32:28.070 --> 00:32:31.130
And so we started to think
about building these specialized

00:32:31.130 --> 00:32:36.050
processors for the deep learning
algorithms we wanted to run

00:32:36.050 --> 00:32:39.350
and TPU V1 has been
in production use

00:32:39.350 --> 00:32:42.440
since 2015 was really the
outcome of that thought

00:32:42.440 --> 00:32:43.490
exercise.

00:32:43.490 --> 00:32:46.550
And it's in production use
based on every query you do,

00:32:46.550 --> 00:32:50.090
on every translation you
do, speech processing, image

00:32:50.090 --> 00:32:53.980
crossing, AlphaGo use
a collection of these.

00:32:53.980 --> 00:32:56.840
This is the actual racks
of machines that were

00:32:56.840 --> 00:32:58.118
competed in the AlphaGo match.

00:32:58.118 --> 00:32:59.660
You can see the
little Go board we've

00:32:59.660 --> 00:33:03.580
commemorated with on the side.

00:33:03.580 --> 00:33:07.005
And then we started to tackle
the bigger problem of not just

00:33:07.005 --> 00:33:09.130
inference, which is we
already have a trained model

00:33:09.130 --> 00:33:11.505
and you just want to apply
it, but how do you actually do

00:33:11.505 --> 00:33:16.120
training in an accelerated way.

00:33:16.120 --> 00:33:19.960
And so the second
version of TPUs

00:33:19.960 --> 00:33:21.910
are for training and inference.

00:33:21.910 --> 00:33:25.160
And that's one of
the TPU devices,

00:33:25.160 --> 00:33:26.980
which has four chips on it.

00:33:26.980 --> 00:33:30.100
This is TPU V3, which
also has four chips on it.

00:33:30.100 --> 00:33:32.500
It's got water cooling.

00:33:32.500 --> 00:33:35.750
So it's slightly scary to
have water in your computers,

00:33:35.750 --> 00:33:39.030
but we do.

00:33:39.030 --> 00:33:41.328
And then we designed
these systems

00:33:41.328 --> 00:33:43.620
to be configured together
into larger configurations we

00:33:43.620 --> 00:33:44.730
call pods.

00:33:44.730 --> 00:33:47.280
So this is a TPU V2 pod.

00:33:47.280 --> 00:33:52.050
This is a bigger TPU V3
pod with water cooling.

00:33:52.050 --> 00:33:54.780
You can actually see one of the
racks of this in the machine

00:33:54.780 --> 00:33:58.570
learning dome.

00:33:58.570 --> 00:34:02.400
And really these things
actually do provide

00:34:02.400 --> 00:34:03.660
a lot of computational power.

00:34:03.660 --> 00:34:06.480
Individual devices
with the four chips

00:34:06.480 --> 00:34:10.949
are up to 420 teraflops have
a fair amount of memory.

00:34:10.949 --> 00:34:16.350
And then the actual
pods themselves are

00:34:16.350 --> 00:34:17.972
up to 100 petaflops of compute.

00:34:17.972 --> 00:34:19.889
This is a pretty substantial
amount of compute

00:34:19.889 --> 00:34:23.400
and really lets you
very quickly try machine

00:34:23.400 --> 00:34:26.610
learning research experiments,
train very large production

00:34:26.610 --> 00:34:29.340
models on large data
sets, and these are also

00:34:29.340 --> 00:34:32.670
now available through
our cloud products.

00:34:32.670 --> 00:34:35.862
As of yesterday, I think we
announced them to be in beta.

00:34:35.862 --> 00:34:37.320
One of the keys to
performance here

00:34:37.320 --> 00:34:40.440
is the network interconnect
between the chips in the pods

00:34:40.440 --> 00:34:43.830
is actually your
super high speed 2D

00:34:43.830 --> 00:34:46.350
mesh with wrap around links.

00:34:46.350 --> 00:34:48.389
That's why it's toroidal.

00:34:48.389 --> 00:34:50.850
And that means you can
essentially program this thing

00:34:50.850 --> 00:34:52.500
as if it's a single computer.

00:34:52.500 --> 00:34:54.120
And the software
underneath the covers

00:34:54.120 --> 00:34:58.830
takes care of distributing
the computation appropriately

00:34:58.830 --> 00:35:01.190
and can do very fast all
reduced kind of operations

00:35:01.190 --> 00:35:04.140
and broadcast operations.

00:35:04.140 --> 00:35:09.390
And so, for example, you
can use a full TPU V2 pod

00:35:09.390 --> 00:35:17.520
to train ImageNet in 7.9 minutes
versus the same problem using

00:35:17.520 --> 00:35:18.330
eight GPUs.

00:35:18.330 --> 00:35:20.865
You get 27 times faster
training at lower cost.

00:35:25.020 --> 00:35:28.540
The V3 pod is actually
even substantially larger.

00:35:28.540 --> 00:35:30.550
You can train an
ImageNet model in scratch

00:35:30.550 --> 00:35:33.700
in less than two minutes,
more than a million images

00:35:33.700 --> 00:35:37.210
per second in training, which is
essentially the entire ImageNet

00:35:37.210 --> 00:35:39.890
data set every second.

00:35:39.890 --> 00:35:42.040
And you can train very
large BERT language models,

00:35:42.040 --> 00:35:44.560
for example, as I was
discussing on stage

00:35:44.560 --> 00:35:47.230
in the keynote yesterday
in about 76 minutes

00:35:47.230 --> 00:35:50.540
on a fairly large corpus of data
which normally would take days.

00:35:50.540 --> 00:35:52.930
And so that really helps
make our researchers

00:35:52.930 --> 00:35:55.872
and ML production
systems more productive

00:35:55.872 --> 00:35:57.580
by being able to
experiment more quickly.

00:35:57.580 --> 00:36:00.040
If you can run an experiment
in two minutes, that's

00:36:00.040 --> 00:36:02.450
a very different kind of
science and engineering

00:36:02.450 --> 00:36:04.720
you do than if that
same experiment would

00:36:04.720 --> 00:36:05.870
take you a day and a half.

00:36:05.870 --> 00:36:06.370
Right.

00:36:06.370 --> 00:36:08.440
You just think about
running more experiments,

00:36:08.440 --> 00:36:11.190
trying more things.

00:36:11.190 --> 00:36:16.150
And we have lots of
models already available.

00:36:16.150 --> 00:36:16.740
OK.

00:36:16.740 --> 00:36:22.607
So let's take some of
the ideas we talked about

00:36:22.607 --> 00:36:24.440
and think about how
they might fit together.

00:36:24.440 --> 00:36:26.500
So I said we want these
really large models

00:36:26.500 --> 00:36:29.395
but have them be
sparsely activated.

00:36:29.395 --> 00:36:32.020
I think one of the things we're
doing wrong in machine learning

00:36:32.020 --> 00:36:33.930
is we tend to train a
machine learning model

00:36:33.930 --> 00:36:35.678
to do a single thing.

00:36:35.678 --> 00:36:37.220
And then we have a
different problem.

00:36:37.220 --> 00:36:40.355
We tend to train a different
model to do that other thing.

00:36:40.355 --> 00:36:42.730
And I think really we should
be thinking about how can we

00:36:42.730 --> 00:36:45.400
train models that
do many, many things

00:36:45.400 --> 00:36:48.460
and leverage the
expertise that they have

00:36:48.460 --> 00:36:51.460
in doing many things to then
be able to take on a new task

00:36:51.460 --> 00:36:54.130
and learn to do that new task
more quickly and with less

00:36:54.130 --> 00:36:55.330
data.

00:36:55.330 --> 00:36:57.260
This is, essentially,
multi task learning.

00:36:57.260 --> 00:37:00.370
But often multi task
learning in practice today

00:37:00.370 --> 00:37:03.340
means three or four
or five tasks, not

00:37:03.340 --> 00:37:05.020
thousands or millions.

00:37:05.020 --> 00:37:08.620
I think we really want to be
thinking bigger and bolder

00:37:08.620 --> 00:37:12.340
about really doing in the limit
one model for all of the things

00:37:12.340 --> 00:37:15.040
we care about.

00:37:15.040 --> 00:37:16.930
And obviously,
we're going to try

00:37:16.930 --> 00:37:21.260
to train this large model
using fancy ML hardware.

00:37:21.260 --> 00:37:21.760
OK.

00:37:21.760 --> 00:37:22.600
So how might this look?

00:37:22.600 --> 00:37:24.100
So I imagine we've
trained a model

00:37:24.100 --> 00:37:25.660
on a bunch of different tasks.

00:37:25.660 --> 00:37:27.770
And it's learned these
different components,

00:37:27.770 --> 00:37:30.430
which can be sometimes shared
across different tasks,

00:37:30.430 --> 00:37:32.860
sometimes independent,
specialized

00:37:32.860 --> 00:37:34.180
for a particular task.

00:37:34.180 --> 00:37:36.560
And now a new task comes along.

00:37:36.560 --> 00:37:40.190
So with the AutoML style
reinforcement learning,

00:37:40.190 --> 00:37:44.710
we should be able to use an
RL logarithm to find pathways

00:37:44.710 --> 00:37:46.960
through this model
that actually get us

00:37:46.960 --> 00:37:49.128
into a pretty good
state for that new task,

00:37:49.128 --> 00:37:51.670
because it hopefully has some
commonalities with other things

00:37:51.670 --> 00:37:54.000
we've already learned.

00:37:54.000 --> 00:37:58.750
And then we might have some way
to add capacity to the system

00:37:58.750 --> 00:38:01.840
so that for a task where we
really care about accuracy,

00:38:01.840 --> 00:38:05.120
we can add a bit of capacity and
start to use that for this task

00:38:05.120 --> 00:38:09.280
and have that pathway be more
specialized for that task

00:38:09.280 --> 00:38:11.462
and therefore hopefully
more accurate.

00:38:11.462 --> 00:38:13.670
And I think that's an
interesting direction to go in.

00:38:13.670 --> 00:38:16.780
How can we think more about
building a system like that

00:38:16.780 --> 00:38:20.230
than the current kind of
models we have today where

00:38:20.230 --> 00:38:23.560
we tend to fully activate the
entire model for every example

00:38:23.560 --> 00:38:27.740
and tend to have them
just for a single task?

00:38:27.740 --> 00:38:28.250
OK.

00:38:28.250 --> 00:38:34.628
I want to close on how we should
be thinking about using machine

00:38:34.628 --> 00:38:36.170
learning and all
the different places

00:38:36.170 --> 00:38:38.240
that we might consider using it.

00:38:38.240 --> 00:38:40.010
And I think one of
the things that I'm

00:38:40.010 --> 00:38:42.950
really proud of as a company
is that last year we published

00:38:42.950 --> 00:38:45.260
a set of principles
by which we think

00:38:45.260 --> 00:38:47.780
about how we're going
to use machine learning

00:38:47.780 --> 00:38:48.840
for different things.

00:38:48.840 --> 00:38:51.687
And I think these
seven things when

00:38:51.687 --> 00:38:54.020
we look at using machine
learning in any of our products

00:38:54.020 --> 00:38:58.430
or settings we think carefully
about how are we actually

00:38:58.430 --> 00:39:01.040
fulfilling these
principles by using

00:39:01.040 --> 00:39:02.910
machine learning in this way.

00:39:02.910 --> 00:39:06.237
And I think there's more on
the actual principles website

00:39:06.237 --> 00:39:08.570
that you can go find, but I
think this is really, really

00:39:08.570 --> 00:39:09.070
important.

00:39:09.070 --> 00:39:11.120
And I'll point out that
some of these things

00:39:11.120 --> 00:39:15.333
are evolving research
areas as well as principles

00:39:15.333 --> 00:39:16.250
that we want to apply.

00:39:16.250 --> 00:39:20.210
So for example, number two,
avoid creating or reinforcing

00:39:20.210 --> 00:39:21.060
unfair bias.

00:39:21.060 --> 00:39:22.760
And bias in machine
learning models

00:39:22.760 --> 00:39:25.880
is a very real problem that you
get from a variety of sources.

00:39:25.880 --> 00:39:27.658
Could be you have
biased training data.

00:39:27.658 --> 00:39:29.450
Could be you're training
on real world data

00:39:29.450 --> 00:39:31.610
and the world does
itself is biased

00:39:31.610 --> 00:39:34.470
in ways that we don't want.

00:39:34.470 --> 00:39:38.900
And so there is research that
we can apply and extend in

00:39:38.900 --> 00:39:41.120
how do we reduce
bias or eliminate it

00:39:41.120 --> 00:39:43.085
from machine learning models.

00:39:43.085 --> 00:39:44.960
And so this is an example
of some of the work

00:39:44.960 --> 00:39:49.140
we've been doing on
bias and fairness.

00:39:49.140 --> 00:39:52.490
But what we try to do
in our use of ML models

00:39:52.490 --> 00:39:55.160
is apply the best
known practices

00:39:55.160 --> 00:39:57.470
for our actual
production use but also

00:39:57.470 --> 00:40:01.250
advance the state of the art in
understanding bias and fairness

00:40:01.250 --> 00:40:03.510
and making it better.

00:40:03.510 --> 00:40:07.340
And so with that, in conclusion,
deep neural nets and machine

00:40:07.340 --> 00:40:09.440
learning are really
tackling some of the world's

00:40:09.440 --> 00:40:10.610
great challenges I think.

00:40:10.610 --> 00:40:13.568
I think we're really making
progress in a number of areas.

00:40:13.568 --> 00:40:15.110
There's a lot of
interesting problems

00:40:15.110 --> 00:40:17.660
to tackle and to still work on.

00:40:17.660 --> 00:40:20.150
And they're going to affect
not just computer science.

00:40:20.150 --> 00:40:20.650
Right.

00:40:20.650 --> 00:40:23.690
We're affecting many, many
aspects of human endeavor

00:40:23.690 --> 00:40:27.270
like medicine, science,
other kinds of things.

00:40:27.270 --> 00:40:29.120
And so I think it's a
great responsibility

00:40:29.120 --> 00:40:31.700
that we have to make sure
that we do these things right

00:40:31.700 --> 00:40:35.030
and to continue to push
for the state of the art

00:40:35.030 --> 00:40:36.540
and apply it to great things.

00:40:36.540 --> 00:40:38.090
So thank you very much.

00:40:38.090 --> 00:40:41.440
[MUSIC PLAYING]

