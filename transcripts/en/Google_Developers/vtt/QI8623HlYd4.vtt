WEBVTT
Kind: captions
Language: en

00:00:03.094 --> 00:00:04.810
RYAN BOYD: Hello, everyone.

00:00:04.810 --> 00:00:07.390
Hope you're all here today for
the Crunching Big Data with

00:00:07.390 --> 00:00:09.380
BigQuery session.

00:00:09.380 --> 00:00:11.590
It's quite early in the morning,
yet we still managed

00:00:11.590 --> 00:00:12.610
to get a decent crowd.

00:00:12.610 --> 00:00:13.820
That's exciting.

00:00:13.820 --> 00:00:16.470
For those of you who have
colleagues that didn't make

00:00:16.470 --> 00:00:19.060
it, you can encourage them to
watch the YouTube video

00:00:19.060 --> 00:00:20.110
afterwards.

00:00:20.110 --> 00:00:21.730
Hopefully, you'll like
the session.

00:00:21.730 --> 00:00:23.170
My name is Ryan Boyd.

00:00:23.170 --> 00:00:25.030
I'm a developer advocate
here at Google.

00:00:25.030 --> 00:00:26.870
And I'm here with
Jordan Tigani.

00:00:26.870 --> 00:00:31.320
And Jordan is a software
engineer on our BigQuery team.

00:00:31.320 --> 00:00:32.140
So thanks, Jordan.

00:00:32.140 --> 00:00:34.340
I'm going to give the
introduction here.

00:00:34.340 --> 00:00:37.130
And then Jordan's going to come
back later and talk about

00:00:37.130 --> 00:00:39.000
the exciting stuff.

00:00:39.000 --> 00:00:42.300
All right, so the first question
that you might have

00:00:42.300 --> 00:00:48.740
when looking at this slide title
is, what is big data?

00:00:48.740 --> 00:00:51.770
Big data is a term that's
often used to represent

00:00:51.770 --> 00:00:57.260
different things in the tech
press or in industry.

00:00:57.260 --> 00:00:59.670
And for our session here today,
we really want to

00:00:59.670 --> 00:01:01.820
define what it means.

00:01:01.820 --> 00:01:07.850
So the question I have for you
today is how big is big?

00:01:07.850 --> 00:01:11.690
How many of you think one
million rows is big data?

00:01:11.690 --> 00:01:12.900
Is that big for you?

00:01:12.900 --> 00:01:14.870
All right, a handful.

00:01:14.870 --> 00:01:17.350
10 million?

00:01:17.350 --> 00:01:17.970
All right.

00:01:17.970 --> 00:01:19.660
All right.

00:01:19.660 --> 00:01:22.280
100 million rows?

00:01:22.280 --> 00:01:24.080
All right, we actually have
most of the audience there

00:01:24.080 --> 00:01:25.610
with 100 million rows.

00:01:25.610 --> 00:01:28.530
For us, 100 million rows is
kind of getting there.

00:01:28.530 --> 00:01:31.890
But 500 million rows, come on,
give us some applause for 500

00:01:31.890 --> 00:01:32.770
million rows of data.

00:01:32.770 --> 00:01:33.221
[APPLAUSE]

00:01:33.221 --> 00:01:37.270
RYAN BOYD: All right,
so 500 million rows.

00:01:37.270 --> 00:01:40.180
BigQuery is really optimized
for hundreds of millions of

00:01:40.180 --> 00:01:43.320
rows to billions of
rows of data.

00:01:43.320 --> 00:01:44.570
It's a lot of data.

00:01:44.570 --> 00:01:47.890
And you might wonder, what
does Google do with such

00:01:47.890 --> 00:01:50.400
massive amounts of data?

00:01:50.400 --> 00:01:52.650
Big data at Google, it's
really important.

00:01:52.650 --> 00:01:55.700
And we've been dealing with big
data in this space or even

00:01:55.700 --> 00:01:59.380
much larger, pretty much,
since we started.

00:01:59.380 --> 00:02:02.030
A lot of companies haven't
really got into

00:02:02.030 --> 00:02:03.850
that size back then.

00:02:03.850 --> 00:02:07.030
But if you look at some of our
numbers here, we have 60 hours

00:02:07.030 --> 00:02:11.110
of video uploaded to YouTube
every minute.

00:02:11.110 --> 00:02:14.200
We have 100 million gigabytes
of data in the

00:02:14.200 --> 00:02:15.720
Google search index.

00:02:15.720 --> 00:02:17.280
And that was in 2010.

00:02:17.280 --> 00:02:18.740
We haven't had an updated
stat here.

00:02:18.740 --> 00:02:21.580
But in 2010, there were
100 million gigabytes.

00:02:21.580 --> 00:02:24.410
And then, apparently,
now there are 425

00:02:24.410 --> 00:02:26.500
million users of Gmail.

00:02:26.500 --> 00:02:31.230
I just had to update this number
just now from 350.

00:02:31.230 --> 00:02:35.680
So Google has services that
generate massive amounts of

00:02:35.680 --> 00:02:39.930
data, that require processing
and working with massive

00:02:39.930 --> 00:02:42.920
amounts of data.

00:02:42.920 --> 00:02:45.840
Now, it used to be that
companies that wanted to work

00:02:45.840 --> 00:02:49.130
with data would load it
on a database server,

00:02:49.130 --> 00:02:51.020
something like this.

00:02:51.020 --> 00:02:53.520
They would load all their data
on that database server.

00:02:53.520 --> 00:02:57.120
And in order to process more
data, they would increase the

00:02:57.120 --> 00:02:59.090
memory, add a few CPUs.

00:02:59.090 --> 00:03:01.440
And it would get costly, but
it was all kind of stuck in

00:03:01.440 --> 00:03:03.560
that single box.

00:03:03.560 --> 00:03:05.820
Google really looked at this
and said, you know what, we

00:03:05.820 --> 00:03:08.190
don't just have one server.

00:03:08.190 --> 00:03:10.960
We have dozens, hundreds,
thousands of

00:03:10.960 --> 00:03:12.410
servers around the world.

00:03:12.410 --> 00:03:16.010
And we can use all of those
servers in order to process

00:03:16.010 --> 00:03:18.820
our massive amounts of data.

00:03:18.820 --> 00:03:23.280
But we needed the software that
enabled us to do that.

00:03:23.280 --> 00:03:26.680
We needed software that we could
run very quick queries,

00:03:26.680 --> 00:03:30.330
to process the data generated by
our products, to iterate on

00:03:30.330 --> 00:03:33.970
those products and make them
better for our users.

00:03:33.970 --> 00:03:36.790
And most of the software for
distributed computing on

00:03:36.790 --> 00:03:41.910
large-scale environments like
this would handle the queries

00:03:41.910 --> 00:03:44.300
that you wanted to do, but would
give you the results in

00:03:44.300 --> 00:03:47.900
a matter of minutes or in a
matter of hours, not in a

00:03:47.900 --> 00:03:48.890
matter of seconds.

00:03:48.890 --> 00:03:52.680
And we really wanted that ad
hoc, iterative analysis to

00:03:52.680 --> 00:03:54.660
improve our products.

00:03:54.660 --> 00:03:56.260
So Google invented our internal

00:03:56.260 --> 00:03:58.960
technology called Dremel.

00:03:58.960 --> 00:04:02.400
Dremel is really a technology
that allows you to do SQL-like

00:04:02.400 --> 00:04:07.090
queries on massive amounts of
data, hundreds of millions,

00:04:07.090 --> 00:04:09.570
billions of rows, gigabytes,
terabytes,

00:04:09.570 --> 00:04:11.320
large amounts of data.

00:04:11.320 --> 00:04:15.590
You could do SQL-like queries
and get results quickly.

00:04:15.590 --> 00:04:19.200
So what types of queries
do we use Dremel for?

00:04:19.200 --> 00:04:22.230
Google uses Dremel for all sorts
of different things.

00:04:22.230 --> 00:04:26.040
Google uses Dremel everywhere
from understanding our

00:04:26.040 --> 00:04:29.860
products and our usage of
products to building things,

00:04:29.860 --> 00:04:33.820
products like Google Books,
and in our Ads

00:04:33.820 --> 00:04:35.120
environment as well.

00:04:35.120 --> 00:04:38.330
So let's give you
some examples.

00:04:38.330 --> 00:04:41.310
Here, the first one is going
to be finding top installed

00:04:41.310 --> 00:04:42.530
market applications.

00:04:42.530 --> 00:04:45.090
Whether this is the Google
Apps Marketplace, which I

00:04:45.090 --> 00:04:49.440
worked on for a few years, or
the Google Play Store, or what

00:04:49.440 --> 00:04:52.490
have you, this is the type of
query that we would do.

00:04:52.490 --> 00:04:57.460
We basically say, give me the
top 20 apps and the count of

00:04:57.460 --> 00:04:59.400
installs from an install log.

00:04:59.400 --> 00:05:03.160
In this case, for 2012, we shard
our install log so we

00:05:03.160 --> 00:05:06.020
can do all of 2012
in this query.

00:05:06.020 --> 00:05:07.430
And we can get a result
on something

00:05:07.430 --> 00:05:08.870
like that in 20 seconds.

00:05:08.870 --> 00:05:12.230
And that literally is scanning
through every single row of

00:05:12.230 --> 00:05:15.900
this install log and
getting us the

00:05:15.900 --> 00:05:18.370
result in about 20 seconds.

00:05:18.370 --> 00:05:22.430
But all of these products
need servers to run on.

00:05:22.430 --> 00:05:25.060
And these servers sometimes
have issues.

00:05:25.060 --> 00:05:27.590
And we need to monitor the
status of these servers.

00:05:27.590 --> 00:05:30.460
So we also have an operations
team at Google, of course.

00:05:30.460 --> 00:05:34.100
And this operations team still
uses the same underlying

00:05:34.100 --> 00:05:38.980
Dremel technology to do things
like find slow servers.

00:05:38.980 --> 00:05:43.970
So here is the query that we can
do to find slow servers.

00:05:43.970 --> 00:05:48.170
All we're doing here is finding
the source machine and

00:05:48.170 --> 00:05:50.360
the number of requests that
that source machine is

00:05:50.360 --> 00:05:54.320
processing that are taking
greater than four seconds,

00:05:54.320 --> 00:05:55.960
grouping that, ordering it.

00:05:55.960 --> 00:06:00.610
And again, we have our result
in about 20 seconds.

00:06:00.610 --> 00:06:03.330
This is really powerful for us,
because it allows us to

00:06:03.330 --> 00:06:05.540
really drill down and
get the answers to

00:06:05.540 --> 00:06:06.840
our questions quickly.

00:06:09.410 --> 00:06:11.150
All right, BigQuery.

00:06:11.150 --> 00:06:14.390
BigQuery is Google's
externalization of the Dremel

00:06:14.390 --> 00:06:15.050
technology.

00:06:15.050 --> 00:06:18.820
BigQuery gives you the power
to do this as well.

00:06:18.820 --> 00:06:21.370
It gives you the power to
store your data with the

00:06:21.370 --> 00:06:25.180
reliability, redundancy, and
consistency that Google

00:06:25.180 --> 00:06:28.050
expects of its own data.

00:06:28.050 --> 00:06:30.740
It also gives you the power to
go from data to meaning at

00:06:30.740 --> 00:06:32.560
scale, quickly.

00:06:32.560 --> 00:06:36.040
And that's what this
is all about.

00:06:36.040 --> 00:06:39.320
So developers are using it for
as wide a range of different

00:06:39.320 --> 00:06:42.610
things as Google uses
it ourselves.

00:06:42.610 --> 00:06:44.850
Developers are using it for
game and social media

00:06:44.850 --> 00:06:47.450
analytics, infrastructure
monitoring, advertising

00:06:47.450 --> 00:06:51.960
campaign optimization, and
sensor data analysis.

00:06:51.960 --> 00:06:54.930
A whole wide range of different
types of companies,

00:06:54.930 --> 00:06:59.000
different types of data, but
BigQuery really allows them to

00:06:59.000 --> 00:07:01.990
analyze their data quickly and
iterate on it and answer

00:07:01.990 --> 00:07:05.320
questions that they weren't able
to answer before, because

00:07:05.320 --> 00:07:09.520
they simply didn't have the
staff or the resources to deal

00:07:09.520 --> 00:07:12.810
with data at that scale.

00:07:12.810 --> 00:07:14.520
So our agenda here today,
I'm going to show you

00:07:14.520 --> 00:07:15.580
the power of BigQuery.

00:07:15.580 --> 00:07:17.440
I showed you a little
bit already.

00:07:17.440 --> 00:07:19.480
We're going to actually
show it to you live on

00:07:19.480 --> 00:07:21.690
some public data sets.

00:07:21.690 --> 00:07:24.360
Then I'll talk about loading
your data into BigQuery,

00:07:24.360 --> 00:07:26.240
running your queries.

00:07:26.240 --> 00:07:28.450
And then Jordan is going to
come up and give you the

00:07:28.450 --> 00:07:32.720
underlying architecture design
of how BigQuery works.

00:07:32.720 --> 00:07:35.480
We really hope that, by diving
underneath the covers of

00:07:35.480 --> 00:07:38.640
BigQuery and showing you how
it works, that it will give

00:07:38.640 --> 00:07:41.940
you the intuition that you need
to understand how to form

00:07:41.940 --> 00:07:44.240
your queries and get
the answers to the

00:07:44.240 --> 00:07:46.620
questions that you have.

00:07:46.620 --> 00:07:48.790
And for that, Jordan is also
going to give you some

00:07:48.790 --> 00:07:52.130
advanced queries on, again,
some public data sets, the

00:07:52.130 --> 00:07:53.780
GitHub data.

00:07:53.780 --> 00:07:56.760
So one of the public data sets
we have is a bunch of the

00:07:56.760 --> 00:08:00.640
GitHub commits from the GitHub
source repository.

00:08:00.640 --> 00:08:02.880
And so Jordan's going to dive
into that and show you how to

00:08:02.880 --> 00:08:06.700
do things like 7-day
active users.

00:08:06.700 --> 00:08:09.160
So let's dive in.

00:08:09.160 --> 00:08:11.930
BigQuery, at its heart,
is an API.

00:08:11.930 --> 00:08:15.750
It's a developer interface
for you developers to

00:08:15.750 --> 00:08:17.840
analyze your data.

00:08:17.840 --> 00:08:20.410
But on top of that developer
interface, we've actually

00:08:20.410 --> 00:08:22.080
built a UI as well.

00:08:22.080 --> 00:08:25.700
And you can all access that at
bigquery.cloud.google.com.

00:08:25.700 --> 00:08:27.030
You could do that now,
if you're sick of

00:08:27.030 --> 00:08:27.910
listening to me--

00:08:27.910 --> 00:08:29.880
hopefully, not--

00:08:29.880 --> 00:08:30.960
or later.

00:08:30.960 --> 00:08:33.320
But let me show you
some queries in

00:08:33.320 --> 00:08:34.570
this interface here.

00:08:37.539 --> 00:08:41.240
All right, so this is the
BigQuery interface.

00:08:41.240 --> 00:08:43.429
Hopefully, you can
all read it here.

00:08:43.429 --> 00:08:46.750
And on the left-hand side, we
have the public data samples.

00:08:46.750 --> 00:08:49.470
These are all our public data
sets that we've made available

00:08:49.470 --> 00:08:52.710
on BigQuery, like I said,
the GitHub time line.

00:08:52.710 --> 00:08:55.340
But some of the other things
here are things like

00:08:55.340 --> 00:08:58.760
Shakespeare, words in the
Shakespeare vocabulary,

00:08:58.760 --> 00:09:03.430
Wikipedia, all the revision
history, et cetera.

00:09:03.430 --> 00:09:05.200
But the one I'm going to dive
in here today is the

00:09:05.200 --> 00:09:06.390
natality data set.

00:09:06.390 --> 00:09:09.750
And the natality data set is a
record of all births in the

00:09:09.750 --> 00:09:13.540
United States in the last
40 years or so.

00:09:13.540 --> 00:09:16.850
So every single birth in the
United States is recorded in

00:09:16.850 --> 00:09:18.750
this table.

00:09:18.750 --> 00:09:21.420
And let's see how big that is,
how many people were born in

00:09:21.420 --> 00:09:23.550
the last 40 years or so.

00:09:23.550 --> 00:09:24.800
Any guesses?

00:09:26.730 --> 00:09:27.520
Shout out a guess.

00:09:27.520 --> 00:09:29.740
How many people were born since,
I think it goes from

00:09:29.740 --> 00:09:32.480
1972, something like that.

00:09:32.480 --> 00:09:33.470
AUDIENCE: 100 million.

00:09:33.470 --> 00:09:35.450
AUDIENCE: 500 million.

00:09:35.450 --> 00:09:38.360
RYAN BOYD: All right, so some of
you made some guesses from

00:09:38.360 --> 00:09:41.740
hundreds to 300, 500 million.

00:09:41.740 --> 00:09:44.860
We'll just do a simple
COUNT(*) here.

00:09:44.860 --> 00:09:47.170
And I have it zoomed-in, so
we need to scroll down.

00:09:47.170 --> 00:09:51.590
It's actually 137 million rows
in this table, so we're

00:09:51.590 --> 00:09:53.570
getting to be at the
point of being big.

00:09:53.570 --> 00:09:57.570
It's not quite as big as
BigQuery is able to process.

00:09:57.570 --> 00:10:00.260
But it's a great demonstration
data set, because it's some

00:10:00.260 --> 00:10:01.570
interesting data.

00:10:01.570 --> 00:10:04.930
So you saw I just did a
simple COUNT(*) here.

00:10:04.930 --> 00:10:07.270
BigQuery doesn't do indexes, so
this went over all of the

00:10:07.270 --> 00:10:09.580
data and did a count of it.

00:10:09.580 --> 00:10:11.880
So let's actually show
you what some of this

00:10:11.880 --> 00:10:13.670
data looks like here.

00:10:17.120 --> 00:10:23.250
And I'm going to zoom out
a little bit here.

00:10:23.250 --> 00:10:24.600
I'm going to query this table.

00:10:24.600 --> 00:10:28.380
And we have a bunch of fields
here, so the year, month, day

00:10:28.380 --> 00:10:34.270
of births, the state, whether
the child's male, race, weight

00:10:34.270 --> 00:10:37.920
of the child, whether the child
was a twin, some health

00:10:37.920 --> 00:10:41.540
data on the mother, all sorts of
different data that we have

00:10:41.540 --> 00:10:47.050
about children in
this data here.

00:10:47.050 --> 00:10:50.330
And this comes from
the US government.

00:10:50.330 --> 00:10:51.440
Believe me, this isn't
the type of data

00:10:51.440 --> 00:10:54.130
that Google has directly.

00:10:54.130 --> 00:10:56.540
But anyway, so we're selecting
all that information.

00:10:56.540 --> 00:11:00.880
And we're going to say for
a particular date here.

00:11:00.880 --> 00:11:04.740
So where year is--

00:11:04.740 --> 00:11:06.336
call out a year, someone.

00:11:06.336 --> 00:11:07.430
AUDIENCE: 1986.

00:11:07.430 --> 00:11:10.990
RYAN BOYD: 1986 is what
I heard first, 1986.

00:11:10.990 --> 00:11:12.250
And a month?

00:11:12.250 --> 00:11:13.615
AUDIENCE: How about April?

00:11:13.615 --> 00:11:14.525
AUDIENCE: October.

00:11:14.525 --> 00:11:16.110
RYAN BOYD: I heard April first.

00:11:16.110 --> 00:11:16.820
And a day?

00:11:16.820 --> 00:11:17.665
AUDIENCE: Seven [? or nine. ?]

00:11:17.665 --> 00:11:20.400
RYAN BOYD: Seven.

00:11:20.400 --> 00:11:25.330
All right, so April 7, 1986,
give me all the children that

00:11:25.330 --> 00:11:26.140
were born then.

00:11:26.140 --> 00:11:28.540
And how many children
were there?

00:11:28.540 --> 00:11:32.330
And we can see we're crawling
across this data set, 4.3

00:11:32.330 --> 00:11:36.200
seconds, processing 13.1
gigabytes of data.

00:11:36.200 --> 00:11:40.040
And that's 13.1 gigabytes of
data in the columns I touched.

00:11:40.040 --> 00:11:42.200
There's more data here,
but we store this data

00:11:42.200 --> 00:11:43.180
in a columnar format.

00:11:43.180 --> 00:11:46.660
So these are the columns that
I've touched, 13.1 gigabytes

00:11:46.660 --> 00:11:48.710
of data, 4.3 seconds.

00:11:48.710 --> 00:11:51.480
And it looks like we
had 10,000 babies

00:11:51.480 --> 00:11:54.490
born on that day.

00:11:54.490 --> 00:11:56.850
And we can restrict it down
and then say we're in

00:11:56.850 --> 00:11:59.060
California, go into
California.

00:11:59.060 --> 00:12:02.140
So this is the ad hoc, iterative
analysis that

00:12:02.140 --> 00:12:04.750
BigQuery is so great at,
producing results in just a

00:12:04.750 --> 00:12:09.470
handful of seconds here across
large scales of data.

00:12:09.470 --> 00:12:12.510
So 1,378 in just California.

00:12:12.510 --> 00:12:13.770
But one thing I want
to show you.

00:12:13.770 --> 00:12:15.880
So we had 13.1 gigabytes
of data.

00:12:15.880 --> 00:12:20.170
Let's say that we don't need
a lot of this information.

00:12:20.170 --> 00:12:23.810
And we just restrict it down
to a handful of columns.

00:12:30.960 --> 00:12:35.130
And so, in this case, we went
over 4.92 gigabytes of data.

00:12:35.130 --> 00:12:38.810
So BigQuery actually bills based
off of the amount of

00:12:38.810 --> 00:12:41.010
data that you touch when
running your queries.

00:12:41.010 --> 00:12:43.680
It's $35 a terabyte.

00:12:43.680 --> 00:12:47.470
And so, in this case, we've
only processed on 4.92

00:12:47.470 --> 00:12:48.680
gigabytes of data.

00:12:48.680 --> 00:12:51.440
You only have to select the
columns that you're really

00:12:51.440 --> 00:12:52.380
interested in looking at.

00:12:52.380 --> 00:12:55.700
And you'll only be billed for
the columns that you select.

00:12:55.700 --> 00:12:57.840
All right, so we have this.

00:12:57.840 --> 00:13:00.220
Now, I promised you that
BigQuery is actually a

00:13:00.220 --> 00:13:02.050
developer interface.

00:13:02.050 --> 00:13:07.450
So let's look at something that
is built on top of it.

00:13:07.450 --> 00:13:09.280
This is an example.

00:13:09.280 --> 00:13:11.370
This is a company called
QlikView, which is a

00:13:11.370 --> 00:13:13.070
visualization provider.

00:13:13.070 --> 00:13:16.100
And they've built a connector
for BigQuery.

00:13:16.100 --> 00:13:18.460
And this connector for BigQuery
allows you to

00:13:18.460 --> 00:13:21.380
visualize any of your data
that's in BigQuery.

00:13:21.380 --> 00:13:25.980
And one of the demos that we
launched last week was this

00:13:25.980 --> 00:13:30.010
QlikView demo on top of the
birth statistics data.

00:13:30.010 --> 00:13:31.700
And you can drill
down onto it.

00:13:31.700 --> 00:13:33.150
You can filter all the data.

00:13:33.150 --> 00:13:36.130
And its running live queries
against BigQuery.

00:13:36.130 --> 00:13:37.200
So check this out.

00:13:37.200 --> 00:13:38.970
And there was another
one of the demos--

00:13:38.970 --> 00:13:40.590
sorry, I guess I loaded
this earlier when--

00:13:43.300 --> 00:13:44.570
but you can check this out.

00:13:44.570 --> 00:13:46.420
We launched, actually,
another demo with a

00:13:46.420 --> 00:13:48.270
partner called Bime.

00:13:48.270 --> 00:13:51.750
And that demo is on the same
data plus some business data,

00:13:51.750 --> 00:13:53.870
another visualization
provider.

00:13:53.870 --> 00:13:56.880
And we'll have more and more
third-parties in our ecosystem

00:13:56.880 --> 00:13:59.230
releasing things too.

00:13:59.230 --> 00:13:59.710
All right.

00:13:59.710 --> 00:14:03.650
But let's show you how to
actually do this yourself.

00:14:03.650 --> 00:14:06.320
So over here, I have a
Google spreadsheet.

00:14:06.320 --> 00:14:08.910
And this Google spreadsheet
has a big button here that

00:14:08.910 --> 00:14:13.380
says, "Run Query, Run." I'm
going to click that.

00:14:13.380 --> 00:14:16.460
And what this is going to do
is actually execute a query

00:14:16.460 --> 00:14:20.190
against BigQuery and going to
put the results of that query

00:14:20.190 --> 00:14:22.150
into the spreadsheet.

00:14:22.150 --> 00:14:25.130
And the data that we're
looking at here

00:14:25.130 --> 00:14:26.000
is a couple of things.

00:14:26.000 --> 00:14:29.320
We're looking at the average
age of the first birth of a

00:14:29.320 --> 00:14:33.870
mother, so a mother's first
child, how old was the mother,

00:14:33.870 --> 00:14:38.100
and the average weight of that
baby broken down by year.

00:14:38.100 --> 00:14:40.680
And then you'll see, column B, I
actually added something for

00:14:40.680 --> 00:14:43.810
my personal fun experimentation
here, and that

00:14:43.810 --> 00:14:47.910
is red state or blue state
in the 2008 election.

00:14:47.910 --> 00:14:51.340
So how does the weight of a
mother and/or the age of a

00:14:51.340 --> 00:14:54.440
mother's first birth differ,
depending on whether the

00:14:54.440 --> 00:14:57.250
mother was in a red state
or a blue state.

00:14:57.250 --> 00:15:01.140
And for those of you who aren't
from the US, red state

00:15:01.140 --> 00:15:03.550
is Republicans, blue states
is Democrats.

00:15:03.550 --> 00:15:06.600
And let's look at that.

00:15:06.600 --> 00:15:07.920
So we have the data here.

00:15:07.920 --> 00:15:11.890
And then I've actually created
some pivot tables in advance

00:15:11.890 --> 00:15:14.530
that wait till we
get the data in.

00:15:14.530 --> 00:15:20.260
And this is the birth age of a
mother for her first child in

00:15:20.260 --> 00:15:22.400
a blue state versus red state.

00:15:22.400 --> 00:15:26.610
Even I did not expect this wide
of a difference here.

00:15:26.610 --> 00:15:32.090
It looks like we're at about
24 for the red states and

00:15:32.090 --> 00:15:35.370
about 25-ish for the
blue states.

00:15:35.370 --> 00:15:37.250
There's almost a year
difference, or maybe even

00:15:37.250 --> 00:15:41.550
more, between the age of a
mother at her first birth in a

00:15:41.550 --> 00:15:43.150
blue state versus a red state.

00:15:43.150 --> 00:15:45.740
So it's fun data.

00:15:45.740 --> 00:15:48.370
And baby weight.

00:15:48.370 --> 00:15:51.790
Now, I'm not a doctor here,
but this is pretty

00:15:51.790 --> 00:15:52.990
interesting.

00:15:52.990 --> 00:15:54.760
Back in 1970--

00:15:54.760 --> 00:15:56.640
sorry, I didn't format the
bottom there, but those are

00:15:56.640 --> 00:15:57.890
the years--

00:15:57.890 --> 00:16:00.710
back in 1970, the age
was about the same.

00:16:00.710 --> 00:16:01.740
The age of a mother--

00:16:01.740 --> 00:16:05.300
sorry, the weight of babies
was about the same.

00:16:05.300 --> 00:16:08.070
But then, as we proceeded
forward to present day, it

00:16:08.070 --> 00:16:12.960
looks like blue state babies are
heavier by a decent margin

00:16:12.960 --> 00:16:14.080
than red state babies.

00:16:14.080 --> 00:16:15.340
And really, I'm not a doctor.

00:16:15.340 --> 00:16:17.860
I don't want to explain why
that is, but it's just

00:16:17.860 --> 00:16:20.260
interesting data.

00:16:20.260 --> 00:16:22.400
So you might be curious
as to how we did this.

00:16:22.400 --> 00:16:25.730
And we did this with
Google Apps Script.

00:16:25.730 --> 00:16:28.535
And I'm just hopping over
to the script editor.

00:16:28.535 --> 00:16:30.640
So how many of you have used
Google Apps Script before?

00:16:30.640 --> 00:16:31.760
I'm curious.

00:16:31.760 --> 00:16:33.000
Wow, a lot of you.

00:16:33.000 --> 00:16:34.340
All right, this is fantastic.

00:16:34.340 --> 00:16:36.360
I don't need to explain
too much.

00:16:36.360 --> 00:16:38.980
But for those of you who
haven't, JavaScript-based

00:16:38.980 --> 00:16:41.660
language executed on the server
side integrates with a

00:16:41.660 --> 00:16:43.730
lot of Google APIs.

00:16:43.730 --> 00:16:45.480
But up at the top here,
we basically

00:16:45.480 --> 00:16:48.190
have a query defined.

00:16:48.190 --> 00:16:52.190
We have then something that's
getting our active

00:16:52.190 --> 00:16:53.730
spreadsheet, a method's
getting our active

00:16:53.730 --> 00:16:54.680
spreadsheet.

00:16:54.680 --> 00:16:58.980
And then we have a single line
here that's executing our

00:16:58.980 --> 00:17:01.180
BigQuery query.

00:17:01.180 --> 00:17:03.360
And it's running
asynchronously.

00:17:03.360 --> 00:17:06.690
So here, we're then looping
through and checking for the

00:17:06.690 --> 00:17:10.079
result of our query every
500 milliseconds.

00:17:10.079 --> 00:17:12.260
And when the result is complete,
then we're just

00:17:12.260 --> 00:17:15.460
outputting the data to our
Google spreadsheet.

00:17:15.460 --> 00:17:18.550
So 67 lines of code with a lot
of formatting that could

00:17:18.550 --> 00:17:23.040
probably fit in about 50 lines
of code to get these results

00:17:23.040 --> 00:17:26.190
from BigQuery and populate them
into our spreadsheet and

00:17:26.190 --> 00:17:29.570
then run analysis on it in the
spreadsheet, which is a tool

00:17:29.570 --> 00:17:32.380
lots of people are
familiar with.

00:17:32.380 --> 00:17:35.680
All right, so let's talk about
how you do this yourself, how

00:17:35.680 --> 00:17:38.350
you get your data
into BigQuery.

00:17:41.740 --> 00:17:44.390
The first thing that you
should be aware of is,

00:17:44.390 --> 00:17:47.700
typically, when you're working
with databases, you're doing

00:17:47.700 --> 00:17:51.230
what's called denormalizing
your data.

00:17:51.230 --> 00:17:53.750
You're basically taking your--

00:17:53.750 --> 00:17:55.850
sorry, you're normalizing
your data.

00:17:55.850 --> 00:17:59.030
You're basically taking
the duplicate records

00:17:59.030 --> 00:18:00.430
and removing them.

00:18:00.430 --> 00:18:04.540
So for instance, if we have a
birth_record table here and

00:18:04.540 --> 00:18:07.330
then we have the parent's
record, if the same parent

00:18:07.330 --> 00:18:10.170
gave birth to multiple children,
they would only have

00:18:10.170 --> 00:18:14.160
one row in the parent record
and many rows in the

00:18:14.160 --> 00:18:17.060
birth_record table.

00:18:17.060 --> 00:18:19.740
With BigQuery, we actually
denormalized this data.

00:18:19.740 --> 00:18:22.160
And we put it all
into one table.

00:18:22.160 --> 00:18:24.880
This actually makes it a lot
more efficient to run our

00:18:24.880 --> 00:18:27.300
queries and do our analysis.

00:18:27.300 --> 00:18:29.870
And BigQuery really is meant
for that analysis part.

00:18:29.870 --> 00:18:32.070
It's not meant to constantly
be updated or

00:18:32.070 --> 00:18:32.840
anything like that.

00:18:32.840 --> 00:18:36.020
You can append to it, but you
can't update the data in it.

00:18:36.020 --> 00:18:38.020
It's really meant for
this analysis.

00:18:38.020 --> 00:18:41.830
So you load the data in in
a denormalized fashion.

00:18:41.830 --> 00:18:44.110
And that makes it a lot faster
for us to do analysis.

00:18:44.110 --> 00:18:48.530
And then Jordan will talk
more about why that is.

00:18:48.530 --> 00:18:53.560
But you load it in in a simple
format called CSV.

00:18:53.560 --> 00:18:55.120
You all probably recognize
what a CSV

00:18:55.120 --> 00:18:56.590
format file looks like.

00:18:56.590 --> 00:18:57.420
It's really simple.

00:18:57.420 --> 00:19:02.240
You get your data in this CSV
file, load it into BigQuery.

00:19:02.240 --> 00:19:05.980
And you can load it in either
coming directly from your

00:19:05.980 --> 00:19:10.100
machine to BigQuery, or you can
use Google Cloud Storage

00:19:10.100 --> 00:19:13.230
as an intermediary as you
load your data in.

00:19:13.230 --> 00:19:15.930
And that is a lot better from
a performance standpoint and

00:19:15.930 --> 00:19:20.340
handles larger amounts
of data.

00:19:20.340 --> 00:19:24.600
So once your data's in, you
need to run your queries.

00:19:24.600 --> 00:19:28.010
And I've shown you how to do
it in Google Apps Script.

00:19:28.010 --> 00:19:31.620
But we have a variety of
libraries, Java, Python, .NET,

00:19:31.620 --> 00:19:37.010
PHP, JavaScript, Apps Script,
more, Ruby, Objective-C--

00:19:37.010 --> 00:19:40.350
I think there are even others,
all sorts of different

00:19:40.350 --> 00:19:42.880
libraries that you can use
to run your queries.

00:19:42.880 --> 00:19:45.310
And they're all as simple as I
showed you with Apps Script.

00:19:45.310 --> 00:19:47.640
It's really just, you
define your query.

00:19:47.640 --> 00:19:49.270
You send your query over.

00:19:49.270 --> 00:19:50.490
You look for a result.

00:19:50.490 --> 00:19:52.370
And then you get the results.

00:19:52.370 --> 00:19:54.960
But of course, it's
a REST-based API.

00:19:54.960 --> 00:19:57.280
So you can do this in any
language you want.

00:19:57.280 --> 00:20:01.370
If you feel like coding in COBOL
and COBOL has some new

00:20:01.370 --> 00:20:05.770
HTTP package or something like
that, then feel free to use

00:20:05.770 --> 00:20:09.560
BigQuery from your COBOL.

00:20:09.560 --> 00:20:12.530
All right, I'm going to invite
Jordan back up here.

00:20:12.530 --> 00:20:15.130
And Jordan, like I said, is
going to give you, really, for

00:20:15.130 --> 00:20:17.240
those geeks in the crowd,
those data geeks, the

00:20:17.240 --> 00:20:19.910
underlying architecture
of BigQuery.

00:20:19.910 --> 00:20:21.800
And hopefully, you get some
intuition about how to build

00:20:21.800 --> 00:20:22.410
your own queries.

00:20:22.410 --> 00:20:24.015
JORDAN TIGANI: Thanks, Ryan.

00:20:24.015 --> 00:20:24.865
Can you hear me?

00:20:24.865 --> 00:20:25.643
RYAN BOYD: Uh.

00:20:25.643 --> 00:20:26.893
JORDAN TIGANI: Hello?

00:20:29.590 --> 00:20:30.000
RYAN BOYD: Do you have
your mic on?

00:20:30.000 --> 00:20:30.956
JORDAN TIGANI: Hi, everybody.

00:20:30.956 --> 00:20:32.868
I'd like to start with
a quick poll.

00:20:32.868 --> 00:20:35.258
How many of you have ever
written any kind of--

00:20:35.258 --> 00:20:36.410
RYAN BOYD: Is his mic on?

00:20:36.410 --> 00:20:37.712
JORDAN TIGANI: [INAUDIBLE]

00:20:37.712 --> 00:20:39.450
or whatever process?

00:20:39.450 --> 00:20:40.570
Well, a lot of you.

00:20:40.570 --> 00:20:44.472
How many of you [INAUDIBLE]?

00:20:44.472 --> 00:20:46.456
It looks like there is
a bunch of people.

00:20:46.456 --> 00:20:48.936
So how many of you would run
a regular expression

00:20:48.936 --> 00:20:50.186
[INAUDIBLE]?

00:20:52.904 --> 00:20:53.400
All right.

00:20:53.400 --> 00:20:54.888
[INAUDIBLE].

00:20:54.888 --> 00:20:56.376
Not so many.

00:21:06.296 --> 00:21:07.288
[INAUDIBLE].

00:21:07.288 --> 00:21:08.538
I appreciate it.

00:21:11.256 --> 00:21:15.720
So [INAUDIBLE] lot of experience
[INAUDIBLE]

00:21:15.720 --> 00:21:18.034
when you have had that bad
experience, you learn what

00:21:18.034 --> 00:21:20.680
works, what doesn't work,
[INAUDIBLE] your data,

00:21:20.680 --> 00:21:22.168
[INAUDIBLE].

00:21:22.168 --> 00:21:23.418
[INAUDIBLE].

00:21:32.584 --> 00:21:35.064
So BigQuery [INAUDIBLE].

00:21:38.040 --> 00:21:41.016
[LAUGHTER]

00:21:41.016 --> 00:21:41.790
JORDAN TIGANI: Wait.

00:21:41.790 --> 00:21:43.050
I have to get a picture
of this.

00:21:43.050 --> 00:21:43.700
Come on.

00:21:43.700 --> 00:21:44.544
[LAUGHTER]

00:21:44.544 --> 00:21:45.810
JORDAN TIGANI: Wow.

00:21:45.810 --> 00:21:48.220
OK.

00:21:48.220 --> 00:21:49.500
Hopefully, you'll be able
to hear me now.

00:21:49.500 --> 00:21:53.685
[APPLAUSE]

00:21:53.685 --> 00:21:54.320
JORDAN TIGANI: All right.

00:21:54.320 --> 00:21:58.120
So BigQuery is a totally
different, fundamental

00:21:58.120 --> 00:21:59.470
underlying architecture.

00:21:59.470 --> 00:22:01.940
And so the intuition you have
about what's going to run

00:22:01.940 --> 00:22:04.950
fast, what's going to
run well isn't going

00:22:04.950 --> 00:22:06.080
to hold up any more.

00:22:06.080 --> 00:22:08.750
There's no such thing
as an index.

00:22:08.750 --> 00:22:11.110
And as Ryan mentioned, you
should denormalize your data,

00:22:11.110 --> 00:22:12.550
instead of normalizing it.

00:22:12.550 --> 00:22:14.743
And actually, running a regular
expression over every

00:22:14.743 --> 00:22:18.270
row in a large table isn't
really any slower than just

00:22:18.270 --> 00:22:21.020
counting the rows
in that table.

00:22:21.020 --> 00:22:23.570
So hopefully, by the end of this
talk, you'll have learned

00:22:23.570 --> 00:22:26.600
a little bit about how BigQuery
works and can use

00:22:26.600 --> 00:22:30.230
that knowledge to develop some
intuition about what kind of

00:22:30.230 --> 00:22:31.890
things are going to run fast,
what kind of things aren't

00:22:31.890 --> 00:22:36.480
going to run fast and how you
can make them run faster when

00:22:36.480 --> 00:22:38.970
you do run into roadblocks.

00:22:38.970 --> 00:22:42.940
So to start out with a little
review on the relational

00:22:42.940 --> 00:22:48.750
database now, virtually every
relational database uses the

00:22:48.750 --> 00:22:52.400
same fundamental data structure
to access its data,

00:22:52.400 --> 00:22:55.350
whether it's MySQL or
SQL Server Oracle,

00:22:55.350 --> 00:22:57.500
and that's the B-tree.

00:22:57.500 --> 00:23:00.270
The B-tree is an on-disk
data structure.

00:23:00.270 --> 00:23:05.010
And it's primarily good at
finding things quickly.

00:23:05.010 --> 00:23:07.100
So it's sort of like a binary
tree, only with

00:23:07.100 --> 00:23:08.300
a much larger fanout.

00:23:08.300 --> 00:23:10.110
So a query comes in.

00:23:10.110 --> 00:23:10.790
It starts at the root.

00:23:10.790 --> 00:23:14.700
Let's say you're looking for
a particular customer ID.

00:23:14.700 --> 00:23:17.210
That will tell you which of
the children to look at.

00:23:17.210 --> 00:23:20.560
And you keep iterating down the
tree until you actually

00:23:20.560 --> 00:23:22.210
find the data.

00:23:22.210 --> 00:23:26.890
And that's great when you have
one of these B-trees.

00:23:26.890 --> 00:23:28.410
Let's say this was
customer ID.

00:23:28.410 --> 00:23:31.000
Well, what if you're looking
up transaction ID and you

00:23:31.000 --> 00:23:32.200
don't have an index?

00:23:32.200 --> 00:23:35.200
Does anybody know what
would happen?

00:23:35.200 --> 00:23:35.640
Anyone?

00:23:35.640 --> 00:23:36.080
AUDIENCE: Table scan.

00:23:36.080 --> 00:23:39.140
JORDAN TIGANI: Table
scan, exactly.

00:23:39.140 --> 00:23:40.920
I had to have a plant
in the front row.

00:23:43.740 --> 00:23:49.660
So a lot of the underlying
theory behind databases has

00:23:49.660 --> 00:23:52.540
been around for a long time.

00:23:52.540 --> 00:23:54.520
People don't usually
realize how long.

00:23:54.520 --> 00:23:56.740
But even so, this guy was
way ahead of his time.

00:23:56.740 --> 00:23:57.931
[LAUGHTER]

00:23:57.931 --> 00:24:00.710
JORDAN TIGANI: But what I think
he was getting at is

00:24:00.710 --> 00:24:03.590
that, when you have really large
data, it's going to take

00:24:03.590 --> 00:24:05.680
a long time to do
that table scan.

00:24:05.680 --> 00:24:10.250
And on modern disk hardware,
it's going to be about three

00:24:10.250 --> 00:24:15.940
hours to scan over an
entire terabyte.

00:24:15.940 --> 00:24:19.730
And this becomes problematic
when you're trying to do ad

00:24:19.730 --> 00:24:20.570
hoc type queries.

00:24:20.570 --> 00:24:25.200
And an example I like to use
is top-level domains, so,

00:24:25.200 --> 00:24:27.320
like, whether it's
a .net or a .com.

00:24:27.320 --> 00:24:31.390
And let's say you're looking
for who accessed your site

00:24:31.390 --> 00:24:32.690
from various--

00:24:32.690 --> 00:24:35.260
or who referred to
your site from

00:24:35.260 --> 00:24:37.100
various top-level domains.

00:24:37.100 --> 00:24:39.760
And you want to do a
GROUP BY on that.

00:24:39.760 --> 00:24:44.050
In a relational database, you
would first need to create a

00:24:44.050 --> 00:24:46.740
column that had the top-level
domain, because you can't

00:24:46.740 --> 00:24:50.570
really index on the
end of a field.

00:24:50.570 --> 00:24:53.150
And then you need to create an
index on it in order to do any

00:24:53.150 --> 00:24:56.610
kind of reasonable query.

00:24:56.610 --> 00:25:01.220
That can be, really, a pain in
the butt if you're just trying

00:25:01.220 --> 00:25:02.810
to figure out if this
is interesting.

00:25:02.810 --> 00:25:06.100
So you don't want to spend a
half hour or an hour just

00:25:06.100 --> 00:25:10.390
indexing something so that you
can run a simple query to find

00:25:10.390 --> 00:25:12.490
out if it's valuable.

00:25:12.490 --> 00:25:17.210
And a lot of the complexity of
databases is really designed

00:25:17.210 --> 00:25:21.230
to avoid doing table scans,
because they're so expensive.

00:25:21.230 --> 00:25:24.990
So some folks at Google realized
that they want to

00:25:24.990 --> 00:25:26.630
continue to run ad
hoc queries.

00:25:26.630 --> 00:25:28.560
And the data is getting
bigger.

00:25:28.560 --> 00:25:30.480
And so it's harder and harder
to avoid table scans.

00:25:30.480 --> 00:25:33.530
And they just said, what if we
don't bother avoiding the

00:25:33.530 --> 00:25:34.450
table scan?

00:25:34.450 --> 00:25:37.710
What if we just embraced
it and make it fast?

00:25:37.710 --> 00:25:41.120
And so their goal was to perform
a one-terabyte table

00:25:41.120 --> 00:25:43.480
scan in a single second.

00:25:43.480 --> 00:25:46.720
And being Google, we don't like
to scale anything up.

00:25:46.720 --> 00:25:47.890
We don't like to buy
expensive hardware.

00:25:47.890 --> 00:25:50.070
We like to buy more hardware.

00:25:50.070 --> 00:25:52.650
So if you just think about how
many disks you need to run

00:25:52.650 --> 00:25:53.860
that, it's a lot of disks.

00:25:53.860 --> 00:25:56.830
How many processors you're going
to need, if you're not

00:25:56.830 --> 00:26:01.370
going to pre-aggregate and
pre-calculate things, it's a

00:26:01.370 --> 00:26:02.350
lot of disks.

00:26:02.350 --> 00:26:08.110
Luckily, being Google, this
kind of hardware is not

00:26:08.110 --> 00:26:09.300
considered a lot.

00:26:09.300 --> 00:26:15.400
If you guys saw the Compute
Engine keynote yesterday,

00:26:15.400 --> 00:26:18.640
100,000 cores is where it starts
to get interesting.

00:26:18.640 --> 00:26:24.240
5,000, not such a big deal.

00:26:24.240 --> 00:26:28.190
So if you're resigned to do a
table scan, you don't really

00:26:28.190 --> 00:26:30.590
need a data structure
that is designed to

00:26:30.590 --> 00:26:31.860
look things up quickly.

00:26:31.860 --> 00:26:33.530
And so they use the
column store.

00:26:33.530 --> 00:26:37.430
And column stores are very
popular right now.

00:26:37.430 --> 00:26:40.740
There's even a column storage
engine for MySQL.

00:26:40.740 --> 00:26:43.020
But essentially, the way they
work is, instead of storing

00:26:43.020 --> 00:26:45.280
things in record order format
where you store an entire

00:26:45.280 --> 00:26:51.330
record sequentially in the file,
you store every column

00:26:51.330 --> 00:26:52.860
in a separate file.

00:26:52.860 --> 00:26:54.310
And there's two advantages
to that.

00:26:54.310 --> 00:26:57.070
The first is, if you're only
querying over a couple of the

00:26:57.070 --> 00:27:00.820
columns, you only have to
read those columns.

00:27:00.820 --> 00:27:01.670
That's pretty simple.

00:27:01.670 --> 00:27:02.840
The other thing is that
columns tend to

00:27:02.840 --> 00:27:03.850
compress very well.

00:27:03.850 --> 00:27:05.850
And compression algorithms
tend to work by removing

00:27:05.850 --> 00:27:07.330
redundancy.

00:27:07.330 --> 00:27:10.400
There's not a whole lot of
redundancy within a row.

00:27:10.400 --> 00:27:14.200
If you think about it, if you've
got user ID, phone

00:27:14.200 --> 00:27:19.670
number and user's favorite
color in your table, that

00:27:19.670 --> 00:27:22.080
information, there's not a
lot of redundancy there.

00:27:22.080 --> 00:27:25.930
But if you just look at, say,
the user's phone number, a lot

00:27:25.930 --> 00:27:27.310
of phone numbers look similar.

00:27:27.310 --> 00:27:29.740
Or user's favorite color,
there may not be

00:27:29.740 --> 00:27:30.520
many favorite colors.

00:27:30.520 --> 00:27:33.910
There's plenty of opportunity
to do compression there.

00:27:33.910 --> 00:27:35.120
So that will get you,
maybe, one or

00:27:35.120 --> 00:27:36.170
two orders of magnitude.

00:27:36.170 --> 00:27:39.680
We're still not anywhere near
where we would need to be to

00:27:39.680 --> 00:27:42.880
be able to do this sort of
one-terabyte table scan or

00:27:42.880 --> 00:27:45.540
interesting queries over
data that size.

00:27:45.540 --> 00:27:48.620
So we're going to have to figure
out how to re-architect

00:27:48.620 --> 00:27:50.300
the actual computation.

00:27:50.300 --> 00:27:51.827
And so some of you may
say, oh, well, why

00:27:51.827 --> 00:27:53.760
not just use MapReduce?

00:27:53.760 --> 00:27:57.040
MapReduce is a technology that
was invented at Google.

00:27:57.040 --> 00:27:59.080
We released a white paper,
I think it was

00:27:59.080 --> 00:28:00.210
about 10 years ago.

00:28:00.210 --> 00:28:05.430
And there's a popular open
source package, called Hadoop,

00:28:05.430 --> 00:28:07.150
that's the open source
implementation to that.

00:28:07.150 --> 00:28:09.720
Has anyone used Hadoop
in the room?

00:28:09.720 --> 00:28:12.720
OK, we have a bunch
of Hadoop users.

00:28:12.720 --> 00:28:16.030
Any of the people that use
Hadoop, would you use Hadoop

00:28:16.030 --> 00:28:19.030
to do interactive queries?

00:28:19.030 --> 00:28:20.690
OK, not many.

00:28:20.690 --> 00:28:22.530
Hadoop is a fantastic product.

00:28:22.530 --> 00:28:24.020
I mean, it's very cool.

00:28:24.020 --> 00:28:25.860
It will solve a lot of
problems for you.

00:28:25.860 --> 00:28:29.710
But if you really want to do
iterative interactive style

00:28:29.710 --> 00:28:32.420
things, it's not necessarily
the right tool to use.

00:28:32.420 --> 00:28:38.740
And in order to understand why
MapReduce wasn't used for

00:28:38.740 --> 00:28:41.740
this, I want to go into a little
bit of the architecture

00:28:41.740 --> 00:28:44.210
of MapReduce and hope that
I can do this quickly.

00:28:44.210 --> 00:28:49.690
So MapReduce has a single
controller that commands a

00:28:49.690 --> 00:28:50.380
bunch of workers.

00:28:50.380 --> 00:28:52.690
And the workers are actually,
true to their name, going to

00:28:52.690 --> 00:28:53.940
do the actual work.

00:28:56.210 --> 00:28:58.330
So the first step is map.

00:28:58.330 --> 00:29:02.210
And map, you basically run a
single computation over the

00:29:02.210 --> 00:29:04.330
entire data set in parallel.

00:29:04.330 --> 00:29:07.140
And you output some other
information, usually key value

00:29:07.140 --> 00:29:09.740
pairs that are used
by the next stage.

00:29:09.740 --> 00:29:12.740
Next stage is, surprisingly,
reduce.

00:29:12.740 --> 00:29:16.740
Reduce takes those key value
pairs, and everybody that got

00:29:16.740 --> 00:29:19.370
the same key will go to
the same reducer.

00:29:19.370 --> 00:29:24.170
So say we're doing a GROUP BY
the user's favorite color.

00:29:24.170 --> 00:29:27.950
Everybody that liked purple
would go to the same reducer.

00:29:27.950 --> 00:29:29.480
And say we were trying
to count those up.

00:29:29.480 --> 00:29:31.880
The reducer would count those
up and then would write the

00:29:31.880 --> 00:29:34.310
results back.

00:29:34.310 --> 00:29:35.250
And hey, that's awesome.

00:29:35.250 --> 00:29:37.960
That should be pretty
fast, right?

00:29:37.960 --> 00:29:42.840
There's a problem in that
there's a step that I skipped

00:29:42.840 --> 00:29:45.970
over, called shuffle, that
people usually don't think

00:29:45.970 --> 00:29:47.120
about, because it's
not in the name.

00:29:47.120 --> 00:29:48.390
It's not in MapReduce.

00:29:48.390 --> 00:29:51.050
But it really should be
MapShuffleReduce.

00:29:51.050 --> 00:29:52.910
And shuffle is slow.

00:29:52.910 --> 00:29:56.130
I don't know if any of you guys
caught the Compute Engine

00:29:56.130 --> 00:30:01.290
talk yesterday where they
showed a partner had run

00:30:01.290 --> 00:30:07.270
TeraSort over one of their data
sets using a 1,000-node

00:30:07.270 --> 00:30:09.880
cluster on Compute Engine.

00:30:09.880 --> 00:30:11.550
And they did it in about a
minute and 20 seconds.

00:30:11.550 --> 00:30:16.168
And that's very close to the
world record for that speed.

00:30:16.168 --> 00:30:19.790
So even world record speeds
for shuffle are

00:30:19.790 --> 00:30:21.000
still going to be slow.

00:30:21.000 --> 00:30:24.830
And the other problem is, for
interesting queries, you may

00:30:24.830 --> 00:30:28.550
have to do multiple passes
of map and reduce.

00:30:28.550 --> 00:30:32.316
You can't always just do
one map and one reduce.

00:30:35.870 --> 00:30:40.310
So obviously, we're going to
look somewhere different for

00:30:40.310 --> 00:30:42.890
figuring out how to perform
this computation.

00:30:42.890 --> 00:30:44.660
And I like this quote a lot.

00:30:44.660 --> 00:30:47.370
It's from the late,
great Jim Gray.

00:30:47.370 --> 00:30:50.580
But he basically said you need
to move your computation to

00:30:50.580 --> 00:30:54.270
the data when you're working
on large data.

00:30:54.270 --> 00:30:58.610
So that just informs a little
bit of how BigQuery

00:30:58.610 --> 00:30:59.840
architecture works.

00:30:59.840 --> 00:31:02.040
And so this is what
it looks like.

00:31:02.040 --> 00:31:03.290
Does that look familiar
to anybody?

00:31:06.590 --> 00:31:09.520
So this is the same slide
as the B-tree slide.

00:31:09.520 --> 00:31:12.440
But the key difference is,
these boxes are no longer

00:31:12.440 --> 00:31:13.800
on-disk data structures.

00:31:13.800 --> 00:31:15.970
These are actual server
machines.

00:31:15.970 --> 00:31:19.390
These are actual compute
nodes that have

00:31:19.390 --> 00:31:21.110
disk, processor, RAM.

00:31:24.580 --> 00:31:27.620
And so the way it works is
the query comes in at the

00:31:27.620 --> 00:31:30.390
root of this tree.

00:31:30.390 --> 00:31:33.930
The query is transformed and
sent to its children.

00:31:33.930 --> 00:31:36.320
And it's transformed so that
it operates over smaller

00:31:36.320 --> 00:31:37.600
portions of the data.

00:31:37.600 --> 00:31:39.970
And then it's sent to
the next level.

00:31:39.970 --> 00:31:45.480
And each one of those will then
read the data, perform

00:31:45.480 --> 00:31:48.970
the query, and pass results
back up the tree.

00:31:48.970 --> 00:31:54.100
So one way of thinking about
this is it's kind of a B-tree

00:31:54.100 --> 00:31:56.670
index built on-the-fly.

00:31:56.670 --> 00:31:59.320
Another way of thinking about
it is it's kind of like a

00:31:59.320 --> 00:32:02.410
MapReduce, with the bottom row
being the mappers, the next

00:32:02.410 --> 00:32:05.580
row being the reducers and the
top row being the controller.

00:32:05.580 --> 00:32:07.605
But you don't have
to do a shuffle.

00:32:07.605 --> 00:32:12.200
You have a very high bandwidth
connection between the mapper

00:32:12.200 --> 00:32:12.990
and reducer.

00:32:12.990 --> 00:32:16.910
And it's very easy to do
multiple rounds, because it's

00:32:16.910 --> 00:32:19.800
essentially just another RPC.

00:32:19.800 --> 00:32:22.450
And finally, a way of looking
at it is, basically, it's a

00:32:22.450 --> 00:32:26.380
way to decompose relational
algebra into

00:32:26.380 --> 00:32:27.290
the parallel portions.

00:32:27.290 --> 00:32:33.880
For example, many of the
aggregation functions can be

00:32:33.880 --> 00:32:38.020
paralyzed while the total sum
of a partial sum is just the

00:32:38.020 --> 00:32:40.210
sum of the partial sums.

00:32:40.210 --> 00:32:43.510
And so I want to go into a
couple of examples of actual

00:32:43.510 --> 00:32:46.170
SQL and describe how this
is going to work.

00:32:46.170 --> 00:32:48.560
So for simple aggregates,
COUNT.

00:32:48.560 --> 00:32:53.860
If you think about how this
works, it's just, each node

00:32:53.860 --> 00:32:56.900
counts the data, passes
the sum back up.

00:32:56.900 --> 00:32:59.880
And then you get the total
sum at the end.

00:32:59.880 --> 00:33:03.950
MAX is just the maximum of
the partial maximums.

00:33:03.950 --> 00:33:06.160
STDDEV, there's an
online algorithm

00:33:06.160 --> 00:33:06.990
for standard deviation.

00:33:06.990 --> 00:33:09.720
Or you just need to keep
track of the sum

00:33:09.720 --> 00:33:12.310
and the sum of squares.

00:33:12.310 --> 00:33:13.835
The first two are going
to be very easy in

00:33:13.835 --> 00:33:15.160
a relational database.

00:33:15.160 --> 00:33:19.100
COUNT is probably just going to
be metadata on the B-tree.

00:33:19.100 --> 00:33:25.190
MAX, you just have to read down
one side of the B-tree.

00:33:25.190 --> 00:33:27.240
But a relational database would
be hard pressed to do

00:33:27.240 --> 00:33:28.080
standard deviation.

00:33:28.080 --> 00:33:33.370
It's possible that it could
work, but really, it's

00:33:33.370 --> 00:33:36.170
probably not going to work
without a table scan.

00:33:36.170 --> 00:33:38.250
But here's an example of
something that a relational

00:33:38.250 --> 00:33:40.920
database won't be able
to do very quickly.

00:33:40.920 --> 00:33:44.030
And here, we're doing complex
string operations or we're

00:33:44.030 --> 00:33:47.690
doing a regular expression
or a string containment.

00:33:47.690 --> 00:33:51.590
And this is essentially no more
difficult for BigQuery

00:33:51.590 --> 00:33:56.440
than those aggregations
on the previous slide.

00:33:56.440 --> 00:33:59.020
Another thing that you will use
a lot in BigQuery that you

00:33:59.020 --> 00:34:02.530
may not use as much in
relational databases is the

00:34:02.530 --> 00:34:04.680
nested SELECT or
the subselect.

00:34:04.680 --> 00:34:08.570
And the reason this is often
very slow in relational

00:34:08.570 --> 00:34:10.690
databases is because the query
optimizer has a hard time

00:34:10.690 --> 00:34:13.949
reasoning about the inner
SELECT statement.

00:34:13.949 --> 00:34:19.090
But BigQuery, it's pretty
straightforward.

00:34:19.090 --> 00:34:23.340
When you're resigned to doing
a table scan, you don't have

00:34:23.340 --> 00:34:27.219
to worry quite so much about
how you perform the query.

00:34:27.219 --> 00:34:30.454
And I'll show you an example
of that a little later.

00:34:30.454 --> 00:34:32.250
BigQuery also supports JOIN.

00:34:32.250 --> 00:34:33.760
And we call it small join.

00:34:33.760 --> 00:34:35.719
And so what that means is
at least one of the

00:34:35.719 --> 00:34:38.409
tables has to be small.

00:34:38.409 --> 00:34:41.739
And I think this is best
explained by a picture.

00:34:41.739 --> 00:34:45.590
So small join, we take the small
table and we send it to

00:34:45.590 --> 00:34:48.179
every single node in the
computation tree.

00:34:48.179 --> 00:34:51.639
And then the large table is
read as normally, but each

00:34:51.639 --> 00:34:55.460
node then gets to perform its
portion of the join and pass

00:34:55.460 --> 00:34:57.050
the results back up.

00:34:57.050 --> 00:34:59.560
But you can see why the small
table has to be reasonably

00:34:59.560 --> 00:35:06.680
small, because we have to send
it to thousands of nodes.

00:35:06.680 --> 00:35:09.500
And I wouldn't be doing my job
if I was just talking about

00:35:09.500 --> 00:35:11.370
how awesome BigQuery was and the
things that worked really

00:35:11.370 --> 00:35:14.690
well and I didn't mention
the warts.

00:35:14.690 --> 00:35:17.400
And one thing that people tend
to find when they start using

00:35:17.400 --> 00:35:19.630
BigQuery is they
hit the dreaded

00:35:19.630 --> 00:35:21.890
"Response too large" error.

00:35:21.890 --> 00:35:25.080
And so, in this case, we're
trying to select a couple of

00:35:25.080 --> 00:35:29.900
fields from a very large table
without applying a filter.

00:35:29.900 --> 00:35:31.380
And if you think about
this, say we have

00:35:31.380 --> 00:35:32.820
a terabyte of data.

00:35:32.820 --> 00:35:34.270
We're reading that
terabyte of data.

00:35:34.270 --> 00:35:36.340
And then we're passing that
to fewer and fewer nodes.

00:35:36.340 --> 00:35:39.570
And eventually, it's got
to go out the top.

00:35:39.570 --> 00:35:43.280
So you're going to try to send
a terabyte of data through a

00:35:43.280 --> 00:35:43.970
single node.

00:35:43.970 --> 00:35:45.680
That's not going to work well.

00:35:45.680 --> 00:35:49.300
So BigQuery disallows that.

00:35:49.300 --> 00:35:52.410
Generally, you can get around
this by adding a limit clause.

00:35:52.410 --> 00:35:59.330
And usually, you didn't really
want to get that

00:35:59.330 --> 00:36:00.130
terabyte of data back.

00:36:00.130 --> 00:36:03.430
You probably are just interested
in seeing a few of

00:36:03.430 --> 00:36:05.920
the results.

00:36:05.920 --> 00:36:07.660
There's another case where
you'll get a "Response too

00:36:07.660 --> 00:36:08.580
large" error.

00:36:08.580 --> 00:36:11.080
And it tends to be
non-intuitive.

00:36:11.080 --> 00:36:12.830
And that's when you're doing a
GROUP BY, and you have a lot

00:36:12.830 --> 00:36:14.620
of values in a GROUP BY.

00:36:14.620 --> 00:36:19.466
And what's essentially happening
is that, say you're

00:36:19.466 --> 00:36:21.980
doing a GROUP BY and then you're
doing an outer COUNT.

00:36:21.980 --> 00:36:23.720
So you're just returning
one value.

00:36:23.720 --> 00:36:25.060
It seems like, well, I'm just
returning one value.

00:36:25.060 --> 00:36:26.720
Why should I get a "Response
too large"?

00:36:26.720 --> 00:36:32.320
But as data is passed between
the levels, that can,

00:36:32.320 --> 00:36:34.520
essentially, have
too many rows.

00:36:34.520 --> 00:36:37.970
And that often happens with
a large GROUP BY.

00:36:37.970 --> 00:36:40.410
A trick to get around that
is, essentially, to

00:36:40.410 --> 00:36:42.250
sub-sample the results.

00:36:42.250 --> 00:36:48.001
It doesn't sound very nice, but
you can statistically get

00:36:48.001 --> 00:36:50.400
a very good sampling of the
underlying results.

00:36:50.400 --> 00:36:53.130
In this case, we're using the
HASH function, which is sort

00:36:53.130 --> 00:36:53.990
of like [? getHashCode ?]

00:36:53.990 --> 00:36:55.230
in Java.

00:36:55.230 --> 00:36:57.900
But it's a bit stronger than
that, a bit more resistant to

00:36:57.900 --> 00:36:58.350
collisions.

00:36:58.350 --> 00:37:01.810
And we're hashing the user IDs
and taking 1/10 of them.

00:37:01.810 --> 00:37:06.990
And I'll show you, when I get
to the queries, how this

00:37:06.990 --> 00:37:08.670
actually can be a very
good approximation.

00:37:11.250 --> 00:37:16.400
One more case that's related to
GROUP BY that BigQuery has

00:37:16.400 --> 00:37:17.170
a workaround.

00:37:17.170 --> 00:37:20.370
If you're doing a GROUP BY and
an ORDER BY, so that you can

00:37:20.370 --> 00:37:23.360
essentially get the most popular
things or the top

00:37:23.360 --> 00:37:26.780
things, you can use
the TOP API.

00:37:26.780 --> 00:37:29.560
And the TOP API is a statistical
approximation, but

00:37:29.560 --> 00:37:32.200
it's an extremely good
statistical approximation.

00:37:32.200 --> 00:37:33.530
It was designed by
statisticians.

00:37:33.530 --> 00:37:38.390
And it's used heavily
at Google.

00:37:38.390 --> 00:37:40.400
And that can get
you out of some

00:37:40.400 --> 00:37:42.710
"Response too large" holes.

00:37:42.710 --> 00:37:46.700
OK, so I'm going to
go to the demo.

00:37:46.700 --> 00:37:50.230
And the situation here is,
you're in the middle of a

00:37:50.230 --> 00:37:53.980
flame war on Hacker News about
which programming languages

00:37:53.980 --> 00:37:55.070
are the most awesome.

00:37:55.070 --> 00:37:56.620
And you've got three factions.

00:37:56.620 --> 00:38:01.070
You've got the static language
fanatics that say that static

00:38:01.070 --> 00:38:02.250
typing is the best.

00:38:02.250 --> 00:38:06.190
You've got the dynamic language
zealots who think

00:38:06.190 --> 00:38:12.380
that dynamic languages allow
you to code faster.

00:38:12.380 --> 00:38:14.280
And then you've got the
functional programming

00:38:14.280 --> 00:38:17.220
eggheads who feel like
functional programming hurts

00:38:17.220 --> 00:38:17.630
your brain.

00:38:17.630 --> 00:38:19.940
And anything that hurts has
got to be good for you.

00:38:19.940 --> 00:38:22.060
And so what we want to do
is we want to solve this

00:38:22.060 --> 00:38:24.060
once and for all.

00:38:24.060 --> 00:38:25.310
And we want to jump
into the fray.

00:38:25.310 --> 00:38:29.170
But we want to actually back
it up with data, unlike,

00:38:29.170 --> 00:38:30.240
perhaps, some of the
other people

00:38:30.240 --> 00:38:32.040
involved in the argument.

00:38:32.040 --> 00:38:34.560
And so we're going to use
the GitHub data set.

00:38:34.560 --> 00:38:39.240
And we want to compute the
relative popularity of these

00:38:39.240 --> 00:38:42.840
types of languages over time.

00:38:42.840 --> 00:38:46.650
And to compute the relative
popularity, we're going to use

00:38:46.650 --> 00:38:47.900
the 7-day active metric.

00:38:47.900 --> 00:38:50.520
And that's the number of people
that have been active

00:38:50.520 --> 00:38:54.980
in any of these languages
over a 7-day period.

00:38:54.980 --> 00:38:56.330
And we want to do this
in one query.

00:39:07.520 --> 00:39:09.480
All right, so here's
that query.

00:39:09.480 --> 00:39:12.190
Isn't that great?

00:39:12.190 --> 00:39:14.250
Don't bother trying to read
this or understand this.

00:39:14.250 --> 00:39:16.760
I'm going to build it up from
the constituent pieces.

00:39:16.760 --> 00:39:19.150
But this will be online
afterwards.

00:39:19.150 --> 00:39:24.340
We'll post a link that you can
look at these in more detail.

00:39:24.340 --> 00:39:28.000
So this, it's got four
nested SELECTS.

00:39:28.000 --> 00:39:30.550
It's got several GROUP BYs.

00:39:30.550 --> 00:39:32.460
And it's got a JOIN.

00:39:32.460 --> 00:39:35.050
Trust me, it's a cool query.

00:39:35.050 --> 00:39:38.280
But I want to start with
something simpler.

00:39:38.280 --> 00:39:41.880
And so, in order to get our
7-day actives, we only need

00:39:41.880 --> 00:39:44.090
three fields from the
over 200 that are in

00:39:44.090 --> 00:39:46.380
this GitHub data set.

00:39:46.380 --> 00:39:50.135
And so we're going to use actor,
which is a person who's

00:39:50.135 --> 00:39:54.460
making changes, created_at,
which is the time that they

00:39:54.460 --> 00:39:58.760
made the change, and
repository_language is which

00:39:58.760 --> 00:40:02.470
programming language
they used.

00:40:02.470 --> 00:40:07.120
And OK, hopefully, we can get
some actual results to show up

00:40:07.120 --> 00:40:09.057
on the screen.

00:40:09.057 --> 00:40:10.307
Here we go.

00:40:12.810 --> 00:40:17.560
So often, when you're starting
interactive queries, you just

00:40:17.560 --> 00:40:19.420
want to look at the data.

00:40:19.420 --> 00:40:21.020
You want to see what
the format is.

00:40:21.020 --> 00:40:24.830
And we can see that actor is
just the username, created_at

00:40:24.830 --> 00:40:32.250
is a UTC time string, and
repository_language is the

00:40:32.250 --> 00:40:34.310
language that we used.

00:40:34.310 --> 00:40:36.030
So we're doing 7-day actives.

00:40:36.030 --> 00:40:37.390
We're clearly going to need
to do some kind of

00:40:37.390 --> 00:40:41.810
math on this time.

00:40:41.810 --> 00:40:44.170
It's hard to do math
over a string.

00:40:44.170 --> 00:40:47.070
So what we want to do is we want
to turn this-- we might

00:40:47.070 --> 00:40:48.400
be able to come up with a
regular expression that would

00:40:48.400 --> 00:40:49.760
do it, but it would
be really nasty.

00:40:49.760 --> 00:40:55.580
So BigQuery provides a function
called parse_utc

00:40:55.580 --> 00:40:56.990
microseconds.

00:40:56.990 --> 00:40:58.240
Oops.

00:41:00.806 --> 00:41:02.790
Here we go.

00:41:02.790 --> 00:41:03.791
And--

00:41:03.791 --> 00:41:04.733
[LAUGHS]

00:41:04.733 --> 00:41:06.150
Sorry.

00:41:06.150 --> 00:41:09.850
And there's another function
that we want that's called

00:41:09.850 --> 00:41:19.890
"UTC microseconds today" that
essentially will just truncate

00:41:19.890 --> 00:41:23.280
the time stamp to
the day level.

00:41:23.280 --> 00:41:25.620
Because we don't necessarily
care what time of day

00:41:25.620 --> 00:41:28.040
something happened, we just
care what day it is.

00:41:28.040 --> 00:41:31.870
And so we can see the
results of this

00:41:31.870 --> 00:41:33.640
in a couple of seconds.

00:41:33.640 --> 00:41:36.350
So now here we have
the time stamps.

00:41:36.350 --> 00:41:38.390
I'm going to get rid of
repository_language here

00:41:38.390 --> 00:41:43.530
because often, when you're
solving a complex problem,

00:41:43.530 --> 00:41:46.310
it's easier to break it down
into easier problems.

00:41:46.310 --> 00:41:51.860
So at first, let's just
look at 1-day actives

00:41:51.860 --> 00:41:54.180
for the whole site.

00:41:54.180 --> 00:42:00.030
So we're getting redundant
values for if someone checked

00:42:00.030 --> 00:42:02.030
in multiple times per day
or checked into multiple

00:42:02.030 --> 00:42:03.280
repositories per day.

00:42:05.400 --> 00:42:07.500
Let's remove some of these
redundant values by doing a

00:42:07.500 --> 00:42:15.840
GROUP by day, actor.

00:42:15.840 --> 00:42:17.530
So now we have--

00:42:17.530 --> 00:42:18.780
awesome.

00:42:21.560 --> 00:42:24.160
Oh, yes, and as day.

00:42:24.160 --> 00:42:25.410
Thank you.

00:42:28.130 --> 00:42:28.900
Do a GROUP BY.

00:42:28.900 --> 00:42:33.760
OK, so here we're going
to have these unique

00:42:33.760 --> 00:42:35.930
actor and day pairs.

00:42:35.930 --> 00:42:41.130
And so, in order to find out how
many of those are per day,

00:42:41.130 --> 00:42:43.330
we're going to just wrap this
in an outer SELECT.

00:42:43.330 --> 00:42:44.580
So--

00:42:47.470 --> 00:42:48.825
oops, day--

00:42:57.150 --> 00:42:58.496
we need to group by day again.

00:43:01.330 --> 00:43:04.450
And I'll remove that
limit, because we

00:43:04.450 --> 00:43:07.371
want all the results.

00:43:07.371 --> 00:43:13.130
Group, goop, I cannot
spell today.

00:43:13.130 --> 00:43:15.130
This should take a couple
more seconds.

00:43:15.130 --> 00:43:18.650
This is a more complex query.

00:43:18.650 --> 00:43:25.730
And so I should get
a reasonable

00:43:25.730 --> 00:43:29.040
ordering on this one.

00:43:29.040 --> 00:43:29.320
There we go.

00:43:29.320 --> 00:43:32.840
OK, so here's the number
of 1-day actives for,

00:43:32.840 --> 00:43:33.560
essentially, every day.

00:43:33.560 --> 00:43:40.274
And we could reparse that day
back into the string value.

00:43:40.274 --> 00:43:45.110
OK, so as I was talking about
before, when you're doing a

00:43:45.110 --> 00:43:48.050
GROUP BY and there's a lot of
intermediate values, sometimes

00:43:48.050 --> 00:43:51.000
it'll, I call it, blow
up one of the shards.

00:43:51.000 --> 00:43:54.350
It basically is too many
results to return back.

00:43:54.350 --> 00:43:57.290
So what I'm going to do here is
I'm just going to show an

00:43:57.290 --> 00:44:00.370
example of how you
would subsample.

00:44:00.370 --> 00:44:01.660
So we can also--

00:44:01.660 --> 00:44:03.076
oops.

00:44:03.076 --> 00:44:09.525
We could filter by where
the hash of the actor.

00:44:14.740 --> 00:44:19.715
And then, since we're filtering
dividing by 10, we

00:44:19.715 --> 00:44:24.110
need to multiply by 10 here.

00:44:24.110 --> 00:44:26.610
And we can get this in
a couple of seconds.

00:44:26.610 --> 00:44:29.560
So this is the filtered
version.

00:44:29.560 --> 00:44:31.930
And the numbers look
kind of similar.

00:44:31.930 --> 00:44:35.610
And you may say, well, how
similar are those?

00:44:35.610 --> 00:44:38.770
And so here's the query that
will actually determine how

00:44:38.770 --> 00:44:40.600
good of an approximation
that was.

00:44:40.600 --> 00:44:46.540
And perhaps this looks like an
intimidating query, but it's

00:44:46.540 --> 00:44:47.790
actually really simple.

00:44:50.620 --> 00:44:53.100
This is the query I just ran.

00:44:53.100 --> 00:44:56.410
This is the unfiltered query.

00:44:56.410 --> 00:45:00.620
This is the filtered
query right here.

00:45:00.620 --> 00:45:04.400
And we're just joining those
two on day so that we can

00:45:04.400 --> 00:45:06.190
compare the various days.

00:45:06.190 --> 00:45:12.540
And then we're computing the
error, so the difference over

00:45:12.540 --> 00:45:15.220
the expected value times
100, so that we

00:45:15.220 --> 00:45:17.150
get the percent error.

00:45:17.150 --> 00:45:19.340
And that's giving us the
percent error per day.

00:45:19.340 --> 00:45:23.240
And then we just take the
average, so that we get the

00:45:23.240 --> 00:45:23.950
average error.

00:45:23.950 --> 00:45:28.590
And when we run that, we see
that it's less than 1% error.

00:45:28.590 --> 00:45:31.670
So that's a pretty good
approximation.

00:45:31.670 --> 00:45:33.950
OK, so that's great.

00:45:33.950 --> 00:45:35.390
We have 1-day actives.

00:45:35.390 --> 00:45:38.930
But what we really wanted
is 7-day actives.

00:45:38.930 --> 00:45:44.010
And so this is a slightly more
complicated query, but we've

00:45:44.010 --> 00:45:45.090
seen all these parts before.

00:45:45.090 --> 00:45:48.755
So this is the same sampled
query that we had before.

00:45:51.850 --> 00:45:55.700
And we're doing a JOIN, just
like we did in the last query,

00:45:55.700 --> 00:45:56.940
but this part's a little
bit different.

00:45:56.940 --> 00:46:00.440
And this is basically just going
to return all of the

00:46:00.440 --> 00:46:02.850
days that anybody was active.

00:46:02.850 --> 00:46:04.750
And we're going to play a little
trick in relation to

00:46:04.750 --> 00:46:05.500
algebra here.

00:46:05.500 --> 00:46:09.160
When you do a JOIN, if there's
multiple things that match the

00:46:09.160 --> 00:46:11.710
JOIN key on both the left and
the right, you get the

00:46:11.710 --> 00:46:12.260
cross-product.

00:46:12.260 --> 00:46:13.860
You get all of the things on
the left with all of the

00:46:13.860 --> 00:46:14.430
things on the right.

00:46:14.430 --> 00:46:18.310
So here, we're joining
on a constant.

00:46:18.310 --> 00:46:20.740
So we're getting all of
the days with all

00:46:20.740 --> 00:46:23.760
of the active users.

00:46:23.760 --> 00:46:26.000
And what this is going to allow
us to do is then we

00:46:26.000 --> 00:46:31.910
filter those by where the day
is within seven days of the

00:46:31.910 --> 00:46:33.750
user making a change.

00:46:33.750 --> 00:46:37.950
And this allows us to let
someone's change last for

00:46:37.950 --> 00:46:40.310
seven days.

00:46:40.310 --> 00:46:44.230
And so here we get the
7-day actives.

00:46:44.230 --> 00:46:48.820
And there's one more thing that
I want to show that's

00:46:48.820 --> 00:46:51.395
going to enable us to get
to this programming

00:46:51.395 --> 00:46:53.230
language shoot out.

00:46:53.230 --> 00:46:57.490
And what that is is you can
sometimes call it a pivot.

00:46:57.490 --> 00:47:02.180
But what we want to do is we
want to create a pseudo field

00:47:02.180 --> 00:47:03.550
out of the fields that
we already have.

00:47:03.550 --> 00:47:06.400
We want to take the language
that the people are using and

00:47:06.400 --> 00:47:08.370
figure out whether they're
statically typed, dynamically

00:47:08.370 --> 00:47:10.490
typed, or functional.

00:47:10.490 --> 00:47:12.230
And so we can use if statements
here for this.

00:47:12.230 --> 00:47:15.650
So you can see, if the
statements is any of these

00:47:15.650 --> 00:47:17.830
statically typed languages,
then we'll call it static.

00:47:17.830 --> 00:47:19.400
If it's any of these dynamically
typed languages,

00:47:19.400 --> 00:47:21.100
we'll call it dynamic.

00:47:21.100 --> 00:47:23.780
And I listed a few more
functional ones.

00:47:23.780 --> 00:47:26.240
And I also have a regular
expression here to match

00:47:26.240 --> 00:47:28.800
anything with LISP, because I'm
a little bit sympathetic

00:47:28.800 --> 00:47:30.400
to the functional programming
and I was hoping

00:47:30.400 --> 00:47:32.330
that it would do well.

00:47:32.330 --> 00:47:35.950
But this is a relatively
straightforward query, once

00:47:35.950 --> 00:47:37.510
you get past the ifs.

00:47:37.510 --> 00:47:43.310
So those are, essentially, all
the pieces that we need for

00:47:43.310 --> 00:47:44.810
this giant query that
I showed you before.

00:47:44.810 --> 00:47:47.650
So we have the ifs that
I just showed.

00:47:47.650 --> 00:47:49.170
And we have a couple
of aggregations.

00:47:49.170 --> 00:47:51.530
And the only difference is we're
using these aggregations

00:47:51.530 --> 00:47:56.730
to pull these results back out
through the inner queries.

00:47:56.730 --> 00:48:00.100
But we have the same--

00:48:00.100 --> 00:48:01.350
where is it--

00:48:05.110 --> 00:48:09.380
the same thing with we're
pulling out all of the days.

00:48:09.380 --> 00:48:15.370
And essentially, we're doing
the hashed sampling with a

00:48:15.370 --> 00:48:17.560
GROUP BY day an active.

00:48:17.560 --> 00:48:19.810
And then we're just computing
some to the top.

00:48:19.810 --> 00:48:22.050
And hopefully, we can see some
of these results here.

00:48:22.050 --> 00:48:26.630
We see that it looks like,
unfortunately, dynamically

00:48:26.630 --> 00:48:30.290
typed languages are far more
popular on GitHub than--

00:48:30.290 --> 00:48:34.340
probably twice as popular
as static.

00:48:34.340 --> 00:48:38.210
And functional is not doing
so well at all.

00:48:38.210 --> 00:48:43.090
So I guess now it's settled.

00:48:43.090 --> 00:48:45.310
No one has to argue about which
one of these is more

00:48:45.310 --> 00:48:48.440
popular any more.

00:48:48.440 --> 00:48:57.460
All righty, so hopefully, you
guys have learned a little bit

00:48:57.460 --> 00:49:02.200
about what big data is, what
big data means at Google.

00:49:02.200 --> 00:49:03.920
Hopefully, you learned a little
bit about how the

00:49:03.920 --> 00:49:06.870
architecture works and how
that architecture can

00:49:06.870 --> 00:49:12.080
influence the queries you run,
and how to think about what

00:49:12.080 --> 00:49:16.490
queries you can run.

00:49:16.490 --> 00:49:17.150
Thanks.

00:49:17.150 --> 00:49:18.075
Anybody have any questions?

00:49:18.075 --> 00:49:28.650
[APPLAUSE]

00:49:28.650 --> 00:49:29.900
AUDIENCE: Hi.

00:49:33.150 --> 00:49:35.110
Does it support limit paging?

00:49:35.110 --> 00:49:42.720
So I could say, SELECT limit, 10
comma 20, 20 comma 30, and

00:49:42.720 --> 00:49:43.970
page through my results?

00:49:47.475 --> 00:49:48.930
JORDAN TIGANI: You'd have
to do the full limit.

00:49:48.930 --> 00:49:53.320
And then you can page through,
once you get those results.

00:49:53.320 --> 00:49:57.430
And there is also an n-th, which
only allows you to get

00:49:57.430 --> 00:49:58.430
one row at a time.

00:49:58.430 --> 00:50:03.280
But if you need to get the
millionth result, n-th can let

00:50:03.280 --> 00:50:04.536
you do that.

00:50:04.536 --> 00:50:05.225
AUDIENCE: OK.

00:50:05.225 --> 00:50:10.360
And also, the small JOIN seems
like a way to get around

00:50:10.360 --> 00:50:13.420
denormalizing all of our data,
as long as we have a small

00:50:13.420 --> 00:50:15.250
enough table that we would
be joining with.

00:50:15.250 --> 00:50:16.880
JORDAN TIGANI: That's
absolutely true.

00:50:19.710 --> 00:50:21.565
There's a performance bug right
now, which is why I

00:50:21.565 --> 00:50:25.270
didn't show some of
the JOINS live.

00:50:25.270 --> 00:50:28.380
The small JOINS are much slower
than they should be.

00:50:28.380 --> 00:50:29.590
That's getting faster.

00:50:29.590 --> 00:50:31.880
I was hoping we could
have that fixed by

00:50:31.880 --> 00:50:33.680
today, but it wasn't.

00:50:33.680 --> 00:50:36.690
But I think that also just
shows, with BigQuery, we

00:50:36.690 --> 00:50:39.280
release a new version
every week.

00:50:39.280 --> 00:50:40.510
It's going to continue
to get better.

00:50:40.510 --> 00:50:42.670
Things are going to continue
to get faster.

00:50:42.670 --> 00:50:43.730
As more people start using
it, we're going

00:50:43.730 --> 00:50:45.400
to get larger clusters.

00:50:45.400 --> 00:50:49.490
So hopefully, we can turn
that into a positive.

00:50:49.490 --> 00:50:50.600
AUDIENCE: All right,
thank you.

00:50:50.600 --> 00:50:50.900
JORDAN TIGANI: Sure.

00:50:50.900 --> 00:50:53.490
RYAN BOYD: For instance, the red
state, blue state analysis

00:50:53.490 --> 00:50:56.280
that I was doing on the states
was a very small JOIN.

00:50:56.280 --> 00:50:57.460
I mean, there's only
50 states.

00:50:57.460 --> 00:50:58.650
Each one is red or blue.

00:50:58.650 --> 00:51:01.614
So that was one example
that we talked about.

00:51:01.614 --> 00:51:03.526
JORDAN TIGANI: Yes?

00:51:03.526 --> 00:51:06.090
AUDIENCE: I have a high-level
question.

00:51:06.090 --> 00:51:10.490
Let's say, if I have a small
[INAUDIBLE], so to get

00:51:10.490 --> 00:51:12.670
properly MySQL to start with.

00:51:12.670 --> 00:51:15.550
And later on, the data get
bigger and bigger.

00:51:15.550 --> 00:51:19.030
So I have to migrate to this
kind of big data storage.

00:51:19.030 --> 00:51:20.430
So what's the kind of
strategy you will

00:51:20.430 --> 00:51:22.350
recommend for the developer?

00:51:22.350 --> 00:51:26.900
We have to prepare two sets of
SQL when we migrate to a

00:51:26.900 --> 00:51:29.320
bigger data set strategy.

00:51:29.320 --> 00:51:33.502
Then we have to change over.

00:51:33.502 --> 00:51:33.945
JORDAN TIGANI: Do you want to?

00:51:33.945 --> 00:51:35.420
RYAN BOYD: Yeah, I
can take that.

00:51:35.420 --> 00:51:39.630
Basically, it's really easy
to get data into BigQuery.

00:51:39.630 --> 00:51:44.110
You can basically keep track of
what new data you're adding

00:51:44.110 --> 00:51:45.825
to your database and every
now and then do

00:51:45.825 --> 00:51:47.290
an import into BigQuery.

00:51:47.290 --> 00:51:49.460
You could do 1,000
imports a day.

00:51:49.460 --> 00:51:53.960
So take your data out of your
SQL database into CSV format

00:51:53.960 --> 00:51:56.040
and put it into BigQuery.

00:51:56.040 --> 00:51:59.210
You will notice some great
performance gains, when you do

00:51:59.210 --> 00:52:01.070
things like aggregation
queries.

00:52:01.070 --> 00:52:03.470
So for instance, on that
natality data that I showed,

00:52:03.470 --> 00:52:08.550
the 137 million rows, I loaded
that into a MySQL server in a

00:52:08.550 --> 00:52:12.600
large instance on EC2 and
ran queries on it.

00:52:12.600 --> 00:52:15.790
And doing an average of the
birth weight took about a

00:52:15.790 --> 00:52:17.920
minute, whereas BigQuery
took a few seconds.

00:52:17.920 --> 00:52:21.880
So I'd really recommend you
keep your SQL database in

00:52:21.880 --> 00:52:23.770
order to keep your data up
to date, in order to

00:52:23.770 --> 00:52:24.940
serve your data live.

00:52:24.940 --> 00:52:27.660
But when you're trying to do
your analytics, then export

00:52:27.660 --> 00:52:31.360
that data out of your SQL
database into BigQuery.

00:52:31.360 --> 00:52:34.180
You could even do things like
sharding, based off of day.

00:52:34.180 --> 00:52:37.070
And you can select from multiple
tables in BigQuery

00:52:37.070 --> 00:52:39.070
and union them together in your
results, if you have your

00:52:39.070 --> 00:52:40.532
data sharded.

00:52:40.532 --> 00:52:43.750
Does that help?

00:52:43.750 --> 00:52:46.270
Come up later if you have
any follow-up on that.

00:52:46.270 --> 00:52:48.570
AUDIENCE: Can you summarize
what sorts of

00:52:48.570 --> 00:52:52.220
public data are available?

00:52:52.220 --> 00:52:53.110
RYAN BOYD: Sure.

00:52:53.110 --> 00:52:55.996
Where's that question
coming from?

00:52:55.996 --> 00:52:57.856
Oh.

00:52:57.856 --> 00:53:00.350
Oh, the lights are a little
bit blinding.

00:53:00.350 --> 00:53:04.000
Public data, so Wikipedia
revision history, going back

00:53:04.000 --> 00:53:07.950
to about two years ago, there's
350 million rows.

00:53:07.950 --> 00:53:10.753
You can grab an updated version
of that data set and

00:53:10.753 --> 00:53:11.530
load it in yourself.

00:53:11.530 --> 00:53:14.600
There's 450 million rows in
that, basically, every

00:53:14.600 --> 00:53:17.340
revision made to every
Wikipedia article.

00:53:17.340 --> 00:53:19.450
It's really interesting data
to play around with.

00:53:19.450 --> 00:53:21.830
We have the natality data that
I just showed you, the birth

00:53:21.830 --> 00:53:23.010
certificates.

00:53:23.010 --> 00:53:26.180
We have Ngrams.

00:53:26.180 --> 00:53:28.240
I'm trying to think of all
the different other ones.

00:53:28.240 --> 00:53:28.860
JORDAN TIGANI: Yeah.

00:53:28.860 --> 00:53:30.410
So the Ngrams are the
Google Books sets.

00:53:30.410 --> 00:53:31.290
We don't have all that much.

00:53:31.290 --> 00:53:33.880
And we had a little bit
of a hard time finding

00:53:33.880 --> 00:53:35.530
large public data sets.

00:53:35.530 --> 00:53:38.220
So if you guys have data sets
that are large and public that

00:53:38.220 --> 00:53:39.850
you'd like us to include
in the public

00:53:39.850 --> 00:53:42.230
samples, let us know.

00:53:42.230 --> 00:53:46.600
And we would love to have more
interesting things out there.

00:53:46.600 --> 00:53:50.130
RYAN BOYD: The largest one, I
think, we have is the network

00:53:50.130 --> 00:53:51.260
performance data.

00:53:51.260 --> 00:53:52.330
I forget what it's called.

00:53:52.330 --> 00:53:52.730
JORDAN TIGANI: M-Lab.

00:53:52.730 --> 00:53:53.560
RYAN BOYD: M-Lab, yeah.

00:53:53.560 --> 00:53:55.780
The M-Lab data set, which is
kind of network performance

00:53:55.780 --> 00:53:58.150
data, it's a little hard for
demos because not everyone

00:53:58.150 --> 00:53:59.900
understands the underlying
data.

00:53:59.900 --> 00:54:01.853
But that is a very large
data set that you

00:54:01.853 --> 00:54:03.170
can play around with.

00:54:03.170 --> 00:54:04.410
Is there something specifically
that you're

00:54:04.410 --> 00:54:06.914
looking for?

00:54:06.914 --> 00:54:07.900
No.

00:54:07.900 --> 00:54:09.020
All right.

00:54:09.020 --> 00:54:09.870
OK?

00:54:09.870 --> 00:54:12.980
AUDIENCE: Do you have any tips
for making the development

00:54:12.980 --> 00:54:14.410
process cheaper?

00:54:14.410 --> 00:54:18.010
So if you're charging by
terabytes of data touched, and

00:54:18.010 --> 00:54:21.160
just there in the demo you
touched a few gigs.

00:54:21.160 --> 00:54:25.800
I have great ideas for moving
a data warehouse to BigQuery

00:54:25.800 --> 00:54:26.880
or log data.

00:54:26.880 --> 00:54:32.200
And while it might not consume
a whole lot of disk by your

00:54:32.200 --> 00:54:36.506
standards, for my developers
who are not accustomed to

00:54:36.506 --> 00:54:40.880
working with BigQuery,
it could get pricey.

00:54:40.880 --> 00:54:44.550
RYAN BOYD: We typically look
at it as being less pricey

00:54:44.550 --> 00:54:46.140
than alternative solutions.

00:54:46.140 --> 00:54:50.410
And we've found customers
that tell us that.

00:54:50.410 --> 00:54:52.360
There is a separate charge
for storage than

00:54:52.360 --> 00:54:53.210
there is for queries.

00:54:53.210 --> 00:54:55.220
So you can upload your
terabytes of data.

00:54:55.220 --> 00:54:57.720
And the cost of uploading
that data is cheap.

00:54:57.720 --> 00:55:00.040
It's when you run your queries
and you touch the terabytes of

00:55:00.040 --> 00:55:04.030
data that you're getting
the cost incurred.

00:55:04.030 --> 00:55:08.610
But $35 per terabyte, you
compare that to I started a

00:55:08.610 --> 00:55:11.516
bunch of Hadoop instances
running high.

00:55:11.516 --> 00:55:14.250
But I think I had 15 machines
that I was running.

00:55:14.250 --> 00:55:17.960
And I ended up with a $900 bill
after a couple of days.

00:55:17.960 --> 00:55:20.670
That's a lot more expensive,
in my mind, because,

00:55:20.670 --> 00:55:21.710
basically, you're keeping those

00:55:21.710 --> 00:55:23.040
resources always running.

00:55:23.040 --> 00:55:25.380
Whereas, we're only charging you
for the actual queries and

00:55:25.380 --> 00:55:27.800
all that you're doing.

00:55:27.800 --> 00:55:29.920
And our queries are going across
hundreds and hundreds

00:55:29.920 --> 00:55:32.060
of servers to get you
those fast results.

00:55:32.060 --> 00:55:36.160
JORDAN TIGANI: And if you do
have data that's that large,

00:55:36.160 --> 00:55:39.590
you can use the hash trick to
sample out some of that data

00:55:39.590 --> 00:55:42.860
and get a million rows that
they can work on.

00:55:42.860 --> 00:55:45.730
And when you have a million
rows, those queries are

00:55:45.730 --> 00:55:49.870
actually really cheap, if you're
only touching a couple

00:55:49.870 --> 00:55:50.480
of megabytes.

00:55:50.480 --> 00:55:53.240
It takes a lot of megabytes
to add up to a terabyte.

00:55:53.240 --> 00:55:56.420
RYAN BOYD: The other thing is
actually sharding the data.

00:55:56.420 --> 00:56:02.390
So I demoed the sharding on
that 2012, like the market

00:56:02.390 --> 00:56:04.810
statistics from 2012.

00:56:04.810 --> 00:56:06.860
Google actually does
sharding on a daily

00:56:06.860 --> 00:56:09.330
basis, on a weekly basis.

00:56:09.330 --> 00:56:12.730
So we have tables for each day
and a table for each week.

00:56:12.730 --> 00:56:14.960
So if you're just trying to look
at a smaller portion of

00:56:14.960 --> 00:56:19.390
your data to learn and run your
queries on, definitely

00:56:19.390 --> 00:56:20.020
shard it up.

00:56:20.020 --> 00:56:23.050
And then you can do UNION
queries to join everything

00:56:23.050 --> 00:56:26.810
together when you want
to look over a larger

00:56:26.810 --> 00:56:29.390
amount of data at once.

00:56:29.390 --> 00:56:31.330
Because we're doing those full
table scans, that will really

00:56:31.330 --> 00:56:33.470
limit the amount of data that
you touch and it will allow

00:56:33.470 --> 00:56:34.560
you to still perform
the real queries.

00:56:34.560 --> 00:56:38.128
AUDIENCE: Thank you.

00:56:38.128 --> 00:56:39.090
JORDAN TIGANI: Somebody
in the back?

00:56:39.090 --> 00:56:39.460
AUDIENCE: Yeah.

00:56:39.460 --> 00:56:43.530
We're trying to use BigQuery
for log analysis.

00:56:43.530 --> 00:56:46.460
Can you talk about some of the
potential gotchas that we may

00:56:46.460 --> 00:56:49.130
encounter doing so,
specifically

00:56:49.130 --> 00:56:52.590
around revolving logs?

00:56:52.590 --> 00:56:55.250
So not necessarily keeping the
full, entire history of logs,

00:56:55.250 --> 00:56:57.200
but just the last, say,
30 days or something.

00:56:57.200 --> 00:56:59.900
JORDAN TIGANI: So we have added
recently a feature that

00:56:59.900 --> 00:57:01.370
allows you to set an expiration

00:57:01.370 --> 00:57:02.620
time on your tables.

00:57:02.620 --> 00:57:10.410
So you can continue to add
tables, and the older ones

00:57:10.410 --> 00:57:13.740
will just drop off
as they age out.

00:57:13.740 --> 00:57:23.070
We don't have any ways right now
to window your query over

00:57:23.070 --> 00:57:26.536
a particular time and particular
set of tables.

00:57:26.536 --> 00:57:28.770
RYAN BOYD: But you can specify
them all individually.

00:57:28.770 --> 00:57:29.010
JORDAN TIGANI: Right.

00:57:29.010 --> 00:57:31.700
RYAN BOYD: The queries can
get a little bit long.

00:57:31.700 --> 00:57:34.980
AUDIENCE: Is it possible to
query over a single data set

00:57:34.980 --> 00:57:36.950
that contains multiple tables,
so you don't have to do that

00:57:36.950 --> 00:57:40.060
union and figure out
programmatically what the set

00:57:40.060 --> 00:57:41.760
of tables that you
care about are.

00:57:41.760 --> 00:57:44.706
Instead, just query the
whole data set.

00:57:44.706 --> 00:57:45.810
JORDAN TIGANI: Not currently.

00:57:45.810 --> 00:57:49.340
But I wouldn't be surprised
if it was added

00:57:49.340 --> 00:57:50.220
some time in the future.

00:57:50.220 --> 00:57:51.157
RYAN BOYD: In the near future.

00:57:51.157 --> 00:57:52.350
[LAUGHS]

00:57:52.350 --> 00:57:53.410
Nice hint there, Jordan.

00:57:53.410 --> 00:57:55.210
JORDAN TIGANI: Thanks.

00:57:55.210 --> 00:57:57.020
AUDIENCE: Hey, I was going
to ask, what's

00:57:57.020 --> 00:57:59.140
the best append strategy?

00:57:59.140 --> 00:58:02.280
I see that you can do it by just
uploading a new CSV and

00:58:02.280 --> 00:58:04.150
telling it to add it
to the data set.

00:58:04.150 --> 00:58:08.930
Or there's actually an APPEND
command in the API.

00:58:08.930 --> 00:58:13.940
Let's say you have sensors
putting in 100,000 records

00:58:13.940 --> 00:58:15.500
every day or hour or whatever.

00:58:15.500 --> 00:58:18.830
It's like, what's the most
efficient way to add that

00:58:18.830 --> 00:58:22.070
stuff in as it comes in live?

00:58:22.070 --> 00:58:24.100
JORDAN TIGANI: Especially right
now, I would tend to try

00:58:24.100 --> 00:58:25.930
to aggregate that
a little bit.

00:58:25.930 --> 00:58:29.150
So rather than doing a whole lot
of little ones, I would do

00:58:29.150 --> 00:58:32.560
fewer larger APPENDS.

00:58:32.560 --> 00:58:36.150
And if you're going to want to
do a lot of your queries only

00:58:36.150 --> 00:58:41.190
over a smaller portion of that,
you might want to use

00:58:41.190 --> 00:58:46.540
several tables or
daily tables.

00:58:46.540 --> 00:58:50.590
The syntax for including
multiple tables is just a

00:58:50.590 --> 00:58:54.620
comma, so you could say, from A
comma B, which confuses some

00:58:54.620 --> 00:58:56.900
people because, elsewhere,
that means

00:58:56.900 --> 00:59:00.230
join those two tables.

00:59:00.230 --> 00:59:04.775
AUDIENCE: So say I
have my database.

00:59:04.775 --> 00:59:08.580
And I'm using it to serve it
to my users, so every so

00:59:08.580 --> 00:59:11.980
often, I want to use BigQuery
to do some analytics and to

00:59:11.980 --> 00:59:14.180
study how things are going.

00:59:14.180 --> 00:59:16.350
You mentioned it's all an API.

00:59:16.350 --> 00:59:24.670
So I was wondering
which way could I

00:59:24.670 --> 00:59:26.750
automate the appending?

00:59:26.750 --> 00:59:28.640
Say I want it every week.

00:59:28.640 --> 00:59:32.410
I want to have a fresh version
of my database up in BigQuery

00:59:32.410 --> 00:59:34.510
to do my analysis.

00:59:34.510 --> 00:59:35.720
What would be the best
way to do that?

00:59:35.720 --> 00:59:38.295
How does the API look like
for something like that?

00:59:38.295 --> 00:59:40.830
RYAN BOYD: I would just do a
cron job, or something like

00:59:40.830 --> 00:59:44.550
that, that you would select the
last weeks of data out of

00:59:44.550 --> 00:59:45.310
your database.

00:59:45.310 --> 00:59:48.810
And I believe MySQL has a
MySQL import function.

00:59:48.810 --> 00:59:50.920
I think they probably have an
export function as well that

00:59:50.920 --> 00:59:53.990
will generate a CSV
file for you.

00:59:53.990 --> 00:59:55.280
And just do that.

00:59:55.280 --> 00:59:58.040
And import that data
into BigQuery.

00:59:58.040 --> 01:00:00.860
The import process I,
unfortunately, was supposed to

01:00:00.860 --> 01:00:01.830
show you, but didn't.

01:00:01.830 --> 01:00:06.260
But all you really have to do
was generate your file.

01:00:06.260 --> 01:00:08.840
And then you could do the
import through the UI.

01:00:08.840 --> 01:00:12.370
And this is the same
thing as you do the

01:00:12.370 --> 01:00:14.890
import through the API.

01:00:14.890 --> 01:00:18.790
You basically specify the
schema, the table ID, and then

01:00:18.790 --> 01:00:21.300
the location of the file
on Cloud Storage.

01:00:21.300 --> 01:00:23.540
And there's just a few little,
simple options that you can

01:00:23.540 --> 01:00:24.760
specify then.

01:00:24.760 --> 01:00:26.970
So you can just regularly import
your data from your

01:00:26.970 --> 01:00:30.850
code in a cron job on a weekly
basis, would probably be the

01:00:30.850 --> 01:00:31.670
best strategy there.

01:00:31.670 --> 01:00:34.185
AUDIENCE: And it would be
just a post to some URL?

01:00:34.185 --> 01:00:36.750
RYAN BOYD: Yeah, exactly.

01:00:36.750 --> 01:00:38.860
Typically, we upload it through
Google Cloud Storage.

01:00:38.860 --> 01:00:41.820
So there's command line binaries
that you can use for

01:00:41.820 --> 01:00:45.200
that, or there are libraries, or
you can manually construct

01:00:45.200 --> 01:00:47.910
your post to upload it
to Cloud Storage.

01:00:47.910 --> 01:00:53.250
And then you just do a post to
BigQuery to insert a new job

01:00:53.250 --> 01:00:54.720
to say ingest this data.

01:00:54.720 --> 01:00:55.130
AUDIENCE: Thanks.

01:00:55.130 --> 01:00:56.380
Thanks.

01:00:58.996 --> 01:01:02.580
AUDIENCE: So how long would it
take to import a terabyte of

01:01:02.580 --> 01:01:06.676
data from Google Cloud Storage
into BigQuery?

01:01:06.676 --> 01:01:09.510
RYAN BOYD: I don't know
on a terabyte of data.

01:01:09.510 --> 01:01:13.830
I did the Wikipedia data set
of 450 million rows, and it

01:01:13.830 --> 01:01:16.790
was about a half hour process
for the ingestion.

01:01:16.790 --> 01:01:18.990
A lot of that, though, is the
ingestion job just sitting in

01:01:18.990 --> 01:01:21.140
a queue, because we only
have so many machines

01:01:21.140 --> 01:01:22.220
that process ingestion.

01:01:22.220 --> 01:01:25.880
So there's a little latency
before the job kicks off, but

01:01:25.880 --> 01:01:28.950
the jobs actually go
really quickly.

01:01:28.950 --> 01:01:34.150
I don't know how many gigabytes
the Wikipedia data

01:01:34.150 --> 01:01:37.708
set was, but it's definitely
not small.

01:01:37.708 --> 01:01:41.270
AUDIENCE: So I support
the DOD community.

01:01:41.270 --> 01:01:44.120
And the data lives at
the test ranges.

01:01:44.120 --> 01:01:47.350
And we're talking petabytes,
not terabytes.

01:01:47.350 --> 01:01:50.230
Some of these tools would
be very useful to us for

01:01:50.230 --> 01:01:53.250
analyzing those large
data sets.

01:01:53.250 --> 01:01:55.630
Is there any process for being
able to get some of these

01:01:55.630 --> 01:01:58.852
tools to where our data lives?

01:01:58.852 --> 01:02:00.770
JORDAN TIGANI: We don't have a
process like that right now,

01:02:00.770 --> 01:02:05.930
but you might contact one
of the salespeople.

01:02:05.930 --> 01:02:11.780
If there's the possibility for
really large data sets, they

01:02:11.780 --> 01:02:15.830
might be able to come up with
a way to import those in a

01:02:15.830 --> 01:02:16.920
one-off thing.

01:02:16.920 --> 01:02:17.750
RYAN BOYD: Yeah.

01:02:17.750 --> 01:02:21.730
Now that we have compute with
Linux VMs in the cloud, maybe

01:02:21.730 --> 01:02:24.890
you can move your data onto our
infrastructure, even if

01:02:24.890 --> 01:02:28.550
it's using some proprietary
code.

01:02:28.550 --> 01:02:29.530
All right, it looks like that's

01:02:29.530 --> 01:02:31.570
the last of the questions.

01:02:31.570 --> 01:02:35.410
We do also have a very active
tag on Stack Overflow.

01:02:35.410 --> 01:02:37.620
And Michael, up here in the
front row, was actually

01:02:37.620 --> 01:02:40.140
answering Stack Overflow
questions while Jordan and I

01:02:40.140 --> 01:02:44.870
were speaking So ask your
questions there, or find us

01:02:44.870 --> 01:02:46.030
out after the talk.

01:02:46.030 --> 01:02:47.570
So thank you, guys.

01:02:47.570 --> 01:02:47.870
JORDAN TIGANI: Thanks.

01:02:47.870 --> 01:03:03.550
[APPLAUSE]

