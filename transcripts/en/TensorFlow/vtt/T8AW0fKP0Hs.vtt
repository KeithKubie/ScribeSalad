WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.664
♪ (music) ♪

00:00:05.664 --> 00:00:08.258
(clapping)

00:00:08.258 --> 00:00:09.450
Hello.

00:00:09.450 --> 00:00:10.928
My name is Alex,

00:00:10.928 --> 00:00:13.386
and I'm here to tell you
about <i>Eager Execution,</i>

00:00:13.386 --> 00:00:16.244
which I think you've heard
in the last two talks now.

00:00:16.244 --> 00:00:18.672
But I'm here to tell you
what it's actually about.

00:00:18.962 --> 00:00:21.575
This new, imperative,
object-oriented,

00:00:21.575 --> 00:00:24.046
pythonic way of using TensorFlow,

00:00:24.046 --> 00:00:25.846
that we're introducing to you today

00:00:25.846 --> 00:00:27.253
as part of TensorFlow core.

00:00:27.643 --> 00:00:31.216
So, you know because you're here,
or because you're watching the livestream,

00:00:31.216 --> 00:00:33.051
I hope, that <i>TensorFlow</i> has been

00:00:33.051 --> 00:00:36.671
this graph execution engine
for machine learning,

00:00:36.671 --> 00:00:38.693
that lets you run graphs
at like high scale

00:00:38.693 --> 00:00:40.860
and all sorts of other nice things.

00:00:40.860 --> 00:00:42.031
But has it?

00:00:42.031 --> 00:00:46.426
And why did we choose to go
with graphs in the first place?

00:00:46.426 --> 00:00:48.965
Since now I'm going to tell you
about <i>Eager Execution,</i>

00:00:48.965 --> 00:00:51.470
where we move beyond
what we can achieve with graphs,

00:00:51.470 --> 00:00:54.644
I think it's a good idea to recap
why we bothered.

00:00:54.844 --> 00:00:57.429
And a really good reason
why you want

00:00:57.429 --> 00:01:01.052
to have your computation represented
as a platform-independent graph

00:01:01.542 --> 00:01:04.654
is that once you have that,
it's very easy to differentiate the graph.

00:01:04.654 --> 00:01:06.007
And I went to grad school

00:01:06.007 --> 00:01:08.756
before auto diff was standard
in machine learning toolkits

00:01:08.756 --> 00:01:10.726
and I do not wish that on any one.

00:01:10.726 --> 00:01:13.788
Like, it's... life is much
better now trust me

00:01:13.788 --> 00:01:16.731
Also, if you have a platform-independent,

00:01:16.731 --> 00:01:19.171
abstract representation
of your computation,

00:01:19.171 --> 00:01:22.575
you can just go and deploy it
to pretty much anything you want.

00:01:22.575 --> 00:01:25.291
You can run it on the TPU.
You can run it on the GPU.

00:01:25.291 --> 00:01:27.605
You can put it on a phone;
you can put it on a Raspberry Pi.

00:01:27.605 --> 00:01:29.882
There are like all sorts of cool
deployment scenarios

00:01:29.882 --> 00:01:31.318
you're going to hear about today.

00:01:31.318 --> 00:01:34.831
And it's really valuable to have this kind
of platform-independent view.

00:01:35.351 --> 00:01:39.660
Also, compilers work with data flow
graphs internally,

00:01:40.090 --> 00:01:42.683
and they know how to do
all sorts of nice optimizations

00:01:42.683 --> 00:01:45.110
that rely on having a global view
of your computation,

00:01:45.490 --> 00:01:48.323
like constant folding, common
subexpression elimination,

00:01:48.743 --> 00:01:50.874
like data laying and things like that.

00:01:50.874 --> 00:01:54.566
And a lot of these optimizations
are really deep learning specific.

00:01:54.566 --> 00:01:58.430
We can choose how to properly
lay out your channels and your height,

00:01:58.430 --> 00:01:59.421
and width and stuff.

00:01:59.421 --> 00:02:01.164
So, your convolutions are faster.

00:02:02.574 --> 00:02:05.864
And finally, a key reason that's very
important to us at Google,

00:02:05.864 --> 00:02:07.271
and I hope important to you as well

00:02:07.271 --> 00:02:09.141
is that once you have
a platform-independent

00:02:09.141 --> 00:02:11.261
representation of your computation,

00:02:11.261 --> 00:02:14.284
you can just deploy it, and distribute it
across hundreds of machines

00:02:14.284 --> 00:02:16.624
or a TPU-pod, like they showed earlier.

00:02:16.624 --> 00:02:18.513
And this is a very seamless process.

00:02:19.183 --> 00:02:20.744
So, since graphs are so good,

00:02:20.744 --> 00:02:23.260
what made us think that now it's a good
idea to move beyond them

00:02:23.260 --> 00:02:24.761
and let you do <i>eager execution</i>?

00:02:25.641 --> 00:02:27.922
Good place to start
is that you don't actually

00:02:27.922 --> 00:02:29.828
have to give up
automatic differentiation.

00:02:29.828 --> 00:02:33.354
I'm sure like you're familiar with like
other frameworks, like <i>Pythons</i>

00:02:33.354 --> 00:02:35.371
<i>Autograph</i> that let...

00:02:35.371 --> 00:02:37.257
sorry <i>Autograd,</i>

00:02:37.257 --> 00:02:39.798
that let you differentiate
dynamic code and...

00:02:40.298 --> 00:02:41.858
You don't need to have an a priori

00:02:41.858 --> 00:02:43.307
of your computation differentiated.

00:02:43.307 --> 00:02:45.687
You can just build up a trace as you go,

00:02:45.687 --> 00:02:48.006
and then walk back the trace
to compute gradients.

00:02:48.636 --> 00:02:52.555
Also, if you don't stop
to build a platform...

00:02:52.555 --> 00:02:54.672
like this computational graph,

00:02:54.672 --> 00:02:56.649
you can iterate a lot more quickly.

00:02:56.649 --> 00:02:58.421
You can play with your model
as you build it.

00:02:58.421 --> 00:02:59.960
You can inspect it.

00:02:59.960 --> 00:03:01.084
You can poke and prod at it.

00:03:01.084 --> 00:03:04.094
And this can let you
just be more productive

00:03:04.454 --> 00:03:06.560
when you're like making all these changes.

00:03:07.350 --> 00:03:10.598
Also, you can run your model
for debuggers and profilers

00:03:10.598 --> 00:03:15.311
and add all sorts of analysis tools
to them, to just really understand

00:03:15.311 --> 00:03:17.032
how they are doing what they are doing.

00:03:17.712 --> 00:03:20.661
And finally, if we don't force you
to represent your computation

00:03:20.661 --> 00:03:23.370
in a separate way than the host
programming language you are using,

00:03:23.680 --> 00:03:25.124
you can just use all

00:03:25.124 --> 00:03:28.767
the machinery of your host programming
language to do control flow and data flow,

00:03:28.767 --> 00:03:30.213
and complicated data structures,

00:03:30.213 --> 00:03:34.747
which for some models is key to being
able to make your model working at all.

00:03:35.997 --> 00:03:39.234
So, I hope you're not wondering,
"How do I get to use this?"

00:03:39.234 --> 00:03:41.192
And the way you use this is super easy.

00:03:41.192 --> 00:03:42.760
You import TensorFlow
and you call

00:03:42.760 --> 00:03:44.800
<i>tf.enable_eager_execution</i>

00:03:45.180 --> 00:03:46.728
And once you do that,

00:03:46.728 --> 00:03:49.418
what happens is anytime you run
a TensorFlow operation,

00:03:49.418 --> 00:03:51.326
like in this case a <i>.matmul,</i>

00:03:51.326 --> 00:03:53.611
instead of TensorFlow building a graph

00:03:53.611 --> 00:03:57.713
that later when executed is going
to run that matrix multiplication,

00:03:57.713 --> 00:04:00.669
we just immediately run
that matrix multiplication for you

00:04:00.669 --> 00:04:01.996
and give you the result.

00:04:01.996 --> 00:04:03.037
And you can print it,

00:04:03.037 --> 00:04:04.812
you can slice it, you can dice it,

00:04:04.812 --> 00:04:06.430
you can do whatever you want with it.

00:04:07.020 --> 00:04:08.848
And because things
are happening immediately,

00:04:08.848 --> 00:04:11.238
you can have highly dynamic control flow

00:04:11.238 --> 00:04:14.565
that depends on the actual values
of the computation you're executing.

00:04:14.565 --> 00:04:16.344
And here is just a simple

00:04:16.344 --> 00:04:19.405
Wolfe conditions line search
example that I wrote.

00:04:19.405 --> 00:04:21.908
And it doesn't matter, it just matters;
it has like <i>while loops</i>

00:04:21.908 --> 00:04:24.585
that depend on complicated values

00:04:24.585 --> 00:04:26.346
that are computer-based
on the computation.

00:04:26.346 --> 00:04:28.926
And this runs just fine
on whatever device you have.

00:04:30.966 --> 00:04:34.299
And together with this
<i>enable_eager_execution</i> thing,

00:04:34.299 --> 00:04:37.073
we're also bringing you
a few symbols in TensorFlow

00:04:37.073 --> 00:04:40.192
that make it easier for you
to write code that's going to work,

00:04:40.192 --> 00:04:42.535
both when building graphs
and when we're executing eagerly.

00:04:43.015 --> 00:04:46.591
And we're also bringing you
a new way of doing gradients,

00:04:46.591 --> 00:04:49.146
because I'm sure you're familiar now
with how we do gradients

00:04:49.146 --> 00:04:50.577
in normal TensorFlow.

00:04:50.577 --> 00:04:53.323
Where you just create your variable,
you create your loss function,

00:04:53.323 --> 00:04:56.278
and I hope you can think of a better
loss function than this one.

00:04:57.438 --> 00:05:00.495
And then you call <i>tf.gradients</i>
to differentiate it.

00:05:01.325 --> 00:05:05.839
But when you have <i>eager execution</i>, we try
to be as efficient as we possibly can

00:05:05.839 --> 00:05:07.849
And if you're going to...
one thing to think about is

00:05:07.849 --> 00:05:09.449
that if you're going to differentiate
a computation,

00:05:09.449 --> 00:05:12.381
you need to keep track
in memory of information

00:05:12.381 --> 00:05:14.091
about what happened so far,

00:05:14.091 --> 00:05:16.006
like your activations
and things like that.

00:05:16.006 --> 00:05:19.116
But I don't want you to pay
for the cost of this tracking

00:05:19.116 --> 00:05:22.128
when you're not computing gradients
because performance is really

00:05:22.128 --> 00:05:23.587
like the whole reason why we're doing this

00:05:23.587 --> 00:05:25.903
is because we want to use these big,
nice pieces of hardware

00:05:25.903 --> 00:05:27.633
that train models super fast.

00:05:27.633 --> 00:05:31.227
So, when <i>eager execution</i> is enabled,

00:05:31.227 --> 00:05:34.478
when you want to compute gradients,
and you use this little context manager

00:05:34.478 --> 00:05:36.680
to keep a tape active,
and the tape just records

00:05:36.680 --> 00:05:39.310
all the operations you execute,
so we can play it back

00:05:39.310 --> 00:05:42.404
when you've computed the gradients.
Otherwise, the API is the same.

00:05:44.061 --> 00:05:46.100
Also, writing training loops in <i>eager</i>,

00:05:46.100 --> 00:05:48.449
as Derrick pointed out,

00:05:48.449 --> 00:05:49.559
is much...

00:05:50.159 --> 00:05:52.033
is very easy and straightforward.

00:05:52.033 --> 00:05:53.604
You can just use a <i>Python for loop</i>

00:05:53.604 --> 00:05:57.034
to iterate over your data sets
and data sets work in <i>eager</i> just fine.

00:05:57.365 --> 00:05:59.546
And they work with the same high
performance you get

00:05:59.546 --> 00:06:01.388
in the graph execution engine.

00:06:01.388 --> 00:06:03.480
Then you can just do your predictions,

00:06:03.480 --> 00:06:05.449
compute your gradients,
supply your gradients

00:06:05.449 --> 00:06:07.769
and do all the things
you're used to doing.

00:06:08.209 --> 00:06:11.714
But really the interesting thing
about <i>eager execution</i>

00:06:11.714 --> 00:06:14.777
is not when you're just writing
this code as finished,

00:06:14.777 --> 00:06:15.977
that is done,

00:06:15.977 --> 00:06:17.474
that we already know works,

00:06:17.474 --> 00:06:20.545
but when you're still developing,
when you want to do things like debug.

00:06:20.920 --> 00:06:22.249
So...

00:06:23.049 --> 00:06:27.019
when <i>eager execution</i> is enabled,
you can just take any model code--

00:06:27.019 --> 00:06:29.785
and I used my simple,
silly gradient example here--

00:06:29.785 --> 00:06:34.074
add notes to like drop into the <i>Python
debugger</i> anywhere you want.

00:06:34.444 --> 00:06:36.077
And once you're in the <i>Python debugger</i>,

00:06:36.077 --> 00:06:38.627
you have the full power
of debugging available.

00:06:38.627 --> 00:06:41.820
You can print the value of any <i>tensor</i>,
you can change the value of any <i>tensor</i>.

00:06:42.125 --> 00:06:44.974
You can run any operation
you want on any <i>tensor</i>.

00:06:44.974 --> 00:06:47.752
And this will hopefully empower you

00:06:47.752 --> 00:06:50.770
to really understand
what's going on in your models

00:06:50.770 --> 00:06:52.901
and really be able to fix
any problems you have.

00:06:53.751 --> 00:06:57.028
You can also take <i>eager execution</i>
code and profile it,

00:06:57.028 --> 00:07:00.608
using whatever profiling tool you are most
familiar and comfortable with.

00:07:00.608 --> 00:07:03.789
So, here I have a little [inaudible] model
that just does a .<i>matmul</i>

00:07:03.789 --> 00:07:04.973
and a <i>bias_add</i>.

00:07:04.973 --> 00:07:07.641
And let's pretend I don't know
which of these operations

00:07:07.641 --> 00:07:09.763
is going to be the bottleneck.
Which one is lower?

00:07:09.763 --> 00:07:12.806
And I'm sure you all know the answer
that the <i>matmul</i> is a lot more expensive.

00:07:12.806 --> 00:07:15.417
But here you can just run your code
through your <i>Python profiler</i>

00:07:15.417 --> 00:07:17.586
like you would do with any
other programming job,

00:07:17.586 --> 00:07:20.328
and find out that the <i>matmul</i>
is like 15 times more expensive

00:07:20.328 --> 00:07:23.963
for my batch size here
than my <i>bias</i> addition.

00:07:25.383 --> 00:07:27.895
And also,

00:07:27.895 --> 00:07:30.872
by the way those examples are run
on the Google collaboratory thing,

00:07:30.872 --> 00:07:32.017
which is this...

00:07:32.857 --> 00:07:36.685
completely public-shared,
GPU-capable interface

00:07:36.685 --> 00:07:39.767
for Jupiter notebooks
that are like hosted on Google Cloud.

00:07:39.767 --> 00:07:42.443
It's pretty cool and I think
we have a demo on <i>eager</i>

00:07:42.443 --> 00:07:44.603
that's hosted on that.
And you can play out with later,

00:07:44.603 --> 00:07:46.818
or if you're on livestream,
you can play out of it now

00:07:46.818 --> 00:07:48.098
if you can find the link.

00:07:49.008 --> 00:07:50.175
But together with <i>eager</i>,

00:07:50.175 --> 00:07:51.972
we're bringing you a lot of new APIs

00:07:51.972 --> 00:07:54.252
that make it easier for you
to build TensorFlow graphs

00:07:54.602 --> 00:07:56.672
and to execute models.

00:07:56.672 --> 00:07:58.473
And these APIs are compatible

00:07:58.473 --> 00:08:00.916
with both <i>eager execution</i>
and graph building.

00:08:01.476 --> 00:08:05.023
So one that's been 
a recurring low priority feature request

00:08:05.023 --> 00:08:07.269
is how to customize gradients 
in TensorFlow.

00:08:07.269 --> 00:08:10.382
And I'm sure you are familiar with a few
of the tricks that people have,

00:08:10.382 --> 00:08:12.783
like <i>stop gradients</i> and <i>functions</i>
and things like that.

00:08:13.223 --> 00:08:17.022
But we're introducing a new API that works
in both <i>eager</i> and graph execution

00:08:17.022 --> 00:08:20.275
And what I like about this example is that
it's a thing that's been asked by many,

00:08:20.275 --> 00:08:21.802
many people, how to do it.

00:08:21.802 --> 00:08:23.784
If I want to run my forward pass

00:08:23.784 --> 00:08:27.115
and then in the backward pass,
take the <i>gradient</i> of a particular <i>tensor</i>

00:08:27.115 --> 00:08:31.213
and clip it, clip it's <i>norm</i> to keep it
small to prevent it from exploding.

00:08:31.213 --> 00:08:32.960
And it just takes six lines of code

00:08:32.960 --> 00:08:35.695
to make a version of <i>tf.identity</i>
that in the backward pass

00:08:35.695 --> 00:08:37.702
clips its gradient,
and I think this is really cool.

00:08:37.702 --> 00:08:40.031
And I look forward to seeing
what you guys can do with this

00:08:40.031 --> 00:08:41.801
when you're doing
more than six lines of code

00:08:41.801 --> 00:08:44.790
and solving all sorts of new,
interesting research problems.

00:08:46.280 --> 00:08:50.811
A big, big change when programming
with <i>eager</i> from <i>graph</i>

00:08:50.811 --> 00:08:52.739
that I really want you to stop
and think about

00:08:52.739 --> 00:08:55.879
is that we're trying
to make everything as pythonic

00:08:55.879 --> 00:08:57.776
and object-oriented as possible.

00:08:58.176 --> 00:09:01.415
So, variables in TensorFlow are...

00:09:02.535 --> 00:09:05.155
usually are a complicated thing 
to think about.

00:09:05.155 --> 00:09:07.509
But when <i>eager execution</i> is enabled,
it's much simpler.

00:09:07.509 --> 00:09:09.932
A TensorFlow variable
is just a <i>Python</i> object.

00:09:09.932 --> 00:09:11.316
You create one, you have it.

00:09:11.316 --> 00:09:14.173
You can write, you can change its value,
you can read its value.

00:09:14.173 --> 00:09:16.965
When the last reference to it goes away,
you get your memory back,

00:09:16.965 --> 00:09:18.665
even if it's your GPU memory.

00:09:19.135 --> 00:09:22.935
So, if you want to share <i>variables</i>,
you just reuse those objects.

00:09:22.935 --> 00:09:26.147
You don't worry about <i>variable</i> scopes
or any other complicated structure.

00:09:26.967 --> 00:09:31.260
And because we have this
object-oriented approach to variables,

00:09:31.260 --> 00:09:34.100
we can look at some
of the APIs in TensorFlow

00:09:34.100 --> 00:09:37.510
and like rethink them in a way
that's a little more object-oriented

00:09:37.510 --> 00:09:38.824
and easier to use.

00:09:38.824 --> 00:09:39.936
And a very...

00:09:39.936 --> 00:09:42.599
one that really stood out to us
as needing an overhaul

00:09:42.599 --> 00:09:44.155
was the <i>metrics</i> API.

00:09:44.155 --> 00:09:48.130
So, we're introducing
this new <i>tfe.metrics</i> package,

00:09:48.130 --> 00:09:50.287
where each metric has two methods,

00:09:50.287 --> 00:09:53.642
one that updates the value
and one that gives you the result.

00:09:53.642 --> 00:09:57.009
And hopefully, this is an API that everyone
is going to find familiar to use

00:09:57.009 --> 00:10:01.038
and please don't try to compare
this with the other metrics API.

00:10:01.038 --> 00:10:02.025
(laughs)

00:10:02.275 --> 00:10:06.726
We're also giving you a way to do
object-oriented saving

00:10:06.726 --> 00:10:08.015
of TensorFlow models.

00:10:08.015 --> 00:10:10.169
If you've tried looking
at TensorFlow checkpoints now,

00:10:10.169 --> 00:10:12.129
you know that they depend
on variable names.

00:10:12.129 --> 00:10:15.085
And variable names depend not just
on the name you gave to a variable,

00:10:15.085 --> 00:10:18.160
but on all other variables
which are present in your graph.

00:10:18.160 --> 00:10:21.640
This can make it a little hard for you
to save and load subsets of your model

00:10:21.640 --> 00:10:23.742
and really control
what's in your checkpoint.

00:10:23.742 --> 00:10:26.473
So we're introducing
a completely object-oriented

00:10:27.193 --> 00:10:29.753
<i>Python</i> object-based saving API,

00:10:29.753 --> 00:10:32.164
where you...

00:10:32.164 --> 00:10:34.491
it's like <i>Python pickle</i>,
like any variable that's reachable

00:10:34.491 --> 00:10:36.872
from your model gets saved
and your model gets saved.

00:10:36.872 --> 00:10:38.875
You can save any subset of your model.

00:10:38.875 --> 00:10:41.141
You can load any subset of your model.

00:10:41.141 --> 00:10:45.453
You can even use this <i>tfe.checkpoint</i>
object to build things you want to save

00:10:45.453 --> 00:10:46.584
that have more than a model.

00:10:46.584 --> 00:10:48.816
And here we have an optimizer
and a <i>global_step</i>

00:10:48.816 --> 00:10:51.168
but really you can put
whatever you want in there.

00:10:51.548 --> 00:10:55.917
And the idea is that this object graph
that eventually goes down to variables

00:10:55.917 --> 00:10:58.449
is something you can save and load.
So you can have your [GAN]

00:10:58.449 --> 00:11:00.697
and save and load your discriminators
and your generators

00:11:00.697 --> 00:11:01.704
separate from each other.

00:11:01.704 --> 00:11:03.870
Then you can take your discriminator
and load it backup

00:11:03.870 --> 00:11:06.911
as like another newer network that you
can use in another part of a model.

00:11:06.911 --> 00:11:11.782
And this should give you a lot more
control to get a lot more out

00:11:12.092 --> 00:11:13.541
of TensorFlow checkpointing.

00:11:14.211 --> 00:11:16.816
But the real question that everybody
asks me when I tell them

00:11:16.816 --> 00:11:19.154
that I work on <i>eager execution</i>
is, Is it fast?

00:11:19.154 --> 00:11:20.769
Because graphs...

00:11:20.769 --> 00:11:23.177
have this high performance promise.

00:11:23.177 --> 00:11:26.229
So, how fast can it make this thing
that runs <i>Python</i> code all the time?

00:11:26.229 --> 00:11:28.445
And the answer is
that we can make it fast enough.

00:11:28.445 --> 00:11:31.873
For models that are highly
computationally intensive,

00:11:31.873 --> 00:11:34.300
you pretty much don't see
any <i>Python</i> overhead,

00:11:34.300 --> 00:11:36.290
and we're as fast as graph TensorFlow.

00:11:37.360 --> 00:11:38.613
For...

00:11:38.613 --> 00:11:43.145
sometimes slightly faster,
in reasons that I don't fully understand,

00:11:43.145 --> 00:11:45.293
even for highly dynamic models,

00:11:45.293 --> 00:11:50.025
you have comparative performance
with anything else you can find.

00:11:50.465 --> 00:11:53.562
And please don't get attached
to these numbers.

00:11:53.562 --> 00:11:55.684
We have many more benchmarks
in our codebase.

00:11:55.684 --> 00:11:58.330
And we're optimizing <i>eager</i>
performance very aggressively,

00:11:58.330 --> 00:12:01.431
but I hope that the message you'll get
out of this is that if your model

00:12:01.431 --> 00:12:02.496
can keep a GPU busy,

00:12:02.496 --> 00:12:05.304
if you're doing large convolusions,
large matrix multiplications,

00:12:05.604 --> 00:12:09.048
there is almost no cost in experimenting
and doing your research and model building

00:12:09.048 --> 00:12:10.815
with <i>eager execution</i> turned on.

00:12:11.305 --> 00:12:13.332
But when you're doing smaller things,

00:12:13.332 --> 00:12:15.388
there are some overheads
and I want to go over them.

00:12:15.388 --> 00:12:18.211
But again don't get attached to them
because we're being very aggressive

00:12:18.211 --> 00:12:19.560
about optimizing this.

00:12:19.560 --> 00:12:21.492
If you just run a <i>no op</i> in TensorFlow,

00:12:21.492 --> 00:12:22.663
like an identity,

00:12:22.663 --> 00:12:24.576
it takes almost a microsecond
to execute it.

00:12:24.576 --> 00:12:26.904
If you run that with <i>eager
execution</i> turned on,

00:12:26.904 --> 00:12:29.018
there's an extra microsecond of overhead.

00:12:29.018 --> 00:12:30.642
If you're tracing gradients,

00:12:30.642 --> 00:12:33.620
there are another 3 microseconds
of overhead that you get.

00:12:33.620 --> 00:12:36.886
But if you're just enqueuing
something on the GPU stream,

00:12:36.886 --> 00:12:40.585
that alone takes like
single-digit microsecond.

00:12:40.585 --> 00:12:45.963
So, if you can execute enough
computation to keep a GPU busy,

00:12:45.963 --> 00:12:49.640
you're unlikely to see anything bad
from using <i>eager execution</i>,

00:12:49.640 --> 00:12:51.910
and again these numbers
are improving very quickly.

00:12:52.520 --> 00:12:54.096
Please don't get too attached to them.

00:12:54.876 --> 00:13:00.410
But there is this large ecosystem
of TensorFlow code libraries,

00:13:00.410 --> 00:13:02.979
models, frameworks, checkpoints

00:13:02.979 --> 00:13:05.453
that I don't think anyone
wants to give up.

00:13:05.453 --> 00:13:08.126
And I don't want you to give up
if you want to use<i> eager execution</i>.

00:13:08.506 --> 00:13:10.177
So, we're also thinking really hard

00:13:10.177 --> 00:13:13.276
about how you can interoperate
between <i>eager</i> and <i>graph</i>.

00:13:14.356 --> 00:13:17.611
One way is to like call
into <i>graphs</i> from <i>eager</i> code.

00:13:17.611 --> 00:13:20.518
And you can do that with <i>tfe.make_template</i>

00:13:20.518 --> 00:13:22.387
which has this create
graph <i>function</i> argument

00:13:22.387 --> 00:13:24.611
when you pass through
to it with build a graph,

00:13:24.611 --> 00:13:26.630
for that little <i>Python function</i>
that you wrote.

00:13:26.630 --> 00:13:30.795
And then you can use it an manipulate
and call the graph from <i>eager execution</i>.

00:13:31.185 --> 00:13:32.966
We also have the reverse,

00:13:32.966 --> 00:13:36.116
which is how to call
into <i>eager</i> from a graph.

00:13:36.416 --> 00:13:39.094
Let's say you have a big graph
that you understand everything in it,

00:13:39.094 --> 00:13:41.084
but there's a little chunk
of your computation

00:13:41.084 --> 00:13:43.402
that you really don't know how to express.

00:13:43.402 --> 00:13:46.983
And either you don't know,
or you don't want to bother expressing it,

00:13:46.983 --> 00:13:49.358
in using like TensorFlow graphs.

00:13:49.358 --> 00:13:51.156
So you can wrap it in a <i>tfe.py_func</i>

00:13:51.156 --> 00:13:54.643
and what you get in there
when the <i>Python function</i> is executing

00:13:54.643 --> 00:13:57.956
are <i>eager tensors</i>, that you can
run any TensorFlow op in,

00:13:57.956 --> 00:14:00.966
including convolutions and other things
that are not available in <i>my_py</i>.

00:14:00.966 --> 00:14:02.380
But you can also look at the values

00:14:02.380 --> 00:14:04.593
and inspect and use dynamic
control flow in there.

00:14:05.023 --> 00:14:07.195
So, I hope with these two things together,

00:14:07.195 --> 00:14:10.135
you can really reuse <i>eager</i>
and <i>graph</i> code across.

00:14:10.135 --> 00:14:13.847
But really the easiest way
to get <i>eager</i> and <i>graph</i> compatibility

00:14:13.847 --> 00:14:17.524
is to just write model code
that's going to work in both ways.

00:14:17.524 --> 00:14:18.781
And if you think about it,

00:14:18.781 --> 00:14:22.404
once your model is fully written,
debugged and [tested],

00:14:23.134 --> 00:14:27.829
there's not much there that tells you
whether you need to build a graph

00:14:27.829 --> 00:14:29.345
or to execute <i>eagerly</i>.

00:14:30.145 --> 00:14:32.344
So write, iterate, debug in <i>eager</i>

00:14:32.344 --> 00:14:35.046
and then import that same code
into a graph,

00:14:35.046 --> 00:14:37.184
put it in <i>Estimator,</i>
deploy it on a TPU pod,

00:14:37.184 --> 00:14:38.720
deploy it on a GPU,

00:14:38.720 --> 00:14:40.448
distribute it and do whatever you want.

00:14:40.448 --> 00:14:43.325
And like this is what we've done
in our example models

00:14:43.325 --> 00:14:46.019
and there's going to be a link
in the end of the presentation,

00:14:46.019 --> 00:14:48.299
so you don't need to like worry
about writing this down.

00:14:49.199 --> 00:14:51.290
So, here is some practical advice for you.

00:14:53.020 --> 00:14:55.042
Write code that's going to work well

00:14:55.042 --> 00:14:57.437
when executing <i>eagerly</i>
and when building graphs.

00:14:57.437 --> 00:14:59.892
And to do that, use
the <i>Keras layers.</i> They're great.

00:14:59.892 --> 00:15:01.891
They're object-oriented;
they're pythonic.

00:15:01.891 --> 00:15:04.602
They're easy to understand,
manipulate and play around with.

00:15:04.602 --> 00:15:07.279
Use the <i>Keras model</i>
to stitch those layers together,

00:15:07.279 --> 00:15:10.780
that will guide you in saving and loading
and training and all sorts of things

00:15:10.780 --> 00:15:13.398
automatically if you want,
but you're not forced to use those.

00:15:13.398 --> 00:15:15.843
Use <i>tf.contrib.summary</i>
instead of <i>tf.summary</i>.

00:15:15.843 --> 00:15:18.872
They will move to the tensor board 
open source package very soon.

00:15:18.872 --> 00:15:20.408
So, if you're watching this on video,

00:15:20.408 --> 00:15:21.998
it probably already happened.

00:15:22.918 --> 00:15:25.550
Use the <i>tfe.metrics</i>
instead of the <i>tf.metrics</i>,

00:15:25.550 --> 00:15:27.910
because these are object-oriented,
friendlier to use,

00:15:27.910 --> 00:15:29.400
and friendlier in <i>eager</i>.

00:15:29.940 --> 00:15:31.896
And use the object-based saving,

00:15:31.896 --> 00:15:36.005
which is a much nicer
user experience anyway.

00:15:36.005 --> 00:15:38.202
So, I hope you're going to want
to do this all the time.

00:15:38.202 --> 00:15:39.776
If you do all of this,

00:15:39.776 --> 00:15:42.221
it's highly likely your code
is going to work super well

00:15:42.221 --> 00:15:44.618
in both <i>eager execution</i>
and graph building.

00:15:45.428 --> 00:15:47.418
So now, I'd like to take some time

00:15:47.418 --> 00:15:50.598
to tell you why you should
enable <i>eager execution</i>.

00:15:50.968 --> 00:15:52.541
You know like a real good...

00:15:52.541 --> 00:15:55.648
important reason for us that led us
to build this in the first place

00:15:55.648 --> 00:15:57.329
is that if you're new to machine learning,

00:15:57.329 --> 00:15:59.289
or you're new to TensorFlow
and you want to learn,

00:15:59.289 --> 00:16:02.343
being able to play with these objects
and manipulate them directly

00:16:02.753 --> 00:16:06.386
is just a much nicer experience
than having to build a graph

00:16:06.386 --> 00:16:08.238
and interact with later in a session.

00:16:08.238 --> 00:16:09.803
It's a lot more intuitive,

00:16:09.803 --> 00:16:11.810
lets you understand
what's going on much better.

00:16:11.810 --> 00:16:15.577
So I've shown you, just by all means
go straight into your execution,

00:16:15.577 --> 00:16:16.692
play around with it

00:16:16.692 --> 00:16:19.905
and figure out how to get graphs later.

00:16:20.655 --> 00:16:24.620
Also, if you're a researcher
and you're quickly iterating over models.

00:16:24.620 --> 00:16:27.011
You're changing their internal
properties and...

00:16:27.011 --> 00:16:30.354
you're comparing them and you're
trying to do models that are non trivial

00:16:30.354 --> 00:16:33.100
that we in the TensorFlow team 
were not thinking about

00:16:33.100 --> 00:16:34.669
when we designed TensorFlow.

00:16:34.669 --> 00:16:37.790
<i>Eager execution</i> will make it much easier
for you to understand what's going on,

00:16:37.790 --> 00:16:39.170
to debug what's going on,

00:16:39.170 --> 00:16:41.958
to be productive in advance.

00:16:41.958 --> 00:16:45.452
So, if you're a researcher
this is for you.

00:16:45.452 --> 00:16:49.780
Also, if your model's not working
and you want to understand why,

00:16:49.780 --> 00:16:53.490
being able to enable <i>eager execution</i>
and then [start through] it in a debugger,

00:16:53.490 --> 00:16:55.364
change some values, play around with it.

00:16:55.364 --> 00:16:59.907
Understand that is priceless,
and that has saved me a lot of time.

00:17:00.987 --> 00:17:04.676
Similarly, if you want to profile
a model using like the full power

00:17:04.976 --> 00:17:08.707
of whatever tool you like
to use to profile <i>Python</i>,

00:17:08.707 --> 00:17:10.243
<i>eager execution</i> is your friend.

00:17:10.613 --> 00:17:13.805
Also, there are some models 
like recursive RNN's

00:17:13.805 --> 00:17:16.241
that are just much easier to express

00:17:16.241 --> 00:17:18.479
if you don't need to put
your entire computation

00:17:18.479 --> 00:17:20.054
in a static data flow graph.

00:17:20.054 --> 00:17:21.734
If you're working on one of those models,

00:17:22.394 --> 00:17:24.466
<i>eager execution</i> is also a choice for you.

00:17:24.466 --> 00:17:27.882
But really the reason
I think you should enable this

00:17:27.882 --> 00:17:29.428
is that it's fun.

00:17:29.428 --> 00:17:33.016
It's a very nice and intuitive way
of interacting with TensorFlow,

00:17:33.016 --> 00:17:35.712
and I hope you're going to have
a lot of fun experimenting with it.

00:17:35.712 --> 00:17:38.830
So now, I would like
to point to a few things.

00:17:38.830 --> 00:17:40.917
Some of my colleagues,
sitting over there now,

00:17:40.917 --> 00:17:43.759
they're going to be in the demo
room during the break

00:17:43.759 --> 00:17:45.873
with laptops, with [collabs]

00:17:45.873 --> 00:17:47.193
that are like Jupiter notebooks

00:17:47.193 --> 00:17:49.327
to let you type and try
<i>eager</i> mode there.

00:17:49.327 --> 00:17:50.652
Please go give it a try.

00:17:50.652 --> 00:17:52.878
Or if you're watching this
on the livestream,

00:17:54.268 --> 00:17:55.565
type that short link--

00:17:55.565 --> 00:17:58.124
hopefully it will stay on the screen
long enough for you to type it--

00:17:58.444 --> 00:17:59.967
and play with it right now.

00:18:00.907 --> 00:18:01.981
It's really nice.

00:18:01.981 --> 00:18:04.198
We have a <i>Getting Started
Guide</i> on TensorFlow

00:18:04.198 --> 00:18:05.879
that should be live now.

00:18:05.879 --> 00:18:07.549
<i>programmers_guide/eager</i>

00:18:07.549 --> 00:18:09.846
that tells you what you need
to know about <i>eager execution</i>

00:18:09.846 --> 00:18:12.398
and what you need to know
about starting to use TensorFlow

00:18:12.398 --> 00:18:13.856
using <i>eager execution</i>.

00:18:14.156 --> 00:18:15.836
We also have a ton of example models,

00:18:15.836 --> 00:18:19.401
like from RNNs to [inaudible]
to all sorts of things,

00:18:19.771 --> 00:18:21.868
that are available behind that link.

00:18:21.868 --> 00:18:23.963
And I encourage you
to look at them and see

00:18:23.963 --> 00:18:25.522
how it's easy to write the model

00:18:25.522 --> 00:18:29.686
and how easy it is to also reuse
the same code from graphs

00:18:29.686 --> 00:18:30.954
for deployment.

00:18:31.754 --> 00:18:33.986
We have deployment
for graphs for all models

00:18:33.986 --> 00:18:35.573
except for the highly dynamic ones

00:18:35.573 --> 00:18:38.485
that are just really hard
to write in a graph form.

00:18:38.485 --> 00:18:39.499
And give it a try.

00:18:39.499 --> 00:18:40.797
If you give it a try,

00:18:40.797 --> 00:18:42.129
let us know how it went.

00:18:42.559 --> 00:18:44.409
We're super excited
to share this with you.

00:18:44.409 --> 00:18:46.823
I hope you're going to have
a great time playing with this.

00:18:46.823 --> 00:18:48.622
And, yes.

00:18:48.622 --> 00:18:50.080
Thank you.

00:18:50.080 --> 00:18:52.828
(clapping)

00:18:54.484 --> 00:18:59.561
♪ (music) ♪

