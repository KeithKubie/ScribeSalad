WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.410
♪ (music) ♪

00:00:07.370 --> 00:00:09.749
Well, thank you everybody for being here.

00:00:09.749 --> 00:00:17.170
Today we're going to be giving a talk
about the new TensorFlow debugger,

00:00:17.170 --> 00:00:19.709
which comes included with TensorBoard,

00:00:19.709 --> 00:00:25.410
and it's basically a debugger
like you would see in an IDE,

00:00:25.410 --> 00:00:27.870
that lets you step in,
set breakpoints and models,

00:00:27.870 --> 00:00:29.910
and watch tensors.

00:00:29.910 --> 00:00:31.980
But before we do that,

00:00:31.980 --> 00:00:34.229
I'd like to give you some background
on TensorBoard,

00:00:34.229 --> 00:00:37.490
and some of the other developments
that happened in the last year,

00:00:37.490 --> 00:00:41.140
which we unfortunately
don't have too much time to go into,

00:00:42.380 --> 00:00:47.910
but TensorBoard is basically
a web application--

00:00:47.920 --> 00:00:54.330
it's a suite of web applications--
that was authored by about 20 people,

00:00:54.330 --> 00:00:58.110
and it's all packed into a two megabyte
command-line web server

00:00:58.110 --> 00:00:59.870
that works offline.

00:01:01.410 --> 00:01:04.359
And TensorBoard can be used
for many purposes,

00:01:04.359 --> 00:01:07.080
with all the different plugins
that have been baked into it.

00:01:07.080 --> 00:01:09.330
The one you're all
probably most familiar with,

00:01:09.330 --> 00:01:11.439
for those of you
who have used TensorBoard,

00:01:11.439 --> 00:01:13.859
is the Scalars Dashboard.

00:01:13.859 --> 00:01:15.719
You can plot anything you want.

00:01:15.719 --> 00:01:20.610
It could be loss curves, accuracy, etc.

00:01:20.610 --> 00:01:23.600
And these things sort of
help us understand

00:01:23.600 --> 00:01:27.279
whether or not our model is converging
on optimal solutions.

00:01:27.279 --> 00:01:31.450
And here is a really interesting,
underutilized feature,

00:01:31.800 --> 00:01:34.730
called the embedding projector.

00:01:34.730 --> 00:01:36.659
And this was originally written by Google,

00:01:36.659 --> 00:01:41.571
so we could do things like
project our data into 3D space,

00:01:41.571 --> 00:01:42.790
see how things cluster,

00:01:42.790 --> 00:01:44.071
like if you're doing MNIST,

00:01:44.071 --> 00:01:47.070
the sevens go over here,
and the nines go there.

00:01:47.070 --> 00:01:48.749
And we actually recently...

00:01:48.749 --> 00:01:52.810
What you see on this screen is,
we got a really cool contribution

00:01:52.810 --> 00:01:56.300
from Francois Luus at IBM Research,

00:01:56.300 --> 00:01:59.420
and he sent us some pull requests
on our GitHub repository,

00:01:59.420 --> 00:02:02.182
since we develop in the open,

00:02:02.182 --> 00:02:06.851
and what he did was, he basically added
interactive label editing,

00:02:06.851 --> 00:02:13.100
so you can sort of go in there
and change things,

00:02:13.100 --> 00:02:15.420
algorithms like t-SNE,

00:02:15.420 --> 00:02:18.920
give your data-- sort of reveal
the structure of your data,

00:02:18.920 --> 00:02:21.030
to learn more, search Google for:

00:02:21.030 --> 00:02:24.490
"Interactive Supervision
with TensorBoard."

00:02:24.490 --> 00:02:29.520
This is another really amazing
contribution that we received

00:02:29.520 --> 00:02:32.450
from a university student
named Chris Anderson.

00:02:32.459 --> 00:02:35.069
It's called the Beholder plugin.

00:02:35.069 --> 00:02:39.140
And this basically gives you
a real-time visual glimpse

00:02:39.140 --> 00:02:41.260
into TensorFlow data structures.

00:02:41.260 --> 00:02:43.540
Like, for example,
as your training script is running,

00:02:43.540 --> 00:02:47.459
it's real time-- and it does
require a hard drive, by the way.

00:02:47.459 --> 00:02:51.610
It doesn't work with something like GCS
at this point in time.

00:02:51.610 --> 00:02:55.689
I think this could be
a very useful tool going forward,

00:02:55.689 --> 00:02:58.870
in terms of model explainability.

00:02:59.410 --> 00:03:03.780
Now, TensorBoard also has some
new plugins for optimization.

00:03:03.780 --> 00:03:08.730
Cloud recently contributed
a TPU profiling plugin.

00:03:08.730 --> 00:03:13.890
And TPU hardware is a little different
from what many of you might be used to,

00:03:13.890 --> 00:03:16.320
And TensorBoard with this plugin

00:03:16.320 --> 00:03:18.970
can really help you
get the most out of your hardware,

00:03:18.970 --> 00:03:22.540
and ensure that it's being
properly utilized.

00:03:23.380 --> 00:03:25.960
Now, the TensorBoard ecosystem...

00:03:25.960 --> 00:03:29.070
Part of the goal of this talk,
before we get into the demo,

00:03:29.070 --> 00:03:33.129
is I want to attract more folks
in the community

00:03:33.129 --> 00:03:36.119
to get involved
with TensorBoard development.

00:03:36.119 --> 00:03:38.530
We use many of the tools
you're familiar with,

00:03:38.530 --> 00:03:40.470
such as Typescript and Polymer.

00:03:40.470 --> 00:03:43.679
We also use some tools
you might not be familiar with,

00:03:43.679 --> 00:03:47.280
like Bazel, but we use it
for very good reasons.

00:03:47.280 --> 00:03:49.109
You can also explore our readmes,

00:03:49.109 --> 00:03:52.949
if you go to that folder for all
the different plugins we wrote originally.

00:03:52.949 --> 00:03:55.130
Now, with TensorBoard,

00:03:55.130 --> 00:04:00.039
the reason why this is just a little bit
more challenging

00:04:00.039 --> 00:04:02.530
compared to some of
the other web applications

00:04:02.530 --> 00:04:05.350
you may have used in the past,
or written in the past,

00:04:05.350 --> 00:04:08.499
is we deal with
very challenging requirements,

00:04:08.499 --> 00:04:12.039
like, this thing needs to work offline;

00:04:12.039 --> 00:04:19.980
it needs to be able to build regardless 
of corporate or national firewalls

00:04:19.980 --> 00:04:22.720
that may block certain URLs
when it's downloading things.

00:04:22.720 --> 00:04:26.070
For example, one of the first things I did
when I joined the TensorBoard team,

00:04:26.070 --> 00:04:28.289
wasn't actually
visualizing machine learning,

00:04:28.289 --> 00:04:31.620
but rather adding a contribution to Bazel

00:04:31.620 --> 00:04:36.240
which helps downloads be
carrier-grade internationally.

00:04:37.090 --> 00:04:40.340
And there there are a
whole variety of challenges,

00:04:40.340 --> 00:04:43.340
like when it comes to an application like this,

00:04:43.340 --> 00:04:46.640
but those burdens are things
we've mostly solved for you,

00:04:46.640 --> 00:04:48.980
and here is a concrete example:

00:04:48.980 --> 00:04:54.400
Writing that toilsome thousand-line file
was what it took

00:04:54.400 --> 00:04:58.170
to make TensorBoard look beautiful offline
anywhere in the world,

00:04:58.170 --> 00:05:01.540
without having to ping <i>fonts.google.com,</i>

00:05:01.540 --> 00:05:06.860
and that's just one of the many burdens
that the TensorBoard team carries

00:05:06.860 --> 00:05:09.430
on behalf of plugin authors.

00:05:10.940 --> 00:05:14.430
Now, I want to give you
a quick introduction

00:05:14.430 --> 00:05:19.720
for Xin Zhang who is the author
of this TensorFlow debugger,

00:05:19.720 --> 00:05:23.240
and with the help of Chi Zeng.

00:05:23.240 --> 00:05:26.120
So, as I mentioned earlier,

00:05:26.120 --> 00:05:27.560
TensorBoard has always done
a good job

00:05:27.560 --> 00:05:29.860
being the flashlight
that gives us broad overviews

00:05:29.860 --> 00:05:33.280
of what's happening
inside these black boxes of models.

00:05:33.280 --> 00:05:39.910
What the TensorFlow debugger plugin does
is it turns that flashlight into an X-ray.

00:05:39.910 --> 00:05:47.560
Using this plugin, you can literally watch
the tensors as they flow in real time,

00:05:47.560 --> 00:05:52.430
while having complete control
over the entire process.

00:05:52.760 --> 00:05:56.550
This X-ray is what's going to make it
possible for you to pinpoint problems

00:05:56.550 --> 00:05:59.520
we've previously found
difficult to identify,

00:05:59.520 --> 00:06:01.630
perhaps down to the tiniest NaN,

00:06:01.630 --> 00:06:04.860
at the precise moments when they happen.

00:06:04.860 --> 00:06:07.550
That's why we call it an X-ray.

00:06:07.550 --> 00:06:11.360
It reveals the graph of math
beneath the abstractions we love,

00:06:11.360 --> 00:06:13.510
such as Keras or Estimator,

00:06:13.510 --> 00:06:16.810
or as was recently announced today, Swift.

00:06:16.810 --> 00:06:19.550
Whichever tools you're using,

00:06:19.550 --> 00:06:23.280
this can potentially be a very helpful
troubleshooting tool.

00:06:23.280 --> 00:06:28.340
So, I'd like to introduce its author,
Xin Zhang, who can show you a demo.

00:06:29.950 --> 00:06:31.510
Thank you very much.

00:06:31.510 --> 00:06:33.120
(applause)

00:06:35.469 --> 00:06:38.509
Okay, so, in a moment
the screencast will start.

00:06:41.439 --> 00:06:44.420
Great. Thank You, Justine,
for the generous intro.

00:06:44.420 --> 00:06:46.139
So I'm Xin Zhang,

00:06:46.139 --> 00:06:49.339
and I'm very glad and honored to present
the featured plugin for TensorBoard,

00:06:49.339 --> 00:06:51.380
among many awesome, amazing plugins

00:06:51.380 --> 00:06:53.670
that people have created
for TensorBoard so far.

00:06:53.670 --> 00:06:56.279
So, the debugger plugin, for some of you

00:06:56.279 --> 00:06:59.100
who may know TensorFlow debugger
or tfdbg,

00:06:59.100 --> 00:07:01.229
it's the graphical user interface
of tfdbg,

00:07:01.229 --> 00:07:05.430
and tfdbg has had only a command line
interface until recently.

00:07:05.430 --> 00:07:07.040
So, like the command line interface,

00:07:07.040 --> 00:07:09.350
the debugger plugin will allow you 
to look into

00:07:09.350 --> 00:07:11.690
internals of a run intensive flow model,

00:07:11.690 --> 00:07:17.399
but in a much more intuitive, interactive,
and richer environment in a browser.

00:07:17.399 --> 00:07:21.020
So, in this example, in this talk
I'm going to show you two examples.

00:07:21.020 --> 00:07:25.230
One example of how to use the tool
to understand and probe and visualize

00:07:25.230 --> 00:07:27.899
a working model that doesn't have any bugs in it;

00:07:27.899 --> 00:07:29.490
I'm also going to show you

00:07:29.490 --> 00:07:33.060
how to use the tool to debug a model
with a bug in it,

00:07:33.060 --> 00:07:36.900
so you can see how to use the tool
to get to the root cause of the problems

00:07:36.900 --> 00:07:38.130
and fix them.

00:07:38.130 --> 00:07:40.319
So, first let's take 
a look at the first example.

00:07:40.319 --> 00:07:43.390
And that's the example being shown
on the right part of the screen right now.

00:07:43.390 --> 00:07:47.779
So it's a very simple TensorFlow program
that does some regression,

00:07:47.779 --> 00:07:52.140
using some generated synthetic data,

00:07:52.140 --> 00:07:54.310
and if we run a program in the console,

00:07:54.310 --> 00:07:55.680
we can see its works.

00:07:55.680 --> 00:07:59.610
We can see a constant decrease
in the loss value during training.

00:07:59.610 --> 00:08:01.270
Now, even though the model works,

00:08:01.270 --> 00:08:04.189
we have no knowledge
of how the model works.

00:08:04.189 --> 00:08:06.720
And that's mainly
because in graph mode,

00:08:06.720 --> 00:08:09.680
<i>Session.run</i> is a black box
that wraps all the computation

00:08:09.680 --> 00:08:12.130
in one single line of Python code.

00:08:12.130 --> 00:08:14.419
Now, what if we want to look
into the model?

00:08:14.419 --> 00:08:17.110
What if we want to look at the matrix
multiplication results in a dense layer?

00:08:17.110 --> 00:08:19.999
And what if we want to look
at the gradient

00:08:19.999 --> 00:08:22.790
on the matmul operation, and so forth?

00:08:22.790 --> 00:08:25.989
The TensorFlow debugger,
or TensorBoard debugger plugin,

00:08:25.989 --> 00:08:27.960
is a tool that can allow you to do that.

00:08:27.960 --> 00:08:30.590
So, to start the tool, we start the usual
TensorBoard binary,

00:08:30.590 --> 00:08:32.390
with a special flag, debugger port.

00:08:32.390 --> 00:08:34.919
So, in this case we specify
the port to be 7,000.

00:08:34.919 --> 00:08:36.619
Now, once the binary is running,

00:08:36.619 --> 00:08:39.819
we can navigate to our TensorBoard URL
in the browser.

00:08:39.819 --> 00:08:43.229
Now, at the startup, the plugin tells you

00:08:43.229 --> 00:08:45.660
that it's waiting for connections
from TensorFlow <i>Session.run,</i>

00:08:45.660 --> 00:08:47.910
and that's because we haven't
started the program yet.

00:08:47.910 --> 00:08:50.499
It also provides you code snippets
that you can use,

00:08:50.499 --> 00:08:54.050
whether you are using <i>tf.Session,</i>
Estimators or Keras models,

00:08:54.050 --> 00:08:56.730
and in this example 
we're using <i>tf.Session,</i>

00:08:56.730 --> 00:09:00.180
so we're going to copy and paste
two lines of code into our program.

00:09:00.180 --> 00:09:02.230
The first line is an import line,

00:09:02.230 --> 00:09:05.609
and the second line is a line that
wraps your original <i>tf.Session</i> objects

00:09:05.609 --> 00:09:07.739
with a special wrapper 
that has the information

00:09:07.739 --> 00:09:11.070
of where to connect the port number.

00:09:11.070 --> 00:09:14.949
Now, with our programming
instrumented,

00:09:14.949 --> 00:09:16.349
we can start the program again.

00:09:16.349 --> 00:09:18.470
Now, as soon as the program starts,

00:09:18.470 --> 00:09:21.750
we can see the graphical user interface
in the browser, switched to a mode

00:09:21.750 --> 00:09:26.240
that shows you the graph 
of the sessions are run in two ways:

00:09:26.240 --> 00:09:27.630
In a tree view on the left,

00:09:27.630 --> 00:09:29.759
and in a graph on the right.

00:09:29.759 --> 00:09:31.750
In the bottom left corner,
you can also see what

00:09:31.750 --> 00:09:33.660
<i>Session.run</i> is currently executing.

00:09:33.660 --> 00:09:37.650
The tree structure on the right
corresponds to namescopes in your model.

00:09:37.650 --> 00:09:43.010
For example, the dense namescope
corresponds to the dense layer.

00:09:43.010 --> 00:09:45.929
One thing you can do,
is to open a source code on view

00:09:45.929 --> 00:09:48.120
to look at the correspondence
between your graph nodes,

00:09:48.120 --> 00:09:52.360
and which lines of the Python program
created those nodes.

00:09:52.360 --> 00:09:54.020
If you click matmul, for example,

00:09:54.020 --> 00:09:56.490
you can see the corresponding node
in the graph view.

00:09:56.490 --> 00:09:58.630
You can also see which line
of the Python source code

00:09:58.630 --> 00:10:00.550
is responsible for creating that node.

00:10:00.550 --> 00:10:03.340
In this case, it's where we call
dense layer, as expected.

00:10:04.370 --> 00:10:07.340
If you click another,

00:10:07.340 --> 00:10:08.940
if you click the last answer,

00:10:08.940 --> 00:10:10.769
you will see the corresponding node
in the graph again,

00:10:10.769 --> 00:10:15.660
and you can also see where it's created
in the Python source code,

00:10:15.660 --> 00:10:19.029
and it's where we call 
the mean squared error.

00:10:20.269 --> 00:10:25.099
And the gradients namescope corresponds to
the backpropagation parts of the model,

00:10:25.099 --> 00:10:26.730
so that's how the model's trained.

00:10:26.730 --> 00:10:28.250
You can click around, poke around,

00:10:28.250 --> 00:10:34.810
and explore how a TensorFlow model
does optimization and backpropagation,

00:10:34.810 --> 00:10:36.140
if you are interested.

00:10:36.140 --> 00:10:37.520
And these nodes are all created

00:10:37.520 --> 00:10:39.959
when we call
gradient descent optimizer.

00:10:39.959 --> 00:10:42.920
You can continue to any node of a graph
and pause there.

00:10:42.920 --> 00:10:47.139
So, we have just continued
to the matmul node in the dense layer.

00:10:47.139 --> 00:10:51.240
You can also continue to the gradient
on the matmul node, which we just did.

00:10:51.240 --> 00:10:53.810
And in the bottom right
corner of the screen,

00:10:53.810 --> 00:10:56.980
you're looking at succinct summaries
of the tensor values we have continued to,

00:10:56.980 --> 00:10:59.360
you can look at their datatype,
their shape,

00:10:59.360 --> 00:11:02.100
and also the range of their values.

00:11:02.100 --> 00:11:08.150
So, in the health pills, you can look 
at how many of those values

00:11:08.150 --> 00:11:10.570
are zero, or negative, or positive,
and so forth.

00:11:10.570 --> 00:11:13.010
If you hover your mouse cursor
over those health pills,

00:11:13.010 --> 00:11:14.350
you can get more information,

00:11:14.350 --> 00:11:16.640
such as the mean
and the standard deviation

00:11:16.640 --> 00:11:19.300
of the values in the tensor.

00:11:19.300 --> 00:11:23.080
So, next we can click these links
to open a detailed view of those tensors.

00:11:23.080 --> 00:11:26.860
So, in these detailed views you can apply
NumPy style slicing

00:11:26.860 --> 00:11:28.490
to reduce the dimensionality,

00:11:28.490 --> 00:11:30.960
so it's easier to look at the values
of high dimensional tensors.

00:11:30.960 --> 00:11:33.470
We have just reduced the dimension
from two to one.

00:11:33.470 --> 00:11:35.810
So we're looking at the value as a curve.

00:11:35.810 --> 00:11:38.320
Now, next we're going to
continue to the last tensor,

00:11:38.320 --> 00:11:40.440
which is a scalar,

00:11:40.440 --> 00:11:45.440
and the shape is an empty list
as we can see here.

00:11:45.440 --> 00:11:48.070
We can switch to the full history mode;

00:11:48.070 --> 00:11:50.840
we can look at how the value changes
as the model is being trained.

00:11:50.840 --> 00:11:53.610
So with the focus trim mode enabled,

00:11:53.610 --> 00:11:56.410
we're going to continue over a
number of <i>Session.runs</i>,

00:11:56.410 --> 00:11:58.830
like, 50 of them,
so we can see in real-time

00:11:58.830 --> 00:12:00.420
how the last value decreases,

00:12:00.420 --> 00:12:04.070
and how the values on the matmul
and its gradient changes.

00:12:04.070 --> 00:12:09.050
So that's how you can use the tool as an
X-ray, or X-ray animator for your models,

00:12:09.050 --> 00:12:11.890
to have a better understanding of
how your model works.

00:12:11.890 --> 00:12:16.540
So, next let's take a look at an example
of a broken model.

00:12:16.540 --> 00:12:19.410
So that's the debug MNIST model
that was shipped with TensorFlow.

00:12:19.410 --> 00:12:23.150
That's the only broken model
we ship with TensorFlow,

00:12:23.150 --> 00:12:26.590
as far as I know, and I'm proud to be
the author of it.

00:12:26.590 --> 00:12:30.350
And if we run the model, we can see
the model doesn't quite work.

00:12:30.350 --> 00:12:34.660
After two iterations of training,
the accuracy gets stuck at about 10%.

00:12:34.660 --> 00:12:37.870
Now we suspect that there might be
some bad numerical values,

00:12:37.870 --> 00:12:40.370
like NaN - Not a Number, 
or infinities in the model,

00:12:40.370 --> 00:12:43.870
but we're not sure which nodes
of the graph are responsible

00:12:43.870 --> 00:12:45.860
for generating those NaNs and infinities.

00:12:45.860 --> 00:12:48.790
So to answer that question,
we can use the debugger plugin tool.

00:12:48.790 --> 00:12:54.600
We start the banner again,
and do a <i>Refres</i>h in our browser,

00:12:54.600 --> 00:12:57.480
and then we can start our debug
MNIST example with a special flag

00:12:57.480 --> 00:13:00.850
to connect to the debugger plugin.

00:13:01.530 --> 00:13:04.460
So again we're looking at the graph.

00:13:04.460 --> 00:13:08.550
Now in order to find the nodes responsible
for the infinities or NaNs,

00:13:08.550 --> 00:13:12.950
we can check the checkpoints to
activate watch points for all the tensors,

00:13:12.950 --> 00:13:14.700
and we can use the
conditional breakpoint feature

00:13:14.700 --> 00:13:16.179
to continue the running the model

00:13:16.179 --> 00:13:18.949
until any tensor contains 
infinities or NaNs.

00:13:18.949 --> 00:13:21.320
So right now you're seeing a list
of tensor values.

00:13:21.320 --> 00:13:26.590
Those are a complete list of tensors
involved in training the model.

00:13:26.590 --> 00:13:29.370
So, in a moment,
the model is going to stop,

00:13:29.370 --> 00:13:34.270
and that's because it has hit an infinity
in the tensor call <i>cross_entropy/Log().</i>

00:13:34.270 --> 00:13:37.350
Now, we can see in the health pill--

00:13:37.350 --> 00:13:41.060
we can also see in the
detailed tensor view--

00:13:41.060 --> 00:13:44.409
those orange lines, 
and those show you the infinity values.

00:13:44.409 --> 00:13:47.160
Now the question is, why do those
infinity values happen?

00:13:47.160 --> 00:13:49.040
So, we can go back to the source code,

00:13:49.040 --> 00:13:51.650
and find the line of Python code
where it's created,

00:13:51.650 --> 00:13:53.660
and that's where we call <i>tf.Log.</i>

00:13:53.660 --> 00:13:59.430
We can also open up the graph view,
and we see the input,

00:13:59.430 --> 00:14:00.800
so we can trace the inputs.

00:14:00.800 --> 00:14:03.510
In this case the input is the Softmax tensor.

00:14:03.510 --> 00:14:08.140
So we can click expand and highlight
to look at the value of the inputs to Log,

00:14:08.140 --> 00:14:12.430
which is Softmax, and now we know
that there are indeed five values

00:14:12.430 --> 00:14:15.410
of the tensor which are zero,
and the reason for the infinity

00:14:15.410 --> 00:14:17.210
is because we're taking log of zero.

00:14:17.210 --> 00:14:20.670
Now, with that knowledge we can go
back to our source code and fix it,

00:14:20.670 --> 00:14:25.130
although we're not going 
to do this in this demo here.

00:14:26.780 --> 00:14:31.040
Alright so that's 
theTensorBoard debugging plugin,

00:14:31.040 --> 00:14:34.370
and I encourage you to use it,
explore it,

00:14:34.370 --> 00:14:37.030
and hopefully it will help you understand
your model better,

00:14:37.030 --> 00:14:39.420
and help you fix bugs much more quickly.

00:14:39.420 --> 00:14:43.089
You can just use this simple command line
on TensorBoard with a special flag.

00:14:43.089 --> 00:14:46.840
And with that, I would like 
to hand this back to Justine.

00:14:48.399 --> 00:14:51.399
(applause)

00:14:53.160 --> 00:14:55.660
Well, thank you Xin Zhang,

00:14:56.710 --> 00:15:00.180
I thought that was
a really interesting demo,

00:15:00.180 --> 00:15:04.810
and it was a great leap forward
for TensorBoard,

00:15:04.810 --> 00:15:09.650
and it really shows that one of the things
we've been doing recently,

00:15:09.650 --> 00:15:12.670
is rather than being a simple
read-only reporting tool,

00:15:12.670 --> 00:15:16.060
we're trying to explore
more interactive directions,

00:15:16.060 --> 00:15:17.680
as we've shown you today.

00:15:17.680 --> 00:15:20.930
And this is something that folks who
are productionizing TensorBoard,

00:15:20.930 --> 00:15:25.860
such as Cubeflow,
should take into consideration.

00:15:25.860 --> 00:15:28.730
We also want to attract more contributors.

00:15:28.730 --> 00:15:32.730
We have two approaches for this,
where you can develop

00:15:32.730 --> 00:15:35.460
in our official repo,
and send us pull requests.

00:15:35.460 --> 00:15:37.490
We do our work in the open.

00:15:37.490 --> 00:15:41.140
This does need approval
on security footprint, etc.

00:15:41.140 --> 00:15:45.450
And there is an escape hatch,
if that doesn't work out.

00:15:45.450 --> 00:15:48.490
You can independently develop plugins,

00:15:48.490 --> 00:15:50.560
you can create custom static builds,

00:15:50.560 --> 00:15:53.610
without anyone's approval you can do
whatever you want,

00:15:53.610 --> 00:15:57.940
because part of the goal on this team
is to liberate the tools.

00:15:58.280 --> 00:16:02.020
With that said, I just want to thank
all of you for attending,

00:16:02.020 --> 00:16:04.140
and I want to thank those of you
watching on YouTube,

00:16:04.140 --> 00:16:07.150
if you like this talk,
please hashtag Twitter,

00:16:07.150 --> 00:16:09.000
or, you know, reach out.

00:16:09.000 --> 00:16:10.810
Thank you again.

00:16:10.810 --> 00:16:12.860
(applause)

00:16:13.130 --> 00:16:17.319
♪ (music) ♪

