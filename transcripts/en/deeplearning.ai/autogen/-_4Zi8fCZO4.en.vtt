WEBVTT
Kind: captions
Language: en

00:00:00.089 --> 00:00:01.969 align:start position:0%
 
in<00:00:00.269><c> the</c><00:00:00.420><c> previous</c><00:00:00.810><c> video</c><00:00:01.140><c> you</c><00:00:01.199><c> saw</c><00:00:01.650><c> how</c><00:00:01.920><c> you</c>

00:00:01.969 --> 00:00:01.979 align:start position:0%
in the previous video you saw how you
 

00:00:01.979 --> 00:00:04.370 align:start position:0%
in the previous video you saw how you
can<00:00:02.340><c> use</c><00:00:02.580><c> mini-batch</c><00:00:03.360><c> gradient</c><00:00:03.389><c> descent</c><00:00:04.200><c> to</c>

00:00:04.370 --> 00:00:04.380 align:start position:0%
can use mini-batch gradient descent to
 

00:00:04.380 --> 00:00:06.260 align:start position:0%
can use mini-batch gradient descent to
start<00:00:04.890><c> making</c><00:00:05.069><c> progress</c><00:00:05.430><c> to</c><00:00:05.850><c> start</c><00:00:06.089><c> taking</c>

00:00:06.260 --> 00:00:06.270 align:start position:0%
start making progress to start taking
 

00:00:06.270 --> 00:00:08.240 align:start position:0%
start making progress to start taking
gradient<00:00:06.720><c> descent</c><00:00:07.020><c> steps</c><00:00:07.290><c> even</c><00:00:07.680><c> when</c><00:00:08.099><c> you're</c>

00:00:08.240 --> 00:00:08.250 align:start position:0%
gradient descent steps even when you're
 

00:00:08.250 --> 00:00:09.799 align:start position:0%
gradient descent steps even when you're
just<00:00:08.429><c> partway</c><00:00:08.820><c> through</c><00:00:09.179><c> processing</c><00:00:09.719><c> your</c>

00:00:09.799 --> 00:00:09.809 align:start position:0%
just partway through processing your
 

00:00:09.809 --> 00:00:11.600 align:start position:0%
just partway through processing your
training<00:00:10.170><c> set</c><00:00:10.349><c> even</c><00:00:10.590><c> for</c><00:00:10.830><c> the</c><00:00:10.889><c> first</c><00:00:11.099><c> time</c><00:00:11.340><c> in</c>

00:00:11.600 --> 00:00:11.610 align:start position:0%
training set even for the first time in
 

00:00:11.610 --> 00:00:14.240 align:start position:0%
training set even for the first time in
this<00:00:12.150><c> video</c><00:00:12.480><c> you</c><00:00:12.960><c> learn</c><00:00:13.200><c> more</c><00:00:13.349><c> details</c><00:00:13.650><c> of</c><00:00:14.130><c> how</c>

00:00:14.240 --> 00:00:14.250 align:start position:0%
this video you learn more details of how
 

00:00:14.250 --> 00:00:16.340 align:start position:0%
this video you learn more details of how
to<00:00:14.309><c> implement</c><00:00:14.730><c> gradient</c><00:00:15.059><c> descent</c><00:00:15.599><c> and</c><00:00:15.780><c> gain</c><00:00:16.320><c> a</c>

00:00:16.340 --> 00:00:16.350 align:start position:0%
to implement gradient descent and gain a
 

00:00:16.350 --> 00:00:18.470 align:start position:0%
to implement gradient descent and gain a
better<00:00:16.650><c> understanding</c><00:00:17.400><c> of</c><00:00:17.550><c> what</c><00:00:18.090><c> is</c><00:00:18.210><c> doing</c>

00:00:18.470 --> 00:00:18.480 align:start position:0%
better understanding of what is doing
 

00:00:18.480 --> 00:00:20.630 align:start position:0%
better understanding of what is doing
and<00:00:18.690><c> why</c><00:00:18.840><c> it</c><00:00:18.900><c> works</c><00:00:19.230><c> with</c><00:00:20.070><c> batch</c><00:00:20.369><c> gradient</c>

00:00:20.630 --> 00:00:20.640 align:start position:0%
and why it works with batch gradient
 

00:00:20.640 --> 00:00:23.090 align:start position:0%
and why it works with batch gradient
descent<00:00:20.880><c> on</c><00:00:21.420><c> every</c><00:00:22.080><c> iteration</c><00:00:22.260><c> you</c><00:00:22.920><c> go</c>

00:00:23.090 --> 00:00:23.100 align:start position:0%
descent on every iteration you go
 

00:00:23.100 --> 00:00:25.220 align:start position:0%
descent on every iteration you go
through<00:00:23.369><c> the</c><00:00:23.460><c> entire</c><00:00:23.789><c> training</c><00:00:24.029><c> set</c><00:00:24.420><c> and</c><00:00:24.600><c> you</c>

00:00:25.220 --> 00:00:25.230 align:start position:0%
through the entire training set and you
 

00:00:25.230 --> 00:00:27.769 align:start position:0%
through the entire training set and you
would<00:00:25.320><c> expect</c><00:00:25.710><c> the</c><00:00:25.830><c> costs</c><00:00:26.340><c> to</c><00:00:26.939><c> go</c><00:00:27.150><c> down</c><00:00:27.449><c> on</c>

00:00:27.769 --> 00:00:27.779 align:start position:0%
would expect the costs to go down on
 

00:00:27.779 --> 00:00:31.130 align:start position:0%
would expect the costs to go down on
every<00:00:28.260><c> single</c><00:00:28.500><c> iteration</c><00:00:29.689><c> so</c><00:00:30.689><c> if</c><00:00:30.750><c> we</c><00:00:30.810><c> plot</c><00:00:31.019><c> the</c>

00:00:31.130 --> 00:00:31.140 align:start position:0%
every single iteration so if we plot the
 

00:00:31.140 --> 00:00:33.709 align:start position:0%
every single iteration so if we plot the
cost<00:00:31.410><c> function</c><00:00:31.830><c> J</c><00:00:32.099><c> as</c><00:00:32.369><c> a</c><00:00:33.239><c> function</c><00:00:33.660><c> of</c>

00:00:33.709 --> 00:00:33.719 align:start position:0%
cost function J as a function of
 

00:00:33.719 --> 00:00:35.479 align:start position:0%
cost function J as a function of
different<00:00:34.020><c> iterations</c><00:00:34.140><c> it</c><00:00:34.829><c> should</c><00:00:35.010><c> decrease</c>

00:00:35.479 --> 00:00:35.489 align:start position:0%
different iterations it should decrease
 

00:00:35.489 --> 00:00:37.880 align:start position:0%
different iterations it should decrease
on<00:00:35.730><c> every</c><00:00:35.940><c> single</c><00:00:36.329><c> iteration</c><00:00:36.690><c> and</c><00:00:37.410><c> if</c><00:00:37.590><c> it</c><00:00:37.710><c> ever</c>

00:00:37.880 --> 00:00:37.890 align:start position:0%
on every single iteration and if it ever
 

00:00:37.890 --> 00:00:39.860 align:start position:0%
on every single iteration and if it ever
goes<00:00:38.100><c> up</c><00:00:38.340><c> even</c><00:00:38.700><c> on</c><00:00:38.820><c> one</c><00:00:38.969><c> iteration</c><00:00:39.570><c> then</c>

00:00:39.860 --> 00:00:39.870 align:start position:0%
goes up even on one iteration then
 

00:00:39.870 --> 00:00:41.209 align:start position:0%
goes up even on one iteration then
something's<00:00:40.320><c> wrong</c><00:00:40.410><c> maybe</c><00:00:40.770><c> the</c><00:00:40.920><c> learning</c>

00:00:41.209 --> 00:00:41.219 align:start position:0%
something's wrong maybe the learning
 

00:00:41.219 --> 00:00:43.940 align:start position:0%
something's wrong maybe the learning
rates<00:00:41.430><c> too</c><00:00:41.969><c> big</c><00:00:42.239><c> on</c><00:00:42.920><c> mini-batch</c><00:00:43.920><c> gradient</c>

00:00:43.940 --> 00:00:43.950 align:start position:0%
rates too big on mini-batch gradient
 

00:00:43.950 --> 00:00:46.639 align:start position:0%
rates too big on mini-batch gradient
descent<00:00:44.760><c> though</c><00:00:44.969><c> if</c><00:00:45.300><c> you</c><00:00:45.480><c> plot</c><00:00:45.750><c> progress</c><00:00:46.469><c> in</c>

00:00:46.639 --> 00:00:46.649 align:start position:0%
descent though if you plot progress in
 

00:00:46.649 --> 00:00:48.979 align:start position:0%
descent though if you plot progress in
your<00:00:46.680><c> cost</c><00:00:46.980><c> function</c><00:00:47.430><c> then</c><00:00:48.090><c> it</c><00:00:48.600><c> may</c><00:00:48.780><c> not</c>

00:00:48.979 --> 00:00:48.989 align:start position:0%
your cost function then it may not
 

00:00:48.989 --> 00:00:51.500 align:start position:0%
your cost function then it may not
decrease<00:00:49.350><c> on</c><00:00:49.890><c> every</c><00:00:49.950><c> iteration</c><00:00:50.510><c> in</c>

00:00:51.500 --> 00:00:51.510 align:start position:0%
decrease on every iteration in
 

00:00:51.510 --> 00:00:54.229 align:start position:0%
decrease on every iteration in
particular<00:00:52.020><c> on</c><00:00:52.469><c> every</c><00:00:53.370><c> iteration</c><00:00:53.550><c> you're</c>

00:00:54.229 --> 00:00:54.239 align:start position:0%
particular on every iteration you're
 

00:00:54.239 --> 00:01:01.340 align:start position:0%
particular on every iteration you're
processing<00:00:54.899><c> some</c><00:00:55.260><c> X</c><00:00:55.860><c> T</c><00:00:57.620><c> YT</c><00:00:58.620><c> and</c><00:00:59.840><c> so</c><00:01:00.840><c> if</c><00:01:01.170><c> you</c>

00:01:01.340 --> 00:01:01.350 align:start position:0%
processing some X T YT and so if you
 

00:01:01.350 --> 00:01:06.530 align:start position:0%
processing some X T YT and so if you
plot<00:01:01.760><c> the</c><00:01:02.760><c> cost</c><00:01:03.059><c> function</c><00:01:03.270><c> J</c><00:01:03.570><c> T</c><00:01:05.390><c> which</c><00:01:06.390><c> is</c>

00:01:06.530 --> 00:01:06.540 align:start position:0%
plot the cost function J T which is
 

00:01:06.540 --> 00:01:12.590 align:start position:0%
plot the cost function J T which is
computed<00:01:08.390><c> using</c><00:01:09.530><c> just</c><00:01:10.530><c> X</c><00:01:10.770><c> T</c><00:01:11.070><c> YT</c><00:01:11.280><c> then</c><00:01:12.240><c> it's</c><00:01:12.479><c> as</c>

00:01:12.590 --> 00:01:12.600 align:start position:0%
computed using just X T YT then it's as
 

00:01:12.600 --> 00:01:15.289 align:start position:0%
computed using just X T YT then it's as
if<00:01:12.810><c> on</c><00:01:12.960><c> every</c><00:01:13.229><c> iteration</c><00:01:13.650><c> you're</c><00:01:14.340><c> training</c><00:01:15.000><c> on</c>

00:01:15.289 --> 00:01:15.299 align:start position:0%
if on every iteration you're training on
 

00:01:15.299 --> 00:01:17.240 align:start position:0%
if on every iteration you're training on
a<00:01:15.330><c> different</c><00:01:15.869><c> training</c><00:01:16.740><c> cycle</c><00:01:17.100><c> really</c>

00:01:17.240 --> 00:01:17.250 align:start position:0%
a different training cycle really
 

00:01:17.250 --> 00:01:19.399 align:start position:0%
a different training cycle really
trading<00:01:17.610><c> on</c><00:01:17.700><c> a</c><00:01:17.729><c> different</c><00:01:18.180><c> meaning</c><00:01:18.360><c> batch</c><00:01:18.630><c> so</c>

00:01:19.399 --> 00:01:19.409 align:start position:0%
trading on a different meaning batch so
 

00:01:19.409 --> 00:01:21.170 align:start position:0%
trading on a different meaning batch so
you<00:01:19.500><c> plot</c><00:01:19.740><c> the</c><00:01:19.799><c> cost</c><00:01:20.159><c> function</c><00:01:20.280><c> J</c><00:01:20.549><c> you're</c><00:01:21.030><c> more</c>

00:01:21.170 --> 00:01:21.180 align:start position:0%
you plot the cost function J you're more
 

00:01:21.180 --> 00:01:22.760 align:start position:0%
you plot the cost function J you're more
likely<00:01:21.390><c> to</c><00:01:21.570><c> see</c><00:01:21.900><c> something</c><00:01:22.290><c> that</c><00:01:22.320><c> looks</c><00:01:22.680><c> like</c>

00:01:22.760 --> 00:01:22.770 align:start position:0%
likely to see something that looks like
 

00:01:22.770 --> 00:01:23.210 align:start position:0%
likely to see something that looks like
this

00:01:23.210 --> 00:01:23.220 align:start position:0%
this
 

00:01:23.220 --> 00:01:26.179 align:start position:0%
this
it<00:01:23.310><c> should</c><00:01:23.549><c> trend</c><00:01:23.880><c> downwards</c><00:01:24.450><c> but</c><00:01:25.320><c> it</c><00:01:25.680><c> is</c><00:01:25.830><c> also</c>

00:01:26.179 --> 00:01:26.189 align:start position:0%
it should trend downwards but it is also
 

00:01:26.189 --> 00:01:31.140 align:start position:0%
it should trend downwards but it is also
going<00:01:26.369><c> to</c><00:01:26.490><c> be</c><00:01:26.580><c> a</c><00:01:26.670><c> little</c><00:01:26.850><c> bit</c><00:01:27.000><c> noisier</c>

00:01:31.140 --> 00:01:31.150 align:start position:0%
 
 

00:01:31.150 --> 00:01:34.920 align:start position:0%
 
you<00:01:31.180><c> plot</c><00:01:31.450><c> J</c><00:01:32.320><c> of</c><00:01:32.620><c> T</c><00:01:33.610><c> has</c><00:01:34.420><c> your</c><00:01:34.600><c> training</c>

00:01:34.920 --> 00:01:34.930 align:start position:0%
you plot J of T has your training
 

00:01:34.930 --> 00:01:36.300 align:start position:0%
you plot J of T has your training
mini-batch<00:01:35.350><c> gradient</c><00:01:35.380><c> descent</c><00:01:35.950><c> it</c><00:01:36.160><c> may</c><00:01:36.250><c> be</c>

00:01:36.300 --> 00:01:36.310 align:start position:0%
mini-batch gradient descent it may be
 

00:01:36.310 --> 00:01:39.360 align:start position:0%
mini-batch gradient descent it may be
over<00:01:36.670><c> multiple</c><00:01:37.120><c> epochs</c><00:01:37.770><c> you</c><00:01:38.770><c> might</c><00:01:38.950><c> expect</c><00:01:39.280><c> to</c>

00:01:39.360 --> 00:01:39.370 align:start position:0%
over multiple epochs you might expect to
 

00:01:39.370 --> 00:01:41.400 align:start position:0%
over multiple epochs you might expect to
see<00:01:39.460><c> a</c><00:01:39.670><c> curve</c><00:01:40.000><c> like</c><00:01:40.180><c> this</c><00:01:40.390><c> so</c><00:01:40.750><c> as</c><00:01:40.840><c> okay</c><00:01:41.200><c> if</c><00:01:41.290><c> it</c>

00:01:41.400 --> 00:01:41.410 align:start position:0%
see a curve like this so as okay if it
 

00:01:41.410 --> 00:01:44.250 align:start position:0%
see a curve like this so as okay if it
doesn't<00:01:41.440><c> go</c><00:01:41.770><c> down</c><00:01:41.950><c> on</c><00:01:42.190><c> every</c><00:01:42.460><c> iteration</c><00:01:43.260><c> but</c>

00:01:44.250 --> 00:01:44.260 align:start position:0%
doesn't go down on every iteration but
 

00:01:44.260 --> 00:01:46.800 align:start position:0%
doesn't go down on every iteration but
it<00:01:44.410><c> should</c><00:01:44.590><c> trend</c><00:01:44.980><c> downwards</c><00:01:45.670><c> and</c><00:01:45.910><c> the</c><00:01:46.420><c> reason</c>

00:01:46.800 --> 00:01:46.810 align:start position:0%
it should trend downwards and the reason
 

00:01:46.810 --> 00:01:48.480 align:start position:0%
it should trend downwards and the reason
it'll<00:01:47.050><c> be</c><00:01:47.170><c> a</c><00:01:47.200><c> little</c><00:01:47.470><c> bit</c><00:01:47.560><c> noisy</c><00:01:47.710><c> is</c><00:01:48.190><c> that</c>

00:01:48.480 --> 00:01:48.490 align:start position:0%
it'll be a little bit noisy is that
 

00:01:48.490 --> 00:01:53.940 align:start position:0%
it'll be a little bit noisy is that
maybe<00:01:49.500><c> x1</c><00:01:50.790><c> y1</c><00:01:51.870><c> it's</c><00:01:52.870><c> just</c><00:01:53.140><c> a</c><00:01:53.230><c> relatively</c><00:01:53.680><c> easy</c>

00:01:53.940 --> 00:01:53.950 align:start position:0%
maybe x1 y1 it's just a relatively easy
 

00:01:53.950 --> 00:01:56.010 align:start position:0%
maybe x1 y1 it's just a relatively easy
meaning<00:01:54.820><c> batch</c><00:01:54.970><c> so</c><00:01:55.300><c> your</c><00:01:55.420><c> cost</c><00:01:55.660><c> might</c><00:01:55.870><c> be</c><00:01:55.990><c> a</c>

00:01:56.010 --> 00:01:56.020 align:start position:0%
meaning batch so your cost might be a
 

00:01:56.020 --> 00:01:58.310 align:start position:0%
meaning batch so your cost might be a
bit<00:01:56.170><c> lower</c><00:01:56.350><c> but</c><00:01:57.070><c> then</c><00:01:57.190><c> maybe</c><00:01:57.490><c> just</c><00:01:57.820><c> by</c><00:01:57.940><c> chance</c>

00:01:58.310 --> 00:01:58.320 align:start position:0%
bit lower but then maybe just by chance
 

00:01:58.320 --> 00:02:02.100 align:start position:0%
bit lower but then maybe just by chance
x2<00:01:59.320><c> y2</c><00:01:59.590><c> is</c><00:02:00.250><c> just</c><00:02:00.790><c> a</c><00:02:00.850><c> harder</c><00:02:01.330><c> mini</c><00:02:01.600><c> batch</c><00:02:01.840><c> maybe</c>

00:02:02.100 --> 00:02:02.110 align:start position:0%
x2 y2 is just a harder mini batch maybe
 

00:02:02.110 --> 00:02:04.230 align:start position:0%
x2 y2 is just a harder mini batch maybe
even<00:02:02.410><c> some</c><00:02:02.590><c> let's</c><00:02:03.070><c> label</c><00:02:03.250><c> examples</c><00:02:03.640><c> in</c><00:02:04.000><c> it</c><00:02:04.119><c> in</c>

00:02:04.230 --> 00:02:04.240 align:start position:0%
even some let's label examples in it in
 

00:02:04.240 --> 00:02:05.370 align:start position:0%
even some let's label examples in it in
which<00:02:04.360><c> case</c><00:02:04.390><c> the</c><00:02:04.690><c> cost</c><00:02:04.869><c> would</c><00:02:05.050><c> be</c><00:02:05.110><c> a</c><00:02:05.200><c> bit</c>

00:02:05.370 --> 00:02:05.380 align:start position:0%
which case the cost would be a bit
 

00:02:05.380 --> 00:02:07.620 align:start position:0%
which case the cost would be a bit
higher<00:02:05.590><c> and</c><00:02:05.890><c> so</c><00:02:06.460><c> on</c><00:02:06.640><c> so</c><00:02:06.850><c> that's</c><00:02:06.970><c> why</c><00:02:07.210><c> you</c><00:02:07.270><c> get</c>

00:02:07.620 --> 00:02:07.630 align:start position:0%
higher and so on so that's why you get
 

00:02:07.630 --> 00:02:10.380 align:start position:0%
higher and so on so that's why you get
these<00:02:07.840><c> oscillations</c><00:02:08.619><c> as</c><00:02:09.070><c> you</c><00:02:09.220><c> plot</c><00:02:09.459><c> the</c><00:02:10.030><c> cost</c>

00:02:10.380 --> 00:02:10.390 align:start position:0%
these oscillations as you plot the cost
 

00:02:10.390 --> 00:02:12.210 align:start position:0%
these oscillations as you plot the cost
when<00:02:11.050><c> you're</c><00:02:11.170><c> running</c><00:02:11.320><c> mini</c><00:02:11.709><c> batch</c><00:02:11.980><c> gradient</c>

00:02:12.210 --> 00:02:12.220 align:start position:0%
when you're running mini batch gradient
 

00:02:12.220 --> 00:02:14.580 align:start position:0%
when you're running mini batch gradient
descent<00:02:12.430><c> now</c><00:02:13.390><c> one</c><00:02:13.780><c> of</c><00:02:13.900><c> the</c><00:02:13.990><c> parameters</c><00:02:14.470><c> you</c>

00:02:14.580 --> 00:02:14.590 align:start position:0%
descent now one of the parameters you
 

00:02:14.590 --> 00:02:16.770 align:start position:0%
descent now one of the parameters you
need<00:02:14.770><c> to</c><00:02:14.890><c> choose</c><00:02:15.160><c> is</c><00:02:15.430><c> the</c><00:02:15.700><c> size</c><00:02:15.970><c> of</c><00:02:16.240><c> your</c><00:02:16.330><c> mini</c>

00:02:16.770 --> 00:02:16.780 align:start position:0%
need to choose is the size of your mini
 

00:02:16.780 --> 00:02:20.520 align:start position:0%
need to choose is the size of your mini
batch<00:02:17.130><c> so</c><00:02:18.130><c> M</c><00:02:18.430><c> was</c><00:02:19.030><c> the</c><00:02:19.180><c> training</c><00:02:19.480><c> set</c><00:02:19.930><c> size</c><00:02:20.170><c> on</c>

00:02:20.520 --> 00:02:20.530 align:start position:0%
batch so M was the training set size on
 

00:02:20.530 --> 00:02:25.949 align:start position:0%
batch so M was the training set size on
one<00:02:21.250><c> extreme</c><00:02:21.730><c> if</c><00:02:21.970><c> the</c><00:02:22.240><c> mini</c><00:02:22.510><c> batch</c><00:02:22.810><c> size</c><00:02:24.959><c> is</c>

00:02:25.949 --> 00:02:25.959 align:start position:0%
one extreme if the mini batch size is
 

00:02:25.959 --> 00:02:29.250 align:start position:0%
one extreme if the mini batch size is
equal<00:02:26.860><c> to</c><00:02:27.010><c> M</c><00:02:27.330><c> then</c><00:02:28.330><c> you</c><00:02:28.570><c> just</c><00:02:28.810><c> end</c><00:02:28.990><c> up</c><00:02:29.200><c> with</c>

00:02:29.250 --> 00:02:29.260 align:start position:0%
equal to M then you just end up with
 

00:02:29.260 --> 00:02:36.090 align:start position:0%
equal to M then you just end up with
bosch<00:02:30.190><c> gradient</c><00:02:30.910><c> descent</c><00:02:34.620><c> alright</c><00:02:35.620><c> so</c><00:02:35.830><c> in</c>

00:02:36.090 --> 00:02:36.100 align:start position:0%
bosch gradient descent alright so in
 

00:02:36.100 --> 00:02:37.830 align:start position:0%
bosch gradient descent alright so in
this<00:02:36.220><c> extreme</c><00:02:36.700><c> you</c><00:02:37.090><c> would</c><00:02:37.239><c> just</c><00:02:37.450><c> have</c><00:02:37.630><c> one</c>

00:02:37.830 --> 00:02:37.840 align:start position:0%
this extreme you would just have one
 

00:02:37.840 --> 00:02:42.270 align:start position:0%
this extreme you would just have one
mini<00:02:38.110><c> batch</c><00:02:38.459><c> x1</c><00:02:39.540><c> y1</c><00:02:40.540><c> and</c><00:02:41.080><c> this</c><00:02:41.350><c> mini</c><00:02:41.620><c> batch</c><00:02:41.890><c> is</c>

00:02:42.270 --> 00:02:42.280 align:start position:0%
mini batch x1 y1 and this mini batch is
 

00:02:42.280 --> 00:02:45.720 align:start position:0%
mini batch x1 y1 and this mini batch is
equal<00:02:42.730><c> to</c><00:02:43.000><c> your</c><00:02:43.870><c> entire</c><00:02:44.260><c> training</c><00:02:44.620><c> set</c><00:02:44.890><c> so</c>

00:02:45.720 --> 00:02:45.730 align:start position:0%
equal to your entire training set so
 

00:02:45.730 --> 00:02:47.759 align:start position:0%
equal to your entire training set so
setting<00:02:46.150><c> the</c><00:02:46.180><c> movie</c><00:02:46.450><c> batch</c><00:02:46.630><c> size</c><00:02:46.870><c> M</c><00:02:47.200><c> just</c>

00:02:47.759 --> 00:02:47.769 align:start position:0%
setting the movie batch size M just
 

00:02:47.769 --> 00:02:49.830 align:start position:0%
setting the movie batch size M just
gives<00:02:48.040><c> you</c><00:02:48.130><c> back</c><00:02:48.280><c> gradient</c><00:02:48.730><c> descent</c><00:02:49.030><c> the</c>

00:02:49.830 --> 00:02:49.840 align:start position:0%
gives you back gradient descent the
 

00:02:49.840 --> 00:02:51.930 align:start position:0%
gives you back gradient descent the
other<00:02:50.140><c> extreme</c><00:02:50.680><c> would</c><00:02:50.920><c> be</c><00:02:50.980><c> if</c><00:02:51.370><c> your</c><00:02:51.670><c> mini</c>

00:02:51.930 --> 00:02:51.940 align:start position:0%
other extreme would be if your mini
 

00:02:51.940 --> 00:03:00.300 align:start position:0%
other extreme would be if your mini
batch<00:02:52.180><c> size</c><00:02:55.230><c> were</c><00:02:56.230><c> equal</c><00:02:56.650><c> to</c><00:02:56.830><c> 1</c><00:02:59.040><c> this</c><00:03:00.040><c> gives</c>

00:03:00.300 --> 00:03:00.310 align:start position:0%
batch size were equal to 1 this gives
 

00:03:00.310 --> 00:03:02.240 align:start position:0%
batch size were equal to 1 this gives
you<00:03:00.430><c> an</c><00:03:00.489><c> algorithm</c><00:03:00.970><c> called</c><00:03:01.269><c> stochastic</c>

00:03:02.240 --> 00:03:02.250 align:start position:0%
you an algorithm called stochastic
 

00:03:02.250 --> 00:03:09.050 align:start position:0%
you an algorithm called stochastic
gradient<00:03:03.250><c> descent</c><00:03:04.320><c> and</c><00:03:06.870><c> here</c><00:03:07.870><c> every</c><00:03:08.380><c> example</c>

00:03:09.050 --> 00:03:09.060 align:start position:0%
gradient descent and here every example
 

00:03:09.060 --> 00:03:19.170 align:start position:0%
gradient descent and here every example
is<00:03:10.060><c> his</c><00:03:10.360><c> own</c><00:03:10.600><c> mini</c><00:03:10.840><c> batch</c><00:03:17.640><c> so</c><00:03:18.640><c> what</c>

00:03:19.170 --> 00:03:19.180 align:start position:0%
is his own mini batch so what
 

00:03:19.180 --> 00:03:21.479 align:start position:0%
is his own mini batch so what
in<00:03:19.540><c> this</c><00:03:19.689><c> case</c><00:03:19.930><c> as</c><00:03:20.230><c> you</c><00:03:20.379><c> look</c><00:03:20.560><c> at</c><00:03:20.769><c> you</c><00:03:21.280><c> know</c><00:03:21.340><c> the</c>

00:03:21.479 --> 00:03:21.489 align:start position:0%
in this case as you look at you know the
 

00:03:21.489 --> 00:03:27.089 align:start position:0%
in this case as you look at you know the
first<00:03:21.730><c> mini</c><00:03:22.030><c> batch</c><00:03:22.920><c> so</c><00:03:23.920><c> x1</c><00:03:24.489><c> y1</c><00:03:25.859><c> but</c><00:03:26.859><c> when</c><00:03:27.010><c> you</c>

00:03:27.089 --> 00:03:27.099 align:start position:0%
first mini batch so x1 y1 but when you
 

00:03:27.099 --> 00:03:29.640 align:start position:0%
first mini batch so x1 y1 but when you
meanie<00:03:27.340><c> batch</c><00:03:27.579><c> sizes</c><00:03:27.879><c> 1</c><00:03:28.299><c> this</c><00:03:28.599><c> just</c><00:03:28.900><c> has</c><00:03:29.109><c> you</c>

00:03:29.640 --> 00:03:29.650 align:start position:0%
meanie batch sizes 1 this just has you
 

00:03:29.650 --> 00:03:31.349 align:start position:0%
meanie batch sizes 1 this just has you
know<00:03:29.739><c> your</c><00:03:30.010><c> first</c><00:03:30.250><c> training</c><00:03:30.519><c> example</c><00:03:30.640><c> and</c><00:03:31.269><c> you</c>

00:03:31.349 --> 00:03:31.359 align:start position:0%
know your first training example and you
 

00:03:31.359 --> 00:03:33.089 align:start position:0%
know your first training example and you
take<00:03:31.569><c> your</c><00:03:31.599><c> it</c><00:03:31.840><c> into</c><00:03:32.079><c> sense</c><00:03:32.319><c> that</c><00:03:32.530><c> with</c><00:03:32.980><c> your</c>

00:03:33.089 --> 00:03:33.099 align:start position:0%
take your it into sense that with your
 

00:03:33.099 --> 00:03:35.910 align:start position:0%
take your it into sense that with your
first<00:03:33.280><c> training</c><00:03:33.549><c> example</c><00:03:33.640><c> and</c><00:03:34.209><c> then</c><00:03:34.810><c> you</c><00:03:35.019><c> mix</c>

00:03:35.910 --> 00:03:35.920 align:start position:0%
first training example and then you mix
 

00:03:35.920 --> 00:03:38.429 align:start position:0%
first training example and then you mix
take<00:03:36.549><c> a</c><00:03:36.579><c> look</c><00:03:36.819><c> at</c><00:03:37.030><c> your</c><00:03:37.209><c> second</c><00:03:37.689><c> mini</c><00:03:38.200><c> batch</c>

00:03:38.429 --> 00:03:38.439 align:start position:0%
take a look at your second mini batch
 

00:03:38.439 --> 00:03:40.259 align:start position:0%
take a look at your second mini batch
which<00:03:38.739><c> is</c><00:03:38.889><c> just</c><00:03:39.099><c> your</c><00:03:39.250><c> second</c><00:03:40.060><c> training</c>

00:03:40.259 --> 00:03:40.269 align:start position:0%
which is just your second training
 

00:03:40.269 --> 00:03:42.179 align:start position:0%
which is just your second training
example<00:03:40.359><c> and</c><00:03:40.959><c> take</c><00:03:41.379><c> your</c><00:03:41.500><c> grandest</c><00:03:41.859><c> and</c><00:03:42.010><c> step</c>

00:03:42.179 --> 00:03:42.189 align:start position:0%
example and take your grandest and step
 

00:03:42.189 --> 00:03:44.099 align:start position:0%
example and take your grandest and step
with<00:03:42.370><c> that</c><00:03:42.400><c> and</c><00:03:42.879><c> then</c><00:03:43.299><c> you</c><00:03:43.420><c> do</c><00:03:43.599><c> with</c><00:03:43.810><c> the</c><00:03:43.930><c> third</c>

00:03:44.099 --> 00:03:44.109 align:start position:0%
with that and then you do with the third
 

00:03:44.109 --> 00:03:45.869 align:start position:0%
with that and then you do with the third
training<00:03:44.379><c> example</c><00:03:44.829><c> and</c><00:03:44.859><c> so</c><00:03:45.069><c> on</c><00:03:45.189><c> looking</c><00:03:45.819><c> at</c>

00:03:45.869 --> 00:03:45.879 align:start position:0%
training example and so on looking at
 

00:03:45.879 --> 00:03:47.580 align:start position:0%
training example and so on looking at
just<00:03:46.090><c> one</c><00:03:46.299><c> single</c><00:03:46.599><c> training</c><00:03:46.989><c> example</c><00:03:47.049><c> at</c><00:03:47.500><c> a</c>

00:03:47.580 --> 00:03:47.590 align:start position:0%
just one single training example at a
 

00:03:47.590 --> 00:03:52.440 align:start position:0%
just one single training example at a
time<00:03:49.139><c> so</c><00:03:50.139><c> let's</c><00:03:50.530><c> look</c><00:03:50.769><c> at</c><00:03:50.950><c> what</c><00:03:51.269><c> these</c><00:03:52.269><c> two</c>

00:03:52.440 --> 00:03:52.450 align:start position:0%
time so let's look at what these two
 

00:03:52.450 --> 00:03:55.140 align:start position:0%
time so let's look at what these two
extremes<00:03:52.840><c> will</c><00:03:53.079><c> do</c><00:03:53.260><c> on</c><00:03:53.530><c> optimizing</c><00:03:54.310><c> this</c><00:03:54.819><c> cost</c>

00:03:55.140 --> 00:03:55.150 align:start position:0%
extremes will do on optimizing this cost
 

00:03:55.150 --> 00:03:57.240 align:start position:0%
extremes will do on optimizing this cost
function<00:03:55.389><c> if</c><00:03:55.810><c> these</c><00:03:56.079><c> are</c><00:03:56.260><c> the</c><00:03:56.319><c> contours</c><00:03:56.859><c> of</c><00:03:57.040><c> a</c>

00:03:57.240 --> 00:03:57.250 align:start position:0%
function if these are the contours of a
 

00:03:57.250 --> 00:03:59.039 align:start position:0%
function if these are the contours of a
cost<00:03:57.489><c> function</c><00:03:57.639><c> trying</c><00:03:58.180><c> to</c><00:03:58.239><c> minimize</c><00:03:58.569><c> so</c><00:03:58.870><c> the</c>

00:03:59.039 --> 00:03:59.049 align:start position:0%
cost function trying to minimize so the
 

00:03:59.049 --> 00:04:03.330 align:start position:0%
cost function trying to minimize so the
your<00:03:59.680><c> minimum</c><00:04:00.099><c> is</c><00:04:00.250><c> there</c><00:04:00.459><c> then</c><00:04:01.859><c> -</c><00:04:02.859><c> gradient</c>

00:04:03.330 --> 00:04:03.340 align:start position:0%
your minimum is there then - gradient
 

00:04:03.340 --> 00:04:05.940 align:start position:0%
your minimum is there then - gradient
descent<00:04:03.730><c> might</c><00:04:04.120><c> start</c><00:04:04.450><c> somewhere</c><00:04:04.719><c> and</c><00:04:05.079><c> be</c>

00:04:05.940 --> 00:04:05.950 align:start position:0%
descent might start somewhere and be
 

00:04:05.950 --> 00:04:08.990 align:start position:0%
descent might start somewhere and be
able<00:04:06.040><c> to</c><00:04:06.159><c> take</c><00:04:06.400><c> relatively</c><00:04:07.709><c> low</c><00:04:08.709><c> noise</c>

00:04:08.990 --> 00:04:09.000 align:start position:0%
able to take relatively low noise
 

00:04:09.000 --> 00:04:12.929 align:start position:0%
able to take relatively low noise
relatively<00:04:10.000><c> large</c><00:04:10.659><c> steps</c><00:04:11.109><c> and</c><00:04:11.519><c> you</c><00:04:12.519><c> know</c><00:04:12.609><c> just</c>

00:04:12.929 --> 00:04:12.939 align:start position:0%
relatively large steps and you know just
 

00:04:12.939 --> 00:04:16.170 align:start position:0%
relatively large steps and you know just
keep<00:04:13.329><c> marching</c><00:04:13.599><c> to</c><00:04:14.469><c> the</c><00:04:14.590><c> minimum</c><00:04:14.950><c> in</c><00:04:15.519><c> contrast</c>

00:04:16.170 --> 00:04:16.180 align:start position:0%
keep marching to the minimum in contrast
 

00:04:16.180 --> 00:04:19.409 align:start position:0%
keep marching to the minimum in contrast
with<00:04:17.019><c> so</c><00:04:17.500><c> costly</c><00:04:17.859><c> gradient</c><00:04:18.310><c> descent</c><00:04:18.639><c> if</c><00:04:19.269><c> you</c>

00:04:19.409 --> 00:04:19.419 align:start position:0%
with so costly gradient descent if you
 

00:04:19.419 --> 00:04:20.849 align:start position:0%
with so costly gradient descent if you
start<00:04:19.690><c> somewhere</c><00:04:19.959><c> let's</c><00:04:20.380><c> pick</c><00:04:20.500><c> a</c><00:04:20.530><c> different</c>

00:04:20.849 --> 00:04:20.859 align:start position:0%
start somewhere let's pick a different
 

00:04:20.859 --> 00:04:23.279 align:start position:0%
start somewhere let's pick a different
starting<00:04:21.160><c> point</c><00:04:21.479><c> then</c><00:04:22.479><c> on</c><00:04:22.810><c> every</c><00:04:23.110><c> iteration</c>

00:04:23.279 --> 00:04:23.289 align:start position:0%
starting point then on every iteration
 

00:04:23.289 --> 00:04:25.469 align:start position:0%
starting point then on every iteration
you're<00:04:24.039><c> taking</c><00:04:24.370><c> bring</c><00:04:24.550><c> descends</c><00:04:25.000><c> with</c><00:04:25.120><c> just</c><00:04:25.419><c> a</c>

00:04:25.469 --> 00:04:25.479 align:start position:0%
you're taking bring descends with just a
 

00:04:25.479 --> 00:04:27.719 align:start position:0%
you're taking bring descends with just a
single<00:04:25.690><c> training</c><00:04:26.110><c> example</c><00:04:26.229><c> so</c><00:04:26.949><c> most</c><00:04:27.520><c> of</c><00:04:27.639><c> the</c>

00:04:27.719 --> 00:04:27.729 align:start position:0%
single training example so most of the
 

00:04:27.729 --> 00:04:29.909 align:start position:0%
single training example so most of the
time<00:04:27.909><c> you</c><00:04:28.180><c> hit</c><00:04:28.419><c> to</c><00:04:28.810><c> what</c><00:04:28.990><c> the</c><00:04:29.139><c> global</c><00:04:29.530><c> minimum</c>

00:04:29.909 --> 00:04:29.919 align:start position:0%
time you hit to what the global minimum
 

00:04:29.919 --> 00:04:31.800 align:start position:0%
time you hit to what the global minimum
but<00:04:30.159><c> sometimes</c><00:04:30.970><c> you</c><00:04:31.210><c> hit</c><00:04:31.389><c> in</c><00:04:31.539><c> the</c><00:04:31.630><c> wrong</c>

00:04:31.800 --> 00:04:31.810 align:start position:0%
but sometimes you hit in the wrong
 

00:04:31.810 --> 00:04:33.600 align:start position:0%
but sometimes you hit in the wrong
direction<00:04:32.020><c> if</c><00:04:32.530><c> you</c><00:04:32.680><c> know</c><00:04:32.830><c> that</c><00:04:33.039><c> one</c><00:04:33.250><c> example</c>

00:04:33.600 --> 00:04:33.610 align:start position:0%
direction if you know that one example
 

00:04:33.610 --> 00:04:35.689 align:start position:0%
direction if you know that one example
happens<00:04:34.570><c> to</c><00:04:34.690><c> point</c><00:04:34.930><c> you</c><00:04:35.080><c> in</c><00:04:35.169><c> a</c><00:04:35.260><c> bad</c><00:04:35.440><c> direction</c>

00:04:35.689 --> 00:04:35.699 align:start position:0%
happens to point you in a bad direction
 

00:04:35.699 --> 00:04:38.700 align:start position:0%
happens to point you in a bad direction
so<00:04:36.699><c> stochastic</c><00:04:37.270><c> great</c><00:04:37.449><c> descent</c><00:04:37.840><c> can</c><00:04:38.500><c> be</c>

00:04:38.700 --> 00:04:38.710 align:start position:0%
so stochastic great descent can be
 

00:04:38.710 --> 00:04:41.909 align:start position:0%
so stochastic great descent can be
extremely<00:04:39.130><c> noisy</c><00:04:39.460><c> and</c><00:04:40.060><c> on</c><00:04:40.720><c> average</c><00:04:40.960><c> takes</c><00:04:41.770><c> you</c>

00:04:41.909 --> 00:04:41.919 align:start position:0%
extremely noisy and on average takes you
 

00:04:41.919 --> 00:04:45.360 align:start position:0%
extremely noisy and on average takes you
in<00:04:42.039><c> a</c><00:04:42.130><c> good</c><00:04:42.340><c> direction</c><00:04:42.630><c> but</c><00:04:43.630><c> um</c><00:04:44.370><c> sometimes</c>

00:04:45.360 --> 00:04:45.370 align:start position:0%
in a good direction but um sometimes
 

00:04:45.370 --> 00:04:46.740 align:start position:0%
in a good direction but um sometimes
you're<00:04:45.580><c> headed</c><00:04:45.849><c> in</c><00:04:45.909><c> the</c><00:04:45.970><c> wrong</c><00:04:46.090><c> direction</c><00:04:46.150><c> as</c>

00:04:46.740 --> 00:04:46.750 align:start position:0%
you're headed in the wrong direction as
 

00:04:46.750 --> 00:04:49.469 align:start position:0%
you're headed in the wrong direction as
well<00:04:46.960><c> as</c><00:04:47.260><c> the</c><00:04:48.099><c> constant</c><00:04:48.400><c> descent</c><00:04:49.000><c> won't</c><00:04:49.270><c> ever</c>

00:04:49.469 --> 00:04:49.479 align:start position:0%
well as the constant descent won't ever
 

00:04:49.479 --> 00:04:51.360 align:start position:0%
well as the constant descent won't ever
converge<00:04:50.080><c> you're</c><00:04:50.349><c> always</c><00:04:50.560><c> just</c><00:04:51.070><c> kind</c><00:04:51.310><c> of</c>

00:04:51.360 --> 00:04:51.370 align:start position:0%
converge you're always just kind of
 

00:04:51.370 --> 00:04:54.029 align:start position:0%
converge you're always just kind of
oscillate<00:04:52.240><c> and</c><00:04:52.479><c> wander</c><00:04:52.840><c> around</c><00:04:52.990><c> the</c><00:04:53.590><c> region</c>

00:04:54.029 --> 00:04:54.039 align:start position:0%
oscillate and wander around the region
 

00:04:54.039 --> 00:04:55.620 align:start position:0%
oscillate and wander around the region
of<00:04:54.159><c> the</c><00:04:54.250><c> minimum</c><00:04:54.610><c> but</c><00:04:54.820><c> it</c><00:04:54.849><c> won't</c><00:04:55.090><c> ever</c><00:04:55.300><c> just</c>

00:04:55.620 --> 00:04:55.630 align:start position:0%
of the minimum but it won't ever just
 

00:04:55.630 --> 00:04:57.360 align:start position:0%
of the minimum but it won't ever just
head<00:04:55.870><c> to</c><00:04:56.020><c> the</c><00:04:56.110><c> minimum</c><00:04:56.470><c> and</c><00:04:56.650><c> stay</c><00:04:56.919><c> there</c>

00:04:57.360 --> 00:04:57.370 align:start position:0%
head to the minimum and stay there
 

00:04:57.370 --> 00:05:01.010 align:start position:0%
head to the minimum and stay there
in<00:04:57.759><c> practice</c><00:04:58.659><c> the</c><00:04:59.470><c> mini</c><00:04:59.740><c> batch</c><00:04:59.979><c> size</c><00:05:00.310><c> you</c><00:05:00.610><c> use</c>

00:05:01.010 --> 00:05:01.020 align:start position:0%
in practice the mini batch size you use
 

00:05:01.020 --> 00:05:07.279 align:start position:0%
in practice the mini batch size you use
will<00:05:02.020><c> be</c><00:05:02.050><c> somewhere</c><00:05:02.530><c> in</c><00:05:02.680><c> between</c>

00:05:07.279 --> 00:05:07.289 align:start position:0%
 
 

00:05:07.289 --> 00:05:11.749 align:start position:0%
 
some<00:05:08.289><c> moves</c><00:05:08.500><c> in</c><00:05:08.650><c> in</c><00:05:08.889><c> 1</c><00:05:09.340><c> +</c><00:05:09.639><c> M</c><00:05:10.120><c> +</c><00:05:10.449><c> 1</c><00:05:10.479><c> nm</c><00:05:11.229><c> are</c>

00:05:11.749 --> 00:05:11.759 align:start position:0%
some moves in in 1 + M + 1 nm are
 

00:05:11.759 --> 00:05:14.610 align:start position:0%
some moves in in 1 + M + 1 nm are
respectively<00:05:12.759><c> too</c><00:05:13.120><c> small</c><00:05:13.419><c> and</c><00:05:13.690><c> too</c><00:05:13.720><c> large</c><00:05:14.169><c> and</c>

00:05:14.610 --> 00:05:14.620 align:start position:0%
respectively too small and too large and
 

00:05:14.620 --> 00:05:17.460 align:start position:0%
respectively too small and too large and
here's<00:05:15.340><c> why</c><00:05:15.580><c> if</c><00:05:16.270><c> you</c><00:05:16.509><c> use</c><00:05:16.750><c> batch</c><00:05:17.139><c> gradient</c>

00:05:17.460 --> 00:05:17.470 align:start position:0%
here's why if you use batch gradient
 

00:05:17.470 --> 00:05:22.900 align:start position:0%
here's why if you use batch gradient
descent

00:05:22.900 --> 00:05:22.910 align:start position:0%
 
 

00:05:22.910 --> 00:05:30.170 align:start position:0%
 
so<00:05:23.910><c> this</c><00:05:24.150><c> is</c><00:05:24.300><c> your</c><00:05:24.449><c> mini</c><00:05:24.810><c> batch</c><00:05:25.050><c> size</c><00:05:25.699><c> equals</c><00:05:26.699><c> M</c>

00:05:30.170 --> 00:05:30.180 align:start position:0%
 
 

00:05:30.180 --> 00:05:33.060 align:start position:0%
 
then<00:05:31.180><c> you're</c><00:05:31.540><c> processing</c><00:05:32.080><c> a</c><00:05:32.380><c> huge</c><00:05:32.710><c> training</c>

00:05:33.060 --> 00:05:33.070 align:start position:0%
then you're processing a huge training
 

00:05:33.070 --> 00:05:35.550 align:start position:0%
then you're processing a huge training
set<00:05:33.430><c> on</c><00:05:33.670><c> every</c><00:05:34.510><c> innovation</c><00:05:34.900><c> so</c><00:05:35.260><c> the</c><00:05:35.380><c> main</c>

00:05:35.550 --> 00:05:35.560 align:start position:0%
set on every innovation so the main
 

00:05:35.560 --> 00:05:37.650 align:start position:0%
set on every innovation so the main
disadvantage<00:05:36.250><c> of</c><00:05:36.280><c> this</c><00:05:36.760><c> is</c><00:05:37.060><c> that</c><00:05:37.300><c> it</c><00:05:37.480><c> takes</c>

00:05:37.650 --> 00:05:37.660 align:start position:0%
disadvantage of this is that it takes
 

00:05:37.660 --> 00:05:41.480 align:start position:0%
disadvantage of this is that it takes
too<00:05:38.200><c> much</c><00:05:38.350><c> time</c><00:05:38.710><c> too</c><00:05:39.520><c> long</c><00:05:39.820><c> per</c><00:05:40.750><c> iteration</c>

00:05:41.480 --> 00:05:41.490 align:start position:0%
too much time too long per iteration
 

00:05:41.490 --> 00:05:43.380 align:start position:0%
too much time too long per iteration
assuming<00:05:42.490><c> you</c><00:05:42.580><c> have</c><00:05:42.670><c> a</c><00:05:42.700><c> very</c><00:05:42.910><c> large</c><00:05:43.150><c> training</c>

00:05:43.380 --> 00:05:43.390 align:start position:0%
assuming you have a very large training
 

00:05:43.390 --> 00:05:44.790 align:start position:0%
assuming you have a very large training
set<00:05:43.660><c> if</c><00:05:43.870><c> you</c><00:05:43.960><c> have</c><00:05:44.050><c> you're</c><00:05:44.200><c> a</c><00:05:44.230><c> small</c><00:05:44.530><c> training</c>

00:05:44.790 --> 00:05:44.800 align:start position:0%
set if you have you're a small training
 

00:05:44.800 --> 00:05:47.070 align:start position:0%
set if you have you're a small training
set<00:05:44.920><c> then</c><00:05:45.310><c> bachelor</c><00:05:45.700><c> in</c><00:05:45.760><c> descent</c><00:05:46.090><c> is</c><00:05:46.270><c> fine</c><00:05:46.510><c> if</c>

00:05:47.070 --> 00:05:47.080 align:start position:0%
set then bachelor in descent is fine if
 

00:05:47.080 --> 00:05:49.590 align:start position:0%
set then bachelor in descent is fine if
you<00:05:47.290><c> go</c><00:05:47.530><c> to</c><00:05:47.590><c> the</c><00:05:47.890><c> opposite</c><00:05:48.610><c> if</c><00:05:48.910><c> you</c><00:05:49.120><c> use</c><00:05:49.330><c> the</c>

00:05:49.590 --> 00:05:49.600 align:start position:0%
you go to the opposite if you use the
 

00:05:49.600 --> 00:05:56.190 align:start position:0%
you go to the opposite if you use the
conflict-ridden<00:05:50.440><c> you're</c><00:05:50.710><c> sent</c><00:05:53.250><c> then</c><00:05:55.200><c> it's</c>

00:05:56.190 --> 00:05:56.200 align:start position:0%
conflict-ridden you're sent then it's
 

00:05:56.200 --> 00:05:58.080 align:start position:0%
conflict-ridden you're sent then it's
nice<00:05:56.470><c> that</c><00:05:56.530><c> you</c><00:05:56.950><c> get</c><00:05:57.130><c> to</c><00:05:57.280><c> make</c><00:05:57.460><c> progress</c><00:05:57.700><c> after</c>

00:05:58.080 --> 00:05:58.090 align:start position:0%
nice that you get to make progress after
 

00:05:58.090 --> 00:06:00.630 align:start position:0%
nice that you get to make progress after
processing<00:05:58.780><c> just</c><00:05:58.930><c> one</c><00:05:59.140><c> example</c><00:05:59.640><c> that's</c>

00:06:00.630 --> 00:06:00.640 align:start position:0%
processing just one example that's
 

00:06:00.640 --> 00:06:02.610 align:start position:0%
processing just one example that's
actually<00:06:00.940><c> not</c><00:06:01.120><c> a</c><00:06:01.180><c> problem</c><00:06:01.540><c> and</c><00:06:01.990><c> the</c><00:06:02.140><c> noisiness</c>

00:06:02.610 --> 00:06:02.620 align:start position:0%
actually not a problem and the noisiness
 

00:06:02.620 --> 00:06:05.430 align:start position:0%
actually not a problem and the noisiness
can<00:06:03.460><c> be</c><00:06:03.610><c> ameliorated</c><00:06:04.060><c> or</c><00:06:04.690><c> can</c><00:06:04.870><c> be</c><00:06:04.960><c> reduced</c><00:06:05.320><c> by</c>

00:06:05.430 --> 00:06:05.440 align:start position:0%
can be ameliorated or can be reduced by
 

00:06:05.440 --> 00:06:07.650 align:start position:0%
can be ameliorated or can be reduced by
just<00:06:05.770><c> using</c><00:06:05.920><c> a</c><00:06:06.160><c> smaller</c><00:06:06.430><c> learning</c><00:06:06.640><c> rate</c><00:06:07.060><c> but</c>

00:06:07.650 --> 00:06:07.660 align:start position:0%
just using a smaller learning rate but
 

00:06:07.660 --> 00:06:09.210 align:start position:0%
just using a smaller learning rate but
the<00:06:07.750><c> huge</c><00:06:07.990><c> disadvantage</c><00:06:08.350><c> the</c><00:06:08.860><c> stochastic</c>

00:06:09.210 --> 00:06:09.220 align:start position:0%
the huge disadvantage the stochastic
 

00:06:09.220 --> 00:06:12.480 align:start position:0%
the huge disadvantage the stochastic
green<00:06:09.640><c> descent</c><00:06:10.060><c> is</c><00:06:10.270><c> that</c><00:06:10.960><c> you</c><00:06:11.710><c> lose</c><00:06:11.980><c> almost</c>

00:06:12.480 --> 00:06:12.490 align:start position:0%
green descent is that you lose almost
 

00:06:12.490 --> 00:06:17.540 align:start position:0%
green descent is that you lose almost
all<00:06:13.000><c> your</c><00:06:13.180><c> speed</c><00:06:13.480><c> up</c><00:06:15.060><c> from</c><00:06:16.060><c> vectorization</c>

00:06:17.540 --> 00:06:17.550 align:start position:0%
all your speed up from vectorization
 

00:06:17.550 --> 00:06:20.130 align:start position:0%
all your speed up from vectorization
because<00:06:18.550><c> here</c><00:06:18.880><c> you're</c><00:06:19.210><c> processing</c><00:06:19.570><c> a</c><00:06:19.780><c> single</c>

00:06:20.130 --> 00:06:20.140 align:start position:0%
because here you're processing a single
 

00:06:20.140 --> 00:06:22.260 align:start position:0%
because here you're processing a single
training<00:06:20.410><c> example</c><00:06:20.530><c> at</c><00:06:21.010><c> a</c><00:06:21.070><c> time</c><00:06:21.100><c> the</c><00:06:22.060><c> way</c><00:06:22.240><c> you</c>

00:06:22.260 --> 00:06:22.270 align:start position:0%
training example at a time the way you
 

00:06:22.270 --> 00:06:24.660 align:start position:0%
training example at a time the way you
process<00:06:22.840><c> each</c><00:06:23.050><c> example</c><00:06:23.560><c> is</c><00:06:23.950><c> going</c><00:06:24.190><c> to</c><00:06:24.310><c> be</c><00:06:24.400><c> very</c>

00:06:24.660 --> 00:06:24.670 align:start position:0%
process each example is going to be very
 

00:06:24.670 --> 00:06:27.450 align:start position:0%
process each example is going to be very
inefficient<00:06:25.230><c> so</c><00:06:26.230><c> what</c><00:06:26.590><c> works</c><00:06:26.830><c> best</c><00:06:27.130><c> in</c>

00:06:27.450 --> 00:06:27.460 align:start position:0%
inefficient so what works best in
 

00:06:27.460 --> 00:06:31.380 align:start position:0%
inefficient so what works best in
practice<00:06:28.240><c> is</c><00:06:28.540><c> something</c><00:06:28.990><c> in</c><00:06:29.200><c> between</c><00:06:30.390><c> where</c>

00:06:31.380 --> 00:06:31.390 align:start position:0%
practice is something in between where
 

00:06:31.390 --> 00:06:37.590 align:start position:0%
practice is something in between where
you<00:06:31.510><c> have</c><00:06:31.750><c> some</c><00:06:32.140><c> you</c><00:06:32.800><c> know</c><00:06:36.000><c> mini</c><00:06:37.000><c> batch</c><00:06:37.270><c> size</c>

00:06:37.590 --> 00:06:37.600 align:start position:0%
you have some you know mini batch size
 

00:06:37.600 --> 00:06:44.880 align:start position:0%
you have some you know mini batch size
that<00:06:38.440><c> not</c><00:06:39.010><c> too</c><00:06:39.220><c> big</c><00:06:39.430><c> or</c><00:06:39.580><c> too</c><00:06:39.790><c> small</c><00:06:41.940><c> and</c><00:06:43.890><c> this</c>

00:06:44.880 --> 00:06:44.890 align:start position:0%
that not too big or too small and this
 

00:06:44.890 --> 00:06:48.660 align:start position:0%
that not too big or too small and this
gives<00:06:45.250><c> you</c><00:06:46.680><c> impractical</c><00:06:47.680><c> fastest</c><00:06:48.190><c> learning</c>

00:06:48.660 --> 00:06:48.670 align:start position:0%
gives you impractical fastest learning
 

00:06:48.670 --> 00:06:53.700 align:start position:0%
gives you impractical fastest learning
and<00:06:50.880><c> you</c><00:06:51.880><c> notice</c><00:06:52.270><c> that</c><00:06:52.420><c> this</c><00:06:52.870><c> has</c><00:06:53.080><c> two</c><00:06:53.380><c> good</c>

00:06:53.700 --> 00:06:53.710 align:start position:0%
and you notice that this has two good
 

00:06:53.710 --> 00:06:55.740 align:start position:0%
and you notice that this has two good
things<00:06:53.920><c> going</c><00:06:54.250><c> for</c><00:06:54.490><c> it</c><00:06:54.580><c> one</c><00:06:54.850><c> is</c><00:06:55.120><c> that</c><00:06:55.300><c> you</c><00:06:55.510><c> do</c>

00:06:55.740 --> 00:06:55.750 align:start position:0%
things going for it one is that you do
 

00:06:55.750 --> 00:06:59.040 align:start position:0%
things going for it one is that you do
get<00:06:56.020><c> a</c><00:06:56.080><c> lot</c><00:06:56.380><c> of</c><00:06:56.530><c> vectorization</c><00:06:57.420><c> so</c><00:06:58.420><c> in</c><00:06:58.840><c> the</c>

00:06:59.040 --> 00:06:59.050 align:start position:0%
get a lot of vectorization so in the
 

00:06:59.050 --> 00:07:02.250 align:start position:0%
get a lot of vectorization so in the
example<00:06:59.710><c> we</c><00:07:00.190><c> use</c><00:07:00.400><c> on</c><00:07:00.580><c> the</c><00:07:00.670><c> previous</c><00:07:01.120><c> video</c><00:07:01.480><c> if</c>

00:07:02.250 --> 00:07:02.260 align:start position:0%
example we use on the previous video if
 

00:07:02.260 --> 00:07:04.200 align:start position:0%
example we use on the previous video if
your<00:07:02.710><c> mini</c><00:07:02.980><c> batch</c><00:07:03.190><c> size</c><00:07:03.460><c> was</c><00:07:03.760><c> a</c><00:07:03.790><c> thousand</c>

00:07:04.200 --> 00:07:04.210 align:start position:0%
your mini batch size was a thousand
 

00:07:04.210 --> 00:07:06.270 align:start position:0%
your mini batch size was a thousand
examples<00:07:04.480><c> then</c><00:07:05.050><c> you</c><00:07:05.590><c> know</c><00:07:05.710><c> you</c><00:07:05.830><c> might</c><00:07:06.070><c> go</c><00:07:06.220><c> to</c>

00:07:06.270 --> 00:07:06.280 align:start position:0%
examples then you know you might go to
 

00:07:06.280 --> 00:07:08.460 align:start position:0%
examples then you know you might go to
vectorize<00:07:06.730><c> across</c><00:07:07.210><c> a</c><00:07:07.240><c> thousand</c><00:07:07.810><c> examples</c><00:07:07.930><c> so</c>

00:07:08.460 --> 00:07:08.470 align:start position:0%
vectorize across a thousand examples so
 

00:07:08.470 --> 00:07:09.810 align:start position:0%
vectorize across a thousand examples so
it's<00:07:08.620><c> going</c><00:07:08.800><c> to</c><00:07:08.860><c> be</c><00:07:08.920><c> much</c><00:07:09.130><c> faster</c><00:07:09.340><c> than</c>

00:07:09.810 --> 00:07:09.820 align:start position:0%
it's going to be much faster than
 

00:07:09.820 --> 00:07:12.060 align:start position:0%
it's going to be much faster than
processing<00:07:10.300><c> the</c><00:07:10.900><c> examples</c><00:07:11.350><c> one</c><00:07:11.620><c> at</c><00:07:11.770><c> a</c><00:07:11.860><c> time</c>

00:07:12.060 --> 00:07:12.070 align:start position:0%
processing the examples one at a time
 

00:07:12.070 --> 00:07:21.830 align:start position:0%
processing the examples one at a time
and<00:07:13.230><c> second</c><00:07:14.230><c> you</c><00:07:14.350><c> can</c><00:07:14.500><c> also</c><00:07:14.880><c> make</c><00:07:15.880><c> progress</c>

00:07:21.830 --> 00:07:21.840 align:start position:0%
 
 

00:07:21.840 --> 00:07:24.719 align:start position:0%
 
without<00:07:22.840><c> needing</c><00:07:23.230><c> to</c><00:07:23.260><c> wait</c>

00:07:24.719 --> 00:07:24.729 align:start position:0%
without needing to wait
 

00:07:24.729 --> 00:07:31.710 align:start position:0%
without needing to wait
till<00:07:25.729><c> you</c><00:07:26.000><c> process</c><00:07:26.479><c> the</c><00:07:26.629><c> entire</c><00:07:26.720><c> training</c><00:07:27.379><c> set</c>

00:07:31.710 --> 00:07:31.720 align:start position:0%
 
 

00:07:31.720 --> 00:07:34.180 align:start position:0%
 
so<00:07:32.720><c> again</c><00:07:32.960><c> using</c><00:07:33.229><c> the</c><00:07:33.440><c> numbers</c><00:07:33.740><c> we</c><00:07:33.889><c> have</c><00:07:34.039><c> in</c>

00:07:34.180 --> 00:07:34.190 align:start position:0%
so again using the numbers we have in
 

00:07:34.190 --> 00:07:36.879 align:start position:0%
so again using the numbers we have in
the<00:07:34.280><c> previous</c><00:07:34.490><c> video</c><00:07:34.940><c> in</c><00:07:35.419><c> epochal</c><00:07:36.199><c> each</c><00:07:36.590><c> path</c>

00:07:36.879 --> 00:07:36.889 align:start position:0%
the previous video in epochal each path
 

00:07:36.889 --> 00:07:38.499 align:start position:0%
the previous video in epochal each path
to<00:07:37.099><c> your</c><00:07:37.190><c> training</c><00:07:37.520><c> set</c><00:07:37.669><c> allows</c><00:07:37.970><c> you</c><00:07:38.180><c> to</c><00:07:38.330><c> take</c>

00:07:38.499 --> 00:07:38.509 align:start position:0%
to your training set allows you to take
 

00:07:38.509 --> 00:07:41.920 align:start position:0%
to your training set allows you to take
5000<00:07:39.199><c> gradient</c><00:07:39.860><c> descent</c><00:07:40.159><c> steps</c><00:07:40.870><c> so</c><00:07:41.870><c> in</c>

00:07:41.920 --> 00:07:41.930 align:start position:0%
5000 gradient descent steps so in
 

00:07:41.930 --> 00:07:43.839 align:start position:0%
5000 gradient descent steps so in
practice<00:07:42.349><c> there</c><00:07:42.530><c> be</c><00:07:42.620><c> some</c><00:07:42.830><c> in-between</c><00:07:43.490><c> mini</c>

00:07:43.839 --> 00:07:43.849 align:start position:0%
practice there be some in-between mini
 

00:07:43.849 --> 00:07:47.290 align:start position:0%
practice there be some in-between mini
batch<00:07:44.090><c> size</c><00:07:44.479><c> that</c><00:07:44.960><c> works</c><00:07:45.229><c> best</c><00:07:45.620><c> and</c><00:07:45.919><c> so</c><00:07:46.610><c> with</c>

00:07:47.290 --> 00:07:47.300 align:start position:0%
batch size that works best and so with
 

00:07:47.300 --> 00:07:48.879 align:start position:0%
batch size that works best and so with
mini<00:07:47.539><c> bearing</c><00:07:47.960><c> assembly</c><00:07:48.169><c> to</c><00:07:48.379><c> start</c><00:07:48.620><c> here</c>

00:07:48.879 --> 00:07:48.889 align:start position:0%
mini bearing assembly to start here
 

00:07:48.889 --> 00:07:51.189 align:start position:0%
mini bearing assembly to start here
maybe<00:07:49.430><c> one</c><00:07:49.729><c> iteration</c><00:07:50.300><c> does</c><00:07:50.509><c> this</c><00:07:50.720><c> two</c>

00:07:51.189 --> 00:07:51.199 align:start position:0%
maybe one iteration does this two
 

00:07:51.199 --> 00:07:54.490 align:start position:0%
maybe one iteration does this two
iterations<00:07:51.590><c> three</c><00:07:52.190><c> four</c><00:07:52.729><c> you</c><00:07:53.629><c> know</c><00:07:53.750><c> and</c><00:07:53.960><c> it's</c>

00:07:54.490 --> 00:07:54.500 align:start position:0%
iterations three four you know and it's
 

00:07:54.500 --> 00:07:56.320 align:start position:0%
iterations three four you know and it's
not<00:07:54.650><c> a</c><00:07:54.710><c> guarantee</c><00:07:55.220><c> to</c><00:07:55.370><c> always</c><00:07:55.789><c> head</c><00:07:56.030><c> toward</c>

00:07:56.320 --> 00:07:56.330 align:start position:0%
not a guarantee to always head toward
 

00:07:56.330 --> 00:08:00.309 align:start position:0%
not a guarantee to always head toward
the<00:07:56.360><c> minimum</c><00:07:56.840><c> but</c><00:07:57.759><c> it</c><00:07:58.759><c> tends</c><00:07:59.120><c> to</c><00:07:59.210><c> head</c><00:07:59.930><c> more</c>

00:08:00.309 --> 00:08:00.319 align:start position:0%
the minimum but it tends to head more
 

00:08:00.319 --> 00:08:01.870 align:start position:0%
the minimum but it tends to head more
consistently<00:08:00.889><c> in</c><00:08:01.009><c> the</c><00:08:01.130><c> rational</c><00:08:01.460><c> minimum</c>

00:08:01.870 --> 00:08:01.880 align:start position:0%
consistently in the rational minimum
 

00:08:01.880 --> 00:08:03.790 align:start position:0%
consistently in the rational minimum
than<00:08:02.060><c> stochastic</c><00:08:02.599><c> during</c><00:08:02.750><c> descent</c><00:08:03.080><c> and</c><00:08:03.319><c> then</c>

00:08:03.790 --> 00:08:03.800 align:start position:0%
than stochastic during descent and then
 

00:08:03.800 --> 00:08:05.800 align:start position:0%
than stochastic during descent and then
it<00:08:03.949><c> doesn't</c><00:08:04.250><c> always</c><00:08:04.430><c> exactly</c><00:08:05.090><c> convert</c><00:08:05.599><c> your</c>

00:08:05.800 --> 00:08:05.810 align:start position:0%
it doesn't always exactly convert your
 

00:08:05.810 --> 00:08:08.260 align:start position:0%
it doesn't always exactly convert your
oscillate<00:08:06.380><c> in</c><00:08:06.470><c> a</c><00:08:06.530><c> very</c><00:08:06.770><c> small</c><00:08:07.130><c> region</c><00:08:07.340><c> if</c>

00:08:08.260 --> 00:08:08.270 align:start position:0%
oscillate in a very small region if
 

00:08:08.270 --> 00:08:09.909 align:start position:0%
oscillate in a very small region if
that's<00:08:08.539><c> an</c><00:08:08.690><c> issue</c><00:08:08.840><c> you</c><00:08:09.080><c> can</c><00:08:09.139><c> always</c><00:08:09.530><c> reduce</c>

00:08:09.909 --> 00:08:09.919 align:start position:0%
that's an issue you can always reduce
 

00:08:09.919 --> 00:08:11.980 align:start position:0%
that's an issue you can always reduce
the<00:08:10.280><c> learning</c><00:08:10.400><c> rate</c><00:08:10.789><c> slowly</c><00:08:11.210><c> we'll</c><00:08:11.630><c> talk</c><00:08:11.810><c> more</c>

00:08:11.980 --> 00:08:11.990 align:start position:0%
the learning rate slowly we'll talk more
 

00:08:11.990 --> 00:08:13.809 align:start position:0%
the learning rate slowly we'll talk more
about<00:08:12.050><c> learning</c><00:08:12.470><c> rate</c><00:08:12.860><c> detail</c><00:08:13.220><c> how</c><00:08:13.460><c> to</c><00:08:13.490><c> reduce</c>

00:08:13.809 --> 00:08:13.819 align:start position:0%
about learning rate detail how to reduce
 

00:08:13.819 --> 00:08:16.390 align:start position:0%
about learning rate detail how to reduce
our<00:08:14.000><c> learning</c><00:08:14.300><c> rate</c><00:08:14.449><c> in</c><00:08:14.599><c> a</c><00:08:14.630><c> later</c><00:08:14.870><c> video</c><00:08:15.139><c> so</c><00:08:15.560><c> if</c>

00:08:16.390 --> 00:08:16.400 align:start position:0%
our learning rate in a later video so if
 

00:08:16.400 --> 00:08:18.850 align:start position:0%
our learning rate in a later video so if
the<00:08:16.940><c> mini</c><00:08:17.240><c> batch</c><00:08:17.419><c> size</c><00:08:17.719><c> should</c><00:08:17.960><c> not</c><00:08:18.139><c> be</c><00:08:18.289><c> M</c><00:08:18.500><c> and</c>

00:08:18.850 --> 00:08:18.860 align:start position:0%
the mini batch size should not be M and
 

00:08:18.860 --> 00:08:20.110 align:start position:0%
the mini batch size should not be M and
should<00:08:19.039><c> not</c><00:08:19.159><c> be</c><00:08:19.310><c> one</c><00:08:19.550><c> but</c><00:08:19.909><c> it</c><00:08:19.969><c> should</c><00:08:20.090><c> be</c>

00:08:20.110 --> 00:08:20.120 align:start position:0%
should not be one but it should be
 

00:08:20.120 --> 00:08:22.360 align:start position:0%
should not be one but it should be
something<00:08:20.360><c> in</c><00:08:20.719><c> between</c><00:08:20.860><c> how</c><00:08:21.860><c> do</c><00:08:21.919><c> you</c><00:08:22.099><c> go</c><00:08:22.250><c> about</c>

00:08:22.360 --> 00:08:22.370 align:start position:0%
something in between how do you go about
 

00:08:22.370 --> 00:08:23.800 align:start position:0%
something in between how do you go about
choosing<00:08:22.759><c> it</c><00:08:23.120><c> well</c><00:08:23.509><c> here</c><00:08:23.690><c> are</c><00:08:23.780><c> some</c>

00:08:23.800 --> 00:08:23.810 align:start position:0%
choosing it well here are some
 

00:08:23.810 --> 00:08:26.619 align:start position:0%
choosing it well here are some
guidelines<00:08:24.219><c> first</c><00:08:25.219><c> if</c><00:08:25.550><c> you</c><00:08:25.759><c> have</c><00:08:26.030><c> a</c><00:08:26.060><c> small</c>

00:08:26.619 --> 00:08:26.629 align:start position:0%
guidelines first if you have a small
 

00:08:26.629 --> 00:08:32.920 align:start position:0%
guidelines first if you have a small
training<00:08:26.960><c> set</c><00:08:31.000><c> just</c><00:08:32.000><c> use</c><00:08:32.209><c> batch</c><00:08:32.630><c> gradient</c>

00:08:32.920 --> 00:08:32.930 align:start position:0%
training set just use batch gradient
 

00:08:32.930 --> 00:08:37.480 align:start position:0%
training set just use batch gradient
descent<00:08:35.649><c> you</c><00:08:36.649><c> know</c><00:08:36.770><c> if</c><00:08:36.979><c> you</c><00:08:37.099><c> have</c><00:08:37.190><c> the</c><00:08:37.279><c> small</c>

00:08:37.480 --> 00:08:37.490 align:start position:0%
descent you know if you have the small
 

00:08:37.490 --> 00:08:40.600 align:start position:0%
descent you know if you have the small
training<00:08:37.789><c> set</c><00:08:37.909><c> then</c><00:08:38.890><c> no</c><00:08:39.890><c> point</c><00:08:40.159><c> using</c><00:08:40.310><c> the</c>

00:08:40.600 --> 00:08:40.610 align:start position:0%
training set then no point using the
 

00:08:40.610 --> 00:08:42.190 align:start position:0%
training set then no point using the
batch<00:08:40.880><c> render</c><00:08:41.089><c> send</c><00:08:41.419><c> you</c><00:08:41.570><c> can</c><00:08:41.750><c> process</c><00:08:41.959><c> the</c>

00:08:42.190 --> 00:08:42.200 align:start position:0%
batch render send you can process the
 

00:08:42.200 --> 00:08:43.750 align:start position:0%
batch render send you can process the
whole<00:08:42.349><c> training</c><00:08:42.620><c> site</c><00:08:42.890><c> quite</c><00:08:43.159><c> fast</c><00:08:43.430><c> so</c><00:08:43.669><c> you</c>

00:08:43.750 --> 00:08:43.760 align:start position:0%
whole training site quite fast so you
 

00:08:43.760 --> 00:08:45.910 align:start position:0%
whole training site quite fast so you
might<00:08:43.909><c> as</c><00:08:44.029><c> well</c><00:08:44.060><c> use</c><00:08:44.360><c> factory</c><00:08:44.870><c> innocent</c><00:08:45.260><c> what</c>

00:08:45.910 --> 00:08:45.920 align:start position:0%
might as well use factory innocent what
 

00:08:45.920 --> 00:08:47.740 align:start position:0%
might as well use factory innocent what
the<00:08:46.010><c> small</c><00:08:46.250><c> training</c><00:08:46.490><c> set</c><00:08:46.610><c> mean</c><00:08:46.970><c> I</c><00:08:47.270><c> would</c><00:08:47.540><c> say</c>

00:08:47.740 --> 00:08:47.750 align:start position:0%
the small training set mean I would say
 

00:08:47.750 --> 00:08:50.760 align:start position:0%
the small training set mean I would say
you<00:08:48.230><c> know</c><00:08:48.350><c> this</c><00:08:48.589><c> less</c><00:08:48.829><c> than</c><00:08:49.070><c> maybe</c><00:08:49.490><c> 2000</c><00:08:50.270><c> um</c>

00:08:50.760 --> 00:08:50.770 align:start position:0%
you know this less than maybe 2000 um
 

00:08:50.770 --> 00:08:52.930 align:start position:0%
you know this less than maybe 2000 um
would<00:08:51.770><c> be</c><00:08:51.890><c> perfectly</c><00:08:52.250><c> fine</c><00:08:52.459><c> to</c><00:08:52.640><c> just</c><00:08:52.700><c> use</c>

00:08:52.930 --> 00:08:52.940 align:start position:0%
would be perfectly fine to just use
 

00:08:52.940 --> 00:08:55.600 align:start position:0%
would be perfectly fine to just use
battery<00:08:53.390><c> and</c><00:08:53.480><c> descent</c><00:08:53.779><c> otherwise</c><00:08:54.740><c> if</c><00:08:55.399><c> you</c>

00:08:55.600 --> 00:08:55.610 align:start position:0%
battery and descent otherwise if you
 

00:08:55.610 --> 00:08:58.030 align:start position:0%
battery and descent otherwise if you
have<00:08:55.640><c> a</c><00:08:55.760><c> bigger</c><00:08:55.970><c> training</c><00:08:56.270><c> set</c><00:08:56.770><c> typical</c><00:08:57.770><c> mini</c>

00:08:58.030 --> 00:08:58.040 align:start position:0%
have a bigger training set typical mini
 

00:08:58.040 --> 00:09:05.769 align:start position:0%
have a bigger training set typical mini
batch<00:08:58.310><c> sizes</c><00:08:59.770><c> would</c><00:09:00.770><c> be</c><00:09:03.399><c> anything</c><00:09:04.399><c> from</c><00:09:04.730><c> 64</c><00:09:05.540><c> up</c>

00:09:05.769 --> 00:09:05.779 align:start position:0%
batch sizes would be anything from 64 up
 

00:09:05.779 --> 00:09:09.310 align:start position:0%
batch sizes would be anything from 64 up
to<00:09:06.560><c> maybe</c><00:09:07.040><c> 512</c><00:09:07.880><c> are</c><00:09:08.149><c> quite</c><00:09:08.570><c> typical</c><00:09:09.079><c> and</c>

00:09:09.310 --> 00:09:09.320 align:start position:0%
to maybe 512 are quite typical and
 

00:09:09.320 --> 00:09:11.889 align:start position:0%
to maybe 512 are quite typical and
difference<00:09:10.130><c> of</c><00:09:10.250><c> the</c><00:09:10.310><c> way</c><00:09:10.610><c> computer</c><00:09:11.300><c> memory</c><00:09:11.690><c> is</c>

00:09:11.889 --> 00:09:11.899 align:start position:0%
difference of the way computer memory is
 

00:09:11.899 --> 00:09:14.290 align:start position:0%
difference of the way computer memory is
laid<00:09:12.170><c> out</c><00:09:12.200><c> in</c><00:09:12.500><c> Access</c><00:09:12.920><c> sometimes</c><00:09:13.850><c> you</c><00:09:14.060><c> code</c>

00:09:14.290 --> 00:09:14.300 align:start position:0%
laid out in Access sometimes you code
 

00:09:14.300 --> 00:09:17.230 align:start position:0%
laid out in Access sometimes you code
runs<00:09:14.540><c> faster</c><00:09:15.140><c> if</c><00:09:15.470><c> your</c><00:09:16.100><c> mini</c><00:09:16.399><c> batch</c><00:09:16.610><c> size</c><00:09:16.880><c> is</c><00:09:17.209><c> a</c>

00:09:17.230 --> 00:09:17.240 align:start position:0%
runs faster if your mini batch size is a
 

00:09:17.240 --> 00:09:20.410 align:start position:0%
runs faster if your mini batch size is a
lot<00:09:17.839><c> as</c><00:09:18.020><c> the</c><00:09:18.110><c> power</c><00:09:18.380><c> of</c><00:09:18.560><c> two</c><00:09:18.829><c> alright</c><00:09:19.550><c> so</c><00:09:19.730><c> 64</c><00:09:20.270><c> is</c>

00:09:20.410 --> 00:09:20.420 align:start position:0%
lot as the power of two alright so 64 is
 

00:09:20.420 --> 00:09:24.250 align:start position:0%
lot as the power of two alright so 64 is
2<00:09:20.750><c> to</c><00:09:20.990><c> the</c><00:09:21.020><c> 6</c><00:09:21.320><c> to</c><00:09:21.740><c> the</c><00:09:21.770><c> 7</c><00:09:22.279><c> 2</c><00:09:23.000><c> to</c><00:09:23.149><c> the</c><00:09:23.240><c> 8</c><00:09:23.420><c> 2</c><00:09:23.990><c> to</c><00:09:24.140><c> the</c>

00:09:24.250 --> 00:09:24.260 align:start position:0%
2 to the 6 to the 7 2 to the 8 2 to the
 

00:09:24.260 --> 00:09:27.910 align:start position:0%
2 to the 6 to the 7 2 to the 8 2 to the
9<00:09:24.500><c> so</c><00:09:25.339><c> often</c><00:09:26.120><c> I'll</c><00:09:26.390><c> implement</c><00:09:27.020><c> my</c><00:09:27.440><c> mini</c><00:09:27.709><c> batch</c>

00:09:27.910 --> 00:09:27.920 align:start position:0%
9 so often I'll implement my mini batch
 

00:09:27.920 --> 00:09:30.430 align:start position:0%
9 so often I'll implement my mini batch
size<00:09:28.160><c> to</c><00:09:28.399><c> be</c><00:09:28.520><c> a</c><00:09:28.550><c> power</c><00:09:28.850><c> of</c><00:09:28.940><c> 2</c><00:09:29.270><c> I</c><00:09:29.540><c> know</c><00:09:30.110><c> in</c><00:09:30.380><c> the</c>

00:09:30.430 --> 00:09:30.440 align:start position:0%
size to be a power of 2 I know in the
 

00:09:30.440 --> 00:09:32.350 align:start position:0%
size to be a power of 2 I know in the
previous<00:09:30.860><c> video</c><00:09:31.130><c> I</c><00:09:31.190><c> use</c><00:09:31.579><c> in</c><00:09:31.760><c> the</c><00:09:31.880><c> batch</c><00:09:32.149><c> size</c>

00:09:32.350 --> 00:09:32.360 align:start position:0%
previous video I use in the batch size
 

00:09:32.360 --> 00:09:35.050 align:start position:0%
previous video I use in the batch size
of<00:09:32.660><c> 1000</c><00:09:33.380><c> if</c><00:09:33.560><c> you</c><00:09:34.190><c> really</c><00:09:34.430><c> want</c><00:09:34.610><c> to</c><00:09:34.730><c> do</c><00:09:34.880><c> that</c>

00:09:35.050 --> 00:09:35.060 align:start position:0%
of 1000 if you really want to do that
 

00:09:35.060 --> 00:09:35.810 align:start position:0%
of 1000 if you really want to do that
work

00:09:35.810 --> 00:09:35.820 align:start position:0%
work
 

00:09:35.820 --> 00:09:38.960 align:start position:0%
work
you<00:09:35.970><c> just</c><00:09:36.240><c> use</c><00:09:36.750><c> zero</c><00:09:37.190><c> 1024</c><00:09:38.190><c> which</c><00:09:38.400><c> is</c><00:09:38.580><c> to</c><00:09:38.850><c> the</c>

00:09:38.960 --> 00:09:38.970 align:start position:0%
you just use zero 1024 which is to the
 

00:09:38.970 --> 00:09:39.680 align:start position:0%
you just use zero 1024 which is to the
power<00:09:39.180><c> of</c><00:09:39.210><c> 10</c>

00:09:39.680 --> 00:09:39.690 align:start position:0%
power of 10
 

00:09:39.690 --> 00:09:42.710 align:start position:0%
power of 10
and<00:09:39.780><c> you</c><00:09:40.350><c> do</c><00:09:40.500><c> see</c><00:09:40.740><c> mean</c><00:09:41.550><c> batch</c><00:09:41.760><c> sizes</c><00:09:42.060><c> of</c><00:09:42.540><c> size</c>

00:09:42.710 --> 00:09:42.720 align:start position:0%
and you do see mean batch sizes of size
 

00:09:42.720 --> 00:09:47.120 align:start position:0%
and you do see mean batch sizes of size
1024<00:09:43.430><c> it</c><00:09:44.430><c> is</c><00:09:44.610><c> a</c><00:09:44.640><c> bit</c><00:09:44.940><c> more</c><00:09:45.090><c> rare</c><00:09:45.470><c> this</c><00:09:46.470><c> range</c><00:09:46.800><c> of</c>

00:09:47.120 --> 00:09:47.130 align:start position:0%
1024 it is a bit more rare this range of
 

00:09:47.130 --> 00:09:49.190 align:start position:0%
1024 it is a bit more rare this range of
mini<00:09:47.820><c> batch</c><00:09:48.030><c> size</c><00:09:48.300><c> is</c><00:09:48.540><c> a</c><00:09:48.570><c> little</c><00:09:48.720><c> bit</c><00:09:49.020><c> more</c>

00:09:49.190 --> 00:09:49.200 align:start position:0%
mini batch size is a little bit more
 

00:09:49.200 --> 00:09:52.930 align:start position:0%
mini batch size is a little bit more
common<00:09:49.920><c> one</c><00:09:50.880><c> last</c><00:09:51.150><c> tip</c><00:09:51.450><c> is</c><00:09:51.720><c> to</c><00:09:52.200><c> make</c><00:09:52.380><c> sure</c><00:09:52.410><c> that</c>

00:09:52.930 --> 00:09:52.940 align:start position:0%
common one last tip is to make sure that
 

00:09:52.940 --> 00:10:00.280 align:start position:0%
common one last tip is to make sure that
your<00:09:53.940><c> mini</c><00:09:54.300><c> batch</c><00:09:56.360><c> all</c><00:09:57.360><c> of</c><00:09:57.750><c> your</c><00:09:57.960><c> XT</c><00:09:58.880><c> comma</c><00:09:59.880><c> Y</c><00:10:00.150><c> T</c>

00:10:00.280 --> 00:10:00.290 align:start position:0%
your mini batch all of your XT comma Y T
 

00:10:00.290 --> 00:10:04.520 align:start position:0%
your mini batch all of your XT comma Y T
that<00:10:01.290><c> that</c><00:10:01.320><c> fits</c><00:10:01.830><c> in</c><00:10:02.690><c> you</c><00:10:03.690><c> know</c><00:10:03.810><c> CPU</c><00:10:04.260><c> GPU</c>

00:10:04.520 --> 00:10:04.530 align:start position:0%
that that fits in you know CPU GPU
 

00:10:04.530 --> 00:10:09.860 align:start position:0%
that that fits in you know CPU GPU
memory<00:10:05.960><c> and</c><00:10:08.000><c> this</c><00:10:09.000><c> really</c><00:10:09.270><c> depends</c><00:10:09.600><c> on</c><00:10:09.720><c> your</c>

00:10:09.860 --> 00:10:09.870 align:start position:0%
memory and this really depends on your
 

00:10:09.870 --> 00:10:11.510 align:start position:0%
memory and this really depends on your
application<00:10:09.960><c> and</c><00:10:10.650><c> how</c><00:10:10.710><c> large</c><00:10:11.010><c> the</c><00:10:11.190><c> single</c>

00:10:11.510 --> 00:10:11.520 align:start position:0%
application and how large the single
 

00:10:11.520 --> 00:10:14.480 align:start position:0%
application and how large the single
training<00:10:11.760><c> example</c><00:10:11.880><c> is</c><00:10:12.450><c> but</c><00:10:12.920><c> if</c><00:10:13.920><c> you</c><00:10:14.280><c> ever</c>

00:10:14.480 --> 00:10:14.490 align:start position:0%
training example is but if you ever
 

00:10:14.490 --> 00:10:16.190 align:start position:0%
training example is but if you ever
process<00:10:15.030><c> a</c><00:10:15.060><c> mini</c><00:10:15.360><c> batch</c><00:10:15.570><c> that</c><00:10:15.600><c> doesn't</c>

00:10:16.190 --> 00:10:16.200 align:start position:0%
process a mini batch that doesn't
 

00:10:16.200 --> 00:10:18.260 align:start position:0%
process a mini batch that doesn't
actually<00:10:16.320><c> fit</c><00:10:16.680><c> in</c><00:10:16.890><c> CPU</c><00:10:17.340><c> GPU</c><00:10:17.520><c> memory</c><00:10:17.820><c> whatever</c>

00:10:18.260 --> 00:10:18.270 align:start position:0%
actually fit in CPU GPU memory whatever
 

00:10:18.270 --> 00:10:21.080 align:start position:0%
actually fit in CPU GPU memory whatever
using<00:10:18.600><c> the</c><00:10:18.780><c> process</c><00:10:19.170><c> the</c><00:10:19.830><c> data</c><00:10:20.220><c> then</c><00:10:20.730><c> you</c><00:10:20.850><c> find</c>

00:10:21.080 --> 00:10:21.090 align:start position:0%
using the process the data then you find
 

00:10:21.090 --> 00:10:23.090 align:start position:0%
using the process the data then you find
that<00:10:21.180><c> the</c><00:10:21.330><c> performance</c><00:10:21.840><c> suddenly</c><00:10:22.650><c> falls</c><00:10:22.890><c> off</c>

00:10:23.090 --> 00:10:23.100 align:start position:0%
that the performance suddenly falls off
 

00:10:23.100 --> 00:10:26.060 align:start position:0%
that the performance suddenly falls off
a<00:10:23.130><c> cliff</c><00:10:23.550><c> and</c><00:10:23.760><c> is</c><00:10:24.150><c> suddenly</c><00:10:24.480><c> much</c><00:10:24.720><c> worse</c><00:10:25.040><c> so</c><00:10:26.040><c> I</c>

00:10:26.060 --> 00:10:26.070 align:start position:0%
a cliff and is suddenly much worse so I
 

00:10:26.070 --> 00:10:27.740 align:start position:0%
a cliff and is suddenly much worse so I
hope<00:10:26.400><c> this</c><00:10:26.580><c> gives</c><00:10:26.790><c> you</c><00:10:26.970><c> a</c><00:10:27.030><c> sense</c><00:10:27.390><c> of</c><00:10:27.600><c> the</c>

00:10:27.740 --> 00:10:27.750 align:start position:0%
hope this gives you a sense of the
 

00:10:27.750 --> 00:10:30.170 align:start position:0%
hope this gives you a sense of the
typical<00:10:28.230><c> range</c><00:10:28.710><c> of</c><00:10:28.920><c> mini</c><00:10:29.160><c> batch</c><00:10:29.400><c> sizes</c><00:10:29.730><c> that</c>

00:10:30.170 --> 00:10:30.180 align:start position:0%
typical range of mini batch sizes that
 

00:10:30.180 --> 00:10:32.900 align:start position:0%
typical range of mini batch sizes that
people<00:10:30.480><c> use</c><00:10:30.740><c> in</c><00:10:31.740><c> practice</c><00:10:32.040><c> of</c><00:10:32.490><c> course</c><00:10:32.730><c> the</c>

00:10:32.900 --> 00:10:32.910 align:start position:0%
people use in practice of course the
 

00:10:32.910 --> 00:10:34.430 align:start position:0%
people use in practice of course the
mini<00:10:33.120><c> batch</c><00:10:33.330><c> size</c><00:10:33.600><c> is</c><00:10:33.900><c> actually</c><00:10:34.230><c> another</c>

00:10:34.430 --> 00:10:34.440 align:start position:0%
mini batch size is actually another
 

00:10:34.440 --> 00:10:36.740 align:start position:0%
mini batch size is actually another
hyper<00:10:34.800><c> parameter</c><00:10:35.430><c> that</c><00:10:36.000><c> you</c><00:10:36.060><c> might</c><00:10:36.510><c> do</c><00:10:36.720><c> a</c>

00:10:36.740 --> 00:10:36.750 align:start position:0%
hyper parameter that you might do a
 

00:10:36.750 --> 00:10:39.020 align:start position:0%
hyper parameter that you might do a
quick<00:10:37.110><c> search</c><00:10:37.590><c> over</c><00:10:37.860><c> to</c><00:10:38.310><c> try</c><00:10:38.460><c> to</c><00:10:38.520><c> figure</c><00:10:38.910><c> out</c>

00:10:39.020 --> 00:10:39.030 align:start position:0%
quick search over to try to figure out
 

00:10:39.030 --> 00:10:42.290 align:start position:0%
quick search over to try to figure out
which<00:10:39.600><c> one</c><00:10:39.840><c> is</c><00:10:40.770><c> most</c><00:10:41.130><c> efficient</c><00:10:41.640><c> at</c><00:10:41.820><c> reducing</c>

00:10:42.290 --> 00:10:42.300 align:start position:0%
which one is most efficient at reducing
 

00:10:42.300 --> 00:10:44.810 align:start position:0%
which one is most efficient at reducing
your<00:10:42.420><c> cost</c><00:10:42.690><c> function</c><00:10:42.870><c> J</c><00:10:43.320><c> so</c><00:10:43.980><c> what</c><00:10:44.340><c> I</c><00:10:44.400><c> would</c><00:10:44.520><c> do</c>

00:10:44.810 --> 00:10:44.820 align:start position:0%
your cost function J so what I would do
 

00:10:44.820 --> 00:10:46.940 align:start position:0%
your cost function J so what I would do
is<00:10:45.030><c> just</c><00:10:45.060><c> try</c><00:10:45.600><c> a</c><00:10:45.630><c> several</c><00:10:46.230><c> different</c><00:10:46.560><c> values</c>

00:10:46.940 --> 00:10:46.950 align:start position:0%
is just try a several different values
 

00:10:46.950 --> 00:10:49.250 align:start position:0%
is just try a several different values
try<00:10:47.610><c> a</c><00:10:47.640><c> few</c><00:10:47.910><c> different</c><00:10:47.970><c> powers</c><00:10:48.630><c> of</c><00:10:48.810><c> two</c><00:10:48.990><c> and</c>

00:10:49.250 --> 00:10:49.260 align:start position:0%
try a few different powers of two and
 

00:10:49.260 --> 00:10:51.500 align:start position:0%
try a few different powers of two and
then<00:10:49.860><c> see</c><00:10:50.160><c> if</c><00:10:50.280><c> you</c><00:10:50.370><c> could</c><00:10:50.550><c> pick</c><00:10:50.970><c> one</c><00:10:51.210><c> that</c>

00:10:51.500 --> 00:10:51.510 align:start position:0%
then see if you could pick one that
 

00:10:51.510 --> 00:10:54.080 align:start position:0%
then see if you could pick one that
makes<00:10:52.260><c> your</c><00:10:52.470><c> gradient</c><00:10:53.220><c> descent</c><00:10:53.490><c> optimization</c>

00:10:54.080 --> 00:10:54.090 align:start position:0%
makes your gradient descent optimization
 

00:10:54.090 --> 00:10:56.450 align:start position:0%
makes your gradient descent optimization
algorithm<00:10:54.510><c> as</c><00:10:54.750><c> efficient</c><00:10:55.230><c> as</c><00:10:55.350><c> possible</c><00:10:55.860><c> but</c>

00:10:56.450 --> 00:10:56.460 align:start position:0%
algorithm as efficient as possible but
 

00:10:56.460 --> 00:10:58.430 align:start position:0%
algorithm as efficient as possible but
hopefully<00:10:57.030><c> this</c><00:10:57.300><c> gives</c><00:10:57.600><c> you</c><00:10:57.780><c> a</c><00:10:57.930><c> set</c><00:10:58.320><c> of</c>

00:10:58.430 --> 00:10:58.440 align:start position:0%
hopefully this gives you a set of
 

00:10:58.440 --> 00:11:01.310 align:start position:0%
hopefully this gives you a set of
guidelines<00:10:58.680><c> for</c><00:10:59.340><c> how</c><00:10:59.910><c> to</c><00:10:59.970><c> get</c><00:11:00.600><c> started</c><00:11:00.840><c> with</c>

00:11:01.310 --> 00:11:01.320 align:start position:0%
guidelines for how to get started with
 

00:11:01.320 --> 00:11:03.500 align:start position:0%
guidelines for how to get started with
that<00:11:01.680><c> type</c><00:11:01.950><c> of</c><00:11:01.980><c> parameter</c><00:11:02.460><c> search</c><00:11:02.700><c> you</c><00:11:03.390><c> now</c>

00:11:03.500 --> 00:11:03.510 align:start position:0%
that type of parameter search you now
 

00:11:03.510 --> 00:11:05.600 align:start position:0%
that type of parameter search you now
know<00:11:03.720><c> how</c><00:11:03.990><c> to</c><00:11:04.020><c> implement</c><00:11:04.320><c> mimi</c><00:11:05.010><c> bash</c><00:11:05.280><c> great</c>

00:11:05.600 --> 00:11:05.610 align:start position:0%
know how to implement mimi bash great
 

00:11:05.610 --> 00:11:07.520 align:start position:0%
know how to implement mimi bash great
descent<00:11:06.000><c> and</c><00:11:06.150><c> make</c><00:11:06.690><c> your</c><00:11:06.810><c> algorithm</c><00:11:07.140><c> run</c><00:11:07.320><c> much</c>

00:11:07.520 --> 00:11:07.530 align:start position:0%
descent and make your algorithm run much
 

00:11:07.530 --> 00:11:09.380 align:start position:0%
descent and make your algorithm run much
faster<00:11:07.560><c> especially</c><00:11:08.220><c> when</c><00:11:08.850><c> you're</c><00:11:08.970><c> trading</c><00:11:09.180><c> on</c>

00:11:09.380 --> 00:11:09.390 align:start position:0%
faster especially when you're trading on
 

00:11:09.390 --> 00:11:11.420 align:start position:0%
faster especially when you're trading on
a<00:11:09.420><c> large</c><00:11:09.690><c> training</c><00:11:09.990><c> set</c><00:11:10.290><c> but</c><00:11:10.530><c> it</c><00:11:11.130><c> turns</c><00:11:11.310><c> out</c>

00:11:11.420 --> 00:11:11.430 align:start position:0%
a large training set but it turns out
 

00:11:11.430 --> 00:11:13.100 align:start position:0%
a large training set but it turns out
they're<00:11:11.640><c> even</c><00:11:12.060><c> more</c><00:11:12.210><c> efficient</c><00:11:12.660><c> algorithms</c>

00:11:13.100 --> 00:11:13.110 align:start position:0%
they're even more efficient algorithms
 

00:11:13.110 --> 00:11:14.990 align:start position:0%
they're even more efficient algorithms
than<00:11:13.260><c> gradient</c><00:11:13.530><c> descent</c><00:11:13.740><c> or</c><00:11:14.220><c> mini</c><00:11:14.460><c> battery</c><00:11:14.910><c> in</c>

00:11:14.990 --> 00:11:15.000 align:start position:0%
than gradient descent or mini battery in
 

00:11:15.000 --> 00:11:16.850 align:start position:0%
than gradient descent or mini battery in
this<00:11:15.090><c> end</c><00:11:15.330><c> let's</c><00:11:15.570><c> start</c><00:11:16.110><c> talking</c><00:11:16.290><c> about</c><00:11:16.650><c> them</c>

00:11:16.850 --> 00:11:16.860 align:start position:0%
this end let's start talking about them
 

00:11:16.860 --> 00:11:19.670 align:start position:0%
this end let's start talking about them
in<00:11:17.040><c> the</c><00:11:17.190><c> next</c><00:11:17.370><c> few</c><00:11:17.550><c> videos</c>

