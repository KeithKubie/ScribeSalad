WEBVTT
Kind: captions
Language: en

00:00:00.179 --> 00:00:02.570
 
when you breach a neural network one of

00:00:02.570 --> 00:00:02.580
when you breach a neural network one of
 

00:00:02.580 --> 00:00:04.340
when you breach a neural network one of
the choices you get to make is what

00:00:04.340 --> 00:00:04.350
the choices you get to make is what
 

00:00:04.350 --> 00:00:06.710
the choices you get to make is what
activation functions use independent

00:00:06.710 --> 00:00:06.720
activation functions use independent
 

00:00:06.720 --> 00:00:09.589
activation functions use independent
layers as well as at the output unit of

00:00:09.589 --> 00:00:09.599
layers as well as at the output unit of
 

00:00:09.599 --> 00:00:11.480
layers as well as at the output unit of
your neural network so far we've just

00:00:11.480 --> 00:00:11.490
your neural network so far we've just
 

00:00:11.490 --> 00:00:13.160
your neural network so far we've just
been using the sigmoid activation

00:00:13.160 --> 00:00:13.170
been using the sigmoid activation
 

00:00:13.170 --> 00:00:16.070
been using the sigmoid activation
function but sometimes other choices can

00:00:16.070 --> 00:00:16.080
function but sometimes other choices can
 

00:00:16.080 --> 00:00:18.740
function but sometimes other choices can
work much better let's take a look at

00:00:18.740 --> 00:00:18.750
work much better let's take a look at
 

00:00:18.750 --> 00:00:20.929
work much better let's take a look at
some of the options in the forward

00:00:20.929 --> 00:00:20.939
some of the options in the forward
 

00:00:20.939 --> 00:00:23.269
some of the options in the forward
propagation steps for the neural network

00:00:23.269 --> 00:00:23.279
propagation steps for the neural network
 

00:00:23.279 --> 00:00:26.089
propagation steps for the neural network
we have these two steps where we use the

00:00:26.089 --> 00:00:26.099
we have these two steps where we use the
 

00:00:26.099 --> 00:00:28.730
we have these two steps where we use the
sigmoid function here so that sigmoid is

00:00:28.730 --> 00:00:28.740
sigmoid function here so that sigmoid is
 

00:00:28.740 --> 00:00:32.600
sigmoid function here so that sigmoid is
called an activation function and G is

00:00:32.600 --> 00:00:32.610
called an activation function and G is
 

00:00:32.610 --> 00:00:37.580
called an activation function and G is
the familiar sigmoid function N equals 1

00:00:37.580 --> 00:00:37.590
the familiar sigmoid function N equals 1
 

00:00:37.590 --> 00:00:40.459
the familiar sigmoid function N equals 1
over 1 plus e to the negative Z so in

00:00:40.459 --> 00:00:40.469
over 1 plus e to the negative Z so in
 

00:00:40.469 --> 00:00:42.590
over 1 plus e to the negative Z so in
the more general case we can have a

00:00:42.590 --> 00:00:42.600
the more general case we can have a
 

00:00:42.600 --> 00:00:49.729
the more general case we can have a
different function G of Z visually right

00:00:49.729 --> 00:00:49.739
different function G of Z visually right
 

00:00:49.739 --> 00:00:53.299
different function G of Z visually right
here where G could be a nonlinear

00:00:53.299 --> 00:00:53.309
here where G could be a nonlinear
 

00:00:53.309 --> 00:00:56.000
here where G could be a nonlinear
function that may not be the sigmoid

00:00:56.000 --> 00:00:56.010
function that may not be the sigmoid
 

00:00:56.010 --> 00:00:59.240
function that may not be the sigmoid
function so for example the sigmoid

00:00:59.240 --> 00:00:59.250
function so for example the sigmoid
 

00:00:59.250 --> 00:01:01.869
function so for example the sigmoid
function goes between 0 &amp; 1 an

00:01:01.869 --> 00:01:01.879
function goes between 0 &amp; 1 an
 

00:01:01.879 --> 00:01:04.280
function goes between 0 &amp; 1 an
activation function that almost always

00:01:04.280 --> 00:01:04.290
activation function that almost always
 

00:01:04.290 --> 00:01:06.859
activation function that almost always
works better than the sigmoid function

00:01:06.859 --> 00:01:06.869
works better than the sigmoid function
 

00:01:06.869 --> 00:01:10.310
works better than the sigmoid function
is the 10h function or the hyperbolic

00:01:10.310 --> 00:01:10.320
is the 10h function or the hyperbolic
 

00:01:10.320 --> 00:01:14.179
is the 10h function or the hyperbolic
tangent function so this is Z this is a

00:01:14.179 --> 00:01:14.189
tangent function so this is Z this is a
 

00:01:14.189 --> 00:01:19.969
tangent function so this is Z this is a
this is a equals 10 H of Z and this goes

00:01:19.969 --> 00:01:19.979
this is a equals 10 H of Z and this goes
 

00:01:19.979 --> 00:01:25.700
this is a equals 10 H of Z and this goes
between plus 1 and minus 1 the formula

00:01:25.700 --> 00:01:25.710
between plus 1 and minus 1 the formula
 

00:01:25.710 --> 00:01:31.069
between plus 1 and minus 1 the formula
for the 10h function is e to the Z minus

00:01:31.069 --> 00:01:31.079
for the 10h function is e to the Z minus
 

00:01:31.079 --> 00:01:36.789
for the 10h function is e to the Z minus
e to the negative Z over there some and

00:01:36.789 --> 00:01:36.799
e to the negative Z over there some and
 

00:01:36.799 --> 00:01:40.130
e to the negative Z over there some and
it's actually mathematically a shifted

00:01:40.130 --> 00:01:40.140
it's actually mathematically a shifted
 

00:01:40.140 --> 00:01:43.639
it's actually mathematically a shifted
version of the sigmoid function so as a

00:01:43.639 --> 00:01:43.649
version of the sigmoid function so as a
 

00:01:43.649 --> 00:01:46.340
version of the sigmoid function so as a
you know sigmoid function just like that

00:01:46.340 --> 00:01:46.350
you know sigmoid function just like that
 

00:01:46.350 --> 00:01:49.819
you know sigmoid function just like that
but shift it so that it now crosses a

00:01:49.819 --> 00:01:49.829
but shift it so that it now crosses a
 

00:01:49.829 --> 00:01:52.069
but shift it so that it now crosses a
zero zero point and rescale so it goes

00:01:52.069 --> 00:01:52.079
zero zero point and rescale so it goes
 

00:01:52.079 --> 00:01:54.560
zero zero point and rescale so it goes
to G minus one and plus one and it turns

00:01:54.560 --> 00:01:54.570
to G minus one and plus one and it turns
 

00:01:54.570 --> 00:01:58.520
to G minus one and plus one and it turns
out that for hidden units if you let the

00:01:58.520 --> 00:01:58.530
out that for hidden units if you let the
 

00:01:58.530 --> 00:02:08.540
out that for hidden units if you let the
function G of Z be equal to 10 into Z

00:02:08.540 --> 00:02:08.550
function G of Z be equal to 10 into Z
 

00:02:08.550 --> 00:02:10.480
function G of Z be equal to 10 into Z
almost always works better than the

00:02:10.480 --> 00:02:10.490
almost always works better than the
 

00:02:10.490 --> 00:02:12.890
almost always works better than the
sigmoid function because with values

00:02:12.890 --> 00:02:12.900
sigmoid function because with values
 

00:02:12.900 --> 00:02:15.350
sigmoid function because with values
between plus 1 and minus 1 the mean of

00:02:15.350 --> 00:02:15.360
between plus 1 and minus 1 the mean of
 

00:02:15.360 --> 00:02:17.180
between plus 1 and minus 1 the mean of
the activations that come out of your

00:02:17.180 --> 00:02:17.190
the activations that come out of your
 

00:02:17.190 --> 00:02:19.880
the activations that come out of your
head and they're closer to having a zero

00:02:19.880 --> 00:02:19.890
head and they're closer to having a zero
 

00:02:19.890 --> 00:02:22.370
head and they're closer to having a zero
mean and so just as sometimes when you

00:02:22.370 --> 00:02:22.380
mean and so just as sometimes when you
 

00:02:22.380 --> 00:02:23.570
mean and so just as sometimes when you
train a learning algorithm

00:02:23.570 --> 00:02:23.580
train a learning algorithm
 

00:02:23.580 --> 00:02:25.700
train a learning algorithm
you might Center the data and have your

00:02:25.700 --> 00:02:25.710
you might Center the data and have your
 

00:02:25.710 --> 00:02:29.120
you might Center the data and have your
data has zero mean using a technique

00:02:29.120 --> 00:02:29.130
data has zero mean using a technique
 

00:02:29.130 --> 00:02:31.190
data has zero mean using a technique
instead of a sigmoid function kind of

00:02:31.190 --> 00:02:31.200
instead of a sigmoid function kind of
 

00:02:31.200 --> 00:02:34.700
instead of a sigmoid function kind of
has the effect of centering your data so

00:02:34.700 --> 00:02:34.710
has the effect of centering your data so
 

00:02:34.710 --> 00:02:36.770
has the effect of centering your data so
that the mean of the data is close to

00:02:36.770 --> 00:02:36.780
that the mean of the data is close to
 

00:02:36.780 --> 00:02:39.440
that the mean of the data is close to
the zero rather than maybe a 0.5 and

00:02:39.440 --> 00:02:39.450
the zero rather than maybe a 0.5 and
 

00:02:39.450 --> 00:02:41.120
the zero rather than maybe a 0.5 and
this actually makes learning for the

00:02:41.120 --> 00:02:41.130
this actually makes learning for the
 

00:02:41.130 --> 00:02:43.490
this actually makes learning for the
next layer a little bit easier we'll say

00:02:43.490 --> 00:02:43.500
next layer a little bit easier we'll say
 

00:02:43.500 --> 00:02:45.440
next layer a little bit easier we'll say
more about this in the second course

00:02:45.440 --> 00:02:45.450
more about this in the second course
 

00:02:45.450 --> 00:02:46.400
more about this in the second course
when we talk about optimization

00:02:46.400 --> 00:02:46.410
when we talk about optimization
 

00:02:46.410 --> 00:02:49.040
when we talk about optimization
algorithms as well but one takeaway is

00:02:49.040 --> 00:02:49.050
algorithms as well but one takeaway is
 

00:02:49.050 --> 00:02:52.310
algorithms as well but one takeaway is
that I pretty much never use the sigmoid

00:02:52.310 --> 00:02:52.320
that I pretty much never use the sigmoid
 

00:02:52.320 --> 00:02:54.230
that I pretty much never use the sigmoid
activation function anymore

00:02:54.230 --> 00:02:54.240
activation function anymore
 

00:02:54.240 --> 00:02:56.390
activation function anymore
the 10h function is almost always

00:02:56.390 --> 00:02:56.400
the 10h function is almost always
 

00:02:56.400 --> 00:02:59.540
the 10h function is almost always
strictly superior the one exception is

00:02:59.540 --> 00:02:59.550
strictly superior the one exception is
 

00:02:59.550 --> 00:03:03.560
strictly superior the one exception is
for the output layer because if Y is

00:03:03.560 --> 00:03:03.570
for the output layer because if Y is
 

00:03:03.570 --> 00:03:07.400
for the output layer because if Y is
either 0 or 1 then it makes sense for Y

00:03:07.400 --> 00:03:07.410
either 0 or 1 then it makes sense for Y
 

00:03:07.410 --> 00:03:10.580
either 0 or 1 then it makes sense for Y
hat to be a number that you want to

00:03:10.580 --> 00:03:10.590
hat to be a number that you want to
 

00:03:10.590 --> 00:03:13.670
hat to be a number that you want to
output that's between 0 and 1 rather

00:03:13.670 --> 00:03:13.680
output that's between 0 and 1 rather
 

00:03:13.680 --> 00:03:16.550
output that's between 0 and 1 rather
than between minus 1 and 1 so the one

00:03:16.550 --> 00:03:16.560
than between minus 1 and 1 so the one
 

00:03:16.560 --> 00:03:19.340
than between minus 1 and 1 so the one
exception where I would use the sigmoid

00:03:19.340 --> 00:03:19.350
exception where I would use the sigmoid
 

00:03:19.350 --> 00:03:21.260
exception where I would use the sigmoid
activation function is when you are

00:03:21.260 --> 00:03:21.270
activation function is when you are
 

00:03:21.270 --> 00:03:23.810
activation function is when you are
using binary classification in which

00:03:23.810 --> 00:03:23.820
using binary classification in which
 

00:03:23.820 --> 00:03:26.030
using binary classification in which
case you values the sigmoid activation

00:03:26.030 --> 00:03:26.040
case you values the sigmoid activation
 

00:03:26.040 --> 00:03:29.690
case you values the sigmoid activation
function for the output layer so G of Z

00:03:29.690 --> 00:03:29.700
function for the output layer so G of Z
 

00:03:29.700 --> 00:03:35.150
function for the output layer so G of Z
2 here is equal to Sigma of Z 2 and so

00:03:35.150 --> 00:03:35.160
2 here is equal to Sigma of Z 2 and so
 

00:03:35.160 --> 00:03:37.160
2 here is equal to Sigma of Z 2 and so
what you see in this example is where

00:03:37.160 --> 00:03:37.170
what you see in this example is where
 

00:03:37.170 --> 00:03:40.780
what you see in this example is where
you might have a 10h activation function

00:03:40.780 --> 00:03:40.790
you might have a 10h activation function
 

00:03:40.790 --> 00:03:46.490
you might have a 10h activation function
for the hidden layer and sigmoid for the

00:03:46.490 --> 00:03:46.500
for the hidden layer and sigmoid for the
 

00:03:46.500 --> 00:03:48.560
for the hidden layer and sigmoid for the
output layer so the activation functions

00:03:48.560 --> 00:03:48.570
output layer so the activation functions
 

00:03:48.570 --> 00:03:49.940
output layer so the activation functions
can be different for different layers

00:03:49.940 --> 00:03:49.950
can be different for different layers
 

00:03:49.950 --> 00:03:52.430
can be different for different layers
and sometimes to denote that the

00:03:52.430 --> 00:03:52.440
and sometimes to denote that the
 

00:03:52.440 --> 00:03:54.140
and sometimes to denote that the
activation functions are different for

00:03:54.140 --> 00:03:54.150
activation functions are different for
 

00:03:54.150 --> 00:03:56.560
activation functions are different for
different layers we might use these

00:03:56.560 --> 00:03:56.570
different layers we might use these
 

00:03:56.570 --> 00:03:59.780
different layers we might use these
square brackets superscripts as law to

00:03:59.780 --> 00:03:59.790
square brackets superscripts as law to
 

00:03:59.790 --> 00:04:03.260
square brackets superscripts as law to
indicate that G of square bracket 1 may

00:04:03.260 --> 00:04:03.270
indicate that G of square bracket 1 may
 

00:04:03.270 --> 00:04:05.030
indicate that G of square bracket 1 may
be different than G Oh square bracket to

00:04:05.030 --> 00:04:05.040
be different than G Oh square bracket to
 

00:04:05.040 --> 00:04:06.920
be different than G Oh square bracket to
Grandma gain square bracket 1

00:04:06.920 --> 00:04:06.930
Grandma gain square bracket 1
 

00:04:06.930 --> 00:04:09.350
Grandma gain square bracket 1
superscript refers to this layer and

00:04:09.350 --> 00:04:09.360
superscript refers to this layer and
 

00:04:09.360 --> 00:04:11.449
superscript refers to this layer and
superscript square bracket 2 refers to

00:04:11.449 --> 00:04:11.459
superscript square bracket 2 refers to
 

00:04:11.459 --> 00:04:13.360
superscript square bracket 2 refers to
the Alpha layer

00:04:13.360 --> 00:04:13.370
the Alpha layer
 

00:04:13.370 --> 00:04:15.670
the Alpha layer
now one of the downsides of both the

00:04:15.670 --> 00:04:15.680
now one of the downsides of both the
 

00:04:15.680 --> 00:04:17.890
now one of the downsides of both the
sigmoid function and the 10-ish function

00:04:17.890 --> 00:04:17.900
sigmoid function and the 10-ish function
 

00:04:17.900 --> 00:04:20.740
sigmoid function and the 10-ish function
is that it Z is either very large or

00:04:20.740 --> 00:04:20.750
is that it Z is either very large or
 

00:04:20.750 --> 00:04:22.840
is that it Z is either very large or
very small then the gradient or the

00:04:22.840 --> 00:04:22.850
very small then the gradient or the
 

00:04:22.850 --> 00:04:24.450
very small then the gradient or the
derivative or the slope of this function

00:04:24.450 --> 00:04:24.460
derivative or the slope of this function
 

00:04:24.460 --> 00:04:27.550
derivative or the slope of this function
becomes very small so Z is very large or

00:04:27.550 --> 00:04:27.560
becomes very small so Z is very large or
 

00:04:27.560 --> 00:04:30.129
becomes very small so Z is very large or
Z is very small the slope of the

00:04:30.129 --> 00:04:30.139
Z is very small the slope of the
 

00:04:30.139 --> 00:04:33.159
Z is very small the slope of the
function you know ends up being close to

00:04:33.159 --> 00:04:33.169
function you know ends up being close to
 

00:04:33.169 --> 00:04:35.260
function you know ends up being close to
zero and so this can slow down gradient

00:04:35.260 --> 00:04:35.270
zero and so this can slow down gradient
 

00:04:35.270 --> 00:04:38.350
zero and so this can slow down gradient
descent so one other choice that is very

00:04:38.350 --> 00:04:38.360
descent so one other choice that is very
 

00:04:38.360 --> 00:04:41.800
descent so one other choice that is very
popular in machine learning is what's

00:04:41.800 --> 00:04:41.810
popular in machine learning is what's
 

00:04:41.810 --> 00:04:44.890
popular in machine learning is what's
called the rectified linear unit so the

00:04:44.890 --> 00:04:44.900
called the rectified linear unit so the
 

00:04:44.900 --> 00:04:50.710
called the rectified linear unit so the
rayleigh function looks like this and a

00:04:50.710 --> 00:04:50.720
rayleigh function looks like this and a
 

00:04:50.720 --> 00:04:57.100
rayleigh function looks like this and a
formula is a equals max of 0 comma Z so

00:04:57.100 --> 00:04:57.110
formula is a equals max of 0 comma Z so
 

00:04:57.110 --> 00:05:00.490
formula is a equals max of 0 comma Z so
the derivative is 1 so long as Z is

00:05:00.490 --> 00:05:00.500
the derivative is 1 so long as Z is
 

00:05:00.500 --> 00:05:03.520
the derivative is 1 so long as Z is
positive and derivative or the slope is

00:05:03.520 --> 00:05:03.530
positive and derivative or the slope is
 

00:05:03.530 --> 00:05:05.950
positive and derivative or the slope is
0 when Z is negative if you're

00:05:05.950 --> 00:05:05.960
0 when Z is negative if you're
 

00:05:05.960 --> 00:05:07.570
0 when Z is negative if you're
implementing this technically the

00:05:07.570 --> 00:05:07.580
implementing this technically the
 

00:05:07.580 --> 00:05:10.180
implementing this technically the
derivative when Z is exactly 0 is not

00:05:10.180 --> 00:05:10.190
derivative when Z is exactly 0 is not
 

00:05:10.190 --> 00:05:10.900
derivative when Z is exactly 0 is not
well-defined

00:05:10.900 --> 00:05:10.910
well-defined
 

00:05:10.910 --> 00:05:12.520
well-defined
but when you implement is in the

00:05:12.520 --> 00:05:12.530
but when you implement is in the
 

00:05:12.530 --> 00:05:15.189
but when you implement is in the
computer the often you get exactly is

00:05:15.189 --> 00:05:15.199
computer the often you get exactly is
 

00:05:15.199 --> 00:05:19.420
computer the often you get exactly is
equal to 0 0 0 0 0 0 0 0 0 0 0 it is

00:05:19.420 --> 00:05:19.430
equal to 0 0 0 0 0 0 0 0 0 0 0 it is
 

00:05:19.430 --> 00:05:21.400
equal to 0 0 0 0 0 0 0 0 0 0 0 it is
very small so you don't need to worry

00:05:21.400 --> 00:05:21.410
very small so you don't need to worry
 

00:05:21.410 --> 00:05:23.830
very small so you don't need to worry
about it in practice you could pretend a

00:05:23.830 --> 00:05:23.840
about it in practice you could pretend a
 

00:05:23.840 --> 00:05:26.830
about it in practice you could pretend a
derivative when Z is equal to 0 you can

00:05:26.830 --> 00:05:26.840
derivative when Z is equal to 0 you can
 

00:05:26.840 --> 00:05:30.850
derivative when Z is equal to 0 you can
pretend is either 1 or 0 MN and you can

00:05:30.850 --> 00:05:30.860
pretend is either 1 or 0 MN and you can
 

00:05:30.860 --> 00:05:32.650
pretend is either 1 or 0 MN and you can
work just fine so the fact that is not

00:05:32.650 --> 00:05:32.660
work just fine so the fact that is not
 

00:05:32.660 --> 00:05:35.920
work just fine so the fact that is not
differentiable the fact that so here are

00:05:35.920 --> 00:05:35.930
differentiable the fact that so here are
 

00:05:35.930 --> 00:05:37.779
differentiable the fact that so here are
some rules of thumb for choosing

00:05:37.779 --> 00:05:37.789
some rules of thumb for choosing
 

00:05:37.789 --> 00:05:41.860
some rules of thumb for choosing
activation functions if your output is 0

00:05:41.860 --> 00:05:41.870
activation functions if your output is 0
 

00:05:41.870 --> 00:05:43.629
activation functions if your output is 0
one value if your I'm using binary

00:05:43.629 --> 00:05:43.639
one value if your I'm using binary
 

00:05:43.639 --> 00:05:45.640
one value if your I'm using binary
classification then the sigmoid

00:05:45.640 --> 00:05:45.650
classification then the sigmoid
 

00:05:45.650 --> 00:05:47.260
classification then the sigmoid
activation function is a very natural

00:05:47.260 --> 00:05:47.270
activation function is a very natural
 

00:05:47.270 --> 00:05:49.930
activation function is a very natural
for is for the upper layer and then for

00:05:49.930 --> 00:05:49.940
for is for the upper layer and then for
 

00:05:49.940 --> 00:05:56.969
for is for the upper layer and then for
all other units on value or the

00:05:56.969 --> 00:05:56.979
 
 

00:05:56.979 --> 00:06:02.950
 
rectified linear unit is increasingly

00:06:02.950 --> 00:06:02.960
rectified linear unit is increasingly
 

00:06:02.960 --> 00:06:05.230
rectified linear unit is increasingly
the default choice of activation

00:06:05.230 --> 00:06:05.240
the default choice of activation
 

00:06:05.240 --> 00:06:07.750
the default choice of activation
function so if you're not sure what to

00:06:07.750 --> 00:06:07.760
function so if you're not sure what to
 

00:06:07.760 --> 00:06:11.290
function so if you're not sure what to
use um for your head in there I would

00:06:11.290 --> 00:06:11.300
use um for your head in there I would
 

00:06:11.300 --> 00:06:14.260
use um for your head in there I would
just use the relu activation function

00:06:14.260 --> 00:06:14.270
just use the relu activation function
 

00:06:14.270 --> 00:06:15.640
just use the relu activation function
that's what you see most people using

00:06:15.640 --> 00:06:15.650
that's what you see most people using
 

00:06:15.650 --> 00:06:17.920
that's what you see most people using
these days although sometimes people

00:06:17.920 --> 00:06:17.930
these days although sometimes people
 

00:06:17.930 --> 00:06:20.580
these days although sometimes people
also use the tannish activation function

00:06:20.580 --> 00:06:20.590
also use the tannish activation function
 

00:06:20.590 --> 00:06:23.020
also use the tannish activation function
once this advantage of the value is that

00:06:23.020 --> 00:06:23.030
once this advantage of the value is that
 

00:06:23.030 --> 00:06:26.480
once this advantage of the value is that
the derivative is equal to 0 when we

00:06:26.480 --> 00:06:26.490
the derivative is equal to 0 when we
 

00:06:26.490 --> 00:06:28.610
the derivative is equal to 0 when we
negative in practice this works just

00:06:28.610 --> 00:06:28.620
negative in practice this works just
 

00:06:28.620 --> 00:06:31.700
negative in practice this works just
fine but there is another version of the

00:06:31.700 --> 00:06:31.710
fine but there is another version of the
 

00:06:31.710 --> 00:06:33.950
fine but there is another version of the
value called the VG value will give you

00:06:33.950 --> 00:06:33.960
value called the VG value will give you
 

00:06:33.960 --> 00:06:35.420
value called the VG value will give you
the formula on the next slide but

00:06:35.420 --> 00:06:35.430
the formula on the next slide but
 

00:06:35.430 --> 00:06:38.719
the formula on the next slide but
instead of it being zero when Z is

00:06:38.719 --> 00:06:38.729
instead of it being zero when Z is
 

00:06:38.729 --> 00:06:40.550
instead of it being zero when Z is
negative it just takes a slight slope

00:06:40.550 --> 00:06:40.560
negative it just takes a slight slope
 

00:06:40.560 --> 00:06:42.909
negative it just takes a slight slope
like so so this is called the Whiskey

00:06:42.909 --> 00:06:42.919
like so so this is called the Whiskey
 

00:06:42.919 --> 00:06:47.900
like so so this is called the Whiskey
value this usually works better than the

00:06:47.900 --> 00:06:47.910
value this usually works better than the
 

00:06:47.910 --> 00:06:51.110
value this usually works better than the
value activation function although it's

00:06:51.110 --> 00:06:51.120
value activation function although it's
 

00:06:51.120 --> 00:06:53.870
value activation function although it's
just not used as much in practice either

00:06:53.870 --> 00:06:53.880
just not used as much in practice either
 

00:06:53.880 --> 00:06:55.790
just not used as much in practice either
one should be fine although it you had

00:06:55.790 --> 00:06:55.800
one should be fine although it you had
 

00:06:55.800 --> 00:06:58.370
one should be fine although it you had
to pick one I usually just used in

00:06:58.370 --> 00:06:58.380
to pick one I usually just used in
 

00:06:58.380 --> 00:07:00.439
to pick one I usually just used in
random and the advantage of both the

00:07:00.439 --> 00:07:00.449
random and the advantage of both the
 

00:07:00.449 --> 00:07:02.960
random and the advantage of both the
value and the least value is that for a

00:07:02.960 --> 00:07:02.970
value and the least value is that for a
 

00:07:02.970 --> 00:07:06.230
value and the least value is that for a
lot of the space of Z the derivative of

00:07:06.230 --> 00:07:06.240
lot of the space of Z the derivative of
 

00:07:06.240 --> 00:07:07.820
lot of the space of Z the derivative of
the activation function the slope of the

00:07:07.820 --> 00:07:07.830
the activation function the slope of the
 

00:07:07.830 --> 00:07:10.879
the activation function the slope of the
activation function is very different

00:07:10.879 --> 00:07:10.889
activation function is very different
 

00:07:10.889 --> 00:07:13.670
activation function is very different
from zero and so in practice using the

00:07:13.670 --> 00:07:13.680
from zero and so in practice using the
 

00:07:13.680 --> 00:07:15.680
from zero and so in practice using the
regular activation function your new

00:07:15.680 --> 00:07:15.690
regular activation function your new
 

00:07:15.690 --> 00:07:17.480
regular activation function your new
network will often learn much faster

00:07:17.480 --> 00:07:17.490
network will often learn much faster
 

00:07:17.490 --> 00:07:20.210
network will often learn much faster
than you're using the 10 age or the

00:07:20.210 --> 00:07:20.220
than you're using the 10 age or the
 

00:07:20.220 --> 00:07:22.700
than you're using the 10 age or the
sigmoid activation function and the main

00:07:22.700 --> 00:07:22.710
sigmoid activation function and the main
 

00:07:22.710 --> 00:07:25.219
sigmoid activation function and the main
reason is that on this less of this

00:07:25.219 --> 00:07:25.229
reason is that on this less of this
 

00:07:25.229 --> 00:07:26.659
reason is that on this less of this
effect of the slope of the function

00:07:26.659 --> 00:07:26.669
effect of the slope of the function
 

00:07:26.669 --> 00:07:29.390
effect of the slope of the function
going to zero which slows down learning

00:07:29.390 --> 00:07:29.400
going to zero which slows down learning
 

00:07:29.400 --> 00:07:32.180
going to zero which slows down learning
and I know that for half of the range of

00:07:32.180 --> 00:07:32.190
and I know that for half of the range of
 

00:07:32.190 --> 00:07:35.060
and I know that for half of the range of
Z the slope of value is zero but in

00:07:35.060 --> 00:07:35.070
Z the slope of value is zero but in
 

00:07:35.070 --> 00:07:37.339
Z the slope of value is zero but in
practice enough of your hidden units

00:07:37.339 --> 00:07:37.349
practice enough of your hidden units
 

00:07:37.349 --> 00:07:39.770
practice enough of your hidden units
will have Z greater than zero so

00:07:39.770 --> 00:07:39.780
will have Z greater than zero so
 

00:07:39.780 --> 00:07:41.420
will have Z greater than zero so
learning can still be quite fast for

00:07:41.420 --> 00:07:41.430
learning can still be quite fast for
 

00:07:41.430 --> 00:07:43.670
learning can still be quite fast for
most training examples so let's just

00:07:43.670 --> 00:07:43.680
most training examples so let's just
 

00:07:43.680 --> 00:07:45.770
most training examples so let's just
quickly recap there are pros and cons of

00:07:45.770 --> 00:07:45.780
quickly recap there are pros and cons of
 

00:07:45.780 --> 00:07:47.600
quickly recap there are pros and cons of
different activation functions here's

00:07:47.600 --> 00:07:47.610
different activation functions here's
 

00:07:47.610 --> 00:07:50.029
different activation functions here's
the sigmoid activation function I would

00:07:50.029 --> 00:07:50.039
the sigmoid activation function I would
 

00:07:50.039 --> 00:07:52.790
the sigmoid activation function I would
say never use this except for the output

00:07:52.790 --> 00:07:52.800
say never use this except for the output
 

00:07:52.800 --> 00:07:54.260
say never use this except for the output
layer if you are doing binary

00:07:54.260 --> 00:07:54.270
layer if you are doing binary
 

00:07:54.270 --> 00:07:56.330
layer if you are doing binary
classification when we almost never use

00:07:56.330 --> 00:07:56.340
classification when we almost never use
 

00:07:56.340 --> 00:07:59.510
classification when we almost never use
this and the reason I almost never use

00:07:59.510 --> 00:07:59.520
this and the reason I almost never use
 

00:07:59.520 --> 00:08:02.719
this and the reason I almost never use
this is because the 10 H is pretty much

00:08:02.719 --> 00:08:02.729
this is because the 10 H is pretty much
 

00:08:02.729 --> 00:08:05.029
this is because the 10 H is pretty much
strictly superior so the 10 inch

00:08:05.029 --> 00:08:05.039
strictly superior so the 10 inch
 

00:08:05.039 --> 00:08:12.080
strictly superior so the 10 inch
activation function is this and then the

00:08:12.080 --> 00:08:12.090
activation function is this and then the
 

00:08:12.090 --> 00:08:13.399
activation function is this and then the
default the most commonly used

00:08:13.399 --> 00:08:13.409
default the most commonly used
 

00:08:13.409 --> 00:08:16.490
default the most commonly used
activation function is the Randy which

00:08:16.490 --> 00:08:16.500
activation function is the Randy which
 

00:08:16.500 --> 00:08:19.339
activation function is the Randy which
is this so you're not sure what else you

00:08:19.339 --> 00:08:19.349
is this so you're not sure what else you
 

00:08:19.349 --> 00:08:23.990
is this so you're not sure what else you
use use this one and maybe you know feel

00:08:23.990 --> 00:08:24.000
use use this one and maybe you know feel
 

00:08:24.000 --> 00:08:27.730
use use this one and maybe you know feel
free also to try to leak your value

00:08:27.730 --> 00:08:27.740
free also to try to leak your value
 

00:08:27.740 --> 00:08:34.610
free also to try to leak your value
where um might be 0.01 G comma Z right

00:08:34.610 --> 00:08:34.620
where um might be 0.01 G comma Z right
 

00:08:34.620 --> 00:08:36.900
where um might be 0.01 G comma Z right
so a is the max of

00:08:36.900 --> 00:08:36.910
so a is the max of
 

00:08:36.910 --> 00:08:40.380
so a is the max of
0.01 times Z and Z so that gives you

00:08:40.380 --> 00:08:40.390
0.01 times Z and Z so that gives you
 

00:08:40.390 --> 00:08:43.950
0.01 times Z and Z so that gives you
this some dent in the function you might

00:08:43.950 --> 00:08:43.960
this some dent in the function you might
 

00:08:43.960 --> 00:08:47.630
this some dent in the function you might
say you know why is that constant 0.01

00:08:47.630 --> 00:08:47.640
say you know why is that constant 0.01
 

00:08:47.640 --> 00:08:51.600
say you know why is that constant 0.01
well you can also make that another

00:08:51.600 --> 00:08:51.610
well you can also make that another
 

00:08:51.610 --> 00:08:53.250
well you can also make that another
parameter of the learning algorithm and

00:08:53.250 --> 00:08:53.260
parameter of the learning algorithm and
 

00:08:53.260 --> 00:08:54.660
parameter of the learning algorithm and
some people say that works even better

00:08:54.660 --> 00:08:54.670
some people say that works even better
 

00:08:54.670 --> 00:08:58.440
some people say that works even better
but I hardly see people do that so but

00:08:58.440 --> 00:08:58.450
but I hardly see people do that so but
 

00:08:58.450 --> 00:08:59.640
but I hardly see people do that so but
if you feel like trying it in your

00:08:59.640 --> 00:08:59.650
if you feel like trying it in your
 

00:08:59.650 --> 00:09:01.350
if you feel like trying it in your
application you know please feel free to

00:09:01.350 --> 00:09:01.360
application you know please feel free to
 

00:09:01.360 --> 00:09:03.420
application you know please feel free to
do so and and you can just see how it

00:09:03.420 --> 00:09:03.430
do so and and you can just see how it
 

00:09:03.430 --> 00:09:05.790
do so and and you can just see how it
works and how long works and stick with

00:09:05.790 --> 00:09:05.800
works and how long works and stick with
 

00:09:05.800 --> 00:09:08.070
works and how long works and stick with
it if it gives you a good result so I

00:09:08.070 --> 00:09:08.080
it if it gives you a good result so I
 

00:09:08.080 --> 00:09:09.660
it if it gives you a good result so I
hope that gives you a sense of some of

00:09:09.660 --> 00:09:09.670
hope that gives you a sense of some of
 

00:09:09.670 --> 00:09:11.520
hope that gives you a sense of some of
the choices of activation functions you

00:09:11.520 --> 00:09:11.530
the choices of activation functions you
 

00:09:11.530 --> 00:09:13.590
the choices of activation functions you
can use in your neural network one of

00:09:13.590 --> 00:09:13.600
can use in your neural network one of
 

00:09:13.600 --> 00:09:15.300
can use in your neural network one of
the themes we'll see in deep learning is

00:09:15.300 --> 00:09:15.310
the themes we'll see in deep learning is
 

00:09:15.310 --> 00:09:17.280
the themes we'll see in deep learning is
that you often have a lot of different

00:09:17.280 --> 00:09:17.290
that you often have a lot of different
 

00:09:17.290 --> 00:09:19.260
that you often have a lot of different
choices in how you build your neural

00:09:19.260 --> 00:09:19.270
choices in how you build your neural
 

00:09:19.270 --> 00:09:21.120
choices in how you build your neural
network ranging from number of hidden

00:09:21.120 --> 00:09:21.130
network ranging from number of hidden
 

00:09:21.130 --> 00:09:23.370
network ranging from number of hidden
units to the chosen activation function

00:09:23.370 --> 00:09:23.380
units to the chosen activation function
 

00:09:23.380 --> 00:09:25.530
units to the chosen activation function
to how you neutralize the waves which

00:09:25.530 --> 00:09:25.540
to how you neutralize the waves which
 

00:09:25.540 --> 00:09:27.600
to how you neutralize the waves which
we'll see later a lot of choices like

00:09:27.600 --> 00:09:27.610
we'll see later a lot of choices like
 

00:09:27.610 --> 00:09:30.180
we'll see later a lot of choices like
that and it turns out that is sometimes

00:09:30.180 --> 00:09:30.190
that and it turns out that is sometimes
 

00:09:30.190 --> 00:09:32.310
that and it turns out that is sometimes
difficult to get good guidelines for

00:09:32.310 --> 00:09:32.320
difficult to get good guidelines for
 

00:09:32.320 --> 00:09:34.260
difficult to get good guidelines for
exactly what will work best for your

00:09:34.260 --> 00:09:34.270
exactly what will work best for your
 

00:09:34.270 --> 00:09:36.600
exactly what will work best for your
problem so throw these three courses out

00:09:36.600 --> 00:09:36.610
problem so throw these three courses out
 

00:09:36.610 --> 00:09:38.850
problem so throw these three courses out
keep on giving you a sense of what I see

00:09:38.850 --> 00:09:38.860
keep on giving you a sense of what I see
 

00:09:38.860 --> 00:09:40.470
keep on giving you a sense of what I see
in the industry in terms of what's more

00:09:40.470 --> 00:09:40.480
in the industry in terms of what's more
 

00:09:40.480 --> 00:09:43.080
in the industry in terms of what's more
or less popular but for your application

00:09:43.080 --> 00:09:43.090
or less popular but for your application
 

00:09:43.090 --> 00:09:45.000
or less popular but for your application
with your applications idiosyncrasies

00:09:45.000 --> 00:09:45.010
with your applications idiosyncrasies
 

00:09:45.010 --> 00:09:46.920
with your applications idiosyncrasies
it's actually very difficult to know in

00:09:46.920 --> 00:09:46.930
it's actually very difficult to know in
 

00:09:46.930 --> 00:09:49.470
it's actually very difficult to know in
advance exactly what will work best so a

00:09:49.470 --> 00:09:49.480
advance exactly what will work best so a
 

00:09:49.480 --> 00:09:51.120
advance exactly what will work best so a
common piece of variance would be if

00:09:51.120 --> 00:09:51.130
common piece of variance would be if
 

00:09:51.130 --> 00:09:52.380
common piece of variance would be if
you're not sure which one of these

00:09:52.380 --> 00:09:52.390
you're not sure which one of these
 

00:09:52.390 --> 00:09:54.330
you're not sure which one of these
activation functions work press you know

00:09:54.330 --> 00:09:54.340
activation functions work press you know
 

00:09:54.340 --> 00:09:56.790
activation functions work press you know
try them all and then evaluate on like a

00:09:56.790 --> 00:09:56.800
try them all and then evaluate on like a
 

00:09:56.800 --> 00:09:59.190
try them all and then evaluate on like a
hotel validation set or like a

00:09:59.190 --> 00:09:59.200
hotel validation set or like a
 

00:09:59.200 --> 00:10:01.170
hotel validation set or like a
development set which we'll talk about

00:10:01.170 --> 00:10:01.180
development set which we'll talk about
 

00:10:01.180 --> 00:10:04.050
development set which we'll talk about
later and see which one works better and

00:10:04.050 --> 00:10:04.060
later and see which one works better and
 

00:10:04.060 --> 00:10:07.170
later and see which one works better and
then go of that and I think that by

00:10:07.170 --> 00:10:07.180
then go of that and I think that by
 

00:10:07.180 --> 00:10:09.480
then go of that and I think that by
testing these different choices for your

00:10:09.480 --> 00:10:09.490
testing these different choices for your
 

00:10:09.490 --> 00:10:12.480
testing these different choices for your
application you'd be better at future

00:10:12.480 --> 00:10:12.490
application you'd be better at future
 

00:10:12.490 --> 00:10:14.040
application you'd be better at future
proofing your neural network

00:10:14.040 --> 00:10:14.050
proofing your neural network
 

00:10:14.050 --> 00:10:16.950
proofing your neural network
architecture on against the same

00:10:16.950 --> 00:10:16.960
architecture on against the same
 

00:10:16.960 --> 00:10:18.930
architecture on against the same
procedure problem as well evolutions of

00:10:18.930 --> 00:10:18.940
procedure problem as well evolutions of
 

00:10:18.940 --> 00:10:22.320
procedure problem as well evolutions of
the algorithms rather than you know if I

00:10:22.320 --> 00:10:22.330
the algorithms rather than you know if I
 

00:10:22.330 --> 00:10:24.000
the algorithms rather than you know if I
were to tell you always use a random

00:10:24.000 --> 00:10:24.010
were to tell you always use a random
 

00:10:24.010 --> 00:10:26.190
were to tell you always use a random
activation and don't use anything else

00:10:26.190 --> 00:10:26.200
activation and don't use anything else
 

00:10:26.200 --> 00:10:28.050
activation and don't use anything else
that that just may or may not apply for

00:10:28.050 --> 00:10:28.060
that that just may or may not apply for
 

00:10:28.060 --> 00:10:30.060
that that just may or may not apply for
whatever problem you end up working on

00:10:30.060 --> 00:10:30.070
whatever problem you end up working on
 

00:10:30.070 --> 00:10:30.780
whatever problem you end up working on
you know either

00:10:30.780 --> 00:10:30.790
you know either
 

00:10:30.790 --> 00:10:32.400
you know either
either in the near future on the distant

00:10:32.400 --> 00:10:32.410
either in the near future on the distant
 

00:10:32.410 --> 00:10:36.210
either in the near future on the distant
future all right so that was a choice of

00:10:36.210 --> 00:10:36.220
future all right so that was a choice of
 

00:10:36.220 --> 00:10:38.040
future all right so that was a choice of
activation functions you see the most

00:10:38.040 --> 00:10:38.050
activation functions you see the most
 

00:10:38.050 --> 00:10:40.350
activation functions you see the most
popular activation functions there's one

00:10:40.350 --> 00:10:40.360
popular activation functions there's one
 

00:10:40.360 --> 00:10:42.930
popular activation functions there's one
other question that sometimes is asked

00:10:42.930 --> 00:10:42.940
other question that sometimes is asked
 

00:10:42.940 --> 00:10:45.150
other question that sometimes is asked
which is why do you even need to use an

00:10:45.150 --> 00:10:45.160
which is why do you even need to use an
 

00:10:45.160 --> 00:10:46.950
which is why do you even need to use an
activation function at all why not just

00:10:46.950 --> 00:10:46.960
activation function at all why not just
 

00:10:46.960 --> 00:10:49.110
activation function at all why not just
do away with that so let's talk about

00:10:49.110 --> 00:10:49.120
do away with that so let's talk about
 

00:10:49.120 --> 00:10:49.770
do away with that so let's talk about
that

00:10:49.770 --> 00:10:49.780
that
 

00:10:49.780 --> 00:10:50.400
that
in the neck

00:10:50.400 --> 00:10:50.410
in the neck
 

00:10:50.410 --> 00:10:53.220
in the neck
video and what you see why new network

00:10:53.220 --> 00:10:53.230
video and what you see why new network
 

00:10:53.230 --> 00:10:55.019
video and what you see why new network
do need some sort of nonlinear

00:10:55.019 --> 00:10:55.029
do need some sort of nonlinear
 

00:10:55.029 --> 00:10:58.259
do need some sort of nonlinear
activation function

