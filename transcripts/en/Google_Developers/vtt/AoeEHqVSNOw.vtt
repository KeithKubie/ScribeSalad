WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:06.030
[MUSIC PLAYING]

00:00:06.030 --> 00:00:06.710
Hey, everyone.

00:00:06.710 --> 00:00:07.910
Welcome back.

00:00:07.910 --> 00:00:10.220
In this episode, we're going
to do something special,

00:00:10.220 --> 00:00:13.164
and that's write our own
classifier from scratch.

00:00:13.164 --> 00:00:14.580
If you're new to
machine learning,

00:00:14.580 --> 00:00:16.170
this is a big milestone.

00:00:16.170 --> 00:00:18.560
Because if you can follow
along and do this on your own,

00:00:18.560 --> 00:00:21.890
it means you understand an
important piece of the puzzle.

00:00:21.890 --> 00:00:23.670
The classifier we're
going to write today

00:00:23.670 --> 00:00:26.390
is a scrappy version
of k-Nearest Neighbors.

00:00:26.390 --> 00:00:29.660
That's one of the simplest
classifiers around.

00:00:29.660 --> 00:00:32.860
First, here's a quick outline of
what we'll do in this episode.

00:00:32.860 --> 00:00:35.170
We'll start with our code
from Episode 4, Let's

00:00:35.170 --> 00:00:36.570
Write a Pipeline.

00:00:36.570 --> 00:00:39.290
Recall in that episode we
did a simple experiment.

00:00:39.290 --> 00:00:42.720
We imported a data set and
split it into train and test.

00:00:42.720 --> 00:00:44.580
We used train to
train a classifier,

00:00:44.580 --> 00:00:47.150
and test to see how
accurate it was.

00:00:47.150 --> 00:00:48.760
Writing the
classifier is the part

00:00:48.760 --> 00:00:50.940
we're going to focus on today.

00:00:50.940 --> 00:00:52.740
Previously we imported
the classifier

00:00:52.740 --> 00:00:55.280
from a library using
these two lines.

00:00:55.280 --> 00:00:58.190
Here we'll comment them
out and write our own.

00:00:58.190 --> 00:01:01.539
The rest of the pipeline
will stay exactly the same.

00:01:01.539 --> 00:01:03.830
I'll pop in and out of the
screencast to explain things

00:01:03.830 --> 00:01:05.940
as we go along.

00:01:05.940 --> 00:01:08.530
To start, let's run our
pipeline to remind ourselves

00:01:08.530 --> 00:01:10.120
what the accuracy was.

00:01:10.120 --> 00:01:12.370
As you can see, it's over 90%.

00:01:12.370 --> 00:01:14.260
And that's the goal
for the classifier

00:01:14.260 --> 00:01:15.660
we'll write ourselves.

00:01:15.660 --> 00:01:17.680
Now let's comment
out that import.

00:01:17.680 --> 00:01:19.540
Right off the bat,
this breaks our code.

00:01:19.540 --> 00:01:22.250
So the first thing we need
to do is fix our pipeline.

00:01:22.250 --> 00:01:24.850
And to do that, we'll implement
a class for our classifier.

00:01:27.690 --> 00:01:29.580
I'll call it ScrappyKNN.

00:01:29.580 --> 00:01:31.690
And by scrappy, I
mean bare bones.

00:01:31.690 --> 00:01:33.710
Just enough to get it working.

00:01:33.710 --> 00:01:38.110
Next, I'll change our
pipeline to use it.

00:01:38.110 --> 00:01:40.880
Now let's see what methods
we need to implement.

00:01:40.880 --> 00:01:42.950
Looking at the interface
for a classifier,

00:01:42.950 --> 00:01:45.500
we see there are two
we care about-- fit,

00:01:45.500 --> 00:01:47.470
which does the
training, and predict,

00:01:47.470 --> 00:01:49.270
which does the prediction.

00:01:49.270 --> 00:01:51.600
First we'll declare
our fit method.

00:01:51.600 --> 00:01:54.440
Remember this takes the features
and labels for the training set

00:01:54.440 --> 00:01:57.680
as input, so we'll add
parameters for those.

00:01:57.680 --> 00:02:00.090
Now let's move on to
our predict method.

00:02:00.090 --> 00:02:03.470
As input, this receives the
features for our testing data.

00:02:03.470 --> 00:02:06.920
And as output, it returns
predictions for the labels.

00:02:06.920 --> 00:02:09.389
Our first goal is to get
the pipeline working,

00:02:09.389 --> 00:02:12.230
and to understand
what these methods do.

00:02:12.230 --> 00:02:14.180
So before we write
our real classifier,

00:02:14.180 --> 00:02:15.810
we'll start with
something simpler.

00:02:15.810 --> 00:02:18.240
We'll write a random classifier.

00:02:18.240 --> 00:02:21.690
And by random, I mean
we'll just guess the label.

00:02:21.690 --> 00:02:25.310
To start, we'll add some code
to the fit and predict methods.

00:02:25.310 --> 00:02:28.460
In fit, I'll store the
training data in this class.

00:02:28.460 --> 00:02:30.480
You can think of this
as just memorizing it.

00:02:30.480 --> 00:02:32.840
And you'll see why
we do that later on.

00:02:32.840 --> 00:02:34.670
Inside the predict
method, remember

00:02:34.670 --> 00:02:37.410
that we'll need to return
a list of predictions.

00:02:37.410 --> 00:02:40.090
That's because the parameter,
X_test, is actually

00:02:40.090 --> 00:02:42.730
a 2D array, or list of lists.

00:02:42.730 --> 00:02:46.410
Each row contains the features
for one testing example.

00:02:46.410 --> 00:02:48.170
To make a prediction
for each row,

00:02:48.170 --> 00:02:50.830
I'll just randomly pick a
label from the training data

00:02:50.830 --> 00:02:53.340
and append that to
our predictions.

00:02:53.340 --> 00:02:55.660
At this point, our
pipeline is working again.

00:02:55.660 --> 00:02:58.029
So let's run it and
see how well it does.

00:02:58.029 --> 00:03:00.070
Recall there are three
different types of flowers

00:03:00.070 --> 00:03:05.070
in the iris dataset, so
accuracy should be about 33%.

00:03:05.070 --> 00:03:07.110
Now we know the interface
for a classifier.

00:03:07.110 --> 00:03:09.060
But when we started
this exercise,

00:03:09.060 --> 00:03:11.770
our accuracy was above 90%.

00:03:11.770 --> 00:03:14.500
So let's see if
we can do better.

00:03:14.500 --> 00:03:16.850
To do that, we'll
implement our classifier,

00:03:16.850 --> 00:03:19.380
which is based on
k-Nearest Neighbors.

00:03:19.380 --> 00:03:22.469
Here's the intuition for
how that algorithm works.

00:03:22.469 --> 00:03:24.760
Let's return to our drawings
of green dots and red dots

00:03:24.760 --> 00:03:26.400
from the last episode.

00:03:26.400 --> 00:03:27.990
Imagine the dots we
see on the screen

00:03:27.990 --> 00:03:30.880
are the training data we
memorized in the fit method,

00:03:30.880 --> 00:03:33.420
say for a toy dataset.

00:03:33.420 --> 00:03:36.220
Now imagine we're asked to make
a prediction for this testing

00:03:36.220 --> 00:03:38.260
point that I'll
draw here in gray.

00:03:38.260 --> 00:03:39.960
How can we do that?

00:03:39.960 --> 00:03:41.890
Well in a nearest
neighbor classifier,

00:03:41.890 --> 00:03:44.220
it works exactly like it sounds.

00:03:44.220 --> 00:03:45.940
We'll find the
training point that's

00:03:45.940 --> 00:03:48.070
closest to the testing point.

00:03:48.070 --> 00:03:50.392
This point is the
nearest neighbor.

00:03:50.392 --> 00:03:51.850
Then we'll predict
that the testing

00:03:51.850 --> 00:03:54.170
point has the same label.

00:03:54.170 --> 00:03:56.880
For example, we'll guess that
this testing dot is green,

00:03:56.880 --> 00:03:59.880
because that's the color
of its nearest neighbor.

00:03:59.880 --> 00:04:02.430
As another example, if we
had a testing dot over here,

00:04:02.430 --> 00:04:04.170
we'd guess that it's red.

00:04:04.170 --> 00:04:06.400
Now what about this one
right in the middle?

00:04:06.400 --> 00:04:08.730
Imagine that this dot is
equidistant to the nearest

00:04:08.730 --> 00:04:10.750
green dot and the
nearest red one.

00:04:10.750 --> 00:04:13.570
There's a tie, so how
could we classify it?

00:04:13.570 --> 00:04:15.960
Well one way is we could
randomly break the tie.

00:04:15.960 --> 00:04:18.970
But there's another way,
and that's where k comes in.

00:04:18.970 --> 00:04:20.680
K is the number of
neighbors we consider

00:04:20.680 --> 00:04:22.340
when making our prediction.

00:04:22.340 --> 00:04:25.530
If k was 1, we'd just look at
the closest training point.

00:04:25.530 --> 00:04:28.170
But if k was 3, we'd look
at the three closest.

00:04:28.170 --> 00:04:30.860
In this case, two of those
are green and one is red.

00:04:30.860 --> 00:04:34.410
To predict, we could vote and
predict the majority class.

00:04:34.410 --> 00:04:36.230
Now there's more detail
to this algorithm,

00:04:36.230 --> 00:04:38.540
but that's enough
to get us started.

00:04:38.540 --> 00:04:40.200
To code this up,
first we'll need a way

00:04:40.200 --> 00:04:42.700
to find the nearest neighbor.

00:04:42.700 --> 00:04:44.840
And to do that, we'll
measure the straight line

00:04:44.840 --> 00:04:48.950
distance between two points,
just like you do with a ruler.

00:04:48.950 --> 00:04:52.100
There's a formula for that
called the Euclidean Distance,

00:04:52.100 --> 00:04:54.310
and here's what the
formula looks like.

00:04:54.310 --> 00:04:56.590
It measures the distance
between two points,

00:04:56.590 --> 00:04:59.250
and it works a bit like
the Pythagorean Theorem.

00:04:59.250 --> 00:05:02.350
A squared plus B squared
equals C squared.

00:05:02.350 --> 00:05:04.790
You can think of this term
as A, or the difference

00:05:04.790 --> 00:05:06.840
between the first two features.

00:05:06.840 --> 00:05:08.670
Likewise, you can think
of this term as B,

00:05:08.670 --> 00:05:11.170
or the difference between
the second pair of features.

00:05:11.170 --> 00:05:14.740
And the distance we compute is
the length of the hypotenuse.

00:05:14.740 --> 00:05:16.460
Now here's something cool.

00:05:16.460 --> 00:05:17.940
Right now we're
computing distance

00:05:17.940 --> 00:05:20.670
in two-dimensional space,
because we have just two

00:05:20.670 --> 00:05:22.780
features in our toy dataset.

00:05:22.780 --> 00:05:25.970
But what if we had three
features or three dimensions?

00:05:25.970 --> 00:05:28.200
Well then we'd be in a cube.

00:05:28.200 --> 00:05:30.450
We can still visualize
how to measure distance

00:05:30.450 --> 00:05:32.500
in the space with a ruler.

00:05:32.500 --> 00:05:35.270
But what if we had four
features or four dimensions,

00:05:35.270 --> 00:05:36.710
like we do in iris?

00:05:36.710 --> 00:05:38.500
Well, now we're in
a hypercube, and we

00:05:38.500 --> 00:05:40.740
can't visualize this very easy.

00:05:40.740 --> 00:05:42.450
The good news is the
Euclidean Distance

00:05:42.450 --> 00:05:46.010
works the same way regardless
of the number of dimensions.

00:05:46.010 --> 00:05:50.050
With more features, we can just
add more terms to the equation.

00:05:50.050 --> 00:05:52.060
You can find more details
about this online.

00:05:54.670 --> 00:05:56.770
Now let's code up
Euclidean distance.

00:05:56.770 --> 00:05:58.270
There are plenty
of ways to do that,

00:05:58.270 --> 00:06:00.370
but we'll use a library
called scipy that's

00:06:00.370 --> 00:06:02.460
already installed by Anaconda.

00:06:02.460 --> 00:06:05.380
Here, A and B are lists
of numeric features.

00:06:05.380 --> 00:06:07.330
Say A is a point from
our training data,

00:06:07.330 --> 00:06:09.800
and B is a point from
our testing data.

00:06:09.800 --> 00:06:12.860
This function returns the
distance between them.

00:06:12.860 --> 00:06:14.440
That's all the math
we need, so now

00:06:14.440 --> 00:06:17.390
let's take a look at the
algorithm for a classifier.

00:06:17.390 --> 00:06:19.480
To make a prediction
for a test point,

00:06:19.480 --> 00:06:22.250
we'll calculate the distance
to all the training points.

00:06:22.250 --> 00:06:25.030
Then we'll predict the testing
point has the same label

00:06:25.030 --> 00:06:27.140
as the closest one.

00:06:27.140 --> 00:06:28.910
I'll delete the random
prediction we made,

00:06:28.910 --> 00:06:31.610
and replace it with a method
that finds the closest training

00:06:31.610 --> 00:06:33.470
point to the test point.

00:06:33.470 --> 00:06:35.580
For this video hard,
I'll hard-code k to 1,

00:06:35.580 --> 00:06:38.090
so we're writing a nearest
neighbor classifier.

00:06:38.090 --> 00:06:40.100
The k variable won't
appear in our code,

00:06:40.100 --> 00:06:42.950
since we'll always just
find the closest point.

00:06:42.950 --> 00:06:45.700
Inside this method, we'll loop
over all the training points

00:06:45.700 --> 00:06:48.250
and keep track of the
closest one so far.

00:06:48.250 --> 00:06:50.650
Remember that we memorized
the training data in our fit

00:06:50.650 --> 00:06:54.190
function, and X_train
contains the features.

00:06:54.190 --> 00:06:56.910
To start, I'll calculate the
distance from the test point

00:06:56.910 --> 00:06:58.740
to the first training point.

00:06:58.740 --> 00:07:00.990
I'll use this variable to
keep track of the shortest

00:07:00.990 --> 00:07:02.584
distance we've found so far.

00:07:02.584 --> 00:07:04.000
And I'll use this
variable to keep

00:07:04.000 --> 00:07:07.400
track of the index of the
training point that's closest.

00:07:07.400 --> 00:07:09.910
We'll need this later
to retrieve its label.

00:07:09.910 --> 00:07:12.370
Now we'll iterate over all
the other training points.

00:07:12.370 --> 00:07:14.410
And every time we
find a closer one,

00:07:14.410 --> 00:07:16.150
we'll update our variables.

00:07:16.150 --> 00:07:18.100
Finally, we'll use
the index to return

00:07:18.100 --> 00:07:22.110
the label for the
closest training example.

00:07:22.110 --> 00:07:24.780
At this point, we have a working
nearest neighbor classifier,

00:07:24.780 --> 00:07:29.510
so let's run it and see
what the accuracy is.

00:07:29.510 --> 00:07:31.330
As you can see, it's over 90%.

00:07:31.330 --> 00:07:32.250
And we did it.

00:07:32.250 --> 00:07:34.240
When you run this on
your own, the accuracy

00:07:34.240 --> 00:07:36.906
might be a bit different because
of randomness in the train test

00:07:36.906 --> 00:07:38.530
split.

00:07:38.530 --> 00:07:40.740
Now if you can code this
up and understand it,

00:07:40.740 --> 00:07:42.670
that's a big
accomplishment because it

00:07:42.670 --> 00:07:46.254
means you can write a simple
classifier from scratch.

00:07:46.254 --> 00:07:47.920
Now, there are a
number of pros and cons

00:07:47.920 --> 00:07:50.850
to this algorithm, many of
which you can find online.

00:07:50.850 --> 00:07:53.580
The basic pro is that it's
relatively easy to understand,

00:07:53.580 --> 00:07:56.000
and works reasonably
well for some problems.

00:07:56.000 --> 00:07:57.670
And the basic cons
are that it's slow,

00:07:57.670 --> 00:07:59.990
because it has to iterate
over every training point

00:07:59.990 --> 00:08:01.550
to make a prediction.

00:08:01.550 --> 00:08:04.070
And importantly, as
we saw in Episode 3,

00:08:04.070 --> 00:08:06.384
some features are more
informative than others.

00:08:06.384 --> 00:08:08.050
But there's not an
easy way to represent

00:08:08.050 --> 00:08:10.120
that in k-Nearest Neighbors.

00:08:10.120 --> 00:08:12.150
In the long run, we
want a classifier

00:08:12.150 --> 00:08:15.260
that learns more complex
relationships between features

00:08:15.260 --> 00:08:17.290
and the label we're
trying to predict.

00:08:17.290 --> 00:08:19.490
A decision tree is a
good example of that.

00:08:19.490 --> 00:08:22.120
And a neural network like we
saw in TensorFlow Playground

00:08:22.120 --> 00:08:23.730
is even better.

00:08:23.730 --> 00:08:24.940
OK, hope that was helpful.

00:08:24.940 --> 00:08:26.227
Thanks as always for watching.

00:08:26.227 --> 00:08:28.560
You can follow me on Twitter
for updates and, of course,

00:08:28.560 --> 00:08:29.310
Google Developers.

00:08:29.310 --> 00:08:32.190
And I'll see you guys next time.

00:08:32.190 --> 00:08:35.540
[MUSIC PLAYING]

