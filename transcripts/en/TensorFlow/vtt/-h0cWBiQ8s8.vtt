WEBVTT
Kind: captions
Language: en

00:00:00.001 --> 00:00:03.388
♪ (intro music) ♪

00:00:06.792 --> 00:00:07.873
Hello, everyone.

00:00:08.522 --> 00:00:11.212
My name is Igor, I'm going to...

00:00:11.212 --> 00:00:14.338
I work in <i>TensorFlow</i> team
and I'm going to talk to you today

00:00:14.808 --> 00:00:16.387
about distributed TensorFlow.

00:00:17.919 --> 00:00:20.982
Well, why would you care
about distributed TensorFlow?

00:00:21.932 --> 00:00:23.599
Many of you know the answer probably.

00:00:23.599 --> 00:00:25.185
But just in case,

00:00:26.329 --> 00:00:30.686
it's a way for your models
to train faster and be more parallel.

00:00:31.199 --> 00:00:33.410
It's a way for you
to get more things done.

00:00:33.410 --> 00:00:34.564
It's read quicker.

00:00:35.939 --> 00:00:40.058
When you train deep learning models,
it can take a long time.

00:00:40.557 --> 00:00:43.555
And when I say a long time,
I mean weeks.

00:00:45.046 --> 00:00:48.157
With all the available hardware
to you out there,

00:00:48.657 --> 00:00:52.868
scaling up to hundreds of CPUs
or GPUs can really make a difference.

00:00:55.944 --> 00:00:57.186
How could you scale up?

00:00:57.187 --> 00:01:00.520
Well, you could just add a GPU
to your machine.

00:01:00.520 --> 00:01:02.929
In this case, this is just plug and play.

00:01:03.732 --> 00:01:07.581
You insert the GPU, TensorFlow
handles all the details for you,

00:01:08.065 --> 00:01:11.457
and you see a nice bump
in the training speed.

00:01:12.924 --> 00:01:15.130
You could also insert multiple GPUs.

00:01:16.751 --> 00:01:22.442
In this case, you'd have
to write additional code.

00:01:22.901 --> 00:01:24.472
You need to replicate your model,

00:01:24.926 --> 00:01:28.275
you need to combine <i>gradients</i>
from every GPU,

00:01:28.756 --> 00:01:31.026
and if you're using batch norm layer,

00:01:31.026 --> 00:01:35.714
you have the tricky question of what
to do with the statistics on each GPU.

00:01:37.095 --> 00:01:41.785
The point I'm trying to make is
that you need to do additional work

00:01:42.405 --> 00:01:43.412
to make this work,

00:01:43.413 --> 00:01:46.358
and you need to learn stuff
that you didn't plan in learning.

00:01:48.747 --> 00:01:50.454
You can also use multiple machines.

00:01:50.742 --> 00:01:53.290
And this situation is
similar to the one before,

00:01:53.823 --> 00:01:57.918
but in this case, your bottleneck
is probably going to be that communication

00:01:57.919 --> 00:01:59.387
between the machines.

00:02:00.493 --> 00:02:03.700
You'll start thinking
about minimizing that communication,

00:02:03.701 --> 00:02:06.365
and probably doing more work luckily.

00:02:06.365 --> 00:02:08.984
For example, combining the <i>gradients</i>
on the local GPUs

00:02:08.984 --> 00:02:12.252
before exchanging them
with a remote GPUs.

00:02:13.789 --> 00:02:16.829
Unless specialized
networking hardware is used,

00:02:17.433 --> 00:02:23.108
the coordination cost in this setup
are probably going to limit your scaling.

00:02:24.464 --> 00:02:26.987
But there is an approach,
there's a solution to this.

00:02:29.556 --> 00:02:31.223
This approach is called parameter server.

00:02:33.569 --> 00:02:36.363
Some hosts we call them parameter servers,

00:02:36.760 --> 00:02:39.245
they're going to only
hold training weights.

00:02:39.514 --> 00:02:41.260
Other hosts, workers,

00:02:42.118 --> 00:02:45.372
they're going to have
a copy of the TensorFlow graph.

00:02:46.429 --> 00:02:50.008
They're going to get their own input,
compute their own <i>gradient</i>,

00:02:50.564 --> 00:02:53.444
and then just go ahead
and update the training weights

00:02:53.452 --> 00:02:55.992
without any coordination
with other workers.

00:02:56.468 --> 00:02:59.047
So this is an approach
with low coordination

00:02:59.665 --> 00:03:02.110
between a large number of hosts.

00:03:02.832 --> 00:03:03.937
And there you go,

00:03:03.937 --> 00:03:06.880
this scales well
and we've been doing this at Google

00:03:07.414 --> 00:03:08.464
for a long time.

00:03:10.388 --> 00:03:12.071
But there is a wrinkle with this approach.

00:03:12.071 --> 00:03:15.015
You give up synchronicity
and that has benefits.

00:03:17.228 --> 00:03:19.693
And if you think about
the parameter server approach,

00:03:19.693 --> 00:03:23.020
it's an approach from the CPU era

00:03:23.886 --> 00:03:28.703
with all the reliable communication
between GPUs.

00:03:29.828 --> 00:03:33.391
we can consider designs
which have tighter coupling

00:03:33.856 --> 00:03:36.753
and more coordination
between the workers.

00:03:37.764 --> 00:03:41.209
One such approach is
based on <i>all-reduce</i> .

00:03:41.647 --> 00:03:43.346
That's not a new idea.

00:03:44.180 --> 00:03:49.997
The general goal of <i>all-reduce</i>
is to combine all the values

00:03:49.998 --> 00:03:52.356
and distribute results
to all the processes.

00:03:54.886 --> 00:03:57.838
<i>All-reduce</i> is kind of tricky
to explain on the slide,

00:03:58.742 --> 00:04:01.191
but you can think of its results

00:04:01.542 --> 00:04:05.908
as reduce operation
followed by a broadcast operation.

00:04:07.044 --> 00:04:09.957
But don't think of it that way
in terms of performance.

00:04:10.743 --> 00:04:14.283
It's a fused algorithm
and it's way more efficient

00:04:14.284 --> 00:04:17.005
than those two operations together.

00:04:17.934 --> 00:04:19.187
In addition to it,

00:04:21.329 --> 00:04:22.821
hardware vendors...

00:04:24.823 --> 00:04:28.438
hardware vendors often supply specialized
or reduce implementations

00:04:28.869 --> 00:04:33.226
that TensorFlow can secretly use
behind the scenes to help you.

00:04:34.718 --> 00:04:36.337
Alternative approaches,

00:04:36.909 --> 00:04:40.091
they typically send all data
to a central place.

00:04:40.661 --> 00:04:43.001
<i>All-reduce</i> is not going
to have such a bottleneck,

00:04:43.486 --> 00:04:48.129
because it distributes coordination
between GPUs way more evenly.

00:04:49.098 --> 00:04:50.671
With every tick of <i>all-reduce</i>,

00:04:51.064 --> 00:04:54.723
each GPU sends and receives
a part of the final answer.

00:04:56.630 --> 00:05:00.154
So how could <i>all-reduce</i>
help us with our models?

00:05:00.601 --> 00:05:03.467
Well, consider...
let's say you have two GPUs.

00:05:03.680 --> 00:05:07.870
You copied layers
and the variables on average GPU,

00:05:07.871 --> 00:05:10.156
and you perform the <i>forward pass</i>.

00:05:10.600 --> 00:05:11.624
Nice and parallel.

00:05:12.322 --> 00:05:14.338
But then during the <i>backward pass</i>,

00:05:14.777 --> 00:05:16.411
as the <i>gradients</i> become available,

00:05:16.816 --> 00:05:21.034
we can use <i>all-reduce</i>
to combine those <i>gradients</i>

00:05:21.034 --> 00:05:23.573
with their counterparts from other GPUs.

00:05:25.640 --> 00:05:27.434
In addition to that,
because of the way...

00:05:28.053 --> 00:05:30.767
In addition to that <i>gradients</i>
from the outer layers

00:05:31.363 --> 00:05:34.827
are available before the <i>gradients</i>
from the other layers.

00:05:34.827 --> 00:05:41.647
So we could overlap backward propagation,
computation and/or reduce communication.

00:05:42.125 --> 00:05:44.078
That gives you even more steps per second.

00:05:45.640 --> 00:05:51.512
The bottom line is when communication
between GPUs is reliable,

00:05:52.171 --> 00:05:54.854
<i>all-reduce</i> can be fast
and allow you to scale well.

00:05:56.820 --> 00:05:59.749
How could you use <i>all-reduce</i>
in TensorFlow?

00:06:00.467 --> 00:06:04.586
Well, so far in this talk I told you
that to take advantage of multiple GPUs,

00:06:04.594 --> 00:06:06.102
you need to write additional code,

00:06:06.727 --> 00:06:08.275
change your model
and learn stuff.

00:06:13.529 --> 00:06:14.912
Chances are you're using--

00:06:15.180 --> 00:06:18.149
you're following our advice
of using the highest level API

00:06:18.150 --> 00:06:19.583
that works for your <i>use case</i>.

00:06:20.178 --> 00:06:21.750
That probably is e<i>stimator</i>.

00:06:22.242 --> 00:06:26.097
With e<i>stimator</i>, your model is
specified by the model function,

00:06:26.097 --> 00:06:29.962
and it has no knowledge
about GPUs or devices.

00:06:31.018 --> 00:06:34.864
So to have that model use multiple GPUs,

00:06:35.262 --> 00:06:36.666
you just need to add one line.

00:06:37.694 --> 00:06:41.177
You need to pass an <i>instance</i>
of a new <i>class</i> called mirror strategy.

00:06:42.274 --> 00:06:44.611
And mirror strategy

00:06:44.611 --> 00:06:49.076
is one implementation
of our new distribution strategy API.

00:06:50.615 --> 00:06:55.075
Distribution strategy tells
TensorFlow how to replicate your model.

00:06:57.708 --> 00:06:58.835
Oops, sorry!

00:07:01.374 --> 00:07:02.580
Another thing I want to say is

00:07:02.581 --> 00:07:05.026
that mirror strategy
takes a number of--

00:07:05.026 --> 00:07:08.175
could take a number of GPUs
or a list of GPUs to use,

00:07:08.730 --> 00:07:11.071
or you can not give it
any arguments at all

00:07:11.086 --> 00:07:14.126
and then it will just
figure out what GPUs to use.

00:07:17.755 --> 00:07:20.953
Mirror strategy works in the way
exactly as I described before.

00:07:21.842 --> 00:07:25.381
It replicates your model,
it uses <i>all-reduce</i> for communication.

00:07:30.184 --> 00:07:34.152
So <i>gradient</i> updates from every GPUs,

00:07:34.597 --> 00:07:39.527
from all GPUs, they're going to be
combined before updating the weights.

00:07:40.154 --> 00:07:44.511
and each copy of your model on every
GPU is part of a single TensorFlow graph.

00:07:45.263 --> 00:07:48.485
That means this is in-graph replication
with synchronous training

00:07:48.819 --> 00:07:51.906
that uses <i>all-reduce</i> on many GPUs.

00:07:53.279 --> 00:07:57.701
Now the last 10 minutes
are kind of a waste of time for you

00:07:58.545 --> 00:08:00.077
if this doesn't perform well.

00:08:01.068 --> 00:08:02.274
And it does perform well.

00:08:05.266 --> 00:08:08.194
As you add GPUs,
this implementation scales well.

00:08:09.204 --> 00:08:12.005
We have a team at TensorFlow
that specifically works

00:08:12.078 --> 00:08:16.956
on fast implementations of averages
for various machine configurations,

00:08:17.491 --> 00:08:21.541
and this implementation
gets 90% scaling on eight GPUs.

00:08:22.879 --> 00:08:28.799
And again, it didn't require any change
to the user's model.

00:08:32.447 --> 00:08:33.866
It didn't require any change

00:08:33.868 --> 00:08:36.977
because we changed
everything in TensorFlow

00:08:36.979 --> 00:08:38.396
that's not your model.

00:08:38.396 --> 00:08:40.986
Things like <i>optimizer</i>,
<i>batch_norm</i>, <i>summaries</i>.

00:08:41.241 --> 00:08:42.765
Everything that writes state

00:08:42.984 --> 00:08:44.849
now needs to become distribution-aware.

00:08:45.368 --> 00:08:47.622
That means that it has to learn

00:08:47.860 --> 00:08:51.401
how to combine its state with other GPUs.

00:08:52.808 --> 00:08:59.804
And this is important
because alternative API is out there.

00:09:00.084 --> 00:09:03.563
They typically ask you
to rephrase your model

00:09:03.929 --> 00:09:07.349
to supply optimizers,
for example, separately,

00:09:07.716 --> 00:09:12.582
so that they can do all
the state coordination behind the scenes.

00:09:16.589 --> 00:09:18.121
And if you have some experience

00:09:18.122 --> 00:09:21.137
with training your models
on multiple GPUs,

00:09:21.138 --> 00:09:22.348
you might be wondering,

00:09:22.348 --> 00:09:26.224
"Well, can I save my model

00:09:26.224 --> 00:09:29.224
on a computer with eight GPUs

00:09:29.224 --> 00:09:34.725
and then do an evaluation on it
on a computer with, say, no GPUs?"

00:09:35.572 --> 00:09:37.722
Typically this causes a problem,

00:09:37.722 --> 00:09:41.042
but with distribution strategy API,

00:09:41.043 --> 00:09:44.494
we maintain the backward compatibility
on the checkpoint level,

00:09:45.527 --> 00:09:51.449
so mirror strategy it has multiple
copies of the state on every GPU,

00:09:51.876 --> 00:09:54.329
and that state is in sync.

00:09:54.867 --> 00:09:57.296
So mirror strategy is going
to save only one copy,

00:09:57.297 --> 00:10:02.129
and then at the restore time,
it's only going to restore that state

00:10:02.129 --> 00:10:03.793
to required number of GPUs.

00:10:04.471 --> 00:10:07.440
So this <i>use case</i> is supported.

00:10:09.640 --> 00:10:14.259
Distribution strategy works
with <i>Eager</i> mode as well,

00:10:14.769 --> 00:10:17.690
but we are still
fine-tuning the performance,

00:10:18.461 --> 00:10:21.233
and distribution strategy
is a very general API

00:10:21.234 --> 00:10:24.162
that I hope in the future
will support many <i>use case</i>s.

00:10:25.388 --> 00:10:27.254
it's not tied to e<i>stimator</i>,

00:10:27.284 --> 00:10:34.055
and we are working in two ways
of creating even more better APIs

00:10:34.064 --> 00:10:35.572
based on distribution strategy.

00:10:38.535 --> 00:10:39.932
We in the future...

00:10:39.932 --> 00:10:41.393
soon, pretty soon...

00:10:41.394 --> 00:10:47.856
we intend to support many kinds
of distributed training.

00:10:47.856 --> 00:10:52.112
<i>Synchronous</i>, <i>Asynchronous
multi-node</i>, <i>model parallelism</i>,

00:10:52.133 --> 00:10:56.109
all of that is going to be supported based
as part of distribution strategy API.

00:10:56.490 --> 00:10:59.117
But until then, for multi-node solution,

00:10:59.117 --> 00:11:02.920
please continue using
<i>estimator.train_and_evaluate</i>,

00:11:03.431 --> 00:11:04.677
or try <i>Horovod!</i>*

00:11:04.939 --> 00:11:09.074
That's an outstanding community project
that also offers a <i>multi-node</i> solution.

00:11:14.452 --> 00:11:17.387
Mirror strategies are available
for you in our <i>nightly build</i>.

00:11:17.848 --> 00:11:20.960
And we are very, very
actively working on it,

00:11:21.514 --> 00:11:26.330
And it's a product of work of many people.

00:11:27.145 --> 00:11:30.511
And I would really
encourage you to try it out,

00:11:30.511 --> 00:11:32.606
and let us know
what do you think about it

00:11:33.414 --> 00:11:36.826
via <i>Github</i>
or talk to us after my talk.

00:11:38.072 --> 00:11:40.747
Alright, thank you,
thanks for your attention.

00:11:40.946 --> 00:11:44.287
(applause)

00:11:45.388 --> 00:11:48.396
♪ (outro music) ♪

