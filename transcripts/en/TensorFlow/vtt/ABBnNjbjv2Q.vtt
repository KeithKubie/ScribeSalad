WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.451
[MUSIC PLAYING]

00:00:08.270 --> 00:00:09.520
MAYA GUPTA: Hey, good morning.

00:00:09.520 --> 00:00:11.450
I'm Maya from Google AI.

00:00:11.450 --> 00:00:13.820
And I'm excited to share
with you about TF Lattice.

00:00:13.820 --> 00:00:15.590
This is the same
technology that we

00:00:15.590 --> 00:00:17.648
use at Google for
many, dozens really,

00:00:17.648 --> 00:00:19.940
of our production models,
where we care about the model

00:00:19.940 --> 00:00:21.315
behavior and we
need to make sure

00:00:21.315 --> 00:00:23.990
that we can guarantee it's
working sensibly everywhere.

00:00:23.990 --> 00:00:25.407
In particular,
we're going to show

00:00:25.407 --> 00:00:27.650
how you can control your
models to have monotonicity.

00:00:27.650 --> 00:00:29.612
So let me just dive
into an example.

00:00:29.612 --> 00:00:31.070
An example I'm
going to use here is

00:00:31.070 --> 00:00:33.260
a benchmark dataset
from UCI, so you

00:00:33.260 --> 00:00:35.000
could play with this at home.

00:00:35.000 --> 00:00:37.642
And we have plenty of data
here, 24,000 training examples.

00:00:37.642 --> 00:00:39.350
And there's more
features in the dataset.

00:00:39.350 --> 00:00:40.640
We're just going to
look at two, so that we

00:00:40.640 --> 00:00:42.320
can visualize what's happening.

00:00:42.320 --> 00:00:44.790
And what I'm showing here
is the actual training data.

00:00:44.790 --> 00:00:47.570
So the two features are, are
you single or are you married?

00:00:47.570 --> 00:00:50.330
And how many months has it
been since you paid your bills?

00:00:50.330 --> 00:00:51.440
We're trying to
solve the regression

00:00:51.440 --> 00:00:52.898
problem of predicting
are you going

00:00:52.898 --> 00:00:54.630
to default on your credit.

00:00:54.630 --> 00:00:57.315
So what you can see is that
while there's 24,000 samples,

00:00:57.315 --> 00:00:59.690
there's not that many people
who haven't paid their bills

00:00:59.690 --> 00:01:00.870
for six months or more.

00:01:00.870 --> 00:01:02.360
So we're starting to get
a bit sparse in that part

00:01:02.360 --> 00:01:03.440
of the region space.

00:01:03.440 --> 00:01:05.010
And what you're seeing
here is the training data,

00:01:05.010 --> 00:01:07.700
in the mean and the confidence
interval of the actual training

00:01:07.700 --> 00:01:08.370
data.

00:01:08.370 --> 00:01:11.320
So what happens when we
go and train a model?

00:01:11.320 --> 00:01:13.270
Sort of visualize
what's going to happen.

00:01:13.270 --> 00:01:14.853
Is a going to turn
out to be important

00:01:14.853 --> 00:01:16.055
if you're single or married?

00:01:16.055 --> 00:01:16.555
Not really.

00:01:16.555 --> 00:01:17.685
It doesn't matter if
you're single or married,

00:01:17.685 --> 00:01:20.320
it really matters how long it's
been since you paid your bills.

00:01:20.320 --> 00:01:22.090
But the big surprise
here is the model

00:01:22.090 --> 00:01:24.680
believes that it's
better if you've not

00:01:24.680 --> 00:01:26.680
paid your bills for seven
months than six months

00:01:26.680 --> 00:01:28.850
that you're going to get
a higher credit score.

00:01:28.850 --> 00:01:30.700
So this is sad.

00:01:30.700 --> 00:01:32.350
This is bad AI behavior.

00:01:32.350 --> 00:01:35.382
And this is one of the reasons
that some people don't like AI.

00:01:35.382 --> 00:01:36.840
And here we're just
looking at this

00:01:36.840 --> 00:01:38.320
in a two-dimensional space.

00:01:38.320 --> 00:01:40.630
If we had the full 24
dimensional features

00:01:40.630 --> 00:01:42.130
or 30 or 100 features,
there's going

00:01:42.130 --> 00:01:44.255
to be pockets of the space
where this sort of thing

00:01:44.255 --> 00:01:45.910
may be happening
and we may not even

00:01:45.910 --> 00:01:49.030
realize that we're getting this
sort of bad, strange, possibly

00:01:49.030 --> 00:01:51.160
unethical behavior
from our model.

00:01:51.160 --> 00:01:52.283
So what can we do?

00:01:52.283 --> 00:01:53.950
Well, we say, well,
this is overfitting.

00:01:53.950 --> 00:01:55.290
Let's just regularize.

00:01:55.290 --> 00:01:57.743
But any of your standard
regularization techniques

00:01:57.743 --> 00:01:58.910
are going to be problematic.

00:01:58.910 --> 00:02:00.820
It's going to be hard to
really fix this without hurting

00:02:00.820 --> 00:02:01.775
your model accuracy.

00:02:01.775 --> 00:02:03.400
And it's going to be
hard to even check

00:02:03.400 --> 00:02:04.930
if you really fixed it.

00:02:04.930 --> 00:02:06.910
So what the TF
Lattice package does

00:02:06.910 --> 00:02:09.160
is it hits this
problem exactly, and it

00:02:09.160 --> 00:02:11.710
lets you have monotonicity
as a regularizer.

00:02:11.710 --> 00:02:13.840
You can say when you put
together your features,

00:02:13.840 --> 00:02:15.840
for this feature it
should only hurt my model,

00:02:15.840 --> 00:02:17.980
it should only hurt my
score if I haven't paid

00:02:17.980 --> 00:02:20.540
my bills for a longer time.

00:02:20.540 --> 00:02:22.750
And so you can see here,
this fixes that trend

00:02:22.750 --> 00:02:24.610
in the high number of months.

00:02:24.610 --> 00:02:26.233
And we get the same
flexibility model.

00:02:26.233 --> 00:02:28.150
We actually get slightly
better test accuracy,

00:02:28.150 --> 00:02:30.540
but we've solved the exact
problem that we need to solve

00:02:30.540 --> 00:02:32.863
and we now have a guarantee
on what the model is doing.

00:02:32.863 --> 00:02:34.780
And if we had 100 features,
it would similarly

00:02:34.780 --> 00:02:36.310
guarantee that in
all those pockets

00:02:36.310 --> 00:02:40.270
of 100 dimensional space,
this was working correctly.

00:02:40.270 --> 00:02:43.410
So how does TF
Lattice package do it?

00:02:43.410 --> 00:02:46.750
Well, under the hood, the kind
of function class it's using

00:02:46.750 --> 00:02:50.280
are lattices, and these are
just interpolated lookup tables.

00:02:50.280 --> 00:02:52.410
This is possibly the
oldest way humanity

00:02:52.410 --> 00:02:54.402
has for representing
function, right?

00:02:54.402 --> 00:02:56.610
You've seen these in the
back of your math textbooks.

00:02:56.610 --> 00:03:01.290
You can find them in tables from
actuary in 1960s, et cetera.

00:03:01.290 --> 00:03:03.630
So in a one-dimensional
space, these are simply

00:03:03.630 --> 00:03:05.580
piecewise linear functions.

00:03:05.580 --> 00:03:07.590
But with the TF
Lattice package, you

00:03:07.590 --> 00:03:10.830
can represent high
dimensional functions also

00:03:10.830 --> 00:03:12.898
with these
multidimensional lattices,

00:03:12.898 --> 00:03:14.190
multidimensional lookup tables.

00:03:14.190 --> 00:03:16.110
Here's an example with
just two features.

00:03:16.110 --> 00:03:18.510
We're building a function
with 10 lookup table parameter

00:03:18.510 --> 00:03:19.680
values there.

00:03:19.680 --> 00:03:21.300
And the lookup table
parameter values

00:03:21.300 --> 00:03:23.580
are trained using empirical
risk minimization.

00:03:23.580 --> 00:03:26.145
It's all the same training
that you would see with DNN.

00:03:26.145 --> 00:03:28.020
It's just the parameters
now represent what's

00:03:28.020 --> 00:03:29.340
happening with our function.

00:03:29.340 --> 00:03:31.697
And so it's much easier to
control from monotonicity

00:03:31.697 --> 00:03:33.780
because there's a lot of
structure to the function

00:03:33.780 --> 00:03:36.330
space.

00:03:36.330 --> 00:03:37.980
And with the TF
Lattice package, you

00:03:37.980 --> 00:03:41.317
can kind of check and choose
how much flexibility you want.

00:03:41.317 --> 00:03:43.650
So on the one extreme, you
can just make a linear model.

00:03:43.650 --> 00:03:45.500
Very easy to make a
linear model monotonic.

00:03:45.500 --> 00:03:47.250
You can generalized
additive models, where

00:03:47.250 --> 00:03:48.662
you're using those 1D lattices.

00:03:48.662 --> 00:03:50.370
You can do these
multidimensional lattice

00:03:50.370 --> 00:03:50.870
models.

00:03:50.870 --> 00:03:52.287
If you have a lot
of features, you

00:03:52.287 --> 00:03:53.850
may want an ensemble
of lattices.

00:03:53.850 --> 00:03:55.502
And we've set this
up with layers,

00:03:55.502 --> 00:03:57.960
so you can mix and match and
plug the players with other TF

00:03:57.960 --> 00:04:00.002
layers and create sort of
cascade of deep lattice

00:04:00.002 --> 00:04:01.707
networks.

00:04:01.707 --> 00:04:03.790
Everything in the package
it gives you smoothness.

00:04:03.790 --> 00:04:05.283
So compared to
decision trees, you

00:04:05.283 --> 00:04:07.450
won't have this sort of
piecewise constant behavior.

00:04:07.450 --> 00:04:09.400
You get smooth models
and the monoticity

00:04:09.400 --> 00:04:13.090
guarantees that you select.

00:04:13.090 --> 00:04:16.269
And here's an example of
five-layer deep lattice

00:04:16.269 --> 00:04:19.269
network where these squares
are these 1D calibraters.

00:04:19.269 --> 00:04:23.080
And with the launch of
TF 2.0, TF Lattice 2.0

00:04:23.080 --> 00:04:25.000
will also be coming
out in a month or two,

00:04:25.000 --> 00:04:29.260
and we'll support
Keras layers as well.

00:04:29.260 --> 00:04:32.390
OK, so there's a GitHub
link that you can get to.

00:04:32.390 --> 00:04:34.610
And there's a nice
set of tutorials

00:04:34.610 --> 00:04:36.950
that walks through sort of
all the complexity of what

00:04:36.950 --> 00:04:37.850
you might want to do.

00:04:37.850 --> 00:04:39.725
Also shows you how to
like layer these things

00:04:39.725 --> 00:04:42.228
on top of DNNs for
later layers, et cetera.

00:04:42.228 --> 00:04:43.770
The tutorials are
sort of standalone.

00:04:43.770 --> 00:04:45.110
You can just work with
them and figure out

00:04:45.110 --> 00:04:46.027
how to use this stuff.

00:04:46.027 --> 00:04:47.990
If you want to dig into
the technical details,

00:04:47.990 --> 00:04:49.280
we have a series of papers.

00:04:49.280 --> 00:04:51.662
I'll just point you to
this most recent paper

00:04:51.662 --> 00:04:53.120
and you can track
back through some

00:04:53.120 --> 00:04:54.288
of the literature from that.

00:04:54.288 --> 00:04:55.580
All right, thank you very much.

00:04:55.580 --> 00:04:56.480
[APPLAUSE]

00:04:56.480 --> 00:04:59.530
[MUSIC PLAYING]

