WEBVTT
Kind: captions
Language: en

00:00:00.120 --> 00:00:02.480 align:start position:0%
 
in<00:00:00.510><c> this</c><00:00:01.140><c> video</c><00:00:01.410><c> we'll</c><00:00:01.680><c> finally</c><00:00:02.190><c> see</c><00:00:02.340><c> how</c><00:00:02.429><c> we</c>

00:00:02.480 --> 00:00:02.490 align:start position:0%
in this video we'll finally see how we
 

00:00:02.490 --> 00:00:04.820 align:start position:0%
in this video we'll finally see how we
can<00:00:02.730><c> train</c><00:00:03.060><c> a</c><00:00:03.330><c> conditional</c><00:00:03.929><c> random</c><00:00:04.080><c> field</c><00:00:04.500><c> and</c>

00:00:04.820 --> 00:00:04.830 align:start position:0%
can train a conditional random field and
 

00:00:04.830 --> 00:00:06.980 align:start position:0%
can train a conditional random field and
so<00:00:04.980><c> will</c><00:00:05.130><c> first</c><00:00:05.400><c> discuss</c><00:00:05.850><c> the</c><00:00:06.150><c> last</c><00:00:06.480><c> function</c>

00:00:06.980 --> 00:00:06.990 align:start position:0%
so will first discuss the last function
 

00:00:06.990 --> 00:00:10.129 align:start position:0%
so will first discuss the last function
that<00:00:07.109><c> we'll</c><00:00:07.379><c> use</c><00:00:07.410><c> to</c><00:00:08.090><c> train</c><00:00:09.090><c> and</c><00:00:09.480><c> learn</c><00:00:09.510><c> our</c>

00:00:10.129 --> 00:00:10.139 align:start position:0%
that we'll use to train and learn our
 

00:00:10.139 --> 00:00:14.480 align:start position:0%
that we'll use to train and learn our
conditional<00:00:10.710><c> random</c><00:00:10.830><c> field</c><00:00:13.340><c> so</c><00:00:14.340><c> in</c><00:00:14.400><c> the</c>

00:00:14.480 --> 00:00:14.490 align:start position:0%
conditional random field so in the
 

00:00:14.490 --> 00:00:17.000 align:start position:0%
conditional random field so in the
previous<00:00:14.759><c> videos</c><00:00:15.089><c> we've</c><00:00:15.299><c> defined</c><00:00:15.750><c> what</c><00:00:16.199><c> we</c>

00:00:17.000 --> 00:00:17.010 align:start position:0%
previous videos we've defined what we
 

00:00:17.010 --> 00:00:18.740 align:start position:0%
previous videos we've defined what we
meant<00:00:17.190><c> by</c><00:00:17.279><c> a</c><00:00:17.369><c> linear</c><00:00:17.789><c> chain</c><00:00:18.119><c> conditional</c>

00:00:18.740 --> 00:00:18.750 align:start position:0%
meant by a linear chain conditional
 

00:00:18.750 --> 00:00:19.990 align:start position:0%
meant by a linear chain conditional
random<00:00:18.810><c> field</c><00:00:19.170><c> so</c><00:00:19.320><c> that's</c><00:00:19.470><c> the</c><00:00:19.650><c> specific</c>

00:00:19.990 --> 00:00:20.000 align:start position:0%
random field so that's the specific
 

00:00:20.000 --> 00:00:21.980 align:start position:0%
random field so that's the specific
conditional<00:00:21.000><c> random</c><00:00:21.119><c> field</c><00:00:21.480><c> we'll</c><00:00:21.630><c> consider</c>

00:00:21.980 --> 00:00:21.990 align:start position:0%
conditional random field we'll consider
 

00:00:21.990 --> 00:00:25.490 align:start position:0%
conditional random field we'll consider
so<00:00:22.680><c> P</c><00:00:22.920><c> of</c><00:00:22.949><c> Y</c><00:00:23.340><c> given</c><00:00:23.369><c> X</c><00:00:24.119><c> square</c><00:00:24.480><c> Y</c><00:00:24.720><c> is</c><00:00:24.779><c> a</c><00:00:24.960><c> sequence</c>

00:00:25.490 --> 00:00:25.500 align:start position:0%
so P of Y given X square Y is a sequence
 

00:00:25.500 --> 00:00:27.830 align:start position:0%
so P of Y given X square Y is a sequence
of<00:00:25.650><c> label</c><00:00:26.010><c> and</c><00:00:26.310><c> X</c><00:00:26.609><c> is</c><00:00:26.789><c> sequence</c><00:00:27.150><c> of</c><00:00:27.269><c> inputs</c><00:00:27.630><c> is</c>

00:00:27.830 --> 00:00:27.840 align:start position:0%
of label and X is sequence of inputs is
 

00:00:27.840 --> 00:00:31.569 align:start position:0%
of label and X is sequence of inputs is
going<00:00:28.410><c> to</c><00:00:28.590><c> be</c><00:00:28.710><c> the</c><00:00:28.859><c> exponential</c><00:00:29.340><c> of</c><00:00:29.910><c> a</c><00:00:30.119><c> sum</c><00:00:30.390><c> of</c>

00:00:31.569 --> 00:00:31.579 align:start position:0%
going to be the exponential of a sum of
 

00:00:31.579 --> 00:00:36.889 align:start position:0%
going to be the exponential of a sum of
unary<00:00:33.020><c> potential</c><00:00:34.250><c> log</c><00:00:35.250><c> factors</c><00:00:35.760><c> and</c><00:00:36.090><c> pairwise</c>

00:00:36.889 --> 00:00:36.899 align:start position:0%
unary potential log factors and pairwise
 

00:00:36.899 --> 00:00:39.560 align:start position:0%
unary potential log factors and pairwise
log<00:00:37.350><c> factors</c><00:00:37.829><c> divided</c><00:00:38.760><c> by</c><00:00:38.910><c> normalization</c>

00:00:39.560 --> 00:00:39.570 align:start position:0%
log factors divided by normalization
 

00:00:39.570 --> 00:00:42.080 align:start position:0%
log factors divided by normalization
constant<00:00:40.200><c> that</c><00:00:40.410><c> we</c><00:00:41.190><c> have</c><00:00:41.340><c> to</c><00:00:41.489><c> compute</c><00:00:41.850><c> with</c>

00:00:42.080 --> 00:00:42.090 align:start position:0%
constant that we have to compute with
 

00:00:42.090 --> 00:00:44.240 align:start position:0%
constant that we have to compute with
some<00:00:42.540><c> dynamic</c><00:00:43.290><c> programming</c><00:00:43.800><c> using</c><00:00:44.160><c> the</c>

00:00:44.240 --> 00:00:44.250 align:start position:0%
some dynamic programming using the
 

00:00:44.250 --> 00:00:49.549 align:start position:0%
some dynamic programming using the
forward<00:00:44.489><c> backward</c><00:00:45.110><c> algorithm</c><00:00:46.129><c> and</c><00:00:47.719><c> so</c><00:00:48.719><c> thus</c>

00:00:49.549 --> 00:00:49.559 align:start position:0%
forward backward algorithm and so thus
 

00:00:49.559 --> 00:00:51.560 align:start position:0%
forward backward algorithm and so thus
far<00:00:49.890><c> in</c><00:00:50.430><c> the</c><00:00:50.640><c> previous</c><00:00:50.940><c> videos</c><00:00:51.239><c> I</c><00:00:51.420><c> always</c>

00:00:51.560 --> 00:00:51.570 align:start position:0%
far in the previous videos I always
 

00:00:51.570 --> 00:00:53.660 align:start position:0%
far in the previous videos I always
assumed<00:00:52.079><c> that</c><00:00:52.620><c> our</c><00:00:52.890><c> conditional</c><00:00:53.579><c> random</c>

00:00:53.660 --> 00:00:53.670 align:start position:0%
assumed that our conditional random
 

00:00:53.670 --> 00:00:55.459 align:start position:0%
assumed that our conditional random
field<00:00:54.000><c> was</c><00:00:54.239><c> already</c><00:00:54.600><c> trained</c><00:00:55.079><c> and</c><00:00:55.289><c> I've</c>

00:00:55.459 --> 00:00:55.469 align:start position:0%
field was already trained and I've
 

00:00:55.469 --> 00:00:56.900 align:start position:0%
field was already trained and I've
talked<00:00:55.770><c> about</c><00:00:55.890><c> how</c><00:00:56.430><c> we</c><00:00:56.489><c> can</c><00:00:56.760><c> perform</c>

00:00:56.900 --> 00:00:56.910 align:start position:0%
talked about how we can perform
 

00:00:56.910 --> 00:00:58.939 align:start position:0%
talked about how we can perform
inference<00:00:57.510><c> compute</c><00:00:57.899><c> the</c><00:00:57.960><c> partition</c><00:00:58.440><c> function</c>

00:00:58.939 --> 00:00:58.949 align:start position:0%
inference compute the partition function
 

00:00:58.949 --> 00:01:01.099 align:start position:0%
inference compute the partition function
perform<00:00:59.760><c> classification</c><00:01:00.510><c> and</c><00:01:00.660><c> so</c><00:01:00.809><c> on</c>

00:01:01.099 --> 00:01:01.109 align:start position:0%
perform classification and so on
 

00:01:01.109 --> 00:01:02.660 align:start position:0%
perform classification and so on
so<00:01:01.199><c> now</c><00:01:01.289><c> we'll</c><00:01:01.469><c> finally</c><00:01:01.710><c> look</c><00:01:02.070><c> at</c><00:01:02.219><c> how</c><00:01:02.370><c> we</c><00:01:02.430><c> can</c>

00:01:02.660 --> 00:01:02.670 align:start position:0%
so now we'll finally look at how we can
 

00:01:02.670 --> 00:01:04.850 align:start position:0%
so now we'll finally look at how we can
train<00:01:02.940><c> a</c><00:01:02.969><c> conditional</c><00:01:03.510><c> random</c><00:01:03.629><c> field</c><00:01:04.019><c> on</c><00:01:04.229><c> some</c>

00:01:04.850 --> 00:01:04.860 align:start position:0%
train a conditional random field on some
 

00:01:04.860 --> 00:01:08.450 align:start position:0%
train a conditional random field on some
available<00:01:05.400><c> data</c><00:01:07.010><c> so</c><00:01:08.010><c> again</c><00:01:08.250><c> we'll</c><00:01:08.430><c> use</c>

00:01:08.450 --> 00:01:08.460 align:start position:0%
available data so again we'll use
 

00:01:08.460 --> 00:01:11.149 align:start position:0%
available data so again we'll use
empirical<00:01:09.119><c> risk</c><00:01:09.510><c> minimization</c><00:01:09.810><c> as</c><00:01:10.619><c> our</c>

00:01:11.149 --> 00:01:11.159 align:start position:0%
empirical risk minimization as our
 

00:01:11.159 --> 00:01:13.340 align:start position:0%
empirical risk minimization as our
guiding<00:01:11.729><c> principle</c><00:01:12.360><c> for</c><00:01:12.390><c> deriving</c><00:01:13.020><c> and</c>

00:01:13.340 --> 00:01:13.350 align:start position:0%
guiding principle for deriving and
 

00:01:13.350 --> 00:01:15.740 align:start position:0%
guiding principle for deriving and
learning<00:01:13.619><c> algorithm</c><00:01:14.040><c> that</c><00:01:14.820><c> is</c><00:01:15.060><c> we</c><00:01:15.299><c> are</c><00:01:15.330><c> going</c>

00:01:15.740 --> 00:01:15.750 align:start position:0%
learning algorithm that is we are going
 

00:01:15.750 --> 00:01:18.830 align:start position:0%
learning algorithm that is we are going
to<00:01:15.869><c> choose</c><00:01:16.290><c> a</c><00:01:16.590><c> loss</c><00:01:17.100><c> function</c><00:01:17.400><c> and</c><00:01:17.909><c> to</c><00:01:18.509><c> train</c><00:01:18.810><c> a</c>

00:01:18.830 --> 00:01:18.840 align:start position:0%
to choose a loss function and to train a
 

00:01:18.840 --> 00:01:21.980 align:start position:0%
to choose a loss function and to train a
conditional<00:01:19.530><c> random</c><00:01:19.619><c> field</c><00:01:20.009><c> on</c><00:01:20.689><c> training</c><00:01:21.689><c> the</c>

00:01:21.980 --> 00:01:21.990 align:start position:0%
conditional random field on training the
 

00:01:21.990 --> 00:01:24.950 align:start position:0%
conditional random field on training the
training<00:01:22.619><c> set</c><00:01:22.920><c> that</c><00:01:22.950><c> is</c><00:01:23.310><c> a</c><00:01:23.340><c> set</c><00:01:23.700><c> of</c><00:01:24.509><c> pairs</c><00:01:24.840><c> of</c>

00:01:24.950 --> 00:01:24.960 align:start position:0%
training set that is a set of pairs of
 

00:01:24.960 --> 00:01:28.100 align:start position:0%
training set that is a set of pairs of
input<00:01:25.920><c> sequences</c><00:01:26.490><c> and</c><00:01:26.820><c> target</c><00:01:27.360><c> sequences</c><00:01:27.840><c> or</c>

00:01:28.100 --> 00:01:28.110 align:start position:0%
input sequences and target sequences or
 

00:01:28.110 --> 00:01:30.499 align:start position:0%
input sequences and target sequences or
label<00:01:28.409><c> sequences</c><00:01:28.920><c> will</c><00:01:29.670><c> minimize</c><00:01:30.180><c> the</c>

00:01:30.499 --> 00:01:30.509 align:start position:0%
label sequences will minimize the
 

00:01:30.509 --> 00:01:32.960 align:start position:0%
label sequences will minimize the
average<00:01:30.750><c> of</c><00:01:31.409><c> the</c><00:01:31.710><c> sum</c><00:01:32.040><c> of</c><00:01:32.250><c> the</c><00:01:32.549><c> loss</c><00:01:32.729><c> function</c>

00:01:32.960 --> 00:01:32.970 align:start position:0%
average of the sum of the loss function
 

00:01:32.970 --> 00:01:34.880 align:start position:0%
average of the sum of the loss function
where<00:01:33.450><c> we</c><00:01:33.479><c> compare</c><00:01:34.079><c> to</c><00:01:34.290><c> what</c><00:01:34.439><c> extent</c><00:01:34.799><c> our</c>

00:01:34.880 --> 00:01:34.890 align:start position:0%
where we compare to what extent our
 

00:01:34.890 --> 00:01:39.260 align:start position:0%
where we compare to what extent our
model<00:01:35.280><c> fits</c><00:01:35.579><c> well</c><00:01:35.880><c> with</c><00:01:36.600><c> the</c><00:01:37.189><c> target</c><00:01:38.270><c> sequence</c>

00:01:39.260 --> 00:01:39.270 align:start position:0%
model fits well with the target sequence
 

00:01:39.270 --> 00:01:41.870 align:start position:0%
model fits well with the target sequence
information<00:01:39.900><c> we</c><00:01:40.020><c> have</c><00:01:40.229><c> so</c><00:01:41.220><c> we'll</c><00:01:41.400><c> optimize</c>

00:01:41.870 --> 00:01:41.880 align:start position:0%
information we have so we'll optimize
 

00:01:41.880 --> 00:01:46.399 align:start position:0%
information we have so we'll optimize
this<00:01:42.119><c> average</c><00:01:42.600><c> loss</c><00:01:42.899><c> along</c><00:01:43.500><c> so</c><00:01:44.189><c> plus</c><00:01:44.549><c> the</c><00:01:45.420><c> way</c>

00:01:46.399 --> 00:01:46.409 align:start position:0%
this average loss along so plus the way
 

00:01:46.409 --> 00:01:49.700 align:start position:0%
this average loss along so plus the way
the<00:01:46.649><c> regularizer</c><00:01:47.130><c> to</c><00:01:48.049><c> control</c><00:01:49.049><c> to</c><00:01:49.590><c> what</c>

00:01:49.700 --> 00:01:49.710 align:start position:0%
the regularizer to control to what
 

00:01:49.710 --> 00:01:51.980 align:start position:0%
the regularizer to control to what
extent<00:01:50.009><c> we</c><00:01:50.100><c> won</c><00:01:50.280><c> our</c><00:01:50.310><c> model</c><00:01:50.700><c> to</c><00:01:50.909><c> fit</c><00:01:51.540><c> well</c><00:01:51.810><c> the</c>

00:01:51.980 --> 00:01:51.990 align:start position:0%
extent we won our model to fit well the
 

00:01:51.990 --> 00:01:54.499 align:start position:0%
extent we won our model to fit well the
data<00:01:52.229><c> and</c><00:01:52.560><c> to</c><00:01:53.460><c> be</c><00:01:53.579><c> able</c><00:01:53.670><c> to</c><00:01:53.820><c> control</c><00:01:54.270><c> for</c>

00:01:54.499 --> 00:01:54.509 align:start position:0%
data and to be able to control for
 

00:01:54.509 --> 00:01:57.020 align:start position:0%
data and to be able to control for
overfitting<00:01:55.250><c> so</c><00:01:56.250><c> again</c><00:01:56.430><c> we're</c><00:01:56.579><c> casting</c>

00:01:57.020 --> 00:01:57.030 align:start position:0%
overfitting so again we're casting
 

00:01:57.030 --> 00:01:58.700 align:start position:0%
overfitting so again we're casting
learning<00:01:57.360><c> as</c><00:01:57.479><c> optimization</c><00:01:58.170><c> like</c><00:01:58.320><c> we've</c><00:01:58.530><c> done</c>

00:01:58.700 --> 00:01:58.710 align:start position:0%
learning as optimization like we've done
 

00:01:58.710 --> 00:02:01.459 align:start position:0%
learning as optimization like we've done
in<00:01:58.920><c> the</c><00:01:59.009><c> regular</c><00:01:59.310><c> neural</c><00:01:59.640><c> network</c><00:02:00.030><c> and</c><00:02:00.469><c> though</c>

00:02:01.459 --> 00:02:01.469 align:start position:0%
in the regular neural network and though
 

00:02:01.469 --> 00:02:03.469 align:start position:0%
in the regular neural network and though
ideally<00:02:01.860><c> we'd</c><00:02:02.100><c> like</c><00:02:02.280><c> to</c><00:02:02.430><c> optimize</c><00:02:02.850><c> the</c>

00:02:03.469 --> 00:02:03.479 align:start position:0%
ideally we'd like to optimize the
 

00:02:03.479 --> 00:02:05.389 align:start position:0%
ideally we'd like to optimize the
classification<00:02:04.409><c> error</c><00:02:04.619><c> so</c><00:02:04.920><c> in</c><00:02:05.009><c> this</c><00:02:05.159><c> case</c>

00:02:05.389 --> 00:02:05.399 align:start position:0%
classification error so in this case
 

00:02:05.399 --> 00:02:07.190 align:start position:0%
classification error so in this case
perhaps<00:02:05.909><c> would</c><00:02:06.149><c> like</c><00:02:06.299><c> to</c><00:02:06.390><c> optimize</c><00:02:06.570><c> the</c><00:02:06.869><c> sum</c>

00:02:07.190 --> 00:02:07.200 align:start position:0%
perhaps would like to optimize the sum
 

00:02:07.200 --> 00:02:10.369 align:start position:0%
perhaps would like to optimize the sum
of<00:02:07.409><c> per</c><00:02:07.950><c> label</c><00:02:08.190><c> classification</c><00:02:09.179><c> there</c><00:02:09.390><c> this</c>

00:02:10.369 --> 00:02:10.379 align:start position:0%
of per label classification there this
 

00:02:10.379 --> 00:02:12.140 align:start position:0%
of per label classification there this
error<00:02:10.619><c> again</c><00:02:10.979><c> is</c><00:02:11.160><c> non</c><00:02:11.310><c> smooth</c><00:02:11.640><c> so</c><00:02:11.819><c> we</c><00:02:11.910><c> have</c><00:02:12.030><c> to</c>

00:02:12.140 --> 00:02:12.150 align:start position:0%
error again is non smooth so we have to
 

00:02:12.150 --> 00:02:13.309 align:start position:0%
error again is non smooth so we have to
use<00:02:12.300><c> what</c><00:02:12.840><c> we</c><00:02:13.110><c> use</c>

00:02:13.309 --> 00:02:13.319 align:start position:0%
use what we use
 

00:02:13.319 --> 00:02:14.599 align:start position:0%
use what we use
we<00:02:13.379><c> call</c><00:02:13.560><c> a</c><00:02:13.590><c> surrogate</c><00:02:14.069><c> loss</c><00:02:14.280><c> or</c><00:02:14.519><c> an</c>

00:02:14.599 --> 00:02:14.609 align:start position:0%
we call a surrogate loss or an
 

00:02:14.609 --> 00:02:16.369 align:start position:0%
we call a surrogate loss or an
alternative<00:02:15.120><c> loss</c><00:02:15.299><c> that</c><00:02:15.540><c> sort</c><00:02:15.870><c> of</c><00:02:15.989><c> looks</c><00:02:16.170><c> like</c>

00:02:16.369 --> 00:02:16.379 align:start position:0%
alternative loss that sort of looks like
 

00:02:16.379 --> 00:02:18.559 align:start position:0%
alternative loss that sort of looks like
what<00:02:16.590><c> we</c><00:02:16.709><c> want</c><00:02:16.890><c> to</c><00:02:16.980><c> optimize</c><00:02:17.189><c> but</c><00:02:18.030><c> that</c><00:02:18.359><c> is</c>

00:02:18.559 --> 00:02:18.569 align:start position:0%
what we want to optimize but that is
 

00:02:18.569 --> 00:02:21.770 align:start position:0%
what we want to optimize but that is
better<00:02:19.260><c> behaved</c><00:02:19.739><c> and</c><00:02:20.700><c> specifically</c><00:02:21.540><c> in</c><00:02:21.659><c> this</c>

00:02:21.770 --> 00:02:21.780 align:start position:0%
better behaved and specifically in this
 

00:02:21.780 --> 00:02:23.539 align:start position:0%
better behaved and specifically in this
case<00:02:21.840><c> is</c><00:02:22.139><c> going</c><00:02:22.290><c> to</c><00:02:22.349><c> be</c><00:02:22.409><c> differentiable</c><00:02:23.280><c> so</c><00:02:23.340><c> we</c>

00:02:23.539 --> 00:02:23.549 align:start position:0%
case is going to be differentiable so we
 

00:02:23.549 --> 00:02:26.949 align:start position:0%
case is going to be differentiable so we
can<00:02:23.700><c> perform</c><00:02:23.909><c> gradient</c><00:02:24.540><c> descent</c><00:02:24.870><c> training</c>

00:02:26.949 --> 00:02:26.959 align:start position:0%
can perform gradient descent training
 

00:02:26.959 --> 00:02:29.959 align:start position:0%
can perform gradient descent training
and<00:02:27.959><c> again</c><00:02:28.170><c> we'll</c><00:02:28.349><c> use</c><00:02:28.849><c> stochastic</c><00:02:29.849><c> gradient</c>

00:02:29.959 --> 00:02:29.969 align:start position:0%
and again we'll use stochastic gradient
 

00:02:29.969 --> 00:02:33.080 align:start position:0%
and again we'll use stochastic gradient
descent<00:02:30.569><c> that</c><00:02:30.840><c> is</c><00:02:31.560><c> the</c><00:02:32.280><c> way</c><00:02:32.400><c> where</c><00:02:32.609><c> training</c>

00:02:33.080 --> 00:02:33.090 align:start position:0%
descent that is the way where training
 

00:02:33.090 --> 00:02:34.520 align:start position:0%
descent that is the way where training
is<00:02:33.269><c> going</c><00:02:33.420><c> to</c><00:02:33.510><c> proceed</c><00:02:33.689><c> is</c><00:02:33.989><c> that</c><00:02:34.170><c> we'll</c><00:02:34.319><c> first</c>

00:02:34.520 --> 00:02:34.530 align:start position:0%
is going to proceed is that we'll first
 

00:02:34.530 --> 00:02:36.229 align:start position:0%
is going to proceed is that we'll first
initialize<00:02:35.010><c> the</c><00:02:35.159><c> parameters</c><00:02:35.579><c> of</c><00:02:35.670><c> our</c><00:02:35.819><c> model</c>

00:02:36.229 --> 00:02:36.239 align:start position:0%
initialize the parameters of our model
 

00:02:36.239 --> 00:02:38.360 align:start position:0%
initialize the parameters of our model
for<00:02:36.450><c> a</c><00:02:36.930><c> certain</c><00:02:37.230><c> number</c><00:02:37.349><c> of</c><00:02:37.560><c> iterations</c><00:02:38.099><c> will</c>

00:02:38.360 --> 00:02:38.370 align:start position:0%
for a certain number of iterations will
 

00:02:38.370 --> 00:02:40.729 align:start position:0%
for a certain number of iterations will
cycle<00:02:38.639><c> through</c><00:02:38.879><c> our</c><00:02:39.389><c> pairs</c><00:02:39.870><c> of</c><00:02:40.139><c> input</c>

00:02:40.729 --> 00:02:40.739 align:start position:0%
cycle through our pairs of input
 

00:02:40.739 --> 00:02:43.610 align:start position:0%
cycle through our pairs of input
sequences<00:02:41.280><c> and</c><00:02:41.579><c> target</c><00:02:42.299><c> sequences</c><00:02:42.780><c> we'll</c>

00:02:43.610 --> 00:02:43.620 align:start position:0%
sequences and target sequences we'll
 

00:02:43.620 --> 00:02:46.429 align:start position:0%
sequences and target sequences we'll
compute<00:02:44.129><c> the</c><00:02:44.609><c> update</c><00:02:45.599><c> direction</c><00:02:46.049><c> which</c><00:02:46.290><c> is</c>

00:02:46.429 --> 00:02:46.439 align:start position:0%
compute the update direction which is
 

00:02:46.439 --> 00:02:48.709 align:start position:0%
compute the update direction which is
going<00:02:46.620><c> to</c><00:02:46.709><c> be</c><00:02:46.889><c> minus</c><00:02:47.400><c> the</c><00:02:47.430><c> gradient</c><00:02:48.030><c> of</c><00:02:48.180><c> the</c>

00:02:48.709 --> 00:02:48.719 align:start position:0%
going to be minus the gradient of the
 

00:02:48.719 --> 00:02:53.539 align:start position:0%
going to be minus the gradient of the
loss<00:02:48.930><c> for</c><00:02:49.409><c> my</c><00:02:49.980><c> current</c><00:02:50.700><c> pair</c><00:02:51.540><c> of</c><00:02:52.400><c> input</c><00:02:53.400><c> and</c>

00:02:53.539 --> 00:02:53.549 align:start position:0%
loss for my current pair of input and
 

00:02:53.549 --> 00:02:56.869 align:start position:0%
loss for my current pair of input and
target<00:02:54.120><c> sequences</c><00:02:54.599><c> -</c><00:02:55.379><c> also</c><00:02:55.919><c> the</c><00:02:56.400><c> gradient</c><00:02:56.760><c> of</c>

00:02:56.869 --> 00:02:56.879 align:start position:0%
target sequences - also the gradient of
 

00:02:56.879 --> 00:02:58.789 align:start position:0%
target sequences - also the gradient of
the<00:02:57.090><c> regularizer</c><00:02:57.629><c> and</c><00:02:58.169><c> then</c><00:02:58.439><c> I'll</c><00:02:58.560><c> take</c><00:02:58.769><c> a</c>

00:02:58.789 --> 00:02:58.799 align:start position:0%
the regularizer and then I'll take a
 

00:02:58.799 --> 00:03:00.140 align:start position:0%
the regularizer and then I'll take a
small<00:02:59.099><c> step</c>

00:03:00.140 --> 00:03:00.150 align:start position:0%
small step
 

00:03:00.150 --> 00:03:04.459 align:start position:0%
small step
so<00:03:00.840><c> alpha</c><00:03:01.709><c> times</c><00:03:02.280><c> the</c><00:03:02.669><c> update</c><00:03:03.329><c> direction</c><00:03:03.900><c> to</c>

00:03:04.459 --> 00:03:04.469 align:start position:0%
so alpha times the update direction to
 

00:03:04.469 --> 00:03:07.699 align:start position:0%
so alpha times the update direction to
change<00:03:04.829><c> my</c><00:03:05.629><c> vector</c><00:03:06.629><c> from</c><00:03:06.870><c> is</c><00:03:07.169><c> my</c><00:03:07.199><c> set</c><00:03:07.590><c> of</c>

00:03:07.699 --> 00:03:07.709 align:start position:0%
change my vector from is my set of
 

00:03:07.709 --> 00:03:09.800 align:start position:0%
change my vector from is my set of
parameters<00:03:08.219><c> and</c><00:03:08.430><c> so</c><00:03:09.209><c> again</c><00:03:09.419><c> I</c><00:03:09.480><c> have</c><00:03:09.689><c> to</c>

00:03:09.800 --> 00:03:09.810 align:start position:0%
parameters and so again I have to
 

00:03:09.810 --> 00:03:12.199 align:start position:0%
parameters and so again I have to
specify<00:03:10.049><c> a</c><00:03:10.349><c> loss</c><00:03:10.620><c> function</c><00:03:10.829><c> a</c><00:03:11.280><c> specified</c>

00:03:12.199 --> 00:03:12.209 align:start position:0%
specify a loss function a specified
 

00:03:12.209 --> 00:03:13.729 align:start position:0%
specify a loss function a specified
procedure<00:03:12.599><c> for</c><00:03:12.810><c> computing</c><00:03:13.290><c> the</c><00:03:13.379><c> gradient</c>

00:03:13.729 --> 00:03:13.739 align:start position:0%
procedure for computing the gradient
 

00:03:13.739 --> 00:03:15.740 align:start position:0%
procedure for computing the gradient
with<00:03:13.980><c> respect</c><00:03:14.010><c> to</c><00:03:14.370><c> the</c><00:03:14.519><c> loss</c><00:03:14.669><c> function</c><00:03:15.120><c> I</c><00:03:15.299><c> also</c>

00:03:15.740 --> 00:03:15.750 align:start position:0%
with respect to the loss function I also
 

00:03:15.750 --> 00:03:17.929 align:start position:0%
with respect to the loss function I also
have<00:03:15.930><c> to</c><00:03:16.049><c> specify</c><00:03:16.230><c> the</c><00:03:16.949><c> regularizer</c><00:03:17.310><c> and</c><00:03:17.819><c> its</c>

00:03:17.929 --> 00:03:17.939 align:start position:0%
have to specify the regularizer and its
 

00:03:17.939 --> 00:03:19.909 align:start position:0%
have to specify the regularizer and its
gradient<00:03:18.209><c> as</c><00:03:18.540><c> well</c><00:03:18.900><c> as</c><00:03:19.049><c> the</c><00:03:19.199><c> initialization</c>

00:03:19.909 --> 00:03:19.919 align:start position:0%
gradient as well as the initialization
 

00:03:19.919 --> 00:03:21.979 align:start position:0%
gradient as well as the initialization
method<00:03:20.250><c> so</c><00:03:20.849><c> these</c><00:03:20.970><c> two</c><00:03:21.180><c> parts</c><00:03:21.389><c> are</c><00:03:21.629><c> not</c><00:03:21.750><c> really</c>

00:03:21.979 --> 00:03:21.989 align:start position:0%
method so these two parts are not really
 

00:03:21.989 --> 00:03:23.089 align:start position:0%
method so these two parts are not really
going<00:03:22.109><c> to</c><00:03:22.199><c> change</c><00:03:22.409><c> so</c><00:03:22.650><c> we're</c><00:03:22.799><c> not</c><00:03:22.919><c> really</c>

00:03:23.089 --> 00:03:23.099 align:start position:0%
going to change so we're not really
 

00:03:23.099 --> 00:03:25.069 align:start position:0%
going to change so we're not really
gonna<00:03:23.250><c> guess</c><00:03:23.489><c> discussed</c><00:03:24.000><c> them</c><00:03:24.180><c> will</c><00:03:24.479><c> focus</c><00:03:24.750><c> on</c>

00:03:25.069 --> 00:03:25.079 align:start position:0%
gonna guess discussed them will focus on
 

00:03:25.079 --> 00:03:27.860 align:start position:0%
gonna guess discussed them will focus on
choosing<00:03:25.769><c> a</c><00:03:25.979><c> loss</c><00:03:26.129><c> function</c><00:03:26.340><c> and</c><00:03:26.870><c> also</c>

00:03:27.860 --> 00:03:27.870 align:start position:0%
choosing a loss function and also
 

00:03:27.870 --> 00:03:30.379 align:start position:0%
choosing a loss function and also
specifying<00:03:28.859><c> how</c><00:03:28.979><c> we</c><00:03:29.099><c> compute</c><00:03:29.519><c> the</c><00:03:29.819><c> parameter</c>

00:03:30.379 --> 00:03:30.389 align:start position:0%
specifying how we compute the parameter
 

00:03:30.389 --> 00:03:32.479 align:start position:0%
specifying how we compute the parameter
gradient<00:03:30.840><c> and</c><00:03:31.049><c> in</c><00:03:31.470><c> this</c><00:03:31.859><c> particular</c><00:03:32.099><c> video</c>

00:03:32.479 --> 00:03:32.489 align:start position:0%
gradient and in this particular video
 

00:03:32.489 --> 00:03:34.429 align:start position:0%
gradient and in this particular video
we'll<00:03:32.819><c> look</c><00:03:33.269><c> at</c><00:03:33.479><c> the</c><00:03:33.810><c> loss</c><00:03:33.959><c> function</c><00:03:34.139><c> would</c>

00:03:34.429 --> 00:03:34.439 align:start position:0%
we'll look at the loss function would
 

00:03:34.439 --> 00:03:40.069 align:start position:0%
we'll look at the loss function would
optimize<00:03:37.459><c> so</c><00:03:38.459><c> again</c><00:03:38.699><c> because</c><00:03:39.389><c> a</c><00:03:39.629><c> conditional</c>

00:03:40.069 --> 00:03:40.079 align:start position:0%
optimize so again because a conditional
 

00:03:40.079 --> 00:03:42.140 align:start position:0%
optimize so again because a conditional
random<00:03:40.169><c> field</c><00:03:40.620><c> estimates</c><00:03:41.400><c> the</c><00:03:41.639><c> conditional</c>

00:03:42.140 --> 00:03:42.150 align:start position:0%
random field estimates the conditional
 

00:03:42.150 --> 00:03:44.179 align:start position:0%
random field estimates the conditional
probability<00:03:42.689><c> of</c><00:03:42.720><c> the</c><00:03:42.989><c> target</c><00:03:43.409><c> given</c><00:03:43.709><c> the</c>

00:03:44.179 --> 00:03:44.189 align:start position:0%
probability of the target given the
 

00:03:44.189 --> 00:03:47.659 align:start position:0%
probability of the target given the
input<00:03:44.489><c> what</c><00:03:45.209><c> is</c><00:03:45.449><c> an</c><00:03:46.439><c> actual</c><00:03:46.889><c> thing</c><00:03:47.040><c> to</c><00:03:47.250><c> do</c><00:03:47.430><c> is</c>

00:03:47.659 --> 00:03:47.669 align:start position:0%
input what is an actual thing to do is
 

00:03:47.669 --> 00:03:50.719 align:start position:0%
input what is an actual thing to do is
to<00:03:47.879><c> maximize</c><00:03:48.540><c> the</c><00:03:48.569><c> probability</c><00:03:49.040><c> assigned</c><00:03:50.040><c> to</c>

00:03:50.719 --> 00:03:50.729 align:start position:0%
to maximize the probability assigned to
 

00:03:50.729 --> 00:03:54.140 align:start position:0%
to maximize the probability assigned to
the<00:03:50.879><c> true</c><00:03:51.209><c> target</c><00:03:51.599><c> the</c><00:03:52.199><c> sequence</c><00:03:52.739><c> given</c><00:03:53.609><c> some</c>

00:03:54.140 --> 00:03:54.150 align:start position:0%
the true target the sequence given some
 

00:03:54.150 --> 00:03:56.869 align:start position:0%
the true target the sequence given some
input<00:03:54.599><c> sequence</c><00:03:55.019><c> as</c><00:03:55.859><c> extracted</c><00:03:56.549><c> from</c><00:03:56.639><c> our</c>

00:03:56.869 --> 00:03:56.879 align:start position:0%
input sequence as extracted from our
 

00:03:56.879 --> 00:04:00.559 align:start position:0%
input sequence as extracted from our
training<00:03:57.269><c> set</c><00:03:57.479><c> and</c><00:03:58.069><c> as</c><00:03:59.069><c> we've</c><00:03:59.819><c> seen</c><00:04:00.090><c> in</c><00:04:00.389><c> the</c>

00:04:00.559 --> 00:04:00.569 align:start position:0%
training set and as we've seen in the
 

00:04:00.569 --> 00:04:02.659 align:start position:0%
training set and as we've seen in the
case<00:04:00.780><c> of</c><00:04:01.079><c> a</c><00:04:01.319><c> regular</c><00:04:01.709><c> neural</c><00:04:01.979><c> network</c><00:04:02.340><c> instead</c>

00:04:02.659 --> 00:04:02.669 align:start position:0%
case of a regular neural network instead
 

00:04:02.669 --> 00:04:04.759 align:start position:0%
case of a regular neural network instead
of<00:04:02.939><c> maximizing</c><00:04:03.479><c> probabilities</c><00:04:04.439><c> we</c><00:04:04.620><c> can</c>

00:04:04.759 --> 00:04:04.769 align:start position:0%
of maximizing probabilities we can
 

00:04:04.769 --> 00:04:07.159 align:start position:0%
of maximizing probabilities we can
minimize<00:04:05.310><c> the</c><00:04:05.939><c> negative</c><00:04:06.540><c> log</c><00:04:06.870><c> of</c><00:04:06.930><c> the</c>

00:04:07.159 --> 00:04:07.169 align:start position:0%
minimize the negative log of the
 

00:04:07.169 --> 00:04:09.830 align:start position:0%
minimize the negative log of the
probability<00:04:07.439><c> which</c><00:04:08.159><c> is</c><00:04:08.340><c> more</c><00:04:08.579><c> better</c><00:04:09.389><c> behaved</c>

00:04:09.830 --> 00:04:09.840 align:start position:0%
probability which is more better behaved
 

00:04:09.840 --> 00:04:13.610 align:start position:0%
probability which is more better behaved
and<00:04:10.139><c> yield</c><00:04:10.560><c> simpler</c><00:04:11.039><c> math</c><00:04:11.250><c> and</c><00:04:11.579><c> so</c><00:04:12.479><c> to</c><00:04:13.349><c> firm</c><00:04:13.500><c> it</c>

00:04:13.610 --> 00:04:13.620 align:start position:0%
and yield simpler math and so to firm it
 

00:04:13.620 --> 00:04:16.009 align:start position:0%
and yield simpler math and so to firm it
as<00:04:13.739><c> a</c><00:04:13.769><c> minimization</c><00:04:14.239><c> problem</c><00:04:15.239><c> we</c><00:04:15.659><c> minimize</c>

00:04:16.009 --> 00:04:16.019 align:start position:0%
as a minimization problem we minimize
 

00:04:16.019 --> 00:04:17.870 align:start position:0%
as a minimization problem we minimize
the<00:04:16.229><c> negative</c><00:04:16.409><c> log</c><00:04:16.859><c> likelihood</c><00:04:17.400><c> so</c><00:04:17.579><c> our</c><00:04:17.699><c> last</c>

00:04:17.870 --> 00:04:17.880 align:start position:0%
the negative log likelihood so our last
 

00:04:17.880 --> 00:04:20.569 align:start position:0%
the negative log likelihood so our last
function<00:04:18.120><c> with</c><00:04:18.449><c> the</c><00:04:18.599><c> minus</c><00:04:19.109><c> the</c><00:04:19.469><c> log</c><00:04:19.680><c> of</c><00:04:19.919><c> the</c>

00:04:20.569 --> 00:04:20.579 align:start position:0%
function with the minus the log of the
 

00:04:20.579 --> 00:04:23.180 align:start position:0%
function with the minus the log of the
probability<00:04:21.419><c> of</c><00:04:21.449><c> the</c><00:04:21.989><c> true</c><00:04:22.289><c> target</c><00:04:22.919><c> so</c>

00:04:23.180 --> 00:04:23.190 align:start position:0%
probability of the true target so
 

00:04:23.190 --> 00:04:24.680 align:start position:0%
probability of the true target so
normally<00:04:23.520><c> this</c><00:04:23.669><c> would</c><00:04:23.849><c> be</c><00:04:23.880><c> the</c><00:04:24.060><c> true</c><00:04:24.240><c> target</c>

00:04:24.680 --> 00:04:24.690 align:start position:0%
normally this would be the true target
 

00:04:24.690 --> 00:04:26.370 align:start position:0%
normally this would be the true target
given<00:04:25.590><c> the</c>

00:04:26.370 --> 00:04:26.380 align:start position:0%
given the
 

00:04:26.380 --> 00:04:29.280 align:start position:0%
given the
input<00:04:26.800><c> sequence</c><00:04:27.660><c> so</c><00:04:28.660><c> this</c><00:04:28.810><c> is</c><00:04:28.930><c> really</c><00:04:29.080><c> the</c>

00:04:29.280 --> 00:04:29.290 align:start position:0%
input sequence so this is really the
 

00:04:29.290 --> 00:04:31.320 align:start position:0%
input sequence so this is really the
same<00:04:29.470><c> that</c><00:04:29.740><c> we've</c><00:04:29.890><c> done</c><00:04:30.070><c> in</c><00:04:30.370><c> a</c><00:04:30.730><c> regular</c><00:04:30.910><c> neural</c>

00:04:31.320 --> 00:04:31.330 align:start position:0%
same that we've done in a regular neural
 

00:04:31.330 --> 00:04:33.330 align:start position:0%
same that we've done in a regular neural
network<00:04:31.720><c> where</c><00:04:32.320><c> we</c><00:04:32.530><c> have</c><00:04:32.680><c> an</c><00:04:32.860><c> input</c><00:04:33.010><c> vector</c>

00:04:33.330 --> 00:04:33.340 align:start position:0%
network where we have an input vector
 

00:04:33.340 --> 00:04:35.370 align:start position:0%
network where we have an input vector
and<00:04:33.520><c> we're</c><00:04:34.000><c> trying</c><00:04:34.330><c> to</c><00:04:34.480><c> classify</c><00:04:34.690><c> it</c><00:04:34.990><c> to</c><00:04:35.200><c> do</c><00:04:35.350><c> a</c>

00:04:35.370 --> 00:04:35.380 align:start position:0%
and we're trying to classify it to do a
 

00:04:35.380 --> 00:04:38.430 align:start position:0%
and we're trying to classify it to do a
set<00:04:35.650><c> of</c><00:04:35.890><c> C</c><00:04:36.670><c> classes</c><00:04:37.180><c> the</c><00:04:37.660><c> only</c><00:04:37.960><c> difference</c><00:04:38.290><c> is</c>

00:04:38.430 --> 00:04:38.440 align:start position:0%
set of C classes the only difference is
 

00:04:38.440 --> 00:04:40.440 align:start position:0%
set of C classes the only difference is
that<00:04:38.470><c> now</c><00:04:38.800><c> this</c><00:04:39.280><c> is</c><00:04:39.460><c> a</c><00:04:39.490><c> sequence</c><00:04:39.910><c> and</c><00:04:40.150><c> this</c><00:04:40.300><c> is</c>

00:04:40.440 --> 00:04:40.450 align:start position:0%
that now this is a sequence and this is
 

00:04:40.450 --> 00:04:43.620 align:start position:0%
that now this is a sequence and this is
also<00:04:40.690><c> a</c><00:04:40.720><c> sequence</c><00:04:41.250><c> one</c><00:04:42.250><c> sort</c><00:04:43.090><c> of</c><00:04:43.150><c> subtle</c>

00:04:43.620 --> 00:04:43.630 align:start position:0%
also a sequence one sort of subtle
 

00:04:43.630 --> 00:04:46.700 align:start position:0%
also a sequence one sort of subtle
difference<00:04:44.050><c> also</c><00:04:44.410><c> is</c><00:04:44.650><c> that</c><00:04:45.030><c> while</c><00:04:46.030><c> in</c>

00:04:46.700 --> 00:04:46.710 align:start position:0%
difference also is that while in
 

00:04:46.710 --> 00:04:48.900 align:start position:0%
difference also is that while in
non-sequential<00:04:47.710><c> classification</c><00:04:48.640><c> if</c><00:04:48.820><c> you</c>

00:04:48.900 --> 00:04:48.910 align:start position:0%
non-sequential classification if you
 

00:04:48.910 --> 00:04:50.400 align:start position:0%
non-sequential classification if you
have<00:04:49.000><c> ten</c><00:04:49.240><c> classes</c><00:04:49.690><c> we</c><00:04:49.840><c> might</c><00:04:50.020><c> actually</c>

00:04:50.400 --> 00:04:50.410 align:start position:0%
have ten classes we might actually
 

00:04:50.410 --> 00:04:53.280 align:start position:0%
have ten classes we might actually
explicitly<00:04:50.800><c> compute</c><00:04:51.580><c> the</c><00:04:52.120><c> full</c><00:04:52.480><c> conditional</c>

00:04:53.280 --> 00:04:53.290 align:start position:0%
explicitly compute the full conditional
 

00:04:53.290 --> 00:04:55.530 align:start position:0%
explicitly compute the full conditional
distribution<00:04:53.470><c> P</c><00:04:54.070><c> of</c><00:04:54.160><c> Y</c><00:04:54.280><c> given</c><00:04:54.310><c> X</c><00:04:54.820><c> by</c><00:04:55.060><c> iterating</c>

00:04:55.530 --> 00:04:55.540 align:start position:0%
distribution P of Y given X by iterating
 

00:04:55.540 --> 00:04:58.170 align:start position:0%
distribution P of Y given X by iterating
over<00:04:55.600><c> all</c><00:04:55.870><c> classes</c><00:04:56.350><c> and</c><00:04:56.590><c> so</c><00:04:57.310><c> obtaining</c><00:04:58.090><c> the</c>

00:04:58.170 --> 00:04:58.180 align:start position:0%
over all classes and so obtaining the
 

00:04:58.180 --> 00:05:00.960 align:start position:0%
over all classes and so obtaining the
full<00:04:58.420><c> vector</c><00:04:58.630><c> of</c><00:04:58.930><c> probability</c><00:04:59.590><c> now</c><00:05:00.340><c> with</c><00:05:00.940><c> a</c>

00:05:00.960 --> 00:05:00.970 align:start position:0%
full vector of probability now with a
 

00:05:00.970 --> 00:05:02.550 align:start position:0%
full vector of probability now with a
sequence<00:05:01.420><c> we</c><00:05:01.570><c> can</c><00:05:01.600><c> do</c><00:05:01.870><c> this</c><00:05:02.020><c> because</c><00:05:02.230><c> the</c>

00:05:02.550 --> 00:05:02.560 align:start position:0%
sequence we can do this because the
 

00:05:02.560 --> 00:05:05.370 align:start position:0%
sequence we can do this because the
number<00:05:02.980><c> of</c><00:05:03.250><c> sequences</c><00:05:03.700><c> is</c><00:05:04.180><c> exponential</c><00:05:04.900><c> so</c>

00:05:05.370 --> 00:05:05.380 align:start position:0%
number of sequences is exponential so
 

00:05:05.380 --> 00:05:08.670 align:start position:0%
number of sequences is exponential so
really<00:05:05.650><c> when</c><00:05:05.860><c> we're</c><00:05:06.160><c> computing</c><00:05:07.120><c> this</c><00:05:07.720><c> we'll</c>

00:05:08.670 --> 00:05:08.680 align:start position:0%
really when we're computing this we'll
 

00:05:08.680 --> 00:05:13.110 align:start position:0%
really when we're computing this we'll
use<00:05:10.170><c> forward</c><00:05:11.170><c> backward</c><00:05:11.560><c> to</c><00:05:11.830><c> get</c><00:05:12.400><c> our</c><00:05:12.610><c> alpha</c><00:05:13.060><c> or</c>

00:05:13.110 --> 00:05:13.120 align:start position:0%
use forward backward to get our alpha or
 

00:05:13.120 --> 00:05:15.480 align:start position:0%
use forward backward to get our alpha or
beta<00:05:13.420><c> table</c><00:05:13.990><c> and</c><00:05:14.260><c> to</c><00:05:14.620><c> compute</c><00:05:14.980><c> the</c><00:05:15.040><c> partition</c>

00:05:15.480 --> 00:05:15.490 align:start position:0%
beta table and to compute the partition
 

00:05:15.490 --> 00:05:18.570 align:start position:0%
beta table and to compute the partition
function<00:05:15.940><c> and</c><00:05:16.470><c> we'll</c><00:05:17.470><c> use</c><00:05:17.740><c> then</c><00:05:17.980><c> explicitly</c>

00:05:18.570 --> 00:05:18.580 align:start position:0%
function and we'll use then explicitly
 

00:05:18.580 --> 00:05:20.160 align:start position:0%
function and we'll use then explicitly
the<00:05:18.730><c> partition</c><00:05:19.300><c> function</c><00:05:19.690><c> into</c><00:05:20.140><c> the</c>

00:05:20.160 --> 00:05:20.170 align:start position:0%
the partition function into the
 

00:05:20.170 --> 00:05:23.220 align:start position:0%
the partition function into the
expression<00:05:20.560><c> for</c><00:05:21.040><c> P</c><00:05:21.940><c> of</c><00:05:22.030><c> Y</c><00:05:22.210><c> given</c><00:05:22.240><c> X</c><00:05:22.720><c> and</c><00:05:22.930><c> then</c>

00:05:23.220 --> 00:05:23.230 align:start position:0%
expression for P of Y given X and then
 

00:05:23.230 --> 00:05:25.320 align:start position:0%
expression for P of Y given X and then
we'll<00:05:23.440><c> take</c><00:05:23.800><c> minus</c><00:05:24.130><c> the</c><00:05:24.280><c> log</c><00:05:24.460><c> of</c><00:05:24.640><c> that</c><00:05:24.790><c> so</c><00:05:25.240><c> we</c>

00:05:25.320 --> 00:05:25.330 align:start position:0%
we'll take minus the log of that so we
 

00:05:25.330 --> 00:05:28.530 align:start position:0%
we'll take minus the log of that so we
won't<00:05:25.420><c> actually</c><00:05:25.690><c> compute</c><00:05:26.910><c> we</c><00:05:27.910><c> don't</c><00:05:28.030><c> have</c><00:05:28.240><c> the</c>

00:05:28.530 --> 00:05:28.540 align:start position:0%
won't actually compute we don't have the
 

00:05:28.540 --> 00:05:30.540 align:start position:0%
won't actually compute we don't have the
full<00:05:28.780><c> distribution</c><00:05:28.990><c> over</c><00:05:29.680><c> all</c><00:05:30.190><c> sequences</c>

00:05:30.540 --> 00:05:30.550 align:start position:0%
full distribution over all sequences
 

00:05:30.550 --> 00:05:33.150 align:start position:0%
full distribution over all sequences
that<00:05:31.090><c> just</c><00:05:31.810><c> because</c><00:05:32.020><c> that's</c><00:05:32.200><c> too</c><00:05:32.770><c> big</c><00:05:32.950><c> of</c><00:05:33.070><c> an</c>

00:05:33.150 --> 00:05:33.160 align:start position:0%
that just because that's too big of an
 

00:05:33.160 --> 00:05:36.150 align:start position:0%
that just because that's too big of an
object<00:05:33.580><c> it's</c><00:05:33.760><c> something</c><00:05:34.420><c> that's</c><00:05:35.040><c> there's</c><00:05:36.040><c> an</c>

00:05:36.150 --> 00:05:36.160 align:start position:0%
object it's something that's there's an
 

00:05:36.160 --> 00:05:37.440 align:start position:0%
object it's something that's there's an
exponential<00:05:36.550><c> of</c><00:05:36.670><c> all</c><00:05:36.760><c> sequences</c><00:05:37.030><c> that</c><00:05:37.360><c> I</c>

00:05:37.440 --> 00:05:37.450 align:start position:0%
exponential of all sequences that I
 

00:05:37.450 --> 00:05:38.550 align:start position:0%
exponential of all sequences that I
would<00:05:37.570><c> be</c><00:05:37.630><c> an</c><00:05:37.720><c> exponential</c><00:05:38.260><c> number</c><00:05:38.320><c> of</c>

00:05:38.550 --> 00:05:38.560 align:start position:0%
would be an exponential number of
 

00:05:38.560 --> 00:05:40.830 align:start position:0%
would be an exponential number of
probabilities<00:05:39.340><c> so</c><00:05:40.240><c> that's</c><00:05:40.390><c> the</c><00:05:40.570><c> last</c>

00:05:40.830 --> 00:05:40.840 align:start position:0%
probabilities so that's the last
 

00:05:40.840 --> 00:05:42.300 align:start position:0%
probabilities so that's the last
function<00:05:41.080><c> we'll</c><00:05:41.440><c> use</c><00:05:41.680><c> for</c><00:05:41.950><c> training</c><00:05:42.250><c> a</c>

00:05:42.300 --> 00:05:42.310 align:start position:0%
function we'll use for training a
 

00:05:42.310 --> 00:05:45.360 align:start position:0%
function we'll use for training a
conditional<00:05:42.850><c> random</c><00:05:42.970><c> field</c>

