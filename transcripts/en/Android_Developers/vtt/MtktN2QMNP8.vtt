WEBVTT
Kind: captions
Language: en

00:00:00.600 --> 00:00:02.683
SARAH KARAM: So we are
going to get some chairs up

00:00:02.683 --> 00:00:06.030
on stage, along with
our wonderful panelists.

00:00:06.030 --> 00:00:09.109
While we're doing that,
while we are getting set up,

00:00:09.109 --> 00:00:11.150
we are going to talk about
building with empathy.

00:00:11.150 --> 00:00:12.890
I would imagine not
many of you guys

00:00:12.890 --> 00:00:16.304
talk about this on a daily
basis or in your 2018 planning

00:00:16.304 --> 00:00:17.220
or anything like that.

00:00:17.220 --> 00:00:19.100
So I thought, it's been
a couple of minutes.

00:00:19.100 --> 00:00:21.560
Just teeing up what that means.

00:00:21.560 --> 00:00:24.110
And some perspectives on what
we're going to talk about.

00:00:24.110 --> 00:00:25.450
Thank you.

00:00:25.450 --> 00:00:27.500
Thanks.

00:00:27.500 --> 00:00:30.500
So empathy, the ability
to understand and share

00:00:30.500 --> 00:00:32.509
the feelings of another.

00:00:32.509 --> 00:00:34.610
The American Association
of Psychology

00:00:34.610 --> 00:00:38.270
proposes it is the defining
trait that makes us human.

00:00:38.270 --> 00:00:41.390
And the absence of
which enables things

00:00:41.390 --> 00:00:44.510
like oppression, persecution,
violence, warfare,

00:00:44.510 --> 00:00:46.560
all the rotten things in life.

00:00:46.560 --> 00:00:51.410
So I think on a personal
level, on a one-to-one basis,

00:00:51.410 --> 00:00:54.290
we all agree that it is an
incredibly important quality

00:00:54.290 --> 00:00:57.560
and something that I hope we
all try to exude in daily lives.

00:00:57.560 --> 00:00:59.370
But what does it mean
on a business level?

00:00:59.370 --> 00:01:01.286
What does it mean if
you're building products,

00:01:01.286 --> 00:01:02.550
making decisions every day?

00:01:02.550 --> 00:01:03.980
Can you do that with empathy?

00:01:03.980 --> 00:01:05.199
And does it matter?

00:01:05.199 --> 00:01:05.990
Should you do that?

00:01:05.990 --> 00:01:07.970
What are the costs
of not doing that?

00:01:07.970 --> 00:01:09.612
Or doing that?

00:01:09.612 --> 00:01:11.320
And there are really
two key perspectives

00:01:11.320 --> 00:01:12.153
to think about this.

00:01:12.153 --> 00:01:14.750
One is, as the business
decision-maker, or the product

00:01:14.750 --> 00:01:18.319
decision-maker, can
you be empathy-driven?

00:01:18.319 --> 00:01:20.360
You know, we talk about
data-driven all the time.

00:01:20.360 --> 00:01:22.400
And we use it as
a positive trait.

00:01:22.400 --> 00:01:24.890
You know, he's data-driven
and he's amazing.

00:01:24.890 --> 00:01:27.470
But we rarely talk about being
empathy-driven and thinking

00:01:27.470 --> 00:01:29.300
about that in your
decision-making.

00:01:29.300 --> 00:01:32.210
And an example that came to
mind when thinking about this

00:01:32.210 --> 00:01:33.125
was Wonder Bread.

00:01:33.125 --> 00:01:34.250
And I don't know how many--

00:01:34.250 --> 00:01:35.375
I didn't grow up in the US.

00:01:35.375 --> 00:01:37.940
So for me, Wonder Bread
was kind of a revelation

00:01:37.940 --> 00:01:38.730
when I moved here.

00:01:38.730 --> 00:01:39.604
I've never tasted it.

00:01:39.604 --> 00:01:43.470
But they pioneered sliced
bread in the 1920s.

00:01:43.470 --> 00:01:46.310
You know, they were kind of seen
as a huge innovator in the food

00:01:46.310 --> 00:01:47.610
industry.

00:01:47.610 --> 00:01:51.260
The term, "the best
thing since sliced bread"

00:01:51.260 --> 00:01:52.730
from Wonder Bread.

00:01:52.730 --> 00:01:55.760
They saw huge success
in the 1950s and '60s.

00:01:55.760 --> 00:01:59.630
About 25% to 30% of
American caloric intake

00:01:59.630 --> 00:02:01.250
was Wonder Bread.

00:02:01.250 --> 00:02:04.410
Nothing has been as
prolific since then.

00:02:04.410 --> 00:02:08.060
Not Coca-Cola, not high
fructose corn syrup.

00:02:08.060 --> 00:02:11.630
In 2012, they filed
for bankruptcy.

00:02:11.630 --> 00:02:13.600
And you know, obviously
many reasons for that.

00:02:13.600 --> 00:02:16.130
But one of the main
reasons that is suggested

00:02:16.130 --> 00:02:18.620
is the rise of health
food, organic food,

00:02:18.620 --> 00:02:20.450
as [INAUDIBLE] was mentioning.

00:02:20.450 --> 00:02:22.045
Greater awareness.

00:02:22.045 --> 00:02:23.420
And if you were
a product manager

00:02:23.420 --> 00:02:26.270
at Wonder Bread in the
1950s, and you proposed

00:02:26.270 --> 00:02:30.470
to do an A/B test to
sell whole wheat bread,

00:02:30.470 --> 00:02:32.810
I wonder how many of us
would have been successful

00:02:32.810 --> 00:02:34.040
with the A/B test.

00:02:34.040 --> 00:02:36.420
Or whether the leadership of
the company would have been,

00:02:36.420 --> 00:02:37.400
ah, it is a great idea.

00:02:37.400 --> 00:02:38.360
Go ahead with that.

00:02:38.360 --> 00:02:42.350
And I think what is interesting
is that often there can

00:02:42.350 --> 00:02:45.050
be a trade-off potentially
between performance

00:02:45.050 --> 00:02:47.270
and building a better
experience, a more enriching

00:02:47.270 --> 00:02:48.360
experience for your users.

00:02:48.360 --> 00:02:50.210
And how do you
make that trade-off

00:02:50.210 --> 00:02:50.960
and how do you
even know if you're

00:02:50.960 --> 00:02:52.327
going in the right direction?

00:02:52.327 --> 00:02:54.910
And the other perspective, which
is incredibly relevant to you

00:02:54.910 --> 00:03:00.620
all, as technology product
managers, decision-makers,

00:03:00.620 --> 00:03:03.500
is the need to build
products with empathy.

00:03:03.500 --> 00:03:05.215
Now with the rise
of voice search,

00:03:05.215 --> 00:03:07.340
we're seeing over a billion
voice queries on Google

00:03:07.340 --> 00:03:11.254
now per week, with
assistance and AI

00:03:11.254 --> 00:03:13.670
and new models of engaging
with users and natural language

00:03:13.670 --> 00:03:14.810
processing.

00:03:14.810 --> 00:03:17.990
Users expect your product
to behave like a person.

00:03:17.990 --> 00:03:20.450
And more than that, to
behave like a friend.

00:03:20.450 --> 00:03:23.270
And your friend isn't
always nice to you.

00:03:23.270 --> 00:03:25.940
Your friend is not
robotic, kind of answering

00:03:25.940 --> 00:03:27.590
in binary ones and zeros.

00:03:27.590 --> 00:03:29.540
Your friend understands you.

00:03:29.540 --> 00:03:32.240
And the same-- you
don't expect the same.

00:03:32.240 --> 00:03:34.460
So Esha might have a
friend who she expects

00:03:34.460 --> 00:03:35.090
to behave in a certain way.

00:03:35.090 --> 00:03:37.673
I have a different expectation,
like we saw in the IDEO video.

00:03:37.673 --> 00:03:40.980
So it is a very
challenging problem.

00:03:40.980 --> 00:03:43.310
So with that, I would
love to introduce my three

00:03:43.310 --> 00:03:44.457
wonderful panelists.

00:03:44.457 --> 00:03:46.040
As you guys do quick
intros, I'm going

00:03:46.040 --> 00:03:48.620
to scan through images
of your products.

00:03:48.620 --> 00:03:52.080
So you guys can talk
a bit about that.

00:03:52.080 --> 00:03:52.580
Go ahead.

00:03:52.580 --> 00:03:53.400
ESHA GUPTA: Great.

00:03:53.400 --> 00:03:55.560
Well, thanks for
having us here today.

00:03:55.560 --> 00:03:56.290
Hi, everyone.

00:03:56.290 --> 00:03:57.400
My name is Esha Gupta.

00:03:57.400 --> 00:04:01.170
I am a co-founder and the
director of content at Hooked,

00:04:01.170 --> 00:04:03.780
which is a chat fiction app.

00:04:03.780 --> 00:04:05.820
We currently have
30 million readers,

00:04:05.820 --> 00:04:11.010
and our goal with Hooked is to
redefine the way that people

00:04:11.010 --> 00:04:13.740
read and write and
consume fiction stories

00:04:13.740 --> 00:04:15.689
on their phones.

00:04:15.689 --> 00:04:16.980
SAM MANDEL: Hi, I'm Sam Mandel.

00:04:16.980 --> 00:04:18.245
I'm the CEO of Poncho.

00:04:18.245 --> 00:04:19.620
I'm also a partner
at Beta Works,

00:04:19.620 --> 00:04:21.536
which is the start-up
studio in New York where

00:04:21.536 --> 00:04:23.122
Poncho was created.

00:04:23.122 --> 00:04:25.080
Our goal with Poncho is
to create a new weather

00:04:25.080 --> 00:04:27.510
service aimed at millennials
and younger people.

00:04:27.510 --> 00:04:30.180
But as we thought
about what that meant,

00:04:30.180 --> 00:04:32.730
we decided we wanted
to create kind

00:04:32.730 --> 00:04:34.440
of an experience of
getting the weather

00:04:34.440 --> 00:04:36.540
from a friend, or a
friendly acquaintance,

00:04:36.540 --> 00:04:38.250
or something in that category.

00:04:38.250 --> 00:04:40.900
And we are really
trying to do two things.

00:04:40.900 --> 00:04:43.050
One is figure out
what it means to have

00:04:43.050 --> 00:04:45.019
native content for
the messaging layer,

00:04:45.019 --> 00:04:47.310
because we felt like that is
where Poncho should really

00:04:47.310 --> 00:04:48.090
live.

00:04:48.090 --> 00:04:50.340
And the other is
eventually how to build

00:04:50.340 --> 00:04:54.510
a UI that would work
for AI back ends

00:04:54.510 --> 00:04:56.940
and how we could bring
that sort of friendliness

00:04:56.940 --> 00:05:01.380
on the front end to more
data-driven, and assembling

00:05:01.380 --> 00:05:02.922
sort of a data
package, like weather.

00:05:02.922 --> 00:05:05.421
Now, one of the ways we do that,
we actually have two comedy

00:05:05.421 --> 00:05:06.300
writers on our team.

00:05:06.300 --> 00:05:09.750
And they are comedians by night
and write for Poncho by day.

00:05:09.750 --> 00:05:11.430
And they actually
write the forecast.

00:05:11.430 --> 00:05:12.971
But everything else
we do with Poncho

00:05:12.971 --> 00:05:16.290
is sort of done algorithmically,
both on our apps

00:05:16.290 --> 00:05:17.280
for iOS and Android.

00:05:17.280 --> 00:05:20.427
And also, we have one
of the best chatbots

00:05:20.427 --> 00:05:22.260
out there on Facebook
Messenger and a couple

00:05:22.260 --> 00:05:23.832
of other platforms.

00:05:23.832 --> 00:05:25.790
In fact, we just got our
Webby award yesterday.

00:05:25.790 --> 00:05:27.150
It finally came
in the mail and--

00:05:27.150 --> 00:05:27.600
SARAH KARAM: Congratulations.

00:05:27.600 --> 00:05:29.455
SAM MANDEL: --it said
"World's Best Chatbot" on it,

00:05:29.455 --> 00:05:30.080
so we're proud.

00:05:30.080 --> 00:05:32.917
DANIELLE KRETTEK:
That's awesome.

00:05:32.917 --> 00:05:34.500
I hope you are sharing
it with Poncho.

00:05:34.500 --> 00:05:37.339
SAM MANDEL: Absolutely.

00:05:37.339 --> 00:05:38.630
DANIELLE KRETTEK: I'm Danielle.

00:05:38.630 --> 00:05:40.440
I run Google's Empathy Lab.

00:05:40.440 --> 00:05:43.830
And the way that that kind of
works at a place like Google,

00:05:43.830 --> 00:05:46.470
is I work with our research
and machine intelligence

00:05:46.470 --> 00:05:49.440
gang, sort of on all of the
guts in the intelligence,

00:05:49.440 --> 00:05:51.570
the souls of these things.

00:05:51.570 --> 00:05:54.370
Also with the Assistant gang.

00:05:54.370 --> 00:05:57.761
And then am deeply embedded
in the design organization

00:05:57.761 --> 00:05:58.260
as well.

00:05:58.260 --> 00:06:01.530
Because, you know, everybody
needs some empathy.

00:06:01.530 --> 00:06:05.640
So basically, I work
across all of those teams,

00:06:05.640 --> 00:06:08.790
and connecting those
teams, kind of as the glue.

00:06:08.790 --> 00:06:10.740
And a way that I often
talk about my work

00:06:10.740 --> 00:06:14.130
is, Google is incredible at IQ.

00:06:14.130 --> 00:06:17.670
And what we are really
starting to build and develop

00:06:17.670 --> 00:06:18.840
at this point is our EQ.

00:06:18.840 --> 00:06:22.170
Because you can imagine, if
our EQ was equal to our IQ,

00:06:22.170 --> 00:06:24.700
how compelling the space is.

00:06:24.700 --> 00:06:26.800
And I think, for this
new design paradigm,

00:06:26.800 --> 00:06:30.270
which is about sort of
interaction relationship,

00:06:30.270 --> 00:06:33.270
it's like-- it is an imperative
now, rather than something

00:06:33.270 --> 00:06:35.730
that's just like oh, its
this, like, squishy layer.

00:06:35.730 --> 00:06:37.860
Or sort of like
design once was--

00:06:37.860 --> 00:06:40.670
this great thing that some
people got and other people

00:06:40.670 --> 00:06:41.170
didn't.

00:06:41.170 --> 00:06:42.960
It's like, we're at
that stage with this.

00:06:42.960 --> 00:06:44.190
It's like, are we
going to acknowledge

00:06:44.190 --> 00:06:45.400
what's really happening here?

00:06:45.400 --> 00:06:45.900
So--

00:06:45.900 --> 00:06:46.860
SARAH KARAM: Awesome.

00:06:46.860 --> 00:06:49.369
Well, thank you all so
much for being here.

00:06:49.369 --> 00:06:50.910
Esha, I would love
to start with you.

00:06:50.910 --> 00:06:53.850
You know, Hooked, given
its focus on chat stories--

00:06:53.850 --> 00:06:56.640
you know, stories are inherently
kind of empathetic and all

00:06:56.640 --> 00:06:58.020
about relating to one another.

00:06:58.020 --> 00:07:00.054
Was building your products
with that in mind,

00:07:00.054 --> 00:07:01.470
and building kind
of a personality

00:07:01.470 --> 00:07:04.350
into your product,
an obvious endeavor?

00:07:04.350 --> 00:07:07.409
Or was it something that
kind of had to come about,

00:07:07.409 --> 00:07:09.450
and you had to kind of
make big product decisions

00:07:09.450 --> 00:07:12.120
to decide how to do
that within the UI?

00:07:12.120 --> 00:07:14.470
ESHA GUPTA: Yeah,
that's a great question.

00:07:14.470 --> 00:07:16.020
So I think the
short answer to that

00:07:16.020 --> 00:07:19.860
is, there wasn't a lot
of conscious discussion

00:07:19.860 --> 00:07:23.580
or decision around building
a personality into the app.

00:07:23.580 --> 00:07:27.600
I think we think about it more
in terms of, what is, I guess,

00:07:27.600 --> 00:07:30.315
the personality of the
main character of a story.

00:07:30.315 --> 00:07:32.190
And how does that speak
to the audience we're

00:07:32.190 --> 00:07:34.920
trying to target that story to?

00:07:34.920 --> 00:07:37.875
But what I will say, and I just
want to circle back and kind

00:07:37.875 --> 00:07:39.750
of piggyback off of the
Wonder Bread analogy,

00:07:39.750 --> 00:07:41.970
because I just
think it's so good.

00:07:41.970 --> 00:07:44.730
And it made me, as you were
saying it, think about,

00:07:44.730 --> 00:07:47.190
maybe we are kind of
experiencing like our Wonder

00:07:47.190 --> 00:07:50.370
Bread moment with mobile.

00:07:50.370 --> 00:07:53.280
And what I mean
by that is, we've

00:07:53.280 --> 00:07:55.110
sort of had this
first slew of apps

00:07:55.110 --> 00:07:57.720
that have come out that
are really addicting

00:07:57.720 --> 00:08:00.180
and we're using them
constantly and they are really

00:08:00.180 --> 00:08:02.250
entertaining, but
perhaps they leave

00:08:02.250 --> 00:08:06.360
us feeling a little bit
dissatisfied and kind

00:08:06.360 --> 00:08:08.010
of unhappy at times.

00:08:08.010 --> 00:08:10.387
You know, we probably--
we're at peak social.

00:08:10.387 --> 00:08:12.720
We're all maybe spending a
lot more time on social media

00:08:12.720 --> 00:08:14.220
than we like.

00:08:14.220 --> 00:08:16.590
And that experience,
I think, sometimes

00:08:16.590 --> 00:08:18.450
leaves us feeling just--

00:08:18.450 --> 00:08:20.230
like there's a
lack of something.

00:08:20.230 --> 00:08:22.890
And so, in many ways,
that is the reason

00:08:22.890 --> 00:08:25.150
why we started Hooked.

00:08:25.150 --> 00:08:29.940
We felt like we wanted to be
kind of a whole-wheat option.

00:08:29.940 --> 00:08:33.419
And you know, there
isn't sort of a lot of--

00:08:33.419 --> 00:08:35.010
there isn't a lot
of options out there

00:08:35.010 --> 00:08:37.770
that feel like a nourishing
alternative to what

00:08:37.770 --> 00:08:38.760
is out there.

00:08:38.760 --> 00:08:41.190
And so yeah, so I think
with that in mind,

00:08:41.190 --> 00:08:43.830
we thought, OK, well,
there are many studies that

00:08:43.830 --> 00:08:46.320
show that there is a high
correlation between the amount

00:08:46.320 --> 00:08:51.810
of fiction a person consumes and
their ability to score highly

00:08:51.810 --> 00:08:54.990
or well on tests that measure
empathy and social skills,

00:08:54.990 --> 00:08:56.020
and even happiness.

00:08:56.020 --> 00:08:58.770
And so you take that
idea in and of itself,

00:08:58.770 --> 00:09:01.640
plus just the team's
personal experience

00:09:01.640 --> 00:09:04.980
of reading and feeling like
you're sort of building empathy

00:09:04.980 --> 00:09:06.460
in this meditative state.

00:09:06.460 --> 00:09:09.870
So how do you take those
facts and experiences

00:09:09.870 --> 00:09:13.560
and translate them into
an experience for mobile,

00:09:13.560 --> 00:09:14.200
basically?

00:09:14.200 --> 00:09:16.950
And so that's kind of like
where our whole journey started.

00:09:16.950 --> 00:09:20.040
And I think, from
our perspective, too,

00:09:20.040 --> 00:09:22.124
you can't really expect
us to put down our phones.

00:09:22.124 --> 00:09:23.623
I don't think that
we are ever going

00:09:23.623 --> 00:09:25.530
to ever reach a point
where we are going

00:09:25.530 --> 00:09:27.870
to live in a digital detox.

00:09:27.870 --> 00:09:30.540
And so, that said, it
is sort of like, OK,

00:09:30.540 --> 00:09:33.120
how do you start creating
those more nourishing,

00:09:33.120 --> 00:09:36.150
like, whole-wheat options
that still taste really good

00:09:36.150 --> 00:09:37.950
and are really,
really addicting,

00:09:37.950 --> 00:09:39.630
but also leave
you feeling better

00:09:39.630 --> 00:09:41.597
than maybe the Wonder
Breads of the world?

00:09:41.597 --> 00:09:43.680
SARAH KARAM: I like that
you carried that through.

00:09:43.680 --> 00:09:45.346
Maybe we'll just have
goggles, and then,

00:09:45.346 --> 00:09:46.530
who needs to hold anything?

00:09:46.530 --> 00:09:48.000
It will all be
right in your face.

00:09:48.000 --> 00:09:50.350
Sam, you are the token
male on the panel.

00:09:50.350 --> 00:09:53.600
So thank you for being that.

00:09:53.600 --> 00:09:56.484
SAM MANDEL: It is great for
a change, I have to say.

00:09:56.484 --> 00:09:57.900
SARAH KARAM: So
you're approaching

00:09:57.900 --> 00:09:58.830
this kind of with two hats.

00:09:58.830 --> 00:10:00.121
You know, Poncho is a start-up.

00:10:00.121 --> 00:10:04.170
And you also advise other
companies as well as start-ups.

00:10:04.170 --> 00:10:06.780
Do you find that given-- we
are all resource-constrained,

00:10:06.780 --> 00:10:09.850
but especially at a start-up,
you've got to make trade-offs.

00:10:09.850 --> 00:10:11.010
Do you find it--

00:10:11.010 --> 00:10:13.551
have you ever had to make those
tough decisions between like,

00:10:13.551 --> 00:10:15.810
well, we need to really grow
our core metric, whatever

00:10:15.810 --> 00:10:17.600
that is right now.

00:10:17.600 --> 00:10:20.650
And it could be at the expense
of this really enriching,

00:10:20.650 --> 00:10:25.020
engaging experience that we
spend all our time building.

00:10:25.020 --> 00:10:26.840
Or you always kind
of know at your heart

00:10:26.840 --> 00:10:29.850
that the Poncho character,
and investing in that,

00:10:29.850 --> 00:10:31.860
and the comedic side
of it, is always

00:10:31.860 --> 00:10:34.465
going to trump more
money, more engagement?

00:10:34.465 --> 00:10:36.840
SAM MANDEL: You know, we are
just reaching that point now

00:10:36.840 --> 00:10:37.570
in our history.

00:10:37.570 --> 00:10:39.180
I think up till
now it's been easy,

00:10:39.180 --> 00:10:42.330
because really engagement
was the core metric we

00:10:42.330 --> 00:10:45.020
used internally, and
engagement and retention.

00:10:45.020 --> 00:10:46.830
And we felt like
that was the best.

00:10:46.830 --> 00:10:49.950
Our goal was to become part
of people's daily routine.

00:10:49.950 --> 00:10:52.890
You know, we see the average
user like twice a day.

00:10:52.890 --> 00:10:58.390
We have, I think, by far the
highest chatbot retention of I

00:10:58.390 --> 00:10:59.730
think any bot.

00:10:59.730 --> 00:11:02.400
And our app retention
is very, very high, too.

00:11:02.400 --> 00:11:04.710
And so we sort of--

00:11:04.710 --> 00:11:06.990
those goals and what we
were measuring sort of

00:11:06.990 --> 00:11:08.640
reinforced this
idea that we wanted

00:11:08.640 --> 00:11:11.850
to be friends with the user,
not abuse that relationship.

00:11:11.850 --> 00:11:15.605
When we send notifications,
we specifically design them

00:11:15.605 --> 00:11:17.190
so you don't have to open them.

00:11:17.190 --> 00:11:20.504
One of the other weather
apps sends one saying, like,

00:11:20.504 --> 00:11:21.670
check out the weather today.

00:11:21.670 --> 00:11:23.220
And we actually give
you the weather,

00:11:23.220 --> 00:11:25.591
so that our use case
paradigm is sort of,

00:11:25.591 --> 00:11:27.090
you're in bed in
the morning, you're

00:11:27.090 --> 00:11:29.910
looking at your texts and
messages from your friends,

00:11:29.910 --> 00:11:32.630
and you have one from Poncho
that tells you the weather.

00:11:32.630 --> 00:11:36.480
So that was-- we didn't
have to really push it.

00:11:36.480 --> 00:11:39.081
Now we're just
starting a project

00:11:39.081 --> 00:11:41.580
where we are going to try and
sell-- have Poncho sell things

00:11:41.580 --> 00:11:42.330
to you.

00:11:42.330 --> 00:11:44.290
And so we really--

00:11:44.290 --> 00:11:46.540
and we know that not all
users are going to like that.

00:11:46.540 --> 00:11:48.750
And we have to figure
out-- we think of Poncho

00:11:48.750 --> 00:11:50.430
as a virtual influencer.

00:11:50.430 --> 00:11:53.490
We want to build him up
as a character who you can

00:11:53.490 --> 00:11:54.760
relate to and buy things from.

00:11:54.760 --> 00:11:57.780
And so the way we've
approached that, we've

00:11:57.780 --> 00:11:59.760
done some experiments,
and there was definitely

00:11:59.760 --> 00:12:01.080
some user pushback.

00:12:01.080 --> 00:12:05.100
We're now going to-- we're now
doing a bigger series of tests,

00:12:05.100 --> 00:12:08.190
basically towards
launching a product that

00:12:08.190 --> 00:12:10.440
is going to have some Poncho
branding on it that we're

00:12:10.440 --> 00:12:11.820
doing in partnership.

00:12:11.820 --> 00:12:15.182
And the first thing we
did was create a backstory

00:12:15.182 --> 00:12:16.140
for why it's happening.

00:12:16.140 --> 00:12:18.930
We have a character bible for
Poncho that we've written,

00:12:18.930 --> 00:12:22.620
and most of that is not
really visible to users.

00:12:22.620 --> 00:12:25.080
Like, I know Poncho's
major in college,

00:12:25.080 --> 00:12:27.120
which was Canadian film.

00:12:27.120 --> 00:12:29.500
But we've never revealed
that to the user.

00:12:29.500 --> 00:12:29.630
SARAH KARAM: Now we all know.

00:12:29.630 --> 00:12:31.457
SAM MANDEL: But it
just sort of helps us--

00:12:31.457 --> 00:12:33.540
it just helps us sort of
get in the mood for like,

00:12:33.540 --> 00:12:34.440
is this something he would do?

00:12:34.440 --> 00:12:35.856
Is this something
he would not do?

00:12:35.856 --> 00:12:39.162
And that has to do with
creating someone who--

00:12:39.162 --> 00:12:41.370
he is sort of a friend, but
we think of him as almost

00:12:41.370 --> 00:12:43.161
between a friend and
acquaintance, sort of.

00:12:43.161 --> 00:12:45.450
Like he's not your
best friend, but he's

00:12:45.450 --> 00:12:46.780
somebody you're friendly with.

00:12:46.780 --> 00:12:50.070
And so we thought, OK, well,
why would he be selling this?

00:12:50.070 --> 00:12:54.510
And now we're going to go
forward and basically do

00:12:54.510 --> 00:12:56.890
a lot of A/B testing
in terms of--

00:12:56.890 --> 00:13:00.210
with subsets of our users,
to see what there is the most

00:13:00.210 --> 00:13:00.934
pushback to.

00:13:00.934 --> 00:13:02.850
But we always want to
be transparent about it.

00:13:02.850 --> 00:13:05.400
And a big part of that
story is going to be,

00:13:05.400 --> 00:13:06.750
like, Poncho's rent went up.

00:13:06.750 --> 00:13:08.550
And you know, he
lives in Brooklyn.

00:13:08.550 --> 00:13:11.250
And he's got to make his rent.

00:13:11.250 --> 00:13:13.770
And his job at Poncho,
Inc. where he works

00:13:13.770 --> 00:13:14.730
is not cutting it.

00:13:14.730 --> 00:13:16.620
So he's got this
kind of side gig.

00:13:16.620 --> 00:13:18.840
And then we're going to
test how much of that

00:13:18.840 --> 00:13:21.010
we expose to the users.

00:13:21.010 --> 00:13:22.470
Our goal is to sell the product.

00:13:22.470 --> 00:13:26.230
It's not necessarily to be
consistent with the story.

00:13:26.230 --> 00:13:29.940
Although the writers and the
whole team, the engineers too,

00:13:29.940 --> 00:13:31.260
we all get into the story.

00:13:31.260 --> 00:13:33.051
And we always have to
fight that temptation

00:13:33.051 --> 00:13:34.570
to surface too much of that.

00:13:34.570 --> 00:13:38.220
And our goal is to
surface just enough of it

00:13:38.220 --> 00:13:41.040
so that we don't
get this pushback.

00:13:41.040 --> 00:13:43.080
But we do think of
ourselves as being

00:13:43.080 --> 00:13:45.390
sort of in a relationship
with the user.

00:13:45.390 --> 00:13:47.760
And we think that that drives
a lot of our retention.

00:13:47.760 --> 00:13:49.890
Because people feel like
if they delete the Poncho

00:13:49.890 --> 00:13:52.730
app or something, they just
feel a little guilty about it,

00:13:52.730 --> 00:13:53.130
is what people have told us.

00:13:53.130 --> 00:13:54.670
SARAH KARAM: "I deleted Poncho."

00:13:54.670 --> 00:13:56.940
SAM MANDEL: And so
we want to approach

00:13:56.940 --> 00:14:00.960
even things that the user--

00:14:00.960 --> 00:14:04.315
might not be their first choice
to see advertising or commerce

00:14:04.315 --> 00:14:05.430
in Poncho.

00:14:05.430 --> 00:14:08.400
But we want to be
transparent in a way that's

00:14:08.400 --> 00:14:10.917
true to the character
about why we're doing that.

00:14:10.917 --> 00:14:12.250
So we'll just see how that goes.

00:14:12.250 --> 00:14:14.580
But we're trying to do
that in the same spirit

00:14:14.580 --> 00:14:16.330
as we've designed the
rest of the service.

00:14:16.330 --> 00:14:18.120
SARAH KARAM: So you
talked about testing.

00:14:18.120 --> 00:14:20.700
And Danielle, I imagine this
must be like a Herculean feat

00:14:20.700 --> 00:14:25.160
at Google, to try to get people
out of the mindset of purely

00:14:25.160 --> 00:14:26.580
data-driven--

00:14:26.580 --> 00:14:30.240
you know, there's 60
whatever here, and 59 here.

00:14:30.240 --> 00:14:32.010
We're going with the 60.

00:14:32.010 --> 00:14:34.830
How have you approached
moving people

00:14:34.830 --> 00:14:37.710
into more of an EQ
measurement mindset?

00:14:37.710 --> 00:14:39.689
And are you doing
that quantifiably?

00:14:39.689 --> 00:14:40.980
Have you found ways to do that?

00:14:40.980 --> 00:14:45.390
Or does it need to be more
qualitative and about CSAT

00:14:45.390 --> 00:14:46.599
and softer metrics?

00:14:46.599 --> 00:14:48.390
DANIELLE KRETTEK: It's
got to be all of it.

00:14:48.390 --> 00:14:53.490
I mean, the thing that kind
of is like the blessing

00:14:53.490 --> 00:14:57.150
and the curse of this space
is that it's invisible.

00:14:57.150 --> 00:14:59.700
We're trying to literally
make things visible

00:14:59.700 --> 00:15:00.867
that are invisible.

00:15:00.867 --> 00:15:02.700
We're also trying to
make things comfortable

00:15:02.700 --> 00:15:03.658
that are uncomfortable.

00:15:03.658 --> 00:15:05.450
Like in terms of
talking about feelings,

00:15:05.450 --> 00:15:08.262
it's like, I literally will just
walk into a room and be like,

00:15:08.262 --> 00:15:09.970
we're going to talk
about feelings today.

00:15:09.970 --> 00:15:13.290
And people are like,
face-melting terror, right?

00:15:13.290 --> 00:15:16.170
So I think it's kind of
just tackling that head on

00:15:16.170 --> 00:15:18.020
and saying, humans are messy.

00:15:18.020 --> 00:15:19.800
There's the rational
and the irrational.

00:15:19.800 --> 00:15:21.070
There's the stuff
that makes sense.

00:15:21.070 --> 00:15:22.528
There's the stuff
you can research,

00:15:22.528 --> 00:15:24.210
the stuff that is
unresearchable.

00:15:24.210 --> 00:15:26.400
The whole point of
talking to people

00:15:26.400 --> 00:15:28.020
and understanding
the full story.

00:15:28.020 --> 00:15:30.911
There was a great line
from a psychologist

00:15:30.911 --> 00:15:32.660
that I work with, who
was basically, like,

00:15:32.660 --> 00:15:35.900
the only way that you can know a
person, and to truly know them,

00:15:35.900 --> 00:15:37.212
is when you know their story.

00:15:37.212 --> 00:15:39.170
So when we look at the
full story, some of that

00:15:39.170 --> 00:15:41.600
is going to be the pattern
recognition in the metrics.

00:15:41.600 --> 00:15:45.170
And some of it is going to be
all of the narrative superglue

00:15:45.170 --> 00:15:48.150
and messiness that kind of
makes them and their life.

00:15:48.150 --> 00:15:52.100
And so what I make a big point
of doing in my conversations

00:15:52.100 --> 00:15:57.320
with our gang at Google is to
quote the Martian and science

00:15:57.320 --> 00:15:59.480
the shit out of it.

00:15:59.480 --> 00:16:04.370
What I tend to do is speak a
lot about the way people feel

00:16:04.370 --> 00:16:05.870
and experience things.

00:16:05.870 --> 00:16:07.310
And I get into a lot of that.

00:16:07.310 --> 00:16:09.680
I make a lot of short
documentary films

00:16:09.680 --> 00:16:12.410
that sort of let people
tell their own stories.

00:16:12.410 --> 00:16:14.480
And really, when you
think about films,

00:16:14.480 --> 00:16:18.074
you're making feelings,
like, in the other person.

00:16:18.074 --> 00:16:20.240
When you're telling a story,
you're making feelings.

00:16:20.240 --> 00:16:24.230
And so what I try to do is have
those stories tell themselves.

00:16:24.230 --> 00:16:26.870
And then there is so much
around emotional intelligence

00:16:26.870 --> 00:16:29.330
and social intelligence,
and you know, the psychology

00:16:29.330 --> 00:16:32.630
of these spaces, that what I
find myself doing is, instead

00:16:32.630 --> 00:16:35.390
of trying to invent
new metrics, is

00:16:35.390 --> 00:16:37.940
to look at the metrics that
are out there around machine

00:16:37.940 --> 00:16:41.070
intelligence and the idea of
learning someone and growing

00:16:41.070 --> 00:16:44.810
with someone and getting to know
them better in a hit or a miss,

00:16:44.810 --> 00:16:48.320
and then fill that in with
things like the Gottmans'

00:16:48.320 --> 00:16:48.830
method.

00:16:48.830 --> 00:16:50.510
So, show of hands
for folks in the room

00:16:50.510 --> 00:16:52.010
that know the Gottmans?

00:16:52.010 --> 00:16:54.560
It means you all probably
have wonderful marriages.

00:16:54.560 --> 00:16:56.459
So the Gottmans got
famous in the '90s

00:16:56.459 --> 00:16:59.000
for-- basically they were the
people where you could sit down

00:16:59.000 --> 00:17:01.400
with them for 20 minutes, and
they would tell you, like,

00:17:01.400 --> 00:17:03.510
how long your marriage
was going to last.

00:17:03.510 --> 00:17:05.000
They had their "Love Lab."

00:17:05.000 --> 00:17:05.630
SARAH KARAM: Like
a fortune teller?

00:17:05.630 --> 00:17:06.369
How would they know?

00:17:06.369 --> 00:17:07.952
DANIELLE KRETTEK:
They basically, they

00:17:07.952 --> 00:17:09.990
were all the kind
of micro-gestures.

00:17:09.990 --> 00:17:11.495
And like all of that stuff.

00:17:11.495 --> 00:17:13.369
And then, also just like
galvanic-- you know,

00:17:13.369 --> 00:17:14.234
all the labby stuff.

00:17:14.234 --> 00:17:16.400
But they could literally
sit with you for 20 minutes

00:17:16.400 --> 00:17:17.690
and be like, you guys
aren't going to make it.

00:17:17.690 --> 00:17:18.814
You'll make it seven years.

00:17:18.814 --> 00:17:20.119
You'll make it 14 years.

00:17:20.119 --> 00:17:22.089
And I mean, can you
imagine the stress?

00:17:22.089 --> 00:17:25.712
Like the worst dinner
party guests ever.

00:17:25.712 --> 00:17:27.500
But what's rad about
what they did is,

00:17:27.500 --> 00:17:28.967
they also became known for--

00:17:28.967 --> 00:17:30.550
like kind of in the
'90s, when divorce

00:17:30.550 --> 00:17:34.400
was hockey sticking-- this
idea of the magic ratio, which

00:17:34.400 --> 00:17:35.420
is five to one.

00:17:35.420 --> 00:17:39.710
And so the way that that
works is, for every five

00:17:39.710 --> 00:17:41.570
positive interactions
that you have,

00:17:41.570 --> 00:17:43.245
it's OK if there is one bad one.

00:17:43.245 --> 00:17:44.870
And what was interesting
about this is,

00:17:44.870 --> 00:17:47.565
it was like basically
agnostic to coupling style.

00:17:47.565 --> 00:17:49.940
People were like, oh, my God,
if we like, you know, fight

00:17:49.940 --> 00:17:51.470
all the time, but we
make up all the time,

00:17:51.470 --> 00:17:53.240
is that better than if we
are like conflict diverse?

00:17:53.240 --> 00:17:53.865
Or is it worse?

00:17:53.865 --> 00:17:55.239
Or like, who is
going to make it?

00:17:55.239 --> 00:17:56.150
It was like the fear.

00:17:56.150 --> 00:17:58.447
Like people care about
their relationships.

00:17:58.447 --> 00:18:00.030
And they were like,
it doesn't matter.

00:18:00.030 --> 00:18:01.360
What matters is the ratio.

00:18:01.360 --> 00:18:04.190
It's like, people have
used like the piggy bank.

00:18:04.190 --> 00:18:05.690
You know, putting
points in the bank

00:18:05.690 --> 00:18:07.524
and all that stuff--
other metaphors for it.

00:18:07.524 --> 00:18:09.231
But I think what's
interesting about that

00:18:09.231 --> 00:18:11.150
is, I've run a lot of
research saying, well,

00:18:11.150 --> 00:18:12.980
if we're in a
paradigm where this

00:18:12.980 --> 00:18:15.800
is the interface, and
its relationship-driven,

00:18:15.800 --> 00:18:18.350
then the rules for
relationships and what

00:18:18.350 --> 00:18:20.330
works should borrow over here.

00:18:20.330 --> 00:18:22.857
And what I found is that
five-to-one metric bears out.

00:18:22.857 --> 00:18:24.440
So when you're
designing the research,

00:18:24.440 --> 00:18:26.440
how are you asking the
right narrative questions

00:18:26.440 --> 00:18:27.492
to get at the truth.

00:18:27.492 --> 00:18:29.450
I mean, like I immediately
am like, oh, my God,

00:18:29.450 --> 00:18:31.100
I wonder if people
with Poncho are

00:18:31.100 --> 00:18:34.557
going to be, like, how is
the side hustle going, man?

00:18:34.557 --> 00:18:35.390
Like in the morning.

00:18:35.390 --> 00:18:36.710
Like if I were to
get a text from him,

00:18:36.710 --> 00:18:38.160
I would be like, how's it going?

00:18:38.160 --> 00:18:39.590
And these types of things.

00:18:39.590 --> 00:18:43.730
How do you design for that messy
space and at the same time,

00:18:43.730 --> 00:18:48.290
be looking at the really
core architectures of what

00:18:48.290 --> 00:18:49.920
feels right in a relationship?

00:18:49.920 --> 00:18:52.976
The five-to-one is just,
like, when it feels right.

00:18:52.976 --> 00:18:54.350
I feel like I'm
constantly trying

00:18:54.350 --> 00:18:57.200
to find ways to
put pins on things

00:18:57.200 --> 00:19:00.029
that are the most uncomfortable,
weird parts of being human.

00:19:00.029 --> 00:19:01.570
SAM MANDEL: You
know, can I just say,

00:19:01.570 --> 00:19:04.070
when we designed
the Poncho bot, I'd

00:19:04.070 --> 00:19:05.930
never heard of that
five-to-one ratio.

00:19:05.930 --> 00:19:07.510
I'm sure none of
the team knows it.

00:19:07.510 --> 00:19:09.980
But we just decided that
we had to, basically,

00:19:09.980 --> 00:19:13.040
if you asked the Poncho bot
a question about the weather,

00:19:13.040 --> 00:19:15.430
we had to understand
it 80% of the time.

00:19:15.430 --> 00:19:18.270
And if we couldn't do that,
then we weren't succeeding.

00:19:18.270 --> 00:19:20.270
And so, it's kind of funny
that that-- we just--

00:19:20.270 --> 00:19:21.840
but we, as a collective,
we sat around,

00:19:21.840 --> 00:19:24.140
we were like, that's the
metric that we have to go for.

00:19:24.140 --> 00:19:25.070
DANIELLE KRETTEK: That's
a good point, too,

00:19:25.070 --> 00:19:27.200
because I think you guys
intuitively got there.

00:19:27.200 --> 00:19:31.110
And that's kind of the
head bleep of the space.

00:19:31.110 --> 00:19:36.140
Is if you really check
in, if you drop in, you--

00:19:36.140 --> 00:19:38.319
we all know the answers
of what feels good,

00:19:38.319 --> 00:19:40.610
what doesn't feel good, what
we believe, what we don't.

00:19:40.610 --> 00:19:42.020
What story, like, actually--

00:19:42.020 --> 00:19:44.840
I mean, literally using your
app, I was like, oh, my God,

00:19:44.840 --> 00:19:46.370
it's not loading fast enough.

00:19:46.370 --> 00:19:49.130
Like, why doesn't this
story text me faster?

00:19:49.130 --> 00:19:51.920
And that sense of
when you're in it,

00:19:51.920 --> 00:19:53.240
versus when you're out of it.

00:19:53.240 --> 00:19:56.330
And I think it's, how do we
find metrics that actually match

00:19:56.330 --> 00:19:59.750
that, versus being
driven by what's

00:19:59.750 --> 00:20:01.860
the actual best
metric for this thing.

00:20:01.860 --> 00:20:04.130
We can kind of get
lost in our heads.

00:20:04.130 --> 00:20:06.470
Like a big thing I talk
about is how we often

00:20:06.470 --> 00:20:08.720
think that we're designing
for people's modes,

00:20:08.720 --> 00:20:12.860
like airplane mode, this, that,
what game mode, stress mode.

00:20:12.860 --> 00:20:15.080
I think what's funny
is, people when they--

00:20:15.080 --> 00:20:17.804
like a research project I
ran that you know about was,

00:20:17.804 --> 00:20:19.220
I was, like, OK,
it's really weird

00:20:19.220 --> 00:20:21.470
to ask people what their
ideal flow of information

00:20:21.470 --> 00:20:23.870
is for notifications and
this, that, and the other.

00:20:23.870 --> 00:20:26.480
And people were kind of
asking me about the tray.

00:20:26.480 --> 00:20:28.670
And I was like, OK,
forget the tray.

00:20:28.670 --> 00:20:30.440
Like, I went to people
and I was like, you

00:20:30.440 --> 00:20:31.890
know what airplane mode is.

00:20:31.890 --> 00:20:33.180
Give me like 10 more.

00:20:33.180 --> 00:20:35.427
And all of their
modes were moods.

00:20:35.427 --> 00:20:36.510
They weren't modes at all.

00:20:36.510 --> 00:20:39.330
So just that, like, little- the
extra "o" makes a difference.

00:20:39.330 --> 00:20:42.000
I think intuitively being
like, oh, I think that's right.

00:20:42.000 --> 00:20:44.230
How do I make a metric for that?

00:20:44.230 --> 00:20:46.865
SARAH KARAM: You talked
about kind of maybe mimicking

00:20:46.865 --> 00:20:48.240
a great relationship
where you've

00:20:48.240 --> 00:20:51.430
got kind of this good ratio
of positive and negative.

00:20:51.430 --> 00:20:54.740
But in that paradigm, you as
a person in that relationship,

00:20:54.740 --> 00:20:55.934
you are yourself.

00:20:55.934 --> 00:20:56.850
You are not a machine.

00:20:56.850 --> 00:20:58.433
You can't switch who
you are, and have

00:20:58.433 --> 00:21:00.510
an equally effective
relationship with somebody

00:21:00.510 --> 00:21:01.010
else.

00:21:01.010 --> 00:21:03.140
You can do that in
your product, right?

00:21:03.140 --> 00:21:05.640
So you can be a different
person to one user

00:21:05.640 --> 00:21:07.290
and another to another user.

00:21:07.290 --> 00:21:09.927
Is that-- I guess, how do
you think about that when

00:21:09.927 --> 00:21:12.510
you're building-- and you know,
for others in the audience who

00:21:12.510 --> 00:21:15.960
are either building your own
natural language interface,

00:21:15.960 --> 00:21:18.910
or you are just kind of thinking
about your brand personality--

00:21:18.910 --> 00:21:20.850
do you stay as one brand?

00:21:20.850 --> 00:21:23.094
Or do you mimic
whatever the user wants?

00:21:23.094 --> 00:21:24.510
And what are the
trade-offs there?

00:21:24.510 --> 00:21:27.422
And how do you guys
think about that?

00:21:27.422 --> 00:21:29.130
ESHA GUPTA: Yeah,
well, I'll jump in here

00:21:29.130 --> 00:21:31.770
and say that, one
interesting thing that

00:21:31.770 --> 00:21:34.770
happened in the past year
with Hooked that we noticed

00:21:34.770 --> 00:21:37.920
is that, when we first started
collecting stories for the app,

00:21:37.920 --> 00:21:40.440
our main goal was to get stories
that were super addicting.

00:21:40.440 --> 00:21:43.500
Because we're like, OK, goal
is to get people to read.

00:21:43.500 --> 00:21:44.400
How do you do that?

00:21:44.400 --> 00:21:46.530
You need like
Buzzfeed of fiction.

00:21:46.530 --> 00:21:48.030
You need it to be
super addicting.

00:21:48.030 --> 00:21:49.965
People want to tap,
get to the end.

00:21:49.965 --> 00:21:51.340
And so when we
first started out,

00:21:51.340 --> 00:21:54.115
we incentivized authors
for that particular metric.

00:21:54.115 --> 00:21:55.740
The metric of success
we were measuring

00:21:55.740 --> 00:21:57.910
was completion rate of stories.

00:21:57.910 --> 00:21:59.730
And so we collected
a bunch of stories

00:21:59.730 --> 00:22:02.760
that had really high
engagement, and we

00:22:02.760 --> 00:22:04.170
thought all was going well.

00:22:04.170 --> 00:22:06.860
And then we ran
sentiment analysis

00:22:06.860 --> 00:22:08.610
after we had a good
collection of stories,

00:22:08.610 --> 00:22:12.420
I think about, like,
200, 250, on the comments

00:22:12.420 --> 00:22:15.600
and the likes-to-loves
ratios and like,

00:22:15.600 --> 00:22:18.030
the positive to negative
words in the comments.

00:22:18.030 --> 00:22:21.330
We just ran a huge
analysis of, can we

00:22:21.330 --> 00:22:24.330
gauge what people actually
feel about this story?

00:22:24.330 --> 00:22:27.750
And what we learned
is that people

00:22:27.750 --> 00:22:30.170
hated cliffhanger endings.

00:22:30.170 --> 00:22:33.240
They were so angry that they
would get to the end of a story

00:22:33.240 --> 00:22:34.860
and it wouldn't
have this resolute,

00:22:34.860 --> 00:22:36.510
kind of satisfying ending.

00:22:36.510 --> 00:22:38.310
And they felt almost betrayed.

00:22:38.310 --> 00:22:41.387
And so what we saw is
that, well, first of all,

00:22:41.387 --> 00:22:43.595
it's really hard to write
a good ending to any story.

00:22:43.595 --> 00:22:45.750
And that's just like a challenge
of writing a good fiction

00:22:45.750 --> 00:22:46.500
story.

00:22:46.500 --> 00:22:47.970
But it's particularly
challenging

00:22:47.970 --> 00:22:52.029
in a compressed format with the
goal of being super engaging

00:22:52.029 --> 00:22:53.320
in such a short amount of time.

00:22:53.320 --> 00:22:56.052
And so we kind of went
back to the drawing board.

00:22:56.052 --> 00:22:57.510
And we're like,
OK, this is clearly

00:22:57.510 --> 00:23:00.540
important from an emotional
perspective for our readers.

00:23:00.540 --> 00:23:03.030
And so we changed our
incentive structure

00:23:03.030 --> 00:23:05.670
and asked authors to
revisit their stories,

00:23:05.670 --> 00:23:07.170
and then moving
forward, like really

00:23:07.170 --> 00:23:09.810
invest in thinking about
how to write endings

00:23:09.810 --> 00:23:11.220
with more closure.

00:23:11.220 --> 00:23:13.620
And then we re-ran the
sentiment analysis,

00:23:13.620 --> 00:23:16.630
and user satisfaction
just shot way up.

00:23:16.630 --> 00:23:18.570
And so I use incentives
as an example

00:23:18.570 --> 00:23:21.804
because I think it wasn't,
though in retrospect it's

00:23:21.804 --> 00:23:23.970
obvious, it wasn't something
we were really thinking

00:23:23.970 --> 00:23:25.350
about in the beginning.

00:23:25.350 --> 00:23:28.860
And this was something we
heard from our readers,

00:23:28.860 --> 00:23:31.270
like across the board,
that they wanted.

00:23:31.270 --> 00:23:34.350
And so we did sort of maneuver
ourselves to cater to that.

00:23:34.350 --> 00:23:37.560
Because what it taught us
is that measuring based

00:23:37.560 --> 00:23:40.240
on just one metric,
engagement, isn't enough.

00:23:40.240 --> 00:23:42.900
We have to always ask ourselves
with every story, what

00:23:42.900 --> 00:23:45.840
is the emotional experience
that this person is potentially

00:23:45.840 --> 00:23:47.550
having with this
piece as a whole?

00:23:47.550 --> 00:23:48.780
Is it good or is it bad?

00:23:48.780 --> 00:23:52.522
And if it's bad, we need to
figure out how to make it good.

00:23:52.522 --> 00:23:53.730
SARAH KARAM: Yeah, I imagine.

00:23:53.730 --> 00:23:57.640
I don't know if any media
producers are in the room.

00:23:57.640 --> 00:24:00.030
And Netflix, I
have a bone to pick

00:24:00.030 --> 00:24:01.560
with every original
content piece.

00:24:01.560 --> 00:24:03.450
Because everything
is a cliffhanger.

00:24:03.450 --> 00:24:05.370
And it forces you
to keep staying

00:24:05.370 --> 00:24:07.350
addicted to a specific show.

00:24:07.350 --> 00:24:09.840
But I mean, I think it
goes back to whether--

00:24:09.840 --> 00:24:13.730
users are not necessarily
always having a positive feeling

00:24:13.730 --> 00:24:15.480
after their interaction
with your product.

00:24:15.480 --> 00:24:17.944
And is that-- do you always
want to make the user happy?

00:24:17.944 --> 00:24:19.360
I know that is a
strange question.

00:24:19.360 --> 00:24:21.810
But in real life,
the people who you

00:24:21.810 --> 00:24:24.540
want to be around all
the time don't always

00:24:24.540 --> 00:24:28.800
kind of placate to your every
need, to keep you smiling.

00:24:28.800 --> 00:24:31.305
I think in the IDEO study,
there was an example of somebody

00:24:31.305 --> 00:24:33.930
who said they wanted somebody to
challenge them, or kind of not

00:24:33.930 --> 00:24:37.020
let them get away with
doing whatever they want.

00:24:37.020 --> 00:24:39.510
Is that something
that you can build in?

00:24:39.510 --> 00:24:41.389
And should build into products?

00:24:41.389 --> 00:24:42.930
SAM MANDEL: I don't
know if this is--

00:24:42.930 --> 00:24:45.221
maybe this isn't the exact
same question you're asking.

00:24:45.221 --> 00:24:47.730
But we look at a lot of
the digital assistants.

00:24:47.730 --> 00:24:50.770
Because Poncho is kind of a
digital assistant in some ways.

00:24:50.770 --> 00:24:52.724
And one of the
things we consciously

00:24:52.724 --> 00:24:54.390
decided we didn't
want to do with Poncho

00:24:54.390 --> 00:24:57.090
is make him too accommodating.

00:24:57.090 --> 00:25:00.079
You know, because
I think that one

00:25:00.079 --> 00:25:01.620
of the issues with
digital assistants

00:25:01.620 --> 00:25:03.750
is that they are
pretty subservient.

00:25:03.750 --> 00:25:06.810
And because they all have
female personalities,

00:25:06.810 --> 00:25:08.480
it's sort of like even worse.

00:25:08.480 --> 00:25:10.879
And it just--
there's just some--

00:25:10.879 --> 00:25:12.420
I think they can
get a little creepy.

00:25:12.420 --> 00:25:13.950
And they're also not human.

00:25:13.950 --> 00:25:14.830
Because human--

00:25:14.830 --> 00:25:16.830
SARAH KARAM: Except the
Google Assistant, right?

00:25:16.830 --> 00:25:17.850
SAM MANDEL: The
Google Assistant is--

00:25:17.850 --> 00:25:18.430
SARAH KARAM: I'm just kidding.

00:25:18.430 --> 00:25:20.971
SAM MANDEL: I like the Google
Assistant because it is so dry.

00:25:20.971 --> 00:25:22.130
Like it sort of--

00:25:22.130 --> 00:25:24.990
it feels more robotic
in a good way.

00:25:24.990 --> 00:25:27.430
But like it's not trying
to do anything else.

00:25:27.430 --> 00:25:28.560
But I think that the--

00:25:28.560 --> 00:25:29.190
DANIELLE KRETTEK:
Are you saying this

00:25:29.190 --> 00:25:30.040
because I'm sitting
right next to you?

00:25:30.040 --> 00:25:30.607
[LAUGHTER]

00:25:30.607 --> 00:25:32.440
SAM MANDEL: But I think
the Google Assistant

00:25:32.440 --> 00:25:33.370
has a certain dryness.

00:25:33.370 --> 00:25:34.724
I really do mean that.

00:25:34.724 --> 00:25:35.640
But in a positive way.

00:25:35.640 --> 00:25:39.570
Like it's not pretending
to be a person exactly.

00:25:39.570 --> 00:25:42.750
But I think that with
Poncho, we decided

00:25:42.750 --> 00:25:45.140
like, he's going to be
somebody who you're not--

00:25:45.140 --> 00:25:46.304
he is not your best friend.

00:25:46.304 --> 00:25:47.470
He has his own things to do.

00:25:47.470 --> 00:25:48.928
So even little
messages, sometimes,

00:25:48.928 --> 00:25:51.090
like when you talk to the
chatbot, and he's like,

00:25:51.090 --> 00:25:52.050
hey, I was charging my phone.

00:25:52.050 --> 00:25:52.550
Sorry.

00:25:52.550 --> 00:25:53.925
You know, I didn't
see you there.

00:25:53.925 --> 00:25:54.890
Or something like that.

00:25:54.890 --> 00:25:56.239
SARAH KARAM: But can I ask why?

00:25:56.239 --> 00:25:58.530
Like was that an-- sorry to
interrupt you, but was that

00:25:58.530 --> 00:25:59.424
an obvious choice?

00:25:59.424 --> 00:26:00.840
To think of him
as you didn't want

00:26:00.840 --> 00:26:01.930
him to be your best friend?

00:26:01.930 --> 00:26:03.110
SAM MANDEL: We didn't
want him to be servile.

00:26:03.110 --> 00:26:05.910
We wanted him to be someone who
you wanted to be friends with.

00:26:05.910 --> 00:26:08.160
Like he was a little
cooler than you.

00:26:08.160 --> 00:26:10.830
And you were sort
of trying to like--

00:26:10.830 --> 00:26:13.079
you know, we wanted to
have that relationship,

00:26:13.079 --> 00:26:15.120
because we thought that
was just more intriguing,

00:26:15.120 --> 00:26:17.220
and people would be--

00:26:17.220 --> 00:26:19.590
we always want people to
kind of try to know more.

00:26:19.590 --> 00:26:22.410
Now, that being said,
we want their reaction

00:26:22.410 --> 00:26:24.060
when they've used Poncho.

00:26:24.060 --> 00:26:25.770
Like our user
promise is that sort

00:26:25.770 --> 00:26:27.220
of moment of zen in the morning.

00:26:27.220 --> 00:26:29.200
And you know, we
think about what

00:26:29.200 --> 00:26:31.350
that-- our use case is
basically morning use

00:26:31.350 --> 00:26:32.820
of the average user.

00:26:32.820 --> 00:26:35.100
We have like two
sessions a day per user.

00:26:35.100 --> 00:26:38.100
But we think about that morning
use case as fundamental.

00:26:38.100 --> 00:26:40.560
And we want-- especially
now, like, things have just

00:26:40.560 --> 00:26:43.200
gotten worse, like you
dread opening your phone.

00:26:43.200 --> 00:26:45.150
You know, who knows
what is in the news?

00:26:45.150 --> 00:26:48.082
We just want that
moment to leave you

00:26:48.082 --> 00:26:49.290
with a little bit of a smile.

00:26:49.290 --> 00:26:51.270
And that's kind of
our brand promise.

00:26:51.270 --> 00:26:56.190
So if we don't succeed
there, then we're

00:26:56.190 --> 00:26:58.399
not delivering what
we've promised to do.

00:26:58.399 --> 00:26:59.940
DANIELLE KRETTEK:
I think another way

00:26:59.940 --> 00:27:02.550
to answer your
question, too, is about,

00:27:02.550 --> 00:27:05.820
like, the identity
being elastic.

00:27:05.820 --> 00:27:07.710
Having a sense of
elasticity to it,

00:27:07.710 --> 00:27:10.290
so that it can change over
time in a narrative arc,

00:27:10.290 --> 00:27:13.170
so that it can change in
skills that are added.

00:27:13.170 --> 00:27:16.440
Or things like
Poncho's side hustle.

00:27:16.440 --> 00:27:21.330
I think the thing that we know
to be most true about humans

00:27:21.330 --> 00:27:22.320
is that we change.

00:27:22.320 --> 00:27:23.460
And we're never one thing.

00:27:23.460 --> 00:27:25.710
And just when we think
we've figured ourselves out,

00:27:25.710 --> 00:27:26.730
we're kind of not that.

00:27:26.730 --> 00:27:29.145
Like learning and growing
and figuring it out

00:27:29.145 --> 00:27:31.270
and not feeling like you
have anything figured out.

00:27:31.270 --> 00:27:33.060
And like the human
experience being

00:27:33.060 --> 00:27:35.550
like this, whether you
screenshot it as a day,

00:27:35.550 --> 00:27:37.740
or whether you look
at it as a movie arc.

00:27:37.740 --> 00:27:41.040
I mean, "Boyhood" and
"ET" are two of the--

00:27:41.040 --> 00:27:43.230
like I have two
four-minute edits for both

00:27:43.230 --> 00:27:45.000
of those that I
use constantly when

00:27:45.000 --> 00:27:46.530
I talk about machine learning.

00:27:46.530 --> 00:27:49.020
And the reason why
is that we understand

00:27:49.020 --> 00:27:52.090
growth and change and
variability and personality,

00:27:52.090 --> 00:27:54.090
and when things are great,
and when things suck,

00:27:54.090 --> 00:27:56.520
and the fact that all
of that's welcome.

00:27:56.520 --> 00:27:58.170
We understand that
in the context

00:27:58.170 --> 00:28:00.030
of each other and stories.

00:28:00.030 --> 00:28:03.480
And if you can apply a backstory
and background and narrative.

00:28:03.480 --> 00:28:05.760
And like, tech isn't
perfect, and assistance

00:28:05.760 --> 00:28:06.780
won't be perfect.

00:28:06.780 --> 00:28:08.520
And the weather
is unpredictable.

00:28:08.520 --> 00:28:12.340
And maybe Poncho is mysterious,
or the assistance this or that.

00:28:12.340 --> 00:28:14.460
I think just that sense of--

00:28:14.460 --> 00:28:16.410
the best speakers,
the best films,

00:28:16.410 --> 00:28:19.290
the things that really move
you and you feel connected to,

00:28:19.290 --> 00:28:21.030
that bond is because
they make you feel

00:28:21.030 --> 00:28:22.260
a little bit of everything.

00:28:22.260 --> 00:28:22.620
SARAH KARAM: Yep.

00:28:22.620 --> 00:28:24.060
DANIELLE KRETTEK: And I think
that little bit of everything

00:28:24.060 --> 00:28:26.720
is something that you
design in with a light hand.

00:28:26.720 --> 00:28:29.280
But an assistant
that is a cheerleader

00:28:29.280 --> 00:28:32.784
and is always kind of a positive
thing would drive you crazy.

00:28:32.784 --> 00:28:34.200
It would be like
Reese Witherspoon

00:28:34.200 --> 00:28:35.850
from "Election"
following him around.

00:28:35.850 --> 00:28:37.320
No, right?

00:28:37.320 --> 00:28:41.130
So I think we often want to
go to the positive high-note

00:28:41.130 --> 00:28:42.120
place.

00:28:42.120 --> 00:28:43.329
But that's not where we live.

00:28:43.329 --> 00:28:45.328
So how do we get really
authentic and vulnerable

00:28:45.328 --> 00:28:45.880
about that?

00:28:45.880 --> 00:28:48.570
And like designing for
those spaces with these--

00:28:48.570 --> 00:28:50.850
I mean, the thing about
devices and our experiences

00:28:50.850 --> 00:28:53.640
with these presences
now is it's 24/7.

00:28:53.640 --> 00:28:54.980
The on-off switch is a joke.

00:28:54.980 --> 00:28:55.950
They're everywhere.

00:28:55.950 --> 00:28:58.410
So we need a lot
more granularity

00:28:58.410 --> 00:29:02.312
and gradient in that aspect of
the experience emotionally now.

00:29:02.312 --> 00:29:04.020
SARAH KARAM: We have
a couple of minutes.

00:29:04.020 --> 00:29:07.092
I would love-- I don't know
how many of you in the audience

00:29:07.092 --> 00:29:08.550
do a lot of thinking
in this space,

00:29:08.550 --> 00:29:11.490
or if you already have kind
of ways to measure longer term

00:29:11.490 --> 00:29:14.380
empathy, emotion, all that.

00:29:14.380 --> 00:29:17.179
If not, you know, what would
you guys give as advice?

00:29:17.179 --> 00:29:19.470
You know, as you guys have
started thinking about this,

00:29:19.470 --> 00:29:21.136
if you were doing it
again from scratch,

00:29:21.136 --> 00:29:23.520
or just starting this, what
are some things that you

00:29:23.520 --> 00:29:25.395
would recommend to
anyone in that position?

00:29:28.242 --> 00:29:30.569
ESHA GUPTA: I guess--

00:29:30.569 --> 00:29:32.110
this is maybe going
to sound generic.

00:29:32.110 --> 00:29:34.480
We as a team,
internally, often are

00:29:34.480 --> 00:29:36.550
debating which direction to go.

00:29:36.550 --> 00:29:38.500
It's usually a 50/50 split.

00:29:38.500 --> 00:29:40.120
And we will have
emotional reactions

00:29:40.120 --> 00:29:42.190
to what we think we should do.

00:29:42.190 --> 00:29:44.020
But then we always just
kind of settle back

00:29:44.020 --> 00:29:47.800
into just testing it, and
seeing what the results are.

00:29:47.800 --> 00:29:51.580
But when they are close,
taking it with a grain of salt.

00:29:51.580 --> 00:29:54.100
And so like for the
ending of stories,

00:29:54.100 --> 00:29:56.687
ultimately, financially,
it wasn't a great decision

00:29:56.687 --> 00:29:58.270
because we had to
pay our authors more

00:29:58.270 --> 00:30:00.160
for the same amount of stories.

00:30:00.160 --> 00:30:03.460
But we decided
that, kind of based

00:30:03.460 --> 00:30:05.142
on just like the
negativity surrounding

00:30:05.142 --> 00:30:06.850
the emotional experience
for our readers,

00:30:06.850 --> 00:30:08.650
that it might be
something worth doing.

00:30:08.650 --> 00:30:12.370
And even though, maybe,
there isn't an actual data

00:30:12.370 --> 00:30:14.740
point around how it's
increased our revenue

00:30:14.740 --> 00:30:19.180
from that perspective, the
point is, users seem happier.

00:30:19.180 --> 00:30:21.370
And so therefore,
for us, it's a win.

00:30:21.370 --> 00:30:22.609
And so, I think--

00:30:22.609 --> 00:30:23.150
I don't know.

00:30:23.150 --> 00:30:26.200
There's not like-- emotions
are, like Danielle is saying,

00:30:26.200 --> 00:30:28.870
you can't measure them with
quite as much granularity.

00:30:28.870 --> 00:30:29.650
And so--

00:30:29.650 --> 00:30:30.290
SARAH KARAM: You
trust your intuition?

00:30:30.290 --> 00:30:31.960
ESHA GUPTA: You just have
to trust your intuition.

00:30:31.960 --> 00:30:33.550
It's like, does this
thing make me feel shitty?

00:30:33.550 --> 00:30:34.758
Or does it make me feel good?

00:30:34.758 --> 00:30:36.926
You know, and if the
answer is pretty clear,

00:30:36.926 --> 00:30:39.300
I feel like that should be
the thing that helps you point

00:30:39.300 --> 00:30:41.131
your arrow in either direction.

00:30:41.131 --> 00:30:43.630
SARAH KARAM: Sam, Danielle, do
you guys have words of wisdom

00:30:43.630 --> 00:30:45.906
for new entrants to the space?

00:30:45.906 --> 00:30:48.280
DANIELLE KRETTEK: I think,
well, I'd say like two things.

00:30:48.280 --> 00:30:50.779
Like we all know that moment
in the movie where you're like,

00:30:50.779 --> 00:30:52.354
don't go in there.

00:30:52.354 --> 00:30:53.770
There's this thing
where, I think,

00:30:53.770 --> 00:30:57.070
as we're designing stuff, it's
like, when you're following

00:30:57.070 --> 00:31:00.790
your feelings, when you're in
an experience, your intuition,

00:31:00.790 --> 00:31:03.520
whether you're someone that
acknowledges that or not,

00:31:03.520 --> 00:31:04.760
is in play.

00:31:04.760 --> 00:31:06.550
And I think when we
tend to design stuff,

00:31:06.550 --> 00:31:08.080
we look at it within
the container.

00:31:08.080 --> 00:31:10.276
So it's like blow
away the container.

00:31:10.276 --> 00:31:11.650
And then, I think
the other piece

00:31:11.650 --> 00:31:14.470
of that is the
invitation for you,

00:31:14.470 --> 00:31:18.610
in whatever function or
discipline you are in, is to--

00:31:18.610 --> 00:31:21.514
like we really do need to
tap all of our humanity

00:31:21.514 --> 00:31:22.930
as we are working
on these things.

00:31:22.930 --> 00:31:25.850
Like with this
interaction space,

00:31:25.850 --> 00:31:29.760
you're not just the designer,
the researcher, the PM,

00:31:29.760 --> 00:31:30.330
the whatever.

00:31:30.330 --> 00:31:31.831
You've got to bring
the human piece.

00:31:31.831 --> 00:31:33.288
Because it's the
mess that is going

00:31:33.288 --> 00:31:34.870
to make this stuff
believable, that's

00:31:34.870 --> 00:31:36.037
going to make it compelling.

00:31:36.037 --> 00:31:37.453
And it's going to
make it welcome.

00:31:37.453 --> 00:31:39.070
Because I think
the endgame of all

00:31:39.070 --> 00:31:42.187
of this that's potentially
there is, like,

00:31:42.187 --> 00:31:44.020
this world can either
feel really harmonious

00:31:44.020 --> 00:31:46.321
with all of these kind of
helpful agents and friends

00:31:46.321 --> 00:31:46.820
and things.

00:31:46.820 --> 00:31:49.120
It's like your social
circle expands.

00:31:49.120 --> 00:31:51.080
Or it's like total
schizophrenia.

00:31:51.080 --> 00:31:54.800
So that's like where,
in 1987, like Apple

00:31:54.800 --> 00:31:56.000
had their HCI principles.

00:31:56.000 --> 00:31:58.000
And we are, like, here's
how this needs to work.

00:31:58.000 --> 00:32:00.550
And I think we are all
working on that recipe

00:32:00.550 --> 00:32:01.940
together right now.

00:32:01.940 --> 00:32:04.190
SAM MANDEL: I would just say
I have always wanted to--

00:32:04.190 --> 00:32:05.689
I was a philosophy
major in college.

00:32:05.689 --> 00:32:08.140
So I'm going to quote Immanuel
Kant, the great German

00:32:08.140 --> 00:32:09.517
philosopher.

00:32:09.517 --> 00:32:11.600
Probably never been done
in a Google event before.

00:32:11.600 --> 00:32:13.120
But he had this
thing where he said,

00:32:13.120 --> 00:32:15.577
like, you treat
people ethically,

00:32:15.577 --> 00:32:17.410
you should treat them
as ends in themselves,

00:32:17.410 --> 00:32:19.060
not as a means to an end.

00:32:19.060 --> 00:32:21.460
And I do think about that
in terms of like app design

00:32:21.460 --> 00:32:22.770
and product design.

00:32:22.770 --> 00:32:25.600
We shouldn't think
of our users as means

00:32:25.600 --> 00:32:28.560
to our end, where we're trying
to drive certain metrics.

00:32:28.560 --> 00:32:31.947
You know, we try to think,
OK, what do they want to do?

00:32:31.947 --> 00:32:33.280
And are we helping them do that?

00:32:33.280 --> 00:32:35.680
And that's kind of our--
that's our fundamental design

00:32:35.680 --> 00:32:36.230
principle.

00:32:36.230 --> 00:32:37.480
SARAH KARAM: Yeah, wise words.

00:32:37.480 --> 00:32:41.230
Thank you all so
much for joining us.

00:32:41.230 --> 00:32:43.800
In a segue we're actually
going to be talking to Brad--

00:32:43.800 --> 00:32:45.800
or Brad is going be talking
about the Assistant.

00:32:45.800 --> 00:32:48.740
So very relevant to
this conversation.

00:32:48.740 --> 00:32:49.880
Yeah, thank you again.

00:32:49.880 --> 00:32:52.350
Thanks everyone for being here.

