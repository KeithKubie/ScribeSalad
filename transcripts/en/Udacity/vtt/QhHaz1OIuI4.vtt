WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:03.250
Although conditional entropy can tell us when two variables

00:00:03.250 --> 00:00:06.210
are completely independent, it is not an adequate measure of

00:00:06.210 --> 00:00:09.832
dependence. Now consider the conditional entropy of y given the

00:00:09.832 --> 00:00:13.940
variable x. This conditional entropy may be small if x

00:00:13.940 --> 00:00:18.480
tells us a great deal about y or that x of y is very small to begin with. So we

00:00:18.480 --> 00:00:22.570
need another measure of dependence to measure the relationship between

00:00:22.570 --> 00:00:25.920
x and y and we call that as mutual information.

00:00:25.920 --> 00:00:32.479
It is denoted by the symbol I. And it is given as, the entropy of y, subtracted

00:00:32.479 --> 00:00:34.930
by the entropy of x given y. So

00:00:34.930 --> 00:00:37.470
mutual information is a measure of the reduction of

00:00:37.470 --> 00:00:40.400
randomness of a variable given knowledge of some

00:00:40.400 --> 00:00:43.350
other variable. If you like to understand the derivations

00:00:43.350 --> 00:00:47.500
for these particular identities, I'll refer you to Charles's

00:00:47.500 --> 00:00:51.080
notes on, on this topic. But we'll jump directly

00:00:51.080 --> 00:00:52.970
into an example and try to calculate these

00:00:52.970 --> 00:00:54.968
values and understand what it means to have a

00:00:54.968 --> 00:00:57.128
high value of mutual information or low value of

00:00:57.128 --> 00:01:01.140
mutual information. So let's do that as a quiz.

