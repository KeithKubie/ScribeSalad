WEBVTT
Kind: captions
Language: en

00:00:00.226 --> 00:00:03.189
♪ (music) ♪

00:00:06.517 --> 00:00:10.525
Hello, everyone. Welcome back.
Welcome back.

00:00:11.685 --> 00:00:14.243
I'm Jeremiah and this is Andrew.

00:00:14.537 --> 00:00:17.136
We are here from the TensorFlow Hub team.

00:00:17.553 --> 00:00:20.582
We are based in Zurich, Switzerland,

00:00:20.582 --> 00:00:23.404
and we're excited 
to share TensorFlow Hub today.

00:00:25.969 --> 00:00:30.353
So this first slide is actually one 
that I stole-- I took it from a colleague,

00:00:30.353 --> 00:00:33.872
Noah Fidel, who leads TensorFlow Serving,

00:00:33.872 --> 00:00:38.043
and Noah uses this slide 
to tell a personal story,

00:00:38.294 --> 00:00:45.088
and it kind of shows the growth of things,
the type of tools that we use

00:00:45.088 --> 00:00:47.137
to do software engineering.

00:00:47.217 --> 00:00:49.709
And it shows how they mature over time.

00:00:50.242 --> 00:00:54.085
And he also connects this 
to a similar thing happening,

00:00:54.194 --> 00:00:56.810
the tools we use to do machine learning.

00:00:57.950 --> 00:00:59.951
And he draws these connections.

00:00:59.970 --> 00:01:03.773
We're rediscovering things
as we grow our machine learning tools,

00:01:03.773 --> 00:01:07.928
things like the machine learning 
equivalent of source control,

00:01:07.928 --> 00:01:11.397
and the machine learning equivalent
of continuous integration.

00:01:11.397 --> 00:01:14.897
And Noah also makes 
this really interesting observation

00:01:14.897 --> 00:01:17.318
that this growth 
on the machine learning side

00:01:17.318 --> 00:01:20.898
is lagging behind 
the software engineering side

00:01:20.898 --> 00:01:23.737
by 15, 20 years.

00:01:24.854 --> 00:01:27.263
So this creates 
a really interesting opportunity, right?

00:01:27.263 --> 00:01:29.493
We can look at software engineering.

00:01:29.558 --> 00:01:32.171
We can look at some of the things 
that have happened there,

00:01:32.171 --> 00:01:37.366
and think about what kind of impact 
they may have on machine learning.

00:01:38.586 --> 00:01:43.275
And so looking at software engineering,
there's something that's so fundamental,

00:01:43.337 --> 00:01:48.041
it's almost easy to skip over,
and that's this idea of sharing code,

00:01:48.041 --> 00:01:49.453
shared repositories.

00:01:49.961 --> 00:01:53.526
On the surface, this makes us 
immediately more productive.

00:01:53.526 --> 00:01:55.658
We can search for code.
We can download it.

00:01:55.658 --> 00:01:56.773
We can use it.

00:01:56.773 --> 00:01:59.938
But it has these really powerful 
second order effects.

00:02:00.368 --> 00:02:03.188
This actually changes 
the way we write code.

00:02:03.768 --> 00:02:07.248
We refactor our code.
We put it in libraries.

00:02:07.482 --> 00:02:10.719
We share those libraries,
and this really makes people

00:02:10.719 --> 00:02:15.230
even more productive,
and it's the same dynamic

00:02:15.230 --> 00:02:18.110
that we want to create 
from machine learning

00:02:18.110 --> 00:02:19.789
with TensorFlow Hub.

00:02:20.894 --> 00:02:26.178
TensorFlow Hub lets you build, 
share and use

00:02:26.178 --> 00:02:28.337
pieces of machine learning.

00:02:29.988 --> 00:02:31.527
So why is this important?

00:02:31.542 --> 00:02:34.869
Well, anyone who's done 
machine learning from scratch knows

00:02:34.869 --> 00:02:36.724
you need a lot to do it well.

00:02:37.201 --> 00:02:40.583
You need an algorithm; you need data.

00:02:40.583 --> 00:02:42.484
You need compute power and expertise,

00:02:42.484 --> 00:02:44.962
and if you're missing any of these,
you're out of luck.

00:02:46.244 --> 00:02:50.665
So TensorFlow Hub lets you distill 
all these things down

00:02:50.665 --> 00:02:54.914
into a reusable package we call a module.

00:02:54.914 --> 00:02:56.703
Those modules go in TensorFlow Hub

00:02:56.703 --> 00:03:00.843
where people can discover them
and then easily use them.

00:03:02.575 --> 00:03:06.526
So you'll notice I'm saying module
instead of model.

00:03:07.016 --> 00:03:09.766
It turns out that a model is 
a little bit too big

00:03:09.766 --> 00:03:11.745
to really encourage sharing.

00:03:12.323 --> 00:03:14.706
If you have a model, 
you can use that model

00:03:14.706 --> 00:03:17.457
if you have the exact inputs it wants

00:03:17.457 --> 00:03:20.682
and you expect 
the exact outputs it provides.

00:03:21.207 --> 00:03:23.785
If there's any little differences,
you're kind of out of luck.

00:03:24.644 --> 00:03:26.999
So modules are a smaller piece.

00:03:27.380 --> 00:03:31.096
If you think of a model
like a binary,

00:03:31.096 --> 00:03:33.616
think of a module like a library.

00:03:37.475 --> 00:03:40.303
So on the inside, a module 
is actually a saved model,

00:03:40.303 --> 00:03:43.234
so this lets us package up 
the algorithm in the form of a graph,

00:03:43.234 --> 00:03:45.324
lets us package up the weights.

00:03:45.400 --> 00:03:48.120
You can do things like initialize,
use assets.

00:03:49.394 --> 00:03:54.021
And our libraries make it very easy 
to instantiate these

00:03:54.021 --> 00:03:55.608
in your TensorFlow code.

00:03:55.706 --> 00:03:59.018
So you can compose these 
in interesting ways.

00:03:59.018 --> 00:04:01.707
This makes things very reusable.

00:04:01.707 --> 00:04:03.413
You can produce one 
of these and share it.

00:04:03.695 --> 00:04:05.821
These are also retrainable.

00:04:06.157 --> 00:04:08.407
Once you patch it in 
to your bigger program,

00:04:08.407 --> 00:04:10.608
you can back propagate 
through it just like normal.

00:04:11.169 --> 00:04:14.079
And this is really powerful
because if you do happen

00:04:14.079 --> 00:04:18.028
to have enough data,
you can customize the TF Hub module

00:04:18.028 --> 00:04:20.338
for your own application.

00:04:20.887 --> 00:04:24.889
And to tell us a little bit more
about some of those applications,

00:04:24.889 --> 00:04:27.625
I'll hand it over to Andrew.

00:04:28.547 --> 00:04:29.920
Thanks, Jeremiah.

00:04:30.133 --> 00:04:33.949
Let's look at a specific example
of using a TF Hub module

00:04:33.949 --> 00:04:35.412
for image retraining.

00:04:35.741 --> 00:04:37.696
Let's say that we're going to make an app

00:04:37.696 --> 00:04:40.684
that can classify 
rabbit breeds from photos.

00:04:40.787 --> 00:04:43.877
The problem is we only have
a couple hundred examples,

00:04:43.877 --> 00:04:45.374
probably not enough to train

00:04:45.374 --> 00:04:48.338
an entire image classification model
from scratch.

00:04:49.756 --> 00:04:53.148
But what we could do is start 
from an existing general purpose

00:04:53.148 --> 00:04:54.757
image classification model.

00:04:55.108 --> 00:04:57.838
Most of the high performing ones
these days are trained

00:04:57.838 --> 00:05:00.479
on millions of examples
and they can easily classify

00:05:00.479 --> 00:05:02.304
thousands of categories.

00:05:02.755 --> 00:05:08.262
So we want to reuse the architecture
and the trained weights of that model

00:05:08.262 --> 00:05:10.093
without the classification layers.

00:05:10.333 --> 00:05:14.685
And in that way, we can add
our own rabbit classifier on top,

00:05:14.685 --> 00:05:18.527
and we can train it 
on our own rabbit examples

00:05:18.527 --> 00:05:21.194
and keep the reused weights fixed.

00:05:21.965 --> 00:05:28.107
So since we're using TensorFlow Hub,
our first stop is <i>tensorflow.org/hub</i>

00:05:28.383 --> 00:05:32.226
where we can find a list 
of all the newly released state of the art

00:05:32.226 --> 00:05:35.124
and also the well-known image modules.

00:05:35.606 --> 00:05:38.422
Some of them include 
the classification layers,

00:05:38.422 --> 00:05:42.528
and some of them remove them
just providing a feature vector as output.

00:05:43.648 --> 00:05:46.366
So we'll choose one of the feature
vector ones for this case.

00:05:46.521 --> 00:05:50.473
Let's use NASNet which is 
a state of the art image module

00:05:50.473 --> 00:05:53.179
that was created 
by a neural architecture search.

00:05:54.546 --> 00:05:57.474
So you just paste the URL
of a module,

00:05:57.566 --> 00:06:00.943
and TensorFlow Hub takes care 
of downloading the graph

00:06:00.943 --> 00:06:04.020
and all of its weights
and importing it into your model.

00:06:04.378 --> 00:06:08.410
And in that one line, you're ready 
to use the module like any function

00:06:08.419 --> 00:06:10.812
so here we just provide 
a batch of inputs,

00:06:10.812 --> 00:06:13.058
and we get back our feature vectors.

00:06:13.058 --> 00:06:17.150
We add a classification layer on top,
and we output our predictions.

00:06:17.641 --> 00:06:21.590
But in that one line, 
you get a huge amount of value.

00:06:21.590 --> 00:06:26.301
In this particular case, 
more than 62,000 hours of GPU time

00:06:26.301 --> 00:06:30.574
went into finding the best architecture
for NASNet and training the result.

00:06:30.808 --> 00:06:37.333
And all of the expertise, research, 
testing, that the authors put into that--

00:06:37.333 --> 00:06:39.260
that's all built into the module.

00:06:41.455 --> 00:06:44.414
Plus that module can be fine-tuned 
with your model,

00:06:44.483 --> 00:06:48.985
so if you have enough examples
you can potentially get better performance

00:06:48.985 --> 00:06:52.843
if you use a low learning rate,
if you set the trainable parameter

00:06:52.843 --> 00:06:56.387
to true and if you use 
the training version of the graph.

00:06:59.056 --> 00:07:03.609
So NASNet is available in a large size 
as well as a mobile-sized module,

00:07:03.609 --> 00:07:06.634
and then there's also
the new progressive NASNet,

00:07:07.499 --> 00:07:10.473
and then on a number 
of new MobileNet modules

00:07:10.473 --> 00:07:14.880
for doing on-device image classification
as well as some industry standard ones

00:07:14.880 --> 00:07:16.497
like Inception and ResNet.

00:07:16.704 --> 00:07:20.064
That complete list is 
at <i>tensorflow.org/hub</i>.

00:07:21.061 --> 00:07:23.345
And of course all those modules 
are pre-trained

00:07:23.345 --> 00:07:25.530
using the TF slim checkpoints,

00:07:25.530 --> 00:07:27.593
and they're ready 
to be used for classification

00:07:27.593 --> 00:07:30.036
or as feature vector inputs 
to your own model.

00:07:31.887 --> 00:07:33.871
Okay, let's look at another example--

00:07:33.871 --> 00:07:36.631
in this case, doing 
a little bit of text classification.

00:07:37.002 --> 00:07:40.568
So we'd like to know whether 
a restaurant review is

00:07:40.568 --> 00:07:42.522
a positive or negative sentiment.

00:07:42.941 --> 00:07:45.022
So as Jeremiah mentioned,

00:07:45.022 --> 00:07:47.288
one of the great things 
about TensorFlow Hub

00:07:47.288 --> 00:07:49.923
is that it packages 
the graph with the data.

00:07:49.923 --> 00:07:52.169
For our text and embedding modules,

00:07:52.169 --> 00:07:55.539
that means that all 
of the preprocessing is included,

00:07:55.539 --> 00:07:59.892
doing things like normalizing 
and tokenizing operations.

00:08:00.217 --> 00:08:03.904
So we can use 
a pre-trained sentence embedding module

00:08:03.904 --> 00:08:06.783
to map a full sentence 
to an embedding vector.

00:08:08.271 --> 00:08:11.491
So if we want to classify 
some restaurant reviews,

00:08:11.496 --> 00:08:14.031
then we just take one 
of those sentence embedding modules,

00:08:14.031 --> 00:08:17.593
we add our own classification layer on top

00:08:17.593 --> 00:08:19.697
and then we train with our reviews

00:08:19.697 --> 00:08:22.839
while we keep 
the sentence module's weights fixed.

00:08:25.540 --> 00:08:30.533
And just like for the image modules,
<i>tensorflow.org/hub</i> lists a number

00:08:30.533 --> 00:08:32.423
of different text modules.

00:08:32.447 --> 00:08:36.900
We have neural network language models
that are trained on genus

00:08:36.900 --> 00:08:40.776
for English, Japanese, German and Spanish.

00:08:40.776 --> 00:08:44.398
We also have Word2vec models 
trained on Wikipedia,

00:08:44.398 --> 00:08:47.414
as well as a new module 
called ELMo that models

00:08:47.414 --> 00:08:52.454
the characteristics of word use
and how those uses vary across context.

00:08:53.926 --> 00:08:57.338
And then there's also something 
really new, as in today.

00:08:57.338 --> 00:09:01.542
You may have seen a new paper 
this morning from Ray Kurzweil's team.

00:09:01.542 --> 00:09:04.524
This is the Universal Sentence Encoder.

00:09:04.535 --> 00:09:07.363
It's a sentence-level embedding module.

00:09:07.433 --> 00:09:11.885
It's trained on a variety of tasks,
and it enables a variety of tasks,

00:09:11.885 --> 00:09:13.604
in other words, universal.

00:09:13.880 --> 00:09:17.536
So some of the things that it's good for
are semantic similarity

00:09:17.536 --> 00:09:22.811
doing custom text classification,
clustering and semantic search.

00:09:23.470 --> 00:09:26.434
But the best part about it 
is how little training data

00:09:26.434 --> 00:09:30.250
is actually required
to adapt the module to your own problem.

00:09:31.170 --> 00:09:33.520
So that sounds great 
in our particular case.

00:09:33.631 --> 00:09:36.370
Let's try it 
on the restaurant review task.

00:09:38.035 --> 00:09:41.185
So we just paste that URL from the paper,

00:09:41.185 --> 00:09:45.341
and like before TensorFlow Hub downloads
the module and inserts it into your graph.

00:09:46.156 --> 00:09:49.126
But this time, we're using 
the text embedding column.

00:09:49.126 --> 00:09:53.030
That way we can feed into an estimator
to do the classification part.

00:09:54.635 --> 00:09:57.451
And just like 
with the image retraining example,

00:09:57.451 --> 00:09:59.264
this module can be fine-tuned

00:09:59.264 --> 00:10:01.687
with your model 
by setting trainable to true.

00:10:02.051 --> 00:10:04.160
Of course, you have 
to lower the learning rate

00:10:04.160 --> 00:10:06.973
so that you don't ruin 
the existing weights that are in there,

00:10:06.973 --> 00:10:09.843
but it's something that's worth exploring
if you have enough data.

00:10:11.964 --> 00:10:14.862
Now let's take a closer look 
at that URL.

00:10:15.169 --> 00:10:18.599
As Jeremiah mentioned,
a module is a program,

00:10:18.599 --> 00:10:22.172
so make sure what you're executing
is from a location that you trust.

00:10:22.827 --> 00:10:26.541
In this case, the module 
is from <i>tfhub.dev</i>

00:10:26.541 --> 00:10:30.715
so that's our new source
for Google-provided modules

00:10:30.715 --> 00:10:33.917
like NASNet 
and a universal sentence encoder.

00:10:34.099 --> 00:10:38.228
Over time, we would like to create 
a place where you can also publish

00:10:38.228 --> 00:10:39.836
the modules that you create.

00:10:41.450 --> 00:10:44.468
So in this case, Google is the publisher,

00:10:44.468 --> 00:10:47.915
and Universal Sentence Encoder 
is the name of the module.

00:10:48.822 --> 00:10:52.179
And finally, the version number is one.

00:10:52.674 --> 00:10:56.797
So TensorFlow Hub considers modules
to be immutable,

00:10:57.048 --> 00:11:00.425
that way you don't have to worry 
about the weights changing

00:11:00.425 --> 00:11:02.049
between training sessions.

00:11:02.279 --> 00:11:07.601
So that module URL 
and all of the module URLs

00:11:07.601 --> 00:11:10.329
on <i>tfhub.dev</i> include a version number.

00:11:12.938 --> 00:11:16.588
And you can take that URL
and paste it into your browser

00:11:16.630 --> 00:11:18.528
and see the complete documentation

00:11:18.528 --> 00:11:21.309
for any module that's hosted on <i>tfhub.dev</i>.

00:11:21.892 --> 00:11:24.857
Here's the particular one 
for the Universal Sentence Encoder.

00:11:27.275 --> 00:11:30.006
Then we also have modules
for other domains

00:11:30.006 --> 00:11:33.015
besides text classification
and image retraining,

00:11:33.015 --> 00:11:37.280
like a generative image module
that contains a progressive GAN

00:11:37.280 --> 00:11:39.365
that was trained on [inaudible].

00:11:39.677 --> 00:11:41.405
And we added another module

00:11:41.405 --> 00:11:44.131
that was based 
on Deep Local Features network

00:11:44.131 --> 00:11:47.649
which can identify the key points
of landmark images.

00:11:48.596 --> 00:11:53.990
Both of those have great Colab notebooks
that are available on <i>tensorflow.org/hub.</i>

00:11:54.264 --> 00:11:56.968
In fact, the images here 
were created from them.

00:11:58.048 --> 00:12:00.517
And we're going to be adding
more modules over time

00:12:00.517 --> 00:12:04.089
for tasks like audio and video
over the next few months.

00:12:06.031 --> 00:12:09.034
But most importantly,
we're really excited to see

00:12:09.034 --> 00:12:11.274
what you build with TensorFlow Hub.

00:12:11.274 --> 00:12:14.174
Use the hashtag <i>#tfhub</i>
when you build notebooks

00:12:14.174 --> 00:12:15.700
and share modules.

00:12:15.700 --> 00:12:19.766
And visit <i>tensorflow.org/hub</i>
for links to our documentation,

00:12:19.766 --> 00:12:25.411
including examples with tutorials,
interactive notebooks and code labs

00:12:25.411 --> 00:12:27.800
and our new discussion mailing list.

00:12:28.630 --> 00:12:32.701
So from Jeremiah, me and everyone
on our team in Zurich,

00:12:32.701 --> 00:12:35.134
I want to thank you so much.

00:12:36.024 --> 00:12:38.116
(applause)

00:12:38.345 --> 00:12:41.193
♪ (music) ♪

