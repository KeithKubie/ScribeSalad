WEBVTT
Kind: captions
Language: en

00:00:00.329 --> 00:00:03.050
if the basic technical idea is behind

00:00:03.050 --> 00:00:03.060
if the basic technical idea is behind
 

00:00:03.060 --> 00:00:05.690
if the basic technical idea is behind
deep learning behind your networks have

00:00:05.690 --> 00:00:05.700
deep learning behind your networks have
 

00:00:05.700 --> 00:00:07.460
deep learning behind your networks have
been around for decades why are they

00:00:07.460 --> 00:00:07.470
been around for decades why are they
 

00:00:07.470 --> 00:00:09.860
been around for decades why are they
only just now taking off in this video

00:00:09.860 --> 00:00:09.870
only just now taking off in this video
 

00:00:09.870 --> 00:00:12.080
only just now taking off in this video
let's go over some of the main drivers

00:00:12.080 --> 00:00:12.090
let's go over some of the main drivers
 

00:00:12.090 --> 00:00:14.120
let's go over some of the main drivers
behind the rise of deep learning because

00:00:14.120 --> 00:00:14.130
behind the rise of deep learning because
 

00:00:14.130 --> 00:00:16.160
behind the rise of deep learning because
I think this will help you that the spot

00:00:16.160 --> 00:00:16.170
I think this will help you that the spot
 

00:00:16.170 --> 00:00:18.080
I think this will help you that the spot
the best opportunities within your own

00:00:18.080 --> 00:00:18.090
the best opportunities within your own
 

00:00:18.090 --> 00:00:20.840
the best opportunities within your own
organization to apply these to over the

00:00:20.840 --> 00:00:20.850
organization to apply these to over the
 

00:00:20.850 --> 00:00:22.429
organization to apply these to over the
last few years a lot of people have

00:00:22.429 --> 00:00:22.439
last few years a lot of people have
 

00:00:22.439 --> 00:00:24.230
last few years a lot of people have
asked me Andrew why is deep learning

00:00:24.230 --> 00:00:24.240
asked me Andrew why is deep learning
 

00:00:24.240 --> 00:00:26.810
asked me Andrew why is deep learning
certainly working so well and when a

00:00:26.810 --> 00:00:26.820
certainly working so well and when a
 

00:00:26.820 --> 00:00:28.939
certainly working so well and when a
marsan question this is usually the

00:00:28.939 --> 00:00:28.949
marsan question this is usually the
 

00:00:28.949 --> 00:00:31.099
marsan question this is usually the
picture I draw for them let's say we

00:00:31.099 --> 00:00:31.109
picture I draw for them let's say we
 

00:00:31.109 --> 00:00:33.200
picture I draw for them let's say we
plot a figure where on the horizontal

00:00:33.200 --> 00:00:33.210
plot a figure where on the horizontal
 

00:00:33.210 --> 00:00:36.170
plot a figure where on the horizontal
axis we plot the amount of data we have

00:00:36.170 --> 00:00:36.180
axis we plot the amount of data we have
 

00:00:36.180 --> 00:00:39.260
axis we plot the amount of data we have
for a task and let's say on the vertical

00:00:39.260 --> 00:00:39.270
for a task and let's say on the vertical
 

00:00:39.270 --> 00:00:42.560
for a task and let's say on the vertical
axis we plot the performance on above

00:00:42.560 --> 00:00:42.570
axis we plot the performance on above
 

00:00:42.570 --> 00:00:44.420
axis we plot the performance on above
learning algorithms such as the accuracy

00:00:44.420 --> 00:00:44.430
learning algorithms such as the accuracy
 

00:00:44.430 --> 00:00:48.170
learning algorithms such as the accuracy
of our spam classifier or our ad click

00:00:48.170 --> 00:00:48.180
of our spam classifier or our ad click
 

00:00:48.180 --> 00:00:51.950
of our spam classifier or our ad click
predictor or the accuracy of our neural

00:00:51.950 --> 00:00:51.960
predictor or the accuracy of our neural
 

00:00:51.960 --> 00:00:53.959
predictor or the accuracy of our neural
net for figuring out the position of

00:00:53.959 --> 00:00:53.969
net for figuring out the position of
 

00:00:53.969 --> 00:00:56.389
net for figuring out the position of
other calls for our self-driving car it

00:00:56.389 --> 00:00:56.399
other calls for our self-driving car it
 

00:00:56.399 --> 00:00:58.430
other calls for our self-driving car it
turns out if you plot the performance of

00:00:58.430 --> 00:00:58.440
turns out if you plot the performance of
 

00:00:58.440 --> 00:01:00.260
turns out if you plot the performance of
a traditional learning algorithm like

00:01:00.260 --> 00:01:00.270
a traditional learning algorithm like
 

00:01:00.270 --> 00:01:02.450
a traditional learning algorithm like
support vector machine or logistic

00:01:02.450 --> 00:01:02.460
support vector machine or logistic
 

00:01:02.460 --> 00:01:04.700
support vector machine or logistic
regression as a function of the amount

00:01:04.700 --> 00:01:04.710
regression as a function of the amount
 

00:01:04.710 --> 00:01:07.609
regression as a function of the amount
of data you have you might get a curve

00:01:07.609 --> 00:01:07.619
of data you have you might get a curve
 

00:01:07.619 --> 00:01:09.710
of data you have you might get a curve
that looks like this where the

00:01:09.710 --> 00:01:09.720
that looks like this where the
 

00:01:09.720 --> 00:01:11.660
that looks like this where the
performance improves for a while as you

00:01:11.660 --> 00:01:11.670
performance improves for a while as you
 

00:01:11.670 --> 00:01:14.270
performance improves for a while as you
add more data but after a while the

00:01:14.270 --> 00:01:14.280
add more data but after a while the
 

00:01:14.280 --> 00:01:16.190
add more data but after a while the
performance you know pretty much

00:01:16.190 --> 00:01:16.200
performance you know pretty much
 

00:01:16.200 --> 00:01:18.620
performance you know pretty much
plateaus right suppose your horizontal

00:01:18.620 --> 00:01:18.630
plateaus right suppose your horizontal
 

00:01:18.630 --> 00:01:21.170
plateaus right suppose your horizontal
lines enjoy that very well you know was

00:01:21.170 --> 00:01:21.180
lines enjoy that very well you know was
 

00:01:21.180 --> 00:01:25.310
lines enjoy that very well you know was
it they didn't know what to do with huge

00:01:25.310 --> 00:01:25.320
it they didn't know what to do with huge
 

00:01:25.320 --> 00:01:28.130
it they didn't know what to do with huge
amounts of data and what happened in our

00:01:28.130 --> 00:01:28.140
amounts of data and what happened in our
 

00:01:28.140 --> 00:01:30.679
amounts of data and what happened in our
society over the last 10 years maybe is

00:01:30.679 --> 00:01:30.689
society over the last 10 years maybe is
 

00:01:30.689 --> 00:01:32.840
society over the last 10 years maybe is
that for a lot of problems we went from

00:01:32.840 --> 00:01:32.850
that for a lot of problems we went from
 

00:01:32.850 --> 00:01:34.810
that for a lot of problems we went from
having a relatively small amount of data

00:01:34.810 --> 00:01:34.820
having a relatively small amount of data
 

00:01:34.820 --> 00:01:38.600
having a relatively small amount of data
to having you know often a fairly large

00:01:38.600 --> 00:01:38.610
to having you know often a fairly large
 

00:01:38.610 --> 00:01:40.969
to having you know often a fairly large
amount of data and all of this was

00:01:40.969 --> 00:01:40.979
amount of data and all of this was
 

00:01:40.979 --> 00:01:43.969
amount of data and all of this was
thanks to the digitization of a society

00:01:43.969 --> 00:01:43.979
thanks to the digitization of a society
 

00:01:43.979 --> 00:01:46.969
thanks to the digitization of a society
where so much human activity is now in

00:01:46.969 --> 00:01:46.979
where so much human activity is now in
 

00:01:46.979 --> 00:01:48.710
where so much human activity is now in
the digital realm we spend so much time

00:01:48.710 --> 00:01:48.720
the digital realm we spend so much time
 

00:01:48.720 --> 00:01:51.170
the digital realm we spend so much time
on the computers on websites on mobile

00:01:51.170 --> 00:01:51.180
on the computers on websites on mobile
 

00:01:51.180 --> 00:01:54.310
on the computers on websites on mobile
apps and activities on digital devices

00:01:54.310 --> 00:01:54.320
apps and activities on digital devices
 

00:01:54.320 --> 00:01:57.950
apps and activities on digital devices
creates data and thanks to the rise of

00:01:57.950 --> 00:01:57.960
creates data and thanks to the rise of
 

00:01:57.960 --> 00:02:00.350
creates data and thanks to the rise of
inexpensive cameras built into our cell

00:02:00.350 --> 00:02:00.360
inexpensive cameras built into our cell
 

00:02:00.360 --> 00:02:02.359
inexpensive cameras built into our cell
phones accelerometers all sorts of

00:02:02.359 --> 00:02:02.369
phones accelerometers all sorts of
 

00:02:02.369 --> 00:02:05.899
phones accelerometers all sorts of
sensors in the Internet of Things we

00:02:05.899 --> 00:02:05.909
sensors in the Internet of Things we
 

00:02:05.909 --> 00:02:07.880
sensors in the Internet of Things we
also just have been collecting one more

00:02:07.880 --> 00:02:07.890
also just have been collecting one more
 

00:02:07.890 --> 00:02:11.119
also just have been collecting one more
and more data so over the last 20 years

00:02:11.119 --> 00:02:11.129
and more data so over the last 20 years
 

00:02:11.129 --> 00:02:12.860
and more data so over the last 20 years
for a lot of applications we just

00:02:12.860 --> 00:02:12.870
for a lot of applications we just
 

00:02:12.870 --> 00:02:13.550
for a lot of applications we just
accumulate

00:02:13.550 --> 00:02:13.560
accumulate
 

00:02:13.560 --> 00:02:16.309
accumulate
a lot more data more than traditional

00:02:16.309 --> 00:02:16.319
a lot more data more than traditional
 

00:02:16.319 --> 00:02:17.540
a lot more data more than traditional
learning algorithms were able to

00:02:17.540 --> 00:02:17.550
learning algorithms were able to
 

00:02:17.550 --> 00:02:20.510
learning algorithms were able to
effectively take advantage of and what

00:02:20.510 --> 00:02:20.520
effectively take advantage of and what
 

00:02:20.520 --> 00:02:22.550
effectively take advantage of and what
new network lead turns out that if you

00:02:22.550 --> 00:02:22.560
new network lead turns out that if you
 

00:02:22.560 --> 00:02:26.300
new network lead turns out that if you
train a small neural net then this

00:02:26.300 --> 00:02:26.310
train a small neural net then this
 

00:02:26.310 --> 00:02:28.460
train a small neural net then this
performance maybe looks like that

00:02:28.460 --> 00:02:28.470
performance maybe looks like that
 

00:02:28.470 --> 00:02:31.339
performance maybe looks like that
if you train a somewhat larger Internet

00:02:31.339 --> 00:02:31.349
if you train a somewhat larger Internet
 

00:02:31.349 --> 00:02:34.580
if you train a somewhat larger Internet
that's called as a medium-sized internet

00:02:34.580 --> 00:02:34.590
that's called as a medium-sized internet
 

00:02:34.590 --> 00:02:36.320
that's called as a medium-sized internet
to fall in something a little bit better

00:02:36.320 --> 00:02:36.330
to fall in something a little bit better
 

00:02:36.330 --> 00:02:39.890
to fall in something a little bit better
and if you train a very large neural net

00:02:39.890 --> 00:02:39.900
and if you train a very large neural net
 

00:02:39.900 --> 00:02:42.170
and if you train a very large neural net
then it's the form and often just keeps

00:02:42.170 --> 00:02:42.180
then it's the form and often just keeps
 

00:02:42.180 --> 00:02:44.570
then it's the form and often just keeps
getting better and better so couple

00:02:44.570 --> 00:02:44.580
getting better and better so couple
 

00:02:44.580 --> 00:02:46.880
getting better and better so couple
observations one is if you want to hit

00:02:46.880 --> 00:02:46.890
observations one is if you want to hit
 

00:02:46.890 --> 00:02:49.400
observations one is if you want to hit
this very high level of performance then

00:02:49.400 --> 00:02:49.410
this very high level of performance then
 

00:02:49.410 --> 00:02:52.610
this very high level of performance then
you need two things first often you need

00:02:52.610 --> 00:02:52.620
you need two things first often you need
 

00:02:52.620 --> 00:02:54.410
you need two things first often you need
to be able to train a big enough neural

00:02:54.410 --> 00:02:54.420
to be able to train a big enough neural
 

00:02:54.420 --> 00:02:57.350
to be able to train a big enough neural
network in order to take advantage of

00:02:57.350 --> 00:02:57.360
network in order to take advantage of
 

00:02:57.360 --> 00:02:59.660
network in order to take advantage of
the huge amount of data and second you

00:02:59.660 --> 00:02:59.670
the huge amount of data and second you
 

00:02:59.670 --> 00:03:02.000
the huge amount of data and second you
need to be out here on the x axes you do

00:03:02.000 --> 00:03:02.010
need to be out here on the x axes you do
 

00:03:02.010 --> 00:03:05.420
need to be out here on the x axes you do
need a lot of data so we often say that

00:03:05.420 --> 00:03:05.430
need a lot of data so we often say that
 

00:03:05.430 --> 00:03:07.789
need a lot of data so we often say that
scale has been driving deep learning

00:03:07.789 --> 00:03:07.799
scale has been driving deep learning
 

00:03:07.799 --> 00:03:10.850
scale has been driving deep learning
progress and by scale I mean both the

00:03:10.850 --> 00:03:10.860
progress and by scale I mean both the
 

00:03:10.860 --> 00:03:12.890
progress and by scale I mean both the
size of the neural network we need just

00:03:12.890 --> 00:03:12.900
size of the neural network we need just
 

00:03:12.900 --> 00:03:15.140
size of the neural network we need just
a new network a lot of hidden units a

00:03:15.140 --> 00:03:15.150
a new network a lot of hidden units a
 

00:03:15.150 --> 00:03:17.059
a new network a lot of hidden units a
lot of parameters a lot of connections

00:03:17.059 --> 00:03:17.069
lot of parameters a lot of connections
 

00:03:17.069 --> 00:03:21.470
lot of parameters a lot of connections
as well as scale of the data in fact

00:03:21.470 --> 00:03:21.480
as well as scale of the data in fact
 

00:03:21.480 --> 00:03:23.900
as well as scale of the data in fact
today one of the most reliable ways to

00:03:23.900 --> 00:03:23.910
today one of the most reliable ways to
 

00:03:23.910 --> 00:03:25.430
today one of the most reliable ways to
get better performance in the neural

00:03:25.430 --> 00:03:25.440
get better performance in the neural
 

00:03:25.440 --> 00:03:27.380
get better performance in the neural
network is often to either train a

00:03:27.380 --> 00:03:27.390
network is often to either train a
 

00:03:27.390 --> 00:03:29.930
network is often to either train a
bigger network or throw more data at it

00:03:29.930 --> 00:03:29.940
bigger network or throw more data at it
 

00:03:29.940 --> 00:03:31.819
bigger network or throw more data at it
and that only works up to a point

00:03:31.819 --> 00:03:31.829
and that only works up to a point
 

00:03:31.829 --> 00:03:33.349
and that only works up to a point
because eventually you run out of data

00:03:33.349 --> 00:03:33.359
because eventually you run out of data
 

00:03:33.359 --> 00:03:35.630
because eventually you run out of data
or eventually then your network is so

00:03:35.630 --> 00:03:35.640
or eventually then your network is so
 

00:03:35.640 --> 00:03:37.759
or eventually then your network is so
big that it takes too long to train but

00:03:37.759 --> 00:03:37.769
big that it takes too long to train but
 

00:03:37.769 --> 00:03:40.190
big that it takes too long to train but
just improving scale has actually taken

00:03:40.190 --> 00:03:40.200
just improving scale has actually taken
 

00:03:40.200 --> 00:03:42.680
just improving scale has actually taken
us a long way in the world of learning

00:03:42.680 --> 00:03:42.690
us a long way in the world of learning
 

00:03:42.690 --> 00:03:45.800
us a long way in the world of learning
in order to make this diagram a bit more

00:03:45.800 --> 00:03:45.810
in order to make this diagram a bit more
 

00:03:45.810 --> 00:03:48.050
in order to make this diagram a bit more
technically precise and just add a few

00:03:48.050 --> 00:03:48.060
technically precise and just add a few
 

00:03:48.060 --> 00:03:49.910
technically precise and just add a few
more things I wrote the amount of data

00:03:49.910 --> 00:03:49.920
more things I wrote the amount of data
 

00:03:49.920 --> 00:03:53.030
more things I wrote the amount of data
on the x-axis technically this is amount

00:03:53.030 --> 00:03:53.040
on the x-axis technically this is amount
 

00:03:53.040 --> 00:03:57.890
on the x-axis technically this is amount
of labeled data where by label data

00:03:57.890 --> 00:03:57.900
of labeled data where by label data
 

00:03:57.900 --> 00:04:00.170
of labeled data where by label data
I mean training examples we have both

00:04:00.170 --> 00:04:00.180
I mean training examples we have both
 

00:04:00.180 --> 00:04:03.620
I mean training examples we have both
the input X and the label Y I went to

00:04:03.620 --> 00:04:03.630
the input X and the label Y I went to
 

00:04:03.630 --> 00:04:05.900
the input X and the label Y I went to
introduce a little bit of notation that

00:04:05.900 --> 00:04:05.910
introduce a little bit of notation that
 

00:04:05.910 --> 00:04:07.699
introduce a little bit of notation that
we'll use later in this course we're

00:04:07.699 --> 00:04:07.709
we'll use later in this course we're
 

00:04:07.709 --> 00:04:10.759
we'll use later in this course we're
going to use lowercase alphabet to

00:04:10.759 --> 00:04:10.769
going to use lowercase alphabet to
 

00:04:10.769 --> 00:04:12.530
going to use lowercase alphabet to
denote the size of my training sets or

00:04:12.530 --> 00:04:12.540
denote the size of my training sets or
 

00:04:12.540 --> 00:04:13.729
denote the size of my training sets or
the number of training examples

00:04:13.729 --> 00:04:13.739
the number of training examples
 

00:04:13.739 --> 00:04:15.680
the number of training examples
this lowercase M so that's the

00:04:15.680 --> 00:04:15.690
this lowercase M so that's the
 

00:04:15.690 --> 00:04:18.979
this lowercase M so that's the
horizontal axis couple other details to

00:04:18.979 --> 00:04:18.989
horizontal axis couple other details to
 

00:04:18.989 --> 00:04:20.300
horizontal axis couple other details to
this Tigger

00:04:20.300 --> 00:04:20.310
this Tigger
 

00:04:20.310 --> 00:04:23.330
this Tigger
in this regime of smaller training sets

00:04:23.330 --> 00:04:23.340
in this regime of smaller training sets
 

00:04:23.340 --> 00:04:26.960
in this regime of smaller training sets
the relative ordering of the algorithms

00:04:26.960 --> 00:04:26.970
the relative ordering of the algorithms
 

00:04:26.970 --> 00:04:29.690
the relative ordering of the algorithms
is actually not very well defined so if

00:04:29.690 --> 00:04:29.700
is actually not very well defined so if
 

00:04:29.700 --> 00:04:31.580
is actually not very well defined so if
you don't have a lot of training data is

00:04:31.580 --> 00:04:31.590
you don't have a lot of training data is
 

00:04:31.590 --> 00:04:34.490
you don't have a lot of training data is
often up to your skill at hand

00:04:34.490 --> 00:04:34.500
often up to your skill at hand
 

00:04:34.500 --> 00:04:36.500
often up to your skill at hand
engineering features that determines the

00:04:36.500 --> 00:04:36.510
engineering features that determines the
 

00:04:36.510 --> 00:04:39.080
engineering features that determines the
foreman so it's quite possible that if

00:04:39.080 --> 00:04:39.090
foreman so it's quite possible that if
 

00:04:39.090 --> 00:04:41.900
foreman so it's quite possible that if
someone training an SVM is more

00:04:41.900 --> 00:04:41.910
someone training an SVM is more
 

00:04:41.910 --> 00:04:44.060
someone training an SVM is more
motivated to hand engineer features and

00:04:44.060 --> 00:04:44.070
motivated to hand engineer features and
 

00:04:44.070 --> 00:04:46.310
motivated to hand engineer features and
someone training even large their own

00:04:46.310 --> 00:04:46.320
someone training even large their own
 

00:04:46.320 --> 00:04:48.290
someone training even large their own
that may be in this small training set

00:04:48.290 --> 00:04:48.300
that may be in this small training set
 

00:04:48.300 --> 00:04:50.720
that may be in this small training set
regime the SEM could do better

00:04:50.720 --> 00:04:50.730
regime the SEM could do better
 

00:04:50.730 --> 00:04:53.120
regime the SEM could do better
so you know in this region to the left

00:04:53.120 --> 00:04:53.130
so you know in this region to the left
 

00:04:53.130 --> 00:04:55.010
so you know in this region to the left
of the figure the relative ordering

00:04:55.010 --> 00:04:55.020
of the figure the relative ordering
 

00:04:55.020 --> 00:04:57.080
of the figure the relative ordering
between gene algorithms is not that well

00:04:57.080 --> 00:04:57.090
between gene algorithms is not that well
 

00:04:57.090 --> 00:04:59.540
between gene algorithms is not that well
defined and performance depends much

00:04:59.540 --> 00:04:59.550
defined and performance depends much
 

00:04:59.550 --> 00:05:01.909
defined and performance depends much
more on your skill at engine features

00:05:01.909 --> 00:05:01.919
more on your skill at engine features
 

00:05:01.919 --> 00:05:03.379
more on your skill at engine features
and other mobile details of the

00:05:03.379 --> 00:05:03.389
and other mobile details of the
 

00:05:03.389 --> 00:05:05.960
and other mobile details of the
algorithms and there's only in this some

00:05:05.960 --> 00:05:05.970
algorithms and there's only in this some
 

00:05:05.970 --> 00:05:08.840
algorithms and there's only in this some
big data regime very large training sets

00:05:08.840 --> 00:05:08.850
big data regime very large training sets
 

00:05:08.850 --> 00:05:11.990
big data regime very large training sets
very large M regime in the right that we

00:05:11.990 --> 00:05:12.000
very large M regime in the right that we
 

00:05:12.000 --> 00:05:14.659
very large M regime in the right that we
more consistently see largely Ronettes

00:05:14.659 --> 00:05:14.669
more consistently see largely Ronettes
 

00:05:14.669 --> 00:05:17.629
more consistently see largely Ronettes
dominating the other approaches and so

00:05:17.629 --> 00:05:17.639
dominating the other approaches and so
 

00:05:17.639 --> 00:05:19.550
dominating the other approaches and so
if any of your friends ask you why are

00:05:19.550 --> 00:05:19.560
if any of your friends ask you why are
 

00:05:19.560 --> 00:05:21.590
if any of your friends ask you why are
known as you know taking off I would

00:05:21.590 --> 00:05:21.600
known as you know taking off I would
 

00:05:21.600 --> 00:05:23.690
known as you know taking off I would
encourage you to draw this picture for

00:05:23.690 --> 00:05:23.700
encourage you to draw this picture for
 

00:05:23.700 --> 00:05:26.719
encourage you to draw this picture for
them as well so I will say that in the

00:05:26.719 --> 00:05:26.729
them as well so I will say that in the
 

00:05:26.729 --> 00:05:28.880
them as well so I will say that in the
early days in their modern rise of deep

00:05:28.880 --> 00:05:28.890
early days in their modern rise of deep
 

00:05:28.890 --> 00:05:29.300
early days in their modern rise of deep
learning

00:05:29.300 --> 00:05:29.310
learning
 

00:05:29.310 --> 00:05:32.060
learning
it was scaled data and scale of

00:05:32.060 --> 00:05:32.070
it was scaled data and scale of
 

00:05:32.070 --> 00:05:34.909
it was scaled data and scale of
computation just our ability to Train

00:05:34.909 --> 00:05:34.919
computation just our ability to Train
 

00:05:34.919 --> 00:05:36.320
computation just our ability to Train
very large dinner networks

00:05:36.320 --> 00:05:36.330
very large dinner networks
 

00:05:36.330 --> 00:05:39.469
very large dinner networks
either on a CPU or GPU that enabled us

00:05:39.469 --> 00:05:39.479
either on a CPU or GPU that enabled us
 

00:05:39.479 --> 00:05:41.840
either on a CPU or GPU that enabled us
to make a lot of progress but

00:05:41.840 --> 00:05:41.850
to make a lot of progress but
 

00:05:41.850 --> 00:05:43.580
to make a lot of progress but
increasingly especially in the last

00:05:43.580 --> 00:05:43.590
increasingly especially in the last
 

00:05:43.590 --> 00:05:45.790
increasingly especially in the last
several years we've seen tremendous

00:05:45.790 --> 00:05:45.800
several years we've seen tremendous
 

00:05:45.800 --> 00:05:48.350
several years we've seen tremendous
algorithmic innovation as well so I also

00:05:48.350 --> 00:05:48.360
algorithmic innovation as well so I also
 

00:05:48.360 --> 00:05:50.529
algorithmic innovation as well so I also
don't want to understate that

00:05:50.529 --> 00:05:50.539
don't want to understate that
 

00:05:50.539 --> 00:05:53.690
don't want to understate that
interestingly many of the algorithmic

00:05:53.690 --> 00:05:53.700
interestingly many of the algorithmic
 

00:05:53.700 --> 00:05:56.930
interestingly many of the algorithmic
innovations have been about trying to

00:05:56.930 --> 00:05:56.940
innovations have been about trying to
 

00:05:56.940 --> 00:06:01.129
innovations have been about trying to
make neural networks run much faster so

00:06:01.129 --> 00:06:01.139
make neural networks run much faster so
 

00:06:01.139 --> 00:06:03.500
make neural networks run much faster so
as a concrete example one of the huge

00:06:03.500 --> 00:06:03.510
as a concrete example one of the huge
 

00:06:03.510 --> 00:06:05.300
as a concrete example one of the huge
breakthroughs in your networks has been

00:06:05.300 --> 00:06:05.310
breakthroughs in your networks has been
 

00:06:05.310 --> 00:06:08.719
breakthroughs in your networks has been
switching from a sigmoid function which

00:06:08.719 --> 00:06:08.729
switching from a sigmoid function which
 

00:06:08.729 --> 00:06:12.320
switching from a sigmoid function which
looks like this to a railer function

00:06:12.320 --> 00:06:12.330
looks like this to a railer function
 

00:06:12.330 --> 00:06:14.750
looks like this to a railer function
which we talked about briefly in an

00:06:14.750 --> 00:06:14.760
which we talked about briefly in an
 

00:06:14.760 --> 00:06:18.469
which we talked about briefly in an
early video that looks like this if you

00:06:18.469 --> 00:06:18.479
early video that looks like this if you
 

00:06:18.479 --> 00:06:20.180
early video that looks like this if you
don't understand the details of one

00:06:20.180 --> 00:06:20.190
don't understand the details of one
 

00:06:20.190 --> 00:06:22.250
don't understand the details of one
about the state don't worry about it but

00:06:22.250 --> 00:06:22.260
about the state don't worry about it but
 

00:06:22.260 --> 00:06:24.379
about the state don't worry about it but
it turns out that one of the problems of

00:06:24.379 --> 00:06:24.389
it turns out that one of the problems of
 

00:06:24.389 --> 00:06:26.000
it turns out that one of the problems of
using sigmoid functions and machine

00:06:26.000 --> 00:06:26.010
using sigmoid functions and machine
 

00:06:26.010 --> 00:06:27.860
using sigmoid functions and machine
learning is that there these regions

00:06:27.860 --> 00:06:27.870
learning is that there these regions
 

00:06:27.870 --> 00:06:29.510
learning is that there these regions
here where the slope of the function

00:06:29.510 --> 00:06:29.520
here where the slope of the function
 

00:06:29.520 --> 00:06:30.270
here where the slope of the function
would

00:06:30.270 --> 00:06:30.280
would
 

00:06:30.280 --> 00:06:32.910
would
gradient is nearly zero and so learning

00:06:32.910 --> 00:06:32.920
gradient is nearly zero and so learning
 

00:06:32.920 --> 00:06:35.340
gradient is nearly zero and so learning
becomes really slow because when you

00:06:35.340 --> 00:06:35.350
becomes really slow because when you
 

00:06:35.350 --> 00:06:37.050
becomes really slow because when you
implement gradient descent and gradient

00:06:37.050 --> 00:06:37.060
implement gradient descent and gradient
 

00:06:37.060 --> 00:06:39.629
implement gradient descent and gradient
is zero the parameters just change very

00:06:39.629 --> 00:06:39.639
is zero the parameters just change very
 

00:06:39.639 --> 00:06:41.460
is zero the parameters just change very
slowly and so learning is very slow

00:06:41.460 --> 00:06:41.470
slowly and so learning is very slow
 

00:06:41.470 --> 00:06:44.730
slowly and so learning is very slow
whereas by changing the what's called

00:06:44.730 --> 00:06:44.740
whereas by changing the what's called
 

00:06:44.740 --> 00:06:46.440
whereas by changing the what's called
the activation function the neural

00:06:46.440 --> 00:06:46.450
the activation function the neural
 

00:06:46.450 --> 00:06:48.590
the activation function the neural
network to use this function called the

00:06:48.590 --> 00:06:48.600
network to use this function called the
 

00:06:48.600 --> 00:06:52.050
network to use this function called the
value function of the rectified linear

00:06:52.050 --> 00:06:52.060
value function of the rectified linear
 

00:06:52.060 --> 00:06:54.960
value function of the rectified linear
unit our elu the gradient is equal to

00:06:54.960 --> 00:06:54.970
unit our elu the gradient is equal to
 

00:06:54.970 --> 00:06:57.060
unit our elu the gradient is equal to
one for all positive values of input

00:06:57.060 --> 00:06:57.070
one for all positive values of input
 

00:06:57.070 --> 00:07:00.210
one for all positive values of input
right and so the gradient is much less

00:07:00.210 --> 00:07:00.220
right and so the gradient is much less
 

00:07:00.220 --> 00:07:03.090
right and so the gradient is much less
likely to gradually shrink to zero and

00:07:03.090 --> 00:07:03.100
likely to gradually shrink to zero and
 

00:07:03.100 --> 00:07:04.740
likely to gradually shrink to zero and
the gradient here the slope of this line

00:07:04.740 --> 00:07:04.750
the gradient here the slope of this line
 

00:07:04.750 --> 00:07:07.290
the gradient here the slope of this line
is zero on the left but it turns out

00:07:07.290 --> 00:07:07.300
is zero on the left but it turns out
 

00:07:07.300 --> 00:07:09.510
is zero on the left but it turns out
that just by switching to the sigmoid

00:07:09.510 --> 00:07:09.520
that just by switching to the sigmoid
 

00:07:09.520 --> 00:07:12.570
that just by switching to the sigmoid
function to the rayleigh function has

00:07:12.570 --> 00:07:12.580
function to the rayleigh function has
 

00:07:12.580 --> 00:07:14.400
function to the rayleigh function has
made an algorithm called gradient

00:07:14.400 --> 00:07:14.410
made an algorithm called gradient
 

00:07:14.410 --> 00:07:16.950
made an algorithm called gradient
descent work much faster and so this is

00:07:16.950 --> 00:07:16.960
descent work much faster and so this is
 

00:07:16.960 --> 00:07:19.159
descent work much faster and so this is
an example of maybe relatively simple

00:07:19.159 --> 00:07:19.169
an example of maybe relatively simple
 

00:07:19.169 --> 00:07:22.020
an example of maybe relatively simple
algorithm in Bayesian but ultimately the

00:07:22.020 --> 00:07:22.030
algorithm in Bayesian but ultimately the
 

00:07:22.030 --> 00:07:23.850
algorithm in Bayesian but ultimately the
impact of this algorithmic innovation

00:07:23.850 --> 00:07:23.860
impact of this algorithmic innovation
 

00:07:23.860 --> 00:07:27.510
impact of this algorithmic innovation
was it really hope computation so the

00:07:27.510 --> 00:07:27.520
was it really hope computation so the
 

00:07:27.520 --> 00:07:29.070
was it really hope computation so the
regimen quite a lot of examples like

00:07:29.070 --> 00:07:29.080
regimen quite a lot of examples like
 

00:07:29.080 --> 00:07:31.230
regimen quite a lot of examples like
this of where we change the algorithm

00:07:31.230 --> 00:07:31.240
this of where we change the algorithm
 

00:07:31.240 --> 00:07:33.330
this of where we change the algorithm
because it allows that code to run much

00:07:33.330 --> 00:07:33.340
because it allows that code to run much
 

00:07:33.340 --> 00:07:35.130
because it allows that code to run much
faster and this allows us to train

00:07:35.130 --> 00:07:35.140
faster and this allows us to train
 

00:07:35.140 --> 00:07:37.469
faster and this allows us to train
bigger neural networks or to do so the

00:07:37.469 --> 00:07:37.479
bigger neural networks or to do so the
 

00:07:37.479 --> 00:07:39.510
bigger neural networks or to do so the
reason or multi-client even when we have

00:07:39.510 --> 00:07:39.520
reason or multi-client even when we have
 

00:07:39.520 --> 00:07:42.240
reason or multi-client even when we have
a large network roam all the data the

00:07:42.240 --> 00:07:42.250
a large network roam all the data the
 

00:07:42.250 --> 00:07:45.800
a large network roam all the data the
other reason that fast computation is

00:07:45.800 --> 00:07:45.810
other reason that fast computation is
 

00:07:45.810 --> 00:07:48.600
other reason that fast computation is
important is that it turns out the

00:07:48.600 --> 00:07:48.610
important is that it turns out the
 

00:07:48.610 --> 00:07:51.060
important is that it turns out the
process of training your network this is

00:07:51.060 --> 00:07:51.070
process of training your network this is
 

00:07:51.070 --> 00:07:53.700
process of training your network this is
very intuitive often you have an idea

00:07:53.700 --> 00:07:53.710
very intuitive often you have an idea
 

00:07:53.710 --> 00:07:56.340
very intuitive often you have an idea
for a neural network architecture and so

00:07:56.340 --> 00:07:56.350
for a neural network architecture and so
 

00:07:56.350 --> 00:07:58.010
for a neural network architecture and so
you implement your idea and code

00:07:58.010 --> 00:07:58.020
you implement your idea and code
 

00:07:58.020 --> 00:08:01.050
you implement your idea and code
implementing your idea then lets you run

00:08:01.050 --> 00:08:01.060
implementing your idea then lets you run
 

00:08:01.060 --> 00:08:02.820
implementing your idea then lets you run
an experiment which tells you how well

00:08:02.820 --> 00:08:02.830
an experiment which tells you how well
 

00:08:02.830 --> 00:08:05.040
an experiment which tells you how well
your neural network does and then by

00:08:05.040 --> 00:08:05.050
your neural network does and then by
 

00:08:05.050 --> 00:08:07.500
your neural network does and then by
looking at it you go back to change the

00:08:07.500 --> 00:08:07.510
looking at it you go back to change the
 

00:08:07.510 --> 00:08:10.020
looking at it you go back to change the
details of your new network and then you

00:08:10.020 --> 00:08:10.030
details of your new network and then you
 

00:08:10.030 --> 00:08:12.920
details of your new network and then you
go around this circle over and over and

00:08:12.920 --> 00:08:12.930
go around this circle over and over and
 

00:08:12.930 --> 00:08:15.870
go around this circle over and over and
when your new network takes a long time

00:08:15.870 --> 00:08:15.880
when your new network takes a long time
 

00:08:15.880 --> 00:08:18.540
when your new network takes a long time
to Train it just takes a long time to go

00:08:18.540 --> 00:08:18.550
to Train it just takes a long time to go
 

00:08:18.550 --> 00:08:21.390
to Train it just takes a long time to go
around this cycle and there's a huge

00:08:21.390 --> 00:08:21.400
around this cycle and there's a huge
 

00:08:21.400 --> 00:08:24.029
around this cycle and there's a huge
difference in your productivity building

00:08:24.029 --> 00:08:24.039
difference in your productivity building
 

00:08:24.039 --> 00:08:26.730
difference in your productivity building
effective neural networks when you can

00:08:26.730 --> 00:08:26.740
effective neural networks when you can
 

00:08:26.740 --> 00:08:29.550
effective neural networks when you can
have an idea and try it and see the work

00:08:29.550 --> 00:08:29.560
have an idea and try it and see the work
 

00:08:29.560 --> 00:08:34.159
have an idea and try it and see the work
in ten minutes or maybe ammos a day

00:08:34.159 --> 00:08:34.169
in ten minutes or maybe ammos a day
 

00:08:34.169 --> 00:08:36.360
in ten minutes or maybe ammos a day
versus if you've to train your neural

00:08:36.360 --> 00:08:36.370
versus if you've to train your neural
 

00:08:36.370 --> 00:08:39.480
versus if you've to train your neural
network for a month which sometimes does

00:08:39.480 --> 00:08:39.490
network for a month which sometimes does
 

00:08:39.490 --> 00:08:40.580
network for a month which sometimes does
happened

00:08:40.580 --> 00:08:40.590
happened
 

00:08:40.590 --> 00:08:42.560
happened
because you get a result back you know

00:08:42.560 --> 00:08:42.570
because you get a result back you know
 

00:08:42.570 --> 00:08:44.660
because you get a result back you know
in ten minutes or maybe in a day you

00:08:44.660 --> 00:08:44.670
in ten minutes or maybe in a day you
 

00:08:44.670 --> 00:08:47.240
in ten minutes or maybe in a day you
should just try a lot more ideas and be

00:08:47.240 --> 00:08:47.250
should just try a lot more ideas and be
 

00:08:47.250 --> 00:08:49.160
should just try a lot more ideas and be
much more likely to discover in your

00:08:49.160 --> 00:08:49.170
much more likely to discover in your
 

00:08:49.170 --> 00:08:50.600
much more likely to discover in your
network and it works well for your

00:08:50.600 --> 00:08:50.610
network and it works well for your
 

00:08:50.610 --> 00:08:53.710
network and it works well for your
application and so faster computation

00:08:53.710 --> 00:08:53.720
application and so faster computation
 

00:08:53.720 --> 00:08:57.890
application and so faster computation
has really helped in terms of speeding

00:08:57.890 --> 00:08:57.900
has really helped in terms of speeding
 

00:08:57.900 --> 00:08:59.720
has really helped in terms of speeding
up the rate at which you can get an

00:08:59.720 --> 00:08:59.730
up the rate at which you can get an
 

00:08:59.730 --> 00:09:02.600
up the rate at which you can get an
experimental result back and this has

00:09:02.600 --> 00:09:02.610
experimental result back and this has
 

00:09:02.610 --> 00:09:05.390
experimental result back and this has
really helped both practitioners of

00:09:05.390 --> 00:09:05.400
really helped both practitioners of
 

00:09:05.400 --> 00:09:07.540
really helped both practitioners of
neuro networks as well as researchers

00:09:07.540 --> 00:09:07.550
neuro networks as well as researchers
 

00:09:07.550 --> 00:09:10.640
neuro networks as well as researchers
working and deep learning iterate much

00:09:10.640 --> 00:09:10.650
working and deep learning iterate much
 

00:09:10.650 --> 00:09:13.310
working and deep learning iterate much
faster and improve your ideas much

00:09:13.310 --> 00:09:13.320
faster and improve your ideas much
 

00:09:13.320 --> 00:09:16.579
faster and improve your ideas much
faster and so all this has also been a

00:09:16.579 --> 00:09:16.589
faster and so all this has also been a
 

00:09:16.589 --> 00:09:18.560
faster and so all this has also been a
huge boon to the entire deep learning

00:09:18.560 --> 00:09:18.570
huge boon to the entire deep learning
 

00:09:18.570 --> 00:09:21.019
huge boon to the entire deep learning
research community which has been

00:09:21.019 --> 00:09:21.029
research community which has been
 

00:09:21.029 --> 00:09:23.360
research community which has been
incredible with just you know inventing

00:09:23.360 --> 00:09:23.370
incredible with just you know inventing
 

00:09:23.370 --> 00:09:25.610
incredible with just you know inventing
new algorithms and making nonstop

00:09:25.610 --> 00:09:25.620
new algorithms and making nonstop
 

00:09:25.620 --> 00:09:28.910
new algorithms and making nonstop
progress on that front so these are some

00:09:28.910 --> 00:09:28.920
progress on that front so these are some
 

00:09:28.920 --> 00:09:30.980
progress on that front so these are some
of the forces powering the rise of deep

00:09:30.980 --> 00:09:30.990
of the forces powering the rise of deep
 

00:09:30.990 --> 00:09:33.560
of the forces powering the rise of deep
learning but the good news is that these

00:09:33.560 --> 00:09:33.570
learning but the good news is that these
 

00:09:33.570 --> 00:09:35.990
learning but the good news is that these
forces are still working powerfully to

00:09:35.990 --> 00:09:36.000
forces are still working powerfully to
 

00:09:36.000 --> 00:09:38.480
forces are still working powerfully to
make deep learning even better Tech Data

00:09:38.480 --> 00:09:38.490
make deep learning even better Tech Data
 

00:09:38.490 --> 00:09:41.120
make deep learning even better Tech Data
society is still throwing up one more

00:09:41.120 --> 00:09:41.130
society is still throwing up one more
 

00:09:41.130 --> 00:09:43.790
society is still throwing up one more
digital data or take computation with

00:09:43.790 --> 00:09:43.800
digital data or take computation with
 

00:09:43.800 --> 00:09:45.650
digital data or take computation with
the rise of specialized hardware like

00:09:45.650 --> 00:09:45.660
the rise of specialized hardware like
 

00:09:45.660 --> 00:09:48.290
the rise of specialized hardware like
GPUs and faster networking many types of

00:09:48.290 --> 00:09:48.300
GPUs and faster networking many types of
 

00:09:48.300 --> 00:09:50.930
GPUs and faster networking many types of
hardware I'm actually quite confident

00:09:50.930 --> 00:09:50.940
hardware I'm actually quite confident
 

00:09:50.940 --> 00:09:53.240
hardware I'm actually quite confident
that our ability to do very large neural

00:09:53.240 --> 00:09:53.250
that our ability to do very large neural
 

00:09:53.250 --> 00:09:55.130
that our ability to do very large neural
networks or should a computation point

00:09:55.130 --> 00:09:55.140
networks or should a computation point
 

00:09:55.140 --> 00:09:57.310
networks or should a computation point
of view will keep on getting better and

00:09:57.310 --> 00:09:57.320
of view will keep on getting better and
 

00:09:57.320 --> 00:10:00.350
of view will keep on getting better and
take algorithms relative learning

00:10:00.350 --> 00:10:00.360
take algorithms relative learning
 

00:10:00.360 --> 00:10:02.870
take algorithms relative learning
research communities though continuously

00:10:02.870 --> 00:10:02.880
research communities though continuously
 

00:10:02.880 --> 00:10:05.060
research communities though continuously
phenomenal at innovating on the

00:10:05.060 --> 00:10:05.070
phenomenal at innovating on the
 

00:10:05.070 --> 00:10:07.670
phenomenal at innovating on the
algorithms front so because of this I

00:10:07.670 --> 00:10:07.680
algorithms front so because of this I
 

00:10:07.680 --> 00:10:09.829
algorithms front so because of this I
think that we can be optimistic answer

00:10:09.829 --> 00:10:09.839
think that we can be optimistic answer
 

00:10:09.839 --> 00:10:11.360
think that we can be optimistic answer
the optimistic the deep learning will

00:10:11.360 --> 00:10:11.370
the optimistic the deep learning will
 

00:10:11.370 --> 00:10:13.640
the optimistic the deep learning will
keep on getting better for many years to

00:10:13.640 --> 00:10:13.650
keep on getting better for many years to
 

00:10:13.650 --> 00:10:14.110
keep on getting better for many years to
come

00:10:14.110 --> 00:10:14.120
come
 

00:10:14.120 --> 00:10:17.090
come
so that let's go on to the last video of

00:10:17.090 --> 00:10:17.100
so that let's go on to the last video of
 

00:10:17.100 --> 00:10:18.530
so that let's go on to the last video of
the section where we'll talk a little

00:10:18.530 --> 00:10:18.540
the section where we'll talk a little
 

00:10:18.540 --> 00:10:20.270
the section where we'll talk a little
bit more about what you learn from this

00:10:20.270 --> 00:10:20.280
bit more about what you learn from this
 

00:10:20.280 --> 00:10:22.610
bit more about what you learn from this
course

