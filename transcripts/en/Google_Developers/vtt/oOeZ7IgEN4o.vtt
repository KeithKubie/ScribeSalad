WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:03.061
[MUSIC PLAYING]

00:00:03.560 --> 00:00:04.900
LILY PENG: Hi, everyone.

00:00:04.900 --> 00:00:06.360
So, I'm Lily.

00:00:06.360 --> 00:00:11.430
I work on our medical
imaging team in Brain.

00:00:11.430 --> 00:00:13.530
In a previous life,
I was a doctor,

00:00:13.530 --> 00:00:16.437
and I've been repurposed as
a product manager at Google.

00:00:16.437 --> 00:00:18.180
[LAUGHTER]

00:00:20.460 --> 00:00:24.120
One of the projects that we've
been working on in our group

00:00:24.120 --> 00:00:29.940
is using deep learning
for retinal imaging.

00:00:29.940 --> 00:00:32.490
In particular, we are
looking at a disease

00:00:32.490 --> 00:00:34.995
called diabetic retinopathy.

00:00:34.995 --> 00:00:37.740
Other than a mouthful,
it's actually also

00:00:37.740 --> 00:00:41.520
the fastest growing cause
of blindness in the world,

00:00:41.520 --> 00:00:44.265
and it's because this is a
complication of diabetes.

00:00:48.760 --> 00:00:51.930
There are 415 million people
in the world with diabetes,

00:00:51.930 --> 00:00:54.630
and each one of them
is at risk for going

00:00:54.630 --> 00:01:01.200
blind due to what we call
DR, or Diabetic Retinopathy.

00:01:01.200 --> 00:01:04.610
The key to preventing
blindness is regular screening.

00:01:04.610 --> 00:01:08.790
Every guideline worldwide
recommends about once a year

00:01:08.790 --> 00:01:11.924
screening, and it's because
this is pretty asymptomatic

00:01:11.924 --> 00:01:14.340
until you get to a point where
there's irreversible vision

00:01:14.340 --> 00:01:14.940
loss.

00:01:14.940 --> 00:01:19.230
And at that point, it's a
little too late to intervene.

00:01:19.230 --> 00:01:22.980
This is done by taking a picture
using a specialized camera

00:01:22.980 --> 00:01:25.230
of the back of the eye
through your pupil,

00:01:25.230 --> 00:01:28.470
and then a doctor
grades these images.

00:01:28.470 --> 00:01:32.370
We look for these little
hemorrhages and little spots

00:01:32.370 --> 00:01:36.570
on the image, and we grade
them on a five-class scale

00:01:36.570 --> 00:01:39.000
from no disease to
the end stage, which

00:01:39.000 --> 00:01:42.070
is sort of proliferative DR.

00:01:42.070 --> 00:01:44.580
In many places in
the world, including

00:01:44.580 --> 00:01:47.355
in India where our
story originated,

00:01:47.355 --> 00:01:51.530
there are just simply not
enough doctors to do this task.

00:01:51.530 --> 00:01:55.650
In India, there is a shortage
of 127,000-some eye doctors,

00:01:55.650 --> 00:01:58.980
and because of this and
other systematic issues,

00:01:58.980 --> 00:02:02.820
about half of people actually
suffer vision loss before

00:02:02.820 --> 00:02:04.477
they're even diagnosed.

00:02:04.477 --> 00:02:06.310
For something that's
completely preventable,

00:02:06.310 --> 00:02:08.310
this is sort of unacceptable.

00:02:08.310 --> 00:02:11.670
Here is a picture of
some of the people

00:02:11.670 --> 00:02:13.920
who are waiting in
line to get screened.

00:02:13.920 --> 00:02:17.040
Even if you get to a place
where there is screening,

00:02:17.040 --> 00:02:18.125
there is a long wait.

00:02:18.125 --> 00:02:21.780
There is long turnaround
time, and so a lot of people

00:02:21.780 --> 00:02:24.600
end up being lost to care.

00:02:24.600 --> 00:02:27.780
The other issue is that
even when available,

00:02:27.780 --> 00:02:32.010
doctors are
surprisingly variable.

00:02:32.010 --> 00:02:36.000
Here in this graph, each color
represents a different class

00:02:36.000 --> 00:02:38.910
of category of
disease, and each row

00:02:38.910 --> 00:02:42.000
is a patient image
of that fundus image,

00:02:42.000 --> 00:02:44.580
and each column represents
an ophthalmologist.

00:02:44.580 --> 00:02:46.500
These are US board-certified
ophthalmologists,

00:02:46.500 --> 00:02:49.350
and we had given
them the test set

00:02:49.350 --> 00:02:52.230
when we were trying
to attack this problem

00:02:52.230 --> 00:02:56.580
to see what the grades
were for each of them.

00:02:56.580 --> 00:02:59.910
And as you can see,
when there's no disease,

00:02:59.910 --> 00:03:01.520
there is pretty good agreement.

00:03:01.520 --> 00:03:03.570
There's one person
who thinks otherwise,

00:03:03.570 --> 00:03:07.290
but everyone is the
consensus is there.

00:03:07.290 --> 00:03:09.582
And then, of course, you
look at the end stage

00:03:09.582 --> 00:03:11.040
when there's
proliferative disease,

00:03:11.040 --> 00:03:12.900
there's good
agreement there, too.

00:03:12.900 --> 00:03:14.820
But in between,
there's actually a lot

00:03:14.820 --> 00:03:17.359
of variability and
disagreement about where

00:03:17.359 --> 00:03:19.150
this should actually
fit, even though there

00:03:19.150 --> 00:03:20.640
are pretty well-known
guidelines,

00:03:20.640 --> 00:03:22.306
and it's because human
beings in general

00:03:22.306 --> 00:03:27.360
just aren't super great at
being very precise about what

00:03:27.360 --> 00:03:28.890
we see in that image.

00:03:28.890 --> 00:03:31.530
And of course, you can see
the two highlighted rows

00:03:31.530 --> 00:03:32.880
there in black.

00:03:32.880 --> 00:03:35.040
These images got every
grade in the book, right?

00:03:35.040 --> 00:03:37.440
So depending on who you
saw, your management

00:03:37.440 --> 00:03:40.140
would be kind of different.

00:03:40.140 --> 00:03:44.250
A little bit more
about that later.

00:03:44.250 --> 00:03:47.730
Where we thought we could
help was let's train a model.

00:03:47.730 --> 00:03:51.570
And so we actually
built a labeling tool.

00:03:51.570 --> 00:03:54.210
We started off with
130,000 images,

00:03:54.210 --> 00:03:56.280
and we've gotten
much more since then,

00:03:56.280 --> 00:03:58.680
and we hired an army
of ophthalmologists

00:03:58.680 --> 00:04:00.660
to help us label.

00:04:00.660 --> 00:04:03.360
And from our 54
ophthalmologists,

00:04:03.360 --> 00:04:07.440
we got 880,000 diagnoses
for these images.

00:04:07.440 --> 00:04:09.510
And you can see from
the previous slide

00:04:09.510 --> 00:04:12.780
why we did that, because
sometimes it took up to seven

00:04:12.780 --> 00:04:17.100
reads to get
something consistent.

00:04:17.100 --> 00:04:19.500
Then what we did after
we got this data cleaned

00:04:19.500 --> 00:04:23.340
up and labeled, we used
our trusty, dusty inception

00:04:23.340 --> 00:04:26.680
network that works for a lot
of image recognition tasks,

00:04:26.680 --> 00:04:30.840
from cats to puppies to
melanoma, and now DR.

00:04:30.840 --> 00:04:35.400
And we trained it to detect
these five class predictions,

00:04:35.400 --> 00:04:38.749
but we also asked it to
predict housekeeping things

00:04:38.749 --> 00:04:40.290
that may be important
for a clinician

00:04:40.290 --> 00:04:42.690
to know-- whether
or not this image is

00:04:42.690 --> 00:04:44.289
of sufficient
quality for grading,

00:04:44.289 --> 00:04:46.080
whether or not this is
a left or right eye.

00:04:46.080 --> 00:04:48.030
Sometimes we get confused.

00:04:48.030 --> 00:04:49.680
And also the field
of view, which

00:04:49.680 --> 00:04:54.840
is like what part of the
retina you're actually seeing.

00:04:54.840 --> 00:04:57.450
And then we built a
front end to this.

00:04:57.450 --> 00:05:00.540
I'm going to try a demo here.

00:05:00.540 --> 00:05:04.090
This is literally
what I call a toaster.

00:05:04.090 --> 00:05:06.040
We try to drag and
drop something.

00:05:06.040 --> 00:05:10.209
I don't know actually how
to-- how do I move the cursor?

00:05:10.209 --> 00:05:10.750
Oh, there we.

00:05:10.750 --> 00:05:15.250
Are So I'm going to
open up a web browser,

00:05:15.250 --> 00:05:17.590
and hopefully that works.

00:05:17.590 --> 00:05:23.890
I'm going to drag one
of our images over.

00:05:23.890 --> 00:05:24.880
I can't see it.

00:05:24.880 --> 00:05:26.360
Oh, there we go.

00:05:26.360 --> 00:05:30.830
And it analyzing.

00:05:30.830 --> 00:05:32.540
It should be
faster, but the demo

00:05:32.540 --> 00:05:35.230
gods-- oh, it's cooperating.

00:05:35.230 --> 00:05:38.076
So here we are able to
tell you that there's

00:05:38.076 --> 00:05:39.200
proliferative disease here.

00:05:39.200 --> 00:05:41.030
There is no what
we call DMU, which

00:05:41.030 --> 00:05:45.022
is a different
type of DR. And we

00:05:45.022 --> 00:05:46.730
are saying that this
is somewhere between

00:05:46.730 --> 00:05:50.300
moderate and severe, and
this is indeed something

00:05:50.300 --> 00:05:52.640
between moderate and severe.

00:05:52.640 --> 00:05:55.220
I kind of showed you how it
works on a case-by-case basis,

00:05:55.220 --> 00:05:59.510
but then how does it work
over a lot of images?

00:05:59.510 --> 00:06:02.540
Well, here we actually
published and shared

00:06:02.540 --> 00:06:05.930
how we did this work in the
"Journal of the American

00:06:05.930 --> 00:06:09.860
Medical Association," and
this is one of the tests

00:06:09.860 --> 00:06:11.570
or the validation
sets that we use.

00:06:11.570 --> 00:06:15.620
The model was not trained or
tested on this previously.

00:06:15.620 --> 00:06:21.470
And out of 9,963 images,
we predicted whether or not

00:06:21.470 --> 00:06:23.660
it had referrable disease.

00:06:23.660 --> 00:06:26.030
The y-axis is sensitivity.

00:06:26.030 --> 00:06:28.640
The x-axis is 1
minus specificity.

00:06:28.640 --> 00:06:31.730
And our algorithm and
the two black dots,

00:06:31.730 --> 00:06:35.155
if you can actually
see it, is in black.

00:06:35.155 --> 00:06:36.030
That's our algorithm.

00:06:36.030 --> 00:06:37.530
And then the little
colored dots are

00:06:37.530 --> 00:06:41.190
US board-certified
ophthalmologists.

00:06:41.190 --> 00:06:43.820
And to the left is good, and you
can see that essentially we're

00:06:43.820 --> 00:06:46.340
very close to most of the
ophthalmologists in terms

00:06:46.340 --> 00:06:47.760
of performance.

00:06:47.760 --> 00:06:50.150
And in fact, if you
look at our F-score,

00:06:50.150 --> 00:06:53.360
and you compare the algorithm's
F-score to that of the median,

00:06:53.360 --> 00:06:56.720
ophthalmologists were sort
of in the middle of the pack.

00:06:56.720 --> 00:06:59.720
One of the reasons we also
decided to publish in JAMA

00:06:59.720 --> 00:07:02.570
was because we believe that
engaging the medical community

00:07:02.570 --> 00:07:05.450
is really important to
get these technologies out

00:07:05.450 --> 00:07:08.760
into the hands of the people
who could actually use them.

00:07:08.760 --> 00:07:10.260
It was actually
quite well-received.

00:07:10.260 --> 00:07:14.360
You can see some quotes from
real doctors about our work,

00:07:14.360 --> 00:07:17.750
and so we're really
excited about that.

00:07:17.750 --> 00:07:19.400
How did TensorFlow help us?

00:07:19.400 --> 00:07:22.910
Well, every step
of the way, I think

00:07:22.910 --> 00:07:26.360
it helped us really start
with quick prototyping.

00:07:26.360 --> 00:07:28.610
We had started our architecture,
pre-trained models,

00:07:28.610 --> 00:07:32.650
and we actually were able to
try out different variations

00:07:32.650 --> 00:07:34.400
of neuro-networks, and
we actually found--

00:07:34.400 --> 00:07:36.250
I mean, we literally
found that Inception--

00:07:36.250 --> 00:07:37.470
B3 at this point--

00:07:37.470 --> 00:07:38.600
worked the best.

00:07:38.600 --> 00:07:41.330
But we could try out
things very quickly.

00:07:41.330 --> 00:07:42.410
And we also pre-trained.

00:07:42.410 --> 00:07:46.505
So we actually pre-trained
on the classic image net,

00:07:46.505 --> 00:07:50.870
and we found there was a
boost in performance there.

00:07:50.870 --> 00:07:54.020
It also helped us to
experiment at scale, so

00:07:54.020 --> 00:07:56.400
GPU support and fast training.

00:07:56.400 --> 00:07:58.760
This allows us to run all
these different experiments,

00:07:58.760 --> 00:08:00.980
different sort of labeling.

00:08:00.980 --> 00:08:03.040
If we had new labels
or different labels,

00:08:03.040 --> 00:08:05.390
it kind of helped us do that.

00:08:05.390 --> 00:08:07.880
And finally, what I
think is really important

00:08:07.880 --> 00:08:11.330
is that it really allowed our
team to reinvest the efforts,

00:08:11.330 --> 00:08:15.170
so the blocker was no
longer in machine learning

00:08:15.170 --> 00:08:16.160
and in training.

00:08:19.080 --> 00:08:22.850
That's been hard to do, and if
you look at what we did here,

00:08:22.850 --> 00:08:25.640
we actually applied very
straightforward ML techniques

00:08:25.640 --> 00:08:26.660
here.

00:08:26.660 --> 00:08:29.210
What was the magic
sauce was actually

00:08:29.210 --> 00:08:31.880
finding the right
problem, getting the data,

00:08:31.880 --> 00:08:35.200
getting agreement about
what was in the image,

00:08:35.200 --> 00:08:38.870
and then we were able to use
this package, these tools that

00:08:38.870 --> 00:08:42.049
were able to allow us to train
the models, that actually

00:08:42.049 --> 00:08:44.810
performed really, really well.

00:08:44.810 --> 00:08:46.910
And that also, then,
allows our team

00:08:46.910 --> 00:08:50.180
to focus on validating
the algorithm

00:08:50.180 --> 00:08:53.180
and figure out ways to deploy
it into health systems, which

00:08:53.180 --> 00:08:56.120
in itself is a huge challenge.

00:08:56.120 --> 00:08:57.140
What's next for us?

00:08:57.140 --> 00:08:58.010
We train a model.

00:08:58.010 --> 00:09:00.280
It works really well.

00:09:00.280 --> 00:09:03.110
Now we need to actually
clinically validate it.

00:09:03.110 --> 00:09:06.440
We've been working with two
hospitals in India, Aravind

00:09:06.440 --> 00:09:09.020
and Sankara, and they're
running clinical trials

00:09:09.020 --> 00:09:11.040
of the algorithm as we speak.

00:09:11.040 --> 00:09:12.640
Actually, Aravind's
finished, and they

00:09:12.640 --> 00:09:14.400
have found essentially
the same results--

00:09:14.400 --> 00:09:16.960
that we were slightly
better than the average

00:09:16.960 --> 00:09:19.250
of their ophthalmologists there.

00:09:19.250 --> 00:09:22.190
And so what we're doing is
working with a fellow Alphabet

00:09:22.190 --> 00:09:25.670
company, Verily, that's a
life science-focused company,

00:09:25.670 --> 00:09:28.020
and a hardware
maker called Nikon.

00:09:28.020 --> 00:09:30.620
You may not have heard
of that little company.

00:09:30.620 --> 00:09:34.430
But the idea is now
that the algorithm

00:09:34.430 --> 00:09:36.937
works pretty well,
the bottleneck becomes

00:09:36.937 --> 00:09:39.020
the hardware, because we
need a specialized camera

00:09:39.020 --> 00:09:40.980
to take these pictures.

00:09:40.980 --> 00:09:44.630
So we're working with the
hardware manufacturers

00:09:44.630 --> 00:09:48.500
to essentially figure out ways
to deploy lightweight hardware

00:09:48.500 --> 00:09:51.000
that's easy to use, et cetera.

00:09:54.980 --> 00:09:57.470
Taking a step back,
one of the main reasons

00:09:57.470 --> 00:10:01.250
I got into medicine
was I was an MD-PhD,

00:10:01.250 --> 00:10:04.850
and so I really was very excited
about bringing breakthrough

00:10:04.850 --> 00:10:07.340
science from bench to bedside.

00:10:07.340 --> 00:10:09.674
And there's a part of you,
when you go through training,

00:10:09.674 --> 00:10:11.673
and you're a PhD, and
you're like, this is never

00:10:11.673 --> 00:10:13.430
going to happen,
because it's just not

00:10:13.430 --> 00:10:15.562
possible to solve
these problems.

00:10:15.562 --> 00:10:18.020
And with TensorFlow and all
the work that's been done here,

00:10:18.020 --> 00:10:20.390
it's actually
possible to do that.

00:10:20.390 --> 00:10:21.890
It's possible to
train algorithms

00:10:21.890 --> 00:10:25.010
that really can help
physicians deliver care

00:10:25.010 --> 00:10:26.180
where people need it most.

00:10:26.180 --> 00:10:27.680
[APPLAUSE]

00:10:27.680 --> 00:10:31.330
[MUSIC PLAYING]

