1
00:00:00,770 --> 00:00:05,410
The real question is like who
gets to decide what is offensive,

2
00:00:05,411 --> 00:00:08,410
what is not? I'm sure you're
aware of the learn to code fiasco.

3
00:00:08,800 --> 00:00:13,800
Uh Oh I her for saying learn to code
and it was really mocking this idea that

4
00:00:18,071 --> 00:00:21,610
people were telling coal miners who
are losing their jobs, you know,

5
00:00:21,611 --> 00:00:24,370
hey there's jobs in computer
programming and like let this out.

6
00:00:24,670 --> 00:00:25,990
You should learn to code.

7
00:00:26,260 --> 00:00:30,490
And so people started mocking people by
saying learn to code and then learn to

8
00:00:30,491 --> 00:00:33,430
code. Apparently according
to Jack Dorsey and Vigia,

9
00:00:33,490 --> 00:00:38,490
it got connected to a antisemitic remarks
and hate remarks and [inaudible] level

10
00:00:40,361 --> 00:00:42,730
of Internet. This is data, that nonsense.

11
00:00:42,731 --> 00:00:45,610
It's fascinating because that doesn't
mean anything like learn to code is not

12
00:00:45,611 --> 00:00:46,444
offensive.
It's like,

13
00:00:46,570 --> 00:00:50,140
well it's ridiculous to ask a 50 year
old man as a coal miner to learn to code

14
00:00:50,141 --> 00:00:53,230
and it doesn't have a formal education.
That is ridiculous.

15
00:00:53,440 --> 00:00:55,960
But I mean the fact that you get
banned for life from saying that,

16
00:00:56,170 --> 00:00:57,490
that's actually even more ridiculous.

17
00:00:57,910 --> 00:01:02,890
Yeah. But then there are cases where
people go and they try to create like a,

18
00:01:03,340 --> 00:01:07,010
if someone, if someone who look there
are antisemites out there, right?

19
00:01:07,210 --> 00:01:11,830
They do try to come up with like ways
to indicate antisemitism to each other

20
00:01:11,831 --> 00:01:15,340
that other people won't detect. You
know what I mean? Via, you know, slang,

21
00:01:15,370 --> 00:01:18,490
basically inner slang. Right. Um, and uh,

22
00:01:18,491 --> 00:01:21,250
at some point someone needs to be able to
say, okay, wait, hold on second, this is,

23
00:01:21,260 --> 00:01:24,340
we figured out this 87 x slang,
so we're not gonna allow you to say,

24
00:01:24,341 --> 00:01:26,170
you know what I mean? It's
like shit, like 88 or what?

25
00:01:26,171 --> 00:01:27,420
Isn't that a thing where it's like,
yeah,

26
00:01:27,470 --> 00:01:29,420
doubt. Yeah, age, age.
This is though Hitler

27
00:01:29,760 --> 00:01:31,230
the problem, right? This is the,

28
00:01:31,231 --> 00:01:35,390
this is the exact problem that you're
talking about. Um, so, but the contrary,

29
00:01:35,420 --> 00:01:38,250
the contradiction that all of
these platforms have, right,

30
00:01:38,820 --> 00:01:41,520
is the early days of the Internet,
remember the early days,

31
00:01:41,550 --> 00:01:46,550
it was like people were really concerned
that people would start suing websites

32
00:01:48,061 --> 00:01:51,300
because of what was on the website,
right? Like, um, like the pirate bay,

33
00:01:51,301 --> 00:01:52,530
the big torrent site,
you know,

34
00:01:52,531 --> 00:01:54,780
like you're gonna get sued because
you've got DVD screeners on their, no,

35
00:01:54,781 --> 00:01:56,910
hold on a second. We're just, this is
just where people can upload the shit.

36
00:01:56,940 --> 00:02:00,060
You know, Google getting sued
because they would, you know,

37
00:02:00,330 --> 00:02:03,720
direct someone in the DVDs, Koreans,
they searched for leaks, DVD screener,

38
00:02:03,721 --> 00:02:04,554
you know what I mean?

39
00:02:04,740 --> 00:02:07,140
And so that was a big concern in the
early days of the Internet, right?

40
00:02:07,410 --> 00:02:09,480
And so we established this
precedent, they're like, no, no, no.

41
00:02:09,481 --> 00:02:13,410
These sites don't have responsibility
for that right there just how people are

42
00:02:13,411 --> 00:02:15,930
connecting to things. They're not
the people doing the bad shit, right?

43
00:02:16,080 --> 00:02:17,130
You go up to the people doing the batch,

44
00:02:17,210 --> 00:02:22,110
not the people who made it possible to
find the bad shit. Right now though,

45
00:02:22,200 --> 00:02:24,030
we are in such a place where,

46
00:02:24,090 --> 00:02:27,000
so all these businesses built themselves
on the idea of youtube, right?

47
00:02:27,180 --> 00:02:29,310
We don't make anything at Youtube.

48
00:02:29,460 --> 00:02:33,210
We just give you a place to upload your
videos, right? So at first that's fine.

49
00:02:33,240 --> 00:02:36,200
All right, just take down the antisemitic,
you know, white supremacy videos, right?

50
00:02:36,270 --> 00:02:40,800
Whatever. Right? But now
there's so, so many of them.
Right? And also not only that,

51
00:02:40,801 --> 00:02:44,520
Youtube's algorithm is directing people
towards them and youtube is selling ads

52
00:02:44,521 --> 00:02:49,470
against them and making money at them.
Right? Um, and at the same time, like,

53
00:02:49,471 --> 00:02:53,280
you know, these, these videos
exist, right? Um, uh, and uh,

54
00:02:53,281 --> 00:02:54,600
at the same time they're
still trying to say, well,

55
00:02:54,601 --> 00:02:56,910
we have no responsibility for that
happening. It's like, hold on a second.

56
00:02:56,911 --> 00:03:01,911
You guys built a system where any kind
of content is allowed and you are,

57
00:03:02,140 --> 00:03:02,371
you,

58
00:03:02,371 --> 00:03:04,990
you've also built the sentence and that's
directing people to that content and

59
00:03:04,991 --> 00:03:07,060
you build a system that's making
money off of that content.

60
00:03:07,240 --> 00:03:09,250
I think you guys have a little
bit of responsibility. Now.

61
00:03:09,280 --> 00:03:12,790
I agree that the question of
who polices it or whatever,

62
00:03:12,791 --> 00:03:15,190
that's an extremely
complicated conversation.

63
00:03:15,191 --> 00:03:17,680
But like that's what I'm just saying
about these companies trying to have it

64
00:03:17,681 --> 00:03:18,371
both ways.

65
00:03:18,371 --> 00:03:22,360
They're trying to say we have no
responsibility for what's on the platform,

66
00:03:22,361 --> 00:03:26,950
but also we've allowed this kind
of content to go up. You know,

67
00:03:26,980 --> 00:03:31,690
I don't know if you to
profits on antisemitic videos.

68
00:03:31,720 --> 00:03:34,420
I don't know. Having a test,
I talk about it in my show.

69
00:03:34,820 --> 00:03:39,820
Demonetizing aspect of a youtube that
affects people whenever anything's even

70
00:03:40,361 --> 00:03:44,290
remotely controversial,
controversial. They've started,
they've started doing that,

71
00:03:44,350 --> 00:03:48,820
um, up until like, there's a case to talk
about in my show, um, uh, like a year,

72
00:03:48,821 --> 00:03:52,900
year or two ago where they
were like running under
Armour brand under Armour ads

73
00:03:52,930 --> 00:03:56,300
on like white supremacy, youtube videos,
you know what I mean? Yeah. And that,

74
00:03:56,301 --> 00:03:56,860
that was like,

75
00:03:56,860 --> 00:03:59,800
and their algorithm was
causing this algorithm was
causing that to happen. Right?

76
00:03:59,801 --> 00:04:02,290
And these are videos getting a hundred
thousands of hits and it was like major

77
00:04:02,291 --> 00:04:05,890
brands and those brands found out, oh
wow. And this is the title of these video.

78
00:04:05,891 --> 00:04:07,200
Like what kind of get on with,

79
00:04:07,210 --> 00:04:11,260
they say obvious antisemitic things in
these were obvious enough that anybody

80
00:04:11,261 --> 00:04:14,140
would be pissed off about
him. Yeah. Um, and uh,

81
00:04:14,170 --> 00:04:17,470
so that's where the demonetization thing
came from, right? Because they're like,

82
00:04:17,500 --> 00:04:22,330
oh shit. Now the people who actually
pay us, the advertisers are pissed off,

83
00:04:22,360 --> 00:04:23,260
right? Yeah. So, okay,

84
00:04:23,261 --> 00:04:25,960
let's put a bandaid on the problem
and let's demonetize videos. Right.

85
00:04:26,140 --> 00:04:27,730
Here's the problem.
Now they're doing that.

86
00:04:27,731 --> 00:04:29,380
They're just doing that algorithmically,
right?

87
00:04:29,381 --> 00:04:31,360
They're choosing which videos to
demonetize monetize algorithmically.

88
00:04:31,361 --> 00:04:33,550
So sometimes they demonetize
stuff that they shouldn't.

89
00:04:33,760 --> 00:04:37,270
That's definitely happened a ton and a
lot of shit is still getting through the

90
00:04:37,271 --> 00:04:40,780
cracks. You know? And again, they're
saying, okay, we did what we did,

91
00:04:40,781 --> 00:04:41,561
what we had to know.

92
00:04:41,561 --> 00:04:44,500
You guys didn't solve the fucking
problem because now people are pissed off

93
00:04:44,501 --> 00:04:45,334
again.
Right?

94
00:04:45,580 --> 00:04:49,450
So that is the fine that these companies
are in there based on this premise of

95
00:04:49,451 --> 00:04:52,720
we don't moderate, moderate
anything. But when you do that,

96
00:04:52,870 --> 00:04:55,930
a lot of shit comes in and now you're
in the position where it, sorry,

97
00:04:55,931 --> 00:04:58,090
it's still your house.
The Shit's happened in your house.

98
00:04:58,180 --> 00:05:01,360
You went through the House party
dude, like, like the base got broken.

99
00:05:01,360 --> 00:05:02,560
It's your fault.
At the end of the day,

100
00:05:02,590 --> 00:05:04,690
you have to take some responsibility
for it. And like, well,

101
00:05:04,691 --> 00:05:06,400
how am I supposed to police 200 kids?
I don't know.

102
00:05:06,401 --> 00:05:08,580
You're the one who threw
the party. Yeah. You know,

103
00:05:08,620 --> 00:05:11,710
that's a good analogy because
it's the scale that's the problem.

104
00:05:11,711 --> 00:05:15,490
It's probably more like 200,000 kids in
a house. Totally. Because it's theirs.

105
00:05:15,520 --> 00:05:17,520
It's unmanageable. Yeah.
When you think about it,

106
00:05:17,521 --> 00:05:20,500
how many different people are on youtube
and how many different countries are

107
00:05:20,501 --> 00:05:24,290
uploading videos and you
know, some of them are isis
beheading videos. Yeah. Those,

108
00:05:24,370 --> 00:05:24,790
those,

109
00:05:24,790 --> 00:05:28,960
like there's a bunch of cartel videos
that people have sent me to on Youtube and

110
00:05:28,961 --> 00:05:30,900
they stay up for a little while.
Yeah.

111
00:05:30,910 --> 00:05:33,550
You get to watch some horrible shit for
a little while before they catch on.

