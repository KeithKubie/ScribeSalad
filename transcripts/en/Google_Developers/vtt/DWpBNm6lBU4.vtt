WEBVTT
Kind: captions
Language: en

00:00:00.500 --> 00:00:02.510
Hi, I'm Simon Newton,
a software engineer

00:00:02.510 --> 00:00:04.170
on Google's Cloud
Networking team.

00:00:04.170 --> 00:00:06.170
In this talk, I'll cover
the two different types

00:00:06.170 --> 00:00:08.919
of load balancing available
on Google Compute Engine.

00:00:08.919 --> 00:00:10.460
This will demonstrate
how you can now

00:00:10.460 --> 00:00:12.150
take advantage of
Google's technology

00:00:12.150 --> 00:00:15.290
to improve the performance
of your own web applications.

00:00:15.290 --> 00:00:18.250
At Google, we spent the last 15
years building infrastructure

00:00:18.250 --> 00:00:20.940
to make our websites
faster and more reliable.

00:00:20.940 --> 00:00:24.230
Today, we use these systems
to serve YouTube, Maps, Gmail,

00:00:24.230 --> 00:00:26.590
Search, and many other products.

00:00:26.590 --> 00:00:29.042
But first, let's go
back in time to 1999,

00:00:29.042 --> 00:00:31.250
and take a look at the
evolution of Google's frontend

00:00:31.250 --> 00:00:33.269
serving infrastructure.

00:00:33.269 --> 00:00:35.310
In the beginning, Google's
serving infrastructure

00:00:35.310 --> 00:00:37.520
was very simple, since
we only had a single data

00:00:37.520 --> 00:00:39.200
center in California.

00:00:39.200 --> 00:00:41.160
We had a small number of
web servers configured

00:00:41.160 --> 00:00:43.350
behind a hardware load balancer.

00:00:43.350 --> 00:00:45.650
The hardware load balancer
had a virtual IP address,

00:00:45.650 --> 00:00:48.710
abbreviated as a VIP, which
was provided to clients

00:00:48.710 --> 00:00:50.590
by one of our four DNS servers.

00:00:50.590 --> 00:00:52.784
There was very little
redundancy in the system.

00:00:52.784 --> 00:00:54.950
So if there was ever a
problem with the data center,

00:00:54.950 --> 00:00:57.580
Google.com would be
down for everyone.

00:00:57.580 --> 00:01:00.300
In 2000, as more people
came to rely on Google,

00:01:00.300 --> 00:01:02.490
we added additional data
centers on the East Coast

00:01:02.490 --> 00:01:04.042
of the United States.

00:01:04.042 --> 00:01:06.000
With these data centers,
there was a second set

00:01:06.000 --> 00:01:09.090
of web servers and another
hardware load balancer.

00:01:09.090 --> 00:01:11.540
When clients resolved
www.google.com,

00:01:11.540 --> 00:01:14.710
they were randomly sent to
one of the two locations.

00:01:14.710 --> 00:01:17.210
This offered up some redundancy,
but increased latency

00:01:17.210 --> 00:01:18.610
for many of our users.

00:01:18.610 --> 00:01:20.300
Half the time, clients
on the West Coast

00:01:20.300 --> 00:01:22.730
were sent to the other
side of the country.

00:01:22.730 --> 00:01:24.500
To improve the
situation, we switched

00:01:24.500 --> 00:01:26.780
to geolocating DNS servers.

00:01:26.780 --> 00:01:29.510
The DNS servers would use the
IP address of the clients DNS

00:01:29.510 --> 00:01:32.370
resolver to determine
which VIP to return.

00:01:32.370 --> 00:01:34.500
This meant that in most
cases, user requests

00:01:34.500 --> 00:01:37.210
was sent to the
closest data center.

00:01:37.210 --> 00:01:38.980
However, sometimes
during peak times,

00:01:38.980 --> 00:01:41.430
the closest data center
had insufficient capacity,

00:01:41.430 --> 00:01:43.260
so DNS servers would
send the clients

00:01:43.260 --> 00:01:45.240
to the next closest data center.

00:01:45.240 --> 00:01:47.720
Overflowing requests in this
way was bad for our users,

00:01:47.720 --> 00:01:49.386
because it meant we
weren't serving them

00:01:49.386 --> 00:01:51.410
from the optimal location,
and so their latency

00:01:51.410 --> 00:01:53.140
was higher than normal.

00:01:53.140 --> 00:01:54.850
At Google, we refer
to this method

00:01:54.850 --> 00:01:58.800
of balancing requests using
DNS as frontend load balancing.

00:01:58.800 --> 00:02:00.890
By this time, Google was
offering more products

00:02:00.890 --> 00:02:03.980
than just web search, and we
needed a way to direct requests

00:02:03.980 --> 00:02:05.510
based on their URL.

00:02:05.510 --> 00:02:07.690
We introduced a new system
called the Google Front

00:02:07.690 --> 00:02:09.680
End or GFE.

00:02:09.680 --> 00:02:12.320
The GFE accepts the
client's TCP connection

00:02:12.320 --> 00:02:14.650
and inspects the host
header and URL path

00:02:14.650 --> 00:02:16.430
to determine which
backend service should

00:02:16.430 --> 00:02:18.030
handle the request.

00:02:18.030 --> 00:02:19.770
Backend services are
groups of servers

00:02:19.770 --> 00:02:22.090
that can handle a
particular class of traffic.

00:02:22.090 --> 00:02:24.095
For example, requests
to maps on google.com

00:02:24.095 --> 00:02:25.690
will be sent to
our maps servers,

00:02:25.690 --> 00:02:27.486
while requests to
news.google.com

00:02:27.486 --> 00:02:29.950
will be sent to
our news servers.

00:02:29.950 --> 00:02:33.380
The GFE is also responsible for
health checking their backends.

00:02:33.380 --> 00:02:35.910
If a backend server fails
to respond to health checks,

00:02:35.910 --> 00:02:39.190
the GFEs will stop sending
traffic to the failed backend.

00:02:39.190 --> 00:02:41.670
By carefully tuning the GFE
health checking parameters,

00:02:41.670 --> 00:02:44.500
we were able to upgrade the
kernels or binaries of the GFE

00:02:44.500 --> 00:02:48.250
backends without disrupting
any user requests.

00:02:48.250 --> 00:02:51.020
The GFEs also maintain
persistent TCP connection

00:02:51.020 --> 00:02:52.850
to their backends,
so the connections

00:02:52.850 --> 00:02:55.540
are ready to use as soon
as the request arrives.

00:02:55.540 --> 00:02:59.600
This also helps us reduce
latency for our users.

00:02:59.600 --> 00:03:01.430
At this point, we
required the ability

00:03:01.430 --> 00:03:03.640
to serve users from a
different data center

00:03:03.640 --> 00:03:05.690
to the ones the GFEs
were running in.

00:03:05.690 --> 00:03:07.940
During maintenance windows
or failure scenarios,

00:03:07.940 --> 00:03:10.630
we want to continue terminating
the client's TCP connections

00:03:10.630 --> 00:03:13.230
at the local GFE, but then
use the backend service

00:03:13.230 --> 00:03:15.240
in an adjacent data center.

00:03:15.240 --> 00:03:18.320
To support this, we introduced
another load balancing system

00:03:18.320 --> 00:03:21.560
called the Global Software
Load Balancer or GSLB.

00:03:21.560 --> 00:03:23.460
This system allows us
to set per data center

00:03:23.460 --> 00:03:26.050
capacity for each
backend service.

00:03:26.050 --> 00:03:28.760
When the rate of incoming
requests for service

00:03:28.760 --> 00:03:30.740
exceeds the local
capacity, requests

00:03:30.740 --> 00:03:32.930
overflow to the next
closest location.

00:03:32.930 --> 00:03:35.650
And we call this
backend load balancing.

00:03:35.650 --> 00:03:38.340
As Google continued to expand,
we added new data centers

00:03:38.340 --> 00:03:41.170
in Europe, Asia,
and South America.

00:03:41.170 --> 00:03:43.362
By locating GFEs
closer to the users,

00:03:43.362 --> 00:03:44.820
we could reduce
the round trip time

00:03:44.820 --> 00:03:46.890
between the user and the GFE.

00:03:46.890 --> 00:03:48.880
This reduce the
TCP handshake time,

00:03:48.880 --> 00:03:53.190
and in turn, further decreased
the latency for our users.

00:03:53.190 --> 00:03:55.040
As Google continued
to grow, we found

00:03:55.040 --> 00:03:57.280
that off-the-shelf hardware
load balancers no longer

00:03:57.280 --> 00:04:00.050
met our performance and
scalability requirements.

00:04:00.050 --> 00:04:02.550
So we replaced all of the load
balancers with our own custom

00:04:02.550 --> 00:04:04.712
built solution called MagLev.

00:04:04.712 --> 00:04:06.920
This enabled us to experiment
with new load balancing

00:04:06.920 --> 00:04:10.850
algorithms and increase their
liability of the entire system.

00:04:10.850 --> 00:04:13.939
Finally, DNS-based load
balancing is far from perfect,

00:04:13.939 --> 00:04:15.980
since it can only load
balance at the granularity

00:04:15.980 --> 00:04:18.240
of a single DNS resolver.

00:04:18.240 --> 00:04:20.760
If two or more groups of
users are in different regions

00:04:20.760 --> 00:04:23.150
but behind the same DNS
resolver, some of them

00:04:23.150 --> 00:04:24.830
may be sent to the
wrong location.

00:04:24.830 --> 00:04:27.670
We often see this situation
with open resolvers.

00:04:27.670 --> 00:04:30.370
For example, if one group
of users is in India

00:04:30.370 --> 00:04:32.520
and the other is in Japan,
and both groups share

00:04:32.520 --> 00:04:35.980
the same resolver, we may end
up sending them all to Taiwan,

00:04:35.980 --> 00:04:37.910
rather than sending
each group of users

00:04:37.910 --> 00:04:40.302
to their best location.

00:04:40.302 --> 00:04:42.010
There are also some
resolvers and clients

00:04:42.010 --> 00:04:44.960
that do not want to know
the DNS record TTLs.

00:04:44.960 --> 00:04:46.670
Even with a short
five minute TTL,

00:04:46.670 --> 00:04:49.170
it can take a while for a load
to shift from one location

00:04:49.170 --> 00:04:50.140
to another.

00:04:50.140 --> 00:04:52.230
This makes us slow to
respond to outages,

00:04:52.230 --> 00:04:55.780
and leaves a long tail of
clients stuck to a single VIP.

00:04:55.780 --> 00:04:58.320
To avoid this, we've developed
a method of sending clients

00:04:58.320 --> 00:05:00.460
to the closest data
center without relying

00:05:00.460 --> 00:05:02.580
on DNS geolocation.

00:05:02.580 --> 00:05:04.210
This means we can
use a single VIP,

00:05:04.210 --> 00:05:06.376
and still give our customers
the low latency they've

00:05:06.376 --> 00:05:08.172
come to expect from Google.

00:05:08.172 --> 00:05:09.630
Google's global
load balancer knows

00:05:09.630 --> 00:05:12.050
where the clients are located
and directs the packets

00:05:12.050 --> 00:05:13.660
to the closest location.

00:05:13.660 --> 00:05:16.820
Using a single VIP means we
can increase the TTL of our DNS

00:05:16.820 --> 00:05:19.750
records which further
reduces latency.

00:05:19.750 --> 00:05:22.000
You can now take advantage
of this same infrastructure

00:05:22.000 --> 00:05:23.530
with Google Cloud Platform.

00:05:23.530 --> 00:05:28.530
We offer two types of load
balancing, network and HTTP.

00:05:28.530 --> 00:05:31.250
Network Load Balancing,
launched in 2013,

00:05:31.250 --> 00:05:33.952
swaps traffic over with
VMs within a region.

00:05:33.952 --> 00:05:35.410
It doesn't want
modify the packets,

00:05:35.410 --> 00:05:38.140
so VM instances can use the
source IP address of the client

00:05:38.140 --> 00:05:39.420
if required.

00:05:39.420 --> 00:05:43.580
Network Load Balancing can be
used with both TCP and UDP.

00:05:43.580 --> 00:05:47.050
The second and latest offering
is HTTP Load Balancing.

00:05:47.050 --> 00:05:50.660
HTTP Load Balancing offers
a single global IP address.

00:05:50.660 --> 00:05:53.090
It uses the latest techniques
described in this video

00:05:53.090 --> 00:05:54.940
to send requests
to the closest VM

00:05:54.940 --> 00:05:57.510
instance with
available capacity.

00:05:57.510 --> 00:06:00.400
This means DNS-based load
balancing is not required,

00:06:00.400 --> 00:06:04.450
so we avoid the issues with
split resolver populations.

00:06:04.450 --> 00:06:07.240
HTTP Load Balancing uses
Google's global deployment

00:06:07.240 --> 00:06:10.304
footprint by connecting
clients to the closest GFE.

00:06:10.304 --> 00:06:12.220
And taking advantage of
persistent connections

00:06:12.220 --> 00:06:15.440
to the VMs, we can reduce the
latency of web applications

00:06:15.440 --> 00:06:18.980
even if the VMs are
located in a single region.

00:06:18.980 --> 00:06:21.690
Advanced customers can also
take advantage of URL maps

00:06:21.690 --> 00:06:23.800
with a HTTP Load Balancing.

00:06:23.800 --> 00:06:25.760
Just as Google uses
different backend services

00:06:25.760 --> 00:06:27.790
for different products,
URL maps allow

00:06:27.790 --> 00:06:30.160
customers to send
portions of the URL space

00:06:30.160 --> 00:06:32.180
to different groups of VMs.

00:06:32.180 --> 00:06:34.470
For example, you may want
to serve static files

00:06:34.470 --> 00:06:36.260
from a different
set of VM instances

00:06:36.260 --> 00:06:37.810
than the dynamic content.

00:06:37.810 --> 00:06:40.872
HTTP load balancing
allows you to do this.

00:06:40.872 --> 00:06:42.330
Hopefully, this
gives you a glimpse

00:06:42.330 --> 00:06:44.570
of how Google front end
infrastructure has evolved

00:06:44.570 --> 00:06:46.190
and how you can
take advantage of it

00:06:46.190 --> 00:06:48.000
to improve the performance
and reliability

00:06:48.000 --> 00:06:49.720
of own applications.

00:06:49.720 --> 00:06:52.110
So whether you're a large
enterprise or a new start up,

00:06:52.110 --> 00:06:56.310
we invite you to benefit from
using Google's Cloud Platform.

