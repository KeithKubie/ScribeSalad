WEBVTT
Kind: captions
Language: en

00:00:00.061 --> 00:00:02.434
♪ (music) ♪

00:00:05.994 --> 00:00:07.154
(Derek) Thanks.

00:00:07.154 --> 00:00:09.117
(applause)

00:00:09.117 --> 00:00:10.731
Okay, thank you, Jeff.

00:00:10.731 --> 00:00:13.568
And wow, it's really amazing to see 
how people are using TensorFlow

00:00:13.568 --> 00:00:15.499
to make the world a better place.

00:00:15.499 --> 00:00:17.588
So, as Jeff said, my name's Derek Murray.

00:00:17.588 --> 00:00:19.926
I work on the core TensorFlow team.

00:00:19.926 --> 00:00:22.137
I'm thrilled to be here 
at the second day of the summit

00:00:22.137 --> 00:00:23.587
to tell you about <i>tf.data</i>,

00:00:23.587 --> 00:00:26.335
which is our new library
that helps you get all of your data

00:00:26.335 --> 00:00:30.006
from cat pictures to cassava pictures
into TensorFlow.

00:00:30.506 --> 00:00:33.108
So why are we here talking 
about input pipelines?

00:00:33.108 --> 00:00:36.377
After all, they're usually overshadowed
by the more glamorous aspects

00:00:36.377 --> 00:00:38.917
of a machine learning job, 
the matrix multiplications,

00:00:38.917 --> 00:00:40.417
and the convolutions.

00:00:40.417 --> 00:00:43.497
But I now hear that they're actually 
extremely important,

00:00:43.497 --> 00:00:46.806
because input data
is the lifeblood of machine learning,

00:00:46.806 --> 00:00:50.494
and their current algorithms
and hardware are so thirsty for data

00:00:50.494 --> 00:00:53.927
that we need a powerful input pipeline
to be able to keep up with them.

00:00:56.746 --> 00:00:57.996
That's the button. There we go.

00:00:57.996 --> 00:01:01.526
Alright, so when I kicked off
the <i>tf.data</i> project last year,

00:01:01.526 --> 00:01:03.934
TensorFlow had room
to improve in this regard.

00:01:03.934 --> 00:01:07.708
You can either feed in data from Python
at each step, which was kind of slow,

00:01:07.708 --> 00:01:10.535
or you could set up
queue runners to feed your data,

00:01:10.535 --> 00:01:13.727
and these are a little challenging to use.

00:01:13.727 --> 00:01:15.785
So, I decided to focus on three themes

00:01:15.785 --> 00:01:18.876
which are going to be
the main topics of my talk today.

00:01:18.876 --> 00:01:20.136
First is performance,

00:01:20.136 --> 00:01:23.369
because when we're training models
on state-of-the-art accelerator hardware,

00:01:23.369 --> 00:01:24.968
like Cloud TPU pods,

00:01:24.968 --> 00:01:27.337
or NVIDIA GTX supercomputers,

00:01:27.337 --> 00:01:30.417
we have a moral imperative
to keep those accelerators busy

00:01:30.417 --> 00:01:32.548
with new data at all times.

00:01:33.198 --> 00:01:34.656
The second is flexibility,

00:01:34.656 --> 00:01:38.516
because we want to be able to handle
any kind of data in TensorFlow itself.

00:01:38.516 --> 00:01:40.866
We don't want to have
to rely on external tools

00:01:40.866 --> 00:01:43.975
so that it's possible to experiment
with different views of your data

00:01:43.975 --> 00:01:45.847
on an interactive time scale.

00:01:46.637 --> 00:01:49.566
And then, third, we have ease of use,
because as Jeff said,

00:01:49.566 --> 00:01:52.904
our overall mission with TensorFlow
is to open up machine learning

00:01:52.904 --> 00:01:55.069
to users of all abilities.

00:01:55.069 --> 00:01:57.335
And I know from spending
a lot of time on stack overflow

00:01:57.335 --> 00:01:58.405
that it can be a big leap

00:01:58.405 --> 00:02:01.476
from following along 
with your first MNIST tutorial

00:02:01.476 --> 00:02:04.008
to training your first model
on your own data.

00:02:04.008 --> 00:02:07.695
And a good input pipeline [story]
can help smooth this transition.

00:02:07.695 --> 00:02:10.573
So we aim to make <i>tf.data</i> 
the only library you need

00:02:10.573 --> 00:02:12.543
for all input processing in TensorFlow.

00:02:12.543 --> 00:02:13.873
How do we go about that?

00:02:13.873 --> 00:02:17.505
Well, we took inspiration
from the worlds of databases

00:02:17.505 --> 00:02:20.975
and functional programming languages,
and designed <i>tf.data</i> as a library

00:02:20.975 --> 00:02:24.837
of simple composable tools 
that we can divide up into three buckets.

00:02:25.727 --> 00:02:29.085
First, we have the tools to extract data
from a wide range of sources.

00:02:29.085 --> 00:02:32.865
These can range from in-memory erase
to multiterabyte files

00:02:32.865 --> 00:02:36.245
that are sharded across 
the distributive file system like GCS.

00:02:37.265 --> 00:02:39.366
Then we have the tools
to transform your data,

00:02:39.366 --> 00:02:41.564
and these enable you
to change the representation,

00:02:41.564 --> 00:02:44.541
extract features from your data,
perform data augmentation,

00:02:44.541 --> 00:02:47.856
and ultimately convert 
your raw data into the tensors

00:02:47.856 --> 00:02:49.944
that you'll use to train your model.

00:02:51.734 --> 00:02:55.055
And finally, we have a set of tools 
for loading the transformed data

00:02:55.055 --> 00:02:57.986
into the memory of your GPUs 
and TPU accelerators,

00:02:57.986 --> 00:03:00.564
and this can be particularly important
for M2M performance

00:03:00.564 --> 00:03:02.853
as we'll talk about a bit later on.

00:03:02.853 --> 00:03:06.493
Well, that's the high-level pitch,
but what does it look like in real code?

00:03:06.493 --> 00:03:09.842
Well, this is the standard <i>tf.data</i>
input pipeline for reading out

00:03:09.842 --> 00:03:13.703
a set of example protos
from a set of TFRecord files.

00:03:13.703 --> 00:03:16.735
And I bet that 90% of all 
TensorFlow input pipelines

00:03:16.735 --> 00:03:17.735
start off this way.

00:03:17.735 --> 00:03:18.807
In fact, it's so common,

00:03:18.807 --> 00:03:21.444
we've wrapped this very pipeline up
in the single utility function,

00:03:21.444 --> 00:03:23.242
but just for pedagogical reasons,

00:03:23.242 --> 00:03:25.873
it's useful to spell out
the individual parts.

00:03:26.473 --> 00:03:29.134
So we start the extraction phase
by getting a list of files,

00:03:29.134 --> 00:03:32.813
and these can be on your local disc
or just as easily in GCS or S3,

00:03:32.813 --> 00:03:35.724
and we extract 
the TFRecords from those files.

00:03:36.294 --> 00:03:39.223
Then we use a sequence 
of functional transformations

00:03:39.223 --> 00:03:40.562
to pre-process the data.

00:03:40.562 --> 00:03:45.605
And here we took inspiration
from languages like <i>C#</i>, <i>Java</i>, and <i>Scala</i>,

00:03:45.605 --> 00:03:49.698
which choose method chaining 
to build up a pipeline of dataset objects

00:03:49.698 --> 00:03:53.730
and high order functional operators
like map and filter

00:03:53.730 --> 00:03:57.354
to let you customize
the behavior of that pipeline.

00:03:57.354 --> 00:03:58.803
And then, finally, in the load phase,

00:03:58.803 --> 00:04:01.882
we tell the TensorFlow
how to get data out of the dataset.

00:04:01.882 --> 00:04:04.383
One of the easiest ways
to do this is to create an iterator,

00:04:04.383 --> 00:04:06.612
and just like its namesake in Python,

00:04:06.612 --> 00:04:10.210
this gives you a sequential access 
to the elements of the dataset.

00:04:10.210 --> 00:04:12.152
A sequential access
is typically what you want

00:04:12.152 --> 00:04:15.003
when you're doing mini batch training
for a deep neural network,

00:04:15.003 --> 00:04:18.410
and we'll see some ways to soup up
this part of the pipeline later on.

00:04:19.100 --> 00:04:20.542
Now that I've given you an overview,

00:04:20.542 --> 00:04:22.352
I want to come back
to each of our key themes

00:04:22.352 --> 00:04:25.932
and tell you how we've been advancing
each of them over the last few months.

00:04:25.932 --> 00:04:27.152
Let's start with performance--

00:04:27.152 --> 00:04:29.772
so you remember
all those exciting performance results

00:04:29.772 --> 00:04:31.750
that Megan told you about in the keynote?

00:04:31.750 --> 00:04:35.412
Well, every one of them was measured
using real input data

00:04:35.412 --> 00:04:36.872
and a <i>tf.data</i> input pipeline,

00:04:36.872 --> 00:04:40.551
you can download and use
in your own programs.

00:04:40.551 --> 00:04:42.011
And in our set of benchmarks

00:04:42.011 --> 00:04:44.090
there's one experiment 
that I personally like

00:04:44.090 --> 00:04:45.779
and it measures
the performance of training

00:04:45.779 --> 00:04:48.979
an infinitely fast image model
on real data

00:04:48.979 --> 00:04:52.399
in order to tease out 
any bottlenecks in the <i>tf.data</i> pipeline.

00:04:52.399 --> 00:04:56.011
And when we ran this benchmark
last week on an NVIDIA DGVX1V,

00:04:56.011 --> 00:05:00.692
it managed to process
over 13,000 image net images per second.

00:05:01.312 --> 00:05:03.022
Now that is actually much faster

00:05:03.022 --> 00:05:05.739
than we can train a state-of-the-art model
on current hardware,

00:05:05.739 --> 00:05:07.799
but it's still exciting
for a couple of reasons.

00:05:07.799 --> 00:05:09.693
And first of that, out of all of them

00:05:09.693 --> 00:05:11.663
is that this throughput
has more than doubled

00:05:11.663 --> 00:05:12.982
over the past eight months,

00:05:12.982 --> 00:05:15.839
and it's really a testament
to the great job the whole team has done

00:05:15.839 --> 00:05:18.678
in optimizing TensorFlow performance.

00:05:18.678 --> 00:05:19.680
And the second reason

00:05:19.680 --> 00:05:22.151
is that accelerator's models
are getting faster all the time,

00:05:22.151 --> 00:05:24.452
so we're in a good place to deal
with future generations

00:05:24.452 --> 00:05:26.581
and we've got
this extremely useful benchmark

00:05:26.581 --> 00:05:30.069
which guides us as we continue
to improve <i>tf.data</i> performance.

00:05:30.869 --> 00:05:32.709
So these results
are all very nice in theory,

00:05:32.709 --> 00:05:35.211
but how do you actually get 
that kind of performance?

00:05:35.211 --> 00:05:39.021
Well, one option is you can go and get up,
fork the TensorFlow benchmarks project

00:05:39.021 --> 00:05:40.950
and use that code in your own program.

00:05:40.950 --> 00:05:42.600
If you're training a model and image now

00:05:42.600 --> 00:05:44.447
you should probably just do that.

00:05:44.447 --> 00:05:46.627
But, maybe you have
a different problem to solve

00:05:46.627 --> 00:05:49.171
so we've recently launched
a performance guide

00:05:49.171 --> 00:05:51.551
for <i>tf.data</i> on <i>tensorflow.org</i>,

00:05:51.551 --> 00:05:55.348
and this guide is full of useful
theoretical and practical information

00:05:55.348 --> 00:05:58.359
that gives you the ability
to put our optimizations into practice

00:05:58.359 --> 00:05:59.949
on your own pipelines.

00:06:00.529 --> 00:06:02.099
To support this on the technical side,

00:06:02.099 --> 00:06:04.399
we've been adding a raft
of new features to <i>tf.data</i>

00:06:04.399 --> 00:06:05.978
to help achieve this performance.

00:06:05.978 --> 00:06:10.087
One that I particularly want to call out
is this new GPU prefetching optimization

00:06:10.087 --> 00:06:12.127
that's going to be a part 
of TensorFlow 1.8,

00:06:12.127 --> 00:06:15.807
although you can start playing with it
on the night we build, right away.

00:06:15.807 --> 00:06:19.017
And up to this point,
<i>tf.data</i> has been exclusively for codes

00:06:19.017 --> 00:06:20.286
that runs on the CPU,

00:06:20.286 --> 00:06:23.876
and this marks our first foray
into running pipelines on GPUs as well.

00:06:23.876 --> 00:06:25.578
There's a lot more I can say on this topic

00:06:25.578 --> 00:06:29.018
and we're still developing these features,
so watch this space.

00:06:29.018 --> 00:06:31.638
But let's go back to looking again
at our program from earlier on.

00:06:31.638 --> 00:06:34.490
I'll show you how to put
some of these techniques into practice.

00:06:34.490 --> 00:06:36.678
So first off, when you're dealing
with a large dataset

00:06:36.678 --> 00:06:38.258
sharded across a cloud storage service,

00:06:38.258 --> 00:06:40.088
like GCS or S3,

00:06:40.088 --> 00:06:42.738
you can speed things up
by reading multiple files in parallel

00:06:42.738 --> 00:06:45.366
to increase the effect
of throughput into your model.

00:06:45.366 --> 00:06:47.156
You can now turn this on
with a single option

00:06:47.156 --> 00:06:51.795
to the TFRecord dataset 
called <i>num_parallel_reads</i>.

00:06:51.795 --> 00:06:54.745
Then, in the transformation phase, 
you can often improve performance

00:06:54.745 --> 00:06:57.948
by switching to fuse the versions
of various transformations.

00:06:57.948 --> 00:06:59.797
So here I fuse shuffle and repeat

00:06:59.797 --> 00:07:03.567
that avoids a stall
between epochs as it buffers.

00:07:03.567 --> 00:07:06.517
And I've also fused together 
the map and batch transformations

00:07:06.517 --> 00:07:10.348
which paralyzes both the execution
of the function in the map

00:07:10.348 --> 00:07:13.666
and the data transfer
of each element into the batch tensor,

00:07:13.666 --> 00:07:16.547
and together these two optimizations
get big speedups

00:07:16.547 --> 00:07:19.627
for models that consume
a large volume of data per step.

00:07:20.257 --> 00:07:22.726
And last but not least,
we have the GPU prefetching

00:07:22.726 --> 00:07:23.727
that I just mentioned.

00:07:23.727 --> 00:07:27.177
So this ensures
that the next batch of input data

00:07:27.177 --> 00:07:31.087
is in memory on the GPU 
when the next step is ready to begin.

00:07:31.087 --> 00:07:32.847
And this optimization
has been a crucial part

00:07:32.847 --> 00:07:34.588
of our CNN benchmarks for a while now,

00:07:34.588 --> 00:07:37.205
but achieving it involved
manually staging buffers

00:07:37.205 --> 00:07:39.197
from the CPU memory to GPU memory,

00:07:39.197 --> 00:07:42.346
so changing up the structure
of your program to do this.

00:07:42.346 --> 00:07:45.725
The new prefetch to the device API 
gives you the same performance

00:07:45.725 --> 00:07:49.294
and only requires you to add
one line of code to your input pipeline

00:07:49.294 --> 00:07:50.783
to get the benefit.

00:07:51.493 --> 00:07:53.945
So this is really just 
the CliffsNotes version.

00:07:53.945 --> 00:07:57.174
I encourage you to watch 
my colleague Bren Saeta's talk

00:07:57.174 --> 00:07:58.734
later here at the summit.

00:07:58.734 --> 00:08:02.392
He'll show you a more scientific approach
to pipeline performance optimization.

00:08:02.392 --> 00:08:04.543
And of course, you can check out
the performance guide

00:08:04.543 --> 00:08:06.204
on <i>tensorflow.org</i>.

00:08:07.024 --> 00:08:10.225
So now let's switch gears 
and move on to our second theme

00:08:10.225 --> 00:08:12.174
which is flexibility.

00:08:12.174 --> 00:08:13.174
And originally,

00:08:13.174 --> 00:08:16.814
the flexibility in <i>tf.data</i> stemmed
from the functional transformation.

00:08:16.814 --> 00:08:19.726
So they allow you to put 
any TensorFlow computation,

00:08:19.726 --> 00:08:23.110
any fragment of a TensorFlow graph,
at any point in your pipeline.

00:08:23.110 --> 00:08:25.778
And so, for example, if you have 
some existing TensorFlow code

00:08:25.778 --> 00:08:27.464
for pre-processing images,

00:08:27.464 --> 00:08:29.713
you can stick it inside a dataset.map

00:08:29.713 --> 00:08:32.713
and start using it
in your pipeline right away.

00:08:32.713 --> 00:08:36.144
An initial version of <i>tf.data</i>
had sort of traded on this

00:08:36.144 --> 00:08:38.714
and it let you pass a list of tensors in

00:08:38.714 --> 00:08:41.484
and get a list of tensors out
from these transformations.

00:08:41.484 --> 00:08:44.744
But we heard back from our users
that they had more sophisticated things

00:08:44.744 --> 00:08:46.952
and complicated feature structure.

00:08:46.952 --> 00:08:48.792
So we had a support
for nested structure types

00:08:48.792 --> 00:08:50.504
like tuples and dictionaries.

00:08:50.504 --> 00:08:54.032
And in TensorFlow 1.5, we had
a native support for sparse tensors,

00:08:54.032 --> 00:08:55.222
which are particularly useful

00:08:55.222 --> 00:08:57.712
when you're dealing 
with complex categorical data

00:08:57.712 --> 00:08:59.632
and training embedding models.

00:09:00.332 --> 00:09:02.643
So, at this point, 
if there are TensorFlow ops

00:09:02.643 --> 00:09:05.023
for everything you want to do, 
then you're all set.

00:09:05.023 --> 00:09:07.284
But one thing that we've learned
over the last few years

00:09:07.284 --> 00:09:09.423
is that not everything
is most naturally expressed

00:09:09.423 --> 00:09:10.971
as a TensorFlow graph.

00:09:10.971 --> 00:09:12.983
So we've been working 
to give you alternative ways

00:09:12.983 --> 00:09:15.151
to build up your <i>tf.data</i> pipelines.

00:09:15.641 --> 00:09:18.762
The first way is by adding 
<i>Dataset.from.generator</i>,

00:09:18.762 --> 00:09:22.666
and what this allows you to do
is build a pipeline from a Python function

00:09:22.666 --> 00:09:24.201
that generates [non PyRates].

00:09:24.201 --> 00:09:28.841
And this gives you the flexibility 
to wrap existing Python code in a dataset

00:09:28.841 --> 00:09:30.983
and benefit from some
of our performance optimizations

00:09:30.983 --> 00:09:33.283
like prefetching to GPUs.

00:09:34.033 --> 00:09:36.882
The other way might be a little
more appealing to power users,

00:09:36.882 --> 00:09:39.352
so we've opened up
the backend API to <i>tf.data</i>

00:09:39.352 --> 00:09:43.500
and you can now build
your own dataset kernel plug-ins in C++,

00:09:43.500 --> 00:09:46.461
and loading them using
TensorFlow's custom op mechanism.

00:09:46.461 --> 00:09:48.253
We've already heard
from some of our partners

00:09:48.253 --> 00:09:52.032
that this ability is useful
for integrating with custom file formats

00:09:52.032 --> 00:09:53.599
and in-house storage systems.

00:09:53.599 --> 00:09:54.961
And we're dogfooding this approach

00:09:54.961 --> 00:09:58.102
for some of the new datasets
and limitations in <i>tf.contrib</i>,

00:09:58.102 --> 00:10:00.522
like the recently added Kafka dataset.

00:10:00.522 --> 00:10:02.080
So I'm really looking forward to seeing

00:10:02.080 --> 00:10:04.061
what some of you will build
with this new API

00:10:04.061 --> 00:10:06.781
and, of course, I'd encourage you
to contribute things back

00:10:06.781 --> 00:10:07.780
via pool request.

00:10:07.780 --> 00:10:10.800
We're really excited to get 
contributions from the community

00:10:10.800 --> 00:10:12.632
in this part of the project.

00:10:13.152 --> 00:10:16.262
Okay, the final thing 
I want to cover is ease of use.

00:10:16.262 --> 00:10:18.920
I want to speak to folks, 
maybe like some of you

00:10:18.920 --> 00:10:21.150
who've used TensorFlow
for a year or more

00:10:21.150 --> 00:10:24.860
and have struggled with the old ways 
of reading data into TensorFlow.

00:10:24.860 --> 00:10:27.131
I usually don't have to make 
much of a case here.

00:10:27.131 --> 00:10:29.330
But our users have 
really high expectations

00:10:29.330 --> 00:10:31.510
and there are new people 
getting their first exposure

00:10:31.510 --> 00:10:32.972
to the library every day.

00:10:32.972 --> 00:10:35.839
So we've continued 
to push hard on usability.

00:10:35.839 --> 00:10:38.841
So I just want to share a few highlights
from recent months.

00:10:39.441 --> 00:10:41.799
First off, as Rajat
told you in the keynote,

00:10:41.799 --> 00:10:43.020
eager execution is here,

00:10:43.020 --> 00:10:46.450
and it makes using <i>tf.data</i>
a lot more pleasant.

00:10:46.450 --> 00:10:48.689
In the next talk, Alex is going 
to tell you a lot more

00:10:48.689 --> 00:10:52.449
about eager mode, 
but from my admittedly bias perspective,

00:10:52.449 --> 00:10:55.337
the best thing about it 
is that you can start treating datasets

00:10:55.337 --> 00:10:57.857
just like any other 
iterable object in Python.

00:10:57.857 --> 00:11:00.210
So you can simply loop over them
with a regular [for loop]

00:11:00.210 --> 00:11:03.989
and there's no more 
"make one-shot interator" required.

00:11:03.989 --> 00:11:06.529
And what's really neat about this
is that it works together

00:11:06.529 --> 00:11:09.819
with <i>tf.data</i> optimizations 
like GPU prefetching.

00:11:09.819 --> 00:11:11.680
So you can combine the efficiency

00:11:11.680 --> 00:11:14.749
of asynchronous graph-based execution
for your input pipeline

00:11:14.749 --> 00:11:18.097
and the flexibility of eager execution
for your model code.

00:11:19.347 --> 00:11:22.309
Next, we've heard feedback
that while power users

00:11:22.309 --> 00:11:26.608
like the compose ability
and configure ability of the data API,

00:11:26.608 --> 00:11:29.938
many users just want an easy way
to follow best practices

00:11:29.938 --> 00:11:31.777
on standard data formats.

00:11:31.777 --> 00:11:35.103
So TensorFlow 1.8 will contain 
new standard recipes

00:11:35.103 --> 00:11:38.420
for <i>tf.example</i> protocol buffers
and for CSV data.

00:11:38.420 --> 00:11:40.317
They make a lot easier
to handle these formats

00:11:40.317 --> 00:11:43.469
and apply all of the best practices
from a performance guide.

00:11:44.059 --> 00:11:46.586
So let's go for one last time 
back to our standard program,

00:11:46.586 --> 00:11:48.016
and as I promised in the beginning,

00:11:48.016 --> 00:11:49.976
the whole thing can be replaced
with a single call

00:11:49.976 --> 00:11:52.416
to make batch features dataset.

00:11:52.416 --> 00:11:55.478
And this performs
all the parallel IO, parallel parsing,

00:11:55.478 --> 00:11:57.597
shuffling, batching 
and prefetching for you.

00:11:57.597 --> 00:12:00.200
And it gives you back a dataset
that you can continue to transform

00:12:00.200 --> 00:12:02.895
using map, filter,
and other transformations.

00:12:02.895 --> 00:12:04.806
And if you have
a performance critical workload,

00:12:04.806 --> 00:12:06.586
we generally recommend
using a binary format

00:12:06.586 --> 00:12:08.989
like <i>tf.Example</i> and TFRecords.

00:12:08.989 --> 00:12:10.989
But for many users 
who are just getting started

00:12:10.989 --> 00:12:14.449
and tend to have smaller data,
they often prefer something simple,

00:12:14.449 --> 00:12:18.439
and comma-separated values,
or CSV format, fits that bill nicely.

00:12:18.439 --> 00:12:22.069
And, in particular, we've noticed
how popular CSV data is on Kaggle

00:12:22.069 --> 00:12:24.738
where there are thousands
of different CSV datasets

00:12:24.738 --> 00:12:26.999
that are available to download for free.

00:12:26.999 --> 00:12:29.697
And this snippet shows you 
how to use the new Kaggle API

00:12:29.697 --> 00:12:31.389
by pip installing Kaggle,

00:12:31.389 --> 00:12:34.979
to download them with just a couple
of simple commands.

00:12:35.589 --> 00:12:36.646
And once you've done that,

00:12:36.646 --> 00:12:39.849
you can call the new <i>make_csv_dataset</i>
function in TensorFlow

00:12:39.849 --> 00:12:42.578
to get the data
out of those dead unloaded files.

00:12:42.578 --> 00:12:45.948
In this case, we'd say dataset
of a million news headlines,

00:12:45.948 --> 00:12:49.076
and I think it's quite simple,
it's only got two features.

00:12:49.076 --> 00:12:51.559
But what I particularly like
about this new API

00:12:51.559 --> 00:12:55.348
is that it takes care of figuring out
the types and the caller names for you

00:12:55.348 --> 00:12:57.929
and it dramatically cuts down 
the amount of boilerplate

00:12:57.929 --> 00:13:00.298
you have to write to work
with this kind of data.

00:13:00.698 --> 00:13:03.648
Finally, we've been working
to improve the integration

00:13:03.648 --> 00:13:06.577
between <i>tf.data</i>
and high-level TensorFlow APIs

00:13:06.577 --> 00:13:08.738
like estimators and Keras.

00:13:08.738 --> 00:13:13.247
Now the Keras support is still 
in the pipeline, if you'll excuse the pun.

00:13:13.247 --> 00:13:16.466
But if you want to switch 
our CSV parsing code

00:13:16.466 --> 00:13:18.396
for eager mode using estimators,

00:13:18.396 --> 00:13:20.917
it's a simple matter
of returning the dataset

00:13:20.917 --> 00:13:23.039
from an estimator's input function--

00:13:23.039 --> 00:13:25.038
no more iterator required--

00:13:25.038 --> 00:13:28.053
and then you pass that input function
to the estimator's train method

00:13:28.053 --> 00:13:29.708
and we're good to go.

00:13:29.708 --> 00:13:32.286
And really, the approach
we're taking on usability here

00:13:32.286 --> 00:13:35.927
is to make <i>tf.data</i> objects,
like the datasets and the iterators,

00:13:35.927 --> 00:13:38.567
as natural and pythonic as possible.

00:13:38.567 --> 00:13:41.328
And features like eager execution
and the high-level APIs

00:13:41.328 --> 00:13:43.078
are making this a lot easier to do.

00:13:43.078 --> 00:13:45.408
Our eventual goal
is to make it so seamless

00:13:45.408 --> 00:13:47.676
that your input processing infrastructure

00:13:47.676 --> 00:13:51.287
becomes a natural extension
of your TensorFlow program.

00:13:52.757 --> 00:13:55.577
Well, that is about all the time I have.

00:13:55.577 --> 00:13:56.577
So just to recap.

00:13:56.577 --> 00:13:59.166
I told you in the beginning
that our mission for <i>tf.data</i>

00:13:59.166 --> 00:14:01.017
was to make a library 
for input processing

00:14:01.017 --> 00:14:04.117
that is fast, flexible, and easy to use.

00:14:04.967 --> 00:14:06.964
I hope I've convinced you
in the last 15 minutes

00:14:06.964 --> 00:14:08.934
that we've achieved 
all three of these goals.

00:14:08.934 --> 00:14:10.355
So you'll understand when I'll say

00:14:10.355 --> 00:14:12.957
that <i>tf.data</i> is the one library
we recommend

00:14:12.957 --> 00:14:15.487
for all input processing in TensorFlow.

00:14:16.357 --> 00:14:17.545
If you want to find out more,

00:14:17.545 --> 00:14:20.582
there is a ton of documentation
about <i>tf.data</i> on <i>tensorflow.org</i>.

00:14:20.582 --> 00:14:21.596
We cover how-tos

00:14:21.596 --> 00:14:24.565
and the performance guidance
that I mentioned earlier.

00:14:24.565 --> 00:14:27.506
And both the TensorFlow benchmarks 
and the official models repositories

00:14:27.506 --> 00:14:29.121
have examples of high performance

00:14:29.121 --> 00:14:32.826
and readable input pipelines
written in <i>tf.data</i>.

00:14:33.366 --> 00:14:34.823
And with all of this information,

00:14:34.823 --> 00:14:37.406
and knowing the real creativity
of this community,

00:14:37.406 --> 00:14:41.165
I'm really looking forward to seeing
what all of you build with this library.

00:14:41.485 --> 00:14:43.074
Thanks a lot for listening.

00:14:43.074 --> 00:14:45.365
(applause)

00:14:45.365 --> 00:14:48.105
♪ (music) ♪

