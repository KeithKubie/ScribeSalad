WEBVTT
Kind: captions
Language: en

00:00:00.260 --> 00:00:03.120
So now that we have an idea
of what POMDPs are about.

00:00:03.120 --> 00:00:05.650
Maybe we should talk a little bit
about what it might mean to do

00:00:05.650 --> 00:00:08.730
reinforcement learning in them because
we basically talked about planning.

00:00:08.730 --> 00:00:12.649
If we have a model of the POMDP we can
run some calculations we can run value

00:00:12.649 --> 00:00:17.650
iteration to get a way of deciding
what to do actually did I say that?

00:00:17.650 --> 00:00:18.550
I think I did not say that.

00:00:18.550 --> 00:00:20.690
I think what I said is how
you get a value function.

00:00:20.690 --> 00:00:23.520
Is it clear how you would use
a value function to get a policy?

00:00:23.520 --> 00:00:26.230
&gt;&gt; In the same way you would for
an MDP right?

00:00:26.230 --> 00:00:27.357
Yes? No? That's not true?

00:00:27.357 --> 00:00:28.200
&gt;&gt; Yeah that's right.

00:00:28.200 --> 00:00:31.170
If you have the model, then you can
use one step look ahead with the value

00:00:31.170 --> 00:00:34.210
function to figure out what the optimal
action is in any given believe state.

00:00:34.210 --> 00:00:38.130
Once we've run valuation and gotten
an approximation of the optimal value

00:00:38.130 --> 00:00:42.708
function, and what about that valuation
doesn't actually necessarily converge.

00:00:42.708 --> 00:00:43.390
&gt;&gt; It doesn't?

00:00:43.390 --> 00:00:44.590
You didn't tell me that.

00:00:44.590 --> 00:00:47.185
&gt;&gt; I did actually when we
do in less than triple a.

00:00:47.185 --> 00:00:48.171
&gt;&gt; Lesson triple a?

00:00:48.171 --> 00:00:50.360
&gt;&gt; Yeah the advanced algorithm analysis.

00:00:50.360 --> 00:00:53.300
Well we said there is that value
iteration converges in the limit

00:00:53.300 --> 00:00:55.400
to the right value function but

00:00:55.400 --> 00:00:58.710
after any finite number of steps it need
not have the optimal value function.

00:00:58.710 --> 00:01:01.620
But it will after some finite number
of steps have the optimal policy

00:01:01.620 --> 00:01:04.560
that's not going to be true in the POMDP
case because there's an infinite number

00:01:04.560 --> 00:01:05.604
of states.

00:01:05.604 --> 00:01:06.290
&gt;&gt; I see.
&gt;&gt; But

00:01:06.290 --> 00:01:09.170
it is going to be the case that
we get an arbitrarily good

00:01:09.170 --> 00:01:12.550
approximation after some
finite number of iterations.

00:01:12.550 --> 00:01:14.700
&gt;&gt; Wait arbitrarily good
approximation of what?

00:01:14.700 --> 00:01:15.880
&gt;&gt; Of the optimal value function.

00:01:15.880 --> 00:01:17.500
&gt;&gt; Wait but
not necessarily optimal policy?

00:01:17.500 --> 00:01:20.720
&gt;&gt; Yeah if you do one step back ups or
one step look ahead

00:01:20.720 --> 00:01:24.440
with a near optimal value function
you get a near optimal policy?

00:01:24.440 --> 00:01:25.940
&gt;&gt; Okay so that's nearly good.

00:01:25.940 --> 00:01:29.500
&gt;&gt; [LAUGH] Now that we've
talked about planning in POMPDs

00:01:29.500 --> 00:01:31.100
we should talk about
reinforcement learning in POMPDs.

00:01:31.100 --> 00:01:32.940
&gt;&gt; Charles, what's the difference
between planning and

00:01:32.940 --> 00:01:33.830
reinforcement learning?

00:01:33.830 --> 00:01:35.960
&gt;&gt; Well, in planning you know everything
in a reinforcement learning you

00:01:35.960 --> 00:01:39.200
don't know everything and
you have to explore and exploit and

00:01:39.200 --> 00:01:41.340
do all that other stuff in
order to find things out like

00:01:41.340 --> 00:01:44.940
you don't know the model necessarily and
that makes the problem harder so

00:01:44.940 --> 00:01:49.600
what you're telling me is reinforcement
learning is harder then planning

00:01:49.600 --> 00:01:51.442
in the way you're using the words
&gt;&gt; And

00:01:51.442 --> 00:01:54.610
POMDPs are more difficult
to deal with than MDPs.

00:01:54.610 --> 00:01:58.800
So reinforcement learning in POMDPs
should be like, is that additive,

00:01:58.800 --> 00:02:00.070
multiplicative?

00:02:00.070 --> 00:02:02.270
&gt;&gt; So reinforcement
learning in POMDPs is hard.

00:02:02.270 --> 00:02:03.550
&gt;&gt; Hm.
&gt;&gt; Like, super hard.

00:02:03.550 --> 00:02:09.181
Actually planning in POMDPs is formally
undecidable in the sense that [LAUGH]

00:02:09.181 --> 00:02:10.086
&gt;&gt; Undecidable?

00:02:10.086 --> 00:02:12.490
&gt;&gt; Yeah,
[LAUGH] if I give you a POMDP and

00:02:12.490 --> 00:02:16.380
ask, is this the optimal first action
to take from this belief state?

00:02:16.380 --> 00:02:18.970
If you could solve that problem you
could solve the halting problem.

00:02:18.970 --> 00:02:19.790
&gt;&gt; Holy cow!

00:02:19.790 --> 00:02:21.890
&gt;&gt; Wait, wait a minute, wait a minute,
wait a minute, wait a minute.

00:02:21.890 --> 00:02:24.250
That has profound implications.

00:02:24.250 --> 00:02:25.490
&gt;&gt; No, it doesn't.
&gt;&gt; Yes it does,

00:02:25.490 --> 00:02:29.060
because we're human beings
running around in the world

00:02:29.060 --> 00:02:31.690
we're living in a POMDP because
we can't know everything.

00:02:31.690 --> 00:02:35.220
So you telling me that even
if I could relive life

00:02:35.220 --> 00:02:38.870
an infinite number of times I still
don't know what the right thing to do is

00:02:38.870 --> 00:02:40.500
you're saying that life is unknowable.

00:02:40.500 --> 00:02:42.390
It's undecidable you can
never know what to do.

00:02:42.390 --> 00:02:43.450
That's profound.

00:02:43.450 --> 00:02:44.510
Wow I need a moment.

