WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:01.880
[MUSIC PLAYING]

00:00:03.290 --> 00:00:05.790
YOSSI MATIAS: [INAUDIBLE]
artificial intelligence

00:00:05.790 --> 00:00:11.190
and how it impacts anything
from social good to ambient

00:00:11.190 --> 00:00:12.443
intelligence.

00:00:12.443 --> 00:00:14.610
Let me start, actually,
with a personal reflection--

00:00:14.610 --> 00:00:19.550
a recent experience I had
in the mountains of Rwanda.

00:00:19.550 --> 00:00:23.340
I had the privilege to spend an
hour with a family of gorillas

00:00:23.340 --> 00:00:25.020
in their natural habitat.

00:00:25.020 --> 00:00:27.390
And it was fascinating to
see how they were interacting

00:00:27.390 --> 00:00:28.830
with each other.

00:00:28.830 --> 00:00:31.830
It's clearly-- they
have their own language.

00:00:31.830 --> 00:00:35.460
And by the way, they didn't pay
much attention to my presence

00:00:35.460 --> 00:00:37.765
there, as you can see.

00:00:37.765 --> 00:00:42.980
But this reminded me of a
famous gorilla called Koko.

00:00:42.980 --> 00:00:45.740
Koko actually learn to speak--

00:00:45.740 --> 00:00:47.120
learned a language.

00:00:47.120 --> 00:00:52.200
And she learned over 1,000 words
of vocabulary and 1,000 signs--

00:00:52.200 --> 00:00:54.500
2,000 words in spoken English.

00:00:54.500 --> 00:00:57.260
And it took her 46
years to learn it.

00:00:57.260 --> 00:00:59.360
And she became quite
a celebrity, in fact--

00:00:59.360 --> 00:01:01.550
made the cover of
"National Geographic."

00:01:01.550 --> 00:01:04.830
But this also brings up
this interesting question--

00:01:04.830 --> 00:01:06.980
in a way, many
people would not even

00:01:06.980 --> 00:01:13.060
believe that a gorilla could
learn how to speak before Koko.

00:01:13.060 --> 00:01:15.260
And the question is--

00:01:15.260 --> 00:01:17.590
what else can we
teach about speaking?

00:01:17.590 --> 00:01:18.740
How about computers?

00:01:18.740 --> 00:01:21.110
Can we teach computers to speak?

00:01:21.110 --> 00:01:23.210
Can we have a conversation
with computers?

00:01:23.210 --> 00:01:27.540
How far can we take artificial
intelligence in that direction?

00:01:27.540 --> 00:01:29.910
And if we do that, what
are the ramifications?

00:01:29.910 --> 00:01:32.620
And while gorillas already
have their special language.

00:01:32.620 --> 00:01:36.510
In computers, we actually
need to teach it from scratch.

00:01:36.510 --> 00:01:38.570
So artificial
intelligence is actually

00:01:38.570 --> 00:01:40.440
bringing some hope
in that direction.

00:01:40.440 --> 00:01:43.910
But before actually
going deeper into that,

00:01:43.910 --> 00:01:45.970
I'd like to talk
about another aspect

00:01:45.970 --> 00:01:48.110
of artificial intelligence.

00:01:48.110 --> 00:01:51.350
So first-- obviously, we're
seeing that in many products

00:01:51.350 --> 00:01:52.990
around our life--

00:01:52.990 --> 00:01:57.140
this is now one nice example
that I like, which is--

00:01:57.140 --> 00:02:01.010
we all have our phones
taking pictures quite a bit.

00:02:01.010 --> 00:02:03.410
If I'd like to search
for a photo out

00:02:03.410 --> 00:02:05.480
of my ever-growing gallery.

00:02:05.480 --> 00:02:09.020
Today, sometime, all I need
to do is to just search--

00:02:09.020 --> 00:02:12.170
just search for Birds in Sunset.

00:02:12.170 --> 00:02:14.360
And it could bring
those pictures to me--

00:02:14.360 --> 00:02:16.550
these are actually from
my own photo galleries--

00:02:16.550 --> 00:02:18.213
without ever labeling those.

00:02:18.213 --> 00:02:19.880
And in a way, I don't
pay much attention

00:02:19.880 --> 00:02:22.430
to it, which is one of
the magic of technology--

00:02:22.430 --> 00:02:25.860
when we stop paying
attention-- when it just works.

00:02:25.860 --> 00:02:29.468
So really we take
it for granted,

00:02:29.468 --> 00:02:31.760
but it used to be magic--
the fact that we can actually

00:02:31.760 --> 00:02:34.370
identify birds and sunsets
without ever labeling it,

00:02:34.370 --> 00:02:35.600
without training it.

00:02:35.600 --> 00:02:37.400
And this has really
become possible

00:02:37.400 --> 00:02:41.920
by, obviously, advancements
in machine learning and AI--

00:02:41.920 --> 00:02:45.680
more notably in deep learning,
deep neural networks.

00:02:45.680 --> 00:02:49.460
And very simplistically,
an image taken as an input

00:02:49.460 --> 00:02:51.950
to a deep learning network--

00:02:51.950 --> 00:02:55.308
the pixels activate
those so-called neurons,

00:02:55.308 --> 00:02:57.350
which are essentially a
computer program inspired

00:02:57.350 --> 00:03:01.520
by how we believe some of
the branch functions work.

00:03:01.520 --> 00:03:04.250
And then those neuron
activities is propagating

00:03:04.250 --> 00:03:06.650
through the layers
until, at the end,

00:03:06.650 --> 00:03:09.900
we have a classification--
whether it's a dog or a cat.

00:03:09.900 --> 00:03:13.760
And the way in which
this network is built

00:03:13.760 --> 00:03:16.158
is by training it
on many examples--

00:03:16.158 --> 00:03:18.200
in a way, similar to how
we're training ourselves

00:03:18.200 --> 00:03:20.070
to recognize a new object.

00:03:20.070 --> 00:03:23.813
So obviously, this made huge
strides over the last decade.

00:03:23.813 --> 00:03:24.980
And this technology is now--

00:03:24.980 --> 00:03:28.580
I'm sure you heard it
many times recently.

00:03:28.580 --> 00:03:30.290
But the beauty about
this technology

00:03:30.290 --> 00:03:33.830
is that, while it can
be used for many fun

00:03:33.830 --> 00:03:38.190
stuff, such as searching
images in our photo album,

00:03:38.190 --> 00:03:41.370
it can also solve some
pretty serious problems.

00:03:41.370 --> 00:03:44.960
And again, such
inspiring example

00:03:44.960 --> 00:03:47.420
is what it can do in health.

00:03:47.420 --> 00:03:50.420
And again, many of you heard
about diabetic retinopathy.

00:03:50.420 --> 00:03:54.860
This is a condition that may
affect people with diabetics.

00:03:54.860 --> 00:03:58.490
Actually, if not
treated, it can lead

00:03:58.490 --> 00:04:02.660
to loss of vision, loss of
sight-- to blindness sometimes.

00:04:02.660 --> 00:04:07.400
And the good news
is that doctors

00:04:07.400 --> 00:04:10.790
can diagnose it by inspecting
an image of the retina.

00:04:10.790 --> 00:04:13.100
The bad news is that
there's a huge shortage

00:04:13.100 --> 00:04:15.470
of eye doctors, which put
at risk millions of people

00:04:15.470 --> 00:04:17.690
worldwide to lose their sight.

00:04:17.690 --> 00:04:21.290
And the inspiring example
here is that research

00:04:21.290 --> 00:04:26.270
done a few years ago, where one
could train machine learning

00:04:26.270 --> 00:04:30.680
models to help diagnosis,
which is empowered

00:04:30.680 --> 00:04:34.110
with what experts can do, and
therefore compliment that.

00:04:34.110 --> 00:04:36.350
And by doing that,
now all you need to do

00:04:36.350 --> 00:04:39.170
is to bring a person
to a scanning device

00:04:39.170 --> 00:04:43.310
to take a picture, have the
system help out with diagnosis,

00:04:43.310 --> 00:04:46.460
and then save the
eyesight of that person.

00:04:46.460 --> 00:04:50.720
This is actually an image
taken from [INAUDIBLE],,

00:04:50.720 --> 00:04:54.600
a clinic in India, where there's
already a pilot going on.

00:04:54.600 --> 00:04:56.720
So what has started
as a research,

00:04:56.720 --> 00:04:58.550
became a realization--

00:04:58.550 --> 00:05:00.800
is now actually already
deployed in places

00:05:00.800 --> 00:05:04.700
such as India and Thailand,
which is very inspiring to us.

00:05:04.700 --> 00:05:06.830
What other problems
can we solve?

00:05:06.830 --> 00:05:09.950
And indeed, some of you
may have heard last year

00:05:09.950 --> 00:05:13.610
about follow-up research
of looking into the retina.

00:05:13.610 --> 00:05:15.980
And what started out
as an exploration--

00:05:15.980 --> 00:05:18.800
what can we learn from an
image of the retina, perhaps?

00:05:18.800 --> 00:05:22.280
Can we identify some other
characteristics of a person

00:05:22.280 --> 00:05:23.890
just by looking at the image?

00:05:23.890 --> 00:05:28.520
Turns out to actually have
discovery of phenomena

00:05:28.520 --> 00:05:31.310
that we don't even know about.

00:05:31.310 --> 00:05:34.040
The research published
showed that you

00:05:34.040 --> 00:05:39.880
can identify some of
cardiac signals and risks

00:05:39.880 --> 00:05:41.610
based just on the
image of retina--

00:05:41.610 --> 00:05:44.950
a fact that was not known to
the medical community before,

00:05:44.950 --> 00:05:48.280
which is a great example of
how technology can help us not

00:05:48.280 --> 00:05:52.690
only solve known problems
or provide solutions

00:05:52.690 --> 00:05:55.990
to known methods that we
want to solve but, also,

00:05:55.990 --> 00:05:58.910
give us new insights that
we were not even aware of.

00:05:58.910 --> 00:06:01.490
So this, obviously,
brings up more hope.

00:06:01.490 --> 00:06:06.760
And indeed, just recently we
just announced these research

00:06:06.760 --> 00:06:10.390
showing how we can train
machine learning models

00:06:10.390 --> 00:06:14.350
with the right methodology
to help out with the scanning

00:06:14.350 --> 00:06:16.840
to have an earlier detection
for lung cancer, which

00:06:16.840 --> 00:06:20.870
is one of the deadliest
diseases out there.

00:06:20.870 --> 00:06:22.420
And again, think
about all the impact

00:06:22.420 --> 00:06:24.230
that we will have
with this technology.

00:06:24.230 --> 00:06:28.090
So these are all very exciting
progress that we see worldwide.

00:06:28.090 --> 00:06:31.570
And I'm really excited about
the possibilities for health.

00:06:31.570 --> 00:06:35.900
But when we think about the
impact of AI to our world,

00:06:35.900 --> 00:06:38.532
it's not only to health--
which is hugely important.

00:06:38.532 --> 00:06:40.990
There are many other societal
problems that we can actually

00:06:40.990 --> 00:06:42.700
address with AI.

00:06:42.700 --> 00:06:45.580
And one such area that I'd like
to actually spend a few minutes

00:06:45.580 --> 00:06:47.800
about is crisis response.

00:06:47.800 --> 00:06:51.230
And crises are situations
where people find themselves

00:06:51.230 --> 00:06:54.960
in danger.

00:06:54.960 --> 00:06:57.780
And once somebody
is in crisis, they

00:06:57.780 --> 00:07:00.630
have a very strong need
for help, for information,

00:07:00.630 --> 00:07:01.750
for technology.

00:07:01.750 --> 00:07:03.300
So to reflect on
that, I'm going back

00:07:03.300 --> 00:07:07.660
to my own personal experience
8 and 1/2 years ago.

00:07:07.660 --> 00:07:11.470
This was in December, 2010.

00:07:11.470 --> 00:07:14.070
I have a team in Israel,
in Tel Aviv and Haifa.

00:07:14.070 --> 00:07:17.040
And that day, I was
in our Haifa office.

00:07:17.040 --> 00:07:20.730
And we had one of the deadliest
fires in the Carmel Mountains,

00:07:20.730 --> 00:07:22.720
which is not too
far from the office.

00:07:22.720 --> 00:07:24.570
So this picture was
taken by my camera.

00:07:24.570 --> 00:07:26.730
I actually checked the
metadata on the picture.

00:07:26.730 --> 00:07:31.230
It's by Google Nexus 1 phone.

00:07:31.230 --> 00:07:33.600
And at that point, it
was clear-- there's

00:07:33.600 --> 00:07:34.570
something going on.

00:07:34.570 --> 00:07:36.720
But searching for
information on the internet

00:07:36.720 --> 00:07:38.680
revealed very
little information.

00:07:38.680 --> 00:07:40.980
And I was really eager to
ask questions, such as,

00:07:40.980 --> 00:07:42.000
what should I be doing?

00:07:42.000 --> 00:07:44.250
So I evacuate the office?

00:07:44.250 --> 00:07:47.658
Should I alert people to leave
their homes or something?

00:07:47.658 --> 00:07:49.950
And there was no information
available on the internet.

00:07:49.950 --> 00:07:51.920
It wasn't until I called
the mayor's office--

00:07:51.920 --> 00:07:54.660
I got a number with some
actionable information.

00:07:54.660 --> 00:07:57.330
And actually, asking
my search team

00:07:57.330 --> 00:08:00.313
to put this information
online, as a result,

00:08:00.313 --> 00:08:01.980
for anybody searching
for the same stuff

00:08:01.980 --> 00:08:03.330
that I was searching--

00:08:03.330 --> 00:08:06.300
this was actually one
of our early launches

00:08:06.300 --> 00:08:09.870
of actionable information
crisis response.

00:08:09.870 --> 00:08:11.730
And later on, we
used it for dozens

00:08:11.730 --> 00:08:13.680
of other such situations.

00:08:13.680 --> 00:08:16.990
And we had a lot of investment
in crisis over the years.

00:08:16.990 --> 00:08:19.030
But a couple years ago--

00:08:19.030 --> 00:08:22.540
and I should mention that I
was not alone in doing this.

00:08:22.540 --> 00:08:24.690
So what we see is that,
whenever there's a crisis,

00:08:24.690 --> 00:08:27.240
people are turning to Google
to search for information.

00:08:27.240 --> 00:08:29.970
You can see that, during
floods, during hurricanes,

00:08:29.970 --> 00:08:33.360
during explosions, they're
looking for information

00:08:33.360 --> 00:08:37.380
not only because they'd like
to learn about what's going on

00:08:37.380 --> 00:08:39.360
but because they'd
like to take action.

00:08:39.360 --> 00:08:42.159
They'd like to know what they
should be doing about it.

00:08:42.159 --> 00:08:44.850
So to that end, we
actually built a team.

00:08:44.850 --> 00:08:46.290
And we launched,
a couple of years

00:08:46.290 --> 00:08:49.500
ago, a product within Search,
called SOS Alerts where,

00:08:49.500 --> 00:08:51.300
every time there's
a big crisis, we're

00:08:51.300 --> 00:08:53.850
trying to pull together
and provide the best

00:08:53.850 --> 00:08:57.180
information that we can, so as
to help people about questions

00:08:57.180 --> 00:08:58.260
such as, what's going on?

00:08:58.260 --> 00:08:58.890
Where is it?

00:08:58.890 --> 00:09:00.030
What should I be doing?

00:09:00.030 --> 00:09:01.410
How can I help?

00:09:01.410 --> 00:09:03.325
And since then,
unfortunately, there

00:09:03.325 --> 00:09:05.430
are now a lot of
crises out there.

00:09:05.430 --> 00:09:12.030
There were over 250 activations
worldwide and tens of thousands

00:09:12.030 --> 00:09:14.010
alerts, which are
public alerts coming

00:09:14.010 --> 00:09:16.110
from various
governmental agencies

00:09:16.110 --> 00:09:18.330
directly into people's
phones, with a total

00:09:18.330 --> 00:09:22.070
of over 2 billion
views on those crises.

00:09:22.070 --> 00:09:23.930
Which-- in many
cases, we're hearing

00:09:23.930 --> 00:09:26.060
about how helpful it is.

00:09:26.060 --> 00:09:28.020
However, we also
learned, in the process,

00:09:28.020 --> 00:09:32.060
there's certain crises that we
cannot provide a lot of health.

00:09:32.060 --> 00:09:34.610
And in fact, one of the most
devastating natural disasters

00:09:34.610 --> 00:09:36.380
is our floods.

00:09:36.380 --> 00:09:41.660
Floods are affecting up to
230 million people per year

00:09:41.660 --> 00:09:43.010
globally.

00:09:43.010 --> 00:09:46.130
They are actually believed to
be responsible for over 6,000

00:09:46.130 --> 00:09:48.320
fatalities per year globally.

00:09:48.320 --> 00:09:51.993
And some of those can
be, perhaps, avoided,

00:09:51.993 --> 00:09:53.660
if people would have
enough information.

00:09:53.660 --> 00:09:56.180
Which typically-- we
just don't have it.

00:09:56.180 --> 00:09:58.310
And we see those
examples, in fact--

00:09:58.310 --> 00:10:01.550
when you do have
information about floods,

00:10:01.550 --> 00:10:03.450
then people can take action.

00:10:03.450 --> 00:10:07.460
So for example, this is
testimonial in places

00:10:07.460 --> 00:10:09.470
that we did have
information about floods

00:10:09.470 --> 00:10:11.150
and people could take action.

00:10:11.150 --> 00:10:14.480
And just last week,
we saw an example--

00:10:14.480 --> 00:10:16.400
actually, a very
beautiful example--

00:10:16.400 --> 00:10:19.340
about how the Indian
government in Bangladesh

00:10:19.340 --> 00:10:22.100
took action during
the Cyclone Penny.

00:10:22.100 --> 00:10:24.560
And because there was a
pretty good prediction

00:10:24.560 --> 00:10:26.600
about where the cyclone
is going to be heading,

00:10:26.600 --> 00:10:27.590
they could take action.

00:10:27.590 --> 00:10:29.630
And based on preparation
for many years

00:10:29.630 --> 00:10:31.430
and a great execution,
they actually

00:10:31.430 --> 00:10:34.580
managed to mobilize many
people and get them out

00:10:34.580 --> 00:10:35.702
of the danger zone.

00:10:35.702 --> 00:10:37.910
But there are many other
floods where, actually, this

00:10:37.910 --> 00:10:39.350
is not happening.

00:10:39.350 --> 00:10:42.260
The reason being--
for example, whenever

00:10:42.260 --> 00:10:46.338
it is the monsoon season,
rivers are starting to rise.

00:10:46.338 --> 00:10:47.880
And quite often, we
know that there's

00:10:47.880 --> 00:10:49.088
going to be floods somewhere.

00:10:49.088 --> 00:10:51.050
But we're not sure
exactly where.

00:10:51.050 --> 00:10:54.170
And the difference
between knowing

00:10:54.170 --> 00:10:56.810
just generally versus
knowing exactly

00:10:56.810 --> 00:10:58.470
makes all the
difference in the world.

00:10:58.470 --> 00:11:02.000
So what we can see
here is the situation

00:11:02.000 --> 00:11:04.490
where the accuracy is not good.

00:11:04.490 --> 00:11:06.385
You know that a flood
is maybe going there.

00:11:06.385 --> 00:11:08.760
But if you don't know exactly
where it's going to happen,

00:11:08.760 --> 00:11:11.612
people are not going to
take action, as we learned.

00:11:11.612 --> 00:11:13.070
And governments
cannot take action,

00:11:13.070 --> 00:11:15.630
because there's too
much area to cover.

00:11:15.630 --> 00:11:17.340
So it's just not practical.

00:11:17.340 --> 00:11:20.150
So the question is, can we
get these higher-accuracy

00:11:20.150 --> 00:11:22.160
predictions?

00:11:22.160 --> 00:11:24.800
And as I'm going to discuss--

00:11:24.800 --> 00:11:26.840
indeed, this turns
out to be possible.

00:11:26.840 --> 00:11:29.242
However, two years
ago, if you'd asked me

00:11:29.242 --> 00:11:31.700
if it's possible to provide
this kind of flood forecasting,

00:11:31.700 --> 00:11:33.020
I would say, I don't know.

00:11:33.020 --> 00:11:35.550
Because there's a lot going
in into this computation

00:11:35.550 --> 00:11:37.220
of predictions of flood.

00:11:37.220 --> 00:11:40.850
So after some
exploration, we managed

00:11:40.850 --> 00:11:45.000
to figure out that, actually,
we could do something about it.

00:11:45.000 --> 00:11:48.360
And we did have some progress
on flood forecasting.

00:11:48.360 --> 00:11:51.440
So the way we're
treating it is by, first,

00:11:51.440 --> 00:11:53.510
taking very accurate
data elevation

00:11:53.510 --> 00:11:55.190
models of the
terrain of the places

00:11:55.190 --> 00:11:56.700
that we're trying to predict.

00:11:56.700 --> 00:11:59.810
And this alone is a very
interesting and important

00:11:59.810 --> 00:12:03.950
computational problem, based
on taking aerial imagery,

00:12:03.950 --> 00:12:07.130
using machine learning to
reconstruct the terrain,

00:12:07.130 --> 00:12:09.680
and then using machine
learning to recalibrate

00:12:09.680 --> 00:12:13.360
what's going on there,
based on historical data.

00:12:13.360 --> 00:12:16.930
Once we do that, we can
run thousands-- sometimes,

00:12:16.930 --> 00:12:19.060
hundreds of thousands--
of simulations,

00:12:19.060 --> 00:12:21.370
using physical models,
hydraulic models,

00:12:21.370 --> 00:12:24.430
where we try to anticipate
where the water is going to go.

00:12:24.430 --> 00:12:26.050
In order to do that,
we need to have

00:12:26.050 --> 00:12:28.060
some readings of the
water, so that we

00:12:28.060 --> 00:12:30.065
can base our computations on.

00:12:30.065 --> 00:12:32.440
And we have here a partnership
with the Indian government

00:12:32.440 --> 00:12:34.540
to gather exactly this data.

00:12:34.540 --> 00:12:37.240
And of course, all that
needs to be done per place,

00:12:37.240 --> 00:12:39.130
because the soil
may be different.

00:12:39.130 --> 00:12:41.870
We need to match it
versus past experience.

00:12:41.870 --> 00:12:44.290
So there's a lot going
on in order to do that.

00:12:44.290 --> 00:12:46.690
And getting into this
effort, it wasn't clear

00:12:46.690 --> 00:12:48.260
whether this is feasible or not.

00:12:48.260 --> 00:12:49.640
It turns out that it is.

00:12:49.640 --> 00:12:54.190
And indeed, just last year, we
announced the pilot in Patna,

00:12:54.190 --> 00:12:56.440
this bluish area
that you see here,

00:12:56.440 --> 00:12:59.890
where we had accuracy of
90% prediction a few hours

00:12:59.890 --> 00:13:03.110
before the floods themselves,
which was very encouraging.

00:13:03.110 --> 00:13:06.370
So we doubled up on that.

00:13:06.370 --> 00:13:09.590
And we already had an
activation back in September,

00:13:09.590 --> 00:13:13.270
where we could actually
send out to people's phone,

00:13:13.270 --> 00:13:15.040
with 90% accuracy,
the area where

00:13:15.040 --> 00:13:16.970
the flood is going to occur.

00:13:16.970 --> 00:13:18.790
So based on that, we doubled up.

00:13:18.790 --> 00:13:21.190
And now, we are expanding.

00:13:21.190 --> 00:13:26.120
And we expect our effort to go
and cover order of magnitude

00:13:26.120 --> 00:13:32.380
into much of the rivers
in India and help out

00:13:32.380 --> 00:13:35.410
many more millions of
people and then, also,

00:13:35.410 --> 00:13:37.540
work on disseminating
the information,

00:13:37.540 --> 00:13:39.730
so people could just
get notifications

00:13:39.730 --> 00:13:41.470
on their phone,
which will drive them

00:13:41.470 --> 00:13:44.140
into the alert about
what's going on.

00:13:44.140 --> 00:13:46.960
And they could even go to
Map to go deeper and navigate

00:13:46.960 --> 00:13:47.830
themselves.

00:13:47.830 --> 00:13:49.330
So this is very
encouraging-- to see

00:13:49.330 --> 00:13:52.060
how exploration of
something that we don't even

00:13:52.060 --> 00:13:56.683
know possible turned out to
be something that is feasible.

00:13:56.683 --> 00:13:58.600
And we're very excited
about the collaboration

00:13:58.600 --> 00:14:01.220
also with the government.

00:14:01.220 --> 00:14:05.060
But I'd like to also point
out another instance of how

00:14:05.060 --> 00:14:07.190
AI can be used by everybody.

00:14:07.190 --> 00:14:09.630
And this is another
very inspiring example.

00:14:09.630 --> 00:14:13.880
These are two high school
students, Sanjana and Aditya,

00:14:13.880 --> 00:14:16.880
who actually took
machine learning models

00:14:16.880 --> 00:14:17.900
from TensorFlow.

00:14:17.900 --> 00:14:20.450
And they built up
small devices, using

00:14:20.450 --> 00:14:22.910
some parts they put themselves.

00:14:22.910 --> 00:14:25.400
And they put them in various
areas of their forest

00:14:25.400 --> 00:14:27.410
in California to learn.

00:14:27.410 --> 00:14:29.900
And they trained the
models to identify

00:14:29.900 --> 00:14:32.480
when there is a risk of
fire because of deadwood

00:14:32.480 --> 00:14:34.310
and other parameters.

00:14:34.310 --> 00:14:36.530
And by being able to
alert about the risk,

00:14:36.530 --> 00:14:39.530
they could actually send
notifications to Cal Fire,

00:14:39.530 --> 00:14:42.080
so they can prioritize
going to check these areas

00:14:42.080 --> 00:14:44.090
and, hopefully,
prevent some fires.

00:14:44.090 --> 00:14:47.450
And the fact that you can
have two high schoolers take,

00:14:47.450 --> 00:14:52.310
off the shelf, libraries
and just use them and solve

00:14:52.310 --> 00:14:56.000
an important problem, I
think, is a great opportunity

00:14:56.000 --> 00:14:57.440
that we have today.

00:14:57.440 --> 00:14:59.310
And it's just increasing.

00:14:59.310 --> 00:15:01.760
So if we think about
these two examples,

00:15:01.760 --> 00:15:07.580
these are good illustrations
of why we started the Google AI

00:15:07.580 --> 00:15:12.500
for Social Good program, where
we're looking more closely

00:15:12.500 --> 00:15:17.450
into a how to first double
up or focus on our research

00:15:17.450 --> 00:15:20.430
in engineering around
these societal problems

00:15:20.430 --> 00:15:23.990
and, on the other hand, how we
work with building an ecosystem

00:15:23.990 --> 00:15:28.170
and supporting the ecosystem,
with the realization that we

00:15:28.170 --> 00:15:30.500
cannot solve all
problems obviously.

00:15:30.500 --> 00:15:31.920
And in many cases,
actually, we're

00:15:31.920 --> 00:15:34.130
not even aware of the
problems that could be solved.

00:15:34.130 --> 00:15:37.220
So the opportunity for
everybody to actually pitch

00:15:37.220 --> 00:15:40.820
in and identify
problems and bring

00:15:40.820 --> 00:15:43.670
their own expertise, I
think, is a great opportunity

00:15:43.670 --> 00:15:44.820
that we have.

00:15:44.820 --> 00:15:50.720
And along with the program, we
announced the Impact Challenge,

00:15:50.720 --> 00:15:54.740
where we asked everybody
to come and submit a grant

00:15:54.740 --> 00:15:59.060
and apply for
grants and support.

00:15:59.060 --> 00:16:02.210
And we just announced
the 20 winners

00:16:02.210 --> 00:16:04.490
for that, which are coming
from all over the world

00:16:04.490 --> 00:16:06.800
and are also
touching many areas--

00:16:06.800 --> 00:16:09.590
agriculture and
emergency services.

00:16:09.590 --> 00:16:12.415
And some of them are
about societal issues.

00:16:12.415 --> 00:16:14.810
And some of them are
about suicide prevention.

00:16:14.810 --> 00:16:17.300
So we see a lot of variety.

00:16:17.300 --> 00:16:19.190
But also-- it was
great to see-- we

00:16:19.190 --> 00:16:23.510
had over 2,600 applications
from 119 countries.

00:16:23.510 --> 00:16:25.370
And 40% of them--

00:16:25.370 --> 00:16:27.360
without any background
in machine learning.

00:16:27.360 --> 00:16:29.480
So the hope is that, for
many of them, perhaps,

00:16:29.480 --> 00:16:31.730
getting some
mentorship and exposure

00:16:31.730 --> 00:16:34.760
is going to actually
help them even accelerate

00:16:34.760 --> 00:16:36.260
some of the
developments in areas

00:16:36.260 --> 00:16:38.150
where machine learning
can actually help.

00:16:38.150 --> 00:16:41.330
And indeed, in addition
to the monetary funds,

00:16:41.330 --> 00:16:44.810
we're going to kick off
support by a program

00:16:44.810 --> 00:16:47.060
that we have called
Launchpad, which is

00:16:47.060 --> 00:16:48.400
about mentorship of startups.

00:16:48.400 --> 00:16:51.600
So the notion of working with
the community is not new.

00:16:51.600 --> 00:16:53.930
In fact, we have
Launchpad program

00:16:53.930 --> 00:16:56.130
that has been running
now for a few years.

00:16:56.130 --> 00:16:59.810
And we are focusing now on
Launchpad Accelerator that

00:16:59.810 --> 00:17:02.120
provides mentorship on
machine learning and AI

00:17:02.120 --> 00:17:03.860
to startups all over the world.

00:17:03.860 --> 00:17:08.060
We had over 200 startups
going through that program

00:17:08.060 --> 00:17:11.440
and thousands of startups going
through mentorship in general

00:17:11.440 --> 00:17:12.349
by Launchpad.

00:17:12.349 --> 00:17:14.339
And Launchpad is
a global program.

00:17:14.339 --> 00:17:18.710
In fact, it started in
our office in Israel,

00:17:18.710 --> 00:17:20.060
experimentally.

00:17:20.060 --> 00:17:24.230
And since then, it's been
running in Israel, in Africa,

00:17:24.230 --> 00:17:25.770
in many other countries.

00:17:25.770 --> 00:17:27.800
And by the way,
talking about Africa--

00:17:27.800 --> 00:17:30.200
it's great to see
talent going everywhere

00:17:30.200 --> 00:17:31.400
and how to encourage that.

00:17:31.400 --> 00:17:34.790
So I was inspired,
visiting there last year,

00:17:34.790 --> 00:17:37.580
when we inaugurated a program
called the African Master

00:17:37.580 --> 00:17:39.470
for Machine Intelligence--

00:17:39.470 --> 00:17:43.370
and getting to talk with
some of the students who

00:17:43.370 --> 00:17:45.860
are representing the next
generation of leaders.

00:17:45.860 --> 00:17:48.140
And again, this is a
kind of reinforcement.

00:17:48.140 --> 00:17:50.700
And whenever I go
everywhere in the world,

00:17:50.700 --> 00:17:53.480
you can see that, really, the
talent, the entrepreneurship,

00:17:53.480 --> 00:17:58.038
the opportunity is everywhere,
which is really exciting.

00:17:58.038 --> 00:18:00.580
Let me also point out that, when
we think about collaboration

00:18:00.580 --> 00:18:04.840
in community, it's not only
big companies and startups

00:18:04.840 --> 00:18:05.945
and NGOs.

00:18:05.945 --> 00:18:07.320
Sometimes, there's
an opportunity

00:18:07.320 --> 00:18:10.150
to do even broader
collaboration.

00:18:10.150 --> 00:18:12.130
So I'll just highlight
this collaboration

00:18:12.130 --> 00:18:15.910
that we're having
with the World Bank

00:18:15.910 --> 00:18:17.830
and the United Nations
and additional tech

00:18:17.830 --> 00:18:21.765
companies to try and address
the problem of famine.

00:18:21.765 --> 00:18:23.140
So it turns out
that, even today,

00:18:23.140 --> 00:18:24.940
famine is a big
problem worldwide.

00:18:24.940 --> 00:18:26.930
It can impact
millions of people.

00:18:26.930 --> 00:18:29.950
And one of the single
most important items

00:18:29.950 --> 00:18:34.390
to help these people is to
identify famine early enough--

00:18:34.390 --> 00:18:38.590
not wait X months after it
happened to then take action.

00:18:38.590 --> 00:18:40.090
So we have collaboration
where we're

00:18:40.090 --> 00:18:44.830
trying to help out on developing
with machine learning models

00:18:44.830 --> 00:18:49.270
that can identify the
indicators of food security

00:18:49.270 --> 00:18:52.160
and, thereby, enable the World
Bank and other organizations

00:18:52.160 --> 00:18:53.890
to take actions earlier--

00:18:53.890 --> 00:18:58.940
to divert resources before
it becomes a real crisis.

00:18:58.940 --> 00:19:01.300
So the opportunity
to have collaboration

00:19:01.300 --> 00:19:03.870
on a large scale
and global scale

00:19:03.870 --> 00:19:06.100
by everybody for social good--

00:19:06.100 --> 00:19:07.790
and incorporating
AI to do that--

00:19:07.790 --> 00:19:11.050
I think, is pretty significant.

00:19:11.050 --> 00:19:15.760
Now with that, I'd like to
actually switch gear and talk

00:19:15.760 --> 00:19:17.650
about--

00:19:17.650 --> 00:19:21.130
oh, one last note about it
is that, even on floods,

00:19:21.130 --> 00:19:23.920
where we're focusing a lot
and we have a sizable team--

00:19:23.920 --> 00:19:26.590
even here, we understand
that we can only

00:19:26.590 --> 00:19:29.630
talk about a small
subset of the problem.

00:19:29.630 --> 00:19:31.960
And there are many
problems that aren't floods

00:19:31.960 --> 00:19:34.420
that we don't necessarily
have the expertise.

00:19:34.420 --> 00:19:39.700
So just recently we brought
together some 80 researchers

00:19:39.700 --> 00:19:42.010
and people working
for governments

00:19:42.010 --> 00:19:45.190
and other organizations
to discuss

00:19:45.190 --> 00:19:48.610
how we could bring together
expertise in hydrology,

00:19:48.610 --> 00:19:52.360
in physics, and machine learning
and have this conversation

00:19:52.360 --> 00:19:56.620
to see how we can actually,
together, tackle these problems

00:19:56.620 --> 00:19:59.340
and get some more
progress on that.

00:19:59.340 --> 00:20:01.400
And with that, I'd
like to switch.

00:20:01.400 --> 00:20:05.740
So if AI here is about how
to solve societal problems

00:20:05.740 --> 00:20:07.750
and really impact people's
lives-- sometimes,

00:20:07.750 --> 00:20:11.200
saves those lives, save
the site, make them safe--

00:20:11.200 --> 00:20:13.960
there is another
domain where I find

00:20:13.960 --> 00:20:19.540
that AI is becoming increasingly
more significant in our lives.

00:20:19.540 --> 00:20:22.150
And I'd like to talk a little
about conversational AI,

00:20:22.150 --> 00:20:25.130
about how we can have
conversations among ourselves.

00:20:25.130 --> 00:20:30.400
So going back to the questions
raised by [INAUDIBLE],, so many

00:20:30.400 --> 00:20:34.690
of us grew up on the
aspirational magical experience

00:20:34.690 --> 00:20:36.880
of Star Trek.

00:20:36.880 --> 00:20:40.720
Could we ever speak with a
computer in a natural way,

00:20:40.720 --> 00:20:45.070
like we do with a person, just
ask questions, get answers,

00:20:45.070 --> 00:20:46.385
get advice?

00:20:46.385 --> 00:20:48.010
We don't need to
learn new methodology.

00:20:48.010 --> 00:20:49.040
We have the interface.

00:20:49.040 --> 00:20:50.230
That's what we do.

00:20:50.230 --> 00:20:53.050
And in fact, this goes
back even to Alan Turing,

00:20:53.050 --> 00:20:54.490
founder of computer science.

00:20:54.490 --> 00:20:57.400
He was already asking the
question about, can people

00:20:57.400 --> 00:21:00.250
talk to computers,
much in the same way

00:21:00.250 --> 00:21:02.350
that they talk to
each other, famously

00:21:02.350 --> 00:21:04.368
known as the Turing Test.

00:21:04.368 --> 00:21:05.410
Now, just think about it.

00:21:05.410 --> 00:21:07.360
If you could have
this conversation,

00:21:07.360 --> 00:21:11.480
obviously, it makes it
accessible to many more people.

00:21:11.480 --> 00:21:13.420
Because all you need
is to ask the question

00:21:13.420 --> 00:21:15.560
or to ask for
something to get done.

00:21:15.560 --> 00:21:18.730
So this is, obviously,
a big problem.

00:21:18.730 --> 00:21:21.550
But we've seen some
progress in recent years.

00:21:21.550 --> 00:21:24.400
And many of them-- actually,
we don't pay much attention.

00:21:24.400 --> 00:21:26.200
So today, many of
us already, when

00:21:26.200 --> 00:21:27.880
they want to search
for information,

00:21:27.880 --> 00:21:29.673
they would just
ask their phones.

00:21:29.673 --> 00:21:31.840
Many would just talk to
their assistant or the phone

00:21:31.840 --> 00:21:33.177
to set up an alarm.

00:21:33.177 --> 00:21:34.510
And it should just work-- right?

00:21:34.510 --> 00:21:36.730
There are hundreds or
thousands of different ways

00:21:36.730 --> 00:21:39.070
in which you'd like
to set your alarm.

00:21:39.070 --> 00:21:42.110
And you expect it to just work.

00:21:42.110 --> 00:21:44.170
And of course, there are
many other situations

00:21:44.170 --> 00:21:47.720
in which we'd like to use
conversational technology.

00:21:47.720 --> 00:21:50.620
And the reason why we're making
this progress, of course,

00:21:50.620 --> 00:21:52.480
is by having speech
recognition, which

00:21:52.480 --> 00:21:54.290
is becoming better and better.

00:21:54.290 --> 00:21:56.440
And the ability for
speech synthesis--

00:21:56.440 --> 00:21:59.220
to take text and to
read it back to user.

00:21:59.220 --> 00:22:00.720
And there are
additional ingredients

00:22:00.720 --> 00:22:03.160
of a conversation that we
have had some progress on.

00:22:03.160 --> 00:22:03.850
So for example--

00:22:03.850 --> 00:22:05.140
Smart Reply.

00:22:05.140 --> 00:22:06.790
Whereas-- we have a question.

00:22:06.790 --> 00:22:09.910
We can get a guess about
what the answer may be,

00:22:09.910 --> 00:22:12.700
just based on experience,
which is actually

00:22:12.700 --> 00:22:14.500
sometimes surprising.

00:22:14.500 --> 00:22:16.000
Some people are
offended by the fact

00:22:16.000 --> 00:22:19.370
that they're a
little predictable.

00:22:19.370 --> 00:22:20.890
Smart Compose is
another example.

00:22:20.890 --> 00:22:24.190
In fact, this technology,
at some point, I believe,

00:22:24.190 --> 00:22:27.200
some April Fools'
joke by some folks.

00:22:27.200 --> 00:22:28.600
And today, they
are just working.

00:22:28.600 --> 00:22:31.507
And it's pretty
magical when it does.

00:22:31.507 --> 00:22:32.590
These are two ingredients.

00:22:32.590 --> 00:22:34.330
But I'm very
excited about, also,

00:22:34.330 --> 00:22:38.520
the technology that we can
actually speak a text--

00:22:38.520 --> 00:22:41.770
the text-to-speech synthesis,
which became very natural.

00:22:41.770 --> 00:22:45.860
So for example-- could
we just listen to pages?

00:22:45.860 --> 00:22:48.360
So in fact, we
launched last year--

00:22:48.360 --> 00:22:53.380
and we have a version of
the Google application

00:22:53.380 --> 00:22:57.130
that works in India and
Brazil and other places

00:22:57.130 --> 00:23:00.220
where, sometimes, you have
lower-end phones or internet

00:23:00.220 --> 00:23:01.660
connection, which
is constrained.

00:23:01.660 --> 00:23:02.830
It's called Google Go.

00:23:02.830 --> 00:23:05.590
And we launched with the ability
to just read whenever you

00:23:05.590 --> 00:23:06.790
see the browser or web page.

00:23:06.790 --> 00:23:07.490
You should just listen to it.

00:23:07.490 --> 00:23:07.750
[AUDIO PLAYBACK]

00:23:07.750 --> 00:23:09.850
- Up to 15% of slots,
from the new runway

00:23:09.850 --> 00:23:12.238
would be dedicated to
improving domestic connections,

00:23:12.238 --> 00:23:14.530
and the government hoped that
the increased competition

00:23:14.530 --> 00:23:17.230
with existing routes would give
greater choice to passengers.

00:23:17.230 --> 00:23:18.035
[END PLAYBACK]

00:23:18.035 --> 00:23:19.660
So this is an example
that you can just

00:23:19.660 --> 00:23:22.510
listen to the page in your
Google Go application.

00:23:22.510 --> 00:23:25.042
And the beauty here-- this
also highlights the words.

00:23:25.042 --> 00:23:26.500
So obviously, it's
for convenience.

00:23:26.500 --> 00:23:27.730
But for many people,
it turns out,

00:23:27.730 --> 00:23:29.650
it's quite important,
because of literacy--

00:23:29.650 --> 00:23:32.950
if they find it difficult to
read the text, either because

00:23:32.950 --> 00:23:34.360
of literacy issue
or because it's

00:23:34.360 --> 00:23:36.320
a foreign language to them.

00:23:36.320 --> 00:23:40.570
And we just announced
using the same technology

00:23:40.570 --> 00:23:43.552
to solve related problems, such
as "what did you see text."

00:23:43.552 --> 00:23:44.260
You're on the go.

00:23:44.260 --> 00:23:45.340
You're on the street.

00:23:45.340 --> 00:23:47.050
You'd like to understand
what's going on.

00:23:47.050 --> 00:23:50.500
Today, all you need
with, again, Let's Go,

00:23:50.500 --> 00:23:54.550
is to point out your camera to
that-- your telephone to that--

00:23:54.550 --> 00:23:55.383
and to hear what's--

00:23:55.383 --> 00:23:56.050
[VIDEO PLAYBACK]

00:23:56.050 --> 00:23:57.370
- Information for cardholders--

00:23:57.370 --> 00:24:00.160
all customers using old,
proprietary, magnetic stripe

00:24:00.160 --> 00:24:01.350
cards should be advised.

00:24:01.350 --> 00:24:02.130
[END PLAYBACK]

00:24:02.130 --> 00:24:04.130
And the beauty about
technology is that, now, we

00:24:04.130 --> 00:24:06.400
can start to put them
together and connect them

00:24:06.400 --> 00:24:08.600
in various shapes
and various forms.

00:24:08.600 --> 00:24:11.305
So for example, Translate
is technology that

00:24:11.305 --> 00:24:12.430
used to be science fiction.

00:24:12.430 --> 00:24:14.310
But today, we just
expect it to work.

00:24:14.310 --> 00:24:15.505
You open a web page.

00:24:15.505 --> 00:24:17.090
It's in a different language.

00:24:17.090 --> 00:24:18.010
You hit Translate.

00:24:18.010 --> 00:24:18.900
And you see it.

00:24:18.900 --> 00:24:20.000
And it's pretty good.

00:24:20.000 --> 00:24:22.240
It still have room
for improvement.

00:24:22.240 --> 00:24:23.560
But you get a sense of it.

00:24:23.560 --> 00:24:25.870
So what if we just
bring them together?

00:24:25.870 --> 00:24:26.780
[VIDEO PLAYBACK]

00:24:26.780 --> 00:24:30.120
- [SPEAKING SPANISH]

00:24:30.120 --> 00:24:30.890
[END PLAYBACK]

00:24:30.890 --> 00:24:34.220
Just by putting them together,
suddenly, we reduced barrier.

00:24:34.220 --> 00:24:37.640
Suddenly, a person that could
otherwise not read the sign,

00:24:37.640 --> 00:24:40.950
for some reason, cannot
hear what's going on there.

00:24:40.950 --> 00:24:44.000
Now, another
inspiring project was

00:24:44.000 --> 00:24:46.100
developed by Dimitri and Chen.

00:24:46.100 --> 00:24:50.310
So Dimitri is deaf.

00:24:50.310 --> 00:24:55.040
And he worked on taking the
speech recognition technology

00:24:55.040 --> 00:24:58.250
and making it work in what
we call Live Transcribe.

00:24:58.250 --> 00:25:01.290
So you can just listen
to the conversation

00:25:01.290 --> 00:25:02.940
and seeing it on your phone.

00:25:02.940 --> 00:25:06.418
And this has been, now,
going on for some time.

00:25:06.418 --> 00:25:08.210
But when you think
about these technologies

00:25:08.210 --> 00:25:10.400
of speech recognition--

00:25:10.400 --> 00:25:14.270
text-to-speech-- there's
more that we can do.

00:25:14.270 --> 00:25:18.440
So if you cannot hear today,
we can just click on the button

00:25:18.440 --> 00:25:22.430
and get those live captions,
which we just announced.

00:25:22.430 --> 00:25:24.740
And think about people
who cannot hear.

00:25:24.740 --> 00:25:27.230
Now, everything that
your phone is talking

00:25:27.230 --> 00:25:28.540
becomes available for them.

00:25:31.184 --> 00:25:31.851
[VIDEO PLAYBACK]

00:25:31.851 --> 00:25:33.950
- Do you like the blueberries?

00:25:33.950 --> 00:25:34.600
- Yeah.

00:25:34.600 --> 00:25:36.010
- Blueberries?

00:25:36.010 --> 00:25:38.020
Delicious.

00:25:38.020 --> 00:25:39.900
A couple more.

00:25:39.900 --> 00:25:41.280
Mm.

00:25:41.280 --> 00:25:42.330
[INAUDIBLE]

00:25:42.330 --> 00:25:43.480
[END PLAYBACK]

00:25:44.350 --> 00:25:48.910
So you can apply it for,
essentially, anything

00:25:48.910 --> 00:25:51.750
that your phone
would sound to you.

00:25:51.750 --> 00:25:55.182
And if you think about
it, for some people,

00:25:55.182 --> 00:25:56.640
it's actually making
the difference

00:25:56.640 --> 00:25:59.490
between understanding what's
going on and what not.

00:25:59.490 --> 00:26:03.380
For many, or for all of
us, it's really about

00:26:03.380 --> 00:26:05.130
when you get into
situations that you just

00:26:05.130 --> 00:26:08.400
don't want to hear the sound or
you just want to also read it.

00:26:08.400 --> 00:26:10.140
Now, the way I'm
thinking about it

00:26:10.140 --> 00:26:12.240
is that this is
really re-imagining

00:26:12.240 --> 00:26:14.130
what the Mute button is.

00:26:14.130 --> 00:26:17.320
Mute is not about, I don't
want to know what's being said.

00:26:17.320 --> 00:26:19.980
It's about, I don't want
any noise to be made.

00:26:19.980 --> 00:26:22.620
So you can actually
just cross modality.

00:26:22.620 --> 00:26:26.430
You can just move the hearing
from the modality of audio

00:26:26.430 --> 00:26:28.020
to a modality of screen.

00:26:28.020 --> 00:26:30.600
And if you think about
removing barriers and the fact

00:26:30.600 --> 00:26:33.120
that, with technology
today, we can just

00:26:33.120 --> 00:26:36.750
move the conversation in a
fluent way between modalities

00:26:36.750 --> 00:26:39.720
or have them in both
modalities, from audio to text,

00:26:39.720 --> 00:26:42.660
from text to audio, the
opportunities are plenty.

00:26:42.660 --> 00:26:44.640
And we have more
examples-- right?

00:26:44.640 --> 00:26:48.090
So now, going back to Dimitri.

00:26:48.090 --> 00:26:51.750
He can hear what other
people are saying,

00:26:51.750 --> 00:26:53.880
because we have voice
recognition that works pretty

00:26:53.880 --> 00:26:58.080
well for those who are
talking with common accents

00:26:58.080 --> 00:26:59.970
and common pronunciation.

00:26:59.970 --> 00:27:03.210
But when he's speaking--
because he has a heavy accent

00:27:03.210 --> 00:27:05.100
and he became deaf
at a younger age--

00:27:05.100 --> 00:27:08.673
our speech recognition
devices actually don't really

00:27:08.673 --> 00:27:09.840
understanding him very well.

00:27:09.840 --> 00:27:10.935
And you've seen it before.

00:27:10.935 --> 00:27:13.395
[VIDEO PLAYBACK]

00:27:25.604 --> 00:27:26.400
[END PLAYBACK]

00:27:26.400 --> 00:27:29.090
So the premise of--

00:27:29.090 --> 00:27:31.430
just talk to your phone
and get stuff done--

00:27:31.430 --> 00:27:33.540
doesn't quite work for him.

00:27:33.540 --> 00:27:36.590
And this doesn't quite
fit our commitment

00:27:36.590 --> 00:27:38.810
to make Google work for
everyone and to make

00:27:38.810 --> 00:27:40.830
technology work for everyone.

00:27:40.830 --> 00:27:44.270
So we embarked on this
project, Euphonia,

00:27:44.270 --> 00:27:47.150
which actually was starting
out by asking ourselves,

00:27:47.150 --> 00:27:51.590
can we help ALS patients,
whose speech deteriorates,

00:27:51.590 --> 00:27:53.300
to be understood?

00:27:53.300 --> 00:27:56.690
And we trained and we built up
technology that can actually

00:27:56.690 --> 00:28:01.270
take personalized training of
models for speech recognition.

00:28:01.270 --> 00:28:03.740
And Dimitri actually
took these models.

00:28:03.740 --> 00:28:05.210
He worked with our engineers.

00:28:05.210 --> 00:28:07.670
And he's put a lot
of time to train

00:28:07.670 --> 00:28:09.890
on thousands of sentences.

00:28:09.890 --> 00:28:14.627
And the result was actually
quite not what we expected.

00:28:14.627 --> 00:28:16.460
The error rate that
he's getting is actually

00:28:16.460 --> 00:28:20.360
in par with what we're
getting on a regular speech

00:28:20.360 --> 00:28:21.110
recognition.

00:28:21.110 --> 00:28:23.210
Of course, this is
still in research stage.

00:28:23.210 --> 00:28:26.180
And we need to find
ways not to rely

00:28:26.180 --> 00:28:27.827
on those thousands of training.

00:28:27.827 --> 00:28:29.660
But here's the example
of what we get today.

00:28:29.660 --> 00:28:30.900
[VIDEO PLAYBACK]

00:28:45.257 --> 00:28:45.840
[END PLAYBACK]

00:28:45.840 --> 00:28:48.170
So this is pretty
transformative, of course.

00:28:48.170 --> 00:28:51.812
And again, we are getting
some very positive feedback

00:28:51.812 --> 00:28:53.270
from ALS patients
who we're working

00:28:53.270 --> 00:28:56.330
with about the ability for them
to express themselves in a way

00:28:56.330 --> 00:28:58.710
that, now, it can be understood.

00:28:58.710 --> 00:29:01.130
So I'm really excited
about this technology.

00:29:01.130 --> 00:29:04.363
And again, as it goes
with accessibility--

00:29:04.363 --> 00:29:05.780
interestingly
enough, quite often,

00:29:05.780 --> 00:29:08.210
when you develop
for the hard cases,

00:29:08.210 --> 00:29:11.090
it turns out that you can make
progress for many more cases

00:29:11.090 --> 00:29:12.270
that you think about.

00:29:12.270 --> 00:29:17.220
So for example, the team
wrote a research paper,

00:29:17.220 --> 00:29:19.130
which showed that the
same technology that

00:29:19.130 --> 00:29:21.230
is used for those
extreme cases can

00:29:21.230 --> 00:29:24.050
have a tremendous
improvement over known data

00:29:24.050 --> 00:29:28.190
sets with accents.

00:29:28.190 --> 00:29:30.980
And the way this works is,
essentially, applying--

00:29:30.980 --> 00:29:33.510
building up models,
training data

00:29:33.510 --> 00:29:35.740
sets, based on
visual spectrograms,

00:29:35.740 --> 00:29:37.040
then training them.

00:29:37.040 --> 00:29:39.830
And we need plenty of them,
in order to really build it

00:29:39.830 --> 00:29:41.070
in a personalized way.

00:29:41.070 --> 00:29:44.540
So these are the different kind
of fingerprints, if you will,

00:29:44.540 --> 00:29:48.970
of the diverse
spectrograms that are here.

00:29:48.970 --> 00:29:51.190
So these are all good.

00:29:51.190 --> 00:29:55.440
And when we talk me into our
phone and with our assistants,

00:29:55.440 --> 00:29:58.500
this is the problems that
we're typically facing.

00:29:58.500 --> 00:30:01.500
But I actually want to
go a little bit back

00:30:01.500 --> 00:30:03.670
to an older technology--

00:30:03.670 --> 00:30:04.200
the phone.

00:30:04.200 --> 00:30:06.450
And I appreciate
that some of you,

00:30:06.450 --> 00:30:09.700
perhaps, are not familiar
with this device.

00:30:09.700 --> 00:30:11.790
So this is a phone,
as it used to be

00:30:11.790 --> 00:30:13.600
at some point in the past.

00:30:13.600 --> 00:30:16.940
So it turns out that, even
though much of our activities

00:30:16.940 --> 00:30:19.200
are with our assistants,
with our phones,

00:30:19.200 --> 00:30:21.390
there are plenty of
cases that people still

00:30:21.390 --> 00:30:23.880
need to pick up the phone
to make a conversation.

00:30:23.880 --> 00:30:27.120
And for example, it turns
out that, even though you'd

00:30:27.120 --> 00:30:29.000
expect that a lot
of what we do today

00:30:29.000 --> 00:30:31.620
is just online
reservation, et cetera,

00:30:31.620 --> 00:30:35.430
60% of businesses in the US
that are relying on reservation

00:30:35.430 --> 00:30:37.657
do not have an online
reservation setup.

00:30:37.657 --> 00:30:39.240
And if you really
want to talk to them

00:30:39.240 --> 00:30:41.760
or to do something with them,
you need to pick up the phone.

00:30:41.760 --> 00:30:43.750
And pick up the phone--

00:30:43.750 --> 00:30:45.720
so we need to take care of that.

00:30:45.720 --> 00:30:48.180
And then, pick up the phone,
sometimes, is irritating.

00:30:48.180 --> 00:30:51.270
And for some, it's impossible,
either because of circumstances

00:30:51.270 --> 00:30:53.650
or because accessibility
of other reasons.

00:30:53.650 --> 00:30:56.370
So to that end, we
announced last year Duplex,

00:30:56.370 --> 00:31:01.320
which enables one to have a
phone call made on their behalf

00:31:01.320 --> 00:31:04.200
by the assistant to
make phone reservations

00:31:04.200 --> 00:31:06.550
and to do it in a
pretty natural way.

00:31:06.550 --> 00:31:08.310
And indeed, today,
this is already

00:31:08.310 --> 00:31:11.010
working in 44 states in the US.

00:31:11.010 --> 00:31:15.300
And we get some great feedback
from users and from businesses.

00:31:15.300 --> 00:31:19.230
And what enabled
Duplex to do the job

00:31:19.230 --> 00:31:21.480
is really combination
of many technologies.

00:31:21.480 --> 00:31:23.160
And they have to
do with analysis

00:31:23.160 --> 00:31:27.180
of the audio source and
the speech recognition

00:31:27.180 --> 00:31:29.400
and the text-to-speech
and, of course,

00:31:29.400 --> 00:31:32.303
using deep learning networks
that are built for that,

00:31:32.303 --> 00:31:33.720
because we need
to understand also

00:31:33.720 --> 00:31:35.400
the intent of the conversation.

00:31:35.400 --> 00:31:37.470
And all that is possible,
because we focused

00:31:37.470 --> 00:31:39.490
on very specific domains.

00:31:39.490 --> 00:31:41.550
So we could actually
build those models

00:31:41.550 --> 00:31:44.610
in the way that work
sufficiently high-quality.

00:31:44.610 --> 00:31:47.080
To do that, along with a
lot of machinery around it,

00:31:47.080 --> 00:31:50.430
it touches real-time
supervised trainings and more.

00:31:50.430 --> 00:31:52.260
And of course, we
also added to that.

00:31:52.260 --> 00:31:56.340
Since our aspiration was to have
a very natural conversation,

00:31:56.340 --> 00:32:00.475
then we added those particular
speech disfluencies, which

00:32:00.475 --> 00:32:01.725
are part of the communication.

00:32:01.725 --> 00:32:03.640
They are part of
the conversation.

00:32:03.640 --> 00:32:06.000
They are actually part
of how we communicate.

00:32:06.000 --> 00:32:09.450
It's part of how
we're saying, no,

00:32:09.450 --> 00:32:14.050
in a soft way, in a polite way.

00:32:14.050 --> 00:32:16.952
It's how we acknowledge, while
waiting for the other side

00:32:16.952 --> 00:32:17.910
to actually talk to us.

00:32:17.910 --> 00:32:19.535
There's actually a
field in linguistics

00:32:19.535 --> 00:32:22.270
called Pragmatics, which looks
into some of these speech

00:32:22.270 --> 00:32:23.310
disfluencies.

00:32:23.310 --> 00:32:25.440
And these, put in
all together, enables

00:32:25.440 --> 00:32:28.650
us to have a conversation
that is natural

00:32:28.650 --> 00:32:31.090
and can achieve this goal.

00:32:31.090 --> 00:32:33.300
So there are many
other natural ways

00:32:33.300 --> 00:32:37.200
in which natural
conversation could help us.

00:32:37.200 --> 00:32:40.500
And I really encourage
everybody to think

00:32:40.500 --> 00:32:43.000
and to apply their imagination
about what can be done.

00:32:43.000 --> 00:32:45.270
In fact, some time
ago, I was having

00:32:45.270 --> 00:32:48.210
a conversation with my wife
about conversational AI.

00:32:48.210 --> 00:32:51.293
So she actually pointed
out to me an opportunity.

00:32:51.293 --> 00:32:52.710
She actually
challenged me-- well,

00:32:52.710 --> 00:32:54.630
if you can do conversational
AI, why can't you

00:32:54.630 --> 00:32:56.620
solve this problem for me?

00:32:56.620 --> 00:32:59.730
And then she pointed out that,
every time the phone calls

00:32:59.730 --> 00:33:04.090
and there's an unknown
number, she has a dilemma.

00:33:04.090 --> 00:33:06.750
Well-- perhaps it's a sick
child or a sick parent.

00:33:06.750 --> 00:33:08.940
But if she would
answer the phone,

00:33:08.940 --> 00:33:11.310
then it's likely going
to be somebody trying

00:33:11.310 --> 00:33:14.580
to set her on a cruise
or a new insurance, which

00:33:14.580 --> 00:33:16.120
is pretty annoying.

00:33:16.120 --> 00:33:18.270
And I'm sure we've
all experienced that.

00:33:18.270 --> 00:33:21.240
So could we use conversational
AI to help with that?

00:33:21.240 --> 00:33:25.710
Could we use it to actually
take some of the burden from us,

00:33:25.710 --> 00:33:28.690
give us back our control of
our time, of our attention,

00:33:28.690 --> 00:33:30.090
et cetera?

00:33:30.090 --> 00:33:32.850
So indeed-- fast
forward-- we announced

00:33:32.850 --> 00:33:35.730
just a few months ago, a
feature called Call Screen.

00:33:35.730 --> 00:33:38.850
And Call Screen is
utilizing conversational AI

00:33:38.850 --> 00:33:42.540
in a basic way to really answer
the phone on your behalf,

00:33:42.540 --> 00:33:45.660
if you'd like to,
on unknown numbers,

00:33:45.660 --> 00:33:47.680
and to help you figure
out who's calling,

00:33:47.680 --> 00:33:51.180
by asking the other side
and then showing you

00:33:51.180 --> 00:33:55.550
the result in real-time,
by transcribing it to you

00:33:55.550 --> 00:33:57.640
and then enabling
you to actually ask

00:33:57.640 --> 00:34:00.350
some additional questions and
eventually pick up the phone

00:34:00.350 --> 00:34:03.930
or decline or just report spam.

00:34:03.930 --> 00:34:06.240
And really, this is
facilitated by a combination

00:34:06.240 --> 00:34:08.969
of all these technologies--
of speech recognition,

00:34:08.969 --> 00:34:11.570
of text-to-speech synthesis.

00:34:11.570 --> 00:34:13.980
And everything runs on device.

00:34:13.980 --> 00:34:15.230
So it can be offline.

00:34:15.230 --> 00:34:16.909
It's totally private.

00:34:16.909 --> 00:34:19.389
And the fact that it
can run on a device

00:34:19.389 --> 00:34:23.600
is really one of the significant
advancements that we've seen.

00:34:23.600 --> 00:34:26.449
And you've heard
about it as well--

00:34:26.449 --> 00:34:30.560
the fact that we can actually
do things, which is instant,

00:34:30.560 --> 00:34:35.690
isolated, it's yours,
and it doesn't actually

00:34:35.690 --> 00:34:36.800
leave your device.

00:34:36.800 --> 00:34:39.830
So obviously, this is a
highly popular feature,

00:34:39.830 --> 00:34:41.580
because everybody can relate to.

00:34:41.580 --> 00:34:43.580
It was actually interesting
to hear from people,

00:34:43.580 --> 00:34:47.929
not only how happy
they are for never

00:34:47.929 --> 00:34:50.090
need to speak with
telemarketing again,

00:34:50.090 --> 00:34:53.230
but you also heard from a few
people that, because of that,

00:34:53.230 --> 00:34:55.880
they actually took a call
that they would otherwise

00:34:55.880 --> 00:34:58.670
ignore that turned out to
be a hugely important call.

00:34:58.670 --> 00:35:03.500
So think about it as a way to
help get control of our time,

00:35:03.500 --> 00:35:04.670
of our attention.

00:35:04.670 --> 00:35:07.580
And of course, this can
go a long way as well.

00:35:07.580 --> 00:35:11.540
So one direction in which
this can go is actually,

00:35:11.540 --> 00:35:14.300
again, coming from an
engineer on my team

00:35:14.300 --> 00:35:16.290
who came to me one
day and say, hey,

00:35:16.290 --> 00:35:18.800
my heart is really
with accessibility.

00:35:18.800 --> 00:35:22.580
And I'd like to take
Call Screen and extend it

00:35:22.580 --> 00:35:25.070
so that people who are deaf
could have a full phone

00:35:25.070 --> 00:35:30.820
conversation, by adding
additional technologies.

00:35:30.820 --> 00:35:34.270
And here, in order to do that,
you need to be synchronous.

00:35:34.270 --> 00:35:36.610
You need to be able
to type in real-time.

00:35:36.610 --> 00:35:38.575
But then we could use
all those features

00:35:38.575 --> 00:35:39.700
that we mentioned earlier--

00:35:39.700 --> 00:35:42.410
Smart Compose, Smart Reply.

00:35:42.410 --> 00:35:45.782
And indeed, this is
still in research stage.

00:35:45.782 --> 00:35:46.740
But this is an example.

00:35:46.740 --> 00:35:47.407
[VIDEO PLAYBACK]

00:35:47.407 --> 00:35:48.413
[PHONE CHIMING]

00:35:50.300 --> 00:35:52.610
- Hi, this is Nicole's
assistive chat.

00:35:52.610 --> 00:35:53.750
She'll see what you say.

00:35:53.750 --> 00:35:57.530
And her responses will be read
back to you-- starting now.

00:35:57.530 --> 00:35:58.710
- Hi, Nicole, it's Jamie.

00:35:58.710 --> 00:35:59.210
How are you?

00:36:02.370 --> 00:36:04.290
- Hey, Jamie.

00:36:04.290 --> 00:36:04.920
I'm good.

00:36:04.920 --> 00:36:06.470
And you?

00:36:06.470 --> 00:36:08.970
- Great-- are we still on for
your 1:00 PM haircut tomorrow?

00:36:13.290 --> 00:36:15.500
- Sorry-- can you do 3:00 PM?

00:36:15.500 --> 00:36:17.010
- Uh-- yes.

00:36:17.010 --> 00:36:18.390
I can do 3:00 PM.

00:36:18.390 --> 00:36:19.890
We have a lot to catch up on.

00:36:19.890 --> 00:36:23.590
I want to hear all
about your trip.

00:36:23.590 --> 00:36:25.960
- Perfect-- thumbs up.

00:36:25.960 --> 00:36:27.430
- Great, see you tomorrow.

00:36:27.430 --> 00:36:28.227
Bye.

00:36:28.227 --> 00:36:28.810
[END PLAYBACK]

00:36:28.810 --> 00:36:30.850
So think about everything
that comes together

00:36:30.850 --> 00:36:32.860
in order to facilitate this--

00:36:32.860 --> 00:36:35.800
speech recognition,
text-to-speech,

00:36:35.800 --> 00:36:38.040
guessing how to
type, helping it out.

00:36:38.040 --> 00:36:39.790
But think about the
ramifications of that.

00:36:39.790 --> 00:36:41.740
It means that,
essentially, a person

00:36:41.740 --> 00:36:44.540
could have a regular phone
conversation on the one hand.

00:36:44.540 --> 00:36:49.180
The other person doesn't need
to talk or hear, perhaps,

00:36:49.180 --> 00:36:50.770
because they cannot
talk or hear.

00:36:50.770 --> 00:36:52.600
Perhaps, it's not
convenient to them.

00:36:52.600 --> 00:36:54.400
Perhaps, it's a new
generation that doesn't

00:36:54.400 --> 00:36:55.483
like to talk on the phone.

00:36:55.483 --> 00:36:58.200
They are just used to chatting.

00:36:58.200 --> 00:36:59.640
This is cross-modality.

00:36:59.640 --> 00:37:01.470
Perhaps, you're in a
meeting or on a flight

00:37:01.470 --> 00:37:04.670
and you'd like to take a phone
call over some other ways.

00:37:04.670 --> 00:37:07.120
And think about-- if we would
integrate, in the future--

00:37:07.120 --> 00:37:10.215
say-- translation--
the opportunity

00:37:10.215 --> 00:37:12.090
to actually have a
conversation with somebody

00:37:12.090 --> 00:37:16.288
in different language, totally
seamless, totally ambient,

00:37:16.288 --> 00:37:18.330
in a way that, actually,
we don't pay attention--

00:37:18.330 --> 00:37:19.800
just reducing barriers.

00:37:19.800 --> 00:37:22.720
It just lets people to
communicate in a better way.

00:37:22.720 --> 00:37:26.340
So these are pretty exciting
opportunities, I think.

00:37:26.340 --> 00:37:30.590
I think these are
examples where we're

00:37:30.590 --> 00:37:35.743
solving a problem,
sometimes, for one person

00:37:35.743 --> 00:37:37.160
or because we're
excited about it.

00:37:37.160 --> 00:37:40.670
And then it turns out to
solve a bigger problem.

00:37:40.670 --> 00:37:43.420
But in many of these examples--

00:37:43.420 --> 00:37:45.570
to reflect from them--

00:37:45.570 --> 00:37:49.060
many of them were
actually situations

00:37:49.060 --> 00:37:51.490
that we not necessarily
anticipated.

00:37:51.490 --> 00:37:55.190
They came out from a single
passion or from exploration.

00:37:55.190 --> 00:37:56.690
Or in the case of
flood forecasting,

00:37:56.690 --> 00:37:58.660
we didn't know
that it's solvable.

00:37:58.660 --> 00:38:05.510
Actually, even when starting
Call Screen and Live Relay,

00:38:05.510 --> 00:38:07.010
it wasn't clear
that we can actually

00:38:07.010 --> 00:38:09.920
put the technologies altogether
to get to the right level.

00:38:09.920 --> 00:38:12.788
Until we start, we don't
even know that we do that.

00:38:12.788 --> 00:38:14.330
And I was reminded
of that, actually,

00:38:14.330 --> 00:38:17.240
in a recent vacation
in Spain when

00:38:17.240 --> 00:38:19.610
having a morning run
and, at some point,

00:38:19.610 --> 00:38:23.960
deciding spontaneously
to take a detour

00:38:23.960 --> 00:38:29.270
and found these amazing bays
and landscapes that otherwise I

00:38:29.270 --> 00:38:30.077
would miss.

00:38:30.077 --> 00:38:32.410
So I think that research and
technology have this nature

00:38:32.410 --> 00:38:35.080
that, quite often, we need
to have this exploration

00:38:35.080 --> 00:38:36.560
and we need to try.

00:38:36.560 --> 00:38:40.280
And that's some of the magic
that's happening on those.

00:38:40.280 --> 00:38:45.070
Now, reflecting on where we
are with conversational AI--

00:38:45.070 --> 00:38:47.660
I think we're in
very exciting times.

00:38:47.660 --> 00:38:50.510
Think about all the
technologies and everything

00:38:50.510 --> 00:38:54.950
that could happen if we have
all these technologies get

00:38:54.950 --> 00:38:57.620
to a much, even better accuracy,
in a way that we don't even

00:38:57.620 --> 00:38:58.830
pay attention to them.

00:38:58.830 --> 00:39:01.900
Think about the barriers
that can be reduced,

00:39:01.900 --> 00:39:04.340
the fact that every
person can ask a question,

00:39:04.340 --> 00:39:08.330
can get stuff done in a
seamless way, in an ambient way.

00:39:08.330 --> 00:39:11.555
The magic is that, when
the technology just works,

00:39:11.555 --> 00:39:12.680
it becomes totally ambient.

00:39:12.680 --> 00:39:16.530
That's what I'd like to think
about as ambient intelligence.

00:39:16.530 --> 00:39:21.600
And another reflection
is, if we think back,

00:39:21.600 --> 00:39:23.350
today, we're having
all these technologies

00:39:23.350 --> 00:39:26.560
and all these products
that a few years ago

00:39:26.560 --> 00:39:29.560
were just aspirational.

00:39:29.560 --> 00:39:33.050
We didn't even know if we
can actually solve them.

00:39:33.050 --> 00:39:34.780
We didn't know that
actually today we

00:39:34.780 --> 00:39:38.210
could have these conversations
and hear back and have all

00:39:38.210 --> 00:39:40.630
this stuff just working for us.

00:39:40.630 --> 00:39:43.550
Which interestingly, if
you think about the future,

00:39:43.550 --> 00:39:45.200
what is it going to entail?

00:39:45.200 --> 00:39:47.360
So it's very
difficult to predict.

00:39:47.360 --> 00:39:53.650
But I think that one can expect
that, even though I don't think

00:39:53.650 --> 00:39:56.770
we're going to see anytime
soon a "Beam me up, Scotty"

00:39:56.770 --> 00:40:00.220
kind of technology, for many
other technologies, if you

00:40:00.220 --> 00:40:03.250
can dream them, you can
probably build them.

00:40:03.250 --> 00:40:04.480
Thank you very much.

00:40:04.480 --> 00:40:07.230
[MUSIC PLAYING]

