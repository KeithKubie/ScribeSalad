WEBVTT
Kind: captions
Language: en

00:00:00.340 --> 00:00:00.920
Okay, Michael.
So

00:00:00.920 --> 00:00:03.610
with that, I think we're kind
of done with this lesson.

00:00:03.610 --> 00:00:07.550
Those are the three things
that I wanted to mention and

00:00:07.550 --> 00:00:10.670
I think we can now go back
to where you left off and

00:00:10.670 --> 00:00:13.200
talk about what we have learned,
except I have the pen.

00:00:13.200 --> 00:00:15.500
And so
you have to tell me what we've learned.

00:00:15.500 --> 00:00:16.210
I don't have to tell you.

00:00:16.210 --> 00:00:16.710
&gt;&gt; This is true.

00:00:16.710 --> 00:00:18.560
So now when you see the three things,

00:00:18.560 --> 00:00:20.500
what were the three things
that you talked about?

00:00:20.500 --> 00:00:23.456
&gt;&gt; Your trying to trick me into
telling me what we learned.

00:00:23.456 --> 00:00:25.411
&gt;&gt; [LAUGH] Yeah a little bit.

00:00:25.411 --> 00:00:26.261
All right, so.

00:00:26.261 --> 00:00:26.941
All right.

00:00:26.941 --> 00:00:29.420
So when I had the pen, we talked about.

00:00:29.420 --> 00:00:32.380
Well two main things,
well the overall topic is the CCC.

00:00:32.380 --> 00:00:32.960
&gt;&gt; Right.

00:00:32.960 --> 00:00:35.595
&gt;&gt; Which is Comedy Central Committees.

00:00:35.595 --> 00:00:36.738
[SOUND]
&gt;&gt; No!

00:00:36.738 --> 00:00:39.150
&gt;&gt; Closed caption clowns.

00:00:39.150 --> 00:00:39.680
&gt;&gt; No.

00:00:39.680 --> 00:00:42.450
&gt;&gt; So, coordinating,
communicating, and coaching.

00:00:42.450 --> 00:00:43.050
&gt;&gt; Right.

00:00:43.050 --> 00:00:44.300
&gt;&gt; Ha!
&gt;&gt; I think what we learned is that

00:00:44.300 --> 00:00:45.440
alliteration can be fun.

00:00:45.440 --> 00:00:50.000
&gt;&gt; Well, sure, and in particular,
I talked about two particular models.

00:00:50.000 --> 00:00:52.394
One was the DEC POMD VP model.

00:00:52.394 --> 00:00:54.590
Which least at least let us define or

00:00:54.590 --> 00:00:58.640
ask questions about what it means to
coordinate and communicate optimally

00:00:58.640 --> 00:01:03.500
when we've got agents that don't share
a brain but they do share the reward

00:01:03.500 --> 00:01:07.280
function so what's good for each one
of them is good for the community.

00:01:07.280 --> 00:01:07.900
&gt;&gt; Right.

00:01:07.900 --> 00:01:10.430
Shared our distinct brains.

00:01:10.430 --> 00:01:12.240
&gt;&gt; I like that.

00:01:12.240 --> 00:01:15.020
&gt;&gt; Then we talked about
the inverse reinforcement learning

00:01:15.020 --> 00:01:19.090
idea which was that we could actually
use examples of behavior demonstrations

00:01:19.090 --> 00:01:23.030
of behavior to infer reward functions
and then once we have those reward

00:01:23.030 --> 00:01:25.465
functions we could try to
optimize them in other settings.

00:01:25.465 --> 00:01:28.180
&gt;&gt; Yep we go from behaviors to
rewards instead of going from

00:01:28.180 --> 00:01:29.890
rewards to behavior.

00:01:29.890 --> 00:01:32.070
And that is what makes it inverse.

00:01:32.070 --> 00:01:34.060
&gt;&gt; And having those rewards
means that it generalizes to

00:01:34.060 --> 00:01:34.970
other settings as well.

00:01:34.970 --> 00:01:38.860
&gt;&gt; Right, and I think that a point that
I made on that, when I started taking

00:01:38.860 --> 00:01:43.520
over the pen, was that this actually
is a kind of reward shaping,

00:01:43.520 --> 00:01:47.830
by getting human beings to give us
hints about what we ought to be doing.

00:01:47.830 --> 00:01:50.240
&gt;&gt; Okay, but
in the form of behavior demonstrations.

00:01:50.240 --> 00:01:52.080
But in the form of
behavior demonstrations.

00:01:52.080 --> 00:01:55.610
And that is a nice little
point worth making,

00:01:55.610 --> 00:01:59.270
which is demonstrations
are one way to do coaching.

00:01:59.270 --> 00:02:00.980
What were the other ways
we did some coaching?

00:02:00.980 --> 00:02:03.950
&gt;&gt; So,
yeah demonstrations is what we did and

00:02:03.950 --> 00:02:07.130
then there was this kind
of policy feedback policy.

00:02:07.130 --> 00:02:08.789
I guess you called it policy shaping.

00:02:08.789 --> 00:02:10.190
&gt;&gt; Right.
&gt;&gt; But it was a sort of idea that you

00:02:10.190 --> 00:02:12.680
can get feedback on the actions
being taken by the agent and

00:02:12.680 --> 00:02:13.900
that can be incorporated or

00:02:13.900 --> 00:02:18.130
combined with other suggestions as well
as what the learner is picking up on.

00:02:18.130 --> 00:02:22.270
&gt;&gt; And so between reward shaping,
demonstrations and policy shaping,

00:02:22.270 --> 00:02:28.070
we got a nice little set of ways that
human beings can communicate to agents.

00:02:28.070 --> 00:02:30.050
So let's see.
I know I did more than this.

00:02:30.050 --> 00:02:31.260
There were some examples that I used.

00:02:31.260 --> 00:02:32.560
What kind of examples did I use?

00:02:32.560 --> 00:02:35.215
&gt;&gt; Well, in the policy shaping setting,
we talked about Pac-Man.

00:02:35.215 --> 00:02:36.240
Pac-Man
&gt;&gt; Yeah.

00:02:36.240 --> 00:02:39.390
So we learned that Pac-Man is like
the one example you should use

00:02:39.390 --> 00:02:40.140
for everything.

00:02:40.140 --> 00:02:45.300
&gt;&gt; Sure except for what we did at
the end, end which was drama management.

00:02:45.300 --> 00:02:47.550
&gt;&gt; Right,
right we talked about drama management.

00:02:47.550 --> 00:02:49.260
And in particular, [LAUGH] yeah.

00:02:49.260 --> 00:02:51.820
In particular we introduced TTD-MDPs.

00:02:51.820 --> 00:02:56.470
Where TTD MDP has something to
do with trajectories and drama.

00:02:56.470 --> 00:02:57.900
&gt;&gt; Targets and stuff.

00:02:57.900 --> 00:02:58.720
That's exactly right.

00:02:58.720 --> 00:03:01.360
And I think we had a nice
little conversation here,

00:03:01.360 --> 00:03:04.630
tying some of these things
back to things we did before.

00:03:04.630 --> 00:03:08.940
We actually returned and
brought up again constraints and

00:03:08.940 --> 00:03:11.720
options as a way of doing coaching and
communication.

00:03:11.720 --> 00:03:16.100
And try to really drive home the point
that there are some human beings and

00:03:16.100 --> 00:03:19.060
they think a particular way and
then there are machines and

00:03:19.060 --> 00:03:20.870
they think a particular way.

00:03:20.870 --> 00:03:24.695
And we should try to move
the machines closer to the humans.

00:03:24.695 --> 00:03:27.455
Rather than forcing the humans to come
all the way over to the machines.

00:03:27.455 --> 00:03:28.565
&gt;&gt; Yeah I like that as a philosophy.

00:03:28.565 --> 00:03:30.225
&gt;&gt; And
that's really what I think is important.

00:03:30.225 --> 00:03:31.275
Yeah that's a good philosophy.

00:03:31.275 --> 00:03:33.515
Well I think that covers
everything Michael.

00:03:33.515 --> 00:03:34.695
&gt;&gt; Nice.
&gt;&gt; I feel pretty good about that.

00:03:34.695 --> 00:03:36.405
Nice, so does this mean we're done?

00:03:36.405 --> 00:03:38.855
I think this means we're done
with the class, kind of.

00:03:38.855 --> 00:03:40.405
&gt;&gt; Really?
So maybe we should do some kind of

00:03:40.405 --> 00:03:42.575
wrap up to see how
everything ties together.

00:03:42.575 --> 00:03:43.075
&gt;&gt; Yeah, let's do that.

00:03:43.075 --> 00:03:43.965
That way I don't have to say goodbye,

00:03:43.965 --> 00:03:47.980
why don't you come down to Atlanta and
maybe we can record some stuff together.

00:03:47.980 --> 00:03:49.120
&gt;&gt; Great I will have done that.

00:03:49.120 --> 00:03:51.382
&gt;&gt; Perfect.
Or is it perfect or

00:03:51.382 --> 00:03:53.834
is that the past perfect poo perfect.

00:03:53.834 --> 00:03:54.410
&gt;&gt; Poo perfect.

00:03:54.410 --> 00:03:56.335
&gt;&gt; Yes the perfect good perfect.

00:03:56.335 --> 00:03:58.938
Okay Michael,
well I will see you in Atlanta soon.

00:03:58.938 --> 00:03:59.851
&gt;&gt; Bye Charles.

00:03:59.851 --> 00:04:00.351
&gt;&gt; Bye.

