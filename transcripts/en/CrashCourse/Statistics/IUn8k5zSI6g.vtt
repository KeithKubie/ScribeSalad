WEBVTT
Kind: captions
Language: en

00:00:03.110 --> 00:00:05.920
Hi, I’m Adriene Hill, and welcome back to
Crash Course Statistics.

00:00:05.920 --> 00:00:10.160
In the last episode, we talked about using
Machine Learning with data that already has

00:00:10.160 --> 00:00:12.160
categories that we want to predict.

00:00:12.160 --> 00:00:16.010
Like teaching a computer to tell whether an
image contains a hotdog or not.

00:00:16.010 --> 00:00:19.640
Or using health information to predict whether
someone has diabetes.

00:00:19.640 --> 00:00:22.090
But sometimes we don’t have labels.

00:00:22.090 --> 00:00:25.290
Sometimes we want to create labels that don’t
exist yet.

00:00:25.290 --> 00:00:29.220
Like if we wanted to use test and homework
grades to create 3 different groups of students

00:00:29.220 --> 00:00:30.380
in your Stats course.

00:00:30.380 --> 00:00:34.730
If you group similar students together, you
can target each group with a specific review

00:00:34.730 --> 00:00:37.620
session that addresses its unique needs.

00:00:37.620 --> 00:00:39.300
Hopefully leading to better grades!

00:00:39.300 --> 00:00:43.079
Because the groups don’t already exist,
we call this Unsupervised Machine Learning

00:00:43.079 --> 00:00:47.190
since we can’t give our models feedback
on whether they’re right or not.

00:00:47.190 --> 00:00:50.390
There are no “True” categories to compare
our groups with.

00:00:50.390 --> 00:00:54.970
Putting data into groups that don’t already
exist might seem kinda weird but today we’ll

00:00:54.970 --> 00:01:01.430
explore two types of Clustering--the main
type of Unsupervised Machine Learning: k-means

00:01:01.430 --> 00:01:03.399
and Hierarchical clustering.

00:01:03.400 --> 00:01:06.500
And we’ll see how creating new groups can
actually help us a lot.

00:01:06.500 --> 00:01:15.820
INTRO

00:01:15.820 --> 00:01:17.380
Let’s say you own a pizza restaurant.

00:01:17.380 --> 00:01:20.600
You’ve been collecting data on your customers’
pizza eating habits.

00:01:20.600 --> 00:01:22.930
Like how many pizzas a person orders a week.

00:01:22.930 --> 00:01:25.740
And the average number of toppings they get
on their pizzas.

00:01:25.740 --> 00:01:29.590
You’re rolling out a new coupon program
and you want to create 3 groups of customers

00:01:29.590 --> 00:01:33.080
and make custom coupons to target their needs.

00:01:33.080 --> 00:01:35.950
Maybe 2-for-1 five-topping medium pizzas.

00:01:35.950 --> 00:01:38.330
Or 20% off all plain cheese pizza.

00:01:38.330 --> 00:01:39.939
Or free pineapple topping!

00:01:39.939 --> 00:01:43.000
So let’s use k-means to create 3 customer
groups.

00:01:43.000 --> 00:01:44.450
First, we plot our data:

00:01:44.450 --> 00:01:47.720
All we know right now is that we want 3 separate
groups.

00:01:47.720 --> 00:01:53.570
So, what the k-means algorithm does is select
3 random points on your graph.

00:01:53.570 --> 00:01:56.820
Usually these are data points from your set,
but they don’t have to be.

00:01:56.820 --> 00:02:00.810
Then, we treat these random points as the
centers of our 3 groups.

00:02:00.810 --> 00:02:02.420
So we call them “centroids”.

00:02:02.420 --> 00:02:07.900
We assign each data point (the points in black) to the group of the centroid that it’s closest to.

00:02:07.900 --> 00:02:10.140
This point here is closest to the Green center.

00:02:10.140 --> 00:02:11.739
So we’ll assign it to the green group.

00:02:11.739 --> 00:02:16.530
Once we assign each point to the group it’s
closest to, we now have three groups, or clusters.

00:02:16.530 --> 00:02:22.040
Now that each group has some members, we calculate
the current centroid for each group.

00:02:22.040 --> 00:02:26.780
And now that we have the new centroids we’ll
repeat this process of assigning every point

00:02:26.780 --> 00:02:31.110
to the closest centroid and then recalculating
the new centroids.

00:02:31.110 --> 00:02:35.650
The computer will do this over and over again
until the centroids “converge”.

00:02:35.650 --> 00:02:41.050
And here, converge means that the centroids
and groups stop changing, even as you keep

00:02:41.050 --> 00:02:42.280
repeating these steps .

00:02:42.280 --> 00:02:46.300
Once it converges, you have your 3 groups,
or clusters.

00:02:46.300 --> 00:02:49.440
We can then look at the clusters and decide
which coupons to send.

00:02:49.440 --> 00:02:52.970
For example, this group doesn’t order many
pizzas each week but when they do, they order

00:02:52.970 --> 00:02:54.870
a LOT of toppings.

00:02:54.870 --> 00:02:57.990
So they might like the “Buy 3 toppings get
2 free” coupon.

00:02:57.990 --> 00:03:02.370
Whereas this group, who orders a lot of simple
pizzas, might like the “20% off Medium-2

00:03:02.370 --> 00:03:03.370
topping-Pizzas” coupon.

00:03:03.370 --> 00:03:08.440
(This is probably also the pineapple group
since really, there aren’t that many things

00:03:08.440 --> 00:03:11.200
that pair well with pineapple and cheese.)

00:03:11.200 --> 00:03:14.810
If you were a scientist, you might want to
look at the differences in health outcomes

00:03:14.810 --> 00:03:17.630
between the three pizza ordering groups.

00:03:17.630 --> 00:03:21.110
Like whether the group that orders a lot of
pizza has higher cholesterol.

00:03:21.110 --> 00:03:24.730
You may even want to look at the data in 5
clusters instead of 3.

00:03:24.730 --> 00:03:26.269
And k-means will help you do that.

00:03:26.269 --> 00:03:31.030
It will even allow you to create 5 clusters
of Crash Course Viewers based on how many

00:03:31.030 --> 00:03:35.220
Raccoons they think they can fight off, and
the number of Pieces of Pizza they claim to

00:03:35.220 --> 00:03:36.580
eat a week.

00:03:36.580 --> 00:03:38.569
This is actual survey data from you all.

00:03:38.569 --> 00:03:40.690
A K-means clustering created these 5 groups.

00:03:40.690 --> 00:03:44.051
We can see that this green group is PRETTY
confident that they could fight off a lot

00:03:44.051 --> 00:03:45.660
of raccoons.

00:03:45.660 --> 00:03:47.140
But 100 raccoons?

00:03:47.140 --> 00:03:47.900
No.

00:03:47.980 --> 00:03:50.320
On the other hand, we also see the light blue
group.

00:03:50.320 --> 00:03:54.530
They have perhaps more reasonable expectations
about their raccoon fighting abilities, they

00:03:54.530 --> 00:03:56.430
also eat a lot of pizza each week.

00:03:56.430 --> 00:04:01.230
Which makes me wonder…could they get the
pizza delivery folks to help out if we go

00:04:01.230 --> 00:04:02.650
to war with the raccoons?

00:04:02.650 --> 00:04:06.280
Unlike the Supervised Machine Learning we
looked at last time, you can’t calculate

00:04:06.280 --> 00:04:10.950
the “accuracy” of your results because
there’s no true groups or labels to compare.

00:04:10.950 --> 00:04:13.019
However, we’re not totally lost.

00:04:13.019 --> 00:04:17.840
There’s one method called the silhouette
score can help us determine how well fit our

00:04:17.840 --> 00:04:20.949
clusters are even without existing labels.

00:04:20.949 --> 00:04:25.140
Roughly speaking, the silhouette score measures
cluster “cohesion and separation” which

00:04:25.140 --> 00:04:29.800
is just a fancy way of saying that the data
points in that cluster are close to each other,

00:04:29.800 --> 00:04:31.800
but far away from points in other clusters.

00:04:31.800 --> 00:04:35.300
Here’s an example of clusters that have
HIGH silhoutte scores.

00:04:35.300 --> 00:04:38.600
And here’s an example of clusters that have
LOW silhouette scores.

00:04:38.600 --> 00:04:42.320
In an ideal world, we prefer HIGH silhouette
scores, because that means that there are

00:04:42.320 --> 00:04:44.599
clear differences between the groups.

00:04:44.599 --> 00:04:49.139
For example, if you clustered data from lollipops
and Filet Mignon based on sugar, fat, and

00:04:49.139 --> 00:04:53.419
protein content the two groups would be VERY
far apart from each other, with very little

00:04:53.419 --> 00:04:56.169
overlap--leading to high silhouette scores.

00:04:56.169 --> 00:04:59.939
But if you clustered data from Filet Mignon
and a New York Strip steak, the data would

00:04:59.939 --> 00:05:04.449
probably have lower silhouette scores, because
the two groups would be closer together - there’d

00:05:04.449 --> 00:05:05.819
probably be more overlap.

00:05:05.819 --> 00:05:10.509
Putting data into groups is useful, but sometimes,
we want to know more about the structure of

00:05:10.509 --> 00:05:11.509
our clusters.

00:05:11.509 --> 00:05:14.059
Like whether there are subgroups--or subclusters.

00:05:14.059 --> 00:05:17.840
Like in real life when we could look at two
groups: people who eat meat and those who

00:05:17.840 --> 00:05:18.840
don’t.

00:05:18.840 --> 00:05:22.050
The differences between the groups’ health
or beliefs might be interesting, but we also

00:05:22.050 --> 00:05:26.439
know that people who eat meat could be broken
up into even smaller groups like people who

00:05:26.439 --> 00:05:28.500
do and don’t eat red meat.

00:05:28.500 --> 00:05:30.259
These subgroups can be pretty interesting
too.

00:05:30.259 --> 00:05:35.520
A different type of clustering called Hierarchical
Clustering allows you to look at the hierarchical

00:05:35.520 --> 00:05:38.099
structure of these groups and subgroups.

00:05:38.099 --> 00:05:40.419
For example, look at these ADORABLE dogs.

00:05:40.419 --> 00:05:44.369
We could use hierarchical clustering to cluster
these dogs into groups.

00:05:44.369 --> 00:05:46.779
First, each dog starts off as its own group.

00:05:46.779 --> 00:05:50.379
Then, we start merging clusters together based
on how similar they are.

00:05:50.379 --> 00:05:55.360
For example, we’ll put these two dogs together
to form one cluster, and these two dogs together

00:05:55.360 --> 00:05:56.479
to form another.

00:05:56.479 --> 00:05:59.929
Each of these clusters--we could call this
one “Retrievers” and this one “Terriers”,

00:05:59.929 --> 00:06:01.800
is made up of smaller clusters.

00:06:01.800 --> 00:06:05.210
Now that we have 2 clusters, we can merge
them together, so that all the dogs are in

00:06:05.210 --> 00:06:06.210
one cluster.

00:06:06.210 --> 00:06:10.119
Again, this cluster is made up of a bunch
of sub clusters which are themselves made

00:06:10.119 --> 00:06:12.110
up of even smaller sub clusters.

00:06:12.110 --> 00:06:15.509
It’s turtles I mean clusters all the way
down.

00:06:15.509 --> 00:06:18.770
This graph of how the clusters are related
to each other is called a dendrogram.

00:06:18.770 --> 00:06:23.379
The further up the dendrogram that two clusters
join, the less similar they are.

00:06:23.379 --> 00:06:28.569
Golden and Curly Coated Retrievers connect
lower down than Golden Retrievers and Cairn

00:06:28.569 --> 00:06:29.140
Terriers.

00:06:29.140 --> 00:06:33.140
One compelling application of hierarchical
clustering is to look for subgroups of people

00:06:33.140 --> 00:06:35.920
with Autism Spectrum Disorder--or ASD.

00:06:35.920 --> 00:06:40.319
Previously, disorders like Autism, Aspergers
and Childhood Disintegrative Disorder (CDD)

00:06:40.319 --> 00:06:44.149
were considered separate diagnoses, even though
they share some common traits.

00:06:44.149 --> 00:06:49.189
But, in the latest version of the Diagnostic
and Statistical Manual of Mental Disorders--or

00:06:49.189 --> 00:06:54.059
DSM - these disorders are now classified as
a single disorder that has various levels

00:06:54.059 --> 00:06:59.119
of severity, hence the Spectrum part of Autism
Spectrum Disorder.

00:06:59.119 --> 00:07:02.419
ASD now applies to a large range of traits.

00:07:02.419 --> 00:07:05.789
Since ASD covers such a large range, it can
be useful to

00:07:05.789 --> 00:07:10.779
create clusters of similar people in order
to better understand Autism and provide more

00:07:10.779 --> 00:07:13.389
targeted and effective treatments.

00:07:13.389 --> 00:07:18.349
Not everyone with an ASD diagnosis is going
to benefit from the same kinds and intensities

00:07:18.349 --> 00:07:19.349
of therapy.

00:07:19.349 --> 00:07:24.530
A group at Chapman University set out to look
more closely at groups of people with ASD.

00:07:24.530 --> 00:07:29.819
They started with 16 profiles representing
different groups of people with an ASD diagnosis.

00:07:29.819 --> 00:07:34.979
Each profile has a score between 0 and 1 on
8 different developmental domain.

00:07:34.979 --> 00:07:38.310
Low scores in one of these domains means it
might need improvement.

00:07:38.310 --> 00:07:42.899
Unlike our pizza example which had only 2
measurements--# of pizza toppings and # of

00:07:42.899 --> 00:07:45.949
pizzas ordered per week--this time we have
8 measurements.

00:07:45.949 --> 00:07:49.819
This can make it tough to visually represent
the distance between clusters.

00:07:49.819 --> 00:07:51.189
But the ideas are the same.

00:07:51.189 --> 00:07:55.360
Just like two points can be close together
in 1 or 2 dimensions, they can be close together

00:07:55.360 --> 00:07:56.729
in 8 dimensions.

00:07:56.729 --> 00:08:00.610
When the researchers looked at the 16 profiles,
they grouped them together based on their

00:08:00.610 --> 00:08:03.110
8 developmental domain scores.

00:08:03.110 --> 00:08:07.540
In this case, we take all 16 profiles and
put each one in their own “cluster”, so

00:08:07.540 --> 00:08:10.949
we have 16 clusters, each with one profile
in them.

00:08:10.949 --> 00:08:13.199
Then, we start combining clusters that are
close together.

00:08:13.199 --> 00:08:18.180
And then we combine those , and we keep going
until every profile is in one big cluster.

00:08:18.180 --> 00:08:19.589
Here’s the dendrogram.

00:08:19.589 --> 00:08:24.419
We can see that there are 5 major clusters,
each made up of smaller clusters.

00:08:24.419 --> 00:08:29.499
The research team used radar graphs, which
look like this, to display each cluster’s

00:08:29.499 --> 00:08:31.709
8 domain scores on a circle.

00:08:31.709 --> 00:08:35.099
Low scores are near the center, high scores
near the edge of the circle.

00:08:35.099 --> 00:08:39.550
This main cluster, which they called Cluster
E, has scores consistent with someone who

00:08:39.550 --> 00:08:41.649
is considered high functioning.

00:08:41.649 --> 00:08:45.940
Before the change to the DSM, individuals
in the cluster might have been diagnosed with

00:08:45.940 --> 00:08:46.940
Asperger’s.

00:08:46.940 --> 00:08:51.750
The Radar graph here shows the scores for
the 6 original data points that were put in

00:08:51.750 --> 00:08:57.120
Cluster E. While there are some small differences,
we can see that overall the patterns look

00:08:57.120 --> 00:08:58.120
similar.

00:08:58.120 --> 00:09:02.620
So Cluster E might benefit from a less intense
therapy plan, while other Clusters with lower

00:09:02.620 --> 00:09:06.920
scores--like Cluster D--may benefit from more
intensive therapy.

00:09:06.920 --> 00:09:11.839
Creating profiles of similar cases might allow
care providers to create more effective, targeted

00:09:11.839 --> 00:09:16.920
therapies that can more efficiently help people
with an ASD diagnosis.

00:09:16.920 --> 00:09:21.300
If an individual’s insurance only covers
say 7 hours of therapy a week, we want to

00:09:21.300 --> 00:09:23.950
make sure it’s as effective as possible.

00:09:23.950 --> 00:09:28.149
It can also help researchers and therapists
determine why some people respond well to

00:09:28.149 --> 00:09:29.379
treatments, and others don’t.

00:09:29.379 --> 00:09:33.779
The type of hierarchical clustering that we’ve
been doing so far is called Agglomerative,

00:09:33.779 --> 00:09:35.410
or bottom-up clustering.

00:09:35.410 --> 00:09:39.720
That’s because all the data points start
off as their own cluster, and are merged together

00:09:39.720 --> 00:09:41.040
until there’s only one.

00:09:41.040 --> 00:09:44.839
Often, we don’t have structured groups as
a part of our data, but still want to create

00:09:44.839 --> 00:09:47.980
profiles of people or data points that are
similar.

00:09:47.980 --> 00:09:50.069
Unsupervised Machine Learning can do that.

00:09:50.069 --> 00:09:55.940
It allows us to use things that we’ve observed--like
the tiny stature of Terriers, or raccoon-fighting

00:09:55.940 --> 00:09:59.940
confidence --and create groups of dogs, or
people that are similar to each other.

00:09:59.940 --> 00:10:03.240
While we don’t always want categorize people,
putting them into groups can help give them

00:10:03.240 --> 00:10:08.459
better deals on pizza, or better suggestions
for books or even better medical interventions.

00:10:08.459 --> 00:10:13.260
And for the record, I am always happy to help moderately confident raccoon fighting pizza

00:10:13.260 --> 00:10:14.880
eaters fight raccoons.

00:10:14.920 --> 00:10:17.840
Just call me. Thanks for watching. I'll see you next time.

