WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:04.600
[MUSIC PLAYING]

00:00:04.600 --> 00:00:09.050
- So I want to thank
the Radcliffe staff

00:00:09.050 --> 00:00:15.920
for making this sabbatical
such a wonderful time.

00:00:15.920 --> 00:00:20.870
Also, our dean, Lizabeth
Cohen, and then staff.

00:00:20.870 --> 00:00:26.180
And also the fellow fellows for
providing a really interacting,

00:00:26.180 --> 00:00:29.240
stimulating environment.

00:00:29.240 --> 00:00:33.820
So I just realized that suddenly
more than half my sabbatical

00:00:33.820 --> 00:00:34.910
is over.

00:00:34.910 --> 00:00:39.810
Almost I guess in two weeks, 2/3
of my sabbatical will be over.

00:00:39.810 --> 00:00:44.290
I should start contemplating
how to get back.

00:00:44.290 --> 00:00:46.640
We have been so
much spoiled here,

00:00:46.640 --> 00:00:51.520
so it's going to be a challenge
to adapt to the life in London.

00:00:52.797 --> 00:00:58.610
So today my plan is to
talk about two things, most

00:00:58.610 --> 00:01:01.260
of the time in this talk.

00:01:01.260 --> 00:01:04.230
The first part will be
an introduction, really,

00:01:04.230 --> 00:01:08.390
to Bayesian statistical
inference, what Markov chain,

00:01:08.390 --> 00:01:10.730
Monte Carlo means.

00:01:10.730 --> 00:01:16.190
And then the second part will be
a introduction into coalescent.

00:01:16.190 --> 00:01:19.610
So when we got those
two things together,

00:01:19.610 --> 00:01:20.920
exciting things will happen.

00:01:20.920 --> 00:01:23.700
But then you can see what
we are struggling with.

00:01:26.920 --> 00:01:30.620
So I'm going to give you
a very quick introduction

00:01:30.620 --> 00:01:35.760
into Bayesian statistics and
Markov chain Monte Carlo.

00:01:35.760 --> 00:01:40.850
So first, the concept
of probability--

00:01:40.850 --> 00:01:45.520
to mention what probability
means in this context.

00:01:45.520 --> 00:01:48.890
It's a very basic
terminology in statistics,

00:01:48.890 --> 00:01:53.870
but even today, after
more than 100 years,

00:01:53.870 --> 00:01:56.690
it's very difficult to define.

00:01:56.690 --> 00:01:59.340
So here are two concepts.

00:01:59.340 --> 00:02:03.860
The first was one the
frequentist definition.

00:02:03.860 --> 00:02:08.990
For example, I can say that
if I have a fair coin--

00:02:08.990 --> 00:02:13.315
by that I mean the
probability of heads is 1/2.

00:02:13.315 --> 00:02:14.690
Essentially, I'm
saying that if I

00:02:14.690 --> 00:02:20.930
flip the coin many, many times,
the proportion of the times

00:02:20.930 --> 00:02:26.390
that I see heads becomes
closer, closer to 1/2.

00:02:26.390 --> 00:02:29.180
So that's a
frequentist definition,

00:02:29.180 --> 00:02:32.650
with probability defined as
a proportion or frequency.

00:02:35.510 --> 00:02:39.850
A second definition is
the Bayesian definition,

00:02:39.850 --> 00:02:44.390
and that's basically saying
that a probability measures

00:02:44.390 --> 00:02:47.210
my degree of belief.

00:02:47.210 --> 00:02:51.680
Maybe when I say there's a
chance or probability of 80%

00:02:51.680 --> 00:02:56.820
that it will snow tomorrow,
I mean how much confidence

00:02:56.820 --> 00:02:59.540
I have in that statement.

00:02:59.540 --> 00:03:05.100
In particular, we don't have
many repetitions of tomorrow.

00:03:05.100 --> 00:03:06.540
So that doesn't sound right.

00:03:06.540 --> 00:03:08.690
I think we do have many
repetitions of tomorrow.

00:03:08.690 --> 00:03:11.340
But you know what I mean, yeah.

00:03:11.340 --> 00:03:12.490
Tomorrow is after today.

00:03:15.990 --> 00:03:19.580
So this pretty much
divides statisticians

00:03:19.580 --> 00:03:21.980
into two big camps--

00:03:21.980 --> 00:03:24.620
the frequentist,
or sometimes called

00:03:24.620 --> 00:03:30.720
the classical statistics,
and the Bayesian.

00:03:30.720 --> 00:03:33.880
Oftentimes they don't
talk to each other.

00:03:33.880 --> 00:03:38.060
So with this slide, you
choose one definition,

00:03:38.060 --> 00:03:40.470
and you stick to it.

00:03:40.470 --> 00:03:44.840
So in my talk today, I'm going
to use the Bayesian definition,

00:03:44.840 --> 00:03:50.840
where the probability is
a degree of our belief.

00:03:50.840 --> 00:03:53.840
You can be a bit more
specific about this.

00:03:56.800 --> 00:04:01.770
And [INAUDIBLE] divides the
Bayesians into two camps--

00:04:01.770 --> 00:04:04.970
the subjective
Bayesians-- they believe

00:04:04.970 --> 00:04:11.220
we can really talk about our
personal, subjective belief

00:04:11.220 --> 00:04:16.720
on ant statement or
anything that is uncertain.

00:04:16.720 --> 00:04:20.420
Or the objective
Bayesians, who try

00:04:20.420 --> 00:04:28.340
to say that we can rationally
measure our degree of belief.

00:04:28.340 --> 00:04:31.310
Unfortunately, I think
the rational Bayesians,

00:04:31.310 --> 00:04:35.110
or objective Bayesians,
have lost out.

00:04:35.110 --> 00:04:37.400
So now the Bayesian
statisticians,

00:04:37.400 --> 00:04:41.750
they tend to be the
subjective Bayesians.

00:04:41.750 --> 00:04:47.270
So I'll come to this slide later
on, but as the term indicates,

00:04:47.270 --> 00:04:52.160
subjective Bayesians use
subjective probabilities.

00:04:52.160 --> 00:04:56.420
My biologist colleagues
all dislike this very much.

00:04:56.420 --> 00:05:01.140
They don't like our present
option is to influence

00:05:01.140 --> 00:05:02.750
the statistical inference.

00:05:02.750 --> 00:05:06.110
But this is very much
in the foundation

00:05:06.110 --> 00:05:07.315
of statistical inference.

00:05:07.315 --> 00:05:10.270
It's not very stable.

00:05:10.270 --> 00:05:13.530
So a lot of unstable ground.

00:05:13.530 --> 00:05:19.560
So similarly, we use
statistical distributions

00:05:19.560 --> 00:05:26.160
to describe our uncertainties
for any parameter or any number

00:05:26.160 --> 00:05:27.816
that we are interested in.

00:05:27.816 --> 00:05:32.070
So I want to explain--

00:05:32.070 --> 00:05:35.050
I'm starting from [INAUDIBLE]
and if you are a mathematician,

00:05:35.050 --> 00:05:36.180
this might be simple.

00:05:36.180 --> 00:05:39.350
But I'm not
trivialising too much.

00:05:39.350 --> 00:05:42.550
So I will just
tell you that have

00:05:42.550 --> 00:05:47.250
one slide with some
equations, but not really many

00:05:47.250 --> 00:05:48.360
slides with equations.

00:05:48.360 --> 00:05:52.730
I just copy the
equations back and forth.

00:05:52.730 --> 00:05:56.580
OK, so we use a
statistical distribution

00:05:56.580 --> 00:06:03.730
to describe our uncertain
knowledge about any parameter.

00:06:03.730 --> 00:06:07.740
Let's say that in
this example, we

00:06:07.740 --> 00:06:11.810
have a parameter theta that's
the probability of heads

00:06:11.810 --> 00:06:13.780
for a coin.

00:06:13.780 --> 00:06:17.340
So obviously, this
goes from 0 to 1.

00:06:17.340 --> 00:06:19.350
So we will describe
the uncertainty

00:06:19.350 --> 00:06:22.380
or uncertain knowledge
about this parameter-- we

00:06:22.380 --> 00:06:24.480
use statistical distributions.

00:06:24.480 --> 00:06:26.820
So I'm just be using
this simple example

00:06:26.820 --> 00:06:32.090
to explain how you would read
a statistical distribution.

00:06:32.090 --> 00:06:38.190
So basically, you have this
curve, like this where.

00:06:38.190 --> 00:06:42.180
The area under the curve over
an interval-- for example,

00:06:42.180 --> 00:06:45.330
here it's from 0.1 to 0.2.

00:06:45.330 --> 00:06:48.390
The area here gives
us the probability

00:06:48.390 --> 00:06:53.160
that this parameter theta
is between 0.1 and 0.2.

00:06:53.160 --> 00:06:58.830
Obviously, the total area
under the curve has to be one.

00:06:58.830 --> 00:07:03.540
So that's all I did
for my talk here.

00:07:03.540 --> 00:07:08.070
And now you can say that
with this distribution,

00:07:08.070 --> 00:07:11.790
we think that theta is
most likely around 0.5,

00:07:11.790 --> 00:07:15.100
because [INAUDIBLE].

00:07:15.100 --> 00:07:21.342
So if we talk about
intervals of 0.1--

00:07:21.342 --> 00:07:24.060
so around 0.5,
that interval will

00:07:24.060 --> 00:07:28.770
have a lot of area,
or probability mass,

00:07:28.770 --> 00:07:30.480
so it's more likely.

00:07:30.480 --> 00:07:36.150
Whereas extreme values close
to 0 or 1 are less likely.

00:07:36.150 --> 00:07:42.810
With this distribution, we feel
that theta is either close to 0

00:07:42.810 --> 00:07:45.200
or close to 1.

00:07:45.200 --> 00:07:51.910
OK so, that's how the
distribution is interpreted.

00:07:51.910 --> 00:07:54.776
My question-- I'm
not going into this,

00:07:54.776 --> 00:07:56.150
but I'll just ask
you a question.

00:07:56.150 --> 00:07:57.840
You can think about it.

00:07:57.840 --> 00:08:00.920
Suppose I know nothing
about this theta,

00:08:00.920 --> 00:08:04.410
except that it's
between 0 and 1.

00:08:04.410 --> 00:08:08.240
What kind of distribution
describes my knowledge--

00:08:08.240 --> 00:08:12.150
or my lack of knowledge?

00:08:12.150 --> 00:08:14.425
Huh?

00:08:14.425 --> 00:08:16.340
That's a good answer, yes.

00:08:16.340 --> 00:08:20.990
Some people think that this
flat distribution might

00:08:20.990 --> 00:08:25.610
mean that you have no
knowledge, but it's not yet

00:08:25.610 --> 00:08:29.050
accepted-- that's not true.

00:08:29.050 --> 00:08:30.650
So the short answer
is that there

00:08:30.650 --> 00:08:35.789
is no such distribution that
can represent total ignorance.

00:08:35.789 --> 00:08:39.870
That's also the reason why the
objective Bayesians' a lost

00:08:39.870 --> 00:08:40.419
art.

00:08:40.419 --> 00:08:44.860
So there are subjective
Bayesians are having the day.

00:08:44.860 --> 00:08:53.040
OK, now this is my slide
with a few simple equations.

00:08:53.040 --> 00:08:57.780
I'm going through
these, and then the rest

00:08:57.780 --> 00:09:00.000
will follow rather easily.

00:09:00.000 --> 00:09:03.800
It's a simple case
of estimating--

00:09:03.800 --> 00:09:07.390
this probability is called
a binomial probability,

00:09:07.390 --> 00:09:09.740
like probability of heads.

00:09:09.740 --> 00:09:15.300
However, this was
the problem that's

00:09:15.300 --> 00:09:22.200
dealt with in Thomas Bayes's
famous paper, which obviously,

00:09:22.200 --> 00:09:26.280
Bayesian statistics
is named after him.

00:09:26.280 --> 00:09:32.790
This is long before statistics
as a subject area existed.

00:09:32.790 --> 00:09:36.300
So also, this has all the
conceptual difficulties

00:09:36.300 --> 00:09:39.040
we face today.

00:09:39.040 --> 00:09:43.460
So we are interested
in the parameter theta

00:09:43.460 --> 00:09:45.470
as probability of heads.

00:09:45.470 --> 00:09:52.470
The theta we have 4 heads
out of 10 coin tosses.

00:09:52.470 --> 00:09:57.780
You can say admittedly that
40% is a good estimate, right?

00:09:57.780 --> 00:10:00.060
Yeah, everybody can say that.

00:10:00.060 --> 00:10:02.640
So why are we spending
so much energy on this?

00:10:06.620 --> 00:10:08.340
So the simple answer--

00:10:08.340 --> 00:10:14.310
everybody can say we are allowed
to judge some principles.

00:10:14.310 --> 00:10:17.130
You can't formulate a principle.

00:10:17.130 --> 00:10:20.730
You can say how it works
with a simple example.

00:10:20.730 --> 00:10:23.550
If you are happy, you
can apply the principle

00:10:23.550 --> 00:10:26.940
to very big, complicated
problems for which you

00:10:26.940 --> 00:10:29.770
don't have the intuition.

00:10:29.770 --> 00:10:34.770
OK, so if we do
Bayesian, we have

00:10:34.770 --> 00:10:39.450
to have a prior distribution
for the parameter

00:10:39.450 --> 00:10:42.120
before we can look
at the data, really--

00:10:42.120 --> 00:10:44.130
before we analyze the data.

00:10:44.130 --> 00:10:46.560
And this is very much
disturbing for some

00:10:46.560 --> 00:10:49.380
of my biologist colleagues.

00:10:49.380 --> 00:10:53.460
They say that I want
to know this parameter.

00:10:53.460 --> 00:10:55.310
That's why I did
the experiment--

00:10:55.310 --> 00:10:57.150
I collect the data.

00:10:57.150 --> 00:11:01.410
Instead of you telling
me what the parameter is,

00:11:01.410 --> 00:11:05.310
you are asking me to
formulate a prior distribution

00:11:05.310 --> 00:11:08.520
to start with.

00:11:08.520 --> 00:11:13.740
So yeah-- what R.A. Fisher,
the famous statistician,

00:11:13.740 --> 00:11:16.420
was trying to do that
without this prior.

00:11:16.420 --> 00:11:19.550
But I think the effort is
considered to be, in general,

00:11:19.550 --> 00:11:21.110
unsuccessful.

00:11:21.110 --> 00:11:24.500
One description is that
he's trying to make omelette

00:11:24.500 --> 00:11:26.630
we've breaking the egg.

00:11:26.630 --> 00:11:28.560
Now, today we are with Bayesian.

00:11:28.560 --> 00:11:30.870
So whether you
like it or not, we

00:11:30.870 --> 00:11:35.100
have to use a prior to start.

00:11:35.100 --> 00:11:39.120
Let's say that we use
this uniform distribution

00:11:39.120 --> 00:11:40.040
as the prior.

00:11:40.040 --> 00:11:42.420
This is very commonly used.

00:11:42.420 --> 00:11:45.660
As Markov explained, some
people have the impression

00:11:45.660 --> 00:11:50.174
that this is probably not
adding any information

00:11:50.174 --> 00:11:52.670
to the other parameter already.

00:11:55.210 --> 00:12:02.710
Thomas Bayes had a
very long section

00:12:02.710 --> 00:12:06.670
trying to argue
that this doesn't

00:12:06.670 --> 00:12:11.800
seem to be adding information,
or assuming added information.

00:12:11.800 --> 00:12:19.390
But in general, that is
believed to be a failed attempt.

00:12:19.390 --> 00:12:24.740
OK, so have to have a
prior for the parameter.

00:12:24.740 --> 00:12:26.585
Let's say we have the uniform.

00:12:26.585 --> 00:12:29.620
And the component of
the Bayesian inference

00:12:29.620 --> 00:12:31.690
is this likelihood--

00:12:31.690 --> 00:12:33.610
the concept likelihood.

00:12:33.610 --> 00:12:37.180
Likelihood means
probably as well.

00:12:37.180 --> 00:12:40.120
They are just English words
that mean the same thing.

00:12:40.120 --> 00:12:46.530
But this term has been recruited
in statistics to mean something

00:12:46.530 --> 00:12:48.980
in particular.

00:12:48.980 --> 00:12:53.910
It's the probability that theta,
given the parameter theta.

00:12:53.910 --> 00:12:59.220
So our case, we have
4 heads, 6 tails.

00:12:59.220 --> 00:13:03.050
So essentially, you multiply
that probability of heads

00:13:03.050 --> 00:13:05.230
and the probabilities of tails.

00:13:05.230 --> 00:13:10.370
So you write down theta 4 times.

00:13:10.370 --> 00:13:13.390
One minus theta-- that's
the probabilities of tails.

00:13:13.390 --> 00:13:16.330
6 times you multiply
them together.

00:13:16.330 --> 00:13:18.180
Some of you may notice this--

00:13:18.180 --> 00:13:21.790
often sometimes there's a
binomial coefficient there.

00:13:21.790 --> 00:13:25.520
That's not important,
so I don't have it here.

00:13:25.520 --> 00:13:29.230
So the idea of using
the probability

00:13:29.230 --> 00:13:35.840
that theta to help us decide
on the parameter values--

00:13:35.840 --> 00:13:40.950
Last year to R.A. Fisher,
this is his paper of 1912,

00:13:40.950 --> 00:13:44.540
when he was a third-year
undergraduate.

00:13:44.540 --> 00:13:46.410
So when he was 22.

00:13:46.410 --> 00:13:49.990
This is still a
fairly widely read.

00:13:49.990 --> 00:13:52.810
I wonder how many of
us write papers that

00:13:52.810 --> 00:13:54.610
are still read 100 years later.

00:13:57.170 --> 00:14:03.040
So the idea here is that
we already have the data--

00:14:03.040 --> 00:14:05.140
4 heads, 6 tails.

00:14:05.140 --> 00:14:10.680
We are interested in the
probability of heads.

00:14:10.680 --> 00:14:15.270
Usually, you use this formula
to calculate the probability

00:14:15.270 --> 00:14:18.140
of the number of heads.

00:14:18.140 --> 00:14:20.740
Well, I'll tell you the
probability of heads.

00:14:20.740 --> 00:14:22.680
You use the binomial formula.

00:14:22.680 --> 00:14:27.570
But this is a inverse
problem of going

00:14:27.570 --> 00:14:30.910
from a theta, to the
population, to the parameter.

00:14:30.910 --> 00:14:35.390
Or going from
effect to the cause.

00:14:35.390 --> 00:14:37.270
So it's much more difficult.

00:14:37.270 --> 00:14:40.110
Anyway, Fisher's
idea was that we

00:14:40.110 --> 00:14:43.830
could look at this
probability, theta,

00:14:43.830 --> 00:14:46.880
for different values of theta.

00:14:46.880 --> 00:14:50.780
And then the particular
value of theta

00:14:50.780 --> 00:14:56.480
that makes the theta we all
observe look very likely

00:14:56.480 --> 00:14:58.246
will be favored.

00:14:58.246 --> 00:14:59.120
Does that make sense?

00:15:02.580 --> 00:15:06.490
Yeah, so we already
have that theta.

00:15:06.490 --> 00:15:12.160
And then given the probability
of heads-- like 0.4 of 0.5--

00:15:12.160 --> 00:15:16.350
we can calculate the probability
of the theta we already

00:15:16.350 --> 00:15:18.910
observed.

00:15:18.910 --> 00:15:24.090
I will choose the
parameter value

00:15:24.090 --> 00:15:28.140
so that the data we already
have have a high probability.

00:15:28.140 --> 00:15:31.880
I can plot this as
a function of theta.

00:15:31.880 --> 00:15:37.650
You'll find that the value of
this probability for the theta

00:15:37.650 --> 00:15:43.710
is larger when theta is
0.4 than when theta is 0.5.

00:15:43.710 --> 00:15:45.840
And then we think
that the true theta

00:15:45.840 --> 00:15:52.350
is more likely to be
0.4 rather than 0.5.

00:15:52.350 --> 00:15:58.050
It's not automatic it's like
a quantum leap in your mind.

00:16:01.540 --> 00:16:03.270
There are also
controversies, yeah.

00:16:03.270 --> 00:16:09.200
So not really-- not all fixed.

00:16:09.200 --> 00:16:13.760
OK, so we go through
that Bayesian procedure.

00:16:13.760 --> 00:16:16.570
We have a prior
at the likelihood.

00:16:16.570 --> 00:16:19.060
So the likelihood really--

00:16:19.060 --> 00:16:24.780
commonly accepted that this
summarized the information

00:16:24.780 --> 00:16:26.870
about the parameter theta.

00:16:26.870 --> 00:16:33.230
And then we have the
posterior distribution,

00:16:33.230 --> 00:16:35.590
which gives us the
distribution of theta

00:16:35.590 --> 00:16:40.240
when we have both the
prior and the theta.

00:16:40.240 --> 00:16:43.850
So in a way, the posterior
combines the information

00:16:43.850 --> 00:16:46.220
in that prior and the theta.

00:16:46.220 --> 00:16:48.500
And the way that we
combine the information

00:16:48.500 --> 00:16:52.190
is just to multiply these
two pieces together.

00:16:52.190 --> 00:16:53.630
So that's this part here.

00:16:53.630 --> 00:16:56.750
So you multiply this too.

00:16:56.750 --> 00:17:05.329
Essentially, this gives us
the relative preferences

00:17:05.329 --> 00:17:08.380
for different values of theta.

00:17:08.380 --> 00:17:11.980
However, one trouble comes in.

00:17:11.980 --> 00:17:14.650
When you multiple
these two terms,

00:17:14.650 --> 00:17:20.750
this does not satisfy
our requirements

00:17:20.750 --> 00:17:23.725
that the area under
the curve is 1.

00:17:26.390 --> 00:17:28.980
So in a previous
slide, I explained that

00:17:28.980 --> 00:17:31.210
with a statistical distribution.

00:17:31.210 --> 00:17:34.800
you want the area of
the curve to be one.

00:17:34.800 --> 00:17:38.130
But that's not
satisfied when you just

00:17:38.130 --> 00:17:41.230
modify those two together.

00:17:41.230 --> 00:17:44.580
In fact, you need to
somehow rescale this

00:17:44.580 --> 00:17:49.210
so that this becomes a proper
distribution with the area

00:17:49.210 --> 00:17:50.410
to be one.

00:17:50.410 --> 00:17:52.920
So the numerator here--

00:17:52.920 --> 00:17:55.140
that's called the
normalizing constant.

00:17:55.140 --> 00:17:58.380
It's normalizing
the distribution

00:17:58.380 --> 00:18:02.470
so that the sum is 1.

00:18:02.470 --> 00:18:06.570
So in a way, if you modify
those two and plot here,

00:18:06.570 --> 00:18:09.060
you'll get a curve like this.

00:18:09.060 --> 00:18:14.580
But you don't really know
the labels on the y-axis.

00:18:14.580 --> 00:18:20.190
So what this does is to allow
us to put the numbers here

00:18:20.190 --> 00:18:26.230
on the y-axis, so that the
area under this curve is 1.

00:18:26.230 --> 00:18:32.150
Yeah, that's my introduction to
Bayesian statistical inference.

00:18:32.150 --> 00:18:36.460
I also explained that,
computationally, it's

00:18:36.460 --> 00:18:39.805
difficult to calculate
normalizing constant.

00:18:39.805 --> 00:18:42.630
So basically, mathematically,
it's the integral.

00:18:42.630 --> 00:18:44.980
When you have three
parameters, you

00:18:44.980 --> 00:18:46.750
have a three dimensional
integral that's

00:18:46.750 --> 00:18:48.730
very difficult to calculate.

00:18:48.730 --> 00:18:51.190
So now we're going
to move on to explain

00:18:51.190 --> 00:18:57.820
how you use Markov chain Monte
Carlo to avoid this calculation

00:18:57.820 --> 00:18:59.980
and still manage with inference.

00:19:02.560 --> 00:19:06.360
OK, so Markov
chain Monte Carlo--

00:19:06.360 --> 00:19:08.395
Monte Carlo-- the
first Monte Carlo

00:19:08.395 --> 00:19:11.580
is the world famous
gambling [INAUDIBLE]..

00:19:11.580 --> 00:19:15.600
And those things
developed before Las Vegas

00:19:15.600 --> 00:19:19.650
become the top gambling city.

00:19:19.650 --> 00:19:24.750
So Monte Carlo, named
after the gambling city,

00:19:24.750 --> 00:19:30.310
really means computer
simulation using random numbers.

00:19:30.310 --> 00:19:33.300
So very briefly, I just
have an example here

00:19:33.300 --> 00:19:37.480
to explain the basic idea.

00:19:37.480 --> 00:19:42.770
Suppose we want to estimate
pi, which is 3.14 something.

00:19:42.770 --> 00:19:45.450
[INAUDIBLE] we know.

00:19:45.450 --> 00:19:51.290
But suppose we want to use
this kind of simulation,

00:19:51.290 --> 00:19:55.650
or like gambling,
to estimate it.

00:19:55.650 --> 00:20:00.070
We can draw a
square with side 2,

00:20:00.070 --> 00:20:04.800
and inside, we draw a
circle with radius 1.

00:20:04.800 --> 00:20:11.850
And then we kind of flip for
a little ball onto the square.

00:20:11.850 --> 00:20:16.980
And then count the
proportion of times

00:20:16.980 --> 00:20:22.560
that the ball hits the circle.

00:20:22.560 --> 00:20:28.740
So the square has area of 4,
and the circle has a area of pi.

00:20:28.740 --> 00:20:32.600
So the probability
that we hit the circle

00:20:32.600 --> 00:20:34.830
is going to be pi over 4.

00:20:34.830 --> 00:20:35.340
OK?

00:20:35.340 --> 00:20:40.680
So this way, just imagine that
you threw a ball 1,000 times.

00:20:40.680 --> 00:20:43.770
You got 780 successes.

00:20:43.770 --> 00:20:45.280
I just made this up.

00:20:45.280 --> 00:20:47.210
I didn't do the experiment.

00:20:47.210 --> 00:20:51.900
You can do it yourself
at home, if you like.

00:20:51.900 --> 00:20:54.540
So then the estimate
of pi is going

00:20:54.540 --> 00:20:56.500
to be something like this.

00:20:56.500 --> 00:20:59.350
So this idea is quite simple.

00:20:59.350 --> 00:21:04.370
So there was actually a famous
experiment done by [INAUDIBLE]..

00:21:04.370 --> 00:21:07.870
Again, this is long
before statistics existed.

00:21:07.870 --> 00:21:12.560
to do exactly this.

00:21:12.560 --> 00:21:17.990
So we are, basically,
using random numbers,

00:21:17.990 --> 00:21:25.410
as you do in a Casino,
to do simulation.

00:21:25.410 --> 00:21:30.190
And then Markov chain,
really briefly--

00:21:30.190 --> 00:21:33.390
a chain means a sequence.

00:21:33.390 --> 00:21:37.910
So essentially, we get a
sequence of values or states

00:21:37.910 --> 00:21:42.710
that have the so-called
Markovian property.

00:21:42.710 --> 00:21:46.720
This is also called the
memoryless property.

00:21:46.720 --> 00:21:49.770
Written in words, you say
that given the present,

00:21:49.770 --> 00:21:53.200
the future does not
depend on the past.

00:21:53.200 --> 00:21:56.900
I like this one very much,
because it describes my state

00:21:56.900 --> 00:22:00.260
as very, very accurately--

00:22:00.260 --> 00:22:02.200
memoryless property.

00:22:02.200 --> 00:22:07.670
So let's say that, if we think
there are two kinds of weathers

00:22:07.670 --> 00:22:11.670
here, sunny and snow.

00:22:11.670 --> 00:22:16.820
If we think that the chances
for sunny or snow tomorrow--

00:22:16.820 --> 00:22:19.160
would it depend on
today's weather,

00:22:19.160 --> 00:22:22.960
but not yesterday's or earlier.

00:22:22.960 --> 00:22:26.730
Then you can just work
out some probabilities

00:22:26.730 --> 00:22:32.375
for such transition from today's
weather to tomorrow's weather.

00:22:32.375 --> 00:22:37.880
And that means the sequence
of the weather over the days

00:22:37.880 --> 00:22:39.230
will be a Markov chain.

00:22:39.230 --> 00:22:41.240
So it has no memory.

00:22:41.240 --> 00:22:45.620
Given today's status,
tomorrow's status

00:22:45.620 --> 00:22:49.015
will not depend on
the past status.

00:22:52.745 --> 00:22:56.000
So I have to talk about the
two pieces in this name.

00:22:56.000 --> 00:22:58.760
Basically, Markov
chain Monte Carlo

00:22:58.760 --> 00:23:01.600
is a computer
simulation [? avenue ?]

00:23:01.600 --> 00:23:05.540
that generates a sequence
of values of states

00:23:05.540 --> 00:23:07.640
which form a Markov chain.

00:23:07.640 --> 00:23:10.850
So I'm now going to use
the coin toss example

00:23:10.850 --> 00:23:14.210
to explain how this works.

00:23:14.210 --> 00:23:18.270
So this is Markov
chain Monte Carlo.

00:23:18.270 --> 00:23:23.380
Sorry my curve does not match
the previous one I showed you,

00:23:23.380 --> 00:23:28.950
but this is supposed to show
the [INAUDIBLE] of the prior--

00:23:28.950 --> 00:23:35.130
the likelihood, which
is not normalized.

00:23:35.130 --> 00:23:43.320
So suppose we are somewhere
with a theta value,

00:23:43.320 --> 00:23:52.140
and then we use a sliding
window around the current value

00:23:52.140 --> 00:23:54.990
to propose a new value.

00:23:54.990 --> 00:24:00.510
Let's say that we
propose this theta star.

00:24:00.510 --> 00:24:05.850
So then we decide whether we
will accept this new value.

00:24:05.850 --> 00:24:11.700
We do that by looking at
whether we are going up or down.

00:24:11.700 --> 00:24:16.470
If we are going up, so this
is our normalized posterior.

00:24:16.470 --> 00:24:22.110
If we are going up, we accept
without any hesitation.

00:24:22.110 --> 00:24:26.850
But if the proposed
value is to go downhill,

00:24:26.850 --> 00:24:29.480
we don't reject
it straight away.

00:24:29.480 --> 00:24:32.370
We accept with a
probability that's

00:24:32.370 --> 00:24:35.190
the ratio of the two heights.

00:24:35.190 --> 00:24:37.830
Let's say this is a 5.

00:24:37.830 --> 00:24:39.320
We are currently at 20.

00:24:39.320 --> 00:24:42.200
That's the high at 20.

00:24:42.200 --> 00:24:45.450
So we calculate the
ratio, which gives us 1/4.

00:24:45.450 --> 00:24:47.790
So we can flip a coin twice.

00:24:47.790 --> 00:24:49.960
If we get two heads, we accept.

00:24:49.960 --> 00:24:54.150
So we accept with
probability 1/4.

00:24:54.150 --> 00:24:56.135
So that's the full algorithm.

00:24:56.135 --> 00:24:59.490
So every time, you
propose a new value

00:24:59.490 --> 00:25:02.490
around your current
value, and you decide

00:25:02.490 --> 00:25:05.160
whether you reject or accept.

00:25:05.160 --> 00:25:07.850
If you accept, you
move to the new place,

00:25:07.850 --> 00:25:10.050
and you repeat the algorithm.

00:25:10.050 --> 00:25:16.550
If you reject, you stay where
you are, and you propose again.

00:25:16.550 --> 00:25:19.325
OK so then suppose--

00:25:24.175 --> 00:25:28.850
this shows running
that algorithm twice--

00:25:28.850 --> 00:25:33.360
once with the blue dots,
another with the red circles.

00:25:33.360 --> 00:25:37.040
So let's say that with a
red one, I start from 0.1

00:25:37.040 --> 00:25:42.460
and propose a value
that's like 0.155.

00:25:42.460 --> 00:25:45.270
That's accepted, so I
move to the new value.

00:25:45.270 --> 00:25:50.140
And then here, I'm
at 0.322 I guess.

00:25:50.140 --> 00:25:52.400
I propose some values,
but they are rejected.

00:25:52.400 --> 00:25:57.280
I stay here for quite some
time before I move again.

00:25:57.280 --> 00:26:02.920
So those values that are
visited form a Markov chain.

00:26:02.920 --> 00:26:06.760
You can say from
that algorithm, where

00:26:06.760 --> 00:26:10.240
we are moving into the
next state only depends

00:26:10.240 --> 00:26:12.070
on where we are right now--

00:26:12.070 --> 00:26:17.590
but not the states we
visited in the past.

00:26:17.590 --> 00:26:22.830
So the magic of the algorithm
is that those values

00:26:22.830 --> 00:26:29.730
that are visited are a
sample from the posterior.

00:26:29.730 --> 00:26:35.040
So if you're [INAUDIBLE] those
values and draw a histogram,

00:26:35.040 --> 00:26:38.370
you find that you
have this curve that

00:26:38.370 --> 00:26:44.900
matches the curve I showed
you earlier, very precisely.

00:26:47.650 --> 00:26:49.690
So I'm going to mention
a few other features

00:26:49.690 --> 00:26:54.360
of that algorithm, which I more
or less already touched on.

00:26:57.760 --> 00:27:00.000
The first is that
sequence of the states

00:27:00.000 --> 00:27:02.340
visited form a Markov chain.

00:27:02.340 --> 00:27:04.110
I explained this already.

00:27:04.110 --> 00:27:05.910
Given the current
value, you don't

00:27:05.910 --> 00:27:11.370
get to know the past values
to decide where you go.

00:27:11.370 --> 00:27:16.910
The second feature is that
the [? status ?] distribution

00:27:16.910 --> 00:27:19.030
with the chain is the posterior.

00:27:19.030 --> 00:27:22.860
What this means is
that, in the long run,

00:27:22.860 --> 00:27:29.310
we will be visiting the
peak of the mountain

00:27:29.310 --> 00:27:31.505
more than the deep valleys.

00:27:34.380 --> 00:27:37.710
Remember, it's more or less
a hill climbing algorithm.

00:27:37.710 --> 00:27:39.870
It tends to go uphill.

00:27:39.870 --> 00:27:42.670
But if it's going downhill,
it tends to hesitate.

00:27:42.670 --> 00:27:46.730
So it will spend more
time around the peak.

00:27:46.730 --> 00:27:50.470
So mathematically, you can
prove that, actually, you're

00:27:50.470 --> 00:27:56.120
just visiting the
distribution correctly.

00:27:59.020 --> 00:28:01.025
A third feature
of this algorithm

00:28:01.025 --> 00:28:05.540
is that you don't need
to know the posterior--

00:28:05.540 --> 00:28:08.630
the normalized
posterior distribution.

00:28:08.630 --> 00:28:10.850
You can run this
algorithm as nice

00:28:10.850 --> 00:28:19.580
as you can calculate the
ratio of the two posters.

00:28:19.580 --> 00:28:23.120
So the reason I hesitate a
little bit to write this--

00:28:23.120 --> 00:28:28.865
I copied this formula
from the previous slide.

00:28:28.865 --> 00:28:34.580
So to decide whether
we accept or reject,

00:28:34.580 --> 00:28:39.420
we need to calculate the
ratio of the posterior

00:28:39.420 --> 00:28:42.090
for the new value
of the parameter

00:28:42.090 --> 00:28:45.800
with the posterior
of the old value.

00:28:45.800 --> 00:28:48.570
So in the formula, we
had this thing here

00:28:48.570 --> 00:28:51.240
that's the numerator--

00:28:51.240 --> 00:28:57.340
so it's a denominator that
occurs here, so we cancel this.

00:29:00.760 --> 00:29:06.600
So I think many of us left
school a long time ago.

00:29:06.600 --> 00:29:12.630
So I went to the internet
to search for some intuitive

00:29:12.630 --> 00:29:15.130
explanations of rules
of cancellation,

00:29:15.130 --> 00:29:18.070
and I found something
very exciting to show you.

00:29:18.070 --> 00:29:22.020
Here, this is-- uh, yeah.

00:29:22.020 --> 00:29:24.470
This-- OK.

00:29:24.470 --> 00:29:29.655
This is probably applying
cancellation too aggressively.

00:29:29.655 --> 00:29:30.990
But you get an idea.

00:29:34.230 --> 00:29:40.010
Anyway, so the
cancellation here is valid,

00:29:40.010 --> 00:29:43.710
and so we carry on
with the algorithm.

00:29:43.710 --> 00:29:49.020
You may also be interested to
know that this Markov chain

00:29:49.020 --> 00:29:56.430
Monte Carlo algorithm is the
byproduct of the hydrogen bomb

00:29:56.430 --> 00:29:58.480
project.

00:29:58.480 --> 00:30:04.050
Maybe this is the best
outcome of that project.

00:30:04.050 --> 00:30:12.700
The paper was published in 1953
by several famous physicists

00:30:12.700 --> 00:30:17.160
working in Los
Alamos on the H-bomb.

00:30:17.160 --> 00:30:23.525
OK, so this is my introduction
to Bayesian statistics.

00:30:23.525 --> 00:30:29.740
Now I'm going to talk about
genetics and coalescent.

00:30:29.740 --> 00:30:37.170
So we have 22 chromosomes here.

00:30:37.170 --> 00:30:41.520
They are called autosomes, plus
another pair of chromosomes

00:30:41.520 --> 00:30:47.050
that are called the sex
chromosomes, X and Y.

00:30:47.050 --> 00:30:51.090
So in the kind of work
we do, we use genetics,

00:30:51.090 --> 00:30:53.280
there's chromosomes,
or the DNA sequences

00:30:53.280 --> 00:30:59.210
to trace the genealogical
histories of individuals

00:30:59.210 --> 00:31:01.120
or human populations.

00:31:01.120 --> 00:31:05.520
So you can imagine
that the Y chromosome

00:31:05.520 --> 00:31:09.570
traces the paternal history.

00:31:09.570 --> 00:31:13.500
So each boy has a father.

00:31:13.500 --> 00:31:15.465
The father has a father.

00:31:15.465 --> 00:31:19.800
So if you trace
that paternal line,

00:31:19.800 --> 00:31:24.640
you can basically
trace the history.

00:31:24.640 --> 00:31:29.460
So for two boys, you can
trace the parental lineages

00:31:29.460 --> 00:31:33.990
until you find a great
great grandfather.

00:31:33.990 --> 00:31:37.815
The same thing for the female.

00:31:37.815 --> 00:31:40.470
You can use the
mitochondrial genome

00:31:40.470 --> 00:31:46.100
to trace the maternal history.

00:31:46.100 --> 00:31:50.610
Now. for the autosomes,
you have two chromosomes,

00:31:50.610 --> 00:31:53.570
one from the mom,
another from the dad.

00:31:53.570 --> 00:32:00.780
And they have a complicated
process of recombination.

00:32:00.780 --> 00:32:04.410
And as a result, the different
segments of the genome

00:32:04.410 --> 00:32:08.400
can give us different histories.

00:32:08.400 --> 00:32:11.160
This is like recombination.

00:32:11.160 --> 00:32:14.520
So in my talk here,
there are different ways

00:32:14.520 --> 00:32:15.850
of analyzing the data.

00:32:15.850 --> 00:32:18.420
In my talk here,
I will just focus

00:32:18.420 --> 00:32:30.120
on short segments in the
autosomes, so that the size--

00:32:30.120 --> 00:32:33.310
each segment has one history.

00:32:33.310 --> 00:32:35.940
But then different
segments have more or less

00:32:35.940 --> 00:32:37.980
independent histories.

00:32:37.980 --> 00:32:45.060
So we are going to use the short
DNA sequences in the genome

00:32:45.060 --> 00:32:49.070
to look at the
genealogical relationships

00:32:49.070 --> 00:32:51.220
and to trace the history.

00:32:51.220 --> 00:32:56.760
So here is one imaginary case.

00:32:56.760 --> 00:33:00.190
We talk about population
of only 10 individuals.

00:33:00.190 --> 00:33:03.630
It's a very small. population.

00:33:03.630 --> 00:33:07.235
Each individual
has two sequences.

00:33:07.235 --> 00:33:11.550
We can imagine those two
are from one individual,

00:33:11.550 --> 00:33:13.570
and then these are from another.

00:33:13.570 --> 00:33:19.410
So in the traditional population
genetics, time runs forward.

00:33:19.410 --> 00:33:22.500
So those other generations--

00:33:22.500 --> 00:33:26.490
so the sequences
[INAUDIBLE] descendents.

00:33:26.490 --> 00:33:30.000
My parents have children.

00:33:30.000 --> 00:33:35.430
But we turn these process
backwards in time.

00:33:35.430 --> 00:33:37.170
We focus on a sample--

00:33:37.170 --> 00:33:42.100
let's say we have five
sequences we sample today,

00:33:42.100 --> 00:33:45.180
and then we'll trace
their genealogical history

00:33:45.180 --> 00:33:46.990
backwards in time.

00:33:46.990 --> 00:33:50.240
So then we have this process
that's called coalescent.

00:33:50.240 --> 00:33:54.750
So coalescent basically
means the joining

00:33:54.750 --> 00:33:59.760
of the sequences when we
found the common ancestor.

00:34:04.070 --> 00:34:07.430
This model here is called
the Fisher-Wright model,

00:34:07.430 --> 00:34:11.900
and this backwards view
of the same process

00:34:11.900 --> 00:34:14.929
is called the coalescent.

00:34:14.929 --> 00:34:18.190
Let's go to some pictures
here, that's Fisher-Wright.

00:34:18.190 --> 00:34:23.330
And this is John Kingman, who's
responsible for the development

00:34:23.330 --> 00:34:26.030
of the coalescent.

00:34:26.030 --> 00:34:30.230
Fisher is both a statistician
and a geneticist.

00:34:33.139 --> 00:34:35.989
Not ordinary ones, I suppose.

00:34:35.989 --> 00:34:43.790
So for two sequences, it takes
an average 2N generations

00:34:43.790 --> 00:34:47.610
for them to find
a common ancestor.

00:34:47.610 --> 00:34:53.739
So maybe intuitively, you can
think about a small village

00:34:53.739 --> 00:34:56.900
where they have
inbreeding-- they don't

00:34:56.900 --> 00:35:00.880
marry with outside people.

00:35:00.880 --> 00:35:07.410
And then you can imagine that
over just a few generations,

00:35:07.410 --> 00:35:12.340
two people will find a
grandma or great grandma.

00:35:12.340 --> 00:35:19.010
So coalescent happens much
faster in a small population.

00:35:19.010 --> 00:35:25.430
So the average waiting
time for the two sequences

00:35:25.430 --> 00:35:29.270
is 2N, which is the number
of sequences, basically.

00:35:29.270 --> 00:35:33.860
So this explains that
the population size

00:35:33.860 --> 00:35:38.180
determines the rate at which
the coalescent happens.

00:35:38.180 --> 00:35:42.710
Just as a reference, for
the human population,

00:35:42.710 --> 00:35:49.160
we have the estimate of
the average difference

00:35:49.160 --> 00:35:52.310
between-- added to
human sequences.

00:35:52.310 --> 00:35:59.110
So that's slightly less than
1 difference per 1,000 size,

00:35:59.110 --> 00:36:01.460
or per KB.

00:36:01.460 --> 00:36:05.030
So this genetic
variation measure

00:36:05.030 --> 00:36:08.160
is calculated using
this result here.

00:36:08.160 --> 00:36:11.750
So the average waiting
time is 2N generations.

00:36:11.750 --> 00:36:14.920
If the the mutation rate
is mu per generation,

00:36:14.920 --> 00:36:17.840
you just multiply the
time with the rate

00:36:17.840 --> 00:36:20.890
to get the genetic distance.

00:36:20.890 --> 00:36:26.030
But you have two sides on these
like pedigree or genealogical

00:36:26.030 --> 00:36:26.580
tree.

00:36:26.580 --> 00:36:30.830
So you multiply
2N by mu by 2, you

00:36:30.830 --> 00:36:36.440
get 4mu, which gives us the
average genetic distance

00:36:36.440 --> 00:36:40.430
between two sequences.

00:36:40.430 --> 00:36:44.600
So that's two
genes, or sequences.

00:36:44.600 --> 00:36:48.430
Where you move to 20 sequences--

00:36:48.430 --> 00:36:53.600
here are some genealogical
trees, called gene trees,

00:36:53.600 --> 00:36:55.970
which we simulated.

00:36:55.970 --> 00:37:01.440
The first is that there's a lot
of fluctuation in this process.

00:37:01.440 --> 00:37:06.290
So the waiting time
fluctuates a lot.

00:37:06.290 --> 00:37:08.240
The other features that--

00:37:08.240 --> 00:37:13.360
the waiting time tends to
be much longer when you

00:37:13.360 --> 00:37:15.670
have fewer [INAUDIBLE] left.

00:37:15.670 --> 00:37:18.950
So on average, the
last waiting time

00:37:18.950 --> 00:37:21.880
is half of the total waiting
time for this sample.

00:37:24.890 --> 00:37:26.930
So that's coalescent.

00:37:26.930 --> 00:37:31.520
What I did with my
collaborator, Bruce Rannala,

00:37:31.520 --> 00:37:36.170
is to extend this to
the multispecies case.

00:37:36.170 --> 00:37:39.630
So the extension is
very, very much trivial.

00:37:39.630 --> 00:37:44.340
So here we have two species
of populations A and B.

00:37:44.340 --> 00:37:50.810
So basically, when you
have sequences from A,

00:37:50.810 --> 00:37:54.452
they can coalesce when
you go backwards in time.

00:37:54.452 --> 00:37:57.770
But sequence from A-- they can
not coalesce from sequences

00:37:57.770 --> 00:38:04.740
from B. You have to wait until
you reach the common ancestor.

00:38:04.740 --> 00:38:09.950
So apart from that, it's very
much the single population

00:38:09.950 --> 00:38:11.980
coalescing.

00:38:11.980 --> 00:38:17.650
So I'm going to show you
some results concerning

00:38:17.650 --> 00:38:19.610
human, chimp, gorilla--

00:38:19.610 --> 00:38:21.420
these great apes.

00:38:21.420 --> 00:38:25.385
So I use the George W.
Bush as our representative

00:38:25.385 --> 00:38:28.630
of the human.

00:38:28.630 --> 00:38:32.300
But it doesn't matter
who you use there.

00:38:32.300 --> 00:38:36.140
But you can tell this
slide is quite old.

00:38:36.140 --> 00:38:38.580
I just feel like some people
feel nostalgic I think.

00:38:42.060 --> 00:38:50.220
OK, so if you have a human
sequence, a chimp sequence,

00:38:50.220 --> 00:38:53.760
a gorilla sequence
from one segment,

00:38:53.760 --> 00:38:59.230
you make it a genealogical tree
that's like the red one here.

00:38:59.230 --> 00:39:02.430
But if you look at another
segment of the genome,

00:39:02.430 --> 00:39:07.030
you may have a tree that
looks like the blue one here.

00:39:07.030 --> 00:39:11.010
So they are all
three possibilities,

00:39:11.010 --> 00:39:13.690
which are listed here.

00:39:13.690 --> 00:39:17.940
So in this particular case, we
have a very stable estimate.

00:39:17.940 --> 00:39:21.300
So the human, chimp
are close to each other

00:39:21.300 --> 00:39:23.880
in terms of the relationships.

00:39:23.880 --> 00:39:28.510
So 70% of genomic regions
will share this relationship.

00:39:28.510 --> 00:39:33.250
But 30% of the other
genome convergence--

00:39:33.250 --> 00:39:38.520
the sequences have a different
relationship with the species.

00:39:38.520 --> 00:39:42.680
To many people, this
looks very surprising.

00:39:42.680 --> 00:39:47.640
But that's a very
stable estimate.

00:39:47.640 --> 00:39:52.260
So this multispecies
coalescent is very much similar

00:39:52.260 --> 00:39:54.780
to the single species
version, except that we

00:39:54.780 --> 00:39:58.910
have species
divergence times here,

00:39:58.910 --> 00:40:02.100
as well as the population
size parameters which set

00:40:02.100 --> 00:40:05.090
that rate of the coalescent.

00:40:05.090 --> 00:40:10.170
OK, so we are using this
multispecies coalescent

00:40:10.170 --> 00:40:13.500
to make many kinds
of inferences.

00:40:13.500 --> 00:40:19.920
For example, we can do
species delimitation,

00:40:19.920 --> 00:40:23.595
like trying to work out whether
you have one species or two.

00:40:23.595 --> 00:40:30.540
We can estimate the phylogeny
or the phylogenetic tree

00:40:30.540 --> 00:40:31.590
of the species.

00:40:31.590 --> 00:40:35.220
Quite a few of these
Judith mentioned already.

00:40:35.220 --> 00:40:37.950
So I'm just going to
show you a few slides.

00:40:37.950 --> 00:40:42.300
This one's somebody's
analysis using these methods

00:40:42.300 --> 00:40:47.050
to decide that there were
four giraffe species rather

00:40:47.050 --> 00:40:48.570
than one.

00:40:48.570 --> 00:40:52.710
Species delimitation is one of
the most controversial areas

00:40:52.710 --> 00:40:58.000
in biology, so
this also generated

00:40:58.000 --> 00:41:00.660
a lot of controversies.

00:41:00.660 --> 00:41:03.250
So the theta we have,
I did like this, just

00:41:03.250 --> 00:41:08.370
to give you a rough
idea of what we do.

00:41:08.370 --> 00:41:15.319
You have the sequence
alignment from the genomes,

00:41:15.319 --> 00:41:16.860
and then we have
some kind of a model

00:41:16.860 --> 00:41:21.260
to describe the changes of these
so-called nucleotides, T, C, A,

00:41:21.260 --> 00:41:23.400
G, during the evolution.

00:41:26.160 --> 00:41:29.130
Essentially we have
the species phylogeny.

00:41:29.130 --> 00:41:31.130
Given the species
phylogeny, we have

00:41:31.130 --> 00:41:34.650
this gene genealogy
describing the relationship

00:41:34.650 --> 00:41:36.140
of the sequences.

00:41:36.140 --> 00:41:40.390
And then we use models to
calculate the probabilities.

00:41:40.390 --> 00:41:43.800
So again, we have to deal
with a prior likelihood, which

00:41:43.800 --> 00:41:47.280
give us the posterior.

00:41:47.280 --> 00:41:51.760
The algorithm is very much like
that hill-climbing algorithm.

00:41:51.760 --> 00:41:57.310
I'm not going into the details,
but you just change something

00:41:57.310 --> 00:41:58.620
in the program.

00:41:58.620 --> 00:42:02.540
You decide whether you are
going to accept or reject,

00:42:02.540 --> 00:42:04.860
and in the long run,
you will be visiting

00:42:04.860 --> 00:42:07.855
the species tree in
proportion to their posterior.

00:42:11.070 --> 00:42:14.116
So I'm going just to show you--

00:42:14.116 --> 00:42:18.860
yes, some of those
algorithms for introducing

00:42:18.860 --> 00:42:22.930
changes can be very complex.

00:42:22.930 --> 00:42:27.280
So that's basically what
we spend our time on.

00:42:27.280 --> 00:42:31.780
So I'm just going to show you
one example about the gibbons.

00:42:31.780 --> 00:42:36.700
So those two are one
species, and then there

00:42:36.700 --> 00:42:39.900
are four genera of gibbons.

00:42:39.900 --> 00:42:41.720
The phylogeny
among these gibbons

00:42:41.720 --> 00:42:45.730
are very much
[INAUDIBLE] very messy.

00:42:48.290 --> 00:42:52.510
So this indicates the
messiness of the--

00:42:52.510 --> 00:42:55.670
difficulty of working
out the relationships.

00:42:55.670 --> 00:43:00.670
So we managed to use our program
to analyze this large data

00:43:00.670 --> 00:43:02.660
set-- to get a tree.

00:43:02.660 --> 00:43:07.250
So I think I'm going to
just quickly run to the end.

00:43:07.250 --> 00:43:10.600
So we got our tree, and that
seems to be very reliable.

00:43:10.600 --> 00:43:12.940
And we did some
simulations to share

00:43:12.940 --> 00:43:17.280
that the answers are trustable.

00:43:20.000 --> 00:43:25.030
The multispecies coalescent
provides a powerful framework

00:43:25.030 --> 00:43:27.790
for analysis of the
genetic sequence theta.

00:43:27.790 --> 00:43:31.960
There are two approaches
in using the framework

00:43:31.960 --> 00:43:34.210
to analyze real data.

00:43:34.210 --> 00:43:37.960
Some people go for the
quick and dirty methods,

00:43:37.960 --> 00:43:41.980
but they are very powerful and
can manage large data sets.

00:43:41.980 --> 00:43:46.465
We are going for the more
delicate, sophisticated method,

00:43:46.465 --> 00:43:50.770
but we have a lot of trouble
with the computation.

00:43:50.770 --> 00:43:53.080
Our programs typically
run for a few weeks--

00:43:53.080 --> 00:43:54.190
sometimes a few months.

00:43:57.630 --> 00:44:01.000
OK, so I'm going
to just acknowledge

00:44:01.000 --> 00:44:03.910
my collaborator Bruce Rannala.

00:44:03.910 --> 00:44:06.970
He'll actually be visiting
here next month, in case

00:44:06.970 --> 00:44:08.590
you want to talk with him.

00:44:08.590 --> 00:44:12.020
So we are developing
those methods.

00:44:12.020 --> 00:44:16.100
Chengmin did the analysis
of the gibbon data.

00:44:16.100 --> 00:44:21.090
So I'll just show you
one of his slides here.

00:44:21.090 --> 00:44:25.330
People analyzing
the-- this is a flux

00:44:25.330 --> 00:44:27.685
to work on their phylogenies.

00:44:27.685 --> 00:44:31.510
I like this, because they
are also playing a card game,

00:44:31.510 --> 00:44:32.250
apparently.

00:44:32.250 --> 00:44:35.890
They're just having some kind
of tricks under the table.

00:44:35.890 --> 00:44:40.360
I think they are supposed to
mean that the species actually

00:44:40.360 --> 00:44:41.690
hybridized.

00:44:41.690 --> 00:44:46.030
But here I like it
because the gambling,

00:44:46.030 --> 00:44:51.700
in the same way that we rely
on those methods developed

00:44:51.700 --> 00:44:56.040
in the context of gambling to
make genetic [? inferences. ?]

00:44:56.040 --> 00:44:58.060
Sorry, I'm a bit
rushed at the end.

00:44:58.060 --> 00:44:58.660
Thank you.

00:44:58.660 --> 00:45:00.760
[APPLAUSE]

00:45:00.760 --> 00:45:04.110
[MUSIC PLAYING]

