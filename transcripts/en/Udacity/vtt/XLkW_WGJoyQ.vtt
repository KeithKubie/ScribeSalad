WEBVTT
Kind: captions
Language: en

00:00:00.450 --> 00:00:02.270
All right, so here's a summary of
what we're just talking about.

00:00:02.270 --> 00:00:04.950
We have, actually have these two
different forms of the bellman equation.

00:00:04.950 --> 00:00:07.970
One V which we can think
of as standing for value.

00:00:07.970 --> 00:00:11.260
And one Q which we can think
of as standing for quality.

00:00:11.260 --> 00:00:13.940
But mostly they're just letters from
the end of the alphabet that we

00:00:13.940 --> 00:00:14.880
made up names for.

00:00:14.880 --> 00:00:16.950
So you had U for utility.

00:00:16.950 --> 00:00:18.510
This is V for value and Q for quality.

00:00:18.510 --> 00:00:20.900
But anyway, the point is that
these are just expressions,

00:00:20.900 --> 00:00:25.410
that represent different pieces of
this overall sequence of states and

00:00:25.410 --> 00:00:26.530
rewards and actions.

00:00:26.530 --> 00:00:29.740
&gt;&gt; Okay, so I got two questions.

00:00:29.740 --> 00:00:31.120
One is, why?

00:00:31.120 --> 00:00:34.560
I mean, why are we bothering to
create this quality function,

00:00:34.560 --> 00:00:38.060
when the value function worked
just fine, thank you very much?

00:00:38.060 --> 00:00:42.450
Well, I'll ask the second question
after you give me a satisfactory

00:00:42.450 --> 00:00:43.730
answer to the first one.

00:00:43.730 --> 00:00:44.425
&gt;&gt; So why?

00:00:44.425 --> 00:00:46.980
So, in some sense because
they're both equally good.

00:00:46.980 --> 00:00:49.400
But in fact what's going to
turn out to be the case,

00:00:49.400 --> 00:00:51.760
is that this Q form of
the Bellman equation.

00:00:51.760 --> 00:00:55.050
Is going to be much more useful in
the context of reinforcement learning.

00:00:55.050 --> 00:00:57.690
And the reason for
that is we're going to be able to take

00:00:57.690 --> 00:01:02.370
expectations of this quantity
using just experienced data.

00:01:02.370 --> 00:01:06.470
And you don't need to actually to have
direct access to the reward function or

00:01:06.470 --> 00:01:07.290
the transition function.

00:01:07.290 --> 00:01:08.000
To do that.

00:01:08.000 --> 00:01:10.580
Whereas if we're going to
try to learn V values.

00:01:10.580 --> 00:01:14.410
The only way to connect one
V value to the next V value

00:01:14.410 --> 00:01:16.860
is by knowing the transitions and
rewards.

00:01:16.860 --> 00:01:19.380
So this is going to be really helpful in
the context of reinforcement learning.

00:01:19.380 --> 00:01:21.350
Where we don't know the transitions and
rewards in advance.

00:01:22.440 --> 00:01:23.560
&gt;&gt; Okay, I'll buy that.

00:01:23.560 --> 00:01:26.180
So here's my second question then.

00:01:26.180 --> 00:01:31.350
You've got V function for
what happens when you're at a state.

00:01:31.350 --> 00:01:34.900
You've got a Q function would
happen when you're at an action.

00:01:34.900 --> 00:01:37.680
There's at least one more thing
when you're at that infinite

00:01:37.680 --> 00:01:40.510
jar jar jar jar jar, and that's R.

00:01:41.980 --> 00:01:42.680
&gt;&gt; Yes.

00:01:42.680 --> 00:01:47.060
You're saying we can group things this
way, or we can group things this way.

00:01:47.060 --> 00:01:48.680
So why can't we group things this way?

00:01:50.490 --> 00:01:52.770
&gt;&gt; Sure, that's what I'm asking.

00:01:52.770 --> 00:01:54.600
&gt;&gt; because if we go one step more
than that then we're back to

00:01:54.600 --> 00:01:56.080
the max again and that was our V.

00:01:56.080 --> 00:01:56.600
&gt;&gt; Right.

00:01:56.600 --> 00:02:00.150
&gt;&gt; So yeah, that seems to be the third
member of our little trio that's

00:02:00.150 --> 00:02:00.880
been left out.

00:02:00.880 --> 00:02:02.960
So I think maybe we should
have a quiz about that.

00:02:02.960 --> 00:02:03.580
&gt;&gt; Of course we should.

