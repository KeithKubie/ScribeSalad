WEBVTT
Kind: captions
Language: en

00:00:00.000 --> 00:00:02.007
Let's analyze this algorithm.

00:00:02.007 --> 00:00:05.934
What we want to know is how much work is it going to take for us to complete this algorithm,

00:00:05.934 --> 00:00:10.035
how many operations are we going to do, and how many steps is that going to take?

00:00:10.035 --> 00:00:12.436
So the analysis is straightforward.

00:00:12.436 --> 00:00:17.396
For either flavor, inclusive or exclusive, for each of the n iterations of the loop,

00:00:17.396 --> 00:00:21.507
we take 1 step and we do 1 operation.

00:00:21.507 --> 00:00:28.934
Thus we require n operations to complete the algorithm, and we also require n steps.

00:00:28.934 --> 00:00:33.679
But the parallelization of this algorithm is more complex than with reduce.

00:00:33.679 --> 00:00:36.793
With reduce it was really clear that the different pieces of the computation tree

00:00:36.793 --> 00:00:40.926
could be computed in parallel. With scan this is much less clear.

00:00:40.926 --> 00:00:43.626
So the questions you should be asking yourself now are

00:00:43.626 --> 00:00:46.227
a) how do we compute scan and parallel at all,

00:00:46.227 --> 00:00:49.420
and b) how can we reduce the work and step complexity

00:00:49.420 --> 00:00:52.272
of a parallel implementation as much as possible?

00:00:52.288 --> 00:00:56.218
But even before we get to those 2 questions, let's answer a larger question.

00:00:56.218 --> 00:01:00.158
Why do I care about the parallelization of scan in the 1st place?

00:01:00.158 --> 00:01:02.917
So here's a high level, intuitive explanation.

00:01:02.917 --> 00:01:05.743
Many computations have the following form.

00:01:05.743 --> 00:01:09.717
We want to process a list of inputs, which I'm showing here with blue circles,

00:01:09.717 --> 00:01:13.809
and create a list of outputs, which I'm showing here with red circles.

00:01:13.809 --> 00:01:19.057
And we begin by computing this 1st output from this 1st input.

00:01:19.057 --> 00:01:24.753
Then we use that 1st output and the 2nd input to compute the 2nd output,

00:01:24.753 --> 00:01:31.226
use that 2nd output and the 3rd input to create the 3rd output, and so on.

00:01:31.226 --> 00:01:35.006
For instance, in our checkbook example we used the previous balance

00:01:35.006 --> 00:01:39.311
and the next check amount to compute the current balance.

00:01:39.311 --> 00:01:43.141
This creates a dependency structure that looks like what I've drawn in green.

00:01:43.141 --> 00:01:47.264
When we structure our computation like this, we see that it has no apparent concurrency.

00:01:47.264 --> 00:01:49.469
We're only doing 1 operation at a time.

00:01:49.469 --> 00:01:55.056
First we compute the 1st output, and then we use the 1st output as an input to get the 2nd output,

00:01:55.056 --> 00:01:58.801
we use this output to get the 3rd output, and so on.

00:01:58.801 --> 00:02:03.173
Now, this is a very serial structure, and if we implement it in this way

00:02:03.173 --> 00:02:06.029
it would be a poor fit for a GPU indeed.

00:02:06.029 --> 00:02:10.038
Computations like this can often be expressed as a scan.

00:02:10.038 --> 00:02:13.864
And if we can characterize our computation in terms of scan, that's great,

00:02:13.864 --> 00:02:17.228
because--let me tell you a little secret--we can parallelize scan

00:02:17.228 --> 00:02:20.880
and make it run fast on a parallel processor like a GPU.

00:02:20.880 --> 00:02:24.017
Now, not all computations with this sort of dependency structure

00:02:24.017 --> 00:02:26.601
will fit into the scan model, but many will.

00:02:26.601 --> 00:02:30.828
And for those computations, leveraging an efficient parallel scan implementation

00:02:30.828 --> 00:02:34.356
turns the problem from something that would be a poor fit for a GPU

00:02:34.356 --> 00:02:37.269
into something that is now a much better fit.

00:02:37.269 --> 00:02:40.843
So how do we parallelize a scan computation?

